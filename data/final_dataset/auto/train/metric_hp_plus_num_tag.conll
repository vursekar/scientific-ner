This	O
paper	O
describes	O
the	O
result	O
of	O
team	O
-	O
Maoqin	O
at	O
DravidianLangTech	O
-	O
EACL2021	O
.	O
The	O
provided	O
task	O
consists	O
of	O
three	O
languages	O
(	O
Tamil	O
,	O
Malayalam	O
,	O
and	O
Kannada	O
)	O
,	O
I	O
only	O
participate	O
in	O
one	O
of	O
the	O
language	O
task	O
-	O
Malayalam	O
.	O
The	O
goal	O
of	O
this	O
task	O
is	O
to	O
identify	O
offensive	O
language	O
content	O
of	O
the	O
code	O
-	O
mixed	O
dataset	O
of	O
comments	O
/	O
posts	O
in	O
Dravidian	O
Languages	O
(	O
Tamil	O
-	O
English	O
,	O
Malayalam	O
-	O
English	O
,	O
and	O
Kannada	O
-	O
English	O
)	O
collected	O
from	O
social	O
media	O
.	O
This	O
is	O
a	O
classification	O
task	O
at	O
the	O
comment	O
/	O
post	O
level	O
.	O
Given	O
a	O
Youtube	O
comment	O
,	O
systems	O
have	O
to	O
classify	O
it	O
into	O
Notoffensive	O
,	O
Offensive	O
-	O
untargeted	O
,	O
Offensivetargeted	O
-	O
individual	O
,	O
Offensive	O
-	O
targeted	O
-	O
group	O
,	O
Offensive	O
-	O
targeted	O
-	O
other	O
,	O
or	O
Not	O
-	O
in	O
-	O
indentedlanguage	O
.	O
I	O
use	O
the	O
transformer	O
-	O
based	O
language	O
model	O
with	O
BiGRU	B-MethodName
-	O
Attention	O
to	O
complete	O
this	O
task	O
.	O
To	O
prove	O
the	O
validity	O
of	O
the	O
model	O
,	O
I	O
also	O
use	O
some	O
other	O
neural	O
network	O
models	O
for	O
comparison	O
.	O
And	O
finally	O
,	O
the	O
team	O
ranks	O
5th	O
in	O
this	O
task	O
with	O
a	O
weighted	O
average	B-MetricName
F1	I-MetricName
score	O
of	O
0.93	O
on	O
the	O
private	O
leader	O
board	O
.	O

Offensive	O
language	O
refers	O
to	O
direct	O
or	O
indirect	O
use	O
of	O
verbal	O
abuse	O
,	O
slander	O
,	O
contempt	O
,	O
ridicule	O
,	O
and	O
other	O
means	O
to	O
infringe	O
or	O
damage	O
the	O
dignity	O
,	O
spiritual	O
world	O
,	O
and	O
mental	O
health	O
of	O
others	O
.	O
It	O
will	O
seriously	O
affect	O
the	O
mental	O
state	O
of	O
others	O
,	O
disrupt	O
work	O
,	O
the	O
life	O
and	O
learning	O
order	O
of	O
others	O
,	O
and	O
seriously	O
pollute	O
the	O
public	O
opinion	O
environment	O
of	O
the	O
entire	O
network	O
(	O
Schmidt	O
and	O
Wiegand	O
,	O
2017	O
)	O
.	O
Due	O
to	O
the	O
development	O
of	O
the	O
Internet	O
and	O
the	O
popularity	O
of	O
anonymous	O
comments	O
,	O
many	O
offensive	O
languages	O
have	O
spread	O
on	O
the	O
Internet	O
and	O
caused	O
trouble	O
to	O
relevant	O
personnel	O
Mahesan	O
,	O
2019	O
,	O
2020a	O
,	O
b	O
)	O
.	O
Relevant	O
organizations	O
should	O
take	O
measures	O
to	O
prevent	O
this	O
from	O
happening	O
.	O
It	O
is	O
unrealistic	O
to	O
judge	O
whether	O
online	O
sentences	O
are	O
completely	O
offended	O
by	O
humans	O
.	O
There	O
-	O
fore	O
,	O
mechanical	O
methods	O
must	O
be	O
used	O
to	O
distinguish	O
whether	O
the	O
language	O
is	O
offensive	O
.	O
The	O
task	O
is	O
to	O
directly	O
test	O
whether	O
the	O
system	O
can	O
distinguish	O
offensive	O
language	O
in	O
Dravidian	O
languages	O
.	O
Dravidian	O
languages	O
are	O
a	O
group	O
of	O
languages	O
spoken	O
by	O
220	O
million	O
people	O
,	O
predominantly	O
in	O
southern	O
India	O
and	O
northern	O
Sri	O
Lanka	O
,	O
but	O
also	O
in	O
other	O
areas	O
of	O
South	O
Asia	O
.	O
The	O
Dravidian	O
languages	O
were	O
first	O
recorded	O
in	O
Tamili	O
script	O
inscribed	O
on	O
cave	O
walls	O
in	O
Tamil	O
Nadu	O
's	O
Madurai	O
and	O
Tirunelveli	O
districts	O
in	O
the	O
6th	O
century	O
BCE	O
.	O
The	O
Dravidian	O
languages	O
are	O
closely	O
related	O
languages	O
the	O
are	O
under	O
-	O
resourced	O
(	O
Chakravarthi	O
,	O
2020	O
)	O
.	O
Existing	O
deep	O
learning	O
and	O
pre	O
-	O
training	O
models	O
have	O
achieved	O
good	O
results	O
on	O
other	O
tasks	O
(	O
Zampieri	O
et	O
al	O
,	O
2019	O
)	O
,	O
so	O
I	O
use	O
the	O
deep	O
learning	O
method	O
to	O
deal	O
with	O
the	O
related	O
task	O
.	O
According	O
to	O
the	O
latest	O
related	O
research	O
progress	O
,	O
the	O
transformer	O
-	O
based	O
language	O
model	O
has	O
become	O
my	O
preferred	O
model	O
.	O
Because	O
the	O
pre	O
-	O
trained	O
and	O
fine	O
-	O
tuned	O
transformersbased	O
models	O
have	O
shown	O
excellent	O
performance	O
in	O
many	O
NLP	O
problems	O
,	O
such	O
as	O
sentiment	O
classification	O
and	O
automatic	O
extraction	O
of	O
text	O
summaries	O
.	O
So	O
I	O
choose	O
ALBERT	B-MethodName
(	O
Lan	O
et	O
al	O
,	O
2019	O
)	O
as	O
my	O
basic	O
model	O
in	O
this	O
task	O
.	O
To	O
get	O
a	O
more	O
effective	O
and	O
higher	O
accuracy	B-MetricName
model	O
,	O
BiGRU	B-MethodName
combined	O
with	O
attention	O
.	O
To	O
prove	O
the	O
effectiveness	O
of	O
this	O
model	O
,	O
I	O
have	O
also	O
done	O
comparative	O
experiments	O
with	O
other	O
neural	O
networks	O
.	O
In	O
this	O
task	O
,	O
my	O
model	O
is	O
an	O
effective	O
way	O
to	O
perform	O
well	O
.	O
To	O
obtain	O
as	O
much	O
effective	O
information	O
as	O
possible	O
from	O
the	O
limited	O
data	O
,	O
I	O
also	O
use	O
the	O
5	O
-	O
fold	O
cross	O
-	O
validation	O
method	O
.	O
my	O
model	O
achieves	O
the	O
desired	O
result	O
.	O
The	O
rest	O
of	O
this	O
article	O
is	O
structured	O
as	O
follows	O
.	O
Section	O
2	O
introduces	O
related	O
work	O
.	O
Model	O
and	O
data	O
preparation	O
are	O
described	O
in	O
Section	O
3	O
.	O
Experiments	O
and	O
evaluation	O
are	O
described	O
in	O
Section	O
4	O
.	O
Section	O
5	O
describes	O
the	O
results	O
of	O
my	O
work	O
.	O
The	O
conclusions	O
and	O
future	O
work	O
are	O
drawn	O
in	O
Section	O
6	O
.	O

There	O
are	O
many	O
competitions	O
about	O
offensive	O
language	O
detection	O
(	O
such	O
as	O
HASOC	O
(	O
Chakravarthi	O
et	O
al	O
,	O
2020c	O
;	O
Mandl	O
et	O
al	O
,	O
2020	O
)	O
and	O
TRAC	O
(	O
Kumar	B-DatasetName
et	O
al	O
,	O
2018	O
)	O
)	O
,	O
and	O
many	O
corresponding	O
methods	O
have	O
been	O
produced	O
.	O
People	O
often	O
tend	O
to	O
abstract	O
this	O
task	O
into	O
a	O
text	B-TaskName
classification	I-TaskName
task	O
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
.	O
Text	B-TaskName
classification	I-TaskName
is	O
called	O
extracting	O
features	O
from	O
original	O
text	O
data	O
and	O
predicting	O
the	O
category	O
of	O
text	O
data	O
based	O
on	O
these	O
features	O
.	O
In	O
the	O
past	O
few	O
decades	O
,	O
many	O
models	O
for	O
text	B-TaskName
classification	I-TaskName
have	O
been	O
proposed	O
(	O
Qian	O
,	O
2020	O
)	O
.	O
From	O
the	O
1960s	O
to	O
the	O
2010s	O
,	O
text	B-TaskName
classification	I-TaskName
models	O
based	O
on	O
shallow	O
learning	O
dominated	O
.	O
Shallow	O
learning	O
means	O
statistical	O
-	O
based	O
models	O
such	O
as	O
Naive	O
Bayes	O
(	O
NB	O
)	O
,	O
K	O
Nearest	O
Neighbors	O
(	O
KNN	O
)	O
(	O
Cover	O
and	O
Hart	O
,	O
1967	O
)	O
and	O
Support	O
Vector	O
Machines	O
(	O
SVM	B-MethodName
)	O
.	O
Compared	O
with	O
earlier	O
rulebased	O
methods	O
,	O
this	O
method	O
has	O
obvious	O
advantages	O
in	O
accuracy	B-MetricName
and	O
stability	O
.	O
However	O
,	O
these	O
methods	O
still	O
require	O
functional	O
design	O
,	O
which	O
is	O
time	O
-	O
consuming	O
and	O
expensive	O
.	O
In	O
addition	O
,	O
they	O
usually	O
ignore	O
the	O
natural	O
order	O
structure	O
or	O
context	O
information	O
in	O
the	O
text	O
data	O
,	O
which	O
makes	O
learning	O
the	O
semantic	O
information	O
of	O
words	O
difficult	O
.	O
Since	O
the	O
2010s	O
,	O
text	B-TaskName
classification	I-TaskName
has	O
gradually	O
changed	O
from	O
a	O
shallow	O
learning	O
model	O
to	O
a	O
deep	O
learning	O
model	O
.	O
Compared	O
with	O
methods	O
based	O
on	O
shallow	O
learning	O
,	O
deep	O
learning	O
methods	O
avoid	O
the	O
manual	O
design	O
of	O
rules	O
and	O
functions	O
and	O
automatically	O
provide	O
semantically	O
meaningful	O
representations	O
for	O
text	O
mining	O
.	O
Therefore	O
,	O
most	O
of	O
the	O
text	B-TaskName
classification	I-TaskName
research	O
work	O
is	O
based	O
on	O
DNN	O
(	O
Yu	O
et	O
al	O
,	O
2013	O
)	O
,	O
which	O
is	O
a	O
data	O
-	O
driven	O
method	O
with	O
high	O
computational	O
complexity	O
.	O
Few	O
studies	O
have	O
focused	O
on	O
shallow	O
learning	O
models	O
to	O
solve	O
the	O
limitations	O
of	O
computation	O
and	O
data	O
.	O
The	O
shallow	O
learning	O
model	O
speeds	O
up	O
the	O
text	B-TaskName
classification	I-TaskName
speed	O
,	O
improves	O
the	O
accuracy	B-MetricName
,	O
and	O
expands	O
the	O
application	O
range	O
of	O
shallow	O
learning	O
.	O
The	O
shallow	O
learning	O
method	O
is	O
a	O
type	O
of	O
machine	O
learning	O
.	O
It	O
learns	O
from	O
data	O
,	O
which	O
is	O
a	O
predefined	O
function	O
that	O
is	O
important	O
to	O
the	O
performance	O
of	O
the	O
predicted	O
value	O
.	O
However	O
,	O
element	O
engineering	O
is	O
an	O
arduous	O
and	O
giant	O
job	O
.	O
Before	O
training	O
the	O
classifier	O
,	O
we	O
need	O
to	O
collect	O
knowledge	O
or	O
experience	O
to	O
extract	O
features	O
from	O
the	O
original	O
text	O
.	O
The	O
shallow	O
learning	O
method	O
trains	O
the	O
initial	O
classifier	O
based	O
on	O
various	O
text	O
features	O
extracted	O
from	O
the	O
original	O
text	O
.	O
For	O
small	O
data	O
sets	O
,	O
under	O
the	O
limita	O
-	O

The	O
ALBERT	B-MethodName
model	O
belongs	O
to	O
transformer	O
-	O
based	O
language	O
models	O
.	O
The	O
ALBERT	B-MethodName
model	O
is	O
improved	O
on	O
the	O
basis	O
of	O
Bidirectional	O
Encoder	O
Representations	O
for	O
Transformers	O
(	O
BERT	B-MethodName
)	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
model	O
.	O
It	O
has	O
designed	O
a	O
parameter	O
reduction	O
method	O
to	O
reduce	O
memory	O
consumption	O
by	O
changing	O
the	O
result	O
of	O
the	O
original	O
embedding	O
parameter	O
P	O
(	O
the	O
product	O
of	O
the	O
vocabulary	O
size	O
V	O
and	O
the	O
hidden	B-HyperparameterName
layer	I-HyperparameterName
size	I-HyperparameterName
H	O
)	O
.	O
V	O
*	O
H	O
=	O
P	O
V	O
*	O
E	O
+	O
E	O
*	O
H	O
=	O
P	O
(	O
1	O
)	O
E	O
represents	O
the	O
size	O
of	O
the	O
low	O
-	O
dimensional	O
embedding	O
space	O
.	O
In	O
BERT	B-MethodName
,	O
E	O
=	O
H.	O
While	O
in	O
AL	O
-	O
BERT	B-MethodName
,	O
H	O
>	O
>	O
E	O
,	O
so	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
will	O
be	O
greatly	O
reduced	O
.	O
At	O
the	O
same	O
time	O
,	O
the	O
self	O
-	O
supervised	O
loss	B-MetricName
is	O
used	O
to	O
focus	O
on	O
the	O
internal	O
coherence	O
in	O
the	O
construction	O
of	O
sentences	O
.	O
The	O
ALBERT	B-MethodName
model	O
implements	O
three	O
embedding	O
layers	O
:	O
word	O
embedding	O
,	O
position	O
embedding	O
,	O
and	O
segment	O
embedding	O
.	O
The	O
token	O
embedding	O
layer	O
predicts	O
each	O
word	O
as	O
a	O
fixed	O
-	O
size	O
vector	O
.	O
Position	O
embedding	O
is	O
used	O
to	O
retain	O
position	O
information	O
,	O
use	O
a	O
vector	O
to	O
randomly	O
initialize	O
each	O
position	O
,	O
add	O
model	O
training	O
,	O
and	O
finally	O
obtain	O
an	O
embedding	O
containing	O
position	O
information	O
.	O
Segment	O
embedding	O
helps	O
BERT	B-MethodName
distinguish	O
between	O
paired	O
input	O
sequences	O
.	O

In	O
this	O
task	O
,	O
I	O
use	O
the	O
ALBERT	B-MethodName
model	O
to	O
pre	O
-	O
train	O
the	O
task	O
.	O
For	O
the	O
ALBERT	B-MethodName
model	O
,	O
the	O
main	O
hyperparameters	O
I	O
pay	O
attention	O
to	O
are	O
the	O
training	O
step	B-HyperparameterName
size	I-HyperparameterName
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
and	O
learning	B-HyperparameterName
rate	I-HyperparameterName
.	O
The	O
parameters	O
of	O
my	O
model	O
are	O
shown	O
in	O
Table	O
2	O
.	O
I	O
have	O
obtained	O
good	O
performance	O
using	O
the	O
ALBERT	B-MethodName
-	O
BASE	B-MethodName
.	O
1	O
model	O
.	O
Considering	O
that	O
BiGRU	B-MethodName
-	O
Attention	O
can	O
capture	O
contextual	O
information	O
well	O
and	O
extract	O
text	O
information	O
features	O
more	O
accurately	O
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
,	O
I	O
add	O
it	O
after	O
AL	O
-	O
BERT	B-MethodName
.	O
I	O
use	O
the	O
development	O
data	O
set	O
to	O
verify	O
the	O
performance	O
of	O
the	O
models	O
.	O
The	O
standard	O
of	O
judgment	O
is	O
a	O
weighted	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
,	O
and	O
this	O
standard	O
is	O
the	O
judgment	O
standard	O
used	O
for	O
my	O
task	O
.	O
Table3	O
lists	O
the	O
results	O
of	O
various	O
models	O
described	O
previously	O
.	O
The	O
best	O
performance	O
is	O
in	O
bold	O
.	O
My	O
model	O
gets	O
the	O
best	O
performance	O
of	O
0.93	O
.	O
As	O
shown	O
in	O
the	O
table	O
my	O
model	O
can	O
greatly	O
improve	O
the	O
performance	O
and	O
my	O
overall	O
approach	O
achieved	O
5th	O
place	O
on	O
the	O
final	O
leader	O
board	O
.	O

Today	O
when	O
many	O
practitioners	O
run	O
basic	O
NLP	O
on	O
the	O
entire	O
web	O
and	O
large	O
-	O
volume	O
traffic	O
,	O
faster	O
methods	O
are	O
paramount	O
to	O
saving	O
time	O
and	O
energy	O
costs	O
.	O
Recent	O
advances	O
in	O
GPU	O
hardware	O
have	O
led	O
to	O
the	O
emergence	O
of	O
bi	O
-	O
directional	O
LSTMs	O
as	O
a	O
standard	O
method	O
for	O
obtaining	O
pertoken	O
vector	O
representations	O
serving	O
as	O
input	O
to	O
labeling	O
tasks	O
such	O
as	O
NER	B-TaskName
(	O
often	O
followed	O
by	O
prediction	O
in	O
a	O
linear	O
-	O
chain	O
CRF	B-MethodName
)	O
.	O
Though	O
expressive	O
and	O
accurate	O
,	O
these	O
models	O
fail	O
to	O
fully	O
exploit	O
GPU	O
parallelism	O
,	O
limiting	O
their	O
computational	O
efficiency	O
.	O
This	O
paper	O
proposes	O
a	O
faster	O
alternative	O
to	O
Bi	O
-	O
LSTMs	O
for	O
NER	B-TaskName
:	O
Iterated	O
Dilated	O
Convolutional	O
Neural	O
Networks	O
(	O
ID	O
-	O
CNNs	O
)	O
,	O
which	O
have	O
better	O
capacity	O
than	O
traditional	O
CNNs	O
for	O
large	O
context	O
and	O
structured	B-TaskName
prediction	I-TaskName
.	O
Unlike	O
LSTMs	O
whose	O
sequential	O
processing	O
on	O
sentences	O
of	O
length	O
N	O
requires	O
O	O
(	O
N	O
)	O
time	O
even	O
in	O
the	O
face	O
of	O
parallelism	O
,	O
ID	O
-	O
CNNs	O
permit	O
fixed	O
-	O
depth	O
convolutions	O
to	O
run	O
in	O
parallel	O
across	O
entire	O
documents	O
.	O
We	O
describe	O
a	O
distinct	O
combination	O
of	O
network	O
structure	O
,	O
parameter	O
sharing	O
and	O
training	O
procedures	O
that	O
enable	O
dramatic	O
14	O
-	O
20x	O
testtime	O
speedups	O
while	O
retaining	O
accuracy	B-MetricName
comparable	O
to	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
.	O
Moreover	O
,	O
ID	O
-	O
CNNs	O
trained	O
to	O
aggregate	O
context	O
from	O
the	O
entire	O
document	O
are	O
even	O
more	O
accurate	O
while	O
maintaining	O
8x	O
faster	O
test	O
time	O
speeds	O
.	O

In	O
order	O
to	O
democratize	O
large	O
-	O
scale	O
NLP	O
and	O
information	O
extraction	O
while	O
minimizing	O
our	O
environmental	O
footprint	O
,	O
we	O
require	O
fast	O
,	O
resource	O
-	O
efficient	O
methods	O
for	O
sequence	O
tagging	O
tasks	O
such	O
as	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
.	O
Speed	O
is	O
not	O
sufficient	O
of	O
course	O
:	O
they	O
must	O
also	O
be	O
expressive	O
enough	O
to	O
tolerate	O
the	O
tremendous	O
lexical	O
variation	O
in	O
input	O
data	O
.	O
The	O
massively	O
parallel	O
computation	O
facilitated	O
by	O
GPU	O
hardware	O
has	O
led	O
to	O
a	O
surge	O
of	O
successful	O
neural	O
network	O
architectures	O
for	O
sequence	O
labeling	O
(	O
Ling	O
et	O
al	O
,	O
2015	O
;	O
Ma	O
and	O
Hovy	O
,	O
2016	O
;	O
Chiu	O
and	O
Nichols	O
,	O
2016	O
;	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
.	O
While	O
these	O
models	O
are	O
expressive	O
and	O
accurate	O
,	O
they	O
fail	O
to	O
fully	O
exploit	O
the	O
parallelism	O
opportunities	O
of	O
a	O
GPU	O
,	O
and	O
thus	O
their	O
speed	O
is	O
limited	O
.	O
Specifically	O
,	O
they	O
employ	O
either	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
for	O
feature	O
extraction	O
,	O
or	O
Viterbi	O
inference	O
in	O
a	O
structured	O
output	O
model	O
,	O
both	O
of	O
which	O
require	O
sequential	O
computation	O
across	O
the	O
length	O
of	O
the	O
input	O
.	O
Instead	O
,	O
parallelized	O
runtime	O
independent	O
of	O
the	O
length	O
of	O
the	O
sequence	O
saves	O
time	O
and	O
energy	O
costs	O
,	O
maximizing	O
GPU	O
resource	O
usage	O
and	O
minimizing	O
the	O
amount	O
of	O
time	O
it	O
takes	O
to	O
train	O
and	O
evaluate	O
models	O
.	O
Convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
provide	O
exactly	O
this	O
property	O
(	O
Kim	O
,	O
2014	O
;	O
Kalchbrenner	O
et	O
al	O
,	O
2014	O
)	O
.	O
Rather	O
than	O
composing	O
representations	O
incrementally	O
over	O
each	O
token	O
in	O
a	O
sequence	O
,	O
they	O
apply	O
filters	O
in	O
parallel	O
across	O
the	O
entire	O
sequence	O
at	O
once	O
.	O
Their	O
computational	O
cost	O
grows	O
with	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
,	O
but	O
not	O
the	O
input	O
size	O
,	O
up	O
to	O
the	O
memory	O
and	O
threading	O
limitations	O
of	O
the	O
hardware	O
.	O
This	O
provides	O
,	O
for	O
example	O
,	O
audio	B-TaskName
generation	I-TaskName
models	O
that	O
can	O
be	O
trained	O
in	O
parallel	O
.	O
Despite	O
the	O
clear	O
computational	O
advantages	O
of	O
CNNs	O
,	O
RNNs	O
have	O
become	O
the	O
standard	O
method	O
for	O
composing	O
deep	O
representations	O
of	O
text	O
.	O
This	O
is	O
because	O
a	O
token	O
encoded	O
by	O
a	O
bidirectional	O
RNN	O
will	O
incorporate	O
evidence	O
from	O
the	O
entire	O
input	O
sequence	O
,	O
but	O
the	O
CNN	O
's	O
representation	O
is	O
limited	O
by	O
the	O
effective	O
input	O
width	O
1	O
of	O
the	O
network	O
:	O
the	O
size	O
of	O
the	O
input	O
context	O
which	O
is	O
observed	O
,	O
directly	O
or	O
indirectly	O
,	O
by	O
the	O
representation	O
of	O
a	O
token	O
at	O
a	O
given	O
layer	O
in	O
the	O
network	O
.	O
Specifically	O
,	O
in	O
a	O
network	O
composed	O
of	O
a	O
series	O
of	O
stacked	O
convolutional	O
layers	O
of	O
convolution	B-MethodName
width	O
w	O
,	O
the	O
number	O
r	O
of	O
context	O
tokens	O
incorporated	O
into	O
a	O
token	O
's	O
representation	O
at	O
a	O
given	O
layer	O
l	O
,	O
is	O
given	O
by	O
r	O
=	O
l	O
(	O
w	O
−	O
1	O
)	O
+	O
1	O
.	O
The	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
required	O
to	O
incorporate	O
the	O
entire	O
input	O
context	O
grows	O
linearly	O
with	O
the	O
length	O
of	O
the	O
sequence	O
.	O
To	O
avoid	O
this	O
scaling	O
,	O
one	O
could	O
pool	O
representations	O
across	O
the	O
sequence	O
,	O
but	O
this	O
is	O
not	O
appropriate	O
for	O
sequence	O
labeling	O
,	O
since	O
it	O
reduces	O
the	O
output	O
resolution	O
of	O
the	O
representation	O
.	O
In	O
response	O
,	O
this	O
paper	O
presents	O
an	O
application	O
of	O
dilated	O
convolutions	O
(	O
Yu	O
and	O
Koltun	O
,	O
2016	O
)	O
for	O
sequence	O
labeling	O
(	O
Figure	O
1	O
)	O
.	O
For	O
dilated	O
convolutions	O
,	O
the	O
effective	O
input	O
width	O
can	O
grow	O
exponentially	O
with	O
the	O
depth	O
,	O
with	O
no	O
loss	B-MetricName
in	O
resolution	O
at	O
each	O
layer	O
and	O
with	O
a	O
modest	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
to	O
estimate	O
.	O
Like	O
typical	O
CNN	O
layers	O
,	O
dilated	O
convolutions	O
operate	O
on	O
a	O
sliding	O
window	O
of	O
context	O
over	O
the	O
sequence	O
,	O
but	O
unlike	O
conventional	O
convolutions	O
,	O
the	O
context	O
need	O
not	O
be	O
consecutive	O
;	O
the	O
dilated	O
window	O
skips	O
over	O
every	O
dilation	O
width	O
d	O
inputs	O
.	O
By	O
stacking	O
layers	O
of	O
dilated	O
convolutions	O
of	O
exponentially	O
increasing	O
dilation	O
width	O
,	O
we	O
can	O
expand	O
the	O
size	O
of	O
the	O
effective	O
input	O
width	O
to	O
cover	O
the	O
entire	O
length	O
of	O
most	O
sequences	O
using	O
only	O
a	O
few	O
layers	O
:	O
The	O
size	O
of	O
the	O
effective	O
input	O
width	O
for	O
a	O
token	O
at	O
layer	O
l	O
is	O
now	O
given	O
by	O
2	O
l+1	O
−1	O
.	O
More	O
concretely	O
,	O
just	O
four	O
stacked	O
dilated	O
convolutions	O
of	O
width	O
3	O
produces	O
token	O
representations	O
with	O
a	O
n	O
effective	O
input	O
width	O
of	O
31	O
tokens	O
-	O
longer	O
than	O
the	O
average	O
sentence	O
length	O
(	O
23	O
)	O
in	O
the	O
Penn	B-DatasetName
TreeBank	I-DatasetName
.	O
Our	O
overall	O
iterated	O
dilated	O
CNN	O
architecture	O
(	O
ID	O
-	O
CNN	O
)	O
repeatedly	O
applies	O
the	O
same	O
block	O
of	O
dilated	O
convolutions	O
to	O
token	O
-	O
wise	O
representations	O
.	O
This	O
parameter	O
sharing	O
prevents	O
overfitting	O
and	O
also	O
provides	O
opportunities	O
to	O
inject	O
supervision	O
on	O
intermediate	O
activations	O
of	O
the	O
network	O
.	O
Similar	O
to	O
models	O
that	O
use	O
logits	O
produced	O
by	O
an	O
RNN	O
,	O
the	O
ID	O
-	O
CNN	O
provides	O
two	O
methods	O
for	O
performing	O
prediction	O
:	O
we	O
can	O
predict	O
each	O
token	O
's	O
label	O
independently	O
,	O
or	O
by	O
running	O
Viterbi	O
inference	O
in	O
a	O
chain	O
structured	O
graphical	O
model	O
.	O

1	O
What	O
we	O
call	O
effective	O
input	O
width	O
here	O
is	O
known	O
as	O
the	O
receptive	O
field	O
in	O
the	O
vision	O
literature	O
,	O
drawing	O
an	O
analogy	O
to	O
the	O
visual	O
receptive	O
field	O
of	O
a	O
neuron	O
in	O
the	O
retina	O
.	O
5.0	O
English	O
NER	B-TaskName
,	O
we	O
demonstrate	O
significant	O
speed	O
gains	O
of	O
our	O
ID	O
-	O
CNNs	O
over	O
various	O
recurrent	O
models	O
,	O
while	O
maintaining	O
similar	O
F1	B-MetricName
performance	O
.	O
When	O
performing	O
prediction	O
using	O
independent	O
classification	O
,	O
the	O
ID	O
-	O
CNN	O
consistently	O
outperforms	O
a	O
bidirectional	B-MethodName
LSTM	I-MethodName
(	O
Bi	O
-	O
LSTM	B-MethodName
)	O
,	O
and	O
performs	O
on	O
par	O
with	O
inference	O
in	O
a	O
CRF	B-MethodName
with	O
logits	O
from	O
a	O
Bi	O
-	O
LSTM	B-MethodName
(	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
)	O
.	O
As	O
an	O
extractor	O
of	O
per	O
-	O
token	O
logits	O
for	O
a	O
CRF	B-MethodName
,	O
our	O
model	O
out	O
-	O
performs	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
.	O
We	O
also	O
apply	O
ID	O
-	O
CNNs	O
to	O
entire	O
documents	O
,	O
where	O
independent	O
token	B-TaskName
classification	I-TaskName
is	O
as	O
accurate	O
as	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
while	O
decoding	O
almost	O
8×	O
faster	O
.	O
The	O
clear	O
accuracy	B-MetricName
gains	O
resulting	O
from	O
incorporating	O
broader	O
context	O
suggest	O
that	O
these	O
models	O
could	O
similarly	O
benefit	O
many	O
other	O
contextsensitive	O
NLP	O
tasks	O
which	O
have	O
until	O
now	O
been	O
limited	O
by	O
the	O
computational	O
complexity	O
of	O
existing	O
context	O
-	O
rich	O
models	O
.	O
2	O
2	O
Background	O

CNNs	O
in	O
NLP	O
are	O
typically	O
one	O
-	O
dimensional	O
,	O
applied	O
to	O
a	O
sequence	O
of	O
vectors	O
representing	O
tokens	O
rather	O
than	O
to	O
a	O
two	O
-	O
dimensional	O
grid	O
of	O
vectors	O
representing	O
pixels	O
.	O
In	O
this	O
setting	O
,	O
a	O
convolutional	O
neural	O
network	O
layer	O
is	O
equivalent	O
to	O
applying	O
an	O
affine	O
transformation	O
,	O
W	O
c	O
to	O
a	O
sliding	O
window	O
of	O
width	O
r	O
tokens	O
on	O
either	O
side	O
of	O
each	O
token	O
in	O
the	O
sequence	O
.	O
Here	O
,	O
and	O
throughout	O
the	O
paper	O
,	O
we	O
do	O
not	O
explicitly	O
write	O
the	O
bias	O
terms	O
in	O
affine	O
transformations	O
.	O
The	O
convolutional	O
operator	O
applied	O
to	O
each	O
token	O
x	O
t	O
with	O
output	O
c	O
t	O
is	O
defined	O
as	O
:	O
c	O
t	O
=	O
W	O
c	O
r	O
k=0	O
x	O
t±k	O
,	O
(	O
3	O
)	O
where	O
is	O
vector	O
concatenation	O
.	O
Dilated	O
convolutions	O
perform	O
the	O
same	O
operation	O
,	O
except	O
rather	O
than	O
transforming	O
adjacent	O
in	O
-	O
puts	O
,	O
the	O
convolution	B-MethodName
is	O
defined	O
over	O
a	O
wider	O
effective	O
input	O
width	O
by	O
skipping	O
over	O
δ	B-HyperparameterName
inputs	O
at	O
a	O
time	O
,	O
where	O
δ	B-HyperparameterName
is	O
the	O
dilation	O
width	O
.	O
We	O
define	O
the	O
dilated	B-MethodName
convolution	I-MethodName
operator	O
:	O
c	O
t	O
=	O
W	O
c	O
r	O
k=0	O
x	O
t±kδ	O
.	O
(	O
4	O
)	O
A	O
dilated	B-MethodName
convolution	I-MethodName
of	O
width	O
1	O
is	O
equivalent	O
to	O
a	O
simple	O
convolution	B-MethodName
.	O
Using	O
the	O
same	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
as	O
a	O
simple	O
convolution	B-MethodName
with	O
the	O
same	O
radius	O
(	O
i.e.	O
W	O
c	O
has	O
the	O
same	O
dimensionality	O
)	O
,	O
the	O
δ	B-HyperparameterName
>	O
1	O
dilated	B-MethodName
convolution	I-MethodName
incorporates	O
broader	O
context	O
into	O
the	O
representation	O
of	O
a	O
token	O
than	O
a	O
simple	O
convolution	B-MethodName
.	O

We	O
can	O
leverage	O
the	O
ability	O
of	O
dilated	O
convolutions	O
to	O
incorporate	O
global	O
context	O
without	O
losing	O
important	O
local	O
information	O
by	O
stacking	O
dilated	O
convolutions	O
of	O
increasing	O
width	O
.	O
First	O
described	O
for	O
pixel	O
classification	O
in	O
computer	O
vision	O
,	O
Yu	O
and	O
Koltun	O
(	O
2016	O
)	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
image	O
segmentation	O
benchmarks	O
by	O
stacking	O
dilated	O
convolutions	O
with	O
exponentially	O
increasing	O
rates	O
of	O
dilation	O
,	O
a	O
technique	O
they	O
refer	O
to	O
as	O
multiscale	O
context	O
aggregation	O
.	O
By	O
feeding	O
the	O
outputs	O
of	O
each	O
dilated	B-MethodName
convolution	I-MethodName
as	O
the	O
input	O
to	O
the	O
next	O
,	O
increasingly	O
non	O
-	O
local	O
information	O
is	O
incorporated	O
into	O
each	O
pixel	O
's	O
representation	O
.	O
Performing	O
a	O
dilation	O
-	O
1	O
convolution	B-MethodName
in	O
the	O
first	O
layer	O
ensures	O
that	O
no	O
pixels	O
within	O
the	O
effective	O
input	O
width	O
of	O
any	O
pixel	O
are	O
excluded	O
.	O
By	O
doubling	O
the	O
dilation	O
width	O
at	O
each	O
layer	O
,	O
the	O
size	O
of	O
the	O
effective	O
input	O
width	O
grows	O
exponentially	O
while	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
grows	O
only	O
linearly	O
with	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
,	O
so	O
a	O
pixel	O
representation	O
quickly	O
incorporates	O
rich	O
global	O
evidence	O
from	O
the	O
entire	O
image	O
.	O
Unfortunately	O
,	O
simply	O
increasing	O
the	O
depth	O
of	O
stacked	O
dilated	O
CNNs	O
causes	O
considerable	O
overfitting	O
in	O
our	O
experiments	O
.	O
In	O
response	O
,	O
we	O
present	O
Iterated	O
Dilated	O
CNNs	O
(	O
ID	O
-	O
CNNs	O
)	O
,	O
which	O
instead	O
apply	O
the	O
same	O
small	O
stack	O
of	O
dilated	O
convolutions	O
multiple	O
times	O
,	O
each	O
iterate	O
taking	O
as	O
input	O
the	O
result	O
of	O
the	O
last	O
application	O
.	O
Repeatedly	O
employing	O
the	O
same	O
parameters	O
in	O
a	O
recurrent	O
fashion	O
provides	O
both	O
broad	O
effective	O
input	O
width	O
and	O
desirable	O
generalization	O
capabilities	O
.	O
We	O
also	O
obtain	O
significant	O
accuracy	B-MetricName
gains	O
with	O
a	O
training	O
objective	O
that	O
strives	O
for	O
accurate	O
labeling	O
after	O
each	O
iterate	O
,	O
allowing	O
follow	O
-	O
on	O
iterations	O
to	O
observe	O
and	O
resolve	O
dependency	O
violations	O
.	O

The	O
network	O
takes	O
as	O
input	O
a	O
sequence	O
of	O
T	O
vectors	O
x	O
t	O
,	O
and	O
outputs	O
a	O
sequence	O
of	O
per	O
-	O
class	O
scores	O
h	O
t	O
,	O
which	O
serve	O
either	O
as	O
the	O
local	O
conditional	O
distributions	O
of	O
Eqn	O
.	O
(	O
1	O
)	O
or	O
the	O
local	O
factors	O
ψ	O
t	O
of	O
Eqn	O
.	O
(	O
2	O
)	O
.	O
We	O
denote	O
the	O
jth	O
dilated	O
convolutional	O
layer	O
of	O
dilation	O
width	O
δ	B-HyperparameterName
as	O
D	O
(	O
j	O
)	O
δ	B-HyperparameterName
.	O
The	O
first	O
layer	O
in	O
the	O
net	O
-	O
work	O
is	O
a	O
dilation	O
-	O
1	O
convolution	B-MethodName
D	O
(	O
0	B-DatasetName
)	O
1	O
that	O
transforms	O
the	O
input	O
to	O
a	O
representation	O
i	O
t	O
:	O
i	O
t	O
=	O
D	O
(	O
0	B-DatasetName
)	O
1	O
x	O
t	O
(	O
5	O
)	O
Next	O
,	O
L	O
c	O
layers	O
of	O
dilated	O
convolutions	O
of	O
exponentially	O
increasing	O
dilation	O
width	O
are	O
applied	O
to	O
i	O
t	O
,	O
folding	O
in	O
increasingly	O
broader	O
context	O
into	O
the	O
embedded	O
representation	O
of	O
x	O
t	O
at	O
each	O
layer	O
.	O
Let	O
r	O
(	O
)	O
denote	O
the	O
ReLU	B-MethodName
activation	B-HyperparameterName
function	I-HyperparameterName
(	O
Glorot	O
et	O
al	O
,	O
2011	O
)	O
.	O
Beginning	O
with	O
c	O
t	O
(	O
0	B-DatasetName
)	O
=	O
i	O
t	O
we	O
define	O
the	O
stack	O
of	O
layers	O
with	O
the	O
following	O
recurrence	O
:	O
c	O
t	O
(	O
j	O
)	O
=	O
r	O
D	O
(	O
j−1	O
)	O
2	O
Lc−1	O
c	O
t	O
(	O
j−1	O
)	O
(	O
6	O
)	O
and	O
add	O
a	O
final	O
dilation	O
-	O
1	O
layer	O
to	O
the	O
stack	O
:	O
c	O
t	O
(	O
Lc+1	O
)	O
=	O
r	O
D	O
(	O
Lc	O
)	O
1	O
c	O
t	O
(	O
Lc	O
)	O
(	O
7	O
)	O
We	O
refer	O
to	O
this	O
stack	O
of	O
dilated	O
convolutions	O
as	O
a	O
block	O
B	O
(	O
)	O
,	O
which	O
has	O
output	O
resolution	O
equal	O
to	O
its	O
input	O
resolution	O
.	O
To	O
incorporate	O
even	O
broader	O
context	O
without	O
over	O
-	O
fitting	O
,	O
we	O
avoid	O
making	O
B	O
deeper	O
,	O
and	O
instead	O
iteratively	O
apply	O
B	O
L	O
b	O
times	O
,	O
introducing	O
no	O
extra	O
parameters	O
.	O
Starting	O
with	O
b	O
t	O
(	O
1	O
)	O
=	O
B	O
(	O
i	O
t	O
)	O
:	O
b	O
t	O
(	O
k	O
)	O
=	O
B	O
b	O
t	O
(	O
k−1	O
)	O
(	O
8	O
)	O
We	O
apply	O
a	O
simple	O
affine	O
transformation	O
W	O
o	O
to	O
this	O
final	O
representation	O
to	O
obtain	O
per	O
-	O
class	O
scores	O
for	O
each	O
token	O
x	O
t	O
:	O
h	O
t	O
(	O
L	O
b	O
)	O
=	O
W	O
o	O
b	O
t	O
(	O
L	O
b	O
)	O
(	O
9	O
)	O

Our	O
main	O
focus	O
is	O
to	O
apply	O
the	O
ID	O
-	O
CNN	O
an	O
encoder	O
to	O
produce	O
per	O
-	O
token	O
logits	O
for	O
the	O
first	O
conditional	O
model	O
described	O
in	O
Sec	O
.	O
2.1	O
,	O
where	O
tags	O
are	O
conditionally	O
independent	O
given	O
deep	O
features	O
,	O
since	O
this	O
will	O
enable	O
prediction	O
that	O
is	O
parallelizable	O
across	O
the	O
length	O
of	O
the	O
input	O
sequence	O
.	O
Here	O
,	O
maximum	O
likelihood	O
training	O
is	O
straightforward	O
because	O
the	O
likelihood	O
decouples	O
into	O
the	O
sum	O
of	O
the	O
likelihoods	O
of	O
independent	O
logistic	B-MethodName
regression	I-MethodName
problems	O
for	O
every	O
tag	O
,	O
with	O
natural	O
parameters	O
given	O
by	O
Eqn	O
.	O
(	O
9	O
)	O
:	O
1	O
T	O
T	O
t=1	O
log	O
P	O
(	O
y	O
t	O
|	O
h	O
t	O
(	O
L	O
b	O
)	O
)	O
(	O
10	O
)	O
We	O
can	O
also	O
use	O
the	O
ID	O
-	O
CNN	O
as	O
logits	O
for	O
the	O
CRF	B-MethodName
model	O
(	O
Eqn	O
.	O
(	O
2	O
)	O
)	O
,	O
where	O
the	O
partition	O
function	O
and	O
its	O
gradient	O
are	O
computed	O
using	O
the	O
forward	O
-	O
backward	O
algorithm	O
.	O
We	O
next	O
present	O
an	O
alternative	O
training	O
method	O
that	O
helps	O
bridge	O
the	O
gap	O
between	O
these	O
two	O
techniques	O
.	O
Sec	O
.	O
2.1	O
identifies	O
that	O
the	O
CRF	B-MethodName
has	O
preferable	O
sample	O
complexity	O
and	O
accuracy	B-MetricName
since	O
prediction	O
directly	O
reasons	O
in	O
the	O
space	O
of	O
structured	O
outputs	O
.	O
In	O
response	O
,	O
we	O
compile	O
some	O
of	O
this	O
reasoning	O
in	O
output	O
space	O
into	O
ID	O
-	O
CNN	O
feature	O
extraction	O
.	O
Instead	O
of	O
explicit	O
reasoning	O
over	O
output	O
labels	O
during	O
inference	O
,	O
we	O
train	O
the	O
network	O
such	O
that	O
each	O
block	O
is	O
predictive	O
of	O
output	O
labels	O
.	O
Subsequent	O
blocks	O
learn	O
to	O
correct	O
dependency	O
violations	O
of	O
their	O
predecessors	O
,	O
refining	O
the	O
final	O
sequence	O
prediction	O
.	O
To	O
do	O
so	O
,	O
we	O
first	O
define	O
predictions	O
of	O
the	O
model	O
after	O
each	O
of	O
the	O
L	O
b	O
applications	O
of	O
the	O
block	O
.	O
Let	O
h	O
t	O
(	O
k	O
)	O
be	O
the	O
result	O
of	O
applying	O
the	O
matrix	O
W	O
o	O
from	O
(	O
9	O
)	O
to	O
b	O
t	O
(	O
k	O
)	O
,	O
the	O
output	O
of	O
block	O
k.	O
We	O
minimize	O
the	O
average	O
of	O
the	O
losses	O
for	O
each	O
application	O
of	O
the	O
block	O
:	O
1	O
L	O
b	O
L	O
b	O
k=1	O
1	O
T	O
T	O
t=1	O
log	O
P	O
(	O
y	O
t	O
|	O
h	O
t	O
(	O
k	O
)	O
)	O
.	O
(	O
11	O
)	O
By	O
rewarding	O
accurate	O
predictions	O
after	O
each	O
application	O
of	O
the	O
block	O
,	O
we	O
learn	O
a	O
model	O
where	O
later	O
blocks	O
are	O
used	O
to	O
refine	O
initial	O
predictions	O
.	O
The	O
loss	B-MetricName
also	O
helps	O
reduce	O
the	O
vanishing	O
gradient	O
problem	O
(	O
Hochreiter	O
,	O
1998	O
)	O
for	O
deep	O
architectures	O
.	O
Such	O
an	O
approach	O
has	O
been	O
applied	O
in	O
a	O
variety	O
of	O
contexts	O
for	O
training	O
very	O
deep	O
networks	O
in	O
computer	O
vision	O
(	O
Romero	O
et	O
al	O
,	O
2014	O
;	O
Szegedy	O
et	O
al	O
,	O
2015	O
;	O
Lee	O
et	O
al	O
,	O
2015	O
;	O
Gülçehre	O
and	O
Bengio	O
,	O
2016	O
)	O
,	O
but	O
not	O
to	O
our	O
knowledge	O
in	O
NLP	O
.	O
We	O
apply	O
dropout	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
to	O
the	O
raw	O
inputs	O
x	O
t	O
and	O
to	O
each	O
block	O
's	O
output	O
b	O
t	O
b	O
)	O
to	O
help	O
prevent	O
overfitting	O
.	O
The	O
version	O
of	O
dropout	O
typically	O
used	O
in	O
practice	O
has	O
the	O
undesirable	O
property	O
that	O
the	O
randomized	O
predictor	O
used	O
at	O
train	O
time	O
differs	O
from	O
the	O
fixed	O
one	O
used	O
at	O
test	O
time	O
.	O
Ma	O
et	O
al	O
(	O
2017	O
)	O
present	O
dropout	O
with	O
expectationlinear	O
regularization	O
,	O
which	O
explicitly	O
regularizes	O
these	O
two	O
predictors	O
to	O
behave	O
similarly	O
.	O
All	O
of	O
our	O
best	O
reported	O
results	O
include	O
such	O
regularization	O
.	O
This	O
is	O
the	O
first	O
investigation	O
of	O
the	O
technique	O
's	O
effectiveness	O
for	O
NLP	O
,	O
including	O
for	O
RNNs	O
.	O
We	O
encourage	O
its	O
further	O
application	O
.	O

The	O
state	O
-	O
of	O
-	O
the	O
art	O
models	O
for	O
sequence	O
labeling	O
include	O
an	O
inference	O
step	O
that	O
searches	O
the	O
space	O
of	O
possible	O
output	O
sequences	O
of	O
a	O
chain	O
-	O
structured	O
graphical	O
model	O
,	O
or	O
approximates	O
this	O
search	O
with	O
a	O
beam	O
(	O
Collobert	O
et	O
al	O
,	O
2011	O
;	O
Weiss	O
et	O
al	O
,	O
2015	O
;	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Ma	O
and	O
Hovy	O
,	O
2016	O
;	O
Chiu	O
and	O
Nichols	O
,	O
2016	O
)	O
.	O
These	O
outperform	O
similar	O
systems	O
that	O
use	O
the	O
same	O
features	O
,	O
but	O
independent	O
local	O
predictions	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
greedy	O
sequential	O
prediction	O
(	O
Daumé	O
III	O
et	O
al	O
,	O
2009	O
)	O
approach	O
of	O
Ratinov	O
and	O
Roth	O
(	O
2009	O
)	O
,	O
which	O
employs	O
lexicalized	O
features	O
,	O
gazetteers	O
,	O
and	O
word	O
clusters	O
,	O
outperforms	O
CRFs	O
with	O
similar	O
features	O
.	O
LSTMs	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
were	O
used	O
for	O
NER	B-TaskName
as	O
early	O
as	O
the	O
CoNLL	O
shared	O
task	O
in	O
2003	O
(	O
Hammerton	O
,	O
2003	O
;	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
.	O
More	O
recently	O
,	O
a	O
wide	O
variety	O
of	O
neural	O
network	O
architectures	O
for	O
NER	B-TaskName
have	O
been	O
proposed	O
.	O
Collobert	O
et	O
al	O
(	O
2011	O
)	O
employ	O
a	O
one	O
-	O
layer	O
CNN	O
with	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
,	O
capitalization	O
and	O
lexicon	O
features	O
,	O
and	O
CRF	B-MethodName
-	O
based	O
prediction	O
.	O
Huang	O
et	O
al	O
(	O
2015	O
)	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	B-MetricName
on	O
partof	O
-	O
speech	O
,	O
chunking	B-TaskName
and	O
NER	B-TaskName
using	O
a	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
.	O
Lample	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
two	O
models	O
which	O
incorporated	O
Bi	O
-	O
LSTM	B-MethodName
-	O
composed	O
character	O
embeddings	O
alongside	O
words	O
:	O
a	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
,	O
and	O
a	O
greedy	O
stack	O
LSTM	B-MethodName
which	O
uses	O
a	O
simple	O
shift	O
-	O
reduce	O
grammar	O
to	O
compose	O
words	O
into	O
labeled	O
entities	O
.	O
Their	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
obtained	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
four	O
languages	O
without	O
word	O
shape	O
or	O
lexicon	O
features	O
.	O
Ma	O
and	O
Hovy	O
(	O
2016	O
)	O
use	O
CNNs	O
rather	O
than	O
LSTMs	O
to	O
compose	O
characters	O
in	O
a	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
,	O
achieving	O
state	O
-	O
ofthe	O
-	O
art	O
performance	O
on	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
and	O
CoNLL	O
NER	B-TaskName
without	O
lexicons	O
.	O
Chiu	O
and	O
Nichols	O
(	O
2016	O
)	O
evaluate	O
a	O
similar	O
network	O
but	O
propose	O
a	O
novel	O
method	O
for	O
encoding	O
lexicon	O
matches	O
,	O
presenting	O
results	O
on	O
CoNLL	O
and	O
OntoNotes	B-DatasetName
NER	B-TaskName
.	O
Yang	O
et	O
al	O
(	O
2016	O
)	O
use	O
GRU	B-MethodName
-	O
CRFs	O
with	O
GRUcomposed	O
character	O
embeddings	O
of	O
words	O
to	O
train	O
a	O
single	O
network	O
on	O
many	O
tasks	O
and	O
languages	O
.	O
In	O
general	O
,	O
distributed	O
representations	O
for	O
text	O
can	O
provide	O
useful	O
generalization	O
capabilities	O
for	O
NER	B-TaskName
systems	O
,	O
since	O
they	O
can	O
leverage	O
unsupervised	B-TaskName
pre	I-TaskName
-	I-TaskName
training	I-TaskName
of	O
distributed	O
word	O
representations	O
(	O
Turian	O
et	O
al	O
,	O
2010	O
;	O
Collobert	O
et	O
al	O
,	O
2011	O
;	O
Passos	O
et	O
al	O
,	O
2014	O
)	O
.	O
Though	O
our	O
models	O
would	O
also	O
likely	O
benefit	O
from	O
additional	O
features	O
such	O
as	O
character	O
representations	O
and	O
lexicons	O
,	O
we	O
focus	O
on	O
simpler	O
models	O
which	O
use	O
word	O
-	O
embeddings	O
alone	O
,	O
leaving	O
more	O
elaborate	O
input	O
representations	O
to	O
future	O
work	O
.	O
In	O
these	O
NER	B-TaskName
approaches	O
,	O
CNNs	O
were	O
used	O
for	O
low	O
-	O
level	O
feature	O
extraction	O
that	O
feeds	O
into	O
alternative	O
architectures	O
.	O
Overall	O
,	O
end	O
-	O
to	O
-	O
end	O
CNNs	O
have	O
mainly	O
been	O
used	O
in	O
NLP	O
for	O
sentence	B-TaskName
classification	I-TaskName
,	O
where	O
the	O
output	O
representation	O
is	O
lower	O
resolution	O
than	O
that	O
of	O
the	O
input	O
Kim	O
(	O
2014	O
)	O
;	O
Kalchbrenner	O
et	O
al	O
(	O
2014	O
)	O
;	O
;	O
Toutanova	O
et	O
al	O
(	O
2015	O
)	O
.	O
Lei	O
et	O
al	O
(	O
2015	O
)	O
present	O
a	O
CNN	O
variant	O
where	O
convolutions	O
adaptively	O
skip	O
neighboring	O
words	O
.	O
While	O
the	O
flexibility	O
of	O
this	O
model	O
is	O
powerful	O
,	O
its	O
adaptive	O
behavior	O
is	O
not	O
well	O
-	O
suited	O
to	O
GPU	O
acceleration	O
.	O
Our	O
work	O
draws	O
on	O
the	O
use	O
of	O
dilated	O
convolutions	O
for	O
image	O
segmentation	O
in	O
the	O
computer	O
vision	O
community	O
(	O
Yu	O
and	O
Koltun	O
,	O
2016	O
;	O
Chen	O
et	O
al	O
,	O
2015	O
)	O
.	O
Similar	O
to	O
our	O
block	O
,	O
Yu	O
and	O
Koltun	O
(	O
2016	O
)	O
employ	O
a	O
context	O
-	O
module	O
of	O
stacked	O
dilated	O
convolutions	O
of	O
exponentially	O
increasing	O
dilation	O
width	O
.	O
Dilated	O
convolutions	O
were	O
recently	O
applied	O
to	O
the	O
task	O
of	O
speech	O
generation	O
(	O
van	O
den	O
,	O
and	O
concurrent	O
with	O
this	O
work	O
,	O
posted	O
a	O
pre	O
-	O
print	O
describing	O
the	O
similar	O
ByteNet	O
network	O
for	O
machine	B-TaskName
translation	I-TaskName
that	O
uses	O
dilated	O
convolutions	O
in	O
the	O
encoder	O
and	O
decoder	O
components	O
.	O
Our	O
basic	O
model	O
architecture	O
is	O
similar	O
to	O
that	O
of	O
the	O
ByteNet	O
encoder	O
,	O
except	O
that	O
the	O
inputs	O
to	O
our	O
model	O
are	O
tokens	O
and	O
not	O
bytes	O
.	O
Additionally	O
,	O
we	O
present	O
a	O
novel	O
loss	B-MetricName
and	O
parameter	O
sharing	O
scheme	O
to	O
facilitate	O
training	O
models	O
on	O
much	O
smaller	O
datasets	O
than	O
those	O
used	O
by	O
.	O
We	O
are	O
the	O
first	O
to	O
use	O
dilated	O
convolutions	O
for	O
sequence	O
labeling	O
.	O
The	O
broad	O
effective	O
input	O
width	O
of	O
the	O
ID	O
-	O
CNN	O
helps	O
aggregate	O
document	O
-	O
level	O
context	O
.	O
Ratinov	O
and	O
Roth	O
(	O
2009	O
)	O
incorporate	O
document	O
context	O
in	O
their	O
greedy	O
model	O
by	O
adding	O
features	O
based	O
on	O
tagged	O
entities	O
within	O
a	O
large	O
,	O
fixed	O
window	O
of	O
tokens	O
.	O
Prior	O
work	O
has	O
also	O
posed	O
a	O
structured	O
model	O
that	O
couples	O
predictions	O
across	O
the	O
whole	O
document	O
(	O
Bunescu	O
and	O
Mooney	O
,	O
2004	O
;	O
Sutton	O
and	O
McCallum	O
,	O
2004	O
;	O
Finkel	O
et	O
al	O
,	O
2005	O
)	O
.	O

We	O
describe	O
experiments	O
on	O
two	O
benchmark	O
English	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
datasets	O
.	O
On	O
CoNLL	O
-	O
2003	O
English	O
NER	B-TaskName
,	O
our	O
ID	O
-	O
CNN	O
performs	O
on	O
par	O
with	O
a	O
Bi	O
-	O
LSTM	B-MethodName
not	O
only	O
when	O
used	O
to	O
produce	O
per	O
-	O
token	O
logits	O
for	O
structured	O
inference	O
,	O
but	O
the	O
ID	O
-	O
CNN	O
with	O
greedy	O
decoding	O
also	O
performs	O
on	O
-	O
par	O
with	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
while	O
running	O
at	O
more	O
than	O
14	O
times	O
the	O
speed	O
.	O
We	O
also	O
observe	O
a	O
performance	O
boost	O
in	O
almost	O
all	O
models	O
when	O
broadening	O
the	O
context	O
to	O
incorporate	O
entire	O
documents	O
,	O
achieving	O
an	O
average	B-MetricName
F1	I-MetricName
of	O
90.65	O
on	O
CoNLL	O
-	O
2003	O
,	O
out	O
-	O
performing	O
the	O
sentence	O
-	O
level	O
model	O
while	O
still	O
decoding	O
at	O
nearly	O
8	O
times	O
the	O
speed	O
of	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
.	O

We	O
evaluate	O
using	O
labeled	O
data	O
from	O
the	O
CoNLL	O
-	O
2003	O
shared	O
task	O
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
and	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
(	O
Hovy	O
et	O
al	O
,	O
2006	O
;	O
Pradhan	O
et	O
al	O
,	O
2006	O
)	O
.	O
Following	O
previous	O
work	O
,	O
we	O
use	O
the	O
same	O
OntoNotes	B-DatasetName
data	O
split	O
used	O
for	O
co	O
-	O
reference	O
resolution	O
in	O
the	O
CoNLL	O
-	O
2012	O
shared	O
task	O
(	O
Pradhan	O
et	O
al	O
,	O
2012	O
)	O
.	O
For	O
both	O
datasets	O
,	O
we	O
convert	O
the	O
IOB	O
boundary	O
encoding	O
to	O
BILOU	O
as	O
previous	O
work	O
found	O
this	O
encoding	O
to	O
result	O
in	O
improved	O
performance	O
(	O
Ratinov	O
and	O
Roth	O
,	O
2009	O
)	O
.	O
As	O
in	O
previous	O
work	O
we	O
evaluate	O
the	O
performance	O
of	O
our	O
models	O
using	O
segment	O
-	O
level	O
micro	O
-	O
averaged	O
F1	B-MetricName
score	I-MetricName
.	O
Hyperparameters	O
that	O
resulted	O
in	O
the	O
best	O
performance	O
on	O
the	O
validation	O
set	O
were	O
selected	O
via	O
grid	O
search	O
.	O
A	O
more	O
detailed	O
description	O
of	O
the	O
data	O
,	O
evaluation	O
,	O
optimization	O
and	O
data	O
pre	O
-	O
processing	O
can	O
be	O
found	O
in	O
the	O
Appendix	O
.	O

We	O
compare	O
our	O
ID	O
-	O
CNN	O
against	O
strong	O
LSTM	B-MethodName
and	O
CNN	O
baselines	O
:	O
a	O
Bi	O
-	O
LSTM	B-MethodName
with	O
local	O
decoding	O
,	O
and	O
one	O
with	O
CRF	B-MethodName
decoding	O
(	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
)	O
.	O
We	O
also	O
compare	O
against	O
a	O
non	O
-	O
dilated	O
CNN	O
architecture	O
with	O
the	O
same	O
number	O
of	O
convolutional	O
layers	O
as	O
our	O
dilated	O
network	O
(	O
4	O
-	O
layer	O
CNN	O
)	O
and	O
one	O
with	O
enough	O
layers	O
to	O
incorporate	O
an	O
effective	O
input	O
width	O
of	O
the	O
same	O
size	O
as	O
that	O
of	O
the	O
dilated	O
network	O
(	O
5	O
-	O
layer	O
CNN	O
)	O
to	O
demonstrate	O
that	O
the	O
dilated	O
convolutions	O
more	O
effectively	O
aggregate	O
contextual	O
information	O
than	O
simple	O
convolutions	O
(	O
i.e.	O
using	O
fewer	O
parameters	O
)	O
.	O
We	O
also	O
compare	O
our	O
document	O
-	O
level	O
ID	O
-	O
CNNs	O
to	O
a	O
baseline	O
which	O
does	O
not	O
share	O
parameters	O
between	O
blocks	O
(	O
noshare	O
)	O
and	O
one	O
that	O
computes	O
loss	B-MetricName
only	O
at	O
the	O
last	O
block	O
,	O
rather	O
than	O
after	O
every	O
iterated	O
block	O
of	O
dilated	O
convolutions	O
(	O
1	O
-	O
loss	B-MetricName
)	O
.	O
We	O
do	O
not	O
compare	O
with	O
deeper	O
or	O
more	O
elaborate	O
CNN	O
architectures	O
for	O
a	O
number	O
of	O
reasons	O
:	O
1	O
)	O
Fast	O
train	O
and	O
test	O
performance	O
are	O
highly	O
desirable	O
for	O
NLP	O
practitioners	O
,	O
and	O
deeper	O
models	O
require	O
more	O
computation	O
time	O
2	O
)	O
more	O
complicated	O
models	O
tend	O
to	O
over	O
-	O
fit	O
on	O
this	O
relatively	O
small	O
dataset	O
and	O
3	O
)	O
most	O
accurate	O
deep	O
CNN	O
architectures	O
repeatedly	O
up	O
-	O
sample	O
and	O
down	O
-	O
sample	O
the	O
inputs	O
.	O
We	O
do	O
not	O
compare	O
to	O
stacked	O
LSTMs	O
for	O
similar	O
reasons	O
-	O
a	O
single	O
LSTM	B-MethodName
is	O
already	O
slower	O
than	O
a	O
4	O
-	O
layer	O
CNN	O
.	O
Since	O
our	O
task	O
is	O
sequence	O
labeling	O
,	O
we	O
desire	O
a	O
model	O
that	O
maintains	O
the	O
token	O
-	O
level	O
resolution	O
of	O
the	O
input	O
,	O
making	O
dilated	O
convolutions	O
an	O
elegant	O
solution	O
.	O

Table	O
1	O
lists	O
F1	B-MetricName
scores	O
of	O
models	O
predicting	O
with	O
sentence	O
-	O
level	O
context	O
on	O
CoNLL	O
-	O
2003	O
.	O
For	O
models	O
that	O
we	O
trained	O
,	O
we	O
report	O
F1	B-MetricName
and	O
standard	O
deviation	O
obtained	O
by	O
averaging	O
over	O
10	O
random	O
restarts	O
.	O
The	O
Viterbi	O
-	O
decoding	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
and	O
ID	O
-	O
CNN	O
-	O
CRF	B-MethodName
and	O
greedy	O
ID	O
-	O
CNN	O
obtain	O
the	O
highest	O
average	O
scores	O
,	O
with	O
the	O
ID	O
-	O
CNN	O
-	O
CRF	B-MethodName
outperforming	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
by	O
0.11	O
points	O
of	O
F1	B-MetricName
on	O
average	O
,	O
and	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
out	O
-	O
performing	O
the	O
greedy	O
ID	O
-	O
CNN	O
by	O
0.11	O
as	O
well	O
.	O
Our	O
greedy	O
ID	O
-	O
CNN	O
outperforms	O
the	O
Bi	O
-	O
LSTM	B-MethodName
and	O
the	O
4	O
-	O
layer	O
CNN	O
,	O
which	O
uses	O
the	O
same	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
as	O
the	O
ID	O
-	O
CNN	O
,	O
and	O
performs	O
similarly	O
to	O
the	O
5	O
-	O
layer	O
CNN	O
which	O
uses	O
more	O
parameters	O
but	O
covers	O
the	O
same	O
effective	O
input	O
width	O
.	O
All	O
CNN	O
models	O
out	O
-	O
perform	O
the	O
Bi	O
-	O
Model	O
F1	B-MetricName
Ratinov	O
and	O
Roth	O
(	O
2009	O
)	O
86.82	O
Collobert	O
et	O
al	O
(	O
2011	O
)	O
86.96	O
Lample	O
et	O
al	O
(	O
2016	O
)	O
90.33	O
Bi	O
-	O
LSTM	B-MethodName
89.34	O
±	O
0.28	O
4	O
-	O
layer	O
CNN	O
89.97	O
±	O
0.20	O
5	O
-	O
layer	O
CNN	O
90.23	O
±	O
0.16	O
ID	O
-	O
CNN	O
90.32	O
±	O
0.26	O
Collobert	O
et	O
al	O
(	O
2011	O
)	O
88.67	O
Passos	O
et	O
al	O
(	O
2014	O
)	O
90.05	O
Lample	O
et	O
al	O
(	O
2016	O
)	O
90.20	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
(	O
re	O
-	O
impl	O
)	O
90.43	O
±	O
0.12	O
ID	O
-	O
CNN	O
-	O
CRF	B-MethodName
90.54	O
±	O
0.18	O
LSTM	B-MethodName
when	O
paired	O
with	O
greedy	O
decoding	O
,	O
suggesting	O
that	O
CNNs	O
are	O
better	O
token	O
encoders	O
than	O
Bi	O
-	O
LSTMs	O
for	O
independent	O
logistic	B-MethodName
regression	I-MethodName
.	O
When	O
paired	O
with	O
Viterbi	O
decoding	O
,	O
our	O
ID	O
-	O
CNN	O
performs	O
on	O
par	O
with	O
the	O
Bi	O
-	O
LSTM	B-MethodName
,	O
showing	O
that	O
the	O
ID	O
-	O
CNN	O
is	O
also	O
an	O
effective	O
token	O
encoder	O
for	O
structured	O
inference	O
.	O
Our	O
ID	O
-	O
CNN	O
is	O
not	O
only	O
a	O
better	O
token	O
encoder	O
than	O
the	O
Bi	O
-	O
LSTM	B-MethodName
but	O
it	O
is	O
also	O
faster	O
.	O
Table	O
2	O
lists	O
relative	O
decoding	O
times	O
on	O
the	O
CoNLL	O
development	O
set	O
,	O
compared	O
to	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
.	O
We	O
report	O
decoding	O
times	O
using	O
the	O
fastest	O
batch	B-HyperparameterName
size	I-HyperparameterName
for	O
each	O
method	O
.	O
3	O
The	O
ID	O
-	O
CNN	O
model	O
decodes	O
nearly	O
50	O
%	O
faster	O
than	O
the	O
Bi	O
-	O
LSTM	B-MethodName
.	O
With	O
Viterbi	O
decoding	O
,	O
the	O
gap	O
closes	O
somewhat	O
but	O
the	O
ID	O
-	O
CNN	O
-	O
CRF	B-MethodName
still	O
comes	O
out	O
ahead	O
,	O
about	O
30	O
%	O
faster	O
than	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
.	O
The	O
most	O
vast	O
speed	O
improvements	O
come	O
when	O
comparing	O
the	O
greedy	O
ID	O
-	O
CNN	O
to	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
-	O
our	O
ID	O
-	O
CNN	O
is	O
more	O
than	O
14	O
times	O
faster	O
than	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
at	O
test	O
time	O
,	O
with	O
comparable	O
accuracy	B-MetricName
.	O
The	O
5	O
-	O
layer	O
CNN	O
,	O
which	O
observes	O
the	O
same	O
effective	O
input	O
width	O
as	O
the	O
ID	O
-	O
CNN	O
but	O
with	O
more	O
parameters	O
,	O
performs	O
at	O
about	O
the	O
same	O
speed	O
as	O
the	O
ID	O
-	O
CNN	O
in	O
our	O
experiments	O
.	O
With	O
a	O
better	O
implementation	O
of	O
dilated	O
convolutions	O
than	O
currently	O
included	O
in	O
TensorFlow	O
,	O
we	O
would	O
expect	O
the	O
ID	O
-	O
CNN	O
to	O
be	O
notably	O
faster	O
than	O
3	O
:	O
Comparison	O
of	O
models	O
trained	O
with	O
and	O
without	O
expectation	O
-	O
linear	O
dropout	O
regularization	O
(	O
DR	O
)	O
.	O
DR	O
improves	O
all	O
models	O
.	O
the	O
5	O
-	O
layer	O
CNN	O
.	O
We	O
emphasize	O
the	O
importance	O
of	O
the	O
dropout	O
regularizer	O
of	O
Ma	O
et	O
al	O
(	O
2017	O
)	O
in	O
Table	O
3	O
,	O
where	O
we	O
observe	O
increased	O
F1	B-MetricName
for	O
every	O
model	O
trained	O
with	O
expectation	O
-	O
linear	O
dropout	O
regularization	O
.	O
Dropout	B-MethodName
is	O
important	O
for	O
training	O
neural	O
network	O
models	O
that	O
generalize	O
well	O
,	O
especially	O
on	O
relatively	O
small	O
NLP	O
datasets	O
such	O
as	O
CoNLL	O
-	O
2003	O
.	O
We	O
recommend	O
this	O
regularizer	O
as	O
a	O
simple	O
and	O
helpful	O
tool	O
for	O
practitioners	O
training	O
neural	O
networks	O
for	O
NLP	O
.	O

In	O
Table	O
4	O
we	O
show	O
that	O
adding	O
document	O
-	O
level	O
context	O
improves	O
every	O
model	O
on	O
CoNLL	O
-	O
2003	O
.	O
Incorporating	O
document	O
-	O
level	O
context	O
further	O
improves	O
our	O
greedy	O
ID	O
-	O
CNN	O
model	O
,	O
attaining	O
90.65	O
average	B-MetricName
F1	I-MetricName
.	O
We	O
believe	O
this	O
model	O
sees	O
greater	O
improvement	O
with	O
the	O
addition	O
of	O
document	O
-	O
level	O
context	O
than	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
due	O
to	O
the	O
ID	O
-	O
CNN	O
learning	O
a	O
feature	O
function	O
better	O
suited	O
for	O
representing	O
broad	O
context	O
,	O
in	O
contrast	O
with	O
the	O
Bi	O
-	O
LSTM	B-MethodName
which	O
,	O
though	O
better	O
than	O
a	O
simple	O
RNN	O
at	O
encoding	O
long	O
memories	O
of	O
sequences	O
,	O
may	O
reach	O
its	O
limit	O
when	O
provided	O
with	O
sequences	O
more	O
than	O
1	O
,	O
000	O
tokens	O
long	O
such	O
as	O
entire	O
documents	O
.	O
We	O
also	O
note	O
that	O
our	O
combination	O
of	O
training	O
objective	O
(	O
Eqn	O
.	O
11	O
)	O
and	O
tied	O
parameters	O
(	O
Eqn	O
.	O
5	O
compares	O
models	O
trained	O
to	O
incorporate	O
entire	O
document	O
context	O
using	O
the	O
document	O
baselines	O
described	O
in	O
Section	O
6.2	O
.	O
In	O
Table	O
6	O
we	O
show	O
that	O
,	O
in	O
addition	O
to	O
being	O
more	O
accurate	O
,	O
our	O
ID	O
-	O
CNN	O
model	O
is	O
also	O
much	O
faster	O
than	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
when	O
incorporating	O
context	O
from	O
entire	O
documents	O
,	O
decoding	O
at	O
almost	O
8	O
times	O
the	O
speed	O
.	O
On	O
these	O
long	O
sequences	O
,	O
it	O
also	O
tags	O
at	O
more	O
than	O
4.5	O
times	O
the	O
speed	O
of	O
the	O
greedy	O
Bi	O
-	O
LSTM	B-MethodName
,	O
demonstrative	O
of	O
the	O
benefit	O
of	O
our	O
ID	O
-	O
CNNs	O
context	O
-	O
aggregating	O
computation	O
that	O
does	O
not	O
depend	O
on	O
the	O
length	O
of	O
the	O
sequence	O
.	O

We	O
observe	O
similar	O
patterns	O
on	O
OntoNotes	B-DatasetName
as	O
we	O
do	O
on	O
CoNLL	O
.	O
icalized	O
greedy	O
model	O
of	O
Ratinov	O
and	O
Roth	O
(	O
2009	O
)	O
,	O
and	O
our	O
ID	O
-	O
CNN	O
out	O
-	O
performs	O
the	O
Bi	O
-	O
LSTM	B-MethodName
as	O
well	O
as	O
the	O
more	O
complex	O
model	O
of	O
Durrett	O
and	O
Klein	O
(	O
2014	O
)	O
which	O
leverages	O
the	O
parallel	O
coreference	O
annotation	O
available	O
in	O
the	O
OntoNotes	B-DatasetName
corpus	O
to	O
predict	O
named	O
entities	O
jointly	O
with	O
entity	B-TaskName
linking	I-TaskName
and	O
co	O
-	O
reference	O
.	O
Our	O
greedy	O
model	O
is	O
out	O
-	O
performed	O
by	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
reported	O
in	O
Chiu	O
and	O
Nichols	O
(	O
2016	O
)	O
as	O
well	O
as	O
our	O
own	O
re	O
-	O
implementation	O
,	O
which	O
appears	O
to	O
be	O
the	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
this	O
dataset	O
.	O
The	O
gap	O
between	O
our	O
greedy	O
model	O
and	O
those	O
using	O
Viterbi	O
decoding	O
is	O
wider	O
than	O
on	O
CoNLL	O
.	O
We	O
believe	O
this	O
is	O
due	O
to	O
the	O
more	O
diverse	O
set	O
of	O
entities	O
in	O
OntoNotes	B-DatasetName
,	O
which	O
also	O
tend	O
to	O
be	O
much	O
longer	O
-	O
the	O
average	O
length	O
of	O
a	O
multi	O
-	O
token	O
named	O
entity	O
segment	O
in	O
CoNLL	O
is	O
about	O
one	O
token	O
shorter	O
than	O
in	O
OntoNotes	B-DatasetName
.	O
These	O
long	O
entities	O
benefit	O
more	O
from	O
explicit	O
structured	O
constraints	O
enforced	O
in	O
Viterbi	O
decoding	O
.	O
Still	O
,	O
our	O
ID	O
-	O
CNN	O
outperforms	O
all	O
other	O
greedy	O
methods	O
,	O
achieving	O
our	O
goal	O
of	O
learning	O
a	O
better	O
token	O
encoder	O
for	O
structured	B-TaskName
prediction	I-TaskName
.	O
Incorporating	O
greater	O
context	O
significantly	O
boosts	O
the	O
score	O
of	O
our	O
greedy	O
model	O
on	O
OntoNotes	B-DatasetName
,	O
whereas	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
performs	O
more	O
poorly	O
.	O
In	O
Table	O
7	O
,	O
we	O
also	O
list	O
the	O
F1	B-MetricName
of	O
our	O
ID	O
-	O
CNN	O
model	O
and	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
trained	O
on	O
entire	O
document	O
context	O
.	O
For	O
the	O
first	O
time	O
,	O
we	O
see	O
the	O
score	O
decrease	O
when	O
more	O
context	O
is	O
added	O
to	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
,	O
though	O
the	O
ID	O
-	O
CNN	O
,	O
whose	O
sentence	O
model	O
a	O
lower	O
score	O
than	O
that	O
of	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
,	O
sees	O
an	O
increase	O
.	O
We	O
believe	O
the	O
decrease	O
in	O
the	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
occurs	O
because	O
of	O
the	O
nature	O
of	O
the	O
OntoNotes	B-DatasetName
dataset	O
compared	O
to	O
CoNLL	O
-	O
2003	O
:	O
CoNLL	O
-	O
2003	O
contains	O
a	O
particularly	O
high	O
proportion	O
of	O
ambiguous	O
entities	O
,	O
7	O
perhaps	O
leading	O
to	O
more	O
benefit	O
from	O
document	O
context	O
that	O
helps	O
with	O
disambiguation	O
.	O
In	O
this	O
scenario	O
,	O
adding	O
the	O
wider	O
context	O
may	O
just	O
add	O
noise	O
to	O
the	O
high	O
-	O
scoring	O
Bi	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
,	O
whereas	O
the	O
less	O
accurate	O
dilated	O
model	O
can	O
still	O
benefit	O
from	O
the	O
refined	O
predictions	O
of	O
the	O
iterated	O
dilated	O
convolutions	O
.	O

Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
is	O
one	O
of	O
the	O
foundations	O
of	O
many	O
downstream	O
tasks	O
such	O
as	O
relation	B-TaskName
extraction	I-TaskName
,	O
event	B-TaskName
detection	I-TaskName
,	O
and	O
knowledge	O
graph	B-TaskName
construction	I-TaskName
.	O
NER	B-TaskName
models	O
require	O
vast	O
amounts	O
of	O
labeled	O
data	O
to	O
learn	O
and	O
identify	O
patterns	O
that	O
humans	O
can	O
not	O
continuously	O
.	O
It	O
is	O
really	O
about	O
getting	O
accurate	O
data	O
to	O
train	O
the	O
models	O
.	O
When	O
end	O
-	O
to	O
-	O
end	O
neural	O
models	O
achieve	O
excellent	O
performance	O
on	O
NER	B-TaskName
in	O
various	O
domains	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Liu	O
et	O
al	O
,	O
2018	O
;	O
Luan	O
et	O
al	O
,	O
2018	O
;	O
Zeng	O
et	O
al	O
,	O
,	O
2021	O
,	O
building	O
useful	O
and	O
challenging	O
NER	B-TaskName
benchmarks	O
,	O
such	O
as	O
CoNLL03	B-DatasetName
,	O
WNUT16	O
,	O
and	O
SCIERC	B-DatasetName
,	O
contributes	O
significantly	O
to	O
the	O
research	O
community	O
.	O
Data	O
annotation	O
plays	O
a	O
crucial	O
role	O
in	O
building	O
benchmarks	O
and	O
ensuring	O
NLP	O
models	O
are	O
trained	O
with	O
the	O
correct	O
information	O
to	O
learn	O
from	O
(	O
Luan	O
et	O
al	O
,	O
2018	O
;	O
.	O
Producing	O
the	O
necessary	O
annotation	O
from	O
any	O
asset	O
at	O
scale	O
is	O
a	O
challenge	O
,	O
mainly	O
because	O
of	O
the	O
complexity	O
involved	O
with	O
annotation	O
.	O
Getting	O
the	O
most	O
accurate	O
labels	O
demands	O
time	O
and	O
expertise	O
.	O
Label	O
mistakes	O
can	O
hardly	O
be	O
avoided	O
,	O
especially	O
when	O
the	O
labeling	O
process	O
splits	O
the	O
data	O
into	O
multiple	O
sets	O
for	O
distributed	O
annotation	O
.	O
The	O
mistakes	O
cause	O
label	O
inconsistency	O
between	O
subsets	O
of	O
annotated	O
data	O
(	O
e.g.	O
,	O
training	O
set	O
and	O
test	O
set	O
or	O
multiple	O
training	O
subsets	O
)	O
.	O
For	O
example	O
,	O
in	O
the	O
CoNLL03	B-DatasetName
dataset	O
(	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
,	O
a	O
standard	O
NER	B-TaskName
benchmark	O
that	O
has	O
been	O
cited	O
over	O
2	O
,	O
300	O
times	O
,	O
label	O
mistakes	O
were	O
found	O
in	O
5.38	O
%	O
of	O
the	O
test	O
set	O
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
.	O
Note	O
that	O
the	O
stateof	O
-	O
the	O
-	O
art	O
results	O
on	O
CoNLL03	B-DatasetName
have	O
achieved	O
an	O
F1	B-MetricName
score	I-MetricName
of	O
∼	O
.93	O
.	O
So	O
even	O
if	O
the	O
label	O
mistakes	O
make	O
up	O
a	O
tiny	O
part	O
,	O
they	O
can	O
not	O
be	O
negligible	O
when	O
researchers	O
are	O
trying	O
to	O
improve	O
the	O
results	O
further	O
.	O
In	O
the	O
work	O
of	O
Wang	O
et	O
al	O
,	O
five	O
annotators	O
were	O
recruited	O
to	O
correct	O
the	O
label	O
mistakes	O
.	O
Compared	O
to	O
the	O
original	O
test	O
set	O
results	O
,	O
the	O
corrected	O
test	O
set	O
results	O
are	O
more	O
accurate	O
and	O
stable	O
.	O
However	O
,	O
two	O
critical	O
issues	O
were	O
not	O
resolved	O
in	O
this	O
process	O
:	O
i	O
)	O
How	O
to	O
identify	O
label	O
inconsistency	O
between	O
the	O
subsets	O
of	O
annotated	O
data	O
?	O
ii	O
)	O
How	O
to	O
validate	O
that	O
the	O
label	O
consistency	O
was	O
recovered	O
by	O
the	O
correction	O
?	O
Another	O
example	O
is	O
SCIERC	B-DatasetName
(	O
Luan	O
et	O
al	O
,	O
2018	O
)	O
(	O
cited	O
∼50	O
times	O
)	O
which	O
is	O
a	O
multi	O
-	O
task	O
(	O
including	O
NER	B-TaskName
)	O
benchmark	O
in	O
AI	O
domain	O
.	O
It	O
has	O
1	O
,	O
861	O
sentences	O
for	O
training	O
,	O
455	O
for	O
dev	O
,	O
and	O
551	O
for	O
test	O
.	O
When	O
we	O
looked	O
at	O
the	O
false	O
predictions	O
given	O
by	O
SCIIE	O
which	O
was	O
a	O
multi	O
-	O
task	O
model	O
released	O
along	O
with	O
the	O
SCIERC	B-DatasetName
dataset	O
,	O
we	O
found	O
that	O
as	O
many	O
as	O
147	O
(	O
26.7	O
%	O
of	O
the	O
test	O
set	O
)	O
sentences	O
were	O
not	O
properly	O
annotated	O
.	O
(	O
We	O
also	O
recruited	O
five	O
annotators	O
and	O
counted	O
a	O
mistake	O
when	O
all	O
the	O
annotators	O
report	O
it	O
.	O
)	O
Three	O
examples	O
are	O
given	O
in	O
Table	O
1	O
:	O
two	O
of	O
them	O
have	O
wrong	O
entity	O
types	O
;	O
the	O
third	O
has	O
a	O
wrong	O
span	O
boundary	O
.	O
As	O
shown	O
in	O
the	O
experiments	O
section	O
,	O
after	O
the	O
correction	O
,	O
the	O
NER	B-TaskName
performance	O
becomes	O
more	O
accurate	O
and	O
stable	O
.	O
Table	O
1	O
:	O
Three	O
examples	O
to	O
compare	O
original	O
and	O
corrected	O
annotation	O
in	O
the	O
test	O
set	O
of	O
the	O
SCIERC	B-DatasetName
dataset	O
.	O
If	O
the	O
annotation	O
on	O
the	O
test	O
set	O
consistently	O
followed	O
the	O
"	O
codebook	O
"	O
that	O
was	O
used	O
to	O
annotate	O
training	O
data	O
,	O
the	O
entities	O
in	O
the	O
first	O
two	O
examples	O
would	O
be	O
labelled	O
as	O
"	O
Task	O
"	O
(	O
not	O
"	O
Method	O
"	O
)	O
for	O
sure	O
.	O

We	O
design	O
a	O
neural	O
ranking	O
model	O
to	O
score	O
all	O
the	O
candidates	O
that	O
underwent	O
splitting	O
and	O
deletion	O
,	O
V	O
=	O
{	O
v	O
1	O
,	O
v	O
2	O
,	O
.	O
.	O
.	O
,	O
v	O
n	O
}	O
,	O
then	O
feed	O
the	O
top	O
-	O
ranked	O
one	O
to	O
the	O
lexical	O
paraphrasing	O
model	O
for	O
the	O
final	O
output	O
.	O
We	O
train	O
the	O
model	O
on	O
a	O
standard	O
text	B-TaskName
simplification	I-TaskName
corpus	O
consisting	O
of	O
pairs	O
of	O
complex	O
sentence	O
x	O
and	O
manually	O
simplified	O
reference	O
y.	O
Scoring	O
Function	O
.	O
To	O
assess	O
the	O
"	O
goodness	O
"	O
of	O
each	O
candidate	O
v	O
i	O
during	O
training	O
,	O
we	O
define	O
the	O
gold	O
scoring	O
function	O
g	O
*	O
as	O
a	O
length	O
-	O
penalized	O
BERTscore	O
:	O
g	O
*	O
(	O
v	O
i	O
,	O
y	O
)	O
=	O
e	O
−λ	O
|	O
|	O
φv	O
i	O
−φy	O
|	O
|	O
×	O
BERT	B-MethodName
Score	B-MetricName
(	O
v	O
i	O
,	O
y	O
)	O
(	O
1	O
)	O
BERTScore	O
(	O
Zhang	O
et	O
al	O
,	O
2020b	O
)	O
is	O
a	O
text	B-TaskName
similarity	I-TaskName
metric	O
that	O
uses	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
embeddings	O
to	O
find	O
soft	O
matches	O
between	O
word	O
pieces	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
instead	O
of	O
exact	O
string	O
matching	O
.	O
We	O
introduce	O
a	O
length	O
penalty	O
to	O
favor	O
the	O
candidates	O
that	O
are	O
of	O
similar	O
length	O
to	O
the	O
human	O
reference	O
y	O
and	O
penalize	O
those	O
that	O
deviate	O
from	O
the	O
target	O
compression	O
ratio	O
φ	O
y	O
.	O
λ	O
defines	O
the	O
extent	O
of	O
penalization	O
and	O
is	O
set	O
to	O
1	O
in	O
our	O
experiments	O
.	O
φ	O
v	O
i	O
represents	O
the	O
compression	O
ratios	O
of	O
v	O
i	O
compared	O
to	O
the	O
input	O
x.	O
In	O
principle	O
,	O
other	O
similarity	O
metrics	O
can	O
also	O
be	O
used	O
for	O
scoring	O
.	O
Pairwise	O
Ranking	O
Model	O
.	O
We	O
train	O
the	O
ranking	O
model	O
in	O
a	O
pairwise	O
setup	O
since	O
BERTScore	O
is	O
sensitive	O
to	O
the	O
relative	O
rather	O
than	O
absolute	O
similarity	O
,	O
when	O
comparing	O
multiple	O
candidates	O
with	O
the	O
same	O
reference	O
.	O
We	O
transform	O
the	O
gold	O
ranking	O
of	O
V	O
(	O
|	O
V	O
|	O
=	O
n	O
)	O
into	O
n	O
2	O
pairwise	O
comparisons	O
for	O
every	O
candidate	O
pair	O
,	O
and	O
learn	O
to	O
minimize	O
the	O
pairwise	O
ranking	O
violations	O
using	O
hinge	O
loss	B-MetricName
:	O
L	O
M	O
R	O
=	O
1	O
m	O
m	O
k=1	O
1	O
n	O
2	O
k	O
n	O
k	O
i=1	O
n	O
k	O
j=1	O
,	O
i	O
=	O
j	O
max	O
(	O
0	B-DatasetName
,	O
1	O
−	O
l	O
k	O
ij	O
d	O
k	O
ij	O
)	O
d	O
k	O
ij	O
=	O
g	O
(	O
v	O
k	O
i	O
)	O
−	O
g	O
(	O
v	O
k	O
j	O
)	O
l	O
k	O
ij	O
=	O
sign	O
g	O
*	O
(	O
v	O
k	O
i	O
,	O
y	O
k	O
)	O
−	O
g	O
*	O
(	O
v	O
k	O
j	O
,	O
y	O
k	O
)	O
(	O
2	O
)	O
where	O
g	O
(	O
.	O
)	O
is	O
a	O
feedforward	O
neural	O
network	O
,	O
m	O
is	O
the	O
number	O
of	O
training	O
complex	O
-	O
simple	O
sentence	O
pairs	O
,	O
k	O
is	O
the	O
index	O
of	O
training	O
examples	O
,	O
and	O
n	O
k	O
represents	O
the	O
number	O
of	O
generated	O
candidates	O
(	O
2.1	O
)	O
.	O
On	O
average	O
,	O
n	O
k	O
is	O
about	O
14.5	O
for	O
a	O
sentence	O
of	O
30	O
words	O
,	O
and	O
can	O
be	O
larger	O
for	O
longer	O
sentences	O
.	O
We	O
consider	O
10	O
randomly	O
sampled	O
candidates	O
for	O
each	O
complex	O
sentence	O
during	O
training	O
.	O
Features	O
.	O
For	O
the	O
feedforward	B-MethodName
network	I-MethodName
g	O
(	O
.	O
)	O
,	O
we	O
use	O
the	O
following	O
features	O
:	O
number	O
of	O
words	O
in	O
v	O
i	O
and	O
x	O
,	O
compression	O
ratio	O
of	O
v	O
i	O
with	O
respect	O
to	O
x	O
,	O
Jaccard	O
similarity	O
between	O
v	O
i	O
and	O
x	O
,	O
the	O
rules	O
applied	O
on	O
x	O
to	O
obtain	O
v	O
i	O
,	O
and	O
the	O
number	O
of	O
rule	O
applications	O
.	O
We	O
vectorize	O
all	O
the	O
real	O
-	O
valued	O
features	O
using	O
Gaussian	O
binning	O
(	O
Maddela	O
and	O
Xu	O
,	O
2018	O
)	O
,	O
which	O
has	O
shown	O
to	O
help	O
neural	O
models	O
trained	O
on	O
numerical	O
features	O
(	O
Liu	O
et	O
al	O
,	O
2016	O
;	O
Sil	O
et	O
al	O
,	O
2017	O
;	O
.	O
We	O
concatenate	O
these	O
vectors	O
before	O
feeding	O
them	O
to	O
the	O
ranking	O
model	O
.	O
We	O
score	O
each	O
candidate	O
v	O
i	O
separately	O
and	O
rank	O
them	O
in	O
the	O
decreasing	O
order	O
of	O
g	O
(	O
v	O
i	O
)	O
.	O
We	O
provide	O
implementation	O
details	O
in	O
Appendix	O
A.	O

We	O
then	O
paraphrase	O
the	O
top	O
-	O
ranked	O
candidatev	O
V	O
to	O
generate	O
the	O
final	O
simplification	O
outputŷ	O
.	O
Our	O
paraphrase	B-TaskName
generation	I-TaskName
model	O
can	O
explicitly	O
control	O
the	O
extent	O
of	O
lexical	O
paraphrasing	O
by	O
specifying	O
the	O
percentage	O
of	O
words	O
to	O
be	O
copied	O
from	O
the	O
input	O
sentence	O
as	O
a	O
soft	O
constraint	O
.	O
We	O
also	O
introduce	O
a	O
data	B-TaskName
augmentation	I-TaskName
method	O
to	O
encourage	O
our	O
model	O
to	O
generate	O
more	O
diverse	O
outputs	O
.	O
Base	O
Model	O
.	O
Our	O
base	O
generation	O
model	O
is	O
a	O
Transformer	B-MethodName
encoder	O
-	O
decoder	O
initialized	O
by	O
the	O
BERT	B-MethodName
checkpoint	O
(	O
?	O
)	O
,	O
which	O
achieved	O
the	O
best	O
reported	O
performance	O
on	O
text	B-TaskName
simplification	I-TaskName
in	O
the	O
recent	O
work	O
.	O
We	O
enhance	O
this	O
model	O
with	O
an	O
attention	O
-	O
based	O
copy	O
mechanism	O
to	O
encourage	O
lexical	O
paraphrasing	O
,	O
while	O
remaining	O
faithful	O
to	O
the	O
input	O
.	O
Copy	O
Control	O
.	O
Given	O
the	O
input	O
candidatev	O
=	O
(	O
v	O
1	O
,	O
v	O
2	O
,	O
.	O
.	O
.	O
,	O
v	O
l	O
)	O
of	O
l	O
words	O
and	O
the	O
percentage	O
of	O
copying	O
cp	O
(	O
0	B-DatasetName
,	O
1	O
]	O
,	O
our	O
goal	O
is	O
to	O
paraphrase	O
the	O
rest	O
of	O
(	O
1	O
−	O
cp	O
)	O
×	O
l	O
words	O
inv	O
to	O
a	O
simpler	O
version	O
.	O
To	O
achieve	O
this	O
,	O
we	O
convert	O
cp	O
into	O
a	O
vector	O
of	O
the	O
same	O
dimension	O
as	O
BERT	B-MethodName
embeddings	O
using	O
Gaussian	O
binning	O
(	O
Maddela	O
and	O
Xu	O
,	O
2018	O
)	O
and	O
add	O
it	O
to	O
the	O
beginning	O
of	O
the	O
input	O
sequencev	O
.	O
The	O
Transformer	B-MethodName
encoder	O
then	O
produces	O
a	O
sequence	O
of	O
context	O
-	O
aware	O
hidden	O
states	O
H	O
=	O
(	O
h	O
1	O
,	O
h	O
2	O
.	O
.	O
.	O
h	O
l	O
)	O
,	O
where	O
h	O
i	O
corresponds	O
to	O
the	O
hidden	O
state	O
ofv	O
i	O
.	O
Each	O
h	O
i	O
is	O
fed	O
into	O
the	O
copy	O
network	O
which	O
predicts	O
the	O
probability	O
p	O
i	O
that	O
wordv	O
i	O
should	O
be	O
copied	O
to	O
output	O
.	O
We	O
create	O
a	O
new	O
hidden	O
stateh	O
i	O
by	O
adding	O
h	O
i	O
to	O
a	O
vector	O
u	O
scaled	O
according	O
to	O
p	O
i	O
.	O
In	O
other	O
words	O
,	O
the	O
scaled	O
version	O
of	O
u	O
informs	O
the	O
decoder	O
whether	O
the	O
word	O
should	O
be	O
copied	O
.	O
A	O
single	O
vector	O
u	O
is	O
used	O
across	O
all	O
sentences	O
and	O
hidden	O
states	O
,	O
and	O
is	O
randomly	O
initialized	O
then	O
updated	O
during	O
training	O
.	O
More	O
formally	O
,	O
the	O
encoding	O
process	O
can	O
be	O
described	O
as	O
follows	O
:	O
(	O
h	O
1	O
,	O
h	O
2	O
,	O
.	O
.	O
.	O
,	O
h	O
l	O
)	O
=	O
encoder	O
(	O
[	O
cp	O
;	O
v	O
1	O
,	O
v	O
2	O
,	O
.	O
.	O
.	O
,	O
v	O
l	O
]	O
)	O
h	O
i	O
=	O
h	O
i	O
+	O
p	O
i	O
u	O
,	O
H	O
=	O
(	O
h	O
1	O
,	O
h	O
2	O
,	O
.	O
.	O
.	O
,	O
h	O
l	O
)	O
(	O
3	O
)	O
The	O
Transformer	B-MethodName
decoder	I-MethodName
generates	O
the	O
output	O
sequence	O
fromH.	O
Our	O
copy	O
mechanism	O
is	O
incorporated	O
into	O
the	O
encoder	O
rather	O
than	O
copying	O
the	O
input	O
words	O
during	O
the	O
decoding	O
steps	O
(	O
Gu	O
et	O
al	O
,	O
2016	O
;	O
See	O
et	O
al	O
,	O
2017	O
)	O
.	O
Unless	O
otherwise	O
specified	O
,	O
we	O
use	O
the	O
average	O
copy	O
ratio	O
of	O
the	O
training	O
dataset	O
,	O
0.7	O
,	O
for	O
our	O
experiments	O
.	O
Multi	O
-	O
task	O
Training	O
.	O
We	O
train	O
the	O
paraphrasing	O
model	O
and	O
the	O
copy	O
network	O
in	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
setup	O
,	O
where	O
predicting	O
whether	O
a	O
word	O
should	O
be	O
copied	O
serves	O
as	O
an	O
auxiliary	O
task	O
.	O
The	O
gold	O
labels	O
for	O
this	O
task	O
are	O
obtained	O
by	O
checking	O
if	O
each	O
word	O
in	O
the	O
input	O
sentence	O
also	O
appears	O
in	O
the	O
human	O
reference	O
.	O
When	O
a	O
word	O
occurs	O
multiple	O
times	O
in	O
the	O
input	O
,	O
we	O
rely	O
on	O
the	O
monolingual	O
word	B-TaskName
alignment	I-TaskName
results	O
from	O
JacanaAlign	O
(	O
Yao	O
et	O
al	O
,	O
2013	O
)	O
to	O
determine	O
which	O
occurrence	O
is	O
the	O
one	O
that	O
gets	O
copied	O
.	O
We	O
train	O
the	O
Transformer	B-MethodName
model	O
and	O
the	O
copy	O
network	O
jointly	O
by	O
minimizing	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
for	O
both	O
decoder	O
generation	O
and	O
binary	O
word	O
classification	O
.	O
We	O
provide	O
implementation	O
and	O
training	O
details	O
in	O
Appendix	O
A.	O
Data	B-TaskName
Augmentation	I-TaskName
.	O
The	O
sentence	O
pairs	O
in	O
the	O
training	O
corpus	O
often	O
exhibit	O
a	O
variable	O
mix	O
of	O
splitting	O
and	O
deletion	O
operations	O
along	O
with	O
paraphras	O
-	O
ing	O
(	O
see	O
Figure	O
1	O
for	O
an	O
example	O
)	O
,	O
which	O
makes	O
it	O
difficult	O
for	O
the	O
encoder	O
-	O
decoder	O
models	O
to	O
learn	O
paraphrases	O
.	O
Utilizing	O
DisSim	O
,	O
we	O
create	O
additional	O
training	O
data	O
that	O
focuses	O
on	O
lexical	O
paraphrasing	O
For	O
each	O
sentence	O
pair	O
x	O
,	O
y	O
,	O
we	O
first	O
generate	O
a	O
set	O
of	O
candidates	O
V	O
=	O
{	O
v	O
1	O
,	O
v	O
2	O
,	O
.	O
.	O
.	O
,	O
v	O
n	O
}	O
by	O
applying	O
DisSim	O
to	O
x	O
,	O
as	O
described	O
in	O
2.1	O
.	O
Then	O
,	O
we	O
select	O
a	O
a	O
subset	O
of	O
V	O
,	O
called	O
V	O
=	O
{	O
v	O
1	O
,	O
v	O
2	O
,	O
.	O
.	O
.	O
,	O
v	O
n	O
}	O
(	O
V	O
V	O
)	O
that	O
are	O
fairly	O
close	O
to	O
the	O
reference	O
y	O
,	O
but	O
have	O
only	O
undergone	O
splitting	O
and	O
deletion	O
.	O
We	O
score	O
each	O
candidate	O
v	O
i	O
using	O
the	O
length	O
-	O
penalized	O
BERTScore	O
g	O
*	O
(	O
v	O
i	O
,	O
y	O
)	O
in	O
Eq	O
.	O
(	O
1	O
)	O
,	O
and	O
discard	O
those	O
with	O
scores	O
lower	O
than	O
0.5	O
.	O
While	O
calculating	O
g	O
*	O
,	O
we	O
set	O
φ	O
y	O
and	O
λ	O
to	O
1	O
and	O
2	O
respectively	O
to	O
favor	O
candidates	O
of	O
similar	O
length	O
to	O
the	O
reference	O
y.	O
We	O
also	O
discard	O
the	O
candidates	O
that	O
have	O
different	O
number	O
of	O
split	O
sentences	O
with	O
respect	O
to	O
the	O
reference	O
.	O
Finally	O
,	O
we	O
train	O
our	O
model	O
on	O
the	O
filtered	O
candidate	O
-	O
reference	O
sentence	O
pairs	O
v	O
1	O
,	O
y	O
,	O
v	O
2	O
,	O
y	O
,	O
.	O
.	O
.	O
,	O
v	O
n	O
,	O
y	O
,	O
which	O
focus	O
on	O
lexical	O
paraphrasing	O
,	O
in	O
addition	O
to	O
x	O
,	O
y	O
.	O

We	O
train	O
and	O
evaluate	O
our	O
models	O
on	O
Newsela	B-DatasetName
(	O
Xu	O
et	O
al	O
,	O
2015	O
)	O
3	O
and	O
Wikipedia	O
copora	O
(	O
Zhu	O
et	O
al	O
,	O
2010	O
;	O
Woodsend	O
and	O
Lapata	O
,	O
2011	O
;	O
Coster	O
and	O
Kauchak	O
,	O
2011	O
Table	O
2	O
:	O
Automatic	O
evaluation	O
results	O
on	O
NEWSELA	B-DatasetName
-	O
AUTO	O
test	O
set	O
.	O
We	O
report	O
SARI	O
,	O
the	O
main	O
automatic	O
metric	O
for	O
simplification	O
,	O
and	O
its	O
three	O
edit	O
scores	O
namely	O
precision	O
for	O
delete	O
(	O
del	O
)	O
and	O
F1	B-MetricName
scores	O
for	O
add	O
and	O
keep	O
operations	O
.	O
We	O
also	O
report	O
FKGL	O
(	O
FK	O
)	O
,	O
average	O
sentence	O
length	O
(	O
SLen	O
)	O
,	O
output	O
length	O
(	O
OLen	O
)	O
,	O
compression	O
ratio	O
(	O
CR	O
)	O
,	O
self	O
-	O
BLEU	B-MetricName
(	O
s	O
-	O
BL	O
)	O
,	O
percentage	O
of	O
sentence	O
splits	O
(	O
%	O
split	O
)	O
,	O
average	O
percentage	O
of	O
new	O
words	O
added	O
to	O
the	O
output	O
(	O
%	O
new	O
)	O
,	O
and	O
percentage	O
of	O
sentences	O
identical	O
to	O
the	O
input	O
(	O
%	O
eq	O
)	O
.	O
Bold	O
typeface	O
denotes	O
the	O
best	O
performances	O
(	O
i.e.	O
,	O
closest	O
to	O
the	O
reference	O
)	O
.	O
articles	O
with	O
each	O
article	O
rewritten	O
by	O
professional	O
editors	O
for	O
students	O
in	O
different	O
grades	O
.	O
We	O
used	O
the	O
complex	O
-	O
simple	O
sentence	O
pairs	O
automatically	O
aligned	O
by	O
,	O
called	O
the	O
NEWSELA	B-DatasetName
-	O
AUTO	O
dataset	O
.	O
To	O
capture	O
sentence	O
splitting	O
,	O
we	O
joined	O
the	O
adjacent	O
sentences	O
in	O
the	O
simple	O
article	O
that	O
are	O
aligned	O
to	O
the	O
same	O
sentence	O
in	O
the	O
complex	O
article	O
.	O
Following	O
Štajner	O
et	O
al	O
(	O
2015	O
)	O
,	O
we	O
removed	O
the	O
sentence	O
pairs	O
with	O
high	O
(	O
>	O
0.9	O
)	O
and	O
low	O
(	O
<	O
0.1	O
)	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
scores	O
,	O
which	O
mostly	O
correspond	O
to	O
the	O
near	O
identical	O
and	O
semantically	O
divergent	O
sentence	O
pairs	O
respectively	O
.	O
The	O
final	O
dataset	O
consists	O
of	O
259	O
,	O
778	O
train	O
,	O
32	O
,	O
689	O
validation	O
and	O
33	O
,	O
391	O
test	O
complex	O
-	O
simple	O
sentence	O
pairs	O
,	O
where	O
∼30	O
%	O
of	O
pairs	O
involve	O
sentence	O
splitting	O
.	O
Besides	O
Newsela	B-DatasetName
,	O
we	O
also	O
provide	O
the	O
details	O
of	O
experiments	O
on	O
Wikipedia	O
corpus	O
in	O
Appendix	O
F	O
,	O
which	O
show	O
similar	O
trends	O
.	O
To	O
demonstrate	O
that	O
our	O
model	O
can	O
be	O
controlled	O
to	O
generate	O
diverse	O
simplifications	O
,	O
we	O
evaluate	O
under	O
the	O
following	O
settings	O
:	O
(	O
i	O
)	O
Standard	O
evaluation	O
on	O
the	O
NEWSELA	B-DatasetName
-	O
AUTO	O
test	O
set	O
similar	O
to	O
the	O
methodology	O
in	O
the	O
recent	O
literature	O
Dong	O
et	O
al	O
,	O
2019	O
;	O
Zhang	O
and	O
Lapata	O
,	O
2017	O
)	O
,	O
and	O
(	O
ii	O
)	O
Evaluation	O
on	O
different	O
subsets	O
of	O
the	O
NEWSELA	B-DatasetName
-	O
AUTO	O
test	O
set	O
that	O
concentrate	O
on	O
a	O
specific	O
operation	O
.	O
We	O
selected	O
9	O
,	O
356	O
sentence	O
pairs	O
with	O
sentence	O
splits	O
for	O
split	O
-	O
focused	O
evaluation	O
.	O
Similarly	O
,	O
we	O
chose	O
9	O
,	O
511	O
sentence	O
pairs	O
with	O
compression	O
ratio	O
<	O
0.7	O
and	O
without	O
sentences	O
splits	O
to	O
evaluate	O
delete	O
-	O
focused	O
simplification	O
.	O
We	O
created	O
a	O
new	O
dataset	O
,	O
called	O
NEWSELA	B-DatasetName
-	O
TURK	O
,	O
to	O
evaluate	O
lexical	O
paraphrasing	O
.	O
4	O
Similar	O
to	O
the	O
WIKIPEDIA	O
-	O
TURK	O
benchmark	O
corpus	O
(	O
Xu	O
et	O
al	O
,	O
2016	O
)	O
,	O
NEWSELA	B-DatasetName
-	O
TURK	O
consists	O
of	O
human	O
-	O
written	O
references	O
focused	O
on	O
lexical	O
para	O
-	O
phrasing	O
.	O
We	O
first	O
selected	O
sentence	O
pairs	O
from	O
the	O
NEWSELA	B-DatasetName
-	O
AUTO	O
test	O
set	O
of	O
roughly	O
similar	O
length	O
(	O
compression	O
ratio	O
between	O
0.8	O
and	O
1.2	O
)	O
and	O
no	O
sentence	O
splits	O
because	O
they	O
more	O
likely	O
involve	O
paraphrasing	O
.	O
Then	O
,	O
we	O
asked	O
Amazon	O
Mechanical	O
Turk	O
workers	O
to	O
simplify	O
the	O
complex	O
sentence	O
without	O
any	O
loss	B-MetricName
in	O
meaning	O
.	O
5	O
To	O
ensure	O
the	O
quality	O
of	O
simplifications	O
,	O
we	O
manually	O
selected	O
the	O
workers	O
using	O
the	O
qualification	O
test	O
proposed	O
in	O
Alva	O
-	O
Manchego	O
et	O
al	O
(	O
2020	O
)	O
,	O
during	O
which	O
the	O
workers	O
were	O
asked	O
to	O
simplify	O
three	O
sentences	O
.	O
We	O
selected	O
top	O
35	O
%	O
of	O
the	O
300	O
workers	O
that	O
participated	O
in	O
the	O
test	O
.	O
We	O
periodically	O
checked	O
the	O
submissions	O
and	O
removed	O
the	O
bad	O
workers	O
.	O
In	O
the	O
end	O
,	O
we	O
collected	O
500	O
sentences	O
with	O
4	O
references	O
for	O
each	O
sentence	O
.	O

Metrics	O
.	O
We	O
report	O
SARI	O
(	O
Xu	O
et	O
al	O
,	O
2016	O
)	O
,	O
which	O
averages	O
the	O
F1	B-MetricName
/	O
precision	O
of	O
n	O
-	O
grams	O
(	O
n	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
}	O
)	O
inserted	O
,	O
deleted	O
and	O
kept	O
when	O
compared	O
to	O
human	O
references	O
.	O
More	O
specifically	O
,	O
it	O
computes	O
the	O
F1	B-MetricName
score	I-MetricName
for	O
the	O
n	O
-	O
grams	O
that	O
are	O
added	O
(	O
add	O
)	O
,	O
8	O
which	O
is	O
an	O
important	O
indicator	O
if	O
a	O
model	O
is	O
good	O
at	O
paraphrasing	O
.	O
The	O
model	O
's	O
deletion	O
capability	O
is	O
measured	O
by	O
the	O
F1	B-MetricName
score	I-MetricName
for	O
n	O
-	O
grams	O
that	O
are	O
kept	O
(	O
keep	O
)	O
and	O
precision	O
for	O
those	O
deleted	O
(	O
del	O
)	O
.	O
9	O
To	O
evaluate	O
a	O
model	O
's	O
para	O
-	O
8	O
We	O
slightly	O
improved	O
the	O
SARI	O
implementation	O
by	O
Xu	O
et	O
al	O
(	O
2016	O
)	O
to	O
exclude	O
the	O
spurious	O
ngrams	O
while	O
calculating	O
the	O
F1	B-MetricName
score	I-MetricName
for	O
add	O
.	O
For	O
example	O
,	O
if	O
the	O
input	O
contains	O
the	O
phrase	O
"	O
is	O
very	O
beautiful	O
"	O
,	O
the	O
phrase	O
"	O
is	O
beautiful	O
"	O
is	O
treated	O
as	O
a	O
new	O
phrase	O
in	O
the	O
original	O
implementation	O
even	O
though	O
it	O
is	O
caused	O
by	O
the	O
delete	O
operation	O
.	O
9	O
SARI	O
score	O
of	O
a	O
reference	O
with	O
itself	O
may	O
not	O
always	O
be	O
100	O
as	O
it	O
considers	O
0	B-DatasetName
divided	O
by	O
0	B-DatasetName
as	O
0	B-DatasetName
,	O
instead	O
of	O
1	O
,	O
when	O
calculating	O
n	O
-	O
gram	O
precision	O
and	O
recall	O
.	O
This	O
avoids	O
the	O
inflation	O
of	O
del	O
scores	O
when	O
the	O
input	O
is	O
same	O
as	O
the	O
output	O
.	O
phrasing	O
capability	O
and	O
diversity	O
,	O
we	O
calculate	O
the	O
BLEU	B-MetricName
score	I-MetricName
with	O
respect	O
to	O
the	O
input	O
(	O
s	O
-	O
BL	O
)	O
,	O
the	O
percentage	O
of	O
new	O
words	O
(	O
%	O
new	O
)	O
added	O
,	O
and	O
the	O
percentage	O
of	O
system	O
outputs	O
identical	O
to	O
the	O
input	O
(	O
%	O
eq	O
)	O
.	O
Low	O
s	O
-	O
BL	O
,	O
%	O
eq	O
,	O
or	O
high	O
%	O
new	O
indicate	O
that	O
the	O
system	O
is	O
less	O
conservative	O
.	O
We	O
also	O
report	O
Flesch	O
-	O
Kincaid	O
(	O
FK	O
)	O
grade	O
level	O
readability	O
(	O
Kincaid	O
and	O
Chissom	O
,	O
1975	O
)	O
,	O
average	O
sentence	O
length	O
(	O
SLen	O
)	O
,	O
the	O
percentage	O
of	O
splits	O
(	O
%	O
split	O
)	O
,	O
compression	O
ratio	O
(	O
CR	O
)	O
,	O
and	O
average	O
output	O
length	O
(	O
OLen	O
)	O
.	O
We	O
do	O
not	O
report	O
BLEU	B-MetricName
because	O
it	O
often	O
does	O
not	O
correlate	O
with	O
simplicity	O
(	O
Sulem	O
et	O
al	O
,	O
2018a	O
,	O
b	O
;	O
Xu	O
et	O
al	O
,	O
2016	O
)	O
.	O
Table	O
6	O
:	O
Human	O
evaluation	O
of	O
100	O
random	O
simplifications	O
from	O
the	O
NEWSELA	B-DatasetName
-	O
AUTO	O
test	O
set	O
and	O
the	O
split	O
-	O
focused	O
subset	O
of	O
the	O
same	O
test	O
set	O
.	O
Has	O
Split	O
and	O
Correct	O
Split	O
denote	O
the	O
percentage	O
of	O
the	O
output	O
sentences	O
that	O
have	O
undergone	O
splitting	O
and	O
the	O
percentage	O
of	O
coherent	O
splits	O
respectively	O
.	O
*	O
denotes	O
that	O
our	O
model	O
is	O
significantly	O
better	O
than	O
the	O
corresponding	O
baseline	O
(	O
according	O
to	O
a	O
t	O
-	O
test	O
with	O
p	O
<	O
0.05	O
)	O
.	O

deletion	O
as	O
they	O
show	O
high	O
self	O
-	O
BLEU	B-MetricName
(	O
>	O
66.5	O
)	O
and	O
FK	O
(	O
>	O
8.8	O
)	O
scores	O
despite	O
having	O
compression	O
ratios	O
similar	O
to	O
other	O
systems	O
.	O
Transformer	B-MethodName
model	O
alone	O
is	O
rather	O
conservative	O
and	O
copies	O
10.2	O
%	O
of	O
the	O
sentences	O
directly	O
to	O
the	O
output	O
.	O
Although	O
Hybrid	O
-	O
NG	O
makes	O
more	O
changes	O
than	O
any	O
other	O
baselines	O
,	O
its	O
SARI	O
and	O
add	O
scores	O
are	O
3.7	O
and	O
1.7	O
points	O
lower	O
than	O
our	O
model	O
indicating	O
that	O
it	O
generates	O
more	O
errors	O
.	O
Our	O
model	O
achieves	O
the	O
lowest	O
self	O
-	O
BLEU	B-MetricName
(	O
48.7	O
)	O
,	O
FK	O
(	O
7.9	O
)	O
,	O
and	O
percentage	O
of	O
sentences	O
identical	O
to	O
the	O
input	O
(	O
0.4	O
)	O
,	O
and	O
the	O
highest	O
add	O
(	O
3.3	O
)	O
score	O
and	O
percentage	O
of	O
new	O
words	O
(	O
16.2	O
%	O
)	O
.	O
In	O
other	O
words	O
,	O
our	O
system	O
is	O
the	O
least	O
conservative	O
,	O
generates	O
more	O
good	O
paraphrases	O
,	O
and	O
mimics	O
the	O
human	O
references	O
better	O
.	O
We	O
provide	O
examples	O
of	O
system	O
outputs	O
in	O
Table	O
9	O
and	O
Appendix	O
C.	O
Tables	O
3	O
,	O
4	O
,	O
and	O
5	O
show	O
the	O
results	O
on	O
NEWSELA	B-DatasetName
-	O
TURK	O
,	O
split	O
-	O
focused	O
,	O
and	O
delete	O
-	O
focused	O
subsets	O
of	O
NEWSELA	B-DatasetName
-	O
AUTO	O
test	O
set	O
respectively	O
.	O
For	O
these	O
experiments	O
,	O
we	O
configure	O
our	O
model	O
to	O
focus	O
on	O
specific	O
operations	O
(	O
details	O
in	O
2.4	O
)	O
.	O
Our	O
model	O
again	O
outperforms	O
the	O
existing	O
systems	O
according	O
to	O
SARI	O
,	O
add	O
score	O
,	O
and	O
percentage	O
of	O
new	O
words	O
,	O
which	O
means	O
that	O
our	O
model	O
is	O
performing	O
more	O
meaningful	O
paraphrasing	O
.	O
We	O
show	O
that	O
we	O
can	O
control	O
the	O
extent	O
of	O
paraphrasing	O
by	O
varying	O
the	O
copy	O
ratio	O
(	O
cp	O
)	O
.	O
Our	O
model	O
splits	O
93.5	O
%	O
of	O
the	O
sentences	O
,	O
which	O
is	O
substantially	O
better	O
than	O
the	O
other	O
models	O
.	O

China	O
's	O
air	O
pollution	O
is	O
very	O
unhealthy	O
.	O
Hybrid	O
-	O
NG	O
experts	O
say	O
the	O
government	O
's	O
air	O
pollution	O
exacts	O
a	O
toll	O
on	O
human	O
health	O
.	O
LSTM	B-MethodName
experts	O
say	O
china	O
's	O
air	O
pollution	O
exacts	O
a	O
tremendous	O
toll	O
on	O
human	O
health	O
.	O
Transformer	B-MethodName
bert	O
experts	O
say	O
china	O
's	O
pollution	O
has	O
a	O
tremendous	O
effect	O
on	O
human	O
health	O
.	O
EditNTS	O
experts	O
say	O
china	O
's	O
air	O
pollution	O
can	O
cause	O
human	O
health	O
.	O
Our	O
Model	O
(	O
cp	O
=	O
0.6	O
)	O
experts	O
say	O
china	O
's	O
air	O
pollution	O
is	O
a	O
big	O
problem	O
for	O
human	O
health	O
.	O
Our	O
Model	O
(	O
cp	O
=	O
0.7	O
)	O
experts	O
say	O
china	O
's	O
air	O
pollution	O
can	O
cause	O
a	O
lot	O
of	O
damage	O
on	O
human	O
health	O
.	O
Our	O
Model	O
(	O
cp	O
=	O
0.8	O
)	O
experts	O
say	O
china	O
's	O
air	O
pollution	O
is	O
a	O
huge	O
toll	O
on	O
human	O
health	O
.	O
Table	O
9	O
:	O
Examples	O
of	O
system	O
outputs	O
.	O
Red	O
marks	O
the	O
errors	O
;	O
blue	O
marks	O
good	O
paraphrases	O
.	O
cp	O
is	O
a	O
soft	O
constraint	O
that	O
denotes	O
the	O
percentage	O
of	O
words	O
that	O
can	O
be	O
copied	O
from	O
the	O
input	O
.	O
Kriz	O
et	O
al	O
,	O
2019	O
;	O
Dong	O
et	O
al	O
,	O
2019	O
;	O
at	O
the	O
cost	O
of	O
controllability	O
and	O
performance	O
as	O
shown	O
in	O
this	O
paper	O
.	O
Controllable	O
text	B-TaskName
simplification	I-TaskName
has	O
been	O
attempted	O
before	O
,	O
but	O
only	O
with	O
limited	O
capability	O
.	O
Scarton	O
and	O
Specia	O
(	O
2018	O
)	O
and	O
added	O
additional	O
tokens	O
to	O
the	O
input	O
representing	O
grade	O
level	O
,	O
length	O
,	O
lexical	O
,	O
and	O
structural	O
complexity	O
constraints	O
.	O
Nishihara	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
a	O
loss	B-MetricName
which	O
controls	O
word	O
complexity	O
,	O
while	O
Mallinson	O
and	O
Lapata	O
(	O
2019	O
)	O
concatenated	O
constraints	O
to	O
each	O
word	O
embedding	O
.	O
Kumar	B-DatasetName
et	O
al	O
(	O
2020	O
)	O
proposed	O
a	O
linguistic	O
scoring	O
function	O
to	O
control	O
the	O
edits	O
to	O
the	O
input	O
.	O
Another	O
long	O
body	O
of	O
research	O
focuses	O
on	O
a	O
single	O
simplification	O
operation	O
and	O
can	O
be	O
broadly	O
divided	O
into	O
three	O
categories	O
:	O
(	O
1	O
)	O
Lexical	B-TaskName
Simplification	I-TaskName
(	O
Specia	O
et	O
al	O
,	O
2012	O
;	O
Horn	O
et	O
al	O
,	O
2014	O
;	O
Glavaš	O
and	O
Štajner	O
,	O
2015	O
;	O
Paetzold	O
andSpecia	O
,	O
2017	O
,	O
2015	O
;	O
Maddela	O
and	O
Xu	O
,	O
2018	O
;	O
Qiang	O
et	O
al	O
,	O
2020	O
)	O
,	O
where	O
complex	O
words	O
are	O
substituted	O
with	O
simpler	O
words	O
.	O
(	O
2	O
)	O
Syntactic	O
Simplification	O
(	O
Siddharthan	O
,	O
2006	O
;	O
Aharoni	O
and	O
Goldberg	O
,	O
2018	O
;	O
Botha	O
et	O
al	O
,	O
2018	O
;	O
Niklaus	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
deals	O
exclusively	O
with	O
sentence	O
splitting	O
,	O
and	O
(	O
3	O
)	O
Sentence	B-DatasetName
Compression	I-DatasetName
(	O
Filippova	O
et	O
al	O
,	O
2015	O
;	O
Rush	O
et	O
al	O
,	O
2015	O
;	O
Nallapati	O
et	O
al	O
,	O
2016	O
;	O
See	O
et	O
al	O
,	O
2017	O
;	O
Baziotis	O
et	O
al	O
,	O
2019	O
)	O
,	O
where	O
the	O
goal	O
is	O
to	O
shorten	O
the	O
input	O
sentence	O
by	O
removing	O
its	O
irrelevant	O
content	O
.	O

We	O
implemented	O
two	O
separate	O
Transformer	B-MethodName
models	O
for	O
neural	O
deletion	O
and	O
split	O
component	O
(	O
2.1	O
)	O
and	O
paraphrase	B-TaskName
generation	I-TaskName
(	O
2.3	O
)	O
using	O
the	O
Fairseq	O
12	O
toolkit	O
.	O
Both	O
the	O
encoder	O
and	O
decoder	O
follow	O
BERT	B-MethodName
base	O
13	O
architecture	O
,	O
while	O
the	O
encoder	O
is	O
also	O
initialized	O
with	O
BERT	B-MethodName
base	O
checkpoint	O
.	O
For	O
neural	O
deletion	O
and	O
split	O
component	O
,	O
we	O
used	O
a	O
beam	O
search	O
of	O
width	O
10	O
to	O
generate	O
candidates	O
.	O
The	O
copy	O
attention	O
mechanism	O
is	O
a	O
feedforward	B-MethodName
network	I-MethodName
containing	O
3	O
hidden	O
layers	O
,	O
1000	O
nodes	O
in	O
each	O
layer	O
with	O
tanh	B-MethodName
activation	I-MethodName
,	O
and	O
a	O
single	O
linear	O
output	O
node	O
with	O
sigmoid	B-MethodName
activation	I-MethodName
.	O
We	O
used	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.0001	O
,	O
linear	O
learning	B-HyperparameterName
rate	I-HyperparameterName
warmup	O
of	O
40k	O
steps	O
,	O
and	O
100k	O
training	O
steps	O
.	O
We	O
used	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	O
.	O
We	O
used	O
BERT	B-MethodName
WordPiece	B-MethodName
tokenizer	O
.	O
During	O
inference	O
,	O
we	O
constrained	O
the	O
beam	O
-	O
search	O
to	O
not	O
repeat	O
trigrams	O
and	O
emitted	O
sentences	O
that	O
avoided	O
aggressive	O
deletion	O
(	O
compression	O
ratio	O
[	O
0.9	O
,	O
1.2	O
]	O
.	O
We	O
chose	O
the	O
best	O
checkpoint	O
based	O
on	O
the	O
SARI	O
score	O
(	O
Xu	O
et	O
al	O
,	O
2016	O
)	O
on	O
the	O
dev	O
set	O
.	O
We	O
saved	O
a	O
checkpoint	O
after	O
every	O
epoch	O
.	O
We	O
did	O
not	O
perform	O
any	O
hyperparameter	O
search	O
and	O
directly	O
used	O
the	O
hyperparameters	O
of	O
the	O
BERT	B-MethodName
-	O
initialized	O
Transformer	B-MethodName
described	O
in	O
?	O
.	O
The	O
model	O
takes	O
10	O
hours	O
to	O
train	O
on	O
1	O
NVIDIA	O
GeForce	O
GPU	O
.	O
Our	O
pairwise	O
ranking	O
model	O
,	O
implemented	O
using	O
the	O
PyTorch	O
framework	O
,	O
consists	O
of	O
3	O
hidden	O
layers	O
,	O
100	O
nodes	O
in	O
each	O
layer	O
,	O
tanh	B-MethodName
activation	I-MethodName
,	O
and	O
a	O
single	O
linear	O
output	O
node	O
.	O
We	O
used	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.01	O
and	O
10	O
epochs	O
.	O
We	O
applied	O
a	O
dropout	O
of	O
0.2	O
.	O
For	O
Gaussian	O
binning	O
,	O
we	O
vectorized	O
the	O
numerical	O
features	O
into	O
10	O
dimensional	O
vectors	O
.	O
The	O
model	O
takes	O
half	O
hour	O
to	O
train	O
on	O
1	O
NVIDIA	O
GeForce	O
GPU	O
.	O
We	O
do	O
not	O
perform	O
any	O
extensive	O
hyperparameter	O
tuning	O
.	O
We	O
just	O
examined	O
few	O
values	O
for	O
learning	B-HyperparameterName
rate	I-HyperparameterName
(	O
0.001	O
,	O
0.01	O
and	O
0.1	O
)	O
and	O
chose	O
the	O
best	O
based	O
on	O
the	O
SARI	O
score	O
on	O
the	O
dev	O
set	O
.	O
We	O
used	O
the	O
original	O
code	O
for	O
DisSim	O
.	O
14	O

the	O
room	O
echoed	O
with	O
the	O
sounds	O
of	O
song	O
,	O
the	O
beat	O
of	O
drums	O
,	O
the	O
voices	O
of	O
young	O
men	O
who	O
are	O
hungry	O
and	O
legs	O
.	O
Our	O
Model	O
(	O
cp	O
=	O
0.6	O
)	O
the	O
sound	O
of	O
the	O
room	O
was	O
full	O
of	O
sounds	O
of	O
young	O
men	O
and	O
the	O
voices	O
of	O
cellos	O
.	O
Our	O
Model	O
(	O
cp	O
=	O
0.7	O
)	O
the	O
sound	O
of	O
the	O
room	O
sounded	O
like	O
a	O
lot	O
of	O
music	O
,	O
and	O
the	O
voices	O
of	O
young	O
men	O
.	O
Our	O
Model	O
(	O
cp	O
=	O
0.8	O
)	O
the	O
sound	O
of	O
the	O
room	O
sounded	O
like	O
a	O
song	O
,	O
the	O
beat	O
of	O
drums	O
,	O
and	O
the	O
voices	O
of	O
young	O
men	O
.	O
Table	O
11	O
:	O
Automatic	O
evaluation	O
results	O
on	O
a	O
subset	O
of	O
Newsela	B-DatasetName
test	O
set	O
that	O
focuses	O
on	O
paraphrasing	O
(	O
8371	O
complexsimple	O
sentence	O
with	O
compression	O
ratio	O
>	O
0.9	O
and	O
no	O
splits	O
)	O
.	O
We	O
control	O
the	O
extent	O
of	O
paraphrasing	O
of	O
our	O
models	O
by	O
specifying	O
the	O
percentage	O
of	O
words	O
to	O
be	O
copied	O
(	O
cp	O
)	O
from	O
the	O
input	O
as	O
a	O
soft	O
constraint	O
.	O
36.1	O
2.5	O
67.4	O
38.5	O
11.7	O
20.9	O
22.4	O
1.02	O
6.4	O
63.5	O
13.5	O
0.0	O
Our	O
Model	O
35.9	O
4.7	O
63.6	O
39.6	O
9.2	O
14.7	O
19.8	O
0.9	O
33.7	O
63.2	O
12.9	O
9.2	O
Our	O
Model	O
(	O
no	O
split	O
;	O
cp	O
=	O
0.6	O
)	O
36.5	O
4.9	O
63.2	O
41.4	O
10.8	O
18.6	O
19.9	O
0.89	O
6.7	O
61.9	O
12.4	O
3.9	O
Our	O
Model	O
(	O
no	O
split	O
;	O
cp	O
=	O
0.7	O
)	O
37	O
.	O
5	O
4.3	O
68.8	O
39.4	O
11.2	O
19.1	O
20.9	O
0.94	O
8.9	O
72.6	O
8.6	O
12.3	O
Our	O
Model	O
(	O
no	O
split	O
;	O
cp	O
=	O
0.8	O
)	O
37	O
.	O
0	B-DatasetName
3.8	O
72.0	O
35.3	O
11.7	O
19.8	O
21.7	O
0.97	O
8.4	O
80.4	O
6.6	O
24.5	O
Table	O
12	O
:	O
Automatic	O
evaluation	O
results	O
on	O
TURK	O
dataset	O
(	O
Xu	O
et	O
al	O
,	O
2015	O
)	O
that	O
focuses	O
on	O
lexical	O
paraphrasing	O
.	O
(	O
Alva	O
-	O
Manchego	O
et	O
al	O
,	O
2020	O
)	O
dataset	O
that	O
contains	O
all	O
the	O
three	O
simplification	O
operations	O
.	O
We	O
use	O
the	O
complex	O
-	O
simple	O
sentence	O
pairs	O
from	O
WIKI	O
-	O
AUTO	O
,	O
which	O
contains	O
138	O
,	O
095	O
article	O
pairs	O
and	O
604k	O
non	O
-	O
identical	O
aligned	O
and	O
partially	O
-	O
aligned	O
sentence	O
pairs	O
.	O
To	O
capture	O
sentence	O
splitting	O
,	O
we	O
join	O
the	O
sentences	O
in	O
the	O
simple	O
article	O
mapped	O
to	O
the	O
same	O
sentence	O
in	O
the	O
complex	O
article	O
.	O
Similar	O
to	O
Newsela	B-DatasetName
,	O
we	O
remove	O
the	O
sentence	O
pairs	O
with	O
high	O
(	O
>	O
0.9	O
)	O
and	O
low	O
(	O
<	O
0.1	O
)	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
scores	O
.	O
For	O
validation	O
and	O
testing	O
purposes	O
,	O
we	O
use	O
the	O
following	O
two	O
corpora	O
:	O
(	O
i	O
)	O
TURK	O
corpus	O
(	O
Xu	O
et	O
al	O
,	O
2015	O
)	O
for	O
lexical	O
paraphrasing	O
and	O
(	O
ii	O
)	O
ASSET	B-DatasetName
corpus	I-DatasetName
(	O
Alva	O
-	O
Manchego	O
et	O
al	O
,	O
2020	O
)	O
for	O
multiple	O
rewrite	O
operations	O
.	O
While	O
the	O
former	O
corpus	O
has	O
8	O
humanwritten	O
references	O
for	O
2000	O
validation	O
and	O
359	O
test	O
sentences	O
,	O
the	O
latter	O
corpus	O
provides	O
10	O
references	O
for	O
the	O
same	O
sentences	O
.	O
We	O
remove	O
the	O
validation	O
and	O
test	O
sentences	O
from	O
the	O
training	O
corpus	O
.	O
Tables	O
12	O
and	O
13	O
show	O
the	O
results	O
on	O
TURK	O
and	O
ASSET	B-DatasetName
respectively	O
.	O

We	O
use	O
various	O
neural	O
network	O
learned	O
embeddings	O
as	O
similarity	O
feature	O
in	O
our	O
system	O
.	O
The	O
system	O
uses	O
Siamese	B-MethodName
network	I-MethodName
to	O
learn	O
these	O
similarity	O
measures	O
.	O
Siamese	O
nets	O
were	O
first	O
introduced	O
in	O
the	O
early	O
1990s	O
by	O
(	O
Bromley	O
et	O
al	O
,	O
1993	O
)	O
to	O
solve	O
signature	O
verification	O
as	O
an	O
image	B-TaskName
matching	I-TaskName
problem	O
.	O
A	O
siamese	O
neural	O
network	O
consists	O
of	O
twin	O
networks	O
which	O
accept	O
distinct	O
inputs	O
but	O
are	O
joined	O
by	O
an	O
energy	O
function	O
at	O
the	O
top	O
.	O
This	O
function	O
computes	O
some	O
metric	O
between	O
the	O
highest	O
level	O
feature	O
representation	O
on	O
each	O
side	O
.	O
The	O
weights	O
between	O
both	O
the	O
networks	O
are	O
shared	O
generally	O
,	O
so	O
that	O
they	O
project	O
the	O
similar	O
texts	O
not	O
far	O
in	O
the	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
.	O
We	O
use	O
contrastive	O
loss	B-MetricName
described	O
in	O
(	O
Chopra	O
et	O
al	O
,	O
2005	O
)	O
as	O
the	O
loss	B-MetricName
function	O
to	O
the	O
Siamese	B-MethodName
network	I-MethodName
.	O
Glove	O
pretrained	O
vectors	O
(	O
300	O
dimension	O
)	O
are	O
fed	O
as	O
input	O
to	O
the	O
neural	O
network	O
.	O
The	O
final	O
neural	O
embeddings	O
are	O
generated	O
by	O
various	O
architectures	O
.	O
Figure	O
1	O
shows	O
a	O
siamese	B-MethodName
network	I-MethodName
,	O
where	O
X	O
1	O
represents	O
the	O
original	O
question	O
text	O
and	O
X	O
2	O
represents	O
the	O
candidate	O
question	O
text	O
.	O
G	O
W	O
represents	O
a	O
complex	O
nonlinear	O
function	O
which	O
is	O
represented	O
by	O
neural	O
network	O
having	O
weights	O
W	O
.	O
The	O
euclidean	O
distance	O
of	O
the	O
vectors	O
is	O
used	O
to	O
compute	O
the	O
contrastive	O
loss	B-MetricName
.	O
The	O
goal	O
is	O
to	O
minimize	O
the	O
distance	O
in	O
the	O
embedding	O
space	O
of	O
the	O
similar	O
question	O
text	O
and	O
maximize	O
for	O
non	O
similar	O
pairs	O
.	O
The	O
contrastive	O
loss	B-MetricName
can	O
be	O
given	O
by	O
following	O
equation	O
:	O
L	O
=	O
Y	O
|	O
|	O
G	O
W	O
(	O
X	O
1	O
)	O
,	O
G	O
W	O
(	O
X	O
2	O
)	O
|	O
|	O
2	O
+	O
(	O
1	O
−	O
Y	O
)	O
max	O
(	O
0	B-DatasetName
,	O
m	O
−	O
|	O
|	O
G	O
W	O
(	O
X	O
1	O
)	O
,	O
G	O
W	O
(	O
X	O
2	O
)	O
|	O
|	O
2	O
)	O
where	O
Y	O
is	O
annotated	O
tag	O
,	O
1	O
if	O
X	O
1	O
and	O
X	O
2	O
are	O
similar	O
,	O
0	B-DatasetName
otherwise	O
.	O
m	O
is	O
margin	O
parameter	O
for	O
hinge	O
loss	B-MetricName
,	O
which	O
is	O
kept	O
1	O
for	O
all	O
our	O
networks	O
.	O
We	O
use	O
following	O
networks	O
to	O
generate	O
text	O
embedding	O
:	O
Long	O
Short	O
Term	O
Memory	O
LSTM	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
are	O
popular	O
variant	O
of	O
the	O
the	O
recurrent	O
neural	O
network	O
architecture	O
that	O
captures	O
the	O
long	O
term	O
dependency	O
in	O
text	O
and	O
deals	O
with	O
vanishing	O
gradient	O
problem	O
.	O
Recently	O
LSTMs	O
have	O
been	O
very	O
successful	O
in	O
various	O
NLP	O
tasks	O
.	O

Figure	O
2	O
shows	O
a	O
bidirectional	O
recurrent	O
neural	O
network	O
architecture	O
.	O
Bi	O
-	O
directional	O
RNN	O
processes	O
the	O
text	O
in	O
both	O
directions	O
with	O
separate	O
hidden	O
units	O
and	O
these	O
hidden	O
representations	O
are	O
concatenated	O
together	O
to	O
create	O
final	O
hidden	O
embedding	O
.	O
For	O
bi	O
-	O
directional	O
LSTM	B-MethodName
,	O
the	O
hidden	O
unit	O
is	O
a	O
LSTM	B-MethodName
cell	O
combining	O
of	O
various	O
gates	O
.	O
We	O
use	O
a	O
bidirectional	B-MethodName
LSTM	I-MethodName
to	O
generate	O
a	O
256	O
dimensional	O
vector	O
for	O
pair	O
of	O
text	O
and	O
train	O
the	O
model	O
by	O
back	O
propagation	O
using	O
contrastive	O
loss	B-MetricName
.	O
Gated	B-MethodName
Recurrent	I-MethodName
Unit	I-MethodName
Gated	B-MethodName
recurrent	I-MethodName
unit	I-MethodName
(	O
GRU	B-MethodName
)	O
(	O
Chung	O
et	O
al	O
,	O
2014	O
)	O
is	O
another	O
variant	O
of	O
RNN	O
which	O
were	O
introduced	O
recently	O
as	O
compared	O
to	O
LSTM	B-MethodName
.	O
They	O
also	O
have	O
seen	O
similar	O
success	O
as	O
LSTM	B-MethodName
in	O
various	O
NLP	O
tasks	O
.	O
We	O
use	O
Bi	O
-	O
GRU	B-MethodName
as	O
another	O
network	O
to	O
generate	O
the	O
neural	O
embeddings	O
trained	O
by	O
siamese	B-MethodName
network	I-MethodName
similar	O
to	O
Bi	O
-	O
LSTM	B-MethodName
.	O
The	O
final	O
hidden	O
embedding	O
size	O
is	O
256	O
dimension	O
for	O
our	O
Bi	O
-	O
GRU	B-MethodName
network	O
also	O
.	O
Convolution	B-MethodName
Net	O
We	O
also	O
use	O
convolution	B-MethodName
networks	O
as	O
another	O
neural	O
network	O
architecture	O
to	O
generate	O
embeddings	O
inside	O
the	O
siamese	B-MethodName
network	I-MethodName
.	O
We	O
use	O
1D	O
-	O
convolution	B-MethodName
with	O
128	O
kernels	O
,	O
stride	O
of	O
5	O
followed	O
by	O
1D	O
-	O
max	O
pool	O
with	O
pool	O
-	O
size	O
of	O
5	O
and	O
finally	O
a	O
dense	O
layer	O
to	O
create	O
a	O
128	O
dimension	O
vector	O
.	O
Implementation	O
Details	O
We	O
use	O
Keras	O
1	O
library	O
with	O
Theano	O
(	O
Theano	O
Development	O
Team	O
,	O
2016	O
)	O
backend	O
to	O
train	O
above	O
3	O
models	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
64	O
and	O
dropout	O
rate	O
is	O
0.25	O
.	O
We	O
run	O
25	O
epochs	O
for	O
each	O
of	O
these	O
3	O
networks	O
training	O
.	O
It	O
takes	O
couple	O
of	O
hours	O
to	O
train	O
on	O
CPU	O
.	O
Instead	O
of	O
using	O
the	O
entire	O
vectors	O
into	O
our	O
final	O
classifier	O
,	O
we	O
compute	O
cosine	O
similarity	O
of	O
learned	O
vectors	O
of	O
both	O
the	O
question	O
text	O
(	O
for	O
each	O
of	O
the	O
3	O
networks	O
)	O
and	O
use	O
that	O
as	O
a	O
feature	O
in	O
our	O
system	O
.	O

Apart	O
from	O
the	O
neural	O
network	O
learned	O
semantic	O
features	O
,	O
we	O
also	O
employ	O
semantic	B-TaskName
similarity	I-TaskName
between	O
question	O
text	O
generated	O
by	O
semantic	O
net	O
.	O
We	O
use	O
the	O
sentence	O
similarity	O
described	O
in	O
(	O
Li	O
et	O
al	O
,	O
2006	O
)	O
using	O
WordNet	O
as	O
semantic	O
net	O
.	O
The	O
paper	O
describes	O
various	O
heuristics	O
used	O
to	O
generate	O
this	O
sentence	O
similarity	O
.	O
First	O
,	O
word	O
pair	O
similarity	O
is	O
generated	O
as	O
a	O
function	O
of	O
the	O
shortest	O
path	O
between	O
the	O
words	O
and	O
height	O
of	O
their	O
lowest	O
common	O
subsumer	O
(	O
LCS	O
)	O
.	O
This	O
combines	O
the	O
word	O
1	O
https://github.com/fchollet/keras	O
similarity	O
with	O
their	O
specificity	O
(	O
abstract	O
vs	O
specific	O
concept	O
)	O
.	O
Then	O
the	O
sentence	O
similarity	O
is	O
obtained	O
as	O
a	O
linear	O
combination	O
of	O
semantic	B-TaskName
similarity	I-TaskName
and	O
the	O
word	O
order	O
similarity	O
.	O
To	O
generate	O
semantic	B-TaskName
similarity	I-TaskName
,	O
cosine	O
between	O
semantic	O
vectors	O
is	O
obtained	O
.	O
The	O
semantic	O
vectors	O
are	O
generated	O
by	O
creating	O
sentence	O
vector	O
of	O
word	O
presence	O
and	O
their	O
similarity	O
.	O
Word	O
order	O
similarity	O
is	O
computed	O
in	O
the	O
similar	O
way	O
as	O
semantic	B-TaskName
similarity	I-TaskName
but	O
the	O
position	O
of	O
word	O
in	O
the	O
sentence	O
is	O
used	O
to	O
generate	O
the	O
word	O
order	O
vector	O
.	O
Finally	O
a	O
linear	O
combination	O
of	O
these	O
two	O
similarity	O
features	O
is	O
used	O
as	O
the	O
similarity	O
measure	O
between	O
question	O
texts	O
.	O
We	O
use	O
the	O
same	O
hyper	O
-	O
parameters	O
as	O
original	O
paper	O
that	O
give	O
the	O
best	O
results	O
i.e.	O
α	B-HyperparameterName
=	O
0.2	O
,	O
β	B-HyperparameterName
=	O
0.45	O
,	O
η	O
=	O
0.4	O
,	O
φ	O
=	O
0.2	O
,	O
δ	B-HyperparameterName
=	O
0.85	O
.	O
The	O
feature	O
encodes	O
semantic	B-TaskName
similarity	I-TaskName
and	O
gives	O
boost	O
to	O
system	O
,	O
shown	O
in	O
the	O
results	O
table	O
.	O

There	O
has	O
been	O
a	O
lot	O
of	O
research	O
in	O
machine	B-TaskName
translation	I-TaskName
and	O
summarization	B-TaskName
community	O
to	O
find	O
metrics	O
that	O
correlate	O
with	O
human	O
judgement	O
on	O
these	O
tasks	O
.	O
We	O
compute	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
metrics	O
for	O
1	O
,	O
2	O
,	O
3	O
and	O
4	O
grams	O
and	O
compute	O
a	O
weighted	O
addition	O
(	O
weights	O
=	O
0.1	O
,	O
0.1	O
,	O
0.3	O
,	O
0.5	O
)	O
.	O
We	O
also	O
compute	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
(	O
Lin	O
,	O
2004	O
)	O
,	O
which	O
is	O
recall	O
oriented	O
similarity	O
measure	O
based	O
on	O
longest	O
common	O
subsequence	O
(	O
LCS	O
)	O
,	O
as	O
a	O
feature	O
in	O
our	O
system	O
.	O

The	O
results	O
generated	O
by	O
the	O
system	O
on	O
test	O
data	O
were	O
submitted	O
as	O
an	O
entry	O
to	O
SemEval	O
-	O
2017	O
task	O
3	O
subtask	O
B.	O
Our	O
primary	O
entry	O
achieved	O
second	O
place	O
on	O
the	O
MAP	B-DatasetName
which	O
was	O
official	O
metric	O
for	O
ranking	O
.	O
Also	O
it	O
achieved	O
highest	O
MRR	B-MetricName
amongst	O
all	O
the	O
primary	O
submissions	O
.	O
Table	O
1	O
shows	O
the	O
dev	O
and	O
test	O
set	O
accuracy	B-MetricName
for	O
our	O
system	O
with	O
each	O
feature	O
applied	O
incrementally	O
.	O
Our	O
both	O
contrastive	O
submissions	O
trained	O
on	O
SVM	B-MethodName
achieved	O
better	O
test	O
accuracy	B-MetricName
than	O
training	O
on	O
Logistic	B-MethodName
Regression	I-MethodName
.	O
Thus	O
the	O
Ranking	O
-	O
SVM	B-MethodName
is	O
able	O
to	O
generalize	O
better	O
.	O
We	O
also	O
experimented	O
with	O
pointwise	O
learning	O
to	O
rank	O
method	O
and	O
got	O
inferior	O
results	O
thus	O
corroborating	O
the	O
fact	O
that	O
pairwise	O
methods	O
are	O
helping	O
our	O
system	O
in	O
achieving	O
better	O
accuracy	B-MetricName
.	O

Pre	O
-	O
trained	O
feature	O
extractors	O
,	O
such	O
as	O
BERT	B-MethodName
for	O
natural	O
language	O
processing	O
and	O
VGG	B-MethodName
for	O
computer	O
vision	O
,	O
have	O
become	O
effective	O
methods	O
for	O
improving	O
deep	O
learning	O
models	O
without	O
requiring	O
more	O
labeled	O
data	O
.	O
While	O
effective	O
,	O
these	O
feature	O
extractors	O
may	O
be	O
prohibitively	O
large	O
for	O
some	O
deployment	O
scenarios	O
.	O
We	O
explore	O
weight	O
pruning	O
for	O
BERT	B-MethodName
and	O
ask	O
:	O
how	O
does	O
compression	O
during	O
pretraining	O
affect	O
transfer	B-TaskName
learning	I-TaskName
?	O
We	O
find	O
that	O
pruning	O
affects	O
transfer	B-TaskName
learning	I-TaskName
in	O
three	O
broad	O
regimes	O
.	O
Low	O
levels	O
of	O
pruning	O
(	O
30	O
-	O
40	O
%	O
)	O
do	O
not	O
affect	O
pre	O
-	O
training	O
loss	B-MetricName
or	O
transfer	O
to	O
downstream	O
tasks	O
at	O
all	O
.	O
Medium	O
levels	O
of	O
pruning	O
increase	O
the	O
pre	O
-	O
training	O
loss	B-MetricName
and	O
prevent	O
useful	O
pre	O
-	O
training	O
information	O
from	O
being	O
transferred	O
to	O
downstream	O
tasks	O
.	O
High	O
levels	O
of	O
pruning	O
additionally	O
prevent	O
models	O
from	O
fitting	O
downstream	O
datasets	O
,	O
leading	O
to	O
further	O
degradation	O
.	O
Finally	O
,	O
we	O
observe	O
that	O
finetuning	O
BERT	B-MethodName
on	O
a	O
specific	O
task	O
does	O
not	O
improve	O
its	O
prunability	O
.	O
We	O
conclude	O
that	O
BERT	B-MethodName
can	O
be	O
pruned	O
once	O
during	O
pre	O
-	O
training	O
rather	O
than	O
separately	O
for	O
each	O
task	O
without	O
affecting	O
performance	O
.	O

Pre	O
-	O
trained	O
feature	O
extractors	O
,	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
for	O
natural	O
language	O
processing	O
and	O
VGG	B-MethodName
(	O
Simonyan	O
and	O
Zisserman	O
,	O
2014	O
)	O
for	O
computer	O
vision	O
,	O
have	O
become	O
effective	O
methods	O
for	O
improving	O
the	O
performance	O
of	O
deep	O
learning	O
models	O
.	O
In	O
the	O
last	O
year	O
,	O
models	O
similar	O
to	O
BERT	B-MethodName
have	O
become	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
many	O
NLP	O
tasks	O
,	O
including	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
NLI	O
)	O
,	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
,	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
etc	O
.	O
These	O
models	O
follow	O
a	O
pre	O
-	O
training	O
paradigm	O
:	O
they	O
are	O
trained	O
on	O
a	O
large	O
amount	O
of	O
unlabeled	O
text	O
via	O
a	O
task	O
that	O
resembles	O
language	O
modeling	O
(	O
Yang	O
et	O
al	O
,	O
2019	O
;	O
Chan	O
et	O
al	O
,	O
2019	O
)	O
and	O
are	O
then	O
fine	O
-	O
tuned	O
on	O
a	O
smaller	O
amount	O
of	O
"	O
downstream	O
"	O
data	O
,	O
which	O
is	O
labeled	O
for	O
a	O
specific	O
task	O
.	O
Pre	O
-	O
trained	O
models	O
usually	O
achieve	O
higher	O
accuracy	B-MetricName
than	O
any	O
model	O
trained	O
on	O
downstream	O
data	O
alone	O
.	O
The	O
pre	O
-	O
training	O
paradigm	O
,	O
while	O
effective	O
,	O
still	O
has	O
some	O
problems	O
.	O
While	O
some	O
claim	O
that	O
language	O
model	O
pre	O
-	O
training	O
is	O
a	O
"	O
universal	O
language	O
learning	O
task	O
"	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
,	O
there	O
is	O
no	O
theoretical	O
justification	O
for	O
this	O
,	O
only	O
empirical	O
evidence	O
.	O
Second	O
,	O
due	O
to	O
the	O
size	O
of	O
the	O
pre	O
-	O
training	O
dataset	O
,	O
BERT	B-MethodName
models	O
tend	O
to	O
be	O
slow	O
and	O
require	O
impractically	O
large	O
amounts	O
of	O
GPU	O
memory	O
.	O
BERT	B-MethodName
-	O
Large	O
can	O
only	O
be	O
used	O
with	O
access	O
to	O
a	O
Google	B-DatasetName
TPU	O
,	O
and	O
BERT	B-MethodName
-	O
Base	O
requires	O
some	O
optimization	O
tricks	O
such	O
as	O
gradient	B-MethodName
checkpointing	I-MethodName
or	O
gradient	O
accumulation	O
to	O
be	O
trained	O
effectively	O
on	O
consumer	O
hardware	O
(	O
Sohoni	O
et	O
al	O
,	O
2019	O
)	O
.	O
Training	O
BERT	B-MethodName
-	O
Base	O
from	O
scratch	O
costs	O
∼$7k	O
and	O
emits	O
∼1438	O
pounds	O
of	O
CO	O
2	O
(	O
Strubell	O
et	O
al	O
,	O
2019	O
)	O
.	O
Model	B-TaskName
compression	I-TaskName
(	O
Bucila	O
et	O
al	O
,	O
2006	O
)	O
,	O
which	O
attempts	O
to	O
shrink	O
a	O
model	O
without	O
losing	O
accuracy	B-MetricName
,	O
is	O
a	O
viable	O
approach	O
to	O
decreasing	O
GPU	O
usage	O
.	O
It	O
might	O
also	O
be	O
used	O
to	O
trade	O
accuracy	B-MetricName
for	O
memory	O
in	O
some	O
low	O
-	O
resource	O
cases	O
,	O
such	O
as	O
deploying	O
to	O
smartphones	O
for	O
real	O
-	O
time	O
prediction	O
.	O
The	O
main	O
questions	O
this	O
paper	O
attempts	O
to	O
answer	O
are	O
:	O
Does	O
compressing	O
BERT	B-MethodName
impede	O
it	O
's	O
ability	O
to	O
transfer	O
to	O
new	O
tasks	O
?	O
And	O
does	O
fine	O
-	O
tuning	O
make	O
BERT	B-MethodName
more	O
or	O
less	O
compressible	O
?	O
To	O
explore	O
these	O
questions	O
,	O
we	O
compressed	O
English	O
BERT	B-MethodName
using	O
magnitude	O
weight	O
pruning	O
(	O
Han	O
et	O
al	O
,	O
2015	O
)	O
and	O
observed	O
the	O
results	O
on	O
transfer	B-TaskName
learning	I-TaskName
to	O
the	O
General	B-DatasetName
Language	O
Understanding	O
Evaluation	O
(	O
GLUE	B-DatasetName
)	O
benchmark	O
,	O
a	O
diverse	O
set	O
of	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
tasks	O
including	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
NLI	O
,	O
and	O
textual	O
similarity	O
evaluation	O
.	O
We	O
chose	O
magnitude	O
weight	O
pruning	O
,	O
which	O
compresses	O
models	O
by	O
removing	O
weights	O
close	O
to	O
0	B-DatasetName
,	O
because	O
it	O
is	O
one	O
of	O
the	O
most	O
fine	O
-	O
grained	O
and	O
effective	O
compression	O
methods	O
and	O
because	O
there	O
are	O
many	O
interesting	O
ways	O
to	O
view	O
pruning	O
,	O
which	O
we	O
explore	O
in	O
the	O
next	O
section	O
.	O
Our	O
findings	O
are	O
as	O
follows	O
:	O
Low	O
levels	O
of	O
pruning	O
(	O
30	O
-	O
40	O
%	O
)	O
do	O
not	O
increase	O
pre	O
-	O
training	O
loss	B-MetricName
or	O
affect	O
transfer	O
to	O
downstream	O
tasks	O
at	O
all	O
.	O
Medium	O
levels	O
of	O
pruning	O
increase	O
the	O
pre	O
-	O
training	O
loss	B-MetricName
and	O
prevent	O
useful	O
pre	O
-	O
training	O
information	O
from	O
being	O
transferred	O
to	O
downstream	O
tasks	O
.	O
This	O
information	O
is	O
not	O
equally	O
useful	O
to	O
each	O
task	O
;	O
tasks	O
degrade	O
linearly	O
with	O
pre	O
-	O
train	O
loss	B-MetricName
,	O
but	O
at	O
different	O
rates	O
.	O
High	O
levels	O
of	O
pruning	O
,	O
depending	O
on	O
the	O
size	O
of	O
the	O
downstream	O
dataset	O
,	O
may	O
additionally	O
degrade	O
performance	O
by	O
preventing	O
models	O
from	O
fitting	O
downstream	O
datasets	O
.	O
Finally	O
,	O
we	O
observe	O
that	O
fine	O
-	O
tuning	O
BERT	B-MethodName
on	O
a	O
specific	O
task	O
does	O
not	O
improve	O
its	O
prunability	O
or	O
change	O
the	O
order	O
of	O
pruning	O
by	O
a	O
meaningful	O
amount	O
.	O
To	O
our	O
knowledge	O
,	O
prior	O
work	O
had	O
not	O
shown	O
whether	O
BERT	B-MethodName
could	O
be	O
compressed	O
in	O
a	O
taskgeneric	O
way	O
,	O
keeping	O
the	O
benefits	O
of	O
pre	O
-	O
training	O
while	O
avoiding	O
costly	O
experimentation	O
associated	O
with	O
compressing	O
and	O
re	O
-	O
training	O
BERT	B-MethodName
multiple	O
times	O
.	O
Nor	O
had	O
it	O
shown	O
whether	O
BERT	B-MethodName
could	O
be	O
over	O
-	O
pruned	O
for	O
a	O
memory	O
/	O
accuracy	B-MetricName
trade	O
-	O
off	O
for	O
deployment	O
to	O
low	O
-	O
resource	O
devices	O
.	O
In	O
this	O
work	O
,	O
we	O
conclude	O
that	O
BERT	B-MethodName
can	O
be	O
pruned	O
prior	O
to	O
distribution	O
without	O
affecting	O
it	O
's	O
universality	O
,	O
and	O
that	O
BERT	B-MethodName
may	O
be	O
over	O
-	O
pruned	O
during	O
pre	O
-	O
training	O
for	O
a	O
reasonable	O
accuracy	B-MetricName
trade	O
-	O
off	O
for	O
certain	O
tasks	O
.	O
2	O
Pruning	O
:	O
Compression	O
,	O
Regularization	O
,	O
Architecture	O
Search	O
Neural	O
network	B-TaskName
pruning	I-TaskName
involves	O
examining	O
a	O
trained	O
network	O
and	O
removing	O
parts	O
deemed	O
to	O
be	O
unnecessary	O
by	O
some	O
heuristic	O
saliency	O
criterion	O
.	O
One	O
might	O
remove	O
weights	O
,	O
neurons	O
,	O
layers	O
,	O
channels	O
,	O
attention	O
heads	O
,	O
etc	O
.	O
depending	O
on	O
which	O
heuristic	O
is	O
used	O
.	O
Below	O
,	O
we	O
describe	O
three	O
different	O
lenses	O
through	O
which	O
we	O
might	O
interpret	O
pruning	O
.	O
Compression	O
Pruning	O
a	O
neural	O
network	O
decreases	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
required	O
to	O
specify	O
the	O
model	O
,	O
which	O
decreases	O
the	O
disk	O
space	O
required	O
to	O
store	O
it	O
.	O
This	O
allows	O
large	O
models	O
to	O
be	O
deployed	O
on	O
edge	O
computing	O
devices	O
like	O
smartphones	O
.	O
Pruning	O
can	O
also	O
increase	O
inference	O
speed	O
if	O
whole	O
neurons	O
or	O
convolutional	O
channels	O
are	O
pruned	O
,	O
which	O
reduces	O
GPU	O
usage	O
.	O
1	O
Regularization	O
Pruning	O
a	O
neural	O
network	O
also	O
regularizes	O
it	O
.	O
We	O
might	O
consider	O
pruning	O
to	O
be	O
a	O
form	O
of	O
permanent	O
dropout	O
(	O
Molchanov	O
et	O
al	O
,	O
2017	O
)	O
or	O
a	O
heuristic	O
-	O
based	O
L0	O
regularizer	O
(	O
Louizos	O
et	O
al	O
,	O
2018	O
)	O
.	O
Through	O
this	O
lens	O
,	O
pruning	O
decreases	O
the	O
complexity	O
of	O
the	O
network	O
and	O
therefore	O
narrows	O
the	O
range	O
of	O
possible	O
functions	O
it	O
can	O
express	O
.	O
2	O
The	O
main	O
difference	O
between	O
L0	O
or	O
L1	B-MethodName
regularization	I-MethodName
and	O
weight	O
pruning	O
is	O
that	O
the	O
former	O
induce	O
sparsity	O
via	O
a	O
penalty	O
on	O
the	O
loss	B-MetricName
function	O
,	O
which	O
is	O
learned	O
during	O
gradient	O
descent	O
via	O
stochastic	O
relaxation	O
.	O
It	O
's	O
not	O
clear	O
which	O
approach	O
is	O
more	O
principled	O
or	O
preferred	O
.	O
(	O
Gale	O
et	O
al	O
,	O
2019	O
)	O
Sparse	O
Architecture	O
Search	O
Finally	O
,	O
we	O
can	O
view	O
neural	O
network	B-TaskName
pruning	I-TaskName
as	O
a	O
type	O
of	O
sparse	O
architecture	O
search	O
.	O
Liu	O
et	O
al	O
(	O
2019b	O
)	O
and	O
Frankle	O
and	O
Carbin	O
(	O
2019	O
)	O
show	O
that	O
they	O
can	O
train	O
carefully	O
re	O
-	O
initialized	O
pruned	O
architectures	O
to	O
similar	O
performance	O
levels	O
as	O
dense	O
networks	O
.	O
Under	O
this	O
lens	O
,	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
(	O
SGD	B-MethodName
)	O
induces	O
network	O
sparsity	O
,	O
and	O
pruning	O
simply	O
makes	O
that	O
sparsity	O
explicit	O
.	O
These	O
sparse	O
architectures	O
,	O
along	O
with	O
the	O
appropriate	O
initializations	O
,	O
are	O
sometimes	O
referred	O
to	O
as	O
"	O
lottery	O
tickets	O
.	O
"	O
3	O

In	O
this	O
work	O
,	O
we	O
focus	O
on	O
weight	O
magnitude	O
pruning	O
because	O
it	O
is	O
one	O
of	O
the	O
most	O
fine	O
-	O
grained	O
and	O
effective	O
pruning	O
methods	O
.	O
It	O
also	O
has	O
a	O
compelling	O
saliency	O
criterion	O
(	O
Han	O
et	O
al	O
,	O
2015	O
)	O
:	O
if	O
a	O
weight	O
is	O
close	O
to	O
zero	O
,	O
then	O
its	O
input	O
is	O
effectively	O
ignored	O
,	O
which	O
means	O
the	O
weight	O
can	O
be	O
pruned	O
.	O
Magnitude	O
weight	O
pruning	O
itself	O
is	O
a	O
simple	O
procedure	O
:	O
1	O
.	O
Pick	O
a	O
target	O
percentage	O
of	O
weights	O
to	O
be	O
pruned	O
,	O
say	O
50	O
%	O
.	O
2	O
.	O
Calculate	O
a	O
threshold	O
such	O
that	O
50	O
%	O
of	O
weight	O
magnitudes	O
are	O
under	O
that	O
threshold	O
.	O
3	O
.	O
Remove	O
those	O
weights	O
.	O
4	O
.	O
Continue	O
training	O
the	O
network	O
to	O
recover	O
any	O
lost	O
accuracy	B-MetricName
.	O
5	O
.	O
Optionally	O
,	O
return	O
to	O
step	O
1	O
and	O
increase	O
the	O
percentage	O
of	O
weights	O
pruned	O
.	O
This	O
procedure	O
is	O
conveniently	O
implemented	O
in	O
a	O
Tensorflow	O
(	O
Abadi	O
et	O
al	O
,	O
2016	O
)	O
package	O
4	O
,	O
which	O
we	O
use	O
(	O
Zhu	O
and	O
Gupta	O
,	O
2017	O
)	O
.	O
Calculating	O
a	O
threshold	O
and	O
pruning	O
can	O
be	O
done	O
for	O
all	O
network	O
parameters	O
holistically	O
(	O
global	O
pruning	O
)	O
or	O
for	O
each	O
weight	O
matrix	O
individually	O
(	O
matrix	O
-	O
local	O
pruning	O
)	O
.	O
Both	O
methods	O
will	O
prune	O
to	O
the	O
same	O
sparsity	O
,	O
but	O
in	O
global	O
pruning	O
the	O
sparsity	O
might	O
be	O
unevenly	O
distributed	O
across	O
weight	O
matrices	O
.	O
We	O
use	O
matrix	O
-	O
local	O
pruning	O
because	O
it	O
is	O
more	O
popular	O
in	O
the	O
community	O
.	O
5	O
For	O
information	O
on	O
other	O
pruning	O
techniques	O
,	O
we	O
recommend	O
Gale	O
et	O
al	O
(	O
2019	O
)	O
and	O
Liu	O
et	O
al	O
(	O
2019b	O
)	O
.	O

BERT	B-MethodName
-	O
Base	O
consists	O
of	O
12	O
encoder	O
layers	O
,	O
each	O
of	O
which	O
contains	O
6	O
prunable	O
matrices	O
:	O
4	O
for	O
the	O
multiheaded	O
self	O
-	O
attention	O
and	O
2	O
for	O
the	O
layer	O
's	O
output	O
feed	O
-	O
forward	O
network	O
.	O
Recall	B-MetricName
that	O
self	O
-	O
attention	O
first	O
projects	O
layer	O
inputs	O
into	O
key	O
,	O
query	O
,	O
and	O
value	O
embeddings	O
via	O
linear	O
projections	O
.	O
While	O
there	O
is	O
a	O
separate	O
key	O
,	O
query	O
,	O
and	O
value	O
projection	O
matrix	O
for	O
each	O
attention	O
head	O
,	O
implementations	O
typically	O
"	O
stack	O
"	O
matrices	O
from	O
each	O
attention	O
head	O
,	O
resulting	O
in	O
only	O
3	O
parameter	O
matrices	O
:	O
one	O
for	O
key	O
projections	O
,	O
one	O
for	O
value	O
projections	O
,	O
and	O
one	O
for	O
query	O
projections	O
.	O
We	O
prune	O
each	O
of	O
these	O
matrices	O
separately	O
,	O
calculating	O
a	O
threshold	O
for	O
each	O
.	O
We	O
also	O
prune	O
the	O
linear	O
output	O
projection	O
,	O
which	O
combines	O
outputs	O
from	O
each	O
attention	O
head	O
into	O
a	O
single	O
embedding	O
.	O
6	O
We	O
prune	O
word	B-TaskName
embeddings	I-TaskName
in	O
the	O
same	O
way	O
we	O
prune	O
feed	O
-	O
foward	O
networks	O
and	O
self	O
-	O
attention	O
parameters	O
.	O
7	O
The	O
justification	O
is	O
similar	O
:	O
if	O
a	O
word	O
embedding	O
value	O
is	O
close	O
to	O
zero	O
,	O
we	O
can	O
assume	O
it	O
's	O
zero	O
and	O
store	O
the	O
rest	O
in	O
a	O
sparse	O
matrix	O
.	O
This	O
is	O
useful	O
because	O
token	O
/	O
subword	O
embeddings	O
tend	O
to	O
account	O
for	O
a	O
large	O
portion	O
of	O
a	O
natural	O
language	O
model	O
's	O
memory	O
.	O
In	O
BERT	B-MethodName
-	O
Base	O
specifically	O
,	O
5	O
The	O
weights	O
in	O
almost	O
every	O
matrix	O
in	O
BERT	B-MethodName
-	O
Base	O
are	O
approximately	O
normally	O
distributed	O
with	O
mean	O
0	B-DatasetName
and	O
variance	O
between	O
0.03	O
and	O
0.05	O
(	O
Table	O
A	O
)	O
.	O
This	O
similarity	O
may	O
imply	O
that	O
global	O
pruning	O
would	O
perform	O
similarly	O
to	O
matrix	O
-	O
local	O
pruning	O
.	O
6	O
We	O
could	O
have	O
calculated	O
a	O
single	O
threshold	O
for	O
the	O
entire	O
self	O
-	O
attention	O
layer	O
or	O
for	O
each	O
attention	O
head	O
separately	O
.	O
Similar	O
to	O
global	O
pruning	O
vs.	O
matrix	O
-	O
local	O
pruning	O
,	O
it	O
's	O
not	O
clear	O
which	O
one	O
should	O
be	O
preferred	O
.	O
7	O
Interestingly	O
,	O
pruning	O
word	B-TaskName
embeddings	I-TaskName
is	O
slightly	O
more	O
interpretable	O
that	O
pruning	O
other	O
matrices	O
.	O
See	O
Figure	O
?	O
?	O
for	O
a	O
heatmap	B-MethodName
of	O
embedding	O
magnitudes	O
,	O
which	O
shows	O
that	O
shorter	O
subwords	O
tend	O
to	O
be	O
pruned	O
more	O
than	O
longer	O
subwords	O
and	O
that	O
certain	O
dimensions	O
are	O
almost	O
never	O
pruned	O
in	O
any	O
subword	O
.	O
the	O
embeddings	O
account	O
for	O
∼21	O
%	O
of	O
the	O
model	O
's	O
memory	O
.	O
Our	O
experimental	O
code	O
for	O
pruning	O
BERT	B-MethodName
,	O
based	O
on	O
the	O
public	O
BERT	B-MethodName
repository	O
,	O
is	O
available	O
here	O
.	O
8	O

We	O
perform	O
weight	O
magnitude	O
pruning	O
on	O
a	O
pretrained	O
BERT	B-MethodName
-	O
Base	O
model	O
.	O
9	O
We	O
select	O
sparsities	O
from	O
0	B-DatasetName
%	O
to	O
90	O
%	O
in	O
increments	O
of	O
10	O
%	O
and	O
gradually	O
prune	O
BERT	B-MethodName
to	O
this	O
sparsity	O
over	O
the	O
first	O
10k	O
steps	O
of	O
training	O
.	O
We	O
continue	O
pre	O
-	O
training	O
on	O
English	O
Wikipedia	O
and	O
BookCorpus	B-DatasetName
for	O
another	O
90k	O
steps	O
to	O
regain	O
any	O
lost	O
accuracy	B-MetricName
.	O
10	O
The	O
resulting	O
pre	O
-	O
training	O
losses	O
are	O
shown	O
in	O
Table	O
1	O
.	O
We	O
then	O
fine	O
-	O
tune	O
these	O
pruned	O
models	O
on	O
tasks	O
from	O
the	O
General	B-DatasetName
Language	O
Understanding	O
Evaluation	O
(	O
GLUE	B-DatasetName
)	O
benchmark	O
,	O
which	O
is	O
a	O
standard	O
set	O
of	O
9	O
tasks	O
that	O
include	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
,	O
etc	O
.	O
We	O
avoid	O
WNLI	B-DatasetName
,	O
which	O
is	O
known	O
to	O
be	O
problematic	O
.	O
11	O
We	O
also	O
avoid	O
tasks	O
with	O
less	O
than	O
5k	O
training	O
examples	O
because	O
the	O
results	O
tend	O
to	O
be	O
noisy	O
(	O
RTE	B-DatasetName
,	O
MRPC	B-DatasetName
,	O
STS	B-DatasetName
-	I-DatasetName
B	I-DatasetName
)	O
.	O
We	O
fine	O
-	O
tune	O
a	O
separate	O
model	O
on	O
each	O
of	O
the	O
remaining	O
5	O
GLUE	B-DatasetName
tasks	O
for	O
3	O
epochs	O
and	O
try	O
4	O
learning	O
rates	O
:	O
[	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
]	O
×	O
10	O
−5	O
.	O
The	O
best	O
evaluation	O
accuracies	O
are	O
averaged	O
and	O
plotted	O
in	O
Figure	O
1	O
.	O
Individual	O
task	O
results	O
are	O
in	O
Table	O
1	O
.	O
BERT	B-MethodName
can	O
be	O
used	O
as	O
a	O
static	O
feature	O
-	O
extractor	O
or	O
as	O
a	O
pre	O
-	O
trained	O
model	O
which	O
is	O
fine	O
-	O
tuned	O
endto	O
-	O
end	O
.	O
In	O
all	O
experiments	O
,	O
we	O
fine	O
-	O
tune	O
weights	O
in	O
all	O
layers	O
of	O
BERT	B-MethodName
on	O
downstream	O
tasks	O
.	O

Pruning	O
involves	O
two	O
steps	O
:	O
it	O
deletes	O
the	O
information	O
stored	O
in	O
a	O
weight	O
by	O
setting	O
it	O
to	O
0	B-DatasetName
and	O
then	O
regularizes	O
the	O
model	O
by	O
preventing	O
that	O
weight	O
from	O
changing	O
during	O
further	O
training	O
.	O
To	O
disentangle	O
these	O
two	O
effects	O
(	O
model	O
complexity	O
restriction	O
and	O
information	O
deletion	O
)	O
,	O
we	O
repeat	O
the	O
experiments	O
from	O
Section	O
3.2	O
with	O
an	O
identical	O
pre	O
-	O
training	O
setup	O
,	O
but	O
instead	O
of	O
pruning	O
we	O
simply	O
set	O
the	O
weights	O
to	O
0	B-DatasetName
and	O
allow	O
them	O
to	O
vary	O
during	O
downstream	O
training	O
.	O
This	O
deletes	O
the	O
pre	O
-	O
training	O
information	O
associated	O
with	O
the	O
weight	O
but	O
does	O
not	O
prevent	O
the	O
model	O
from	O
fitting	O
downstream	O
datasets	O
by	O
keeping	O
the	O
weight	O
at	O
zero	O
during	O
downstream	O
training	O
.	O
We	O
also	O
fine	O
-	O
tune	O
on	O
downstream	O
tasks	O
until	O
training	O
loss	B-MetricName
becomes	O
comparable	O
to	O
models	O
with	O
no	O
pruning	O
.	O
We	O
trained	O
most	O
models	O
for	O
epochs	O
rather	O
than	O
3	O
.	O
Models	O
with	O
70	O
-	O
90	O
%	O
information	O
deletion	O
required	O
15	O
epochs	O
to	O
fit	O
the	O
training	O
data	O
.	O
The	O
results	O
are	O
also	O
included	O
in	O
Figure	O
1	O
and	O
Table	O
1	O
.	O

We	O
might	O
expect	O
that	O
BERT	B-MethodName
would	O
be	O
more	O
compressible	O
after	O
downstream	O
fine	O
-	O
tuning	O
.	O
Intuitively	O
,	O
the	O
information	O
needed	O
for	O
downstream	O
tasks	O
is	O
a	O
subset	O
of	O
the	O
information	O
learned	O
during	O
pretraining	O
;	O
some	O
tasks	O
require	O
more	O
semantic	O
information	O
than	O
syntactic	O
,	O
and	O
vice	O
-	O
versa	O
.	O
We	O
should	O
be	O
able	O
to	O
discard	O
the	O
"	O
extra	O
"	O
information	O
and	O
only	O
keep	O
what	O
we	O
need	O
for	O
,	O
say	O
,	O
parsing	O
(	O
Li	O
and	O
Eisner	O
,	O
2019	O
)	O
.	O
For	O
magnitude	O
weight	O
pruning	O
specifically	O
,	O
we	O
might	O
expect	O
downstream	O
training	O
to	O
change	O
the	O
distribution	O
of	O
weights	O
in	O
the	O
parameter	O
matrices	O
.	O
This	O
,	O
in	O
turn	O
,	O
changes	O
the	O
sort	O
-	O
order	O
of	O
the	O
absolute	O
values	O
of	O
those	O
weights	O
,	O
which	O
changes	O
the	O
order	O
that	O
we	O
prune	O
them	O
in	O
.	O
This	O
new	O
pruning	O
order	O
,	O
hypothetically	O
,	O
would	O
be	O
less	O
degrading	O
to	O
our	O
specific	O
downstream	O
task	O
.	O
To	O
test	O
this	O
,	O
we	O
fine	O
-	O
tuned	O
pre	O
-	O
trained	O
BERT	B-MethodName
-	O
Base	O
on	O
downstream	O
data	O
for	O
3	O
epochs	O
.	O
We	O
then	O
pruned	O
at	O
various	O
sparsity	O
levels	O
and	O
continued	O
training	O
for	O
5	O
more	O
epochs	O
(	O
7	O
for	O
80/90	O
%	O
sparsity	O
)	O
,	O
at	O
which	O
point	O
the	O
training	O
losses	O
became	O
comparable	O
to	O
those	O
of	O
models	O
pruned	O
during	O
pretraining	O
.	O
We	O
repeat	O
this	O
for	O
learning	O
rates	O
in	O
[	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
]	O
×10	O
−5	O
and	O
show	O
the	O
results	O
with	O
the	O
best	O
development	O
accuracy	B-MetricName
in	O
Figure	O
1	O
/	O
Table	O
1	O
.	O
We	O
also	O
measure	O
the	O
difference	O
in	O
which	O
weights	O
are	O
selected	O
for	O
pruning	O
during	O
pre	O
-	O
training	O
vs.	O
downstream	O
fine	O
-	O
tuning	O
and	O
plot	O
the	O
results	O
in	O
Figure	O
3	O
.	O

Figure	O
1	O
shows	O
that	O
the	O
first	O
30	O
-	O
40	O
%	O
of	O
weights	O
pruned	O
by	O
magnitude	O
weight	O
pruning	O
do	O
not	O
impact	O
pre	O
-	O
training	O
loss	B-MetricName
or	O
inference	O
on	O
any	O
downstream	O
task	O
.	O
These	O
weights	O
can	O
be	O
pruned	O
either	O
before	O
or	O
after	O
fine	O
-	O
tuning	O
.	O
This	O
makes	O
sense	O
from	O
the	O
perspective	O
of	O
pruning	O
as	O
sparse	O
architecture	O
search	O
:	O
when	O
we	O
initialize	O
BERT	B-MethodName
-	O
Base	O
,	O
we	O
initialize	O
many	O
possible	O
subnetworks	O
.	O
SGD	B-MethodName
selects	O
the	O
best	O
one	O
for	O
pre	O
-	O
training	O
and	O
pushes	O
the	O
rest	O
of	O
the	O
weights	O
to	O
0	B-DatasetName
.	O
We	O
can	O
then	O
prune	O
those	O
weights	O
without	O
affecting	O
the	O
output	O
of	O
the	O
network	O
.	O
12	O

Past	O
40	O
%	O
pruning	O
,	O
performance	O
starts	O
to	O
degrade	O
.	O
Pre	O
-	O
training	O
loss	B-MetricName
increases	O
as	O
we	O
prune	O
weights	O
necessary	O
for	O
fitting	O
the	O
pre	O
-	O
training	O
data	O
(	O
Table	O
1	O
)	O
.	O
Feature	O
activations	O
of	O
the	O
hidden	O
layers	O
start	O
to	O
diverge	O
from	O
models	O
with	O
low	O
levels	O
of	O
pruning	O
(	O
Figure	O
2	O
)	O
.	O
13	O
Downstream	O
accuracy	B-MetricName
also	O
begins	O
to	O
degrade	O
at	O
this	O
point	O
.	O
Why	O
does	O
pruning	O
at	O
these	O
levels	O
hurt	O
downstream	O
performance	O
?	O
On	O
one	O
hand	O
,	O
pruning	O
deletes	O
pre	O
-	O
training	O
information	O
by	O
setting	O
weights	O
to	O
0	B-DatasetName
,	O
preventing	O
the	O
transfer	O
of	O
the	O
useful	O
inductive	O
biases	O
learned	O
during	O
pre	O
-	O
training	O
.	O
On	O
the	O
other	O
hand	O
,	O
pruning	O
regularizes	O
the	O
model	O
by	O
keeping	O
certain	O
weights	O
at	O
zero	O
,	O
which	O
might	O
prevent	O
fitting	O
downstream	O
datasets	O
.	O
Figure	O
1	O
and	O
Table	O
1	O
show	O
information	O
deletion	O
is	O
the	O
main	O
cause	O
of	O
performance	O
degradation	O
between	O
40	O
-	O
60	O
%	O
sparsity	O
,	O
since	O
pruning	O
and	O
information	O
deletion	O
degrade	O
models	O
by	O
the	O
same	O
amount	O
.	O
Information	O
deletion	O
would	O
not	O
be	O
a	O
problem	O
if	O
pretraining	O
and	O
downstream	O
datasets	O
contained	O
similar	O
information	O
.	O
However	O
,	O
pre	O
-	O
training	O
is	O
effective	O
precisely	O
because	O
the	O
pre	O
-	O
training	O
dataset	O
is	O
much	O
larger	O
than	O
the	O
labeled	O
downstream	O
dataset	O
,	O
which	O
allows	O
learning	O
of	O
more	O
robust	O
representations	O
.	O
We	O
see	O
that	O
the	O
main	O
obstacle	O
to	O
compressing	O
pre	O
-	O
trained	O
models	O
is	O
maintaining	O
the	O
inductive	O
bias	O
of	O
the	O
model	O
learned	O
during	O
pre	O
-	O
training	O
.	O
Encoding	O
this	O
bias	O
requires	O
many	O
more	O
weights	O
than	O
fitting	O
downstream	O
datasets	O
,	O
and	O
it	O
can	O
not	O
be	O
recovered	O
due	O
to	O
a	O
fundamental	O
information	O
gap	O
between	O
pretraining	O
and	O
downstream	O
datasets	O
.	O
14	O
This	O
leads	O
us	O
to	O
believe	O
that	O
the	O
amount	O
a	O
model	O
can	O
be	O
pruned	O
12	O
We	O
know	O
,	O
however	O
,	O
that	O
increasing	O
the	O
size	O
of	O
BERT	B-MethodName
to	O
BERT	B-MethodName
-	O
Large	O
improves	O
performance	O
.	O
This	O
view	O
does	O
not	O
fully	O
explain	O
why	O
even	O
an	O
obviously	O
under	O
-	O
parameterized	O
model	O
should	O
become	O
sparse	O
.	O
This	O
may	O
be	O
caused	O
by	O
dropout	O
,	O
or	O
it	O
may	O
be	O
a	O
general	O
property	O
of	O
our	O
training	O
regime	O
(	O
SGD	B-MethodName
)	O
.	O
Perhaps	O
an	O
extension	O
of	O
to	O
under	O
-	O
parameterized	O
models	O
would	O
provide	O
some	O
insight	O
.	O
13	O
We	O
believe	O
this	O
observation	O
may	O
point	O
towards	O
a	O
more	O
principled	O
stopping	O
criterion	O
for	O
pruning	O
.	O
Currently	O
,	O
the	O
only	O
way	O
to	O
know	O
how	O
much	O
to	O
prune	O
is	O
by	O
trial	O
and	O
(	O
dev	O
-	O
set	O
)	O
error	O
.	O
Predictors	O
of	O
performance	O
degradation	O
while	O
pruning	O
might	O
help	O
us	O
decide	O
which	O
level	O
of	O
sparsity	O
is	O
appropriate	O
for	O
a	O
given	O
trained	O
network	O
without	O
trying	O
many	O
at	O
once	O
.	O
14	O
We	O
might	O
consider	O
finding	O
a	O
lottery	O
ticket	O
for	O
BERT	B-MethodName
,	O
which	O
we	O
would	O
expect	O
to	O
fit	O
the	O
GLUE	B-DatasetName
training	O
data	O
just	O
as	O
well	O
as	O
pre	O
-	O
trained	O
BERT	B-MethodName
.	O
However	O
,	O
we	O
predict	O
that	O
the	O
lottery	O
-	O
ticket	O
will	O
not	O
reach	O
similar	O
generalization	O
levels	O
unless	O
the	O
lottery	O
ticket	O
encodes	O
enough	O
information	O
to	O
close	O
the	O
information	O
gap	O
.	O
Also	O
shown	O
are	O
models	O
with	O
information	O
deletion	O
during	O
pre	O
-	O
training	O
(	O
orange	O
)	O
,	O
models	O
pruned	O
after	O
downstream	O
fine	O
-	O
tuning	O
(	O
green	O
)	O
,	O
and	O
models	O
pruned	O
randomly	O
during	O
pre	O
-	O
training	O
instead	O
of	O
by	O
lowest	O
magnitude	O
(	O
red	O
)	O
.	O
30	O
-	O
40	O
%	O
of	O
weights	O
can	O
be	O
pruned	O
using	O
magnitude	O
weight	O
pruning	O
without	O
decreasing	O
dowsntream	O
accuracy	B-MetricName
.	O
Notice	O
that	O
information	O
deletion	O
fits	O
the	O
training	O
data	O
better	O
than	O
un	O
-	O
pruned	O
models	O
at	O
all	O
sparsity	O
levels	O
but	O
does	O
not	O
fully	O
recover	O
evaluation	O
accuracy	B-MetricName
.	O
Also	O
,	O
models	O
pruned	O
after	O
downstream	O
fine	O
-	O
tuning	O
have	O
the	O
same	O
or	O
worse	O
development	O
accuracy	B-MetricName
,	O
despite	O
achieving	O
lower	O
training	O
losses	O
.	O
Note	O
:	O
none	O
of	O
the	O
pruned	O
models	O
are	O
overfitting	O
because	O
un	O
-	O
pruned	O
models	O
have	O
the	O
lowest	O
training	O
loss	B-MetricName
and	O
the	O
highest	O
development	O
accuracy	B-MetricName
.	O
While	O
the	O
results	O
for	O
individual	O
tasks	O
are	O
in	O
Table	O
1	O
,	O
each	O
task	O
does	O
not	O
vary	O
much	O
from	O
the	O
average	O
trend	O
,	O
with	O
an	O
exception	O
discussed	O
in	O
Section	O
4.3	O
.	O
Figure	O
2	O
:	O
(	O
Left	O
)	O
Pre	O
-	O
training	O
loss	B-MetricName
predicts	O
information	O
deletion	O
GLUE	B-DatasetName
accuracy	B-MetricName
linearly	O
as	O
sparsity	O
increases	O
.	O
We	O
believe	O
the	O
slope	O
of	O
each	O
line	O
tells	O
us	O
how	O
much	O
a	O
bit	O
of	O
BERT	B-MethodName
is	O
worth	O
to	O
each	O
task	O
.	O
(	O
CoLA	B-DatasetName
at	O
90	O
%	O
is	O
excluded	O
from	O
the	O
line	O
of	O
best	O
fit	O
.	O
)	O
(	O
Right	O
)	O
The	O
cosine	O
similarities	O
of	O
features	O
extracted	O
for	O
a	O
subset	O
of	O
the	O
pre	O
-	O
training	O
development	O
data	O
before	O
and	O
after	O
pruning	O
.	O
Features	O
are	O
extracted	O
from	O
activations	O
of	O
all	O
12	O
layers	O
of	O
BERT	B-MethodName
and	O
compared	O
layer	O
-	O
wise	O
to	O
a	O
model	O
that	O
has	O
not	O
been	O
pruned	O
.	O
As	O
performance	O
degrades	O
,	O
cosine	O
similarities	O
of	O
features	O
decreases	O
.	O
is	O
limited	O
by	O
the	O
largest	O
dataset	O
the	O
model	O
has	O
been	O
trained	O
on	O
:	O
in	O
this	O
case	O
,	O
the	O
pre	O
-	O
training	O
dataset	O
.	O
15	O

At	O
70	O
%	O
sparsity	O
and	O
above	O
,	O
models	O
with	O
information	O
deletion	O
recover	O
some	O
accuracy	B-MetricName
w.r.t	O
.	O
pruned	O
models	O
,	O
so	O
complexity	O
restriction	O
is	O
a	O
secondary	O
cause	O
of	O
performance	O
degradation	O
.	O
However	O
,	O
these	O
models	O
do	O
not	O
recover	O
all	O
evaluation	O
accuracy	B-MetricName
,	O
despite	O
matching	O
un	O
-	O
pruned	O
model	O
's	O
training	O
loss	B-MetricName
.	O
Table	O
1	O
shows	O
that	O
on	O
the	O
MNLI	B-DatasetName
and	O
QQP	B-DatasetName
tasks	O
,	O
which	O
have	O
the	O
largest	O
amount	O
of	O
training	O
data	O
,	O
information	O
deletion	O
performs	O
much	O
better	O
than	O
pruning	O
.	O
In	O
contrast	O
,	O
models	O
do	O
not	O
recover	O
as	O
well	O
on	O
SST	B-DatasetName
-	O
2	O
and	O
CoLA	B-DatasetName
,	O
which	O
have	O
less	O
data	O
.	O
We	O
believe	O
this	O
is	O
because	O
the	O
larger	O
datasets	O
require	O
larger	O
models	O
to	O
fit	O
,	O
so	O
complexity	O
restriction	O
becomes	O
an	O
issue	O
earlier	O
.	O
We	O
might	O
be	O
concerned	O
that	O
poorly	O
performing	O
models	O
are	O
over	O
-	O
fitting	O
,	O
since	O
they	O
have	O
lower	O
training	O
losses	O
than	O
unpruned	O
models	O
.	O
But	O
the	O
best	O
performing	O
information	O
-	O
deleted	O
models	O
have	O
the	O
lowest	O
training	O
error	O
of	O
all	O
,	O
so	O
overfitting	O
seems	O
unlikely	O
.	O
16	O

We	O
've	O
seen	O
that	O
over	O
-	O
pruning	O
BERT	O
deletes	B-MethodName
information	O
useful	O
for	O
downstream	O
tasks	O
.	O
Is	O
this	O
information	O
equally	O
useful	O
to	O
all	O
tasks	O
?	O
We	O
might	O
consider	O
the	O
pre	O
-	O
training	O
loss	O
as	B-MetricName
a	O
proxy	O
for	O
how	O
much	O
pre	O
-	O
training	O
information	O
we	O
've	O
deleted	O
in	O
total	O
.	O
Similarly	O
,	O
the	O
performance	O
of	O
informationdeletion	O
models	O
is	O
a	O
proxy	O
for	O
how	O
much	O
of	O
that	O
information	O
was	O
useful	O
for	O
each	O
task	O
.	O
Figure	O
2	O
shows	O
that	O
the	O
pre	O
-	O
training	O
loss	O
linearly	O
predicts	B-MetricName
the	O
effects	O
of	O
information	O
deletion	O
on	O
downstream	O
accuracy	O
.	O
For	B-MetricName
every	O
bit	O
of	O
information	O
we	O
delete	O
from	O
BERT	O
,	O
it	B-MethodName
appears	O
only	O
a	O
fraction	O
is	O
useful	O
for	O
CoLA	O
,	O
and	B-DatasetName
an	O
even	O
smaller	O
fraction	O
useful	O
for	O
QQP	O
.	O
17	B-DatasetName
This	O
relationship	O
should	O
be	O
taken	O
into	O
account	O
when	O
considering	O
the	O
memory	O
/	O
accuracy	O
trade	O
-	B-MetricName
off	O
of	O
overpruning	O
.	O
Pruning	O
an	O
extra	O
30	O
%	O
of	O
BERT	O
's	O
weights	B-MethodName
Figure	O
3	O
:	O
(	O
Top	O
)	O
The	O
measured	O
difference	O
in	O
pruning	O
masks	O
between	O
models	O
pruned	O
during	O
pre	O
-	O
training	O
and	O
models	O
pruned	O
during	O
downstream	O
fine	O
-	O
tuning	O
.	O
As	O
predicted	O
,	O
the	O
differences	O
are	O
less	O
than	O
6	O
%	O
,	O
since	O
finetuning	O
only	O
changes	O
the	O
magnitude	O
sorting	O
order	O
of	O
weights	O
locally	O
,	O
not	O
globally	O
.	O
(	O
Bottom	O
)	O
The	O
average	O
GLUE	O
development	O
accuracy	B-DatasetName
and	O
pruning	B-MetricName
mask	O
difference	O
for	O
models	O
trained	O
on	O
downstream	O
datasets	O
before	O
pruning	O
60	O
%	O
at	O
learning	O
rate	O
5e	B-HyperparameterName
-	I-HyperparameterName
5	O
.	O
After	O
pruning	O
,	O
models	O
are	O
trained	O
for	O
an	O
additional	O
2	O
epochs	O
to	O
regain	O
accuracy	O
.	O
We	B-MetricName
see	O
that	O
training	O
between	O
3	O
and	O
12	O
epochs	O
before	O
pruning	O
does	O
not	O
change	O
which	O
weights	O
are	O
pruned	O
or	O
improve	O
performance	O
.	O
is	O
worth	O
only	O
one	O
accuracy	O
point	O
on	B-MetricName
QQP	O
but	O
10	B-DatasetName
points	O
on	O
CoLA	O
.	O
It	B-DatasetName
's	O
unclear	O
,	O
however	O
,	O
whether	O
this	O
is	O
because	O
the	O
pre	O
-	O
training	O
task	O
is	O
less	O
relevant	O
to	O
QQP	O
or	O
whether	B-DatasetName
QQP	O
simply	O
has	B-DatasetName
a	O
bigger	O
dataset	O
with	O
more	O
information	O
content	O
.	O
18	O

Since	O
pre	O
-	O
training	O
information	O
deletion	O
plays	O
a	O
central	O
role	O
in	O
performance	O
degradation	O
while	O
overpruning	O
,	O
we	O
might	O
expect	O
that	O
downstream	O
fine	O
-	O
tuning	O
would	O
improve	O
prunability	O
by	O
making	O
important	O
weights	O
more	O
salient	O
(	O
increasing	O
their	O
magnitude	O
)	O
.	O
However	O
,	O
Figure	O
1	O
shows	O
that	O
models	O
pruned	O
after	O
downstream	O
fine	O
-	O
tuning	O
do	O
not	O
surpass	O
the	O
development	O
accuracies	O
of	O
models	O
pruned	O
during	O
pre	O
-	O
training	O
,	O
despite	O
achieving	O
similar	O
training	O
losses	O
.	O
Figure	O
3	O
shows	O
fine	O
-	O
tuning	O
changes	O
which	O
weights	O
are	O
pruned	O
by	O
less	O
than	O
6	O
%	O
.	O
Why	O
does	O
n't	O
fine	O
-	O
tuning	O
change	O
which	O
weights	O
are	O
pruned	O
much	O
?	O
Table	O
2	O
shows	O
that	O
the	O
magnitude	O
sorting	O
order	O
of	O
weights	O
is	O
mostly	O
preserved	O
;	O
weights	O
move	O
on	O
average	O
0	B-DatasetName
-	O
4	O
%	O
away	O
from	O
their	O
starting	O
positions	O
in	O
the	O
sort	O
order	O
.	O
We	O
also	O
see	O
that	O
high	O
magnitude	O
weights	O
are	O
more	O
stable	O
than	O
lower	O
ones	O
(	O
Figure	O
6	O
)	O
.	O
Our	O
experiments	O
suggest	O
that	O
training	O
on	O
downstream	O
data	O
before	O
pruning	O
is	O
too	O
blunt	O
an	O
instrument	O
to	O
improve	O
prunability	O
.	O
Even	O
so	O
,	O
we	O
might	O
consider	O
simply	O
training	O
on	O
the	O
downstream	O
tasks	O
for	O
much	O
longer	O
,	O
which	O
would	O
increase	O
the	O
difference	O
in	O
weights	O
pruned	O
.	O
However	O
,	O
Figure	O
4	O
shows	O
that	O
even	O
after	O
an	O
epoch	O
of	O
downstream	O
fine	O
-	O
tuning	O
,	O
weights	O
quickly	O
re	O
-	O
stabilize	O
in	O
a	O
new	O
sorting	O
order	O
,	O
meaning	O
longer	O
downstream	O
training	O
will	O
have	O
only	O
a	O
marginal	O
effect	O
on	O
which	O
weights	O
are	O
pruned	O
.	O
Indeed	O
,	O
Figure	O
3	O
shows	O
that	O
the	O
weights	O
selected	O
for	O
60	O
%	O
pruning	O
quickly	O
stabilize	O
and	O
evaluation	O
accuracy	B-MetricName
does	O
not	O
improve	O
with	O
more	O
training	O
before	O
pruning	O
.	O

Compressing	O
BERT	B-MethodName
for	O
Specific	O
Tasks	O
Section	O
5	O
showed	O
that	O
downstream	O
fine	O
-	O
tuning	O
does	O
not	O
increase	O
prunability	O
.	O
However	O
,	O
several	O
alternative	O
compression	O
approaches	O
have	O
been	O
proposed	O
to	O
discard	O
non	O
-	O
task	O
-	O
specific	O
information	O
.	O
Li	O
and	O
Eisner	O
(	O
2019	O
)	O
used	O
an	O
information	O
bottleneck	O
to	O
discard	O
non	O
-	O
syntactic	O
information	O
.	O
Tang	O
et	O
al	O
(	O
2019	O
)	O
used	O
BERT	B-MethodName
as	O
a	O
knowledge	B-MethodName
distillation	I-MethodName
teacher	O
to	O
compress	O
relevant	O
information	O
into	O
smaller	O
Bi	O
-	O
LSTMs	O
,	O
while	O
Kuncoro	O
et	O
al	O
(	O
2019	O
)	O
took	O
a	O
similar	O
distillation	O
approach	O
.	O
While	O
fine	O
-	O
tuning	O
does	O
not	O
increase	O
prunability	O
,	O
task	O
-	O
specific	O
knowledge	O
might	O
be	O
extracted	O
from	O
BERT	B-MethodName
with	O
other	O
methods	O
.	O
Attention	O
Head	O
Pruning	O
previously	O
showed	O
redundancy	O
in	O
transformer	O
models	O
by	O
pruning	O
entire	O
attention	O
heads	O
.	O
Michel	O
et	O
al	O
(	O
2019	O
)	O
showed	O
that	O
after	O
fine	O
-	O
tuning	O
on	O
MNLI	B-DatasetName
,	O
up	O
to	O
40	O
%	O
of	O
attention	O
heads	O
can	O
be	O
pruned	O
from	O
BERT	B-MethodName
without	O
affecting	O
test	O
accuracy	B-MetricName
.	O
They	O
show	O
redundancy	O
in	O
BERT	B-MethodName
after	O
fine	O
-	O
tuning	O
on	O
a	O
single	O
downstream	O
task	O
;	O
in	O
contrast	O
,	O
our	O
work	O
emphasizes	O
the	O
interplay	O
between	O
compression	O
and	O
transfer	B-TaskName
learning	I-TaskName
to	O
many	O
tasks	O
,	O
pruning	O
both	O
before	O
and	O
after	O
finetuning	O
.	O
Also	O
,	O
magnitude	O
weight	O
pruning	O
allows	O
us	O
to	O
additionally	O
prune	O
the	O
feed	O
-	O
foward	O
networks	O
and	O
sub	O
-	O
word	B-TaskName
embeddings	I-TaskName
in	O
BERT	B-MethodName
(	O
not	O
just	O
selfattention	O
)	O
,	O
which	O
account	O
for	O
∼72	O
%	O
of	O
BERT	B-MethodName
's	O
total	O
memory	O
usage	O
.	O
We	O
suspect	O
that	O
attention	O
head	O
pruning	O
and	O
weight	O
pruning	O
remove	O
different	O
redundancies	O
from	O
BERT	B-MethodName
.	O
Figure	O
4	O
shows	O
that	O
weight	O
pruning	O
does	O
not	O
prune	O
any	O
specific	O
attention	O
head	O
much	O
more	O
than	O
the	O
pruning	O
rate	O
for	O
the	O
whole	O
model	O
.	O
It	O
is	O
not	O
clear	O
,	O
however	O
,	O
whether	O
weight	O
pruning	O
and	O
recovery	O
training	O
makes	O
attention	O
heads	O
less	O
prunable	O
by	O
distributing	O
functionality	O
to	O
unused	O
heads	O
.	O

We	O
've	O
shown	O
that	O
encoding	O
BERT	O
's	B-MethodName
inductive	O
bias	O
requires	O
many	O
more	O
weights	O
than	O
are	O
required	O
to	O
fit	O
downstream	O
data	O
.	O
Future	O
work	O
on	O
compressing	O
pre	O
-	O
trained	O
models	O
should	O
focus	O
on	O
maintaining	O
that	O
inductive	O
bias	O
and	O
quantifying	O
its	O
relevance	O
to	O
various	O
tasks	O
during	O
accuracy	O
/	B-MetricName
memory	O
trade	O
-	O
offs	O
.	O
For	O
magnitude	O
weight	O
pruning	O
,	O
we	O
've	O
shown	O
that	O
30	O
-	O
40	O
%	O
of	O
the	O
weights	O
do	O
not	O
encode	O
any	O
useful	O
inductive	O
bias	O
and	O
can	O
be	O
discarded	O
without	O
affecting	O
BERT	O
's	O
universality	B-MethodName
.	O
The	O
relevance	O
of	O
the	O
rest	O
of	O
the	O
weights	O
vary	O
from	O
task	O
to	O
task	O
,	O
and	O
fine	O
-	O
tuning	O
on	O
downstream	O
tasks	O
does	O
not	O
change	O
the	O
nature	O
of	O
this	O
trade	O
-	O
off	O
by	O
changing	O
which	O
weights	O
are	O
pruned	O
.	O
In	O
future	O
work	O
,	O
we	O
will	O
investigate	O
the	O
factors	O
that	O
influence	O
language	O
modeling	O
's	O
relevance	O
to	O
downstream	O
tasks	O
and	O
how	O
to	O
improve	O
compression	O
in	O
a	O
task	O
-	O
general	O
way	O
.	O
It	O
's	O
reasonable	O
to	O
believe	O
that	O
these	O
conclusions	O
will	O
generalize	O
to	O
other	O
pre	O
-	O
trained	O
language	O
models	O
such	O
as	O
Kermit	O
(	O
Chan	O
et	O
al	O
,	O
2019	O
)	O
,	O
XLNet	O
(	O
Yang	B-MethodName
et	O
al	O
,	O
2019	O
)	O
,	O
GPT	O
-	O
2	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
,	O
RoBERTa	O
(	O
Liu	B-MethodName
et	O
al	O
,	O
2019a	O
)	O
or	O
ELMO	O
(	O
Peters	B-MethodName
et	O
al	O
,	O
2018	O
)	O
.	O
All	O
of	O
these	O
learn	O
some	O
variant	O
of	O
language	O
modeling	O
,	O
and	O
most	O
use	O
Transformer	O
architectures	O
.	B-MethodName
While	O
it	O
remains	O
to	O
be	O
shown	O
in	O
future	O
work	O
,	O
viewing	O
pruning	O
as	O
architecture	O
search	O
implies	O
these	O
models	O
will	O
be	O
prunable	O
due	O
to	O
the	O
training	O
dynamics	O
inherent	O
to	O
neural	O
networks	O
.	O
1	O
.	O
Pre	O
-	O
training	O
losses	O
are	O
omitted	O
for	O
models	O
pruned	O
after	O
downstream	O
fine	O
-	O
tuning	O
because	O
it	O
is	O
not	O
clear	O
how	O
to	O
measure	O
their	O
performance	O
on	O
the	O
pre	O
-	O
training	O
task	O
in	O
a	O
fair	O
way	O
.	O
Figure	O
5	O
:	O
The	O
sum	O
of	O
weights	O
pruned	O
at	O
each	O
sparsity	O
level	O
for	O
one	O
shot	O
pruning	O
of	O
BERT	O
.	O
Given	B-MethodName
the	O
motivation	O
for	O
our	O
saliency	O
criterion	O
,	O
it	O
seems	O
strange	O
that	O
such	O
a	O
large	O
magnitude	O
of	O
weights	O
can	O
be	O
pruned	O
without	O
decreasing	O
accuracy	O
.	O
LR	B-MetricName
MNLI	O
QQP	O
QNL	B-DatasetName
SST	B-DatasetName
-	O
2	B-DatasetName
CoLA	O
2e	O
-	B-DatasetName
5	O
1.91	O
±	O
1.81	O
1.82	O
±	O
1.72	O
1.27	O
±	O
1.22	O
1.06	O
±	O
1.03	O
0.79	O
±	O
0.77	O
3e	O
-	O
5	O
2.68	O
±	O
2.51	O
2.56	O
±	O
2.40	O
1.79	O
±	O
1.69	O
1.54	O
±	O
1.47	O
1.06	O
±	O
1.03	O
4e	O
-	O
5	O
3.41	O
±	O
3.18	O
3.30	O
±	O
3.10	O
2.31	O
±	O
2.19	O
1.99	O
±	O
1.89	O
1.11	O
±	O
1.09	O
5e	O
-	O
5	O
4.12	O
±	O
3.83	O
4.02	O
±	O
3.74	O
2.77	O
±	O
2.62	O
2.38	O
±	O
2.29	O
1.47	O
±	O
1.43	O
Table	O
2	O
:	O
We	O
compute	O
the	O
magnitude	O
sorting	O
order	O
of	O
each	O
weight	O
before	O
and	O
after	O
downstream	O
fine	O
-	O
tuning	O
.	O
If	O
a	O
weight	O
's	O
original	O
position	O
is	O
59	O
/	O
100	O
before	O
fine	O
-	O
tuning	O
and	O
63	O
/	O
100	O
after	O
fine	O
-	O
tuning	O
,	O
then	O
that	O
weight	O
moved	O
4	O
%	O
in	O
the	O
sorting	O
order	O
.	O
We	O
then	O
list	O
the	O
average	O
movement	O
of	O
weights	O
in	O
each	O
model	O
,	O
along	O
with	O
the	O
standard	O
deviation	O
.	O
Sorting	O
order	O
changes	O
mostly	O
locally	O
across	O
tasks	O
:	O
a	O
weight	O
moves	O
,	O
on	O
average	O
,	O
0	O
-	O
4	B-DatasetName
%	O
away	O
from	O
its	O
starting	O
position	O
.	O
As	O
expected	O
,	O
larger	O
datasets	O
and	O
larger	O
learning	O
rates	O
have	O
more	O
movement	O
(	O
per	O
epoch	O
)	O
.	O
We	O
also	O
see	O
that	O
higher	O
magnitude	O
weights	O
are	O
more	O
stable	O
than	O
lower	O
weights	O
,	O
see	O
Figure	O
6	O
.	O
Figure	O
6	O
:	O
We	O
show	O
how	O
weight	O
sort	O
order	O
movements	O
are	O
distributed	O
during	O
fine	O
-	O
tuning	O
,	O
given	O
a	O
weight	O
's	O
starting	O
magnitude	O
.	O
We	O
see	O
that	O
higher	O
magnitude	O
weights	O
are	O
more	O
stable	O
than	O
lower	O
magnitude	O
weights	O
and	O
do	O
not	O
move	O
as	O
much	O
in	O
the	O
sort	O
order	O
.	O
This	O
plot	O
is	O
nearly	O
identical	O
for	O
every	O
model	O
and	O
learning	O
rate	O
,	B-HyperparameterName
so	I-HyperparameterName
we	O
only	O
show	O
it	O
once	O
.	O
Figure	O
7	O
:	O
A	O
heatmap	O
of	O
the	B-MethodName
weight	O
magnitudes	O
of	O
the	O
12	O
horizontally	O
stacked	O
self	O
-	O
attention	O
key	O
projection	O
matrices	O
for	O
layer	O
1	O
.	O
A	O
banding	O
pattern	O
can	O
be	O
seen	O
:	O
the	O
highest	O
values	O
of	O
the	O
matrix	O
tend	O
to	O
cluster	O
in	O
certain	O
attention	O
heads	O
.	O
This	O
pattern	O
appears	O
in	O
most	O
of	O
the	O
self	O
-	O
attention	O
parameter	O
matrices	O
,	O
but	O
it	O
does	O
not	O
cause	O
pruning	O
to	O
prune	O
one	O
head	O
more	O
than	O
another	O
.	O
However	O
,	O
it	O
may	O
prove	O
to	O
be	O
a	O
useful	O
heuristic	O
for	O
attention	O
head	O
pruning	O
,	O
which	O
would	O
not	O
require	O
making	O
many	O
passes	O
over	O
the	O
training	O
data	O
.	O
(	O
Right	O
)	O
A	O
heatmap	O
of	O
the	B-MethodName
weight	O
magnitudes	O
of	O
BERT	O
's	O
subword	B-MethodName
embeddings	O
.	O
Interestingly	O
,	O
pruning	O
BERT	O
embeddings	O
are	B-MethodName
more	O
interpretable	O
;	O
we	O
can	O
see	O
shorter	O
subwords	O
(	O
top	O
rows	O
)	O
have	O
smaller	O
magnitude	O
values	O
and	O
thus	O
will	O
be	O
pruned	O
earlier	O
than	O
other	O
subword	O
embeddings	O
.	O

QG	O
often	O
uses	O
standard	O
evaluation	O
metrics	O
from	O
text	B-TaskName
summarization	I-TaskName
and	O
machine	B-TaskName
translation	I-TaskName
(	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
,	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
,	O
METEOR	B-DatasetName
(	O
Banerjee	O
and	O
Lavie	O
,	O
2005	O
)	O
,	O
etc	O
.	O
)	O
.	O
However	O
,	O
such	O
metrics	O
do	O
not	O
provide	O
an	O
accurate	O
evaluation	O
for	O
QG	O
task	O
(	O
Novikova	O
et	O
al	O
,	O
2017	O
)	O
,	O
especially	O
when	O
the	O
input	O
passage	O
is	O
long	O
(	O
and	O
many	O
acceptable	O
questions	O
that	O
differ	O
from	O
the	O
gold	O
question	O
can	O
be	O
generated	O
)	O
.	O
Thus	O
,	O
to	O
alleviate	O
shortcomings	O
associated	O
with	O
n	O
-	O
gram	O
based	O
similarity	O
metrics	O
,	O
we	O
use	O
BLEURT	O
(	O
Sellam	O
et	O
al	O
,	O
2020	O
)	O
(	O
BLEURT	O
-	O
20	O
)	O
,	O
which	O
is	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
evaluation	O
metric	O
in	O
WMT	O
Metrics	O
shared	O
task	O
.	O
BLEURT	O
is	O
a	O
BERT	B-MethodName
-	O
based	O
model	O
that	O
uses	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
to	O
evaluate	O
a	O
generated	O
text	O
by	O
giving	O
it	O
a	O
value	O
mostly	O
between	O
0.0	O
and	O
1.0	O
.	O
In	O
our	O
experiments	O
,	O
we	O
consider	O
BLEURT	O
as	O
the	O
main	O
metric	O
for	O
the	O
evaluation	O
.	O
We	O
also	O
report	O
standard	O
MT	O
metric	O
BLEU	B-MetricName
(	O
1	O
-	O
4	O
ngrams	O
)	O
,	O
and	O
perform	O
an	O
additional	O
manual	O
evaluation	O
.	O
Manual	O
evaluation	O
is	O
required	O
in	O
our	O
collected	O
dataset	O
,	O
because	O
teachers	O
wrote	O
a	O
single	O
question	O
per	O
skill	O
for	O
a	O
given	O
story	O
,	O
where	O
the	O
model	O
might	O
generate	O
other	O
possible	O
questions	O
for	O
the	O
same	O
skill	O
.	O

We	O
fine	O
-	O
tune	O
a	O
T5	B-MethodName
model	O
(	O
t5	B-MethodName
-	O
base	O
from	O
Hugging	O
-	O
Face	O
library	O
)	O
using	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8	O
and	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e−4	O
.	O
We	O
use	O
a	O
maximum	O
sequence	O
length	O
of	O
512	O
for	O
the	O
encoder	O
,	O
and	O
128	O
for	O
the	O
decoder	O
7	O
.	O
We	O
tested	O
the	O
T5	B-MethodName
-	O
large	O
model	O
,	O
but	O
we	O
did	O
not	O
notice	O
any	O
improvements	O
considering	O
BLEURT	O
metric	O
.	O
We	O
train	O
all	O
models	O
for	O
a	O
maximum	O
of	O
ten	O
epochs	O
with	O
an	O
early	B-MethodName
stopping	I-MethodName
value	O
of	O
1	O
(	O
patience	O
)	O
based	O
on	O
the	O
validation	O
loss	B-MetricName
.	O
We	O
use	O
a	O
single	O
NVIDIA	O
TITAN	B-DatasetName
RTX	O
with	O
24	O
G	O
RAM	B-MethodName
.	O
For	O
HTA	O
,	O
we	O
validate	O
on	O
a	O
combined	O
version	O
of	O
the	O
validation	O
sets	O
from	O
both	O
datasets	O
(	O
SQuAD	B-DatasetName
and	O
CosmosQA	B-DatasetName
)	O
.	O
Regarding	O
the	O
collected	O
dataset	O
validation	O
set	O
,	O
we	O
use	O
stratified	O
sampling	O
:	O
we	O
took	O
a	O
random	O
10	O
%	O
of	O
stories	O
from	O
each	O
skill	O
since	O
the	O
dataset	O
is	O
unbalanced	O
.	O
We	O
apply	O
the	O
same	O
strategy	O
with	O
the	O
test	O
set	O
but	O
with	O
a	O
value	O
of	O
20	O
%	O
.	O

Table	O
2	O
presents	O
the	O
results	O
of	O
the	O
proposed	O
HTA	O
-	O
WTA	O
method	O
with	O
the	O
baselines	O
.	O
We	O
can	O
see	O
that	O
out	O
of	O
the	O
baselines	O
,	O
T5	B-MethodName
-	O
WTA	O
performs	O
best	O
in	O
terms	O
of	O
BLEURT	O
score	O
(	O
32.96	O
%	O
)	O
,	O
followed	O
by	O
NQG	O
-	O
Max	O
with	O
a	O
value	O
of	O
31.78	O
%	O
.	O
Given	O
its	O
high	O
BLEURT	O
score	O
,	O
it	O
is	O
surprising	O
that	O
T5	B-MethodName
-	O
WTA	O
model	O
has	O
low	O
BLEU	B-MetricName
-	O
4	O
.	O
This	O
implies	O
that	O
the	O
generated	O
questions	O
use	O
rich	O
vocabulary	O
,	O
making	O
them	O
different	O
from	O
the	O
gold	O
in	O
terms	O
of	O
overlapping	O
ngrams	O
,	O
but	O
semantically	O
similar	O
leading	O
to	O
higher	O
BLEURT	O
score	O
.	O
As	O
shown	O
in	O
the	O
table	O
,	O
HTA	O
-	O
WTA	O
's	O
BLEURT	O
score	O
outperforms	O
all	O
of	O
the	O
previous	O
QG	O
models	O
by	O
a	O
noticeable	O
margin	O
,	O
showing	O
that	O
including	O
the	O
skill	O
name	O
information	O
plays	O
an	O
important	O
role	O
in	O
generating	O
the	O
intended	O
questions	O
.	O
Also	O
,	O
training	O
on	O
more	O
QG	O
datasets	O
improves	O
the	O
performance	O
.	O
We	O
also	O
noted	O
that	O
the	O
CGC	O
-	O
QG	O
model	O
achieves	O
a	O
higher	O
BLEU	B-MetricName
-	O
1	O
than	O
our	O
HTA	O
-	O
WTA	O
model	O
.	O
We	O
argue	O
that	O
this	O
is	O
because	O
the	O
Clue	O
Words	O
Prediction	O
Module	O
learns	O
important	O
cues	O
,	O
increasing	O
the	O
uni	O
-	O
gram	O
overlap	O
with	O
the	O
gold	O
references	O
(	O
BLEU	B-MetricName
-	O
1	O
)	O
.	O
Regarding	O
the	O
generated	O
questions	O
type	O
,	O
in	O
Table	O
3	O
we	O
show	O
the	O
performance	O
of	O
the	O
T5	B-MethodName
-	O
based	O
models	O
per	O
question	O
type	O
(	O
inferential	O
and	O
literal	O
)	O
.	O
Though	O
One	O
-	O
Step	O
and	O
HTA	O
-	O
WTA	O
models	O
were	O
trained	O
on	O
the	O
same	O
amount	O
of	O
data	O
,	O
the	O
results	O
show	O
that	O
HTA	O
-	O
WTA	O
model	O
clearly	O
performs	O
better	O
than	O
the	O
One	O
-	O
Step	O
model	O
,	O
especially	O
on	O
inferential	O
questions	O
.	O
We	O
see	O
a	O
similar	O
scenario	O
when	O
comparing	O
One	O
-	O
Step	O
and	O
T5	B-MethodName
-	O
WTA	O
models	O
,	O
yet	O
,	O
the	O
gap	O
is	O
smaller	O
.	O
In	O
general	O
,	O
we	O
can	O
notice	O
that	O
the	O
performance	O
gaps	O
for	O
the	O
inferential	O
questions	O
are	O
larger	O
than	O
the	O
literal	O
ones	O
.	O
Thus	O
,	O
we	O
can	O
conclude	O
that	O
HTA	O
-	O
WTA	O
is	O
generating	O
more	O
correct	O
inferential	O
questions	O
,	O
which	O
is	O
challenging	O
.	O
This	O
experiment	O
concludes	O
that	O
transformers	O
-	O
based	O
models	O
are	O
capable	O
of	O
asking	O
questions	O
beyond	O
the	O
literal	O
meaning	O
of	O
the	O
text	O
.	O
This	O
confirms	O
what	O
was	O
shown	O
by	O
Liu	O
et	O
al	O
(	O
2021a	O
)	O
regarding	O
the	O
skills	O
that	O
language	O
models	O
can	O
acquire	O
.	O
Additionally	O
,	O
as	O
some	O
training	O
questions	O
directly	O
quote	O
text	O
from	O
the	O
given	O
story	O
.	O
The	O
T5	B-MethodName
model	O
was	O
able	O
to	O
learn	O
how	O
to	O
quote	O
the	O
proper	O
segment	O
of	O
the	O
passage	O
when	O
generating	O
questions	O
.	O
The	O
One	O
-	O
Step	O
model	O
performs	O
similarly	O
to	O
the	O
baselines	O
,	O
although	O
it	O
has	O
been	O
trained	O
using	O
the	O
T5	B-MethodName
model	O
and	O
on	O
all	O
three	O
datasets	O
.	O
This	O
may	O
be	O
due	O
to	O
the	O
fact	O
that	O
we	O
did	O
not	O
include	O
the	O
skill	O
name	O
in	O
the	O
encoder	O
,	O
which	O
guides	O
the	O
model	O
to	O
generate	O
skill	O
related	O
questions	O
.	O
To	O
better	O
understand	O
the	O
differences	O
between	O
the	O
outputs	O
of	O
One	O
-	O
Step	O
and	O
HTA	O
-	O
WTA	O
models	O
,	O
we	O
used	O
human	O
evaluation	O
.	O
This	O
evaluation	O
is	O
to	O
assess	O
the	O
quality	O
of	O
the	O
generated	O
question	O
in	O
terms	O
of	O
1	O
.	O
Answerability	O
(	O
Ay	O
)	O
,	O
2	O
.	O
Fluency	O
(	O
Fy	O
)	O
,	O
and	O
3	O
.	O
Grammaticality	O
(	O
Gy	O
)	O
categories	O
,	O
following	O
Harrison	O
and	O
Walker	O
(	O
2018	O
)	O
;	O
Azevedo	O
et	O
al	O
(	O
2020	O
)	O
.	O
We	O
include	O
these	O
three	O
criteria	O
as	O
questions	O
may	O
have	O
high	O
Fluency	O
and	O
Grammaticality	O
scores	O
,	O
but	O
not	O
be	O
answerable	O
.	O
We	O
select	O
a	O
sample	O
of	O
110	O
story	O
-	O
question	O
pairs	O
from	O
the	O
test	O
dataset	O
,	O
for	O
both	O
models	O
.	O
Then	O
,	O
we	O
perform	O
a	O
human	O
evaluation	O
using	O
crowdworkers	O
on	O
Amazon	O
Mechanical	O
Turk	O
.	O
We	O
use	O
a	O
"	O
master	O
"	O
qualification	O
criteria	O
to	O
restrict	O
the	O
participation	O
of	O
workers	O
in	O
our	O
evaluation	O
study	O
to	O
those	O
who	O
have	O
a	O
high	O
historical	O
HIT	O
accuracy	B-MetricName
,	O
and	O
workers	O
are	O
required	O
to	O
be	O
located	O
in	O
an	O
English	O
speaking	O
country	O
.	O
Each	O
HIT	O
was	O
answered	O
by	O
three	O
workers	O
.	O
Each	O
worker	O
needs	O
reads	O
the	O
story	O
,	O
and	O
provides	O
ratings	O
(	O
1	O
-	O
5	O
,	O
low	O
to	O
high	O
)	O
for	O
the	O
generated	O
questions	O
,	O
and	O
the	O
three	O
criteria	O
.	O
Table	O
4	O
shows	O
the	O
average	O
rating	O
assigned	O
by	O
the	O
workers	O
for	O
the	O
3	O
criteria	O
.	O
Originally	O
,	O
we	O
hypothesized	O
that	O
adding	O
the	O
skill	O
name	O
to	O
the	O
input	O
would	O
force	O
the	O
model	O
to	O
formulate	O
a	O
specific	O
SBRCS	O
question	O
,	O
even	O
if	O
it	O
is	O
not	O
applicable	O
to	O
the	O
current	O
passage	O
.	O
Omitting	O
the	O
skill	O
name	O
may	O
allow	O
the	O
model	O
score	O
high	O
values	O
as	O
it	O
has	O
been	O
left	O
to	O
decide	O
the	O
question	O
.	O
The	O
results	O
show	O
that	O
both	O
models	O
are	O
similar	O
in	O
terms	O
of	O
the	O
given	O
categories	O
,	O
except	O
that	O
HTA	O
-	O
WTA	O
performs	O
slightly	O
better	O
in	O
all	O
of	O
the	O
three	O
categories	O
.	O
However	O
,	O
these	O
results	O
refute	O
our	O
claim	O
and	O
show	O
that	O
adding	O
the	O
skill	O
information	O
makes	O
the	O
model	O
generates	O
slightly	O
better	O
questions	O
in	O
terms	O
of	O
quality	O
.	O
In	O
Section	O
A.4	O
,	O
we	O
present	O
an	O
ablation	O
test	O
and	O
discuss	O
some	O
causes	O
of	O
errors	O
in	O
generating	O
questions	O
.	O
Impact	O
of	O
Skill	O
Name	O
Token	O
.	O
In	O
order	O
to	O
quantify	O
the	O
impact	O
of	O
skill	O
name	O
in	O
the	O
input	O
,	O
we	O
do	O
another	O
human	O
manual	O
evaluation	O
to	O
assess	O
how	O
beneficial	O
the	O
skill	O
name	O
token	O
is	O
when	O
we	O
add	O
it	O
to	O
the	O
HTA	O
-	O
WTA	O
model	O
.	O
Thus	O
,	O
we	O
ask	O
two	O
professional	O
persons	O
who	O
were	O
involved	O
in	O
the	O
annotation	O
process	O
to	O
assign	O
skill	O
names	O
to	O
the	O
generated	O
questions	O
of	O
both	O
One	O
-	O
Step	O
and	O
HTA	O
-	O
WTA	O
models	O
.	O
We	O
selected	O
these	O
models	O
as	O
they	O
were	O
trained	O
on	O
the	O
same	O
amount	O
of	O
data	O
;	O
the	O
only	O
difference	O
between	O
them	O
is	O
that	O
the	O
HTA	O
-	O
WTA	O
model	O
uses	O
the	O
skill	O
name	O
token	O
.	O
We	O
utilize	O
the	O
same	O
question	O
sample	O
that	O
was	O
used	O
in	O
the	O
previous	O
human	O
evaluation	O
experiment	O
.	O
Few	O
annotation	O
conflicts	O
were	O
found	O
and	O
were	O
solved	O
after	O
a	O
discussion	O
.	O
We	O
evaluate	O
the	O
results	O
using	O
accuracy	B-MetricName
(	O
see	O
Table	O
4	O
)	O
.	O
The	O
result	O
for	O
One	O
-	O
Step	O
model	O
is	O
0.16	O
,	O
and	O
0.8	O
for	O
HTA	O
-	O
WTA	O
model	O
.	O
We	O
can	O
clearly	O
see	O
a	O
large	O
gap	O
in	O
accuracy	B-MetricName
between	O
both	O
models	O
,	O
and	O
this	O
becomes	O
clear	O
with	O
the	O
skills	O
that	O
have	O
a	O
low	O
number	O
of	O
instances	O
in	O
the	O
dataset	O
(	O
e.g.	O
Figurative	O
Language	O
,	O
Predicting	O
,	O
etc	O
.	O
)	O
.	O
This	O
result	O
shows	O
that	O
,	O
in	O
addition	O
to	O
using	O
the	O
skill	O
name	O
token	O
to	O
control	O
the	O
skill	O
of	O
the	O
generated	O
questions	O
,	O
it	O
helps	O
the	O
model	O
to	O
learn	O
the	O
underrepresented	O
skills	O
in	O
the	O
dataset	O
.	O
Table	O
6	O
in	O
Appendix	O
A.5	O
presents	O
the	O
F1	B-MetricName
scores	O
per	O
skill	O
name	O
.	O
We	O
also	O
notice	O
that	O
HTA	O
-	O
WTA	O
model	O
performed	O
perfectly	O
on	O
the	O
given	O
sample	O
of	O
Predicting	O
and	O
Figurative	O
Language	O
(	O
F1	B-MetricName
is	O
1.0	O
for	O
each	O
skill	O
)	O
.	O
This	O
is	O
an	O
interesting	O
result	O
given	O
that	O
the	O
type	O
of	O
the	O
questions	O
for	O
both	O
skills	O
is	O
inferential	O
,	O
which	O
is	O
harder	O
to	O
generate	O
compared	O
to	O
the	O
literal	O
questions	O
.	O
Few	O
-	O
Shot	O
Generation	O
.	O
The	O
process	O
of	O
manually	O
writing	O
questions	O
to	O
assess	O
humans	O
SBRCS	O
is	O
difficult	O
.	O
In	O
some	O
stories	O
,	O
professional	O
writers	O
find	O
obstacles	O
in	O
writing	O
questions	O
for	O
some	O
skills	O
as	O
those	O
skills	O
require	O
high	O
attention	O
and	O
advanced	O
reasoning	O
skills	O
to	O
be	O
written	O
.	O
We	O
can	O
see	O
that	O
in	O
our	O
own	O
dataset	O
,	O
as	O
some	O
skills	O
have	O
fewer	O
questions	O
(	O
e.g.	O
Predicting	O
,	O
Visualizing	O
,	O
etc	O
.	O
)	O
.	O
Thus	O
,	O
in	O
this	O
experiment	O
,	O
we	O
evaluate	O
the	O
performance	O
of	O
HTA	O
-	O
WTA	O
model	O
when	O
we	O
inject	O
a	O
low	O
percentage	O
of	O
the	O
skills	O
'	O
instances	O
into	O
the	O
training	O
set	O
.	O
This	O
experiment	O
will	O
simulate	O
the	O
case	O
when	O
training	O
a	O
model	O
on	O
a	O
dataset	O
that	O
contains	O
few	O
skills	O
'	O
instances	O
.	O
We	O
use	O
the	O
stratified	O
sampling	O
technique	O
when	O
sampling	O
fewer	O
instances	O
from	O
the	O
collected	O
dataset	O
.	O
Figure	O
3	O
shows	O
that	O
injecting	O
only	O
10	O
%	O
of	O
the	O
data	O
led	O
to	O
a	O
boost	O
in	O
performance	O
of	O
5.99	O
(	O
BLEURT	O
)	O
.	O
The	O
result	O
at	O
10	O
%	O
(	O
33.21	O
%	O
)	O
exceeds	O
the	O
results	O
of	O
most	O
of	O
the	O
baselines	O
and	O
is	O
higher	O
than	O
T5	B-MethodName
-	O
WTA	O
and	O
NQG	O
-	O
MAX	O
models	O
when	O
trained	O
on	O
all	O
the	O
datasets	O
(	O
see	O
Table	O
2	O
)	O
.	O
In	O
Table	O
A.6	O
in	O
the	O
appendix	O
,	O
we	O
present	O
the	O
results	O
considering	O
other	O
models	O
and	O
metrics	O
.	O
In	O
most	O
cases	O
,	O
the	O
performance	O
gradually	O
improves	O
as	O
data	O
grows	O
.	O
We	O
notice	O
a	O
small	O
drop	O
when	O
we	O
move	O
from	O
10	O
%	O
to	O
30	O
%	O
.	O
This	O
behaviour	O
was	O
previously	O
reported	O
by	O
Stappen	O
et	O
al	O
(	O
2020	O
)	O
.	O
Further	O
research	O
is	O
needed	O
to	O
investigate	O
the	O
causes	O
of	O
this	O
behaviour	O
.	O

Ablation	O
Test	O
.	O
The	O
results	O
of	O
our	O
experiments	O
confirmed	O
the	O
importance	O
of	O
both	O
the	O
skill	O
name	O
token	O
and	O
the	O
two	O
-	O
steps	O
training	O
method	O
.	O
To	O
quantify	O
the	O
impact	O
of	O
including	O
the	O
skill	O
name	O
token	O
,	O
we	O
run	O
T5	B-MethodName
-	O
WTA	O
without	O
including	O
the	O
skill	O
name	O
token	O
(	O
T5	B-MethodName
-	O
WTA	O
-	O
unskilled	O
)	O
.	O
We	O
compare	O
the	O
T5	B-MethodName
-	O
WTAunskilled	O
to	O
the	O
One	O
-	O
Step	O
model	O
;	O
the	O
only	O
difference	O
between	O
these	O
models	O
is	O
that	O
One	O
-	O
Step	O
model	O
includes	O
SQuAD	B-DatasetName
and	O
CosmosQA	B-DatasetName
datasets	O
in	O
the	O
training	O
data	O
.	O
The	O
ablation	O
test	O
results	O
in	O
Table	O
5	O
shows	O
that	O
the	O
skill	O
name	O
token	O
and	O
the	O
additional	O
training	O
data	O
both	O
increase	O
model	O
performance	O
.	O
T5	B-MethodName
-	O
WTA	O
-	O
unskilled	O
BLEURT	O
performance	O
is	O
lower	O
than	O
the	O
BLEURT	O
scores	O
of	O
the	O
other	O
two	O
models	O
.	O
Error	B-MetricName
Analysis	O
.	O
Here	O
we	O
are	O
interested	O
in	O
further	O
understanding	O
the	O
HTA	O
-	O
WTA	O
model	O
's	O
performance	O
.	O
We	O
manually	O
examined	O
several	O
generated	O
questions	O
to	O
understand	O
the	O
sources	O
of	O
its	O
errors	O
.	O
Given	O
the	O
unbalanced	O
status	O
of	O
the	O
dataset	O
,	O
we	O
found	O
that	O
the	O
model	O
does	O
not	O
always	O
generate	O
an	O
appropriate	O
question	O
for	O
a	O
given	O
skill	O
name	O
,	O
especially	O
when	O
that	O
skill	O
is	O
underrepresented	O
in	O
the	O
data	O
(	O
e.g.	O
Visualizing	O
,	O
Figurative	O
Language	O
,	O
etc	O
.	O
)	O
.	O
In	O
some	O
cases	O
,	O
the	O
model	O
learned	O
the	O
style	O
of	O
the	O
skill	O
's	O
questions	O
,	O
but	O
in	O
the	O
given	O
context	O
,	O
the	O
generated	O
question	O
could	O
not	O
be	O
answered	O
.	O
As	O
an	O
example	O
,	O
the	O
following	O
generated	O
figurative	O
language	O
question	O
quoted	O
a	O
sentence	O
from	O
a	O
story	O
about	O
the	O
space	O
.	O
The	O
sentence	O
is	O
an	O
event	O
in	O
the	O
story	O
and	O
not	O
a	O
figurative	O
language	O
:	O
Which	O
figurative	O
language	O
technique	O
is	O
being	O
used	O
in	O
the	O
phrase	O
"	O
The	O
first	O
safe	O
trip	O
into	O
space	O
"	O
?	O
This	O
happens	O
even	O
for	O
very	O
common	O
skill	O
categories	O
,	O
again	O
due	O
to	O
the	O
difficulty	O
(	O
or	O
even	O
impossibility	O
)	O
of	O
generating	O
questions	O
for	O
some	O
skill	O
and	O
story	O
pairs	O
.	O
The	O
other	O
kind	O
of	O
error	O
is	O
the	O
subjectivity	O
in	O
selecting	O
the	O
"	O
correct	O
"	O
words	O
from	O
the	O
story	O
.	O
For	O
instance	O
,	O
giving	O
the	O
following	O
Vocabulary	O
question	O
from	O
the	O
dataset	O
:	O
What	O
is	O
the	O
correct	O
definition	O
of	O
the	O
word	O
"	O
decoy	O
"	O
as	O
it	O
is	O
used	O
in	O
the	O
story	O
?	O
For	O
this	O
kind	O
of	O
question	O
,	O
annotators	O
chose	O
words	O
that	O
can	O
have	O
multiple	O
meanings	O
,	O
some	O
of	O
which	O
may	O
be	O
unfamiliar	O
to	O
school	O
children	O
.	O
The	O
process	O
of	O
choosing	O
those	O
words	O
is	O
subjective	O
.	O
Although	O
both	O
annotators	O
agreed	O
on	O
the	O
word	O
in	O
the	O
previous	O
example	O
,	O
the	O
model	O
chose	O
to	O
select	O
another	O
word	O
from	O
the	O
story	O
(	O
"	O
panting	O
"	O
)	O
.	O
In	O
other	O
cases	O
,	O
the	O
question	O
asks	O
about	O
the	O
definition	O
of	O
a	O
word	O
within	O
a	O
sentence	O
from	O
the	O
story	O
(	O
e.g.	O
What	O
is	O
the	O
meaning	O
of	O
"	O
word	O
"	O
as	O
it	O
is	O
used	O
in	O
this	O
sentence	O
:	O
"	O
quoted	O
sentence	O
"	O
)	O
.	O
We	O
noted	O
that	O
when	O
the	O
model	O
generated	O
the	O
question	O
,	O
it	O
selects	O
the	O
correct	O
word	O
but	O
sometimes	O
used	O
a	O
randomly	O
quoted	O
sentence	O
from	O
the	O
story	O
that	O
did	O
n't	O
contain	O
the	O
word	O
.	O

In	O
Table	O
7	O
,	O
we	O
show	O
the	O
few	O
-	O
shot	O
experiment	O
's	O
results	O
considering	O
both	O
scoring	O
metrics	O
(	O
BLEU	B-MetricName
,	O
and	O
BLUERT	O
)	O
.	O
We	O
do	O
not	O
experiment	O
with	O
One	O
-	O
Step	O
model	O
as	O
we	O
need	O
to	O
sample	O
SQuAD	B-DatasetName
and	O
Cos	O
-	O
mosQA	O
datasets	O
when	O
we	O
sample	O
the	O
collected	O
data	O
;	O
it	O
is	O
hard	O
to	O
set	O
up	O
a	O
fair	O
comparison	O
here	O
as	O
,	O
for	O
instance	O
,	O
sampling	O
10	O
%	O
of	O
SQuAD	B-DatasetName
dataset	O
is	O
larger	O
than	O
the	O
whole	O
collected	O
dataset	O
.	O

Text	B-TaskName
Generation	I-TaskName
Model	O
Text	B-TaskName
generation	I-TaskName
models	O
learn	O
to	O
generate	O
a	O
sentence	O
Y	O
=	O
(	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
T	O
)	O
of	O
length	O
T	O
,	O
possibly	O
conditioned	O
on	O
some	O
context	O
X.	O
Here	O
each	O
y	O
t	O
is	O
a	O
token	O
from	O
vocabulary	O
A.	O
Starting	O
from	O
the	O
initial	O
state	O
s	O
0	B-DatasetName
,	O
a	O
recurrent	O
neural	O
network	O
(	O
RNN	O
)	O
produces	O
a	O
sequence	O
of	O
states	O
(	O
s	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
T	O
)	O
given	O
an	O
input	O
sentence	O
-	O
feature	O
representation	O
(	O
e	O
(	O
y	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
e	O
(	O
y	O
T	O
)	O
)	O
,	O
where	O
e	O
(	O
)	O
denotes	O
a	O
word	O
embedding	O
function	O
mapping	O
a	O
token	O
to	O
its	O
ddimensional	O
feature	O
representation	O
.	O
The	O
states	O
are	O
recursively	O
updated	O
with	O
a	O
function	O
known	O
as	O
the	O
cell	O
:	O
s	O
t	O
=	O
h	O
(	O
s	O
t−1	O
,	O
e	O
(	O
y	O
t	O
)	O
)	O
.	O
One	O
typically	O
assigns	O
the	O
following	O
probability	O
to	O
an	O
observation	O
y	O
at	O
location	O
t	O
:	O
p	O
(	O
y	O
|	O
Y	O
<	O
t	O
)	O
=	O
[	O
softmax	B-MethodName
(	O
g	O
(	O
s	O
t	O
)	O
)	O
]	O
y	O
.	O
Together	O
(	O
g	O
,	O
h	O
)	O
specifies	O
a	O
probabilistic	O
model	O
π	O
,	O
i.e.	O
,	O
log	O
π	O
(	O
Y	O
)	O
=	O
t	O
log	O
p	O
(	O
y	O
t	O
|	O
Y	O
<	O
t	O
)	O
.	O
(	O
1	O
)	O
To	O
train	O
the	O
model	O
π	O
,	O
one	O
typically	O
uses	O
maximum	O
likelihood	O
estimation	O
(	O
MLE	O
)	O
,	O
via	O
minimizing	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
,	O
i.e.	O
,	O
J	O
MLE	O
(	O
π	O
)	O
=	O
−E	O
[	O
log	O
π	O
(	O
Y	O
)	O
]	O
.	O
In	O
order	O
to	O
generate	O
sentence	O
Y	O
s	O
from	O
a	O
(	O
trained	O
)	O
model	O
,	O
one	O
iteratively	O
applies	O
the	O
following	O
operations	O
:	O
y	O
s	O
t+1	O
∼	O
Multi	O
(	O
1	O
,	O
softmax	B-MethodName
(	O
g	O
(	O
s	O
t	O
)	O
)	O
)	O
,	O
(	O
2	O
)	O
s	O
t	O
=	O
h	O
(	O
s	O
t−1	O
,	O
e	O
(	O
y	O
s	O
t	O
)	O
)	O
,	O
where	O
Multi	O
(	O
1	O
,	O
)	O
denotes	O
one	O
draw	O
from	O
a	O
multinomial	O
distribution	O
.	O
Model	O
-	O
Based	O
Imitation	B-TaskName
Learning	I-TaskName
Text	B-TaskName
generation	I-TaskName
can	O
be	O
considered	O
as	O
an	O
RL	O
problem	O
with	O
a	O
large	O
number	O
of	O
discrete	O
actions	O
,	O
deterministic	O
transitions	O
,	O
and	O
deterministic	O
terminal	O
rewards	O
.	O
It	O
can	O
be	O
formulated	O
as	O
a	O
Markov	O
decision	O
process	O
(	O
MDP	O
)	O
M	O
=	O
S	O
,	O
A	O
,	O
P	O
,	O
r	O
,	O
γ	B-HyperparameterName
,	O
where	O
S	O
is	O
the	O
state	O
space	O
,	O
A	O
is	O
the	O
action	O
space	O
,	O
P	O
is	O
the	O
deterministic	O
environment	O
dynamics	O
,	O
r	O
(	O
s	O
,	O
y	O
)	O
is	O
a	O
reward	O
function	O
,	O
and	O
γ	B-HyperparameterName
(	O
0	B-DatasetName
,	O
1	O
)	O
is	O
the	O
discrete	O
-	O
time	O
discount	O
factor	O
.	O
The	O
policy	O
π	O
φ	O
,	O
parameterized	O
by	O
φ	O
,	O
maps	O
each	O
state	O
s	O
S	O
to	O
a	O
probability	O
distribution	O
over	O
A.	O
The	O
objective	O
is	O
to	O
maximize	O
the	O
expected	O
reward	O
:	O
J	O
(	O
π	O
)	O
=	O
t=1	O
E	O
P	O
,	O
π	O
γ	B-HyperparameterName
t−1	O
r	O
(	O
s	O
t	O
,	O
y	O
t	O
)	O
.	O
In	O
model	O
-	O
based	O
imitation	B-TaskName
learning	I-TaskName
(	O
Baram	O
et	O
al	O
,	O
2017	O
;	O
Cheng	O
et	O
al	O
,	O
2019	O
)	O
,	O
a	O
model	O
is	O
built	O
to	O
make	O
predictions	O
for	O
future	O
state	O
s	O
t+	O
t	O
conditioned	O
on	O
the	O
current	O
state	O
1	O
,	O
which	O
can	O
be	O
used	O
for	O
action	O
selection	O
,	O
e.g.	O
,	O
next	O
-	O
token	O
generation	O
.	O
This	O
model	O
is	O
typically	O
a	O
discrete	O
-	O
time	O
system	O
,	O
taking	O
the	O
current	O
state	O
-	O
action	O
pair	O
(	O
s	O
t	O
,	O
y	O
t	O
)	O
as	O
input	O
,	O
and	O
outputting	O
an	O
estimate	O
of	O
the	O
future	O
state	O
s	O
t+	O
t	O
at	O
time	O
t	O
+	O
t.	O
At	O
each	O
step	O
t	O
,	O
y	O
t	O
is	O
chosen	O
based	O
on	O
the	O
model	O
,	O
and	O
the	O
model	O
will	O
re	O
-	O
plan	O
with	O
the	O
updated	O
information	O
from	O
the	O
dynamics	O
.	O
This	O
control	O
scheme	O
is	O
different	O
from	O
a	O
standard	O
model	O
-	O
based	O
method	O
,	O
and	O
is	O
referred	O
to	O
as	O
model	O
-	O
predictive	O
control	O
(	O
MPC	O
)	O
(	O
Nagabandi	O
et	O
al	O
,	O
2017	O
)	O
.	O
Note	O
that	O
in	O
our	O
setting	O
,	O
the	O
state	O
in	O
RL	O
typically	O
corresponds	O
to	O
the	O
current	O
generated	O
sentences	O
Y	O
1	O
,	O
...	O
,	O
t	O
instead	O
of	O
the	O
RNN	O
state	O
of	O
generator	O
(	O
decoder	O
)	O
.	O

The	O
guider	O
network	O
,	O
implemented	O
as	O
an	O
RNN	O
with	O
LSTM	B-MethodName
units	O
,	O
is	O
adopted	O
to	O
model	O
environment	O
dynamics	O
to	O
assist	O
text	B-TaskName
generation	I-TaskName
.	O
The	O
idea	O
is	O
to	O
train	O
a	O
guider	O
network	O
such	O
that	O
its	O
predicted	O
sentence	O
features	O
at	O
each	O
time	O
step	O
are	O
used	O
to	O
assist	O
next	O
-	O
word	O
generation	O
and	O
construct	O
intermediate	O
rewards	O
,	O
which	O
in	O
turn	O
are	O
used	O
to	O
optimize	O
the	O
sentence	O
generator	O
.	O
Denote	O
the	O
guider	O
network	O
as	O
G	O
ψ	O
(	O
s	O
G	O
t−1	O
,	O
f	O
t	O
)	O
,	O
with	O
parameters	O
ψ	O
and	O
input	O
arguments	O
(	O
s	O
G	O
t−1	O
,	O
f	O
t	O
)	O
at	O
time	O
t	O
,	O
to	O
explicitly	O
write	O
out	O
the	O
dependency	O
on	O
the	O
guider	O
network	O
latent	O
state	O
s	O
G	O
t−1	O
from	O
the	O
previous	O
time	O
step	O
.	O
Here	O
f	O
t	O
is	O
the	O
input	O
to	O
the	O
LSTM	B-MethodName
guider	O
,	O
which	O
represents	O
the	O
feature	O
of	O
the	O
current	O
generated	O
sentence	O
extracted	O
1	O
t	O
>	O
1	O
;	O
the	O
model	O
predicts	O
future	O
states	O
based	O
on	O
the	O
collected	O
trajectories	O
.	O
f	O
G	O
t	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
z	O
U	O
c	O
7	O
C	O
4	O
R	O
M	O
y	O
k	O
u	O
C	O
+	O
q	O
Q	O
K	O
u	O
z	O
B	O
Z	O
q	O
w	O
S	O
L	O
z	O
z	O
0	B-DatasetName
=	O
"	O
>	O
A	O
A	O
A	O
B	O
/	O
n	O
i	O
c	O
b	O
V	O
B	O
P	O
S	O
8	O
M	O
w	O
H	O
E	O
3	O
n	O
v	O
z	O
n	O
/	O
V	O
c	O
W	O
T	O
l	O
+	O
A	O
Q	O
P	O
I	O
1	O
2	O
D	O
v	O
Q	O
4	O
8	O
K	O
D	O
H	O
C	O
W	O
4	O
O	O
t	O
l	O
r	O
S	O
N	O
N	O
3	O
C	O
0	B-DatasetName
q	O
Q	O
k	O
q	O
T	O
B	O
K	O
w	O
a	O
/	O
i	O
x	O
Y	O
M	O
i	O
X	O
v	O
0	B-DatasetName
c	O
3	O
v	O
w	O
2	O
p	O
l	O
s	O
P	O
u	O
v	O
k	O
g	O
5	O
P	O
H	O
e	O
7	O
0	B-DatasetName
d	O
e	O
X	O
p	O
A	O
w	O
q	O
r	O
T	O
j	O
f	O
F	O
u	O
V	O
l	O
d	O
W	O
1	O
9	O
Y	O
3	O
q	O
Z	O
m	O
1	O
r	O
e	O
2	O
d	O
3	O
z	O
9	O
4	O
/	O
6	O
C	O
m	O
R	O
S	O
k	O
y	O
6	O
W	O
D	O
A	O
h	O
+	O
w	O
F	O
S	O
h	O
F	O
F	O
O	O
u	O
p	O
p	O
q	O
R	O
v	O
q	O
J	O
J	O
C	O
g	O
O	O
G	O
L	O
k	O
P	O
J	O
l	O
e	O
F	O
f	O
/	O
9	O
I	O
p	O
K	O
K	O
C	O
3	O
+	O
l	O
p	O
Q	O
r	O
w	O
Y	O
j	O
T	O
i	O
N	O
K	O
E	O
b	O
a	O
S	O
L	O
5	O
9	O
l	O
A	O
0	B-DatasetName
D	O
w	O
U	O
I	O
1	O
j	O
c	O
0	B-DatasetName
F	O
o	O
9	O
z	O
P	O
d	O
P	O
5	O
w	O
7	O
d	O
t	O
1	O
p	O
+	O
H	O
M	O
A	O
J	O
e	O
J	O
W	O
5	O
I	O
6	O
K	O
N	O
H	O
x	O
7	O
a	O
9	O
h	O
K	O
H	O
A	O
a	O
E	O
6	O
4	O
x	O
Q	O
0	B-DatasetName
o	O
N	O
X	O
C	O
f	O
R	O
X	O
o	O
a	O
k	O
p	O
p	O
i	O
R	O
v	O
D	O
Z	O
M	O
F	O
U	O
k	O
Q	O
n	O
q	O
A	O
R	O
G	O
R	O
j	O
K	O
U	O
U	O
y	O
U	O
l	O
8	O
3	O
i	O
5	O
/	O
D	O
U	O
K	O
C	O
G	O
M	O
h	O
D	O
S	O
H	O
a	O
z	O
h	O
T	O
f	O
2	O
9	O
k	O
K	O
F	O
Z	O
F	O
Q	O
D	O
M	O
Z	O
I	O
z	O
1	O
W	O
i	O
1	O
4	O
h	O
/	O
u	O
c	O
N	O
U	O
h	O
1	O
d	O
e	O
h	O
n	O
l	O
S	O
a	O
o	O
J	O
x	O
/	O
O	O
H	O
o	O
p	O
R	O
B	O
L	O
W	O
D	O
R	O
B	O
Q	O
y	O
p	O
J	O
F	O
i	O
z	O
q	O
S	O
E	O
I	O
S	O
2	O
q	O
y	O
Q	O
j	O
x	O
G	O
E	O
m	O
F	O
t	O
G	O
q	O
u	O
Z	O
E	O
t	O
z	O
F	O
L	O
y	O
+	O
T	O
X	O
r	O
P	O
h	O
n	O
j	O
e	O
a	O
t	O
6	O
1	O
6	O
u	O
1	O
X	O
W	O
U	O
Q	O
X	O
H	O
4	O
A	O
S	O
c	O
A	O
R	O
d	O
c	O
g	O
D	O
a	O
4	O
A	O
R	O
3	O
Q	O
B	O
R	O
h	O
k	O
4	O
B	O
m	O
8	O
g	O
j	O
f	O
r	O
y	O
X	O
q	O
x	O
3	O
q	O
2	O
P	O
+	O
W	O
j	O
F	O
K	O
n	O
c	O
O	O
w	O
R	O
9	O
Y	O
n	O
z	O
/	O
E	O
B	O
Z	O
X	O
0	B-DatasetName
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
'	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
f	O
F	O
W	O
B	O
I	O
b	O
p	O
6	O
2	O
Q	O
E	O
4	O
8	O
9	O
e	O
p	O
H	O
A	O
8	O
8	O
w	O
i	O
M	O
y	O
o	O
m	O
E	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
w	O
M	O
x	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
e	O
y	O
K	O
o	O
N	O
6	O
K	O
X	O
j	O
x	O
W	O
c	O
G	O
2	O
h	O
X	O
U	O
o	O
2	O
z	O
b	O
a	O
h	O
2	O
W	O
x	O
I	O
s	O
o	O
W	O
y	O
9	O
E	O
d	O
4	O
8	O
a	O
D	O
i	O
1	O
f	O
/	O
j	O
z	O
X	O
9	O
j	O
2	O
u	O
5	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
p	O
A	O
Q	O
3	O
1	O
v	O
O	O
+	O
U	O
W	O
l	O
t	O
f	O
W	O
N	O
z	O
q	O
7	O
x	O
d	O
2	O
d	O
n	O
d	O
2	O
z	O
+	O
o	O
H	O
h	O
4	O
9	O
m	O
T	O
T	O
T	O
l	O
A	O
U	O
0	B-DatasetName
F	O
a	O
l	O
u	O
R	O
8	O
Q	O
w	O
w	O
S	O
U	O
L	O
L	O
L	O
e	O
C	O
t	O
Z	O
V	O
m	O
J	O
I	O
k	O
E	O
a	O
0	B-DatasetName
W	O
j	O
u	O
5	O
n	O
f	O
G	O
j	O
N	O
t	O
e	O
C	O
o	O
f	O
7	O
U	O
S	O
x	O
M	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
i	O
X	O
V	O
S	O
q	O
z	O
s	O
m	O
W	O
g	O
1	O
5	O
r	O
1	O
r	O
z	O
6	O
t	O
4	O
c	O
e	O
J	O
X	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
p	O
z	O
R	O
L	O
m	O
L	O
R	O
U	O
E	O
G	O
M	O
6	O
v	O
q	O
d	O
s	O
m	O
B	O
N	O
t	O
O	O
R	O
V	O
s	O
W	O
u	O
l	O
m	O
h	O
i	O
l	O
C	O
R	O
2	O
T	O
A	O
O	O
o	O
5	O
K	O
k	O
j	O
A	O
T	O
5	O
v	O
N	O
z	O
p	O
/	O
j	O
M	O
K	O
X	O
0	B-DatasetName
c	O
p	O
9	O
q	O
V	O
t	O
H	O
i	O
u	O
/	O
p	O
7	O
I	O
S	O
W	O
L	O
M	O
J	O
I	O
l	O
c	O
Z	O
0	B-DatasetName
L	O
s	O
0	B-DatasetName
C	O
x	O
7	O
M	O
/	O
E	O
/	O
r	O
5	O
P	O
Z	O
+	O
D	O
r	O
M	O
u	O
V	O
S	O
Z	O
Z	O
Z	O
I	O
u	O
F	O
s	O
W	O
Z	O
w	O
D	O
b	O
F	O
s	O
9	O
9	O
x	O
n	O
2	O
t	O
G	O
r	O
Z	O
g	O
4	O
Q	O
q	O
j	O
m	O
7	O
l	O
Z	O
M	O
h	O
0	B-DatasetName
Q	O
T	O
a	O
l	O
1	O
C	O
F	O
R	O
e	O
C	O
v	O
/	O
z	O
y	O
K	O
g	O
k	O
u	O
6	O
j	O
d	O
1	O
/	O
+	O
G	O
y	O
1	O
r	O
g	O
t	O
0	B-DatasetName
i	O
j	O
D	O
C	O
Z	O
z	O
C	O
O	O
f	O
h	O
w	O
B	O
Q	O
2	O
4	O
h	O
y	O
Y	O
E	O
Q	O
G	O
E	O
E	O
z	O
/	O
A	O
K	O
b	O
0	B-DatasetName
i	O
h	O
F	O
/	O
S	O
O	O
P	O
h	O
a	O
t	O
J	O
V	O
T	O
M	O
H	O
M	O
M	O
f	O
o	O
M	O
8	O
f	O
5	O
y	O
2	O
P	O
e	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
f	O
F	O
W	O
B	O
I	O
b	O
p	O
6	O
2	O
Q	O
E	O
4	O
8	O
9	O
e	O
p	O
H	O
A	O
8	O
8	O
w	O
i	O
M	O
y	O
o	O
m	O
E	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
w	O
M	O
x	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
e	O
y	O
K	O
o	O
N	O
6	O
K	O
X	O
j	O
x	O
W	O
c	O
G	O
2	O
h	O
X	O
U	O
o	O
2	O
z	O
b	O
a	O
h	O
2	O
W	O
x	O
I	O
s	O
o	O
W	O
y	O
9	O
E	O
d	O
4	O
8	O
a	O
D	O
i	O
1	O
f	O
/	O
j	O
z	O
X	O
9	O
j	O
2	O
u	O
5	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
p	O
A	O
Q	O
3	O
1	O
v	O
O	O
+	O
U	O
W	O
l	O
t	O
f	O
W	O
N	O
z	O
q	O
7	O
x	O
d	O
2	O
d	O
n	O
d	O
2	O
z	O
+	O
o	O
H	O
h	O
4	O
9	O
m	O
T	O
T	O
T	O
l	O
A	O
U	O
0	B-DatasetName
F	O
a	O
l	O
u	O
R	O
8	O
Q	O
w	O
w	O
S	O
U	O
L	O
L	O
L	O
e	O
C	O
t	O
Z	O
V	O
m	O
J	O
I	O
k	O
E	O
a	O
0	B-DatasetName
W	O
j	O
u	O
5	O
n	O
f	O
G	O
j	O
N	O
t	O
e	O
C	O
o	O
f	O
7	O
U	O
S	O
x	O
M	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
i	O
X	O
V	O
S	O
q	O
z	O
s	O
m	O
W	O
g	O
1	O
5	O
r	O
1	O
r	O
z	O
6	O
t	O
4	O
c	O
e	O
J	O
X	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
p	O
z	O
R	O
L	O
m	O
L	O
R	O
U	O
E	O
G	O
M	O
6	O
v	O
q	O
d	O
s	O
m	O
B	O
N	O
t	O
O	O
R	O
V	O
s	O
W	O
u	O
l	O
m	O
h	O
i	O
l	O
C	O
R	O
2	O
T	O
A	O
O	O
o	O
5	O
K	O
k	O
j	O
A	O
T	O
5	O
v	O
N	O
z	O
p	O
/	O
j	O
M	O
K	O
X	O
0	B-DatasetName
c	O
p	O
9	O
q	O
V	O
t	O
H	O
i	O
u	O
/	O
p	O
7	O
I	O
S	O
W	O
L	O
M	O
J	O
I	O
l	O
c	O
Z	O
0	B-DatasetName
L	O
s	O
0	B-DatasetName
C	O
x	O
7	O
M	O
/	O
E	O
/	O
r	O
5	O
P	O
Z	O
+	O
D	O
r	O
M	O
u	O
V	O
S	O
Z	O
Z	O
Z	O
I	O
u	O
F	O
s	O
W	O
Z	O
w	O
D	O
b	O
F	O
s	O
9	O
9	O
x	O
n	O
2	O
t	O
G	O
r	O
Z	O
g	O
4	O
Q	O
q	O
j	O
m	O
7	O
l	O
Z	O
M	O
h	O
0	B-DatasetName
Q	O
T	O
a	O
l	O
1	O
C	O
F	O
R	O
e	O
C	O
v	O
/	O
z	O
y	O
K	O
g	O
k	O
u	O
6	O
j	O
d	O
1	O
/	O
+	O
G	O
y	O
1	O
r	O
g	O
t	O
0	B-DatasetName
i	O
j	O
D	O
C	O
Z	O
z	O
C	O
O	O
f	O
h	O
w	O
B	O
Q	O
2	O
4	O
h	O
y	O
Y	O
E	O
Q	O
G	O
E	O
E	O
z	O
/	O
A	O
K	O
b	O
0	B-DatasetName
i	O
h	O
F	O
/	O
S	O
O	O
P	O
h	O
a	O
t	O
J	O
V	O
T	O
M	O
H	O
M	O
M	O
f	O
o	O
M	O
8	O
f	O
5	O
y	O
2	O
P	O
e	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
f	O
F	O
W	O
B	O
I	O
b	O
p	O
6	O
2	O
Q	O
E	O
4	O
8	O
9	O
e	O
p	O
H	O
A	O
8	O
8	O
w	O
i	O
M	O
y	O
o	O
m	O
E	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
w	O
M	O
x	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
e	O
y	O
K	O
o	O
N	O
6	O
K	O
X	O
j	O
x	O
W	O
c	O
G	O
2	O
h	O
X	O
U	O
o	O
2	O
z	O
b	O
a	O
h	O
2	O
W	O
x	O
I	O
s	O
o	O
W	O
y	O
9	O
E	O
d	O
4	O
8	O
a	O
D	O
i	O
1	O
f	O
/	O
j	O
z	O
X	O
9	O
j	O
2	O
u	O
5	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
p	O
A	O
Q	O
3	O
1	O
v	O
O	O
+	O
U	O
W	O
l	O
t	O
f	O
W	O
N	O
z	O
q	O
7	O
x	O
d	O
2	O
d	O
n	O
d	O
2	O
z	O
+	O
o	O
H	O
h	O
4	O
9	O
m	O
T	O
T	O
T	O
l	O
A	O
U	O
0	B-DatasetName
F	O
a	O
l	O
u	O
R	O
8	O
Q	O
w	O
w	O
S	O
U	O
L	O
L	O
L	O
e	O
C	O
t	O
Z	O
V	O
m	O
J	O
I	O
k	O
E	O
a	O
0	B-DatasetName
W	O
j	O
u	O
5	O
n	O
f	O
G	O
j	O
N	O
t	O
e	O
C	O
o	O
f	O
7	O
U	O
S	O
x	O
M	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
i	O
X	O
V	O
S	O
q	O
z	O
s	O
m	O
W	O
g	O
1	O
5	O
r	O
1	O
r	O
z	O
6	O
t	O
4	O
c	O
e	O
J	O
X	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
p	O
z	O
R	O
L	O
m	O
L	O
R	O
U	O
E	O
G	O
M	O
6	O
v	O
q	O
d	O
s	O
m	O
B	O
N	O
t	O
O	O
R	O
V	O
s	O
W	O
u	O
l	O
m	O
h	O
i	O
l	O
C	O
R	O
2	O
T	O
A	O
O	O
o	O
5	O
K	O
k	O
j	O
A	O
T	O
5	O
v	O
N	O
z	O
p	O
/	O
j	O
M	O
K	O
X	O
0	B-DatasetName
c	O
p	O
9	O
q	O
V	O
t	O
H	O
i	O
u	O
/	O
p	O
7	O
I	O
S	O
W	O
L	O
M	O
J	O
I	O
l	O
c	O
Z	O
0	B-DatasetName
L	O
s	O
0	B-DatasetName
C	O
x	O
7	O
M	O
/	O
E	O
/	O
r	O
5	O
P	O
Z	O
+	O
D	O
r	O
M	O
u	O
V	O
S	O
Z	O
Z	O
Z	O
I	O
u	O
F	O
s	O
W	O
Z	O
w	O
D	O
b	O
F	O
s	O
9	O
9	O
x	O
n	O
2	O
t	O
G	O
r	O
Z	O
g	O
4	O
Q	O
q	O
j	O
m	O
7	O
l	O
Z	O
M	O
h	O
0	B-DatasetName
Q	O
T	O
a	O
l	O
1	O
C	O
F	O
R	O
e	O
C	O
v	O
/	O
z	O
y	O
K	O
g	O
k	O
u	O
6	O
j	O
d	O
1	O
/	O
+	O
G	O
y	O
1	O
r	O
g	O
t	O
0	B-DatasetName
i	O
j	O
D	O
C	O
Z	O
z	O
C	O
O	O
f	O
h	O
w	O
B	O
Q	O
2	O
4	O
h	O
y	O
Y	O
E	O
Q	O
G	O
E	O
E	O
z	O
/	O
A	O
K	O
b	O
0	B-DatasetName
i	O
h	O
F	O
/	O
S	O
O	O
P	O
h	O
a	O
t	O
J	O
V	O
T	O
M	O
H	O
M	O
M	O
f	O
o	O
M	O
8	O
f	O
5	O
y	O
2	O
P	O
e	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
f	O
F	O
W	O
B	O
I	O
b	O
p	O
6	O
2	O
Q	O
E	O
4	O
8	O
9	O
e	O
p	O
H	O
A	O
8	O
8	O
w	O
i	O
M	O
y	O
o	O
m	O
E	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
w	O
M	O
x	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
e	O
y	O
K	O
o	O
N	O
6	O
K	O
X	O
j	O
x	O
W	O
c	O
G	O
2	O
h	O
X	O
U	O
o	O
2	O
z	O
b	O
a	O
h	O
2	O
W	O
x	O
I	O
s	O
o	O
W	O
y	O
9	O
E	O
d	O
4	O
8	O
a	O
D	O
i	O
1	O
f	O
/	O
j	O
z	O
X	O
9	O
j	O
2	O
u	O
5	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
p	O
A	O
Q	O
3	O
1	O
v	O
O	O
+	O
U	O
W	O
l	O
t	O
f	O
W	O
N	O
z	O
q	O
7	O
x	O
d	O
2	O
d	O
n	O
d	O
2	O
z	O
+	O
o	O
H	O
h	O
4	O
9	O
m	O
T	O
T	O
T	O
l	O
A	O
U	O
0	B-DatasetName
F	O
a	O
l	O
u	O
R	O
8	O
Q	O
w	O
w	O
S	O
U	O
L	O
L	O
L	O
e	O
C	O
t	O
Z	O
V	O
m	O
J	O
I	O
k	O
E	O
a	O
0	B-DatasetName
W	O
j	O
u	O
5	O
n	O
f	O
G	O
j	O
N	O
t	O
e	O
C	O
o	O
f	O
7	O
U	O
S	O
x	O
M	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
i	O
X	O
V	O
S	O
q	O
z	O
s	O
m	O
W	O
g	O
1	O
5	O
r	O
1	O
r	O
z	O
6	O
t	O
4	O
c	O
e	O
J	O
X	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
p	O
z	O
R	O
L	O
m	O
L	O
R	O
U	O
E	O
G	O
M	O
6	O
v	O
q	O
d	O
s	O
m	O
B	O
N	O
t	O
O	O
R	O
V	O
s	O
W	O
u	O
l	O
m	O
h	O
i	O
l	O
C	O
R	O
2	O
T	O
A	O
O	O
o	O
5	O
K	O
k	O
j	O
A	O
T	O
5	O
v	O
N	O
z	O
p	O
/	O
j	O
M	O
K	O
X	O
0	B-DatasetName
c	O
p	O
9	O
q	O
V	O
t	O
H	O
i	O
u	O
/	O
p	O
7	O
I	O
S	O
W	O
L	O
M	O
J	O
I	O
l	O
c	O
Z	O
0	B-DatasetName
L	O
s	O
0	B-DatasetName
C	O
x	O
7	O
M	O
/	O
E	O
/	O
r	O
5	O
P	O
Z	O
+	O
D	O
r	O
M	O
u	O
V	O
S	O
Z	O
Z	O
Z	O
I	O
u	O
F	O
s	O
W	O
Z	O
w	O
D	O
b	O
F	O
s	O
9	O
9	O
x	O
n	O
2	O
t	O
G	O
r	O
Z	O
g	O
4	O
Q	O
q	O
j	O
m	O
7	O
l	O
Z	O
M	O
h	O
0	B-DatasetName
Q	O
T	O
a	O
l	O
1	O
C	O
F	O
R	O
e	O
C	O
v	O
/	O
z	O
y	O
K	O
g	O
k	O
u	O
6	O
j	O
d	O
1	O
/	O
+	O
G	O
y	O
1	O
r	O
g	O
t	O
0	B-DatasetName
i	O
j	O
D	O
C	O
Z	O
z	O
C	O
O	O
f	O
h	O
w	O
B	O
Q	O
2	O
4	O
h	O
y	O
Y	O
E	O
Q	O
G	O
E	O
E	O
z	O
/	O
A	O
K	O
b	O
0	B-DatasetName
i	O
h	O
F	O
/	O
S	O
O	O
P	O
h	O
a	O
t	O
J	O
V	O
T	O
M	O
H	O
M	O
M	O
f	O
o	O
M	O
8	O
f	O
5	O
y	O
2	O
P	O
e	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
y	O
t	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
G	O
q	O
4	O
R	O
7	O
h	O
6	O
x	O
w	O
e	O
U	O
t	O
W	O
p	O
N	O
B	O
q	O
A	O
5	O
V	O
P	O
E	O
x	O
B	O
C	O
Y	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
m	O
q	O
o	O
N	O
4	O
K	O
X	O
j	O
x	O
W	O
t	O
B	O
/	O
Q	O
h	O
r	O
L	O
Z	O
b	O
t	O
q	O
l	O
m	O
0	B-DatasetName
3	O
Y	O
n	O
Q	O
i	O
h	O
9	O
C	O
d	O
4	O
8	O
a	O
C	O
I	O
V	O
3	O
+	O
R	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Q	O
S	O
K	O
F	O
Q	O
d	O
f	O
9	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
e	O
J	O
U	O
M	O
9	O
5	O
k	O
s	O
Y	O
x	O
1	O
J	O
6	O
C	O
G	O
S	O
6	O
F	O
4	O
E	O
w	O
V	O
K	O
3	O
k	O
k	O
0	B-DatasetName
p	O
1	O
E	O
g	O
e	O
T	O
s	O
Y	O
3	O
8	O
7	O
8	O
9	O
h	O
P	O
X	O
R	O
s	O
T	O
q	O
E	O
b	O
O	O
E	O
+	O
x	O
E	O
d	O
K	O
h	O
E	O
K	O
R	O
t	O
F	O
K	O
D	O
1	O
k	O
f	O
+	O
+	O
W	O
K	O
W	O
3	O
X	O
n	O
I	O
K	O
v	O
E	O
y	O
0	B-DatasetName
k	O
F	O
c	O
j	O
T	O
6	O
5	O
a	O
/	O
e	O
I	O
G	O
Z	O
p	O
x	O
B	O
U	O
y	O
S	O
Y	O
3	O
p	O
e	O
m	O
6	O
C	O
/	O
o	O
R	O
q	O
F	O
E	O
z	O
y	O
a	O
a	O
m	O
X	O
G	O
p	O
5	O
Q	O
N	O
q	O
Z	O
D	O
3	O
r	O
V	O
U	O
0	B-DatasetName
Y	O
g	O
b	O
f	O
z	O
I	O
/	O
d	O
U	O
r	O
O	O
r	O
D	O
I	O
g	O
Y	O
a	O
x	O
t	O
K	O
S	O
R	O
z	O
9	O
f	O
f	O
E	O
h	O
E	O
b	O
G	O
Z	O
F	O
F	O
g	O
O	O
y	O
O	O
K	O
I	O
7	O
P	O
s	O
z	O
c	O
T	O
/	O
v	O
G	O
6	O
K	O
4	O
b	O
U	O
/	O
E	O
S	O
p	O
J	O
k	O
S	O
u	O
2	O
W	O
B	O
S	O
m	O
k	O
m	O
B	O
M	O
Z	O
n	O
+	O
T	O
g	O
d	O
C	O
c	O
o	O
c	O
w	O
s	O
o	O
U	O
w	O
L	O
e	O
y	O
t	O
h	O
I	O
6	O
o	O
p	O
Q	O
5	O
t	O
O	O
y	O
Y	O
b	O
g	O
L	O
b	O
+	O
8	O
S	O
l	O
q	O
1	O
q	O
n	O
d	O
R	O
r	O
d	O
1	O
f	O
V	O
u	O
o	O
3	O
e	O
R	O
x	O
F	O
O	O
I	O
F	O
T	O
O	O
A	O
c	O
P	O
r	O
q	O
A	O
O	O
d	O
9	O
C	O
A	O
J	O
j	O
A	O
Y	O
w	O
j	O
O	O
8	O
w	O
p	O
s	O
j	O
n	O
R	O
f	O
n	O
3	O
f	O
l	O
Y	O
t	O
B	O
a	O
c	O
f	O
O	O
Y	O
Y	O
/	O
s	O
D	O
5	O
/	O
A	O
F	O
x	O
W	O
4	O
3	O
f	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
y	O
t	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
G	O
q	O
4	O
R	O
7	O
h	O
6	O
x	O
w	O
e	O
U	O
t	O
W	O
p	O
N	O
B	O
q	O
A	O
5	O
V	O
P	O
E	O
x	O
B	O
C	O
Y	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
m	O
q	O
o	O
N	O
4	O
K	O
X	O
j	O
x	O
W	O
t	O
B	O
/	O
Q	O
h	O
r	O
L	O
Z	O
b	O
t	O
q	O
l	O
m	O
0	B-DatasetName
3	O
Y	O
n	O
Q	O
i	O
h	O
9	O
C	O
d	O
4	O
8	O
a	O
C	O
I	O
V	O
3	O
+	O
R	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Q	O
S	O
K	O
F	O
Q	O
d	O
f	O
9	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
e	O
J	O
U	O
M	O
9	O
5	O
k	O
s	O
Y	O
x	O
1	O
J	O
6	O
C	O
G	O
S	O
6	O
F	O
4	O
E	O
w	O
V	O
K	O
3	O
k	O
k	O
0	B-DatasetName
p	O
1	O
E	O
g	O
e	O
T	O
s	O
Y	O
3	O
8	O
7	O
8	O
9	O
h	O
P	O
X	O
R	O
s	O
T	O
q	O
E	O
b	O
O	O
E	O
+	O
x	O
E	O
d	O
K	O
h	O
E	O
K	O
R	O
t	O
F	O
K	O
D	O
1	O
k	O
f	O
+	O
+	O
W	O
K	O
W	O
3	O
X	O
n	O
I	O
K	O
v	O
E	O
y	O
0	B-DatasetName
k	O
F	O
c	O
j	O
T	O
6	O
5	O
a	O
/	O
e	O
I	O
G	O
Z	O
p	O
x	O
B	O
U	O
y	O
S	O
Y	O
3	O
p	O
e	O
m	O
6	O
C	O
/	O
o	O
R	O
q	O
F	O
E	O
z	O
y	O
a	O
a	O
m	O
X	O
G	O
p	O
5	O
Q	O
N	O
q	O
Z	O
D	O
3	O
r	O
V	O
U	O
0	B-DatasetName
Y	O
g	O
b	O
f	O
z	O
I	O
/	O
d	O
U	O
r	O
O	O
r	O
D	O
I	O
g	O
Y	O
a	O
x	O
t	O
K	O
S	O
R	O
z	O
9	O
f	O
f	O
E	O
h	O
E	O
b	O
G	O
Z	O
F	O
F	O
g	O
O	O
y	O
O	O
K	O
I	O
7	O
P	O
s	O
z	O
c	O
T	O
/	O
v	O
G	O
6	O
K	O
4	O
b	O
U	O
/	O
E	O
S	O
p	O
J	O
k	O
S	O
u	O
2	O
W	O
B	O
S	O
m	O
k	O
m	O
B	O
M	O
Z	O
n	O
+	O
T	O
g	O
d	O
C	O
c	O
o	O
c	O
w	O
s	O
o	O
U	O
w	O
L	O
e	O
y	O
t	O
h	O
I	O
6	O
o	O
p	O
Q	O
5	O
t	O
O	O
y	O
Y	O
b	O
g	O
L	O
b	O
+	O
8	O
S	O
l	O
q	O
1	O
q	O
n	O
d	O
R	O
r	O
d	O
1	O
f	O
V	O
u	O
o	O
3	O
e	O
R	O
x	O
F	O
O	O
I	O
F	O
T	O
O	O
A	O
c	O
P	O
r	O
q	O
A	O
O	O
d	O
9	O
C	O
A	O
J	O
j	O
A	O
Y	O
w	O
j	O
O	O
8	O
w	O
p	O
s	O
j	O
n	O
R	O
f	O
n	O
3	O
f	O
l	O
Y	O
t	O
B	O
a	O
c	O
f	O
O	O
Y	O
Y	O
/	O
s	O
D	O
5	O
/	O
A	O
F	O
x	O
W	O
4	O
3	O
f	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
d1	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
x	O
f	O
5	O
j	O
g	O
l	O
o	O
e	O
i	O
p	O
z	O
E	O
z	O
b	O
j	O
N	O
z	O
t	O
U	O
C	O
c	O
V	O
3	O
7	O
h	O
E	O
Y	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
q	O
Q	O
Y	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
g	O
9	O
o	O
Q	O
9	O
l	O
s	O
t	O
u	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
E	O
q	O
7	O
/	O
I	O
m	O
/	O
/	O
G	O
b	O
Z	O
u	O
D	O
t	O
j	O
4	O
Y	O
e	O
L	O
w	O
3	O
w	O
8	O
y	O
8	O
I	O
J	O
H	O
C	O
o	O
O	O
t	O
+	O
O	O
4	O
W	O
N	O
z	O
a	O
3	O
t	O
n	O
e	O
J	O
u	O
a	O
W	O
/	O
/	O
4	O
P	O
C	O
o	O
f	O
H	O
z	O
S	O
N	O
n	O
G	O
q	O
G	O
W	O
+	O
x	O
W	O
M	O
a	O
6	O
G	O
1	O
D	O
D	O
p	O
V	O
C	O
8	O
h	O
Q	O
I	O
l	O
7	O
y	O
a	O
a	O
0	B-DatasetName
y	O
i	O
Q	O
v	O
B	O
N	O
M	O
b	O
u	O
d	O
+	O
5	O
4	O
l	O
r	O
I	O
2	O
L	O
1	O
i	O
N	O
O	O
E	O
+	O
x	O
E	O
d	O
K	O
T	O
E	O
U	O
j	O
K	O
K	O
V	O
H	O
s	O
K	O
B	O
N	O
y	O
h	O
X	O
3	O
K	O
q	O
7	O
A	O
F	O
k	O
n	O
X	O
k	O
4	O
q	O
k	O
K	O
M	O
5	O
K	O
H	O
/	O
1	O
w	O
5	O
i	O
l	O
E	O
V	O
f	O
I	O
J	O
D	O
W	O
m	O
5	O
7	O
k	O
J	O
+	O
h	O
n	O
V	O
K	O
J	O
j	O
k	O
s	O
1	O
I	O
/	O
N	O
T	O
y	O
h	O
b	O
E	O
J	O
H	O
v	O
G	O
e	O
p	O
o	O
h	O
E	O
3	O
f	O
r	O
Y	O
4	O
d	O
U	O
Y	O
u	O
r	O
B	O
K	O
S	O
Y	O
a	O
x	O
t	O
K	O
S	O
Q	O
L	O
9	O
f	O
d	O
E	O
R	O
i	O
N	O
j	O
p	O
l	O
F	O
g	O
O	O
y	O
O	O
K	O
Y	O
7	O
P	O
q	O
z	O
c	O
X	O
/	O
v	O
F	O
6	O
K	O
w	O
x	O
s	O
/	O
E	O
y	O
p	O
J	O
k	O
S	O
u	O
2	O
X	O
D	O
R	O
M	O
J	O
c	O
G	O
Y	O
z	O
P	O
8	O
m	O
o	O
d	O
C	O
c	O
o	O
Z	O
x	O
a	O
Q	O
p	O
k	O
W	O
9	O
l	O
b	O
C	O
x	O
l	O
R	O
T	O
h	O
j	O
a	O
d	O
k	O
g	O
3	O
B	O
W	O
3	O
1	O
5	O
n	O
b	O
R	O
r	O
V	O
e	O
+	O
q	O
W	O
r	O
u	O
v	O
V	O
x	O
r	O
1	O
P	O
I	O
4	O
i	O
n	O
M	O
E	O
5	O
X	O
I	O
I	O
H	O
1	O
9	O
C	O
A	O
O	O
2	O
h	O
C	O
C	O
x	O
i	O
M	O
4	O
B	O
l	O
e	O
4	O
c	O
2	O
R	O
z	O
o	O
v	O
z	O
7	O
n	O
w	O
s	O
W	O
w	O
t	O
O	O
P	O
n	O
M	O
K	O
f	O
+	O
B	O
8	O
/	O
g	O
D	O
q	O
Q	O
Y	O
2	O
C	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
d2	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
F	O
v	O
7	O
l	O
i	O
5	O
4	O
W	O
D	O
O	O
5	O
8	O
8	O
U	O
B	O
Z	O
M	O
8	O
e	O
W	O
5	O
Q	O
H	O
B	O
X	O
G	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
q	O
Q	O
Y	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
g	O
9	O
o	O
Q	O
9	O
l	O
s	O
p	O
u	O
3	O
S	O
z	O
S	O
b	O
s	O
b	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
E	O
q	O
7	O
/	O
I	O
m	O
/	O
/	O
G	O
b	O
Z	O
u	O
D	O
t	O
j	O
4	O
Y	O
e	O
L	O
w	O
3	O
w	O
8	O
y	O
8	O
I	O
B	O
F	O
c	O
G	O
9	O
f	O
9	O
d	O
g	O
o	O
b	O
m	O
1	O
v	O
b	O
O	O
8	O
X	O
d	O
0	B-DatasetName
t	O
7	O
+	O
w	O
e	O
F	O
R	O
+	O
f	O
i	O
k	O
r	O
e	O
N	O
U	O
M	O
W	O
y	O
x	O
W	O
M	O
S	O
q	O
G	O
1	O
C	O
N	O
g	O
k	O
t	O
s	O
G	O
W	O
4	O
E	O
d	O
h	O
O	O
F	O
N	O
A	O
o	O
E	O
d	O
o	O
L	O
J	O
7	O
d	O
z	O
v	O
P	O
K	O
H	O
S	O
P	O
J	O
a	O
P	O
Z	O
p	O
q	O
g	O
H	O
9	O
G	O
R	O
5	O
E	O
P	O
O	O
q	O
L	O
H	O
S	O
Q	O
z	O
i	O
o	O
D	O
c	O
o	O
V	O
t	O
+	O
o	O
u	O
Q	O
N	O
a	O
J	O
l	O
5	O
M	O
K	O
5	O
G	O
g	O
O	O
y	O
l	O
/	O
9	O
M	O
G	O
Z	O
p	O
h	O
N	O
I	O
w	O
Q	O
b	O
X	O
u	O
e	O
W	O
5	O
i	O
/	O
I	O
w	O
q	O
w	O
5	O
n	O
A	O
W	O
a	O
m	O
f	O
a	O
k	O
w	O
o	O
m	O
9	O
A	O
R	O
9	O
i	O
y	O
V	O
N	O
E	O
L	O
t	O
Z	O
4	O
t	O
T	O
Z	O
+	O
T	O
C	O
K	O
i	O
E	O
Z	O
x	O
s	O
q	O
W	O
N	O
G	O
S	O
h	O
/	O
p	O
7	O
I	O
a	O
K	O
T	O
1	O
N	O
A	O
p	O
s	O
Z	O
0	B-DatasetName
T	O
N	O
W	O
K	O
9	O
6	O
c	O
/	O
E	O
/	O
r	O
5	O
e	O
a	O
4	O
Y	O
2	O
f	O
c	O
Z	O
m	O
k	O
B	O
i	O
V	O
b	O
L	O
h	O
q	O
m	O
g	O
p	O
i	O
Y	O
z	O
P	O
8	O
m	O
I	O
V	O
f	O
I	O
j	O
J	O
h	O
a	O
Q	O
p	O
n	O
i	O
9	O
l	O
b	O
C	O
x	O
l	O
R	O
R	O
Z	O
m	O
w	O
6	O
J	O
R	O
u	O
C	O
t	O
/	O
r	O
y	O
O	O
m	O
n	O
X	O
q	O
t	O
5	O
V	O
t	O
X	O
Z	O
f	O
r	O
z	O
T	O
q	O
e	O
R	O
x	O
F	O
O	O
I	O
N	O
z	O
u	O
A	O
Q	O
P	O
r	O
q	O
E	O
B	O
d	O
9	O
C	O
E	O
F	O
j	O
A	O
Y	O
w	O
T	O
O	O
8	O
w	O
p	O
s	O
j	O
n	O
B	O
f	O
n	O
3	O
f	O
l	O
Y	O
t	O
h	O
a	O
c	O
f	O
O	O
Y	O
U	O
/	O
s	O
D	O
5	O
/	O
A	O
H	O
r	O
x	O
Y	O
2	O
D	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
dN	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
s	O
5	O
E	O
Z	O
Q	O
S	O
S	O
l	O
O	O
i	O
a	O
F	O
e	O
5	O
s	O
r	O
L	O
h	O
m	O
U	O
a	O
j	O
D	O
J	O
V	O
c	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
k	O
k	O
t	O
6	O
L	O
H	O
g	O
x	O
Z	O
N	O
U	O
t	O
B	O
/	O
Q	O
h	O
r	O
L	O
Z	O
T	O
N	O
q	O
l	O
m	O
0	B-DatasetName
3	O
Y	O
3	O
Q	O
i	O
l	O
9	O
C	O
d	O
4	O
8	O
a	O
C	O
I	O
V	O
3	O
+	O
R	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Q	O
S	O
q	O
4	O
N	O
q	O
7	O
7	O
7	O
a	O
y	O
t	O
b	O
2	O
x	O
u	O
b	O
R	O
d	O
2	O
i	O
r	O
t	O
7	O
+	O
w	O
e	O
H	O
p	O
a	O
P	O
j	O
l	O
k	O
4	O
y	O
x	O
b	O
D	O
J	O
E	O
p	O
G	O
o	O
T	O
k	O
A	O
1	O
C	O
i	O
6	O
x	O
a	O
b	O
g	O
R	O
2	O
E	O
k	O
V	O
0	B-DatasetName
j	O
g	O
Q	O
2	O
A	O
5	O
G	O
N	O
z	O
O	O
/	O
/	O
Y	O
R	O
K	O
8	O
0	B-DatasetName
Q	O
+	O
m	O
n	O
G	O
K	O
f	O
k	O
w	O
H	O
k	O
k	O
e	O
c	O
U	O
W	O
O	O
l	O
h	O
7	O
B	O
/	O
1	O
y	O
+	O
V	O
3	O
Y	O
o	O
7	O
B	O
1	O
k	O
l	O
X	O
k	O
7	O
K	O
k	O
K	O
P	O
R	O
L	O
3	O
3	O
1	O
w	O
o	O
R	O
l	O
M	O
U	O
r	O
D	O
B	O
N	O
W	O
6	O
6	O
7	O
m	O
p	O
8	O
S	O
d	O
U	O
G	O
c	O
4	O
E	O
T	O
o	O
u	O
9	O
T	O
G	O
N	O
K	O
2	O
Y	O
g	O
O	O
s	O
G	O
u	O
p	O
p	O
D	O
F	O
q	O
f	O
z	O
I	O
/	O
d	O
U	O
r	O
O	O
r	O
R	O
K	O
S	O
K	O
F	O
G	O
2	O
p	O
C	O
F	O
z	O
9	O
f	O
f	O
E	O
h	O
M	O
Z	O
a	O
j	O
+	O
P	O
A	O
d	O
s	O
b	O
U	O
D	O
P	O
W	O
y	O
N	O
x	O
P	O
/	O
8	O
7	O
q	O
Z	O
i	O
a	O
7	O
9	O
C	O
Z	O
d	O
p	O
Z	O
l	O
C	O
y	O
x	O
a	O
I	O
o	O
E	O
8	O
Q	O
k	O
Z	O
P	O
Y	O
3	O
C	O
b	O
l	O
C	O
Z	O
s	O
T	O
Y	O
E	O
s	O
o	O
U	O
t	O
7	O
c	O
S	O
N	O
q	O
S	O
K	O
M	O
m	O
P	O
T	O
K	O
d	O
o	O
Q	O
v	O
O	O
W	O
X	O
V	O
0	B-DatasetName
m	O
r	O
W	O
v	O
E	O
u	O
K	O
9	O
X	O
7	O
W	O
r	O
l	O
e	O
y	O
+	O
M	O
o	O
w	O
C	O
m	O
c	O
w	O
Q	O
V	O
4	O
c	O
A	O
V	O
1	O
u	O
I	O
U	O
G	O
N	O
I	O
H	O
B	O
A	O
J	O
7	O
h	O
F	O
d	O
4	O
c	O
4	O
b	O
w	O
4	O
7	O
8	O
7	O
H	O
o	O
n	O
X	O
N	O
y	O
W	O
d	O
O	O
4	O
A	O
+	O
c	O
z	O
x	O
8	O
W	O
R	O
I	O
2	O
f	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
d1	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
x	O
f	O
5	O
j	O
g	O
l	O
o	O
e	O
i	O
p	O
z	O
E	O
z	O
b	O
j	O
N	O
z	O
t	O
U	O
C	O
c	O
V	O
3	O
7	O
h	O
E	O
Y	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
q	O
Q	O
Y	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
g	O
9	O
o	O
Q	O
9	O
l	O
s	O
t	O
u	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
E	O
q	O
7	O
/	O
I	O
m	O
/	O
/	O
G	O
b	O
Z	O
u	O
D	O
t	O
j	O
4	O
Y	O
e	O
L	O
w	O
3	O
w	O
8	O
y	O
8	O
I	O
J	O
H	O
C	O
o	O
O	O
t	O
+	O
O	O
4	O
W	O
N	O
z	O
a	O
3	O
t	O
n	O
e	O
J	O
u	O
a	O
W	O
/	O
/	O
4	O
P	O
C	O
o	O
f	O
H	O
z	O
S	O
N	O
n	O
G	O
q	O
G	O
W	O
+	O
x	O
W	O
M	O
a	O
6	O
G	O
1	O
D	O
D	O
p	O
V	O
C	O
8	O
h	O
Q	O
I	O
l	O
7	O
y	O
a	O
a	O
0	B-DatasetName
y	O
i	O
Q	O
v	O
B	O
N	O
M	O
b	O
u	O
d	O
+	O
5	O
4	O
l	O
r	O
I	O
2	O
L	O
1	O
i	O
N	O
O	O
E	O
+	O
x	O
E	O
d	O
K	O
T	O
E	O
U	O
j	O
K	O
K	O
V	O
H	O
s	O
K	O
B	O
N	O
y	O
h	O
X	O
3	O
K	O
q	O
7	O
A	O
F	O
k	O
n	O
X	O
k	O
4	O
q	O
k	O
K	O
M	O
5	O
K	O
H	O
/	O
1	O
w	O
5	O
i	O
l	O
E	O
V	O
f	O
I	O
J	O
D	O
W	O
m	O
5	O
7	O
k	O
J	O
+	O
h	O
n	O
V	O
K	O
J	O
j	O
k	O
s	O
1	O
I	O
/	O
N	O
T	O
y	O
h	O
b	O
E	O
J	O
H	O
v	O
G	O
e	O
p	O
o	O
h	O
E	O
3	O
f	O
r	O
Y	O
4	O
d	O
U	O
Y	O
u	O
r	O
B	O
K	O
S	O
Y	O
a	O
x	O
t	O
K	O
S	O
Q	O
L	O
9	O
f	O
d	O
E	O
R	O
i	O
N	O
j	O
p	O
l	O
F	O
g	O
O	O
y	O
O	O
K	O
Y	O
7	O
P	O
q	O
z	O
c	O
X	O
/	O
v	O
F	O
6	O
K	O
w	O
x	O
s	O
/	O
E	O
y	O
p	O
J	O
k	O
S	O
u	O
2	O
X	O
D	O
R	O
M	O
J	O
c	O
G	O
Y	O
z	O
P	O
8	O
m	O
o	O
d	O
C	O
c	O
o	O
Z	O
x	O
a	O
Q	O
p	O
k	O
W	O
9	O
l	O
b	O
C	O
x	O
l	O
R	O
T	O
h	O
j	O
a	O
d	O
k	O
g	O
3	O
B	O
W	O
3	O
1	O
5	O
n	O
b	O
R	O
r	O
V	O
e	O
+	O
q	O
W	O
r	O
u	O
v	O
V	O
x	O
r	O
1	O
P	O
I	O
4	O
i	O
n	O
M	O
E	O
5	O
X	O
I	O
I	O
H	O
1	O
9	O
C	O
A	O
O	O
2	O
h	O
C	O
C	O
x	O
i	O
M	O
4	O
B	O
l	O
e	O
4	O
c	O
2	O
R	O
z	O
o	O
v	O
z	O
7	O
n	O
w	O
s	O
W	O
w	O
t	O
O	O
P	O
n	O
M	O
K	O
f	O
+	O
B	O
8	O
/	O
g	O
D	O
q	O
Q	O
Y	O
2	O
C	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
d2	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
F	O
v	O
7	O
l	O
i	O
5	O
4	O
W	O
D	O
O	O
5	O
8	O
8	O
U	O
B	O
Z	O
M	O
8	O
e	O
W	O
5	O
Q	O
H	O
B	O
X	O
G	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
q	O
Q	O
Y	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
g	O
9	O
o	O
Q	O
9	O
l	O
s	O
p	O
u	O
3	O
S	O
z	O
S	O
b	O
s	O
b	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
E	O
q	O
7	O
/	O
I	O
m	O
/	O
/	O
G	O
b	O
Z	O
u	O
D	O
t	O
j	O
4	O
Y	O
e	O
L	O
w	O
3	O
w	O
8	O
y	O
8	O
I	O
B	O
F	O
c	O
G	O
9	O
f	O
9	O
d	O
g	O
o	O
b	O
m	O
1	O
v	O
b	O
O	O
8	O
X	O
d	O
0	B-DatasetName
t	O
7	O
+	O
w	O
e	O
F	O
R	O
+	O
f	O
i	O
k	O
r	O
e	O
N	O
U	O
M	O
W	O
y	O
x	O
W	O
M	O
S	O
q	O
G	O
1	O
C	O
N	O
g	O
k	O
t	O
s	O
G	O
W	O
4	O
E	O
d	O
h	O
O	O
F	O
N	O
A	O
o	O
E	O
d	O
o	O
L	O
J	O
7	O
d	O
z	O
v	O
P	O
K	O
H	O
S	O
P	O
J	O
a	O
P	O
Z	O
p	O
q	O
g	O
H	O
9	O
G	O
R	O
5	O
E	O
P	O
O	O
q	O
L	O
H	O
S	O
Q	O
z	O
i	O
o	O
D	O
c	O
o	O
V	O
t	O
+	O
o	O
u	O
Q	O
N	O
a	O
J	O
l	O
5	O
M	O
K	O
5	O
G	O
g	O
O	O
y	O
l	O
/	O
9	O
M	O
G	O
Z	O
p	O
h	O
N	O
I	O
w	O
Q	O
b	O
X	O
u	O
e	O
W	O
5	O
i	O
/	O
I	O
w	O
q	O
w	O
5	O
n	O
A	O
W	O
a	O
m	O
f	O
a	O
k	O
w	O
o	O
m	O
9	O
A	O
R	O
9	O
i	O
y	O
V	O
N	O
E	O
L	O
t	O
Z	O
4	O
t	O
T	O
Z	O
+	O
T	O
C	O
K	O
i	O
E	O
Z	O
x	O
s	O
q	O
W	O
N	O
G	O
S	O
h	O
/	O
p	O
7	O
I	O
a	O
K	O
T	O
1	O
N	O
A	O
p	O
s	O
Z	O
0	B-DatasetName
T	O
N	O
W	O
K	O
9	O
6	O
c	O
/	O
E	O
/	O
r	O
5	O
e	O
a	O
4	O
Y	O
2	O
f	O
c	O
Z	O
m	O
k	O
B	O
i	O
V	O
b	O
L	O
h	O
q	O
m	O
g	O
p	O
i	O
Y	O
z	O
P	O
8	O
m	O
I	O
V	O
f	O
I	O
j	O
J	O
h	O
a	O
Q	O
p	O
n	O
i	O
9	O
l	O
b	O
C	O
x	O
l	O
R	O
R	O
Z	O
m	O
w	O
6	O
J	O
R	O
u	O
C	O
t	O
/	O
r	O
y	O
O	O
m	O
n	O
X	O
q	O
t	O
5	O
V	O
t	O
X	O
Z	O
f	O
r	O
z	O
T	O
q	O
e	O
R	O
x	O
F	O
O	O
I	O
N	O
z	O
u	O
A	O
Q	O
P	O
r	O
q	O
E	O
B	O
d	O
9	O
C	O
E	O
F	O
j	O
A	O
Y	O
w	O
T	O
O	O
8	O
w	O
p	O
s	O
j	O
n	O
B	O
f	O
n	O
3	O
f	O
l	O
Y	O
t	O
h	O
a	O
c	O
f	O
O	O
Y	O
U	O
/	O
s	O
D	O
5	O
/	O
A	O
H	O
r	O
x	O
Y	O
2	O
D	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
dN	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
s	O
5	O
E	O
Z	O
Q	O
S	O
S	O
l	O
O	O
i	O
a	O
F	O
e	O
5	O
s	O
r	O
L	O
h	O
m	O
U	O
a	O
j	O
D	O
J	O
V	O
c	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
k	O
k	O
t	O
6	O
L	O
H	O
g	O
x	O
Z	O
N	O
U	O
t	O
B	O
/	O
Q	O
h	O
r	O
L	O
Z	O
T	O
N	O
q	O
l	O
m	O
0	B-DatasetName
3	O
Y	O
3	O
Q	O
i	O
l	O
9	O
C	O
d	O
4	O
8	O
a	O
C	O
I	O
V	O
3	O
+	O
R	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Q	O
S	O
q	O
4	O
N	O
q	O
7	O
7	O
7	O
a	O
y	O
t	O
b	O
2	O
x	O
u	O
b	O
R	O
d	O
2	O
i	O
r	O
t	O
7	O
+	O
w	O
e	O
H	O
p	O
a	O
P	O
j	O
l	O
k	O
4	O
y	O
x	O
b	O
D	O
J	O
E	O
p	O
G	O
o	O
T	O
k	O
A	O
1	O
C	O
i	O
6	O
x	O
a	O
b	O
g	O
R	O
2	O
E	O
k	O
V	O
0	B-DatasetName
j	O
g	O
Q	O
2	O
A	O
5	O
G	O
N	O
z	O
O	O
/	O
/	O
Y	O
R	O
K	O
8	O
0	B-DatasetName
Q	O
+	O
m	O
n	O
G	O
K	O
f	O
k	O
w	O
H	O
k	O
k	O
e	O
c	O
U	O
W	O
O	O
l	O
h	O
7	O
B	O
/	O
1	O
y	O
+	O
V	O
3	O
Y	O
o	O
7	O
B	O
1	O
k	O
l	O
X	O
k	O
7	O
K	O
k	O
K	O
P	O
R	O
L	O
3	O
3	O
1	O
w	O
o	O
R	O
l	O
M	O
U	O
r	O
D	O
B	O
N	O
W	O
6	O
6	O
7	O
m	O
p	O
8	O
S	O
d	O
U	O
G	O
c	O
4	O
E	O
T	O
o	O
u	O
9	O
T	O
G	O
N	O
K	O
2	O
Y	O
g	O
O	O
s	O
G	O
u	O
p	O
p	O
D	O
F	O
q	O
f	O
z	O
I	O
/	O
d	O
U	O
r	O
O	O
r	O
R	O
K	O
S	O
K	O
F	O
G	O
2	O
p	O
C	O
F	O
z	O
9	O
f	O
f	O
E	O
h	O
M	O
Z	O
a	O
j	O
+	O
P	O
A	O
d	O
s	O
b	O
U	O
D	O
P	O
W	O
y	O
N	O
x	O
P	O
/	O
8	O
7	O
q	O
Z	O
i	O
a	O
7	O
9	O
C	O
Z	O
d	O
p	O
Z	O
l	O
C	O
y	O
x	O
a	O
I	O
o	O
E	O
8	O
Q	O
k	O
Z	O
P	O
Y	O
3	O
C	O
b	O
l	O
C	O
Z	O
s	O
T	O
Y	O
E	O
s	O
o	O
U	O
t	O
7	O
c	O
S	O
N	O
q	O
S	O
K	O
M	O
m	O
P	O
T	O
K	O
d	O
o	O
Q	O
v	O
O	O
W	O
X	O
V	O
0	B-DatasetName
m	O
r	O
W	O
v	O
E	O
u	O
K	O
9	O
X	O
7	O
W	O
r	O
l	O
e	O
y	O
+	O
M	O
o	O
w	O
C	O
m	O
c	O
w	O
Q	O
V	O
4	O
c	O
A	O
V	O
1	O
u	O
I	O
U	O
G	O
N	O
I	O
H	O
B	O
A	O
J	O
7	O
h	O
F	O
d	O
4	O
c	O
4	O
b	O
w	O
4	O
7	O
8	O
7	O
H	O
o	O
n	O
X	O
N	O
y	O
W	O
d	O
O	O
4	O
A	O
+	O
c	O
z	O
x	O
8	O
W	O
R	O
I	O
2	O
f	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
d1	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
x	O
f	O
5	O
j	O
g	O
l	O
o	O
e	O
i	O
p	O
z	O
E	O
z	O
b	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
j	O
N	O
z	O
t	O
U	O
C	O
c	O
V	O
3	O
7	O
h	O
E	O
Y	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
q	O
Q	O
Y	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
g	O
9	O
o	O
Q	O
9	O
l	O
s	O
t	O
u	O
3	O
S	O
z	O
S	O
b	O
s	O
T	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
E	O
q	O
7	O
/	O
I	O
m	O
/	O
/	O
G	O
b	O
Z	O
u	O
D	O
t	O
j	O
4	O
Y	O
e	O
L	O
w	O
3	O
w	O
8	O
y	O
8	O
I	O
J	O
H	O
C	O
o	O
O	O
t	O
+	O
O	O
4	O
W	O
N	O
z	O
a	O
3	O
t	O
n	O
e	O
J	O
u	O
a	O
W	O
/	O
/	O
4	O
P	O
C	O
o	O
f	O
H	O
z	O
S	O
N	O
n	O
G	O
q	O
G	O
W	O
+	O
x	O
W	O
M	O
a	O
6	O
G	O
1	O
D	O
D	O
p	O
V	O
C	O
8	O
h	O
Q	O
I	O
l	O
7	O
y	O
a	O
a	O
0	B-DatasetName
y	O
i	O
Q	O
v	O
B	O
N	O
M	O
b	O
u	O
d	O
+	O
5	O
4	O
l	O
r	O
I	O
2	O
L	O
1	O
i	O
N	O
O	O
E	O
+	O
x	O
E	O
d	O
K	O
T	O
E	O
U	O
j	O
K	O
K	O
V	O
H	O
s	O
K	O
B	O
N	O
y	O
h	O
X	O
3	O
K	O
q	O
7	O
A	O
F	O
k	O
n	O
X	O
k	O
4	O
q	O
k	O
K	O
M	O
5	O
K	O
H	O
/	O
1	O
w	O
5	O
i	O
l	O
E	O
V	O
f	O
I	O
J	O
D	O
W	O
m	O
5	O
7	O
k	O
J	O
+	O
h	O
n	O
V	O
K	O
J	O
j	O
k	O
s	O
1	O
I	O
/	O
N	O
T	O
y	O
h	O
b	O
E	O
J	O
H	O
v	O
G	O
e	O
p	O
o	O
h	O
E	O
3	O
f	O
r	O
Y	O
4	O
d	O
U	O
Y	O
u	O
r	O
B	O
K	O
S	O
Y	O
a	O
x	O
t	O
K	O
S	O
Q	O
L	O
9	O
f	O
d	O
E	O
R	O
i	O
N	O
j	O
p	O
l	O
F	O
g	O
O	O
y	O
O	O
K	O
Y	O
7	O
P	O
q	O
z	O
c	O
X	O
/	O
v	O
F	O
6	O
K	O
w	O
x	O
s	O
/	O
E	O
y	O
p	O
J	O
k	O
S	O
u	O
2	O
X	O
D	O
R	O
M	O
J	O
c	O
G	O
Y	O
z	O
P	O
8	O
m	O
o	O
d	O
C	O
c	O
o	O
Z	O
x	O
a	O
Q	O
p	O
k	O
W	O
9	O
l	O
b	O
C	O
x	O
l	O
R	O
T	O
h	O
j	O
a	O
d	O
k	O
g	O
3	O
B	O
W	O
3	O
1	O
5	O
n	O
b	O
R	O
r	O
V	O
e	O
+	O
q	O
W	O
r	O
u	O
v	O
V	O
x	O
r	O
1	O
P	O
I	O
4	O
i	O
n	O
M	O
E	O
5	O
X	O
I	O
I	O
H	O
1	O
9	O
C	O
A	O
O	O
2	O
h	O
C	O
C	O
x	O
i	O
M	O
4	O
B	O
l	O
e	O
4	O
c	O
2	O
R	O
z	O
o	O
v	O
z	O
7	O
n	O
w	O
s	O
W	O
w	O
t	O
O	O
P	O
n	O
M	O
K	O
f	O
+	O
B	O
8	O
/	O
g	O
D	O
q	O
Q	O
Y	O
2	O
C	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
d2	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
F	O
v	O
7	O
l	O
i	O
5	O
4	O
W	O
D	O
O	O
5	O
8	O
8	O
U	O
B	O
Z	O
M	O
8	O
e	O
W	O
5	O
Q	O
H	O
B	O
X	O
G	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
q	O
Q	O
Y	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
g	O
9	O
o	O
Q	O
9	O
l	O
s	O
p	O
u	O
3	O
S	O
z	O
S	O
b	O
s	O
b	O
o	O
Q	O
S	O
+	O
h	O
O	O
8	O
e	O
F	O
D	O
E	O
q	O
7	O
/	O
I	O
m	O
/	O
/	O
G	O
b	O
Z	O
u	O
D	O
t	O
j	O
4	O
Y	O
e	O
L	O
w	O
3	O
w	O
8	O
y	O
8	O
I	O
B	O
F	O
c	O
G	O
9	O
f	O
9	O
d	O
g	O
o	O
b	O
m	O
1	O
v	O
b	O
O	O
8	O
X	O
d	O
0	B-DatasetName
t	O
7	O
+	O
w	O
e	O
F	O
R	O
+	O
f	O
i	O
k	O
r	O
e	O
N	O
U	O
M	O
W	O
y	O
x	O
W	O
M	O
S	O
q	O
G	O
1	O
C	O
N	O
g	O
k	O
t	O
s	O
G	O
W	O
4	O
E	O
d	O
h	O
O	O
F	O
N	O
A	O
o	O
E	O
d	O
o	O
L	O
J	O
7	O
d	O
z	O
v	O
P	O
K	O
H	O
S	O
P	O
J	O
a	O
P	O
Z	O
p	O
q	O
g	O
H	O
9	O
G	O
R	O
5	O
E	O
P	O
O	O
q	O
L	O
H	O
S	O
Q	O
z	O
i	O
o	O
D	O
c	O
o	O
V	O
t	O
+	O
o	O
u	O
Q	O
N	O
a	O
J	O
l	O
5	O
M	O
K	O
5	O
G	O
g	O
O	O
y	O
l	O
/	O
9	O
M	O
G	O
Z	O
p	O
h	O
N	O
I	O
w	O
Q	O
b	O
X	O
u	O
e	O
W	O
5	O
i	O
/	O
I	O
w	O
q	O
w	O
5	O
n	O
A	O
W	O
a	O
m	O
f	O
a	O
k	O
w	O
o	O
m	O
9	O
A	O
R	O
9	O
i	O
y	O
V	O
N	O
E	O
L	O
t	O
Z	O
4	O
t	O
T	O
Z	O
+	O
T	O
C	O
K	O
i	O
E	O
Z	O
x	O
s	O
q	O
W	O
N	O
G	O
S	O
h	O
/	O
p	O
7	O
I	O
a	O
K	O
T	O
1	O
N	O
A	O
p	O
s	O
Z	O
0	B-DatasetName
T	O
N	O
W	O
K	O
9	O
6	O
c	O
/	O
E	O
/	O
r	O
5	O
e	O
a	O
4	O
Y	O
2	O
f	O
c	O
Z	O
m	O
k	O
B	O
i	O
V	O
b	O
L	O
h	O
q	O
m	O
g	O
p	O
i	O
Y	O
z	O
P	O
8	O
m	O
I	O
V	O
f	O
I	O
j	O
J	O
h	O
a	O
Q	O
p	O
n	O
i	O
9	O
l	O
b	O
C	O
x	O
l	O
R	O
R	O
Z	O
m	O
w	O
6	O
J	O
R	O
u	O
C	O
t	O
/	O
r	O
y	O
O	O
m	O
n	O
X	O
q	O
t	O
5	O
V	O
t	O
X	O
Z	O
f	O
r	O
z	O
T	O
q	O
e	O
R	O
x	O
F	O
O	O
I	O
N	O
z	O
u	O
A	O
Q	O
P	O
r	O
q	O
E	O
B	O
d	O
9	O
C	O
E	O
F	O
j	O
A	O
Y	O
w	O
T	O
O	O
8	O
w	O
p	O
s	O
j	O
n	O
B	O
f	O
n	O
3	O
f	O
l	O
Y	O
t	O
h	O
a	O
c	O
f	O
O	O
Y	O
U	O
/	O
s	O
D	O
5	O
/	O
A	O
H	O
r	O
x	O
Y	O
2	O
D	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
dN	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
s	O
5	O
E	O
Z	O
Q	O
S	O
S	O
l	O
O	O
i	O
a	O
F	O
e	O
5	O
s	O
r	O
L	O
h	O
m	O
U	O
a	O
j	O
D	O
J	O
V	O
c	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
k	O
k	O
t	O
6	O
L	O
H	O
g	O
x	O
Z	O
N	O
U	O
t	O
B	O
/	O
Q	O
h	O
r	O
L	O
Z	O
T	O
N	O
q	O
l	O
m	O
0	B-DatasetName
3	O
Y	O
3	O
Q	O
i	O
l	O
9	O
C	O
d	O
4	O
8	O
a	O
C	O
I	O
V	O
3	O
+	O
R	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Q	O
S	O
q	O
4	O
N	O
q	O
7	O
7	O
7	O
a	O
y	O
t	O
b	O
2	O
x	O
u	O
b	O
R	O
d	O
2	O
i	O
r	O
t	O
7	O
+	O
w	O
e	O
H	O
p	O
a	O
P	O
j	O
l	O
k	O
4	O
y	O
x	O
b	O
D	O
J	O
E	O
p	O
G	O
o	O
T	O
k	O
A	O
1	O
C	O
i	O
6	O
x	O
a	O
b	O
g	O
R	O
2	O
E	O
k	O
V	O
0	B-DatasetName
j	O
g	O
Q	O
2	O
A	O
5	O
G	O
N	O
z	O
O	O
/	O
/	O
Y	O
R	O
K	O
8	O
0	B-DatasetName
Q	O
+	O
m	O
n	O
G	O
K	O
f	O
k	O
w	O
H	O
k	O
k	O
e	O
c	O
U	O
W	O
O	O
l	O
h	O
7	O
B	O
/	O
1	O
y	O
+	O
V	O
3	O
Y	O
o	O
7	O
B	O
1	O
k	O
l	O
X	O
k	O
7	O
K	O
k	O
K	O
P	O
R	O
L	O
3	O
3	O
1	O
w	O
o	O
R	O
l	O
M	O
U	O
r	O
D	O
B	O
N	O
W	O
6	O
6	O
7	O
m	O
p	O
8	O
S	O
d	O
U	O
G	O
c	O
4	O
E	O
T	O
o	O
u	O
9	O
T	O
G	O
N	O
K	O
2	O
Y	O
g	O
O	O
s	O
G	O
u	O
p	O
p	O
D	O
F	O
q	O
f	O
z	O
I	O
/	O
d	O
U	O
r	O
O	O
r	O
R	O
K	O
S	O
K	O
F	O
G	O
2	O
p	O
C	O
F	O
z	O
9	O
f	O
f	O
E	O
h	O
M	O
Z	O
a	O
j	O
+	O
P	O
A	O
d	O
s	O
b	O
U	O
D	O
P	O
W	O
y	O
N	O
x	O
P	O
/	O
8	O
7	O
q	O
Z	O
i	O
a	O
7	O
9	O
C	O
Z	O
d	O
p	O
Z	O
l	O
C	O
y	O
x	O
a	O
I	O
o	O
E	O
8	O
Q	O
k	O
Z	O
P	O
Y	O
3	O
C	O
b	O
l	O
C	O
Z	O
s	O
T	O
Y	O
E	O
s	O
o	O
U	O
t	O
7	O
c	O
S	O
N	O
q	O
S	O
K	O
M	O
m	O
P	O
T	O
K	O
d	O
o	O
Q	O
v	O
O	O
W	O
X	O
V	O
0	B-DatasetName
m	O
r	O
W	O
v	O
E	O
u	O
K	O
9	O
X	O
7	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
E	O
Q	O
F	O
9	O
V	O
Y	O
Q	O
w	O
a	O
O	O
C	O
/	O
Z	O
C	O
m	O
h	O
M	O
1	O
2	O
U	O
h	O
c	O
3	O
m	O
7	O
g	O
7	O
E	O
U	O
v	O
o	O
v	O
/	O
D	O
i	O
X	O
/	O
H	O
i	O
Q	O
R	O
G	O
v	O
e	O
v	O
P	O
f	O
u	O
K	O
0	B-DatasetName
5	O
a	O
P	O
X	O
B	O
s	O
o	O
/	O
3	O
Z	O
p	O
i	O
Z	O
F	O
6	O
Z	O
S	O
G	O
H	O
T	O
d	O
T	O
2	O
d	O
i	O
c	O
m	O
p	O
6	O
Z	O
n	O
Z	O
u	O
v	O
r	O
S	O
w	O
u	O
L	O
S	O
8	O
U	O
l	O
5	O
d	O
a	O
5	O
g	O
k	O
0	B-DatasetName
x	O
z	O
q	O
P	O
J	O
G	O
J	O
b	O
o	O
X	O
M	O
g	O
B	O
Q	O
K	O
6	O
i	O
h	O
Q	O
Q	O
i	O
v	O
V	O
w	O
O	O
J	O
Q	O
Q	O
j	O
O	O
8	O
P	O
h	O
7	O
6	O
z	O
V	O
v	O
Q	O
R	O
i	O
T	O
q	O
A	O
v	O
s	O
p	O
d	O
G	O
L	O
W	O
U	O
y	O
I	O
S	O
n	O
K	O
G	O
V	O
g	O
n	O
I	O
1	O
9	O
8	O
N	O
E	O
d	O
k	O
0	B-DatasetName
/	O
t	O
h	O
+	O
N	O
B	O
g	O
F	O
S	O
H	O
7	O
V	O
g	O
q	O
i	O
f	O
h	O
x	O
l	O
K	O
4	O
Q	O
8	O
T	O
8	O
R	O
P	O
H	O
B	O
9	O
m	O
W	O
A	O
O	O
0	B-DatasetName
G	O
5	O
4	O
l	O
b	O
d	O
E	O
e	O
h	O
f	O
4	O
h	O
W	O
k	O
Q	O
g	O
q	O
c	O
B	O
e	O
U	O
P	O
v	O
5	O
v	O
w	O
L	O
A	O
a	O
F	O
X	O
D	O
J	O
j	O
2	O
p	O
6	O
b	O
Y	O
i	O
d	O
n	O
G	O
g	O
W	O
X	O
M	O
C	O
j	O
5	O
m	O
Y	O
G	O
U	O
8	O
W	O
v	O
W	O
g	O
7	O
a	O
l	O
i	O
s	O
V	O
g	O
O	O
v	O
n	O
o	O
r	O
g	O
H	O
d	O
s	O
k	O
q	O
X	O
R	O
o	O
m	O
2	O
T	O
y	O
E	O
d	O
q	O
T	O
8	O
7	O
c	O
h	O
a	O
b	O
4	O
e	O
a	O
2	O
M	O
m	O
Z	O
4	O
Z	O
c	O
a	O
9	O
o	O
f	O
i	O
f	O
1	O
8	O
4	O
w	O
O	O
u	O
z	O
k	O
Q	O
q	O
U	O
Z	O
g	O
j	O
1	O
t	O
N	O
C	O
j	O
K	O
J	O
M	O
W	O
E	O
D	O
k	O
O	O
i	O
X	O
a	O
G	O
B	O
o	O
+	O
x	O
b	O
w	O
r	O
g	O
W	O
d	O
l	O
f	O
K	O
r	O
5	O
h	O
m	O
H	O
G	O
2	O
U	O
J	O
R	O
u	O
C	O
N	O
3	O
7	O
y	O
X	O
9	O
L	O
Y	O
r	O
X	O
p	O
7	O
1	O
d	O
3	O
z	O
/	O
U	O
r	O
t	O
q	O
I	O
h	O
j	O
j	O
m	O
y	O
Q	O
T	O
b	O
J	O
N	O
P	O
H	O
J	O
A	O
a	O
u	O
S	O
U	O
n	O
J	O
E	O
6	O
4	O
e	O
S	O
e	O
P	O
J	O
J	O
n	O
8	O
u	O
I	O
8	O
O	O
E	O
/	O
O	O
q	O
/	O
P	O
2	O
X	O
T	O
r	O
h	O
F	O
D	O
3	O
r	O
5	O
B	O
e	O
c	O
9	O
y	O
/	O
a	O
2	O
q	O
B	O
N	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
CNN	O
MLP	B-DatasetName
w	O
t	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
g	O
J	O
w	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
2	O
H	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
1	O
L	O
8	O
6	O
V	O
q	O
1	O
r	O
N	O
8	O
E	O
i	O
u	O
C	O
o	O
z	O
b	O
t	O
S	O
d	O
4	O
M	O
Z	O
l	O
B	O
c	O
c	O
W	O
2	O
q	O
F	O
k	O
M	O
n	O
f	O
a	O
0	B-DatasetName
E	O
x	O
m	O
S	O
O	O
4	O
I	O
p	O
f	O
Q	O
F	O
X	O
L	O
h	O
R	O
f	O
D	O
B	O
3	O
v	O
o	O
3	O
p	O
z	O
0	B-DatasetName
K	O
t	O
B	O
w	O
I	O
f	O
5	O
y	O
T	O
k	O
3	O
p	O
O	O
U	O
S	O
l	O
o	O
K	O
g	O
i	O
+	O
v	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
X	O
3	O
/	O
c	O
P	O
G	O
v	O
7	O
h	O
0	B-DatasetName
X	O
G	O
z	O
8	O
W	O
S	O
L	O
y	O
g	O
i	O
M	O
R	O
K	O
E	O
K	O
0	B-DatasetName
0	B-DatasetName
u	O
4	O
R	O
S	O
U	O
1	O
R	O
i	O
R	O
J	O
Y	O
a	O
8	O
0	B-DatasetName
y	O
P	O
N	O
E	O
Y	O
T	O
e	O
Z	O
3	O
C	O
3	O
y	O
7	O
j	O
M	O
a	O
K	O
w	O
v	O
9	O
S	O
N	O
M	O
S	O
4	O
5	O
y	O
P	O
t	O
M	O
y	O
k	O
4	O
O	O
S	O
s	O
z	O
r	O
D	O
Z	O
C	O
t	O
r	O
B	O
U	O
m	O
w	O
T	O
w	O
j	O
W	O
0	B-DatasetName
Y	O
K	O
1	O
h	O
8	O
3	O
O	O
Q	O
F	O
q	O
L	O
K	O
U	O
Z	O
N	O
Q	O
3	O
N	O
p	O
+	O
G	O
J	O
Q	O
U	O
z	O
7	O
g	O
h	O
K	O
R	O
T	O
O	O
/	O
U	O
F	O
l	O
s	O
e	O
R	O
i	O
w	O
k	O
f	O
Y	O
d	O
6	O
h	O
5	O
j	O
j	O
a	O
e	O
L	O
c	O
e	O
c	O
s	O
3	O
P	O
n	O
p	O
C	O
w	O
r	O
j	O
D	O
u	O
a	O
2	O
N	O
L	O
9	O
+	O
W	O
L	O
G	O
c	O
2	O
u	O
n	O
e	O
e	O
J	O
u	O
5	O
p	O
z	O
G	O
9	O
m	O
+	O
2	O
M	O
P	O
/	O
L	O
+	O
h	O
V	O
l	O
1	O
/	O
F	O
M	O
6	O
r	O
I	O
i	O
1	O
G	O
L	O
1	O
U	O
V	O
Y	O
p	O
R	O
g	O
V	O
b	O
7	O
M	O
x	O
S	O
a	O
V	O
C	O
Q	O
m	O
j	O
r	O
g	O
w	O
k	O
g	O
3	O
K	O
x	O
N	O
j	O
b	O
r	O
g	O
g	O
1	O
4	O
z	O
v	O
O	O
g	O
j	O
/	O
b	O
r	O
w	O
J	O
0	B-DatasetName
W	O
X	O
7	O
p	O
h	O
0	B-DatasetName
+	O
B	O
F	O
C	O
H	O
U	O
z	O
i	O
D	O
C	O
w	O
j	O
h	O
C	O
m	O
7	O
h	O
H	O
j	O
o	O
Q	O
g	O
Y	O
A	O
U	O
X	O
u	O
D	O
N	O
G	O
3	O
u	O
v	O
3	O
v	O
u	O
q	O
q	O
p	O
q	O
3	O
7	O
u	O
w	O
E	O
f	O
s	O
n	O
7	O
+	O
A	O
a	O
q	O
K	O
Y	O
o	O
N	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
8	O
L	O
i	O
q	O
q	O
k	O
O	O
r	O
4	O
y	O
R	O
e	O
y	O
v	O
W	O
h	O
W	O
M	O
T	O
n	O
L	O
M	O
j	O
k	O
C	O
0	B-DatasetName
c	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
3	O
n	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
+	O
F	O
t	O
r	O
1	O
e	O
r	O
W	O
T	O
b	O
A	O
I	O
r	O
s	O
q	O
M	O
G	O
3	O
U	O
n	O
u	O
H	O
F	O
Z	O
0	B-DatasetName
b	O
G	O
F	O
d	O
i	O
i	O
Z	O
9	O
E	O
4	O
b	O
m	O
s	O
k	O
M	O
y	O
R	O
2	O
l	O
l	O
D	O
6	O
C	O
G	O
x	O
c	O
q	O
P	O
p	O
Y	O
7	O
3	O
8	O
b	O
0	B-DatasetName
Z	O
6	O
G	O
t	O
B	O
w	O
I	O
f	O
5	O
y	O
T	O
k	O
3	O
h	O
P	O
n	O
S	O
l	O
r	O
y	O
/	O
W	O
9	O
v	O
b	O
X	O
1	O
j	O
c	O
2	O
u	O
7	O
t	O
F	O
P	O
e	O
r	O
e	O
z	O
t	O
H	O
1	O
Q	O
P	O
K	O
4	O
8	O
2	O
K	O
4	O
z	O
A	O
U	O
G	O
Q	O
q	O
M	O
6	O
2	O
Y	O
W	O
1	O
R	O
S	O
Y	O
0	B-DatasetName
i	O
S	O
F	O
L	O
Z	O
y	O
g	O
z	O
y	O
N	O
F	O
T	O
b	O
j	O
4	O
c	O
0	B-DatasetName
0	B-DatasetName
b	O
z	O
6	O
h	O
s	O
T	O
L	O
T	O
D	O
z	O
T	O
K	O
M	O
U	O
p	O
5	O
X	O
8	O
t	O
E	O
C	O
k	O
7	O
O	O
u	O
n	O
/	O
u	O
U	O
r	O
d	O
a	O
8	O
K	O
e	O
/	O
V	O
+	O
5	O
i	O
3	O
t	O
e	O
Y	O
t	O
a	O
j	O
u	O
C	O
P	O
/	O
I	O
+	O
f	O
w	O
D	O
A	O
+	O
Y	O
x	O
i	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
8	O
T	O
P	O
j	O
8	O
p	O
W	O
5	O
m	O
g	O
7	O
r	O
I	O
V	O
N	O
u	O
D	O
P	O
D	O
9	O
n	O
l	O
C	O
0	B-DatasetName
6	O
8	O
k	B-HyperparameterName
=	I-HyperparameterName
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
T	O
8	O
J	O
A	O
E	O
J	O
3	O
i	O
F	O
+	O
I	O
X	O
6	O
t	O
H	O
L	O
R	O
m	O
L	O
i	O
i	O
b	O
R	O
e	O
1	O
B	O
v	O
R	O
i	O
0	B-DatasetName
e	O
M	O
V	O
k	O
i	O
g	O
I	O
d	O
t	O
l	O
C	O
x	O
u	O
2	O
2	O
2	O
Z	O
3	O
q	O
i	O
E	O
N	O
P	O
8	O
G	O
L	O
B	O
z	O
V	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
c	O
u	O
0	B-DatasetName
I	O
O	O
C	O
L	O
5	O
n	O
k	O
5	O
b	O
2	O
Z	O
z	O
M	O
w	O
L	O
U	O
y	O
k	O
M	O
u	O
u	O
6	O
3	O
U	O
1	O
p	O
Z	O
X	O
V	O
v	O
f	O
K	O
G	O
9	O
W	O
t	O
r	O
Z	O
3	O
d	O
v	O
e	O
q	O
+	O
w	O
c	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
L	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
O	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
r	O
X	O
B	O
0	B-DatasetName
P	O
f	O
V	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
c	O
8	O
i	O
O	O
l	O
A	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
h	O
r	O
1	O
p	O
z	O
6	O
+	O
4	O
M	O
Z	O
J	O
l	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
J	O
y	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
E	O
d	O
z	O
0	B-DatasetName
+	O
v	O
+	O
T	O
G	O
w	O
V	O
g	O
g	O
X	O
U	O
Y	O
K	O
F	O
G	O
t	O
/	O
r	O
V	O
6	O
W	O
W	O
i	O
S	O
F	O
G	O
T	O
U	O
N	O
z	O
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
S	O
f	O
V	O
L	O
q	O
Z	O
4	O
S	O
l	O
l	O
I	O
z	O
r	O
g	O
H	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
t	O
m	O
p	O
E	O
3	O
J	O
i	O
l	O
T	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
p	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
j	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
k	O
V	O
v	O
K	O
v	O
7	O
n	O
d	O
T	O
K	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
8	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
k	O
3	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
w	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
k	O
0	B-DatasetName
Z	O
2	O
n	O
Q	O
q	O
N	O
g	O
R	O
v	O
8	O
e	O
V	O
l	O
4	O
p	O
/	O
V	O
L	O
+	O
v	O
e	O
r	O
V	O
t	O
r	O
X	O
B	O
V	O
p	O
l	O
O	O
E	O
I	O
j	O
u	O
E	O
U	O
P	O
D	O
i	O
H	O
B	O
t	O
x	O
A	O
E	O
3	O
x	O
g	O
M	O
I	O
B	O
n	O
e	O
I	O
U	O
3	O
R	O
z	O
o	O
v	O
z	O
r	O
v	O
z	O
M	O
W	O
8	O
t	O
O	O
c	O
X	O
M	O
I	O
f	O
y	O
B	O
8	O
/	O
k	O
D	O
2	O
/	O
m	O
N	O
s	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
J	O
D	O
O	O
I	O
I	O
T	O
8	O
O	O
A	O
c	O
6	O
n	O
A	O
D	O
D	O
f	O
C	O
B	O
Q	O
R	O
+	O
e	O
4	O
R	O
X	O
e	O
H	O
O	O
m	O
8	O
O	O
O	O
/	O
O	O
x	O
6	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
J	O
D	O
O	O
I	O
I	O
T	O
8	O
O	O
A	O
c	O
6	O
n	O
A	O
D	O
D	O
f	O
C	O
B	O
Q	O
R	O
+	O
e	O
4	O
R	O
X	O
e	O
H	O
O	O
m	O
8	O
O	O
O	O
/	O
O	O
x	O
6	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
X	O
/	O
B	O
b	O
P	O
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
P	O
Q	O
R	O
M	O
1	O
p	O
m	O
B	O
h	O
x	O
d	O
K	O
1	O
e	O
n	O
S	O
b	O
L	O
+	O
g	O
J	O
w	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
2	O
H	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
1	O
L	O
8	O
6	O
V	O
q	O
1	O
r	O
N	O
8	O
E	O
i	O
u	O
C	O
o	O
z	O
b	O
t	O
S	O
d	O
4	O
M	O
Z	O
l	O
B	O
c	O
c	O
W	O
2	O
q	O
F	O
k	O
M	O
n	O
f	O
a	O
0	B-DatasetName
E	O
x	O
m	O
S	O
O	O
4	O
I	O
p	O
f	O
Q	O
F	O
X	O
L	O
h	O
R	O
f	O
D	O
B	O
3	O
v	O
o	O
3	O
p	O
z	O
0	B-DatasetName
K	O
t	O
B	O
w	O
I	O
f	O
5	O
y	O
T	O
k	O
3	O
p	O
O	O
U	O
S	O
l	O
o	O
K	O
g	O
i	O
+	O
v	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
X	O
3	O
/	O
c	O
P	O
G	O
v	O
7	O
h	O
0	B-DatasetName
X	O
G	O
z	O
8	O
W	O
S	O
L	O
y	O
g	O
i	O
M	O
R	O
K	O
E	O
K	O
0	B-DatasetName
0	B-DatasetName
u	O
4	O
R	O
S	O
U	O
1	O
R	O
i	O
R	O
J	O
Y	O
a	O
8	O
0	B-DatasetName
y	O
P	O
N	O
E	O
Y	O
T	O
e	O
Z	O
3	O
C	O
3	O
y	O
7	O
j	O
M	O
a	O
K	O
w	O
v	O
9	O
S	O
N	O
M	O
S	O
4	O
5	O
y	O
P	O
t	O
M	O
y	O
k	O
4	O
O	O
S	O
s	O
z	O
r	O
D	O
Z	O
C	O
t	O
r	O
B	O
U	O
m	O
w	O
T	O
w	O
j	O
W	O
0	B-DatasetName
Y	O
K	O
1	O
h	O
8	O
3	O
O	O
Q	O
F	O
q	O
L	O
K	O
U	O
Z	O
N	O
Q	O
3	O
N	O
p	O
+	O
G	O
J	O
Q	O
U	O
z	O
7	O
g	O
h	O
K	O
R	O
T	O
O	O
/	O
U	O
F	O
l	O
s	O
e	O
R	O
i	O
w	O
k	O
f	O
Y	O
d	O
6	O
h	O
5	O
j	O
j	O
a	O
e	O
L	O
c	O
e	O
c	O
s	O
3	O
P	O
n	O
p	O
C	O
w	O
r	O
j	O
D	O
u	O
a	O
2	O
N	O
L	O
9	O
+	O
W	O
L	O
G	O
c	O
2	O
u	O
n	O
e	O
e	O
J	O
u	O
5	O
p	O
z	O
G	O
9	O
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
R	O
3	O
i	O
G	O
V	O
3	O
i	O
z	O
n	O
q	O
w	O
X	O
6	O
9	O
3	O
6	O
m	O
I	O
9	O
W	O
r	O
H	O
L	O
n	O
A	O
P	O
7	O
A	O
+	O
v	O
w	O
B	O
y	O
q	O
i	O
T	O
5	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
y	O
1	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
J	O
R	O
+	O
a	O
W	O
z	O
1	O
b	O
K	O
b	O
9	O
6	O
X	O
A	O
l	O
h	O
t	O
4	O
q	O
X	O
n	O
K	O
5	O
V	O
3	O
S	O
k	B-HyperparameterName
=	I-HyperparameterName
"	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
G	O
q	O
4	O
R	O
7	O
h	O
6	O
x	O
w	O
e	O
U	O
t	O
W	O
p	O
N	O
B	O
q	O
A	O
5	O
.	O
.	O
.	O
m	O
+	O
2	O
M	O
P	O
/	O
L	O
+	O
h	O
V	O
l	O
1	O
/	O
F	O
M	O
6	O
r	O
I	O
i	O
1	O
G	O
L	O
1	O
U	O
V	O
Y	O
p	O
R	O
g	O
V	O
b	O
7	O
M	O
x	O
S	O
a	O
V	O
C	O
Q	O
m	O
j	O
r	O
g	O
w	O
k	O
g	O
3	O
K	O
x	O
N	O
j	O
b	O
r	O
g	O
g	O
1	O
4	O
z	O
v	O
O	O
g	O
j	O
/	O
b	O
r	O
w	O
J	O
0	B-DatasetName
W	O
X	O
7	O
p	O
h	O
0	B-DatasetName
+	O
B	O
F	O
C	O
H	O
U	O
z	O
i	O
D	O
C	O
w	O
j	O
h	O
C	O
m	O
7	O
h	O
H	O
j	O
o	O
Q	O
g	O
Y	O
A	O
U	O
X	O
u	O
D	O
N	O
G	O
3	O
u	O
v	O
3	O
v	O
u	O
q	O
q	O
p	O
q	O
3	O
7	O
u	O
w	O
E	O
f	O
s	O
n	O
7	O
+	O
A	O
a	O
q	O
K	O
Y	O
o	O
N	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
k	O
v	O
u	O
1	O
4	O
B	O
G	O
D	O
B	O
I	O
h	O
R	O
l	O
F	O
R	O
I	O
G	O
u	O
9	O
Z	O
3	O
v	O
M	O
Y	O
U	O
y	O
g	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
n	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
o	O
p	O
k	O
L	O
K	O
y	O
W	O
F	O
R	O
I	O
T	O
F	O
X	O
C	O
A	O
m	O
x	O
I	O
L	O
I	O
x	O
F	O
I	O
r	O
R	O
S	O
E	O
1	O
W	O
O	O
4	O
7	O
R	O
W	O
H	O
T	O
u	O
y	O
H	O
V	O
A	O
V	O
+	O
i	O
g	O
s	O
D	O
I	O
B	O
4	O
H	O
D	O
b	O
e	O
B	O
q	O
f	O
t	O
A	O
C	O
0	B-DatasetName
n	O
f	O
f	O
L	O
p	O
z	O
p	O
b	O
v	O
u	O
z	O
j	O
n	O
T	O
B	O
v	O
P	O
+	O
3	O
Z	O
q	O
G	O
5	O
t	O
b	O
2	O
z	O
v	O
1	O
3	O
c	O
Z	O
e	O
c	O
/	O
/	O
g	O
0	B-DatasetName
G	O
0	B-DatasetName
1	O
H	O
7	O
Q	O
s	O
F	O
K	O
E	O
B	O
k	O
V	O
y	O
q	O
f	O
o	O
w	O
1	O
5	O
U	O
z	O
Q	O
w	O
D	O
D	O
D	O
a	O
T	O
9	O
X	O
F	O
G	O
c	O
x	O
p	O
7	O
1	O
4	O
c	O
l	O
P	O
5	O
v	O
U	O
e	O
q	O
N	O
J	O
P	O
i	O
3	O
k	O
x	O
z	O
G	O
m	O
V	O
4	O
J	O
F	O
j	O
K	O
C	O
D	O
Z	O
W	O
G	O
r	O
q	O
t	O
s	O
A	O
x	O
j	O
y	O
R	O
M	O
9	O
z	O
e	O
y	O
B	O
+	O
u	O
F	O
s	O
6	O
L	O
a	O
9	O
j	O
j	O
c	O
H	O
W	O
i	O
f	O
+	O
k	O
r	O
R	O
h	O
i	O
e	O
7	O
Q	O
/	O
Q	O
o	O
T	O
S	O
Y	O
q	O
M	O
C	O
k	O
M	O
4	O
1	O
n	O
r	O
g	O
e	O
7	O
m	O
J	O
S	O
q	O
w	O
M	O
I	O
5	O
z	O
O	O
G	O
m	O
G	O
h	O
a	O
Y	O
7	O
J	O
B	O
I	O
/	O
o	O
w	O
F	O
K	O
B	O
M	O
6	O
q	O
j	O
c	O
h	O
5	O
9	O
h	O
k	O
6	O
t	O
k	O
q	O
B	O
U	O
K	O
j	O
v	O
C	O
o	O
L	O
n	O
6	O
+	O
0	B-DatasetName
W	O
J	O
M	O
1	O
1	O
l	O
s	O
z	O
c	O
z	O
b	O
M	O
Z	O
6	O
1	O
a	O
v	O
E	O
/	O
7	O
x	O
B	O
Y	O
d	O
L	O
L	O
q	O
G	O
Q	O
i	O
L	O
w	O
w	O
V	O
Z	O
P	O
F	O
R	O
W	O
n	O
B	O
k	O
J	O
K	O
p	O
6	O
Q	O
A	O
l	O
T	O
l	O
B	O
g	O
+	O
t	O
Q	O
Q	O
T	O
x	O
W	O
x	O
W	O
R	O
M	O
Z	O
Y	O
Y	O
W	O
J	O
s	O
W	O
w	O
1	O
b	O
g	O
r	O
+	O
6	O
8	O
j	O
o	O
J	O
z	O
j	O
t	O
X	O
H	O
f	O
/	O
O	O
g	O
z	O
o	O
c	O
w	O
w	O
m	O
c	O
g	O
Q	O
8	O
X	O
c	O
A	O
2	O
3	O
0	B-DatasetName
I	O
U	O
A	O
C	O
D	O
z	O
B	O
C	O
7	O
z	O
B	O
u	O
/	O
P	O
s	O
v	O
D	O
o	O
f	O
i	O
7	O
Z	O
q	O
z	O
r	O
K	O
2	O
I	O
/	O
g	O
D	O
5	O
/	O
M	O
H	O
b	O
U	O
6	O
S	O
g	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
k	O
v	O
u	O
1	O
4	O
B	O
G	O
D	O
B	O
I	O
h	O
R	O
l	O
F	O
R	O
I	O
G	O
u	O
9	O
Z	O
3	O
v	O
M	O
Y	O
U	O
y	O
g	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
n	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
o	O
p	O
k	O
L	O
K	O
y	O
W	O
F	O
R	O
I	O
T	O
F	O
X	O
C	O
A	O
m	O
x	O
I	O
L	O
I	O
x	O
F	O
I	O
r	O
R	O
S	O
E	O
1	O
W	O
O	O
4	O
7	O
R	O
W	O
H	O
T	O
u	O
y	O
H	O
V	O
A	O
V	O
+	O
i	O
g	O
s	O
D	O
I	O
B	O
4	O
H	O
D	O
b	O
e	O
B	O
q	O
f	O
t	O
A	O
C	O
0	B-DatasetName
n	O
f	O
f	O
L	O
p	O
z	O
p	O
b	O
v	O
u	O
z	O
j	O
n	O
T	O
B	O
v	O
P	O
+	O
3	O
Z	O
q	O
G	O
5	O
t	O
b	O
2	O
z	O
v	O
1	O
3	O
c	O
Z	O
e	O
c	O
/	O
/	O
g	O
0	B-DatasetName
G	O
0	B-DatasetName
1	O
H	O
7	O
Q	O
s	O
F	O
K	O
E	O
B	O
k	O
V	O
y	O
q	O
f	O
o	O
w	O
1	O
5	O
U	O
z	O
Q	O
w	O
D	O
D	O
D	O
a	O
T	O
9	O
X	O
F	O
G	O
c	O
x	O
p	O
7	O
1	O
4	O
c	O
l	O
P	O
5	O
v	O
U	O
e	O
q	O
N	O
J	O
P	O
i	O
3	O
k	O
x	O
z	O
G	O
m	O
V	O
4	O
J	O
F	O
j	O
K	O
C	O
D	O
Z	O
W	O
G	O
r	O
q	O
t	O
s	O
A	O
x	O
j	O
y	O
R	O
M	O
9	O
z	O
e	O
y	O
B	O
+	O
u	O
F	O
s	O
6	O
L	O
a	O
9	O
j	O
j	O
c	O
H	O
W	O
i	O
f	O
+	O
k	O
r	O
R	O
h	O
i	O
e	O
7	O
Q	O
/	O
Q	O
o	O
T	O
S	O
Y	O
q	O
M	O
C	O
k	O
M	O
4	O
1	O
n	O
r	O
g	O
e	O
7	O
m	O
J	O
S	O
q	O
w	O
M	O
I	O
5	O
z	O
O	O
G	O
m	O
G	O
h	O
a	O
Y	O
7	O
J	O
B	O
I	O
/	O
o	O
w	O
F	O
K	O
B	O
M	O
6	O
q	O
j	O
c	O
h	O
5	O
9	O
h	O
k	O
6	O
t	O
k	O
q	O
B	O
U	O
K	O
j	O
v	O
C	O
o	O
L	O
n	O
6	O
+	O
0	B-DatasetName
W	O
J	O
M	O
1	O
1	O
l	O
s	O
z	O
c	O
z	O
b	O
M	O
Z	O
6	O
1	O
a	O
v	O
E	O
/	O
7	O
x	O
B	O
Y	O
d	O
L	O
L	O
q	O
G	O
Q	O
i	O
L	O
w	O
w	O
V	O
Z	O
P	O
F	O
R	O
W	O
n	O
B	O
k	O
J	O
K	O
p	O
6	O
Q	O
A	O
l	O
T	O
l	O
B	O
g	O
+	O
t	O
Q	O
Q	O
T	O
x	O
W	O
x	O
W	O
R	O
M	O
Z	O
Y	O
Y	O
W	O
J	O
s	O
W	O
w	O
1	O
b	O
g	O
r	O
+	O
6	O
8	O
j	O
o	O
J	O
z	O
j	O
t	O
X	O
H	O
f	O
/	O
O	O
g	O
z	O
o	O
c	O
w	O
w	O
m	O
c	O
g	O
Q	O
8	O
X	O
c	O
A	O
2	O
3	O
0	B-DatasetName
I	O
U	O
A	O
C	O
D	O
z	O
B	O
C	O
7	O
z	O
B	O
u	O
/	O
P	O
s	O
v	O
D	O
o	O
f	O
i	O
7	O
Z	O
q	O
z	O
r	O
K	O
2	O
I	O
/	O
g	O
D	O
5	O
/	O
M	O
H	O
b	O
U	O
6	O
S	O
g	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
S	O
d	O
O	O
L	O
O	O
h	O
y	O
v	O
h	O
a	O
t	O
7	O
G	O
h	O
d	O
S	O
U	O
z	O
X	O
L	O
f	O
g	O
4	O
p	O
i	O
J	O
4	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
z	O
C	O
b	O
y	O
l	O
/	O
K	O
Y	O
w	O
s	O
F	O
h	O
U	O
S	O
U	O
5	O
W	O
w	O
A	O
F	O
s	O
F	O
C	O
2	O
O	O
R	O
C	O
K	O
3	O
U	O
R	O
J	O
X	O
j	O
O	O
K	O
1	O
V	O
J	O
4	O
5	O
s	O
B	O
1	O
S	O
F	O
P	O
g	O
o	O
L	O
A	O
y	O
B	O
W	O
3	O
o	O
S	O
N	O
t	O
8	O
F	O
p	O
M	O
0	B-DatasetName
D	O
L	O
S	O
Z	O
Z	O
P	O
d	O
9	O
8	O
n	O
n	O
y	O
/	O
M	O
O	O
F	O
P	O
a	O
c	O
b	O
6	O
t	O
l	O
d	O
W	O
1	O
9	O
Y	O
3	O
N	O
2	O
l	O
Z	O
9	O
e	O
2	O
d	O
3	O
b	O
9	O
9	O
u	O
H	O
N	O
w	O
r	O
k	O
U	O
t	O
C	O
P	O
S	O
K	O
4	O
k	O
L	O
0	B-DatasetName
Q	O
K	O
8	O
p	O
Z	O
S	O
j	O
3	O
N	O
N	O
K	O
e	O
9	O
T	O
F	O
K	O
c	O
h	O
J	O
x	O
2	O
w	O
/	O
F	O
1	O
6	O
X	O
c	O
f	O
q	O
F	O
R	O
M	O
p	O
H	O
d	O
6	O
k	O
t	O
E	O
g	O
w	O
c	O
O	O
U	O
x	O
Y	O
x	O
g	O
b	O
a	O
S	O
B	O
3	O
f	O
A	O
L	O
P	O
x	O
Q	O
8	O
U	O
p	O
P	O
E	O
X	O
K	O
j	O
n	O
T	O
w	O
d	O
2	O
0	B-DatasetName
2	O
k	O
5	O
M	O
6	O
B	O
l	O
4	O
l	O
a	O
k	O
C	O
R	O
U	O
6	O
A	O
/	O
v	O
L	O
j	O
w	O
T	O
J	O
E	O
5	O
p	O
q	O
w	O
r	O
F	O
S	O
f	O
d	O
f	O
J	O
d	O
F	O
B	O
g	O
q	O
R	O
n	O
h	O
d	O
F	O
r	O
3	O
c	O
0	B-DatasetName
U	O
z	O
T	O
M	O
Z	O
4	O
S	O
P	O
u	O
G	O
p	O
j	O
i	O
h	O
K	O
i	O
h	O
m	O
0	B-DatasetName
a	O
f	O
o	O
x	O
C	O
g	O
R	O
i	O
o	O
U	O
0	B-DatasetName
J	O
9	O
V	O
o	O
p	O
v	O
7	O
e	O
K	O
H	O
C	O
i	O
y	O
m	O
x	O
m	O
M	O
s	O
F	O
6	O
p	O
B	O
a	O
9	O
U	O
v	O
z	O
P	O
6	O
+	O
c	O
6	O
v	O
g	O
g	O
K	O
l	O
m	O
a	O
5	O
p	O
i	O
m	O
Z	O
P	O
x	O
T	O
n	O
H	O
G	O
m	O
B	O
y	O
h	O
5	O
Q	O
x	O
C	O
Q	O
l	O
m	O
k	O
8	O
M	O
w	O
U	O
Q	O
y	O
k	O
x	O
W	O
R	O
E	O
Z	O
a	O
Y	O
a	O
N	O
N	O
W	O
3	O
Z	O
T	O
g	O
L	O
n	O
5	O
5	O
m	O
X	O
h	O
n	O
r	O
c	O
u	O
W	O
e	O
+	O
s	O
0	B-DatasetName
2	O
1	O
d	O
V	O
G	O
z	O
U	O
4	O
g	O
m	O
M	O
4	O
B	O
R	O
f	O
O	O
o	O
Q	O
0	B-DatasetName
3	O
0	B-DatasetName
A	O
E	O
P	O
C	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
/	O
V	O
k	O
v	O
V	O
j	O
v	O
1	O
s	O
d	O
8	O
d	O
M	O
W	O
q	O
d	O
g	O
7	O
h	O
D	O
6	O
z	O
P	O
H	O
8	O
l	O
o	O
k	O
+	O
A	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
U	O
0	B-DatasetName
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
m	O
h	O
N	O
N	O
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
g	O
M	O
h	O
Y	O
y	O
t	O
y	O
h	O
J	O
9	O
O	O
i	O
N	O
I	O
M	O
Q	O
F	O
4	O
c	O
M	O
z	O
J	O
C	O
Y	O
s	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
+	O
X	O
i	O
c	O
b	O
V	O
C	O
9	O
T	O
s	O
M	O
w	O
G	O
P	O
x	O
S	O
/	O
k	O
r	O
5	O
S	O
2	O
F	O
k	O
s	O
a	O
i	O
Q	O
m	O
K	O
o	O
E	O
I	O
Q	O
F	O
b	O
B	O
Q	O
t	O
j	O
k	O
Q	O
i	O
t	O
1	O
E	O
S	O
V	O
4	O
7	O
i	O
t	O
V	O
c	O
e	O
J	O
b	O
A	O
d	O
U	O
h	O
T	O
4	O
K	O
C	O
w	O
M	O
g	O
V	O
t	O
6	O
E	O
j	O
b	O
f	O
B	O
a	O
T	O
N	O
A	O
y	O
0	B-DatasetName
m	O
W	O
T	O
3	O
f	O
f	O
J	O
5	O
8	O
v	O
T	O
D	O
l	O
T	O
2	O
n	O
G	O
+	O
r	O
c	O
r	O
K	O
6	O
t	O
r	O
6	O
R	O
n	O
W	O
z	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
n	O
1	O
/	O
f	O
v	O
V	O
Z	O
J	O
J	O
Q	O
j	O
2	O
S	O
8	O
E	O
R	O
2	O
Q	O
6	O
w	O
o	O
Z	O
4	O
J	O
6	O
m	O
m	O
l	O
O	O
u	O
6	O
m	O
k	O
O	O
A	O
4	O
5	O
7	O
Y	O
T	O
j	O
6	O
8	O
L	O
v	O
P	O
F	O
C	O
p	O
W	O
C	O
L	O
u	O
9	O
C	O
S	O
l	O
Q	O
Y	O
y	O
H	O
g	O
g	O
0	B-DatasetName
Y	O
w	O
d	O
p	O
I	O
f	O
b	O
v	O
u	O
5	O
3	O
6	O
Y	O
8	O
E	O
h	O
N	O
Y	O
n	O
O	O
h	O
r	O
j	O
/	O
t	O
2	O
w	O
2	O
n	O
6	O
c	O
y	O
A	O
l	O
o	O
l	O
b	O
k	O
g	O
a	O
U	O
a	O
P	O
f	O
t	O
L	O
z	O
9	O
K	O
S	O
B	O
Z	O
T	O
o	O
Q	O
n	O
H	O
S	O
v	O
V	O
c	O
J	O
9	O
V	O
B	O
j	O
q	O
V	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
m	O
q	O
o	O
N	O
4	O
K	O
X	O
j	O
x	O
W	O
t	O
B	O
/	O
Q	O
h	O
r	O
L	O
Z	O
b	O
t	O
q	O
l	O
m	O
0	B-DatasetName
3	O
Y	O
n	O
Q	O
i	O
h	O
9	O
C	O
d	O
4	O
8	O
a	O
C	O
I	O
V	O
3	O
+	O
R	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Q	O
S	O
K	O
F	O
Q	O
d	O
f	O
9	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
e	O
J	O
U	O
M	O
9	O
5	O
k	O
s	O
Y	O
x	O
1	O
J	O
6	O
C	O
G	O
S	O
6	O
F	O
4	O
E	O
w	O
V	O
K	O
3	O
k	O
k	O
0	B-DatasetName
p	O
1	O
E	O
g	O
e	O
T	O
s	O
Y	O
3	O
8	O
7	O
8	O
9	O
h	O
P	O
X	O
R	O
s	O
T	O
q	O
E	O
b	O
O	O
E	O
+	O
x	O
E	O
d	O
K	O
h	O
E	O
K	O
R	O
t	O
F	O
K	O
D	O
1	O
n	O
f	O
6	O
5	O
c	O
r	O
b	O
t	O
W	O
d	O
g	O
6	O
w	O
S	O
L	O
y	O
c	O
V	O
y	O
N	O
H	O
o	O
l	O
7	O
9	O
6	O
g	O
5	O
i	O
l	O
E	O
V	O
f	O
I	O
J	O
D	O
W	O
m	O
6	O
7	O
k	O
J	O
+	O
h	O
O	O
q	O
U	O
T	O
D	O
J	O
p	O
6	O
V	O
e	O
a	O
n	O
h	O
C	O
2	O
Z	O
g	O
O	O
e	O
d	O
d	O
S	O
R	O
S	O
N	O
u	O
/	O
M	O
n	O
8	O
1	O
C	O
k	O
5	O
s	O
8	O
q	O
A	O
h	O
L	O
G	O
2	O
p	O
Z	O
D	O
M	O
1	O
d	O
8	O
T	O
E	O
x	O
o	O
Z	O
k	O
0	B-DatasetName
W	O
B	O
7	O
Y	O
w	O
o	O
j	O
s	O
y	O
y	O
N	O
x	O
P	O
/	O
8	O
7	O
o	O
p	O
h	O
t	O
f	O
+	O
R	O
K	O
g	O
k	O
R	O
a	O
7	O
Y	O
Y	O
l	O
G	O
Y	O
S	O
o	O
I	O
x	O
m	O
f	O
1	O
N	O
B	O
k	O
J	O
z	O
h	O
j	O
K	O
z	O
h	O
D	O
I	O
t	O
7	O
K	O
2	O
E	O
j	O
a	O
i	O
m	O
D	O
G	O
0	B-DatasetName
6	O
J	O
R	O
u	O
C	O
t	O
/	O
z	O
y	O
K	O
m	O
n	O
V	O
q	O
t	O
5	O
F	O
t	O
X	O
Z	O
/	O
W	O
a	O
n	O
f	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
m	O
q	O
o	O
N	O
4	O
K	O
X	O
j	O
x	O
W	O
t	O
B	O
/	O
Q	O
h	O
r	O
L	O
Z	O
b	O
t	O
q	O
l	O
m	O
0	B-DatasetName
3	O
Y	O
n	O
Q	O
i	O
h	O
9	O
C	O
d	O
4	O
8	O
a	O
C	O
I	O
V	O
3	O
+	O
R	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Q	O
S	O
K	O
F	O
Q	O
d	O
f	O
9	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
e	O
J	O
U	O
M	O
9	O
5	O
k	O
s	O
Y	O
x	O
1	O
J	O
6	O
C	O
G	O
S	O
6	O
F	O
4	O
E	O
w	O
V	O
K	O
3	O
k	O
k	O
0	B-DatasetName
p	O
1	O
E	O
g	O
e	O
T	O
s	O
Y	O
3	O
8	O
7	O
8	O
9	O
h	O
P	O
X	O
R	O
s	O
T	O
q	O
E	O
b	O
O	O
E	O
+	O
x	O
E	O
d	O
K	O
h	O
E	O
K	O
R	O
t	O
F	O
K	O
D	O
1	O
m	O
/	O
1	O
i	O
9	O
X	O
3	O
K	O
o	O
7	O
B	O
1	O
k	O
l	O
X	O
k	O
4	O
q	O
k	O
K	O
P	O
R	O
L	O
3	O
/	O
1	O
B	O
j	O
F	O
L	O
I	O
6	O
6	O
Q	O
S	O
W	O
p	O
M	O
1	O
3	O
M	O
T	O
9	O
C	O
d	O
U	O
o	O
2	O
C	O
S	O
T	O
0	B-DatasetName
u	O
9	O
1	O
P	O
C	O
E	O
s	O
j	O
E	O
d	O
8	O
q	O
6	O
l	O
i	O
k	O
b	O
c	O
+	O
J	O
P	O
5	O
q	O
V	O
N	O
y	O
Z	O
p	O
U	O
B	O
C	O
W	O
N	O
t	O
S	O
y	O
G	O
Z	O
q	O
7	O
8	O
n	O
J	O
j	O
Q	O
y	O
J	O
o	O
s	O
C	O
2	O
x	O
l	O
R	O
H	O
J	O
l	O
l	O
b	O
y	O
b	O
+	O
5	O
3	O
V	O
T	O
D	O
K	O
/	O
9	O
i	O
V	O
B	O
J	O
i	O
l	O
y	O
x	O
x	O
a	O
I	O
w	O
l	O
Q	O
R	O
j	O
M	O
v	O
u	O
b	O
D	O
I	O
T	O
m	O
D	O
G	O
V	O
m	O
C	O
W	O
V	O
a	O
2	O
F	O
s	O
J	O
G	O
1	O
F	O
N	O
G	O
d	O
p	O
0	B-DatasetName
S	O
j	O
Y	O
E	O
b	O
/	O
n	O
l	O
V	O
d	O
K	O
q	O
V	O
V	O
P	O
E	O
x	O
B	O
C	O
Y	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
m	O
q	O
o	O
N	O
4	O
K	O
X	O
j	O
x	O
W	O
t	O
B	O
/	O
Q	O
h	O
r	O
L	O
Z	O
b	O
t	O
q	O
l	O
m	O
0	B-DatasetName
3	O
Y	O
n	O
Q	O
i	O
h	O
9	O
C	O
d	O
4	O
8	O
a	O
C	O
I	O
V	O
3	O
+	O
R	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Q	O
S	O
K	O
F	O
Q	O
d	O
f	O
9	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
e	O
J	O
U	O
M	O
9	O
5	O
k	O
s	O
Y	O
x	O
1	O
J	O
6	O
C	O
G	O
S	O
6	O
F	O
4	O
E	O
w	O
V	O
K	O
3	O
k	O
k	O
0	B-DatasetName
p	O
1	O
E	O
g	O
e	O
T	O
s	O
Y	O
3	O
8	O
7	O
8	O
9	O
h	O
P	O
X	O
R	O
s	O
T	O
q	O
E	O
b	O
O	O
E	O
+	O
x	O
E	O
d	O
K	O
h	O
E	O
K	O
R	O
t	O
F	O
K	O
D	O
1	O
k	O
f	O
+	O
+	O
W	O
K	O
W	O
3	O
X	O
n	O
I	O
K	O
v	O
E	O
y	O
0	B-DatasetName
k	O
F	O
c	O
j	O
T	O
6	O
5	O
a	O
/	O
e	O
I	O
G	O
Z	O
p	O
x	O
B	O
U	O
y	O
S	O
Y	O
3	O
p	O
e	O
m	O
6	O
C	O
/	O
o	O
R	O
q	O
F	O
E	O
z	O
y	O
a	O
a	O
m	O
X	O
G	O
p	O
5	O
Q	O
N	O
q	O
Z	O
D	O
3	O
r	O
V	O
U	O
0	B-DatasetName
Y	O
g	O
b	O
f	O
z	O
I	O
/	O
d	O
U	O
r	O
O	O
r	O
D	O
I	O
g	O
Y	O
a	O
x	O
t	O
K	O
S	O
R	O
z	O
9	O
f	O
f	O
E	O
h	O
E	O
b	O
G	O
Z	O
F	O
F	O
g	O
O	O
y	O
O	O
K	O
I	O
7	O
P	O
s	O
z	O
c	O
T	O
/	O
v	O
G	O
6	O
K	O
4	O
b	O
U	O
/	O
E	O
S	O
p	O
J	O
k	O
S	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
v	O
S	O
B	O
h	O
V	O
2	O
n	O
G	O
+	O
r	O
b	O
X	O
1	O
j	O
c	O
2	O
t	O
7	O
c	O
p	O
O	O
d	O
X	O
d	O
v	O
/	O
+	O
D	O
Q	O
P	O
j	O
r	O
u	O
K	O
Z	O
F	O
K	O
T	O
L	O
p	O
Y	O
M	O
C	O
E	O
H	O
A	O
V	O
K	O
E	O
U	O
U	O
6	O
6	O
m	O
m	O
p	O
G	O
B	O
o	O
k	O
k	O
K	O
A	O
4	O
Y	O
6	O
Q	O
f	O
T	O
6	O
8	O
L	O
v	O
P	O
x	O
K	O
p	O
q	O
O	O
D	O
3	O
O	O
k	O
u	O
I	O
F	O
6	O
M	O
x	O
p	O
x	O
H	O
F	O
S	O
B	O
v	O
J	O
t	O
2	O
v	O
5	O
K	O
B	O
A	O
s	O
V	O
F	O
l	O
s	O
L	O
q	O
h	O
m	O
v	O
n	O
6	O
4	O
8	O
e	O
2	O
6	O
0	B-DatasetName
3	O
D	O
m	O
g	O
K	O
v	O
E	O
L	O
U	O
k	O
d	O
l	O
O	O
j	O
4	O
9	O
t	O
c	O
o	O
F	O
D	O
i	O
N	O
C	O
d	O
e	O
Y	O
I	O
a	O
W	O
G	O
r	O
p	O
N	O
o	O
L	O
0	B-DatasetName
d	O
S	O
U	O
8	O
z	O
I	O
r	O
D	O
p	O
K	O
F	O
U	O
k	O
Q	O
n	O
q	O
I	O
x	O
G	O
R	O
r	O
K	O
U	O
U	O
y	O
U	O
l	O
8	O
/	O
D	O
z	O
+	O
C	O
Z	O
U	O
U	O
I	O
Y	O
C	O
W	O
k	O
O	O
1	O
3	O
C	O
u	O
/	O
t	O
7	O
I	O
U	O
a	O
y	O
K	O
e	O
G	O
Y	O
y	O
R	O
n	O
q	O
i	O
l	O
r	O
1	O
C	O
/	O
M	O
8	O
b	O
p	O
j	O
q	O
6	O
8	O
n	O
L	O
K	O
k	O
1	O
Q	O
T	O
j	O
h	O
c	O
P	O
R	O
S	O
m	O
D	O
W	O
s	O
C	O
i	O
C	O
R	O
h	O
S	O
S	O
b	O
B	O
m	O
m	O
S	O
E	O
I	O
S	O
2	O
q	O
y	O
Q	O
j	O
x	O
B	O
E	O
m	O
F	O
t	O
+	O
q	O
q	O
a	O
E	O
t	O
z	O
l	O
L	O
6	O
+	O
S	O
X	O
r	O
P	O
h	O
X	O
j	O
S	O
a	O
d	O
6	O
1	O
6	O
u	O
1	O
X	O
W	O
U	O
Q	O
E	O
n	O
4	O
B	O
S	O
c	O
A	O
x	O
d	O
c	O
g	O
j	O
a	O
4	O
B	O
R	O
3	O
Q	O
B	O
R	O
h	O
k	O
4	O
B	O
m	O
8	O
g	O
j	O
f	O
r	O
y	O
X	O
q	O
x	O
3	O
q	O
2	O
P	O
x	O
e	O
i	O
a	O
V	O
e	O
7	O
U	O
w	O
B	O
9	O
Y	O
n	O
z	O
8	O
F	O
g	O
Z	O
T	O
1	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
s	O
G	O
1	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
l	O
c	O
G	O
u	O
L	O
g	O
G	O
f	O
K	O
D	O
o	O
+	O
l	O
T	O
5	O
W	O
7	O
Y	O
v	O
r	O
A	O
Q	O
H	O
M	O
V	O
G	O
I	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
/	O
H	O
i	O
c	O
b	O
V	O
D	O
N	O
S	O
8	O
M	O
w	O
H	O
E	O
3	O
9	O
n	O
P	O
O	O
r	O
u	O
q	O
O	O
X	O
4	O
B	O
A	O
8	O
j	O
X	O
Y	O
O	O
9	O
D	O
j	O
w	O
o	O
M	O
c	O
J	O
7	O
g	O
O	O
2	O
W	O
t	O
I	O
0	B-DatasetName
3	O
c	O
L	O
S	O
p	O
C	O
S	O
p	O
U	O
M	O
r	O
8	O
V	O
7	O
x	O
4	O
U	O
M	O
S	O
r	O
f	O
4	O
g	O
3	O
/	O
x	O
v	O
T	O
r	O
Q	O
f	O
d	O
f	O
B	O
D	O
y	O
e	O
O	O
/	O
3	O
I	O
y	O
8	O
v	O
S	O
B	O
h	O
V	O
2	O
n	O
G	O
+	O
r	O
b	O
X	O
1	O
j	O
c	O
2	O
t	O
7	O
c	O
p	O
O	O
d	O
X	O
d	O
v	O
/	O
+	O
D	O
Q	O
P	O
j	O
r	O
u	O
K	O
Z	O
F	O
K	O
T	O
L	O
p	O
Y	O
M	O
C	O
E	O
H	O
A	O
V	O
K	O
E	O
U	O
U	O
6	O
6	O
m	O
m	O
p	O
G	O
B	O
o	O
k	O
k	O
K	O
A	O
4	O
Y	O
6	O
Q	O
f	O
T	O
6	O
8	O
L	O
v	O
P	O
x	O
K	O
p	O
q	O
O	O
D	O
3	O
O	O
k	O
u	O
I	O
F	O
6	O
M	O
x	O
p	O
x	O
H	O
F	O
S	O
B	O
v	O
J	O
t	O
2	O
v	O
5	O
K	O
B	O
A	O
s	O
V	O
F	O
l	O
s	O
L	O
q	O
h	O
m	O
v	O
v	O
t	O
w	O
4	O
9	O
t	O
1	O
p	O
+	O
H	O
M	O
A	O
V	O
e	O
J	O
W	O
5	O
I	O
6	O
K	O
N	O
H	O
x	O
7	O
a	O
9	O
R	O
K	O
H	O
A	O
a	O
E	O
6	O
4	O
x	O
Q	O
0	B-DatasetName
o	O
N	O
X	O
S	O
f	O
R	O
X	O
o	O
6	O
k	O
p	O
p	O
i	O
R	O
W	O
X	O
W	O
U	O
K	O
p	O
I	O
g	O
P	O
E	O
V	O
j	O
M	O
j	O
S	O
U	O
o	O
5	O
g	O
o	O
L	O
5	O
+	O
H	O
n	O
8	O
E	O
z	O
o	O
4	O
Q	O
w	O
E	O
t	O
I	O
c	O
r	O
u	O
F	O
c	O
/	O
b	O
2	O
R	O
o	O
1	O
g	O
V	O
8	O
c	O
x	O
k	O
j	O
P	O
R	O
E	O
L	O
X	O
u	O
F	O
+	O
J	O
8	O
3	O
T	O
H	O
V	O
0	B-DatasetName
5	O
e	O
W	O
U	O
J	O
6	O
k	O
m	O
H	O
C	O
8	O
e	O
i	O
l	O
I	O
G	O
t	O
Y	O
B	O
F	O
E	O
z	O
C	O
k	O
k	O
m	O
D	O
N	O
M	O
k	O
M	O
Q	O
l	O
t	O
R	O
k	O
h	O
X	O
i	O
C	O
J	O
M	O
L	O
a	O
9	O
F	O
U	O
1	O
J	O
b	O
j	O
L	O
X	O
1	O
4	O
l	O
v	O
W	O
b	O
D	O
v	O
W	O
g	O
0	B-DatasetName
7	O
1	O
r	O
1	O
d	O
q	O
u	O
s	O
o	O
w	O
J	O
O	O
w	O
C	O
k	O
4	O
B	O
y	O
6	O
4	O
B	O
G	O
1	O
w	O
C	O
z	O
q	O
g	O
C	O
z	O
D	O
I	O
w	O
D	O
N	O
4	O
B	O
W	O
/	O
W	O
k	O
/	O
V	O
i	O
v	O
V	O
v	O
S	O
B	O
h	O
V	O
2	O
n	O
G	O
+	O
r	O
b	O
X	O
1	O
j	O
c	O
2	O
t	O
7	O
c	O
p	O
O	O
d	O
X	O
d	O
v	O
/	O
+	O
D	O
Q	O
P	O
j	O
r	O
u	O
K	O
Z	O
F	O
K	O
T	O
L	O
p	O
Y	O
M	O
C	O
E	O
H	O
A	O
V	O
K	O
E	O
U	O
U	O
6	O
6	O
m	O
m	O
p	O
G	O
B	O
o	O
k	O
k	O
K	O
A	O
4	O
Y	O
6	O
Q	O
f	O
T	O
6	O
8	O
L	O
v	O
P	O
x	O
K	O
p	O
q	O
O	O
D	O
3	O
O	O
k	O
u	O
I	O
F	O
6	O
M	O
x	O
p	O
x	O
H	O
F	O
S	O
B	O
v	O
J	O
t	O
2	O
v	O
5	O
K	O
B	O
A	O
s	O
V	O
F	O
l	O
s	O
L	O
q	O
h	O
m	O
f	O
v	O
P	O
h	O
x	O
r	O
f	O
r	O
T	O
s	O
O	O
Z	O
A	O
6	O
4	O
S	O
t	O
y	O
R	O
1	O
U	O
K	O
L	O
j	O
2	O
1	O
+	O
j	O
U	O
O	O
A	O
0	B-DatasetName
J	O
l	O
x	O
j	O
h	O
p	O
Q	O
a	O
u	O
k	O
6	O
i	O
v	O
R	O
x	O
J	O
T	O
T	O
E	O
j	O
s	O
+	O
o	O
o	O
V	O
S	O
R	O
B	O
e	O
I	O
r	O
G	O
Z	O
G	O
g	O
o	O
R	O
z	O
F	O
R	O
X	O
j	O
4	O
P	O
P	O
4	O
N	O
n	O
R	O
g	O
l	O
h	O
J	O
K	O
Q	O
5	O
X	O
M	O
O	O
5	O
+	O
n	O
s	O
j	O
R	O
7	O
E	O
q	O
4	O
p	O
n	O
J	O
G	O
O	O
m	O
J	O
W	O
v	O
Y	O
K	O
8	O
T	O
9	O
v	O
m	O
O	O
r	O
o	O
y	O
s	O
s	O
p	O
T	O
1	O
J	O
N	O
O	O
F	O
4	O
8	O
F	O
K	O
U	O
M	O
a	O
g	O
G	O
L	O
J	O
m	O
B	O
I	O
J	O
c	O
G	O
a	O
Z	O
Y	O
Y	O
g	O
L	O
K	O
n	O
J	O
C	O
v	O
E	O
E	O
S	O
Y	O
S	O
1	O
6	O
a	O
t	O
q	O
S	O
n	O
C	O
X	O
v	O
7	O
x	O
K	O
e	O
s	O
2	O
G	O
e	O
9	O
F	O
o	O
3	O
r	O
X	O
q	O
7	O
V	O
Z	O
Z	O
R	O
w	O
W	O
c	O
g	O
F	O
N	O
w	O
D	O
l	O
x	O
w	O
C	O
d	O
r	O
g	O
F	O
n	O
R	O
A	O
F	O
2	O
C	O
Q	O
g	O
W	O
f	O
w	O
C	O
t	O
6	O
s	O
J	O
+	O
v	O
F	O
e	O
r	O
c	O
+	O
F	O
q	O
N	O
r	O
V	O
r	O
l	O
T	O
A	O
3	O
9	O
g	O
f	O
f	O
4	O
A	O
o	O
O	O
a	O
U	O
s	O
w	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
f	O
G	O
1	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
G	O
/	O
j	O
y	O
b	O
d	O
u	O
z	O
g	O
T	O
n	O
G	O
+	O
M	O
Y	O
L	O
R	O
r	O
v	O
P	O
d	O
H	O
T	O
m	O
X	O
8	O
w	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
/	O
n	O
i	O
c	O
b	O
V	O
B	O
P	O
S	O
8	O
M	O
w	O
H	O
E	O
3	O
n	O
v	O
z	O
n	O
/	O
V	O
c	O
W	O
T	O
l	O
+	O
A	O
Q	O
P	O
I	O
1	O
2	O
D	O
v	O
Q	O
4	O
8	O
K	O
D	O
H	O
C	O
W	O
4	O
O	O
t	O
l	O
r	O
S	O
N	O
N	O
3	O
C	O
0	B-DatasetName
q	O
Q	O
k	O
q	O
T	O
B	O
K	O
w	O
a	O
/	O
i	O
x	O
Y	O
M	O
i	O
X	O
v	O
0	B-DatasetName
c	O
3	O
v	O
w	O
2	O
p	O
l	O
s	O
P	O
u	O
v	O
k	O
g	O
5	O
P	O
H	O
e	O
7	O
0	B-DatasetName
d	O
e	O
X	O
p	O
A	O
w	O
q	O
r	O
T	O
j	O
f	O
F	O
u	O
V	O
l	O
d	O
W	O
1	O
9	O
Y	O
3	O
q	O
Z	O
m	O
1	O
r	O
e	O
2	O
d	O
3	O
z	O
9	O
4	O
/	O
6	O
C	O
m	O
R	O
S	O
k	O
y	O
6	O
W	O
D	O
A	O
h	O
+	O
w	O
F	O
S	O
h	O
F	O
F	O
O	O
u	O
p	O
p	O
q	O
R	O
v	O
q	O
J	O
J	O
C	O
g	O
O	O
G	O
L	O
k	O
P	O
J	O
l	O
e	O
F	O
f	O
/	O
9	O
I	O
p	O
K	O
K	O
C	O
3	O
+	O
l	O
p	O
Q	O
r	O
w	O
Y	O
j	O
T	O
i	O
N	O
K	O
E	O
b	O
a	O
S	O
L	O
5	O
9	O
l	O
A	O
0	B-DatasetName
D	O
w	O
U	O
I	O
1	O
j	O
c	O
0	B-DatasetName
F	O
o	O
9	O
z	O
P	O
3	O
P	O
z	O
h	O
2	O
r	O
f	O
r	O
T	O
s	O
O	O
Z	O
A	O
S	O
4	O
T	O
t	O
y	O
R	O
1	O
U	O
K	O
L	O
j	O
2	O
1	O
/	O
D	O
U	O
O	O
A	O
0	B-DatasetName
J	O
l	O
x	O
j	O
h	O
p	O
Q	O
a	O
u	O
E	O
6	O
i	O
v	O
Q	O
x	O
J	O
T	O
T	O
E	O
j	O
e	O
W	O
2	O
Y	O
K	O
p	O
I	O
g	O
P	O
E	O
E	O
j	O
M	O
j	O
C	O
U	O
o	O
5	O
g	O
o	O
L	O
5	O
v	O
F	O
z	O
+	O
G	O
p	O
U	O
U	O
I	O
Y	O
C	O
W	O
k	O
O	O
1	O
3	O
C	O
m	O
/	O
t	O
7	O
I	O
U	O
K	O
y	O
K	O
g	O
G	O
Y	O
y	O
R	O
n	O
q	O
s	O
F	O
r	O
1	O
C	O
/	O
M	O
8	O
b	O
p	O
D	O
q	O
6	O
9	O
D	O
L	O
K	O
k	O
1	O
Q	O
T	O
j	O
u	O
c	O
P	O
R	O
S	O
m	O
D	O
W	O
s	O
C	O
i	O
C	O
x	O
h	O
S	O
S	O
b	O
B	O
m	O
U	O
0	B-DatasetName
M	O
Q	O
l	O
t	O
R	O
k	O
h	O
X	O
i	O
M	O
J	O
M	O
L	O
a	O
N	O
F	O
Y	O
z	O
J	O
b	O
i	O
L	O
X	O
1	O
4	O
m	O
v	O
W	O
b	O
D	O
P	O
W	O
8	O
0	B-DatasetName
b	O
1	O
v	O
1	O
d	O
q	O
u	O
s	O
o	O
w	O
q	O
O	O
w	O
Q	O
k	O
4	O
A	O
y	O
6	O
4	O
A	O
G	O
1	O
w	O
A	O
z	O
q	O
g	O
C	O
z	O
D	O
I	O
w	O
D	O
N	O
4	O
B	O
W	O
/	O
W	O
k	O
/	O
V	O
i	O
v	O
V	O
s	O
f	O
8	O
9	O
G	O
K	O
V	O
e	O
4	O
c	O
g	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
X	O
b	O
C	O
V	O
s	O
Q	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
f	O
G	O
2	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
y	O
L	O
3	O
s	O
0	B-DatasetName
4	O
B	O
p	O
/	O
V	O
q	O
G	O
k	O
J	O
3	O
0	B-DatasetName
G	O
8	O
q	O
C	O
M	O
a	O
m	O
W	O
T	O
Q	O
g	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
/	O
n	O
i	O
c	O
b	O
V	O
B	O
P	O
S	O
8	O
M	O
w	O
H	O
E	O
3	O
n	O
v	O
z	O
n	O
/	O
V	O
c	O
W	O
T	O
l	O
+	O
A	O
Q	O
P	O
I	O
1	O
2	O
D	O
v	O
Q	O
4	O
8	O
K	O
D	O
H	O
C	O
W	O
4	O
O	O
t	O
l	O
r	O
S	O
N	O
N	O
3	O
C	O
0	B-DatasetName
q	O
Q	O
k	O
q	O
T	O
B	O
K	O
w	O
a	O
/	O
i	O
x	O
Y	O
M	O
i	O
X	O
v	O
0	B-DatasetName
c	O
3	O
v	O
w	O
2	O
p	O
l	O
s	O
P	O
u	O
v	O
k	O
g	O
5	O
P	O
H	O
e	O
7	O
0	B-DatasetName
d	O
e	O
X	O
p	O
A	O
w	O
q	O
r	O
T	O
j	O
f	O
F	O
u	O
V	O
l	O
d	O
W	O
1	O
9	O
Y	O
3	O
q	O
Z	O
m	O
1	O
r	O
e	O
2	O
d	O
3	O
z	O
9	O
4	O
/	O
6	O
C	O
m	O
R	O
S	O
k	O
y	O
6	O
W	O
D	O
A	O
h	O
+	O
w	O
F	O
S	O
h	O
F	O
F	O
O	O
u	O
p	O
p	O
q	O
R	O
v	O
q	O
J	O
J	O
C	O
g	O
O	O
G	O
L	O
k	O
P	O
J	O
l	O
e	O
F	O
f	O
/	O
9	O
I	O
p	O
K	O
K	O
C	O
3	O
+	O
l	O
p	O
Q	O
r	O
w	O
Y	O
j	O
T	O
i	O
N	O
K	O
E	O
b	O
a	O
S	O
L	O
5	O
9	O
l	O
A	O
0	B-DatasetName
D	O
w	O
U	O
I	O
1	O
j	O
c	O
0	B-DatasetName
F	O
o	O
9	O
z	O
P	O
m	O
v	O
n	O
D	O
t	O
W	O
/	O
X	O
n	O
Y	O
Y	O
z	O
A	O
1	O
w	O
m	O
b	O
k	O
n	O
q	O
o	O
E	O
T	O
H	O
t	O
7	O
+	O
G	O
o	O
c	O
B	O
p	O
T	O
L	O
j	O
G	O
D	O
C	O
k	O
1	O
c	O
J	O
1	O
E	O
e	O
x	O
m	O
S	O
m	O
m	O
J	O
G	O
8	O
t	O
o	O
w	O
V	O
S	O
R	O
B	O
e	O
I	O
J	O
G	O
Z	O
G	O
A	O
o	O
R	O
z	O
F	O
R	O
X	O
j	O
a	O
L	O
n	O
8	O
N	O
T	O
o	O
4	O
Q	O
w	O
E	O
t	O
I	O
c	O
r	O
u	O
F	O
M	O
/	O
b	O
2	O
R	O
o	O
V	O
g	O
V	O
A	O
c	O
1	O
k	O
j	O
P	O
R	O
Y	O
L	O
X	O
q	O
F	O
+	O
J	O
8	O
3	O
S	O
H	O
V	O
0	B-DatasetName
6	O
W	O
W	O
U	O
J	O
6	O
k	O
m	O
H	O
M	O
8	O
f	O
i	O
l	O
I	O
G	O
t	O
Y	O
B	O
F	O
F	O
z	O
C	O
k	O
k	O
m	O
D	O
N	O
p	O
o	O
Y	O
g	O
L	O
K	O
n	O
J	O
C	O
v	O
E	O
Y	O
S	O
Y	O
S	O
1	O
a	O
a	O
x	O
m	O
S	O
n	O
A	O
X	O
v	O
J	O
i	O
g	O
N	O
G	O
h	O
s	O
H	O
8	O
t	O
v	O
S	O
H	O
j	O
0	B-DatasetName
Q	O
q	O
K	O
v	O
i	O
D	O
z	O
h	O
L	O
i	O
x	O
W	O
j	O
K	O
a	O
U	O
Q	O
x	O
0	B-DatasetName
k	O
b	O
y	O
7	O
U	O
Y	O
+	O
C	O
Q	O
Q	O
L	O
V	O
R	O
a	O
b	O
C	O
6	O
r	O
C	O
z	O
5	O
3	O
C	O
t	O
5	O
t	O
O	O
y	O
1	O
k	O
A	O
r	O
h	O
O	O
3	O
I	O
k	O
1	O
Q	O
o	O
e	O
f	O
b	O
X	O
5	O
N	O
Q	O
4	O
D	O
Q	O
m	O
X	O
G	O
O	O
G	O
l	O
B	O
q	O
7	O
T	O
q	O
K	O
9	O
H	O
E	O
l	O
N	O
M	O
S	O
N	O
F	O
f	O
Z	O
I	O
q	O
k	O
i	O
A	O
8	O
R	O
1	O
M	O
y	O
N	O
p	O
S	O
j	O
m	O
C	O
g	O
v	O
X	O
4	O
Q	O
v	O
4	O
I	O
V	O
R	O
Q	O
h	O
g	O
J	O
a	O
Q	O
7	O
X	O
c	O
K	O
H	O
+	O
3	O
s	O
h	O
R	O
r	O
M	O
p	O
4	O
Z	O
j	O
J	O
G	O
e	O
q	O
Z	O
W	O
v	O
V	O
L	O
8	O
z	O
x	O
u	O
n	O
O	O
r	O
r	O
x	O
c	O
s	O
q	O
T	O
V	O
B	O
O	O
O	O
l	O
w	O
9	O
F	O
K	O
Y	O
N	O
a	O
w	O
L	O
I	O
J	O
G	O
F	O
J	O
J	O
s	O
G	O
a	O
Z	O
I	O
Q	O
h	O
L	O
a	O
r	O
J	O
C	O
P	O
E	O
M	O
S	O
Y	O
W	O
3	O
6	O
q	O
p	O
s	O
S	O
3	O
N	O
U	O
v	O
r	O
5	O
N	O
B	O
u	O
+	O
V	O
e	O
t	O
d	O
r	O
3	O
n	O
W	O
a	O
3	O
U	O
9	O
V	O
R	O
A	O
2	O
f	O
g	O
H	O
F	O
w	O
C	O
F	O
1	O
y	O
D	O
L	O
r	O
g	O
D	O
P	O
d	O
A	O
H	O
G	O
G	O
T	O
g	O
G	O
b	O
y	O
C	O
N	O
+	O
v	O
J	O
e	O
r	O
H	O
e	O
r	O
Y	O
/	O
l	O
6	O
I	O
Z	O
V	O
7	O
T	O
T	O
A	O
H	O
1	O
i	O
f	O
P	O
x	O
w	O
d	O
l	O
Q	O
Q	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
s	O
G	O
0	B-DatasetName
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
w	O
7	O
+	O
O	O
A	O
h	O
R	O
3	O
E	O
4	O
2	O
B	O
D	O
J	O
q	O
r	O
e	O
W	O
7	O
O	O
K	O
f	O
c	O
p	O
C	O
Q	O
0	B-DatasetName
=	O
"	O
>	O
A	O
A	O
A	O
B	O
/	O
n	O
i	O
c	O
b	O
V	O
B	O
P	O
S	O
8	O
M	O
w	O
H	O
E	O
3	O
n	O
v	O
z	O
n	O
/	O
V	O
c	O
W	O
T	O
l	O
+	O
A	O
Q	O
P	O
I	O
1	O
2	O
D	O
v	O
Q	O
4	O
8	O
K	O
D	O
H	O
C	O
W	O
4	O
O	O
t	O
l	O
r	O
S	O
N	O
N	O
3	O
C	O
0	B-DatasetName
q	O
Q	O
k	O
q	O
T	O
B	O
K	O
w	O
a	O
/	O
i	O
x	O
Y	O
M	O
i	O
X	O
v	O
0	B-DatasetName
c	O
3	O
v	O
w	O
2	O
p	O
l	O
s	O
P	O
u	O
v	O
k	O
g	O
5	O
P	O
H	O
e	O
7	O
0	B-DatasetName
d	O
e	O
X	O
p	O
A	O
w	O
q	O
r	O
T	O
j	O
f	O
F	O
u	O
V	O
l	O
d	O
W	O
1	O
9	O
Y	O
3	O
q	O
Z	O
m	O
1	O
r	O
e	O
2	O
d	O
3	O
z	O
9	O
4	O
/	O
6	O
C	O
m	O
R	O
S	O
k	O
y	O
6	O
W	O
D	O
A	O
h	O
+	O
w	O
F	O
S	O
h	O
F	O
F	O
O	O
u	O
p	O
p	O
q	O
R	O
v	O
q	O
J	O
J	O
C	O
g	O
O	O
G	O
L	O
k	O
P	O
J	O
l	O
e	O
F	O
f	O
/	O
9	O
I	O
p	O
K	O
K	O
C	O
3	O
+	O
l	O
p	O
Q	O
r	O
w	O
Y	O
j	O
T	O
i	O
N	O
K	O
E	O
b	O
a	O
S	O
L	O
5	O
9	O
l	O
A	O
0	B-DatasetName
D	O
w	O
U	O
I	O
1	O
j	O
c	O
0	B-DatasetName
F	O
V	O
e	O
5	O
n	O
T	O
v	O
5	O
w	O
7	O
d	O
t	O
1	O
p	O
+	O
H	O
M	O
A	O
J	O
e	O
J	O
W	O
5	O
I	O
6	O
K	O
N	O
H	O
x	O
7	O
a	O
9	O
h	O
K	O
H	O
A	O
a	O
E	O
6	O
4	O
x	O
Q	O
0	B-DatasetName
o	O
N	O
X	O
C	O
f	O
R	O
X	O
o	O
a	O
k	O
p	O
p	O
i	O
R	O
v	O
D	O
Z	O
M	O
F	O
U	O
k	O
Q	O
n	O
q	O
A	O
R	O
G	O
R	O
j	O
K	O
U	O
U	O
y	O
U	O
l	O
8	O
3	O
i	O
5	O
/	O
D	O
U	O
K	O
C	O
G	O
M	O
h	O
D	O
S	O
H	O
a	O
z	O
h	O
T	O
f	O
2	O
9	O
k	O
K	O
F	O
Z	O
F	O
Q	O
D	O
M	O
Z	O
I	O
z	O
1	O
W	O
i	O
1	O
4	O
h	O
/	O
u	O
c	O
N	O
U	O
h	O
1	O
d	O
e	O
h	O
n	O
l	O
S	O
a	O
o	O
J	O
x	O
/	O
O	O
H	O
o	O
p	O
R	O
B	O
L	O
W	O
D	O
R	O
B	O
Q	O
y	O
p	O
J	O
F	O
i	O
z	O
q	O
S	O
E	O
I	O
S	O
2	O
q	O
y	O
Q	O
j	O
x	O
G	O
E	O
m	O
F	O
t	O
G	O
q	O
u	O
Z	O
E	O
t	O
z	O
F	O
L	O
y	O
+	O
T	O
X	O
r	O
P	O
h	O
n	O
j	O
e	O
a	O
t	O
6	O
1	O
6	O
u	O
1	O
X	O
W	O
U	O
Q	O
X	O
H	O
4	O
A	O
S	O
c	O
A	O
R	O
d	O
c	O
g	O
D	O
a	O
4	O
A	O
R	O
3	O
Q	O
B	O
R	O
h	O
k	O
4	O
B	O
m	O
8	O
g	O
j	O
f	O
r	O
y	O
X	O
q	O
x	O
3	O
q	O
2	O
P	O
+	O
W	O
j	O
F	O
K	O
n	O
c	O
O	O
w	O
R	O
9	O
Y	O
n	O
z	O
9	O
w	O
O	O
J	O
W	O
9	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
f	O
G	O
t	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
z	O
U	O
c	O
7	O
C	O
4	O
R	O
M	O
y	O
k	O
u	O
C	O
+	O
q	O
Q	O
K	O
u	O
z	O
B	O
Z	O
q	O
w	O
S	O
L	O
z	O
z	O
0	B-DatasetName
=	O
"	O
>	O
A	O
A	O
A	O
B	O
/	O
n	O
i	O
c	O
b	O
V	O
B	O
P	O
S	O
8	O
M	O
w	O
H	O
E	O
3	O
n	O
v	O
z	O
n	O
/	O
V	O
c	O
W	O
T	O
l	O
+	O
A	O
Q	O
P	O
I	O
1	O
2	O
D	O
v	O
Q	O
4	O
8	O
K	O
D	O
H	O
C	O
W	O
4	O
O	O
t	O
l	O
r	O
S	O
N	O
N	O
3	O
C	O
0	B-DatasetName
q	O
Q	O
k	O
q	O
T	O
B	O
K	O
w	O
a	O
/	O
i	O
x	O
Y	O
M	O
i	O
X	O
v	O
0	B-DatasetName
c	O
3	O
v	O
w	O
2	O
p	O
l	O
s	O
P	O
u	O
v	O
k	O
g	O
5	O
P	O
H	O
e	O
7	O
0	B-DatasetName
d	O
e	O
X	O
p	O
A	O
w	O
q	O
r	O
T	O
j	O
f	O
F	O
u	O
V	O
l	O
d	O
W	O
1	O
9	O
Y	O
3	O
q	O
Z	O
m	O
1	O
r	O
e	O
2	O
d	O
3	O
z	O
9	O
4	O
/	O
6	O
C	O
m	O
R	O
S	O
k	O
y	O
6	O
W	O
D	O
A	O
h	O
+	O
w	O
F	O
S	O
h	O
F	O
F	O
O	O
u	O
p	O
p	O
q	O
R	O
v	O
q	O
J	O
J	O
C	O
g	O
O	O
G	O
L	O
k	O
P	O
J	O
l	O
e	O
F	O
f	O
/	O
9	O
I	O
p	O
K	O
K	O
C	O
3	O
+	O
l	O
p	O
Q	O
r	O
w	O
Y	O
j	O
T	O
i	O
N	O
K	O
E	O
b	O
a	O
S	O
L	O
5	O
9	O
l	O
A	O
0	B-DatasetName
D	O
w	O
U	O
I	O
1	O
j	O
c	O
0	B-DatasetName
F	O
o	O
9	O
z	O
P	O
d	O
P	O
5	O
w	O
7	O
d	O
t	O
1	O
p	O
+	O
H	O
M	O
A	O
J	O
e	O
J	O
W	O
5	O
I	O
6	O
K	O
N	O
H	O
x	O
7	O
a	O
9	O
h	O
K	O
H	O
A	O
a	O
E	O
6	O
4	O
x	O
Q	O
0	B-DatasetName
o	O
N	O
X	O
C	O
f	O
R	O
X	O
o	O
a	O
k	O
p	O
p	O
i	O
R	O
v	O
D	O
Z	O
M	O
F	O
U	O
k	O
Q	O
n	O
q	O
A	O
R	O
G	O
R	O
j	O
K	O
U	O
U	O
y	O
U	O
l	O
8	O
3	O
i	O
5	O
/	O
D	O
U	O
K	O
C	O
G	O
M	O
h	O
D	O
S	O
H	O
a	O
z	O
h	O
T	O
f	O
2	O
9	O
k	O
K	O
F	O
Z	O
F	O
Q	O
D	O
M	O
Z	O
I	O
z	O
1	O
W	O
i	O
1	O
4	O
h	O
/	O
u	O
c	O
N	O
U	O
h	O
1	O
d	O
e	O
h	O
n	O
l	O
S	O
a	O
o	O
J	O
x	O
/	O
O	O
H	O
o	O
p	O
R	O
B	O
L	O
W	O
D	O
R	O
B	O
Q	O
y	O
p	O
J	O
F	O
i	O
z	O
q	O
S	O
E	O
I	O
S	O
2	O
q	O
y	O
Q	O
j	O
x	O
G	O
E	O
m	O
F	O
t	O
G	O
q	O
u	O
Z	O
E	O
t	O
z	O
F	O
L	O
y	O
+	O
T	O
X	O
r	O
P	O
h	O
n	O
j	O
e	O
a	O
t	O
6	O
1	O
6	O
u	O
1	O
X	O
W	O
U	O
Q	O
X	O
H	O
4	O
A	O
S	O
c	O
A	O
R	O
d	O
c	O
g	O
D	O
a	O
4	O
A	O
R	O
3	O
Q	O
B	O
R	O
h	O
k	O
4	O
B	O
m	O
8	O
g	O
j	O
f	O
r	O
y	O
X	O
q	O
x	O
3	O
q	O
2	O
P	O
+	O
W	O
j	O
F	O
K	O
n	O
c	O
O	O
w	O
R	O
9	O
Y	O
n	O
z	O
/	O
E	O
B	O
Z	O
X	O
0	B-DatasetName
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
J	O
i	O
g	O
N	O
G	O
h	O
s	O
H	O
8	O
t	O
v	O
S	O
H	O
j	O
0	B-DatasetName
Q	O
q	O
K	O
v	O
i	O
D	O
z	O
h	O
L	O
i	O
x	O
W	O
j	O
K	O
a	O
U	O
Q	O
x	O
0	B-DatasetName
k	O
b	O
y	O
7	O
U	O
Y	O
+	O
C	O
Q	O
Q	O
L	O
V	O
R	O
a	O
b	O
C	O
0	B-DatasetName
a	O
F	O
n	O
7	O
u	O
F	O
b	O
z	O
e	O
d	O
l	O
r	O
M	O
A	O
X	O
C	O
d	O
u	O
R	O
Z	O
q	O
g	O
Q	O
s	O
+	O
3	O
v	O
y	O
a	O
h	O
w	O
G	O
l	O
M	O
u	O
M	O
Y	O
M	O
K	O
T	O
V	O
2	O
n	O
U	O
R	O
7	O
O	O
Z	O
K	O
a	O
Y	O
k	O
a	O
K	O
+	O
i	O
R	O
V	O
J	O
E	O
F	O
4	O
j	O
q	O
Z	O
k	O
b	O
C	O
h	O
H	O
M	O
V	O
F	O
e	O
v	O
g	O
h	O
f	O
w	O
A	O
u	O
j	O
h	O
D	O
A	O
S	O
0	B-DatasetName
h	O
y	O
u	O
4	O
U	O
L	O
9	O
v	O
Z	O
G	O
j	O
W	O
J	O
X	O
x	O
z	O
G	O
S	O
M	O
9	O
E	O
y	O
t	O
e	O
q	O
X	O
4	O
n	O
z	O
d	O
O	O
d	O
X	O
T	O
j	O
5	O
Z	O
Q	O
n	O
q	O
S	O
Y	O
c	O
L	O
x	O
+	O
K	O
U	O
g	O
a	O
1	O
g	O
G	O
U	O
T	O
M	O
K	O
S	O
S	O
Y	O
M	O
0	B-DatasetName
y	O
Q	O
x	O
C	O
W	O
1	O
G	O
S	O
F	O
e	O
I	O
Y	O
k	O
w	O
t	O
r	O
0	B-DatasetName
V	O
T	O
c	O
l	O
u	O
K	O
t	O
f	O
X	O
i	O
e	O
D	O
d	O
s	O
u	O
9	O
a	O
r	O
X	O
v	O
O	O
8	O
1	O
u	O
p	O
6	O
q	O
j	O
B	O
s	O
7	O
A	O
O	O
b	O
g	O
E	O
L	O
r	O
g	O
G	O
X	O
X	O
A	O
H	O
e	O
q	O
A	O
P	O
M	O
M	O
j	O
A	O
M	O
3	O
g	O
F	O
b	O
9	O
a	O
T	O
9	O
W	O
K	O
9	O
W	O
x	O
/	O
L	O
0	B-DatasetName
Q	O
2	O
r	O
2	O
m	O
m	O
A	O
P	O
7	O
A	O
+	O
f	O
w	O
A	O
J	O
r	O
Z	O
T	O
4	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
f	O
2	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
v	O
5	O
4	O
x	O
g	O
q	O
V	O
m	O
/	O
n	O
q	O
9	O
D	O
y	O
N	O
z	O
G	O
r	O
d	O
c	O
1	O
F	O
u	O
6	O
O	O
P	O
g	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
/	O
H	O
i	O
c	O
b	O
V	O
D	O
N	O
S	O
8	O
M	O
w	O
H	O
E	O
3	O
9	O
n	O
P	O
O	O
r	O
u	O
q	O
O	O
X	O
4	O
B	O
A	O
8	O
j	O
X	O
Y	O
O	O
9	O
D	O
j	O
w	O
4	O
n	O
G	O
C	O
+	O
4	O
C	O
t	O
l	O
D	O
R	O
N	O
t	O
7	O
A	O
0	B-DatasetName
K	O
U	O
k	O
q	O
l	O
F	O
L	O
/	O
F	O
S	O
8	O
e	O
F	O
P	O
H	O
q	O
H	O
+	O
L	O
N	O
/	O
8	O
Z	O
0	B-DatasetName
6	O
0	B-DatasetName
E	O
3	O
H	O
4	O
Q	O
8	O
3	O
v	O
v	O
9	O
y	O
M	O
s	O
L	O
E	O
k	O
a	O
V	O
d	O
p	O
x	O
v	O
a	O
2	O
N	O
z	O
a	O
3	O
t	O
n	O
t	O
7	O
Z	O
X	O
3	O
z	O
8	O
4	O
P	O
D	O
q	O
2	O
T	O
0	B-DatasetName
4	O
H	O
S	O
q	O
Q	O
S	O
k	O
z	O
4	O
W	O
T	O
M	O
h	O
R	O
g	O
B	O
R	O
h	O
l	O
J	O
O	O
+	O
p	O
p	O
q	O
R	O
U	O
S	O
I	O
J	O
i	O
g	O
N	O
G	O
h	O
s	O
H	O
8	O
t	O
v	O
S	O
H	O
j	O
0	B-DatasetName
Q	O
q	O
K	O
v	O
i	O
D	O
z	O
h	O
L	O
i	O
x	O
W	O
j	O
K	O
a	O
U	O
Q	O
x	O
0	B-DatasetName
k	O
b	O
y	O
7	O
U	O
Y	O
+	O
C	O
Q	O
Q	O
L	O
V	O
R	O
a	O
b	O
C	O
0	B-DatasetName
a	O
F	O
n	O
7	O
c	O
L	O
3	O
2	O
4	O
6	O
L	O
W	O
c	O
B	O
u	O
E	O
7	O
c	O
i	O
j	O
R	O
B	O
h	O
Z	O
5	O
v	O
f	O
0	B-DatasetName
1	O
C	O
g	O
d	O
O	O
Y	O
c	O
I	O
0	B-DatasetName
Z	O
U	O
m	O
r	O
s	O
O	O
o	O
n	O
2	O
c	O
i	O
Q	O
1	O
x	O
Y	O
w	O
U	O
9	O
U	O
m	O
q	O
S	O
I	O
L	O
w	O
H	O
E	O
3	O
J	O
2	O
F	O
C	O
O	O
Y	O
q	O
K	O
8	O
f	O
B	O
G	O
+	O
g	O
B	O
d	O
G	O
C	O
W	O
E	O
k	O
p	O
D	O
l	O
c	O
w	O
4	O
X	O
6	O
e	O
y	O
N	O
H	O
s	O
S	O
r	O
j	O
m	O
c	O
k	O
Y	O
6	O
Z	O
l	O
a	O
9	O
U	O
r	O
x	O
P	O
2	O
+	O
c	O
6	O
u	O
j	O
G	O
y	O
y	O
l	O
P	O
U	O
k	O
0	B-DatasetName
4	O
X	O
j	O
4	O
U	O
p	O
Q	O
x	O
q	O
A	O
c	O
s	O
m	O
Y	O
E	O
g	O
l	O
w	O
Z	O
p	O
l	O
h	O
i	O
A	O
s	O
q	O
c	O
k	O
K	O
8	O
Q	O
x	O
J	O
h	O
L	O
X	O
p	O
q	O
2	O
5	O
K	O
c	O
F	O
e	O
/	O
v	O
E	O
4	O
G	O
7	O
Z	O
Z	O
7	O
1	O
W	O
r	O
f	O
d	O
5	O
r	O
d	O
T	O
l	O
V	O
H	O
D	O
Z	O
y	O
B	O
c	O
3	O
A	O
J	O
X	O
H	O
A	O
N	O
u	O
u	O
A	O
O	O
9	O
E	O
A	O
f	O
Y	O
J	O
C	O
B	O
Z	O
/	O
A	O
K	O
3	O
q	O
w	O
n	O
6	O
8	O
V	O
6	O
t	O
z	O
6	O
W	O
o	O
x	O
t	O
W	O
t	O
d	O
M	O
A	O
f	O
2	O
B	O
9	O
/	O
g	O
A	O
L	O
M	O
p	O
T	O
5	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
f	O
t	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
g	O
7	O
5	O
7	O
F	O
+	O
X	O
y	O
u	O
G	O
B	O
m	O
K	O
w	O
U	O
j	O
f	O
a	O
Z	O
q	O
y	O
B	O
p	O
Y	O
F	O
O	O
g	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
/	O
H	O
i	O
c	O
b	O
V	O
D	O
N	O
S	O
8	O
M	O
w	O
H	O
E	O
3	O
9	O
n	O
P	O
O	O
r	O
u	O
q	O
O	O
X	O
4	O
B	O
A	O
8	O
j	O
X	O
Y	O
O	O
9	O
D	O
j	O
w	O
4	O
n	O
G	O
C	O
+	O
4	O
C	O
t	O
l	O
D	O
R	O
N	O
t	O
7	O
A	O
0	B-DatasetName
K	O
U	O
k	O
q	O
l	O
F	O
L	O
/	O
F	O
S	O
8	O
e	O
F	O
P	O
H	O
q	O
H	O
+	O
L	O
N	O
/	O
8	O
Z	O
0	B-DatasetName
6	O
0	B-DatasetName
E	O
3	O
H	O
4	O
Q	O
8	O
3	O
v	O
v	O
9	O
y	O
M	O
s	O
L	O
E	O
k	O
a	O
V	O
d	O
p	O
x	O
v	O
a	O
2	O
N	O
z	O
a	O
3	O
t	O
n	O
t	O
7	O
Z	O
X	O
3	O
z	O
8	O
4	O
P	O
D	O
q	O
2	O
T	O
0	B-DatasetName
4	O
H	O
S	O
q	O
Q	O
S	O
k	O
z	O
4	O
W	O
T	O
M	O
h	O
R	O
g	O
B	O
R	O
h	O
l	O
J	O
O	O
+	O
p	O
p	O
q	O
R	O
U	O
S	O
I	O
J	O
i	O
g	O
N	O
G	O
h	O
s	O
H	O
8	O
t	O
v	O
S	O
H	O
j	O
0	B-DatasetName
Q	O
q	O
K	O
v	O
i	O
D	O
z	O
h	O
L	O
i	O
x	O
W	O
j	O
K	O
a	O
U	O
Q	O
x	O
0	B-DatasetName
k	O
b	O
y	O
7	O
U	O
Y	O
+	O
C	O
Q	O
Q	O
L	O
V	O
R	O
a	O
b	O
C	O
0	B-DatasetName
a	O
F	O
n	O
+	O
v	O
C	O
t	O
5	O
t	O
O	O
y	O
1	O
k	O
A	O
r	O
h	O
O	O
3	O
I	O
k	O
1	O
Q	O
o	O
e	O
f	O
b	O
X	O
5	O
N	O
Q	O
4	O
D	O
Q	O
m	O
X	O
G	O
O	O
G	O
l	O
B	O
q	O
7	O
T	O
q	O
K	O
9	O
H	O
E	O
l	O
N	O
M	O
S	O
N	O
F	O
f	O
Z	O
I	O
q	O
k	O
i	O
A	O
8	O
R	O
1	O
M	O
y	O
N	O
p	O
S	O
j	O
m	O
C	O
g	O
v	O
X	O
4	O
Q	O
v	O
4	O
I	O
V	O
R	O
Q	O
h	O
g	O
J	O
a	O
Q	O
7	O
X	O
c	O
K	O
H	O
+	O
3	O
s	O
h	O
R	O
r	O
M	O
p	O
4	O
Z	O
j	O
J	O
G	O
e	O
q	O
Z	O
W	O
v	O
V	O
L	O
8	O
z	O
x	O
u	O
n	O
O	O
r	O
r	O
x	O
c	O
s	O
q	O
T	O
V	O
B	O
O	O
O	O
l	O
w	O
9	O
F	O
K	O
Y	O
N	O
a	O
w	O
L	O
I	O
J	O
G	O
F	O
J	O
J	O
s	O
G	O
a	O
Z	O
I	O
Q	O
h	O
L	O
a	O
r	O
J	O
C	O
P	O
E	O
M	O
S	O
Y	O
W	O
3	O
6	O
q	O
p	O
s	O
S	O
3	O
N	O
U	O
v	O
r	O
5	O
N	O
B	O
u	O
+	O
V	O
e	O
t	O
d	O
r	O
3	O
n	O
W	O
a	O
3	O
U	O
9	O
V	O
R	O
A	O
2	O
f	O
g	O
H	O
F	O
w	O
C	O
F	O
1	O
y	O
D	O
L	O
r	O
g	O
D	O
P	O
d	O
A	O
H	O
G	O
G	O
T	O
g	O
G	O
b	O
y	O
C	O
N	O
+	O
v	O
J	O
e	O
r	O
H	O
e	O
r	O
Y	O
/	O
l	O
6	O
I	O
Z	O
V	O
7	O
T	O
T	O
A	O
H	O
1	O
i	O
f	O
P	O
2	O
9	O
8	O
l	O
T	O
s	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
=	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
.	O
.	O
.	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
e	O
3	O
5	O
2	O
g	O
W	O
f	O
r	O
l	O
v	O
f	O
1	O
6	O
w	O
M	O
E	O
b	O
X	O
2	O
S	O
1	O
Z	O
U	O
Q	O
C	O
Q	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
B	O
I	O
v	O
g	O
q	O
S	O
R	O
V	O
U	O
G	O
8	O
F	O
L	O
x	O
4	O
r	O
2	O
A	O
9	O
o	O
Q	O
9	O
l	O
s	O
N	O
u	O
3	O
a	O
z	O
W	O
7	O
Y	O
n	O
Q	O
i	O
l	O
9	O
D	O
9	O
4	O
8	O
a	O
C	O
I	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
4	O
Q	O
c	O
/	O
7	O
d	O
g	O
p	O
r	O
6	O
x	O
u	O
b	O
W	O
8	O
X	O
t	O
0	B-DatasetName
s	O
7	O
u	O
3	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
6	O
j	O
W	O
7	O
i	O
8	O
r	O
9	O
Z	O
s	O
8	O
j	O
i	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
K	O
D	O
z	O
C	O
M	O
7	O
z	O
C	O
m	O
6	O
O	O
c	O
F	O
+	O
f	O
d	O
+	O
V	O
i	O
0	B-DatasetName
F	O
p	O
x	O
8	O
5	O
h	O
j	O
+	O
w	O
P	O
n	O
8	O
A	O
b	O
q	O
4	O
j	O
z	O
M	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
s	O
1	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
m	O
Q	O
9	O
7	O
r	O
i	O
6	O
n	O
c	O
S	O
F	O
3	O
A	O
L	O
P	O
b	O
D	O
O	O
J	O
n	O
M	O
3	O
9	O
1	O
o	O
r	O
k	B-HyperparameterName
=	I-HyperparameterName
"	O
>	O
A	O
A	O
A	O
B	O
+	O
n	O
i	O
c	O
b	O
V	O
C	O
7	O
T	O
s	O
M	O
w	O
F	O
L	O
0	B-DatasetName
p	O
r	O
1	O
J	O
e	O
K	O
Y	O
w	O
s	O
F	O
h	O
U	O
S	O
U	O
5	O
U	O
U	O
J	O
G	O
C	O
r	O
x	O
M	O
J	O
Y	O
J	O
P	O
q	O
Q	O
2	O
i	O
h	O
y	O
H	O
L	O
e	O
1	O
6	O
j	O
i	O
R	O
7	O
Y	O
C	O
q	O
0	B-DatasetName
E	O
9	O
h	O
Y	O
Q	O
A	O
h	O
V	O
r	O
6	O
E	O
j	O
b	O
/	O
B	O
a	O
T	O
O	O
t+1	O
=	O
g	O
(	O
s	O
t	O
)	O
v	O
5	O
B	O
+	O
f	O
C	O
o	O
Z	O
V	O
S	O
m	O
K	O
W	O
t	O
S	O
J	O
Z	O
T	O
u	O
h	O
M	O
Q	O
w	O
w	O
S	O
V	O
r	O
I	O
k	O
f	O
B	O
O	O
q	O
l	O
m	O
J	O
A	O
k	O
F	O
a	O
4	O
e	O
j	O
2	O
5	O
n	O
f	O
f	O
m	O
L	O
a	O
c	O
C	O
U	O
f	O
c	O
J	O
y	O
y	O
I	O
C	O
E	O
D	O
y	O
W	O
N	O
O	O
C	O
V	O
q	O
p	O
1	O
R	O
O	O
R	O
Q	O
t	O
M	O
v	O
V	O
7	O
y	O
q	O
N	O
4	O
e	O
7	O
S	O
v	O
y	O
c	O
V	O
C	O
B	O
H	O
o	O
1	O
/	O
+	O
6	O
k	O
W	O
K	O
Z	O
g	O
m	O
T	O
S	O
A	O
U	O
x	O
p	O
u	O
t	O
7	O
K	O
Q	O
Y	O
T	O
o	O
p	O
F	O
T	O
w	O
a	O
a	O
l	O
X	O
m	O
Z	O
Y	O
S	O
u	O
i	O
I	O
D	O
F	O
j	O
X	O
U	O
k	O
k	O
S	O
Z	O
o	O
L	O
J	O
/	O
N	O
q	O
p	O
e	O
2	O
a	O
V	O
y	O
I	O
2	O
V	O
t	O
i	O
X	O
R	O
n	O
a	O
u	O
/	O
J	O
y	O
Y	O
k	O
M	O
W	O
a	O
c	O
h	O
L	O
Y	O
z	O
I	O
T	O
g	O
0	B-DatasetName
y	O
9	O
5	O
M	O
/	O
M	O
/	O
r	O
Z	O
h	O
h	O
f	O
B	O
x	O
M	O
u	O
0	B-DatasetName
w	O
y	O
Z	O
p	O
I	O
t	O
F	O
c	O
S	O
Z	O
c	O
V	O
O	O
7	O
s	O
d	O
T	O
f	O
i	O
m	O
l	O
E	O
U	O
Y	O
0	B-DatasetName
s	O
I	O
1	O
d	O
z	O
e	O
6	O
t	O
I	O
h	O
0	B-DatasetName
Y	O
S	O
i	O
D	O
a	O
h	O
k	O
Q	O
/	O
C	O
X	O
X	O
1	O
4	O
l	O
r	O
V	O
r	O
V	O
v	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
l	O
+	O
S	O
u	O
E	O
U	O
v	O
F	O
h	O
Z	O
d	O
t	O
P	O
G	O
c	O
f	O
L	O
D	O
R	O
X	O
6	O
U	O
r	O
4	O
l	O
+	O
s	O
=	O
"	O
>	O
A	O
by	O
an	O
encoder	O
network	O
.	O
Specifically	O
,	O
let	O
the	O
current	O
generated	O
sentence	O
be	O
Y	O
1	O
...	O
t	O
(	O
encouraged	O
to	O
be	O
the	O
same	O
as	O
parts	O
of	O
a	O
training	O
sentence	O
in	O
training	O
)	O
,	O
with	O
f	O
t	O
calculated	O
as	O
:	O
A	O
A	O
C	O
F	O
X	O
i	O
c	O
b	O
V	O
D	O
L	O
S	O
g	O
M	O
x	O
F	O
M	O
3	O
U	O
V	O
6	O
2	O
v	O
U	O
Z	O
d	O
u	O
g	O
k	O
W	O
o	O
K	O
G	O
W	O
m	O
C	O
u	O
p	O
C	O
K	O
L	O
h	O
x	O
Z	O
w	O
X	O
7	O
g	O
H	O
Y	O
Y	O
M	O
p	O
m	O
0	B-DatasetName
D	O
c	O
0	B-DatasetName
8	O
S	O
O	O
4	O
I	O
Z	O
Z	O
i	O
f	O
c	O
O	O
O	O
v	O
u	O
H	O
G	O
h	O
i	O
F	O
v	O
B	O
n	O
X	O
9	O
j	O
+	O
l	O
j	O
Y	O
1	O
g	O
M	O
h	O
h	O
3	O
P	O
u	O
5	O
d	O
5	O
7	O
v	O
F	O
h	O
w	O
B	O
Z	O
b	O
1	O
Y	O
+	O
S	O
W	O
l	O
l	O
d	O
W	O
1	O
/	O
L	O
r	O
h	O
Y	O
3	O
N	O
r	O
e	O
0	B-DatasetName
d	O
c	O
3	O
e	O
v	O
o	O
a	O
J	O
E	O
U	O
l	O
a	O
n	O
k	O
Y	O
h	O
k	O
y	O
y	O
O	O
K	O
C	O
R	O
6	O
y	O
O	O
n	O
A	O
Q	O
r	O
B	O
V	O
L	O
R	O
g	O
J	O
P	O
s	O
K	O
Y	O
3	O
u	O
B	O
n	O
5	O
z	O
U	O
c	O
m	O
F	O
Y	O
/	O
C	O
B	O
x	O
j	O
G	O
z	O
A	O
l	O
I	O
L	O
+	O
R	O
d	O
T	O
g	O
l	O
o	O
y	O
T	O
V	O
P	O
0	B-DatasetName
4	O
4	O
X	O
C	O
V	O
8	O
N	O
A	O
/	O
3	O
h	O
u	O
8	O
x	O
N	O
4	O
c	O
T	O
O	O
8	O
D	O
X	O
u	O
l	O
W	O
Y	O
c	O
l	O
b	O
l	O
w	O
7	O
J	O
p	O
F	O
q	O
2	O
y	O
N	O
g	O
R	O
e	O
J	O
P	O
S	O
V	O
F	O
N	O
E	O
X	O
N	O
N	O
b	O
8	O
7	O
f	O
k	O
S	O
T	O
g	O
I	O
V	O
A	O
B	O
V	O
G	O
q	O
b	O
V	O
s	O
x	O
O	O
C	O
m	O
R	O
w	O
K	O
l	O
g	O
W	O
a	O
G	O
T	O
K	O
B	O
Y	O
T	O
O	O
i	O
A	O
9	O
1	O
t	O
Y	O
0	B-DatasetName
J	O
A	O
F	O
T	O
T	O
j	O
q	O
+	O
K	O
s	O
N	O
H	O
W	O
v	O
F	O
x	O
N	O
5	O
L	O
6	O
h	O
Y	O
D	O
H	O
6	O
t	O
+	O
O	O
l	O
A	O
R	O
q	O
t	O
J	O
2	O
u	O
D	O
A	O
j	O
0	B-DatasetName
1	O
b	O
w	O
3	O
E	O
v	O
/	O
z	O
2	O
g	O
l	O
0	B-DatasetName
L	O
5	O
2	O
U	O
h	O
3	O
E	O
C	O
L	O
K	O
S	O
T	O
Q	O
d	O
1	O
E	O
Y	O
I	O
j	O
w	O
K	O
C	O
L	O
s	O
c	O
8	O
k	O
o	O
i	O
K	O
E	O
m	O
h	O
E	O
q	O
u	O
d	O
8	O
W	O
0	B-DatasetName
T	O
y	O
S	O
h	O
o	O
I	O
M	O
s	O
6	O
B	O
D	O
s	O
+	O
Z	O
M	O
X	O
S	O
a	O
N	O
S	O
t	O
s	O
/	O
K	O
l	O
f	O
v	O
z	O
Y	O
v	O
V	O
q	O
G	O
k	O
c	O
f	O
t	O
=	O
Enc	O
(	O
Y	O
1	O
...	O
t	O
)	O
.	O
The	O
initial	O
state	O
of	O
the	O
guider	O
network	O
is	O
the	O
encoded	O
feature	O
of	O
a	O
true	O
input	O
sentence	O
by	O
the	O
same	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
,	O
i.e.	O
,	O
s	O
G	O
0	B-DatasetName
=	O
Enc	O
(	O
X	O
)	O
,	O
where	O
Enc	O
(	O
)	O
denotes	O
the	O
encoder	O
transformation	O
,	O
implemented	O
with	O
a	O
CNN	O
.	O
Importantly	O
,	O
the	O
input	O
to	O
the	O
guider	O
network	O
,	O
at	O
each	O
time	O
point	O
,	O
is	O
defined	O
by	O
features	O
from	O
the	O
entire	O
sentence	O
generated	O
to	O
that	O
point	O
.	O
This	O
provides	O
an	O
important	O
"	O
guide	O
"	O
to	O
the	O
LSTM	B-MethodName
decoder	O
,	O
accounting	O
for	O
the	O
global	O
properties	O
of	O
the	O
generated	O
text	O
.	O
Text	B-TaskName
Generation	I-TaskName
with	O
Planning	O
We	O
first	O
explain	O
how	O
one	O
uses	O
the	O
guider	O
network	O
to	O
guide	O
next	O
-	O
word	O
generation	O
for	O
the	O
generator	O
(	O
the	O
LSTM	B-MethodName
decoder	O
in	O
Figure	O
1	O
)	O
.	O
Our	O
framework	O
is	O
inspired	O
by	O
the	O
MPC	O
method	O
(	O
Nagabandi	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
can	O
be	O
regarded	O
as	O
a	O
type	O
of	O
plan	O
-	O
ahead	O
attention	O
mechanism	O
.	O
Given	O
the	O
feature	O
f	O
t	O
at	O
time	O
t	O
from	O
the	O
current	O
input	O
sentence	O
,	O
the	O
guider	O
network	O
produces	O
a	O
prediction	O
G	O
ψ	O
(	O
s	O
G	O
t−1	O
,	O
f	O
t	O
)	O
as	O
a	O
future	O
feature	O
representation	O
,	O
by	O
feeding	O
f	O
t	O
into	O
the	O
LSTM	B-MethodName
guider	O
.	O
Since	O
the	O
training	O
of	O
the	O
guider	O
network	O
is	O
based	O
on	O
real	O
data	O
(	O
detailed	O
in	O
the	O
next	O
paragraph	O
)	O
,	O
the	O
predicted	O
feature	O
contains	O
global	O
-	O
structure	O
information	O
of	O
the	O
training	O
sentences	O
.	O
To	O
utilize	O
such	O
information	O
to	O
predict	O
the	O
next	O
word	O
,	O
we	O
combine	O
the	O
predicted	O
feature	O
with	O
the	O
output	O
of	O
the	O
decoder	O
by	O
constructing	O
an	O
attention	O
-	O
like	O
mechanism	O
.	O
Specifically	O
,	O
we	O
first	O
apply	O
a	O
linear	O
transformation	O
ϕ	O
on	O
the	O
predicted	O
feature	O
G	O
ψ	O
(	O
s	O
G	O
t−1	O
,	O
f	O
t	O
)	O
,	O
forming	O
a	O
weight	O
vector	O
w	O
t	O
ϕ	O
G	O
ψ	O
(	O
s	O
G	O
t−1	O
,	O
f	O
t	O
)	O
.	O
The	O
weight	O
w	O
t	O
is	O
applied	O
to	O
the	O
output	O
O	O
t	O
of	O
the	O
LSTM	B-MethodName
decoder	O
by	O
an	O
element	O
-	O
wise	O
multiplication	O
operation	O
.	O
The	O
result	O
is	O
then	O
fed	O
into	O
a	O
softmax	B-MethodName
layer	O
to	O
generate	O
the	O
next	O
token	O
y	O
t	O
.	O
Formally	O
,	O
the	O
generative	O
process	O
is	O
written	O
as	O
:	O
O	O
t	O
=	O
g	O
(	O
s	O
t−1	O
)	O
,	O
w	O
t	O
=	O
ϕ	O
(	O
G	O
ψ	O
(	O
s	O
G	O
t−1	O
,	O
f	O
t	O
)	O
)	O
,	O
(	O
5	O
)	O
y	O
t	O
∼	O
Multi	O
(	O
1	O
,	O
softmax	B-MethodName
(	O
O	O
t	O
w	O
t	O
)	O
)	O
,	O
(	O
6	O
)	O
s	O
G	O
t	O
=	O
h	O
G	O
(	O
s	O
G	O
t−1	O
,	O
f	O
t	O
)	O
,	O
s	O
t	O
=	O
h	O
(	O
s	O
t−1	O
,	O
e	O
(	O
y	O
t	O
)	O
)	O
.	O
(	O
7	O
)	O
Guider	O
Network	O
Training	O
Given	O
a	O
sentence	O
of	O
feature	O
representations	O
(	O
f	O
1	O
,	O
f	O
2	O
,	O
.	O
.	O
.	O
f	O
T	O
)	O
for	O
a	O
training	O
sentence	O
,	O
we	O
seek	O
to	O
update	O
the	O
guider	O
network	O
such	O
that	O
it	O
is	O
able	O
to	O
predict	O
f	O
t+c	O
given	O
f	O
t	O
,	O
where	O
c	O
>	O
0	B-DatasetName
is	O
the	O
number	O
of	O
steps	O
that	O
are	O
looked	O
ahead	O
.	O
We	O
implement	O
this	O
by	O
forcing	O
the	O
predicted	O
feature	O
,	O
G	O
ψ	O
(	O
s	O
G	O
t	O
,	O
f	O
t	O
)	O
,	O
to	O
match	O
both	O
the	O
sentence	O
feature	O
f	O
t+c	O
(	O
first	O
term	O
in	O
(	O
8	O
)	O
)	O
and	O
the	O
corresponding	O
feature	O
-	O
changing	O
direction	O
(	O
second	O
term	O
in	O
(	O
8	O
)	O
)	O
.	O
This	O
is	O
formalized	O
by	O
maximizing	O
an	O
objective	O
function	O
of	O
the	O
following	O
form	O
at	O
time	O
t	O
:	O
J	O
ψ	O
G	O
=	O
D	O
cos	O
f	O
t+c	O
,	O
G	O
ψ	O
(	O
s	O
G	O
t−1	O
,	O
f	O
t	O
)	O
(	O
8	O
)	O
+	O
D	O
cos	O
f	O
t+c	O
−	O
f	O
t	O
,	O
G	O
ψ	O
(	O
s	O
G	O
t−1	O
,	O
f	O
t	O
)	O
−	O
f	O
t	O
,	O
where	O
D	O
cos	O
(	O
,	O
)	O
denotes	O
the	O
cosine	O
similarity	O
2	O
.	O
By	O
maximizing	O
(	O
8	O
)	O
,	O
an	O
ideal	O
guider	O
network	O
should	O
be	O
able	O
to	O
predict	O
the	O
true	O
next	O
words	O
conditioned	O
on	O
the	O
current	O
word	O
in	O
a	O
sentence	O
.	O
As	O
a	O
result	O
,	O
the	O
prediction	O
is	O
used	O
to	O
construct	O
an	O
intermediate	O
reward	O
,	O
used	O
to	O
update	O
the	O
generator	O
(	O
the	O
LSTM	B-MethodName
decoder	O
)	O
,	O
as	O
described	O
further	O
below	O
.	O

As	O
in	O
many	O
RL	O
-	O
based	O
text	O
-	O
generation	O
methods	O
,	O
such	O
as	O
SeqGAN	O
and	O
LeakGAN	O
(	O
Guo	O
et	O
al	O
,	O
2017	O
)	O
,	O
the	O
generator	O
is	O
updated	O
based	O
on	O
policy	O
-	O
gradient	O
methods	O
.	O
As	O
a	O
result	O
,	O
collecting	O
rewards	O
in	O
the	O
generation	O
process	O
is	O
critical	O
.	O
Though	O
SeqGAN	O
has	O
proposed	O
to	O
use	O
rollout	O
to	O
get	O
rewards	O
for	O
each	O
generated	O
word	O
,	O
the	O
variance	O
of	O
the	O
rewards	O
is	O
typically	O
too	O
high	O
to	O
be	O
useful	O
practically	O
.	O
In	O
addition	O
,	O
the	O
computational	O
cost	O
may	O
be	O
too	O
high	O
for	O
practical	O
use	O
.	O
We	O
below	O
describe	O
how	O
to	O
use	O
the	O
proposed	O
guider	O
network	O
to	O
define	O
intermediate	O
rewards	O
,	O
leading	O
to	O
a	O
definition	O
of	O
feature	O
-	O
matching	O
reward	O
.	O
Feature	O
-	O
Matching	O
Rewards	O
We	O
first	O
define	O
an	O
intermediate	O
reward	O
to	O
generate	O
a	O
particular	O
word	O
.	O
The	O
idea	O
is	O
to	O
match	O
the	O
ground	O
-	O
truth	O
features	O
from	O
the	O
CNN	O
encoder	O
in	O
Figure	O
1	O
with	O
those	O
generated	O
from	O
the	O
guider	O
network	O
.	O
Equation	O
(	O
8	O
)	O
indicates	O
that	O
the	O
further	O
the	O
generated	O
feature	O
is	O
from	O
the	O
true	O
feature	O
,	O
the	O
smaller	O
the	O
reward	O
should	O
be	O
.	O
To	O
this	O
end	O
,	O
for	O
each	O
time	O
t	O
,	O
we	O
define	O
the	O
intermediate	O
reward	O
for	O
generating	O
the	O
current	O
word	O
as	O
:	O
r	O
g	O
t	O
=	O
1	O
2c	O
c	O
i=1	O
(	O
D	O
cos	O
(	O
f	O
t	O
,	O
f	O
t	O
)	O
+	O
D	O
cos	O
(	O
f	O
t	O
−	O
f	O
t−i	O
,	O
f	O
t	O
−	O
f	O
t−i	O
)	O
)	O
,	O
wheref	O
t	O
=	O
G	O
ψ	O
(	O
s	O
G	O
t−c−1	O
,	O
f	O
t−c	O
)	O
is	O
the	O
predicted	O
feature	O
.	O
Intuitively	O
,	O
f	O
t	O
−	O
f	O
t−i	O
measures	O
the	O
difference	O
between	O
the	O
generated	O
sentences	O
in	O
feature	O
space	O
;	O
the	O
reward	O
is	O
high	O
if	O
it	O
matches	O
the	O
predicted	O
feature	O
transitionf	O
t	O
−	O
f	O
t−i	O
from	O
the	O
guider	O
network	O
.	O
At	O
the	O
last	O
step	O
of	O
text	B-TaskName
generation	I-TaskName
,	O
i.e.	O
,	O
t	O
=	O
T	O
,	O
the	O
corresponding	O
reward	O
measures	O
the	O
quality	O
of	O
the	O
whole	O
generated	O
sentence	O
,	O
thus	O
it	O
is	O
called	O
a	O
final	O
reward	O
.	O
The	O
final	O
reward	O
is	O
defined	O
differently	O
from	O
the	O
intermediate	O
reward	O
,	O
discussed	O
below	O
for	O
both	O
the	O
unconditional	O
-	O
and	O
conditional	O
-	O
generation	O
cases	O
.	O
Note	O
that	O
a	O
token	O
generated	O
at	O
time	O
t	O
will	O
influence	O
not	O
only	O
the	O
rewards	O
received	O
at	O
that	O
time	O
but	O
also	O
the	O
rewards	O
at	O
subsequent	O
time	O
steps	O
.	O
Thus	O
we	O
propose	O
to	O
define	O
the	O
cumulative	O
reward	O
,	O
T	O
i	O
=	O
t	O
γ	B-HyperparameterName
i	O
r	O
g	O
i	O
with	O
γ	B-HyperparameterName
a	O
discount	O
factor	O
,	O
as	O
a	O
featurematching	O
reward	O
.	O
Intuitively	O
,	O
this	O
encourages	O
the	O
generator	O
to	O
focus	O
on	O
achieving	O
higher	O
long	O
-	O
term	O
rewards	O
.	O
Finally	O
,	O
in	O
order	O
to	O
apply	O
policy	O
gradient	O
to	O
update	O
the	O
generator	O
,	O
we	O
combine	O
the	O
featurematching	O
reward	O
with	O
the	O
problem	O
-	O
specific	O
final	O
reward	O
,	O
to	O
form	O
a	O
Q	O
-	O
value	O
reward	O
specified	O
below	O
.	O
Similar	O
to	O
SeqGAN	O
,	O
the	O
final	O
reward	O
is	O
defined	O
as	O
the	O
output	O
of	O
a	O
discriminator	O
,	O
evaluating	O
the	O
quality	O
of	O
the	O
whole	O
generated	O
sentence	O
,	O
i.e.	O
,	O
the	O
smaller	O
the	O
output	O
,	O
the	O
less	O
likely	O
the	O
generation	O
is	O
a	O
true	O
sentence	O
.	O
As	O
a	O
result	O
,	O
we	O
combine	O
the	O
adversarial	O
reward	O
r	O
f	O
[	O
0	B-DatasetName
,	O
1	O
]	O
by	O
the	O
discriminator	O
(	O
Yu	O
et	O
al	O
Generate	O
a	O
sequence	O
Y	O
1	O
...	O
T	O
∼	O
π	O
φ	O
.	O

Compute	O
Q	O
t	O
,	O
and	O
update	O
π	O
φ	O
.	O
8	O
:	O
end	O
while	O
2017	O
)	O
with	O
the	O
guider	O
-	O
matching	O
rewards	O
,	O
to	O
define	O
a	O
Q	O
-	O
value	O
reward	O
as	O
Q	O
t	O
=	O
(	O
T	O
i	O
=	O
t	O
γ	B-HyperparameterName
i	O
r	O
g	O
i	O
)	O
×	O
r	O
f	O
.	O
Generator	O
Optimization	O
The	O
generator	O
is	O
initialized	O
by	O
pre	O
-	O
training	O
on	O
sentences	O
with	O
an	O
autoencoder	B-MethodName
structure	O
,	O
based	O
on	O
MLE	O
training	O
.	O
After	O
that	O
,	O
the	O
final	O
Q	O
-	O
value	O
reward	O
Q	O
t	O
is	O
used	O
as	O
a	O
reward	O
for	O
each	O
time	O
t	O
,	O
with	O
standard	O
policy	O
gradient	O
optimization	O
methods	O
to	O
update	O
the	O
generator	O
.	O
Specifically	O
,	O
the	O
policy	O
gradient	O
is	O
∇	O
φ	O
J	O
=	O
E	O
(	O
s	O
t−1	O
,	O
yt	O
)	O
∼ρπ	O
[	O
Q	O
t	O
∇	O
φ	O
log	O
p	O
(	O
y	O
t	O
|	O
s	O
t−1	O
;	O
φ	O
,	O
ϕ	O
)	O
]	O
,	O
∇	O
ϕ	O
J	O
=	O
E	O
(	O
s	O
t−1	O
,	O
yt	O
)	O
∼ρπ	O
[	O
Q	O
t	O
∇	O
ϕ	O
log	O
p	O
(	O
y	O
t	O
|	O
s	O
t−1	O
;	O
φ	O
,	O
ϕ	O
)	O
]	O
,	O
where	O
p	O
(	O
y	O
t	O
|	O
s	O
t−1	O
;	O
φ	O
,	O
ϕ	O
)	O
is	O
the	O
probability	O
of	O
generating	O
y	O
t	O
given	O
s	O
t−1	O
in	O
the	O
generator	O
.	O
Algorithm	O
1	O
describes	O
the	O
proposed	O
model	O
-	O
based	O
imitation	B-TaskName
learning	I-TaskName
framework	O
for	O
text	B-TaskName
generation	I-TaskName
.	O
Model	O
-	O
based	O
or	O
Model	O
-	O
free	O
Text	B-TaskName
generation	I-TaskName
seeks	O
to	O
generate	O
the	O
next	O
word	O
(	O
action	O
)	O
given	O
the	O
current	O
(	O
sub	O
-	O
)	O
sentence	O
(	O
state	O
)	O
.	O
The	O
generator	O
is	O
considered	O
as	O
an	O
agent	B-DatasetName
that	O
learns	O
a	O
policy	O
to	O
predict	O
the	O
next	O
word	O
given	O
its	O
current	O
state	O
.	O
In	O
previous	O
work	O
(	O
Ranzato	O
et	O
al	O
,	O
2016	O
)	O
,	O
a	O
metric	O
reward	O
is	O
given	O
and	O
the	O
generator	O
is	O
trained	O
to	O
only	O
maximize	O
the	O
metric	O
reward	O
by	O
trial	O
,	O
thus	O
this	O
is	O
model	O
-	O
free	O
learning	O
.	O
In	O
the	O
proposed	O
method	O
,	O
the	O
guider	O
network	O
models	O
the	O
environment	O
dynamics	O
,	O
and	O
is	O
trained	O
by	O
minimizing	O
the	O
cosine	O
similarity	O
between	O
the	O
prediction	O
and	O
the	O
ground	O
truth	O
on	O
real	O
text	O
.	O
For	O
generator	O
training	O
,	O
it	O
maximizes	O
the	O
reward	O
which	O
is	O
determined	O
by	O
the	O
metric	O
and	O
guider	O
network	O
,	O
and	O
thus	O
is	O
model	O
-	O
free	O
learning	O
with	O
model	O
-	O
based	O
boosting	O
(	O
Gu	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
model	O
predictive	O
control	O
scheme	O
is	O
included	O
in	O
our	O
method	O
,	O
where	O
the	O
guider	O
network	O
is	O
used	O
to	O
help	O
next	O
-	O
word	O
selection	O
at	O
each	O
time	O
-	O
step	O
.	O
I	O
x	O
L	O
1	O
g	O
O	O
O	O
U	O
B	O
z	O
E	O
d	O
K	O
B	O
E	O
J	O
R	O
t	O
F	O
K	O
r	O
X	O
E	O
v	O
x	O
z	O
N	O
v	O
0	B-DatasetName
q	O
v	O
W	O
3	O
L	O
o	O
7	O
A	O
1	O
k	O
m	O
X	O
k	O
F	O
q	O
U	O
K	O
D	O
Z	O
q	O
3	O
5	O
1	O
+	O
w	O
n	O
L	O
Y	O
q	O
6	O
Q	O
S	O
W	O
p	O
M	O
x	O
3	O
N	O
T	O
D	O
H	O
K	O
q	O
U	O
T	O
D	O
J	O
J	O
5	O
V	O
u	O
Z	O
n	O
h	O
K	O
2	O
Y	O
g	O
O	O
e	O
M	O
d	O
S	O
R	O
W	O
N	O
u	O
g	O
n	O
x	O
2	O
7	O
o	O
S	O
c	O
W	O
K	O
V	O
P	O
o	O
k	O
T	O
b	O
U	O
k	O
h	O
m	O
6	O
u	O
+	O
J	O
n	O
M	O
b	O
G	O
j	O
O	O
P	O
Q	O
d	O
s	O
Y	O
U	O
h	O
2	O
b	O
R	O
m	O
4	O
r	O
/	O
e	O
Z	O
0	B-DatasetName
M	O
o	O
6	O
s	O
g	O
F	O
y	O
r	O
N	O
k	O
C	O
s	O
2	O
X	O
x	O
R	O
l	O
k	O
m	O
B	O
C	O
p	O
r	O
+	O
T	O
v	O
t	O
C	O
c	O
o	O
R	O
x	O
b	O
Q	O
p	O
k	O
W	O
9	O
l	O
b	O
C	O
h	O
l	O
R	O
T	O
h	O
j	O
a	O
h	O
i	O
g	O
3	O
B	O
W	O
3	O
x	O
5	O
m	O
f	O
j	O
n	O
9	O
e	O
u	O
6	O
d	O
3	O
9	O
R	O
a	O
9	O
w	O
U	O
a	O
Z	O
T	O
h	O
C	O
I	O
7	O
h	O
F	O
D	O
y	O
4	O
h	O
A	O
b	O
c	O
Q	O
R	O
N	O
8	O
Y	O
D	O
C	O
C	O
Z	O
3	O
i	O
F	O
N	O
y	O
d	O
1	O
X	O
p	O
x	O
3	O
5	O
2	O
P	O
e	O
W	O
n	O
K	O
K	O
m	O
U	O
P	O
4	O
A	O
+	O
f	O
z	O
B	O
3	O
1	O
D	O
j	O
z	O
Q	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
a	O
k	O
G	O
l	O
X	O
l	O
B	O
3	O
d	O
d	O
W	O
H	O
T	O
a	O
2	O
P	O
N	O
m	O
t	O
m	O
K	O
s	O
T	O
F	O
H	O
I	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
Z	O
B	O
E	O
E	O
o	O
i	O
g	O
n	O
o	O
r	O
e	O
v	O
F	O
Y	O
w	O
d	O
h	O
C	O
G	O
8	O
p	O
m	O
u	O
2	O
m	O
X	O
b	O
j	O
Z	O
h	O
d	O
y	O
K	O
U	O
0	B-DatasetName
B	O
/	O
h	O
x	O
Y	O
O	O
K	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
F	O
Q	O
d	O
f	O
9	O
d	O
k	O
o	O
r	O
q	O
2	O
v	O
r	O
G	O
+	O
X	O
N	O
y	O
t	O
b	O
2	O
z	O
u	O
5	O
e	O
d	O
f	O
/	O
g	O
0	B-DatasetName
S	O
S	O
Z	O
Z	O
t	O
x	O
n	O
i	O
U	O
x	O
0	B-DatasetName
O	O
6	O
S	O
G	O
S	O
6	O
G	O
4	O
j	O
w	O
I	O
l	O
b	O
6	O
e	O
a	O
0	B-DatasetName
z	O
i	O
U	O
v	O
B	O
W	O
O	O
b	O
q	O
d	O
+	O
6	O
4	O
l	O
r	O
I	O
x	O
L	O
1	O
g	O
O	O
O	O
U	O
B	O
z	O
E	O
d	O
K	O
B	O
E	O
J	O
R	O
t	O
F	O
K	O
r	O
X	O
E	O
v	O
x	O
z	O
N	O
v	O
0	B-DatasetName
q	O
v	O
W	O
3	O
L	O
o	O
7	O
A	O
1	O
k	O
m	O
X	O
k	O
F	O
q	O
U	O
K	O
D	O
Z	O
q	O
3	O
5	O
1	O
+	O
w	O
n	O
L	O
Y	O
q	O
6	O
Q	O
S	O
W	O
p	O
M	O
x	O
3	O
N	O
T	O
D	O
H	O
K	O
q	O
U	O
T	O
D	O
J	O
J	O
5	O
V	O
u	O
Z	O
n	O
h	O
K	O
2	O
Y	O
g	O
O	O
e	O
M	O
d	O
S	O
R	O
W	O
N	O
u	O
g	O
n	O
x	O
2	O
7	O
o	O
S	O
c	O
W	O
K	O
V	O
P	O
o	O
k	O
T	O
b	O
U	O
k	O
h	O
m	O
6	O
u	O
+	O
J	O
n	O
M	O
b	O
G	O
j	O
O	O
P	O
Q	O
d	O
s	O
Y	O
U	O
h	O
2	O
b	O
R	O
m	O
4	O
r	O
/	O
e	O
Z	O
0	B-DatasetName
M	O
o	O
6	O
s	O
g	O
F	O
y	O
r	O
N	O
k	O
C	O
s	O
2	O
X	O
x	O
R	O
l	O
k	O
m	O
B	O
C	O
p	O
r	O
+	O
T	O
v	O
t	O
C	O
c	O
o	O
R	O
x	O
b	O
Q	O
p	O
k	O
W	O
9	O
l	O
b	O
C	O
h	O
l	O
R	O
T	O
h	O
j	O
a	O
h	O
i	O
g	O
3	O
B	O
W	O
3	O
x	O
5	O
m	O
f	O
j	O
n	O
9	O
e	O
u	O
6	O
d	O
3	O
9	O
R	O
a	O
9	O
w	O
U	O
a	O
Z	O
T	O
h	O
C	O
I	O
7	O
h	O
F	O
D	O
y	O
4	O
h	O
A	O
b	O
c	O
Q	O
R	O
N	O
8	O
Y	O
D	O
C	O
C	O
Z	O
3	O
i	O
F	O
N	O
y	O
d	O
1	O
X	O
p	O
x	O
3	O
5	O
2	O
P	O
e	O
W	O
n	O
K	O
K	O
m	O
U	O
P	O
4	O
A	O
+	O
f	O
z	O
B	O
3	O
1	O
D	O
j	O
z	O
Q	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
a	O
k	O
G	O
l	O
X	O
l	O
B	O
3	O
d	O
d	O
W	O
H	O
T	O
a	O
2	O
P	O
N	O
m	O
t	O
m	O
K	O
s	O
T	O
F	O
H	O
I	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
Z	O
B	O
E	O
E	O
o	O
i	O
g	O
n	O
o	O
r	O
e	O
v	O
F	O
Y	O
w	O
d	O
h	O
C	O
G	O
8	O
p	O
m	O
u	O
2	O
m	O
X	O
b	O
j	O
Z	O
h	O
d	O
y	O
K	O
U	O
0	B-DatasetName
B	O
/	O
h	O
x	O
Y	O
O	O
K	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
F	O
Q	O
d	O
f	O
9	O
d	O
k	O
o	O
r	O
q	O
2	O
v	O
r	O
G	O
+	O
X	O
N	O
y	O
t	O
b	O
2	O
z	O
u	O
5	O
e	O
d	O
f	O
/	O
g	O
0	B-DatasetName
S	O
S	O
Z	O
Z	O
t	O
x	O
n	O
i	O
U	O
x	O
0	B-DatasetName
O	O
6	O
S	O
G	O
S	O
6	O
G	O
4	O
j	O
w	O
I	O
l	O
b	O
6	O
e	O
a	O
0	B-DatasetName
z	O
i	O
U	O
v	O
B	O
W	O
O	O
b	O
q	O
d	O
+	O
6	O
4	O
l	O
r	O
I	O
x	O
L	O
1	O
g	O
O	O
O	O
U	O
B	O
z	O
E	O
d	O
K	O
B	O
E	O
J	O
R	O
t	O
F	O
K	O
r	O
X	O
E	O
v	O
x	O
z	O
N	O
v	O
0	B-DatasetName
q	O
v	O
W	O
3	O
L	O
o	O
7	O
A	O
1	O
k	O
m	O
X	O
k	O
F	O
q	O
U	O
K	O
D	O
Z	O
q	O
3	O
5	O
1	O
+	O
w	O
n	O
L	O
Y	O
q	O
6	O
Q	O
S	O
W	O
p	O
M	O
x	O
3	O
N	O
T	O
D	O
H	O
K	O
q	O
U	O
T	O
D	O
J	O
J	O
5	O
V	O
u	O
Z	O
n	O
h	O
K	O
2	O
Y	O
g	O
O	O
e	O
M	O
d	O
S	O
R	O
W	O
N	O
u	O
g	O
n	O
x	O
2	O
7	O
o	O
S	O
c	O
W	O
K	O
V	O
P	O
o	O
k	O
T	O
b	O
U	O
k	O
h	O
m	O
6	O
u	O
+	O
J	O
n	O
M	O
b	O
G	O
j	O
O	O
P	O
Q	O
d	O
s	O
Y	O
U	O
h	O
2	O
b	O
R	O
m	O
4	O
r	O
/	O
e	O
Z	O
0	B-DatasetName
M	O
o	O
6	O
s	O
g	O
F	O
y	O
r	O
N	O
k	O
C	O
s	O
2	O
X	O
x	O
R	O
l	O
k	O
m	O
B	O
C	O
p	O
r	O
+	O
T	O
v	O
t	O
C	O
c	O
o	O
R	O
x	O
b	O
Q	O
p	O
k	O
W	O
9	O
l	O
b	O
C	O
h	O
l	O
R	O
T	O
h	O
j	O
a	O
h	O
i	O
g	O
3	O
B	O
W	O
3	O
x	O
5	O
m	O
f	O
j	O
n	O
9	O
e	O
u	O
6	O
d	O
3	O
9	O
R	O
a	O
9	O
w	O
U	O
a	O
Z	O
T	O
h	O
C	O
I	O
7	O
h	O
F	O
D	O
y	O
4	O
h	O
A	O
b	O
c	O
Q	O
R	O
N	O
8	O
Y	O
D	O
C	O
C	O
Z	O
3	O
i	O
F	O
N	O
y	O
d	O
1	O
X	O
p	O
x	O
3	O
5	O
2	O
P	O
e	O
W	O
n	O
K	O
K	O
m	O
U	O
P	O
4	O
A	O
+	O
f	O
z	O
B	O
3	O
1	O
D	O
j	O
z	O
Q	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
a	O
k	O
G	O
l	O
X	O
l	O
B	O
3	O
d	O
d	O
W	O
H	O
T	O
a	O
2	O
P	O
N	O
m	O
t	O
m	O
K	O
s	O
T	O
F	O
H	O
I	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
7	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
Z	O
B	O
E	O
E	O
o	O
i	O
g	O
n	O
o	O
r	O
e	O
v	O
F	O
Y	O
w	O
d	O
h	O
C	O
G	O
8	O
p	O
m	O
u	O
2	O
m	O
X	O
b	O
j	O
Z	O
h	O
d	O
y	O
K	O
U	O
0	B-DatasetName
B	O
/	O
h	O
x	O
Y	O
O	O
K	O
V	O
/	O
+	O
P	O
N	O
/	O
+	O
N	O
2	O
z	O
Y	O
H	O
b	O
X	O
0	B-DatasetName
w	O
8	O
H	O
h	O
v	O
h	O
p	O
l	O
5	O
Y	O
S	O
q	O
F	O
Q	O
d	O
f	O
9	O
d	O
k	O
o	O
r	O
q	O
2	O
v	O
r	O
G	O
+	O
X	O
N	O
y	O
t	O
b	O
2	O
z	O
u	O
5	O
e	O
d	O
f	O
/	O
g	O
0	B-DatasetName
S	O
S	O
Z	O
Z	O
t	O
x	O
n	O
i	O
U	O
x	O
0	B-DatasetName
O	O
6	O
S	O
G	O
S	O
6	O
G	O
4	O
j	O
w	O
I	O
l	O
b	O
6	O
e	O
a	O
0	B-DatasetName
z	O
i	O
U	O
v	O
B	O
W	O
O	O
b	O
q	O
d	O
+	O
6	O
4	O
l	O
r	O
I	O
x	O
L	O
1	O
g	O
O	O
O	O
U	O
B	O
z	O
E	O
d	O
K	O
B	O
E	O
J	O
R	O
t	O
F	O
K	O
r	O
X	O
E	O
v	O
x	O
z	O
N	O
v	O
0	B-DatasetName
q	O
v	O
W	O
3	O
L	O
o	O
7	O
A	O
1	O
k	O
m	O
X	O
k	O
F	O
q	O
U	O
K	O
D	O
Z	O
q	O
3	O
5	O
1	O
+	O
w	O
n	O
L	O
Y	O
q	O
6	O
Q	O
S	O
W	O
p	O
M	O
x	O
3	O
N	O
T	O
D	O
H	O
K	O
q	O
U	O
T	O
D	O
J	O
J	O
5	O
V	O
u	O
Z	O
n	O
h	O
K	O
2	O
Y	O
g	O
O	O
e	O
M	O
d	O
S	O
R	O
W	O
N	O
u	O
g	O
n	O
x	O
2	O
7	O
o	O
S	O
c	O
W	O
K	O
V	O
P	O
o	O
k	O
T	O
b	O
U	O
k	O
h	O
m	O
6	O
u	O
+	O
J	O
n	O
M	O
b	O
G	O
j	O
O	O
P	O
Q	O
d	O
s	O
Y	O
U	O
h	O
2	O
b	O
R	O
m	O
4	O
r	O
/	O
e	O
Z	O
0	B-DatasetName
M	O
o	O
6	O
s	O
g	O
F	O
y	O
r	O
N	O
k	O
C	O
s	O
2	O
X	O
x	O
R	O
l	O
k	O
m	O
B	O
C	O
p	O
r	O
+	O
T	O
v	O
t	O
C	O
c	O
o	O
R	O
x	O
b	O
Q	O
p	O
k	O
W	O
9	O
l	O
b	O
C	O
h	O
l	O
R	O
T	O
h	O
j	O
a	O
h	O
i	O
g	O
3	O
B	O
W	O
3	O
x	O
5	O
m	O
f	O
j	O
n	O
9	O
e	O
u	O
6	O
d	O
3	O
9	O
R	O
a	O
9	O
w	O
U	O
a	O
Z	O
T	O
h	O
C	O
I	O
7	O
h	O
F	O
D	O
y	O
4	O
h	O
A	O
b	O
c	O
Q	O
R	O
N	O
8	O
Y	O
D	O
C	O
C	O
Z	O
3	O
i	O
F	O
N	O
y	O
d	O
1	O
X	O
p	O
x	O
3	O
5	O
2	O
P	O
e	O
W	O
n	O
K	O
K	O
m	O
U	O
P	O
4	O
A	O
+	O
f	O
z	O
B	O
3	O
1	O
D	O
j	O
z	O
Q	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
w	O
t	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
J	O
D	O
O	O
I	O
I	O
T	O
8	O
O	O
A	O
c	O
6	O
n	O
A	O
D	O
D	O
f	O
C	O
B	O
Q	O
R	O
+	O
e	O
4	O
R	O
X	O
e	O
H	O
O	O
m	O
8	O
O	O
O	O
/	O
O	O
x	O
6	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
J	O
D	O
O	O
I	O
I	O
T	O
8	O
O	O
A	O
c	O
6	O
n	O
A	O
D	O
D	O
f	O
C	O
B	O
Q	O
R	O
+	O
e	O
4	O
R	O
X	O
e	O
H	O
O	O
m	O
8	O
O	O
O	O
/	O
O	O
x	O
6	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
J	O
D	O
O	O
I	O
I	O
T	O
8	O
O	O
A	O
c	O
6	O
n	O
A	O
D	O
D	O
f	O
C	O
B	O
Q	O
R	O
+	O
e	O
4	O
R	O
X	O
e	O
H	O
O	O
m	O
8	O
O	O
O	O
/	O
O	O
x	O
6	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
X	O
/	O
B	O
b	O
P	O
P	O
Q	O
R	O
M	O
1	O
p	O
m	O
B	O
h	O
x	O
d	O
K	O
1	O
e	O
n	O
S	O
b	O
L	O
+	O
g	O
J	O
w	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
2	O
H	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
1	O
L	O
8	O
6	O
V	O
q	O
1	O
r	O
N	O
8	O
E	O
i	O
u	O
C	O
o	O
z	O
b	O
t	O
S	O
d	O
4	O
M	O
Z	O
l	O
B	O
c	O
c	O
W	O
2	O
q	O
F	O
k	O
M	O
n	O
f	O
a	O
0	B-DatasetName
E	O
x	O
m	O
S	O
O	O
4	O
I	O
p	O
f	O
Q	O
F	O
X	O
L	O
h	O
R	O
f	O
D	O
B	O
3	O
v	O
o	O
3	O
p	O
z	O
0	B-DatasetName
K	O
t	O
B	O
w	O
I	O
f	O
5	O
y	O
T	O
k	O
3	O
p	O
O	O
U	O
S	O
l	O
o	O
K	O
g	O
i	O
+	O
v	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
X	O
3	O
/	O
c	O
P	O
G	O
v	O
7	O
h	O
0	B-DatasetName
X	O
G	O
z	O
8	O
W	O
S	O
L	O
y	O
g	O
i	O
M	O
R	O
K	O
E	O
K	O
0	B-DatasetName
0	B-DatasetName
u	O
4	O
R	O
S	O
U	O
1	O
R	O
i	O
R	O
J	O
Y	O
a	O
8	O
0	B-DatasetName
y	O
P	O
N	O
E	O
Y	O
T	O
e	O
Z	O
3	O
C	O
3	O
y	O
7	O
j	O
M	O
a	O
K	O
w	O
v	O
9	O
S	O
N	O
M	O
S	O
4	O
5	O
y	O
P	O
t	O
M	O
y	O
k	O
4	O
O	O
S	O
s	O
z	O
r	O
D	O
Z	O
C	O
t	O
r	O
B	O
U	O
m	O
w	O
T	O
w	O
j	O
W	O
0	B-DatasetName
Y	O
K	O
1	O
h	O
8	O
3	O
O	O
Q	O
F	O
q	O
L	O
K	O
U	O
Z	O
N	O
Q	O
3	O
N	O
p	O
+	O
G	O
J	O
Q	O
U	O
z	O
7	O
g	O
h	O
K	O
R	O
T	O
O	O
/	O
U	O
F	O
l	O
s	O
e	O
R	O
i	O
w	O
k	O
f	O
Y	O
d	O
6	O
h	O
5	O
j	O
j	O
a	O
e	O
L	O
c	O
e	O
c	O
s	O
3	O
P	O
n	O
p	O
C	O
w	O
r	O
j	O
D	O
u	O
a	O
2	O
N	O
L	O
9	O
+	O
W	O
L	O
G	O
c	O
2	O
u	O
n	O
e	O
e	O
J	O
u	O
5	O
p	O
z	O
G	O
9	O
m	O
+	O
2	O
M	O
P	O
/	O
L	O
+	O
h	O
V	O
l	O
1	O
/	O
F	O
M	O
6	O
r	O
I	O
i	O
1	O
G	O
L	O
1	O
U	O
V	O
Y	O
p	O
R	O
g	O
V	O
b	O
7	O
M	O
x	O
S	O
a	O
V	O
C	O
Q	O
m	O
j	O
r	O
g	O
w	O
k	O
g	O
3	O
K	O
x	O
N	O
j	O
b	O
r	O
g	O
g	O
1	O
4	O
z	O
v	O
O	O
g	O
j	O
/	O
b	O
r	O
w	O
J	O
0	B-DatasetName
W	O
X	O
7	O
p	O
h	O
0	B-DatasetName
+	O
B	O
F	O
C	O
H	O
U	O
z	O
i	O
D	O
C	O
w	O
j	O
h	O
C	O
m	O
7	O
h	O
H	O
j	O
o	O
Q	O
g	O
Y	O
A	O
U	O
X	O
u	O
D	O
N	O
G	O
3	O
u	O
v	O
3	O
v	O
u	O
q	O
q	O
p	O
q	O
3	O
7	O
u	O
w	O
E	O
f	O
s	O
n	O
7	O
+	O
A	O
a	O
q	O
K	O
Y	O
o	O
N	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
8	O
L	O
i	O
q	O
q	O
k	O
O	O
r	O
4	O
y	O
R	O
e	O
y	O
v	O
W	O
h	O
W	O
M	O
T	O
n	O
L	O
M	O
j	O
k	O
C	O
0	B-DatasetName
c	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
3	O
n	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
+	O
F	O
t	O
r	O
1	O
e	O
r	O
W	O
T	O
b	O
A	O
I	O
r	O
s	O
q	O
M	O
G	O
3	O
U	O
n	O
u	O
H	O
F	O
Z	O
0	B-DatasetName
b	O
G	O
F	O
d	O
i	O
i	O
Z	O
9	O
E	O
4	O
b	O
m	O
s	O
k	O
M	O
y	O
R	O
2	O
l	O
l	O
D	O
6	O
C	O
G	O
x	O
c	O
q	O
P	O
p	O
Y	O
7	O
3	O
8	O
b	O
0	B-DatasetName
Z	O
6	O
G	O
t	O
B	O
w	O
I	O
f	O
5	O
y	O
T	O
k	O
3	O
h	O
P	O
n	O
S	O
l	O
r	O
y	O
/	O
W	O
9	O
v	O
b	O
X	O
1	O
j	O
c	O
2	O
u	O
7	O
t	O
F	O
P	O
e	O
r	O
e	O
z	O
t	O
H	O
1	O
Q	O
P	O
K	O
4	O
8	O
2	O
K	O
4	O
z	O
A	O
U	O
G	O
Q	O
q	O
M	O
6	O
2	O
Y	O
W	O
1	O
R	O
S	O
Y	O
0	B-DatasetName
i	O
S	O
F	O
L	O
Z	O
y	O
g	O
z	O
y	O
N	O
F	O
T	O
b	O
j	O
4	O
c	O
0	B-DatasetName
0	B-DatasetName
b	O
z	O
6	O
h	O
s	O
T	O
L	O
T	O
D	O
z	O
T	O
K	O
M	O
U	O
p	O
5	O
X	O
8	O
t	O
E	O
C	O
k	O
7	O
O	O
u	O
n	O
/	O
u	O
U	O
r	O
d	O
a	O
8	O
+	O
v	O
+	O
T	O
G	O
w	O
V	O
g	O
g	O
X	O
U	O
Y	O
K	O
F	O
G	O
t	O
/	O
r	O
V	O
6	O
W	O
W	O
i	O
S	O
F	O
G	O
T	O
U	O
N	O
z	O
a	O
d	O
u	O
D	O
n	O
F	O
I	O
2	O
5	O
I	O
S	O
k	O
U	O
T	O
s	O
q	O
d	O
w	O
m	O
L	O
O	O
x	O
Z	O
D	O
3	O
s	O
e	O
1	O
Q	O
8	O
x	O
R	O
t	O
N	O
J	O
6	O
N	O
O	O
m	O
G	O
n	O
z	O
u	O
m	O
x	O
J	O
D	O
P	O
u	O
a	O
G	O
I	O
z	O
9	O
/	O
e	O
L	O
M	O
U	O
+	O
t	O
H	O
a	O
W	O
x	O
u	O
5	O
l	O
y	O
G	O
t	O
j	O
l	O
b	O
G	O
r	O
+	O
l	O
7	O
U	O
L	O
S	O
i	O
6	O
j	O
s	O
d	O
R	O
5	O
Q	O
a	O
j	O
F	O
/	O
K	O
O	O
k	O
U	O
I	O
w	O
y	O
N	O
t	O
2	O
b	O
9	O
a	O
R	O
B	O
Q	O
W	O
r	O
k	O
g	O
A	O
s	O
j	O
3	O
a	O
x	O
M	O
D	O
L	O
j	O
h	O
g	O
l	O
w	O
7	O
Z	O
V	O
d	O
C	O
s	O
L	O
z	O
y	O
K	O
o	O
T	O
n	O
9	O
a	O
t	O
6	O
c	O
O	O
d	O
D	O
C	O
Y	O
7	O
h	O
B	O
M	O
4	O
g	O
g	O
A	O
u	O
4	O
h	O
l	O
t	O
o	O
Q	O
A	O
g	O
C	O
+	O
v	O
A	O
C	O
b	O
/	O
D	O
u	O
K	O
e	O
/	O
V	O
+	O
5	O
i	O
3	O
t	O
e	O
Y	O
t	O
a	O
j	O
u	O
C	O
P	O
/	O
I	O
+	O
f	O
w	O
D	O
A	O
+	O
Y	O
x	O
i	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
8	O
L	O
i	O
q	O
q	O
k	O
O	O
r	O
4	O
y	O
R	O
e	O
y	O
v	O
W	O
h	O
W	O
M	O
T	O
n	O
L	O
M	O
j	O
k	O
C	O
0	B-DatasetName
c	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
3	O
n	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
+	O
F	O
t	O
r	O
1	O
e	O
r	O
W	O
T	O
b	O
A	O
I	O
r	O
s	O
q	O
M	O
G	O
3	O
U	O
n	O
u	O
H	O
F	O
Z	O
0	B-DatasetName
b	O
G	O
F	O
d	O
i	O
i	O
Z	O
9	O
E	O
4	O
b	O
m	O
s	O
k	O
M	O
y	O
R	O
2	O
l	O
l	O
D	O
6	O
C	O
G	O
x	O
c	O
q	O
P	O
p	O
Y	O
7	O
3	O
8	O
b	O
0	B-DatasetName
Z	O
6	O
G	O
t	O
B	O
w	O
I	O
f	O
5	O
y	O
T	O
k	O
3	O
h	O
P	O
n	O
S	O
l	O
r	O
y	O
/	O
W	O
9	O
v	O
b	O
X	O
1	O
j	O
c	O
2	O
u	O
7	O
t	O
F	O
P	O
e	O
r	O
e	O
z	O
t	O
H	O
1	O
Q	O
P	O
K	O
4	O
8	O
2	O
K	O
4	O
z	O
A	O
U	O
G	O
Q	O
q	O
M	O
6	O
2	O
Y	O
W	O
1	O
R	O
S	O
Y	O
0	B-DatasetName
i	O
S	O
F	O
L	O
Z	O
y	O
g	O
z	O
y	O
N	O
F	O
T	O
b	O
j	O
4	O
c	O
0	B-DatasetName
0	B-DatasetName
b	O
z	O
6	O
h	O
s	O
T	O
L	O
T	O
D	O
z	O
T	O
K	O
M	O
U	O
p	O
5	O
X	O
8	O
t	O
E	O
C	O
k	O
7	O
O	O
u	O
n	O
/	O
u	O
U	O
r	O
d	O
a	O
8	O
+	O
v	O
+	O
T	O
G	O
w	O
V	O
g	O
g	O
X	O
U	O
Y	O
K	O
F	O
G	O
t	O
/	O
r	O
V	O
6	O
W	O
W	O
i	O
S	O
F	O
G	O
T	O
U	O
N	O
z	O
a	O
d	O
u	O
D	O
n	O
F	O
I	O
2	O
5	O
I	O
S	O
k	O
U	O
T	O
s	O
q	O
d	O
w	O
m	O
L	O
O	O
x	O
Z	O
D	O
3	O
s	O
e	O
1	O
Q	O
8	O
x	O
R	O
t	O
N	O
J	O
6	O
N	O
O	O
m	O
G	O
n	O
z	O
u	O
m	O
x	O
J	O
D	O
P	O
u	O
a	O
G	O
I	O
z	O
9	O
/	O
e	O
L	O
M	O
U	O
+	O
t	O
H	O
a	O
W	O
x	O
u	O
5	O
l	O
y	O
G	O
t	O
j	O
l	O
b	O
G	O
r	O
+	O
l	O
7	O
U	O
L	O
S	O
i	O
6	O
j	O
s	O
d	O
R	O
5	O
Q	O
a	O
j	O
F	O
/	O
K	O
O	O
k	O
U	O
I	O
w	O
y	O
N	O
t	O
2	O
b	O
9	O
a	O
R	O
B	O
Q	O
W	O
r	O
k	O
g	O
A	O
s	O
j	O
3	O
a	O
x	O
M	O
D	O
L	O
j	O
h	O
g	O
l	O
w	O
7	O
Z	O
V	O
d	O
C	O
s	O
L	O
z	O
y	O
K	O
o	O
T	O
n	O
9	O
a	O
t	O
6	O
c	O
O	O
d	O
D	O
C	O
Y	O
7	O
h	O
B	O
M	O
4	O
g	O
g	O
A	O
u	O
4	O
h	O
l	O
t	O
o	O
Q	O
A	O
g	O
C	O
+	O
v	O
A	O
C	O
b	O
/	O
D	O
u	O
K	O
e	O
/	O
V	O
+	O
5	O
i	O
3	O
t	O
e	O
Y	O
t	O
a	O
j	O
u	O
C	O
P	O
/	O
I	O
+	O
f	O
w	O
D	O
A	O
+	O
Y	O
x	O
i	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
8	O
T	O
P	O
j	O
8	O
p	O
W	O
5	O
m	O
g	O
7	O
r	O
I	O
V	O
N	O
u	O
D	O
P	O
D	O
9	O
n	O
l	O
C	O
0	B-DatasetName
6	O
8	O
k	B-HyperparameterName
=	I-HyperparameterName
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
T	O
8	O
J	O
A	O
E	O
J	O
3	O
i	O
F	O
+	O
I	O
X	O
6	O
t	O
H	O
L	O
R	O
m	O
L	O
i	O
i	O
b	O
R	O
e	O
1	O
B	O
v	O
R	O
i	O
0	B-DatasetName
e	O
M	O
V	O
k	O
i	O
g	O
I	O
d	O
t	O
l	O
C	O
x	O
u	O
2	O
2	O
2	O
Z	O
3	O
q	O
i	O
E	O
N	O
P	O
8	O
G	O
L	O
B	O
z	O
V	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
c	O
u	O
0	B-DatasetName
I	O
O	O
C	O
L	O
5	O
n	O
k	O
5	O
b	O
2	O
Z	O
z	O
M	O
w	O
L	O
U	O
y	O
k	O
M	O
u	O
u	O
6	O
3	O
U	O
1	O
p	O
Z	O
X	O
V	O
v	O
f	O
K	O
G	O
9	O
W	O
t	O
r	O
Z	O
3	O
d	O
v	O
e	O
q	O
+	O
w	O
c	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
L	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
O	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
r	O
X	O
B	O
0	B-DatasetName
P	O
f	O
V	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
c	O
8	O
i	O
O	O
l	O
A	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
h	O
r	O
1	O
p	O
z	O
6	O
+	O
4	O
M	O
Z	O
J	O
l	O
4	O
B	O
a	O
l	O
B	O
g	O
W	O
a	O
v	O
+	O
t	O
X	O
t	O
J	O
y	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
E	O
d	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
S	O
f	O
V	O
L	O
q	O
Z	O
4	O
S	O
l	O
l	O
I	O
z	O
r	O
g	O
H	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
t	O
m	O
p	O
E	O
3	O
J	O
i	O
l	O
T	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
p	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
j	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
k	O
V	O
v	O
K	O
v	O
7	O
n	O
d	O
T	O
K	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
8	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
k	O
3	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
w	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
k	O
0	B-DatasetName
Z	O
2	O
n	O
Q	O
q	O
N	O
g	O
R	O
v	O
8	O
e	O
V	O
l	O
4	O
p	O
/	O
V	O
L	O
+	O
v	O
e	O
r	O
V	O
t	O
r	O
X	O
B	O
V	O
p	O
l	O
O	O
E	O
I	O
j	O
u	O
E	O
U	O
P	O
D	O
i	O
H	O
B	O
t	O
x	O
A	O
E	O
3	O
x	O
g	O
M	O
I	O
B	O
n	O
e	O
I	O
U	O
3	O
R	O
z	O
o	O
v	O
z	O
r	O
v	O
z	O
M	O
W	O
8	O
t	O
O	O
c	O
X	O
M	O
I	O
f	O
y	O
B	O
8	O
/	O
k	O
D	O
2	O
/	O
m	O
N	O
s	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
J	O
D	O
O	O
I	O
I	O
T	O
8	O
O	O
A	O
c	O
6	O
n	O
A	O
D	O
D	O
f	O
C	O
B	O
Q	O
R	O
+	O
e	O
4	O
R	O
X	O
e	O
H	O
O	O
m	O
8	O
O	O
O	O
/	O
O	O
x	O
6	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O
J	O
D	O
O	O
I	O
I	O
T	O
8	O
O	O
A	O
c	O
6	O
n	O
A	O
D	O
D	O
f	O
C	O
B	O
Q	O
R	O
+	O
e	O
4	O
R	O
X	O
e	O
H	O
O	O
m	O
8	O
O	O
O	O
/	O
O	O
x	O
6	O
x	O
1	O
y	O
S	O
l	O
m	O
D	O
u	O
A	O
P	O
n	O
M	O
8	O
f	O
3	O
T	O
m	O
N	O
t	O
g	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
q	O
3	O
U	O
T	O
9	O
y	O
t	O
x	O
i	O
6	O
J	O
a	O
G	O
M	O
3	O
Y	O
4	O
F	O
l	O
9	O
w	O
H	O
X	O
p	O
O	O
G	O
M	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
X	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
4	O
W	O
e	O
t	O
X	O
1	O
a	O
O	O
X	O
x	O
S	O
J	O
4	O
K	O
o	O
k	O
I	O
6	O
q	O
3	O
o	O
x	O
W	O
N	O
F	O
Y	O
w	O
t	O
t	O
K	O
J	O
v	O
t	O
p	O
l	O
2	O
6	O
2	O
Y	O
T	O
d	O
i	O
V	O
J	O
C	O
f	O
4	O
I	O
X	O
D	O
y	O
p	O
e	O
/	O
U	O
f	O
e	O
/	O
D	O
d	O
u	O
2	O
x	O
y	O
0	B-DatasetName
9	O
c	O
H	O
A	O
4	O
7	O
0	B-DatasetName
Z	O
Z	O
u	O
a	O
F	O
q	O
R	O
Q	O
G	O
X	O
f	O
f	O
b	O
W	O
V	O
p	O
e	O
W	O
V	O
1	O
b	O
L	O
2	O
2	O
U	O
N	O
7	O
e	O
2	O
d	O
3	O
Y	O
r	O
e	O
/	O
s	O
P	O
J	O
s	O
k	O
0	B-DatasetName
4	O
z	O
5	O
L	O
Z	O
K	O
J	O
b	O
I	O
T	O
V	O
c	O
C	O
s	O
V	O
9	O
F	O
C	O
h	O
5	O
K	O
9	O
W	O
c	O
x	O
q	O
H	O
k	O
z	O
X	O
B	O
4	O
P	O
f	O
G	O
b	O
j	O
1	O
w	O
b	O
k	O
a	O
h	O
7	O
H	O
K	O
U	O
8	O
i	O
G	O
l	O
f	O
i	O
U	O
g	O
w	O
i	O
l	O
a	O
6	O
e	O
+	O
p	O
i	O
t	O
1	O
J	O
1	O
a	O
+	O
4	O
U	O
Z	O
J	O
F	O
4	O
B	O
a	O
l	O
C	O
g	O
U	O
a	O
3	O
8	O
t	O
X	O
p	O
J	O
S	O
y	O
L	O
u	O
U	O
I	O
m	O
q	O
T	O
F	O
t	O
z	O
0	B-DatasetName
0	B-DatasetName
o	O
K	O
g	O
i	O
+	O
v	O
t	O
r	O
W	O
9	O
s	O
7	O
t	O
X	O
3	O
/	O
c	O
P	O
G	O
v	O
7	O
h	O
0	B-DatasetName
X	O
G	O
z	O
8	O
W	O
S	O
L	O
y	O
g	O
i	O
M	O
R	O
K	O
E	O
K	O
0	B-DatasetName
0	B-DatasetName
u	O
4	O
R	O
S	O
U	O
1	O
R	O
i	O
R	O
J	O
Y	O
a	O
8	O
0	B-DatasetName
y	O
P	O
N	O
E	O
Y	O
T	O
e	O
Z	O
3	O
C	O
3	O
y	O
7	O
j	O
M	O
a	O
K	O
w	O
v	O
9	O
S	O
N	O
M	O
S	O
4	O
5	O
y	O
P	O
t	O
M	O
y	O
k	O
4	O
O	O
S	O
s	O
z	O
r	O
D	O
Z	O
C	O
t	O
r	O
B	O
U	O
m	O
w	O
T	O
w	O
j	O
W	O
0	B-DatasetName
Y	O
K	O
1	O
h	O
8	O
3	O
O	O
Q	O
F	O
q	O
L	O
K	O
U	O
Z	O
N	O
Q	O
3	O
N	O
p	O
+	O
G	O
J	O
Q	O
U	O
z	O
7	O
g	O
h	O
K	O
R	O
T	O
O	O
/	O
U	O
F	O
l	O
s	O
e	O
R	O
i	O
w	O
k	O
f	O
Y	O
d	O
6	O
h	O
5	O
j	O
j	O
a	O
e	O
L	O
c	O
e	O
c	O
s	O
3	O
P	O
n	O
p	O
C	O
w	O
r	O
j	O
D	O
u	O
a	O
2	O
N	O
L	O
9	O
+	O
W	O
L	O
G	O
c	O
2	O
u	O
n	O
e	O
e	O
J	O
u	O
5	O
p	O
z	O
G	O
9	O
m	O
+	O
2	O
M	O
P	O
/	O
L	O
+	O
h	O
V	O
l	O
1	O
/	O
F	O
M	O
6	O
r	O
I	O
i	O
1	O
G	O
L	O
1	O
U	O
V	O
Y	O
p	O
R	O
g	O
V	O
b	O
7	O
M	O
x	O
S	O
a	O
V	O
C	O
Q	O
m	O
j	O
r	O
g	O
w	O
k	O
g	O
3	O
K	O
x	O
N	O
j	O
b	O
r	O
g	O
g	O
1	O
4	O
z	O
v	O
O	O
g	O
j	O
/	O
b	O
r	O
w	O
J	O
0	B-DatasetName
W	O
X	O
7	O
p	O
h	O
0	B-DatasetName
+	O
B	O
F	O
C	O
H	O
U	O
z	O
i	O
D	O
C	O
w	O
j	O
h	O
C	O
m	O
7	O
h	O
H	O
j	O
o	O
Q	O
g	O
Y	O
A	O
U	O
X	O
u	O
D	O
N	O
G	O
3	O
u	O
v	O
3	O
v	O
u	O
q	O
q	O
p	O
q	O
3	O
7	O
u	O
w	O
E	O
f	O
s	O
n	O
7	O
+	O
A	O
a	O
q	O
K	O
Y	O
o	O
N	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
Y	O
f	O
B	O
p	O
l	O
r	O
X	O
h	O
t	O
V	O
g	O
c	O
d	O
Z	O
q	O
6	O
r	O
a	O
0	B-DatasetName
D	O
R	O
Y	O
N	O
h	O
O	O
c	O
A	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
4	O
H	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
+	O
F	O
t	O
r	O
1	O
e	O
r	O
W	O
T	O
b	O
A	O
I	O
r	O
s	O
q	O
M	O
G	O
3	O
U	O
n	O
u	O
N	O
B	O
l	O
B	O
c	O
c	O
W	O
2	O
r	O
F	O
k	O
0	B-DatasetName
j	O
t	O
t	O
a	O
C	O
Y	O
z	O
J	O
H	O
e	O
E	O
U	O
v	O
o	O
M	O
b	O
l	O
y	O
o	O
+	O
F	O
T	O
u	O
f	O
B	O
v	O
T	O
n	O
4	O
W	O
2	O
H	O
g	O
h	O
8	O
n	O
J	O
O	O
Q	O
e	O
0	B-DatasetName
+	O
c	O
K	O
2	O
n	O
J	O
9	O
7	O
+	O
9	O
t	O
f	O
W	O
N	O
z	O
a	O
3	O
t	O
0	B-DatasetName
k	O
5	O
5	O
t	O
7	O
K	O
3	O
f	O
1	O
A	O
9	O
r	O
D	O
z	O
a	O
r	O
D	O
A	O
C	O
Q	O
5	O
G	O
p	O
z	O
L	O
R	O
i	O
b	O
l	O
F	O
J	O
j	O
S	O
F	O
J	O
U	O
t	O
j	O
K	O
D	O
f	O
I	O
0	B-DatasetName
V	O
t	O
i	O
M	O
h	O
z	O
f	O
T	O
v	O
P	O
m	O
M	O
x	O
s	O
p	O
M	O
P	O
9	O
A	O
o	O
x	O
y	O
j	O
l	O
f	O
S	O
0	B-DatasetName
T	O
K	O
T	O
g	O
5	O
K	O
0	B-DatasetName
y	O
6	O
9	O
H	O
T	O
b	O
r	O
d	O
b	O
8	O
u	O
j	O
8	O
T	O
W	O
4	O
V	O
g	O
A	O
T	O
V	O
Y	O
q	O
N	O
G	O
t	O
f	O
n	O
V	O
6	O
m	O
S	O
h	O
S	O
1	O
C	O
Q	O
U	O
t	O
7	O
Y	O
d	O
+	O
D	O
l	O
F	O
Y	O
2	O
5	O
I	O
C	O
o	O
W	O
T	O
c	O
q	O
e	O
w	O
m	O
H	O
M	O
x	O
5	O
H	O
1	O
s	O
O	O
9	O
Q	O
8	O
R	O
R	O
u	O
N	O
Z	O
8	O
N	O
O	O
2	O
K	O
l	O
z	O
e	O
i	O
z	O
J	O
j	O
D	O
u	O
a	O
2	O
M	O
z	O
9	O
/	O
W	O
L	O
M	O
U	O
2	O
t	O
H	O
a	O
e	O
x	O
u	O
p	O
p	O
w	O
G	O
d	O
j	O
m	O
b	O
m	O
v	O
9	O
l	O
7	O
Y	O
K	O
S	O
y	O
2	O
g	O
s	O
d	O
V	O
4	O
Q	O
a	O
j	O
H	O
/	O
K	O
C	O
k	O
U	O
o	O
4	O
x	O
N	O
N	O
2	O
c	O
9	O
a	O
V	O
C	O
Q	O
G	O
j	O
n	O
g	O
w	O
k	O
g	O
3	O
K	O
x	O
M	O
D	O
b	O
r	O
g	O
g	O
1	O
0	B-DatasetName
/	O
Z	O
l	O
R	O
A	O
s	O
r	O
7	O
w	O
K	O
4	O
X	O
n	O
9	O
q	O
h	O
7	O
c	O
+	O
1	O
C	O
C	O
Y	O
z	O
i	O
B	O
M	O
w	O
j	O
g	O
A	O
q	O
7	O
h	O
D	O
h	O
o	O
Q	O
g	O
g	O
A	O
J	O
L	O
/	O
A	O
G	O
7	O
5	O
7	O
2	O
X	O
r	O
2	O
P	O
e	O
V	O
t	O
r	O
3	O
q	O
K	O
2	O
I	O
/	O
g	O
j	O
7	O
/	O
M	O
H	O
5	O
m	O
a	O
N	O
C	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
Y	O
f	O
B	O
p	O
l	O
r	O
X	O
h	O
t	O
V	O
g	O
c	O
d	O
Z	O
q	O
6	O
r	O
a	O
0	B-DatasetName
D	O
R	O
Y	O
N	O
h	O
O	O
c	O
A	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
4	O
H	O
i	O
c	O
b	O
Z	O
D	O
N	O
S	O
g	O
M	O
x	O
F	O
I	O
X	O
v	O
+	O
F	O
t	O
r	O
1	O
e	O
r	O
W	O
T	O
b	O
A	O
I	O
r	O
s	O
q	O
M	O
G	O
3	O
U	O
n	O
u	O
N	O
B	O
l	O
B	O
c	O
c	O
W	O
2	O
r	O
F	O
k	O
0	B-DatasetName
j	O
t	O
t	O
a	O
C	O
Y	O
z	O
J	O
H	O
e	O
E	O
U	O
v	O
o	O
M	O
b	O
l	O
y	O
o	O
+	O
F	O
T	O
u	O
f	O
B	O
v	O
T	O
n	O
4	O
W	O
2	O
H	O
g	O
h	O
8	O
n	O
J	O
O	O
Q	O
e	O
0	B-DatasetName
+	O
c	O
K	O
2	O
n	O
J	O
9	O
7	O
+	O
9	O
t	O
f	O
W	O
N	O
z	O
a	O
3	O
t	O
0	B-DatasetName
k	O
5	O
5	O
t	O
7	O
K	O
3	O
f	O
1	O
A	O
9	O
r	O
D	O
z	O
a	O
r	O
D	O
A	O
C	O
Q	O
5	O
G	O
p	O
z	O
L	O
R	O
i	O
b	O
l	O
F	O
J	O
j	O
S	O
F	O
J	O
U	O
t	O
j	O
K	O
D	O
f	O
I	O
0	B-DatasetName
V	O
t	O
i	O
M	O
h	O
z	O
f	O
T	O
v	O
P	O
m	O
M	O
x	O
s	O
p	O
M	O
P	O
9	O
A	O
o	O
x	O
y	O
j	O
l	O
f	O
S	O
0	B-DatasetName
T	O
K	O
T	O
g	O
5	O
K	O
0	B-DatasetName
y	O
6	O
9	O
H	O
T	O
b	O
r	O
d	O
b	O
8	O
u	O
j	O
8	O
T	O
W	O
4	O
V	O
g	O
A	O
T	O
V	O
Y	O
q	O
N	O
G	O
t	O
f	O
n	O
V	O
6	O
m	O
S	O
h	O
S	O
1	O
C	O
Q	O
U	O
t	O
7	O
Y	O
d	O
+	O
D	O
l	O
F	O
Y	O
2	O
5	O
I	O
C	O
o	O
W	O
T	O
c	O
q	O
e	O
w	O
m	O
H	O
M	O
x	O
5	O
H	O
1	O
s	O
O	O
9	O
Q	O
8	O
R	O
R	O
u	O
N	O
Z	O
8	O
N	O
O	O
2	O
K	O
l	O
z	O
e	O
i	O
z	O
J	O
j	O
D	O
u	O
a	O
2	O
M	O
z	O
9	O
/	O
W	O
L	O
M	O
U	O
2	O
t	O
H	O
a	O
e	O
x	O
u	O
p	O
p	O
w	O
G	O
d	O
j	O
m	O
b	O
m	O
v	O
9	O
l	O
7	O
Y	O
K	O
S	O
y	O
2	O
g	O
s	O
d	O
V	O
4	O
Q	O
a	O
j	O
H	O
/	O
K	O
C	O
k	O
U	O
o	O
4	O
x	O
N	O
N	O
2	O
c	O
9	O
a	O
V	O
C	O
Q	O
G	O
j	O
n	O
g	O
w	O
k	O
g	O
3	O
K	O
x	O
M	O
D	O
b	O
r	O
g	O
g	O
1	O
0	B-DatasetName
/	O
Z	O
l	O
R	O
A	O
s	O
r	O
7	O
w	O
K	O
4	O
X	O
n	O
9	O
q	O
h	O
7	O
c	O
+	O
1	O
C	O
C	O
Y	O
z	O
i	O
B	O
M	O
w	O
j	O
g	O
A	O
q	O
7	O
h	O
D	O
h	O
o	O
Q	O
g	O
g	O
A	O
J	O
L	O
/	O
A	O
G	O
7	O
5	O
7	O
2	O
X	O
r	O
2	O
P	O
e	O
V	O
t	O
r	O
3	O
q	O
K	O
2	O
I	O
/	O
g	O
j	O
7	O
/	O
M	O
H	O
5	O
m	O
a	O
N	O
C	O
A	O
=	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
y	O
o	O
u	O
q	O
m	O
G	O
/	O
h	O
y	O
L	O
p	O
9	O
/	O
9	O
P	O
M	O
T	O
/	O
p	O
R	O
g	O
i	O
x	O
y	O
K	O
l	O
G	O
w	O
S	O
Q	O
f	O
l	O
z	O
u	O
Z	O
4	O
S	O
l	O
l	O
Q	O
9	O
r	O
n	O
b	O
U	O
s	O
V	O
j	O
b	O
k	O
J	O
8	O
u	O
m	O
p	O
Y	O
3	O
J	O
s	O
l	O
R	O
6	O
J	O
E	O
m	O
1	O
L	O
I	O
Z	O
m	O
q	O
v	O
y	O
d	O
y	O
G	O
h	O
s	O
z	O
i	O
k	O
P	O
b	O
G	O
V	O
M	O
c	O
m	O
H	O
l	O
v	O
I	O
v	O
7	O
n	O
t	O
T	O
O	O
M	O
L	O
o	O
J	O
c	O
q	O
D	O
R	O
D	O
r	O
t	O
h	O
s	O
U	O
Z	O
R	O
J	O
g	O
g	O
m	O
Z	O
/	O
E	O
1	O
6	O
Q	O
n	O
O	O
G	O
c	O
m	O
Q	O
J	O
Z	O
V	O
r	O
Y	O
W	O
w	O
k	O
b	O
U	O
E	O
0	B-DatasetName
Z	O
2	O
n	O
T	O
K	O
N	O
g	O
R	O
v	O
/	O
u	O
V	O
F	O
4	O
p	O
/	O
W	O
L	O
m	O
v	O
e	O
7	O
V	O
m	O
1	O
f	O
l	O
W	O
k	O
U	O
Y	O

We	O
first	O
review	O
related	O
works	O
that	O
combine	O
RL	O
and	O
GAN	B-MethodName
for	O
text	B-TaskName
generation	I-TaskName
.	O
As	O
one	O
of	O
the	O
most	O
rep	O
-	O
resentative	O
models	O
in	O
this	O
direction	O
,	O
SeqGAN	O
adopts	O
Monte	O
-	O
Carlo	O
search	O
to	O
calculate	O
rewards	O
.	O
However	O
,	O
such	O
a	O
method	O
introduces	O
high	O
variance	O
in	O
policy	O
optimization	O
.	O
There	O
are	O
a	O
number	O
of	O
works	O
proposed	O
subsequently	O
to	O
improve	O
the	O
reward	O
-	O
generation	O
process	O
.	O
For	O
example	O
,	O
RankGAN	O
(	O
Lin	O
et	O
al	O
,	O
2017	O
)	O
proposes	O
to	O
replace	O
the	O
reward	O
from	O
the	O
GAN	B-MethodName
discriminator	O
with	O
a	O
rankingbased	O
reward	O
,	O
MaliGAN	O
(	O
Che	O
et	O
al	O
,	O
2017	O
)	O
modifies	O
the	O
GAN	B-MethodName
objective	O
and	O
proposes	O
techniques	O
to	O
reduce	O
gradient	O
variance	O
,	O
MaskGAN	O
uses	O
a	O
filling	O
technique	O
to	O
define	O
a	O
Q	O
-	O
value	O
reward	O
for	O
sentence	B-TaskName
completion	I-TaskName
,	O
RelGAN	O
(	O
Nie	O
et	O
al	O
,	O
2019	O
)	O
uses	O
a	O
relational	O
memory	O
based	O
generator	O
for	O
the	O
long	O
-	O
distance	O
dependency	O
modeling	O
,	O
FM	O
-	O
GAN	B-MethodName
uses	O
a	O
feature	O
mover	O
distance	O
to	O
match	O
features	O
of	O
real	O
and	O
generated	O
sentences	O
inspired	O
by	O
optimal	O
transport	O
(	O
Chen	O
et	O
al	O
,	O
2019	O
;	O
,	O
and	O
LeakGAN	O
(	O
Guo	O
et	O
al	O
,	O
2017	O
)	O
tries	O
to	O
address	O
the	O
sparse	O
-	O
reward	O
issue	O
for	O
long	O
-	O
text	B-TaskName
generation	I-TaskName
with	O
hierarchical	O
RL	O
by	O
utilizing	O
the	O
leaked	O
information	O
from	O
a	O
GAN	B-MethodName
discriminator	O
.	O
One	O
problem	O
of	O
LeakGAN	O
is	O
that	O
it	O
tends	O
to	O
overfit	O
the	O
training	O
data	O
,	O
yielding	O
generated	O
sentences	O
that	O
are	O
often	O
not	O
diverse	O
.	O
By	O
contrast	O
,	O
by	O
relying	O
on	O
a	O
model	O
-	O
based	O
imitation	B-TaskName
learning	I-TaskName
approach	O
,	O
our	O
method	O
learns	O
global	O
-	O
structure	O
information	O
,	O
which	O
generates	O
more	O
-	O
diverse	O
sentences	O
,	O
and	O
can	O
be	O
extended	O
to	O
conditional	B-TaskName
text	I-TaskName
generation	I-TaskName
.	O
designed	O
a	O
differentiable	O
nested	O
Wasserstein	O
distance	O
for	O
semantic	O
matching	O
,	O
which	O
can	O
be	O
applied	O
for	O
further	O
improvement	O
.	O
RL	O
techniques	O
can	O
also	O
be	O
used	O
in	O
other	O
ways	O
for	O
text	B-TaskName
generation	I-TaskName
(	O
Bachman	O
and	O
Precup	O
,	O
2015	O
)	O
.	O
For	O
example	O
,	O
Ranzato	O
et	O
al	O
(	O
2016	O
)	O
trained	O
a	O
Seq2Seq	B-MethodName
model	O
by	O
directly	O
optimizing	O
the	O
BLEU	B-MetricName
/	O
ROUGE	O
scores	O
with	O
the	O
REINFORCE	B-MethodName
algorithm	O
.	O
To	O
reduce	O
variance	O
of	O
the	O
vanilla	O
REINFORCE	B-MethodName
,	O
Bahdanau	O
et	O
al	O
(	O
2017	O
)	O
adopted	O
the	O
actor	O
-	O
critic	O
framework	O
for	O
sequence	O
prediction	O
.	O
Furthermore	O
,	O
Rennie	O
et	O
al	O
(	O
2016	O
)	O
trained	O
a	O
baseline	O
algorithm	O
with	O
a	O
greedy	O
decoding	O
scheme	O
for	O
the	O
REINFORCE	B-MethodName
method	O
.	O
Note	O
that	O
all	O
these	O
methods	O
can	O
only	O
obtain	O
reward	O
after	O
a	O
whole	O
sentence	O
is	O
generated	O
.	O
Planning	O
techniques	O
in	O
RL	O
have	O
also	O
been	O
explored	O
to	O
improve	O
text	B-TaskName
generation	I-TaskName
(	O
Gulcehre	O
et	O
al	O
,	O
2017	O
;	O
Serdyuk	O
et	O
al	O
,	O
2018	O
)	O
.	O
introduced	O
the	O
selfimitation	O
scheme	O
to	O
exploit	O
historical	O
high	O
-	O
quality	O
sentences	O
for	O
enhanced	O
exploration	O
.	O
Compared	O
to	O
these	O
related	O
works	O
,	O
the	O
proposed	O
guider	O
network	O
can	O
provide	O
a	O
planning	O
mechanism	O
and	O
intermediate	O
rewards	O
.	O
generation	O
.	O
More	O
details	O
of	O
GMGAN	O
are	O
provided	O
in	O
Appendix	O
D.	O

We	O
use	O
the	O
COCO	B-DatasetName
Image	O
Captions	O
Dataset	O
,	O
in	O
which	O
most	O
sentences	O
have	O
a	O
length	O
of	O
about	O
10	O
words	O
.	O
Since	O
we	O
consider	O
unconditional	O
text	B-TaskName
generation	I-TaskName
,	O
only	O
image	O
captions	O
are	O
used	O
as	O
the	O
training	O
data	O
.	O
After	O
preprocessing	O
,	O
we	O
use	O
120	O
,	O
000	O
random	O
sample	O
sentences	O
as	O
the	O
training	O
set	O
,	O
and	O
10	O
,	O
000	O
as	O
the	O
test	O
set	O
.	O
The	O
BLEU	B-MetricName
scores	O
with	O
different	O
methods	O
are	O
listed	O
in	O
Table	O
1	O
.	O
We	O
observe	O
that	O
GM	O
-	O
GAN	B-MethodName
performs	O
significantly	O
better	O
than	O
the	O
baseline	O
models	O
.	O
Specifically	O
,	O
besides	O
achieving	O
higher	O
test	O
-	O
BLEU	B-MetricName
scores	O
,	O
the	O
proposed	O
method	O
also	O
generates	O
samples	O
with	O
very	O
good	O
diversity	O
in	O
terms	O
of	O
self	O
-	O
BLEU	B-MetricName
scores	O
.	O
LeakGAN	O
represents	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
adversarial	B-TaskName
text	I-TaskName
generation	O
,	O
however	O
,	O
its	O
diversity	O
measurement	O
is	O
relatively	O
poor	O
(	O
Zhu	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
suspect	O
that	O
the	O
high	O
BLEU	B-MetricName
score	I-MetricName
achieved	O
by	O
LeakGAN	O
is	O
due	O
to	O
its	O
mode	O
collapse	O
on	O
some	O
good	O
samples	O
,	O
resulting	O
in	O
high	O
self	O
-	O
BLEU	B-MetricName
scores	O
.	O
Other	O
baselines	O
achieve	O
lower	O
self	O
-	O
BLEU	B-MetricName
scores	O
since	O
they	O
can	O
not	O
generate	O
reasonable	O
sentences	O
.	O
Long	O
Text	B-TaskName
Generation	I-TaskName
:	O
EMNLP2017	O
WMT	O
Following	O
(	O
Zhu	O
et	O
al	O
,	O
2018	O
)	O
Human	O
Evaluation	O
Simply	O
relying	O
on	O
the	O
above	O
metrics	O
is	O
not	O
sufficient	O
to	O
evaluate	O
the	O
proposed	O
method	O
(	O
Caccia	O
et	O
al	O
,	O
2018	O
)	O
.	O
Following	O
previous	O
work	O
(	O
Guo	O
et	O
al	O
,	O
2017	O
)	O
,	O
we	O
perform	O
human	O
evaluations	O
using	O
Amazon	O
Mechnical	O
Turk	O
,	O
evaluating	O
the	O
text	O
quality	O
based	O
on	O
readability	O
and	O
meaningfulness	O
(	O
whether	O
sentences	O
make	O
sense	O
)	O
on	O
the	O
EMNNLP2017	O
WMT	O
News	O
dataset	O
.	O
We	O
ask	O
the	O
worker	O
to	O
rate	O
the	O
input	O
sentence	O
with	O
scores	O
scal	O
-	O
ing	O
from	O
1	O
to	O
5	O
,	O
with	O
1	O
as	O
the	O
worst	O
score	O
and	O
5	O
as	O
the	O
best	O
.	O
The	O
detailed	O
criteria	O
is	O
listed	O
in	O
Table	O
3	O
.	O
We	O
require	O
all	O
the	O
workers	O
to	O
be	O
native	O
English	O
speakers	O
,	O
with	O
approval	O
rate	O
higher	O
than	O
90	O
%	O
and	O
at	O
least	O
100	O
assignments	O
completed	O
.	O
We	O
randomly	O
sample	O
100	O
sentences	O
generated	O
by	O
each	O
model	O
.	O
Ten	O
native	O
English	O
speakers	O
on	O
Amazon	O
Mechanical	O
Turk	O
are	O
asked	O
to	O
rate	O
each	O
sentence	O
.	O
The	O
average	O
human	O
rating	O
scores	O
are	O
shown	O
in	O
Table	O
4	O
,	O
indicating	O
GMGAN	O
achieves	O
higher	O
human	O
scores	O
compared	O
to	O
other	O
methods	O
.	O
As	O
examples	O
,	O
Table	O
5	O
illustrates	O
some	O
generated	O
samples	O
by	O
GMGAN	O
and	O
its	O
baselines	O
.	O
The	O
performance	O
on	O
the	O
two	O
datasets	O
indicates	O
that	O
the	O
generated	O
sentences	O
of	O
GMGAN	O
are	O
of	O
higher	O
global	O
consistency	O
and	O
better	O
readability	O
than	O
SeqGAN	O
and	O
LeakGAN	O
.	O
More	O
generated	O
examples	O
are	O
provided	O
in	O
the	O
Appendix	O
.	O

(	O
1	O
)	O
Bicycles	O
are	O
parked	O
near	O
a	O
row	O
of	O
large	O
trees	O
near	O
a	O
sidewalk	O
.	O
(	O
2	O
)	O
A	O
married	O
couple	O
posing	O
in	O
front	O
of	O
a	O
piece	O
of	O
birthday	O
cake	O
.	O
(	O
1	O
)	O
"	O
Sometimes	O
decisions	O
are	O
big	O
,	O
but	O
they	O
're	O
easy	O
to	O
make	O
,	O
"	O
he	O
told	O
The	O
Sunday	O
Times	O
in	O
the	O
New	O
Year	O
.	O
(	O
2	O
)	O
A	O
BBC	O
star	O
has	O
been	O
questioned	O
by	O
police	O
on	O
suspicion	O
of	O
sexual	O
assault	O
against	O
a	O
23	O
-	O
year	O
-	O
old	O
man	O
,	O
it	O
was	O
reported	O
last	O
night	O
.	O
It	O
is	O
grammatically	O
wrong	O
to	O
select	O
'	O
was	O
'	O
for	O
the	O
generator	O
,	O
thus	O
the	O
guider	O
network	O
gives	O
a	O
small	O
reward	O
.	O
We	O
can	O
see	O
that	O
the	O
rewards	O
become	O
lower	O
with	O
more	O
time	O
steps	O
,	O
which	O
is	O
consistent	O
with	O
the	O
exposure	O
bias	O
.	O
Model	O
Acc	B-MetricName
(	O
%	O
)	O
BLEU	B-MetricName
BLEU	B-MetricName
-	O
ref	O
CVAE	B-MethodName
73.9	O
20.7	O
7.8	O
Controllable	O
(	O
Hu	O
et	O
al	O
,	O
2017	O
)	O
86.7	O
58.4	O
-	O
BackTrans	O
(	O
Prabhumoye	O
et	O
al	O
,	O
2018	O
)	O
91.2	O
2.8	O
2.0	O
DeleteAndRetrieval	O
(	O
Li	O
et	O
al	O
,	O
2018a	O
)	O
88.9	O
36.8	O
14.7	O
Guider	O
(	O
Ours	O
)	O
92.7	O
52.1	O
25.4	O

We	O
test	O
the	O
proposed	O
framework	O
on	O
the	O
non	O
-	O
parallel	O
text	O
-	O
style	O
-	O
transfer	O
task	O
,	O
where	O
the	O
goal	O
is	O
to	O
transfer	O
one	O
sentence	O
in	O
one	O
style	O
(	O
e.g.	O
,	O
positive	O
)	O
to	O
a	O
similar	O
sentence	O
but	O
with	O
a	O
different	O
style	O
(	O
e.g.	O
,	O
negative	O
)	O
.	O
Pair	O
-	O
wise	O
information	O
should	O
be	O
inferred	O
from	O
the	O
training	O
data	O
,	O
which	O
becomes	O
more	O
challenging	O
.	O
For	O
a	O
fair	O
comparison	O
,	O
we	O
use	O
the	O
same	O
data	O
and	O
its	O
split	O
method	O
as	O
in	O
.	O
Specifically	O
,	O
there	O
are	O
444	O
,	O
000	O
,	O
63	O
,	O
500	O
,	O
and	O
127	O
,	O
000	O
sentences	O
with	O
either	O
positive	O
or	O
negative	O
sentiments	O
in	O
the	O
training	O
,	O
validation	O
and	O
test	O
sets	O
,	O
respectively	O
.	O
To	O
measure	O
whether	O
the	O
original	O
sentences	O
(	O
in	O
the	O
test	O
set	O
)	O
have	O
been	O
transferred	O
to	O
the	O
desired	O
sentiment	O
,	O
we	O
follow	O
the	O
settings	O
of	O
and	O
employ	O
a	O
pretrained	O
CNN	O
classifier	O
,	O
which	O
achieves	O
an	O
accuracy	B-MetricName
of	O
97.4	O
%	O
on	O
the	O
validation	O
set	O
,	O
to	O
evaluate	O
the	O
transferred	O
sentences	O
.	O
We	O
also	O
report	O
the	O
BLEU	B-MetricName
scores	O
with	O
original	O
sentences	O
(	O
BLEU	B-MetricName
)	O
and	O
human	O
references	O
(	O
BLEU	B-MetricName
-	O
ref	O
)	O
(	O
Li	O
et	O
al	O
,	O
2018a	O
)	O
,	O
to	O
evaluate	O
the	O
content	O
preservation	O
of	O
transferred	O
sentences	O
.	O
Results	O
are	O
summarized	O
in	O
Table	O
7	O
.	O
Our	O
proposed	O
model	O
exhibits	O
higher	O
transfer	O
accuracy	B-MetricName
and	O
better	O
content	O
preservation	O
,	O
indicating	O
the	O
guider	O
network	O
provides	O
good	O
sentiment	O
guidance	O
to	O
better	O
preserve	O
the	O
content	O
information	O
.	O

We	O
conduct	O
experiments	O
on	O
image	B-TaskName
captioning	I-TaskName
(	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
,	O
2015	O
)	O
,	O
investigating	O
benefits	O
brought	O
by	O
the	O
Guider	O
network	O
.	O
In	O
image	B-TaskName
captioning	I-TaskName
,	O
instead	O
of	O
using	O
a	O
discriminator	O
to	O
define	O
final	O
rewards	O
for	O
generated	O
sentence	O
,	O
we	O
adopt	O
evaluation	O
metrics	O
computed	O
based	O
on	O
human	O
references	O
.	O
The	O
final	O
rewards	O
appear	O
more	O
important	O
as	O
they	O
contain	O
reference	O
(	O
ground	O
-	O
truth	O
)	O
information	O
.	O
Feature	O
-	O
matching	O
rewards	O
work	O
as	O
a	O
regularizer	O
of	O
the	O
final	O
rewards	O
.	O
We	O
call	O
our	O
model	O
in	O
this	O
setting	O
a	O
guider	O
-	O
matching	O
sequence	O
training	O
(	O
GMST	O
)	O
model	O
.	O
An	O
overview	O
of	O
GMST	O
is	O
provided	O
in	O
the	O
Appendix	O
.	O
We	O
test	O
our	O
proposed	O
model	O
on	O
the	O
MS	O
COCO	B-DatasetName
dataset	O
(	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
,	O
2015	O
)	O
,	O
containing	O
123	O
,	O
287	O
images	O
in	O
total	O
.	O
Each	O
image	O
is	O
annotated	O
with	O
at	O
least	O
5	O
captions	O
.	O
Following	O
Karpathy	O
's	O
split	O
(	O
Karpathy	O
and	O
Fei	O
-	O
Fei	O
,	O
2015	O
)	O
,	O
5	O
,	O
000	O
images	O
are	O
used	O
for	O
both	O
validation	O
and	O
testing	O
.	O
We	O
report	O
BLEU	B-MetricName
-	O
k	O
(	O
k	O
from	O
1	O
to	O
4	O
)	O
,	O
CIDEr	B-MetricName
(	O
Vedantam	O
et	O
al	O
,	O
2015	O
)	O
,	O
and	O
ME	O
-	O
TEOR	O
(	O
Banerjee	O
and	O
Lavie	O
,	O
2005	O
)	O
scores	O
.	O
We	O
consider	O
two	O
settings	O
:	O
(	O
i	O
)	O
using	O
a	O
pre	O
-	O
trained	O
152layer	O
ResNet	B-MethodName
(	O
He	O
et	O
al	O
,	O
2016	O
)	O
for	O
feature	O
extraction	O
,	O
where	O
we	O
take	O
the	O
output	O
of	O
the	O
2048	B-DatasetName
-	O
way	O
pool5	O
layer	O
from	O
ResNet	B-MethodName
-	O
152	O
,	O
pretrained	O
on	O
the	O
ImageNet	B-DatasetName
dataset	O
;	O
and	O
(	O
ii	O
)	O
using	O
semantic	O
tags	O
detected	O
from	O
the	O
image	O
as	O
features	O
.	O
We	O
use	O
an	O
LSTM	B-MethodName
with	O
512	O
hidden	O
units	O
with	O
mini	O
-	O
batches	O
of	O
size	O
64	O
.	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
is	O
used	O
for	O
optimization	O
,	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
2	O
×	O
10	O
−4	O
.	O
We	O
pretrain	O
the	O
captioning	O
model	O
for	O
the	O
maximum	O
20	O
epochs	O
,	O
then	O
use	O
the	O
reinforcement	O
learning	O
to	O
train	O
it	O
for	O
20	O
epochs	O
and	O
test	O
on	O
the	O
best	O
model	O
on	O
the	O
validation	O
set	O
.	O
The	O
results	O
are	O
summarized	O
in	O
comparing	O
an	O
AutoEncoder	B-MethodName
(	O
AE	B-MethodName
)	O
with	O
a	O
variant	O
implemented	O
by	O
adding	O
a	O
guider	O
network	O
(	O
Guider	O
)	O
,	O
improvements	O
are	O
observed	O
.	O
We	O
compare	O
the	O
proposed	O
GMST	O
with	O
SCST	B-MethodName
.	O
Note	O
the	O
main	O
difference	O
between	O
GMST	O
and	O
SCST	B-MethodName
is	O
that	O
the	O
former	O
employs	O
our	O
proposed	O
feature	O
-	O
matching	O
reward	O
,	O
while	O
the	O
latter	O
only	O
considers	O
the	O
final	O
reward	O
provided	O
by	O
evaluation	O
metrics	O
.	O
GMST	O
achieves	O
higher	O
scores	O
compared	O
with	O
SCST	B-MethodName
on	O
its	O
optimized	O
metrics	O
.	O
The	O
gain	O
of	O
GMST	O
compared	O
with	O
SCST	B-MethodName
comes	O
from	O
the	O
immediate	O
rewards	O
,	O
which	O
can	O
maintain	O
the	O
semantic	O
consistency	O
and	O
sentence	O
structure	O
,	O
preventing	O
language	O
-	O
fluency	O
damage	O
caused	O
by	O
only	O
focusing	O
on	O
evaluation	O
metrics	O
.	O
Specifically	O
,	O
the	O
average	O
length	O
of	O
generated	O
sentence	O
with	O
a	O
Guider	O
is	O
15.7	O
,	O
and	O
12.9	O
for	O
traditional	O
generator	O
.	O
Comparison	O
with	O
MLE	O
The	O
guider	O
network	O
models	O
the	O
long	O
-	O
term	O
dependency	O
and	O
overcome	O
the	O
issue	O
of	O
sparse	O
reward	O
inspired	O
by	O
model	O
predictive	O
control	O
(	O
MPC	O
)	O
.	O
The	O
experiments	O
aim	O
to	O
quantify	O
the	O
gain	O
when	O
incorporating	O
MPC	O
for	O
imitation	B-TaskName
learning	I-TaskName
,	O
i.e.	O
,	O
MLE	O
and	O
RL	O
finetune	O
.	O
We	O
provide	O
an	O
additional	O
comparison	O
with	O
Caccia	O
et	O
al	O
(	O
2018	O
)	O
and	O
evaluate	O
the	O
diversity	O
and	O
quality	O
with	O
BLEU	B-MetricName
scores	O
.	O
We	O
also	O
report	O
the	O
F1	B-MetricName
-	O
BLEU	B-MetricName
which	O
considers	O
both	O
diversity	O
and	O
quality	O
in	O
Table	O
10	O
.	O
(	O
Caccia	O
et	O
al	O
,	O
2018	O
)	O
.	O

For	O
Image	O
COCO	B-DatasetName
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
the	O
generator	O
is	O
0.0002	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
the	O
guider	O
0.0002	O
,	O
the	O
maximum	O
length	O
of	O
sequence	O
is	O
25	O
.	O
For	O
WMT	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
the	O
guider	O
0.0002	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
the	O
guider	O
0.0002	O
,	O
the	O
maximum	O
length	O
of	O
sequence	O
is	O
50	O
.	O
We	O
use	O
c	O
=	O
4	O
chosen	O
from	O
[	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
8	O
]	O
and	O
γ	B-HyperparameterName
=	O
0.25	O
chosen	O
from	O
[	O
0.1	O
,	O
0.25	O
,	O
0.5	O
,	O
0.75	O
,	O
0.99	O
]	O
.	O
We	O
use	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
optimization	O
algorithm	O
to	O
train	O
the	O
guider	O
,	O
generator	O
and	O
discriminator	O
.	O
For	O
both	O
tasks	O
,	O
the	O
LSTM	B-MethodName
state	O
of	O
dimension	O
for	O
the	O
generator	O
is	O
300	O
,	O
and	O
the	O
LSTM	B-MethodName
state	O
of	O
dimension	O
for	O
the	O
generator	O
is	O
300	O
.	O
The	O
dimension	O
of	O
word	O
-	O
embedding	O
is	O
300	O
.	O
The	O
output	O
dimension	O
of	O
the	O
linear	O
transformation	O
connecting	O
guider	O
and	O
generator	O
is	O
600×10	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
Discriminator	O
is	O
0.001	O
.	O

For	O
Image	B-TaskName
Captioning	I-TaskName
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
the	O
guider	O
0.0002	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
the	O
guider	O
0.0002	O
,	O
the	O
maximum	O
length	O
of	O
sequence	O
is	O
25	O
.	O
For	O
Style	B-TaskName
transfer	I-TaskName
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
the	O
guider	O
0.0001	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
the	O
guider	O
0.0001	O
,	O
the	O
maximum	O
length	O
of	O
sequence	O
is	O
15	O
.	O

Algorithm	O
2	O
Guider	O
Matching	O
Generative	B-MethodName
Adversarial	I-MethodName
Network	I-MethodName
(	O
GMGAN	O
)	O
Require	O
:	O
generator	O
policy	O
π	O
φ	O
;	O
discriminator	O
D	O
θ	B-HyperparameterName
;	O
guider	O
network	O
G	O
ψ	O
;	O
a	O
sequence	O
dataset	O
S	O
=	O
{	O
X	O
1	O
...	O
T	O
}	O
.	O
1	O
:	O
Initialize	O
G	O
ψ	O
,	O
π	O
φ	O
,	O
D	O
θ	B-HyperparameterName
with	O
random	O
weights	O
.	O
2	O
:	O
Pretrain	O
generator	O
π	O
φ	O
,	O
guider	O
G	O
ψ	O
and	O
discriminator	O
D	O
θ	B-HyperparameterName
with	O
MLE	O
loss	B-MetricName
.	O
3	O
:	O
repeat	O
4	O
:	O
for	O
g	O
-	O
steps	O
do	O
5	O
:	O
Generate	O
a	O
sequence	O
Y	O
1	O
...	O
T	O
∼	O
π	O
φ	O
.	O

Test	O
-	O
BLEU	B-MetricName
-	O
2	O
3	O
4	O
5	O
Self	O
-	O
BLEU	B-MetricName
-	O
2	O
3	O
4	O
SeqGAN	O
0.820	O
0.604	O
0.361	O
0.211	O
0.807	O
0.577	O
0.278	O
RankGAN	O
(	O
Lin	O
et	O
al	O
,	O
2017	O
)	O
0.852	O
0.637	O
0.389	O
0.248	O
0.822	O
0.592	O
0.230	O
GSGAN	O
(	O
Kusner	O
and	O
Miguel	O
,	O
2016	O
)	O
0.810	O
0.566	O
0.335	O
0.197	O
0.785	O
0.522	O
0.230	O
TextGAN	O
0.910	O
0.728	O
0.484	O
0.306	O
0.806	O
0.548	O
0.217	O
LeakGAN	O
(	O
Guo	O
et	O
al	O
,	O
2017	O
)	O
0.922	O
0.797	O
0.602	O
0.416	O
0.912	O
0.825	O
0.689	O
MLE	O
(	O
Caccia	O
et	O
al	O
,	O
2018	O
)	O
0.902	O
0.706	O
0.470	O
0.392	O
0.787	O
0.646	O
0.485	O
GMGAN	O
(	O
ours	O
)	O
0.949	O
0.823	O
0.635	O
0.421	O
0.746	O
0.511	O
0.319	O
(	O
Lin	O
et	O
al	O
,	O
2017	O
)	O
0.723	O
0.440	O
0.210	O
0.107	O
0.672	O
0.346	O
0.119	O
GSGAN	O
(	O
Kusner	O
and	O
Miguel	O
,	O
2016	O
)	O
0.723	O
0.440	O
0.210	O
0.107	O
0.807	O
0.680	O
0.450	O
TextGAN	O
0.777	O
0.529	O
0.305	O
0.161	O
0.806	O
0.662	O
0.448	O
LeakGAN	O
(	O
Guo	O
et	O
al	O
,	O
2017	O
)	O
0.923	O
0.757	O
0.546	O
0.335	O
0.837	O
0.683	O
0.513	O
MLE	O
(	O
Caccia	O
et	O
al	O
,	O
2018	O
)	O
0.902	O
0.706	O
0.470	O
0.392	O
0.787	O
0.646	O
0.485	O
GMGAN	O
(	O
ours	O
)	O
0.923	O
0.727	O
0.491	O
0.303	O
0.814	O
0.576	O
0.328	O

Encoder	O
as	O
the	O
feature	O
extractor	O
For	O
unconditional	O
generation	O
,	O
the	O
feature	O
extractor	O
that	O
generates	O
inputs	O
for	O
the	O
guider	O
network	O
shares	O
the	O
CNN	O
part	O
of	O
the	O
encoder	O
.	O
We	O
stop	O
gradients	O
from	O
the	O
guider	O
network	O
to	O
the	O
encoder	O
CNN	O
in	O
the	O
training	O
process	O
.	O
For	O
conditional	O
generation	O
,	O
we	O
use	O
a	O
pretrained	O
feature	O
extractor	O
,	O
trained	O
similarly	O
to	O
the	O
unconditional	O
generation	O
.	O
Training	O
procedure	O
As	O
with	O
many	O
imitationlearning	O
models	O
(	O
Bahdanau	O
et	O
al	O
,	O
2017	O
;	O
Rennie	O
et	O
al	O
,	O
2016	O
;	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
,	O
we	O
first	O
train	O
the	O
encoder	O
-	O
decoder	O
part	O
based	O
on	O
the	O
off	O
-	O
policy	O
data	O
with	O
an	O
MLE	O
loss	B-MetricName
.	O
Then	O
we	O
use	O
RL	O
training	O
to	O
fine	O
-	O
tune	O
the	O
trained	O
generator	O
.	O
We	O
adaptively	O
transfer	O
the	O
training	O
from	O
MLE	O
loss	B-MetricName
to	O
RL	O
loss	B-MetricName
,	O
similar	O
to	O
(	O
Paulus	O
et	O
al	O
,	O
2017	O
;	O
Ranzato	O
et	O
al	O
,	O
2016	O
)	O
.	O

We	O
focus	O
on	O
adversarial	B-TaskName
text	I-TaskName
generation	O
,	O
and	O
compare	O
our	O
approach	O
with	O
a	O
number	O
of	O
related	O
works	O
(	O
Guo	O
et	O
al	O
,	O
2017	O
;	O
Lin	O
et	O
al	O
,	O
2017	O
;	O
Zhu	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
this	O
setting	O
,	O
a	O
discriminator	O
in	O
the	O
GAN	B-MethodName
framework	O
is	O
added	O
to	O
the	O
model	O
in	O
Figure	O
1	O
to	O
guide	O
the	O
generator	O
to	O
generate	O
high	O
-	O
quality	O
sentences	O
.	O
This	O
is	O
implemented	O
by	O
defining	O
the	O
final	O
reward	O
to	O
be	O
the	O
output	O
of	O
the	O
discriminator	O
.	O
All	O
baseline	O
experiments	O
are	O
implemented	O
on	O
the	O
texygen	B-DatasetName
platform	I-DatasetName
(	O
Zhu	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
adopt	O
the	O
BLEU	B-MetricName
score	I-MetricName
,	O
referenced	O
by	O
the	O
test	O
set	O
(	O
test	O
-	O
BLEU	B-MetricName
,	O
higher	O
value	O
implies	O
better	O
quality	O
)	O
and	O
itself	O
(	O
self	O
-	O
BLEU	B-MetricName
,	O
lower	O
value	O
implies	O
better	O
diversity	O
)	O
(	O
Zhu	O
et	O
al	O
,	O
2018	O
)	O
to	O
evaluate	O
quality	O
of	O
generated	O
samples	O
,	O
where	O
test	O
-	O
BLEU	B-MetricName
evaluates	O
the	O
reality	O
of	O
generated	O
samples	O
,	O
and	O
self	O
-	O
BLEU	B-MetricName
measures	O
the	O
diversity	O
.	O
A	O
good	O
generator	O
should	O
achieve	O
both	O
a	O
high	O
test	O
-	O
BLEU	B-MetricName
score	I-MetricName
and	O
a	O
low	O
self	O
-	O
BLEU	B-MetricName
score	I-MetricName
.	O
In	O
practice	O
,	O
we	O
use	O
t	O
=	O
c	O
=	O
4	O
and	O
γ	B-HyperparameterName
=	O
0.25	O
.	O
We	O
call	O
the	O
proposed	O
method	O
guidermatching	O
GAN	B-MethodName
(	O
GMGAN	O
)	O
for	O
unconditional	O
text	O
Res152	O
-	O
SCST	B-MethodName
:	O
a	O
group	O
of	O
zebras	O
standing	O
in	O
a	O
eld	O
.	O
Res152	O
-	O
GMST	O
:	O
a	O
herd	O
of	O
zebras	O
standing	O
in	O
a	O
eld	O
of	O
grass	O
.	O
Tag	O
-	O
SCST	B-MethodName
:	O
a	O
zebra	O
and	O
a	O
zebra	O
drinking	O
water	O
from	O
a	O
eld	O
of	O
grass	O
.	O
Tag	O
-	O
GMST	O
:	O
a	O
group	O
of	O
zebras	O
drinking	O
water	O
in	O
the	O
eld	O
of	O
grass	O
.	O
Res152	O
-	O
SCST	B-MethodName
:	O
a	O
group	O
of	O
people	O
walking	O
down	O
a	O
skateboard	O
.	O
Res152	O
-	O
GMST	O
:	O
a	O
group	O
of	O
people	O
standing	O
on	O
a	O
street	O
with	O
a	O
skateboard	O
.	O
Tag	O
-	O
SCST	B-MethodName
:	O
a	O
woman	O
walking	O
down	O
a	O
street	O
with	O
a	O
skateboard	O
.	O
Tag	O
-	O
GMST	O
:	O
a	O
black	O
and	O
white	O
photo	O
of	O
a	O
man	O
riding	O
a	O
skateboard	O
.	O
Res152	O
-	O
SCST	B-MethodName
:	O
a	O
baby	O
sing	O
next	O
to	O
a	O
baby	O
girae	O
.	O
Res152	O
-	O
GMST	O
:	O
a	O
lile	O
baby	O
sing	O
next	O
to	O
a	O
baby	O
holding	O
a	O
teddy	O
bear	O
.	O
Tag	O
-	O
SCST	B-MethodName
:	O
a	O
black	O
and	O
white	O
photo	O
of	O
a	O
woman	O
holding	O
a	O
teddy	O
bear	O
.	O
Tag	O
-	O
GMST	O
:	O
a	O
black	O
and	O
white	O
photo	O
of	O
a	O
man	O
and	O
a	O
woman	O
holding	O
a	O
teddy	O
bear	O
.	O
Res152	O
-	O
SCST	B-MethodName
:	O
a	O
trac	O
light	O
on	O
a	O
street	O
with	O
a	O
in	O
the	O
.	O
Res152	O
-	O
GMST	O
:	O
a	O
trac	O
light	O
on	O
the	O
side	O
of	O
a	O
street	O
.	O
Tag	O
-	O
SCST	B-MethodName
:	O
a	O
trac	O
light	O
on	O
a	O
street	O
with	O
a	O
green	O
.	O
Tag	O
-	O
GMST	O
:	O
a	O
red	O
trac	O
light	O
sing	O
on	O
the	O
side	O
of	O
a	O
road	O
.	O
Generate	O
a	O
sequence	O
Y	O
1	O
...	O
T	O
∼	O
π	O
φ	O
.	O

Following	O
the	O
few	O
other	O
research	O
and	O
asked	O
for	O
"	O
based	O
on	O
the	O
store	O
to	O
protect	O
older	O
,	O
nor	O
this	O
.	O
But	O
there	O
,	O
nor	O
believe	O
that	O
it	O
has	O
reached	O
a	O
the	O
person	O
to	O
know	O
what	O
never	O
-	O
he	O
needed	O
.	O
The	O
trump	O
administration	O
later	O
felt	O
the	O
alarm	O
was	O
a	O
their	O
doctors	O
are	O
given	O
.	O
We	O
have	O
been	O
the	O
time	O
of	O
single	O
things	O
what	O
people	O
do	O
not	O
need	O
to	O
get	O
careful	O
with	O
too	O
hurt	O
after	O
wells	O
then	O
.	O
If	O
he	O
was	O
waited	O
same	O
out	O
the	O
group	O
of	O
fewer	O
friends	O
a	O
more	O
injured	O
work	O
under	O
it	O
.	O
It	O
will	O
access	O
like	O
the	O
going	O
on	O
an	O
"	O
go	O
back	O
there	O
and	O
believe	O
.	O
Premier	O
as	O
well	O
as	O
color	O
looking	O
to	O
put	O
back	O
on	O
a	O
his	O
is	O
.	O
So	O
,	O
even	O
though	O
:	O
"	O
don	O
'	O
t	O
want	O
to	O
understand	O
it	O
at	O
an	O
opportunity	O
for	O
our	O
work	O
.	O
I	O
was	O
shocked	O
,	O
nor	O
don	O
'	O
t	O
know	O
if	O
mate	O
,	O
don	O
'	O
t	O
have	O
survived	O
,	O
So	O
one	O
point	O
like	O
ten	O
years	O
old	O
,	O
but	O
a	O
sure	O
,	O
nor	O
with	O
myself	O
more	O
people	O
substantial	O
.	O
And	O
if	O
an	O
way	O
of	O
shoes	O
of	O
crimes	O
the	O
processes	O
need	O
to	O
run	O
the	O
billionaire	O
.	O
Now	O
that	O
their	O
people	O
had	O
trained	O
and	O
people	O
the	O
children	O
live	O
an	O
actor	O
,	O
nor	O
what	O
trump	O
had	O
.	O
However	O
,	O
heavily	O
she	O
been	O
told	O
at	O
about	O
four	O
during	O
an	O
innocent	O
person	O
.	O
LeakGAN	O
The	O
country	O
has	O
a	O
reputation	O
for	O
cheap	O
medical	O
costs	O
and	O
high	O
-	O
attack	O
on	O
a	O
oil	O
for	O
more	O
than	O
to	O
higher	O
its	O
-	O
wage	O
increase	O
to	O
increase	O
access	O
to	O
the	O
UK	O
the	O
UK	O
women	O
from	O
the	O
UK	O
'	O
s	O
third	O
nuclear	O
in	O
the	O
last	O
couple	O
of	O
weeks	O
.	O
I	O
'	O
ve	O
been	O
watching	O
it	O
through	O
,	O
and	O
when	O
the	O
most	O
important	O
time	O
it	O
is	O
going	O
to	O
be	O
so	O
important	O
.	O
I	O
'	O
m	O
hopeful	O
that	O
as	O
that	O
process	O
moves	O
along	O
,	O
that	O
the	O
U	O
.	O
S	O
.	O
Attorney	O
will	O
share	O
as	O
much	O
as	O
far	O
as	O
possible	O
.	O
The	O
main	O
thing	O
for	O
should	O
go	O
in	O
with	O
the	O
new	O
contract	O
,	O
so	O
the	O
rest	O
of	O
the	O
Premier	O
League	O
is	O
there	O
to	O
grow	O
up	O
and	O
be	O
there	O
,	O
"	O
she	O
said	O
.	O
I	O
think	O
the	O
main	O
reason	O
for	O
their	O
sudden	O
is	O
however	O
,	O
I	O
didn	O
'	O
t	O
get	O
any	O
big	O
thing	O
,	O
"	O
he	O
says	O
,	O
who	O
is	O
the	O
whole	O
problem	O
on	O
the	O
U	O
.	O
S	O
.	O
Supreme	O
Court	O
and	O
rule	O
had	O
any	O
broken	O
.	O
The	O
average	O
age	O
of	O
Saudi	O
citizens	O
is	O
still	O
very	O
potential	O
for	O
the	O
next	O
year	O
in	O
the	O
past	O
year	O
,	O
over	O
the	O
last	O
year	O
he	O
realised	O
he	O
has	O
had	O
his	O
massive	O
and	O
family	O
and	O
home	O
.	O
"	O
I	O
think	O
Ted	O
is	O
under	O
a	O
lot	O
of	O
people	O
really	O
want	O
a	O
"	O
and	O
then	O
the	O
opportunity	O
to	O
put	O
on	O
life	O
for	O
security	O
for	O
them	O
to	O
try	O
and	O
keep	O
up	O
.	O
The	O
new	O
website	O
,	O
set	O
to	O
launch	O
March	O
1	O
,	O
but	O
the	O
U	O
.	O
S	O
is	O
to	O
give	O
up	O
the	O
time	O
the	O
case	O
can	O
lead	O
to	O
a	O
more	O
than	O
three	O
months	O
of	O
three	O
months	O
to	O
be	O
new	O
home	O
.	O
It	O
'	O
s	O
a	O
pub	O
;	O
though	O
it	O
was	O
going	O
to	O
be	O
that	O
,	O
but	O
,	O
not	O
,	O
but	O
I	O
am	O
not	O
the	O
right	O
thing	O
to	O
live	O
,	O
"	O
she	O
said	O
.	O
"	O
I	O
'	O
m	O
not	O
saying	O
method	O
writing	O
is	O
the	O
only	O
way	O
to	O
get	O
in	O
the	O
bedroom	O
to	O
get	O
through	O
the	O
season	O
and	O
we	O
'	O
ll	O
be	O
over	O
again	O
,	O
"	O
he	O
says	O
.	O
I	O
'	O
m	O
not	O
suggesting	O
that	O
our	O
jobs	O
or	O
our	O
love	O
our	O
years	O
because	O
I	O
have	O
a	O
couple	O
of	O
games	O
where	O
I	O
want	O
it	O
to	O
be	O
.	O
The	O
German	O
government	O
said	O
31	O
suspects	O
were	O
briefly	O
detained	O
for	O
questioning	O
after	O
the	O
New	O
Year	O
'	O
s	O
Eve	O
trouble	O
,	O
among	O
them	O
not	O
allowed	O
to	O
stay	O
in	O
the	O
long	O
-	O
term	O
.	O
It	O
was	O
a	O
punishment	O
carried	O
out	O
by	O
experts	O
in	O
violence	O
,	O
and	O
it	O
was	O
hard	O
to	O
me	O
he	O
loved	O
the	O
man	O
and	O
he	O
'	O
s	O
got	O
off	O
to	O
support	O
me	O
in	O
the	O
future	O
.	O
"	O
I	O
'	O
ve	O
known	O
him	O
,	O
all	O
that	O
just	O
over	O
the	O
last	O
two	O
weeks	O
and	O
for	O
the	O
last	O
10	O
years	O
,	O
I	O
'	O
ll	O
have	O
one	O
day	O
of	O
my	O
life	O
,	O
"	O
she	O
said	O
.	O
The	O
main	O
idea	O
behind	O
my	O
health	O
and	O
I	O
think	O
we	O
saw	O
in	O
work	O
of	O
our	O
country	O
was	O
in	O
big	O
fourth	O
-	O
up	O
come	O
up	O
with	O
a	O
little	O
you	O
'	O
ve	O
ever	O
.	O
he	O
Kings	O
had	O
needed	O
scoring	O
from	O
the	O
left	O
side	O
,	O
too	O
,	O
and	O
King	O
has	O
provided	O
that	O
since	O
his	O
return	O
are	O
the	O
of	O
the	O
first	O
three	O
quarters	O
of	O
the	O
game	O
.	O
It	O
'	O
s	O
going	O
to	O
be	O
a	O
good	O
test	O
for	O
us	O
and	O
we	O
are	O
on	O
the	O
right	O
way	O
to	O
be	O
able	O
to	O
get	O
through	O
it	O
on	O
every	O
day	O
on	O
the	O
year	O
.	O
GMGAN	O
But	O
it	O
'	O
s	O
grown	O
up	O
a	O
little	O
now	O
,	O
and	O
might	O
be	O
ready	O
for	O
actually	O
putting	O
into	O
your	O
house	O
.	O
More	O
than	O
a	O
dozen	O
Republicans	O
and	O
a	O
handful	O
of	O
Democrats	O
have	O
announced	O
they	O
are	O
running	O
for	O
their	O
party	O
'	O
s	O
2016	O
presidential	O
nomination	O
,	O
and	O
when	O
they	O
were	O
wealthy	O
in	O
2010	O
right	O
,	O
what	O
he	O
has	O
.	O
And	O
with	O
a	O
growing	O
following	O
of	O
more	O
than	O
45	O
,	O
000	O
people	O
on	O
Facebook	O
,	O
awareness	O
of	O
their	O
work	O
is	O
on	O
the	O
rise	O
.	O
In	O
all	O
age	O
groups	O
,	O
for	O
instance	O
,	O
more	O
people	O
cited	O
retirement	O
as	O
the	O
reason	O
for	O
being	O
out	O
of	O
the	O
labour	O
force	O
,	O
and	O
it	O
wasn	O
'	O
t	O
a	O
problem	O
in	O
big	O
.	O
I	O
had	O
to	O
train	O
really	O
,	O
really	O
hard	O
and	O
that	O
'	O
s	O
the	O
advice	O
I	O
can	O
give	O
,	O
because	O
if	O
you	O
don	O
'	O
t	O
work	O
hard	O
somebody	O
else	O
will	O
.	O
I	O
am	O
picking	O
up	O
two	O
cars	O
tomorrow	O
and	O
taking	O
them	O
down	O
south	O
tomorrow	O
if	O
all	O
goes	O
according	O
to	O
plan	O
,	O
"	O
he	O
said	O
.	O
The	O
team	O
looked	O
into	O
the	O
influence	O
of	O
marriage	O
on	O
weight	O
loss	B-MetricName
after	O
surgery	O
-	O
as	O
well	O
as	O
the	O
effects	O
of	O
surgery	O
on	O
the	O
quality	O
of	O
his	O
administration	O
and	O
rest	O
on	O
the	O
world	O
.	O
Two	O
former	O
prime	O
ministers	O
were	O
set	O
to	O
face	O
off	O
in	O
the	O
second	O
round	O
of	O
a	O
presidential	O
election	O
in	O
New	O
Hampshire	O
.	O
A	O
third	O
more	O
complaints	O
were	O
made	O
about	O
the	O
accounts	O
between	O
April	O
and	O
December	O
last	O
year	O
than	O
in	O
the	O
whole	O
of	O
2014	O
/	O
15	O
.	O
United	O
Airlines	O
subsequently	O
worked	O
to	O
get	O
those	O
passengers	O
back	O
in	O
the	O
air	O
so	O
they	O
could	O
get	O
to	O
Colorado	O
,	O
the	O
airline	O
spokesman	O
said	O
.	O
Mr	O
Brown	O
was	O
standing	O
in	O
the	O
kitchen	O
when	O
he	O
started	O
to	O
feel	O
a	O
bit	O
cold	O
-	O
and	O
he	O
noticed	O
the	O
door	O
had	O
disappeared	O
.	O
She	O
has	O
focused	O
instead	O
on	O
where	O
she	O
parts	O
ways	O
with	O
her	O
rival	O
on	O
other	O
issues	O
,	O
like	O
to	O
have	O
someone	O
with	O
a	O
president	O
has	O
revealed	O
.	O
Once	O
,	O
an	O
ex	O
-	O
boyfriend	O
and	O
I	O
lived	O
with	O
her	O
for	O
two	O
months	O
after	O
we	O
came	O
back	O
from	O
travelling	O
.	O
He	O
had	O
faced	O
10	O
years	O
in	O
prison	O
on	O
the	O
charges	O
but	O
the	O
first	O
government	O
have	O
been	O
made	O
at	O
the	O
recent	O
peak	O
.	O
"	O
We	O
weren	O
'	O
t	O
exposed	O
to	O
things	O
we	O
didn	O
'	O
t	O
have	O
in	O
the	O
same	O
way	O
kids	O
these	O
days	O
are	O
,	O
"	O
said	O
Obama	O
.	O
I	O
have	O
no	O
idea	O
what	O
it	O
is	O
,	O
but	O
there	O
is	O
definitely	O
an	O
intelligence	O
-	O
a	O
higher	O
intelligence	O
-	O
at	O
work	O
you	O
have	O
you	O
want	O
to	O
make	O
sure	O
you	O
are	O
going	O
into	O
the	O
local	O
community	O
.	O
His	O
current	O
club	O
have	O
confirmed	O
they	O
would	O
be	O
willing	O
to	O
listen	O
to	O
offers	O
for	O
the	O
attacking	O
midfielder	O
,	O
but	O
we	O
did	O
not	O
have	O
the	O
right	O
manager	O
-	O
there	O
'	O
s	O
summer	O
to	O
be	O
in	O
a	O
big	O
.	O
We	O
are	O
in	O
the	O
last	O
16	O
and	O
the	O
target	O
is	O
always	O
to	O
win	O
in	O
the	O
Champions	O
League	O
and	O
will	O
continue	O
at	O
the	O
best	O
level	O
to	O
be	O
the	O
coach	O
.	O
People	O
are	O
seeing	O
that	O
you	O
can	O
go	O
into	O
real	O
estate	O
and	O
do	O
really	O
well	O
and	O
do	O
something	O
we	O
want	O
and	O
if	O
we	O
make	O
the	O
right	O
decision	O
,	O
and	O
how	O
we	O
will	O
be	O
doing	O
it	O
is	O
.	O
Table	O
13	O
:	O
Generated	O
Examples	O
on	O
EMNLP2017	O
WMT	O
.	O
Original	O
:	O
i	O
'	O
m	O
so	O
lucky	O
to	O
have	O
found	O
this	O
place	O
!	O
Guider	O
:	O
i	O
'	O
m	O
so	O
embarrassed	O
that	O
i	O
picked	O
this	O
place	O
.	O
Original	O
:	O
awesome	O
place	O
,	O
very	O
friendly	O
staff	O
and	O
the	O
food	O
is	O
great	O
!	O
Guider	O
:	O
disgusting	O
place	O
,	O
horrible	O
staff	O
and	O
extremely	O
rude	O
customer	O
service	O
.	O
Original	O
:	O
this	O
was	O
my	O
first	O
time	O
trying	O
thai	O
food	O
and	O
the	O
waitress	O
was	O
amazing	O
!	O
Guider	O
:	O
this	O
was	O
my	O
first	O
experience	O
with	O
the	O
restaurant	O
and	O
we	O
were	O
absolutely	O
disappointed	O
.	O
Original	O
:	O
thanks	O
to	O
this	O
place	O
!	O
Guider	O
:	O
sorry	O
but	O
this	O
place	O
is	O
horrible	O
.	O
Original	O
:	O
the	O
staff	O
was	O
warm	O
and	O
friendly	O
.	O
Guider	O
:	O
the	O
staff	O
was	O
slow	O
and	O
rude	O
.	O
Original	O
:	O
great	O
place	O
and	O
huge	O
store	O
.	O
Guider	O
:	O
horrible	O
place	O
like	O
ass	O
screw	O
.	O
Original	O
:	O
the	O
service	O
is	O
friendly	O
and	O
quick	O
especially	O
if	O
you	O
sit	O
in	O
the	O
bar	O
.	O
Guider	O
:	O
the	O
customer	O
service	O
is	O
like	O
ok	O
-	O
definitely	O
a	O
reason	O
for	O
never	O
go	O
back	O
..	O
Original	O
:	O
everything	O
is	O
always	O
delicious	O
and	O
the	O
staff	O
is	O
wonderful	O
.	O
Guider	O
:	O
everything	O
is	O
always	O
awful	O
and	O
their	O
service	O
is	O
amazing	O
.	O
Original	O
:	O
best	O
place	O
to	O
have	O
lunch	O
and	O
or	O
dinner	O
.	O
Guider	O
:	O
worst	O
place	O
i	O
have	O
ever	O
eaten	O
.	O
Original	O
:	O
best	O
restaurant	O
in	O
the	O
world	O
!	O
Guider	O
:	O
worst	O
dining	O
experience	O
ever	O
!	O
Original	O
:	O
you	O
'll	O
be	O
back	O
!	O
Guider	O
:	O
you	O
're	O
very	O
disappointed	O
!	O
Original	O
:	O
you	O
will	O
be	O
well	O
cared	O
for	O
here	O
!	O
Guider	O
:	O
you	O
will	O
not	O
be	O
back	O
to	O
spend	O
your	O
money	O
.	O
Original	O
:	O
they	O
were	O
delicious	O
!	O
Guider	O
:	O
they	O
were	O
overcooked	O
.	O
Original	O
:	O
seriously	O
the	O
best	O
service	O
i	O
'	O
ve	O
ever	O
had	O
.	O
Guider	O
:	O
seriously	O
the	O
worst	O
service	O
i	O
'	O
ve	O
ever	O
experienced	O
.	O
Original	O
:	O
it	O
's	O
delicious	O
!	O
Guider	O
:	O
it	O
's	O
awful	O
.	O
this	O
place	O
is	O
phenomenal	O
.	O
Original	O
:	O
this	O
was	O
bad	O
experience	O
from	O
the	O
start	O
.	O
Guider	O
:	O
the	O
food	O
here	O
was	O
amazing	O
good	O
.	O
Original	O
:	O
very	O
rude	O
lady	O
for	O
testing	O
my	O
integrity	O
.	O
Guider	O
:	O
very	O
nice	O
atmosphere	O
for	O
an	O
amazing	O
lunch	O
!	O
Original	O
:	O
they	O
recently	O
renovated	O
rooms	O
but	O
should	O
have	O
renovated	O
management	O
and	O
staff	O
.	O
Guider	O
:	O
great	O
management	O
and	O
the	O
staff	O
is	O
friendly	O
and	O
helpful	O
.	O
Original	O
:	O
this	O
store	O
is	O
not	O
a	O
good	O
example	O
of	O
sprint	O
customer	O
service	O
though	O
.	O
Guider	O
:	O
this	O
store	O
is	O
always	O
good	O
,	O
consistent	O
and	O
they	O
're	O
friendly	O
.	O
Original	O
:	O
one	O
of	O
my	O
least	O
favorite	O
ross	O
locations	O
.	O
Guider	O
:	O
one	O
of	O
my	O
favorite	O
spots	O
.	O
Original	O
:	O
horrible	O
in	O
attentive	O
staff	O
.	O
Guider	O
:	O
great	O
front	O
desk	O
staff	O
!	O
Original	O
:	O
the	O
dining	O
area	O
looked	O
like	O
a	O
hotel	O
meeting	O
room	O
.	O
Guider	O
:	O
the	O
dining	O
area	O
is	O
nice	O
and	O
cool	O
.	O
Original	O
:	O
never	O
ever	O
try	O
to	O
sell	O
your	O
car	O
at	O
co	O
part	O
!	O
Guider	O
:	O
highly	O
recommend	O
to	O
everyone	O
and	O
recommend	O
this	O
spot	O
for	O
me	O
!	O
Original	O
:	O
i	O
ordered	O
the	O
filet	O
mignon	O
and	O
it	O
was	O
not	O
impressive	O
at	O
all	O
.	O
Guider	O
:	O
i	O
had	O
the	O
lamb	O
and	O
it	O
was	O
so	O
good	O
.	O

One	O
line	O
of	O
work	O
in	O
style	B-TaskName
transfer	I-TaskName
attempts	O
to	O
learn	O
disentangled	O
latent	O
representation	O
for	O
style	O
and	O
content	O
,	O
and	O
transfer	O
style	O
by	O
manipulating	O
latent	O
representation	O
of	O
style	O
(	O
Shen	O
et	O
al	O
,	O
2017	O
)	O
.	O
Although	O
these	O
approaches	O
perform	O
well	O
with	O
one	O
style	O
at	O
a	O
time	O
,	O
they	O
do	O
not	O
trivially	O
scale	O
to	O
multidimensional	O
style	B-TaskName
transfer	I-TaskName
.	O
Several	O
other	O
works	O
develop	O
unsupervised	O
approach	O
for	O
style	B-TaskName
transfer	I-TaskName
by	O
employing	O
Denoising	B-TaskName
Autoencoding	O
(	O
DAE	O
)	O
(	O
Fu	O
et	O
al	O
,	O
2017	O
)	O
and	O
back	O
-	O
translation	O
(	O
BT	O
)	O
(	O
Lample	O
et	O
al	O
,	O
2018	O
)	O
loss	B-MetricName
to	O
develop	O
interaction	O
and	O
hence	O
transfer	O
between	O
the	O
source	O
and	O
target	O
domain	O
.	O
Subramanian	O
et	O
al	O
(	O
2018	O
)	O
extend	O
this	O
approach	O
to	O
multiple	O
styles	O
by	O
conditioning	O
on	O
average	O
of	O
embedding	O
of	O
each	O
target	O
attribute	O
and	O
using	O
combination	O
of	O
DAE	O
and	O
back	O
-	O
translation	O
techniques	O
.	O
DAE	O
takes	O
as	O
input	O
a	O
sentence	O
x	O
from	O
style	O
s	O
and	O
tries	O
to	O
reconstruct	O
sentence	O
x	O
from	O
its	O
corrupted	O
versionx	O
.	O
This	O
relies	O
on	O
the	O
assumption	O
that	O
the	O
input	O
sentence	O
x	O
is	O
from	O
a	O
certain	O
style	O
combination	O
s	O
=	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
.	O
.	O
.	O
,	O
s	O
k	O
}	O
.	O
Similarly	O
back	O
translation	O
(	O
BT	O
)	O
objective	O
with	O
input	O
sentence	O
x	O
from	O
style	O
s	O
,	O
first	O
estimates	O
x	O
=	O
f	O
(	O
x	O
,	O
s	O
)	O
,	O
where	O
s	O
=	O
s	O
and	O
then	O
reconstruct	O
x	O
fromx	O
=	O
f	O
(	O
x	O
,	O
s	O
)	O
.	O
Thus	O
,	O
these	O
approaches	O
are	O
inherently	O
dependent	O
on	O
knowledge	O
of	O
annotation	O
of	O
each	O
sentence	O
with	O
all	O
the	O
style	O
combinations	O
.	O
Dai	O
et	O
al	O
(	O
2019	O
)	O
achieve	O
state	O
-	O
ofthe	O
-	O
art	O
style	B-TaskName
transfer	I-TaskName
in	O
single	O
style	O
dimensions	O
by	O
employing	O
transformer	O
-	O
based	O
model	O
in	O
conjunction	O
with	O
classifier	O
-	O
based	O
discriminator	O
.	O
In	O
addition	O
to	O
discriminator	O
losses	O
,	O
their	O
proposed	O
technique	O
uses	O
self	O
-	O
reconstruction	O
and	O
cycle	O
reconstruction	O
losses	O
,	O
which	O
similar	O
to	O
DAE	O
and	O
BT	O
losses	O
are	O
also	O
reliant	O
on	O
availability	O
of	O
jointly	O
annotated	O
data	O
to	O
be	O
extendable	O
to	O
multiple	O
style	O
setup	O
.	O
Language	O
modeling	O
is	O
integral	O
to	O
several	O
natural	O
language	O
generation	O
(	O
NLG	O
)	O
tasks	O
like	O
text	B-TaskName
summarization	I-TaskName
,	O
spelling	B-TaskName
correction	I-TaskName
,	O
image	B-TaskName
captioning	I-TaskName
,	O
etc	O
.	O
The	O
model	O
architecture	O
for	O
these	O
tasks	O
has	O
evolved	O
from	O
n	O
-	O
gram	O
based	O
methods	O
to	O
Recurrent	O
Neural	O
Networks	O
to	O
transformer	O
architectures	O
.	O
The	O
introduction	O
of	O
Transformer	B-MethodName
-	O
based	O
architecture	O
accompanied	O
with	O
generative	O
pre	O
-	O
training	O
(	O
Radford	O
,	O
2018	O
)	O
capabilities	O
have	O
led	O
to	O
strong	O
improvements	O
in	O
many	O
downstream	O
generation	O
and	O
GLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
tasks	O
.	O
Generative	O
pre	O
-	O
training	O
aims	O
to	O
adapt	O
a	O
large	O
Transformer	B-MethodName
language	O
model	O
to	O
large	O
unsupervised	O
corpus	O
.	O
This	O
capability	O
of	O
generative	O
pre	O
-	O
training	O
is	O
exploited	O
in	O
many	O
large	O
language	O
models	O
like	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
GPT	B-MethodName
-	O
2	O
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
,	O
ERNIE	O
2.0	O
(	O
Sun	O
et	O
al	O
,	O
2020	O
)	O
which	O
have	O
the	O
ability	O
to	O
perform	O
tasks	O
like	O
reading	B-TaskName
comprehension	I-TaskName
(	O
Xu	O
et	O
al	O
,	O
2019	O
)	O
,	O
summarization	B-TaskName
(	O
Liu	O
and	O
Lapata	O
,	O
2019	O
)	O
,	O
question	O
-	O
answering	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
and	O
translation	O
(	O
Clinchant	O
et	O
al	O
,	O
2019	O
)	O
in	O
zero	O
-	O
shot	O
and	O
few	O
-	O
shot	O
settings	O
.	O
Recently	O
these	O
pre	O
-	O
trained	O
generative	O
language	O
models	O
have	O
been	O
explored	O
in	O
translation	O
(	O
Conneau	O
and	O
Lample	O
,	O
2019	O
)	O
and	O
style	B-TaskName
transfer	I-TaskName
tasks	O
(	O
Syed	O
et	O
al	O
,	O
2020	O
)	O
.	O
Conneau	O
and	O
Lample	O
(	O
2019	O
)	O
develop	O
cross	O
-	O
lingual	O
models	O
for	O
unsupervised	B-TaskName
machine	I-TaskName
translation	I-TaskName
by	O
initializing	O
encoder	O
and	O
decoder	O
with	O
a	O
pre	O
-	O
trained	O
language	O
model	O
trained	O
on	O
Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
objective	O
and	O
fine	O
-	O
tuning	O
the	O
encoderdecoder	O
framework	O
with	O
adversarial	O
training	O
.	O
Syed	O
et	O
al	O
(	O
2020	O
)	O
extend	O
this	O
to	O
stylized	O
re	O
-	O
writing	O
task	O
by	O
employing	O
DAE	O
during	O
fine	O
-	O
tuning	O
.	O
The	O
joint	O
encoder	O
-	O
decoder	O
framework	O
learns	O
to	O
reconstruct	O
sentences	O
in	O
target	O
-	O
domain	O
from	O
its	O
noisy	O
version	O
using	O
DAE	O
objective	O
.	O
As	O
previously	O
discussed	O
,	O
the	O
DAE	O
objective	O
is	O
reliant	O
on	O
the	O
corpus	O
being	O
tagged	O
for	O
the	O
target	O
domain	O
style	O
(	O
or	O
combination	O
of	O
style	O
)	O
and	O
restricts	O
the	O
generalization	O
of	O
this	O
setup	O
to	O
multiple	O
attributes	O
.	O
We	O
overcome	O
this	O
by	O
employing	O
discriminative	O
language	O
models	O
to	O
assist	O
the	O
decoder	O
with	O
feedback	O
for	O
various	O
target	O
styles	O
.	O
Shen	O
et	O
al	O
(	O
2017	O
)	O
show	O
that	O
even	O
with	O
nonparallel	O
data	O
,	O
the	O
content	O
distribution	O
across	O
source	O
and	O
target	O
style	O
is	O
shared	O
.	O
Based	O
on	O
this	O
,	O
a	O
language	O
model	O
trained	O
on	O
target	O
style	O
will	O
have	O
high	O
perplexity	B-MetricName
on	O
transferred	O
text	O
if	O
it	O
does	O
not	O
match	O
target	O
style	O
and	O
low	O
perplexity	B-MetricName
otherwise	O
.	O
Yang	O
et	O
al	O
(	O
2018	O
)	O
exploit	O
this	O
ability	O
of	O
language	O
models	O
to	O
replace	O
standard	O
binary	O
classifier	O
-	O
based	O
discriminators	O
with	O
an	O
implicitly	O
trained	O
language	O
model	O
as	O
discriminator	O
.	O
They	O
show	O
that	O
using	O
the	O
language	O
model	O
as	O
structured	O
discriminator	O
allows	O
for	O
more	O
stable	O
training	O
by	O
eliminating	O
the	O
adversarial	O
step	O
.	O
We	O
extend	O
this	O
idea	O
to	O
a	O
multi	O
-	O
discriminator	O
approach	O
.	O
Training	O
a	O
LM	O
on	O
combination	O
of	O
target	O
styles	O
is	O
not	O
possible	O
in	O
absence	O
of	O
jointly	O
labelled	O
dataset	O
.	O
Due	O
to	O
this	O
,	O
we	O
attempt	O
to	O
use	O
multiple	O
discriminators	O
for	O
each	O
of	O
the	O
target	O
styles	O
.	O
Since	O
with	O
multiple	O
styles	O
,	O
the	O
underlying	O
corpus	O
is	O
independently	O
acquired	O
,	O
the	O
variation	O
in	O
content	O
distribution	O
across	O
different	O
styles	O
is	O
more	O
noticeable	O
.	O
Consequently	O
,	O
an	O
independently	O
trained	O
LM	O
on	O
one	O
of	O
the	O
target	O
styles	O
might	O
have	O
high	O
perplexity	B-MetricName
even	O
if	O
the	O
transferred	O
sentence	O
fits	O
in	O
the	O
corresponding	O
target	O
style	O
,	O
due	O
to	O
the	O
content	O
space	O
of	O
source	O
sentence	O
.	O
To	O
equip	O
discriminative	O
LM	O
with	O
more	O
generalized	O
notion	O
of	O
content	O
,	O
we	O
use	O
large	O
transformer	O
-	O
based	O
LM	O
pre	O
-	O
trained	O
on	O
large	O
unsupervised	O
corpus	O
to	O
establish	O
generic	O
content	O
distribution	O
before	O
style	O
-	O
oriented	O
fine	O
-	O
tuning	O
.	O

Our	O
proposed	O
approach	O
has	O
two	O
key	O
elementsa	O
Transformer	B-MethodName
-	O
based	O
encoder	O
-	O
decoder	O
model	O
initialized	O
with	O
a	O
pre	O
-	O
trained	O
Transformer	B-MethodName
Language	O
Model	O
and	O
fine	O
-	O
tuned	O
on	O
DAE	O
loss	B-MetricName
to	O
achieve	O
style	B-TaskName
transfer	I-TaskName
(	O
Section	O
3.1	O
)	O
and	O
the	O
multiple	O
language	O
models	O
as	O
discriminators	O
stacked	O
together	O
to	O
enable	O
multi	O
-	O
style	B-TaskName
transfer	I-TaskName
(	O
Section	O
3.2	O
)	O
.	O

Similar	O
to	O
Syed	O
et	O
al	O
(	O
2020	O
)	O
,	O
we	O
first	O
pre	O
-	O
train	O
a	O
Transformer	B-MethodName
-	O
based	O
language	O
model	O
with	O
Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
objective	O
on	O
English	O
Wikipedia	O
data	O
extracted	O
using	O
WikiExtractor	O
.	O
1	O
This	O
equips	O
LM	O
with	O
the	O
ability	O
to	O
predict	O
masked	O
words	O
over	O
a	O
large	O
corpus	O
.	O
Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
leverages	O
bidirectional	O
context	O
of	O
the	O
input	O
,	O
thus	O
enabling	O
better	O
language	O
understanding	O
.	O
Following	O
Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
objective	O
from	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
,	O
we	O
randomly	O
sample	O
15	O
%	O
of	O
the	O
tokens	O
from	O
the	O
text	O
stream	O
and	O
replace	O
them	O
with	O
the	O
[	O
MASK	O
]	O
token	O
80	O
%	O
of	O
the	O
time	O
,	O
by	O
a	O
random	O
token	O
10	O
%	O
of	O
the	O
time	O
and	O
keep	O
them	O
unchanged	O
10	O
%	O
of	O
the	O
time	O
,	O
with	O
the	O
objective	O
of	O
predicting	O
the	O
original	O
identity	O
of	O
the	O
masked	O
word	O
based	O
on	O
its	O
bidirectional	O
context	O
.	O
To	O
enable	O
style	B-TaskName
transfer	I-TaskName
from	O
a	O
given	O
sentence	O
to	O
target	O
style	O
,	O
we	O
use	O
independently	O
trained	O
language	O
models	O
(	O
LMs	O
)	O
to	O
initialize	O
the	O
encoder	O
and	O
decoder	O
and	O
connect	O
these	O
with	O
randomly	O
initialized	O
attention	B-HyperparameterName
layers	I-HyperparameterName
to	O
arrive	O
at	O
a	O
encoder	O
-	O
decoder	O
setup	O
.	O
As	O
discussed	O
by	O
Syed	O
et	O
al	O
(	O
2020	O
)	O
,	O
the	O
Transformer	B-MethodName
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
allows	O
such	O
independent	O
initialization	O
by	O
implicitly	O
aligning	O
encoder	O
-	O
decoder	O
layers	O
via	O
attention	O
mechanism	O
.	O
Pre	O
-	O
training	O
an	O
encoder	O
only	O
transformer	O
on	O
generative	O
task	O
and	O
then	O
leveraging	O
it	O
to	O
initialize	O
as	O
both	O
encoder	O
and	O
decoder	O
as	O
opposed	O
to	O
pretraining	O
a	O
joint	O
encoder	O
-	O
decoder	O
model	O
has	O
several	O
advantages	O
.	O
Transformer	B-MethodName
-	O
based	O
models	O
with	O
encoder	O
-	O
only	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
or	O
decoder	O
-	O
only	O
blocks	O
have	O
been	O
shown	O
to	O
perform	O
well	O
in	O
generative	O
pre	O
-	O
training	O
task	O
.	O
Clearly	O
,	O
pre	O
-	O
training	O
a	O
single	O
transformer	O
block	O
on	O
generative	O
task	O
and	O
then	O
utilizing	O
it	O
as	O
both	O
encoder	O
and	O
decoder	O
blocks	O
has	O
lower	O
computational	O
cost	O
than	O
training	O
the	O
entire	O
encoder	O
-	O
decoder	O
block	O
jointly	O
.	O
Moreover	O
,	O
this	O
also	O
enables	O
us	O
to	O
use	O
the	O
same	O
pre	O
-	O
trained	O
model	O
to	O
initialize	O
both	O
style	B-MethodName
transfer	I-MethodName
module	I-MethodName
and	O
the	O
discriminator	O
models	O
,	O
explained	O
in	O
the	O
following	O
section	O
.	O
This	O
is	O
not	O
only	O
computationally	O
more	O
efficient	O
but	O
it	O
also	O
closely	O
ties	O
the	O
underlying	O
language	O
distribution	O
of	O
the	O
two	O
modules	O
.	O
This	O
is	O
expected	O
to	O
make	O
the	O
discriminative	O
feedback	O
more	O
effective	O
while	O
fine	O
tuning	O
the	O
transfer	O
model	O
for	O
multiple	O
styles	O
.	O
In	O
Syed	O
et	O
al	O
(	O
2020	O
)	O
's	O
setup	O
,	O
both	O
encoder	O
and	O
decoder	O
in	O
the	O
style	B-MethodName
transfer	I-MethodName
module	I-MethodName
are	O
initialized	O
with	O
the	O
pre	O
-	O
trained	O
language	O
model	O
(	O
trained	O
on	O
MLM	B-DatasetName
objective	O
)	O
.	O
Instead	O
,	O
we	O
initialize	O
the	O
decoder	O
with	O
the	O
language	O
model	O
fine	O
-	O
tuned	O
with	O
the	O
target	O
style	O
using	O
Causal	O
Language	O
Modeling	O
(	O
CLM	O
)	O
objective	O
,	O
before	O
training	O
the	O
joint	O
encoder	O
-	O
decoder	O
model	O
,	O
as	O
detailed	O
in	O
Section	O
3.2	O
.	O
The	O
encoder	O
is	O
initialized	O
with	O
the	O
pre	O
-	O
trained	O
model	O
directly	O
.	O
Aligning	O
the	O
decoder	O
to	O
the	O
distribution	O
of	O
the	O
tar	O
-	O
.	O
.	O
.	O
.	O
get	O
style	O
helps	O
speed	O
up	O
the	O
fine	O
-	O
tuning	O
process	O
as	O
decoder	O
is	O
more	O
adept	O
at	O
generating	O
stylized	O
outputs	O
.	O
This	O
does	O
not	O
add	O
to	O
computational	O
overhead	O
as	O
these	O
fine	O
-	O
tuned	O
models	O
are	O
repurposed	O
as	O
discriminators	O
for	O
stylistic	O
feedback	O
(	O
Section	O
3.2	O
)	O
.	O
To	O
instill	O
style	O
-	O
awareness	O
to	O
the	O
encoder	O
-	O
decoder	O
setup	O
initialized	O
with	O
pre	O
-	O
trained	O
Transformer	B-MethodName
models	O
,	O
we	O
fine	O
-	O
tune	O
it	O
with	O
Denoising	B-MethodName
Autoencoder	I-MethodName
(	O
DAE	O
)	O
loss	B-MetricName
using	O
the	O
target	O
-	O
domain	O
corpus	O
.	O
In	O
case	O
of	O
multiple	O
styles	O
,	O
we	O
use	O
a	O
randomized	O
mixture	O
of	O
target	O
-	O
domain	O
corpus	O
from	O
each	O
of	O
the	O
target	O
styles	O
.	O
Under	O
the	O
DAE	O
objective	O
,	O
the	O
encoder	O
takes	O
a	O
noisy	O
masked	O
versionx	O
of	O
the	O
text	O
x	O
as	O
input	O
and	O
attempts	O
to	O
fill	O
in	O
the	O
mask	O
token	O
as	O
per	O
the	O
MLM	B-DatasetName
objective	O
that	O
it	O
was	O
pre	O
-	O
trained	O
on	O
.	O
In	O
turn	O
,	O
the	O
decoder	O
re	O
-	O
creates	O
stylistic	O
version	O
of	O
original	O
sentence	O
from	O
this	O
noisy	O
output	O
from	O
the	O
encoder	O
.	O
The	O
overall	O
training	O
objective	O
is	O
L	O
DAE	O
(	O
θ	B-HyperparameterName
G	O
)	O
=	O
E	O
x∼T	O
[	O
−	O
log	O
P	O
θ	B-HyperparameterName
G	O
(	O
x	O
|	O
x	O
)	O
]	O
,	O
(	O
1	O
)	O
where	O
θ	B-HyperparameterName
G	O
are	O
the	O
trainable	O
parameters	O
of	O
the	O
encoder	O
-	O
decoder	O
model	O
.	O
The	O
noisy	O
version	O
of	O
sentence	O
x	O
from	O
the	O
target	O
corpus	O
T	O
is	O
obtained	O
after	O
dropping	O
tokens	O
from	O
x	O
with	O
probability	O
p	O
drop	O
and	O
masking	O
with	O
a	O
probability	O
of	O
p	O
mask	O
.	O
In	O
conjunction	O
,	O
the	O
encoder	O
and	O
decoder	O
enable	O
style	B-TaskName
transfer	I-TaskName
to	O
the	O
target	O
style	O
.	O
The	O
noteworthy	O
aspect	O
here	O
is	O
that	O
the	O
model	O
has	O
no	O
sense	O
of	O
source	O
style	O
and	O
is	O
trained	O
to	O
generate	O
sentences	O
to	O
match	O
the	O
style	O
of	O
the	O
target	O
-	O
domain	O
corpus	O
with	O
which	O
it	O
is	O
trained	O
.	O

To	O
extend	O
the	O
single	O
-	O
dimensional	O
style	B-TaskName
transfer	I-TaskName
setup	O
above	O
to	O
multi	O
-	O
dimensional	O
setting	O
,	O
we	O
use	O
language	O
models	O
as	O
discriminators	O
to	O
provide	O
the	O
feedback	O
to	O
the	O
model	O
for	O
partially	O
annotated	O
nature	O
of	O
input	O
data	O
.	O
As	O
opposed	O
to	O
a	O
classifier	O
-	O
based	O
discriminator	O
,	O
the	O
language	O
model	O
as	O
discriminator	O
takes	O
into	O
account	O
the	O
wider	O
language	O
distribution	O
of	O
the	O
target	O
style	O
.	O
Additionally	O
,	O
such	O
a	O
setup	O
allows	O
us	O
to	O
use	O
only	O
the	O
target	O
style	O
corpus	O
for	O
training	O
the	O
transfer	O
model	O
,	O
whereas	O
the	O
classifier	O
would	O
require	O
both	O
source	O
and	O
target	O
style	O
corpus	O
to	O
distinguish	O
between	O
a	O
sentence	O
as	O
being	O
from	O
one	O
style	O
or	O
another	O
.	O
Inspired	B-DatasetName
by	O
Yang	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
fine	O
-	O
tune	O
a	O
language	O
model	O
on	O
the	O
target	O
style	O
s	O
i	O
,	O
so	O
that	O
the	O
language	O
model	O
is	O
equipped	O
with	O
language	O
distribution	O
of	O
target	O
domain	O
data	O
.	O
This	O
entails	O
generating	O
the	O
probability	O
of	O
next	O
token	O
,	O
given	O
the	O
previous	O
tokens	O
-	O
also	O
known	O
as	O
Causal	O
Language	O
Modeling	O
objective	O
(	O
Conneau	O
and	O
Lample	O
,	O
2019	O
)	O
.	O
The	O
training	O
loss	B-MetricName
for	O
the	O
LM	O
for	O
target	O
style	O
s	O
i	O
with	O
corresponding	O
corpus	O
T	O
i	O
is	O
Ex∼T	O
i	O
n	O
t=1	O
[	O
−	O
log	O
PLM	O
(	O
xt	O
|	O
x1	O
,	O
.	O
.	O
.	O
,	O
xt−1	O
)	O
]	O
(	O
2	O
)	O
We	O
show	O
in	O
our	O
experiments	O
that	O
such	O
a	O
finetuning	O
step	O
transforms	O
language	O
distribution	O
of	O
this	O
language	O
model	O
to	O
style	O
s	O
i	O
and	O
hence	O
serve	O
as	O
softdiscriminator	O
for	O
our	O
framework	O
.	O
We	O
exploit	O
this	O
capability	O
of	O
language	O
models	O
to	O
imbibe	O
style	O
of	O
fine	O
-	O
tuning	O
corpus	O
by	O
employing	O
language	O
models	O
as	O
style	O
discriminators	O
for	O
transferred	O
sentences	O
.	O
This	O
is	O
based	O
on	O
the	O
idea	O
that	O
if	O
the	O
transferred	O
sentence	O
does	O
not	O
fit	O
well	O
in	O
the	O
target	O
style	O
,	O
then	O
the	O
perplexity	B-MetricName
of	O
language	O
model	O
fine	O
-	O
tuned	O
on	O
that	O
style	O
will	O
be	O
high	O
(	O
Section	O
4.1	O
)	O
.	O
For	O
k	O
-	O
dimensional	O
style	B-TaskName
transfer	I-TaskName
with	O
target	O
styles	O
s	O
=	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
.	O
.	O
.	O
,	O
s	O
k	O
}	O
,	O
we	O
independently	O
finetune	O
k	O
language	O
models	O
on	O
each	O
of	O
the	O
target	O
styles	O
.	O
As	O
discussed	O
in	O
Yang	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
are	O
able	O
to	O
forgo	O
the	O
adversarial	O
training	O
for	O
the	O
discriminator	O
,	O
since	O
the	O
fine	O
-	O
tuned	O
discriminative	O
language	O
model	O
is	O
implicitly	O
capable	O
of	O
assigning	O
high	O
perplexity	B-MetricName
to	O
negative	O
samples	O
(	O
out	O
-	O
of	O
-	O
style	O
samples	O
)	O
,	O
as	O
shown	O
in	O
Section	O
4.1	O
.	O
For	O
the	O
transferred	O
sentence	O
x	O
,	O
the	O
training	O
objective	O
for	O
each	O
target	O
style	O
s	O
i	O
is	O
,	O
argmin	O
θ	B-HyperparameterName
G	O
L	O
s	O
i	O
=	O
E	O
x∼T	O
,	O
x	O
∼P	O
θ	B-HyperparameterName
G	O
(	O
x	O
)	O
n	O
t=1	O
−	O
log	O
P	O
LM	O
i	O
(	O
x	O
t	O
|	O
x	O
1	O
,	O
..	O
,	O
x	O
t−1	O
)	O
(	O
3	O
)	O
This	O
dictates	O
that	O
transferred	O
sentence	O
x	O
has	O
low	O
perplexity	B-MetricName
on	O
the	O
language	O
model	O
fine	O
-	O
tuned	O
on	O
style	O
s	O
i	O
,	O
for	O
each	O
target	O
style	O
s	O
i	O
.	O
However	O
,	O
we	O
can	O
not	O
directly	O
find	O
the	O
argmin	O
θ	B-HyperparameterName
G	O
using	O
gradient	O
descent	O
because	O
of	O
discrete	O
sampling	O
of	O
x	O
∼	O
P	O
θ	B-HyperparameterName
G	O
(	O
x	O
)	O
.	O
To	O
account	O
for	O
this	O
,	O
we	O
use	O
a	O
policy	O
gradient	O
reinforcement	O
learning	O
approach	O
using	O
REINFORCE	B-MethodName
algorithm	O
(	O
Sutton	O
et	O
al	O
,	O
1999	O
)	O
.	O
The	O
reward	O
for	O
an	O
input	O
sequence	O
x	O
to	O
the	O
style	O
discriminator	O
LM	O
i	O
is	O
calculated	O
as	O
,	O
r	O
(	O
x	O
)	O
=	O
n	O
t=1	O
log	O
P	O
LM	O
i	O
(	O
x	O
t	O
|	O
x	O
1	O
,	O
..	O
,	O
x	O
t−1	O
)	O
(	O
4	O
)	O
Using	O
these	O
rewards	O
,	O
the	O
RL	O
objective	O
is	O
to	O
minimize	O
the	O
loss	B-MetricName
L	O
s	O
i	O
given	O
by	O
,	O
L	O
s	O
i	O
=	O
E	O
x∼T	O
,	O
x	O
∼P	O
θ	B-HyperparameterName
G	O
(	O
x	O
)	O
(	O
r	O
(	O
x	O
)	O
−	O
r	O
(	O
x	O
)	O
)	O
[	O
−	O
log	O
P	O
θ	B-HyperparameterName
G	O
(	O
x	O
|	O
x	O
)	O
]	O
(	O
5	O
)	O
for	O
style	O
s	O
i	O
,	O
where	O
P	O
θ	B-HyperparameterName
G	O
(	O
x	O
|	O
x	O
)	O
is	O
as	O
in	O
Equation	O
1and	O
r	O
(	O
x	O
)	O
is	O
the	O
reward	O
in	O
the	O
Equation	O
4	O
for	O
the	O
transferred	O
sentence	O
x	O
.	O
The	O
rewards	O
r	O
(	O
x	O
)	O
represents	O
the	O
baseline	O
reward	O
of	O
greedily	O
sampling	O
the	O
input	O
sequence	O
x	O
by	O
the	O
style	O
discriminator	O
LM	O
i	O
.	O
For	O
the	O
style	O
combination	O
s	O
=	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
.	O
.	O
.	O
,	O
s	O
k	O
}	O
,	O
the	O
joint	O
encoder	O
-	O
decoder	O
model	O
is	O
trained	O
on	O
randomized	O
mixture	O
of	O
data	O
from	O
each	O
of	O
the	O
targetdomain	O
corpus	O
.	O
The	O
mixture	O
is	O
thus	O
agnostic	O
of	O
individual	O
style	O
of	O
each	O
of	O
the	O
sentence	O
and	O
the	O
discriminative	O
LM	O
for	O
each	O
style	O
guides	O
the	O
generation	O
towards	O
that	O
specific	O
style	O
by	O
rewarding	O
style	O
adherence	O
in	O
the	O
transferred	O
sentence	O
.	O
Randomized	O
mixture	O
of	O
training	O
corpus	O
across	O
styles	O
allows	O
for	O
unified	O
and	O
cohesive	O
understanding	O
of	O
multiple	O
styles	O
by	O
diversifying	O
rewards	O
from	O
different	O
discriminators	O
across	O
samples	O
.	O
The	O
overall	O
training	O
loss	B-MetricName
for	O
the	O
joint	O
encoder	O
-	O
decoder	O
model	O
is	O
L	O
=	O
λDAEEx∼T	O
[	O
−	O
log	O
P	O
θ	B-HyperparameterName
(	O
x	O
|	O
x	O
)	O
]	O
+	O
k	O
i=1	O
λiL	O
s	O
i	O
,	O
(	O
6	O
)	O
where	O
L	O
s	O
i	O
is	O
as	O
defined	O
in	O
Equation	O
5	O
,	O
and	O
λ	O
DAE	O
and	O
{	O
λ	O
i	O
}	O
k	O
i=1	O
are	O
hyper	O
-	O
parameters	O
.	O
The	O
overall	O
training	O
process	O
is	O
summarized	O
in	O
Figure	O
1	O
.	O
First	O
,	O
we	O
pre	O
-	O
train	O
a	O
transformer	O
model	O
with	O
Masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
objective	O
as	O
shown	O
in	O
Figure	O
1	O
(	O
Left	O
)	O
.	O
We	O
then	O
initialize	O
discriminator	O
model	O
with	O
this	O
pre	O
-	O
trained	O
language	O
model	O
and	O
fine	O
-	O
tune	O
it	O
with	O
Causal	O
language	O
modeling	O
objective	O
,	O
shown	O
in	O
Figure	O
1	O
(	O
Right	O
)	O
,	O
for	O
each	O
target	O
style	O
.	O
Finally	O
,	O
we	O
initialize	O
the	O
encoder	O
and	O
decoder	O
of	O
the	O
style	B-MethodName
transfer	I-MethodName
module	I-MethodName
with	O
the	O
pretrained	O
and	O
style	O
-	O
specific	O
fine	O
-	O
tuned	O
language	O
models	O
,	O
respectively	O
.	O
In	O
case	O
of	O
multiple	O
styles	O
,	O
the	O
decoder	O
can	O
be	O
initialized	O
with	O
the	O
language	O
model	O
which	O
is	O
fine	O
-	O
tuned	O
with	O
CLM	O
loss	B-MetricName
on	O
the	O
mixture	O
of	O
data	O
from	O
target	O
styles	O
,	O
i.e.	O
,	O
CLM	O
loss	B-MetricName
in	O
Equation	O
2	O
with	O
x	O
∼	O
T	O
.	O
The	O
joint	O
encoder	O
-	O
decoder	O
model	O
(	O
Figure	O
1	O
(	O
Centre	O
)	O
)	O
is	O
then	O
trained	O
with	O
a	O
combination	O
of	O
DAE	O
objective	O
and	O
rewards	O
from	O
fine	O
-	O
tuned	O
discriminators	O
of	O
respective	O
target	O
styles	O
.	O

We	O
experiment	O
with	O
a	O
combination	O
of	O
sentiment	O
and	O
formality	O
styles	O
.	O
For	O
sentiment	O
,	O
we	O
use	O
a	O
mixture	O
of	O
IMDB	B-DatasetName
(	O
Maas	O
et	O
al	O
,	O
2011	O
)	O
and	O
Yelp	O
dataset	O
(	O
Li	O
et	O
al	O
,	O
2018	O
)	O
with	O
300k	O
examples	O
in	O
the	O
positive	O
and	O
negative	O
sentiment	O
each	O
.	O
For	O
formality	O
,	O
we	O
use	O
GYAFC	B-DatasetName
corpus	O
(	O
Rao	O
and	O
Tetreault	O
,	O
2018	O
)	O
which	O
has	O
104k	O
examples	O
in	O
each	O
formal	O
and	O
informal	O
class	O
.	O
The	O
test	O
set	O
has	O
3000	O
and	O
4849	O
examples	O
for	O
sentiment	O
and	O
formality	O
respectively	O
,	O
following	O
the	O
data	O
split	O
available	O
in	O
Dai	O
et	O
al	O
(	O
2019	O
)	O
;	O
Rao	O
and	O
Tetreault	O
(	O
2018	O
)	O
.	O
For	O
both	O
datasets	O
,	O
the	O
training	O
corpus	O
is	O
non	O
-	O
parallel	O
and	O
the	O
test	O
corpus	O
has	O
human	O
written	O
references	O
available	O
,	O
which	O
we	O
use	O
for	O
content	O
evaluation	O
(	O
Section	O
4.2	O
)	O
.	O
For	O
pre	O
-	O
training	O
,	O
we	O
use	O
12	O
-	O
layer	O
Transformer	B-MethodName
model	O
with	O
512	O
hidden	O
units	O
,	O
16	O
heads	O
,	O
a	O
dropout	O
rate	O
of	O
0.1	O
and	O
learned	O
positional	O
embedding	O
.	O
We	O
train	O
our	O
models	O
with	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
,	O
and	O

To	O
evaluate	O
style	O
variation	O
across	O
language	O
models	O
fine	O
-	O
tuned	O
on	O
different	O
styles	O
,	O
we	O
compare	O
the	O
generations	O
of	O
the	O
fine	O
-	O
tuned	O
models	O
.	O
For	O
singledimensional	O
style	O
evaluation	O
,	O
we	O
generate	O
sentences	O
from	O
models	O
fine	O
-	O
tuned	O
on	O
negative	O
corpus	O
and	O
positive	O
corpus	O
and	O
compare	O
the	O
style	O
accuracy	B-MetricName
of	O
generated	O
sentences	O
.	O
The	O
style	O
accuracy	B-MetricName
is	O
evaluated	O
by	O
employing	O
a	O
FastText	B-MethodName
(	O
Joulin	O
et	O
al	O
,	O
2016	O
)	O
classifier	O
trained	O
on	O
the	O
corresponding	O
style	O
dimension	O
.	O
For	O
instance	O
,	O
the	O
classifier	O
for	O
evaluating	O
sentiment	O
accuracy	B-MetricName
is	O
trained	O
on	O
sentiment	O
corpus	O
tagged	O
with	O
positive	O
and	O
negative	O
class	O
in	O
IMDB	B-DatasetName
and	O
Yelp	O
data	O
.	O
Table	O
1	O
shows	O
the	O
accuracy	B-MetricName
of	O
sentences	O
generated	O
by	O
a	O
model	O
fine	O
-	O
tuned	O
on	O
style	O
s	O
i	O
as	O
belonging	O
to	O
the	O
class	O
s	O
i	O
.	O
For	O
both	O
sentiment	O
and	O
formality	O
,	O
the	O
fine	O
-	O
tuned	O
language	O
models	O
are	O
able	O
to	O
generate	O
text	O
faithful	O
to	O
the	O
target	O
style	O
dimension	O
.	O
Thus	O
,	O
we	O
conclude	O
that	O
the	O
language	O
models	O
trained	O
on	O
style	O
s	O
i	O
are	O
able	O
to	O
capture	O
the	O
essence	O
of	O
the	O
corresponding	O
style	O
reasonably	O
well	O
.	O
These	O
accuracies	O
are	O
an	O
indication	O
of	O
the	O
style	O
awareness	O
in	O
these	O
fine	O
-	O
tuned	O
LMs	O
.	O
We	O
,	O
therefore	O
,	O
employ	O
the	O
perplexities	O
of	O
these	O
fine	O
-	O
tuned	O
language	O
models	O
to	O
gauge	O
the	O
style	O
of	O
the	O
input	O
text	O
to	O
guide	O
our	O
style	B-TaskName
transfer	I-TaskName
model	O
.	O
As	O
discussed	O
in	O
discriminative	O
modeling	O
(	O
Section	O
3.2	O
)	O
,	O
the	O
model	O
fine	O
-	O
tuned	O
with	O
corpus	O
from	O
a	O
certain	O
style	O
is	O
expected	O
to	O
have	O
high	O
perplexity	B-MetricName
on	O
sentence	O
not	O
from	O
that	O
style	O
and	O
low	O
perplexity	B-MetricName
otherwise	O
.	O
To	O
this	O
end	O
,	O
we	O
experiment	O
with	O
two	O
models	O
independently	O
finetuned	O
on	O
positive	O
and	O
negative	O
corpus	O
.	O
We	O
calculate	O
the	O
perplexity	B-MetricName
of	O
each	O
of	O
these	O
models	O
on	O
the	O
test	O
corpus	O
from	O
the	O
same	O
style	O
and	O
from	O
the	O
opposite	O
style	O
.	O
As	O
seen	O
in	O
Table	O
2	O
,	O
the	O
perplexity	B-MetricName
for	O
each	O
model	O
is	O
substantially	O
lower	O
on	O
the	O
same	O
corpus	O
as	O
compared	O
to	O
that	O
on	O
the	O
opposite	O
corpus	O
.	O
This	O
implies	O
that	O
a	O
language	O
model	O
fine	O
-	O
tuned	O
on	O
positive	O
corpus	O
shows	O
higher	O
perplexity	B-MetricName
for	O
negative	O
sentences	O
and	O
lower	O
for	O
positive	O
sentences	O
and	O
vice	O
versa	O
.	O
This	O
corroborates	O
the	O
effectiveness	O
of	O
these	O
fine	O
-	O
tuned	O
language	O
models	O
to	O
serve	O
as	O
discriminators	O
for	O
training	O
the	O
style	B-MethodName
transfer	I-MethodName
module	I-MethodName
.	O

We	O
measure	O
the	O
performance	O
of	O
our	O
model	O
and	O
the	O
baselines	O
based	O
on	O
the	O
style	O
control	O
,	O
content	O
preservation	O
and	O
fluency	O
.	O
To	O
measure	O
the	O
accuracy	B-MetricName
of	O
style	B-TaskName
transfer	I-TaskName
,	O
we	O
train	O
two	O
Fasttext	B-MethodName
2	O
classifiers	O
independently	O
for	O
sentiment	O
and	O
formality	O
using	O
the	O
train	O
corpus	O
,	O
as	O
described	O
in	O
Section	O
4.1	O
.	O
These	O
classifiers	O
have	O
accuracy	B-MetricName
of	O
93.74	O
%	O
and	O
88.95	O
%	O
respectively	O
on	O
test	O
corpus	O
of	O
respective	O
datasets	O
.	O
We	O
note	O
that	O
formality	O
as	O
a	O
style	O
is	O
more	O
intricately	O
designed	O
,	O
so	O
we	O
also	O
check	O
lexical	O
scoring	O
by	O
Brooke	O
et	O
al	O
(	O
2010	O
)	O
to	O
evaluate	O
formality	O
,	O
which	O
uses	O
a	O
formality	O
lexicon	O
to	O
assign	O
formality	O
score	O
between	O
−1	O
(	O
informal	O
)	O
and	O
1	O
(	O
formal	O
)	O
to	O
each	O
word	O
and	O
averages	O
it	O
.	O
We	O
scale	O
these	O
scores	O
between	O
0	B-DatasetName
-	O
100	O
,	O
where	O
higher	O
(	O
100	O
)	O
lexical	O
score	O
signifies	O
formal	O
style	O
and	O
lower	O
(	O
0	B-DatasetName
)	O
score	O
signifies	O
informal	O
style	O
.	O
For	O
informal	O
target	O
style	O
,	O
we	O
report	O
lexical	O
score	O
as	O
100	O
−	O
n	O
,	O
so	O
that	O
a	O
higher	O
average	O
lexical	O
score	O
signifies	O
a	O
better	O
transfer	O
for	O
either	O
polarity	O
.	O
To	O
measure	O
content	O
preservation	O
on	O
transfer	O
,	O
we	O
calculate	O
the	O
BLEU	B-MetricName
score	I-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
between	O
the	O
transferred	O
sentence	O
and	O
the	O
input	O
sentence	O
(	O
self	O
-	O
BLEU	B-MetricName
)	O
.	O
Besides	O
this	O
,	O
we	O
also	O
calculate	O
BLEU	B-MetricName
score	I-MetricName
between	O
the	O
transferred	O
sentence	O
generated	O
by	O
our	O
model	O
and	O
the	O
corresponding	O
human	O
reference	O
transferred	O
sentence	O
,	O
available	O
for	O
GYAFC	B-DatasetName
and	O
Yelp	O
corpus	O
(	O
ref	O
-	O
BLEU	B-MetricName
)	O
.	O
Since	O
both	O
these	O
corpus	O
account	O
for	O
transfer	O
across	O
only	O
one	O
style	O
dimension	O
each	O
,	O
the	O
provided	O
references	O
are	O
only	O
partial	O
indication	O
of	O
expected	O
outcome	O
.	O
This	O
Dai	O
et	O
al	O
,	O
2019	O
)	O
,	O
Cascaded	O
Discriminative	O
LM	O
method	O
and	O
multi	O
-	O
style	B-TaskName
transfer	I-TaskName
using	O
Adapted	O
Rewriting	O
LM	O
(	O
Syed	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
upward	O
arrow	O
signifies	O
that	O
higher	O
is	O
better	O
and	O
vice	O
versa	O
.	O
Score	B-MetricName
of	O
near	O
100	O
on	O
formality	O
lexical	O
scoring	O
imply	O
the	O
transferred	O
text	O
is	O
close	O
in	O
formality	O
to	O
the	O
target	O
corpus	O
.	O
is	O
also	O
apparent	O
from	O
low	O
ref	O
-	O
BLEU	B-MetricName
scores	O
for	O
our	O
model	O
as	O
well	O
as	O
baselines	O
.	O
Since	O
,	O
the	O
results	O
are	O
presented	O
on	O
aggregated	O
dataset	O
from	O
both	O
these	O
style	O
dimensions	O
,	O
this	O
evaluation	O
is	O
still	O
able	O
to	O
provide	O
reasonable	O
indication	O
of	O
content	O
preservation	O
.	O
To	O
measure	O
the	O
fluency	O
of	O
the	O
text	O
,	O
we	O
calculate	O
perplexity	B-MetricName
assigned	O
to	O
the	O
generated	O
text	O
sequence	O
by	O
a	O
language	O
model	O
trained	O
on	O
the	O
train	O
corpus	O
,	O
as	O
is	O
standard	O
in	O
style	B-TaskName
transfer	I-TaskName
literature	O
(	O
Dai	O
et	O
al	O
,	O
2019	O
;	O
Subramanian	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
perplexity	B-MetricName
is	O
the	O
measure	O
of	O
log	O
likelihood	O
of	O
the	O
generated	O
sentence	O
on	O
the	O
language	O
model	O
.	O
A	O
lower	O
perplexity	B-MetricName
is	O
indicative	O
of	O
a	O
more	O
fluent	O
sentence	O
.	O
We	O
use	O
a	O
generative	O
transformer	O
-	O
based	O
language	O
model	O
trained	O
on	O
the	O
dataset	O
combined	O
from	O
two	O
styles	O
.	O
Dai	O
et	O
al	O
(	O
2019	O
)	O
use	O
transformer	O
-	O
based	O
model	O
(	O
Style	O
Transformer	B-MethodName
)	O
for	O
single	O
-	O
dimensional	O
style	B-TaskName
transfer	I-TaskName
.	O
We	O
train	O
two	O
independent	O
Style	O
Transformer	B-MethodName
models	O
for	O
sentiment	O
and	O
formality	O
transfer	O
and	O
then	O
perform	O
transfer	O
one	O
after	O
another	O
to	O
compare	O
results	O
with	O
our	O
model	O
.	O
We	O
term	O
this	O
as	O
Cascaded	O
Style	O
Transformer	B-MethodName
setup	O
.	O
The	O
Style	O
Transformer	B-MethodName
model	O
is	O
shown	O
to	O
have	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
single	O
-	O
dimensional	O
style	B-TaskName
transfer	I-TaskName
;	O
thus	O
it	O
provides	O
an	O
estimate	O
of	O
the	O
performance	O
of	O
sequential	O
single	O
style	B-TaskName
transfer	I-TaskName
.	O
We	O
also	O
experiment	O
with	O
Adapted	O
Rewriting	O
LM	O
(	O
Syed	O
et	O
al	O
,	O
2020	O
)	O
as	O
another	O
baseline	O
.	O
Their	O
work	O
on	O
style	O
rewriting	O
to	O
match	O
author	O
-	O
specific	O
style	O
does	O
not	O
require	O
explicit	O
annotations	O
for	O
the	O
various	O
aspects	O
that	O
constitutes	O
an	O
author	O
's	O
style	O
,	O
but	O
is	O
based	O
on	O
the	O
assumption	O
that	O
the	O
training	O
corpus	O
reflects	O
the	O
target	O
style	O
.	O
In	O
this	O
context	O
,	O
we	O
train	O
their	O
framework	O
on	O
the	O
mixture	O
of	O
data	O
from	O
the	O
respective	O
target	O
styles	O
and	O
report	O
the	O
performance	O
.	O
These	O
are	O
the	O
closest	O
baselines	O
to	O
our	O
proposed	O
approach	O
,	O
since	O
other	O
works	O
dealing	O
with	O
multi	O
-	O
style	B-TaskName
transfer	I-TaskName
assume	O
presence	O
of	O
jointly	O
annotated	O
dataset	O
,	O
which	O
is	O
a	O
stronger	O
assumption	O
that	O
we	O
aim	O
to	O
relax	O
.	O
In	O
addition	O
to	O
our	O
proposed	O
model	O
with	O
multiple	O
style	B-TaskName
transfer	I-TaskName
,	O
we	O
also	O
train	O
our	O
encoder	O
-	O
decoder	O
architecture	O
with	O
single	O
discriminative	O
LM	O
for	O
one	O
style	O
at	O
a	O
time	O
and	O
perform	O
two	O
stage	O
transfer	O
,	O
similar	O
to	O
one	O
with	O
Cascaded	O
Style	O
Transformer	B-MethodName
(	O
Dai	O
et	O
al	O
,	O
2019	O
)	O
setup	O
.	O

The	O
results	O
in	O
Table	O
3	O
show	O
that	O
our	O
model	O
achieves	O
better	O
style	O
control	O
than	O
the	O
Cascaded	O
Style	O
Transformer	B-MethodName
(	O
Dai	O
et	O
al	O
,	O
2019	O
)	O
as	O
well	O
as	O
the	O
joint	O
transfer	O
using	O
Syed	O
et	O
al	O
(	O
2020	O
)	O
for	O
both	O
sentiment	O
and	O
formality	O
.	O
As	O
seen	O
in	O
Table	O
3	O
,	O
cascaded	O
style	B-TaskName
transfer	I-TaskName
models	O
perform	O
poorly	O
on	O
content	O
preservation	O
.	O
This	O
is	O
because	O
transferring	O
style	O
one	O
after	O
other	O
leads	O
to	O
huge	O
loss	B-MetricName
in	O
content	O
,	O
thus	O
both	O
the	O
two	O
-	O
stage	O
models	O
score	O
lower	O
on	O
content	O
preservation	O
metrics	O
,	O
both	O
w.r.t	O
.	O
the	O
input	O
text	O
and	O
the	O
reference	O
transferred	O
text	O
.	O
This	O
demonstrates	O
the	O
advantage	O
of	O
using	O
single	O
model	O
to	O
control	O
for	O
multiple	O
styles	O
.	O
The	O
effect	O
can	O
also	O
be	O
observed	O
in	O
Table	O
4	O
which	O
demonstrates	O
qualitative	O
results	O
for	O
Cascaded	O
Style	O
Transformer	B-MethodName
model	O
and	O
our	O
model	O
.	O
We	O
can	O
see	O
in	O
many	O
cases	O
content	O
loses	O
the	O
underlying	O
meaning	O
of	O
source	O
sentence	O
during	O
the	O
twostage	O
transfer	O
,	O
whereas	O
our	O
model	O
is	O
able	O
to	O
retain	O
original	O
meaning	O
of	O
the	O
sentence	O
well	O
,	O
corroborating	O
the	O
findings	O
of	O
automatic	O
evaluation	O
.	O
Among	O
the	O
cascaded	O
models	O
,	O
the	O
Discriminative	O
LM	O
scores	O
marginally	O
better	O
on	O
content	O
preservation	O
than	O
the	O
Style	O
Transformer	B-MethodName
model	O
.	O
We	O
attribute	O
this	O
to	O
initialization	O
with	O
the	O
same	O
pre	O
-	O
trained	O
LM	O
resulting	O
in	O
shared	O
content	O
space	O
in	O
the	O
underlying	O
single	O
style	B-TaskName
transfer	I-TaskName
models	O
.	O
However	O
,	O
due	O
to	O
independent	O
training	O
of	O
the	O
two	O
single	O
style	B-TaskName
transfer	I-TaskName
models	O
,	O
they	O
are	O
not	O
able	O
to	O
model	O
interplay	O
between	O
these	O
styles	O
and	O
hence	O
perform	O
worse	O
on	O
style	O
control	O
than	O
our	O
proposed	O
model	O
trained	O
jointly	O
on	O
multiple	O
styles	O
.	O
Our	O
model	O
also	O
scores	O
better	O
on	O
fluency	O
,	O
as	O
seen	O
in	O
Table	O
3	O
.	O
This	O
is	O
also	O
apparent	O
from	O
the	O
exam	O
-	O
Give	O
your	O
brother	O
some	O
money	O
and	O
tell	O
him	O
to	O
take	O
a	O
hike	O
.	O
Just	O
give	O
your	O
brother	O
some	O
time	O
and	O
it	O
will	O
be	O
good	O
again	O
.	O
Give	O
your	O
brother	O
some	O
money	O
and	O
request	O
him	O
to	O
leave	O
.	O

To	O
augment	O
automatic	O
evaluation	O
results	O
,	O
we	O
conduct	O
a	O
human	O
study	O
to	O
evaluate	O
the	O
model	O
outputs	O
across	O
various	O
dimensions	O
such	O
as	O
content	O
preservation	O
,	O
style	O
control	O
,	O
fluency	O
,	O
and	O
overall	O
trans	O
-	O
fer	O
quality	O
.	O
Based	O
on	O
comparable	O
style	O
control	O
in	O
Cascaded	O
Style	O
Transformer	B-MethodName
and	O
our	O
proposed	O
approach	O
on	O
automatic	O
metrics	O
,	O
we	O
compare	O
the	O
transfer	O
quality	O
across	O
these	O
two	O
models	O
by	O
a	O
small	O
-	O
scale	O
human	O
study	O
.	O
We	O
select	O
40	O
sentences	O
,	O
with	O
10	O
examples	O
from	O
each	O
combinations	O
of	O
sentiment	O
and	O
formality	O
as	O
target	O
style	O
,	O
and	O
collect	O
annotations	O
from	O
4	O
-	O
5	O
participants	O
for	O
each	O
example	O
.	O
Out	O
of	O
resulting	O
annotations	O
,	O
more	O
than	O
85	O
%	O
annotations	O
favoured	O
our	O
results	O
over	O
baseline	O
.	O
The	O
average	O
participant	O
rating	O
across	O
different	O
dimensions	O
is	O
shown	O
in	O
Table	O
5	O
.	O
We	O
test	O
the	O
statistical	O
significance	O
of	O
these	O
results	O
using	O
z	O
-	O
test	O
statistic	O
.	O
With	O
α	B-HyperparameterName
=	O
0.05	O
,	O
the	O
preferences	O
indicated	O
in	O
human	O
study	O
are	O
significant	O
across	O
all	O
metrics	O
.	O
These	O
results	O
are	O
in	O
line	O
with	O
our	O
automatic	O
evaluations	O
and	O
add	O
confidence	O
to	O
the	O
efficacy	O
of	O
our	O
proposed	O
approach	O
in	O
achieving	O
style	B-TaskName
transfer	I-TaskName
across	O
multiple	O
dimensions	O
.	O

Numerical	O
reasoning	O
skills	O
are	O
essential	O
for	O
complex	O
question	B-TaskName
answering	I-TaskName
(	O
CQA	O
)	O
over	O
text	O
.	O
It	O
requires	O
opertaions	O
including	O
counting	O
,	O
comparison	O
,	O
addition	O
and	O
subtraction	O
.	O
A	O
successful	O
approach	O
to	O
CQA	O
on	O
text	O
,	O
Neural	O
Module	O
Networks	O
(	O
NMNs	O
)	O
,	O
follows	O
the	O
programmer	O
-	O
interpreter	O
paradigm	O
and	O
leverages	O
specialised	O
modules	O
to	O
perform	O
compositional	O
reasoning	O
.	O
However	O
,	O
the	O
NMNs	O
framework	O
does	O
not	O
consider	O
the	O
relationship	O
between	O
numbers	O
and	O
entities	O
in	O
both	O
questions	O
and	O
paragraphs	O
.	O
We	O
propose	O
effective	O
techniques	O
to	O
improve	O
NMNs	O
'	O
numerical	O
reasoning	O
capabilities	O
by	O
making	O
the	O
interpreter	O
questionaware	O
and	O
capturing	O
the	O
relationship	O
between	O
entities	O
and	O
numbers	O
.	O
On	O
the	O
same	O
subset	O
of	O
the	O
DROP	B-DatasetName
dataset	O
for	O
CQA	O
on	O
text	O
,	O
experimental	O
results	O
show	O
that	O
our	O
additions	O
outperform	O
the	O
original	O
NMNs	O
by	O
3.0	O
points	O
for	O
the	O
overall	O
F1	B-MetricName
score	I-MetricName
.	O

Complex	O
Question	B-TaskName
Answering	I-TaskName
(	O
CQA	O
)	O
is	O
a	O
challenging	O
task	O
,	O
requiring	O
a	O
model	O
to	O
perform	O
compositional	O
and	O
numerical	O
reasoning	O
.	O
Originally	O
proposed	O
for	O
the	O
visual	B-DatasetName
question	I-DatasetName
answering	I-DatasetName
(	O
VQA	B-TaskName
)	O
task	O
,	O
Neural	O
Module	O
Networks	O
(	O
NMNs	O
)	O
(	O
Andreas	O
et	O
al	O
,	O
2016	O
)	O
have	O
recently	O
been	O
adopted	O
to	O
tackle	O
the	O
CQA	O
problem	O
over	O
text	O
(	O
Gupta	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
NMNs	O
is	O
an	O
end	O
-	O
to	O
-	O
end	O
differentiable	O
model	O
in	O
the	O
programmer	O
-	O
interpreter	O
paradigm	O
(	O
Guo	O
et	O
al	O
,	O
2020	O
;	O
Hua	O
et	O
al	O
,	O
2020a	O
,	O
b	O
)	O
.	O
Briefly	O
,	O
the	O
programmer	O
learns	O
to	O
map	O
each	O
question	O
into	O
a	O
program	O
,	O
i.e.	O
a	O
sequence	O
of	O
neural	O
modules	O
,	O
and	O
the	O
interpreter	O
then	O
"	O
executes	O
"	O
the	O
program	O
,	O
operationalized	O
by	O
modules	O
,	O
on	O
the	O
paragraph	O
to	O
yield	O
the	O
answer	O
for	O
different	O
types	O
of	O
complex	O
questions	O
.	O
NMNs	O
achieves	O
the	O
best	O
performance	O
on	O
a	O
subset	O
of	O
the	O
challenging	O
DROP	B-DatasetName
dataset	O
(	O
Dua	O
et	O
al	O
,	O
2019	O
)	O
and	O
is	O
interpertable	O
by	O
nature	O
.	O
However	O
,	O
NMNs	O
'	O
performance	O
advantage	O
is	O
not	O
consistent	O
,	O
as	O
it	O
underperforms	O
in	O
some	O
types	O
of	O
questions	O
that	O
require	O
numerical	O
reasoning	O
.	O
For	O
instance	O
,	O
for	O
date	O
-	O
compare	O
questions	O
,	O
MTMSN	O
(	O
Hu	O
et	O
al	O
,	O
2019	O
)	O
achieves	O
an	O
F1	B-MetricName
score	I-MetricName
of	O
85.2	O
1	O
,	O
whereas	O
NMNs	O
'	O
performance	O
is	O
82.6	O
.	O
Similarly	O
,	O
for	O
count	O
questions	O
,	O
the	O
F1	B-MetricName
score	I-MetricName
is	O
61.6	O
for	O
MTMSN	O
and	O
55.7	O
for	O
NMNs	O
.	O
This	O
performance	O
gap	O
stems	O
from	O
two	O
deficiencies	O
of	O
NMNs	O
,	O
which	O
we	O
describe	O
below	O
with	O
the	O
help	O
of	O
two	O
examples	O
in	O
Figure	O
1	O
.	O
Firstly	O
,	O
NMNs	O
'	O
interpreter	O
is	O
oblivious	O
to	O
the	O
question	O
when	O
executing	O
number	O
-	O
related	O
modules	O
.	O
For	O
executing	O
number	O
-	O
related	O
modules	O
,	O
the	O
interpreter	O
only	O
receives	O
the	O
paragraph	O
as	O
input	O
,	O
but	O
not	O
the	O
question	O
.	O
Such	O
a	O
lack	O
of	O
direct	O
interactions	O
with	O
the	O
question	O
impairs	O
model	O
performance	O
:	O
the	O
entities	O
in	O
the	O
question	O
,	O
which	O
may	O
also	O
occur	O
in	O
the	O
paragraph	O
,	O
can	O
help	O
locate	O
significant	O
and	O
relevant	O
numbers	O
to	O
produce	O
the	O
final	O
answer	O
.	O
In	O
the	O
first	O
example	O
in	O
Figure	O
1	O
,	O
if	O
the	O
interpreter	O
is	O
aware	O
of	O
the	O
correct	O
event	O
mentioned	O
in	O
the	O
question	O
(	O
i.e.	O
"	O
the	O
Constituent	O
Assembly	O
being	O
elected	O
"	O
)	O
,	O
it	O
can	O
easily	O
find	O
the	O
same	O
event	O
in	O
the	O
paragraph	O
and	O
further	O
locate	O
its	O
date	O
(	O
"	O
12	O
November	O
"	O
)	O
precisely	O
.	O
Without	O
this	O
knowledge	O
,	O
the	O
original	O
NMNs	O
found	O
the	O
wrong	O
event	O
(	O
i.e.	O
"	O
dissolved	O
the	O
Constituent	O
Assembly	O
"	O
)	O
,	O
thus	O
the	O
wrong	O
date	O
(	O
"	O
January	O
1918	O
"	O
)	O
,	O
leading	O
to	O
an	O
incorrect	O
answer	O
.	O
Secondly	O
,	O
NMNs	O
disregards	O
the	O
relative	O
positioning	O
of	O
entities	O
and	O
their	O
related	O
numbers	O
in	O
the	O
paragraph	O
.	O
Although	O
NMNs	O
can	O
learn	O
separate	O
distributions	O
over	O
numbers	O
extracted	O
from	O
a	O
paragraph	O
,	O
it	O
does	O
not	O
have	O
an	O
effective	O
mechanism	O
to	O
identify	O
the	O
number	O
that	O
connects	O
to	O
a	O
given	O
entity	O
.	O
Such	O
an	O
ability	O
to	O
recognise	O
the	O
association	O
among	O
numbers	O
and	O
entities	O
is	O
vital	O
for	O
learning	O
numerical	O
reasoning	O
skills	O
:	O
the	O
operation	O
between	O
numbers	O
is	O
meaningful	O
only	O
when	O
they	O
refer	O
to	O
the	O
same	O
entity	O
or	O
the	O
same	O
type	O
of	O
entities	O
.	O
The	O
second	O
example	O
in	O
Figure	O
1	O
illustrates	O
the	O
positioning	O
of	O
entities	O
and	O
their	O
related	O
numbers	O
.	O
With	O
only	O
a	O
constraint	O
on	O
a	O
window	O
around	O
an	O
entity	O
,	O
the	O
NMNs	O
'	O
interpreter	O
tends	O
to	O
identify	O
the	O
nearest	O
number	O
as	O
the	O
related	O
one	O
to	O
a	O
given	O
entity	O
(	O
"	O
August	O
1996	O
to	O
December	O
1997	O
"	O
for	O
entity	O
"	O
PUK	O
and	O
KDP	O
later	O
co	O
-	O
operated	O
"	O
)	O
,	O
resulting	O
in	O
wrong	O
predictions	O
.	O
Figure	O
1	O
:	O
Two	O
examples	O
in	O
the	O
DROP	B-DatasetName
(	O
Dua	O
et	O
al	O
,	O
2019	O
)	O
dataset	O
that	O
demonstrate	O
the	O
deficienties	O
of	O
NMNs	O
.	O
Tokens	O
pertinent	O
to	O
our	O
discussion	O
are	O
highlighted	O
in	O
red	O
,	O
and	O
their	O
relevant	O
numbers	O
are	O
highlighted	O
in	O
orange	O
.	O
Solid	O
blue	O
lines	O
are	O
predictions	O
of	O
our	O
model	O
,	O
while	O
dotted	O
blue	O
lines	O
show	O
the	O
predictions	O
of	O
NMNs	O
.	O
We	O
propose	O
three	O
simple	O
and	O
effective	O
mechanisms	O
to	O
improve	O
NMNs	O
'	O
numerical	O
reasoning	O
capabilities	O
.	O
Firstly	O
,	O
we	O
improve	O
the	O
interpreter	O
to	O
make	O
it	O
questionaware	O
.	O
By	O
explicitly	O
conditioning	O
the	O
execution	O
on	O
the	O
question	O
,	O
the	O
interpreter	O
can	O
exploit	O
the	O
information	O
contained	O
in	O
the	O
question	O
.	O
Secondly	O
,	O
we	O
propose	O
an	O
intuitive	O
constraint	O
to	O
better	O
relate	O
numbers	O
and	O
their	O
corresponding	O
entities	O
in	O
the	O
paragraph	O
.	O
Finally	O
,	O
we	O
strengthen	O
the	O
auxiliary	O
loss	B-MetricName
to	O
increase	O
attention	O
values	O
of	O
entities	O
in	O
closer	O
vicinity	O
within	O
a	O
sentence	O
.	O
Experimental	O
results	O
show	O
that	O
our	O
modifications	O
significantly	O
improve	O
NMNs	O
'	O
numerical	O
reasoning	O
performance	O
by	O
up	O
to	O
3.0	O
absolute	O
F1	B-MetricName
points	O
.	O
With	O
minor	O
modification	O
,	O
these	O
mechanisms	O
are	O
simple	O
enough	O
to	O
be	O
applied	O
to	O
other	O
modular	O
approaches	O
.	O

In	O
this	O
section	O
,	O
we	O
will	O
discuss	O
the	O
deficiencies	O
of	O
NMNs	O
described	O
in	O
Section	O
1	O
and	O
propose	O
three	O
techniques	O
to	O
overcome	O
these	O
problems	O
.	O
Considering	O
the	O
importance	O
of	O
questions	O
while	O
executing	O
programs	O
,	O
we	O
incorporate	O
a	O
question	O
-	O
to	O
-	O
paragraph	O
alignment	O
matrix	O
to	O
form	O
a	O
question	O
-	O
aware	O
interpreter	O
in	O
Section	O
3.1	O
.	O
In	O
Section	O
3.2	O
,	O
the	O
correspondence	O
between	O
numbers	O
and	O
their	O
related	O
entities	O
is	O
enhanced	O
with	O
a	O
simple	O
and	O
effective	O
constraint	O
on	O
number	O
-	O
related	O
modules	O
.	O
In	O
Section	O
3.3	O
,	O
we	O
strengthen	O
the	O
auxiliary	O
loss	B-MetricName
function	O
in	O
NMNs	O
to	O
further	O
concentrate	O
attention	O
in	O
the	O
same	O
sentence	O
.	O

The	O
interpreter	O
in	O
the	O
NMNs	O
framework	O
is	O
responsible	O
for	O
executing	O
specialised	O
modules	O
given	O
the	O
context	O
(	O
i.e.	O
paragraph	O
)	O
.	O
For	O
number	O
-	O
related	O
modules	O
such	O
as	O
"	O
find	O
-	O
num	O
"	O
,	O
the	O
question	O
is	O
not	O
taken	O
into	O
account	O
,	O
which	O
limits	O
NMNs	O
'	O
performance	O
on	O
numerical	O
reasoning	O
,	O
as	O
information	O
in	O
the	O
question	O
is	O
not	O
taken	O
into	O
account	O
.	O
As	O
an	O
example	O
,	O
let	O
us	O
take	O
a	O
clear	O
look	O
at	O
the	O
"	O
find	O
-	O
num	O
"	O
module	O
in	O
NMNs	O
.	O
find	O
-	O
num	O
(	O
P	O
)	O
T	O
2	O
.	O
This	O
module	O
takes	O
as	O
input	O
the	O
distribution	O
over	O
paragraph	O
tokens	O
,	O
and	O
produces	O
output	O
an	O
distribution	O
over	O
the	O
numbers	O
:	O
S	O
n	O
ij	O
=	O
P	O
i	O
T	O
W	O
n	O
P	O
n	O
j	O
,	O
(	O
1	O
)	O
A	O
n	O
i	O
=	O
softmax	B-MethodName
(	O
S	O
n	O
i	O
)	O
,	O
(	O
2	O
)	O
T	O
=	O
i	O
P	O
i	O
A	O
n	O
i	O
,	O
(	O
3	O
)	O
where	O
input	O
P	O
and	O
output	O
T	O
are	O
distributions	O
over	O
paragraph	O
tokens	O
and	O
numbers	O
respectively	O
,	O
P	O
is	O
the	O
paragraph	O
token	O
representations	O
,	O
i	O
is	O
the	O
index	O
of	O
the	O
i	O
th	O
paragraph	O
token	O
,	O
n	O
j	O
is	O
the	O
index	O
of	O
the	O
j	O
th	O
number	O
token	O
,	O
and	O
W	O
n	O
is	O
a	O
learnable	O
matrix	O
.	O
Note	O
that	O
when	O
computing	O
the	O
similarity	O
matrix	O
between	O
the	O
paragraph	O
token	O
P	O
i	O
and	O
the	O
number	O
token	O
P	O
n	O
j	O
in	O
Equation	O
1	O
,	O
there	O
is	O
no	O
interaction	O
with	O
the	O
question	O
.	O
When	O
the	O
correct	O
number	O
types	O
or	O
related	O
entities	O
can	O
be	O
easily	O
found	O
in	O
the	O
question	O
,	O
incorporating	O
the	O
question	O
in	O
"	O
find	O
-	O
num	O
"	O
can	O
help	O
narrow	O
down	O
the	O
search	O
of	O
numbers	O
in	O
the	O
paragraph	O
.	O
The	O
first	O
example	O
in	O
Figure	O
1	O
shows	O
that	O
the	O
NMNs	O
fails	O
to	O
locate	O
the	O
correct	O
number	O
as	O
the	O
wrong	O
event	O
is	O
recognized	O
,	O
without	O
interacting	O
with	O
the	O
question	O
.	O
Inspired	B-DatasetName
by	O
this	O
idea	O
,	O
we	O
propose	O
the	O
question	O
-	O
toparagraph	O
alignment	O
modification	O
to	O
number	O
-	O
related	O
modules	O
.	O
Specifically	O
,	O
the	O
definition	O
of	O
"	O
find	O
-	O
num	O
"	O
is	O
modified	O
as	O
follows	O
:	O
find	O
-	O
num	O
(	O
P	O
,	O
Q	O
)	O
T	O
n	O
,	O
where	O
the	O
additional	O
input	O
Q	O
obtained	O
from	O
the	O
programmer	O
represents	O
the	O
distribution	O
over	O
question	O
tokens	O
,	O
and	O
the	O
new	O
output	O
is	O
represented	O
by	O
T	O
n	O
.	O
Additional	O
computational	O
steps	O
(	O
Equation	O
4	O
to	O
7	O
below	O
)	O
are	O
added	O
after	O
Equation	O
3	O
:	O
S	O
n	O
kj	O
=	O
Q	O
k	O
T	O
W	O
n	O
P	O
n	O
j	O
,	O
(	O
4	O
)	O
A	O
n	O
k	B-HyperparameterName
=	I-HyperparameterName
softmax	B-MethodName
(	O
S	O
n	O
k	O
)	O
,	O
(	O
5	O
)	O
T	O
=	O
k	O
Q	O
k	O
A	O
n	O
k	O
,	O
(	O
6	O
)	O
T	O
n	O
=	O
λ	O
T	O
+	O
(	O
1−λ	O
)	O
T	O
,	O
(	O
7	O
)	O
where	O
Q	O
is	O
the	O
question	O
token	O
representations	O
and	O
k	O
is	O
the	O
index	O
of	O
the	O
k	O
th	O
question	O
token	O
.	O
As	O
can	O
be	O
seen	O
from	O
the	O
above	O
equations	O
,	O
the	O
input	O
of	O
the	O
improved	O
"	O
find	O
-	O
num	O
"	O
module	O
is	O
extended	O
to	O
include	O
not	O
only	O
paragraph	O
but	O
also	O
question	O
token	O
distributions	O
instead	O
of	O
only	O
the	O
paragraph	O
.	O
More	O
precisely	O
,	O
T	O
is	O
another	O
alignment	O
matrix	O
between	O
all	O
question	O
tokens	O
and	O
number	O
tokens	O
,	O
using	O
the	O
same	O
form	O
of	O
Bi	O
-	O
linear	O
attention	O
computation	O
as	O
T	O
.	O
Finally	O
,	O
the	O
new	O
distribution	O
T	O
n	O
is	O
produced	O
by	O
the	O
weighted	O
sum	O
of	O
T	O
and	O
T	O
with	O
an	O
additional	O
hyperparameters	O
λ	O
.	O
Here	O
we	O
fix	O
λ=0.5	O
so	O
that	O
NMNs	O
treats	O
the	O
paragraph	O
and	O
the	O
question	O
equally	O
.	O
Other	O
numberrelated	O
modules	O
are	O
also	O
revised	O
in	O
a	O
similar	O
way	O
,	O
e.g.	O
"	O
find	O
-	O
date	O
"	O
,	O
"	O
compare	O
-	O
num	O
-	O
lt	O
-	O
than	O
"	O
,	O
"	O
find	O
-	O
max	O
-	O
num	O
"	O
.	O

Gupta	O
et	O
al	O
(	O
2020	O
)	O
employed	O
an	O
auxiliary	O
loss	B-MetricName
to	O
constrain	O
the	O
relative	O
positioning	O
of	O
output	O
tokens	O
with	O
respect	O
to	O
input	O
tokens	O
in	O
the	O
"	O
find	O
-	O
num	O
"	O
,	O
"	O
find	O
-	O
date	O
"	O
and	O
"	O
relocate	O
"	O
modules	O
.	O
For	O
instance	O
,	O
the	O
auxiliary	O
loss	B-MetricName
for	O
the	O
"	O
find	O
-	O
num	O
"	O
module	O
is	O
as	O
follows	O
:	O
H	O
n	O
loss	B-MetricName
=	O
−	O
m	O
i=1	O
log	O
(	O
Nt	O
j=0	O
1	O
n	O
j	O
[	O
i±W	O
]	O
A	O
n	O
ij	O
)	O
,	O
(	O
10	O
)	O
where	O
A	O
n	O
ij	O
is	O
from	O
Equation	O
2	O
.	O
The	O
loss	B-MetricName
enables	O
the	O
model	O
to	O
concentrate	O
the	O
attention	O
mass	O
of	O
output	O
tokens	O
within	O
a	O
window	O
of	O
size	O
W	O
(	O
e.g.	O
W	O
=	O
10	O
)	O
.	O
However	O
,	O
these	O
loss	B-MetricName
functions	O
still	O
allow	O
irrelevant	O
numbers	O
to	O
have	O
spuriously	O
high	O
attention	O
values	O
.	O
Taking	O
the	O
second	O
line	O
in	O
Figure	O
1	O
as	O
an	O
example	O
,	O
based	O
on	O
the	O
loss	B-MetricName
computation	O
procedures	O
,	O
the	O
number	O
"	O
December	O
1997	O
"	O
will	O
be	O
also	O
"	O
found	O
"	O
and	O
connected	O
to	O
the	O
entity	O
"	O
PUK	O
and	O
KDP	O
"	O
in	O
NMNs	O
.	O
Obviously	O
,	O
this	O
irrelevant	O
year	O
information	O
should	O
not	O
be	O
taken	O
into	O
consideration	O
.	O
Therefore	O
,	O
we	O
propose	O
to	O
strengthen	O
the	O
auxiliary	O
loss	B-MetricName
to	O
further	O
concentrate	O
attention	O
mass	O
to	O
those	O
tokens	O
within	O
the	O
same	O
sentence	O
:	O
H	O
n	O
loss	B-MetricName
=	O
−	O
m	O
i=1	O
log	O
(	O
Nt	O
j=0	O
1	O
(	O
n	O
j	O
st	O
)	O
(	O
i	O
st	O
)	O
A	O
n	O
ij	O
)	O
,	O
(	O
11	O
)	O
where	O
the	O
s	O
t	O
is	O
the	O
token	O
index	O
set	O
for	O
the	O
t	O
th	O
sentence	O
in	O
the	O
paragraph	O
.	O
In	O
this	O
way	O
,	O
the	O
year	O
"	O
2003	O
"	O
is	O
the	O
only	O
consideration	O
for	O
the	O
previous	O
example	O
.	O

We	O
evaluate	O
model	O
performance	O
on	O
the	O
same	O
subset	O
of	O
the	O
DROP	B-DatasetName
dataset	O
used	O
by	O
the	O
original	O
NMNs	O
(	O
Gupta	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
contains	O
approx	O
.	O
19	O
,	O
500	O
QA	O
pairs	O
for	O
training	O
,	O
440	O
for	O
validation	O
and	O
1	O
,	O
700	O
for	O
testing	O
.	O
The	O
training	O
procedures	O
and	O
hyper	O
-	O
parameter	O
settings	O
are	O
the	O
same	O
as	O
the	O
original	O
NMNs	O
(	O
Gupta	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
report	O
F1	B-MetricName
and	O
Exact	B-MetricName
Match	I-MetricName
(	O
EM	B-MetricName
)	O
scores	O
following	O
the	O
literature	O
(	O
Dua	O
et	O
al	O
,	O
2019	O
;	O
Gupta	O
et	O
al	O
,	O
2020	O
)	O
.	O

Table	O
1	O
shows	O
the	O
main	O
results	O
,	O
where	O
"	O
original	O
"	O
represents	O
the	O
performance	O
of	O
the	O
original	O
NMNs	O
(	O
Gupta	O
et	O
al	O
,	O
2020	O
)	O
.	O
Row	O
4	O
,	O
"	O
+	O
qai+nepc+aux	O
"	O
,	O
is	O
our	O
full	O
model	O
,	O
which	O
includes	O
the	O
question	O
-	O
aware	O
interpreter	O
(	O
+	O
qai	O
)	O
,	O
the	O
number	O
-	O
entity	O
positional	O
constraint	O
(	O
+	O
nepc	O
)	O
,	O
and	O
the	O
improved	O
auxiliary	O
loss	B-MetricName
(	O
+	O
aux	O
)	O
.	O
It	O
can	O
be	O
observed	O
that	O
compared	O
to	O
"	O
original	O
"	O
,	O
our	O
full	O
model	O
achieves	O
significantly	O
higher	O
performance	O
with	O
F1	B-MetricName
of	O
80.4	O
and	O
EM	B-MetricName
of	O
76.6	O
,	O
representing	O
an	O
increase	O
of	O
3.0	O
and	O
2.6	O
absolute	O
points	O
respectively	O
.	O
Besides	O
,	O
our	O
significant	O
test	O
shows	O
p≤0.01	O
.	O

original	O
(	O
Gupta	O
et	O
al	O
,	O
2020	O
)	O
We	O
also	O
conduct	O
an	O
ablation	O
study	O
to	O
discuss	O
the	O
contribution	O
of	O
individual	O
technique	O
.	O
The	O
second	O
line	O
,	O
"	O
+	O
qai	O
"	O
,	O
is	O
the	O
results	O
with	O
the	O
question	O
-	O
aware	O
interpreter	O
employed	O
only	O
.	O
For	O
this	O
variant	O
,	O
the	O
F1	B-MetricName
and	O
EM	B-MetricName
scores	O
improve	O
on	O
the	O
original	O
baseline	O
by	O
1.6	O
and	O
0.9	O
points	O
respectively	O
.	O
With	O
the	O
addition	O
of	O
the	O
number	O
-	O
entity	O
positional	O
constraint	O
,	O
"	O
+	O
nepc	O
"	O
,	O
results	O
show	O
an	O
improvement	O
of	O
2.5	O
and	O
2.0	O
points	O
for	O
F1	B-MetricName
and	O
EM	B-MetricName
when	O
comparing	O
with	O
"	O
original	O
"	O
.	O
These	O
results	O
show	O
that	O
all	O
of	O
the	O
three	O
techniques	O
are	O
effective	O
in	O
improving	O
numerical	O
reasoning	O
skills	O
for	O
NMNs	O
.	O
We	O
also	O
report	O
performance	O
by	O
subsets	O
of	O
different	O
question	O
types	O
in	O
Table	O
2	O
.	O
Except	O
for	O
the	O
numbercompare	O
type	O
,	O
our	O
model	O
improves	O
on	O
the	O
original	O
NMNs	O
across	O
all	O
other	O
types	O
of	O
questions	O
significantly	O
,	O
by	O
at	O
least	O
3.2	O
absolute	O
points	O
for	O
F1	B-MetricName
.	O
In	O
addition	O
,	O
our	O
model	O
outperforms	O
aforementioned	O
MTMSN	O
(	O
Hu	O
et	O
al	O
,	O
2019	O
)	O

Neural	O
Moudule	O
Networks	O
(	O
NMNs	O
)	O
represent	O
an	O
interpretable	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approach	O
to	O
complex	O
question	B-TaskName
answering	I-TaskName
over	O
text	O
.	O
In	O
this	O
paper	O
,	O
we	O
further	O
improve	O
NMNs	O
'	O
numerical	O
reasoning	O
capabilities	O
,	O
by	O
making	O
the	O
interpreter	O
question	O
-	O
aware	O
and	O
placing	O
stronger	O
constraints	O
on	O
the	O
relative	O
positioning	O
of	O
entities	O
and	O
their	O
related	O
numbers	O
.	O
Experimental	O
results	O
show	O
that	O
our	O
approach	O
significantly	O
improves	O
NMNs	O
'	O
numerical	O
reasoning	O
ability	O
,	O
with	O
an	O
increase	O
in	O
F1	B-MetricName
of	O
3.0	O
absolute	O
points	O
.	O

Due	O
to	O
the	O
page	O
limitation	O
,	O
we	O
did	O
n't	O
include	O
more	O
baselines	O
,	O
such	O
as	O
NAQANet	O
(	O
Dua	O
et	O
al	O
,	O
2019	O
)	O
.	O
After	O
running	O
on	O
the	O
same	O
split	O
of	O
DROP	B-DatasetName
dataset	O
,	O
the	O
F1	B-MetricName
and	O
EM	B-MetricName
scores	O
by	O
NAQANet	O
are	O
62.1	O
%	O
and	O
57.9	O
%	O
respectively	O
,	O
which	O
are	O
substantially	O
lower	O
than	O
our	O
results	O
in	O
Table	O
1	O
,	O
by	O
over	O
17	O
%	O
for	O
both	O
scores	O
.	O
And	O
we	O
did	O
apply	O
these	O
components	O
in	O
Section	O
3	O
to	O
other	O
modules	O
,	O
such	O
as	O
the	O
"	O
extract	O
-	O
argument	O
"	O
module	O
(	O
extracts	O
spans	O
or	O
tokens	O
from	O
paragraphs	O
)	O
,	O
and	O
also	O
obtained	O
better	O
results	O
(	O
0.5	O
%	O
F1	B-MetricName
increase	O
)	O
.	O
Besides	O
,	O
for	O
different	O
question	O
types	O
,	O
their	O
statistics	O
on	O
the	O
test	O
set	O
can	O
be	O
found	O
in	O
Table	O
4	O
.	O
Current	O
NMNs	O
(	O
Gupta	O
et	O
al	O
,	O
2020	O
)	O
does	O
not	O
support	O
other	O
arithmetic	O
datasets	O
,	O
since	O
some	O
arithmetic	O
operations	O
,	O
including	O
addition	O
,	O
are	O
not	O
supported	O
.	O
Extending	O
related	O
arithmetic	O
modules	O
is	O
one	O
of	O
our	O
future	O
work	O
,	O
based	O
on	O
which	O
the	O
NMNs	O
could	O
be	O
trained	O
on	O
other	O
datsets	O
.	O

Widely	O
adopted	O
Transformer	B-MethodName
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
has	O
obviated	O
the	O
need	O
for	O
sequential	O
processing	O
of	O
the	O
input	O
that	O
is	O
enforced	O
in	O
traditional	O
Recurrent	O
Neural	O
Networks	O
(	O
RNN	O
)	O
.	O
As	O
a	O
result	O
,	O
compared	O
to	O
a	O
single	O
-	O
layered	O
LSTM	B-MethodName
or	O
RNN	O
model	O
,	O
a	O
single	O
-	O
layered	O
Transformer	B-MethodName
model	O
is	O
computationally	O
more	O
efficient	O
,	O
reflecting	O
in	O
a	O
relatively	O
shorter	O
training	O
time	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
This	O
advantage	O
encourages	O
the	O
training	O
of	O
deep	O
Transformer	B-MethodName
-	O
based	O
language	O
models	O
on	O
largescale	O
datasets	O
.	O
Their	O
learning	O
on	O
large	O
corpora	O
has	O
already	O
attained	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
SOTA	O
)	O
performances	O
in	O
many	O
downstream	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
tasks	O
.	O
A	O
large	O
number	O
of	O
SOTA	O
machine	O
learning	O
systems	O
even	O
beyond	O
NLP	O
(	O
Lu	O
et	O
al	O
,	O
2019	O
)	O
are	O
inspired	O
by	O
the	O
building	O
blocks	O
of	O
Transformer	B-MethodName
that	O
is	O
multi	O
-	O
head	O
self	O
-	O
attention	O
(	O
Radford	O
et	O
al	O
,	O
2018	O
;	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
.	O
A	O
model	O
employing	O
an	O
attention	O
-	O
based	O
mechanism	O
generates	O
a	O
probability	O
distribution	O
a	O
=	O
{	O
a	O
1	O
,	O
.	O
.	O
.	O
,	O
a	O
n	O
}	O
over	O
the	O
n	O
input	O
units	O
z	O
=	O
{	O
z	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
n	O
}	O
.	O
The	O
idea	O
is	O
to	O
perform	O
a	O
weighted	O
sum	O
of	O
inputs	O
,	O
denoted	O
by	O
n	O
i=1	O
a	O
i	O
z	O
i	O
,	O
to	O
produce	O
a	O
more	O
context	O
-	O
involved	O
output	O
.	O
The	O
attention	O
vector	O
,	O
a	O
,	O
are	O
commonly	O
interpreted	O
as	O
scores	O
signifying	O
the	O
relative	O
importance	O
of	O
input	O
units	O
.	O
However	O
,	O
counter	O
-	O
intuitively	O
,	O
it	O
is	O
recently	O
observed	O
that	O
the	O
weights	O
generated	O
in	O
the	O
model	O
do	O
not	O
provide	O
meaningful	O
explanations	O
(	O
Jain	O
and	O
Wallace	O
,	O
2019	O
;	O
Wiegreffe	O
and	O
Pinter	O
,	O
2019	O
)	O
.	O
Attention	O
weights	O
are	O
(	O
structurally	O
)	O
identifiable	O
if	O
we	O
can	O
uniquely	O
determine	O
them	O
from	O
the	O
output	O
of	O
the	O
attention	O
unit	O
(	O
Brunner	O
et	O
al	O
,	O
2019	O
)	O
.	O
Identifiability	O
of	O
the	O
attention	O
weights	O
is	O
critical	O
to	O
the	O
model	O
's	O
prediction	O
to	O
be	O
interpretable	O
and	O
replicable	O
.	O
If	O
the	O
weights	O
are	O
not	O
unique	O
,	O
explanatory	O
insights	O
from	O
them	O
might	O
be	O
misleading	O
.	O
The	O
self	O
-	O
attention	O
transforms	O
an	O
input	O
sequence	O
of	O
vectors	O
z	O
=	O
{	O
z	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
n	O
}	O
to	O
a	O
contextualized	O
output	O
sequence	O
y	O
=	O
{	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
n	O
}	O
,	O
where	O
y	O
k	B-HyperparameterName
=	I-HyperparameterName
n	O
i=1	O
a	O
(	O
k	O
,	O
i	O
)	O
z	O
i	O
.	O
The	O
scalar	O
a	O
(	O
k	O
,	O
i	O
)	O
captures	O
how	O
much	O
of	O
the	O
i	O
th	O
token	O
contributes	O
to	O
the	O
contextualization	O
of	O
k	O
th	O
token	O
.	O
A	O
Transformer	B-MethodName
layer	O
consists	O
of	O
multiple	O
heads	O
,	O
where	O
each	O
head	O
performs	O
selfattention	O
computations	O
,	O
we	O
break	O
the	O
head	O
computations	O
in	O
two	O
phases	O
:	O
Phase	O
1	O
:	O
Calculation	O
of	O
attention	O
weights	O
a	O
(	O
k	O
,	O
i	O
)	O
.	O
It	O
involves	O
mapping	O
input	O
tokens	O
to	O
key	O
and	O
query	O
vectors	O
.	O
The	O
dot	O
product	O
of	O
k	O
th	O
query	O
vector	O
and	O
i	O
th	O
key	O
vector	O
gives	O
a	O
(	O
k	O
,	O
i	O
)	O
.	O
Phase	O
2	O
:	O
Calculation	O
of	O
a	O
contextualized	O
representation	O
for	O
each	O
token	O
.	O
It	O
involves	O
mapping	O
input	O
tokens	O
to	O
the	O
value	O
vectors	O
.	O
The	O
contextualized	O
representation	O
for	O
k	O
th	O
token	O
can	O
be	O
computed	O
by	O
the	O
weighted	O
average	O
of	O
the	O
value	O
vectors	O
,	O
where	O
the	O
weight	O
of	O
i	O
th	O
token	O
is	O
a	O
(	O
k	O
,	O
i	O
)	O
computed	O
in	O
first	O
phase	O
.	O
The	O
identifiability	O
in	O
Transformer	B-MethodName
has	O
been	O
recently	O
studied	O
by	O
Brunner	O
et	O
al	O
(	O
2019	O
)	O
which	O
provides	O
theoretical	O
claims	O
that	O
under	O
mild	O
conditions	O
of	O
input	O
length	O
,	O
attention	O
weights	O
are	O
not	O
unique	O
to	O
the	O
head	O
's	O
output	O
.	O
Essentially	O
their	O
proof	O
was	O
dedicated	O
to	O
the	O
analysis	O
of	O
the	O
computations	O
in	O
the	O
second	O
phase	O
,	O
i.e.	O
,	O
token	O
contextualization	O
.	O
However	O
,	O
the	O
theoretical	O
analysis	O
ignored	O
the	O
crucial	O
first	O
phase	O
where	O
the	O
attention	O
weights	O
are	O
generated	O
.	O
Intrinsic	O
to	O
their	O
analysis	O
,	O
the	O
attention	O
identifiability	O
can	O
be	O
studied	O
by	O
studying	O
only	O
the	O
second	O
phase	O
of	O
head	O
computations	O
.	O
However	O
,	O
even	O
if	O
we	O
find	O
another	O
set	O
of	O
weights	O
from	O
the	O
second	O
phase	O
,	O
it	O
depends	O
on	O
the	O
first	O
phase	O
if	O
those	O
weights	O
can	O
be	O
generated	O
as	O
the	O
part	O
of	O
key	O
-	O
query	O
multiplication	O
.	O
In	O
this	O
work	O
,	O
we	O
probe	O
the	O
identifiability	O
of	O
attention	O
weights	O
in	O
Transformer	B-MethodName
from	O
a	O
perspective	O
that	O
was	O
ignored	O
in	O
Brunner	O
et	O
al	O
(	O
2019	O
)	O
.	O
We	O
explore	O
the	O
previously	O
overlooked	O
first	O
phase	O
of	O
selfattention	O
for	O
its	O
contribution	O
to	O
the	O
identifiability	O
in	O
Transformer	B-MethodName
.	O
During	O
our	O
analysis	O
of	O
the	O
first	O
phase	O
,	O
we	O
uncover	O
the	O
critical	O
constraint	O
imposed	O
by	O
the	O
size	O
of	O
the	O
key	O
vector	O
1	O
d	O
k	O
.	O
The	O
flow	O
of	O
analysis	O
can	O
be	O
described	O
as	O
We	O
first	O
show	O
that	O
the	O
attention	O
weights	O
are	O
identifiable	O
for	O
the	O
input	O
sequence	O
length	O
d	O
s	O
no	O
longer	O
than	O
the	O
size	O
of	O
value	O
vector	O
d	O
v	O
(	O
3.1	O
)	O
(	O
Brunner	O
et	O
al	O
,	O
2019	O
)	O
2	O
.	O
For	O
the	O
case	O
when	O
d	O
s	O
>	O
d	O
v	O
,	O
we	O
analyse	O
the	O
attention	O
weights	O
as	O
raw	O
dot	O
-	O
product	O
(	O
logits	O
)	O
and	O
the	O
softmaxed	O
dot	O
-	O
product	O
(	O
probability	O
simplex	O
)	O
,	O
independently	O
.	O
An	O
important	O
theoretical	O
finding	O
is	O
that	O
both	O
versions	O
are	O
prone	O
to	O
be	O
unidentifiable	O
.	O
In	O
the	O
case	O
of	O
attention	O
weights	O
as	O
logits	O
(	O
3.2.1	O
)	O
,	O
we	O
analytically	O
construct	O
another	O
set	O
of	O
attention	O
weights	O
to	O
claim	O
the	O
unidentifiability	O
.	O
In	O
the	O
case	O
of	O
attention	O
weights	O
as	O
1	O
The	O
size	O
of	O
key	O
and	O
query	O
vector	O
is	O
expected	O
to	O
be	O
the	O
same	O
due	O
to	O
the	O
subsequent	O
dot	O
product	O
operation	O
2	O
The	O
sequence	O
length	O
denotes	O
number	O
of	O
tokens	O
at	O
input	O
.	O
softmaxed	O
logits	O
(	O
3.2.2	O
)	O
,	O
we	O
find	O
the	O
attention	O
identifiability	O
to	O
be	O
highly	O
dependent	O
on	O
d	O
k	O
.	O
Thus	O
,	O
the	O
size	O
of	O
key	O
vector	O
plays	O
an	O
important	O
role	O
in	O
the	O
identifiability	O
of	O
the	O
self	O
-	O
attention	O
head	O
.	O
The	O
pieces	O
of	O
evidence	O
suggest	O
that	O
the	O
current	O
analysis	O
in	O
Brunner	O
et	O
al	O
(	O
2019	O
)	O
ignored	O
the	O
crucial	O
constraints	O
from	O
the	O
first	O
phase	O
in	O
their	O
analysis	O
.	O
To	O
resolve	O
the	O
unidentifiability	O
problem	O
,	O
we	O
propose	O
two	O
simple	O
solutions	O
(	O
4	O
)	O
.	O
For	O
the	O
regular	O
setting	O
of	O
the	O
Transformer	B-MethodName
encoder	O
where	O
d	O
v	O
depends	O
on	O
the	O
number	O
of	O
attention	O
heads	O
and	O
token	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
,	O
we	O
propose	O
to	O
reduce	O
d	O
k	O
.	O
This	O
may	O
lead	O
to	O
more	O
identifiable	O
attention	O
weights	O
.	O
Alternatively	O
,	O
as	O
a	O
more	O
concrete	O
solution	O
,	O
we	O
propose	O
to	O
set	O
d	O
v	O
equal	O
to	O
token	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
while	O
adding	O
head	O
outputs	O
as	O
opposed	O
to	O
the	O
regular	O
approach	O
of	O
concatenation	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Embedding	B-HyperparameterName
dimension	I-HyperparameterName
can	O
be	O
tuned	O
according	O
to	O
the	O
sequence	O
length	O
up	O
to	O
which	O
identifiability	O
is	O
desired	O
.	O
We	O
evaluate	O
the	O
performance	O
of	O
the	O
proposed	O
variants	O
on	O
varied	O
text	B-TaskName
classification	I-TaskName
tasks	O
comprising	O
of	O
ten	O
datasets	O
(	O
5	O
)	O
.	O
In	O
this	O
paper	O
,	O
our	O
goal	O
is	O
to	O
provide	O
concrete	O
theoretical	O
analysis	O
,	O
experimental	O
observations	O
,	O
and	O
possible	O
simple	O
solutions	O
to	O
identifiability	O
of	O
attention	O
weights	O
in	O
Transformer	B-MethodName
.	O
The	O
idea	O
behind	O
identifiable	O
variants	O
of	O
the	O
Transformer	B-MethodName
is	O
-	O
the	O
harder	O
it	O
is	O
to	O
obtain	O
alternative	O
attention	O
weights	O
,	O
the	O
likelier	O
is	O
they	O
are	O
identifiable	O
,	O
which	O
is	O
a	O
desirable	O
property	O
of	O
the	O
architecture	O
.	O
Thus	O
,	O
our	O
contribution	O
are	O
as	O
follows	O
:	O
We	O
provide	O
a	O
concrete	O
theoretical	O
analysis	O
of	O
identifiability	O
of	O
attention	O
weights	O
which	O
was	O
missing	O
in	O
the	O
previous	O
work	O
by	O
Brunner	O
et	O
al	O
(	O
2019	O
)	O
.	O
We	O
provide	O
Transformer	B-MethodName
variants	O
that	O
are	O
identifiable	O
and	O
validate	O
them	O
empirically	O
by	O
analysing	O
the	O
numerical	O
rank	O
of	O
the	O
attention	O
matrix	O
generated	O
in	O
the	O
self	O
-	O
attention	O
head	O
of	O
the	O
Transformer	B-MethodName
encoder	O
.	O
The	O
variants	O
have	O
strong	O
mathematical	O
support	O
and	O
simple	O
to	O
adopt	O
in	O
the	O
standard	O
Transformer	B-MethodName
settings	O
.	O
We	O
provide	O
empirical	O
evaluations	O
on	O
varied	O
text	B-TaskName
classification	I-TaskName
tasks	O
that	O
show	O
higher	O
identifiability	O
does	O
not	O
compromise	O
with	O
the	O
task	O
's	O
performance	O
.	O
2	O
Background	O

We	O
base	O
our	O
analysis	O
on	O
the	O
building	O
block	O
of	O
Transformer	B-MethodName
,	O
i.e.	O
,	O
the	O
encoder	O
layer	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
layer	O
has	O
two	O
sub	O
-	O
layers	O
.	O
First	O
sublayer	O
performs	O
the	O
multi	O
-	O
head	O
self	O
-	O
attention	O
,	O
and	O
second	O
is	O
feed	O
-	O
forward	O
network	O
.	O
Given	O
a	O
sequence	O
of	O
tokens	O
{	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
ds	O
}	O
,	O
an	O
embedding	O
layer	O
transforms	O
it	O
to	O
a	O
set	O
of	O
vector	O
{	O
z	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
ds	O
}	O
R	O
de	O
,	O
where	O
d	O
e	O
denotes	O
token	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
.	O
To	O
this	O
set	O
,	O
we	O
add	O
vectors	O
encoding	O
positional	O
information	O
of	O
tokens	O
{	O
p	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
ds	O
}	O
R	O
de	O
.	O
Multi	B-MethodName
-	I-MethodName
head	I-MethodName
Attention	I-MethodName
.	O
Input	O
to	O
a	O
head	O
of	O
multihead	O
self	O
-	O
attention	O
module	O
is	O
W	O
R	O
ds×de	O
,	O
i.e.	O
,	O
a	O
sequence	O
of	O
d	O
s	O
tokens	O
lying	O
in	O
a	O
d	O
e	O
-	O
dimensional	O
embedding	O
space	O
.	O
Tokens	O
are	O
projected	O
to	O
d	O
q	O
-	O
size	O
query	O
,	O
d	O
k	O
-	O
size	O
key	O
,	O
and	O
d	O
v	O
-	O
size	O
value	O
vectors	O
using	O
linear	O
layers	O
,	O
resulting	O
in	O
the	O
respective	O
matrices	O
-	O
Query	O
Q	O
R	O
ds×dq	O
,	O
Key	O
K	O
R	O
ds×d	O
k	O
,	O
and	O
Value	O
V	O
R	O
ds×dv	O
.	O
The	O
attention	O
weights	O
A	O
R	O
ds×ds	O
can	O
be	O
computed	O
by	O
A	O
=	O
softmax	B-MethodName
Q	O
K	O
T	O
d	O
q	O
.	O
(	O
1	O
)	O
The	O
(	O
i	O
,	O
j	O
)	O
th	O
element	O
of	O
A	O
shows	O
how	O
much	O
of	O
i	O
th	O
token	O
is	O
influenced	O
by	O
j	O
th	O
token	O
.	O
The	O
output	O
of	O
a	O
head	O
H	O
R	O
ds×de	O
is	O
given	O
by	O
H	O
=	O
A	O
V	O
D	O
=	O
A	O
T	O
,	O
(	O
2	O
)	O
where	O
D	O
R	O
dv×de	O
is	O
a	O
linear	B-MethodName
layer	I-MethodName
and	O
the	O
matrix	O
T	O
R	O
ds×de	O
denotes	O
the	O
operation	O
V	O
D.	O
The	O
R	O
ds×de	O
output	O
of	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
can	O
be	O
expressed	O
as	O
a	O
summation	O
over	O
H	O
obtained	O
for	O
each	O
head	O
3	O
.	O
The	O
i	O
th	O
row	O
of	O
multi	O
-	O
head	O
output	O
matrix	O
corresponds	O
to	O
the	O
d	O
e	O
dimensional	O
contextualized	O
representation	O
of	O
i	O
th	O
input	O
token	O
.	O
In	O
the	O
original	O
work	O
,	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
,	O
the	O
multi	O
-	O
head	O
operation	O
is	O
described	O
as	O
the	O
concatenation	O
of	O
A	O
V	O
obtained	O
from	O
each	O
head	O
followed	O
by	O
a	O
linear	O
transformation	O
D	O
R	O
de×de	O
.	O
Both	O
the	O
explanations	O
are	O
associated	O
with	O
the	O
same	O
sequence	O
of	O
matrix	O
operations	O
as	O
shown	O
in	O
fig	O
.	O
1	O
.	O
In	O
regular	O
Transformer	B-MethodName
setting	O
,	O
a	O
token	O
vector	O
is	O
t	O
i	O
{	O
(	O
z	O
j	O
+	O
p	O
j	O
)	O
}	O
ds	O
i=1	O
is	O
d	O
e	O
=	O
512	O
dimensional	O
,	O
number	O
of	O
heads	O
h=8	O
,	O
size	O
of	O
d	O
k	B-HyperparameterName
=	I-HyperparameterName
d	O
q	O
=	O
d	O
v	O
=	O
d	O
e	O
/h=64	O
.	O
Feed	O
-	O
Forward	O
Network	O
.	O
This	O
sub	O
-	O
layer	O
performs	O
the	O
following	O
transformations	O
on	O
each	O
token	O
representation	O
at	O
the	O
output	O
of	O
a	O
head	O
:	O
y	O
1	O
=	O
Linear	O
1	O
(	O
Norm	O
(	O
t	O
i	O
+	O
head	O
output	O
for	O
t	O
i	O
)	O
)	O
y	O
2	O
=	O
Norm	O
(	O
t	O
i	O
+	O
ReLU	B-MethodName
(	O
Linear	O
2	O
(	O
y	O
1	O
)	O
)	O
)	O
Linear	O
1	O
and	O
Linear	O
2	O
are	O
linear	O
layers	O
with	O
2048	B-DatasetName
and	O
512	O
nodes	O
,	O
respectively	O
.	O
Norm	O
denotes	O
minibatch	O
layer	B-MethodName
normalization	I-MethodName
.	O

Since	O
the	O
logits	O
matrix	O
A	O
is	O
obtained	O
from	O
the	O
product	O
of	O
Q	O
and	O
K	O
T	O
,	O
we	O
can	O
assert	O
that	O
rank	O
(	O
A	O
)	O
≤	O
min	O
rank	O
(	O
Q	O
)	O
,	O
rank	O
(	O
K	O
T	O
)	O
≤	O
min	O
d	O
e	O
,	O
d	O
k	O
,	O
d	O
q	O
,	O
d	O
e	O
=	O
d	O
k	O
.	O
(	O
8	O
)	O
Therefore	O
,	O
the	O
rank	O
of	O
attention	O
matrix	O
producible	O
by	O
the	O
head	O
in	O
the	O
first	O
phase	O
of	O
self	O
-	O
attention	O
can	O
at	O
most	O
be	O
equal	O
to	O
the	O
size	O
of	O
key	O
vectors	O
d	O
k	O
.	O
On	O
this	O
basis	O
,	O
the	O
head	O
can	O
produce	O
only	O
those	O
A	O
+	O
Ã	O
satisfying	O
rank	O
(	O
A	O
+	O
Ã	O
)	O
≤	O
d	O
k	O
(	O
constraint	O
-	O
R2	O
)	O
Proposition	O
3.3	O
.	O
There	O
exists	O
a	O
non	O
-	O
trivialÃ	O
that	O
satisfy	O
(	O
A	O
+	O
Ã	O
)	O
T	O
=	O
A	O
T	O
and	O
constraint	O
-	O
R2	O
.	O
Hence	O
,	O
A	O
is	O
unidentifiable	O
.	O
Proof	O
.	O
Let	O
a	O
1	O
,	O
.	O
.	O
.	O
,	O
a	O
ds	O
andã	O
1	O
,	O
.	O
.	O
.	O
,	O
ã	O
ds	O
denote	O
rows	O
of	O
A	O
andÃ	O
,	O
respectively	O
.	O
Without	O
the	O
loss	B-MetricName
of	O
generality	O
,	O
let	O
a	O
1	O
,	O
.	O
.	O
.	O
,	O
a	O
d	O
k	O
be	O
linearly	O
independent	O
rows	O
.	O
For	O
all	O
j	O
>	O
d	O
k	O
,	O
a	O
j	O
can	O
be	O
represented	O
as	O
a	O
linear	O
combination	O
d	O
k	O
i=1	O
λ	O
j	O
i	O
a	O
i	O
,	O
where	O
λ	O
j	O
i	O
is	O
a	O
scalar	O
.	O
Next	O
,	O
we	O
independently	O
choose	O
first	O
k	O
rows	O
ofÃ	O
that	O
are	O
{	O
ã	O
1	O
,	O
.	O
.	O
.	O
,	O
ã	O
d	O
k	O
}	O
from	O
LN	O
(	O
T	O
)	O
.	O
From	O
the	O
same	O
set	O
of	O
coefficients	O
of	O
linear	O
combination	O
λ	O
j	O
i	O
for	O
i	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
k	O
}	O
and	O
j	O
{	O
d	O
k+1	O
,	O
.	O
.	O
.	O
,	O
d	O
s	O
}	O
,	O
we	O
can	O
construct	O
j	O
th	O
row	O
ofÃ	O
asã	O
j	O
=	O
d	O
k	O
i=1	O
λ	O
j	O
iã	O
i	O
.	O
Now	O
,	O
since	O
we	O
can	O
construct	O
the	O
j	O
th	O
row	O
of	O
(	O
A	O
+	O
Ã	O
)	O
from	O
the	O
linear	O
combination	O
of	O
its	O
first	O
d	O
k	O
rows	O
as	O
d	O
k	O
i=1	O
λ	O
j	O
i	O
(	O
a	O
i	O
+	O
ã	O
i	O
)	O
,	O
the	O
rank	O
of	O
(	O
A	O
+	O
Ã	O
)	O
is	O
not	O
more	O
than	O
d	O
k	O
.	O
For	O
a	O
set	O
of	O
vectors	O
lying	O
in	O
a	O
linear	O
space	O
,	O
a	O
vector	O
formed	O
by	O
their	O
linear	O
combination	O
should	O
also	O
lie	O
in	O
the	O
same	O
space	O
.	O
Thus	O
,	O
the	O
artificially	O
constructed	O
rows	O
of	O
A	O
belongs	O
to	O
LN	O
(	O
T	O
)	O
.	O
Therefore	O
,	O
there	O
exist	O
anÃ	O
that	O
establishes	O
the	O
proposition	O
which	O
claims	O
the	O
unidentifiability	O
of	O
A.	O

The	O
softmax	B-MethodName
over	O
attention	O
logits	O
generates	O
attention	O
weights	O
with	O
each	O
row	O
of	O
A	O
(	O
i.e.	O
,	O
a	O
i	O
's	O
)	O
is	O
constrained	O
to	O
be	O
a	O
probability	O
distribution	O
.	O
Hence	O
,	O
we	O
can	O
define	O
constraint	O
overÃ	O
as	O
(	O
A	O
+	O
Ã	O
)	O
≥	O
0	B-DatasetName
(	O
P1	O
)	O
A	O
T	O
=	O
0	B-DatasetName
(	O
P2	O
)	O
A	O
1	O
=	O
0	B-DatasetName
.	O
(	O
P3	B-DatasetName
)	O
P1	O
is	O
non	O
-	O
negativity	O
constraint	O
on	O
(	O
A	O
+	O
Ã	O
)	O
as	O
it	O
is	O
supposed	O
to	O
be	O
the	O
output	O
of	O
softmax	B-MethodName
;	O
P2	O
de	O
-	O
notesÃ	O
LN	O
(	O
T	O
)	O
;	O
P3	B-DatasetName
can	O
be	O
derived	O
from	O
the	O
fact	O
(	O
A	O
+	O
Ã	O
)	O
1	O
=	O
1	O
=	O
⇒	O
(	O
A	O
1	O
+	O
Ã	O
1	O
)	O
=	O
1	O
=	O
⇒Ã	O
1	O
=	O
0	B-DatasetName
as	O
(	O
A	O
1	O
=	O
1	O
)	O
.	O
Where	O
1	O
R	O
ds	O
is	O
the	O
vector	O
of	O
ones	O
.	O
The	O
constraint	O
in	O
P2	O
and	O
P3	B-DatasetName
can	O
be	O
combined	O
and	O
reformulated	O
asÃ	O
[	O
T	O
,	O
1	O
]	O
=	O
0	B-DatasetName
.	O
Following	O
the	O
similar	O
analysis	O
as	O
in	O
eq	O
.	O
(	O
7	O
)	O
,	O
we	O
can	O
obtain	O
dim	O
LN	O
(	O
[	O
T	O
,	O
1	O
]	O
)	O
=	O
max	O
d	O
s	O
−	O
(	O
d	O
v	O
+	O
1	O
)	O
,	O
0	B-DatasetName
.	O
Disregarding	O
the	O
extreme	O
cases	O
when	O
a	O
i	O
is	O
a	O
one	O
-	O
hot	O
distribution	O
,	O
Brunner	O
et	O
al	O
(	O
2019	O
)	O
proved	O
the	O
existence	O
and	O
construction	O
of	O
non	O
-	O
trivialÃ	O
's	O
satisfying	O
all	O
the	O
constraints	O
P1	O
,	O
P2	O
,	O
and	O
P3	B-DatasetName
.	O
5	O
However	O
,	O
the	O
proof	O
by	O
Brunner	O
et	O
al	O
(	O
2019	O
)	O
missed	O
the	O
constraint	O
-	O
R2	O
,	O
hence	O
the	O
existence	O
of	O
a	O
non	O
-	O
trivialÃ	O
satisfying	O
only	O
the	O
set	O
of	O
constraints	O
P1	O
,	O
P2	O
and	O
P3	B-DatasetName
may	O
not	O
be	O
a	O
valid	O
proposition	O
to	O
claim	O
attention	O
weights	O
unidentifiability	O
.	O
Essentially	O
,	O
the	O
work	O
largely	O
ignored	O
the	O
constraints	O
coming	O
from	O
the	O
rank	O
of	O
the	O
matrix	O
that	O
produces	O
A	O
after	O
softmax	B-MethodName
6	O
.	O
Let	O
A	O
l	O
denote	O
logits	O
Q	O
K	O
T	O
√	O
dq	O
and	O
softmax	B-MethodName
(	O
A	O
l	O
)	O
=	O
(	O
A	O
+	O
Ã	O
)	O
,	O
where	O
softmax	B-MethodName
is	O
operated	O
over	O
each	O
row	O
of	O
A	O
l	O
.	O
We	O
add	O
an	O
extra	O
constraint	O
on	O
A	O
l	O
rank	O
(	O
A	O
l	O
)	O
≤	O
d	O
k	O
.	O
(	O
P4	O
)	O
The	O
constraint	O
P4	O
confirms	O
if	O
there	O
exists	O
a	O
logit	O
matrix	O
A	O
l	O
that	O
can	O
generate	O
(	O
A	O
+	O
Ã	O
)	O
,	O
given	O
constraints	O
P1	O
,	O
P2	O
,	O
and	O
P3	B-DatasetName
are	O
satisfied	O
.	O
The	O
possibility	O
of	O
such	O
an	O
A	O
l	O
will	O
provide	O
sufficient	O
evidence	O
that	O
A	O
is	O
unidentifiable	O
.	O
Next	O
,	O
we	O
investigate	O
how	O
the	O
existence	O
ofÃ	O
is	O
impacted	O
by	O
the	O
size	O
of	O
key	O
vector	O
d	O
k	O
(	O
query	O
and	O
key	O
vector	O
sizes	O
are	O
the	O
same	O
,	O
i.e.	O
,	O
d	O
q	O
=	O
d	O
k	O
)	O
.	O
Let	O
(	O
A	O
+	O
Ã	O
)	O
(	O
i	O
,	O
k	O
)	O
denotes	O
(	O
i	O
,	O
k	O
)	O
th	O
element	O
of	O
the	O
matrix	O
.	O
We	O
can	O
retrieve	O
the	O
set	O
of	O
matrices	O
A	O
l	O
such	O
that	O
softmax	B-MethodName
(	O
A	O
l	O
)	O
=	O
A	O
+	O
Ã	O
,	O
where	O
for	O
some	O
arbitrary	O
c	O
i	O
R	O
;	O
log	O
denotes	O
natural	O
logarithm	O
.	O
As	O
shown	O
in	O
fig	O
.	O
3	O
,	O
the	O
column	O
vectors	O
of	O
A	O
l	O
can	O
be	O
written	O
as	O
c	O
+	O
â	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
+	O
â	O
ds	O
.	O
A	O
l	O
(	O
i	O
,	O
k	O
)	O
=	O
c	O
i	O
+	O
log	O
(	O
A	O
+	O
Ã	O
)	O
(	O
i	O
,	O
k	O
)	O
(	O
9	O
)	O
For	O
an	O
arbitrarily	O
pickedÃ	O
satisfying	O
constraint	O
P1	O
,	O
P2	O
,	O
and	O
P3	B-DatasetName
,	O
the	O
dimensions	O
of	O
affine	O
span	O
S	O
of	O
{	O
â	O
1	O
,	O
.	O
.	O
.	O
,	O
â	O
ds	O
}	O
could	O
be	O
as	O
high	O
as	O
d	O
s	O
−	O
1	O
(	O
fig	O
.	O
4	O
)	O
.	O
In	O
such	O
cases	O
,	O
the	O
best	O
one	O
could	O
do	O
is	O
to	O
choose	O
a	O
c	O
a	O
S	O
such	O
that	O
the	O
dimension	O
of	O
the	O
linear	O
span	O
of	O
{	O
â	O
1	O
−	O
c	O
a	O
,	O
.	O
.	O
.	O
,	O
â	O
ds	O
−	O
c	O
a	O
}	O
,	O
i.e.	O
,	O
rank	O
(	O
A	O
l	O
)	O
is	O
d	O
s	O
−	O
1	O
.	O
Hence	O
,	O
to	O
satisfy	O
P4	O
,	O
d	O
s	O
−	O
1	O
≤	O
d	O
k	B-HyperparameterName
=	I-HyperparameterName
⇒	O
d	O
s	O
≤	O
d	O
k	O
+	O
1	O
.	O
Thus	O
,	O
the	O
set	O
of	O
(	O
A	O
+	O
Ã	O
)	O
satisfying	O
constraint	O
P1	O
,	O
P2	O
and	O
P3	B-DatasetName
are	O
not	O
always	O
obtainable	O
from	O
attention	O
head	O
for	O
d	O
s	O
>	O
d	O
k	O
.	O
We	O
postulate	O
Although	O
it	O
is	O
easier	O
to	O
constructÃ	O
satisfying	O
constraints	O
P1	O
,	O
P2	O
and	O
P3	B-DatasetName
,	O
it	O
is	O
hard	O
to	O
constructÃ	O
satisfying	O
constraint	O
P4	O
over	O
the	O
rank	O
of	O
logit	O
matrix	O
A	O
l	O
.	O
Therefore	O
,	O
A	O
becomes	O
more	O
identifiable	O
as	O
the	O
size	O
of	O
key	O
vector	O
decreases	O
.	O
Figure	O
4	O
:	O
This	O
is	O
a	O
simplified	O
illustration	O
for	O
the	O
case	O
d	O
s	O
=	O
3	O
.	O
Affine	O
space	O
(	O
translated	O
linear	O
subspace	O
)	O
spanned	O
by	O
vectorsâ	O
1	O
,	O
â	O
2	O
andâ	O
3	O
.	O
c	O
a	O
can	O
be	O
any	O
arbitrary	O
vector	O
in	O
affine	O
space	O
.	O
By	O
putting	O
c	O
=	O
−c	O
a	O
,	O
we	O
can	O
obtain	O
a	O
linear	O
subspace	O
whose	O
rank	O
is	O
equal	O
to	O
rank	O
of	O
the	O
affine	O
subspace	O
.	O
Experimental	O
evidence	O
.	O
We	O
conduct	O
an	O
experiment	O
to	O
validate	O
the	O
minimum	O
possible	O
numerical	O
rank	O
of	O
A	O
l	O
by	O
constructingÃ.	O
ForÃ	O
to	O
be	O
obtainable	O
from	O
the	O
phase	O
1	O
,	O
the	O
minimum	O
possible	O
rank	O
of	O
A	O
l	O
should	O
not	O
be	O
higher	O
than	O
d	O
k	O
.	O
From	O
IMDB	B-DatasetName
dataset	O
(	O
5	O
)	O
,	O
we	O
randomly	O
sample	O
a	O
set	O
of	O
reviews	O
with	O
token	O
sequence	O
length	O
d	O
s	O
ranging	O
from	O
66	O
to	O
128	O
7	O
.	O
For	O
each	O
review	O
,	O
we	O
construct	O
1000Ã	O
's	O
satisfying	O
constraints	O
P1	O
,	O
P2	O
,	O
and	O
P3	B-DatasetName
-	O
First	O
,	O
we	O
train	O
a	O
Transformer	B-MethodName
encoder	O
-	O
based	O
IMDB	B-DatasetName
review	O
sentiment	O
classifier	O
(	O
6	O
)	O
.	O
We	O
obtain	O
an	O
orthonormal	O
basis	O
for	O
the	O
left	O
null	O
space	O
of	O
[	O
T	O
,	O
1	O
]	O
using	O
singular	O
value	O
decomposition	O
.	O
To	O
form	O
anÃ	O
,	O
we	O
generate	O
d	O
s	O
random	O
linear	O
combinations	O
of	O
the	O
basis	O
vectors	O
(	O
one	O
for	O
each	O
of	O
its	O
row	O
)	O
.	O
Each	O
set	O
of	O
linear	O
combination	O
coefficients	O
is	O
sampled	O
uniformly	O
from	O
[	O
−10	O
,	O
10	O
]	O
.	O
All	O
the	O
rows	O
are	O
then	O
scaled	O
to	O
satisfy	O
the	O
constraint	O
P1	O
as	O
mentioned	O
in	O
Brunner	O
et	O
al	O
(	O
2019	O
)	O
.	O
Using	O
eq	O
.	O
(	O
9	O
)	O
,	O
we	O
obtain	O
a	O
minimum	O
rank	O
matrix	O
A	O
l	O
's	O
by	O
putting	O
c	O
=	O
−â	O
1	O
.	O
Figure	O
5	O
depicts	O
the	O
obtained	O
numerical	O
rank	O
of	O
A	O
l	O
.	O
We	O
observed	O
all	O
the	O
obtained	O
A	O
l	O
from	O
(	O
A	O
+	O
Ã	O
)	O
(	O
using	O
eq	O
.	O
(	O
9	O
)	O
)	O
are	O
full	O
-	O
row	O
rank	O
matrices	O
.	O
However	O
,	O
from	O
the	O
first	O
phase	O
of	O
self	O
-	O
attention	O
,	O
the	O
maximum	O
obtainable	O
rank	O
of	O
A	O
l	O
is	O
d	O
k	B-HyperparameterName
=	I-HyperparameterName
64	O
.	O
Thus	O
,	O
the	O
experimentally	O
constructed	O
A	O
l	O
's	O
do	O
not	O
claim	O
unidentifiability	O
of	O
A	O
as	O
it	O
fails	O
to	O
satisfy	O
the	O
constraint	O
P4	O
,	O
while	O
for	O
Brunner	O
et	O
al	O
(	O
2019	O
)	O
,	O
it	O
falls	O
under	O
the	O
solution	O
set	O
to	O
prove	O
unidentifiability	O
as	O
it	O
meets	O
constraints	O
P1	O
,	O
P2	O
and	O
P3	B-DatasetName
.	O

Based	O
on	O
the	O
Identifiability	O
analysis	O
in	O
3	O
,	O
we	O
propose	O
basic	O
solutions	O
to	O
make	O
Transformer	B-MethodName
's	O
attention	O
weights	O
identifiable	O
.	O
Decoupling	O
d	O
k	O
.	O
Contrary	O
to	O
the	O
regular	O
Transformer	B-MethodName
setting	O
where	O
d	O
k	B-HyperparameterName
=	I-HyperparameterName
d	O
v	O
,	O
a	O
simple	O
approach	O
is	O
to	O
decrease	O
the	O
value	O
of	O
d	O
k	O
that	O
is	O
the	O
size	O
of	O
the	O
key	O
and	O
query	O
vector	O
.	O
It	O
will	O
reduce	O
the	O
possible	O
solutions	O
ofÃ	O
by	O
putting	O
harder	O
constraints	O
on	O
the	O
rank	O
of	O
attention	O
logits	O
,	O
i.e.	O
,	O
A	O
l	O
in	O
eq	O
.	O
(	O
9	O
)	O
.	O
However	O
,	O
theoretically	O
,	O
d	O
k	O
decides	O
the	O
upper	O
bound	O
on	O
dimensions	O
of	O
the	O
space	O
to	O
which	O
token	O
embeddings	O
are	O
projected	O
before	O
the	O
dot	O
product	O
.	O
Higher	O
the	O
upper	O
bound	O
,	O
more	O
degree	O
of	O
freedom	O
to	O
choose	O
the	O
subspace	O
dimensions	O
as	O
compared	O
to	O
the	O
lower	O
d	O
k	O
variants	O
.	O
Thus	O
,	O
there	O
is	O
a	O
plausible	O
trade	O
-	O
off	O
when	O
choosing	O
between	O
d	O
k	O
induced	O
identifiability	O
and	O
the	O
upper	O
bound	O
on	O
the	O
dimension	O
of	O
projected	O
space	O
.	O
Head	O
Addition	O
.	O
To	O
resolve	O
the	O
unidentifiability	O
issue	O
when	O
sequence	O
length	O
exceeds	O
the	O
size	O
of	O
value	O
vector	O
,	O
we	O
propose	O
to	O
keep	O
the	O
value	O
vector	O
size	O
and	O
token	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
to	O
be	O
more	O
than	O
(	O
or	O
equal	O
to	O
)	O
the	O
maximum	O
allowed	O
input	O
tokens	O
,	O
i.e.	O
,	O
d	O
v	O
≥	O
d	O
s	O
-	O
max	O
.	O
In	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
,	O
d	O
v	O
was	O
bound	O
to	O
be	O
equal	O
to	O
d	O
e	O
/h	O
,	O
where	O
d	O
e	O
is	O
token	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
and	O
h	O
is	O
number	O
of	O
heads	O
.	O
This	O
constraint	O
on	O
d	O
v	O
is	O
because	O
of	O
the	O
concatenation	O
of	O
h	O
self	O
-	O
attention	O
heads	O
to	O
produce	O
d	O
e	O
-	O
sized	O
output	O
at	O
the	O
first	O
sub	O
-	O
layer	O
of	O
the	O
encoder	O
.	O
Thus	O
,	O
to	O
decouple	O
d	O
v	O
from	O
this	O
constraint	O
,	O
we	O
keep	O
d	O
v	O
=	O
d	O
e	O
and	O
add	O
each	O
head	O
's	O
output	O
.	O
8	O

Setting	O
up	O
the	O
encoder	O
.	O
We	O
normalize	O
the	O
text	O
by	O
lower	O
casing	O
,	O
removing	O
special	O
characters	O
,	O
etc	O
.	O
9	O
For	O
each	O
task	O
,	O
we	O
construct	O
separate	O
1	O
-	O
Gram	O
vocabulary	O
(	O
U	O
)	O
and	O
initialize	O
a	O
trainable	O
randomly	O
sampled	O
token	O
embedding	O
(	O
U	O
×	O
d	O
e	O
)	O
from	O
N	O
(	O
0	B-DatasetName
,	O
1	O
)	O
.	O
Similarly	O
,	O
we	O
randomly	O
initialize	O
a	O
(	O
d	O
s	O
-	O
max	O
×	O
d	O
e	O
)	O
positional	O
embedding	O
.	O
The	O
encoder	O
(	O
2.2	O
)	O
takes	O
input	O
a	O
sequence	O
of	O
token	O
vectors	O
(	O
d	O
s	O
×	O
d	O
e	O
)	O
with	O
added	O
positional	O
vectors	O
.	O
The	O
input	O
is	O
then	O
projected	O
to	O
key	O
and	O
query	O
vector	O
of	O
size	O
d	O
k	O
{	O
1	O
,	O
2	O
,	O
4	O
,	O
8	O
,	O
16	O
,	O
32	O
,	O
64	O
,	O
128	O
,	O
256	O
}	O
.	O
For	O
the	O
regular	O
Transformer	B-MethodName
setting	O
,	O
we	O
fix	O
the	O
number	O
of	O
heads	O
h	O
to	O
8	O
and	O
the	O
size	O
of	O
value	O
vector	O
d	O
v	O
=	O
d	O
e	O
/h	O
that	O
is	O
64	O
.	O
For	O
each	O
token	O
at	O
the	O
input	O
,	O
the	O
outputs	O
of	O
attention	O
heads	O
are	O
concatenated	O
to	O
generate	O
a	O
d	O
e	O
-	O
sized	O
vector	O
.	O
For	O
the	O
identifiable	O
variant	O
of	O
the	O
Transformer	B-MethodName
encoder	O
,	O
d	O
v	O
=	O
d	O
e	O
=	O
512	O
,	O
this	O
is	O
equal	O
to	O
d	O
s	O
-	O
max	O
to	O
keep	O
it	O
identifiable	O
up	O
to	O
the	O
maximum	O
permissible	O
number	O
of	O
tokens	O
.	O
The	O
outputs	O
of	O
all	O
the	O
heads	O
are	O
then	O
added	O
.	O
Each	O
token	O
's	O
contextualized	O
representations	O
(	O
added	O
head	O
outputs	O
)	O
are	O
then	O
passed	O
through	O
the	O
feed	O
-	O
forward	O
network	O
(	O
2.2	O
)	O
.	O
For	O
classification	O
,	O
we	O
use	O
the	O
encoder	O
layer	O
's	O
output	O
for	O
the	O
first	O
token	O
and	O
pass	O
it	O
through	O
a	O
linear	O
classification	O
layer	O
.	O
In	O
datasets	O
with	O
more	O
than	O
two	O
classes	O
,	O
the	O
classifier	O
output	O
is	O
softmaxed	O
.	O
In	O
the	O
case	O
of	O
SNLI	B-DatasetName
,	O
we	O
use	O
the	O
shared	O
encoder	O
for	O
both	O
premise	O
and	O
hypothesis	O
;	O
the	O
output	O
of	O
their	O
first	O
tokens	O
is	O
then	O
concatenated	O
just	O
before	O
the	O
final	O
classification	O
layer	O
.	O
We	O
use	O
Adam	B-MethodName
optimizer	B-HyperparameterName
,	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
=	O
0.001	O
,	O
to	O
minimize	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
between	O
the	O
target	O
and	O
predicted	O
label	O
.	O
For	O
all	O
the	O
experiments	O
,	O
we	O
keep	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
as	O
256	O
and	O
train	O
for	O
20	O
epochs	O
.	O
We	O
report	O
the	O
test	O
accuracy	B-MetricName
obtained	O
at	O
the	O
epoch	O
with	O
the	O
best	O
validation	O
accuracy	B-MetricName
.	O
Numerical	O
rank	O
.	O
To	O
generate	O
the	O
numerical	O
rank	O
plot	O
on	O
IMDB	B-DatasetName
dataset	O
as	O
shown	O
in	O
fig	O
.	O
2	O
,	O
we	O
train	O
a	O
separate	O
Transformer	B-MethodName
encoder	O
-	O
based	O
classifier	O
.	O
For	O
a	O
particular	O
d	O
s	O
value	O
,	O
we	O
sample	O
100	O
reviews	O
from	O
the	O
dataset	O
with	O
token	O
length	O
≥	O
d	O
s	O
and	O
clip	O
each	O
review	O
to	O
the	O
maximum	O
length	O
d	O
s	O
.	O
The	O
clipping	O
will	O
ensure	O
the	O
number	O
of	O
tokens	O
is	O
d	O
s	O
before	O
feeding	O
it	O
to	O
the	O
encoder	O
.	O
The	O
numerical	O
rank	O
is	O
calculated	O
for	O
T	O
's	O
obtained	O
from	O
the	O
first	O
head	O
of	O
the	O
encoder	O
.	O

For	O
the	O
identifiable	O
variant	O
,	O
similar	O
to	O
3.1	O
,	O
we	O
plot	O
the	O
numerical	O
rank	O
of	O
T	O
with	O
input	O
sequence	O
length	O
as	O
shown	O
in	O
fig	O
.	O
6	O
.	O
Unlike	O
fig	O
.	O
2	O
,	O
where	O
dim	O
LN	O
(	O
T	O
)	O
linearly	O
increases	O
after	O
d	O
s	O
=	O
64	O
,	O
we	O
find	O
the	O
dimension	O
is	O
zero	O
for	O
a	O
larger	O
d	O
s	O
(	O
∼	O
380	O
)	O
.	O
The	O
zero	O
dimensional	O
(	O
left	O
)	O
null	O
space	O
of	O
T	O
con	O
-	O
firms	O
there	O
exist	O
no	O
nontrivial	O
solution	O
to	O
the	O
constraint	O
constraint	O
-	O
R2	O
,	O
i.e.	O
,	O
Ã	O
=	O
{	O
0	B-DatasetName
}	O
.	O
Thus	O
,	O
the	O
attention	O
weights	O
A	O
are	O
identifiable	O
for	O
a	O
larger	O
range	O
of	O
length	O
of	O
the	O
input	O
sequence	O
.	O
Figure	O
6	O
:	O
Scatter	O
plots	O
in	O
red	O
and	O
blue	O
show	O
rank	O
(	O
T	O
)	O
and	O
dim	O
LN	O
(	O
T	O
)	O
,	O
respectively	O
,	O
for	O
matrices	O
T	O
obtained	O
from	O
the	O
second	O
phase	O
of	O
attention	O
by	O
feeding	O
IMDB	B-DatasetName
samples	O
to	O
the	O
encoder	O
.	O
The	O
green	O
line	O
shows	O
the	O
desired	O
rank	O
(	O
T	O
)	O
for	O
which	O
dim	O
LN	O
(	O
T	O
)	O
=	O
0	B-DatasetName
and	O
thus	O
attention	O
weights	O
are	O
identifiable	O
.	O
It	O
is	O
important	O
that	O
the	O
identifiability	O
of	O
attention	O
weights	O
should	O
not	O
come	O
at	O
the	O
cost	O
of	O
reduced	O
performance	O
of	O
the	O
model	O
.	O
To	O
investigate	O
this	O
issue	O
,	O
we	O
compare	O
the	O
performance	O
of	O
the	O
identifiable	O
Transformer	B-MethodName
encoder	O
against	O
its	O
regular	O
settings	O
(	O
6	O
)	O
on	O
varied	O
text	B-TaskName
classification	I-TaskName
tasks	O
.	O
For	O
the	O
regular	O
setting	O
,	O
as	O
discussed	O
in	O
4	O
as	O
one	O
of	O
the	O
solutions	O
,	O
the	O
Transformer	B-MethodName
can	O
be	O
made	O
identifiable	O
by	O
decreasing	O
the	O
size	O
of	O
the	O
key	O
vector	O
d	O
k	O
.	O
The	O
rows	O
of	O
the	O
Table	O
1	O
corresponding	O
to	O
Con	O
denotes	O
regular	O
Transformer	B-MethodName
setting	O
with	O
varying	O
size	O
of	O
key	O
vector	O
.	O
We	O
observe	O
the	O
classification	O
accuracy	B-MetricName
at	O
the	O
lower	O
d	O
k	O
is	O
comparable	O
or	O
higher	O
than	O
large	O
d	O
k	O
values	O
,	O
thus	O
,	O
the	O
enhanced	O
identifiability	O
does	O
not	O
compromise	O
with	O
the	O
model	O
's	O
classification	O
accuracy	B-MetricName
.	O
However	O
,	O
we	O
notice	O
a	O
general	O
performance	O
decline	O
with	O
an	O
increase	O
in	O
the	O
size	O
of	O
the	O
key	O
vector	O
.	O
We	O
speculate	O
that	O
for	O
simple	O
classification	O
tasks	O
,	O
the	O
lower	O
-	O
dimensional	O
projection	O
for	O
key	O
and	O
query	O
vector	O
works	O
well	O
.	O
However	O
,	O
as	O
the	O
task	O
becomes	O
more	O
involved	O
,	O
a	O
higher	O
dimension	O
for	O
the	O
projected	O
subspace	O
could	O
be	O
essential	O
.	O
Nonetheless	O
,	O
as	O
we	O
do	O
not	O
have	O
strong	O
theoretical	O
findings	O
,	O
we	O
leave	O
this	O
observation	O
for	O
future	O
work	O
.	O
Another	O
solution	O
to	O
identifiability	O
is	O
to	O
increase	O
d	O
v	O
to	O
d	O
e	O
and	O
add	O
the	O
heads	O
'	O
outputs	O
.	O
This	O
setting	O
corresponds	O
to	O
the	O
Add	O
rows	O
in	O
the	O
Table	O
1	O
.	O
For	O
key	O
vector	O
size	O
d	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
,	O
2	O
,	O
and	O
4	O
,	O
We	O
find	O
the	O
identifiable	O
Transformer	B-MethodName
's	O
performance	O
is	O
comparable	O
to	O
the	O
regular	O
settings	O
.	O
For	O
d	O
k	O
≥	O
8	O
,	O
as	O
a	O
general	O
observation	O
,	O
we	O
find	O
the	O
performance	O
of	O
Add	O
does	O
not	O
drop	O
as	O
drastically	O
as	O
Con	O
with	O
an	O
increase	O
in	O
d	O
k	O
.	O
This	O
could	O
be	O
due	O
to	O
the	O
larger	O
size	O
of	O
value	O
vector	O
leading	O
to	O
the	O
more	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
in	O
Add	O
that	O
compensate	O
for	O
the	O
significant	O
reduction	O
in	O
the	O
model	O
's	O
accuracy	B-MetricName
.	O
On	O
the	O
large	O
-	O
scale	O
datasets	O
,	O
we	O
observe	O
that	O
Add	O
performs	O
slightly	O
better	O
than	O
Con	O
.	O
Intuitively	O
,	O
as	O
shown	O
in	O
fig	O
.	O
1	O
,	O
we	O
can	O
increase	O
the	O
size	O
of	O
value	O
vector	O
to	O
increase	O
the	O
dimension	O
of	O
the	O
space	O
on	O
which	O
each	O
token	O
is	O
projected	O
.	O
A	O
higher	O
dimensional	O
subspace	O
can	O
contain	O
more	O
semantic	O
information	O
to	O
perform	O
the	O
specific	O
task	O
.	O
Even	O
though	O
the	O
theoretical	O
analysis	O
shows	O
the	O
possibility	O
of	O
a	O
full	O
row	O
rank	O
of	O
T	O
and	O
identifiable	O
attention	O
weights	O
,	O
the	O
T	O
obtained	O
from	O
a	O
trained	O
model	O
might	O
not	O
contain	O
all	O
the	O
rows	O
linearly	O
independent	O
as	O
d	O
s	O
increases	O
.	O
We	O
can	O
explain	O
this	O
from	O
the	O
semantic	O
similarities	O
between	O
words	O
cooccurring	O
together	O
(	O
Harris	O
,	O
1954	O
)	O
.	O
The	O
similarity	O
is	O
captured	O
as	O
the	O
semantic	O
relationship	O
,	O
such	O
as	O
dot	O
product	O
,	O
between	O
vectors	O
in	O
a	O
linear	O
space	O
.	O
As	O
the	O
number	O
of	O
tokens	O
in	O
a	O
sentence	O
,	O
i.e.	O
,	O
d	O
s	O
increases	O
,	O
it	O
becomes	O
more	O
likely	O
to	O
obtain	O
a	O
token	O
vector	O
from	O
the	O
linear	O
combination	O
of	O
other	O
tokens	O
.	O

We	O
introduce	O
NLQuAD	O
,	O
the	O
first	O
data	O
set	O
with	O
baseline	O
methods	O
for	O
non	O
-	O
factoid	O
long	O
question	B-TaskName
answering	I-TaskName
,	O
a	O
task	O
requiring	O
documentlevel	O
language	O
understanding	O
.	O
In	O
contrast	O
to	O
existing	O
span	O
detection	O
question	B-TaskName
answering	I-TaskName
data	O
sets	O
,	O
NLQuAD	O
has	O
non	O
-	O
factoid	O
questions	O
that	O
are	O
not	O
answerable	O
by	O
a	O
short	O
span	O
of	O
text	O
and	O
demanding	O
multiple	O
-	O
sentence	O
descriptive	O
answers	O
and	O
opinions	O
.	O
We	O
show	O
the	O
limitation	O
of	O
the	O
F1	B-MetricName
score	I-MetricName
for	O
evaluation	O
of	O
long	O
answers	O
and	O
introduce	O
Intersection	O
over	O
Union	O
(	O
IoU	B-MetricName
)	O
,	O
which	O
measures	O
position	O
-	O
sensitive	O
overlap	O
between	O
the	O
predicted	O
and	O
the	O
target	O
answer	O
spans	O
.	O
To	O
establish	O
baseline	O
performances	O
,	O
we	O
compare	O
BERT	B-MethodName
,	O
RoBERTa	B-MethodName
,	O
and	O
Longformer	B-MethodName
models	O
.	O
Experimental	O
results	O
and	O
human	O
evaluations	O
show	O
that	O
Longformer	B-MethodName
outperforms	O
the	O
other	O
architectures	O
,	O
but	O
results	O
are	O
still	O
far	O
behind	O
a	O
human	O
upper	O
bound	O
,	O
leaving	O
substantial	O
room	O
for	O
improvements	O
.	O
NLQuAD	O
's	O
samples	O
exceed	O
the	O
input	O
limitation	O
of	O
most	O
pretrained	O
Transformer	B-MethodName
-	O
based	O
models	O
,	O
encouraging	O
future	O
research	O
on	O
long	O
sequence	O
language	O
models	O
.	O
1	O

Over	O
the	O
last	O
few	O
years	O
,	O
there	O
have	O
been	O
remarkable	O
improvements	O
in	O
the	O
area	O
of	O
Machine	B-TaskName
Reading	I-TaskName
Comprehension	I-TaskName
(	O
MRC	O
)	O
and	O
open	B-TaskName
-	I-TaskName
domain	I-TaskName
Question	I-TaskName
Answering	I-TaskName
(	O
QA	O
)	O
due	O
to	O
the	O
availability	O
of	O
large	O
scale	O
data	O
sets	O
such	O
as	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
and	O
pre	O
-	O
trained	O
language	O
models	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
.	O
Although	O
non	O
-	O
factoid	O
questions	O
represent	O
a	O
large	O
number	O
of	O
real	O
-	O
life	O
questions	O
,	O
current	O
QA	O
data	O
sets	O
barely	O
cover	O
this	O
area	O
.	O
The	O
reason	O
is	O
that	O
context	O
passages	O
in	O
existing	O
QA	O
data	O
sets	O
are	O
mostly	O
very	O
short	O
and	O
questions	O
mostly	O
factoid	O
,	O
i.e.	O
,	O
can	O
be	O
answered	O
by	O
simple	O
facts	O
or	O
entities	O
such	O
as	O
a	O
person	O
name	O
and	O
location	O
(	O
Jurafsky	O
and	O
Martin	O
,	O
2019	O
)	O
.	O
Little	O
attention	O
has	O
been	O
1	O
Dataset	O
and	O
Models	O
:	O
github.com/asoleimanib/NLQuAD	O
Question	O
:	O
How	O
are	O
people	O
coping	O
in	O
the	O
lockdown	O
?	O
Headline	O
:	O
China	O
coronavirus	O
:	O
Death	O
toll	O
rises	O
as	O
more	O
cities	O
restrict	O
travel	O
Document	O
:	O
China	O
has	O
widened	O
its	O
travel	O
restrictions	O
in	O
Hubei	O
province	O
-	O
the	O
centre	O
of	O
the	O
coronavirus	O
outbreak	O
-	O
as	O
the	O
death	O
toll	O
climbed	O
to	O
26	O
.	O
The	O
restrictions	O
will	O
affect	O
at	O
least	O
20	O
million	O
people	O
across	O
10	O
cities	O
,	O
including	O
the	O
capital	O
,	O
Wuhan	O
,	O
where	O
the	O
virus	O
emerged	O
.	O
On	O
Thursday	O
,	O
a	O
coronavirus	O
patient	O
died	O
in	O
northern	O
Hebei	O
province	O
-	O
making	O
it	O
the	O
first	O
death	O
outside	O
Hubei	O
.	O
[	O
...	O
]	O
We	O
now	O
know	O
this	O
is	O
not	O
a	O
virus	O
that	O
will	O
burn	O
out	O
on	O
its	O
own	O
and	O
disappear	O
.	O
[	O
...	O
]	O
And	O
we	O
still	O
do	O
n't	O
know	O
when	O
people	O
are	O
contagious	O
.	O
Is	O
it	O
before	O
symptoms	O
appear	O
,	O
or	O
only	O
after	O
severe	O
symptoms	O
emerge	O
?	O
One	O
is	O
significantly	O
harder	O
to	O
stop	O
spreading	O
than	O
the	O
other	O
.	O
[	O
...	O
]	O
One	O
doctor	O
,	O
who	O
requested	O
anonymity	O
,	O
describes	O
the	O
conditions	O
at	O
a	O
hospital	O
in	O
Wuhan	O
.	O
[	O
...	O
]	O
"	O
I	O
was	O
planning	O
to	O
stay	O
in	O
my	O
apartment	O
because	O
I	O
'm	O
scared	O
to	O
go	O
to	O
the	O
gym	O
,	O
and	O
I	O
'm	O
scared	O
to	O
go	O
to	O
out	O
in	O
public	O
,	O
and	O
not	O
many	O
people	O
are	O
willing	O
to	O
go	O
out	O
.	O
"	O
(	O
141	O
words	O
)	O
.	O
Vietnam	O
and	O
Singapore	O
were	O
on	O
Thursday	O
added	O
to	O
the	O
nations	O
recording	O
confirmed	O
cases	O
,	O
joining	O
Thailand	O
,	O
the	O
US	O
,	O
Taiwan	O
and	O
South	O
Korea	O
.	O
[	O
...	O
]	O
Taiwan	O
has	O
banned	O
people	O
arriving	O
from	O
Wuhan	O
and	O
the	O
US	O
state	O
department	O
warned	O
American	O
travellers	O
to	O
exercise	O
increased	O
caution	O
in	O
China	O
.	O
(	O
document	O
length	O
:	O
921	O
words	O
)	O
Figure	O
1	O
:	O
A	O
question	O
-	O
answer	O
pair	O
in	O
NLQuAD	O
.	O
QA	O
models	O
must	O
predict	O
the	O
answer	O
span	O
within	O
the	O
context	O
document	O
.	O
The	O
correct	O
answer	O
span	O
is	O
bolded	O
.	O
We	O
extract	O
questions	O
and	O
answers	O
,	O
respectively	O
,	O
from	O
the	O
subheadings	O
and	O
the	O
sub	O
-	O
section	O
bodies	O
from	O
real	O
-	O
word	O
English	O
news	O
articles	O
.	O
Two	O
other	O
questions	O
based	O
on	O
the	O
same	O
article	O
:	O
Can	O
the	O
Coronavirus	O
be	O
stopped	O
?	O
What	O
's	O
the	O
global	O
situation	O
?	O
paid	O
to	O
non	O
-	O
factoid	O
and	O
open	O
-	O
ended	O
questions	O
that	O
require	O
complex	O
answers	O
such	O
as	O
descriptions	O
or	O
opinions	O
(	O
Hashemi	O
et	O
al	O
,	O
2020	O
)	O
.	O
Answers	O
to	O
nonfactoid	O
questions	O
extend	O
to	O
multiple	O
sentences	O
or	O
paragraphs	O
having	O
few	O
words	O
overlapping	O
with	O
the	O
question	O
(	O
Cohen	O
and	O
Croft	O
,	O
2016	O
)	O
.	O
Non	O
-	O
factoid	O
QA	O
facilitates	O
document	O
assistance	O
systems	O
,	O
where	O
for	O
example	O
,	O
journalists	O
can	O
seek	O
assistance	O
to	O
highlight	O
relevant	O
opinions	O
and	O
interpretations	O
.	O
It	O
can	O
further	O
motivate	O
more	O
research	O
on	O
long	O
sequence	O
language	O
models	O
.	O
Therefore	O
,	O
a	O
high	O
-	O
quality	O
data	O
set	O
in	O
this	O
area	O
is	O
clearly	O
desired	O
.	O
To	O
support	O
research	O
towards	O
non	O
-	O
factoid	O
and	O
long	O
QA	O
tasks	O
and	O
to	O
address	O
the	O
existing	O
shortcomings	O
as	O
identified	O
above	O
,	O
we	O
have	O
built	O
NLQuAD	O
,	O
a	O
non	O
-	O
factoid	O
long	O
question	O
answering	O
data	B-TaskName
set	I-TaskName
.	O
NLQuAD	O
contains	O
31k	O
non	O
-	O
factoid	O
questions	O
and	O
long	O
answers	O
collected	O
from	O
13k	O
BBC	O
news	O
articles	O
.	O
We	O
extract	O
questions	O
and	O
answers	O
from	O
the	O
articles	O
'	O
sub	O
-	O
headings	O
and	O
the	O
following	O
body	O
paragraphs	O
of	O
the	O
sub	O
-	O
headings	O
(	O
see	O
Figure	O
1	O
)	O
.	O
Questions	O
in	O
NLQuAD	O
are	O
not	O
answerable	O
by	O
a	O
short	O
span	O
of	O
text	O
within	O
the	O
documents	O
.	O
This	O
is	O
in	O
contrast	O
to	O
existing	O
long	O
-	O
context	O
but	O
factoid	O
QA	O
data	O
sets	O
such	O
as	O
NewsQA	O
(	O
Trischler	B-DatasetName
et	O
al	O
,	O
2017	O
)	O
,	O
TriviaQA	O
(	O
Joshi	B-DatasetName
et	O
al	O
,	O
2017	O
)	O
,	O
NarrativeQA	O
(	O
Kočiský	B-DatasetName
et	O
al	O
,	O
2018	O
)	O
,	O
DuoRC	O
(	O
Saha	B-DatasetName
et	O
al	O
,	O
2018	O
)	O
,	O
HotpotQA	O
(	O
Yang	B-DatasetName
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
Natural	O
Questions	O
(	B-DatasetName
Kwiatkowski	I-DatasetName
et	O
al	O
,	O
2019	O
)	O
.	O
Although	O
these	O
data	O
sets	O
contain	O
long	O
documents	O
,	O
questions	O
are	O
answerable	O
by	O
short	O
entities	O
or	O
a	O
span	O
of	O
entities	O
.	O
In	O
particular	O
,	O
Natural	O
Questions	O
covers	B-DatasetName
two	I-DatasetName
types	O
of	O
short	O
and	O
long	O
answers	O
.	O
However	O
,	O
due	O
to	O
its	O
factoid	O
questions	O
,	O
most	O
long	O
answers	O
are	O
still	O
sections	O
containing	O
exactly	O
the	O
short	O
answers	O
and	O
so	O
are	O
trivial	O
(	O
e.g.	O
,	O
"	O
Where	O
is	O
the	O
world	O
's	O
largest	O
ice	O
sheet	O
...	O
?	O
"	O
,	O
Short	O
:	O
"	O
Antarctica	O
"	O
;	O
Long	O
:	O
"	O
The	O
Antarctic	O
ice	O
sheet	O
is	O
the	O
largest	O
single	O
mass	O
of	O
ice	O
on	O
Earth	O
...	O
"	O
)	O
.	O
Furthermore	O
,	O
although	O
a	O
small	O
portion	O
(	O
13	O
%	O
)	O
of	O
Natural	O
Questions	O
samples	B-DatasetName
have	I-DatasetName
only	O
long	O
answers	O
,	O
they	O
are	O
still	O
spans	O
of	O
simple	O
facts	O
.	O
For	O
example	O
,	O
"	O
Who	O
is	O
the	O
author	O
of	O
the	O
book	O
Arabian	O
Nights	O
?	O
"	O
has	O
no	O
short	O
answer	O
simply	O
because	O
there	O
are	O
multiple	O
authors	O
:	O
"	O
The	O
work	O
was	O
collected	O
over	O
many	O
centuries	O
by	O
various	O
authors	O
,	O
translators	O
...	O
"	O
.	O
In	O
contrast	O
,	O
we	O
address	O
non	O
-	O
factoid	O
questions	O
requiring	O
complex	O
answers	O
like	O
opinions	O
and	O
explanations	O
.	O
NLQuAD	O
's	O
answers	O
are	O
open	O
and	O
not	O
predefined	O
.	O
Figure	O
3	O
and	O
Table	O
3	O
present	O
our	O
question	O
types	O
.	O
NLQuAD	O
's	O
questions	O
are	O
also	O
not	O
self	O
-	O
contained	O
.	O
For	O
example	O
,	O
"	O
How	O
are	O
people	O
coping	O
in	O
the	O
lockdown	O
?	O
"	O
or	O
"	O
What	O
's	O
the	O
global	O
situation	O
?	O
"	O
can	O
not	O
be	O
answered	O
without	O
the	O
context	O
from	O
the	O
document	O
(	O
see	O
Figure	O
1	O
)	O
.	O
Section	O
3.2	O
discusses	O
our	O
question	O
types	O
in	O
detail	O
.	O
In	O
most	O
existing	O
QA	O
data	O
sets	O
such	O
as	O
SQuAD	O
,	O
crowd	B-DatasetName
-	O
workers	O
generate	O
questions	O
based	O
on	O
provided	O
short	O
passages	O
and	O
extract	O
answers	O
from	O
the	O
passages	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
.	O
This	O
method	O
of	O
question	O
generation	O
can	B-TaskName
make	I-TaskName
QA	O
samples	O
trivial	O
because	O
models	O
can	O
simply	O
detect	O
the	O
most	O
related	O
span	O
to	O
the	O
question	O
by	O
guessing	O
based	O
on	O
shallow	O
pattern	O
matching	O
(	O
Kočiský	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
contrast	O
,	O
all	O
annotations	O
in	O
NLQuAD	O
are	O
done	O
automatically	O
and	O
directly	O
based	O
on	O
the	O
news	O
articles	O
themselves	O
.	O
NLQuAD	O
,	O
unlike	O
MS	O
MARCO	O
(	B-DatasetName
Bajaj	I-DatasetName
et	O
al	O
,	O
2016	O
)	O
and	O
ELI5	O
(	O
Fan	B-DatasetName
et	O
al	O
,	O
2019	O
)	O
,	O
does	O
not	O
use	O
information	O
retrieval	O
(	B-TaskName
IR	I-TaskName
)	O
methods	O
to	O
collect	O
supporting	O
documents	O
.	O
Retrieved	O
documents	O
in	O
these	O
data	O
sets	O
are	O
not	O
guaranteed	O
to	O
contain	O
all	O
facts	O
required	O
to	O
answer	O
the	O
question	O
or	O
they	O
occasionally	O
just	O
contain	O
information	O
related	O
to	O
the	O
question	O
but	O
no	O
answers	O
.	O
NLQuAD	O
requires	O
document	O
-	O
level	O
language	O
understanding	O
.	O
With	O
an	O
average	O
document	O
length	O
and	O
answer	O
length	O
of	O
877	O
and	O
175	O
words	O
,	O
respectively	O
,	O
it	O
exceeds	O
the	O
maximum	O
input	O
length	O
of	O
the	O
state	O
of	O
the	O
art	O
QA	O
models	O
such	O
as	O
BERT	O
(	O
Devlin	B-MethodName
et	O
al	O
,	O
2018	O
)	O
and	O
RoBERTa	O
(	O
Liu	B-MethodName
et	O
al	O
,	O
2019	O
)	O
due	O
to	O
their	O
memory	O
and	O
computational	O
requirements	O
.	O
Thus	O
,	O
training	O
and	O
evaluating	O
the	O
(	O
document	O
,	O
question	O
,	O
answer	O
)	O
tuples	O
is	O
impossible	O
using	O
such	O
models	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
.	O
It	O
is	O
worth	O
noting	O
that	O
it	O
is	O
also	O
harder	O
to	O
perform	O
pre	O
-	O
selection	O
methods	O
before	O
the	O
final	O
span	O
detection	O
because	O
our	O
answers	O
are	O
long	O
.	O
Meanwhile	O
,	O
most	O
of	O
our	O
questions	O
are	O
not	O
self	O
-	O
contained	O
.	O
For	O
example	O
,	O
to	O
answer	O
the	O
question	O
"	O
How	O
are	O
people	O
coping	O
in	O
the	O
lockdown	O
?	O
"	O
(	O
Figure	O
1	O
)	O
,	O
the	O
system	O
needs	O
to	O
read	O
the	O
document	O
to	O
interpret	O
the	O
concept	O
of	O
"	O
lockdown	O
"	O
and	O
then	O
locate	O
the	O
information	O
regarding	O
the	O
people	O
's	O
behaviour	O
.	O
We	O
also	O
show	O
the	O
shortcomings	O
of	O
the	O
F1	O
score	O
and	B-MetricName
ROUGE	I-MetricName
-	O
N	O
scores	O
in	O
evaluating	O
long	O
sequences	O
.	O
There	O
is	O
a	O
higher	O
chance	O
of	O
overlap	O
between	O
the	O
word	O
N	O
-	O
grams	O
in	O
two	O
long	O
sequences	O
causing	O
F1	O
and	O
ROUGE	B-MetricName
-	O
N	O
to	O
over	O
-	O
estimate	O
the	O
performance	O
.	O
Therefore	O
,	O
we	O
propose	O
to	O
use	O
Intersection	O
over	O
Union	O
(	O
IoU	O
)	O
measuring	B-MetricName
position	O
-	O
sensitive	O
overlap	O
between	O
two	O
spans	O
.	O
In	O
summary	O
,	O
our	O
contributions	O
are	O
as	O
follows	O
:	O
(	O
1	O
)	O
We	O
introduce	O
a	O
new	O
data	O
set	O
for	O
non	O
-	O
factoid	O
long	O
QA	O
that	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
is	O
the	O
first	O
data	O
set	O
requiring	O
long	O
answer	O
span	O
detection	O
given	O
non	O
-	O
self	O
-	O
contained	O
and	O
non	O
-	O
factoid	O
questions	O
;	O
(	O
2	O
)	O
We	O
show	O
the	O
limitations	O
of	O
the	O
F1	O
score	O
in	B-MetricName
evaluating	I-MetricName
long	O
answers	O
and	O
propose	O
a	O
new	O
evaluation	O
metric	O
;	O
(	O
3	O
)	O
To	O
establish	O
baseline	O
results	O
,	O
we	O
experiment	O
with	O
three	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
:	O
BERT	O
,	O
RoBERTa	B-MethodName
,	O
and	B-MethodName
Longformer	O
,	O
and	B-MethodName
compare	O
them	O
with	O
human	O
performance	O
.	O
To	O
handle	O
the	O
input	O
length	O
limitations	O
of	O
BERT	O
and	O
RoBERTa	B-MethodName
,	O
we	B-MethodName
pro	O
-	O
pose	O
to	O
train	O
these	O
models	O
in	O
a	O
sliding	O
-	O
window	O
approach	O
;	O
(	O
4	O
)	O
We	O
finally	O
show	O
that	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
have	O
limited	O
performance	O
in	O
the	O
non	O
-	O
factoid	O
long	O
QA	O
task	O
.	O

Exact	B-MetricName
Match	I-MetricName
(	O
EM	B-MetricName
)	O
and	O
the	O
macro	O
-	O
averaged	O
F1	B-MetricName
score	I-MetricName
are	O
the	O
two	O
main	O
evaluation	O
metrics	O
in	O
the	O
span	O
detection	O
QA	O
task	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
.	O
Exact	B-MetricName
Match	I-MetricName
determines	O
if	O
the	O
prediction	O
exactly	O
matches	O
the	O
target	O
which	O
can	O
be	O
a	O
too	O
strict	O
criterion	O
for	O
long	O
answers	O
.	O
The	O
F1	B-MetricName
score	I-MetricName
measures	O
the	O
overlap	O
between	O
the	O
words	O
in	O
the	O
prediction	O
and	O
the	O
target	O
.	O
It	O
treats	O
sequences	O
as	O
a	O
bag	O
of	O
words	O
.	O
Unfortunately	O
,	O
in	O
long	O
answers	O
,	O
it	O
is	O
highly	O
likely	O
that	O
a	O
random	O
,	O
long	O
span	O
shares	O
a	O
considerable	O
number	O
of	O
tokens	O
with	O
the	O
target	O
span	O
.	O
The	O
ROUGE	O
-	O
N	O
scores	O
(	O
Lin	O
and	O
Och	O
,	O
2004	O
)	O
,	O
which	O
are	O
primarily	O
used	O
for	O
sequence	O
generation	O
evaluation	O
,	O
have	O
the	O
same	O
drawback	O
in	O
long	O
sequences	O
.	O
ROUGE	O
-	O
N	O
measures	O
the	O
N	O
-	O
gram	O
overlap	O
between	O
the	O
prediction	O
and	O
target	O
.	O
High	O
chances	O
of	O
overlap	O
of	O
unigrams	O
and	O
bigrams	O
in	O
long	O
sequences	O
cause	O
ROUGE	O
-	O
1	O
and	O
ROUGE	O
-	O
2	O
to	O
over	O
-	O
estimate	O
performance	O
.	O
The	O
same	O
holds	O
for	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
with	O
the	O
Longest	O
Common	O
Sub	O
-	O
sequence	O
(	O
LCS	O
)	O
because	O
of	O
a	O
high	O
chance	O
of	O
longer	O
LCSs	O
between	O
two	O
long	O
sequences	O
.	O
To	O
better	O
take	O
sequence	O
similarities	O
into	O
account	O
,	O
we	O
propose	O
to	O
evaluate	O
models	O
with	O
the	O
Intersection	O
over	O
Union	O
(	O
IoU	B-MetricName
)	O
score	O
,	O
also	O
known	O
as	O
Jaccard	O
Index	O
.	O
IoU	B-MetricName
is	O
defined	O
as	O
follows	O
:	O
IoU	B-MetricName
=	O
|	O
p	O
∩	O
t	O
|	O
|	O
p	O
∪	O
t	O
|	O
Question	O
:	O
How	O
did	O
we	O
get	O
here	O
?	O
Headline	O
:	O
Eta	O
disarms	O
:	O
French	O
police	O
find	O
3.5	O
tonnes	O
of	O
weapons	O
Target	O
Answer	O
:	O
Slowly	O
,	O
and	O
with	O
many	O
false	O
starts	O
.	O
Eta	O
used	O
parts	O
of	O
south	O
-	O
western	O
France	O
as	O
a	O
base	O
,	O
even	O
though	O
most	O
of	O
its	O
operations	O
were	O
against	O
Spanish	O
targets	O
in	O
Spain	O
.	O
The	O
group	O
has	O
,	O
however	O
,	O
killed	O
some	O
French	O
policemen	O
,	O
but	O
mostly	O
during	O
police	O
raids	O
on	O
members	O
of	O
the	O
group	O
.	O
Etaś	O
first	O
ceasefire	O
was	O
in	O
1998	O
,	O
but	O
collapsed	O
the	O
following	O
year	O
.	O
A	O
similar	O
declaration	O
in	O
2006	O
only	O
lasted	O
a	O
matter	O
of	O
months	O
,	O
ending	O
when	O
Eta	O
bombed	O
an	O
airport	O
car	O
park	O
,	O
killing	O
two	O
people	O
.	O
Four	O
years	O
later	O
,	O
in	O
2010	O
,	O
Eta	O
announced	O
it	O
would	O
not	O
carry	O
out	O
further	O
attacks	O
and	O
in	O
January	O
2011	O
,	O
it	O
declared	O
a	O
permanent	O
and	O
"	O
internationally	O
verifiable	O
"	O
ceasefire	O
but	O
refused	O
to	O
disarm	O
.	O
In	O
recent	O
years	O
,	O
police	O
in	O
France	O
and	O
Spain	O
have	O
arrested	O
hundreds	O
of	O
Eta	O
figures	O
and	O
seized	O
many	O
of	O
its	O
weapons	O
.	O
Etaś	O
political	O
wing	O
,	O
Herri	O
Batasuna	O
,	O
was	O
banned	O
by	O
the	O
Spanish	O
government	O
,	O
which	O
argued	O
that	O
the	O
two	O
groups	O
were	O
inextricably	O
linked	O
.	O
Prediction	O
:	O
The	O
group	O
was	O
set	O
up	O
more	O
than	O
50	O
years	O
ago	O
in	O
the	O
era	O
of	O
Spanish	O
dictator	O
General	B-DatasetName
Franco	O
,	O
who	O
repressed	O
the	O
Basques	O
politically	O
and	O
culturally	O
.	O
Eta	O
's	O
goal	O
was	O
to	O
create	O
an	O
independent	O
Basque	O
state	O
out	O
of	O
territory	O
in	O
south	O
-	O
west	O
France	O
and	O
northern	O
Spain	O
.	O
Its	O
first	O
known	O
killing	O
was	O
in	O
1968	O
,	O
when	O
a	O
secret	O
police	O
chief	O
was	O
shot	O
dead	O
in	O
the	O
Basque	O
city	O
of	O
San	O
Sebastian	O
.	O
France	O
and	O
Spain	O
refuse	O
to	O
negotiate	O
with	O
Eta	O
,	O
which	O
is	O
on	O
the	O
EU	O
blacklist	O
of	O
terrorist	O
organisations	O
.	O
Figure	O
5	O
:	O
A	O
prediction	O
span	O
that	O
is	O
semantically	O
different	O
from	O
the	O
target	O
span	O
but	O
has	O
a	O
F1=30	O
%	O
(	O
Prec.=43	O
%	O
,	O
Rec.=23	O
%	O
)	O
and	O
IoU=0	O
.	O
Red	O
shows	O
the	O
overlapping	O
words	O
in	O
the	O
prediction	O
span	O
with	O
the	O
target	O
.	O
Articles	O
(	O
a	O
,	O
an	O
,	O
the	O
)	O
and	O
punctuations	O
are	O
discarded	O
before	O
overlapping	O
calculation	O
.	O
(	O
ROUGE	O
-	O
1=32	O
%	O
,	O
ROUGE	O
-	O
2=4	O
%	O
,	O
ROUGE	O
-	O
L=24	O
%	O
)	O
where	O
p	O
and	O
t	O
and	O
are	O
the	O
predicted	O
and	O
target	O
contiguous	O
intervals	O
over	O
the	O
context	O
document	O
,	O
containing	O
the	O
positions	O
of	O
the	O
tokens	O
.	O
Intersec	O
-	O
tion	O
(	O
p	O
∩	O
t	O
=	O
{	O
x	O
|	O
x	O
p	O
and	O
x	O
t	O
}	O
)	O
measures	O
the	O
overlapping	O
interval	O
and	O
union	O
(	O
∪	O
)	O
is	O
defined	O
as	O
p	O
∪	O
t	O
=	O
{	O
x	O
|	O
x	O
p	O
or	O
x	O
t	O
}	O
.	O
Figure	O
4	O
(	O
left	O
/	O
middle	O
)	O
compares	O
the	O
F1	B-MetricName
and	O
ROUGE	O
-	O
N	O
scores	O
and	O
IoU	B-MetricName
for	O
the	O
Longformer	B-MethodName
model	O
on	O
the	O
development	O
set	O
.	O
The	O
F1	B-MetricName
and	O
ROUGE	O
-	O
N	O
scores	O
are	O
always	O
higher	O
than	O
IoU	B-MetricName
,	O
but	O
the	O
metrics	O
perform	O
similarly	O
in	O
their	O
higher	O
values	O
.	O
Somewhat	O
surprisingly	O
,	O
the	O
F1	B-MetricName
score	I-MetricName
can	O
be	O
up	O
to	O
40	O
%	O
while	O
there	O
is	O
no	O
overlap	O
between	O
the	O
two	O
spans	O
and	O
IoU=0	O
.	O
We	O
manually	O
inspected	O
the	O
spans	O
with	O
F1>0	O
and	O
IoU=0	O
and	O
saw	O
no	O
significant	O
semantic	B-TaskName
similarity	I-TaskName
between	O
the	O
predicted	O
answer	O
span	O
and	O
the	O
target	O
span	O
.	O
The	O
same	O
pattern	O
repeats	O
for	O
the	O
ROUGE	O
-	O
N	O
scores	O
.	O
ROUGE	O
-	O
1	O
similar	O
to	O
F1	B-MetricName
can	O
reach	O
40	O
%	O
while	O
IoU=0	O
,	O
but	O
ROUGE	O
-	O
2	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
are	O
less	O
prone	O
to	O
such	O
over	O
-	O
estimation	O
due	O
to	O
lower	O
chance	O
of	O
overlap	O
of	O
bigrams	O
than	O
unigrams	O
and	O
shorter	O
LCSs	O
in	O
two	O
random	O
non	O
-	O
overlapping	O
sequences	O
.	O
Figure	O
4	O
(	O
right	O
)	O
indicates	O
that	O
the	O
F1	B-MetricName
and	O
ROUGE	O
-	O
N	O
scores	O
are	O
higher	O
than	O
IoU	B-MetricName
for	O
longer	O
answers	O
reiterating	O
the	O
fact	O
that	O
these	O
scores	O
over	O
-	O
estimate	O
more	O
for	O
longer	O
sequences	O
.	O
Figure	O
5	O
shows	O
two	O
spans	O
in	O
a	O
document	O
with	O
high	O
F1	B-MetricName
and	O
ROUGE	O
-	O
N	O
percentages	O
,	O
but	O
different	O
meanings	O
.	O

We	O
use	O
the	O
BM25L	O
ranking	O
function	O
(	O
Trotman	O
et	O
al	O
,	O
2014	O
)	O
to	O
investigate	O
how	O
a	O
basic	O
IR	O
approach	O
can	O
detect	O
answer	O
spans	O
using	O
TF	O
-	O
IDF	O
features	O
.	O
We	O
adopt	O
a	O
sliding	O
window	O
approach	O
with	O
a	O
window	O
size	O
of	O
512	O
and	O
a	O
stride	O
of	O
one	O
sentence	O
.	O
We	O
compare	O
BM25L	O
with	O
random	O
window	O
(	O
span	O
)	O
selection	O
and	O
the	O
first	O
and	O
last	O
window	O
selection	O
in	O
the	O
documents	O
.	O
Table	O
4	O
presents	O
the	O
results	O
of	O
the	O
ranking	O
functions	O
.	O
In	O
the	O
BM25L	O
-	O
oracle	O
,	O
we	O
set	O
the	O
window	O
size	O
to	O
the	O
target	O
answer	O
span	O
size	O
.	O
BM25L	O
-	O
oracle	O
outperforms	O
the	O
other	O
methods	O
but	O
the	O
results	O
are	O
far	O
from	O
perfect	O
.	O
There	O
is	O
no	O
significant	O
difference	O
between	O
BM25L	O
and	O
other	O
methods	O
.	O
The	O
results	O
restate	O
the	O
fact	O
that	O
there	O
is	O
little	O
word	O
overlap	O
between	O
non	O
-	O
factoid	O
questions	O
and	O
their	O
answers	O
.	O
We	O
analyze	O
the	O
performance	O
of	O
BERT	B-MethodName
and	O
RoBERTa	B-MethodName
with	O
different	O
hyper	O
-	O
parameters	O
on	O
the	O
development	O
set	O
in	O
Table	O
5	O
.	O
Smaller	O
strides	O
,	O
i.e.	O
,	O
higher	O
overlap	O
between	O
the	O
segments	O
,	O
and	O
warm	O
-	O
up	O
contribute	O
to	O
better	O
performances	O
.	O
RoBERTa	B-MethodName
constantly	O
outperforms	O
BERT	B-MethodName
,	O
which	O
is	O
to	O
be	O
expected	O
as	O
RoBERTa	B-MethodName
is	O
optimized	O
robustly	O
during	O
the	O
pretraining	O
.	O
We	O
use	O
the	O
HuggingFace	O
's	O
Transformers	O
(	O
Wolf	O
et	O
al	O
,	O
2019	O
)	O
code	O
4	O
and	O
train	O
the	O
base	O
and	O
large	O
models	O
on	O
2	O
and	O
4	O
GPUs	O
,	O
respectively	O
.	O
We	O
have	O
to	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
12	O
and	O
8	O
,	O
respectively	O
,	O
for	O
the	O
base	O
and	O
large	O
models	O
because	O
of	O
the	O
long	O
input	O
sequence	O
size	O
and	O
memory	O
limitations	O
.	O
We	O
use	O
the	O
official	O
AllenAI	O
Longformer	B-MethodName
code	O
5	O
to	O
train	O
Longformer	B-MethodName
on	O
NLQuAD	O
.	O
We	O
use	O
the	O
same	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
12	O
(	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
1	O
and	O
gradient	O
accumulation	O
over	O
12	O
batches	O
)	O
and	O
learning	B-HyperparameterName
rate	I-HyperparameterName
warmup	O
for	O
the	O
first	O
1	O
,	O
000	O
steps	O
.	O
Due	O
to	O
memory	O
requirements	O
,	O
we	O
limit	O
the	O
experiments	O
to	O
only	O
the	O
Longformer	B-MethodName
base	O
model	O
(	O
the	O
large	O
model	O
can	O
not	O
fit	O
on	O
our	O
GPUs	O
even	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
1	O
)	O
.	O
We	O
ran	O
the	O
experiments	O
on	O
2	O
NVIDIA	O
P40	O
(	O
24	O
GB	O
GPU	O
memory	O
)	O
for	O
about	O
one	O
day	O
for	O
5	O
epochs	O
.	O
Similarly	O
,	O
we	O
choose	O
the	O
best	O
epoch	O
based	O
on	O
the	O
performance	O
on	O
the	O
development	O
set	O
.	O
Table	O
6	O
summarizes	O
the	O
scores	O
obtained	O
by	O
the	O
baseline	O
systems	O
on	O
the	O
NLQuAD	O
evaluation	O
set	O
.	O
While	O
Longformer	B-MethodName
significantly	O
outperforms	O
BERT	B-MethodName
and	O
RoBERTa	B-MethodName
,	O
its	O
performance	O
,	O
particularly	O
in	O
terms	O
of	O
IoU	B-MetricName
and	O
EM	B-MetricName
,	O
is	O
far	O
from	O
perfect	O
.	O
This	O
demonstrates	O
that	O
NLQuAD	O
and	O
non	O
-	O
factoid	O
QA	O
is	O
still	O
an	O
open	O
problem	O
for	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O

To	O
ensure	O
that	O
the	O
samples	O
are	O
of	O
high	O
quality	O
,	O
in	O
addition	O
to	O
the	O
initial	O
investigation	O
and	O
pre	O
-	O
processing	O
steps	O
,	O
we	O
asked	O
four	O
volunteers	O
to	O
investigate	O
50	O
random	O
samples	O
from	O
the	O
evaluation	O
set	O
.	O
They	O
rated	O
the	O
goodness	O
of	O
answers	O
on	O
a	O
3	O
-	O
point	O
scale	O
:	O
(	O
1	O
:	O
Irrelevant	O
answer	O
;	O
2	O
:	O
Good	O
answer	O
after	O
adding	O
or	O
removing	O
some	O
sentences	O
;	O
3	O
:	O
Perfect	O
answer	O
)	O
.	O
The	O
average	O
score	O
is	O
2.56	O
indicating	O
the	O
high	O
quality	O
of	O
NLQuAD	O
's	O
QA	O
samples	O
.	O
In	O
order	O
to	O
benchmark	O
human	O
performance	O
,	O
we	O
asked	O
the	O
four	O
volunteers	O
to	O
answer	O
50	O
questions	O
,	O
a	O
randomly	O
sampled	O
subset	O
of	O
evaluation	O
set	O
.	O
They	O
were	O
given	O
unlimited	O
time	O
to	O
detect	O
the	O
answers	O
,	O
but	O
on	O
average	O
,	O
it	O
took	O
them	O
about	O
270	O
seconds	O
to	O
answer	O
a	O
question	O
.	O
ing	O
the	O
best	O
human	O
answer	O
in	O
terms	O
of	O
our	O
primary	O
evaluation	O
metric	O
(	O
IoU	B-MetricName
)	O
for	O
each	O
sample	O
.	O
While	O
NLQuAD	O
is	O
a	O
challenging	O
task	O
both	O
for	O
humans	O
and	O
the	O
state	O
of	O
the	O
art	O
QA	O
models	O
,	O
the	O
human	O
upper	O
bound	O
performance	O
significantly	O
outperforms	O
the	O
models	O
.	O
We	O
suspect	O
that	O
the	O
mediocre	O
average	O
of	O
human	O
performance	O
,	O
considering	O
the	O
high	O
score	O
of	O
the	O
target	O
answers	O
,	O
might	O
be	O
because	O
volunteers	O
are	O
not	O
familiar	O
with	O
the	O
articles	O
'	O
writing	O
style	O
or	O
they	O
might	O
have	O
become	O
exhausted	O
by	O
reading	O
long	O
articles	O
.	O
Furthermore	O
,	O
we	O
asked	O
another	O
volunteer	O
to	O
compare	O
the	O
target	O
answers	O
with	O
the	O
predicted	O
answers	O
in	O
a	O
pairwise	O
comparison	O
for	O
100	O
samples	O
.	O
Figure	O
6	O
shows	O
that	O
the	O
target	O
answers	O
are	O
preferred	O
in	O
37	O
%	O
and	O
64	O
%	O
of	O
cases	O
over	O
the	O
Longformer	B-MethodName
and	O
RoBERTa	B-MethodName
predictions	O
,	O
respectively	O
.	O
The	O
human	O
evaluation	O
is	O
in	O
line	O
with	O
the	O
results	O
shown	O
in	O
Table	O
6	O
and	O
Table	O
7	O
.	O

Figure	O
7	O
compares	O
the	O
performance	O
of	O
BERT	B-MethodName
,	O
RoBERTa	B-MethodName
,	O
and	O
Longformer	B-MethodName
for	O
instances	O
with	O
different	O
document	O
and	O
answer	O
lengths	O
.	O
As	O
expected	O
,	O
both	O
longer	O
documents	O
and	O
longer	O
answers	O
are	O
harder	O
for	O
the	O
models	O
.	O
Surprisingly	O
,	O
BERT	B-MethodName
and	O
RoBERTa	B-MethodName
outperform	O
Longformer	B-MethodName
for	O
longer	O
answers	O
.	O
The	O
same	O
pattern	O
occurs	O
for	O
F1	B-MetricName
and	O
EM	B-MetricName
(	O
not	O
shown	O
in	O
the	O
figure	O
)	O
.	O
Figure	O
7	O
(	O
right	O
)	O
shows	O
that	O
RoBERTa	B-MethodName
and	O
BERT	B-MethodName
behave	O
completely	O
differently	O
compared	O
to	O
Longformer	B-MethodName
for	O
longer	O
answer	O
lengths	O
.	O
The	O
former	O
models	O
have	O
a	O
bias	O
to	O
predict	O
longer	O
spans	O
while	O
Longformer	B-MethodName
under	O
-	O
estimates	O
the	O
length	O
of	O
the	O
answer	O
span	O
.	O
This	O
different	O
behaviour	O
might	O
be	O
due	O
to	O
the	O
sliding	O
window	O
approach	O
and	O
the	O
prediction	O
aggregation	O
in	O
the	O
RoBERTa	B-MethodName
and	O
BERT	B-MethodName
models	O
and	O
the	O
attention	O
dilation	O
strategy	O
in	O
Longformer	B-MethodName
.	O

We	O
introduce	O
NLQuAD	O
,	O
a	O
non	O
-	O
factoid	O
long	O
question	B-TaskName
answering	I-TaskName
data	O
set	O
from	O
BBC	O
news	O
articles	O
.	O
NLQuAD	O
's	O
question	O
types	O
and	O
the	O
long	O
lengths	O
of	O
its	O
context	O
documents	O
as	O
well	O
as	O
answers	O
,	O
make	O
it	O
a	O
challenging	O
real	O
-	O
world	O
task	O
.	O
We	O
propose	O
to	O
use	O
Intersection	O
over	O
Union	O
(	O
IoU	B-MetricName
)	O
as	O
an	O
evaluation	O
metric	O
for	O
long	O
question	B-TaskName
answering	I-TaskName
.	O
To	O
establish	O
a	O
baseline	O
performance	O
,	O
we	O
experimented	O
with	O
the	O
BERT	B-MethodName
,	O
RoBERTa	B-MethodName
,	O
and	O
Longformer	B-MethodName
question	B-TaskName
answering	I-TaskName
models	O
.	O
Longformer	B-MethodName
outperforms	O
the	O
other	O
methods	O
with	O
an	O
IoU	B-MetricName
of	O
73.57	O
%	O
,	O
but	O
the	O
results	O
show	O
that	O
the	O
performance	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
question	B-TaskName
answering	I-TaskName
systems	O
is	O
far	O
from	O
perfect	O
.	O
We	O
hope	O
NLQuAD	O
will	O
inspire	O
more	O
research	O
in	O
the	O
area	O
of	O
document	O
-	O
level	O
language	O
understanding	O
and	O
question	B-TaskName
answering	I-TaskName
.	O

There	O
can	O
be	O
many	O
possible	O
POLAR	O
dimensions	O
,	O
which	O
requires	O
to	O
select	O
the	O
most	O
suitable	O
ones	O
.	O
That	O
is	O
,	O
we	O
want	O
to	O
define	O
a	O
limited	O
set	O
of	O
opposites	O
that	O
best	O
describes	O
words	O
or	O
emoji	O
w.r.t	O
.	O
interpretability	O
across	O
the	O
whole	O
embedding	O
.	O
Extremal	O
Word	O
Score	B-MetricName
(	O
EWSO	O
)	O
.	O
We	O
propose	O
a	O
new	O
metric	O
to	O
measure	O
the	O
quality	O
of	O
polar	O
dimensions	O
complementing	O
heuristics	O
from	O
(	O
Mathew	O
et	O
al	O
,	O
2020	O
)	O
.	O
It	O
measures	O
the	O
embedding	O
confidence	O
and	O
consistency	O
along	O
available	O
differentials	O
.	O
The	O
idea	O
of	O
POLAR	O
ρ	O
is	O
that	O
directions	O
represent	O
semantics	O
within	O
the	O
input	O
embedding	O
.	O
We	O
determine	O
embedded	O
terms	O
shortest	O
distance	O
to	O
these	O
axes	O
via	O
orthogonal	O
projection	O
;	O
we	O
use	O
resulting	O
intersections	O
as	O
the	O
position	O
w.r.t	O
.	O
the	O
directions	O
.	O
That	O
is	O
,	O
as	O
a	O
new	O
heuristic	O
,	O
for	O
each	O
of	O
our	O
differentials	O
dir	O
i	O
,	O
we	O
look	O
out	O
for	O
k	B-HyperparameterName
=	I-HyperparameterName
10	O
embedded	O
words	O
at	O
the	O
extremes	O
(	O
having	O
the	O
highest	O
scores	O
in	O
each	O
direction	O
)	O
and	O
take	O
their	O
average	O
cosine	O
distance	O
within	O
the	O
original	O
embedding	O
D	O
to	O
the	O
differential	O
as	O
a	O
measure	O
.	O
This	O
results	O
in	O
the	O
average	O
similarity	O
of	O
existing	O
extremal	O
words	O
on	O
our	O
scale	O
-	O
a	O
heuristic	O
that	O
represents	O
the	O
skew	O
-	O
whiffiness	O
within	O
the	O
extremes	O
on	O
a	O
differential	O
scale	O
.	O

Preprocessing	O
.	O
We	O
tokenize	O
sentences	O
with	O
spaCy	O
and	O
remove	O
stopwords	O
.	O
To	O
increase	O
amounts	O
of	O
available	O
data	O
,	O
we	O
remove	O
all	O
emoji	O
modifiers	O
(	O
skin	O
tone	O
and	O
gender	O
)	O
:	O
{	O
,	O
,	O
}	O
.	O
Due	O
to	O
German	O
language	O
,	O
we	O
keep	O
capitalization	O
.	O
Original	O
Embedding	O
.	O
We	O
use	O
gensim	O
implementation	O
of	O
Word2Vec	O
(	O
W2V	O
)	O
.	O
A	O
qualitative	O
investigation	O
suggests	O
that	O
skip	O
-	O
gram	O
works	O
better	O
than	O
CBOW	O
(	O
better	O
word	O
analogy	O
)	O
.	O
We	O
kept	O
training	O
parameters	O
largely	O
at	O
defaults	O
including	O
negative	O
sampling	O
,	O
opting	O
for	O
d	O
=	O
300	O
dimensions	O
.	O
Interpretable	O
Embedding	O
.	O
The	O
actual	O
application	O
of	O
embedding	O
transformation	O
is	O
simple	O
.	O
We	O
create	O
the	O
matrix	O
of	O
differentials	O
dir	O
,	O
the	O
POLAR	O
subspace	O
,	O
according	O
to	O
our	O
antonym	O
-	O
set	O
P	O
words	O
∪	O
P	O
emoji	O
(	O
3.2	O
)	O
.	O
After	O
normalizing	O
the	O
subspace	O
vectors	O
,	O
we	O
create	O
all	O
embedding	O
vectors	O
via	O
projec	O
-	O
tion	O
with	O
interpret	O
W	O
Mixed	O
E	O
Words	O
(	O
W	O
/	O
W	O
)	O
(	O
W	O
/	O
M	O
)	O
(	O
W	O
/	O
E	O
)	O
Emoji	O
(	O
E	O
/	O
W	O
)	O
(	O
E	O
/	O
M	O
)	O
(	O
E	O
/	O
E	O
)	O
(	O
−	O
E	O
v	O
=	O
d	O
ir	O
T	O
−	O
W	O
v	O
,	O
∀v	O
V.	O
Though	O
normalization	O
requires	O
careful	O
later	O
additions	O
to	O
the	O
PO	O
-	O
LAR	O
space	O
,	O
we	O
opted	O
for	O
standard	O
normalization	O
,	O
E	O
stdnrm	O
=	O
[	O
E	O
−	O
mean	O
(	O
E	O
)	O
]	O
std	O
(	O
E	O
)	O
−1	O
,	O
to	O
ensure	O
that	O
the	O
whole	O
embedding	O
space	O
aligns	O
properly	O
around	O
the	O
center	O
of	O
gravity	O
on	O
each	O
differential	O
scale	O
.	O
We	O
select	O
the	O
best	O
suited	O
opposites	O
for	O
a	O
given	O
embedding	O
space	O
by	O
using	O
the	O
Extremal	O
Word	O
Score	B-MetricName
(	O
2.3	O
)	O
for	O
d=500	O
+	O
44	O
dimensions	O
(	O
words	O
+	O
emoji	O
)	O
.	O

Emoji	O
.	O
As	O
a	O
byproduct	O
,	O
we	O
also	O
show	O
if	O
emoji	O
opposites	O
are	O
preferred	O
over	O
words	O
.	O
That	O
is	O
,	O
we	O
focus	O
on	O
the	O
mixed	O
campaigns	O
describing	O
words	O
and	O
emoji	O
with	O
words	O
and	O
emoji	O
(	O
*	O
/M	O
)	O
.	O
We	O
establish	O
a	O
baseline	O
by	O
filtering	O
the	O
counts	O
for	O
all	O
non	O
-	O
POLAR	O
ρ	O
randomly	O
chosen	O
dimensions	O
being	O
word	O
or	O
emoji	O
representing	O
a	O
Bernoulli	O
experiment	O
.	O
I.e.	O
,	O
along	O
the	O
random	O
dimensions	O
,	O
our	O
coders	O
chose	O
228	O
vs.	O
221	O
and	O
167	O
vs.	O
187	O
words	O
over	O
emoji	O
.	O
Applying	O
chi	O
-	O
squared	O
statistics	O
indicates	O
,	O
that	O
both	O
types	O
(	O
words	O
and	O
emoji	O
)	O
are	O
chosen	O
equally	O
often	O
at	O
least	O
can	O
not	O
be	O
rejected	O
.	O
We	O
next	O
analyze	O
the	O
POLAR	O
ρ	O
chosen	O
dimensions	O
in	O
the	O
mixed	O
campaigns	O
.	O
Here	O
,	O
coders	O
chose	O
words	O
over	O
emoji	O
as	O
follows	O
:	O
465	O
vs.	O
336	O
in	O
the	O
(	O
W	O
/	O
M	O
)	O
,	O
and	O
414	O
vs.	O
482	O
in	O
the	O
(	O
E	O
/	O
M	O
)	O
campaign	O
.	O
We	O
find	O
statistically	O
significant	O
favors	O
for	O
words	O
to	O
interpret	O
words	O
and	O
emoji	O
to	O
describe	O
emoji	O
.	O
Scale	O
Usage	O
.	O
We	O
find	O
no	O
evidence	O
for	O
any	O
directional	O
biases	O
within	O
our	O
preference	O
test	O
(	O
cf	O
.	O
3c	O
)	O
.	O
Coder	O
Agreement	O
.	O
While	O
the	O
aggregate	O
results	O
are	O
compelling	O
,	O
we	O
use	O
the	O
Krippendorff	O
-	O
alpha	B-HyperparameterName
metric	O
to	O
measure	O
coder	O
agreement	O
along	O
all	O
six	O
campaigns	O
as	O
shown	O
Tab	O
.	O
2	O
;	O
higher	O
scores	O
depict	O
better	O
agreement	O
.	O
We	O
split	O
the	O
overall	O
results	O
by	O
test	O
first	O
(	O
Selection	O
&	O
Preference	O
)	O
,	O
but	O
also	O
show	O
additional	O
agreement	O
results	O
for	O
preferences	O
along	O
POLAR	O
ρ	O
chosen	O
dimensions	O
and	O
their	O
random	O
counterpart	O
.	O
Most	O
agreement	O
is	O
within	O
the	O
moderate	O
regime	O
.	O
This	O
observation	O
does	O
not	O
come	O
unexpected	O
from	O
our	O
five	O
non	O
-	O
expert	O
classifiers	O
per	O
task	O
.	O
Overall	O
,	O
we	O
find	O
that	O
coders	O
agree	O
better	O
for	O
well	O
-	O
performing	O
campaigns	O
.	O
We	O
identify	O
the	O
best	O
agreement	O
scores	O
for	O
interpreting	O
emoji	O
with	O
emoji	O
(	O
E	O
/	O
E	O
)	O
;	O
coders	O
agree	O
least	O
in	O
the	O
worst	O
performing	O
explaining	O
words	O
with	O
emoji	O
campaign	O
(	O
W	O
/	O
E	O
)	O
.	O
For	O
the	O
preference	O
test	O
,	O
we	O
subdivide	O
our	O
results	O
into	O
POLAR	O
ρ	O
chosen	O
differentials	O
and	O
compare	O
them	O
to	O
the	O
randomly	O
chosen	O
ones	O
.	O
While	O
the	O
agreement	O
on	O
the	O
random	O
opposites	O
is	O
only	O
fair	O
,	O
the	O
agreement	O
on	O
POLAR	O
ρ	O
chosen	O
opposites	O
is	O
consistently	O
better	O
:	O
Estimating	O
differential	O
scale	O
directions	O
via	O
POLAR	O
ρ	O
for	O
words	O
yields	O
moderate	O
agreement	O
,	O
whereas	O
coders	O
consistently	O
align	O
substantially	O
in	O
interpreting	O
emoji	O
.	O
We	O
presume	O
emoji	O
may	O
convey	O
limited	O
ideas	O
,	O
but	O
are	O
easier	O
to	O
grasp	O
,	O
have	O
better	O
readability	O
;	O
the	O
campaings	O
interpreting	O
emoji	O
(	O
E/	O
*	O
)	O
were	O
generally	O
accomplished	O
faster	O
.	O

This	O
paper	O
describes	O
the	O
NoahNMT	O
system	O
submitted	O
to	O
the	O
WMT	O
2021	O
shared	O
task	O
of	O
Very	O
Low	O
Resource	O
Supervised	O
Machine	B-TaskName
Translation	I-TaskName
.	O
The	O
system	O
is	O
a	O
standard	O
Transformer	B-MethodName
model	O
equipped	O
with	O
our	O
recent	O
technique	O
of	O
dual	O
transfer	O
.	O
It	O
also	O
employs	O
widely	O
used	O
techniques	O
that	O
are	O
known	O
to	O
be	O
helpful	O
for	O
neural	O
machine	B-TaskName
translation	I-TaskName
,	O
including	O
iterative	O
backtranslation	O
,	O
selected	O
finetuning	O
,	O
and	O
ensemble	O
.	O
The	O
final	O
submission	O
achieves	O
the	O
top	O
BLEU	B-MetricName
for	O
three	O
translation	O
directions	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
the	O
NoahNMT	O
system	O
submitted	O
to	O
one	O
of	O
the	O
WMT	O
2021	O
shared	O
tasks	O
.	O
The	O
shared	O
task	O
features	O
both	O
unsupervised	B-TaskName
machine	I-TaskName
translation	I-TaskName
and	O
very	O
low	O
resource	O
supervised	O
machine	B-TaskName
translation	I-TaskName
.	O
As	O
our	O
core	O
technique	O
is	O
mainly	O
suitable	O
for	O
low	O
resource	O
supervised	O
machine	B-TaskName
translation	I-TaskName
,	O
we	O
participated	O
in	O
four	O
translation	O
directions	O
between	O
Chuvash	O
-	O
Russian	O
(	O
chv	O
-	O
ru	O
)	O
and	O
Upper	O
Sorbian	O
-	O
German	O
(	O
hsb	O
-	O
de	O
)	O
.	O
Our	O
core	O
technique	O
is	O
called	O
dual	O
transfer	O
(	O
Zhang	O
et	O
al	O
,	O
2021	O
)	O
,	O
which	O
belongs	O
to	O
the	O
family	O
of	O
transfer	B-TaskName
learning	I-TaskName
.	O
It	O
transfers	O
from	O
both	O
high	O
resource	O
neural	O
machine	B-TaskName
translation	I-TaskName
model	O
and	O
pretrained	O
language	O
model	O
to	O
improve	O
the	O
quality	O
of	O
low	O
resource	O
machine	B-TaskName
translation	I-TaskName
.	O
During	O
the	O
preparation	O
for	O
the	O
shared	O
task	O
,	O
we	O
conducted	O
additional	O
experiments	O
that	O
supplement	O
the	O
original	O
paper	O
,	O
including	O
the	O
choice	O
of	O
parent	O
language	O
,	O
the	O
validation	O
of	O
Transformer	B-MethodName
big	O
model	O
,	O
and	O
the	O
usage	O
of	O
dual	O
transfer	O
along	O
with	O
iterative	O
back	O
-	O
translation	O
.	O
In	O
addition	O
,	O
we	O
also	O
applied	O
proven	O
techniques	O
to	O
strengthen	O
the	O
quality	O
of	O
our	O
system	O
,	O
including	O
selected	O
finetuning	O
and	O
ensemble	O
.	O
Our	O
final	O
submission	O
achieves	O
the	O
top	O
BLEU	B-MetricName
on	O
the	O
blind	O
test	O
sets	O
for	O
three	O
translation	O
directions	O
:	O
chv	O
ru	O
,	O
ru	O
chv	O
,	O
and	O
hsb	O
de	O
.	O

Selected	O
finetuning	O
aims	O
to	O
deal	O
with	O
the	O
domain	O
difference	O
that	O
may	O
exist	O
between	O
the	O
test	O
set	O
and	O
the	O
training	O
set	O
.	O
Given	O
the	O
source	O
side	O
of	O
the	O
test	O
set	O
,	O
we	O
try	O
to	O
select	O
similar	O
source	O
sentences	O
from	O
the	O
training	O
set	O
,	O
and	O
then	O
finetune	O
the	O
translation	O
model	O
on	O
the	O
selected	O
subset	O
of	O
training	O
sentence	O
pairs	O
.	O
We	O
use	O
BM25	O
(	O
Robertson	O
and	O
Zaragoza	O
,	O
2009	O
)	O
to	O
calculate	O
the	O
similarity	O
between	O
two	O
sentences	O
for	O
retrieval	O
.	O
The	O
BM25	O
score	O
between	O
a	O
query	O
sentence	O
Q	O
and	O
a	O
sentence	O
D	O
in	O
the	O
corpus	O
for	O
parent	O
language	O
chv	O
ru	O
BLEU	B-MetricName
kk	O
18.47	O
en	O
18.61	O
retrieval	O
C	O
is	O
given	O
by	O
s	O
(	O
D	O
,	O
Q	O
)	O
=	O
L	O
Q	O
i=1	O
IDF	O
(	O
q	O
i	O
)	O
(	O
k	O
+	O
1	O
)	O
TF	O
(	O
q	O
i	O
,	O
D	O
)	O
k	O
1	O
−	O
b	O
+	O
b	O
L	O
D	O
Lavg	O
+	O
TF	O
(	O
q	O
i	O
,	O
D	O
)	O
,	O
where	O
the	O
query	O
sentence	O
Q	O
is	O
a	O
sequence	O
of	O
L	O
Q	O
subwords	O
{	O
q	O
i	O
}	O
L	O
Q	O
i=1	O
,	O
IDF	O
(	O
q	O
i	O
)	O
is	O
the	O
Inverse	O
Docu	O
-	O
ment	O
Frequency	O
for	O
q	O
i	O
in	O
the	O
corpus	O
C	O
,	O
TF	O
(	O
q	O
i	O
,	O
D	O
)	O
is	O
the	O
Term	O
Frequency	O
for	O
q	O
i	O
in	O
the	O
sentence	O
D	O
,	O
L	O
D	O
is	O
the	O
length	O
of	O
the	O
sentence	O
D	O
,	O
L	O
avg	O
is	O
the	O
average	O
length	O
of	O
the	O
corpus	O
C	O
,	O
k	O
and	O
b	O
are	O
hyperparameters	O
,	O
which	O
are	O
set	O
as	O
1.5	O
and	O
0.75	O
,	O
respectively	O
.	O
Based	O
on	O
the	O
BM25	O
score	O
,	O
we	O
calculate	O
the	O
similarity	O
between	O
a	O
source	O
test	O
sentence	O
(	O
as	O
the	O
query	O
sentence	O
)	O
and	O
the	O
source	O
sentences	O
in	O
the	O
training	O
set	O
to	O
obtain	O
the	O
top	O
500	O
sentences	O
.	O
After	O
performing	O
the	O
selection	O
for	O
all	O
the	O
source	O
test	O
sentences	O
,	O
we	O
merge	O
them	O
and	O
remove	O
duplicates	O
to	O
obtain	O
the	O
set	O
for	O
finetuning	O
.	O
3	O
Experimental	O
Setup	O

We	O
collected	O
allowed	O
data	O
for	O
the	O
involved	O
languages	O
and	O
followed	O
the	O
same	O
preprocessing	O
pipeline	O
of	O
punctuation	O
normalization	O
and	O
tokenization	O
,	O
using	O
scripts	O
from	O
Moses	O
2	O
.	O
The	O
English	O
monolingual	O
data	O
came	O
from	O
the	O
English	O
original	O
side	O
of	O
ru	O
-	O
en	O
back	O
-	O
translated	O
news	O
3	O
,	O
but	O
its	O
automatic	O
translation	O
to	O
Russian	O
was	O
discarded	O
.	O
The	O
provided	O
Chuvash	O
-	O
Russian	O
dictionary	O
was	O
not	O
used	O
.	O
Each	O
language	O
was	O
encoded	O
with	O
byte	B-MethodName
pair	I-MethodName
encoding	I-MethodName
(	O
BPE	B-MethodName
)	O
(	O
Sennrich	O
et	O
al	O
,	O
2016b	O
)	O
.	O
The	O
BPE	B-MethodName
codes	O
and	O
vocabularies	O
were	O
learned	O
on	O
each	O
language	O
's	O
monolingual	O
data	O
,	O
and	O
then	O
used	O
to	O
segment	O
parallel	O
data	O
.	O
We	O
used	O
32k	O
merge	O
operations	O
for	O
all	O
languages	O
.	O
After	O
BPE	B-MethodName
segmentation	O
,	O
we	O
discarded	O
sentences	O
with	O
more	O
than	O
128	O
subwords	O
,	O
and	O
cleaned	O
parallel	O
data	O
with	O
length	O
ratio	O
1.5	O
.	O
Training	O
data	O
statistics	O
is	O
provided	O
in	O
Table	O
1	O
.	O
Note	O
that	O
we	O
experimented	O
with	O
Kazakh	O
(	O
kk	O
)	O
data	O
(	O
Section	O
4.1	O
)	O
,	O
but	O
did	O
not	O
use	O
it	O
for	O
our	O
final	O
submission	O
.	O
Evaluation	O
on	O
test	O
sets	O
is	O
given	O
by	O
SacreBLEU	B-MetricName
4	O
(	O
Post	O
,	O
2018	O
)	O
,	O
after	O
BPE	B-MethodName
removal	O
and	O
detokenization	O
.	O

We	O
use	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
as	O
our	O
translation	O
model	O
,	O
but	O
with	O
slight	O
modifications	O
that	O
follow	O
the	O
implementation	O
of	O
BERT	B-MethodName
5	O
.	O
The	O
absolute	O
position	O
embeddings	O
are	O
also	O
learned	O
as	O
in	O
BERT	B-MethodName
.	O
The	O
encoder	O
and	O
decoder	O
embeddings	O
are	O
independent	O
because	O
each	O
language	O
manages	O
its	O
own	O
vocabulary	O
,	O
but	O
we	O
tie	O
the	O
decoder	O
input	O
and	O
output	O
embeddings	O
(	O
Press	O
and	O
Wolf	O
,	O
2017	O
)	O
.	O
We	O
apply	O
dropout	O
with	O
probability	O
0.1	O
.	O
We	O
use	O
LazyAdam	O
as	O
the	O
optimizer	B-HyperparameterName
.	O
Learning	B-HyperparameterName
rate	I-HyperparameterName
warms	O
up	O
for	O
16	O
,	O
000	O
steps	O
and	O
then	O
follows	O
inverse	O
square	O
root	O
decay	O
.	O
The	O
peak	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
5	O
×	O
10	O
−4	O
for	O
parent	O
translation	O
models	O
,	O
and	O
1	O
×	O
10	O
−4	O
for	O
child	O
translation	O
models	O
.	O
Early	B-MethodName
stopping	I-MethodName
occurs	O
when	O
the	O
validation	O
BLEU	B-MetricName
does	O
not	O
improve	O
for	O
10	O
checkpoints	O
.	O
We	O
set	O
checkpoint	O
frequency	O
to	O
2	O
,	O
000	O
updates	O
for	O
parent	O
translation	O
models	O
and	O
1	O
,	O
000	O
updates	O
for	O
child	O
translation	O
models	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
6	O
,	O
144	O
tokens	O
per	O
GPU	O
and	O
8	O
NVIDIA	O
V100	O
GPUs	O
are	O
used	O
.	O
Hyperparameters	O
for	O
BERT	B-MethodName
are	O
the	O
same	O
as	O
in	O
the	O
original	O
paper	O
(	O
Zhang	O
et	O
al	O
,	O
2021	O
)	O
.	O
For	O
selected	O
finetuning	O
,	O
we	O
use	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
as	O
the	O
optimizer	B-HyperparameterName
,	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
1	O
×	O
10	O
−5	O
.	O
We	O
finetune	O
for	O
10	O
,	O
000	O
updates	O
,	O
and	O
save	O
a	O
checkpoint	O
every	O
100	O
updates	O
.	O
The	O
checkpoint	O
with	O
the	O
highest	O
validation	O
BLEU	B-MetricName
is	O
kept	O
.	O

We	O
ran	O
five	O
iterations	O
of	O
iterative	O
back	O
-	O
translation	O
.	O
Results	O
are	O
shown	O
in	O
Table	O
5	O
.	O
The	O
best	O
BLEU	B-MetricName
scores	O
are	O
attained	O
with	O
two	O
or	O
three	O
iterations	O
.	O
Another	O
observation	O
is	O
that	O
iterative	O
back	O
-	O
translation	O
brings	O
larger	O
improvements	O
for	O
chv	O
ru	O
and	O
hsb	O
de	O
than	O
ru	O
chv	O
and	O
de	O
hsb	O
.	O
This	O
is	O
probably	O
because	O
the	O
monolingual	O
data	O
for	O
chv	O
and	O
hsb	O
are	O
small	O
in	O
quantity	O
.	O

We	O
validate	O
the	O
effectiveness	O
of	O
ensemble	O
on	O
hsb	O
de	O
and	O
de	O
hsb	O
,	O
by	O
performing	O
ensemble	O
decoding	O
from	O
the	O
five	O
models	O
from	O
iterative	O
back	O
-	O
translation	O
.	O
Results	O
in	O
Table	O
7	O
demonstrate	O
that	O
ensemble	O
gives	O
BLEU	B-MetricName
improvements	O
of	O
about	O
0.8	O
.	O

Baseline	O
(	O
Brun	O
and	O
Nikoulina	O
,	O
2018	O
)	O
-	O
38.10	O
TAS	O
-	O
LPM	B-MethodName
-	O
CRF	B-MethodName
(	O
Wan	O
et	O
al	O
,	O
2020	O
)	O
54.76	O
64.66	O
TAS	O
-	O
SW	O
-	O
CRF	B-MethodName
(	O
Wan	O
et	O
al	O
,	O
2020	O
)	O
57.51	O
65.89	O
TAS	O
-	O
SW	O
-	O
TO	O
(	O
Wan	O
et	O
al	O
,	O
2020	O
)	O
58.09	O
65.44	O
all	O
experiments	O
.	O
T5	B-MethodName
closely	O
follows	O
the	O
original	O
encoder	O
-	O
decoder	O
architecture	O
of	O
the	O
Transformer	B-MethodName
model	O
,	O
with	O
some	O
slight	O
differences	O
such	O
as	O
different	O
position	O
embedding	O
schemes	O
.	O
Therefore	O
,	O
the	O
encoder	O
and	O
decoder	O
of	O
it	O
have	O
similar	O
parameter	O
size	O
as	O
the	O
BERT	B-MethodName
-	O
BASE	B-MethodName
model	O
.	O
For	O
all	O
tasks	O
,	O
we	O
use	O
similar	O
experimental	O
settings	O
for	O
simplicity	O
:	O
we	O
train	O
the	O
model	O
with	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	O
and	O
accumulate	O
gradients	O
every	O
two	O
batches	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
be	O
3e	O
-	O
4	O
.	O
The	O
model	O
is	O
trained	O
up	O
to	O
20	O
epochs	O
for	O
the	O
AOPE	O
,	O
UABSA	O
,	O
and	O
ASTE	O
task	O
and	O
30	O
epochs	O
for	O
the	O
TASD	O
task	O
.	O

The	O
main	O
results	O
for	O
the	O
AOPE	O
,	O
UABSA	O
,	O
ASTE	O
,	O
TASD	O
task	O
are	O
reported	O
in	O
Tables	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
respectively	O
.	O
For	O
our	O
proposed	O
GAS	O
framework	O
,	O
we	O
also	O
present	O
the	O
raw	O
results	O
without	O
the	O
proposed	O
prediction	O
normalization	O
strategy	O
(	O
with	O
the	O
suffix	O
"	O
-	O
R	O
"	O
)	O
.	O
All	O
results	O
are	O
the	O
average	B-MetricName
F1	I-MetricName
scores	O
across	O
5	O
runs	O
with	O
different	O
random	O
seeds	B-DatasetName
.	O
It	O
is	O
noticeable	O
that	O
our	O
proposed	O
methods	O
,	O
based	O
on	O
either	O
annotation	O
-	O
style	O
or	O
extraction	O
-	O
style	O
modeling	O
,	O
establish	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
almost	O
all	O
cases	O
.	O
The	O
only	O
exception	O
is	O
on	O
the	O
Rest15	O
dataset	O
for	O
the	O
AOPE	O
task	O
,	O
our	O
method	O
is	O
still	O
on	O
par	O
with	O
the	O
previous	O
best	O
performance	O
.	O
It	O
shows	O
that	O
tackling	O
various	O
ABSA	O
tasks	O
with	O
the	O
proposed	O
unified	O
generative	O
method	O
is	O
an	O
effective	O
solution	O
.	O
Moreover	O
,	O
we	O
can	O
see	O
that	O
our	O
method	O
performs	O
especially	O
well	O
on	O
the	O
ASTE	O
and	O
TASD	O
tasks	O
,	O
the	O
proposed	O
extraction	O
-	O
style	O
method	O
outperforms	O
the	O
previous	O
best	O
models	O
by	O
7.6	O
and	O
3.7	O
average	B-MetricName
F1	I-MetricName
scores	O
(	O
across	O
different	O
datasets	O
)	O
on	O
them	O
respectively	O
.	O
It	O
implies	O
that	O
incorporating	O
the	O
label	O
semantics	O
and	O
appropriately	O
modeling	O
the	O
interactions	O
among	O
those	O
sentiment	O
elements	O
are	O
essential	O
for	O
tackling	O
complex	O
ABSA	O
problems	O
.	O

State	O
-	O
of	O
-	O
the	O
-	O
art	O
dialogue	O
models	O
still	O
often	O
stumble	O
with	O
regards	O
to	O
factual	O
accuracy	B-MetricName
and	O
self	O
-	O
contradiction	O
.	O
Anecdotally	O
,	O
they	O
have	O
been	O
observed	O
to	O
fail	O
to	O
maintain	O
character	O
identity	O
throughout	O
discourse	O
;	O
and	O
more	O
specifically	O
,	O
may	O
take	O
on	O
the	O
role	O
of	O
their	O
interlocutor	O
.	O
In	O
this	O
work	O
we	O
formalize	O
and	O
quantify	O
this	O
deficiency	O
,	O
and	O
show	O
experimentally	O
through	O
human	O
evaluations	O
that	O
this	O
is	O
indeed	O
a	O
problem	O
.	O
In	O
contrast	O
,	O
we	O
show	O
that	O
discriminative	O
models	O
trained	O
specifically	O
to	O
recognize	O
who	O
is	O
speaking	O
can	O
perform	O
well	O
;	O
and	O
further	O
,	O
these	O
can	O
be	O
used	O
as	O
automated	O
metrics	O
.	O
Finally	O
,	O
we	O
evaluate	O
a	O
wide	O
variety	O
of	O
mitigation	O
methods	O
,	O
including	O
changes	O
to	O
model	O
architecture	O
,	O
training	O
protocol	O
,	O
and	O
decoding	O
strategy	O
.	O
Our	O
best	O
models	O
reduce	O
mistaken	O
identity	O
issues	O
by	O
nearly	O
65	O
%	O
according	O
to	O
human	O
annotators	O
,	O
while	O
simultaneously	O
improving	O
engagingness	O
.	O
Despite	O
these	O
results	O
,	O
we	O
find	O
that	O
maintaining	O
character	O
identity	O
still	O
remains	O
a	O
challenging	O
problem	O
.	O

The	O
exchange	O
of	O
stories	O
from	O
one	O
's	O
past	O
,	O
or	O
descriptions	O
of	O
activities	O
in	O
one	O
's	O
present	O
,	O
are	O
a	O
fundamental	O
part	O
of	O
human	O
discourse	O
.	O
Trustworthy	O
human	O
conversationalists	O
keep	O
their	O
stories	O
roughly	O
straight	O
within	O
a	O
conversation	O
.	O
An	O
interlocutor	O
taking	O
on	O
your	O
own	O
stories	O
and	O
persona	O
as	O
theirs	O
is	O
especially	O
jarring	O
and	O
unnatural	O
.	O
However	O
,	O
despite	O
the	O
improvements	O
in	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
open	O
-	O
domain	O
dialogue	O
modeling	O
,	O
both	O
in	O
terms	O
of	O
distributional	O
accuracy	B-MetricName
metrics	O
like	O
perplexity	B-MetricName
,	O
and	O
subjectively	O
in	O
terms	O
of	O
human	O
judgements	O
(	O
Adiwardana	O
et	O
al	O
,	O
2020	O
;	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
,	O
interactions	O
with	O
those	O
agents	O
reveal	O
that	O
they	O
can	O
not	O
keep	O
their	O
stories	O
straight	O
.	O
In	O
particular	O
,	O
they	O
are	O
likely	O
to	O
take	O
on	O
the	O
role	O
of	O
their	O
interlocutor	O
;	O
for	O
example	O
,	O
if	O
an	O
agent	B-DatasetName
's	O
partner	O
says	O
they	O
are	O
a	O
software	O
engineer	O
,	O
the	O
agent	B-DatasetName
is	O
likely	O
to	O
say	O
it	O
is	O
a	O
software	O
engineer	O
too	O
(	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
,	O
or	O
worse	O
,	O
appropriate	O
their	O
partners	O
just	O
told	O
tale	O
of	O
a	O
trip	O
to	O
NAACL	O
as	O
their	O
own	O
.	O
Some	O
(	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
and	O
fine	O
-	O
tuned	O
on	O
LIGHT	O
(	O
Urbanek	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
bold	O
words	O
in	O
red	O
highlight	O
the	O
model	O
mistaking	O
its	O
identity	O
for	O
its	O
partner	O
's	O
.	O
(	O
Top	O
)	O
The	O
model	O
believes	O
it	O
is	O
a	O
thief	O
,	O
rather	O
than	O
a	O
guest	O
.	O
(	O
Bottom	O
)	O
The	O
model	O
believes	O
it	O
is	O
a	O
hunter	O
rather	O
than	O
a	O
helper	O
.	O
Token	O
probabilities	O
are	O
given	O
at	O
the	O
position	O
of	O
the	O
mistake	O
for	O
the	O
two	O
names	O
.	O
example	O
failure	O
cases	O
are	O
given	O
in	O
Table	O
1	O
,	O
where	O
models	O
incorrectly	O
take	O
on	O
the	O
name	O
,	O
role	O
or	O
activities	O
of	O
their	O
partner	O
instead	O
of	O
their	O
assigned	O
role	O
.	O
These	O
failures	O
are	O
related	O
to	O
the	O
general	O
problems	O
of	O
repetition	O
in	O
language	O
models	O
(	O
Holtzman	O
et	O
al	O
,	O
2020	O
)	O
,	O
the	O
weak	O
influence	O
of	O
word	O
order	O
(	O
Sinha	O
et	O
al	O
,	O
2021	O
)	O
and	O
inability	O
to	O
avoid	O
contradictions	O
(	O
Nie	O
et	O
al	O
,	O
2021	O
)	O
.	O
In	O
this	O
work	O
we	O
formalize	O
and	O
quantify	O
this	O
behavior	O
,	O
show	O
that	O
to	O
some	O
extent	O
it	O
can	O
be	O
detected	O
automatically	O
with	O
a	O
specifically	O
trained	O
classifier	O
,	O
and	O
then	O
study	O
a	O
wide	O
variety	O
of	O
mitigations	O
.	O
These	O
include	O
multi	O
-	O
objective	O
training	O
,	O
unlikelihood	O
training	O
,	O
classifier	O
-	O
assisted	O
re	O
-	O
ranking	O
based	O
generation	O
,	O
and	O
several	O
forms	O
modifying	O
the	O
attention	O
mechanisms	O
of	O
the	O
decoder	O
in	O
a	O
sequence	B-MethodName
to	I-MethodName
sequence	I-MethodName
model	O
.	O
Our	O
best	O
methods	O
can	O
reduce	O
mistaken	O
identity	O
issues	O
by	O
65	O
%	O
,	O
while	O
simultaneously	O
improving	O
inconversation	O
engagingness	O
;	O
indeed	O
,	O
our	O
models	O
that	O
can	O
stick	O
to	O
their	O
role	O
in	O
conversation	O
are	O
judged	O
by	O
humans	O
to	O
be	O
significantly	O
more	O
engaging	O
than	O
their	O
baseline	O
counterparts	O
.	O
Despite	O
these	O
advances	O
,	O
we	O
find	O
that	O
there	O
is	O
still	O
considerable	O
space	O
to	O
improve	O
these	O
results	O
further	O
in	O
future	O
work	O
.	O
We	O
make	O
publicly	O
available	O
both	O
our	O
trained	O
models	O
and	O
code	O
to	O
reproduce	O
results	O
1	O
.	O

We	O
first	O
define	O
a	O
metric	O
,	O
role	O
-	O
playing	O
accuracy	B-MetricName
(	O
RPA	O
)	O
,	O
to	O
denote	O
how	O
often	O
a	O
model	O
's	O
responses	O
are	O
"	O
in	O
-	O
character	O
"	O
;	O
by	O
this	O
,	O
we	O
mean	O
how	O
often	O
the	O
model	O
's	O
response	O
could	O
feasibly	O
be	O
said	O
by	O
their	O
character	O
,	O
given	O
their	O
assigned	O
character	O
identity	O
.	O
Measuring	O
RPA	O
is	O
a	O
non	O
-	O
trivial	O
task	O
for	O
a	O
variety	O
of	O
reasons	O
.	O
First	O
,	O
some	O
conversations	O
involve	O
pairs	O
that	O
can	O
reasonably	O
say	O
similar	O
things	O
(	O
priest	O
vs.	O
priestess	O
,	O
man	O
vs.	O
woman	O
,	O
wizard	O
vs.	O
witch	O
)	O
.	O
Second	O
,	O
opening	O
lines	O
are	O
often	O
more	O
generic	O
(	O
"	O
hello	O
"	O
,	O
"	O
how	O
fare	O
your	O
travels	O
today	O
"	O
)	O
,	O
so	O
either	O
character	O
can	O
say	O
it	O
in	O
conversation	O
.	O
The	O
third	O
reason	O
stems	O
from	O
the	O
data	O
that	O
we	O
study	O
;	O
we	O
are	O
relying	O
on	O
crowdsourced	O
data	O
in	O
which	O
humans	O
are	O
required	O
to	O
portray	O
their	O
characters	O
.	O
Some	O
crowdworkers	O
may	O
be	O
better	O
than	O
others	O
,	O
and	O
there	O
may	O
be	O
some	O
noise	O
in	O
the	O
dataset	O
in	O
which	O
,	O
e.g.	O
,	O
a	O
horse	O
may	O
proclaim	O
its	O
love	O
for	O
a	O
queen	O
,	O
or	O
a	O
knight	O
may	O
discuss	O
at	O
length	O
the	O
kingdom	O
's	O
tax	O
collecting	O
.	O
Given	O
the	O
difficulties	O
above	O
,	O
our	O
primary	O
measure	O
of	O
RPA	O
involves	O
human	O
annotation	O
of	O
model	O
responses	O
,	O
specifically	O
evaluating	O
whether	O
a	O
candidate	O
response	O
fits	O
a	O
given	O
model	O
's	O
character	O
.	O
We	O
thus	O
have	O
human	O
crowdworkers	O
chat	O
with	O
each	O
model	O
in	O
a	O
LIGHT	O
setting	O
;	O
each	O
is	O
given	O
a	O
character	O
and	O
asked	O
to	O
role	O
-	O
play	O
,	O
while	O
the	O
human	O
an	O
-	O
notates	O
each	O
model	O
response	O
,	O
determining	O
whether	O
the	O
model	O
is	O
in	O
character	O
:	O
we	O
denote	O
this	O
metric	O
as	O
"	O
Mistaken	O
Identity	O
"	O
in	O
our	O
experiments	O
,	O
and	O
other	O
utterance	O
-	O
level	O
annotations	O
are	O
collected	O
.	O
Further	O
details	O
regarding	O
human	O
evaluation	O
are	O
outlined	O
in	O
Section	O
4.7	O
.	O
Despite	O
the	O
efficacy	O
of	O
human	O
evaluation	O
,	O
it	O
is	O
both	O
costly	O
and	O
slow	O
;	O
as	O
a	O
proxy	O
,	O
we	O
thus	O
train	O
models	O
specifically	O
designed	O
to	O
identify	O
whether	O
a	O
candidate	O
response	O
from	O
a	O
model	O
fits	O
the	O
model	O
's	O
role	O
,	O
and	O
denote	O
these	O
as	O
"	O
RPA	O
Classifiers	O
"	O
.	O
We	O
employ	O
poly	O
-	O
encoder	O
transformers	O
(	O
Humeau	O
et	O
al	O
,	O
2020	O
)	O
to	O
learn	O
this	O
metric	O
,	O
and	O
structure	O
the	O
task	O
as	O
a	O
ranking	O
one	O
;	O
the	O
model	O
receives	O
the	O
LIGHT	O
setting	O
and	O
prior	O
utterances	O
of	O
dialogue	O
as	O
input	O
,	O
as	O
well	O
as	O
the	O
response	O
currently	O
under	O
consideration	O
,	O
and	O
the	O
model	O
must	O
choose	O
the	O
correct	O
character	O
from	O
a	O
fixed	O
set	O
of	O
candidates	O
.	O
We	O
also	O
explore	O
RPA	O
classifiers	O
trained	O
on	O
all	O
partially	O
complete	O
sequences	O
of	O
labels	O
,	O
such	O
that	O
the	O
classifiers	O
can	O
determine	O
the	O
character	O
speaking	O
without	O
requiring	O
the	O
full	O
utterance	O
;	O
we	O
call	O
these	O
left	O
-	O
to	O
-	O
right	O
(	O
LTR	O
)	O
RPA	O
classifiers	O
.	O
Further	O
details	O
about	O
how	O
our	O
RPA	O
classifiers	O
are	O
built	O
are	O
given	O
in	O
Appendix	O
B.	O

We	O
can	O
employ	O
an	O
RPA	O
classifier	O
in	O
response	B-TaskName
generation	I-TaskName
by	O
using	O
it	O
to	O
rank	O
candidate	O
model	O
outputs	O
.	O
Utterance	O
Re	O
-	O
ranking	O
:	O
Given	O
a	O
set	O
of	O
candidate	O
responses	O
,	O
the	O
RPA	O
classifier	O
can	O
re	O
-	O
score	O
the	O
set	O
and	O
return	O
the	O
response	O
yielding	O
the	O
highest	O
probability	O
of	O
staying	O
in	O
character	O
(	O
according	O
to	O
the	O
RPA	O
score	O
on	O
the	O
complete	O
candidate	O
generations	O
)	O
.	O
The	O
dialogue	O
models	O
employ	O
beam	O
-	O
search	O
to	O
generate	O
responses	O
,	O
and	O
the	O
candidates	O
for	O
re	O
-	O
ranking	O
are	O
the	O
beams	O
within	O
beam	O
-	O
search	O
.	O
We	O
also	O
try	O
nucleus	O
sampling	O
(	O
Holtzman	O
et	O
al	O
,	O
2020	O
)	O
and	O
delayed	O
beam	O
-	O
search	O
(	O
Massarelli	O
et	O
al	O
,	O
2020	O
)	O
to	O
see	O
whether	O
more	O
diverse	O
candidates	O
have	O
any	O
effect	O
.	O
Partial	O
And	O
Complete	O
Efficient	O
Re	O
-	O
ranking	O
(	O
PACER	O
)	O
:	O
Re	O
-	O
ranking	O
only	O
the	O
final	O
beam	O
candidates	O
may	O
be	O
suboptimal	O
because	O
it	O
is	O
well	O
known	O
that	O
those	O
candidates	O
are	O
not	O
very	O
diverse	O
(	O
Kulikov	O
et	O
al	O
,	O
2019	O
)	O
,	O
meaning	O
there	O
may	O
not	O
be	O
any	O
good	O
candidates	O
to	O
choose	O
from	O
in	O
this	O
final	O
set	O
.	O
In	O
order	O
to	O
generate	O
utterances	O
that	O
agree	O
with	O
our	O
classifiers	O
,	O
a	O
possible	O
improvement	O
is	O
to	O
generate	O
the	O
utterance	O
such	O
that	O
partial	O
generations	O
also	O
agree	O
with	O
the	O
classifier	O
when	O
generating	O
left	O
-	O
to	O
-	O
right	O
,	O
ensuring	O
that	O
good	O
candidates	O
are	O
surfaced	O
.	O
With	O
access	O
to	O
LTR	O
RPA	O
classifiers	O
,	O
we	O
can	O
apply	O
re	O
-	O
ranking	O
to	O
partial	O
sequences	O
.	O
Unfortunately	O
,	O
re	O
-	O
ranking	O
at	O
every	O
step	O
of	O
beam	O
search	O
,	O
for	O
every	O
token	O
,	O
requires	O
significant	O
computation	O
,	O
such	O
as	O
in	O
the	O
recent	O
FUDGE	O
method	O
(	O
Yang	O
and	O
Klein	O
,	O
2021	O
)	O
.	O
FUDGE	O
re	O
-	O
scores	O
tokens	O
at	O
each	O
decoding	O
step	O
by	O
multiplying	O
the	O
classifier	O
probability	O
with	O
each	O
token	O
probability	O
,	O
and	O
renormalizing	O
,	O
which	O
is	O
used	O
for	O
control	O
tasks	O
with	O
lightweight	O
classifiers	O
in	O
order	O
to	O
be	O
tractable	O
.	O
In	O
our	O
proposed	O
approach	O
,	O
called	O
PACER	O
,	O
we	O
re	O
-	O
score	O
candidate	O
tokens	O
,	O
for	O
each	O
beam	O
,	O
according	O
to	O
the	O
probability	O
that	O
their	O
inclusion	O
yields	O
the	O
appropriate	O
character	O
classification	O
,	O
and	O
then	O
finally	O
re	O
-	O
rank	O
the	O
complete	O
candidate	O
beams	O
.	O
To	O
make	O
this	O
efficient	O
,	O
we	O
crucially	O
score	O
only	O
a	O
small	O
proportion	O
of	O
decoding	O
steps	O
(	O
e.g.	O
,	O
5	O
%	O
of	O
token	O
positions	O
)	O
as	O
well	O
as	O
for	O
only	O
a	O
few	O
candidate	O
rescored	O
tokens	O
(	O
e.g.	O
,	O
top	O
10	O
only	O
)	O
.	O
We	O
can	O
control	O
these	O
hyperparameters	O
to	O
explore	O
the	O
speed	O
vs.	O
accuracy	B-MetricName
trade	O
-	O
off	O
.	O

Table	O
4	O
gives	O
results	O
for	O
RPA	O
-	O
based	O
re	O
-	O
ranking	O
of	O
generation	O
models	O
.	O
Automated	O
results	O
show	O
a	O
slight	O
bump	O
in	O
F1	B-MetricName
on	O
the	O
LIGHT	O
valid	O
set	O
,	O
and	O
indeed	O
a	O
bump	O
in	O
RPA	O
.	O
Including	O
the	O
intra	O
-	O
generation	O
re	O
-	O
ranking	O
with	O
PACER	O
yields	O
an	O
even	O
higher	O
RPA	O
score	O
.	O
Table	O
3	O
contains	O
the	O
results	O
of	O
varying	O
the	O
candidate	O
tokens	O
re	O
-	O
ranked	O
per	O
intra	O
-	O
generation	O
step	O
(	O
#	O
Toks	O
)	O
and	O
number	O
of	O
partial	O
re	O
-	O
ranking	O
steps	O
(	O
Freq	O
)	O
,	O
both	O
in	O
terms	O
of	O
generation	O
metrics	O
/	O
RPA	O
and	O
relative	O
computational	O
cost	O
compared	O
to	O
reranking	O
.	O
Increasing	O
#	O
of	O
toks	O
or	O
increasing	O
the	O
frequency	O
can	O
lead	O
to	O
improved	O
F1	B-MetricName
and	O
RPA	O
,	O
but	O
with	O
significant	O
latency	O
increase	O
for	O
too	O
high	O
values	O
(	O
e.g.	O
over	O
11x	O
when	O
applying	O
re	O
-	O
ranking	O
for	O
every	O
partial	O
step	O
using	O
the	O
top	O
10	O
tokens	O
each	O
time	O
)	O
.	O
Applying	O
both	O
partial	O
and	O
final	O
complete	O
ranking	O
helps	O
performance	O
.	O
Note	O
that	O
re	O
-	O
ranker	O
models	O
use	O
the	O
same	O
model	O
to	O
re	O
-	O
rank	O
that	O
is	O
being	O
used	O
to	O
measure	O
RPA	O
afterwards	O
,	O
making	O
that	O
metric	O
biased	O
.	O
Hence	O
,	O
human	O
evaluations	O
are	O
required	O
for	O
this	O
model	O
,	O
which	O
will	O
be	O
detailed	O
in	O
Section	O
4.7	O
,	O
and	O
which	O
will	O
indicate	O
that	O
re	O
-	O
ranking	O
does	O
in	O
fact	O
help	O
.	O

Results	O
of	O
unlikelihood	O
(	O
UL	O
)	O
training	O
are	O
also	O
given	O
in	O
Table	O
4	O
.	O
We	O
apply	O
UL	O
loss	B-MetricName
to	O
the	O
128	O
-	O
truncation	O
model	O
in	O
two	O
different	O
ways	O
:	O
(	O
1	O
)	O
Top	O
-	O
1	O
:	O
apply	O
the	O
loss	B-MetricName
on	O
the	O
token	O
that	O
yields	O
the	O
most	O
incorrect	O
partial	O
sequence	O
RPA	O
classification	O
;	O
(	O
2	O
)	O
All	O
:	O
apply	O
the	O
loss	B-MetricName
to	O
all	O
tokens	O
that	O
yield	O
an	O
incorrect	O
RPA	O
classification	O
on	O
partial	O
sequences	O
.	O
The	O
RPA	O
UL	O
methods	O
suffer	O
compared	O
to	O
the	O
baselines	O
in	O
terms	O
of	O
PPL	O
and	O
F1	B-MetricName
,	O
yet	O
they	O
retain	O
similar	O
RPA	O
metrics	O
.	O
We	O
hypothesize	O
that	O
while	O
the	O
UL	O
loss	B-MetricName
can	O
adjust	O
the	O
model	O
to	O
refrain	O
from	O
generating	O
outof	O
-	O
character	O
responses	O
,	O
there	O
are	O
still	O
far	O
too	O
many	O
other	O
tokens	O
that	O
may	O
yield	O
similar	O
outcomes	O
that	O
are	O
not	O
penalized	O
.	O
Table	O
12	O
in	O
Appendix	O
D	O
includes	O
similar	O
results	O
with	O
the	O
1024	O
-	O
truncation	O
model	O
.	O

Multi	O
-	O
objective	O
training	O
results	O
are	O
in	O
Table	O
5	O
,	O
where	O
the	O
base	O
model	O
is	O
a	O
1024	O
-	O
truncation	O
model	O
.	O
We	O
measure	O
generation	O
metrics	O
in	O
terms	O
of	O
RPA	O
(	O
with	O
PPL	O
and	O
F1	B-MetricName
in	O
Table	O
13	O
in	O
Appendix	O
E	O
)	O
,	O
and	O
classification	O
metrics	O
in	O
terms	O
of	O
Hits@1/427	O
as	O
before	O
.	O
The	O
model	O
is	O
able	O
to	O
predict	O
the	O
appropriate	O
character	O
using	O
either	O
the	O
decoder	O
outputs	O
or	O
the	O
en	O
-	O
coder+decoder	O
outputs	O
.	O
hits@1	B-MetricName
for	O
the	O
best	O
model	O
)	O
,	O
this	O
does	O
not	O
translate	O
to	O
substantial	O
RPA	O
improvements	O
over	O
the	O
baseline	O
.	O

We	O
show	O
results	O
for	O
the	O
automated	O
grounding	O
of	O
expanded	O
attention	O
in	O
Table	O
7	O
.	O
Attempting	O
to	O
use	O
the	O
decoder	O
attention	O
weights	O
to	O
select	O
expanded	O
attention	O
context	O
yields	O
no	O
additional	O
benefits	O
,	O
which	O
is	O
not	O
surprising	O
:	O
if	O
the	O
model	O
could	O
identify	O
the	O
pertinent	O
components	O
of	O
the	O
input	O
beforehand	O
,	O
it	O
would	O
not	O
require	O
a	O
reattention	O
.	O
The	O
trainable	O
mask	O
does	O
not	O
yield	O
any	O
benefits	O
either	O
.	O
However	O
,	O
using	O
the	O
RPA	O
classifier	O
attention	O
weights	O
to	O
inform	O
the	O
model	O
which	O
tokens	O
to	O
re	O
-	O
attend	O
to	O
yields	O
improved	O
performance	O
across	O
all	O
three	O
metrics	O
compared	O
to	O
the	O
baseline	O
,	O
and	O
PPL	O
is	O
nearly	O
the	O
same	O
as	O
profile	O
grounding	O
(	O
12.19	O
vs.	O
12.18	O
)	O
,	O
while	O
RPA	O
trails	O
slightly	O
behind	O
(	O
91.11	O
vs.	O
91.79	O
)	O
.	O
We	O
also	O
include	O
the	O
usage	O
of	O
the	O
bottom	O
-	O
k	O
tokens	O
from	O
the	O
classifier	O
weights	O
to	O
emphasize	O
that	O
there	O
is	O
indeed	O
signal	O
from	O
the	O
top	O
-	O
k	O
,	O
as	O
using	O
the	O
bottom	O
tokens	O
does	O
not	O
help	O
.	O
Automated	O
Grounding	O
+	O
Multi	O
-	O
Objective	O
Table	O
5	O
shows	O
that	O
combining	O
automated	O
grounding	O
with	O
the	O
multi	O
-	O
objective	O
task	O
yields	O
higher	O
hits@1	B-MetricName
compared	O
to	O
not	O
using	O
the	O
trainable	O
mask	O
,	O
especially	O
in	O
the	O
first	O
stage	O
of	O
multi	O
-	O
objective	O
training	O
.	O
However	O
,	O
RPA	O
scores	O
are	O
only	O
fractionally	O
better	O
than	O
the	O
baseline	O
.	O
Appendix	O
E	O
includes	O
results	O
across	O
more	O
settings	O
(	O
see	O
Table	O
13	O
and	O
Table	O
14	O
)	O
.	O
Expanded	O
Attention	O
+	O
RPA	O
Re	O
-	O
ranking	O
The	O
expanded	O
attention	O
and	O
RPA	O
re	O
-	O
ranker	O
methods	O
can	O
also	O
both	O
be	O
applied	O
to	O
obtain	O
effective	O
models	O
.	O
Results	O
are	O
in	O
Table	O
4	O
;	O
indeed	O
,	O
the	O
combination	O
yields	O
the	O
highest	O
F1	B-MetricName
and	O
RPA	O
scores	O
.	O

We	O
further	O
explored	O
three	O
decoding	O
settings	O
:	O
standard	O
beam	O
-	O
search	O
,	O
delayed	O
beam	O
search	O
(	O
Massarelli	O
et	O
al	O
,	O
2020	O
)	O
and	O
nucleus	O
sampling	O
(	O
Holtzman	O
et	O
al	O
,	O
2020	O
)	O
,	O
both	O
in	O
a	O
re	O
-	O
ranking	O
setting	O
and	O
not	O
.	O
When	O
considering	O
performance	O
on	O
automated	O
metrics	O
(	O
provided	O
in	O
Table	O
20	O
in	O
the	O
Appendix	O
)	O
,	O
we	O
see	O
that	O
generation	O
settings	O
other	O
than	O
beam	O
search	O
,	O
when	O
using	O
a	O
re	O
-	O
ranker	O
,	O
yield	O
lower	O
F1	B-MetricName
scores	O
but	O
higher	O
RPA	O
scores	O
,	O
as	O
the	O
RPA	O
re	O
-	O
ranker	O
has	O
more	O
diversity	O
of	O
candidate	O
responses	O
from	O
which	O
to	O
choose	O
;	O
however	O
,	O
these	O
methods	O
perform	O
worse	O
in	O
human	O
evaluations	O
,	O
with	O
nucleus	O
sampling	O
reranking	O
yielding	O
far	O
more	O
problems	O
and	O
far	O
lower	O
engagingness	O
ratings	O
.	O
Qualitative	O
analysis	O
of	O
outputs	O
on	O
the	O
test	O
set	O
are	O
in	O
Appendix	O
J.1	O
.	O

The	O
RPA	O
classifier	O
models	O
are	O
trained	O
with	O
a	O
cross	O
-	O
entropy	O
loss	B-MetricName
over	O
the	O
correct	O
label	O
,	O
with	O
99	O
random	O
negatives	O
chosen	O
from	O
the	O
training	O
set	O
;	O
we	O
ensured	O
that	O
each	O
character	O
in	O
conversation	O
showed	O
up	O
in	O
the	O
set	O
of	O
candidate	O
labels	O
.	O
The	O
models	O
were	O
trained	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	O
on	O
4	O
32	O
GB	O
GPUs	O
,	O
with	O
early	B-MethodName
stopping	I-MethodName
on	O
the	O
validation	O
set	O
according	O
to	O
valid	O
accuracy	B-MetricName
.	O
We	O
used	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
weight	B-MethodName
decay	I-MethodName
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2019	O
)	O
,	O
sweeping	O
over	O
learning	O
rates	O
{	O
1e	O
−	O
5	O
,	O
5e	O
−	O
6	O
}	O
.	O
Generative	O
Models	O
All	O
variants	O
of	O
generative	O
models	O
were	O
trained	O
using	O
8	O
32	O
GB	O
GPUs	O
,	O
with	O
early	B-MethodName
stopping	I-MethodName
on	O
perplexity	B-MetricName
on	O
the	O
validation	O
set	O
.	O
We	O
used	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
,	O
sweeping	O
over	O
learning	O
rates	O
{	O
1e	O
−	O
5	O
,	O
7e	O
−	O
6	O
}	O
,	O
training	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	O
for	O
the	O
short	O
-	O
truncation	O
models	O
,	O
and	O
32	O
for	O
the	O
long	O
-	O
truncation	O
models	O
.	O
For	O
the	O
multiobjective	O
models	O
,	O
we	O
used	O
the	O
same	O
loss	B-MetricName
(	O
and	O
negative	O
-	O
sampling	O
)	O
setup	O
as	O
the	O
RPA	O
classifiers	O
for	O
the	O
character	O
accuracy	B-MetricName
objective	O
.	O
During	O
inference	O
,	O
unless	O
otherwise	O
specified	O
,	O
we	O
generated	O
using	O
beam	O
-	O
search	O
with	O
beam	O
size	O
of	O
10	O
,	O
enforcing	O
a	O
minimum	O
length	O
of	O
20	O
,	O
and	O
with	O
tri	O
-	O
gram	O
blocking	O
with	O
respect	O
to	O
both	O
the	O
context	O
and	O
the	O
current	O
generation	O
.	O

In	O
Table	O
12	O
,	O
we	O
compare	O
UL	O
models	O
across	O
different	O
truncation	O
lengths	O
;	O
the	O
same	O
story	O
applies	O
to	O
the	O
1024	O
-	O
truncation	O
models	O
.	O
We	O
additionally	O
include	O
a	O
third	O
method	O
,	O
Random	O
-	O
3	O
,	O
where	O
we	O
apply	O
the	O
loss	B-MetricName
randomly	O
to	O
3	O
tokens	O
that	O
yield	O
incorrect	O
RPA	O
classifications	O
.	O
This	O
method	O
performs	O
about	O
the	O
same	O
as	O
the	O
Top	O
-	O
1	O
method	O
,	O
but	O
the	O
RPA	O
is	O
lower	O
,	O
indicating	O
that	O
the	O
Top	O
-	O
1	O
method	O
at	O
least	O
is	O
providing	O
some	O
signal	O
.	O
E	O
Multi	O
-	O
Objective	O
:	O
Additional	O
Results	O

Table	O
13	O
displays	O
full	O
PPL	O
and	O
F1	B-MetricName
scores	O
corresponding	O
to	O
the	O
models	O
in	O
Table	O
5	O
.	O

We	O
experiment	O
with	O
various	O
generation	O
settings	O
,	O
with	O
or	O
without	O
re	O
-	O
rankers	O
;	O
results	O
are	O
in	O
Table	O
20	O
.	O
For	O
the	O
baseline	O
and	O
re	O
-	O
ranker	O
models	O
,	O
beam	O
search	O
yields	O
the	O
highest	O
F1	B-MetricName
scores	O
;	O
RPA	O
can	O
be	O
improved	O
with	O
the	O
other	O
inference	O
methods	O
when	O
combined	O
with	O
a	O
re	O
-	O
ranker	O
.	O
We	O
believe	O
this	O
may	O
be	O
due	O
to	O
the	O
higher	O
diversity	O
of	O
candidate	O
responses	O
generated	O
from	O
those	O
methods	O
.	O

We	O
analyze	O
the	O
correlation	O
between	O
human	O
annotations	O
and	O
the	O
automatic	O
metrics	O
collected	O
on	O
the	O
LIGHT	O
validation	O
set	O
,	O
as	O
shown	O
in	O
Figure	O
3	O
;	O
we	O
note	O
some	O
interesting	O
trends	O
:	O
Perplexity	B-MetricName
perplexity	B-MetricName
appears	O
to	O
be	O
positively	O
correlated	O
with	O
mistaken	O
identity	O
,	O
and	O
negatively	O
correlated	O
with	O
engagingness	O
.	O
So	O
,	O
perplexity	B-MetricName
is	O
a	O
good	O
indicator	O
of	O
how	O
fluent	O
and	O
engaging	O
the	O
model	O
is	O
in	O
conversation	O
,	O
and	O
can	O
indirectly	O
point	O
to	O
a	O
better	O
understanding	O
of	O
the	O
role	O
-	O
playing	O
task	O
.	O
An	O
important	O
note	O
is	O
that	O
we	O
only	O
tested	O
this	O
amongst	O
models	O
of	O
the	O
same	O
size	O
,	O
and	O
only	O
for	O
the	O
models	O
we	O
tested	O
,	O
so	O
it	O
is	O
not	O
clear	O
that	O
larger	O
models	O
will	O
necessarily	O
bring	O
improvements	O
.	O
F1	B-MetricName
F1	B-MetricName
word	O
overlap	O
is	O
positively	O
correlated	O
with	O
engagingness	O
as	O
well	O
,	O
so	O
F1	B-MetricName
may	O
be	O
a	O
good	O
proxy	O
of	O
model	O
performance	O
.	O
Correlation	O
with	O
mistaken	O
identity	O
is	O
negative	O
here	O
,	O
implying	O
that	O
better	O
F1	B-MetricName
corresponds	O
with	O
better	O
role	O
-	O
playing	O
ability	O
.	O
However	O
,	O
we	O
note	O
that	O
F1	B-MetricName
is	O
not	O
a	O
catch	O
-	O
all	O
metric	O
(	O
Liu	O
et	O
al	O
,	O
2016	O
)	O
.	O
RPA	O
RPA	O
appears	O
to	O
be	O
strongly	O
negatively	O
correlated	O
with	O
mistaken	O
identity	O
,	O
indicating	O
that	O
it	O
is	O
indeed	O
a	O
good	O
measure	O
of	O
the	O
model	O
's	O
ability	O
to	O
stay	O
in	O
character	O
.	O
It	O
is	O
weakly	O
negatively	O
correlated	O
with	O
the	O
other	O
issues	O
,	O
and	O
is	O
somewhat	O
positively	O
correlated	O
with	O
engagingness	O
as	O
well	O
.	O
These	O
correlations	O
give	O
us	O
confidence	O
that	O
our	O
RPA	O
classifiers	O
are	O
adequately	O
measuring	O
role	O
-	O
playing	O
ability	O
within	O
models	O
.	O

In	O
Figure	O
1	O
,	O
we	O
see	O
RPA	O
results	O
across	O
turns	O
of	O
conversation	O
for	O
a	O
wider	O
variety	O
of	O
models	O
.	O
Human	O
The	O
human	O
outputs	O
are	O
most	O
often	O
correct	O
on	O
the	O
first	O
turn	O
,	O
with	O
gradual	O
decay	O
of	O
accuracy	B-MetricName
throughout	O
the	O
conversation	O
(	O
according	O
to	O
RPA	O
)	O
.	O

The	O
vanilla	O
baseline	O
suffers	O
a	O
pretty	O
dramatic	O
drop	O
off	O
after	O
the	O
first	O
couple	O
of	O
turns	O
;	O
the	O
long	O
-	O
context	O
model	O
achieves	O
slightly	O
higher	O
character	O
accuracy	B-MetricName
overall	O
but	O
we	O
see	O
similar	O
drop	O
offs	O
farther	O
down	O
the	O
conversation	O
.	O
RPA	O
UL	O
The	O
unlikelihood	O
models	O
seem	O
to	O
recover	O
somewhat	O
in	O
the	O
initial	O
turns	O
of	O
conversation	O
,	O
however	O
later	O
turns	O
still	O
yield	O
sharp	O
drop	O
offs	O
in	O
RPA	O
.	O
Multi	O
-	O
objective	O
Similarly	O
to	O
the	O
UL	O
case	O
,	O
we	O
see	O
the	O
most	O
gains	O
in	O
initial	O
turns	O
compare	O
to	O
the	O
vanilla	O
baselines	O
;	O
however	O
,	O
we	O
see	O
even	O
more	O
dramatic	O
drop	O
offs	O
towards	O
the	O
end	O
of	O
the	O
conversation	O
.	O
Expanded	O
Attention	O
With	O
profile	O
grounding	O
,	O
we	O
see	O
near	O
-	O
human	O
performance	O
,	O
with	O
even	O
better	O
performance	O
towards	O
the	O
end	O
of	O
the	O
conversation	O
.	O
The	O
automatic	O
grounding	O
improves	O
over	O
the	O
baseline	O
but	O
is	O
slightly	O
worse	O
than	O
profile	O
grounding	O
.	O
Combining	O
automated	O
grounding	O
with	O
multi	O
-	O
objective	O
training	O
leads	O
to	O
some	O
benefits	O
in	O
earlier	O
turns	O
,	O
but	O
later	O
turns	O
still	O
suffer	O
.	O
Re	O
-	O
ranking	O
Although	O
we	O
're	O
using	O
the	O
same	O
RPA	O
classifier	O
to	O
both	O
re	O
-	O
ranker	O
and	O
score	O
the	O
model	O
outputs	O
,	O
it	O
is	O
still	O
interesting	O
to	O
examine	O
on	O
which	O
turns	O
the	O
re	O
-	O
ranker	O
benefits	O
the	O
model	O
the	O
most	O
.	O
We	O
see	O
in	O
the	O
last	O
set	O
of	O
graphs	O
that	O
beam	O
re	O
-	O
ranking	O
Figure	O
4	O
:	O
Vanilla	O
Attention	O
.	O
The	O
speaker	O
here	O
is	O
the	O
mermaid	O
,	O
whose	O
partner	O
is	O
a	O
sea	O
-	O
witch	O
.	O
The	O
last	O
utterance	O
from	O
the	O
sea	O
-	O
witch	O
is	O
,	O
"	O
What	O
are	O
you	O
doing	O
on	O
the	O
turquoise	O
shore	O
?	O
"	O
.	O
The	O
mermaid	O
responds	O
,	O
"	O
I	O
've	O
been	O
catching	O
waves	O
with	O
the	O
dolphins	O
all	O
morning	O
.	O
What	O
kind	O
of	O
victims	O
do	O
you	O
expect	O
to	O
find	O
in	O
a	O
tranquil	O
place	O
like	O
this	O
?	O
"	O
.	O
The	O
vanilla	O
model	O
spreads	O
its	O
attention	O
across	O
the	O
whole	O
context	O
;	O
blue	O
boxes	O
at	O
the	O
top	O
are	O
attentions	O
over	O
the	O
character	O
descriptions	O
,	O
while	O
the	O
bottom	O
box	O
is	O
attention	O
over	O
the	O
word	O
"	O
victims	O
"	O
.	O
Figure	O
5	O
:	O
Profile	O
Expanded	O
Attention	O
.	O
The	O
speaker	O
here	O
is	O
the	O
mermaid	O
,	O
whose	O
partner	O
is	O
a	O
sea	O
-	O
witch	O
.	O
The	O
last	O
utterance	O
from	O
the	O
sea	O
-	O
witch	O
is	O
,	O
"	O
What	O
are	O
you	O
doing	O
on	O
the	O
turquoise	O
shore	O
?	O
"	O
.	O
The	O
mermaid	O
responds	O
,	O
"	O
I	O
've	O
been	O
catching	O
waves	O
with	O
the	O
dolphins	O
all	O
morning	O
.	O
What	O
kind	O
of	O
victims	O
do	O
you	O
expect	O
to	O
find	O
in	O
a	O
tranquil	O
place	O
like	O
this	O
?	O
"	O
.	O
Left	O
original	O
attention	O
over	O
the	O
full	O
context	O
;	O
Right	O
expanded	O
attention	O
over	O
the	O
additional	O
context	O
.	O
The	O
top	O
two	O
boxes	O
are	O
the	O
partner	O
name	O
and	O
self	O
name	O
;	O
the	O
bottom	O
box	O
on	O
the	O
left	O
refers	O
to	O
"	O
victims	O
"	O
,	O
and	O
on	O
the	O
right	O
refers	O
to	O
the	O
"	O
dolphins	O
"	O
.	O

Simultaneous	O
Translation	B-TaskName
aims	O
to	O
translate	O
the	O
speech	O
of	O
a	O
source	O
language	O
into	O
a	O
target	O
language	O
as	O
quickly	O
as	O
possible	O
without	O
interrupting	O
the	O
speaker	O
.	O
Typically	O
,	O
a	O
simultaneous	O
translation	O
system	O
is	O
comprised	O
of	O
an	O
auto	O
-	O
speech	B-TaskName
-	I-TaskName
recognition	I-TaskName
(	O
ASR	O
)	O
model	O
and	O
a	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	O
)	O
model	O
.	O
The	O
ASR	O
model	O
transforms	O
the	O
audio	O
signal	O
into	O
the	O
text	O
of	O
source	O
language	O
and	O
the	O
MT	O
model	O
translates	O
the	O
source	O
text	O
into	O
the	O
target	O
language	O
.	O
Recent	O
studies	O
on	O
simultaneous	O
translation	O
(	O
Cho	O
and	O
Esipova	O
,	O
2016	O
;	O
Ma	O
et	O
al	O
,	O
2019	O
;	O
Arivazhagan	O
et	O
al	O
,	O
2019	O
)	O
focus	O
on	O
the	O
trade	O
-	O
off	O
between	O
translation	O
quality	O
and	O
latency	O
.	O
They	O
explore	O
a	O
policy	O
that	O
determines	O
when	O
to	O
begin	O
translating	O
with	O
the	O
input	O
of	O
a	O
stream	O
of	O
transcription	O
.	O
However	O
,	O
there	O
is	O
a	O
gap	O
between	O
transcription	O
and	O
ASR	O
that	O
some	O
ASR	O
model	O
does	O
n't	O
provide	O
punctuations	O
or	O
can	O
not	O
provide	O
accurate	O
punctuation	O
in	O
realtime	O
,	O
while	O
the	O
transcription	O
is	O
always	O
well	O
-	O
formed	O
.	O
See	O
Figure	O
1	O
for	O
illustration	O
.	O
Without	O
sentence	O
boundaries	O
,	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
wait	O
-	O
k	O
model	O
takes	O
insufficient	O
text	O
as	O
input	O
and	O
produces	O
an	O
incorrect	O
translation	O
.	O
Therefore	O
,	O
sentence	O
boundary	B-TaskName
detection	I-TaskName
(	O
or	O
sentence	B-TaskName
segmentation	I-TaskName
)	O
1	O
plays	O
an	O
important	O
role	O
to	O
narrow	O
the	O
gap	O
between	O
the	O
ASR	O
and	O
transcription	O
.	O
A	O
good	O
segmentation	O
will	O
not	O
only	O
improve	O
translation	O
quality	O
but	O
also	O
reduce	O
latency	O
.	O
Studies	O
of	O
sentence	B-TaskName
segmentation	I-TaskName
falls	O
into	O
one	O
of	O
the	O
following	O
two	O
bins	O
:	O
The	O
strategy	O
performs	O
segmentation	O
from	O
a	O
speech	O
perspective	O
.	O
Fügen	O
et	O
al	O
(	O
2007	O
)	O
and	O
Bangalore	O
et	O
al	O
(	O
2012	O
)	O
used	O
prosodic	O
pauses	O
in	O
speech	B-TaskName
recognition	I-TaskName
as	O
segmentation	O
boundaries	O
.	O
This	O
method	O
is	O
effective	O
in	O
dialogue	O
scenarios	O
,	O
with	O
clear	O
silence	O
during	O
the	O
conversation	O
.	O
However	O
,	O
it	O
does	O
not	O
work	O
well	O
in	O
long	O
speech	O
audio	O
,	O
such	O
as	O
lecture	O
scenarios	O
.	O
According	O
to	O
Venuti	O
(	O
2012	O
)	O
,	O
silence	O
-	O
based	O
chunking	B-TaskName
accounts	O
for	O
only	O
6.6	O
%	O
,	O
10	O
%	O
,	O
and	O
17.1	O
%	O
in	O
English	O
,	O
French	O
,	O
and	O
German	O
,	O
respectively	O
.	O
Indicating	O
that	O
in	O
most	O
cases	O
,	O
it	O
can	O
not	O
effectively	O
detect	O
boundaries	O
for	O
streaming	O
words	O
.	O
The	O
strategy	O
takes	O
segmentation	O
as	O
a	O
standard	O
text	O
processing	O
problem	O
.	O
The	O
studies	O
considered	O
the	O
problem	O
as	O
classification	O
or	O
sequence	O
labeling	O
,	O
based	O
on	O
SVM	B-MethodName
,	O
conditional	O
random	O
filed	O
(	O
CRFs	O
)	O
(	O
Lu	O
and	O
Ng	O
,	O
2010	O
;	O
Wang	O
et	O
al	O
,	O
2012	O
;	O
Ueffing	O
et	O
al	O
,	O
2013	O
)	O
.	O
Other	O
researches	O
utilized	O
language	O
model	O
,	O
either	O
based	O
on	O
N	O
-	O
gram	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
or	O
recurrent	O
neural	O
network	O
(	O
RNN	O
)	O
(	O
Tilk	O
and	O
Alumäe	O
,	O
2015	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
classification	O
to	O
solve	O
the	O
problem	O
of	O
sentence	B-TaskName
segmentation	I-TaskName
from	O
the	O
perspective	O
of	O
text	O
.	O
Instead	O
of	O
predicting	O
a	O
sentence	O
boundary	O
for	O
a	O
certain	O
position	O
,	O
we	O
propose	O
a	O
multiposition	O
boundary	O
prediction	O
approach	O
.	O
Specifically	O
,	O
for	O
a	O
source	O
text	O
x	O
=	O
{	O
x	O
1	O
,	O
...	O
,	O
x	O
T	O
}	O
,	O
we	O
calculate	O
the	O
probability	O
of	O
predicting	O
sentence	O
boundary	O
Figure	O
1	O
:	O
An	O
English	O
-	O
to	O
-	O
German	O
example	O
that	O
translates	O
from	O
a	O
streaming	O
source	O
with	O
and	O
without	O
sentence	O
boundaries	O
.	O
We	O
take	O
the	O
wait	O
-	O
K	O
model	O
(	O
Ma	O
et	O
al	O
,	O
2019	O
)	O
for	O
illustration	O
,	O
K=3	O
here	O
.	O
The	O
wait3	O
model	O
first	O
performs	O
three	O
READ	O
(	O
wait	O
)	O
action	O
at	O
the	O
beginning	O
of	O
each	O
sentence	O
(	O
as	O
shown	O
in	O
blue	O
)	O
,	O
and	O
then	O
alternating	O
one	O
READ	O
with	O
one	O
WRITE	O
action	O
in	O
the	O
following	O
steps	O
.	O
Given	O
the	O
input	O
source	O
without	O
sentence	O
boundaries	O
(	O
in	O
the	O
4	O
th	O
line	O
)	O
,	O
the	O
wait3	O
model	O
(	O
in	O
the	O
5	O
th	O
line	O
)	O
does	O
n't	O
take	O
the	O
three	O
READ	O
action	O
at	O
the	O
beginning	O
of	O
following	O
sentences	O
.	O
Therefore	O
,	O
the	O
English	O
phrase	O
"	O
it	O
's	O
going	O
to	O
"	O
,	O
which	O
should	O
have	O
been	O
translated	O
as	O
"	O
wird	O
"	O
,	O
produced	O
a	O
meaningless	O
translation	O
"	O
es	O
ist	O
geht	O
dass	O
"	O
with	O
limited	O
context	O
during	O
wait3	O
model	O
inference	O
.	O
after	O
x	O
t	O
,	O
t	O
=	O
T	O
,	O
T	O
−	O
1	O
,	O
...	O
,	O
T	O
−	O
M	O
.	O
Thus	O
the	O
latency	O
of	O
translation	O
can	O
be	O
controlled	O
within	O
L+M	O
words	O
,	O
where	O
L	O
is	O
the	O
length	O
of	O
the	O
sentence	O
.	O
Inspired	B-DatasetName
by	O
the	O
recent	O
pre	O
-	O
training	O
techniques	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Sun	O
et	O
al	O
,	O
2019	O
)	O
that	O
successfully	O
used	O
in	O
many	O
NLP	O
tasks	O
,	O
we	O
used	O
a	O
pre	O
-	O
trained	O
model	O
for	O
initialization	O
and	O
fine	O
-	O
tune	O
the	O
model	O
on	O
the	O
source	O
side	O
of	O
the	O
sentence	O
.	O
Overall	O
,	O
the	O
contributions	O
are	O
as	O
follows	O
:	O
We	O
propose	O
a	O
novel	O
sentence	B-TaskName
segmentation	I-TaskName
method	O
based	O
on	O
pre	O
-	O
trained	O
language	O
representations	O
,	O
which	O
have	O
been	O
successfully	O
used	O
in	O
various	O
NLP	O
tasks	O
.	O
Our	O
method	O
dynamically	O
predicts	O
the	O
boundary	O
at	O
multiple	O
locations	O
,	O
rather	O
than	O
a	O
specific	O
location	O
,	O
achieving	O
high	O
accuracy	B-MetricName
with	O
low	O
latency	O
.	O

Given	O
a	O
streaming	O
input	O
x	O
=	O
{	O
x	O
1	O
,	O
...	O
,	O
x	O
t	O
,	O
...	O
,	O
x	O
T	O
}	O
,	O
the	O
task	O
of	O
sentence	B-TaskName
segmentation	I-TaskName
is	O
to	O
determine	O
whether	O
x	O
t	O
x	O
is	O
the	O
end	O
of	O
a	O
sentence	O
.	O
Thus	O
the	O
task	O
can	O
be	O
considered	O
as	O
a	O
classification	O
problem	O
,	O
that	O
is	O
p	O
(	O
y	O
t	O
|	O
x	O
,	O
θ	B-HyperparameterName
)	O
,	O
where	O
y	O
t	O
{	O
0	B-DatasetName
,	O
1	O
}	O
.	O
However	O
,	O
in	O
simultaneous	O
translation	O
scenario	O
,	O
the	O
latency	O
is	O
unacceptable	O
if	O
we	O
take	O
the	O
full	O
source	O
text	O
as	O
contextual	O
information	O
.	O
Thus	O
we	O
should	O
limit	O
the	O
context	B-HyperparameterName
size	I-HyperparameterName
and	O
make	O
a	O
decision	O
dynamically	O
.	O
As	O
the	O
input	O
is	O
a	O
word	O
streaming	O
,	O
the	O
sentence	O
boundary	B-TaskName
detection	I-TaskName
problem	O
can	O
be	O
transformed	O
as	O
,	O
whether	O
there	O
exists	O
a	O
sentence	O
boundary	O
until	O
the	O
current	O
word	O
x	O
t	O
.	O
Thus	O
we	O
can	O
use	O
the	O
word	O
streaming	O
as	O
a	O
context	O
to	O
make	O
a	O
prediction	O
.	O
We	O
propose	O
a	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
model	O
to	O
predict	O
the	O
probability	O
of	O
a	O
few	O
words	O
before	O
x	O
t	O
as	O
sentence	O
boundaries	O
(	O
Section	O
3.1	O
)	O
.	O
We	O
use	O
the	O
ERNIE	O
framework	O
to	O
first	O
pre	O
-	O
train	O
a	O
language	O
representation	O
and	O
then	O
fine	O
-	O
tune	O
it	O
to	O
sentence	O
boundary	B-TaskName
detection	I-TaskName
(	O
Section	O
3.2	O
)	O
.	O
We	O
also	O
propose	O
a	O
dynamic	O
voted	O
inference	O
strategy	O
(	O
Section	O
3.3	O
)	O
.	O

Our	O
training	O
data	O
is	O
extracted	O
from	O
paragraphs	O
.	O
Question	O
marks	O
,	O
exclamation	O
marks	O
,	O
and	O
semicolons	O
are	O
mapped	O
to	O
periods	O
and	O
all	O
other	O
punctuation	O
symbols	O
are	O
removed	O
from	O
the	O
corpora	O
.	O
Then	O
for	O
every	O
two	O
adjacent	O
sentences	O
in	O
a	O
paragraph	O
,	O
we	O
concatenate	O
them	O
to	O
form	O
a	O
long	O
sequence	O
,	O
x.	O
We	O
record	O
the	O
position	O
of	O
the	O
period	O
as	O
r	O
and	O
then	O
remove	O
the	O
period	O
from	O
the	O
sequence	O
.	O
For	O
x	O
=	O
(	O
x	O
1	O
,	O
x	O
2	O
,	O
...	O
,	O
x	O
N	O
)	O
with	O
N	O
words	O
,	O
we	O
generate	O
r	O
+	O
M	O
samples	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
...	O
,	O
(	O
r	O
+	O
M	O
)	O
,	O
in	O
the	O
form	O
of	O
<	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
t	O
)	O
,	O
y	O
t	O
>	O
,	O
where	O
y	O
t	O
is	O
the	O
label	O
that	O
:	O
y	O
t	O
=	O
φ	O
,	O
if	O
t	O
<	O
r	O
−	O
(	O
t	O
−	O
r	O
)	O
,	O
if	O
t	O
[	O
r	O
,	O
r	O
+	O
M	O
]	O
(	O
1	O
)	O
Note	O
that	O
if	O
the	O
length	O
of	O
the	O
second	O
sentence	O
is	O
less	O
than	O
M	O
,	O
we	O
concatenate	O
subsequent	O
sentences	O
until	O
r	O
+	O
M	O
samples	O
are	O
collected	O
.	O
Then	O
we	O
define	O
the	O
loss	B-MetricName
function	O
as	O
follows	O
:	O
J	O
(	O
θ	B-HyperparameterName
)	O
=	O
(	O
x	O
,	O
r	O
)	O
D	O
log	O
(	O
r−1	O
t=1	O
p	O
(	O
y	O
t	O
=	O
φ	O
|	O
x	O
≤t	O
;	O
θ	B-HyperparameterName
)	O
+	O
r+M	O
t	O
=	O
r	O
p	O
(	O
y	O
t	O
=	O
−	O
(	O
t	O
−	O
r	O
)	O
)	O
|	O
x	O
≤t	O
;	O
θ	B-HyperparameterName
)	O
)	O
(	O
2	O
)	O
where	O
D	O
is	O
the	O
dataset	O
that	O
contains	O
pairs	O
of	O
concatenated	O
sentences	O
x	O
and	O
its	O
corresponding	O
position	O
of	O
the	O
removed	O
periods	O
r.	O
M	O
is	O
a	O
hyperparameter	O
denotes	O
the	O
number	O
of	O
waiting	O
words	O
.	O
Note	O
that	O
our	O
method	O
differs	O
from	O
previous	O
work	O
in	O
the	O
manner	O
of	O
classification	O
.	O
predicts	O
whether	O
a	O
word	O
x	O
t	O
labeled	O
as	O
the	O
end	O
of	O
a	O
sentence	O
or	O
not	O
by	O
a	O
binary	O
classification	O
:	O
p	O
(	O
y	O
t	O
=	O
0	B-DatasetName
|	O
x	O
t+2	O
t−2	O
)	O
+	O
p	O
(	O
y	O
t	O
=	O
1	O
|	O
x	O
t+2	O
t−2	O
)	O
=	O
1	O
(	O
3	O
)	O
where	O
y	O
t	O
=	O
0	B-DatasetName
means	O
x	O
t	O
is	O
not	O
the	O
end	O
of	O
a	O
sentence	O
and	O
y	O
t	O
=	O
1	O
means	O
x	O
t	O
is	O
the	O
end	O
.	O
x	O
t+2	O
t−2	O
denotes	O
5	O
words	O
x	O
t−2	O
,	O
x	O
t−1	O
,	O
...	O
,	O
x	O
t+2	O
.	O
Some	O
other	O
language	O
-	O
model	O
based	O
work	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
calculates	O
probabilities	O
over	O
all	O
words	O
in	O
the	O
vocabulary	O
including	O
the	O
period	O
:	O
w	O
V	O
∪	O
"	O
.	O
"	O
p	O
(	O
y	O
t	O
=	O
w	O
|	O
x	O
≤t	O
)	O
=	O
1	O
(	O
4	O
)	O
and	O
decides	O
whether	O
x	O
t	O
is	O
a	O
sentence	O
boundary	O
by	O
comparing	O
the	O
probability	O
of	O
y	O
t	O
=	O
"	O
.	O
"	O
and	O
y	O
t	O
=	O
The	O
performance	O
of	O
these	O
methods	O
is	O
limited	O
by	O
incomplete	O
semantics	O
,	O
without	O
considering	O
global	O
boundary	B-TaskName
detection	I-TaskName
.	O
In	O
our	O
methods	O
,	O
we	O
leverage	O
more	O
future	O
words	O
and	O
restrict	O
classes	O
globally	O
:	O
x	O
t+1	O
.	O
1	O
2	O
…	O
−4	O
−3	O
−2	O
1	O
2	O
…	O
−4	O
−3	O
−2	O
−1	O
1	O
2	O
…	O
−4	O
−3	O
−2	O
−1	O
=	O
0	B-DatasetName
|	O
1	O
,	O
…	O
,	O
−2	O
=	O
−1	O
|	O
1	O
,	O
…	O
,	O
−1	O
=	O
−2	O
|	O
1	O
,	O
…	O
,	O
p	O
(	O
y	O
t	O
=	O
φ	O
|	O
x	O
≤t	O
)	O
+	O
M	O
m=0	O
p	O
(	O
y	O
t	O
=	O
−m	O
|	O
x	O
≤t	O
)	O
=	O
1	O
(	O
5	O
)	O
The	O
restriction	O
is	O
motivated	O
that	O
in	O
a	O
lecture	O
scenario	O
,	O
where	O
a	O
sentence	O
could	O
not	O
be	O
very	O
short	O
that	O
contains	O
only	O
1	O
or	O
2	O
words	O
.	O
Thus	O
,	O
the	O
probability	O
distribution	O
prohibits	O
that	O
adjacent	O
words	O
to	O
be	O
the	O
end	O
of	O
sentences	O
at	O
the	O
same	O
time	O
.	O

At	O
inference	O
time	O
,	O
we	O
predict	O
sentence	O
boundaries	O
sequentially	O
with	O
a	O
dynamic	O
voting	O
strategy	O
.	O
Each	O
time	O
a	O
new	O
word	O
x	O
t	O
is	O
received	O
,	O
we	O
predict	O
the	O
probability	O
of	O
M	O
+	O
1	O
classes	O
as	O
shown	O
in	O
the	O
bottom	O
of	O
Figure	O
3	O
,	O
then	O
calculate	O
if	O
the	O
probability	O
of	O
previous	O
M	O
+	O
1	O
positions	O
(	O
x	O
t−M	O
,	O
x	O
t−M	O
+1	O
,	O
x	O
t	O
)	O
is	O
larger	O
then	O
a	O
threshold	O
θ	B-HyperparameterName
T	O
h	O
.	O
If	O
yes	O
,	O
we	O
add	O
a	O
sentence	O
boundary	O
at	O
the	O
corresponding	O
position	O
.	O
Otherwise	O
,	O
we	O
continue	O
to	O
receive	O
new	O
words	O
.	O
Note	O
that	O
the	O
probability	O
is	O
adopted	O
as	O
the	O
voted	O
probability	O
.	O
While	O
the	O
probability	O
of	O
adding	O
a	O
sentence	O
boundary	O
after	O
x	O
t−M	O
has	O
M	O
+	O
1	O
probabilities	O
to	O
calculate	O
the	O
average	O
,	O
the	O
number	O
of	O
probabilities	O
to	O
determine	O
whether	O
it	O
is	O
a	O
sentence	O
boundary	O
at	O
subsequent	O
positions	O
is	O
less	O
than	O
M	O
+	O
1	O
.	O
Here	O
we	O
use	O
the	O
voted	O
average	O
of	O
existing	O
probabilities	O
.	O
Specifically	O
,	O
to	O
judge	O
whether	O
x	O
t	O
is	O
a	O
sentence	O
boundary	O
,	O
it	O
needs	O
t	O
−	O
t	O
+	O
1	O
probabilities	O
:	O
1	O
t	O
−	O
t	O
+	O
1	O
t−t	O
m=0	O
p	O
(	O
y	O
=	O
−m	O
|	O
x	O
1	O
,	O
...	O
,	O
x	O
t+m	O
)	O
(	O
6	O
)	O
where	O
t	O
[	O
t	O
−	O
M	O
,	O
t	O
]	O
.	O
If	O
more	O
than	O
one	O
sentence	O
boundary	O
probabilities	O
for	O
x	O
t−M	O
,	O
...	O
,	O
x	O
t	O
exceeds	O
the	O
threshold	O
θ	B-HyperparameterName
T	O
h	O
at	O
the	O
same	O
time	O
,	O
we	O
choose	O
the	O
front	O
-	O
most	O
position	O
as	O
a	O
sentence	O
boundary	O
.	O
This	O
is	O
consistent	O
with	O
our	O
training	O
process	O
,	O
that	O
is	O
,	O
if	O
there	O
is	O
a	O
sample	O
of	O
two	O
or	O
more	O
sentence	O
boundaries	O
,	O
we	O
ignore	O
the	O
following	O
and	O
label	O
the	O
class	O
y	O
t	O
according	O
to	O
the	O
first	O
boundary	O
.	O
This	O
is	O
because	O
we	O
generate	O
samples	O
with	O
each	O
period	O
in	O
the	O
original	O
paragraph	O
as	O
depicted	O
in	O
Section	O
3.2	O
.	O
From	O
another	O
point	O
of	O
view	O
,	O
the	O
strategy	O
can	O
also	O
compensate	O
for	O
some	O
incorrect	O
suppression	O
of	O
adjacent	O
boundaries	O
,	O
thereby	O
improving	O
online	O
prediction	O
accuracy	B-MetricName
.	O

Experiments	O
are	O
conducted	O
on	O
English	O
-	O
German	O
(	O
En	O
-	O
De	O
)	O
simultaneous	O
translation	O
.	O
We	O
evaluate	O
1	O
)	O
the	O
F	O
-	O
score	O
2	O
of	O
sentence	O
boundary	B-TaskName
detection	I-TaskName
and	O
2	O
)	O
case	O
-	O
sensitive	O
tokenized	O
4	O
-	O
gram	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
as	O
the	O
final	O
translation	O
effect	O
of	O
the	O
segmented	O
sentences	O
.	O
To	O
reduce	O
the	O
impact	O
of	O
the	O
ASR	O
system	O
,	O
we	O
use	O
the	O
transcription	O
without	O
punctuation	O
in	O
both	O
training	O
and	O
evaluation	O
.	O
The	O
datasets	O
used	O
in	O
our	O
experiments	O
are	O
listed	O
in	O
Table	O
1	O
.	O
We	O
use	O
two	O
parallel	O
corpus	O
from	O
machine	B-TaskName
translation	I-TaskName
task	O
:	O
WMT	O
14	O
3	O
and	O
IWSLT	O
14	O
4	O
.	O
WMT	O
14	O
is	O
a	O
text	O
translation	O
corpus	O
including	O
4.4	O
M	O
sentences	O
,	O
mainly	O
on	O
news	O
and	O
web	O
sources	O
.	O
And	O
IWSLT	O
14	O
is	O
a	O
speech	O
translation	O
corpus	O
of	O
TED	O
lectures	O
with	O
transcribed	O
text	O
and	O
corresponding	O
translation	O
.	O
Here	O
we	O
only	O
use	O
the	O
text	O
part	O
in	O
it	O
,	O
containing	O
0.19	O
M	O
sentences	O
in	O
the	O
training	O
set	O
.	O
We	O
train	O
the	O
machine	B-TaskName
translation	I-TaskName
model	O
on	O
WMT	O
14	O
with	O
the	O
base	O
version	O
of	O
the	O
Transformer	B-MethodName
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
,	O
achieving	O
a	O
BLEU	B-MetricName
score	I-MetricName
of	O
27.2	O
on	O
newstest2014	O
.	O
And	O
our	O
sentence	O
boundary	B-TaskName
detection	I-TaskName
model	O
is	O
trained	O
on	O
the	O
source	O
transcription	O
of	O
IWSLT	O
14	O
unless	O
otherwise	O
specified	O
(	O
Section	O
4.3	O
)	O
.	O
To	O
evaluate	O
the	O
system	O
performance	O
,	O
we	O
merge	O
the	O
IWSLT	O
test	O
set	O
of	O
4	O
years	O
(	O
2010	O
)	O
(	O
2011	O
)	O
(	O
2012	O
)	O
(	O
2013	O
)	O
(	O
2014	O
)	O
to	O
construct	O
a	O
big	O
test	O
set	O
of	O
7040	O
sentences	O
.	O
The	O
overall	O
statistics	O
of	O
our	O
dataset	O
is	O
shown	O
in	O
Table	O
1	O
.	O
We	O
evaluate	O
our	O
model	O
and	O
two	O
existing	O
methods	O
listed	O
below	O
:	O
dynamic	O
-	O
base	O
is	O
our	O
proposed	O
method	O
that	O
detect	O
sentence	O
boundaries	O
dynamically	O
using	O
a	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
.	O
dynamic	O
-	O
force	O
adds	O
a	O
constraint	O
on	O
dynamicbase	O
.	O
In	O
order	O
to	O
keep	O
in	O
line	O
with	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
,	O
we	O
add	O
a	O
constraint	O
that	O
sentence	O
should	O
be	O
force	O
segmented	O
if	O
longer	O
than	O
θ	B-HyperparameterName
l	O
.	O
N	O
-	O
gram	O
is	O
the	O
method	O
using	O
an	O
N	O
-	O
gram	O
language	O
model	O
to	O
compare	O
the	O
probability	O
of	O
adding	O
vs.	O
not	O
adding	O
a	O
boundary	O
at	O
x	O
t	O
after	O
receiving	O
x	O
t−N	O
+1	O
,	O
...	O
,	O
x	O
t	O
.	O
We	O
implement	O
according	O
to	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
.	O
T	O
-	O
LSTM	B-MethodName
uses	O
a	O
RNN	O
-	O
based	O
classification	O
model	O
with	O
two	O
classes	O
.	O
We	O
implement	O
a	O
unidirectional	O
RNN	O
and	O
perform	O
training	O
according	O
to	O
(	O
Tilk	O
and	O
Alumäe	O
,	O
2015	O
)	O
5	O
.	O
Our	O
classifier	O
in	O
dynamic	O
-	O
base	O
and	O
dynamicforce	O
is	O
trained	O
under	O
ERNIE	O
base	O
framework	O
.	O
We	O
use	O
the	O
released	O
6	O
parameters	O
obtained	O
at	O
pretraining	O
step	O
as	O
initialization	O
.	O
In	O
the	O
fine	O
-	O
tuning	O
stage	O
,	O
we	O
use	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e	O
−5	O
.	O
5	O
we	O
only	O
keep	O
the	O
two	O
classes	O
of	O
period	O
and	O
φ	O
in	O
this	O
work	O
6	O
https://github.com/PaddlePaddle/ERNIE	O

Table	O
2	O
reports	O
the	O
results	O
of	O
source	O
sentence	B-TaskName
segmentation	I-TaskName
on	O
En	O
-	O
De	O
translation	O
,	O
where	O
the	O
latency	O
is	O
measured	O
by	O
Consecutive	O
Wait	O
(	O
CW	O
)	O
(	O
Gu	O
et	O
al	O
,	O
2017	O
)	O
,	O
the	O
number	O
of	O
words	O
between	O
two	O
translate	O
actions	O
.	O
To	O
eliminate	O
the	O
impact	O
of	O
the	O
different	O
policies	O
in	O
simultaneous	O
translation	O
,	O
we	O
only	O
execute	O
translation	O
at	O
the	O
end	O
of	O
each	O
sentence	O
.	O
Therefore	O
,	O
the	O
CW	O
here	O
denotes	O
the	O
sentence	O
length	O
L	O
plus	O
the	O
number	O
of	O
future	O
words	O
M	O
.	O
We	O
calculate	O
its	O
average	O
and	O
maximum	O
value	O
as	O
"	O
avgCW	O
"	O
and	O
"	O
maxCW	O
"	O
,	O
respectively	O
.	O
Better	O
performance	O
expect	O
high	O
F	O
-	O
score	O
,	O
BLEU	B-MetricName
,	O
and	O
low	O
latency	O
(	O
CW	O
)	O
.	O
The	O
translation	O
effect	O
obtained	O
by	O
using	O
the	O
groundtruth	O
period	O
as	O
the	O
sentence	B-TaskName
segmentation	I-TaskName
is	O
shown	O
in	O
the	O
first	O
line	O
of	O
Oracle	O
.	O
The	O
N	O
-	O
gram	O
method	O
calculate	O
the	O
probability	O
of	O
add	O
(	O
p	O
add	O
)	O
and	O
not	O
add	O
(	O
p	O
not	O
)	O
period	O
at	O
each	O
position	O
,	O
and	O
decide	O
whether	O
to	O
chunk	O
by	O
comparing	O
whether	O
p	O
add	O
/p	O
not	O
exceeds	O
θ	B-HyperparameterName
T	O
h	O
.	O
The	O
N	O
-	O
gram	O
method	O
without	O
threshold	O
tuning	O
(	O
with	O
θ	B-HyperparameterName
T	O
h	O
=	O
e	O
0.0	O
)	O
divides	O
sentences	O
into	O
small	O
pieces	O
,	O
achieving	O
the	O
lowest	O
average	O
latency	O
of	O
6.64	O
.	O
However	O
,	O
the	O
Fscore	O
of	O
segmentation	O
is	O
very	O
low	O
because	O
of	O
the	O
incomplete	O
essence	O
of	O
the	O
n	O
-	O
gram	O
feature	O
.	O
Notable	O
,	O
the	O
precision	O
and	O
recall	O
differs	O
much	O
(	O
precision	O
=	O
0.33	O
,	O
recall	O
=	O
0.78	O
)	O
in	O
this	O
setup	O
.	O
Therefore	O
,	O
we	O
need	O
to	O
choose	O
a	O
better	O
threshold	O
by	O
grid	O
search	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
.	O
With	O
θ	B-HyperparameterName
T	O
h	O
equals	O
to	O
e	O
2.0	O
,	O
the	O
F	O
-	O
score	O
of	O
N	O
-	O
gram	O
method	O
increased	O
a	O
little	O
bit	O
(	O
0.46	O
0.48	O
)	O
,	O
with	O
a	O
more	O
balanced	O
precision	O
and	O
recall	O
(	O
precision	O
=	O
0.51	O
,	O
recall	O
=	O
0.48	O
)	O
.	O
However	O
,	O
the	O
max	O
latency	O
runs	O
out	O
of	O
control	O
,	O
resulting	O
in	O
a	O
maximum	O
of	O
161	O
words	O
in	O
a	O
sentence	O
.	O
We	O
also	O
tried	O
to	O
shorten	O
the	O
latency	O
of	O
the	O
N	O
-	O
gram	O
method	O
by	O
force	O
segmentation	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
,	O
but	O
the	O
result	O
was	O
very	O
poor	O
(	O
precision	O
=	O
0.33	O
,	O
recall	O
=	O
0.40	O
)	O
.	O
The	O
T	O
-	O
LSTM	B-MethodName
method	O
with	O
the	O
hidden	O
size	O
of	O
256	O
performs	O
better	O
than	O
N	O
-	O
gram	O
,	O
but	O
the	O
F	O
-	O
score	O
and	O
BLEU	B-MetricName
is	O
still	O
limited	O
.	O
On	O
the	O
contrary	O
,	O
our	O
dynamic	O
-	O
based	O
approaches	O
with	O
M	O
=	O
1	O
achieve	O
the	O
best	O
F	O
-	O
score	O
at	O
0.74	O
and	O
the	O
final	O
translation	O
is	O
very	O
close	O
to	O
the	O
result	O
of	O
Oracle	O
.	O
In	O
particular	O
,	O
the	O
precision	O
and	O
recall	O
reached	O
about	O
0.72	O
and	O
0.77	O
in	O
both	O
dynamic	O
-	O
force	O
and	O
dynamic	O
-	O
base	O
,	O
respectively	O
.	O
Accurate	O
sentence	B-TaskName
segmentation	I-TaskName
brings	O
better	O
performance	O
in	O
translation	O
,	O
bringing	O
an	O
improvement	O
of	O
1.55	O
over	O
T	O
-	O
LSTM	B-MethodName
.	O
Moreover	O
,	O
our	O
approach	O
is	O
not	O
inferior	O
in	O
terms	O
of	O
latency	O
.	O
Both	O
average	O
latency	O
and	O
max	O
latency	O
is	O
controlled	O
at	O
a	O
relatively	O
low	O
level	O
.	O
It	O
is	O
interesting	O
to	O
note	O
that	O
,	O
dynamic	O
-	O
force	O
performs	O
better	O
than	O
dynamic	O
-	O
base	O
,	O
in	O
terms	O
of	O
latency	O
and	O
BLEU	B-MetricName
.	O
This	O
suggests	O
the	O
effectiveness	O
of	O
the	O
force	O
segmentation	O
strategy	O
,	O
that	O
is	O
,	O
select	O
the	O
chunking	B-TaskName
location	O
with	O
a	O
sentence	O
length	O
limitation	O
will	O
not	O
affect	O
the	O
accuracy	B-MetricName
of	O
segmentation	O
,	O
and	O
would	O
enhance	O
the	O
translation	O
effect	O
.	O

According	O
to	O
Section	O
3.2	O
,	O
the	O
order	O
between	O
sentences	O
of	O
original	O
corpora	O
would	O
affect	O
the	O
generation	O
of	O
training	O
samples	O
.	O
In	O
this	O
section	O
,	O
we	O
investigate	O
the	O
effect	O
of	O
various	O
data	O
reordering	O
strategies	O
.	O
A	O
basic	O
method	O
is	O
to	O
use	O
the	O
original	O
sentence	O
order	O
of	O
speech	O
corpora	O
,	O
denote	O
as	O
Basic	O
.	O
However	O
,	O
the	O
samples	O
generated	O
is	O
limited	O
,	O
which	O
makes	O
the	O
model	O
easy	O
to	O
over	O
-	O
fit	O
.	O
To	O
overcome	O
this	O
problem	O
,	O
we	O
adopt	O
two	O
methods	O
to	O
expand	O
data	O
scale	O
:	O
1	O
)	O
Duplicate	O
the	O
original	O
data	O
multiple	O
times	O
or	O
2	O
)	O
Add	O
Synthetic	O
adjacent	O
sentences	O
,	O
through	O
randomly	O
selecting	O
two	O
sentences	O
from	O
the	O
corpora	O
.	O
These	O
two	O
methods	O
greatly	O
expand	O
the	O
total	O
amount	O
of	O
data	O
,	O
but	O
the	O
gain	O
to	O
the	O
model	O
is	O
uncertain	O
.	O
As	O
an	O
alternative	O
,	O
we	O
explore	O
a	O
Sort	O
method	O
,	O
to	O
sort	O
sentences	O
according	O
to	O
alphabetic	O
order	O
.	O
The	O
performance	O
of	O
the	O
four	O
training	O
data	O
organization	O
methods	O
is	O
shown	O
in	O
Figure	O
4	O
,	O
all	O
built	O
on	O
IWSLT2014	O
and	O
conducted	O
under	O
the	O
setup	O
of	O
M	O
=	O
1	O
and	O
θ	B-HyperparameterName
l	O
=	O
40	O
.	O
It	O
is	O
clear	O
that	O
Basic	O
,	O
Duplicate	O
and	O
Synthetic	O
are	O
all	O
involved	O
in	O
the	O
problem	O
of	O
over	O
-	O
fitting	O
.	O
They	O
quickly	O
achieved	O
their	O
best	O
results	O
and	O
then	O
gradually	O
declined	O
.	O
Surprisingly	O
,	O
the	O
Sort	O
approach	O
is	O
prominent	O
in	O
both	O
segmentation	O
accuracy	B-MetricName
and	O
translation	O
performance	O
.	O
This	O
may	O
be	O
due	O
to	O
the	O
following	O
reasons	O
:	O
1	O
)	O
Sentence	B-TaskName
classification	I-TaskName
is	O
not	O
a	O
difficult	O
task	O
,	O
especially	O
when	O
M	O
=	O
1	O
for	O
3	O
-	O
class	O
classification	O
(	O
y	O
[	O
φ	O
,	O
0	B-DatasetName
,	O
−1	O
]	O
)	O
,	O
making	O
the	O
task	O
easy	O
to	O
over	O
-	O
fit	O
.	O
2	O
)	O
Compared	O
with	O
Basic	O
,	O
Duplicate	O
is	O
more	O
abundant	O
in	O
the	O
sample	O
combination	O
in	O
batch	O
training	O
,	O
but	O
there	O
is	O
no	O
essential	O
difference	O
between	O
the	O
two	O
methods	O
.	O
3	O
)	O
Synthetic	O
hardly	O
profits	O
our	O
model	O
,	O
because	O
the	O
synthesized	O
data	O
may	O
be	O
very	O
simple	O
due	O
to	O
random	O
selection	O
.	O
4	O
)	O
Sort	O
may	O
simulate	O
difficult	O
cases	O
in	O
real	O
scenes	O
and	O
train	O
them	O
pertinently	O
,	O
bringing	O
it	O
a	O
poor	O
performance	O
at	O
start	O
but	O
not	O
prone	O
to	O
overfit	O
.	O
There	O
are	O
many	O
samples	O
with	O
identical	O
head	O
and	O
tail	O
words	O
in	O
the	O
sorted	O
data	O
,	O
such	O
as	O
:	O
"	O
and	O
it	O
gives	O
me	O
a	O
lot	O
of	O
hope	O
and	O
...	O
"	O
and	O
"	O
that	O
means	O
there	O
's	O
literally	O
thousands	O
of	O
new	O
ideas	O
that	O
...	O
"	O
.	O
Even	O
human	O
beings	O
find	O
it	O
difficult	O
to	O
determine	O
whether	O
the	O
words	O
before	O
is	O
sentence	O
boundaries	O
of	O
these	O
samples	O
.	O
In	O
Basic	O
,	O
Duplicate	O
and	O
Synthetic	O
methods	O
,	O
such	O
samples	O
are	O
usually	O
submerged	O
in	O
a	O
large	O
quantity	O
of	O
simple	O
samples	O
.	O
However	O
,	O
the	O
data	O
organization	O
mode	O
of	O
Sort	O
greatly	O
strengthens	O
the	O
model	O
's	O
ability	O
to	O
learn	O
these	O
difficult	O
samples	O
.	O
There	O
is	O
no	O
need	O
to	O
worry	O
that	O
the	O
Sort	O
method	O
can	O
not	O
cover	O
simple	O
samples	O
.	O
Because	O
we	O
sort	O
by	O
rows	O
in	O
source	O
file	O
,	O
and	O
some	O
of	O
the	O
rows	O
contain	O
multiple	O
sentences	O
(	O
an	O
average	O
of	O
1.01	O
sentences	O
per	O
row	O
)	O
,	O
which	O
are	O
in	O
real	O
speech	O
order	O
.	O
We	O
argue	O
that	O
these	O
sentences	O
are	O
sufficient	O
to	O
model	O
the	O
classification	O
of	O
simple	O
samples	O
,	O
based	O
on	O
the	O
rapid	O
overfit	O
performance	O
of	O
the	O
other	O
three	O
methods	O
.	O

Next	O
,	O
we	O
turn	O
to	O
the	O
question	O
that	O
how	O
does	O
the	O
domain	O
of	O
training	O
corpus	O
affects	O
results	O
.	O
With	O
the	O
test	O
set	O
unchanged	O
,	O
we	O
compare	O
the	O
sentence	O
boundary	O
detections	O
model	O
trained	O
on	O
out	O
-	O
of	O
-	O
domain	O
corpora	O
WMT	O
14	O
and	O
in	O
-	O
domain	O
corpora	O
IWSLT	O
14	O
,	O
respectively	O
.	O
As	O
mentioned	O
before	O
,	O
WMT	O
14	O
is	O
a	O
larger	O
text	O
translation	O
corpus	O
mainly	O
on	O
news	O
and	O
web	O
sources	O
.	O
But	O
the	O
test	O
set	O
comes	O
from	O
IWSLT	O
,	O
which	O
contains	O
transcriptions	O
of	O
TED	O
lectures	O
of	O
various	O
directions	O
.	O
Intuitively	O
,	O
larger	O
dataset	O
provides	O
more	O
diverse	O
samples	O
,	O
but	O
due	O
to	O
domain	O
changes	O
,	O
it	O
does	O
not	O
necessarily	O
lead	O
to	O
improvements	O
in	O
accuracy	B-MetricName
.	O
The	O
performance	O
of	O
various	O
models	O
trained	O
on	O
WMT14	B-DatasetName
is	O
shown	O
in	O
Table	O
3	O
.	O
Dynamic	O
-	O
force	O
also	O
achieves	O
the	O
best	O
translation	O
performance	O
with	O
a	O
relatively	O
small	O
latency	O
on	O
average	O
and	O
limited	O
the	O
max	O
latency	O
within	O
40	O
words	O
.	O
However	O
,	O
it	O
underperforms	O
the	O
same	O
model	O
trained	O
on	O
IWSLT2014	O
(	O
as	O
shown	O
in	O
Table	O
2	O
)	O
,	O
demonstrating	O
its	O
sensitivity	O
to	O
the	O
training	O
domain	O
.	O
On	O
the	O
contrary	O
,	O
N	O
-	O
gram	O
and	O
T	O
-	O
LSTM	B-MethodName
is	O
hardly	O
affected	O
.	O
For	O
N	O
-	O
gram	O
,	O
one	O
possible	O
reason	O
is	O
the	O
before	O
mentioned	O
weakness	O
of	O
the	O
N	O
-	O
gram	O
:	O
segmentation	O
depends	O
on	O
only	O
N	O
previous	O
words	O
,	O
which	O
is	O
more	O
steady	O
compared	O
to	O
the	O
whole	O
sentence	O
,	O
thus	O
eliminating	O
the	O
perturbation	O
of	O
whole	O
sentence	O
brought	O
by	O
the	O
domain	O
variation	O
.	O
For	O
T	O
-	O
LSTM	B-MethodName
,	O
it	O
even	O
improves	O
a	O
little	O
compared	O
with	O
its	O
in	O
-	O
domain	O
performance	O
.	O
This	O
may	O
be	O
due	O
to	O
the	O
lack	O
of	O
training	O
samples	O
.	O
0.19	O
M	O
sentences	O
of	O
IWSLT2014	O
is	O
insufficient	O
to	O
fit	O
the	O
parameters	O
of	O
T	O
-	O
LSTM	B-MethodName
.	O
Thus	O
the	O
model	O
would	O
benefit	O
from	O
increasing	O
the	O
corpus	O
size	O
.	O
However	O
,	O
our	O
method	O
needs	O
less	O
data	O
in	O
training	O
because	O
our	O
model	O
has	O
been	O
pre	O
-	O
trained	O
.	O
Based	O
on	O
a	O
powerful	O
representation	O
,	O
we	O
need	O
only	O
a	O
small	O
amount	O
of	O
training	O
data	O
in	O
fine	O
-	O
tuning	O
,	O
which	O
is	O
best	O
aligned	O
with	O
the	O
test	O
set	O
in	O
the	O
domain	O
.	O

Next	O
,	O
we	O
discuss	O
the	O
effect	O
of	O
changing	O
θ	B-HyperparameterName
.	O
The	O
performance	O
of	O
dynamic	O
-	O
force	O
with	O
varying	O
θ	B-HyperparameterName
l	O
is	O
shown	O
in	O
Table	O
4	O
.	O
Smaller	O
θ	B-HyperparameterName
l	O
brings	O
shorter	O
latency	O
,	O
as	O
well	O
as	O
worse	O
performance	O
.	O
The	O
effect	O
is	O
extremely	O
poor	O
with	O
θ	B-HyperparameterName
l	O
=	O
10	O
.	O
There	O
are	O
two	O
possible	O
reasons	O
:	O
1	O
)	O
Constraint	O
sentence	O
length	O
less	O
than	O
θ	B-HyperparameterName
l	O
is	O
too	O
harsh	O
under	O
small	O
θ	B-HyperparameterName
l	O
,	O
2	O
)	O
The	O
discrepancy	O
between	O
the	O
unrestricted	O
training	O
and	O
length	O
-	O
restricted	O
testing	O
causes	O
the	O
poor	O
effect	O
.	O
We	O
first	O
focus	O
on	O
the	O
second	O
possible	O
reason	O
.	O
While	O
the	O
difference	O
between	O
dynamic	O
-	O
base	O
and	O
dynamic	O
-	O
force	O
is	O
only	O
in	O
prediction	O
,	O
we	O
want	O
to	O
know	O
whether	O
we	O
can	O
achieve	O
better	O
results	O
by	O
controlling	O
the	O
length	O
of	O
training	O
samples	O
.	O
Accordingly	O
,	O
we	O
only	O
use	O
the	O
samples	O
shorter	O
than	O
a	O
fixed	O
value	O
:	O
θ	B-HyperparameterName
l	O
in	O
training	O
phrase	O
.	O
At	O
inference	O
time	O
,	O
we	O
use	O
both	O
dynamic	O
-	O
force	O
with	O
the	O
same	O
sentence	O
length	O
constraint	O
θ	B-HyperparameterName
l	O
and	O
dynamic	O
-	O
base	O
to	O
predict	O
sentence	O
boundaries	O
.	O
As	O
elaborated	O
in	O
Figure	O
5	O
,	O
For	O
each	O
pair	O
of	O
curves	O
with	O
a	O
same	O
θ	B-HyperparameterName
l	O
,	O
dynamic	O
-	O
force	O
and	O
dynamic	O
-	O
base	O
present	O
similar	O
performance	O
.	O
This	O
demonstrates	O
the	O
main	O
reason	O
for	O
the	O
poor	O
performance	O
with	O
small	O
θ	B-HyperparameterName
l	O
is	O
not	O
the	O
training	O
-	O
testing	O
discrepancy	O
but	O
lies	O
in	O
the	O
first	O
reason	O
that	O
the	O
force	O
constraint	O
is	O
too	O
harsh	O
.	O
Moreover	O
,	O
it	O
is	O
interesting	O
to	O
find	O
that	O
the	O
performance	O
of	O
θ	B-HyperparameterName
l	O
=	O
80	O
is	O
similar	O
with	O
θ	B-HyperparameterName
l	O
=	O
40	O
at	O
the	O
beginning	O
but	O
falls	O
a	O
little	O
during	O
training	O
.	O
This	O
probably	O
because	O
the	O
setup	O
with	O
θ	B-HyperparameterName
l	O
=	O
40	O
can	O
filter	O
some	O
inaccurate	O
cases	O
,	O
as	O
the	O
average	O
number	O
of	O
words	O
in	O
IWSLT2014	O
training	O
set	O
is	O
20.26	O
.	O

We	O
investigate	O
whether	O
can	O
we	O
achieve	O
better	O
performance	O
with	O
more	O
or	O
less	O
future	O
words	O
.	O
We	O
experiment	O
with	O
M	O
from	O
0	B-DatasetName
to	O
5	O
.	O
The	O
result	O
is	O
shown	O
in	O
Table	O
5	O
.	O
Reducing	O
M	O
to	O
zero	O
means	O
that	O
do	O
not	O
refer	O
to	O
any	O
future	O
words	O
in	O
prediction	O
.	O
This	O
degrades	O
performance	O
a	O
lot	O
,	O
proving	O
the	O
effectiveness	O
of	O
adding	O
future	O
words	O
in	O
prediction	O
.	O
Increase	O
M	O
from	O
1	O
to	O
2	O
also	O
promote	O
the	O
performance	O
in	O
both	O
sentence	O
boundary	B-TaskName
detection	I-TaskName
f	O
-	O
score	O
and	O
the	O
system	O
BLEU	B-MetricName
.	O
However	O
,	O
as	O
more	O
future	O
words	O
added	O
(	O
increase	O
M	O
to	O
3	O
and	O
4	O
)	O
,	O
the	O
improvement	O
becomes	O
less	O
obvious	O
.	O

Some	O
work	O
takes	O
a	O
fixed	O
size	O
of	O
words	O
as	O
input	O
.	O
Focus	O
on	O
utilizing	O
a	O
limited	O
size	O
of	O
the	O
streaming	O
input	O
,	O
they	O
predict	O
the	O
probability	O
of	O
putting	O
a	O
boundary	O
at	O
a	O
specific	O
position	O
x	O
t	O
by	O
a	O
N	O
-	O
gram	O
lan	O
-	O
guage	O
model	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
or	O
a	O
classification	O
model	O
Yarmohammadi	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
language	O
-	O
model	O
based	O
method	O
make	O
decision	O
depends	O
on	O
N	O
words	O
(	O
x	O
t−N	O
+2	O
,	O
...	O
,	O
x	O
t+1	O
)	O
and	O
compares	O
its	O
probability	O
with	O
(	O
x	O
t−N	O
+2	O
,	O
...	O
,	O
x	O
t	O
,	O
"	O
.	O
"	O
)	O
.	O
The	O
classification	O
model	O
takes	O
features	O
of	O
N	O
words	O
around	O
x	O
t	O
and	O
classifies	O
to	O
two	O
classes	O
denoting	O
x	O
t	O
is	O
a	O
sentence	O
boundary	O
or	O
not	O
.	O
The	O
main	O
deficiency	O
of	O
this	O
method	O
is	O
that	O
the	O
dependencies	O
outside	O
the	O
input	O
window	O
are	O
lost	O
,	O
resulting	O
in	O
low	O
accuracy	B-MetricName
.	O

Some	O
other	O
work	O
focuses	O
on	O
restoring	O
punctuation	O
and	O
capitalization	O
using	O
the	O
whole	O
sentence	O
.	O
To	O
improve	O
the	O
sentence	O
boundary	O
classification	O
accuracy	B-MetricName
,	O
some	O
work	O
upgrade	O
the	O
N	O
-	O
gram	O
input	O
to	O
variable	O
-	O
length	O
input	O
by	O
using	O
recurrent	O
neural	O
network	O
(	O
RNN	O
)	O
(	O
Tilk	O
and	O
Alumäe	O
,	O
2015	O
;	O
Salloum	O
et	O
al	O
,	O
2017	O
)	O
.	O
Some	O
other	O
work	O
takes	O
punctuation	O
restoration	O
as	O
a	O
sequence	O
labeling	O
problem	O
and	O
investigates	O
using	O
Conditional	O
Random	O
Fields	O
(	O
CRFs	O
)	O
(	O
Lu	O
and	O
Ng	O
,	O
2010	O
;	O
Wang	O
et	O
al	O
,	O
2012	O
;	O
Ueffing	O
et	O
al	O
,	O
2013	O
)	O
.	O
Peitz	O
et	O
al	O
(	O
2011	O
)	O
and	O
Cho	O
et	O
al	O
(	O
2012	O
)	O
treats	O
this	O
problem	O
as	O
a	O
machine	B-TaskName
translation	I-TaskName
task	O
,	O
training	O
to	O
translate	O
non	O
-	O
punctuated	O
transcription	O
into	O
punctuated	O
text	O
.	O
However	O
,	O
all	O
these	O
methods	O
utilize	O
the	O
whole	O
sentence	O
information	O
,	O
which	O
is	O
not	O
fit	O
for	O
the	O
simultaneous	O
translation	O
scenario	O
.	O
Moreover	O
,	O
the	O
translation	O
model	O
based	O
methods	O
require	O
multiple	O
steps	O
of	O
decoding	O
,	O
making	O
it	O
unsuitable	O
for	O
online	O
prediction	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
online	O
sentence	O
boundary	B-TaskName
detection	I-TaskName
approach	O
.	O
With	O
the	O
input	O
of	O
streaming	O
words	O
,	O
our	O
model	O
predicts	O
the	O
probability	O
of	O
multiple	O
positions	O
rather	O
than	O
a	O
certain	O
position	O
.	O
By	O
adding	O
this	O
adjacent	O
position	O
constraint	O
and	O
using	O
dynamic	O
prediction	O
,	O
our	O
method	O
achieves	O
higher	O
accuracy	B-MetricName
with	O
lower	O
latency	O
.	O
We	O
also	O
incorporate	O
the	O
pre	O
-	O
trained	O
technique	O
,	O
ERNIE	O
to	O
implement	O
our	O
classification	O
model	O
.	O
The	O
empirical	O
results	O
on	O
IWSLT2014	O
demonstrate	O
that	O
our	O
approach	O
achieves	O
significant	O
improvements	O
of	O
0.19	O
F	O
-	O
score	O
on	O
sentence	B-TaskName
segmentation	I-TaskName
and	O
1.55	O
BLEU	B-MetricName
points	O
compared	O
with	O
the	O
language	O
-	O
model	O
based	O
methods	O
.	O

Computational	O
approaches	O
to	O
noun	O
ellipsis	O
resolution	O
has	O
been	O
sparse	O
,	O
with	O
only	O
a	O
naive	O
rulebased	O
approach	O
that	O
uses	O
syntactic	O
feature	O
constraints	O
for	O
marking	O
noun	O
ellipsis	O
licensors	O
and	O
selecting	O
their	O
antecedents	O
.	O
In	O
this	O
paper	O
,	O
we	O
further	O
the	O
ellipsis	O
research	O
by	O
exploring	O
several	O
statistical	O
and	O
neural	O
models	O
for	O
both	O
the	O
subtasks	O
involved	O
in	O
the	O
ellipsis	O
resolution	O
process	O
and	O
addressing	O
the	O
representation	O
and	O
contribution	O
of	O
manual	O
features	O
proposed	O
in	O
previous	O
research	O
.	O
Using	O
the	O
best	O
performing	O
models	O
,	O
we	O
build	O
an	O
end	O
-	O
to	O
-	O
end	O
supervised	O
Machine	O
Learning	O
(	O
ML	O
)	O
framework	O
for	O
this	O
task	O
that	O
improves	O
the	O
existing	O
F1	B-MetricName
score	I-MetricName
by	O
16.55	O
%	O
for	O
the	O
detection	O
and	O
14.97	O
%	O
for	O
the	O
resolution	O
subtask	O
.	O
Our	O
experiments	O
demonstrate	O
robust	O
scores	O
through	O
pretrained	O
BERT	B-MethodName
(	O
Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
)	O
embeddings	O
for	O
word	O
representation	O
,	O
and	O
more	O
so	O
the	O
importance	O
of	O
manual	O
features	O
-	O
once	O
again	O
highlighting	O
the	O
syntactic	O
and	O
semantic	O
characteristics	O
of	O
the	O
ellipsis	O
phenomenon	O
.	O
For	O
the	O
classification	O
decision	O
,	O
we	O
notice	O
that	O
a	O
simple	O
Multilayar	O
Perceptron	O
(	O
MLP	B-DatasetName
)	O
works	O
well	O
for	O
the	O
detection	O
of	O
ellipsis	O
;	O
however	O
,	O
Recurrent	O
Neural	O
Networks	O
(	O
RNN	O
)	O
are	O
a	O
better	O
choice	O
for	O
the	O
much	O
harder	O
resolution	O
step	O
.	O

We	O
detection	O
task	O
,	O
we	O
take	O
the	O
annotated	O
946	O
positive	O
samples	O
(	O
exophoric	O
)	O
and	O
randomly	O
choose	O
946	O
negative	O
samples	O
.	O
Similarly	O
,	O
for	O
the	O
resolution	O
task	O
,	O
we	O
take	O
438	O
positive	O
samples	O
(	O
endophoric	O
)	O
and	O
438	O
randomly	O
chosen	O
negative	O
samples	O
.	O
We	O
perform	O
a	O
standard	O
70	O
-	O
10	O
-	O
20	O
split	O
to	O
obtain	O
the	O
train	O
,	O
development	O
and	O
test	O
set	O
respectively	O
,	O
and	O
follow	O
the	O
5	O
-	O
fold	O
cross	O
validation	O
procedure	O
to	O
capture	O
both	O
classes	O
properly	O
in	O
each	O
case	O
.	O
For	O
MLP	B-DatasetName
,	O
we	O
take	O
a	O
simple	O
,	O
two	O
-	O
layer	O
feedforward	B-MethodName
network	I-MethodName
(	O
FFNN	O
)	O
or	O
two	O
layers	O
of	O
multiple	O
computational	O
units	O
interconnected	O
in	O
a	O
feed	O
-	O
forward	O
way	O
without	O
loops	O
.	O
We	O
have	O
a	O
single	O
hidden	O
layer	O
with	O
768	O
neurons	O
and	O
a	O
sigmoid	O
function	O
.	O
A	O
unidirectional	O
weight	O
connection	O
exists	O
between	O
the	O
two	O
successive	O
layers	O
.	O
The	O
classification	O
decision	O
is	O
made	O
by	O
turning	O
the	O
input	O
vector	O
representations	O
of	O
a	O
word	O
with	O
its	O
context	O
into	O
a	O
score	O
.	O
The	O
network	O
has	O
a	O
softmax	B-MethodName
output	O
layer	O
.	O
For	O
bi	O
-	O
LSTM	B-MethodName
,	O
we	O
have	O
embedding	O
layer	O
,	O
time	O
-	O
distributed	O
translate	O
layer	O
,	O
Bi	O
-	O
LSTM	B-MethodName
(	O
RNN	O
)	O
layer	O
,	O
batch	B-MethodName
normalization	I-MethodName
layer	O
,	O
dropout	O
layer	O
and	O
prediction	O
layer	O
.	O
The	O
activation	O
used	O
is	O
Softmax	B-MethodName
.	O
The	O
loss	B-MetricName
function	O
is	O
calculated	O
with	O
cross	O
entropy	O
.	O
We	O
train	O
in	O
batch	O
sizes	O
of	O
16	O
and	O
early	B-MethodName
stopping	I-MethodName
with	O
max	O
epochs	O
of	O
100	O
.	O
In	O
early	B-MethodName
stopping	I-MethodName
the	O
patience	O
is	O
kept	O
to	O
be	O
10	O
and	O
the	O
optimizer	B-HyperparameterName
used	O
is	O
Adam	B-MethodName
.	O
We	O
use	O
default	O
values	O
for	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
.	O
We	O
use	O
Keras	O
(	O
Chollet	O
et	O
al	O
,	O
2015	O
)	O
for	O
coding	O
the	O
models	O
.	O

We	O
evaluate	O
the	O
performance	O
of	O
our	O
models	O
in	O
terms	O
of	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
,	O
computed	O
by	O
taking	O
an	O
average	B-MetricName
F1	I-MetricName
-	O
scores	O
obtained	O
from	O
the	O
5	O
-	O
folds	O
results	O
.	O
We	O
experiment	O
with	O
sixteen	O
models	O
each	O
for	O
the	O
noun	O
ellipsis	O
detection	O
and	O
resolution	O
.	O
The	O
results	O
on	O
the	O
testset	O
for	O
Precision	B-MetricName
,	O
Recall	B-MetricName
and	O
F1	B-MetricName
-	I-MetricName
Score	I-MetricName
values	O
are	O
presented	O
in	O
Table	O
.2	O
.	O
As	O
expected	O
,	O
the	O
neural	O
models	O
perform	O
significantly	O
better	O
than	O
the	O
statistical	O
ones	O
for	O
both	O
the	O
subtasks	O
.	O
Our	O
experiments	O
show	O
that	O
for	O
the	O
detection	O
task	O
,	O
BERT	B-MethodName
embeddings	O
with	O
a	O
simple	O
MLP	B-DatasetName
gives	O
best	O
scores	O
.	O
This	O
is	O
expected	O
because	O
,	O
BERT	B-MethodName
currently	O
provides	O
the	O
most	O
powerful	O
contextual	O
word	O
representations	O
,	O
using	O
12	O
separate	O
attention	O
mechanism	O
for	O
each	O
layer	O
,	O
where	O
,	O
at	O
each	O
layer	O
,	O
each	O
token	O
can	O
focus	O
on	O
12	O
distinct	O
aspects	O
of	O
other	O
tokens	O
.	O
Since	O
Transformers	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
use	O
many	O
distinct	O
attention	O
heads	O
(	O
12	O
*	O
12=144	O
for	O
the	O
base	O
BERT	B-MethodName
model	O
)	O
,	O
each	O
head	O
can	O
focus	O
on	O
a	O
different	O
kind	O
of	O
constituent	O
combinations	O
,	O
making	O
BERT	B-MethodName
broadly	O
attending	O
over	O
the	O
whole	O
sentence	O
.	O
In	O
our	O
task	O
,	O
the	O
(	O
Khullar	O
et	O
al	O
,	O
2019	O
)	O
and	O
the	O
neural	O
model	O
presented	O
in	O
this	O
paper	O
.	O
input	O
and	O
output	O
,	O
but	O
they	O
are	O
not	O
innately	O
designed	O
to	O
capture	O
temporal	O
relationships	O
within	O
a	O
sentence	O
.	O
Hence	O
,	O
although	O
they	O
perform	O
well	O
for	O
a	O
task	O
like	O
detection	O
that	O
needs	O
local	O
information	O
,	O
they	O
are	O
outperformed	O
by	O
bi	O
-	O
LSTMs	O
on	O
the	O
resolution	O
task	O
that	O
requires	O
capturing	O
a	O
deeper	O
relationship	O
between	O
the	O
antecedent	O
and	O
the	O
elided	O
noun	O
.	O
We	O
also	O
note	O
that	O
manual	O
feature	O
addition	O
boosts	O
results	O
greatly	O
for	O
all	O
models	O
,	O
highlighting	O
that	O
ellipsis	O
is	O
a	O
syntactically	O
constrained	O
phenomenon	O
.	O
We	O
finally	O
integrate	O
the	O
best	O
models	O
for	O
each	O
subtask	O
into	O
an	O
end	O
-	O
to	O
-	O
end	O
pipeline	O
,	O
as	O
in	O
Figure	O
2	O
.	O
Now	O
,	O
instead	O
of	O
the	O
gold	O
vectors	O
(	O
from	O
the	O
annotations	O
)	O
,	O
the	O
resolution	O
model	O
is	O
fed	O
the	O
ouput	O
licensor	O
vector	O
from	O
the	O
detection	O
model	O
.	O
This	O
obviously	O
results	O
into	O
error	O
propagation	O
into	O
the	O
second	O
model	O
,	O
and	O
lowers	O
the	O
precision	O
value	O
to	O
82.52	O
%	O
,	O
recall	O
to	O
78.66	O
%	O
and	O
consequently	O
,	O
the	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
to	O
80.55	O
%	O
of	O
the	O
final	O
system	O
.	O
The	O
error	O
in	O
the	O
final	O
system	O
comes	O
from	O
failing	O
to	O
detect	O
actual	O
licensors	O
,	O
wrongly	O
identifying	O
non	O
-	O
licensor	O
words	O
and	O
correct	O
licensor	O
detection	O
but	O
failed	O
antecedent	O
resolution	O
.	O
We	O
run	O
our	O
final	O
system	O
on	O
the	O
curated	O
dataset	O
prepared	O
by	O
(	O
Khullar	O
et	O
al	O
,	O
2019	O
)	O
and	O
compare	O
the	O
results	O
with	O
their	O
rule	O
-	O
based	O
approach	O
.	O
As	O
expected	O
,	O
this	O
model	O
improves	O
the	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
by	O
16.55	O
%	O
for	O
noun	O
ellipsis	O
detection	O
and	O
14.97	O
%	O
for	O
noun	O
ellipsis	O
res	O
-	O
olution	O
.	O
See	O
Table	O
3	O
.	O
The	O
even	O
higher	O
accuracy	B-MetricName
on	O
the	O
curated	O
dataset	O
can	O
be	O
explained	O
by	O
the	O
nature	O
of	O
the	O
sentences	O
in	O
this	O
dataset	O
which	O
are	O
from	O
textbooks	O
,	O
and	O
,	O
hence	O
,	O
free	O
of	O
grammatical	O
errors	O
,	O
etc	O
.	O
-	O
resulting	O
into	O
improved	O
parser	O
performance	O
in	O
the	O
pre	O
-	O
processing	O
step	O
.	O
Although	O
,	O
the	O
presented	O
models	O
achieve	O
high	O
scores	O
on	O
both	O
the	O
tasks	O
separately	O
and	O
in	O
the	O
pipeline	O
process	O
,	O
the	O
results	O
can	O
be	O
further	O
improved	O
with	O
hyper	O
-	O
parameter	O
tuning	O
and	O
additional	O
regularization	O
.	O

In	O
this	O
section	O
,	O
we	O
survey	O
related	O
attention	O
mechanisms	O
(	O
Bahdanau	O
et	O
al	O
,	O
2014	O
)	O
and	O
review	O
the	O
most	O
relevant	O
studies	O
on	O
information	O
bottleneck	O
(	O
IB	O
)	O
(	O
Tishby	O
et	O
al	O
,	O
1999	O
)	O
.	O
Attention	O
has	O
been	O
proved	O
can	O
help	O
explain	O
the	O
internals	O
of	O
neural	O
models	O
(	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Wiegreffe	O
and	O
Pinter	O
,	O
2019	O
)	O
though	O
it	O
is	O
limited	O
(	O
Jain	O
and	O
Wallace	O
,	O
2019	O
)	O
.	O
Many	O
researchers	O
try	O
to	O
improve	O
the	O
interpretability	O
of	O
the	O
attention	O
mechanisms	O
.	O
leveraged	O
small	O
amounts	O
of	O
word	O
-	O
level	O
annotations	O
to	O
regularize	O
attention	O
.	O
Kim	O
et	O
al	O
(	O
2017	O
)	O
introduced	O
a	O
structured	O
attention	O
mechanism	O
to	O
learn	O
attention	O
variants	O
from	O
explicit	O
probabilistic	O
semantics	O
.	O
Barrett	O
et	O
al	O
(	O
2018	O
)	O
;	O
Bao	O
et	O
al	O
(	O
2018	O
)	O
aligned	O
explanations	O
with	O
human	O
-	O
provided	O
rationales	O
to	O
improve	O
the	O
explanation	O
of	O
attention	O
.	O
Unlike	O
these	O
methods	O
that	O
require	O
prior	O
attributions	O
or	O
human	O
explanations	O
,	O
the	O
VAT	O
method	O
enforces	O
the	O
attention	O
to	O
learn	O
the	O
vital	O
information	O
while	O
filter	O
the	O
noise	O
via	O
IB	O
.	O
A	O
series	O
of	O
studies	O
motivate	O
us	O
to	O
utilize	O
IB	O
to	O
improve	O
the	O
explanations	O
of	O
attention	O
mechanisms	O
.	O
Li	O
and	O
Eisner	O
(	O
2019	O
)	O
compressed	O
the	O
pre	O
-	O
trained	O
embedding	O
(	O
e.g.	O
,	O
BERT	B-MethodName
,	O
ELMO	B-MethodName
)	O
,	O
remaining	O
only	O
the	O
information	O
that	O
helps	O
a	O
discriminative	O
parser	O
through	O
variational	O
IB	O
.	O
Zhmoginov	O
et	O
al	O
(	O
2019	O
)	O
utilized	O
the	O
IB	O
approach	O
to	O
discover	O
the	O
salient	O
region	O
.	O
Some	O
works	O
(	O
Jiang	O
et	O
al	O
,	O
2020	O
;	O
Chen	O
et	O
al	O
,	O
2018	O
;	O
Guan	O
et	O
al	O
,	O
2019	O
;	O
Schulz	O
et	O
al	O
,	O
2020	O
;	O
Bang	O
et	O
al	O
,	O
2019	O
)	O
proposed	O
to	O
identify	O
vital	O
features	O
or	O
attributions	O
via	O
IB	O
.	O
Moreover	O
,	O
Chen	O
and	O
Ji	O
(	O
2020	O
)	O
designed	O
a	O
variational	O
mask	O
strategy	O
to	O
delete	O
the	O
useless	O
words	O
in	O
the	O
text	O
.	O
As	O
far	O
as	O
we	O
are	O
aware	O
,	O
we	O
are	O
the	O
first	O
ones	O
to	O
leverage	O
IB	O
into	O
attention	O
mechanisms	O
to	O
train	O
more	O
interpretable	O
attention	O
with	O
better	O
accuracy	B-MetricName
.	O

Our	O
framework	O
is	O
composed	O
of	O
a	O
learner	O
and	O
a	O
compressor	O
,	O
which	O
performs	O
fine	O
-	O
tuning	O
and	O
compressing	O
iteratively	O
(	O
Figure	O
1	O
)	O
.	O
The	O
learner	O
aims	O
to	O
learn	O
a	O
task	O
-	O
specific	O
contextual	O
word	O
representation	O
by	O
fine	O
-	O
tuning	O
.	O
The	O
compressor	O
enforces	O
the	O
model	O
to	O
learn	O
task	O
-	O
relevant	O
information	O
while	O
reduce	O
irrelevant	O
information	O
via	O
IB	O
.	O
We	O
iteratively	O
perform	O
the	O
learner	O
and	O
compressor	O
(	O
fine	O
-	O
tuning	O
and	O
compressing	O
)	O
to	O
improve	O
each	O
other	O
.	O
Learner	O
.	O
We	O
adopt	O
a	O
basic	O
attention	O
-	O
based	O
neural	O
network	O
model	O
as	O
a	O
learner	O
to	O
learn	O
representations	O
of	O
the	O
words	O
based	O
on	O
the	O
good	O
attention	O
weights	O
learned	O
by	O
the	O
compressor	O
.	O
The	O
model	O
is	O
optimized	O
by	O
cross	O
-	O
entropy	O
loss	B-MetricName
to	O
learn	O
the	O
label	O
-	O
relevant	O
information	O
.	O
In	O
this	O
phase	O
,	O
we	O
fix	O
the	O
attention	O
's	O
parameters	O
so	O
that	O
the	O
model	O
will	O
focus	O
on	O
updating	O
the	O
encoder	O
to	O
learn	O
word	O
representations	O
.	O
Compressor	O
.	O
To	O
restrict	O
the	O
attention	O
to	O
capture	O
the	O
vital	O
information	O
while	O
reduce	O
the	O
noise	O
,	O
we	O
integrate	O
IB	O
into	O
attention	O
mechanisms	O
to	O
compress	O
the	O
text	O
attentive	O
representation	O
.	O
We	O
fix	O
the	O
encoder	O
's	O
parameters	O
so	O
that	O
the	O
model	O
will	O
focus	O
on	O
learning	O
the	O
attention	O
weights	O
based	O
on	O
current	O
representations	O
obtained	O
from	O
the	O
learner	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
our	O
learner	O
,	O
which	O
is	O
an	O
attention	O
-	O
based	O
neural	O
network	O
model	O
.	O
First	O
,	O
given	O
a	O
text	O
T	O
"	O
tw	O
1	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
|	O
T	O
|	O
u	O
,	O
where	O
|	O
T	O
|	O
is	O
the	O
length	O
of	O
text	O
T	O
,	O
we	O
feed	O
it	O
into	O
an	O
encoder	O
with	O
a	O
First	O
,	O
we	O
obtain	O
the	O
input	O
text	O
's	O
word	O
representations	O
X	O
via	O
an	O
encoder	O
trained	O
by	O
the	O
learner	O
.	O
Then	O
,	O
we	O
calculate	O
Z	O
by	O
compressing	O
the	O
text	O
representation	O
R	O
that	O
is	O
the	O
weighted	O
sum	O
of	O
X	O
based	O
on	O
the	O
attention	O
α	B-HyperparameterName
,	O
while	O
remaining	O
the	O
maximum	O
information	O
to	O
judge	O
Y	O
by	O
inputting	O
Z	O
into	O
a	O
MLP	B-DatasetName
classifier	O
for	O
predicting	O
.	O
word	O
embedding	O
layer	O
.	O
We	O
adopt	O
LSTM	B-MethodName
and	O
BERT	B-MethodName
models	O
as	O
our	O
encoder	O
,	O
and	O
other	O
models	O
can	O
also	O
be	O
applied	O
to	O
our	O
framework	O
.	O
We	O
obtain	O
the	O
contextaware	O
word	O
representations	O
x	O
"	O
rx	O
1	O
,	O
x	O
2	O
,	O
...	O
,	O
x	O
|	O
T	O
|	O
s	O
,	O
where	O
x	O
i	O
is	O
the	O
hidden	O
vector	O
of	O
the	O
word	O
w	O
i	O
.	O
x	O
"	O
encoderpT	O
,	O
θ	B-HyperparameterName
encoder	O
q	O
,	O
(	O
)	O
1	O
where	O
θ	B-HyperparameterName
encoder	O
is	O
the	O
parameters	O
of	O
the	O
encoder	O
.	O
Based	O
on	O
the	O
contextual	O
word	O
representations	O
,	O
attention	O
mechanism	O
(	O
Bahdanau	O
et	O
al	O
,	O
2014	O
)	O
2	O
is	O
utilized	O
to	O
capture	O
the	O
important	O
parts	O
in	O
the	O
text	O
and	O
obtain	O
the	O
text	O
representation	O
R	O
,	O
which	O
is	O
calculated	O
as	O
,	O
R	O
"	O
n	O
ÿ	O
i"1	O
α	B-HyperparameterName
i	O
x	O
i	O
α	B-HyperparameterName
i	O
"	O
softmaxpv	O
J	O
a	O
tanhpW	O
a	O
x	O
i	O
qq	O
(	O
2	O
)	O
where	O
θ	B-HyperparameterName
attention	O
"	O
tv	O
a	O
,	O
W	O
a	O
u	O
is	O
the	O
trainable	O
parameters	O
of	O
the	O
attention	O
,	O
which	O
is	O
not	O
updated	O
in	O
this	O
step	O
to	O
learn	O
the	O
word	O
representation	O
x	O
based	O
the	O
good	O
attention	O
learned	O
by	O
the	O
compressor	O
.	O
α	B-HyperparameterName
"	O
rα	O
1	O
,	O
α	B-HyperparameterName
2	O
,	O
...	O
,	O
α	B-HyperparameterName
|	O
T	O
|	O
s	O
is	O
the	O
attention	O
weights	O
.	O
Finally	O
,	O
we	O
input	O
the	O
text	O
representation	O
R	O
into	O
a	O
multi	O
-	O
layer	O
perceptron	O
(	O
MLP	B-DatasetName
)	O
to	O
predict	O
the	O
probability	O
.	O
The	O
cross	O
-	O
entropy	O
loss	B-MetricName
is	O
used	O
to	O
optimize	O
the	O
model	O
.	O

The	O
learner	O
optimizes	O
the	O
sentence	O
representations	O
by	O
minimizing	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
,	O
which	O
does	O
not	O
restrict	O
the	O
model	O
to	O
ignore	O
the	O
useless	O
information	O
.	O
Thus	O
,	O
we	O
compress	O
sentence	O
representations	O
R	O
into	O
a	O
latent	O
representation	O
Z	O
that	O
retains	O
most	O
useful	O
information	O
to	O
infer	O
the	O
label	O
Y	O
.	O
We	O
propose	O
to	O
accomplish	O
this	O
by	O
integrating	O
VIB	O
into	O
the	O
attention	O
mechanism	O
(	O
Figure	O
2	O
)	O
.	O
To	O
ensure	O
Z	O
contains	O
maximum	O
ability	O
to	O
predict	O
Y	O
(	O
IpZ	O
;	O
Y	O
q	O
)	O
while	O
has	O
the	O
least	O
redundant	O
information	O
form	O
R	O
(	O
´	O
IpZ	O
;	O
Rq	O
)	O
,	O
we	O
use	O
the	O
standard	O
IB	O
theory	O
(	O
Tishby	O
et	O
al	O
,	O
1999	O
)	O
where	O
KLr¨	O
}	O
¨s	O
represents	O
Kullback	O
-	O
Leibler	O
divergence	O
.	O
Specifically	O
,	O
we	O
regard	O
ppyq	O
as	O
constant	O
and	O
then	O
minimize	O
E	O
p	O
θ	B-HyperparameterName
py	O
,	O
zq	O
rlog	O
q	O
φ	O
py	O
|	O
zqs	O
.	O
Since	O
we	O
must	O
first	O
sample	O
r	O
to	O
sample	O
y	O
,	O
z	O
from	O
p	O
θ	B-HyperparameterName
pr	O
,	O
y	O
,	O
zq	O
,	O
the	O
lower	O
bound	O
of	O
IpZ	O
;	O
Y	O
q	O
is	O
computed	O
as	O
,	O
IpZ	O
;	O
Y	O
q	O
ě	O
E	O
ppr	O
,	O
yq	O
rE	O
p	O
θ	B-HyperparameterName
pz	O
|	O
rq	O
rlog	O
q	O
φ	O
py	O
|	O
zqss	O
(	O
5	O
)	O
We	O
calculate	O
the	O
upper	O
bound	O
of	O
IpZ	O
;	O
Rq	O
by	O
replacing	O
p	O
θ	B-HyperparameterName
pzq	O
with	O
a	O
variational	O
distribution	O
r	O
ψ	O
pzq	O
,	O
3	O
We	O
give	O
the	O
main	O
steps	O
as	O
follows	O
and	O
the	O
detailed	O
derivation	O
is	O
provided	O
in	O
supplementary	O
materials	O
.	O
4	O
Y	O
Ñ	O
R	O
Ñ	O
Z	O
:	O
Y	O
and	O
Z	O
are	O
independent	O
given	O
R.	O
Then	O
,	O
we	O
obtain	O
the	O
lower	O
bound	O
L	O
of	O
IB	O
by	O
substituting	O
Equation	O
5	O
and	O
7	O
into	O
Equation	O
3	O
:	O
L	O
"	O
E	O
ppr	O
,	O
yq	O
rE	O
p	O
θ	B-HyperparameterName
pz	O
|	O
rq	O
rlog	O
q	O
φ	O
py	O
|	O
zqś	O
β¨KLrp	O
θ	B-HyperparameterName
pz	O
|	O
rq	O
}	O
r	O
ψ	O
pzqss	O
(	O
8	O
)	O
The	O
first	O
component	O
in	O
L	O
is	O
to	O
keep	O
the	O
most	O
useful	O
information	O
in	O
p	O
θ	B-HyperparameterName
pz	O
|	O
rq	O
for	O
inferring	O
y	O
,	O
while	O
the	O
second	O
one	O
is	O
to	O
regularize	O
p	O
θ	B-HyperparameterName
pz	O
|	O
rq	O
with	O
a	O
predefined	O
prior	O
distribution	O
r	O
ψ	O
pzq	O
(	O
e.g.	O
,	O
Gaussian	O
distribution	O
)	O
.	O
To	O
compute	O
p	O
θ	B-HyperparameterName
pz	O
|	O
rq	O
,	O
we	O
adopt	O
the	O
reparametrization	O
trick	O
for	O
multivariate	O
Gaussians	O
(	O
Rezende	O
et	O
al	O
,	O
2014	O
)	O
,	O
which	O
obtains	O
the	O
gradient	O
of	O
parameters	O
that	O
derive	O
z	O
from	O
a	O
random	O
noise	O
.	O
z	O
"	O
u`σ	O
d	O
,	O
"	O
N	O
p0	O
,	O
Iq	O
(	O
9	O
)	O
where	O
d	O
means	O
element	O
-	O
wise	O
multiplication	O
.	O
u	O
and	O
σ	O
denote	O
the	O
mean	O
and	O
covariance	O
defined	O
by	O
two	O
functions	O
of	O
R	O
,	O
where	O
R	O
"	O
α¨x	O
that	O
is	O
learned	O
based	O
on	O
attention	O
.	O
In	O
particular	O
,	O
two	O
MLP	B-DatasetName
are	O
used	O
to	O
predict	O
u	O
and	O
σ	O
.	O
Finally	O
,	O
we	O
input	O
the	O
z	O
into	O
a	O
MLP	B-DatasetName
to	O
predict	O
q	O
φ	O
py	O
|	O
zq	O
and	O
optimize	O
the	O
attention	O
's	O
parameter	O
via	O
Equation	O
8	O
.	O

We	O
report	O
the	O
accuracy	B-MetricName
of	O
our	O
VAT	O
and	O
baselines	O
based	O
on	O
LSTM	B-MethodName
and	O
BERT	B-MethodName
(	O
Table	O
2	O
)	O
.	O
From	O
these	O
results	O
,	O
we	O
find	O
the	O
following	O
observations	O
:	O
1	O
)	O
our	O
models	O
(	O
LSTM	B-MethodName
/	O
BERT	B-MethodName
-	O
VAT	O
)	O
outperform	O
all	O
the	O
corresponding	O
baselines	O
over	O
all	O
the	O
eight	O
datasets	O
,	O
which	O
denotes	O
the	O
effectiveness	O
of	O
our	O
VAT	O
on	O
both	O
LSTM	B-MethodName
and	O
BERT	B-MethodName
-	O
based	O
models	O
;	O
2	O
)	O
compared	O
with	O
attention	O
-	O
based	O
models	O
(	O
LSTM	B-MethodName
/	O
BERT	B-MethodName
-	O
ATT	O
)	O
,	O
our	O
models	O
obtain	O
better	O
results	O
.	O
It	O
indicates	O
reducing	O
the	O
irrelevant	O
information	O
in	O
input	O
via	O
VAT	O
can	O
improve	O
the	O
performance	O
of	O
the	O
models	O
.	O
Furthermore	O
,	O
we	O
visualize	O
the	O
sentence	O
representations	O
obtained	O
from	O
LSTM	B-MethodName
/	O
BERT	B-MethodName
-	O
ATT	O
and	O
-	O
VAT	O
models	O
(	O
Figure	O
3	O
)	O
.	O
We	O
randomly	O
select	O
1000	O
samples	O
from	O
the	O
test	O
set	O
for	O
each	O
dataset	O
.	O
We	O
can	O
find	O
that	O
our	O
VAT	O
model	O
can	O
reduce	O
the	O
distance	O
of	O
the	O
samples	O
in	O
a	O
class	O
and	O
add	O
the	O
distance	O
of	O
the	O
samples	O
in	O
different	O
classes	O
.	O
For	O
example	O
,	O
it	O
is	O
hard	O
to	O
split	O
the	O
positive	O
samples	O
from	O
the	O
negative	O
ones	O
based	O
on	O
the	O
representations	O
obtained	O
from	O
LSTM	B-MethodName
-	O
ATT	O
for	O
the	O
IMDB	B-DatasetName
dataset	O
,	O
while	O
the	O
divider	O
line	O
based	O
on	O
our	O
VAT	O
is	O
clear	O
.	O
These	O
ob	O
-	O
servations	O
show	O
our	O
VAT	O
model	O
can	O
learn	O
a	O
better	O
task	O
-	O
specific	O
representation	O
by	O
enforcing	O
the	O
model	O
to	O
reduce	O
the	O
task	O
-	O
irrelevant	O
information	O
.	O

In	O
this	O
section	O
,	O
we	O
evaluate	O
our	O
VAT	O
model	O
using	O
two	O
metrics	O
,	O
AOPC	O
and	O
post	O
-	O
hoc	O
accuracy	B-MetricName
,	O
which	O
are	O
widely	O
used	O
for	O
explanations	O
(	O
Chen	O
and	O
Ji	O
,	O
2020	O
)	O
.	O
Note	O
that	O
well	O
-	O
trained	O
LSTM	B-MethodName
/	O
BERT	B-MethodName
-	O
base	O
is	O
used	O
for	O
evaluating	O
the	O
performance	O
of	O
classification	O
.	O
AOPC	O
.	O
To	O
evaluate	O
the	O
faithfulness	O
of	O
explanations	O
to	O
our	O
models	O
,	O
we	O
adopt	O
the	O
area	O
over	O
the	O
perturbation	O
curve	O
(	O
AOPC	O
)	O
(	O
Nguyen	O
,	O
2018	O
;	O
Samek	O
et	O
al	O
,	O
2016	O
)	O
metric	O
.	O
It	O
calculates	O
the	O
average	O
change	O
of	O
accuracy	B-MetricName
over	O
test	O
data	O
by	O
deleting	O
top	O
K	O
words	O
via	O
attentive	O
weights	O
.	O
The	O
larger	O
the	O
value	O
of	O
AOPC	O
,	O
the	O
better	O
the	O
explanations	O
of	O
the	O
models	O
.	O
Table	O
3	O
displays	O
the	O
results	O
with	O
K	O
"	O
5	O
.	O
We	O
compare	O
our	O
models	O
with	O
random	O
and	O
basic	O
attention	O
-	O
based	O
models	O
.	O
From	O
the	O
results	O
,	O
we	O
observe	O
that	O
:	O
1	O
)	O
basic	O
attention	O
-	O
based	O
models	O
(	O
LSTM	B-MethodName
/	O
BERT	B-MethodName
-	O
ATT	O
)	O
can	O
find	O
the	O
important	O
words	O
in	O
the	O
sentence	O
to	O
some	O
extent	O
.	O
Comparing	O
with	O
random	O
(	O
Random	O
)	O
,	O
LSTM	B-MethodName
/	O
BERT	B-MethodName
-	O
ATT	O
obtains	O
significant	O
improvement	O
;	O
2	O
)	O
Our	O
models	O
(	O
LSTM	B-MethodName
/	O
BERT	B-MethodName
-	O
VAT	O
)	O
outperform	O
the	O
standard	O
attention	O
-	O
based	O
models	O
.	O
It	O
indicates	O
that	O
integrating	O
VIB	O
into	O
the	O
attention	O
mechanism	O
can	O
help	O
improve	O
the	O
interpretability	O
of	O
the	O
models	O
by	O
filtering	O
the	O
useless	O
information	O
;	O
3	O
)	O
BERT	B-MethodName
model	O
is	O
sensitive	O
to	O
the	O
context	O
;	O
deleting	O
the	O
words	O
will	O
destroy	O
the	O
semantic	O
information	O
of	O
the	O
sentence	O
and	O
significantly	O
affect	O
the	O
model	O
's	O
performance	O
.	O
We	O
also	O
explore	O
the	O
influence	O
of	O
top	O
-	O
K	O
(	O
Figure	O
4	O
)	O
.	O
Intuitively	O
,	O
the	O
more	O
words	O
we	O
delete	O
,	O
the	O
larger	O
accuracy	B-MetricName
the	O
models	O
reduce	O
.	O
Our	O
models	O
reduce	O
more	O
performance	O
than	O
random	O
and	O
attention	O
-	O
based	O
models	O
.	O
For	O
the	O
IMDB	B-DatasetName
dataset	O
,	O
when	O
deleting	O
top	O
20	O
words	O
(	O
average	O
length	O
is	O
268	O
)	O
,	O
the	O
accuracy	B-MetricName
reduces	O
about	O
19	O
points	O
for	O
our	O
LSTM	B-MethodName
-	O
VAT	O
model	O
while	O
it	O
is	O
about	O
2	O
points	O
for	O
the	O
random	O
model	O
.	O
Post	O
-	O
hoc	O
Accuracy	B-MetricName
.	O
We	O
also	O
adopt	O
the	O
post	O
-	O
hoc	O
accuracy	B-MetricName
(	O
Chen	O
et	O
al	O
,	O
2018	O
)	O
to	O
evaluate	O
the	O
influence	O
of	O
task	O
-	O
specific	O
essential	O
words	O
on	O
the	O
performance	O
of	O
LSTM	B-MethodName
-	O
based	O
and	O
BERT	B-MethodName
-	O
based	O
models	O
.	O
For	O
each	O
test	O
sample	O
,	O
we	O
select	O
the	O
top	O
K	O
words	O
based	O
on	O
their	O
attentive	O
weights	O
as	O
input	O
to	O
make	O
a	O
prediction	O
and	O
compare	O
it	O
with	O
the	O
ground	O
truth	O
.	O
Table	O
4	O
presents	O
the	O
performance	O
with	O
K	O
"	O
5	O
.	O
First	O
,	O
it	O
is	O
interesting	O
to	O
find	O
that	O
the	O
post	O
-	O
hoc	O
accuracy	B-MetricName
with	O
five	O
most	O
important	O
words	O
on	O
Sbuj	O
dataset	O
(	O
89.10	O
)	O
is	O
even	O
better	O
than	O
the	O
original	O
sentence	O
(	O
89.00	O
)	O
.	O
Additionally	O
,	O
we	O
obtain	O
comparable	O
results	O
with	O
only	O
five	O
words	O
for	O
SST	B-DatasetName
-	O
1	O
,	O
SST	B-DatasetName
-	O
2	O
,	O
and	O
Twitter	O
datasets	O
.	O
These	O
show	O
that	O
our	O
model	O
can	O
reduce	O
the	O
noise	O
information	O
since	O
most	O
of	O
the	O
words	O
are	O
useless	O
for	O
predictions	O
in	O
some	O
cases	O
.	O
Second	O
,	O
for	O
BERT	B-MethodName
-	O
based	O
models	O
,	O
the	O
context	O
words	O
are	O
also	O
important	O
for	O
classification	O
even	O
though	O
they	O
may	O
not	O
be	O
task	O
-	O
specific	O
.	O
Similarly	O
,	O
we	O
investigate	O
the	O
influence	O
of	O
top	O
-	O
K	O
for	O
post	O
-	O
hoc	O
(	O
Figure	O
5	O
)	O
.	O
The	O
LSTM	B-MethodName
-	O
base	O
model	O
with	O
top	O
-	O
10	O
words	O
selected	O
by	O
our	O
LSTM	B-MethodName
-	O
VAT	O
model	O
can	O
achieve	O
comparable	O
results	O
with	O
the	O
original	O
samples	O
in	O
most	O
cases	O
.	O
Additionally	O
,	O
for	O
the	O
IMDB	B-DatasetName
dataset	O
,	O
the	O
accuracy	B-MetricName
of	O
LSTM	B-MethodName
-	O
base	O
with	O
one	O
word	O
selected	O
by	O
our	O
VAT	O
model	O
is	O
even	O
better	O
than	O
the	O
one	O
with	O
20	O
words	O
selected	O
randomly	O
.	O

We	O
perform	O
semi	O
-	O
supervised	O
word	O
-	O
level	O
sentiment	O
detection	O
in	O
Twitter	O
(	O
Rosenthal	O
et	O
al	O
,	O
2015	O
(	O
Rosenthal	O
et	O
al	O
,	O
,	O
2014	O
to	O
evaluate	O
the	O
interpretability	O
of	O
our	O
VAT	O
.	O
This	O
task	O
requires	O
to	O
detect	O
the	O
sentiment	O
words	O
in	O
a	O
tweet	O
via	O
the	O
sentiment	O
polarity	O
of	O
the	O
whole	O
tweet	O
.	O
In	O
the	O
following	O
example	O
from	O
the	O
dataset	O
,	O
positive	O
words	O
(	O
"	O
good	O
"	O
and	O
"	O
fantastic	O
"	O
)	O
are	O
marked	O
with	O
a	O
bold	O
font	O
and	O
the	O
overall	O
polarity	O
of	O
the	O
tweet	O
is	O
positive	O
:	O
Good	O
morning	O
becky	O
!	O
Thursday	O
is	O
going	O
to	O
be	O
fantastic	O
!	O
We	O
use	O
the	O
SemEval	B-DatasetName
2013	I-DatasetName
Twitter	O
dataset	O
,	O
which	O
contains	O
word	O
-	O
level	O
sentiment	O
annotation	O
.	O
We	O
remove	O
the	O
samples	O
with	O
the	O
neutral	O
sentiment	O
.	O
We	O
report	O
word	O
-	O
level	O
precision	O
,	O
recall	O
,	O
and	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
for	O
evaluating	O
the	O
models	O
(	O
Table	O
5	O
)	O
,	O
the	O
same	O
as	O
.	O
Note	O
that	O
we	O
select	O
the	O
top	O
-	O
K	O
(	O
we	O
set	O
it	O
as	O
1	O
and	O
5	O
here	O
)	O
words	O
according	O
to	O
the	O
attention	O
weights	O
as	O
the	O
sentiment	O
words	O
.	O
We	O
compare	O
our	O
VAT	O
model	O
with	O
random	O
and	O
attention	O
-	O
based	O
models	O
.	O
The	O
results	O
show	O
attentionbased	O
models	O
can	O
capture	O
the	O
important	O
words	O
in	O
the	O
text	O
,	O
to	O
a	O
certain	O
extent	O
.	O
Since	O
our	O
VAT	O
can	O
reduce	O
irrelevant	O
information	O
,	O
it	O
performs	O
better	O
than	O
the	O
standard	O
attention	O
model	O
.	O
Also	O
,	O
LSTM	B-MethodName
-	O
based	O
models	O
outperform	O
BERT	B-MethodName
-	O
based	O
models	O
for	O
this	O
task	O
in	O
most	O
cases	O
.	O
It	O
is	O
because	O
that	O
BERT	B-MethodName
learns	O
much	O
semantic	O
information	O
from	O
the	O
text	O
,	O
and	O
context	O
information	O
plays	O
a	O
vital	O
role	O
in	O
prediction	O
.	O

We	O
propose	O
to	O
train	O
the	O
learner	O
and	O
compressor	O
iteratively	O
so	O
that	O
the	O
learner	O
optimizes	O
the	O
word	O
representations	O
based	O
on	O
the	O
good	O
attention	O
,	O
and	O
the	O
compressor	O
optimizes	O
the	O
attention	O
based	O
on	O
the	O
good	O
word	O
representations	O
.	O
To	O
have	O
a	O
deep	O
look	O
at	O
how	O
it	O
works	O
,	O
we	O
first	O
provide	O
our	O
VAT	O
model	O
's	O
accuracy	B-MetricName
with	O
different	O
iterations	O
(	O
Table	O
6	O
)	O
.	O
From	O
the	O
results	O
,	O
we	O
can	O
find	O
that	O
the	O
model	O
's	O
performance	O
will	O
improve	O
at	O
first	O
,	O
then	O
it	O
will	O
converge	O
.	O
Positive	O
Negative	O
P@1	B-MetricName
R@1	B-MetricName
F1@1	O
P@5	O
R@5	B-MetricName
F1@5	O
P@1	B-MetricName
R@1	B-MetricName
F1@1	O
P@5	O
R@	O
Also	O
,	O
we	O
draw	O
change	O
of	O
the	O
sentence	O
representation	O
with	O
different	O
iterations	O
(	O
Figure	O
6	O
)	O
.	O
Similarly	O
,	O
we	O
observe	O
that	O
fine	O
-	O
tuning	O
and	O
compressing	O
iteratively	O
can	O
improve	O
the	O
sentence	O
representations	O
.	O
The	O
samples	O
with	O
the	O
same	O
class	O
are	O
close	O
,	O
and	O
the	O
samples	O
with	O
different	O
classes	O
have	O
a	O
large	O
distance	O
.	O

The	O
predominant	O
neural	O
architectures	O
in	O
machine	B-TaskName
translation	I-TaskName
are	O
recurrent	O
encoder	O
-	O
decoder	O
networks	O
(	O
Graves	O
,	O
2012	O
;	O
Cho	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
encoder	O
is	O
a	O
recurrent	O
neural	O
network	O
(	O
RNN	O
)	O
based	O
on	O
gated	O
recurrent	O
units	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
;	O
Cho	O
et	O
al	O
,	O
2014	O
)	O
to	O
map	O
the	O
input	O
sequence	O
into	O
a	O
vector	O
representation	O
.	O
Often	O
a	O
bi	O
-	O
directional	O
RNN	O
(	O
Schuster	O
and	O
Paliwal	O
,	O
1997	O
)	O
is	O
used	O
,	O
which	O
consists	O
of	O
two	O
RNNs	O
that	O
process	O
the	O
input	O
in	O
opposite	O
directions	O
,	O
and	O
the	O
final	O
states	O
of	O
both	O
RNNs	O
are	O
concatenated	O
as	O
the	O
input	O
encoding	O
.	O
The	O
decoder	O
consists	O
of	O
a	O
second	O
RNN	O
,	O
which	O
takes	O
the	O
input	O
encoding	O
,	O
and	O
sequentially	O
samples	O
the	O
output	O
sequence	O
one	O
to	O
-	O
ken	O
at	O
a	O
time	O
whilst	O
updating	O
its	O
state	O
.	O
While	O
best	O
known	O
for	O
their	O
use	O
in	O
visual	O
recognition	O
models	O
,	O
(	O
Oord	O
et	O
al	O
,	O
2016a	O
;	O
Salimans	O
et	O
al	O
,	O
2017	O
;	O
Reed	O
et	O
al	O
,	O
2017	O
;	O
Oord	O
et	O
al	O
,	O
2016c	O
)	O
.	O
Recent	O
works	O
also	O
introduced	O
convolutional	O
networks	O
to	O
natural	O
language	O
processing	O
.	O
The	O
first	O
convolutional	O
apporaches	O
to	O
encoding	O
variablelength	O
sequences	O
consist	O
of	O
stacking	O
word	O
vectors	O
,	O
applying	O
1D	O
convolutions	O
then	O
aggregating	O
with	O
a	O
max	O
-	O
pooling	O
operator	O
over	O
time	O
(	O
Collobert	O
and	O
Weston	O
,	O
2008	O
;	O
Kalchbrenner	O
et	O
al	O
,	O
2014	O
;	O
Kim	O
,	O
2014	O
)	O
.	O
For	O
sequence	O
generation	O
,	O
the	O
works	O
of	O
Ranzato	O
et	O
al	O
(	O
2016	O
)	O
;	O
Bahdanau	O
et	O
al	O
(	O
2017	O
)	O
;	O
Gehring	O
et	O
al	O
(	O
2017a	O
)	O
mix	O
a	O
convolutional	O
encoder	O
with	O
an	O
RNN	O
decoder	O
.	O
The	O
first	O
entirely	O
convolutional	O
encoder	O
-	O
decoder	O
models	O
where	O
introduced	O
by	O
,	O
but	O
they	O
did	O
not	O
improve	O
over	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
recurrent	O
architectures	O
.	O
Gehring	O
et	O
al	O
(	O
2017b	O
)	O
outperformed	O
deep	O
LSTMs	O
for	O
machine	B-TaskName
translation	I-TaskName
1D	O
CNNs	O
with	O
gated	O
linear	O
units	O
(	O
Meng	O
et	O
al	O
,	O
2015	O
;	O
Oord	O
et	O
al	O
,	O
2016c	O
;	O
Dauphin	O
et	O
al	O
,	O
2017	O
)	O
in	O
both	O
the	O
encoder	O
and	O
decoder	O
modules	O
.	O
Such	O
CNN	O
-	O
based	O
models	O
differ	O
from	O
their	O
RNN	O
-	O
based	O
counterparts	O
in	O
that	O
temporal	O
connections	O
are	O
placed	O
between	O
layers	O
of	O
the	O
network	O
,	O
rather	O
than	O
within	O
layers	O
.	O
See	O
Figure	O
2	O
for	O
a	O
conceptual	O
illustration	O
.	O
This	O
apparently	O
small	O
difference	O
in	O
connectivity	O
has	O
two	O
important	O
consequences	O
.	O
First	O
,	O
it	O
makes	O
the	O
field	O
of	O
view	O
grow	O
linearly	O
across	O
layers	O
in	O
the	O
convolutional	O
network	O
,	O
while	O
it	O
is	O
unbounded	O
within	O
layers	O
in	O
the	O
recurrent	O
network	O
.	O
Second	O
,	O
while	O
the	O
activations	O
in	O
the	O
RNN	O
can	O
only	O
be	O
computed	O
in	O
a	O
sequential	O
manner	O
,	O
they	O
can	O
be	O
computed	O
in	O
parallel	O
across	O
the	O
temporal	O
dimension	O
in	O
the	O
convolutional	O
case	O
.	O
In	O
all	O
the	O
recurrent	O
or	O
convolutional	O
models	O
mentioned	O
above	O
,	O
each	O
of	O
the	O
input	O
and	O
output	O
sequences	O
are	O
processed	O
separately	O
as	O
a	O
onedimensional	O
sequence	O
by	O
the	O
encoder	O
and	O
decoder	O
respectively	O
.	O
Attention	O
mechanisms	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
;	O
Luong	O
et	O
al	O
,	O
2015	O
;	O
Xu	O
et	O
al	O
,	O
2015	O
)	O
were	O
introduced	O
as	O
an	O
interface	O
between	O
the	O
encoder	O
and	O
decoder	O
modules	O
.	O
During	O
encoding	O
,	O
the	O
attention	O
model	O
finds	O
which	O
hidden	O
states	O
from	O
the	O
source	O
code	O
are	O
the	O
most	O
salient	O
for	O
generating	O
the	O
next	O
target	O
token	O
.	O
This	O
is	O
achieved	O
by	O
evaluating	O
a	O
"	O
context	O
vector	O
"	O
which	O
,	O
in	O
its	O
most	O
basic	O
form	O
,	O
is	O
a	O
weighted	O
average	O
of	O
the	O
source	O
features	O
.	O
The	O
weights	O
of	O
the	O
summation	O
are	O
predicted	O
by	O
a	O
small	O
neural	O
network	O
that	O
scores	O
these	O
features	O
condi	O
-	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
propose	O
an	O
architecture	O
relying	O
entirely	O
on	O
attention	O
.	O
Positional	O
input	O
coding	O
together	O
with	O
self	O
-	O
attention	O
(	O
Parikh	O
et	O
al	O
,	O
2016	O
;	O
replaces	O
recurrent	O
and	O
convolutional	O
layers	O
.	O
Huang	O
et	O
al	O
(	O
2018	O
)	O
use	O
an	O
attentionlike	O
gating	O
mechanism	O
to	O
alleviate	O
an	O
assumption	O
of	O
monotonic	O
alignment	O
in	O
the	O
phrase	O
-	O
based	O
translation	O
model	O
of	O
.	O
treat	O
the	O
sentence	O
alignment	O
as	O
a	O
latent	O
variable	O
which	O
they	O
infer	O
using	O
a	O
variational	B-MethodName
inference	I-MethodName
network	O
during	O
training	O
to	O
optimize	O
a	O
variational	O
lower	O
-	O
bound	O
on	O
the	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
.	O
Beyond	O
uni	O
-	O
dimensional	O
encoding	O
/	O
decoding	O
.	O
proposed	O
a	O
2D	O
LSTM	B-MethodName
model	O
similar	O
to	O
our	O
2D	O
CNN	O
for	O
machine	B-TaskName
translation	I-TaskName
.	O
Like	O
our	O
model	O
,	O
a	O
2D	O
grid	O
is	O
defined	O
across	O
the	O
input	O
and	O
output	O
sequences	O
,	O
as	O
in	O
Figure	O
1	O
.	O
In	O
their	O
model	O
,	O
each	O
cell	O
takes	O
input	O
from	O
its	O
left	O
and	O
bottom	O
neighbor	O
.	O
In	O
a	O
second	O
LSTM	B-MethodName
stream	O
,	O
each	O
cell	O
takes	O
input	O
from	O
its	O
left	O
and	O
top	O
neighbor	O
,	O
as	O
well	O
as	O
from	O
the	O
corresponding	O
cell	O
in	O
the	O
first	O
stream	O
.	O
They	O
also	O
observed	O
that	O
such	O
a	O
structure	O
implements	O
an	O
implicit	O
form	O
of	O
attention	O
,	O
by	O
producing	O
an	O
input	O
encoding	O
that	O
depends	O
on	O
the	O
output	O
sequence	O
produced	O
so	O
far	O
.	O
Wu	O
et	O
al	O
(	O
2017	O
)	O
used	O
a	O
CNN	O
over	O
the	O
2D	O
source	O
-	O
target	O
representation	O
as	O
in	O
our	O
work	O
,	O
but	O
only	O
as	O
a	O
discriminator	O
in	O
an	O
adversarial	O
training	O
setup	O
.	O
They	O
do	O
not	O
use	O
masked	O
convolutions	O
,	O
since	O
their	O
CNN	O
is	O
used	O
to	O
predict	O
if	O
a	O
given	O
sourcetarget	O
pair	O
is	O
a	O
human	O
or	O
machine	B-TaskName
translation	I-TaskName
.	O
A	O
standard	O
encoder	O
-	O
decoder	O
model	O
with	O
attention	O
is	O
used	O
to	O
generate	O
translations	O
.	O

We	O
use	O
the	O
DenseNet	B-MethodName
convolutional	O
architecture	O
,	O
which	O
is	O
the	O
state	O
of	O
the	O
art	O
for	O
image	B-TaskName
classification	I-TaskName
tasks	O
.	O
Layers	O
are	O
densely	O
connected	O
,	O
meaning	O
that	O
each	O
layer	O
takes	O
as	O
input	O
the	O
activations	O
of	O
all	O
the	O
preceding	O
layers	O
,	O
rather	O
than	O
just	O
the	O
last	O
one	O
,	O
to	O
produce	O
its	O
g	O
feature	O
maps	O
.	O
The	O
parameter	O
g	O
is	O
called	O
the	O
"	O
growth	O
rate	O
"	O
as	O
it	O
is	O
the	O
number	O
of	O
appended	O
channels	O
to	O
the	O
network	O
's	O
output	O
at	O
each	O
layer	O
.	O
The	O
long	O
-	O
distance	O
connections	O
in	O
the	O
network	O
improve	O
gradient	O
flow	O
to	O
early	O
network	O
layers	O
during	O
training	O
,	O
which	O
is	O
beneficial	O
for	O
deeper	O
networks	O
.	O
Each	O
layer	O
first	O
batch	O
-	O
normalizes	O
(	O
Ioffe	O
and	O
Szegedy	O
,	O
2015	O
)	O
its	O
input	O
and	O
apply	O
a	O
ReLU	B-MethodName
(	O
Nair	O
and	O
Hinton	O
,	O
2010	O
)	O
non	O
-	O
linearity	O
.	O
To	O
reduce	O
the	O
computation	O
cost	O
,	O
each	O
layer	O
first	O
computes	O
4	O
g	O
channels	O
using	O
a	O
1×1	O
convolution	B-MethodName
from	O
the	O
f	O
0	B-DatasetName
+	O
(	O
l	O
−	O
1	O
)	O
g	O
input	O
channels	O
to	O
layer	O
l	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
L	O
}	O
.	O
This	O
is	O
followed	O
by	O
a	O
second	O
batch	O
-	O
normalization	O
and	O
ReLU	B-MethodName
non	O
-	O
linearity	O
.	O
The	O
second	O
convolution	B-MethodName
has	O
(	O
k	O
×	O
k	O
2	O
)	O
kernels	O
,	O
i.e.	O
masked	O
as	O
illustrated	O
in	O
Figure	O
1	O
,	O
and	O
generates	O
the	O
g	O
output	O
features	O
maps	O
to	O
which	O
we	O
apply	O
dropout	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
architecture	O
of	O
the	O
densely	O
connected	O
network	O
is	O
illustrated	O
in	O
Figure	O
3	O
.	O
We	O
optionally	O
use	O
gated	O
linear	O
units	O
(	O
Dauphin	O
et	O
al	O
,	O
2017	O
)	O
Target	O
sequence	O
prediction	O
.	O
Starting	O
from	O
the	O
initial	O
f	O
0	B-DatasetName
feature	O
maps	O
,	O
each	O
layer	O
l	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
L	O
}	O
of	O
our	O
DenseNet	B-MethodName
produces	O
a	O
tensor	O
H	O
l	O
of	O
size	O
|	O
t	O
|	O
×	O
|	O
s	O
|	O
×	O
f	O
l	O
,	O
where	O
f	O
l	O
is	O
the	O
number	O
of	O
output	O
channels	O
of	O
that	O
layer	O
.	O
To	O
compute	O
a	O
distribution	O
over	O
the	O
tokens	O
in	O
the	O
output	O
vocabulary	O
,	O
we	O
need	O
to	O
collapse	O
the	O
second	O
dimension	O
of	O
the	O
tensor	O
,	O
which	O
is	O
given	O
by	O
the	O
variable	O
length	O
of	O
the	O
input	O
sequence	O
,	O
to	O
retrieve	O
a	O
unique	O
encoding	O
for	O
each	O
target	O
position	O
.	O
The	O
simplest	O
aggregation	O
approach	O
is	O
to	O
apply	O
max	O
-	O
pooling	O
over	O
the	O
input	O
sequence	O
to	O
obtain	O
a	O
tensor	O
H	O
pool	O
R	O
|	O
t	O
|	O
×f	O
L	O
,	O
i.e.	O
H	O
pool	O
i	O
d	O
=	O
max	O
j	O
{	O
1	O
,	O
...	O
,	O
|	O
s	O
|	O
}	O
H	O
L	O
ijd	O
.	O
(	O
2	O
)	O
Alternatively	O
,	O
we	O
can	O
use	O
average	O
-	O
pooling	O
over	O
the	O
input	O
sequence	O
:	O
H	O
pool	O
i	O
d	O
=	O
1	O
|	O
s	O
|	O
j	O
{	O
1	O
,	O
...	O
,	O
|	O
s	O
|	O
}	O
H	O
L	O
ijd	O
.	O
(	O
3	O
)	O
The	O
scaling	O
with	O
the	O
inverse	O
square	O
-	O
root	O
of	O
the	O
source	O
length	O
acts	O
as	O
a	O
variance	O
stabilization	O
term	O
,	O
which	O
we	O
find	O
to	O
be	O
more	O
effective	O
in	O
practice	O
than	O
a	O
simple	O
averaging	O
.	O
The	O
pooled	O
features	O
are	O
then	O
transformed	O
to	O
predictions	O
over	O
the	O
output	O
vocabulary	O
V	O
,	O
by	O
linearly	O
mapping	O
them	O
with	O
a	O
matrix	O
E	O
R	O
|	O
V	O
|	O
×f	O
L	O
to	O
the	O
vocabulary	O
dimension	O
|	O
V	O
|	O
,	O
and	O
then	O
applying	O
a	O
soft	O
-	O
max	O
.	O
Thus	O
the	O
probability	O
distribution	O
over	O
V	O
for	O
the	O
i	O
-	O
th	O
output	O
token	O
is	O
obtained	O
as	O
p	O
i	O
=	O
SoftMax	B-MethodName
(	O
EH	O
pool	O
i	O
)	O
.	O
(	O
4	O
)	O
Alternatively	O
,	O
we	O
can	O
use	O
E	O
to	O
project	O
to	O
dimension	O
d	O
t	O
,	O
and	O
then	O
multiply	O
with	O
the	O
target	O
word	O
embedding	O
matrix	O
used	O
to	O
define	O
the	O
input	O
tensor	O
.	O
This	O
reduces	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
and	O
generally	O
improves	O
the	O
performance	O
.	O
Implicit	O
sentence	O
alignment	O
.	O
For	O
a	O
given	O
output	O
token	O
position	O
i	O
,	O
the	O
max	O
-	O
pooling	O
operator	O
of	O
Eq	O
.	O
(	O
2	O
)	O
partitions	O
the	O
f	O
L	O
channels	O
by	O
assigning	O
them	O
across	O
the	O
source	O
tokens	O
j.	O
Let	O
us	O
define	O
B	O
ij	O
=	O
{	O
d	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
f	O
L	O
}	O
|	O
j	O
=	O
arg	O
max	O
(	O
H	O
L	O
ijd	O
)	O
}	O
as	O
the	O
channels	O
assigned	O
to	O
source	O
token	O
j	O
for	O
output	O
token	O
i.	O
The	O
energy	O
that	O
enters	O
into	O
the	O
softmax	B-MethodName
to	O
predict	O
token	O
w	O
V	O
for	O
the	O
i	O
-	O
th	O
output	O
position	O
is	O
given	O
by	O
e	O
iw	O
=	O
d	O
{	O
1	O
,	O
...	O
,	O
f	O
L	O
}	O
E	O
wd	O
H	O
pool	O
i	O
d	O
(	O
5	O
)	O
=	O
j	O
{	O
1	O
,	O
...	O
,	O
|	O
s	O
|	O
}	O
d	O
B	O
ij	O
E	O
wd	O
H	O
L	O
ijd	O
.	O
(	O
6	O
)	O
The	O
total	O
contribution	O
of	O
the	O
j	O
-	O
th	O
input	O
token	O
is	O
thus	O
given	O
by	O
α	B-HyperparameterName
ij	O
=	O
d	O
B	O
ij	O
E	O
wd	O
H	O
L	O
ijd	O
,	O
(	O
7	O
)	O
where	O
we	O
dropped	O
the	O
dependence	O
on	O
w	O
for	O
simplicity	O
.	O
As	O
we	O
will	O
show	O
experimentally	O
in	O
the	O
next	O
section	O
,	O
visualizing	O
the	O
values	O
α	B-HyperparameterName
ij	O
for	O
the	O
groundtruth	O
output	O
tokens	O
,	O
we	O
can	O
recover	O
an	O
implicit	O
sentence	O
alignment	O
used	O
by	O
the	O
model	O
.	O

Data	O
and	O
pre	O
-	O
processing	O
.	O
We	O
experiment	O
with	O
the	O
IWSLT	O
2014	O
bilingual	O
dataset	O
(	O
Cettolo	O
et	O
al	O
,	O
2014	O
)	O
,	O
which	O
contains	O
transcripts	O
of	O
TED	O
talks	O
aligned	O
at	O
sentence	O
level	O
,	O
and	O
translate	O
between	O
German	O
(	O
De	O
)	O
and	O
English	O
(	O
En	O
)	O
in	O
both	O
directions	O
.	O
Following	O
the	O
setup	O
of	O
(	O
Edunov	O
et	O
al	O
,	O
2018	O
)	O
,	O
sentences	O
longer	O
than	O
175	O
tokens	O
and	O
pairs	O
with	O
length	O
ratio	O
exceeding	O
1.5	O
were	O
removed	O
from	O
the	O
original	O
data	O
.	O
There	O
are	O
160	O
+	O
7	O
K	O
training	O
sentence	O
pairs	O
,	O
7	O
K	O
of	O
which	O
are	O
separated	O
and	O
used	O
for	O
validation	O
/	O
development	O
.	O
We	O
report	O
results	O
on	O
a	O
test	O
set	O
of	O
6	O
,	O
578	O
pairs	O
obtained	O
by	O
concatenating	O
dev2010	O
and	O
tst2010	O
-	O
2013	O
.	O
We	O
tokenized	O
and	O
lowercased	O
all	O
data	O
using	O
the	O
standard	O
scripts	O
from	O
the	O
Moses	O
toolkit	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
.	O
For	O
open	O
-	O
vocabulary	O
translation	O
,	O
we	O
segment	O
sequences	O
using	O
joint	O
byte	B-MethodName
pair	I-MethodName
encoding	I-MethodName
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
with	O
14	O
K	O
merge	O
operations	O
on	O
the	O
concatenation	O
of	O
source	O
and	O
target	O
languages	O
.	O
This	O
results	O
in	O
a	O
German	O
and	O
English	O
vocabularies	O
of	O
around	O
12	O
K	O
and	O
9	O
K	O
types	O
respectively	O
.	O
Implementation	O
details	O
.	O
Unless	O
stated	O
otherwise	O
,	O
we	O
use	O
DenseNets	O
with	O
masked	O
convolutional	O
filters	O
of	O
size	O
5	O
×	O
3	O
,	O
as	O
given	O
by	O
the	O
light	O
blue	O
area	O
in	O
Figure	O
1	O
.	O
To	O
train	O
our	O
models	O
,	O
we	O
use	O
maximum	O
likelihood	O
estimation	O
(	O
MLE	O
)	O
with	O
Adam	B-MethodName
(	O
β	B-HyperparameterName
1	O
=	O
0.9	O
,	O
β	B-HyperparameterName
2	O
=	O
0.999	O
,	O
=	O
1e	O
−8	O
)	O
starting	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e	O
−4	O
that	O
we	O
scale	O
by	O
a	O
factor	O
of	O
0.8	O
if	O
no	O
improvement	O
(	O
δ	B-HyperparameterName
≤	O
0.01	O
)	O
is	O
noticed	O
on	O
the	O
validation	O
loss	B-MetricName
after	O
three	O
evaluations	O
,	O
we	O
evaluate	O
every	O
8	O
K	O
updates	O
.	O
After	O
training	O
all	O
models	O
up	O
to	O
40	O
epochs	O
,	O
the	O
best	O
performing	O
model	O
on	O
the	O
validation	O
set	O
is	O
used	O
for	O
decoding	O
the	O
test	O
set	O
.	O
We	O
use	O
a	O
beam	O
-	O
search	O
of	O
width	O
5	O
without	O
any	O
length	O
or	O
coverage	O
penalty	O
and	O
measure	O
translation	O
quality	O
using	O
the	O
BLEU	B-MetricName
metric	O
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
.	O
Baselines	O
.	O
For	O
comparison	O
with	O
state	O
-	O
of	O
-	O
theart	O
architectures	O
,	O
we	O
implemented	O
a	O
bidirectional	B-MethodName
LSTM	I-MethodName
encoder	O
-	O
decoder	O
model	O
with	O
dotproduct	O
attention	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
;	O
Luong	O
et	O
al	O
,	O
2015	O
)	O
using	O
PyTorch	O
(	O
Paszke	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
used	O
Facebook	O
AI	O
Research	O
Sequence	O
-	O
to	O
-	O
Sequence	O
Toolkit	O
(	O
Gehring	O
et	O
al	O
,	O
2017b	O
)	O
to	O
train	O
the	O
ConvS2S	O
and	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
models	O
on	O
our	O
data	O
.	O
For	O
the	O
Bi	O
-	O
LSTM	B-MethodName
encoder	O
-	O
decoder	O
,	O
the	O
encoder	O
is	O
a	O
single	O
layer	O
bidirectional	B-MethodName
LSTM	I-MethodName
with	O
input	O
embeddings	O
of	O
size	O
128	O
and	O
a	O
hidden	O
state	O
of	O
size	O
with	O
different	O
pooling	O
operators	O
and	O
using	O
gated	O
convolutional	O
units	O
.	O

)	O
.	O
The	O
decoder	O
is	O
a	O
single	O
layer	O
LSTM	B-MethodName
with	O
similar	O
input	O
size	O
and	O
a	O
hidden	O
size	O
of	O
256	O
,	O
the	O
target	O
input	O
embeddings	O
are	O
also	O
used	O
in	O
the	O
pre	O
-	O
softmax	B-MethodName
projection	O
.	O
For	O
regularization	O
,	O
we	O
apply	O
a	O
dropout	O
of	O
rate	O
0.2	O
to	O
the	O
inputs	O
of	O
both	O
encoder	O
and	O
decoder	O
and	O
to	O
the	O
output	O
of	O
the	O
decoder	O
prior	O
to	O
softmax	B-MethodName
.	O
As	O
in	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
,	O
we	O
refer	O
to	O
this	O
model	O
as	O
RNNsearch	O
.	O
The	O
ConvS2S	O
model	O
we	O
trained	O
has	O
embeddings	O
of	O
dimension	O
256	O
,	O
a	O
16	O
-	O
layers	O
encoder	O
and	O
12	O
-	O
layers	O
decoder	O
.	O
Each	O
convolution	B-MethodName
uses	O
3×1	O
filters	O
and	O
is	O
followed	O
by	O
a	O
gated	B-MethodName
linear	I-MethodName
unit	I-MethodName
with	O
a	O
total	O
of	O
2	O
×	O
256	O
channels	O
.	O
Residual	O
connections	O
link	O
the	O
input	O
of	O
a	O
convolutional	O
block	O
to	O
its	O
output	O
.	O
We	O
first	O
trained	O
the	O
default	O
architecture	O
for	O
this	O
dataset	O
as	O
suggested	O
in	O
FairSeq	O
(	O
Gehring	O
et	O
al	O
,	O
2017b	O
)	O
,	O
which	O
has	O
only	O
4	O
layers	O
in	O
the	O
encoder	O
and	O
3	O
in	O
the	O
decoder	O
,	O
but	O
achieved	O
better	O
results	O
with	O
the	O
deeper	O
version	O
described	O
above	O
.	O
The	O
model	O
is	O
trained	O
with	O
MLE	O
using	O
Nesterov	B-MethodName
accelerated	I-MethodName
gradient	I-MethodName
with	O
a	O
momentum	O
of	O
0.99	O
and	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.25	O
decaying	O
by	O
a	O
factor	O
of	O
0.1	O
every	O
epoch	O
.	O
ConvS2S	O
is	O
also	O
regularized	O
with	O
a	O
dropout	O
rate	O
of	O
0.2	O
.	O
For	O
the	O
transformer	O
model	O
,	O
use	O
the	O
settings	O
of	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
We	O
use	O
token	O
embeddings	O
of	O
dimension	O
512	O
,	O
and	O
the	O
encoder	O
and	O
decoder	O
have	O
6	O
layers	O
and	O
8	O
attention	O
heads	O
.	O
For	O
the	O
inner	O
layer	O
in	O
the	O
per	O
-	O
position	O
feed	O
-	O
forawrd	O
network	O
we	O
use	O
d	O
f	O
f	O
=	O
2048	B-DatasetName
.	O
For	O
MLE	O
training	O
we	O
use	O
Adam	B-MethodName
(	O
β	B-HyperparameterName
1	O
=	O
0.9	O
,	O
β	B-HyperparameterName
2	O
=	O
0.98	O
,	O
=	O
1e	O
−8	O
)	O
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
,	O
and	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
starting	O
from	O
1e	O
−5	O
that	O
is	O
increased	O
during	O
4	O
,	O
000	O
warm	O
-	O
up	O
steps	O
then	O
used	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e	O
−4	O
that	O
follows	O
an	O
inverse	O
-	O
square	O
-	O
root	O
schedule	O
afterwards	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Similar	O
to	O
previous	O
models	O
we	O
set	O
the	O
dropout	O
rate	O
to	O
0.2	O
.	O

Architecture	O
evaluation	O
.	O
In	O
this	O
section	O
we	O
explore	O
the	O
impact	O
of	O
several	O
parameters	O
of	O
our	O
model	O
:	O
the	O
token	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
,	O
depth	O
,	O
growth	O
rate	O
and	O
filter	O
sizes	O
.	O
We	O
also	O
evaluate	O
different	O
aggregation	O
mechanisms	O
across	O
the	O
source	O
dimension	O
:	O
max	O
-	O
pooling	O
,	O
average	O
-	O
pooling	O
,	O
and	O
attention	O
.	O
In	O
each	O
chosen	O
setting	O
,	O
we	O
train	O
five	O
models	O
with	O
different	O
initializations	O
and	O
report	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
the	O
BLEU	B-MetricName
scores	O
.	O
We	O
also	O
state	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
of	O
each	O
model	O
and	O
the	O
computational	O
cost	O
of	O
training	O
,	O
estimated	O
in	O
a	O
similar	O
way	O
as	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
,	O
based	O
on	O
the	O
wall	O
clock	O
time	O
of	O
training	O
and	O
the	O
GPU	O
single	O
precision	O
specs	O
.	O
In	O
Table	O
1	O
we	O
see	O
that	O
using	O
max	O
-	O
pooling	O
instead	O
average	O
-	O
pooling	O
across	O
the	O
source	O
dimension	O
increases	O
the	O
performance	O
with	O
around	O
2	O
BLEU	B-MetricName
points	O
.	O
Scaling	O
the	O
average	O
representation	O
with	O
|	O
s	O
|	O
Eq	O
.	O
(	O
3	O
)	O
helped	O
improving	O
the	O
performance	O
but	O
it	O
is	O
still	O
largely	O
outperformed	O
by	O
the	O
max	O
-	O
pooling	O
.	O
Adding	O
gated	O
linear	O
units	O
on	O
top	O
of	O
each	O
convolutional	O
layer	O
does	O
not	O
improve	O
the	O
BLEU	B-MetricName
scores	O
,	O
but	O
increases	O
the	O
variance	O
due	O
to	O
the	O
additional	O
parameters	O
.	O
Stand	O
-	O
alone	O
selfattention	O
i.e.	O
weighted	O
average	O
-	O
pooling	O
is	O
slightly	O
better	O
than	O
uniform	O
average	O
-	O
pooling	O
but	O
it	O
is	O
still	O
outperformed	O
by	O
max	O
-	O
pooling	O
.	O
Concatenating	O
the	O
max	O
-	O
pooled	O
features	O
(	O
Eq	O
.	O
(	O
2	O
)	O
)	O
with	O
the	O
representation	O
obtained	O
with	O
self	O
-	O
attention	O
(	O
Eq	O
.	O
(	O
9	O
)	O
)	O
leads	O
to	O
a	O
small	O
but	O
significant	O
increase	O
in	O
performance	O
,	O
from	O
33.70	O
to	O
33.81	O
.	O
In	O
the	O
remainder	O
of	O
our	O
experiments	O
we	O
only	O
use	O
max	O
-	O
pooling	O
for	O
simplicity	O
,	O
unless	O
stated	O
otherwise	O
.	O
In	O
Figure	O
4	O
we	O
consider	O
the	O
effect	O
of	O
the	O
token	O
embedding	O
size	O
,	O
the	O
growth	O
rate	O
of	O
the	O
network	O
,	O
and	O
its	O
depth	O
.	O
The	O
token	O
embedding	O
size	O
together	O
with	O
the	O
growth	O
rate	O
g	O
control	O
the	O
number	O
of	O
features	O
that	O
are	O
passed	O
though	O
the	O
pooling	O
operator	O
along	O
the	O
source	O
dimension	O
,	O
and	O
that	O
can	O
be	O
used	O
used	O
for	O
token	O
prediction	O
.	O
Using	O
the	O
same	O
embedding	O
size	O
d	O
=	O
d	O
t	O
=	O
d	O
s	O
on	O
both	O
source	O
and	O
target	O
,	O
the	O
total	O
number	O
of	O
features	O
for	O
token	O
prediction	O
produced	O
by	O
the	O
network	O
is	O
f	O
L	O
=	O
2d	O
+	O
gL.	O
In	O
Figure	O
4	O
we	O
see	O
that	O
for	O
token	O
embedding	O
sizes	O
between	O
128	O
to	O
256	O
lead	O
to	O
BLEU	B-MetricName
scores	O
vary	O
between	O
33.5	O
and	O
34	O
.	O
Smaller	O
embedding	O
sizes	O
quickly	O
degrade	O
the	O
performance	O
to	O
32.2	O
for	O
embeddings	O
of	O
size	O
64	O
.	O
The	O
growth	O
rate	O
(	O
g	O
)	O
has	O
an	O
important	O
impact	O
on	O
performance	O
,	O
increasing	O
it	O
from	O
8	O
to	O
32	O
increases	O
the	O
BLEU	B-MetricName
scrore	O
by	O
more	O
than	O
2.5	O
point	O
.	O
Beyond	O
g	O
=	O
32	O
performance	O
saturates	O
and	O
we	O
observe	O
only	O
a	O
small	O
improvement	O
.	O
For	O
a	O
good	O
trade	O
-	O
off	O
between	O
performance	O
and	O
computational	O
cost	O
we	O
choose	O
g	O
=	O
32	O
for	O
the	O
remaining	O
experiments	O
.	O
The	O
depth	O
of	O
the	O
network	O
also	O
has	O
an	O
important	O
impact	O
on	O
performance	O
,	O
increasing	O
the	O
BLEU	B-MetricName
score	I-MetricName
by	O
about	O
2	O
points	O
when	O
increasing	O
the	O
depth	O
from	O
8	O
to	O
24	O
layers	O
.	O
Beyond	O
this	O
point	O
performance	O
drops	O
due	O
to	O
over	O
-	O
fitting	O
,	O
which	O
means	O
we	O
should	O
either	O
increase	O
the	O
dropout	O
rate	O
or	O
add	O
another	O
level	O
of	O
regularization	O
before	O
considering	O
deeper	O
networks	O
.	O
The	O
receptive	O
field	O
of	O
our	O
model	O
is	O
controlled	O
by	O
its	O
depth	O
and	O
the	O
filter	O
size	O
.	O
In	O
Table	O
2	O
,	O
we	O
note	O
that	O
narrower	O
receptive	O
fields	O
are	O
better	O
than	O
larger	O
ones	O
with	O
less	O
layers	O
at	O
equivalent	O
complextities	O
e.g.	O
comparing	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
,	O
L	O
=	O
20	O
)	O
to	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
5	O
,	O
L	O
=	O
12	O
)	O
,	O
and	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
5	O
,	O
L	O
=	O
16	O
)	O
with	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
7	O
,	O
L	O
=	O
12	O
)	O
.	O
Comparison	O
to	O
the	O
state	O
of	O
the	O
art	O
.	O
We	O
compare	O
our	O
results	O
to	O
the	O
state	O
of	O
the	O
art	O
in	O
Ta	O
-	O
ble	O
3	O
for	O
both	O
directions	O
German	O
-	O
English	O
(	O
De	O
-	O
En	O
)	O
and	O
English	O
-	O
German	O
(	O
En	O
-	O
De	O
)	O
.	O
We	O
refer	O
to	O
our	O
model	O
as	O
Pervasive	O
Attention	O
.	O
Unless	O
stated	O
otherwise	O
,	O
the	O
parameters	O
of	O
all	O
models	O
are	O
trained	O
using	O
maximum	O
likelihood	O
estimation	O
(	O
MLE	O
)	O
.	O
For	O
some	O
models	O
we	O
additionally	O
report	O
results	O
obtained	O
with	O
sequence	O
level	O
estimation	O
(	O
SLE	O
,	O
e.g.	O
using	O
reinforcement	O
learning	O
approaches	O
)	O
,	O
typically	O
aiming	O
directly	O
to	O
optimize	O
the	O
BLEU	B-MetricName
measure	O
rather	O
than	O
the	O
likelihood	O
of	O
correct	O
translation	O
.	O
First	O
of	O
all	O
we	O
find	O
that	O
all	O
results	O
obtained	O
using	O
byte	O
-	O
pair	O
encodings	O
(	O
BPE	B-MethodName
)	O
are	O
superior	O
to	O
wordbased	O
results	O
.	O
Our	O
model	O
has	O
about	O
the	O
same	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
as	O
RNNsearch	O
,	O
yet	O
improves	O
performance	O
by	O
almost	O
3	O
BLEU	B-MetricName
points	O
.	O
It	O
is	O
also	O
better	O
than	O
the	O
recent	O
work	O
of	O
on	O
recurrent	O
architectures	O
with	O
variational	O
attention	O
.	O
Our	O
model	O
outperforms	O
both	O
the	O
recent	O
transformer	O
approach	O
of	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
and	O
the	O
convolutional	O
model	O
of	O
Gehring	O
et	O
al	O
(	O
2017b	O
)	O
in	O
both	O
translation	O
directions	O
,	O
while	O
having	O
about	O
3	O
to	O
8	O
times	O
fewer	O
parameters	O
.	O
Our	O
model	O
has	O
an	O
equivalent	O
training	O
cost	O
to	O
the	O
transformer	O
(	O
as	O
implemented	O
in	O
fairseq	O
)	O
while	O
the	O
convs2s	O
implementation	O
is	O
well	O
optimized	O
with	O
fast	O
running	O
1dconvolutions	O
leading	O
to	O
shorter	O
training	O
times	O
.	O
Performance	O
across	O
sequence	O
lengths	O
.	O
In	O
Figure	O
5	O
we	O
consider	O
translation	O
quality	O
as	O
a	O
function	O
of	O
sentence	O
length	O
,	O
and	O
compare	O
our	O
model	O
to	O
RNNsearch	O
,	O
ConvS2S	O
and	O
Transformer	B-MethodName
.	O
Our	O
model	O
gives	O
the	O
best	O
results	O
across	O
all	O
sentence	O
lengths	O
,	O
except	O
for	O
the	O
longest	O
ones	O
where	O
ConvS2S	O
and	O
Transformer	B-MethodName
are	O
better	O
.	O
Overall	O
,	O
our	O
model	O
combines	O
the	O
strong	O
performance	O
of	O
RNNsearch	O
on	O
short	O
sentences	O
with	O
good	O
perfor	O
-	O
Implicit	O
sentence	O
alignments	O
.	O
Following	O
the	O
method	O
described	O
in	O
Section	O
3	O
,	O
we	O
illustrate	O
in	O
Figure	O
6	O
the	O
implicit	O
sentence	O
alignments	O
the	O
maxpooling	O
operator	O
produces	O
in	O
our	O
model	O
.	O
For	O
reference	O
we	O
also	O
show	O
the	O
alignment	O
produced	O
by	O
our	O
model	O
using	O
self	O
-	O
attention	O
.	O
We	O
see	O
that	O
with	O
both	O
max	O
-	O
pooling	O
and	O
attention	O
qualitatively	O
similar	O
implicit	O
sentence	O
alignments	O
emerge	O
.	O
Notice	O
in	O
the	O
first	O
example	O
how	O
the	O
max	O
-	O
pool	O
model	O
,	O
when	O
writing	O
I	O
've	O
been	O
working	O
,	O
looks	O
at	O
arbeite	O
but	O
also	O
at	O
seit	O
which	O
indicates	O
the	O
past	O
tense	O
of	O
the	O
former	O
.	O
Also	O
notice	O
some	O
cases	O
of	O
non	O
-	O
monotonic	O
alignment	O
.	O
In	O
the	O
first	O
example	O
for	O
some	O
time	O
occurs	O
at	O
the	O
end	O
of	O
the	O
English	O
sentence	O
,	O
but	O
seit	O
einiger	O
zeit	O
appears	O
earlier	O
in	O
the	O
German	O
source	O
.	O
For	O
the	O
second	O
example	O
there	O
is	O
non	O
-	O
monotonic	O
alignment	O
around	O
the	O
negation	O
at	O
the	O
start	O
of	O
the	O
sentence	O
.	O
The	O
first	O
example	O
illustrates	O
the	O
ability	O
of	O
the	O
model	O
to	O
translate	O
proper	O
names	O
by	O
breaking	O
them	O
down	O
into	O
BPE	O
units	B-MethodName
.	O
In	O
the	O
second	O
example	O
the	O
German	O
word	O
Karriereweg	O
is	O
broken	O
into	O
the	O
four	O
BPE	O
units	B-MethodName
karri	O
,	O
er	O
,	O
e	O
,	O
weg	O
.	O
The	O
first	O
and	O
the	O
fourth	O
are	O
mainly	O
used	O
to	O
produce	O
the	O
English	O
a	O
carreer	O
,	O
while	O
for	O
the	O
subsequent	O
path	O
the	O
model	O
looks	O
at	O
weg	O
.	O
Finally	O
,	O
we	O
can	O
observe	O
an	O
interesting	O
pattern	O
in	O
the	O
alignment	O
map	O
for	O
several	O
phrases	O
across	O
the	O
three	O
examples	O
.	O
A	O
rough	O
lower	O
triangular	O
pattern	O
is	O
observed	O
for	O
the	O
English	O
phrases	O
for	O
some	O
time	O
,	O
and	O
it	O
's	O
fantastic	O
,	O
and	O
it	O
's	O
not	O
,	O
a	O
little	O
step	O
,	O
and	O
in	O
that	O
direction	O
.	O
In	O
all	O
these	O
cases	O
the	O
phrase	O
seems	O
to	O
be	O
decoded	O
as	O
a	O
unit	O
,	O
where	O
features	O
are	O
first	O
taken	O
across	O
the	O
entire	O
corresponding	O
source	O
Figure	O
6	O
:	O
Implicit	O
BPE	O
token	B-MethodName
-	O
level	O
alignments	O
produced	O
by	O
our	O
Pervasive	O
Attention	O
model	O
.	O
For	O
the	O
maxpooling	O
aggregation	O
we	O
visualize	O
α	O
obtained	B-HyperparameterName
with	O
Eq	O
.	O
(	O
7	O
)	O
and	O
for	O
self	O
-	O
attention	O
the	O
weights	O
ρ	O
of	O
Eq	O
.	O
(	O
8	O
)	O
.	O
(	O
Bahdanau	O
et	O
al	O
,	O
2017	O
)	O
27.56	O
Bi	O
-	O
GRU	O
(	B-MethodName
MLE+SLE	O
)	O
(	O
Bahdanau	O
et	O
al	O
,	O
2017	O
)	O
28.53	O
Conv	O
-	O
LSTM	O
(	B-MethodName
deep+pos	O
)	O
(	O
Gehring	O
et	O
al	O
,	O
2017a	O
)	O
30.4	O
NPMT	O
+	O
language	O
model	O
(	O
Huang	O
et	O
al	O
,	O
2018	O
)	O
30.08	O
25.36	O

Our	O
NMT	O
systems	O
were	O
standard	O
base	O
Transformer	B-MethodName
models	O
trained	O
using	O
the	O
Marian	O
NMT	O
framework	O
(	O
Junczys	O
-	O
Dowmunt	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
trained	O
separate	O
,	O
unidirectional	O
models	O
for	O
each	O
language	O
pair	O
.	O
Hyperparameters	O
such	O
as	O
label	B-MethodName
smoothing	I-MethodName
,	O
dropout	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
,	O
number	O
of	O
encoder	O
/	O
decoder	O
layers	O
,	O
number	O
of	O
attention	O
heads	O
,	O
embedding	O
dimensionality	O
,	O
etc	O
.	O
,	O
were	O
held	O
fixed	O
across	O
all	O
language	O
pairs	O
.	O
The	O
validation	O
frequency	O
was	O
every	O
500	O
updates	O
,	O
and	O
training	O
was	O
continued	O
for	O
50	O
epochs	O
or	O
until	O
the	O
primary	O
validation	O
metric	O
(	O
ce	O
-	O
mean	O
-	O
words	O
,	O
or	O
mean	O
word	O
cross	O
-	O
entropy	O
score	O
)	O
failed	O
to	O
improve	O
for	O
five	O
consecutive	O
checkpoints	O
.	O
Our	O
models	O
were	O
trained	O
on	O
AWS	O
P3	B-DatasetName
instances	O
using	O
4	O
NVIDIA	O
Tesla	O
V100	O
GPUs	O
.	O

Our	O
results	O
show	O
that	O
for	O
most	O
language	O
pairs	O
,	O
a	O
shared	O
vocabulary	O
of	O
size	O
8	O
,	O
000	O
achieved	O
the	O
best	O
performance	O
.	O
For	O
the	O
Korean	O
Japanese	O
and	O
Japanese	O
Korean	O
language	O
pairs	O
,	O
using	O
a	O
vocabulary	O
size	O
of	O
32	O
,	O
000	O
produced	O
better	O
results	O
.	O
Using	O
a	O
split	O
vocabulary	O
for	O
these	O
language	O
pairs	O
also	O
resulted	O
in	O
better	O
performance	O
,	O
whereas	O
a	O
shared	O
vocabulary	O
was	O
advantageous	O
for	O
all	O
other	O
language	O
pairs	O
.	O
In	O
all	O
cases	O
,	O
the	O
inclusion	O
of	O
back	O
translated	O
training	O
data	O
resulted	O
in	O
higher	O
validation	O
scores	O
.	O
Table	O
1	O
shows	O
our	O
results	O
in	O
terms	O
of	O
BLEU	B-MetricName
scores	O
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
as	O
calculated	O
on	O
our	O
local	O
machines	O
.	O
Due	O
to	O
differences	O
in	O
processing	O
,	O
these	O
scores	O
do	O
not	O
match	O
the	O
scores	O
reported	O
by	O
the	O
Organizers	O
.	O

This	O
paper	O
describes	O
the	O
system	O
for	O
our	O
participation	O
of	O
team	O
Wanghao	O
-	O
ftd	O
-	O
SJTU	O
in	O
the	O
CoNLL	O
2017	O
Shared	O
Task	O
:	O
Multilingual	O
Parsing	O
from	O
Raw	O
Text	O
to	O
Universal	B-DatasetName
Dependencies	I-DatasetName
.	O
In	O
this	O
work	O
,	O
we	O
design	O
a	O
system	O
based	O
on	O
UDPipe	O
1	O
for	O
universal	O
dependency	B-TaskName
parsing	I-TaskName
,	O
where	O
transitionbased	O
models	O
are	O
trained	O
for	O
different	O
treebanks	O
.	O
Our	O
system	O
directly	O
takes	O
raw	O
texts	O
as	O
input	O
,	O
performing	O
several	O
intermediate	O
steps	O
like	O
tokenizing	O
and	O
tagging	O
,	O
and	O
finally	O
generates	O
the	O
corresponding	O
dependency	O
trees	O
.	O
For	O
the	O
special	O
surprise	O
languages	O
for	O
this	O
task	O
,	O
we	O
adopt	O
a	O
delexicalized	O
strategy	O
and	O
predict	O
based	O
on	O
transfer	B-TaskName
learning	I-TaskName
from	O
other	O
related	O
languages	O
.	O
In	O
the	O
final	O
evaluation	O
of	O
the	O
shared	O
task	O
,	O
our	O
system	O
achieves	O
a	O
result	O
of	O
66.53	O
%	O
in	O
macro	O
-	O
averaged	O
LAS	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
.	O

Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
UD	B-DatasetName
)	O
(	O
Nivre	O
et	O
al	O
,	O
2016	O
(	O
Nivre	O
et	O
al	O
,	O
,	O
2017b	O
and	O
universal	O
dependency	B-TaskName
parsing	I-TaskName
take	O
efforts	O
to	O
build	O
cross	O
-	O
linguistically	O
treebank	O
annotation	O
and	O
develop	O
cross	O
-	O
lingual	O
learning	O
to	O
parse	O
many	O
languages	O
even	O
low	O
-	O
resource	O
languages	O
.	O
Universal	B-DatasetName
Dependencies	I-DatasetName
release	O
2.0	O
2	O
(	O
Nivre	O
et	O
al	O
,	O
2017b	O
)	O
includes	O
rich	O
languages	O
and	O
treebanks	O
resources	O
and	O
the	O
parsing	O
task	O
in	O
CoNLL	O
2017	O
is	O
based	O
on	O
this	O
dataset	O
.	O
In	O
fact	O
,	O
dependency	B-TaskName
parsing	I-TaskName
has	O
been	O
adopted	O
as	O
topic	O
of	O
the	O
shared	O
task	O
in	O
CoNLL	O
-	O
X	O
andCoNLL	O
-	O
2007	O
(	O
Buchholz	O
andMarsi	O
,	O
2006	O
;	O
Nivre	O
et	O
al	O
,	O
2007	O
)	O
,	O
which	O
have	O
been	O
the	O
milestones	O
for	O
the	O
researching	O
field	O
of	O
parsing	O
.	O
This	O
time	O
,	O
the	O
task	O
is	O
taking	O
a	O
universal	O
annotation	O
version	O
and	O
trying	O
to	O
exploit	O
cross	O
-	O
linguistic	O
similarities	O
between	O
various	O
languages	O
.	O
In	O
this	O
paper	O
,	O
we	O
describe	O
the	O
system	O
of	O
team	O
Wanghao	O
-	O
ftd	O
-	O
SJTU	O
for	O
the	O
CoNLL	O
2017	O
Shared	O
Task	O
:	O
Multilingual	O
Parsing	O
from	O
Raw	O
Text	O
to	O
Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
Zeman	O
et	O
al	O
,	O
2017	O
)	O
.	O
For	O
this	O
task	O
,	O
we	O
only	O
use	O
provided	O
treebanks	O
to	O
train	O
models	O
without	O
any	O
other	O
resources	O
including	O
pretrained	O
embeddings	O
.	O
For	O
dependency	B-TaskName
parsing	I-TaskName
,	O
there	O
have	O
been	O
two	O
major	O
parsing	O
methods	O
:	O
graph	O
-	O
based	O
and	O
transition	O
-	O
based	O
.	O
The	O
former	O
searches	O
for	O
the	O
final	O
tree	O
through	O
graph	O
algorithms	O
by	O
decomposing	O
trees	O
into	O
factors	O
,	O
utilizing	O
ingenious	O
dynamic	O
programming	O
algorithms	O
(	O
Eisner	O
,	O
1996	O
;	O
McDonald	O
et	O
al	O
,	O
2005	O
;	O
McDonald	O
and	O
Pereira	O
,	O
2006	O
)	O
;	O
while	O
the	O
latter	O
parses	O
sentences	O
by	O
making	O
a	O
series	O
of	O
shift	O
-	O
reduce	O
decisions	O
(	O
Yamada	O
and	O
Matsumoto	O
,	O
2003	O
;	O
Nivre	O
,	O
2003	O
)	O
.	O
In	O
our	O
system	O
,	O
we	O
will	O
utilize	O
the	O
transition	O
-	O
based	O
system	O
for	O
its	O
simplicity	O
and	O
relatively	O
lower	O
computation	O
cost	O
.	O
Transition	B-TaskName
-	I-TaskName
based	I-TaskName
dependency	I-TaskName
parsing	I-TaskName
takes	O
linear	O
time	O
complexity	O
and	O
utilizes	O
rich	O
features	O
to	O
make	O
structural	O
prediction	O
(	O
Zhang	O
and	O
Clark	O
,	O
2008	O
;	O
Zhang	O
and	O
Nivre	O
,	O
2011	O
)	O
.	O
Specifically	O
,	O
a	O
buffer	O
for	O
input	O
words	O
,	O
a	O
stack	O
for	O
partially	O
built	O
structure	O
and	O
shift	O
-	O
reduce	O
actions	O
are	O
basic	O
elements	O
in	O
a	O
transition	B-TaskName
-	I-TaskName
based	I-TaskName
dependency	I-TaskName
parsing	I-TaskName
.	O
For	O
the	O
transition	O
systems	O
of	O
dependency	B-TaskName
parsing	I-TaskName
,	O
there	O
have	O
been	O
two	O
major	O
ones	O
:	O
arc	O
-	O
standard	O
and	O
arc	O
-	O
eager	O
(	O
Nivre	O
,	O
2008	O
)	O
.	O
Our	O
system	O
adopts	O
the	O
former	O
,	O
whose	O
basic	O
algorithm	O
can	O
be	O
described	O
as	O
following	O
:	O
Start	O
:	O
σ	O
=	O
[	O
ROOT	O
]	O
,	O
β	B-HyperparameterName
=	O
w	O
1	O
,	O
...	O
,	O
w	O
n	O
,	O
A	O
=	O
1	O
.	O
Shift	O
:	O
σ	O
,	O
w	O
i	O
|	O
β	B-HyperparameterName
,	O
A	O
σ	O
|	O
w	O
i	O
,	O
β	B-HyperparameterName
,	O
A	O
2	O
.	O
Left	O
-	O
Arc	O
r	O
:	O
σ	O
|	O
w	O
i	O
|	O
w	O
j	O
,	O
β	B-HyperparameterName
,	O
A	O
σ	O
|	O
w	O
j	O
,	O
β	B-HyperparameterName
,	O
A	O
∪	O
r	O
(	O
w	O
j	O
,	O
w	O
i	O
)	O
3	O
.	O
Right	O
-	O
Arc	O
r	O
:	O
σ	O
|	O
w	O
i	O
|	O
w	O
j	O
,	O
β	B-HyperparameterName
,	O
A	O
σ	O
|	O
w	O
i	O
,	O
β	B-HyperparameterName
,	O
A	O
∪	O
r	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
Finish	O
:	O
σ	O
=	O
[	O
w	O
]	O
,	O
β	B-HyperparameterName
=	O
where	O
σ	O
,	O
β	B-HyperparameterName
,	O
A	O
represent	O
the	O
stack	O
,	O
queue	O
and	O
the	O
actions	O
respectively	O
.	O
One	O
major	O
difference	O
for	O
parsing	O
between	O
the	O
situation	O
of	O
current	O
and	O
that	O
of	O
ten	O
years	O
ago	O
is	O
that	O
recently	O
we	O
have	O
seen	O
a	O
rising	O
of	O
neural	O
network	O
based	O
methods	O
in	O
the	O
field	O
of	O
Natural	O
Language	O
Processing	O
and	O
parsing	O
has	O
also	O
been	O
greatly	O
changed	O
by	O
the	O
neural	O
methods	O
.	O
With	O
distributed	O
representation	O
for	O
words	O
and	O
sentences	O
and	O
the	O
powerful	O
non	O
-	O
linear	O
calculation	O
ability	O
of	O
the	O
neural	O
networks	O
,	O
we	O
could	O
explore	O
deeper	O
syntactic	O
and	O
maybe	O
semantic	O
meaning	O
in	O
text	O
analysis	O
,	O
and	O
both	O
graph	O
-	O
based	O
(	O
Pei	O
et	O
al	O
,	O
2015	O
;	O
Wang	O
and	O
Chang	O
,	O
2016	O
)	O
and	O
transition	O
-	O
based	O
(	O
Chen	O
and	O
Manning	O
,	O
2014	O
;	O
Weiss	O
et	O
al	O
,	O
2015	O
;	O
Dyer	O
et	O
al	O
,	O
2015	O
;	O
Andor	O
et	O
al	O
,	O
2016	O
)	O
parsing	O
have	O
benefited	O
a	O
lot	O
from	O
neural	O
representation	O
learnings	O
.	O
In	O
our	O
system	O
,	O
the	O
model	O
,	O
which	O
is	O
trained	O
by	O
UDPipe	O
,	O
for	O
the	O
transition	O
action	O
predictor	O
is	O
also	O
based	O
on	O
neural	O
network	O
,	O
which	O
is	O
similar	O
to	O
the	O
one	O
of	O
Chen	O
and	O
Manning	O
(	O
2014	O
)	O
.	O
For	O
this	O
shared	O
task	O
,	O
our	O
system	O
is	O
built	O
based	O
on	O
UDpipe	O
(	O
Straka	O
et	O
al	O
,	O
2016	O
)	O
,	O
which	O
provides	O
a	O
pipeline	O
from	O
raw	O
text	O
to	O
dependency	O
structures	O
,	O
including	O
a	O
tokenizer	O
,	O
taggers	O
and	O
the	O
dependency	O
predictor	O
.	O
We	O
trained	O
and	O
tuned	O
the	O
models	O
on	O
different	O
treebanks	O
,	O
and	O
in	O
the	O
final	O
evaluation	O
,	O
a	O
score	O
of	O
66.53	O
%	O
in	O
macro	O
-	O
averaged	O
LAS	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
measurement	O
is	O
achieved	O
.	O
In	O
the	O
task	O
,	O
there	O
are	O
several	O
surprise	O
languages	O
which	O
lack	O
of	O
annotated	O
resources	O
,	O
which	O
means	O
it	O
is	O
hard	O
to	O
train	O
specified	O
models	O
for	O
those	O
languages	O
.	O
To	O
tackle	O
this	O
problem	O
,	O
we	O
exploit	O
the	O
universal	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
(	O
POS	O
)	O
tags	O
,	O
which	O
could	O
be	O
represented	O
as	O
crosslingual	O
knowledge	O
to	O
avoid	O
language	O
-	O
specific	O
information	O
,	O
and	O
adopting	O
a	O
delexicalized	O
and	O
crosslingual	O
method	O
,	O
which	O
relies	O
solely	O
on	O
universal	O
POS	O
tags	O
and	O
annotated	O
data	O
in	O
close	O
-	O
related	O
languages	O
.	O
The	O
rest	O
of	O
the	O
paper	O
is	O
organized	O
as	O
follows	O
:	O
Section	O
2	O
describes	O
our	O
system	O
overview	O
,	O
Section	O
3	O
elaborates	O
the	O
components	O
of	O
the	O
system	O
,	O
Section	O
4	O
shows	O
the	O
experiments	O
and	O
results	O
for	O
our	O
participation	O
in	O
the	O
shared	O
task	O
,	O
and	O
Section	O
5	O
concludes	O
this	O
paper	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
the	O
universal	O
dependency	O
parser	O
for	O
our	O
participation	O
in	O
the	O
CoNLL	O
2017	O
shared	O
task	O
.	O
The	O
official	O
evaluation	O
shows	O
that	O
our	O
system	O
achieves	O
66.53	O
%	O
in	O
macroaveraged	O
LAS	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
measurement	O
on	O
the	O
official	O
blind	O
test	O
.	O
Further	O
improvements	O
could	O
be	O
obtained	O
by	O
more	O
carefully	O
fine	O
-	O
tuning	O
models	O
and	O
adopting	O
more	O
sophisticated	O
neural	O
models	O
.	O

The	O
proposed	O
method	O
produces	O
pseudo	O
labelŝ	O
p	O
(	O
y	O
|	O
x	O
j	O
)	O
for	O
unlabeled	O
documents	O
{	O
x	O
j	O
}	O
m	O
j=1	O
.	O
When	O
true	O
labels	O
{	O
(	O
x	O
i	O
,	O
y	O
i	O
)	O
}	O
n	O
i=1	O
are	O
available	O
,	O
we	O
can	O
train	O
a	O
new	O
discriminative	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
p	O
θ	B-HyperparameterName
(	O
y	O
|	O
x	O
)	O
using	O
both	O
true	O
and	O
pseudo	O
labels	O
(	O
θ	B-HyperparameterName
is	O
the	O
model	O
parameter	O
)	O
:	O
J	O
(	O
θ	B-HyperparameterName
)	O
=	O
n	O
i=1	O
y	O
Y	O
−1	O
{	O
y	O
i	O
=	O
y	O
}	O
log	O
p	O
θ	B-HyperparameterName
(	O
y	O
|	O
x	O
i	O
)	O
+	O
λ	O
θ	B-HyperparameterName
2	O
+	O
µ	O
m	O
j=1	O
y	O
Y	O
−p	O
(	O
y	O
|	O
x	O
j	O
)	O
log	O
p	O
θ	B-HyperparameterName
(	O
y	O
|	O
x	O
j	O
)	O
.	O
(	O
5	O
)	O
To	O
find	O
the	O
balance	O
of	O
pseudo	O
vs.	O
true	O
labels	O
in	O
(	O
5	O
)	O
,	O
we	O
search	O
the	O
hyperparameter	O
µ	O
on	O
a	O
5point	O
grid	O
{	O
10	O
−2	O
,	O
10	O
−1	O
,	O
0.4	O
,	O
0.7	O
,	O
1	O
}	O
.	O
We	O
expect	O
pseudo	O
labels	O
to	O
have	O
comparable	O
importance	O
as	O
true	O
labels	O
when	O
n	O
is	O
small	O
(	O
fine	O
granularity	O
for	O
µ	O
[	O
10	O
−1	O
,	O
1	O
]	O
)	O
,	O
and	O
their	O
importance	O
will	O
diminish	O
as	O
n	O
gets	O
large	O
(	O
µ	O
=	O
10	O
−2	O
)	O
.	O
µ	O
is	O
automatically	O
selected	O
such	O
that	O
it	O
gives	O
the	O
best	O
5	O
-	O
fold	O
crossvalidation	O
accuracy	B-MetricName
on	O
n	O
true	O
labels	O
.	O

Retrieval	O
-	O
based	O
methods	O
.	O
We	O
use	O
language	O
modeling	O
retrieval	O
function	O
with	O
Dirichlet	O
smoothing	O
(	O
Zhai	O
and	O
Lafferty	O
,	O
2001	O
)	O
(	O
µ	O
=	O
2500	O
)	O
to	O
match	O
a	O
document	O
to	O
class	O
labels	O
(	O
IR	O
)	O
.	O
The	O
top	O
10	O
results	O
are	O
then	O
used	O
as	O
pseudo	O
-	O
labeled	O
documents	O
to	O
retrain	O
three	O
classifiers	O
:	O
IR+Roc	O
:	O
a	O
Rocchio	O
classifier	O
(	O
α	B-HyperparameterName
=	O
1	O
,	O
β	B-HyperparameterName
=	O
0.5	O
,	O
γ	B-HyperparameterName
=	O
0	B-DatasetName
)	O
;	O
IR+NB	O
:	O
a	O
multinomial	O
naive	O
Bayes	O
classifier	O
(	O
Laplace	O
smoothing	O
,	O
α	B-HyperparameterName
=	O
0.01	O
)	O
;	O
IR+LR	O
a	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
(	O
linear	O
kernel	O
,	O
C	O
=	O
1	O
)	O
.	O
Semi	O
-	O
supervised	O
methods	O
.	O
ST	O
-	O
0	B-DatasetName
:	O
the	O
initial	O
self	O
-	O
training	O
classifier	O
using	O
class	O
labels	O
as	O
"	O
training	O
documents	O
"	O
(	O
multinomial	O
naïve	O
Bayes	O
,	O
Laplace	O
smoothing	O
α	B-HyperparameterName
=	O
0.01	O
)	O
.	O
ST	O
-	O
1	O
:	O
ST	O
-	O
0	B-DatasetName
retrained	O
on	O
10	O
most	O
confident	O
documents	O
predicted	O
by	O
itself	O
.	O
GE	O
:	O
a	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
trained	O
using	O
generalized	O
expectation	O
criteria	O
(	O
Druck	O
et	O
al	O
,	O
2008	O
)	O
.	O
Class	O
labels	O
are	O
used	O
as	O
labeled	O
features	O
.	O
sLDA	O
:	O
a	O
supervised	O
topic	O
model	O
trained	O
using	O
seeded	O
LDA	B-MethodName
(	O
Jagarlamudi	O
et	O
al	O
,	O
2012	O
)	O
.	O
Besides	O
k	O
seeded	O
topics	O
(	O
k	O
is	O
the	O
number	O
of	O
classes	O
)	O
,	O
we	O
use	O
an	O
extra	O
topic	O
to	O
account	O
for	O
other	O
content	O
in	O
the	O
corpus	O
.	O
Word	O
embedding	O
-	O
based	O
methods	O
.	O
Cosine	O
:	O
a	O
centroid	O
-	O
based	O
classifier	O
,	O
where	O
class	O
definitions	O
and	O
documents	O
are	O
represented	O
as	O
average	O
of	O
word	O
vectors	O
.	O
WENB	O
:	O
The	O
proposed	O
method	O
(	O
Section	O
3	O
)	O
.	O
WENB+LR	O
:	O
a	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
trained	O
only	O
on	O
pseudo	O
labels	O
produced	O
by	O
WENB	O
(	O
Section	O
3.1	O
,	O
n	O
=	O
0	B-DatasetName
)	O
.	O
For	O
general	O
domain	O
tasks	O
,	O
we	O
take	O
raw	O
text	O
from	O
English	O
Wikipedia	O
,	O
English	O
news	O
crawl	O
(	O
WMT	O
,	O
2014	O
)	O
,	O
and	O
1	O
billion	O
word	O
news	O
corpus	O
(	O
Chelba	O
et	O
al	O
,	O
2013	O
)	O
to	O
train	O
word	O
vectors	O
.	O
For	O
medical	B-DatasetName
domain	I-DatasetName
tasks	O
,	O
we	O
take	O
raw	O
text	O
from	O
MEDLINE	O
abstracts	O
(	O
NLM	O
,	O
2018	O
)	O
to	O
train	O
word	O
vectors	O
.	O
We	O
find	O
50	O
-	O
dimensional	O
skip	O
-	O
gram	O
word	O
vectors	O
perform	O
reasonably	O
well	O
in	O
the	O
experiments	O
.	O

Label	O
savings	O
.	O
Table	O
2	O
shows	O
that	O
overall	O
,	O
class	O
labels	O
can	O
train	O
text	O
classifiers	O
remarkably	O
better	O
than	O
majority	O
guess	O
.	O
This	O
is	O
no	O
small	O
feat	O
considering	O
that	O
the	O
classifier	O
has	O
not	O
seen	O
any	O
labeled	O
documents	O
yet	O
.	O
Such	O
performance	O
gain	O
essentially	O
comes	O
"	O
for	O
free	O
"	O
,	O
as	O
any	O
text	B-TaskName
classification	I-TaskName
task	O
has	O
to	O
start	O
by	O
defining	O
classes	O
.	O
In	O
Table	O
3	O
,	O
we	O
report	O
the	O
number	O
of	O
true	O
labels	O
needed	O
for	O
a	O
logistic	B-MethodName
regression	I-MethodName
model	O
to	O
achieve	O
the	O
same	O
performance	O
as	O
WENB+LR	O
.	O
The	O
most	O
significant	O
savings	O
happen	O
on	O
short	O
documents	O
:	O
class	O
labels	O
are	O
equivalent	O
to	O
hundreds	O
to	O
thousands	O
of	O
labeled	O
documents	O
at	O
the	O
beginning	O
of	O
the	O
training	O
process	O
.	O
Effect	O
of	O
document	O
length	O
.	O
On	O
short	O
documents	O
(	O
Wiki	O
Titles	O
,	O
News	O
Titles	O
,	O
Y	O
Questions	O
)	O
,	O
leveraging	O
unlabeled	O
data	O
does	O
not	O
help	O
with	O
most	O
semi	O
-	O
supervised	O
methods	O
due	O
to	O
severe	O
vocabulary	O
mismatch	O
.	O
The	O
proposed	O
methods	O
(	O
WENB	O
and	O
WENB+LR	O
)	O
show	O
robust	O
performance	O
,	O
because	O
pretrained	O
word	O
vectors	O
can	O
capture	O
semantic	B-TaskName
similarity	I-TaskName
even	O
without	O
any	O
word	O
overlap	O
between	O
a	O
class	O
label	O
and	O
a	O
document	O
.	O
This	O
prior	O
knowledge	O
is	O
essential	O
when	O
documents	O
are	O
short	O
.	O
On	O
long	O
documents	O
(	O
20	O
News	O
,	O
Reuters	O
,	O
Med	O
WSD	O
)	O
,	O
leveraging	O
unlabeled	O
data	O
helps	O
,	O
since	O
long	O
documents	O
have	O
richer	O
content	O
and	O
are	O
more	O
likely	O
to	O
contain	O
not	O
only	O
label	O
words	O
themselves	O
,	O
but	O
also	O
other	O
topic	O
-	O
specific	O
words	O
.	O
Retrieval	O
-	O
based	O
and	O
semi	O
-	O
supervised	O
methods	O
are	O
able	O
to	O
learn	O
these	O
words	O
by	O
exploiting	O
intra	O
-	O
document	O
word	O
co	O
-	O
occurrences	O
.	O
Performance	O
of	O
other	O
methods	O
.	O
Learning	O
from	O
class	O
labels	O
themselves	O
provides	O
very	O
limited	O
help	O
(	O
IR	O
and	O
ST	O
-	O
0	B-DatasetName
)	O
.	O
Using	O
class	O
labels	O
as	O
search	O
queries	O
and	O
labeled	O
documents	O
are	O
closely	O
related	O
:	O
IR	O
and	O
ST	O
-	O
0	B-DatasetName
perform	O
similarly	O
;	O
so	O
do	O
IR+NB	O
and	O
ST	O
-	O
1	O
.	O
When	O
using	O
class	O
labels	O
as	O
search	O
queries	O
,	O
2	O
shows	O
a	O
salient	O
warm	O
-	O
start	O
effect	O
on	O
a	O
balanced	O
binary	O
classification	O
task	O
in	O
20	O
News	O
.	O
The	O
weight	O
µ	O
of	O
pseudo	O
labels	O
increases	O
when	O
true	O
labels	O
are	O
few	O
(	O
initial	O
classifier	O
as	O
an	O
informative	O
prior	O
)	O
.	O
As	O
expected	O
,	O
µ	O
decreases	O
when	O
true	O
labels	O
become	O
abundant	O
.	O
Figure	O
3	O
shows	O
another	O
binary	O
classification	O
task	O
in	O
20	O
News	O
where	O
the	O
warm	O
-	O
start	O
effect	O
is	O
limited	O
.	O
Correspondingly	O
,	O
µ	O
quickly	O
diminishes	O
as	O
more	O
true	O
labels	O
are	O
available	O
.	O
With	O
100	O
or	O
more	O
true	O
labels	O
,	O
pseudo	O
labels	O
have	O
a	O
negligible	O
weight	O
(	O
µ	O
=	O
10	O
−2	O
)	O
.	O
In	O
machine	O
learning	O
terms	O
,	O
these	O
pseudo	O
labels	O
specify	O
an	O
incorrect	O
prior	O
that	O
the	O
model	O
should	O
quickly	O
forget	O
,	O
so	O
that	O
it	O
will	O
not	O
hinder	O
the	O
overall	O
learning	O
process	O
.	O
A	O
closer	O
investigation	O
reveals	O
that	O
the	O
word	O
vector	O
for	O
mideast	O
(	O
the	O
class	O
label	O
of	O
one	O
topic	O
in	O
Figure	O
3	O
)	O
is	O
not	O
well	O
-	O
trained	O
.	O
This	O
is	O
because	O
in	O
general	O
text	O
corpus	O
,	O
the	O
word	O
mideast	O
is	O
rather	O
infrequent	O
compared	O
to	O
commonly	O
used	O
alternatives	O
,	O
such	O
as	O
middle	O
east	O
.	O
The	O
word	O
vector	O
of	O
mideast	O
is	O
surrounded	O
by	O
other	O
infrequent	O
words	O
or	O
misspellings	O
(	O
such	O
as	O
hizballah	O
,	O
jubeir	O
,	O
saudis	O
,	O
isreal	O
)	O
as	O
opposed	O
to	O
more	O
frequent	O
and	O
relevant	O
ones	O
(	O
such	O
as	O
israel	O
,	O
israeli	O
,	O
saudi	O
,	O
arab	O
)	O
.	O
Since	O
WENB	O
uses	O
the	O
semantic	O
knowledge	O
in	O
word	O
vectors	O
to	O
infer	O
pseudo	O
labels	O
,	O
the	O
quality	O
of	O
class	O
label	O
word	O
vectors	O
will	O
affect	O
the	O
pseudo	O
label	O
accuracy	B-MetricName
.	O

Slot	O
-	O
filling	O
,	O
Translation	B-TaskName
,	O
Intent	B-TaskName
classification	I-TaskName
,	O
and	O
Language	B-TaskName
identification	I-TaskName
,	O
or	O
STIL	O
,	O
is	O
a	O
newly	O
-	O
proposed	O
task	O
for	O
multilingual	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
(	O
NLU	O
)	O
.	O
By	O
performing	O
simultaneous	O
slot	B-TaskName
filling	I-TaskName
and	O
translation	O
into	O
a	O
single	O
output	O
language	O
(	O
English	O
in	O
this	O
case	O
)	O
,	O
some	O
portion	O
of	O
downstream	O
system	O
components	O
can	O
be	O
monolingual	O
,	O
reducing	O
development	O
and	O
maintenance	O
cost	O
.	O
Results	O
are	O
given	O
using	O
the	O
multilingual	O
BART	B-MethodName
model	O
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
fine	O
-	O
tuned	O
on	O
7	O
languages	O
using	O
the	O
MultiATIS++	O
dataset	O
.	O
When	O
no	O
translation	O
is	O
performed	O
,	O
mBART	B-MethodName
's	O
performance	O
is	O
comparable	O
to	O
the	O
current	O
state	O
of	O
the	O
art	O
system	O
(	O
Cross	O
-	O
Lingual	O
BERT	B-MethodName
by	O
Xu	O
et	O
al	O
(	O
2020	O
)	O
)	O
for	O
the	O
languages	O
tested	O
,	O
with	O
better	O
average	O
intent	B-TaskName
classification	I-TaskName
accuracy	B-MetricName
(	O
96.07	O
%	O
versus	O
95.50	O
%	O
)	O
but	O
worse	O
average	O
slot	O
F1	B-MetricName
(	O
89.87	O
%	O
versus	O
90.81	O
%	O
)	O
.	O
When	O
simultaneous	O
translation	O
is	O
performed	O
,	O
average	O
intent	B-TaskName
classification	I-TaskName
accuracy	B-MetricName
degrades	O
by	O
only	O
1.7	O
%	O
relative	O
and	O
average	O
slot	O
F1	B-MetricName
degrades	O
by	O
only	O
1.2	O
%	O
relative	O
.	O

The	O
multilingual	O
BART	B-MethodName
(	O
mBART	B-MethodName
)	O
model	O
architecture	O
was	O
used	O
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
,	O
as	O
well	O
as	O
the	O
pretrained	O
mBART.cc25	O
model	O
described	O
in	O
the	O
same	O
paper	O
.	O
The	O
model	O
consists	O
of	O
12	O
encoder	O
layers	O
,	O
12	O
decoder	O
layers	O
,	O
a	O
hidden	B-HyperparameterName
layer	I-HyperparameterName
size	I-HyperparameterName
of	O
1	O
,	O
024	O
,	O
and	O
16	O
attention	O
heads	O
,	O
yielding	O
a	O
parameter	O
count	O
of	O
680M.	O
The	O
mBART.cc25	O
model	O
was	O
trained	O
on	O
25	O
languages	O
for	O
500k	O
steps	O
using	O
a	O
1.4	O
TB	O
corpus	O
of	O
scraped	O
website	O
data	O
taken	O
from	O
Common	B-DatasetName
Crawl	I-DatasetName
(	O
Wenzek	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
model	O
was	O
trained	O
to	O
reconstruct	O
masked	O
tokens	O
and	O
to	O
rearrange	O
scrambled	O
sentences	O
.	O
SentencePiece	B-MethodName
tokenization	O
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
was	O
used	O
for	O
mBART.cc25	O
with	O
a	O
sub	O
-	O
word	O
vocabulary	O
size	O
of	O
250k	O
.	O

The	O
same	O
vocabulary	O
as	O
that	O
of	O
the	O
pretrained	O
model	O
was	O
used	O
for	O
this	O
work	O
,	O
and	O
SentencePiece	B-MethodName
tokenization	O
was	O
performed	O
on	O
the	O
full	O
sequence	O
,	O
including	O
the	O
slot	O
tags	O
,	O
intent	O
tags	O
,	O
and	O
language	O
tags	O
.	O
For	O
all	O
mBART	B-MethodName
experiments	O
and	O
datasets	O
,	O
data	O
from	O
all	O
languages	O
were	O
shuffled	O
together	O
.	O
The	O
fairseq	O
library	O
was	O
used	O
for	O
all	O
experimentation	O
(	O
Ott	O
et	O
al	O
,	O
2019	O
)	O
.	O
Training	O
was	O
performed	O
on	O
8	O
Nvidia	O
V100	O
GPUs	O
(	O
16	O
GB	O
)	O
using	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	O
,	O
layer	B-MethodName
normalization	I-MethodName
for	O
both	O
the	O
encoder	O
and	O
the	O
decoder	O
(	O
Xu	O
et	O
al	O
,	O
2019	O
)	O
;	O
label	O
smoothed	O
cross	O
entropy	O
with	O
=	O
0.2	O
(	O
Szegedy	O
et	O
al	O
,	O
2016	O
)	O
;	O
the	O
ADAM	B-DatasetName
optimizer	B-HyperparameterName
with	O
β	B-HyperparameterName
1	O
=	O
0.9	O
and	O
β	B-HyperparameterName
2	O
=	O
0.999	O
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
;	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3	O
×	O
10	O
−5	O
with	O
polynomial	O
decay	O
over	O
20	O
,	O
000	O
updates	O
after	O
1	O
epoch	O
of	O
warmup	O
;	O
attention	B-MethodName
dropout	I-MethodName
of	O
0.1	O
and	O
dropout	O
of	O
0.2	O
elsewhere	O
;	O
and	O
FP16	O
type	O
for	O
weights	O
.	O
Each	O
model	O
was	O
trained	O
for	O
19	O
epochs	O
,	O
which	O
took	O
5	O
-	O
6	O
hours	O
.	O

Results	O
from	O
the	O
models	O
are	O
given	O
in	O
Table	O
3	O
.	O
Statistical	O
significance	O
was	O
evaluated	O
using	O
the	O
Wilson	O
method	O
(	O
Wilson	O
,	O
1927	O
)	O
with	O
95	O
%	O
confidence	O
.	O
Xu	O
et	O
al	O
(	O
2020	O
)	O
Examining	O
the	O
first	O
training	O
configuration	O
(	O
1	O
,	O
496	O
samples	O
for	O
Hindi	O
and	O
626	O
for	O
Turkish	O
)	O
,	O
the	O
nontranslated	O
mBART	B-MethodName
's	O
macro	O
-	O
averaged	O
intent	B-TaskName
classification	I-TaskName
(	O
96.07	O
%	O
)	O
outperforms	O
Cross	O
-	O
Lingual	O
BERT	B-MethodName
by	O
Xu	O
et	O
al	O
(	O
2020	O
)	O
(	O
95.50	O
%	O
)	O
,	O
but	O
slot	O
F1	B-MetricName
is	O
worse	O
(	O
89.87	O
%	O
for	O
non	O
-	O
translated	O
mBART	B-MethodName
and	O
90.81	O
%	O
for	O
Cross	O
-	O
Lingual	O
BERT	B-MethodName
)	O
.	O
The	O
differences	O
are	O
statistically	O
significant	O
in	O
both	O
cases	O
.	O

When	O
translation	O
is	O
performed	O
(	O
the	O
STIL	O
task	O
)	O
,	O
intent	B-TaskName
classification	I-TaskName
accuracy	B-MetricName
degrades	O
by	O
1.7	O
%	O
relative	O
from	O
96.07	O
%	O
to	O
94.40	O
%	O
,	O
and	O
slot	O
F1	B-MetricName
degrades	O
by	O
1.2	O
%	O
relative	O
from	O
89.87	O
%	O
to	O
88.79	O
%	O
.	O
The	O
greatest	O
degradation	O
occurred	O
for	O
utterances	O
involving	O
flight	O
number	O
,	O
airfare	O
,	O
and	O
airport	O
name	O
(	O
in	O
that	O
order	O
)	O
.	O

Adding	O
105	O
more	O
Hindi	O
and	O
12	O
more	O
Turkish	O
training	O
examples	O
results	O
in	O
improved	O
performance	O
for	O
the	O
translated	O
,	O
STIL	O
mBART	B-MethodName
model	O
.	O
Macro	O
-	O
averaged	O
intent	B-TaskName
classification	I-TaskName
improves	O
from	O
94.40	O
%	O
to	O
95.94	O
%	O
,	O
and	O
slot	O
F1	B-MetricName
improves	O
from	O
88.79	O
%	O
to	O
90.10	O
%	O
,	O
both	O
of	O
which	O
are	O
statistically	O
significant	O
.	O
By	O
adding	O
these	O
117	O
samples	O
,	O
the	O
STIL	O
mBART	B-MethodName
model	O
matches	O
the	O
performance	O
(	O
within	O
confidence	O
intervals	O
)	O
of	O
the	O
non	O
-	O
translated	O
mBART	B-MethodName
model	O
.	O
This	O
finding	O
suggests	O
that	O
the	O
STIL	O
models	O
may	O
require	O
more	O
training	O
data	O
than	O
traditional	O
,	O
non	O
-	O
translated	O
slot	B-TaskName
filling	I-TaskName
models	O
.	O
Additionally	O
,	O
by	O
adding	O
more	O
Hindi	O
and	O
Turkish	O
data	O
,	O
both	O
the	O
intent	O
accuracy	B-MetricName
and	O
the	O
slot	B-TaskName
filling	I-TaskName
F1	B-MetricName
improves	O
for	O
every	O
individual	O
language	O
of	O
the	O
translated	O
,	O
STIL	O
models	O
,	O
suggesting	O
that	O
some	O
portion	O
of	O
the	O
internal	O
,	O
learned	O
representation	O
is	O
language	O
agnostic	O
.	O
Finally	O
,	O
the	O
results	O
suggest	O
that	O
there	O
is	O
a	O
trainingsize	O
-	O
dependent	O
performance	O
advantage	O
in	O
using	O
a	O
single	O
output	O
language	O
,	O
as	O
contrasted	O
with	O
the	O
nontranslated	O
mBART	B-MethodName
model	O
,	O
for	O
which	O
the	O
intent	B-TaskName
classification	I-TaskName
accuracy	B-MetricName
and	O
slot	O
F1	B-MetricName
does	O
not	O
improve	O
(	O
with	O
statistical	O
significance	O
)	O
when	O
using	O
the	O
additional	O
Hindi	O
and	O
Turkish	O
training	O
samples	O
.	O

Language	B-TaskName
identification	I-TaskName
F1	B-MetricName
is	O
above	O
99.7	O
%	O
for	O
all	O
languages	O
,	O
with	O
perfect	O
performance	O
in	O
many	O
cases	O
.	O
(	O
Qin	O
et	O
al	O
,	O
2019	O
)	O
97.5	O
Joint	O
BERT	B-MethodName
+	O
CRF	B-MethodName
(	O
Chen	O
et	O
al	O
,	O
2019	O
)	O
97.9	O
Non	O
-	O
translated	O
mBART	B-MethodName
,	O
with	O
Perfect	O
performance	O
on	O
Chinese	O
and	O
Hindi	O
is	O
unsurprising	O
given	O
their	O
unique	O
scripts	O
versus	O
the	O
other	O
languages	O
tested	O
.	O

Compositor	O
attribution	O
,	O
the	O
clustering	O
of	O
pages	O
in	O
a	O
historical	O
printed	O
document	O
by	O
the	O
individual	O
who	O
set	O
the	O
type	O
,	O
is	O
a	O
bibliographic	O
task	O
that	O
relies	O
on	O
analysis	O
of	O
orthographic	O
variation	O
and	O
inspection	O
of	O
visual	O
details	O
of	O
the	O
printed	O
page	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
novel	O
unsupervised	O
model	O
that	O
jointly	O
describes	O
the	O
textual	O
and	O
visual	O
features	O
needed	O
to	O
distinguish	O
compositors	O
.	O
Applied	O
to	O
images	O
of	O
Shakespeare	O
's	O
First	O
Folio	O
,	O
our	O
model	O
predicts	O
attributions	O
that	O
agree	O
with	O
the	O
manual	O
judgements	O
of	O
bibliographers	O
with	O
an	O
accuracy	B-MetricName
of	O
87	O
%	O
,	O
even	O
on	O
text	O
that	O
is	O
the	O
output	O
of	O
OCR	O
.	O

Our	O
computational	O
approach	O
to	O
compositor	O
attribution	O
operates	O
on	O
the	O
sources	O
of	O
evidence	O
that	O
have	O
been	O
considered	O
by	O
bibliographers	O
.	O
In	O
particular	O
,	O
we	O
focus	O
on	O
jointly	O
modeling	O
patterns	O
of	O
orthographic	O
variation	O
and	O
spacing	O
preferences	O
across	O
pages	O
of	O
a	O
document	O
,	O
treating	O
compositor	O
assignments	O
as	O
latent	O
variables	O
in	O
a	O
generative	O
model	O
.	O
We	O
assume	O
access	O
to	O
a	O
diplomatic	O
transcription	O
of	O
the	O
document	O
(	O
a	O
transcription	O
faith	O
-	O
ful	O
to	O
the	O
original	O
orthography	O
)	O
,	O
which	O
we	O
automatically	O
align	O
with	O
a	O
modernized	O
version	O
.	O
2	O
We	O
experiment	O
with	O
both	O
manually	O
and	O
automatically	O
(	O
OCR	O
)	O
produced	O
transcriptions	O
,	O
and	O
assume	O
access	O
to	O
pixel	O
-	O
level	O
spacing	O
information	O
on	O
each	O
page	O
,	O
which	O
can	O
be	O
extracted	O
using	O
OCR	O
as	O
described	O
in	O
Section	O
4	O
.	O
Figure	O
2	O
shows	O
the	O
generative	O
process	O
.	O
In	O
our	O
model	O
,	O
each	O
of	O
I	O
total	O
pages	O
is	O
generated	O
independently	O
.	O
The	O
compositor	O
assignment	O
for	O
the	O
ith	O
page	O
is	O
represented	O
by	O
the	O
variable	O
c	O
i	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
C	O
}	O
and	O
is	O
generated	O
from	O
a	O
multinomial	O
prior	O
.	O
For	O
page	O
i	O
,	O
each	O
diplomatic	O
word	O
,	O
d	O
ij	O
,	O
is	O
generated	O
conditioned	O
on	O
the	O
corresponding	O
modern	O
word	O
,	O
m	O
ij	O
,	O
and	O
the	O
compositor	O
who	O
set	O
the	O
page	O
,	O
c	O
i	O
.	O
Finally	O
,	O
the	O
model	O
produces	O
the	O
pixel	O
width	O
of	O
the	O
space	O
after	O
each	O
medial	O
comma	O
,	O
s	O
ik	O
,	O
again	O
conditioned	O
on	O
the	O
compositor	O
,	O
c	O
i	O
.	O
The	O
joint	O
distribution	O
for	O
page	O
i	O
,	O
conditioned	O
on	O
modern	O
text	O
,	O
takes	O
the	O
following	O
form	O
:	O
P	O
(	O
{	O
d	O
ij	O
}	O
,	O
{	O
s	O
ik	O
}	O
,	O
c	O
i	O
|	O
{	O
m	O
ij	O
}	O
)	O
=	O
P	O
(	O
c	O
i	O
)	O
[	O
Prior	O
on	O
compositors	O
]	O
J	O
i	O
j=1	O
P	O
(	O
d	O
ij	O
|	O
m	O
ij	O
,	O
c	O
i	O
;	O
w	O
c	O
i	O
)	O
[	O
Orthographic	O
model	O
]	O
K	O
i	O
k=1	O
P	O
(	O
s	O
ik	O
|	O
c	O
i	O
;	O
θ	B-HyperparameterName
c	O
i	O
)	O
[	O
Whitespace	O
model	O
]	O

Manual	O
analysis	O
of	O
spacing	O
has	O
revealed	O
differences	O
across	O
pages	O
.	O
In	O
particular	O
,	O
the	O
choice	O
of	O
spaced	O
or	O
non	O
-	O
spaced	O
punctuation	O
marks	O
is	O
hypothesized	O
by	O
biobliographers	O
to	O
be	O
indicative	O
of	O
compositor	O
preference	O
and	O
specific	O
typecase	O
.	O
We	O
add	O
whitespace	O
distance	O
to	O
our	O
model	O
to	O
capture	O
those	O
observations	O
.	O
While	O
bibliographers	O
only	O
made	O
a	O
coarse	O
distinction	O
between	O
spaced	O
or	O
nonspaced	O
commas	O
,	O
in	O
our	O
model	O
we	O
generate	O
medial	O
comma	O
spacing	O
widths	O
,	O
s	O
ik	O
,	O
that	O
are	O
measured	O
in	O
pixels	O
to	O
enable	O
finer	O
-	O
grained	O
analysis	O
.	O
We	O
use	O
a	O
simple	O
multinomial	O
parameterization	O
where	O
each	O
pixel	O
width	O
is	O
treated	O
as	O
a	O
separate	O
outcome	O
up	O
to	O
some	O
maximum	O
allowable	O
width	O
:	O
s	O
ik	O
|	O
c	O
i	O
∼	O
M	O
ult	O
(	O
θ	B-HyperparameterName
c	O
i	O
)	O
Here	O
,	O
θ	B-HyperparameterName
c	O
represents	O
the	O
vector	O
of	O
multinomial	O
spacing	O
parameters	O
corresponding	O
to	O
compositor	O
c.	O
We	O
choose	O
this	O
parameterization	O
because	O
it	O
can	O
capture	O
non	O
-	O
unimodal	O
whitespace	O
preference	O
distributions	O
,	O
as	O
depicted	O
in	O
Figure	O
2	O
,	O
and	O
it	O
makes	O
learning	O
simple	O
.	O

Modern	O
and	O
diplomatic	O
words	O
and	O
spacing	O
variables	O
are	O
observed	O
,	O
while	O
compositor	O
assignments	O
are	O
latent	O
.	O
In	O
order	O
to	O
fit	O
the	O
model	O
to	O
an	O
input	O
document	O
we	O
estimate	O
the	O
orthographic	O
preference	O
parameters	O
,	O
w	O
c	O
,	O
and	O
spacing	O
preference	O
parameters	O
,	O
θ	B-HyperparameterName
c	O
,	O
for	O
each	O
compositor	O
using	O
EM	B-MetricName
.	O
The	O
E	O
-	O
step	O
is	O
accomplished	O
via	O
a	O
tractable	O
sum	O
over	O
compositor	O
assignments	O
,	O
while	O
the	O
M	O
-	O
step	O
for	O
w	O
c	O
is	O
accomplished	O
via	O
gradient	O
ascent	O
(	O
Berg	O
-	O
Kirkpatrick	O
et	O
al	O
,	O
2010	O
)	O
.	O
The	O
M	O
-	O
step	O
for	O
spacing	O
parameters	O
,	O
θ	B-HyperparameterName
c	O
,	O
uses	O
the	O
standard	O
multinomial	O
update	O
.	O
Predicting	O
compositor	O
groups	O
is	O
accomplished	O
via	O
an	O
Model	O
Setup	O

Ocular	O
OCR	O
Transcription	O
Hinman	O
Attr	O
Blayney	O
Attr	O
Hinman	O
Attr	O
Blayney	O
Attr	O
1	O
-	O
to	O
-	O
1	O
M	O
-	O
to	O
-	O
1	O
1	O
-	O
to	O
-	O
1	O
M	O
-	O
to	O
-	O
1	O
1	O
-	O
to	O
-	O
1	O
M	O
-	O
to	O
-	O
1	O
1	O
-	O
to	O
-	O
1	O
M	O
-	O
to	O
-	O
1	O
(	O
Hinman	O
,	O
1963	O
;	O
Howard	O
-	O
Hill	O
,	O
1973	O
,	O
1976	O
,	O
1980Taylor	O
,	O
1981	O
;	O
O'Connor	O
,	O
1975	O
;	O
Werstine	O
,	O
1982	O
)	O
.	O
We	O
also	O
evaluate	O
our	O
system	O
against	O
an	O
earlier	O
,	O
highly	O
influential	O
model	O
proposed	O
by	O
Hinman	O
(	O
1963	O
)	O
,	O
which	O
we	O
approximate	O
by	O
reverting	O
certain	O
compositor	O
divisions	O
in	O
Blayney	O
's	O
attribution	O
.	O
Hinman	O
's	O
attribution	O
posited	O
five	O
compositors	O
,	O
while	O
Blayney	O
's	O
posited	O
eight	O
.	O
In	O
experiments	O
,	O
we	O
set	O
the	O
model	O
's	O
maximum	O
number	O
of	O
compositors	O
to	O
C	O
=	O
5	O
when	O
evaluating	O
on	O
Hinman	O
's	O
attribution	O
,	O
and	O
use	O
C	O
=	O
8	O
with	O
Blayney	O
's	O
.	O
We	O
compute	O
the	O
one	O
-	O
to	O
-	O
one	O
and	O
many	O
-	O
to	O
-	O
one	O
accuracy	B-MetricName
,	O
mapping	O
the	O
recovered	O
page	O
groups	O
to	O
the	O
gold	O
compositors	O
to	O
maximize	O
accuracy	B-MetricName
,	O
as	O
is	O
standard	O
for	O
many	O
unsupervised	O
clustering	O
tasks	O
,	O
e.g.	O
POS	O
induction	O
(	O
see	O
Christodoulopoulos	O
et	O
al	O
(	O
2010	O
)	O
)	O
.	O
BASIC	O
model	O
variant	O
:	O
We	O
evaluate	O
a	O
simple	O
baseline	O
model	O
that	O
uses	O
a	O
multinomial	O
parameterization	O
for	O
generating	O
diplomatic	O
words	O
and	O
does	O
not	O
incorporate	O
spacing	O
information	O
.	O
We	O
use	O
two	O
different	O
options	O
for	O
selection	O
of	O
spelling	O
variants	O
to	O
be	O
considered	O
by	O
the	O
model	O
.	O
First	O
,	O
we	O
consider	O
only	O
the	O
three	O
words	O
selected	O
by	O
Hinman	O
:	O
do	O
,	O
go	O
and	O
here	O
(	O
referred	O
to	O
as	O
HINMAN	O
)	O
.	O
Second	O
,	O
we	O
use	O
a	O
larger	O
,	O
automatically	O
selected	O
,	O
word	O
list	O
(	O
referred	O
to	O
as	O
AUTO	O
)	O
.	O
Here	O
,	O
we	O
select	O
all	O
modern	O
words	O
with	O
frequency	O
greater	O
than	O
70	O
that	O
are	O
not	O
names	O
and	O
that	O
exhibit	O
sufficient	O
variance	O
in	O
diplomatic	O
spellings	O
(	O
most	O
common	O
diplomatic	O
spelling	O
occurs	O
in	O
less	O
than	O
80	O
%	O
of	O
aligned	O
tokens	O
)	O
.	O
For	O
our	O
full	O
model	O
,	O
described	O
in	O
the	O
next	O
section	O
,	O
we	O
always	O
use	O
the	O
larger	O
AUTO	O
word	O
list	O
.	O
FEAT	O
model	O
variant	O
:	O
We	O
run	O
experiments	O
with	O
several	O
variants	O
of	O
our	O
full	O
model	O
,	O
described	O
in	O
Section	O
3	O
(	O
referred	O
to	O
as	O
FEAT	O
since	O
they	O
use	O
a	O
feature	O
-	O
based	O
parameterization	O
of	O
diplomatic	O
word	O
generation	O
.	O
)	O
We	O
try	O
ablations	O
of	O
WORD	B-DatasetName
and	O
EDIT	O
features	O
,	O
as	O
well	O
as	O
model	O
variants	O
with	O
and	O
without	O
the	O
spacing	O
generation	O
component	O
(	O
referred	O
to	O
as	O
SPACE	B-DatasetName
.	O
)	O
We	O
refer	O
to	O
the	O
full	O
model	O
that	O
includes	O
both	O
types	O
of	O
features	O
and	O
spacing	O
generation	O
as	O
ALL	O
.	O

Our	O
experimental	O
results	O
are	O
presented	O
in	O
Table	O
1	O
.	O
The	O
BASIC	O
variant	O
,	O
modeled	O
after	O
Hinman	O
's	O
original	O
procedure	O
,	O
substantially	O
outperforms	O
the	O
random	O
baseline	O
,	O
with	O
the	O
HINMAN	O
word	O
list	O
outperforming	O
the	O
larger	O
AUTO	O
word	O
list	O
.	O
However	O
,	O
use	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
u	O
o	O
u	O
w	O
!	O
u	O
DEL	O
!	O
B	O
A	O
C	O
E	O
D	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
Comp	O
B	O
Comp	O
A	O
Comp	O
C	O
Comp	O
E	O
Comp	O
D	O
!	O
!	O
!	O
!	O
Figure	O
3	O
:	O
Learned	O
behaviors	O
of	O
the	O
Folio	O
compositors	O
.	O
Our	O
model	O
only	O
detected	O
the	O
presence	O
of	O
five	O
compositors	O
(	O
ranked	O
according	O
to	O
number	O
of	O
pages	O
the	O
compositor	O
set	O
in	O
our	O
model	O
's	O
prediction	O
)	O
.	O
Compositor	O
D	O
's	O
habit	O
of	O
omitting	O
u	O
(	O
yong	O
vs.	O
young	O
)	O
and	O
compositor	O
C	O
's	O
usage	O
of	O
spaced	O
medial	O
commas	O
were	O
also	O
noticed	O
in	O
Taylor	O
(	O
1981	O
)	O
.	O
of	O
the	O
larger	O
word	O
list	O
with	O
feature	O
-	O
based	O
models	O
yields	O
large	O
gains	O
in	O
all	O
scenarios	O
,	O
including	O
evaluation	O
on	O
Hinman	O
's	O
original	O
attributions	O
and	O
while	O
using	O
OCR	O
diplomatic	O
transcriptions	O
.	O
The	O
bestperforming	O
model	O
for	O
both	O
manually	O
transcribed	O
and	O
OCR	O
text	O
uses	O
EDIT	O
features	O
in	O
conjunction	O
with	O
spacing	O
generation	O
and	O
achieves	O
an	O
accuracy	B-MetricName
of	O
up	O
to	O
87	O
%	O
.	O
Including	O
WORD	B-DatasetName
features	O
on	O
top	O
of	O
this	O
leads	O
to	O
slightly	O
reduced	O
performance	O
,	O
perhaps	O
as	O
a	O
result	O
of	O
the	O
substantially	O
increased	O
number	O
of	O
free	O
parameters	O
.	O
In	O
the	O
OCR	O
scenario	O
,	O
the	O
addition	O
of	O
WORD	B-DatasetName
features	O
on	O
top	O
of	O
EDIT	O
decreases	O
accuracy	B-MetricName
,	O
unlike	O
the	O
same	O
experiment	O
with	O
the	O
manual	O
transcription	O
.	O
This	O
is	O
possibly	O
a	O
result	O
of	O
the	O
reduced	O
reliability	O
of	O
full	O
word	O
forms	O
due	O
to	O
mistakes	O
in	O
OCR	O
.	O
Particularly	O
interesting	O
is	O
the	O
result	O
that	O
spacing	O
,	O
rarely	O
a	O
factor	O
considered	O
in	O
NLP	O
models	O
,	O
improves	O
the	O
accuracy	B-MetricName
significantly	O
for	O
our	O
system	O
when	O
compared	O
with	O
EDIT	O
features	O
alone	O
.	O
Because	O
pixel	O
-	O
level	O
visual	O
information	O
and	O
arbitrary	O
orthographic	O
patterns	O
are	O
also	O
the	O
most	O
difficult	O
features	O
to	O
measure	O
manually	O
,	O
our	O
results	O
give	O
strong	O
evidence	O
to	O
the	O
assertion	O
that	O
NLP	O
-	O
style	O
models	O
can	O
aid	O
bibliographers	O
.	O

The	O
input	O
is	O
a	O
liberal	O
news	O
corpus	O
D	O
L	O
=	O
{	O
d	O
L	O
i	O
}	O
|	O
D	O
L	O
|	O
i=1	O
and	O
a	O
conservative	O
news	O
corpus	O
D	O
R	O
=	O
{	O
d	O
R	O
i	O
}	O
|	O
D	O
R	O
|	O
i=1	O
(	O
L	O
denotes	O
"	O
Left	O
"	O
and	O
R	O
denotes	O
"	O
Right	O
"	O
)	O
,	O
where	O
d	O
L	O
i	O
is	O
an	O
article	O
from	O
D	O
L	O
and	O
d	O
R	O
i	O
is	O
an	O
article	O
from	O
D	O
R	O
.	O
A	O
news	O
article	O
is	O
represented	O
as	O
a	O
sequence	O
of	O
tokens	O
:	O
d	O
k	B-HyperparameterName
=	I-HyperparameterName
(	O
w	O
k	O
i	O
)	O
|	O
d	O
k	O
|	O
i=1	O
.	O
Given	O
a	O
topic	O
model	O
trained	O
on	O
the	O
combined	O
corpus	O
D	O
C	O
=	O
D	O
L	O
∪	O
D	O
R	O
with	O
a	O
set	O
of	O
modeled	O
topics	O
T	O
=	O
{	O
t	O
i	O
}	O
K	O
i=1	O
where	O
t	O
i	O
represents	O
a	O
topic	O
,	O
we	O
aim	O
to	O
learn	O
a	O
model	O
f	O
We	O
train	O
an	O
LDA	B-MethodName
model	O
on	O
the	O
combined	O
corpus	O
and	O
extract	O
2	O
topics	O
.	O
Top	O
-	O
4	O
keywords	O
on	O
topic	O
t	O
1	O
are	O
"	O
briefing	O
"	O
,	O
"	O
trump	O
"	O
,	O
"	O
president	O
"	O
and	O
"	O
white_house	O
"	O
.	O
Top	O
-	O
2	O
most	O
relevant	O
documents	O
on	O
topic	O
t	O
1	O
are	O
d	O
L	O
1	O
and	O
d	O
L	O
2	O
for	O
CNN	O
and	O
d	O
R	O
1	O
and	O
d	O
R	O
2	O
for	O
Fox	O
.	O
d	O
L	O
3	O
and	O
d	O
R	O
3	O
are	O
not	O
among	O
the	O
most	O
relevant	O
documents	O
of	O
this	O
topic	O
and	O
are	O
excluded	O
in	O
the	O
embedding	O
generation	O
step	O
.	O
Note	O
that	O
we	O
set	O
K	B-HyperparameterName
=	I-HyperparameterName
2	O
(	O
No	O
.	O
of	O
topics	O
)	O
,	O
m	O
=	O
4	O
(	O
No	O
.	O
of	O
keywords	O
)	O
,	O
and	O
n	O
=	O
2	O
(	O
No	O
.	O
of	O
documents	O
)	O
,	O
just	O
for	O
clear	O
demonstration	O
.	O
(	O
b	O
)	O
Partisanship	O
learning	O
.	O
We	O
finetune	O
a	O
pretrained	O
language	O
model	O
to	O
classify	O
the	O
partisanship	O
(	O
liberal	O
vs.	O
conservative	O
)	O
of	O
input	O
documents	O
.	O
(	O
c	O
)	O
Topic	O
embedding	O
generation	O
and	O
similarity	O
measuring	O
.	O
We	O
provide	O
a	O
step	O
by	O
step	O
illustration	O
of	O
DC	O
keyword	O
embedding	O
DC	O
topic	O
embedding	O
CC	O
topic	O
embedding	O
on	O
topic	O
t	O
1	O
.	O
In	O
the	O
two	O
input	O
corpora	O
,	O
the	O
tokens	O
that	O
are	O
among	O
the	O
top	O
-	O
4	O
keywords	O
of	O
topic	O
t	O
1	O
are	O
highlighted	O
in	O
bold	O
.	O
Take	O
document	O
d	O
L	O
1	O
from	O
CNN	O
as	O
an	O
example	O
.	O
The	O
weighted	O
average	O
of	O
the	O
DC	O
keyword	O
embeddings	O
(	O
H	O
d	O
L	O
1	O
(	O
president	O
)	O
,	O
H	O
d	O
L	O
1	O
(	O
trump	O
)	O
,	O
and	O
H	O
d	O
L	O
1	O
(	O
briefing	O
)	O
)	O
is	O
defined	O
as	O
the	O
DC	O
topic	O
embedding	O
H	O
d	O
L	O
1	O
(	O
t	O
1	O
)	O
with	O
keyword	O
coefficients	O
given	O
by	O
Eq	O
.	O
2	O
;	O
note	O
that	O
H	O
d	O
L	O
1	O
(	O
criticize	O
)	O
is	O
excluded	O
because	O
"	O
criticize	O
"	O
is	O
not	O
among	O
the	O
top	O
-	O
4	O
keywords	O
of	O
topic	O
t	O
1	O
.	O
Similarly	O
we	O
can	O
obtain	O
the	O
DC	O
topic	O
embeddings	O
for	O
d	O
L	O
2	O
,	O
d	O
R	O
1	O
and	O
d	O
R	O
2	O
.	O
The	O
DC	O
topic	O
embeddings	O
are	O
further	O
aggregated	O
into	O
CC	O
topic	O
embeddings	O
H	O
D	O
L	O
(	O
t	O
1	O
)	O
and	O
H	O
D	O
R	O
(	O
t	O
1	O
)	O
(	O
document	O
coefficients	O
are	O
from	O
Eq	O
.	O
3	O
)	O
and	O
the	O
cosine	O
distance	O
between	O
them	O
is	O
used	O
as	O
a	O
measure	O
of	O
polarization	O
of	O
the	O
two	O
corpora	O
on	O
topic	O
t	O
1	O
.	O
that	O
is	O
able	O
to	O
detect	O
the	O
topic	O
polarization	O
between	O
D	O
L	O
and	O
D	O
R	O
on	O
topics	O
in	O
T	O
and	O
output	O
a	O
ranking	O
of	O
topics	O
based	O
on	O
polarization	O
,	O
such	O
that	O
f	O
(	O
D	O
L	O
,	O
D	O
R	O
,	O
T	O
)	O
=	O
(	O
t	O
k	O
)	O
K	O
k=1	O
,	O
i	O
>	O
j	O
⇔	O
β	B-HyperparameterName
(	O
t	O
i	O
,	O
D	O
L	O
,	O
D	O
R	O
)	O
<	O
β	B-HyperparameterName
(	O
t	O
j	O
,	O
D	O
L	O
,	O
D	O
R	O
)	O
,	O
(	O
1	O
)	O
where	O
β	B-HyperparameterName
(	O
t	O
,	O
D	O
L	O
,	O
D	O
R	O
)	O
represents	O
the	O
polarization	O
score	O
of	O
topic	O
t	O
between	O
D	O
L	O
and	O
D	O
R	O
.	O

As	O
we	O
will	O
see	O
in	O
Sections	O
3.4	O
and	O
3.5	O
,	O
the	O
contextualized	O
topic	O
embeddings	O
are	O
generated	O
from	O
a	O
pretrained	O
language	O
model	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
and	O
cosine	O
distance	O
between	O
the	O
topic	O
embeddings	O
from	O
two	O
corpora	O
are	O
used	O
as	O
a	O
measure	O
of	O
topic	O
polarization	O
.	O
The	O
idea	O
is	O
inspired	O
by	O
static	O
word	O
embedding	O
models	O
like	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
where	O
the	O
authors	O
measure	O
the	O
similarity	O
between	O
words	O
by	O
the	O
cosine	O
similarity	O
between	O
the	O
word	B-TaskName
embeddings	I-TaskName
.	O
However	O
,	O
to	O
apply	O
this	O
measure	O
of	O
similarity	O
,	O
the	O
model	O
should	O
be	O
fitted	O
on	O
the	O
target	O
corpus	O
.	O
To	O
fit	O
the	O
pretrained	O
language	O
model	O
on	O
the	O
news	O
corpora	O
,	O
we	O
can	O
use	O
one	O
of	O
the	O
two	O
training	O
tasks	O
:	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
or	O
partisanship	O
recognition	O
.	O
We	O
decide	O
on	O
the	O
second	O
task	O
because	O
1	O
)	O
it	O
is	O
more	O
time	O
efficient	O
;	O
2	O
)	O
it	O
informs	O
the	O
language	O
model	O
of	O
the	O
partisan	O
divisions	O
between	O
different	O
news	O
sources	O
,	O
enhancing	O
the	O
language	O
model	O
's	O
ability	O
to	O
encode	O
the	O
polarization	O
arising	O
from	O
partisan	O
differences	O
in	O
its	O
output	O
.	O
This	O
idea	O
is	O
similar	O
to	O
(	O
Webson	O
et	O
al	O
,	O
2020	O
)	O
where	O
the	O
authors	O
call	O
the	O
embedding	O
space	O
of	O
the	O
language	O
model	O
after	O
finetuning	O
as	O
"	O
connotation	O
space	O
"	O
.	O
As	O
a	O
result	O
,	O
given	O
a	O
document	O
d	O
D	O
C	O
,	O
the	O
model	O
is	O
optimized	O
to	O
classify	O
whether	O
it	O
is	O
from	O
D	O
L	O
or	O
D	O
R	O
by	O
a	O
binary	O
cross	O
-	O
entropy	O
loss	B-MetricName
,	O
where	O
the	O
[	O
CLS	O
]	O
embedding	O
is	O
used	O
to	O
represent	O
the	O
document	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
(	O
b	O
)	O
.	O

After	O
obtaining	O
the	O
CC	O
topic	O
embeddings	O
H	O
D	O
L	O
(	O
t	O
i	O
)	O
and	O
H	O
D	O
R	O
(	O
t	O
i	O
)	O
of	O
the	O
two	O
corpora	O
D	O
L	O
and	O
D	O
R	O
on	O
topic	O
t	O
i	O
,	O
using	O
two	O
different	O
sets	O
of	O
top	O
-	O
n	O
most	O
relevant	O
documents	O
from	O
D	O
L	O
and	O
D	O
R	O
respectively	O
,	O
we	O
measure	O
the	O
ideology	O
similarity	O
(	O
and	O
then	O
polarization	O
)	O
based	O
on	O
the	O
cosine	O
similarity	O
between	O
them	O
,	O
such	O
that	O
c	O
=	O
cos_sim	O
(	O
H	O
D	O
L	O
(	O
t	O
i	O
)	O
,	O
H	O
D	O
R	O
(	O
t	O
i	O
)	O
)	O
,	O
β	B-HyperparameterName
(	O
D	O
L	O
,	O
D	O
R	O
,	O
t	O
i	O
)	O
=	O
0.5	O
*	O
(	O
1	O
−	O
c	O
)	O
[	O
0	B-DatasetName
,	O
1	O
]	O
.	O
(	O
6	O
)	O
A	O
higher	O
value	O
of	O
β	B-HyperparameterName
indicates	O
more	O
polarization	O
.	O
Therefore	O
,	O
the	O
polarization	O
-	O
based	O
ranked	O
topic	O
list	O
f	O
(	O
D	O
L	O
,	O
D	O
R	O
,	O
T	O
)	O
is	O
computed	O
based	O
on	O
the	O
corresponding	O
polarization	O
scores	O
(	O
β	B-HyperparameterName
(	O
D	O
L	O
,	O
D	O
R	O
,	O
t	O
i	O
)	O
)	O
K	O
i=1	O
.	O
4	O
Experiments	O
and	O
Results	O

Data	O
Preprocessing	O
.	O
We	O
build	O
a	O
global	O
vocabulary	O
containing	O
unigrams	O
and	O
bigrams	O
from	O
the	O
six	O
news	O
sources	O
.	O
We	O
perform	O
lemmatization	B-TaskName
via	O
LDA	B-MethodName
Topic	O
Modeling	O
.	O
We	O
train	O
the	O
topic	O
model	O
using	O
articles	O
from	O
all	O
six	O
sources	O
to	O
create	O
a	O
global	O
topic	O
set	O
.	O
The	O
number	O
of	O
topics	O
K	O
is	O
selected	O
from	O
a	O
grid	O
search	O
in	O
[	O
10	O
,	O
50	O
]	O
and	O
the	O
model	O
with	O
K	B-HyperparameterName
=	I-HyperparameterName
39	O
produces	O
the	O
best	O
coherence	O
value	O
(	O
Röder	O
et	O
al	O
,	O
2015	O
)	O
.	O
From	O
the	O
39	O
topics	O
we	O
remove	O
9	O
of	O
them	O
regarding	O
advertisements	O
,	O
sport	O
events	O
,	O
gossip	O
news	O
and	O
recipes	O
,	O
and	O
30	O
topics	O
are	O
left	O
;	O
the	O
removed	O
topics	O
are	O
more	O
factual	O
and	O
contain	O
less	O
ideologies	O
from	O
the	O
news	O
media	O
,	O
which	O
is	O
less	O
worth	O
studying	O
.	O
Different	O
from	O
(	O
Demszky	O
et	O
al	O
,	O
2019	O
)	O
that	O
assigns	O
only	O
one	O
topic	O
with	O
the	O
highest	O
probability	O
to	O
a	O
document	O
,	O
we	O
allow	O
a	O
document	O
to	O
be	O
assigned	O
multiple	O
topics	O
with	O
different	O
probabilities	O
.	O
We	O
represent	O
each	O
topic	O
with	O
its	O
top	O
-	O
10	O
keywords	O
because	O
given	O
a	O
topic	O
t	O
i	O
we	O
empirically	O
find	O
that	O
10	O
j=1	O
p	O
ij	O
>	O
0.95	O
;	O
and	O
we	O
keep	O
the	O
top	O
-	O
10	O
most	O
relevant	O
documents	O
to	O
represent	O
a	O
topic	O
because	O
on	O
some	O
topics	O
,	O
the	O
documents	O
beyond	O
the	O
top	O
-	O
10	O
list	O
are	O
obviously	O
irrelevant	O
and	O
will	O
bias	O
the	O
polarization	O
study	O
regarding	O
the	O
topic	O
.	O
In	O
Table	O
1	O
we	O
show	O
the	O
top	O
-	O
10	O
keywords	O
of	O
topics	O
that	O
are	O
discussed	O
in	O
this	O
paper	O
.	O
For	O
a	O
complete	O
list	O
of	O
topics	O
please	O
refer	O
to	O
Appendix	O
B.	O
Learning	O
Partisanship	O
.	O
We	O
finetune	O
the	O
pretrained	O
bert	O
-	O
base	O
-	O
uncased	O
model	O
from	O
huggingface	O
Transformers	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
to	O
classify	O
the	O
news	O
articles	O
according	O
to	O
their	O
political	O
leanings	O
,	O
or	O
partisanship	O
.	O
To	O
smooth	O
over	O
the	O
differences	O
in	O
style	O
and	O
writing	O
between	O
the	O
sources	O
and	O
render	O
the	O
model	O
primarily	O
sensitive	O
to	O
political	O
divisions	O
,	O
we	O
aggregate	O
CNN	O
,	O
Huff	O
,	O
and	O
NYP	O
to	O
create	O
a	O
holistic	O
Liberal	O
corpus	O
,	O
and	O
similarly	O
aggregate	O
Fox	O
,	O
Breit	O
and	O
NYP	O
to	O
create	O
a	O
holistic	O
Conservative	O
corpus	O
and	O
optimize	O
the	O
model	O
to	O
classify	O
whether	O
an	O
article	O
is	O
from	O
Liberal	O
or	O
Conservative	O
.	O
In	O
fact	O
,	O
finetuning	O
a	O
BERT	B-MethodName
model	O
to	O
recognize	O
differences	O
only	O
between	O
CNN	O
vs.	O
Fox	O
is	O
likely	O
to	O
make	O
it	O
end	O
up	O
capturing	O
the	O
writing	O
style	O
differences	O
and	O
ignoring	O
political	O
differences	O
,	O
since	O
the	O
former	O
is	O
an	O
easier	O
task	O
.	O
For	O
more	O
details	O
about	O
the	O
training	O
process	O
please	O
refer	O
to	O
Appendix	O
C.	O
Idx	O
Top	O
-	O
10	O
keywords	O
(	O
and	O
two	O
defined	O
stances	O
)	O

As	O
ground	O
truth	O
for	O
the	O
evaluation	O
of	O
PaCTE	O
,	O
we	O
annotate	O
the	O
topic	O
polarization	O
scores	O
on	O
a	O
subset	O
of	O
the	O
30	O
modeled	O
topics	O
.	O
We	O
asked	O
three	O
annotators	O
to	O
select	O
10	O
topics	O
and	O
define	O
two	O
polarized	O
political	O
stances	O
on	O
each	O
selected	O
topic	O
,	O
and	O
they	O
reached	O
an	O
agreement	O
on	O
T	O
labeled	O
=	O
{	O
t	O
1	O
,	O
t	O
2	O
,	O
t	O
8	O
,	O
t	O
9	O
,	O
t	O
10	O
,	O
t	O
11	O
,	O
t	O
12	O
,	O
t	O
27	O
,	O
t	O
30	O
,	O
t	O
33	O
}	O
,	O
as	O
shown	O
in	O
Table	O
1	O
.	O
Then	O
on	O
each	O
topic	O
in	O
T	O
labeled	O
,	O
we	O
selected	O
60	O
relevant	O
documents	O
(	O
10	O
from	O
each	O
of	O
the	O
six	O
sources	O
)	O
,	O
and	O
asked	O
three	O
annotators	O
to	O
decide	O
which	O
stance	O
they	O
belong	O
to	O
(	O
label	O
it	O
as	O
0/1	O
)	O
.	O
If	O
the	O
document	O
does	O
not	O
have	O
a	O
clear	O
stance	O
,	O
it	O
was	O
labeled	O
as	O
−1	O
.	O
On	O
each	O
document	O
,	O
the	O
majority	O
label	O
from	O
the	O
annotations	O
was	O
used	O
as	O
the	O
final	O
annotation	O
.	O
Please	O
refer	O
to	O
Appendix	O
D	O
for	O
more	O
details	O
about	O
the	O
annotation	O
process	O
.	O
Denoting	O
the	O
number	O
of	O
negative	O
labels	O
(	O
0	B-DatasetName
)	O
and	O
positive	O
labels	O
(	O
1	O
)	O
in	O
corpus	O
D	O
on	O
topic	O
t	O
as	O
N	O
t	O
D	O
(	O
0	B-DatasetName
)	O
and	O
N	O
t	O
D	O
(	O
1	O
)	O
respectively	O
,	O
the	O
leaning	O
of	O
the	O
corpus	O
on	O
the	O
topic	O
is	O
quantified	O
as	O
le	O
(	O
D	O
,	O
t	O
)	O
=	O
(	O
N	O
t	O
D	O
(	O
1	O
)	O
−N	O
t	O
D	O
(	O
0	B-DatasetName
)	O
)	O
/	O
|	O
D	O
|	O
[	O
−1	O
,	O
1	O
]	O
.	O
(	O
7	O
)	O
Intuitively	O
,	O
le	O
(	O
D	O
,	O
t	O
)	O
reflects	O
how	O
much	O
the	O
corpus	O
is	O
aligned	O
with	O
the	O
stance	O
labeled	O
as	O
1	O
.	O
Notably	O
,	O
the	O
documents	O
labeled	O
with	O
−1	O
are	O
not	O
counted	O
because	O
they	O
do	O
not	O
display	O
a	O
clear	O
political	O
standing	O
.	O
Accordingly	O
,	O
the	O
ground	O
-	O
truth	O
polarization	O
score	O
between	O
a	O
liberal	O
corpus	O
D	O
L	O
and	O
a	O
conservative	O
corpus	O
D	O
R	O
on	O
topic	O
t	O
is	O
computed	O
as	O
the	O
difference	O
between	O
the	O
leanings	O
of	O
the	O
two	O
corpora	O
,	O
such	O
that	O
α	B-HyperparameterName
(	O
D	O
L	O
,	O
D	O
R	O
,	O
t	O
)	O
=	O
|	O
le	O
(	O
D	O
L	O
,	O
t	O
)	O
−	O
le	O
(	O
D	O
R	O
,	O
t	O
)	O
|	O
/2	O
[	O
0	B-DatasetName
,	O
1	O
]	O
.	O
(	O
8	O
)	O
A	O
higher	O
value	O
of	O
α	B-HyperparameterName
signifies	O
more	O
polarization	O
.	O
As	O
a	O
result	O
,	O
the	O
ground	O
-	O
truth	O
polarization	O
-	O
based	O
topic	O
ranked	O
list	O
l	O
gt	O
(	O
D	O
L	O
,	O
D	O
R	O
,	O
T	O
labeled	O
)	O
between	O
a	O
liberal	O
corpus	O
D	O
L	O
and	O
a	O
conservative	O
corpus	O
D	O
R	O
is	O
computed	O
based	O
on	O
the	O
corresponding	O
ground	O
-	O
truth	O
polarization	O
scores	O
(	O
α	B-HyperparameterName
(	O
D	O
L	O
,	O
D	O
R	O
,	O
t	O
)	O
|	O
t	O
T	O
labeled	O
)	O
.	O

The	O
news	O
articles	O
are	O
split	O
into	O
the	O
training	O
set	O
comprising	O
topical	O
documents	O
and	O
validation	O
set	O
comprising	O
non	O
-	O
topical	O
documents	O
.	O
Non	O
-	O
topical	O
documents	O
have	O
small	O
probabilities	O
(	O
<	O
0.15	O
)	O
categorized	O
to	O
all	O
topics	O
.	O
We	O
do	O
such	O
a	O
split	O
because	O
all	O
documents	O
are	O
assigned	O
a	O
partisanship	O
label	O
,	O
but	O
not	O
all	O
of	O
them	O
are	O
topical	O
.	O
For	O
the	O
topical	O
documents	O
from	O
which	O
we	O
will	O
generate	O
contextualized	O
topic	O
embeddings	O
,	O
we	O
use	O
them	O
as	O
the	O
training	O
data	O
to	O
finetune	O
the	O
language	O
model	O
during	O
the	O
training	O
phase	O
.	O
As	O
a	O
result	O
,	O
train	O
set	O
has	O
30	O
,	O
571	O
documents	O
and	O
the	O
validation	O
set	O
has	O
35	O
,	O
797	O
documents	O
.	O
The	O
model	O
is	O
trained	O
for	O
30	O
epochs	O
and	O
we	O
pick	O
the	O
one	O
with	O
the	O
best	O
performance	O
on	O
validation	O
set	O
for	O
the	O
subsequent	O
topic	O
embedding	O
generation	O
.	O
We	O
train	O
the	O
model	O
using	O
Adam	B-MethodName
optimizer	B-HyperparameterName
,	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
1e	O
-	O
5	O
and	O
weight	B-MethodName
decay	I-MethodName
5e	O
-	O
4	O
.	O
We	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	O
and	O
train	O
the	O
model	O
on	O
4	O
RTX	O
2080	O
GPUs	O
.	O
Each	O
epoch	O
takes	O
about	O
10	O
minutes	O
.	O
The	O
best	O
validation	O
F1	B-MetricName
score	I-MetricName
on	O
classifying	O
partisanship	O
is	O
91.3	O
.	O

To	O
sample	O
clips	O
that	O
were	O
likely	O
candidates	O
,	O
we	O
trained	O
a	O
simple	O
autoencoder	B-MethodName
for	O
audio	O
clips	O
of	O
single	O
words	O
synthesized	O
using	O
the	O
Amazon	O
Polly	O
speech	B-TaskName
synthesis	I-TaskName
system	O
.	O
Treating	O
the	O
autoencoder	B-MethodName
's	O
low	O
-	O
dimensional	O
latent	O
space	O
as	O
a	O
proxy	O
for	O
perceptual	O
space	O
,	O
we	O
searched	O
for	O
clips	O
that	O
travel	O
through	O
more	O
of	O
the	O
space	O
as	O
the	O
playback	O
rate	O
is	O
slowed	O
from	O
1.0×	O
to	O
0.6×.	O
Intuitively	O
,	O
a	O
longer	O
path	O
through	O
encoder	O
space	O
should	O
correspond	O
to	O
a	O
more	O
dramatic	O
change	O
in	O
perception	O
as	O
the	O
clip	O
is	O
slowed	O
down	O
(	O
Section	O
3	O
presents	O
some	O
data	O
supporting	O
this	O
)	O
.	O
Concretely	O
,	O
we	O
computed	O
a	O
score	O
S	O
proportional	O
to	O
the	O
length	O
of	O
the	O
curve	O
swept	O
by	O
the	O
encoder	O
E	O
in	O
latent	O
space	O
as	O
the	O
clip	O
is	O
slowed	O
down	O
,	O
normalized	O
by	O
the	O
straight	O
-	O
line	O
distance	O
traveled	O
:	O
that	O
is	O
,	O
we	O
define	O
S	O
(	O
c	O
)	O
=	O
0.6×	O
r=1.0×	O
|	O
|	O
dE	O
(	O
c	O
,	O
r	O
)	O
/dr	O
|	O
|	O
dr	O
|	O
|	O
E	O
(	O
c	O
,	O
0.6×	O
)	O
−E	O
(	O
c	O
,	O
1.0×	O
)	O
|	O
|	O
.	O
Then	O
,	O
with	O
probability	O
proportional	O
to	O
e	O
0.2	O
S	O
,	O
we	O
importancesampled	O
200	O
clips	O
from	O
the	O
set	O
of	O
audio	O
clips	O
of	O
the	O
top	O
10	O
,	O
000	O
English	O
words	O
,	O
each	O
spoken	O
by	O
all	O
16	O
voices	O
offered	O
by	O
Amazon	O
Polly	O
(	O
spanning	O
American	O
,	O
British	O
,	O
Indian	O
,	O
Australian	O
,	O
and	O
Welsh	O
accents	O
,	O
and	O
male	O
and	O
female	O
voices	O
)	O
.	O
The	O
distributions	O
of	O
S	O
in	O
the	O
population	O
and	O
our	O
sample	O
is	O
shown	O
in	O
Figure	O
2	O
.	O
Autoencoder	B-MethodName
details	O
Our	O
autoencoder	B-MethodName
operates	O
on	O
one	O
-	O
second	O
audio	O
clips	O
sampled	O
at	O
22	O
,	O
050	O
Hz	O
,	O
which	O
are	O
converted	O
to	O
spectrograms	O
with	O
a	O
window	O
size	O
of	O
256	O
and	O
then	O
flattened	O
to	O
vectors	O
in	O
R	O
90	O
,	O
000	O
.	O
The	O
encoder	O
is	O
a	O
linear	O
map	O
to	O
R	O
512	O
with	O
ReLU	B-MethodName
activations	O
,	O
and	O
the	O
decoder	O
is	O
a	O
linear	O
map	O
back	O
to	O
R	O
90	O
,	O
000	O
space	O
with	O
pointwise	O
squaring	O
.	O
We	O
used	O
an	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
lr=0.01	O
,	O
training	O
on	O
a	O
corpus	O
of	O
16	O
,	O
000	O
clips	O
(	O
randomly	O
resampled	O
to	O
between	O
0.6x	O
and	O
1.0x	O
the	O
original	O
speed	O
)	O
for	O
70	O
epochs	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	O
(	O
≈	O
8	O
hours	O
on	O
an	O
AWS	O
c5.4xlarge	O
EC2	O
instance	O
)	O
.	O

To	O
improve	O
the	O
robustness	O
of	O
our	O
model	O
,	O
our	O
team	O
apply	O
cross	O
-	O
validation	O
for	O
training	O
.	O
Firstly	O
,	O
by	O
using	O
different	O
random	O
seeds	B-DatasetName
,	O
we	O
divided	O
the	O
training	O
set	O
which	O
included	O
all	O
three	O
languages	O
ten	O
times	O
.	O
Through	O
this	O
process	O
,	O
we	O
obtained	O
10	O
folds	O
of	O
data	O
,	O
which	O
contain	O
15768	O
training	O
samples	O
and	O
1751	O
validation	O
samples	O
in	O
each	O
fold	O
.	O
During	O
the	O
finetuning	O
process	O
,	O
we	O
used	O
random	B-MethodName
search	I-MethodName
to	O
optimize	O
hyper	O
-	O
parameters	O
like	O
epochs	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
.	O
By	O
using	O
F1	B-MetricName
-	I-MetricName
Score	I-MetricName
as	O
our	O
evaluation	O
metric	O
,	O
the	O
best	O
model	O
at	O
all	O
the	O
ten	O
-	O
fold	O
of	O
training	O
is	O
saved	O
.	O
Finally	O
,	O
by	O
making	O
predictions	O
on	O
the	O
test	O
set	O
,	O
we	O
save	O
the	O
mean	O
of	O
the	O
probability	O
of	O
all	O
ten	O
best	O
-	O
saved	O
models	O
.	O
This	O
result	O
is	O
our	O
final	O
output	O
of	O
ERNIE	O
-	O
M.	O
Cross	O
-	O
validation	O
process	O
is	O
shown	O
in	O
Figure	O
1	O
.	O

Predicting	O
user	O
intent	O
and	O
detecting	O
the	O
corresponding	O
slots	O
from	O
text	O
are	O
two	O
key	O
problems	O
in	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
(	O
NLU	O
)	O
.	O
Since	O
annotated	O
datasets	O
are	O
only	O
available	O
for	O
a	O
handful	O
of	O
languages	O
,	O
our	O
work	O
focuses	O
particularly	O
on	O
a	O
zero	O
-	O
shot	O
scenario	O
where	O
the	O
target	O
language	O
is	O
unseen	O
during	O
training	O
.	O
In	O
the	O
context	O
of	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
,	O
this	O
task	O
is	O
typically	O
approached	O
using	O
representations	O
from	O
pre	O
-	O
trained	O
multilingual	O
language	O
models	O
such	O
as	O
mBERT	B-MethodName
or	O
by	O
fine	O
-	O
tuning	O
on	O
data	O
automatically	O
translated	O
into	O
the	O
target	O
language	O
.	O
We	O
propose	O
a	O
novel	O
method	O
which	O
augments	O
monolingual	O
source	O
data	O
using	O
multilingual	O
code	O
-	O
switching	O
via	O
random	O
translations	O
,	O
to	O
enhance	O
generalizability	O
of	O
large	O
multilingual	O
language	O
models	O
when	O
fine	O
-	O
tuning	O
them	O
for	O
downstream	O
tasks	O
.	O
Experiments	O
on	O
the	O
Mul	O
-	O
tiATIS++	O
benchmark	O
show	O
that	O
our	O
method	O
leads	O
to	O
an	O
average	O
improvement	O
of	O
+4.2	O
%	O
in	O
accuracy	B-MetricName
for	O
the	O
intent	O
task	O
and	O
+1.8	O
%	O
in	O
F1	B-MetricName
for	O
the	O
slot	O
-	O
filling	O
task	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
across	O
8	O
typologically	O
diverse	O
languages	O
.	O
We	O
also	O
study	O
the	O
impact	O
of	O
code	O
-	O
switching	O
into	O
different	O
families	O
of	O
languages	O
on	O
downstream	O
performance	O
.	O
Furthermore	O
,	O
we	O
present	O
an	O
application	O
of	O
our	O
method	O
for	O
crisis	O
informatics	O
using	O
a	O
new	O
human	O
-	O
annotated	O
tweet	O
dataset	O
of	O
slot	B-TaskName
filling	I-TaskName
in	O
English	O
and	O
Haitian	O
Creole	O
,	O
collected	O
during	O
the	O
Haiti	O
earthquake	O
.	O
1	O

Joint	O
training	O
is	O
traditionally	O
used	O
for	O
intent	O
prediction	O
and	O
slot	B-TaskName
filling	I-TaskName
to	O
exploit	O
the	O
correlation	O
between	O
the	O
two	O
tasks	O
.	O
This	O
is	O
done	O
by	O
feeding	O
the	O
feature	O
vectors	O
of	O
one	O
model	O
to	O
another	O
or	O
by	O
sharing	O
layers	O
of	O
a	O
neural	O
network	O
followed	O
by	O
training	O
the	O
tasks	O
together	O
.	O
So	O
,	O
a	O
standard	O
joint	O
model	O
loss	B-MetricName
can	O
be	O
defined	O
as	O
a	O
combination	O
of	O
intent	O
(	O
L	O
i	O
)	O
and	O
slot	O
(	O
L	O
sl	O
)	O
losses	O
.	O
i.e.	O
,	O
L	O
=	O
αL	O
i	O
+	O
βL	O
sl	O
,	O
where	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
are	O
corresponding	O
task	O
weights	O
.	O
Prior	O
works	O
(	O
Goo	O
et	O
al	O
,	O
2018	O
;	O
Schuster	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
and	O
Lane	O
,	O
2016	O
;	O
Haihong	O
et	O
al	O
,	O
2019	O
)	O
that	O
use	O
BiL	O
-	O
STM	O
or	O
RNN	O
are	O
now	O
modified	O
to	O
BERT	B-MethodName
-	O
based	O
implementations	O
explored	O
in	O
more	O
recent	O
works	O
Hardalov	O
et	O
al	O
,	O
2020	O
;	O
.	O
A	O
standard	O
Joint	O
model	O
consists	O
of	O
BERT	B-MethodName
outputs	O
from	O
the	O
final	O
hidden	O
state	O
(	O
classification	O
(	O
CLS	O
)	O
token	O
for	O
intent	O
and	O
m	O
word	O
tokens	O
for	O
slots	O
)	O
fed	O
to	O
linear	O
layers	O
to	O
get	O
intent	O
and	O
slot	O
predictions	O
.	O
Assuming	O
h	O
cls	O
represents	O
the	O
CLS	O
token	O
and	O
h	O
m	O
represents	O
a	O
token	O
from	O
the	O
remaining	O
word	O
-	O
level	O
tokens	O
,	O
the	O
BERT	B-MethodName
model	O
outputs	O
are	O
defined	O
as	O
:	O
p	O
i	O
=	O
sof	B-DatasetName
tmax	O
(	O
W	O
i	O
h	O
cls	O
+	O
b	O
i	O
)	O
p	O
sl	O
m	O
=	O
sof	B-DatasetName
tmax	O
(	O
W	O
sl	O
hm	O
+	O
b	O
sl	O
)	O
∀m	O
(	O
1	O
)	O
with	O
a	O
multi	O
-	O
class	O
cross	O
-	O
entropy	O
loss	B-MetricName
3	O
for	O
both	O
intent	O
(	O
L	O
i	O
)	O
and	O
slots	O
(	O
L	O
sl	O
)	O
.	O
We	O
will	O
use	O
this	O
model	O
as	O
2	O
Each	O
of	O
the	O
Sino	O
-	O
Tibetan	O
,	O
Koreanic	O
,	O
and	O
Japonic	O
families	O
have	O
a	O
single	O
high	O
-	O
resource	O
member	O
(	O
Chinese	O
,	O
Korean	O
,	O
Japanese	O
respectively	O
)	O
.	O
We	O
only	O
group	O
them	O
as	O
an	O
additional	O
interesting	O
data	O
point	O
,	O
not	O
because	O
we	O
ascribe	O
to	O
any	O
theories	O
that	O
link	O
them	O
typologically	O
.	O
3	O
L	O
=	O
−	O
1	O
n	O
∑︁	O
n	O
i=1	O
[	O
y	O
log	O
ŷ	O
]	O
our	O
baseline	O
for	O
joint	O
training	O
.	O
Our	O
goal	O
will	O
be	O
to	O
show	O
that	O
code	O
-	O
switching	O
on	O
top	O
of	O
joint	O
training	O
improves	O
the	O
performance	O
.	O
The	O
output	O
of	O
Algorithm	O
1	O
will	O
be	O
the	O
input	O
used	O
for	O
joint	O
training	O
on	O
BERT	B-MethodName
for	O
code	O
-	O
switched	O
experiments	O
.	O
3	O
Datasets	O
Benchmark	O
Dataset	O
.	O
We	O
use	O
the	O
latest	O
multilingual	O
benchmark	O
dataset	O
of	O
MultiATIS++	O
,	O
which	O
was	O
created	O
by	O
manually	O
translating	O
the	O
original	O
ATIS	B-DatasetName
(	O
Price	O
,	O
1990	O
)	O
dataset	O
from	O
English	O
(	O
en	O
)	O
to	O
8	O
other	O
languages	O
:	O
Spanish	O
(	O
es	O
)	O
,	O
Portuguese	O
(	O
pt	O
)	O
,	O
German	O
(	O
de	O
)	O
,	O
French	O
(	O
fr	O
)	O
,	O
Chinese	O
(	O
zh	O
)	O
,	O
Japanese	O
(	O
ja	O
)	O
,	O
Hindi	O
(	O
hi	O
)	O
,	O
and	O
Turkish	O
(	O
tr	O
)	O
.	O
The	O
dataset	O
consists	O
of	O
utterances	O
for	O
each	O
language	O
with	O
an	O
'	O
intent	O
'	O
label	O
for	O
'	O
flight	O
intent	O
'	O
and	O
'	O
slot	O
'	O
labels	O
for	O
the	O
word	O
tokens	O
in	O
BIO	O
format	O
.	O
A	O
sample	O
datapoint	O
in	O
English	O
is	O
shown	O
in	O
Figure	O
2	O
.	O
model	O
on	O
the	O
validation	O
set	O
.	O
Consistent	O
with	O
the	O
metrics	O
reported	O
for	O
intent	O
prediction	O
and	O
slot	B-TaskName
filling	I-TaskName
evaluation	O
in	O
the	O
past	O
,	O
we	O
also	O
accuracy	B-MetricName
for	O
intent	O
and	O
micro	B-MetricName
F1	I-MetricName
5	O
to	O
measure	O
slot	O
performance	O
.	O

Effect	O
of	O
Multilingual	O
Code	O
-	O
Switching	O
.	O
Table	O
3	O
describes	O
performance	O
evaluation	O
on	O
the	O
Multi	O
-	O
ATIS++	O
dataset	O
.	O
When	O
compared	O
to	O
the	O
state	O
-	O
ofthe	O
-	O
art	O
jointly	O
trained	O
English	O
-	O
only	O
baseline	O
,	O
we	O
see	O
a	O
+4.2	O
%	O
boost	O
in	O
intent	O
accuracy	B-MetricName
and	O
+1.8	O
%	O
boost	O
in	O
slot	O
F1	B-MetricName
scores	O
on	O
average	O
by	O
augmenting	O
the	O
dataset	O
via	O
multilingual	O
code	O
-	O
switching	O
without	O
requiring	O
the	O
target	O
language	O
.	O
From	O
the	O
significance	O
tests	O
,	O
except	O
for	O
Spanish	O
and	O
German	O
,	O
all	O
other	O
languages	O
were	O
helped	O
by	O
code	O
-	O
switching	O
for	O
intent	B-TaskName
detection	I-TaskName
.	O
For	O
slot	B-TaskName
filling	I-TaskName
,	O
improvement	O
on	O
Portuguese	O
and	O
French	O
is	O
insignificant	O
.	O
This	O
suggests	O
that	O
code	O
-	O
switching	O
primarily	O
helped	O
languages	O
that	O
are	O
morphologically	O
more	O
different	O
from	O
the	O
source	O
language	O
(	O
English	O
)	O
.	O
For	O
example	O
,	O
Hindi	O
and	O
Turkish	O
have	O
the	O
highest	O
intent	O
performance	O
improvement	O
of	O
+16.1	O
%	O
and	O
+9.8	O
%	O
respectively	O
.	O
And	O
for	O
slots	O
,	O
Hindi	O
and	O
Chinese	O
with	O
+6.0	O
%	O
and	O
+4.3	O
%	O
respectively	O
.	O
Japanese	O
showed	O
+4	O
%	O
improvement	O
for	O
intent	O
and	O
+3.4	O
%	O
for	O
slots	O
.	O
The	O
runtime	O
of	O
the	O
models	O
in	O
Table	O
5	O
(	O
Appendix	O
B	O
)	O
shows	O
that	O
code	O
-	O
switching	O
is	O
expensive	O
,	O
taking	O
up	O
to	O
five	O
hours	O
for	O
five	O
augmentation	O
rounds	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
5	O
)	O
.	O
This	O
is	O
because	O
there	O
are	O
k	O
times	O
more	O
data	O
compared	O
to	O
the	O
monolingual	O
source	O
data	O
.	O
Increasing	O
the	O
number	O
of	O
code	O
-	O
switchings	O
(	O
k	O
)	O
for	O
a	O
sentence	O
from	O
5	O
to	O
50	O
improves	O
the	O
performance	O
by	O
+1	O
%	O
,	O
while	O
increasing	O
the	O
run	O
-	O
time	O
by	O
a	O
large	O
margin	O
.	O
Hence	O
,	O
such	O
tradeoffs	O
should	O
be	O
considered	O
when	O
picking	O
k	O
for	O
real	O
-	O
world	O
applications	O
where	O
time	O
to	O
deployment	O
might	O
be	O
of	O
the	O
essense	O
.	O
In	O
the	O
translate	O
-	O
train	O
(	O
upper	O
bound	O
)	O
scenario	O
,	O
it	O
is	O
not	O
immediately	O
clear	O
if	O
augmentation	O
helps	O
,	O
since	O
data	O
in	O
the	O
same	O
language	O
as	O
the	O
target	O
are	O
always	O
preferable	O
to	O
other	O
language	O
or	O
code	O
-	O
switched	O
data	O
.	O
At	O
a	O
minimum	O
,	O
augmentation	O
does	O
not	O
hinder	O
upper	O
-	O
bound	O
performance	O
(	O
Table	O
3	O
)	O
.	O
For	O
both	O
intent	O
and	O
slot	O
performance	O
,	O
the	O
chunklevel	O
model	O
remained	O
robust	O
across	O
the	O
languages	O
.	O
For	O
intent	O
,	O
the	O
difference	O
between	O
word	O
-	O
level	O
and	O
sentence	O
-	O
level	O
was	O
insignificant	O
.	O
For	O
slot	O
,	O
sentencelevel	O
was	O
in	O
par	O
with	O
chunk	O
-	O
level	O
on	O
average	O
.	O
Thus	O
,	O
we	O
think	O
that	O
code	O
-	O
switching	O
at	O
chunk	O
-	O
level	O
is	O
safer	O
for	O
avoiding	O
semantic	O
discrepancies	O
(	O
which	O
are	O
a	O
danger	O
in	O
the	O
word	O
-	O
level	O
)	O
while	O
also	O
better	O
encouraging	O
intra	O
-	O
sentence	O
language	O
neutrality	O
.	O
Evaluation	O
on	O
Disaster	B-DatasetName
Dataset	O
.	O
We	O
found	O
that	O
disaster	O
data	O
is	O
more	O
challenging	O
than	O
the	O
ATIS	B-DatasetName
dataset	O
for	O
transfer	B-TaskName
learning	I-TaskName
in	O
NLU	O
.	O
The	O
predictive	O
performance	O
is	O
shown	O
in	O
Table	O
4	O
.	O
Code	O
-	O
Switching	O
improved	O
intent	O
accuracy	B-MetricName
by	O
+12.5	O
%	O
and	O
slot	O
F1	B-MetricName
by	O
+2.3	O
%	O
,	O
which	O
is	O
quite	O
promising	O
considering	O
the	O
domain	O
mismatch	O
(	O
tweets	O
vs	O
airline	O
guides	O
)	O
.	O
Joint	O
training	O
added	O
+0.9	O
%	O
improvement	O
to	O
intent	O
accuracy	B-MetricName
,	O
however	O
did	O
not	O
seem	O
to	O
help	O
slot	O
F1	B-MetricName
.	O
This	O
might	O
imply	O
a	O
weaker	O
correlation	O
between	O
the	O
two	O
tasks	O
in	O
real	O
-	O
world	O
data	O
,	O
i.e.	O
a	O
mention	O
of	O
'	O
food	O
'	O
or	O
'	O
shelter	O
'	O
in	O
a	O
tweet	O
may	O
not	O
always	O
mean	O
that	O
there	O
is	O
a	O
'	O
request	O
'	O
or	O
vice	O
-	O
versa	O
.	O
The	O
upper	O
bound	O
of	O
translate	O
-	O
train	O
method	O
did	O
not	O
perform	O
any	O
better	O
than	O
the	O
randomly	O
code	O
-	O
switched	O
model	O
which	O
seemed	O
counter	O
-	O
intuitive	O
.	O
This	O
might	O
be	O
due	O
to	O
the	O
lack	O
of	O
strong	O
representation	O
for	O
Haitian	O
Creole	O
in	O
the	O
pre	O
-	O
trained	O
model	O
,	O
although	O
it	O
is	O
similar	O
to	O
French	O
,	O
or	O
due	O
to	O
the	O
limitation	O
of	O
the	O
machine	B-TaskName
translation	I-TaskName
system	O
.	O
Impact	O
of	O
Language	O
Families	O
.	O
Results	O
of	O
language	O
family	O
analysis	O
are	O
shown	O
in	O
Figure	O
3	O
for	O
the	O
4	O
languages	O
that	O
showed	O
significant	O
improvements	O
for	O
both	O
intent	O
and	O
slots	O
in	O
Table	O
3	O
.	O
The	O
input	O
in	O
English	O
is	O
independently	O
code	O
-	O
switched	O
using	O
6	O
different	O
language	O
families	O
.	O
Note	O
that	O
the	O
target	O
language	O
is	O
always	O
excluded	O
from	O
the	O
group	O
when	O
evaluating	O
on	O
the	O
same	O
,	O
i.e.	O
Hindi	O
is	O
excluded	O
from	O
Indo	O
-	O
Aryan	O
family	O
when	O
that	O
family	O
is	O
being	O
evaluated	O
on	O
it	O
.	O
Translate	O
-	O
train	O
model	O
is	O
provided	O
as	O
a	O
frame	O
of	O
reference	O
and	O
upper	O
bound	O
.	O
Generally	O
,	O
as	O
expected	O
,	O
we	O
found	O
that	O
language	O
families	O
helped	O
their	O
corresponding	O
languages	O
,	O
i.e.	O
Romance	O
helped	O
Spanish	O
,	O
Germanic	O
helped	O
German	O
,	O
and	O
so	O
on	O
.	O
An	O
exception	O
is	O
our	O
loose	O
group	O
of	O
Sino	O
-	O
Tibetan	O
,	O
Koreanic	O
,	O
and	O
Japonic	O
languages	O
-	O
for	O
both	O
Chinese	O
and	O
Japanese	O
,	O
languages	O
from	O
the	O
Turkic	O
language	O
family	O
helped	O
more	O
than	O
others	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
Sino	O
-	O
tibetan	O
,	O
Japonic	O
,	O
and	O
Koreanic	O
group	O
helps	O
Hindi	O
more	O
than	O
other	O
Indo	O
-	O
European	O
languages	O
.	O
We	O
believe	O
this	O
highlights	O
the	O
necessity	O
for	O
methods	O
like	O
the	O
one	O
of	O
Xia	O
et	O
al	O
(	O
2020	O
)	O
that	O
can	O
a	O
priori	O
identify	O
the	O
best	O
helper	O
language	O
or	O
group	O
of	O
languages	O
that	O
can	O
benefit	O
downstream	O
tasks	O
for	O
low	O
resource	O
languages	O
.	O
Control	O
Experiments	O
on	O
k.	O
Hyperparameter	O
k	O
controls	O
the	O
amount	O
of	O
code	O
-	O
switched	O
data	O
.	O
k	B-HyperparameterName
=	I-HyperparameterName
0	B-DatasetName
represents	O
original	O
size	O
with	O
no	O
code	O
-	O
switching	O
,	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
represents	O
original	O
size	O
with	O
code	O
-	O
switching	O
,	O
and	O
k	B-HyperparameterName
=	I-HyperparameterName
10	O
means	O
10	O
-	O
times	O
more	O
code	O
-	O
switched	O
data	O
than	O
the	O
original	O
.	O
The	O
main	O
experiments	O
in	O
Table	O
3	O
use	O
k	B-HyperparameterName
=	I-HyperparameterName
5	O
.	O
Figure	O
4	O
shows	O
how	O
varying	O
k	O
affects	O
performance	O
.	O
For	O
this	O
analysis	O
,	O
we	O
consider	O
4	O
target	O
languages	O
on	O
which	O
code	O
-	O
switching	O
produced	O
significant	O
results	O
in	O
Table	O
3	O
on	O
both	O
Intent	O
Accuracy	B-MetricName
and	O
Slot	O
F1	B-MetricName
:	O
Chinese	O
,	O
Japanese	O
,	O
Hindi	O
,	O
and	O
Turkish	O
.	O
Intuitively	O
,	O
we	O
observe	O
that	O
as	O
k	O
increases	O
,	O
too	O
much	O
code	O
-	O
switching	O
becomes	O
expensive	O
in	O
terms	O
of	O
runtime	O
,	O
while	O
performance	O
improvement	O
slowly	O
plateaus	O
.	O
For	O
Slot	O
F1	B-MetricName
performance	O
in	O
all	O
four	O
cases	O
,	O
unlike	O
Intent	O
,	O
we	O
observe	O
an	O
interesting	O
dip	O
when	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
,	O
which	O
represents	O
the	O
augmentation	O
having	O
just	O
one	O
copy	O
of	O
codeswitching	O
(	O
without	O
the	O
original	O
non	O
-	O
code	O
-	O
switched	O
data	O
)	O
,	O
as	O
compared	O
to	O
k	B-HyperparameterName
=	I-HyperparameterName
0	B-DatasetName
.	O
Adding	O
the	O
original	O
data	O
to	O
one	O
round	O
of	O
code	O
-	O
switched	O
data	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
2	O
)	O
leads	O
to	O
big	O
improvements	O
.	O
Overall	O
,	O
we	O
see	O
improvement	O
for	O
both	O
tasks	O
,	O
with	O
Slot	O
F1	B-MetricName
plateauing	O
earlier	O
.	O
Table	O
5	O
and	O
Figure	O
10	O
in	O
Appendix	O
B	O
show	O
the	O
impact	O
of	O
code	O
-	O
switching	O
on	O
training	O
runtime	O
,	O
which	O
increases	O
as	O
k	O
increases	O
.	O
Thus	O
,	O
finding	O
an	O
optimal	O
value	O
of	O
k	O
and	O
specific	O
language	O
groups	O
are	O
essential	O
for	O
downstream	O
applications	O
.	O
mBERT	B-MethodName
versus	O
XLM	B-MethodName
-	O
R.	O
Additional	O
performance	O
evaluations	O
and	O
benefits	O
of	O
code	O
-	O
switching	O
on	O
XLM	B-MethodName
-	O
R	O
(	O
Conneau	O
et	O
al	O
,	O
2020a	O
)	O
,	O
a	O
stronger	O
multilingual	O
language	O
model	O
,	O
are	O
provided	O
in	O
Appendix	O
A.	O
Note	O
that	O
XLM	B-MethodName
-	O
R	O
is	O
trained	O
using	O
Common	O
-	O
Crawl	O
and	O
is	O
likely	O
to	O
be	O
exposed	O
to	O
some	O
code	O
-	O
switched	O
data	O
.	O
Thus	O
,	O
we	O
focus	O
primarily	O
on	O
mBERT	B-MethodName
which	O
largely	O
remains	O
monolingual	O
at	O
the	O
sentence	O
-	O
level	O
to	O
identify	O
the	O
unbiased	O
impact	O
of	O
code	O
-	O
switching	O
during	O
fine	O
-	O
tuning	O
.	O
Furthermore	O
,	O
runtime	O
and	O
hyperparameter	O
tuning	O
along	O
with	O
insights	O
into	O
layers	O
to	O
freeze	O
before	O
training	O
are	O
shown	O
in	O
Appendix	O
B.	O
Error	B-MetricName
Analysis	O
.	O
Selecting	O
intent	O
classes	O
with	O
support	O
>	O
10	O
,	O
Figure	O
5	O
shows	O
how	O
each	O
class	O
is	O
positively	O
or	O
negatively	O
impacted	O
by	O
code	O
-	O
switching	O
.	O
Improvement	O
was	O
primarily	O
on	O
'	O
airfare	O
'	O
,	O
'	O
distance	O
'	O
'	O
capacity	O
'	O
,	O
'	O
airline	O
'	O
,	O
and	O
'	O
ground_service	O
'	O
which	O
had	O
longer	O
sentences	O
such	O
as	O
'	O
Please	O
tell	O
me	O
which	O
airline	O
has	O
the	O
most	O
departures	O
from	O
Atlanta	O
'	O
when	O
compared	O
to	O
'	O
abbreviations	O
'	O
and	O
'	O
airport	O
'	O
classes	O
that	O
included	O
very	O
short	O
phrases	O
like	O
'	O
What	O
does	O
EA	O
mean	O
?	O
'	O
However	O
,	O
note	O
that	O
Spanish	O
and	O
German	O
did	O
not	O
improve	O
much	O
,	O
aligning	O
with	O
our	O
results	O
in	O
Table	O
3	O
.	O
For	O
slot	O
labels	O
in	O
Figure	O
6	O
,	O
we	O
selected	O
the	O
ones	O
with	O
support	O
>	O
50	O
and	O
that	O
have	O
different	O
characteristics	O
,	O
e.g.	O
'	O
name	O
'	O
,	O
'	O
code	O
'	O
,	O
etc	O
.	O
The	O
overall	O
trend	O
in	O
slot	O
performance	O
shows	O
improvements	O
for	O
labels	O
such	O
as	O
'	O
day_name	O
'	O
,	O
'	O
airport_code	O
'	O
,	O
and	O
'	O
city_name	O
'	O
and	O
slight	O
variations	O
in	O
labels	O
such	O
as	O
'	O
fight_number	O
'	O
and	O
'	O
period_of_day	O
'	O
,	O
implying	O
textual	O
slots	O
benefiting	O
over	O
numeric	O
ones	O
.	O

We	O
conduct	O
an	O
additional	O
analysis	O
on	O
XLM	B-MethodName
-	O
R	O
(	O
Conneau	O
et	O
al	O
,	O
2020a	O
)	O
and	O
compare	O
it	O
with	O
mBERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
implementation	O
is	O
very	O
similar	O
in	O
PyTorch	O
(	O
Paszke	O
et	O
al	O
,	O
2019	O
)	O
but	O
using	O
the	O
pre	O
-	O
trained	O
xlm	B-MethodName
-	O
roberta	O
-	O
base	O
with	O
RobertaForSequenceClassification	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
as	O
the	O
XLM	B-MethodName
-	O
R	O
model	O
.	O
We	O
observe	O
that	O
,	O
setting	O
k	B-HyperparameterName
=	I-HyperparameterName
5	O
,	O
XLM	B-MethodName
-	O
R	O
outperforms	O
mBERT	B-MethodName
on	O
average	O
(	O
by	O
2	O
%	O
Intent	O
Accuracy	B-MetricName
and	O
1.5	O
%	O
Slot	O
F1	B-MetricName
)	O
.	O
Individually	O
,	O
XLM	B-MethodName
-	O
R	O
improved	O
Chinese	O
,	O
Japanese	O
,	O
Portuguese	O
,	O
and	O
Turkish	O
for	O
Intent	O
Prediction	O
and	O
German	O
,	O
Chinese	O
,	O
Japanese	O
,	O
Portuguese	O
,	O
and	O
Hindi	O
for	O
Slot	B-TaskName
Filling	I-TaskName
as	O
shown	O
in	O
Figure	O
7	O
.	O
We	O
observe	O
a	O
trend	O
similar	O
to	O
mBERT	B-MethodName
with	O
k	O
on	O
XLM	B-MethodName
-	O
R	O
shown	O
in	O
Figure	O
8	O
.	O
However	O
,	O
for	O
XLM	B-MethodName
-	O
R	O
,	O
we	O
observe	O
that	O
randomized	O
code	O
-	O
switching	O
did	O
not	O
help	O
Chinese	O
for	O
Intent	O
Prediction	O
and	O
Hindi	O
for	O
Slot	O
F1	B-MetricName
.	O
If	O
codeswitched	O
to	O
a	O
specific	O
language	O
family	O
,	O
instead	O
of	O
switching	O
to	O
random	O
languages	O
,	O
it	O
might	O
improve	O
their	O
performance	O
.	O
A	O
deeper	O
dive	O
into	O
XLM	B-MethodName
-	O
R	O
and	O
language	O
families	O
are	O
left	O
for	O
future	O
work	O
.	O

For	O
joint	O
training	O
with	O
same	O
task	O
weights	O
,	O
we	O
tuned	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
using	O
grid	O
search	O
to	O
see	O
the	O
strength	O
of	O
correlation	O
between	O
the	O
tasks	O
.	O
For	O
intent	O
,	O
the	O
(	O
α	B-HyperparameterName
,	O
β	B-HyperparameterName
)	O
combination	O
of	O
(	O
1.0	O
,	O
0.6	O
)	O
performed	O
well	O
,	O
while	O
(	O
1.0	O
,	O
1.0	O
)	O
for	O
slots	O
.	O
This	O
suggests	O
that	O
intent	O
benefiting	O
slot	O
might	O
be	O
slightly	O
more	O
than	O
slot	O
benefiting	O
intent	O
.	O
Additionally	O
,	O
during	O
fine	O
-	O
tuning	O
,	O
freezing	O
the	O
layers	O
of	O
the	O
transformer	O
affected	O
the	O
model	O
performance	O
as	O
shown	O
in	O
Figure	O
9	O
.	O
Keeping	O
the	O
first	O
8	O
layers	O
frozen	O
gave	O
the	O
best	O
performance	O
.	O
By	O
freezing	O
the	O
earlier	O
layers	O
,	O
the	O
transformer	O
can	O
retain	O
its	O
most	O
fundamental	O
feature	O
information	O
gained	O
from	O
the	O
massive	O
pre	O
-	O
training	O
step	O
,	O
and	O
by	O
unfreezing	O
some	O
top	O
layers	O
,	O
it	O
can	O
undergo	O
fine	O
-	O
tuning	O
.	O
Additionally	O
,	O
latency	O
for	O
training	O
a	O
code	O
-	O
switched	O
model	O
is	O
shown	O
in	O
Table	O
5	O
and	O
how	O
runtime	O
varies	O
with	O
an	O
increase	O
in	O
k	O
is	O
shown	O
in	O
Figure	O
10	O
.	O

Document	O
-	O
level	O
information	O
extraction	O
(	O
IE	O
)	O
tasks	O
have	O
recently	O
begun	O
to	O
be	O
revisited	O
in	O
earnest	O
using	O
the	O
end	O
-	O
to	O
-	O
end	O
neural	O
network	O
techniques	O
that	O
have	O
been	O
successful	O
on	O
their	O
sentence	O
-	O
level	O
IE	O
counterparts	O
.	O
Evaluation	O
of	O
the	O
approaches	O
,	O
however	O
,	O
has	O
been	O
limited	O
in	O
a	O
number	O
of	O
dimensions	O
.	O
In	O
particular	O
,	O
the	O
precision	O
/	O
recall	O
/	O
F1	B-MetricName
scores	O
typically	O
reported	O
provide	O
few	O
insights	O
on	O
the	O
range	O
of	O
errors	O
the	O
models	O
make	O
.	O
We	O
build	O
on	O
the	O
work	O
of	O
Kummerfeld	O
and	O
Klein	O
(	O
2013	O
)	O
to	O
propose	O
a	O
transformation	O
-	O
based	O
framework	O
for	O
automating	O
error	O
analysis	O
in	O
document	O
-	O
level	O
event	O
and	O
(	O
N	O
-	O
ary	O
)	O
relation	B-TaskName
extraction	I-TaskName
.	O
We	O
employ	O
our	O
framework	O
to	O
compare	O
two	O
state	O
-	O
of	O
-	O
theart	O
document	O
-	O
level	O
template	O
-	O
filling	O
approaches	O
on	O
datasets	O
from	O
three	O
domains	O
;	O
and	O
then	O
,	O
to	O
gauge	O
progress	O
in	O
IE	O
since	O
its	O
inception	O
30	O
years	O
ago	O
,	O
vs.	O
four	O
systems	O
from	O
the	O
MUC	O
-	O
4	O
(	O
1992	O
)	O
evaluation	O
.	O
1	O

Although	O
information	O
extraction	O
(	O
IE	O
)	O
research	O
has	O
almost	O
uniformly	O
focused	O
on	O
sentence	O
-	O
level	O
relation	O
and	O
event	B-TaskName
extraction	I-TaskName
(	O
Grishman	O
,	O
2019	O
)	O
,	O
the	O
earliest	O
research	O
in	O
the	O
area	O
formulated	O
the	O
task	O
at	O
the	O
document	O
level	O
.	O
Consider	O
,	O
for	O
example	O
,	O
the	O
first	O
large	O
-	O
scale	O
(	O
for	O
the	O
time	O
)	O
evaluations	O
of	O
IE	O
systems	O
-	O
e.g.	O
MUC	O
-	O
3	O
(	O
1991	O
)	O
and	O
MUC	O
-	O
4	O
(	O
1992	O
)	O
.	O
Each	O
involved	O
a	O
complex	O
document	B-TaskName
-	I-TaskName
level	I-TaskName
event	I-TaskName
extraction	I-TaskName
task	O
:	O
there	O
were	O
24	O
types	O
of	O
events	O
,	O
over	O
a	O
dozen	O
event	O
arguments	O
(	O
or	O
roles	O
)	O
to	O
be	O
identified	O
for	O
each	O
event	O
;	O
documents	O
could	O
contain	O
zero	O
to	O
tens	O
of	O
events	O
,	O
and	O
extracting	O
argument	O
entities	O
(	O
or	O
role	O
fillers	O
)	O
required	O
noun	O
phrase	O
coreference	B-TaskName
resolution	I-TaskName
to	O
ensure	O
interpretability	O
for	O
the	O
end	O
-	O
user	O
(	O
e.g.	O
to	O
ensure	O
that	O
multiple	O
distinct	O
mentions	O
of	O
the	O
same	O
entity	O
in	O
the	O
output	O
were	O
not	O
misinterpreted	O
as	O
references	O
to	O
distinct	O
entities	O
)	O
.	O
The	O
task	O
was	O
challenging	O
:	O
information	O
relevant	O
for	O
a	O
single	O
event	O
could	O
be	O
scattered	O
across	O
the	O
document	O
or	O
repeated	O
in	O
multiple	O
places	O
;	O
relevant	O
information	O
might	O
need	O
to	O
be	O
shared	O
across	O
multiple	O
events	O
;	O
information	O
regarding	O
different	O
events	O
could	O
be	O
intermingled	O
.	O
In	O
Figure	O
1	O
,	O
for	O
example	O
,	O
the	O
DISEASE	O
"	O
Newcastle	O
"	O
is	O
mentioned	O
well	O
before	O
its	O
associated	O
event	O
is	O
mentioned	O
(	O
via	O
the	O
triggering	O
phrase	O
"	O
the	O
disease	O
has	O
killed	O
"	O
)	O
;	O
that	O
same	O
mention	O
of	O
"	O
Newcastle	O
"	O
must	O
again	O
be	O
recognized	O
as	O
the	O
DISEASE	O
in	O
a	O
second	O
event	O
;	O
and	O
the	O
COUNTRY	O
of	O
the	O
first	O
event	O
(	O
"	O
Honduras	O
"	O
)	O
appears	O
only	O
in	O
the	O
sentence	O
describing	O
the	O
second	O
event	O
.	O
In	O
fact	O
,	O
the	O
problem	O
of	O
document	O
-	O
level	O
information	O
extraction	O
has	O
only	O
recently	O
begun	O
to	O
be	O
revisited	O
(	O
Quirk	O
and	O
Poon	O
,	O
2017	O
;	O
Jain	O
et	O
al	O
,	O
2020	O
;	O
Du	O
et	O
al	O
,	O
2021b	O
,	O
a	O
;	O
Li	O
et	O
al	O
,	O
2021	O
;	O
Du	O
,	O
2021	O
;	O
Yang	O
et	O
al	O
,	O
2021	O
)	O
in	O
part	O
in	O
an	O
attempt	O
to	O
test	O
the	O
power	O
of	O
end	O
-	O
to	O
-	O
end	O
neural	O
network	O
techniques	O
that	O
have	O
been	O
so	O
successful	O
on	O
their	O
sentence	O
-	O
level	O
counterparts	O
.	O
2	O
Evaluation	O
,	O
however	O
,	O
has	O
been	O
limited	O
in	O
a	O
number	O
of	O
ways	O
.	O
First	O
,	O
despite	O
the	O
relative	O
complexity	O
of	O
the	O
task	O
,	O
approaches	O
are	O
only	O
evaluated	O
with	O
respect	O
to	O
their	O
overall	O
performance	O
scores	O
(	O
e.g.	O
precision	O
,	O
recall	O
,	O
and	O
F1	B-MetricName
)	O
.	O
Even	O
though	O
scores	O
at	O
the	O
role	O
level	O
are	O
sometimes	O
included	O
,	O
no	O
systematic	O
analysis	O
or	O
characterization	O
of	O
the	O
types	O
of	O
errors	O
that	O
occur	O
is	O
typically	O
done	O
.	O
The	O
latter	O
is	O
needed	O
to	O
determine	O
strategies	O
to	O
improve	O
performance	O
,	O
to	O
obtain	O
more	O
informative	O
cross	O
-	O
system	O
and	O
cross	O
-	O
genre	O
comparisons	O
,	O
and	O
to	O
identify	O
and	O
track	O
broader	O
advances	O
in	O
the	O
field	O
as	O
the	O
underlying	O
approaches	O
evolve	O
.	O
To	O
date	O
,	O
for	O
example	O
,	O
there	O
has	O
been	O
no	O
attempt	O
to	O
directly	O
compare	O
the	O
error	O
landscape	O
and	O
distribution	O
of	O
[	O
Trigger	O
]	O
[	O
Trigger	O
]	O
Input	O
Document	O

As	O
in	O
Jurafsky	O
and	O
Martin	O
(	O
2021	O
)	O
,	O
we	O
will	O
refer	O
to	O
document	O
-	O
level	O
information	O
extraction	O
tasks	O
as	O
template	O
-	O
filling	O
tasks	O
and	O
use	O
the	O
term	O
going	O
forward	O
to	O
refer	O
to	O
both	O
event	B-TaskName
extraction	I-TaskName
and	O
documentlevel	O
relation	B-TaskName
extraction	I-TaskName
tasks	O
.	O
Given	O
a	O
document	O
,	O
D	O
,	O
and	O
an	O
IE	O
template	O
specification	O
consisting	O
of	O
a	O
predetermined	O
list	O
of	O
roles	O
R	O
1	O
,	O
R	O
2	O
,	O
...	O
,	O
R	O
i	O
associated	O
with	O
each	O
type	O
of	O
relevant	O
event	O
for	O
the	O
task	O
of	O
interest	O
,	O
the	O
goal	O
for	O
template	O
filling	O
is	O
to	O
extract	O
from	O
D	O
,	O
one	O
output	O
template	O
,	O
T	O
for	O
every	O
relevant	O
event	O
/	O
relation	O
e	O
1	O
,	O
e	O
2	O
,	O
.	O
.	O
.	O
,	O
e	O
n	O
present	O
in	O
the	O
document	O
.	O
Notably	O
,	O
in	O
the	O
general	O
case	O
,	O
n	O
≥	O
0	B-DatasetName
and	O
is	O
not	O
specified	O
as	O
part	O
of	O
the	O
input	O
.	O
In	O
each	O
output	O
template	O
,	O
its	O
roles	O
are	O
filled	O
with	O
the	O
corresponding	O
role	O
filler	O
(	O
s	O
)	O
,	O
which	O
can	O
be	O
inferred	O
or	O
extracted	O
from	O
the	O
document	O
depending	O
on	O
the	O
predetermined	O
role	O
types	O
.	O
We	O
consider	O
two	O
role	O
types	O
here	O
:	O
5	O
Set	O
-	O
fill	O
roles	O
,	O
which	O
must	O
be	O
filled	O
with	O
exactly	O
one	O
role	O
filler	O
from	O
a	O
finite	O
set	O
supplied	O
in	O
the	O
template	O
specification	O
.	O
An	O
example	O
of	O
a	O
set	O
-	O
fill	O
role	O
in	O
Figure	O
1	O
is	O
STATUS	O
,	O
which	O
can	O
be	O
confirmed	O
,	O
possible	O
,	O
or	O
suspected	O
.	O
String	O
-	O
fill	O
roles	O
,	O
whose	O
role	O
filler	O
(	O
s	O
)	O
are	O
spans	O
extracted	O
from	O
the	O
document	O
,	O
or	O
left	O
empty	O
if	O
no	O
corresponding	O
role	O
filler	O
is	O
found	O
in	O
the	O
document	O
.	O
VICTIMS	O
,	O
DISEASE	O
and	O
COUNTRY	O
are	O
string	O
-	O
fill	O
roles	O
in	O
Figure	O
1	O
.	O
Some	O
string	O
-	O
fill	O
roles	O
allow	O
multiple	O
fillers	O
;	O
for	O
example	O
,	O
there	O
might	O
be	O
more	O
than	O
one	O
VICTIMS	O
.	O
Importantly	O
,	O
for	O
document	O
-	O
level	O
template	O
filling	O
,	O
exactly	O
one	O
string	O
should	O
be	O
included	O
for	O
each	O
role	O
filler	O
entity	O
(	O
typically	O
a	O
canonical	O
mention	O
of	O
the	O
entity	O
)	O
,	O
i.e.	O
coreferent	O
mentions	O
of	O
the	O
same	O
entity	O
are	O
not	O
permitted	O
.	O
Evaluation	O
.	O
We	O
use	O
the	O
standard	O
(	O
exact	O
-	O
match	O
)	O
F1	B-MetricName
score	I-MetricName
(	O
Chinchor	O
,	O
1991	O
)	O
to	O
evaluate	O
the	O
output	O
5	O
There	O
are	O
potentially	O
more	O
role	O
types	O
depending	O
on	O
the	O
dataset	O
(	O
e.g.	O
normalized	O
dates	O
,	O
times	O
,	O
locations	O
)	O
;	O
we	O
will	O
not	O
consider	O
those	O
here	O
.	O
produced	O
by	O
a	O
template	O
-	O
filling	O
system	O
:	O
F	O
1	O
=	O
2	O
Precision	B-MetricName
Recall	B-MetricName
Precision	B-MetricName
+	O
Recall	B-MetricName

The	O
first	O
stage	O
of	O
the	O
error	O
analysis	O
tool	O
involves	O
matching	O
each	O
system	O
-	O
predicted	O
template	O
to	O
the	O
best	O
-	O
matching	O
gold	O
template	O
for	O
each	O
document	O
in	O
the	O
dataset	O
.	O
In	O
particular	O
,	O
the	O
overall	O
F1	B-MetricName
score	I-MetricName
for	O
a	O
given	O
document	O
can	O
vary	O
based	O
on	O
how	O
a	O
predicted	O
template	O
is	O
individually	O
matched	O
with	O
a	O
gold	O
template	O
(	O
or	O
left	O
unmatched	O
)	O
.	O
Specifically	O
,	O
for	O
each	O
document	O
,	O
we	O
recursively	O
generate	O
all	O
possible	O
template	O
matchings	O
-	O
where	O
each	O
predicted	O
template	O
is	O
matched	O
(	O
if	O
possible	O
)	O
to	O
a	O
gold	O
template	O
.	O
In	O
particular	O
,	O
for	O
a	O
document	O
with	O
P	O
predicted	O
templates	O
and	O
G	O
gold	O
templates	O
,	O
the	O
total	O
number	O
of	O
possible	O
template	O
matchings	O
is	O
:	O
Note	O
that	O
template	O
matching	O
can	O
result	O
in	O
unmatched	O
predicted	O
templates	O
(	O
Spurious	O
Templates	O
)	O
,	O
as	O
well	O
as	O
unmatched	O
gold	O
templates	O
(	O
Missing	O
Templates	O
)	O
.	O
1	O
+	O
P	O
1	O
G	O
+	O
P	O
2	O
G	O
(	O
G	O
−	O
1	O
)	O
+	O
...	O
+	O
G	O
!	O
(	O
G	O
−	O
P	O
)	O
!	O
,	O
if	O
G	O
−	O
P	O
≥	O
0	B-DatasetName
1	O
+	O
P	O
1	O
G	O
+	O
P	O
2	O
G	O
(	O
G	O
−	O
1	O
)	O
+	O
...	O
+	O
P	O
G	O
G	O
!	O
,	O
if	O
G	O
−	O
P	O
<	O
0	B-DatasetName
=	O
min	O
(	O
P	O
,	O
G	O
)	O
i=0	O
P	O
i	O
G	O
!	O
(	O
G	O
−	O
i	O
)	O
!	O
Next	O
,	O
for	O
each	O
predicted	O
-	O
gold	O
pair	O
in	O
a	O
template	O
matching	O
,	O
we	O
iterate	O
through	O
all	O
its	O
roles	O
and	O
recursively	O
generate	O
all	O
possible	O
mention	O
matchings	O
,	O
in	O
each	O
of	O
which	O
a	O
predicted	O
role	O
filler	O
is	O
matched	O
(	O
if	O
possible	O
)	O
to	O
a	O
set	O
of	O
coreferent	O
gold	O
role	O
fillers	O
.	O
Similar	O
to	O
template	O
matching	O
,	O
the	O
process	O
of	O
mention	O
matching	O
can	O
also	O
result	O
in	O
unmatched	O
predicted	O
role	O
fillers	O
(	O
Spurious	O
Role	O
Fillers	O
)	O
and	O
unmatched	O
coreferent	O
sets	O
of	O
gold	O
role	O
fillers	O
(	O
Missing	O
Role	O
Fillers	O
)	O
.	O
Through	O
the	O
process	O
,	O
each	O
predicted	O
role	O
filler	O
increases	O
the	O
denominator	O
of	O
the	O
total	O
precision	O
by	O
1	O
,	O
and	O
each	O
set	O
of	O
coreferent	O
gold	O
role	O
fillers	O
increases	O
the	O
denominator	O
of	O
total	O
recall	O
by	O
1	O
.	O
Whenever	O
there	O
is	O
a	O
matched	O
mention	O
pair	O
in	O
which	O
the	O
predicted	O
role	O
filler	O
has	O
an	O
exact	B-MetricName
match	I-MetricName
to	O
an	O
element	O
of	O
the	O
set	O
of	O
coreferent	O
gold	O
role	O
fillers	O
,	O
this	O
adds	O
1	O
to	O
the	O
numerator	O
of	O
both	O
precision	O
and	O
recall	O
.	O
These	O
counts	O
are	O
calculated	O
for	O
each	O
template	O
matching	O
.	O
Using	O
precision	O
and	O
recall	O
,	O
the	O
total	O
F1	B-MetricName
score	I-MetricName
across	O
all	O
the	O
slots	O
/	O
roles	O
is	O
calculated	O
and	O
the	O
template	O
matching	O
with	O
the	O
highest	O
total	O
F1	B-MetricName
score	I-MetricName
is	O
chosen	O
.	O
If	O
there	O
are	O
ties	O
,	O
the	O
template	O
matching	O
with	O
the	O
fewest	O
errors	O
is	O
chosen	O
(	O
see	O
Section	O
4.3	O
)	O
.	O

The	O
second	O
part	O
of	O
the	O
error	O
analysis	O
tool	O
involves	O
changing	O
the	O
predicted	O
templates	O
to	O
the	O
desired	O
gold	O
templates	O
with	O
the	O
help	O
of	O
a	O
fixed	O
set	O
of	O
transformations	O
as	O
detailed	O
below	O
.	O
Alter	O
Span	O
transforms	O
a	O
role	O
filler	O
into	O
the	O
gold	O
role	O
filler	O
with	O
the	O
lowest	O
span	O
comparison	O
score	O
(	O
SCS	O
)	O
.	O
The	O
tool	O
provides	O
two	O
options	O
for	O
computing	O
the	O
SCS	O
between	O
two	O
spans	O
,	O
and	O
each	O
depends	O
only	O
on	O
the	O
starting	O
and	O
ending	O
indices	O
of	O
the	O
spans	O
.	O
6	O
SCS	O
can	O
be	O
interpreted	O
as	O
distance	O
and	O
is	O
0	B-DatasetName
between	O
two	O
identical	O
spans	O
,	O
and	O
1	O
for	O
non	O
-	O
overlapping	O
spans	O
.	O
The	O
two	O
modes	O
are	O
given	O
as	O
follows	O
:	O
a	O
)	O
absolute	O
:	O
This	O
mode	O
captures	O
the	O
(	O
positive	O
)	O
distance	O
between	O
the	O
starting	O
(	O
and	O
ending	O
)	O
character	O
offsets	O
of	O
spans	O
x	O
and	O
y	O
in	O
the	O
document	O
,	O
and	O
scales	O
that	O
value	O
by	O
the	O
sum	O
of	O
the	O
lengths	O
of	O
x	O
and	O
y	O
,	O
capping	O
it	O
at	O
a	O
maximum	O
of	O
1	O
.	O
SCS	O
=	O
max	O
1	O
,	O
|	O
xstart−ystart	O
|	O
+	O
|	O
x	O
end	O
−y	O
end	O
|	O
length	O
(	O
x	O
)	O
+	O
length	O
(	O
y	O
)	O
b	O
)	O
geometric	O
mean	O
:	O
This	O
mode	O
captures	O
the	O
degree	O
of	O
disjointedness	O
between	O
spans	O
x	O
and	O
y	O
by	O
dividing	O
the	O
length	O
of	O
the	O
overlap	O
between	O
the	O
two	O
spans	O
with	O
respect	O
to	O
each	O
of	O
their	O
lengths	O
,	O
multiplying	O
those	O
two	O
fractions	O
,	O
and	O
subtracting	O
the	O
final	O
result	O
from	O
1	O
.	O
If	O
si	O
is	O
the	O
length	O
of	O
the	O
intersection	O
of	O
x	O
and	O
y	O
,	O
and	O
neither	O
x	O
nor	O
y	O
have	O
length	O
0	B-DatasetName
,	O
SCS	O
is	O
calculated	O
as	O
shown	O
below	O
;	O
otherwise	O
,	O
SCS	O
is	O
1	O
.	O
overlap	O
=	O
min	O
(	O
x	O
end	O
,	O
y	O
end	O
)	O
−	O
max	O
(	O
x	O
start	O
,	O
y	O
start	O
)	O
si	O
=	O
max	O
(	O
0	B-DatasetName
,	O
overlap	O
)	O
SCS	O
=	O
1	O
−	O
si	O
2	O
length	O
(	O
x	O
)	O
*	O
length	O
(	O
y	O
)	O
Thus	O
,	O
if	O
the	O
predicted	O
role	O
filler	O
is	O
an	O
exact	B-MetricName
match	I-MetricName
for	O
the	O
gold	O
role	O
filler	O
,	O
the	O
SCS	O
is	O
0	B-DatasetName
.	O
If	O
there	O
is	O
some	O
overlap	O
between	O
the	O
spans	O
,	O
the	O
SCS	O
is	O
between	O
0	B-DatasetName
and	O
1	O
(	O
not	O
inclusive	O
)	O
,	O
and	O
if	O
there	O
is	O
no	O
overlap	O
between	O
the	O
spans	O
,	O
the	O
SCS	O
is	O
1	O
.	O
The	O
order	O
of	O
comparison	O
of	O
the	O
spans	O
does	O
n't	O
change	O
the	O
SCS	O
score	O
for	O
both	O
modes	O
.	O
As	O
the	O
absolute	O
mode	O
is	O
less	O
sensitive	O
to	O
changes	O
in	O
span	O
indices	O
as	O
compared	O
to	O
the	O
geometric	O
mean	O
,	O
we	O
chose	O
geometric	O
mean	O
for	O
our	O
analysis	O
,	O
as	O
tiny	O
changes	O
in	O
index	O
positions	O
result	O
in	O
a	O
bigger	O
change	O
in	O
the	O
SCS	O
score	O
.	O
Alter	O
Role	O
changes	O
the	O
role	O
of	O
a	O
role	O
filler	O
to	O
another	O
role	O
within	O
the	O
same	O
template	O
.	O
Remove	O
Duplicate	O
Role	O
Filler	O
removes	O
a	O
role	O
filler	O
that	O
is	O
coreferent	O
to	O
an	O
already	O
matched	O
role	O
filler	O
.	O
Remove	O
Cross	O
Template	O
Spurious	O
Role	O
Filler	O
removes	O
a	O
role	O
filler	O
that	O
would	O
be	O
correct	O
if	O
present	O
in	O
another	O
template	O
(	O
in	O
the	O
same	O
role	O
)	O
.	O
Remove	O
Spurious	O
Role	O
Filler	O
removes	O
a	O
role	O
filler	O
that	O
has	O
not	O
been	O
mentioned	O
in	O
any	O
of	O
the	O
gold	O
templates	O
for	O
a	O
given	O
document	O
.	O
Introduce	O
Role	O
Filler	O
introduces	O
a	O
role	O
filler	O
that	O
was	O
not	O
present	O
in	O
the	O
predicted	O
template	O
but	O
was	O
required	O
to	O
be	O
present	O
in	O
the	O
matching	O
gold	O
template	O
.	O
Remove	O
Template	O
removes	O
a	O
predicted	O
template	O
that	O
could	O
not	O
be	O
matched	O
to	O
any	O
gold	O
template	O
for	O
a	O
given	O
document	O
.	O
Introduce	O
Template	O
introduces	O
a	O
template	O
that	O
can	O
be	O
matched	O
to	O
an	O
unmatched	O
gold	O
template	O
for	O
a	O
given	O
document	O
.	O
For	O
a	O
given	O
document	O
,	O
all	O
singleton	O
Alter	O
Span	O
and	O
Alter	O
Role	O
transformations	O
,	O
as	O
well	O
as	O
sets	O
of	O
Alter	O
Span	O
+	O
Alter	O
Role	O
transformations	O
,	O
are	O
applied	O
first	O
.	O
The	O
other	O
transformations	O
are	O
applied	O
in	O
the	O
order	O
in	O
which	O
they	O
were	O
detected	O
,	O
which	O
is	O
dependent	O
on	O
the	O
order	O
of	O
predicted	O
and	O
gold	O
template	O
pairs	O
in	O
the	O
optimized	O
matching	O
and	O
the	O
order	O
of	O
the	O
slots	O
/	O
roles	O
in	O
the	O
template	O
.	O

Table	O
2	O
shows	O
the	O
results	O
of	O
evaluating	O
DyGIE++	O
and	O
GTT	O
on	O
the	O
SciREX	B-DatasetName
,	O
ProMED	O
,	O
and	O
MUC	O
-	O
4	O
datasets	O
.	O
We	O
can	O
see	O
that	O
all	O
models	O
perform	O
substantially	O
worse	O
on	O
scientific	O
texts	O
(	O
ProMED	O
,	O
SciREX	B-DatasetName
)	O
as	O
compared	O
to	O
news	O
(	O
MUC	O
-	O
4	O
)	O
,	O
likely	O
because	O
the	O
model	O
base	O
is	O
pretrained	O
for	O
generalpurpose	O
NLP	O
applications	O
(	O
BERT	B-MethodName
)	O
or	O
there	O
are	O
not	O
enough	O
examples	O
of	O
scientific	O
-	O
style	O
text	O
in	O
the	O
pretraining	O
corpus	O
(	O
SciBERT	O
)	O
.	O
In	O
addition	O
,	O
models	O
seem	O
to	O
perform	O
better	O
on	O
the	O
news	O
-	O
style	O
ProMED	O
dataset	O
than	O
the	O
scientific	O
-	O
paper	O
-	O
based	O
long	O
-	O
text	O
SciREX	B-DatasetName
dataset	O
.	O
This	O
can	O
be	O
explained	O
by	O
the	O
fact	O
that	O
all	O
four	O
models	O
handle	O
a	O
maximum	O
of	O
512	O
tokens	O
as	O
inputs	O
,	O
while	O
the	O
average	O
length	O
of	O
a	O
SciREX	B-DatasetName
document	O
is	O
5401	O
tokens	O
.	O
Thus	O
,	O
a	O
majority	O
of	O
the	O
text	O
is	O
truncated	O
and	O
,	O
hence	O
,	O
unavailable	O
to	O
the	O
models	O
.	O
Nevertheless	O
,	O
we	O
see	O
an	O
increase	O
in	O
F1	B-MetricName
scores	O
for	O
all	O
SciBERT	O
-	O
based	O
models	O
when	O
compared	O
to	O
their	O
BERT	B-MethodName
counterparts	O
for	O
the	O
SciREX	B-DatasetName
dataset	O
.	O
The	O
same	O
trend	O
is	O
seen	O
for	O
DyGIE++	O
for	O
ProMED	O
,	O
but	O
not	O
for	O
GTT	O
.	O
This	O
can	O
be	O
explained	O
by	O
the	O
fact	O
that	O
GTT	O
(	O
SciBERT	O
)	O
has	O
more	O
Missing	O
Template	O
errors	O
than	O
GTT	O
(	O
BERT	B-MethodName
)	O
.	O
So	O
even	O
if	O
GTT	O
(	O
SciBERT	O
)	O
performs	O
better	O
on	O
the	O
scientific	O
slot	O
VICTIMS	O
,	O
i.e.	O
it	O
extracts	O
more	O
scientific	O
information	O
,	O
it	O
does	O
not	O
identify	O
relevant	O
events	O
as	O
well	O
as	O
GTT	O
(	O
BERT	B-MethodName
)	O
,	O
reducing	O
the	O
F1	B-MetricName
score	I-MetricName
across	O
the	O
remaining	O
slots	O
.	O
From	O
the	O
error	O
count	O
results	O
in	O
Figure	O
4	O
,	O
we	O
see	O
that	O
GTT	O
makes	O
fewer	O
Missing	O
Template	O
errors	O
than	O
DyGIE++	O
on	O
the	O
MUC	O
-	O
4	O
dataset	O
(	O
86	O
vs.	O
97	O
)	O
.	O
However	O
,	O
there	O
is	O
no	O
significant	O
difference	O
in	O
the	O
number	O
of	O
missing	O
templates	O
between	O
the	O
two	O
models	O
on	O
the	O
ProMED	O
and	O
SciREX	B-DatasetName
datasets	O
.	O
This	O
could	O
be	O
because	O
DyGIE++	O
is	O
prone	O
to	O
overgeneration	O
-	O
there	O
are	O
significantly	O
more	O
Spurious	O
Role	O
Filler	O
and	O
Spurious	O
Template	O
errors	O
as	O
compared	O
to	O
the	O
results	O
of	O
GTT	O
.	O
Since	O
we	O
use	O
heuristics	O
that	O
create	O
templates	O
based	O
on	O
the	O
extracted	O
role	O
fillers	O
,	O
this	O
increases	O
the	O
probability	O
that	O
there	O
was	O
a	O
possible	O
match	O
to	O
a	O
gold	O
template	O
,	O
reducing	O
the	O
number	O
of	O
Missing	O
Template	O
Errors	O
.	O
We	O
can	O
also	O
conclude	O
that	O
DyGIE++	O
is	O
worse	O
at	O
coreference	B-TaskName
resolution	I-TaskName
when	O
compared	O
to	O
GTT	O
as	O
DyGIE++	O
makes	O
more	O
Duplicate	O
Role	O
Filler	O
errors	O
across	O
all	O
datasets	O
.	O
Overall	O
,	O
we	O
find	O
that	O
the	O
major	O
source	O
of	O
error	O
for	O
both	O
GTT	O
and	O
DyGIE++	O
across	O
all	O
the	O
datasets	O
is	O
missing	O
recall	O
in	O
the	O
form	O
of	O
Missing	O
Role	O
Filler	O
and	O
Missing	O
Template	O
errors	O
.	O

Table	O
3	O
presents	O
the	O
precision	O
,	O
recall	O
,	O
and	O
F1	B-MetricName
performance	O
on	O
the	O
MUC	O
-	O
4	O
dataset	O
for	O
early	O
models	O
from	O
1992	O
alongside	O
those	O
of	O
the	O
more	O
recent	O
DyGIE++	O
and	O
GTT	O
models	O
.	O
We	O
summarize	O
key	O
findings	O
below	O
.	O
The	O
best	O
of	O
the	O
early	O
models	O
(	O
GE	O
NLToolset	O
)	O
performs	O
better	O
than	O
either	O
of	O
the	O
modern	O
models	O
.	O
It	O
does	O
so	O
by	O
doing	O
a	O
better	O
job	O
balancing	O
precision	O
and	O
recall	O
,	O
whereas	O
GTT	O
and	O
DyGIE++	O
exhibit	O
much	O
higher	O
precision	O
and	O
much	O
lower	O
recall	O
.	O
Table	O
4	O
:	O
Span	O
Errors	O
in	O
early	O
models	O
.	O
The	O
differences	O
between	O
the	O
predicted	O
mention	O
and	O
its	O
best	O
gold	O
mention	O
match	O
according	O
to	O
our	O
analysis	O
tool	O
are	O
in	O
bold	O
.	O
The	O
early	O
models	O
have	O
more	O
span	O
errors	O
than	O
the	O
modern	O
DyGIE++	O
and	O
GTT	O
models	O
.	O
The	O
representative	O
kinds	O
of	O
span	O
errors	O
from	O
the	O
1992	O
model	O
outputs	O
are	O
shown	O
in	O
Table	O
4	O
.	O
One	O
interesting	O
difference	O
between	O
the	O
span	O
errors	O
in	O
the	O
early	O
models	O
and	O
the	O
modern	O
models	O
is	O
that	O
the	O
predicted	O
mentions	O
include	O
longer	O
spans	O
with	O
more	O
information	O
than	O
is	O
indicated	O
in	O
the	O
best	O
gold	O
mention	O
match	O
.	O
Some	O
could	O
be	O
due	O
to	O
errors	O
in	O
dataset	O
annotation	O
;	O
for	O
example	O
,	O
maoist	O
shining	O
path	O
group	O
versus	O
shining	O
path	O
but	O
a	O
significant	O
number	O
of	O
the	O
span	O
errors	O
occur	O
as	O
the	O
early	O
models	O
seem	O
to	O
extract	O
the	O
entire	O
sentence	O
or	O
clause	O
which	O
contains	O
the	O
desired	O
role	O
filler	O
mention	O
.	O
The	O
modern	O
models	O
tend	O
to	O
leave	O
off	O
parts	O
of	O
the	O
desired	O
spans	O
,	O
and	O
if	O
they	O
do	O
predict	O
larger	O
spans	O
than	O
required	O
,	O
are	O
only	O
off	O
by	O
a	O
few	O
words	O
.	O
The	O
early	O
models	O
have	O
fewer	O
Missing	O
Template	O
and	O
Missing	O
Role	O
Filler	O
errors	O
as	O
compared	O
to	O
the	O
modern	O
models	O
.	O
However	O
,	O
the	O
former	O
also	O
have	O
more	O
Spurious	O
Template	O
and	O
Spurious	O
Role	O
Filler	O
errors	O
than	O
the	O
latter	O
,	O
indicating	O
these	O
models	O
mitigate	O
the	O
issue	O
of	O
Missing	O
Templates	O
through	O
over	O
-	O
generation	O
.	O
The	O
early	O
models	O
have	O
fewer	O
Incorrect	O
Role	O
errors	O
as	O
compared	O
to	O
modern	O
models	O
.	O
However	O
,	O
since	O
all	O
the	O
models	O
make	O
relatively	O
few	O
such	O
errors	O
,	O
it	O
suggests	O
that	O
role	O
classification	O
for	O
predicted	O
mentions	O
is	O
not	O
a	O
major	O
problem	O
for	O
modern	O
models	O
.	O
The	O
main	O
source	O
of	O
error	O
for	O
both	O
early	O
and	O
modern	O
models	O
is	O
missing	O
recall	O
due	O
to	O
missing	O
templates	O
and	O
missing	O
role	O
fillers	O
.	O
This	O
strongly	O
suggests	O
future	O
systems	O
can	O
maximize	O
their	O
performance	O
by	O
being	O
less	O
conservative	O
in	O
role	O
filler	O
detection	O
and	O
focusing	O
on	O
improvement	O
of	O
the	O
recall	O
,	O
even	O
at	O
the	O
expense	O
of	O
potentially	O
decreasing	O
some	O
precision	O
.	O

This	O
work	O
explores	O
subtypes	O
of	O
Spurious	O
Role	O
Filler	O
errors	O
extensively	O
,	O
however	O
,	O
we	O
would	O
like	O
to	O
further	O
analyze	O
Missing	O
Role	O
Filler	O
and	O
templatelevel	O
errors	O
for	O
more	O
fine	O
-	O
grained	O
error	O
subtypes	O
and	O
the	O
linguistic	O
reasons	O
behind	O
why	O
they	O
occur	O
.	O
Due	O
to	O
the	O
pairwise	O
comparisons	O
between	O
all	O
predicted	O
and	O
gold	O
mentions	O
in	O
a	O
role	O
for	O
all	O
pairs	O
of	O
predicted	O
and	O
gold	O
templates	O
in	O
an	O
example	O
,	O
the	O
error	O
analysis	O
tool	O
is	O
slow	O
when	O
the	O
number	O
of	O
both	O
the	O
predicted	O
and	O
gold	O
templates	O
as	O
well	O
as	O
the	O
number	O
of	O
role	O
fillers	O
in	O
the	O
templates	O
is	O
high	O
.	O
Thus	O
,	O
we	O
would	O
also	O
like	O
to	O
improve	O
the	O
time	O
complexity	O
of	O
our	O
template	O
(	O
and	O
mention	O
)	O
matching	O
algorithms	O
using	O
an	O
approach	O
like	O
bipartite	O
matching	O
(	O
Yang	O
et	O
al	O
,	O
2021	O
)	O
.	O
Currently	O
,	O
the	O
error	O
analysis	O
tool	O
reports	O
exact	B-MetricName
match	I-MetricName
precision	O
/	O
recall	O
/	O
F1	B-MetricName
which	O
is	O
more	O
suitable	O
for	O
string	O
-	O
fill	O
roles	O
.	O
We	O
would	O
like	O
to	O
extend	O
the	O
tool	O
to	O
further	O
analyze	O
set	O
-	O
fill	O
roles	O
by	O
implementing	O
metrics	O
such	O
as	O
false	O
-	O
positive	O
rate	O
.	O
We	O
used	O
a	O
limited	O
number	O
of	O
models	O
in	O
this	O
paper	O
as	O
we	O
aimed	O
to	O
develop	O
and	O
test	O
the	O
usability	O
of	O
our	O
error	O
analysis	O
tool	O
.	O
In	O
the	O
future	O
,	O
however	O
,	O
we	O
would	O
like	O
to	O
test	O
our	O
tool	O
on	O
a	O
wider	O
range	O
of	O
models	O
,	O
in	O
addition	O
to	O
running	O
more	O
experiments	O
in	O
order	O
to	O
reach	O
more	O
generalizable	O
conclusions	O
about	O
the	O
behavior	O
of	O
IE	O
models	O
.	O

The	O
specific	O
list	O
of	O
transformations	O
applied	O
in	O
the	O
error	O
correction	O
process	O
.	O
(	O
1	O
)	O
Span	O
Error	B-MetricName
.	O
Each	O
singleton	O
Alter	O
Span	O
transformation	O
is	O
mapped	O
to	O
a	O
Span	O
Error	B-MetricName
.	O
A	O
Span	O
Error	B-MetricName
occurs	O
when	O
a	O
predicted	O
role	O
filler	O
becomes	O
an	O
exact	B-MetricName
match	I-MetricName
to	O
the	O
a	O
gold	O
role	O
filer	O
only	O
upon	O
span	O
alteration	O
.	O
(	O
2	O
)	O
Duplicate	O
Role	O
Filler	O
.	O
Each	O
singleton	O
Remove	O
Duplicate	O
Role	O
Filler	O
transformation	O
is	O
mapped	O
to	O
a	O
Duplicate	O
Role	O
Filler	O
error	O
.	O
A	O
Duplicate	O
Role	O
Filler	O
error	O
occurs	O
when	O
a	O
spurious	O
role	O
filler	O
is	O
coreferent	O
to	O
an	O
already	O
matched	O
role	O
filler	O
and	O
is	O
treated	O
as	O
a	O
separate	O
entity	O
.	O
This	O
happens	O
when	O
the	O
system	O
fails	O
at	O
coreference	B-TaskName
resolution	I-TaskName
.	O
(	O
3	O
)	O
Duplicate	O
Partially	O
Matched	O
Role	O
Filler	O
(	O
Spurious	O
)	O
.	O
Same	O
as	O
(	O
2	O
)	O
above	O
,	O
but	O
with	O
an	O
added	O
Alter	O
Span	O
transformation	O
applied	O
first	O
to	O
account	O
for	O
partial	O
matching	O
.	O
This	O
happens	O
when	O
the	O
system	O
fails	O
at	O
correct	O
span	O
extraction	O
and	O
coreference	B-TaskName
resolution	I-TaskName
.	O
(	O
4	O
)	O
Spurious	O
Role	O
Filler	O
.	O
Each	O
singleton	O
Remove	O
Spurious	O
Role	O
Filler	O
transformation	O
is	O
mapped	O
to	O
a	O
Spurious	O
Role	O
Filler	O
error	O
.	O
A	O
Spurious	O
Role	O
Filler	O
error	O
occurs	O
when	O
a	O
mention	O
is	O
extracted	O
from	O
the	O
text	O
with	O
no	O
connection	O
to	O
the	O
gold	O
templates	O
.	O
(	O
5	O
)	O
Missing	O
Role	O
Filler	O
.	O
Each	O
singleton	O
Introduce	O
Role	O
Filler	O
transformation	O
is	O
mapped	O
to	O
a	O
Missing	O
Role	O
Filler	O
error	O
.	O
A	O
Missing	O
Role	O
Filler	O
error	O
occurs	O
when	O
a	O
role	O
filler	O
is	O
present	O
in	O
the	O
gold	O
template	O
but	O
not	O
the	O
predicted	O
template	O
for	O
a	O
given	O
role	O
.	O
(	O
6	O
)	O
Incorrect	O
Role	O
.	O
Each	O
singleton	O
Alter	O
Role	O
transformation	O
is	O
mapped	O
to	O
an	O
Incorrect	O
Role	O
.	O
An	O
Incorrect	O
Role	O
occurs	O
when	O
a	O
spurious	O
role	O
filler	O
is	O
assigned	O
to	O
the	O
incorrect	O
role	O
within	O
the	O
same	O
template	O
,	O
i.e.	O
the	O
role	O
filler	O
would	O
have	O
been	O
correct	O
if	O
present	O
filled	O
in	O
another	O
slot	O
/	O
role	O
in	O
the	O
same	O
template	O
.	O
This	O
happens	O
when	O
the	O
system	O
fails	O
at	O
correct	O
role	O
assignment	O
.	O
(	O
7	O
)	O
Incorrect	O
Role	O
+	O
Partially	O
Matched	O
Filler	O
.	O
Same	O
as	O
(	O
4	O
)	O
above	O
,	O
but	O
with	O
an	O
added	O
Alter	O
Span	O
transformation	O
applied	O
first	O
to	O
account	O
for	O
partial	O
matching	O
.	O
This	O
happens	O
when	O
the	O
system	O
fails	O
at	O
correct	O
span	O
extraction	O
and	O
role	O
assignment	O
.	O
(	O
8	O
)	O
Wrong	O
Template	O
for	O
Role	O
Filler	O
.	O
Each	O
singleton	O
Remove	O
Cross	O
Template	O
Spurious	O
Role	O
Filler	O
transformation	O
is	O
mapped	O
to	O
a	O
Wrong	O
Template	O
for	O
Role	O
Filler	O
error	O
.	O
A	O
Wrong	O
Template	O
for	O
Role	O
Filler	O
occurs	O
when	O
a	O
spurious	O
role	O
filler	O
in	O
one	O
template	O
can	O
be	O
assigned	O
to	O
the	O
correct	O
role	O
in	O
another	O
template	O
,	O
i.e.	O
it	O
would	O
be	O
correct	O
if	O
it	O
had	O
been	O
placed	O
in	O
another	O
template	O
.	O
This	O
happens	O
when	O
the	O
system	O
fails	O
at	O
correct	O
event	O
assignment	O
.	O
(	O
9	O
)	O
Wrong	O
Template	O
for	O
Partially	O
Matched	O
Role	O
Filler	O
.	O
Same	O
as	O
(	O
6	O
)	O
above	O
,	O
but	O
with	O
an	O
added	O
Alter	O
Span	O
transformation	O
applied	O
first	O
to	O
account	O
for	O
partial	O
matching	O
.	O
This	O
happens	O
when	O
the	O
system	O
fails	O
at	O
correct	O
span	O
extraction	O
and	O
event	O
assignment	O
.	O
(	O
10	O
)	O
Wrong	O
Template	O
+	O
Wrong	O
Role	O
.	O
An	O
Alter	O
Role	O
and	O
Remove	O
Cross	O
Template	O
Spurious	O
Role	O
Filler	O
transformation	O
are	O
applied	O
to	O
the	O
same	O
predicted	O
role	O
filler	O
in	O
that	O
order	O
to	O
be	O
mapped	O
to	O
a	O
Wrong	O
Template	O
+	O
Wrong	O
Role	O
error	O
.	O
A	O
Wrong	O
Template	O
+	O
Wrong	O
Role	O
error	O
occurs	O
when	O
a	O
spurious	O
role	O
filler	O
can	O
be	O
assigned	O
to	O
another	O
role	O
in	O
another	O
template	O
.	O
This	O
happens	O
when	O
the	O
system	O
fails	O
at	O
correct	O
role	O
assignment	O
and	O
event	O
assignment	O
.	O
(	O
11	O
)	O
Wrong	O
Template	O
+	O
Wrong	O
Role	O
+	O
Partially	O
Matched	O
Filler	O
.	O
Same	O
as	O
(	O
8	O
)	O
above	O
,	O
but	O
with	O
an	O
added	O
Alter	O
Span	O
transformation	O
applied	O
first	O
to	O
account	O
for	O
partial	O
matching	O
.	O
This	O
happens	O
when	O
the	O
system	O
fails	O
at	O
correct	O
span	O
extraction	O
,	O
role	O
assignment	O
and	O
event	O
assignment	O
.	O
(	O

Quality	O
assurance	O
for	O
data	O
.	O
Nuanced	O
understanding	O
of	O
data	O
is	O
requisite	O
for	O
drawing	O
sound	O
scientific	O
conclusions	O
.	O
In	O
particular	O
,	O
without	O
evaluating	O
for	O
the	O
quality	O
and	O
accuracy	B-MetricName
of	O
data	O
used	O
to	O
test	O
models	O
,	O
it	O
is	O
impossible	O
to	O
be	O
certain	O
that	O
progress	O
is	O
being	O
made	O
and	O
that	O
successive	O
iterations	O
of	O
models	O
truly	O
make	O
progress	O
on	O
the	O
underlying	O
task	O
or	O
linguistic	O
phenomena	O
of	O
interest	O
.	O
Within	O
NLP	O
,	O
iconic	O
datasets	O
such	O
as	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
(	O
Marcus	O
et	O
al	O
,	O
1993	O
)	O
have	O
sustained	O
subareas	O
such	O
as	O
language	B-TaskName
modelling	I-TaskName
,	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
,	O
and	O
syntactic	O
parsing	O
for	O
years	O
due	O
to	O
the	O
painstaking	O
annotation	O
efforts	O
put	O
into	O
making	O
these	O
high	O
-	O
fidelity	O
resources	O
.	O
And	O
in	O
the	O
context	O
of	O
summarization	B-TaskName
,	O
initial	O
datasets	O
,	O
such	O
as	O
those	O
produced	O
during	O
the	O
Document	O
Understanding	O
Conference	O
(	O
DUC	O
)	O
and	O
Text	O
Analysis	O
Conference	O
(	O
TAC	O
)	O
evaluations	O
,	O
implemented	O
fine	O
-	O
grained	O
verification	O
of	O
data	O
quality	O
.	O
2	O
In	O
part	O
due	O
to	O
the	O
emergence	O
of	O
data	O
-	O
hungry	O
modelling	O
techniques	O
,	O
the	O
demands	O
for	O
larger	O
datasets	O
often	O
render	O
quality	O
assurance	O
procedures	O
of	O
this	O
standard	O
to	O
be	O
impractical	O
and	O
infeasible	O
.	O
Nonetheless	O
,	O
several	O
recent	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
datasets	O
(	O
Bowman	O
et	O
al	O
,	O
2015	O
;	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
;	O
Suhr	O
et	O
al	O
,	O
2017	O
)	O
institute	O
explicit	O
qualitycontrol	O
procedures	O
in	O
crowd	O
-	O
sourcing	O
dataset	O
construction	O
(	O
Zaidan	O
and	O
Callison	O
-	O
Burch	O
,	O
2011	O
;	O
Yan	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2014	O
;	O
Callison	O
-	O
Burch	O
et	O
al	O
,	O
2015	O
)	O
,	O
such	O
as	O
using	O
additional	O
annotators	O
to	O
validate	O
annotations	O
(	O
c.f	O
.	O
Geva	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
the	O
sibling	O
subfield	O
of	O
machine	B-TaskName
translation	I-TaskName
,	O
which	O
often	O
shares	O
similar	O
modelling	O
challenges	O
and	O
evaluation	O
regimes	O
as	O
summarization	B-TaskName
due	O
to	O
the	O
shared	O
nature	O
of	O
being	O
sequence	O
-	O
to	O
-	O
sequence	O
natural	O
language	O
generation	O
tasks	O
,	O
the	O
annual	O
WMT	O
conference	O
3	O
consistently	O
furnishes	O
high	O
quality	O
data	O
.	O
In	O
summary	O
,	O
ensuring	O
data	O
quality	O
is	O
both	O
crucial	O
and	O
challenging	O
.	O
And	O
in	O
comparison	O
with	O
other	O
subareas	O
of	O
NLP	O
,	O
we	O
argue	O
that	O
summarization	B-TaskName
has	O
lagged	O
behind	O
in	O
rigorously	O
ensuring	O
the	O
quality	O
of	O
widely	O
-	O
used	O
datasets	O
.	O
Relating	O
data	O
quality	O
and	O
model	O
quality	O
.	O
The	O
correctness	O
and	O
quality	O
of	O
data	O
inherently	O
bounds	O
what	O
can	O
be	O
learned	O
from	O
the	O
data	O
about	O
the	O
task	O
of	O
interest	O
.	O
From	O
an	O
information	O
-	O
theoretic	O
perspective	O
,	O
this	O
can	O
be	O
made	O
fully	O
formal	O
as	O
follows	O
:	O
For	O
fully	O
learning	O
-	O
based	O
methods	O
,	O
especially	O
those	O
with	O
weak	O
/	O
minimal	O
inductive	O
biases	O
such	O
as	O
neural	O
networks	O
,	O
I	O
(	O
S	O
;	O
A	O
)	O
is	O
approximately	O
zero	O
.	O
While	O
I	O
(	O
S	O
;	O
P	O
)	O
may	O
be	O
greater	O
than	O
zero	O
(	O
e.g.	O
language	B-TaskName
modelling	I-TaskName
pretraining	O
provides	O
statistical	O
information	O
that	O
may	O
facilitate	O
a	O
model	O
to	O
avoid	O
a	O
priori	O
unlikely	O
summaries	O
)	O
,	O
standard	O
pretraining	O
regimes	O
such	O
as	O
large	O
-	O
scale	O
language	B-TaskName
modelling	I-TaskName
over	O
generic	O
text	O
corpora	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Raffel	O
et	O
al	O
,	O
2019	O
)	O
are	O
likely	O
insufficient	O
to	O
meaningfully	O
learn	O
to	O
summarize	O
.	O
Under	O
these	O
assumptions	O
,	O
the	O
mutual	O
information	O
between	O
S	O
and	O
M	O
is	O
critically	O
upper	O
-	O
bounded	O
in	O
terms	O
of	O
I	O
(	O
S	O
;	O
T	O
)	O
.	O
We	O
hypothesize	O
that	O
the	O
quality	O
of	O
the	O
training	O
dataset	O
T	O
is	O
highly	O
correlated	O
with	O
its	O
mutual	O
information	O
with	O
respect	O
to	O
the	O
summarization	B-TaskName
task	O
S	O
,	O
I	O
(	O
S	O
;	O
T	O
)	O
.	O
One	O
size	O
does	O
not	O
fit	O
all	O
.	O
Spärck	O
Jones	O
(	O
1999	O
)	O
famously	O
argued	O
that	O
summarization	B-TaskName
systems	O
should	O
be	O
understood	O
conditional	O
on	O
the	O
context	O
in	O
which	O
they	O
will	O
be	O
used	O
.	O
In	O
recent	O
years	O
,	O
the	O
field	O
has	O
significantly	O
departed	O
from	O
this	O
perspective	O
and	O
primarily	O
studied	O
"	O
general	O
-	O
purpose	O
summarization	B-TaskName
"	O
(	O
Kryscinski	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
she	O
denounced	O
as	O
ignis	O
fatuus	O
.	O
With	O
our	O
work	O
,	O
we	O
adopt	O
the	O
perspective	O
that	O
for	O
all	O
datasets	O
it	O
is	O
strictly	O
preferable	O
to	O
have	O
all	O
properties	O
quantified	O
;	O
it	O
is	O
the	O
responsibility	O
of	O
practitioners	O
building	O
summarization	B-TaskName
systems	O
to	O
accurately	O
weight	O
different	O
metrics	O
based	O
on	O
their	O
ultimate	O
goals	O
and	O
use	O
cases	O
.	O
As	O
such	O
,	O
we	O
refrain	O
from	O
providing	O
prescriptive	O
domain	O
-	O
agnostic	O
or	O
context	O
-	O
agnostic	O
notions	O
of	O
summarization	B-TaskName
.	O

In	O
this	O
work	O
,	O
we	O
evaluate	O
the	O
quality	O
of	O
a	O
dataset	O
by	O
aggregating	O
scores	O
for	O
each	O
example	O
in	O
the	O
dataset	O
.	O
We	O
conjecture	O
that	O
for	O
many	O
NLP	O
tasks	O
,	O
estimating	O
the	O
quality	O
of	O
a	O
particular	O
data	O
example	O
is	O
of	O
similar	O
complexity	O
as	O
correctly	O
performing	O
the	O
task	O
on	O
the	O
example	O
.	O
5	O
Nevertheless	O
,	O
for	O
summarization	B-TaskName
,	O
our	O
insight	O
is	O
that	O
various	O
aspects	O
of	O
a	O
summarization	B-TaskName
example	O
(	O
a	O
document	O
-	O
summary	O
pair	O
)	O
can	O
be	O
reliably	O
estimated	O
by	O
re	O
-	O
purposing	O
existing	O
NLP	O
methods	O
.	O
We	O
are	O
guided	O
by	O
pioneering	O
work	O
(	O
Luhn	O
,	O
1958	O
;	O
Edmundson	O
,	O
1969	O
;	O
Mani	O
,	O
1999	O
)	O
that	O
defined	O
core	O
properties	O
of	O
summarization	B-TaskName
systems	O
and	O
influential	O
sub	O
-	O
sequent	O
work	O
(	O
Radev	O
et	O
al	O
,	O
2002	O
;	O
Nenkova	O
,	O
2006	O
;	O
Nenkova	O
and	O
McKeown	O
,	O
2012	O
;	O
Peyrard	O
,	O
2019a	O
)	O
that	O
refined	O
and	O
extended	O
these	O
properties	O
.	O
From	O
this	O
literature	O
,	O
we	O
specifically	O
study	O
compression	O
,	O
topic	O
similarity	O
,	O
abstractivity	O
,	O
redundancy	O
,	O
and	O
semantic	O
coherence	O
as	O
these	O
properties	O
are	O
of	O
recurring	O
and	O
sustained	O
interest	O
.	O
6	O
For	O
each	O
abstract	O
property	O
,	O
numerous	O
concrete	O
methods	O
can	O
be	O
proposed	O
to	O
quantify	O
it	O
.	O
In	O
Appendix	O
A	O
,	O
we	O
describe	O
alternatives	O
we	O
considered	O
and	O
detail	O
how	O
we	O
decided	O
which	O
methods	O
performed	O
best	O
.	O
We	O
restrict	O
discussion	O
to	O
the	O
bestperforming	O
approaches	O
in	O
the	O
main	O
paper	O
.	O
Notation	O
.	O
Our	O
metrics	O
will	O
assume	O
indexed	O
sets	O
D	O
,	O
S	O
such	O
that	O
summary	O
S	O
i	O
S	O
summarizes	O
document	O
D	O
i	O
D.	O
The	O
length	O
in	O
words	O
of	O
a	O
sequence	O
s	O
is	O
|	O
s	O
|	O
and	O
the	O
length	O
in	O
sentences	O
is	O
s	O
.	O
Each	O
metric	O
assigns	O
a	O
value	O
x	O
[	O
0	B-DatasetName
,	O
1	O
]	O
to	O
every	O
(	O
D	O
i	O
,	O
S	O
i	O
)	O
where	O
1	O
is	O
the	O
maximal	O
score	O
and	O
example	O
-	O
level	O
scores	O
are	O
averaged	O
to	O
yield	O
a	O
dataset	O
-	O
level	O
score	O
.	O
Compression	O
.	O
We	O
quantify	O
compression	O
at	O
the	O
word	O
(	O
w	O
)	O
and	O
sentence	O
(	O
s	O
)	O
levels	O
:	O
CMP	O
w	O
(	O
D	O
i	O
,	O
S	O
i	O
)	O
=	O
1	O
−	O
|	O
S	O
i	O
|	O
|	O
D	O
i	O
|	O
(	O
1	O
)	O
CMP	O
s	O
(	O
D	O
i	O
,	O
S	O
i	O
)	O
=	O
1	O
−	O
S	O
i	O
D	O
i	O
(	O
2	O
)	O
Topic	O
Similarity	O
.	O
We	O
learn	O
a	O
topic	O
model	O
M	O
on	O
training	O
corpus	O
T	O
with	O
k	O
topics	O
using	O
LDA	B-MethodName
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
and	O
quantify	O
topic	O
similarity	O
by	O
comparing	O
the	O
inferred	O
topic	O
distributions	O
θ	B-HyperparameterName
D	O
i	O
|	O
M	O
,	O
θ	B-HyperparameterName
S	O
i	O
|	O
M	O
for	O
a	O
given	O
summary	O
and	O
document	O
:	O
TS	B-MethodName
(	O
D	O
i	O
,	O
S	O
i	O
)	O
=	O
1	O
−	O
JS	O
(	O
θ	B-HyperparameterName
D	O
i	O
|	O
M	O
,	O
θ	B-HyperparameterName
S	O
i	O
|	O
M	O
)	O
(	O
3	O
)	O
where	O
JS	O
is	O
the	O
Jensen	O
-	O
Shannon	O
distance	O
.	O
We	O
set	O
k	B-HyperparameterName
=	I-HyperparameterName
20	O
and	O
T	O
=	O
D.	O
Abstractivity	O
.	O
Grusky	O
et	O
al	O
(	O
2018	O
)	O
introduced	O
fragments	O
F	O
(	O
D	O
i	O
,	O
S	O
i	O
)	O
,	O
which	O
are	O
greedily	O
-	O
matched	O
spans	O
shared	O
between	O
D	O
i	O
and	O
S	O
i	O
.	O
We	O
quantify	O
abstractivity	O
as	O
a	O
normalized	O
function	O
of	O
the	O
aggregate	O
fragment	O
length	O
;	O
our	O
definition	O
generalizes	O
the	O
definition	O
of	O
Grusky	O
et	O
al	O
(	O
2018	O
)	O
.	O
We	O
set	O
p	O
=	O
1	O
.	O
ABS	O
p	O
(	O
D	O
i	O
,	O
S	O
i	O
)	O
=	O
1	O
−	O
f	O
F	O
(	O
D	O
i	O
,	O
S	O
i	O
)	O
|	O
f	O
|	O
p	O
|	O
S	O
i	O
|	O
p	O
(	O
4	O
)	O
Redundancy	O
.	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
implicitly	O
penalizes	O
redundancy	O
but	O
underestimates	O
its	O
detrimental	O
impacts	O
(	O
Chaganty	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
we	O
find	O
that	O
ROUGE	O
is	O
effective	O
for	O
identifying	O
redundancy	O
given	O
the	O
definitional	O
focus	O
on	O
overlapping	O
spans	O
.	O
We	O
quantify	O
redundancy	O
as	O
the	O
average	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
6	O
Different	O
names	O
and	O
interpretations	O
have	O
been	O
given	O
for	O
these	O
properties	O
in	O
the	O
literature	O
.	O
We	O
revisit	O
this	O
in	O
Appendix	O
A	O
in	O
discussing	O
alternate	O
metrics	O
.	O
F	O
-	O
score	O
for	O
all	O
pairs	O
of	O
distinct	O
sentences	O
in	O
the	O
summary	O
.	O
RED	B-DatasetName
(	O
S	O
i	O
)	O
=	O
mean	O
(	O
x	O
,	O
y	O
)	O
S	O
i	O
×S	O
i	O
,	O
x	O
=	O
y	O
ROUGE	O
(	O
x	O
,	O
y	O
)	O
(	O
5	O
)	O
Semantic	O
Coherence	O
.	O
We	O
evaluate	O
the	O
semantic	O
coherence	O
of	O
multi	O
-	O
sentence	O
summaries	O
by	O
predicting	O
the	O
probability	O
of	O
each	O
successive	O
sentence	O
conditioned	O
on	O
the	O
previous	O
one	O
using	O
a	O
powerful	O
language	O
model	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
pretrained	O
with	O
precisely	O
this	O
objective	O
.	O
SC	O
(	O
S	O
i	O
)	O
=	O
|	O
|	O
S	O
|	O
|	O
j=2	O
1	O
BERT	B-MethodName
(	O
S	O
j	O
i	O
|	O
S	O
j−1	O
i	O
)	O
|	O
|	O
S	O
i	O
|	O
|	O
−	O
1	O
(	O
6	O
)	O
4	O
Data	O
We	O
study	O
the	O
following	O
10	O
summarization	B-TaskName
datasets	O
that	O
have	O
been	O
frequently	O
used	O
in	O
recent	O
years	O
.	O
7	O
Table	O
1	O
contains	O
standard	O
dataset	O
statistics	O
in	O
the	O
upper	O
half	O
and	O
our	O
aspect	O
-	O
level	O
scores	O
in	O
the	O
lower	O
half	O
;	O
datasets	O
are	O
grouped	O
by	O
domain	O
.	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
(	O
Hermann	O
et	O
al	O
,	O
2015	O
;	O
Nallapati	O
et	O
al	O
,	O
2016	O
)	O
is	O
a	O
dataset	O
composed	O
of	O
CNN	O
and	O
Daily	O
Mail	O
news	O
articles	O
with	O
summaries	O
that	O
are	O
a	O
concatenated	O
list	O
of	O
highlight	O
bullet	O
points	O
.	O
NYT	O
(	O
Sandhaus	O
,	O
2008	O
)	O
is	O
a	O
dataset	O
of	O
curated	O
New	O
York	O
Times	O
articles	O
paired	O
with	O
abstracts	O
written	O
by	O
library	O
scientists	O
.	O
NWS	O
(	O
Grusky	O
et	O
al	O
,	O
2018	O
)	O
is	O
the	O
Newsroom	O
dataset	O
of	O
news	O
articles	O
drawn	O
from	O
38	O
top	O
English	O
publishers	O
paired	O
with	O
multi	O
-	O
sentence	O
summaries	O
written	O
by	O
the	O
original	O
authors	O
and	O
editors	O
.	O
GW	O
(	O
Graff	O
and	O
Cieri	O
,	O
2003	O
)	O
is	O
the	O
Gigaword	O
headline	O
generation	O
dataset	O
that	O
some	O
refer	O
to	O
as	O
a	O
summarization	B-TaskName
dataset	O
(	O
Rush	O
et	O
al	O
,	O
2015	O
;	O
Chopra	O
et	O
al	O
,	O
2016	O
)	O
.	O
Examples	O
in	O
the	O
dataset	O
are	O
drawn	O
from	O
seven	O
news	O
sources	O
and	O
are	O
the	O
article	O
prefix	O
paired	O
with	O
its	O
headline	O
.	O
XSum	B-DatasetName
(	O
Narayan	O
et	O
al	O
,	O
2018	O
)	O
is	O
an	O
extreme	B-TaskName
summarization	I-TaskName
dataset	O
where	O
BBC	O
articles	O
are	O
paired	O
with	O
single	O
-	O
sentence	O
summaries	O
written	O
generally	O
by	O
the	O
author	O
of	O
the	O
article	O
that	O
tries	O
to	O
motivate	O
the	O
BBC	O
audience	O
to	O
read	O
the	O
article	O
by	O
answering	O
"	O
What	O
is	O
the	O
article	O
about	O
?	O
"	O
.	O
PeerRead	B-DatasetName
(	O
Kang	O
et	O
al	O
,	O
2018	O
)	O
is	O
a	O
dataset	O
of	O
paper	O
drafts	O
from	O
top	O
-	O
tier	O
computer	O
science	O
venues	O
as	O
well	O
as	O
arXiv	B-DatasetName
.	O
8	O
Consistent	O
with	O
its	O
use	O
in	O
the	O
summarization	B-TaskName
community	O
,	O
we	O
consider	O
the	O
full	O
introduction	O
to	O
be	O
the	O
source	O
document	O
and	O
the	O
ab	O
-	O
stract	O
to	O
be	O
the	O
target	O
summary	O
.	O
PubMed	O
(	O
Cohan	O
et	O
al	O
,	O
2018	O
)	O
is	O
a	O
dataset	O
of	O
papers	O
drawn	O
from	O
the	O
biomedical	O
and	O
life	O
sciences	O
.	O
Unlike	O
PeerRead	B-DatasetName
,	O
the	O
full	O
paper	O
is	O
taken	O
as	O
the	O
document	O
but	O
the	O
summary	O
is	O
still	O
specified	O
as	O
the	O
abstract	O
.	O
TL	O
;	O
DR	O
(	O
Völske	O
et	O
al	O
,	O
2017	O
)	O
is	O
a	O
dataset	O
of	O
userwritten	O
articles	O
from	O
the	O
social	O
media	O
platform	O
Reddit	B-DatasetName
along	O
with	O
the	O
author	O
-	O
provided	O
courtesy	O
summaries	O
that	O
tend	O
to	O
be	O
multi	O
-	O
sentence	O
.	O
Völske	O
et	O
al	O
(	O
2017	O
)	O
applied	O
a	O
series	O
of	O
preprocessing	O
procedures	O
to	O
filter	O
out	O
bot	O
-	O
generated	O
content	O
.	O
AMI	O
(	O
Carletta	O
et	O
al	O
,	O
2005	O
)	O
is	O
a	O
dataset	O
of	O
transcribed	O
meetings	O
,	O
some	O
which	O
are	O
naturally	O
occurring	O
and	O
the	O
rest	O
of	O
which	O
are	O
elicited	O
,	O
with	O
handannotated	O
summaries	O
.	O
The	O
transcription	O
process	O
has	O
multiple	O
steps	O
that	O
are	O
described	O
extensively	O
by	O
Carletta	O
et	O
al	O
(	O
2005	O
)	O
.	O
Various	O
additional	O
data	O
provided	O
within	O
the	O
AMI	O
dataset	O
is	O
neglected	O
in	O
this	O
work	O
.	O
MovieScript	O
(	O
Gorinski	O
and	O
Lapata	O
,	O
2015	O
)	O
is	O
a	O
dataset	O
of	O
movie	O
scripts	O
drawn	O
from	O
the	O
Script	O
-	O
Base	O
corpus	O
that	O
are	O
aligned	O
with	O
user	O
-	O
written	O
summaries	O
sourced	O
either	O
from	O
Wikipedia	O
or	O
IMDB	B-DatasetName
.	O
Various	O
additional	O
data	O
provided	O
within	O
the	O
Movi	B-DatasetName
-	O
eScript	O
dataset	O
is	O
neglected	O
in	O
this	O
work	O
.	O

In	O
the	O
main	O
paper	O
,	O
we	O
compute	O
topic	O
similarity	O
using	O
the	O
Jensen	O
-	O
Shannon	O
distance	O
.	O
We	O
initially	O
considered	O
the	O
Kullback	O
-	O
Leibler	O
(	O
KL	O
)	O
divergence	O
.	O
While	O
the	O
JS	O
distance	O
and/or	O
divergence	O
has	O
been	O
more	O
frequently	O
used	O
in	O
the	O
context	O
of	O
similarity	O
in	O
topic	O
modelling	O
,	O
the	O
KL	O
divergence	O
is	O
also	O
frequently	O
considered	O
.	O
Intuitively	O
and	O
under	O
some	O
interpretations	O
,	O
the	O
asymmetry	O
of	O
the	O
KL	O
divergence	O
may	O
be	O
desirable	O
as	O
the	O
extent	O
to	O
which	O
a	O
summary	O
is	O
topically	O
similar	O
to	O
a	O
document	O
may	O
not	O
be	O
the	O
same	O
as	O
the	O
extent	O
to	O
which	O
a	O
document	O
is	O
topically	O
similar	O
to	O
a	O
summary	O
.	O
In	O
spite	O
of	O
this	O
,	O
in	O
viewing	O
the	O
results	O
using	O
KL	O
,	O
we	O
found	O
that	O
the	O
measure	O
lacked	O
discriminative	O
power	O
in	O
disambiguating	O
examples	O
we	O
believed	O
were	O
more	O
topically	O
similar	O
than	O
others	O
.	O
We	O
qualitatively	O
found	O
the	O
judgments	O
via	O
the	O
JS	O
distance	O
to	O
be	O
accurate	O
.	O
That	O
said	O
,	O
the	O
judgments	O
between	O
the	O
measures	O
tended	O
to	O
be	O
highly	O
correlated	O
as	O
the	O
Spearman	O
rank	O
correlation	O
coefficient	O
was	O
ρ	O
≥	O
0.7	O
for	O
all	O
topic	O
modelling	O
settings	O
and	O
in	O
most	O
cases	O
exceeded	O
0.8	O
.	O
We	O
also	O
considered	O
a	O
topic	O
model	O
learned	O
using	O
both	O
the	O
documents	O
and	O
summaries	O
D	O
∪	O
S	O
and	O
just	O
the	O
documents	O
D.	O
Both	O
are	O
natural	O
choices	O
,	O
with	O
using	O
the	O
documents	O
being	O
more	O
general	O
in	O
some	O
sense	O
as	O
the	O
topic	O
similarity	O
of	O
a	O
summary	O
should	O
be	O
able	O
to	O
be	O
assigned	O
without	O
requiring	O
the	O
summary	O
collection	O
.	O
We	O
further	O
considered	O
several	O
choices	O
for	O
the	O
number	O
of	O
topics	O
as	O
well	O
.	O
In	O
Table	O
5	O
,	O
we	O
report	O
the	O
full	O
results	O
for	O
all	O
pairs	O
of	O
(	O
training	O
corpus	O
T	O
,	O
#	O
topics	O
k	O
)	O
for	O
all	O
(	O
T	O
,	O
k	O
)	O
{	O
D	O
∪	O
S	O
,	O
D	O
}	O
×	O
{	O
10	O
,	O
20	O
,	O
50	O
,	O
100	O
}	O
.	O
In	O
all	O
cases	O
,	O
the	O
number	O
of	O
training	O
examples	O
is	O
truncated	O
to	O
20000	O
(	O
hence	O
10000	O
summaries	O
and	O
10000	O
documents	O
when	O
using	O
the	O
training	O
corpus	O
of	O
D	O
∪	O
S	O
)	O
.	O
We	O
fix	O
the	O
number	O
of	O
training	O
documents	O
across	O
datasets	O
to	O
attempt	O
to	O
control	O
for	O
the	O
confound	O
of	O
larger	O
datasets	O
inducing	O
higher	O
quality	O
topic	B-TaskName
models	I-TaskName
.	O
We	O
did	O
not	O
observe	O
significant	O
changes	O
in	O
the	O
result	O
by	O
relaxing	O
this	O
(	O
i.e.	O
using	O
the	O
full	O
datasets	O
instead	O
of	O
just	O
20000	O
examples	O
)	O
.	O
We	O
find	O
that	O
there	O
is	O
significant	O
variation	O
in	O
crossdataset	O
rankings	O
with	O
respect	O
to	O
these	O
two	O
parameters	O
.	O
We	O
chose	O
to	O
report	O
the	O
results	O
corresponding	O
to	O
k	B-HyperparameterName
=	I-HyperparameterName
20	O
,	O
T	O
=	O
D.	O
We	O
chose	O
the	O
value	O
for	O
k	O
based	O
on	O
qualitative	O
judgments	O
about	O
topic	O
quality	O
for	O
CNN	B-DatasetName
-	I-DatasetName
DM	I-DatasetName
,	O
PeerRead	B-DatasetName
,	O
and	O
AMI	O
,	O
as	O
we	O
considered	O
these	O
to	O
be	O
a	O
diverse	O
subset	O
of	O
all	O
10	O
datasets	O
.	O
The	O
topics	O
we	O
observed	O
were	O
highly	O
disjoint	O
and	O
reasonably	O
aligned	O
with	O
our	O
intuitions	O
about	O
what	O
sensible	O
topics	O
should	O
be	O
.	O
We	O
chose	O
the	O
value	O
for	O
T	O
based	O
on	O
the	O
generality	O
referenced	O
previously	O
.	O
While	O
the	O
results	O
are	O
substantially	O
different	O
for	O
D	O
versus	O
D	O
∪	O
S	O
,	O
we	O
did	O
not	O
find	O
any	O
consistent	O
and	O
interpretable	O
discriminative	O
properties	O
between	O
the	O
two	O
.	O

In	O
the	O
main	O
paper	O
,	O
we	O
compute	O
redundancy	O
scores	O
for	O
each	O
distinct	O
sentence	O
pair	O
using	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
Fmeasure	O
and	O
then	O
average	O
these	O
individual	O
values	O
to	O
get	O
a	O
score	O
for	O
the	O
entire	O
summary	O
.	O
Alternatively	O
,	O
we	O
considered	O
other	O
ROUGE	O
scores	O
(	O
specifically	O
ROUGE	O
-	O
1	O
and	O
ROUGE	O
-	O
2	O
)	O
as	O
well	O
as	O
max	B-MethodName
pooling	I-MethodName
the	O
sentence	O
pair	O
scores	O
.	O
We	O
report	O
these	O
results	O
below	O
in	O
Table	O
7	O
.	O
We	O
do	O
not	O
observe	O
significant	O
changes	O
with	O
the	O
specific	O
ROUGE	O
metric	O
considered	O
(	O
i.e.	O
a	O
Spearman	O
ρ	O
of	O
1.0	O
which	O
indicates	O
a	O
perfect	O
correlation	O
in	O
the	O
case	O
of	O
max	B-MethodName
pooling	I-MethodName
across	O
the	O
ROUGE	O
variants	O
)	O
.	O
We	O
do	O
see	O
substantial	O
differences	O
between	O
averaging	O
and	O
max	B-MethodName
pooling	I-MethodName
;	O
we	O
find	O
that	O
max	B-MethodName
pooling	I-MethodName
turns	O
out	O
to	O
precisely	O
correlate	O
(	O
ρ	O
=	O
1.0	O
)	O
with	O
the	O
average	O
summary	O
length	O
.	O
This	O
is	O
somewhat	O
expected	O
,	O
given	O
that	O
the	O
max	O
-	O
pooled	O
redundancy	O
estimates	O
does	O
n't	O
inherently	O
control	O
for	O
summary	O
length	O
.	O
We	O
therefore	O
chose	O
to	O
report	O
redundancy	O
scores	O
using	O
averaging	O
as	O
we	O
also	O
qualitatively	O
found	O
them	O
to	O
be	O
more	O
useful	O
and	O
characteristic	O
,	O
especially	O
for	O
datasets	O
such	O
as	O
AMI	O
and	O
the	O
Scientific	O
datasets	O
as	O
max	B-MethodName
pooling	I-MethodName
was	O
overly	O
aggressive	O
.	O
While	O
the	O
nuances	O
of	O
the	O
specific	O
ROUGE	O
variant	O
did	O
not	O
significantly	O
impact	O
trends	O
in	O
redundancy	O
scores	O
,	O
we	O
chose	O
to	O
report	O
the	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
scores	O
in	O
the	O
main	O
paper	O
as	O
we	O
(	O
highly	O
subjectively	O
)	O
found	O
the	O
values	O
to	O
be	O
most	O
interpretable	O
/	O
consistent	O
with	O
values	O
we	O
would	O
have	O
assigned	O
.	O

We	O
lowercase	O
all	O
terms	O
,	O
remove	O
stopwords	O
using	O
the	O
list	O
specified	O
in	O
NLTK	O
(	O
Loper	O
and	O
Bird	O
,	O
2002	O
)	O
,	O
and	O
lemmatize	O
using	O
SpaCy	O
(	O
Honnibal	O
and	O
Montani	O
,	O
2017	O
)	O
.	O
We	O
only	O
retain	O
words	O
tagged	O
with	O
a	O
POS	O
category	O
in	O
{	O
NOUN	O
,	O
ADJ	O
,	O
VERB	O
,	O
ADV	O
}	O
by	O
the	O
SpaCy	O
POS	O
tagger	O
.	O
We	O
use	O
LDA	B-MethodName
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
to	O
learn	O
all	O
topic	B-TaskName
models	I-TaskName
and	O
rely	O
on	O
the	O
implementation	O
in	O
Gensim	O
(	O
Řehůřek	O
and	O
Sojka	O
,	O
2010	O
)	O
based	O
on	O
specification	O
of	O
Hoffman	O
et	O
al	O
(	O
2010	O
)	O
.	O
All	O
hyperparameters	O
are	O
set	O
as	O
default	O
and	O
we	O
discussed	O
the	O
number	O
of	O
topics	O
k	O
and	O
training	O
corpus	O
T	O
in	O
A.2	O
with	O
the	O
results	O
in	O
the	O
main	O
paper	O
using	O
k	B-HyperparameterName
=	I-HyperparameterName
20	O
and	O
T	O
=	O
D	O
where	O
T	O
is	O
truncated	O
to	O
be	O
at	O
most	O
20000	O
documents	O
.	O
We	O
compute	O
the	O
Jensen	O
-	O
Shannon	O
distance	O
using	O
SciPy	O
(	O
Virtanen	O
et	O
al	O
,	O
2020	O
)	O
.	O

We	O
make	O
use	O
of	O
the	O
native	O
Python	O
reimplementation	O
of	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
,	O
easy	O
-	O
rouge	O
.	O
19	O
All	O
scores	O
reported	O
in	O
the	O
main	O
paper	O
use	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
and	O
use	O
the	O
computed	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
score	O
.	O

All	O
metrics	O
reported	O
in	O
the	O
main	O
paper	O
can	O
be	O
computed	O
over	O
all	O
datasets	O
in	O
less	O
than	O
10	O
ten	O
hours	O
on	O
a	O
single	O
CPU	O
.	O
The	O
only	O
model	O
with	O
a	O
nontrivial	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
used	O
in	O
this	O
work	O
is	O
the	O
bert	O
-	O
base	O
-	O
uncased	O
models	O
we	O
use	O
in	O
measuring	O
semantic	O
coherence	O
.	O
We	O
refer	O
readers	O
to	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
for	O
more	O
details	O
and	O
to	O
the	O
HuggingFace	O
implementation	O
we	O
reference	O
previously	O
.	O

We	O
use	O
the	O
SimulEval	O
toolkit	O
.	O
The	O
toolkit	O
provides	O
a	O
simple	O
interface	O
for	O
evaluation	O
of	O
simultaneous	O
(	O
speech	O
)	O
translation	O
.	O
It	O
reports	O
the	O
quality	O
metric	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
;	O
Post	O
,	O
2018	O
)	O
and	O
latency	O
metrics	O
Average	O
Proportion	O
(	O
AP	B-DatasetName
,	O
Cho	O
and	O
Esipova	O
2016	O
)	O
,	O
Average	O
Lagging	O
(	O
AL	O
,	O
Ma	O
et	O
al	O
2019	O
)	O
,	O
and	O
Differentiable	O
Average	O
Lagging	O
(	O
DAL	O
,	O
Cherry	O
and	O
Foster	O
2019	O
)	O
modified	O
for	O
speech	O
source	O
.	O
Specifically	O
,	O
we	O
implement	O
an	O
Agent	B-DatasetName
class	O
.	O
We	O
have	O
to	O
implement	O
two	O
important	O
functions	O
:	O
policy	O
(	O
state	O
)	O
and	O
predict	O
(	O
state	O
)	O
,	O
where	O
state	O
is	O
the	O
state	O
of	O
the	O
agent	B-DatasetName
(	O
e.g.	O
,	O
read	O
processed	O
input	O
,	O
emitted	O
tokens	O
,	O
...	O
)	O
.	O
The	O
policy	O
function	O
returns	O
the	O
action	O
of	O
the	O
agent	B-DatasetName
:	O
(	O
1	O
)	O
READ	O
to	O
request	O
more	O
input	O
,	O
(	O
2	O
)	O
WRITE	O
to	O
emit	O
new	O
hypothesis	O
tokens	O
.	O
We	O
implement	O
the	O
policy	O
as	O
specified	O
in	O
Algorithm	O
1	O
.	O
The	O
default	O
action	O
is	O
READ	O
.	O
If	O
there	O
is	O
a	O
new	O
chunk	O
,	O
we	O
perform	O
the	O
inference	O
and	O
use	O
the	O
pref	O
ix	O
(	O
W	O
c	O
)	O
function	O
to	O
find	O
the	O
stable	O
prefix	O
.	O
If	O
there	O
are	O
new	O
tokens	O
to	O
display	O
(	O
i.e.	O
,	O
|	O
pref	O
ix	O
(	O
W	O
c	O
)	O
|	O
>	O
|	O
pref	O
ix	O
(	O
W	O
c−1	O
)	O
|	O
)	O
,	O
we	O
return	O
the	O
WRITE	O
action	O
.	O
As	O
soon	O
as	O
our	O
agent	B-DatasetName
emits	O
an	O
endof	O
-	O
sequence	O
(	O
EOS	O
)	O
token	O
,	O
the	O
inference	O
of	O
the	O
utterance	O
is	O
finished	O
by	O
the	O
SimulEval	O
.	O
We	O
noticed	O
that	O
our	O
model	O
was	O
emitting	O
the	O
EOS	O
token	O
quite	O
often	O
,	O
especially	O
in	O
the	O
early	O
chunks	O
.	O
Hence	O
,	O
we	O
ignore	O
the	O
EOS	O
if	O
returned	O
by	O
our	O
model	O
and	O
continue	O
the	O
inference	O
until	O
the	O
end	O
of	O
the	O
source	O
.	O
3	O
Algorithm	O
1	O
Policy	O
function	O
Require	O
:	O
state	O
if	O
state.new_input	O
>	O
chunk_size	O
then	O
hypothesis	O
predict	O
(	O
state	O
)	O
if	O
|	O
hypothesis	O
|	O
>	O
0	B-DatasetName
then	O
return	O
W	O
RIT	O
E	O
end	O
if	O
end	O
if	O
return	O
READ	O

As	O
we	O
could	O
see	O
in	O
Section	O
5.1	O
,	O
the	O
shorter	O
chunk	O
sizes	O
tend	O
to	O
perform	O
worse	O
.	O
One	O
of	O
the	O
reasons	O
might	O
be	O
the	O
limited	O
context	O
of	O
the	O
early	O
chunks	O
.	O
6	O
To	O
increase	O
the	O
early	O
context	O
,	O
we	O
prolong	O
the	O
first	O
chunk	O
to	O
2	O
seconds	O
.	O
The	O
results	O
are	O
in	O
Table	O
1	O
.	O
We	O
see	O
a	O
slight	O
(	O
0.3	O
BLEU	B-MetricName
)	O
increase	O
in	O
quality	O
for	O
a	O
chunk	O
size	O
of	O
250	O
ms	O
,	O
though	O
the	O
initial	O
wait	O
does	O
not	O
improve	O
the	O
BLEU	B-MetricName
and	O
a	O
considerable	O
increase	O
in	O
the	O
latency	O
.	O
The	O
performance	O
seems	O
to	O
be	O
influenced	O
mainly	O
by	O
the	O
chunk	O
size	O
.	O
The	O
reason	O
for	O
smaller	O
chunks	O
'	O
under	O
-	O
performance	O
might	O
be	O
caused	O
by	O
(	O
1	O
)	O
acoustic	O
uncertainty	O
towards	O
the	O
end	O
of	O
a	O
chunk	O
(	O
e.g.	O
,	O
words	O
often	O
get	O
cut	O
in	O
the	O
middle	O
)	O
,	O
or	O
(	O
2	O
)	O
insufficient	O
information	O
difference	O
between	O
two	O
consecutive	O
chunks	O
.	O
This	O
is	O
supported	O
by	O
the	O
observation	O
in	O
Figure	O
3	O
.	O
Increasing	O
the	O
number	O
of	O
consecutive	O
chunks	O
(	O
i.e.	O
,	O
increasing	O
the	O
context	O
for	O
the	O
decision	O
)	O
considered	O
in	O
the	O
local	O
agreement	O
strategy	O
(	O
LA	O
-	O
2	O
,	O
3	O
,	O
4	O
)	O
,	O
improves	O
the	O
quality	O
,	O
while	O
it	O
adds	O
latency	O
.	O

Interestingly	O
,	O
we	O
noticed	O
that	O
some	O
of	O
the	O
strategies	O
achieved	O
negative	O
average	O
lagging	O
(	O
e.g.	O
,	O
LA	O
-	O
2	O
in	O
Section	O
5.1	O
)	O
with	O
a	O
chunk	O
size	O
of	O
250	O
ms	O
has	O
AL	O
of	O
-	O
36	O
ms	O
)	O
.	O
After	O
a	O
closer	O
examination	O
of	O
the	O
outputs	O
,	O
we	O
found	O
that	O
the	O
negative	O
AL	O
is	O
in	O
utterances	O
where	O
the	O
hypothesis	O
is	O
significantly	O
longer	O
than	O
the	O
reference	O
.	O
Recall	B-MetricName
the	O
average	O
latency	O
for	O
speech	O
input	O
defined	O
by	O
:	O
AL	O
speech	O
=	O
1	O
τ	O
′	O
(	O
|	O
X	O
|	O
)	O
τ	O
′	O
(	O
|	O
X	O
|	O
)	O
i=1	O
d	O
i	O
−	O
d	O
*	O
i	O
,	O
(	O
4	O
)	O
where	O
d	O
i	O
=	O
j	O
k=1	O
T	O
k	O
,	O
j	O
is	O
the	O
index	O
of	O
raw	O
audio	O
segment	O
that	O
has	O
been	O
read	O
when	O
generating	O
y	O
i	O
,	O
T	O
k	O
is	O
duration	O
of	O
raw	O
audio	O
segment	O
,	O
τ	O
′	O
(	O
|	O
X	O
|	O
)	O
=	O
min	O
{	O
i	O
|	O
d	O
i	O
=	O
|	O
X	O
|	O
j=1	O
T	O
j	O
}	O
and	O
d	O
*	O
i	O
are	O
the	O
delays	O
of	O
an	O
ideal	O
policy	O
:	O
d	O
*	O
i	O
=	O
(	O
i	O
−	O
1	O
)	O
×	O
|	O
X	O
|	O
j=1	O
T	O
j	O
/	O
|	O
Y	O
*	O
|	O
,	O
(	O
5	O
)	O
where	O
Y	O
*	O
is	O
reference	O
translation	O
.	O
If	O
the	O
hypothesis	O
is	O
longer	O
than	O
the	O
reference	O
,	O
then	O
d	O
*	O
i	O
>	O
d	O
i	O
,	O
making	O
the	O
sum	O
argument	O
in	O
Equation	O
(	O
4	O
)	O
negative	O
.	O
On	O
the	O
other	O
hand	O
,	O
if	O
we	O
use	O
the	O
length	O
of	O
the	O
hypothesis	O
instead	O
,	O
then	O
a	O
shorter	O
hypothesis	O
would	O
benefit	O
.	O
7	O
We	O
,	O
therefore	O
,	O
suggest	O
using	O
the	O
maximum	O
of	O
both	O
to	O
prevent	O
the	O
advantage	O
of	O
either	O
a	O
shorter	O
or	O
a	O
longer	O
hypothesis	O
:	O
d	O
*	O
i	O
=	O
(	O
i	O
−	O
1	O
)	O
×	O
|	O
X	O
|	O
j=1	O
T	O
j	O
/max	O
(	O
|	O
Y	O
|	O
,	O
|	O
Y	O
*	O
|	O
)	O
.	O
(	O
6	O
)	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
submitted	O
system	O
.	O
We	O
follow	O
the	O
allowed	O
training	O
data	O
and	O
pretrained	O
models	O
and	O
therefore	O
our	O
submission	O
is	O
constrained	O
(	O
see	O
Section	O
4.2.1	O
for	O
model	O
description	O
)	O
.	O
For	O
stable	O
hypothesis	O
detection	O
,	O
we	O
decided	O
to	O
use	O
the	O
local	O
agreement	O
strategy	O
with	O
n	O
=	O
2	O
.	O
As	O
shown	O
in	O
Section	O
5.2	O
,	O
the	O
LA	O
-	O
2	O
has	O
the	O
best	O
latencyquality	O
trade	O
-	O
off	O
along	O
with	O
other	O
LA	O
-	O
n	O
strategies	O
.	O
To	O
achieve	O
the	O
different	O
latency	O
regimes	O
,	O
we	O
use	O
various	O
chunk	O
sizes	O
,	O
depending	O
on	O
the	O
language	O
pair	O
.	O
We	O
decided	O
not	O
to	O
use	O
larger	O
n	O
>	O
2	O
to	O
control	O
the	O
latency	O
,	O
as	O
it	O
increases	O
the	O
computation	O
complexity	O
while	O
having	O
the	O
same	O
effect	O
as	O
using	O
a	O
different	O
chunk	O
size	O
.	O
The	O
results	O
on	O
MuST	B-DatasetName
-	I-DatasetName
C	I-DatasetName
tst	O
-	O
COMMON	O
are	O
in	O
Table	O
2	O
.	O
The	O
quality	O
-	O
latency	O
trade	O
-	O
off	O
is	O
in	O
Figure	O
4	O
.	O
From	O
Table	O
2	O
and	O
Figure	O
4	O
,	O
we	O
can	O
see	O
that	O
the	O
proposed	O
method	O
works	O
well	O
on	O
two	O
different	O
models	O
and	O
various	O
language	O
pairs	O
.	O
We	O
see	O
that	O
an	O
improvement	O
in	O
the	O
offline	O
model	O
(	O
offline	O
BLEU	B-MetricName
of	O
31.36	O
and	O
33.14	O
for	O
Model	O
A	O
and	O
B	O
,	O
respectively	O
)	O
leads	O
to	O
improvement	O
in	O
the	O
online	O
regime	O
.	O
Finally	O
,	O
we	O
see	O
that	O
our	O
method	O
beats	O
the	O
best	O
IWSLT	O
2021	O
system	O
(	O
USTC	O
-	O
NELSLIP	O
(	O
Liu	O
et	O
al	O
,	O
2021	O
)	O
)	O
in	O
medium	O
and	O
high	O
latency	O
regimes	O
using	O
both	O
models	O
(	O
i.e.	O
,	O
a	O
model	O
trained	O
from	O
scratch	O
and	O
a	O
model	O
based	O
on	O
pretrained	O
wav2vec	O
and	O
mBART	B-MethodName
)	O
,	O
and	O
is	O
almost	O
on	O
par	O
in	O
the	O
low	O
latency	O
regime	O
(	O
Model	O
A	O
is	O
losing	O
0.35	O
BLEU	B-MetricName
and	O
Model	O
B	O
is	O
losing	O
0.47	O
BLEU	B-MetricName
)	O
.	O

This	O
paper	O
describes	O
our	O
submission	O
to	O
the	O
constrained	O
track	O
of	O
WMT21	O
shared	O
news	O
translation	O
task	O
.	O
We	O
focus	O
on	O
the	O
three	O
relatively	O
low	O
resource	O
language	O
pairs	O
Bengali	O
↔	O
Hindi	O
,	O
English	O
↔	O
Hausa	O
and	O
Xhosa	O
↔	O
Zulu	O
.	O
To	O
overcome	O
the	O
limitation	O
of	O
relatively	O
low	O
parallel	O
data	O
we	O
train	O
a	O
multilingual	O
model	O
using	O
a	O
multitask	O
objective	O
employing	O
both	O
parallel	O
and	O
monolingual	O
data	O
.	O
In	O
addition	O
,	O
we	O
augment	O
the	O
data	O
using	O
back	O
translation	O
.	O
We	O
also	O
train	O
a	O
bilingual	O
model	O
incorporating	O
back	O
translation	O
and	O
knowledge	B-MethodName
distillation	I-MethodName
then	O
combine	O
the	O
two	O
models	O
using	O
sequence	O
-	O
to	O
-	O
sequence	O
mapping	O
.	O
We	O
see	O
around	O
70	O
%	O
relative	O
gain	O
in	O
BLEU	B-MetricName
point	O
for	O
En	O
↔	O
Ha	O
and	O
around	O
25	O
%	O
relative	O
improvements	O
for	O
Bn	O
↔	O
Hi	O
and	O
Xh	O
↔	O
Zu	O
compared	O
to	O
bilingual	O
baselines	O
.	O

System	O
combination	O
or	O
ensembling	O
is	O
known	O
to	O
improve	O
the	O
performance	O
over	O
individual	O
systems	O
.	O
There	O
are	O
many	O
ways	O
to	O
create	O
an	O
ensemble	O
(	O
Liu	O
et	O
al	O
,	O
2018	O
;	O
Dabre	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
example	O
,	O
individual	O
models	O
obtained	O
from	O
different	O
checkpoints	O
during	O
the	O
same	O
training	O
or	O
by	O
training	O
models	O
sharing	O
the	O
same	O
vocab	O
and	O
architecture	O
using	O
different	O
data	O
or	O
simply	O
different	O
random	O
seeds	B-DatasetName
can	O
be	O
combined	O
using	O
model	O
averaging	O
techniques	O
.	O
Here	O
,	O
we	O
opt	O
to	O
combine	O
different	O
models	O
since	O
it	O
generally	O
leads	O
to	O
better	O
performance	O
because	O
different	O
models	O
tend	O
to	O
be	O
more	O
complementary	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
a	O
simple	O
and	O
effective	O
method	O
to	O
combine	O
completely	O
different	O
architectures	O
.	O
The	O
proposed	O
method	O
could	O
be	O
also	O
used	O
in	O
conjunction	O
with	O
checkpoint	O
and	O
model	O
averaging	O
for	O
further	O
gains	O
,	O
but	O
we	O
have	O
n't	O
tried	O
this	O
in	O
our	O
experiments	O
due	O
to	O
time	O
limitations	O
.	O
The	O
basic	O
idea	O
of	O
our	O
combination	O
is	O
very	O
simple	O
.	O
Assume	O
we	O
have	O
the	O
translation	O
pair	O
x	O
y	O
where	O
y	O
is	O
the	O
reference	O
translation	O
.	O
The	O
output	O
of	O
model	O
1	O
is	O
the	O
pair	O
x	O
y1	O
and	O
the	O
output	O
of	O
model	O
2	O
is	O
the	O
pair	O
x	O
y2	O
.	O
This	O
can	O
be	O
generalized	O
to	O
multiple	O
systems	O
but	O
we	O
limited	O
our	O
combination	O
to	O
only	O
two	O
models	O
.	O
We	O
train	O
a	O
new	O
model	O
that	O
takes	O
the	O
set	O
of	O
hypotheses	O
(	O
possibly	O
augmented	O
by	O
the	O
source	O
sentence	O
)	O
from	O
the	O
two	O
models	O
to	O
generate	O
the	O
target	O
sentence	O
.	O
Thus	O
this	O
model	O
combines	O
the	O
outputs	O
of	O
two	O
models	O
in	O
the	O
ensemble	O
to	O
produce	O
a	O
translation	O
closer	O
to	O
the	O
original	O
target	O
sentence	O
i.e.	O
<	O
HY	O
P	O
>	O
y1	O
<	O
HY	O
P	O
>	O
y2	O
y.	O
We	O
also	O
experimented	O
with	O
adding	O
the	O
source	O
to	O
the	O
input	O
i.e.	O
<	O
SRC	O
>	O
x	O
<	O
HY	O
P	O
>	O
y1	O
<	O
HY	O
P	O
>	O
y2	O
y	O
which	O
led	O
to	O
around	O
0.3	O
BLEU	B-MetricName
improvement	O
for	O
Ha	O
En	O
,	O
but	O
we	O
have	O
n't	O
tried	O
on	O
other	O
pairs	O
due	O
to	O
time	O
limitation	O
.	O
All	O
combination	O
models	O
use	O
6	O
layers	O
encoder	O
and	O
decoder	O
and	O
a	O
64	O
K	O
vocabulary	O
similar	O
to	O
the	O
multilingual	O
system	O
.	O
These	O
combination	O
models	O
use	O
the	O
full	O
bitext	O
and	O
dev	O
data	O
provided	O
in	O
WMT21	O
as	O
shown	O
in	O
Table	O
1	O
.	O
The	O
system	O
combination	O
is	O
outlined	O
in	O
Figure	O
1	O
.	O
This	O
ensembling	O
technique	O
can	O
be	O
thought	O
of	O
as	O
providing	O
both	O
system	O
combination	O
and	O
post	O
-	O
editing	O
capabilities	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
results	O
of	O
our	O
intermediate	O
and	O
final	O
systems	O
.	O
We	O
report	O
Sacre	O
-	O
BLEU	B-MetricName
(	O
Post	O
,	O
2018	O
)	O
on	O
the	O
validation	O
set	O
released	O
in	O
WMT21	O
,	O
and	O
both	O
SacreBLEU	B-MetricName
and	O
COMET	O
(	O
Rei	O
et	O
al	O
,	O
2020	O
)	O
using	O
the	O
available	O
implementation	O
3	O
on	O
the	O
official	O
test	O
set	O
released	O
in	O
WMT21	O
.	O
The	O
results	O
for	O
the	O
six	O
submitted	O
language	O
pairs	O
are	O
in	O
Tables	O
3	O
-	O
5	O
.	O
The	O
first	O
row	O
in	O
each	O
table	O
shows	O
the	O
bilingual	O
baseline	O
which	O
performs	O
relatively	O
poor	O
due	O
to	O
the	O
limited	O
amount	O
of	O
parallel	O
data	O
for	O
each	O
pair	O
.	O
This	O
is	O
followed	O
by	O
the	O
four	O
multilingual	O
systems	O
with	O
different	O
objectives	O
.	O
It	O
is	O
clear	O
that	O
adding	O
a	O
monolingual	O
objective	O
brings	O
nice	O
improvements	O
for	O
all	O
language	O
pairs	O
.	O
The	O
M	O
T	O
+	O
DAE	O
and	O
M	O
T	O
+	O
M	O
LM	O
+	O
DAE	O
perform	O
closely	O
for	O
all	O
language	O
pairs	O
indicating	O
that	O
target	O
monolingual	O
data	O
is	O
most	O
important	O
.	O
The	O
next	O
two	O
rows	O
show	O
the	O
results	O
of	O
adding	O
back	O
-	O
translated	O
data	O
to	O
the	O
multilingual	O
model	O
and	O
a	O
bilingual	O
baseline	O
using	O
back	O
-	O
translated	O
and	O
knowledge	O
distilled	O
data	O
generated	O
from	O
the	O
best	O
multilingual	O
model	O
.	O
As	O
expected	O
adding	O
back	O
translation	O
brings	O
significant	O
improvement	O
to	O
all	O
language	O
pairs	O
.	O
Also	O
using	O
the	O
multilingual	O
model	O
to	O
create	O
data	O
for	O
a	O
bilingual	O
model	O
shows	O
excellent	O
results	O
that	O
outperform	O
the	O
multilingual	O
model	O
.	O
Finally	O
,	O
the	O
ensemble	O
,	O
as	O
expected	O
,	O
performs	O
better	O
than	O
the	O
individual	O
models	O
.	O
The	O
significant	O
difference	O
between	O
reported	O
improvements	O
in	O
Ha	O
↔	O
En	O
and	O
other	O
directions	O
shows	O
the	O
effectiveness	O
of	O
adding	O
De	O
↔	O
En	O
parallel	O
and	O
monolingual	O
data	O
that	O
helps	O
English	O
centric	O
directions	O
more	O
than	O
other	O
directions	O
.	O
We	O
evaluated	O
the	O
final	O
submitted	O
systems	O
on	O
the	O
official	O
test	O
set	O
released	O
in	O
WMT21	O
as	O
shown	O
in	O
Table	O
6	O
.	O

This	O
paper	O
describes	O
our	O
submission	O
to	O
the	O
constrained	O
track	O
of	O
WMT21	O
.	O
We	O
focus	O
on	O
the	O
three	O
relatively	O
low	O
resource	O
language	O
pairs	O
Bn	O
↔	O
Hi	O
,	O
En	O
↔	O
Ha	O
and	O
Xh	O
↔	O
Zu	O
.	O
To	O
overcome	O
the	O
limitation	O
of	O
relatively	O
low	O
parallel	O
data	O
we	O
train	O
a	O
multilingual	O
model	O
using	O
a	O
multitask	O
objective	O
recently	O
proposed	O
in	O
(	O
Wang	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
addition	O
,	O
we	O
augment	O
the	O
data	O
using	O
back	O
translation	O
.	O
We	O
also	O
use	O
the	O
resulting	O
multilingual	O
model	O
to	O
create	O
a	O
bilingual	O
model	O
incorporating	O
back	O
translation	O
and	O
knowledge	B-MethodName
distillation	I-MethodName
.	O
Finally	O
,	O
we	O
combine	O
the	O
two	O
models	O
,	O
using	O
a	O
flexible	O
sequence	O
-	O
to	O
-	O
sequence	O
approach	O
,	O
to	O
yield	O
our	O
submitted	O
systems	O
.	O
We	O
see	O
large	O
gains	O
up	O
to	O
8	O
-	O
10	O
BLEU	B-MetricName
points	O
for	O
En	O
↔	O
Ha	O
and	O
nice	O
improvements	O
of	O
up	O
to	O
2	O
-	O
3	O
BLEU	B-MetricName
points	O
for	O
Bn	O
↔	O
Hi	O
and	O
Xh	O
↔	O
Zu	O
.	O

Exemplar	O
-	O
based	O
generative	O
models	O
(	O
Wu	O
et	O
al	O
,	O
2019	O
;	O
Cai	O
et	O
al	O
,	O
2019b	O
;	O
Gupta	O
et	O
al	O
,	O
2021	O
)	O
for	O
open	O
-	O
domain	O
conversation	O
combine	O
a	O
retrieval	O
model	O
(	O
Humeau	O
et	O
al	O
,	O
2019	O
;	O
Mazare	O
et	O
al	O
,	O
2018	O
;	O
Kim	O
et	O
al	O
,	O
2021	O
)	O
and	O
a	O
generative	O
model	O
(	O
Adiwardana	O
et	O
al	O
,	O
2020	O
;	O
Roller	O
et	O
al	O
,	O
2021	O
;	O
Figure	O
1	O
:	O
Responses	O
generated	O
by	O
the	O
three	O
exemplarbased	O
generative	O
models	O
.	O
RetNRef	O
ignores	O
the	O
exemplar	O
during	O
response	B-TaskName
generation	I-TaskName
,	O
RetNRef	O
α	B-HyperparameterName
generates	O
the	O
response	O
highly	O
over	O
-	O
fitted	O
to	O
the	O
exemplar	O
,	O
and	O
RetNRef	O
trained	O
with	O
our	O
training	O
method	O
(	O
CORGE	O
)	O
well	O
utilizes	O
the	O
exemplar	O
to	O
produce	O
a	O
more	O
fluent	O
response	O
than	O
that	O
of	O
the	O
others	O
.	O
Zhang	O
et	O
al	O
,	O
2020	O
;	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
into	O
a	O
single	O
framework	O
to	O
generate	O
responses	O
in	O
two	O
steps	O
:	O
(	O
1	O
)	O
the	O
retriever	O
searches	O
an	O
exemplar	O
using	O
the	O
given	O
context	O
as	O
a	O
query	O
,	O
and	O
(	O
2	O
)	O
the	O
generator	O
produces	O
a	O
response	O
based	O
on	O
the	O
given	O
context	O
and	O
the	O
retrieved	O
exemplar	O
.	O
Exemplar	O
-	O
based	O
generative	O
models	O
produce	O
more	O
specific	O
responses	O
than	O
vanilla	O
generative	O
models	O
while	O
being	O
more	O
fluent	O
than	O
retrieval	O
models	O
.	O
Despite	O
their	O
success	O
,	O
exemplar	O
-	O
based	O
generative	O
models	O
have	O
two	O
major	O
shortcomings	O
.	O
Primitive	O
exemplar	O
-	O
based	O
generative	O
models	O
Cai	O
et	O
al	O
,	O
2019a	O
)	O
tend	O
to	O
entirely	O
ignore	O
the	O
exemplars	O
and	O
produce	O
responses	O
similar	O
to	O
those	O
of	O
vanilla	O
generative	O
models	O
.	O
This	O
is	O
due	O
to	O
the	O
one	O
-	O
to	O
-	O
many	O
problem	O
(	O
Li	O
et	O
al	O
,	O
2016	O
)	O
where	O
there	O
are	O
many	O
possible	O
responses	O
for	O
each	O
dialogue	O
context	O
.	O
During	O
the	O
training	O
phase	O
,	O
the	O
retrieved	O
exemplar	O
is	O
not	O
helpful	O
for	O
generating	O
the	O
gold	O
response	O
when	O
the	O
exemplar	O
retrieved	O
for	O
the	O
given	O
context	O
is	O
significantly	O
different	O
from	O
the	O
gold	O
response	O
.	O
This	O
leads	O
exemplar	O
-	O
based	O
generative	O
models	O
to	O
ignore	O
the	O
exemplar	O
while	O
generating	O
responses	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
(	O
a	O
)	O
.	O
To	O
address	O
this	O
issue	O
,	O
recent	O
exemplar	O
-	O
based	O
generative	O
models	O
utilize	O
the	O
gold	O
response	O
(	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
or	O
the	O
slightly	O
perturbed	O
gold	O
response	O
(	O
Cai	O
et	O
al	O
,	O
2019b	O
)	O
as	O
an	O
exemplar	O
in	O
the	O
training	O
phase	O
.	O
However	O
,	O
these	O
training	O
methods	O
cause	O
the	O
generator	O
to	O
rely	O
heavily	O
on	O
the	O
retrieved	O
exemplar	O
,	O
i.e.	O
the	O
generator	O
resorts	O
to	O
copying	O
the	O
provided	O
tokens	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
(	O
b	O
)	O
.	O
These	O
two	O
disadvantages	O
of	O
existing	O
exemplar	O
-	O
based	O
generative	O
models	O
can	O
adversely	O
affect	O
the	O
quality	O
of	O
the	O
generated	O
response	O
.	O
Therefore	O
,	O
we	O
introduce	O
CORGE	O
(	O
COnnecting	O
Retriever	O
and	O
GEnerator	O
)	O
,	O
a	O
simple	O
training	O
method	O
of	O
exemplar	O
-	O
based	O
generative	O
models	O
considering	O
the	O
one	O
-	O
to	O
-	O
many	O
problem	O
of	O
the	O
open	O
-	O
domain	O
conversation	O
.	O
As	O
inspired	O
by	O
Wu	O
et	O
al	O
(	O
2019	O
)	O
,	O
CORGE	O
first	O
utilizes	O
the	O
gold	O
response	O
instead	O
of	O
dialogue	O
context	O
as	O
the	O
query	O
for	O
the	O
retriever	O
to	O
select	O
exemplars	O
that	O
are	O
similar	O
to	O
the	O
gold	O
response	O
.	O
The	O
retrieved	O
exemplars	O
ensure	O
that	O
exemplar	O
-	O
based	O
generative	O
models	O
utilize	O
their	O
semantics	O
while	O
generating	O
the	O
gold	O
response	O
at	O
the	O
training	O
phase	O
.	O
Since	O
the	O
exemplars	O
are	O
retrieved	O
by	O
the	O
gold	O
response	O
,	O
some	O
of	O
them	O
are	O
lexically	O
identical	O
or	O
too	O
similar	O
to	O
the	O
gold	O
response	O
.	O
These	O
exemplars	O
lead	O
exemplar	O
-	O
based	O
generative	O
models	O
to	O
be	O
trained	O
to	O
depend	O
on	O
the	O
exemplar	O
heavily	O
.	O
Thus	O
,	O
CORGE	O
then	O
eliminates	O
the	O
exemplars	O
based	O
on	O
the	O
distance	O
between	O
the	O
exemplars	O
and	O
the	O
gold	O
response	O
to	O
alleviate	O
the	O
dependency	O
of	O
the	O
generative	O
models	O
on	O
the	O
exemplars	O
.	O
Here	O
,	O
we	O
employ	O
Jaccard	O
similarity	O
to	O
measure	O
the	O
distance	O
(	O
Guu	O
et	O
al	O
,	O
2018	O
;	O
Cai	O
et	O
al	O
,	O
2019a	O
;	O
Wu	O
et	O
al	O
,	O
2019	O
)	O
.	O
However	O
,	O
as	O
the	O
selected	O
exemplars	O
solely	O
depend	O
on	O
the	O
gold	O
response	O
,	O
some	O
of	O
them	O
may	O
be	O
irrelevant	O
to	O
the	O
given	O
context	O
,	O
which	O
results	O
in	O
exemplar	O
-	O
based	O
generative	O
models	O
still	O
ignoring	O
the	O
retrieved	O
exemplar	O
.	O
To	O
solve	O
this	O
,	O
CORGE	O
utilizes	O
the	O
relevance	O
scores	O
between	O
the	O
context	O
and	O
the	O
exemplar	O
to	O
weight	O
the	O
relevant	O
exemplars	O
and	O
penalizes	O
irrelevant	O
exemplars	O
to	O
the	O
given	O
context	O
.	O
Extensive	O
experiments	O
show	O
that	O
CORGE	O
is	O
generally	O
applicable	O
to	O
the	O
existing	O
exemplar	O
-	O
based	O
generative	O
models	O
and	O
improves	O
the	O
quality	O
of	O
generated	O
responses	O
regarding	O
appropriateness	O
and	O
informativeness	O
.	O

As	O
we	O
select	O
the	O
exemplar	O
totally	O
based	O
on	O
the	O
gold	O
response	O
,	O
some	O
of	O
kNE	O
could	O
be	O
relevant	O
to	O
the	O
gold	O
response	O
r	O
i	O
but	O
irrelevant	O
to	O
the	O
given	O
context	O
c	O
i	O
.	O
Therefore	O
,	O
we	O
condition	O
the	O
generator	O
with	O
the	O
relevance	O
score	O
of	O
kNE	O
to	O
reward	O
the	O
relevant	O
exemplars	O
and	O
penalize	O
irrelevant	O
exemplars	O
.	O
Using	O
the	O
retriever	O
R	O
,	O
we	O
calculate	O
the	O
relevance	O
score	O
S	O
R	O
(	O
z	O
i	O
,	O
j	O
,	O
c	O
i	O
)	O
per	O
each	O
selected	O
exemplar	O
z	O
i	O
,	O
j	O
,	O
then	O
apply	O
the	O
softmax	B-MethodName
function	O
to	O
the	O
relevance	O
score	O
to	O
1	O
Note	O
that	O
S	O
R	O
(	O
z	O
,	O
c	O
)	O
and	O
S	O
R	O
′	O
(	O
z	O
,	O
ri	O
)	O
use	O
the	O
same	O
retriever	O
,	O
but	O
they	O
are	O
computed	O
differently	O
.	O
Please	O
refer	O
to	O
how	O
we	O
calculate	O
the	O
score	O
S	O
R	O
′	O
(	O
z	O
,	O
ri	O
)	O
and	O
S	O
R	O
(	O
z	O
,	O
c	O
)	O
in	O
the	O
Supplementary	O
Materials	O
.	O
obtain	O
the	O
normalized	O
relevance	O
score	O
P	O
R	O
(	O
z	O
i	O
,	O
j	O
,	O
c	O
i	O
)	O
.	O
Then	O
we	O
replace	O
the	O
traditional	O
likelihood	O
with	O
the	O
weighted	O
likelihood	O
using	O
the	O
normalized	O
score	O
.	O
Our	O
final	O
training	O
objective	O
is	O
to	O
minimize	O
the	O
loss	B-MetricName
function	O
L	O
=	O
n	O
i=1	O
L	O
(	O
r	O
i	O
,	O
c	O
i	O
)	O
where	O
:	O
L	O
(	O
ri	O
,	O
ci	O
)	O
=	O
−	O
log	O
z	O
Z	O
i	O
P	O
R	O
(	O
z	O
,	O
ci	O
)	O
P	O
G	O
(	O
ri	O
|	O
ci	O
,	O
z	O
)	O
(	O
1	O
)	O
The	O
gradient	O
of	O
the	O
generator	O
G	O
is	O
calculated	O
as	O
follows	O
:	O
∇	O
G	O
L	O
(	O
ri	O
,	O
ci	O
)	O
=	O
−α	O
z	O
Z	O
i	O
P	O
R	O
(	O
z	O
,	O
ci	O
)	O
∇	O
G	O
(	O
P	O
G	O
(	O
ri	O
|	O
ci	O
,	O
z	O
)	O
)	O
,	O
(	O
2	O
)	O
where	O
α	B-HyperparameterName
−1	O
=	O
z	O
Z	O
i	O
P	O
R	O
(	O
z	O
,	O
c	O
i	O
)	O
P	O
G	O
(	O
r	O
i	O
|	O
c	O
i	O
,	O
z	O
)	O
.	O
This	O
equation	O
demonstrates	O
that	O
the	O
gradient	O
of	O
the	O
generator	O
G	O
is	O
scaled	O
by	O
the	O
normalized	O
relevance	O
score	O
P	O
R	O
(	O
z	O
,	O
c	O
i	O
)	O
,	O
which	O
indicates	O
that	O
the	O
generator	O
is	O
less	O
updated	O
when	O
the	O
retrieved	O
exemplar	O
z	O
is	O
not	O
relevant	O
to	O
the	O
given	O
context	O
c	O
i	O
.	O
This	O
procedure	O
helps	O
the	O
model	O
ignore	O
the	O
irrelevant	O
exemplars	O
.	O
Thus	O
,	O
the	O
generator	O
learns	O
to	O
fetch	O
tokens	O
from	O
the	O
exemplar	O
more	O
easily	O
,	O
which	O
is	O
relevant	O
to	O
the	O
gold	O
response	O
.	O
Difference	O
between	O
CORGE	O
and	O
Knowledgegrounded	O
generative	O
models	O
The	O
way	O
of	O
leveraging	O
the	O
relevance	O
scores	O
is	O
already	O
employed	O
by	O
knowledge	O
-	O
grounded	O
generative	O
models	O
(	O
Lewis	O
et	O
al	O
,	O
2020	O
;	O
Sachan	O
et	O
al	O
,	O
2021	O
)	O
in	O
open	B-TaskName
-	I-TaskName
domain	I-TaskName
question	I-TaskName
answering	I-TaskName
.	O
However	O
,	O
there	O
is	O
a	O
significant	O
difference	O
between	O
our	O
CORGE	O
and	O
knowledgegrounded	O
generative	O
models	O
.	O
CORGE	O
uses	O
the	O
relevance	O
score	O
P	O
R	O
(	O
z	O
,	O
c	O
i	O
)	O
to	O
penalize	O
the	O
irrelevant	O
exemplars	O
z	O
to	O
the	O
given	O
context	O
c	O
i	O
since	O
the	O
exemplars	O
are	O
retrieved	O
by	O
S	O
R	O
′	O
(	O
z	O
,	O
r	O
i	O
)	O
.	O
Knowledgegrounded	O
generative	O
models	O
use	O
it	O
as	O
the	O
latent	O
variable	O
to	O
jointly	O
train	O
the	O
retriever	O
R	O
and	O
generator	O
G.	O
Especially	O
,	O
knowledge	O
-	O
grounded	O
generative	O
models	O
also	O
tend	O
to	O
ignore	O
the	O
retrieved	O
exemplars	O
due	O
Context	O
Retrieval	O
indicates	O
the	O
exemplar	O
retrieved	O
by	O
using	O
the	O
context	O
as	O
a	O
query	O
,	O
and	O
kNE	O
shows	O
the	O
exemplars	O
selected	O
by	O
using	O
the	O
gold	O
response	O
as	O
a	O
query	O
.	O
Sim	O
measures	O
the	O
lexical	O
similarity	O
between	O
the	O
gold	O
response	O
and	O
the	O
exemplar	O
and	O
P	O
R	O
(	O
z	O
,	O
c	O
)	O
indicates	O
the	O
normalized	O
relevance	O
score	O
calculated	O
by	O
retriever	O
.	O
to	O
the	O
one	O
-	O
to	O
-	O
many	O
nature	O
in	O
open	O
-	O
domain	O
conversation	O
when	O
the	O
retriever	O
and	O
generator	O
are	O
jointly	O
trained	O
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
do	O
not	O
perform	O
the	O
joint	O
learning	O
of	O
the	O
retriever	O
and	O
the	O
generator	O
,	O
but	O
freeze	O
the	O
retriever	O
while	O
training	O
the	O
generator	O
.	O

Retrieval	O
and	O
Generative	O
Models	O
Bi	O
-	O
encoder	O
256	O
M	O
(	O
Mazare	O
et	O
al	O
,	O
2018	O
)	O
and	O
Blender	B-MethodName
90	O
M	O
(	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
are	O
considered	O
as	O
a	O
baseline	O
retrieval	O
model	O
and	O
a	O
baseline	O
generative	O
model	O
.	O
Further	O
,	O
they	O
are	O
also	O
employed	O
as	O
a	O
retriever	O
and	O
a	O
generator	O
of	O
the	O
following	O
exemplarbased	O
generative	O
baselines	O
,	O
respectively	O
.	O
(	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
,	O
and	O
MatToGen	O
(	O
Cai	O
et	O
al	O
,	O
2019b	O
)	O
,	O
as	O
baselines	O
.	O
RetNRef	O
concatenates	O
the	O
retrieved	O
exemplar	O
with	O
the	O
given	O
context	O
as	O
the	O
input	O
of	O
the	O
generator	O
to	O
produce	O
the	O
response	O
.	O
RetNRef	O
α	B-HyperparameterName
is	O
the	O
dialogue	O
retrieval	O
version	O
of	O
RetNRef	O
,	O
which	O
adopts	O
α	B-HyperparameterName
-	O
blending	O
to	O
escape	O
from	O
simply	O
ignoring	O
the	O
retrieved	O
exemplars	O
(	O
α	B-HyperparameterName
=	O
0.5	O
)	O
.	O
MatToGen	O
extracts	O
the	O
meaningful	O
tokens	O
from	O
the	O
exemplar	O
to	O
provide	O
them	O
to	O
the	O
generator	O
.	O
To	O
verify	O
the	O
effectiveness	O
of	O
our	O
training	O
method	O
,	O
we	O
apply	O
CORGE	O
to	O
RetNRef	O
and	O
Mat	O
-	O
ToGen	O
instead	O
of	O
their	O
training	O
method	O
.	O
They	O
are	O
denoted	O
as	O
RetNRef	O
+	O
CORGE	O
and	O
MatTo	O
-	O
Gen+CORGE	O
,	O
respectively	O
.	O
Knowledge	O
-	O
grounded	O
Generative	O
Models	O
Although	O
RAG	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
and	O
KIF	O
(	O
Fan	O
et	O
al	O
,	O
2021	O
)	O
are	O
proposed	O
to	O
perform	O
knowledgegrounded	O
generation	O
tasks	O
,	O
we	O
employ	O
RAG	B-MethodName
and	O
KIF	O
as	O
baselines	O
since	O
they	O
have	O
a	O
similar	O
form	O
with	O
exemplar	O
-	O
based	O
generative	O
models	O
.	O
Our	O
experiments	O
demonstrate	O
that	O
these	O
knowledge	O
-	O
grounded	O
generative	O
models	O
can	O
not	O
be	O
directly	O
applied	O
to	O
the	O
open	O
-	O
domain	O
conversation	O
.	O

To	O
verify	O
the	O
effectiveness	O
of	O
our	O
training	O
method	O
CORGE	O
,	O
we	O
conduct	O
a	O
pair	O
-	O
wise	O
comparison	O
through	O
the	O
human	O
evaluation	O
following	O
.	O
We	O
use	O
two	O
criteria	O
:	O
Appropriateness	O
and	O
Informativeness	O
.	O
Appropriateness	O
measures	O
how	O
the	O
generated	O
response	O
is	O
fluent	O
,	O
logical	O
,	O
and	O
appropriate	O
to	O
the	O
given	O
context	O
.	O
Informativeness	O
measures	O
how	O
the	O
generated	O
response	O
has	O
meaningful	O
information	O
relevant	O
to	O
the	O
given	O
context	O
.	O
We	O
use	O
Amazon	O
Mechanical	O
Turk	O
to	O
collect	O
the	O
annotations	O
,	O
and	O
more	O
details	O
are	O
described	O
in	O
the	O
Supplementary	B-DatasetName
Material	I-DatasetName
.	O
We	O
also	O
employ	O
the	O
automatic	O
evaluation	O
metrics	O
,	O
Perplexity	B-MetricName
(	O
PPL	O
)	O
,	O
Dist	O
-	O
n	O
,	O
and	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
,	O
to	O
analyze	O
the	O
generated	O
responses	O
of	O
each	O
model	O
.	O
PPL	O
measures	O
how	O
well	O
the	O
model	O
predicts	O
a	O
response	O
based	O
on	O
the	O
given	O
input	O
context	O
,	O
and	O
lower	O
PPL	O
indicates	O
that	O
the	O
model	O
predicts	O
the	O
response	O
better	O
.	O
To	O
analyze	O
how	O
much	O
the	O
exemplar	O
-	O
based	O
generative	O
model	O
leverages	O
the	O
retrieved	O
exemplar	O
,	O
we	O
introduce	O
two	O
variants	O
of	O
PPL	O
by	O
utilizing	O
conditional	O
probability	O
when	O
exemplars	O
are	O
given	O
:	O
(	O
1	O
)	O
PPL	O
gold	O
uses	O
the	O
conditional	O
probability	O
P	O
G	O
(	O
r	O
|	O
c	O
,	O
r	O
)	O
,	O
which	O
assumes	O
the	O
situation	O
when	O
the	O
gold	O
response	O
is	O
given	O
as	O
an	O
exemplar	O
,	O
and	O
(	O
2	O
)	O
PPL	O
ret	O
uses	O
the	O
conditional	O
probability	O
P	O
G	O
(	O
r	O
|	O
c	O
,	O
z	O
)	O
where	O
z	O
is	O
the	O
retrieved	O
exemplar	O
by	O
using	O
S	O
R	O
′	O
(	O
z	O
,	O
r	O
)	O
.	O
Lower	O
PPL	O
gold	O
denotes	O
that	O
the	O
exemplar	O
-	O
based	O
generative	O
model	O
predicts	O
the	O
gold	O
response	O
well	O
when	O
the	O
gold	O
response	O
is	O
given	O
as	O
an	O
exemplar	O
.	O
Lower	O
PPL	O
ret	O
indicates	O
that	O
the	O
exemplar	O
-	O
based	O
generative	O
model	O
well	O
leverages	O
the	O
provided	O
exemplar	O
to	O
predict	O
the	O
gold	O
response	O
.	O
Dist	O
-	O
n	O
(	O
Li	O
et	O
al	O
,	O
2016	O
)	O
is	O
the	O
ratio	O
of	O
distinct	O
ngrams	O
to	O
a	O
total	O
number	O
of	O
n	O
-	O
grams	O
for	O
all	O
the	O
generated	O
responses	O
,	O
which	O
measures	O
the	O
degree	O
of	O
the	O
diversity	O
of	O
the	O
generated	O
responses	O
.	O
BLEU	B-MetricName
(	O
z	O
,	O
r	O
)	O
is	O
adopted	O
to	O
measure	O
the	O
degree	O
of	O
the	O
token	O
overlap	O
between	O
the	O
provided	O
exemplar	O
and	O
the	O
generated	O
response	O
pair	O
(	O
z	O
,	O
r	O
)	O
.	O
A	O
higher	O
BLEU	B-MetricName
(	O
z	O
,	O
r	O
)	O
score	O
indicates	O
that	O
the	O
generator	O
copies	O
more	O
from	O
the	O
provided	O
exemplar	O
while	O
generating	O
the	O
response	O
.	O

We	O
provide	O
the	O
details	O
of	O
our	O
implementation	O
in	O
the	O
Supplementary	B-DatasetName
Material	I-DatasetName
.	O
We	O
will	O
the	O
source	O
codes	O
of	O
CORGE	O
for	O
the	O
reproducibility	O
of	O
the	O
conducted	O
experiments	O
.	O
6	O
Experimental	O
Results	O
PPL	O
ret	O
than	O
RetNRef	O
+	O
CORGE	O
.	O
This	O
result	O
demonstrates	O
that	O
RetNRef	O
α	B-HyperparameterName
does	O
not	O
make	O
good	O
use	O
of	O
the	O
retrieved	O
exemplar	O
except	O
when	O
the	O
gold	O
response	O
is	O
given	O
as	O
the	O
retrieved	O
exemplar	O
.	O
From	O
this	O
observation	O
,	O
we	O
claim	O
that	O
RetNRef	O
α	B-HyperparameterName
generates	O
a	O
response	O
highly	O
over	O
-	O
fitted	O
to	O
the	O
selected	O
exemplar	O
,	O
which	O
is	O
caused	O
by	O
utilizing	O
the	O
gold	O
response	O
as	O
an	O
exemplar	O
in	O
the	O
training	O
phase	O
.	O
The	O
same	O
goes	O
for	O
MatToGen	O
,	O
where	O
applying	O
CORGE	O
mitigates	O
the	O
over	O
-	O
fitting	O
issue	O
.	O

Higher	O
Dist	O
-	O
n	O
of	O
RetNRef	O
+	O
CORGE	O
and	O
Mat	O
-	O
ToGen+CORGE	O
compared	O
to	O
Blender	B-MethodName
90	O
M	O
shows	O
that	O
our	O
exemplar	O
-	O
based	O
generative	O
models	O
produce	O
more	O
diverse	O
responses	O
than	O
the	O
vanilla	O
generative	O
model	O
.	O
Moreover	O
,	O
RetNRef	O
+	O
CORGE	O
has	O
higher	O
Dist	O
-	O
n	O
than	O
RetNRef	O
,	O
which	O
shows	O
that	O
utilizing	O
the	O
exemplars	O
helps	O
the	O
generator	O
diversify	O
the	O
responses	O
.	O
Although	O
RetNRef	O
α	B-HyperparameterName
is	O
the	O
only	O
one	O
that	O
achieves	O
comparable	O
Dist	O
-	O
n	O
to	O
that	O
of	O
the	O
vanilla	O
retrieval	O
model	O
,	O
Bi	O
-	O
encoder	O
256	O
M	O
,	O
it	O
is	O
derived	O
from	O
an	O
over	O
-	O
fitting	O
to	O
the	O
exemplar	O
considering	O
the	O
gap	O
between	O
PPL	O
gold	O
and	O
PPL	O
ret	O
,	O
resulting	O
in	O
the	O
degradation	O
of	O
appropriateness	O
and	O
informativeness	O
in	O
human	O
evaluation	O
.	O
Average	O
BLEU	B-MetricName
(	O
z	O
,	O
r	O
)	O
scores	O
implicitly	O
measure	O
the	O
overlap	O
between	O
the	O
retrieved	O
exemplar	O
and	O
the	O
generated	O
response	O
;	O
thus	O
,	O
a	O
higher	O
degree	O
of	O
BLEU	B-MetricName
(	O
z	O
,	O
r	O
)	O
indicates	O
that	O
the	O
generator	O
depends	O
more	O
on	O
the	O
retrieved	O
exemplar	O
.	O
RetNRef	O
shows	O
a	O
negligible	O
BLEU	B-MetricName
(	O
z	O
,	O
r	O
)	O
score	O
,	O
which	O
reaffirms	O
that	O
the	O
model	O
is	O
almost	O
not	O
utilizing	O
the	O
retrieved	O
exemplar	O
.	O
RetNRef	O
α	B-HyperparameterName
and	O
MatToGen	O
have	O
higher	O
BLEU	B-MetricName
(	O
z	O
,	O
r	O
)	O
scores	O
compared	O
to	O
RetNRef	O
+	O
CORGE	O
and	O
MatToGen+CORGE	O
,	O
respectively	O
,	O
which	O
verifies	O
that	O
the	O
former	O
depends	O
more	O
on	O
the	O
retrieved	O
exemplar	O
than	O
the	O
latter	O
.	O

The	O
automatic	O
evaluation	O
results	O
in	O
Table	O
3	O
confirm	O
that	O
knowledge	O
-	O
grounded	O
generative	O
models	O
are	O
ignoring	O
the	O
exemplar	O
.	O
PPL	O
gold	O
,	O
PPL	O
ret	O
,	O
and	O
Dist	O
-	O
n	O
of	O
RAG	B-MethodName
and	O
KIF	O
have	O
a	O
similar	O
degree	O
to	O
those	O
of	O
Blender	B-MethodName
90	O
M	O
,	O
which	O
implies	O
that	O
the	O
exemplars	O
are	O
not	O
providing	O
useful	O
information	O
while	O
generating	O
the	O
response	O
.	O
The	O
average	O
BLEU	B-MetricName
(	O
z	O
,	O
r	O
)	O
score	O
also	O
has	O
a	O
poor	O
degree	O
,	O
indicating	O
almost	O
no	O
overlap	O
between	O
the	O
retrieved	O
exemplars	O
and	O
the	O
generated	O
responses	O
.	O
We	O
explain	O
that	O
these	O
results	O
are	O
originated	O
from	O
the	O
difference	O
between	O
the	O
open	O
-	O
domain	O
conversation	O
and	O
knowledge	O
-	O
grounded	O
generation	O
tasks	O
.	O
While	O
training	O
knowledge	O
-	O
grounded	O
generative	O
models	O
,	O
they	O
use	O
P	O
R	O
(	O
z	O
,	O
c	O
)	O
to	O
fetch	O
the	O
external	O
knowledge	O
.	O
However	O
,	O
the	O
generator	O
also	O
ignores	O
the	O
retrieved	O
exemplar	O
due	O
to	O
the	O
one	O
-	O
to	O
-	O
many	O
nature	O
of	O
the	O
open	O
-	O
domain	O
conversation	O
.	O
In	O
addition	O
,	O
we	O
observe	O
that	O
jointly	O
training	O
the	O
retriever	O
with	O
the	O
generator	O
causes	O
the	O
retriever	O
stuck	O
in	O
the	O
local	O
minima	O
.	O
As	O
shown	O
in	O
Figure	O
4	O
,	O
the	O
standard	O
deviation	O
of	O
normalized	O
relevance	O
scores	O
P	O
R	O
(	O
z	O
,	O
c	O
)	O
computed	O
by	O
the	O
retriever	O
almost	O
gets	O
near	O
zero	O
when	O
the	O
retriever	O
of	O
RAG	B-MethodName
is	O
jointly	O
trained	O
.	O
A	O
smaller	O
standard	O
deviation	O
means	O
the	O
relevance	O
scores	O
are	O
getting	O
flattened	O
.	O
Although	O
knowledge	O
-	O
grounded	O
generative	O
models	O
empirically	O
have	O
shown	O
that	O
jointly	O
training	O
the	O
retriever	O
and	O
generator	O
improves	O
the	O
performance	O
in	O
knowledge	O
-	O
intensive	O
NLP	O
tasks	O
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
,	O
in	O
open	O
-	O
domain	O
conversation	O
,	O
the	O
retrieved	O
exemplars	O
are	O
ignored	O
.	O
Thus	O
,	O
the	O
retriever	O
learns	O
to	O
produce	O
an	O
uninformative	O
relevance	O
score	O
.	O
As	O
a	O
result	O
,	O
the	O
retriever	O
collapses	O
,	O
which	O
means	O
the	O
retriever	O
may	O
return	O
inappropriate	O
exemplars	O
to	O
the	O
generator	O
(	O
also	O
shown	O
in	O
the	O
example	O
of	O
KIF	O
and	O
RAG	B-MethodName
in	O
Table	O
4	O
)	O
.	O
Intriguingly	O
,	O
jointly	O
training	O
the	O
retriever	O
with	O
CORGE	O
also	O
causes	O
the	O
retriever	O
scores	O
to	O
be	O
flattened	O
,	O
as	O
shown	O
in	O
Figure	O
4	O
,	O
and	O
we	O
empirically	O
observe	O
the	O
minor	O
collapse	O
of	O
the	O
retriever	O
as	O
we	O
experienced	O
in	O
RAG	B-MethodName
as	O
well	O
.	O
Thus	O
,	O
CORGE	O
does	O
not	O
jointly	O
train	O
the	O
retriever	O
.	O

For	O
automatic	O
metrics	O
,	O
we	O
calculate	O
the	O
metric	O
for	O
each	O
case	O
and	O
take	O
the	O
average	O
of	O
those	O
values	O
.	O
When	O
calculating	O
BLEU	B-MetricName
,	O
we	O
use	O
sentence_bleu	O
function	O
in	O
nltk	O
python	O
package	O
(	O
Loper	O
and	O
Bird	O
,	O
2002	O
)	O
.	O

We	O
investigate	O
the	O
utility	O
of	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
with	O
attention	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
to	O
generate	O
concrete	O
realizations	O
of	O
abstract	O
task	O
descriptions	O
.	O
We	O
hypothesize	O
that	O
models	O
that	O
learn	O
explicit	O
alignments	O
are	O
particularly	O
amenable	O
to	O
interpretable	O
analysis	O
on	O
the	O
task	O
.	O
Therefore	O
,	O
in	O
addition	O
to	O
using	O
the	O
global	O
attention	O
model	O
of	O
(	O
Luong	O
et	O
al	O
,	O
2015	O
)	O
,	O
we	O
adapt	O
the	O
transducer	O
model	O
proposed	O
by	O
Yu	O
et	O
al	O
(	O
2016	O
)	O
,	O
which	O
uses	O
learned	O
latent	O
discrete	O
variables	O
to	O
model	O
phraseto	O
-	O
phrase	O
alignments	O
.	O
In	O
contrast	O
to	O
many	O
standard	O
neural	O
models	O
,	O
this	O
approach	O
enables	O
us	O
to	O
incorporate	O
prior	O
knowledge	O
about	O
the	O
alignment	O
structure	O
,	O
and	O
to	O
extract	O
interpretable	O
alignments	O
between	O
task	O
phrases	O
.	O
Closely	O
related	O
architectures	O
have	O
been	O
proposed	O
for	O
segmental	O
sequence	O
modeling	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
and	O
phrase	O
-	O
based	O
neural	O
machine	B-TaskName
translation	I-TaskName
(	O
Huang	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
train	O
the	O
transducer	O
models	O
using	O
Viterbi	O
EM	B-MetricName
(	O
after	O
doing	O
marginal	O
likelihood	O
training	O
for	O
the	O
initial	O
iterations	O
)	O
,	O
as	O
we	O
found	O
it	O
gave	O
higher	O
predictive	O
accuracy	B-MetricName
than	O
marginal	O
likelihood	O
training	O
only	O
.	O
Following	O
Yu	O
et	O
al	O
(	O
2016	O
)	O
we	O
experiment	O
with	O
both	O
a	O
fixed	O
alignment	O
transition	O
probability	O
model	O
and	O
a	O
transition	O
model	O
with	O
a	O
neural	O
parameterization	O
.	O
Cloze	O
task	O
prediction	O
is	O
performed	O
greedily	O
.	O
3	O
At	O
each	O
slot	O
the	O
Viterbi	O
alignment	O
of	O
the	O
prefix	O
of	O
the	O
sequence	O
up	O
to	O
that	O
slot	O
is	O
computed	O
.	O
See	O
appendix	O
7	O
for	O
model	O
details	O
.	O
4	O
We	O
also	O
evaluate	O
the	O
performance	O
of	O
a	O
language	B-TaskName
modelling	I-TaskName
baseline	O
and	O
a	O
seq2seq	B-MethodName
model	O
without	O
attention	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
,	O
to	O
compare	O
the	O
effect	O
of	O
not	O
modeling	O
alignment	O
at	O
all	O
.	O
We	O
expect	O
all	O
the	O
models	O
to	O
implicitly	O
capture	O
aspects	O
of	O
world	O
knowledge	O
.	O
However	O
,	O
the	O
discrete	O
latent	O
variable	O
models	O
provide	O
Viterbi	O
alignments	O
over	O
the	O
training	O
data	O
,	O
from	O
which	O
we	O
can	O
compile	O
a	O
look	O
-	O
up	O
table	O
with	O
the	O
extracted	O
knowledge	O
.	O
In	O
neural	O
attention	O
models	O
,	O
this	O
knowledge	O
is	O
only	O
weakly	O
recoverable	O
:	O
extracting	O
information	O
requires	O
hand	O
tuning	O
attention	O
thresholds	O
and	O
there	O
is	O
no	O
direct	O
way	O
to	O
extract	O
contiguous	O
alignments	O
for	O
multi	O
-	O
word	O
phrases	O
.	O

During	O
generation	O
,	O
we	O
provide	O
the	O
model	O
with	O
the	O
number	O
of	O
words	O
in	O
each	O
blank	O
to	O
be	O
predicted	O
.	O
We	O
consider	O
two	O
setups	O
for	O
evaluating	O
examples	O
with	O
multiple	O
blanks	O
,	O
both	O
assuming	O
that	O
predictions	O
are	O
made	O
left	O
-	O
to	O
-	O
right	O
:	O
Oracle	O
,	O
where	O
the	O
gold	O
prediction	O
of	O
each	O
blank	O
is	O
fed	O
into	O
the	O
model	O
to	O
condition	O
on	O
for	O
future	O
predictions	O
,	O
and	O
Greedy	O
,	O
where	O
the	O
model	O
prediction	O
is	O
used	O
for	O
future	O
predictions	O
.	O
We	O
compute	O
the	O
proportion	O
of	O
exact	O
word	O
matches	O
over	O
each	O
blank	O
and	O
the	O
precision	O
of	O
the	O
top	O
k	B-HyperparameterName
=	I-HyperparameterName
5	O
predictions	O
for	O
both	O
setups	O
.	O
Additionally	O
we	O
compute	O
the	O
average	O
surprisal	O
of	O
the	O
gold	O
prediction	O
(	O
conditioning	O
on	O
oracle	O
predictions	O
)	O
.	O
The	O
surprisal	O
of	O
a	O
word	O
(	O
Attneave	O
,	O
1959	O
;	O
Hale	O
,	O
2001	O
)	O
is	O
its	O
negative	O
log	O
probability	O
under	O
the	O
model	O
:	O
−log	O
(	O
P	O
(	O
w	O
i	O
|	O
w	O
1	O
:	O
i−1	O
)	O
)	O
.	O
The	O
higher	O
the	O
probability	O
of	O
the	O
ground	O
truth	O
,	O
the	O
lower	O
the	O
model	O
's	O
"	O
surprise	O
"	O
at	O
seeing	O
it	O
in	O
that	O
context	O
.	O
Finally	O
,	O
as	O
a	O
quantitative	O
proxy	O
for	O
interpretability	O
,	O
we	O
report	O
the	O
length	O
of	O
the	O
transducer	O
models	O
'	O
average	O
Viterbi	O
alignment	O
span	O
:	O
our	O
goal	O
is	O
a	O
model	O
which	O
balances	O
low	O
average	O
alignment	O
lengths	O
and	O
high	O
matching	O
or	O
ranking	O
scores	O
.	O

We	O
report	O
results	O
on	O
the	O
prediction	O
task	O
in	O
Table	O
4	O
.	O
First	O
we	O
consider	O
models	O
trained	O
only	O
on	O
our	O
dataset	O
:	O
All	O
the	O
models	O
that	O
incorporate	O
a	O
notion	O
of	O
alignment	O
do	O
substantially	O
better	O
than	O
those	O
who	O
do	O
not	O
.	O
We	O
see	O
that	O
our	O
transducer	O
model	O
with	O
fixed	O
alignment	O
transition	O
probabilities	O
performs	O
best	O
in	O
terms	O
of	O
predictive	O
accuracy	B-MetricName
(	O
exact	B-MetricName
match	I-MetricName
and	O
top	O
-	O
5	O
precision	O
)	O
,	O
while	O
the	O
seqseq	O
model	O
with	O
attention	O
is	O
the	O
next	O
best	O
in	O
most	O
comparisons	O
.	O
The	O
model	O
with	O
parameterized	O
transitions	O
has	O
the	O
lowest	O
surprisal	O
though	O
,	O
as	O
it	O
is	O
more	O
confident	O
about	O
the	O
alignment	O
predictions	O
it	O
is	O
making	O
.	O
Using	O
average	O
alignment	O
length	O
to	O
quantify	O
whether	O
the	O
phrase	O
alignments	O
exhibit	O
desirable	O
structure	O
,	O
we	O
see	O
that	O
the	O
alignments	O
found	O
by	O
the	O
unparameterized	O
transition	O
model	O
(	O
average	O
length	O
6.18	O
)	O
are	O
significantly	O
shorter	O
than	O
those	O
of	O
the	O
parameterized	O
model	O
(	O
average	O
length	O
16.61	O
)	O
.	O
Investigation	O
shows	O
that	O
the	O
paramaterized	O
model	O
mostly	O
learns	O
degenerate	O
alignments	O
,	O
aligning	O
most	O
of	O
the	O
concrete	O
sequence	O
to	O
either	O
the	O
start	O
or	O
end	O
of	O
the	O
abstract	O
sentence	O
.	O
In	O
contrast	O
,	O
qualitative	O
analysis	O
of	O
the	O
unparameterized	O
transition	O
model	O
show	O
that	O
its	O
alignments	O
learn	O
desirable	O
correspondences	O
(	O
see	O
Figure	O
2	O
)	O
.	O
Therefore	O
among	O
our	O
proposed	O
models	O
(	O
trained	O
on	O
in	O
-	O
domain	O
data	O
only	O
)	O
the	O
transducer	O
with	O
unparameterized	O
transitions	O
satisfies	O
our	O
desiderata	O
of	O
displaying	O
both	O
good	O
predictive	O
power	O
for	O
word	O
generation	O
,	O
and	O
learning	O
interpretable	O
alignments	O
.	O
Given	O
the	O
recent	O
success	O
of	O
massively	O
pre	O
-	O
...	O
7	O
E	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
H	O
n	O
f	O
T	O
m	O
l	O
t	O
f	O
W	O
N	O
z	O
q	O
7	O
x	O
d	O
2	O
d	O
n	O
d	O
2	O
z	O
+	O
o	O
H	O
h	O
6	O
1	O
T	O
J	O
J	O
p	O
x	O
p	O
s	O
s	O
k	O
Y	O
n	O
u	O
h	O
N	O
R	O
w	O
K	O
R	O
R	O
v	O
o	O
k	O
D	O
J	O
O	O
6	O
n	O
m	O
N	O
A	O
4	O
l	O
b	O
4	O
f	O
j	O
2	O
5	O
n	O
f	O
f	O
u	O
L	O
a	O
i	O
E	O
Q	O
9	O
4	O
i	O
T	O
l	O
Q	O
U	O
y	O
H	O
S	O
k	O
S	O
C	O
U	O
b	O
T	O
S	O
g	O
+	O
u	O
6	O
/	O
W	O
r	O
N	O
c	O
7	O
0	B-DatasetName
5	O
y	O
C	O
r	O
x	O
C	O
1	O
K	O
D	O
A	O
o	O
1	O
+	O
9	O
a	O
s	O
3	O
S	O
F	O
g	O
W	O
c	O
4	O
V	O
M	O
U	O
m	O
O	O
6	O
v	O
p	O
d	O
i	O
k	O
F	O
O	O
N	O
g	O
k	O
k	O
+	O
r	O
f	O
Q	O
y	O
w	O
1	O
P	O
K	O
x	O
n	O
T	O
I	O
u	O
5	O
Y	O
q	O
G	O
n	O
M	O
T	O
5	O
P	O
N	O
T	O
p	O
+	O
T	O
M	O
K	O
g	O
M	O
S	O
J	O
d	O
q	O
W	O
Q	O
j	O
J	O
X	O
f	O
0	B-DatasetName
/	O
k	O
N	O
D	O
Z	O
m	O
E	O
o	O
e	O
2	O
M	O
6	O
Y	O
4	O
M	O
s	O
v	O
e	O
T	O
P	O
z	O
P	O
6	O
2	O
Y	O
Y	O
X	O
Q	O
e	O
5	O
U	O
G	O
m	O
G	O
X	O
L	O
H	O
F	O
o	O
i	O
i	O
7	O
E	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
H	O
n	O
f	O
T	O
m	O
l	O
t	O
f	O
W	O
N	O
z	O
q	O
7	O
x	O
d	O
2	O
d	O
n	O
d	O
2	O
z	O
+	O
o	O
H	O
h	O
6	O
1	O
T	O
J	O
J	O
p	O
x	O
p	O
s	O
s	O
k	O
Y	O
n	O
u	O
h	O
N	O
R	O
w	O
K	O
R	O
R	O
v	O
o	O
k	O
D	O
J	O
O	O
6	O
n	O
m	O
N	O
A	O
4	O
l	O
b	O
4	O
f	O
j	O
2	O
5	O
n	O
f	O
f	O
u	O
L	O
a	O
i	O
E	O
Q	O
9	O
4	O
i	O
T	O
l	O
Q	O
U	O
y	O
H	O
S	O
k	O
S	O
C	O
U	O
b	O
T	O
S	O
g	O
+	O
u	O
6	O
/	O
W	O
r	O
N	O
c	O
7	O
0	B-DatasetName
5	O
y	O
C	O
r	O
x	O
C	O
1	O
K	O
D	O
A	O
o	O
1	O
+	O
9	O
a	O
s	O
3	O
S	O
F	O
g	O
W	O
c	O
4	O
V	O
M	O
U	O
m	O
O	O
6	O
v	O
p	O
d	O
i	O
k	O
F	O
O	O
N	O
g	O
k	O
k	O
+	O
r	O
f	O
Q	O
y	O
w	O
1	O
P	O
K	O
x	O
n	O
T	O
I	O
u	O
5	O
Y	O
q	O
G	O
n	O
M	O
T	O
5	O
P	O
N	O
T	O
p	O
+	O
T	O
M	O
K	O
g	O
M	O
S	O
J	O
d	O
q	O
W	O
Q	O
j	O
J	O
X	O
f	O
0	B-DatasetName
/	O
k	O
N	O
D	O
Z	O
m	O
E	O
o	O
e	O
2	O
M	O
6	O
Y	O
4	O
M	O
s	O
v	O
e	O
T	O
P	O
z	O
P	O
6	O
2	O
Y	O
Y	O
X	O
Q	O
e	O
5	O
U	O
G	O
m	O
G	O
X	O
L	O
H	O
F	O
o	O
i	O
i	O
7	O
E	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
K	O
U	O
w	O
6	O
H	O
n	O
f	O
T	O
m	O
l	O
t	O
f	O
W	O
N	O
z	O
q	O
7	O
x	O
d	O
2	O
d	O
n	O
d	O
2	O
z	O
+	O
o	O
H	O
h	O
6	O
1	O
T	O
J	O
J	O
p	O
x	O
p	O
s	O
s	O
k	O
Y	O
n	O
u	O
h	O
N	O
R	O
w	O
K	O
R	O
R	O
v	O
o	O
k	O
D	O
J	O
O	O
6	O
n	O
m	O
N	O
A	O
4	O
l	O
b	O
4	O
f	O
j	O
2	O
5	O
n	O
f	O
f	O
u	O
L	O
a	O
i	O
E	O
Q	O
9	O
4	O
i	O
T	O
l	O
Q	O
U	O
y	O
H	O
S	O
k	O
S	O
C	O
U	O
b	O
T	O
S	O
g	O
+	O
u	O
6	O
/	O
W	O
r	O
N	O
c	O
7	O
0	B-DatasetName
5	O
y	O
C	O
r	O
x	O
C	O
1	O
K	O
D	O
A	O
o	O
1	O
+	O
9	O
a	O
s	O
3	O
S	O
F	O
g	O
W	O
c	O
4	O
V	O
M	O
U	O
m	O
O	O
6	O
v	O
p	O
d	O
i	O
k	O
F	O
O	O
N	O
g	O
k	O
k	O
+	O
r	O
f	O
Q	O
y	O
w	O
1	O
P	O
K	O
x	O
n	O
T	O
I	O
u	O
5	O
Y	O
q	O
G	O
n	O
M	O
T	O
5	O
P	O
N	O
T	O
p	O
+	O
T	O
M	O
K	O
g	O
M	O
S	O
J	O
d	O
q	O
W	O
Q	O
j	O
J	O
X	O
f	O
0	B-DatasetName
/	O
k	O
N	O
D	O
Z	O
m	O
E	O
o	O
e	O
2	O
M	O
6	O
Y	O
4	O
M	O
s	O
v	O
e	O
T	O
P	O
z	O
P	O
6	O
2	O
Y	O
Y	O
X	O
Q	O
e	O
5	O
U	O
G	O
m	O
G	O
X	O
L	O
H	O
F	O
o	O
i	O
i	O
7	O
G	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
A	O
X	O
X	O
x	O
v	O
O	O
+	O
n	O
d	O
L	O
a	O
+	O
s	O
b	O
m	O
V	O
n	O
m	O
7	O
s	O
r	O
O	O
7	O
t	O
3	O
/	O
g	O
H	O
h	O
6	O
1	O
d	O
J	O
I	O
p	O
h	O
k	O
2	O
W	O
i	O
E	O
R	O
1	O
Q	O
q	O
p	O
R	O
c	O
I	O
l	O
N	O
w	O
4	O
3	O
A	O
T	O
q	O
q	O
Q	O
x	O
q	O
H	O
A	O
d	O
j	O
i	O
+	O
n	O
f	O
n	O
t	O
J	O
1	O
S	O
a	O
J	O
/	O
L	O
R	O
T	O
F	O
I	O
M	O
Y	O
j	O
q	O
U	O
P	O
O	O
K	O
M	O
G	O
i	O
s	O
9	O
Y	O
N	O
/	O
r	O
u	O
1	O
W	O
v	O
5	O
s	O
1	O
B	O
V	O
o	O
l	O
f	O
k	O
C	O
o	O
U	O
a	O
P	O
T	O
d	O
r	O
9	O
4	O
g	O
Y	O
V	O
m	O
M	O
0	B-DatasetName
j	O
B	O
B	O
t	O
e	O
7	O
6	O
X	O
m	O
q	O
C	O
n	O
C	O
r	O
D	O
m	O
c	O
B	O
p	O
p	O
Z	O
d	O
p	O
T	O
C	O
k	O
b	O
0	B-DatasetName
y	O
F	O
2	O
L	O
Z	O
U	O
0	B-DatasetName
R	O
h	O
3	O
k	O
8	O
1	O
O	O
n	O
5	O
M	O
w	O
q	O
A	O
x	O
I	O
l	O
y	O
p	O
Y	O
0	B-DatasetName
Z	O
K	O
7	O
+	O
n	O
s	O
h	O
p	O
r	O
P	O
U	O
k	O
D	O
m	O
1	O
n	O
T	O
M	O
1	O
I	O
L	O
3	O
s	O
z	O
8	O
T	O
+	O
v	O
m	O
5	O
n	O
o	O
O	O
s	O
i	O
5	O
T	O
D	O
O	O
D	O
k	O
i	O
0	B-DatasetName
W	O
R	O
Z	O
k	O
g	O
J	O
i	O
G	O
z	O
v	O
8	O
m	O
A	O
K	O
2	O
R	O
G	O
T	O
C	O
y	O
h	O
T	O
H	O
F	O
7	O
K	O
2	O
E	O
j	O
q	O
i	O
g	O
z	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
R	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
O	O
0	B-DatasetName
r	O
j	O
Y	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
N	O
S	O
V	O
A	O
7	O
X	O
C	O
5	O
w	O
l	O
a	O
7	O
N	O
G	O
f	O
X	O
i	O
t	O
f	O
H	O
c	O
X	O
t	O
Z	O
N	O
8	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
o	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
0	B-DatasetName
G	O
P	O
R	O
i	O
8	O
e	O
K	O
9	O
g	O
P	O
a	O
U	O
D	O
b	O
b	O
S	O
b	O
t	O
0	B-DatasetName
s	O
w	O
m	O
7	O
G	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
A	O
X	O
X	O
x	O
v	O
O	O
+	O
n	O
d	O
L	O
a	O
+	O
s	O
b	O
m	O
V	O
n	O
m	O
7	O
s	O
r	O
O	O
7	O
t	O
3	O
/	O
g	O
H	O
h	O
6	O
1	O
d	O
J	O
I	O
p	O
h	O
k	O
2	O
W	O
i	O
E	O
R	O
1	O
Q	O
q	O
p	O
R	O
c	O
I	O
l	O
N	O
w	O
4	O
3	O
A	O
T	O
q	O
q	O
Q	O
x	O
q	O
H	O
A	O
d	O
j	O
i	O
+	O
n	O
f	O
n	O
t	O
J	O
1	O
S	O
a	O
J	O
/	O
L	O
R	O
T	O
F	O
I	O
M	O
Y	O
j	O
q	O
U	O
P	O
O	O
K	O
M	O
G	O
i	O
s	O
9	O
Y	O
N	O
/	O
r	O
u	O
1	O
W	O
v	O
5	O
s	O
1	O
B	O
V	O
o	O
l	O
f	O
k	O
C	O
o	O
U	O
a	O
P	O
T	O
d	O
r	O
9	O
4	O
g	O
Y	O
V	O
m	O
M	O
0	B-DatasetName
j	O
B	O
B	O
t	O
e	O
7	O
6	O
X	O
m	O
q	O
C	O
n	O
C	O
r	O
D	O
m	O
c	O
B	O
p	O
p	O
Z	O
d	O
p	O
T	O
C	O
k	O
b	O
0	B-DatasetName
y	O
F	O
2	O
L	O
Z	O
U	O
0	B-DatasetName
R	O
h	O
3	O
k	O
8	O
1	O
O	O
n	O
5	O
M	O
w	O
q	O
A	O
x	O
I	O
l	O
y	O
p	O
Y	O
0	B-DatasetName
Z	O
K	O
7	O
+	O
n	O
s	O
h	O
p	O
r	O
P	O
U	O
k	O
D	O
m	O
1	O
n	O
T	O
M	O
1	O
I	O
L	O
3	O
s	O
z	O
8	O
T	O
+	O
v	O
m	O
5	O
n	O
o	O
O	O
s	O
i	O
5	O
T	O
D	O
O	O
D	O
k	O
i	O
0	B-DatasetName
W	O
R	O
Z	O
k	O
g	O
J	O
i	O
G	O
z	O
v	O
8	O
m	O
A	O
K	O
2	O
R	O
G	O
T	O
C	O
y	O
h	O
T	O
H	O
F	O
7	O
K	O
2	O
E	O
j	O
q	O
i	O
g	O
z	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
R	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
O	O
0	B-DatasetName
r	O
j	O
Y	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
N	O
S	O
V	O
A	O
7	O
X	O
C	O
5	O
w	O
l	O
a	O
7	O
N	O
G	O
f	O
X	O
i	O
t	O
f	O
H	O
c	O
X	O
t	O
Z	O
N	O
8	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
o	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
0	B-DatasetName
G	O
P	O
R	O
i	O
8	O
e	O
K	O
9	O
g	O
P	O
a	O
U	O
D	O
b	O
b	O
S	O
b	O
t	O
0	B-DatasetName
s	O
w	O
m	O
7	O
G	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
A	O
X	O
X	O
x	O
v	O
O	O
+	O
n	O
d	O
L	O
a	O
+	O
s	O
b	O
m	O
V	O
n	O
m	O
7	O
s	O
r	O
O	O
7	O
t	O
3	O
/	O
g	O
H	O
h	O
6	O
1	O
d	O
J	O
I	O
p	O
h	O
k	O
2	O
W	O
i	O
E	O
R	O
1	O
Q	O
q	O
p	O
R	O
c	O
I	O
l	O
N	O
w	O
4	O
3	O
A	O
T	O
q	O
q	O
Q	O
x	O
q	O
H	O
A	O
d	O
j	O
i	O
+	O
n	O
f	O
n	O
t	O
J	O
1	O
S	O
a	O
J	O
/	O
L	O
R	O
T	O
F	O
I	O
M	O
Y	O
j	O
q	O
U	O
P	O
O	O
K	O
M	O
G	O
i	O
s	O
9	O
Y	O
N	O
/	O
r	O
u	O
1	O
W	O
v	O
5	O
s	O
1	O
B	O
V	O
o	O
l	O
f	O
k	O
C	O
o	O
U	O
a	O
P	O
T	O
d	O
r	O
9	O
4	O
g	O
Y	O
V	O
m	O
M	O
0	B-DatasetName
j	O
B	O
B	O
t	O
e	O
7	O
6	O
X	O
m	O
q	O
C	O
n	O
C	O
r	O
D	O
m	O
c	O
B	O
p	O
p	O
Z	O
d	O
p	O
T	O
C	O
k	O
b	O
0	B-DatasetName
y	O
F	O
2	O
L	O
Z	O
U	O
0	B-DatasetName
R	O
h	O
3	O
k	O
8	O
1	O
O	O
n	O
5	O
M	O
w	O
q	O
A	O
x	O
I	O
l	O
y	O
p	O
Y	O
0	B-DatasetName
Z	O
K	O
7	O
+	O
n	O
s	O
h	O
p	O
r	O
P	O
U	O
k	O
D	O
m	O
1	O
n	O
T	O
M	O
1	O
I	O
L	O
3	O
s	O
z	O
8	O
T	O
+	O
v	O
m	O
5	O
n	O
o	O
O	O
s	O
i	O
5	O
T	O
D	O
O	O
D	O
k	O
i	O
0	B-DatasetName
W	O
R	O
Z	O
k	O
g	O
J	O
i	O
G	O
z	O
v	O
8	O
m	O
A	O
K	O
2	O
R	O
G	O
T	O
C	O
y	O
h	O
T	O
H	O
F	O
7	O
K	O
2	O
E	O
j	O
q	O
i	O
g	O
z	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
R	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
O	O
0	B-DatasetName
r	O
j	O
Y	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
N	O
S	O
V	O
A	O
7	O
X	O
C	O
5	O
w	O
l	O
a	O
7	O
N	O
G	O
f	O
X	O
i	O
t	O
f	O
H	O
c	O
X	O
t	O
Z	O
N	O
8	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
o	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
0	B-DatasetName
G	O
P	O
R	O
i	O
8	O
e	O
K	O
9	O
g	O
P	O
a	O
U	O
D	O
b	O
b	O
S	O
b	O
t	O
0	B-DatasetName
s	O
w	O
m	O
7	O
G	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
A	O
X	O
X	O
x	O
v	O
O	O
+	O
n	O
d	O
L	O
a	O
+	O
s	O
b	O
m	O
V	O
n	O
m	O
7	O
s	O
r	O
O	O
7	O
t	O
3	O
/	O
g	O
H	O
h	O
6	O
1	O
d	O
J	O
I	O
p	O
h	O
k	O
2	O
W	O
i	O
E	O
R	O
1	O
Q	O
q	O
p	O
R	O
c	O
I	O
l	O
N	O
w	O
4	O
3	O
A	O
T	O
q	O
q	O
Q	O
x	O
q	O
H	O
A	O
d	O
j	O
i	O
+	O
n	O
f	O
n	O
t	O
J	O
1	O
S	O
a	O
J	O
/	O
L	O
R	O
T	O
F	O
I	O
M	O
Y	O
j	O
q	O
U	O
P	O
O	O
K	O
M	O
G	O
i	O
s	O
9	O
Y	O
N	O
/	O
r	O
u	O
1	O
W	O
v	O
5	O
s	O
1	O
B	O
V	O
o	O
l	O
f	O
k	O
C	O
o	O
U	O
a	O
P	O
T	O
d	O
r	O
9	O
4	O
g	O
Y	O
V	O
m	O
M	O
0	B-DatasetName
j	O
B	O
B	O
t	O
e	O
7	O
6	O
X	O
m	O
q	O
C	O
n	O
C	O
r	O
D	O
m	O
c	O
B	O
p	O
p	O
Z	O
d	O
p	O
T	O
C	O
k	O
b	O
0	B-DatasetName
y	O
F	O
2	O
L	O
Z	O
U	O
0	B-DatasetName
R	O
h	O
3	O
k	O
8	O
1	O
O	O
n	O
5	O
M	O
w	O
q	O
A	O
x	O
I	O
l	O
y	O
p	O
Y	O
0	B-DatasetName
Z	O
K	O
7	O
+	O
n	O
s	O
h	O
p	O
r	O
P	O
U	O
k	O
D	O
m	O
1	O
n	O
T	O
M	O
1	O
I	O
L	O
3	O
s	O
z	O
8	O
T	O
+	O
v	O
m	O
5	O
n	O
o	O
O	O
s	O
i	O
5	O
T	O
D	O
O	O
D	O
k	O
i	O
0	B-DatasetName
W	O
R	O
Z	O
k	O
g	O
J	O
i	O
G	O
z	O
v	O
8	O
m	O
A	O
K	O
2	O
R	O
G	O
T	O
C	O
y	O
h	O
T	O
H	O
F	O
7	O
K	O
2	O
E	O
j	O
q	O
i	O
g	O
z	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
R	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
O	O
0	B-DatasetName
r	O
j	O
Y	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
e6	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
2	O
M	O
f	O
j	O
m	O
J	O
D	O
q	O
M	O
2	O
1	O
J	O
4	O
r	O
d	O
2	O
G	O
a	O
r	O
P	O
c	O
w	O
F	O
A	O
s	O
o	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
q	O
s	O
e	O
i	O
F	O
4	O
8	O
V	O
7	O
Q	O
e	O
0	B-DatasetName
o	O
W	O
y	O
2	O
k	O
3	O
b	O
p	O
Z	O
h	O
N	O
2	O
N	O
0	B-DatasetName
I	O
J	O
/	O
Q	O
l	O
e	O
P	O
C	O
j	O
i	O
1	O
V	O
/	O
k	O
z	O
X	O
/	O
j	O
t	O
s	O
1	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
k	O
A	O
i	O
u	O
j	O
e	O
t	O
+	O
O	O
4	O
W	O
1	O
9	O
Y	O
3	O
N	O
r	O
e	O
J	O
2	O
a	O
W	O
d	O
3	O
b	O
/	O
+	O
g	O
f	O
H	O
j	O
U	O
0	B-DatasetName
n	O
G	O
q	O
G	O
D	O
Z	O
Z	O
L	O
G	O
L	O
V	O
C	O
a	O
h	O
G	O
w	O
S	O
U	O
2	O
D	O
T	O
c	O
C	O
O	O
4	O
l	O
C	O
G	O
g	O
U	O
C	O
2	O
8	O
H	O
4	O
d	O
u	O
a	O
3	O
n	O
1	O
B	O
p	O
H	O
s	O
t	O
H	O
M	O
0	B-DatasetName
n	O
Q	O
j	O
+	O
h	O
Q	O
8	O
p	O
A	O
z	O
a	O
q	O
z	O
0	B-DatasetName
g	O
P	O
1	O
a	O
v	O
1	O
x	O
x	O
q	O
+	O
4	O
c	O
Z	O
J	O
V	O
4	O
O	O
a	O
l	O
A	O
j	O
k	O
a	O
/	O
/	O
N	O
U	O
b	O
x	O
C	O
y	O
N	O
U	O
B	O
o	O
m	O
q	O
N	O
Z	O
d	O
z	O
0	B-DatasetName
2	O
M	O
n	O
1	O
F	O
l	O
O	O
B	O
M	O
4	O
L	O
f	O
V	O
S	O
j	O
Q	O
l	O
l	O
Y	O
z	O
r	O
E	O
r	O
q	O
W	O
S	O
R	O
q	O
j	O
9	O
b	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
D	O
C	O
W	O
N	O
m	O
S	O
h	O
s	O
z	O
V	O
3	O
x	O
M	O
Z	O
j	O
b	O
S	O
e	O
R	O
I	O
H	O
t	O
j	O
K	O
g	O
Z	O
6	O
W	O
V	O
v	O
J	O
v	O
7	O
n	O
d	O
V	O
M	O
T	O
X	O
v	O
s	O
Z	O
l	O
0	B-DatasetName
l	O
q	O
U	O
L	O
L	O
F	O
o	O
j	O
A	O
V	O
x	O
M	O
R	O
k	O
9	O
j	O
c	O
Z	O
c	O
I	O
X	O
M	O
i	O
I	O
k	O
l	O
l	O
C	O
l	O
u	O
b	O
y	O
V	O
s	O
R	O
B	O
V	O
l	O
x	O
q	O
Z	O
T	O
s	O
i	O
F	O
4	O
y	O
y	O
+	O
v	O
k	O
t	O
Z	O
F	O
1	O
X	O
O	O
r	O
3	O
v	O
1	O
l	O
p	O
X	O
6	O
T	O
x	O
1	O
G	O
E	O
E	O
z	O
i	O
F	O
c	O
/	O
D	O
g	O
C	O
u	O
p	O
w	O
B	O
w	O
1	O
o	O
A	O
o	O
M	O
h	O
P	O
M	O
M	O
r	O
v	O
D	O
n	O
C	O
e	O
X	O
H	O
e	O
n	O
Y	O
9	O
F	O
a	O
8	O
H	O
J	O
Z	O
4	O
7	O
h	O
D	O
5	O
z	O
P	O
H	O
/	O
Z	O
D	O
j	O
Z	O
I	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
2	O
M	O
f	O
j	O
m	O
J	O
D	O
q	O
M	O
2	O
1	O
J	O
4	O
r	O
d	O
2	O
G	O
a	O
r	O
P	O
c	O
w	O
F	O
A	O
s	O
o	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
q	O
s	O
e	O
i	O
F	O
4	O
8	O
V	O
7	O
Q	O
e	O
0	B-DatasetName
o	O
W	O
y	O
2	O
k	O
3	O
b	O
p	O
Z	O
h	O
N	O
2	O
N	O
0	B-DatasetName
I	O
J	O
/	O
Q	O
l	O
e	O
P	O
C	O
j	O
i	O
1	O
V	O
/	O
k	O
z	O
X	O
/	O
j	O
t	O
s	O
1	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
k	O
A	O
i	O
u	O
j	O
e	O
t	O
+	O
O	O
4	O
W	O
1	O
9	O
Y	O
3	O
N	O
r	O
e	O
J	O
2	O
a	O
W	O
d	O
3	O
b	O
/	O
+	O
g	O
f	O
H	O
j	O
U	O
0	B-DatasetName
n	O
G	O
q	O
G	O
D	O
Z	O
Z	O
L	O
G	O
L	O
V	O
C	O
a	O
h	O
G	O
w	O
S	O
U	O
2	O
D	O
T	O
c	O
C	O
O	O
4	O
l	O
C	O
G	O
g	O
U	O
C	O
2	O
8	O
H	O
4	O
d	O
u	O
a	O
3	O
n	O
1	O
B	O
p	O
H	O
s	O
t	O
H	O
M	O
0	B-DatasetName
n	O
Q	O
j	O
+	O
h	O
Q	O
8	O
p	O
A	O
z	O
a	O
q	O
z	O
0	B-DatasetName
g	O
P	O
1	O
a	O
v	O
1	O
x	O
x	O
q	O
+	O
4	O
c	O
Z	O
J	O
V	O
4	O
O	O
a	O
l	O
A	O
j	O
k	O
a	O
/	O
/	O
N	O
U	O
b	O
x	O
C	O
y	O
N	O
U	O
B	O
o	O
m	O
q	O
N	O
Z	O
d	O
z	O
0	B-DatasetName
2	O
M	O
n	O
1	O
F	O
l	O
O	O
B	O
M	O
4	O
L	O
f	O
V	O
S	O
j	O
Q	O
l	O
l	O
Y	O
z	O
r	O
E	O
r	O
q	O
W	O
S	O
R	O
q	O
j	O
9	O
b	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
D	O
C	O
W	O
N	O
m	O
S	O
h	O
s	O
z	O
V	O
3	O
x	O
M	O
Z	O
j	O
b	O
S	O
e	O
R	O
I	O
H	O
t	O
j	O
K	O
g	O
Z	O
6	O
W	O
V	O
v	O
J	O
v	O
7	O
n	O
d	O
V	O
M	O
T	O
X	O
v	O
s	O
Z	O
l	O
0	B-DatasetName
l	O
q	O
U	O
L	O
L	O
F	O
o	O
j	O
A	O
V	O
x	O
M	O
R	O
k	O
9	O
j	O
c	O
Z	O
c	O
I	O
X	O
M	O
i	O
I	O
k	O
l	O
l	O
C	O
l	O
u	O
b	O
y	O
V	O
s	O
R	O
B	O
V	O
l	O
x	O
q	O
Z	O
T	O
s	O
i	O
F	O
4	O
y	O
y	O
+	O
v	O
k	O
t	O
Z	O
F	O
1	O
X	O
O	O
r	O
3	O
v	O
1	O
l	O
p	O
X	O
6	O
T	O
x	O
1	O
G	O
E	O
E	O
z	O
i	O
F	O
c	O
/	O
D	O
g	O
C	O
u	O
p	O
w	O
B	O
w	O
1	O
o	O
A	O
o	O
M	O
h	O
P	O
M	O
M	O
r	O
v	O
D	O
n	O
C	O
e	O
X	O
H	O
e	O
n	O
Y	O
9	O
F	O
a	O
8	O
H	O
J	O
Z	O
4	O
7	O
h	O
D	O
5	O
z	O
P	O
H	O
/	O
Z	O
D	O
j	O
Z	O
I	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
2	O
M	O
f	O
j	O
m	O
J	O
D	O
q	O
M	O
2	O
1	O
J	O
4	O
r	O
d	O
2	O
G	O
a	O
r	O
P	O
c	O
w	O
F	O
A	O
s	O
o	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
q	O
s	O
e	O
i	O
F	O
4	O
8	O
V	O
7	O
Q	O
e	O
0	B-DatasetName
o	O
W	O
y	O
2	O
k	O
3	O
b	O
p	O
Z	O
h	O
N	O
2	O
N	O
0	B-DatasetName
I	O
J	O
/	O
Q	O
l	O
e	O
P	O
C	O
j	O
i	O
1	O
V	O
/	O
k	O
z	O
X	O
/	O
j	O
t	O
s	O
1	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
k	O
A	O
i	O
u	O
j	O
e	O
t	O
+	O
O	O
4	O
W	O
1	O
9	O
Y	O
3	O
N	O
r	O
e	O
J	O
2	O
a	O
W	O
d	O
3	O
b	O
/	O
+	O
g	O
f	O
H	O
j	O
U	O
0	B-DatasetName
n	O
G	O
q	O
G	O
D	O
Z	O
Z	O
L	O
G	O
L	O
V	O
C	O
a	O
h	O
G	O
w	O
S	O
U	O
2	O
D	O
T	O
c	O
C	O
O	O
4	O
l	O
C	O
G	O
g	O
U	O
C	O
2	O
8	O
H	O
4	O
d	O
u	O
a	O
3	O
n	O
1	O
B	O
p	O
H	O
s	O
t	O
H	O
M	O
0	B-DatasetName
n	O
Q	O
j	O
+	O
h	O
Q	O
8	O
p	O
A	O
z	O
a	O
q	O
z	O
0	B-DatasetName
g	O
P	O
1	O
a	O
v	O
1	O
x	O
x	O
q	O
+	O
4	O
c	O
Z	O
J	O
V	O
4	O
O	O
a	O
l	O
A	O
j	O
k	O
a	O
/	O
/	O
N	O
U	O
b	O
x	O
C	O
y	O
N	O
U	O
B	O
o	O
m	O
q	O
N	O
Z	O
d	O
z	O
0	B-DatasetName
2	O
M	O
n	O
1	O
F	O
l	O
O	O
B	O
M	O
4	O
L	O
f	O
V	O
S	O
j	O
Q	O
l	O
l	O
Y	O
z	O
r	O
E	O
r	O
q	O
W	O
S	O
R	O
q	O
j	O
9	O
b	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
D	O
C	O
W	O
N	O
m	O
S	O
h	O
s	O
z	O
V	O
3	O
x	O
M	O
Z	O
j	O
b	O
S	O
e	O
R	O
I	O
H	O
t	O
j	O
K	O
g	O
Z	O
6	O
W	O
V	O
v	O
J	O
v	O
7	O
n	O
d	O
V	O
M	O
T	O
X	O
v	O
s	O
Z	O
l	O
0	B-DatasetName
l	O
q	O
U	O
L	O
L	O
F	O
o	O
j	O
A	O
V	O
x	O
M	O
R	O
k	O
9	O
j	O
c	O
Z	O
c	O
I	O
X	O
M	O
i	O
I	O
k	O
l	O
l	O
C	O
l	O
u	O
b	O
y	O
V	O
s	O
R	O
B	O
V	O
l	O
x	O
q	O
Z	O
T	O
s	O
i	O
F	O
4	O
y	O
y	O
+	O
v	O
k	O
t	O
Z	O
F	O
1	O
X	O
O	O
r	O
3	O
v	O
1	O
l	O
p	O
X	O
6	O
T	O
x	O
1	O
G	O
E	O
E	O
z	O
i	O
F	O
c	O
/	O
D	O
g	O
C	O
u	O
p	O
w	O
B	O
w	O
1	O
o	O
A	O
o	O
M	O
h	O
P	O
M	O
M	O
r	O
v	O
D	O
n	O
C	O
e	O
X	O
H	O
e	O
n	O
Y	O
9	O
F	O
a	O
8	O
H	O
J	O
Z	O
4	O
7	O
h	O
D	O
5	O
z	O
P	O
H	O
/	O
Z	O
D	O
j	O
Z	O
I	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
2	O
M	O
f	O
j	O
m	O
J	O
D	O
q	O
M	O
2	O
1	O
J	O
4	O
r	O
d	O
2	O
G	O
a	O
r	O
P	O
c	O
w	O
F	O
A	O
s	O
o	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
q	O
s	O
e	O
i	O
F	O
4	O
8	O
V	O
7	O
Q	O
e	O
0	B-DatasetName
o	O
W	O
y	O
2	O
k	O
3	O
b	O
p	O
Z	O
h	O
N	O
2	O
N	O
0	B-DatasetName
I	O
J	O
/	O
Q	O
l	O
e	O
P	O
C	O
j	O
i	O
1	O
V	O
/	O
k	O
z	O
X	O
/	O
j	O
t	O
s	O
1	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
k	O
A	O
i	O
u	O
j	O
e	O
t	O
+	O
O	O
4	O
W	O
1	O
9	O
Y	O
3	O
N	O
r	O
e	O
J	O
2	O
a	O
W	O
d	O
3	O
b	O
/	O
+	O
g	O
f	O
H	O
j	O
U	O
0	B-DatasetName
n	O
G	O
q	O
G	O
D	O
Z	O
Z	O
L	O
G	O
L	O
V	O
C	O
a	O
h	O
G	O
w	O
S	O
U	O
2	O
D	O
T	O
c	O
C	O
O	O
4	O
l	O
C	O
G	O
g	O
U	O
C	O
2	O
8	O
H	O
4	O
d	O
u	O
a	O
3	O
n	O
1	O
B	O
p	O
H	O
s	O
t	O
H	O
M	O
0	B-DatasetName
n	O
Q	O
j	O
+	O
h	O
Q	O
8	O
p	O
A	O
z	O
a	O
q	O
z	O
0	B-DatasetName
g	O
P	O
1	O
a	O
v	O
1	O
x	O
x	O
q	O
+	O
4	O
c	O
Z	O
J	O
V	O
4	O
O	O
a	O
l	O
A	O
j	O
k	O
a	O
/	O
/	O
N	O
U	O
b	O
x	O
C	O
y	O
N	O
U	O
B	O
o	O
m	O
q	O
N	O
Z	O
d	O
z	O
0	B-DatasetName
2	O
M	O
n	O
1	O
F	O
l	O
O	O
B	O
M	O
4	O
L	O
f	O
V	O
S	O
j	O
Q	O
l	O
l	O
Y	O
z	O
r	O
E	O
r	O
q	O
W	O
S	O
R	O
q	O
j	O
9	O
b	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
D	O
C	O
W	O
N	O
m	O
S	O
h	O
s	O
z	O
V	O
3	O
x	O
M	O
Z	O
j	O
b	O
S	O
e	O
R	O
I	O
H	O
t	O
j	O
K	O
g	O
Z	O
6	O
W	O
V	O
v	O
J	O
v	O
7	O
n	O
d	O
V	O
M	O
T	O
X	O
v	O
s	O
Z	O
l	O
0	B-DatasetName
l	O
q	O
U	O
L	O
L	O
F	O
o	O
j	O
A	O
V	O
x	O
M	O
R	O
k	O
9	O
j	O
c	O
Z	O
c	O
I	O
X	O
M	O
i	O
I	O
k	O
l	O
l	O
C	O
l	O
u	O
b	O
y	O
V	O
s	O
R	O
B	O
V	O
l	O
x	O
q	O
Z	O
T	O
s	O
i	O
F	O
4	O
y	O
y	O
+	O
v	O
k	O
t	O
Z	O
F	O
1	O
X	O
O	O
r	O
3	O
v	O
1	O
l	O
p	O
X	O
6	O
T	O
x	O
1	O
G	O
E	O
E	O
z	O
i	O
F	O
c	O
/	O
D	O
g	O
C	O
u	O
p	O
w	O
B	O
w	O
1	O
o	O
A	O
o	O
M	O
h	O
P	O
M	O
M	O
r	O
v	O
D	O
n	O
C	O
e	O
X	O
H	O
e	O
n	O
Y	O
9	O
F	O
a	O
8	O
H	O
J	O
Z	O
4	O
7	O
h	O
D	O
5	O
z	O
P	O
H	O
/	O
Z	O
D	O
j	O
Z	O
I	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
...	O
7	O
G	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
A	O
X	O
X	O
x	O
v	O
O	O
+	O
n	O
d	O
L	O
a	O
+	O
s	O
b	O
m	O
V	O
n	O
m	O
7	O
s	O
r	O
O	O
7	O
t	O
3	O
/	O
g	O
H	O
h	O
6	O
1	O
d	O
J	O
I	O
p	O
h	O
k	O
2	O
W	O
i	O
E	O
R	O
1	O
Q	O
q	O
p	O
R	O
c	O
I	O
l	O
N	O
w	O
4	O
3	O
A	O
T	O
q	O
q	O
Q	O
x	O
q	O
H	O
A	O
d	O
j	O
i	O
+	O
n	O
f	O
n	O
t	O
J	O
1	O
S	O
a	O
J	O
/	O
L	O
R	O
T	O
F	O
I	O
M	O
Y	O
j	O
q	O
U	O
P	O
O	O
K	O
M	O
G	O
i	O
s	O
9	O
Y	O
N	O
/	O
r	O
u	O
1	O
W	O
v	O
5	O
s	O
1	O
B	O
V	O
o	O
l	O
f	O
k	O
C	O
o	O
U	O
a	O
P	O
T	O
d	O
r	O
9	O
4	O
g	O
Y	O
V	O
m	O
M	O
0	B-DatasetName
j	O
B	O
B	O
t	O
e	O
7	O
6	O
X	O
m	O
q	O
C	O
n	O
C	O
r	O
D	O
m	O
c	O
B	O
p	O
p	O
Z	O
d	O
p	O
T	O
C	O
k	O
b	O
0	B-DatasetName
y	O
F	O
2	O
L	O
Z	O
U	O
0	B-DatasetName
R	O
h	O
3	O
k	O
8	O
1	O
O	O
n	O
5	O
M	O
w	O
q	O
A	O
x	O
I	O
l	O
y	O
p	O
Y	O
0	B-DatasetName
Z	O
K	O
7	O
+	O
n	O
s	O
h	O
p	O
r	O
P	O
U	O
k	O
D	O
m	O
1	O
n	O
T	O
M	O
1	O
I	O
L	O
3	O
s	O
z	O
8	O
T	O
+	O
v	O
m	O
5	O
n	O
o	O
O	O
s	O
i	O
5	O
T	O
D	O
O	O
D	O
k	O
i	O
0	B-DatasetName
W	O
R	O
Z	O
k	O
g	O
J	O
i	O
G	O
z	O
v	O
8	O
m	O
A	O
K	O
2	O
R	O
G	O
T	O
C	O
y	O
h	O
T	O
H	O
F	O
7	O
K	O
2	O
E	O
j	O
q	O
i	O
g	O
z	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
R	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
O	O
0	B-DatasetName
r	O
j	O
Y	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
N	O
S	O
V	O
A	O
7	O
X	O
C	O
5	O
w	O
l	O
a	O
7	O
N	O
G	O
f	O
X	O
i	O
t	O
f	O
H	O
c	O
X	O
t	O
Z	O
N	O
8	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
o	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
0	B-DatasetName
G	O
P	O
R	O
i	O
8	O
e	O
K	O
9	O
g	O
P	O
a	O
U	O
D	O
b	O
b	O
S	O
b	O
t	O
0	B-DatasetName
s	O
w	O
m	O
7	O
G	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
A	O
X	O
X	O
x	O
v	O
O	O
+	O
n	O
d	O
L	O
a	O
+	O
s	O
b	O
m	O
V	O
n	O
m	O
7	O
s	O
r	O
O	O
7	O
t	O
3	O
/	O
g	O
H	O
h	O
6	O
1	O
d	O
J	O
I	O
p	O
h	O
k	O
2	O
W	O
i	O
E	O
R	O
1	O
Q	O
q	O
p	O
R	O
c	O
I	O
l	O
N	O
w	O
4	O
3	O
A	O
T	O
q	O
q	O
Q	O
x	O
q	O
H	O
A	O
d	O
j	O
i	O
+	O
n	O
f	O
n	O
t	O
J	O
1	O
S	O
a	O
J	O
/	O
L	O
R	O
T	O
F	O
I	O
M	O
Y	O
j	O
q	O
U	O
P	O
O	O
K	O
M	O
G	O
i	O
s	O
9	O
Y	O
N	O
/	O
r	O
u	O
1	O
W	O
v	O
5	O
s	O
1	O
B	O
V	O
o	O
l	O
f	O
k	O
C	O
o	O
U	O
a	O
P	O
T	O
d	O
r	O
9	O
4	O
g	O
Y	O
V	O
m	O
M	O
0	B-DatasetName
j	O
B	O
B	O
t	O
e	O
7	O
6	O
X	O
m	O
q	O
C	O
n	O
C	O
r	O
D	O
m	O
c	O
B	O
p	O
p	O
Z	O
d	O
p	O
T	O
C	O
k	O
b	O
0	B-DatasetName
y	O
F	O
2	O
L	O
Z	O
U	O
0	B-DatasetName
R	O
h	O
3	O
k	O
8	O
1	O
O	O
n	O
5	O
M	O
w	O
q	O
A	O
x	O
I	O
l	O
y	O
p	O
Y	O
0	B-DatasetName
Z	O
K	O
7	O
+	O
n	O
s	O
h	O
p	O
r	O
P	O
U	O
k	O
D	O
m	O
1	O
n	O
T	O
M	O
1	O
I	O
L	O
3	O
s	O
z	O
8	O
T	O
+	O
v	O
m	O
5	O
n	O
o	O
O	O
s	O
i	O
5	O
T	O
D	O
O	O
D	O
k	O
i	O
0	B-DatasetName
W	O
R	O
Z	O
k	O
g	O
J	O
i	O
G	O
z	O
v	O
8	O
m	O
A	O
K	O
2	O
R	O
G	O
T	O
C	O
y	O
h	O
T	O
H	O
F	O
7	O
K	O
2	O
E	O
j	O
q	O
i	O
g	O
z	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
R	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
O	O
0	B-DatasetName
r	O
j	O
Y	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
N	O
S	O
V	O
A	O
7	O
X	O
C	O
5	O
w	O
l	O
a	O
7	O
N	O
G	O
f	O
X	O
i	O
t	O
f	O
H	O
c	O
X	O
t	O
Z	O
N	O
8	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
o	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
0	B-DatasetName
G	O
P	O
R	O
i	O
8	O
e	O
K	O
9	O
g	O
P	O
a	O
U	O
D	O
b	O
b	O
S	O
b	O
t	O
0	B-DatasetName
s	O
w	O
m	O
7	O
G	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
A	O
X	O
X	O
x	O
v	O
O	O
+	O
n	O
d	O
L	O
a	O
+	O
s	O
b	O
m	O
V	O
n	O
m	O
7	O
s	O
r	O
O	O
7	O
t	O
3	O
/	O
g	O
H	O
h	O
6	O
1	O
d	O
J	O
I	O
p	O
h	O
k	O
2	O
W	O
i	O
E	O
R	O
1	O
Q	O
q	O
p	O
R	O
c	O
I	O
l	O
N	O
w	O
4	O
3	O
A	O
T	O
q	O
q	O
Q	O
x	O
q	O
H	O
A	O
d	O
j	O
i	O
+	O
n	O
f	O
n	O
t	O
J	O
1	O
S	O
a	O
J	O
/	O
L	O
R	O
T	O
F	O
I	O
M	O
Y	O
j	O
q	O
U	O
P	O
O	O
K	O
M	O
G	O
i	O
s	O
9	O
Y	O
N	O
/	O
r	O
u	O
1	O
W	O
v	O
5	O
s	O
1	O
B	O
V	O
o	O
l	O
f	O
k	O
C	O
o	O
U	O
a	O
P	O
T	O
d	O
r	O
9	O
4	O
g	O
Y	O
V	O
m	O
M	O
0	B-DatasetName
j	O
B	O
B	O
t	O
e	O
7	O
6	O
X	O
m	O
q	O
C	O
n	O
C	O
r	O
D	O
m	O
c	O
B	O
p	O
p	O
Z	O
d	O
p	O
T	O
C	O
k	O
b	O
0	B-DatasetName
y	O
F	O
2	O
L	O
Z	O
U	O
0	B-DatasetName
R	O
h	O
3	O
k	O
8	O
1	O
O	O
n	O
5	O
M	O
w	O
q	O
A	O
x	O
I	O
l	O
y	O
p	O
Y	O
0	B-DatasetName
Z	O
K	O
7	O
+	O
n	O
s	O
h	O
p	O
r	O
P	O
U	O
k	O
D	O
m	O
1	O
n	O
T	O
M	O
1	O
I	O
L	O
3	O
s	O
z	O
8	O
T	O
+	O
v	O
m	O
5	O
n	O
o	O
O	O
s	O
i	O
5	O
T	O
D	O
O	O
D	O
k	O
i	O
0	B-DatasetName
W	O
R	O
Z	O
k	O
g	O
J	O
i	O
G	O
z	O
v	O
8	O
m	O
A	O
K	O
2	O
R	O
G	O
T	O
C	O
y	O
h	O
T	O
H	O
F	O
7	O
K	O
2	O
E	O
j	O
q	O
i	O
g	O
z	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
R	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
O	O
0	B-DatasetName
r	O
j	O
Y	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
N	O
S	O
V	O
A	O
7	O
X	O
C	O
5	O
w	O
l	O
a	O
7	O
N	O
G	O
f	O
X	O
i	O
t	O
f	O
H	O
c	O
X	O
t	O
Z	O
N	O
8	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
o	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
0	B-DatasetName
G	O
P	O
R	O
i	O
8	O
e	O
K	O
9	O
g	O
P	O
a	O
U	O
D	O
b	O
b	O
S	O
b	O
t	O
0	B-DatasetName
s	O
w	O
m	O
7	O
G	O
6	O
G	O
E	O
/	O
g	O
Q	O
v	O
H	O
h	O
T	O
x	O
6	O
i	O
/	O
y	O
5	O
r	O
9	O
x	O
2	O
+	O
a	O
g	O
r	O
Q	O
8	O
G	O
H	O
u	O
/	O
N	O
M	O
D	O
M	O
v	O
T	O
A	O
X	O
X	O
x	O
v	O
O	O
+	O
n	O
d	O
L	O
a	O
+	O
s	O
b	O
m	O
V	O
n	O
m	O
7	O
s	O
r	O
O	O
7	O
t	O
3	O
/	O
g	O
H	O
h	O
6	O
1	O
d	O
J	O
I	O
p	O
h	O
k	O
2	O
W	O
i	O
E	O
R	O
1	O
Q	O
q	O
p	O
R	O
c	O
I	O
l	O
N	O
w	O
4	O
3	O
A	O
T	O
q	O
q	O
Q	O
x	O
q	O
H	O
A	O
d	O
j	O
i	O
+	O
n	O
f	O
n	O
t	O
J	O
1	O
S	O
a	O
J	O
/	O
L	O
R	O
T	O
F	O
I	O
M	O
Y	O
j	O
q	O
U	O
P	O
O	O
K	O
M	O
G	O
i	O
s	O
9	O
Y	O
N	O
/	O
r	O
u	O
1	O
W	O
v	O
5	O
s	O
1	O
B	O
V	O
o	O
l	O
f	O
k	O
C	O
o	O
U	O
a	O
P	O
T	O
d	O
r	O
9	O
4	O
g	O
Y	O
V	O
m	O
M	O
0	B-DatasetName
j	O
B	O
B	O
t	O
e	O
7	O
6	O
X	O
m	O
q	O
C	O
n	O
C	O
r	O
D	O
m	O
c	O
B	O
p	O
p	O
Z	O
d	O
p	O
T	O
C	O
k	O
b	O
0	B-DatasetName
y	O
F	O
2	O
L	O
Z	O
U	O
0	B-DatasetName
R	O
h	O
3	O
k	O
8	O
1	O
O	O
n	O
5	O
M	O
w	O
q	O
A	O
x	O
I	O
l	O
y	O
p	O
Y	O
0	B-DatasetName
Z	O
K	O
7	O
+	O
n	O
s	O
h	O
p	O
r	O
P	O
U	O
k	O
D	O
m	O
1	O
n	O
T	O
M	O
1	O
I	O
L	O
3	O
s	O
z	O
8	O
T	O
+	O
v	O
m	O
5	O
n	O
o	O
O	O
s	O
i	O
5	O
T	O
D	O
O	O
D	O
k	O
i	O
0	B-DatasetName
W	O
R	O
Z	O
k	O
g	O
J	O
i	O
G	O
z	O
v	O
8	O
m	O
A	O
K	O
2	O
R	O
G	O
T	O
C	O
y	O
h	O
T	O
H	O
F	O
7	O
K	O
2	O
E	O
j	O
q	O
i	O
g	O
z	O
N	O
p	O
2	O
K	O
D	O
c	O
F	O
f	O
f	O
n	O
m	O
V	O
t	O
C	O
5	O
q	O
v	O
l	O
f	O
z	O
7	O
y	O
+	O
r	O
9	O
Z	O
s	O
i	O
j	O
j	O
K	O
c	O
w	O
C	O
m	O
c	O
g	O
w	O
9	O
X	O
U	O
I	O
c	O
7	O
a	O
E	O
A	O
T	O
G	O
A	O
z	O
h	O
G	O
V	O
7	O
h	O
z	O
R	O
H	O
O	O
i	O
/	O
P	O
u	O
f	O
C	O
x	O
a	O
S	O
0	B-DatasetName
4	O
x	O
c	O
w	O
x	O
/	O
4	O
H	O
z	O
+	O
A	O
O	O
0	B-DatasetName
r	O
j	O
Y	O
w	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
e6	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
2	O
M	O
f	O
j	O
m	O
J	O
D	O
q	O
M	O
2	O
1	O
J	O
4	O
r	O
d	O
2	O
G	O
a	O
r	O
P	O
c	O
w	O
F	O
A	O
s	O
o	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
q	O
s	O
e	O
i	O
F	O
4	O
8	O
V	O
7	O
Q	O
e	O
0	B-DatasetName
o	O
W	O
y	O
2	O
k	O
3	O
b	O
p	O
Z	O
h	O
N	O
2	O
N	O
0	B-DatasetName
I	O
J	O
/	O
Q	O
l	O
e	O
P	O
C	O
j	O
i	O
1	O
V	O
/	O
k	O
z	O
X	O
/	O
j	O
t	O
s	O
1	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
k	O
A	O
i	O
u	O
j	O
e	O
t	O
+	O
O	O
4	O
W	O
1	O
9	O
Y	O
3	O
N	O
r	O
e	O
J	O
2	O
a	O
W	O
d	O
3	O
b	O
/	O
+	O
g	O
f	O
H	O
j	O
U	O
0	B-DatasetName
n	O
G	O
q	O
G	O
D	O
Z	O
Z	O
L	O
G	O
L	O
V	O
C	O
a	O
h	O
G	O
w	O
S	O
U	O
2	O
D	O
T	O
c	O
C	O
O	O
4	O
l	O
C	O
G	O
g	O
U	O
C	O
2	O
8	O
H	O
4	O
d	O
u	O
a	O
3	O
n	O
1	O
B	O
p	O
H	O
s	O
t	O
H	O
M	O
0	B-DatasetName
n	O
Q	O
j	O
+	O
h	O
Q	O
8	O
p	O
A	O
z	O
a	O
q	O
z	O
0	B-DatasetName
g	O
P	O
1	O
a	O
v	O
1	O
x	O
x	O
q	O
+	O
4	O
c	O
Z	O
J	O
V	O
4	O
O	O
a	O
l	O
A	O
j	O
k	O
a	O
/	O
/	O
N	O
U	O
b	O
x	O
C	O
y	O
N	O
U	O
B	O
o	O
m	O
q	O
N	O
Z	O
d	O
z	O
0	B-DatasetName
2	O
M	O
n	O
1	O
F	O
l	O
O	O
B	O
M	O
4	O
L	O
f	O
V	O
S	O
j	O
Q	O
l	O
l	O
Y	O
z	O
r	O
E	O
r	O
q	O
W	O
S	O
R	O
q	O
j	O
9	O
b	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
D	O
C	O
W	O
N	O
m	O
S	O
h	O
s	O
z	O
V	O
3	O
x	O
M	O
Z	O
j	O
b	O
S	O
e	O
R	O
I	O
H	O
t	O
j	O
K	O
g	O
Z	O
6	O
W	O
V	O
v	O
J	O
v	O
7	O
n	O
d	O
V	O
M	O
T	O
X	O
v	O
s	O
Z	O
l	O
0	B-DatasetName
l	O
q	O
U	O
L	O
L	O
F	O
o	O
j	O
A	O
V	O
x	O
M	O
R	O
k	O
9	O
j	O
c	O
Z	O
c	O
I	O
X	O
M	O
i	O
I	O
k	O
l	O
l	O
C	O
l	O
u	O
b	O
y	O
V	O
s	O
R	O
B	O
V	O
l	O
x	O
q	O
Z	O
T	O
s	O
i	O
F	O
4	O
y	O
y	O
+	O
v	O
k	O
t	O
Z	O
F	O
1	O
X	O
O	O
r	O
3	O
v	O
1	O
l	O
p	O
X	O
6	O
T	O
x	O
1	O
G	O
E	O
E	O
z	O
i	O
F	O
c	O
/	O
D	O
g	O
C	O
u	O
p	O
w	O
B	O
w	O
1	O
o	O
A	O
o	O
M	O
h	O
P	O
M	O
M	O
r	O
v	O
D	O
n	O
C	O
e	O
X	O
H	O
e	O
n	O
Y	O
9	O
F	O
a	O
8	O
H	O
J	O
Z	O
4	O
7	O
h	O
D	O
5	O
z	O
P	O
H	O
/	O
Z	O
D	O
j	O
Z	O
I	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
2	O
M	O
f	O
j	O
m	O
J	O
D	O
q	O
M	O
2	O
1	O
J	O
4	O
r	O
d	O
2	O
G	O
a	O
r	O
P	O
c	O
w	O
F	O
A	O
s	O
o	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
q	O
s	O
e	O
i	O
F	O
4	O
8	O
V	O
7	O
Q	O
e	O
0	B-DatasetName
o	O
W	O
y	O
2	O
k	O
3	O
b	O
p	O
Z	O
h	O
N	O
2	O
N	O
0	B-DatasetName
I	O
J	O
/	O
Q	O
l	O
e	O
P	O
C	O
j	O
i	O
1	O
V	O
/	O
k	O
z	O
X	O
/	O
j	O
t	O
s	O
1	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
k	O
A	O
i	O
u	O
j	O
e	O
t	O
+	O
O	O
4	O
W	O
1	O
9	O
Y	O
3	O
N	O
r	O
e	O
J	O
2	O
a	O
W	O
d	O
3	O
b	O
/	O
+	O
g	O
f	O
H	O
j	O
U	O
0	B-DatasetName
n	O
G	O
q	O
G	O
D	O
Z	O
Z	O
L	O
G	O
L	O
V	O
C	O
a	O
h	O
G	O
w	O
S	O
U	O
2	O
D	O
T	O
c	O
C	O
O	O
4	O
l	O
C	O
G	O
g	O
U	O
C	O
2	O
8	O
H	O
4	O
d	O
u	O
a	O
3	O
n	O
1	O
B	O
p	O
H	O
s	O
t	O
H	O
M	O
0	B-DatasetName
n	O
Q	O
j	O
+	O
h	O
Q	O
8	O
p	O
A	O
z	O
a	O
q	O
z	O
0	B-DatasetName
g	O
P	O
1	O
a	O
v	O
1	O
x	O
x	O
q	O
+	O
4	O
c	O
Z	O
J	O
V	O
4	O
O	O
a	O
l	O
A	O
j	O
k	O
a	O
/	O
/	O
N	O
U	O
b	O
x	O
C	O
y	O
N	O
U	O
B	O
o	O
m	O
q	O
N	O
Z	O
d	O
z	O
0	B-DatasetName
2	O
M	O
n	O
1	O
F	O
l	O
O	O
B	O
M	O
4	O
L	O
f	O
V	O
S	O
j	O
Q	O
l	O
l	O
Y	O
z	O
r	O
E	O
r	O
q	O
W	O
S	O
R	O
q	O
j	O
9	O
b	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
D	O
C	O
W	O
N	O
m	O
S	O
h	O
s	O
z	O
V	O
3	O
x	O
M	O
Z	O
j	O
b	O
S	O
e	O
R	O
I	O
H	O
t	O
j	O
K	O
g	O
Z	O
6	O
W	O
V	O
v	O
J	O
v	O
7	O
n	O
d	O
V	O
M	O
T	O
X	O
v	O
s	O
Z	O
l	O
0	B-DatasetName
l	O
q	O
U	O
L	O
L	O
F	O
o	O
j	O
A	O
V	O
x	O
M	O
R	O
k	O
9	O
j	O
c	O
Z	O
c	O
I	O
X	O
M	O
i	O
I	O
k	O
l	O
l	O
C	O
l	O
u	O
b	O
y	O
V	O
s	O
R	O
B	O
V	O
l	O
x	O
q	O
Z	O
T	O
s	O
i	O
F	O
4	O
y	O
y	O
+	O
v	O
k	O
t	O
Z	O
F	O
1	O
X	O
O	O
r	O
3	O
v	O
1	O
l	O
p	O
X	O
6	O
T	O
x	O
1	O
G	O
E	O
E	O
z	O
i	O
F	O
c	O
/	O
D	O
g	O
C	O
u	O
p	O
w	O
B	O
w	O
1	O
o	O
A	O
o	O
M	O
h	O
P	O
M	O
M	O
r	O
v	O
D	O
n	O
C	O
e	O
X	O
H	O
e	O
n	O
Y	O
9	O
F	O
a	O
8	O
H	O
J	O
Z	O
4	O
7	O
h	O
D	O
5	O
z	O
P	O
H	O
/	O
Z	O
D	O
j	O
Z	O
I	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
1	O
2	O
M	O
f	O
j	O
m	O
J	O
D	O
q	O
M	O
2	O
1	O
J	O
4	O
r	O
d	O
2	O
G	O
a	O
r	O
P	O
c	O
w	O
F	O
A	O
s	O
o	O
=	O
"	O
>	O
A	O
A	O
A	O
B	O
6	O
n	O
i	O
c	O
b	O
V	O
B	O
N	O
S	O
8	O
N	O
A	O
E	O
J	O
3	O
U	O
r	O
1	O
q	O
/	O
q	O
h	O
6	O
9	O
L	O
B	O
b	O
B	O
U	O
0	B-DatasetName
l	O
E	O
q	O
s	O
e	O
i	O
F	O
4	O
8	O
V	O
7	O
Q	O
e	O
0	B-DatasetName
o	O
W	O
y	O
2	O
k	O
3	O
b	O
p	O
Z	O
h	O
N	O
2	O
N	O
0	B-DatasetName
I	O
J	O
/	O
Q	O
l	O
e	O
P	O
C	O
j	O
i	O
1	O
V	O
/	O
k	O
z	O
X	O
/	O
j	O
t	O
s	O
1	O
B	O
W	O
x	O
8	O
M	O
P	O
N	O
6	O
b	O
Y	O
W	O
Z	O
e	O
k	O
A	O
i	O
u	O
j	O
e	O
t	O
+	O
O	O
4	O
W	O
1	O
9	O
Y	O
3	O
N	O
r	O
e	O
J	O
2	O
a	O
W	O
d	O
3	O
b	O
/	O
+	O
g	O
f	O
H	O
j	O
U	O
0	B-DatasetName
n	O
G	O
q	O
G	O
D	O
Z	O
Z	O
L	O
G	O
L	O
V	O
C	O
a	O
h	O
G	O
w	O
S	O
U	O
2	O
D	O
T	O
c	O
C	O
O	O
4	O
l	O
C	O
G	O
g	O
U	O
C	O
2	O
8	O
H	O
4	O
d	O
u	O
a	O
3	O
n	O
1	O
B	O
p	O
H	O
s	O
t	O
H	O
M	O
0	B-DatasetName
n	O
Q	O
j	O
+	O
h	O
Q	O
8	O
p	O
A	O
z	O
a	O
q	O
z	O
0	B-DatasetName
g	O
P	O
1	O
a	O
v	O
1	O
x	O
x	O
q	O
+	O
4	O
c	O
Z	O
J	O
V	O
4	O
O	O
a	O
l	O
A	O
j	O
k	O
a	O
/	O
/	O
N	O
U	O
b	O
x	O
C	O
y	O
N	O
U	O
B	O
o	O
m	O
q	O
N	O
Z	O
d	O
z	O
0	B-DatasetName
2	O
M	O
n	O
1	O
F	O
l	O
O	O
B	O
M	O
4	O
L	O
f	O
V	O
S	O
j	O
Q	O
l	O
l	O
Y	O
z	O
r	O
E	O
r	O
q	O
W	O
S	O
R	O
q	O
j	O
9	O
b	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
D	O
C	O
W	O
N	O
m	O
S	O
h	O
s	O
z	O
V	O
3	O
x	O
M	O
Z	O
j	O
b	O
S	O
e	O
R	O
I	O
H	O
t	O
j	O
K	O
g	O
Z	O
6	O
W	O
V	O
v	O
J	O
v	O
7	O
n	O
d	O
V	O
M	O
T	O
X	O
v	O
s	O
Z	O
l	O
0	B-DatasetName
l	O
q	O
U	O
L	O
L	O
F	O
o	O
j	O
A	O
V	O
x	O
M	O
R	O
k	O
9	O
j	O
c	O
Z	O
c	O
I	O
X	O
M	O
i	O
I	O
k	O
l	O
l	O
C	O
l	O
u	O
b	O
y	O
V	O
s	O
R	O
B	O
V	O
l	O
x	O
q	O
Z	O
T	O
s	O
i	O
F	O
4	O
y	O
y	O
+	O
v	O
k	O
t	O
Z	O
F	O
1	O
X	O
O	O
r	O
3	O
v	O
1	O
l	O
p	O
X	O
6	O
T	O
x	O
1	O
G	O
E	O
E	O
z	O
i	O
F	O
c	O
/	O
D	O
g	O
C	O
u	O
p	O
w	O
B	O
w	O
1	O
o	O
A	O
o	O
M	O
h	O
P	O
M	O
M	O
r	O
v	O
D	O
n	O
C	O
e	O
X	O
H	O
/	O
+	O
g	O
f	O
H	O
j	O
U	O
0	B-DatasetName
n	O
G	O
q	O
G	O
D	O
Z	O
Z	O
L	O
G	O
L	O
V	O
C	O
a	O
h	O
G	O
w	O
S	O
U	O
2	O
D	O
T	O
c	O
C	O
O	O
4	O
l	O
C	O
G	O
g	O
U	O
C	O
2	O
8	O
H	O
4	O
d	O
u	O
a	O
3	O
n	O
1	O
B	O
p	O
H	O
s	O
t	O
H	O
M	O
0	B-DatasetName
n	O
Q	O
j	O
+	O
h	O
Q	O
8	O
p	O
A	O
z	O
a	O
q	O
z	O
0	B-DatasetName
g	O
P	O
1	O
a	O
v	O
1	O
x	O
x	O
q	O
+	O
4	O
c	O
Z	O
J	O
V	O
4	O
O	O
a	O
l	O
A	O
j	O
k	O
a	O
/	O
/	O
N	O
U	O
b	O
x	O
C	O
y	O
N	O
U	O
B	O
o	O
m	O
q	O
N	O
Z	O
d	O
z	O
0	B-DatasetName
2	O
M	O
n	O
1	O
F	O
l	O
O	O
B	O
M	O
4	O
L	O
f	O
V	O
S	O
j	O
Q	O
l	O
l	O
Y	O
z	O
r	O
E	O
r	O
q	O
W	O
S	O
R	O
q	O
j	O
9	O
b	O
H	O
7	O
q	O
l	O
J	O
x	O
Z	O
Z	O
U	O
D	O
C	O
W	O
N	O
m	O
S	O
h	O
s	O
z	O
V	O
3	O
x	O
M	O
Z	O
j	O
b	O
S	O
e	O
R	O
I	O
H	O
t	O
j	O
K	O
g	O
Z	O
6	O
W	O
V	O
v	O
J	O
v	O
7	O
n	O
d	O
V	O
M	O
T	O
X	O
v	O
s	O
Z	O
l	O
0	B-DatasetName
l	O
q	O
U	O
L	O
L	O
F	O
o	O
j	O
A	O
V	O
x	O
M	O
R	O
k	O
9	O
j	O
c	O
Z	O
c	O
I	O
X	O
M	O
i	O
I	O
k	O
l	O
l	O
C	O
l	O
u	O
b	O
y	O
V	O
s	O
R	O
B	O
V	O
l	O
x	O
q	O
Z	O
T	O
s	O
i	O
F	O
4	O
y	O
y	O
+	O
v	O
k	O
t	O
Z	O
F	O
1	O
X	O
O	O
r	O
3	O
v	O
1	O
l	O
p	O
X	O
6	O
T	O
x	O
1	O
G	O
E	O
E	O
z	O
i	O
F	O
c	O
/	O
D	O
g	O
C	O
u	O
p	O
w	O
B	O
w	O
1	O
o	O
A	O
o	O
M	O
h	O
P	O
M	O
M	O
r	O
v	O
D	O
n	O
C	O
e	O
X	O
H	O
e	O
n	O
Y	O
9	O
F	O
a	O
8	O
H	O
J	O
Z	O
4	O
7	O
h	O
D	O
5	O
z	O
P	O
H	O
/	O
Z	O
D	O
j	O
Z	O
I	O
=	O
<	O
/	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
>	O
trained	O
language	O
models	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
are	O
interested	O
if	O
these	O
approaches	O
transfer	O
to	O
our	O
cloze	O
task	O
.	O
We	O
evaluate	O
the	O
OpenAI	O
GPT	B-MethodName
transformer	O
language	O
model	O
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
with	O
and	O
without	O
fine	O
-	O
tuning	O
.	O
Without	O
fine	O
-	O
tuning	O
this	O
model	O
does	O
slightly	O
worse	O
than	O
our	O
best	O
domainspecific	O
model	O
.	O
With	O
fine	O
-	O
tuning	O
,	O
its	O
accuracy	B-MetricName
is	O
substantially	O
higher	O
,	O
but	O
it	O
still	O
suffers	O
from	O
the	O
same	O
fundamental	O
limitations	O
as	O
our	O
other	O
models	O
(	O
see	O
Table	O
5	O
)	O
.	O
The	O
transformer	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
attention	O
is	O
multi	O
-	O
headed	O
and	O
multi	O
-	O
layered	O
which	O
prohibits	O
direct	O
interpretability	O
.	O

Next	O
we	O
briefly	O
describe	O
the	O
dynamic	O
program	O
used	O
to	O
marginalize	O
over	O
alignments	O
during	O
training	O
and	O
to	O
find	O
the	O
most	O
likely	O
alignments	O
of	O
a	O
given	O
alignment	O
during	O
inference	O
;	O
we	O
refer	O
the	O
reader	O
to	O
Yu	O
,	O
Buys	O
,	O
and	O
Blunsom	O
(	O
2016	O
)	O
for	O
a	O
more	O
thorough	O
treatment	O
.	O
The	O
forward	O
variable	O
α	B-HyperparameterName
i	O
(	O
j	O
)	O
representing	O
p	O
(	O
y	O
1	O
:	O
j	O
,	O
a	O
j	O
=	O
i	O
|	O
x	O
1	O
:	O
i	O
)	O
is	O
recursively	O
as	O
αi	O
(	O
j	O
)	O
=	O
p	O
(	O
yj	O
|	O
i	O
,	O
x1	O
:	O
i	O
,	O
y1	O
:	O
j−1	O
)	O
×	O
i	O
k=1	O
α	B-HyperparameterName
k	O
(	O
j	O
−	O
1	O
)	O
p	O
(	O
aj	O
=	O
i	O
|	O
k	O
,	O
x	O
1	O
:	O
k	O
,	O
y1	O
:	O
j−1	O
)	O
.	O
(	O
7	O
)	O
The	O
marginal	O
likelihood	O
objective	O
is	O
to	O
train	O
the	O
model	O
to	O
optimize	O
α	B-HyperparameterName
m	O
(	O
n	O
)	O
=	O
p	O
(	O
y	O
1	O
:	O
n	O
,	O
a	O
n	O
=	O
m	O
|	O
x	O
1	O
:	O
m	O
)	O
.	O
The	O
gradients	O
are	O
computed	O
with	O
automatic	O
differentiation	O
;	O
as	O
this	O
is	O
is	O
equivalent	O
to	O
using	O
the	O
forward	O
-	O
backward	O
algorithm	O
to	O
estimate	O
the	O
gradients	O
(	O
Eisner	O
,	O
2016	O
)	O
,	O
only	O
the	O
forward	O
algorithm	O
has	O
to	O
be	O
implemented	O
.	O
To	O
make	O
the	O
implementation	O
GPU	O
-	O
efficient	O
,	O
we	O
vectorize	O
the	O
computation	O
of	O
α	B-HyperparameterName
.	O
The	O
computation	O
iterates	O
through	O
decoding	O
steps	O
,	O
each	O
of	O
which	O
can	O
be	O
generated	O
from	O
an	O
alignment	O
to	O
any	O
of	O
the	O
encoder	O
tokens	O
.	O
We	O
can	O
efficiently	O
construct	O
a	O
transition	O
matrix	O
T	O
,	O
corresponding	O
to	O
all	O
possible	O
encoder	O
states	O
performing	O
all	O
possible	O
shifts	O
,	O
and	O
emission	O
matrix	O
E	O
j	O
which	O
is	O
a	O
gather	O
by	O
word	O
index	O
j.	O
To	O
compute	O
the	O
forward	O
probabilities	O
at	O
each	O
timestep	O
,	O
the	O
current	O
forward	O
probabilities	O
are	O
first	O
multiplied	O
by	O
all	O
possible	O
transitions	O
.	O
A	O
sum	O
in	O
logspace	O
collapses	O
all	O
paths	O
,	O
and	O
the	O
emission	O
(	O
word	O
generation	O
)	O
probabilities	O
are	O
multiplied	O
to	O
obtain	O
the	O
new	O
forward	O
probabilities	O
.	O
When	O
fixed	O
transition	O
probabilities	O
are	O
used	O
,	O
T	O
is	O
precomputed	O
.	O

Latent	O
variable	O
models	O
can	O
be	O
trained	O
either	O
through	O
directly	O
optimizing	O
the	O
likelihood	O
objective	O
through	O
gradient	O
descent	O
(	O
as	O
described	O
above	O
)	O
,	O
or	O
with	O
the	O
Expectation	O
Maximization	O
(	O
EM	B-MetricName
)	O
algorithm	O
(	O
Dempster	O
et	O
al	O
,	O
1977	O
)	O
,	O
which	O
alternates	O
between	O
calculating	O
expectations	O
over	O
the	O
values	O
of	O
the	O
latent	O
variables	O
given	O
the	O
current	O
parameters	O
,	O
and	O
maximizing	O
the	O
expected	O
complete	O
data	O
log	O
likelihood	O
given	O
those	O
expectations	O
.	O
We	O
consider	O
training	O
our	O
alignment	O
model	O
with	O
Viterbi	O
EM	B-MetricName
(	O
Brown	O
et	O
al	O
,	O
1993	O
)	O
,	O
also	O
known	O
as	O
"	O
hard	O
"	O
EM	B-MetricName
,	O
where	O
at	O
each	O
iteration	O
the	O
most	O
likely	O
assignment	O
of	O
the	O
hidden	O
variables	O
(	O
alignments	O
)	O
are	O
found	O
and	O
the	O
parameters	O
are	O
updated	O
to	O
optimize	O
the	O
log	O
likelihood	O
given	O
those	O
alignments	O
.	O
Viterbi	O
EM	B-MetricName
has	O
been	O
shown	O
to	O
give	O
superior	O
performance	O
to	O
standard	O
EM	B-MetricName
on	O
unsupervised	O
parsing	O
(	O
Spitkovsky	O
et	O
al	O
,	O
2010	O
)	O
,	O
due	O
to	O
better	O
convergence	O
properties	O
in	O
practice	O
by	O
making	O
the	O
distribution	O
more	O
peaked	O
.	O
We	O
perform	O
batched	O
Viterbi	O
EM	B-MetricName
training	O
by	O
computing	O
the	O
Viterbi	O
alignments	O
for	O
a	O
batch	O
,	O
and	O
then	O
performing	O
a	O
gradient	O
step	O
based	O
on	O
treating	O
those	O
alignments	O
as	O
observations	O
.	O
We	O
follow	O
a	O
two	O
-	O
stage	O
training	O
procedure	O
:	O
we	O
first	O
directly	O
optimize	O
the	O
marginal	O
likelihood	O
with	O
batched	O
SGD	B-MethodName
to	O
find	O
a	O
reasonable	O
initial	O
distribu	O
-	O
tion	O
over	O
alignments	O
,	O
before	O
switching	O
to	O
Viterbi	O
EM	B-MetricName
training	O
.	O
Such	O
a	O
strategy	O
has	O
been	O
shown	O
to	O
reduce	O
the	O
chance	O
that	O
the	O
model	O
will	O
get	O
stuck	O
in	O
local	O
optima	O
(	O
Spitkovsky	O
et	O
al	O
,	O
2011	O
)	O
.	O

We	O
apply	O
the	O
trained	O
models	O
to	O
multiple	O
inference	O
problems	O
to	O
evaluate	O
how	O
well	O
they	O
are	O
capturing	O
script	O
knowledge	O
.	O
The	O
first	O
is	O
finding	O
the	O
most	O
likely	O
alignment	O
given	O
a	O
pair	O
of	O
abstract	O
and	O
concrete	O
sequences	O
.	O
We	O
use	O
the	O
standard	O
Viterbi	O
algorithm	O
,	O
in	O
which	O
we	O
replace	O
the	O
sum	O
in	O
equation	O
(	O
7	O
)	O
with	O
max	O
,	O
and	O
keep	O
track	O
of	O
the	O
index	O
corresponding	O
to	O
each	O
value	O
of	O
α	B-HyperparameterName
during	O
the	O
forward	O
computation	O
.	O
The	O
most	O
likely	O
alignment	O
can	O
then	O
be	O
traced	O
back	O
from	O
a	O
n	O
=	O
m.	O
The	O
second	O
inference	O
problem	O
is	O
slot	O
-	O
filling	O
,	O
for	O
application	O
to	O
the	O
cloze	O
task	O
.	O
Given	O
an	O
abstract	O
sentence	O
and	O
a	O
partially	O
-	O
filled	O
concrete	O
sequence	O
,	O
we	O
want	O
to	O
use	O
the	O
model	O
to	O
predict	O
words	O
to	O
fill	O
the	O
given	O
blanks	O
.	O
To	O
make	O
the	O
prediction	O
,	O
we	O
sample	O
5	O
candidate	O
sequences	O
by	O
predicting	O
words	O
for	O
each	O
slot	O
,	O
in	O
left	O
-	O
to	O
-	O
right	O
order	O
,	O
and	O
then	O
choosing	O
the	O
sequence	O
with	O
the	O
highest	O
overall	O
probability	O
.	O
Words	O
are	O
predicted	O
by	O
sampling	O
with	O
temperature	O
0.1	O
,	O
in	O
order	O
to	O
peak	O
the	O
distribution	O
while	O
still	O
allowing	O
some	O
diversity	O
in	O
the	O
samples	O
.	O
The	O
motivation	O
for	O
selecting	O
the	O
final	O
output	O
from	O
multiple	O
samples	O
is	O
that	O
the	O
original	O
samples	O
are	O
biased	O
,	O
as	O
they	O
are	O
only	O
conditioned	O
on	O
the	O
left	O
context	O
.	O
At	O
the	O
start	O
of	O
the	O
prediction	O
for	O
each	O
slot	O
,	O
the	O
Viterbi	O
alignment	O
of	O
the	O
prefix	O
of	O
the	O
sequence	O
up	O
to	O
the	O
start	O
of	O
that	O
slot	O
is	O
re	O
-	O
predicted	O
,	O
independent	O
of	O
previous	O
alignment	O
predictions	O
.	O
Consequently	O
alignment	O
decisions	O
can	O
be	O
revised	O
,	O
and	O
the	O
slot	O
alignments	O
are	O
no	O
longer	O
constrained	O
to	O
be	O
monotonic	O
,	O
which	O
makes	O
the	O
slot	O
prediction	O
model	O
more	O
flexible	O
.	O
For	O
the	O
parameterized	O
transition	O
model	O
,	O
the	O
slot	O
alignment	O
is	O
predicted	O
greedily	O
by	O
incrementing	O
the	O
last	O
predicted	O
alignment	O
while	O
the	O
shift	O
probability	O
is	O
greater	O
than	O
0.5	O
.	O
The	O
fixed	O
transition	O
model	O
assumes	O
that	O
the	O
alignment	O
of	O
the	O
word	O
preceding	O
the	O
slot	O
is	O
shared	O
across	O
the	O
slot	O
.	O

Deep	O
energy	O
-	O
based	O
models	O
are	O
powerful	O
,	O
but	O
pose	O
challenges	O
for	O
learning	O
and	O
inference	O
(	O
Belanger	O
and	O
McCallum	O
,	O
2016	O
)	O
.	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
developed	O
an	O
efficient	O
framework	O
for	O
energy	O
-	O
based	O
models	O
by	O
training	O
"	O
inference	O
networks	O
"	O
to	O
approximate	O
structured	O
inference	O
instead	O
of	O
using	O
gradient	O
descent	O
.	O
However	O
,	O
their	O
alternating	O
optimization	O
approach	O
suffers	O
from	O
instabilities	O
during	O
training	O
,	O
requiring	O
additional	O
loss	B-MetricName
terms	O
and	O
careful	O
hyperparameter	O
tuning	O
.	O
In	O
this	O
paper	O
,	O
we	O
contribute	O
several	O
strategies	O
to	O
stabilize	O
and	O
improve	O
this	O
joint	O
training	O
of	O
energy	O
functions	O
and	O
inference	O
networks	O
for	O
structured	B-TaskName
prediction	I-TaskName
.	O
We	O
design	O
a	O
compound	O
objective	O
to	O
jointly	O
train	O
both	O
costaugmented	O
and	O
test	O
-	O
time	O
inference	O
networks	O
along	O
with	O
the	O
energy	O
function	O
.	O
We	O
propose	O
joint	O
parameterizations	O
for	O
the	O
inference	O
networks	O
that	O
encourage	O
them	O
to	O
capture	O
complementary	O
functionality	O
during	O
learning	O
.	O
We	O
empirically	O
validate	O
our	O
strategies	O
on	O
two	O
sequence	O
labeling	O
tasks	O
,	O
showing	O
easier	O
paths	O
to	O
strong	O
performance	O
than	O
prior	O
work	O
,	O
as	O
well	O
as	O
further	O
improvements	O
with	O
global	O
energy	O
terms	O
.	O

Energy	O
-	O
based	O
modeling	O
(	O
LeCun	O
et	O
al	O
,	O
2006	O
)	O
associates	O
a	O
scalar	O
compatibility	O
measure	O
to	O
each	O
configuration	O
of	O
input	O
and	O
output	O
variables	O
.	O
Belanger	O
and	O
McCallum	O
(	O
2016	O
)	O
formulated	O
deep	O
energy	O
-	O
based	O
models	O
for	O
structured	B-TaskName
prediction	I-TaskName
,	O
which	O
they	O
called	O
structured	B-TaskName
prediction	I-TaskName
energy	O
networks	O
(	O
SPENs	O
)	O
.	O
SPENs	O
use	O
arbitrary	O
neural	O
networks	O
to	O
define	O
the	O
scoring	O
function	O
over	O
input	O
/	O
output	O
pairs	O
.	O
However	O
,	O
this	O
flexibility	O
leads	O
to	O
challenges	O
for	O
learning	O
and	O
inference	O
.	O
The	O
original	O
work	O
on	O
SPENs	O
used	O
gradient	O
descent	O
for	O
structured	O
inference	O
(	O
Belanger	O
and	O
McCallum	O
,	O
2016	O
;	O
Belanger	O
et	O
al	O
,	O
2017	O
)	O
.	O
Gimpel	O
(	O
2018	O
,	O
2019	O
)	O
found	O
improvements	O
in	O
both	O
speed	O
and	O
accuracy	B-MetricName
by	O
replacing	O
the	O
use	O
of	O
gradient	O
descent	O
with	O
a	O
method	O
that	O
trains	O
a	O
neural	O
network	O
(	O
called	O
an	O
"	O
inference	O
network	O
"	O
)	O
to	O
do	O
inference	O
directly	O
.	O
Their	O
formulation	O
,	O
which	O
jointly	O
trains	O
the	O
inference	O
network	O
and	O
energy	O
function	O
,	O
is	O
similar	O
to	O
training	O
in	O
generative	O
adversarial	O
networks	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
,	O
which	O
is	O
known	O
to	O
suffer	O
from	O
practical	O
difficulties	O
in	O
training	O
due	O
to	O
the	O
use	O
of	O
alternating	O
optimization	O
(	O
Salimans	O
et	O
al	O
,	O
2016	O
)	O
.	O
To	O
stabilize	O
training	O
,	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
experimented	O
with	O
several	O
additional	O
terms	O
in	O
the	O
training	O
objectives	O
,	O
finding	O
performance	O
to	O
be	O
dependent	O
on	O
their	O
inclusion	O
.	O
Moreover	O
,	O
when	O
using	O
the	O
approach	O
of	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
,	O
there	O
is	O
a	O
mismatch	O
between	O
the	O
training	O
and	O
test	O
-	O
time	O
uses	O
of	O
the	O
trained	O
inference	O
network	O
.	O
During	O
training	O
with	O
hinge	O
loss	B-MetricName
,	O
the	O
inference	O
network	O
is	O
actually	O
trained	O
to	O
do	O
"	O
costaugmented	O
"	O
inference	O
.	O
However	O
,	O
at	O
test	O
time	O
,	O
the	O
goal	O
is	O
to	O
simply	O
minimize	O
the	O
energy	O
without	O
any	O
cost	O
term	O
.	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
fine	O
-	O
tuned	O
the	O
cost	O
-	O
augmented	O
network	O
to	O
match	O
the	O
test	O
-	O
time	O
criterion	O
,	O
but	O
found	O
only	O
minimal	O
change	O
from	O
this	O
fine	O
-	O
tuning	O
.	O
This	O
suggests	O
that	O
the	O
cost	O
-	O
augmented	O
network	O
was	O
mostly	O
acting	O
as	O
a	O
test	O
-	O
time	O
inference	O
network	O
by	O
convergence	O
,	O
which	O
may	O
be	O
hindering	O
the	O
potential	O
contributions	O
of	O
cost	O
-	O
augmented	O
inference	O
in	O
max	O
-	O
margin	O
structured	O
learning	O
(	O
Tsochantaridis	O
et	O
al	O
,	O
2004	O
;	O
Taskar	O
et	O
al	O
,	O
2004	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
contribute	O
a	O
new	O
training	O
objective	O
for	O
SPENs	O
that	O
addresses	O
the	O
above	O
concern	O
and	O
also	O
contribute	O
several	O
techniques	O
for	O
stabilizing	O
and	O
improving	O
learning	O
.	O
We	O
empirically	O
validate	O
our	O
strategies	O
on	O
two	O
sequence	O
labeling	O
tasks	O
from	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
,	O
namely	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
.	O
We	O
show	O
easier	O
paths	O
to	O
strong	O
performance	O
than	O
prior	O
work	O
,	O
as	O
well	O
as	O
further	O
improvements	O
with	O
global	O
energy	O
terms	O
.	O
We	O
summarize	O
our	O
list	O
of	O
contributions	O
as	O
follows	O
.	O
We	O
design	O
a	O
compound	O
objective	O
under	O
the	O
SPEN	O
framework	O
to	O
jointly	O
train	O
the	O
"	O
trainingtime	O
"	O
cost	O
-	O
augmented	O
inference	O
network	O
and	O
test	O
-	O
time	O
inference	O
network	O
(	O
Section	O
3	O
)	O
.	O
We	O
propose	O
shared	O
parameterizations	O
for	O
the	O
two	O
inference	O
networks	O
so	O
as	O
to	O
encourage	O
them	O
to	O
capture	O
complementary	O
functionality	O
while	O
reducing	O
the	O
total	O
number	O
of	O
trained	O
parameters	O
(	O
Section	O
3.1	O
)	O
.	O
Quantitative	O
and	O
qualitative	O
analysis	O
shows	O
clear	O
differences	O
in	O
the	O
characteristics	O
of	O
the	O
two	O
networks	O
(	O
Table	O
3	O
)	O
.	O
We	O
present	O
three	O
methods	O
to	O
streamline	O
and	O
stabilize	O
training	O
that	O
help	O
with	O
both	O
the	O
old	O
and	O
new	O
objectives	O
(	O
Section	O
4	O
)	O
.	O
We	O
propose	O
global	O
energy	O
terms	O
to	O
capture	O
long	O
-	O
distance	O
dependencies	O
and	O
obtain	O
further	O
improvements	O
(	O
Section	O
5	O
)	O
.	O
While	O
SPENs	O
have	O
been	O
used	O
for	O
multiple	O
NLP	O
tasks	O
,	O
including	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
(	O
Belanger	O
and	O
McCallum	O
,	O
2016	O
)	O
,	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
(	O
Tu	O
and	O
Gimpel	O
,	O
2018	O
)	O
,	O
and	O
semantic	B-TaskName
role	I-TaskName
labeling	I-TaskName
(	O
Belanger	O
et	O
al	O
,	O
2017	O
)	O
,	O
they	O
are	O
not	O
widely	O
used	O
in	O
NLP	O
.	O
Structured	B-TaskName
prediction	I-TaskName
is	O
extremely	O
common	O
in	O
NLP	O
,	O
but	O
is	O
typically	O
approached	O
using	O
methods	O
that	O
are	O
more	O
limited	O
than	O
SPENs	O
(	O
such	O
as	O
conditional	O
random	O
fields	O
)	O
or	O
models	O
that	O
suffer	O
from	O
a	O
train	O
/	O
test	O
mismatch	O
(	O
such	O
as	O
most	O
auto	O
-	O
regressive	O
models	O
)	O
.	O
SPENs	O
offer	O
a	O
maximally	O
expressive	O
framework	O
for	O
structured	B-TaskName
prediction	I-TaskName
while	O
avoiding	O
the	O
train	O
/	O
test	O
mismatch	O
and	O
therefore	O
offer	O
great	O
potential	O
for	O
NLP	O
.	O
However	O
,	O
the	O
training	O
and	O
inference	O
have	O
deterred	O
NLP	O
researchers	O
.	O
While	O
we	O
have	O
found	O
benefit	O
from	O
training	O
inference	O
networks	O
for	O
machine	B-TaskName
translation	I-TaskName
in	O
recent	O
work	O
(	O
Tu	O
et	O
al	O
,	O
2020b	O
)	O
,	O
that	O
work	O
assumed	O
a	O
fixed	O
,	O
pretrained	O
energy	O
function	O
.	O
Our	O
hope	O
is	O
that	O
the	O
methods	O
in	O
this	O
paper	O
will	O
enable	O
SPENs	O
to	O
be	O
applied	O
to	O
a	O
larger	O
set	O
of	O
applications	O
,	O
including	O
generation	O
tasks	O
in	O
the	O
future	O
.	O

We	O
denote	O
the	O
input	O
space	O
by	O
X	O
.	O
For	O
an	O
input	O
x	O
X	O
,	O
we	O
denote	O
the	O
structured	O
output	O
space	O
by	O
Y	O
(	O
x	O
)	O
.	O
The	O
entire	O
space	O
of	O
structured	O
outputs	O
is	O
denoted	O
Y	O
=	O
∪	O
x	O
X	O
Y	O
(	O
x	O
)	O
.	O
A	O
SPEN	O
(	O
Belanger	O
and	O
McCallum	O
,	O
2016	O
)	O
defines	O
an	O
energy	O
function	O
E	O
Θ	B-HyperparameterName
:	O
X	O
×	O
Y	O
R	O
parameterized	O
by	O
Θ	B-HyperparameterName
that	O
computes	O
a	O
scalar	O
energy	O
for	O
an	O
input	O
/	O
output	O
pair	O
.	O
At	O
test	O
time	O
,	O
for	O
a	O
given	O
input	O
x	O
,	O
prediction	O
is	O
done	O
by	O
choosing	O
the	O
output	O
with	O
lowest	O
energy	O
:	O
y	O
=	O
arg	O
min	O
y	O
Y	O
(	O
x	O
)	O
E	O
Θ	B-HyperparameterName
(	O
x	O
,	O
y	O
)	O
(	O
1	O
)	O
However	O
,	O
solving	O
equation	O
(	O
1	O
)	O
requires	O
combinatorial	O
algorithms	O
because	O
Y	O
is	O
a	O
structured	O
,	O
discrete	O
space	O
.	O
This	O
becomes	O
intractable	O
when	O
E	O
Θ	B-HyperparameterName
does	O
not	O
decompose	O
into	O
a	O
sum	O
over	O
small	O
"	O
parts	O
"	O
of	O
y.	O
Belanger	O
and	O
McCallum	O
(	O
2016	O
)	O
relaxed	O
this	O
problem	O
by	O
allowing	O
the	O
discrete	O
vector	O
y	O
to	O
be	O
continuous	O
;	O
Y	O
R	O
denotes	O
the	O
relaxed	O
output	O
space	O
.	O
They	O
solved	O
the	O
relaxed	O
problem	O
by	O
using	O
gradient	O
descent	O
to	O
iteratively	O
minimize	O
the	O
energy	O
with	O
respect	O
to	O
y.	O
The	O
energy	O
function	O
parameters	O
Θ	B-HyperparameterName
are	O
trained	O
using	O
a	O
structured	O
hinge	O
loss	B-MetricName
which	O
requires	O
repeated	O
cost	O
-	O
augmented	O
inference	O
during	O
training	O
.	O
Using	O
gradient	O
descent	O
for	O
the	O
repeated	O
costaugmented	O
inference	O
steps	O
is	O
time	O
-	O
consuming	O
and	O
makes	O
learning	O
unstable	O
(	O
Belanger	O
et	O
al	O
,	O
2017	O
)	O
.	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
replaced	O
gradient	O
descent	O
with	O
a	O
neural	O
network	O
trained	O
to	O
do	O
efficient	O
inference	O
.	O
This	O
"	O
inference	O
network	O
"	O
A	O
Ψ	O
:	O
X	O
Y	O
R	O
is	O
parameterized	O
by	O
Ψ	O
and	O
trained	O
with	O
the	O
goal	O
that	O
A	O
Ψ	O
(	O
x	O
)	O
≈	O
arg	O
min	O
y	O
Y	O
R	O
(	O
x	O
)	O
E	O
Θ	B-HyperparameterName
(	O
x	O
,	O
y	O
)	O
(	O
2	O
)	O
When	O
training	O
the	O
energy	O
function	O
parameters	O
Θ	B-HyperparameterName
,	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
replaced	O
the	O
cost	O
-	O
augmented	O
inference	O
step	O
in	O
the	O
structured	O
hinge	O
loss	B-MetricName
from	O
Belanger	O
and	O
McCallum	O
(	O
2016	O
)	O
with	O
a	O
costaugmented	O
inference	O
network	O
F	O
Φ	O
:	O
F	O
Φ	O
(	O
x	O
)	O
≈	O
arg	O
min	O
y	O
Y	O
R	O
(	O
x	O
)	O
(	O
E	O
Θ	B-HyperparameterName
(	O
x	O
,	O
y	O
)	O
−	O
(	O
y	O
,	O
y	O
*	O
)	O
)	O
(	O
3	O
)	O
where	O
is	O
a	O
structured	O
cost	O
function	O
that	O
computes	O
the	O
distance	O
between	O
its	O
two	O
arguments	O
.	O
We	O
use	O
L1	O
distance	O
for	O
.	O
This	O
inference	O
problem	O
involves	O
finding	O
an	O
output	O
with	O
low	O
energy	O
but	O
high	O
cost	O
relative	O
to	O
the	O
gold	O
standard	O
.	O
Thus	O
,	O
it	O
is	O
not	O
wellaligned	O
with	O
the	O
test	O
-	O
time	O
inference	O
problem	O
.	O
Here	O
is	O
the	O
specific	O
objective	O
to	O
jointly	O
train	O
Θ	B-HyperparameterName
(	O
parameters	O
of	O
the	O
energy	O
function	O
)	O
and	O
Φ	O
(	O
parameters	O
of	O
the	O
cost	O
-	O
augmented	O
inference	O
network	O
)	O
:	O
min	O
Θ	B-HyperparameterName
max	O
Φ	O
x	O
i	O
,	O
y	O
i	O
D	O
[	O
(	O
F	O
Φ	O
(	O
x	O
i	O
)	O
,	O
y	O
i	O
)	O
−	O
E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
F	O
Φ	O
(	O
x	O
i	O
)	O
)	O
+	O
E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
y	O
i	O
)	O
]	O
+	O
(	O
4	O
)	O
where	O
D	O
is	O
the	O
set	O
of	O
training	O
pairs	O
,	O
[	O
h	O
]	O
+	O
=	O
max	O
(	O
0	B-DatasetName
,	O
h	O
)	O
,	O
and	O
is	O
a	O
structured	O
cost	O
function	O
that	O
computes	O
the	O
distance	O
between	O
its	O
two	O
arguments	O
.	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
alternatively	O
optimized	O
Θ	B-HyperparameterName
and	O
Φ	O
,	O
which	O
is	O
similar	O
to	O
training	O
in	O
generative	O
adversarial	O
networks	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
inference	O
network	O
is	O
analogous	O
to	O
the	O
generator	O
and	O
the	O
energy	O
function	O
is	O
analogous	O
to	O
the	O
discriminator	O
.	O
As	O
alternating	O
optimization	O
can	O
be	O
difficult	O
in	O
practice	O
(	O
Salimans	O
et	O
al	O
,	O
2016	O
)	O
,	O
Tu	O
&	O
Gimpel	O
experimented	O
with	O
including	O
several	O
additional	O
terms	O
in	O
the	O
above	O
objective	O
to	O
stabilize	O
training	O
.	O
After	O
the	O
training	O
of	O
the	O
energy	O
function	O
,	O
an	O
inference	O
network	O
A	O
Ψ	O
for	O
test	O
-	O
time	O
prediction	O
is	O
finetuned	O
with	O
the	O
goal	O
shown	O
in	O
Eq	O
.	O
(	O
2	O
)	O
.	O
More	O
specifically	O
,	O
for	O
the	O
fine	O
-	O
tuning	O
step	O
,	O
we	O
first	O
initialize	O
Ψ	O
with	O
Φ	O
;	O
next	O
,	O
we	O
do	O
gradient	O
descent	O
according	O
to	O
the	O
following	O
objective	O
to	O
learn	O
Ψ	O
:	O
Ψ	O
arg	O
min	O
Ψ	O
x	O
X	O
E	O
Θ	B-HyperparameterName
(	O
x	O
,	O
A	O
Ψ	O
(	O
x	O
)	O
)	O
where	O
X	O
is	O
a	O
set	O
of	O
training	O
or	O
validation	O
inputs	O
.	O
It	O
could	O
also	O
be	O
the	O
test	O
inputs	O
in	O
a	O
transductive	O
setting	O
.	O
3	O
An	O
Objective	O
for	O
Joint	O
Learning	O
of	O
Inference	O
Networks	O
One	O
challenge	O
with	O
the	O
above	O
optimization	O
problem	O
is	O
that	O
it	O
requires	O
training	O
a	O
separate	O
inference	O
network	O
A	O
Ψ	O
for	O
test	O
-	O
time	O
prediction	O
after	O
the	O
energy	O
function	O
is	O
trained	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
alternative	O
that	O
trains	O
the	O
energy	O
function	O
and	O
both	O
inference	O
networks	O
jointly	O
.	O
In	O
particular	O
,	O
we	O
use	O
a	O
"	O
compound	O
"	O
objective	O
that	O
combines	O
two	O
widely	O
-	O
used	O
losses	O
in	O
structured	B-TaskName
prediction	I-TaskName
.	O
We	O
first	O
present	O
it	O
without	O
inference	O
networks	O
:	O
min	O
Θ	B-HyperparameterName
x	O
i	O
,	O
y	O
i	O
D	O
max	O
y	O
(	O
(	O
y	O
,	O
y	O
i	O
)	O
−E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
y	O
)	O
+	O
E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
y	O
i	O
)	O
)	O
+	O
margin	O
-	O
rescaled	O
hinge	O
loss	B-MetricName
+	O
λ	O
max	O
y	O
(	O
−E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
y	O
)	O
+	O
E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
y	O
i	O
)	O
)	O
+	O
perceptron	O
loss	B-MetricName
(	O
5	O
)	O
As	O
indicated	O
,	O
this	O
loss	B-MetricName
can	O
be	O
viewed	O
as	O
the	O
sum	O
of	O
the	O
margin	O
-	O
rescaled	O
hinge	O
and	O
perceptron	O
losses	O
for	O
SPENs	O
.	O
Two	O
different	O
inference	O
problems	O
are	O
represented	O
.	O
The	O
margin	O
-	O
rescaled	O
hinge	O
loss	B-MetricName
contains	O
cost	O
-	O
augmented	O
inference	O
,	O
shown	O
as	O
part	O
of	O
Eq	O
.	O
(	O
3	O
)	O
.	O
The	O
perceptron	O
loss	B-MetricName
contains	O
the	O
test	O
-	O
time	O
inference	O
problem	O
,	O
which	O
is	O
shown	O
in	O
Eq	O
.	O
(	O
1	O
)	O
.	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
used	O
a	O
single	O
inference	O
network	O
for	O
solving	O
both	O
problems	O
,	O
so	O
it	O
was	O
trained	O
as	O
a	O
cost	O
-	O
augmented	O
inference	O
network	O
during	O
training	O
and	O
then	O
fine	O
-	O
tuned	O
as	O
a	O
test	O
-	O
time	O
inference	O
network	O
afterward	O
.	O
We	O
avoid	O
this	O
issue	O
by	O
training	O
two	O
inference	O
networks	O
,	O
A	O
Ψ	O
for	O
test	O
-	O
time	O
inference	O
and	O
F	O
Φ	O
for	O
cost	O
-	O
augmented	O
inference	O
:	O
min	O
Θ	B-HyperparameterName
max	O
Φ	O
,	O
Ψ	O
x	O
i	O
,	O
y	O
i	O
D	O
[	O
(	O
F	O
Φ	O
(	O
x	O
i	O
)	O
,	O
y	O
i	O
)	O
−E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
F	O
Φ	O
(	O
x	O
i	O
)	O
)	O
+	O
E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
y	O
i	O
)	O
]	O
+	O
+	O
λ	O
[	O
−E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
A	O
Ψ	O
(	O
x	O
i	O
)	O
)	O
+	O
E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
y	O
i	O
)	O
]	O
+	O
(	O
6	O
)	O
We	O
treat	O
this	O
optimization	O
problem	O
as	O
a	O
minimax	O
game	O
and	O
find	O
a	O
saddle	O
point	O
for	O
the	O
game	O
similar	O
to	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
and	O
Goodfellow	O
et	O
al	O
(	O
2014	O
)	O
.	O
We	O
use	O
minibatch	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
and	O
alternately	O
optimize	O
Θ	B-HyperparameterName
,	O
Φ	O
,	O
and	O
Ψ.	O
The	O
objective	O
for	O
the	O
energy	O
parameters	O
Θ	B-HyperparameterName
in	O
minibatch	O
M	O
is	O
:	O
Θ	B-HyperparameterName
arg	O
min	O
Θ	B-HyperparameterName
x	O
i	O
,	O
y	O
i	O
M	O
(	O
F	O
Φ	O
(	O
x	O
i	O
)	O
,	O
y	O
i	O
)	O
−E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
F	O
Φ	O
(	O
x	O
i	O
)	O
)	O
+	O
E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
y	O
i	O
)	O
+	O
+	O
λ	O
−E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
A	O
Ψ	O
(	O
x	O
i	O
)	O
)	O
+	O
E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
y	O
i	O
)	O
+	O
When	O
we	O
remove	O
0	B-DatasetName
-	O
truncation	O
(	O
see	O
Sec	O
.	O
4.1	O
for	O
the	O
motivation	O
)	O
,	O
the	O
objective	O
for	O
the	O
inference	O
network	O
parameters	O
in	O
minibatch	O
M	O
is	O
:	O
Ψ	O
,	O
Φ	O
arg	O
max	O
Ψ	O
,	O
Φ	O
x	O
i	O
,	O
y	O
i	O
M	O
(	O
F	O
Φ	O
(	O
x	O
i	O
)	O
,	O
y	O
i	O
)	O
−	O
E	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
F	O
Φ	O
(	O
x	O
i	O
)	O
)	O
−	O
λE	O
Θ	B-HyperparameterName
(	O
x	O
i	O
,	O
A	O
Ψ	O
(	O
x	O
i	O
)	O
)	O

If	O
we	O
were	O
to	O
train	O
independent	O
inference	O
networks	O
A	O
Ψ	O
and	O
F	O
Φ	O
,	O
this	O
new	O
objective	O
could	O
be	O
much	O
slower	O
than	O
the	O
approach	O
of	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
.	O
However	O
,	O
the	O
compound	O
objective	O
offers	O
several	O
natural	O
options	O
for	O
defining	O
joint	O
parameterizations	O
of	O
the	O
two	O
inference	O
networks	O
.	O
We	O
consider	O
three	O
options	O
which	O
are	O
visualized	O
in	O
Figure	O
1	O
and	O
described	O
below	O
:	O
separated	O
:	O
F	O
Φ	O
and	O
A	O
Ψ	O
are	O
two	O
independent	O
networks	O
with	O
their	O
own	O
architectures	O
and	O
parameters	O
as	O
shown	O
in	O
Figure	O
1	O
(	O
a	O
)	O
.	O
shared	O
:	O
F	O
Φ	O
and	O
A	O
Ψ	O
share	O
a	O
"	O
feature	O
"	O
network	O
as	O
shown	O
in	O
Figure	O
1	O
(	O
b	O
)	O
.	O
We	O
consider	O
this	O
option	O
because	O
both	O
F	O
Φ	O
and	O
A	O
Ψ	O
are	O
trained	O
to	O
produce	O
output	O
labels	O
with	O
low	O
energy	O
.	O
However	O
F	O
Φ	O
also	O
needs	O
to	O
produce	O
output	O
labels	O
with	O
high	O
cost	O
(	O
i.e.	O
,	O
far	O
from	O
the	O
gold	O
standard	O
)	O
.	O
stacked	O
:	O
the	O
cost	O
-	O
augmented	O
network	O
F	O
Φ	O
is	O
a	O
function	O
of	O
the	O
output	O
of	O
the	O
test	O
-	O
time	O
network	O
A	O
Ψ	O
and	O
the	O
gold	O
standard	O
output	O
y.	O
That	O
is	O
,	O
F	O
Φ	O
(	O
x	O
,	O
y	O
)	O
=	O
q	O
(	O
A	O
Ψ	O
(	O
x	O
)	O
,	O
y	O
)	O
where	O
q	O
is	O
a	O
parameterized	O
function	O
.	O
This	O
is	O
depicted	O
in	O
Figure	O
1	O
(	O
c	O
)	O
.	O
We	O
block	O
the	O
gradient	O
at	O
A	O
Ψ	O
when	O
updating	O
Φ.	O
For	O
the	O
q	O
function	O
in	O
the	O
stacked	O
option	O
,	O
we	O
use	O
an	O
affine	O
transformation	O
on	O
the	O
concatenation	O
of	O
the	O
inference	O
network	O
label	O
distribution	O
and	O
the	O
gold	O
standard	O
one	O
-	O
hot	O
vector	O
.	O
That	O
is	O
,	O
denoting	O
the	O
vector	O
at	O
position	O
t	O
of	O
the	O
cost	O
-	O
augmented	O
network	O
output	O
by	O
F	O
Φ	O
(	O
x	O
,	O
y	O
)	O
t	O
,	O
we	O
have	O
:	O
F	O
Φ	O
(	O
x	O
,	O
y	O
)	O
t	O
=	O
softmax	B-MethodName
(	O
W	O
q	O
[	O
A	O
Ψ	O
(	O
x	O
)	O
t	O
;	O
y	O
(	O
t	O
)	O
]	O
+	O
b	O
q	O
)	O
where	O
semicolon	O
(	O
;	O
)	O
is	O
vertical	O
concatenation	O
,	O
y	O
(	O
t	O
)	O
(	O
position	O
t	O
of	O
y	O
)	O
is	O
an	O
L	O
-	O
dimensional	O
one	O
-	O
hot	O
vector	O
,	O
A	O
Ψ	O
(	O
x	O
)	O
t	O
is	O
the	O
vector	O
at	O
position	O
t	O
of	O
A	O
Ψ	O
(	O
x	O
)	O
,	O
W	O
q	O
is	O
an	O
L	O
×	O
2L	O
matrix	O
,	O
and	O
b	O
q	O
is	O
a	O
bias	O
.	O
One	O
motivation	O
for	O
these	O
parameterizations	O
is	O
to	O
reduce	O
the	O
total	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
in	O
the	O
procedure	O
.	O
Generally	O
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
is	O
expected	O
to	O
decrease	O
when	O
moving	O
from	O
separated	O
to	O
shared	O
to	O
stacked	O
.	O
We	O
will	O
compare	O
the	O
three	O
options	O
empirically	O
in	O
our	O
experiments	O
,	O
in	O
terms	O
of	O
both	O
accuracy	B-MetricName
and	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
.	O
Another	O
motivation	O
,	O
specifically	O
for	O
the	O
third	O
option	O
,	O
is	O
to	O
distinguish	O
the	O
two	O
inference	O
networks	O
in	O
terms	O
of	O
their	O
learned	O
functionality	O
.	O
With	O
all	O
three	O
parameterizations	O
,	O
the	O
cost	O
-	O
augmented	O
network	O
will	O
be	O
trained	O
to	O
produce	O
an	O
output	O
that	O
differs	O
from	O
the	O
gold	O
standard	O
,	O
due	O
to	O
the	O
presence	O
of	O
the	O
(	O
)	O
term	O
in	O
the	O
combined	O
objective	O
.	O
However	O
,	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
found	O
that	O
the	O
trained	O
cost	O
-	O
augmented	O
network	O
was	O
barely	O
affected	O
by	O
fine	O
-	O
tuning	O
for	O
the	O
test	O
-	O
time	O
inference	O
objective	O
.	O
This	O
suggests	O
that	O
the	O
cost	O
-	O
augmented	O
network	O
was	O
mostly	O
acting	O
as	O
a	O
test	O
-	O
time	O
inference	O
network	O
by	O
the	O
time	O
of	O
convergence	O
.	O
With	O
the	O
stacked	O
parameterization	O
,	O
however	O
,	O
we	O
explicitly	O
provide	O
the	O
gold	O
standard	O
y	O
to	O
the	O
cost	O
-	O
augmented	O
network	O
,	O
permitting	O
it	O
to	O
learn	O
to	O
change	O
the	O
predictions	O
of	O
the	O
test	O
-	O
time	O
network	O
in	O
appropriate	O
ways	O
to	O
improve	O
the	O
energy	O
function	O
.	O

Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
used	O
the	O
following	O
objective	O
for	O
the	O
cost	O
-	O
augmented	O
inference	O
network	O
(	O
maximizing	O
it	O
with	O
respect	O
to	O
Φ	O
)	O
:	O
l	O
0	B-DatasetName
=	O
[	O
(	O
F	O
Φ	O
(	O
x	O
)	O
,	O
y	O
)	O
−	O
E	O
Θ	B-HyperparameterName
(	O
x	O
,	O
F	O
Φ	O
(	O
x	O
)	O
)	O
+	O
E	O
Θ	B-HyperparameterName
(	O
x	O
,	O
y	O
)	O
]	O
+	O
where	O
[	O
h	O
]	O
+	O
=	O
max	O
(	O
0	B-DatasetName
,	O
h	O
)	O
.	O
However	O
,	O
there	O
are	O
two	O
potential	O
reasons	O
why	O
l	O
0	B-DatasetName
will	O
equal	O
zero	O
and	O
trigger	O
no	O
gradient	O
update	O
.	O
First	O
,	O
E	O
Θ	B-HyperparameterName
(	O
the	O
energy	O
function	O
,	O
corresponding	O
to	O
the	O
discriminator	O
in	O
a	O
GAN	B-MethodName
)	O
may	O
already	O
be	O
well	O
-	O
trained	O
,	O
and	O
it	O
can	O
easily	O
separate	O
the	O
gold	O
standard	O
output	O
from	O
the	O
costaugmented	O
inference	O
network	O
output	O
.	O
Second	O
,	O
the	O
cost	O
-	O
augmented	O
inference	O
network	O
(	O
corresponding	O
to	O
the	O
generator	O
in	O
a	O
GAN	B-MethodName
)	O
could	O
be	O
so	O
poorly	O
trained	O
that	O
the	O
energy	O
of	O
its	O
output	O
is	O
very	O
large	O
,	O
leading	O
the	O
margin	O
constraints	O
to	O
be	O
satisfied	O
and	O
l	O
0	B-DatasetName
to	O
be	O
zero	O
.	O
In	O
standard	O
margin	O
-	O
rescaled	O
max	O
-	O
margin	O
learning	O
in	O
structured	B-TaskName
prediction	I-TaskName
(	O
Taskar	O
et	O
al	O
,	O
2004	O
;	O
Tsochantaridis	O
et	O
al	O
,	O
2004	O
)	O
,	O
the	O
cost	O
-	O
augmented	O
inference	O
step	O
is	O
performed	O
exactly	O
(	O
or	O
approximately	O
with	O
reasonable	O
guarantee	O
of	O
effectiveness	O
)	O
,	O
ensuring	O
that	O
when	O
l	O
0	B-DatasetName
is	O
zero	O
,	O
the	O
energy	O
parameters	O
are	O
well	O
trained	O
.	O
However	O
,	O
in	O
our	O
case	O
,	O
l	O
0	B-DatasetName
may	O
be	O
zero	O
simply	O
because	O
the	O
cost	O
-	O
augmented	O
inference	O
network	O
is	O
undertrained	O
,	O
which	O
will	O
be	O
the	O
case	O
early	O
in	O
training	O
.	O
Then	O
,	O
when	O
using	O
zero	O
truncation	O
,	O
the	O
gradient	O
of	O
the	O
inference	O
network	O
parameters	O
will	O
be	O
0	B-DatasetName
.	O
This	O
is	O
likely	O
why	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
found	O
it	O
important	O
to	O
add	O
several	O
stabilization	O
terms	O
to	O
the	O
l	O
0	B-DatasetName
objective	O
.	O
We	O
find	O
that	O
by	O
instead	O
removing	O
the	O
truncation	O
,	O
learning	O
stabilizes	O
and	O
becomes	O
less	O
dependent	O
on	O
these	O
additional	O
terms	O
.	O
Note	O
that	O
we	O
retain	O
the	O
truncation	O
at	O
zero	O
when	O
updating	O
the	O
energy	O
parameters	O
Θ.	O
As	O
shown	O
in	O
Figure	O
2	O
(	O
a	O
)	O
,	O
without	O
any	O
stabilization	O
terms	O
and	O
with	O
truncation	O
,	O
the	O
inference	O
network	O
will	O
barely	O
move	O
from	O
its	O
starting	O
point	O
and	O
learning	O
fails	O
overall	O
.	O
However	O
,	O
without	O
truncation	O
,	O
the	O
inference	O
network	O
can	O
work	O
well	O
even	O
without	O
any	O
stabilization	O
terms	O
.	O

Tu	O
and	O
proposed	O
adding	O
a	O
local	O
cross	O
entropy	O
(	O
CE	O
)	O
loss	B-MetricName
,	O
which	O
is	O
the	O
sum	O
of	O
the	O
label	O
cross	O
entropy	O
losses	O
over	O
all	O
positions	O
in	O
the	O
sequence	O
,	O
to	O
stabilize	O
inference	O
network	O
training	O
.	O
We	O
similarly	O
find	O
this	O
term	O
to	O
help	O
speed	O
up	O
convergence	O
and	O
improve	O
accuracy	B-MetricName
.	O
Figure	O
2	O
(	O
b	O
)	O
shows	O
faster	O
convergence	O
to	O
high	O
accuracy	B-MetricName
when	O
adding	O
the	O
local	O
CE	O
term	O
.	O
See	O
Section	O
7	O
for	O
more	O
details	O
.	O

When	O
training	O
SPENs	O
with	O
inference	O
networks	O
,	O
the	O
inference	O
network	O
parameters	O
are	O
nested	O
within	O
the	O
energy	O
function	O
.	O
We	O
found	O
that	O
the	O
gradient	O
components	O
of	O
the	O
inference	O
network	O
parameters	O
consequently	O
have	O
smaller	O
absolute	O
values	O
than	O
those	O
of	O
the	O
energy	O
function	O
parameters	O
.	O
So	O
,	O
we	O
alternate	O
between	O
k	O
≥	O
1	O
steps	O
of	O
optimizing	O
the	O
inference	O
network	O
parameters	O
(	O
"	O
I	O
steps	O
"	O
)	O
and	O
one	O
step	O
of	O
optimizing	O
the	O
energy	O
function	O
parameters	O
(	O
"	O
E	O
steps	O
"	O
)	O
.	O
We	O
find	O
this	O
strategy	O
especially	O
helpful	O
when	O
using	O
complex	O
inference	O
network	O
architectures	O
.	O
To	O
analyze	O
,	O
we	O
compute	O
the	O
cost	O
-	O
augmented	O
loss	B-MetricName
l	O
1	O
=	O
(	O
F	O
Φ	O
(	O
x	O
)	O
,	O
y	O
)	O
−	O
E	O
Θ	B-HyperparameterName
(	O
x	O
,	O
F	O
Φ	O
(	O
x	O
)	O
)	O
and	O
the	O
margin	O
-	O
rescaled	O
hinge	O
loss	B-MetricName
With	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
,	O
the	O
setting	O
used	O
by	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
,	O
the	O
inference	O
network	O
lags	O
behind	O
the	O
energy	O
,	O
making	O
the	O
energy	O
parameter	O
updates	O
very	O
small	O
,	O
as	O
shown	O
by	O
the	O
small	O
norms	O
in	O
Fig	O
.	O
3	O
(	O
c	O
)	O
.	O
The	O
inference	O
network	O
gradient	O
norm	O
(	O
Fig	O
.	O
3	O
(	O
d	O
)	O
)	O
remains	O
high	O
,	O
indicating	O
underfitting	O
.	O
However	O
,	O
increasing	O
k	O
too	O
much	O
also	O
harms	O
learning	O
,	O
as	O
evi	O
-	O
denced	O
by	O
the	O
"	O
plateau	O
"	O
effect	O
in	O
the	O
l	O
1	O
curves	O
for	O
k	B-HyperparameterName
=	I-HyperparameterName
50	O
;	O
this	O
indicates	O
that	O
the	O
energy	O
function	O
is	O
lagging	O
behind	O
the	O
inference	O
network	O
.	O
Using	O
k	B-HyperparameterName
=	I-HyperparameterName
5	O
leads	O
to	O
more	O
of	O
a	O
balance	O
between	O
l	O
1	O
and	O
l	O
0	B-DatasetName
and	O
gradient	O
norms	O
that	O
are	O
mostly	O
decreasing	O
during	O
training	O
.	O
We	O
treat	O
k	O
as	O
a	O
hyperparameter	O
that	O
is	O
tuned	O
in	O
our	O
experiments	O
.	O
l	O
0	B-DatasetName
=	O
[	O
(	O
F	O
Φ	O
(	O
x	O
)	O
,	O
y	O
)	O
−	O
E	O
Θ	B-HyperparameterName
(	O
x	O
,	O
F	O
Φ	O
(	O
x	O
)	O
)	O
+	O
E	O
Θ	B-HyperparameterName
(	O
x	O
,	O
y	O
)	O
]	O
+	O
averaged	O
There	O
is	O
a	O
potential	O
connection	O
between	O
our	O
use	O
of	O
multiple	O
I	O
steps	O
and	O
a	O
similar	O
procedure	O
used	O
in	O
GANs	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
.	O
In	O
the	O
GAN	B-MethodName
objective	O
,	O
the	O
discriminator	O
D	O
is	O
updated	O
in	O
the	O
inner	O
loop	O
,	O
and	O
they	O
alternate	O
between	O
multiple	O
update	O
steps	O
for	O
D	O
and	O
one	O
update	O
step	O
for	O
G.	O
In	O
this	O
section	O
,	O
we	O
similarly	O
found	O
benefit	O
from	O
multiple	O
steps	O
of	O
inner	O
loop	O
optimization	O
for	O
every	O
step	O
of	O
the	O
outer	O
loop	O
.	O
However	O
,	O
the	O
analogy	O
is	O
limited	O
,	O
since	O
GAN	B-MethodName
training	O
involves	O
sampling	O
noise	O
vectors	O
and	O
using	O
them	O
to	O
generate	O
data	O
,	O
while	O
there	O
are	O
no	O
noise	O
vectors	O
or	O
explicitly	O
-	O
generated	O
samples	O
in	O
our	O
framework	O
.	O

For	O
our	O
sequence	O
labeling	O
experiments	O
in	O
this	O
paper	O
,	O
the	O
input	O
x	O
is	O
a	O
length	O
-	O
T	O
sequence	O
of	O
tokens	O
,	O
and	O
the	O
output	O
y	O
is	O
a	O
sequence	O
of	O
labels	O
of	O
length	O
T	O
.	O
We	O
use	O
y	O
t	O
to	O
denote	O
the	O
output	O
label	O
at	O
position	O
t	O
,	O
where	O
y	O
t	O
is	O
a	O
vector	O
of	O
length	O
L	O
(	O
the	O
number	O
of	O
labels	O
in	O
the	O
label	O
set	O
)	O
and	O
where	O
y	O
t	O
,	O
j	O
is	O
the	O
jth	O
entry	O
of	O
the	O
vector	O
y	O
t	O
.	O
In	O
the	O
original	O
output	O
space	O
Y	O
(	O
x	O
)	O
,	O
y	O
t	O
,	O
j	O
is	O
1	O
for	O
a	O
single	O
j	O
and	O
0	B-DatasetName
for	O
all	O
others	O
.	O
In	O
the	O
relaxed	O
output	O
space	O
Y	O
R	O
(	O
x	O
)	O
,	O
y	O
t	O
,	O
j	O
can	O
be	O
interpreted	O
as	O
the	O
probability	O
of	O
the	O
tth	O
position	O
being	O
labeled	O
with	O
label	O
j.	O
We	O
then	O
use	O
the	O
following	O
energy	O
for	O
sequence	O
labeling	O
(	O
Tu	O
and	O
Gimpel	O
,	O
2018	O
)	O
:	O
E	O
Θ	B-HyperparameterName
(	O
x	O
,	O
y	O
)	O
=	O
−	O
T	O
t=1	O
L	O
j=1	O
y	O
t	O
,	O
j	O
U	O
j	O
b	O
(	O
x	O
,	O
t	O
)	O
+	O
T	O
t=1	O
y	O
t−1	O
W	O
y	O
t	O
(	O
7	O
)	O
where	O
U	O
j	O
R	O
d	O
is	O
a	O
parameter	O
vector	O
for	O
label	O
j	O
and	O
the	O
parameter	O
matrix	O
W	O
R	O
L×L	O
contains	O
label	O
-	O
pair	O
parameters	O
.	O
Also	O
,	O
b	O
(	O
x	O
,	O
t	O
)	O
R	O
d	O
denotes	O
the	O
"	O
input	O
feature	O
vector	O
"	O
for	O
position	O
t.	O
We	O
define	O
b	O
to	O
be	O
the	O
d	O
-	O
dimensional	O
BiLSTM	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
hidden	O
vector	O
at	O
t.	O
The	O
full	O
set	O
of	O
energy	O
parameters	O
Θ	B-HyperparameterName
includes	O
the	O
U	O
j	O
vectors	O
,	O
W	O
,	O
and	O
the	O
parameters	O
of	O
the	O
BiLSTM	B-MethodName
.	O
Global	O
Energies	O
for	O
Sequence	O
Labeling	O
.	O
In	O
addition	O
to	O
new	O
training	O
strategies	O
,	O
we	O
also	O
experiment	O
with	O
several	O
global	O
energy	O
terms	O
for	O
sequence	O
labeling	O
.	O
Eq	O
.	O
(	O
7	O
)	O
shows	O
the	O
base	O
energy	O
,	O
and	O
to	O
capture	O
long	O
-	O
distance	O
dependencies	O
,	O
we	O
include	O
global	O
energy	O
(	O
GE	O
)	O
terms	O
in	O
the	O
form	O
of	O
Eq	O
.	O
(	O
8	O
)	O
.	O
We	O
use	O
h	O
to	O
denote	O
an	O
LSTM	B-MethodName
tag	O
language	O
model	O
(	O
TLM	O
)	O
that	O
takes	O
a	O
sequence	O
of	O
labels	O
as	O
input	O
and	O
returns	O
a	O
distribution	O
over	O
next	O
labels	O
.	O
We	O
define	O
y	O
t	O
=	O
h	O
(	O
y	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
y	O
t−1	O
)	O
to	O
be	O
the	O
distribution	O
given	O
the	O
preceding	O
label	O
vectors	O
(	O
under	O
a	O
LSTM	B-MethodName
language	O
model	O
)	O
.	O
Then	O
,	O
the	O
energy	O
term	O
is	O
:	O
E	O
TLM	O
(	O
y	O
)	O
=	O
−	O
T	O
+1	O
t=1	O
log	O
y	O
t	O
y	O
t	O
(	O
8	O
)	O
where	O
y	O
0	B-DatasetName
is	O
the	O
start	O
-	O
of	O
-	O
sequence	O
symbol	O
and	O
y	O
T	O
+1	O
is	O
the	O
end	O
-	O
of	O
-	O
sequence	O
symbol	O
.	O
This	O
energy	O
returns	O
the	O
negative	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
under	O
the	O
TLM	O
of	O
the	O
candidate	O
output	O
y.	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
pretrained	O
their	O
h	O
on	O
a	O
large	O
,	O
automatically	O
-	O
tagged	O
corpus	O
and	O
fixed	O
its	O
parameters	O
when	O
optimizing	O
Θ.	O
Our	O
approach	O
has	O
one	O
critical	O
difference	O
.	O
We	O
instead	O
do	O
not	O
pretrain	O
h	O
,	O
and	O
its	O
parameters	O
are	O
learned	O
when	O
optimizing	O
Θ.	O
We	O
show	O
that	O
even	O
without	O
pretraining	O
,	O
our	O
global	O
energy	O
terms	O
are	O
still	O
able	O
to	O
capture	O
useful	O
additional	O
information	O
.	O
We	O
also	O
propose	O
new	O
global	O
energy	O
terms	O
.	O
Define	O
y	O
t	O
=	O
h	O
(	O
y	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
y	O
t−1	O
)	O
where	O
h	O
is	O
an	O
LSTM	B-MethodName
TLM	O
that	O
takes	O
a	O
sequence	O
of	O
labels	O
as	O
input	O
and	O
returns	O
a	O
distribution	O
over	O
next	O
labels	O
.	O
First	O
,	O
we	O
add	O
a	O
TLM	O
in	O
the	O
backward	O
direction	O
(	O
denoted	O
y	O
t	O
analogously	O
to	O
the	O
forward	O
TLM	O
)	O
.	O
Second	O
,	O
we	O
include	O
words	O
as	O
additional	O
inputs	O
to	O
forward	O
and	O
backward	O
TLMs	O
.	O
We	O
define	O
y	O
t	O
=	O
g	O
(	O
x	O
0	B-DatasetName
,	O
...	O
,	O
x	O
t−1	O
,	O
y	O
0	B-DatasetName
,	O
...	O
,	O
y	O
t−1	O
)	O
where	O
g	O
is	O
a	O
forward	O
LSTM	B-MethodName
TLM	O
.	O
We	O
define	O
the	O
backward	O
version	O
similarly	O
(	O
denoted	O
y	O
t	O
)	O
.	O
The	O
global	O
energy	O
is	O
therefore	O
E	O
GE	O
(	O
y	O
)	O
=	O
−	O
T	O
+1	O
t=1	O
log	O
(	O
y	O
t	O
y	O
t	O
)	O
+	O
log	O
(	O
y	O
t	O
y	O
t	O
)	O
+	O
γ	B-HyperparameterName
log	O
(	O
y	O
t	O
y	O
t	O
)	O
+	O
log	O
(	O
y	O
t	O
y	O
t	O
)	O
(	O
9	O
)	O
Here	O
γ	B-HyperparameterName
is	O
a	O
hyperparameter	O
that	O
is	O
tuned	O
.	O
We	O
experiment	O
with	O
three	O
settings	O
for	O
the	O
global	O
energy	O
:	O
GE	O
(	O
a	O
)	O
:	O
forward	O
TLM	O
as	O
in	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
;	O
GE	O
(	O
b	O
)	O
:	O
forward	O
and	O
backward	O
TLMs	O
(	O
γ	B-HyperparameterName
=	O
0	B-DatasetName
)	O
;	O
GE	O
(	O
c	O
)	O
:	O
all	O
four	O
TLMs	O
in	O
Eq	O
.	O
(	O
9	O
)	O
.	O

We	O
consider	O
two	O
sequence	O
labeling	O
tasks	O
:	O
Twitter	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
(	O
POS	O
)	O
tagging	O
(	O
Gimpel	O
et	O
al	O
,	O
2011	O
)	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
;	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
.	O
Twitter	O
Part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
Speech	O
(	O
POS	O
)	O
Tagging	O
.	O
We	O
use	O
the	O
Twitter	O
POS	O
data	O
from	O
Gimpel	O
et	O
al	O
(	O
2011	O
)	O
and	O
Owoputi	O
et	O
al	O
(	O
2013	O
)	O
which	O
contain	O
25	O
tags	O
.	O
We	O
use	O
100	O
-	O
dimensional	O
skip	O
-	O
gram	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
embeddings	O
from	O
Tu	O
et	O
al	O
(	O
2017	O
)	O
.	O
Like	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
,	O
we	O
use	O
a	O
BiLSTM	B-MethodName
to	O
compute	O
the	O
input	O
feature	O
vector	O
for	O
each	O
position	O
,	O
using	O
hidden	O
size	O
100	O
.	O
We	O
also	O
use	O
BiLSTMs	O
for	O
the	O
inference	O
networks	O
.	O
The	O
output	O
of	O
the	O
inference	O
network	O
is	O
a	O
softmax	B-MethodName
function	O
,	O
so	O
the	O
inference	O
network	O
will	O
produce	O
a	O
distribution	O
over	O
labels	O
at	O
each	O
position	O
.	O
The	O
∆	O
is	O
L1	O
distance	O
.	O
We	O
train	O
the	O
inference	O
network	O
using	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
(	O
SGD	B-MethodName
)	O
with	O
momentum	O
and	O
train	O
the	O
energy	O
parameters	O
using	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
.	O
We	O
also	O
explore	O
training	O
the	O
inference	O
network	O
using	O
Adam	B-MethodName
when	O
not	O
using	O
the	O
local	O
CE	O
loss	B-MetricName
.	O
1	O
In	O
experiments	O
with	O
the	O
local	O
CE	O
term	O
,	O
its	O
weight	O
is	O
set	O
to	O
1	O
.	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
.	O
We	O
use	O
the	O
CoNLL	B-DatasetName
2003	I-DatasetName
English	O
dataset	O
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
.	O
We	O
use	O
the	O
BIOES	O
tagging	O
scheme	O
,	O
following	O
previous	O
work	O
(	O
Ratinov	O
and	O
Roth	O
,	O
2009	O
)	O
,	O
resulting	O
in	O
17	O
NER	B-TaskName
labels	O
.	O
We	O
use	O
100	O
-	O
dimensional	O
pretrained	O
GloVe	B-MethodName
embeddings	I-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
task	O
is	O
evaluated	O
using	O
F1	B-MetricName
score	I-MetricName
computed	O
with	O
the	O
conlleval	O
script	O
.	O
The	O
architectures	O
for	O
the	O
feature	O
networks	O
in	O
the	O
energy	O
function	O
and	O
inference	O
networks	O
are	O
all	O
BiLSTMs	O
.	O
The	O
architectures	O
for	O
tag	O
language	O
models	O
are	O
LSTMs	O
.	O
We	O
use	O
a	O
dropout	O
keep	O
-	O
prob	O
of	O
0.7	O
for	O
all	O
LSTM	B-MethodName
cells	O
.	O
The	O
hidden	O
size	O
for	O
all	O
LSTMs	O
is	O
128	O
.	O
We	O
use	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
and	O
do	O
early	B-MethodName
stopping	I-MethodName
on	O
the	O
development	O
set	O
.	O
We	O
use	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5	O
10	O
−4	O
.	O
Similar	O
to	O
above	O
,	O
the	O
weight	O
for	O
the	O
CE	O
term	O
is	O
set	O
to	O
1	O
.	O
We	O
consider	O
three	O
NER	B-TaskName
modeling	O
configurations	O
.	O
NER	B-TaskName
uses	O
only	O
words	O
as	O
input	O
and	O
pretrained	O
,	O
fixed	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
.	O
The	O
inference	O
network	O
architecture	O
is	O
a	O
one	O
-	O
layer	O
BiLSTM	B-MethodName
.	O
GloVe	B-MethodName
embeddings	I-MethodName
.	O
NER+	O
uses	O
words	O
,	O
the	O
case	O
of	O
the	O
first	O
letter	O
,	O
POS	O
tags	O
,	O
and	O
chunk	O
labels	O
,	O
as	O
well	O
as	O
pretrained	O
GloVe	B-MethodName
embeddings	I-MethodName
with	O
fine	O
-	O
tuning	O
.	O
NER++	O
includes	O
everything	O
in	O
NER+	O
as	O
well	O
as	O
character	O
-	O
based	O
word	O
representations	O
obtained	O
using	O
a	O
convolutional	O
network	O
over	O
the	O
character	O
sequence	O
in	O
each	O
word	O
.	O
Unless	O
otherwise	O
indicated	O
,	O
our	O
SPENs	O
use	O
the	O
energy	O
in	O
Eq	O
.	O
(	O
7	O
)	O
.	O

Effect	O
of	O
Zero	O
Truncation	O
and	O
Local	O
CE	O
Loss	O
.	O
Table	O
1	O
shows	O
results	O
for	O
zero	O
truncation	O
and	O
the	O
local	O
CE	O
term	O
.	O
Training	O
fails	O
for	O
both	O
tasks	O
when	O
using	O
zero	O
truncation	O
without	O
CE	O
.	O
Removing	O
truncation	O
makes	O
learning	O
succeed	O
and	O
leads	O
to	O
effective	O
models	O
even	O
without	O
using	O
CE	O
.	O
However	O
,	O
when	O
using	O
the	O
local	O
CE	O
term	O
,	O
truncation	O
has	O
little	O
effect	O
on	O
performance	O
.	O
The	O
importance	O
of	O
CE	O
in	O
prior	O
work	O
(	O
Tu	O
and	O
Gimpel	O
,	O
2018	O
)	O
is	O
likely	O
due	O
to	O
the	O
fact	O
that	O
truncation	O
was	O
being	O
used	O
.	O
The	O
local	O
CE	O
term	O
is	O
useful	O
for	O
both	O
tasks	O
,	O
though	O
it	O
appears	O
more	O
helpful	O
for	O
tagging	O
.	O
2	O
This	O
may	O
be	O
because	O
POS	O
tagging	O
is	O
a	O
more	O
local	O
task	O
.	O
Regardless	O
,	O
for	O
both	O
tasks	O
,	O
as	O
shown	O
in	O
Section	O
4.2	O
,	O
the	O
inclusion	O
of	O
the	O
CE	O
term	O
speeds	O
convergence	O
and	O
improves	O
training	O
stability	O
.	O
For	O
example	O
,	O
on	O
NER	B-TaskName
,	O
using	O
the	O
CE	O
term	O
reduces	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
chosen	O
by	O
early	B-MethodName
stopping	I-MethodName
from	O
∼100	O
to	O
∼25	O
.	O
For	O
POS	O
,	O
using	O
the	O
CE	O
term	O
reduces	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
from	O
∼150	O
to	O
∼60	O
.	O
Effect	O
of	O
Compound	O
Objective	O
and	O
Joint	O
Parameterizations	O
.	O
The	O
compound	O
objective	O
is	O
the	O
sum	O
of	O
the	O
margin	O
-	O
rescaled	O
and	O
perceptron	O
losses	O
,	O
and	O
outperforms	O
them	O
both	O
(	O
see	O
Table	O
2	O
)	O
.	O
Across	O
all	O
tasks	O
,	O
the	O
shared	O
and	O
stacked	O
parameterizations	O
are	O
more	O
accurate	O
than	O
the	O
previous	O
objectives	O
.	O
For	O
the	O
separated	O
parameterization	O
,	O
the	O
performance	O
drops	O
slightly	O
for	O
NER	B-TaskName
,	O
likely	O
due	O
to	O
the	O
larger	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
.	O
The	O
shared	O
and	O
stacked	O
options	O
have	O
fewer	O
parameters	O
to	O
train	O
than	O
the	O
separated	O
option	O
,	O
and	O
the	O
stacked	O
version	O
processes	O
examples	O
at	O
the	O
fastest	O
rate	O
during	O
training	O
.	O
The	O
top	O
part	O
of	O
Table	O
3	O
shows	O
how	O
the	O
performance	O
of	O
the	O
test	O
-	O
time	O
inference	O
network	O
A	O
Ψ	O
and	O
the	O
cost	O
-	O
augmented	O
inference	O
network	O
F	O
Φ	O
vary	O
when	O
using	O
the	O
new	O
compound	O
objective	O
.	O
The	O
differences	O
between	O
F	O
Φ	O
and	O
A	O
Ψ	O
are	O
larger	O
than	O
in	O
the	O
baseline	O
configuration	O
,	O
showing	O
that	O
the	O
two	O
are	O
learning	O
complementary	O
functionality	O
.	O
With	O
the	O
stacked	O
parameterization	O
,	O
the	O
cost	O
-	O
augmented	O
network	O
F	O
Φ	O
receives	O
as	O
an	O
additional	O
input	O
the	O
gold	O
standard	O
label	O
sequence	O
,	O
which	O
leads	O
to	O
the	O
largest	O
differences	O
as	O
the	O
cost	O
-	O
augmented	O
network	O
can	O
explicitly	O
favor	O
incorrect	O
labels	O
.	O
3	O
The	O
bottom	O
part	O
of	O
Table	O
3	O
shows	O
qualitative	O
differences	O
between	O
the	O
two	O
inference	O
networks	O
.	O
On	O
the	O
POS	O
development	O
set	O
,	O
we	O
count	O
the	O
differences	O
between	O
the	O
predictions	O
of	O
A	O
Ψ	O
and	O
F	O
Φ	O
when	O
A	O
Ψ	O
makes	O
the	O
correct	O
prediction	O
.	O
4	O
F	O
Φ	O
tends	O
to	O
output	O
tags	O
that	O
are	O
highly	O
confusable	O
with	O
those	O
output	O
by	O
A	O
Ψ	O
.	O
For	O
example	O
,	O
it	O
often	O
outputs	O
proper	O
noun	O
when	O
the	O
gold	O
standard	O
is	O
common	O
noun	O
or	O
vice	O
versa	O
.	O
It	O
also	O
captures	O
the	O
ambiguities	O
among	O
adverbs	O
,	O
adjectives	O
,	O
and	O
prepositions	O
.	O
Global	O
Energies	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
4	O
.	O
Adding	O
the	O
backward	O
(	O
b	O
)	O
and	O
word	O
-	O
augmented	O
TLMs	O
(	O
c	O
)	O
improves	O
over	O
using	O
only	O
the	O
forward	O
TLM	O
from	O
Tu	O
and	O
Gimpel	O
(	O
2018	O
)	O
.	O
With	O
the	O
global	O
energies	O
,	O
our	O
performance	O
is	O
comparable	O
to	O
several	O
strong	O
results	O
(	O
90.94	O
of	O
Lample	O
et	O
al	O
,	O
2016	O
and91.37	O
of	O
Ma	O
andHovy	O
,	O
2016	O
)	O
.	O
However	O
,	O
it	O
is	O
still	O
lower	O
than	O
the	O
state	O
of	O
the	O
art	O
(	O
Akbik	O
et	O
al	O
,	O
2018	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
likely	O
due	O
to	O
the	O
lack	O
of	O
contextualized	O
embeddings	O
.	O
In	O
other	O
work	O
,	O
we	O
proposed	O
and	O
evaluated	O
several	O
other	O
high	O
-	O
order	O
energy	O
terms	O
for	O
sequence	O
labeling	O
using	O
our	O
framework	O
(	O
Tu	O
et	O
al	O
,	O
2020a	O
)	O
.	O

There	O
are	O
several	O
efforts	O
aimed	O
at	O
stabilizing	O
and	O
improving	O
learning	O
in	O
generative	O
adversarial	O
networks	O
(	O
GANs	O
)	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
;	O
Salimans	O
et	O
al	O
,	O
2016	O
;	O
Zhao	O
et	O
al	O
,	O
2017	O
;	O
from	O
overcoming	O
learning	O
difficulties	O
by	O
modifying	O
loss	B-MetricName
functions	O
and	O
optimization	O
,	O
and	O
GANs	O
have	O
become	O
more	O
successful	O
and	O
popular	O
as	O
a	O
result	O
.	O
Notably	O
,	O
Wasserstein	O
GANs	O
provided	O
the	O
first	O
convergence	O
measure	O
in	O
GAN	B-MethodName
training	O
using	O
Wasserstein	O
distance	O
.	O
To	O
compute	O
Wasserstein	O
distance	O
,	O
the	O
discriminator	O
uses	O
weight	O
clipping	O
,	O
which	O
limits	O
network	O
capacity	O
.	O
Weight	O
clipping	O
was	O
subsequently	O
replaced	O
with	O
a	O
gradient	O
norm	O
constraint	O
(	O
Gulrajani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Miyato	O
et	O
al	O
(	O
2018	O
)	O
proposed	O
a	O
novel	O
weight	B-MethodName
normalization	I-MethodName
technique	O
called	O
spectral	B-MethodName
normalization	I-MethodName
.	O
These	O
methods	O
may	O
be	O
applicable	O
to	O
the	O
similar	O
optimization	O
problems	O
solved	O
in	O
learning	O
SPENs	O
.	O
Another	O
direction	O
may	O
be	O
to	O
explore	O
alternative	O
training	O
objectives	O
for	O
SPENs	O
,	O
such	O
as	O
those	O
that	O
use	O
weaker	O
supervision	O
than	O
complete	O
structures	O
(	O
Rooshenas	O
et	O
al	O
,	O
2018	O
(	O
Rooshenas	O
et	O
al	O
,	O
,	O
2019Naskar	O
et	O
al	O
,	O
2020	O
)	O
.	O

We	O
linearize	O
the	O
constituency	B-TaskName
parsing	I-TaskName
outputs	O
,	O
similar	O
to	O
Tran	O
et	O
al	O
(	O
2018	O
)	O
.	O
We	O
use	O
the	O
following	O
equation	O
plus	O
global	O
energy	O
in	O
the	O
form	O
of	O
Eq	O
.	O
(	O
8	O
)	O
as	O
the	O
energy	O
function	O
:	O
E	O
Θ	B-HyperparameterName
(	O
x	O
,	O
y	O
)	O
=	O
−	O
T	O
t=1	O
L	O
j=1	O
y	O
t	O
,	O
j	O
U	O
j	O
b	O
(	O
x	O
,	O
t	O
)	O
+	O
T	O
t=1	O
y	O
t−1	O
W	O
y	O
t	O
Here	O
,	O
b	O
has	O
a	O
seq2seq	B-MethodName
-	O
with	O
-	O
attention	O
architecture	O
identical	O
to	O
Tran	O
et	O
al	O
(	O
2018	O
)	O
.	O
In	O
particular	O
,	O
here	O
is	O
the	O
list	O
of	O
implementation	O
decisions	O
.	O
We	O
can	O
write	O
b	O
=	O
g	O
f	O
where	O
f	O
(	O
which	O
we	O
call	O
the	O
"	O
feature	O
network	O
"	O
)	O
takes	O
in	O
an	O
input	O
sentence	O
,	O
passes	O
it	O
through	O
the	O
encoder	O
,	O
and	O
passes	O
the	O
encoder	O
output	O
to	O
the	O
decoder	O
feature	O
layer	O
to	O
obtain	O
hidden	O
states	O
;	O
g	O
takes	O
in	O
the	O
hidden	O
states	O
and	O
passes	O
them	O
into	O
the	O
rest	O
of	O
the	O
layers	O
in	O
the	O
decoder	O
.	O
In	O
our	O
experiments	O
,	O
the	O
cost	O
-	O
augmented	O
inference	O
network	O
F	O
Φ	O
,	O
test	O
-	O
time	O
inference	O
network	O
A	O
Ψ	O
,	O
and	O
b	O
of	O
the	O
energy	O
function	O
above	O
share	O
the	O
same	O
feature	O
network	O
(	O
defined	O
as	O
f	O
above	O
)	O
.	O
The	O
feature	O
network	O
(	O
f	O
)	O
component	O
of	O
b	O
is	O
pretrained	O
using	O
the	O
feed	O
-	O
forward	O
local	O
crossentropy	O
objective	O
.	O
The	O
cost	O
-	O
augmented	O
inference	O
network	O
F	O
Φ	O
and	O
the	O
test	O
-	O
time	O
inference	O
network	O
A	O
Ψ	O
are	O
both	O
pretrained	O
using	O
the	O
feed	O
-	O
forward	O
local	O
cross	O
-	O
entropy	O
objective	O
.	O
The	O
seq2seq	B-MethodName
baseline	O
achieves	O
82.80	O
F1	B-MetricName
on	O
the	O
development	O
set	O
in	O
our	O
replication	O
of	O
Tran	O
et	O
al	O
(	O
2018	O
)	O
.	O
Using	O
a	O
SPEN	O
with	O
our	O
stacked	O
parameterization	O
,	O
we	O
obtain	O
83.22	O
F1	B-MetricName
.	O

Subtask	O
1	O
requires	O
a	O
detection	O
model	O
that	O
uses	O
only	O
the	O
textual	O
features	O
of	O
the	O
meme	O
content	O
and	O
detects	O
which	O
of	O
the	O
20	O
propaganda	O
techniques	O
were	O
used	O
.	O
This	O
is	O
a	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
problem	O
for	O
text	O
,	O
based	O
on	O
the	O
pre	O
-	O
trained	O
ALBERT	B-MethodName
model	O
and	O
added	O
a	O
Text	O
-	O
CNN	O
layer	O
.	O
As	O
illustrated	O
in	O
Figure	O
2	O
,	O
the	O
proposed	O
model	O
includes	O
an	O
ALBERT	B-MethodName
layer	O
,	O
a	O
Text	O
-	O
CNN	O
layer	O
,	O
a	O
fully	O
connected	O
layer	O
,	O
and	O
an	O
output	O
layer	O
.	O
ALBERT	B-MethodName
(	O
Lan	O
et	O
al	O
,	O
2020	O
)	O
is	O
a	O
lite	O
BERT	B-MethodName
for	O
self	B-TaskName
-	I-TaskName
supervised	I-TaskName
learning	I-TaskName
of	O
language	O
representations	O
,	O
which	O
uses	O
layer	O
-	O
to	O
-	O
layer	O
parameter	O
sharing	O
to	O
reduce	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
of	O
the	O
model	O
,	O
which	O
not	O
only	O
speeds	O
up	O
the	O
model	O
training	O
but	O
also	O
outperforms	O
BERT	B-MethodName
on	O
certain	O
datasets	O
.	O
With	O
our	O
model	O
,	O
the	O
pretrained	O
ALBERT	B-MethodName
model	O
is	O
fine	O
-	O
tuned	O
to	O
obtain	O
a	O
512	O
×	O
768	O
hidden	O
representation	O
matrix	O
for	O
subsequent	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
of	O
text	O
.	O
Text	O
-	O
CNN	O
(	O
Kim	O
,	O
2014	O
)	O
is	O
a	O
convolutional	O
neural	O
network	O
applied	O
to	O
a	O
text	B-TaskName
classification	I-TaskName
task	O
,	O
using	O
multiple	O
kernels	O
of	O
different	O
sizes	O
to	O
extract	O
key	O
information	O
in	O
sentences	O
,	O
and	O
is	O
thus	O
able	O
to	O
better	O
capture	O
the	O
local	O
relevance	O
.	O
In	O
this	O
layer	O
,	O
we	O
used	O
three	O
different	O
sizes	O
of	O
onedimensional	O
convolution	B-MethodName
kernels	O
,	O
i.e.	O
,	O
3	O
,	O
4	O
,	O
and	O
5	O
,	O
to	O
extract	O
information	O
from	O
the	O
hidden	O
representation	O
matrix	O
output	O
from	O
the	O
ALBERT	B-MethodName
layer	O
for	O
the	O
final	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
text	I-TaskName
classification	I-TaskName
.	O

All	O
models	O
used	O
the	O
TensorFlow2	O
backend	O
,	O
and	O
all	O
BERT	B-MethodName
-	O
based	O
models	O
were	O
implemented	O
using	O
the	O
HuggingFace	O
Transformers	O
toolkit	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Ba	O
and	O
Kingma	O
,	O
2015	O
)	O
was	O
used	O
to	O
update	O
all	O
trainable	O
parameters	O
.	O
The	O
loss	B-MetricName
functions	O
in	O
subtasks	O
1	O
and	O
3	O
were	O
binary	O
cross	O
-	O
entropy	O
,	O
and	O
subtask	O
2	O
was	O
categorical	O
cross	O
-	O
entropy	O
.	O
The	O
hyper	O
-	O
parameters	O
in	O
the	O
model	O
training	O
process	O
were	O
obtained	O
using	O
a	O
grid	O
-	O
search	O
strategy	O
,	O
as	O
shown	O
in	O
Table	O
1	O
.	O
Once	O
the	O
optimal	O
settings	O
of	O
the	O
parameters	O
were	O
obtained	O
,	O
they	O
were	O
used	O
for	O
classification	O
on	O
the	O
test	O
sets	O
of	O
different	O
corpora	O
.	O

Table	O
2	O
presents	O
the	O
results	O
of	O
Subtask	O
1	O
.	O
We	O
conducted	O
experiments	O
on	O
several	O
pre	O
-	O
trained	O
models	O
including	O
BERT	B-MethodName
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
ALBERT	B-MethodName
combined	O
with	O
the	O
Text	O
-	O
CNN	O
layer	O
,	O
and	O
observed	O
that	O
the	O
ALBERT	B-MethodName
and	O
Text	O
-	O
CNN	O
models	O
achieved	O
the	O
best	O
performance	O
,	O
the	O
reason	O
for	O
which	O
may	O
be	O
that	O
the	O
training	O
datasets	O
are	O
small	O
,	O
and	O
a	O
serious	O
overfitting	O
will	O
occur	O
by	O
directly	O
finetuning	O
BERT	B-MethodName
.	O
Furthermore	O
,	O
the	O
experiments	O
show	O
that	O
the	O
ALBERT	B-MethodName
model	O
has	O
fewer	O
parameters	O
and	O
performs	O
better	O
on	O
small	O
datasets	O
.	O
Adding	O
a	O
Text	O
-	O
CNN	O
layer	O
after	O
the	O
BERT	B-MethodName
-	O
based	O
model	O
can	O
better	O
extract	O
the	O
local	O
relevance	O
information	O
of	O
the	O
text	O
,	O
which	O
not	O
only	O
effectively	O
alleviates	O
the	O
overfitting	O
phenomenon	O
it	O
also	O
effectively	O
improves	O
the	O
model	O
performance	O
.	O
In	O
subtask	O
2	O
,	O
the	O
results	O
of	O
our	O
proposed	O
multitask	O
sequence	O
labeling	O
model	O
on	O
the	O
dev	O
set	O
are	O
F	O
1	O
-	O
score	O
of	O
0.215	O
,	O
Precision	B-MetricName
of	O
0.378	O
,	O
and	O
Recall	B-MetricName
of	O
0.151	O
.	O
The	O
results	O
on	O
the	O
test	O
set	O
are	O
F	O
1	O
-	O
score	O
of	O
0.091	O
,	O
Precision	B-MetricName
of	O
0.186	O
,	O
and	O
Recall	B-MetricName
of	O
0.061	O
.	O
Table	O
3	O
shows	O
the	O
results	O
of	O
Subtask	O
3	O
.	O
It	O
can	O
be	O
observed	O
that	O
ResNet18	O
works	O
better	O
than	O
VGG16	O
when	O
using	O
both	O
ALBERT	B-MethodName
and	O
ALBERT	B-MethodName
-	O
Text	O
-	O
CNN	O
models	O
.	O
The	O
performance	O
was	O
improved	O
by	O
adding	O
a	O
Text	O
-	O
CNN	O
layer	O
to	O
the	O
text	O
channel	O
.	O
Considering	O
that	O
the	O
micro	O
F	O
1	O
-	O
scores	O
are	O
relatively	O
close	O
,	O
we	O
selected	O
the	O
models	O
with	O
the	O
top	O
-	O
three	O
F	O
1	O
-	O
scores	O
and	O
used	O
hard	O
voting	O
to	O
generate	O
the	O
results	O
for	O
comparison	O
.	O
For	O
all	O
three	O
subtasks	O
,	O
the	O
proposed	O
systems	O
achieved	O
micro	O
F	O
1	O
-	O
scores	O
of	O
0.492	O
,	O
0.091	O
,	O
and	O
0.446	O
on	O
the	O
test	O
set	O
,	O
respectively	O
.	O
The	O
results	O
of	O
all	O
models	O
exceeded	O
the	O
baseline	O
.	O
However	O
,	O
there	O
is	O
a	O
considerable	O
decrease	O
compared	O
to	O
the	O
scores	O
of	O
0.625	O
,	O
0.215	O
,	O
and	O
0.636	O
achieved	O
on	O
the	O
dev	O
set	O
.	O

Mclarty	O
et	O
al	O
(	O
2018	O
)	O
trained	O
a	O
Support	B-MethodName
Vector	I-MethodName
Machine	I-MethodName
(	O
SVM	B-MethodName
)	O
on	O
pre	O
-	O
vocalic	O
/r/	O
and	O
vowels	O
,	O
and	O
their	O
approach	O
did	O
quite	O
well	O
in	O
classifying	O
prevocalic	O
/r	O
/	O
s.	O
They	O
then	O
took	O
this	O
pre	O
-	O
trained	O
model	O
and	O
applied	O
it	O
to	O
classifying	O
postvocalic	O
/r/	O
tokens	O
,	O
which	O
classified	O
84	O
%	O
as	O
vowels	O
,	O
and	O
15	O
%	O
as	O
/r/.	O
As	O
they	O
describe	O
,	O
this	O
is	O
likely	O
because	O
all	O
postvocalic	O
segments	O
still	O
contain	O
vowel	O
-	O
like	O
properties	O
;	O
furthermore	O
,	O
their	O
training	O
set	O
excluded	O
postvocalic	O
/r/	O
so	O
the	O
accuracy	B-MetricName
is	O
expected	O
to	O
decrease	O
.	O
However	O
,	O
their	O
method	O
did	O
not	O
perform	O
as	O
well	O
in	O
comparison	O
to	O
humans	O
.	O
On	O
tokens	O
where	O
there	O
was	O
no	O
ground	O
truth	O
,	O
humans	O
only	O
agreed	O
with	O
the	O
SVM	B-MethodName
classification	O
about	O
55	O
%	O
of	O
the	O
time	O
.	O

In	O
early	O
testing	O
,	O
we	O
attempted	O
classification	O
into	O
rful	O
,	O
r	O
-	O
less	O
,	O
and	O
unknown	O
,	O
but	O
this	O
did	O
not	O
provide	O
strong	O
results	O
so	O
we	O
simplified	O
to	O
a	O
binary	O
classification	O
.	O
From	O
the	O
beginning	O
of	O
this	O
project	O
,	O
we	O
knew	O
we	O
wanted	O
to	O
use	O
a	O
machine	O
learning	O
approach	O
,	O
so	O
before	O
using	O
neural	O
networks	O
we	O
tried	O
some	O
easier	O
classifiers	O
.	O
However	O
,	O
we	O
did	O
not	O
get	O
encouraging	O
results	O
.	O
For	O
example	O
,	O
our	O
Random	O
Forest	O
Classifier	O
only	O
gave	O
about	O
54	O
%	O
accuracy	B-MetricName
.	O
When	O
we	O
tried	O
simpler	O
neural	O
networks	O
,	O
these	O
gave	O
much	O
more	O
promising	O
results	O
to	O
we	O
chose	O
to	O
pursue	O
this	O
method	O
.	O

Following	O
standard	O
methods	O
of	O
Automatic	B-TaskName
Speech	I-TaskName
Recognition	I-TaskName
,	O
we	O
converted	O
the	O
audio	O
to	O
12	O
Mel	O
-	O
Frequency	O
-	O
Cepstral	O
-	O
Coefficients	O
(	O
MFCCs	O
)	O
.	O
We	O
used	O
the	O
12	O
MFCCs	O
,	O
similar	O
to	O
Mclarty	O
et	O
al	O
.	O
For	O
each	O
vowel+	O
(	O
r	O
)	O
sequence	O
,	O
we	O
normalized	O
across	O
the	O
length	O
to	O
extract	O
100	O
time	O
-	O
points	O
per	O
token	O
,	O
as	O
shown	O
in	O
figure	O
1	O
.	O
In	O
the	O
training	O
,	O
MFCCs	O
were	O
more	O
effective	O
than	O
traditional	O
sociophonetic	O
/r/	O
correlates	O
F2	O
and	O
F3	O
(	O
Thomas	O
,	O
2011	O
)	O
.	O
These	O
samples	O
were	O
used	O
in	O
the	O
model	O
architecture	O
as	O
shown	O
in	O
figure	O
1	O
,	O
where	O
there	O
are	O
100	O
samples	O
for	O
each	O
vowel	O
+	O
/r/	O
sequence	O
.	O
The	O
Gated	B-MethodName
Recurrent	I-MethodName
Unit	I-MethodName
is	O
shown	O
in	O
more	O
detail	O
in	O
figure	O
2	O
,	O
where	O
we	O
can	O
see	O
the	O
input	O
from	O
the	O
previous	O
timestep	O
and	O
layer	O
,	O
and	O
how	O
this	O
is	O
filtered	O
through	O
gates	O
using	O
tanh	O
and	O
sigmoid	B-MethodName
activation	I-MethodName
functions	O
.	O
Importantly	O
,	O
no	O
work	O
on	O
coding	O
rhoticity	O
has	O
made	O
use	O
of	O
Recurrent	O
Neural	O
Networks	O
,	O
and	O
we	O
believe	O
our	O
methods	O
are	O
a	O
promising	O
step	O
.	O
We	O
used	O
Gated	O
Recurrent	O
Units	O
Chung	O
et	O
al	O
,	O
2014	O
)	O
to	O
train	O
our	O
system	O
to	O
classify	O
vowel+	O
(	O
r	O
)	O
tokens	O
as	O
r	O
-	O
ful	O
or	O
r	O
-	O
less	O
.	O
Following	O
standard	O
methods	O
in	O
machine	O
-	O
learning	O
,	O
we	O
split	O
the	O
data	O
in	O
order	O
to	O
train	O
with	O
80	O
%	O
of	O
the	O
data	O
and	O
test	O
with	O
20	O
%	O
.	O
We	O
chose	O
hyperparameters	O
based	O
on	O
a	O
grid	O
search	O
using	O
3	O
-	O
fold	O
cross	O
validation	O
(	O
only	O
3	O
due	O
to	O
the	O
small	O
dataset	O
)	O
.	O
We	O
saved	O
the	O
test	O
set	O
to	O
validate	O
results	O
.	O
The	O
hidden	B-HyperparameterName
layer	I-HyperparameterName
size	I-HyperparameterName
was	O
50	O
nodes	O
,	O
and	O
dense	O
layer	O
size	O
was	O
200	O
nodes	O
.	O
For	O
regularization	O
we	O
used	O
a	O
kernel	O
L2	B-HyperparameterName
regularization	I-HyperparameterName
for	O
the	O
dense	O
layer	O
and	O
we	O
used	O
both	O
activation	O
L2	O
and	O
Recurrent	O
L2	O
for	O
the	O
GRU	B-MethodName
layer	O
.	O
All	O
of	O
the	O
alphas	O
for	O
this	O
regularization	O
are	O
0.01	O
.	O
The	O
optimization	O
method	O
was	O
RMSprop	B-MethodName
,	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
was	O
0.001	O
.	O

In	O
figure	O
3	O
,	O
we	O
see	O
the	O
Normalized	O
Confusion	O
Matrix	O
,	O
which	O
summarizes	O
our	O
results	O
by	O
lining	O
up	O
true	O
labels	O
and	O
predicted	O
labels	O
for	O
our	O
rhotic	O
and	O
non	O
-	O
rhotic	O
tokens	O
.	O
We	O
consider	O
this	O
binary	O
classification	O
either	O
rhotic	O
(	O
positive	O
)	O
or	O
non	O
-	O
rhotic	O
(	O
negative	O
)	O
.	O
In	O
this	O
way	O
we	O
can	O
see	O
the	O
proportion	O
of	O
true	O
positives	O
(	O
predicted	O
to	O
be	O
rhotic	O
and	O
indeed	O
truly	O
rhotic	O
)	O
,	O
false	O
positive	O
(	O
predicted	O
to	O
be	O
rhotic	O
but	O
actually	O
non	O
-	O
rhotic	O
)	O
,	O
true	O
negative	O
(	O
predicted	O
to	O
be	O
non	O
-	O
rhotic	O
and	O
actually	O
non	O
-	O
rhotic	O
)	O
,	O
and	O
false	O
negative	O
(	O
predicted	O
to	O
be	O
non	O
-	O
rhotic	O
and	O
actually	O
rhotic	O
)	O
.	O
In	O
deciding	O
which	O
model	O
to	O
use	O
,	O
we	O
tried	O
a	O
few	O
different	O
configurations	O
.	O
We	O
used	O
the	O
sampled	O
MFCCs	O
(	O
as	O
described	O
earlier	O
,	O
figure	O
1	O
)	O
as	O
well	O
as	O
Bark	O
measurements	O
that	O
were	O
extracted	O
also	O
at	O
100	O
time	O
-	O
points	O
across	O
the	O
vowel	O
.	O
Because	O
our	O
MFCC	O
data	O
is	O
multi	O
-	O
dimensional	O
and	O
time	O
-	O
dependent	O
,	O
we	O
wanted	O
to	O
see	O
how	O
a	O
Convolutional	O
Neural	O
Network	O
would	O
perform	O
(	O
table	O
1	O
)	O
,	O
but	O
it	O
turned	O
out	O
not	O
to	O
be	O
as	O
high	O
in	O
performance	O
as	O
our	O
earlier	O
model	O
.	O
Figure	O
4	O
shows	O
the	O
Receiver	O
Operating	O
Characteristic	O
(	O
ROC	O
)	O
for	O
our	O
model	O
(	O
created	O
using	O
scikitlearn	O
)	O
,	O
which	O
is	O
fairly	O
good	O
by	O
machine	O
learning	O
standards	O
.	O
The	O
Area	O
Under	O
the	O
Curve	O
(	O
AUC	B-MetricName
,	O
as	O
noted	O
in	O
Table	O
1	O
)	O
is	O
0.892	O
,	O
and	O
as	O
evident	O
from	O
the	O
graph	O
,	O
is	O
much	O
closer	O
to	O
1	O
.	O
Our	O
system	O
had	O
81.1	O
%	O
accuracy	B-MetricName
with	O
the	O
human	O
analysts	O
in	O
judging	O
tokens	O
as	O
r	O
-	O
less	O
or	O
r	O
-	O
ful	O
,	O
scoring	O
0.829	O
for	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
.	O
We	O
also	O
used	O
the	O
Heselwood	O
et	O
al	O
approach	O
(	O
section	O
3.1	O
)	O
of	O
classifying	O
front	O
or	O
back	O
vowels	O
to	O
see	O
how	O
accurately	O
it	O
would	O
perform	O
on	O
the	O
same	O
test	O
dataset	O
.	O
This	O
classification	O
gave	O
an	O
average	O
speaker	O
accuracy	B-MetricName
of	O
63.3	O
%	O
and	O
an	O
average	O
token	O
accuracy	B-MetricName
of	O
62.1	O
%	O
(	O
Table	O
2	O
)	O
,	O
much	O
lower	O
than	O
our	O
best	O
model	O
's	O
overall	B-MetricName
accuracy	I-MetricName
(	O
i.e.	O
average	O
across	O
all	O
tokens	O
)	O
of	O
81.1	O
%	O
(	O
Table	O
1	O
)	O
.	O
Average	O
Speaker	O
Accuracy	B-MetricName
63.3	O
%	O
Average	O
Token	O
Accuracy	B-MetricName
62.1	O
%	O

The	O
initial	O
results	O
of	O
this	O
study	O
are	O
promising	O
.	O
Our	O
results	O
are	O
quite	O
strong	O
,	O
as	O
shown	O
by	O
the	O
metrics	O
in	O
Table	O
1	O
.	O
When	O
testing	O
the	O
Heselwood	O
et	O
al	O
approach	O
(	O
Table	O
2	O
)	O
,	O
it	O
only	O
predicted	O
correctly	O
approximately	O
60	O
%	O
of	O
the	O
time	O
;	O
our	O
model	O
performs	O
significantly	O
better	O
,	O
at	O
an	O
accuracy	B-MetricName
of	O
81.1	O
%	O
(	O
Table	O
1	O
)	O
.	O
It	O
seems	O
that	O
we	O
are	O
also	O
slightly	O
better	O
at	O
predicting	O
rhotic	O
tokens	O
than	O
non	O
-	O
rhotic	O
(	O
Figure	O
3	O
)	O
,	O
which	O
likely	O
has	O
to	O
do	O
with	O
the	O
fact	O
that	O
we	O
have	O
more	O
rhotic	O
tokens	O
in	O
total	O
.	O
We	O
aimed	O
to	O
reach	O
human	O
levels	O
-	O
considering	O
that	O
analyst	O
agreement	O
is	O
89.9	O
%	O
for	O
our	O
dataset	O
(	O
as	O
mentioned	O
above	O
)	O
,	O
our	O
accuracy	B-MetricName
of	O
81.1	O
%	O
is	O
quite	O
good	O
.	O
However	O
,	O
these	O
numbers	O
are	O
not	O
strictly	O
comparable	O
as	O
we	O
discarded	O
tokens	O
that	O
proved	O
difficult	O
for	O
human	O
analysts	O
.	O
In	O
future	O
development	O
of	O
this	O
method	O
,	O
we	O
want	O
to	O
consider	O
any	O
sources	O
of	O
error	O
on	O
our	O
part	O
.	O
For	O
example	O
,	O
some	O
audio	O
and	O
text	O
files	O
could	O
be	O
misaligned	O
so	O
we	O
might	O
consider	O
hand	O
-	O
correcting	O
these	O
alignments	O
.	O
However	O
,	O
the	O
nature	O
of	O
the	O
neural	O
network	O
could	O
correct	O
for	O
this	O
in	O
that	O
it	O
learns	O
to	O
forget	O
irrelevant	O
or	O
noisy	O
data	O
.	O
By	O
gathering	O
more	O
data	O
,	O
we	O
would	O
expect	O
that	O
our	O
accuracy	B-MetricName
would	O
improve	O
and	O
eventually	O
reach	O
a	O
plateau	O
where	O
additional	O
speakers	O
would	O
not	O
affect	O
anything	O
.	O
Additionally	O
,	O
a	O
study	O
that	O
involves	O
cross	O
-	O
corpus	O
analysis	O
could	O
provide	O
greater	O
insight	O
into	O
how	O
this	O
model	O
might	O
be	O
applicable	O
on	O
a	O
larger	O
scale	O
,	O
and	O
how	O
well	O
our	O
model	O
actually	O
performs	O
.	O
Furthermore	O
,	O
if	O
we	O
had	O
3	O
analysts	O
rather	O
than	O
2	O
,	O
we	O
could	O
have	O
used	O
a	O
majority	O
vote	O
for	O
classifying	O
tokens	O
,	O
and	O
would	O
not	O
have	O
to	O
discard	O
tokens	O
where	O
rhoticity	O
was	O
ambiguous	O
.	O
A	O
shortcoming	O
of	O
this	O
study	O
is	O
that	O
it	O
only	O
involves	O
speech	O
that	O
is	O
elicited	O
through	O
readingideally	O
future	O
studies	O
would	O
involve	O
free	O
speech	O
in	O
order	O
to	O
use	O
more	O
natural	O
speech	O
.	O
R	O
-	O
dropping	O
is	O
a	O
crucial	O
sociolinguistic	O
variable	O
for	O
English	O
dialect	O
research	O
in	O
the	O
US	O
Northeast	O
,	O
Great	O
Britain	O
,	O
Australia	O
,	O
New	O
Zealand	O
,	O
Singapore	O
,	O
and	O
other	O
locations	O
.	O
Our	O
neural	O
network	O
model	O
takes	O
a	O
significant	O
step	O
toward	O
automation	O
of	O
this	O
key	O
variable	O
.	O
In	O
the	O
future	O
,	O
we	O
will	O
continue	O
optimizing	O
and	O
improving	O
our	O
model	O
.	O
Other	O
groups	O
have	O
studied	O
automated	O
methods	O
for	O
coding	O
sociolinguistic	O
variables	O
(	O
Yuan	O
and	O
Liberman	O
,	O
2011	O
;	O
Bailey	O
,	O
2016	O
)	O
,	O
and	O
there	O
are	O
great	O
ideas	O
to	O
be	O
found	O
in	O
these	O
works	O
.	O
When	O
automated	O
methods	O
for	O
rhoticity	O
reach	O
the	O
accuracy	B-MetricName
level	O
of	O
humans	O
,	O
along	O
with	O
consistency	O
and	O
full	O
replicability	O
,	O
this	O
will	O
open	O
the	O
floodgates	O
to	O
large	O
amounts	O
of	O
/r/	O
data	O
and	O
greatly	O
expand	O
sociolinguistic	O
knowledge	O
of	O
dialect	O
variation	O
around	O
the	O
world	O
,	O
efficiently	O
allowing	O
studies	O
to	O
be	O
replicated	O
across	O
research	O
groups	O
.	O

We	O
introduce	O
PubMedQA	B-DatasetName
,	O
a	O
novel	O
biomedical	O
question	B-TaskName
answering	I-TaskName
(	O
QA	O
)	O
dataset	O
collected	O
from	O
PubMed	O
abstracts	O
.	O
The	O
task	O
of	O
Pub	O
-	O
MedQA	O
is	O
to	O
answer	O
research	O
questions	O
with	O
yes	O
/	O
no	O
/	O
maybe	O
(	O
e.g.	O
:	O
Do	O
preoperative	O
statins	O
reduce	O
atrial	O
fibrillation	O
after	O
coronary	O
artery	O
bypass	O
grafting	O
?	O
)	O
using	O
the	O
corresponding	O
abstracts	O
.	O
PubMedQA	B-DatasetName
has	O
1k	O
expert	O
-	O
annotated	O
,	O
61.2k	O
unlabeled	O
and	O
211.3k	O
artificially	O
generated	O
QA	O
instances	O
.	O
Each	O
PubMedQA	B-DatasetName
instance	O
is	O
composed	O
of	O
(	O
1	O
)	O
a	O
question	O
which	O
is	O
either	O
an	O
existing	O
research	O
article	O
title	O
or	O
derived	O
from	O
one	O
,	O
(	O
2	O
)	O
a	O
context	O
which	O
is	O
the	O
corresponding	O
abstract	O
without	O
its	O
conclusion	O
,	O
(	O
3	O
)	O
a	O
long	O
answer	O
,	O
which	O
is	O
the	O
conclusion	O
of	O
the	O
abstract	O
and	O
,	O
presumably	O
,	O
answers	O
the	O
research	O
question	O
,	O
and	O
(	O
4	O
)	O
a	O
yes	O
/	O
no	O
/	O
maybe	O
answer	O
which	O
summarizes	O
the	O
conclusion	O
.	O
Pub	O
-	O
MedQA	O
is	O
the	O
first	O
QA	O
dataset	O
where	O
reasoning	O
over	O
biomedical	O
research	O
texts	O
,	O
especially	O
their	O
quantitative	O
contents	O
,	O
is	O
required	O
to	O
answer	O
the	O
questions	O
.	O
Our	O
best	O
performing	O
model	O
,	O
multi	O
-	O
phase	O
fine	O
-	O
tuning	O
of	O
BioBERT	O
with	O
long	O
answer	O
bag	O
-	O
of	O
-	O
word	O
statistics	O
as	O
additional	O
supervision	O
,	O
achieves	O
68.1	O
%	O
accuracy	B-MetricName
,	O
compared	O
to	O
single	O
human	O
performance	O
of	O
78.0	O
%	O
accuracy	B-MetricName
and	O
majority	O
-	O
baseline	O
of	O
55.2	O
%	O
accuracy	B-MetricName
,	O
leaving	O
much	O
room	O
for	O
improvement	O
.	O
PubMedQA	B-DatasetName
is	O
publicly	O
available	O
at	O
https://pubmedqa.github.io	O
.	O

We	O
fine	O
-	O
tune	O
BioBERT	O
on	O
Pub	O
-	O
MedQA	O
as	O
a	O
baseline	O
.	O
BioBERT	O
is	O
initialized	O
with	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
and	O
further	O
pretrained	O
on	O
PubMed	O
abstracts	O
and	O
PMC	O
7	O
articles	O
.	O
Expectedly	O
,	O
it	O
vastly	O
outperforms	O
BERT	B-MethodName
in	O
various	O
biomedical	O
NLP	O
tasks	O
.	O
We	O
denote	O
the	O
original	O
transformer	O
weights	O
of	O
BioBERT	O
as	O
θ	B-HyperparameterName
0	B-DatasetName
.	O
While	O
fine	O
-	O
tuning	O
,	O
we	O
feed	O
PubMedQA	B-DatasetName
questions	O
and	O
contexts	O
(	O
or	O
long	O
answers	O
)	O
,	O
separated	O
7	O
https://www.ncbi.nlm.nih.gov/pmc/	O
by	O
the	O
special	O
[	O
SEP	O
]	O
token	O
,	O
to	O
BioBERT	O
.	O
The	O
yes	O
/	O
no	O
/	O
maybe	O
labels	O
are	O
predicted	O
using	O
the	O
special	O
[	O
CLS	O
]	O
embedding	O
using	O
a	O
softmax	B-MethodName
function	O
.	O
Cross	O
-	O
entropy	O
loss	B-MetricName
of	O
predicted	O
and	O
true	O
label	O
distribution	O
is	O
denoted	O
as	O
L	O
QA	O
.	O

Under	O
reasoning	O
-	O
required	O
setting	O
,	O
long	O
answers	O
are	O
available	O
in	O
training	O
but	O
not	O
inference	O
phase	O
.	O
We	O
use	O
them	O
as	O
an	O
additional	O
signal	O
for	O
training	O
:	O
similar	O
to	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
regularizing	O
neural	O
machine	B-TaskName
translation	I-TaskName
models	O
with	O
binary	O
bag	O
-	O
of	O
-	O
word	O
(	O
BoW	O
)	O
statistics	O
,	O
we	O
fine	O
-	O
tune	O
BioBERT	O
with	O
an	O
auxiliary	O
task	O
of	O
predicting	O
the	O
binary	O
BoW	O
statistics	O
of	O
the	O
long	O
answers	O
,	O
also	O
using	O
the	O
special	O
[	O
CLS	O
]	O
embedding	O
.	O
We	O
minimize	O
binary	O
crossentropy	O
loss	B-MetricName
of	O
this	O
auxiliary	O
task	O
:	O
L	O
BoW	O
=	O
−	O
1	O
N	O
i	O
b	O
i	O
logb	O
i	O
+	O
(	O
1	O
−	O
b	O
i	O
)	O
log	O
(	O
1	O
−b	O
i	O
)	O
where	O
b	O
i	O
andb	O
i	O
are	O
ground	O
-	O
truth	O
and	O
predicted	O
probability	O
of	O
whether	O
token	O
i	O
is	O
in	O
the	O
long	O
answers	O
(	O
i.e.	O
:	O
b	O
i	O
{	O
0	B-DatasetName
,	O
1	O
}	O
andb	O
i	O
[	O
0	B-DatasetName
,	O
1	O
]	O
)	O
,	O
and	O
N	O
is	O
the	O
BoW	O
vocabulary	O
size	O
.	O
The	O
total	O
loss	B-MetricName
is	O
:	O
L	O
=	O
L	O
QA	O
+	O
βL	O
BoW	O

Training	O
(	O
Reasoning	O
-	O
required	O
)	O
(	O
q	O
A	O
,	O
c	O
A	O
)	O
,	O
l	O
A	O
(	O
q	O
U	O
,	O
c	O
U	O
)	O
,	O
l	O
U	O
pseudo	O
(	O
q	O
L	O
,	O
c	O
L	O
)	O
,	O
l	O
L	O
(	O
q	O
A	O
,	O
a	O
A	O
)	O
,	O
l	O
A	O
(	O
q	O
L	O
,	O
a	O
L	O
)	O
,	O
l	O
L	O
(	O
q	O
U	O
,	O
a	O
U	O
)	O
✓	O
0	B-DatasetName
✓	O
0	B-DatasetName
✓	O
I	O
✓	O
II	O
✓	O
F	O
✓	O
B	O
1	O
✓	O
B	O
2	O
Fine	O
-	O
tuning	O
Supervision	O
Pseudolabeling	O
eq	O
.	O
(	O
1	O
)	O
eq	O
.	O
(	O
2	O
)	O
eq	O
.	O
(	O
3	O
)	O
eq	O
.	O
(	O
5	O
)	O
eq	O
.	O
(	O
6	O
)	O
eq	O
.	O
(	O
4	O
In	O
reasoning	O
-	O
free	O
setting	O
which	O
we	O
use	O
for	O
bootstrapping	O
,	O
the	O
regularization	O
coefficient	O
β	B-HyperparameterName
is	O
set	O
to	O
0	B-DatasetName
because	O
long	O
answers	O
are	O
directly	O
used	O
as	O
input	O
.	O

Since	O
PQA	O
-	O
A	O
and	O
PQA	O
-	O
U	O
have	O
different	O
properties	O
from	O
the	O
ultimate	O
test	O
set	O
of	O
PQA	O
-	O
L	O
,	O
BioBERT	O
is	O
fine	O
-	O
tuned	O
in	O
a	O
multi	O
-	O
phase	O
style	O
on	O
different	O
subsets	O
.	O
Fig	O
.	O
5	O
shows	O
the	O
architecture	O
of	O
this	O
training	O
schedule	O
.	O
We	O
use	O
q	O
,	O
c	O
,	O
a	O
,	O
l	O
to	O
denote	O
question	O
,	O
context	O
,	O
long	O
answer	O
and	O
yes	O
/	O
no	O
/	O
maybe	O
label	O
of	O
instances	O
,	O
respectively	O
.	O
Their	O
source	O
subsets	O
are	O
indexed	O
by	O
the	O
superscripts	O
of	O
A	O
for	O
PQA	O
-	O
A	O
,	O
U	O
for	O
PQA	O
-	O
U	O
and	O
L	O
for	O
PQA	O
-	O
L.	O
Phase	O
I	O
Fine	O
-	O
tuning	O
on	O
PQA	O
-	O
A	O
:	O
PQA	O
-	O
A	O
is	O
automatically	O
collected	O
whose	O
questions	O
and	O
labels	O
are	O
artificially	O
generated	O
.	O
As	O
a	O
result	O
,	O
questions	O
of	O
PQA	O
-	O
A	O
might	O
differ	O
a	O
lot	O
from	O
those	O
of	O
PQA	O
-	O
U	O
and	O
PQA	O
-	O
L	O
,	O
and	O
it	O
only	O
has	O
yes	O
/	O
no	O
labels	O
with	O
a	O
very	O
imbalanced	O
distribution	O
(	O
92.8	O
%	O
yes	O
v.s.	O
7.2	O
%	O
no	O
)	O
.	O
Despite	O
these	O
drawbacks	O
,	O
PQA	O
-	O
A	O
has	O
substantial	O
training	O
instances	O
so	O
models	O
could	O
still	O
benefit	O
from	O
it	O
as	O
a	O
pre	O
-	O
training	O
step	O
.	O
Thus	O
,	O
in	O
Phase	O
I	O
of	O
multi	O
-	O
phase	O
fine	O
-	O
tuning	O
,	O
we	O
initialize	O
BioBERT	O
with	O
θ	B-HyperparameterName
0	B-DatasetName
,	O
and	O
fine	O
-	O
tune	O
it	O
on	O
PQA	O
-	O
A	O
using	O
question	O
and	O
context	O
as	O
input	O
:	O
θ	B-HyperparameterName
I	O
argmin	O
θ	B-HyperparameterName
L	O
(	O
BioBERT	O
θ	B-HyperparameterName
(	O
q	O
A	O
,	O
c	O
A	O
)	O
,	O
l	O
A	O
)	O
(	O
1	O
)	O
Phase	O
II	O
Fine	O
-	O
tuning	O
on	O
Bootstrapped	O
PQA	O
-	O
U	O
:	O
To	O
fully	O
utilize	O
the	O
unlabeled	O
instances	O
in	O
PQA	O
-	O
U	O
,	O
we	O
exploit	O
the	O
easiness	O
of	O
reasoning	O
-	O
free	O
setting	O
to	O
pseudo	O
-	O
label	O
these	O
instances	O
with	O
a	O
bootstrapping	O
strategy	O
:	O
first	O
,	O
we	O
initialize	O
BioBERT	O
with	O
θ	B-HyperparameterName
0	B-DatasetName
,	O
and	O
fine	O
-	O
tune	O
it	O
on	O
PQA	O
-	O
A	O
using	O
question	O
and	O
long	O
answer	O
(	O
reasoning	O
-	O
free	O
)	O
,	O
θ	B-HyperparameterName
B	O
1	O
argmin	O
θ	B-HyperparameterName
L	O
(	O
BioBERT	O
θ	B-HyperparameterName
(	O
q	O
A	O
,	O
a	O
A	O
)	O
,	O
l	O
A	O
)	O
(	O
2	O
)	O
then	O
we	O
further	O
fine	O
-	O
tune	O
BioBERT	O
θ	B-HyperparameterName
B	O
1	O
on	O
PQA	O
-	O
L	O
,	O
also	O
under	O
the	O
reasoning	O
-	O
free	O
setting	O
:	O
θ	B-HyperparameterName
B	O
2	O
argmin	O
θ	B-HyperparameterName
L	O
(	O
BioBERT	O
θ	B-HyperparameterName
(	O
q	O
L	O
,	O
a	O
L	O
)	O
,	O
l	O
L	O
)	O
(	O
3	O
)	O
We	O
pseudo	O
-	O
label	O
PQA	O
-	O
U	O
instances	O
using	O
the	O
most	O
confident	O
predictions	O
of	O
BioBERT	O
θ	B-HyperparameterName
B	O
2	O
for	O
each	O
class	O
.	O
Confidence	O
is	O
simply	O
defined	O
by	O
the	O
corresponding	O
softmax	B-MethodName
probability	O
and	O
then	O
we	O
label	O
a	O
subset	O
which	O
has	O
the	O
same	O
proportions	O
of	O
yes	O
/	O
no	O
/	O
maybe	O
labels	O
as	O
those	O
in	O
the	O
PQA	O
-	O
L	O
:	O
l	O
U	O
pseudo	O
BioBERT	O
θ	B-HyperparameterName
B	O
2	O
(	O
q	O
U	O
,	O
a	O
U	O
)	O
(	O
4	O
)	O
In	O
phase	O
II	O
,	O
we	O
fine	O
-	O
tune	O
BioBERT	O
θ	B-HyperparameterName
I	O
on	O
the	O
bootstrapped	O
PQA	O
-	O
U	O
using	O
question	O
and	O
context	O
(	O
under	O
reasoning	O
-	O
required	O
setting	O
)	O
:	O
θ	B-HyperparameterName
II	O
argmin	O
θ	B-HyperparameterName
L	O
(	O
BioBERT	O
θ	B-HyperparameterName
(	O
q	O
U	O
,	O
c	O
U	O
)	O
,	O
l	O
U	O
pseudo	O
)	O
(	O
5	O
)	O
Final	O
Phase	O
Fine	O
-	O
tuning	O
on	O
PQA	O
-	O
L	O
:	O
In	O
the	O
final	O
phase	O
,	O
we	O
fine	O
-	O
tune	O
BioBERT	O
θ	B-HyperparameterName
II	O
on	O
PQA	O
-	O
L	O
:	O
θ	B-HyperparameterName
F	O
argmin	O
θ	B-HyperparameterName
L	O
(	O
BioBERT	O
θ	B-HyperparameterName
(	O
q	O
L	O
,	O
c	O
L	O
)	O
,	O
l	O
L	O
)	O
(	O
6	O
)	O
Final	O
predictions	O
on	O
instances	O
of	O
PQA	O
-	O
L	O
validation	O
and	O
test	O
sets	O
are	O
made	O
using	O
BioBERT	O
θ	B-HyperparameterName
F	O
:	O
l	O
pred	O
=	O
BioBERT	O
θ	B-HyperparameterName
F	O
(	O
q	O
L	O
,	O
c	O
L	O
)	O

Human	O
performance	O
is	O
measured	O
during	O
the	O
annotation	O
:	O
As	O
shown	O
in	O
Algorithm	O
1	O
,	O
annotations	O
of	O
annotator	O
1	O
and	O
annotator	O
2	O
are	O
used	O
to	O
calculate	O
reasoning	O
-	O
free	O
and	O
reasoning	O
-	O
required	O
human	O
performance	O
,	O
respectively	O
,	O
against	O
the	O
discussed	O
ground	O
truth	O
labels	O
.	O
Human	O
performance	O
on	O
the	O
test	O
set	O
of	O
PQA	O
-	O
L	O
is	O
shown	O
in	O
Table	O
4	O
.	O
We	O
only	O
test	O
single	O
-	O
annotator	O
performance	O
due	O
to	O
limited	O
resources	O
.	O
show	O
that	O
an	O
ensemble	O
of	O
annotators	O
perform	O
significantly	O
better	O
than	O
single	O
-	O
annotator	O
,	O
so	O
the	O
results	O
reported	O
in	O
Table	O
4	O
are	O
the	O
lower	O
bounds	O
of	O
human	O
performance	O
.	O
Under	O
reasoning	O
-	O
free	O
setting	O
where	O
the	O
annotator	O
can	O
see	O
the	O
conclusions	O
,	O
a	O
single	O
human	O
achieves	O
90.4	O
%	O
accuracy	B-MetricName
and	O
84.2	O
%	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
.	O
Under	O
reasoning	O
-	O
required	O
setting	O
,	O
the	O
task	O
be	O
-	O
comes	O
much	O
harder	O
,	O
but	O
it	O
's	O
still	O
possible	O
for	O
humans	O
to	O
solve	O
:	O
a	O
single	O
annotator	O
can	O
get	O
78.0	O
%	O
accuracy	B-MetricName
and	O
72.2	O
%	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
.	O

We	O
report	O
the	O
test	O
set	O
performance	O
of	O
different	O
models	O
and	O
training	O
schedules	O
in	O
Table	O
5	O
.	O
In	O
general	O
,	O
multi	O
-	O
phase	O
fine	O
-	O
tuning	O
of	O
BioBERT	O
with	O
additional	O
supervision	O
outperforms	O
other	O
baselines	O
by	O
large	O
margins	O
,	O
but	O
the	O
results	O
are	O
still	O
much	O
worse	O
than	O
just	O
single	O
-	O
human	O
performance	O
.	O
Comparison	O
of	O
Models	O
:	O
A	O
trend	O
of	O
BioBERT	O
>	O
ESIM	B-MethodName
w/	O
BioELMo	O
>	O
BiLSTM	B-MethodName
>	O
shallow	O
features	O
>	O
majority	O
,	O
conserves	O
across	O
different	O
training	O
schedules	O
on	O
both	O
accuracy	B-MetricName
and	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
.	O
Fine	O
-	O
tuned	O
BioBERT	O
is	O
better	O
than	O
state	O
-	O
of	O
-	O
theart	O
recurrent	O
model	O
of	O
ESIM	B-MethodName
w/	O
BioELMo	O
,	O
probably	O
because	O
BioELMo	O
weights	O
are	O
fixed	O
while	O
all	O
BioBERT	O
parameters	O
can	O
be	O
fine	O
-	O
tuned	O
,	O
which	O
better	O
benefit	O
from	O
the	O
pre	O
-	O
training	O
settings	O
.	O
Comparison	O
of	O
Training	O
Schedules	O
:	O
Multiphase	O
fine	O
-	O
tuning	O
setting	O
gets	O
5	O
out	O
of	O
9	O
modelwise	O
best	O
accuracy	B-MetricName
/	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
.	O
Due	O
to	O
lack	O
of	O
annotated	O
data	O
,	O
training	O
only	O
on	O
the	O
PQA	O
-	O
L	O
(	O
final	O
phase	O
only	O
)	O
generates	O
similar	O
results	O
as	O
the	O
majority	O
baseline	O
.	O
In	O
phase	O
I	O
+	O
Final	O
setting	O
where	O
models	O
are	O
pre	O
-	O
trained	O
on	O
PQA	O
-	O
A	O
,	O
we	O
observe	O
significant	O
improvements	O
on	O
accuracy	B-MetricName
and	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
and	O
some	O
models	O
even	O
achieve	O
their	O
best	O
accuracy	B-MetricName
under	O
this	O
setting	O
.	O
This	O
indicates	O
that	O
a	O
hard	O
task	O
with	O
limited	O
training	O
instances	O
can	O
be	O
at	O
least	O
partially	O
solved	O
by	O
pre	O
-	O
training	O
on	O
a	O
large	O
automatically	O
collected	O
dataset	O
when	O
the	O
tasks	O
are	O
similarly	O
formatted	O
.	O
Improvements	O
are	O
also	O
observed	O
in	O
phase	O
II	O
+	O
Final	O
setting	O
,	O
though	O
less	O
significant	O
than	O
those	O
of	O
phase	O
I	O
+	O
Final	O
.	O
As	O
expected	O
,	O
multi	O
-	O
phase	O
finetuning	O
schedule	O
is	O
better	O
than	O
single	O
-	O
phase	O
,	O
due	O
to	O
different	O
properties	O
of	O
the	O
subsets	O
.	O
Additional	O
Supervision	O
:	O
Despite	O
its	O
simplicity	O
,	O
the	O
auxiliary	O
task	O
of	O
long	O
answer	O
BoW	O
prediction	O
clearly	O
improves	O
the	O
performance	O
:	O
most	O
results	O
(	O
28/40	O
)	O
are	O
better	O
with	O
such	O
additional	O
supervision	O
than	O
without	O
.	O

In	O
this	O
section	O
we	O
show	O
the	O
intermediate	O
results	O
of	O
multi	O
-	O
phase	O
fine	O
-	O
tuning	O
schedule	O
.	O
Phase	O
I	O
:	O
Results	O
are	O
shown	O
in	O
Table	O
6	O
.	O
Phase	O
I	O
is	O
fine	O
-	O
tuning	O
on	O
PQA	O
-	O
A	O
using	O
question	O
and	O
context	O
.	O
Since	O
PQA	O
-	O
A	O
is	O
imbalanced	O
due	O
to	O
its	O
collection	O
process	O
,	O
a	O
trivial	O
majority	O
baseline	O
gets	O
92.76	O
%	O
accuracy	B-MetricName
.	O
Other	O
models	O
have	O
better	O
accuracy	B-MetricName
and	O
especially	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
than	O
majority	O
baseline	O
.	O
Finetuned	O
BioBERT	O
performs	O
best	O
.	O
Bootstrapping	O
:	O
Results	O
are	O
shown	O
in	O
Table	O
7	O
.	O
Bootstrapping	O
is	O
a	O
three	O
-	O
step	O
process	O
:	O
fine	O
-	O
tuning	O
on	O
PQA	O
-	O
A	O
,	O
then	O
on	O
PQA	O
-	O
L	O
and	O
pseudo	O
-	O
labeling	O
PQA	O
-	O
U.	O
All	O
three	O
steps	O
are	O
using	O
question	O
and	O
long	O
answer	O
as	O
input	O
.	O
Expectedly	O
,	O
models	O
perform	O
better	O
in	O
this	O
reasoning	O
-	O
free	O
setting	O
than	O
they	O
do	O
in	O
reasoning	O
-	O
required	O
setting	O
(	O
for	O
PQA	O
-	O
A	O
,	O
Eq	O
.	O
2	O
results	O
in	O
Table	O
7	O
are	O
better	O
than	O
the	O
performance	O
in	O
Table	O
6	O
;	O
for	O
PQA	O
-	O
L	O
,	O
Eq	O
.	O
3	O
results	O
in	O
Table	O
7	O
are	O
better	O
than	O
the	O
performance	O
in	O
Table	O
5	O
)	O
.	O

Online	O
misogyny	O
is	O
a	O
pernicious	O
social	O
problem	O
that	O
risks	O
making	O
online	O
platforms	O
toxic	O
and	O
unwelcoming	O
to	O
women	O
.	O
We	O
present	O
a	O
new	O
hierarchical	O
taxonomy	O
for	O
online	O
misogyny	O
,	O
as	O
well	O
as	O
an	O
expert	O
labelled	O
dataset	O
to	O
enable	O
automatic	O
classification	O
of	O
misogynistic	O
content	O
.	O
The	O
dataset	O
consists	O
of	O
6	O
,	O
567	O
labels	O
for	O
Reddit	B-DatasetName
posts	O
and	O
comments	O
.	O
As	O
previous	O
research	O
has	O
found	O
untrained	O
crowdsourced	O
annotators	O
struggle	O
with	O
identifying	O
misogyny	O
,	O
we	O
hired	O
and	O
trained	O
annotators	O
and	O
provided	O
them	O
with	O
robust	O
annotation	O
guidelines	O
.	O
We	O
report	O
baseline	O
classification	O
performance	O
on	O
the	O
binary	O
classification	O
task	O
,	O
achieving	O
accuracy	B-MetricName
of	O
0.93	O
and	O
F1	B-MetricName
of	O
0.43	O
.	O
The	O
codebook	O
and	O
datasets	O
are	O
made	O
freely	O
available	O
for	O
future	O
researchers	O
.	O

Most	O
previous	O
classification	O
work	O
on	O
online	O
misogyny	O
has	O
used	O
data	O
from	O
Twitter	O
(	O
Waseem	O
and	O
Hovy	O
,	O
2016	O
;	O
Jha	O
and	O
Mamidi	O
,	O
2017	O
)	O
.	O
However	O
,	O
social	O
scientific	O
and	O
ethnographic	O
research	O
shows	O
that	O
Reddit	B-DatasetName
is	O
increasingly	O
home	O
to	O
numerous	O
misogynistic	O
communities	O
.	O
Reddit	B-DatasetName
is	O
a	O
social	O
news	O
website	O
organised	O
in	O
to	O
topic	O
-	O
based	O
communities	O
.	O
Each	O
subreddit	O
acts	O
as	O
a	O
message	O
board	O
where	O
users	O
make	O
posts	O
and	O
hold	O
discussions	O
in	O
comment	O
threads	O
on	O
those	O
posts	O
.	O
In	O
recent	O
years	O
it	O
has	O
become	O
a	O
hub	O
for	O
anti	O
-	O
feminist	O
activism	O
online	O
(	O
Massanari	O
,	O
2017	O
;	O
Ging	O
and	O
Siapera	O
,	O
2018	O
)	O
.	O
It	O
is	O
also	O
home	O
to	O
many	O
misogynistic	O
communities	O
,	O
particularly	O
those	O
associated	O
with	O
the	O
'	O
manosphere	O
'	O
,	O
a	O
loosely	O
connected	O
set	O
of	O
communities	O
which	O
perpetuate	O
traditional	O
forms	O
of	O
misogyny	O
and	O
develop	O
new	O
types	O
of	O
misogynistic	O
discourse	O
which	O
in	O
turn	O
spread	O
to	O
other	O
online	O
spaces	O
(	O
Ging	O
,	O
2017	O
;	O
Zuckerberg	O
,	O
2018	O
;	O
Ging	O
et	O
al	O
,	O
2019	O
;	O
Farrell	O
et	O
al	O
,	O
2019	O
;	O
Ribeiro	O
et	O
al	O
,	O
2020	O
)	O
.	O
Recent	O
research	O
suggests	O
that	O
the	O
rate	O
of	O
misogynistic	O
content	O
in	O
the	O
Reddit	B-DatasetName
manosphere	O
is	O
growing	O
and	O
such	O
content	O
is	O
increasingly	O
more	O
violent	O
(	O
Farrell	O
et	O
al	O
,	O
2019	O
)	O
.	O
Waseem	O
and	O
Hovy	O
(	O
2016	O
)	O
provided	O
a	O
widelyused	O
dataset	O
for	O
abusive	B-TaskName
language	I-TaskName
classification	O
.	O
They	O
used	O
expert	O
annotators	O
to	O
identify	O
sexist	O
and	O
racist	O
tweets	O
based	O
on	O
a	O
set	O
of	O
criteria	O
drawn	O
from	O
critical	O
race	O
theory	O
.	O
The	O
tweets	O
were	O
initially	O
labelled	O
by	O
the	O
authors	O
then	O
reviewed	O
by	O
a	O
third	O
annotator	O
.	O
The	O
resulting	O
dataset	O
consists	O
of	O
17k	O
tweets	O
,	O
of	O
which	O
20	O
%	O
are	O
labelled	O
as	O
sexist	O
.	O
However	O
85	O
%	O
of	O
the	O
disagreements	O
between	O
annotators	O
were	O
over	O
sexism	O
labels	O
,	O
which	O
shows	O
that	O
even	O
experienced	O
coders	O
of	O
abusive	B-TaskName
language	I-TaskName
can	O
have	O
difficultly	O
identifying	O
gendered	O
abuse	O
.	O
Jha	O
and	O
Mamidi	O
(	O
2017	O
)	O
extended	O
on	O
the	O
Waseem	O
and	O
Hovy	O
(	O
2016	O
)	O
dataset	O
to	O
distinguish	O
between	O
between	O
'	O
benevolent	O
'	O
and	O
'	O
hostile	O
'	O
sexism	O
(	O
Glick	O
and	O
Fiske	O
,	O
1997	O
)	O
.	O
They	O
classed	O
all	O
sexist	O
labels	O
in	O
the	O
previous	O
dataset	O
as	O
'	O
Hostile	O
'	O
and	O
all	O
non	O
-	O
sexist	O
labels	O
as	O
'	O
Other	O
'	O
.	O
They	O
then	O
augmented	O
the	O
dataset	O
by	O
collecting	O
tweets	O
using	O
keyword	O
sampling	O
on	O
benevolently	O
sexist	O
phrases	O
(	O
e.g.	O
'	O
smart	O
for	O
a	O
girl	O
'	O
)	O
and	O
extracted	O
those	O
manually	O
identified	O
as	O
'	O
benevolent	O
sexism	O
'	O
.	O
In	O
the	O
combined	O
dataset	O
of	O
10	O
,	O
095	O
unique	O
tweets	O
712	O
were	O
labelled	O
as	O
'	O
benevolent	O
'	O
,	O
2	O
,	O
254	O
as	O
'	O
hostile	O
'	O
,	O
and	O
7	O
,	O
129	O
as	O
'	O
not	O
sexist	O
'	O
.	O
They	O
thus	O
found	O
that	O
in	O
the	O
data	O
hostile	O
sexism	O
was	O
more	O
than	O
three	O
times	O
as	O
common	O
as	O
the	O
benevolent	O
form	O
.	O
Their	O
work	O
highlights	O
the	O
need	O
for	O
greater	O
attention	O
to	O
be	O
placed	O
on	O
forms	O
of	O
'	O
subtle	O
abuse	O
'	O
,	O
particularly	O
for	O
online	O
misogyny	O
(	O
Jurgens	O
et	O
al	O
,	O
2019	O
)	O
.	O
developed	O
a	O
taxonomy	O
with	O
five	O
categories	O
of	O
misogyny	O
,	O
drawn	O
from	O
the	O
work	O
of	O
Poland	O
(	O
2016	O
)	O
:	O
Stereotype	O
&	O
Objectification	O
,	O
Dominance	O
,	O
Derailing	O
,	O
Sexual	O
Harassment	O
&	O
Threats	O
of	O
Violence	O
,	O
Discredit	O
.	O
They	O
used	O
a	O
combination	O
of	O
expert	O
and	O
crowdsourced	O
annotation	O
to	O
apply	O
the	O
taxonomy	O
and	O
present	O
a	O
dataset	O
of	O
4	O
,	O
454	O
tweets	O
with	O
balanced	O
levels	O
of	O
misogynistic	O
and	O
non	O
-	O
misogynistic	O
content	O
.	O
A	O
shared	O
task	O
confirmed	O
that	O
the	O
dataset	O
could	O
be	O
used	O
to	O
distinguish	O
misogynistic	O
and	O
non	O
-	O
misogynistic	O
content	O
with	O
high	O
accuracy	B-MetricName
,	O
but	O
performance	O
was	O
lower	O
in	O
differentiating	O
between	O
types	O
of	O
misogyny	O
.	O
Lynn	O
et	O
al	O
(	O
2019b	O
)	O
provide	O
a	O
dataset	O
of	O
2k	O
Urban	O
Dictionary	O
definitions	O
of	O
which	O
half	O
are	O
labelled	O
as	O
misogynistic	O
.	O
In	O
Lynn	O
et	O
al	O
(	O
2019a	O
)	O
they	O
show	O
that	O
deep	O
learning	O
techniques	O
had	O
greater	O
accuracy	B-MetricName
in	O
detecting	O
misogyny	O
than	O
conventional	O
machine	O
learning	O
techniques	O
.	O

For	O
the	O
level	O
one	O
binary	O
task	O
the	O
Fleiss	O
'	O
Kappa	O
is	O
0.484	O
and	O
the	O
Krippendorf	O
's	O
alpha	B-HyperparameterName
is	O
0.487	O
.	O
By	O
conventional	O
NLP	O
standards	O
these	O
results	O
appear	O
low	O
.	O
However	O
they	O
are	O
equivalent	O
to	O
,	O
or	O
above	O
,	O
those	O
of	O
existing	O
abusive	O
content	O
datasets	O
.	O
Sanguinetti	O
et	O
al	O
(	O
2018	O
)	O
report	O
category	O
-	O
wise	O
Kappas	O
from	O
k=0.37	O
for	O
offence	O
to	O
k=0.54	O
for	O
hate	O
.	O
Gomez	O
et	O
al	O
(	O
2020	O
)	O
have	O
a	O
Kappa	O
of	O
0.15	O
in	O
the	O
"	O
MMH150	O
"	O
dataset	O
of	O
hateful	B-DatasetName
memes	I-DatasetName
.	O
Fortuna	O
and	O
Nunes	O
(	O
2018	O
)	O
report	O
a	O
Kappa	O
of	O
0.17	O
for	O
a	O
text	O
-	O
only	O
task	O
.	O
Krippendorf	O
's	O
alpha	B-HyperparameterName
is	O
similar	O
to	O
the	O
0.45	O
reported	O
by	O
Wulczyn	O
et	O
al	O
(	O
2017	O
)	O
.	O
We	O
also	O
calculated	O
level	O
two	O
category	O
-	O
wise	O
Fleiss	O
'	O
Kappas	O
for	O
each	O
of	O
the	O
17	O
sets	O
of	O
annotator	O
groups	O
,	O
then	O
took	O
the	O
mean	O
across	O
all	O
groups	O
(	O
Ravenscroft	O
et	O
al	O
,	O
2016	O
)	O
.	O
Table	O
1	O
shows	O
the	O
breakdown	O
of	O
Kappas	O
per	O
category	O
.	O
There	O
was	O
greatest	O
agreement	O
for	O
Misogynistic	O
pejoratives	O
(	O
k=0.559	O
)	O
down	O
to	O
the	O
lowest	O
agreement	O
for	O
Misogynistic	O
personal	O
attacks	O
(	O
k=0.145	O
)	O
.	O

As	O
reference	O
points	O
for	O
further	O
research	O
using	O
our	O
dataset	O
,	O
we	O
provide	O
three	O
experimental	O
baselines	O
on	O
the	O
binary	O
task	O
of	O
distinguishing	O
between	O
misogynistic	O
and	O
non	O
-	O
misogynistic	O
content	O
,	O
i.e.	O
level	O
one	O
of	O
our	O
taxonomy	O
.	O
As	O
the	O
simplest	O
baseline	O
,	O
we	O
evaluate	O
a	O
logistic	O
unigram	O
classifier	O
.	O
Further	O
,	O
we	O
evaluate	O
two	O
uncased	O
BERT	B-MethodName
-	O
base	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
-	O
one	O
unweighted	O
,	O
the	O
other	O
using	O
class	O
weights	O
emphasising	O
the	O
minority	O
class	O
,	O
i.e.	O
misogynistic	O
content	O
,	O
to	O
account	O
for	O
class	O
imbalance	O
.	O
For	O
all	O
models	O
,	O
we	O
use	O
the	O
same	O
stratified	O
80/20	O
train	O
/	O
test	O
split	O
of	O
the	O
dataset	O
.	O
Details	O
on	O
model	O
training	O
and	O
parameters	O
can	O
be	O
found	O
in	O
Appendix	O
C.	O
Performance	O
of	O
the	O
three	O
models	O
is	O
shown	O
in	O
Table	O
6	O
.	O
All	O
models	O
perform	O
poorly	O
on	O
misogynistic	O
content	O
,	O
with	O
the	O
logistic	O
classifier	O
performing	O
worst	O
overall	O
.	O
The	O
logistic	O
classifier	O
has	O
the	O
highest	O
precision	O
on	O
misogynistic	O
content	O
(	O
0.88	O
)	O
but	O
very	O
low	O
recall	O
(	O
0.07	O
)	O
and	O
a	O
low	O
F1	B-MetricName
score	I-MetricName
(	O
0.13	O
)	O
.	O
The	O
weighted	O
BERT	B-MethodName
model	O
has	O
the	O
highest	O
recall	O
(	O
0.50	O
)	O
and	O
F1	B-MetricName
score	I-MetricName
(	O
0.43	O
)	O
.	O
Accuracy	B-MetricName
on	O
all	O
test	O
cases	O
,	O
of	O
which	O
91.9	O
%	O
are	O
non	O
-	O
misogynistic	O
,	O
is	O
around	O
0.90	O
across	O
models	O
.	O
The	O
classification	O
task	O
is	O
complicated	O
by	O
the	O
relatively	O
small	O
size	O
of	O
our	O
dataset	O
(	O
n=6	O
,	O
385	O
unique	O
cases	O
)	O
as	O
well	O
as	O
the	O
relatively	O
small	O
proportion	O
of	O
misogynistic	O
cases	O
in	O
it	O
(	O
8.1	O
%	O
)	O
.	O
These	O
issues	O
are	O
common	O
in	O
abusive	O
speech	O
detection	O
(	O
Fortuna	O
and	O
Nunes	O
,	O
2018	O
;	O
Fortuna	O
et	O
al	O
,	O
2020	O
)	O
.	O
To	O
address	O
them	O
,	O
future	O
research	O
can	O
leverage	O
the	O
typology	O
and	O
annotation	O
process	O
we	O
introduced	O
to	O
collect	O
additional	O
cases	O
,	O
particularly	O
misogynistic	O
ones	O
,	O
thus	O
growing	O
and	O
balancing	O
the	O
dataset	O
.	O

Discourse	O
analysis	O
is	O
a	O
crucial	O
analytic	O
level	O
in	O
NLP	O
.	O
In	O
natural	O
language	O
discourse	O
,	O
speakers	O
and	O
writers	O
often	O
rely	O
on	O
implicit	O
inference	O
to	O
signal	O
the	O
kind	O
of	O
contribution	O
they	O
are	O
making	O
to	O
the	O
conversation	O
,	O
as	O
well	O
as	O
key	O
relationships	O
that	O
justify	O
their	O
point	O
of	O
view	O
.	O
While	O
early	O
AI	O
literature	O
is	O
full	O
of	O
case	O
studies	O
suggesting	O
that	O
this	O
inference	O
is	O
complex	O
,	O
open	O
-	O
ended	O
and	O
knowledge	O
-	O
heavy	O
(	O
e.g.	O
,	O
Charniak	O
(	O
1973	O
)	O
;	O
Schank	O
and	O
Abelson	O
(	O
1977	O
)	O
)	O
,	O
recent	O
work	O
on	O
computational	O
discourse	O
coherence	O
offers	O
a	O
different	O
approach	O
.	O
Take	O
the	O
following	O
example	O
from	O
Pitler	O
and	O
Nenkova	O
(	O
2008	O
)	O
:	O
(	O
1	O
)	O
"	O
Alice	O
thought	O
the	O
story	O
was	O
predictable	O
.	O
She	O
found	O
it	O
boring	O
.	O
"	O
This	O
discourse	O
shows	O
the	O
classic	O
pattern	O
of	O
implicit	O
information	O
.	O
The	O
overall	O
point	O
is	O
that	O
Alice	O
had	O
a	O
negative	O
opinion	O
of	O
the	O
story	O
:	O
the	O
underlying	O
explanation	O
is	O
that	O
the	O
story	O
was	O
not	O
interesting	O
because	O
it	O
had	O
no	O
surprises	O
.	O
But	O
given	O
available	O
lexical	O
resources	O
and	O
sentiment	O
detection	O
methods	O
,	O
we	O
can	O
capture	O
such	O
inferences	O
systematically	O
by	O
recognizing	O
that	O
they	O
follow	O
common	O
general	O
patterns	O
,	O
known	O
as	O
"	O
discourse	O
relations	O
"	O
,	O
and	O
are	O
guided	O
by	O
shallow	O
cues	O
.	O
An	O
example	O
of	O
an	O
instance	O
in	O
which	O
discourse	O
analysis	O
can	O
produce	O
insights	O
that	O
may	O
be	O
missed	O
by	O
employing	O
other	O
NLP	O
methods	O
is	O
this	O
example	O
from	O
Taboada	O
(	O
2016	O
)	O
,	O
where	O
without	O
discourse	O
relations	O
it	O
may	O
be	O
difficult	O
to	O
capture	O
sentiment	O
:	O
(	O
2	O
)	O
"	O
While	O
this	O
book	O
is	O
totally	O
different	O
from	O
any	O
other	O
book	O
he	O
has	O
written	O
to	O
date	O
,	O
it	O
did	O
not	O
disappoint	O
me	O
at	O
all	O
.	O
"	O
This	O
represents	O
a	O
Concession	O
relation	O
according	O
to	O
both	O
Rhetorical	O
Structure	O
Theory	O
and	O
the	O
Penn	O
Discourse	O
Treebank	O
(	O
where	O
it	O
is	O
notated	O
as	O
Comparison	O
.	O
Concession	O
)	O
,	O
resolving	O
the	O
incongruity	O
of	O
the	O
first	O
clause	O
being	O
negative	O
and	O
the	O
second	O
clause	O
being	O
positive	O
by	O
illustrating	O
how	O
the	O
negative	O
statement	O
in	O
the	O
subordinate	O
clause	O
is	O
reversed	O
by	O
the	O
positive	O
one	O
in	O
the	O
main	O
clause	O
.	O
The	O
importance	O
of	O
discourse	O
has	O
led	O
to	O
active	O
research	O
based	O
on	O
predicting	O
what	O
coherence	O
relations	O
are	O
present	O
in	O
text	O
based	O
on	O
shallow	O
information	O
.	O
The	O
predicted	O
relations	O
are	O
then	O
used	O
to	O
draw	O
inferences	O
from	O
the	O
text	O
.	O
The	O
value	O
of	O
predicting	O
the	O
semantic	O
classes	O
of	O
coherence	O
relations	O
has	O
been	O
demonstrated	O
in	O
several	O
applications	O
,	O
including	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Marcu	O
,	O
2000	O
;	O
Bhatia	O
et	O
al	O
,	O
2015	O
)	O
,	O
machine	O
comprehension	O
(	O
Narasimhan	O
and	O
Barzilay	O
,	O
2015	O
)	O
,	O
summarization	B-TaskName
(	O
Cohan	O
et	O
al	O
,	O
2018	O
;	O
Marcu	O
,	O
1999	O
;	O
Xu	O
et	O
al	O
,	O
2019	O
;	O
Kikuchi	O
et	O
al	O
,	O
2014	O
)	O
,	O
and	O
predicting	O
instructor	O
intervention	O
in	O
an	O
online	O
course	O
discussion	O
forum	O
(	O
Chandrasekaran	O
et	O
al	O
,	O
2017	O
)	O
.	O
However	O
,	O
it	O
is	O
still	O
the	O
case	O
that	O
few	O
works	O
have	O
so	O
far	O
found	O
discourse	O
relations	O
as	O
key	O
features	O
(	O
Zhong	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
argue	O
that	O
one	O
reason	O
for	O
this	O
gap	O
between	O
theory	O
and	O
empirical	O
evidence	O
is	O
the	O
quality	O
of	O
the	O
parsers	O
exacerbated	O
by	O
the	O
distributional	O
shifts	O
in	O
the	O
texts	O
they	O
need	O
to	O
apply	O
to	O
.	O
The	O
necessity	O
of	O
discourse	O
research	O
has	O
resulted	O
in	O
several	O
shared	O
tasks	O
(	O
Xue	O
et	O
al	O
,	O
2015	O
(	O
Xue	O
et	O
al	O
,	O
,	O
2016	O
and	O
corpora	O
development	O
in	O
multiple	O
languages	O
(	O
Zeyrek	O
and	O
Webber	O
,	O
2008	O
;	O
Meyer	O
et	O
al	O
,	O
2011	O
;	O
Danlos	O
et	O
al	O
,	O
2012	O
;	O
Zhou	O
et	O
al	O
,	O
2014	O
;	O
Zeyrek	O
et	O
al	O
,	O
2020	O
)	O
.	O
Yet	O
shallow	O
discourse	B-TaskName
parsing	I-TaskName
is	O
a	O
very	O
difficult	O
task	O
;	O
more	O
than	O
10	O
years	O
after	O
the	O
introduction	O
of	O
the	O
Penn	O
Discourse	O
Treebank	O
(	O
Eleni	O
Miltsakaki	O
,	O
2004	O
)	O
,	O
performance	O
for	O
English	O
implicit	O
discourse	O
relation	O
recognition	O
has	O
gone	O
from	O
40.2	O
F	O
-	O
1	O
(	O
Lin	O
et	O
al	O
,	O
2009	O
)	O
to	O
47.8	O
(	O
Lee	O
et	O
al	O
,	O
2020	O
)	O
,	O
less	O
than	O
8	O
percentage	O
points	O
;	O
a	O
similar	O
story	O
could	O
be	O
said	O
about	O
the	O
relation	O
prediction	O
performance	O
of	O
RST	O
parsers	O
.	O
Such	O
performance	O
hinders	O
the	O
wider	O
application	O
of	O
parsers	O
.	O
If	O
downstream	O
tasks	O
are	O
to	O
use	O
predicted	O
relation	O
senses	O
,	O
the	O
data	O
to	O
which	O
the	O
systems	O
are	O
applied	O
is	O
typically	O
different	O
from	O
their	O
training	O
data	O
-	O
the	O
Wall	O
Street	O
Journal	O
(	O
WSJ	O
)	O
in	O
a	O
3	O
-	O
year	O
window	O
-	O
to	O
varying	O
degrees	O
.	O
This	O
tends	O
to	O
further	O
aggravate	O
the	O
low	O
performance	O
observed	O
.	O
As	O
a	O
result	O
,	O
often	O
we	O
find	O
that	O
adding	O
parsed	O
discourse	O
relations	O
into	O
models	O
are	O
unhelpful	O
.	O
Although	O
domain	O
difference	O
is	O
a	O
recognized	O
issue	O
in	O
shallow	O
discourse	B-TaskName
parsing	I-TaskName
by	O
existing	O
work	O
(	O
Braud	O
et	O
al	O
,	O
2017	O
;	O
Liu	O
et	O
al	O
,	O
2016	O
)	O
,	O
we	O
still	O
have	O
little	O
understanding	O
of	O
the	O
types	O
of	O
distributional	O
shift	O
that	O
matter	O
and	O
by	O
how	O
much	O
,	O
even	O
within	O
one	O
language	O
.	O
This	O
position	O
paper	O
seeks	O
to	O
shed	O
some	O
light	O
on	O
our	O
current	O
state	O
in	O
discourse	B-TaskName
parsing	I-TaskName
in	O
English	O
.	O
Surprisingly	O
,	O
we	O
found	O
that	O
parsers	O
have	O
some	O
issues	O
even	O
within	O
the	O
same	O
news	O
source	O
as	O
the	O
training	O
set	O
(	O
WSJ	O
)	O
;	O
the	O
differences	O
in	O
accuracy	B-MetricName
were	O
not	O
significant	O
between	O
indomain	O
and	O
out	O
-	O
of	O
-	O
domain	O
data	O
for	O
the	O
qualitative	O
examples	O
that	O
we	O
looked	O
at	O
,	O
although	O
the	O
distribution	O
of	O
errors	O
tend	O
to	O
be	O
different	O
.	O
This	O
differs	O
from	O
other	O
NLP	O
tasks	O
such	O
as	O
entity	O
recognition	O
,	O
where	O
training	O
on	O
data	O
in	O
the	O
target	O
domain	O
increased	O
the	O
F1	B-MetricName
score	I-MetricName
by	O
over	O
20	O
points	O
(	O
Bamman	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
further	O
found	O
that	O
parsers	O
perform	O
differently	O
on	O
implicit	O
discourse	O
relations	O
held	O
within	O
vs.	O
across	O
sentences	O
.	O
We	O
believe	O
these	O
findings	O
are	O
strong	O
evidence	O
for	O
the	O
sensitivity	O
of	O
existing	O
models	O
to	O
distributional	O
shift	O
in	O
terms	O
of	O
both	O
linguistic	O
structure	O
and	O
vocabulary	O
.	O
Additionally	O
,	O
as	O
part	O
of	O
our	O
evaluation	O
,	O
we	O
asked	O
linguists	O
to	O
perform	O
manual	O
annotation	O
,	O
which	O
allowed	O
us	O
to	O
evaluate	O
the	O
accuracy	B-MetricName
of	O
these	O
parsers	O
on	O
plain	O
,	O
unlabeled	O
text	O
,	O
and	O
gain	O
some	O
insight	O
about	O
the	O
mistakes	O
made	O
by	O
the	O
parsers	O
.	O
During	O
the	O
annotation	O
process	O
,	O
we	O
uncovered	O
information	O
that	O
can	O
guide	O
future	O
research	O
,	O
including	O
but	O
not	O
limited	O
to	O
the	O
critical	O
role	O
of	O
context	O
for	O
implicit	O
discourse	O
sense	O
classification	O
.	O
We	O
discuss	O
this	O
need	O
for	O
context	O
,	O
hypothesize	O
what	O
scenarios	O
may	O
cause	O
two	O
arguments	O
to	O
need	O
additional	O
context	O
,	O
and	O
provide	O
some	O
examples	O
for	O
which	O
this	O
is	O
the	O
case	O
.	O
We	O
urge	O
future	O
researchers	O
to	O
consider	O
developing	O
contextaware	O
models	O
for	O
shallow	O
discourse	B-TaskName
parsing	I-TaskName
moving	O
forward	O
.	O
We	O
release	O
our	O
dataset	O
to	O
facilitate	O
further	O
discourse	O
analysis	O
under	O
domain	O
shift	O
.	O
1	O

Transformer	B-MethodName
-	O
based	O
models	O
perform	O
better	O
on	O
linguistically	O
different	O
intra	O
-	O
sentential	O
relations	O
than	O
they	O
do	O
on	O
inter	O
-	O
sentential	O
relations	O
.	O
As	O
mentioned	O
above	O
,	O
we	O
aim	O
to	O
examine	O
the	O
results	O
of	O
distributional	O
shifts	O
in	O
both	O
vocabulary	O
and	O
linguistic	O
structure	O
.	O
Here	O
,	O
we	O
look	O
at	O
shifts	O
in	O
linguistic	O
structure	O
,	O
namely	O
,	O
inter	O
-	O
vs.	O
intra	O
-	O
sentence	O
implicit	O
discourse	O
relations	O
(	O
Hobbs	O
,	O
1985	O
)	O
.	O
The	O
latter	O
was	O
introduced	O
in	O
the	O
PDTB	O
-	O
3	O
(	O
Liang	O
et	O
al	O
,	O
2020	O
)	O
from	O
which	O
we	O
show	O
the	O
following	O
example	O
:	O
(	O
3	O
)	O
...	O
Exxon	O
Corp.	O
built	O
the	O
plant	O
but	O
(	O
I	O
m	O
-	O
plicit	O
=	O
then	O
)	O
closed	O
it	O
in	O
1985	O
Unlike	O
the	O
inter	O
-	O
sentence	O
relations	O
that	O
were	O
annotated	O
across	O
adjacent	O
sentences	O
,	O
implicit	O
intrasentence	O
relations	O
do	O
not	O
occur	O
at	O
well	O
-	O
defined	O
positions	O
,	O
but	O
rather	O
between	O
varied	O
types	O
of	O
syntactic	O
constituents	O
.	O
Additionally	O
,	O
they	O
often	O
co	O
-	O
occur	O
with	O
explicit	O
relations	O
.	O
Table	O
1	O
shows	O
the	O
accuracies	O
of	O
the	O
base	O
and	O
large	O
BERT	B-MethodName
model	O
(	O
Chen	O
et	O
al	O
,	O
2019	O
)	O
on	O
the	O
implicit	B-TaskName
relations	I-TaskName
in	O
the	O
two	O
versions	O
of	O
the	O
PDTB	O
.	O
The	O
results	O
on	O
the	O
PDTB	O
-	O
3	O
are	O
significantly	O
better	O
than	O
those	O
of	O
the	O
PDTB	O
-	O
2	O
,	O
and	O
the	O
model	O
tested	O
on	O
the	O
PDTB	O
-	O
3	O
intra	O
-	O
sentential	O
relations	O
significantly	O
outperformed	O
both	O
(	O
p<0.01	O
,	O
t>11.172	O
)	O
.	O
This	O
mirrors	O
the	O
results	O
found	O
from	O
running	O
the	O
baseline	O
model	O
in	O
Liang	O
et	O
al	O
(	O
2020	O
)	O
on	O
the	O
PDTB	O
-	O
2	O
,	O
PDTB	O
-	O
3	O
,	O
and	O
PDTB	O
-	O
3	O
intra	O
-	O
sentential	O
relations	O
.	O
Figure	O
1	O
shows	O
the	O
accuracy	B-MetricName
of	O
the	O
Wang	O
et	O
al	O
(	O
2017	O
)	O
parser	O
on	O
the	O
inter	O
-	O
sentential	O
and	O
intrasentential	O
relations	O
in	O
the	O
RST	O
,	O
respectively	O
.	O
For	O
the	O
inter	O
-	O
sentential	O
relations	O
,	O
we	O
sampled	O
only	O
the	O
relations	O
between	O
two	O
sentences	O
to	O
have	O
a	O
"	O
fairer	O
"	O
comparison	O
(	O
it	O
is	O
well	O
known	O
that	O
performance	O
suffers	O
on	O
higher	O
levels	O
of	O
the	O
RST	O
tree	O
)	O
.	O
As	O
with	O
the	O
PDTB	O
,	O
these	O
results	O
show	O
a	O
significant	O
improvement	O
in	O
performance	O
when	O
run	O
on	O
only	O
the	O
intra	O
-	O
sentential	O
relations	O
compared	O
to	O
only	O
the	O
intersentential	O
relations	O
.	O
These	O
results	O
drive	O
home	O
the	O
influence	O
of	O
the	O
linguistic	O
and	O
structural	O
differences	O
between	O
intra	O
-	O
and	O
inter	O
-	O
sentence	O
implicit	B-TaskName
relations	I-TaskName
on	O
the	O
performance	O
of	O
the	O
parsers	O
.	O
We	O
initially	O
found	O
this	O
surprising	O
since	O
intra	O
-	O
sentence	O
ones	O
contain	O
arguments	O
with	O
less	O
information	O
than	O
their	O
(	O
full	O
-	O
sentence	O
)	O
intersentence	O
counterparts	O
.	O
However	O
,	O
one	O
explanation	O
for	O
this	O
is	O
that	O
,	O
while	O
looking	O
for	O
relations	O
within	O
sentence	O
boundaries	O
is	O
a	O
problem	O
that	O
has	O
been	O
very	O
explored	O
,	O
and	O
to	O
some	O
extent	O
solved	O
,	O
in	O
various	O
NLP	O
tasks	O
(	O
e.g.	O
syntactic	O
parsing	O
)	O
,	O
there	O
are	O
not	O
as	O
many	O
rules	O
regarding	O
relations	O
that	O
occur	O
across	O
sentence	O
boundaries	O
.	O
Regardless	O
of	O
the	O
cause	O
,	O
these	O
results	O
illustrate	O
that	O
future	O
shallow	O
discourse	O
parsers	O
may	O
benefit	O
from	O
accounting	O
for	O
such	O
linguistic	O
differences	O
explicitly	O
.	O
Parsers	O
struggle	O
to	O
identify	O
implicit	B-TaskName
relations	I-TaskName
from	O
less	O
frequent	O
classes	O
.	O
The	O
second	O
distributional	O
shift	O
we	O
examine	O
is	O
a	O
shift	O
in	O
vocabulary	O
.	O
In	O
order	O
to	O
capture	O
this	O
,	O
we	O
measure	O
the	O
performance	O
across	O
several	O
domain	O
shifts	O
from	O
the	O
PDTB	O
-	O
2	O
using	O
three	O
datasets	O
:	O
WSJ	O
articles	O
from	O
the	O
COHA	O
corpus	O
(	O
Davies	O
,	O
2012	O
)	O
,	O
other	O
news	O
articles	O
from	O
COHA	O
,	O
and	O
the	O
GUM	B-DatasetName
corpus	O
(	O
Zeldes	O
,	O
2017	O
)	O
.	O
The	O
WSJ	O
articles	O
are	O
completely	O
within	O
the	O
domain	O
of	O
the	O
PDTB	O
,	O
but	O
more	O
shifted	O
in	O
timeline	O
than	O
the	O
PDTB	O
test	O
set	O
.	O
The	O
other	O
news	O
articles	O
are	O
in	O
-	O
domain	O
as	O
well	O
,	O
but	O
not	O
from	O
the	O
same	O
source	O
Figure	O
1	O
:	O
F	O
-	O
1	O
scores	O
for	O
running	O
the	O
Wang	O
et	O
al	O
RST	O
parser	O
on	O
the	O
RST	O
Discourse	O
Treebank	O
for	O
inter	O
-	O
sentential	O
(	O
yellow	O
)	O
and	O
intra	O
-	O
sentential	O
(	O
blue	O
)	O
relations	O
(	O
*	O
denotes	O
that	O
this	O
relation	O
was	O
not	O
included	O
in	O
the	O
set	O
of	O
inter	O
-	O
sentential	O
relations	O
)	O
.	O
We	O
can	O
see	O
from	O
this	O
graph	O
that	O
the	O
performance	O
of	O
the	O
parser	O
was	O
improved	O
for	O
the	O
intra	O
-	O
sentential	O
relations	O
compared	O
to	O
the	O
inter	O
-	O
sentential	O
relations	O
.	O
publication	O
,	O
and	O
thus	O
may	O
be	O
linguistically	O
different	O
.	O
The	O
GUM	B-DatasetName
corpus	O
,	O
our	O
out	O
-	O
of	O
-	O
domain	O
dataset	O
,	O
contains	O
data	O
from	O
eight	O
domains	O
:	O
Academic	O
,	O
Bio	B-DatasetName
,	O
Fiction	O
,	O
Interview	B-DatasetName
,	O
News	O
,	O
Travel	O
,	O
How	O
-	O
to	O
guides	O
,	O
and	O
Forum	O
Discussions	O
.	O
It	O
contains	O
gold	O
RST	O
annotations	O
but	O
no	O
PDTB	O
annotations	O
.	O
To	O
quantitatively	O
evaluate	O
the	O
performance	O
of	O
these	O
parsing	O
models	O
,	O
we	O
examine	O
the	O
distribution	O
of	O
the	O
parser	O
predictions	O
and	O
how	O
frequently	O
different	O
senses	O
are	O
predicted	O
.	O
From	O
this	O
,	O
we	O
noticed	O
that	O
only	O
5	O
out	O
of	O
the	O
16	O
PDTB	O
-	O
2	O
level	O
2	O
senses	O
were	O
predicted	O
at	O
all	O
by	O
the	O
Wang	O
and	O
Lan	O
parser	O
,	O
and	O
only	O
7	O
out	O
of	O
16	O
were	O
predicted	O
by	O
the	O
DiscoEval	O
parser	O
.	O
Of	O
these	O
classes	O
,	O
several	O
were	O
predicted	O
less	O
than	O
2	O
%	O
of	O
the	O
time	O
(	O
Table	O
6	O
)	O
.	O
We	O
can	O
also	O
see	O
that	O
in	O
Tables	O
2	O
and	O
3	O
,	O
the	O
Wang	O
et	O
al	O
parser	O
predicted	O
at	O
least	O
38.7	O
%	O
Contingency	O
.	O
Cause	O
for	O
all	O
datasets	O
and	O
the	O
DiscoEval	O
parser	O
predicted	O
at	O
least	O
44	O
%	O
Contingency	O
.	O
Cause	O
,	O
although	O
these	O
percentages	O
were	O
often	O
much	O
higher	O
.	O
Because	O
only	O
24.9	O
%	O
of	O
the	O
total	O
relations	O
contained	O
in	O
the	O
PDTB	O
are	O
Contingency	O
,	O
this	O
overrepresentation	O
of	O
Contingency	O
.	O
Cause	O
in	O
the	O
predictions	O
indicates	O
a	O
strong	O
bias	O
towards	O
Contingency	O
.	O
Indeed	O
,	O
many	O
of	O
the	O
errors	O
found	O
during	O
annotation	O
occurred	O
when	O
the	O
parser	O
predicted	O
Contingency	O
.	O
Cause	O
,	O
the	O
most	O
common	O
level	O
2	O
sense	O
,	O
over	O
a	O
less	O
represented	O
class	O
such	O
as	O
Comparison	O
.	O
Contrast	O
;	O
the	O
precision	O
for	O
Contingency	O
.	O
Cause	O
was	O
0.33	O
,	O
0.14	O
,	O
and	O
0.33	O
for	O
WSJ	O
articles	O
,	O
non	O
-	O
WSJ	O
news	O
articles	O
,	O
and	O
the	O
GUM	B-DatasetName
corpus	O
respectively	O
.	O
This	O
likely	O
contributed	O
to	O
the	O
low	O
accuracy	B-MetricName
for	O
these	O
documents	O
.	O
These	O
results	O
show	O
us	O
that	O
if	O
PDTB	O
parsers	O
are	O
run	O
on	O
plain	O
text	O
documents	O
,	O
whether	O
in	O
-	O
domain	O
or	O
slightly	O
shifted	O
,	O
the	O
results	O
are	O
likely	O
to	O
be	O
overconfident	O
with	O
majority	O
classes	O
and	O
unlikely	O
to	O
predict	O
minority	O
classes	O
.	O
We	O
also	O
obtained	O
the	O
predicted	O
distributions	O
of	O
the	O
RST	O
relations	O
(	O
Table	O
4	O
)	O
on	O
the	O
COHA	O
news	O
articles	O
;	O
we	O
examined	O
these	O
results	O
for	O
the	O
set	O
of	O
WSJ	O
articles	O
as	O
well	O
as	O
the	O
other	O
news	O
articles	O
.	O
We	O
found	O
that	O
relations	O
that	O
are	O
highly	O
represented	O
in	O
the	O
RST	O
Discourse	O
Treebank	O
such	O
as	O
Elaboration	O
,	O
Attribution	O
,	O
and	O
Same	O
Unit	O
were	O
predicted	O
much	O
more	O
frequently	O
than	O
they	O
appear	O
in	O
the	O
RST	O
.	O
However	O
,	O
more	O
minority	O
classes	O
were	O
represented	O
in	O
5	O
,	O
where	O
the	O
results	O
of	O
the	O
parsers	O
are	O
compared	O
to	O
the	O
ground	O
truth	O
labels	O
by	O
the	O
annotators	O
.	O
Across	O
the	O
three	O
corpora	O
,	O
the	O
annotators	O
noticed	O
that	O
in	O
many	O
cases	O
the	O
relation	O
type	O
was	O
labeled	O
as	O
EntRel	O
or	O
NoRel	O
when	O
it	O
should	O
n't	O
have	O
been	O
,	O
or	O
vice	O
versa	O
.	O
This	O
led	O
to	O
discourse	O
senses	O
being	O
predicted	O
for	O
relations	O
that	O
did	O
not	O
have	O
a	O
discourse	O
sense	O
and	O
vice	O
versa	O
.	O
The	O
parsers	O
also	O
often	O
had	O
issues	O
with	O
argument	O
segmentation	O
.	O
For	O
the	O
GUM	B-DatasetName
corpus	O
,	O
segmentation	O
was	O
especially	O
an	O
issue	O
in	O
the	O
travel	O
genre	O
,	O
where	O
headers	O
or	O
captions	O
would	O
be	O
labeled	O
as	O
part	O
of	O
an	O
argument	O
.	O
As	O
is	O
shown	O
in	O
Table	O
5	O
,	O
the	O
percentage	O
of	O
implicit	B-TaskName
relations	I-TaskName
that	O
the	O
parsers	O
got	O
right	O
on	O
the	O
second	O
level	O
appeared	O
to	O
decrease	O
on	O
average	O
as	O
the	O
domain	O
shifted	O
.	O
However	O
,	O
this	O
was	O
a	O
very	O
slight	O
decrease	O
;	O
they	O
had	O
roughly	O
the	O
same	O
level	O
of	O
accuracy	B-MetricName
across	O
all	O
datasets	O
,	O
which	O
was	O
very	O
low	O
.	O
In	O
fact	O
,	O
for	O
all	O
parsing	O
models	O
and	O
datasets	O
,	O
a	O
larger	O
percentage	O
of	O
relations	O
was	O
predicted	O
completely	O
incorrectly	O
.	O
The	O
results	O
of	O
running	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Wang	O
et	O
al	O
(	O
2017	O
)	O
parser	O
on	O
the	O
gold	O
labels	O
of	O
the	O
RST	O
and	O
GUM	B-DatasetName
corpus	O
are	O
shown	O
in	O
Figure	O
2	O
.	O
These	O
results	O
make	O
it	O
clear	O
that	O
the	O
RST	O
parser	O
performs	O
much	O
worse	O
on	O
out	O
-	O
of	O
-	O
domain	O
data	O
than	O
it	O
does	O
on	O
RST	O
corpus	O
data	O
.	O
This	O
is	O
expected	O
;	O
it	O
unsurprisingly	O
does	O
not	O
generalize	O
as	O
well	O
for	O
text	O
outside	O
of	O
its	O
domain	O
as	O
for	O
the	O
news	O
text	O
contained	O
within	O
the	O
corpus	O
test	O
set	O
due	O
to	O
a	O
change	O
in	O
vocabulary	O
.	O
However	O
,	O
in	O
order	O
for	O
discourse	O
parsers	O
to	O
be	O
useful	O
for	O
applications	O
outside	O
of	O
the	O
news	O
domain	O
,	O
models	O
that	O
can	O
more	O
easily	O
adapt	O
to	O
the	O
target	O
domain	O
must	O
be	O
developed	O
.	O
0	B-DatasetName
-	O
2	O
%	O
1	O
2	O
2	O
2	O
3	O
3	O
2	O
-	O
5	O
%	O
1	O
0	B-DatasetName
0	B-DatasetName
2	O
2	O
2	O
>	O
5	O
%	O
3	O
3	O
3	O
3	O
3	O
3	O

More	O
context	O
than	O
the	O
two	O
arguments	O
is	O
needed	O
to	O
determine	O
the	O
correct	O
discourse	O
relation	O
in	O
many	O
cases	O
One	O
potential	O
way	O
to	O
mitigate	O
the	O
impact	O
of	O
domain	O
shift	O
on	O
the	O
performance	O
of	O
shallow	O
discourse	O
parsers	O
is	O
to	O
incorporate	O
context	O
.	O
With	O
a	O
few	O
exceptions	O
(	O
Dai	O
and	O
Huang	O
,	O
2018	O
;	O
Shi	O
and	O
Demberg	O
,	O
2019	O
;	O
Zhang	O
et	O
al	O
,	O
2021	O
)	O
,	O
existing	O
models	O
for	O
shallow	O
discourse	B-TaskName
parsing	I-TaskName
mostly	O
do	O
not	O
Figure	O
3	O
:	O
RST	O
parse	O
tree	O
containing	O
a	O
segment	O
of	O
the	O
relations	O
that	O
were	O
examined	O
in	O
the	O
qualitative	O
analysis	O
.	O
The	O
discourse	O
sense	O
labels	O
on	O
this	O
tree	O
that	O
were	O
examined	O
in	O
our	O
analysis	O
are	O
marked	O
red	O
and	O
green	O
,	O
where	O
green	O
is	O
correct	O
and	O
red	O
is	O
incorrect	O
use	O
input	O
beyond	O
the	O
two	O
adjacent	O
sentences	O
that	O
comprise	O
the	O
arguments	O
of	O
the	O
relation	O
(	O
Kishimoto	O
et	O
al	O
,	O
2020	O
;	O
Chen	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
found	O
that	O
only	O
considering	O
these	O
two	O
sentences	O
is	O
not	O
sufficient	O
even	O
for	O
our	O
expert	O
linguist	O
annotators	O
.	O
Specifically	O
,	O
while	O
annotating	O
the	O
PDTB	O
,	O
the	O
annotators	O
found	O
several	O
examples	O
where	O
,	O
when	O
they	O
looked	O
at	O
the	O
larger	O
context	O
behind	O
the	O
arguments	O
and	O
the	O
sentences	O
where	O
the	O
arguments	O
were	O
contained	O
,	O
their	O
annotations	O
changed	O
.	O
Below	O
,	O
we	O
describe	O
a	O
few	O
examples	O
that	O
demonstrate	O
the	O
mistakes	O
that	O
can	O
be	O
made	O
without	O
the	O
full	O
context	O
and	O
their	O
implications	O
:	O
(	O
4	O
)	O
In	O
this	O
northern	O
latitude	O
it	O
does	O
n't	O
get	O
dark	O
in	O
summer	O
until	O
about	O
10:30	O
p.m.	O
so	O
lighting	O
is	O
operate	O
except	O
at	O
some	O
crazy	O
time	O
like	O
11:45	O
at	O
night	O
,	O
whenever	O
there	O
is	O
power	O
,	O
unless	O
they	O
have	O
stand	O
-	O
by	O
diesel	O
generators	O
.	O
There	O
's	O
a	O
year	O
's	O
supply	O
of	O
diesel	O
oil	O
here	O
.	O
This	O
example	O
is	O
from	O
the	O
Wall	O
Street	O
Journal	O
.	O
At	O
first	O
glimpse	O
,	O
one	O
would	O
think	O
to	O
annotate	O
this	O
as	O
Contingency	O
.	O
Factual	O
present	O
condition	O
,	O
but	O
this	O
does	O
not	O
capture	O
the	O
full	O
context	O
,	O
which	O
is	O
shown	O
below	O
:	O
(	O
5	O
)	O
One	O
housewife	O
says	O
:	O
"	O
With	O
an	O
electric	O
kitchen	O
I	O
have	O
to	O
do	O
my	O
whole	O
day	O
's	O
cook	O
-	O
ing	O
the	O
day	O
before	O
-	O
and	O
that	O
during	O
a	O
couple	O
of	O
hours	O
,	O
not	O
knowing	O
from	O
one	O
minute	O
to	O
the	O
next	O
what	O
time	O
the	O
power	O
is	O
coming	O
on	O
.	O
"	O
In	O
this	O
northern	O
latitude	O
it	O
does	O
n't	O
get	O
dark	O
in	O
summer	O
until	O
about	O
10:30	O
p.m.	O
so	O
lighting	O
is	O
operate	O
except	O
at	O
some	O
crazy	O
time	O
like	O
11:45	O
at	O
night	O
,	O
whenever	O
there	O
is	O
power	O
,	O
unless	O
they	O
have	O
stand	O
-	O
by	O
diesel	O
generators	O
.	O
There	O
's	O
a	O
year	O
's	O
supply	O
of	O
diesel	O
oil	O
here	O
.	O
The	O
additional	O
context	O
,	O
that	O
people	O
in	O
the	O
country	O
described	O
are	O
dealing	O
with	O
electricity	O
issues	O
despite	O
there	O
being	O
a	O
year	O
's	O
worth	O
of	O
diesel	O
supply	O
,	O
is	O
now	O
made	O
clear	O
in	O
this	O
passage	O
.	O
Thus	O
we	O
can	O
conclude	O
that	O
the	O
correct	O
relation	O
here	O
is	O
Comparison	O
.	O
Contrast	O
.	O
Without	O
getting	O
this	O
context	O
and	O
just	O
seeing	O
the	O
two	O
sentences	O
in	O
which	O
the	O
arguments	O
are	O
contained	O
,	O
it	O
is	O
difficult	O
to	O
discern	O
this	O
as	O
an	O
annotator	O
.	O
This	O
shows	O
that	O
by	O
just	O
getting	O
exposure	O
to	O
the	O
two	O
arguments	O
,	O
without	O
additional	O
context	O
,	O
the	O
sense	O
may	O
be	O
marked	O
incorrectly	O
.	O
The	O
Wang	O
and	O
Lan	O
(	O
2015	O
)	O
parser	O
and	O
the	O
DiscoEval	O
parser	O
both	O
predicted	O
this	O
incorrectly	O
,	O
with	O
the	O
Wang	O
and	O
Lan	O
(	O
2015	O
)	O
parser	O
predicting	O
it	O
as	O
Contingency	O
.	O
Cause	O
and	O
the	O
BERT	B-MethodName
parser	O
predicting	O
it	O
as	O
Expansion	O
.	O
Conjunction	O
.	O
Similarly	O
,	O
the	O
following	O
example	O
,	O
also	O
contained	O
in	O
this	O
passage	O
,	O
has	O
a	O
different	O
true	O
annotation	O
than	O
one	O
would	O
think	O
from	O
only	O
seeing	O
the	O
arguments	O
:	O
(	O
6	O
)	O
One	O
housewife	O
says	O
:	O
"	O
With	O
an	O
electric	O
kitchen	O
I	O
have	O
to	O
do	O
my	O
whole	O
day	O
's	O
cooking	O
the	O
day	O
before	O
-	O
and	O
that	O
during	O
a	O
couple	O
of	O
hours	O
,	O
not	O
knowing	O
from	O
one	O
minute	O
to	O
the	O
next	O
what	O
time	O
the	O
power	O
is	O
coming	O
on	O
.	O
"	O
In	O
this	O
northern	O
latitude	O
it	O
does	O
n't	O
get	O
dark	O
in	O
summer	O
until	O
about	O
10:30	O
p.m.	O
so	O
lighting	O
is	O
operate	O
except	O
at	O
some	O
crazy	O
time	O
like	O
11:45	O
at	O
night	O
,	O
whenever	O
there	O
is	O
power	O
,	O
unless	O
they	O
have	O
stand	O
-	O
by	O
diesel	O
generators	O
.	O
The	O
relation	O
may	O
be	O
deemed	O
as	O
Expansion	O
.	O
Instantiation	O
.	O
However	O
,	O
by	O
reading	O
the	O
full	O
text	O
,	O
it	O
is	O
clear	O
that	O
it	O
should	O
be	O
labeled	O
as	O
Contingency	O
.	O
Cause	O
.	O
Like	O
the	O
last	O
example	O
,	O
a	O
clearer	O
view	O
of	O
the	O
full	O
text	O
is	O
needed	O
to	O
determine	O
the	O
proper	O
annotation	O
,	O
not	O
simply	O
the	O
two	O
arguments	O
.	O
These	O
observations	O
provide	O
insights	O
as	O
to	O
why	O
contextual	O
embeddings	O
with	O
document	O
context	O
such	O
as	O
the	O
next	O
sentence	O
prediction	O
task	O
helps	O
with	O
implicit	B-TaskName
discourse	I-TaskName
relation	I-TaskName
classification	I-TaskName
(	O
Shi	O
and	O
Demberg	O
,	O
2019	O
)	O
.	O
More	O
generally	O
,	O
we	O
believe	O
future	O
work	O
on	O
discourse	B-TaskName
parsing	I-TaskName
should	O
look	O
beyond	O
only	O
the	O
arguments	O
of	O
a	O
relation	O
because	O
of	O
the	O
different	O
interpretations	O
one	O
would	O
give	O
when	O
taking	O
the	O
relation	O
in	O
vs.	O
out	O
of	O
context	O
.	O
We	O
believe	O
that	O
argument	O
pairs	O
with	O
low	O
specificity	O
and	O
one	O
or	O
more	O
pronouns	O
may	O
be	O
especially	O
in	O
need	O
of	O
this	O
extra	O
context	O
,	O
but	O
more	O
experimentation	O
will	O
have	O
to	O
be	O
done	O
to	O
confirm	O
this	O
hypothesis	O
.	O
Attachment	O
issues	O
tend	O
to	O
occur	O
throughout	O
the	O
RST	O
parse	O
tree	O
,	O
and	O
relations	O
are	O
often	O
misclassified	O
as	O
Same	O
-	O
Unit	O
and	O
Elaboration	O
.	O
Regarding	O
insights	O
for	O
the	O
RST	O
Discourse	O
Treebank	O
,	O
a	O
piece	O
of	O
the	O
RST	O
tree	O
for	O
this	O
paragraph	O
can	O
be	O
seen	O
in	O
3	O
.	O
Here	O
,	O
the	O
EDU	O
"	O
One	O
housewife	O
says	O
"	O
should	O
attach	O
to	O
the	O
EDU	O
after	O
it	O
,	O
"	O
With	O
an	O
electric	O
kitchen	O
I	O
have	O
to	O
do	O
my	O
whole	O
day	O
's	O
cooking	O
the	O
day	O
before	O
"	O
.	O
However	O
,	O
it	O
instead	O
attaches	O
to	O
EDUs	O
from	O
the	O
preceding	O
sentences	O
,	O
which	O
is	O
incorrect	O
,	O
as	O
these	O
two	O
sentences	O
do	O
not	O
contain	O
what	O
the	O
housewife	O
says	O
.	O
We	O
saw	O
several	O
other	O
attachment	O
issues	O
in	O
the	O
text	O
,	O
including	O
a	O
couple	O
where	O
the	O
attachment	O
should	O
go	O
up	O
/	O
down	O
by	O
several	O
levels	O
.	O
We	O
also	O
saw	O
several	O
instances	O
of	O
the	O
relation	O
being	O
incorrectly	O
tagged	O
as	O
Same	O
-	O
Unit	O
or	O
Elaboration	O
,	O
some	O
of	O
which	O
can	O
be	O
seen	O
in	O
the	O
diagram	O
.	O
Attachment	O
issues	O
are	O
a	O
particular	O
problem	O
for	O
RST	O
parsing	O
due	O
to	O
its	O
hierarchical	O
nature	O
;	O
one	O
at	O
-	O
tachment	O
issue	O
can	O
lead	O
to	O
error	O
propagation	O
where	O
the	O
accuracy	B-MetricName
of	O
the	O
attachments	O
further	O
in	O
the	O
tree	O
is	O
impacted	O
by	O
that	O
of	O
the	O
current	O
one	O
.	O
Reducing	O
this	O
error	O
is	O
of	O
the	O
utmost	O
importance	O
for	O
future	O
parsers	O
.	O

Neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
(	O
Kalchbrenner	O
and	O
Blunsom	O
,	O
2013	O
;	O
Sutskever	O
et	O
al	O
,	O
2014	O
;	O
Bahdanau	O
et	O
al	O
,	O
2015	O
;	O
Gehring	O
et	O
al	O
,	O
2017	O
;	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
has	O
shown	O
its	O
superiority	O
and	O
drawn	O
much	O
attention	O
in	O
recent	O
years	O
.	O
Although	O
the	O
NMT	O
model	O
can	O
achieve	O
promising	O
results	O
for	O
highresource	O
language	O
pairs	O
,	O
it	O
is	O
unaffordable	O
to	O
train	O
separate	O
models	O
for	O
all	O
the	O
language	O
pairs	O
since	O
there	O
are	O
thousands	O
of	O
languages	O
in	O
the	O
world	O
(	O
Tan	O
et	O
al	O
,	O
2019	O
;	O
Aharoni	O
et	O
al	O
,	O
2019	O
;	O
Arivazhagan	O
et	O
al	O
,	O
2019	O
)	O
.	O
A	O
typical	O
solution	O
to	O
reduce	O
the	O
model	O
size	O
and	O
the	O
training	O
cost	O
is	O
to	O
handle	O
multiple	O
languages	O
in	O
a	O
single	O
multilingual	O
neural	O
machine	B-TaskName
translation	I-TaskName
(	O
MNMT	O
)	O
model	O
(	O
Ha	O
et	O
al	O
,	O
2016	O
;	O
Firat	O
et	O
al	O
,	O
2016	O
;	O
Johnson	O
et	O
al	O
,	O
2017	O
;	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
standard	O
paradigm	O
of	O
MNMT	O
proposed	O
by	O
Johnson	O
et	O
al	O
(	O
2017	O
)	O
contains	O
a	O
language	O
-	O
shared	O
encoder	O
and	O
decoder	O
with	O
a	O
special	O
language	O
indicator	O
in	O
the	O
input	O
sentence	O
to	O
determine	O
the	O
target	O
language	O
.	O
Because	O
different	O
languages	O
share	O
all	O
of	O
the	O
model	O
parameters	O
in	O
the	O
standard	O
MNMT	O
model	O
,	O
the	O
model	O
tends	O
to	O
converge	O
to	O
a	O
region	O
where	O
there	O
are	O
low	O
errors	O
for	O
all	O
the	O
languages	O
.	O
Therefore	O
,	O
the	O
MNMT	O
model	O
trained	O
on	O
the	O
combined	O
data	O
generally	O
captures	O
the	O
general	B-TaskName
knowledge	I-TaskName
,	O
but	O
ignores	O
the	O
language	O
-	O
specific	O
knowledge	O
,	O
rendering	O
itself	O
sub	O
-	O
optimal	O
for	O
the	O
translation	O
of	O
a	O
specific	O
language	O
(	O
Sachan	O
and	O
Neubig	O
,	O
2018	O
;	O
Blackwood	O
et	O
al	O
,	O
2018	O
;	O
Wang	O
et	O
al	O
,	O
2020b	O
)	O
.	O
To	O
retain	O
the	O
language	O
-	O
specific	O
knowledge	O
,	O
some	O
researches	O
turn	O
to	O
augment	O
the	O
NMT	O
model	O
with	O
language	O
-	O
specific	O
modules	O
,	O
e.g.	O
,	O
the	O
language	O
-	O
specific	O
attention	O
module	O
(	O
Blackwood	O
et	O
al	O
,	O
2018	O
)	O
,	O
decoupled	O
multilingual	O
encoders	O
and/or	O
decoders	O
(	O
Vázquez	O
et	O
al	O
,	O
2019	O
;	O
Escolano	O
et	O
al	O
,	O
2020	O
)	O
and	O
the	O
lightweight	O
language	O
adapters	O
.	O
However	O
,	O
these	O
methods	O
suffer	O
from	O
the	O
parameter	O
increment	O
problem	O
,	O
because	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
increases	O
linearly	O
with	O
the	O
number	O
of	O
languages	O
.	O
Besides	O
,	O
the	O
structure	O
,	O
size	O
,	O
and	O
location	O
of	O
the	O
module	O
have	O
a	O
large	O
influence	O
on	O
the	O
final	O
performance	O
,	O
which	O
requires	O
specialized	O
manual	O
design	O
.	O
As	O
a	O
result	O
,	O
these	O
problems	O
often	O
prevent	O
the	O
application	O
of	O
these	O
methods	O
in	O
some	O
scenarios	O
.	O
Based	O
on	O
the	O
above	O
,	O
we	O
aim	O
to	O
propose	O
a	O
method	O
that	O
can	O
retain	O
the	O
general	O
and	O
language	O
-	O
specific	O
knowledge	O
,	O
and	O
keep	O
a	O
stable	O
model	O
size	O
as	O
the	O
number	O
of	O
language	O
-	O
pair	O
increases	O
without	O
introducing	O
any	O
specialized	O
module	O
.	O
To	O
achieve	O
this	O
,	O
we	O
propose	O
to	O
divide	O
the	O
model	O
neurons	O
into	O
two	O
parts	O
based	O
on	O
their	O
importance	O
:	O
the	O
general	O
neurons	O
which	O
are	O
used	O
to	O
retain	O
the	O
general	B-TaskName
knowledge	I-TaskName
of	O
all	O
the	O
languages	O
,	O
and	O
the	O
language	O
-	O
specific	O
neurons	O
which	O
are	O
used	O
to	O
retain	O
the	O
language	O
-	O
specific	O
knowledge	O
.	O
Specifically	O
,	O
we	O
first	O
pre	O
-	O
train	O
a	O
standard	O
MNMT	O
model	O
on	O
all	O
language	O
data	O
and	O
then	O
evaluate	O
the	O
importance	O
of	O
each	O
neuron	O
in	O
each	O
language	O
pair	O
.	O
According	O
to	O
their	O
importance	O
,	O
we	O
divide	O
the	O
neurons	O
into	O
the	O
general	O
neurons	O
and	O
the	O
language	O
-	O
specific	O
neurons	O
.	O
After	O
that	O
,	O
we	O
finetune	O
the	O
translation	O
model	O
on	O
all	O
language	O
pairs	O
.	O
In	O
this	O
process	O
,	O
only	O
the	O
general	O
neurons	O
and	O
the	O
corresponding	O
language	O
-	O
specific	O
neurons	O
for	O
the	O
current	O
language	O
pair	O
participate	O
in	O
training	O
.	O
Experimental	O
results	O
on	O
different	O
languages	O
show	O
that	O
the	O
proposed	O
method	O
outperforms	O
several	O
strong	O
baselines	O
.	O
Our	O
contributions	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
We	O
propose	O
a	O
method	O
that	O
can	O
improve	O
the	O
translation	O
performance	O
of	O
the	O
MNMT	O
model	O
without	O
introducing	O
any	O
specialized	O
modules	O
or	O
adding	O
new	O
parameters	O
.	O
We	O
show	O
that	O
the	O
similar	O
languages	O
share	O
some	O
common	O
features	O
that	O
can	O
be	O
captured	O
by	O
some	O
specific	O
neurons	O
of	O
the	O
MNMT	O
model	O
.	O
We	O
show	O
that	O
some	O
modules	O
tend	O
to	O
capture	O
the	O
general	B-TaskName
knowledge	I-TaskName
while	O
some	O
modules	O
are	O
more	O
essential	O
for	O
capturing	O
the	O
languagespecific	O
knowledge	O
.	O

The	O
basic	O
idea	O
of	O
importance	O
evaluation	O
is	O
to	O
determine	O
which	O
neurons	O
are	O
essential	O
to	O
all	O
languages	O
while	O
which	O
neurons	O
are	O
responsible	O
for	O
some	O
specific	O
languages	O
.	O
For	O
a	O
neuron	O
i	O
,	O
its	O
average	O
importance	O
I	O
across	O
language	O
pairs	O
is	O
defined	O
as	O
follow	O
:	O
I	O
(	O
i	O
)	O
=	O
1	O
M	O
M	O
m=1	O
Θ	B-HyperparameterName
m	O
(	O
i	O
)	O
,	O
(	O
1	O
)	O
where	O
the	O
Θ	B-HyperparameterName
(	O
)	O
denotes	O
the	O
importance	O
evaluation	O
function	O
and	O
M	O
denotes	O
the	O
number	O
of	O
language	O
pairs	O
.	O
This	O
value	O
correlates	O
positively	O
with	O
how	O
important	O
the	O
neuron	O
is	O
to	O
all	O
languages	O
.	O
For	O
the	O
importance	O
evaluation	O
function	O
Θ	B-HyperparameterName
(	O
)	O
,	O
we	O
adopt	O
two	O
schemes	O
:	O
one	O
is	O
based	O
on	O
the	O
Taylor	O
Expansion	O
and	O
the	O
other	O
is	O
based	O
on	O
the	O
Absolute	O
Value	O
.	O
Taylor	O
Expansion	O
We	O
adopt	O
a	O
criterion	O
based	O
on	O
the	O
Taylor	O
Expansion	O
(	O
Molchanov	O
et	O
al	O
,	O
2017	O
)	O
,	O
where	O
we	O
directly	O
approximate	O
the	O
change	O
in	O
loss	B-MetricName
when	O
removing	O
a	O
particular	O
neuron	O
.	O
Let	O
h	O
i	O
be	O
the	O
output	O
produced	O
from	O
neuron	O
i	O
and	O
H	O
represents	O
the	O
set	O
of	O
other	O
neurons	O
.	O
Assuming	O
the	O
independence	O
of	O
each	O
neuron	O
in	O
the	O
model	O
,	O
the	O
change	O
of	O
loss	B-MetricName
when	O
removing	O
a	O
certain	O
neuron	O
can	O
be	O
represented	O
as	O
:	O
|	O
∆L	O
(	O
h	O
i	O
)	O
|	O
=	O
|	O
L	O
(	O
H	O
,	O
h	O
i	O
=	O
0	B-DatasetName
)	O
−	O
L	O
(	O
H	O
,	O
h	O
i	O
)	O
|	O
,	O
(	O
2	O
)	O
where	O
L	O
(	O
H	O
,	O
h	O
i	O
=	O
0	B-DatasetName
)	O
is	O
the	O
loss	B-MetricName
value	O
if	O
the	O
neuron	O
i	O
is	O
pruned	O
and	O
L	O
(	O
H	O
,	O
h	O
i	O
)	O
is	O
the	O
loss	B-MetricName
if	O
it	O
is	O
not	O
pruned	O
.	O
For	O
the	O
function	O
L	O
(	O
H	O
,	O
h	O
i	O
)	O
,	O
its	O
Taylor	O
Expansion	O
at	O
point	O
h	O
i	O
=	O
a	O
is	O
:	O
L	O
(	O
H	O
,	O
h	O
i	O
)	O
=	O
N	O
n=0	O
L	O
n	O
(	O
H	O
,	O
a	O
)	O
n	O
!	O
(	O
h	O
i	O
−	O
a	O
)	O
n	O
+	O
R	O
N	O
(	O
h	O
i	O
)	O
,	O
(	O
3	O
)	O
where	O
L	O
n	O
(	O
H	O
,	O
a	O
)	O
is	O
the	O
n	O
-	O
th	O
derivative	O
of	O
L	O
(	O
H	O
,	O
h	O
i	O
)	O
evaluated	O
at	O
point	O
a	O
and	O
R	O
N	O
(	O
h	O
i	O
)	O
is	O
N	O
-	O
th	O
remainder	O
.	O
Then	O
,	O
approximating	O
L	O
(	O
H	O
,	O
h	O
i	O
=	O
0	B-DatasetName
)	O
with	O
a	O
firstorder	O
Taylor	O
polynomial	O
where	O
h	O
i	O
equals	O
zero	O
:	O
L	O
(	O
H	O
,	O
h	O
i	O
=	O
0	B-DatasetName
)	O
=	O
L	O
(	O
H	O
,	O
h	O
i	O
)	O
−	O
∂L	O
(	O
H	O
,	O
h	O
i	O
)	O
∂h	O
i	O
h	O
i	O
−R	O
1	O
(	O
h	O
i	O
)	O
.	O
(	O
4	O
)	O
The	O
remainder	O
R	O
1	O
can	O
be	O
represented	O
in	O
the	O
form	O
of	O
Lagrange	O
:	O
R	O
1	O
(	O
h	O
i	O
)	O
=	O
∂	O
2	O
L	O
(	O
H	O
,	O
h	O
i	O
)	O
∂	O
2	O
δh	O
i	O
h	O
2	O
i	O
,	O
(	O
5	O
)	O
where	O
δ	B-HyperparameterName
(	O
0	B-DatasetName
,	O
1	O
)	O
.	O
Considering	O
the	O
use	O
of	O
ReLU	B-MethodName
activation	B-HyperparameterName
function	I-HyperparameterName
(	O
Glorot	O
et	O
al	O
,	O
2011	O
)	O
in	O
the	O
model	O
,	O
the	O
first	O
derivative	O
of	O
loss	B-MetricName
function	O
tends	O
to	O
be	O
constant	O
,	O
so	O
the	O
second	O
order	O
term	O
tends	O
to	O
be	O
zero	O
in	O
the	O
end	O
of	O
training	O
.	O
Thus	O
,	O
we	O
can	O
ignore	O
the	O
remainder	O
and	O
get	O
the	O
importance	O
evaluation	O
function	O
as	O
follows	O
:	O
Θ	B-HyperparameterName
TE	O
(	O
i	O
)	O
=	O
|	O
∆L	O
(	O
h	O
i	O
)	O
|	O
=	O
∂L	O
(	O
H	O
,	O
h	O
i	O
)	O
∂h	O
i	O
h	O
i	O
.	O
(	O
6	O
)	O
In	O
practice	O
,	O
we	O
need	O
to	O
accumulate	O
the	O
product	O
of	O
the	O
activation	O
and	O
the	O
gradient	O
of	O
the	O
objective	O
function	O
w.r.t	O
to	O
the	O
activation	O
,	O
which	O
is	O
easily	O
computed	O
during	O
back	O
-	O
propagation	O
.	O
Finally	O
,	O
the	O
evaluation	O
function	O
is	O
shown	O
as	O
:	O
Θ	B-HyperparameterName
m	O
TE	O
(	O
i	O
l	O
)	O
=	O
1	O
T	O
m	O
t	O
δL	O
(	O
H	O
,	O
h	O
l	O
i	O
)	O
δh	O
l	O
i	O
h	O
l	O
i	O
,	O
(	O
7	O
)	O
where	O
h	O
l	O
i	O
is	O
the	O
activation	O
value	O
of	O
the	O
i	O
-	O
th	O
neuron	O
of	O
l	O
-	O
th	O
layer	O
and	O
T	O
m	O
is	O
the	O
number	O
of	O
the	O
training	O
examples	O
of	O
language	O
pair	O
m.	O
The	O
criterion	O
is	O
computed	O
on	O
the	O
data	O
of	O
language	O
pair	O
m	O
and	O
averaged	O
over	O
T	O
m	O
.	O
Absolute	O
Value	O
We	O
adopt	O
the	O
magnitude	O
-	O
based	O
neuron	O
importance	O
evaluation	O
scheme	O
(	O
See	O
et	O
al	O
,	O
2016	O
)	O
,	O
where	O
the	O
absolute	O
value	O
of	O
each	O
neuron	O
's	O
activation	O
value	O
is	O
treated	O
as	O
the	O
importance	O
:	O
Θ	B-HyperparameterName
m	O
AV	O
(	O
i	O
l	O
)	O
=	O
1	O
T	O
m	O
t	O
|	O
h	O
l	O
i	O
|	O
.	O
(	O
8	O
)	O
The	O
notations	O
in	O
the	O
above	O
equation	O
are	O
the	O
same	O
as	O
those	O
in	O
the	O
Equation	O
7	O
.	O
After	O
the	O
importance	O
of	O
each	O
neuron	O
is	O
evaluated	O
on	O
the	O
combined	O
data	O
,	O
we	O
need	O
to	O
determine	O
the	O
role	O
of	O
each	O
neuron	O
in	O
the	O
fine	O
-	O
tuning	O
step	O
following	O
the	O
method	O
in	O
the	O
next	O
section	O
.	O

According	O
to	O
the	O
overall	O
importance	O
I	O
(	O
i	O
)	O
in	O
Equation	O
1	O
,	O
the	O
value	O
correlates	O
positively	O
with	O
how	O
important	O
the	O
neuron	O
is	O
to	O
all	O
languages	O
.	O
Therefore	O
,	O
we	O
rank	O
the	O
neurons	O
in	O
each	O
layer	O
based	O
on	O
the	O
importance	O
and	O
make	O
the	O
top	O
ρ	O
percentage	O
as	O
general	O
neurons	O
that	O
are	O
responsible	O
for	O
capturing	O
general	B-TaskName
knowledge	I-TaskName
.	O
Language	O
-	O
specific	O
Neurons	O
Next	O
,	O
we	O
regard	O
other	O
neurons	O
except	O
for	O
the	O
general	O
neurons	O
as	O
the	O
language	O
-	O
specific	O
neurons	O
and	O
determine	O
which	O
language	O
pair	O
to	O
assign	O
them	O
to	O
.	O
To	O
achieve	O
this	O
,	O
we	O
compute	O
an	O
importance	O
threshold	O
for	O
each	O
neuron	O
:	O
λ	O
(	O
i	O
)	O
=	O
k	O
×	O
max	O
(	O
Θ	B-HyperparameterName
m	O
(	O
i	O
)	O
)	O
,	O
m	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
M	O
}	O
,	O
k	O
[	O
0	B-DatasetName
,	O
1	O
]	O
(	O
9	O
)	O
,	O
where	O
max	O
(	O
Θ	B-HyperparameterName
m	O
(	O
i	O
)	O
)	O
denotes	O
the	O
maximum	O
importance	O
of	O
this	O
neuron	O
in	O
all	O
language	O
pairs	O
and	O
k	O
is	O
a	O
hyper	O
-	O
parameter	O
.	O
The	O
neuron	O
will	O
be	O
assigned	O
to	O
the	O
language	O
-	O
pairs	O
whose	O
importance	O
is	O
larger	O
than	O
the	O
threshold	O
.	O
When	O
the	O
importance	O
of	O
neurons	O
is	O
determined	O
,	O
the	O
number	O
of	O
language	O
pairs	O
associated	O
with	O
each	O
neuron	O
can	O
be	O
adjusted	O
according	O
to	O
k.	O
The	O
smaller	O
the	O
k	O
,	O
the	O
more	O
language	O
-	O
pairs	O
will	O
be	O
associated	O
with	O
the	O
specific	O
neurons	O
.	O
In	O
this	O
way	O
,	O
we	O
flexibly	O
determine	O
the	O
language	O
pairs	O
assigned	O
to	O
each	O
neuron	O
according	O
to	O
its	O
importance	O
in	O
different	O
languages	O
.	O
Note	O
that	O
the	O
neuron	O
allocation	O
is	O
based	O
on	O
the	O
importance	O
of	O
language	O
pair	O
.	O
We	O
have	O
also	O
tried	O
other	O
allocation	O
variants	O
,	O
e.g.	O
,	O
based	O
on	O
the	O
source	O
language	O
,	O
target	O
language	O
,	O
and	O
find	O
that	O
the	O
language	O
pair	O
-	O
based	O
method	O
is	O
the	O
best	O
among	O
of	O
these	O
methods	O
.	O
The	O
detailed	O
results	O
are	O
listed	O
in	O
Appendix	O
A.	O
After	O
this	O
step	O
,	O
the	O
model	O
is	O
continually	O
finetuned	O
on	O
the	O
combined	O
multilingual	O
data	O
.	O
If	O
the	O
training	O
data	O
is	O
from	O
a	O
specific	O
language	O
pair	O
,	O
only	O
the	O
general	O
neurons	O
and	O
the	O
language	O
-	O
specific	O
neurons	O
for	O
this	O
language	O
pair	O
will	O
participate	O
in	O
the	O
forward	O
computation	O
and	O
the	O
parameters	O
associated	O
with	O
them	O
will	O
be	O
updated	O
during	O
the	O
backward	O
propagation	O
.	O

For	O
fair	O
comparisons	O
,	O
we	O
implement	O
the	O
proposed	O
method	O
and	O
other	O
contrast	O
methods	O
on	O
the	O
advanced	O
Transformer	B-MethodName
model	O
using	O
the	O
open	O
-	O
source	O
toolkit	O
Fairseq	O
-	O
py	O
(	O
Ott	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
follow	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
to	O
set	O
the	O
configurations	O
of	O
the	O
NMT	O
model	O
,	O
which	O
consists	O
of	O
6	O
stacked	O
encoder	O
/	O
decoder	O
layers	O
with	O
the	O
layer	O
size	O
being	O
512	O
.	O
All	O
the	O
models	O
were	O
trained	O
on	O
4	O
NVIDIA	O
2080Ti	O
GPUs	O
where	O
each	O
was	O
allocated	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4	O
,	O
096	O
tokens	O
for	O
one	O
-	O
to	O
-	O
many	O
scenario	O
and	O
2	O
,	O
048	O
tokens	O
for	O
the	O
many	O
-	O
to	O
-	O
many	O
scenario	O
.	O
We	O
train	O
the	O
baseline	O
model	O
using	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
β	B-HyperparameterName
1	O
=	O
0.9	O
,	O
β	B-HyperparameterName
2	O
=	O
0.98	O
,	O
and	O
=	O
10	O
−9	O
.	O
The	O
proposed	O
models	O
are	O
further	O
trained	O
with	O
corresponding	O
parameters	O
initialized	O
by	O
the	O
pre	O
-	O
trained	O
baseline	O
model	O
.	O
We	O
vary	O
the	O
hyperparameter	O
ρ	O
that	O
controls	O
the	O
proportion	O
of	O
general	O
neurons	O
in	O
each	O
module	O
from	O
80	O
%	O
to	O
95	O
%	O
and	O
set	O
it	O
to	O
90	O
%	O
in	O
our	O
main	O
experiments	O
according	O
to	O
the	O
performance	O
.	O
The	O
detailed	O
results	O
about	O
this	O
hyper	O
-	O
parameter	O
are	O
list	O
in	O
Appendix	O
B.	O
We	O
set	O
the	O
hyper	O
-	O
parameter	O
k	O
to	O
0.7	O
and	O
do	O
more	O
analysis	O
on	O
it	O
in	O
Section	O
5.3	O
.	O
For	O
evaluation	O
,	O
we	O
use	O
beam	O
search	O
with	O
a	O
beam	O
size	O
of	O
4	O
and	O
length	O
penalty	O
α	B-HyperparameterName
=	O
0.6	O
.	O

The	O
final	O
translation	O
is	O
detokenized	O
and	O
then	O
the	O
quality	O
is	O
evaluated	O
using	O
the	O
4	O
-	O
gram	O
case	O
-	O
sensitive	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
with	O
the	O
SacreBLEU	B-MetricName
tool	O
(	O
Post	O
,	O
2018	O
)	O
.	O
4	O
Many	O
-	O
to	O
-	O
Many	O
The	O
results	O
are	O
given	O
in	O
Table	O
1	O
.	O
We	O
can	O
see	O
that	O
the	O
improvements	O
brought	O
by	O
+	O
TS	B-MethodName
and	O
+	O
Adapter	B-MethodName
methods	O
are	O
not	O
large	O
.	O
For	O
the	O
+	O
TS	B-MethodName
method	O
,	O
attention	O
module	O
may	O
be	O
not	O
essential	O
to	O
capture	O
language	O
-	O
specific	O
knowledge	O
,	O
and	O
thus	O
it	O
is	O
difficult	O
to	O
converge	O
to	O
good	O
optima	O
.	O
For	O
the	O
+	O
Adapter	B-MethodName
method	O
,	O
adding	O
an	O
adapter	O
module	O
to	O
the	O
end	O
of	O
each	O
layer	O
may	O
be	O
not	O
appropriate	O
for	O
some	O
languages	O
and	O
hence	O
has	O
a	O
loose	O
capture	O
to	O
the	O
specific	O
features	O
.	O
In	O
all	O
language	O
pairs	O
,	O
our	O
method	O
based	O
on	O
Taylor	O
Expansion	O
outperforms	O
all	O
the	O
baselines	O
in	O
the	O
datasets	O
.	O
Moreover	O
,	O
the	O
parameters	O
in	O
our	O
model	O
are	O
the	O
same	O
as	O
the	O
Multilingual	O
system	O
and	O
less	O
than	O
other	O
baselines	O
.	O
One	O
-	O
to	O
-	O
Many	O
The	O
results	O
are	O
given	O
in	O
Table	O
2	O
,	O
our	O
method	O
exceeds	O
the	O
multilingual	O
baseline	O
in	O
all	O
language	O
pairs	O
and	O
outperforms	O
other	O
baselines	O
in	O
most	O
language	O
pairs	O
without	O
capacity	O
increment	O
.	O
When	O
we	O
expand	O
the	O
model	O
capacity	O
to	O
the	O
level	O
of	O
+	O
Adapter	B-MethodName
,	O
our	O
approach	O
can	O
achieve	O
better	O
translation	O
performance	O
,	O
which	O
demonstrates	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O
Another	O
finding	O
is	O
that	O
the	O
results	O
of	O
the	O
individual	O
baseline	O
are	O
worse	O
than	O
other	O
baselines	O
.	O
The	O
reason	O
may	O
be	O
the	O
training	O
data	O
is	O
not	O
big	O
enough	O
,	O
individual	O
baseline	O
can	O
not	O
get	O
a	O
good	O
enough	O
optimization	O
on	O
0.6	O
M	O
sentences	O
,	O
while	O
the	O
MNMT	O
model	O
can	O
be	O
well	O
trained	O
with	O
a	O
total	O
of	O
7.2	O
M	O
data	O
.	O

In	O
our	O
method	O
,	O
we	O
allocate	O
neurons	O
based	O
on	O
their	O
importance	O
for	O
different	O
languages	O
.	O
The	O
rationality	O
behind	O
this	O
mechanism	O
is	O
that	O
different	O
neurons	O
should	O
have	O
distinct	O
importance	O
values	O
so	O
that	O
these	O
neurons	O
can	O
find	O
their	O
relevant	O
language	O
pairs	O
.	O
Therefore	O
,	O
we	O
show	O
the	O
importance	O
of	O
neurons	O
computed	O
by	O
Taylor	O
Expansion	O
in	O
different	O
modules	O
for	O
the	O
one	O
-	O
to	O
-	O
many	O
(	O
O2	O
M	O
)	O
and	O
many	O
-	O
to	O
-	O
many	O
(	O
M2	O
M	O
)	O
translation	O
tasks	O
.	O
are	O
Spanish	O
and	O
Portuguese	O
,	O
both	O
of	O
which	O
belong	O
to	O
the	O
Western	O
Romance	O
,	O
the	O
Romance	O
branch	O
of	O
the	O
Indo	O
-	O
European	O
family	O
,	O
while	O
the	O
last	O
one	O
is	O
Finnish	O
,	O
a	O
member	O
of	O
the	O
Finnish	O
-	O
Ugric	O
branch	O
of	O
the	O
Ural	O
family	O
.	O
As	O
we	O
can	O
see	O
,	O
the	O
importance	O
of	O
Spanish	O
and	O
Portuguese	O
are	O
always	O
similar	O
in	O
most	O
neurons	O
,	O
but	O
there	O
is	O
no	O
obvious	O
correlation	O
between	O
Finnish	O
and	O
the	O
other	O
two	O
languages	O
.	O
It	O
indicates	O
that	O
similar	O
languages	O
are	O
also	O
similar	O
in	O
the	O
distribution	O
of	O
the	O
neuron	O
importance	O
,	O
which	O
implies	O
that	O
the	O
common	O
features	O
in	O
similar	O
languages	O
can	O
be	O
captured	O
by	O
the	O
same	O
neurons	O
.	O
The	O
results	O
of	O
M2	O
M	O
are	O
shown	O
in	O
Figure	O
2	O
(	O
c	O
)	O
and	O
Figure	O
2	O
(	O
d	O
)	O
,	O
and	O
the	O
language	O
pairs	O
are	O
It	O
En	O
,	O
Ro	O
It	O
,	O
and	O
En	O
Ro	O
,	O
whose	O
BLEU	B-MetricName
scores	O
are	O
0.67	O
,	O
1	O
,	O
and	O
1.7	O
higher	O
than	O
the	O
multilingual	O
baseline	O
,	O
respectively	O
.	O
In	O
most	O
neurons	O
,	O
the	O
highest	O
importance	O
value	O
is	O
twice	O
as	O
high	O
as	O
the	O
lowest	O
and	O
this	O
high	O
variance	O
of	O
importance	O
provides	O
the	O
theoretical	O
basis	O
for	O
later	O
neuron	O
allocation	O
.	O
Moreover	O
,	O
we	O
can	O
see	O
a	O
lot	O
of	O
importance	O
peaks	O
of	O
the	O
two	O
language	O
pairs	O
:	O
Ro	O
It	O
and	O
En	O
Ro	O
,	O
which	O
means	O
that	O
these	O
neurons	O
are	O
especially	O
important	O
for	O
generating	O
the	O
translation	O
for	O
these	O
language	O
pairs	O
.	O
However	O
,	O
the	O
fluctuation	O
of	O
It	O
En	O
is	O
flat	O
with	O
almost	O
no	O
peaks	O
,	O
which	O
means	O
only	O
a	O
few	O
neurons	O
are	O
specific	O
to	O
this	O
language	O
pair	O
.	O
This	O
may	O
be	O
the	O
reason	O
why	O
some	O
language	O
pairs	O
have	O
higher	O
improvements	O
,	O
while	O
some	O
have	O
lower	O
improvements	O
.	O

When	O
the	O
importance	O
of	O
neurons	O
for	O
different	O
languages	O
is	O
determined	O
,	O
the	O
number	O
of	O
language	O
pairs	O
associated	O
with	O
each	O
neuron	O
can	O
be	O
adjusted	O
ac	O
-	O
Figure	O
5	O
:	O
∆	O
BLEU	B-MetricName
over	O
best	O
performance	O
when	O
erasing	O
the	O
general	O
or	O
language	O
-	O
specific	O
neurons	O
randomly	O
on	O
the	O
many	O
-	O
to	O
-	O
many	O
translation	O
task	O
.	O
cording	O
to	O
k.	O
When	O
k	B-HyperparameterName
=	I-HyperparameterName
1.0	O
,	O
the	O
threshold	O
is	O
max	O
(	O
Θ	B-HyperparameterName
m	O
(	O
i	O
)	O
)	O
as	O
computed	O
by	O
Equation	O
9	O
,	O
so	O
the	O
neurons	O
will	O
only	O
be	O
allocated	O
to	O
the	O
language	O
pair	O
with	O
the	O
highest	O
importance	O
,	O
and	O
when	O
k	B-HyperparameterName
=	I-HyperparameterName
0	B-DatasetName
,	O
the	O
threshold	O
is	O
0	B-DatasetName
so	O
the	O
neurons	O
will	O
be	O
shared	O
across	O
all	O
language	O
pairs	O
just	O
like	O
the	O
Multilingual	O
baseline	O
.	O
To	O
better	O
show	O
the	O
overall	O
impact	O
of	O
the	O
hyperparameter	O
k	O
,	O
we	O
vary	O
it	O
from	O
0	B-DatasetName
to	O
1	O
and	O
the	O
results	O
are	O
shown	O
in	O
Figure	O
4	O
.	O
As	O
we	O
can	O
see	O
,	O
the	O
translation	O
performance	O
of	O
the	O
two	O
proposed	O
approaches	O
increases	O
with	O
the	O
increment	O
of	O
k	O
and	O
reach	O
the	O
best	O
performance	O
when	O
k	O
equals	O
0.7	O
.	O
As	O
k	O
continues	O
to	O
increase	O
,	O
the	O
performance	O
deteriorates	O
,	O
which	O
indicates	O
that	O
the	O
over	O
-	O
specific	O
neurons	O
are	O
bad	O
at	O
capturing	O
the	O
common	O
features	O
shared	O
by	O
similar	O
languages	O
and	O
will	O
lead	O
to	O
performance	O
degradation	O
.	O

The	O
main	O
idea	O
of	O
our	O
method	O
is	O
to	O
let	O
the	O
general	B-TaskName
knowledge	I-TaskName
and	O
the	O
language	O
-	O
specific	O
knowledge	O
be	O
captured	O
by	O
different	O
neurons	O
of	O
our	O
method	O
.	O
To	O
verify	O
whether	O
this	O
goal	O
has	O
been	O
achieved	O
,	O
we	O
conduct	O
the	O
following	O
experiments	O
.	O
For	O
the	O
general	B-TaskName
knowledge	I-TaskName
,	O
we	O
randomly	O
erase	O
20	O
%	O
general	O
neurons	O
of	O
the	O
best	O
checkpoint	O
of	O
our	O
method	O
,	O
which	O
means	O
we	O
mask	O
the	O
output	O
value	O
of	O
these	O
neurons	O
to	O
0	B-DatasetName
,	O
then	O
generate	O
translation	O
using	O
it	O
.	O
For	O
languagespecific	O
knowledge	O
,	O
we	O
randomly	O
erase	O
50	O
%	O
specific	O
neurons	O
and	O
then	O
generate	O
translation	O
.	O
As	O
shown	O
in	O
Figure	O
5	O
,	O
when	O
the	O
general	O
neurons	O
are	O
erased	O
,	O
the	O
BLEU	B-MetricName
points	O
of	O
all	O
the	O
language	O
pairs	O
drop	O
a	O
lot	O
(	O
about	O
15	O
to	O
20	O
BLEU	B-MetricName
)	O
,	O
which	O
indicates	O
general	O
neurons	O
do	O
capture	O
the	O
general	B-TaskName
knowledge	I-TaskName
across	O
languages	O
.	O
For	O
specific	O
neurons	O
,	O
we	O
show	O
three	O
language	O
pairs	O
for	O
the	O
sake	O
of	O
convenience	O
.	O
We	O
can	O
see	O
that	O
when	O
the	O
neurons	O
associated	O
with	O
the	O
current	O
language	O
pair	O
are	O
erased	O
,	O
the	O
performance	O
of	O
this	O
language	O
pair	O
decreases	O
greatly	O
.	O
However	O
,	O
the	O
performance	O
of	O
other	O
language	O
pairs	O
only	O
declines	O
slightly	O
,	O
because	O
the	O
specific	O
knowledge	O
captured	O
by	O
these	O
specific	O
neurons	O
are	O
not	O
so	O
important	O
for	O
other	O
languages	O
.	O

We	O
represent	O
the	O
whole	O
corpus	O
D	O
with	O
an	O
undirected	O
graph	O
G	O
=	O
(	O
N	O
,	O
E	O
)	O
,	O
where	O
N	O
and	O
E	O
are	O
nodes	O
and	O
edges	O
in	O
the	O
graph	O
respectively	O
.	O
To	O
model	O
both	O
words	O
and	O
documents	O
,	O
each	O
of	O
them	O
is	O
represented	O
as	O
a	O
node	O
n	O
i	O
N	O
,	O
which	O
gives	O
rise	O
to	O
N	O
=	O
V	O
+	O
D	O
nodes	O
in	O
total	O
,	O
where	O
V	O
is	O
the	O
size	O
of	O
vocabulary	O
V	O
and	O
D	O
is	O
the	O
number	O
of	O
documents	O
in	O
corpus	O
D.	O
An	O
edge	O
(	O
n	O
i	O
,	O
n	O
j	O
)	O
indicates	O
the	O
relevance	O
of	O
node	O
n	O
i	O
and	O
n	O
j	O
,	O
whose	O
weight	O
is	O
determined	O
by	O
A	O
i	O
,	O
j	O
=	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
TF	O
-	O
IDF	O
ij	O
,	O
i	O
D	O
and	O
j	O
V	O
TF	O
-	O
IDF	O
ji	O
,	O
i	O
V	O
and	O
j	O
D	O
1	O
,	O
i	O
=	O
j	O
0	B-DatasetName
,	O
otherwise	O
(	O
1	O
)	O
where	O
A	O
is	O
the	O
adjacency	O
matrix	O
of	O
G	O
and	O
TF	O
-	O
IDF	O
ij	O
denotes	O
the	O
max	O
-	O
normalized	O
TF	O
-	O
IDF	O
(	O
Term	O
Frequency	O
-	O
Inverse	O
Document	O
Frequency	O
)	O
weight	O
of	O
word	O
j	O
in	O
document	O
i.	O
Besides	O
self	O
-	O
connections	O
,	O
we	O
only	O
apply	O
positive	O
weights	O
to	O
edges	O
between	O
documents	O
and	O
words	O
,	O
while	O
rely	O
on	O
the	O
model	O
to	O
capture	O
higher	O
-	O
order	O
relationships	O
,	O
e.g.	O
doc	O
-	O
doc	O
and	O
word	O
-	O
word	O
relationships	O
,	O
by	O
applying	O
graph	O
convolutions	O
on	O
graph	O
G.	O
I	O
X	O
X	O
T	O
I	O
	O
	O
	O
	O
EẐ	O
Z	O
Dirichlet	O
(	O
α	B-HyperparameterName
)	O
GX	O
L	O
rec	O
(	O
X	O
,	O
X	O
)	O
MMD	B-DatasetName
(	O
P	O
Z	O
,	O
QẐ	O
)	O
Figure	O
1	O
:	O
The	O
framework	O
of	O
GTM	O
.	O
Circles	O
denote	O
neural	O
networks	O
.	O
X	O
,	O
I	O
,	O
Ẑ	O
,	O
X	O
,	O
Z	O
are	O
the	O
TF	O
-	O
IDF	O
matrix	O
of	O
the	O
corpus	O
,	O
an	O
identity	O
matrix	O
,	O
latent	O
topics	O
of	O
all	O
documents	O
,	O
reconstructed	O
word	O
weights	O
and	O
topic	O
distributions	O
drawn	O
from	O
the	O
Dirichlet	O
prior	O
respectively	O
.	O
L	O
rec	O
(	O
X	O
,	O
X	O
)	O
and	O
MMD	B-DatasetName
(	O
P	O
Z	O
,	O
Q	O
Z	O
)	O
are	O
training	O
objectives	O
.	O

The	O
proposed	O
GTM	O
consists	O
of	O
an	O
encoder	O
E	O
and	O
a	O
decoder	O
G.	O
The	O
framework	O
is	O
shown	O
in	O
Figure	O
1	O
,	O
and	O
we	O
detail	O
the	O
architecture	O
in	O
the	O
following	O
.	O
The	O
encoder	O
network	O
E	O
maps	O
nodes	O
in	O
G	O
to	O
their	O
topic	O
distributions	O
by	O
iteratively	O
applying	O
graph	O
convolution	B-MethodName
to	O
the	O
node	O
features	O
.	O
Following	O
(	O
Kipf	O
and	O
Welling	O
,	O
2016	O
)	O
,	O
the	O
layer	O
-	O
wise	O
propagation	O
rule	O
of	O
the	O
graph	O
convolution	B-MethodName
at	O
layer	O
l	O
+	O
1	O
[	O
1	O
,	O
L	O
]	O
is	O
defined	O
as	O
H	O
(	O
l+1	O
)	O
=	O
σ	O
(	O
D	O
−	O
1	O
2	O
AD	O
−	O
1	O
2	O
H	O
(	O
l	O
)	O
W	O
(	O
l	O
)	O
)	O
(	O
2	O
)	O
where	O
A	O
R	O
N	O
×N	O
is	O
the	O
adjacency	O
matrix	O
of	O
G	O
,	O
D	O
ii	O
=	O
j	O
A	O
ij	O
,	O
W	O
(	O
l	O
)	O
R	O
d	O
(	O
l	O
)	O
×d	O
(	O
l+1	O
)	O
is	O
a	O
layerspecific	O
weight	O
matrix	O
where	O
d	O
(	O
l	O
)	O
is	O
the	O
output	O
size	O
of	O
layer	O
l	O
,	O
and	O
σ	O
denotes	O
an	O
activation	B-HyperparameterName
function	I-HyperparameterName
that	O
is	O
LeakyReLU	O
(	O
Maas	O
et	O
al	O
,	O
2013	O
)	O
in	O
this	O
paper	O
.	O
H	O
(	O
l	O
)	O
R	O
N	O
×d	O
(	O
l	O
)	O
is	O
the	O
activations	O
of	O
all	O
nodes	O
at	O
layer	O
l	O
and	O
H	O
(	O
0	B-DatasetName
)	O
i	O
is	O
the	O
embedding	O
of	O
node	O
i.	O
At	O
each	O
encoder	O
layer	O
,	O
what	O
the	O
graph	O
convolution	B-MethodName
does	O
is	O
aggregating	O
node	O
features	O
from	O
a	O
node	O
's	O
first	O
-	O
order	O
neighborhood	O
,	O
which	O
consequently	O
enlarges	O
the	O
receptive	O
field	O
of	O
the	O
central	O
node	O
and	O
enables	O
the	O
information	O
propagation	O
between	O
relevant	O
nodes	O
.	O
After	O
successively	O
applying	O
L	O
graph	O
convolution	B-MethodName
layers	O
,	O
the	O
encoding	O
of	O
a	O
node	O
essentially	O
involves	O
its	O
L	O
th	O
-	O
order	O
neighborhood	O
.	O
With	O
L	O
≥	O
2	O
,	O
doc	O
-	O
doc	O
and	O
word	O
-	O
word	O
relationships	O
are	O
naturally	O
captured	O
in	O
the	O
topic	O
inference	O
process	O
.	O
We	O
also	O
add	O
a	O
batch	B-MethodName
normalization	I-MethodName
(	O
Ioffe	O
and	O
Szegedy	O
,	O
2015	O
)	O
after	O
each	O
graph	O
convolution	B-MethodName
.	O
After	O
the	O
graph	O
encoding	O
,	O
a	O
softmax	B-MethodName
is	O
further	O
applied	O
to	O
the	O
node	O
features	O
of	O
a	O
document	O
to	O
produce	O
a	O
multinomial	O
topic	O
distributionẑ	O
R	O
K	O
,	O
where	O
K	O
is	O
the	O
topic	O
number	O
.	O
Based	O
on	O
the	O
inferred	O
topic	O
distributionẑ	O
,	O
the	O
decoder	O
network	O
G	O
tries	O
to	O
restore	O
the	O
original	O
document	O
representations	O
.	O
To	O
achieve	O
this	O
goal	O
,	O
we	O
employ	O
a	O
2	O
-	O
layer	O
MLP	B-DatasetName
with	O
LeakyReLU	O
activation	O
and	O
batch	B-MethodName
normalization	I-MethodName
in	O
the	O
first	O
layer	O
.	O
The	O
output	O
of	O
the	O
MLP	B-DatasetName
decoder	O
is	O
then	O
softmax	B-MethodName
-	O
normalized	O
to	O
generate	O
a	O
word	O
distributionx	O
R	O
V	O
.	O
The	O
decoder	O
is	O
also	O
used	O
to	O
interpret	O
topics	O
.	O
In	O
this	O
case	O
,	O
we	O
feed	O
to	O
the	O
decoder	O
an	O
identity	O
matrix	O
I	O
R	O
K×K	O
,	O
and	O
the	O
decoder	O
output	O
G	O
(	O
I	O
)	O
i	O
is	O
the	O
word	O
distribution	O
of	O
the	O
i	O
-	O
th	O
topic	O
.	O

Based	O
on	O
the	O
Wasserstein	O
Autoencoder	B-MethodName
(	O
Tolstikhin	O
et	O
al	O
,	O
2017	O
)	O
framework	O
,	O
the	O
training	O
objective	O
of	O
GTM	O
is	O
to	O
minimize	O
the	O
document	O
reconstruction	O
loss	B-MetricName
when	O
the	O
latent	O
topic	O
space	O
is	O
constrained	O
by	O
a	O
prior	O
distribution	O
.	O
The	O
reconstruction	O
loss	B-MetricName
is	O
defined	O
as	O
L	O
rec	O
(	O
X	O
,	O
X	O
)	O
=	O
−E	O
(	O
x	O
logx	O
)	O
,	O
(	O
3	O
)	O
where	O
x	O
denotes	O
the	O
TF	O
-	O
IDF	O
of	O
a	O
document	O
andx	O
is	O
the	O
reconstructed	O
word	O
distribution	O
corresponding	O
to	O
x.	O
we	O
use	O
TF	O
-	O
IDF	O
as	O
the	O
reconstruction	O
target	O
since	O
TF	O
-	O
IDF	O
basically	O
preserves	O
the	O
relative	O
importance	O
of	O
words	O
and	O
reduces	O
some	O
background	O
noise	O
that	O
may	O
hurt	O
topic	O
modeling	O
,	O
e.g.	O
,	O
stop	O
words	O
.	O
We	O
impose	O
a	O
Dirichlet	O
prior	O
,	O
the	O
conjugate	O
prior	O
of	O
the	O
multinomial	O
distribution	O
,	O
to	O
the	O
latent	O
topic	O
distributions	O
.	O
Following	O
W	O
-	O
LDA	B-MethodName
(	O
Nan	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
achieve	O
this	O
goal	O
by	O
minimizing	O
the	O
Maximum	O
Mean	O
Discrepancy	O
(	O
MMD	B-DatasetName
)	O
(	O
Gretton	O
et	O
al	O
,	O
2012	O
)	O
between	O
the	O
distribution	O
QẐ	O
of	O
inferred	O
topic	O
distributionsẑ	O
and	O
the	O
Dirichlet	O
prior	O
P	O
Z	O
from	O
which	O
we	O
draw	O
multinomial	O
noises	O
z	O
:	O
MMD	B-DatasetName
(	O
PZ	O
,	O
QẐ	O
)	O
=	O
1	O
m	O
(	O
m	O
−	O
1	O
)	O
i	O
=	O
j	O
k	O
(	O
z	O
(	O
i	O
)	O
,	O
z	O
(	O
j	O
)	O
)	O
+	O
1	O
n	O
(	O
n	O
−	O
1	O
)	O
i	O
=	O
j	O
k	O
(	O
ẑ	O
(	O
i	O
)	O
,	O
ẑ	O
(	O
j	O
)	O
)	O
−	O
2	O
mn	O
i	O
,	O
j	O
k	O
(	O
z	O
(	O
i	O
)	O
,	O
ẑ	O
(	O
j	O
)	O
)	O
,	O
(	O
4	O
)	O
where	O
m	O
and	O
n	O
are	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
from	O
Z	O
andẐ	O
respectively	O
(	O
m	O
and	O
n	O
are	O
batch	O
sizes	O
and	O
they	O
are	O
equal	O
in	O
our	O
experiments	O
)	O
,	O
and	O
k	O
:	O
Z×Z	O
R	O
is	O
the	O
kernel	O
function	O
.	O
We	O
use	O
the	O
information	O
diffusion	O
kernel	O
(	O
Lebanon	O
and	O
Lafferty	O
,	O
2003	O
)	O
as	O
in	O
W	O
-	O
LDA	B-MethodName
:	O
k	O
(	O
z	O
,	O
z	O
)	O
=	O
exp	O
(	O
−	O
arccos	O
2	O
(	O
K	O
i=1	O
z	O
i	O
z	O
i	O
)	O
)	O
,	O
(	O
5	O
)	O
which	O
is	O
sensitive	O
to	O
points	O
near	O
the	O
simplex	O
boundary	O
and	O
thus	O
more	O
suitable	O
for	O
the	O
sparse	O
topic	O
distributions	O
.	O

We	O
evaluate	O
our	O
model	O
on	O
three	O
datasets	O
:	O
20Newsgroups	O
consisting	O
of	O
11	O
,	O
259	O
documents	O
,	O
Grolier	O
consisting	O
of	O
29	O
,	O
762	O
documents	O
,	O
and	O
NYTimes	O
consisting	O
of	O
99	O
,	O
992	O
documents	O
.	O
We	O
use	O
the	O
preprocessed	O
20Newsgroups	O
of	O
(	O
Srivastava	O
and	O
Sutton	O
,	O
2017	O
)	O
,	O
and	O
preprocessed	O
Grolier	O
and	O
NYTimes	O
of	O
(	O
Wang	O
et	O
al	O
,	O
2019a	O
)	O
.	O
We	O
compare	O
the	O
performance	O
of	O
our	O
model	O
with	O
LDA	B-MethodName
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
,	O
NVDM	O
(	O
Miao	O
et	O
al	O
,	O
2016	O
)	O
,	O
ProdLDA	O
(	O
Srivastava	O
and	O
Sutton	O
,	O
2017	O
)	O
,	O
GraphBTM	O
(	O
Zhu	O
et	O
al	O
,	O
2018	O
)	O
,	O
ATM	O
(	O
Wang	O
et	O
al	O
,	O
2019a	O
)	O
and	O
W	O
-	O
LDA	B-MethodName
(	O
Nan	O
et	O
al	O
,	O
2019	O
)	O
using	O
topic	O
coherence	O
measures	O
(	O
Röder	O
et	O
al	O
,	O
2015	O
)	O
.	O
To	O
quantify	O
the	O
understandability	O
of	O
the	O
extracted	O
topics	O
,	O
a	O
topic	O
coherence	O
measure	O
aggregates	O
the	O
relatedness	O
scores	O
of	O
the	O
topic	O
words	O
(	O
topweighted	O
words	O
)	O
of	O
each	O
topic	O
,	O
where	O
the	O
word	O
relatedness	O
scores	O
are	O
estimated	O
based	O
on	O
word	O
co	O
-	O
occurrence	O
statistics	O
on	O
a	O
large	O
external	O
corpus	O
.	O
For	O
example	O
,	O
the	O
NPMI	O
coherence	O
measure	O
(	O
Aletras	O
and	O
Stevenson	O
,	O
2013	O
)	O
applies	O
a	O
sliding	O
window	O
of	O
size	O
10	O
over	O
the	O
Wikipedia	O
corpus	O
to	O
calculate	O
NPMI	O
(	O
Bouma	O
,	O
2009	O
)	O
for	O
word	O
pairs	O
.	O
We	O
use	O
three	O
topic	O
coherence	O
measures	O
in	O
our	O
experiments	O
:	O
C	O
A	O
(	O
Aletras	O
and	O
Stevenson	O
,	O
2013	O
)	O
,	O
C	O
P	O
(	O
Röder	O
et	O
al	O
,	O
2015	O
)	O
,	O
and	O
NPMI	O
.	O
The	O
topic	O
coherence	O
scores	O
are	O
calculated	O
using	O
Palmetto	O
(	O
Röder	O
et	O
al	O
,	O
2015	O
)	O
1	O
.	O
(	O
20	O
,	O
30	O
,	O
50	O
,	O
75	O
,	O
100	O
)	O
.	O
Bold	O
values	O
indicate	O
the	O
best	O
performing	O
model	O
under	O
the	O
corresponding	O
dataset	O
/	O
metric	O
setting	O
.	O
We	O
use	O
2	O
graph	O
convolution	B-MethodName
layers	O
with	O
output	O
dimensions	O
of	O
100	O
and	O
K	O
respectively	O
in	O
the	O
encoder	O
.	O
The	O
hidden	O
size	O
of	O
the	O
decoder	O
is	O
also	O
set	O
to	O
100	O
.	O
We	O
use	O
the	O
RMSProp	B-MethodName
(	O
Hinton	O
et	O
al	O
,	O
2012	O
)	O
optimizer	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.01	O
to	O
train	O
the	O
model	O
for	O
100	O
epochs	O
.	O
Since	O
the	O
training	O
datasets	O
scale	O
up	O
to	O
100	O
K	O
documents	O
,	O
i.e.	O
,	O
100	O
K	O
document	O
nodes	O
in	O
the	O
graph	O
,	O
it	O
is	O
hard	O
to	O
do	O
batch	O
training	O
on	O
a	O
single	O
GPU	O
given	O
the	O
large	O
memory	O
requirements	O
.	O
We	O
solve	O
this	O
issue	O
by	O
mini	O
-	O
batching	O
the	O
datasets	O
and	O
feeding	O
to	O
the	O
model	O
a	O
subgraph	O
consisting	O
of	O
1000	O
document	O
nodes	O
and	O
all	O
word	O
nodes	O
at	O
a	O
training	O
step	O
,	O
which	O
results	O
in	O
efficient	O
training	O
(	O
The	O
training	O
time	O
increases	O
almost	O
linearly	O
with	O
the	O
number	O
of	O
documents	O
)	O
and	O
makes	O
it	O
possible	O
to	O
apply	O
our	O
model	O
to	O
even	O
bigger	O
datasets	O
.	O
The	O
topic	O
coherence	O
results	O
on	O
the	O
three	O
datasets	O
are	O
shown	O
in	O
Table	O
1	O
,	O
where	O
each	O
value	O
is	O
the	O
average	O
of	O
5	O
topic	O
number	O
settings	O
:	O
20	O
,	O
30	O
,	O
50	O
,	O
75	O
,	O
100	O
.	O
From	O
Table	O
1	O
,	O
we	O
can	O
observe	O
that	O
our	O
proposed	O
GTM	O
is	O
the	O
best	O
-	O
performing	O
model	O
under	O
all	O
dataset	O
/	O
metric	O
settings	O
.	O
W	O
-	O
LDA	B-MethodName
,	O
ATM	O
,	O
LDA	B-MethodName
,	O
and	O
GraphBTM	O
alternately	O
achieve	O
the	O
second	O
-	O
best	O
but	O
they	O
are	O
always	O
under	O
-	O
performed	O
compared	O
to	O
our	O
model	O
.	O
As	O
described	O
in	O
section	O
2	O
,	O
GTM	O
is	O
an	O
extension	O
to	O
W	O
-	O
LDA	B-MethodName
with	O
the	O
main	O
difference	O
that	O
GTM	O
models	O
topics	O
in	O
a	O
larger	O
context	O
and	O
incorporates	O
more	O
global	O
information	O
with	O
the	O
graph	O
encoder	O
.	O
Therefore	O
the	O
improvements	O
of	O
GTM	O
over	O
W	O
-	O
LDA	B-MethodName
indicate	O
the	O
effectiveness	O
of	O
such	O
information	O
for	O
topic	O
modeling	O
.	O
We	O
only	O
experimented	O
GraphBTM	O
on	O
20Newsgroups	O
because	O
only	O
20Newsgroups	O
preserves	O
the	O
sequential	O
information	O
that	O
is	O
necessary	O
for	O
GraphBTM	O
to	O
build	O
graphs	O
.	O
GraphBTM	O
performs	O
well	O
on	O
the	O
C	O
A	O
metric	O
,	O
which	O
is	O
reasonable	O
since	O
C	O
A	O
is	O
a	O
coherence	O
measure	O
based	O
on	O
a	O
small	O
sliding	O
window	O
of	O
size	O
5	O
and	O
consequently	O
prefers	O
models	O
concentrating	O
on	O
a	O
smaller	O
context	O
like	O
GraphBTM	O
.	O
However	O
,	O
GraphBTM	O
fails	O
to	O
achieve	O
a	O
high	O
C	O
P	O
or	O
NPMI	O
score	O
,	O
which	O
uses	O
a	O
bigger	O
window	O
(	O
70	O
and	O
10	O
respectively	O
)	O
.	O
To	O
explore	O
how	O
topic	O
coherence	O
results	O
vary	O
w.r.t	O
.	O
different	O
topic	O
numbers	O
,	O
we	O
present	O
in	O
Figure	O
2	O
the	O
topic	O
coherence	O
scores	O
under	O
different	O
topic	O
numbers	O
settings	O
.	O
It	O
can	O
be	O
observed	O
in	O
Figure	O
2	O
that	O
GTM	O
enjoys	O
the	O
best	O
overall	O
performance	O
,	O
achieving	O
the	O
highest	O
scores	O
in	O
most	O
settings	O
.	O
LDA	B-MethodName
has	O
a	O
slightly	O
higher	O
NPMI	O
score	O
on	O
20Newsgroups	O
dataset	O
with	O
75	O
and	O
100	O
topics	O
,	O
nevertheless	O
,	O
GTM	O
outperforms	O
all	O
baseline	O
models	O
with	O
a	O
relatively	O
large	O
margin	O
on	O
other	O
settings	O
of	O
20Newsgroups	O
.	O
NVDM	O
is	O
apparently	O
the	O
worst	O
-	O
performing	O
model	O
,	O
while	O
performances	O
of	O
models	O
other	O
than	O
GTM	O
and	O
NVDM	O
are	O
not	O
so	O
consistent	O
.	O
Notably	O
,	O
W	O
-	O
LDA	B-MethodName
,	O
GraphBTM	O
,	O
and	O
LDA	B-MethodName
obtain	O
the	O
second	O
-	O
best	O
overall	O
C	O
P	O
,	O
C	O
A	O
,	O
and	O
NPMI	O
scores	O
respectively	O
.	O
Another	O
observation	O
from	O
Figure	O
2	O
is	O
that	O
GTM	O
performs	O
better	O
on	O
smaller	O
topics	O
,	O
probably	O
due	O
to	O
the	O
fact	O
that	O
topics	O
become	O
more	O
discriminative	O
against	O
each	O
other	O
when	O
the	O
topic	O
number	O
is	O
small	O
.	O
To	O
gain	O
an	O
intuitive	O
impression	O
on	O
the	O
discovered	O
topics	O
,	O
we	O
present	O
in	O
Table	O
2	O
4	O
topics	O
corresponding	O
to	O
4	O
out	O
of	O
20	O
ground	O
-	O
truth	O
categories	O
of	O
20Newsgroups	O
.	O
It	O
can	O
be	O
observed	O
that	O
the	O
topics	O
discovered	O
by	O
GTM	O
are	O
more	O
coherent	O
and	O
interpretable	O
,	O
containing	O
few	O
off	O
-	O
topic	O
words	O
.	O
As	O
a	O
comparison	O
,	O
GraphBTM	O
's	O
rec.autos	O
topic	O
mixes	O
up	O
automobiles	O
and	O
criminals	O
,	O
W	O
-	O
LDA	B-MethodName
's	O
misc.forsale	O
topic	O
is	O
difficult	O
to	O
identify	O
with	O
too	O
many	O
offtopic	O
words	O
,	O
while	O
LDA	B-MethodName
can	O
not	O
distinguish	O
between	O
rec.autos	O
and	O
misc.forsale	O
well	O
thus	O
recog	O
-	O
nizes	O
them	O
as	O
the	O
same	O
topic	O
.	O
It	O
can	O
be	O
observed	O
that	O
GTM	O
learns	O
more	O
discriminative	O
topics	O
by	O
examining	O
topic	O
words	O
from	O
overlapping	O
topics	O
,	O
e.g.	O
rec.autos	O
and	O
misc.forsale	O
.	O

As	O
aforesaid	O
,	O
only	O
a	O
few	O
studies	O
handle	O
this	O
type	O
of	O
knowledge	O
inference	O
.	O
The	O
m	O
-	O
TransH	O
method	O
(	O
Wen	O
et	O
al	O
,	O
2016	O
)	O
defines	O
n	O
-	O
ary	O
relations	O
as	O
the	O
mappings	O
from	O
the	O
attribute	O
sequences	O
to	O
the	O
attribute	O
values	O
.	O
Each	O
n	O
-	O
ary	O
fact	O
is	O
an	O
instance	O
of	O
the	O
corresponding	O
n	O
-	O
ary	O
relation	O
.	O
Then	O
,	O
m	O
-	O
TransH	O
generalizes	O
TransH	O
(	O
Wang	O
et	O
al	O
,	O
2014	O
)	O
on	O
binary	O
facts	O
to	O
nary	O
facts	O
via	O
attaching	O
each	O
n	O
-	O
ary	O
relation	O
with	O
a	O
hyperplane	O
.	O
RAE	B-MethodName
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
further	O
introduces	O
the	O
likelihood	O
that	O
two	O
attribute	O
values	O
co	O
-	O
participate	O
in	O
a	O
common	O
n	O
-	O
ary	O
fact	O
,	O
and	O
adds	O
the	O
corresponding	O
relatedness	O
loss	B-MetricName
multiplied	O
by	O
a	O
weight	O
factor	O
to	O
the	O
embedding	O
loss	B-MetricName
of	O
m	O
-	O
TransH.	O
Specifically	O
,	O
RAE	B-MethodName
applies	O
a	O
fully	O
-	O
connected	O
neural	O
network	O
to	O
model	O
the	O
above	O
likelihood	O
.	O
Differently	O
,	O
NaLP	O
(	O
Guan	O
et	O
al	O
,	O
2019	O
)	O
represents	O
each	O
n	O
-	O
ary	O
fact	O
as	O
a	O
set	O
of	O
attribute	O
-	O
value	O
pairs	O
directly	O
.	O
Then	O
,	O
convolution	B-MethodName
is	O
adopted	O
to	O
get	O
the	O
embeddings	O
of	O
the	O
attribute	O
-	O
value	O
pairs	O
,	O
and	O
a	O
fully	O
-	O
connected	O
neural	O
network	O
is	O
applied	O
to	O
evaluate	O
their	O
relatedness	O
and	O
finally	O
to	O
obtain	O
the	O
validity	O
score	O
of	O
the	O
input	O
n	O
-	O
ary	O
fact	O
.	O
In	O
these	O
methods	O
,	O
the	O
information	O
in	O
the	O
same	O
n	O
-	O
ary	O
fact	O
is	O
equal	O
-	O
status	O
.	O
Actually	O
,	O
in	O
each	O
n	O
-	O
ary	O
fact	O
,	O
a	O
primary	O
triple	O
can	O
usually	O
be	O
identified	O
with	O
other	O
information	O
as	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
,	O
as	O
exemplified	O
in	O
Section	O
1	O
.	O
Moreover	O
,	O
these	O
methods	O
are	O
deliberately	O
designed	O
only	O
for	O
the	O
inference	O
on	O
whole	O
facts	O
.	O
They	O
have	O
not	O
tackled	O
any	O
distinct	O
inference	O
task	O
.	O
In	O
practice	O
,	O
the	O
newly	O
proposed	O
flexible	O
knowledge	O
inference	O
is	O
also	O
prevalent	O
.	O

The	O
framework	O
of	O
NeuInfer	O
is	O
illustrated	O
in	O
Figure	O
1	O
,	O
with	O
the	O
5	O
-	O
ary	O
fact	O
presented	O
in	O
Section	O
1	O
as	O
an	O
example	O
.	O
For	O
an	O
n	O
-	O
ary	O
fact	O
F	O
ct	O
,	O
we	O
look	O
up	O
the	O
embeddings	O
of	O
its	O
relation	O
r	O
and	O
the	O
attributes	O
in	O
A	O
F	O
ct	O
from	O
the	O
embedding	O
matrix	O
M	O
R	O
R	O
|	O
R	O
|	O
×k	O
of	O
relations	O
and	O
attributes	O
,	O
where	O
R	O
is	O
the	O
set	O
of	O
all	O
the	O
relations	O
and	O
attributes	O
,	O
and	O
k	O
is	O
the	O
dimension	O
of	O
the	O
latent	O
vector	O
space	O
.	O
The	O
embeddings	O
of	O
h	O
,	O
t	O
,	O
and	O
the	O
attribute	O
values	O
in	O
V	O
F	O
ct	O
are	O
looked	O
up	O
from	O
the	O
embedding	O
matrix	O
M	O
E	O
R	O
|	O
E	O
|	O
×k	O
of	O
entities	O
and	O
attribute	O
values	O
,	O
where	O
E	O
is	O
the	O
set	O
of	O
all	O
the	O
entities	O
and	O
attribute	O
values	O
.	O
In	O
what	O
follows	O
,	O
the	O
embeddings	O
are	O
denoted	O
with	O
the	O
same	O
letters	O
but	O
in	O
boldface	O
by	O
convention	O
.	O
As	O
presented	O
in	O
Figure	O
1	O
,	O
these	O
embeddings	O
are	O
fed	O
into	O
the	O
validity	O
evaluation	O
component	O
(	O
the	O
upper	O
part	O
of	O
Figure	O
1	O
)	O
and	O
the	O
compatibility	O
evaluation	O
component	O
(	O
the	O
bottom	O
part	O
of	O
Figure	O
1	O
)	O
to	O
compute	O
the	O
validity	O
score	O
of	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
and	O
the	O
compatibility	O
score	O
of	O
F	O
ct	O
,	O
respectively	O
.	O
These	O
two	O
scores	O
are	O
used	O
to	O
generate	O
the	O
final	O
score	O
of	O
F	O
ct	O
by	O
weighted	O
sum	O
and	O
further	O
compute	O
the	O
loss	B-MetricName
.	O
Note	O
that	O
,	O
following	O
RAE	B-MethodName
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
and	O
NaLP	O
(	O
Guan	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
only	O
apply	O
fully	O
-	O
connected	O
neural	O
networks	O
in	O
NeuInfer	O
.	O

The	O
final	O
score	O
s	O
F	O
ct	O
of	O
F	O
ct	O
is	O
the	O
weighted	O
sum	O
of	O
the	O
above	O
validity	O
score	O
and	O
compatibility	O
score	O
:	O
s	O
F	O
ct	O
=	O
val	O
hrt	O
comp	O
F	O
ct	O
=	O
w	O
val	O
hrt	O
+	O
(	O
1	O
−	O
w	O
)	O
comp	O
F	O
ct	O
,	O
(	O
6	O
)	O
where	O
w	O
(	O
0	B-DatasetName
,	O
1	O
)	O
is	O
the	O
weight	O
factor	O
.	O
If	O
the	O
arity	O
of	O
F	O
ct	O
is	O
2	O
,	O
the	O
final	O
score	O
is	O
equal	O
to	O
the	O
validity	O
score	O
of	O
the	O
primary	O
triple	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
.	O
Then	O
,	O
Equation	O
(	O
6	O
)	O
is	O
reduced	O
to	O
:	O
s	O
F	O
ct	O
=	O
val	O
hrt	O
.	O
(	O
7	O
)	O
Currently	O
,	O
we	O
obtain	O
the	O
final	O
score	O
s	O
F	O
ct	O
of	O
F	O
ct	O
.	O
In	O
addition	O
,	O
F	O
ct	O
has	O
its	O
target	O
score	O
l	O
F	O
ct	O
.	O
By	O
comparing	O
s	O
F	O
ct	O
with	O
l	O
F	O
ct	O
,	O
we	O
get	O
the	O
binary	O
crossentropy	O
loss	B-MetricName
:	O
L	O
F	O
ct	O
=	O
−l	O
F	O
ct	O
logs	O
F	O
ct	O
−	O
(	O
1−l	O
F	O
ct	O
)	O
log	O
(	O
1−s	O
F	O
ct	O
)	O
,	O
(	O
8	O
)	O
where	O
l	O
F	O
ct	O
=	O
1	O
,	O
if	O
F	O
ct	O
T	O
,	O
otherwise	O
F	O
ct	O
T	O
−	O
,	O
l	O
F	O
ct	O
=	O
0	B-DatasetName
.	O
Here	O
,	O
T	O
is	O
the	O
training	O
set	O
and	O
T	O
−	O
is	O
the	O
set	O
of	O
negative	O
samples	O
constructed	O
by	O
corrupting	O
the	O
n	O
-	O
ary	O
facts	O
in	O
T	O
.	O
Specifically	O
,	O
for	O
each	O
n	O
-	O
ary	O
fact	O
in	O
T	O
,	O
we	O
randomly	O
replace	O
one	O
of	O
its	O
elements	O
with	O
a	O
random	O
element	O
in	O
E	O
/	O
R	O
to	O
generate	O
one	O
negative	O
sample	O
not	O
contained	O
in	O
T	O
.	O
We	O
then	O
optimize	O
NeuInfer	O
via	O
backpropagation	O
,	O
and	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
λ	O
is	O
used	O
as	O
the	O
optimizer	B-HyperparameterName
.	O

We	O
conduct	O
experiments	O
on	O
two	O
n	O
-	O
ary	O
datasets	O
.	O
The	O
first	O
one	O
is	O
JF17	O
K	O
(	O
Wen	O
et	O
al	O
,	O
2016	O
;	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
,	O
derived	O
from	O
Freebase	O
(	O
Bollacker	O
et	O
al	O
,	O
2008	O
)	O
.	O
In	O
JF17	O
K	O
,	O
an	O
n	O
-	O
ary	O
relation	O
of	O
a	O
certain	O
type	O
is	O
defined	O
by	O
a	O
fixed	O
number	O
of	O
ordered	O
attributes	O
.	O
Then	O
,	O
any	O
n	O
-	O
ary	O
fact	O
of	O
this	O
relation	O
is	O
denoted	O
as	O
an	O
ordered	O
sequence	O
of	O
attribute	O
values	O
corresponding	O
to	O
the	O
attributes	O
.	O
For	O
example	O
,	O
for	O
all	O
n	O
-	O
ary	O
facts	O
of	O
the	O
n	O
-	O
ary	O
relation	O
olympics.olympic	O
medal	O
honor	O
,	O
they	O
all	O
have	O
four	O
attribute	O
values	O
(	O
e.g.	O
,	O
2008	O
Summer	O
Olympics	O
,	O
U	O
nited	O
States	O
,	O
N	O
atalie	O
Coughlin	O
,	O
and	O
Swimming	O
at	O
the	O
2008	O
Summer	O
Olympics	O
-	O
W	O
omen	O
s	O
4×100	O
metre	O
f	O
reestyle	O
relay	O
)	O
,	O
corresponding	O
to	O
the	O
four	O
ordered	O
attributes	O
of	O
this	O
n	O
-	O
ary	O
relation	O
.	O
The	O
second	O
one	O
is	O
WikiPeople	O
(	O
Guan	O
et	O
al	O
,	O
2019	O
)	O
,	O
derived	O
from	O
Wikidata	O
(	O
Vrandečić	O
and	O
Krötzsch	O
,	O
2014	O
)	O
.	O
Its	O
n	O
-	O
ary	O
facts	O
are	O
more	O
diverse	O
than	O
JF17	O
K	O
's	O
.	O
For	O
example	O
,	O
for	O
all	O
n	O
-	O
ary	O
facts	O
that	O
narrate	O
award	O
-	O
received	O
,	O
some	O
have	O
the	O
attribute	O
together	O
-	O
with	O
,	O
while	O
some	O
others	O
do	O
not	O
.	O
Thus	O
,	O
WikiPeople	O
is	O
more	O
difficult	O
.	O
To	O
run	O
NeuInfer	O
on	O
JF17	O
K	O
and	O
WikiPeople	O
,	O
we	O
transform	O
the	O
representation	O
of	O
their	O
n	O
-	O
ary	O
facts	O
.	O
For	O
JF17	O
K	O
,	O
we	O
need	O
to	O
convert	O
each	O
attribute	O
value	O
sequence	O
of	O
a	O
specific	O
n	O
-	O
ary	O
relation	O
to	O
a	O
primary	O
triple	O
coupled	O
with	O
a	O
set	O
of	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
.	O
The	O
core	O
of	O
this	O
process	O
is	O
to	O
determine	O
the	O
primary	O
triple	O
,	O
formed	O
by	O
merging	O
the	O
two	O
primary	O
attributes	O
of	O
the	O
n	O
-	O
ary	O
relation	O
and	O
the	O
corresponding	O
attribute	O
values	O
.	O
The	O
two	O
primary	O
attributes	O
are	O
selected	O
based	O
on	O
RAE	B-MethodName
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
For	O
each	O
attribute	O
of	O
the	O
n	O
-	O
ary	O
relation	O
,	O
we	O
count	O
the	O
number	O
of	O
its	O
distinct	O
attribute	O
values	O
from	O
all	O
the	O
n	O
-	O
ary	O
facts	O
of	O
this	O
relation	O
.	O
The	O
two	O
attributes	O
that	O
correspond	O
to	O
the	O
largest	O
and	O
second	O
-	O
largest	O
numbers	O
are	O
chosen	O
as	O
the	O
two	O
primary	O
attributes	O
.	O
For	O
WikiPeople	O
,	O
since	O
there	O
is	O
a	O
primary	O
triple	O
for	O
each	O
n	O
-	O
ary	O
fact	O
in	O
Wikidata	O
,	O
with	O
its	O
help	O
,	O
we	O
simply	O
reorganize	O
a	O
set	O
of	O
attribute	O
-	O
value	O
pairs	O
in	O
WikiPeople	O
to	O
a	O
primary	O
triple	O
coupled	O
with	O
a	O
set	O
of	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
.	O
The	O
statistics	O
of	O
the	O
datasets	O
after	O
conversion	O
or	O
reorganization	O
are	O
outlined	O
in	O
ciprocal	O
Rank	O
(	O
MRR	B-MetricName
)	O
and	O
Hits@N	O
.	O
For	O
each	O
n	O
-	O
ary	O
test	O
fact	O
,	O
one	O
of	O
its	O
elements	O
is	O
removed	O
and	O
replaced	O
by	O
all	O
the	O
elements	O
in	O
E	O
/	O
R.	O
These	O
corrupted	O
n	O
-	O
ary	O
facts	O
are	O
fed	O
into	O
NeuInfer	O
to	O
obtain	O
the	O
final	O
scores	O
.	O
Based	O
on	O
these	O
scores	O
,	O
the	O
n	O
-	O
ary	O
facts	O
are	O
sorted	O
in	O
descending	O
order	O
,	O
and	O
the	O
rank	O
of	O
the	O
n	O
-	O
ary	O
test	O
fact	O
is	O
stored	O
.	O
Note	O
that	O
,	O
except	O
the	O
nary	O
test	O
fact	O
,	O
other	O
corrupted	O
n	O
-	O
ary	O
facts	O
existing	O
in	O
the	O
training	O
/	O
validation	O
/	O
test	O
set	O
,	O
are	O
discarded	O
before	O
sorting	O
.	O
This	O
process	O
is	O
repeated	O
for	O
all	O
other	O
elements	O
of	O
the	O
n	O
-	O
ary	O
test	O
fact	O
.	O
Then	O
,	O
MRR	B-MetricName
is	O
the	O
average	O
of	O
these	O
reciprocal	O
ranks	O
,	O
and	O
Hits@N	O
is	O
the	O
proportion	O
of	O
the	O
ranks	O
less	O
than	O
or	O
equal	O
to	O
N	O
.	O
Knowledge	O
inference	O
includes	O
entity	O
inference	O
and	O
relation	O
inference	O
.	O
As	O
presented	O
in	O
Table	O
1	O
,	O
the	O
number	O
of	O
relations	O
and	O
attributes	O
in	O
each	O
dataset	O
is	O
far	O
less	O
than	O
that	O
of	O
entities	O
and	O
attribute	O
values	O
(	O
on	O
JF17	O
K	O
,	O
|	O
R	O
|	O
=	O
501	O
,	O
while	O
|	O
E	O
|	O
=	O
28	O
,	O
645	O
;	O
on	O
WikiPeople	O
,	O
|	O
R	O
|	O
=	O
193	O
,	O
while	O
|	O
E	O
|	O
=	O
47	O
,	O
765	O
)	O
.	O
That	O
is	O
,	O
inferring	O
a	O
relation	O
/	O
attribute	O
is	O
much	O
simpler	O
than	O
inferring	O
an	O
entity	O
/	O
attribute	O
value	O
.	O
Therefore	O
,	O
we	O
adopt	O
MRR	B-MetricName
and	O
Hits@	O
{	O
1	O
,	O
3	O
,	O
10	O
}	O
on	O
entity	O
inference	O
,	O
while	O
pouring	O
attention	O
to	O
more	O
finegrained	O
metrics	O
,	O
i.e.	O
,	O
MRR	B-MetricName
and	O
Hits@1	B-MetricName
on	O
relation	O
inference	O
.	O

The	O
hyper	O
-	O
parameters	O
of	O
NeuInfer	O
are	O
tuned	O
via	O
grid	O
search	O
in	O
the	O
following	O
ranges	O
:	O
The	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
k	O
{	O
50	O
,	O
100	O
}	O
,	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
β	B-HyperparameterName
{	O
128	O
,	O
256	O
}	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
λ	O
{	O
5e	O
−6	O
,	O
1e	O
−5	O
,	O
5e	O
−5	O
,	O
1e	O
−4	O
,	O
5e	O
−4	O
,	O
1e	O
−3	O
}	O
,	O
the	O
numbers	O
n	O
1	O
and	O
n	O
2	O
of	O
the	O
neural	O
network	O
layers	O
of	O
"	O
hrt	O
-	O
FCNs	O
"	O
and	O
"	O
hrtav	O
-	O
FCNs	O
"	O
in	O
{	O
1	O
,	O
2	O
}	O
,	O
the	O
dimension	O
d	O
of	O
the	O
interaction	O
vector	O
o	O
hrta	O
i	O
v	O
i	O
in	O
{	O
50	O
,	O
100	O
,	O
200	O
,	O
400	O
,	O
500	O
,	O
800	O
,	O
1000	O
,	O
1200	O
}	O
,	O
the	O
weight	O
factor	O
w	O
of	O
the	O
scores	O
in	O
{	O
0.1	O
,	O
0.2	O
,	O
.	O
.	O
.	O
,	O
0.9	O
}	O
.	O
The	O
adopted	O
optimal	O
settings	O
are	O
:	O
k	B-HyperparameterName
=	I-HyperparameterName
100	O
,	O
β	B-HyperparameterName
=	O
128	O
,	O
λ	O
=	O
5e	O
−5	O
,	O
n	O
1	O
=	O
2	O
,	O
n	O
2	O
=	O
1	O
,	O
d	O
=	O
1200	O
,	O
and	O
w	O
=	O
0.1	O
for	O
JF17	O
K	O
;	O
k	B-HyperparameterName
=	I-HyperparameterName
100	O
,	O
β	B-HyperparameterName
=	O
128	O
,	O
λ	O
=	O
1e	O
−4	O
,	O
n	O
1	O
=	O
1	O
,	O
n	O
2	O
=	O
1	O
,	O
d	O
=	O
1000	O
,	O
and	O
w	O
=	O
0.3	O
for	O
WikiPeople	O
.	O

The	O
experimental	O
results	O
of	O
simple	O
entity	O
inference	O
are	O
reported	O
in	O
Table	O
2	O
.	O
From	O
the	O
results	O
,	O
it	O
can	O
be	O
observed	O
that	O
NeuInfer	O
performs	O
much	O
better	O
than	O
the	O
best	O
baseline	O
NaLP	O
,	O
which	O
verifies	O
the	O
superiority	O
of	O
NeuInfer	O
.	O
Specifically	O
,	O
on	O
JF17	O
K	O
,	O
the	O
performance	O
gap	O
between	O
NeuInfer	O
and	O
NaLP	O
is	O
significant	O
.	O
In	O
essence	O
,	O
0.151	O
on	O
MRR	B-MetricName
,	O
14.6	O
%	O
on	O
Hits@1	B-MetricName
,	O
16.2	O
%	O
on	O
Hits@3	B-MetricName
,	O
and	O
15.9	O
%	O
on	O
Hits@10	B-MetricName
.	O
On	O
WikiPeople	O
,	O
NeuInfer	O
also	O
outperforms	O
NaLP	O
.	O
It	O
testifies	O
the	O
strength	O
of	O
NeuInfer	O
treating	O
the	O
information	O
in	O
the	O
same	O
n	O
-	O
ary	O
fact	O
discriminatingly	O
.	O
By	O
differentiating	O
the	O
primary	O
triple	O
from	O
other	O
auxiliary	O
description	O
(	O
s	O
)	O
,	O
NeuInfer	O
considers	O
the	O
validity	O
of	O
the	O
primary	O
triple	O
and	O
the	O
compatibility	O
between	O
the	O
primary	O
triple	O
and	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
to	O
model	O
each	O
n	O
-	O
ary	O
fact	O
more	O
appropriately	O
and	O
reasonably	O
.	O
Thus	O
,	O
it	O
is	O
not	O
surprising	O
that	O
NeuInfer	O
beats	O
the	O
baselines	O
.	O
And	O
on	O
simpler	O
JF17	O
K	O
(	O
see	O
Section	O
5.1	O
)	O
,	O
NeuInfer	O
gains	O
more	O
significant	O
performance	O
improvement	O
than	O
on	O
WikiPeople	O
.	O

To	O
further	O
analyze	O
the	O
effectiveness	O
of	O
the	O
proposed	O
NeuInfer	O
method	O
,	O
we	O
look	O
into	O
the	O
breakdown	O
of	O
its	O
performance	O
on	O
different	O
arities	O
,	O
as	O
well	O
as	O
on	O
primary	O
triples	O
and	O
auxiliary	O
descriptions	O
.	O
Without	O
loss	B-MetricName
of	O
generality	O
,	O
here	O
we	O
report	O
only	O
the	O
experimental	O
results	O
on	O
simple	O
entity	O
inference	O
.	O
The	O
test	O
sets	O
are	O
grouped	O
into	O
binary	O
and	O
n	O
-	O
ary	O
(	O
n	O
>	O
2	O
)	O
categories	O
according	O
to	O
the	O
arities	O
of	O
the	O
facts	O
.	O
Table	O
5	O
presents	O
the	O
experimental	O
results	O
of	O
simple	O
entity	O
inference	O
on	O
these	O
two	O
categories	O
of	O
JF17	O
K	O
and	O
WikiPeople	O
.	O
From	O
the	O
tables	O
,	O
we	O
can	O
observe	O
that	O
NeuInfer	O
consistently	O
outperforms	O
the	O
baselines	O
on	O
both	O
categories	O
on	O
simpler	O
JF17K.	O
On	O
more	O
difficult	O
WikiPeople	O
,	O
NeuInfer	O
is	O
comparable	O
to	O
the	O
best	O
baseline	O
NaLP	O
on	O
the	O
binary	O
category	O
and	O
gains	O
much	O
better	O
performance	O
on	O
the	O
n	O
-	O
ary	O
category	O
in	O
terms	O
of	O
the	O
fine	O
-	O
grained	O
MRR	B-MetricName
and	O
Hits@1	B-MetricName
.	O
In	O
general	O
,	O
NeuInfer	O
performs	O
much	O
better	O
on	O
JF17	O
K	O
than	O
on	O
WikiPeople	O
.	O
We	O
attribute	O
this	O
to	O
the	O
simplicity	O
of	O
JF17K.	O
Where	O
does	O
the	O
above	O
performance	O
improvement	O
come	O
from	O
?	O
Is	O
it	O
from	O
inferring	O
the	O
head	O
/	O
tail	O
entities	O
in	O
primary	O
triples	O
or	O
the	O
attribute	O
values	O
in	O
auxiliary	O
descriptions	O
?	O
To	O
go	O
deep	O
into	O
it	O
,	O
we	O
study	O
the	O
performance	O
of	O
NeuInfer	O
on	O
inferring	O
the	O
head	O
/	O
tail	O
entities	O
and	O
the	O
attribute	O
values	O
and	O
compare	O
it	O
with	O
the	O
best	O
baseline	O
NaLP	O
.	O
The	O
detailed	O
experimental	O
results	O
are	O
demonstrated	O
in	O
Tables	O
6	O
and	O
7	O
.	O
It	O
can	O
be	O
observed	O
that	O
NeuInfer	O
brings	O
more	O
performance	O
gain	O
on	O
inferring	O
attribute	O
values	O
.	O
It	O
indicates	O
that	O
combining	O
the	O
validity	O
of	O
the	O
primary	O
triple	O
and	O
the	O
compatibility	O
between	O
the	O
primary	O
triple	O
and	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
to	O
model	O
each	O
n	O
-	O
ary	O
fact	O
is	O
more	O
effective	O
than	O
only	O
considering	O
the	O
relatedness	O
of	O
attribute	O
-	O
value	O
pairs	O
in	O
NaLP	O
,	O
especially	O
for	O
inferring	O
attribute	O
values	O
.	O

In	O
this	O
paper	O
,	O
we	O
distinguished	O
the	O
information	O
in	O
the	O
same	O
n	O
-	O
ary	O
fact	O
and	O
represented	O
each	O
n	O
-	O
ary	O
fact	O
as	O
a	O
primary	O
triple	O
coupled	O
with	O
a	O
set	O
of	O
its	O
auxiliary	O
description	O
(	O
s	O
)	O
.	O
We	O
then	O
proposed	O
a	O
neural	O
network	O
model	O
,	O
NeuInfer	O
,	O
for	O
knowledge	O
inference	O
on	O
n	O
-	O
ary	O
facts	O
.	O
NeuInfer	O
combines	O
the	O
validity	O
evaluation	O
of	O
the	O
primary	O
triple	O
and	O
the	O
compatibility	O
evaluation	O
of	O
the	O
n	O
-	O
ary	O
fact	O
to	O
obtain	O
the	O
validity	O
score	O
of	O
the	O
n	O
-	O
ary	O
fact	O
.	O
In	O
this	O
way	O
,	O
NeuInfer	O
has	O
the	O
ability	O
of	O
well	O
handling	O
simple	O
knowledge	O
inference	O
,	O
which	O
copes	O
with	O
the	O
inference	O
on	O
whole	O
facts	O
.	O
Furthermore	O
,	O
NeuInfer	O
is	O
capable	O
of	O
dealing	O
with	O
the	O
newly	O
proposed	O
flexible	O
knowledge	O
inference	O
,	O
which	O
tackles	O
the	O
inference	O
on	O
partial	O
facts	O
consisting	O
of	O
a	O
primary	O
triple	O
coupled	O
with	O
any	O
number	O
of	O
its	O
auxiliary	O
descriptive	O
attributevalue	O
pair	O
(	O
s	O
)	O
.	O
Experimental	O
results	O
manifest	O
the	O
merits	O
and	O
superiority	O
of	O
NeuInfer	O
.	O
Particularly	O
,	O
on	O
simple	O
entity	O
inference	O
,	O
NeuInfer	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
significantly	O
in	O
terms	O
of	O
all	O
the	O
metrics	O
.	O
NeuInfer	O
improves	O
the	O
performance	O
of	O
Hits@3	B-MetricName
even	O
by	O
16.2	O
%	O
on	O
JF17K.	O
In	O
this	O
paper	O
,	O
we	O
use	O
only	O
n	O
-	O
ary	O
facts	O
in	O
the	O
datasets	O
to	O
conduct	O
knowledge	O
inference	O
.	O
For	O
future	O
works	O
,	O
to	O
further	O
improve	O
the	O
method	O
,	O
we	O
will	O
explore	O
the	O
introduction	O
of	O
additional	O
information	O
,	O
such	O
as	O
rules	O
and	O
external	O
texts	O
.	O

For	O
the	O
dependency	O
analysis	O
,	O
we	O
use	O
an	O
extended	O
version	O
of	O
UDPipeFuture	O
(	O
Straka	O
,	O
2018	O
)	O
which	O
showed	O
its	O
state	O
of	O
the	O
art	O
performance	O
by	O
becoming	O
first	O
in	O
terms	O
of	O
the	O
Morphology	O
-	O
aware	O
Labeled	O
Attachment	O
Score	B-MetricName
(	O
MLAS	O
)	O
3	O
metric	O
at	O
the	O
CoNLL	O
Shared	O
Task	O
of	O
dependency	B-TaskName
parsing	I-TaskName
in	O
2018	O
(	O
Zeman	O
et	O
al	O
,	O
2018	O
)	O
.	O
UDPipeFuture	O
is	O
a	O
POS	O
tagger	O
and	O
graph	O
parser	O
based	O
dependency	O
parser	O
using	O
a	O
BiLSTM	B-MethodName
,	O
inspired	O
by	O
Dozat	O
et	O
al	O
(	O
2017	O
)	O
.	O
Our	O
modification	O
consisted	O
in	O
adding	O
several	O
contextual	O
word	B-TaskName
embeddings	I-TaskName
(	O
with	O
respect	O
to	O
the	O
language	O
)	O
.	O
In	O
order	O
to	O
find	O
the	O
best	O
configuration	O
we	O
experimented	O
with	O
models	O
like	O
multilingual	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
XLM	B-MethodName
-	O
R	O
(	O
Conneau	O
et	O
al	O
,	O
2019	O
)	O
(	O
for	O
both	O
,	O
English	O
and	O
French	O
)	O
,	O
RoBERTA	O
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
(	O
for	O
English	O
)	O
,	O
FlauBERT	O
(	O
Le	O
et	O
al	O
,	O
2020	O
)	O
and	O
CamemBERT	O
(	O
Martin	O
et	O
al	O
,	O
2019	O
)	O
(	O
for	O
French	O
)	O
during	O
the	O
training	O
of	O
the	O
treebanks	O
French	O
-	O
GSD	O
and	O
English	O
-	O
EWT	O
4	O
,	O
of	O
the	O
Universal	B-DatasetName
Dependencies	I-DatasetName
project	O
(	O
UD	B-DatasetName
)	O
(	O
Nivre	O
et	O
al	O
,	O
2016	O
)	O
5	O
.	O
Adding	O
contextual	O
word	O
embedding	O
increases	O
significantly	O
the	O
results	O
for	O
all	O
metrics	O
,	O
LAS	O
,	O
CLAS	O
and	O
MLAS	O
(	O
cf	O
.	O
table	O
1	O
)	O
.	O
This	O
is	O
the	O
case	O
for	O
all	O
languages	O
(	O
of	O
the	O
CoNLL	O
shared	O
task	O
)	O
,	O
where	O
language	O
specific	O
contextual	O
embeddings	O
or	O
multingual	O
ones	O
(	O
as	O
BERT	B-MethodName
or	O
XLM	B-MethodName
-	O
R	O
)	O
improved	O
parsing	O
(	O
Heinecke	O
,	O
2020	O
)	O
French	O
(	O
Fr	O
-	O
GSD	O
)	O
embeddings	O
MLAS	O
CLAS	O
LAS	O
Straka	O
(	O
2018	O
In	O
order	O
to	O
parse	O
simple	O
,	O
quiz	O
-	O
like	O
questions	O
,	O
the	O
training	O
corpora	O
of	O
the	O
two	O
UD	B-DatasetName
treebanks	O
are	O
not	O
appropriate	O
(	O
enough	O
)	O
,	O
since	O
both	O
treebanks	O
do	O
not	O
contain	O
many	O
questions	O
,	O
if	O
at	O
all	O
6	O
.	O
An	O
explanation	O
for	O
bad	O
performance	O
on	O
questions	O
of	O
parser	O
models	O
trained	O
on	O
standard	O
UD	B-DatasetName
is	O
the	O
fact	O
,	O
that	O
in	O
both	O
languages	O
,	O
the	O
syntax	O
of	O
questions	O
differs	O
from	O
the	O
syntax	O
of	O
declarative	O
sentences	O
:	O
apart	O
from	O
wh	O
question	O
words	O
,	O
in	O
English	O
the	O
to	O
do	O
periphrasis	O
is	O
nearly	O
always	O
used	O
in	O
questions	O
.	O
In	O
French	O
,	O
subject	O
and	O
direct	O
objects	O
can	O
be	O
inversed	O
and	O
the	O
est	O
-	O
ce	O
que	O
construction	O
appears	O
frequently	O
.	O
Both	O
,	O
the	O
English	O
to	O
do	O
periphrasis	O
and	O
the	O
French	O
est	O
-	O
ce	O
que	O
construction	O
are	O
absent	O
in	O
declarative	O
sentences	O
.	O
Table	O
2	O
shows	O
the	O
(	O
much	O
lower	O
)	O
results	O
when	O
parsing	O
questions	O
using	O
models	O
trained	O
only	O
on	O
the	O
standard	O
UD	B-DatasetName
treebanks	O
.	O
In	O
order	O
to	O
get	O
a	O
better	O
analysis	O
,	O
we	O
decided	O
to	O
and	O
add	O
this	O
data	O
to	O
the	O
basic	O
treebanks	O
.	O
For	O
English	O
we	O
annotated	O
309	O
questions	O
(	O
plus	O
91	O
questions	O
for	O
validation	O
)	O
from	O
the	O
QALD7	O
(	O
Usbeck	O
et	O
al	O
,	O
2017	O
)	O
and	O
QALD8	O
corpora	O
7	O
.	O
For	O
French	O
we	O
translated	O
the	O
QALD7	O
questions	O
into	O
French	O
and	O
formulated	O
others	O
ourselves	O
(	O
276	O
train	O
,	O
66	O
validation	O
)	O
.	O
For	O
the	O
annotations	O
we	O
followed	O
the	O
general	O
UD	B-DatasetName
guidelines	O
8	O
as	O
well	O
as	O
the	O
treebank	O
specific	O
guidelines	O
of	O
En	O
-	O
EWT	O
and	O
Fr	O
-	O
GSD	O
.	O
As	O
table	O
3	O
shows	O
,	O
the	O
quality	O
of	O
the	O
dependency	O
analysis	O
improves	O
considerably	O
.	O
The	O
contextual	O
word	B-TaskName
embeddings	I-TaskName
CamemBERT	O
(	O
for	O
French	O
)	O
and	O
BERT	B-MethodName
(	O
English	O
)	O
have	O
the	O
biggest	O
impact	O
.	O
We	O
rely	O
on	O
the	O
UdpipeFuture	O
version	O
which	O
we	O
have	O
improved	O
with	O
BERT	B-MethodName
(	O
for	O
English	O
)	O
/CamemBERT	O
(	O
for	O
French	O
)	O
and	O
which	O
gives	O
the	O
best	O
results	O
in	O
terms	O
of	O
dependency	O
analysis	O
,	O
in	O
order	O
to	O
proceed	O
with	O
the	O
partitioning	O
of	O
the	O
question	O
into	O
textual	O
fragments	O
(	O
also	O
called	O
chunks	O
)	O
:	O
Q	O
=	O
{	O
c	O
1	O
,	O
c	O
2	O
,	O
.	O
.	O
.	O
,	O
c	O
n	O
}	O
.	O
If	O
we	O
take	O
the	O
example	O
of	O
the	O
question	O
What	O
is	O
the	O
political	O
party	O
of	O
the	O
mayor	O
of	O
Paris	O
?	O
,	O
the	O
set	O
of	O
textual	O
fragments	O
would	O
be	O
Q	O
=	O
{	O
What	O
,	O
is	O
,	O
the	O
political	O
party	O
of	O
the	O
mayor	O
of	O
Paris	O
}	O
.	O

The	O
existing	O
QAS	O
test	O
sets	O
are	O
more	O
tailored	O
to	O
systems	O
which	O
generate	O
the	O
exact	O
short	O
answer	O
to	O
a	O
question	O
or	O
more	O
focused	O
on	O
the	O
Machine	B-TaskName
Reading	I-TaskName
Comprehension	I-TaskName
task	O
where	O
the	O
answer	O
consists	O
of	O
a	O
text	O
passage	O
from	O
a	O
document	O
containing	O
the	O
short	O
answer	O
.	O
Therefore	O
,	O
we	O
have	O
created	O
a	O
dataset	O
which	O
maps	O
questions	O
extracted	O
from	O
the	O
QALD	O
-	O
7	O
challenge	O
dataset	O
(	O
Usbeck	O
et	O
al	O
,	O
2017	O
)	O
with	O
natural	O
language	O
answers	O
which	O
were	O
defined	O
by	O
a	O
linguist	O
and	O
which	O
we	O
individually	O
reviewed	O
.	O
This	O
dataset	O
called	O
QUEREO	O
consists	O
of	O
150	O
questions	O
with	O
the	O
short	O
answers	O
extracted	O
by	O
the	O
QAS	O
that	O
we	O
described	O
above	O
.	O
We	O
denote	O
an	O
average	O
of	O
three	O
possible	O
gold	O
sanswers	O
in	O
natural	O
language	O
for	O
each	O
question	O
.	O
French	O
and	O
English	O
versions	O
were	O
created	O
for	O
this	O
dataset	O
.	O
As	O
illustrated	O
in	O
figure	O
1	O
,	O
two	O
possible	O
architectures	O
of	O
the	O
approach	O
proposed	O
for	O
answer	B-TaskName
generation	I-TaskName
have	O
been	O
evaluated	O
.	O
The	O
first	O
architecture	O
A1	O
consists	O
in	O
generating	O
all	O
possible	O
answer	O
structures	O
in	O
order	O
to	O
have	O
them	O
evaluated	O
afterwards	O
by	O
a	O
LM	O
which	O
will	O
identify	O
the	O
optimal	O
answer	O
structure	O
to	O
which	O
we	O
generate	O
possible	O
missing	O
words	O
.	O
Architecture	O
A2	O
starts	O
with	O
generating	O
missing	O
words	O
for	O
each	O
structure	O
in	O
S	O
which	O
will	O
then	O
be	O
evaluated	O
by	O
the	O
LM	O
.	O
In	O
this	O
paper	O
,	O
we	O
assume	O
that	O
there	O
is	O
only	O
one	O
missing	O
word	O
per	O
structure	O
.	O
To	O
evaluate	O
the	O
proposed	O
approach	O
,	O
we	O
have	O
referred	O
to	O
standard	O
metrics	O
defined	O
for	O
NLG	O
tasks	O
such	O
as	O
Automatic	O
Translation	B-TaskName
and	O
Summarization	B-TaskName
,	O
as	O
they	O
allow	O
to	O
assess	O
to	O
what	O
extent	O
a	O
generated	O
sentence	O
is	O
similar	O
to	O
the	O
gold	O
sentence	O
.	O
We	O
con	O
-	O
sider	O
three	O
N	O
-	O
gram	O
metrics	O
(	O
BLEU	B-MetricName
,	O
METEOR	B-DatasetName
and	O
ROUGE	O
)	O
and	O
the	O
BERT	B-MethodName
score	O
metric	O
which	O
exploits	O
the	O
pre	O
-	O
trained	O
embeddings	O
of	O
BERT	B-MethodName
to	O
calculate	O
the	O
similarity	O
between	O
the	O
answer	O
generated	O
and	O
the	O
gold	O
answer	O
.	O
To	O
be	O
able	O
to	O
compare	O
the	O
different	O
configurations	O
of	O
the	O
approach	O
,	O
we	O
refer	O
to	O
Friedman	O
's	O
test	O
(	O
Milton	O
,	O
1939	O
)	O
We	O
also	O
conducted	O
a	O
human	O
evaluation	O
study	O
for	O
the	O
French	O
and	O
the	O
English	O
versions	O
of	O
the	O
dataset	O
,	O
in	O
which	O
we	O
asked	O
20	O
native	O
speakers	O
participants	O
to	O
evaluate	O
the	O
relevance	O
of	O
a	O
generated	O
answer	O
(	O
correct	O
or	O
not	O
correct	O
)	O
regarding	O
a	O
given	O
question	O
while	O
indicating	O
the	O
type	O
of	O
errors	O
depicted	O
(	O
grammar	O
,	O
wrong	O
preposition	O
,	O
word	O
order	O
,	O
extra	O
word	O
(	O
s	O
)	O
,	O
etc	O
)	O
.	O
Figure	O
3	O
presents	O
the	O
evaluation	O
framework	O
that	O
we	O
have	O
implemented	O
and	O
provided	O
to	O
the	O
participants	O
.	O
The	O
results	O
of	O
each	O
participant	O
are	O
saved	O
in	O
a	O
json	O
-	O
file	O
(	O
figure	O
4	O
)	O
.	O
The	O
inter	O
-	O
agreement	O
rate	O
between	O
participants	O
reached	O
70	O
%	O
which	O
indicates	O
a	O
substantial	O
agreement	O
.	O
Through	O
the	O
human	O
evaluation	O
study	O
,	O
we	O
wanted	O
to	O
explore	O
to	O
what	O
extent	O
the	O
standard	O
metrics	O
are	O
reliable	O
to	O
assess	O
NLG	O
approaches	O
within	O
the	O
context	O
of	O
question	O
-	O
answering	O
systems	O
.	O
Table	O
4	O
(	O
French	O
dataset	O
)	O
represents	O
the	O
obtained	O
results	O
for	O
the	O
first	O
three	O
best	O
models	O
according	O
to	O
the	O
human	O
evaluation	O
ranking	O
and	O
the	O
Friedman	O
test	O
ranking	O
.	O
We	O
indicate	O
between	O
brackets	O
each	O
model	O
's	O
rank	O
according	O
to	O
the	O
metric	O
used	O
.	O
We	O
note	O
that	O
the	O
highest	O
human	O
accuracy	B-MetricName
score	O
for	O
French	O
of	O
about	O
85	O
%	O
was	O
scored	O
with	O
the	O
first	O
architecture	O
coupled	O
with	O
BERT	B-MethodName
as	O
the	O
generation	O
model	O
(	O
GM	O
)	O
and	O
CamemBERT	O
as	O
the	O
language	O
model	O
(	O
LM	O
)	O
.	O
We	O
also	O
notice	O
that	O
the	O
architecture	O
A1	O
,	O
which	O
considers	O
the	O
LM	O
assessment	O
of	O
the	O
structure	O
before	O
generating	O
missing	O
words	O
,	O
performs	O
better	O
.	O
Surprisingly	O
,	O
as	O
a	O
generative	O
model	O
,	O
the	O
multi	O
-	O
Table	O
4	O
:	O
Model	O
ranking	O
for	O
French	O
dataset	O
according	O
to	O
the	O
human	O
evaluation	O
study	O
(	O
best	O
in	O
bold	O
)	O
and	O
the	O
Friedman	O
test	O
(	O
best	O
in	O
yellow	O
)	O
.	O
"	O
BT	O
"	O
in	O
Column	O
GM	O
stands	O
for	O
BERT	B-MethodName
-	O
base	O
-	O
multilingual	O
-	O
cased	O
.	O
In	O
column	O
LM	O
we	O
use	O
"	O
CmBT	O
"	O
for	O
CamemBERT	O
-	O
base	O
,	O
"	O
BT	O
-	O
ml	O
-	O
c	O
"	O
for	O
BERT	B-MethodName
-	O
base	O
-	O
multilingual	O
-	O
cased	O
,	O
"	O
XRob	O
"	O
for	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
-	O
base	O
,	O
"	O
FBT	O
-	O
s	O
-	O
c	O
"	O
for	O
FlauBERT	O
-	O
small	O
-	O
cased	O
,	O
"	O
FBT	O
-	O
b	O
-	O
uc	O
"	O
for	O
FlauBERT	O
-	O
base	O
-	O
uncased	O
and	O
"	O
clm	O
-	O
1024	O
"	O
for	O
XLM	B-MethodName
-	O
clmenfr	O
-	O
1024	O
lingual	O
BERT	B-MethodName
model	O
predicts	O
missing	O
words	O
better	O
than	O
CamemBERT	O
for	O
French	O
sentences	O
.	O
These	O
findings	O
are	O
also	O
confirmed	O
by	O
the	O
Friedman	O
test	O
where	O
we	O
can	O
clearly	O
see	O
that	O
the	O
first	O
ranked	O
configuration	O
maps	O
the	O
best	O
configuration	O
selected	O
according	O
to	O
the	O
human	O
accuracy	B-MetricName
,	O
with	O
a	O
very	O
slight	O
difference	O
for	O
the	O
other	O
four	O
configurations	O
.	O
Let	O
us	O
see	O
if	O
that	O
means	O
that	O
the	O
four	O
metrics	O
are	O
correlated	O
with	O
the	O
human	O
accuracy	B-MetricName
.	O
According	O
to	O
table	O
6	O
which	O
presents	O
the	O
Pearson	B-MetricName
correlation	I-MetricName
(	O
Benesty	O
et	O
al	O
,	O
2009	O
)	O
of	O
the	O
human	O
accuracy	B-MetricName
with	O
the	O
four	O
metrics	O
and	O
to	O
figure	O
2	O
which	O
illustrates	O
the	O
ranking	O
given	O
by	O
each	O
evaluation	O
metric	O
along	O
with	O
the	O
human	O
judgement	O
for	O
each	O
configuration	O
(	O
i.e.	O
configuration	O
=	O
GM	O
×	O
architecture	O
×	O
LM	O
)	O
tested	O
,	O
we	O
can	O
clearly	O
see	O
that	O
the	O
human	O
evaluation	O
results	O
are	O
positively	O
and	O
strongly	O
correlated	O
with	O
the	O
BLEU	B-MetricName
,	O
the	O
METEOR	B-DatasetName
and	O
the	O
BERT	B-MethodName
scores	O
.	O
These	O
metrics	O
are	O
practically	O
matching	O
the	O
human	O
ranking	O
and	O
thus	O
are	O
obviously	O
able	O
to	O
identify	O
which	O
configuration	O
gives	O
better	O
results	O
.	O
The	O
rouge	O
metric	O
,	O
used	O
for	O
French	O
question	O
/	O
answer	O
evaluation	O
,	O
is	O
moderately	O
correlated	O
with	O
the	O
human	O
evaluation	O
which	O
means	O
that	O
we	O
should	O
not	O
only	O
rely	O
on	O
this	O
metric	O
when	O
assessing	O
such	O
task	O
.	O
On	O
the	O
other	O
hand	O
,	O
when	O
the	O
ROUGE	O
metric	O
is	O
considered	O
with	O
the	O
other	O
metrics	O
,	O
it	O
helps	O
to	O
get	O
closer	O
to	O
the	O
human	O
judgement	O
.	O
Table	O
5	O
presents	O
the	O
results	O
for	O
the	O
English	O
dataset	O
and	O
shows	O
that	O
the	O
best	O
accuracy	B-MetricName
scored	O
is	O
about	O
72	O
%	O
with	O
A1	O
,	O
BERT	B-MethodName
as	O
the	O
generative	O
model	O
and	O
the	O
Generative	O
Pretrained	O
Transformer	B-MethodName
(	O
GPT	B-MethodName
)	O
as	O
the	O
language	O
model	O
.	O
According	O
to	O
the	O
first	O
three	O
configurations	O
,	O
architecture	O
A2	O
prevails	O
and	O
the	O
GPT	B-MethodName
transformer	O
takes	O
over	O
the	O
other	O
lan	O
-	O
guage	O
models	O
.	O
These	O
results	O
are	O
also	O
confirmed	O
by	O
the	O
Friedman	O
test	O
with	O
a	O
very	O
slight	O
difference	O
on	O
the	O
ranking	O
and	O
also	O
upheld	O
with	O
the	O
correlation	O
scores	O
between	O
the	O
human	O
assessment	O
and	O
each	O
of	O
the	O
four	O
metrics	O
as	O
shown	O
by	O
figure	O
5	O
and	O
table	O
6	O
.	O
These	O
findings	O
mean	O
that	O
we	O
actually	O
can	O
rely	O
on	O
the	O
use	O
of	O
these	O
standard	O
metrics	O
to	O
evaluate	O
the	O
answer	B-TaskName
generation	I-TaskName
task	O
for	O
question	O
-	O
answering	O
.	O
We	O
also	O
tried	O
to	O
analyse	O
the	O
errors	O
indicated	O
by	O
the	O
participants	O
.	O
As	O
we	O
can	O
note	O
from	O
figure	O
6	O
,	O
the	O
most	O
common	O
error	O
reported	O
for	O
both	O
English	O
and	O
French	O
datasets	O
is	O
the	O
word	O
order	O
which	O
sheds	O
the	O
light	O
on	O
a	O
problem	O
related	O
to	O
the	O
language	O
model	O
assessment	O
phase	O
.	O
The	O
second	O
most	O
reported	O
error	O
addresses	O
the	O
generation	O
process	O
,	O
whether	O
to	O
indicate	O
that	O
there	O
are	O
one	O
or	O
more	O
missing	O
words	O
within	O
the	O
answer	O
(	O
French	O
)	O
or	O
the	O
presence	O
of	O
some	O
odd	O
words	O
(	O
English	O
)	O
.	O
When	O
trying	O
to	O
get	O
an	O
insight	O
on	O
the	O
answers	O
generated	O
by	O
the	O
current	O
intelligent	O
systems	O
such	O
as	O
Google	B-DatasetName
assistant	O
and	O
Alexa	O
,	O
we	O
noted	O
that	O
these	O
systems	O
are	O
very	O
accurate	O
when	O
extracting	O
the	O
correct	O
answer	O
to	O
a	O
question	O
and	O
can	O
sometimes	O
generate	O
user	O
-	O
friendly	O
answers	O
that	O
help	O
recall	O
the	O
question	O
context	O
,	O
specially	O
with	O
Alexa	O
.	O
However	O
,	O
we	O
noticed	O
that	O
most	O
of	O
the	O
answers	O
generated	O
by	O
these	O
systems	O
are	O
more	O
verbose	O
than	O
necessary	O
,	O
we	O
also	O
found	O
out	O
that	O
when	O
addressing	O
yes	O
/	O
no	O
questions	O
,	O
these	O
systems	O
generally	O
settle	O
for	O
just	O
a	O
yes	O
or	O
no	O
without	O
elaborating	O
,	O
or	O
,	O
on	O
the	O
other	O
hand	O
,	O
present	O
a	O
text	O
span	O
extracted	O
from	O
a	O
Web	O
page	O
and	O
let	O
the	O
user	O
guess	O
the	O
answer	O
.	O
Let	O
us	O
take	O
for	O
example	O
the	O
following	O
question	O
Was	O
US	O
president	O
Jackson	O
involved	O
in	O
a	O
war	O
?	O
Table	O
5	O
:	O
Model	O
ranking	O
for	O
English	O
dataset	O
according	O
to	O
the	O
human	O
evaluation	O
study	O
(	O
best	O
in	O
bold	O
)	O
and	O
the	O
Friedman	O
ranking	O
(	O
best	O
in	O
yellow	O
)	O
.	O
In	O
Column	O
GM	O
we	O
use	O
"	O
BT	O
-	O
ml	O
"	O
for	O
BERT	B-MethodName
-	O
base	O
-	O
multilingual	O
-	O
cased	O
and	O
"	O
BT	O
"	O
for	O
BERTlarge	O
-	O
cased	O
.	O
In	O
column	O
LM	O
"	O
GPT	B-MethodName
"	O
stands	O
for	O
for	O
OpenAI	O
-	O
GPT	B-MethodName
,	O
"	O
GPT2	O
-	O
l	O
"	O
for	O
GPT2	O
-	O
large	O
,	O
"	O
GPT2	O
-	O
m	O
"	O
for	O
GPT2medium	O
,	O
"	O
GPT2	O
"	O
for	O
GPT2	O
,	O
"	O
BT	O
-	O
b	O
-	O
uc	O
"	O
for	O
BERT	B-MethodName
-	O
base	O
-	O
uncased	O
,	O
"	O
mlm	B-DatasetName
-	O
2048	B-DatasetName
"	O
for	O
XLM	B-MethodName
-	O
mlm	B-DatasetName
-	O
en	O
-	O
2048	B-DatasetName
and	O
"	O
BT	O
-	O
l	O
-	O
c	O
"	O
for	O
BERT	B-MethodName
-	O
large	O
-	O
cased	O
.	O
[	O
{	O
"	O
ID	O
"	O
:	O
"	O
quereo_5.4	O
"	O
,	O
"	O
QUESTION	O
"	O
:	O
"	O
Quelles	O
sont	O
les	O
companies	O
d'électronique	O
fondées	O
a	O
Beijing	O
?	O
"	O
,	O
"	O
SHORT_ANSWER	O
"	O
:	O
[	O
"	O
Xiaomi	O
"	O
,	O
"	O
Lenovo	O
"	O
]	O
,	O
"	O
GENERATED_ANSWER	O
"	O
:	O
"	O
Les	O
companies	O
d'électronique	O
fondéesà	O
beijing	O
sont	O
xiao	O
xiaomi	O
et	O
lenovo	O
"	O
,	O
"	O
MISSING_WORD	O
"	O
:	O
"	O
Xiao	O
"	O
,	O
"	O
EVALUATION	O
"	O
:	O
"	O
correcte	O
"	O
,	O
"	O
ERROR	O
"	O
:	O
[	O
"	O
aucun	O
"	O
]	O
,	O
"	O
COMMENT	O
"	O
:	O
"	O
"	O
}	O
,	O
{	O
"	O
ID	O
"	O
:	O
"	O
quereo_8.8	O
"	O
,	O
"	O
QUESTION	O
"	O
:	O
"	O
Combien	O
de	O
films	O
a	O
réalisé	O
Park	O
Chan	O
-	O
wook	O
?	O
"	O
,	O
"	O
SHORT_ANSWER	O
"	O
:	O
[	O
"	O
quatorze	O
"	O
]	O
,	O
"	O
GENERATED_ANSWER	O
"	O
:	O
"	O
Quatorze	O
films	O
a	O
réalisé	O
park	O
chan	O
-	O
wook	O
"	O
,	O
"	O
MISSING_WORD	O
"	O
:	O
"	O
.	O
"	O
,	O
"	O
EVALUATION	O
"	O
:	O
"	O
incorrecte	O
"	O
,	O
"	O
ERROR	O
"	O
:	O
[	O
"	O
ordre	O
"	O
,	O
"	O
accord	O
"	O
]	O
,	O
"	O
COMMENT	O
"	O
:	O
"	O
"	O
}	O
,	O
...	O
]	O
Here	O
's	O
something	O
I	O
found	O
on	O
the	O
Web	O
.	O
According	O
to	O
constitutioncenter.org	O
:	O
After	O
the	O
War	O
of	O
1812	O
,	O
Jackson	O
led	O
military	O
forces	O
against	O
the	O
Indians	O
and	O
was	O
involved	O
in	O
treaties	O
that	O
led	O
to	O
the	O
relocation	O
of	O
Indians	O
.	O
The	O
user	O
has	O
to	O
focus	O
on	O
the	O
returned	O
text	O
fragment	O
in	O
order	O
to	O
guess	O
that	O
the	O
answer	O
to	O
his	O
question	O
is	O
actually	O
yes	O
.	O
This	O
issue	O
was	O
particularly	O
noted	O
when	O
addressing	O
French	O
questions	O
.	O
If	O
we	O
also	O
take	O
the	O
example	O
How	O
many	O
grandchildren	O
did	O
Jacques	O
Cousteau	O
have	O
?	O
the	O
two	O
systems	O
answer	O
as	O
follows	O
:	O
Fabien	O
Cousteau	O
,	O
Alexandra	O
Cousteau	O
,	O
Philippe	O
Cousteau	O
Jr.	O
,	O
Céline	O
Cousteau	O
.	O
Jacques	O
Cousteau	O
's	O
grandchildren	O
were	O
Philippe	O
Cousteau	O
Jr.	O
,	O
Alexandra	O
ousteau	O
,	O
Céline	O
Cousteau	O
,	O
and	O
Fabien	O
Cousteau	O
However	O
,	O
the	O
user	O
is	O
not	O
asking	O
about	O
the	O
names	O
of	O
Cousteau	O
's	O
grand	O
-	O
children	O
and	O
has	O
to	O
guess	O
by	O
himself	O
that	O
the	O
answer	O
for	O
this	O
question	O
is	O
four	O
.	O
A	O
more	O
accurate	O
answer	O
should	O
indicate	O
the	O
exact	O
answer	O
to	O
the	O
question	O
and	O
then	O
elaborate	O
Jacques	O
Cousteau	O
had	O
four	O
grand	O
-	O
children	O
.	O
But	O
these	O
systems	O
perform	O
better	O
in	O
case	O
when	O
the	O
terms	O
employed	O
in	O
the	O
question	O
are	O
not	O
necessarily	O
relevant	O
to	O
the	O
answer	O
.	O
If	O
we	O
take	O
the	O
example	O
of	O
the	O
question	O
who	O
is	O
the	O
wife	O
of	O
Lance	O
Bass	O
,	O
the	O
approach	O
that	O
we	O
propose	O
will	O
generate	O
The	O
wife	O
of	O
Lance	O
Bass	O
is	O
Michael	O
Turchin	O
.	O
As	O
we	O
can	O
note	O
the	O
answer	O
generated	O
was	O
not	O
adapted	O
to	O
the	O
actual	O
answer	O
,	O
while	O
the	O
other	O
systems	O
are	O
able	O
to	O
detect	O
such	O
nuance	O
:	O
Lance	O
Bass	O
is	O
married	O
to	O
Michael	O
Turchin	O
.	O
They	O
have	O
been	O
married	O
since	O
2014	O
.	O
This	O
issue	O
has	O
still	O
to	O
be	O
addressed	O
.	O

We	O
study	O
model	O
-	O
generated	O
answers	O
from	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
LFQA	O
system	O
(	O
Krishna	O
et	O
al	O
,	O
2021	O
)	O
.	O
9	O
Their	O
model	O
uses	O
passage	O
retriever	O
(	O
Guu	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
generates	O
answers	O
based	O
on	O
the	O
retrieved	O
passage	O
with	O
a	O
routing	B-MethodName
transformer	I-MethodName
model	O
.	O
Q	O
/	O
A	O
Validity	O
We	O
collect	O
validity	O
annotation	O
on	O
193	O
model	O
-	O
generated	O
answers	O
,	O
and	O
78	O
are	O
considered	O
invalid	O
,	O
significantly	O
higher	O
ratio	O
than	O
that	O
of	O
human	O
written	O
answers	O
(	O
Table	O
2	O
)	O
.	O
The	O
Fleiss	O
's	O
kappa	O
of	O
QA	O
pair	O
validity	O
is	O
0.26	O
(	O
and	O
0.61	O
after	O
collecting	O
one	O
more	O
label	O
)	O
,	O
substantially	O
lower	O
than	O
the	O
agreement	O
on	O
human	O
written	O
answers	O
(	O
0.51	O
,	O
0.70	O
)	O
while	O
annotated	O
by	O
the	O
same	O
set	O
of	O
annotators	O
.	O
Detailed	O
distribution	O
of	O
invalid	O
reason	O
annotated	O
can	O
be	O
found	O
in	O
Table	O
9	O
.	O
Despite	O
the	O
low	O
agreement	O
,	O
60	O
of	O
them	O
are	O
marked	O
as	O
"	O
No	O
valid	O
answer	O
"	O
by	O
at	O
least	O
two	O
annotators	O
.	O
The	O
flaw	O
of	O
automatic	O
measures	O
was	O
also	O
pointed	O
out	O
by	O
prior	O
work	O
(	O
Krishna	O
et	O
al	O
,	O
2021	O
)	O
,	O
which	O
compares	O
ROUGE	O
between	O
humanwritten	O
and	O
model	O
-	O
generated	O
answers	O
.	O
Our	O
study	O
reiterates	O
that	O
the	O
generated	O
answers	O
received	O
high	O
ROUGE	O
score	O
without	O
answering	O
the	O
question	O
.	O
Roles	O
We	O
proceed	O
to	O
collect	O
sentence	O
-	O
level	O
role	O
annotations	O
on	O
115	O
valid	O
generated	O
long	O
-	O
form	O
answers	O
following	O
the	O
same	O
annotation	O
setup	O
in	O
Section	O
3	O
,	O
and	O
hence	O
our	O
annotators	O
were	O
not	O
asked	O
to	O
evaluate	O
the	O
correctness	O
or	O
the	O
quality	O
of	O
the	O
answers	O
(	O
e.g.	O
whether	O
the	O
generated	O
example	O
makes	O
sense	O
)	O
,	O
focusing	O
on	O
the	O
functional	O
roles	O
of	O
sentences	O
only	O
.	O
We	O
found	O
that	O
the	O
annotators	O
disagree	O
substantially	O
more	O
as	O
compared	O
to	O
the	O
humanwritten	O
answers	O
,	O
with	O
a	O
Fleiss	O
kappa	O
of	O
0.31	O
(	O
vs.	O
0.45	O
for	O
human	O
-	O
written	O
answers	O
)	O
,	O
suggesting	O
that	O
the	O
discourse	O
structure	O
of	O
model	O
-	O
generated	O
answers	O
are	O
less	O
clear	O
,	O
even	O
to	O
our	O
trained	O
annotators	O
.	O
The	O
answer	O
role	O
distribution	O
of	O
model	O
-	O
generated	O
answers	O
is	O
very	O
different	O
from	O
that	O
of	O
the	O
human	O
written	O
answers	O
(	O
Figure	O
4	O
)	O
.	O
The	O
generated	O
answers	O
contain	O
more	O
sentences	O
which	O
provide	O
auxiliary	O
information	O
,	O
and	O
fewer	O
summary	O
sentences	O
.	O
This	O
suggests	O
that	O
model	O
-	O
generated	O
answers	O
contain	O
a	O
higher	O
portion	O
of	O
information	O
tangentially	O
related	O
to	O
what	O
is	O
asked	O
in	O
the	O
question	O
.	O
Model	O
-	O
generated	O
answers	O
also	O
contain	O
fewer	O
example	O
and	O
miscella	O
-	O
9	O
We	O
sampled	O
from	O
four	O
different	O
model	O
configurations	O
reported	O
in	O
their	O
paper	O
,	O
i.e.	O
combination	O
of	O
nucleus	O
sampling	O
threshold	O
p=	O
{	O
0.6	O
,	O
0.9	O
}	O
,	O
and	O
generation	O
conditioning	O
on	O
{	O
predicted	O
,	O
random	O
}	O
passages	O
.	O
The	O
answers	O
we	O
annotated	O
achieved	O
a	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
of	O
23.19	O
,	O
higher	O
than	O
that	O
of	O
human	O
-	O
written	O
answers	O
on	O
the	O
same	O
set	O
of	O
questions	O
(	O
21.28	O
)	O
.	O
neous	O
sentences	O
.	O
Examples	O
of	O
annotated	O
modelgenerated	O
answer	O
can	O
be	O
found	O
in	O
Table	O
10	O
.	O
Overall	O
,	O
our	O
results	O
suggest	O
that	O
machinegenerated	O
long	O
form	O
answers	O
are	O
different	O
from	O
human	O
-	O
written	O
answers	O
,	O
and	O
judging	O
their	O
discourse	O
structure	O
is	O
nontrivial	O
for	O
human	O
annotators	O
,	O
resulting	O
in	O
lower	O
agreement	O
.	O
Recent	O
study	O
(	O
Karpinska	O
et	O
al	O
,	O
2021	O
)	O
also	O
showed	O
that	O
expert	O
annotators	O
showed	O
lower	O
agreement	O
and	O
took	O
longer	O
time	O
to	O
evaluate	O
the	O
coherence	O
of	O
story	O
generated	O
from	O
large	O
-	O
scale	O
language	O
model	O
.	O

Task	O
and	O
Data	O
Given	O
a	O
question	O
q	O
and	O
its	O
longform	O
answer	O
consisting	O
of	O
sentences	O
s	O
1	O
,	O
s	O
2	O
...	O
s	O
n	O
,	O
the	O
goal	O
is	O
to	O
assign	O
each	O
answer	O
sentence	O
s	O
i	O
one	O
of	O
the	O
six	O
roles	O
defined	O
in	O
Section	O
2	O
.	O
As	O
we	O
have	O
annotated	O
more	O
examples	O
from	O
ELI5	B-DatasetName
dataset	O
(	O
411	O
answer	O
paragraphs	O
compared	O
to	O
around	O
100	O
paragraphs	O
in	O
other	O
three	O
datasets	O
(	O
WebGPT	O
,	O
NQ	B-DatasetName
and	O
ELI5	B-DatasetName
-	O
model	O
)	O
)	O
,	O
we	O
randomly	O
split	O
the	O
ELI5	B-DatasetName
longform	O
answers	O
into	O
train	O
,	O
validation	O
and	O
test	O
sets	O
with	O
a	O
70%/15%/15	O
%	O
ratio	O
,	O
and	O
train	O
the	O
model	O
on	O
the	O
training	O
portion	O
.	O
We	O
use	O
all	O
other	O
annotated	O
datasets	O
for	O
evaluation	O
only	O
.	O
For	O
model	O
-	O
generated	O
answers	O
,	O
we	O
filtered	O
185	O
out	O
of	O
1080	O
sentences	O
where	O
model	O
-	O
generated	O
sentences	O
do	O
not	O
have	O
a	O
majority	O
role	O
.	O
This	O
setup	O
also	O
allows	O
us	O
to	O
study	O
domain	O
transfer	O
of	O
role	O
classification	O
model	O
.	O
Metrics	O
We	O
report	O
accuracy	B-MetricName
with	O
respect	O
to	O
the	O
majority	O
role	O
label	O
(	O
or	O
adjudicated	O
one	O
,	O
if	O
majority	O
does	O
n't	O
exist	O
)	O
(	O
Acc	B-MetricName
)	O
,	O
match	O
on	O
any	O
label	O
from	O
three	O
annotators	O
(	O
Match	O
)	O
,	O
F1	B-MetricName
score	I-MetricName
for	O
each	O
role	O
and	O
their	O
macro	O
average	O
score	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
.	O

Lower	O
bounds	O
We	O
present	O
two	O
simple	O
baselines	O
to	O
provide	O
lower	O
bounds	O
:	O
(	O
1	O
)	O
Majority	O
:	O
We	O
predict	O
the	O
most	O
frequent	O
labels	O
in	O
the	O
training	O
data	O
:	O
Answer	O
-	O
Summary	O
.	O
(	O
2	O
)	O
Summary	O
-	O
lead	O
:	O
We	O
predict	O
first	O
two	O
sentences	O
as	O
Answer	O
-	O
Summary	O
,	O
and	O
the	O
rest	O
of	O
the	O
sentences	O
as	O
Answer	O
.	O
Classification	B-TaskName
Models	O
This	O
baseline	O
classifies	O
each	O
sentence	O
independently	O
.	O
We	O
use	O
the	O
[	O
CLS	O
]	O
token	O
from	O
RoBERTa	B-MethodName
-	O
Large	O
model	O
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
which	O
encodes	O
[	O
question	O
<	O
q	O
>	O
ans	O
1	O
...	O
<	O
start	O
>	O
ans	O
i	O
<	O
end	O
>	O
...	O
]	O
,	O
where	O
ans	O
i	O
is	O
the	O
i	O
th	O
sentence	O
in	O
the	O
answer	O
.	O
The	O
training	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
64	O
,	O
with	O
the	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
as	O
5e	O
−	O
5	O
.	O
We	O
used	O
AdamW	B-MethodName
optimizer	B-HyperparameterName
and	O
a	O
linear	O
learning	B-HyperparameterName
rate	I-HyperparameterName
schedule	O
.	O
We	O
train	O
the	O
model	O
for	O
10	O
epochs	O
and	O
report	O
the	O
result	O
of	O
the	O
checkpoint	O
with	O
best	O
validation	O
accuracy	B-MetricName
,	O
averaged	O
across	O
three	O
random	O
seeds	B-DatasetName
.	O
Seq2Seq	B-MethodName
Models	O
We	O
use	O
two	O
variations	O
(	O
base	O
,	O
large	O
)	O
of	O
T5	B-MethodName
model	O
(	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
take	O
the	O
concatenation	O
of	O
question	O
and	O
answer	O
sentences	O
,	O
and	O
output	O
the	O
roles	O
for	O
each	O
sentence	O
sequentially	O
.	O
This	O
model	O
predicts	O
the	O
roles	O
of	O
all	O
sentences	O
in	O
the	O
answer	O
as	O
a	O
single	O
sequence	O
.	O
The	O
input	O
sequence	O
is	O
[	O
question	O
[	O
1	O
]	O
ans	O
1	O
[	O
2	O
]	O
ans	O
2	O
...	O
]	O
,	O
where	O
ans	O
i	O
denotes	O
the	O
i	O
th	O
sentence	O
in	O
the	O
answer	O
,	O
and	O
the	O
target	O
output	O
sequence	O
is	O
set	O
to	O
[	O
[	O
1	O
]	O
role	O
1	O
[	O
2	O
]	O
role	O
2	O
[	O
3	O
]	O
...	O
]	O
,	O
where	O
role	O
i	O
is	O
the	O
corresponding	O
role	O
for	O
ans	O
i	O
(	O
e.g.	O
"	O
Answer	O
"	O
for	O
the	O
Answer	O
role	O
)	O
.	O
We	O
limit	O
the	O
input	O
/	O
output	O
to	O
be	O
512/128	O
tokens	O
.	O
For	O
evaluating	O
the	O
predicted	O
roles	O
,	O
we	O
parse	O
the	O
output	O
string	O
to	O
identify	O
the	O
role	O
predicted	O
for	O
each	O
sentence	O
.	O
We	O
used	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	O
,	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e−4	O
with	O
AdamW	B-MethodName
optimizer	B-HyperparameterName
and	O
a	O
linear	O
learning	B-HyperparameterName
rate	I-HyperparameterName
schedule	O
.	O
We	O
train	O
the	O
model	O
for	O
30	O
epochs	O
and	O
report	O
the	O
result	O
of	O
the	O
checkpoint	O
with	O
the	O
best	O
validation	O
accuracy	B-MetricName
,	O
averaged	O
across	O
three	O
random	O
seeds	B-DatasetName
.	O
Human	O
performance	O
We	O
provide	O
two	O
approximations	O
for	O
human	O
performance	O
:	O
upperbound	O
(	O
u	O
)	O
and	O
lowerbound	O
(	O
l	O
)	O
.	O
(	O
1	O
)	O
Human	O
(	O
u	O
)	O
:	O
We	O
compare	O
each	O
individual	O
annotator	O
's	O
annotation	O
with	O
the	O
majority	O
label	O
.	O
This	O
inflates	O
human	O
performance	O
as	O
one	O
's	O
own	O
judgement	O
affected	O
the	O
majority	O
label	O
.	O
(	O
2	O
)	O
Human	O
(	O
l	O
)	O
:	O
We	O
compare	O
all	O
pairs	O
of	O
annotation	O
and	O
calculate	O
average	B-MetricName
F1	I-MetricName
and	O
accuracy	B-MetricName
of	O
all	O
pairs	O
.	O
For	O
Match	O
,	O
we	O
compute	O
the	O
match	O
for	O
each	O
annotation	O
against	O
the	O
other	O
two	O
annotations	O
.	O

If	O
not	O
,	O
the	O
parents	O
could	O
just	O
take	O
the	O
house	O
and	O
all	O
the	O
money	O
(	O
provided	O
the	O
person	O
did	O
n't	O
have	O
a	O
will	O
Question	O
classification	O
model	O
A	O
difficulty	O
in	O
repurposing	O
NQ	B-DatasetName
is	O
that	O
not	O
all	O
questions	O
with	O
paragraph	O
answers	O
only	O
actually	O
need	O
multiple	O
sentences	O
.	O
To	O
identify	O
complex	O
questions	O
,	O
we	O
built	O
a	O
simple	O
BERT	B-MethodName
-	O
based	O
classifier	O
,	O
trained	O
to	O
distinguish	O
NQ	B-DatasetName
questions	O
with	O
short	O
answers	O
(	O
i.e.	O
,	O
less	O
than	O
five	O
tokens	O
)	O
and	O
ELI5	B-DatasetName
questions	O
.	O
We	O
use	O
the	O
[	O
CLS	O
]	O
token	O
from	O
BERT	B-MethodName
model	O
to	O
perform	O
prediction	O
.	O
We	O
use	O
the	O
original	O
split	O
from	O
the	O
ELI5	B-DatasetName
dataset	O
,	O
and	O
split	O
the	O
NQ	B-DatasetName
open	O
's	O
validation	O
set	O
into	O
val	O
and	O
test	O
set	O
.	O
We	O
preprocessed	O
the	O
questions	O
by	O
converting	O
to	O
lowercase	O
and	O
exclude	O
punctuation	O
to	O
remove	O
syntactic	O
differences	O
between	O
ELI5	B-DatasetName
and	O
NQ	B-DatasetName
questions	O
.	O
We	O
fine	O
-	O
tuned	O
the	O
bert	O
-	O
base	O
-	O
uncased	O
model	O
for	O
3	O
epochs	O
,	O
with	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e	O
−	O
5	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	O
.	O
We	O
use	O
the	O
model	O
with	O
the	O
highest	O
validation	O
F1	B-MetricName
as	O
the	O
question	O
classifier	O
,	O
which	O
achieves	O
F1	B-MetricName
of	O
0.97	O
and	O
0.94	O
on	O
validation	O
and	O
test	O
set	O
respectively	O
.	O
We	O
then	O
run	O
this	O
classifier	O
to	O
select	O
the	O
non	O
factoid	O
questions	O
from	O
NQ	B-DatasetName
questions	O
with	O
long	O
-	O
form	O
answers	O
,	O
which	O
classifies	O
around	O
10	O
%	O
,	O
out	O
of	O
the	O
27	O
,	O
752	O
NQ	B-DatasetName
long	O
questions	O
as	O
non	O
-	O
factoid	O
.	O
Examples	O
are	O
in	O
Table	O
7	O
.	O

In	O
addition	O
to	O
use	O
a	O
pre	O
-	O
defined	O
sentiment	O
lexicon	O
for	O
word	O
-	O
level	O
annotations	O
,	O
we	O
also	O
propose	O
to	O
learn	O
the	O
word	O
-	O
level	O
sentiment	O
supervision	O
,	O
based	O
on	O
PMI	O
and	O
SO	O
.	O
(	O
1	O
)	O
PMI	O
and	O
SO	O
Given	O
a	O
corpus	O
with	O
document	O
-	O
level	O
class	O
labels	O
.	O
We	O
first	O
compute	O
the	O
PMI	O
score	O
between	O
each	O
word	O
t	O
and	O
two	O
class	O
labels	O
P	O
M	O
I	O
(	O
t	O
,	O
+	O
)	O
=	O
log	O
p	O
(	O
+	O
|	O
t	O
)	O
p	O
(	O
+	O
)	O
,	O
(	O
1	O
)	O
P	O
M	O
I	O
(	O
t	O
,	O
−	O
)	O
=	O
log	O
p	O
(	O
−	O
|	O
t	O
)	O
p	O
(	O
−	O
)	O
,	O
(	O
2	O
)	O
where	O
+	O
and	O
−	O
denote	O
the	O
positive	O
and	O
negative	O
document	O
-	O
level	O
class	O
labels	O
,	O
respectively	O
.	O
Second	O
,	O
we	O
compute	O
the	O
SO	O
score	O
for	O
each	O
word	O
t	O
:	O
SO	O
(	O
t	O
)	O
=	O
P	O
M	O
I	O
(	O
t	O
,	O
+	O
)	O
−	O
P	O
M	O
I	O
(	O
t	O
,	O
−	O
)	O
.	O
(	O
3	O
)	O
We	O
call	O
{	O
t	O
,	O
SO	O
(	O
t	O
)	O
}	O
as	O
PMI	O
-	O
SO	O
dictionary	O
.	O
The	O
PMI	O
-	O
SO	O
dictionary	O
was	O
widely	O
used	O
as	O
a	O
corpusbased	O
sentiment	O
lexicon	O
for	O
sentiment	O
classification	O
.	O
By	O
contrast	O
,	O
in	O
our	O
approach	O
,	O
it	O
is	O
the	O
first	O
step	O
to	O
learn	O
the	O
sentiment	O
-	O
aware	O
word	O
representation	O
.	O
Our	O
approach	O
supports	O
two	O
kinds	O
of	O
wordlevel	O
sentiment	O
annotations	O
:	O
1	O
)	O
PMI	O
-	O
SO	O
dictionary	O
with	O
hard	O
sentiment	O
annotation	O
;	O
2	O
)	O
PMI	O
-	O
SO	O
dictionary	O
with	O
soft	O
sentiment	O
annotation	O
.	O
The	O
bias	O
of	O
document	O
-	O
level	O
softmax	B-MethodName
layer	O
θt	O
Weight	O
of	O
word	O
-	O
level	O
softmax	B-MethodName
layer	O
θ	B-HyperparameterName
d	O
Weight	O
of	O
document	O
-	O
level	O
softmax	B-MethodName
layer	O
p	O
(	O
c	O
|	O
et	O
)	O
The	O
sentiment	O
distribution	O
of	O
word	O
t	O
predicted	O
by	O
our	O
model	O
p	O
(	O
c	O
|	O
de	O
)	O
The	O
sentiment	O
distribution	O
of	O
document	O
d	O
predicted	O
by	O
our	O
model	O
p	O
(	O
c	O
|	O
t	O
)	O
The	O
word	O
-	O
level	O
sentiment	O
annotation	O
of	O
word	O
t	O
with	O
respect	O
to	O
class	O
ĉ	O
p	O
(	O
c	O
|	O
d	O
)	O
The	O
document	O
-	O
level	O
sentiment	O
annotation	O
of	O
document	O
d	O
with	O
respect	O
to	O
class	O
c	O
[	O
p	O
(	O
−	O
|	O
t	O
)	O
,	O
p	O
(	O
+	O
|	O
t	O
)	O
]	O
=	O
	O
	O
	O
	O
[	O
0	B-DatasetName
,	O
1	O
]	O
,	O
if	O
SO	O
(	O
t	O
)	O
>	O
0	B-DatasetName
[	O
1	O
,	O
0	B-DatasetName
]	O
,	O
if	O
SO	O
(	O
t	O
)	O
<	O
0	B-DatasetName
random	O
{	O
[	O
1	O
,	O
0	B-DatasetName
]	O
or	O
[	O
0	B-DatasetName
,	O
1	O
]	O
}	O
,	O
otherwise	O
.	O
(	O
4	O
)	O
(	O
3	O
)	O
PMI	O
-	O
SO	O
lexicon	O
with	O
soft	O
sentiment	O
annotation	O
"	O
Soft	O
sentiment	O
annotation	O
"	O
means	O
that	O
the	O
annotation	O
is	O
given	O
by	O
the	O
probability	O
of	O
two	O
sentiment	O
polarities	O
,	O
rather	O
than	O
the	O
class	O
label	O
.	O
We	O
first	O
use	O
the	O
sigmoid	O
function	O
to	O
map	O
the	O
SO	O
score	O
to	O
the	O
range	O
of	O
a	O
probability	O
,	O
and	O
then	O
define	O
[	O
p	O
(	O
−	O
|	O
t	O
)	O
,	O
p	O
(	O
+	O
|	O
t	O
)	O
]	O
=	O
[	O
1	O
−	O
σ	O
(	O
SO	O
(	O
t	O
)	O
)	O
,	O
σ	O
(	O
SO	O
(	O
t	O
)	O
)	O
]	O
(	O
5	O
)	O
as	O
the	O
PMI	O
-	O
SO	O
soft	O
sentiment	O
distribution	O
of	O
the	O
word	O
t.	O

Till	O
now	O
we	O
have	O
obtained	O
both	O
document	O
and	O
word	O
-	O
level	O
sentiment	O
annotations	O
,	O
in	O
the	O
next	O
step	O
,	O
we	O
propose	O
a	O
neural	O
network	O
framework	O
to	O
learn	O
the	O
sentiment	O
-	O
aware	O
word	O
representation	O
by	O
integrating	O
the	O
sentiment	O
supervision	O
at	O
both	O
word	O
and	O
document	O
granularities	O
.	O
We	O
call	O
it	O
"	O
hierarchical	O
sentiment	O
supervision	O
"	O
.	O
The	O
architecture	O
of	O
our	O
model	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
We	O
denote	O
the	O
corpus	O
as	O
D	O
=	O
{	O
d	O
1	O
,	O
d	O
2	O
,	O
...	O
,	O
d	O
N	O
}	O
where	O
N	O
is	O
the	O
size	O
of	O
the	O
corpus	O
.	O
Suppose	O
d	O
k	O
is	O
k	O
-	O
th	O
document	O
in	O
D	O
,	O
and	O
t	O
i	O
represents	O
the	O
i	O
-	O
th	O
word	O
in	O
a	O
document	O
d.	O
The	O
parameters	O
used	O
in	O
our	O
neural	O
network	O
are	O
described	O
in	O
Table	O
1	O
.	O
We	O
construct	O
a	O
embedding	O
matrix	O
C	O
R	O
V	O
×M	O
,	O
of	O
which	O
each	O
row	O
represents	O
the	O
embedding	O
of	O
a	O
word	O
in	O
the	O
vocabulary	O
,	O
where	O
V	O
is	O
the	O
size	O
of	O
the	O
vocabulary	O
and	O
M	O
is	O
the	O
dimension	O
of	O
word	O
embedding	O
.	O
We	O
randomly	O
initialize	O
each	O
element	O
of	O
matrix	O
C	O
with	O
a	O
normal	O
distribution	O
.	O
(	O
1	O
)	O
Word	O
-	O
Level	O
Sentiment	O
Supervision	O
We	O
use	O
the	O
word	O
-	O
level	O
sentiment	O
annotation	O
[	O
p	O
(	O
−	O
|	O
t	O
)	O
,	O
p	O
(	O
+	O
|	O
t	O
)	O
]	O
provided	O
in	O
Section	O
3.1	O
to	O
supervise	O
word	O
representation	B-TaskName
learning	I-TaskName
at	O
the	O
word	O
level	O
.	O
For	O
each	O
word	O
in	O
document	O
d	O
,	O
we	O
map	O
it	O
to	O
a	O
continuous	O
representation	O
as	O
e	O
C	O
and	O
feed	O
e	O
into	O
our	O
model	O
to	O
predict	O
the	O
sentiment	O
distribution	O
of	O
the	O
input	O
word	O
:	O
p	O
(	O
c	O
|	O
e	O
)	O
=	O
sof	B-DatasetName
tmax	O
(	O
θ	B-HyperparameterName
t	O
e	O
+	O
b	O
t	O
)	O
.	O
(	O
6	O
)	O
The	O
cost	O
function	O
is	O
defined	O
as	O
the	O
average	O
cross	O
entropy	O
that	O
measures	O
the	O
difference	O
between	O
the	O
sentiment	O
distribution	O
predicted	O
in	O
our	O
model	O
and	O
the	O
sentiment	O
annotations	O
at	O
the	O
word	O
level	O
:	O
f	O
word	O
=	O
−	O
1	O
T	O
N	O
k=1	O
t	O
d	O
k	O
c	O
{	O
+	O
,	O
−	O
}	O
p	O
(	O
c	O
|	O
t	O
)	O
log	O
p	O
(	O
c	O
|	O
e	O
t	O
)	O
(	O
7	O
)	O
where	O
T	O
is	O
the	O
number	O
of	O
words	O
in	O
corpus	O
.	O
(	O
2	O
)	O
Document	O
-	O
Level	O
Sentiment	O
Supervision	O
We	O
use	O
the	O
document	O
-	O
level	O
sentiment	O
annotations	O
to	O
supervise	O
word	O
representation	B-TaskName
learning	I-TaskName
at	O
the	O
document	O
level	O
.	O
In	O
order	O
to	O
obtain	O
a	O
continuous	O
representation	O
of	O
a	O
document	O
d	O
,	O
we	O
simply	O
use	O
the	O
average	O
embedding	O
of	O
words	O
in	O
d	O
as	O
de	O
:	O
de	O
=	O
1	O
|	O
d	O
|	O
t	O
d	O
e	O
t	O
.	O
(	O
8	O
)	O
We	O
feed	O
de	O
into	O
our	O
model	O
to	O
predict	O
the	O
sentiment	O
probability	O
:	O
t	O
i	O
is	O
the	O
i	O
-	O
th	O
word	O
in	O
d.	O
And	O
e	O
t	O
i	O
represents	O
the	O
embedding	O
of	O
the	O
word	O
t	O
i	O
.	O
We	O
take	O
de	O
,	O
the	O
average	O
embedding	O
of	O
[	O
e	O
t	O
1	O
,	O
e	O
t	O
2	O
,	O
.	O
.	O
.	O
,	O
e	O
tn	O
]	O
,	O
as	O
the	O
representation	O
of	O
document	O
d.	O
We	O
get	O
each	O
embedding	O
of	O
words	O
in	O
d	O
as	O
input	O
to	O
predict	O
its	O
sentiment	O
polarities	O
.	O
We	O
also	O
take	O
de	O
as	O
input	O
to	O
predict	O
the	O
sentiment	O
for	O
document	O
d	O
one	O
time	O
per	O
epoch	O
.	O
p	O
(	O
c	O
|	O
de	O
)	O
=	O
sof	B-DatasetName
tmax	O
(	O
θ	B-HyperparameterName
d	O
de	O
+	O
b	O
d	O
)	O
.	O
(	O
9	O
)	O
Similarly	O
,	O
the	O
cost	O
function	O
is	O
defined	O
as	O
average	O
cross	O
entropy	O
that	O
measures	O
the	O
difference	O
between	O
the	O
sentiment	O
distribution	O
predicted	O
in	O
our	O
model	O
and	O
the	O
sentiment	O
annotation	O
at	O
the	O
document	O
level	O
:	O
f	O
doc	O
=	O
−	O
1	O
N	O
N	O
k=1	O
c	O
{	O
+	O
,	O
−	O
}	O
p	O
(	O
c	O
|	O
d	O
k	O
)	O
log	O
p	O
(	O
c	O
|	O
de	O
k	O
)	O
(	O
10	O
)	O
wherep	O
(	O
c	O
|	O
d	O
k	O
)	O
is	O
the	O
sentiment	O
annotation	O
of	O
doc	O
-	O
ument	O
d	O
k	O
.p	O
(	O
c	O
|	O
d	O
k	O
)	O
=	O
1	O
denotes	O
the	O
class	O
label	O
of	O
d	O
k	O
is	O
positive	O
,	O
otherwisep	O
(	O
c	O
|	O
d	O
k	O
)	O
=	O
0	B-DatasetName
.	O
(	O
3	O
)	O
Word	O
and	O
Document	O
-	O
Level	O
Joint	O
Learning	O
In	O
order	O
to	O
learn	O
the	O
sentiment	O
-	O
aware	O
word	O
representation	O
at	O
both	O
word	O
and	O
document	O
levels	O
,	O
we	O
integrate	O
the	O
cost	O
function	O
of	O
two	O
levels	O
in	O
a	O
weighted	O
combination	O
way	O
.	O
The	O
final	O
cost	O
function	O
is	O
defined	O
as	O
follows	O
:	O
f	O
=	O
αf	O
word	O
+	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
f	O
doc	O
(	O
11	O
)	O
where	O
α	B-HyperparameterName
is	O
a	O
tradeoff	O
parameter	O
(	O
0	B-DatasetName
≤	O
α	B-HyperparameterName
≤	O
1	O
)	O
.	O
The	O
weight	O
of	O
f	O
word	O
can	O
be	O
increased	O
by	O
choosing	O
a	O
lager	O
value	O
of	O
α	B-HyperparameterName
.	O
We	O
train	O
our	O
neural	O
model	O
with	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
and	O
use	O
AdaGrad	B-MethodName
(	O
Duchi	O
et	O
al	O
,	O
2011	O
)	O
to	O
update	O
the	O
parameters	O
.	O

We	O
utilize	O
the	O
public	O
distant	O
-	O
supervision	O
corpus	O
2	O
(	O
Go	O
et	O
al	O
,	O
2009	O
)	O
to	O
learn	O
our	O
lexicons	O
.	O
We	O
set	O
M	O
,	O
the	O
dimension	O
of	O
embedding	O
,	O
as	O
50	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
0.3	O
for	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
optimizer	B-HyperparameterName
.	O
We	O
tune	O
the	O
hyper	O
-	O
parameter	O
α	B-HyperparameterName
in	O
the	O
training	O
process	O
.	O
We	O
evaluate	O
the	O
sentiment	O
lexicons	O
in	O
both	O
supervised	O
and	O
unsupervised	O
sentiment	O
classification	O
tasks	O
,	O
on	O
the	O
SemEval	B-DatasetName
2013	I-DatasetName
-	O
2016	O
datasets	O
.	O
The	O
statistics	O
of	O
evaluation	O
datasets	O
are	O
shown	O
in	O
Table	O
2	O
.	O

To	O
evaluate	O
the	O
effect	O
of	O
the	O
sentiment	O
lexicon	O
in	O
supervised	O
sentiment	O
classification	O
,	O
we	O
report	O
the	O
supervised	O
sentiment	O
classification	O
performance	O
by	O
using	O
some	O
pre	O
-	O
defined	O
lexicon	O
features	O
.	O
We	O
follow	O
(	O
Mohammad	O
et	O
al	O
,	O
2013	O
)	O
to	O
extract	O
the	O
lexicon	O
features	O
as	O
follows	O
:	O
Total	O
count	O
of	O
words	O
in	O
the	O
tweet	O
score	O
of	O
which	O
is	O
greater	O
than	O
0	B-DatasetName
;	O
Total	O
count	O
of	O
words	O
in	O
the	O
tweet	O
score	O
of	O
which	O
is	O
less	O
than	O
0	B-DatasetName
;	O
The	O
sum	O
of	O
scores	O
for	O
all	O
word	O
great	O
than	O
0	B-DatasetName
;	O
2	O
http://help.sentiment140.com/for	O
-	O
students	O
The	O
sum	O
of	O
scores	O
for	O
all	O
word	O
less	O
than	O
0	B-DatasetName
;	O
The	O
max	O
score	O
greater	O
than	O
0	B-DatasetName
;	O
The	O
min	O
score	O
less	O
than	O
0	B-DatasetName
;	O
Non	O
-	O
zero	O
score	O
of	O
the	O
last	O
positive	O
word	O
in	O
the	O
tweet	O
;	O
Non	O
-	O
zero	O
score	O
of	O
the	O
last	O
negative	O
word	O
in	O
the	O
tweet	O
.	O
We	O
report	O
the	O
performance	O
of	O
SVM	B-MethodName
by	O
using	O
these	O
lexicon	O
features	O
.	O
The	O
LIBSVM	O
3	O
toolkit	O
is	O
used	O
with	O
a	O
linear	O
kernel	O
and	O
the	O
penalty	O
parameter	O
is	O
set	O
as	O
the	O
default	O
value	O
.	O
The	O
metric	O
is	O
F	O
1	O
score	O
.	O
Unsupervised	O
Sentiment	O
Classification	B-TaskName
Evaluation	O
:	O
For	O
unsupervised	O
sentiment	O
classification	O
,	O
we	O
sum	O
up	O
the	O
scores	O
of	O
all	O
sentiment	O
words	O
in	O
the	O
document	O
,	O
according	O
to	O
the	O
sentiment	O
lexicon	O
.	O
If	O
the	O
sum	O
is	O
greater	O
than	O
0	B-DatasetName
,	O
the	O
document	O
will	O
be	O
considered	O
as	O
positive	O
,	O
otherwise	O
negative	O
.	O
The	O
unsupervised	O
learning	O
evaluation	O
metric	O
is	O
accuracy	B-MetricName
.	O

We	O
compare	O
our	O
HSSWE	O
method	O
with	O
four	O
sentiment	O
lexicons	O
generated	O
by	O
the	O
related	O
work	O
proposed	O
in	O
recent	O
years	O
:	O
Sentiment140	B-DatasetName
was	O
constructed	O
by	O
Mohammad	O
et	O
al	O
(	O
2013	O
)	O
on	O
tweet	O
corpus	O
based	O
on	O
PMI	O
between	O
each	O
word	O
and	O
the	O
emoticons	O
.	O
HIT	O
was	O
constructed	O
by	O
Tang	O
et	O
al	O
(	O
2014a	O
)	O
with	O
a	O
representation	B-TaskName
learning	I-TaskName
approach	O
.	O
NN	O
was	O
constructed	O
by	O
Vo	O
and	O
Zhang	O
(	O
2016	O
)	O
with	O
a	O
neural	O
network	O
method	O
.	O
ETSL	O
refers	O
to	O
SemEval	O
-	O
2015	O
English	O
Twitter	O
Sentiment	O
Lexicon	O
4	O
(	O
Rosenthal	O
et	O
al	O
,	O
2015	O
;	O
Kiritchenko	O
et	O
al	O
,	O
2014	O
)	O
,	O
which	O
is	O
done	O
using	O
Best	O
-	O
Worst	O
Scaling	O
.	O
Note	O
that	O
Tang	O
et	O
al	O
(	O
2014a	O
)	O
all	O
the	O
comparison	O
experiments	O
on	O
the	O
complete	O
benchmark	O
datasets	O
.	O
Supervised	O
Sentiment	O
Classification	B-TaskName
:	O
We	O
first	O
report	O
the	O
supervised	O
sentiment	O
classification	O
F	O
1	O
score	O
of	O
five	O
compared	O
methods	O
on	O
the	O
Semeval	B-DatasetName
2013	I-DatasetName
-	O
2016	O
datasets	O
in	O
Table	O
3	O
.	O
It	O
can	O
be	O
seen	O
that	O
our	O
HSSWE	O
method	O
gets	O
the	O
best	O
result	O
on	O
all	O
four	O
datasets	O
.	O
It	O
outperforms	O
Sentiment140	B-DatasetName
,	O
HIT	O
,	O
NN	O
and	O
ETSL	O
1.7	O
,	O
2.8	O
,	O
1.9	O
,	O
and	O
3.2	O
percentages	O
on	O
the	O
average	O
of	O
four	O
datasets	O
.	O
The	O
improvements	O
are	O
significant	O
according	O
to	O
the	O
paired	O
t	O
-	O
test	O
.	O
Unsupervised	O
Sentiment	O
Classification	B-TaskName
:	O
We	O
then	O
report	O
the	O
unsupervised	O
sentiment	O
classification	O
accuracy	B-MetricName
of	O
five	O
methods	O
on	O
the	O
Semeval	B-DatasetName
2013	I-DatasetName
-	O
2016	O
datasets	O
in	O
Table	O
4	O
.	O
In	O
can	O
be	O
seen	O
that	O
HSSWE	O
obtains	O
the	O
best	O
performance	O
on	O
Semeval	B-DatasetName
2013	I-DatasetName
-	O
2015	O
.	O
On	O
the	O
Semeval	O
2016	O
dataset	O
,	O
it	O
is	O
slightly	O
lower	O
than	O
ETSL	O
.	O
Across	O
four	O
datasets	O
,	O
the	O
average	B-MetricName
accuracy	I-MetricName
of	O
HSSWE	O
is	O
6.6	O
,	O
3.1	O
,	O
9.6	O
and	O
0.94	O
higher	O
than	O
Sentiment140	B-DatasetName
,	O
HIT	O
,	O
NN	O
and	O
ET	O
-	O
SL	O
,	O
respectively	O
.	O

In	O
order	O
to	O
further	O
verify	O
the	O
effectiveness	O
of	O
our	O
method	O
and	O
analyze	O
which	O
part	O
of	O
our	O
model	O
contributes	O
the	O
most	O
,	O
we	O
carried	O
out	O
the	O
internal	O
comparison	O
within	O
our	O
model	O
.	O
We	O
design	O
the	O
following	O
two	O
simplified	O
versions	O
of	O
our	O
model	O
for	O
comparison	O
:	O
PMI	O
-	O
SO	O
denotes	O
a	O
PMI	O
-	O
SO	O
based	O
sentiment	O
lexicon	O
with	O
soft	O
sentiment	O
annotation	O
learned	O
in	O
Section	O
3.1	O
.	O
Doc	O
-	O
Sup	O
denotes	O
the	O
neural	O
network	O
system	O
with	O
only	O
document	O
-	O
level	O
sentiment	O
supervision	O
.	O
It	O
equals	O
to	O
HSSWE	O
when	O
α	B-HyperparameterName
=	O
0	B-DatasetName
.	O
Actually	O
,	O
HSSWE	O
can	O
be	O
viewed	O
as	O
a	O
"	O
combination	O
"	O
of	O
PMI	O
-	O
SO	O
and	O
Doc	O
-	O
Sup	O
.	O
In	O
Tables	O
5	O
and	O
6	O
,	O
we	O
report	O
the	O
comparison	O
results	O
on	O
supervised	O
and	O
unsupervised	O
sentiment	O
classification	O
respectively	O
.	O
Supervised	O
Sentiment	O
Classification	B-TaskName
:	O
As	O
is	O
shown	O
in	O
Table	O
5	O
,	O
two	O
basic	O
models	O
PMI	O
-	O
SO	O
and	O
Doc	O
-	O
Sup	O
show	O
similar	O
performance	O
.	O
They	O
have	O
distinct	O
superiority	O
across	O
different	O
datasets	O
.	O
But	O
both	O
are	O
significantly	O
lower	O
than	O
HSSWE	O
.	O
It	O
shows	O
that	O
by	O
combing	O
the	O
supervision	O
at	O
both	O
document	O
and	O
word	O
levels	O
,	O
it	O
can	O
indeed	O
improve	O
the	O
quality	O
of	O
sentiment	O
-	O
aware	O
word	O
embedding	O
and	O
the	O
subsequent	O
sentiment	O
lexicon	O
.	O
Unsupervised	O
Sentiment	O
Classification	B-TaskName
:	O
As	O
is	O
shown	O
in	O
Table	O
6	O
,	O
the	O
conclusions	O
are	O
similar	O
with	O
that	O
in	O
supervised	O
sentiment	O
classification	O
:	O
HSS	O
-	O
WE	O
achieves	O
the	O
significantly	O
better	O
performance	O
.	O

In	O
this	O
section	O
,	O
we	O
discuss	O
the	O
tradeoff	O
between	O
two	O
parts	O
of	O
supervisions	O
by	O
turning	O
the	O
tradeoff	O
parameter	O
α	B-HyperparameterName
.	O
When	O
α	B-HyperparameterName
is	O
0	B-DatasetName
,	O
HSSWE	O
only	O
benefits	O
from	O
the	O
document	O
-	O
level	O
sentiment	O
supervision	O
and	O
when	O
α	B-HyperparameterName
is	O
1	O
,	O
HSSWE	O
benefits	O
from	O
only	O
word	O
-	O
level	O
sentiment	O
supervision	O
.	O
We	O
observe	O
that	O
HSSWE	O
performs	O
better	O
when	O
α	B-HyperparameterName
is	O
in	O
the	O
range	O
of	O
[	O
0.45	O
,	O
0.55	O
]	O
.	O
By	O
integrating	O
two	O
component	O
parts	O
of	O
sentiment	O
supervision	O
,	O
HSSWE	O
has	O
significant	O
superiority	O
over	O
that	O
learned	O
from	O
either	O
one	O
.	O

Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
is	O
a	O
key	O
component	O
of	O
many	O
text	O
processing	O
pipelines	O
and	O
it	O
is	O
thus	O
essential	O
for	O
this	O
component	O
to	O
be	O
robust	O
to	O
different	O
types	O
of	O
input	O
.	O
However	O
,	O
domain	O
transfer	O
of	O
NER	B-TaskName
models	O
with	O
data	O
from	O
multiple	O
genres	O
has	O
not	O
been	O
widely	O
studied	O
.	O
To	O
this	O
end	O
,	O
we	O
conduct	O
NER	B-TaskName
experiments	O
in	O
three	O
predictive	O
setups	O
on	O
data	O
from	O
:	O
a	O
)	O
multiple	O
domains	O
;	O
b	O
)	O
multiple	O
domains	O
where	O
the	O
genre	O
label	O
is	O
unknown	O
at	O
inference	O
time	O
;	O
c	O
)	O
domains	O
not	O
encountered	O
in	O
training	O
.	O
We	O
introduce	O
a	O
new	O
architecture	O
tailored	O
to	O
this	O
task	O
by	O
using	O
shared	O
and	O
private	O
domain	O
parameters	O
and	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
.	O
This	O
consistently	O
outperforms	O
all	O
other	O
baseline	O
and	O
competitive	O
methods	O
on	O
all	O
three	O
experimental	O
setups	O
,	O
with	O
differences	O
ranging	O
between	O
+1.95	O
to	O
+3.11	O
average	B-MetricName
F1	I-MetricName
across	O
multiple	O
genres	O
when	O
compared	O
to	O
standard	O
approaches	O
.	O
These	O
results	O
illustrate	O
the	O
challenges	O
that	O
need	O
to	O
be	O
taken	O
into	O
account	O
when	O
building	O
real	O
-	O
world	O
NLP	O
applications	O
that	O
are	O
robust	O
to	O
various	O
types	O
of	O
text	O
and	O
the	O
methods	O
that	O
can	O
help	O
,	O
at	O
least	O
partially	O
,	O
alleviate	O
these	O
issues	O
.	O

We	O
propose	O
a	O
new	O
architecture	O
based	O
on	O
the	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
tailored	O
to	O
the	O
three	O
proposed	O
experimental	O
setups	O
.	O
Our	O
proposed	O
architecture	O
enhances	O
the	O
base	O
architecture	O
with	O
three	O
components	O
:	O
a	O
)	O
domain	O
-	O
specific	O
and	O
-	O
independent	O
feed	O
-	O
forward	O
layers	O
that	O
process	O
the	O
BiLSTM	B-MethodName
outputs	O
;	O
b	O
)	O
domain	O
-	O
specific	O
and	O
-	O
independent	O
feed	O
forward	O
layers	O
CRFs	O
;	O
c	O
)	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
objective	O
that	O
learns	O
domain	O
labels	O
as	O
an	O
auxiliary	O
task	O
.	O
The	O
proposed	O
architecture	O
changes	O
are	O
motivated	O
by	O
the	O
aim	O
of	O
capturing	O
commonalities	O
in	O
which	O
named	O
entities	O
are	O
referred	O
to	O
,	O
in	O
any	O
given	O
genre	O
,	O
while	O
still	O
allowing	O
for	O
the	O
model	O
to	O
tease	O
apart	O
and	O
exploit	O
domain	O
-	O
specific	O
aspects	O
.	O
The	O
architecture	O
is	O
also	O
designed	O
to	O
capture	O
these	O
commonalities	O
across	O
label	O
relationships	O
,	O
which	O
can	O
vary	O
across	O
domains	O
.	O
In	O
addition	O
,	O
the	O
multi	O
-	O
task	O
objective	O
further	O
assists	O
the	O
model	O
to	O
leverage	O
domaindependent	O
and	O
-	O
independent	O
components	O
.	O
The	O
choice	O
of	O
input	O
representation	O
is	O
orthogonal	O
to	O
the	O
proposed	O
architecture	O
and	O
our	O
extensions	O
to	O
the	O
architecture	O
can	O
be	O
combined	O
with	O
any	O
input	O
repre	O
-	O
sentation	O
.	O
The	O
model	O
architecture	O
is	O
presented	O
in	O
Figure	O
1	O
and	O
described	O
below	O
:	O
Private	O
and	O
Shared	O
Layers	O
We	O
rely	O
on	O
the	O
shared	O
-	O
private	O
paradigm	O
where	O
the	O
model	O
learns	O
both	O
a	O
shared	O
representation	O
across	O
all	O
domains	O
and	O
is	O
useful	O
when	O
the	O
domain	O
of	O
the	O
input	O
is	O
unknown	O
or	O
unseen	O
in	O
training	O
,	O
and	O
a	O
private	O
domain	O
representation	O
that	O
mostly	O
helps	O
tagging	O
in	O
that	O
domain	O
.	O
We	O
model	O
the	O
shared	O
and	O
private	O
features	O
at	O
both	O
the	O
feature	O
mapping	O
stage	O
connecting	O
the	O
BiLSTM	B-MethodName
outputs	O
to	O
the	O
CRF	B-MethodName
(	O
s	O
)	O
and	O
at	O
the	O
CRF	B-MethodName
level	O
.	O
We	O
expect	O
the	O
features	O
extracted	O
by	O
the	O
BiLSTM	B-MethodName
layers	O
to	O
model	O
the	O
structure	O
of	O
the	O
input	O
across	O
all	O
domains	O
.	O
The	O
feed	O
-	O
forward	O
layers	O
capture	O
the	O
domainspecific	O
and	O
-	O
independent	O
information	O
by	O
using	O
private	O
output	O
layers	O
for	O
each	O
domain	O
and	O
one	O
shared	O
output	O
layer	O
.	O
In	O
training	O
,	O
the	O
BiLSTM	B-MethodName
outputs	O
are	O
projected	O
to	O
both	O
the	O
shared	O
layer	O
and	O
the	O
private	O
layer	O
based	O
on	O
the	O
domain	O
label	O
provided	O
in	O
training	O
.	O
The	O
CRF	B-MethodName
layer	O
is	O
used	O
to	O
make	O
a	O
global	O
decision	O
for	O
the	O
entire	O
tag	O
sequence	O
by	O
modelling	O
label	O
dependencies	O
.	O
We	O
expect	O
that	O
this	O
decision	O
is	O
,	O
at	O
least	O
partially	O
,	O
dependent	O
on	O
domain	O
-	O
specific	O
relationships	O
in	O
the	O
label	O
space	O
.	O
Hence	O
,	O
each	O
feedforward	O
layer	O
feeds	O
into	O
either	O
private	O
CRFs	O
(	O
one	O
for	O
each	O
domain	O
)	O
or	O
a	O
shared	O
CRF	B-MethodName
.	O
The	O
separation	O
of	O
the	O
shared	O
and	O
private	O
layers	O
could	O
happen	O
before	O
the	O
CRF	B-MethodName
stage	O
(	O
late	O
separation	O
)	O
or	O
before	O
the	O
feed	O
-	O
forward	O
layer	O
stage	O
(	O
early	O
separation	O
)	O
.	O
We	O
investigate	O
the	O
influence	O
of	O
each	O
individual	O
addition	O
on	O
the	O
multi	O
-	O
domain	O
performance	O
in	O
our	O
analysis	O
section	O
through	O
ablation	O
studies	O
.	O
Given	O
an	O
input	O
,	O
both	O
the	O
shared	O
and	O
the	O
private	O
parameters	O
are	O
used	O
in	O
learning	O
to	O
predict	O
the	O
output	O
.	O
The	O
set	O
of	O
private	O
parameters	O
for	O
each	O
domain	O
are	O
only	O
updated	O
by	O
data	O
from	O
the	O
same	O
domain	O
while	O
the	O
set	O
of	O
shared	O
parameters	O
are	O
updated	O
in	O
a	O
pooled	O
way	O
by	O
taking	O
all	O
available	O
data	O
points	O
in	O
the	O
training	O
stage	O
regardless	O
of	O
the	O
domain	O
characteristics	O
.	O
For	O
a	O
given	O
data	O
point	O
,	O
inference	O
can	O
be	O
run	O
either	O
by	O
:	O
a	O
)	O
passing	O
it	O
though	O
the	O
private	O
components	O
if	O
the	O
domain	O
label	O
is	O
known	O
;	O
b	O
)	O
through	O
the	O
shared	O
components	O
if	O
the	O
domain	O
label	O
in	O
unknown	O
or	O
the	O
domain	O
of	O
the	O
data	O
is	O
unseen	O
in	O
training	O
.	O
To	O
this	O
end	O
,	O
the	O
objective	O
function	O
for	O
the	O
private	O
and	O
shared	O
layers	O
is	O
:	O
LNER	O
SP	O
(	O
x	O
,	O
y	O
)	O
=	O
LNER	O
S	O
(	O
x	O
,	O
y	O
)	O
+	O
LNER	O
P	O
(	O
x	O
,	O
y	O
)	O
(	O
1	O
)	O
where	O
L	O
N	O
ER	O
S	O
and	O
L	O
N	O
ER	O
P	O
stand	O
for	O
the	O
shared	O
layer	O
loss	B-MetricName
and	O
private	O
layer	O
loss	B-MetricName
respectively	O
.	O
Multi	B-TaskName
-	I-TaskName
Task	I-TaskName
Learning	I-TaskName
of	O
Domain	O
Labels	O
Further	O
,	O
to	O
better	O
guide	O
the	O
learning	O
process	O
,	O
we	O
augment	O
our	O
architecture	O
with	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
objective	O
.	O
Through	O
this	O
,	O
the	O
model	O
learns	O
to	O
predict	O
the	O
domain	O
label	O
of	O
each	O
sample	O
in	O
training	O
as	O
an	O
auxiliary	O
task	O
.	O
The	O
architecture	O
uses	O
average	B-MethodName
pooling	I-MethodName
on	O
BiLSTM	B-MethodName
outputs	O
followed	O
by	O
a	O
fully	O
connected	O
layer	O
.	O
Finally	O
,	O
softmax	B-MethodName
is	O
applied	O
over	O
the	O
learned	O
domain	O
feature	O
to	O
obtain	O
a	O
probability	O
distribution	O
of	O
all	O
domain	O
labels	O
.	O
The	O
domain	O
classification	O
objective	O
is	O
to	O
minimize	O
the	O
crossentropy	O
loss	B-MetricName
L	O
domain	O
(	O
x	O
,	O
y	O
d	O
)	O
for	O
an	O
input	O
x	O
with	O
domain	O
label	O
y	O
d	O
.	O
The	O
global	O
objective	O
function	O
is	O
the	O
combination	O
of	O
the	O
NER	B-TaskName
loss	B-MetricName
function	O
and	O
domain	O
loss	B-MetricName
:	O
L	O
(	O
x	O
;	O
y	O
,	O
y	O
d	O
)	O
=	O
LNER	O
SP	O
(	O
x	O
,	O
y	O
)	O
+	O
L	O
domain	O
(	O
x	O
,	O
y	O
d	O
)	O
(	O
2	O
)	O
4	O
Experimental	O
setup	O

For	O
our	O
experiments	O
,	O
we	O
largely	O
follow	O
the	O
training	O
and	O
evaluation	O
procedure	O
used	O
in	O
(	O
Akbik	O
et	O
al	O
,	O
2018	O
)	O
.	O
As	O
hyperparameters	O
,	O
we	O
follow	O
most	O
suggestions	O
outlined	O
in	O
the	O
in	O
-	O
depth	O
study	O
on	O
model	O
robustness	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2017	O
)	O
.	O
Our	O
training	O
uses	O
256	O
hidden	O
states	O
for	O
BiLSTM	B-MethodName
with	O
mini	B-HyperparameterName
-	I-HyperparameterName
batch	I-HyperparameterName
size	I-HyperparameterName
of	O
32	O
.	O
The	O
model	O
parameters	O
are	O
updated	O
using	O
back	O
-	O
propagation	O
and	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
1e	O
−3	O
with	O
weight	B-MethodName
decay	I-MethodName
value	O
1e	O
−5	O
.	O
The	O
model	O
is	O
regularized	O
with	O
a	O
locked	O
dropout	O
rate	O
of	O
0.5	O
.	O
We	O
use	O
300	O
-	O
dimensional	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
as	O
described	O
in	O
Section	O
3.1	O
,	O
whereas	O
the	O
character	O
LSTM	B-MethodName
is	O
randomly	O
initialized	O
and	O
has	O
a	O
hidden	O
dimension	O
of	O
64	O
.	O
The	O
embeddings	O
are	O
updated	O
on	O
the	O
training	O
data	O
.	O
When	O
training	O
the	O
domain	O
features	O
together	O
with	O
the	O
NER	B-TaskName
(	O
PoolDomain+DomainFeat	O
)	O
,	O
we	O
set	O
the	O
domain	O
embedding	O
size	O
to	O
128	O
.	O
We	O
train	O
all	O
models	O
for	O
20	O
epochs	O
and	O
report	O
the	O
results	O
for	O
the	O
model	O
performing	O
best	O
on	O
the	O
joint	O
development	O
set	O
of	O
the	O
open	O
data	O
set	O
collection	O
.	O

First	O
,	O
we	O
compare	O
models	O
when	O
assuming	O
the	O
domain	O
label	O
of	O
each	O
test	O
document	O
is	O
known	O
at	O
inference	O
time	O
.	O
The	O
results	O
are	O
listed	O
in	O
Table	O
2	O
.	O
Our	O
proposed	O
method	O
-	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
(	O
P	O
)	O
-	O
obtains	O
the	O
best	O
results	O
across	O
the	O
entire	O
test	O
collection	O
in	O
both	O
micro	O
-	O
average	O
(	O
+0.43	O
)	O
and	O
macro	O
-	O
average	O
(	O
+1.94	O
)	O
compared	O
to	O
all	O
other	O
approaches	O
and	O
performs	O
best	O
on	O
7	O
out	O
of	O
the	O
8	O
domains	O
.	O
The	O
second	O
best	O
method	O
is	O
the	O
PoolDo	O
-	O
main+DomainFeat	O
which	O
uses	O
the	O
domain	O
feature	O
as	O
input	O
.	O
Our	O
method	O
consistently	O
surpasses	O
the	O
in	O
-	O
domain	O
classifiers	O
(	O
InDomain	O
)	O
on	O
microaverage	O
(	O
+1.48	O
)	O
and	O
macro	O
-	O
average	O
(	O
+3.11	O
)	O
,	O
showing	O
the	O
limitations	O
of	O
naive	O
modeling	O
approaches	O
.	O
Although	O
increases	O
exist	O
across	O
all	O
domains	O
,	O
these	O
are	O
most	O
prominent	O
in	O
domains	O
like	O
TC	O
(	O
+5.36	O
)	O
that	O
have	O
a	O
low	O
density	O
of	O
named	O
entities	O
and	O
where	O
indomain	O
models	O
have	O
access	O
to	O
limited	O
amounts	O
of	O
data	O
.	O
However	O
,	O
the	O
in	O
-	O
domain	O
performance	O
is	O
better	O
than	O
the	O
pooled	O
method	O
of	O
training	O
,	O
which	O
shows	O
consistent	O
drops	O
in	O
performance	O
on	O
some	O
domains	O
(	O
-	O
8.69	O
on	O
WB	O
,	O
-	O
6.77	O
on	O
BC	O
,	O
-	O
1.98	O
on	O
CoNLL	O
)	O
,	O
where	O
information	O
from	O
other	O
domains	O
did	O
not	O
benefit	O
the	O
model	O
.	O
Table	O
2	O
:	O
Experimental	O
results	O
on	O
the	O
eight	O
data	O
sets	O
,	O
as	O
well	O
as	O
micro	O
(	O
µ	O
-	O
)	O
and	O
macro	O
(	O
M	O
-	O
)	O
averaged	O
across	O
data	O
sets	O
.	O
Performance	O
is	O
measured	O
using	O
micro	B-MetricName
F1	I-MetricName
score	O
.	O
The	O
rows	O
with	O
indicate	O
methods	O
that	O
can	O
be	O
applied	O
when	O
the	O
domain	O
label	O
is	O
not	O
known	O
at	O
inference	O
time	O
.	O
(	O
S	O
)	O
and	O
(	O
P	O
)	O
denote	O
if	O
inference	O
is	O
done	O
through	O
the	O
shared	O
(	O
S	O
)	O
or	O
private	O
(	O
P	O
)	O
layers	O
of	O
the	O
architecture	O
.	O
Results	O
in	O
bold	O
are	O
the	O
best	O
across	O
all	O
models	O
,	O
those	O
underlined	O
are	O
best	O
across	O
methods	O
that	O
work	O
with	O
unknown	O
domain	O
labels	O
.	O

We	O
now	O
focus	O
on	O
the	O
experimental	O
setup	O
where	O
domain	O
labels	O
are	O
unknown	O
for	O
each	O
data	O
point	O
at	O
inference	O
time	O
.	O
This	O
is	O
akin	O
to	O
a	O
setup	O
where	O
the	O
user	O
is	O
agnostic	O
to	O
the	O
data	O
the	O
model	O
was	O
trained	O
on	O
.	O
As	O
only	O
a	O
subset	O
of	O
the	O
models	O
can	O
perform	O
inference	O
in	O
this	O
scenario	O
,	O
the	O
results	O
are	O
a	O
subset	O
of	O
those	O
in	O
Table	O
2	O
.	O
Our	O
model	O
-	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
(	O
S	O
)	O
-	O
gains	O
the	O
best	O
overall	O
performance	O
in	O
this	O
setup	O
,	O
with	O
1.95	O
macro	O
-	O
average	B-MetricName
F1	I-MetricName
increase	O
over	O
the	O
next	O
best	O
method	O
(	O
InDomain+DomainClassifier	O
)	O
.	O
The	O
other	O
standard	O
baseline	O
for	O
domain	B-TaskName
adaptation	I-TaskName
(	O
PoolDomain	O
)	O
obtains	O
a	O
similar	O
performance	O
(	O
−2.19	O
compared	O
to	O
our	O
method	O
)	O
to	O
the	O
in	O
-	O
domain	O
approach	O
,	O
which	O
shows	O
the	O
benefits	O
of	O
multidomain	O
adaptation	O
.	O
PoolDomain	O
-	O
Init	O
is	O
performing	O
overall	O
poorly	O
,	O
which	O
shows	O
that	O
the	O
INIT	O
transfer	B-TaskName
learning	I-TaskName
strategy	O
that	O
is	O
somewhat	O
effective	O
for	O
source	O
-	O
target	O
domain	B-TaskName
adaptation	I-TaskName
does	O
not	O
work	O
well	O
in	O
the	O
multidomain	O
setup	O
.	O
Our	O
intuition	O
is	O
that	O
this	O
technique	O
is	O
unable	O
to	O
learn	O
robust	O
features	O
sequentially	O
across	O
N	O
domains	O
,	O
as	O
it	O
performs	O
poorly	O
on	O
the	O
initial	O
trained	O
domains	O
.	O
PoolDomain	O
-	O
GradRev	O
gains	O
relatively	O
weak	O
performance	O
overall	O
,	O
lower	O
than	O
the	O
in	O
-	O
domain	O
baseline	O
.	O

Finally	O
,	O
we	O
show	O
the	O
results	O
on	O
the	O
experimental	O
setup	O
where	O
the	O
test	O
data	O
is	O
the	O
four	O
'	O
Zero	O
-	O
Shot	O
Genres	O
'	O
,	O
which	O
were	O
not	O
used	O
in	O
during	O
training	O
.	O
labels	O
,	O
as	O
we	O
assume	O
that	O
in	O
this	O
setup	O
,	O
the	O
end	O
-	O
user	O
does	O
not	O
have	O
knowledge	O
about	O
the	O
domains	O
used	O
in	O
training	O
and	O
which	O
of	O
these	O
are	O
most	O
similar	O
to	O
the	O
test	O
point	O
.	O
Results	O
show	O
that	O
our	O
proposed	O
method	O
obtains	O
again	O
the	O
best	O
results	O
,	O
with	O
a	O
consistent	O
margin	O
of	O
2.24	O
macro	O
-	O
average	B-MetricName
F1	I-MetricName
improvement	O
over	O
the	O
next	O
method	O
.	O
Pooling	O
all	O
data	O
(	O
PoolDomain	O
)	O
obtains	O
better	O
performance	O
than	O
building	O
in	O
-	O
domain	O
classifiers	O
with	O
domain	O
classification	O
(	O
InDomain+DomainClassifier	O
)	O
unlike	O
in	O
the	O
other	O
setups	O
.	O
This	O
also	O
shows	O
that	O
the	O
zero	O
-	O
shot	O
domains	O
we	O
used	O
are	O
indeed	O
different	O
to	O
any	O
of	O
the	O
ones	O
in	O
training	O
and	O
pooling	O
all	O
data	O
manages	O
to	O
build	O
a	O
slightly	O
more	O
robust	O
model	O
than	O
individual	O
ones	O
trained	O
on	O
less	O
data	O
.	O
The	O
in	O
-	O
domain	O
models	O
perform	O
5.21	O
F1	B-MetricName
points	O
lower	O
than	O
our	O
approach	O
,	O
the	O
largest	O
gap	O
in	O
all	O
experimental	O
setups	O
,	O
highlighting	O
the	O
robustness	O
of	O
the	O
multi	O
-	O
domain	O
modeling	O
approach	O
.	O
The	O
MultDomain	O
-	O
SP	O
(	O
S	O
)	O
model	O
is	O
second	O
best	O
,	O
and	O
as	O
this	O
is	O
the	O
base	O
for	O
our	O
method	O
,	O
we	O
discuss	O
its	O
performance	O
in	O
the	O
ablation	O
study	O
from	O
the	O
next	O
section	O
.	O

In	O
order	O
to	O
understand	O
the	O
limitations	O
of	O
the	O
multidomain	O
setup	O
,	O
we	O
study	O
whether	O
the	O
models	O
we	O
can	O
build	O
from	O
the	O
available	O
data	O
could	O
theoretically	O
achieve	O
better	O
overall	O
performance	O
.	O
We	O
use	O
an	O
oracle	O
-	O
based	O
selection	O
technique	O
on	O
the	O
in	O
-	O
domain	O
models	O
to	O
select	O
,	O
after	O
the	O
prediction	O
and	O
using	O
the	O
gold	O
labels	O
the	O
model	O
which	O
performed	O
best	O
for	O
each	O
test	O
instance	O
,	O
as	O
selected	O
using	O
F1	B-MetricName
score	I-MetricName
or	O
,	O
if	O
there	O
are	O
no	O
entities	O
,	O
the	O
model	O
with	O
most	O
O	O
predictions	O
.	O
If	O
multiple	O
models	O
are	O
tied	O
,	O
we	O
choose	O
one	O
at	O
random	O
.	O
The	O
oracle	O
thus	O
provides	O
the	O
counterfactually	O
"	O
Optimal	O
"	O
strategy	O
of	O
model	B-TaskName
selection	I-TaskName
for	O
each	O
test	O
instance	O
and	O
represents	O
an	O
upper	O
bound	O
on	O
strategies	O
relying	O
on	O
InDomain	O
models	O
.	O
Table	O
5	O
compares	O
the	O
oracle	O
strategy	O
predictions	O
with	O
the	O
InDomain+DomainClassifier	O
and	O
the	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
model	O
.	O
The	O
results	O
show	O
that	O
even	O
though	O
our	O
model	O
improves	O
substantially	O
over	O
the	O
in	O
-	O
domain	O
models	O
,	O
an	O
oracle	O
selection	O
method	O
would	O
push	O
performance	O
much	O
higher	O
(	O
+6.73	O
F1	B-MetricName
on	O
the	O
open	O
data	O
)	O
.	O
This	O
highlights	O
both	O
the	O
variability	O
of	O
NER	B-TaskName
models	O
trained	O
on	O
different	O
data	O
sets	O
and	O
that	O
there	O
is	O
potentially	O
more	O
room	O
for	O
improvements	O
in	O
the	O
multi	O
-	O
domain	O
setup	O
.	O

The	O
Supplementary	B-DatasetName
Material	I-DatasetName
shows	O
a	O
breakdown	O
of	O
the	O
domain	O
prediction	O
labels	O
for	O
three	O
methods	O
:	O
domain	O
classification	O
,	O
domain	O
prediction	O
in	O
the	O
proposed	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
model	O
and	O
the	O
oracle	O
in	O
-	O
domain	O
choice	O
on	O
gold	O
data	O
.	O
The	O
oracle	O
strategy	O
selects	O
the	O
predictions	O
from	O
all	O
in	O
-	O
domain	O
models	O
.	O
Based	O
on	O
this	O
,	O
we	O
analyzed	O
the	O
performance	O
of	O
each	O
individual	O
in	O
-	O
domain	O
model	O
when	O
tested	O
on	O
all	O
domains	O
in	O
Table	O
6	O
.	O
We	O
find	O
that	O
although	O
the	O
Oracle	O
strategy	O
uses	O
a	O
mix	O
of	O
models	O
,	O
any	O
model	O
alone	O
is	O
unable	O
to	O
generalize	O
to	O
other	O
domains	O
(	O
67.19	O
vs.	O
84.68	O
best	O
InDomain	O
model	O
compared	O
to	O
the	O
best	O
overall	O
model	O
)	O
.	O
In	O
the	O
zero	O
-	O
shot	O
genres	O
,	O
the	O
Twitter	O
model	O
performs	O
close	O
to	O
the	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
model	O
(	O
-	O
0.56	O
F1	B-MetricName
)	O
,	O
albeit	O
it	O
is	O
24	O
F1	B-MetricName
lower	O
on	O
the	O
multi	O
-	O
domain	O
setup	O
.	O
This	O
reinforces	O
that	O
learning	O
shared	O
domain	O
features	O
as	O
opposed	O
to	O
learning	O
individual	O
models	O
helps	O
boost	O
performance	O
and	O
is	O
more	O
robust	O
to	O
different	O
types	O
of	O
inputs	O
.	O

Finally	O
,	O
we	O
compare	O
the	O
runtime	O
difference	O
across	O
various	O
methods	O
listed	O
in	O
the	O
experiment	O
section	O
to	O
test	O
the	O
practical	O
implications	O
of	O
using	O
our	O
pro	O
-	O
posed	O
multi	O
-	O
domain	O
modelling	O
approach	O
.	O
In	O
test	O
phase	O
,	O
we	O
set	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
as	O
128	O
.	O
Table	O
7	O
shows	O
the	O
average	O
time	O
of	O
inference	O
time	O
used	O
for	O
each	O
model	O
.	O
Our	O
proposed	O
model	O
architecture	O
takes	O
0.15	O
ms	O
(	O
33	O
%	O
increase	O
)	O
longer	O
for	O
inference	O
than	O
InDomain	O
or	O
PoolDomain	O
models	O
,	O
which	O
is	O
a	O
result	O
of	O
more	O
model	O
parameters	O
.	O
However	O
,	O
our	O
proposed	O
architecture	O
is	O
still	O
0.19	O
ms	O
faster	O
than	O
using	O
the	O
InDomain+DomainClassifier	O
approach	O
.	O
In	O
addition	O
to	O
inference	O
runtime	O
,	O
we	O
also	O
find	O
that	O
the	O
training	O
time	O
is	O
not	O
significantly	O
more	O
than	O
the	O
combined	O
training	O
time	O
of	O
N	O
in	O
-	O
domain	O
models	O
.	O
The	O
main	O
additions	O
are	O
that	O
of	O
the	O
shared	O
layers	O
and	O
the	O
auxiliary	O
task	O
to	O
the	O
components	O
of	O
the	O
N	O
in	O
-	O
domain	O
models	O
and	O
is	O
thus	O
a	O
constant	O
addition	O
in	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
to	O
the	O
total	O
of	O
N	O
indomain	O
models	O
.	O
Hence	O
,	O
the	O
model	O
would	O
scale	O
by	O
a	O
constant	O
with	O
respect	O
to	O
the	O
number	O
of	O
input	O
domains	O
(	O
N+1	O
number	O
of	O
components	O
,	O
where	O
N	O
is	O
the	O
number	O
of	O
domains	O
)	O
.	O
This	O
should	O
allow	O
our	O
pro	O
-	O
posed	O
model	O
to	O
scale	O
to	O
a	O
large	O
number	O
of	O
domains	O
.	O
This	O
highlights	O
that	O
the	O
proposed	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
model	O
is	O
a	O
viable	O
option	O
for	O
real	O
-	O
world	O
applications	O
.	O

Robustness	O
of	O
NLP	O
models	O
is	O
essential	O
to	O
their	O
wider	O
adoption	O
and	O
usability	O
.	O
Existing	O
NER	B-TaskName
approaches	O
are	O
widely	O
faced	O
with	O
limited	O
scalability	O
when	O
applied	O
to	O
data	O
that	O
spans	O
multiple	O
domains	O
.	O
This	O
paper	O
introduced	O
three	O
experimental	O
setups	O
that	O
provide	O
a	O
framework	O
for	O
evaluating	O
the	O
robustness	O
of	O
NER	B-TaskName
models	O
.	O
These	O
include	O
learning	O
from	O
data	O
in	O
multiple	O
domains	O
and	O
testing	O
on	O
all	O
domains	O
,	O
when	O
the	O
domain	O
label	O
of	O
the	O
test	O
point	O
is	O
unknown	O
and	O
when	O
this	O
does	O
not	O
belong	O
to	O
a	O
domain	O
seen	O
in	O
training	O
.	O
Building	O
on	O
past	O
research	O
,	O
we	O
proposed	O
a	O
new	O
neural	O
architecture	O
that	O
achieves	O
substantial	O
improvements	O
of	O
up	O
to	O
5	O
F1	B-MetricName
points	O
when	O
compared	O
to	O
standard	O
methods	O
.	O
Future	O
work	O
will	O
focus	O
on	O
domain	B-TaskName
adaptation	I-TaskName
at	O
the	O
embedding	O
layer	O
.	O
Finally	O
,	O
we	O
plot	O
the	O
domain	O
prediction	O
distribution	O
on	O
the	O
zero	O
-	O
shot	O
genre	O
data	O
in	O
Figure	O
3	O
.	O
We	O
find	O
that	O
similar	O
to	O
the	O
confusion	O
matrices	O
,	O
the	O
oracle	O
strategy	O
has	O
a	O
more	O
even	O
spread	O
in	O
domain	O
selection	O
.	O
We	O
observe	O
similar	O
patterns	O
to	O
the	O
confusion	O
matrices	O
for	O
the	O
InDomain+DomainClassifier	O
and	O
MultDomain	O
-	O
SP	O
-	O
Aux	O
models	O
.	O

This	O
paper	O
presents	O
a	O
English	O
-	O
Korean	O
parallel	O
dataset	O
that	O
collects	O
381	O
K	O
news	O
articles	O
where	O
1	O
,	O
400	O
of	O
them	O
,	O
comprising	O
10	O
K	O
sentences	O
,	O
are	O
manually	O
labeled	O
for	O
crosslingual	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
.	O
The	O
annotation	O
guidelines	O
for	O
the	O
two	O
languages	O
are	O
developed	O
in	O
parallel	O
,	O
that	O
yield	O
the	O
inter	O
-	O
annotator	O
agreement	O
scores	O
of	O
91	O
and	O
88	O
%	O
for	O
English	O
and	O
Korean	O
respectively	O
,	O
indicating	O
sublime	O
quality	O
annotation	O
in	O
our	O
dataset	O
.	O
Three	O
types	O
of	O
crosslingual	O
learning	O
approaches	O
,	O
direct	O
model	O
transfer	O
,	O
embedding	O
projection	O
,	O
and	O
annotation	O
projection	O
,	O
are	O
used	O
to	O
develop	O
zero	O
-	O
shot	O
Korean	O
NER	B-TaskName
models	O
.	O
Our	O
best	O
model	O
gives	O
the	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
51	O
%	O
that	O
is	O
very	O
encouraging	O
,	O
considering	O
the	O
extremely	O
distinct	O
natures	O
of	O
these	O
two	O
languages	O
.	O
This	O
is	O
pioneering	O
work	O
that	O
explores	O
zero	O
-	O
shot	O
crosslingual	O
learning	O
between	O
English	O
and	O
Korean	O
and	O
provides	O
rich	O
parallel	O
annotation	O
for	O
a	O
core	O
NLP	O
task	O
such	O
as	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
.	O

We	O
conduct	O
a	O
team	O
of	O
graduate	O
students	O
majoring	O
in	O
Data	O
Science	O
to	O
manually	O
tag	O
named	O
entities	O
on	O
all	O
parallel	O
sentences	O
in	O
the	O
DEV	O
and	O
TST	O
sets	O
by	O
taking	O
the	O
following	O
3	O
steps	O
:	O
5	O
1	O
.	O
For	O
English	O
,	O
the	O
pseudo	O
-	O
annotated	O
entities	O
are	O
revised	O
by	O
the	O
OntoNotes	B-DatasetName
named	O
entity	O
guidelines	O
(	O
BBN	O
,	O
2014	O
;	O
Maekawa	O
,	O
2018	O
)	O
,	O
and	O
missing	O
entities	O
are	O
annotated	O
as	O
necessary	O
.	O
2	O
.	O
For	O
Korean	O
,	O
the	O
pseudo	O
-	O
annotated	O
entities	O
are	O
revised	O
to	O
match	O
the	O
English	O
tagset	O
,	O
and	O
missing	O
entities	O
are	O
annotated	O
as	O
necessary	O
.	O
3	O
.	O
Let	O
E	O
=	O
{	O
e	O
1	O
,	O
.	O
.	O
.	O
,	O
e	O
n	O
}	O
and	O
K	B-HyperparameterName
=	I-HyperparameterName
{	O
k	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
m	O
}	O
be	O
the	O
lists	O
of	O
entities	O
from	O
Steps	O
1	O
and	O
2	O
for	O
a	O
English	O
and	O
Korean	O
sentence	O
pair	O
,	O
respectively	O
.	O
Every	O
entity	O
pair	O
(	O
e	O
i	O
,	O
k	O
j	O
)	O
is	O
linked	O
in	O
our	O
dataset	O
if	O
e	O
i	O
is	O
the	O
translation	O
of	O
k	O
j	O
.	O
Note	O
that	O
every	O
article	O
in	O
DEV	O
and	O
TST	O
consists	O
of	O
at	O
least	O
5	O
sentences	O
with	O
at	O
least	O
2	O
named	O
entities	O
.	O
Table	O
3	O
shows	O
the	O
statistics	O
of	O
the	O
gold	O
annotation	O
.	O
Out	O
of	O
22	O
,	O
367	O
and	O
21	O
,	O
892	O
named	O
entities	O
annotated	O
in	O
English	O
and	O
Korean	O
sentences	O
,	O
20	O
,	O
300	O
of	O
them	O
are	O
linked	O
across	O
the	O
languages	O
(	O
above	O
90	O
%	O
)	O
.	O
To	O
estimate	O
the	O
inter	O
-	O
annotator	O
agreement	O
,	O
10	O
news	O
articles	O
from	O
each	O
of	O
the	O
first	O
7	O
sections	O
in	O
Table	O
1	O
are	O
randomly	O
picked	O
and	O
double	O
annotated	O
;	O
the	O
rest	O
of	O
DEV	O
and	O
TST	O
are	O
single	O
annotated	O
and	O
sample	O
checked	O
.	O
Table	O
4	O
shows	O
the	O
Cohen	O
's	O
kappa	O
scores	O
measured	O
for	O
the	O
English	O
and	O
Korean	O
annotation	O
.	O
The	O
high	O
labeled	O
matching	O
scores	O
of	O
90.9	O
and	O
88.3	O
are	O
achieved	O
for	O
those	O
two	O
languages	O
respectively	O
,	O
implying	O
that	O
the	O
single	O
annotation	O
in	O
this	O
dataset	O
is	O
expected	O
to	O
be	O
of	O
high	O
quality	O
as	O
well	O
.	O

Let	O
S	O
=	O
{	O
S	O
1	O
,	O
.	O
.	O
.	O
,	O
S	O
n	O
}	O
and	O
T	O
=	O
{	O
T	O
1	O
,	O
.	O
.	O
.	O
,	O
T	O
n	O
}	O
be	O
lists	O
of	O
sentences	O
in	O
the	O
source	O
and	O
target	O
languages	O
,	O
and	O
(	O
S	O
i	O
,	O
T	O
i	O
)	O
be	O
the	O
i'th	O
pair	O
of	O
parallel	O
sentences	O
in	O
those	O
two	O
languages	O
.	O
Let	O
S	O
i	O
=	O
{	O
s	O
i1	O
,	O
.	O
.	O
.	O
,	O
s	O
in	O
}	O
and	O
T	O
i	O
=	O
{	O
t	O
i1	O
,	O
.	O
.	O
.	O
,	O
t	O
i	O
m	O
}	O
where	O
s	O
i	O
and	O
t	O
i	O
are	O
the	O
i'th	O
word	O
in	O
S	O
and	O
T	O
.	O
Then	O
,	O
annotation	O
projection	O
can	O
be	O
performed	O
as	O
proposed	O
by	O
Ni	O
et	O
al	O
(	O
2017	O
)	O
:	O
1	O
.	O
Pseudo	O
-	O
label	O
S	O
∀i	O
S	O
using	O
an	O
existing	O
model	O
in	O
the	O
source	O
language	O
,	O
in	O
our	O
case	O
,	O
ELIT	O
(	O
3.2	O
)	O
.	O
2	O
.	O
Pseudo	O
-	O
align	O
words	O
in	O
every	O
(	O
S	O
i	O
,	O
T	O
i	O
)	O
using	O
an	O
existing	O
tool	O
,	O
in	O
our	O
case	O
,	O
GIZA++	O
(	O
3.2	O
)	O
.	O
If	O
a	O
consecutive	O
word	O
span	O
S	O
j	O
,	O
k	O
i	O
=	O
{	O
s	O
ij	O
,	O
..	O
,	O
s	O
ik	O
}	O
is	O
pseudo	O
-	O
labeled	O
as	O
the	O
entity	O
type	O
as	O
well	O
as	O
pseudo	O
-	O
aligned	O
with	O
a	O
span	O
T	O
a	O
,	O
b	O
i	O
=	O
{	O
t	O
ia	O
,	O
..	O
,	O
t	O
ib	O
}	O
,	O
T	O
a	O
,	O
b	O
i	O
is	O
also	O
pseudo	O
-	O
labeled	O
with	O
.	O
The	O
quality	O
of	O
pseudo	O
annotation	O
hugely	O
depends	O
on	O
the	O
performance	O
of	O
word	B-TaskName
alignment	I-TaskName
,	O
which	O
is	O
generally	O
not	O
robust	O
for	O
the	O
case	O
of	O
distant	O
language	O
pairs	O
such	O
as	O
English	O
and	O
Korean	O
.	O
Thus	O
,	O
we	O
propose	O
a	O
few	O
constraints	O
to	O
filter	O
out	O
noisy	O
annotation	O
.	O
Entity	O
Matching	O
Let	O
ψ	O
be	O
a	O
boolean	O
.	O
If	O
ψ	O
=	O
F	O
,	O
all	O
parallel	O
sentences	O
in	O
(	O
S	O
,	O
T	O
)	O
are	O
used	O
for	O
training	O
.	O
If	O
ψ	O
=	O
T	O
,	O
(	O
S	O
i	O
,	O
T	O
i	O
)	O
is	O
selected	O
for	O
training	O
only	O
if	O
all	O
named	O
entities	O
in	O
S	O
i	O
are	O
properly	O
labeled	O
in	O
T	O
i	O
by	O
the	O
above	O
projection	O
approach	O
.	O
Relative	O
Frequency	O
Let	O
e	O
be	O
an	O
entity	O
term	O
such	O
as	O
"	O
도널드	O
트럼프	O
(	O
Donald	O
Trump	O
)	O
"	O
in	O
Figure	O
1	O
.	O
Let	O
L	O
e	O
be	O
a	O
set	O
of	O
entity	O
types	O
pseudo	O
-	O
annotated	O
for	O
all	O
occurrences	O
of	O
e	O
in	O
the	O
target	O
language	O
.	O
Then	O
,	O
the	O
relative	O
frequency	O
P	O
(	O
|	O
e	O
)	O
for	O
L	O
e	O
and	O
e	O
can	O
be	O
measured	O
as	O
follows	O
,	O
where	O
COUNT	O
(	O
,	O
e	O
)	O
is	O
the	O
number	O
of	O
occurrences	O
for	O
e	O
being	O
labeled	O
as	O
:	O
P	O
(	O
|	O
e	O
)	O
=	O
COUNT	O
(	O
,	O
e	O
)	O
Le	O
COUNT	O
(	O
,	O
e	O
)	O
Impurity	O
Let	O
F	O
e	O
be	O
a	O
set	O
of	O
unique	O
terms	O
in	O
the	O
source	O
language	O
that	O
are	O
pseudo	O
-	O
aligned	O
with	O
the	O
term	O
e	O
labeled	O
as	O
in	O
the	O
target	O
language	O
such	O
that	O
|	O
F	O
e	O
|	O
≤	O
COUNT	O
(	O
l	O
,	O
e	O
)	O
.	O
Then	O
,	O
the	O
impurity	O
M	O
(	O
,	O
e	O
)	O
is	O
measured	O
as	O
follows	O
where	O
α	B-HyperparameterName
is	O
a	O
smoothing	O
factor	O
:	O
M	O
(	O
,	O
e	O
)	O
=	O
|	O
F	O
e	O
|	O
COUNT	O
(	O
l	O
,	O
e	O
)	O
+	O
α	B-HyperparameterName
The	O
relative	O
frequency	O
P	O
(	O
|	O
e	O
)	O
and	O
the	O
impurity	O
M	O
(	O
,	O
e	O
)	O
are	O
used	O
to	O
assess	O
pseudo	O
-	O
annotation	O
reliability	O
.	O

E	O
i	O
=	O
{	O
(	O
1	O
,	O
e	O
1	O
)	O
,	O
..	O
,	O
(	O
q	O
,	O
e	O
q	O
)	O
}	O
be	O
a	O
list	O
of	O
all	O
(	O
entity	O
term	O
,	O
label	O
)	O
pairs	O
in	O
the	O
target	O
sentence	O
T	O
i	O
.	O
For	O
each	O
T	O
i	O
T	O
,	O
the	O
following	O
two	O
scores	O
,	O
f	O
(	O
T	O
i	O
)	O
and	O
g	O
(	O
T	O
i	O
)	O
,	O
are	O
measured	O
to	O
estimate	O
the	O
reliability	O
of	O
the	O
pseudo	O
-	O
annotation	O
in	O
T	O
i	O
:	O
f	O
(	O
T	O
i	O
)	O
=	O
∀	O
(	O
l	O
,	O
e	O
)	O
E	O
i	O
P	O
(	O
|	O
e	O
)	O
|	O
E	O
i	O
|	O
g	O
(	O
T	O
i	O
)	O
=	O
∀	O
(	O
l	O
,	O
e	O
)	O
E	O
i	O
M	O
(	O
|	O
e	O
)	O
|	O
E	O
i	O
|	O
Given	O
the	O
annotation	O
reliability	O
metrics	O
,	O
our	O
data	O
selection	O
scheme	O
heuristic	O
is	O
as	O
follows	O
:	O
f	O
(	O
T	O
i	O
)	O
≥	O
φ	O
;	O
g	O
(	O
T	O
i	O
)	O
≤	O
γ	B-HyperparameterName
;	O
|	O
E	O
i	O
|	O
≥	O
µ	O
;	O
ψ	O
=	O
T	O
|	O
F	O
Only	O
the	O
target	O
sentences	O
satisfying	O
all	O
of	O
the	O
above	O
constraints	O
are	O
used	O
for	O
training	O
given	O
the	O
hyperparameters	O
ψ	O
,	O
α	B-HyperparameterName
,	O
φ	O
,	O
γ	B-HyperparameterName
,	O
and	O
µ.	O

The	O
experimental	O
settings	O
of	O
direct	O
model	O
transfer	O
approach	O
are	O
identical	O
with	O
Wu	O
and	O
Dredze	O
(	O
2019	O
)	O
.	O
We	O
freeze	O
the	O
bottom	O
n	O
layers	O
(	O
including	O
n	O
)	O
of	O
mBERT	B-MethodName
,	O
where	O
layer	O
0	B-DatasetName
is	O
the	O
embedding	O
layer	O
.	O
The	O
cases	O
of	O
n	O
are	O
{	O
-	O
1	O
,	O
0	B-DatasetName
,	O
3	O
,	O
6	O
,	O
9	O
}	O
,	O
where	O
-	O
1	O
denotes	O
fine	O
-	O
tuning	O
all	O
layers	O
in	O
mBERT	B-MethodName
.	O
For	O
word	O
-	O
level	O
classification	O
,	O
a	O
simple	O
linear	O
classification	O
layer	O
with	O
softmax	B-MethodName
is	O
added	O
on	O
mBERT	B-MethodName
.	O
The	O
hyperparameters	O
we	O
experiment	O
on	O
are	O
the	O
combitations	O
of	O
batch	B-HyperparameterName
size	I-HyperparameterName
{	O
16	O
,	O
32	O
}	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
{	O
2e	O
-	O
5	O
,	O
3e	O
-	O
5	O
,	O
5e	O
-	O
5	O
}	O
,	O
and	O
number	O
of	O
max	O
epochs	O
{	O
3	O
,	O
4	O
}	O
.	O

The	O
annotation	O
projection	O
is	O
performed	O
to	O
generate	O
the	O
pseudo	O
-	O
annotated	O
Korean	O
dataset	O
(	O
Section	O
4.3	O
)	O
.	O
The	O
following	O
5	O
hyperparameters	O
are	O
tuned	O
to	O
filter	O
out	O
noisy	O
annotation	O
for	O
training	O
,	O
where	O
ψ	O
,	O
α	B-HyperparameterName
,	O
and	O
γ	B-HyperparameterName
are	O
newly	O
introduced	O
by	O
our	O
work	O
:	O
ψ	O
:	O
if	O
True	O
,	O
keep	O
only	O
sentences	O
whose	O
entities	O
are	O
completely	O
matching	O
between	O
the	O
two	O
languages	O
.	O
α	B-HyperparameterName
:	O
the	O
smoothing	O
factor	O
to	O
measure	O
the	O
impurity	O
.	O
φ	O
:	O
retain	O
sentences	O
whose	O
annotation	O
reliability	O
scores	O
by	O
relative	O
frequency	O
≥	O
this	O
threshold	O
.	O
γ	B-HyperparameterName
:	O
retain	O
sentences	O
whose	O
annotation	O
reliability	O
scores	O
by	O
impurity	O
≤	O
this	O
threshold	O
.	O
µ	O
:	O
retain	O
sentences	O
that	O
contain	O
named	O
entities	O
whose	O
quantities	O
are	O
≥	O
this	O
cutoff	O
.	O
Once	O
the	O
pseudo	O
-	O
annotation	O
is	O
created	O
,	O
all	O
Korean	O
sentences	O
are	O
encoded	O
by	O
mBERT	B-MethodName
to	O
generate	O
Korean	O
embeddings	O
that	O
are	O
fed	O
into	O
the	O
NER	B-TaskName
model	O
.	O

Table	O
5	O
shows	O
the	O
best	O
result	O
of	O
direct	O
model	O
transfer	O
,	O
mBERT	B-MethodName
fine	O
-	O
tuned	O
on	O
OntoNotes	B-DatasetName
NER	B-TaskName
dataset	O
and	O
evaluated	O
on	O
our	O
Korean	O
TST	O
set	O
.	O
All	O
scores	O
are	O
reported	O
in	O
a	O
form	O
of	O
mean	O
(	O
±	O
standard	O
deviation	O
)	O
after	O
three	O
developments	O
.	O
The	O
best	O
model	O
is	O
built	O
under	O
the	O
setting	O
when	O
all	O
layers	O
including	O
the	O
embedding	O
layer	O
of	O
mBERT	B-MethodName
are	O
fine	O
-	O
tuned	O
.	O
6	O
shows	O
the	O
zero	O
-	O
shot	O
results	O
from	O
the	O
embedding	O
projection	O
models	O
in	O
Section	O
5.3	O
.	O
Both	O
mBERT	B-MethodName
and	O
XLM	B-MethodName
-	O
R	O
models	O
showed	O
a	O
performance	O
improvement	O
over	O
2	O
%	O
with	O
embedding	O
transformation	O
.	O
F1	B-MetricName
score	I-MetricName
improves	O
2.47	O
%	O
(	O
36.67	O
%	O
to	O
39.14	O
%	O
)	O
and	O
2.32	O
%	O
(	O
39.36	O
%	O
to	O
41.68	O
%	O
)	O
for	O
mBERT	B-MethodName
and	O
XLM	B-MethodName
-	O
R	O
,	O
respectively	O
.	O
Both	O
models	O
showed	O
the	O
best	O
performance	O
with	O
embedding	O
transformation	O
matrix	O
made	O
of	O
200k	O
w	O
.	O
The	O
number	O
of	O
parallel	O
sentences	O
used	O
for	O
training	O
transformation	O
matrix	O
has	O
a	O
considerable	O
impact	O
on	O
the	O
Zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
.	O
Table	O
7	O
shows	O
the	O
results	O
from	O
the	O
annotation	O
projection	O
models	O
with	O
various	O
configurations	O
.	O
About	O
9	O
%	O
gain	O
is	O
shown	O
by	O
the	O
best	O
model	O
using	O
only	O
the	O
entity	O
matching	O
constraint	O
ψ	O
that	O
effectively	O
filters	O
out	O
55	O
%	O
of	O
the	O
training	O
data	O
(	O
row	O
3	O
)	O
.	O
A	O
relative	O
score	O
of	O
50.25	O
%	O
is	O
achieved	O
by	O
the	O
model	O
using	O
only	O
21	O
%	O
of	O
the	O
training	O
data	O
,	O
implying	O
that	O
a	O
fair	O
amount	O
of	O
noisy	O
annotation	O
is	O
produced	O
by	O
the	O
annotation	O
projection	O
approach	O
.	O
The	O
overall	O
results	O
show	O
that	O
the	O
Annotation	O
Projection	O
approach	O
achieves	O
the	O
best	O
performance	O
,	O
implying	O
that	O
considering	O
the	O
word	O
order	O
of	O
the	O
target	O
language	O
is	O
critical	O
in	O
cross	O
-	O
lingual	O
learning	O
,	O
especially	O
in	O
the	O
case	O
of	O
distant	O
language	O
pairs	O
.	O
We	O
expect	O
further	O
improvement	O
of	O
the	O
annotation	O
projection	O
approach	O
when	O
adapting	O
a	O
more	O
accurate	O
word	B-TaskName
alignment	I-TaskName
tool	O
or	O
a	O
data	O
selection	O
scheme	O
,	O
which	O
we	O
will	O
further	O
investigate	O
.	O
6	O
Analysis	O

The	O
task	O
specific	O
NER	B-TaskName
model	O
used	O
:	O
a	O
2	O
-	O
layer	O
Bi	O
-	O
LSTM	B-MethodName
with	O
a	O
hidden	O
size	O
of	O
768	O
followed	O
by	O
a	O
CRF	B-MethodName
layer	O
.	O
A	O
dropout	O
rate	O
of	O
0.5	O
is	O
applied	O
on	O
the	O
input	O
and	O
the	O
output	O
of	O
the	O
Bi	O
-	O
LSTM	B-MethodName
.	O
Adam	B-MethodName
with	O
default	O
parameters	O
and	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.0001	O
are	O
used	O
for	O
optimization	O
.	O
We	O
trained	O
the	O
model	O
for	O
10	O
epoch	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	O
,	O
and	O
evaluate	O
the	O
model	O
per	O
a	O
epoch	O
.	O
A.4	O
Comparison	O
of	O
NER	B-TaskName
performances	O
(	O
Zero	O
-	O
shot	O
VS	O
Existing	O
)	O
We	O
compare	O
our	O
best	O
performing	O
Zero	O
-	O
shot	O
Korean	O
NER	B-TaskName
model	O
with	O
the	O
existing	O
Korean	O
NER	B-TaskName
model	O
11	O
on	O
TST	O
.	O

Author	O
name	O
disambiguation	O
(	O
AND	O
)	O
refers	O
to	O
the	O
problem	O
of	O
identifying	O
each	O
unique	O
author	O
entity	O
record	O
from	O
all	O
publication	O
records	O
in	O
scholarly	O
databases	O
(	O
Ferreira	O
et	O
al	O
,	O
2012	O
)	O
.	O
It	O
is	O
also	O
an	O
important	O
preprocessing	O
step	O
for	O
a	O
variety	O
of	O
problems	O
.	O
One	O
example	O
is	O
processing	O
author	O
-	O
related	O
queries	O
properly	O
(	O
e.g.	O
,	O
identify	O
all	O
of	O
a	O
particular	O
author	O
's	O
publications	O
)	O
in	O
a	O
digital	O
library	O
search	O
engine	O
.	O
Another	O
is	O
to	O
calculate	O
author	O
-	O
related	O
statistics	O
such	O
as	O
an	O
h	O
-	O
index	O
,	O
and	O
collaboration	O
relationships	O
between	O
authors	O
.	O
Typically	O
,	O
a	O
clustering	O
method	O
is	O
used	O
to	O
calculate	O
AND	O
.	O
Such	O
clustering	O
calculates	O
pairwise	O
similarities	O
between	O
each	O
possible	O
pairs	O
of	O
records	O
that	O
then	O
determines	O
whether	O
each	O
pair	O
should	O
be	O
in	O
the	O
same	O
cluster	O
.	O
Since	O
the	O
number	O
of	O
possible	O
pairs	O
in	O
a	O
database	O
with	O
the	O
number	O
of	O
records	O
n	O
is	O
n	O
(	O
n	O
−	O
1	O
)	O
/2	O
,	O
it	O
grows	O
as	O
O	O
(	O
n	O
2	O
)	O
.	O
Since	O
n	O
can	O
be	O
millions	O
of	O
authors	O
in	O
some	O
databases	O
such	O
as	O
PubMed	O
,	O
AND	O
algorithms	O
need	O
methods	O
that	O
scale	O
,	O
such	O
as	O
a	O
blocking	O
function	O
(	O
Christen	O
,	O
2012	O
)	O
.	O
The	O
blocking	O
function	O
produces	O
a	O
reduced	O
list	O
of	O
candidate	O
pairs	O
,	O
and	O
only	O
the	O
pairs	O
on	O
the	O
list	O
are	O
considered	O
for	O
clustering	O
.	O
Blocking	O
usually	O
consists	O
of	O
blocking	O
predicates	O
.	O
Each	O
predicate	O
is	O
a	O
logical	O
binary	O
function	O
with	O
a	O
combination	O
of	O
an	O
attribute	O
and	O
a	O
similarity	O
criterion	O
.	O
One	O
example	O
can	O
be	O
exact	B-MetricName
match	I-MetricName
of	O
the	O
last	O
name	O
.	O
A	O
simple	O
but	O
effective	O
way	O
of	O
blocking	O
involves	O
manually	O
selecting	O
the	O
predicates	O
,	O
with	O
respect	O
to	O
the	O
data	O
characteristics	O
.	O
Much	O
recent	O
work	O
on	O
large	O
-	O
scale	O
AND	O
uses	O
a	O
heuristic	O
that	O
is	O
the	O
initial	O
match	O
of	O
first	O
name	O
and	O
exact	B-MetricName
match	I-MetricName
of	O
last	O
name	O
(	O
Torvik	O
and	O
Smalheiser	O
,	O
2009	O
;	O
Liu	O
et	O
al	O
,	O
2014	O
;	O
Levin	O
et	O
al	O
,	O
2012	O
;	O
Kim	O
et	O
al	O
,	O
2016	O
)	O
.	O
Although	O
this	O
gives	O
reasonable	O
completeness	O
,	O
it	O
can	O
be	O
problematic	O
when	O
the	O
database	O
is	O
extremely	O
large	O
,	O
such	O
as	O
the	O
author	O
mentions	O
in	O
CiteSeerX	O
(	O
10	O
M	O
publications	O
,	O
32	O
M	O
authors	O
)	O
,	O
PubMed	O
(	O
24	O
M	O
publications	O
,	O
88	O
M	O
authors	O
)	O
,	O
and	O
Web	O
of	O
Science	O
(	O
45	O
M	O
publications	O
,	O
163	O
M	O
authors	O
)	O
1	O
.	O
The	O
blocking	O
results	O
on	O
PubMed	O
using	O
this	O
heuristic	O
are	O
shown	O
in	O
Table	O
1	O
.	O
Note	O
that	O
most	O
of	O
the	O
block	O
sizes	O
are	O
less	O
than	O
100	O
names	O
,	O
but	O
a	O
few	O
blocks	O
are	O
extremely	O
large	O
.	O
Since	O
the	O
number	O
(	O
Kim	O
et	O
al	O
,	O
2016	O
)	O
.	O
To	O
make	O
matters	O
worse	O
,	O
this	O
problem	O
increases	O
in	O
time	O
,	O
since	O
the	O
growth	O
rates	O
of	O
publication	O
records	O
are	O
rapidly	O
increasing	O
.	O
To	O
improve	O
the	O
blocking	O
,	O
there	O
has	O
been	O
work	O
on	O
learning	O
the	O
blocking	O
(	O
Bilenko	O
et	O
al	O
,	O
2006	O
;	O
Michelson	O
and	O
Knoblock	O
,	O
2006	O
;	O
Cao	O
et	O
al	O
,	O
2011	O
;	O
Kejriwal	O
and	O
Miranker	O
,	O
2013	O
;	O
Das	O
Sarma	O
et	O
al	O
,	O
2012	O
;	O
Fisher	O
et	O
al	O
,	O
2015	O
)	O
.	O
These	O
can	O
be	O
categorized	O
into	O
two	O
different	O
methods	O
.	O
One	O
is	O
a	O
disjoint	O
blocking	O
,	O
where	O
each	O
block	O
is	O
separated	O
so	O
each	O
record	O
belongs	O
to	O
a	O
single	O
block	O
.	O
Another	O
is	O
non	O
-	O
disjoint	O
blocking	O
,	O
where	O
some	O
blocks	O
have	O
shared	O
records	O
.	O
Each	O
has	O
advantages	O
.	O
Disjoint	O
blocking	O
can	O
make	O
the	O
clustering	O
step	O
easily	O
parallelized	O
,	O
while	O
non	O
-	O
disjoint	O
blocking	O
often	O
produces	O
smaller	O
blocks	O
.	O
and	O
also	O
has	O
more	O
degrees	O
of	O
freedom	O
from	O
which	O
to	O
select	O
the	O
similarity	O
criterion	O
.	O
Here	O
,	O
we	O
propose	O
to	O
learn	O
a	O
non	O
-	O
disjoint	O
blocking	O
with	O
a	O
conjunctive	O
normal	O
form	O
(	O
CNF	O
)	O
.	O
Our	O
main	O
contributions	O
are	O
:	O
Propose	O
a	O
CNF	O
blocking	O
,	O
which	O
reduces	O
more	O
pairs	O
compared	O
to	O
DNF	O
blocking	O
,	O
in	O
order	O
to	O
achieve	O
a	O
large	O
number	O
of	O
pairs	O
completeness	O
.	O
This	O
also	O
reduces	O
the	O
processing	O
time	O
,	O
which	O
benefits	O
various	O
applications	O
such	O
as	O
online	O
disambiguation	O
,	O
author	O
search	O
,	O
etc	O
.	O
Extend	O
the	O
method	O
to	O
produce	O
disjoint	O
blocks	O
,	O
so	O
that	O
the	O
AND	O
clustering	O
step	O
can	O
be	O
easily	O
parallelized	O
.	O
Compare	O
different	O
gain	O
functions	O
,	O
which	O
are	O
used	O
to	O
find	O
the	O
best	O
blocking	O
predicates	O
for	O
each	O
step	O
of	O
learning	O
.	O
Previous	O
work	O
is	O
discussed	O
in	O
the	O
next	O
session	O
.	O
This	O
is	O
followed	O
by	O
problem	O
definition	O
.	O
Next	O
,	O
we	O
describe	O
learning	O
of	O
CNF	O
blocking	O
and	O
how	O
to	O
use	O
it	O
to	O
ensure	O
the	O
production	O
of	O
disjoint	O
blocks	O
.	O
Next	O
,	O
we	O
evaluate	O
our	O
methods	O
on	O
the	O
PubMed	O
dataset	O
.	O
Finally	O
,	O
the	O
last	O
section	O
consists	O
of	O
a	O
summary	O
work	O
with	O
possible	O
future	O
directions	O
.	O

Our	O
work	O
tackles	O
the	O
same	O
problem	O
with	O
baseline	O
DNF	O
blocking	O
(	O
Bilenko	O
et	O
al	O
,	O
2006	O
;	O
Michelson	O
and	O
Knoblock	O
,	O
2006	O
)	O
,	O
but	O
in	O
a	O
different	O
way	O
to	O
get	O
the	O
optimized	O
blocking	O
function	O
.	O
Let	O
R	O
=	O
{	O
r	O
1	O
,	O
r	O
2	O
,	O
,	O
r	O
n	O
}	O
be	O
the	O
set	O
of	O
records	O
in	O
the	O
database	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
records	O
.	O
Each	O
record	O
r	O
has	O
k	O
attributes	O
,	O
and	O
A	O
be	O
the	O
attribute	O
set	O
A	O
=	O
{	O
a	O
1	O
.a	O
2	O
,	O
,	O
a	O
k	O
}	O
.	O
A	O
blocking	O
predicate	O
p	O
is	O
a	O
combination	O
of	O
an	O
attribute	O
a	O
and	O
a	O
similarity	O
function	O
s	O
defined	O
to	O
a.	O
An	O
example	O
of	O
s	O
is	O
exact	O
string	O
match	O
of	O
a.	O
A	O
blocking	O
predicate	O
can	O
be	O
seen	O
as	O
a	O
logical	O
binary	O
function	O
applied	O
to	O
each	O
pair	O
of	O
records	O
,	O
so	O
p	O
(	O
r	O
x	O
,	O
r	O
y	O
)	O
=	O
{	O
0	B-DatasetName
,	O
1	O
}	O
,	O
where	O
r	O
x	O
,	O
r	O
y	O
R.	O
A	O
blocking	O
function	O
f	O
is	O
a	O
boolean	O
logic	O
formula	O
consisting	O
with	O
blocking	O
predicates	O
p	O
1	O
,	O
p	O
2	O
,	O
,	O
p	O
n	O
,	O
and	O
each	O
predicate	O
is	O
connected	O
with	O
either	O
conjunction	O
or	O
disjunction	O
∨.	O
An	O
example	O
is	O
f	O
example	O
=	O
(	O
p	O
1	O
p	O
2	O
)	O
∨	O
p	O
3	O
.	O
Since	O
it	O
is	O
made	O
up	O
of	O
blocking	O
predicates	O
,	O
f	O
(	O
r	O
x	O
,	O
r	O
y	O
)	O
=	O
{	O
0	B-DatasetName
,	O
1	O
}	O
for	O
all	O
r	O
x	O
,	O
r	O
y	O
R.	O
The	O
goal	O
is	O
to	O
find	O
an	O
optimal	O
blocking	O
function	O
f	O
*	O
that	O
covers	O
a	O
minimum	O
number	O
of	O
record	O
pairs	O
while	O
missing	O
up	O
to	O
a	O
fraction	O
ε	B-HyperparameterName
of	O
total	O
number	O
of	O
matching	O
record	O
pairs	O
.	O
To	O
formalize	O
it	O
,	O
f	O
*	O
=	O
argmin	O
f	O
(	O
rx	O
,	O
ry	O
)	O
R	O
f	O
(	O
r	O
x	O
,	O
r	O
y	O
)	O
such	O
that	O
≥	O
(	O
1	O
−	O
ε	B-HyperparameterName
)	O
×	O
|	O
R	O
+	O
|	O
(	O
1	O
)	O
where	O
R	O
+	O
is	O
set	O
of	O
matching	O
record	O
pairs	O
.	O

Let	O
N	O
egCov	O
be	O
all	O
l	O
in	O
N	O
eg	O
that	O
satisfies	O
T	O
return	O
CN	O
F	O
40	O
:	O
end	O
function	O
sample	O
pairs	O
,	O
P	O
are	O
blocking	O
predicates	O
,	O
and	O
k	O
is	O
maximum	O
number	O
of	O
predicates	O
in	O
each	O
term	O
.	O
The	O
algorithm	O
first	O
generates	O
a	O
set	O
of	O
negated	O
candidate	O
conjunction	O
term	O
T	O
erms	O
from	O
all	O
p	O
in	O
N	O
egP	O
(	O
line	O
19	O
-	O
22	O
)	O
.	O
A	O
dual	O
of	O
the	O
original	O
gain	O
function	O
CALCNEGGAIN	O
selects	O
a	O
predicate	O
for	O
generating	O
a	O
negated	O
candidate	O
conjunction	O
.	O
Then	O
,	O
as	O
in	O
DNF	O
blocking	O
,	O
the	O
sequential	O
covering	O
algorithm	O
is	O
used	O
to	O
learn	O
the	O
negated	O
DNF	O
formula	O
(	O
line	O
26	O
-	O
37	O
)	O
,	O
which	O
iteratively	O
adds	O
a	O
negated	O
conjunction	O
term	O
until	O
it	O
covers	O
the	O
desired	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
.	O
We	O
select	O
a	O
negated	O
conjunction	O
term	O
with	O
a	O
gain	O
function	O
,	O
CALCNEGGAIN	O
.	O
Also	O
,	O
note	O
that	O
the	O
termination	O
condition	O
of	O
the	O
loop	O
(	O
line	O
26	O
)	O
is	O
when	O
ε	B-HyperparameterName
of	O
total	O
positive	O
samples	O
are	O
covered	O
with	O
the	O
learned	O
N	O
egDN	O
F	O
.	O
This	O
ensures	O
that	O
we	O
miss	O
less	O
than	O
ε	B-HyperparameterName
of	O
the	O
total	O
number	O
of	O
positive	O
samples	O
in	O
the	O
final	O
CNF	O
formula	O
.	O
After	O
getting	O
the	O
final	O
N	O
egDN	O
F	O
,	O
it	O
is	O
negated	O
to	O
get	O
the	O
desired	O
CNF	O
.	O

Disjoint	O
blocking	O
functions	O
generate	O
blocks	O
for	O
each	O
record	O
that	O
resides	O
in	O
a	O
single	O
block	O
;	O
thus	O
such	O
blocks	O
are	O
mutually	O
exclusive	O
.	O
It	O
has	O
the	O
advantage	O
that	O
parallelization	O
can	O
be	O
performed	O
efficiently	O
after	O
applying	O
the	O
blocking	O
by	O
running	O
processes	O
for	O
each	O
blocks	O
separately	O
.	O
A	O
blocking	O
function	O
is	O
disjoint	O
if	O
and	O
only	O
if	O
it	O
satisfies	O
the	O
following	O
conditions	O
:	O
1	O
)	O
it	O
only	O
consists	O
of	O
pure	O
conjunction	O
(	O
logical	O
AND	O
)	O
,	O
2	O
)	O
all	O
predicates	O
use	O
non	O
-	O
relative	O
similarity	O
measures	O
.	O
That	O
is	O
,	O
measures	O
that	O
compare	O
the	O
absolute	O
value	O
of	O
blocking	O
key	O
,	O
e.g.	O
exact	B-MetricName
match	I-MetricName
of	O
first	O
n	O
characters	O
.	O
DNF	O
and	O
CNF	O
blocking	O
are	O
both	O
non	O
-	O
disjoint	O
blocking	O
due	O
to	O
the	O
condition	O
1	O
above	O
.	O
We	O
introduce	O
a	O
simple	O
extension	O
to	O
ensure	O
our	O
CNF	O
blocking	O
can	O
produce	O
disjoint	O
blocks	O
.	O
This	O
is	O
done	O
by	O
first	O
producing	O
two	O
blocking	O
functions	O
.	O
The	O
first	O
function	O
learns	O
a	O
blocking	O
function	O
with	O
only	O
conjunctions	O
based	O
on	O
our	O
CNF	O
blocking	O
method	O
using	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
and	O
a	O
limited	O
set	O
of	O
predicates	O
with	O
nonrelative	O
similarity	O
measures	O
.	O
Then	O
,	O
CNF	O
blocking	O
is	O
learned	O
with	O
our	O
k	O
-	O
CNF	O
method	O
with	O
the	O
whole	O
set	O
of	O
predicates	O
for	O
pairs	O
remaining	O
after	O
applying	O
1	O
-	O
CNF	O
(	O
conjunction	O
of	O
single	O
attributes	O
)	O
.	O
We	O
first	O
apply	O
the	O
1	O
-	O
CNF	O
to	O
the	O
whole	O
database	O
to	O
produce	O
disjoint	O
blocks	O
.	O
Then	O
for	O
each	O
block	O
,	O
we	O
apply	O
the	O
second	O
k	O
-	O
CNF	O
blocking	O
function	O
to	O
filter	O
out	O
pairs	O
not	O
satisfies	O
the	O
k	O
-	O
CNF	O
function	O
.	O
This	O
is	O
similar	O
to	O
applying	O
a	O
filter	O
as	O
in	O
Gu	O
and	O
Baxter	O
Khabsa	O
et	O
al	O
(	O
2015	O
)	O
.	O
While	O
they	O
use	O
a	O
heuristic	O
,	O
our	O
method	O
automatically	O
learns	O
the	O
optimal	O
one	O
.	O
Note	O
that	O
this	O
method	O
still	O
produces	O
a	O
CNF	O
since	O
it	O
combines	O
conjunction	O
terms	O
and	O
k	O
-	O
CNF	O
with	O
logical	O
AND	O
.	O

We	O
evaluate	O
our	O
CNF	O
blocking	O
with	O
reduction	O
ratio	O
(	O
RR	B-DatasetName
)	O
,	O
pairs	O
completeness	O
(	O
PC	O
)	O
,	O
and	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
.	O
These	O
metrics	O
are	O
often	O
used	O
to	O
evaluate	O
blocking	O
methods	O
.	O
Those	O
metrics	O
can	O
be	O
calculated	O
as	O
follows	O
:	O
RR	B-DatasetName
=	O
1	O
−	O
p	O
+	O
n	O
P	O
+	O
N	O
,	O
(	O
5	O
)	O
P	O
C	O
=	O
p	O
P	O
,	O
(	O
6	O
)	O
F	O
=	O
2	O
×	O
RR	B-DatasetName
×	O
P	O
C	O
RR	B-DatasetName
+	O
P	O
C	O
.	O
(	O
7	O
)	O
where	O
P	O
,	O
N	O
are	O
the	O
numbers	O
of	O
positive	O
and	O
negative	O
samples	O
,	O
and	O
p	O
,	O
n	O
are	O
the	O
numbers	O
of	O
positive	O
and	O
negative	O
samples	O
covered	O
with	O
the	O
blocking	O
function	O
.	O
RR	B-DatasetName
measures	O
the	O
efficiency	O
of	O
the	O
blocking	O
function	O
,	O
PC	O
measures	O
the	O
quality	O
of	O
the	O
blocking	O
function	O
.	O
F	O
is	O
the	O
harmonic	O
mean	O
of	O
RR	B-DatasetName
and	O
PC	O
.	O

We	O
first	O
define	O
the	O
similarity	O
criterion	O
used	O
for	O
the	O
experiments	O
.	O
We	O
observed	O
an	O
important	O
characteristic	O
of	O
the	O
data	O
:	O
some	O
attributes	O
are	O
empty	O
(	O
e.g.	O
year	O
:	O
7.8	O
%	O
,	O
affiliation	O
:	O
81.1	O
%	O
)	O
or	O
have	O
only	O
partial	O
information	O
(	O
54.5	O
%	O
has	O
only	O
initials	O
for	O
the	O
first	O
name	O
)	O
.	O
To	O
deal	O
with	O
this	O
,	O
we	O
add	O
compatible	O
to	O
those	O
blocking	O
keys	O
.	O
Below	O
is	O
brief	O
explanation	O
of	O
each	O
similarity	O
criterion	O
.	O
exact	O
:	O
Exact	B-MetricName
match	I-MetricName
.	O
f	O
irst	O
(	O
n	O
)	O
,	O
last	O
(	O
n	O
)	O
:	O
First	O
/	O
Last	O
n	O
character	O
match	O
,	O
where	O
n	O
is	O
an	O
integer	O
.	O
We	O
check	O
{	O
1	O
,	O
3	O
,	O
5	O
,	O
7	O
}	O
for	O
name	O
attributes	O
.	O
order	O
:	O
Assigns	O
T	O
rue	O
if	O
both	O
records	O
are	O
first	O
authors	O
,	O
last	O
authors	O
,	O
or	O
non	O
-	O
first	O
and	O
non	O
-	O
last	O
authors	O
.	O
digit	O
(	O
n	O
)	O
:	O
First	O
n	O
digit	O
match	O
.	O
We	O
check	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
for	O
year	O
.	O
compatible	O
:	O
T	O
rue	O
if	O
at	O
least	O
one	O
of	O
the	O
records	O
are	O
empty	O
(	O
Eq	O
.	O
8	O
)	O
.	O
If	O
the	O
key	O
is	O
name	O
,	O
it	O
also	O
checks	O
if	O
the	O
initial	O
matches	O
if	O
one	O
of	O
the	O
records	O
has	O
only	O
initial	O
.	O
compatible	O
(	O
A	O
,	O
B	O
)	O
=	O
T	O
rue	O
if	O
at	O
least	O
one	O
is	O
empty	O
exact	O
(	O
A	O
,	O
B	O
)	O
otherwise	O
(	O
8	O
)	O
cos	O
:	O
Cosine	O
distance	O
of	O
TF	O
-	O
IDF	O
bag	O
-	O
ofwords	O
vector	O
.	O
We	O
check	O
with	O
threshold	O
{	O
0.2	O
,	O
0.4	O
,	O
0.6	O
,	O
0.8	O
}	O
.	O
dif	O
f	O
:	O
Year	O
difference	O
.	O
We	O
use	O
the	O
threshold	O
{	O
2	O
,	O
5	O
,	O
10	O
}	O
.	O
Using	O
those	O
similarity	O
measures	O
,	O
We	O
define	O
two	O
different	O
sets	O
of	O
blocking	O
predicates	O
.	O
blocking	O
predicates	O
used	O
for	O
non	O
-	O
disjoint	O
blocking	O
.	O
Disjoint	O
blocking	O
requires	O
the	O
use	O
of	O
predicates	O
with	O
non	O
-	O
relative	O
similarity	O
measures	O
to	O
ensure	O
blocks	O
are	O
mutually	O
exclusive	O
.	O
For	O
disjoint	O
blocking	O
,	O
we	O
use	O
the	O
set	O
of	O
blocking	O
predicates	O
excluding	O
the	O
ones	O
with	O
the	O
relative	O
similarity	O
measures	O
(	O
exact	O
,	O
compatible	O
,	O
dif	O
f	O
)	O
in	O
Table	O
3	O
.	O

The	O
parameter	O
ε	B-HyperparameterName
is	O
used	O
to	O
vary	O
the	O
PC	O
.	O
We	O
tested	O
values	O
in	O
[	O
0	B-DatasetName
,	O
1	O
]	O
to	O
get	O
the	O
PC	O
-	O
RR	B-DatasetName
curve	O
.	O
k	O
is	O
selected	O
experimentally	O
to	O
calculate	O
the	O
maximum	O
reachable	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
.	O
We	O
use	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
for	O
further	O
experiments	O
.	O

The	O
variational	B-MethodName
autoencoder	I-MethodName
(	O
Kingma	O
and	O
Welling	O
,	O
2014	O
)	O
is	O
an	O
efficient	O
way	O
to	O
handle	O
(	O
continuous	O
)	O
latent	O
variables	O
in	O
neural	O
models	O
.	O
We	O
describe	O
it	O
briefly	O
here	O
,	O
and	O
interested	O
readers	O
can	O
refer	O
to	O
Doersch	O
(	O
2016	O
)	O
for	O
details	O
.	O
The	O
VAE	B-MethodName
learns	O
a	O
generative	O
model	O
of	O
the	O
probability	O
p	O
(	O
x	O
)	O
of	O
observed	O
data	O
x.	O
The	O
generative	O
process	O
consists	O
of	O
first	O
generating	O
a	O
continuous	O
latent	O
variable	O
z	O
conditioned	O
on	O
the	O
observed	O
data	O
x	O
,	O
which	O
is	O
termed	O
as	O
the	O
recognition	O
model	O
q	O
(	O
z	O
|	O
x	O
)	O
(	O
encoder	O
)	O
and	O
then	O
use	O
this	O
latent	O
variable	O
to	O
reconstruct	O
the	O
observation	O
x	O
known	O
as	O
the	O
reconstruction	O
(	O
decoder	O
)	O
model	O
p	O
(	O
x	O
|	O
z	O
)	O
.	O
VAE	B-MethodName
uses	O
the	O
variational	B-MethodName
inference	I-MethodName
to	O
approximate	O
the	O
intractable	O
posterior	O
by	O
learning	O
a	O
parametric	O
posterior	O
distribution	O
for	O
all	O
observations	O
.	O
Th	O
learning	O
objective	O
function	O
is	O
the	O
variational	O
lower	O
bound	O
on	O
the	O
marginal	O
log	O
likelihood	O
of	O
data	O
:	O
log	O
p	O
θ	B-HyperparameterName
(	O
x	O
)	O
≥	O
E	O
z∼q	O
φ	O
(	O
z	O
|	O
x	O
)	O
[	O
log	O
p	O
θ	B-HyperparameterName
(	O
x	O
|	O
z	O
)	O
]	O
−	O
KL	O
(	O
q	O
φ	O
(	O
z	O
|	O
x	O
)	O
|	O
|	O
p	O
(	O
z	O
)	O
)	O
(	O
1	O
)	O
To	O
optimize	O
the	O
parameters	O
with	O
gradient	O
descent	O
,	O
Kingma	O
and	O
Welling	O
(	O
2014	O
)	O
introduce	O
a	O
reparameterization	O
trick	O
that	O
allows	O
for	O
training	O
using	O
simple	O
backpropagation	O
w.r.t	O
.	O
the	O
Gaussian	O
latent	O
variables	O
z.	O

There	O
are	O
two	O
cases	O
to	O
discuss	O
when	O
employing	O
the	O
variational	O
encoder	O
-	O
decoder	O
framework	O
for	O
labeled	O
sequence	O
transduction	O
.	O
First	O
,	O
when	O
the	O
labels	O
of	O
the	O
inflected	O
words	O
are	O
known	O
as	O
is	O
the	O
format	O
of	O
the	O
training	O
data	O
in	O
the	O
shared	O
task	O
,	O
we	O
do	O
n't	O
need	O
to	O
bother	O
introduction	O
the	O
discrete	O
latent	O
variables	O
for	O
the	O
inflected	O
labels	O
.	O
We	O
maximize	O
the	O
variational	O
lower	O
bound	O
on	O
the	O
conditional	O
log	O
likelihood	O
of	O
observing	O
x	O
(	O
t	O
)	O
and	O
y	O
(	O
t	O
)	O
as	O
follows	O
:	O
log	O
p	O
θ	B-HyperparameterName
(	O
x	O
(	O
t	O
)	O
,	O
y	O
(	O
t	O
)	O
|	O
x	O
(	O
s	O
)	O
)	O
≥	O
E	O
z∼q	O
φ	O
(	O
z	O
|	O
x	O
(	O
s	O
)	O
)	O
log	O
p	O
θ	B-HyperparameterName
(	O
x	O
(	O
t	O
)	O
,	O
y	O
(	O
t	O
)	O
,	O
z	O
|	O
x	O
(	O
s	O
)	O
)	O
q	O
φ	O
(	O
z	O
|	O
x	O
(	O
s	O
)	O
)	O
=	O
E	O
z∼q	O
φ	O
(	O
z	O
|	O
x	O
(	O
s	O
)	O
)	O
[	O
log	O
p	O
θ	B-HyperparameterName
(	O
x	O
(	O
t	O
)	O
|	O
y	O
(	O
t	O
)	O
,	O
z	O
)	O
+	O
log	O
p	O
π	O
(	O
y	O
(	O
t	O
)	O
)	O
]	O
−	O
KL	O
(	O
q	O
φ	O
(	O
z	O
|	O
x	O
(	O
s	O
)	O
)	O
|	O
|	O
p	O
(	O
z	O
)	O
)	O
=	O
L	O
l	O
(	O
x	O
(	O
t	O
)	O
,	O
y	O
(	O
t	O
)	O
|	O
x	O
(	O
s	O
)	O
)	O
(	O
2	O
)	O
which	O
is	O
a	O
simple	O
extension	O
to	O
the	O
vanilla	O
variational	O
auto	O
-	O
enocders	O
.	O
Second	O
,	O
in	O
the	O
case	O
of	O
unsupervised	O
learning	O
or	O
when	O
the	O
labels	O
of	O
the	O
inflected	O
word	O
is	O
not	O
observed	O
,	O
we	O
only	O
observe	O
a	O
word	O
or	O
a	O
pair	O
of	O
words	O
and	O
we	O
would	O
like	O
to	O
maximize	O
the	O
log	O
likelihood	O
of	O
the	O
observed	O
data	O
by	O
marginalizing	O
over	O
possible	O
morphological	O
labels	O
,	O
which	O
is	O
consisted	O
to	O
the	O
supervised	O
case	O
above	O
.	O
In	O
this	O
scenario	O
,	O
we	O
can	O
introduce	O
the	O
discrete	O
latent	O
variables	O
for	O
the	O
inflected	O
labels	O
which	O
are	O
used	O
to	O
infer	O
the	O
labels	O
for	O
the	O
target	O
word	O
.	O
Then	O
when	O
decoding	O
the	O
word	O
,	O
we	O
condition	O
both	O
on	O
the	O
continuous	O
and	O
discrete	O
latent	O
variables	O
.	O
For	O
the	O
variational	O
encoder	O
-	O
decoder	O
(	O
MSVED	O
)	O
,	O
the	O
variational	O
lower	O
bound	O
on	O
the	O
conditional	O
log	O
likelihood	O
is	O
affected	O
by	O
the	O
recognition	O
model	O
,	O
and	O
thus	O
is	O
computed	O
as	O
:	O
log	O
p	O
θ	B-HyperparameterName
(	O
x	O
(	O
t	O
)	O
|	O
x	O
(	O
s	O
)	O
)	O
≥E	O
(	O
y	O
(	O
t	O
)	O
,	O
z	O
)	O
∼q	O
φ	O
(	O
y	O
(	O
t	O
)	O
,	O
z	O
|	O
x	O
(	O
s	O
)	O
,	O
x	O
(	O
t	O
)	O
)	O
log	O
p	O
θ	B-HyperparameterName
(	O
x	O
(	O
t	O
)	O
,	O
y	O
(	O
t	O
)	O
,	O
z	O
|	O
x	O
(	O
s	O
)	O
)	O
q	O
φ	O
(	O
y	O
(	O
t	O
)	O
,	O
z	O
|	O
x	O
(	O
s	O
)	O
,	O
x	O
(	O
t	O
)	O
)	O
=	O
E	O
y	O
(	O
t	O
)	O
∼q	O
φ	O
(	O
y	O
(	O
t	O
)	O
|	O
x	O
(	O
t	O
)	O
)	O
[	O
E	O
z∼q	O
φ	O
(	O
z	O
|	O
x	O
(	O
s	O
)	O
)	O
[	O
log	O
p	O
θ	B-HyperparameterName
(	O
x	O
(	O
t	O
)	O
|	O
y	O
(	O
t	O
)	O
,	O
z	O
)	O
]	O
−	O
KL	O
(	O
q	O
φ	O
(	O
z	O
|	O
x	O
(	O
s	O
)	O
)	O
|	O
|	O
p	O
(	O
z	O
)	O
)	O
+	O
log	O
p	O
π	O
(	O
y	O
(	O
t	O
)	O
)	O
−	O
log	O
q	O
φ	O
(	O
y	O
(	O
t	O
)	O
|	O
x	O
(	O
t	O
)	O
)	O
]	O
=	O
L	O
u	O
(	O
x	O
(	O
t	O
)	O
|	O
x	O
(	O
s	O
)	O
)	O
(	O
3	O
)	O
While	O
the	O
unsupervised	O
objective	O
is	O
trained	O
by	O
maximizing	O
the	O
following	O
variational	O
lower	O
bound	O
U	O
(	O
x	O
)	O
on	O
the	O
objective	O
for	O
unlabeled	O
data	O
:	O
log	O
p	O
θ	B-HyperparameterName
(	O
x	O
)	O
≥	O
E	O
(	O
y	O
,	O
z	O
)	O
∼q	O
φ	O
(	O
y	O
,	O
z	O
|	O
x	O
)	O
log	O
p	O
θ	B-HyperparameterName
(	O
x	O
,	O
y	O
,	O
z	O
)	O
q	O
φ	O
(	O
y	O
,	O
z	O
|	O
x	O
)	O
=	O
E	O
y∼q	O
φ	O
(	O
y	O
|	O
x	O
)	O
[	O
E	O
z∼q	O
φ	O
(	O
z	O
|	O
x	O
)	O
[	O
log	O
p	O
θ	B-HyperparameterName
(	O
x	O
|	O
z	O
,	O
y	O
)	O
]	O
−	O
KL	O
(	O
q	O
φ	O
(	O
z	O
|	O
x	O
)	O
|	O
|	O
p	O
(	O
z	O
)	O
)	O
+	O
log	O
p	O
π	O
(	O
y	O
)	O
−	O
log	O
q	O
φ	O
(	O
y	O
|	O
x	O
)	O
]	O
=	O
U	O
(	O
x	O
)	O
(	O
4	O
)	O
Note	O
that	O
when	O
labels	O
are	O
not	O
observed	O
,	O
the	O
inference	O
model	O
q	O
φ	O
(	O
y	O
|	O
x	O
)	O
has	O
the	O
form	O
of	O
a	O
discriminative	O
classifier	O
,	O
thus	O
we	O
can	O
use	O
observed	O
labels	O
as	O
the	O
supervision	O
signal	O
to	O
learn	O
a	O
better	O
classifier	O
.	O
In	O
this	O
case	O
we	O
also	O
minimize	O
the	O
following	O
cross	O
entropy	O
as	O
the	O
classification	O
loss	B-MetricName
:	O
D	O
(	O
x	O
,	O
y	O
)	O
=	O
E	O
(	O
x	O
,	O
y	O
)	O
∼p	O
l	O
(	O
x	O
,	O
y	O
)	O
[	O
−	O
log	O
q	O
φ	O
(	O
y	O
|	O
x	O
)	O
]	O
(	O
5	O
)	O
where	O
p	O
l	O
(	O
x	O
,	O
y	O
)	O
is	O
the	O
distribution	O
of	O
labeled	O
data	O
.	O
To	O
sum	O
up	O
,	O
the	O
semi	O
-	O
supervised	O
model	O
(	O
Semisup	O
)	O
is	O
trained	O
to	O
maximize	O
the	O
variational	O
lower	O
bounds	O
and	O
minimize	O
the	O
classification	O
crossentropy	O
error	O
of	O
5	O
.	O
L	O
(	O
x	O
(	O
s	O
)	O
,	O
x	O
(	O
t	O
)	O
,	O
y	O
(	O
t	O
)	O
,	O
x	O
)	O
=	O
α	B-HyperparameterName
U	O
(	O
x	O
)	O
+	O
L	O
u	O
(	O
x	O
(	O
s	O
)	O
|	O
x	O
(	O
t	O
)	O
)	O
+	O
L	O
l	O
(	O
x	O
(	O
t	O
)	O
,	O
y	O
(	O
t	O
)	O
|	O
x	O
(	O
s	O
)	O
)	O
−	O
D	O
(	O
x	O
(	O
t	O
)	O
,	O
y	O
(	O
t	O
)	O
)	O
(	O
6	O
)	O
The	O
weight	O
α	B-HyperparameterName
controls	O
the	O
relative	O
weight	O
between	O
the	O
loss	B-MetricName
from	O
unlabeled	O
data	O
and	O
labeled	O
data	O
.	O
3	O
Learning	O
MSVED	O

We	O
observe	O
that	O
with	O
the	O
vanilla	O
implementation	O
the	O
KL	O
cost	O
quickly	O
decreases	O
to	O
near	O
zero	O
,	O
setting	O
q	O
φ	O
(	O
z	O
|	O
x	O
)	O
equal	O
to	O
standard	O
normal	O
distribution	O
.	O
In	O
this	O
case	O
,	O
the	O
RNN	O
decoder	O
can	O
easily	O
degenerate	O
into	O
an	O
RNN	O
language	O
model	O
.	O
Hence	O
,	O
the	O
latent	O
variables	O
are	O
ignored	O
by	O
the	O
decoder	O
and	O
can	O
not	O
encode	O
any	O
useful	O
information	O
.	O
The	O
latent	O
variable	O
z	O
learns	O
an	O
undesirable	O
distribution	O
that	O
coincides	O
with	O
the	O
imposed	O
prior	O
distribution	O
but	O
has	O
no	O
contribution	O
to	O
the	O
decoder	O
.	O
To	O
force	O
the	O
decoder	O
to	O
use	O
the	O
latent	O
variables	O
,	O
we	O
take	O
the	O
following	O
two	O
approaches	O
which	O
are	O
similar	O
to	O
Bowman	O
et	O
al	O
(	O
2016	O
)	O
.	O
KL	O
-	O
Divergence	O
Annealing	O
:	O
We	O
add	O
a	O
coefficient	O
λ	O
to	O
the	O
KL	O
cost	O
and	O
gradually	O
anneal	O
it	O
from	O
zero	O
to	O
a	O
predefined	O
threshold	O
λ	O
m	O
.	O
At	O
the	O
early	O
stage	O
of	O
training	O
,	O
we	O
set	O
λ	O
to	O
be	O
zero	O
and	O
let	O
the	O
model	O
first	O
figure	O
out	O
how	O
to	O
project	O
the	O
representation	O
of	O
the	O
source	O
sequence	O
to	O
a	O
roughly	O
right	O
point	O
in	O
the	O
space	O
and	O
then	O
regularize	O
it	O
with	O
the	O
KL	O
cost	O
.	O
This	O
technique	O
can	O
also	O
be	O
seen	O
in	O
(	O
Kočiskỳ	O
et	O
al	O
,	O
2016	O
;	O
Miao	O
and	O
Blunsom	O
,	O
2016	O
)	O
.	O
Input	O
Dropout	B-MethodName
in	O
the	O
Decoder	O
:	O
Besides	O
annealing	O
the	O
KL	O
cost	O
,	O
we	O
also	O
randomly	O
drop	O
out	O
the	O
input	O
token	O
with	O
a	O
probability	O
of	O
β	B-HyperparameterName
at	O
each	O
time	O
step	O
of	O
the	O
decoder	O
during	O
learning	O
.	O
The	O
previous	O
ground	O
-	O
truth	O
token	O
embedding	O
is	O
replaced	O
with	O
a	O
zero	O
vector	O
when	O
dropped	O
.	O
In	O
this	O
way	O
,	O
the	O
RNN	O
decoder	O
could	O
not	O
fully	O
rely	O
on	O
the	O
ground	O
-	O
truth	O
previous	O
token	O
,	O
which	O
ensures	O
that	O
the	O
decoder	O
uses	O
information	O
encoded	O
in	O
the	O
latent	O
variables	O
.	O
k	O
a	O
l	O
b	O
⌃	O
(	O
x	O
)	O
µ	O
(	O
x	O
)	O
✏	O
⇠	O
N	O
(	O
0	B-DatasetName
,	O
1	O
)	O
z	O
<	O
w	O
>	O
k	O
k	O
ä	O
+	O
y	O
T	O
1	O
y	O
T	O
2	O
y	O
T	O
3	O
y	O
T	O
4	O
....	O
......	O
k	O
a	O
l	O
b	O
⌃	O
(	O
x	O
)	O
µ	O
(	O
x	O
)	O
✏	O
⇠	O
N	O
(	O
0	B-DatasetName
,	O
1	O
)	O
z	O
<	O
w	O
>	O
k	O
k	O
a	O
Multinomial	O
Sampling	O
+	O
......	O
y12	O

The	O
overall	O
model	O
architecture	O
is	O
shown	O
in	O
Fig	O
.	O
1	O
.	O
Each	O
character	O
and	O
each	O
label	O
is	O
associated	O
with	O
a	O
continuous	O
vector	O
.	O
We	O
employ	O
Gated	O
Recurrent	O
Units	O
(	O
GRUs	O
)	O
for	O
the	O
encoder	O
and	O
decoder	O
.	O
We	O
use	O
only	O
single	O
directional	O
GRUs	O
as	O
the	O
encoder	O
for	O
the	O
input	O
word	O
x	O
(	O
s	O
)	O
.	O
u	O
is	O
the	O
hidden	O
representation	O
of	O
x	O
(	O
s	O
)	O
which	O
is	O
the	O
last	O
hidden	O
state	O
of	O
GRUs	O
.	O
and	O
is	O
used	O
as	O
the	O
input	O
for	O
the	O
inference	O
model	O
on	O
z.	O
We	O
represent	O
µ	O
(	O
u	O
)	O
and	O
σ	O
2	O
(	O
u	O
)	O
as	O
MLPs	O
and	O
sample	O
z	O
from	O
N	O
(	O
µ	O
(	O
u	O
)	O
,	O
diag	O
(	O
σ	O
2	O
(	O
u	O
)	O
)	O
)	O
,	O
using	O
z	O
=	O
µ	O
+	O
σ	O
,	O
where	O
∼	O
N	O
(	O
0	B-DatasetName
,	O
I	O
)	O
.	O
Similarly	O
,	O
we	O
can	O
obtain	O
the	O
hidden	O
representation	O
of	O
(	O
t	O
)	O
and	O
use	O
this	O
as	O
input	O
to	O
the	O
inference	O
model	O
on	O
each	O
label	O
y	O
x	O
(	O
t	O
)	O
i	O
,	O
which	O
is	O
also	O
an	O
MLP	B-DatasetName
following	O
a	O
softmax	B-MethodName
layer	O
to	O
generate	O
the	O
categorical	O
probabilities	O
of	O
target	O
labels	O
.	O
Other	O
experimental	O
setups	O
:	O
We	O
apply	O
temperature	O
annealing	O
in	O
the	O
Gumble	O
-	O
Softmax	B-MethodName
with	O
the	O
scheme	O
max	O
(	O
0.5	O
,	O
exp	O
(	O
−3e	O
−	O
5	O
t	O
)	O
)	O
every	O
2000	O
updates	O
where	O
t	O
is	O
the	O
update	O
steps	O
.	O
We	O
observe	O
that	O
our	O
model	O
is	O
not	O
sensitive	O
to	O
the	O
temperature	O
in	O
this	O
task	O
.	O
All	O
hyperparameters	O
are	O
tuned	O
on	O
the	O
validation	O
set	O
,	O
and	O
include	O
the	O
following	O
:	O
For	O
KL	O
cost	O
annealing	O
,	O
λ	O
m	O
is	O
set	O
to	O
be	O
0.2	O
for	O
all	O
language	O
settings	O
.	O
For	O
character	O
drop	O
-	O
out	O
at	O
the	O
decoder	O
,	O
we	O
empirically	O
set	O
β	B-HyperparameterName
to	O
be	O
0.4	O
for	O
all	O
languages	O
.	O
We	O
set	O
the	O
dimension	O
of	O
character	O
embeddings	O
to	O
be	O
300	O
,	O
tag	O
label	O
embeddings	O
to	O
be	O
200	O
,	O
RNN	O
hidden	O
state	O
to	O
be	O
256	O
,	O
and	O
latent	O
variable	O
z	O
to	O
be	O
150	O
or	O
100	O
.	O
We	O
set	O
α	B-HyperparameterName
the	O
weight	O
for	O
the	O
unsupervised	O
loss	B-MetricName
to	O
be	O
0.8	O
.	O
We	O
train	O
the	O
model	O
with	O
Adadelta	B-MethodName
(	O
Zeiler	O
,	O
2012	O
)	O
and	O
use	O
early	O
-	O
stop	O
with	O
a	O
patience	O
of	O
5	O
.	O
Our	O
system	O
is	O
an	O
ensemble	O
of	O
five	O
models	O
and	O
the	O
probability	O
vector	O
at	O
each	O
time	O
step	O
is	O
obtained	O
by	O
averaging	O
the	O
output	O
probabilities	O
from	O
each	O
model	O
5	O
Experiments	O

The	O
results	O
on	O
the	O
dev	O
and	O
test	O
data	O
of	O
the	O
52	O
languages	O
are	O
presented	O
in	O
1	O
.	O
We	O
obtain	O
a	O
generation	O
accuracy	B-MetricName
above	O
80	O
%	O
over	O
more	O
than	O
25	O
%	O
languages	O
and	O
an	O
average	O
of	O
87.2	O
%	O
for	O
both	O
dev	O
and	O
test	O
data	O
.	O
The	O
generation	O
accuracy	B-MetricName
is	O
almost	O
consistent	O
on	O
the	O
dev	O
and	O
test	O
data	O
except	O
that	O
the	O
test	O
data	O
accuracy	B-MetricName
of	O
Scottish	O
-	O
Gaelic	O
drops	O
by	O
near	O
21	O
%	O
.	O
We	O
find	O
that	O
only	O
a	O
medium	O
volume	O
of	O
training	O
data	O
is	O
provided	O
for	O
Scottish	O
-	O
Gaelic	O
.	O
This	O
may	O
be	O
the	O
reason	O
why	O
the	O
model	O
trained	O
for	O
Scottish	O
-	O
Gaelic	O
can	O
not	O
generalize	O
as	O
well	O
as	O
other	O
languages	O
.	O
We	O
do	O
not	O
tune	O
the	O
hyper	O
-	O
parameters	O
for	O
each	O
language	O
manually	O
.	O
However	O
,	O
we	O
test	O
on	O
different	O
dimensions	O
for	O
the	O
continuous	O
latent	O
variables	O
.	O
The	O
dimension	O
size	O
we	O
have	O
used	O
included	O
100	O
and	O
150	O
.	O
And	O
we	O
observe	O
significant	O
improvement	O
by	O
using	O
a	O
larger	O
dimension	O
size	O
of	O
latent	O
variables	O
over	O
a	O
portion	O
of	O
languages	O
including	O
Faroese	O
,	O
Lithuanian	O
,	O
Navajo	O
,	O
Scottish	O
-	O
gaelic	O
,	O
Northern	O
-	O
sami	O
,	O
Slovene	O
,	O
Sorani	O
,	O
Slovak	O
.	O
However	O
,	O
we	O
also	O
observe	O
that	O
for	O
some	O
languages	O
including	O
Finnish	O
,	O
German	O
,	O
French	O
,	O
etc	O
,	O
the	O
performance	O
drops	O
signficantly	O
after	O
increasing	O
the	O
size	O
of	O
continuous	O
latent	O
variable	O
dimension	O
.	O
This	O
indicates	O
that	O
for	O
different	O
languages	O
,	O
the	O
continuous	O
space	O
required	O
to	O
encode	O
the	O
lemma	B-DatasetName
and	O
inflected	O
information	O
varies	O
from	O
language	O
to	O
language	O
.	O
We	O
will	O
further	O
investigate	O
this	O
in	O
the	O
future	O
work	O
.	O

Wiki	O
Data	O
While	O
our	O
performance	O
was	O
reasonable	O
,	O
it	O
was	O
not	O
as	O
good	O
as	O
that	O
presented	O
in	O
our	O
previous	O
work	O
(	O
Zhou	O
and	O
Neubig	O
,	O
2017	O
)	O
,	O
nor	O
was	O
it	O
competitive	O
with	O
the	O
highest	O
-	O
scoring	O
models	O
on	O
the	O
shared	O
task	O
.	O
In	O
order	O
to	O
examine	O
the	O
reason	O
for	O
this	O
,	O
we	O
performed	O
several	O
ablations	O
,	O
the	O
results	O
of	O
which	O
are	O
presented	O
in	O
Tab	O
.	O
3	O
First	O
,	O
we	O
first	O
examined	O
the	O
effects	O
of	O
data	B-TaskName
augmentation	I-TaskName
and	O
Wiki	O
Data	O
for	O
semi	O
-	O
supervised	O
learning	O
on	O
the	O
performance	O
of	O
our	O
model	O
.	O
By	O
removing	O
the	O
augmented	O
data	O
from	O
the	O
training	O
set	O
,	O
we	O
observe	O
a	O
large	O
gain	O
in	O
the	O
generation	O
accuracy	B-MetricName
.	O
Besides	O
,	O
we	O
find	O
that	O
Wiki	O
Data	O
for	O
semisupervised	O
learning	O
does	O
n't	O
help	O
much	O
to	O
increase	O
the	O
model	O
's	O
performance	O
.	O
The	O
reasons	O
for	O
this	O
will	O
be	O
examined	O
further	O
in	O
the	O
following	O
section	O
.	O
We	O
additionally	O
reimplemented	O
a	O
vanilla	O
encoder	O
-	O
decoder	O
model	O
with	O
attention	O
that	O
concatenates	O
the	O
input	O
characters	O
and	O
target	O
word	O
tags	O
together	O
with	O
a	O
special	O
token	O
in	O
the	O
middle	O
as	O
the	O
new	O
input	O
sequence	O
to	O
the	O
encoder	O
(	O
Kann	O
and	O
Schütze	O
,	O
2016	O
)	O
.	O
The	O
results	O
show	O
that	O
the	O
vanilla	O
encoder	O
-	O
decoder	O
works	O
better	O
than	O
our	O
model	O
in	O
some	O
cases	O
.	O
We	O
suspect	O
that	O
since	O
task	O
1	O
is	O
purely	O
an	O
inflection	O
task	O
and	O
because	O
semi	O
-	O
supervised	O
learning	O
did	O
not	O
provide	O
a	O
particularly	O
large	O
benefit	O
,	O
a	O
simpler	O
model	O
that	O
utilizes	O
attention	O
may	O
be	O
sufficient	O
.	O
This	O
is	O
in	O
contrast	O
to	O
our	O
previous	O
findings	O
,	O
where	O
semi	O
-	O
supervised	O
learning	O
was	O
highly	O
effective	O
,	O
and	O
the	O
proposed	O
model	O
out	O
-	O
performed	O
the	O
simpler	O
attention	O
-	O
based	O
baseline	O
.	O

Autoregressive	O
generation	O
(	O
AG	O
)	O
models	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
wide	O
range	O
of	O
text	B-TaskName
generation	I-TaskName
tasks	O
,	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
and	O
text	B-TaskName
summarization	I-TaskName
(	O
Rush	O
et	O
al	O
,	O
2015	O
)	O
.	O
Such	O
models	O
generate	O
a	O
token	O
sequence	O
in	O
a	O
left	O
-	O
to	O
-	O
right	O
,	O
token	O
-	O
by	O
-	O
token	O
fashion	O
.	O
The	O
prediction	O
for	O
the	O
next	O
token	O
is	O
conditioned	O
on	O
all	O
previously	O
generated	O
tokens	O
.	O
This	O
characteristic	O
makes	O
it	O
impossible	O
to	O
parallelize	O
the	O
computational	O
overhead	O
for	O
token	O
predictions	O
in	O
different	O
positions	O
,	O
which	O
leads	O
to	O
a	O
relatively	O
high	O
latency	O
in	O
inference	O
.	O
On	O
the	O
other	O
hand	O
,	O
non	O
-	O
autoregressive	O
generation	O
(	O
NAG	O
)	O
models	O
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
have	O
emerged	O
as	O
a	O
promising	O
alternative	O
due	O
to	O
their	O
fast	O
inference	O
speed	O
.	O
NAG	O
models	O
omit	O
the	O
sequential	O
dependencies	O
within	O
the	O
output	O
-	O
side	O
sequence	O
and	O
predict	O
tokens	O
in	O
all	O
positions	O
simultaneously	O
once	O
the	O
output	O
length	O
has	O
been	O
determined	O
beforehand	O
.	O
While	O
NAG	O
models	O
enjoy	O
full	O
parallelism	O
and	O
faster	O
inference	O
,	O
the	O
generation	O
quality	O
of	O
NAG	O
models	O
often	O
lags	O
behind	O
their	O
autoregressive	O
counterparts	O
.	O
In	O
this	O
work	O
,	O
we	O
explore	O
the	O
potential	O
of	O
largescale	O
pre	O
-	O
trained	O
language	O
models	O
for	O
improving	O
the	O
performance	O
of	O
non	O
-	O
autoregressive	O
generation	O
.	O
Specifically	O
,	O
we	O
utilize	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
as	O
the	O
backbone	O
for	O
NAG	O
modelling	O
and	O
extend	O
the	O
architecture	O
of	O
BERT	B-MethodName
with	O
a	O
CRF	B-MethodName
output	O
layer	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
;	O
for	O
better	O
capturing	O
the	O
output	O
-	O
side	O
dependencies	O
.	O
In	O
addition	O
,	O
we	O
analyze	O
two	O
significant	O
limitations	O
that	O
NAG	O
models	O
currently	O
suffer	O
from	O
:	O
(	O
1	O
)	O
the	O
inflexibility	O
of	O
prefixed	O
output	O
length	O
,	O
and	O
(	O
2	O
)	O
the	O
conditional	O
independence	O
of	O
individual	O
token	O
predictions	O
.	O
Accordingly	O
,	O
we	O
devise	O
two	O
solutions	O
to	O
these	O
two	O
problems	O
.	O
First	O
,	O
prior	O
NAG	O
models	O
require	O
the	O
output	O
length	O
to	O
be	O
determined	O
before	O
token	O
generation	O
,	O
thus	O
an	O
extra	O
module	O
for	O
output	O
length	O
prediction	O
is	O
always	O
required	O
.	O
Nevertheless	O
,	O
the	O
most	O
likely	O
length	O
from	O
the	O
prediction	O
module	O
is	O
not	O
necessarily	O
the	O
best	O
-	O
suited	O
one	O
for	O
the	O
token	O
generation	O
model	O
.	O
To	O
this	O
end	O
,	O
previous	O
works	O
(	O
Gu	O
et	O
al	O
,	O
2018	O
;	O
Ma	O
et	O
al	O
,	O
2019	O
)	O
usually	O
rely	O
on	O
length	O
-	O
parallel	O
decoding	O
(	O
LPD	O
)	O
for	O
performance	O
enhancement	O
;	O
that	O
is	O
,	O
generating	O
and	O
re	O
-	O
ranking	O
the	O
results	O
from	O
different	O
output	O
length	O
candidates	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
simple	O
and	O
elegant	O
decoding	O
mechanism	O
that	O
lets	O
the	O
model	O
determine	O
the	O
output	O
length	O
on	O
-	O
the	O
-	O
fly	O
.	O
Specifically	O
,	O
our	O
model	O
dynamically	O
adjusts	O
the	O
output	O
sequence	O
length	O
via	O
emitting	O
an	O
[	O
eos	O
]	O
token	O
at	O
any	O
output	O
position	O
to	O
indicate	O
the	O
ending	O
of	O
the	O
generated	O
sequence	O
.	O
Therefore	O
,	O
we	O
can	O
avoid	O
the	O
additional	O
efforts	O
of	O
output	O
length	O
prediction	O
and	O
results	O
re	O
-	O
ranking	O
.	O
Second	O
,	O
most	O
existing	O
NAG	O
models	O
assume	O
the	O
token	O
predictions	O
in	O
different	O
positions	O
are	O
conditionally	O
independent	O
.	O
As	O
a	O
consequence	O
,	O
they	O
often	O
tend	O
to	O
generate	O
results	O
that	O
are	O
ungrammatical	O
with	O
repetitions	O
(	O
Wang	O
et	O
al	O
,	O
2019b	O
)	O
.	O
To	O
alleviate	O
this	O
problem	O
,	O
we	O
propose	O
a	O
context	O
-	O
aware	O
learning	O
objective	O
which	O
impels	O
the	O
model	O
to	O
output	O
different	O
tokens	O
at	O
adjacent	O
positions	O
,	O
thereby	O
reducing	O
the	O
possibility	O
of	O
repetitive	O
generation	O
.	O
Furthermore	O
,	O
for	O
tasks	O
like	O
text	B-TaskName
summarization	I-TaskName
,	O
the	O
output	O
sequence	O
(	O
summary	O
)	O
is	O
known	O
to	O
be	O
shorter	O
than	O
the	O
source	O
sequence	O
(	O
article	O
)	O
.	O
In	O
such	O
cases	O
,	O
to	O
further	O
improve	O
the	O
model	O
's	O
inference	O
efficiency	O
,	O
we	O
introduce	O
a	O
new	O
ratio	O
-	O
first	O
decoding	O
strategy	O
.	O
Specifically	O
,	O
instead	O
of	O
performing	O
inference	O
on	O
all	O
source	O
-	O
side	O
hidden	O
states	O
,	O
ratio	O
-	O
first	O
generates	O
the	O
result	O
only	O
based	O
on	O
a	O
subset	O
of	O
source	O
hidden	O
states	O
.	O
The	O
subset	O
size	O
is	O
jointly	O
determined	O
by	O
the	O
source	O
length	O
T	O
and	O
a	O
predefined	O
ratio	O
α	B-HyperparameterName
that	O
is	O
set	O
based	O
on	O
our	O
prior	O
knowledge	O
from	O
the	O
data	O
statistics	O
.	O
In	O
the	O
experiments	O
,	O
we	O
show	O
that	O
ratio	O
-	O
first	O
can	O
significantly	O
improve	O
the	O
inference	O
speed	O
while	O
maintaining	O
the	O
generation	O
quality	O
.	O
We	O
evaluate	O
the	O
proposed	O
model	O
on	O
three	O
typical	O
text	B-TaskName
generation	I-TaskName
tasks	O
,	O
including	O
text	B-TaskName
summarization	I-TaskName
,	O
sentence	B-DatasetName
compression	I-DatasetName
and	O
machine	B-TaskName
translation	I-TaskName
.	O
Experimental	O
results	O
show	O
that	O
our	O
model	O
significantly	O
outperforms	O
many	O
strong	O
non	O
-	O
autoregressive	O
baselines	O
,	O
and	O
even	O
performs	O
competitively	O
with	O
several	O
strong	O
autoregressive	O
models	O
.	O
In	O
addition	O
,	O
we	O
conduct	O
extensive	O
analysis	O
experiments	O
to	O
study	O
the	O
effect	O
of	O
individual	O
proposed	O
components	O
.	O
In	O
summary	O
,	O
our	O
contributions	O
are	O
:	O
(	O
1	O
)	O
We	O
propose	O
a	O
novel	O
framework	O
that	O
utilizes	O
BERT	B-MethodName
for	O
text	B-TaskName
generation	I-TaskName
under	O
the	O
non	O
-	O
autoregressive	O
generation	O
paradigm	O
;	O
(	O
2	O
)	O
We	O
propose	O
a	O
decoding	O
mechanism	O
that	O
allows	O
the	O
model	O
to	O
dynamically	O
determine	O
the	O
output	O
length	O
,	O
and	O
a	O
new	O
context	O
-	O
aware	O
learning	O
objective	O
that	O
reduces	O
errors	O
stemming	O
from	O
the	O
output	O
-	O
side	O
conditional	O
independence	O
assumption	O
;	O
(	O
3	O
)	O
We	O
introduce	O
a	O
ratio	O
-	O
first	O
decoding	O
strategy	O
that	O
further	O
improve	O
the	O
model	O
's	O
inference	O
efficiency	O
.	O

We	O
note	O
that	O
the	O
outputs	O
of	O
BERT	B-MethodName
can	O
be	O
divided	O
into	O
two	O
subsets	O
.	O
The	O
first	O
subset	O
ranges	O
from	O
the	O
beginning	O
to	O
the	O
position	O
where	O
the	O
first	O
[	O
eos	O
]	O
is	O
emitted	O
,	O
and	O
the	O
second	O
subset	O
is	O
the	O
rest	O
.	O
For	O
example	O
,	O
in	O
Figure	O
2	O
,	O
the	O
first	O
subset	O
are	O
those	O
corresponding	O
to	O
the	O
output	O
sequence	O
"	O
y	O
(	O
1	O
)	O
y	O
(	O
2	O
)	O
y	O
(	O
3	O
)	O
y	O
(	O
4	O
)	O
[	O
eos	O
]	O
"	O
.	O
As	O
for	O
the	O
second	O
part	O
,	O
we	O
can	O
see	O
that	O
it	O
has	O
little	O
effect	O
on	O
the	O
final	O
output	O
and	O
removing	O
it	O
should	O
not	O
change	O
the	O
result	O
.	O
This	O
indicates	O
that	O
it	O
suffices	O
to	O
only	O
consider	O
the	O
beginning	O
part	O
of	O
BERT	B-MethodName
outputs	O
for	O
improving	O
the	O
inference	O
speed	O
.	O
Especially	O
,	O
for	O
tasks	O
like	O
summarization	B-TaskName
where	O
the	O
target	O
is	O
known	O
to	O
be	O
shorter	O
than	O
the	O
source	O
sequence	O
,	O
we	O
are	O
safe	O
to	O
only	O
use	O
the	O
first	O
[	O
α	B-HyperparameterName
T	O
]	O
outputs	O
of	O
BERT	B-MethodName
to	O
perform	O
inference	O
.	O
Here	O
T	O
denotes	O
the	O
source	O
length	O
,	O
α	B-HyperparameterName
(	O
0.0	O
,	O
1.0	O
)	O
is	O
set	O
based	O
on	O
the	O
data	O
statistics	O
and	O
[	O
]	O
is	O
the	O
integer	O
rounding	O
operation	O
.	O
Formally	O
,	O
given	O
the	O
source	O
sequence	O
X	O
,	O
the	O
ratio	O
-	O
first	O
decoding	O
is	O
defined	O
as	O
Y	O
=	O
arg	O
max	O
Y	O
F	O
(	O
X	O
,	O
Y	O
,	O
α	B-HyperparameterName
)	O
,	O
=	O
arg	O
max	O
Y	O
{	O
[	O
α	B-HyperparameterName
T	O
]	O
i=1	O
Φ	O
y	O
i	O
(	O
h	O
i	O
)	O
+	O
[	O
α	B-HyperparameterName
T	O
]	O
i=2	O
t	O
(	O
y	O
i−1	O
,	O
y	O
i	O
)	O
}	O
.	O
(	O
10	O
)	O
When	O
α	B-HyperparameterName
=	O
1.0	O
,	O
ratio	O
-	O
first	O
degenerates	O
to	O
the	O
standard	O
decoding	O
strategy	O
in	O
CRF	B-MethodName
-	O
based	O
models	O
.	O
It	O
should	O
be	O
noted	O
that	O
,	O
[	O
α	B-HyperparameterName
T	O
]	O
only	O
constrains	O
the	O
maximum	O
length	O
of	O
the	O
generated	O
result	O
,	O
and	O
the	O
actual	O
output	O
length	O
(	O
after	O
removing	O
the	O
generated	O
[	O
eos	O
]	O
tokens	O
)	O
is	O
still	O
decided	O
by	O
the	O
model	O
itself	O
.	O
In	O
the	O
experiment	O
section	O
,	O
we	O
demonstrate	O
that	O
ratio	O
-	O
first	O
can	O
notably	O
improve	O
the	O
inference	O
speed	O
whilst	O
maintaining	O
the	O
generation	O
quality	O
.	O

Due	O
to	O
the	O
conditional	O
independence	O
approximation	O
on	O
output	O
tokens	O
,	O
NAG	O
models	O
often	O
tend	O
to	O
generate	O
repeated	O
tokens	O
(	O
Wang	O
et	O
al	O
,	O
2019b	O
)	O
.	O
One	O
way	O
to	O
alleviate	O
this	O
problem	O
is	O
to	O
introduce	O
implicit	O
dependencies	O
on	O
the	O
output	O
side	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
to	O
use	O
the	O
unlikelihood	O
formulation	O
of	O
Welleck	O
et	O
al	O
(	O
2020	O
)	O
in	O
the	O
context	O
of	O
NAG	O
,	O
where	O
we	O
define	O
the	O
set	O
of	O
negative	O
candidate	O
as	O
the	O
surrounding	O
tokens	O
within	O
a	O
predefined	O
context	O
window	O
c.	O
Formally	O
,	O
given	O
the	O
source	O
sequence	O
X	O
and	O
the	O
target	O
sequence	O
Y	O
with	O
length	O
T	O
,	O
the	O
proposed	O
context	O
-	O
aware	O
objective	O
is	O
defined	O
as	O
:	O
L	O
CA	O
(	O
Y	O
|	O
X	O
)	O
=	O
−	O
T	O
i=1	O
{	O
log	O
p	O
θ	B-HyperparameterName
(	O
y	O
i	O
|	O
h	O
i	O
;	O
X	O
)	O
+	O
l	O
CA	O
(	O
i	O
)	O
}	O
,	O
l	O
CA	O
(	O
i	O
)	O
=	O
j	O
=	O
i+c	O
j	O
=	O
i−c	O
,	O
y	O
j	O
=	O
y	O
i	O
log	O
(	O
1.0	O
−	O
p	O
θ	B-HyperparameterName
(	O
y	O
j	O
|	O
h	O
i	O
;	O
X	O
)	O
)	O
,	O
(	O
11	O
)	O
where	O
h	O
i	O
is	O
the	O
model	O
output	O
state	O
at	O
position	O
i.	O
At	O
position	O
i	O
,	O
the	O
proposed	O
objective	O
maximizes	O
the	O
probability	O
of	O
token	O
y	O
i	O
while	O
minimizing	O
the	O
probabilities	O
of	O
the	O
surrounding	O
tokens	O
.	O
In	O
this	O
way	O
,	O
it	O
discourages	O
the	O
model	O
from	O
generating	O
repetitive	O
tokens	O
at	O
different	O
time	O
steps	O
.	O
The	O
overall	O
learning	O
objective	O
is	O
then	O
defined	O
as	O
L	O
CRF	B-MethodName
=	O
−	O
log	O
P	O
CRF	B-MethodName
(	O
Y	O
|	O
X	O
)	O
,	O
L	O
=	O
L	O
CRF	B-MethodName
+	O
λ	O
L	O
CA	O
,	O
(	O
12	O
)	O
where	O
λ	O
controls	O
the	O
importance	O
of	O
different	O
loss	B-MetricName
terms	O
and	O
P	O
CRF	B-MethodName
(	O
Y	O
|	O
X	O
)	O
is	O
described	O
in	O
Equation	O
(	O
8	O
)	O
.	O

We	O
implement	O
the	O
proposed	O
model	O
with	O
PyTorch	O
(	O
Paszke	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
BERT	B-MethodName
model	O
we	O
use	O
is	O
the	O
Huggingface	O
implementation	O
(	O
Wolf	O
et	O
al	O
,	O
2019	O
)	O
(	O
bert	O
-	O
base	O
-	O
uncased	O
)	O
.	O
To	O
approximate	O
the	O
transition	O
matrix	O
in	O
the	O
CRF	B-MethodName
layer	O
,	O
we	O
set	O
the	O
dimension	O
d	O
of	O
matrices	O
E	O
1	O
and	O
E	O
2	O
as	O
32	O
.	O
For	O
the	O
normalizing	O
factor	O
Z	O
(	O
X	O
)	O
,	O
we	O
set	O
the	O
predefined	O
beam	O
size	O
k	O
as	O
256	O
.	O
As	O
for	O
the	O
overall	O
learning	O
objective	O
,	O
we	O
set	O
the	O
window	O
size	O
c	O
as	O
3	O
and	O
λ	O
as	O
1.0	O
.	O
In	O
training	O
,	O
we	O
use	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
.	O
To	O
measure	O
the	O
relative	O
speedup	O
,	O
we	O
follow	O
the	O
standard	O
setup	O
which	O
runs	O
inference	O
for	O
each	O
individual	O
example	O
separately	O
.	O
The	O
model	O
's	O
inference	O
speed	O
is	O
computed	O
by	O
averaging	O
the	O
results	O
of	O
test	O
cases	O
.	O
For	O
a	O
fair	O
comparison	O
,	O
we	O
measure	O
the	O
inference	O
speed	O
of	O
all	O
models	O
on	O
the	O
same	O
platform	O
.	O

Text	B-TaskName
summarization	I-TaskName
aims	O
to	O
automatically	O
generate	O
a	O
compact	O
summary	O
that	O
retains	O
the	O
most	O
important	O
content	O
of	O
the	O
original	O
text	O
document	O
(	O
Nenkova	O
and	O
McKeown	O
,	O
2012	O
)	O
.	O
In	O
this	O
experiment	O
,	O
we	O
use	O
the	O
Gigawords	O
dataset	O
(	O
Rush	O
et	O
al	O
,	O
2015	O
)	O
as	O
our	O
benchmark	O
.	O
For	O
evaluation	O
,	O
standard	O
metrics	O
including	O
ROUGE	O
-	O
1	O
(	O
R	O
-	O
1	O
)	O
,	O
ROUGE	O
-	O
2	O
(	O
R	O
-	O
2	O
)	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
(	O
R	O
-	O
L	O
)	O
(	O
Lin	O
,	O
2004	O
)	O
are	O
reported	O
.	O
We	O
compare	O
our	O
model	O
with	O
several	O
representative	O
and	O
the	O
latest	O
NAG	O
models	O
,	O
including	O
NAG	O
-	O
NMT	O
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
,	O
NAR	B-DatasetName
-	O
REG	O
(	O
Wang	O
et	O
al	O
,	O
2019b	O
)	O
and	O
NAG	O
-	O
CRF	B-MethodName
.	O
Following	O
previous	O
works	O
,	O
during	O
training	O
,	O
we	O
train	O
a	O
length	O
predictor	O
to	O
predict	O
the	O
output	O
length	O
.	O
During	O
inference	O
,	O
for	O
each	O
NAG	O
baseline	O
,	O
we	O
adopt	O
the	O
length	O
-	O
parallel	O
decoding	O
strategy	O
(	O
LPD	O
-	O
k	O
)	O
,	O
that	O
is	O
,	O
generating	O
k	O
results	O
using	O
the	O
top	O
-	O
k	O
possible	O
output	O
length	O
predictions	O
from	O
the	O
length	O
predictor	O
.	O
The	O
results	O
are	O
then	O
re	O
-	O
ranked	O
by	O
a	O
transformer	O
model	O
to	O
get	O
the	O
final	O
ouput	O
.	O
In	O
the	O
experiment	O
,	O
we	O
report	O
the	O
results	O
of	O
different	O
NAG	O
baselines	O
using	O
LPD	O
-	O
9	O
decoding	O
.	O
In	O
addition	O
,	O
to	O
better	O
examine	O
the	O
effect	O
of	O
using	O
BERT	B-MethodName
in	O
NAG	O
models	O
,	O
we	O
add	O
a	O
BNAG	O
-	O
CRF	B-MethodName
baseline	O
which	O
adopts	O
the	O
same	O
structure	O
of	O
the	O
NAG	O
-	O
CRF	B-MethodName
model	O
but	O
using	O
BERT	B-MethodName
as	O
the	O
encoder	O
.	O
We	O
also	O
compare	O
our	O
model	O
with	O
several	O
strong	O
autoregressive	O
models	O
,	O
which	O
are	O
Luong	O
-	O
NMT	O
(	O
Luong	O
et	O
al	O
,	O
2015	O
)	O
,	O
Pointer	O
-	O
Generator	O
(	O
See	O
et	O
al	O
,	O
2017	O
)	O
,	O
DRGD	O
(	O
Li	O
et	O
al	O
,	O
2017	O
)	O
and	O
Concept	O
Pointer	O
(	O
Wang	O
et	O
al	O
,	O
2019a	O
)	O
.	O
To	O
measure	O
the	O
relative	O
inference	O
speedup	O
,	O
we	O
include	O
transformer	O
as	O
a	O
baseline	O
model	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
1	O
,	O
from	O
which	O
we	O
can	O
see	O
that	O
,	O
by	O
using	O
length	O
-	O
parallel	O
decoding	O
,	O
the	O
performance	O
of	O
all	O
NAG	O
baselines	O
can	O
be	O
notably	O
improved	O
.	O
However	O
,	O
such	O
procedure	O
significantly	O
increases	O
the	O
inference	O
latency	O
.	O
In	O
contrast	O
,	O
our	O
model	O
can	O
self	O
-	O
determine	O
the	O
output	O
length	O
without	O
any	O
re	O
-	O
ranking	O
process	O
.	O
As	O
shown	O
in	O
the	O
results	O
,	O
our	O
model	O
outperforms	O
the	O
best	O
NAG	O
baseline	O
(	O
with	O
LPD	O
)	O
and	O
achieves	O
performances	O
that	O
are	O
comparable	O
with	O
several	O
strong	O
AG	O
models	O
.	O
Comparing	O
the	O
results	O
of	O
BNAG	O
-	O
CRF	B-MethodName
and	O
NAG	O
-	O
CRF	B-MethodName
,	O
we	O
can	O
see	O
that	O
incorporating	O
BERT	B-MethodName
as	O
encoder	O
helps	O
to	O
improve	O
the	O
model	O
performance	O
.	O
Nonetheless	O
,	O
our	O
model	O
still	O
outperforms	O
BNAG	O
-	O
CRF	B-MethodName
with	O
LPD	O
-	O
9	O
decoding	O
.	O
This	O
is	O
because	O
the	O
dynamic	O
length	O
decoding	O
mechanism	O
allows	O
our	O
model	O
to	O
generate	O
results	O
with	O
optimal	O
length	O
,	O
leading	O
to	O
stronger	O
model	O
performances	O
.	O
Finally	O
,	O
we	O
analyze	O
the	O
proposed	O
ratio	O
-	O
first	O
decoding	O
.	O
From	O
the	O
results	O
,	O
we	O
observe	O
a	O
moderate	O
performance	O
drop	O
when	O
using	O
ratio	O
-	O
first	O
(	O
α	B-HyperparameterName
=	O
0.3	O
)	O
.	O
It	O
comes	O
from	O
the	O
fact	O
that	O
,	O
for	O
some	O
input	O
documents	O
with	O
length	O
T	O
,	O
the	O
reference	O
summary	O
is	O
longer	O
than	O
[	O
α	B-HyperparameterName
T	O
]	O
.	O
In	O
such	O
cases	O
,	O
ratio	O
-	O
first	O
fails	O
to	O
generate	O
the	O
complete	O
reference	O
summary	O
,	O
leading	O
to	O
the	O
drop	O
of	O
performance	O
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
can	O
see	O
that	O
,	O
ratio	O
-	O
first	O
can	O
notably	O
improve	O
the	O
inference	O
speedup	O
.	O
With	O
α	B-HyperparameterName
=	O
0.3	O
,	O
our	O
model	O
achieves	O
the	O
highest	O
inference	O
speedup	O
while	O
still	O
outperforms	O
all	O
compared	O
NAG	O
models	O
.	O

Sentence	B-DatasetName
compression	I-DatasetName
aims	O
at	O
compressing	O
a	O
long	O
sentence	O
into	O
a	O
short	O
one	O
by	O
deleting	O
redundant	O
words	O
.	O
In	O
this	O
experiment	O
,	O
we	O
use	O
the	O
Google	B-DatasetName
sentence	B-DatasetName
compression	I-DatasetName
dataset	O
(	O
Filippova	O
and	O
Altun	O
,	O
2013	O
)	O
as	O
our	O
benchmark	O
.	O
For	O
evaluation	O
,	O
we	O
use	O
the	O
standard	O
token	O
-	O
kept	O
-	O
F1	B-MetricName
(	O
F1	B-MetricName
)	O
score	O
.	O
In	O
addition	O
,	O
We	O
also	O
report	O
the	O
results	O
of	O
other	O
standard	O
metrics	O
including	O
ROUGE	O
-	O
1	O
,	O
ROUGE	O
-	O
2	O
and	O
ROUGE	O
-	O
L.	O
Models	O
F1	B-MetricName
R	O
-	O
1	O
R	O
-	O
2	O
R	O
-	O
L	O
We	O
compare	O
the	O
proposed	O
model	O
with	O
the	O
same	O
NAG	O
baselines	O
as	O
in	O
the	O
previous	O
experiment	O
.	O
We	O
also	O
compare	O
our	O
model	O
with	O
several	O
strong	O
autoregressive	O
models	O
,	O
including	O
Bi	O
-	O
LSTM	B-MethodName
-	O
Dep	O
(	O
Filippova	O
et	O
al	O
,	O
2015	O
)	O
,	O
Tagger	O
and	O
Tagger+ILP	O
,	O
HiSAN	O
-	O
Dep	O
and	O
HiSAN	O
(	O
Kamigaito	O
et	O
al	O
,	O
2018	O
)	O
.	O
To	O
measure	O
the	O
inference	O
speedup	O
,	O
we	O
include	O
transformer	O
as	O
a	O
baseline	O
model	O
.	O
The	O
results	O
are	O
presented	O
in	O
Table	O
2	O
,	O
from	O
which	O
we	O
see	O
that	O
our	O
model	O
outperforms	O
the	O
best	O
reported	O
NAG	O
baseline	O
(	O
with	O
LPD	O
)	O
in	O
terms	O
of	O
both	O
the	O
generation	O
quality	O
and	O
inference	O
speed	O
.	O
Comparing	O
with	O
the	O
strong	O
autoregressive	O
models	O
,	O
our	O
model	O
can	O
achieve	O
competitive	O
performance	O
with	O
a	O
over	O
8.42×	O
inference	O
speed	O
up	O
.	O
We	O
also	O
report	O
the	O
results	O
of	O
our	O
model	O
using	O
the	O
ratio	O
-	O
first	O
decoding	O
strategy	O
.	O
By	O
setting	O
α	B-HyperparameterName
as	O
0.7	O
,	O
it	O
achieves	O
a	O
10.00×	O
inference	O
speedup	O
while	O
still	O
outperforming	O
other	O
compared	O
NAG	O
baselines	O
.	O

Machine	B-TaskName
translation	I-TaskName
aims	O
at	O
translating	O
text	O
from	O
the	O
source	O
language	O
to	O
the	O
target	O
language	O
.	O
In	O
this	O
task	O
,	O
we	O
use	O
the	O
IWSLT14	O
German	O
-	O
to	O
-	O
English	O
(	O
DE	O
-	O
EN	O
)	O
dataset	O
as	O
our	O
benchmark	O
.	O
Following	O
previous	O
works	O
,	O
we	O
use	O
the	O
sequence	O
-	O
level	O
knowledge	B-MethodName
distillation	I-MethodName
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
during	O
training	O
.	O
For	O
evaluation	O
,	O
we	O
report	O
results	O
in	O
BLEU	B-MetricName
scores	O
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
.	O
In	O
this	O
experiment	O
,	O
we	O
use	O
the	O
BERT	B-MethodName
model	O
in	O
German	O
language	O
.	O
We	O
compare	O
our	O
model	O
with	O
a	O
range	O
of	O
strong	O
NAG	O
models	O
,	O
including	O
NAG	O
-	O
NMT	O
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
,	O
ENAG	O
-	O
E	O
and	O
ENAG	O
-	O
P	O
(	O
Guo	O
et	O
al	O
,	O
2019	O
)	O
,	O
NAG	O
-	O
REG	O
(	O
Wang	O
et	O
al	O
,	O
2019b	O
)	O
,	O
NAG	O
-	O
CRF	B-MethodName
and	O
BNAG	O
-	O
CRF	B-MethodName
.	O
For	O
each	O
NAG	O
baseline	O
,	O
we	O
also	O
report	O
the	O
results	O
using	O
LPD	O
-	O
9	O
decoding	O
.	O
In	O
addition	O
,	O
we	O
compare	O
our	O
model	O
with	O
several	O
strong	O
autoregressive	O
models	O
,	O
including	O
LSTM	B-MethodName
-	O
based	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
,	O
CNN	O
-	O
based	O
(	O
Gehring	O
et	O
al	O
,	O
2017	O
)	O
and	O
transformer	O
model	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
3	O
,	O
from	O
which	O
we	O
see	O
that	O
our	O
model	O
outperforms	O
the	O
best	O
NAG	O
baseline	O
(	O
with	O
LPD	O
)	O
in	O
terms	O
of	O
both	O
the	O
generation	O
quality	O
and	O
inference	O
speedup	O
.	O
Additionally	O
,	O
we	O
also	O
report	O
the	O
results	O
using	O
the	O
ratio	O
-	O
first	O
decoding	O
.	O
By	O
setting	O
α	B-HyperparameterName
as	O
0.8	O
,	O
the	O
inference	O
speedup	O
can	O
be	O
further	O
boosted	O
to	O
13.92×	O
while	O
the	O
generation	O
quality	O
is	O
still	O
higher	O
than	O
the	O
best	O
NAG	O
baseline	O
.	O

Context	O
-	O
Aware	O
Objective	O
In	O
this	O
part	O
,	O
we	O
study	O
the	O
effect	O
of	O
the	O
context	O
-	O
aware	O
objective	O
.	O
As	O
described	O
in	O
Equation	O
(	O
11	O
)	O
,	O
it	O
aims	O
at	O
alleviating	O
the	O
problem	O
of	O
repetitive	O
generation	O
.	O
To	O
give	O
a	O
quantitative	O
analysis	O
,	O
we	O
use	O
the	O
measurement	O
of	O
sentencelevel	O
repetition	O
(	O
Welleck	O
et	O
al	O
,	O
2020	O
)	O
to	O
compute	O
the	O
ratio	O
of	O
duplicate	O
n	O
-	O
grams	O
(	O
rep	O
-	O
n	O
)	O
in	O
the	O
generated	O
result	O
.	O
This	O
metric	O
is	O
defined	O
as	O
rep	O
-	O
n	O
(	O
Y	O
)	O
=	O
100	O
×	O
(	O
1.0	O
−	O
|	O
unique	O
n	O
-	O
grams	O
(	O
Y	O
)	O
|	O
|	O
n	O
-	O
grams	O
(	O
Y	O
)	O
|	O
)	O
.	O
(	O
13	O
)	O
For	O
each	O
generated	O
result	O
,	O
rep	O
-	O
n	O
is	O
0.0	O
when	O
it	O
has	O
no	O
repeating	O
n	O
-	O
grams	O
.	O
The	O
final	O
result	O
is	O
computed	O
by	O
averaging	O
over	O
the	O
entire	O
evaluation	O
set	O
.	O
We	O
conduct	O
experiments	O
on	O
Gigawords	O
dataset	O
to	O
evaluate	O
the	O
n	O
-	O
gram	O
repetitions	O
ranging	O
from	O
uni	O
-	O
gram	O
to	O
4	O
-	O
gram	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
5	O
,	O
where	O
w/o	O
CA	O
means	O
the	O
model	O
is	O
trained	O
without	O
using	O
context	O
-	O
aware	O
objective	O
and	O
R	O
-	O
L	O
denotes	O
the	O
model	O
's	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
score	O
.	O
Additionally	O
,	O
we	O
also	O
show	O
the	O
results	O
from	O
transformer	O
model	O
for	O
a	O
direct	O
comparison	O
.	O
Comparing	O
the	O
two	O
variants	O
of	O
our	O
model	O
,	O
we	O
see	O
that	O
training	O
with	O
context	O
-	O
aware	O
objective	O
leads	O
to	O
a	O
42	O
%	O
drop	O
on	O
rep	O
-	O
3	O
metric	O
(	O
0.427	O
vs	O
0.741	O
)	O
and	O
a	O
64	O
%	O
drop	O
on	O
rep	O
-	O
4	O
metric	O
(	O
0.106	O
vs	O
0.295	O
)	O
.	O
The	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
results	O
also	O
indicate	O
that	O
the	O
reduction	O
in	O
token	O
repetition	O
can	O
effectively	O
improve	O
the	O
model	O
generation	O
quality	O
.	O
Dynamic	O
Length	O
Determination	O
Next	O
,	O
we	O
examine	O
the	O
importance	O
of	O
the	O
model	O
's	O
ability	O
to	O
dynamically	O
determine	O
the	O
length	O
of	O
the	O
generated	O
output	O
.	O
To	O
this	O
end	O
,	O
we	O
train	O
another	O
model	O
variant	O
by	O
removing	O
the	O
two	O
[	O
eos	O
]	O
tokens	O
from	O
the	O
target	O
sequence	O
.	O
In	O
this	O
way	O
,	O
the	O
model	O
is	O
not	O
able	O
to	O
self	O
-	O
determine	O
the	O
output	O
length	O
throughout	O
the	O
generation	O
process	O
.	O
To	O
perform	O
inference	O
,	O
we	O
use	O
length	O
-	O
parallel	O
decoding	O
(	O
LPD	O
)	O
with	O
different	O
number	O
of	O
length	O
candidates	O
.	O
Formally	O
,	O
for	O
each	O
length	O
candidate	O
l	O
,	O
the	O
model	O
generates	O
the	O
resultỸ	O
as	O
Y	O
=	O
arg	O
max	O
Y	O
{	O
l	O
i=1	O
Φ	O
y	O
i	O
(	O
h	O
i	O
)	O
+	O
l	O
i=2	O
t	O
(	O
y	O
i−1	O
,	O
y	O
i	O
)	O
}	O
.	O
(	O
14	O
)	O
The	O
final	O
result	O
is	O
acquired	O
by	O
re	O
-	O
ranking	O
the	O
generated	O
results	O
with	O
a	O
transformer	O
model	O
.	O
We	O
conduct	O
experiments	O
on	O
the	O
IWSLT14	O
DE	O
-	O
EN	O
dataset	O
in	O
which	O
we	O
try	O
a	O
different	O
number	O
of	O
length	O
candidates	O
,	O
including	O
top	O
-	O
1	O
,	O
top	O
-	O
5	O
and	O
top	O
-	O
10	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
6	O
,	O
from	O
which	O
we	O
can	O
see	O
,	O
as	O
the	O
number	O
of	O
length	O
candidates	O
increases	O
,	O
the	O
model	O
performance	O
increases	O
as	O
well	O
.	O
The	O
reason	O
is	O
that	O
a	O
larger	O
candidates	O
set	O
is	O
more	O
likely	O
to	O
contain	O
the	O
best	O
-	O
suited	O
length	O
for	O
the	O
generation	O
model	O
,	O
leading	O
to	O
better	O
performance	O
.	O
However	O
,	O
such	O
decoding	O
procedure	O
inevitably	O
increases	O
the	O
required	O
computation	O
overhead	O
.	O
We	O
can	O
see	O
that	O
,	O
when	O
setting	O
k	O
as	O
10	O
,	O
the	O
inference	O
speedup	O
decreases	O
from	O
11.84×	O
to	O
6.01×.	O
In	O
contrast	O
,	O
our	O
proposed	O
model	O
is	O
able	O
to	O
determine	O
the	O
optimal	O
output	O
length	O
by	O
itself	O
.	O
Without	O
any	O
re	O
-	O
ranking	O
process	O
,	O
it	O
outperforms	O
the	O
model	O
with	O
LPD	O
-	O
10	O
decoding	O
and	O
achieves	O
the	O
inference	O
speedup	O
that	O
is	O
comparable	O
with	O
the	O
model	O
using	O
LPD	O
-	O
1	O
decoding	O
.	O

We	O
are	O
also	O
interested	O
in	O
the	O
effect	O
of	O
the	O
ratio	O
-	O
first	O
decoding	O
strategy	O
.	O
To	O
provide	O
a	O
quantitative	O
analysis	O
,	O
we	O
perform	O
inference	O
on	O
the	O
Gigawords	O
dataset	O
using	O
ratio	O
-	O
first	O
with	O
different	O
α	B-HyperparameterName
.	O
The	O
experimental	O
results	O
with	O
different	O
α	B-HyperparameterName
are	O
presented	O
in	O
Figure	O
3	O
.	O
It	O
can	O
be	O
observed	O
that	O
,	O
when	O
α	B-HyperparameterName
reaches	O
0.3	O
,	O
the	O
model	O
approximately	O
achieves	O
its	O
optimal	O
performance	O
.	O
At	O
the	O
same	O
time	O
,	O
a	O
notable	O
improvement	O
can	O
be	O
observed	O
in	O
terms	O
of	O
the	O
inference	O
speedup	O
(	O
6.72×	O
9.31×	O
)	O
.	O
Now	O
we	O
illustrate	O
why	O
the	O
near	O
optimal	O
performance	O
can	O
be	O
achieved	O
when	O
α	B-HyperparameterName
reaches	O
0.3	O
.	O
In	O
Figure	O
4	O
,	O
we	O
present	O
the	O
distribution	O
of	O
the	O
target	O
/	O
source	O
length	O
ratio	O
of	O
every	O
data	O
instance	O
in	O
the	O
Gigawords	O
dataset	O
.	O
We	O
can	O
see	O
that	O
,	O
for	O
most	O
cases	O
,	O
the	O
ratio	O
between	O
the	O
target	O
length	O
T	O
and	O
source	O
length	O
T	O
is	O
less	O
than	O
0.3	O
.	O
Recall	B-MetricName
the	O
definition	O
of	O
ratio	O
-	O
first	O
decoding	O
in	O
Equation	O
(	O
10	O
)	O
,	O
the	O
[	O
α	B-HyperparameterName
T	O
]	O
constrains	O
the	O
maximum	O
length	O
of	O
the	O
generated	O
result	O
.	O
Therefore	O
,	O
once	O
we	O
have	O
a	O
prior	O
knowledge	O
on	O
the	O
data	O
statistic	O
,	O
we	O
can	O
easily	O
choose	O
a	O
proper	O
α	B-HyperparameterName
that	O
both	O
improves	O
the	O
inference	O
speed	O
whilst	O
maintaining	O
the	O
generation	O
quality	O
.	O
In	O
this	O
case	O
,	O
a	O
proper	O
α	B-HyperparameterName
could	O
be	O
0.3	O
which	O
is	O
demonstrated	O
by	O
the	O
results	O
in	O
Figure	O
3	O
and	O
4	O
.	O
By	O
setting	O
different	O
α	B-HyperparameterName
,	O
ratio	O
-	O
first	O
provides	O
us	O
an	O
explicit	O
way	O
to	O
control	O
the	O
balance	O
between	O
the	O
inference	O
speed	O
and	O
the	O
generation	O
quality	O
.	O
This	O
property	O
of	O
ratio	O
-	O
first	O
is	O
especially	O
favorable	O
in	O
real	O
-	O
life	O
scenarios	O
where	O
the	O
inference	O
speed	O
is	O
the	O
highest	O
concern	O
.	O

We	O
conduct	O
an	O
ablation	O
study	O
by	O
removing	O
user	O
interaction	O
in	O
reviewing	O
the	O
model	O
-	O
generated	O
edits	O
.	O
Then	O
,	O
we	O
compare	O
the	O
overall	O
quality	O
of	O
final	O
revised	O
documents	O
with	O
and	O
without	O
the	O
human	O
-	O
in	O
-	O
the	O
-	O
loop	O
component	O
.	O
In	O
both	O
HUMAN	O
-	O
HUMAN	O
and	O
SYSTEM	O
-	O
HUMAN	O
setups	O
where	O
users	O
interacted	O
with	O
the	O
system	O
,	O
they	O
were	O
not	O
informed	O
whether	O
the	O
revisions	O
were	O
sampled	O
from	O
our	O
collected	O
iterative	O
revision	O
dataset	O
,	O
or	O
generated	O
by	O
the	O
underlying	O
text	O
revision	O
models	O
.	O
User	O
Study	O
Design	O
.	O
We	O
hired	O
three	O
linguistic	O
experts	O
(	O
English	O
L1	O
,	O
bachelor	O
's	O
or	O
higher	O
degree	O
in	O
Linguistics	O
)	O
to	O
interact	O
with	O
our	O
text	O
revision	O
system	O
.	O
Each	O
user	O
was	O
presented	O
with	O
a	O
text	O
revision	O
(	O
as	O
shown	O
in	O
Figure	O
2d	O
)	O
and	O
asked	O
to	O
accept	O
or	O
reject	O
each	O
edit	O
in	O
the	O
current	O
revision	O
(	O
users	O
were	O
informed	O
which	O
revision	O
depth	O
they	O
were	O
looking	O
at	O
)	O
.	O
For	O
a	O
fair	O
comparison	O
,	O
users	O
were	O
not	O
informed	O
about	O
the	O
source	O
of	O
the	O
edits	O
(	O
human	O
-	O
written	O
vs.	O
model	O
-	O
generated	O
)	O
,	O
and	O
the	O
experiments	O
were	O
conducted	O
separately	O
one	O
after	O
the	O
other	O
.	O
Note	O
that	O
the	O
users	O
were	O
only	O
asked	O
to	O
accept	O
or	O
reject	O
edits	O
,	O
and	O
they	O
had	O
control	O
neither	O
over	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
iterations	I-HyperparameterName
,	O
nor	O
over	O
the	O
stopping	O
criteria	O
.	O
The	O
stopping	O
criteria	O
for	O
the	O
experiment	O
were	O
set	O
by	O
us	O
and	O
designed	O
as	O
:	O
(	O
1	O
)	O
no	O
new	O
edits	O
were	O
made	O
at	O
the	O
following	O
revision	O
depth	O
,	O
or	O
(	O
2	O
)	O
the	O
maximum	O
revision	O
depth	O
t	O
max	O
=	O
3	O
was	O
reached	O
.	O
Data	O
Details	O
.	O
We	O
followed	O
the	O
prior	O
work	O
(	O
Du	O
et	O
al	O
,	O
2022	O
)	O
to	O
collect	O
the	O
text	O
revision	O
data	O
across	O
three	O
domains	O
:	O
ArXiv	B-DatasetName
,	O
Wikipedia	O
and	O
Wikinews	O
.	O
This	O
data	O
was	O
then	O
used	O
to	O
train	O
both	O
the	O
edit	O
intention	O
identification	O
models	O
and	O
the	O
text	O
revision	O
generation	O
model	O
.	O
We	O
split	O
the	O
data	O
into	O
training	O
,	O
validation	O
and	O
test	O
set	O
according	O
to	O
their	O
document	O
For	O
the	O
human	O
evaluation	O
data	O
,	O
we	O
randomly	O
sampled	O
10	O
documents	O
with	O
a	O
maximum	O
revision	O
depth	O
of	O
3	O
from	O
each	O
domain	O
in	O
the	O
test	O
set	O
in	O
Table	O
1	O
.	O
For	O
the	O
evaluation	O
of	O
text	O
revisions	O
made	O
by	O
human	O
writers	O
(	O
HUMAN	O
-	O
HUMAN	O
)	O
,	O
we	O
presented	O
the	O
existing	O
ground	O
-	O
truth	O
references	O
from	O
our	O
collected	O
dataset	O
to	O
users	O
.	O
Since	O
we	O
do	O
not	O
hire	O
additional	O
human	O
writers	O
to	O
perform	O
continuous	O
revisions	O
,	O
we	O
just	O
presented	O
the	O
static	O
human	O
revisions	O
from	O
the	O
original	O
test	O
set	O
to	O
users	O
at	O
each	O
revision	O
depth	O
,	O
and	O
collected	O
the	O
user	O
acceptance	O
statistics	O
as	O
a	O
baseline	O
for	O
our	O
system	O
.	O
For	O
the	O
evaluation	O
of	O
text	O
revisions	O
made	O
by	O
our	O
system	O
(	O
SYSTEM	O
-	O
HUMAN	O
)	O
,	O
we	O
only	O
presented	O
the	O
original	O
source	O
document	O
at	O
the	O
initial	O
revision	O
depth	O
(	O
D	O
0	B-DatasetName
)	O
to	O
our	O
system	O
,	O
and	O
let	O
the	O
system	O
generate	O
edits	O
in	O
the	O
following	O
revision	O
depths	O
,	O
while	O
incorporating	O
the	O
accept	O
/	O
reject	O
decisions	O
on	O
modelgenerated	O
edit	O
suggestions	O
by	O
the	O
users	O
.	O
Note	O
that	O
at	O
each	O
revision	O
depth	O
,	O
the	O
system	O
will	O
only	O
incorporate	O
the	O
edits	O
accepted	O
by	O
users	O
and	O
pass	O
them	O
to	O
the	O
next	O
revision	O
iteration	O
.	O
For	O
text	O
revisions	O
made	O
by	O
our	O
system	O
without	O
human	O
-	O
in	O
-	O
the	O
-	O
loop	O
(	O
SYSTEM	O
-	O
ONLY	O
)	O
,	O
we	O
let	O
the	O
system	O
generate	O
edits	O
in	O
an	O
iterative	O
way	O
and	O
accepted	O
all	O
model	O
-	O
generated	O
edits	O
at	O
each	O
revision	O
depth	O
.	O
Model	O
Details	O
.	O
For	O
both	O
edit	O
intention	O
identification	O
models	O
,	O
we	O
fine	O
-	O
tuned	O
the	O
RoBERTa	B-MethodName
-	O
large	O
pre	O
-	O
trained	O
checkpoint	O
from	O
Hugging	O
-	O
Face	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
for	O
2	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1	O
×	O
10	O
−5	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	O
.	O
The	O
edit	O
-	O
For	O
the	O
text	O
revision	O
generation	O
model	O
,	O
we	O
finetuned	O
the	O
PEGASUS	O
-	O
LARGE	O
(	O
Zhang	O
et	O
al	O
,	O
2020	O
)	O
pre	O
-	O
trained	O
checkpoint	O
from	O
HuggingFace	O
.	O
We	O
set	O
the	O
edit	O
intentions	O
as	O
new	O
special	O
tokens	O
(	O
e.g.	O
,	O
<	O
STYLE	O
>	O
,	O
<	O
FLUENCY	O
>	O
)	O
,	O
and	O
concatenated	O
the	O
edit	O
intention	O
and	O
source	O
sentence	O
together	O
as	O
the	O
input	O
to	O
the	O
model	O
.	O
The	O
output	O
of	O
the	O
model	O
is	O
the	O
revised	O
sentence	O
,	O
and	O
we	O
trained	O
the	O
model	O
with	O
cross	O
-	O
entropy	O
loss	B-MetricName
.	O
We	O
fine	O
-	O
tuned	O
the	O
model	O
for	O
5	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3	O
×	O
10	O
−5	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4	O
.	O
Finally	O
,	O
our	O
text	O
revision	O
generation	O
model	O
achieves	O
41.78	O
SARI	O
score	O
(	O
Xu	O
et	O
al	O
,	O
2016	O
)	O
,	O
81.11	O
BLEU	B-MetricName
score	I-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
and	O
89.08	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
score	O
(	O
Lin	O
,	O
2004	O
)	O
on	O
the	O
test	O
set	O
.	O

Since	O
our	O
research	O
focus	O
is	O
to	O
leverage	O
learned	O
feature	B-TaskName
importance	I-TaskName
to	O
enhance	O
predictions	O
,	O
we	O
followed	O
proven	O
methods	O
for	O
hate	B-DatasetName
speech	I-DatasetName
classification	O
(	O
Gambäck	O
and	O
Sikdar	O
,	O
2017	O
)	O
.	O
All	O
tweets	O
were	O
converted	O
into	O
the	O
Glove	O
twitter	O
embeddings	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
.	O
These	O
embeddings	O
were	O
passed	O
to	O
a	O
Convolution	B-MethodName
Neural	O
Network	O
classifier	O
which	O
mirrored	O
the	O
same	O
architecture	O
used	O
by	O
Yoon	O
Kim	O
for	O
sentence	B-TaskName
classification	I-TaskName
(	O
Kim	O
,	O
2014	O
)	O
.	O
We	O
allowed	O
for	O
parameter	O
tuning	O
with	O
random	B-MethodName
search	I-MethodName
,	O
and	O
the	O
final	O
CNN	O
consisted	O
of	O
three	O
convolutional	O
layers	O
of	O
75	O
filters	O
with	O
kernel	O
sizes	O
of	O
3	O
,	O
4	O
and	O
6	O
.	O
These	O
all	O
received	O
one	O
dimensional	O
max	B-MethodName
pooling	I-MethodName
and	O
a	O
dropout	O
rate	O
of	O
0.4	O
was	O
applied	O
.	O
The	O
output	O
layer	O
is	O
a	O
softmax	B-MethodName
with	O
l2	B-HyperparameterName
regularization	I-HyperparameterName
set	O
at	O
0.029	O
.	O
These	O
parameters	O
were	O
selected	O
by	O
ranking	O
validation	O
AUC	B-MetricName
.	O
This	O
model	O
served	O
as	O
both	O
our	O
baseline	O
model	O
and	O
the	O
input	O
predictions	O
of	O
our	O
predictions	O
enhanced	O
with	O
XAI	O
.	O

NODE	B-MethodName
were	O
introduced	O
as	O
a	O
continuous	O
depth	O
alternative	O
to	O
Residual	O
Networks	O
(	O
ResNets	O
)	O
(	O
He	O
et	O
al	O
,	O
2016	O
)	O
.	O
ResNets	O
uses	O
skip	O
connections	O
to	O
avoid	O
vanishing	O
gradient	O
problems	O
when	O
networks	O
grow	O
deeper	O
.	O
Residual	B-MethodName
block	I-MethodName
output	O
is	O
computed	O
as	O
h	O
t+1	O
=	O
h	O
t	O
+	O
f	O
(	O
h	O
t	O
,	O
θ	B-HyperparameterName
t	O
)	O
,	O
where	O
f	O
(	O
)	O
is	O
a	O
neural	O
network	O
(	O
NN	O
)	O
parameterized	O
by	O
θ	B-HyperparameterName
t	O
and	O
h	O
t	O
representing	O
the	O
hidden	O
representation	O
at	O
depth	O
t.	O
This	O
update	O
is	O
similar	O
to	O
a	O
step	O
in	O
Euler	O
numerical	O
technique	O
used	O
for	O
solving	O
ordinary	O
differential	O
equations	O
(	O
ODE	O
)	O
dh	O
(	O
t	O
)	O
dt	O
=	O
f	O
(	O
h	O
(	O
t	O
)	O
,	O
t	O
,	O
θ	B-HyperparameterName
)	O
.	O
The	O
sequence	O
of	O
residual	B-MethodName
block	I-MethodName
operations	O
in	O
ResNets	O
can	O
be	O
seen	O
as	O
a	O
solution	O
to	O
this	O
ODE	O
.	O
Consequently	O
,	O
NODEs	O
can	O
be	O
interpreted	O
as	O
a	O
continuous	O
equivalent	O
of	O
ResNets	O
modeling	O
the	O
evolution	O
of	O
hidden	O
representationsh	O
(	O
t	O
)	O
over	O
time	O
.	O
For	O
solving	O
ODE	O
,	O
one	O
can	O
use	O
fixed	O
stepsize	O
numerical	O
techniques	O
such	O
as	O
Euler	O
,	O
Runge	O
-	O
Kutta	O
or	O
adaptive	O
step	O
-	O
size	O
methods	O
like	O
Do	O
-	O
pri5	O
(	O
Dormand	O
and	O
Prince	O
,	O
1980	O
)	O
.	O
Solving	O
an	O
ODE	O
requires	O
one	O
to	O
specify	O
an	O
initial	O
value	O
h	O
(	O
0	B-DatasetName
)	O
(	O
input	O
x	O
or	O
its	O
transformation	O
)	O
and	O
can	O
compute	O
the	O
value	O
at	O
t	O
using	O
an	O
ODE	O
solver	O
ODESolverCompute	O
(	O
f	O
θ	B-HyperparameterName
,	O
h	O
(	O
0	B-DatasetName
)	O
,	O
0	B-DatasetName
,	O
t	O
)	O
.	O
An	O
ODE	O
is	O
solved	O
until	O
some	O
end	O
-	O
time	O
T	O
to	O
obtain	O
the	O
final	O
hidden	O
representation	O
h	O
(	O
T	O
)	O
which	O
is	O
used	O
to	O
predict	O
class	O
labelsŷ	O
.	O
For	O
classification	O
problems	O
,	O
cross	O
-	O
entropy	O
loss	B-MetricName
is	O
used	O
and	O
parameters	O
are	O
learnt	O
through	O
adjoint	O
sensitivity	O
method	O
(	O
Zhuang	O
et	O
al	O
,	O
2020	O
;	O
Chen	O
et	O
al	O
,	O
2018	O
)	O
which	O
provides	O
efficient	O
back	O
-	O
propagation	O
and	O
gradient	O
computations	O
.	O

LSTMs	O
are	O
popular	O
for	O
sequence	O
classification	O
but	O
only	O
considers	O
the	O
sequential	O
nature	O
of	O
the	O
data	O
and	O
ignore	O
the	O
temporal	O
features	O
associated	O
with	O
the	O
data	O
in	O
its	O
standard	O
setting	O
.	O
As	O
the	O
posts	O
occur	O
in	O
irregular	O
intervals	O
of	O
time	O
,	O
the	O
nature	O
of	O
a	O
new	O
post	O
will	O
be	O
influenced	O
by	O
the	O
recent	O
posts	O
,	O
influence	O
will	O
be	O
inversely	O
proportional	O
to	O
the	O
time	O
gap	O
.	O
In	O
these	O
situations	O
,	O
it	O
will	O
be	O
beneficial	O
to	O
use	O
a	O
model	O
where	O
the	O
number	O
of	O
transformations	O
depend	O
on	O
the	O
time	O
gap	O
.	O
We	O
propose	O
to	O
use	O
RNODE	O
which	O
considers	O
the	O
arrival	O
time	O
and	O
accordingly	O
the	O
hidden	O
representations	O
are	O
transformed	O
across	O
time	O
.	O
In	O
RNODE	O
,	O
the	O
transformation	O
of	O
a	O
hidden	O
representation	O
h	O
(	O
t	O
i−1	O
)	O
at	O
time	O
t	O
i−1	O
to	O
h	O
(	O
t	O
i	O
)	O
at	O
time	O
t	O
i	O
is	O
governed	O
by	O
an	O
ODE	O
parameterized	O
by	O
a	O
NN	O
f	O
(	O
)	O
.	O
Unlike	O
standard	O
LSTMs	O
where	O
h	O
(	O
t	O
i	O
)	O
is	O
obtained	O
from	O
h	O
(	O
t	O
i−1	O
)	O
as	O
a	O
single	O
NN	O
transformation	O
,	O
RNODE	O
first	O
obtains	O
a	O
hidden	O
representation	O
h	O
′	O
(	O
t	O
i	O
)	O
as	O
a	O
solution	O
to	O
an	O
ODE	O
at	O
time	O
t	O
i	O
with	O
initial	O
value	O
h	O
(	O
t	O
i−1	O
)	O
.	O
The	O
number	O
of	O
update	O
steps	O
in	O
the	O
numerical	O
technique	O
used	O
to	O
solve	O
this	O
ODE	O
depends	O
on	O
the	O
time	O
gap	O
t	O
i	O
−t	O
i−1	O
between	O
the	O
consecutive	O
posts	O
.	O
The	O
hidden	O
representation	O
h	O
′	O
(	O
t	O
i	O
)	O
and	O
input	O
post	O
x	O
i	O
at	O
time	O
t	O
i	O
are	O
passed	O
through	O
neural	O
network	O
transformation	O
(	O
RNNCell	O
(	O
)	O
)	O
to	O
obtain	O
final	O
hidden	O
representation	O
h	O
(	O
t	O
i	O
)	O
,	O
i.e.	O
,	O
h	O
(	O
t	O
i	O
)	O
=	O
RNNCell	O
(	O
h	O
′	O
(	O
t	O
i	O
)	O
,	O
x	O
i	O
)	O
.	O
The	O
process	O
is	O
repeated	O
for	O
every	O
element	O
(	O
x	O
i	O
,	O
t	O
i	O
)	O
in	O
the	O
sequence	O
.	O
The	O
hidden	O
representations	O
associated	O
with	O
the	O
elements	O
in	O
the	O
sequence	O
are	O
then	O
passed	O
to	O
a	O
neural	O
network	O
(	O
NN	O
(	O
)	O
)	O
to	O
obtain	O
the	O
post	O
labels	O
.	O
Using	O
standard	O
cross	O
-	O
entropy	O
loss	B-MetricName
,	O
the	O
parameters	O
of	O
the	O
models	O
are	O
learnt	O
through	O
backpropagation	O
.	O
Figure	O
1	O
provides	O
the	O
detailed	O
architecture	O
of	O
the	O
Bi	O
-	O
directional	O
RNNs	O
(	O
Schuster	O
and	O
Paliwal	O
,	O
1997	O
)	O
such	O
as	O
Bi	O
-	O
LSTMS	O
(	O
Graves	O
et	O
al	O
,	O
2013	O
)	O
were	O
proven	O
to	O
be	O
successful	O
in	O
many	O
sequence	O
labeling	O
tasks	O
in	O
natural	O
language	O
processing	O
such	O
as	O
POS	O
tagging	O
(	O
Huang	O
et	O
al	O
,	O
2015	O
)	O
.	O
They	O
use	O
the	O
information	O
from	O
the	O
past	O
and	O
future	O
to	O
predict	O
the	O
label	O
while	O
standard	O
LSTMs	O
consider	O
only	O
information	O
from	O
the	O
past	O
.	O
We	O
propose	O
a	O
Bi	O
-	O
RNODE	O
model	O
,	O
which	O
uses	O
the	O
sequence	O
of	O
input	O
observations	O
from	O
past	O
and	O
from	O
the	O
future	O
to	O
predict	O
the	O
post	O
label	O
at	O
any	O
time	O
t.	O
It	O
assumes	O
the	O
hidden	O
representation	O
dynamics	O
are	O
influenced	O
not	O
only	O
by	O
the	O
past	O
posts	O
but	O
also	O
by	O
the	O
futures	O
posts	O
.	O
Unlike	O
Bi	O
-	O
LSTMs	O
,	O
Bi	O
-	O
RNODE	O
considers	O
the	O
exact	O
time	O
of	O
the	O
posts	O
and	O
their	O
inter	O
-	O
arrival	O
times	O
in	O
determining	O
the	O
transformations	O
in	O
the	O
hidden	O
representations	O
.	O
Bi	O
-	O
RNODE	O
consists	O
of	O
two	O
RNODE	O
blocks	O
,	O
one	O
performing	O
transformations	O
in	O
the	O
forward	O
direction	O
(	O
in	O
the	O
order	O
of	O
posting	O
times	O
)	O
and	O
the	O
other	O
in	O
the	O
backward	O
direction	O
.	O
The	O
hidden	O
representations	O
H	O
and	O
H	O
b	O
computed	O
by	O
forward	O
and	O
backward	O
RNODE	O
respectively	O
are	O
aggregated	O
either	O
by	O
concatenation	O
or	O
by	O
averaging	O
appropriately	O
to	O
obtain	O
a	O
final	O
hidden	O
representation	O
and	O
is	O
passed	O
through	O
a	O
NN	O
to	O
obtain	O
the	O
post	O
labels	O
.	O
Bi	O
-	O
RNODE	O
is	O
useful	O
when	O
a	O
sequence	O
of	O
posts	O
with	O
their	O
time	O
of	O
occurrence	O
needs	O
to	O
be	O
classified	O
together	O
.	O
Figure	O
2	O
provides	O
an	O
overview	O
of	O
Bi	O
-	O
RNODE	O
model	O
for	O
post	O
classification	O
.	O
For	O
Bi	O
-	O
RNODE	O
,	O
an	O
extra	O
neural	O
network	O
f	O
θ	B-HyperparameterName
′	O
(	O
)	O
is	O
required	O
to	O
compute	O
hidden	O
representations	O
h	O
b	O
(	O
t	O
′	O
i	O
)	O
in	O
the	O
backward	O
direction	O
.	O
Training	O
in	O
Bi	O
-	O
RNODE	O
is	O
done	O
in	O
a	O
similar	O
manner	O
to	O
RNODE	O
,	O
with	O
cross	O
-	O
entropy	O
loss	B-MetricName
and	O
back	O
-	O
propagation	O
to	O
estimate	O
parameters	O
.	O

We	O
conducted	O
experiments	O
to	O
predict	O
the	O
stance	O
of	O
social	O
media	O
posts	O
propagating	O
in	O
seen	O
events	O
and	O
unseen	O
events	O
.	O
-	O
Seen	O
Event	O
Here	O
we	O
train	O
,	O
validate	O
and	O
test	O
on	O
tweets	O
of	O
the	O
same	O
event	O
.	O
Each	O
event	O
data	O
is	O
split	O
60:20:20	O
ratio	O
in	O
sequence	O
of	O
time	O
.	O
This	O
setup	O
helps	O
in	O
predicting	O
the	O
stance	O
of	O
unseen	O
tweets	O
of	O
the	O
same	O
event	O
.	O
-	O
Unseen	O
Event	O
:	O
This	O
setup	O
helps	O
in	O
evaluating	O
performance	O
on	O
an	O
unseen	O
event	O
and	O
training	O
on	O
a	O
larger	O
dataset	O
.	O
Here	O
,	O
training	O
and	O
validation	O
data	O
are	O
formed	O
using	O
data	O
from	O
3	O
events	O
and	O
testing	O
is	O
done	O
on	O
the	O
4	O
th	O
event	O
.	O
Last	O
20	O
%	O
of	O
the	O
training	O
data	O
(	O
after	O
ordering	O
based	O
on	O
time	O
)	O
are	O
set	O
aside	O
for	O
validation	O
.	O
During	O
training	O
,	O
mini	O
-	O
batches	O
are	O
formed	O
only	O
from	O
the	O
tweets	O
belonging	O
to	O
the	O
same	O
event	O
.	O
Baselines	O
:	O
We	O
compared	O
results	O
of	O
our	O
proposed	O
RNODE	O
and	O
Bi	O
-	O
RNODE	O
models	O
with	O
RNN	O
based	O
baselines	O
such	O
LSTM	B-MethodName
(	O
Kochkina	O
et	O
al	O
,	O
2017	O
)	O
,	O
Bi	O
-	O
LSTM	B-MethodName
(	O
Augenstein	O
et	O
al	O
,	O
2016	O
)	O
,	O
GRU	B-MethodName
(	O
Cho	O
et	O
al	O
,	O
2014	O
)	O
,	O
Bi	O
-	O
GRU	B-MethodName
,	O
and	O
Majority	O
(	O
labelling	O
with	O
most	O
frequent	O
class	O
)	O
baseline	O
models	O
.	O
We	O
also	O
use	O
a	O
variant	O
of	O
LSTM	B-MethodName
baseline	O
considering	O
temporal	O
information	O
(	O
Zubiaga	O
et	O
al	O
,	O
2018b	O
)	O
,	O
LSTM	B-MethodName
-	O
timeGap	O
where	O
the	O
time	O
gap	O
of	O
consecutive	O
data	O
points	O
is	O
included	O
as	O
part	O
of	O
the	O
input	O
data	O
.	O
Evaluation	O
Metrics	O
:	O
We	O
consider	O
the	O
standard	O
evaluation	O
metrics	O
such	O
as	O
precision	O
,	O
recall	O
,	O
F1	B-MetricName
and	O
in	O
addition	O
the	O
AUC	B-MetricName
score	O
to	O
account	O
for	O
the	O
data	O
imbalance	O
.	O
We	O
consider	O
a	O
weighted	O
average	O
of	O
the	O
evaluation	O
metrics	O
to	O
compare	O
the	O
performance	O
of	O
models	O
.	O
Hyperparameters	O
:	O
All	O
the	O
models	O
are	O
trained	O
for	O
50	O
epochs	O
with	O
0.01	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
Adam	B-MethodName
optimizer	B-HyperparameterName
,	O
dropout	O
(	O
0.2	O
)	O
regularizer	O
,	O
batchsize	O
of	O
50	O
,	O
hidden	O
representation	O
size	O
of	O
64	O
and	O
cross	O
entropy	O
as	O
the	O
loss	B-MetricName
function	O
.	O
Different	O
hyperparameters	O
like	O
neural	O
network	O
layers	O
(	O
1	O
,	O
2	O
)	O
,	O
numerical	O
methods	O
(	O
Euler	O
,	O
RK4	O
,	O
Dopri5	O
for	O
RNODE	O
and	O
Bi	O
-	O
RNODE	O
)	O
and	O
aggregation	O
strategy	O
(	O
concatenation	O
or	O
averaging	O
for	O
Bi	O
-	O
LSTM	B-MethodName
Bi	O
-	O
GRU	B-MethodName
and	O
Bi	O
-	O
RNODE	O
)	O
are	O
used	O
for	O
all	O
the	O
models	O
and	O
the	O
best	O
configuration	O
is	O
selected	O
from	O
the	O
validation	O
data	O
for	O
different	O
experimental	O
setups	O
and	O
train	O
/	O
test	O
data	O
splits	O
.	O

The	O
results	O
of	O
seen	O
event	O
and	O
unseen	O
event	O
experiment	O
setup	O
can	O
be	O
found	O
in	O
Table	O
1	O
,	O
where	O
the	O
first	O
and	O
second	O
rows	O
for	O
each	O
model	O
provides	O
results	O
on	O
seen	O
event	O
and	O
unseen	O
event	O
respectively	O
.	O
We	O
can	O
observe	O
from	O
Table	O
1	O
that	O
for	O
both	O
seen	O
event	O
and	O
unseen	O
event	O
experiment	O
setup	O
,	O
RNODE	O
and	O
Bi	O
-	O
RNODE	O
models	O
performed	O
better	O
than	O
the	O
baseline	O
models	O
in	O
general	O
for	O
all	O
the	O
3	O
events	O
3	O
.	O
In	O
particular	O
for	O
the	O
seen	O
event	O
setup	O
,	O
Bi	O
-	O
RNODE	O
gives	O
the	O
best	O
result	O
outperforming	O
RNODE	O
and	O
other	O
models	O
for	O
most	O
of	O
the	O
data	O
sets	O
and	O
measures	O
.	O
Under	O
seen	O
event	O
experiment	O
on	O
Syndneysiege	O
event	O
,	O
we	O
plot	O
the	O
ROC	O
curve	O
for	O
all	O
the	O
models	O
in	O
Figure	O
3	O
.	O
We	O
can	O
observe	O
that	O
AUC	B-MetricName
for	O
Figures	O
3	O
(	O
a	O
)	O
and	O
3	O
(	O
e	O
)	O
corresponding	O
to	O
RNODE	O
and	O
Bi	O
-	O
RNODE	O
respectively	O
are	O
higher	O
than	O
LSTM	B-MethodName
,	O
GRU	B-MethodName
,	O
Bi	O
-	O
LSTM	B-MethodName
,	O
and	O
Bi	O
-	O
GRU	B-MethodName
.	O

We	O
propose	O
a	O
new	O
end	O
-	O
to	O
-	O
end	O
model	O
that	O
treats	O
AMR	B-TaskName
parsing	I-TaskName
as	O
a	O
series	O
of	O
dual	O
decisions	O
on	O
the	O
input	O
sequence	O
and	O
the	O
incrementally	O
constructed	O
graph	O
.	O
At	O
each	O
time	O
step	O
,	O
our	O
model	O
performs	O
multiple	O
rounds	O
of	O
attention	O
,	O
reasoning	O
,	O
and	O
composition	O
that	O
aim	O
to	O
answer	O
two	O
critical	O
questions	O
:	O
(	O
1	O
)	O
which	O
part	O
of	O
the	O
input	O
sequence	O
to	O
abstract	O
;	O
and	O
(	O
2	O
)	O
where	O
in	O
the	O
output	O
graph	O
to	O
construct	O
the	O
new	O
concept	O
.	O
We	O
show	O
that	O
the	O
answers	O
to	O
these	O
two	O
questions	O
are	O
mutually	O
causalities	O
.	O
We	O
design	O
a	O
model	O
based	O
on	O
iterative	O
inference	O
that	O
helps	O
achieve	O
better	O
answers	O
in	O
both	O
perspectives	O
,	O
leading	O
to	O
greatly	O
improved	O
parsing	O
accuracy	B-MetricName
.	O
Our	O
experimental	O
results	O
significantly	O
outperform	O
all	O
previously	O
reported	O
SMATCH	O
scores	O
by	O
large	O
margins	O
.	O
Remarkably	O
,	O
without	O
the	O
help	O
of	O
any	O
large	O
-	O
scale	O
pre	O
-	O
trained	O
language	O
model	O
(	O
e.g.	O
,	O
BERT	B-MethodName
)	O
,	O
our	O
model	O
already	O
surpasses	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
using	O
BERT	B-MethodName
.	O
With	O
the	O
help	O
of	O
BERT	B-MethodName
,	O
we	O
can	O
push	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
to	O
80.2	O
%	O
on	O
LDC2017T10	B-DatasetName
(	O
AMR	O
2.0	O
)	O
and	O
75.4	O
%	O
on	O
LDC2014T12	O
(	O
AMR	O
1.0	O
)	O
.	O

Abstract	O
Meaning	O
Representation	O
(	O
AMR	O
)	O
(	O
Banarescu	O
et	O
al	O
,	O
2013	O
)	O
is	O
a	O
broad	O
-	O
coverage	O
semantic	O
formalism	O
that	O
encodes	O
the	O
meaning	O
of	O
a	O
sentence	O
as	O
a	O
rooted	O
,	O
directed	O
,	O
and	O
labeled	O
graph	O
,	O
where	O
nodes	O
represent	O
concepts	O
and	O
edges	O
represent	O
relations	O
(	O
See	O
an	O
example	O
in	O
Figure	O
1	O
)	O
.	O
AMR	B-TaskName
parsing	I-TaskName
is	O
the	O
task	O
of	O
transforming	O
natural	O
language	O
text	O
into	O
AMR	O
.	O
One	O
biggest	O
challenge	O
of	O
AMR	B-TaskName
parsing	I-TaskName
is	O
the	O
lack	O
of	O
explicit	O
alignments	O
between	O
nodes	O
(	O
concepts	O
)	O
in	O
the	O
graph	O
and	O
words	O
in	O
the	O
text	O
.	O
This	O
characteristic	O
not	O
only	O
poses	O
great	O
difficulty	O
in	O
concept	O
*	O
The	O
work	O
described	O
in	O
this	O
paper	O
is	O
substantially	O
supported	O
by	O
grants	O
from	O
the	O
Research	O
Grant	O
Council	O
of	O
the	O
Hong	O
Kong	O
Special	O
Administrative	O
Region	O
,	O
China	O
(	O
Project	O
Code	O
:	O
14204418	O
)	O
and	O
the	O
Direct	O
Grant	O
of	O
the	O
Faculty	O
of	O
Engineering	O
,	O
CUHK	B-DatasetName
(	O
Project	O
Code	O
:	O
4055093	O
)	O
.	O
prediction	O
but	O
also	O
brings	O
a	O
close	O
tie	O
for	O
concept	O
prediction	O
and	O
relation	O
prediction	O
.	O
While	O
most	O
previous	O
works	O
rely	O
on	O
a	O
pre	O
-	O
trained	O
aligner	O
to	O
train	O
a	O
parser	O
,	O
some	O
recent	O
attempts	O
include	O
:	O
modeling	O
the	O
alignments	O
as	O
latent	O
variables	O
(	O
Lyu	O
and	O
Titov	O
,	O
2018	O
)	O
,	O
attention	O
-	O
based	O
sequenceto	O
-	O
sequence	O
transduction	O
models	O
(	O
Barzdins	O
and	O
Gosko	O
,	O
2016	O
;	O
Konstas	O
et	O
al	O
,	O
2017	O
;	O
van	O
Noord	O
and	O
Bos	O
,	O
2017	O
)	O
,	O
and	O
attention	O
-	O
based	O
sequence	O
-	O
to	O
-	O
graph	O
transduction	O
models	O
(	O
Cai	O
and	O
Lam	O
,	O
2019	O
;	O
Zhang	O
et	O
al	O
,	O
2019b	O
)	O
.	O
Sequence	O
-	O
to	O
-	O
graph	O
transduction	O
models	O
build	O
a	O
semantic	O
graph	O
incrementally	O
via	O
spanning	O
one	O
node	O
at	O
every	O
step	O
.	O
This	O
property	O
is	O
appealing	O
in	O
terms	O
of	O
both	O
computational	O
efficiency	O
and	O
cognitive	O
modeling	O
since	O
it	O
mimics	O
what	O
human	O
experts	O
usually	O
do	O
,	O
i.e.	O
,	O
first	O
grasping	O
the	O
core	O
ideas	O
then	O
digging	O
into	O
more	O
details	O
(	O
Banarescu	O
et	O
al	O
,	O
2013	O
;	O
Cai	O
and	O
Lam	O
,	O
2019	O
)	O
.	O
Unfortunately	O
,	O
the	O
parsing	O
accuracy	B-MetricName
of	O
existing	O
works	O
including	O
recent	O
state	O
-	O
of	O
-	O
the	O
-	O
arts	O
(	O
Zhang	O
et	O
al	O
,	O
2019a	O
,	O
b	O
)	O
remain	O
unsatisfactory	O
compared	O
to	O
human	O
-	O
level	O
performance	O
,	O
1	O
especially	O
in	O
cases	O
where	O
the	O
sentences	O
are	O
rather	O
long	O
and	O
informative	O
,	O
which	O
indicates	O
substantial	O
room	O
for	O
improvement	O
.	O
One	O
possible	O
reason	O
for	O
the	O
deficiency	O
is	O
the	O
inherent	O
defect	O
of	O
one	O
-	O
pass	O
prediction	O
process	O
;	O
that	O
is	O
,	O
the	O
lack	O
of	O
the	O
modeling	O
capability	O
of	O
the	O
interactions	O
between	O
concept	O
prediction	O
and	O
relation	O
prediction	O
,	O
which	O
is	O
critical	O
to	O
achieving	O
fullyinformed	O
and	O
unambiguous	O
decisions	O
.	O
We	O
introduce	O
a	O
new	O
approach	O
tackling	O
AMR	B-TaskName
parsing	I-TaskName
,	O
following	O
the	O
incremental	O
sequence	O
-	O
tograph	O
transduction	O
paradigm	O
.	O
We	O
explicitly	O
characterize	O
each	O
spanning	O
step	O
as	O
the	O
efforts	O
for	O
finding	O
which	O
part	O
to	O
abstract	O
with	O
respect	O
to	O
the	O
input	O
sequence	O
,	O
and	O
where	O
to	O
construct	O
with	O
respect	O
to	O
the	O
partially	O
constructed	O
output	O
graph	O
.	O
Equivalently	O
,	O
we	O
treat	O
AMR	B-TaskName
parsing	I-TaskName
as	O
a	O
series	O
of	O
dual	O
decisions	O
on	O
the	O
input	O
sequence	O
and	O
the	O
incrementally	O
constructed	O
graph	O
.	O
Intuitively	O
,	O
the	O
answer	O
of	O
what	O
concept	O
to	O
abstract	O
decides	O
where	O
to	O
construct	O
(	O
i.e.	O
,	O
the	O
relations	O
to	O
existing	O
concepts	O
)	O
,	O
while	O
the	O
answer	O
of	O
where	O
to	O
construct	O
determines	O
what	O
concept	O
to	O
abstract	O
.	O
Our	O
proposed	O
model	O
,	O
supported	O
by	O
neural	O
networks	O
with	O
explicit	O
structure	O
for	O
attention	O
,	O
reasoning	O
,	O
and	O
composition	O
,	O
integrated	O
with	O
an	O
iterative	O
inference	O
algorithm	O
.	O
It	O
iterates	O
between	O
finding	O
supporting	O
text	O
pieces	O
and	O
reading	O
the	O
partially	O
constructed	O
semantic	O
graph	O
,	O
inferring	O
more	O
accurate	O
and	O
harmonious	O
expansion	O
decisions	O
progressively	O
.	O
Our	O
model	O
is	O
aligner	O
-	O
free	O
and	O
can	O
be	O
effectively	O
trained	O
with	O
limited	O
amount	O
of	O
labeled	O
data	O
.	O
Experiments	O
on	O
two	O
AMR	O
benchmarks	O
demonstrate	O
that	O
our	O
parser	O
outperforms	O
the	O
previous	O
best	O
parsers	O
on	O
both	O
benchmarks	O
.	O
It	O
achieves	O
the	O
best	O
-	O
reported	O
SMATCH	O
scores	O
(	O
F1	B-MetricName
)	O
:	O
80.2	O
%	O
on	O
LDC2017T10	B-DatasetName
and	O
75.4	O
%	O
on	O
LDC2014T12	O
,	O
surpassing	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
by	O
large	O
margins	O
.	O

At	O
each	O
sequence	O
reasoning	O
step	O
t	O
,	O
the	O
concept	O
solver	O
receives	O
a	O
state	O
vector	O
y	O
t	O
that	O
carries	O
the	O
latest	O
graph	O
decision	O
and	O
the	O
input	O
sequence	O
memories	O
h	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
n	O
from	O
the	O
sequence	O
encoder	O
,	O
and	O
aims	O
to	O
locate	O
the	O
proper	O
parts	O
in	O
the	O
input	O
sequence	O
to	O
abstract	O
and	O
generate	O
a	O
new	O
concept	O
.	O
We	O
employ	O
the	O
scaled	B-MethodName
dot	I-MethodName
-	I-MethodName
product	I-MethodName
attention	I-MethodName
proposed	O
in	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
to	O
solve	O
this	O
problem	O
.	O
Concretely	O
,	O
we	O
first	O
calculate	O
an	O
attention	O
distribution	O
over	O
all	O
input	O
tokens	O
:	O
α	B-HyperparameterName
t	O
=	O
softmax	B-MethodName
(	O
(	O
W	O
Q	O
y	O
t	O
)	O
T	O
W	O
K	O
h	O
1	O
:	O
n	O
√	O
d	O
k	O
)	O
,	O
where	O
{	O
W	O
Q	O
,	O
W	O
K	O
}	O
R	O
d	O
k	O
×d	O
denote	O
learnable	O
linear	O
projections	O
that	O
transform	O
the	O
input	O
vectors	O
into	O
the	O
query	O
and	O
key	O
subspace	O
respectively	O
,	O
and	O
d	O
k	O
represents	O
the	O
dimensionality	O
of	O
the	O
subspace	O
.	O
The	O
attention	O
weights	O
α	B-HyperparameterName
t	O
R	O
n	O
provide	O
a	O
soft	O
alignment	O
between	O
the	O
new	O
concept	O
and	O
the	O
tokens	O
in	O
the	O
input	O
sequence	O
.	O
We	O
then	O
compute	O
the	O
probability	O
distribution	O
of	O
the	O
new	O
concept	O
label	O
through	O
a	O
hybrid	O
of	O
three	O
channels	O
.	O
First	O
,	O
α	B-HyperparameterName
t	O
is	O
fed	O
through	O
an	O
MLP	B-DatasetName
and	O
softmax	B-MethodName
to	O
obtain	O
a	O
probability	O
distribution	O
over	O
a	O
pre	O
-	O
defined	O
vocabulary	O
:	O
MLP	B-DatasetName
(	O
α	B-HyperparameterName
t	O
)	O
=	O
(	O
W	O
V	O
h	O
1	O
:	O
n	O
)	O
α	B-HyperparameterName
t	O
+	O
y	O
t	O
(	O
1	O
)	O
P	O
(	O
vocab	O
)	O
=	O
softmax	B-MethodName
(	O
W	O
(	O
vocab	O
)	O
MLP	B-DatasetName
(	O
α	B-HyperparameterName
t	O
)	O
+	O
b	O
(	O
vocab	O
)	O
)	O
,	O
where	O
W	O
V	O
R	O
d×d	O
denotes	O
the	O
learnable	O
linear	O
projection	O
that	O
transforms	O
the	O
text	O
memories	O
into	O
the	O
value	O
subspace	O
,	O
and	O
the	O
value	O
vectors	O
are	O
averaged	O
according	O
to	O
α	B-HyperparameterName
t	O
for	O
concept	O
label	O
prediction	O
.	O
Second	O
,	O
the	O
attention	O
weights	O
α	B-HyperparameterName
t	O
directly	O
serve	O
as	O
a	O
copy	O
mechanism	O
(	O
Gu	O
et	O
al	O
,	O
2016	O
;	O
See	O
et	O
al	O
,	O
2017	O
)	O
,	O
i	O
,	O
e.	O
,	O
the	O
probabilities	O
of	O
copying	O
a	O
token	O
lemma	B-DatasetName
from	O
the	O
input	O
text	O
as	O
a	O
node	O
label	O
.	O
Third	O
,	O
to	O
address	O
the	O
attribute	O
values	O
such	O
as	O
person	O
names	O
or	O
numerical	O
strings	O
,	O
we	O
also	O
use	O
α	B-HyperparameterName
t	O
for	O
another	O
copy	O
mechanism	O
that	O
directly	O
copies	O
the	O
original	O
strings	O
of	O
input	O
tokens	O
.	O
The	O
above	O
three	O
channels	O
are	O
combined	O
via	O
a	O
soft	O
switch	O
to	O
control	O
the	O
production	O
of	O
the	O
concept	O
label	O
from	O
different	O
sources	O
:	O
[	O
p	O
0	B-DatasetName
,	O
p	O
1	O
,	O
p	O
2	O
]	O
=	O
softmax	B-MethodName
(	O
W	O
(	O
switch	O
)	O
MLP	B-DatasetName
(	O
α	B-HyperparameterName
t	O
)	O
)	O
,	O
where	O
MLP	B-DatasetName
is	O
the	O
same	O
as	O
in	O
Eq	O
.	O
1	O
,	O
and	O
p	O
0	B-DatasetName
,	O
p	O
1	O
and	O
p	O
2	O
are	O
the	O
probabilities	O
of	O
three	O
prediction	O
channels	O
respectively	O
.	O
Hence	O
,	O
the	O
final	O
prediction	O
probability	O
of	O
a	O
concept	O
c	O
is	O
given	O
by	O
:	O
P	O
(	O
c	O
)	O
=	O
p	O
0	B-DatasetName
P	O
(	O
vocab	O
)	O
(	O
c	O
)	O
+	O
p	O
1	O
(	O
i	O
L	O
(	O
c	O
)	O
α	B-HyperparameterName
t	O
[	O
i	O
]	O
)	O
+	O
p	O
2	O
(	O
i	O
T	O
(	O
c	O
)	O
α	B-HyperparameterName
t	O
[	O
i	O
]	O
)	O
,	O
where	O
[	O
i	O
]	O
indexes	O
the	O
i	O
-	O
th	O
element	O
and	O
L	O
(	O
c	O
)	O
and	O
T	O
(	O
c	O
)	O
are	O
index	O
sets	O
of	O
lemmas	O
and	O
tokens	O
respectively	O
that	O
have	O
the	O
surface	O
form	O
as	O
c.	O

At	O
each	O
graph	O
reasoning	O
step	O
t	O
,	O
the	O
relation	O
solver	O
receives	O
a	O
state	O
vector	O
x	O
t	O
that	O
carries	O
the	O
latest	O
concept	O
decision	O
and	O
the	O
output	O
graph	O
memories	O
s	O
0	B-DatasetName
,	O
s	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
m	O
from	O
the	O
graph	O
encoder	O
,	O
and	O
aims	O
to	O
point	O
out	O
the	O
nodes	O
in	O
the	O
current	O
graph	O
that	O
have	O
an	O
immediate	O
relation	O
to	O
the	O
new	O
concept	O
(	O
source	O
nodes	O
)	O
and	O
generate	O
corresponding	O
edges	O
.	O
Similar	O
to	O
Cai	O
and	O
Lam	O
(	O
2019	O
)	O
;	O
Zhang	O
et	O
al	O
(	O
2019b	O
)	O
,	O
we	O
factorize	O
the	O
task	O
as	O
two	O
stages	O
:	O
First	O
,	O
a	O
relation	O
identification	O
module	O
points	O
to	O
some	O
preceding	O
nodes	O
as	O
source	O
nodes	O
;	O
Then	O
,	O
the	O
relation	B-TaskName
classification	I-TaskName
module	O
predicts	O
the	O
relation	O
type	O
between	O
the	O
new	O
concept	O
and	O
predicted	O
source	O
nodes	O
.	O
We	O
leave	O
the	O
latter	O
to	O
be	O
determined	O
after	O
iterative	O
inference	O
.	O
AMR	O
is	O
a	O
rooted	O
,	O
directed	O
,	O
and	O
acyclic	O
graph	O
.	O
The	O
reason	O
for	O
AMR	O
being	O
a	O
graph	O
instead	O
of	O
a	O
tree	O
is	O
that	O
it	O
allows	O
reentrancies	O
where	O
a	O
concept	O
participates	O
in	O
multiple	O
semantic	O
relations	O
with	O
different	O
semantic	O
roles	O
.	O
Following	O
Cai	O
and	O
Lam	O
(	O
2019	O
)	O
,	O
we	O
use	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
for	O
a	O
more	O
compact	O
parsing	O
procedure	O
where	O
multiple	O
source	O
nodes	O
are	O
simultaneously	O
determined	O
.	O
5	O
Formally	O
,	O
our	O
relation	O
identification	O
module	O
employs	O
H	O
different	O
attention	O
heads	O
,	O
for	O
each	O
head	O
h	O
,	O
we	O
calculate	O
an	O
attention	O
distribution	O
over	O
all	O
existing	O
node	O
(	O
including	O
the	O
dummy	O
node	O
s	O
0	B-DatasetName
)	O
:	O
β	B-HyperparameterName
h	O
t	O
=	O
softmax	B-MethodName
(	O
(	O
W	O
Q	O
h	O
x	O
t	O
)	O
T	O
W	O
K	O
h	O
s	O
0	B-DatasetName
:	O
m	O
√	O
d	O
k	O
)	O
.	O
Then	O
,	O
we	O
take	O
the	O
maximum	O
over	O
different	O
heads	O
as	O
the	O
final	O
edge	O
probabilities	O
:	O
β	B-HyperparameterName
t	O
[	O
i	O
]	O
=	O
H	O
max	O
h=1	O
β	B-HyperparameterName
h	O
t	O
[	O
i	O
]	O
.	O
Therefore	O
,	O
different	O
heads	O
may	O
points	O
to	O
different	O
nodes	O
at	O
the	O
same	O
time	O
.	O
Intuitively	O
,	O
each	O
head	O
represents	O
a	O
distinct	O
relation	O
detector	O
for	O
a	O
particular	O
set	O
of	O
relation	O
types	O
.	O
For	O
each	O
attention	O
head	O
,	O
it	O
will	O
point	O
to	O
a	O
source	O
node	O
if	O
certain	O
relations	O
exist	O
between	O
the	O
new	O
node	O
and	O
the	O
existing	O
graph	O
,	O
otherwise	O
it	O
will	O
point	O
to	O
the	O
dummy	O
node	O
.	O
An	O
example	O
with	O
four	O
attention	O
heads	O
and	O
three	O
existing	O
nodes	O
(	O
excluding	O
the	O
dummy	O
node	O
)	O
is	O
illustrated	O
in	O
Figure	O
3	O
.	O

As	O
described	O
above	O
,	O
the	O
concept	O
solver	O
and	O
the	O
relation	O
solver	O
are	O
conceptually	O
two	O
attention	O
mechanisms	O
over	O
the	O
sequence	O
and	O
graph	O
respectively	O
,	O
addressing	O
the	O
concept	O
prediction	O
and	O
relation	O
prediction	O
separately	O
.	O
The	O
key	O
is	O
to	O
pass	O
the	O
decisions	O
between	O
the	O
solvers	O
so	O
that	O
they	O
can	O
examine	O
each	O
other	O
's	O
answer	O
and	O
make	O
harmonious	O
decisions	O
.	O
Specifically	O
,	O
at	O
each	O
spanning	O
step	O
i	O
,	O
we	O
start	O
the	O
iterative	O
inference	O
by	O
setting	O
x	O
0	B-DatasetName
=	O
h	O
0	B-DatasetName
and	O
solving	O
f	O
(	O
G	O
i	O
,	O
x	O
0	B-DatasetName
)	O
.	O
After	O
the	O
t	O
-	O
th	O
graph	O
reasoning	O
,	O
we	O
compute	O
the	O
state	O
vector	O
y	O
t	O
,	O
which	O
will	O
be	O
handed	O
over	O
to	O
the	O
concept	O
solver	O
as	O
g	O
(	O
W	O
,	O
y	O
t	O
)	O
,	O
as	O
:	O
y	O
t	O
=	O
FFN	O
(	O
y	O
)	O
(	O
x	O
t	O
+	O
(	O
W	O
V	O
h	O
1	O
:	O
n	O
)	O
α	B-HyperparameterName
t	O
)	O
,	O
where	O
FFN	O
(	O
y	O
)	O
is	O
a	O
feed	O
-	O
forward	O
network	O
and	O
W	O
V	O
projects	O
text	O
memories	O
into	O
a	O
value	O
space	O
.	O
Similarly	O
,	O
after	O
the	O
t	O
-	O
th	O
sequence	O
reasoning	O
,	O
we	O
update	O
the	O
state	O
vector	O
from	O
y	O
t	O
to	O
x	O
t+1	O
as	O
:	O
x	O
t+1	O
=	O
FFN	O
(	O
x	O
)	O
(	O
y	O
t	O
+	O
H	O
h=1	O
(	O
W	O
V	O
h	O
s	O
0	B-DatasetName
:	O
n	O
)	O
β	B-HyperparameterName
h	O
t	O
)	O
,	O
where	O
FFN	O
(	O
x	O
)	O
is	O
a	O
feed	O
-	O
forward	O
network	O
and	O
W	O
V	O
h	O
projects	O
graph	O
memories	O
into	O
a	O
value	O
space	O
for	O
each	O
head	O
h.	O
After	O
N	O
steps	O
of	O
iterative	O
inference	O
,	O
i	O
,	O
e.	O
,	O
x	O
0	B-DatasetName
f	O
(	O
G	O
i	O
,	O
x	O
0	B-DatasetName
)	O
y	O
1	O
g	O
(	O
W	O
,	O
y	O
1	O
)	O
x	O
1	O
f	O
(	O
G	O
i	O
,	O
x	O
N	O
−1	O
)	O
y	O
N	O
g	O
(	O
W	O
,	O
y	O
N	O
)	O
x	O
N	O
,	O
we	O
finally	O
employ	O
a	O
deep	O
biaffine	O
classifier	O
(	O
Dozat	O
and	O
Manning	O
,	O
2016	O
)	O
for	O
edge	O
label	O
prediction	O
.	O
The	O
Algorithm	O
1	O
AMR	B-TaskName
Parsing	I-TaskName
via	O
Graph	O
Sequence	O
Iterative	O
Inference	O
Input	O
:	O
the	O
input	O
sentence	O
W	O
=	O
(	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O
.	O
.	O
,	O
w	O
n	O
)	O
Output	O
:	O
the	O
corresponding	O
AMR	O
graph	O
G	O
//	O
compute	O
text	O
memories	O
1	O
:	O
h	O
0	B-DatasetName
,	O
h	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
n	O
=	O
SequenceEncoder	O
(	O
(	O
BOS	O
,	O
w	O
1	O
,	O
.	O
.	O
.	O
,	O
w	O
n	O
)	O
)	O
//	O
initialize	O
graph	O
2	O
:	O
G	O
0	B-DatasetName
=	O
(	O
nodes=	O
{	O
BOG	O
}	O
,	O
edges=	O
)	O
//	O
start	O
graph	O
expansions	O
3	O
:	O
i	O
=	O
0	B-DatasetName
4	O
:	O
while	O
True	O
do	O
5	O
:	O
s	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
s	O
i	O
=	O
GraphEncoder	O
(	O
G	O
i	O
)	O
//	O
the	O
graph	O
memories	O
can	O
be	O
computed	O
*	O
incrementally	O
*	O
6	O
:	O
x	O
0	B-DatasetName
=	O
h	O
0	B-DatasetName
//	O
iterative	O
inference	O
7	O
:	O
for	O
t	O
1	O
to	O
N	O
do	O
8	O
:	O
y	O
t	O
=	O
f	O
(	O
G	O
i	O
,	O
x	O
t−1	O
)	O
//	O
Seq	O
.	O
Graph	O
9	O
:	O
x	O
t	O
=	O
g	O
(	O
W	O
,	O
y	O
t	O
)	O
//	O
Graph	O
Seq	O
.	O
i	O
=	O
i	O
+	O
1	O
16	O
:	O
end	O
while	O
17	O
:	O
return	O
G	O
i	O
classifier	O
uses	O
a	O
biaffine	O
function	O
to	O
score	O
each	O
label	O
,	O
given	O
the	O
final	O
concept	O
representation	O
x	O
N	O
and	O
the	O
node	O
vector	O
s	O
1	O
:	O
m	O
as	O
input	O
.	O
The	O
resulted	O
concept	O
,	O
edge	O
,	O
and	O
edge	O
label	O
predictions	O
will	O
added	O
to	O
the	O
new	O
graph	O
G	O
i+1	O
if	O
the	O
concept	O
prediction	O
is	O
not	O
EOG	O
,	O
a	O
special	O
concept	O
that	O
we	O
add	O
for	O
indicating	O
termination	O
.	O
Otherwise	O
,	O
the	O
whole	O
parsing	O
process	O
is	O
terminated	O
and	O
the	O
current	O
graph	O
is	O
returned	O
as	O
final	O
result	O
.	O
The	O
complete	O
parsing	O
process	O
adopting	O
the	O
iterative	O
inference	O
is	O
described	O
in	O
Algorithm	O
1	O
.	O

Our	O
model	O
is	O
trained	O
with	O
the	O
standard	O
maximum	O
likelihood	O
estimate	O
.	O
The	O
optimization	O
objective	O
is	O
to	O
maximize	O
the	O
sum	O
of	O
the	O
decomposed	O
step	O
-	O
wise	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
,	O
where	O
each	O
is	O
the	O
sum	O
of	O
concept	O
,	O
edge	O
,	O
and	O
edge	O
label	O
probabilities	O
.	O
To	O
facilitate	O
training	O
,	O
we	O
create	O
a	O
reference	O
generation	O
order	O
of	O
nodes	O
by	O
running	O
a	O
breadth	O
-	O
first	O
-	O
traversal	O
over	O
target	O
AMR	O
graphs	O
,	O
as	O
it	O
is	O
cognitively	O
appealing	O
(	O
core	O
-	O
semantic	O
-	O
first	O
principle	O
,	O
Cai	O
and	O
Lam	O
,	O
2019	O
)	O
and	O
the	O
effectiveness	O
of	O
pre	O
-	O
order	O
traversal	O
is	O
also	O
empirically	O
verified	O
by	O
Zhang	O
et	O
al	O
(	O
2019a	O
)	O
in	O
a	O
depth	O
-	O
first	O
setting	O
.	O
For	O
the	O
generation	O
order	O
for	O
sibling	O
nodes	O
,	O
we	O
adopt	O
the	O
uniformly	O
random	O
order	O
and	O
the	O
deterministic	O
order	O
sorted	O
by	O
the	O
relation	O
frequency	O
in	O
a	O
1	O
:	O
1	O
ratio	O
at	O
first	O
then	O
change	O
to	O
the	O
deterministic	O
order	O
only	O
in	O
the	O
final	O
training	O
steps	O
.	O
We	O
empirically	O
find	O
that	O
the	O
deterministic	O
-	O
afterrandom	O
strategy	O
slightly	O
improves	O
performance	O
.	O
During	O
testing	O
,	O
our	O
model	O
searches	O
for	O
the	O
best	O
output	O
graph	O
through	O
beam	O
search	O
based	O
on	O
the	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
at	O
each	O
spanning	O
step	O
.	O
The	O
time	O
complexity	O
of	O
our	O
model	O
is	O
O	O
(	O
k	O
|	O
V	O
|	O
)	O
,	O
where	O
k	O
is	O
the	O
beam	O
size	O
,	O
and	O
|	O
V	O
|	O
is	O
the	O
number	O
of	O
nodes	O
.	O

The	O
performance	O
of	O
AMR	B-TaskName
parsing	I-TaskName
is	O
conventionally	O
evaluated	O
by	O
SMATCH	O
(	O
F1	B-MetricName
)	O
metric	O
.	O
The	O
left	O
block	O
of	O
Table	O
1	O
shows	O
the	O
SMATCH	O
scores	O
on	O
the	O
AMR	O
2.0	O
test	O
set	O
of	O
our	O
models	O
against	O
the	O
previous	O
best	O
approaches	O
and	O
recent	O
competitors	O
.	O
On	O
AMR	O
2.0	O
,	O
we	O
outperform	O
the	O
latest	O
push	O
from	O
Zhang	O
et	O
al	O
(	O
2019b	O
)	O
by	O
3.2	O
%	O
and	O
,	O
for	O
the	O
first	O
time	O
,	O
obtain	O
a	O
parser	O
with	O
over	O
80	O
%	O
SMATCH	O
score	O
.	O
Note	O
that	O
even	O
without	O
BERT	B-MethodName
,	O
our	O
model	O
still	O
outperforms	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
using	O
BERT	B-MethodName
(	O
Zhang	O
et	O
al	O
,	O
2019b	O
,	O
a	O
)	O
with	O
77.3	O
%	O
.	O
This	O
is	O
particularly	O
remarkable	O
since	O
running	O
BERT	B-MethodName
is	O
computationally	O
expensive	O
.	O
As	O
shown	O
in	O
most	O
models	O
trained	O
on	O
AMR	O
2.0	O
.	O
The	O
even	O
more	O
substantial	O
performance	O
gain	O
on	O
the	O
smaller	O
dataset	O
suggests	O
that	O
our	O
method	O
is	O
both	O
effective	O
and	O
dataefficient	O
.	O
Besides	O
,	O
again	O
,	O
our	O
model	O
without	O
BERT	B-MethodName
already	O
surpasses	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
using	O
BERT	B-MethodName
.	O
For	O
ablated	O
models	O
,	O
it	O
can	O
be	O
observed	O
that	O
our	O
models	O
yield	O
the	O
best	O
results	O
in	O
all	O
settings	O
if	O
there	O
are	O
any	O
competitors	O
,	O
indicating	O
BERT	B-MethodName
and	O
graph	O
re	O
-	O
categorization	O
are	O
not	O
the	O
exclusive	O
key	O
for	O
our	O
superior	O
performance	O
.	O

Effect	O
of	O
Iterative	O
Inference	O
We	O
then	O
turn	O
to	O
study	O
the	O
effect	O
of	O
our	O
key	O
idea	O
,	O
namely	O
,	O
the	O
iterative	O
inference	O
design	O
.	O
To	O
this	O
end	O
,	O
we	O
run	O
a	O
set	O
of	O
experiments	O
with	O
different	O
values	O
of	O
the	O
number	O
of	O
the	O
inference	O
steps	O
N	O
.	O
The	O
results	O
on	O
AMR	O
2.0	O
are	O
shown	O
in	O
Figure	O
4	O
(	O
solid	O
line	O
)	O
.	O
As	O
seen	O
,	O
the	O
performance	O
generally	O
goes	O
up	O
when	O
the	O
number	O
of	O
inference	O
steps	O
increases	O
.	O
The	O
difference	O
is	O
most	O
noticeable	O
between	O
1	O
(	O
no	O
iterative	O
reasoning	O
is	O
performed	O
)	O
and	O
2	O
,	O
while	O
later	O
improvements	O
gradually	O
diminish	O
.	O
One	O
important	O
point	O
here	O
is	O
that	O
the	O
model	O
size	O
in	O
terms	O
of	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
is	O
constant	O
regardless	O
of	O
the	O
number	O
of	O
inference	O
steps	O
,	O
making	O
it	O
different	O
from	O
general	O
over	O
-	O
parameterized	O
problems	O
.	O
For	O
a	O
closer	O
study	O
on	O
the	O
effect	O
of	O
the	O
inference	O
steps	O
with	O
respect	O
to	O
the	O
lengths	O
of	O
input	O
sentences	O
,	O
we	O
group	O
sentences	O
into	O
three	O
classes	O
by	O
length	O
and	O
also	O
show	O
the	O
individual	O
results	O
in	O
Figure	O
4	O
(	O
dashed	O
lines	O
)	O
.	O
As	O
seen	O
,	O
the	O
iterative	O
inference	O
helps	O
more	O
for	O
longer	O
sentences	O
,	O
which	O
confirms	O
our	O
intuition	O
that	O
longer	O
and	O
more	O
complex	O
input	O
needs	O
more	O
reasoning	O
.	O
Another	O
interesting	O
observation	O
is	O
that	O
the	O
performance	O
on	O
shorter	O
sentences	O
reaches	O
the	O
peaks	O
earlier	O
.	O
This	O
observation	O
suggests	O
that	O
the	O
number	O
of	O
inference	O
steps	O
can	O
be	O
adjusted	O
according	O
to	O
the	O
input	O
sentence	O
,	O
which	O
we	O
leave	O
as	O
future	O
work	O
.	O

We	O
are	O
also	O
interested	O
in	O
the	O
effect	O
of	O
beam	O
size	O
during	O
testing	O
.	O
Ideally	O
,	O
if	O
a	O
model	O
is	O
able	O
to	O
make	O
accurate	O
predictions	O
in	O
the	O
first	O
place	O
,	O
it	O
should	O
rely	O
less	O
on	O
the	O
search	O
algorithm	O
.	O
We	O
vary	O
the	O
beam	O
size	O
and	O
plot	O
the	O
curve	O
in	O
Figure	O
6	O
.	O
The	O
results	O
show	O
that	O
the	O
performance	O
generally	O
gets	O
better	O
with	O
larger	O
beam	O
sizes	O
.	O
However	O
,	O
a	O
small	O
beam	O
size	O
of	O
2	O
already	O
gets	O
the	O
most	O
of	O
the	O
credits	O
,	O
which	O
suggests	O
that	O
our	O
model	O
is	O
robust	O
enough	O
for	O
time	O
-	O
stressing	O
environments	O
.	O
Visualization	O
We	O
visualize	O
the	O
iterative	O
reasoning	O
process	O
with	O
a	O
case	O
study	O
in	O
Figure	O
5	O
.	O
We	O
illustrate	O
the	O
values	O
of	O
α	B-HyperparameterName
t	O
,	O
β	B-HyperparameterName
t	O
as	O
the	O
iterative	O
inference	O
progresses	O
.	O
As	O
seen	O
,	O
the	O
parser	O
makes	O
mistakes	O
in	O
the	O
first	O
step	O
,	O
but	O
gradually	O
corrects	O
its	O
decisions	O
and	O
finally	O
makes	O
the	O
right	O
predictions	O
.	O
Later	O
reasoning	O
steps	O
typically	O
provide	O
a	O
sharper	O
attention	O
distribution	O
than	O
earlier	O
steps	O
,	O
narrowing	O
down	O
the	O
most	O
likely	O
answer	O
with	O
more	O
confidence	O
.	O
Speed	O
We	O
also	O
report	O
the	O
parsing	O
speed	O
of	O
our	O
non	O
-	O
optimized	O
code	O
:	O
With	O
BERT	B-MethodName
,	O
the	O
parsing	O
speed	O
of	O
our	O
system	O
is	O
about	O
300	O
tokens	O
/	O
s	O
,	O
while	O
without	O
BERT	B-MethodName
,	O
it	O
is	O
about	O
330	O
tokens	O
/	O
s	O
on	O
a	O
single	O
Nvidia	O
P4	O
GPU	O
.	O
The	O
absolute	O
speed	O
depends	O
on	O
various	O
implementation	O
choices	O
and	O
hardware	O
performance	O
.	O
In	O
theory	O
,	O
the	O
time	O
complexity	O
of	O
our	O
parsing	O
algorithm	O
is	O
O	O
(	O
kbn	O
)	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
iterative	O
steps	O
,	O
b	O
is	O
beam	O
size	O
,	O
and	O
n	O
is	O
the	O
graph	O
size	O
(	O
number	O
of	O
nodes	O
)	O
respectively	O
.	O
It	O
is	O
important	O
to	O
note	O
that	O
our	O
algorithm	O
is	O
linear	O
in	O
the	O
graph	O
size	O
.	O

A	O
Hyper	O
-	O
parameter	O
Settings	O
Table	O
3	O
lists	O
the	O
hyper	O
-	O
parameters	O
used	O
in	O
our	O
full	O
models	O
.	O
Char	O
-	O
level	O
CNNs	O
and	O
Transformer	B-MethodName
layers	O
in	O
the	O
sentence	O
encoder	O
and	O
the	O
graph	O
encoder	O
share	O
the	O
same	O
hyper	O
-	O
parameter	O
settings	O
.	O
The	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
we	O
used	O
is	O
the	O
Huggingface	O
's	O
implementation	O
(	O
Wolf	O
et	O
al	O
,	O
2019	O
)	O
(	O
bert	O
-	O
base	O
-	O
cased	O
)	O
.	O
To	O
mitigate	O
overfitting	O
,	O
we	O
apply	O
dropout	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
with	O
the	O
drop	O
rate	O
0.2	O
between	O
different	O
layers	O
.	O
We	O
randomly	O
mask	O
(	O
replacing	O
inputs	O
with	O
a	O
special	O
UNK	O
token	O
)	O
the	O
input	O
lemmas	O
,	O
POS	O
tags	O
,	O
and	O
NER	B-TaskName
tags	O
with	O
a	O
rate	O
of	O
0.33	O
.	O
Parameter	O
optimization	O
is	O
performed	O
with	O
the	O
ADAM	B-DatasetName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
β	B-HyperparameterName
1	O
=	O
0.9	O
and	O
β	B-HyperparameterName
2	O
=	O
0.999	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
schedule	O
is	O
similar	O
to	O
that	O
in	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
,	O
with	O
warm	O
-	O
up	O
steps	O
being	O
set	O
to	O
2K.	O
We	O
use	O
early	B-MethodName
stopping	I-MethodName
on	O
the	O
development	O
set	O
for	O
choosing	O
the	O
best	O
model	O
.	O

Input	O
strictly	O
local	O
(	O
ISL	O
)	O
functions	O
are	O
a	O
class	O
of	O
subregular	O
transductions	O
that	O
have	O
well	O
-	O
understood	O
mathematical	O
and	O
computational	O
properties	O
and	O
that	O
are	O
sufficiently	O
expressive	O
to	O
account	O
for	O
a	O
wide	O
variety	O
of	O
attested	O
morphological	O
and	O
phonological	O
patterns	O
(	O
e.g.	O
,	O
Chandlee	O
,	O
2014	O
;	O
Chandlee	O
,	O
2017	O
;	O
.	O
In	O
this	O
study	O
,	O
we	O
compared	O
several	O
approaches	O
to	O
learning	O
ISL	O
functions	O
:	O
the	O
ISL	O
function	O
learning	O
algorithm	O
(	O
ISLFLA	O
;	O
Chandlee	O
,	O
2014	O
;	O
and	O
the	O
classic	O
OSTIA	O
learner	O
to	O
which	O
it	O
is	O
related	O
(	O
Oncina	O
et	O
al	O
,	O
1993	O
)	O
;	O
the	O
Minimal	O
Generalization	O
Learner	O
(	O
MGL	O
;	O
Hayes	O
,	O
2002	O
,	O
2003	O
)	O
;	O
and	O
a	O
novel	O
deep	O
neural	O
network	O
model	O
presented	O
here	O
(	O
DNN	O
-	O
ISL	O
)	O
.	O
The	O
four	O
models	O
were	O
evaluated	O
on	O
their	O
ability	O
to	O
learn	O
the	O
mapping	O
from	O
feminine	O
singular	O
(	O
fem.sg	O
.	O
)	O
to	O
masculine	O
singular	O
(	O
masc.sg	O
.	O
)	O
surface	O
forms	O
of	O
Catalan	O
adjectives	O
(	O
and	O
,	O
separately	O
,	O
from	O
provided	O
underlying	O
representations	O
to	O
fem.sg	O
.	O
and	O
masc.sg	O
.	O
surface	O
forms	O
)	O
.	O
The	O
mappings	O
to	O
masc.sg	O
.	O
forms	O
in	O
Catalan	O
involve	O
several	O
phonological	O
modifications	O
at	O
the	O
right	O
edge	O
of	O
the	O
word	O
(	O
e.g.	O
,	O
Mascaró	O
,	O
1976	O
)	O
,	O
the	O
empirical	O
focus	O
of	O
our	O
study	O
.	O
The	O
relevant	O
processes	O
include	O
obstruent	O
devoicing	O
and	O
strengthening	O
(	O
e.g.	O
,	O
[	O
rOZ@	O
]	O
fem.sg	O
.	O
[	O
rOtS	O
]	O
masc.sg	O
.	O
'	O
red	O
'	O
)	O
,	O
post	O
-	O
tonic	O
n	O
-	O
Deletion	O
(	O
e.g.	O
,	O
[	O
san@	O
]	O
[	O
sa	O
]	O
'	O
healthy	O
'	O
)	O
,	O
and	O
cluster	O
simplification	O
(	O
e.g.	O
,	O
[	O
blaNk@	O
]	O
[	O
blaN	O
]	O
'	O
white	O
'	O
)	O
.	O
There	O
are	O
opaque	O
,	O
counterfeeding	O
interactions	O
among	O
some	O
of	O
the	O
processes	O
(	O
e.g.	O
,	O
[	O
f@kund@	O
]	O
[	O
f@kun	O
]	O
/	O
*	O
[	O
f@ku	O
]	O
'	O
fertile	O
'	O
)	O
,	O
consistent	O
with	O
the	O
idea	O
that	O
the	O
mappings	O
are	O
input	O
-	O
rather	O
than	O
output	O
-	O
determined	O
(	O
see	O
.	O
A	O
small	O
number	O
of	O
apparent	O
lexical	O
exceptions	O
to	O
the	O
typical	O
modification	O
pattern	O
(	O
e.g.	O
,	O
[	O
blan@	O
]	O
[	O
blan	O
]	O
/	O
*	O
[	O
bla	O
]	O
'	O
soft	O
'	O
)	O
are	O
problematic	O
for	O
ISL	O
learners	O
that	O
assume	O
perfect	O
homogeneity	O
.	O
Our	O
main	O
findings	O
were	O
that	O
the	O
DNN	O
-	O
ISL	O
learner	O
achieved	O
high	O
accuracy	B-MetricName
on	O
the	O
Catalan	O
data	O
,	O
with	O
MGL	O
coming	O
in	O
a	O
close	O
second	O
,	O
while	O
ISLFLA	O
and	O
OSTIA	O
performed	O
much	O
worse	O
-	O
either	O
failing	O
to	O
learn	O
any	O
mapping	O
at	O
all	O
or	O
predicting	O
the	O
correct	O
output	O
for	O
less	O
than	O
5	O
%	O
of	O
held	O
-	O
out	O
cases	O
,	O
even	O
when	O
lexical	O
exceptions	O
were	O
removed	O
from	O
the	O
data	O
(	O
see	O
Table	O
1	O
)	O
.	O

For	O
the	O
purposes	O
of	O
this	O
abstract	O
,	O
we	O
assume	O
familiarity	O
with	O
ISLFLA	O
,	O
OSTIA	O
,	O
and	O
MGL	O
.	O
We	O
verified	O
that	O
the	O
implementation	O
of	O
MGL	O
learns	O
only	O
ISL	O
phonological	O
rules	O
-	O
rules	O
conditioned	O
on	O
local	O
phonological	O
context	O
in	O
the	O
result	O
of	O
a	O
morphological	O
operation	O
such	O
as	O
affixation	O
or	O
truncation	O
-	O
a	O
connection	O
that	O
has	O
not	O
previously	O
been	O
made	O
in	O
the	O
literature	O
.	O
The	O
deep	O
neural	O
network	O
model	O
proposed	O
here	O
(	O
DNN	O
-	O
ISL	O
)	O
also	O
applies	O
morphological	O
operations	O
followed	O
by	O
phonological	O
modifications	O
,	O
the	O
latter	O
being	O
implemented	O
with	O
weighted	O
constraints	O
rather	O
than	O
rules	O
.	O
A	O
phonological	O
constraint	O
as	O
learned	O
by	O
DNN	O
-	O
ISL	O
is	O
defined	O
by	O
:	O
a	O
three	O
-	O
segment	O
featural	O
pattern	O
specifying	O
the	O
input	O
context	O
to	O
which	O
the	O
constraint	O
applies	O
;	O
a	O
preference	O
for	O
one	O
type	O
of	O
modification	O
applied	O
to	O
the	O
center	O
segment	O
of	O
the	O
context	O
(	O
i.e.	O
,	O
deletion	O
,	O
epenthesis	O
before	O
/	O
after	O
,	O
or	O
feature	O
change	O
)	O
;	O
target	O
output	O
features	O
in	O
the	O
case	O
of	O
epenthesis	O
or	O
change	O
;	O
and	O
a	O
real	O
-	O
valued	O
strength	O
.	O
Each	O
constraint	O
computes	O
the	O
degree	O
to	O
which	O
its	O
context	O
matches	O
every	O
three	O
-	O
segment	O
window	O
in	O
the	O
input	O
(	O
i.e.	O
,	O
it	O
applies	O
a	O
novel	O
feature	O
based	O
convolution	B-MethodName
operation	O
to	O
the	O
input	O
)	O
and	O
imposes	O
its	O
preferred	O
modification	O
in	O
proportion	O
to	O
the	O
degree	O
of	O
match	O
and	O
its	O
strength	O
.	O
These	O
preferences	O
are	O
summed	O
over	O
constraints	O
for	O
each	O
input	O
position	O
and	O
applied	O
to	O
the	O
positions	O
independently	O
to	O
derive	O
the	O
phonological	O
output	O
.	O
The	O
parameters	O
of	O
the	O
constraints	O
are	O
straightforwardly	O
interpretable	O
and	O
visualizable	O
as	O
real	O
-	O
valued	O
phonological	O
feature	O
coefficients	O
,	O
modification	O
-	O
type	O
logits	O
,	O
and	O
strengths	O
.	O
The	O
model	O
is	O
fully	O
differentiable	O
and	O
was	O
trained	O
with	O
the	O
Adagrad	B-MethodName
optimizer	B-HyperparameterName
on	O
small	O
mini	O
-	O
batches	O
for	O
20	O
epochs	O
.	O

Coreference	B-TaskName
resolution	I-TaskName
(	O
CR	O
)	O
is	O
an	O
essential	O
part	O
of	O
discourse	O
analysis	O
.	O
Most	O
recently	O
,	O
neural	O
approaches	O
have	O
been	O
proposed	O
to	O
improve	O
over	O
SOTA	O
models	O
from	O
earlier	O
paradigms	O
.	O
So	O
far	O
none	O
of	O
the	O
published	O
neural	O
models	O
leverage	O
external	O
semantic	O
knowledge	O
such	O
as	O
type	O
information	O
.	O
This	O
paper	O
offers	O
the	O
first	O
such	O
model	O
and	O
evaluation	O
,	O
demonstrating	O
modest	O
gains	O
in	O
accuracy	B-MetricName
by	O
introducing	O
either	O
gold	O
standard	O
or	O
predicted	O
types	O
.	O
In	O
the	O
proposed	O
approach	O
,	O
type	O
information	O
serves	O
both	O
to	O
(	O
1	O
)	O
improve	O
mention	O
representation	O
and	O
(	O
2	O
)	O
create	O
a	O
soft	O
type	O
consistency	O
check	O
between	O
coreference	O
candidate	O
mentions	O
.	O
Our	O
evaluation	O
covers	O
two	O
different	O
grain	O
sizes	O
of	O
types	O
over	O
four	O
different	O
benchmark	O
corpora	O
.	O

In	O
this	O
section	O
,	O
we	O
provide	O
the	O
results	O
of	O
our	O
empirical	O
experiments	O
.	O
Evaluation	O
Metrics	O
:	O
We	O
convert	O
all	O
three	O
datasets	O
into	O
the	O
CoNLL	B-DatasetName
2012	I-DatasetName
format	O
and	O
report	O
the	O
F1	B-MetricName
score	I-MetricName
for	O
MUC	O
,	O
B	O
3	O
,	O
and	O
CEAF	O
metrics	O
using	O
the	O
CoNLL	O
-	O
2012	O
official	O
scripts	O
.	O
The	O
performances	O
are	O
compared	O
on	O
the	O
average	B-MetricName
F1	I-MetricName
of	O
the	O
abovementioned	O
metrics	O
.	O
For	O
EmailCoref	O
,	O
OntoNotes	B-DatasetName
,	O
and	O
WikiCoref	B-DatasetName
,	O
we	O
report	O
the	O
mean	O
score	O
of	O
5	O
independent	O
runs	O
of	O
the	O
model	O
with	O
different	O
seeds	B-DatasetName
.	O
Whereas	O
,	O
for	O
LitBank	B-DatasetName
,	O
we	O
present	O
the	O
10	O
-	O
fold	O
cross	O
-	O
validation	O
results	O
.	O
5	O

In	O
order	O
to	O
establish	O
an	O
upper	O
bound	O
for	O
improvement	O
through	O
introduction	O
of	O
type	O
information	O
,	O
our	O
first	O
experiment	O
leverages	O
the	O
original	O
list	O
of	O
entity	O
-	O
types	O
annotated	O
in	O
different	O
corpora	O
(	O
+	O
ET	O
(	O
orig	O
)	O
)	O
,	O
using	O
the	O
gold	O
standard	O
labels	O
for	O
types	O
.	O
Inclusion	O
of	O
entity	O
-	O
type	O
information	O
improves	O
over	O
the	O
baseline	O
for	O
all	O
CR	O
datasets	O
.	O
Table	O
2	O
presents	O
the	O
performance	O
of	O
the	O
baseline	O
model	O
and	O
the	O
model	O
with	O
entity	O
-	O
type	O
information	O
.	O
We	O
find	O
that	O
entity	O
-	O
type	O
information	O
gives	O
a	O
boost	O
of	O
0.96	O
Avg	O
.	O
F1	B-MetricName
(	O
p	O
<	O
0.01	O
)	O
on	O
LitBank	B-DatasetName
which	O
is	O
the	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
score	O
with	O
goldmentions	O
.	O
This	O
suggests	O
that	O
type	O
information	O
is	O
helpful	O
for	O
CR	O
on	O
LitBank	B-DatasetName
despite	O
the	O
heavily	O
skewed	O
distribution	O
of	O
entity	O
-	O
types	O
in	O
this	O
corpus	O
.	O
Similarly	O
,	O
type	O
information	O
also	O
benefits	O
Email	O
-	O
Coref	O
and	O
WikiCoref	B-DatasetName
resulting	O
in	O
an	O
absolute	O
improvement	O
of	O
1.67	O
and	O
2.9	O
Avg	O
.	O
F1	B-MetricName
points	O
respectively	O
(	O
p	O
<	O
0.01	O
)	O
.	O
We	O
also	O
see	O
a	O
2.4	O
Avg	O
.	O
F1	B-MetricName
improvement	O
(	O
p	O
<	O
0.01	O
)	O
on	O
OntoNotes	B-DatasetName
,	O
the	O
largest	O
dataset	O
in	O
this	O
study	O
.	O
This	O
suggests	O
that	O
explicit	O
access	O
to	O
type	O
information	O
is	O
beneficial	O
all	O
over	O
the	O
board	O
,	O
despite	O
the	O
use	O
of	O
contextual	O
representations	O
which	O
have	O
been	O
claimed	O
to	O
model	O
realworld	O
facts	O
and	O
relationships	O
(	O
Petroni	O
et	O
al	O
,	O
2019	O
)	O
.	O
Ablation	O
Results	O
:	O
To	O
understand	O
the	O
contribution	O
of	O
the	O
inclusion	O
of	O
type	O
information	O
to	O
improve	O
mention	O
representation	O
(	O
+	O
ET	O
-	O
self	O
)	O
and	O
type	O
consistency	O
check	O
between	O
candidate	O
mentions	O
(	O
+	O
ET	O
-	O
cross	O
)	O
,	O
we	O
perform	O
an	O
ablation	O
study	O
(	O
Table	O
3	O
)	O
.	O
We	O
find	O
that	O
both	O
components	O
consistently	O
provide	O
significant	O
performance	O
boosts	O
over	O
the	O
baseline	O
.	O
However	O
,	O
their	O
combination	O
(	O
+	O
ET	O
)	O
performs	O
the	O
best	O
across	O
all	O
datasets	O
.	O

Our	O
hypothesis	O
around	O
the	O
use	O
of	O
entity	O
-	O
types	O
was	O
to	O
provide	O
additional	O
information	O
to	O
the	O
model	O
that	O
could	O
be	O
leveraged	O
to	O
minimize	O
errors	O
due	O
to	O
type	O
mismatch	O
in	O
CR	O
.	O
To	O
evaluate	O
if	O
the	O
F1	B-MetricName
score	I-MetricName
improvements	O
achieved	O
by	O
+	O
ET	O
models	O
are	O
because	O
of	O
fewer	O
type	O
mismatch	O
errors	O
,	O
we	O
report	O
the	O
number	O
of	O
coreference	O
clusters	O
detected	O
by	O
the	O
model	O
that	O
contain	O
at	O
least	O
one	O
element	O
with	O
a	O
type	O
that	O
is	O
different	O
from	O
the	O
others	O
in	O
the	O
cluster	O
.	O
Since	O
all	O
of	O
the	O
datasets	O
used	O
in	O
this	O
work	O
only	O
consider	O
identity	O
coreferences	O
(	O
Recasens	O
et	O
al	O
,	O
2011	O
)	O
-	O
with	O
potentially	O
varied	O
definitions	O
of	O
identity	O
(	O
Bamman	O
et	O
al	O
,	O
2020	O
;	O
Pradhan	O
et	O
al	O
,	O
2012	O
)	O
-	O
where	O
the	O
mention	O
is	O
a	O
linguistic	O
"	O
re	O
-	O
packaging	O
"	O
of	O
its	O
antecedent	O
,	O
this	O
measure	O
makes	O
sense	O
.	O
As	O
shown	O
in	O
Table	O
2	O
,	O
the	O
models	O
that	O
score	O
lower	O
on	O
the	O
impurity	O
measure	O
get	O
a	O
higher	O
Avg	B-MetricName
F1	I-MetricName
.	O
This	O
suggests	O
that	O
the	O
aggregate	O
performance	O
improvements	O
are	O
at	O
least	O
partly	O
due	O
to	O
the	O
better	O
mention	O
-	O
mention	O
comparison	O
in	O
+	O
ET	O
systems	O
.	O

Type	B-TaskName
Prediction	I-TaskName
:	O
Our	O
final	O
evaluation	O
of	O
the	O
use	O
of	O
types	O
in	O
coreference	O
is	O
perhaps	O
the	O
most	O
important	O
one	O
as	O
it	O
uses	O
predicted	O
types	O
rather	O
than	O
annotated	O
types	O
,	O
thus	O
demonstrating	O
that	O
the	O
benefits	O
can	O
be	O
achieved	O
in	O
practice	O
.	O
Here	O
we	O
use	O
the	O
Type	B-TaskName
Prediction	I-TaskName
Model	O
described	O
just	O
above	O
.	O
We	O
limit	O
the	O
length	O
of	O
the	O
input	O
sequence	O
to	O
128	O
tokens	O
and	O
use	O
BERT	B-MethodName
-	O
base	O
-	O
cased	O
model	O
for	O
our	O
type	O
-	O
prediction	O
experiments	O
.	O
We	O
perform	O
a	O
fivefold	O
cross	O
-	O
validation	O
to	O
predict	O
the	O
type	O
for	O
each	O
mention	O
in	O
the	O
dataset	O
.	O
Since	O
all	O
four	O
datasets	O
suffer	O
from	O
class	O
-	O
imbalance	O
,	O
we	O
report	O
both	O
Macro	B-MetricName
F1	I-MetricName
score	O
as	O
well	O
as	O
the	O
accuracy	B-MetricName
for	O
the	O
model	O
.	O
The	O
model	O
is	O
trained	O
for	O
20	O
epochs	O
,	O
with	O
earlystopping	O
(	O
patience	O
=	O
10	O
)	O
,	O
and	O
is	O
fine	O
-	O
tuned	O
on	O
the	O
development	O
set	O
for	O
Macro	B-MetricName
F1	I-MetricName
to	O
give	O
more	O
importance	O
to	O
minority	O
type	O
categories	O
.	O
We	O
do	O
not	O
consider	O
NA	O
as	O
a	O
separate	O
class	O
during	O
type	B-TaskName
prediction	I-TaskName
for	O
WikiCoref	B-DatasetName
and	O
OntoNotes	B-DatasetName
.	O
For	O
evaluation	O
of	O
our	O
type	O
-	O
prediction	O
model	O
,	O
we	O
ignore	O
the	O
mentions	O
that	O
do	O
not	O
have	O
an	O
associated	O
gold	O
type	O
(	O
NA	O
)	O
from	O
the	O
final	O
numbers	O
in	O
Table	O
4	O
.	O
As	O
shown	O
,	O
our	O
model	O
performs	O
well	O
on	O
Lit	O
-	O
Bank	O
,	O
EmailCoref	O
,	O
and	O
Ontonotes	B-DatasetName
due	O
to	O
their	O
favorable	O
size	O
in	O
terms	O
of	O
training	O
samples	O
for	O
the	O
BERT	B-MethodName
-	O
based	O
type	O
predictor	O
.	O
WikiCoref	B-DatasetName
,	O
however	O
,	O
proves	O
more	O
challenging	O
as	O
the	O
model	O
only	O
manages	O
38.0	O
Macro	B-MetricName
F1	I-MetricName
points	O
with	O
original	O
(	O
orig	O
)	O
types	O
and	O
45.0	O
with	O
common	O
types	O
(	O
com	O
)	O
,	O
portraying	O
its	O
lack	O
of	O
ability	O
to	O
learn	O
minority	O
type	O
categories	O
with	O
less	O
data	O
.	O
Furthermore	O
,	O
our	O
model	O
finds	O
it	O
easier	O
to	O
predict	O
the	O
common	O
(	O
com	O
)	O
set	O
of	O
types	O
for	O
each	O
dataset	O
as	O
combining	O
multiple	O
corpus	O
-	O
specific	O
types	O
into	O
one	O
partially	O
alleviates	O
the	O
problem	O
of	O
class	O
-	O
imbalance	O
.	O
In	O
line	O
with	O
our	O
expectation	O
,	O
the	O
largest	O
improvement	O
due	O
to	O
common	O
types	O
is	O
seen	O
for	O
OntoNotes	B-DatasetName
where	O
the	O
prob	O
-	O
lem	O
reduces	O
from	O
an	O
18	O
-	O
way	O
classification	O
to	O
a	O
5way	O
classification	O
.	O
Coreference	B-TaskName
Resolution	I-TaskName
:	O
Each	O
mention	O
in	O
the	O
corpus	O
occurs	O
in	O
the	O
test	O
-	O
sets	O
of	O
the	O
five	O
-	O
fold	O
cross	O
-	O
validation	O
type	O
-	O
prediction	O
experiments	O
exactly	O
once	O
.	O
This	O
allows	O
us	O
to	O
infer	O
the	O
type	O
of	O
each	O
mention	O
using	O
the	O
model	O
that	O
is	O
trained	O
on	O
a	O
different	O
subset	O
of	O
the	O
dataset	O
.	O
These	O
inferred	O
types	O
are	O
used	O
in	O
the	O
training	O
and	O
testing	O
of	O
the	O
CR	O
systems	O
in	O
a	O
manner	O
similar	O
to	O
the	O
annotated	O
types	O
.	O
Empirically	O
,	O
we	O
found	O
that	O
the	O
above	O
configuration	O
performs	O
better	O
than	O
using	O
the	O
+	O
ET	O
models	O
trained	O
with	O
annotated	O
types	O
and	O
testing	O
with	O
predicted	O
types	O
,	O
as	O
the	O
former	O
exposes	O
the	O
CR	O
models	O
to	O
the	O
noisy	O
types	O
during	O
training	O
thus	O
allowing	O
them	O
to	O
learn	O
weights	O
that	O
take	O
this	O
noise	O
into	O
account	O
.	O
We	O
report	O
the	O
results	O
for	O
both	O
original	O
(	O
+	O
ET	O
-	O
pred	O
(	O
orig	O
)	O
)	O
and	O
common	O
(	O
+	O
ET	O
-	O
pred	O
(	O
com	O
)	O
)	O
type	O
categories	O
on	O
each	O
dataset	O
.	O
Table	O
5	O
shows	O
the	O
results	O
for	O
performance	O
of	O
the	O
baseline	O
and	O
the	O
type	O
-	O
informed	O
models	O
on	O
the	O
four	O
datasets	O
,	O
where	O
the	O
types	O
are	O
inferred	O
from	O
the	O
model	O
described	O
in	O
Section	O
6.1	O
.	O
We	O
find	O
that	O
the	O
improvements	O
from	O
type	O
-	O
information	O
persist	O
across	O
LitBank	B-DatasetName
,	O
EmailCoref	O
,	O
and	O
OntoNotes	B-DatasetName
despite	O
the	O
use	O
of	O
predicted	O
types	O
,	O
but	O
,	O
quite	O
expectedly	O
,	O
remain	O
smaller	O
than	O
the	O
improvements	O
from	O
the	O
gold	O
annotated	O
types	O
.	O
Scores	O
on	O
WikiCoref	B-DatasetName
show	O
no	O
significant	O
improvement	O
over	O
the	O
baseline	O
,	O
which	O
could	O
be	O
explained	O
by	O
the	O
poor	O
performance	O
of	O
the	O
type	B-TaskName
prediction	I-TaskName
model	O
on	O
this	O
dataset	O
which	O
reduces	O
the	O
potency	O
of	O
the	O
feature	O
for	O
CR	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
attention	O
model	O
in	O
PBAN	O
consists	O
of	O
two	O
parts	O
:	O
including	O
the	O
aspect	O
term	O
to	O
the	O
position	O
-	O
aware	O
sentence	O
part	O
and	O
a	O
position	O
-	O
aware	O
sentence	O
to	O
the	O
aspect	O
term	O
part	O
.	O
For	O
the	O
former	O
part	O
,	O
we	O
can	O
obtain	O
the	O
different	O
hidden	O
contextual	O
representation	O
of	O
a	O
sentence	O
according	O
to	O
different	O
word	O
in	O
aspect	O
term	O
.	O
For	O
the	O
later	O
part	O
,	O
we	O
can	O
obtain	O
the	O
attention	O
weights	O
of	O
the	O
words	O
in	O
aspect	O
term	O
according	O
to	O
the	O
position	O
information	O
,	O
which	O
is	O
used	O
for	O
getting	O
the	O
final	O
representation	O
of	O
a	O
sentence	O
.	O
Details	O
will	O
be	O
described	O
in	O
follwing	O
sections	O
.	O
Aspect	O
term	O
to	O
position	O
-	O
aware	O
sentence	O
attention	O
:	O
A	O
sentence	O
should	O
be	O
represented	O
differently	O
based	O
on	O
different	O
words	O
in	O
aspect	O
term	O
,	O
because	O
different	O
words	O
may	O
have	O
different	O
effects	O
on	O
the	O
final	O
representation	O
of	O
the	O
sentence	O
.	O
We	O
firstly	O
get	O
the	O
hidden	O
contextual	O
representation	O
of	O
the	O
aspect	O
term	O
by	O
the	O
left	O
Bi	O
-	O
GRU	B-MethodName
,	O
and	O
get	O
the	O
hidden	O
contextual	O
representation	O
of	O
inputs	O
(	O
i.e.	O
,	O
the	O
concatenation	O
of	O
word	O
embedding	O
and	O
position	O
embedding	O
)	O
by	O
the	O
right	O
Bi	O
-	O
GRU	B-MethodName
structure	O
.	O
Here	O
,	O
we	O
regard	O
the	O
position	O
embedding	O
as	O
the	O
part	O
of	O
the	O
inputs	O
,	O
because	O
it	O
intuitively	O
represents	O
the	O
relative	O
distance	O
of	O
words	O
in	O
a	O
sentence	O
to	O
the	O
current	O
aspect	O
term	O
as	O
mentioned	O
in	O
section	O
2.1	O
.	O
Then	O
we	O
calculate	O
the	O
attention	O
weights	O
by	O
adopting	O
hidden	O
contextual	O
representation	O
of	O
aspect	O
term	O
and	O
inputs	O
,	O
obtaining	O
the	O
attention	O
weight	O
distribution	O
of	O
sentence	O
corresponding	O
to	O
each	O
word	O
in	O
this	O
aspect	O
term	O
.	O
It	O
can	O
be	O
formulated	O
as	O
follows	O
:	O
s	O
i	O
=	O
N	O
j=1	O
α	B-HyperparameterName
ij	O
h	O
j	O
(	O
2	O
)	O
α	B-HyperparameterName
ij	O
=	O
exp	O
(	O
f	O
(	O
h	O
j	O
,	O
h	O
t	O
i	O
)	O
)	O
N	O
k=1	O
exp	O
(	O
f	O
(	O
h	O
k	O
,	O
h	O
t	O
i	O
)	O
)	O
(	O
3	O
)	O
f	O
(	O
h	O
j	O
,	O
h	O
t	O
i	O
)	O
=	O
tanh	O
(	O
h	O
T	O
j	O
W	O
m	O
h	O
t	O
i	O
+	O
b	O
m	O
)	O
(	O
4	O
)	O
where	O
α	B-HyperparameterName
ij	O
indicates	O
the	O
attention	O
weights	O
from	O
the	O
word	O
h	O
t	O
i	O
in	O
the	O
aspect	O
term	O
to	O
the	O
j	O
-	O
th	O
word	O
in	O
the	O
inputs	O
,	O
and	O
tanh	O
is	O
a	O
non	O
-	O
liner	O
activation	B-HyperparameterName
function	I-HyperparameterName
.	O
W	O
m	O
is	O
the	O
weight	O
matrix	O
and	O
b	O
m	O
is	O
the	O
bias	O
.	O
Subsequently	O
,	O
α	B-HyperparameterName
ij	O
is	O
used	O
to	O
compute	O
a	O
weighted	O
sum	O
of	O
the	O
hidden	O
representation	O
s	O
i	O
,	O
producing	O
a	O
semantic	O
vector	O
that	O
represents	O
the	O
input	O
sequence	O
.	O
Position	O
-	O
aware	O
sentence	O
attention	O
to	O
aspect	O
term	O
:	O
As	O
we	O
mentioned	O
above	O
,	O
different	O
words	O
in	O
aspect	O
term	O
will	O
play	O
different	O
role	O
in	O
judging	O
the	O
sentiment	O
polarity	O
of	O
aspect	O
term	O
.	O
Since	O
we	O
obtain	O
the	O
hidden	O
contextual	O
representation	O
of	O
the	O
inputs	O
by	O
the	O
right	O
Bi	O
-	O
GRU	B-MethodName
,	O
we	O
utilize	O
both	O
the	O
position	O
and	O
semantic	O
information	O
for	O
calculating	O
the	O
attention	O
weights	O
of	O
different	O
words	O
in	O
aspect	O
term	O
.	O
The	O
process	O
can	O
be	O
formulated	O
as	O
follows	O
:	O
h	O
R	O
=	O
M	O
i=1	O
γ	B-HyperparameterName
i	O
s	O
i	O
(	O
5	O
)	O
γ	B-HyperparameterName
i	O
=	O
exp	O
(	O
f	O
(	O
h	O
,	O
h	O
t	O
i	O
)	O
)	O
M	O
k=1	O
exp	O
(	O
f	O
(	O
h	O
,	O
h	O
t	O
k	O
)	O
)	O
(	O
6	O
)	O
f	O
(	O
h	O
,	O
h	O
t	O
i	O
)	O
=	O
tanh	O
(	O
h	O
T	O
W	O
n	O
h	O
t	O
i	O
+	O
b	O
n	O
)	O
(	O
7	O
)	O
h	O
=	O
1	O
N	O
N	O
i=1	O
h	O
i	O
(	O
8	O
)	O
where	O
γ	B-HyperparameterName
i	O
stands	O
for	O
the	O
attention	O
weights	O
from	O
inputs	O
to	O
the	O
words	O
in	O
aspect	O
term	O
,	O
denoting	O
which	O
word	O
in	O
aspect	O
term	O
should	O
be	O
more	O
focused	O
.	O
h	O
is	O
calculated	O
by	O
averagely	O
pooling	O
all	O
Bi	O
-	O
GRU	B-MethodName
hidden	O
states	O
.	O
Later	O
,	O
the	O
sequence	O
representation	O
x	O
is	O
obtained	O
by	O
using	O
a	O
non	O
-	O
linear	B-MethodName
layer	I-MethodName
:	O
x	O
=	O
tanh	O
(	O
W	O
R	O
h	O
R	O
+	O
b	O
R	O
)	O
(	O
9	O
)	O
where	O
W	O
R	O
and	O
b	O
R	O
are	O
weight	O
matrix	O
and	O
bias	O
respectively	O
.	O
We	O
feed	O
x	O
into	O
a	O
linear	B-MethodName
layer	I-MethodName
,	O
the	O
length	O
of	O
whose	O
output	O
equals	O
to	O
the	O
number	O
of	O
class	O
labels	O
S	O
.	O
Finally	O
,	O
we	O
add	O
a	O
softmax	B-MethodName
layer	O
to	O
compute	O
the	O
probability	O
distribution	O
for	O
judging	O
the	O
sentiment	O
polarities	O
as	O
positive	O
,	O
negative	O
or	O
neutral	O
:	O
y	O
=	O
sof	B-DatasetName
tmax	O
(	O
W	O
s	O
x	O
+	O
b	O
s	O
)	O
(	O
10	O
)	O
where	O
W	O
s	O
and	O
b	O
s	O
are	O
the	O
weight	O
matrix	O
and	O
bias	O
respectively	O
for	O
softmax	B-MethodName
layer	O
.	O

The	O
PBAN	O
model	O
can	O
be	O
trained	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
way	O
in	O
a	O
supervised	O
learning	O
framework	O
,	O
the	O
aim	O
of	O
the	O
training	O
is	O
to	O
optimize	O
all	O
the	O
parameters	O
so	O
as	O
to	O
minimize	O
the	O
objective	O
function	O
(	O
loss	B-MetricName
function	O
)	O
as	O
much	O
as	O
possible	O
.	O
In	O
our	O
work	O
,	O
let	O
y	O
i	O
be	O
the	O
correct	O
sentiment	O
polarity	O
,	O
which	O
is	O
represented	O
by	O
one	O
-	O
hot	O
vector	O
,	O
and	O
y	O
i	O
denotes	O
the	O
predicted	O
sentiment	O
polarity	O
for	O
the	O
given	O
sentence	O
.	O
We	O
regard	O
the	O
cross	O
-	O
entropy	O
as	O
the	O
loss	B-MetricName
function	O
,	O
and	O
the	O
formula	O
is	O
as	O
follows	O
:	O
loss	B-MetricName
=	O
−	O
S	O
i=1	O
y	O
i	O
log	O
(	O
y	O
i	O
)	O
+	O
1	O
2	O
λ	O
θ	B-HyperparameterName
2	O
(	O
11	O
)	O
where	O
λ	O
is	O
the	O
regularization	O
factor	O
and	O
θ	B-HyperparameterName
contains	O
all	O
the	O
parameters	O
.	O
Furthermore	O
,	O
in	O
order	O
to	O
avoid	O
over	O
-	O
fitting	O
,	O
we	O
adopt	O
the	O
dropout	O
strategy	O
to	O
enhance	O
our	O
PBAN	O
model	O
.	O

Parameters	O
Setting	O
:	O
In	O
our	O
experiments	O
,	O
all	O
word	O
embedding	O
are	O
initialized	O
by	O
the	O
pre	O
-	O
trained	O
Glove	O
vector	O
2	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
.	O
All	O
the	O
weight	O
matrices	O
are	O
given	O
the	O
initial	O
value	O
by	O
sampling	O
from	O
the	O
uniform	O
distribution	O
U	O
(	O
−0.1	O
,	O
0.1	O
)	O
,	O
and	O
all	O
the	O
biases	O
are	O
set	O
to	O
zero	O
.	O
The	O
dimension	O
of	O
the	O
word	O
embedding	O
and	O
aspect	O
term	O
embedding	O
are	O
set	O
to	O
300	O
,	O
and	O
the	O
number	O
of	O
the	O
hidden	O
units	O
are	O
set	O
to	O
200	O
.	O
The	O
dimension	O
of	O
position	O
embedding	O
is	O
set	O
to	O
100	O
,	O
which	O
is	O
randomly	O
initialized	O
and	O
updated	O
during	O
the	O
training	O
process	O
.	O
We	O
use	O
Tensorflow	O
(	O
Abadi	O
et	O
al	O
,	O
2016	O
)	O
to	O
implement	O
our	O
proposed	O
model	O
and	O
employ	O
the	O
Momentum	O
as	O
the	O
training	O
method	O
,	O
whose	O
momentum	O
parameter	O
γ	B-HyperparameterName
is	O
set	O
to	O
0.9	O
,	O
λ	O
is	O
set	O
to	O
10	O
−6	O
,	O
and	O
the	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
0.01	O
.	O
Dataset	O
:	O
To	O
evaluate	O
our	O
proposed	O
methods	O
,	O
we	O
conduct	O
experiments	O
on	O
the	O
dataset	O
of	O
SemEval	O
2014	O
Task4	O
3	O
,	O
the	O
SemEval	O
2014	O
dataset	O
consists	O
of	O
reviews	O
in	O
Restaurant	O
and	O
Laptop	O
datasets	O
.	O
Each	O
review	O
contains	O
a	O
list	O
of	O
aspect	O
terms	O
and	O
corresponding	O
polarities	O
,	O
which	O
are	O
labeled	O
with	O
{	O
positive	O
,	O
negative	O
,	O
neutral	O
}	O
.	O
Particularly	O
,	O
each	O
aspect	O
term	O
has	O
its	O
character	O
index	O
in	O
the	O
sentence	O
,	O
so	O
that	O
when	O
different	O
aspect	O
term	O
have	O
the	O
same	O
word	O
in	O
a	O
sentence	O
,	O
we	O
can	O
mark	O
the	O
relative	O
position	O
distance	O
of	O
a	O
sentence	O
according	O
to	O
the	O
current	O
aspect	O
term	O
without	O
confusion	O
.	O
Table	O
1	O
shows	O
the	O
training	O
and	O
test	O
sample	O
numbers	O
in	O
each	O
sentiment	O
polarity	O
.	O

In	O
order	O
to	O
evaluate	O
the	O
performance	O
of	O
our	O
model	O
,	O
we	O
compare	O
our	O
model	O
with	O
several	O
baseline	O
models	O
,	O
including	O
LSTM	B-MethodName
,	O
AE	B-MethodName
-	O
LSTM	B-MethodName
,	O
ATAE	O
-	O
LSTM	B-MethodName
,	O
IAN	B-MethodName
(	O
Ma	O
et	O
al	O
,	O
2017	O
)	O
and	O
MemNet	O
(	O
Tang	O
et	O
al	O
,	O
2016	O
LSTM	B-MethodName
:	O
LSTM	B-MethodName
takes	O
the	O
sentence	O
as	O
input	O
so	O
as	O
to	O
get	O
the	O
hidden	O
representation	O
of	O
each	O
word	O
.	O
Then	O
it	O
regards	O
the	O
average	O
value	O
of	O
all	O
hidden	O
states	O
as	O
the	O
representation	O
of	O
sentence	O
,	O
and	O
puts	O
it	O
into	O
softmax	B-MethodName
layer	O
to	O
predict	O
the	O
probability	O
of	O
each	O
sentiment	O
polarity	O
.	O
However	O
,	O
it	O
can	O
not	O
capture	O
any	O
information	O
of	O
aspect	O
term	O
in	O
sentence	O
.	O
AE	B-MethodName
-	O
LSTM	B-MethodName
:	O
AE	B-MethodName
-	O
LSTM	B-MethodName
first	O
models	O
the	O
words	O
in	O
sentence	O
via	O
LSTM	B-MethodName
network	O
and	O
concatenate	O
the	O
aspect	O
embedding	O
to	O
the	O
hidden	O
contextual	O
representation	O
for	O
calculating	O
the	O
attention	O
weights	O
,	O
which	O
are	O
employed	O
to	O
produce	O
the	O
final	O
representation	O
for	O
the	O
input	O
sentence	O
to	O
judge	O
the	O
sentiment	O
polarity	O
.	O
ATAE	O
-	O
LSTM	B-MethodName
:	O
ATAE	O
-	O
LSTM	B-MethodName
extended	O
AE	B-MethodName
-	O
LSTM	B-MethodName
by	O
appending	O
the	O
aspect	O
embedding	O
to	O
each	O
word	O
embedding	O
so	O
as	O
to	O
represent	O
the	O
input	O
sentence	O
,	O
which	O
highlights	O
the	O
role	O
of	O
aspect	O
embedding	O
.	O
The	O
other	O
design	O
of	O
ATAE	O
-	O
LSTM	B-MethodName
is	O
the	O
same	O
as	O
AE	B-MethodName
-	O
LSTM	B-MethodName
.	O
IAN	B-MethodName
:	O
IAN	B-MethodName
considers	O
the	O
separate	O
modeling	O
of	O
aspect	O
terms	O
and	O
sentences	O
respectively	O
.	O
IAN	B-MethodName
is	O
able	O
to	O
interactively	O
learn	O
attentions	O
in	O
the	O
contexts	O
and	O
aspect	O
terms	O
,	O
and	O
generates	O
the	O
representations	O
for	O
aspect	O
terms	O
and	O
contexts	O
separately	O
.	O
Finally	O
,	O
it	O
concatenates	O
the	O
aspect	O
term	O
representation	O
and	O
context	O
representation	O
for	O
predicting	O
the	O
sentiment	O
polarity	O
of	O
the	O
aspect	O
terms	O
within	O
its	O
contexts	O
(	O
Ma	O
et	O
al	O
,	O
2017	O
)	O
.	O
MemNet	O
:	O
MemNet	O
applies	O
attention	O
multiple	O
times	O
on	O
the	O
word	O
embedding	O
,	O
so	O
that	O
more	O
abstractive	O
evidences	O
could	O
be	O
selected	O
from	O
the	O
external	O
memory	O
.	O
The	O
output	O
of	O
the	O
last	O
attention	O
layer	O
is	O
fed	O
to	O
a	O
softmax	B-MethodName
layer	O
for	O
predictions	O
(	O
Tang	O
et	O
al	O
,	O
2016	O
Table	O
2	O
shows	O
the	O
performance	O
of	O
our	O
model	O
and	O
other	O
baseline	O
models	O
on	O
datasets	O
Restaurant	O
and	O
Laptop	O
respectively	O
.	O
We	O
can	O
observe	O
that	O
our	O
proposed	O
PBAN	O
model	O
achieves	O
the	O
best	O
performance	O
among	O
all	O
methods	O
.	O
It	O
is	O
obvious	O
that	O
LSTM	B-MethodName
method	O
gets	O
the	O
worst	O
performance	O
,	O
because	O
it	O
treats	O
aspect	O
term	O
and	O
other	O
words	O
as	O
the	O
same	O
,	O
so	O
that	O
it	O
can	O
not	O
take	O
full	O
advantage	O
of	O
the	O
aspect	O
term	O
information	O
and	O
predicts	O
the	O
same	O
polarity	O
for	O
different	O
aspect	O
terms	O
in	O
a	O
sentence	O
.	O
Furthermore	O
,	O
both	O
AE	B-MethodName
-	O
LSTM	B-MethodName
and	O
ATAE	O
-	O
LSTM	B-MethodName
perform	O
better	O
than	O
LSTM	B-MethodName
model	O
,	O
because	O
they	O
all	O
consider	O
the	O
importance	O
of	O
the	O
aspect	O
term	O
,	O
and	O
utilize	O
the	O
attention	O
mechanism	O
.	O
Specifically	O
,	O
ATAE	O
-	O
LSTM	B-MethodName
outperforms	O
AE	B-MethodName
-	O
LSTM	B-MethodName
since	O
it	O
appends	O
the	O
aspect	O
embedding	O
to	O
each	O
word	O
embedding	O
and	O
takes	O
them	O
as	O
inputs	O
,	O
which	O
helps	O
the	O
model	O
obtain	O
more	O
semantic	O
information	O
related	O
to	O
aspect	O
term	O
.	O
IAN	B-MethodName
realizes	O
the	O
importance	O
of	O
interaction	O
between	O
aspect	O
term	O
and	O
context	O
,	O
and	O
models	O
aspect	O
term	O
and	O
context	O
using	O
two	O
connected	O
attention	O
networks	O
.	O
Thus	O
,	O
IAN	B-MethodName
performs	O
better	O
than	O
ATAE	O
-	O
LSTM	B-MethodName
,	O
and	O
achieves	O
an	O
improvement	O
of	O
1.40	O
points	O
and	O
3.40	O
points	O
on	O
Restaurant	O
and	O
Laptop	O
datasets	O
in	O
Three	O
-	O
class	O
respectively	O
.	O
MemNet	O
(	O
9	O
)	O
utilizes	O
a	O
more	O
complex	O
structure	O
that	O
containing	O
nine	O
computational	O
layers	O
,	O
and	O
it	O
achieves	O
better	O
results	O
compared	O
to	O
IAN	B-MethodName
since	O
MemNet	O
reads	O
the	O
useful	O
information	O
from	O
external	O
memory	O
repeatedly	O
.	O
Although	O
both	O
IAN	B-MethodName
and	O
MemNet	O
models	O
performance	O
better	O
than	O
other	O
methods	O
,	O
they	O
all	O
perform	O
less	O
competitive	O
than	O
our	O
PBAN	O
both	O
on	O
Restaurant	O
and	O
Laptop	O
datasets	O
.	O
For	O
IAN	B-MethodName
model	O
,	O
it	O
interactively	O
learns	O
the	O
attentions	O
between	O
the	O
aspect	O
term	O
and	O
its	O
corresponding	O
sentence	O
,	O
but	O
this	O
attention	O
mechanism	O
is	O
coarse	O
-	O
grained	O
and	O
it	O
does	O
not	O
fully	O
consider	O
the	O
influence	O
of	O
different	O
words	O
in	O
aspect	O
term	O
on	O
the	O
sentence	O
.	O
For	O
MemNet	O
model	O
,	O
although	O
it	O
utilizes	O
the	O
location	O
information	O
,	O
it	O
is	O
mainly	O
used	O
for	O
calculating	O
the	O
memory	O
vectors	O
.	O
Nevertheless	O
,	O
PBAN	O
utilizes	O
the	O
character	O
index	O
of	O
the	O
aspect	O
term	O
(	O
provided	O
in	O
the	O
raw	O
dataset	O
)	O
and	O
adopts	O
relative	O
distance	O
to	O
represent	O
the	O
position	O
sequence	O
.	O
As	O
we	O
have	O
mentioned	O
in	O
previous	O
sections	O
,	O
an	O
aspect	O
term	O
contains	O
several	O
words	O
and	O
different	O
words	O
in	O
aspect	O
term	O
should	O
have	O
different	O
contributions	O
to	O
the	O
final	O
representation	O
of	O
sentence	O
.	O
In	O
PBAN	O
,	O
the	O
position	O
information	O
is	O
regarded	O
as	O
the	O
inputs	O
of	O
the	O
Bi	O
-	O
GRU	B-MethodName
,	O
so	O
it	O
can	O
help	O
calculate	O
the	O
weights	O
of	O
different	O
words	O
in	O
aspect	O
term	O
and	O
improve	O
the	O
final	O
representation	O
of	O
the	O
sentence	O
.	O
Moreover	O
,	O
when	O
different	O
aspect	O
terms	O
contain	O
the	O
same	O
word	O
,	O
our	O
proposed	O
position	O
information	O
can	O
effectively	O
identify	O
the	O
current	O
aspect	O
term	O
without	O
confusion	O
while	O
MemNet	O
can	O
not	O
.	O
Generally	O
speaking	O
,	O
by	O
integrating	O
the	O
position	O
information	O
and	O
the	O
bidirectional	O
attention	O
mechanism	O
,	O
PBAN	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
,	O
and	O
it	O
can	O
effectively	O
judge	O
the	O
sentiment	O
polarity	O
of	O
different	O
aspect	O
term	O
in	O
its	O
corresponding	O
sentence	O
so	O
as	O
to	O
improve	O
the	O
classification	O
accuracy	B-MetricName
.	O

Since	O
a	O
simple	O
and	O
effective	O
method	O
to	O
learn	O
distributed	O
representation	O
was	O
proposed	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
,	O
neural	O
networks	O
enhance	O
target	O
-	O
dependent	O
sentiment	B-TaskName
analysis	I-TaskName
significantly	O
.	O
Vo	O
and	O
Zhang	O
(	O
2015	O
)	O
split	O
a	O
tweet	O
into	O
a	O
left	O
context	O
and	O
a	O
right	O
context	O
according	O
to	O
a	O
given	O
target	O
,	O
using	O
distributed	O
word	O
representations	O
and	O
neural	O
pooling	O
functions	O
to	O
extract	O
features	O
.	O
Tang	O
et	O
al	O
(	O
2015	O
)	O
proposed	O
TD	O
-	O
LSTM	B-MethodName
and	O
TC	O
-	O
LSTM	B-MethodName
,	O
where	O
target	O
information	O
is	O
automatically	O
taken	O
into	O
account	O
.	O
These	O
two	O
models	O
integrated	O
the	O
connections	O
between	O
target	O
words	O
and	O
context	O
words	O
so	O
as	O
to	O
significantly	O
boost	O
the	O
classification	O
accuracy	B-MetricName
.	O
Zhang	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
two	O
gated	O
neural	O
networks	O
,	O
one	O
was	O
used	O
to	O
capture	O
tweet	O
-	O
level	O
syntactic	O
and	O
semantic	O
information	O
,	O
and	O
the	O
other	O
was	O
used	O
to	O
model	O
the	O
interactions	O
between	O
the	O
left	O
context	O
and	O
the	O
right	O
context	O
of	O
a	O
given	O
target	O
.	O
With	O
the	O
gating	O
mechanism	O
,	O
the	O
target	O
influenced	O
the	O
selection	O
of	O
sentiment	O
signals	O
over	O
the	O
context	O
.	O

Since	O
it	O
is	O
intuitive	O
that	O
each	O
language	O
has	O
its	O
own	O
characteristic	O
,	O
we	O
set	O
language	O
-	O
individual	O
attention	O
mechanisms	O
for	O
different	O
languages	O
.	O
In	O
the	O
individual	O
semantic	O
space	O
of	O
the	O
j	O
-	O
th	O
language	O
,	O
we	O
assign	O
a	O
query	O
vector	O
r	O
j	O
to	O
each	O
relation	O
r	O
R.	O
The	O
attention	O
score	O
for	O
each	O
sentence	O
in	O
S	O
j	O
=	O
{	O
x	O
1	O
j	O
,	O
x	O
2	O
j	O
,	O
.	O
.	O
.	O
}	O
is	O
defined	O
as	O
follows	O
,	O
α	B-HyperparameterName
i	O
j	O
=	O
exp	O
(	O
r	O
j	O
y	O
i	O
j	O
)	O
|	O
S	O
j	O
|	O
k=1	O
exp	O
(	O
r	O
j	O
y	O
k	O
j	O
)	O
.	O
(	O
7	O
)	O
The	O
attention	O
scores	O
can	O
be	O
used	O
to	O
compute	O
language	O
-	O
individual	O
textual	O
relation	O
representations	O
,	O
sj	O
=	O
|	O
S	O
j	O
|	O
k=1	O
α	B-HyperparameterName
k	O
j	O
y	O
k	O
j	O
.	O
(	O
8	O
)	O

Besides	O
language	O
-	O
individual	O
attention	O
mechanisms	O
,	O
we	O
also	O
adopt	O
a	O
language	O
-	O
consistent	O
attention	O
to	O
take	O
all	O
sentences	O
in	O
all	O
languages	O
into	O
consideration	O
.	O
In	O
the	O
consistent	O
semantic	O
space	O
,	O
we	O
also	O
assign	O
a	O
query	O
vectorr	O
to	O
each	O
relation	O
r	O
R	O
and	O
the	O
attention	O
score	O
for	O
each	O
sentence	O
is	O
defined	O
as	O
follows	O
,	O
β	B-HyperparameterName
i	O
j	O
=	O
exp	O
(	O
r	O
ȳ	O
i	O
j	O
)	O
n	O
l=1	O
|	O
S	O
l	O
|	O
k=1	O
exp	O
(	O
r	O
ȳ	O
k	O
l	O
)	O
.	O
(	O
9	O
)	O
The	O
attention	O
scores	O
can	O
be	O
used	O
to	O
compute	O
language	O
-	O
consistent	O
textual	O
relation	O
representations	O
,	O
s	O
=	O
n	O
l=1	O
|	O
S	O
l	O
|	O
k=1	O
β	B-HyperparameterName
k	O
lȳ	O
k	O
l	O
.	O
(	O
10	O
)	O

With	O
the	O
language	O
-	O
individual	O
textual	O
relation	O
representations	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
.	O
.	O
.	O
}	O
and	O
the	O
language	O
-	O
consistent	O
textual	O
relation	O
representations	O
,	O
we	O
can	O
estimate	O
the	O
probability	O
p	O
(	O
r	O
|	O
T	O
)	O
over	O
each	O
relation	O
r	O
R	O
,	O
p	O
(	O
r	O
|	O
T	O
)	O
=	O
p	O
(	O
r	O
|	O
s	O
)	O
n	O
j=1	O
p	O
(	O
r	O
|	O
sj	O
)	O
.	O
(	O
11	O
)	O
p	O
(	O
r	O
|	O
s	O
)	O
and	O
p	O
(	O
r	O
|	O
s	O
j	O
)	O
can	O
be	O
defined	O
as	O
follows	O
,	O
p	O
(	O
r	O
|	O
sj	O
)	O
=	O
softmax	B-MethodName
[	O
Rjsj	O
+	O
dj	O
]	O
,	O
p	O
(	O
r	O
|	O
s	O
)	O
=	O
softmax	B-MethodName
[	O
Rs	O
+	O
d	O
]	O
,	O
(	O
12	O
)	O
where	O
d	O
j	O
andd	O
are	O
bias	O
vectors	O
,	O
R	O
j	O
is	O
the	O
specific	O
relation	O
matrix	O
of	O
the	O
j	O
-	O
th	O
language	O
,	O
andR	O
is	O
the	O
consistent	O
relation	O
matrix	O
.	O
We	O
define	O
the	O
objective	O
function	O
to	O
train	O
the	O
relation	O
extractor	O
as	O
follows	O
,	O
min	O
θ	B-HyperparameterName
Lnre	O
(	O
θ	B-HyperparameterName
)	O
=	O
−	O
l	O
log	O
p	O
(	O
r	O
l	O
|	O
T	O
l	O
)	O
,	O
(	O
13	O
)	O
where	O
θ	B-HyperparameterName
is	O
all	O
parameters	O
in	O
the	O
framework	O
.	O
In	O
the	O
training	O
phase	O
,	O
p	O
(	O
r	O
|	O
T	O
)	O
is	O
computed	O
using	O
the	O
labeled	O
relations	O
as	O
the	O
attention	O
queries	O
.	O
In	O
the	O
test	O
phase	O
,	O
we	O
need	O
to	O
use	O
each	O
possible	O
relation	O
as	O
attention	O
queries	O
to	O
compute	O
p	O
(	O
r	O
|	O
T	O
)	O
for	O
relation	O
prediction	O
since	O
the	O
relations	O
are	O
unknown	O
in	O
advance	O
.	O

In	O
our	O
framework	O
,	O
we	O
encode	O
sentences	O
of	O
various	O
languages	O
into	O
a	O
consistent	O
semantic	O
space	O
to	O
grasp	O
the	O
consistency	O
among	O
languages	O
.	O
One	O
possible	O
situation	O
is	O
that	O
sentences	O
of	O
different	O
languages	O
are	O
aggregated	O
in	O
different	O
places	O
of	O
the	O
space	O
and	O
linearly	O
separable	O
.	O
In	O
this	O
case	O
,	O
our	O
purpose	O
of	O
mining	O
substantially	O
consistent	O
relation	O
patterns	O
in	O
different	O
languages	O
is	O
difficult	O
to	O
be	O
reached	O
.	O
Inspired	B-DatasetName
by	O
Ganin	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
adopt	O
adversarial	O
training	O
into	O
our	O
framework	O
to	O
address	O
this	O
problem	O
.	O
In	O
the	O
adversarial	O
training	O
,	O
we	O
define	O
a	O
discriminator	O
to	O
estimate	O
which	O
kind	O
of	O
languages	O
the	O
sentences	O
from	O
.	O
The	O
probability	O
distributions	O
over	O
these	O
sentences	O
are	O
formalized	O
as	O
follows	O
,	O
D	O
(	O
s	O
i	O
j	O
)	O
=	O
softmax	B-MethodName
(	O
MLP	B-DatasetName
(	O
s	O
i	O
j	O
)	O
)	O
,	O
(	O
14	O
)	O
where	O
MLP	B-DatasetName
is	O
a	O
two	O
-	O
layer	O
multilayer	O
perceptron	O
network	O
.	O
Contrary	O
to	O
the	O
discriminator	O
,	O
the	O
consistent	O
sentence	O
encoders	O
are	O
expected	O
to	O
produce	O
sentence	B-TaskName
embeddings	I-TaskName
that	O
can	O
not	O
be	O
reliably	O
predicted	O
by	O
the	O
discriminator	O
.	O
Hence	O
,	O
the	O
adversarial	O
training	O
process	O
is	O
a	O
min	O
-	O
max	O
game	O
and	O
can	O
be	O
formalized	O
as	O
follows	O
,	O
min	O
θ	B-HyperparameterName
C	O
E	O
max	O
θ	B-HyperparameterName
D	O
n	O
j=1	O
|	O
S	O
j	O
|	O
i=1	O
log	O
[	O
D	O
(	O
E	O
C	O
j	O
(	O
x	O
i	O
j	O
)	O
)	O
]	O
j	O
,	O
(	O
15	O
)	O
where	O
[	O
]	O
j	O
is	O
the	O
j	O
-	O
th	O
value	O
of	O
the	O
vector	O
.	O
The	O
formula	O
means	O
that	O
given	O
a	O
sentence	O
of	O
any	O
language	O
,	O
the	O
corresponding	O
sentence	O
encoder	O
of	O
its	O
language	O
generates	O
the	O
sentence	B-TaskName
embedding	I-TaskName
to	O
confuse	O
the	O
discriminator	O
.	O
Meanwhile	O
,	O
the	O
discriminator	O
tries	O
its	O
best	O
to	O
predict	O
the	O
language	O
of	O
the	O
sentence	O
according	O
to	O
the	O
sentence	B-TaskName
embedding	I-TaskName
.	O
After	O
sufficient	O
training	O
,	O
the	O
encoders	O
and	O
the	O
discriminator	O
reach	O
a	O
balance	O
,	O
and	O
sentences	O
of	O
different	O
languages	O
containing	O
similar	O
semantic	O
information	O
can	O
be	O
well	O
encoded	O
into	O
adjacent	O
places	O
of	O
the	O
space	O
.	O
In	O
training	O
,	O
we	O
optimize	O
the	O
following	O
loss	B-MetricName
functions	O
instead	O
of	O
Eq	O
.	O
15	O
,	O
min	O
θ	B-HyperparameterName
C	O
E	O
L	O
E	O
adv	O
(	O
θ	B-HyperparameterName
C	O
E	O
)	O
=	O
l	O
S	O
j	O
T	O
l	O
x	O
i	O
j	O
S	O
j	O
log	O
[	O
D	O
(	O
E	O
C	O
j	O
(	O
x	O
i	O
j	O
)	O
)	O
]	O
j	O
,	O
min	O
θ	B-HyperparameterName
D	O
L	O
D	O
adv	O
(	O
θ	B-HyperparameterName
D	O
)	O
=	O
−	O
l	O
S	O
j	O
T	O
l	O
x	O
i	O
j	O
S	O
j	O
log	O
[	O
D	O
(	O
E	O
C	O
j	O
(	O
x	O
i	O
j	O
)	O
)	O
]	O
j	O
,	O
(	O
16	O
)	O
where	O
θ	B-HyperparameterName
C	O
E	O
and	O
θ	B-HyperparameterName
D	O
are	O
all	O
parameters	O
of	O
the	O
consistent	O
sentence	O
encoders	O
and	O
the	O
discriminator	O
.	O
We	O
notice	O
that	O
language	O
-	O
individual	O
semantics	O
could	O
be	O
wrongly	O
encoded	O
into	O
the	O
consistent	O
semantic	O
space	O
,	O
and	O
may	O
have	O
negative	O
effects	O
on	O
extracting	O
language	O
-	O
consistent	O
features	O
.	O
Inspired	B-DatasetName
by	O
Bousmalis	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
adopt	O
orthogonality	O
constraints	O
to	O
alleviate	O
this	O
issue	O
.	O
We	O
minimize	O
the	O
following	O
penalty	O
function	O
:	O
min	O
θ	B-HyperparameterName
E	O
L	O
penalty	O
(	O
θ	B-HyperparameterName
E	O
)	O
=	O
n	O
j=1	O
I	O
T	O
j	O
Cj	O
F	O
,	O
(	O
17	O
)	O
where	O
I	O
j	O
and	O
C	O
j	O
are	O
two	O
matrices	O
whose	O
row	O
vectors	O
are	O
the	O
embeddings	O
of	O
sentences	O
in	O
the	O
j	O
-	O
th	O
language	O
encoded	O
by	O
E	O
I	O
j	O
and	O
E	O
C	O
j	O
respectively	O
.	O
θ	B-HyperparameterName
E	O
is	O
parameters	O
of	O
the	O
all	O
encoders	O
.	O
And	O
F	O
is	O
the	O
squared	O
Frobenius	O
norm	O
.	O

During	O
training	O
process	O
,	O
we	O
combine	O
the	O
extraction	O
and	O
adversarial	O
objective	O
functions	O
as	O
follows	O
,	O
L	O
=	O
Lnre	O
(	O
θ	B-HyperparameterName
)	O
+	O
λ1L	O
D	O
adv	O
(	O
θ	B-HyperparameterName
D	O
)	O
+	O
λ2L	O
E	O
adv	O
(	O
θ	B-HyperparameterName
C	O
E	O
)	O
+	O
λ3L	O
penalty	O
(	O
θ	B-HyperparameterName
E	O
)	O
,	O
(	O
18	O
)	O
where	O
λ	O
1	O
,	O
λ	O
2	O
,	O
and	O
λ	O
3	O
are	O
harmonic	O
factors	O
.	O
All	O
models	O
are	O
optimized	O
using	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
(	O
SGD	B-MethodName
)	O
.	O
In	O
practice	O
,	O
we	O
integrate	O
λ	O
1	O
and	O
λ	O
2	O
into	O
the	O
alternating	O
ratio	O
among	O
the	O
loss	B-MetricName
functions	O
,	O
and	O
we	O
calibrate	O
a	O
1:1:5	O
ratio	O
among	O
L	O
nre	O
(	O
θ	B-HyperparameterName
)	O
+	O
λ	O
3	O
L	O
penalty	O
(	O
θ	B-HyperparameterName
E	O
)	O
,	O
L	O
D	O
adv	O
(	O
θ	B-HyperparameterName
D	O
)	O
and	O
L	O
E	O
adv	O
(	O
θ	B-HyperparameterName
C	O
E	O
)	O
.	O
λ	O
3	O
is	O
set	O
as	O
0.02	O
.	O

We	O
evaluate	O
our	O
models	O
on	O
a	O
multi	O
-	O
lingual	O
relation	B-TaskName
extraction	I-TaskName
dataset	O
developed	O
by	O
.	O
We	O
evaluate	O
all	O
models	O
by	O
the	O
held	O
-	O
out	O
evaluation	O
following	O
previous	O
works	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
;	O
.	O
In	O
experiments	O
,	O
we	O
report	O
precision	O
-	O
recall	O
curves	O
of	O
recall	O
under	O
0.3	O
since	O
we	O
focus	O
more	O
on	O
the	O
performance	O
of	O
those	O
top	O
-	O
ranked	O
results	O
.	O
To	O
give	O
a	O
complete	O
view	O
of	O
the	O
performance	O
,	O
we	O
also	O
report	O
the	O
area	O
under	O
the	O
curve	O
(	O
AUC	B-MetricName
)	O
.	O

To	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
models	O
AMNRE	O
-	O
CNN	O
and	O
AMNRE	O
-	O
RNN	O
,	O
we	O
compare	O
the	O
proposed	O
models	O
with	O
various	O
neural	O
methods	O
:	O
MNRE	O
-	O
CNN	O
and	O
MNRE	O
-	O
RNN	O
are	O
multi	O
-	O
lingual	O
attention	O
-	O
based	O
NRE	O
models	O
with	O
CNN	O
and	O
RNN	O
sentence	O
encoders	O
respectively	O
;	O
CNN	O
-	O
EN	O
and	O
RNN	O
-	O
EN	O
are	O
vanilla	O
selective	O
-	O
attention	O
NRE	O
models	O
trained	O
with	O
English	O
data	O
,	O
which	O
are	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
in	O
mono	O
-	O
lingual	O
RE	O
(	O
Lin	O
et	O
al	O
,	O
2016	O
)	O
;	O
CNN	O
-	O
CN	O
and	O
RNN	O
-	O
CN	O
are	O
trained	O
with	O
Chinese	O
data	O
;	O
CNN	O
-	O
Joint	O
and	O
RNN	O
-	O
Joint	O
are	O
naive	O
joint	O
models	O
which	O
predict	O
relations	O
by	O
directly	O
summing	O
up	O
ranking	O
scores	O
of	O
both	O
English	O
and	O
Chinese	O
;	O
CNN	O
-	O
Share	O
and	O
RNN	O
-	O
Share	O
are	O
another	O
naive	O
joint	O
models	O
which	O
train	O
English	O
and	O
Chinese	O
models	O
with	O
shared	O
relation	O
embeddings	O
.	O
The	O
results	O
of	O
precision	O
-	O
recall	O
curves	O
are	O
shown	O
in	O
Figure	O
2	O
and	O
the	O
results	O
of	O
AUC	B-MetricName
are	O
shown	O
in	O
Table	O
3	O
.	O
From	O
the	O
results	O
,	O
we	O
have	O
the	O
following	O
observations	O
:	O
(	O
1	O
)	O
Both	O
for	O
CNN	O
and	O
RNN	O
,	O
the	O
models	O
jointly	O
utilizing	O
English	O
and	O
Chinese	O
sentences	O
outperform	O
the	O
models	O
only	O
using	O
mono	O
-	O
lingual	O
sentences	O
.	O
This	O
demonstrates	O
that	O
the	O
rich	O
information	O
in	O
multi	O
-	O
lingual	O
data	O
is	O
useful	O
and	O
can	O
significantly	O
enhance	O
existing	O
NRE	O
models	O
.	O
(	O
2	O
)	O
The	O
-	O
Joint	O
models	O
achieve	O
similar	O
performance	O
with	O
the	O
-	O
Share	O
models	O
,	O
and	O
both	O
of	O
them	O
underperform	O
the	O
MNRE	O
and	O
AMNRE	O
models	O
.	O
They	O
all	O
benefit	O
from	O
the	O
multi	O
-	O
lingual	O
information	O
,	O
but	O
the	O
models	O
with	O
multi	O
-	O
lingual	O
attentions	O
can	O
better	O
take	O
advantage	O
of	O
multi	O
-	O
lingual	O
data	O
.	O
It	O
indicates	O
that	O
designing	O
targeted	O
schemes	O
to	O
extract	O
rich	O
multi	O
-	O
lingual	O
information	O
is	O
crucial	O
.	O
(	O
3	O
)	O
AMNRE	O
achieves	O
the	O
best	O
results	O
among	O
all	O
the	O
baseline	O
models	O
over	O
the	O
entire	O
range	O
of	O
recall	O
in	O
Figure	O
2	O
,	O
even	O
as	O
compared	O
with	O
MNRE	O
.	O
AMNRE	O
also	O
outperforms	O
MNRE	O
with	O
3	O
percentage	O
points	O
increasing	O
in	O
the	O
AUC	B-MetricName
results	O
.	O
It	O
indicates	O
our	O
proposed	O
framework	O
which	O
explicitly	O
encodes	O
languageconsistent	O
and	O
language	O
-	O
individual	O
semantics	O
better	O
extract	O
multi	O
-	O
lingual	O
information	O
,	O
and	O
therefore	O
lead	O
to	O
the	O
significant	O
improvement	O
in	O
RE	O
performance	O
.	O

To	O
further	O
verify	O
that	O
every	O
mono	O
-	O
lingual	O
RE	O
models	O
can	O
benefit	O
from	O
our	O
proposed	O
framework	O
,	O
which	O
explicitly	O
consider	O
language	O
-	O
consistent	O
relation	O
patterns	O
,	O
we	O
train	O
models	O
with	O
multi	O
-	O
lingual	O
data	O
and	O
evaluate	O
the	O
performance	O
of	O
these	O
models	O
in	O
the	O
mono	O
-	O
lingual	O
RE	O
scenario	O
.	O
To	O
show	O
the	O
results	O
clearly	O
,	O
we	O
report	O
the	O
precision	O
-	O
recall	O
curves	O
in	O
Figure	O
3	O
and	O
the	O
AUC	B-MetricName
results	O
in	O
Table	O
4	O
.	O
From	O
the	O
results	O
,	O
we	O
can	O
observe	O
that	O
:	O
(	O
1	O
)	O
As	O
compared	O
with	O
the	O
models	O
directly	O
learned	O
with	O
the	O
mono	O
-	O
lingual	O
data	O
,	O
the	O
models	O
exploiting	O
the	O
multi	O
-	O
lingual	O
information	O
perform	O
better	O
in	O
the	O
mono	O
-	O
lingual	O
scenario	O
.	O
This	O
demonstrates	O
that	O
there	O
is	O
latent	O
consistency	O
among	O
languages	O
,	O
and	O
grasping	O
this	O
consistency	O
from	O
multi	O
-	O
lingual	O
data	O
can	O
provide	O
additional	O
information	O
for	O
models	O
in	O
each	O
language	O
to	O
enhance	O
their	O
results	O
in	O
the	O
mono	O
-	O
lingual	O
scenario	O
.	O
(	O
2	O
)	O
Our	O
proposed	O
models	O
achieve	O
the	O
best	O
precision	O
over	O
the	O
entire	O
range	O
of	O
recall	O
and	O
also	O
significantly	O
improve	O
the	O
AUC	B-MetricName
results	O
as	O
compared	O
with	O
both	O
MNRE	O
and	O
mono	O
-	O
lingual	O
RE	O
models	O
.	O
It	O
indicates	O
that	O
due	O
to	O
the	O
consistent	O
semantic	O
space	O
in	O
our	O
framework	O
,	O
language	O
-	O
consistent	O
information	O
lying	O
in	O
the	O
multi	O
-	O
lingual	O
data	O
is	O
better	O
mined	O
and	O
serve	O
the	O
mono	O
-	O
lingual	O
scenario	O
.	O

We	O
adopt	O
an	O
adversarial	O
training	O
strategy	O
to	O
fuse	O
the	O
features	O
from	O
different	O
languages	O
to	O
extract	O
consistent	O
relation	O
patterns	O
.	O
Orthogonality	O
constraints	O
are	O
also	O
adopted	O
to	O
separate	O
the	O
consistent	O
and	O
individual	O
feature	O
spaces	O
.	O
To	O
measure	O
the	O
effectiveness	O
of	O
them	O
,	O
we	O
conduct	O
an	O
ablation	O
study	O
which	O
compares	O
the	O
proposed	O
models	O
with	O
the	O
similar	O
models	O
but	O
without	O
adversarial	O
training	O
strategy	O
(	O
AMNRE	O
-	O
noA	O
)	O
,	O
without	O
orthogonality	O
constraints	O
(	O
AMNRE	O
-	O
noO	O
)	O
,	O
and	O
without	O
both	O
of	O
them	O
(	O
AMNRE	O
-	O
noBoth	O
)	O
.	O
The	O
AUC	B-MetricName
results	O
are	O
shown	O
in	O
Table	O
5	O
.	O
We	O
can	O
observe	O
that	O
both	O
the	O
adversarial	O
training	O
strategy	O
and	O
orthogonality	O
constraints	O
have	O
significant	O
influence	O
on	O
the	O
performance	O
of	O
our	O
proposed	O
model	O
.	O
This	O
demonstrates	O
the	O
effectiveness	O
of	O
adversarial	O
training	O
strategy	O
and	O
orthogonality	O
constraints	O
for	O
multi	O
-	O
lingual	O
RE	O
.	O
To	O
give	O
a	O
more	O
intuitive	O
picture	O
of	O
the	O
effect	O
of	O
these	O
two	O
mechanisms	O
,	O
we	O
visualize	O
the	O
distribution	O
of	O
sentence	O
feature	O
embeddings	O
encoded	O
by	O
the	O
individual	O
and	O
consistent	O
encoders	O
using	O
t	O
-	O
SNE	O
(	O
Maaten	O
and	O
Hinton	O
,	O
2008	O
)	O
.	O
The	O
results	O
are	O
shown	O
in	O
Figure	O
4	O
.	O
Figure	O
4	O
(	O
a	O
)	O
shows	O
that	O
there	O
are	O
obvious	O
differences	O
between	O
the	O
feature	O
embeddings	O
encoded	O
from	O
the	O
same	O
sentences	O
by	O
individual	O
and	O
consistent	O
encoders	O
.	O
It	O
indicates	O
the	O
orthogonality	O
constraints	O
are	O
effective	O
to	O
separate	O
the	O
individual	O
and	O
consistent	O
latent	O
spaces	O
.	O
From	O
the	O
comparison	O
between	O
Figure	O
4	O
(	O
c	O
)	O
,	O
we	O
can	O
observe	O
that	O
the	O
feature	O
embeddings	O
from	O
different	O
languages	O
are	O
wellmixed	O
due	O
to	O
the	O
adversarial	O
training	O
strategy	O
.	O
We	O
can	O
more	O
easily	O
to	O
grasp	O
latent	O
consistency	O
among	O
languages	O
after	O
multi	O
-	O
feature	O
fusion	O
.	O

Lexical	O
Antonyms	O
.	O
To	O
detect	O
whether	O
an	O
S	O
i	O
m	O
-	O
H	O
int	O
pair	O
uses	O
the	O
lexical	O
antonyms	O
strategy	O
,	O
we	O
first	O
need	O
to	O
build	O
a	O
resource	O
of	O
lexical	O
antonyms	O
.	O
We	O
use	O
the	O
MPQA	B-DatasetName
sentiment	O
Lexicon	O
(	O
Wilson	O
et	O
al	O
,	O
2005	O
)	O
,	O
2004	O
)	O
.	O
We	O
consider	O
a	O
lexical	O
antonym	O
strategy	O
if	O
:	O
1	O
)	O
antonym	O
words	O
are	O
aligned	O
;	O
2	O
)	O
they	O
are	O
the	O
roots	O
of	O
the	O
respective	O
dependency	O
trees	O
or	O
if	O
the	O
nodes	O
modified	O
by	O
the	O
lexical	O
antonyms	O
are	O
the	O
same	O
in	O
their	O
respective	O
trees	O
(	O
e.g.	O
,	O
'	O
can	O
you	O
show	O
any	O
more	O
of	O
steelers	O
"	O
!	O
"	O
show	O
less	O
of	O
steelers	O
"	O
,	O
the	O
candidate	O
lexical	O
antonyms	O
are	O
more	O
and	O
less	O
and	O
they	O
are	O
the	O
objects	O
of	O
the	O
same	O
predicate	O
in	O
S	O
i	O
m	O
-	O
H	O
int	O
:	O
show	O
)	O
.	O
Out	O
of	O
211	O
S	O
i	O
m	O
-	O
H	O
int	O
pairs	O
that	O
are	O
marked	O
as	O
having	O
lexical	O
antonym	O
strategy	O
(	O
dev	O
set	O
)	O
,	O
12	O
instances	O
are	O
identified	O
by	O
only	O
the	O
dependency	O
parses	O
,	O
67	O
instances	O
by	O
the	O
word	O
-	O
alignments	O
,	O
and	O
100	O
instances	O
by	O
both	O
(	O
P	O
/	O
R	O
/	O
F1	B-MetricName
scores	O
are	O
92.1	O
%	O
,	O
77.7	O
%	O
and	O
84.3	O
%	O
)	O
,	O
respectively	O
on	O
dev	O
dataset	O
.	O
However	O
,	O
sometimes	O
both	O
dependency	O
and	O
wordalignment	O
methods	O
fail	O
.	O
In	O
"	O
circling	O
down	O
the	O
bowl	O
.	O
Yay	O
"	O
!	O
"	O
circling	O
down	O
the	O
bowl	O
.	O
awful	O
"	O
,	O
although	O
the	O
lexical	O
antonyms	O
yay	O
and	O
awful	O
exist	O
,	O
neither	O
the	O
alignment	O
nor	O
the	O
dependency	O
trees	O
can	O
detect	O
it	O
(	O
25	O
such	O
instances	O
in	O
the	O
dev	O
set	O
)	O
.	O
To	O
account	O
for	O
this	O
,	O
after	O
having	O
run	O
the	O
dependency	O
and	O
alignment	O
methods	O
,	O
we	O
also	O
just	O
search	O
whether	O
a	O
S	O
i	O
m	O
-	O
H	O
int	O
pair	O
contains	O
a	O
lexical	O
antonym	O
pair	O
.	O
This	O
improves	O
the	O
final	O
recall	O
and	O
on	O
the	O
dev	O
set	O
we	O
achieve	O
89.0	O
%	O
precision	O
,	O
95.7	O
%	O
recall	O
,	O
and	O
92.2	O
%	O
F1	B-MetricName
on	O
dev	O
dataset	O
(	O
Lex	O
ant	O
Strategy	O
;	O
Table	O
3	O
show	O
results	O
both	O
on	O
dev	O
and	O
the	O
test	O
sets	O
)	O
.	O
Note	O
,	O
just	O
searching	O
whether	O
a	O
lexical	O
antonym	O
pair	O
is	O
present	O
in	O
a	O
S	O
i	O
m	O
-	O
H	O
int	O
pair	O
results	O
in	O
low	O
precision	O
(	O
58.6	O
%	O
)	O
but	O
high	O
recall	O
(	O
80	O
%	O
)	O
.	O
Simple	O
negation	O
.	O
This	O
strategy	O
(	O
denoted	O
as	O
Simple	O
neg	O
in	O
Table	O
3	O
and	O
Table	O
4	O
)	O
involves	O
identifying	O
the	O
presence	O
of	O
negation	O
and	O
its	O
scope	O
.	O
Here	O
,	O
however	O
,	O
the	O
scope	O
of	O
negation	O
is	O
con	O
-	O
strained	O
since	O
generally	O
Turkers	O
negated	O
only	O
a	O
single	O
word	O
(	O
i.e.	O
,	O
"	O
love	O
"	O
!	O
"	O
not	O
love	O
"	O
)	O
.	O
Thus	O
our	O
problem	O
is	O
easier	O
than	O
the	O
general	O
problem	O
of	O
finding	O
the	O
scope	O
of	O
negation	O
(	O
Li	O
and	O
Lu	O
,	O
2018	O
;	O
Qian	O
et	O
al	O
,	O
2016	O
;	O
Fancellu	O
et	O
al	O
,	O
2016	O
)	O
.	O
We	O
use	O
30	O
negation	O
markers	O
from	O
Reitan	O
et	O
al	O
(	O
2015	O
)	O
to	O
find	O
negation	O
scope	O
in	O
tweets	O
.	O
We	O
first	O
detect	O
whether	O
a	O
negation	O
marker	O
appears	O
in	O
either	O
H	O
int	O
or	O
S	O
i	O
m	O
,	O
but	O
not	O
in	O
both	O
(	O
negation	O
can	O
appear	O
in	O
S	O
i	O
m	O
for	O
ironic	O
blame	O
)	O
If	O
the	O
marker	O
is	O
used	O
,	O
we	O
extract	O
its	O
parent	O
node	O
from	O
the	O
dependency	O
tree	O
,	O
and	O
if	O
this	O
node	O
is	O
also	O
present	O
in	O
the	O
other	O
utterance	O
,	O
then	O
Negation	O
strategy	O
is	O
selected	O
.	O
For	O
instance	O
,	O
in	O
"	O
looks	O
just	O
like	O
me	O
"	O
!	O
"	O
does	O
not	O
look	O
like	O
me	O
"	O
,	O
the	O
negation	O
not	O
is	O
modifying	O
the	O
main	O
predicate	O
looks	O
in	O
H	O
int	O
,	O
which	O
is	O
also	O
the	O
main	O
predicate	O
in	O
S	O
i	O
m	O
(	O
words	O
are	O
lemmatized	O
)	O
.	O
In	O
the	O
next	O
section	O
,	O
we	O
discuss	O
if	O
the	O
parent	O
nodes	O
are	O
not	O
the	O
same	O
but	O
similar	O
and	O
with	O
different	O
sentiment	O
strength	O
.	O
Weakening	O
the	O
intensity	O
of	O
sentiment	O
.	O
The	O
first	O
strategy	O
-	O
replacing	O
words	O
expressing	O
a	O
high	O
degree	O
of	O
positive	O
/	O
negative	O
sentiment	O
with	O
more	O
neutral	O
ones	O
(	O
'	O
I	O
love	O
being	O
sick	O
"	O
!	O
"	O
I	O
do	O
n't	O
like	O
being	O
sick	O
)	O
-	O
,	O
is	O
applied	O
only	O
in	O
conjunction	O
with	O
the	O
negation	O
strategy	O
.	O
We	O
measure	O
the	O
difference	O
in	O
strength	O
using	O
the	O
Dictionary	O
of	O
Affect	O
(	O
Whissell	O
et	O
al	O
,	O
1986	O
)	O
.	O
Out	O
of	O
31	O
S	O
i	O
m	O
-	O
H	O
int	O
pairs	O
in	O
the	O
dev	O
set	O
,	O
we	O
automatically	O
identify	O
28	O
interpretations	O
that	O
use	O
this	O
approach	O
.	O
For	O
the	O
second	O
strategy	O
-	O
removing	O
the	O
intensifier	O
(	O
I	O
am	O
really	O
happy	O
"	O
!	O
"	O
I	O
am	O
disappointed	O
'	O
)	O
-	O
,	O
we	O
first	O
determine	O
whether	O
the	O
intensifier	O
exists	O
in	O
S	O
i	O
m	O
and	O
is	O
eliminated	O
from	O
H	O
int	O
.	O
We	O
use	O
only	O
adjective	O
and	O
adverb	O
intensifiers	O
from	O
Taboada	O
et	O
al	O
(	O
2011	O
)	O
,	O
primarily	O
to	O
discard	O
conjunctions	O
such	O
as	O
"	O
so	O
"	O
(	O
"	O
no	O
water	O
so	O
I	O
ca	O
n't	O
wash	O
.	O
.	O
.	O
"	O
)	O
.	O
This	O
strategy	O
is	O
used	O
together	O
with	O
both	O
lexical	O
antonyms	O
and	O
Simple	O
negation	O
strategies	O
.	O
For	O
a	O
candidate	O
S	O
i	O
m	O
-	O
H	O
int	O
pair	O
,	O
if	O
the	O
lexical	O
antonym	O
strategy	O
is	O
selected	O
and	O
a	O
S	O
and	O
a	O
H	O
are	O
the	O
lexical	O
antonyms	O
,	O
we	O
determine	O
whether	O
any	O
intensifier	O
modifies	O
a	O
S	O
and	O
no	O
intensifier	O
modifies	O
a	O
H	O
.	O
If	O
the	O
Negation	O
strategy	O
is	O
se	O
-	O
lected	O
,	O
we	O
identify	O
the	O
negated	O
term	O
in	O
the	O
H	O
int	O
and	O
then	O
search	O
its	O
aligned	O
node	O
from	O
the	O
S	O
i	O
m	O
using	O
the	O
word	O
-	O
word	B-TaskName
alignment	I-TaskName
.	O
Next	O
,	O
we	O
search	O
in	O
the	O
S	O
i	O
m	O
if	O
any	O
intensifier	O
is	O
intensifying	O
the	O
aligned	O
term	O
.	O
The	O
strategies	O
are	O
denoted	O
as	O
AN	O
weaksent	O
in	O
Table	O
3	O
and	O
Table	O
4	O
.	O
Interrogative	O
to	O
Declarative	O
Transformation	O
(	O
+	O
Antonym	O
/	O
Neg	O
)	O
.	O
To	O
capture	O
this	O
strategy	O
we	O
need	O
to	O
determine	O
first	O
if	O
the	O
verbal	O
irony	O
was	O
expressed	O
as	O
a	O
rhetorical	O
question	O
.	O
To	O
build	O
a	O
classifier	O
to	O
detect	O
RQ	O
,	O
we	O
collect	O
two	O
categories	O
of	O
tweets	O
(	O
4	O
K	O
each	O
)	O
(	O
1	O
)	O
tweets	O
labeled	O
with	O
#	O
sarcasm	O
or	O
#	O
irony	O
that	O
also	O
contain	O
"	O
?	O
"	O
,	O
and	O
(	O
2	O
)	O
information	O
seeking	O
tweets	O
containing	O
"	O
?	O
"	O
.	O
We	O
train	O
a	O
binary	O
classifier	O
using	O
SVM	B-MethodName
RBF	O
Kernel	O
with	O
default	O
parameters	O
.	O
The	O
features	O
are	O
Twitter	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
(	O
Ghosh	O
et	O
al	O
,	O
2015	O
)	O
,	O
modal	O
verbs	O
,	O
pronouns	O
,	O
interrogative	O
words	O
,	O
negations	O
,	O
and	O
position	O
of	O
"	O
?	O
"	O
in	O
a	O
tweet	O
.	O
We	O
evaluate	O
the	O
training	O
model	O
on	O
the	O
dev	O
data	O
and	O
the	O
P	O
/	O
R	O
/	O
F1	B-MetricName
are	O
53.2	O
%	O
,	O
65.4	O
%	O
,	O
and	O
58.6	O
%	O
,	O
respectively	O
(	O
in	O
future	O
work	O
we	O
plan	O
to	O
develop	O
more	O
accurate	O
models	O
for	O
RQ	O
detection	O
)	O
.	O
Once	O
we	O
detect	O
the	O
ironic	O
message	O
was	O
expressed	O
as	O
a	O
RQ	O
,	O
we	O
identify	O
the	O
specific	O
interpretation	O
strategy	O
accompanying	O
the	O
transformation	O
from	O
interrogative	O
to	O
declarative	O
form	O
:	O
antonym	O
or	O
negation	O
.	O
These	O
combined	O
strategies	O
are	O
denoted	O
as	O
AN	O
I!D	O
in	O
Table	O
3	O
and	O
Table	O
4	O
.	O
Desiderative	O
Constructions	O
:	O
Currently	O
,	O
we	O
use	O
a	O
simple	O
regular	O
expression	O
"	O
I	O
[	O
w	O
]	O
⇤	O
wish	O
"	O
to	O
capture	O
counterfactual	O
cases	O
(	O
AN	O
desiderative	O
in	O
Tables	O
3	O
and	O
Table	O
4	O
)	O
.	O
Note	O
,	O
when	O
the	O
Simple	O
negation	O
and	O
lexical	O
antonyms	O
strategies	O
are	O
combined	O
with	O
other	O
strategy	O
(	O
e.g.	O
,	O
removing	O
of	O
intensifier	O
)	O
,	O
we	O
consider	O
this	O
combined	O
strategy	O
for	O
the	O
interpretation	O
of	O
verbal	O
irony	O
and	O
not	O
the	O
simple	O
negation	O
or	O
lexical	O
antonym	O
strategy	O
(	O
i.e.	O
,	O
we	O
do	O
not	O
double	O
count	O
)	O
.	O

The	O
performance	O
of	O
the	O
models	O
is	O
similar	O
on	O
both	O
test	O
and	O
SIGN	O
test	O
sets	O
,	O
showing	O
consistently	O
good	O
performance	O
(	O
Table	O
3	O
;	O
90	O
%	O
F1	B-MetricName
for	O
all	O
strategies	O
,	O
except	O
the	O
AntPhrase+PragInf	O
and	O
AN	O
I!D	O
)	O
.	O
Given	O
these	O
results	O
,	O
we	O
can	O
now	O
apply	O
these	O
models	O
to	O
study	O
the	O
distribution	O
of	O
these	O
strategies	O
in	O
the	O
entire	O
datasets	O
(	O
Table	O
4	O
)	O
.	O
The	O
strategy	O
distribution	O
between	O
our	O
dataset	O
S	O
i	O
m	O
-	O
H	O
int	O
and	O
SIGN	O
dataset	O
is	O
similar	O
and	O
matches	O
the	O
distribution	O
on	O
the	O
manual	O
annotations	O
on	O
the	O
dev	O
dataset	O
in	O
Table	O
2	O
.	O
The	O
sum	O
of	O
the	O
strategies	O
can	O
exceed	O
the	O
total	O
number	O
of	O
the	O
pairs	O
since	O
a	O
tweet	O
can	O
contain	O
several	O
ironic	O
sentences	O
that	O
are	O
interpreted	O
by	O
Turkers	O
.	O
For	O
instance	O
,	O
in	O
"	O
Dave	O
too	O
nice	O
.	O
.	O
.	O
a	O
nice	O
fella	O
"	O
!	O
"	O
Dave	O
not	O
nice	O
.	O
.	O
.	O
a	O
mean	O
fella	O
"	O
we	O
observe	O
the	O
application	O
of	O
two	O
strategies	O
,	O
lexical	O
antonyms	O
(	O
e.g.	O
,	O
nice	O
!	O
mean	O
)	O
and	O
negation	O
(	O
e.g.	O
,	O
nice	O
!	O
not	O
nice	O
)	O
.	O

Event	O
schemas	O
encode	O
knowledge	O
of	O
stereotypical	O
structures	O
of	O
events	O
and	O
their	O
connections	O
.	O
As	O
events	O
unfold	O
,	O
schemas	O
are	O
crucial	O
to	O
act	O
as	O
a	O
scaffolding	O
.	O
Previous	O
work	O
on	O
event	O
schema	O
induction	O
focuses	O
either	O
on	O
atomic	O
events	O
or	O
linear	O
temporal	O
event	O
sequences	O
,	O
ignoring	O
the	O
interplay	O
between	O
events	O
via	O
arguments	O
and	O
argument	O
relations	O
.	O
We	O
introduce	O
a	O
new	O
concept	O
of	O
Temporal	O
Complex	O
Event	O
Schema	O
:	O
a	O
graph	O
-	O
based	O
schema	O
representation	O
that	O
encompasses	O
events	O
,	O
arguments	O
,	O
temporal	O
connections	O
and	O
argument	O
relations	O
.	O
In	O
addition	O
,	O
we	O
propose	O
a	O
Temporal	O
Event	O
Graph	O
Model	O
that	O
predicts	O
event	O
instances	O
following	O
the	O
temporal	O
complex	O
event	O
schema	O
.	O
To	O
build	O
and	O
evaluate	O
such	O
schemas	O
,	O
we	O
release	O
a	O
new	O
schema	O
learning	O
corpus	O
containing	O
6	O
,	O
399	O
documents	O
accompanied	O
with	O
event	O
graphs	O
,	O
and	O
we	O
have	O
manually	O
constructed	O
gold	O
-	O
standard	O
schemas	O
.	O
Intrinsic	O
evaluations	O
by	O
schema	O
matching	O
and	O
instance	O
graph	O
perplexity	B-MetricName
,	O
prove	O
the	O
superior	O
quality	O
of	O
our	O
probabilistic	O
graph	O
schema	O
library	O
compared	O
to	O
linear	O
representations	O
.	O
Extrinsic	O
evaluation	O
on	O
schema	O
-	O
guided	O
future	O
event	O
prediction	O
further	O
demonstrates	O
the	O
predictive	O
power	O
of	O
our	O
event	O
graph	O
model	O
,	O
significantly	O
outperforming	O
human	O
schemas	O
and	O
baselines	O
by	O
more	O
than	O
23.8	O
%	O
on	O

We	O
use	O
a	O
Graph	O
Neural	O
Network	O
(	O
GNN	O
)	O
(	O
Kipf	O
and	O
Welling	O
,	O
2017	O
)	O
to	O
update	O
node	O
embeddings	O
following	O
the	O
graph	O
structure	O
.	O
Before	O
we	O
run	O
the	O
GNN	O
on	O
the	O
graph	O
,	O
we	O
first	O
add	O
virtual	O
edges	O
between	O
the	O
newly	O
generated	O
event	O
and	O
all	O
previous	O
events	O
,	O
and	O
between	O
new	O
entities	O
and	O
previous	O
entities	O
,	O
shown	O
as	O
dashed	O
lines	O
in	O
Figure	O
2	O
.	O
The	O
virtual	O
edges	O
enable	O
the	O
representations	O
of	O
new	O
nodes	O
to	O
aggregate	O
the	O
messages	O
from	O
previous	O
nodes	O
,	O
which	O
has	O
been	O
proven	O
effective	O
in	O
(	O
Liao	O
et	O
al	O
,	O
2019	O
)	O
.	O
To	O
capture	O
rich	O
semantics	O
of	O
edge	O
types	O
,	O
we	O
pass	O
edge	O
-	O
aware	O
messages	O
during	O
graph	O
propagation	O
.	O
An	O
intuitive	O
way	O
is	O
to	O
encode	O
different	O
edge	O
types	O
with	O
different	O
convolutional	O
filters	O
,	O
which	O
is	O
similar	O
to	O
RGCN	B-MethodName
(	O
Schlichtkrull	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
the	O
number	O
of	O
RGCN	B-MethodName
parameters	O
grows	O
rapidly	O
with	O
the	O
number	O
of	O
edge	O
types	O
and	O
easily	O
becomes	O
unmanageable	O
given	O
the	O
large	O
number	O
of	O
relation	O
types	O
and	O
argument	O
roles	O
in	O
the	O
IE	O
ontology	B-MethodName
.	O
4	O
Instead	O
,	O
we	O
learn	O
a	O
vector	O
representation	O
for	O
each	O
relation	O
type	O
r	O
and	O
argument	O
role	O
a.	O
The	O
message	O
passed	O
through	O
each	O
argument	O
edge	O
e	O
i	O
,	O
a	O
,	O
v	O
j	O
is	O
:	O
m	O
i	O
,	O
j	O
=	O
ReLU	B-MethodName
(	O
W	O
a	O
(	O
(	O
e	O
i	O
−	O
v	O
j	O
)	O
a	O
)	O
)	O
,	O
where	O
denotes	O
concatenation	O
operation	O
.	O
Similarly	O
,	O
the	O
message	O
between	O
two	O
entities	O
v	O
j	O
and	O
v	O
k	O
is	O
:	O
m	O
j	O
,	O
k	B-HyperparameterName
=	I-HyperparameterName
ReLU	B-MethodName
(	O
W	O
r	O
(	O
(	O
v	O
j	O
−	O
v	O
k	O
)	O
r	O
)	O
)	O
.	O
Considering	O
that	O
the	O
direction	O
of	O
the	O
temporal	O
edge	O
is	O
important	O
,	O
we	O
parametrize	O
the	O
message	O
over	O
this	O
edge	O
by	O
assigning	O
two	O
separate	O
weight	O
matrices	O
to	O
the	O
outgoing	O
and	O
incoming	O
vertices	O
:	O
m	O
i	O
,	O
l	O
=	O
ReLU	B-MethodName
(	O
W	O
bfr	O
e	O
i	O
−	O
W	O
aft	O
e	O
l	O
)	O
.	O
We	O
aggregate	O
the	O
messages	O
using	O
edge	O
-	O
aware	O
attention	O
following	O
(	O
Liao	O
et	O
al	O
,	O
2019	O
)	O
:	O
5	O
α	B-HyperparameterName
i	O
,	O
j	O
=	O
σ	O
(	O
MLP	B-DatasetName
(	O
e	O
i	O
−	O
e	O
j	O
)	O
)	O
,	O
where	O
σ	O
is	O
the	O
sigmoid	O
function	O
,	O
and	O
MLP	B-DatasetName
contains	O
two	O
hidden	O
layers	O
with	O
ReLU	B-MethodName
nonlinearities	O
.	O
The	O
event	O
node	O
representation	O
e	O
i	O
is	O
then	O
updated	O
using	O
the	O
messages	O
from	O
its	O
local	O
neighbors	O
N	O
(	O
e	O
i	O
)	O
,	O
similar	O
to	O
entity	O
node	O
representations	O
:	O
e	O
i	O
GRU	B-MethodName
e	O
i	O
j	O
N	O
(	O
e	O
i	O
)	O
α	B-HyperparameterName
i	O
,	O
j	O
m	O
i	O
,	O
j	O
.	O

To	O
predict	O
the	O
temporal	O
dependencies	O
between	O
the	O
new	O
events	O
and	O
existing	O
events	O
,	O
we	O
connect	O
them	O
through	O
temporal	O
edges	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O
These	O
edges	O
are	O
critical	O
for	O
message	O
passing	O
in	O
predicting	O
the	O
next	O
event	O
.	O
We	O
build	O
temporal	O
edges	O
in	O
the	O
last	O
phase	O
of	O
generation	O
,	O
since	O
it	O
relies	O
on	O
the	O
shared	O
and	O
related	O
arguments	O
.	O
Considering	O
that	O
temporal	O
edges	O
are	O
interdependent	O
,	O
we	O
model	O
the	O
generation	O
probability	O
as	O
a	O
mixture	O
of	O
Bernoulli	O
distributions	O
following	O
(	O
Liao	O
et	O
al	O
,	O
2019	O
)	O
:	O
p	O
(	O
e	O
i	O
,	O
e	O
l	O
|	O
e	O
i	O
,	O
e	O
l	O
)	O
=	O
b	O
γ	B-HyperparameterName
b	O
θ	B-HyperparameterName
b	O
,	O
i	O
,	O
l	O
,	O
γ	B-HyperparameterName
1	O
,	O
,	O
γ	B-HyperparameterName
B	O
=	O
Softmax	B-MethodName
i	O
,	O
l	O
MLP	B-DatasetName
(	O
e	O
i	O
−	O
e	O
l	O
)	O
,	O
θ	B-HyperparameterName
1	O
,	O
i	O
,	O
l	O
,	O
,	O
θ	B-HyperparameterName
B	O
,	O
i	O
,	O
l	O
=	O
σ	O
(	O
MLP	B-DatasetName
θ	B-HyperparameterName
(	O
e	O
i	O
−	O
e	O
l	O
)	O
)	O
,	O
where	O
B	O
is	O
the	O
number	O
of	O
mixture	O
components	O
.	O
When	O
B	O
=	O
1	O
,	O
the	O
distribution	O
degenerates	O
to	O
factorized	O
Bernoulli	O
,	O
which	O
assumes	O
the	O
independence	O
of	O
each	O
potential	O
temporal	O
edge	O
conditioned	O
on	O
the	O
existing	O
graph	O
.	O

We	O
train	O
the	O
model	O
by	O
optimizing	O
the	O
negative	O
loglikelihood	O
loss	B-MetricName
,	O
L	O
=	O
G	O
G	O
train	O
−	O
log	O
2	O
p	O
(	O
G	O
)	O
.	O
To	O
compose	O
the	O
schema	O
library	O
for	O
each	O
complex	O
event	O
scenario	O
,	O
we	O
construct	O
instance	O
graphs	O
from	O
related	O
documents	O
to	O
learn	O
a	O
graph	O
model	O
,	O
and	O
then	O
obtain	O
the	O
schema	O
using	O
greedy	O
decoding	O
.	O
4	O
Evaluation	O
Benchmark	O

To	O
evaluate	O
our	O
temporal	O
event	O
graph	O
model	O
,	O
we	O
compute	O
the	O
instance	O
graph	O
perplexity	B-MetricName
by	O
predicting	O
the	O
instance	O
graphs	O
in	O
the	O
test	O
set	O
,	O
PP	O
=	O
2	O
−	O
1	O
|	O
G	O
test	O
|	O
G	O
G	O
test	O
log	O
2	O
p	O
(	O
G	O
)	O
.	O
(	O
1	O
)	O
We	O
calculate	O
the	O
full	O
perplexity	B-MetricName
for	O
the	O
entire	O
graph	O
using	O
Equation	O
(	O
1	O
)	O
,	O
and	O
event	O
perplexity	B-MetricName
using	O
only	O
event	O
nodes	O
,	O
emphasizing	O
the	O
importance	O
of	O
correctly	O
predicting	O
events	O
.	O

Training	O
Details	O
.	O
For	O
our	O
event	O
graph	O
model	O
,	O
the	O
representation	O
dimension	O
is	O
128	O
,	O
and	O
we	O
use	O
a	O
2	O
-	O
layer	O
GNN	O
.	O
The	O
value	O
of	O
B	O
is	O
2	O
.	O
The	O
number	O
of	O
mixture	O
components	O
in	O
temporal	O
classifier	O
is	O
2	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
1e	O
-	O
4	O
.	O
To	O
train	O
event	O
language	O
model	O
baseline	O
,	O
instead	O
of	O
using	O
LSTM	B-MethodName
-	O
based	O
architecture	O
following	O
(	O
Pichotta	O
and	O
Mooney	O
,	O
2016	O
)	O
,	O
we	O
adopt	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
auto	O
-	O
regressive	O
language	O
XLNet	B-MethodName
.	O
In	O
detail	O
,	O
we	O
first	O
linearize	O
the	O
graph	O
using	O
topological	O
sort	O
,	O
and	O
then	O
train	O
XLNet	B-MethodName
9	O
using	O
the	O
dimension	O
of	O
128	O
(	O
the	O
same	O
as	O
our	O
temporal	O
event	O
graph	O
model	O
)	O
,	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
is	O
3	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
1e	O
-	O
4	O
.	O
We	O
select	O
the	O
best	O
model	O
on	O
the	O
validation	O
set	O
.	O
Both	O
of	O
our	O
model	O
and	O
event	O
language	O
model	O
baseline	O
are	O
trained	O
on	O
one	O
Tesla	O
V100	O
GPU	O
with	O
16	O
GB	O
DRAM	O
.	O
For	O
sequential	O
pattern	O
mining	O
,	O
we	O
perform	O
random	O
walk	O
,	O
starting	O
from	O
every	O
node	O
in	O
instance	O
graphs	O
and	O
ending	O
at	O
sink	O
nodes	O
,	O
to	O
obtain	O
event	O
type	O
sequences	O
,	O
and	O
then	O
apply	O
PrefixSpan	O
(	O
Pei	O
et	O
al	O
,	O
2001	O
)	O
10	O
to	O
rank	O
sequential	O
patterns	O
.	O
Evaluation	O
Details	O
.	O
To	O
compose	O
the	O
schema	O
library	O
,	O
we	O
use	O
the	O
first	O
ranked	O
sequence	O
as	O
the	O
schema	O
for	O
these	O
two	O
models	O
.	O
To	O
perform	O
event	O
prediction	O
using	O
baselines	O
,	O
we	O
traverse	O
the	O
input	O
graph	O
to	O
obtain	O
event	O
type	O
sequences	O
,	O
and	O
conduct	O
prediction	O
on	O
all	O
sequences	O
to	O
produce	O
an	O
averaged	O
score	O
.	O
For	O
human	O
schemas	O
,	O
we	O
first	O
linearize	O
them	O
and	O
the	O
input	O
graphs	O
,	O
and	O
find	O
the	O
longest	O
common	O
subsequence	O
between	O
them	O
.	O

Intrinsic	O
Evaluation	O
.	O
In	O
Table	O
3	O
,	O
the	O
significant	O
gain	O
on	O
event	O
match	O
demonstrates	O
the	O
ability	O
of	O
our	O
graph	O
model	O
to	O
keep	O
salient	O
events	O
.	O
On	O
sequence	O
match	O
,	O
our	O
approach	O
achieves	O
larger	O
performance	O
gain	O
compared	O
to	O
baselines	O
when	O
the	O
path	O
length	O
l	O
is	O
longer	O
.	O
It	O
implies	O
that	O
the	O
proposed	O
model	O
is	O
capable	O
of	O
capturing	O
longer	O
and	O
wider	O
temporal	O
dependencies	O
.	O
In	O
the	O
case	O
of	O
connection	O
match	O
,	O
only	O
sequential	O
pattern	O
mining	O
in	O
the	O
baselines	O
can	O
predict	O
connections	O
between	O
events	O
.	O
When	O
compared	O
against	O
sequential	O
pattern	O
mining	O
,	O
our	O
generation	O
model	O
significantly	O
performs	O
better	O
since	O
it	O
considers	O
the	O
inter	O
-	O
dependency	O
of	O
arguments	O
and	O
encodes	O
them	O
with	O
graph	O
structures	O
.	O
Extrinsic	O
Evaluation	O
.	O
On	O
the	O
task	O
of	O
schemaguided	O
event	O
prediction	O
,	O
our	O
graph	O
model	O
obtains	O
significant	O
improvement	O
(	O
see	O
Table	O
4	O
.	O
)	O
The	O
low	O
performance	O
of	O
human	O
schema	O
demonstrates	O
the	O
importance	O
of	O
probabilistically	O
modeling	O
schemas	O
to	O
support	O
downstream	O
tasks	O
.	O
Take	O
Figure	O
3	O
as	O
an	O
example	O
.	O
Human	O
schemas	O
produce	O
incorrect	O
event	O
types	O
such	O
as	O
TRAILHEARING	O
,	O
since	O
it	O
matches	O
the	O
sequence	O
ATTACK	O
DIE	O
TRAILHEARING	O
,	O
incapable	O
of	O
capturing	O
the	O
inter	O
-	O
dependencies	O
between	O
sequences	O
.	O
However	O
,	O
our	O
model	O
is	O
able	O
to	O
customize	O
the	O
prediction	O
to	O
the	O
global	O
context	O
of	O
the	O
input	O
graph	O
,	O
and	O
take	O
into	O
account	O
that	O
there	O
is	O
no	O
AR	O
-	O
REST	O
event	O
or	O
justice	O
-	O
related	O
events	O
in	O
the	O
input	O
graph	O
.	O
Also	O
,	O
the	O
human	O
schema	O
fails	O
to	O
predict	O
INJURE	O
and	O
ATTACK	O
,	O
because	O
it	O
relies	O
on	O
the	O
exact	B-MetricName
match	I-MetricName
of	O
event	O
sequences	O
of	O
lengths	O
l	O
≥	O
2	O
,	O
and	O
can	O
not	O
handle	O
the	O
variants	O
of	O
sequences	O
.	O
This	O
problem	O
can	O
be	O
solved	O
by	O
our	O
probabilistic	O
schema	O
,	O
via	O
modeling	O
the	O
prediction	O
probability	O
conditioned	O
on	O
the	O
existing	O
graph	O
.	O
For	O
example	O
,	O
even	O
though	O
AT	O
-	O
TACK	O
mostly	O
happens	O
before	O
DIE	O
,	O
we	O
learn	O
that	O
ATTACK	O
might	O
repeat	O
after	O
DIE	O
event	O
if	O
there	O
are	O
multiple	O
ATTACK	O
and	O
DETONATE	O
in	O
the	O
existing	O
graph	O
,	O
which	O
means	O
the	O
complex	O
event	O
is	O
about	O
a	O
series	O
of	O
conflict	O
events	O
.	O
Ablation	O
Study	O
.	O
Removing	O
argument	O
generation	O
(	O
"	O
w/o	O
ArgumentGeneration	O
"	O
)	O
generally	O
lowers	O
the	O
performance	O
on	O
all	O
evaluation	O
tasks	O
,	O
since	O
it	O
ignores	O
the	O
coreferential	O
arguments	O
and	O
their	O
relations	O
,	O
but	O
relies	O
solely	O
on	O
the	O
overly	O
simplistic	O
temporal	O
order	O
to	O
connect	O
events	O
.	O
This	O
is	O
especially	O
apparent	O
from	O
the	O
instance	O
graph	O
perplexity	B-MetricName
in	O
Table	O
3	O
.	O
Learning	O
Corpus	O
Size	O
.	O
An	O
average	O
of	O
113	O
instance	O
graphs	O
is	O
used	O
for	O
each	O
complex	O
event	O
type	O
in	O
the	O
IED	O
scenario	O
,	O
and	O
383	O
instance	O
graphs	O
to	O
learn	O
the	O
schema	O
model	O
in	O
the	O
General	B-DatasetName
scenario	O
.	O
The	O
better	O
performance	O
on	O
the	O
IED	O
dataset	O
in	O
Table	O
3	O
shows	O
that	O
the	O
number	O
of	O
instance	O
graphs	O
increases	O
the	O
schema	O
induction	O
performance	O
.	O
Effect	O
of	O
Information	O
Extraction	O
Errors	O
.	O
Based	O
on	O
the	O
error	O
analysis	O
for	O
schemas	O
induced	O
in	O
Table	O
1	O
,	O
the	O
effect	O
of	O
extraction	O
errors	O
can	O
be	O
categorized	O
into	O
:	O
(	O
1	O
)	O
temporal	O
ordering	O
errors	O
:	O
43.3	O
%	O
;	O
(	O
2	O
)	O
missing	O
events	O
:	O
34.4	O
%	O
;	O
(	O
3	O
)	O
missing	O
coreferential	O
events	O
:	O
8.8	O
%	O
;	O
(	O
4	O
)	O
incorrect	O
event	O
type	O
:	O
7.7	O
%	O
;	O
(	O
5	O
)	O
missing	O
coreferential	O
arguments	O
:	O
5.5	O
%	O
.	O
However	O
,	O
even	O
on	O
automatically	O
extracted	O
event	O
graphs	O
with	O
extraction	O
errors	O
,	O
our	O
model	O
significantly	O
performs	O
better	O
on	O
event	O
prediction	O
compared	O
to	O
humanconstructed	O
schemas	O
,	O
as	O
shown	O
in	O
Table	O
4	O
.	O
It	O
demonstrates	O
that	O
our	O
schema	O
induction	O
method	O
is	O
robust	O
and	O
effective	O
to	O
support	O
downstream	O
tasks	O
,	O
even	O
when	O
only	O
provided	O
with	O
noisy	O
data	O
with	O
extraction	O
errors	O
.	O

Not	O
much	O
.	O
It	O
was	O
pretty	O
dull	O
.	O
Blah	O
,	O
you	O
did	O
n't	O
miss	O
anything	O
.	O
Not	O
anything	O
that	O
important	O
.	O
Very	O
little	O
,	O
it	O
was	O
uneventful	O
.	O
Figure	O
1	O
:	O
Diversity	O
metric	O
evaluation	O
:	O
we	O
show	O
two	O
sets	O
of	O
responses	O
to	O
the	O
same	O
question	O
,	O
generated	O
by	O
crowdsourcing	O
workers	O
.	O
While	O
both	O
sets	O
are	O
diverse	O
in	O
terms	O
of	O
form	O
,	O
only	O
set	O
A	O
is	O
diverse	O
in	O
terms	O
of	O
content	O
.	O
Each	O
graph	O
presents	O
the	O
distribution	O
over	O
a	O
diversity	O
metric	O
for	O
sets	O
with	O
high	O
content	O
diversity	O
(	O
blue	O
)	O
and	O
low	O
content	O
diversity	O
(	O
orange	O
)	O
.	O
Distributions	O
are	O
approximated	O
over	O
200	O
sets	O
.	O
We	O
observe	O
that	O
the	O
human	O
score	O
metric	O
(	O
absDHS	O
)	O
separates	O
the	O
two	O
distributions	O
,	O
while	O
an	O
n	O
-	O
gram	O
based	O
metric	O
(	O
distinct	O
-	O
n	O
)	O
fails	O
,	O
illustrating	O
that	O
it	O
does	O
not	O
capture	O
content	O
diversity	O
.	O
The	O
dotted	O
lines	O
correspond	O
to	O
the	O
specific	O
sets	O
A	O
and	O
B	O
presented	O
above	O
.	O
all	O
)	O
,	O
making	O
it	O
difficult	O
to	O
compare	O
competing	O
approaches	O
(	O
Hashimoto	O
et	O
al	O
,	O
2019	O
)	O
.	O
Having	O
a	O
principled	O
and	O
consensual	O
diversity	O
evaluation	O
metric	O
is	O
hence	O
fundamental	O
for	O
the	O
field	O
of	O
NLG	O
.	O
A	O
key	O
challenge	O
in	O
developing	O
diversity	O
evaluation	O
metrics	O
,	O
is	O
the	O
difficulty	O
in	O
determining	O
their	O
efficacy	O
.	O
Unlike	O
metrics	O
for	O
evaluating	O
the	O
quality	O
of	O
generated	O
text	O
,	O
where	O
one	O
can	O
measure	O
correlation	O
between	O
a	O
metric	O
(	O
such	O
as	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
)	O
and	O
human	O
judgement	O
(	O
Zhang	O
et	O
al	O
,	O
2019a	O
;	O
Sagarkar	O
et	O
al	O
,	O
2018	O
)	O
,	O
it	O
is	O
unknown	O
if	O
hu	O
-	O
mans	O
can	O
reliably	O
estimate	O
diversity	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
framework	O
for	O
evaluating	O
diversity	O
metrics	O
(	O
Figure	O
2	O
)	O
.	O
We	O
assume	O
that	O
a	O
tester	O
(	O
human	O
or	O
model	O
)	O
is	O
generating	O
sets	O
of	O
sentences	O
,	O
conditioned	O
on	O
some	O
diversity	O
parameter	O
that	O
controls	O
the	O
diversity	O
of	O
the	O
output	O
sentences	O
.	O
We	O
evaluate	O
the	O
diversity	O
of	O
the	O
sentences	O
using	O
a	O
proposed	O
metric	O
,	O
and	O
measure	O
correlation	O
between	O
the	O
metric	O
and	O
the	O
diversity	O
parameter	O
.	O
High	O
correlation	O
indicates	O
that	O
the	O
metric	O
captures	O
how	O
the	O
diversity	O
parameter	O
affects	O
the	O
model	O
output	O
.	O
We	O
instantiate	O
this	O
framework	O
with	O
two	O
tests	O
.	O
As	O
a	O
preliminary	O
step	O
,	O
we	O
introduce	O
the	O
decoding	O
test	O
:	O
the	O
tester	O
is	O
a	O
neural	O
generation	O
model	O
and	O
the	O
diversity	O
parameter	O
is	O
a	O
decoding	O
parameter	O
,	O
such	O
as	O
softmax	B-MethodName
temperature	O
(	O
Ackley	O
et	O
al	O
,	O
1985	O
)	O
.	O
This	O
parameter	O
controls	O
the	O
skewness	O
of	O
the	O
distribution	O
in	O
every	O
generated	O
token	O
,	O
and	O
has	O
been	O
shown	O
to	O
affect	O
model	O
diversity	O
(	O
Holtzman	O
et	O
al	O
,	O
2019	O
;	O
Caccia	O
et	O
al	O
,	O
2018	O
)	O
.	O
Then	O
,	O
we	O
turn	O
the	O
focus	O
to	O
content	O
diversity	O
,	O
introducing	O
the	O
content	O
test	O
(	O
Figure	O
1	O
)	O
.	O
Here	O
,	O
the	O
tester	O
is	O
a	O
human	O
,	O
and	O
the	O
diversity	O
parameter	O
is	O
a	O
binary	O
variable	O
,	O
where	O
the	O
human	O
is	O
instructed	O
to	O
generate	O
sets	O
of	O
sentences	O
with	O
either	O
high	O
or	O
low	O
diversity	O
in	O
content	O
.	O
We	O
evaluate	O
three	O
families	O
of	O
popular	O
diversity	O
metrics	O
with	O
these	O
tests	O
:	O
(	O
a	O
)	O
n	O
-	O
gram	O
-	O
based	O
metrics	O
that	O
estimate	O
diversity	O
based	O
on	O
surface	O
patterns	O
in	O
a	O
set	O
of	O
generated	O
sentences	O
,	O
(	O
b	O
)	O
neural	O
metrics	O
:	O
we	O
propose	O
a	O
reduction	O
from	O
evaluating	O
sentence	O
similarity	O
to	O
evaluating	O
diversity	O
,	O
then	O
evaluate	O
diversity	O
using	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
sentence	O
similarity	O
models	O
,	O
and	O
(	O
c	O
)	O
human	O
evaluation	O
:	O
we	O
explore	O
multiple	O
ways	O
in	O
which	O
humans	O
can	O
be	O
asked	O
to	O
estimate	O
diversity	O
,	O
resulting	O
in	O
multiple	O
Human	O
Diversity	O
Score	B-MetricName
(	O
HDS	O
)	O
variations	O
.	O
Applying	O
our	O
tests	O
leads	O
to	O
several	O
findings	O
:	O
(	O
i	O
)	O
In	O
the	O
decoding	O
test	O
,	O
n	O
-	O
gram	O
-	O
based	O
metrics	O
correlate	O
well	O
with	O
decoding	O
parameters	O
,	O
such	O
as	O
softmax	B-MethodName
temperature	O
.	O
While	O
the	O
goal	O
of	O
our	O
framework	O
is	O
to	O
evaluate	O
diversity	O
metrics	O
,	O
this	O
result	O
lets	O
us	O
reflect	O
back	O
on	O
the	O
tester	O
itself	O
and	O
conclude	O
that	O
decoding	O
parameters	O
predominantly	O
control	O
the	O
form	O
of	O
text	O
rather	O
than	O
content	O
.	O
(	O
ii	O
)	O
Conversely	O
,	O
n	O
-	O
gram	O
-	O
based	O
metrics	O
perform	O
poorly	O
in	O
the	O
content	O
test	O
.	O
While	O
neural	O
metrics	O
outperform	O
n	O
-	O
gram	O
-	O
based	O
metrics	O
,	O
humans	O
are	O
substantially	O
better	O
than	O
any	O
automatic	O
metric	O
at	O
detecting	O
content	O
diversity	O
.	O
This	O
is	O
illustrated	O
in	O
Figure	O
1	O
,	O
where	O
a	O
human	O
clearly	O
distinguishes	O
between	O
sets	O
that	O
have	O
high	O
(	O
blue	O
)	O
and	O
low	O
(	O
orange	O
)	O
content	O
diver	O
-	O
sity	O
,	O
while	O
n	O
-	O
gram	O
-	O
based	O
metrics	O
fail	O
to	O
do	O
so	O
.	O
Due	O
to	O
this	O
gap	O
,	O
we	O
construct	O
a	O
large	O
dataset	O
focused	O
on	O
content	O
-	O
diversity	O
metrics	O
.	O
We	O
release	O
the	O
Metrics	O
for	O
content	O
Diversity	O
(	O
McDiv	O
)	O
benchmark	O
,	O
a	O
challenge	O
for	O
research	O
in	O
diversity	O
evaluation	O
.	O
To	O
conclude	O
,	O
our	O
main	O
contributions	O
are	O
:	O
A	O
framework	O
for	O
evaluating	O
diversity	O
metrics	O
.	O
Tests	O
instantiating	O
this	O
framework	O
,	O
measuring	O
the	O
sensitivity	O
of	O
metrics	O
to	O
diversity	O
,	O
with	O
a	O
focus	O
on	O
content	O
diversity	O
.	O
Best	O
practices	O
for	O
obtaining	O
diversity	O
evaluations	O
from	O
crowdsourcing	O
workers	O
.	O
Establishing	O
that	O
humans	O
outperform	O
current	O
automatic	O
metrics	O
in	O
detecting	O
content	O
diversity	O
.	O
The	O
McDiv	O
dataset	O
-	O
a	O
benchmark	O
for	O
content	O
diversity	O
aware	O
metrics	O
.	O
The	O
collected	O
data	O
,	O
test	O
scores	O
and	O
code	O
are	O
publicly	O
available	O
,	O
1	O
and	O
can	O
be	O
used	O
to	O
easily	O
compare	O
new	O
diversity	O
metrics	O
to	O
existing	O
results	O
in	O
our	O
framework	O
.	O

Recently	O
,	O
interest	O
in	O
diversity	O
has	O
increased	O
(	O
Du	O
and	O
Black	O
,	O
2019	O
;	O
Holtzman	O
et	O
al	O
,	O
2019	O
)	O
,	O
resulting	O
in	O
multiple	O
proposals	O
for	O
its	O
evaluation	O
.	O
We	O
describe	O
recent	O
approaches	O
,	O
highlighting	O
the	O
need	O
for	O
a	O
standard	O
way	O
to	O
evaluate	O
metrics	O
.	O
Perplexity	B-MetricName
is	O
the	O
standard	O
metric	O
in	O
language	O
modeling	O
,	O
measuring	O
the	O
proximity	O
of	O
a	O
language	O
model	O
(	O
LM	O
)	O
,	O
P	O
LM	O
,	O
to	O
the	O
true	O
distribution	O
,	O
P	O
ref	O
,	O
by	O
approximating	O
the	O
cross	O
-	O
entropy	O
H	O
(	O
P	O
ref	O
,	O
P	O
LM	O
)	O
with	O
held	O
-	O
out	O
data	O
from	O
P	O
ref	O
.	O
Thus	O
,	O
perplexity	B-MetricName
captures	O
to	O
some	O
extent	O
diversity	O
.	O
For	O
example	O
,	O
a	O
dialog	O
model	O
that	O
puts	O
all	O
probability	O
mass	O
on	O
the	O
output	O
"	O
I	O
do	O
n't	O
know	O
"	O
for	O
any	O
given	O
context	O
will	O
obtain	O
infinite	O
perplexity	B-MetricName
once	O
it	O
encounters	O
any	O
other	O
response	O
.	O
This	O
property	O
makes	O
perplexity	B-MetricName
popular	O
in	O
LM	O
-	O
based	O
NLG	O
models	O
,	O
and	O
often	O
it	O
is	O
the	O
only	O
reported	O
measure	O
for	O
diversity	O
(	O
Lewis	O
et	O
al	O
,	O
2017	O
;	O
Fan	O
et	O
al	O
,	O
2018	O
;	O
Wang	O
et	O
al	O
,	O
2019	O
;	O
.	O
However	O
,	O
perplexity	B-MetricName
does	O
not	O
purely	O
measure	O
diversity	O
,	O
and	O
high	O
perplexity	B-MetricName
does	O
not	O
entail	O
low	O
diversity	O
.	O
For	O
example	O
,	O
a	O
LM	O
with	O
a	O
uniform	O
distribution	O
over	O
the	O
vocabulary	O
for	O
each	O
decoded	O
token	O
has	O
high	O
diversity	O
,	O
but	O
its	O
perplexity	B-MetricName
will	O
be	O
extremely	O
high	O
,	O
due	O
to	O
its	O
low	O
quality	O
.	O
Moreover	O
,	O
perplexity	B-MetricName
evaluates	O
a	O
LM	O
,	O
while	O
the	O
diversity	O
of	O
a	O
NLG	O
system	O
is	O
also	O
strongly	O
affected	O
by	O
the	O
decoding	O
procedure	O
.	O
For	O
example	O
,	O
Top	O
-	O
k	O
and	O
nucleus	O
sampling	O
are	O
popular	O
decoding	O
schemes	O
that	O
tradeoff	O
quality	O
and	O
diversity	O
by	O
ignoring	O
some	O
of	O
the	O
LM	O
probability	O
mass	O
(	O
Holtzman	O
et	O
al	O
,	O
2019	O
)	O
.	O
Last	O
,	O
some	O
NLG	O
models	O
,	O
such	O
as	O
Generative	O
Adversarial	O
Networks	O
(	O
GANs	O
)	O
(	O
Yu	O
et	O
al	O
,	O
2017	O
)	O
are	O
not	O
language	O
models	O
.	O
While	O
one	O
can	O
approximate	O
perplexity	B-MetricName
for	O
such	O
models	O
(	O
Tevet	O
et	O
al	O
,	O
2019	O
)	O
,	O
ideally	O
,	O
a	O
metric	O
should	O
not	O
be	O
tied	O
to	O
a	O
model	O
.	O
N	O
-	O
gram	O
-	O
based	O
metrics	O
A	O
popular	O
metric	O
is	O
distinct	O
n	O
-	O
grams	O
(	O
Li	O
et	O
al	O
,	O
2016b	O
)	O
,	O
which	O
computes	O
the	O
proportion	O
of	O
unique	O
n	O
-	O
grams	O
out	O
of	O
the	O
total	O
number	O
of	O
n	O
-	O
grams	O
in	O
a	O
set	O
of	O
generated	O
sentences	O
.	O
Dušek	O
et	O
al	O
(	O
2020	O
)	O
calculated	O
Shannon	O
entropy	O
(	O
Manning	O
et	O
al	O
,	O
1999	O
)	O
based	O
on	O
different	O
n	O
-	O
grams	O
as	O
a	O
measure	O
of	O
lexical	O
diversity	O
.	O
Self	O
-	O
BLEU	B-MetricName
(	O
Zhu	O
et	O
al	O
,	O
2018	O
;	O
Shu	O
et	O
al	O
,	O
2019	O
)	O
measures	O
the	O
BLEU	B-MetricName
score	I-MetricName
of	O
a	O
generated	O
sentence	O
with	O
respect	O
to	O
another	O
generated	O
sentence	O
(	O
rather	O
than	O
a	O
gold	O
reference	O
)	O
.	O
High	O
average	O
Self	O
-	O
BLEU	B-MetricName
indicates	O
high	O
similarity	O
between	O
generated	O
sentences	O
and	O
low	O
diversity	O
.	O
In	O
5	O
we	O
expand	O
this	O
idea	O
and	O
suggest	O
a	O
reduction	O
from	O
any	O
similarity	O
metric	O
to	O
a	O
diversity	O
metric	O
.	O
By	O
design	O
,	O
n	O
-	O
gram	O
based	O
metrics	O
are	O
sensitive	O
to	O
diversity	O
in	O
the	O
form	O
of	O
language	O
,	O
rather	O
than	O
its	O
meaning	O
.	O
Embedding	O
-	O
based	O
metrics	O
A	O
new	O
line	O
of	O
metrics	O
suggests	O
to	O
embed	O
generated	O
sentences	O
in	O
latent	O
space	O
,	O
then	O
evaluate	O
them	O
in	O
this	O
space	O
.	O
Du	O
and	O
Black	O
(	O
2019	O
)	O
suggest	O
to	O
cluster	O
the	O
embedded	O
sentences	O
with	O
k	O
-	O
means	O
,	O
then	O
use	O
its	O
inertia	O
as	O
a	O
measure	O
for	O
diversity	O
.	O
Recently	O
,	O
Lai	O
et	O
al	O
(	O
2020	O
)	O
suggested	O
to	O
consider	O
the	O
volume	O
induced	O
by	O
the	O
embedded	O
sentences	O
as	O
a	O
diversity	O
metric	O
.	O
asked	O
humans	O
to	O
evaluate	O
the	O
internal	O
diversity	O
of	O
a	O
generated	O
essay	O
.	O
Ghandeharioun	O
et	O
al	O
(	O
2019	O
)	O
let	O
crowdsourcing	O
workers	O
interact	O
with	O
a	O
dialog	O
chat	O
-	O
bot	O
,	O
then	O
asked	O
them	O
to	O
evaluate	O
the	O
diversity	O
of	O
a	O
single	O
conversation	O
.	O
In	O
contrast	O
,	O
this	O
paper	O
focuses	O
on	O
the	O
diversity	O
of	O
different	O
responses	O
given	O
a	O
context	O
,	O
as	O
in	O
Zhang	O
et	O
al	O
(	O
2019b	O
)	O
.	O

N	O
-	O
gram	O
-	O
based	O
metrics	O
We	O
evaluate	O
distinct	O
ngrams	O
(	O
distinct	O
-	O
n	O
)	O
,	O
as	O
described	O
in	O
2	O
.	O
We	O
also	O
evaluate	O
n	O
-	O
grams	O
cosine	O
similarity	O
(	O
cos	O
-	O
sim	O
)	O
:	O
a	O
similarity	O
measure	O
computing	O
the	O
cosine	O
between	O
the	O
vectors	O
representing	O
two	O
sentences	O
,	O
where	O
each	O
vector	O
is	O
a	O
count	O
vector	O
over	O
the	O
n	O
-	O
grams	O
that	O
appear	O
in	O
the	O
response	O
.	O
We	O
use	O
the	O
reduction	O
from	O
5	O
to	O
convert	O
this	O
to	O
a	O
diversity	O
measure	O
.	O
In	O
both	O
metrics	O
,	O
rather	O
than	O
choosing	O
the	O
order	O
of	O
the	O
ngrams	O
,	O
we	O
average	O
over	O
n	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
5	O
}	O
,	O
which	O
we	O
found	O
to	O
outperform	O
any	O
single	O
choice	O
of	O
n.	O
Neural	O
metrics	O
We	O
exploit	O
existing	O
BERT	B-MethodName
-	O
based	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
fine	O
-	O
tuned	O
for	O
estimating	O
similarity	O
between	O
two	O
sentences	O
(	O
applying	O
the	O
reduction	O
from	O
5	O
)	O
.	O
BERT	B-MethodName
-	O
STS	B-TaskName
;	O
A	O
BERT	B-MethodName
model	O
fine	O
-	O
tuned	O
on	O
Semantic	B-TaskName
Textual	I-TaskName
Similarity	I-TaskName
(	O
Cer	O
et	O
al	O
,	O
2017	O
)	O
:	O
a	O
collection	O
of	O
sentence	O
pairs	O
annotated	O
with	O
scores	O
from	O
1	O
-	O
5	O
denoting	O
their	O
semantic	B-TaskName
similarity	I-TaskName
.	O
3	O
BERT	B-MethodName
-	O
Score	B-MetricName
(	O
Zhang	O
et	O
al	O
,	O
2019a	O
)	O
;	O
Originally	O
a	O
quality	O
metric	O
,	O
BERT	B-MethodName
-	O
Score	B-MetricName
uses	O
BERT	B-MethodName
's	O
embeddings	O
to	O
measure	O
similarity	O
between	O
two	O
sen	O
-	O
tences	O
.	O
We	O
used	O
RoBERTa	B-MethodName
-	O
large	O
,	O
as	O
suggested	O
by	O
the	O
authors	O
.	O
4	O
Sentence	O
-	O
BERT	B-MethodName
(	O
sent	O
-	O
BERT	B-MethodName
)	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
is	O
a	O
sentence	O
-	O
level	O
embedding	O
model	O
based	O
on	O
BERT	B-MethodName
.	O
We	O
use	O
the	O
cosine	O
similarity	O
between	O
the	O
embeddings	O
of	O
two	O
responses	O
as	O
a	O
similarity	O
metric	O
.	O
In	O
our	O
experiments	O
we	O
used	O
bert	O
-	O
large	O
-	O
nli	O
-	O
stsb	O
-	O
mean	O
-	O
tokens	O
.	O
5	O
Human	O
Metrics	O
We	O
examine	O
four	O
methods	O
for	O
evaluating	O
diversity	O
with	O
humans	O
(	O
see	O
4	O
)	O
,	O
to	O
investigate	O
best	O
practices	O
for	O
obtaining	O
diversity	O
judgment	O
from	O
humans	O
.	O
In	O
all	O
metrics	O
(	O
except	O
ranking	O
)	O
,	O
ratings	O
are	O
from	O
5	O
(	O
highest	O
diversity	O
/	O
similarity	O
)	O
to	O
1	O
(	O
lowest	O
)	O
.	O
The	O
original	O
tasks	O
presented	O
to	O
workers	O
are	O
in	O
Appendix	O
A.	O
Absolute	O
HDS	O
(	O
absHDS	O
)	O
;	O
Given	O
a	O
context	O
c	O
and	O
a	O
set	O
of	O
generated	O
responses	O
S	O
c	O
,	O
rate	O
the	O
level	O
of	O
diversity	O
of	O
S	O
c	O
.	O
Ranking	O
HDS	O
(	O
rnkHDS	O
)	O
;	O
Given	O
a	O
context	O
c	O
and	O
two	O
sets	O
S	O
c	O
,	O
d	O
1	O
,	O
S	O
c	O
,	O
d	O
2	O
generated	O
with	O
different	O
values	O
of	O
the	O
diversity	O
parameter	O
d	O
,	O
rate	O
which	O
set	O
is	O
more	O
diverse	O
.	O
Since	O
this	O
metric	O
did	O
not	O
clearly	O
outperform	O
absHDS	O
,	O
we	O
provide	O
results	O
in	O
Appendix	O
C	O
only	O
.	O
Similarity	O
HDS	O
(	O
simHDS	O
)	O
;	O
Given	O
a	O
context	O
c	O
and	O
a	O
set	O
of	O
generated	O
responses	O
S	O
c	O
,	O
rate	O
the	O
similarity	O
of	O
each	O
two	O
sentences	O
in	O
S	O
c	O
,	O
and	O
then	O
apply	O
the	O
reduction	O
from	O
5	O
.	O
Aspect	O
HDS	O
(	O
aspHDS	O
)	O
;	O
Identical	O
to	O
absHDS	O
,	O
except	O
we	O
explicitly	O
ask	O
about	O
a	O
specific	O
aspect	O
of	O
diversity	O
,	O
namely	O
form	O
and	O
content	O
.	O
6	O

In	O
addition	O
to	O
Spearman	O
's	O
ρ	O
,	O
we	O
report	O
the	O
optimal	O
single	O
-	O
threshold	O
classifier	O
accuracy	B-MetricName
(	O
OCA	O
)	O
,	O
i.e.	O
,	O
the	O
best	O
achievable	O
accuracy	B-MetricName
in	O
predicting	O
the	O
class	O
of	O
a	O
response	O
set	O
(	O
high	O
or	O
low	O
content	O
diversity	O
)	O
for	O
any	O
threshold	O
η	O
on	O
m	O
div	O
,	O
such	O
that	O
if	O
m	O
div	O
(	O
S	O
c	O
)	O
>	O
η	O
the	O
classifier	O
predicts	O
high	O
diversity	O
,	O
and	O
otherwise	O
predicts	O
low	O
diversity	O
.	O
Table	O
4	O
shows	O
the	O
results	O
.	O
N	O
-	O
gram	O
-	O
based	O
metrics	O
perform	O
poorly	O
,	O
indicating	O
they	O
do	O
not	O
measure	O
content	O
diversity	O
well	O
.	O
Neural	O
models	O
perform	O
better	O
than	O
n	O
-	O
gram	O
-	O
based	O
metrics	O
(	O
especially	O
sent	O
-	O
BERT	B-MethodName
)	O
,	O
but	O
there	O
is	O
still	O
a	O
clear	O
gap	O
between	O
automatic	O
metrics	O
and	O
humans	O
.	O
Figure	O
4	O
illustrates	O
the	O
typical	O
distributions	O
of	O
n	O
-	O
gram	O
,	O
neural	O
and	O
human	O
metrics	O
.	O
Clearly	O
,	O
HDS	O
separates	O
high	O
and	O
low	O
content	O
diversity	O
better	O
than	O
neural	O
metrics	O
.	O
In	O
addition	O
,	O
n	O
-	O
gram	O
-	O
based	O
metrics	O
saturate	O
both	O
classes	O
to	O
near	O
maximal	O
values	O
,	O
similarly	O
to	O
decTest	O
.	O
Since	O
conTest	O
isolates	O
content	O
diversity	O
,	O
we	O
used	O
aspHDS	O
to	O
directly	O
rate	O
content	O
and	O
form	O
diversity	O
.	O
Content	O
aspHDS	O
gets	O
similar	O
scores	O
to	O
ab	O
-	O
sHDS	O
,	O
suggesting	O
little	O
gain	O
in	O
asking	O
directly	O
on	O
the	O
tested	O
aspect	O
.	O
Form	O
aspHDS	O
gets	O
low	O
scores	O
compared	O
to	O
absHDS	O
,	O
validating	O
that	O
the	O
form	O
diversity	O
of	O
the	O
two	O
classes	O
is	O
similar	O
.	O
sets	O
and	O
10	O
ratings	O
per	O
set	O
for	O
all	O
experimentsthe	O
minimal	O
values	O
in	O
which	O
results	O
are	O
confidently	O
stable	O
.	O
Results	O
are	O
presented	O
in	O
Figure	O
5	O
.	O

Comparing	O
decTest	O
results	O
of	O
storyGen	O
to	O
other	O
tasks	O
(	O
Table	O
2	O
)	O
,	O
this	O
task	O
is	O
characterised	O
with	O
noisier	O
scores	O
for	O
all	O
metrics	O
(	O
Figures	O
3	O
and	O
6	O
)	O
,	O
hence	O
lower	O
ρ	O
values	O
and	O
higher	O
variance	O
.	O
A	O
possible	O
explanation	O
is	O
larger	O
effect	O
of	O
c	O
on	O
the	O
distribution	O
P	O
gen	O
(	O
s	O
|	O
c	O
)	O
in	O
this	O
task	O
.	O
Tables	O
3	O
,	O
6	O
and	O
7	O
,	O
present	O
decTest	O
absolute	O
scoring	O
experiment	O
using	O
temperature	O
,	O
nucleus	O
sampling	O
and	O
Top	O
-	O
k	O
decoding	O
parameters	O
as	O
d.	O
Top	O
-	O
k	O
consistently	O
yields	O
lower	O
ρ	O
compared	O
to	O
other	O
decoding	O
parameters	O
,	O
especially	O
for	O
storyGen	O
task	O
.	O
This	O
implies	O
that	O
Top	O
-	O
k	O
represents	O
diversity	O
less	O
reliably	O
than	O
other	O
methods	O
.	O
Ranking	O
experiment	O
To	O
examine	O
whether	O
we	O
can	O
improve	O
correlation	O
by	O
asking	O
humans	O
to	O
rank	O
diversity	O
,	O
rather	O
than	O
providing	O
an	O
absolute	O
score	O
,	O
we	O
designed	O
a	O
ranking	O
version	O
of	O
decTest	O
.	O
Each	O
context	O
is	O
given	O
along	O
with	O
two	O
sets	O
(	O
5	O
samples	O
each	O
)	O
,	O
produced	O
with	O
different	O
temperature	O
values	O
.	O
We	O
sweep	O
over	O
temperature	O
differences	O
instead	O
of	O
the	O
absolute	O
temperature	O
values	O
.	O
The	O
human	O
metric	O
in	O
this	O
setting	O
is	O
rnkHDS	O
(	O
see	O
6.2	O
)	O
,	O
and	O
the	O
automatic	O
metrics	O
are	O
the	O
difference	O
between	O
the	O
scores	O
each	O
of	O
the	O
two	O
sets	O
got	O
.	O
We	O
report	O
two	O
measures	O
;	O
The	O
first	O
is	O
Spearman	O
's	O
ρ	O
between	O
the	O
metric	O
and	O
the	O
temperature	O
difference	O
.	O
The	O
second	O
is	O
accuracy	B-MetricName
,	O
i.e.	O
,	O
whether	O
the	O
metric	O
can	O
predict	O
which	O
set	O
has	O
higher	O
temperature	O
(	O
e.g.	O
,	O
in	O
automatic	O
metrics	O
this	O
is	O
whether	O
the	O
sign	O
of	O
the	O
temperature	O
difference	O
and	O
the	O
sign	O
of	O
metric	O
score	O
difference	O
agree	O
)	O
.	O
7	O
Table	O
5	O
summarizes	O
the	O
ranking	O
test	O
results	O
.	O
We	O
observe	O
that	O
humans	O
are	O
better	O
at	O
ranking	O
compared	O
to	O
giving	O
absolute	O
scores	O
(	O
Table	O
2	O
)	O
,	O
and	O
are	O
doing	O
as	O
well	O
as	O
automatic	O
metrics	O
.	O
However	O
,	O
the	O
scores	O
of	O
all	O
automatic	O
metrics	O
also	O
improve	O
,	O
making	O
it	O
difficult	O
to	O
separate	O
between	O
the	O
different	O
metrics	O
.	O

As	O
elaborated	O
in	O
6	O
.	O
tion	O
between	O
temperature	O
differences	O
and	O
each	O
metric	O
score	O
.	O
Accuracy	B-MetricName
(	O
acc	B-MetricName
)	O
of	O
classifying	O
which	O
set	O
has	O
the	O
higher	O
temperature	O
.	O
Standard	O
deviation	O
is	O
up	O
to	O
0.02	O
for	O
all	O
automatic	O
metrics	O
for	O
both	O
Spearman	O
's	O
correlation	O
and	O
accuracy	B-MetricName
.	O
metric	O
to	O
score	O
zero	O
correlation	O
in	O
conTest	O
over	O
this	O
subset	O
.	O
The	O
method	O
of	O
sub	O
-	O
sampling	O
was	O
meant	O
to	O
approximately	O
equalize	O
the	O
distributions	O
of	O
the	O
two	O
classes	O
,	O
low	O
and	O
high	O
content	O
diversity	O
,	O
over	O
the	O
scores	O
of	O
distinct	O
-	O
n	O
metric	O
,	O
and	O
was	O
performed	O
as	O
follows	O
:	O
Sort	O
all	O
collected	O
samples	O
(	O
from	O
both	O
low	O
and	O
high	O
content	O
diversity	O
classes	O
)	O
according	O
to	O
their	O
distinct	O
-	O
n	O
score	O
.	O
Divide	O
the	O
sorted	O
samples	O
to	O
groups	O
with	O
fixed	O
size	O
(	O
40	O
samples	O
each	O
in	O
our	O
case	O
)	O
.	O
From	O
each	O
such	O
group	O
,	O
randomly	O
sample	O
the	O
same	O
amount	O
of	O
samples	O
for	O
each	O
of	O
the	O
two	O
classes	O
.	O
For	O
example	O
,	O
if	O
a	O
group	O
contains	O
5	O
low	O
content	O
diversity	O
samples	O
and	O
35	O
high	O
content	O
diversity	O
samples	O
,	O
we	O
can	O
sample	O
at	O
most	O
5	O
samples	O
for	O
each	O
class	O
.	O

Response	O
set	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
)	O
Response	O
set	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
32	O
)	O
Response	O
set	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
318	O
)	O
Loud	O
Noise	O
.	O
Jane	O
was	O
trying	O
to	O
take	O
a	O
nap	O
.	O
She	O
heard	O
a	O
loud	O
bang	O
in	O
the	O
kitchen	O
.	O
It	O
woke	O
her	O
up	O
.	O
A	O
dish	O
had	O
fallen	O
off	O
the	O
counter	O
.	O
Jane	O
pulled	O
over	O
and	O
started	O
to	O
clean	O
herself	O
.	O
Jane	O
was	O
horrified	O
and	O
dropped	O
her	O
favorite	O
food	O
.	O
Jane	O
was	O
able	O
to	O
finish	O
her	O
car	O
and	O
take	O
a	O
nap	O
.	O
Jane	O
was	O
able	O
to	O
finish	O
her	O
nap	O
.	O
Jane	O
was	O
able	O
to	O
finish	O
her	O
nap	O
.	O
Jane	O
stopped	O
at	O
the	O
store	O
to	O
buy	O
a	O
new	O
one	O
.	O
Jane	O
was	O
able	O
to	O
grab	O
a	O
nap	O
afterwards	O
.	O
Jane	O
was	O
unable	O
to	O
finish	O
her	O
nap	O
since	O
it	O
was	O
lost	O
.	O
Jane	O
pulled	O
over	O
and	O
started	O
to	O
clean	O
up	O
.	O
Jane	O
was	O
able	O
to	O
finish	O
her	O
nap	O
.	O
Jane	O
was	O
able	O
to	O
catch	O
a	O
car	O
using	O
the	O
seat	O
.	O
Jane	O
stopped	O
at	O
the	O
store	O
to	O
buy	O
a	O
new	O
book	O
.	O
Jane	O
was	O
sad	O
her	O
cat	O
dropped	O
out	O
of	O
the	O
kitchen	O
.	O
Jane	O
screamed	O
.	O
Jane	O
was	O
horrified	O
to	O
find	O
her	O
car	O
broken	O
down	O
on	O
the	O
floor	O
.	O
Jane	O
was	O
horrified	O
and	O
dropped	O
her	O
pay	O
phone	O
.	O
Jane	O
was	O
easily	O
able	O
to	O
grab	O
a	O
nap	O
.	O
Jane	O
pulled	O
over	O
and	O
started	O
to	O
cry	O
.	O
Jane	O
pulled	O
over	O
and	O
started	O
to	O
cry	O
.	O
Jane	O
stopped	O
at	O
the	O
store	O
to	O
buy	O
a	O
new	O
dish	O
from	O
the	O
store	O
.	O
Jane	O
comes	O
,	O
noticed	O
a	O
lot	O
of	O
food	O
left	O
under	O
it	O
.	O
Jane	O
was	O
horrified	O
and	O
dropped	O
her	O
book	O
.	O
Jane	O
remembered	O
to	O
take	O
a	O
nap	O
.	O
Jane	O
was	O
since	O
she	O
took	O
a	O
nap	O
while	O
she	O
waited	O
for	O
the	O
refun	O
Jane	O
knew	O
she	O
had	O
no	O
time	O
to	O
finish	O
her	O
book	O
.	O
Jane	O
was	O
glad	O
.	O
Jane	O
was	O
annoyed	O
and	O
began	O
to	O
cry	O
.	O
Jane	O
stopped	O
at	O
the	O
store	O
to	O
buy	O
a	O
new	O
one	O
.	O
Jane	O
wanted	O
to	O
have	O
her	O
car	O
back	O
.	O
Jane	O
was	O
monthed	O
.	O

Response	O
set	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
)	O
Response	O
set	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
32	O
)	O
Response	O
set	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
318	O
)	O
watching	O
curry	O
play	O
in	O
his	O
prime	O
is	O
truly	O
a	O
privilege	O
i	O
know	O
i	O
just	O
do	O
nt	O
want	O
him	O
to	O
play	O
for	O
us	O
he	O
has	O
to	O
be	O
a	O
good	O
center	O
for	O
that	O
he	O
is	O
a	O
great	O
center	O
of	O
football	O
in	O
his	O
prime	O
he	O
s	O
been	O
playing	O
in	O
his	O
prime	O
for	O
a	O
long	O
time	O
he	O
was	O
a	O
great	O
back	O
in	O
the	O
day	O
he	O
s	O
been	O
playing	O
for	O
a	O
while	O
now	O
i	O
do	O
nt	O
know	O
about	O
that	O
he	O
was	O
pretty	O
damn	O
good	O
at	O
that	O
i	O
do	O
nt	O
think	O
he	O
was	O
ever	O
in	O
his	O
prime	O
i	O
do	O
nt	O
think	O
he	O
is	O
a	O
prime	O
minister	O
i	O
do	O
nt	O
know	O
why	O
but	O
i	O
think	O
he	O
is	O
a	O
very	O
good	O
player	O
and	O
and	O
his	O
career	O
as	O
well	O
agreed	O
the	O
way	O
he	O
s	O
playing	O
is	O
awesome	O
it	O
has	O
to	O
be	O
this	O
is	O
just	O
called	O
a	O
job	O
and	O
then	O
being	O
on	O
the	O
field	O
for	O
the	O
first	O
time	O
i	O
do	O
nt	O
see	O
him	O
doing	O
that	O
often	O
enough	O
he	O
just	O
likes	O
to	O
party	O
in	O
the	O
kitchen	O
at	O
this	O
point	O
he	O
s	O
going	O
to	O
be	O
a	O
great	O
star	O
for	O
the	O
rest	O
of	O
the	O
only	O
if	O
he	O
pays	O
well	O
the	O
only	O
thing	O
that	O
can	O
make	O
that	O
kind	O
of	O
difference	O
is	O
how	O
much	O
time	O
you	O
yeah	O
my	O
feeling	O
i	O
mean	O
we	O
do	O
nt	O
like	O
it	O
but	O
it	O
happens	O
all	O
the	O
you	O
are	O
one	O
for	O
real	O
they	O
still	O
have	O
a	O
rule	O
saying	O
they	O
might	O
not	O
be	O
injured	O
yet	O
it	O
really	O
is	O
a	O
necessary	O
thing	O
to	O
do	O
finally	O
some	O
reason	O
to	O
continue	O
watching	O
him	O
at	O
some	O
point	O
yet	O
that	O
would	O
be	O
epic	O
not	O
to	O
mention	O
eating	O
curry	O
dinner	O
is	O
a	O
privilege	O
i	O
just	O
do	O
nt	O
want	O
to	O
turn	O
over	O
for	O
this	O
goal	O
like	O
he	O
does	O
in	O
gt	O
playing	O
in	O
his	O
prime	O
is	O
truly	O
a	O
privilege	O
ftfy	O
so	O
is	O
saying	O
he	O
is	O
in	O
high	O
school	O
i	O
m	O
going	O
this	O
evening	O
when	O
she	O
usually	O
works	O
i	O
think	O
you	O
accidentally	O
a	O
word	O
you	O
are	O
a	O
good	O
man	O
i	O
hope	O
she	O
works	O
i	O
m	O
going	O
to	O
the	O
same	O
time	O
as	O
you	O
when	O
she	O
usually	O
works	O
i	O
am	O
so	O
sorry	O
to	O
hear	O
that	O
i	O
hope	O
she	O
works	O
for	O
you	O
i	O
am	O
so	O
jealous	O
of	O
your	O
work	O
i	O
am	O
so	O
jealous	O
i	O
hope	O
you	O
have	O
fun	O
i	O
hope	O
you	O
get	O
a	O
job	O
at	O
a	O
local	O
bar	O
i	O
hope	O
she	O
works	O
for	O
you	O
Response	O
set	O
(	O
τ	O
=	O
0.25	O
)	O
Response	O
set	O
(	O
τ	O
=	O
0.8	O
)	O
Response	O
set	O
(	O
τ	O
=	O
1.1	O
)	O
Not	O
the	O
hacking	O
.	O
The	O
hacking	O
is	O
the	O
fact	O
that	O
the	O
DNC	O
was	O
hacked	O
.	O
!	O
Not	O
the	O
hacking	O
.	O
The	O
hacking	O
is	O
the	O
real	O
problem	O
.	O
The	O
hacking	O
is	O
the	O
!	O
Not	O
the	O
hacking	O
of	O
the	O
DNC	O
,	O
but	O
the	O
leaks	O
of	O
the	O
emails	O
of	O
the	O
Democratic	O
National	O
Committee	O
.	O
!	O
Not	O
the	O
hacking	O
,	O
but	O
the	O
way	O
it	O
was	O
done	O
.	O
The	O
FBI	O
's	O
investigation	O
into	O
the	O
!	O
Not	O
the	O
hacking	O
of	O
the	O
DNC	O
,	O
but	O
the	O
hacking	O
of	O
the	O
emails	O
of	O
the	O
Democratic	O
National	O
Committee	O
.	O
!	O
Not	O
the	O
hacking	O
of	O
the	O
DNC	O
,	O
but	O
the	O
leaking	O
of	O
the	O
emails	O
.	O
The	O
DNC	O
's	O
!	O
Not	O
the	O
hacking	O
of	O
the	O
DNC	O
.	O
The	O
hacking	O
of	O
the	O
DNC	O
was	O
a	O
"	O
false	O
flag	O
!	O
Not	O
the	O
hacking	O
of	O
the	O
DNC	O
.	O
But	O
the	O
hacking	O
of	O
the	O
RNC	O
.	O
The	O
DNC	O
hack	O
!	O
Not	O
the	O
hacking	O
.	O
The	O
hacking	O
is	O
the	O
problem	O
.	O
The	O
hacking	O
is	O
the	O
problem	O
!	O
Not	O
the	O
hacking	O
of	O
the	O
DNC	O
,	O
but	O
the	O
leaking	O
of	O
the	O
emails	O
.	O
The	O
DNC	O
was	O
hacked	O
,	O
!	O
Not	O
the	O
hacking	O
after	O
all	O
?	O
I	O
'm	O
sure	O
the	O
nation	O
-	O
states	O
that	O
are	O
involved	O
in	O
!	O
Not	O
the	O
hacking	O
that	O
happened	O
on	O
the	O
internal	O
networks	O
of	O
the	O
Energy	O
Department	O
.	O
In	O
fact	O
,	O
according	O
to	O
!	O
Not	O
the	O
hacking	O
of	O
the	O
American	O
public	O
but	O
rather	O
the	O
fraudulent	O
Heisenberg	O
principle	O
that	O
seemed	O
to	O
be	O
!	O
Not	O
the	O
hacking	O
that	O
took	O
place	O
in	O
the	O
DNC	O
last	O
year	O
or	O
the	O
release	O
of	O
hacked	O
emails	O
during	O
the	O
!	O
Not	O
the	O
hacking	O
futurists	O
Cardboard	O
inventor	O
and	O
self	O
-	O
described	O
tinkerer	O
Dennis	O
!	O
Not	O
the	O
hacking	O
alone	O
.	O
In	O
the	O
first	O
half	O
of	O
the	O
report	O
,	O
the	O
hackers	O
tried	O
to	O
create	O
fake	O
!	O
(	O
Top	O
-	O
k	O
)	O
.	O
Bold	O
text	O
is	O
the	O
3	O
-	O
words	O
prompt	O
context	O
.	O

Recent	O
works	O
have	O
shown	O
that	O
supervised	O
models	O
often	O
exploit	O
data	O
artifacts	O
to	O
achieve	O
good	O
test	O
scores	O
while	O
their	O
performance	O
severely	O
degrades	O
on	O
samples	O
outside	O
their	O
training	O
distribution	O
.	O
Contrast	O
sets	O
quantify	O
this	O
phenomenon	O
by	O
perturbing	O
test	O
samples	O
in	O
a	O
minimal	O
way	O
such	O
that	O
the	O
output	O
label	O
is	O
modified	O
.	O
While	O
most	O
contrast	O
sets	O
were	O
created	O
manually	O
,	O
requiring	O
intensive	O
annotation	O
effort	O
,	O
we	O
present	O
a	O
novel	O
method	O
which	O
leverages	O
rich	O
semantic	O
input	O
representation	O
to	O
automatically	O
generate	O
contrast	O
sets	O
for	O
the	O
visual	B-DatasetName
question	I-DatasetName
answering	I-DatasetName
task	O
.	O
Our	O
method	O
computes	O
the	O
answer	O
of	O
perturbed	O
questions	O
,	O
thus	O
vastly	O
reducing	O
annotation	O
cost	O
and	O
enabling	O
thorough	O
evaluation	O
of	O
models	O
'	O
performance	O
on	O
various	O
semantic	O
aspects	O
(	O
e.g.	O
,	O
spatial	O
or	O
relational	B-TaskName
reasoning	I-TaskName
)	O
.	O
We	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
approach	O
on	O
the	O
popular	O
GQA	B-DatasetName
dataset	O
(	O
Hudson	O
and	O
Manning	O
,	O
2019	O
)	O
and	O
its	O
semantic	O
scene	O
graph	O
image	O
representation	O
.	O
We	O
find	O
that	O
,	O
despite	O
GQA	B-DatasetName
's	O
compositionality	O
and	O
carefully	O
balanced	O
label	O
distribution	O
,	O
two	O
strong	O
models	O
drop	O
13	O
-	O
17	O
%	O
in	O
accuracy	B-MetricName
on	O
our	O
automatically	O
-	O
constructed	O
contrast	O
set	O
compared	O
to	O
the	O
original	O
validation	O
set	O
.	O
Finally	O
,	O
we	O
show	O
that	O
our	O
method	O
can	O
be	O
applied	O
to	O
the	O
training	O
set	O
to	O
mitigate	O
the	O
degradation	O
in	O
performance	O
,	O
opening	O
the	O
door	O
to	O
more	O
robust	O
models	O
.	O
1	O

We	O
design	O
a	O
perturbation	O
method	O
which	O
guarantees	O
a	O
change	O
in	O
the	O
gold	O
answer	O
for	O
each	O
question	O
template	O
.	O
For	O
example	O
,	O
looking	O
at	O
Fig	O
.	O
2	O
,	O
for	O
the	O
question	O
template	O
are	O
there	O
X	O
near	O
the	O
Y	O
?	O
(	O
e.g.	O
,	O
"	O
Is	O
there	O
any	O
fence	O
near	O
the	O
players	O
?	O
"	O
)	O
,	O
we	O
replace	O
either	O
X	O
or	O
Y	O
with	O
a	O
probable	O
distractor	O
(	O
e.g.	O
"	O
replace	O
"	O
fence	O
"	O
with	O
"	O
trees	O
"	O
)	O
.	O
We	O
use	O
the	O
scene	O
graph	O
to	O
ensure	O
that	O
the	O
answer	O
to	O
the	O
question	O
is	O
indeed	O
changed	O
.	O
In	O
our	O
example	O
,	O
this	O
would	O
entail	O
grounding	O
"	O
players	O
"	O
in	O
the	O
question	O
to	O
the	O
scene	O
graph	O
(	O
either	O
via	O
exact	B-MetricName
match	I-MetricName
or	O
several	O
other	O
heuristics	O
such	O
as	O
hard	O
-	O
coded	O
lists	O
of	O
synonyms	O
or	O
co	O
-	O
hyponyms	O
)	O
,	O
locating	O
its	O
neighbors	O
,	O
and	O
verifying	O
that	O
none	O
of	O
them	O
are	O
"	O
trees	O
.	O
"	O
We	O
then	O
apply	O
heuristics	O
to	O
fix	O
syntax	O
(	O
e.g.	O
,	O
changing	O
from	O
singular	O
to	O
plural	O
determiner	O
,	O
see	O
Appendix	O
A.3	O
)	O
,	O
and	O
verify	O
that	O
the	O
perturbed	O
sample	O
Are	O
there	O
any	O
cats	O
near	O
the	O
boat	O
?	O
Is	O
there	O
any	O
bush	O
near	O
the	O
boat	O
?	O
Is	O
the	O
X	O
Rel	O
the	O
Y	O
?	O
Is	O
the	O
boy	O
to	O
the	O
right	O
of	O
the	O
man	O
?	O
Is	O
the	O
boy	O
to	O
the	O
left	O
of	O
the	O
man	O
?	O
Is	O
the	O
X	O
Rel	O
the	O
Y	O
?	O
Is	O
the	O
boy	O
to	O
the	O
right	O
of	O
the	O
man	O
?	O
Is	O
the	O
zebra	O
to	O
the	O
right	O
of	O
the	O
man	O
?	O
does	O
not	O
already	O
exist	O
in	O
GQA	B-DatasetName
.	O
The	O
specific	O
perturbation	O
is	O
performed	O
per	O
question	O
template	O
.	O
In	O
question	O
templates	O
with	O
two	O
objects	O
(	O
X	O
and	O
Y	O
)	O
,	O
we	O
replace	O
X	O
with	O
X	O
'	O
,	O
such	O
that	O
X	O
'	O
is	O
correlated	O
with	O
Y	O
in	O
other	O
GQA	B-DatasetName
scene	O
graphs	O
.	O
In	O
question	O
templates	O
with	O
a	O
single	O
object	O
X	O
,	O
we	O
replace	O
X	O
with	O
a	O
textually	O
-	O
similar	O
X	O
'	O
.	O
For	O
example	O
in	O
the	O
first	O
row	O
in	O
Table	O
1	O
we	O
replace	O
dishwasher	O
with	O
dishes	O
.	O
Our	O
perturbation	O
code	O
is	O
publicly	O
available	O
.	O
This	O
process	O
may	O
yield	O
an	O
arbitrarily	O
large	O
number	O
of	O
contrasting	O
samples	O
per	O
question	O
,	O
as	O
there	O
are	O
many	O
candidates	O
for	O
replacing	O
objects	O
participating	O
in	O
questions	O
.	O
We	O
report	O
experiments	O
with	O
up	O
to	O
1	O
,	O
3	O
and	O
5	O
contrasting	O
samples	O
per	O
question	O
.	O
Illustrating	O
the	O
perturbation	O
process	O
.	O
Looking	O
at	O
Fig	O
.	O
1	O
,	O
we	O
see	O
the	O
scene	O
-	O
graph	O
information	O
:	O
objects	O
have	O
bounding	O
-	O
boxes	O
around	O
them	O
in	O
the	O
image	O
(	O
e.g.	O
,	O
zebra	O
)	O
;	O
Objects	O
have	O
attributes	O
(	O
wood	O
is	O
an	O
attribute	O
of	O
the	O
fence	O
object	O
)	O
;	O
and	O
there	O
are	O
relationships	O
between	O
the	O
objects	O
(	O
the	O
puddle	O
is	O
to	O
the	O
right	O
of	O
the	O
zebra	O
,	O
and	O
it	O
is	O
near	O
the	O
fence	O
)	O
.	O
The	O
original	O
(	O
question	O
,	O
answer	O
)	O
pair	O
is	O
(	O
"	O
is	O
there	O
a	O
fence	O
near	O
the	O
puddle	O
?	O
"	O
,	O
"	O
Yes	O
"	O
)	O
.	O
We	O
first	O
identify	O
the	O
question	O
template	O
by	O
regular	O
expressions	O
:	O
"	O
Is	O
there	O
X	O
near	O
the	O
Y	O
"	O
,	O
and	O
isolate	O
X	O
=	O
fence	O
,	O
Y	O
=	O
puddle	O
.	O
The	O
answer	O
is	O
"	O
Yes	O
"	O
,	O
so	O
we	O
know	O
that	O
X	O
is	O
indeed	O
near	O
Y.	O
We	O
then	O
use	O
the	O
existing	O
information	O
given	O
in	O
the	O
scene	O
-	O
graph	O
.	O
We	O
search	O
for	O
X	O
'	O
that	O
is	O
not	O
near	O
Y.	O
To	O
achieve	O
this	O
,	O
we	O
sample	O
a	O
random	O
object	O
(	O
wall	O
)	O
,	O
and	O
verify	O
that	O
it	O
does	O
n't	O
exist	O
in	O
the	O
set	O
of	O
scenegraph	O
objects	O
.	O
This	O
results	O
in	O
a	O
perturbed	O
example	O
"	O
Is	O
there	O
a	O
wall	O
near	O
the	O
puddle	O
?	O
"	O
,	O
and	O
now	O
the	O
ground	O
truth	O
is	O
computed	O
to	O
be	O
"	O
No	O
"	O
.	O
Consider	O
a	O
different	O
example	O
:	O
(	O
"	O
Is	O
the	O
puddle	O
to	O
the	O
left	O
of	O
the	O
zebra	O
?	O
"	O
,	O
"	O
Yes	O
"	O
)	O
.	O
We	O
identify	O
the	O
question	O
template	O
"	O
Is	O
the	O
X	O
Rel	O
the	O
Y	O
"	O
,	O
where	O
X	O
=	O
puddle	O
,	O
Rel	O
=	O
to	O
the	O
left	O
,	O
Y	O
=	O
zebra	O
.	O
The	O
answer	O
is	O
"	O
Yes	O
"	O
.	O
Now	O
we	O
can	O
easily	O
change	O
Rel'=to	O
the	O
right	O
,	O
resulting	O
in	O
the	O
(	O
question	O
,	O
answer	O
)	O
pair	O
(	O
"	O
Is	O
the	O
puddle	O
to	O
the	O
right	O
of	O
the	O
zebra	O
?	O
"	O
,	O
"	O
No	O
"	O
)	O
.	O
We	O
highlight	O
the	O
following	O
:	O
(	O
1	O
)	O
This	O
process	O
is	O
done	O
entirely	O
automatically	O
(	O
we	O
validate	O
it	O
in	O
Section	O
2.3	O
)	O
;	O
(	O
2	O
)	O
The	O
answer	O
is	O
deterministic	O
given	O
the	O
information	O
in	O
the	O
scene	O
-	O
graph	O
;	O
(	O
3	O
)	O
We	O
do	O
not	O
produce	O
unanswerable	O
questions	O
.	O
If	O
we	O
could	O
n't	O
find	O
an	O
alternative	O
atom	O
for	O
which	O
the	O
presuppositions	O
hold	O
,	O
we	O
do	O
not	O
create	O
the	O
perturbed	O
(	O
question	O
,	O
answer	O
)	O
pair	O
;	O
(	O
4	O
)	O
Grounding	O
objects	O
from	O
the	O
question	O
to	O
the	O
scene	O
-	O
graph	O
can	O
be	O
tricky	O
.	O
It	O
can	O
involve	O
exact	B-MetricName
match	I-MetricName
,	O
number	O
match	O
(	O
dogs	O
in	O
the	O
question	O
,	O
and	O
dog	O
in	O
the	O
scene	O
-	O
graph	O
)	O
,	O
hyponyms	O
(	O
animal	O
in	O
the	O
question	O
,	O
and	O
dog	O
in	O
the	O
scene	O
-	O
graph	O
)	O
,	O
and	O
synonyms	O
(	O
motorbike	O
in	O
the	O
question	O
,	O
and	O
motorcycle	O
in	O
the	O
scene	O
-	O
graph	O
)	O
.	O
The	O
details	O
are	O
in	O
the	O
published	O
code	O
;	O
(	O
5	O
)	O
The	O
only	O
difference	O
between	O
the	O
original	O
and	O
the	O
perturbed	O
instance	O
is	O
a	O
single	O
atom	O
:	O
an	O
object	O
,	O
relationship	O
,	O
or	O
attribute	O
.	O

We	O
experiment	O
with	O
two	O
top	O
-	O
performing	O
GQA	B-DatasetName
models	O
,	O
MAC	O
(	O
Hudson	O
and	O
Manning	O
,	O
2018	O
)	O
and	O
LXMERT	B-MethodName
(	O
Tan	O
and	O
Bansal	O
,	O
2019	O
)	O
,	O
3	O
to	O
test	O
their	O
generalization	O
on	O
our	O
automatic	O
contrast	O
sets	O
,	O
leading	O
to	O
various	O
key	O
observations	O
.	O
Models	O
struggle	O
with	O
our	O
contrast	O
set	O
.	O
had	O
the	O
smallest	O
performance	O
drop	O
,	O
potentially	O
because	O
the	O
models	O
performance	O
on	O
such	O
multi	O
-	O
class	O
,	O
subjective	O
questions	O
is	O
relatively	O
low	O
to	O
begin	O
with	O
.	O
Training	O
on	O
perturbed	O
set	O
leads	O
to	O
more	O
robust	O
models	O
.	O
Previous	O
works	O
tried	O
to	O
mitigate	O
spurious	O
datasets	O
biases	O
by	O
explicitly	O
balancing	O
labels	O
during	O
dataset	O
construction	O
(	O
Goyal	O
et	O
al	O
,	O
2017	O
;	O
Zhu	O
et	O
al	O
,	O
2016	O
;	O
Zhang	O
et	O
al	O
,	O
2016	O
)	O
or	O
using	O
adversarial	O
filtering	O
(	O
Zellers	O
et	O
al	O
,	O
2018	O
(	O
Zellers	O
et	O
al	O
,	O
,	O
2019	O
.	O
In	O
this	O
work	O
we	O
take	O
an	O
inoculation	O
approach	O
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
This	O
allows	O
us	O
to	O
measure	O
the	O
contrast	O
consistency	O
of	O
our	O
contrast	O
set	O
,	O
defined	O
as	O
the	O
percentage	O
of	O
the	O
contrast	O
sets	O
for	O
which	O
a	O
model	O
's	O
predictions	O
are	O
correct	O
for	O
all	O
examples	O
in	O
the	O
set	O
(	O
including	O
the	O
original	O
example	O
)	O
.	O
For	O
example	O
,	O
in	O
Fig	O
.	O
1	O
the	O
set	O
size	O
is	O
4	O
,	O
and	O
only	O
2/4	O
predictions	O
are	O
correct	O
.	O
We	O
experiment	O
with	O
1	O
,	O
3	O
,	O
and	O
5	O
augmentations	O
per	O
question	O
with	O
the	O
LXMERT	B-MethodName
model	O
trained	O
on	O
the	O
original	O
GQA	B-DatasetName
training	O
set	O
.	O
Our	O
results	O
(	O
Table	O
4	O
)	O
show	O
that	O
sampling	O
more	O
objects	O
leads	O
to	O
similar	O
accuracy	B-MetricName
levels	O
for	O
the	O
LXMERT	B-MethodName
model	O
,	O
indicating	O
that	O
quality	O
of	O
our	O
contrast	O
sets	O
does	O
not	O
depend	O
on	O
the	O
specific	O
selection	O
of	O
replacements	O
.	O
However	O
,	O
we	O
observe	O
that	O
consistency	O
drops	O
fast	O
as	O
the	O
size	O
of	O
the	O
contrast	O
sets	O
per	O
QA	O
instance	O
grows	O
,	O
indicating	O
that	O
model	O
success	O
on	O
a	O
specific	O
instance	O
does	O
not	O
mean	O
it	O
can	O
generalize	O
robustly	O
to	O
perturbations	O
.	O

Neural	O
methods	O
for	O
abstractive	O
summarization	B-TaskName
(	O
Rush	O
et	O
al	O
,	O
2015	O
;	O
Nallapati	O
et	O
al	O
,	O
2016	O
;	O
Lewis	O
et	O
al	O
,	O
2020	O
;	O
formulate	O
summarization	B-TaskName
as	O
a	O
sequenceto	O
-	O
sequence	O
(	O
Seq2Seq	B-MethodName
)	O
problem	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
,	O
learning	O
to	O
generate	O
the	O
summary	O
in	O
an	O
autoregressive	O
manner	O
.	O
Such	O
models	O
are	O
commonly	O
trained	O
with	O
maximum	O
likelihood	O
estimation	O
(	O
MLE	O
)	O
,	O
maximizing	O
predictive	O
probability	O
of	O
the	O
reference	O
output	O
given	O
the	O
gold	O
sub	O
-	O
sequence	O
before	O
it	O
.	O
However	O
,	O
during	O
inference	O
the	O
model	O
must	O
also	O
generate	O
the	O
output	O
based	O
on	O
possibly	O
erroneous	O
previous	O
steps	O
.	O
This	O
can	O
hurt	O
model	O
performance	O
,	O
a	O
phenomenon	O
often	O
called	O
exposure	O
bias	O
(	O
Bengio	O
et	O
al	O
,	O
2015	O
;	O
Ranzato	O
et	O
al	O
,	O
2016	O
)	O
.	O
To	O
maintain	O
reasonable	O
performance	O
even	O
in	O
the	O
case	O
of	O
a	O
sub	O
-	O
sequence	O
with	O
errors	O
,	O
we	O
argue	O
that	O
the	O
The	O
candidate	O
summaries	O
are	O
generated	O
by	O
a	O
pre	O
-	O
trained	O
model	O
(	O
BART	B-MethodName
)	O
,	O
and	O
we	O
select	O
the	O
best	O
and	O
the	O
worst	O
candidates	O
(	O
w.r.t	O
.	O
ROUGE	O
scores	O
)	O
for	O
each	O
of	O
the	O
samples	O
.	O
High	O
and	O
Low	O
represent	O
the	O
average	O
performance	O
of	O
the	O
best	O
and	O
worst	O
candidates	O
respectively	O
.	O
R	O
-	O
1/2	O
/	O
L	O
are	O
the	O
ROUGE	O
-	O
1/2	O
/	O
L	O
scores	O
.	O
The	O
original	O
BART	B-MethodName
only	O
achieves	O
54.80	O
%	O
accuracy	B-MetricName
.	O
model	O
must	O
accurately	O
estimate	O
relative	O
quality	O
of	O
different	O
generated	O
outputs	O
,	O
since	O
effective	O
inference	O
requires	O
comparison	O
among	O
these	O
candidates	O
.	O
To	O
understand	O
whether	O
existing	O
models	O
can	O
accurately	O
perform	O
such	O
relative	O
comparisons	O
,	O
we	O
conducted	O
a	O
preliminary	O
study	O
on	O
pre	O
-	O
trained	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
,	O
first	O
generating	O
two	O
candidate	O
summaries	O
from	O
the	O
model	O
and	O
observing	O
whether	O
a	O
higher	O
probability	O
is	O
assigned	O
to	O
the	O
candidate	O
with	O
a	O
higher	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
score	O
.	O
As	O
Tab	O
.	O
1	O
shows	O
,	O
the	O
accuracy	B-MetricName
is	O
far	O
from	O
ideal	O
.	O
This	O
is	O
likely	O
due	O
to	O
the	O
fact	O
that	O
MLE	O
training	O
only	O
encourages	O
the	O
model	O
to	O
assign	O
high	O
probability	O
to	O
the	O
reference	O
summary	O
,	O
and	O
is	O
agnostic	O
about	O
any	O
relative	O
comparison	O
between	O
non	O
-	O
reference	O
summaries	O
.	O
However	O
,	O
we	O
argue	O
that	O
it	O
is	O
also	O
important	O
for	O
the	O
order	O
of	O
model	O
scores	O
to	O
be	O
coordinated	O
with	O
the	O
actual	O
quality	O
metrics	O
by	O
which	O
the	O
summaries	O
will	O
be	O
evaluated	O
-	O
higher	O
model	O
scores	O
should	O
indicate	O
better	O
quality	O
summaries	O
.	O
In	O
the	O
following	O
we	O
will	O
refer	O
to	O
models	O
that	O
have	O
such	O
scores	O
as	O
"	O
coordinated	O
"	O
for	O
conciseness	O
.	O
We	O
introduce	O
a	O
training	O
paradigm	O
which	O
requires	O
the	O
abstractive	O
model	O
to	O
be	O
able	O
to	O
be	O
accurate	O
with	O
respect	O
to	O
predicting	O
the	O
tokens	O
in	O
the	O
reference	O
summaries	O
and	O
coordinated	O
with	O
respect	O
to	O
Figure	O
1	O
:	O
Comparison	O
of	O
MLE	O
loss	B-MetricName
(	O
LMLE	O
)	O
and	O
the	O
contrastive	O
loss	B-MetricName
(	O
LCtr	O
)	O
in	O
our	O
method	O
.	O
MLE	O
assumes	O
a	O
deterministic	O
(	O
one	O
-	O
point	O
)	O
distribution	O
,	O
in	O
which	O
the	O
reference	O
summary	O
receives	O
all	O
the	O
probability	O
mass	O
.	O
Our	O
method	O
assumes	O
a	O
nondeterministic	O
distribution	O
in	O
which	O
system	O
-	O
generated	O
summaries	O
also	O
receive	O
probability	O
mass	O
according	O
to	O
their	O
quality	O
.	O
The	O
contrastive	O
loss	B-MetricName
encourages	O
the	O
order	O
of	O
model	O
-	O
predicted	O
probabilities	O
of	O
candidate	O
summaries	O
to	O
be	O
coordinated	O
with	O
the	O
actual	O
quality	O
metric	O
M	O
by	O
which	O
the	O
summaries	O
will	O
be	O
evaluated	O
.	O
We	O
assign	O
the	O
abstractive	O
model	O
a	O
dual	O
role	O
-	O
a	O
single	O
model	O
could	O
be	O
used	O
both	O
as	O
a	O
generation	O
model	O
and	O
a	O
reference	O
-	O
free	O
evaluation	O
model	O
.	O
the	O
candidate	O
summaries	O
.	O
In	O
other	O
words	O
,	O
we	O
give	O
the	O
abstractive	O
model	O
a	O
dual	O
role	O
:	O
as	O
a	O
generation	O
model	O
,	O
it	O
generates	O
the	O
output	O
summaries	O
in	O
an	O
autoregressive	O
way	O
;	O
as	O
an	O
evaluation	O
model	O
,	O
it	O
can	O
be	O
used	O
to	O
score	O
the	O
quality	O
of	O
candidate	O
summaries	O
by	O
estimating	O
a	O
probability	O
distribution	O
over	O
candidate	O
outputs	O
.	O
The	O
generation	O
model	O
is	O
trained	O
using	O
the	O
standard	O
MLE	O
loss	B-MetricName
,	O
but	O
to	O
train	O
the	O
evaluation	O
model	O
we	O
introduce	O
a	O
contrastive	O
loss	B-MetricName
(	O
Hadsell	O
et	O
al	O
,	O
2006	O
)	O
defined	O
over	O
different	O
candidate	O
summaries	O
generated	O
by	O
pre	O
-	O
trained	O
abstractive	O
models	O
(	O
Fig	O
.	O
1	O
)	O
,	O
following	O
previous	O
work	O
on	O
ranking	O
-	O
based	O
or	O
contrastive	B-MethodName
learning	I-MethodName
(	O
Hopkins	O
and	O
May	O
,	O
2011	O
;	O
Zhong	O
et	O
al	O
,	O
2020	O
;	O
Liu	O
et	O
al	O
,	O
2021b	O
)	O
.	O
Our	O
main	O
contribution	O
is	O
to	O
change	O
the	O
target	O
distribution	O
of	O
abstractive	O
models	O
from	O
a	O
one	O
-	O
point	O
deterministic	O
distribution	O
assumed	O
by	O
MLE	O
training	O
to	O
a	O
non	O
-	O
deterministic	O
distribution	O
in	O
which	O
candidate	O
summaries	O
are	O
also	O
assigned	O
probability	O
mass	O
according	O
to	O
their	O
quality	O
.	O
The	O
new	O
SOTA	O
performance	O
on	O
CNN	O
/	O
DailyMail	O
(	O
Hermann	O
et	O
al	O
,	O
2015	O
)	O
and	O
XSum	B-DatasetName
(	O
Narayan	O
et	O
al	O
,	O
2018	O
)	O
datasets	O
demonstrated	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O
Our	O
in	O
-	O
depth	O
analysis	O
also	O
found	O
that	O
the	O
abstractive	O
models	O
trained	O
using	O
our	O
method	O
can	O
estimate	O
the	O
candidate	O
summary	O
quality	O
more	O
accurately	O
,	O
in	O
concert	O
with	O
the	O
the	O
objective	O
of	O
our	O
training	O
paradigm	O
.	O

The	O
goal	O
of	O
abstractive	O
summarization	B-TaskName
is	O
to	O
create	O
a	O
function	O
g	O
that	O
takes	O
a	O
source	O
document	O
D	O
and	O
generates	O
an	O
appropriate	O
summary	O
S	O
S	O
g	O
(	O
D	O
)	O
(	O
1	O
)	O
Training	O
Objective	O
Neural	O
abstractive	O
summarization	B-TaskName
models	O
aim	O
to	O
learn	O
a	O
neural	O
model	O
g	O
that	O
results	O
in	O
good	O
summaries	O
.	O
Maximum	O
likelihood	O
estimation	O
(	O
MLE	O
)	O
is	O
the	O
standard	O
training	O
algorithm	O
.	O
It	O
aims	O
to	O
maximize	O
the	O
likelihood	O
of	O
the	O
reference	O
summary	O
S	O
*	O
,	O
i.e.	O
,	O
θ	B-HyperparameterName
*	O
=	O
argmax	O
θ	B-HyperparameterName
i	O
log	O
p	O
g	O
θ	B-HyperparameterName
(	O
S	O
*	O
(	O
i	O
)	O
|	O
D	O
(	O
i	O
)	O
;	O
θ	B-HyperparameterName
)	O
(	O
2	O
)	O
where	O
θ	B-HyperparameterName
denotes	O
the	O
parameters	O
of	O
g	O
and	O
p	O
g	O
θ	B-HyperparameterName
denotes	O
the	O
probability	O
distribution	O
entailed	O
by	O
these	O
parameters	O
.	O
The	O
summation	O
is	O
over	O
the	O
training	O
set	O
and	O
{	O
D	O
(	O
i	O
)	O
,	O
S	O
*	O
(	O
i	O
)	O
}	O
is	O
the	O
i	O
-	O
th	O
training	O
sample	O
.	O
For	O
a	O
specific	O
sample	O
{	O
D	O
(	O
i	O
)	O
,	O
S	O
*	O
(	O
i	O
)	O
}	O
,	O
Eq	O
.	O
2	O
is	O
equivalent	O
to	O
minimizing	O
the	O
sum	O
of	O
negative	O
loglikelihoods	O
of	O
the	O
tokens	O
{	O
s	O
*	O
1	O
,	O
,	O
s	O
*	O
j	O
,	O
,	O
s	O
*	O
l	O
}	O
in	O
the	O
reference	O
summary	O
S	O
*	O
whose	O
length	O
is	O
l	O
,	O
which	O
is	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
:	O
Lxent	O
=	O
−	O
l	O
j=1	O
s	O
ptrue	O
(	O
s	O
|	O
D	O
,	O
S	O
*	O
<	O
j	O
)	O
log	O
pg	O
θ	B-HyperparameterName
(	O
s	O
|	O
D	O
,	O
S	O
*	O
<	O
j	O
;	O
θ	B-HyperparameterName
)	O
(	O
3	O
)	O
where	O
S	O
*	O
<	O
j	O
denotes	O
the	O
partial	O
reference	O
sequence	O
{	O
s	O
*	O
0	B-DatasetName
,	O
,	O
s	O
*	O
j−1	O
}	O
and	O
s	O
*	O
0	B-DatasetName
is	O
a	O
pre	O
-	O
defined	O
start	O
token	O
.	O
p	O
true	O
is	O
a	O
one	O
-	O
hot	O
distribution	O
under	O
the	O
standard	O
MLE	O
framework	O
:	O
ptrue	O
(	O
s	O
|	O
D	O
,	O
S	O
*	O
<	O
j	O
)	O
=	O
1	O
s	O
=	O
s	O
*	O
j	O
0	B-DatasetName
s	O
=	O
s	O
*	O
j	O
(	O
4	O
)	O
In	O
practice	O
,	O
label	B-MethodName
smoothing	I-MethodName
(	O
Szegedy	O
et	O
al	O
,	O
2016	O
)	O
is	O
a	O
widely	O
used	O
and	O
effective	O
technique	O
that	O
modifies	O
the	O
target	O
distribution	O
in	O
Eq	O
.	O
4	O
to	O
a	O
"	O
soft	O
"	O
label	O
by	O
assigning	O
probability	O
mass	O
β	B-HyperparameterName
to	O
other	O
tokens	O
:	O
ptrue	O
(	O
s	O
|	O
D	O
,	O
S	O
*	O
<	O
j	O
)	O
=	O
1	O
−	O
β	B-HyperparameterName
s	O
=	O
s	O
*	O
j	O
β	B-HyperparameterName
N	O
−1	O
s	O
=	O
s	O
*	O
j	O
(	O
5	O
)	O
where	O
N	O
is	O
the	O
size	O
of	O
the	O
dictionary	O
.	O
Inference	O
and	O
Exposure	O
Bias	O
During	O
inference	O
,	O
the	O
abstractive	O
model	O
g	O
is	O
used	O
to	O
generate	O
the	O
candidate	O
summary	O
in	O
an	O
autoregressive	O
manner	O
.	O
It	O
is	O
intractable	O
to	O
enumerate	O
all	O
the	O
possible	O
candidate	O
outputs	O
,	O
so	O
in	O
practice	O
methods	O
such	O
as	O
beam	O
search	O
are	O
used	O
to	O
reduce	O
the	O
search	O
space	O
.	O
One	O
important	O
step	O
in	O
search	O
is	O
estimating	O
the	O
probability	O
of	O
the	O
next	O
word	O
s	O
t	O
given	O
the	O
previous	O
predicted	O
sequence	O
S	O
<	O
t	O
:	O
p	O
g	O
θ	B-HyperparameterName
(	O
s	O
t	O
|	O
D	O
,	O
S	O
<	O
t	O
;	O
θ	B-HyperparameterName
)	O
(	O
6	O
)	O
Comparing	O
Eq	O
.	O
6	O
with	O
Eq	O
.	O
3	O
,	O
the	O
major	O
difference	O
is	O
that	O
during	O
inference	O
the	O
model	O
makes	O
new	O
predictions	O
based	O
on	O
its	O
own	O
previous	O
predictions	O
S	O
<	O
t	O
instead	O
of	O
the	O
reference	O
S	O
*	O
<	O
t	O
.	O
As	O
a	O
result	O
,	O
even	O
if	O
the	O
generation	O
model	O
g	O
achieves	O
very	O
high	O
accuracy	B-MetricName
w.r.t	O
.	O
Eq	O
.	O
3	O
,	O
once	O
S	O
<	O
t	O
starts	O
to	O
deviate	O
from	O
S	O
*	O
,	O
there	O
is	O
the	O
risk	O
that	O
the	O
performance	O
of	O
g	O
will	O
significantly	O
degrade	O
.	O
This	O
problem	O
has	O
been	O
identified	O
as	O
the	O
exposure	O
bias	O
(	O
Bengio	O
et	O
al	O
,	O
2015	O
)	O
.	O

Eq	O
.	O
6	O
implies	O
that	O
the	O
abstractive	O
model	O
g	O
should	O
be	O
able	O
to	O
assign	O
higher	O
estimated	O
probability	O
to	O
the	O
better	O
candidate	O
summary	O
during	O
inference	O
.	O
However	O
,	O
this	O
intuition	O
is	O
not	O
directly	O
captured	O
in	O
the	O
standard	O
MLE	O
objective	O
used	O
in	O
training	O
-	O
a	O
model	O
obtaining	O
zero	O
MLE	O
loss	B-MetricName
would	O
assign	O
zero	O
probability	O
to	O
any	O
candidate	O
summary	O
different	O
from	O
the	O
reference	O
.	O
This	O
is	O
obviously	O
improper	O
for	O
any	O
task	O
where	O
multiple	O
reasonable	O
generations	O
may	O
exist	O
(	O
Khayrallah	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
also	O
does	O
not	O
say	O
anything	O
about	O
the	O
ordering	O
of	O
two	O
imperfect	O
references	O
.	O
We	O
therefore	O
advocate	O
for	O
making	O
the	O
alternative	O
assumption	O
that	O
the	O
probability	O
of	O
one	O
candidate	O
should	O
be	O
well	O
-	O
correlated	O
with	O
its	O
quality	O
as	O
evaluated	O
by	O
an	O
automatic	O
metric	O
M	O
.	O
Since	O
it	O
is	O
intractable	O
to	O
enumerate	O
all	O
the	O
possible	O
candidate	O
outputs	O
,	O
we	O
only	O
require	O
our	O
model	O
to	O
be	O
able	O
to	O
accurately	O
predict	O
the	O
ranking	O
order	O
of	O
a	O
set	O
of	O
the	O
most	O
probable	O
candidate	O
summariesŜ	O
,	O
which	O
are	O
its	O
own	O
beam	O
search	O
results	O
.	O
In	O
order	O
to	O
achieve	O
this	O
objective	O
,	O
we	O
slightly	O
modify	O
the	O
conditions	O
of	O
Eq	O
.	O
5	O
,	O
maintaining	O
the	O
general	O
functional	O
form	O
,	O
but	O
instead	O
specifying	O
the	O
marginal	O
probability	O
of	O
the	O
non	O
-	O
reference	O
candidates	O
S	O
to	O
be	O
β	B-HyperparameterName
,	O
and	O
encouraging	O
coordination	O
of	O
probabilities	O
and	O
qualities	O
among	O
non	O
-	O
reference	O
candidates	O
as	O
follows	O
:	O
	O
	O
	O
	O
	O
	O
	O
	O
p	O
true	O
†	O
(	O
S	O
|	O
D	O
)	O
=	O
1	O
−	O
β	B-HyperparameterName
S	O
=	O
S	O
*	O
S	O
S	O
p	O
true	O
†	O
(	O
S	O
|	O
D	O
)	O
=	O
β	B-HyperparameterName
S	O
=	O
S	O
*	O
p	O
true	O
†	O
(	O
Si	O
|	O
D	O
)	O
>	O
p	O
true	O
†	O
(	O
Sj	O
|	O
D	O
)	O
∀Si	O
,	O
Sj	O
Ŝ	O
,	O
M	O
(	O
Si	O
)	O
>	O
M	O
(	O
Sj	O
)	O
(	O
7	O
)	O
We	O
next	O
describe	O
precisely	O
how	O
we	O
encourage	O
coordination	O
through	O
contrastive	B-MethodName
learning	I-MethodName
.	O

The	O
candidate	O
quality	O
measure	O
M	O
can	O
be	O
defined	O
in	O
many	O
ways	O
.	O
In	O
this	O
work	O
we	O
define	O
it	O
as	O
the	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
score	O
of	O
a	O
candidate	O
summary	O
S	O
i	O
given	O
the	O
reference	O
summary	O
S	O
*	O
.	O
To	O
coordinate	O
a	O
pre	O
-	O
trained	O
abstractive	O
model	O
,	O
we	O
1	O
)	O
use	O
it	O
to	O
generate	O
different	O
candidate	O
summaries	O
with	O
various	O
levels	O
of	O
quality	O
,	O
2	O
then	O
2	O
)	O
encourage	O
the	O
model	O
to	O
assign	O
higher	O
estimated	O
probabilities	O
to	O
better	O
candidates	O
by	O
fine	O
-	O
tuning	O
the	O
model	O
with	O
a	O
contrastive	O
loss	B-MetricName
,	O
following	O
the	O
previous	O
work	O
(	O
Hopkins	O
and	O
May	O
,	O
2011	O
;	O
Zhong	O
et	O
al	O
,	O
2020	O
)	O
:	O
Lctr	O
=	O
i	O
j	O
>	O
i	O
max	O
(	O
0	B-DatasetName
,	O
f	O
(	O
Sj	O
)	O
−	O
f	O
(	O
Si	O
)	O
+	O
λij	O
)	O
(	O
8	O
)	O
where	O
S	O
i	O
and	O
S	O
j	O
are	O
two	O
different	O
candidate	O
summaries	O
and	O
ROUGE	O
(	O
S	O
i	O
,	O
S	O
*	O
)	O
>	O
ROUGE	O
(	O
S	O
j	O
,	O
S	O
*	O
)	O
,	O
∀i	O
,	O
j	O
,	O
i	O
<	O
j.	O
λ	O
ij	O
is	O
the	O
margin	O
multiplied	O
by	O
the	O
difference	O
in	O
rank	O
between	O
the	O
candidates	O
,	O
i.e.	O
,	O
λ	O
ij	O
=	O
(	O
j	O
−	O
i	O
)	O
*	O
λ	O
.	O
f	O
(	O
S	O
i	O
)	O
is	O
the	O
length	O
-	O
normalized	O
estimated	O
log	O
-	O
probability	O
3	O
f	O
(	O
S	O
)	O
=	O
l	O
t=1	O
log	O
p	O
g	O
θ	B-HyperparameterName
(	O
s	O
t	O
|	O
D	O
,	O
S	O
<	O
t	O
;	O
θ	B-HyperparameterName
)	O
|	O
S	O
|	O
α	B-HyperparameterName
(	O
9	O
)	O
where	O
α	B-HyperparameterName
is	O
the	O
length	O
penalty	O
hyperparameter	O
.	O
This	O
loss	B-MetricName
gives	O
the	O
abstractive	O
model	O
a	O
dual	O
purpose	O
,	O
first	O
as	O
a	O
reference	O
-	O
free	O
evaluation	O
model	O
,	O
which	O
can	O
be	O
used	O
in	O
a	O
two	O
-	O
stage	O
summarization	B-TaskName
pipeline	O
,	O
where	O
it	O
is	O
used	O
to	O
score	O
the	O
candidates	O
generated	O
by	O
a	O
pre	O
-	O
trained	O
generation	O
model	O
and	O
select	O
the	O
final	O
output	O
from	O
them	O
.	O
However	O
,	O
since	O
the	O
autoregressive	O
generation	O
depends	O
on	O
both	O
the	O
token	O
-	O
level	O
prediction	O
accuracy	B-MetricName
and	O
sequencelevel	O
coordination	O
,	O
the	O
model	O
fine	O
-	O
tuned	O
with	O
the	O
contrastive	O
loss	B-MetricName
alone	O
can	O
no	O
longer	O
be	O
used	O
as	O
a	O
generation	O
model	O
.	O
Multi	O
-	O
task	O
Fine	O
-	O
tuning	O
Following	O
Edunov	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
combine	O
the	O
contrastive	O
(	O
Eq	O
.	O
8	O
)	O
and	O
cross	O
-	O
entropy	O
(	O
Eq	O
.	O
3	O
)	O
losses	O
to	O
preserve	O
the	O
generation	O
ability	O
of	O
the	O
pre	O
-	O
trained	O
abstractive	O
model	O
:	O
L	O
mul	O
=	O
L	O
xent	O
+	O
γL	O
ctr	O
(	O
10	O
)	O
where	O
γ	B-HyperparameterName
is	O
the	O
weight	O
of	O
the	O
contrastive	O
loss	B-MetricName
.	O
We	O
note	O
that	O
the	O
contrastive	O
and	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
can	O
effectively	O
complement	O
each	O
other	O
-	O
since	O
the	O
contrastive	O
loss	B-MetricName
is	O
defined	O
on	O
the	O
sequence	O
level	O
,	O
the	O
token	O
-	O
level	O
cross	O
-	O
entropy	O
loss	B-MetricName
serves	O
as	O
a	O
normalization	O
to	O
ensure	O
that	O
the	O
model	O
could	O
assign	O
balanced	O
probability	O
mass	O
across	O
the	O
whole	O
sequence	O
.	O

Training	O
Methods	O
of	O
Seq2Seq	B-MethodName
Models	O
In	O
order	O
to	O
align	O
the	O
training	O
objective	O
and	O
evaluation	O
metric	O
,	O
structured	O
losses	O
have	O
been	O
used	O
for	O
the	O
Seq2Seq	B-MethodName
model	O
training	O
.	O
Among	O
them	O
,	O
marginbased	O
losses	O
(	O
Herbrich	O
et	O
al	O
,	O
1999	O
;	O
Taskar	O
et	O
al	O
,	O
2004	O
;	O
Gimpel	O
and	O
Smith	O
,	O
2010	O
)	O
,	O
which	O
require	O
the	O
model	O
to	O
assign	O
higher	O
probability	O
to	O
the	O
better	O
output	O
,	O
are	O
a	O
major	O
category	O
.	O
Many	O
margin	O
-	O
based	O
losses	O
used	O
in	O
modern	O
seq2seq	B-MethodName
models	O
(	O
Wiseman	O
and	O
Rush	O
,	O
2016	O
;	O
Edunov	O
et	O
al	O
,	O
2018	O
)	O
assume	O
a	O
deterministic	O
(	O
one	O
-	O
point	O
)	O
distribution	O
:	O
a	O
model	O
can	O
achieve	O
zero	O
loss	B-MetricName
if	O
it	O
can	O
assign	O
a	O
much	O
higher	O
probability	O
to	O
the	O
(	O
pseudo	O
)	O
-	O
reference	O
,	O
regardless	O
of	O
relative	O
comparisons	O
of	O
other	O
candidate	O
summaries	O
.	O
By	O
contrast	O
,	O
our	O
method	O
has	O
a	O
non	O
-	O
deterministic	O
assumption	O
(	O
Eq	O
.	O
7	O
)	O
,	O
which	O
focuses	O
on	O
the	O
pair	O
-	O
wise	O
ranking	O
of	O
a	O
set	O
of	O
candidate	O
summaries	O
.	O
One	O
main	O
challenge	O
of	O
directly	O
optimizing	O
a	O
Seq2Seq	B-MethodName
model	O
with	O
quality	O
scores	O
of	O
the	O
output	O
is	O
that	O
the	O
discrete	O
sampling	O
process	O
makes	O
the	O
loss	B-MetricName
non	O
-	O
differentiable	O
.	O
To	O
circumvent	O
this	O
problem	O
,	O
reinforcement	O
learning	O
has	O
been	O
used	O
to	O
reformulate	O
the	O
conditional	B-TaskName
text	I-TaskName
generation	I-TaskName
tasks	O
(	O
Ranzato	O
et	O
al	O
,	O
2016	O
;	O
Bahdanau	O
et	O
al	O
,	O
2016	O
;	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Paulus	O
et	O
al	O
,	O
2018	O
;	O
Li	O
et	O
al	O
,	O
2019	O
)	O
.	O
Compared	O
to	O
this	O
school	O
of	O
methods	O
,	O
our	O
method	O
is	O
based	O
on	O
supervised	O
learning	O
,	O
and	O
it	O
is	O
more	O
stable	O
and	O
less	O
sensitive	O
to	O
the	O
design	O
choices	O
(	O
e.g.	O
reward	O
shaping	O
)	O
,	O
which	O
are	O
well	O
-	O
known	O
challenges	O
of	O
reinforcement	O
learning	O
methods	O
.	O
Minimum	O
risk	O
training	O
(	O
Shen	O
et	O
al	O
,	O
2016	O
;	O
Wieting	O
et	O
al	O
,	O
2019	O
)	O
and	O
other	O
online	O
sampling	O
based	O
methods	O
(	O
Bengio	O
et	O
al	O
,	O
2015	O
;	O
Norouzi	O
et	O
al	O
,	O
2016	O
;	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
belong	O
to	O
another	O
school	O
of	O
methods	O
used	O
to	O
circumvent	O
the	O
problem	O
of	O
non	O
-	O
differentiability	O
.	O
However	O
,	O
they	O
also	O
exhibit	O
similar	O
problems	O
of	O
stability	O
as	O
reinforcement	O
learning	O
.	O
Contrastive	B-MethodName
Learning	I-MethodName
Recently	O
,	O
contrastive	B-MethodName
learning	I-MethodName
(	O
Hadsell	O
et	O
al	O
,	O
2006	O
)	O
has	O
been	O
introduced	O
into	O
several	O
conditional	B-TaskName
text	I-TaskName
generation	I-TaskName
tasks	O
,	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
Pan	O
et	O
al	O
,	O
2021	O
)	O
,	O
text	B-TaskName
summarization	I-TaskName
(	O
Cao	O
and	O
Wang	O
,	O
2021	O
;	O
Xu	O
et	O
al	O
,	O
2021	O
;	O
Sun	O
and	O
Li	O
,	O
2021	O
)	O
,	O
and	O
other	O
tasks	O
(	O
Uehara	O
et	O
al	O
,	O
2020	O
;	O
Cho	O
et	O
al	O
,	O
2021	O
;	O
Lee	O
et	O
al	O
,	O
2021b	O
)	O
.	O
Among	O
these	O
application	O
scenarios	O
,	O
most	O
work	O
deployed	O
contrastive	B-MethodName
learning	I-MethodName
in	O
the	O
latent	O
representation	O
space	O
,	O
following	O
the	O
framework	O
proposed	O
in	O
.	O
However	O
,	O
in	O
this	O
work	O
we	O
adopt	O
contrastive	B-MethodName
learning	I-MethodName
over	O
the	O
discrete	O
space	O
of	O
the	O
generated	O
texts	O
.	O
Besides	O
,	O
instead	O
of	O
constructing	O
the	O
contrastive	B-MethodName
learning	I-MethodName
examples	O
by	O
rule	O
-	O
based	O
methods	O
(	O
e.g.	O
perturbing	O
the	O
reference	O
output	O
)	O
,	O
we	O
use	O
the	O
generation	O
models	O
to	O
construct	O
the	O
examples	O
,	O
which	O
makes	O
the	O
contrastive	B-MethodName
learning	I-MethodName
task	O
closer	O
to	O
the	O
generation	O
task	O
.	O
Sun	O
and	O
Li	O
(	O
2021	O
)	O
also	O
adopted	O
contrastive	B-MethodName
learning	I-MethodName
on	O
the	O
generated	O
texts	O
.	O
However	O
,	O
their	O
formulation	O
belongs	O
to	O
the	O
margin	O
-	O
based	O
losses	O
.	O
We	O
have	O
discussed	O
the	O
difference	O
between	O
our	O
method	O
and	O
the	O
margin	O
-	O
based	O
losses	O
in	O
the	O
previous	O
paragraphs	O
.	O
Discriminative	O
Reranking	O
Discriminative	O
reranking	O
has	O
been	O
widely	O
studied	O
for	O
conditional	O
generation	O
tasks	O
Wan	O
et	O
al	O
,	O
2015	O
;	O
Mizumoto	O
and	O
Matsumoto	O
,	O
2016	O
)	O
.	O
Some	O
recent	O
works	O
Lee	O
et	O
al	O
,	O
2021a	O
)	O
have	O
also	O
explored	O
discriminative	O
reranking	O
of	O
candidates	O
from	O
neural	O
natural	O
language	O
generation	O
models	O
,	O
which	O
adopt	O
large	O
pre	O
-	O
trained	O
language	O
models	O
(	O
e.g.	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
)	O
as	O
the	O
reranker	O
.	O
In	O
this	O
work	O
,	O
we	O
factorize	O
the	O
Seq2Seq	B-MethodName
model	O
(	O
e.g.	O
,	O
BART	B-MethodName
)	O
trained	O
on	O
the	O
same	O
dataset	O
as	O
the	O
reranking	O
model	O
,	O
which	O
maximizes	O
the	O
parameter	O
sharing	O
across	O
two	O
stages	O
.	O
Besides	O
,	O
our	O
approach	O
contributes	O
an	O
instance	O
of	O
leveraging	O
large	O
pre	O
-	O
trained	O
Seq2Seq	B-MethodName
models	O
as	O
a	O
quality	O
estimation	O
model	O
(	O
Yuan	O
et	O
al	O
,	O
2021	O
)	O
.	O

Datasets	O
We	O
mainly	O
use	O
three	O
datasets	O
in	O
our	O
experiments	O
(	O
statistics	O
in	O
Appendix	O
A	O
)	O
.	O
CNNDM	O
4	O
(	O
Hermann	O
et	O
al	O
,	O
2015	O
)	O
is	O
a	O
large	O
scale	O
news	O
dataset	O
.	O
Following	O
Nallapati	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
treat	O
the	O
news	O
articles	O
as	O
the	O
source	O
documents	O
and	O
the	O
associated	O
highlights	O
as	O
the	O
summaries	O
.	O
XSum	B-DatasetName
5	O
(	O
Narayan	O
et	O
al	O
,	O
2018	O
)	O
is	O
a	O
highly	O
abstractive	O
dataset	O
of	O
articles	O
from	O
the	O
British	O
Broadcasting	O
Corporation	O
(	O
BBC	O
)	O
.	O
NYT	O
6	O
(	O
Sandhaus	O
,	O
2008	O
)	O
contains	O
articles	O
from	O
the	O
New	O
York	O
Times	O
and	O
the	O
associated	O
summaries	O
.	O
We	O
follow	O
Kedzie	O
et	O
al	O
(	O
2018	O
)	O
for	O
data	O
preprocessing	O
and	O
splitting	O
,	O
and	O
use	O
the	O
associated	O
archival	O
abstracts	O
as	O
the	O
summaries	O
.	O
Baselines	O
We	O
choose	O
a	O
variety	O
of	O
related	O
models	O
with	O
strong	O
performance	O
as	O
baselines	O
.	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
and	O
PEGASUS	O
are	O
both	O
large	O
pre	O
-	O
trained	O
Seq2Seq	B-MethodName
LMs	O
standard	O
in	O
the	O
literature	O
.	O
GSum	O
(	O
Dou	O
et	O
al	O
,	O
2021	O
)	O
is	O
built	O
on	O
BART	B-MethodName
,	O
and	O
improves	O
performance	O
by	O
using	O
additional	O
guidance	O
from	O
an	O
extractive	O
summarizer	O
.	O
SimCLS	O
introduces	O
a	O
two	O
-	O
stage	O
framework	O
where	O
the	O
pre	O
-	O
trained	O
BART	B-MethodName
model	O
is	O
used	O
to	O
generate	O
candidates	O
and	O
a	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
model	O
is	O
fine	O
-	O
tuned	O
as	O
an	O
evaluation	O
model	O
to	O
score	O
the	O
candidate	O
summaries	O
and	O
select	O
from	O
them	O
.	O
It	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
both	O
CNNDM	O
and	O
XSum	B-DatasetName
.	O
GOLD	O
(	O
Pang	O
and	O
He	O
,	O
2021	O
)	O
uses	O
offline	O
reinforcement	O
learning	O
to	O
train	O
the	O
BART	B-MethodName
model	O
by	O
treating	O
the	O
reference	O
summaries	O
as	O
the	O
demonstrations	O
,	O
a	O
different	O
formulation	O
that	O
can	O
also	O
improve	O
the	O
performance	O
of	O
the	O
original	O
BART	B-MethodName
.	O
SeqCo	O
(	O
Xu	O
et	O
al	O
,	O
2021	O
)	O
and	O
ConSum	O
(	O
Sun	O
and	O
Li	O
,	O
2021	O
)	O
are	O
two	O
recent	O
methods	O
that	O
aim	O
to	O
leverage	O
contrastive	B-MethodName
learning	I-MethodName
to	O
improve	O
the	O
performance	O
of	O
the	O
abstractive	O
summarization	B-TaskName
model	O
(	O
BART	B-MethodName
)	O
.	O
Implementation	O
Details	O
In	O
the	O
following	O
experiments	O
,	O
we	O
use	O
either	O
BART	B-MethodName
or	O
PEGASUS	O
as	O
a	O
backbone	O
.	O
We	O
label	O
our	O
proposed	O
methods	O
BRIO	O
,	O
with	O
two	O
variants	O
:	O
(	O
1	O
)	O
BRIO	O
-	O
Ctr	O
is	O
fine	O
-	O
tuned	O
with	O
the	O
contrastive	O
loss	B-MetricName
(	O
Eq	O
.	O
8	O
)	O
only	O
;	O
(	O
2	O
)	O
BRIO	O
-	O
Mul	O
is	O
fine	O
-	O
tuned	O
with	O
the	O
multi	O
-	O
task	O
loss	B-MetricName
(	O
Eq	O
.	O
10	O
)	O
.	O
We	O
use	O
BRIO	O
-	O
Ctr	O
as	O
an	O
evaluation	O
model	O
that	O
scores	O
different	O
candidate	O
summaries	O
generated	O
by	O
a	O
Seq2Seq	B-MethodName
abstractive	O
model	O
and	O
selects	O
the	O
final	O
output	O
from	O
them	O
,	O
and	O
BRIO	O
-	O
Mul	O
as	O
a	O
standard	O
Seq2Seq	B-MethodName
model	O
that	O
takes	O
the	O
source	O
documents	O
as	O
input	O
and	O
generates	O
the	O
output	O
in	O
an	O
autoregressive	O
manner	O
.	O
Further	O
details	O
are	O
in	O
Appendix	O
B.	O

We	O
further	O
perform	O
some	O
in	O
-	O
depth	O
analyses	O
from	O
diverse	O
perspectives	O
on	O
the	O
CNNDM	O
dataset	O
to	O
gain	O
more	O
insights	O
into	O
our	O
proposed	O
method	O
.	O
Table	O
3	O
:	O
Model	O
performance	O
with	O
different	O
γ	B-HyperparameterName
coefficients	O
weighting	O
the	O
contrastive	O
loss	B-MetricName
(	O
Eq	O
.	O
10	O
)	O
on	O
CNNDM	O
.	O
BRIO	O
-	O
Ctr	O
is	O
trained	O
with	O
the	O
contrastive	O
loss	B-MetricName
only	O
,	O
which	O
no	O
longer	O
preserves	O
its	O
generation	O
ability	O
.	O
We	O
report	O
its	O
performance	O
when	O
it	O
is	O
used	O
as	O
an	O
evaluation	O
model	O
to	O
select	O
from	O
candidate	O
summaries	O
.	O
R	O
-	O
1/2	O
/	O
L	O
are	O
the	O
ROUGE	O
-	O
1/2	O
/	O
L	O
F1	B-MetricName
scores	O
.	O
Coefficients	O
of	O
the	O
Multi	O
-	O
Task	O
Loss	O
The	O
multitask	O
loss	B-MetricName
(	O
Eq	O
.	O
10	O
)	O
used	O
to	O
train	O
our	O
model	O
contains	O
two	O
parts	O
:	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
and	O
the	O
contastive	O
loss	B-MetricName
.	O
As	O
shown	O
in	O
Tab	O
.	O
3	O
,	O
as	O
the	O
weight	O
of	O
the	O
contrastive	O
loss	B-MetricName
(	O
γ	B-HyperparameterName
)	O
increases	O
,	O
the	O
model	O
's	O
performance	O
improves	O
.	O
However	O
,	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
is	O
still	O
necessary	O
to	O
preserve	O
the	O
model	O
's	O
ability	O
as	O
a	O
generation	O
model	O
.	O
We	O
argue	O
that	O
this	O
is	O
because	O
the	O
token	O
level	O
accuracy	B-MetricName
is	O
still	O
important	O
during	O
the	O
autoregressive	O
generation	O
process	O
,	O
where	O
the	O
individual	O
tokens	O
are	O
predicted	O
sequentially	O
.	O
In	O
addition	O
,	O
we	O
also	O
found	O
that	O
the	O
model	O
tends	O
to	O
achieve	O
the	O
best	O
performance	O
(	O
w.r.t	O
the	O
ROUGE	O
scores	O
on	O
the	O
development	O
set	O
)	O
faster	O
with	O
a	O
higher	O
γ	B-HyperparameterName
.	O
Specifically	O
,	O
it	O
requires	O
less	O
than	O
one	O
entire	O
epoch	O
to	O
achieve	O
the	O
best	O
performance	O
on	O
CNNDM	O
,	O
making	O
our	O
approach	O
an	O
efficient	O
fine	O
-	O
tuning	O
method	O
.	O
Coefficient	O
(	O
γ	B-HyperparameterName
)	O
R	O
-	O
1	O
R	O
-	O
2	O
R	O
-	O
L	O
0	B-DatasetName
(	O
Generation	O
-	O
Finetuning	O
as	O
a	O
Loop	O
Since	O
the	O
fine	O
-	O
tuned	O
model	O
(	O
BRIO	O
-	O
Mul	O
)	O
is	O
still	O
able	O
to	O
gen	O
-	O
(	O
Stahlberg	O
and	O
Byrne	O
,	O
2019	O
)	O
,	O
and	O
the	O
generator	O
may	O
not	O
be	O
able	O
to	O
differentiate	O
them	O
from	O
high	O
-	O
quality	O
candidates	O
.	O
In	O
Tab	O
.	O
5	O
,	O
we	O
compare	O
the	O
performance	O
of	O
the	O
pre	O
-	O
trained	O
BART	B-MethodName
and	O
our	O
model	O
(	O
BRIO	O
-	O
Mul	O
)	O
with	O
different	O
beam	O
widths	O
used	O
during	O
inference	O
.	O
We	O
observe	O
that	O
the	O
performance	O
of	O
BART	B-MethodName
goes	O
down	O
as	O
the	O
beam	O
width	O
increases	O
.	O
On	O
the	O
other	O
hand	O
,	O
our	O
model	O
is	O
able	O
to	O
achieve	O
better	O
performance	O
with	O
a	O
larger	O
number	O
of	O
beams	O
,	O
demonstrating	O
that	O
our	O
training	O
method	O
can	O
improve	O
the	O
coordination	O
of	O
the	O
model	O
by	O
encouraging	O
the	O
model	O
to	O
assign	O
estimated	O
probabilities	O
to	O
candidate	O
summaries	O
wellcorrelated	O
with	O
their	O
quality	O
.	O
Training	O
with	O
Different	O
Evaluation	O
Metrics	O
In	O
the	O
previous	O
experiments	O
,	O
we	O
used	O
ROUGE	O
as	O
the	O
evaluation	O
metric	O
to	O
define	O
the	O
target	O
ordering	O
of	O
the	O
candidate	O
summaries	O
(	O
Eq.7	O
)	O
.	O
To	O
evaluate	O
our	O
method	O
's	O
performance	O
beyond	O
ROUGE	O
,	O
we	O
use	O
a	O
model	O
-	O
based	O
semantic	B-TaskName
similarity	I-TaskName
metric	O
,	O
BERTScore	O
(	O
Zhang	O
*	O
et	O
al	O
,	O
2020	O
)	O
,	O
7	O
as	O
the	O
evaluation	O
metric	O
M	O
in	O
Eq.7	O
to	O
compare	O
the	O
performance	O
of	O
different	O
candidate	O
summaries	O
.	O
Then	O
,	O
we	O
trained	O
another	O
version	O
of	O
BRIO	O
-	O
Mul	O
based	O
on	O
the	O
order	O
of	O
candidate	O
summaries	O
calculated	O
by	O
BERTScore	O
.	O
Beams	O
BART	B-MethodName
BRIO	O
-	O
Mul	O
R	O
-	O
1	O
R	O
-	O
2	O
R	O
-	O
1	O
R	O
-	O
The	O
results	O
in	O
Tab	O
.	O
6	O
show	O
that	O
(	O
1	O
)	O
Our	O
model	O
can	O
significantly	O
improve	O
the	O
model	O
performance	O
when	O
either	O
ROUGE	O
or	O
BERTScore	O
is	O
used	O
as	O
the	O
target	O
evaluation	O
metric	O
for	O
ordering	O
candidate	O
summaries	O
.	O
This	O
suggests	O
that	O
it	O
is	O
possible	O
to	O
use	O
our	O
method	O
to	O
optimize	O
any	O
specific	O
target	O
metric	O
,	O
making	O
our	O
method	O
an	O
alternative	O
to	O
reinforcement	O
learning	O
or	O
minimum	O
risk	O
training	O
.	O
(	O
2	O
)	O
Our	O
model	O
that	O
is	O
trained	O
on	O
one	O
evaluation	O
metric	O
(	O
e.g.	O
BERTScore	O
)	O
also	O
achieves	O
improvement	O
on	O
another	O
metric	O
(	O
e.g.	O
ROUGE	O
)	O
compared	O
with	O
the	O
baseline	O
model	O
,	O
which	O
indicates	O
that	O
the	O
improvement	O
made	O
by	O
our	O
model	O
is	O
not	O
from	O
exploiting	O
the	O
potential	O
weaknesses	O
of	O
individual	O
metrics	O
.	O
Besides	O
,	O
this	O
result	O
also	O
demonstrates	O
a	O
non	O
-	O
trivial	O
degree	O
of	O
agreement	O
between	O
ROUGE	O
and	O
BERTScore	O
.	O
Novel	O
n	O
-	O
grams	O
We	O
compare	O
the	O
ratio	O
of	O
novel	O
n	O
-	O
grams	O
in	O
reference	O
,	O
BRIO	O
-	O
Mul	O
's	O
,	O
and	O
BART	B-MethodName
's	O
summaries	O
.	O
As	O
Tab	O
.	O
7	O
shows	O
,	O
our	O
model	O
is	O
more	O
"	O
abstractive	O
"	O
compared	O
to	O
BART	B-MethodName
,	O
although	O
reference	O
summaries	O
still	O
contain	O
more	O
novel	O
n	O
-	O
grams	O
.	O
This	O
is	O
likely	O
due	O
to	O
the	O
fact	O
that	O
our	O
model	O
is	O
optimized	O
at	O
the	O
sequence	O
-	O
level	O
,	O
allowing	O
more	O
freedom	O
for	O
paraphrasing	O
and	O
compression	O
.	O
We	O
further	O
investigate	O
the	O
relation	O
of	O
the	O
"	O
abstractiveness	O
"	O
and	O
model	O
performance	O
by	O
com	O
-	O
7	O
https://github.com/Tiiiger/bert_score	O
.	O
We	O
use	O
its	O
default	O
version	O
for	O
English	O
texts	O
.	O
paring	O
our	O
model	O
(	O
BRIO	O
-	O
Mul	O
)	O
with	O
the	O
baseline	O
model	O
(	O
BART	B-MethodName
)	O
on	O
different	O
buckets	O
of	O
test	O
examples	O
grouped	O
by	O
the	O
"	O
novelty	O
"	O
of	O
the	O
reference	O
summaries	O
,	O
8	O
i.e.	O
,	O
Novelty	O
(	O
D	O
,	O
S	O
*	O
)	O
=	O
g	O
G	O
S	O
*	O
1	O
(	O
g	O
/	O
GD	B-DatasetName
)	O
|	O
GS	O
*	O
|	O
(	O
11	O
)	O
where	O
D	O
and	O
S	O
*	O
are	O
the	O
source	O
document	O
and	O
reference	O
summary	O
respectively	O
,	O
G	O
D	O
and	O
G	O
S	O
*	O
are	O
the	O
sets	O
of	O
bigrams	O
in	O
D	O
and	O
S	O
*	O
,	O
1	O
is	O
the	O
indicator	O
function	O
.	O
The	O
results	O
in	O
Fig	O
.	O
3	O
show	O
that	O
when	O
novelty	O
is	O
higher	O
,	O
(	O
1	O
)	O
all	O
models	O
'	O
performance	O
decreases	O
;	O
(	O
2	O
)	O
our	O
model	O
achieves	O
larger	O
improvement	O
over	O
the	O
baseline	O
model	O
.	O
Rank	O
Correlation	O
We	O
computed	O
the	O
rank	O
correlation	O
between	O
the	O
estimated	O
probabilities	O
of	O
the	O
candidate	O
summaries	O
calculated	O
by	O
the	O
generators	O
and	O
the	O
quality	O
scores	O
of	O
the	O
candidate	O
summaries	O
.	O
We	O
use	O
Eq	O
.	O
9	O
to	O
calculate	O
the	O
estimated	O
probabilities	O
9	O
and	O
we	O
use	O
ROUGE	O
-	O
1	O
as	O
the	O
quality	O
score	O
metric	O
of	O
the	O
candidate	O
summaries	O
.	O
We	O
calculate	O
Spearman	O
's	O
rank	O
correlation	O
for	O
each	O
sample	O
,	O
and	O
use	O
the	O
average	O
score	O
as	O
the	O
overall	O
correlation	O
,	O
We	O
investigated	O
two	O
specific	O
settings	O
:	O
1	O
)	O
ranking	O
candidate	O
summaries	O
generated	O
by	O
a	O
different	O
model	O
(	O
PEGASUS	O
)	O
;	O
2	O
)	O
ranking	O
candidate	O
summaries	O
generated	O
by	O
themselves	O
(	O
BART	B-MethodName
&	O
BRIO	O
-	O
Mul	O
)	O
.	O
We	O
use	O
16	O
candidates	O
in	O
total	O
for	O
calculation	O
.	O
As	O
Tab	O
.	O
8	O
shows	O
,	O
our	O
model	O
achieves	O
better	O
rank	O
correlation	O
on	O
the	O
candidate	O
summaries	O
generated	O
by	O
both	O
itself	O
and	O
the	O
independent	O
model	O
.	O
This	O
suggests	O
that	O
our	O
model	O
can	O
better	O
estimate	O
the	O
quality	O
of	O
candidate	O
summaries	O
.	O

Calibration	O
requires	O
that	O
a	O
model	O
's	O
confidence	O
on	O
its	O
predictions	O
is	O
equal	O
to	O
the	O
accuracy	B-MetricName
of	O
these	O
predictions	O
(	O
Guo	O
et	O
al	O
,	O
2017	O
)	O
.	O
Previous	O
work	O
(	O
Müller	O
et	O
al	O
,	O
2019	O
;	O
Kumar	B-DatasetName
and	O
Sarawagi	O
,	O
2019	O
;	O
has	O
found	O
that	O
a	O
more	O
calibrated	O
text	B-TaskName
generation	I-TaskName
model	O
tends	O
to	O
have	O
better	O
performance	O
,	O
and	O
techniques	O
like	O
label	B-MethodName
smoothing	I-MethodName
can	O
improve	O
both	O
the	O
token	O
-	O
level	O
calibration	O
and	O
sequence	O
-	O
level	O
accuracy	B-MetricName
(	O
i.e.	O
the	O
ability	O
of	O
generating	O
better	O
results	O
)	O
.	O
One	O
intuitive	O
explanation	O
of	O
this	O
phenomenon	O
is	O
to	O
interpret	O
the	O
model	O
's	O
estimated	O
probability	O
of	O
a	O
generated	O
summary	O
as	O
the	O
product	O
of	O
the	O
model	O
's	O
confidences	O
on	O
a	O
series	O
of	O
tokenlevel	O
predictions	O
.	O
Then	O
,	O
since	O
a	O
more	O
calibrated	O
model	O
's	O
confidence	O
estimates	O
better	O
the	O
accuracy	B-MetricName
of	O
its	O
predictions	O
,	O
the	O
model	O
's	O
estimated	O
probability	O
of	O
one	O
sequence	O
should	O
be	O
more	O
indicative	O
of	O
the	O
quality	O
of	O
this	O
sequence	O
,	O
which	O
is	O
essential	O
for	O
the	O
beam	O
search	O
during	O
inference	O
.	O
However	O
,	O
the	O
relation	O
of	O
token	O
-	O
level	O
calibration	O
and	O
sequencelevel	O
performance	O
remains	O
inconclusive	O
(	O
Müller	O
et	O
al	O
,	O
2019	O
)	O
.	O
10	O
For	O
example	O
,	O
a	O
generator	O
that	O
always	O
predicts	O
a	O
uniform	O
distribution	O
over	O
all	O
tokens	O
would	O
be	O
perfectly	O
calibrated	O
,	O
however	O
,	O
such	O
a	O
model	O
would	O
not	O
generate	O
high	O
-	O
quality	O
outputs	O
.	O
We	O
investigate	O
this	O
relation	O
from	O
the	O
opposite	O
direction	O
by	O
evaluating	O
whether	O
our	O
model	O
(	O
BRIO	O
-	O
Mul	O
)	O
,	O
which	O
is	O
trained	O
to	O
have	O
better	O
sequencelevel	O
performance	O
,	O
would	O
also	O
be	O
more	O
calibrated	O
at	O
the	O
token	O
-	O
level	O
compared	O
with	O
the	O
baseline	O
models	O
that	O
are	O
trained	O
using	O
MLE	O
and	O
label	B-MethodName
smoothing	I-MethodName
.	O
We	O
follow	O
previous	O
work	O
by	O
using	O
the	O
Expected	O
Calibration	O
Error	B-MetricName
(	O
Naeini	O
et	O
al	O
,	O
2015	O
)	O
(	O
ECE	O
)	O
as	O
the	O
evaluation	O
metric	O
of	O
calibration	O
:	O
ECE	O
=	O
M	O
m=1	O
|	O
B	O
m	O
|	O
n	O
|	O
acc	B-MetricName
(	O
B	O
m	O
)	O
−	O
conf	O
(	O
B	O
m	O
)	O
|	O
(	O
12	O
)	O
where	O
the	O
samples	O
are	O
grouped	O
into	O
M	O
equal	O
-	O
width	O
buckets	O
by	O
confidence	O
(	O
conf	O
)	O
,	O
B	O
m	O
denotes	O
the	O
m	O
-	O
th	O
bucket	O
,	O
and	O
n	O
is	O
the	O
total	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
.	O
Following	O
,	O
we	O
evaluate	O
model	O
calibration	O
on	O
the	O
system	O
-	O
generated	O
summaries	O
during	O
inference	O
and	O
use	O
the	O
tercom	O
toolkit	O
11	O
to	O
assign	O
labels	O
(	O
correct	O
/	O
incorrect	O
)	O
to	O
the	O
system	O
-	O
generated	O
summaries	O
based	O
on	O
the	O
reference	O
summaries	O
.	O
The	O
results	O
in	O
Tab	O
.	O
9	O
show	O
that	O
BRIO	O
-	O
Mul	O
is	O
better	O
calibrated	O
compared	O
to	O
BART	B-MethodName
,	O
suggesting	O
that	O
our	O
method	O
helps	O
to	O
improve	O
the	O
token	O
-	O
level	O
calibration	O
by	O
explicitly	O
encouraging	O
the	O
model	O
to	O
have	O
more	O
accurate	O
sequence	O
-	O
level	O
probability	O
estimations	O
.	O
The	O
reliability	O
graph	O
is	O
shown	O
in	O
Fig	O
.	O
4	O
.	O
We	O
found	O
that	O
(	O
1	O
)	O
abstractive	O
models	O
are	O
generally	O
over	O
-	O
confident	O
on	O
their	O
own	O
predictions	O
,	O
(	O
2	O
)	O
models	O
are	O
generally	O
more	O
calibrated	O
on	O
XSum	B-DatasetName
than	O
CNNDM	O
.	O
This	O
is	O
likely	O
due	O
to	O
the	O
fact	O
that	O
XSum	B-DatasetName
has	O
shorter	O
summaries	O
therefore	O
it	O
is	O
less	O
likely	O
to	O
be	O
affected	O
by	O
the	O
exposure	O
bias	O
.	O

We	O
use	O
diverse	O
beam	O
search	O
(	O
Vijayakumar	O
et	O
al	O
,	O
2018	O
)	O
where	O
warmup	O
denotes	O
the	O
warmup	O
steps	O
,	O
which	O
is	O
set	O
to	O
10000	O
,	O
step	O
is	O
the	O
number	O
of	O
updating	O
steps	O
,	O
lr	O
is	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
.	O
We	O
set	O
the	O
length	O
penalty	O
factor	O
α	B-HyperparameterName
in	O
the	O
scoring	O
function	O
(	O
Eq	O
.	O
9	O
)	O
to	O
the	O
same	O
value	O
as	O
used	O
in	O
the	O
original	O
beam	O
search	O
.	O
We	O
search	O
the	O
value	O
of	O
the	O
margin	O
λ	O
in	O
the	O
contrastive	O
loss	B-MetricName
(	O
Eq	O
.	O
8	O
)	O
within	O
the	O
range	O
[	O
1	O
×	O
10	O
−5	O
,	O
1	O
]	O
,	O
and	O
decide	O
the	O
value	O
based	O
on	O
the	O
model	O
performance	O
on	O
the	O
validation	O
set	O
.	O
We	O
also	O
performed	O
extensive	O
search	O
for	O
the	O
coefficient	O
γ	B-HyperparameterName
in	O
Eq	O
.	O
10	O
.	O
The	O
specific	O
hyper	O
-	O
parameter	O
setting	O
is	O
reported	O
in	O
Tab	O
.	O
13	O
.	O
We	O
use	O
the	O
standard	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
Perl	O
package	O
15	O
for	O
evaluation	O
.	O
The	O
command	O
line	O
parameters	O
are	O
'	O
-	O
c	O
95	O
-	O
r	O
1000	O
-	O
n	O
2	O
-	O
m	O
'	O
.	O
Before	O
the	O
12	O
The	O
checkpoint	O
is	O
"	O
facebook	O
/	O
bart	O
-	O
large	O
-	O
cnn	O
"	O
,	O
containing	O
around	O
400	O
M	O
parameters	O
.	O
13	O
The	O
checkpoint	O
is	O
"	O
google	O
/	O
pegasus	O
-	O
xsum	B-DatasetName
"	O
"	O
containing	O
around	O
568	O
M	O
parameters	O
.	O
14	O
The	O
checkpoint	O
is	O
"	O
facebook	O
/	O
bart	O
-	O
large	O
"	O
.	O
15	O
https://github.com/summanlp/evaluation/tree/master/	O
ROUGE	O
-	O
RELEASE	O
-	O
1.5.5	O
Datasets	O
λ	O
(	O
Eq	O
.	O
8	O
)	O
α	B-HyperparameterName
(	O
Eq	O
.	O
9	O
)	O
γ	B-HyperparameterName
(	O
Eq	O
.	O
10	O
)	O
CNNDM	O
0.001	O
2.0	O
100	O
XSum	B-DatasetName
0.1	O
0.6	O
100	O
NYT	O
0.001	O
2.0	O
100	O

We	O
first	O
train	O
the	O
student	O
embeddings	O
with	O
the	O
teacher	O
model	O
initialized	O
from	O
BERT	B-MethodName
LARGE	O
.	O
For	O
a	O
given	O
input	O
sequence	O
,	O
we	O
mix	O
the	O
vocabularies	O
by	O
randomly	O
selecting	O
(	O
with	O
probability	O
p	O
SV	O
,	O
a	O
hyperparameter	O
)	O
words	O
from	O
the	O
sequence	O
to	O
segment	O
using	O
the	O
student	O
vocabulary	O
,	O
with	O
the	O
other	O
words	O
segmented	O
using	O
the	O
teacher	O
vocabulary	O
.	O
As	O
in	O
Figure	O
1	O
on	O
the	O
left	O
,	O
for	O
input	O
[	O
'	O
I	O
'	O
,	O
'	O
like	O
'	O
,	O
'	O
machine	O
'	O
,	O
'	O
learning	O
'	O
]	O
,	O
the	O
words	O
'	O
like	O
'	O
and	O
'	O
learning	O
'	O
are	O
segmented	O
using	O
the	O
student	O
vocabulary	O
(	O
in	O
blue	O
)	O
,	O
with	O
the	O
others	O
using	O
the	O
teacher	O
vocabulary	O
(	O
in	O
green	O
)	O
.	O
Similar	O
to	O
Lample	O
and	O
Conneau	O
(	O
2019	O
)	O
,	O
this	O
step	O
seeks	O
to	O
align	O
the	O
student	O
and	O
teacher	O
embeddings	O
for	O
the	O
same	O
tokens	O
:	O
the	O
model	O
learns	O
to	O
predict	O
student	O
tokens	O
using	O
context	O
which	O
is	O
segmented	O
using	O
the	O
teacher	O
vocabulary	O
,	O
and	O
vice	O
versa	O
.	O
Note	O
that	O
since	O
the	O
student	O
embeddings	O
are	O
set	O
to	O
a	O
lower	O
dimension	O
than	O
the	O
teacher	O
embeddings	O
,	O
as	O
they	O
are	O
meant	O
to	O
be	O
used	O
in	O
the	O
smaller	O
student	O
model	O
,	O
we	O
project	O
the	O
student	O
embeddings	O
up	O
to	O
the	O
teacher	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
using	O
a	O
trainable	O
affine	O
layer	O
before	O
these	O
are	O
input	O
to	O
the	O
teacher	O
BERT	B-MethodName
.	O
We	O
choose	O
to	O
keep	O
the	O
two	O
embedding	O
matrices	O
separate	O
despite	O
the	O
high	O
token	O
overlap	O
:	O
this	O
is	O
partly	O
to	O
keep	O
our	O
approach	O
robust	O
to	O
lower	O
vocabulary	O
overlap	O
settings	O
,	O
and	O
partly	O
due	O
to	O
empirical	O
considerations	O
described	O
in	O
Section	O
6	O
.	O
Let	O
θ	B-HyperparameterName
s	O
/eb	O
s	O
and	O
θ	B-HyperparameterName
t	O
/eb	O
t	O
denote	O
the	O
transformer	O
layer	O
and	O
embedding	O
weights	O
for	O
the	O
student	O
and	O
teacher	O
models	O
respectively	O
.	O
The	O
loss	B-MetricName
defined	O
in	O
Equation	O
1	O
is	O
the	O
MLM	B-DatasetName
cross	O
entropy	O
summed	O
over	O
masked	O
positions	O
M	O
t	O
in	O
the	O
teacher	O
input	O
.	O
y	O
i	O
and	O
c	O
i	O
denote	O
the	O
predicted	O
and	O
true	O
tokens	O
at	O
position	O
i	O
respectively	O
and	O
can	O
belong	O
to	O
either	O
vocabulary	O
.	O
v	O
i	O
{	O
s	O
,	O
t	O
}	O
denotes	O
the	O
vocabulary	O
used	O
to	O
segment	O
this	O
token	O
.	O
Separate	O
softmax	B-MethodName
layers	O
P	O
v	O
i	O
are	O
used	O
for	O
token	O
prediction	O
,	O
one	O
for	O
each	O
vocabulary	O
,	O
depending	O
on	O
the	O
segmenting	O
vocabulary	O
v	O
i	O
for	O
token	O
i.	O
All	O
teacher	O
parameters	O
(	O
θ	B-HyperparameterName
t	O
,	O
eb	O
t	O
)	O
and	O
student	O
embeddings	O
(	O
eb	O
s	O
)	O
are	O
updated	O
in	O
this	O
step	O
.	O
L	O
s	O
1	O
=	O
−	O
i	O
Mt	O
(	O
logP	O
v	O
i	O
(	O
y	O
i	O
=	O
c	O
i	O
|	O
θ	B-HyperparameterName
t	O
,	O
eb	O
s	O
,	O
eb	O
t	O
)	O
)	O
(	O
1	O
)	O
Stage	O
II	O
(	O
Student	O
Model	O
Layers	O
)	O
:	O
With	O
student	O
embeddings	O
initialized	O
in	O
stage	O
I	O
,	O
we	O
now	O
train	O
the	O
student	O
model	O
normally	O
i.e.	O
,	O
using	O
only	O
the	O
student	O
vocabulary	O
and	O
discarding	O
the	O
teacher	O
model	O
.	O
Equation	O
2	O
shows	O
the	O
student	O
MLM	B-DatasetName
loss	B-MetricName
where	O
M	O
s	O
is	O
the	O
set	O
of	O
positions	O
masked	O
in	O
the	O
student	O
input	O
.	O
All	O
student	O
model	O
parameters	O
(	O
θ	B-HyperparameterName
s	O
,	O
eb	O
s	O
)	O
are	O
updated	O
.	O
L	O
s	O
2	O
=	O
−	O
i	O
M	O
s	O
logP	O
s	O
(	O
y	O
i	O
=	O
c	O
i	O
|	O
θ	B-HyperparameterName
s	O
,	O
eb	O
s	O
)	O
)	O
(	O
2	O
)	O

Distillation	O
:	O
For	O
all	O
our	O
models	O
,	O
we	O
train	O
the	O
teacher	O
model	O
with	O
mixed	O
-	O
vocabulary	O
inputs	O
(	O
stage	O
I	O
)	O
for	O
500	O
K	O
steps	O
,	O
followed	O
by	O
300	O
K	O
steps	O
of	O
training	O
just	O
the	O
student	O
model	O
(	O
stage	O
II	O
)	O
.	O
We	O
utilize	O
the	O
same	O
corpora	O
as	O
the	O
teacher	O
model	O
i.e.	O
BooksCorpus	O
(	O
Zhu	O
et	O
al	O
,	O
2015	O
)	O
and	O
English	O
Wikipedia	O
.	O
For	O
both	O
stages	O
,	O
up	O
to	O
20	O
input	O
tokens	O
were	O
masked	O
for	O
MLM	B-DatasetName
.	O
In	O
stage	O
I	O
,	O
up	O
to	O
10	O
of	O
these	O
masked	O
tokens	O
were	O
tokenized	O
using	O
the	O
teacher	O
vocabulary	O
,	O
the	O
rest	O
using	O
the	O
student	O
vocabulary	O
.	O
We	O
optimize	O
the	O
loss	B-MetricName
using	O
LAMB	B-MethodName
(	O
You	O
et	O
al	O
,	O
2019	O
)	O
with	O
a	O
max	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
.00125	O
,	O
linear	B-MethodName
warmup	I-MethodName
for	O
the	O
first	O
10	O
%	O
of	O
steps	O
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
2048	B-DatasetName
and	O
sequence	O
length	O
of	O
128	O
.	O
Distillation	O
was	O
done	O
on	O
Cloud	O
TPUs	O
in	O
a	O
8x8	O
pod	O
configuration	O
.	O
p	O
SV	O
,	O
the	O
probability	O
of	O
segmenting	O
a	O
Stage	O
I	O
input	O
word	O
using	O
the	O
student	O
vocabulary	O
,	O
is	O
set	O
to	O
0.5	O
.	O
Finetuning	O
:	O
For	O
all	O
downstream	O
task	O
evaluations	O
on	O
GLUE	B-DatasetName
,	O
we	O
finetune	O
for	O
10	O
epochs	O
using	O
LAMB	B-MethodName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.0001	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	O
.	O
For	O
all	O
experiments	O
on	O
SNIPS	B-DatasetName
,	O
we	O
use	O
ADAM	B-DatasetName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.0001	O
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	O
.	O

Table	O
1	O
shows	O
results	O
on	O
downstream	O
GLUE	B-DatasetName
tasks	O
and	O
model	O
sizes	O
for	O
our	O
proposed	O
models	O
,	O
BERT	B-MethodName
BASE	B-MethodName
/	O
LARGE	O
,	O
and	O
baselines	O
.	O
Our	O
models	O
consistently	O
improve	O
upon	O
the	O
identically	O
parameterized	O
NoKD	O
baselines	O
,	O
indicating	O
mixedvocabulary	O
training	O
is	O
better	O
than	O
training	O
from	O
scratch	O
and	O
avoids	O
a	O
large	O
teacher	O
-	O
student	O
performance	O
gap	O
.	O
Compared	O
with	O
PKD	O
/	O
DistilBERT	B-MethodName
,	O
our	O
6	O
-	O
layer	O
model	O
outperforms	O
PKD	O
3	O
while	O
being	O
>	O
7x	O
smaller	O
and	O
our	O
12	O
-	O
layer	O
model	O
is	O
comparable	O
to	O
PKD	O
6	O
and	O
DistilBERT	B-MethodName
4	O
while	O
being	O
∼5	O
-	O
6x	O
smaller	O
.	O
Interestingly	O
,	O
our	O
models	O
do	O
particularly	O
well	O
on	O
the	O
MRPC	B-DatasetName
task	O
:	O
the	O
6	O
-	O
layer	O
distilled	O
model	O
performs	O
almost	O
as	O
well	O
as	O
PKD	O
6	O
while	O
being	O
over	O
10x	O
smaller	O
.	O
This	O
may	O
be	O
due	O
to	O
our	O
smaller	O
models	O
being	O
data	O
-	O
efficient	O
on	O
the	O
smaller	O
MRPC	B-DatasetName
dataset	O
.	O
TinyBERT	O
and	O
Bert	O
-	O
of	O
-	O
Theseus	O
are	O
trained	O
in	O
task	O
-	O
specific	O
fashion	O
i.e.	O
,	O
a	O
teacher	O
model	O
already	O
finetuned	O
on	O
the	O
downstream	O
task	O
is	O
used	O
for	O
distillation	O
.	O
TinyBERT	O
's	O
non	O
-	O
task	O
-	O
specific	O
model	O
results	O
are	O
reported	O
on	O
GLUE	B-DatasetName
dev	O
sets	O
:	O
these	O
results	O
are	O
,	O
therefore	O
,	O
not	O
directly	O
comparable	O
with	O
ours	O
.	O
Even	O
so	O
,	O
our	O
12	O
-	O
layer	O
model	O
performs	O
credibly	O
compared	O
with	O
the	O
two	O
,	O
presenting	O
a	O
competitive	O
size	O
-	O
accuracy	B-MetricName
tradeoff	O
,	O
particularly	O
when	O
compared	O
to	O
the	O
6x	O
larger	O
BERT	B-MethodName
-	O
of	O
-	O
Theseus	O
.	O
MobileBERT	B-MethodName
performs	O
strongly	O
for	O
the	O
size	O
while	O
being	O
task	O
-	O
agnostic	O
.	O
Our	O
12	O
-	O
layer	O
model	O
,	O
in	O
comparison	O
,	O
retains	O
∼98	O
%	O
of	O
its	O
performance	O
with	O
57	O
%	O
fewer	O
parameters	O
and	O
may	O
thus	O
be	O
bettersuited	O
for	O
use	O
on	O
highly	O
resource	O
-	O
limited	O
devices	O
.	O
TinyBERT	O
sees	O
major	O
gains	O
from	O
task	O
-	O
specific	O
data	B-TaskName
augmentation	I-TaskName
and	O
distillation	O
,	O
and	O
Mobile	O
-	O
BERT	B-MethodName
from	O
student	O
architecture	O
search	O
and	O
bottleneck	O
layers	O
.	O
Notably	O
,	O
our	O
technique	O
targets	O
the	O
student	O
vocabulary	O
without	O
conflicting	O
with	O
any	O
of	O
the	O
above	O
methods	O
and	O
can	O
,	O
in	O
fact	O
,	O
be	O
combined	O
with	O
these	O
methods	O
for	O
even	O
smaller	O
models	O
.	O

Table	O
2	O
shows	O
results	O
on	O
the	O
SNIPS	B-DatasetName
intent	O
and	O
slot	O
tasks	O
for	O
our	O
models	O
and	O
two	O
state	O
-	O
of	O
-	O
theart	O
baselines	O
.	O
Our	O
smallest	O
6	O
-	O
layer	O
model	O
retains	O
over	O
95	O
%	O
of	O
the	O
BERT	B-MethodName
BASE	B-MethodName
model	O
's	O
slot	B-TaskName
filling	I-TaskName
F1	B-MetricName
score	I-MetricName
(	O
Sang	O
and	O
Buchholz	O
,	O
2000	O
)	O
while	O
being	O
30x	O
smaller	O
(	O
<	O
10	O
MB	O
w/o	O
quantization	B-TaskName
)	O
and	O
57x	O
faster	O
on	O
a	O
mobile	O
device	O
,	O
yet	O
task	O
-	O
agnostic	O
.	O
Our	O
other	O
larger	O
distilled	O
models	O
also	O
demonstrate	O
strong	O
performance	O
(	O
0.2	O
-	O
0.5	O
%	O
slot	O
F1	B-MetricName
higher	O
than	O
the	O
respective	O
NoKD	O
baselines	O
)	O
with	O
small	O
model	O
sizes	O
and	O
latencies	O
low	O
enough	O
for	O
real	O
-	O
time	O
inference	O
.	O
This	O
indicates	O
that	O
small	O
multi	O
-	O
task	O
BERT	B-MethodName
models	O
(	O
Tsai	O
et	O
al	O
,	O
2019	O
)	O
present	O
better	O
trade	O
-	O
offs	O
for	O
on	O
-	O
device	O
usage	O
for	O
size	O
,	O
accuracy	B-MetricName
and	O
latency	O
versus	O
recurrent	O
encoder	O
-	O
based	O
models	O
such	O
as	O
StackProp	O
.	O

Impact	O
of	O
vocabulary	O
size	O
:	O
We	O
trained	O
a	O
model	O
from	O
scratch	O
identical	O
to	O
BERT	B-MethodName
BASE	B-MethodName
except	O
with	O
our	O
5	O
K	O
-	O
WP	O
student	O
vocabulary	O
.	O
On	O
the	O
SST	B-DatasetName
-	O
2	O
and	O
MNLI	B-DatasetName
-	I-DatasetName
m	I-DatasetName
dev	O
sets	O
,	O
this	O
model	O
obtained	O
90.9	O
%	O
and	O
83.7	O
%	O
accuracy	B-MetricName
respectively	O
-	O
only	O
1.8	O
%	O
and	O
0.7	O
%	O
lower	O
respectively	O
compared	O
to	O
BERT	B-MethodName
BASE	B-MethodName
.	O
Since	O
embeddings	O
account	O
for	O
a	O
larger	O
fraction	O
of	O
model	O
parameters	O
with	O
fewer	O
layers	O
,	O
we	O
trained	O
another	O
model	O
identical	O
to	O
our	O
6×256	O
model	O
,	O
but	O
with	O
a	O
30	O
K	O
-	O
WP	O
vocabulary	O
and	O
teacher	O
label	O
dis	O
-	O
tillation	O
.	O
This	O
model	O
showed	O
small	O
gains	O
(	O
0.1	O
%	O
/	O
0.5	O
%	O
accuracy	B-MetricName
on	O
SST	B-DatasetName
-	O
2	O
/	O
MNLI	B-DatasetName
-	I-DatasetName
m	I-DatasetName
dev	O
)	O
over	O
our	O
analogous	O
distilled	O
model	O
,	O
but	O
with	O
30	O
%	O
more	O
parameters	O
solely	O
due	O
to	O
the	O
larger	O
vocabulary	O
.	O
This	O
suggests	O
that	O
a	O
small	O
WordPiece	B-MethodName
vocabulary	O
may	O
be	O
almost	O
as	O
effective	O
for	O
sequence	O
classification	O
/	O
tagging	O
tasks	O
,	O
especially	O
for	O
smaller	O
BERT	B-MethodName
models	O
and	O
up	O
to	O
moderately	O
long	O
inputs	O
.	O
Curiously	O
,	O
increasing	O
the	O
student	O
vocabulary	O
size	O
to	O
7	O
K	O
or	O
10	O
K	O
did	O
not	O
lead	O
to	O
an	O
increase	O
in	O
performance	O
on	O
GLUE	B-DatasetName
.	O
We	O
surmise	O
that	O
this	O
may	O
be	O
due	O
to	O
underfitting	O
owing	O
to	O
the	O
embeddings	O
accounting	O
for	O
a	O
larger	O
proportion	O
of	O
the	O
model	O
parameters	O
.	O
Alternative	O
vocabulary	O
pruning	O
:	O
Probing	O
other	O
strategies	O
for	O
a	O
small	O
-	O
vocabulary	O
model	O
,	O
we	O
used	O
the	O
above	O
6×256	O
30	O
K	O
-	O
WP	O
vanilla	O
distilled	O
model	O
to	O
obtain	O
a	O
smaller	O
model	O
by	O
pruning	O
the	O
vocabulary	O
to	O
contain	O
the	O
intersection	O
of	O
the	O
30	O
K	O
and	O
5	O
K	O
vocabularies	O
(	O
total	O
4629	O
WPs	O
)	O
.	O
This	O
model	O
is	O
1.2	O
%	O
smaller	O
than	O
our	O
4928	O
-	O
WP	O
distilled	O
model	O
,	O
but	O
drops	O
0.8	O
%	O
/	O
0.7	O
%	O
on	O
SST	B-DatasetName
-	O
2	O
/	O
MNLI	B-DatasetName
-	I-DatasetName
m	I-DatasetName
dev	O
sets	O
.	O
Furthermore	O
,	O
to	O
exploit	O
the	O
high	O
overlap	O
in	O
vocabularies	O
,	O
we	O
tried	O
running	O
our	O
distillation	O
pipeline	O
but	O
with	O
the	O
embeddings	O
for	O
student	O
tokens	O
(	O
after	O
projecting	O
up	O
to	O
the	O
teacher	O
dimension	O
)	O
also	O
present	O
in	O
the	O
teacher	O
vocabulary	O
tied	O
to	O
the	O
teacher	O
embeddings	O
for	O
those	O
tokens	O
.	O
This	O
model	O
,	O
however	O
,	O
dropped	O
0.7	O
%	O
/	O
0.5	O
%	O
on	O
SST	B-DatasetName
-	O
2	O
/	O
MNLI	B-DatasetName
-	I-DatasetName
m	I-DatasetName
compared	O
to	O
our	O
analogous	O
6×256	O
distilled	O
model	O
.	O
We	O
also	O
tried	O
pretraining	O
BERT	B-MethodName
LARGE	O
from	O
scratch	O
with	O
the	O
5	O
K	O
vocabulary	O
and	O
doing	O
vanilla	O
distillation	O
for	O
a	O
6×256	O
student	O
:	O
this	O
model	O
dropped	O
1.2	O
%	O
/	O
0.7	O
%	O
for	O
SST	B-DatasetName
-	O
2	O
/	O
MNLI	B-DatasetName
-	I-DatasetName
m	I-DatasetName
over	O
our	O
similar	O
distilled	O
model	O
,	O
indicating	O
the	O
efficacy	O
of	O
mixed	O
-	O
vocabulary	O
training	O
over	O
vanilla	O
distillation	O
.	O

The	O
utility	O
of	O
additional	O
semantic	O
information	O
for	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
in	O
an	O
automated	O
dialogue	O
system	O
is	O
the	O
focus	O
of	O
study	O
in	O
this	O
paper	O
.	O
In	O
particular	O
,	O
we	O
show	O
that	O
additional	O
information	O
available	O
in	O
the	O
form	O
of	O
dialogue	O
acts	O
-	O
when	O
used	O
along	O
with	O
context	O
given	O
in	O
the	O
form	O
of	O
dialogue	O
history	O
-	O
improves	O
the	O
performance	O
irrespective	O
of	O
the	O
underlying	O
model	O
being	O
generative	O
or	O
discriminative	O
.	O
In	O
order	O
to	O
show	O
the	O
model	O
agnostic	O
behavior	O
of	O
dialogue	O
acts	O
,	O
we	O
experiment	O
with	O
several	O
well	O
-	O
known	O
models	O
such	O
as	O
sequence	O
-	O
to	O
-	O
sequence	O
encoder	O
-	O
decoder	O
model	O
,	O
hierarchical	O
encoder	O
-	O
decoder	O
model	O
,	O
and	O
Siamese	O
-	O
based	O
models	O
with	O
and	O
without	O
hierarchy	O
;	O
and	O
show	O
that	O
in	O
all	O
models	O
,	O
incorporating	O
dialogue	O
acts	O
improves	O
the	O
performance	O
by	O
a	O
significant	O
margin	O
.	O
We	O
,	O
furthermore	O
,	O
propose	O
a	O
novel	O
way	O
of	O
encoding	O
dialogue	O
act	O
information	O
,	O
and	O
use	O
it	O
along	O
with	O
hierarchical	O
encoder	O
to	O
build	O
a	O
model	O
that	O
can	O
use	O
the	O
sequential	O
dialogue	O
act	O
information	O
in	O
a	O
natural	O
way	O
.	O
Our	O
proposed	O
model	O
achieves	O
an	O
MRR	B-MetricName
of	O
about	O
84.8	O
%	O
for	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
on	O
a	O
newly	O
introduced	O
DailyDialog	B-DatasetName
dataset	O
,	O
and	O
outperform	O
the	O
baseline	O
models	O
.	O
We	O
also	O
provide	O
a	O
detailed	O
analysis	O
of	O
results	O
including	O
key	O
insights	O
that	O
explain	O
the	O
improvement	O
in	O
MRR	B-MetricName
because	O
of	O
dialogue	O
act	O
information	O
.	O

An	O
encoder	O
-	O
decoder	O
is	O
a	O
generative	O
model	O
that	O
works	O
on	O
the	O
idea	O
of	O
obtaining	O
a	O
representation	O
of	O
an	O
input	O
and	O
use	O
it	O
for	O
generating	O
an	O
output	O
.	O
It	O
has	O
two	O
main	O
components	O
,	O
encoder	O
and	O
decoder	O
.	O
The	O
encoder	O
encodes	O
the	O
first	O
K	O
utterances	O
,	O
and	O
the	O
decoder	O
uses	O
that	O
encoding	O
to	O
generate	O
the	O
next	O
K	O
+	O
1	O
th	O
utterance	O
.	O
In	O
a	O
conversation	O
,	O
all	O
words	O
in	O
first	O
K	O
utterances	O
can	O
be	O
stringed	O
together	O
to	O
form	O
a	O
single	O
long	O
chain	O
and	O
passed	O
to	O
an	O
RNN	O
encoder	O
as	O
following	O
:	O
e	O
k	B-HyperparameterName
=	I-HyperparameterName
f	O
1	O
embed	O
(	O
w	O
k	O
)	O
∀k	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
h	O
e	O
k	B-HyperparameterName
=	I-HyperparameterName
f	O
1	O
rnn	O
(	O
h	O
e	O
k−1	O
,	O
e	O
k	O
)	O
∀k	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
(	O
1	O
)	O
where	O
,	O
f	O
1	O
embed	O
represents	O
the	O
embedding	O
layer	O
,	O
whereas	O
f	O
1	O
rnn	O
is	O
the	O
encoder	O
(	O
RNN	O
)	O
.	O
Let	O
v	O
be	O
the	O
final	O
output	O
of	O
the	O
encoder	O
which	O
is	O
considered	O
as	O
a	O
representation	O
of	O
the	O
entire	O
context	O
,	O
and	O
used	O
to	O
initialize	O
the	O
decoder	O
(	O
another	O
RNN	O
)	O
.	O
Mathematically	O
,	O
the	O
sequence	O
of	O
operations	O
at	O
the	O
decoder	O
are	O
as	O
follows	O
:	O
h	O
d	O
0	B-DatasetName
=	O
v	O
h	O
d	O
k	B-HyperparameterName
=	I-HyperparameterName
f	O
2	O
rnn	O
(	O
h	O
d	O
k−1	O
,	O
f	O
2	O
embed	O
(	O
w	O
k	O
)	O
)	O
∀k	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
n	O
−	O
1	O
P	O
k	B-HyperparameterName
=	I-HyperparameterName
Logistic	O
(	O
h	O
d	O
k	O
)	O
.	O
(	O
2	O
)	O
Here	O
,	O
f	O
2	O
embed	O
represents	O
the	O
embedding	O
layer	O
.	O
Logistic	O
is	O
the	O
final	O
layer	O
,	O
which	O
outputs	O
the	O
probability	O
distribution	O
over	O
the	O
vocabulary	O
.	O
Encoder	O
-	O
decoder	O
models	O
are	O
trained	O
to	O
maximize	O
the	O
likelihood	O
of	O
generating	O
the	O
next	O
utterance	O
,	O
however	O
,	O
for	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
,	O
they	O
are	O
tested	O
based	O
on	O
the	O
probability	O
of	O
generating	O
the	O
candidate	O
utterances	O
.	O

Dialogue	O
acts	O
are	O
higher	O
level	O
abstractions	O
assigned	O
to	O
utterances	O
.	O
In	O
our	O
problem	O
setting	O
,	O
we	O
are	O
given	O
a	O
list	O
of	O
dialogue	O
acts	O
da	O
1	O
,	O
da	O
2	O
,	O
.	O
.	O
.	O
da	O
K	O
,	O
corresponding	O
to	O
first	O
K	O
utterances	O
in	O
the	O
conversation	O
.	O
These	O
dialogue	O
acts	O
are	O
treated	O
as	O
an	O
additional	O
sequence	O
of	O
signals	O
that	O
can	O
aid	O
in	O
the	O
learning	O
process	O
,	O
and	O
are	O
passed	O
through	O
an	O
encoder	O
,	O
denoted	O
as	O
Dialog	O
-	O
Act	O
encoder	O
(	O
DA	O
-	O
encoder	O
)	O
.	O
The	O
DA	O
-	O
encoder	O
works	O
on	O
the	O
same	O
principle	O
as	O
the	O
utterance	O
encoder	O
.	O
It	O
builds	O
a	O
dialogue	O
act	O
vocabulary	O
and	O
uses	O
that	O
to	O
learn	O
dialogue	O
act	O
embeddings	O
.	O
Similar	O
to	O
the	O
utterance	O
encoder	O
,	O
the	O
input	O
to	O
the	O
DA	O
-	O
encoder	O
are	O
one	O
hot	O
encodings	O
of	O
the	O
dialogue	O
acts	O
,	O
which	O
are	O
then	O
passed	O
through	O
an	O
embedding	O
layer	O
to	O
learn	O
DA	O
embeddings	O
.	O
These	O
DA	O
embeddings	O
are	O
sent	O
to	O
an	O
RNN	O
to	O
learn	O
dialogue	O
act	O
representations	O
.	O
The	O
sequence	O
of	O
operations	O
for	O
the	O
DA	O
-	O
encoder	O
are	O
as	O
follows	O
:	O
e	O
da	O
k	B-HyperparameterName
=	I-HyperparameterName
f	O
3	O
embed	O
(	O
da	O
k	O
)	O
∀k	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
K	O
h	O
da	O
k	B-HyperparameterName
=	I-HyperparameterName
f	O
4	O
rnn	O
(	O
h	O
da	O
k−1	O
,	O
e	O
da	O
k	O
)	O
∀k	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
K.	O
q	O
K	B-HyperparameterName
=	I-HyperparameterName
h	O
da	O
K	O
(	O
4	O
)	O
The	O
output	O
of	O
the	O
DA	O
-	O
encoder	O
at	O
the	O
last	O
time	O
step	O
(	O
q	O
K	O
)	O
gives	O
us	O
the	O
representation	O
of	O
the	O
entire	O
DA	O
sequence	O
which	O
is	O
then	O
used	O
in	O
the	O
further	O
modeling	O
process	O
in	O
generative	O
and	O
discriminative	O
models	O
.	O
In	O
generative	O
models	O
,	O
it	O
is	O
used	O
in	O
the	O
decoder	O
by	O
concatenating	O
g	O
K	O
and	O
q	O
K	O
,	O
whereas	O
in	O
discriminative	O
models	O
,	O
it	O
is	O
used	O
along	O
with	O
encoder	O
's	O
output	O
by	O
combining	O
g	O
K	O
with	O
q	O
K	O
through	O
a	O
linear	O
combination	O
.	O

Our	O
proposed	O
model	O
,	O
i.e.	O
Dialog	O
-	O
act	O
-	O
driven	O
Hierarchical	O
Siamese	O
Model	O
(	O
HSiamese	O
-	O
DA	O
)	O
,	O
uses	O
the	O
following	O
three	O
components	O
:	O
a	O
hierarchical	O
encoder	O
to	O
obtain	O
a	O
representation	O
that	O
captures	O
the	O
dependencies	O
among	O
K	O
utterances	O
;	O
an	O
utterance	O
encoder	O
to	O
obtain	O
a	O
representation	O
of	O
the	O
candidate	O
response	O
,	O
(	O
K	O
+	O
1	O
)	O
th	O
utterance	O
;	O
a	O
DA	O
-	O
encoder	O
(	O
Equation	O
4	O
)	O
that	O
captures	O
the	O
dependencies	O
among	O
the	O
dialogue	O
acts	O
of	O
the	O
first	O
K	O
utterances	O
.	O
Let	O
the	O
representation	O
obtained	O
from	O
the	O
hierarchical	O
encoder	O
,	O
DA	O
-	O
encoder	O
and	O
utterance	O
encoder	O
be	O
g	O
K	O
,	O
q	O
K	O
and	O
v	O
K+1	O
,	O
respectively	O
.	O
The	O
two	O
representations	O
,	O
g	O
K	O
and	O
q	O
K	O
,	O
are	O
linearly	O
combined	O
to	O
obtained	O
a	O
compositional	O
representation	O
of	O
the	O
context	O
,	O
which	O
is	O
then	O
used	O
along	O
with	O
candidate	O
representation	O
to	O
compute	O
the	O
probability	O
of	O
associating	O
the	O
candidate	O
response	O
with	O
the	O
context	O
using	O
following	O
expression	O
:	O
d	O
K	B-HyperparameterName
=	I-HyperparameterName
α	B-HyperparameterName
*	O
g	O
K	O
+	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
*	O
q	O
K	O
p	O
(	O
s	O
|	O
d	O
K	O
,	O
v	O
K+1	O
)	O
=	O
σ	O
(	O
d	O
T	O
K	O
A	O
v	O
K+1	O
+	O
b	O
)	O
(	O
5	O
)	O
The	O
model	O
is	O
trained	O
by	O
minimizing	O
the	O
cross	O
-	O
entropy	O
of	O
all	O
labeled	O
conversations	O
including	O
positive	O
and	O
negative	O
examples	O
.	O
At	O
the	O
test	O
time	O
,	O
each	O
conversation	O
has	O
K	O
utterances	O
followed	O
by	O
a	O
set	O
of	O
10	O
candidates	O
responses	O
.	O
The	O
system	O
is	O
tested	O
in	O
its	O
ability	O
to	O
assign	O
a	O
higher	O
rank	O
to	O
the	O
true	O
response	O
.	O

In	O
our	O
experiments	O
,	O
the	O
parameters	O
are	O
tuned	O
on	O
validation	O
set	O
while	O
the	O
results	O
are	O
reported	O
on	O
test	O
set	O
.	O
Each	O
utterance	O
in	O
a	O
mini	O
-	O
batch	O
was	O
padded	O
to	O
the	O
maximum	O
length	O
for	O
that	O
batch	O
.	O
The	O
maximum	O
batch	B-HyperparameterName
size	I-HyperparameterName
allowed	O
was	O
32	O
.	O
The	O
word	O
vectors	O
were	O
initialized	O
with	O
the	O
300	O
-	O
dimensional	O
Glove	B-MethodName
embeddings	I-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
and	O
were	O
also	O
updated	O
during	O
training	O
.	O
For	O
the	O
generative	O
models	O
,	O
the	O
utterance	O
encoder	O
,	O
conversation	O
encoder	O
,	O
DA	O
-	O
Encoder	O
and	O
decoder	O
are	O
all	O
GRUs	O
with	O
rnn	O
size	O
set	O
to	O
1000	O
(	O
optimized	O
over	O
100	O
to	O
1200	O
in	O
steps	O
of	O
100	O
)	O
.	O
For	O
the	O
discriminative	O
model	O
,	O
the	O
utterance	O
encoder	O
,	O
conversation	O
encoder	O
,	O
and	O
DA	O
-	O
Encoder	O
are	O
all	O
GRUs	O
with	O
rnn	O
size	O
set	O
to	O
300	O
(	O
optimized	O
over	O
100	O
to	O
500	O
in	O
steps	O
of	O
100	O
)	O
.	O
Dropout	B-MethodName
of	O
0.1	O
(	O
optimized	O
over	O
0.0	O
to	O
0.7	O
in	O
steps	O
of	O
0.1	O
)	O
was	O
applied	O
to	O
embeddings	O
obtained	O
from	O
the	O
output	O
of	O
conversation	O
encoder	O
.	O
Note	O
that	O
,	O
dropout	O
was	O
not	O
used	O
in	O
the	O
discriminative	O
model	O
and	O
its	O
variations	O
.	O
Models	O
were	O
trained	O
to	O
minimize	O
cross	O
entropy	O
using	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.0003	O
(	O
optimized	O
over	O
0.0001	O
,	O
0.0003	O
,	O
0.0005	O
,	O
0.0007	O
,	O
0.001	O
)	O
.	O
We	O
found	O
that	O
a	O
higher	O
learning	B-HyperparameterName
rate	I-HyperparameterName
up	O
-	O
to	O
0.0005	O
helps	O
the	O
model	O
to	O
learn	O
quickly	O
,	O
whereas	O
learning	B-HyperparameterName
rate	I-HyperparameterName
greater	O
than	O
0.0005	O
leads	O
to	O
oscillations	O
.	O

Since	O
our	O
problem	O
formulation	O
is	O
retrieval	O
based	O
,	O
we	O
use	O
standard	O
IR	O
metrics	O
such	O
as	O
Mean	O
Reciprocal	O
Rank	O
(	O
MRR	B-MetricName
)	O
and	O
Recall@k	O
as	O
our	O
evaluation	O
metrics	O
.	O
MRR	B-MetricName
is	O
calculated	O
as	O
the	O
mean	O
of	O
the	O
reciprocal	O
rank	O
of	O
the	O
true	O
candidate	O
response	O
among	O
other	O
candidate	O
responses	O
.	O
Recall@k	O
measures	O
whether	O
the	O
true	O
candidate	O
response	O
appears	O
in	O
a	O
ranked	O
list	O
of	O
k	O
responses	O
.	O
In	O
this	O
work	O
,	O
our	O
hypothesis	O
is	O
that	O
additional	O
information	O
about	O
utterances	O
available	O
in	O
the	O
form	O
of	O
dialogue	O
acts	O
helps	O
irrespective	O
of	O
the	O
underlying	O
model	O
,	O
i.e.	O
generative	O
or	O
discriminative	O
.	O
Results	O
in	O
Table	O
3	O
support	O
our	O
hypothesis	O
.	O
These	O
results	O
clearly	O
indicate	O
that	O
the	O
MRR	B-MetricName
of	O
the	O
true	O
candidate	O
response	O
improves	O
when	O
dialogue	O
acts	O
of	O
previous	O
utterances	O
are	O
provided	O
.	O
From	O
these	O
tables	O
we	O
see	O
that	O
for	O
all	O
underlying	O
models	O
,	O
the	O
dialogue	O
act	O
version	O
performs	O
better	O
than	O
non	O
dialogue	O
act	O
version	O
.	O
These	O
results	O
furthermore	O
indicate	O
that	O
hierarchical	O
version	O
performs	O
better	O
than	O
non	O
-	O
hierarchical	O
version	O
for	O
both	O
generative	O
and	O
discriminative	O
models	O
.	O
In	O
the	O
generative	O
case	O
,	O
the	O
plain	O
ED	O
has	O
an	O
MRR	B-MetricName
of	O
0.474	O
,	O
whereas	O
the	O
same	O
model	O
,	O
when	O
conditioned	O
with	O
DA	O
-	O
Encoder	O
,	O
has	O
an	O
MRR	B-MetricName
of	O
0.54	O
,	O
an	O
improvement	O
of	O
13.9	O
%	O
.	O
The	O
hierarchical	O
encoder	O
-	O
decoder	O
HRED	O
and	O
HRED	O
-	O
DA	O
has	O
an	O
MRR	B-MetricName
of	O
0.523	O
and	O
0.583	O
,	O
respectively	O
,	O
an	O
improvement	O
of	O
11.4	O
%	O
.	O
Generative	O
models	O
are	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
and	O
rather	O
complex	O
in	O
nature	O
,	O
so	O
it	O
is	O
interesting	O
to	O
note	O
that	O
even	O
a	O
much	O
simpler	O
discriminative	O
model	O
,	O
i.e.	O
plain	O
Siamese	O
model	O
,	O
without	O
any	O
dialogue	O
act	O
information	O
,	O
has	O
an	O
MRR	B-MetricName
of	O
0.8	O
compared	O
to	O
0.58	O
of	O
the	O
best	O
performing	O
generative	O
model	O
,	O
i.e.	O
HRED	O
-	O
DA	O
.	O
This	O
observation	O
demonstrates	O
the	O
strength	O
of	O
the	O
discriminative	O
models	O
,	O
and	O
therefore	O
is	O
a	O
motivation	O
behind	O
the	O
proposed	O
model	O
.	O
The	O
proposed	O
model	O
improves	O
these	O
baseline	O
numbers	O
by	O
incorporating	O
hierarchy	O
and	O
dialogue	O
act	O
information	O
,	O
and	O
pushes	O
the	O
MRR	B-MetricName
to	O
0.848	O
.	O

While	O
we	O
have	O
shown	O
that	O
using	O
dialogue	O
act	O
information	O
does	O
help	O
in	O
the	O
next	O
utterance	O
selection	O
task	O
,	O
in	O
this	O
section	O
,	O
we	O
dig	O
deeper	O
and	O
understand	O
reasons	O
for	O
it	O
.	O
In	O
order	O
to	O
do	O
that	O
,	O
we	O
analyze	O
the	O
dialogue	O
act	O
distribution	O
of	O
the	O
test	O
data	O
and	O
model	O
outputs	O
.	O
Although	O
all	O
K	O
dialogue	O
acts	O
corresponding	O
to	O
K	O
utterances	O
in	O
the	O
context	O
might	O
play	O
a	O
role	O
in	O
ranking	O
candidate	O
utterances	O
,	O
the	O
following	O
analysis	O
only	O
uses	O
the	O
pairs	O
of	O
dialogue	O
acts	O
,	O
i.e.	O
dialogue	O
acts	O
of	O
K	O
th	O
and	O
(	O
K	O
+	O
1	O
)	O
th	O
utterances	O
.	O
Tables	O
4	O
(	O
a	O
)	O
,	O
4	O
(	O
b	O
)	O
and	O
4	O
(	O
c	O
)	O
show	O
the	O
distribution	O
of	O
such	O
dialogue	O
act	O
pairs	O
for	O
test	O
data	O
,	O
HSiamese	O
,	O
and	O
HSiamese	O
-	O
DA	O
models	O
respectively	O
.	O
Here	O
,	O
rows	O
indicate	O
the	O
dialogue	O
act	O
of	O
K	O
th	O
utterance	O
,	O
whereas	O
columns	O
indicate	O
the	O
dialogue	O
act	O
of	O
(	O
K	O
+	O
1	O
)	O
th	O
utterance	O
.	O
A	O
cell	O
value	O
indicates	O
the	O
count	O
of	O
utterance	O
pairs	O
with	O
the	O
respective	O
dialogue	O
act	O
combinations	O
where	O
(	O
K	O
+	O
1	O
)	O
th	O
utterance	O
was	O
ranked	O
1	O
.	O
Note	O
that	O
in	O
the	O
test	O
data	O
,	O
(	O
K	O
+	O
1	O
)	O
th	O
utterance	O
is	O
the	O
true	O
candidate	O
response	O
and	O
always	O
have	O
the	O
rank	O
1	O
.	O
For	O
instance	O
,	O
there	O
are	O
742	O
utterance	O
pairs	O
in	O
the	O
test	O
data	O
,	O
where	O
K	O
th	O
and	O
(	O
K	O
+	O
1	O
)	O
th	O
utterances	O
have	O
dialogue	O
acts	O
Q	O
and	O
I	O
,	O
respectively	O
,	O
however	O
,	O
out	O
of	O
those	O
742	O
instances	O
,	O
HSiamese	O
ranked	O
only	O
605	O
as	O
1	O
while	O
HSiamese	O
-	O
DA	O
ranked	O
638	O
as	O
1	O
.	O
From	O
these	O
tables	O
,	O
we	O
draw	O
following	O
observations	O
.	O
Models	O
Learn	O
Dominant	O
Patterns	O
:	O
The	O
first	O
is	O
that	O
there	O
are	O
certain	O
dominant	O
communication	O
patterns	O
that	O
we	O
observe	O
in	O
both	O
,	O
test	O
data	O
and	O
model	O
outputs	O
(	O
See	O
Table	O
4	O
)	O
,	O
suggesting	O
that	O
models	O
are	O
able	O
to	O
learn	O
these	O
patterns	O
and	O
retain	O
them	O
in	O
their	O
outputs	O
.	O
We	O
observe	O
that	O
a	O
Question	O
is	O
often	O
followed	O
by	O
an	O
Information	O
,	O
whereas	O
an	O
Information	O
can	O
be	O
followed	O
by	O
another	O
Information	O
or	O
a	O
Question	O
.	O
A	O
Directive	O
tends	O
to	O
be	O
followed	O
by	O
Commissive	O
.	O
These	O
communication	O
patterns	O
not	O
only	O
make	O
sense	O
intuitively	O
but	O
they	O
are	O
also	O
in	O
agreement	O
with	O
previous	O
studies	O
(	O
Li	O
et	O
al	O
,	O
2017b	O
;	O
Ribeiro	O
et	O
al	O
,	O
2015	O
)	O
.	O
Dialogue	O
Acts	O
Bring	O
Uniformity	O
:	O
The	O
second	O
and	O
a	O
rather	O
more	O
important	O
observation	O
is	O
that	O
dialogue	O
acts	O
help	O
the	O
most	O
for	O
the	O
dialogue	O
act	O
class	O
(	O
DA	O
-	O
class	O
)	O
when	O
the	O
utterances	O
belonging	O
to	O
that	O
class	O
are	O
non	O
-	O
uniform	O
in	O
their	O
linguistic	O
construct	O
.	O
In	O
order	O
to	O
better	O
exlpain	O
this	O
,	O
we	O
first	O
compute	O
the	O
break	O
-	O
up	O
of	O
recall@1	B-MetricName
according	O
to	O
the	O
dialogue	O
act	O
classes	O
.	O
A	O
DA	O
-	O
class	O
of	O
a	O
conversation	O
in	O
the	O
test	O
data	O
is	O
defined	O
based	O
on	O
the	O
dialogue	O
act	O
of	O
the	O
last	O
utterance	O
(	O
K	O
th	O
utterance	O
)	O
in	O
the	O
context	O
.	O
These	O
numbers	O
are	O
shown	O
in	O
the	O
last	O
column	O
of	O
Tables	O
4	O
(	O
b	O
)	O
and	O
4	O
(	O
c	O
)	O
for	O
the	O
respective	O
models	O
.	O
In	O
Table	O
4	O
(	O
b	O
)	O
,	O
first	O
row	O
in	O
recall@1	B-MetricName
column	O
is	O
0.65	O
,	O
which	O
indicates	O
that	O
out	O
of	O
the	O
total	O
number	O
of	O
test	O
conversations	O
where	O
dialogue	O
act	O
of	O
the	O
last	O
utterance	O
of	O
context	O
was	O
I	O
,	O
65	O
%	O
of	O
true	O
candidate	O
responses	O
were	O
ranked	O
1	O
by	O
the	O
HSiamese	O
model	O
.	O
Such	O
a	O
DA	O
-	O
class	O
wise	O
breakup	O
of	O
the	O
recall@1	B-MetricName
numbers	O
helps	O
us	O
do	O
an	O
analysis	O
with	O
respect	O
to	O
individual	O
DA	O
-	O
classes	O
.	O
From	O
this	O
break	O
-	O
up	O
,	O
it	O
is	O
clear	O
that	O
for	O
the	O
HSiamese	O
model	O
,	O
Question	O
DA	O
-	O
class	O
has	O
the	O
best	O
performance	O
of	O
78	O
%	O
whereas	O
Directive	O
has	O
the	O
worst	O
performance	O
of	O
63	O
%	O
.	O
This	O
difference	O
can	O
be	O
attributed	O
to	O
the	O
fact	O
that	O
all	O
utterances	O
with	O
dialogue	O
act	O
as	O
Question	O
have	O
rather	O
uniform	O
construct	O
.	O
Some	O
examples	O
of	O
Question	O
utterances	O
are	O
,	O
'	O
Q	O
:	O
Do	O
you	O
have	O
a	O
fever	O
?	O
'	O
and	O
'	O
Q	O
:	O
Why	O
do	O
you	O
want	O
to	O
work	O
for	O
our	O
company	O
?	O
'	O
,	O
while	O
the	O
examples	O
of	O
Directive	O
utterances	O
are	O
,	O
'	O
D	O
:	O
when	O
we	O
have	O
the	O
final	O
results	O
,	O
we	O
will	O
call	O
you	O
.	O
'	O
and	O
'	O
D	O
:	O
we	O
will	O
take	O
the	O
trip	O
.	O
could	O
you	O
give	O
us	O
a	O
pamphlet	O
?	O
'	O
.	O
From	O
these	O
examples	O
,	O
we	O
observe	O
that	O
utterances	O
belonging	O
to	O
DA	O
-	O
class	O
Question	O
have	O
rather	O
uniform	O
construct	O
in	O
terms	O
of	O
linguistic	O
features	O
,	O
whereas	O
utterances	O
belonging	O
to	O
DA	O
-	O
class	O
Directive	O
are	O
ambiguous	O
-	O
some	O
of	O
the	O
utterances	O
of	O
type	O
Directive	O
can	O
be	O
easily	O
confused	O
for	O
Question	O
.	O
This	O
uniformity	O
makes	O
the	O
learning	O
task	O
easier	O
for	O
Question	O
class	O
,	O
and	O
thereby	O
giving	O
us	O
better	O
results	O
in	O
the	O
next	O
utterance	O
selection	O
task	O
,	O
even	O
for	O
the	O
model	O
that	O
does	O
not	O
use	O
the	O
dialogue	O
act	O
information	O
.	O
This	O
performance	O
difference	O
reduces	O
when	O
we	O
provide	O
the	O
dialogue	O
act	O
information	O
along	O
with	O
the	O
textual	O
content	O
(	O
See	O
5	O
,	O
we	O
show	O
the	O
relative	O
improvement	O
of	O
HSiamese	O
-	O
DA	O
model	O
over	O
HSiamese	O
.	O
From	O
this	O
table	O
,	O
we	O
observe	O
that	O
there	O
are	O
a	O
total	O
of	O
228	O
conversations	O
where	O
the	O
proposed	O
model	O
was	O
able	O
to	O
improve	O
the	O
ranking	O
of	O
true	O
candidate	O
response	O
to	O
1	O
.	O
We	O
further	O
observe	O
that	O
the	O
biggest	O
improvement	O
is	O
in	O
I	O
I	O
,	O
I	O
Q	O
,	O
Q	O
I	O
,	O
and	O
D	O
C	O
,	O
which	O
make	O
sense	O
intuitively	O
.	O
These	O
are	O
dominant	O
patterns	O
observed	O
in	O
the	O
training	O
data	O
which	O
should	O
be	O
preserved	O
in	O
the	O
model	O
output	O
as	O
well	O
,	O
however	O
these	O
patterns	O
will	O
only	O
be	O
preserved	O
when	O
model	O
is	O
able	O
to	O
capture	O
the	O
correct	O
dialogue	O
act	O
information	O
.	O
Since	O
in	O
many	O
cases	O
D	O
and	O
Q	O
have	O
similar	O
construct	O
,	O
without	O
explicit	O
dialogue	O
act	O
information	O
,	O
a	O
model	O
may	O
get	O
confused	O
and	O
may	O
learn	O
patterns	O
not	O
observed	O
in	O
the	O
training	O
data	O
.	O
For	O
example	O
,	O
Q	O
I	O
and	O
D	O
C	O
are	O
the	O
dominant	O
and	O
right	O
patterns	O
in	O
the	O
training	O
data	O
,	O
however	O
in	O
the	O
absence	O
of	O
explicit	O
dialogue	O
act	O
information	O
,	O
the	O
model	O
may	O
get	O
confused	O
between	O
D	O
and	O
Q	O
and	O
may	O
learn	O
D	O
I	O
and	O
Q	O
C	O
instead	O
of	O
the	O
dominant	O
patterns	O
i.e.	O
Q	O
I	O
and	O
D	O
C.	O
With	O
the	O
explicit	O
dialogue	O
act	O
information	O
,	O
this	O
ambiguity	O
is	O
alleviated	O
and	O
model	O
learns	O
the	O
right	O
patterns	O
as	O
demonstrated	O
by	O
Table	O
5	O
.	O
Similar	O
observations	O
are	O
true	O
for	O
other	O
two	O
constructs	O
,	O
i.e.	O
Information	O
and	O
Commisive	O
.	O
Both	O
are	O
rather	O
similar	O
in	O
construct	O
,	O
'	O
I	O
:	O
No	O
,	O
thank	O
you	O
'	O
,	O
'	O
I	O
:	O
It	O
does	O
n't	O
matter	O
.	O
it	O
happens	O
to	O
everyone	O
.	O
'	O
and	O
'	O
C	O
:	O
I	O
knew	O
you	O
'd	O
see	O
it	O
my	O
way	O
.	O
'	O
,	O
'	O
C	O
:	O
Ok	O
,	O
i	O
am	O
ready	O
to	O
think	O
of	O
other	O
things	O
.	O
'	O
,	O
and	O
there	O
is	O
no	O
obvious	O
distinguishing	O
factor	O
.	O
However	O
,	O
providing	O
explicit	O
DA	O
information	O
helps	O
in	O
disambiguation	O
,	O
and	O
learn	O
the	O
patterns	O
that	O
are	O
observed	O
in	O
the	O
training	O
data	O
such	O
as	O
I	O
I	O
,	O
I	O
Q.	O

For	O
the	O
task	O
of	O
next	O
utterance	O
selection	O
,	O
we	O
show	O
that	O
dialogue	O
acts	O
helps	O
achieve	O
better	O
performance	O
irrespective	O
of	O
the	O
underlying	O
model	O
,	O
be	O
it	O
generative	O
or	O
discriminative	O
.	O
We	O
also	O
propose	O
a	O
novel	O
discriminative	O
model	O
that	O
leverages	O
the	O
hierarchical	O
structure	O
in	O
a	O
conversation	O
and	O
dialogue	O
act	O
information	O
to	O
produce	O
much	O
improved	O
results	O
,	O
an	O
MRR	B-MetricName
of	O
0.848	O
.	O
Our	O
results	O
not	O
only	O
show	O
the	O
improvement	O
in	O
performance	O
,	O
but	O
we	O
also	O
present	O
key	O
reasons	O
for	O
it	O
by	O
doing	O
a	O
detailed	O
analysis	O
and	O
drawing	O
key	O
insights	O
that	O
the	O
inclusion	O
of	O
dialogue	O
act	O
information	O
induces	O
uniformity	O
and	O
removes	O
ambiguity	O
.	O

The	O
Arabic	O
dialects	O
have	O
a	O
common	O
written	O
form	O
and	O
unified	O
literary	O
tradition	O
,	O
so	O
it	O
seems	O
most	O
logical	O
to	O
distinguish	O
dialects	O
on	O
the	O
basis	O
of	O
acoustics	O
,	O
and	O
there	O
is	O
a	O
fair	O
amount	O
of	O
work	O
there	O
,	O
including	O
Hanani	O
et	O
al	O
(	O
2013Hanani	O
et	O
al	O
(	O
,	O
2015	O
;	O
.	O
-	O
-	O
Biadsy	O
et	O
al	O
(	O
2009	O
)	O
distinguish	O
four	O
Arabic	O
dialects	O
and	O
MSA	O
based	O
on	O
(	O
audio	O
)	O
phone	O
sequences	O
;	O
the	O
phones	O
were	O
obtained	O
by	O
phone	O
recognizers	O
for	O
English	O
,	O
German	O
,	O
Japanese	O
,	O
Hindi	O
,	O
Mandarin	O
,	O
Spanish	O
,	O
and	O
three	O
different	O
MSA	O
phone	O
-	O
recognizer	O
implementations	O
.	O
The	O
dialects	O
were	O
distinguished	O
by	O
phoneme	O
sequences	O
,	O
and	O
the	O
results	O
of	O
classifications	O
based	O
on	O
each	O
phonerecognizer	O
were	O
combined	O
using	O
a	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
.	O
They	O
train	O
on	O
150	O
hours	O
per	O
dialect	O
of	O
telephone	O
recordings	O
.	O
They	O
report	O
61	O
%	O
accuracy	B-MetricName
on	O
5	O
-	O
second	O
segments	O
,	O
and	O
84	O
%	O
accuracy	B-MetricName
on	O
120	O
second	O
segments	O
.	O
Zaidan	O
and	O
Callison	O
-	O
Burch	O
(	O
2011	O
)	O
describe	O
building	O
a	O
text	O
corpus	O
,	O
based	O
on	O
reader	O
commen	O
-	O
tary	O
on	O
newspaper	O
websites	O
,	O
with	O
significant	O
dialect	O
content	O
;	O
the	O
goal	O
is	O
to	O
provide	O
a	O
corpus	O
to	O
improve	O
machine	B-TaskName
translation	I-TaskName
for	O
Arabic	O
dialects	O
.	O
They	O
used	O
Amazon	O
Mechanical	O
Turk	O
to	O
provide	O
annotation	O
for	O
a	O
portion	O
of	O
the	O
corpus	O
.	O
Zaidan	O
and	O
Callison	O
-	O
Burch	O
(	O
2014	O
)	O
describe	O
the	O
same	O
work	O
in	O
greater	O
detail	O
,	O
including	O
dialect	O
classifiers	O
they	O
built	O
using	O
the	O
Mechanical	O
Turk	O
data	O
for	O
classes	O
and	O
origin	O
metadata	O
as	O
additional	O
features	O
.	O
They	O
say	O
these	O
classifiers	O
are	O
'	O
approaching	O
human	O
quality	O
.	O
'	O
ElFardy	O
and	O
Diab	O
(	O
2013	O
)	O
classify	O
EGY	O
3	O
and	O
MSA	O
sentences	O
from	O
the	O
Zaidan	O
and	O
Callison	O
-	O
Burch	O
(	O
2011	O
)	O
corpus	O
,	O
that	O
is	O
,	O
from	O
text	O
.	O
Not	O
only	O
is	O
this	O
a	O
binary	O
task	O
,	O
but	O
orthographic	O
hints	O
,	O
including	O
repeated	O
long	O
vowels	O
,	O
emojis	O
and	O
multiple	O
punctuations	O
,	O
give	O
strong	O
clues	O
of	O
the	O
register	O
,	O
and	O
hence	O
whether	O
MSA	O
is	O
being	O
employed	O
.	O
They	O
do	O
a	O
number	O
of	O
experiments	O
comparing	O
various	O
preprocessing	O
schemes	O
and	O
different	O
training	O
sizes	O
,	O
ranging	O
from	O
2	O
-	O
28	O
million	O
tokens	O
.	O
They	O
achieve	O
80	O
%	O
-	O
86	O
%	O
accuracy	B-MetricName
for	O
all	O
of	O
their	O
attempts	O
.	O
Malmasi	O
et	O
al	O
(	O
2015	O
)	O
do	O
Arabic	O
dialect	B-TaskName
identification	I-TaskName
from	O
text	O
corpora	O
,	O
including	O
the	O
Multi	O
-	O
Dialect	O
Parallel	O
Corpus	O
of	O
Arabic	O
(	O
Bouamor	O
et	O
al	O
,	O
2014	O
)	O
and	O
the	O
Arabic	O
Online	O
Commentary	O
database	O
(	O
Zaidan	O
and	O
Callison	O
-	O
Burch	O
,	O
2011	O
)	O
.	O
Hanani	O
et	O
al	O
(	O
2015	O
)	O
perform	O
recognition	O
of	O
several	O
Palestinian	O
regional	O
accents	O
,	O
evaluating	O
four	O
different	O
acoustic	O
models	O
,	O
achieving	O
81.5	O
%	O
accuracy	B-MetricName
for	O
their	O
best	O
system	O
,	O
an	O
I	O
-	O
vector	O
framework	O
with	O
64	O
Gaussian	O
components	O
.	O
developed	O
the	O
corpus	O
on	O
which	O
the	O
DSL	O
Arabic	O
shared	O
task	O
is	O
based	O
.	O
Their	O
own	O
dialect	O
detection	O
efforts	O
depended	O
largely	O
on	O
acoustical	O
cues	O
.	O
Arabic	O
dialect	O
recognition	O
appeared	O
in	O
the	O
2016	O
edition	O
of	O
the	O
VarDial	O
workshop	O
's	O
shared	O
task	O
(	O
Malmasi	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
shared	O
task	O
data	O
was	O
text	O
-	O
only	O
.	O
The	O
best	O
classifiers	O
(	O
Malmasi	O
et	O
al	O
,	O
2016	O
;	O
Ionescu	O
and	O
Popescu	O
,	O
2016	O
)	O
for	O
the	O
shared	O
task	O
performed	O
far	O
below	O
the	O
best	O
results	O
reported	O
by	O
some	O
of	O
the	O
preceding	O
researchers	O
,	O
in	O
particular	O
which	O
used	O
some	O
of	O
the	O
same	O
data	O
.	O
Part	O
of	O
the	O
reason	O
must	O
be	O
that	O
the	O
amount	O
of	O
training	O
data	O
for	O
the	O
workshop	O
is	O
much	O
smaller	O
than	O
that	O
used	O
by	O
some	O
of	O
the	O
other	O
researchers	O
;	O
the	O
workshop	O
data	O
also	O
did	O
not	O
include	O
the	O
audio	O
recordings	O
on	O
which	O
the	O
transcripts	O
are	O
based	O
.	O
3	O

We	O
experimented	O
with	O
several	O
neural	O
networks	O
.	O
Our	O
model	O
for	O
the	O
S	O
-	O
1	O
submission	O
uses	O
as	O
input	O
26	O
features	O
which	O
correspond	O
to	O
one	O
of	O
our	O
26	O
pretrained	O
dialect	O
language	O
models	O
.	O
Each	O
feature	O
represents	O
the	O
probability	O
of	O
a	O
given	O
sentence	O
for	O
one	O
language	O
model	O
.	O
The	O
probability	O
scores	O
measure	O
how	O
close	O
each	O
sentence	O
is	O
to	O
the	O
dialect	O
.	O
We	O
train	O
Multilayer	O
Perceptron	O
(	O
MLP	B-DatasetName
)	O
with	O
one	O
hidden	O
(	O
dense	O
)	O
layer	O
with	O
400	O
units	O
.	O
The	O
output	O
of	O
the	O
hidden	O
layer	O
is	O
passed	O
to	O
a	O
final	O
fullyconnected	O
softmax	B-MethodName
layer	O
.	O
The	O
output	O
of	O
the	O
softmax	B-MethodName
layer	O
is	O
a	O
probability	O
distribution	O
over	O
all	O
26	O
classes	O
.	O
The	O
class	O
with	O
the	O
highest	O
probability	O
is	O
predicted	O
as	O
a	O
final	O
output	O
of	O
our	O
model	O
.	O
As	O
an	O
activation	B-HyperparameterName
function	I-HyperparameterName
in	O
the	O
hidden	O
layer	O
of	O
the	O
MLP	B-DatasetName
a	O
Rectified	O
Linear	O
Unit	O
(	O
ReLu	B-MethodName
)	O
is	O
employed	O
.	O
We	O
also	O
tried	O
to	O
combine	O
character	O
n	O
-	O
gram	O
features	O
with	O
the	O
language	O
model	O
features	O
.	O
The	O
input	O
is	O
a	O
sequence	O
of	O
first	O
200	O
character	O
n	O
-	O
grams	O
of	O
a	O
given	O
text	O
.	O
Each	O
sequence	O
of	O
character	O
n	O
-	O
grams	O
is	O
used	O
as	O
a	O
separate	O
input	O
followed	O
by	O
a	O
randomly	O
initialized	O
embedding	O
layer	O
and	O
then	O
two	O
layers	O
of	O
Bidirectional	B-MethodName
LSTM	I-MethodName
(	O
BiLSTM	B-MethodName
)	O
(	O
Graves	O
and	O
Schmidhuber	O
,	O
2005	O
)	O
with	O
64	O
units	O
are	O
employed	O
(	O
see	O
Figure	O
1	O
)	O
.	O
The	O
output	O
vector	O
of	O
the	O
BiLSTM	B-MethodName
layers	O
is	O
concatenated	O
with	O
the	O
language	O
model	O
features	O
and	O
this	O
concatenated	O
vector	O
is	O
passed	O
to	O
the	O
MLP	B-DatasetName
layer	O
with	O
400	O
units	O
(	O
the	O
same	O
as	O
described	O
above	O
)	O
.	O
All	O
models	O
were	O
implemented	O
by	O
using	O
Keras	O
(	O
Chollet	O
et	O
al	O
,	O
2015	O
)	O
with	O
TensorFlow	O
backend	O
(	O
Abadi	O
et	O
al	O
,	O
2015	O
)	O

We	O
tune	O
all	O
hyperparameters	O
on	O
the	O
development	O
data	O
.	O
We	O
train	O
our	O
model	O
with	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
optimizer	B-HyperparameterName
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.01	O
and	O
without	O
any	O
dropout	O
.	O
The	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
is	O
800	O
and	O
we	O
do	O
not	O
use	O
mini	O
-	O
batches	O
or	O
dropout	O
regularization	O
technique	O
.	O
The	O
model	O
with	O
these	O
hyperparameters	O
achieves	O
the	O
best	O
result	O
(	O
0.661	O
macro	O
F	O
1	O
-	O
score	O
)	O
on	O
the	O
development	O
data	O
and	O
was	O
used	O
for	O
the	O
final	O
submission	O
.	O
We	O
also	O
experimented	O
with	O
the	O
n	O
-	O
gram	O
inputs	O
.	O
We	O
tried	O
a	O
different	O
number	O
of	O
character	O
n	O
-	O
grams	O
and	O
we	O
achieve	O
the	O
best	O
result	O
(	O
0.555	O
macro	O
F	O
1score	O
)	O
on	O
the	O
development	O
data	O
using	O
three	O
inputs	O
-	O
character	O
unigrams	O
,	O
bigrams	O
and	O
trigrams	O
,	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.005	O
,	O
mini	O
-	O
batches	O
of	O
size	O
256	O
for	O
11	O
epochs	O
and	O
with	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
.	O

Our	O
tortuous	O
classifier	O
did	O
less	O
well	O
on	O
the	O
tweet	O
data	O
,	O
so	O
we	O
used	O
a	O
simpler	O
classifier	O
.	O
The	O
features	O
are	O
the	O
kenlm	O
language	O
model	O
scores	O
for	O
the	O
21	O
countries	O
,	O
computed	O
for	O
each	O
of	O
the	O
training	O
tweets	O
,	O
then	O
exponentiated	O
and	O
normalized	O
to	O
sum	O
to	O
1	O
.	O
The	O
tweets	O
are	O
classified	O
using	O
y_test	O
=	O
KNeighborsClassifier	O
(	O
n_neighbors=31	O
)	O
.fit	O
(	O
X_train	O
,	O
y_train	O
)	O
.predict	O
(	O
X_test	O
)	O
The	O
users	O
are	O
predicted	O
based	O
on	O
the	O
plurality	O
prediction	O
for	O
all	O
of	O
their	O
tweets	O
,	O
that	O
is	O
,	O
the	O
country	O
to	O
which	O
the	O
largest	O
number	O
of	O
their	O
tweets	O
were	O
assigned	O
.	O
There	O
were	O
a	O
significant	O
number	O
of	O
tweets	O
unavailable	O
,	O
about	O
10	O
%	O
in	O
the	O
training	O
and	O
development	O
sets	O
,	O
and	O
12	O
%	O
in	O
the	O
test	O
set	O
.	O
After	O
the	O
submissions	O
had	O
closed	O
we	O
experimented	O
with	O
eliminating	O
the	O
unavailable	O
and	O
non	O
-	O
Arabic	O
tweets	O
from	O
True	O
labels	O
127	O
0	B-DatasetName
0	B-DatasetName
12	O
3	O
4	O
1	O
7	O
0	B-DatasetName
0	B-DatasetName
30	O
1	O
0	B-DatasetName
0	B-DatasetName
8	O
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
3	O
1	O
0	B-DatasetName
1	O
1	O
0	B-DatasetName
160	O
1	O
1	O
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
8	O
0	B-DatasetName
0	B-DatasetName
1	O
7	O
1	O
1	O
4	O
1	O
4	O
2	O
2	O
2	O
1	O
0	B-DatasetName
1	O
1	O
1	O
0	B-DatasetName
3	O
149	O
4	O
16	O
1	O
0	B-DatasetName
0	B-DatasetName
1	O
10	O
1	O
1	O
1	O
2	O
1	O
8	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
2	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
9	O
0	B-DatasetName
2	O
118	O
2	O
1	O
0	B-DatasetName
6	O
2	O
8	O
9	O
7	O
0	B-DatasetName
2	O
18	O
3	O
0	B-DatasetName
0	B-DatasetName
2	O
0	B-DatasetName
2	O
7	O
1	O
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
21	O
4	O
134	O
1	O
1	O
0	B-DatasetName
0	B-DatasetName
19	O
1	O
4	O
1	O
3	O
4	O
2	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
3	O
1	O
0	B-DatasetName
0	B-DatasetName
1	O
0	B-DatasetName
2	O
0	B-DatasetName
0	B-DatasetName
5	O
2	O
127	O
25	O
1	O
3	O
1	O
1	O
3	O
0	B-DatasetName
1	O
0	B-DatasetName
4	O
3	O
3	O
1	O
0	B-DatasetName
9	O
2	O
5	O
2	O
0	B-DatasetName
0	B-DatasetName
1	O
1	O
1	O
2	O
1	O
29	O
128	O
1	O
2	O
0	B-DatasetName
1	O
6	O
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
8	O
2	O
3	O
0	B-DatasetName
5	O
2	O
3	O
2	O
1	O
0	B-DatasetName
8	O
1	O
0	B-DatasetName
7	O
2	O
1	O
1	O
129	O
2	O
3	O
18	O
0	B-DatasetName
0	B-DatasetName
1	O
9	O
3	O
0	B-DatasetName
1	O
2	O
1	O
3	O
5	O
0	B-DatasetName
0	B-DatasetName
2	O
1	O
2	O
5	O
0	B-DatasetName
6	O
9	O
3	O
1	O
1	O
131	O
2	O
1	O
8	O
0	B-DatasetName
4	O
2	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
2	O
0	B-DatasetName
6	O
1	O
4	O
1	O
11	O
0	B-DatasetName
6	O
4	O
19	O
3	O
37	O
2	O
0	B-DatasetName
0	B-DatasetName
2	O
100	O
2	O
1	O
1	O
5	O
1	O
9	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
3	O
0	B-DatasetName
1	O
0	B-DatasetName
2	O
2	O
16	O
0	B-DatasetName
1	O
16	O
4	O
4	O
2	O
17	O
2	O
3	O
108	O
4	O
1	O
2	O
10	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
8	O
1	O
0	B-DatasetName
1	O
0	B-DatasetName
1	O
1	O
0	B-DatasetName
4	O
2	O
2	O
3	O
1	O
1	O
1	O
0	B-DatasetName
150	O
1	O
8	O
4	O
5	O
0	B-DatasetName
1	O
3	O
1	O
4	O
2	O
1	O
1	O
2	O
1	O
0	B-DatasetName
7	O
1	O
1	O
2	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
2	O
1	O
4	O
136	O
3	O
0	B-DatasetName
2	O
0	B-DatasetName
1	O
2	O
31	O
1	O
1	O
1	O
3	O
0	B-DatasetName
1	O
1	O
1	O
1	O
6	O
4	O
0	B-DatasetName
0	B-DatasetName
2	O
5	O
3	O
3	O
14	O
1	O
123	O
1	O
6	O
0	B-DatasetName
1	O
3	O
0	B-DatasetName
10	O
5	O
5	O
1	O
3	O
1	O
7	O
2	O
1	O
26	O
2	O
1	O
2	O
7	O
5	O
1	O
1	O
6	O
0	B-DatasetName
2	O
121	O
2	O
0	B-DatasetName
0	B-DatasetName
1	O
0	B-DatasetName
2	O
9	O
1	O
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
2	O
3	O
8	O
10	O
1	O
1	O
1	O
0	B-DatasetName
1	O
1	O
3	O
0	B-DatasetName
4	O
2	O
147	O
1	O
12	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
1	O
0	B-DatasetName
2	O
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
3	O
8	O
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
167	O
1	O
1	O
0	B-DatasetName
6	O
4	O
5	O
0	B-DatasetName
1	O
1	O
1	O
7	O
2	O
2	O
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
2	O
1	O
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
1	O
9	O
1	O
158	O
4	O
0	B-DatasetName
7	O
1	O
1	O
0	B-DatasetName
1	O
0	B-DatasetName
2	O
3	O
1	O
3	O
1	O
3	O
1	O
1	O
5	O
1	O
3	O
15	O
1	O
8	O
1	O
10	O
0	B-DatasetName
32	O
83	O
1	O
16	O
4	O
4	O
0	B-DatasetName
1	O
0	B-DatasetName
1	O
7	O
0	B-DatasetName
3	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
2	O
0	B-DatasetName
1	O
1	O
36	O
1	O
2	O
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
139	O
0	B-DatasetName
1	O
0	B-DatasetName
3	O
1	O
1	O
2	O
0	B-DatasetName
0	B-DatasetName
3	O
0	B-DatasetName
2	O
2	O
3	O
6	O
5	O
0	B-DatasetName
13	O
1	O
18	O
1	O
1	O
2	O
9	O
7	O
0	B-DatasetName
115	O
2	O
7	O
0	B-DatasetName
1	O
0	B-DatasetName
3	O
3	O
1	O
12	O
3	O
2	O
2	O
5	O
3	O
2	O
7	O
10	O
0	B-DatasetName
5	O
12	O
2	O
1	O
3	O
2	O
0	B-DatasetName
7	O
109	O
4	O
1	O
1	O
0	B-DatasetName
1	O
1	O
1	O
2	O
1	O
6	O
2	O
1	O
3	O
1	O
1	O
7	O
0	B-DatasetName
9	O
1	O
3	O
4	O
3	O
1	O
0	B-DatasetName
6	O
4	O
140	O
0	B-DatasetName
2	O
0	B-DatasetName
2	O
6	O
0	B-DatasetName
1	O
1	O
2	O
3	O
0	B-DatasetName
3	O
1	O
2	O
0	B-DatasetName
2	O
0	B-DatasetName
2	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
2	O
1	O
3	O
1	O
0	B-DatasetName
146	O
0	B-DatasetName
22	O
1	O
3	O
0	B-DatasetName
2	O
3	O
0	B-DatasetName
0	B-DatasetName
2	O
9	O
2	O
0	B-DatasetName
4	O
0	B-DatasetName
2	O
2	O
4	O
0	B-DatasetName
0	B-DatasetName
1	O
1	O
4	O
1	O
1	O
4	O
151	O
3	O
1	O
training	O
and	O
testing	O
and	O
choosing	O
Saudi	O
Arabia	O
(	O
which	O
is	O
the	O
origin	O
for	O
the	O
plurality	O
of	O
tweets	O
at	O
36	O
%	O
)	O
for	O
users	O
with	O
no	O
remaining	O
tweets	O
.	O
This	O
improved	O
tweet	O
classification	O
accuracy	B-MetricName
by	O
about	O
5	O
%	O
,	O
but	O
actually	O
decreased	O
user	O
classification	O
accuracy	B-MetricName
on	O
the	O
development	O
set	O
.	O

State	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
machine	B-TaskName
translation	I-TaskName
methods	O
employ	O
massive	O
amounts	O
of	O
parameters	O
.	O
Drastically	O
reducing	O
computational	O
costs	O
of	O
such	O
methods	O
without	O
affecting	O
performance	O
has	O
been	O
up	O
to	O
this	O
point	O
unsuccessful	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
FullyQT	O
:	O
an	O
allinclusive	O
quantization	B-TaskName
strategy	O
for	O
the	O
Transformer	B-MethodName
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
show	O
that	O
it	O
is	O
possible	O
to	O
avoid	O
any	O
loss	B-MetricName
in	O
translation	O
quality	O
with	O
a	O
fully	O
quantized	O
Transformer	B-MethodName
.	O
Indeed	O
,	O
compared	O
to	O
fullprecision	O
,	O
our	O
8	O
-	O
bit	O
models	O
score	O
greater	O
or	O
equal	O
BLEU	B-MetricName
on	O
most	O
tasks	O
.	O
Comparing	O
ourselves	O
to	O
all	O
previously	O
proposed	O
methods	O
,	O
we	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
quantization	B-TaskName
results	O
.	O

The	O
idea	O
of	O
using	O
neural	O
networks	O
for	O
machine	B-TaskName
translation	I-TaskName
was	O
only	O
recently	O
proposed	O
(	O
Kalchbrenner	O
and	O
Blunsom	O
,	O
2013	O
;	O
Sutskever	O
et	O
al	O
,	O
2014	O
;	O
.	O
Nonetheless	O
,	O
the	O
approach	O
became	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
the	O
field	O
(	O
Ahmed	O
et	O
al	O
,	O
2017	O
;	O
.	O
A	O
key	O
element	O
of	O
this	O
success	O
was	O
to	O
allow	O
the	O
decoder	O
to	O
attend	O
to	O
all	O
hidden	O
states	O
of	O
the	O
encoder	O
.	O
A	O
few	O
variations	O
to	O
this	O
additive	B-MethodName
attention	I-MethodName
mechanism	O
have	O
been	O
proposed	O
,	O
such	O
as	O
multiplicative	O
and	O
self	O
-	O
attention	O
(	O
Luong	O
et	O
al	O
,	O
2015	O
;	O
Cheng	O
et	O
al	O
,	O
2016	O
;	O
Lin	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
latter	O
formed	O
the	O
basis	O
of	O
the	O
Transformer	B-MethodName
network	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
machine	B-TaskName
translation	I-TaskName
.	O
Inspiring	O
a	O
new	O
wave	O
of	O
work	O
,	O
numerous	O
natural	O
language	O
processing	O
tasks	O
reached	O
new	O
heights	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
.	O
Unfortunately	O
,	O
these	O
models	O
use	O
an	O
enormous	O
amount	O
of	O
parameters	O
.	O
Inference	O
on	O
resource	O
-	O
limited	O
hardware	O
such	O
as	O
edge	O
-	O
devices	O
is	O
thus	O
impractical	O
.	O
A	O
solution	O
to	O
reduce	O
the	O
computational	O
burden	O
of	O
these	O
networks	O
is	O
to	O
lower	O
numerical	O
precision	O
.	O
Consequently	O
,	O
numerical	O
values	O
can	O
be	O
represented	O
using	O
fewer	O
bits	O
(	O
Tang	O
and	O
Kwan	O
,	O
1993	O
;	O
Marchesi	O
et	O
al	O
,	O
1993	O
)	O
.	O
This	O
method	O
called	O
quantization	B-TaskName
has	O
the	O
advantage	O
of	O
providing	O
good	O
compression	O
rates	O
with	O
minimal	O
loss	B-MetricName
in	O
accuracy	B-MetricName
.	O
It	O
is	O
also	O
conveniently	O
supported	O
by	O
most	O
hardware	O
.	O
Properly	O
quantizing	O
the	O
Transformer	B-MethodName
would	O
allow	O
computational	O
speed	O
gains	O
at	O
inference	O
,	O
as	O
well	O
as	O
deployment	O
on	O
more	O
constrained	O
devices	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
quantization	B-TaskName
-	O
aware	O
training	O
strategy	O
for	O
the	O
entire	O
Transformer	B-MethodName
architecture	O
.	O
Our	O
method	O
is	O
easy	O
to	O
implement	O
and	O
results	O
are	O
consistent	O
with	O
the	O
full	O
-	O
precision	O
Transformer	B-MethodName
.	O
We	O
test	O
our	O
approach	O
on	O
multiple	O
translation	O
tasks	O
such	O
as	O
WMT14	B-DatasetName
EN	O
-	O
FR	O
and	O
WMT14	B-DatasetName
EN	O
-	O
DE	O
and	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
quantization	B-TaskName
results	O
.	O
In	O
comparison	O
with	O
full	O
-	O
precision	O
,	O
our	O
quantized	O
models	O
score	O
equal	O
or	O
higher	O
BLEU	B-MetricName
on	O
most	O
tasks	O
.	O
We	O
are	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
first	O
to	O
show	O
that	O
the	O
Transformer	B-MethodName
architecture	O
can	O
be	O
fully	O
quantized	O
without	O
impairing	O
translation	O
quality	O
.	O
We	O
also	O
perform	O
an	O
ablation	O
study	O
and	O
show	O
that	O
quantizing	O
specific	O
components	O
of	O
the	O
Transformer	B-MethodName
improves	O
BLEU	B-MetricName
score	I-MetricName
.	O

Over	O
the	O
years	O
,	O
a	O
large	O
range	O
of	O
methods	O
have	O
been	O
proposed	O
to	O
quantize	O
neural	O
networks	O
.	O
These	O
include	O
,	O
among	O
many	O
others	O
,	O
binary	O
,	O
ternary	O
(	O
Lin	O
et	O
al	O
,	O
2015	O
;	O
,	O
uniform	O
(	O
Jacob	O
et	O
al	O
,	O
2017	O
)	O
and	O
learned	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
quantization	B-TaskName
.	O
These	O
methods	O
can	O
be	O
universally	O
applied	O
to	O
any	O
type	O
of	O
neural	O
network	O
.	O
To	O
maintain	O
performance	O
though	O
,	O
specific	O
architectures	O
usually	O
require	O
custom	O
tailored	O
quantization	B-TaskName
schemes	O
.	O
Several	O
recent	O
work	O
explore	O
recurrent	O
neural	O
network	O
(	O
Jordan	O
,	O
1990	O
)	O
quantization	B-TaskName
.	O
Ott	O
et	O
al	O
(	O
2016	O
)	O
propose	O
an	O
exponential	O
quantization	B-TaskName
method	O
for	O
RNN	O
weights	O
.	O
They	O
find	O
ternary	O
and	O
exponential	O
quantization	B-TaskName
to	O
work	O
well	O
on	O
language	O
modeling	O
and	O
speech	B-TaskName
recognition	I-TaskName
,	O
while	O
binary	O
weights	O
seemed	O
ineffective	O
.	O
quantize	O
weights	O
and	O
activations	O
of	O
both	O
RNNs	O
and	O
LSTMs	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
to	O
2	O
,	O
4	O
and	O
6bit	O
.	O
Meanwhile	O
,	O
propose	O
modifications	O
to	O
the	O
gates	O
and	O
interlinks	O
of	O
quantized	O
LSTM	B-MethodName
and	O
GRU	B-MethodName
cells	O
,	O
as	O
well	O
as	O
a	O
balanced	O
quantization	B-TaskName
method	O
for	O
weights	O
.	O
successfully	O
quantize	O
a	O
stacked	O
sequence	O
-	O
tosequence	O
LSTM	B-MethodName
to	O
8	O
-	O
bit	O
without	O
any	O
loss	B-MetricName
in	O
translation	O
quality	O
.	O
Most	O
recently	O
,	O
Wang	O
et	O
al	O
(	O
2018	O
)	O
propose	O
applying	O
different	O
quantization	B-TaskName
methods	O
for	O
different	O
RNN	O
components	O
.	O
With	O
regards	O
to	O
CNNs	O
(	O
LeCun	O
et	O
al	O
,	O
1989	O
)	O
,	O
various	O
works	O
have	O
also	O
explored	O
quantizing	O
these	O
models	O
.	O
Gong	O
et	O
al	O
(	O
2014	O
)	O
compare	O
matrix	O
factorization	O
,	O
binarization	B-TaskName
,	O
k	B-MethodName
-	I-MethodName
means	I-MethodName
clustering	I-MethodName
,	O
product	O
quantization	B-TaskName
and	O
residual	O
quantization	B-TaskName
of	O
CNNs	O
.	O
Wu	O
et	O
al	O
(	O
2015	O
)	O
apply	O
quantization	B-TaskName
to	O
both	O
kernels	O
and	O
fully	O
connected	O
layers	O
of	O
convolutional	O
neural	O
networks	O
.	O
Rastegari	O
et	O
al	O
(	O
2016	O
)	O
propose	O
using	O
binary	O
weighted	O
filters	O
on	O
AlexNet	B-MethodName
(	O
Krizhevsky	O
et	O
al	O
,	O
2012	O
)	O
.	O
Testing	O
their	O
method	O
on	O
ImageNet	B-DatasetName
,	O
they	O
show	O
classification	O
accuracy	B-MetricName
to	O
be	O
on	O
par	O
with	O
fullprecision	O
.	O
For	O
faster	O
inference	O
and	O
training	O
,	O
use	O
low	O
bitwidth	O
weights	O
,	O
activations	O
and	O
gradients	O
on	O
CNNs	O
.	O
Quantization	B-TaskName
has	O
been	O
applied	O
in	O
tandem	O
with	O
other	O
compression	O
methods	O
.	O
Han	O
et	O
al	O
(	O
2015	O
)	O
combine	O
pruning	O
,	O
quantization	B-TaskName
,	O
weight	O
sharing	O
and	O
Huffman	O
coding	O
.	O
In	O
another	O
line	O
of	O
work	O
,	O
Polino	O
et	O
al	O
(	O
2018	O
)	O
employ	O
quantization	B-TaskName
with	O
knowledge	B-MethodName
distillation	I-MethodName
(	O
Hinton	O
et	O
al	O
,	O
2015	O
)	O
for	O
higher	O
compression	O
rates	O
.	O
Moreover	O
,	O
Chen	O
et	O
al	O
(	O
2018	O
)	O
blend	O
quantization	B-TaskName
with	O
block	O
based	O
low	O
-	O
rank	O
matrix	O
approximation	O
of	O
embeddings	O
.	O

The	O
pruning	O
of	O
neural	O
networks	O
for	O
model	B-TaskName
compression	I-TaskName
has	O
also	O
been	O
largely	O
explored	O
.	O
LeCun	O
et	O
al	O
(	O
1990	O
)	O
were	O
the	O
first	O
to	O
propose	O
a	O
Hessian	O
based	O
method	O
to	O
prune	O
neural	O
net	O
weights	O
.	O
Hassibi	O
et	O
al	O
(	O
1994	O
)	O
later	O
improved	O
the	O
method	O
.	O
More	O
recently	O
,	O
See	O
et	O
al	O
(	O
2016	O
)	O
show	O
that	O
pruning	O
a	O
fully	O
trained	O
model	O
and	O
then	O
retraining	O
it	O
can	O
increase	O
performance	O
over	O
the	O
original	O
non	O
-	O
pruned	O
model	O
.	O
Gradually	O
pruning	O
in	O
tandem	O
with	O
training	O
has	O
also	O
been	O
shown	O
to	O
increase	O
performance	O
(	O
Zhu	O
and	O
Gupta	O
,	O
2017	O
)	O
.	O
To	O
avoid	O
sparse	O
matrices	O
,	O
prune	O
nodes	O
instead	O
of	O
weights	O
.	O
They	O
apply	O
a	O
penalty	O
in	O
the	O
loss	B-MetricName
on	O
the	O
γ	B-HyperparameterName
parameters	O
of	O
batch	B-MethodName
normalization	I-MethodName
layers	O
.	O
With	O
a	O
similar	O
objective	O
,	O
Narang	O
et	O
al	O
(	O
2017b	O
)	O
make	O
better	O
use	O
of	O
hardware	O
by	O
applying	O
pruning	O
and	O
weight	B-MethodName
decay	I-MethodName
in	O
blocks	O
to	O
minimize	O
the	O
number	O
of	O
loaded	O
weight	O
matrix	O
chunks	O
.	O
Similarly	O
to	O
quantization	B-TaskName
,	O
pruning	O
methods	O
have	O
also	O
been	O
adapted	O
to	O
specific	O
architectures	O
.	O
Liu	O
et	O
al	O
(	O
2015	O
)	O
propose	O
an	O
efficient	O
sparse	O
matrix	O
multiplication	O
algorithm	O
for	O
CNNs	O
.	O
As	O
for	O
RNNs	O
,	O
Narang	O
et	O
al	O
(	O
2017a	O
)	O
show	O
sparse	O
pruning	O
to	O
work	O
well	O
on	O
the	O
architecture	O
.	O
In	O
order	O
to	O
maintain	O
dimension	O
consistency	O
,	O
Wen	O
et	O
al	O
(	O
2017	O
)	O
propose	O
to	O
prune	O
all	O
basic	O
LSTM	B-MethodName
structures	O
concurrently	O
.	O
Lastly	O
,	O
Park	O
et	O
al	O
(	O
2018	O
)	O
introduce	O
simple	O
recurrent	O
units	O
(	O
SRUs	O
)	O
for	O
easy	O
pruning	O
of	O
RNNs	O
.	O

Our	O
quantization	B-TaskName
scheme	O
was	O
chosen	O
to	O
be	O
uniform	O
,	O
meaning	O
that	O
the	O
step	B-HyperparameterName
size	I-HyperparameterName
between	O
two	O
quantized	O
values	O
is	O
constant	O
.	O
This	O
choice	O
,	O
which	O
is	O
an	O
additional	O
constraint	O
,	O
was	O
made	O
for	O
practical	O
reasons	O
.	O
It	O
indeed	O
simplifies	O
all	O
computations	O
required	O
during	O
inference	O
,	O
enabling	O
the	O
exploitation	O
of	O
hardware	O
resources	O
more	O
efficiently	O
.	O
If	O
the	O
performance	O
with	O
uniform	O
quantization	B-TaskName
is	O
already	O
on	O
par	O
with	O
fullprecision	O
,	O
then	O
more	O
weighty	O
methods	O
are	O
unnecessary	O
.	O
A	O
brief	O
overview	O
of	O
uniform	O
quantization	B-TaskName
is	O
given	O
in	O
this	O
section	O
.	O
For	O
more	O
details	O
,	O
we	O
refer	O
the	O
reader	O
to	O
Jacob	O
et	O
al	O
(	O
2017	O
)	O
.	O
Given	O
an	O
element	O
x	O
of	O
a	O
tensor	O
X	O
,	O
we	O
apply	O
the	O
quantization	B-TaskName
function	O
Q	O
:	O
Q	O
(	O
x	O
)	O
=	O
clamp	O
(	O
x	O
;	O
x	O
min	O
,	O
xmax	O
)	O
−	O
x	O
min	O
s	O
*	O
s	O
+	O
x	O
min	O
(	O
1	O
)	O
s	O
=	O
xmax	O
−	O
x	O
min	O
2	O
k	O
−	O
1	O
(	O
2	O
)	O
where	O
x	O
min	O
and	O
x	O
max	O
defines	O
the	O
endpoints	O
of	O
the	O
quantization	B-TaskName
interval	O
.	O
When	O
quantization	B-TaskName
is	O
applied	O
to	O
weights	O
,	O
these	O
values	O
are	O
respectively	O
min	O
(	O
X	O
)	O
and	O
max	O
(	O
X	O
)	O
.	O
However	O
,	O
when	O
quantization	B-TaskName
is	O
applied	O
to	O
activations	O
,	O
those	O
values	O
are	O
running	O
estimates	O
.	O
The	O
latter	O
are	O
computed	O
during	O
training	O
,	O
where	O
for	O
every	O
forward	O
pass	O
,	O
the	O
x	O
min	O
and	O
x	O
max	O
variables	O
are	O
updated	O
via	O
an	O
exponential	O
moving	O
average	O
with	O
a	O
momentum	O
of	O
0.9	O
.	O
The	O
clamp	O
function	O
associates	O
all	O
values	O
outside	O
of	O
the	O
[	O
x	O
min	O
,	O
x	O
max	O
]	O
range	O
to	O
the	O
closest	O
endpoint	O
and	O
represents	O
rounding	O
to	O
the	O
nearest	O
integer	O
.	O
The	O
value	O
k	O
is	O
simply	O
the	O
bit	O
precision	O
.	O
For	O
example	O
,	O
in	O
the	O
context	O
of	O
8	O
-	O
bit	O
quantization	B-TaskName
,	O
k	B-HyperparameterName
=	I-HyperparameterName
8	O
.	O
During	O
backpropagation	O
,	O
we	O
use	O
the	O
straightthrough	O
estimator	O
(	O
Hinton	O
,	O
2012	O
)	O
and	O
set	O
the	O
gradients	O
of	O
clamped	O
values	O
to	O
zero	O
.	O
Once	O
training	O
is	O
finished	O
,	O
s	O
and	O
x	O
min	O
are	O
frozen	O
along	O
with	O
the	O
weights	O
.	O

We	O
choose	O
to	O
quantize	O
all	O
operations	O
which	O
can	O
provide	O
a	O
computational	O
speed	O
gain	O
at	O
inference	O
.	O
In	O
this	O
regard	O
,	O
we	O
quantize	O
all	O
matrix	O
multiplications	O
,	O
meaning	O
that	O
the	O
inputs	O
and	O
weights	O
of	O
MatMuls	O
will	O
both	O
be	O
k	O
-	O
bit	O
quantized	O
.	O
The	O
other	O
operations	O
we	O
quantize	O
are	O
divisions	O
,	O
but	O
only	O
if	O
both	O
the	O
numerator	O
and	O
denominator	O
are	O
second	O
or	O
higher	O
rank	O
tensors	O
.	O
For	O
all	O
other	O
operations	O
,	O
such	O
as	O
sums	O
,	O
the	O
computational	O
cost	O
added	O
by	O
the	O
quantization	B-TaskName
operation	O
outweighs	O
the	O
benefit	O
of	O
performing	O
the	O
operation	O
with	O
reduced	O
precision	O
.	O
Hence	O
,	O
we	O
do	O
not	O
quantize	O
such	O
operations	O
.	O
More	O
precisely	O
,	O
we	O
quantize	O
all	O
weights	O
of	O
the	O
Transformer	B-MethodName
,	O
excluding	O
biases	O
.	O
The	O
latter	O
are	O
summed	O
with	O
the	O
INT32	O
output	O
of	O
matrix	O
multiplications	O
and	O
thus	O
provide	O
no	O
additional	O
computational	O
efficiency	O
from	O
being	O
quantized	O
.	O
Furthermore	O
,	O
the	O
memory	O
space	O
of	O
biases	O
is	O
insignificant	O
in	O
comparison	O
to	O
the	O
weight	O
matrices	O
,	O
representing	O
less	O
than	O
0.1	O
%	O
of	O
total	O
weights	O
.	O
For	O
positional	O
embeddings	O
,	O
these	O
are	O
fixed	O
and	O
can	O
thus	O
be	O
quantized	O
once	O
prior	O
to	O
training	O
.	O
The	O
γ	B-HyperparameterName
weights	O
of	O
Layer	O
-	O
Norms	O
are	O
also	O
quantized	O
.	O
As	O
for	O
activations	O
,	O
we	O
quantize	O
the	O
sum	O
of	O
the	O
input	O
embeddings	O
with	O
the	O
positional	O
encodings	O
in	O
both	O
the	O
encoder	O
and	O
decoder	O
.	O
In	O
the	O
Multi	B-MethodName
-	I-MethodName
Head	I-MethodName
Attention	I-MethodName
,	O
we	O
quantize	O
the	O
(	O
Q	O
,	O
K	O
,	O
V	O
)	O
input	O
,	O
the	O
softmax	B-MethodName
's	O
numerator	O
,	O
the	O
softmax	B-MethodName
's	O
denominator	O
,	O
the	O
softmax	B-MethodName
's	O
output	O
and	O
the	O
Scaled	B-MethodName
Dot	I-MethodName
-	I-MethodName
Product	I-MethodName
Attention	I-MethodName
's	O
output	O
.	O
At	O
inference	O
,	O
the	O
softmax	B-MethodName
does	O
not	O
need	O
to	O
be	O
computed	O
in	O
full	O
-	O
precision	O
.	O
Indeed	O
,	O
the	O
exponential	O
function	O
can	O
instead	O
be	O
replaced	O
with	O
a	O
step	O
function	O
.	O
For	O
the	O
position	O
-	O
wise	O
feed	O
-	O
forward	O
networks	O
,	O
we	O
quantize	O
the	O
output	O
of	O
the	O
ReLUs	O
and	O
of	O
the	O
feed	O
-	O
forwards	O
themselves	O
.	O
Finally	O
,	O
for	O
all	O
LayerNorms	O
,	O
we	O
quantize	O
the	O
numerator	O
x−µ	O
,	O
the	O
denominator	O
√	O
σ	O
2	O
+	O
,	O
their	O
quotient	O
and	O
the	O
output	O
of	O
the	O
LayerNorm	O
.	O
A	O
visual	O
guide	O
is	O
provided	O
in	O
appendix	O
A.	O

Instead	O
of	O
using	O
a	O
single	O
set	O
of	O
(	O
s	O
,	O
x	O
min	O
)	O
per	O
quantized	O
tensor	O
,	O
we	O
can	O
quantize	O
subsets	O
of	O
the	O
latter	O
with	O
each	O
its	O
own	O
set	O
of	O
(	O
s	O
,	O
x	O
min	O
)	O
(	O
Alistarh	O
et	O
al	O
,	O
2016	O
)	O
.	O
Even	O
though	O
this	O
adds	O
more	O
scalars	O
,	O
the	O
memory	O
cost	O
is	O
insignificant	O
overall	O
.	O
Furthermore	O
,	O
the	O
added	O
flexibility	O
can	O
greatly	O
alleviate	O
the	O
precision	O
loss	B-MetricName
resulting	O
from	O
all	O
values	O
being	O
mapped	O
to	O
a	O
single	O
low	O
numerical	O
precision	O
domain	O
.	O
We	O
use	O
this	O
bucketing	O
method	O
for	O
all	O
weight	O
matrices	O
,	O
with	O
a	O
number	O
of	O
subset	O
equal	O
to	O
the	O
output	O
dimension	O
.	O
For	O
activations	O
,	O
we	O
use	O
bucketing	O
when	O
quantizing	O
:	O
the	O
sum	O
of	O
input	O
embeddings	O
with	O
the	O
positional	O
encoding	O
,	O
the	O
Q	O
,	O
K	O
,	O
V	O
inputs	O
,	O
the	O
Scaled	B-MethodName
Dot	I-MethodName
-	I-MethodName
Product	I-MethodName
Attention	I-MethodName
's	O
output	O
,	O
the	O
feed	O
-	O
forward	O
's	O
output	O
,	O
the	O
LayerNorm	O
's	O
numerator	O
,	O
quotient	O
and	O
output	O
.	O

Recently	O
,	O
simple	O
quantization	B-TaskName
solutions	O
have	O
been	O
applied	O
to	O
the	O
Transformer	B-MethodName
.	O
Cheong	O
and	O
Daniel	O
(	O
2019	O
)	O
apply	O
k	O
-	O
means	O
quantization	B-TaskName
and	O
binarization	B-TaskName
with	O
two	O
centroids	O
over	O
the	O
weights	O
of	O
the	O
network	O
.	O
For	O
both	O
methods	O
,	O
a	O
look	O
up	O
table	O
associated	O
with	O
each	O
quantized	O
layer	O
is	O
used	O
to	O
map	O
indices	O
to	O
their	O
corresponding	O
centroids	O
.	O
Similarly	O
,	O
Fan	O
(	O
2019	O
)	O
compares	O
binary	O
,	O
4	O
and	O
8	O
-	O
bit	O
uniform	O
quantization	B-TaskName
of	O
the	O
Transformer	B-MethodName
weights	O
.	O
A	O
big	O
disadvantage	O
with	O
quantizing	O
only	O
the	O
weights	O
of	O
a	O
network	O
is	O
that	O
operations	O
must	O
still	O
be	O
performed	O
in	O
full	O
-	O
precision	O
.	O
Even	O
though	O
the	O
parameters	O
'	O
memory	O
usage	O
is	O
reduced	O
,	O
these	O
constantly	O
have	O
to	O
be	O
converted	O
back	O
to	O
full	O
-	O
precision	O
.	O
Achieving	O
quantization	B-TaskName
of	O
both	O
weights	O
and	O
activations	O
is	O
much	O
more	O
beneficial	O
.	O
The	O
first	O
attempt	O
at	O
doing	O
so	O
for	O
the	O
Transformer	B-MethodName
applies	O
8	O
-	O
bit	O
quantization	B-TaskName
on	O
weights	O
and	O
inputs	O
of	O
feed	O
forward	O
layers	O
and	O
binarizes	O
the	O
(	O
Q	O
,	O
K	O
)	O
input	O
of	O
the	O
Multi	B-MethodName
-	I-MethodName
Head	I-MethodName
Attention	I-MethodName
(	O
Tierno	O
,	O
2019	O
)	O
.	O
The	O
scaling	O
factor	O
√	O
d	O
k	O
is	O
approximated	O
by	O
a	O
constant	O
which	O
can	O
be	O
computed	O
as	O
a	O
right	O
bitshift	O
.	O
The	O
method	O
resulted	O
in	O
a	O
huge	O
drop	O
in	O
translation	O
accuracy	B-MetricName
.	O
Achieving	O
better	O
performance	O
,	O
Bhandare	O
et	O
al	O
(	O
2019	O
)	O
quantize	O
certain	O
MatMul	O
operations	O
and	O
use	O
the	O
KL	O
divergence	O
to	O
estimate	O
the	O
most	O
suited	O
parameters	O
for	O
each	O
quantization	B-TaskName
range	O
.	O
They	O
restrain	O
from	O
quantizing	O
all	O
MatMuls	O
,	O
reporting	O
poorer	O
results	O
in	O
accuracy	B-MetricName
.	O
Aside	O
from	O
translation	O
,	O
the	O
concurrent	O
work	O
by	O
Zafrir	O
et	O
al	O
(	O
2019	O
)	O
quantizes	O
the	O
embedding	O
and	O
fully	O
connected	O
layers	O
of	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
Softmax	B-MethodName
and	O
LayerNorm	O
operations	O
are	O
kept	O
in	O
full	O
-	O
precision	O
.	O
On	O
the	O
GLUE	B-DatasetName
benchmark	O
,	O
their	O
loss	B-MetricName
in	O
accuracy	B-MetricName
is	O
minimal	O
compared	O
to	O
the	O
original	O
model	O
.	O
All	O
of	O
these	O
methods	O
omit	O
quantizing	O
the	O
whole	O
Transformer	B-MethodName
architecture	O
,	O
resulting	O
in	O
suboptimal	O
computational	O
efficiency	O
.	O
Furthermore	O
,	O
these	O
solutions	O
all	O
fail	O
to	O
avoid	O
impairing	O
translation	O
quality	O
.	O
Our	O
method	O
achieves	O
both	O
.	O

We	O
apply	O
our	O
quantization	B-TaskName
strategy	O
on	O
both	O
the	O
base	O
and	O
big	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
training	O
setup	O
of	O
all	O
presented	O
models	O
is	O
the	O
same	O
as	O
in	O
the	O
original	O
paper	O
,	O
with	O
the	O
exception	O
that	O
the	O
dropout	O
ratio	O
is	O
set	O
to	O
0.1	O
in	O
all	O
cases	O
.	O
We	O
refer	O
readers	O
to	O
the	O
original	O
paper	O
for	O
experimental	O
details	O
.	O
Our	O
models	O
were	O
first	O
evaluated	O
on	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
/	O
2017	O
English	O
-	O
to	O
-	O
German	O
and	O
WMT	B-DatasetName
2014	I-DatasetName
English	O
-	O
to	O
-	O
French	O
translation	O
tasks	O
.	O
Reported	O
perplexity	B-MetricName
is	O
per	O
token	O
and	O
BLEU	B-MetricName
was	O
measured	O
with	O
multi	O
-	O
bleu.pl	O
1	O
on	O
the	O
newstest2014	O
2	O
test	O
set	O
.	O
We	O
used	O
beam	O
search	O
with	O
a	O
beam	O
size	O
of	O
4	O
and	O
a	O
length	O
penalty	O
of	O
0.6	O
.	O
Unlike	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
,	O
no	O
checkpoint	O
averaging	O
was	O
performed	O
.	O
We	O
compare	O
our	O
results	O
with	O
the	O
original	O
Transformer	B-MethodName
and	O
other	O
8	O
-	O
bit	O
quantization	B-TaskName
methods	O
in	O
Table	O
1	O
.	O
All	O
models	O
are	O
base	O
Transformers	O
.	O
Original	O
uncompressed	O
size	O
is	O
the	O
same	O
in	O
all	O
cases	O
.	O
Most	O
work	O
do	O
not	O
report	O
their	O
compressed	O
model	O
size	O
.	O
For	O
those	O
,	O
we	O
give	O
lower	O
bounds	O
based	O
on	O
their	O
reports	O
.	O
Our	O
BLEU	B-MetricName
score	I-MetricName
was	O
computed	O
on	O
the	O
test	O
set	O
using	O
the	O
checkpoint	O
with	O
the	O
highest	O
validation	O
accuracy	B-MetricName
over	O
In	O
Table	O
2	O
,	O
we	O
show	O
performance	O
of	O
our	O
method	O
on	O
the	O
WMT14	B-DatasetName
EN	O
-	O
DE	O
and	O
WMT14	B-DatasetName
EN	O
-	O
FR	O
for	O
a	O
fixed	O
amount	O
of	O
training	O
steps	O
.	O
We	O
compare	O
our	O
results	O
with	O
two	O
full	O
-	O
precision	O
Transformers	O
:	O
base	O
and	O
big	O
variants	O
.	O
We	O
also	O
evaluate	O
two	O
other	O
quantization	B-TaskName
approaches	O
.	O
The	O
first	O
one	O
is	O
the	O
"	O
default	O
"	O
approach	O
,	O
which	O
is	O
to	O
naively	O
quantize	O
every	O
possible	O
operation	O
.	O
The	O
second	O
approach	O
applies	O
our	O
quantization	B-TaskName
strategy	O
post	O
-	O
training	O
(	O
see	O
section	O
5.3	O
)	O
.	O
In	O
all	O
cases	O
except	O
for	O
post	O
-	O
quantization	B-TaskName
,	O
BLEU	B-MetricName
was	O
computed	O
on	O
the	O
test	O
set	O
using	O
the	O
checkpoint	O
which	O
scored	O
the	O
highest	O
accuracy	B-MetricName
on	O
the	O
validation	O
set	O
.	O
Towards	O
the	O
end	O
of	O
training	O
,	O
we	O
ran	O
one	O
validation	O
epoch	O
for	O
every	O
100	O
training	O
steps	O
.	O
Baselines	O
and	O
FullyQT	O
8	O
-	O
bit	O
results	O
were	O
averaged	O
over	O
5	O
trials	O
.	O
Standard	O
deviation	O
of	O
the	O
BLEU	B-MetricName
scores	O
did	O
not	O
seem	O
higher	O
for	O
any	O
method	O
and	O
ranged	O
between	O
0.09	O
and	O
0.51	O
.	O
Training	O
with	O
quantization	B-TaskName
was	O
about	O
twice	O
as	O
slow	O
as	O
with	O
the	O
baselines	O
.	O
As	O
for	O
post	O
-	O
training	O
quantization	B-TaskName
,	O
the	O
BLEU	B-MetricName
score	I-MetricName
was	O
computed	O
on	O
the	O
test	O
set	O
using	O
the	O
best	O
validation	O
performance	O
out	O
of	O
20	O
trials	O
.	O
The	O
default	O
approach	O
's	O
nan	O
in	O
the	O
EN	O
-	O
FR	O
task	O
is	O
due	O
to	O
numerical	O
instability	O
.	O
By	O
quantizing	O
every	O
operation	O
,	O
zeros	O
in	O
the	O
LayerNorm	O
's	O
denominator	O
are	O
more	O
frequent	O
.	O
Results	O
on	O
additional	O
translation	O
datasets	O
can	O
be	O
found	O
in	O
Table	O
3	O
.	O
All	O
models	O
were	O
trained	O
following	O
the	O
same	O
setup	O
as	O
WMT14	B-DatasetName
EN	O
-	O
FR	O
and	O
WMT14	B-DatasetName
EN	O
-	O
DE	O
.	O
Vocabulary	O
size	O
is	O
set	O
to	O
32k	O
for	O
all	O
models	O
.	O
Since	O
there	O
is	O
no	O
test	O
set	O
for	O
WMT14	B-DatasetName
ES	O
-	O
EN	O
,	O
we	O
used	O
the	O
validation	O
set	O
as	O
a	O
test	O
set	O
and	O
omitted	O
computing	O
any	O
validation	O
epochs	O
during	O
training	O
.	O
Looking	O
at	O
all	O
conducted	O
experiments	O
,	O
including	O
section	O
5.3	O
,	O
translation	O
quality	O
of	O
the	O
8	O
-	O
bit	O
Ful	O
-	O
lyQT	O
models	O
seems	O
to	O
be	O
on	O
par	O
with	O
full	O
-	O
precision	O
.	O
Most	O
of	O
the	O
time	O
,	O
the	O
highest	O
BLEU	B-MetricName
was	O
scored	O
by	O
the	O
quantized	O
model	O
.	O
For	O
example	O
in	O
the	O
case	O
of	O
WMT14	B-DatasetName
EN	O
-	O
DE	O
,	O
the	O
maximum	O
BLEU	B-MetricName
FullyQT	O
base	O
8	O
-	O
bit	O
obtained	O
was	O
26.98	O
,	O
while	O
the	O
baseline	O
's	O
highest	O
was	O
26.64	O
.	O
As	O
for	O
the	O
big	O
models	O
,	O
the	O
max	O
FullyQT	O
scored	O
was	O
27.95	O
,	O
whereas	O
the	O
baseline	O
's	O
was	O
27.43	O
.	O
We	O
looked	O
at	O
training	O
and	O
validation	O
curves	O
to	O
see	O
if	O
quantization	B-TaskName
had	O
any	O
effect	O
,	O
but	O
saw	O
no	O
discernible	O
difference	O
.	O
All	O
models	O
use	O
full	O
-	O
precision	O
biases	O
,	O
s	O
and	O
x	O
min	O
.	O
This	O
amounts	O
to	O
11.61	O
Mb	O
in	O
the	O
base	O
models	O
and	O
23.15	O
Mb	O
in	O
the	O
big	O
models	O
.	O
In	O
the	O
case	O
of	O
8	O
-	O
bit	O
,	O
these	O
represent	O
less	O
than	O
2.35	O
%	O
of	O
the	O
total	O
size	O
.	O
Without	O
bucketing	O
,	O
this	O
would	O
amount	O
to	O
2.18	O
Mb	O
and	O
4.35	O
Mb	O
respectively	O
.	O
We	O
believe	O
the	O
small	O
increase	O
in	O
model	O
size	O
to	O
be	O
worth	O
it	O
.	O
Indeed	O
,	O
in	O
section	O
5.2	O
,	O
we	O
show	O
that	O
training	O
without	O
bucketing	O
leads	O
to	O
poorer	O
translation	O
.	O
Although	O
6	O
-	O
bit	O
quantization	B-TaskName
seems	O
to	O
perform	O
well	O
,	O
the	O
compression	O
advantage	O
over	O
8	O
-	O
bit	O
is	O
usually	O
lost	O
.	O
Most	O
hardware	O
store	O
INT6	O
using	O
either	O
8	O
or	O
32	O
bits	O
.	O
Dedicated	O
hardware	O
is	O
needed	O
to	O
get	O
the	O
full	O
compression	O
advantage	O
.	O
Unless	O
6	O
-	O
bit	O
quantization	B-TaskName
results	O
in	O
better	O
models	O
,	O
8	O
-	O
bit	O
seems	O
like	O
the	O
best	O
choice	O
for	O
most	O
hardware	O
.	O

To	O
better	O
understand	O
which	O
operations	O
are	O
more	O
sensitive	O
to	O
quantization	B-TaskName
,	O
we	O
evaluate	O
such	O
effect	O
on	O
single	O
operations	O
of	O
the	O
Transformer	B-MethodName
.	O
By	O
this	O
,	O
we	O
mean	O
quantizing	O
the	O
operation	O
of	O
a	O
module	O
for	O
all	O
Transformer	B-MethodName
layers	O
.	O
Table	O
4	O
shows	O
results	O
on	O
the	O
WMT14	B-DatasetName
EN	O
-	O
FR	O
translation	O
task	O
for	O
8	O
-	O
bit	O
precision	O
.	O
The	O
effect	O
of	O
bucketing	O
was	O
also	O
evaluated	O
.	O
BLEU	B-MetricName
was	O
computed	O
on	O
the	O
test	O
set	O
after	O
100k	O
steps	O
of	O
training	O
.	O
In	O
24	O
out	O
of	O
27	O
experiments	O
,	O
performance	O
was	O
better	O
than	O
our	O
full	O
-	O
precision	O
baseline	O
of	O
38.34	O
BLEU	B-MetricName
.	O
Solely	O
quantizing	O
the	O
LayerNorm	O
's	O
denominator	O
with	O
no	O
bucketing	O
results	O
in	O
poor	O
performance	O
.	O
The	O
latter	O
also	O
can	O
not	O
be	O
bucketed	O
since	O
all	O
dimensions	O
of	O
the	O
variance	O
tensor	O
vary	O
per	O
batch	O
.	O
To	O
successfully	O
quantize	O
this	O
element	O
without	O
causing	O
any	O
loss	B-MetricName
in	O
performance	O
,	O
we	O
suspect	O
quantizing	O
other	O
elements	O
in	O
the	O
network	O
helps	O
.	O
To	O
further	O
validate	O
our	O
quantization	B-TaskName
scheme	O
,	O
we	O
evaluated	O
four	O
models	O
trained	O
with	O
alterations	O
to	O
our	O
design	O
choices	O
.	O
Results	O
on	O
the	O
WMT14	B-DatasetName
EN	O
-	O
FR	O
task	O
are	O
presented	O
in	O
Table	O
5	O
.	O
All	O
models	O
are	O
8	O
-	O
bit	O
quantized	O
base	O
Transformers	O
.	O
Training	O
procedure	O
is	O
the	O
same	O
as	O
in	O
section	O
5.1	O
.	O

Our	O
method	O
's	O
goal	O
is	O
to	O
increase	O
computational	O
efficiency	O
when	O
inferring	O
with	O
the	O
Transformer	B-MethodName
.	O
To	O
this	O
end	O
,	O
our	O
quantization	B-TaskName
scheme	O
only	O
requires	O
us	O
to	O
learn	O
s	O
and	O
x	O
min	O
.	O
Although	O
we	O
do	O
so	O
throughout	O
the	O
whole	O
training	O
,	O
this	O
is	O
not	O
a	O
necessity	O
.	O
Quantization	B-TaskName
could	O
also	O
be	O
applied	O
later	O
during	O
training	O
.	O
Results	O
for	O
different	O
starting	O
points	O
are	O
compared	O
in	O
Table	O
6	O
.	O
The	O
earliest	O
we	O
start	O
quantizing	O
is	O
at	O
100	O
steps	O
,	O
since	O
we	O
need	O
at	O
least	O
a	O
few	O
steps	O
to	O
assess	O
the	O
running	O
estimates	O
.	O
All	O
models	O
were	O
evaluated	O
on	O
the	O
WMT14	B-DatasetName
EN	O
-	O
DE	O
and	O
WMT14	B-DatasetName
EN	O
-	O
FR	O
translation	O
tasks	O
.	O
BLEU	B-MetricName
was	O
measured	O
on	O
the	O
test	O
set	O
using	O
the	O
checkpoint	O
which	O
scored	O
the	O
highest	O
accuracy	B-MetricName
on	O
the	O
validation	O
set	O
during	O
training	O
.	O
Validation	O
was	O
computed	O
every	O
100	O
training	O
steps	O
towards	O
the	O
end	O
of	O
training	O
.	O
From	O
our	O
observed	O
results	O
,	O
quantizing	O
the	O
model	O
early	O
on	O
seems	O
preferable	O
.	O
Learning	O
quantization	B-TaskName
parameters	O
adds	O
a	O
significant	O
computational	O
cost	O
during	O
training	O
.	O
A	O
major	O
advantage	O
to	O
delaying	O
quantization	B-TaskName
is	O
to	O
perform	O
more	O
training	O
steps	O
in	O
the	O
same	O
given	O
amount	O
of	O
time	O
.	O
Therefore	O
,	O
when	O
training	O
time	O
is	O
a	O
constraint	O
,	O
a	O
possible	O
strategy	O
is	O
to	O
train	O
a	O
model	O
without	O
quantization	B-TaskName
,	O
perform	O
more	O
training	O
steps	O
and	O
finally	O
post	O
-	O
quantize	O
the	O
model	O
.	O
By	O
the	O
latter	O
,	O
we	O
mean	O
keeping	O
all	O
weights	O
fixed	O
and	O
compute	O
the	O
s	O
and	O
x	O
min	O
over	O
a	O
few	O
hundred	O
steps	O
.	O
This	O
is	O
another	O
advantage	O
,	O
since	O
many	O
trials	O
can	O
be	O
performed	O
in	O
search	O
of	O
the	O
best	O
performing	O
candidate	O
.	O
We	O
found	O
post	O
-	O
quantization	B-TaskName
BLEU	B-MetricName
scores	O
to	O
vary	O
by	O
about	O
0.2	O
BLEU	B-MetricName
.	O

To	O
evaluate	O
if	O
our	O
quantization	B-TaskName
scheme	O
generalizes	O
well	O
to	O
other	O
tasks	O
,	O
we	O
evaluate	O
it	O
on	O
two	O
language	O
modeling	O
datasets	O
:	O
WikiText	O
-	O
2	O
and	O
WikiText	O
-	O
103	O
.	O
As	O
the	O
setup	O
,	O
we	O
use	O
PyTorch	O
's	O
language	O
modeling	O
toy	O
example	O
3	O
.	O
The	O
task	O
consists	O
of	O
predicting	O
the	O
sequence	O
{	O
x	O
t+1	O
,	O
,	O
x	O
t+n+1	O
}	O
from	O
the	O
input	O
sequence	O
{	O
x	O
t	O
,	O
,	O
x	O
t+n	O
}	O
.	O
We	O
trained	O
four	O
Transformer	B-MethodName
models	O
,	O
each	O
with	O
different	O
precision	O
.	O
All	O
models	O
consist	O
of	O
two	O
Transformer	B-MethodName
encoder	O
layers	O
,	O
with	O
the	O
embedding	O
and	O
hidden	O
size	O
set	O
to	O
200	O
.	O
Multi	B-MethodName
-	I-MethodName
Head	I-MethodName
Attention	I-MethodName
has	O
two	O
heads	O
with	O
key	O
and	O
value	O
size	O
64	O
.	O
The	O
final	O
word	O
projection	O
layer	O
's	O
weights	O
are	O
shared	O
with	O
the	O
embedding	O
layer	O
.	O
Models	O
were	O
trained	O
for	O
10	O
epochs	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
20	O
and	O
sequence	O
length	O
of	O
35	O
.	O
Learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
5	O
,	O
dropout	O
to	O
0.2	O
and	O
gradient	B-MethodName
clipping	I-MethodName
to	O
0.25	O
.	O
Loss	O
is	O
computed	O
on	O
every	O
element	O
of	O
the	O
output	O
sequence	O
.	O
Results	O
are	O
presented	O
in	O
Table	O
7	O
.	O
Validation	O
was	O
computed	O
every	O
epoch	O
to	O
determine	O
the	O
best	O
candidate	O
.	O
Loss	O
and	O
perplexity	B-MetricName
are	O
computed	O
on	O
the	O
test	O
set	O
and	O
averaged	O
over	O
10	O
trials	O
for	O
WikiText	O
-	O
2	O
and	O
3	O
trials	O
for	O
WikiText	O
-	O
3	O
.	O
See	O
footnote	O
3	O
for	O
any	O
extra	O
details	O
.	O

We	O
experiment	O
with	O
node	O
pruning	O
our	O
Transformer	B-MethodName
models	O
.	O
Once	O
the	O
model	O
is	O
fully	O
trained	O
and	O
quantized	O
,	O
we	O
can	O
further	O
compress	O
it	O
by	O
removing	O
useless	O
nodes	O
.	O
By	O
useless	O
,	O
we	O
mean	O
nodes	O
which	O
do	O
not	O
cause	O
any	O
loss	B-MetricName
in	O
translation	O
quality	O
when	O
removed	O
.	O
We	O
choose	O
to	O
prune	O
nodes	O
instead	O
of	O
independently	O
pruning	O
weights	O
.	O
The	O
latter	O
method	O
usually	O
requires	O
special	O
hardware	O
or	O
software	O
to	O
leverage	O
sparse	O
weight	O
matrices	O
.	O
Pruning	O
nodes	O
results	O
in	O
concretely	O
shrunken	O
models	O
.	O
When	O
getting	O
rid	O
of	O
a	O
node	O
,	O
we	O
remove	O
its	O
corresponding	O
set	O
of	O
weights	O
from	O
the	O
layer	O
outputting	O
it	O
and	O
the	O
following	O
layer	O
receiving	O
the	O
node	O
as	O
input	O
.	O
The	O
only	O
nodes	O
of	O
the	O
Transformer	B-MethodName
which	O
can	O
be	O
removed	O
without	O
causing	O
alterations	O
to	O
other	O
components	O
of	O
the	O
network	O
are	O
the	O
nodes	O
in	O
between	O
the	O
two	O
layers	O
of	O
each	O
feed	O
-	O
forward	O
network	O
.	O
Fortunately	O
,	O
these	O
consist	O
of	O
a	O
substantial	O
portion	O
of	O
the	O
model	O
's	O
weights	O
.	O
In	O
the	O
case	O
of	O
the	O
base	O
Transformer	B-MethodName
,	O
for	O
a	O
respective	O
vocabulary	O
of	O
size	O
37000	O
and	O
32000	O
,	O
39.96	O
%	O
and	O
41.65	O
%	O
of	O
the	O
total	O
weights	O
are	O
owned	O
by	O
the	O
feed	O
-	O
foward	O
networks	O
.	O
This	O
number	O
grows	O
to	O
47.03	O
%	O
and	O
48.18	O
%	O
in	O
the	O
big	O
Transformer	B-MethodName
.	O
To	O
evaluate	O
which	O
nodes	O
can	O
be	O
safely	O
pruned	O
without	O
affecting	O
translation	O
quality	O
,	O
we	O
estimate	O
x	O
max	O
for	O
each	O
node	O
of	O
the	O
ReLU	B-MethodName
output	O
over	O
a	O
few	O
hundred	O
steps	O
.	O
This	O
is	O
done	O
on	O
the	O
training	O
set	O
,	O
using	O
the	O
fully	O
trained	O
model	O
and	O
keeping	O
all	O
other	O
weights	O
frozen	O
.	O
These	O
x	O
max	O
are	O
computed	O
before	O
quantizing	O
the	O
ReLU	B-MethodName
output	O
and	O
do	O
not	O
replace	O
the	O
ones	O
used	O
by	O
the	O
quantization	B-TaskName
process	O
.	O
Figure	O
3	O
in	O
the	O
appendix	O
shows	O
the	O
histogram	O
of	O
these	O
running	O
estimates	O
for	O
one	O
ReLU	B-MethodName
layer	O
in	O
the	O
encoder	O
and	O
one	O
in	O
the	O
decoder	O
.	O
All	O
other	O
ReLU	B-MethodName
layers	O
share	O
the	O
same	O
pattern	O
,	O
where	O
in	O
the	O
encoder	O
there	O
are	O
always	O
multiple	O
x	O
max	O
close	O
to	O
0	B-DatasetName
.	O
This	O
does	O
not	O
happen	O
in	O
the	O
decoder	O
.	O
Once	O
the	O
running	O
estimates	O
are	O
computed	O
,	O
we	O
prune	O
its	O
corresponding	O
node	O
if	O
x	O
max	O
<	O
zσ	O
where	O
z	O
is	O
a	O
hyperparameter	O
and	O
σ	O
the	O
standard	O
deviation	O
of	O
the	O
layer	O
's	O
x	O
max	O
.	O
We	O
empirically	O
found	O
z	O
=	O
0.025	O
to	O
work	O
well	O
,	O
with	O
higher	O
thresholds	O
causing	O
BLEU	B-MetricName
to	O
quickly	O
decay	O
.	O
No	O
retraining	O
of	O
the	O
model	O
is	O
performed	O
after	O
pruning	O
nodes	O
.	O
Using	O
this	O
method	O
,	O
we	O
can	O
further	O
compress	O
the	O
Transformer	B-MethodName
without	O
affecting	O
BLEU	B-MetricName
scores	O
.	O
Our	O
approach	O
has	O
the	O
advantage	O
of	O
being	O
adaptive	O
,	O
meaning	O
the	O
number	O
of	O
nodes	O
pruned	O
per	O
layer	O
will	O
differ	O
as	O
opposed	O
to	O
a	O
fixed	O
pruning	O
ratio	O
method	O
.	O
For	O
example	O
,	O
in	O
the	O
case	O
of	O
the	O
big	O
Transformer	B-MethodName
trained	O
on	O
WMT14	B-DatasetName
EN	O
-	O
FR	O
,	O
169	O
nodes	O
were	O
pruned	O
in	O
the	O
first	O
ReLU	B-MethodName
of	O
the	O
encoder	O
,	O
while	O
in	O
the	O
second	O
,	O
1226	O
were	O
pruned	O
.	O
Nodes	O
in	O
the	O
decoder	O
rarely	O
got	O
pruned	O
,	O
at	O
most	O
4	O
in	O
the	O
whole	O
decoder	O
.	O
Results	O
are	O
presented	O
in	O
Table	O
8	O
.	O
Reported	O
results	O
are	O
averaged	O
on	O
the	O
test	O
set	O
over	O
a	O
few	O
trials	O
.	O
BLEU	B-MetricName
varied	O
by	O
about	O
0.01−0.02	O
.	O
Other	O
approaches	O
usually	O
decide	O
the	O
ratio	O
first	O
and	O
then	O
prune	O
.	O
We	O
compared	O
with	O
two	O
such	O
methods	O
.	O
For	O
each	O
task	O
,	O
we	O
fix	O
their	O
ratio	O
to	O
the	O
average	O
percentage	O
of	O
nodes	O
pruned	O
by	O
our	O
method	O
and	O
only	O
prune	O
nodes	O
in	O
the	O
encoder	O
.	O
The	O
first	O
fixed	O
pruning	O
method	O
uses	O
L1	O
-	O
norm	O
to	O
sort	O
nodes	O
in	O
ascending	O
weight	O
order	O
,	O
while	O
the	O
second	O
sorts	O
the	O
x	O
max	O
,	O
also	O
in	O
ascending	O
order	O
.	O

We	O
proposed	O
a	O
full	O
quantization	B-TaskName
strategy	O
for	O
the	O
Transformer	B-MethodName
architecture	O
.	O
Our	O
objective	O
was	O
to	O
ex	O
-	O
ploit	O
hardware	O
resources	O
as	O
efficiently	O
as	O
possible	O
,	O
quantizing	O
all	O
operations	O
which	O
could	O
provide	O
a	O
computational	O
speed	O
gain	O
.	O
With	O
FullyQT	O
,	O
we	O
achieve	O
higher	O
BLEU	B-MetricName
scores	O
than	O
all	O
other	O
quantization	B-TaskName
methods	O
for	O
the	O
Transformer	B-MethodName
on	O
multiple	O
translation	O
tasks	O
and	O
avoid	O
any	O
loss	B-MetricName
in	O
BLEU	B-MetricName
compared	O
to	O
full	O
-	O
precision	O
.	O
Specifically	O
,	O
out	O
of	O
35	O
experiments	O
,	O
8	O
-	O
bit	O
quantization	B-TaskName
performed	O
better	O
than	O
full	O
-	O
precision	O
in	O
21	O
cases	O
.	O
If	O
instead	O
of	O
minimizing	O
inference	O
time	O
,	O
one	O
wants	O
to	O
maximize	O
translation	O
accuracy	B-MetricName
,	O
then	O
applying	O
quantization	B-TaskName
to	O
only	O
certain	O
components	O
of	O
the	O
Transformer	B-MethodName
seems	O
to	O
be	O
the	O
best	O
option	O
.	O
Indeed	O
,	O
our	O
ablation	O
study	O
showed	O
than	O
BLEU	B-MetricName
score	I-MetricName
could	O
increase	O
even	O
more	O
when	O
only	O
specific	O
elements	O
of	O
the	O
Transformer	B-MethodName
were	O
quantized	O
.	O
Further	O
gains	O
might	O
be	O
possible	O
,	O
but	O
supplementary	O
experiments	O
would	O
be	O
necessary	O
to	O
determine	O
the	O
best	O
combination	O
.	O
We	O
plan	O
on	O
extending	O
our	O
work	O
to	O
variations	O
of	O
the	O
Transformer	B-MethodName
,	O
as	O
well	O
as	O
further	O
exploring	O
the	O
compression	O
of	O
these	O
networks	O
.	O

The	O
difference	O
between	O
@	O
i	O
A	O
and	O
#	O
A	O
is	O
that	O
while	O
@	O
i	O
A	O
searches	O
for	O
an	O
antecedent	O
of	O
type	O
A	O
in	O
a	O
given	O
local	O
context	O
via	O
anaphora	O
resolution	O
,	O
#	O
A	O
introduces	O
an	O
object	O
of	O
type	O
A	O
via	O
the	O
following	O
rule	O
for	O
#	O
-	O
elimination	O
:	O
(	O
18	O
)	O
#	O
-	O
elimination	O
:	O
Let	O
ϕ	O
be	O
a	O
term	O
containing	O
#	O
A	O
as	O
a	O
subterm	O
,	O
where	O
A	O
is	O
a	O
type	O
in	O
which	O
no	O
@	O
-	O
term	O
nor	O
#	O
-	O
term	O
occurs	O
.	O
Suppose	O
that	O
we	O
have	O
a	O
derivation	O
of	O
the	O
form	O
on	O
the	O
left	O
,	O
where	O
ϕ	O
:	O
t	O
is	O
the	O
first	O
node	O
that	O
has	O
type	O
t	O
and	O
depends	O
on	O
#	O
A	O
:	O
A	O
,	O
i.e.	O
,	O
no	O
other	O
judgement	O
of	O
the	O
form	O
ψ	O
:	O
t	O
appears	O
in	O
D	O
2	O
.	O
Then	O
the	O
derivation	O
can	O
be	O
transformed	O
into	O
the	O
one	O
on	O
the	O
right	O
:	O
.	O
.	O
.	O
.	O
D	O
1	O
A	O
:	O
t	O
#	O
A	O
:	O
A	O
#	O
.	O
.	O
.	O
.	O
D	O
2	O
ϕ	O
:	O
t	O
.	O
.	O
.	O
.	O
D	O
1	O
A	O
:	O
t	O
[	O
u	O
:	O
A	O
]	O
n	O
.	O
.	O
.	O
.	O
D	O
2	O
[	O
u/#A	O
]	O
ϕ	O
[	O
u/#A	O
]	O
:	O
t	O
(	O
u	O
:	O
A	O
)	O
×	O
ϕ	O
[	O
u/#A	O
]	O
:	O
t	O
ΣF	O
,	O
n	O
By	O
this	O
rule	O
,	O
if	O
there	O
is	O
a	O
branch	O
containing	O
an	O
underspecified	O
term	O
#	O
A	O
,	O
one	O
can	O
close	O
it	O
by	O
taking	O
the	O
existence	O
of	O
an	O
object	O
of	O
type	O
A	O
as	O
part	O
of	O
the	O
asserted	O
proposition	O
represented	O
as	O
a	O
Σ	O
-	O
type	O
of	O
the	O
form	O
(	O
u	O
:	O
A	O
)	O
×	O
ϕ	O
[	O
u/#A	O
]	O
,	O
where	O
ϕ	O
[	O
u/#A	O
]	O
is	O
the	O
expression	O
obtained	O
by	O
replacing	O
the	O
occurrence	O
of	O
#	O
A	O
in	O
ϕ	O
by	O
u.	O
In	O
the	O
case	O
of	O
(	O
16	O
)	O
,	O
the	O
initial	O
derivation	O
shown	O
on	O
the	O
left	O
in	O
(	O
19	O
)	O
is	O
transformed	O
to	O
the	O
derivation	O
on	O
the	O
right	O
by	O
#	O
-	O
elimination	O
,	O
so	O
we	O
end	O
up	O
with	O
the	O
same	O
representation	O
as	O
(	O
6	O
)	O
.	O
(	O
19	O
)	O
[	O
x	O
:	O
e	O
]	O
1	O
man	O
(	O
x	O
)	O
:	O
t	O
man	O
*	O
:	O
t	O
ΣF	O
,	O
1	O
#	O
man	O
*	O
:	O
man	O
*	O
#	O
π	O
1	O
(	O
#	O
man	O
*	O
)	O
:	O
e	O
ΣE	O
enter	O
(	O
π	O
1	O
(	O
#	O
man	O
*	O
)	O
)	O
:	O
t	O
[	O
x	O
:	O
e	O
]	O
1	O
man	O
(	O
x	O
)	O
:	O
t	O
man	O
*	O
:	O
t	O
ΣF	O
,	O
1	O
[	O
u	O
:	O
man	O
*	O
]	O
1	O
π	O
1	O
(	O
u	O
)	O
:	O
e	O
ΣE	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
:	O
t	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
:	O
t	O
ΣF	O
,	O
1	O
Thus	O
#	O
-	O
elimination	O
allows	O
us	O
to	O
eliminate	O
a	O
#	O
-	O
term	O
from	O
a	O
type	O
and	O
rewrite	O
it	O
to	O
a	O
Σ	O
-	O
type	O
.	O
For	O
notational	O
convenience	O
,	O
we	O
write	O
this	O
transformation	O
as	O
enter	O
(	O
π	O
1	O
(	O
#	O
man	O
*	O
)	O
)	O
#	O
(	O
u	O
:	O
man	O
*	O
)	O
×	O
enter	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
.	O
It	O
should	O
be	O
clear	O
from	O
the	O
above	O
that	O
DTS	O
crucially	O
makes	O
use	O
of	O
underspecification	O
in	O
the	O
interpretations	O
of	O
both	O
pronouns	O
and	O
indefinites	O
.	O
We	O
make	O
the	O
following	O
two	O
assumptions	O
about	O
the	O
way	O
underspecified	O
terms	O
are	O
interpreted	O
in	O
the	O
course	O
of	O
semantic	B-TaskName
composition	I-TaskName
:	O
(	O
20	O
)	O
a.	O
Ban	O
on	O
the	O
duplication	O
of	O
underspecified	O
terms	O
:	O
In	O
a	O
well	O
-	O
formed	O
semantic	O
representation	O
of	O
DTS	O
,	O
an	O
underspecified	O
@	O
-	O
term	O
with	O
the	O
same	O
index	O
can	O
appear	O
at	O
most	O
once	O
.	O
b.	O
Normal	O
form	O
requirement	O
on	O
compositionally	O
derived	O
semantic	O
terms	O
:	O
At	O
each	O
step	O
of	O
semantic	B-TaskName
composition	I-TaskName
,	O
the	O
semantic	O
term	O
assigned	O
to	O
the	O
derived	O
linguistic	O
expression	O
is	O
in	O
β	B-HyperparameterName
-	O
normal	O
form	O
.	O
These	O
restrictions	O
can	O
be	O
thought	O
of	O
as	O
embodying	O
a	O
general	O
requirement	O
that	O
underspecification	O
resolution	O
is	O
not	O
totally	O
unconstrained	O
but	O
is	O
affected	O
by	O
the	O
form	O
of	O
the	O
sentences	O
in	O
which	O
underspecified	O
expressions	O
occur	O
.	O
As	O
such	O
,	O
these	O
restrictions	O
play	O
key	O
roles	O
in	O
the	O
analysis	O
of	O
scope	O
parallelism	O
in	O
the	O
next	O
section	O
.	O

With	O
the	O
treatment	O
of	O
anaphora	O
and	O
indefinites	O
introduced	O
above	O
,	O
the	O
interpretive	O
parallelism	O
exemplified	O
by	O
(	O
1	O
)	O
and	O
(	O
2	O
)	O
falls	O
out	O
automatically	O
as	O
a	O
consequence	O
of	O
the	O
way	O
underspecification	O
is	O
resolved	O
in	O
DTS	O
.	O
Unlike	O
previous	O
proposals	O
(	O
Asudeh	O
and	O
Crouch	O
2002	O
;	O
Steedman	O
2012	O
)	O
,	O
no	O
extra	O
assumption	O
is	O
needed	O
beyond	O
the	O
simple	O
restriction	O
(	O
20	O
)	O
on	O
underspecification	O
resolution	O
introduced	O
in	O
the	O
previous	O
section	O
.	O
We	O
start	O
with	O
the	O
pronominal	O
binding	O
case	O
.	O
To	O
avoid	O
the	O
issue	O
of	O
possessives	O
(	O
which	O
is	O
itself	O
a	O
complex	O
problem	O
)	O
,	O
we	O
illustrate	O
the	O
analysis	O
with	O
the	O
following	O
example	O
involving	O
an	O
embedded	O
clause	O
:	O
(	O
21	O
)	O
Every	O
Englishman	O
thinks	O
,	O
and	O
every	O
American	O
believes	O
,	O
that	O
he	O
is	O
a	O
genius	O
.	O
One	O
technical	O
issue	O
that	O
needs	O
to	O
be	O
addressed	O
first	O
is	O
how	O
to	O
obtain	O
the	O
bound	O
reading	O
for	O
the	O
pronoun	O
in	O
the	O
RNR'ed	O
position	O
to	O
begin	O
with	O
.	O
Note	O
that	O
given	O
the	O
prohibition	O
on	O
the	O
duplication	O
of	O
underspecified	O
terms	O
in	O
DTS	O
,	O
the	O
simple	O
derivation	O
for	O
(	O
21	O
)	O
in	O
(	O
22	O
)	O
can	O
not	O
yield	O
the	O
bound	O
reading	O
for	O
the	O
pronoun	O
.	O
5	O
(	O
22	O
Here	O
,	O
for	O
the	O
pronoun	O
in	O
the	O
RNR'ed	O
S	O
to	O
be	O
bound	O
by	O
the	O
subject	O
quantifiers	O
in	O
each	O
conjunct	O
,	O
the	O
term	O
gen	O
(	O
@	O
1	O
e	O
)	O
first	O
needs	O
to	O
be	O
substituted	O
for	O
variable	O
p	O
in	O
each	O
conjunct	O
(	O
from	O
where	O
the	O
antecedent	O
is	O
syntactically	O
visible	O
,	O
given	O
the	O
definition	O
of	O
anaphora	O
resolution	O
from	O
section	O
2	O
)	O
,	O
but	O
this	O
is	O
precisely	O
the	O
move	O
that	O
is	O
prohibited	O
by	O
the	O
'	O
no	O
duplication	O
of	O
underspecified	O
term	O
'	O
restriction	O
.	O
This	O
means	O
that	O
,	O
in	O
order	O
to	O
obtain	O
the	O
bound	O
reading	O
,	O
we	O
need	O
a	O
slightly	O
more	O
complex	O
syntactic	O
derivation	O
involving	O
(	O
syntactic	O
)	O
type	O
-	O
lifting	O
of	O
both	O
the	O
RNR'ed	O
material	O
and	O
the	O
conjuncts	O
.	O
The	O
effect	O
in	O
a	O
nutshell	O
is	O
that	O
,	O
via	O
type	O
-	O
lifting	O
,	O
we	O
can	O
ensure	O
enough	O
of	O
the	O
'	O
derivational	O
structure	O
'	O
of	O
the	O
sentence	O
to	O
be	O
present	O
in	O
the	O
(	O
beta	B-HyperparameterName
-	O
unreduced	O
)	O
semantic	O
translation	O
to	O
identify	O
the	O
'	O
possible	O
binder	O
'	O
of	O
the	O
pronoun	O
before	O
all	O
the	O
material	O
is	O
actually	O
composed	O
in	O
the	O
(	O
surface	O
)	O
syntax	O
.	O
The	O
derivation	O
for	O
the	O
bound	O
pronoun	O
reading	O
for	O
(	O
21	O
)	O
thus	O
goes	O
as	O
in	O
(	O
23	O
)	O
.	O
5	O
We	O
adopt	O
the	O
abbreviation	O
A	O
eng	O
=	O
def	O
λP.	O
[	O
(	O
u	O
:	O
eng	O
*	O
)	O
P	O
(	O
π1	O
(	O
u	O
)	O
)	O
]	O
,	O
etc	O
.	O
These	O
abbreviations	O
are	O
unpacked	O
at	O
the	O
end	O
of	O
the	O
derivation	O
(	O
via	O
the	O
step	O
designated	O
by	O
the	O
dotted	O
line	O
,	O
which	O
is	O
not	O
part	O
of	O
the	O
syntactic	O
derivation	O
)	O
for	O
clarity	O
of	O
presentation	O
.	O
Type	O
checking	O
for	O
the	O
semantic	O
representation	O
involves	O
a	O
branch	O
to	O
check	O
the	O
type	O
of	O
λRλx	O
.	O
R	O
(	O
gen	O
(	O
@1e	O
)	O
)	O
(	O
x	O
)	O
,	O
which	O
is	O
shown	O
on	O
the	O
left	O
below	O
.	O
Here	O
the	O
assumption	O
e	O
true	O
follows	O
from	O
the	O
hypothesis	O
x	O
:	O
e	O
and	O
thus	O
by	O
@	O
-	O
elimination	O
we	O
can	O
replace	O
@	O
1	O
e	O
with	O
x	O
throughout	O
the	O
derivation	O
.	O
S	O
/	O
S	O
that	O
he	O
is	O
a	O
genius	O
;	O
gen	O
(	O
@1e	O
)	O
;	O
S	O
/E	O
every	O
Englishman	O
thinks	O
and	O
every	O
American	O
believes	O
that	O
he	O
is	O
a	O
genius	O
;	O
λp	O
[	O
(	O
v	O
:	O
A	O
eng	O
(	O
λy.think	O
(	O
p	O
)	O
(	O
y	O
)	O
)	O
)	O
×	O
A	O
am	O
(	O
5	O
[	O
ϕ4	O
;	O
R	O
;	O
VP	O
/	O
S	O
]	O
4	O
that	O
he	O
is	O
a	O
genius	O
;	O
gen	O
(	O
@1e	O
)	O
;	O
S	O
/E	O
ϕ4	O
that	O
he	O
is	O
a	O
genius	O
;	O
R	O
(	O
gen	O
(	O
@1e	O
)	O
)	O
;	O
VP	O
\E	O
ϕ5	O
ϕ4	O
that	O
he	O
is	O
a	O
genius	O
;	O
R	O
(	O
gen	O
(	O
@1e	O
)	O
)	O
(	O
x	O
)	O
;	O
S	O
I	O
5	O
ϕ4	O
that	O
he	O
is	O
a	O
genius	O
;	O
λx	O
.	O
R	O
(	O
gen	O
(	O
@1e	O
)	O
)	O
(	O
x	O
)	O
;	O
VP	O
I	O
4	O
that	O
he	O
is	O
a	O
genius	O
;	O
λRλx	O
.	O
R	O
(	O
gen	O
(	O
@1e	O
)	O
)	O
(	O
x	O
)	O
;	O
(	O
VP	O
/	O
S	O
)	O
\VP	O
/E	O
every	O
Englishman	O
thinks	O
and	O
every	O
American	O
believes	O
that	O
he	O
is	O
a	O
genius	O
;	O
λP	O
[	O
(	O
v	O
:	O
A	O
eng	O
(	O
(	O
24	O
)	O
[	O
x	O
:	O
e	O
]	O
2	O
e	O
true	O
@	O
1	O
e	O
:	O
e	O
@	O
gen	O
(	O
@	O
1	O
e	O
)	O
:	O
t	O
[	O
x	O
:	O
e	O
]	O
2	O
R	O
(	O
gen	O
(	O
@	O
1	O
e	O
)	O
)	O
(	O
x	O
)	O
:	O
t	O
[	O
R	O
:	O
t	O
e	O
t	O
]	O
1	O
λx	O
.	O
R	O
(	O
gen	O
(	O
@	O
1	O
e	O
)	O
)	O
(	O
x	O
)	O
:	O
e	O
t	O
ΠI	O
,	O
2	O
λRλx	O
.	O
R	O
(	O
gen	O
(	O
@	O
1	O
e	O
)	O
)	O
(	O
x	O
)	O
:	O
(	O
t	O
e	O
t	O
)	O
e	O
t	O
ΠI	O
,	O
1	O
[	O
x	O
:	O
e	O
]	O
2	O
gen	O
(	O
x	O
)	O
:	O
t	O
[	O
x	O
:	O
e	O
]	O
2	O
R	O
(	O
gen	O
(	O
x	O
)	O
)	O
(	O
x	O
)	O
:	O
t	O
[	O
R	O
:	O
t	O
e	O
t	O
]	O
1	O
λx	O
.	O
R	O
(	O
gen	O
(	O
x	O
)	O
)	O
(	O
x	O
)	O
:	O
e	O
t	O
ΠI	O
,	O
2	O
λRλx	O
.	O
R	O
(	O
gen	O
(	O
x	O
)	O
)	O
(	O
x	O
)	O
:	O
(	O
t	O
e	O
t	O
)	O
e	O
t	O
ΠI	O
,	O
1	O
Note	O
crucially	O
that	O
here	O
the	O
underspecification	O
for	O
the	O
pronoun	O
term	O
@	O
1	O
e	O
is	O
resolved	O
before	O
the	O
meaning	O
contribution	O
of	O
the	O
RNR'ed	O
S	O
which	O
contains	O
it	O
as	O
a	O
subterm	O
is	O
copies	O
into	O
each	O
conjunct	O
via	O
β	B-HyperparameterName
-	O
reduction	O
.	O
The	O
underspecified	O
term	O
identifies	O
the	O
(	O
λ	O
-	O
bound	O
)	O
subject	O
x	O
of	O
the	O
upstairs	O
clause	O
as	O
its	O
antecedent	O
.	O
After	O
@	O
-	O
elimination	O
and	O
β	B-HyperparameterName
-	O
reduction	O
,	O
we	O
obtain	O
the	O
final	O
translation	O
in	O
(	O
25	O
)	O
,	O
which	O
corresponds	O
to	O
the	O
parallel	O
bound	O
reading	O
for	O
the	O
sentence	O
.	O
For	O
the	O
parallel	O
free	O
pronoun	O
reading	O
,	O
the	O
simpler	O
derivation	O
in	O
(	O
22	O
)	O
would	O
suffice	O
.	O
Since	O
β	B-HyperparameterName
-	O
reduction	O
is	O
prohibited	O
before	O
underspecification	O
resolution	O
,	O
type	O
checking	O
for	O
the	O
underspecified	O
term	O
searches	O
for	O
an	O
appropriate	O
antecedent	O
in	O
the	O
global	O
context	O
(	O
consisting	O
of	O
the	O
previous	O
linguistic	O
discourse	O
and	O
extra	O
-	O
linguistic	O
information	O
)	O
.	O
For	O
concreteness	O
,	O
we	O
assume	O
that	O
the	O
previous	O
utterance	O
was	O
Bobby	O
Fisher	O
is	O
a	O
famous	O
American	O
chess	O
player	O
,	O
and	O
that	O
the	O
judgement	O
bf	O
:	O
e	O
is	O
in	O
the	O
global	O
context	O
.	O
The	O
previous	O
sentence	O
thus	O
provides	O
an	O
antecedent	O
and	O
@	O
1	O
e	O
in	O
(	O
22	O
)	O
is	O
bound	O
to	O
bf	O
.	O
By	O
β	B-HyperparameterName
-	O
reducing	O
the	O
term	O
after	O
anaphora	O
resolution	O
,	O
we	O
obtain	O
(	O
26	O
)	O
,	O
where	O
the	O
pronoun	O
refers	O
to	O
Bobby	O
Fisher	O
in	O
each	O
conjunct	O
.	O
(	O
26	O
)	O
(	O
(	O
u	O
:	O
eng	O
*	O
)	O
think	O
(	O
gen	O
(	O
bf	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
)	O
×	O
(	O
(	O
u	O
:	O
am	O
*	O
)	O
believe	O
(	O
gen	O
(	O
bf	O
)	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
)	O
The	O
quantifier	O
scope	O
case	O
is	O
somewhat	O
different	O
at	O
the	O
level	O
of	O
technical	O
implementations	O
,	O
but	O
at	O
the	O
broader	O
conceptual	O
level	O
,	O
is	O
essentially	O
similar	O
to	O
the	O
pronoun	O
case	O
in	O
that	O
interpretive	O
parallelism	O
falls	O
out	O
from	O
the	O
constraints	O
pertaining	O
to	O
underspecification	O
resolution	O
in	O
the	O
derivation	O
of	O
compositional	O
semantics	O
.	O
Note	O
first	O
that	O
,	O
unlike	O
the	O
case	O
for	O
@	O
-	O
terms	O
,	O
we	O
do	O
n't	O
need	O
to	O
ensure	O
that	O
the	O
derivationally	O
obtained	O
local	O
context	O
is	O
'	O
large	O
enough	O
'	O
to	O
contain	O
the	O
'	O
antecedent	O
'	O
.	O
Thus	O
,	O
the	O
following	O
simple	O
derivation	O
suffices	O
to	O
yield	O
both	O
the	O
wide	O
-	O
scope	O
and	O
narrow	O
-	O
scope	O
readings	O
for	O
the	O
RNR'ed	O
indefinite	O
:	O
(	O
27	O
)	O
.	O
.	O
.	O
Since	O
#	O
-	O
terms	O
do	O
not	O
carry	O
indices	O
,	O
in	O
the	O
case	O
of	O
indefinites	O
,	O
interpretive	O
parallelism	O
follows	O
not	O
from	O
the	O
ban	O
on	O
duplicating	O
indexed	O
underspecified	O
terms	O
(	O
whose	O
role	O
was	O
to	O
ensure	O
'	O
construal	O
identity	O
'	O
in	O
anaphora	O
resolution	O
)	O
,	O
but	O
from	O
an	O
interaction	O
of	O
the	O
normal	O
form	O
requirement	O
for	O
derived	O
semantic	O
terms	O
and	O
the	O
locality	O
requirement	O
on	O
underspecification	O
resolution	O
encoded	O
in	O
the	O
#	O
-	O
elimination	O
rule	O
(	O
18	O
)	O
.	O
Specifically	O
,	O
there	O
are	O
two	O
possible	O
ways	O
for	O
resolving	O
underspecification	O
for	O
the	O
#	O
-	O
term	O
in	O
the	O
semantic	O
translation	O
for	O
the	O
sentence	O
obtained	O
at	O
the	O
final	O
line	O
of	O
(	O
27	O
)	O
.	O
If	O
we	O
resolve	O
underspecification	O
before	O
β	B-HyperparameterName
-	O
reducing	O
the	O
term	O
,	O
we	O
obtain	O
the	O
wide	O
scope	O
reading	O
for	O
the	O
indefinite	O
as	O
in	O
(	O
28	O
)	O
:	O
(	O
28	O
)	O
λx	O
[	O
(	O
v	O
:	O
(	O
u	O
:	O
boy	O
*	O
)	O
admire	O
(	O
x	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
×	O
(	O
(	O
u	O
:	O
girl	O
*	O
)	O
hate	O
(	O
x	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
]	O
(	O
#	O
sax	O
*	O
)	O
#	O
(	O
t	O
:	O
sax	O
*	O
)	O
×	O
λx	O
[	O
(	O
v	O
:	O
(	O
u	O
:	O
boy	O
*	O
)	O
admire	O
(	O
x	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
×	O
(	O
(	O
u	O
:	O
girl	O
*	O
)	O
hate	O
(	O
x	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
]	O
(	O
π	O
1	O
(	O
t	O
)	O
)	O
β	B-HyperparameterName
(	O
t	O
:	O
sax	O
*	O
)	O
×	O
[	O
(	O
v	O
:	O
(	O
u	O
:	O
boy	O
*	O
)	O
admire	O
(	O
π	O
1	O
(	O
t	O
)	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
×	O
(	O
(	O
u	O
:	O
girl	O
*	O
)	O
hate	O
(	O
π	O
1	O
(	O
t	O
)	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
]	O
If	O
,	O
on	O
the	O
other	O
hand	O
,	O
we	O
first	O
β	B-HyperparameterName
-	O
reduce	O
the	O
term	O
and	O
then	O
resolve	O
underspecification	O
,	O
the	O
Σ	O
-	O
type	O
that	O
has	O
the	O
existential	O
force	O
associated	O
with	O
the	O
indefinite	O
is	O
introduced	O
in	O
the	O
smallest	O
local	O
context	O
in	O
each	O
conjunct	O
,	O
via	O
(	O
18	O
)	O
.	O
In	O
this	O
case	O
,	O
the	O
distributive	O
,	O
narrow	O
scope	O
reading	O
obtains	O
for	O
the	O
sentence	O
.	O
(	O
29	O
)	O
λx	O
[	O
(	O
v	O
:	O
(	O
u	O
:	O
boy	O
*	O
)	O
admire	O
(	O
x	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
×	O
(	O
(	O
u	O
:	O
girl	O
*	O
)	O
hate	O
(	O
x	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
]	O
(	O
#	O
sax	O
*	O
)	O
β	B-HyperparameterName
(	O
v	O
:	O
(	O
u	O
:	O
boy	O
*	O
)	O
admire	O
(	O
#	O
sax	O
*	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
×	O
(	O
(	O
u	O
:	O
girl	O
*	O
)	O
hate	O
(	O
#	O
sax	O
*	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
#	O
(	O
v	O
:	O
(	O
u	O
:	O
boy	O
*	O
)	O
(	O
t	O
:	O
sax	O
*	O
)	O
×	O
admire	O
(	O
π	O
1	O
(	O
t	O
)	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
×	O
(	O
(	O
u	O
:	O
girl	O
*	O
)	O
(	O
t	O
:	O
sax	O
*	O
)	O
×	O
hate	O
(	O
π	O
1	O
(	O
t	O
)	O
)	O
(	O
π	O
1	O
(	O
u	O
)	O
)	O
)	O
One	O
may	O
wonder	O
at	O
this	O
point	O
why	O
we	O
impose	O
the	O
normal	O
form	O
requirement	O
on	O
compositionally	O
derived	O
semantic	O
terms	O
.	O
To	O
see	O
why	O
this	O
requirement	O
is	O
needed	O
,	O
assume	O
that	O
no	O
β	B-HyperparameterName
-	O
reduction	O
takes	O
place	O
in	O
the	O
course	O
of	O
the	O
derivataion	O
,	O
and	O
,	O
(	O
as	O
above	O
)	O
that	O
once	O
the	O
semantic	O
representation	O
for	O
the	O
whole	O
sentence	O
is	O
obtained	O
,	O
there	O
is	O
no	O
restriction	O
on	O
the	O
order	O
of	O
β	B-HyperparameterName
-	O
reduction	O
and	O
underspecification	O
resolution	O
for	O
#	O
terms	O
.	O
The	O
following	O
translation	O
would	O
then	O
be	O
assigned	O
to	O
the	O
sentence	O
,	O
and	O
via	O
the	O
underspecification	O
resolution	O
in	O
(	O
30	O
)	O
,	O
a	O
mixed	O
scope	O
reading	O
would	O
incorrectly	O
be	O
predicted	O
to	O
be	O
available	O
:	O
In	O
short	O
,	O
assumption	O
(	O
20b	O
)	O
has	O
the	O
effect	O
of	O
eliminating	O
unnecessary	O
'	O
traces	O
'	O
of	O
derivational	O
history	O
to	O
make	O
unavailable	O
intermediate	O
scope	O
positions	O
that	O
do	O
not	O
reflect	O
the	O
surface	O
form	O
of	O
the	O
sentence	O
.	O
Our	O
proposal	O
treats	O
indefinites	O
as	O
underspecified	O
terms	O
and	O
universals	O
as	O
true	O
quantifiers	O
,	O
and	O
in	O
this	O
respect	O
,	O
resembles	O
the	O
approach	O
by	O
Steedman	O
(	O
2012	O
)	O
.	O
Unlike	O
Steedman	O
's	O
approach	O
,	O
which	O
interleaves	O
underspecification	O
resolution	O
with	O
CCG	O
syntactic	O
combinatorics	O
,	O
our	O
approach	O
separates	O
semantic	O
underspecification	O
resolution	O
from	O
syntax	O
.	O
Nonetheless	O
,	O
the	O
similarity	O
between	O
the	O
two	O
is	O
striking	O
,	O
and	O
it	O
is	O
interesting	O
to	O
note	O
that	O
they	O
both	O
predict	O
that	O
mixed	O
readings	O
are	O
available	O
for	O
examples	O
involving	O
indefinites	O
as	O
subjects	O
and	O
a	O
universal	O
quantifier	O
in	O
the	O
RNR'ed	O
position	O
,	O
such	O
as	O
the	O
following	O
:	O
(	O
31	O
)	O
Some	O
boy	O
loves	O
,	O
and	O
some	O
girl	O
detests	O
,	O
every	O
saxophonist	O
.	O
The	O
judgments	O
are	O
somewhat	O
subtle	O
due	O
to	O
the	O
independent	O
pragmatic	O
preference	O
for	O
parallel	O
readings	O
,	O
but	O
we	O
follow	O
Steedman	O
(	O
2012	O
)	O
in	O
taking	O
this	O
prediction	O
to	O
be	O
essentially	O
correct	O
.	O
One	O
translation	O
that	O
our	O
analysis	O
can	O
assign	O
to	O
(	O
31	O
)	O
is	O
the	O
following	O
:	O
Here	O
,	O
β	B-HyperparameterName
-	O
conversion	O
for	O
the	O
λ	O
-	O
bound	O
variables	O
y	O
,	O
z	O
and	O
P	O
can	O
take	O
place	O
in	O
any	O
order	O
,	O
and	O
the	O
relative	O
scope	O
between	O
the	O
subject	O
indefinites	O
and	O
the	O
object	O
universal	O
depends	O
on	O
the	O
order	O
of	O
application	O
of	O
β	B-HyperparameterName
-	O
conversion	O
and	O
underspecification	O
resolution	O
for	O
the	O
two	O
terms	O
#	O
boy	O
and	O
#	O
girl	O
.	O

We	O
train	O
our	O
TOD	O
-	O
BERT	B-MethodName
based	O
on	O
BERT	B-MethodName
architecture	O
using	O
two	O
loss	B-MetricName
functions	O
:	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
loss	B-MetricName
and	O
response	O
contrastive	O
loss	B-MetricName
(	O
RCL	O
)	O
.	O
Note	O
that	O
the	O
datasets	O
we	O
used	O
can	O
be	O
used	O
to	O
pre	O
-	O
train	O
any	O
existing	O
language	O
model	O
architecture	O
,	O
and	O
here	O
we	O
select	O
BERT	B-MethodName
because	O
it	O
is	O
the	O
most	O
widely	O
used	O
model	O
in	O
NLP	O
research	O
.	O
We	O
use	O
the	O
BERT	B-MethodName
-	O
base	O
uncased	O
model	O
,	O
which	O
is	O
a	O
transformer	O
self	O
-	O
attention	O
encoder	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
with	O
12	O
layers	O
and	O
12	O
attention	O
heads	O
with	O
its	O
hidden	O
size	O
d	O
B	O
=	O
768	O
.	O
To	O
capture	O
speaker	O
information	O
and	O
the	O
underlying	O
interaction	O
behavior	O
in	O
dialogue	O
,	O
we	O
add	O
two	O
special	O
tokens	O
,	O
[	O
USR	O
]	O
and	O
[	O
SYS	O
]	O
,	O
to	O
the	O
bytepair	O
embeddings	O
(	O
Mrkšić	O
et	O
al	O
,	O
2016	O
)	O
.	O
We	O
prefix	O
the	O
special	O
token	O
to	O
each	O
user	O
utterance	O
and	O
system	O
response	O
,	O
and	O
concatenate	O
all	O
the	O
utterances	O
in	O
the	O
same	O
dialogue	O
into	O
one	O
flat	O
sequence	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
.	O
For	O
example	O
,	O
for	O
a	O
dialogue	O
D	O
=	O
{	O
S	O
1	O
,	O
U	O
1	O
,	O
.	O
.	O
.	O
,	O
S	O
n	O
,	O
U	O
n	O
}	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
dialogue	O
turns	O
and	O
each	O
S	O
i	O
or	O
U	O
i	O
contains	O
a	O
sequence	O
of	O
words	O
,	O
the	O
input	O
of	O
the	O
pre	O
-	O
training	O
model	O
is	O
processed	O
as	O
"	O
[	O
SYS	O
]	O
S	O
1	O
[	O
USR	O
]	O
U	O
1	O
.	O
.	O
.	O
"	O
with	O
standard	O
positional	O
embeddings	O
and	O
segmentation	O
embeddings	O
.	O
Masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
is	O
a	O
common	O
pretraining	O
strategy	O
for	O
BERT	B-MethodName
-	O
like	O
architectures	O
,	O
in	O
which	O
a	O
random	O
sample	O
of	O
tokens	O
in	O
the	O
input	O
sequence	O
is	O
selected	O
and	O
replaced	O
with	O
the	O
special	O
token	O
[	O
MASK	O
]	O
.	O
The	O
MLM	B-DatasetName
loss	B-MetricName
function	O
is	O
the	O
crossentropy	O
loss	B-MetricName
on	O
predicting	O
the	O
masked	O
tokens	O
.	O
In	O
the	O
original	O
implementation	O
,	O
random	O
masking	O
and	O
replacement	O
are	O
performed	O
once	O
in	O
the	O
beginning	O
and	O
saved	O
for	O
the	O
training	O
duration	O
.	O
Here	O
we	O
conduct	O
token	O
masking	O
dynamically	O
during	O
batch	O
training	O
.	O
TOD	O
-	O
BERT	B-MethodName
is	O
initialized	O
from	O
BERT	B-MethodName
,	O
a	O
good	O
starting	O
parameter	O
set	O
,	O
then	O
is	O
further	O
pre	O
-	O
trained	O
on	O
those	O
task	O
-	O
oriented	O
corpora	O
.	O
The	O
MLM	B-DatasetName
loss	B-MetricName
function	O
is	O
L	O
mlm	B-DatasetName
=	O
−	O
M	O
m=1	O
log	O
P	O
(	O
x	O
m	O
)	O
,	O
(	O
1	O
)	O
where	O
M	O
is	O
the	O
total	O
number	O
of	O
masked	O
tokens	O
and	O
P	O
(	O
x	O
m	O
)	O
is	O
the	O
predicted	O
probability	O
of	O
the	O
token	O
x	O
m	O
over	O
the	O
vocabulary	O
size	O
.	O
Response	O
contrastive	O
loss	B-MetricName
can	O
also	O
be	O
used	O
for	O
dialogue	O
language	O
modeling	O
since	O
it	O
does	O
not	O
require	O
any	O
additional	O
human	O
annotation	O
.	O
Pretraining	O
with	O
RCL	O
can	O
bring	O
us	O
several	O
advantages	O
:	O
1	O
)	O
we	O
can	O
learn	O
a	O
better	O
representation	O
for	O
the	O
[	O
CLS	O
]	O
token	O
,	O
as	O
it	O
is	O
essential	O
for	O
all	O
the	O
downstream	O
tasks	O
,	O
and	O
2	O
)	O
we	O
encourage	O
the	O
model	O
to	O
capture	O
underlying	O
dialogue	O
sequential	O
order	O
,	O
structure	O
information	O
,	O
and	O
response	O
similarity	O
.	O
Unlike	O
the	O
original	O
next	O
sentence	O
prediction	O
(	O
NSP	O
)	O
objective	O
in	O
BERT	B-MethodName
pre	O
-	O
training	O
,	O
which	O
concatenates	O
two	O
segments	O
A	O
and	O
B	O
to	O
predict	O
whether	O
they	O
are	O
consecutive	O
text	O
with	O
binary	O
classification	O
,	O
we	O
apply	O
a	O
dual	O
-	O
encoder	O
approach	O
(	O
Henderson	O
et	O
al	O
,	O
2019a	O
)	O
and	O
simulate	O
multiple	O
negative	O
samples	O
.	O
We	O
first	O
draw	O
a	O
batch	O
of	O
dialogues	O
{	O
D	O
1	O
,	O
.	O
.	O
.	O
,	O
D	O
b	O
}	O
and	O
split	O
each	O
dialogue	O
at	O
a	O
randomly	O
selected	O
turn	O
t.	O
For	O
example	O
,	O
D	O
1	O
will	O
be	O
separated	O
into	O
two	O
segments	O
,	O
one	O
is	O
the	O
context	O
{	O
S	O
1	O
1	O
,	O
U	O
1	O
1	O
,	O
.	O
.	O
.	O
,	O
S	O
1	O
t	O
,	O
U	O
1	O
t	O
}	O
and	O
the	O
other	O
is	O
the	O
response	O
{	O
S	O
1	O
t+1	O
}	O
.	O
We	O
use	O
TOD	O
-	O
BERT	B-MethodName
to	O
encode	O
all	O
the	O
contexts	O
and	O
their	O
corresponding	O
responses	O
separately	O
.	O
Afterwards	O
,	O
we	O
have	O
a	O
context	O
matrix	O
C	O
R	O
b×d	O
B	O
and	O
a	O
response	O
matrix	O
R	O
R	O
b×d	O
B	O
by	O
taking	O
the	O
output	O
[	O
CLS	O
]	O
representations	O
from	O
the	O
b	O
dialogues	O
.	O
We	O
treat	O
other	O
responses	O
in	O
the	O
same	O
batch	O
as	O
randomly	O
selected	O
negative	O
samples	O
.	O
The	O
RCL	O
objective	O
function	O
is	O
L	O
rcl	O
=	O
−	O
b	O
i=1	O
log	O
M	O
i	O
,	O
i	O
,	O
M	O
=	O
Softmax	B-MethodName
(	O
CR	O
T	O
)	O
R	O
b×b	O
.	O
(	O
2	O
)	O
Increasing	O
batch	B-HyperparameterName
size	I-HyperparameterName
to	O
a	O
certain	O
amount	O
can	O
obtain	O
better	O
performance	O
on	O
downstream	O
tasks	O
,	O
especially	O
for	O
the	O
response	O
selection	O
.	O
The	O
Softmax	B-MethodName
function	O
normalizes	O
the	O
vector	O
per	O
row	O
.	O
In	O
our	O
setting	O
,	O
increasing	O
batch	B-HyperparameterName
size	I-HyperparameterName
also	O
means	O
changing	O
the	O
positive	O
and	O
negative	O
ratio	O
in	O
the	O
contrastive	B-MethodName
learning	I-MethodName
.	O
Batch	B-HyperparameterName
size	I-HyperparameterName
is	O
a	O
hyper	O
-	O
parameter	O
that	O
may	O
be	O
limited	O
by	O
hardware	O
.	O
We	O
also	O
try	O
different	O
negative	O
sampling	O
strategies	O
during	O
pre	O
-	O
training	O
such	O
as	O
local	O
sampling	O
(	O
Saeidi	O
et	O
al	O
,	O
2017	O
)	O
,	O
but	O
do	O
not	O
observe	O
significant	O
change	O
compared	O
to	O
random	O
sampling	O
.	O
Overall	O
pre	O
-	O
training	O
loss	B-MetricName
function	O
is	O
the	O
weighted	O
-	O
sum	O
of	O
L	O
mlm	B-DatasetName
and	O
L	O
rcl	O
,	O
and	O
in	O
our	O
experiments	O
,	O
we	O
simply	O
sum	O
them	O
up	O
.	O
We	O
gradually	O
reduce	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
without	O
a	O
warm	O
-	O
up	O
period	O
.	O
We	O
train	O
TOD	O
-	O
BERT	B-MethodName
with	O
AdamW	B-MethodName
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2017	O
)	O
optimizer	B-HyperparameterName
with	O
a	O
dropout	O
ratio	O
of	O
0.1	O
on	O
all	O
layers	O
and	O
attention	O
weights	O
.	O
GELU	B-MethodName
activation	O
functions	O
(	O
Hendrycks	O
and	O
Gimpel	O
,	O
2016	O
)	O
is	O
used	O
.	O
Models	O
are	O
early	O
-	O
stopped	O
using	O
perplexity	B-MetricName
scores	O
of	O
a	O
held	O
-	O
out	O
development	O
set	O
,	O
with	O
mini	O
-	O
batches	O
containing	O
32	O
sequences	O
of	O
maximum	O
length	O
512	O
tokens	O
.	O
Experiments	O
are	O
conducted	O
on	O
two	O
NVIDIA	O
Tesla	O
V100	O
GPUs	O
.	O

We	O
care	O
the	O
most	O
in	O
this	O
paper	O
whether	O
TOD	O
-	O
BERT	B-MethodName
,	O
a	O
pre	O
-	O
trained	O
language	O
model	O
using	O
aggregated	O
taskoriented	O
corpora	O
,	O
can	O
show	O
any	O
advantage	O
over	O
BERT	B-MethodName
.	O
Therefore	O
,	O
we	O
avoid	O
adding	O
too	O
many	O
additional	O
components	O
on	O
top	O
of	O
its	O
architecture	O
when	O
fine	O
-	O
tuning	O
on	O
each	O
downstream	O
task	O
.	O
Also	O
,	O
we	O
use	O
the	O
same	O
architecture	O
with	O
a	O
similar	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
for	O
a	O
fair	O
comparison	O
.	O
All	O
the	O
model	O
parameters	O
are	O
updated	O
with	O
a	O
gradient	B-MethodName
clipping	I-MethodName
to	O
1.0	O
using	O
the	O
same	O
hyper	O
-	O
parameters	O
during	O
finetuning	O
.	O
We	O
select	O
four	O
crucial	O
task	O
-	O
oriented	O
downstream	O
tasks	O
to	O
evaluate	O
:	O
intent	B-TaskName
recognition	I-TaskName
,	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
,	O
dialogue	O
act	O
prediction	O
,	O
and	O
response	O
selection	O
.	O
All	O
of	O
them	O
are	O
core	O
components	O
in	O
modularized	O
task	O
-	O
oriented	O
systems	O
.	O
Intent	B-TaskName
recognition	I-TaskName
task	O
is	O
a	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
problem	O
,	O
where	O
we	O
input	O
a	O
sentence	O
U	O
and	O
models	O
predict	O
one	O
single	O
intent	O
class	O
over	O
I	O
possible	O
intents	O
.	O
P	O
int	O
=	O
Softmax	B-MethodName
(	O
W	O
1	O
(	O
F	O
(	O
U	O
)	O
)	O
)	O
R	O
I	O
,	O
(	O
3	O
)	O
where	O
F	O
is	O
a	O
pre	O
-	O
trained	O
language	O
model	O
and	O
we	O
use	O
its	O
[	O
CLS	O
]	O
embeddings	O
as	O
the	O
output	O
representation	O
.	O
W	O
1	O
R	O
I×d	O
B	O
is	O
a	O
trainable	O
linear	O
mapping	O
.	O
The	O
model	O
is	O
trained	O
with	O
cross	O
-	O
entropy	O
loss	B-MetricName
between	O
the	O
predicted	O
distributions	O
P	O
int	O
and	O
the	O
true	O
intent	O
labels	O
.	O
Dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
can	O
be	O
treated	O
as	O
a	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
problem	O
using	O
a	O
predefined	O
ontology	B-MethodName
.	O
Unlike	O
intent	O
,	O
we	O
use	O
dialogue	O
history	O
X	O
(	O
a	O
sequence	O
of	O
utterances	O
)	O
as	O
input	O
and	O
a	O
model	O
predicts	O
slot	O
values	O
for	O
each	O
(	O
domain	O
,	O
slot	O
)	O
pair	O
at	O
each	O
dialogue	O
turn	O
.	O
Each	O
corresponding	O
value	O
v	O
j	O
i	O
,	O
the	O
i	O
-	O
th	O
value	O
for	O
the	O
j	O
-	O
th	O
(	O
domain	O
,	O
slot	O
)	O
pair	O
,	O
is	O
passed	O
into	O
a	O
pre	O
-	O
trained	O
model	O
and	O
fixed	O
its	O
representation	O
during	O
training	O
.	O
S	O
j	O
i	O
=	O
Sim	O
(	O
G	O
j	O
(	O
F	O
(	O
X	O
)	O
)	O
,	O
F	O
(	O
v	O
j	O
i	O
)	O
)	O
R	O
1	O
,	O
(	O
4	O
)	O
where	O
Sim	O
is	O
the	O
cosine	O
similarity	O
function	O
,	O
and	O
S	O
j	O
R	O
|	O
v	O
j	O
|	O
is	O
the	O
probability	O
distribution	O
of	O
the	O
j	O
-	O
th	O
(	O
domain	O
,	O
slot	O
)	O
pair	O
over	O
its	O
possible	O
values	O
.	O
G	O
j	O
is	O
the	O
slot	O
projection	O
layer	O
of	O
the	O
j	O
slot	O
,	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
|	O
G	O
|	O
is	O
equal	O
to	O
the	O
number	O
of	O
(	O
domain	O
,	O
slot	O
)	O
pairs	O
.	O
The	O
model	O
is	O
trained	O
with	O
cross	O
-	O
entropy	O
loss	B-MetricName
summed	O
over	O
all	O
the	O
pairs	O
.	O
Dialogue	O
act	O
prediction	O
is	O
a	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
problem	O
because	O
a	O
system	O
response	O
may	O
contain	O
multiple	O
dialogue	O
acts	O
,	O
e.g.	O
,	O
request	O
and	O
inform	O
at	O
the	O
same	O
time	O
.	O
Model	O
take	O
dialogue	O
history	O
as	O
input	O
and	O
predict	O
a	O
binary	O
result	O
for	O
each	O
possible	O
dialogue	O
act	O
:	O
A	O
=	O
Sigmoid	O
(	O
W	O
2	O
(	O
F	O
(	O
X	O
)	O
)	O
)	O
R	O
N	O
,	O
(	O
5	O
)	O
where	O
W	O
2	O
R	O
d	O
B	O
×N	O
is	O
a	O
trainable	O
linear	O
mapping	O
,	O
N	O
is	O
the	O
number	O
of	O
possible	O
dialogue	O
acts	O
,	O
and	O
each	O
value	O
in	O
A	O
is	O
between	O
[	O
0	B-DatasetName
,	O
1	O
]	O
after	O
a	O
Sigmoid	O
layer	O
.	O
The	O
model	O
is	O
trained	O
with	O
binary	O
cross	O
-	O
entropy	O
loss	B-MetricName
and	O
the	O
i	O
-	O
th	O
dialogue	O
act	O
is	O
considered	O
as	O
a	O
triggered	O
dialogue	O
act	O
if	O
A	O
i	O
>	O
0.5	O
.	O
Response	O
selection	O
is	O
a	O
ranking	O
problem	O
,	O
aiming	O
to	O
retrieve	O
the	O
most	O
relative	O
system	O
response	O
from	O
a	O
candidate	O
pool	O
.	O
We	O
use	O
a	O
dual	O
-	O
encoder	O
strategy	O
(	O
Henderson	O
et	O
al	O
,	O
2019b	O
)	O
and	O
compute	O
similarity	O
scores	O
between	O
source	O
X	O
and	O
target	O
Y	O
,	O
r	O
i	O
=	O
Sim	O
(	O
F	O
(	O
X	O
)	O
,	O
F	O
(	O
Y	O
i	O
)	O
)	O
R	O
1	O
,	O
(	O
6	O
)	O
where	O
Y	O
i	O
is	O
the	O
i	O
-	O
th	O
response	O
candidate	O
and	O
r	O
i	O
is	O
its	O
cosine	O
similarity	O
score	O
.	O
Source	O
X	O
can	O
be	O
truncated	O
,	O
and	O
we	O
limit	O
the	O
context	O
lengths	O
to	O
the	O
most	O
recent	O
256	O
tokens	O
in	O
our	O
experiments	O
.	O
We	O
randomly	O
sample	O
several	O
system	O
responses	O
from	O
the	O
corpus	O
as	O
negative	O
samples	O
.	O
Although	O
it	O
may	O
not	O
be	O
a	O
true	O
negative	O
sample	O
,	O
it	O
is	O
common	O
to	O
train	O
a	O
ranker	O
and	O
evaluate	O
its	O
results	O
(	O
Henderson	O
et	O
al	O
,	O
2019a	O
)	O
.	O

For	O
each	O
downstream	O
task	O
,	O
we	O
first	O
conduct	O
the	O
experiments	O
using	O
the	O
whole	O
dataset	O
,	O
and	O
then	O
we	O
simulate	O
the	O
few	O
-	O
shot	O
setting	O
to	O
show	O
the	O
strength	O
of	O
our	O
TOD	O
-	O
BERT	B-MethodName
.	O
We	O
run	O
at	O
least	O
three	O
times	O
with	O
different	O
random	O
seeds	B-DatasetName
for	O
each	O
few	O
-	O
shot	O
experiment	O
to	O
reduce	O
data	O
sampling	O
variance	O
,	O
and	O
we	O
report	O
its	O
mean	O
and	O
standard	O
deviation	O
for	O
these	O
limited	O
data	O
scenarios	O
.	O
We	O
investigate	O
two	O
versions	O
of	O
TOD	O
-	O
BERT	B-MethodName
;	O
one	O
is	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
that	O
only	O
uses	O
MLM	B-DatasetName
loss	B-MetricName
during	O
pre	O
-	O
training	O
,	O
and	O
the	O
other	O
is	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
,	O
which	O
is	O
jointly	O
trained	O
with	O
the	O
MLM	B-DatasetName
and	O
RCL	O
objectives	O
.	O
We	O
compare	O
TOD	O
-	O
BERT	B-MethodName
with	O
BERT	B-MethodName
and	O
other	O
baselines	O
,	O
including	O
two	O
other	O
strong	O
pre	O
-	O
training	O
models	O
GPT	B-MethodName
-	O
2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
and	O
DialoGPT	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
a	O
GPT	B-MethodName
-	O
based	O
model	O
,	O
we	O
use	O
mean	O
pooling	O
of	O
its	O
hidden	O
states	O
as	O
its	O
output	O
representation	O
,	O
which	O
we	O
found	O
it	O
is	O
better	O
than	O
using	O
only	O
the	O
last	O
token	O
.	O

TOD	O
-	O
BERT	B-MethodName
outperforms	O
BERT	B-MethodName
and	O
other	O
strong	O
baselines	O
in	O
one	O
of	O
the	O
largest	O
intent	B-TaskName
recognition	I-TaskName
Model	O
Acc	B-MetricName
(	O
all	O
)	O
Acc	B-MetricName
(	O
in	O
)	O
Acc	B-MetricName
(	O
out	O
)	O
Recall	B-MetricName
(	O
out	O
)	O
1	O
-	O
Shot	O
BERT	B-MethodName
29.3	O
%	O
±	O
3.4	O
%	O
35.7	O
%	O
±	O
4.1	O
%	O
81.3	O
%	O
±	O
0.4	O
%	O
0.4	O
%	O
±	O
0.3	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
38.9	O
%	O
±	O
6.3	O
%	O
47.4	O
%	O
±	O
7.6	O
%	O
81.6	O
%	O
±	O
0.2	O
%	O
0.5	O
%	O
±	O
0.2	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
42.5	O
%	O
±	O
0.1	O
%	O
52.0	O
%	O
±	O
0.1	O
%	O
81.7	O
%	O
±	O
0.1	O
%	O
0.1	O
%	O
±	O
0.1	O
%	O
10	O
-	O
Shot	O
BERT	B-MethodName
75.5	O
%	O
±	O
1.1	O
%	O
88.6	O
%	O
±	O
1.1	O
%	O
84.7	O
%	O
±	O
0.3	O
%	O
16.5	O
%	O
±	O
1.7	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
76.6	O
%	O
±	O
0.8	O
%	O
90.5	O
%	O
±	O
1.2	O
%	O
84.3	O
%	O
±	O
0.2	O
%	O
14.0	O
%	O
±	O
1.3	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
77.3	O
%	O
±	O
0.5	O
%	O
91.0	O
%	O
±	O
0.5	O
%	O
84.5	O
%	O
±	O
0.4	O
%	O
15.3	O
%	O
±	O
2.1	O
%	O
datasets	O
,	O
as	O
shown	O
in	O
Table	O
2	O
.	O
We	O
evaluate	O
accuracy	B-MetricName
on	O
all	O
the	O
data	O
,	O
the	O
in	O
-	O
domain	O
intents	O
only	O
,	O
and	O
the	O
out	O
-	O
of	O
-	O
scope	O
intent	O
only	O
.	O
Note	O
that	O
there	O
are	O
two	O
ways	O
to	O
predict	O
out	O
-	O
of	O
-	O
scope	O
intent	O
,	O
one	O
is	O
to	O
treat	O
it	O
as	O
an	O
additional	O
class	O
,	O
and	O
the	O
other	O
is	O
to	O
set	O
a	O
threshold	O
for	O
prediction	O
confidence	O
.	O
Here	O
we	O
report	O
the	O
results	O
of	O
the	O
first	O
setting	O
.	O
TOD	O
-	O
BERTjnt	O
achieves	O
the	O
highest	O
in	O
-	O
scope	O
and	O
out	O
-	O
of	O
-	O
scope	O
accuracy	B-MetricName
.	O
Besides	O
,	O
we	O
conduct	O
1	O
-	O
shot	O
and	O
10	O
-	O
shot	O
experiments	O
by	O
randomly	O
sampling	O
one	O
and	O
ten	O
utterances	O
from	O
each	O
intent	O
class	O
in	O
the	O
training	O
set	O
.	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
has	O
13.2	O
%	O
all	O
-	O
intent	O
accuracy	B-MetricName
improvement	O
and	O
16.3	O
%	O
in	O
-	O
domain	O
accuracy	B-MetricName
improvement	O
compared	O
to	O
BERT	B-MethodName
in	O
the	O
1	O
-	O
shot	O
setting	O
.	O

Two	O
evaluation	O
metrics	O
are	O
commonly	O
used	O
in	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
task	O
:	O
joint	O
goal	O
accuracy	B-MetricName
and	O
slot	O
accuracy	B-MetricName
.	O
The	O
joint	O
goal	O
accuracy	B-MetricName
compares	O
the	O
predicted	O
dialogue	O
states	O
to	O
the	O
ground	O
truth	O
at	O
each	O
dialogue	O
turn	O
.	O
The	O
ground	O
truth	O
includes	O
slot	O
values	O
for	O
all	O
the	O
possible	O
(	O
domain	O
,	O
slot	O
)	O
pairs	O
.	O
The	O
output	O
is	O
considered	O
as	O
a	O
correct	O
prediction	O
if	O
and	O
only	O
if	O
all	O
the	O
predicted	O
values	O
exactly	O
match	O
its	O
ground	O
truth	O
values	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
slot	O
accuracy	B-MetricName
individually	O
compares	O
each	O
(	O
domain	O
,	O
slot	O
,	O
value	O
)	O
triplet	O
to	O
its	O
ground	O
truth	O
label	O
.	O
In	O
Table	O
5	O
,	O
we	O
compare	O
BERT	B-MethodName
to	O
TOD	O
-	O
BERTjnt	O
on	O
the	O
MWOZ	O
2.1	O
dataset	O
and	O
find	O
the	O
latter	O
has	O
2.4	O
%	O
joint	O
goal	O
accuracy	B-MetricName
improvement	O
.	O
Since	O
the	O
original	O
ontology	B-MethodName
provided	O
by	O
Budzianowski	O
et	O
al	O
(	O
2018	O
)	O
is	O
not	O
complete	O
(	O
some	O
labeled	O
values	O
are	O
not	O
included	O
in	O
the	O
ontology	B-MethodName
)	O
,	O
we	O
create	O
a	O
new	O
ontology	B-MethodName
of	O
all	O
the	O
possible	O
annotated	O
values	O
.	O
We	O
also	O
list	O
several	O
well	O
-	O
known	O
dialogue	O
state	O
trackers	O
as	O
reference	O
,	O
including	O
DSTReader	O
,	O
HyST	O
,	O
TRADE	O
,	O
and	O
ZSDST	O
(	O
Rastogi	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
also	O
report	O
the	O
few	O
-	O
shot	O
experiments	O
using	O
1	O
%	O
,	O
5	O
%	O
,	O
10	O
%	O
,	O
and	O
25	O
%	O
data	O
.	O
Note	O
that	O
1	O
%	O
of	O
data	O
has	O
around	O
84	O
dialogues	O
.	O
TOD	O
-	O
BERT	B-MethodName
outperforms	O
BERT	B-MethodName
in	O
all	O
the	O
setting	O
,	O
which	O
further	O
show	O
the	O
strength	O
of	O
task	O
-	O
oriented	O
dialogue	O
pre	O
-	O
training	O
.	O

We	O
conduct	O
experiments	O
on	O
three	O
different	O
datasets	O
and	O
report	O
micro	B-MetricName
-	I-MetricName
F1	I-MetricName
and	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
scores	O
for	O
the	O
dialogue	O
act	O
prediction	O
task	O
,	O
a	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
problem	O
.	O
For	O
the	O
MWOZ	O
dataset	O
,	O
we	O
remove	O
the	O
domain	O
information	O
from	O
the	O
original	O
system	O
dialogue	O
act	O
labels	O
.	O
For	O
example	O
,	O
the	O
"	O
taxi	O
-	O
inform	O
"	O
will	O
be	O
simplified	O
to	O
"	O
inform	O
"	O
.	O
This	O
process	O
reduces	O
the	O
number	O
of	O
possible	O
dialogue	O
acts	O
from	O
31	O
to	O
13	O
.	O
For	O
DSTC2	O
and	O
GSIM	O
corpora	O
,	O
we	O
follow	O
to	O
apply	O
universal	O
dialogue	O
act	O
mapping	O
that	O
maps	O
the	O
original	O
dialogue	O
act	O
labels	O
to	O
a	O
general	O
dialogue	O
act	O
format	O
,	O
resulting	O
in	O
9	O
and	O
6	O
unique	O
system	O
dialogue	O
acts	O
in	O
DSTC2	O
and	O
GSIM	O
,	O
respectively	O
.	O
We	O
run	O
two	O
other	O
baselines	O
,	O
MLP	B-DatasetName
and	O
RNN	O
,	O
to	O
further	O
show	O
the	O
strengths	O
of	O
BERT	B-MethodName
-	O
based	O
MWOZ	O
(	O
13	O
)	O
DSTC2	O
(	O
9	O
)	O
GSIM	O
(	O
6	O
)	O
micro	B-MetricName
-	I-MetricName
F1	I-MetricName
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
micro	B-MetricName
-	I-MetricName
F1	I-MetricName
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
micro	B-MetricName
-	I-MetricName
F1	I-MetricName
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
1	O
%	O
Data	O
BERT	B-MethodName
84.0	O
%	O
±	O
0.6	O
%	O
66.7	O
%	O
±	O
1.7	O
%	O
77.1	O
%	O
±	O
2.1	O
%	O
25.8	O
%	O
±	O
0.8	O
%	O
67.3	O
%	O
±	O
1.4	O
%	O
26.9	O
%	O
±	O
1.0	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
87.5	O
%	O
±	O
0.6	O
%	O
73.3	O
%	O
±	O
1.5	O
%	O
79.6	O
%	O
±	O
1.0	O
%	O
26.4	O
%	O
±	O
0.5	O
%	O
82.7	O
%	O
±	O
0.7	O
%	O
35.7	O
%	O
±	O
0.3	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
86.9	O
%	O
±	O
0.2	O
%	O
72.4	O
%	O
±	O
0.8	O
%	O
82.9	O
%	O
±	O
0.4	O
%	O
28.0	O
%	O
±	O
0.1	O
%	O
78.4	O
%	O
±	O
3.2	O
%	O
32.9	O
%	O
±	O
2.1	O
%	O
10	O
%	O
Data	O
BERT	B-MethodName
89.7	O
%	O
±	O
0.2	O
%	O
78.4	O
%	O
±	O
0.3	O
%	O
88.2	O
%	O
±	O
0.7	O
%	O
34.8	O
%	O
±	O
1.3	O
%	O
98.4	O
%	O
±	O
0.3	O
%	O
45.1	O
%	O
±	O
0.2	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
90.1	O
%	O
±	O
0.2	O
%	O
78.9	O
%	O
±	O
0.1	O
%	O
91.8	O
%	O
±	O
1.7	O
%	O
39.4	O
%	O
±	O
1.7	O
%	O
99.2	O
%	O
±	O
0.1	O
%	O
45.6	O
%	O
±	O
0.1	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
90.2	O
%	O
±	O
0.2	O
%	O
79.6	O
%	O
±	O
0.7	O
%	O
90.6	O
%	O
±	O
3.2	O
%	O
38.8	O
%	O
±	O
2.2	O
%	O
99.3	O
%	O
±	O
0.1	O
%	O
45.7	O
%	O
±	O
0.0	O
%	O

Slot	O
Acc	B-MetricName
1	O
%	O
Data	O
BERT	B-MethodName
6.4	O
%	O
±	O
1.4	O
%	O
84.4	O
%	O
±	O
1.0	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
9.9	O
%	O
±	O
0.6	O
%	O
86.6	O
%	O
±	O
0.5	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
8.0	O
%	O
±	O
1.0	O
%	O
85.3	O
%	O
±	O
0.4	O
%	O
5	O
%	O
Data	O
BERT	B-MethodName
19.6	O
%	O
±	O
0.1	O
%	O
92.0	O
%	O
±	O
0.5	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
28.1	O
%	O
±	O
1.6	O
%	O
93.9	O
%	O
±	O
0.1	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
28.6	O
%	O
±	O
1.4	O
%	O
93.8	O
%	O
±	O
0.3	O
%	O
10	O
%	O
Data	O
BERT	B-MethodName
32.9	O
%	O
±	O
0.6	O
%	O
94.7	O
%	O
±	O
0.1	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
39.5	O
%	O
±	O
0.7	O
%	O
95.6	O
%	O
±	O
0.1	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
37.0	O
%	O
±	O
0.1	O
%	O
95.2	O
%	O
±	O
0.1	O
%	O
25	O
%	O
Data	O
BERT	B-MethodName
40.8	O
%	O
±	O
1.0	O
%	O
95.8	O
%	O
±	O
0.1	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
44.0	O
%	O
±	O
0.4	O
%	O
96.4	O
%	O
±	O
0.1	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
44.3	O
%	O
±	O
0.3	O
%	O
96.3	O
%	O
±	O
0.2	O
%	O
models	O
.	O
The	O
MLP	B-DatasetName
model	O
simply	O
takes	O
bag	O
-	O
of	O
-	O
word	B-TaskName
embeddings	I-TaskName
to	O
make	O
dialogue	O
act	O
prediction	O
,	O
and	O
the	O
RNN	O
model	O
is	O
a	O
bi	O
-	O
directional	O
GRU	B-MethodName
network	O
.	O
In	O
Table	O
4	O
,	O
one	O
can	O
observe	O
that	O
in	O
full	O
data	O
scenario	O
,	O
TOD	O
-	O
BERT	B-MethodName
consistently	O
works	O
better	O
than	O
BERT	B-MethodName
and	O
other	O
baselines	O
,	O
no	O
matter	O
which	O
datasets	O
or	O
which	O
evaluation	O
metrics	O
.	O
In	O
the	O
fewshot	O
experiments	O
,	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
outperforms	O
BERT	B-MethodName
by	O
3.5	O
%	O
micro	B-MetricName
-	I-MetricName
F1	I-MetricName
and	O
6.6	O
%	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
on	O
MWOZ	O
corpus	O
in	O
the	O
1	O
%	O
data	O
scenario	O
.	O
We	O
also	O
found	O
that	O
10	O
%	O
of	O
training	O
data	O
can	O
achieve	O
good	O
performance	O
that	O
is	O
close	O
to	O
full	O
data	O
training	O
.	O

To	O
evaluate	O
response	O
selection	O
in	O
task	O
-	O
oriented	O
dialogues	O
,	O
we	O
follow	O
the	O
k	O
-	O
to	O
-	O
100	O
accuracy	B-MetricName
,	O
which	O
is	O
becoming	O
a	O
research	O
community	O
standard	O
(	O
Yang	O
et	O
al	O
,	O
2018	O
;	O
Henderson	O
et	O
al	O
,	O
2019a	O
)	O
.	O
The	O
k	O
-	O
of	O
-	O
100	O
MWOZ	O
DSTC2	O
GSIM	O
1	O
-	O
to	O
-	O
100	O
3	O
-	O
to	O
-	O
100	O
1	O
-	O
to	O
-	O
100	O
3	O
-	O
to	O
-	O
100	O
1	O
-	O
to	O
-	O
100	O
3	O
-	O
to	O
-	O
100	O
1	O
%	O
Data	O
BERT	B-MethodName
7.8	O
%	O
±	O
2.0	O
%	O
20.5	O
%	O
±	O
4.4	O
%	O
3.7	O
%	O
±	O
0.6	O
%	O
9.6	O
%	O
±	O
1.3	O
%	O
4.0	O
%	O
±	O
0.4	O
%	O
10.3	O
%	O
±	O
1.1	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
13.0	O
%	O
±	O
1.1	O
%	O
34.6	O
%	O
±	O
0.4	O
%	O
12.5	O
%	O
±	O
6.7	O
%	O
24.9	O
%	O
±	O
10.7	O
%	O
7.2	O
%	O
±	O
4.0	O
%	O
15.4	O
%	O
±	O
8.0	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
-	O
-	O
37.5	O
%	O
±	O
0.6	O
%	O
55.9	O
%	O
±	O
0.4	O
%	O
12.5	O
%	O
±	O
0.9	O
%	O
26.8	O
%	O
±	O
0.8	O
%	O
10	O
%	O
Data	O
BERT	B-MethodName
20.9	O
%	O
±	O
2.6	O
%	O
45.4	O
%	O
±	O
3.8	O
%	O
8.9	O
%	O
±	O
2.3	O
%	O
21.4	O
%	O
±	O
3.1	O
%	O
9.8	O
%	O
±	O
0.1	O
%	O
24.4	O
%	O
±	O
1.2	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
22.3	O
%	O
±	O
3.2	O
%	O
48.7	O
%	O
±	O
4.0	O
%	O
19.0	O
%	O
±	O
16.3	O
%	O
33.8	O
%	O
±	O
20.4	O
%	O
11.2	O
%	O
±	O
2.5	O
%	O
26.0	O
%	O
±	O
2.7	O
%	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
-	O
-	O
49.7	O
%	O
±	O
0.3	O
%	O
66.6	O
%	O
±	O
0.1	O
%	O
23.0	O
%	O
±	O
1.0	O
%	O
42.6	O
%	O
±	O
1.0	O
%	O
Table	O
6	O
:	O
Response	O
selection	O
evaluation	O
results	O
on	O
three	O
corpora	O
for	O
1	O
%	O
,	O
10	O
%	O
and	O
full	O
data	O
setting	O
.	O
We	O
report	O
1	O
-	O
to	O
-	O
100	O
and	O
3	O
-	O
to	O
-	O
100	O
accuracy	B-MetricName
,	O
which	O
is	O
similar	O
to	O
recall1	O
and	O
recall@3	O
given	O
100	O
candidates	O
.	O
metric	O
is	O
computed	O
using	O
a	O
random	O
batch	O
of	O
100	O
examples	O
so	O
that	O
responses	O
from	O
other	O
examples	O
in	O
the	O
same	O
batch	O
can	O
be	O
used	O
as	O
random	O
negative	O
candidates	O
.	O
This	O
allows	O
us	O
to	O
be	O
compute	O
the	O
metric	O
across	O
many	O
examples	O
in	O
batches	O
efficiently	O
.	O
While	O
it	O
is	O
not	O
guaranteed	O
that	O
the	O
random	O
negatives	O
will	O
indeed	O
be	O
"	O
true	O
"	O
negatives	O
,	O
the	O
1	O
-	O
of	O
-	O
100	O
metric	O
still	O
provides	O
a	O
useful	O
evaluation	O
signal	O
.	O
During	O
inference	O
,	O
we	O
run	O
five	O
different	O
random	O
seeds	B-DatasetName
to	O
sample	O
batches	O
and	O
report	O
the	O
average	O
results	O
.	O
In	O
Table	O
6	O
,	O
we	O
conduct	O
response	O
selection	O
experiments	O
on	O
three	O
datasets	O
,	O
MWOZ	O
,	O
DSTC2	O
,	O
and	O
GSIM	O
.	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
achieves	O
65.8	O
%	O
1	O
-	O
to	O
-	O
100	O
accuracy	B-MetricName
and	O
87.0	O
%	O
3	O
-	O
to	O
-	O
100	O
accuracy	B-MetricName
on	O
MWOZ	O
,	O
which	O
surpasses	O
BERT	B-MethodName
by	O
18.3	O
%	O
and	O
11.5	O
%	O
,	O
respectively	O
.	O
The	O
similar	O
results	O
are	O
also	O
consistently	O
observed	O
in	O
DSTC2	O
and	O
GSIM	O
datasets	O
,	O
and	O
the	O
advantage	O
of	O
the	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
is	O
more	O
evident	O
in	O
the	O
few	O
-	O
shot	O
scenario	O
.	O
We	O
do	O
not	O
report	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
for	O
MWOZ	O
few	O
-	O
shot	O
setting	O
because	O
it	O
is	O
not	O
fair	O
to	O
compare	O
them	O
with	O
others	O
as	O
the	O
full	O
MWOZ	O
training	O
set	O
is	O
used	O
for	O
response	O
contrastive	B-MethodName
learning	I-MethodName
during	O
pre	O
-	O
training	O
stage	O
.	O
The	O
response	O
selection	O
results	O
are	O
sensitive	O
to	O
the	O
training	O
batch	B-HyperparameterName
size	I-HyperparameterName
since	O
the	O
larger	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
the	O
harder	O
the	O
prediction	O
.	O
In	O
our	O
experiments	O
,	O
we	O
set	O
batch	B-HyperparameterName
size	I-HyperparameterName
equals	O
to	O
25	O
for	O
all	O
the	O
models	O
.	O

In	O
Figure	O
2	O
,	O
we	O
visualize	O
the	O
embeddings	O
of	O
BERT	B-MethodName
,	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
,	O
and	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
given	O
the	O
same	O
input	O
from	O
the	O
MWOZ	O
test	O
set	O
.	O
Each	O
sample	O
point	O
is	O
a	O
system	O
response	O
representation	O
,	O
which	O
is	O
passed	O
through	O
a	O
pre	O
-	O
trained	O
model	O
and	O
reduced	O
its	O
high	O
-	O
dimension	O
features	O
to	O
a	O
two	O
-	O
dimension	O
point	O
using	O
the	O
t	O
-	O
distributed	O
stochastic	O
neighbor	O
embedding	O
(	O
tSNE	O
)	O
for	O
dimension	O
reduction	O
.	O
Since	O
we	O
know	O
the	O
true	O
domain	O
and	O
dialogue	O
act	O
labels	O
for	O
each	O
utterance	O
,	O
we	O
use	O
different	O
colors	O
to	O
represent	O
different	O
domains	O
and	O
dialogue	O
acts	O
.	O
As	O
one	O
can	O
observe	O
,	O
TOD	O
-	O
BERT	B-MethodName
-	O
jnt	O
has	O
more	O
clear	O
group	O
boundaries	O
than	O
TOD	O
-	O
BERT	B-MethodName
-	O
mlm	B-DatasetName
,	O
and	O
two	O
of	O
them	O
are	O
better	O
than	O
BERT	B-MethodName
.	O
To	O
analyze	O
the	O
results	O
quantitatively	O
,	O
we	O
run	O
Kmeans	O
,	O
a	O
common	O
unsupervised	O
clustering	O
algorithms	O
,	O
on	O
top	O
of	O
the	O
output	O
embeddings	O
of	O
BERT	B-MethodName
and	O
TOD	O
-	O
BERT	B-MethodName
.	O
We	O
set	O
K	O
for	O
K	O
-	O
means	O
equal	O
to	O
10	O
and	O
20	O
.	O
After	O
the	O
clustering	O
,	O
we	O
can	O
assign	O
each	O
utterance	O
in	O
the	O
MWOZ	O
test	O
set	O
to	O
a	O
predicted	O
class	O
.	O
We	O
then	O
compute	O
the	O
normalized	O
mutual	O
information	O
(	O
NMI	B-MetricName
)	O
between	O
the	O
clustering	O
result	O
and	O
the	O
actual	O
domain	O
label	O
for	O
each	O
utterance	O
.	O
Here	O
is	O
what	O
we	O
observe	O
:	O
TOD	O
-	O
BERT	B-MethodName
consistently	O
achieves	O
higher	O
NMI	B-MetricName
scores	O
than	O
BERT	B-MethodName
.	O
For	O
K=10	O
,	O
TOD	O
-	O
BERT	B-MethodName
has	O
a	O
0.143	O
NMI	B-MetricName
score	O
,	O
and	O
BERT	B-MethodName
only	O
has	O
0.094	O
.	O
For	O
K=20	O
,	O
TOD	O
-	O
BERT	B-MethodName
achieves	O
a	O
0.213	O
NMI	B-MetricName
score	O
,	O
while	O
BERT	B-MethodName
has	O
0.109	O
.	O

Vanilla	O
Transformer	B-MethodName
.	O
We	O
describe	O
the	O
original	O
Transformer	B-MethodName
architecture	O
with	O
positional	O
encoding	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
as	O
formalized	O
by	O
Pérez	O
et	O
al	O
(	O
2019	O
)	O
,	O
with	O
some	O
modifications	O
.	O
All	O
vectors	O
in	O
this	O
subsection	O
are	O
from	O
Q	O
d	O
.	O
The	O
transformer	O
,	O
denoted	O
Trans	O
,	O
is	O
a	O
seq	O
-	O
to	O
-	O
seq	O
architecture	O
.	O
Its	O
input	O
consists	O
of	O
(	O
i	O
)	O
a	O
sequence	O
X	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
of	O
vectors	O
,	O
(	O
ii	O
)	O
a	O
seed	O
vector	O
y	O
0	B-DatasetName
.	O
The	O
output	O
is	O
a	O
sequence	O
Y	O
=	O
(	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
r	O
)	O
of	O
vectors	O
.	O
The	O
sequence	O
X	O
is	O
obtained	O
from	O
the	O
sequence	O
(	O
s	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
n	O
)	O
Σ	O
n	O
of	O
symbols	O
by	O
using	O
the	O
embedding	O
mentioned	O
earlier	O
:	O
x	O
i	O
=	O
f	O
(	O
f	O
b	O
(	O
s	O
i	O
)	O
,	O
pos	O
(	O
i	O
)	O
)	O
.	O
The	O
transformer	O
consists	O
of	O
composition	O
of	O
transformer	O
encoder	O
and	O
transformer	B-MethodName
decoder	I-MethodName
.	O
For	O
the	O
feedforward	O
networks	O
in	O
the	O
transformer	O
layers	O
we	O
use	O
the	O
activation	O
as	O
in	O
Siegelmann	O
and	O
Sontag	O
(	O
1992	O
)	O
,	O
namely	O
the	O
saturated	O
linear	O
activation	B-HyperparameterName
function	I-HyperparameterName
σ	O
(	O
x	O
)	O
which	O
takes	O
value	O
0	B-DatasetName
for	O
x	O
<	O
0	B-DatasetName
,	O
value	O
x	O
for	O
0	B-DatasetName
<	O
x	O
<	O
1	O
and	O
value	O
1	O
for	O
x	O
>	O
1	O
.	O
This	O
activation	O
can	O
be	O
easily	O
replaced	O
by	O
the	O
standard	O
ReLU	B-MethodName
activation	O
via	O
σ	O
(	O
x	O
)	O
=	O
ReLU	B-MethodName
(	O
x	O
)	O
−	O
ReLU	B-MethodName
(	O
x	O
−	O
1	O
)	O
.	O
Self	O
-	O
attention	O
.	O
The	O
self	O
-	O
attention	O
mechanism	O
takes	O
as	O
input	O
(	O
i	O
)	O
a	O
query	O
vector	O
q	O
,	O
(	O
ii	O
)	O
a	O
sequence	O
of	O
key	O
vectors	O
K	B-HyperparameterName
=	I-HyperparameterName
(	O
k	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
n	O
)	O
,	O
and	O
(	O
iii	O
)	O
a	O
sequence	O
of	O
value	O
vectors	O
V	O
=	O
(	O
v	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
n	O
)	O
.	O
The	O
q	O
-	O
attention	O
over	O
K	O
and	O
V	O
,	O
denoted	O
Att	O
(	O
q	O
,	O
K	O
,	O
V	O
)	O
,	O
is	O
a	O
vector	O
a	O
=	O
α	B-HyperparameterName
1	O
v	O
1	O
+	O
α	B-HyperparameterName
2	O
v	O
2	O
+	O
+	O
α	B-HyperparameterName
n	O
v	O
n	O
,	O
where	O
(	O
i	O
)	O
(	O
α	B-HyperparameterName
1	O
,	O
.	O
.	O
.	O
,	O
α	B-HyperparameterName
n	O
)	O
=	O
ρ	O
(	O
f	O
att	O
(	O
q	O
,	O
k	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
f	O
att	O
(	O
q	O
,	O
k	O
n	O
)	O
)	O
.	O
(	O
ii	O
)	O
The	O
normalization	O
function	O
ρ	O
:	O
Q	O
n	O
Q	O
n	O
≥0	O
is	O
hardmax	O
:	O
for	O
x	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
Q	O
n	O
,	O
if	O
the	O
maximum	O
value	O
occurs	O
r	O
times	O
among	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
,	O
then	O
hardmax	O
(	O
x	O
)	O
i	O
:	O
=	O
1	O
/	O
r	O
if	O
x	O
i	O
is	O
a	O
maximum	O
value	O
and	O
hardmax	O
(	O
x	O
)	O
i	O
:	O
=	O
0	B-DatasetName
otherwise	O
.	O
In	O
practice	O
,	O
the	O
softmax	B-MethodName
is	O
often	O
used	O
but	O
its	O
output	O
values	O
are	O
in	O
general	O
not	O
rational	O
.	O
(	O
iii	O
)	O
For	O
vanilla	O
transformers	O
,	O
the	O
scoring	O
function	O
f	O
att	O
used	O
is	O
a	O
combination	O
of	O
multiplicative	B-MethodName
attention	I-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
and	O
a	O
non	O
-	O
linear	O
function	O
:	O
f	O
att	O
(	O
q	O
,	O
k	O
i	O
)	O
=	O
−	O
q	O
,	O
k	O
i	O
.	O
This	O
was	O
also	O
used	O
by	O
Pérez	O
et	O
al	O
(	O
2019	O
)	O
.	O
Transformer	B-MethodName
encoder	O
.	O
A	O
single	O
-	O
layer	O
encoder	O
is	O
a	O
function	O
Enc	O
(	O
X	O
;	O
θ	B-HyperparameterName
)	O
,	O
with	O
input	O
X	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
a	O
sequence	O
of	O
vectors	O
in	O
Q	O
d	O
,	O
and	O
parameters	O
θ	B-HyperparameterName
.	O
The	O
output	O
is	O
another	O
sequence	O
Z	O
=	O
(	O
z	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
n	O
)	O
of	O
vectors	O
in	O
Q	O
d	O
.	O
The	O
parame	O
-	O
ters	O
θ	B-HyperparameterName
specify	O
functions	O
Q	O
(	O
)	O
,	O
K	O
(	O
)	O
,	O
V	O
(	O
)	O
,	O
and	O
O	O
(	O
)	O
,	O
all	O
of	O
type	O
Q	O
d	O
Q	O
d	O
.	O
The	O
functions	O
Q	O
(	O
)	O
,	O
K	O
(	O
)	O
,	O
and	O
V	O
(	O
)	O
are	O
linear	O
transformations	O
and	O
O	O
(	O
)	O
an	O
FFN	O
.	O
For	O
1	O
≤	O
i	O
≤	O
n	O
,	O
the	O
output	O
of	O
the	O
self	O
-	O
attention	O
block	O
is	O
produced	O
by	O
a	O
i	O
=	O
Att	O
(	O
Q	O
(	O
x	O
i	O
)	O
,	O
K	O
(	O
X	O
)	O
,	O
V	O
(	O
X	O
)	O
)	O
+	O
x	O
i	O
(	O
1	O
)	O
This	O
operation	O
is	O
also	O
referred	O
to	O
as	O
the	O
encoderencoder	O
attention	O
block	O
.	O
The	O
output	O
Z	O
is	O
computed	O
by	O
z	O
i	O
=	O
O	O
(	O
a	O
i	O
)	O
+	O
a	O
i	O
for	O
1	O
≤	O
i	O
≤	O
n.	O
The	O
addition	O
operations	O
+	O
x	O
i	O
and	O
+	O
a	O
i	O
are	O
the	O
residual	O
connections	O
.	O
The	O
complete	O
L	O
-	O
layer	O
transformer	O
encoder	O
TEnc	O
(	O
L	O
)	O
(	O
X	O
;	O
θ	B-HyperparameterName
)	O
=	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
has	O
the	O
same	O
input	O
X	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
as	O
the	O
single	O
-	O
layer	O
encoder	O
.	O
In	O
contrast	O
,	O
its	O
output	O
K	O
e	O
=	O
(	O
k	O
e	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
e	O
n	O
)	O
and	O
V	O
e	O
=	O
(	O
v	O
e	O
1	O
,	O
.	O
.	O
.	O
v	O
e	O
n	O
)	O
contains	O
two	O
sequences	O
.	O
TEnc	O
(	O
L	O
)	O
is	O
obtained	O
by	O
composition	O
of	O
L	O
singlelayer	O
encoders	O
:	O
let	O
X	O
(	O
0	B-DatasetName
)	O
:	O
=	O
X	O
,	O
and	O
for	O
0	B-DatasetName
≤	O
≤	O
L	O
−	O
1	O
,	O
let	O
X	O
(	O
+1	O
)	O
=	O
Enc	O
(	O
X	O
(	O
)	O
;	O
θ	B-HyperparameterName
)	O
and	O
finally	O
,	O
K	O
e	O
=	O
K	O
(	O
L	O
)	O
(	O
X	O
(	O
L	O
)	O
)	O
,	O
V	O
e	O
=	O
V	O
(	O
L	O
)	O
(	O
X	O
(	O
L	O
)	O
)	O
.	O
Transformer	B-MethodName
decoder	I-MethodName
.	O
The	O
input	O
to	O
a	O
singlelayer	O
decoder	O
is	O
(	O
i	O
)	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
output	O
by	O
the	O
encoder	O
,	O
and	O
(	O
ii	O
)	O
sequence	O
Y	O
=	O
(	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
k	O
)	O
of	O
vectors	O
for	O
k	O
≥	O
1	O
.	O
The	O
output	O
is	O
another	O
sequence	O
Z	O
=	O
(	O
z	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
k	O
)	O
.	O
Similar	O
to	O
the	O
single	O
-	O
layer	O
encoder	O
,	O
a	O
singlelayer	O
decoder	O
is	O
parameterized	O
by	O
functions	O
Q	O
(	O
)	O
,	O
K	O
(	O
)	O
,	O
V	O
(	O
)	O
and	O
O	O
(	O
)	O
and	O
is	O
defined	O
by	O
p	O
t	O
=	O
Att	O
(	O
Q	O
(	O
y	O
t	O
)	O
,	O
K	O
(	O
Y	O
t	O
)	O
,	O
V	O
(	O
Y	O
t	O
)	O
)	O
+	O
y	O
t	O
,	O
(	O
2	O
)	O
a	O
t	O
=	O
Att	O
(	O
p	O
t	O
,	O
K	O
e	O
,	O
V	O
e	O
)	O
+	O
p	O
t	O
,	O
(	O
3	O
)	O
z	O
t	O
=	O
O	O
(	O
a	O
t	O
)	O
+	O
a	O
t	O
,	O
where	O
1	O
≤	O
t	O
≤	O
k.	O
The	O
operation	O
in	O
(	O
2	O
)	O
will	O
be	O
referred	O
to	O
as	O
the	O
decoder	O
-	O
decoder	O
attention	O
block	O
and	O
the	O
operation	O
in	O
(	O
3	O
)	O
as	O
the	O
decoder	O
-	O
encoder	O
attention	O
block	O
.	O
In	O
(	O
2	O
)	O
,	O
positional	O
masking	O
is	O
applied	O
to	O
prevent	O
the	O
network	O
from	O
attending	O
over	O
symbols	O
which	O
are	O
ahead	O
of	O
them	O
.	O
An	O
L	O
-	O
layer	O
Transformer	B-MethodName
decoder	I-MethodName
TDec	O
L	O
(	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
,	O
Y	O
;	O
θ	B-HyperparameterName
)	O
=	O
z	O
is	O
obtained	O
by	O
repeated	O
application	O
of	O
L	O
single	O
-	O
layer	O
decoders	O
each	O
with	O
its	O
own	O
parameters	O
,	O
and	O
a	O
transformation	O
function	O
F	O
:	O
Q	O
d	O
Q	O
d	O
applied	O
to	O
the	O
last	O
vector	O
in	O
the	O
sequence	O
of	O
vectors	O
output	O
by	O
the	O
final	O
decoder	O
.	O
Formally	O
,	O
for	O
0	B-DatasetName
≤	O
≤	O
L−1	O
and	O
Y	O
0	B-DatasetName
:	O
=	O
Y	O
we	O
have	O
Y	O
+1	O
=	O
Dec	O
(	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
,	O
Y	O
;	O
θ	B-HyperparameterName
)	O
,	O
z	O
=	O
F	O
(	O
y	O
L	O
k	O
)	O
.	O
Note	O
that	O
while	O
the	O
output	O
of	O
a	O
single	O
-	O
layer	O
decoder	O
is	O
a	O
sequence	O
of	O
vectors	O
,	O
the	O
output	O
of	O
an	O
L	O
-	O
layer	O
Transformer	B-MethodName
decoder	I-MethodName
is	O
a	O
single	O
vector	O
.	O
The	O
complete	O
Transformer	B-MethodName
.	O
The	O
output	O
Trans	O
(	O
X	O
,	O
y	O
0	B-DatasetName
)	O
=	O
Y	O
is	O
computed	O
by	O
the	O
recurrenceỹ	O
t+1	O
=	O
TDec	O
(	O
TEnc	O
(	O
X	O
)	O
,	O
(	O
y	O
0	B-DatasetName
,	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
t	O
)	O
)	O
,	O
for	O
0	B-DatasetName
≤	O
t	O
≤	O
r	O
−	O
1	O
.	O
We	O
get	O
y	O
t+1	O
by	O
adding	O
positional	O
encoding	O
:	O
y	O
t+1	O
=	O
ỹ	O
t+1	O
+	O
pos	O
(	O
t	O
+	O
1	O
)	O
.	O
Directional	O
Transformer	B-MethodName
.	O
We	O
denote	O
the	O
Transformer	B-MethodName
with	O
only	O
positional	O
masking	O
and	O
no	O
positional	O
encodings	O
as	O
Directional	O
Transformer	B-MethodName
and	O
use	O
them	O
interchangeably	O
.	O
In	O
this	O
case	O
,	O
we	O
use	O
standard	O
multiplicative	B-MethodName
attention	I-MethodName
as	O
the	O
scoring	O
function	O
in	O
our	O
construction	O
,	O
i.e	O
,	O
f	O
att	O
(	O
q	O
,	O
k	O
i	O
)	O
=	O
q	O
,	O
k	O
i	O
.	O
The	O
general	O
architecture	O
is	O
the	O
same	O
as	O
for	O
the	O
vanilla	O
case	O
;	O
the	O
differences	O
due	O
to	O
positional	O
masking	O
are	O
the	O
following	O
.	O
There	O
are	O
no	O
positional	O
encodings	O
.	O
So	O
the	O
input	O
vectors	O
x	O
i	O
only	O
involve	O
f	O
b	O
(	O
s	O
i	O
)	O
.	O
Simi	O
-	O
larly	O
,	O
y	O
t	O
=	O
ỹ	O
t	O
.	O
In	O
(	O
1	O
)	O
,	O
Att	O
(	O
)	O
is	O
replaced	O
by	O
Att	O
(	O
Q	O
(	O
x	O
i	O
)	O
,	O
K	O
(	O
X	O
i	O
)	O
,	O
V	O
(	O
X	O
i	O
)	O
)	O
where	O
X	O
i	O
:	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
i	O
)	O
for	O
1	O
≤	O
i	O
≤	O
n.	O
Similarly	O
,	O
in	O
(	O
3	O
)	O
,	O
Att	O
(	O
)	O
is	O
replaced	O
by	O
Att	O
(	O
p	O
t	O
,	O
K	O
e	O
t	O
,	O
V	O
e	O
t	O
)	O
.	O
Remark	O
1	O
.	O
Our	O
definitions	O
deviate	O
slightly	O
from	O
practice	O
,	O
hard	O
-	O
attention	O
being	O
the	O
main	O
one	O
since	O
hardmax	O
keeps	O
the	O
values	O
rational	O
whereas	O
softmax	B-MethodName
takes	O
the	O
values	O
to	O
irrational	O
space	O
.	O
Previous	O
studies	O
have	O
shown	O
that	O
soft	O
-	O
attention	O
behaves	O
like	O
hard	O
-	O
attention	O
in	O
practice	O
and	O
Hahn	O
(	O
2020	O
)	O
discusses	O
its	O
practical	O
relevance	O
.	O
Remark	O
2	O
.	O
Transformer	B-MethodName
Networks	O
with	O
positional	O
encodings	O
are	O
not	O
necessarily	O
equivalent	O
in	O
terms	O
of	O
their	O
computational	O
expressiveness	O
(	O
Yun	O
et	O
al	O
,	O
2020	O
)	O
to	O
those	O
with	O
only	O
positional	O
masking	O
when	O
considering	O
the	O
encoder	O
only	O
model	O
(	O
as	O
used	O
in	O
BERT	B-MethodName
and	O
GPT	B-MethodName
-	O
2	O
)	O
.	O
Our	O
results	O
in	O
Section	O
4.1	O
show	O
their	O
equivalence	O
in	O
terms	O
of	O
expressiveness	O
for	O
the	O
complete	O
seq	O
-	O
to	O
-	O
seq	O
architecture	O
.	O
4	O
Primary	O
Results	O

In	O
light	O
of	O
Theorem	O
3.1	O
,	O
to	O
prove	O
that	O
Transformers	O
are	O
Turing	O
-	O
complete	O
,	O
it	O
suffices	O
to	O
show	O
that	O
they	O
can	O
simulate	O
RNNs	O
.	O
We	O
say	O
that	O
a	O
Transformer	B-MethodName
simulates	O
an	O
RNN	O
(	O
as	O
defined	O
in	O
Sec	O
.	O
3.1	O
)	O
if	O
on	O
every	O
input	O
s	O
Σ	O
*	O
,	O
at	O
each	O
step	O
t	O
,	O
the	O
vector	O
y	O
t	O
contains	O
the	O
hidden	O
state	O
h	O
t	O
as	O
a	O
subvector	O
,	O
i.e.	O
y	O
t	O
=	O
[	O
h	O
t	O
,	O
]	O
,	O
and	O
halts	O
at	O
the	O
same	O
step	O
as	O
the	O
RNN	O
.	O
Theorem	O
4.1	O
.	O
The	O
class	O
of	O
Transformers	O
with	O
positional	O
encodings	O
is	O
Turing	O
-	O
complete	O
.	O
Proof	O
Sketch	B-DatasetName
.	O
The	O
input	O
s	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
s	O
n	O
Σ	O
*	O
is	O
provided	O
to	O
the	O
transformer	O
as	O
the	O
sequence	O
of	O
vectors	O
x	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
x	O
n	O
,	O
where	O
x	O
i	O
=	O
[	O
0	B-DatasetName
d	O
h	O
,	O
f	O
b	O
(	O
s	O
i	O
)	O
,	O
0	B-DatasetName
d	O
h	O
,	O
i	O
,	O
1	O
]	O
,	O
which	O
has	O
as	O
sub	O
-	O
vector	O
the	O
given	O
base	O
embedding	O
f	O
b	O
(	O
s	O
i	O
)	O
and	O
the	O
positional	O
encoding	O
i	O
,	O
along	O
with	O
extra	O
coordinates	O
set	O
to	O
constant	O
values	O
and	O
will	O
be	O
used	O
later	O
.	O
The	O
basic	O
observation	O
behind	O
our	O
construction	O
of	O
the	O
simulating	O
Transformer	B-MethodName
is	O
that	O
the	O
transformer	B-MethodName
decoder	I-MethodName
can	O
naturally	O
implement	O
the	O
recurrence	O
operations	O
of	O
the	O
type	O
used	O
by	O
RNNs	O
.	O
To	O
this	O
end	O
,	O
the	O
FFN	O
O	O
dec	O
(	O
)	O
of	O
the	O
decoder	O
,	O
which	O
plays	O
the	O
same	O
role	O
as	O
the	O
FFN	O
component	O
of	O
the	O
RNN	O
,	O
needs	O
sequential	O
access	O
to	O
the	O
input	O
in	O
the	O
same	O
way	O
as	O
RNN	O
.	O
But	O
the	O
Transformer	B-MethodName
receives	O
the	O
whole	O
input	O
at	O
the	O
same	O
time	O
.	O
We	O
utilize	O
positional	O
encoding	O
along	O
with	O
the	O
attention	O
mechanism	O
to	O
isolate	O
x	O
t	O
at	O
time	O
t	O
and	O
feed	O
it	O
to	O
O	O
dec	O
(	O
)	O
,	O
thereby	O
simulating	O
the	O
RNN	O
.	O
As	O
stated	O
earlier	O
,	O
we	O
append	O
the	O
input	O
s	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
n	O
of	O
the	O
RNN	O
with	O
$	O
's	O
until	O
it	O
halts	O
.	O
Since	O
the	O
Transformer	B-MethodName
takes	O
its	O
input	O
all	O
at	O
once	O
,	O
appending	O
by	O
$	O
's	O
is	O
not	O
possible	O
(	O
in	O
particular	O
,	O
we	O
do	O
not	O
know	O
how	O
long	O
the	O
computation	O
would	O
take	O
)	O
.	O
Instead	O
,	O
we	O
append	O
the	O
input	O
with	O
a	O
single	O
$	O
.	O
After	O
encountering	O
a	O
$	O
once	O
,	O
the	O
Transformer	B-MethodName
will	O
feed	O
(	O
encoding	O
of	O
)	O
$	O
to	O
O	O
dec	O
(	O
)	O
in	O
subsequent	O
steps	O
until	O
termination	O
.	O
Here	O
we	O
confine	O
our	O
discussion	O
to	O
the	O
case	O
t	O
≤	O
n	O
;	O
the	O
t	O
>	O
n	O
case	O
is	O
slightly	O
different	O
but	O
simpler	O
.	O
The	O
construction	O
is	O
straightforward	O
:	O
it	O
has	O
only	O
one	O
head	O
,	O
one	O
encoder	O
layer	O
and	O
one	O
decoder	O
layer	O
;	O
moreover	O
,	O
the	O
attention	O
mechanisms	O
in	O
the	O
encoder	O
and	O
the	O
decoder	O
-	O
decoder	O
attention	O
block	O
of	O
the	O
decoder	O
are	O
trivial	O
as	O
described	O
below	O
.	O
The	O
encoder	O
attention	O
layer	O
does	O
trivial	O
computation	O
in	O
that	O
it	O
merely	O
computes	O
the	O
identity	O
function	O
:	O
z	O
i	O
=	O
x	O
i	O
,	O
which	O
can	O
be	O
easily	O
achieved	O
,	O
e.g.	O
by	O
using	O
the	O
residual	B-MethodName
connection	I-MethodName
and	O
setting	O
the	O
value	O
vectors	O
to	O
0	B-DatasetName
.	O
The	O
fi	O
-	O
nal	O
K	O
(	O
1	O
)	O
(	O
)	O
and	O
V	O
(	O
1	O
)	O
(	O
)	O
functions	O
bring	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
into	O
useful	O
forms	O
by	O
appropriate	O
linear	O
transformations	O
:	O
k	O
i	O
=	O
[	O
0	B-DatasetName
d	O
b	O
,	O
0	B-DatasetName
d	O
b	O
,	O
0	B-DatasetName
d	O
b	O
,	O
−1	O
,	O
i	O
]	O
and	O
v	O
i	O
=	O
[	O
0	B-DatasetName
d	O
b	O
,	O
f	O
b	O
(	O
s	O
i	O
)	O
,	O
0	B-DatasetName
d	O
b	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
.	O
Thus	O
,	O
the	O
key	O
vectors	O
only	O
encode	O
the	O
positional	O
information	O
and	O
the	O
value	O
vectors	O
only	O
encode	O
the	O
input	O
symbols	O
.	O
The	O
output	O
sequence	O
of	O
the	O
decoder	O
is	O
y	O
1	O
,	O
y	O
2	O
,	O
.	O
.	O
..	O
Our	O
construction	O
will	O
ensure	O
,	O
by	O
induction	O
on	O
t	O
,	O
that	O
y	O
t	O
contains	O
the	O
hidden	O
states	O
h	O
t	O
of	O
the	O
RNN	O
as	O
a	O
sub	O
-	O
vector	O
along	O
with	O
positional	O
information	O
:	O
y	O
t	O
=	O
[	O
h	O
t	O
,	O
0	B-DatasetName
d	O
b	O
,	O
0	B-DatasetName
d	O
b	O
,	O
t	O
+	O
1	O
,	O
1	O
]	O
.	O
This	O
is	O
easy	O
to	O
arrange	O
for	O
t	O
=	O
0	B-DatasetName
,	O
and	O
assuming	O
it	O
for	O
t	O
we	O
prove	O
it	O
for	O
t+1	O
.	O
As	O
for	O
the	O
encoder	O
,	O
the	O
decoder	O
-	O
decoder	O
attention	O
block	O
acts	O
as	O
the	O
identity	O
:	O
p	O
t	O
=	O
y	O
t	O
.	O
Now	O
,	O
using	O
the	O
last	O
but	O
one	O
coordinate	O
in	O
y	O
t	O
representing	O
the	O
time	O
t	O
+	O
1	O
,	O
the	O
attention	O
mechanism	O
Att	O
(	O
p	O
t	O
,	O
K	O
e	O
,	O
V	O
e	O
)	O
can	O
retrieve	O
the	O
embedding	O
of	O
the	O
t	O
-	O
th	O
input	O
symbol	O
x	O
t	O
.	O
This	O
is	O
possible	O
because	O
in	O
the	O
key	O
vector	O
k	O
i	O
mentioned	O
above	O
,	O
almost	O
all	O
coordinates	O
other	O
than	O
the	O
one	O
representing	O
the	O
position	O
i	O
are	O
set	O
to	O
0	B-DatasetName
,	O
allowing	O
the	O
mechanism	O
to	O
only	O
focus	O
on	O
the	O
positional	O
information	O
and	O
not	O
be	O
distracted	O
by	O
the	O
other	O
contents	O
of	O
p	O
t	O
=	O
y	O
t	O
:	O
the	O
scoring	O
function	O
has	O
value	O
f	O
att	O
(	O
p	O
t	O
,	O
k	O
i	O
)	O
=	O
−	O
|	O
p	O
t	O
,	O
k	O
i	O
|	O
=	O
−	O
|	O
i	O
−	O
(	O
t	O
+	O
1	O
)	O
|	O
.	O
For	O
a	O
given	O
t	O
,	O
it	O
is	O
maximized	O
at	O
i	O
=	O
t	O
+	O
1	O
for	O
t	O
<	O
n	O
and	O
at	O
i	O
=	O
n	O
for	O
t	O
≥	O
n.	O
This	O
use	O
of	O
scoring	O
function	O
is	O
similar	O
to	O
Pérez	O
et	O
al	O
(	O
2019	O
)	O
.	O
At	O
this	O
point	O
,	O
O	O
dec	O
(	O
)	O
has	O
at	O
its	O
disposal	O
the	O
hidden	O
state	O
h	O
t	O
(	O
coming	O
from	O
y	O
t	O
via	O
p	O
t	O
and	O
the	O
residual	B-MethodName
connection	I-MethodName
)	O
and	O
the	O
input	O
symbol	O
x	O
t	O
(	O
coming	O
via	O
the	O
attention	O
mechanism	O
and	O
the	O
residual	B-MethodName
connection	I-MethodName
)	O
.	O
Hence	O
O	O
(	O
)	O
can	O
act	O
just	O
like	O
the	O
FFN	O
(	O
Lemma	B-DatasetName
C.4	O
)	O
underlying	O
the	O
RNN	O
to	O
compute	O
h	O
t+1	O
and	O
thus	O
y	O
t+1	O
,	O
proving	O
the	O
induction	O
hypothesis	O
.	O
The	O
complete	O
construction	O
can	O
be	O
found	O
in	O
Sec	O
.	O
C.2	O
in	O
the	O
appendix	O
.	O
Theorem	O
4.2	O
.	O
The	O
class	O
of	O
Transformers	O
with	O
positional	O
masking	O
and	O
no	O
explicit	O
positional	O
encodings	O
is	O
Turing	O
-	O
complete	O
.	O
Proof	O
Sketch	B-DatasetName
.	O
As	O
before	O
,	O
by	O
Theorem	O
3.1	O
it	O
suffices	O
to	O
show	O
that	O
Transformers	O
can	O
simulate	O
RNNs	O
.	O
The	O
input	O
s	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
s	O
n	O
is	O
provided	O
to	O
the	O
transformer	O
as	O
the	O
sequence	O
of	O
vectors	O
x	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
x	O
n	O
,	O
where	O
x	O
i	O
=	O
[	O
0	B-DatasetName
d	O
h	O
,	O
0	B-DatasetName
d	O
h	O
,	O
f	O
b	O
(	O
s	O
i	O
)	O
,	O
s	O
i	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
m	O
,	O
0	B-DatasetName
m	O
,	O
0	B-DatasetName
m	O
]	O
.	O
The	O
general	O
goal	O
for	O
the	O
directional	O
case	O
is	O
similar	O
to	O
the	O
vanilla	O
case	O
,	O
namely	O
we	O
would	O
like	O
the	O
FFN	O
O	O
dec	O
(	O
)	O
of	O
the	O
decoder	O
to	O
directly	O
simulate	O
the	O
computation	O
in	O
the	O
underlying	O
RNN	O
.	O
In	O
the	O
vanilla	O
case	O
,	O
positional	O
encoding	O
and	O
the	O
attention	O
mechanism	O
helped	O
us	O
feed	O
input	O
x	O
t	O
at	O
the	O
t	O
-	O
th	O
iteration	O
of	O
the	O
decoder	O
to	O
O	O
dec	O
(	O
)	O
.	O
However	O
,	O
we	O
no	O
longer	O
have	O
explicit	O
positional	O
information	O
in	O
the	O
input	O
x	O
t	O
such	O
as	O
a	O
coordinate	O
with	O
value	O
t.	O
The	O
key	O
insight	O
is	O
that	O
we	O
do	O
not	O
need	O
the	O
positional	O
information	O
explicitly	O
to	O
recover	O
x	O
t	O
at	O
step	O
t	O
:	O
in	O
our	O
construction	O
,	O
the	O
attention	O
mechanism	O
with	O
masking	O
will	O
recover	O
x	O
t	O
in	O
an	O
indirect	O
manner	O
even	O
though	O
it	O
's	O
not	O
able	O
to	O
"	O
zero	O
in	O
"	O
on	O
the	O
t	O
-	O
th	O
position	O
.	O
Let	O
us	O
first	O
explain	O
this	O
without	O
details	O
of	O
the	O
construction	O
.	O
We	O
maintain	O
in	O
vector	O
ω	O
t	O
Q	O
m	O
,	O
with	O
a	O
coordinate	O
each	O
for	O
symbols	O
in	O
Σ	O
,	O
the	O
fraction	O
of	O
times	O
the	O
symbol	O
has	O
occurred	O
up	O
to	O
step	O
t.	O
Now	O
,	O
at	O
a	O
step	O
t	O
≤	O
n	O
,	O
for	O
the	O
difference	O
ω	O
t	O
−	O
ω	O
t−1	O
(	O
which	O
is	O
part	O
of	O
the	O
query	O
vector	O
)	O
,	O
it	O
can	O
be	O
shown	O
easily	O
that	O
only	O
the	O
coordinate	O
corresponding	O
to	O
s	O
t	O
is	O
positive	O
.	O
Thus	O
after	O
applying	O
the	O
linearized	O
sigmoid	O
σ	O
(	O
ω	O
t	O
−	O
ω	O
t−1	O
)	O
,	O
we	O
can	O
isolate	O
the	O
coordinate	O
corresponding	O
to	O
s	O
t	O
.	O
Now	O
using	O
this	O
query	O
vector	O
,	O
the	O
(	O
hard	O
)	O
attention	O
mechanism	O
will	O
be	O
able	O
to	O
retrieve	O
the	O
value	O
vectors	O
for	O
all	O
indices	O
j	O
such	O
that	O
s	O
j	O
=	O
s	O
t	O
and	O
output	O
their	O
average	O
.	O
Crucially	O
,	O
the	O
value	O
vector	O
for	O
an	O
index	O
j	O
is	O
essentially	O
x	O
j	O
which	O
depends	O
only	O
on	O
s	O
j	O
.	O
Thus	O
,	O
all	O
these	O
vectors	O
are	O
equal	O
to	O
x	O
t	O
,	O
and	O
so	O
is	O
their	O
average	O
.	O
This	O
recovers	O
x	O
t	O
,	O
which	O
can	O
now	O
be	O
fed	O
to	O
O	O
dec	O
(	O
)	O
,	O
simulating	O
the	O
RNN	O
.	O
We	O
now	O
outline	O
the	O
construction	O
and	O
relate	O
it	O
to	O
the	O
above	O
discussion	O
.	O
As	O
before	O
,	O
for	O
simplicity	O
we	O
restrict	O
to	O
the	O
case	O
t	O
≤	O
n.	O
We	O
use	O
only	O
one	O
head	O
,	O
one	O
layer	O
encoder	O
and	O
two	O
layer	O
decoder	O
.	O
The	O
encoder	O
,	O
as	O
in	O
the	O
vanilla	O
case	O
,	O
does	O
very	O
little	O
other	O
than	O
pass	O
information	O
along	O
.	O
The	O
vectors	O
in	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
are	O
obtained	O
by	O
the	O
trivial	O
attention	O
mechanism	O
followed	O
by	O
simple	O
linear	O
transformations	O
:	O
k	O
e	O
i	O
=	O
[	O
0	B-DatasetName
d	O
h	O
,	O
0	B-DatasetName
d	O
h	O
,	O
0	B-DatasetName
d	O
b	O
,	O
s	O
i	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
m	O
,	O
0	B-DatasetName
m	O
,	O
0	B-DatasetName
m	O
]	O
and	O
v	O
e	O
i	O
=	O
[	O
0	B-DatasetName
d	O
h	O
,	O
0	B-DatasetName
d	O
h	O
,	O
f	O
b	O
(	O
s	O
i	O
)	O
,	O
0	B-DatasetName
m	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
m	O
,	O
s	O
i	O
,	O
0	B-DatasetName
m	O
]	O
.	O
Our	O
construction	O
ensures	O
that	O
at	O
step	O
t	O
we	O
have	O
y	O
t	O
=	O
[	O
h	O
t−1	O
,	O
0	B-DatasetName
d	O
h	O
,	O
0	B-DatasetName
d	O
b	O
,	O
0	B-DatasetName
m	O
,	O
1	O
2	O
t	O
,	O
0	B-DatasetName
m	O
,	O
0	B-DatasetName
m	O
,	O
ω	O
t−1	O
]	O
.	O
As	O
before	O
,	O
the	O
proof	O
is	O
by	O
induction	O
on	O
t.	O
In	O
the	O
first	O
layer	O
of	O
decoder	O
,	O
the	O
decoderdecoder	O
attention	O
block	O
is	O
trivial	O
:	O
p	O
(	O
1	O
)	O
t	O
=	O
y	O
t	O
.	O
In	O
the	O
decoder	O
-	O
encoder	O
attention	O
block	O
,	O
we	O
give	O
equal	O
attention	O
to	O
all	O
the	O
t	O
+	O
1	O
values	O
,	O
which	O
along	O
with	O
O	O
enc	O
(	O
)	O
,	O
leads	O
to	O
z	O
(	O
1	O
)	O
t	O
=	O
[	O
h	O
t−1	O
,	O
0	B-DatasetName
d	O
h	O
,	O
0	B-DatasetName
d	O
b	O
,	O
δ	B-HyperparameterName
t	O
,	O
1	O
2	O
t+1	O
,	O
0	B-DatasetName
m	O
,	O
0	B-DatasetName
m	O
,	O
ω	O
t	O
]	O
,	O
where	O
essentially	O
δ	B-HyperparameterName
t	O
=	O
σ	O
(	O
ω	O
t	O
−	O
ω	O
t−1	O
)	O
,	O
except	O
with	O
a	O
change	O
for	O
the	O
last	O
coordinate	O
due	O
to	O
special	O
status	O
of	O
the	O
last	O
symbol	O
$	O
in	O
the	O
processing	O
of	O
RNN	O
.	O
In	O
the	O
second	O
layer	O
,	O
the	O
decoder	O
-	O
decoder	O
attention	O
block	O
is	O
again	O
trivial	O
with	O
p	O
(	O
2	O
)	O
t	O
=	O
z	O
(	O
1	O
)	O
t	O
.	O
We	O
remark	O
that	O
in	O
this	O
construction	O
,	O
the	O
scoring	O
function	O
is	O
the	O
standard	O
multiplicative	B-MethodName
attention	I-MethodName
3	O
.	O
Now	O
p	O
(	O
2	O
)	O
t	O
,	O
k	O
e	O
j	O
=	O
δ	B-HyperparameterName
t	O
,	O
s	O
j	O
=	O
δ	B-HyperparameterName
t	O
,	O
j	O
,	O
which	O
is	O
positive	O
if	O
and	O
only	O
if	O
s	O
j	O
=	O
s	O
t	O
,	O
as	O
mentioned	O
earlier	O
.	O
Thus	O
attention	O
weights	O
in	O
Att	O
(	O
p	O
(	O
2	O
)	O
t	O
,	O
K	O
e	O
t	O
,	O
V	O
e	O
t	O
)	O
satisfy	O
hardmax	O
(	O
p	O
(	O
2	O
)	O
t	O
,	O
k	O
e	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
(	O
2	O
)	O
t	O
,	O
k	O
e	O
t	O
)	O
=	O
1	O
λt	O
(	O
I	O
(	O
s	O
0	B-DatasetName
=	O
s	O
t	O
)	O
,	O
I	O
(	O
s	O
1	O
=	O
s	O
t	O
)	O
,	O
.	O
.	O
.	O
,	O
I	O
(	O
s	O
t	O
=	O
s	O
t	O
)	O
)	O
,	O
where	O
λ	O
t	O
is	O
a	O
normalization	O
constant	O
and	O
I	O
(	O
)	O
is	O
the	O
indicator	O
.	O
See	O
Lemma	B-DatasetName
D.3	O
for	O
more	O
details	O
.	O
At	O
this	O
point	O
,	O
O	O
dec	O
(	O
)	O
has	O
at	O
its	O
disposal	O
the	O
hidden	O
state	O
h	O
t	O
(	O
coming	O
from	O
z	O
(	O
1	O
)	O
t	O
via	O
p	O
(	O
2	O
)	O
t	O
and	O
the	O
residual	B-MethodName
connection	I-MethodName
)	O
and	O
the	O
input	O
symbol	O
x	O
t	O
(	O
coming	O
via	O
the	O
attention	O
mechanism	O
and	O
the	O
residual	B-MethodName
connection	I-MethodName
)	O
.	O
Hence	O
O	O
dec	O
(	O
)	O
can	O
act	O
just	O
like	O
the	O
FFN	O
underlying	O
the	O
RNN	O
to	O
compute	O
h	O
t+1	O
and	O
thus	O
y	O
t+1	O
,	O
proving	O
the	O
induction	O
hypothesis	O
.	O
The	O
complete	O
construction	O
can	O
be	O
found	O
in	O
Sec	O
.	O
D	O
in	O
the	O
Appendix	O
.	O
In	O
practice	O
,	O
found	O
that	O
for	O
NMT	O
,	O
Transformers	O
with	O
only	O
positional	O
masking	O
achieve	O
comparable	O
performance	O
compared	O
to	O
the	O
ones	O
with	O
positional	O
encodings	O
.	O
Similar	O
evidence	O
was	O
found	O
by	O
Tsai	O
et	O
al	O
(	O
2019	O
)	O
.	O
Our	O
proof	O
for	O
directional	O
transformers	O
entails	O
that	O
there	O
is	O
no	O
loss	B-MetricName
of	O
order	O
information	O
if	O
positional	O
information	O
is	O
only	O
provided	O
in	O
the	O
form	O
of	O
masking	O
.	O
However	O
,	O
we	O
do	O
not	O
recommend	O
using	O
masking	O
as	O
a	O
replacement	O
for	O
explicit	O
encodings	O
.	O
The	O
computational	O
equivalence	O
of	O
encoding	O
and	O
masking	O
given	O
by	O
our	O
results	O
implies	O
that	O
any	O
differences	O
in	O
their	O
performance	O
must	O
come	O
from	O
differences	O
in	O
learning	O
dynamics	O
.	O

The	O
results	O
for	O
various	O
components	O
follow	O
from	O
our	O
construction	O
in	O
Theorem	O
4.1	O
.	O
Note	O
that	O
in	O
both	O
the	O
encoder	O
and	O
decoder	O
attention	O
blocks	O
,	O
we	O
need	O
to	O
compute	O
the	O
identity	O
function	O
.	O
We	O
can	O
nullify	O
the	O
role	O
of	O
the	O
attention	O
heads	O
by	O
setting	O
the	O
value	O
vectors	O
to	O
zero	O
and	O
making	O
use	O
of	O
only	O
the	O
residual	O
connections	O
to	O
implement	O
the	O
identity	O
function	O
.	O
Thus	O
,	O
even	O
if	O
we	O
remove	O
those	O
attention	O
heads	O
,	O
the	O
model	O
is	O
still	O
Turing	O
-	O
complete	O
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
can	O
remove	O
the	O
residual	O
connections	O
around	O
the	O
attention	O
blocks	O
and	O
make	O
use	O
of	O
the	O
attention	O
heads	O
to	O
implement	O
the	O
identity	O
function	O
by	O
using	O
positional	O
encodings	O
.	O
Hence	O
,	O
either	O
the	O
attention	O
head	O
or	O
the	O
residual	B-MethodName
connection	I-MethodName
is	O
sufficient	O
to	O
achieve	O
Turing	O
-	O
completeness	O
.	O
A	O
similar	O
argument	O
can	O
be	O
made	O
for	O
the	O
FFN	O
in	O
the	O
encoder	O
layer	O
:	O
either	O
the	O
residual	B-MethodName
connection	I-MethodName
or	O
the	O
FFN	O
is	O
sufficient	O
for	O
Turing	O
-	O
completeness	O
.	O
For	O
the	O
decoder	O
-	O
encoder	O
attention	O
head	O
,	O
since	O
it	O
is	O
the	O
only	O
way	O
for	O
the	O
decoder	O
to	O
obtain	O
information	O
about	O
the	O
input	O
,	O
it	O
is	O
necessary	O
for	O
the	O
completeness	O
.	O
The	O
FFN	O
is	O
the	O
only	O
component	O
that	O
can	O
perform	O
computations	O
based	O
on	O
the	O
input	O
and	O
the	O
computations	O
performed	O
earlier	O
via	O
recurrence	O
and	O
hence	O
,	O
the	O
model	O
is	O
not	O
Turing	O
-	O
complete	O
without	O
it	O
.	O
Figure	O
2	O
summarizes	O
the	O
role	O
of	O
different	O
components	O
with	O
respect	O
to	O
the	O
computational	O
expressiveness	O
of	O
the	O
network	O
.	O
Proposition	O
4.3	O
.	O
The	O
class	O
of	O
Transformers	O
without	O
residual	B-MethodName
connection	I-MethodName
around	O
the	O
decoderencoder	O
attention	O
block	O
is	O
not	O
Turing	O
-	O
complete	O
.	O
Proof	O
Sketch	B-DatasetName
.	O
We	O
confine	O
our	O
discussion	O
to	O
singlelayer	O
decoder	O
;	O
the	O
case	O
of	O
multilayer	O
decoder	O
is	O
similar	O
.	O
Without	O
the	O
residual	B-MethodName
connection	I-MethodName
,	O
the	O
decoder	O
-	O
encoder	O
attention	O
block	O
produces	O
a	O
t	O
=	O
Att	O
(	O
p	O
t	O
,	O
K	O
e	O
,	O
V	O
e	O
)	O
=	O
n	O
i=1	O
α	B-HyperparameterName
i	O
v	O
e	O
i	O
for	O
some	O
α	B-HyperparameterName
i	O
's	O
such	O
that	O
n	O
i	O
α	B-HyperparameterName
i	O
=	O
1	O
.	O
Note	O
that	O
,	O
without	O
residual	B-MethodName
connection	I-MethodName
a	O
t	O
can	O
take	O
on	O
at	O
most	O
2	O
n	O
−	O
1	O
values	O
.	O
This	O
is	O
because	O
by	O
the	O
definition	O
of	O
hard	O
attention	O
the	O
vector	O
(	O
α	B-HyperparameterName
1	O
,	O
.	O
.	O
.	O
,	O
α	B-HyperparameterName
n	O
)	O
is	O
characterized	O
by	O
the	O
set	O
of	O
zero	O
coordinates	O
and	O
there	O
are	O
at	O
most	O
2	O
n	O
−	O
1	O
such	O
sets	O
(	O
all	O
coordinates	O
can	O
not	O
be	O
zero	O
)	O
.	O
This	O
restriction	O
on	O
the	O
number	O
of	O
values	O
on	O
a	O
t	O
holds	O
regardless	O
of	O
the	O
value	O
of	O
p	O
t	O
.	O
If	O
the	O
task	O
requires	O
the	O
network	O
to	O
produce	O
values	O
of	O
a	O
t	O
that	O
come	O
from	O
a	O
set	O
with	O
size	O
at	O
least	O
2	O
n	O
,	O
then	O
the	O
network	O
will	O
not	O
be	O
able	O
to	O
perform	O
the	O
task	O
.	O
Here	O
's	O
an	O
example	O
task	O
:	O
given	O
a	O
number	O
∆	O
(	O
0	B-DatasetName
,	O
1	O
)	O
,	O
the	O
network	O
must	O
produce	O
numbers	O
0	B-DatasetName
,	O
∆	O
,	O
2∆	O
,	O
.	O
.	O
.	O
,	O
k∆	O
,	O
where	O
k	O
is	O
the	O
maximum	O
integer	O
such	O
that	O
k∆	O
≤	O
1	O
.	O
If	O
the	O
network	O
receives	O
a	O
single	O
input	O
∆	O
,	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
vector	O
a	O
t	O
will	O
be	O
a	O
constant	O
(	O
v	O
e	O
1	O
)	O
at	O
any	O
step	O
and	O
hence	O
the	O
output	O
of	O
the	O
network	O
will	O
also	O
be	O
constant	O
at	O
all	O
steps	O
.	O
Thus	O
,	O
the	O
model	O
can	O
not	O
perform	O
such	O
a	O
task	O
.	O
If	O
the	O
input	O
is	O
combined	O
with	O
n	O
−	O
1	O
auxiliary	O
symbols	O
(	O
such	O
as	O
#	O
and	O
$	O
)	O
,	O
then	O
in	O
the	O
network	O
,	O
each	O
a	O
t	O
takes	O
on	O
at	O
most	O
2	O
n	O
−	O
1	O
values	O
.	O
Hence	O
,	O
the	O
model	O
will	O
be	O
incapable	O
of	O
performing	O
the	O
task	O
if	O
∆	O
<	O
1/2	O
n	O
.	O
Such	O
a	O
limitation	O
does	O
not	O
exist	O
with	O
a	O
residual	B-MethodName
connection	I-MethodName
since	O
the	O
vector	O
a	O
t	O
=	O
n	O
i=1	O
α	B-HyperparameterName
i	O
v	O
e	O
i	O
+	O
p	O
t	O
can	O
take	O
arbitrary	O
number	O
of	O
values	O
depending	O
on	O
its	O
prior	O
computations	O
in	O
p	O
t	O
.	O
For	O
further	O
details	O
,	O
see	O
Sec	O
.	O
C.1	O
in	O
the	O
Appendix	O
.	O
Discussion	O
.	O
It	O
is	O
perhaps	O
surprising	O
that	O
residual	B-MethodName
connection	I-MethodName
,	O
originally	O
proposed	O
to	O
assist	O
in	O
the	O
learning	O
ability	O
of	O
very	O
deep	O
networks	O
,	O
plays	O
a	O
vital	O
role	O
in	O
the	O
computational	O
expressiveness	O
of	O
the	O
network	O
.	O
Without	O
it	O
,	O
the	O
model	O
is	O
limited	O
in	O
its	O
capability	O
to	O
make	O
decisions	O
based	O
on	O
predictions	O
in	O
the	O
previous	O
steps	O
.	O
We	O
explore	O
practical	O
implications	O
of	O
this	O
result	O
in	O
section	O
5	O
.	O

In	O
this	O
section	O
,	O
we	O
explore	O
the	O
practical	O
implications	O
of	O
our	O
results	O
.	O
Our	O
experiments	O
are	O
geared	O
towards	O
answering	O
the	O
following	O
questions	O
:	O
Q1	O
.	O
Are	O
there	O
any	O
practical	O
implications	O
of	O
the	O
limitation	O
of	O
Transformers	O
without	O
decoder	O
-	O
encoder	O
residual	O
connections	O
?	O
What	O
tasks	O
can	O
they	O
do	O
or	O
not	O
do	O
compared	O
to	O
vanilla	O
Transformers	O
?	O
Q2	O
.	O
Is	O
there	O
any	O
additional	O
benefit	O
of	O
using	O
positional	O
masking	O
as	O
opposed	O
to	O
absolute	O
positional	O
encoding	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
?	O
Although	O
we	O
showed	O
that	O
Transformers	O
without	O
decoder	O
-	O
encoder	O
residual	B-MethodName
connection	I-MethodName
are	O
not	O
Turing	O
complete	O
,	O
it	O
does	O
not	O
imply	O
that	O
they	O
are	O
incapable	O
of	O
performing	O
all	O
the	O
tasks	O
.	O
Our	O
results	O
suggest	O
that	O
they	O
are	O
limited	O
in	O
their	O
capability	O
to	O
make	O
inferences	O
based	O
on	O
their	O
previous	O
computations	O
,	O
which	O
is	O
required	O
for	O
tasks	O
such	O
as	O
counting	O
and	O
language	O
modeling	O
.	O
However	O
,	O
it	O
can	O
be	O
shown	O
that	O
the	O
model	O
is	O
capable	O
of	O
performing	O
tasks	O
which	O
rely	O
only	O
on	O
information	O
provided	O
at	O
a	O
given	O
step	O
such	O
as	O
copying	O
and	O
mapping	O
.	O
For	O
such	O
tasks	O
,	O
given	O
positional	O
information	O
at	O
a	O
particular	O
step	O
,	O
the	O
model	O
can	O
look	O
up	O
the	O
corresponding	O
input	O
and	O
map	O
it	O
via	O
the	O
FFN	O
.	O
We	O
evaluate	O
these	O
hypotheses	O
via	O
our	O
experiments	O
.	O
For	O
our	O
experiments	O
on	O
synthetic	O
data	O
,	O
we	O
consider	O
two	O
tasks	O
,	O
namely	O
the	O
copy	O
task	O
and	O
the	O
counting	O
task	O
.	O
For	O
the	O
copy	O
task	O
,	O
the	O
goal	O
of	O
a	O
model	O
is	O
to	O
reproduce	O
the	O
input	O
sequence	O
.	O
We	O
sample	O
sentences	O
of	O
lengths	O
between	O
5	O
-	O
12	O
words	O
from	O
Penn	B-DatasetName
Treebank	I-DatasetName
and	O
create	O
a	O
train	O
-	O
test	O
split	O
of	O
40k	O
-	O
1k	O
with	O
all	O
sentences	O
belonging	O
to	O
the	O
same	O
range	O
of	O
length	O
.	O
In	O
the	O
counting	O
task	O
,	O
we	O
create	O
a	O
very	O
simple	O
dataset	O
where	O
the	O
model	O
is	O
given	O
one	O
number	O
between	O
0	B-DatasetName
and	O
100	O
as	O
input	O
and	O
its	O
goal	O
is	O
to	O
predict	O
the	O
next	O
five	O
numbers	O
.	O
Since	O
only	O
a	O
single	O
input	O
is	O
provided	O
to	O
the	O
encoder	O
,	O
it	O
is	O
necessary	O
for	O
the	O
decoder	O
to	O
be	O
able	O
to	O
make	O
inferences	O
based	O
on	O
its	O
previous	O
predictions	O
to	O
perform	O
this	O
task	O
.	O
The	O
benefit	O
of	O
conducting	O
these	O
experiments	O
on	O
synthetic	O
data	O
is	O
that	O
they	O
isolate	O
the	O
phenomena	O
we	O
wish	O
to	O
evaluate	O
.	O
For	O
both	O
these	O
tasks	O
,	O
we	O
compare	O
vanilla	O
Transformer	B-MethodName
with	O
the	O
one	O
without	O
decoder	O
-	O
encoder	O
residual	B-MethodName
connection	I-MethodName
.	O
As	O
a	O
baseline	O
we	O
also	O
consider	O
the	O
model	O
without	O
decoder	O
-	O
decoder	O
residual	B-MethodName
connection	I-MethodName
,	O
since	O
according	O
to	O
our	O
results	O
,	O
that	O
connection	O
does	O
not	O
influence	O
the	O
computational	O
power	O
of	O
the	O
model	O
.	O
We	O
implement	O
a	O
single	O
layer	O
encoderdecoder	O
network	O
with	O
only	O
a	O
single	O
attention	O
head	O
in	O
each	O
block	O
.	O
We	O
then	O
assess	O
the	O
influence	O
of	O
the	O
limitation	O
on	O
Machine	B-TaskName
Translation	I-TaskName
which	O
requires	O
a	O
model	O
to	O
do	O
a	O
combination	O
of	O
both	O
mapping	O
and	O
inferring	O
from	O
computations	O
in	O
previous	O
timesteps	O
.	O
We	O
evaluate	O
the	O
models	O
on	O
IWSLT'14	O
German	O
-	O
English	O
dataset	O
and	O
IWSLT'15	O
English	O
-	O
Vietnamese	O
dataset	O
.	O
We	O
again	O
compare	O
vanilla	O
Transformer	B-MethodName
with	O
the	O
ones	O
without	O
decoder	O
-	O
encoder	O
and	O
decoder	O
-	O
decoder	O
residual	B-MethodName
connection	I-MethodName
.	O
While	O
tuning	O
the	O
models	O
,	O
we	O
vary	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
from	O
1	O
to	O
4	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
warmup	O
steps	O
and	O
the	O
number	O
of	O
heads	O
.	O
Specifications	O
of	O
the	O
models	O
,	O
experimental	O
setup	O
,	O
datasets	O
and	O
sample	O
outputs	O
can	O
be	O
found	O
in	O
Sec	O
.	O
E	O
in	O
the	O
Appendix	O
.	O
Results	O
on	O
the	O
effect	O
of	O
residual	O
connections	O
on	O
synthetic	O
tasks	O
can	O
be	O
found	O
in	O
Table	O
1	O
.	O
As	O
per	O
our	O
hypothesis	O
,	O
all	O
the	O
variants	O
are	O
able	O
to	O
perfectly	O
perform	O
the	O
copy	O
task	O
.	O
For	O
the	O
counting	O
task	O
,	O
the	O
one	O
without	O
decoder	O
-	O
encoder	O
residual	B-MethodName
connection	I-MethodName
is	O
incapable	O
of	O
performing	O
it	O
.	O
However	O
,	O
the	O
other	O
two	O
including	O
the	O
one	O
without	O
decoder	O
-	O
decoder	O
residual	B-MethodName
connection	I-MethodName
are	O
able	O
to	O
accomplish	O
the	O
task	O
by	O
learning	O
to	O
make	O
decisions	O
based	O
on	O
their	O
prior	O
predictions	O
.	O
Table	O
3	O
provides	O
some	O
illustrative	O
sample	O
outputs	O
of	O
the	O
models	O
.	O
For	O
the	O
MT	O
task	O
,	O
results	O
can	O
be	O
found	O
in	O
Table	O
2	O
.	O
While	O
the	O
drop	O
from	O
removing	O
decoder	O
-	O
encoder	O
residual	B-MethodName
connection	I-MethodName
is	O
significant	O
,	O
it	O
is	O
still	O
able	O
to	O
perform	O
reasonably	O
well	O
since	O
the	O
task	O
can	O
be	O
largely	O
fulfilled	O
by	O
mapping	O
different	O
words	O
from	O
one	O
sentence	O
to	O
another	O
.	O
For	O
positional	O
masking	O
,	O
our	O
proof	O
technique	O
suggests	O
that	O
due	O
to	O
lack	O
of	O
positional	O
encodings	O
,	O
the	O
model	O
must	O
come	O
up	O
with	O
its	O
own	O
mechanism	O
to	O
make	O
order	O
related	O
decisions	O
.	O
Our	O
hypothesis	O
is	O
that	O
,	O
if	O
it	O
is	O
able	O
to	O
develop	O
such	O
a	O
mechanism	O
,	O
it	O
should	O
be	O
able	O
to	O
generalize	O
to	O
higher	O
lengths	O
and	O
not	O
overfit	O
on	O
the	O
data	O
it	O
is	O
provided	O
.	O
To	O
evaluate	O
this	O
claim	O
,	O
we	O
simply	O
extend	O
the	O
copy	O
task	O
upto	O
higher	O
lengths	O
.	O
The	O
training	O
set	O
remains	O
the	O
same	O
as	O
before	O
,	O
containing	O
sentences	O
of	O
length	O
5	O
-	O
12	O
words	O
.	O
We	O
create	O
5	O
different	O
validation	O
sets	O
each	O
containing	O
1k	O
sentences	O
each	O
.	O
The	O
first	O
set	O
contains	O
sentences	O
within	O
the	O
same	O
length	O
as	O
seen	O
in	O
training	O
(	O
5	O
-	O
12	O
words	O
)	O
,	O
the	O
second	O
set	O
contains	O
sentences	O
of	O
length	O
13	O
-	O
15	O
words	O
while	O
the	O
third	O
,	O
fourth	O
and	O
fifth	O
sets	O
contain	O
sentences	O
of	O
lengths	O
15	O
-	O
20	O
,	O
21	O
-	O
25	O
and	O
26	O
-	O
30	O
words	O
respectively	O
.	O
We	O
consider	O
two	O
models	O
,	O
one	O
which	O
is	O
provided	O
absolute	O
positional	O
encodings	O
and	O
one	O
where	O
only	O
positional	O
masking	O
is	O
applied	O
.	O
Figure	O
3	O
shows	O
the	O
performance	O
of	O
these	O
models	O
across	O
various	O
lengths	O
.	O
The	O
model	O
with	O
positional	O
masking	O
clearly	O
generalizes	O
up	O
to	O
higher	O
lengths	O
although	O
its	O
performance	O
too	O
degrades	O
at	O
extreme	O
lengths	O
.	O
We	O
found	O
that	O
the	O
model	O
with	O
absolute	O
positional	O
encodings	O
during	O
training	O
overfits	O
on	O
the	O
fact	O
that	O
the	O
13th	O
token	O
is	O
always	O
the	O
terminal	O
symbol	O
.	O
Hence	O
,	O
when	O
evalu	O
-	O
ated	O
on	O
higher	O
lengths	O
it	O
never	O
produces	O
a	O
sentence	O
of	O
length	O
greater	O
than	O
12	O
.	O
Other	O
encoding	O
schemes	O
such	O
as	O
relative	O
positional	O
encodings	O
(	O
Shaw	O
et	O
al	O
,	O
2018	O
;	O
Dai	O
et	O
al	O
,	O
2019	O
)	O
can	O
generalize	O
better	O
,	O
since	O
they	O
are	O
inherently	O
designed	O
to	O
address	O
this	O
particular	O
issue	O
.	O
However	O
,	O
our	O
goal	O
is	O
not	O
to	O
propose	O
masking	O
as	O
a	O
replacement	O
of	O
positional	O
encodings	O
,	O
rather	O
it	O
is	O
to	O
determine	O
whether	O
the	O
mechanism	O
that	O
the	O
model	O
develops	O
during	O
training	O
is	O
helpful	O
in	O
generalizing	O
to	O
higher	O
lengths	O
.	O
Note	O
that	O
,	O
positional	O
masking	O
was	O
not	O
devised	O
by	O
keeping	O
generalization	O
or	O
any	O
other	O
benefit	O
in	O
mind	O
.	O
Our	O
claim	O
is	O
only	O
that	O
,	O
the	O
use	O
of	O
masking	O
does	O
not	O
limit	O
the	O
model	O
's	O
expressiveness	O
and	O
it	O
may	O
benefit	O
in	O
other	O
ways	O
,	O
but	O
during	O
practice	O
one	O
should	O
explore	O
each	O
of	O
the	O
mechanisms	O
and	O
even	O
a	O
combination	O
of	O
both	O
.	O
showed	O
that	O
a	O
combination	O
of	O
both	O
masking	O
and	O
encodings	O
is	O
better	O
able	O
to	O
learn	O
order	O
information	O
as	O
compared	O
to	O
explicit	O
encodings	O
.	O

Denote	O
the	O
set	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
by	O
[	O
n	O
]	O
.	O
Functions	O
defined	O
for	O
scalars	O
are	O
extended	O
to	O
vectors	O
in	O
the	O
natural	O
way	O
:	O
for	O
a	O
function	O
F	O
defined	O
on	O
a	O
set	O
A	O
,	O
for	O
a	O
sequence	O
(	O
a	O
1	O
,	O
.	O
.	O
.	O
,	O
a	O
n	O
)	O
of	O
elements	O
in	O
A	O
,	O
we	O
set	O
F	O
(	O
a	O
1	O
,	O
.	O
.	O
.	O
,	O
a	O
n	O
)	O
:	O
=	O
(	O
F	O
(	O
a	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
F	O
(	O
a	O
n	O
)	O
)	O
.	O
Indicator	O
I	O
(	O
P	O
)	O
is	O
1	O
,	O
if	O
predicate	O
P	O
is	O
true	O
and	O
is	O
0	B-DatasetName
otherwise	O
.	O
For	O
a	O
sequence	O
X	O
=	O
(	O
x	O
n	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
for	O
some	O
n	O
≥	O
0	B-DatasetName
,	O
we	O
set	O
X	O
j	O
:	O
=	O
(	O
x	O
n	O
,	O
.	O
.	O
.	O
,	O
x	O
j	O
)	O
for	O
j	O
{	O
n	O
,	O
i+1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
We	O
will	O
work	O
with	O
an	O
alphabet	O
Σ	O
=	O
{	O
β	B-HyperparameterName
1	O
,	O
.	O
.	O
.	O
,	O
β	B-HyperparameterName
m	O
}	O
,	O
with	O
β	B-HyperparameterName
1	O
=	O
#	O
and	O
β	B-HyperparameterName
m	O
=	O
$	O
.	O
The	O
special	O
symbols	O
#	O
and	O
$	O
correspond	O
to	O
the	O
beginning	O
and	O
end	O
of	O
the	O
input	O
sequence	O
,	O
resp	O
.	O
For	O
a	O
vector	O
v	O
,	O
by	O
0	B-DatasetName
v	O
we	O
mean	O
the	O
all	O
-	O
0	B-DatasetName
vector	O
of	O
the	O
same	O
dimension	O
as	O
v.	O
Lett	O
:	O
=	O
min	O
{	O
t	O
,	O
n	O
}	O

Here	O
we	O
describe	O
the	O
original	O
transformer	O
architecture	O
due	O
to	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
as	O
formalized	O
by	O
(	O
Pérez	O
et	O
al	O
,	O
2019	O
)	O
.	O
While	O
our	O
notation	O
and	O
definitions	O
largely	O
follow	O
(	O
Pérez	O
et	O
al	O
,	O
2019	O
)	O
,	O
they	O
are	O
not	O
identical	O
.	O
The	O
transformer	O
here	O
makes	O
use	O
of	O
positional	O
encoding	O
;	O
later	O
we	O
will	O
discuss	O
the	O
transformer	O
variant	O
using	O
directional	O
attention	O
but	O
without	O
using	O
positional	O
encoding	O
.	O
The	O
transformer	O
,	O
denoted	O
Trans	O
,	O
is	O
a	O
sequenceto	O
-	O
sequence	O
architecture	O
.	O
Its	O
input	O
consists	O
of	O
(	O
i	O
)	O
a	O
sequence	O
X	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
of	O
vectors	O
in	O
Q	O
d	O
,	O
(	O
ii	O
)	O
a	O
seed	O
vector	O
y	O
0	B-DatasetName
Q	O
d	O
.	O
The	O
output	O
is	O
a	O
sequence	O
Y	O
=	O
(	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
r	O
)	O
of	O
vectors	O
in	O
Q	O
d	O
.	O
The	O
sequence	O
X	O
is	O
obtained	O
from	O
the	O
sequence	O
(	O
s	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
s	O
n	O
)	O
Σ	O
n+1	O
of	O
symbols	O
by	O
using	O
the	O
embedding	O
mentioned	O
earlier	O
:	O
x	O
i	O
=	O
f	O
(	O
f	O
b	O
(	O
s	O
i	O
)	O
,	O
pos	O
(	O
i	O
)	O
)	O
for	O
0	B-DatasetName
≤	O
i	O
≤	O
n.	O
The	O
transformer	O
consists	O
of	O
composition	O
of	O
transformer	O
encoder	O
and	O
a	O
transformer	B-MethodName
decoder	I-MethodName
.	O
The	O
transformer	O
encoder	O
is	O
obtained	O
by	O
composing	O
one	O
or	O
more	O
single	O
-	O
layer	O
encoders	O
and	O
similarly	O
the	O
transformer	B-MethodName
decoder	I-MethodName
is	O
obtained	O
by	O
composing	O
one	O
or	O
more	O
single	O
-	O
layer	O
decoders	O
.	O
For	O
the	O
feed	O
-	O
forward	O
networks	O
in	O
the	O
transformer	O
layers	O
we	O
use	O
the	O
activation	O
as	O
in	O
(	O
Siegelmann	O
and	O
Sontag	O
,	O
1992	O
)	O
,	O
namely	O
the	O
saturated	O
linear	O
activation	B-HyperparameterName
function	I-HyperparameterName
:	O
σ	O
(	O
x	O
)	O
=	O
	O
	O
	O
	O
0	B-DatasetName
if	O
x	O
<	O
0	B-DatasetName
,	O
x	O
if	O
0	B-DatasetName
≤	O
x	O
≤	O
1	O
,	O
1	O
if	O
x	O
>	O
1	O
.	O
(	O
4	O
)	O
As	O
mentioned	O
in	O
the	O
main	O
paper	O
,	O
we	O
can	O
easily	O
work	O
with	O
the	O
standard	O
ReLU	B-MethodName
activation	O
via	O
σ	O
(	O
x	O
)	O
=	O
ReLU	B-MethodName
(	O
x	O
)	O
−	O
ReLU	B-MethodName
(	O
x	O
−	O
1	O
)	O
.	O
In	O
the	O
following	O
,	O
after	O
defining	O
these	O
components	O
,	O
we	O
will	O
put	O
them	O
together	O
to	O
specify	O
the	O
full	O
transformer	O
architecture	O
.	O
But	O
we	O
begin	O
with	O
self	O
-	O
attention	O
mechanism	O
which	O
is	O
the	O
central	O
feature	O
of	O
the	O
transformer	O
.	O
Self	O
-	O
attention	O
.	O
The	O
self	O
-	O
attention	O
mechanism	O
takes	O
as	O
input	O
(	O
i	O
)	O
a	O
query	O
vector	O
q	O
,	O
(	O
ii	O
)	O
a	O
sequence	O
of	O
key	O
vectors	O
K	B-HyperparameterName
=	I-HyperparameterName
(	O
k	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
n	O
)	O
,	O
and	O
(	O
iii	O
)	O
a	O
sequence	O
of	O
value	O
vectors	O
V	O
=	O
(	O
v	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
n	O
)	O
.	O
All	O
vectors	O
are	O
in	O
Q	O
d	O
.	O
The	O
q	O
-	O
attention	O
over	O
keys	O
K	O
and	O
values	O
V	O
,	O
denoted	O
by	O
Att	O
(	O
q	O
,	O
K	O
,	O
V	O
)	O
,	O
is	O
a	O
vector	O
a	O
given	O
by	O
(	O
α	B-HyperparameterName
1	O
,	O
.	O
.	O
.	O
,	O
α	B-HyperparameterName
n	O
)	O
=	O
ρ	O
(	O
f	O
att	O
(	O
q	O
,	O
k	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
f	O
att	O
(	O
q	O
,	O
k	O
n	O
)	O
)	O
,	O
a	O
=	O
α	B-HyperparameterName
1	O
v	O
1	O
+	O
α	B-HyperparameterName
2	O
v	O
2	O
+	O
+	O
α	B-HyperparameterName
n	O
v	O
n	O
.	O
The	O
above	O
definition	O
uses	O
two	O
functions	O
ρ	O
and	O
f	O
att	O
which	O
we	O
now	O
describe	O
.	O
For	O
the	O
normalization	O
function	O
ρ	O
:	O
Q	O
n	O
Q	O
n	O
≥0	O
we	O
will	O
use	O
hardmax	O
:	O
for	O
x	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
Q	O
n	O
,	O
if	O
the	O
maximum	O
value	O
occurs	O
r	O
times	O
among	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
,	O
then	O
hardmax	O
(	O
x	O
)	O
i	O
:	O
=	O
1	O
/	O
r	O
if	O
x	O
i	O
is	O
a	O
maximum	O
value	O
and	O
hardmax	O
(	O
x	O
)	O
i	O
:	O
=	O
0	B-DatasetName
otherwise	O
.	O
In	O
practice	O
,	O
the	O
softmax	B-MethodName
is	O
often	O
used	O
but	O
its	O
output	O
values	O
are	O
in	O
general	O
not	O
rational	O
.	O
The	O
names	O
soft	O
-	O
attention	O
and	O
hard	O
-	O
attention	O
are	O
used	O
for	O
the	O
attention	O
mechanism	O
depending	O
on	O
which	O
normalization	O
function	O
is	O
used	O
.	O
For	O
the	O
Turing	O
-	O
completeness	O
proof	O
of	O
vanilla	O
transformers	O
,	O
the	O
scoring	O
function	O
f	O
att	O
used	O
is	O
a	O
combination	O
of	O
multiplicative	B-MethodName
attention	I-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
and	O
a	O
non	O
-	O
linear	O
function	O
:	O
f	O
att	O
(	O
q	O
,	O
k	O
i	O
)	O
=	O
−	O
q	O
,	O
k	O
i	O
.	O
For	O
directional	O
trans	O
-	O
formers	O
,	O
the	O
standard	O
multiplicative	B-MethodName
attention	I-MethodName
is	O
used	O
,	O
that	O
is	O
,	O
f	O
att	O
(	O
q	O
,	O
k	O
i	O
)	O
=	O
q	O
,	O
k	O
i	O
.	O
Transformer	B-MethodName
encoder	O
.	O
A	O
single	O
-	O
layer	O
encoder	O
is	O
a	O
function	O
Enc	O
(	O
X	O
;	O
θ	B-HyperparameterName
)	O
,	O
where	O
θ	B-HyperparameterName
is	O
the	O
parameter	O
vector	O
and	O
the	O
input	O
X	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
is	O
a	O
sequence	O
of	O
vector	O
in	O
Q	O
d	O
.	O
The	O
output	O
is	O
another	O
sequence	O
Z	O
=	O
(	O
z	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
n	O
)	O
of	O
vectors	O
in	O
Q	O
d	O
.	O
The	O
parameters	O
θ	B-HyperparameterName
specify	O
functions	O
Q	O
(	O
)	O
,	O
K	O
(	O
)	O
,	O
V	O
(	O
)	O
,	O
and	O
O	O
(	O
)	O
,	O
all	O
of	O
type	O
Q	O
d	O
Q	O
d	O
.	O
The	O
functions	O
Q	O
(	O
)	O
,	O
K	O
(	O
)	O
,	O
and	O
V	O
(	O
)	O
are	O
usually	O
linear	O
transformations	O
and	O
this	O
will	O
be	O
the	O
case	O
in	O
our	O
constructions	O
:	O
Q	O
(	O
x	O
i	O
)	O
=	O
x	O
T	O
i	O
W	O
Q	O
,	O
K	O
(	O
x	O
i	O
)	O
=	O
x	O
T	O
i	O
W	O
K	O
,	O
V	O
(	O
x	O
i	O
)	O
=	O
x	O
T	O
i	O
W	O
V	O
,	O
where	O
W	O
Q	O
,	O
W	O
K	O
,	O
W	O
V	O
Q	O
d×d	O
.	O
The	O
function	O
O	O
(	O
)	O
is	O
a	O
feed	O
-	O
forward	O
network	O
.	O
The	O
single	O
-	O
layer	O
encoder	O
is	O
then	O
defined	O
by	O
a	O
i	O
=	O
Att	O
(	O
Q	O
(	O
x	O
i	O
)	O
,	O
K	O
(	O
X	O
)	O
,	O
V	O
(	O
X	O
)	O
)	O
+	O
x	O
i	O
,	O
(	O
5	O
)	O
z	O
i	O
=	O
O	O
(	O
a	O
i	O
)	O
+	O
a	O
i	O
.	O
The	O
addition	O
operations	O
+	O
x	O
i	O
and	O
+	O
a	O
i	O
are	O
the	O
residual	O
connections	O
.	O
The	O
operation	O
in	O
(	O
5	O
)	O
is	O
called	O
the	O
encoder	O
-	O
encoder	O
attention	O
block	O
.	O
The	O
complete	O
L	O
-	O
layer	O
transformer	O
encoder	O
TEnc	O
(	O
L	O
)	O
(	O
X	O
;	O
θ	B-HyperparameterName
)	O
has	O
the	O
same	O
input	O
X	O
=	O
(	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
as	O
the	O
single	O
-	O
layer	O
encoder	O
.	O
By	O
contrast	O
,	O
its	O
output	O
consists	O
of	O
two	O
sequences	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
,	O
each	O
a	O
sequence	O
of	O
n	O
vectors	O
in	O
Q	O
d	O
.	O
The	O
encoder	O
TEnc	O
(	O
L	O
)	O
(	O
)	O
is	O
obtained	O
by	O
repeated	O
application	O
of	O
single	O
-	O
layer	O
encoders	O
,	O
each	O
with	O
its	O
own	O
parameters	O
;	O
and	O
at	O
the	O
end	O
,	O
two	O
trasformation	O
functions	O
K	O
L	O
(	O
)	O
and	O
V	O
L	O
(	O
)	O
are	O
applied	O
to	O
the	O
sequence	O
of	O
output	O
vectors	O
at	O
the	O
last	O
layer	O
.	O
Functions	O
K	O
(	O
L	O
)	O
(	O
)	O
and	O
V	O
(	O
L	O
)	O
(	O
)	O
are	O
linear	O
transformations	O
in	O
our	O
constructions	O
.	O
Formally	O
,	O
for	O
1	O
≤	O
≤	O
L	O
−	O
1	O
and	O
X	O
1	O
:	O
=	O
X	O
,	O
we	O
have	O
X	O
+1	O
=	O
Enc	O
(	O
X	O
;	O
θ	B-HyperparameterName
)	O
,	O
K	O
e	O
=	O
K	O
(	O
L	O
)	O
(	O
X	O
L	O
)	O
,	O
V	O
e	O
=	O
V	O
(	O
L	O
)	O
(	O
X	O
L	O
)	O
.	O
The	O
output	O
of	O
the	O
L	O
-	O
layer	O
Transformer	B-MethodName
encoder	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
=	O
TEnc	O
(	O
L	O
)	O
(	O
X	O
)	O
is	O
fed	O
to	O
the	O
Transformer	B-MethodName
decoder	I-MethodName
which	O
we	O
describe	O
next	O
.	O
Transformer	B-MethodName
decoder	I-MethodName
.	O
The	O
input	O
to	O
a	O
singlelayer	O
decoder	O
is	O
(	O
i	O
)	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
,	O
the	O
sequences	O
of	O
key	O
and	O
value	O
vectors	O
output	O
by	O
the	O
encoder	O
,	O
and	O
(	O
ii	O
)	O
a	O
sequence	O
Y	O
=	O
(	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
k	O
)	O
of	O
vectors	O
in	O
Q	O
d	O
.	O
The	O
output	O
is	O
another	O
sequence	O
Z	O
=	O
(	O
z	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
k	O
)	O
of	O
vectors	O
in	O
Q	O
d	O
.	O
Similar	O
to	O
the	O
single	O
-	O
layer	O
encoder	O
,	O
a	O
singlelayer	O
decoder	O
is	O
parameterized	O
by	O
functions	O
Q	O
(	O
)	O
,	O
K	O
(	O
)	O
,	O
V	O
(	O
)	O
and	O
O	O
(	O
)	O
and	O
is	O
defined	O
by	O
p	O
t	O
=	O
Att	O
(	O
Q	O
(	O
y	O
t	O
)	O
,	O
K	O
(	O
Y	O
t	O
)	O
,	O
V	O
(	O
Y	O
t	O
)	O
)	O
+	O
y	O
t	O
,	O
(	O
6	O
)	O
a	O
t	O
=	O
Att	O
(	O
p	O
t	O
,	O
K	O
e	O
,	O
V	O
e	O
)	O
+	O
p	O
t	O
,	O
(	O
7	O
)	O
z	O
t	O
=	O
O	O
(	O
a	O
t	O
)	O
+	O
a	O
t	O
.	O
The	O
operation	O
in	O
(	O
6	O
)	O
will	O
be	O
referred	O
to	O
as	O
the	O
decoder	O
-	O
decoder	O
attention	O
block	O
and	O
the	O
operation	O
in	O
(	O
7	O
)	O
as	O
the	O
decoder	O
-	O
encoder	O
attention	O
block	O
.	O
In	O
the	O
decoder	O
-	O
decoder	O
attention	O
block	O
,	O
positional	O
masking	O
is	O
applied	O
to	O
prevent	O
the	O
network	O
from	O
attending	O
over	O
symbols	O
which	O
are	O
ahead	O
of	O
them	O
.	O
An	O
L	O
-	O
layer	O
Transformer	B-MethodName
decoder	I-MethodName
is	O
obtained	O
by	O
repeated	O
application	O
of	O
L	O
single	O
-	O
layer	O
decoders	O
each	O
with	O
its	O
own	O
parameters	O
and	O
a	O
transformation	O
function	O
F	O
:	O
Q	O
d	O
Q	O
d	O
applied	O
to	O
the	O
last	O
vector	O
in	O
the	O
sequence	O
of	O
vectors	O
output	O
by	O
the	O
final	O
decoder	O
.	O
Formally	O
,	O
for	O
1	O
≤	O
≤	O
L	O
−	O
1	O
and	O
Y	O
1	O
=	O
Y	O
we	O
have	O
Y	O
+1	O
=	O
Dec	O
(	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
,	O
Y	O
;	O
θ	B-HyperparameterName
)	O
,	O
z	O
=	O
F	O
(	O
y	O
L	O
t	O
)	O
.	O
We	O
use	O
z	O
=	O
TDec	O
L	O
(	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
,	O
Y	O
;	O
θ	B-HyperparameterName
)	O
to	O
denote	O
an	O
L	O
-	O
layer	O
Transformer	B-MethodName
decoder	I-MethodName
.	O
Note	O
that	O
while	O
the	O
output	O
of	O
a	O
single	O
-	O
layer	O
decoder	O
is	O
a	O
sequence	O
of	O
vectors	O
,	O
the	O
output	O
of	O
an	O
L	O
-	O
layer	O
Transformer	B-MethodName
decoder	I-MethodName
is	O
a	O
single	O
vector	O
.	O
The	O
complete	O
Transformer	B-MethodName
.	O
A	O
Transformer	B-MethodName
network	O
receives	O
an	O
input	O
sequence	O
X	O
,	O
a	O
seed	O
vector	O
y	O
0	B-DatasetName
,	O
and	O
r	O
N.	O
For	O
t	O
≥	O
0	B-DatasetName
its	O
output	O
is	O
a	O
sequence	O
Y	O
=	O
(	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
r	O
)	O
defined	O
bỹ	O
y	O
t+1	O
=	O
TDec	O
TEnc	O
(	O
X	O
)	O
,	O
(	O
y	O
0	B-DatasetName
,	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
t	O
)	O
.	O
We	O
get	O
y	O
t+1	O
by	O
adding	O
positional	O
encoding	O
:	O
y	O
t+1	O
=	O
ỹ	O
t+1	O
+	O
pos	O
(	O
t	O
+	O
1	O
)	O
.	O
We	O
denote	O
the	O
complete	O
Transformer	B-MethodName
by	O
Trans	O
(	O
X	O
,	O
y	O
0	B-DatasetName
)	O
=	O
Y	O
.	O
The	O
Transformer	B-MethodName
"	O
halts	O
"	O
when	O
y	O
T	O
H	O
,	O
where	O
H	O
is	O
a	O
prespecified	O
halting	O
set	O
.	O
a	O
t	O
=	O
Att	O
(	O
p	O
t	O
,	O
K	O
e	O
,	O
V	O
e	O
)	O
+	O
p	O
t	O
The	O
result	O
follows	O
from	O
the	O
observation	O
that	O
without	O
the	O
residual	O
connections	O
,	O
a	O
t	O
=	O
Att	O
(	O
p	O
t	O
,	O
K	O
e	O
,	O
V	O
e	O
)	O
,	O
which	O
leads	O
to	O
a	O
t	O
=	O
n	O
i=1	O
α	B-HyperparameterName
i	O
v	O
e	O
i	O
for	O
some	O
α	B-HyperparameterName
i	O
s	O
such	O
that	O
n	O
i	O
α	B-HyperparameterName
i	O
=	O
1	O
.	O
Since	O
v	O
e	O
i	O
is	O
produced	O
from	O
the	O
encoder	O
,	O
the	O
vector	O
a	O
t	O
will	O
have	O
no	O
information	O
about	O
its	O
previous	O
hidden	O
state	O
values	O
.	O
Since	O
the	O
previous	O
hidden	O
state	O
information	O
was	O
computed	O
and	O
stored	O
in	O
p	O
t	O
,	O
without	O
the	O
residual	B-MethodName
connection	I-MethodName
,	O
the	O
information	O
in	O
a	O
t	O
depends	O
solely	O
on	O
the	O
output	O
of	O
the	O
encoder	O
.	O
One	O
could	O
argue	O
that	O
since	O
the	O
attention	O
weights	O
α	B-HyperparameterName
i	O
s	O
depend	O
on	O
the	O
query	O
vector	O
p	O
t	O
,	O
it	O
could	O
still	O
use	O
it	O
gain	O
the	O
necessary	O
information	O
from	O
the	O
vectors	O
v	O
e	O
i	O
s.	O
However	O
,	O
note	O
that	O
by	O
definition	O
of	O
hard	O
attention	O
,	O
the	O
attention	O
weights	O
α	B-HyperparameterName
i	O
in	O
a	O
t	O
=	O
n	O
i=1	O
α	B-HyperparameterName
i	O
v	O
e	O
i	O
can	O
either	O
be	O
zero	O
or	O
some	O
nonzero	O
value	O
depending	O
on	O
the	O
attention	O
logits	O
.	O
Since	O
the	O
attention	O
weights	O
α	B-HyperparameterName
i	O
are	O
such	O
that	O
n	O
i	O
α	B-HyperparameterName
i	O
=	O
1	O
and	O
all	O
the	O
nonzero	O
weights	O
are	O
equal	O
to	O
each	O
other	O
.	O
Thus	O
given	O
the	O
constraints	O
there	O
are	O
2	O
n	O
−1	O
ways	O
to	O
attend	O
over	O
n	O
inputs	O
excluding	O
the	O
case	O
where	O
no	O
input	O
is	O
attended	O
over	O
.	O
Hence	O
,	O
the	O
network	O
without	O
decoder	O
-	O
encoder	O
residual	B-MethodName
connection	I-MethodName
with	O
n	O
inputs	O
can	O
have	O
at	O
most	O
2	O
n	O
−1	O
distinct	O
a	O
t	O
values	O
.	O
This	O
implies	O
that	O
the	O
model	O
will	O
be	O
unable	O
to	O
perform	O
a	O
task	O
that	O
takes	O
n	O
inputs	O
and	O
has	O
to	O
produce	O
more	O
than	O
2	O
n	O
−	O
1	O
outputs	O
.	O
Note	O
that	O
,	O
such	O
a	O
limitation	O
will	O
not	O
exist	O
with	O
a	O
residual	B-MethodName
connection	I-MethodName
since	O
the	O
vector	O
a	O
t	O
=	O
Σ	O
n	O
i=1	O
α	B-HyperparameterName
i	O
v	O
e	O
i	O
+	O
p	O
t	O
can	O
take	O
arbitrary	O
number	O
of	O
values	O
depending	O
on	O
its	O
prior	O
computations	O
in	O
p	O
t	O
.	O
As	O
an	O
example	O
to	O
illustrate	O
the	O
limitation	O
,	O
consider	O
the	O
following	O
simple	O
problem	O
,	O
given	O
a	O
value	O
∆	O
,	O
where	O
0	B-DatasetName
≤	O
∆	O
≤	O
1	O
,	O
the	O
network	O
must	O
produce	O
the	O
values	O
0	B-DatasetName
,	O
∆	O
,	O
2∆	O
,	O
.	O
.	O
.	O
,	O
k∆	O
,	O
where	O
k	O
is	O
the	O
maximum	O
integer	O
such	O
that	O
k∆	O
≤	O
1	O
.	O
If	O
the	O
network	O
receives	O
a	O
single	O
input	O
∆	O
,	O
the	O
encoder	O
will	O
produce	O
only	O
one	O
particular	O
output	O
vector	O
and	O
regardless	O
of	O
what	O
the	O
value	O
of	O
the	O
query	O
vector	O
p	O
t	O
is	O
,	O
the	O
vector	O
a	O
t	O
will	O
be	O
constant	O
at	O
every	O
timestep	O
.	O
Since	O
a	O
t	O
is	O
fed	O
to	O
feedforward	B-MethodName
network	I-MethodName
which	O
maps	O
it	O
to	O
z	O
t	O
,	O
the	O
output	O
of	O
the	O
decoder	O
will	O
remain	O
the	O
same	O
at	O
every	O
timestep	O
and	O
it	O
can	O
not	O
produce	O
distinct	O
values	O
.	O
If	O
the	O
input	O
is	O
combined	O
with	O
n	O
−	O
1	O
auxiliary	O
symbols	O
(	O
such	O
as	O
#	O
and	O
$	O
)	O
,	O
then	O
the	O
network	O
can	O
only	O
produce	O
2	O
n	O
−1	O
outputs	O
.	O
Hence	O
,	O
the	O
model	O
will	O
be	O
incapable	O
of	O
performing	O
the	O
task	O
if	O
∆	O
<	O
1/2	O
n	O
.	O
Thus	O
the	O
model	O
can	O
not	O
perform	O
the	O
task	O
defined	O
above	O
which	O
RNNs	O
and	O
Vanilla	O
Transformers	O
can	O
easily	O
do	O
with	O
a	O
simple	O
counting	O
mechanism	O
via	O
their	O
recurrent	O
connection	O
.	O
For	O
the	O
case	O
of	O
multilayer	O
decoder	O
,	O
consider	O
any	O
L	O
layer	O
decoder	O
model	O
.	O
If	O
the	O
residual	B-MethodName
connection	I-MethodName
is	O
removed	O
,	O
the	O
output	O
of	O
decoder	O
-	O
encoder	O
attention	O
block	O
at	O
each	O
layer	O
is	O
a	O
(	O
)	O
t	O
=	O
n	O
i=1	O
α	B-HyperparameterName
(	O
)	O
i	O
v	O
e	O
i	O
for	O
1	O
≤	O
≤	O
L.	O
Observe	O
,	O
that	O
since	O
output	O
of	O
the	O
decoder	O
-	O
encoder	O
attention	O
block	O
in	O
the	O
last	O
(	O
L	O
-	O
th	O
)	O
layer	O
of	O
the	O
decoder	O
is	O
a	O
(	O
L	O
)	O
t	O
=	O
n	O
i=1	O
α	B-HyperparameterName
(	O
L	O
)	O
i	O
v	O
e	O
i	O
.	O
Since	O
the	O
output	O
of	O
the	O
L	O
layer	O
decoder	O
will	O
be	O
a	O
feedforward	B-MethodName
network	I-MethodName
over	O
a	O
(	O
L	O
)	O
t	O
,	O
the	O
computation	O
reduces	O
to	O
the	O
single	O
layer	O
decoder	O
case	O
.	O
Hence	O
,	O
similar	O
to	O
the	O
single	O
layer	O
case	O
,	O
if	O
the	O
task	O
requires	O
the	O
network	O
to	O
produce	O
values	O
of	O
a	O
t	O
that	O
come	O
from	O
a	O
set	O
with	O
size	O
at	O
least	O
2	O
n	O
,	O
then	O
the	O
network	O
will	O
not	O
be	O
able	O
to	O
perform	O
the	O
task	O
.	O
This	O
implies	O
that	O
the	O
model	O
without	O
decoderencoder	O
residual	B-MethodName
connection	I-MethodName
is	O
limited	O
in	O
its	O
capability	O
to	O
perform	O
tasks	O
which	O
requires	O
it	O
to	O
make	O
inferences	O
based	O
on	O
previously	O
generated	O
outputs	O
.	O

Proof	O
of	O
Lemma	B-DatasetName
C.3	O
.	O
We	O
construct	O
a	O
single	O
-	O
layer	O
encoder	O
achieving	O
the	O
desired	O
K	O
e	O
and	O
V	O
e	O
.	O
We	O
make	O
use	O
of	O
the	O
residual	O
connections	O
and	O
via	O
trivial	O
selfattention	O
we	O
get	O
that	O
z	O
i	O
=	O
x	O
i	O
.	O
More	O
specifically	O
for	O
i	O
[	O
n	O
]	O
we	O
have	O
V	O
(	O
1	O
)	O
(	O
x	O
i	O
)	O
=	O
0	B-DatasetName
,	O
a	O
i	O
=	O
0	B-DatasetName
+	O
x	O
i	O
,	O
O	O
(	O
a	O
i	O
)	O
=	O
0	B-DatasetName
,	O
z	O
i	O
=	O
0	B-DatasetName
+	O
a	O
i	O
=	O
x	O
i	O
.	O
V	O
(	O
1	O
)	O
(	O
x	O
i	O
)	O
=	O
0	B-DatasetName
can	O
be	O
achieved	O
by	O
setting	O
the	O
weight	O
matrix	O
as	O
the	O
all	O
-	O
0	B-DatasetName
matrix	O
.	O
Recall	B-MetricName
that	O
x	O
i	O
is	O
defined	O
as	O
x	O
i	O
=	O
[	O
0	B-DatasetName
h	O
,	O
s	O
i	O
,	O
0	B-DatasetName
h	O
,	O
i	O
,	O
1	O
]	O
.	O
We	O
then	O
apply	O
linear	O
transformations	O
in	O
K	O
(	O
z	O
i	O
)	O
=	O
z	O
i	O
W	O
k	O
and	O
V	O
(	O
z	O
i	O
)	O
=	O
z	O
i	O
W	O
v	O
,	O
where	O
W	O
T	O
k	B-HyperparameterName
=	I-HyperparameterName
	O
	O
	O
	O
	O
	O
	O
	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
−1	O
0	B-DatasetName
	O
	O
	O
	O
	O
	O
	O
	O
,	O
and	O
W	O
k	O
Q	O
d×d	O
,	O
and	O
similarly	O
one	O
can	O
obtain	O
v	O
i	O
by	O
setting	O
the	O
submatrix	O
of	O
W	O
v	O
Q	O
d×d	O
formed	O
by	O
the	O
first	O
d	O
−	O
2	O
rows	O
and	O
columns	O
to	O
the	O
identity	O
matrix	O
,	O
and	O
the	O
rest	O
of	O
the	O
entries	O
to	O
zeros	O
.	O
Lemma	B-DatasetName
C.5	O
.	O
Let	O
q	O
t	O
Q	O
d	O
be	O
a	O
query	O
vector	O
such	O
that	O
q	O
=	O
[	O
,	O
.	O
.	O
.	O
,	O
,	O
t	O
+	O
1	O
,	O
1	O
]	O
where	O
t	O
N	O
and	O
'	O
'	O
denotes	O
an	O
arbitrary	O
value	O
.	O
Then	O
we	O
have	O
Att	O
(	O
q	O
t	O
,	O
K	O
e	O
,	O
V	O
e	O
)	O
=	O
[	O
0	B-DatasetName
h	O
,	O
s	O
t+1	O
,	O
0	B-DatasetName
h	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
.	O
(	O
11	O
)	O
Proof	O
.	O
Recall	B-MetricName
that	O
p	O
t	O
=	O
y	O
t	O
=	O
[	O
h	O
t	O
,	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
0	B-DatasetName
,	O
t	O
+	O
1	O
,	O
1	O
]	O
and	O
k	O
i	O
=	O
[	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
0	B-DatasetName
,	O
−1	O
,	O
i	O
]	O
and	O
hence	O
p	O
t	O
,	O
k	O
i	O
=	O
i	O
−	O
(	O
t	O
+	O
1	O
)	O
,	O
f	O
att	O
(	O
p	O
t	O
,	O
k	O
i	O
)	O
=	O
−	O
|	O
i	O
−	O
(	O
t	O
+	O
1	O
)	O
|	O
.	O
Thus	O
,	O
for	O
i	O
[	O
n	O
]	O
,	O
the	O
scoring	O
functionf	O
att	O
(	O
p	O
t	O
,	O
k	O
i	O
)	O
has	O
the	O
maximum	O
value	O
0	B-DatasetName
at	O
index	O
i	O
=	O
t	O
+	O
1	O
if	O
t	O
<	O
n	O
;	O
for	O
t	O
≥	O
n	O
,	O
the	O
maximum	O
value	O
t	O
+	O
1	O
−	O
n	O
is	O
achieved	O
for	O
i	O
=	O
n.	O
Therefore	O
Att	O
(	O
p	O
t	O
,	O
K	O
e	O
,	O
V	O
e	O
)	O
=	O
s	O
t+1	O
.	O
Proof	O
of	O
Lemma	B-DatasetName
C.4	O
.	O
Recall	B-MetricName
that	O
a	O
t	O
=	O
[	O
h	O
t	O
,	O
s	O
t+1	O
,	O
0	B-DatasetName
h	O
,	O
t	O
+	O
1	O
,	O
1	O
]	O
Network	O
O	O
(	O
a	O
t	O
)	O
is	O
of	O
the	O
form	O
O	O
(	O
a	O
t	O
)	O
=	O
W	O
2	O
σ	O
(	O
W	O
1	O
a	O
t	O
+	O
b	O
1	O
)	O
,	O
where	O
W	O
i	O
Q	O
d×d	O
and	O
b	O
Q	O
d	O
and	O
W	O
1	O
=	O
d	O
h	O
d	O
e	O
d	O
h	O
2	O
d	O
h	O
d	O
e	O
d	O
h	O
2	O
	O
	O
	O
	O
	O
W	O
h	O
W	O
x	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
I	O
0	B-DatasetName
0	B-DatasetName
I	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
I	O
	O
	O
	O
	O
	O
and	O
b	O
1	O
=	O
[	O
b	O
h	O
,	O
0	B-DatasetName
s	O
,	O
0	B-DatasetName
h	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
.	O
Hence	O
σ	O
(	O
W	O
1	O
a	O
t	O
+	O
b	O
1	O
)	O
=	O
[	O
σ	O
(	O
W	O
h	O
h	O
t	O
+	O
W	O
x	O
s	O
t+1	O
+	O
b	O
)	O
,	O
s	O
t+1	O
,	O
h	O
t	O
,	O
t	O
+	O
1	O
,	O
1	O
]	O
Next	O
we	O
define	O
W	O
2	O
by	O
W	O
2	O
=	O
d	O
h	O
d	O
e	O
d	O
h	O
2	O
d	O
h	O
d	O
e	O
d	O
h	O
2	O
	O
	O
	O
	O
	O
I	O
0	B-DatasetName
−I	O
0	B-DatasetName
0	B-DatasetName
−I	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
−I	O
	O
	O
	O
	O
	O
.	O
This	O
leads	O
to	O
O	O
(	O
a	O
t	O
)	O
=	O
W	O
2	O
σ	O
(	O
W	O
1	O
a	O
t	O
+	O
b	O
1	O
)	O
=	O
[	O
σ	O
(	O
W	O
h	O
h	O
t	O
+	O
W	O
x	O
s	O
t+1	O
+	O
b	O
)	O
−	O
h	O
t	O
,	O
−s	O
t+1	O
,	O
0	B-DatasetName
h	O
,	O
−	O
(	O
t	O
+	O
1	O
)	O
,	O
−1	O
]	O
,	O
which	O
is	O
what	O
we	O
wanted	O
to	O
prove	O
.	O

The	O
models	O
under	O
consideration	O
are	O
the	O
vanilla	O
Transformer	B-MethodName
,	O
the	O
one	O
without	O
decoder	O
-	O
encoder	O
residual	B-MethodName
connection	I-MethodName
and	O
the	O
one	O
without	O
decoderdecoder	O
residual	B-MethodName
connection	I-MethodName
.	O
For	O
the	O
synthetic	O
tasks	O
,	O
we	O
implement	O
a	O
single	O
layer	O
encoder	O
-	O
decoder	O
network	O
with	O
only	O
a	O
single	O
attention	O
head	O
in	O
each	O
block	O
.	O
Our	O
implementation	O
of	O
the	O
Transformer	B-MethodName
is	O
adapted	O
from	O
the	O
implementation	O
of	O
(	O
Rush	O
,	O
2018	O
)	O
.	O
Table	O
4	O
provides	O
some	O
illustrative	O
sample	O
outputs	O
of	O
the	O
models	O
for	O
the	O
copy	O
task	O
.	O
For	O
the	O
machine	B-TaskName
translation	I-TaskName
task	O
,	O
we	O
use	O
Open	O
-	O
NMT	O
(	O
Klein	O
et	O
al	O
,	O
2017	O
)	O
for	O
our	O
implementation	O
.	O
For	O
preprocessing	O
the	O
German	O
-	O
English	O
dataset	O
we	O
used	O
the	O
script	O
from	O
fairseq	O
.	O
The	O
dataset	O
contains	O
about	O
153k	O
training	O
sentences	O
,	O
7k	O
development	O
sentences	O
and	O
7k	O
test	O
sentences	O
.	O
The	O
hyperparameters	O
to	O
train	O
the	O
vanilla	O
Transformer	B-MethodName
were	O
obtained	O
from	O
fairseq	O
's	O
guidelines	O
.	O
We	O
tuned	O
the	O
parameters	O
on	O
the	O
validation	O
set	O
for	O
the	O
two	O
baseline	O
model	O
.	O
To	O
preprocess	O
the	O
English	O
-	O
Vietnamese	O
dataset	O
,	O
we	O
follow	O
Luong	O
and	O
Manning	O
(	O
2015	O
)	O
.	O
The	O
dataset	O
contains	O
about	O
133k	O
training	O
sentences	O
.	O
We	O
use	O
the	O
tst2012	O
dataset	O
containing	O
1.5k	O
sentences	O
for	O
validation	O
and	O
tst2013	O
containing	O
1.3k	O
sentences	O
as	O
test	O
set	O
.	O
We	O
use	O
noam	O
optimizer	B-HyperparameterName
in	O
all	O
our	O
experiments	O
.	O
While	O
tuning	O
the	O
network	O
,	O
we	O
vary	O
the	O
number	O
of	O
layer	O
from	O
1	O
to	O
4	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
the	O
number	O
of	O
heads	O
,	O
the	O
warmup	O
steps	O
,	O
embedding	O
size	O
and	O
feedforward	O
embedding	O
size	O
.	O

Our	O
implementation	O
for	O
directional	O
transformer	O
is	O
based	O
on	O
but	O
we	O
use	O
only	O
unidirectional	O
masking	O
as	O
opposed	O
to	O
bidirectional	O
used	O
in	O
their	O
setup	O
.	O
While	O
tuning	O
the	O
models	O
,	O
we	O
vary	O
the	O
layers	O
from	O
1	O
to	O
4	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
warmup	O
steps	O
and	O
the	O
number	O
of	O
heads	O
.	O

vectors	O
h	O
i	O
s	O
are	O
reserved	O
for	O
computation	O
related	O
to	O
hidden	O
states	O
of	O
RNNs	O
,	O
s	O
i	O
s	O
are	O
reserved	O
for	O
input	O
embeddings	O
and	O
x	O
i	O
s	O
are	O
reserved	O
for	O
scalar	O
values	O
related	O
to	O
positional	O
operations	O
.	O
Given	O
an	O
input	O
sequence	O
s	O
0	B-DatasetName
s	O
1	O
s	O
2	O
s	O
n	O
Σ	O
*	O
where	O
s	O
0	B-DatasetName
=	O
#	O
and	O
s	O
n	O
=	O
$	O
,	O
we	O
use	O
an	O
embedding	O
function	O
f	O
:	O
Σ	O
Q	O
d	O
defined	O
as	O
Unlike	O
(	O
Pérez	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
use	O
the	O
dot	O
product	O
as	O
our	O
scoring	O
function	O
as	O
used	O
in	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
in	O
the	O
attention	O
mechanism	O
in	O
our	O
construction	O
,	O
f	O
att	O
(	O
q	O
i	O
,	O
k	O
j	O
)	O
=	O
q	O
i	O
,	O
k	O
j	O
.	O
For	O
the	O
computation	O
of	O
the	O
Transformer	B-MethodName
,	O
we	O
also	O
use	O
a	O
vector	O
sequence	O
in	O
Q	O
|	O
Σ	O
|	O
defined	O
by	O
where	O
0	B-DatasetName
≤	O
t	O
≤	O
n.	O
The	O
vector	O
ω	O
t	O
=	O
(	O
ω	O
t	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
ω	O
t	O
,	O
|	O
Σ	O
|	O
)	O
contains	O
the	O
proportion	O
of	O
each	O
input	O
symbol	O
till	O
step	O
t	O
for	O
0	B-DatasetName
≤	O
t	O
≤	O
n.	O
Set	O
ω	O
−1	O
=	O
0	B-DatasetName
.	O
From	O
the	O
defintion	O
of	O
ω	O
t	O
,	O
it	O
follows	O
that	O
at	O
any	O
step	O
1	O
≤	O
k	O
≤	O
|	O
Σ	O
|	O
we	O
have	O
where	O
φ	O
t	O
,	O
k	O
denotes	O
the	O
number	O
of	O
times	O
the	O
k	O
-	O
th	O
symbol	O
β	B-HyperparameterName
k	O
in	O
Σ	O
has	O
appeared	O
till	O
the	O
t	O
-	O
th	O
step	O
.	O
Note	O
that	O
ω	O
t	O
,	O
0	B-DatasetName
=	O
1	O
t+1	O
since	O
the	O
first	O
coordinate	O
corresponds	O
to	O
the	O
proportion	O
of	O
the	O
start	O
symbol	O
#	O
which	O
appears	O
only	O
once	O
at	O
t	O
=	O
0	B-DatasetName
.	O
Similarly	O
,	O
ω	O
t	O
,	O
|	O
Σ	O
|	O
=	O
0	B-DatasetName
for	O
0	B-DatasetName
≤	O
t	O
<	O
n	O
and	O
ω	O
t	O
,	O
|	O
Σ	O
|	O
=	O
1/	O
(	O
t	O
+	O
1	O
)	O
for	O
t	O
≥	O
n	O
,	O
since	O
the	O
end	O
symbol	O
$	O
does	O
n't	O
appear	O
till	O
the	O
end	O
of	O
the	O
input	O
and	O
it	O
appears	O
only	O
once	O
at	O
t	O
=	O
n.	O
We	O
define	O
two	O
more	O
sequences	O
of	O
vectors	O
in	O
Here	O
∆	O
t	O
denotes	O
the	O
difference	O
in	O
the	O
proportion	O
of	O
symbols	O
between	O
the	O
t	O
-	O
th	O
and	O
(	O
t	O
−	O
1	O
)	O
-	O
th	O
steps	O
,	O
with	O
the	O
applicatin	O
of	O
sigmoid	B-MethodName
activation	I-MethodName
.	O
In	O
vector	O
δ	B-HyperparameterName
t	O
,	O
the	O
last	O
coordinate	O
of	O
∆	O
t	O
has	O
been	O
replaced	O
with	O
1/2	O
t+1	O
.	O
The	O
last	O
coordinate	O
in	O
ω	O
t	O
indicates	O
the	O
proportion	O
of	O
the	O
terminal	O
symbol	O
$	O
and	O
hence	O
the	O
last	O
value	O
in	O
∆	O
t	O
denotes	O
the	O
change	O
in	O
proportion	O
of	O
$	O
.	O
We	O
set	O
the	O
last	O
coordinate	O
in	O
δ	B-HyperparameterName
t	O
to	O
an	O
exponentially	O
decreasing	O
sequence	O
so	O
that	O
after	O
n	O
steps	O
we	O
always	O
have	O
a	O
nonzero	O
score	O
for	O
the	O
terminal	O
symbol	O
and	O
it	O
is	O
taken	O
as	O
input	O
in	O
the	O
underlying	O
RNN	O
.	O
Different	O
and	O
perhaps	O
simpler	O
choices	O
for	O
the	O
last	O
coordinate	O
of	O
δ	B-HyperparameterName
t	O
may	O
be	O
possible	O
.	O
Note	O
that	O
0	B-DatasetName
≤	O
∆	O
t	O
,	O
k	O
≤	O
1	O
and	O
0	B-DatasetName
≤	O
δ	B-HyperparameterName
t	O
,	O
k	O
≤	O
1	O
for	O
0	B-DatasetName
≤	O
t	O
≤	O
n	O
and	O
1	O
≤	O
k	O
≤	O
|	O
Σ	O
|	O
.	O
Construction	O
of	O
TEnc	O
.	O
The	O
input	O
to	O
the	O
network	O
DTrans	O
M	O
is	O
the	O
sequence	O
(	O
s	O
0	B-DatasetName
,	O
s	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
n−1	O
,	O
s	O
n	O
)	O
where	O
s	O
0	B-DatasetName
=	O
#	O
and	O
s	O
n	O
=	O
$	O
.	O
Our	O
encoder	O
is	O
a	O
simple	O
single	O
layer	O
network	O
such	O
that	O
TEnc	O
(	O
x	O
0	B-DatasetName
,	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
=	O
(	O
K	O
e	O
,	O
V	O
e	O
)	O
where	O
K	O
e	O
=	O
(	O
k	O
e	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
k	O
e	O
n	O
)	O
and	O
V	O
e	O
=	O
(	O
v	O
e	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
v	O
e	O
n	O
)	O
such	O
that	O
,	O
Similar	O
to	O
our	O
construction	O
of	O
the	O
encoder	O
for	O
vanilla	O
transformer	O
(	O
Lemma	B-DatasetName
C.3	O
)	O
,	O
the	O
above	O
K	O
e	O
and	O
V	O
e	O
can	O
be	O
obtained	O
by	O
making	O
the	O
output	O
of	O
Att	O
(	O
)	O
=	O
0	B-DatasetName
by	O
choosing	O
the	O
V	O
(	O
)	O
to	O
always	O
evaluate	O
to	O
0	B-DatasetName
and	O
similarly	O
for	O
O	O
(	O
)	O
,	O
and	O
using	O
residual	O
connections	O
.	O
Then	O
one	O
can	O
produce	O
K	O
e	O
and	O
V	O
e	O
via	O
simple	O
linear	O
transformations	O
using	O
K	O
(	O
)	O
and	O
V	O
(	O
)	O
.	O
Construction	O
of	O
TDec	O
.	O
At	O
the	O
t	O
-	O
th	O
step	O
we	O
denote	O
the	O
input	O
to	O
the	O
decoder	O
as	O
y	O
t	O
=	O
ỹ	O
t	O
,	O
where	O
0	B-DatasetName
≤	O
t	O
≤	O
r	O
,	O
where	O
r	O
is	O
the	O
step	O
where	O
the	O
decoder	O
halts	O
.	O
Let	O
h	O
−1	O
=	O
0	B-DatasetName
h	O
and	O
h	O
0	B-DatasetName
=	O
0	B-DatasetName
h	O
.	O
We	O
will	O
prove	O
by	O
induction	O
on	O
t	O
that	O
for	O
0	B-DatasetName
≤	O
t	O
≤	O
r	O
we	O
have	O
This	O
is	O
true	O
for	O
t	O
=	O
0	B-DatasetName
by	O
the	O
choice	O
of	O
seed	O
vector	O
:	O
Assuming	O
the	O
truth	O
of	O
(	O
14	O
)	O
for	O
t	O
,	O
we	O
show	O
it	O
for	O
t	O
+	O
1	O
.	O
Layer	O
1	O
.	O
Similar	O
to	O
the	O
construction	O
in	O
Lemma	B-DatasetName
C.3	O
,	O
in	O
the	O
decoder	O
-	O
decoder	O
attention	O
block	O
we	O
set	O
V	O
(	O
1	O
)	O
(	O
)	O
=	O
0	B-DatasetName
d	O
and	O
use	O
the	O
residual	O
connections	O
to	O
set	O
p	O
(	O
1	O
)	O
t	O
=	O
y	O
t	O
.	O
At	O
the	O
t	O
-	O
th	O
step	O
in	O
the	O
decoder	O
-	O
encoder	O
attention	O
block	O
of	O
layer	O
1	O
we	O
have	O
where	O
(	O
α	B-HyperparameterName
t	O
,	O
k	O
ē	O
t	O
=	O
hardmax	O
(	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
0	B-DatasetName
)	O
where	O
In	O
Lemma	B-DatasetName
D.2	O
we	O
construct	O
feed	O
-	O
forward	O
network	O
Layer	O
2	O
.	O
In	O
the	O
first	O
block	O
of	O
layer	O
2	O
,	O
we	O
set	O
the	O
value	O
transformation	O
function	O
to	O
identically	O
zero	O
similar	O
to	O
Lemma	B-DatasetName
C.3	O
,	O
i.e.	O
V	O
(	O
2	O
)	O
(	O
)	O
=	O
0	B-DatasetName
which	O
leads	O
to	O
the	O
output	O
of	O
Att	O
(	O
)	O
to	O
be	O
0	B-DatasetName
and	O
then	O
using	O
the	O
residual	B-MethodName
connection	I-MethodName
we	O
get	O
p	O
In	O
the	O
final	O
block	O
of	O
the	O
decoder	O
in	O
the	O
second	O
layer	O
,	O
the	O
computation	O
for	O
RNN	O
takes	O
place	O
.	O
In	O
Lemma	B-DatasetName
D.4	O
below	O
we	O
construct	O
the	O
feed	O
-	O
forward	O
proving	O
the	O
induction	O
hypothesis	O
(	O
14	O
)	O
for	O
t	O
+	O
1	O
,	O
and	O
completing	O
the	O
simulation	O
of	O
RNN	O
.	O

which	O
is	O
what	O
we	O
wanted	O
to	O
prove	O
.	O
where	O
t	O
≥	O
0	B-DatasetName
and	O
'	O
'	O
denotes	O
an	O
arbitrary	O
value	O
.	O
Then	O
we	O
have	O
Proof	O
.	O
Let	O
t	O
,	O
k	O
ē	O
t	O
be	O
the	O
vector	O
of	O
normalized	O
attention	O
scores	O
in	O
the	O
decoder	O
-	O
encoder	O
attention	O
block	O
of	O
layer	O
2	O
at	O
time	O
t.	O
Then	O
We	O
claim	O
that	O
Claim	O
1	O
.	O
For	O
t	O
≥	O
0	B-DatasetName
we	O
have	O
where	O
λ	O
t	O
is	O
a	O
normalization	O
factor	O
given	O
by	O
λ	O
t	O
=	O
n−1	O
j=0	O
I	O
(	O
s	O
j	O
=	O
s	O
t	O
)	O
.	O
We	O
now	O
prove	O
the	O
lemma	B-DatasetName
assuming	O
the	O
claim	O
above	O
.	O
Denote	O
the	O
L.H.S.	O
in	O
(	O
16	O
)	O
by	O
γ	B-HyperparameterName
t	O
.	O
Note	O
that	O
if	O
s	O
j	O
=	O
s	O
t	O
,	O
then	O
v	O
e	O
j	O
=	O
γ	B-HyperparameterName
t	O
.	O
Now	O
we	O
havē	O
completing	O
the	O
proof	O
of	O
the	O
lemma	B-DatasetName
modulo	O
the	O
proof	O
of	O
the	O
claim	O
,	O
which	O
we	O
prove	O
next	O
.	O
Proof	O
.	O
(	O
of	O
Claim	O
1	O
)	O
For	O
0	B-DatasetName
<	O
t	O
≤	O
n	O
,	O
the	O
vector	O
ω	O
t	O
−	O
ω	O
t−1	O
has	O
the	O
form	O
The	O
last	O
inequality	O
used	O
our	O
assumption	O
that	O
s	O
0	B-DatasetName
=	O
#	O
and	O
that	O
#	O
does	O
not	O
occur	O
at	O
any	O
later	O
time	O
and	O
therefore	O
φ	O
t−1	O
,	O
j	O
<	O
t.	O
On	O
the	O
other	O
hand	O
,	O
if	O
s	O
t	O
=	O
β	B-HyperparameterName
k	O
,	O
then	O
≤	O
0	B-DatasetName
.	O
This	O
leads	O
to	O
,	O
In	O
words	O
,	O
the	O
change	O
in	O
the	O
proportion	O
of	O
a	O
symbol	O
is	O
positive	O
from	O
step	O
t	O
−	O
1	O
to	O
t	O
if	O
and	O
only	O
if	O
it	O
is	O
the	O
input	O
symbol	O
at	O
the	O
t	O
-	O
th	O
step	O
.	O
For	O
0	B-DatasetName
≤	O
t	O
≤	O
n	O
and	O
1	O
≤	O
k	O
≤	O
|	O
Σ	O
|	O
,	O
this	O
leads	O
to	O
Recall	B-MetricName
that	O
p	O
(	O
2	O
)	O
t	O
=	O
z	O
(	O
1	O
)	O
t	O
which	O
comes	O
from	O
(	O
15	O
)	O
,	O
and	O
k	O
e	O
j	O
is	O
defined	O
in	O
(	O
13	O
)	O
.	O
We	O
reproduce	O
these	O
for	O
convenience	O
:	O
δt	O
,	O
1	O
2	O
t+1	O
,	O
0	B-DatasetName
ω	O
,	O
0	B-DatasetName
ω	O
,	O
ωt	O
]	O
,	O
k	O
e	O
j	O
=	O
[	O
0	B-DatasetName
h	O
,	O
0	B-DatasetName
h	O
,	O
0	B-DatasetName
s	O
,	O
s	O
j	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
ω	O
,	O
0	B-DatasetName
ω	O
,	O
0	B-DatasetName
ω	O
]	O
.	O
It	O
now	O
follows	O
that	O
for	O
0	B-DatasetName
<	O
t	O
<	O
n	O
,	O
if	O
0	B-DatasetName
≤	O
j	O
≤	O
t	O
is	O
such	O
that	O
s	O
j	O
=	O
s	O
t	O
,	O
then	O
t	O
,	O
k	O
e	O
j	O
=	O
δ	B-HyperparameterName
t	O
,	O
s	O
j	O
=	O
δ	B-HyperparameterName
t	O
,	O
i	O
=	O
0	B-DatasetName
.	O
And	O
for	O
0	B-DatasetName
<	O
t	O
<	O
n	O
,	O
if	O
0	B-DatasetName
≤	O
j	O
≤	O
t	O
is	O
such	O
that	O
s	O
j	O
=	O
s	O
t	O
=	O
β	B-HyperparameterName
i	O
,	O
then	O
t	O
,	O
k	O
e	O
j	O
=	O
δ	B-HyperparameterName
t	O
,	O
s	O
j	O
=	O
δ	B-HyperparameterName
t	O
,	O
i	O
=	O
t	O
−	O
φ	O
t−1	O
,	O
j	O
t	O
(	O
t	O
+	O
1	O
)	O
≥	O
1	O
t	O
(	O
t	O
+	O
1	O
)	O
.	O
Thus	O
,	O
for	O
0	B-DatasetName
≤	O
t	O
<	O
n	O
,	O
in	O
the	O
vector	O
p	O
(	O
2	O
)	O
t	O
,	O
k	O
e	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
p	O
t	O
,	O
k	O
e	O
t	O
,	O
the	O
largest	O
coordinates	O
are	O
the	O
ones	O
indexed	O
by	O
j	O
with	O
s	O
j	O
=	O
s	O
t	O
and	O
they	O
all	O
equal	O
t−φ	O
t−1	O
,	O
i	O
t	O
(	O
t+1	O
)	O
.	O
All	O
other	O
coordinates	O
are	O
0	B-DatasetName
.	O
For	O
t	O
≥	O
n	O
,	O
only	O
the	O
last	O
coordinate	O
p	O

Multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
(	O
MSA	O
)	O
has	O
been	O
an	O
emerging	O
research	O
field	O
for	O
its	O
potential	O
applications	O
in	O
human	O
-	O
computer	O
interaction	O
.	O
How	O
to	O
effectively	O
fuse	O
multimodal	O
information	O
including	O
textual	O
,	O
acoustic	O
,	O
and	O
visual	O
to	O
predict	O
the	O
sentiment	O
is	O
a	O
very	O
challenging	O
problem	O
and	O
has	O
been	O
addressed	O
by	O
many	O
previous	O
studies	O
.	O
Some	O
works	O
focus	O
on	O
introducing	O
additional	O
information	O
into	O
the	O
fusing	O
model	O
,	O
such	O
as	O
the	O
alignment	O
information	O
between	O
different	O
modal	O
features	O
and	O
unimodal	O
sentiment	O
labels	O
(	O
Yu	O
et	O
al	O
,	O
2021	O
)	O
.	O
And	O
other	O
works	O
consider	O
the	O
semantic	O
gaps	O
between	O
multimodal	O
data	O
and	O
adopt	O
the	O
adversarial	O
learning	O
(	O
Mai	O
et	O
al	O
,	O
2020	O
)	O
and	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
Hazarika	O
et	O
al	O
,	O
2020	O
)	O
to	O
map	O
different	O
modal	O
features	O
into	O
a	O
shared	O
subspace	O
.	O
Despite	O
the	O
apparent	O
success	O
of	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
,	O
their	O
performance	O
decreases	O
sharply	O
,	O
when	O
deployed	O
in	O
the	O
real	O
world	O
.	O
The	O
reason	O
is	O
that	O
the	O
input	O
texts	O
are	O
provided	O
by	O
the	O
ASR	O
models	O
,	O
which	O
usually	O
are	O
with	O
errors	O
because	O
of	O
the	O
limitation	O
of	O
model	O
capacity	O
.	O
To	O
further	O
analyze	O
this	O
problem	O
,	O
we	O
build	O
three	O
real	O
-	O
world	O
multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
datasets	O
based	O
on	O
the	O
existing	O
dataset	O
,	O
CMU	O
-	O
MOSI	B-DatasetName
(	O
Zadeh	O
et	O
al	O
,	O
2016	O
)	O
.	O
Specifically	O
,	O
we	O
adopt	O
three	O
widely	O
used	O
ASR	O
APIs	O
including	O
SpeechBrain	O
,	O
IBM	O
,	O
and	O
iFlytek	B-DatasetName
to	O
process	O
the	O
original	O
audios	O
and	O
obtain	O
the	O
recognized	O
texts	O
.	O
Then	O
,	O
we	O
replace	O
the	O
gold	O
texts	O
in	O
CMU	O
-	O
MOSI	B-DatasetName
with	O
the	O
ASR	O
results	O
and	O
get	O
three	O
realworld	O
datasets	O
,	O
namely	O
MOSI	B-DatasetName
-	O
SpeechBrain	O
,	O
MOSI	B-DatasetName
-	O
IBM	O
,	O
and	O
MOSI	B-DatasetName
-	O
iFlytek	B-DatasetName
.	O
We	O
evaluate	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
,	O
Self	O
-	O
MM	O
(	O
Yu	O
et	O
al	O
,	O
2021	O
)	O
,	O
and	O
report	O
the	O
mean	O
absolute	O
error	O
(	O
MAE	B-MetricName
)	O
on	O
the	O
multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
task	O
.	O
As	O
we	O
can	O
see	O
in	O
Figure	O
1	O
(	O
a	O
)	O
,	O
when	O
the	O
model	O
is	O
deployed	O
in	O
the	O
real	O
world	O
,	O
there	O
is	O
an	O
obvious	O
drop	O
in	O
model	O
performance	O
.	O
The	O
further	O
in	O
-	O
depth	O
analysis	O
of	O
ASR	O
errors	O
shows	O
that	O
the	O
sentiment	O
word	O
substitution	O
error	O
can	O
hurt	O
the	O
MSA	O
model	O
directly	O
.	O
The	O
reason	O
is	O
that	O
the	O
sentiment	O
words	O
in	O
the	O
text	O
are	O
the	O
most	O
important	O
clues	O
in	O
the	O
textual	O
modality	O
for	O
detecting	O
sentiment	O
and	O
incorrectly	O
recognizing	O
them	O
could	O
change	O
the	O
sentiment	O
conveyed	O
by	O
the	O
text	O
.	O
To	O
have	O
an	O
intuitive	O
understanding	O
of	O
the	O
sentiment	O
word	O
substitution	O
error	O
,	O
we	O
take	O
an	O
example	O
in	O
Figure	O
1	O
(	O
b	O
)	O
.	O
The	O
gold	O
text	O
is	O
"	O
And	O
I	O
was	O
really	O
upset	O
about	O
it	O
"	O
,	O
but	O
the	O
ASR	O
model	O
(	O
SpeechBrain	O
)	O
recognizes	O
the	O
sentiment	O
word	O
"	O
upset	O
"	O
wrongly	O
as	O
"	O
set	O
"	O
,	O
which	O
results	O
in	O
the	O
change	O
of	O
the	O
sentiment	O
semantics	O
of	O
the	O
text	O
and	O
directly	O
affects	O
the	O
MSA	O
model	O
performance	O
.	O
We	O
list	O
the	O
percentages	O
of	O
the	O
sentiment	O
word	O
substitution	O
error	O
on	O
the	O
MOSI	B-DatasetName
dataset	O
for	O
three	O
ASR	O
APIs	O
in	O
Figure	O
1	O
(	O
b	O
)	O
.	O
The	O
percentage	O
of	O
the	O
sentiment	O
word	O
substitution	O
error	O
on	O
the	O
MOSI	B-DatasetName
-	O
IBM	O
is	O
17.6	O
%	O
,	O
which	O
means	O
about	O
17	O
of	O
100	O
utterances	O
have	O
this	O
type	O
of	O
error	O
.	O
To	O
further	O
demonstrate	O
the	O
negative	O
effect	O
of	O
the	O
substitution	O
error	O
on	O
the	O
MSA	O
models	O
,	O
we	O
split	O
the	O
test	O
data	O
of	O
MOSI	B-DatasetName
-	O
IBM	O
into	O
two	O
groups	O
by	O
whether	O
there	O
is	O
a	O
substitution	O
error	O
.	O
We	O
evaluate	O
Self	O
-	O
MM	O
on	O
the	O
test	O
data	O
and	O
observe	O
that	O
the	O
misclassification	O
rate	O
of	O
the	O
group	O
in	O
which	O
the	O
substitution	O
error	O
exists	O
is	O
higher	O
than	O
the	O
other	O
group	O
(	O
29.9	O
%	O
vs	O
15.8	O
%	O
)	O
.	O
This	O
result	O
indicates	O
that	O
the	O
sentiment	O
word	O
substitution	O
error	O
could	O
hurt	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
MSA	O
model	O
.	O
To	O
tackle	O
this	O
problem	O
,	O
we	O
propose	O
the	O
sentiment	O
word	O
aware	O
multimodal	O
refinement	O
model	O
,	O
which	O
can	O
detect	O
the	O
positions	O
of	O
the	O
sentiment	O
words	O
in	O
the	O
text	O
and	O
dynamically	O
refine	O
the	O
word	B-TaskName
embeddings	I-TaskName
in	O
the	O
detected	O
positions	O
by	O
incorporating	O
multimodal	O
clues	O
.	O
The	O
basic	O
idea	O
of	O
our	O
approach	O
is	O
shown	O
in	O
Figure	O
1	O
(	O
c	O
)	O
.	O
We	O
consider	O
leveraging	O
the	O
multimodal	O
sentiment	O
information	O
,	O
namely	O
the	O
negative	O
sentiment	O
conveyed	O
by	O
the	O
low	O
voice	O
and	O
sad	O
face	O
,	O
and	O
textual	O
context	O
information	O
to	O
help	O
the	O
model	O
reconstruct	O
the	O
sentiment	O
semantics	O
for	O
the	O
input	O
embeddings	O
.	O
Specifically	O
,	O
we	O
first	O
use	O
the	O
sentiment	O
word	O
location	O
module	O
to	O
detect	O
the	O
positions	O
of	O
sentiment	O
words	O
and	O
meanwhile	O
utilize	O
the	O
strong	O
language	O
model	O
,	O
BERT	B-MethodName
,	O
to	O
generate	O
the	O
candidate	O
sentiment	O
words	O
.	O
Then	O
we	O
propose	O
the	O
multimodal	O
sentiment	O
word	O
refinement	O
module	O
to	O
refine	O
the	O
word	B-TaskName
embeddings	I-TaskName
based	O
on	O
the	O
multimodal	O
context	O
information	O
.	O
The	O
refinement	O
process	O
consists	O
of	O
two	O
parts	O
,	O
filtering	O
and	O
adding	O
.	O
We	O
apply	O
the	O
multimodal	O
gating	O
network	O
to	O
filter	O
out	O
useless	O
information	O
from	O
the	O
input	O
word	B-TaskName
embeddings	I-TaskName
in	O
the	O
filtering	O
process	O
and	O
use	O
the	O
multimodal	O
sentiment	O
word	O
attention	O
network	O
to	O
leverage	O
the	O
useful	O
information	O
from	O
candidate	O
sentiment	O
words	O
as	O
the	O
supplement	O
to	O
the	O
filtered	O
word	B-TaskName
embeddings	I-TaskName
in	O
the	O
adding	O
process	O
.	O
Finally	O
,	O
the	O
refined	O
sentiment	O
word	B-TaskName
embeddings	I-TaskName
are	O
used	O
for	O
multimodal	O
feature	O
fusion	O
.	O
We	O
conduct	O
extensive	O
experiments	O
on	O
the	O
MOSI	B-DatasetName
-	O
SpeechBrain	O
,	O
MOSI	B-DatasetName
-	O
IBM	O
,	O
and	O
MOSI	B-DatasetName
-	O
iFlytek	B-DatasetName
datasets	O
to	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
model	O
.	O
The	O
experimental	O
results	O
show	O
that	O
:	O
(	O
1	O
)	O
There	O
is	O
an	O
obvious	O
performance	O
drop	O
for	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
MSA	O
model	O
,	O
when	O
the	O
model	O
is	O
deployed	O
in	O
the	O
real	O
world	O
taking	O
the	O
ASR	O
outputs	O
as	O
the	O
input	O
of	O
textual	O
modality	O
;	O
(	O
2	O
)	O
Our	O
proposed	O
model	O
outperforms	O
all	O
baselines	O
,	O
which	O
can	O
dynamically	O
refine	O
the	O
sentiment	O
word	B-TaskName
embeddings	I-TaskName
by	O
leveraging	O
multimodal	O
information	O
.	O
The	O
main	O
contributions	O
of	O
this	O
work	O
are	O
as	O
follows	O
:	O
(	O
1	O
)	O
We	O
propose	O
a	O
novel	O
sentiment	O
word	O
aware	O
multimodal	O
refinement	O
model	O
for	O
multimodal	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
,	O
which	O
can	O
dynamically	O
reconstruct	O
the	O
sentiment	O
semantics	O
of	O
the	O
ASR	O
texts	O
with	O
errors	O
by	O
utilizing	O
the	O
multimodal	O
sentiment	O
information	O
resulting	O
in	O
more	O
robust	O
sentiment	O
prediction	O
;	O
(	O
2	O
)	O
We	O
validate	O
the	O
negative	O
effect	O
of	O
the	O
sentiment	O
word	O
substitution	O
error	O
on	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
MSA	O
model	O
through	O
the	O
in	O
-	O
depth	O
analysis	O
;	O
(	O
3	O
)	O
We	O
evaluate	O
our	O
model	O
on	O
three	O
real	O
-	O
world	O
datasets	O
,	O
and	O
the	O
experimental	O
results	O
demonstrate	O
that	O
our	O
model	O
outperforms	O
all	O
baselines	O
.	O

We	O
build	O
three	O
real	O
-	O
world	O
datasets	O
including	O
MOSI	B-DatasetName
-	O
SpeechBrain	O
,	O
MOSI	B-DatasetName
-	O
IBM	O
,	O
and	O
MOSI	B-DatasetName
-	O
iFlytek	B-DatasetName
,	O
on	O
CMU	O
-	O
MOSI	B-DatasetName
(	O
Zadeh	O
et	O
al	O
,	O
2016	O
)	O
.	O
CMU	O
-	O
MOSI	B-DatasetName
CMU	O
multimodal	O
opinion	O
-	O
level	O
sentiment	O
intensity	O
(	O
CMU	O
-	O
MOSI	B-DatasetName
)	O
consists	O
of	O
93	O
videos	O
collected	O
from	O
the	O
YouTube	O
website	O
.	O
The	O
length	O
of	O
the	O
videos	O
varies	O
from	O
2	O
-	O
5	O
mins	O
.	O
These	O
videos	O
are	O
split	O
into	O
2	O
,	O
199	O
short	O
video	O
clips	O
and	O
labeled	O
with	O
sentiment	O
scores	O
from	O
-	O
3	O
(	O
strongly	O
negative	O
)	O
to	O
3	O
(	O
strongly	O
positive	O
)	O
.	O
For	O
multimodal	O
features	O
,	O
we	O
extract	O
the	O
visual	O
features	O
using	O
Facet	O
,	O
which	O
can	O
extract	O
the	O
facial	O
action	O
units	O
(	O
Ekman	O
et	O
al	O
,	O
1980	O
)	O
from	O
each	O
frame	O
.	O
The	O
acoustic	O
features	O
are	O
obtained	O
by	O
applying	O
COVAREP	O
(	O
Degottex	O
et	O
al	O
,	O
2014	O
)	O
,	O
which	O
includes	O
12	O
Mel	O
-	O
frequency	O
cepstral	O
coefficients	O
(	O
MFCCs	O
)	O
and	O
other	O
low	O
-	O
level	O
features	O
.	O
However	O
,	O
the	O
provided	O
texts	O
of	O
the	O
utterances	O
in	O
the	O
MOSI	B-DatasetName
dataset	O
are	O
manually	O
transcribed	O
from	O
the	O
corresponding	O
videos	O
by	O
the	O
expert	O
transcribers	O
,	O
which	O
is	O
unrealistic	O
for	O
the	O
real	O
-	O
world	O
applications	O
to	O
obtain	O
the	O
texts	O
in	O
such	O
a	O
way	O
.	O
To	O
evaluate	O
the	O
models	O
in	O
the	O
real	O
world	O
,	O
we	O
replace	O
the	O
manually	O
gold	O
texts	O
in	O
the	O
dataset	O
with	O
the	O
texts	O
output	O
by	O
the	O
ASR	O
models	O
.	O
We	O
adopt	O
a	O
strong	O
ASR	O
model	O
and	O
two	O
widely	O
used	O
commercial	O
APIs	O
to	O
produce	O
the	O
texts	O
.	O
The	O
utilized	O
ASR	O
model	O
released	O
by	O
Ravanelli	O
et	O
al	O
(	O
2021	O
)	O
is	O
built	O
on	O
the	O
transformer	O
encoder	O
-	O
decoder	O
framework	O
and	O
trained	O
on	O
the	O
Librispeech	B-DatasetName
dataset	O
(	O
Panayotov	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
commercial	O
APIs	O
used	O
by	O
us	O
are	O
IBM	O
2	O
and	O
iFlytek	B-DatasetName
3	O
speech	O
-	O
to	O
-	O
text	O
APIs	O
,	O
which	O
are	O
wildly	O
used	O
by	O
researchers	O
and	O
software	O
developers	O
.	O
Finally	O
,	O
we	O
apply	O
the	O
three	O
ASR	O
models	O
to	O
transcribe	O
the	O
videos	O
into	O
texts	O
and	O
construct	O
three	O
new	O
datasets	O
,	O
namely	O
MOSI	B-DatasetName
-	O
SpeechBrain	O
,	O
MOSI	B-DatasetName
-	O
IBM	O
,	O
and	O
MOSI	B-DatasetName
-	O
iFlytek	B-DatasetName
.	O
We	O
report	O
the	O
WER	O
results	O
of	O
the	O
adopted	O
ASR	O
models	O
on	O
MOSI	B-DatasetName
in	O
Appendix	O
A.	O
Datasets	O
Models	O
Evaluation	O
Metrics	O
Has0	O
-	O
Acc	B-MetricName
↑	O
Has0	O
-	O
F1	B-MetricName
↑	O
Non0	O
-	O
Acc	B-MetricName
↑	O
Non0	O
-	O
F1	B-MetricName
↑	O
MAE	B-MetricName
↓	O
Corr	O
↑	O
MOSI	B-DatasetName
-	O
Noted	O
that	O
,	O
we	O
do	O
not	O
adopt	O
MOSEI	O
,	O
because	O
it	O
does	O
not	O
provide	O
the	O
original	O
video	O
clips	O
for	O
the	O
extracted	O
features	O
and	O
annotated	O
sentiment	O
labels	O
,	O
and	O
we	O
can	O
not	O
process	O
the	O
original	O
audios	O
.	O

We	O
use	O
Adam	B-MethodName
as	O
the	O
optimizer	B-HyperparameterName
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
5e	O
-	O
5	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
64	O
.	O
The	O
sentiment	O
threshold	O
is	O
set	O
to	O
0.5	O
while	O
detecting	O
the	O
sentiment	O
word	O
position	O
.	O
The	O
number	O
of	O
the	O
candidate	O
words	O
k	O
is	O
50	O
.	O
The	O
other	O
hyper	O
-	O
parameters	O
of	O
the	O
model	O
are	O
reported	O
in	O
Appendix	O
B.	O
All	O
experiments	O
are	O
run	O
on	O
an	O
Nvidia	O
Tesla	O
P100	O
GPU	O
.	O
We	O
run	O
five	O
times	O
and	O
report	O
the	O
average	O
performance	O
.	O
The	O
random	O
seeds	B-DatasetName
we	O
used	O
are	O
1111	O
,	O
1112	O
,	O
1113	O
,	O
1114	O
,	O
and	O
1115	O
.	O

For	O
the	O
MOSI	B-DatasetName
-	O
SpeechBrain	O
,	O
MOSI	B-DatasetName
-	O
IBM	O
,	O
and	O
MOSI	B-DatasetName
-	O
iFlytek	B-DatasetName
datasets	O
,	O
following	O
previous	O
work	O
(	O
Yu	O
et	O
al	O
,	O
2021	O
)	O
,	O
we	O
take	O
2	O
-	O
class	O
accuracy	B-MetricName
(	O
Acc	B-MetricName
-	O
2	O
)	O
,	O
F1	B-MetricName
score	I-MetricName
(	O
F1	B-MetricName
)	O
,	O
mean	O
absolute	O
error	O
(	O
MAE	B-MetricName
)	O
,	O
and	O
correlation	O
(	O
Corr	O
)	O
as	O
our	O
evaluation	O
metrics	O
.	O
And	O
for	O
Acc	B-MetricName
-	O
2	O
and	O
F1	B-MetricName
-	I-MetricName
Score	I-MetricName
,	O
we	O
calculate	O
them	O
in	O
two	O
ways	O
,	O
negative	O
/	O
non	O
-	O
negative	O
(	O
Non0	O
-	O
Acc	B-MetricName
,	O
Non0	O
-	O
F1	B-MetricName
)	O
and	O
negative	O
/	O
positive	O
(	O
Has0	O
-	O
Acc	B-MetricName
,	O
Has0	O
-	O
F1	B-MetricName
)	O
.	O
As	O
the	O
prediction	O
results	O
are	O
real	O
values	O
,	O
we	O
obtain	O
the	O
sentiment	O
classification	O
labels	O
by	O
mapping	O
the	O
sentiment	O
scores	O
into	O
labels	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
experiments	O
carried	O
out	O
for	O
the	O
different	O
subtasks	O
and	O
slots	O
and	O
the	O
datasets	O
provided	O
by	O
the	O
organization	O
.	O
These	O
datasets	O
are	O
composed	O
of	O
several	O
reviews	O
,	O
splitted	O
in	O
sentences	O
,	O
for	O
restaurants	O
and	O
laptops	O
topics	O
.	O
The	O
performance	O
of	O
slots	O
1	O
and	O
2	O
,	O
for	O
both	O
subtasks	O
,	O
are	O
measured	O
by	O
means	O
of	O
the	O
F	O
-	O
score	O
,	O
while	O
slot	O
3	O
is	O
evaluated	O
by	O
means	O
of	O
the	O
accuracy	B-MetricName
.	O
Table	O
1	O
represents	O
the	O
precision	O
,	O
recall	O
and	O
Fscore	O
obtained	O
for	O
restaurants	O
datasets	O
and	O
all	O
the	O
slots	O
submitted	O
.	O
For	O
English	O
language	O
,	O
an	O
unconstrained	O
system	O
was	O
presented	O
,	O
while	O
for	O
Spanish	O
language	O
both	O
constrained	O
and	O
unconstrained	O
systems	O
were	O
submitted	O
.	O
The	O
constrained	O
approaches	O
do	O
not	O
need	O
any	O
external	O
resources	O
,	O
but	O
only	O
the	O
training	O
files	O
provided	O
,	O
while	O
in	O
the	O
unconstrained	O
ones	O
,	O
food	O
and	O
drinks	O
lexicon	O
was	O
used	O
in	O
the	O
preprocessing	O
step	O
for	O
identifying	O
different	O
foods	O
and	O
drinks	O
.	O
It	O
can	O
be	O
seen	O
that	O
there	O
is	O
not	O
much	O
difference	O
between	O
constrained	O
and	O
unconstrained	O
systems	O
for	O
Spanish	O
language	O
,	O
so	O
we	O
can	O
assume	O
that	O
the	O
recognition	O
of	O
different	O
names	O
of	O
foods	O
or	O
drinks	O
does	O
not	O
increase	O
the	O
knowledge	O
of	O
the	O
classifiers	O
,	O
perform	O
-	O
ing	O
almost	O
equally	O
.	O
Moreover	O
,	O
we	O
can	O
state	O
that	O
our	O
system	O
perfoms	O
as	O
well	O
for	O
English	O
as	O
for	O
Spanish	O
language	O
.	O
In	O
Table	O
2	O
,	O
the	O
detailed	O
scores	O
for	O
slot	O
3	O
are	O
shown	O
in	O
English	O
language	O
,	O
for	O
restaurants	O
dataset	O
,	O
likewise	O
in	O
Table	O
3	O
As	O
it	O
can	O
be	O
seen	O
in	O
Table	O
2	O
and	O
Table	O
3	O
,	O
the	O
results	O
obtained	O
for	O
the	O
sentiment	O
slot	O
are	O
not	O
quite	O
competitive	O
with	O
the	O
other	O
teams	O
.	O
This	O
can	O
be	O
due	O
to	O
the	O
fact	O
that	O
our	O
system	O
is	O
fully	O
unsupervised	O
,	O
while	O
the	O
others	O
are	O
usually	O
supervised	O
systems	O
,	O
based	O
on	O
training	O
.	O
Moreover	O
,	O
we	O
performed	O
a	O
simple	O
adaptation	O
from	O
our	O
original	O
system	O
,	O
made	O
for	O
sentiment	B-TaskName
analysis	I-TaskName
in	O
Twitter	O
,	O
presented	O
to	O
SemEval	O
2015	O
,	O
so	O
there	O
is	O
still	O
a	O
lot	O
of	O
improvement	O
on	O
this	O
field	O
.	O

Below	O
,	O
we	O
define	O
our	O
DIG	O
formulation	O
that	O
allows	O
interpolations	O
along	O
non	O
-	O
linear	O
paths	O
:	O
DIG	O
i	O
(	O
x	O
)	O
=	O
x	O
i	O
x	O
k	O
i	O
=	O
x	O
i	O
∂F	O
x	O
k	O
∂x	O
i	O
dx	O
k	O
i	O
.	O
(	O
1	O
)	O
Here	O
,	O
x	O
k	O
i	O
refers	O
to	O
the	O
i	O
th	O
dimension	O
of	O
the	O
k	O
th	O
interpolated	O
point	O
between	O
input	O
x	O
and	O
baseline	O
x	O
and	O
F	O
is	O
a	O
neural	O
network	O
.	O
The	O
only	O
constraint	O
on	O
x	O
k	O
i	O
's	O
is	O
that	O
each	O
interpolation	O
should	O
be	O
monotonic	O
between	O
x	O
i	O
and	O
x	O
i	O
,	O
i.e.	O
,	O
∀j	O
,	O
k	O
{	O
1	O
,	O
...	O
,	O
m	O
}	O
;	O
j	O
<	O
k	O
,	O
x	O
i	O
≤	O
x	O
j	O
i	O
≤	O
x	O
k	O
i	O
≤	O
x	O
i	O
if	O
x	O
i	O
≤	O
x	O
i	O
,	O
x	O
i	O
≥	O
x	O
j	O
i	O
≥	O
x	O
k	O
i	O
≥	O
x	O
i	O
otherwise	O
.	O
(	O
2	O
)	O
Here	O
m	O
is	O
the	O
total	O
number	O
of	O
steps	O
for	O
interpolation	O
.	O
This	O
constraint	O
is	O
essential	O
because	O
it	O
allows	O
approximating	O
the	O
integral	O
in	O
Eq	O
.	O
1	O
using	O
Riemann	O
summation	O
2	O
which	O
requires	O
monotonic	O
paths	O
.	O
We	O
note	O
that	O
the	O
interpolation	O
points	O
used	O
by	O
IG	O
naturally	O
satisfy	O
this	O
constraint	O
since	O
they	O
lie	O
along	O
a	O
straight	O
line	O
joining	O
x	O
and	O
x	O
.	O
The	O
key	O
distinction	O
of	O
our	O
formulation	O
from	O
IG	O
is	O
that	O
DIG	O
is	O
agnostic	O
of	O
any	O
fixed	O
step	B-HyperparameterName
size	I-HyperparameterName
parameter	O
α	B-HyperparameterName
and	O
thus	O
allows	O
non	O
-	O
linear	O
interpolation	O
paths	O
in	O
the	O
embedding	O
space	O
.	O
The	O
integral	O
approximation	O
of	O
DIG	O
is	O
defined	O
as	O
follows	O
:	O
DIG	O
approx	O
i	O
(	O
x	O
)	O
=	O
Σ	O
m	O
k=1	O
∂F	O
x	O
k	O
∂x	O
i	O
×	O
x	O
k+1	O
i	O
−	O
x	O
k	O
i	O
,	O
(	O
3	O
)	O
where	O
m	O
is	O
the	O
total	O
number	O
of	O
steps	O
considered	O
for	O
the	O
approximation	O
.	O

In	O
this	O
step	O
,	O
given	O
an	O
anchor	O
word	O
embedding	O
a	O
,	O
we	O
modify	O
the	O
non	O
-	O
monotonic	O
dimensions	O
of	O
a	O
such	O
that	O
they	O
become	O
monotonic	O
w.r.t	O
.	O
w	O
and	O
w	O
.	O
The	O
monotonic	O
dimensions	O
of	O
a	O
vector	O
a	O
is	O
given	O
by	O
:	O
M	O
a	O
=	O
{	O
j	O
|	O
w	O
j	O
≤	O
a	O
j	O
≤	O
w	O
j	O
,	O
j	O
{	O
1	O
,	O
...	O
,	O
D	O
}	O
}	O
∪	O
{	O
j	O
|	O
w	O
j	O
≥	O
a	O
j	O
≥	O
w	O
j	O
,	O
j	O
{	O
1	O
,	O
...	O
,	O
D	O
}	O
}	O
,	O
where	O
D	O
is	O
the	O
word	B-HyperparameterName
embedding	I-HyperparameterName
dimension	I-HyperparameterName
.	O
The	O
number	O
of	O
monotonic	O
dimensions	O
is	O
given	O
by	O
the	O
size	O
of	O
the	O
set	O
defined	O
as	O
|	O
M	O
a	O
|	O
.	O
Thus	O
,	O
the	O
non	O
-	O
monotonic	O
dimensions	O
M	O
a	O
is	O
the	O
set	O
complement	O
of	O
the	O
monotonic	O
dimensions	O
,	O
i.e.	O
,	O
M	O
a	O
=	O
{	O
1	O
,	O
...	O
,	O
D	O
}	O
−	O
M	O
a	O
,	O
where	O
the	O
subtraction	O
is	O
the	O
setdiff	O
operation	O
.	O
Let	O
the	O
final	O
monotonic	O
vector	O
be	O
c.	O
We	O
define	O
the	O
MONOTONIZE	O
operations	O
as	O
follows	O
:	O
c	O
[	O
M	O
a	O
]	O
a	O
[	O
M	O
a	O
]	O
,	O
c	O
[	O
M	O
a	O
]	O
w	O
[	O
M	O
a	O
]	O
−	O
1	O
m	O
×	O
(	O
w	O
[	O
M	O
a	O
]	O
−	O
w	O
[	O
M	O
a	O
]	O
)	O
,	O
where	O
m	O
is	O
the	O
total	O
number	O
of	O
interpolation	O
points	O
we	O
want	O
to	O
select	O
in	O
the	O
path	O
.	O
It	O
can	O
be	O
easily	O
seen	O
that	O
c	O
is	O
monotonic	O
w.r.t	O
.	O
w	O
and	O
w	O
according	O
to	O
the	O
definition	O
in	O
Equation	O
2	O
.	O
ANCHORSEARCH	O
:	O
First	O
,	O
we	O
preprocess	O
the	O
word	O
embedding	O
in	O
V	O
to	O
find	O
the	O
top	O
-	O
K	O
nearest	O
neighbor	O
for	O
each	O
word	O
.	O
We	O
consider	O
this	O
neighborhood	O
for	O
candidate	O
anchor	O
selection	O
.	O
Let	O
us	O
denote	O
the	O
Kneighbors	O
for	O
a	O
word	O
w	O
by	O
KN	O
N	O
V	O
(	O
w	O
)	O
.	O
We	O
define	O
two	O
heuristics	O
to	O
search	O
for	O
the	O
next	O
anchor	O
word	O
:	O
GREEDY	O
and	O
MAXCOUNT	O
.	O
In	O
the	O
GREEDY	O
heuristic	O
,	O
we	O
first	O
compute	O
the	O
monotonic	O
embedding	O
corresponding	O
to	O
each	O
word	O
in	O
the	O
neighborhood	O
KN	O
N	O
V	O
(	O
w	O
)	O
using	O
the	O
MONO	O
-	O
TONIZE	O
step	O
.	O
Then	O
,	O
we	O
select	O
the	O
anchor	O
word	O
a	O
that	O
is	O
closest	O
to	O
its	O
corresponding	O
monotonic	O
embedding	O
obtained	O
from	O
the	O
above	O
step	O
.	O
This	O
can	O
be	O
thought	O
of	O
as	O
minimizing	O
the	O
WAE	O
metric	O
for	O
a	O
single	O
interpolated	O
word	O
.	O
The	O
key	O
intuition	O
here	O
is	O
to	O
locally	O
optimize	O
for	O
smallest	O
perturbations	O
at	O
each	O
iterative	O
selection	O
step	O
.	O
This	O
heuristic	O
is	O
depicted	O
in	O
Figure	O
2a	O
and	O
the	O
algorithm	O
is	O
presented	O
in	O
Algorithm	O
1	O
in	O
Appendix	O
.	O
In	O
the	O
MAXCOUNT	O
heuristic	O
,	O
we	O
select	O
the	O
anchor	O
a	O
as	O
the	O
word	O
in	O
KN	O
N	O
V	O
(	O
w	O
)	O
with	O
the	O
highest	O
number	O
of	O
monotonic	O
dimensions	O
.	O
Precisely	O
,	O
the	O
anchor	O
is	O
given	O
by	O
:	O
a	O
=	O
arg	O
max	O
a	O
KN	O
N	O
V	O
(	O
w	O
)	O
|	O
M	O
a	O
|	O
.	O
The	O
intuition	O
of	O
this	O
heuristic	O
is	O
that	O
the	O
vector	O
with	O
highest	O
number	O
of	O
monotonic	O
dimensions	O
would	O
require	O
the	O
minimum	O
number	O
of	O
dimensions	O
being	O
perturbed	O
in	O
the	O
MONOTONIZE	O
step	O
and	O
hence	O
,	O
would	O
be	O
close	O
to	O
a	O
word	O
in	O
the	O
vocabulary	O
.	O
This	O
heuristic	O
is	O
depicted	O
in	O
Figure	O
2b	O
and	O
the	O
algorithm	O
is	O
presented	O
in	O
Algorithm	O
2	O
in	O
Appendix	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
datasets	O
and	O
models	O
used	O
for	O
evaluating	O
our	O
proposed	O
algorithm	O
.	O
(	O
Wolf	O
et	O
al	O
,	O
2020b	O
)	O
.	O
Method	O
DistilBERT	B-MethodName
RoBERTa	B-MethodName
BERT	B-MethodName
LO	O
↓	O
Comp	O
↑	O
Suff	O
↓	O
WAE	O
↓	O
LO	O
↓	O
Comp	O
↑	O
Suff	O
↓	O
WAE	O
↓	O
LO	O
↓	O
Comp	O
↑	O
Suff	O
↓	O
WAE	O
↓	O
Grad	O
*	O
Method	O
DistilBERT	B-MethodName
RoBERTa	B-MethodName
BERT	B-MethodName
LO	O
↓	O
Comp	O
↑	O
Suff	O
↓	O
WAE	O
↓	O
LO	O
↓	O
Comp	O
↑	O
Suff	O
↓	O
WAE	O
↓	O
LO	O
↓	O
Comp	O
↑	O
Suff	O
↓	O
WAE	O
↓	O
Grad	O
*	O
Language	O
Models	O
.	O
We	O
use	O
pre	O
-	O
trained	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
DistilBERT	B-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
text	B-TaskName
classification	I-TaskName
models	O
individually	O
fine	O
-	O
tuned	O
for	O
SST2	B-DatasetName
,	O
IMDB	B-DatasetName
,	O
and	O
RT	O
datasets	O
.	O
4	O
The	O
fine	O
-	O
tuned	O
checkpoints	O
used	O
are	O
provided	O
by	O
the	O
HuggingFace	O
library	O
(	O
Wolf	O
et	O
al	O
,	O
2020a	O
)	O
.	O
Evaluation	O
Metrics	O
.	O
Following	O
prior	O
literature	O
,	O
we	O
use	O
the	O
following	O
three	O
automated	O
metrics	O
:	O
Log	O
-	O
odds	O
(	O
LO	O
)	O
score	O
(	O
Shrikumar	O
et	O
al	O
,	O
2017	O
)	O
is	O
defined	O
as	O
the	O
average	O
difference	O
of	O
the	O
negative	O
logarithmic	O
probabilities	O
on	O
the	O
predicted	O
class	O
before	O
and	O
after	O
masking	O
the	O
top	O
k%	O
words	O
with	O
zero	O
padding	O
.	O
Lower	O
scores	O
are	O
better	O
.	O
Comprehensiveness	O
(	O
Comp	O
)	O
score	O
(	O
DeYoung	O
et	O
al	O
,	O
2020	O
)	O
is	O
the	O
average	O
difference	O
of	O
the	O
change	O
in	O
predicted	O
class	O
probability	O
before	O
and	O
after	O
removing	O
the	O
top	O
k%	O
words	O
.	O
Similar	O
to	O
Log	O
-	O
odds	O
,	O
this	O
measures	O
the	O
influence	O
of	O
the	O
top	O
-	O
attributed	O
words	O
on	O
the	O
model	O
's	O
prediction	O
.	O
Higher	O
scores	O
are	O
better	O
.	O
Sufficiency	O
(	O
Suff	O
)	O
score	O
(	O
DeYoung	O
et	O
al	O
,	O
2020	O
)	O
is	O
defined	O
as	O
the	O
average	O
difference	O
of	O
the	O
change	O
in	O
predicted	O
class	O
probability	O
before	O
and	O
after	O
keeping	O
only	O
the	O
top	O
k%	O
words	O
.	O
This	O
measures	O
the	O
adequacy	O
of	O
the	O
top	O
k%	O
attributions	O
for	O
model	O
's	O
prediction	O
.	O
Please	O
refer	O
to	O
Appendix	O
C	O
for	O
more	O
details	O
about	O
the	O
evaluation	O
metrics	O
.	O
We	O
use	O
k	B-HyperparameterName
=	I-HyperparameterName
20	O
%	O
in	O
our	O
experiments	O
.	O
In	O
Appendix	O
D	O
we	O
further	O
analyze	O
the	O
effect	O
of	O
changing	O
top	O
-	O
k%	O
on	O
the	O
metrics	O
.	O
Additionally	O
,	O
we	O
use	O
our	O
proposed	O
word	O
-	O
approximation	O
error	O
(	O
WAE	O
)	O
metric	O
to	O
compare	O
DIG	O
with	O
IG	O
.	O
Method	O
DistilBERT	B-MethodName
RoBERTa	B-MethodName
BERT	B-MethodName
LO	O
↓	O
Comp	O
↑	O
Suff	O
↓	O
WAE	O
↓	O
LO	O
↓	O
Comp	O
↑	O
Suff	O
↓	O
WAE	O
↓	O
LO	O
↓	O
Comp	O
↑	O
Suff	O
↓	O
WAE	O
↓	O
Grad	O
*	O

We	O
compare	O
DIG	O
with	O
four	O
representative	O
gradientbased	O
explanation	O
methods	O
-	O
Gradient*Input	O
(	O
Grad*Inp	O
)	O
(	O
Shrikumar	O
et	O
al	O
,	O
2016	O
)	O
,	O
DeepLIFT	O
(	O
Shrikumar	O
et	O
al	O
,	O
2017	O
)	O
,	O
GradShap	O
(	O
Lundberg	O
and	O
Lee	O
,	O
2017	O
)	O
,	O
and	O
integrated	O
gradients	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
.	O
For	O
the	O
IMDB	B-DatasetName
and	O
RT	O
datasets	O
,	O
we	O
randomly	O
sample	O
a	O
subset	O
of	O
2	O
,	O
000	O
reviews	O
from	O
the	O
public	O
test	O
sets	O
to	O
compare	O
the	O
different	O
methods	O
,	O
due	O
to	O
computation	O
costs	O
.	O
For	O
the	O
SST2	B-DatasetName
dataset	O
,	O
we	O
use	O
the	O
complete	O
set	O
of	O
1	O
,	O
821	O
test	O
sentences	O
.	O
The	O
results	O
are	O
shown	O
in	O
Tables	O
1	O
,	O
2	O
,	O
and	O
3	O
for	O
SST2	B-DatasetName
,	O
IMDB	B-DatasetName
,	O
and	O
Rotten	O
Tomatoes	O
respectively	O
.	O
Comparison	O
with	O
baselines	O
.	O
First	O
,	O
we	O
observe	O
that	O
across	O
the	O
nine	O
different	O
settings	O
we	O
studied	O
(	O
three	O
language	O
models	O
per	O
dataset	O
)	O
,	O
DIG	O
consistently	O
outperforms	O
the	O
baselines	O
on	O
eight	O
of	O
the	O
settings	O
.	O
This	O
is	O
valid	O
for	O
all	O
the	O
metrics	O
.	O
We	O
also	O
note	O
that	O
the	O
WAE	O
metric	O
is	O
lower	O
for	O
all	O
variants	O
of	O
DIG	O
compared	O
to	O
IG	O
.	O
This	O
validates	O
that	O
our	O
proposed	O
interpolation	O
strategies	O
for	O
DIG	O
is	O
able	O
to	O
considerably	O
reduce	O
the	O
word	O
-	O
approximation	O
error	O
in	O
the	O
interpolated	O
paths	O
and	O
consistently	O
improving	O
performance	O
on	O
all	O
three	O
explanation	O
evaluation	O
metrics	O
considered	O
.	O
Comparison	O
between	O
variants	O
of	O
DIG	O
.	O
Second	O
,	O
we	O
observe	O
that	O
on	O
average	O
,	O
DIG	O
-	O
GREEDY	O
performs	O
better	O
than	O
DIG	O
-	O
MAXCOUNT	O
.	O
Specifically	O
,	O
we	O
find	O
that	O
DIG	O
-	O
MAXCOUNT	O
does	O
n't	O
outperform	O
DIG	O
-	O
GREEDY	O
by	O
significantly	O
large	O
margins	O
on	O
any	O
setting	O
(	O
while	O
the	O
opposite	O
is	O
true	O
for	O
one	O
setting	O
-	O
RoBERTa	B-MethodName
fine	O
-	O
tuned	O
on	O
IMDB	B-DatasetName
dataset	O
)	O
.	O
This	O
could	O
be	O
because	O
the	O
DIG	O
-	O
GREEDY	O
strategy	O
ensures	O
that	O
the	O
monotonic	O
point	O
c	O
is	O
always	O
close	O
to	O
the	O
anchor	O
a	O
due	O
to	O
the	O
locally	O
greedy	O
selection	O
at	O
each	O
step	O
which	O
is	O
not	O
explicitly	O
guaranteed	O
by	O
DIG	O
-	O
MAXCOUNT	O
.	O
But	O
overall	O
,	O
we	O
do	O
not	O
find	O
any	O
Analysis	O
.	O
Finally	O
,	O
though	O
we	O
are	O
able	O
to	O
achieve	O
good	O
reductions	O
in	O
WAE	O
,	O
we	O
note	O
that	O
the	O
WAE	O
for	O
our	O
interpolation	O
algorithms	O
are	O
not	O
close	O
to	O
zero	O
yet	O
.	O
This	O
leaves	O
some	O
scope	O
to	O
design	O
better	O
interpolation	O
algorithms	O
in	O
future	O
.	O
Moreover	O
,	O
we	O
find	O
that	O
the	O
average	O
Pearson	B-MetricName
correlation	I-MetricName
between	O
log	O
-	O
odds	O
and	O
WAE	O
is	O
0.32	O
and	O
the	O
correlation	O
is	O
0.45	O
if	O
we	O
consider	O
the	O
eight	O
settings	O
where	O
we	O
outperform	O
IG	O
.	O
We	O
discuss	O
the	O
correlations	O
of	O
all	O
the	O
settings	O
in	O
Appendix	O
E.	O
While	O
this	O
suggests	O
a	O
weak	O
correlation	O
between	O
the	O
two	O
metrics	O
,	O
it	O
is	O
hard	O
to	O
comment	O
if	O
there	O
is	O
a	O
causality	O
between	O
the	O
two	O
.	O
This	O
is	O
partially	O
because	O
we	O
believe	O
selection	O
of	O
interpolation	O
points	O
should	O
also	O
take	O
the	O
semantics	O
of	O
the	O
perturbed	O
sentences	O
into	O
consideration	O
,	O
which	O
we	O
do	O
n't	O
strongly	O
enforce	O
in	O
our	O
strategies	O
.	O
Hence	O
,	O
we	O
think	O
that	O
constraining	O
interpolations	O
in	O
a	O
semantically	O
meaningful	O
way	O
is	O
a	O
promising	O
direction	O
to	O
explore	O
.	O

Integrated	O
gradients	O
(	O
IG	O
)	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
for	O
an	O
input	O
x	O
along	O
the	O
i	O
th	O
dimension	O
is	O
defined	O
as	O
follows	O
:	O
IG	O
i	O
(	O
x	O
)	O
=	O
(	O
x	O
i	O
−	O
x	O
i	O
)	O
×	O
1	O
α=0	O
∂F	O
(	O
x	O
+	O
α×	O
(	O
x−x	O
)	O
)	O
∂x	O
i	O
dα	O
.	O
(	O
5	O
)	O
Here	O
,	O
F	O
is	O
the	O
neural	O
network	O
,	O
x	O
is	O
a	O
baseline	O
embedding	O
,	O
and	O
α	B-HyperparameterName
is	O
the	O
step	B-HyperparameterName
size	I-HyperparameterName
.	O
Simply	O
put	O
,	O
integrated	O
gradients	O
algorithm	O
works	O
by	O
sampling	O
points	O
at	O
a	O
uniform	O
spacing	O
along	O
a	O
straight	O
-	O
line	O
between	O
the	O
input	O
and	O
the	O
baseline	O
,	O
and	O
summing	O
the	O
model	O
's	O
gradient	O
at	O
the	O
inputs	O
for	O
each	O
interpolated	O
points	O
.	O
To	O
compute	O
this	O
integral	O
efficiently	O
,	O
the	O
authors	O
propose	O
a	O
Riemann	O
summation	O
approximation	O
defined	O
below	O
:	O
IG	O
approx	O
i	O
(	O
x	O
)	O
=	O
(	O
xi	O
−	O
x	O
i	O
)	O
×	O
Σ	O
m	O
k=1	O
∂F	O
(	O
x	O
+	O
k	O
m	O
×	O
(	O
x−x	O
)	O
)	O
)	O
∂x	O
i	O
×	O
1	O
m	O
,	O
(	O
6	O
)	O
where	O
m	O
is	O
the	O
total	O
number	O
of	O
steps	O
considered	O
for	O
the	O
approximation	O
.	O
Next	O
,	O
we	O
briefly	O
describe	O
how	O
IG	O
is	O
used	O
to	O
explain	O
a	O
model	O
's	O
prediction	O
which	O
takes	O
a	O
sentence	O
as	O
input	O
(	O
for	O
example	O
,	O
the	O
model	O
can	O
be	O
a	O
text	B-TaskName
classification	I-TaskName
network	O
)	O
.	O
Let	O
S	O
=	O
[	O
w	O
0	B-DatasetName
..	O
w	O
n	O
]	O
be	O
a	O
sentence	O
of	O
length	O
n	O
and	O
w	O
i	O
be	O
the	O
i	O
th	O
word	O
embedding	O
of	O
the	O
sentence	O
.	O
Also	O
,	O
let	O
F	O
be	O
a	O
text	O
-	O
classification	O
model	O
,	O
i.e.	O
,	O
y	O
=	O
F	O
(	O
S	O
)	O
.	O
Then	O
,	O
IG	O
calculates	O
the	O
attribution	O
for	O
each	O
dimension	O
of	O
a	O
word	O
embedding	O
w	O
i	O
.	O
The	O
interpolation	O
points	O
required	O
for	O
Equation	O
6	O
are	O
generated	O
by	O
linearly	O
interpolating	O
the	O
word	O
embedding	O
between	O
w	O
i	O
and	O
a	O
baseline	O
word	O
embedding	O
(	O
usually	O
chosen	O
as	O
the	O
pad	O
embedding	O
)	O
.	O
Then	O
,	O
using	O
Eq	O
.	O
6	O
,	O
the	O
attribution	O
for	O
the	O
i	O
th	O
dimension	O
of	O
w	O
is	O
calculated	O
.	O
The	O
final	O
word	O
attribution	O
is	O
the	O
sum	O
of	O
the	O
attributions	O
for	O
each	O
dimension	O
of	O
the	O
word	O
embedding	O
.	O

It	O
is	O
easy	O
to	O
see	O
that	O
the	O
approximation	O
of	O
integrated	O
gradients	O
is	O
a	O
special	O
case	O
of	O
DIG	O
.	O
Note	O
that	O
the	O
k	O
th	O
linear	O
interpolation	O
of	O
the	O
i	O
th	O
dimension	O
of	O
input	O
x	O
for	O
IG	O
can	O
be	O
represented	O
as	O
:	O
x	O
k	O
i	O
=	O
x	O
i	O
+	O
k	O
m	O
×	O
(	O
x	O
i	O
−	O
x	O
i	O
)	O
.	O
(	O
7	O
)	O
Substituting	O
Eq	O
.	O
7	O
in	O
Eq	O
.	O
3	O
gives	O
us	O
Eq	O
.	O
6	O
.	O
Sundararajan	O
et	O
al	O
(	O
2017	O
)	O
define	O
path	O
methods	O
as	O
the	O
general	O
form	O
of	O
integrated	O
gradients	O
that	O
are	O
applicable	O
for	O
all	O
monotonic	O
paths	O
between	O
the	O
input	O
and	O
the	O
baseline	O
.	O
Our	O
DIG	O
approach	O
is	O
a	O
reformulation	O
of	O
the	O
path	O
method	O
where	O
the	O
paths	O
are	O
not	O
necessarily	O
parameterized	O
by	O
α	B-HyperparameterName
,	O
making	O
it	O
more	O
applicable	O
for	O
discrete	O
data	O
domain	O
.	O
Hence	O
,	O
DIG	O
also	O
satisfies	O
all	O
the	O
theoretical	O
properties	O
applicable	O
for	O
path	O
methods	O
-	O
Implementation	O
Invariance	O
,	O
Sensitivity	O
,	O
Linearity	O
,	O
and	O
Completeness	O
.	O
We	O
refer	O
the	O
readers	O
to	O
Proposition	O
2	O
in	O
Sundararajan	O
et	O
al	O
(	O
2017	O
)	O
for	O
more	O
technical	O
details	O
.	O

We	O
compute	O
the	O
Pearson	B-MetricName
correlation	I-MetricName
between	O
logodds	O
and	O
WAE	O
for	O
each	O
dataset	O
+	O
LM	O
pair	O
.	O
For	O
this	O
,	O
we	O
consider	O
the	O
metric	O
values	O
for	O
IG	O
,	O
DIG	O
-	O
GREEDY	O
,	O
and	O
DIG	O
-	O
MAXCOUNT	O
and	O
report	O
the	O
correlations	O
for	O
each	O
setting	O
in	O
Table	O
6	O
.	O
We	O
observe	O
that	O
,	O
there	O
is	O
a	O
strong	O
correlation	O
on	O
average	O
for	O
DistilBERT	B-MethodName
.	O
For	O
BERT	B-MethodName
and	O
RoBERTa	B-MethodName
we	O
find	O
a	O
weak	O
positive	O
and	O
negative	O
correlation	O
respectively	O
.	O

Typically	O
,	O
performance	O
evaluations	O
of	O
automated	O
event	O
coding	O
engines	O
are	O
carried	O
out	O
with	O
respect	O
to	O
benchmarks	O
made	O
of	O
annotated	O
linguistic	O
units	O
(	O
e.g.	O
clause	O
,	O
sentence	O
or	O
document	O
)	O
.	O
While	O
this	O
is	O
crucial	O
in	O
order	O
to	O
factorize	O
the	O
individual	O
,	O
linguistic	O
subtasks	O
composing	O
the	O
event	B-TaskName
extraction	I-TaskName
process	O
,	O
it	O
does	O
not	O
estimate	O
the	O
overall	O
usability	O
of	O
machinecoded	O
event	O
data	O
sets	O
for	O
micro	O
-	O
level	O
modelling	O
of	O
social	O
processes	O
,	O
particularly	O
in	O
the	O
domain	O
of	O
socio	O
-	O
political	O
and	O
armed	O
conflict	O
,	O
where	O
spatial	O
analysis	O
has	O
become	O
standard	O
.	O
The	O
complex	O
dynamics	O
of	O
the	O
Black	O
Lives	O
Matter	O
movement	O
and	O
its	O
varied	O
media	O
coverage	O
by	O
news	O
outlets	O
and	O
social	O
media	O
make	O
it	O
a	O
particularly	O
relevant	O
use	O
case	O
for	O
assessing	O
the	O
capability	O
of	O
automated	O
,	O
Event	B-TaskName
Extraction	I-TaskName
systems	O
to	O
model	O
socio	O
-	O
political	O
processes	O
.	O
The	O
Task	O
3	O
:	O
"	O
Discovering	O
Black	O
Lives	O
Matter	O
Events	O
"	O
1	O
organized	O
in	O
the	O
context	O
of	O
the	O
Challenges	O
and	O
Applications	O
of	O
Automated	O
Extraction	O
of	O
Socio	O
-	O
political	O
Events	O
from	O
Text	O
(	O
CASE	O
)	O
2021	O
workshop	O
aims	O
at	O
doing	O
so	O
by	O
challenging	O
Event	B-TaskName
Extraction	I-TaskName
(	O
EE	O
)	O
engines	O
to	O
extract	O
a	O
collection	O
of	O
protest	O
events	O
from	O
two	O
heterogeneous	O
text	O
collections	O
(	O
i.e.	O
,	O
news	O
and	O
social	O
media	O
)	O
and	O
then	O
measuring	O
a	O
number	O
of	O
spatiotemporal	O
correlation	O
coefficients	O
against	O
a	O
curated	O
Gold	O
Standard	O
data	O
set	O
of	O
protest	O
incidents	O
from	O
the	O
BLM	O
movement	O
.	O
During	O
May	O
and	O
June	O
of	O
2020	O
,	O
protests	O
occurred	O
across	O
the	O
globe	O
in	O
response	O
to	O
the	O
murder	O
of	O
George	O
Floyd	O
,	O
an	O
unarmed	O
Black	O
man	O
,	O
by	O
Derek	O
Chauvin	O
,	O
a	O
white	O
police	O
officer	O
.	O
In	O
the	O
U.S.	O
,	O
the	O
number	O
of	O
locations	O
holding	O
demonstrations	O
related	O
to	O
this	O
murder	O
outnumbered	O
any	O
other	O
demonstration	O
in	O
U.S.	O
history	O
(	O
Putnam	O
et	O
al	O
,	O
2020	O
)	O
.	O
These	O
events	O
were	O
more	O
often	O
than	O
not	O
associated	O
with	O
the	O
Black	O
Lives	O
Matter	O
(	O
BLM	O
)	O
movement	O
,	O
either	O
(	O
1	O
)	O
directly	O
through	O
organizing	O
or	O
(	O
2	O
)	O
indirectly	O
through	O
the	O
slogan	O
"	O
Black	O
Lives	O
Matter	O
"	O
or	O
shared	O
political	O
agendas	O
such	O
as	O
police	O
abolition	O
and	O
protests	O
against	O
police	O
violence	O
towards	O
Black	O
communi	O
-	O
ties	O
.	O
Since	O
its	O
inception	O
in	O
2013	O
,	O
the	O
Black	O
Lives	O
Matter	O
movement	O
,	O
a	O
loose	O
network	O
of	O
affiliated	O
organizations	O
,	O
has	O
organized	O
demonstrations	O
around	O
a	O
large	O
number	O
of	O
police	O
shootings	O
and	O
killings	O
and	O
sought	O
to	O
raise	O
awareness	O
of	O
systematic	O
violence	O
against	O
Black	O
communities	O
.	O
While	O
support	O
for	O
Black	O
Lives	O
Matter	O
has	O
varied	O
over	O
its	O
lifetime	O
(	O
Horowitz	O
,	O
2020	O
)	O
,	O
the	O
work	O
done	O
over	O
the	O
past	O
years	O
laid	O
the	O
foundation	O
for	O
the	O
global	O
response	O
seen	O
in	O
the	O
wake	O
of	O
George	O
Floyd	O
's	O
murder	O
.	O
This	O
task	O
is	O
the	O
third	O
in	O
a	O
series	O
of	O
tasks	O
at	O
CASE	O
2021	O
workshop	O
(	O
Hürriyetoglu	O
et	O
al	O
,	O
2021b	O
)	O
.	O
The	O
first	O
task	O
is	O
concerned	O
with	O
protest	O
news	O
detection	O
at	O
multiple	O
text	O
resolutions	O
(	O
e.g.	O
,	O
the	O
document	O
and	O
sentence	O
level	O
)	O
and	O
in	O
multiple	O
languages	O
:	O
English	O
,	O
Hindi	O
,	O
Portuguese	O
,	O
and	O
Spanish	O
(	O
Hürriyetoglu	O
et	O
al	O
,	O
2021a	O
)	O
.	O
Teams	O
which	O
participated	O
in	O
Task	O
1	O
were	O
invited	O
to	O
participate	O
in	O
this	O
third	O
task	O
:	O
"	O
Discovering	O
Black	O
Lives	O
Matter	O
Events	O
in	O
the	O
United	O
States	O
"	O
.	O
This	O
task	O
is	O
an	O
evaluation	O
only	O
task	O
,	O
where	O
all	O
models	O
are	O
(	O
1	O
)	O
trained	O
on	O
the	O
data	O
supplied	O
in	O
Task	O
1	O
,	O
(	O
2	O
)	O
applied	O
to	O
the	O
news	O
and	O
social	O
media	O
data	O
(	O
i.e	O
,	O
New	O
York	O
Times	O
and	O
Twitter	O
data	O
)	O
,	O
and	O
(	O
3	O
)	O
evaluated	O
on	O
a	O
manually	O
curated	O
,	O
Gold	O
Standard	O
BLM	O
protest	O
event	O
list	O
.	O
Each	O
team	O
's	O
system	O
is	O
compared	O
to	O
simple	O
baselines	O
in	O
order	O
to	O
properly	O
evaluate	O
their	O
accuracy	B-MetricName
.	O

Summary	O
measures	O
such	O
as	O
precision	O
,	O
recall	O
,	O
and	O
F1	B-MetricName
are	O
limited	O
in	O
their	O
capacity	O
to	O
inform	O
about	O
the	O
quality	O
of	O
the	O
predictions	O
of	O
an	O
automated	O
system	O
(	O
Derczynski	B-DatasetName
,	O
2016	O
;	O
Yacouby	O
and	O
Axman	O
,	O
2020	O
)	O
.	O
Moreover	O
,	O
evaluating	O
capabilities	O
of	O
a	O
system	O
on	O
detecting	O
socio	O
-	O
political	O
events	O
from	O
text	O
requires	O
additional	O
metrics	O
such	O
as	O
spatio	O
-	O
temporal	O
correlation	O
of	O
the	O
system	O
output	O
and	O
the	O
actual	O
distribution	O
of	O
the	O
events	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
;	O
Althaus	O
et	O
al	O
,	O
2021	O
)	O
.	O
Several	O
studies	O
focused	O
on	O
assessing	O
the	O
correlation	O
of	O
machine	O
-	O
coded	O
event	O
data	O
sets	O
with	O
Gold	O
Standards	O
based	O
on	O
disaggregated	O
event	O
counts	O
,	O
for	O
example	O
Ward	O
et	O
al	O
(	O
2013	O
)	O
andSchrodt	O
andAnalytics	O
(	O
2015	O
)	O
.	O
Hammond	O
and	O
Weidmann	O
(	O
2014	O
)	O
applied	O
disaggregation	O
of	O
events	O
incidents	O
across	O
PRIO	O
-	O
GRID	O
geographical	O
cells	O
(	O
Tollefsen	O
et	O
al	O
,	O
2012	O
)	O
to	O
assess	O
the	O
Global	O
Database	O
of	O
Events	O
,	O
Language	O
and	O
Tone	O
(	O
GDELT	O
)	O
data	O
approximation	O
of	O
the	O
spatio	O
-	O
temporal	O
pattern	O
of	O
conflicts	O
.	O
Zavarella	O
et	O
al	O
(	O
2020	O
)	O
adapted	O
this	O
method	O
to	O
administrative	O
units	O
for	O
measuring	O
the	O
impact	O
of	O
event	O
de	O
-	O
duplication	O
on	O
increasing	O
correlation	O
with	O
the	O
Armed	O
Conflict	O
Location	O
and	O
Event	O
Data	O
(	O
ACLED	O
)	O
data	O
sets	O
for	O
a	O
number	O
of	O
conflicts	O
in	O
Africa	O
.	O
In	O
this	O
report	O
we	O
report	O
on	O
an	O
evaluation	O
task	O
,	O
which	O
we	O
refer	O
as	O
Task	O
3	O
,	O
we	O
provide	O
a	O
detailed	O
analysis	O
of	O
the	O
capabilities	O
of	O
the	O
best	O
performing	O
systems	O
on	O
Task	O
1	O
(	O
Hürriyetoglu	O
et	O
al	O
,	O
2021a	O
)	O
in	O
this	O
respect	O
.	O
We	O
believe	O
this	O
effort	O
will	O
shed	O
light	O
on	O
system	O
performances	O
beyond	O
precision	O
,	O
recall	O
,	O
and	O
F1	B-MetricName
.	O

The	O
first	O
analysis	O
only	O
considers	O
the	O
total	O
number	O
of	O
"	O
activated	O
"	O
cells	O
(	O
i.e.	O
,	O
for	O
Figure	O
1	O
:	O
The	O
geo	O
-	O
referenced	O
BLM	O
protest	O
event	O
records	O
from	O
Gold	O
Standard	O
(	O
small	O
yellow	O
dots	O
)	O
overlaid	O
with	O
the	O
PRIO	O
-	O
GRID	O
cells	O
over	O
the	O
US	O
.	O
The	O
larger	O
red	O
and	O
blue	O
dots	O
represent	O
events	O
recognized	O
by	O
the	O
Baseline	O
system	O
from	O
NYT	O
and	O
Twitter	O
,	O
respectively	O
.	O
which	O
at	O
least	O
one	O
Protest	O
event	O
was	O
recorded	O
)	O
,	O
in	O
the	O
system	O
output	O
and	O
Gold	O
Standard	O
data	O
set	O
.	O
This	O
time	B-TaskName
series	I-TaskName
analysis	I-TaskName
is	O
sufficient	O
to	O
estimate	O
how	O
well	O
the	O
automatic	O
systems	O
capture	O
the	O
time	O
trends	O
of	O
the	O
protest	O
movement	O
.	O
However	O
,	O
it	O
does	O
not	O
compute	O
accuracy	B-MetricName
of	O
system	O
data	O
in	O
estimating	O
the	O
spatial	O
variation	O
of	O
the	O
target	O
process	O
.	O

Table	O
1	O
shows	O
the	O
Pearson	O
r	O
,	O
Spearman	B-MetricName
correlation	I-MetricName
coefficient	O
ρ	O
,	O
and	O
Root	O
Mean	O
Squared	O
Error	B-MetricName
(	O
RMSE	B-MetricName
)	O
for	O
the	O
total	O
daily	O
protest	O
cell	O
counts	O
of	O
the	O
Baseline	O
and	O
participant	O
systems	O
,	O
over	O
the	O
35	O
days	O
target	O
time	O
range	O
.	O
When	O
a	O
run	O
for	O
both	O
source	O
types	O
exists	O
for	O
a	O
system	O
,	O
we	O
also	O
evaluate	O
the	O
union	O
of	O
the	O
two	O
event	O
sets	O
(	O
noted	O
as	O
"	O
Merged	O
"	O
in	O
Tables	O
)	O
.	O
Here	O
,	O
the	O
correlations	O
are	O
between	O
the	O
total	O
number	O
of	O
cells	O
per	O
day	O
where	O
the	O
system	O
found	O
an	O
event	O
vs.	O
the	O
number	O
of	O
cells	O
where	O
event	O
happened	O
according	O
to	O
the	O
Gold	O
Standard	O
(	O
i.e.	O
,	O
temporal	O
patterns	O
and	O
not	O
spatial	O
patterns	O
)	O
.	O
These	O
correlation	O
measures	O
are	O
tolerant	O
to	O
errors	O
in	O
geocoding	O
(	O
as	O
far	O
as	O
the	O
events	O
are	O
located	O
in	O
U.S.	O
)	O
and	O
evaluate	O
the	O
capability	O
of	O
the	O
system	O
to	O
detect	O
protest	O
events	O
in	O
the	O
news	O
and	O
social	O
media	O
,	O
independent	O
of	O
their	O
location	O
.	O
We	O
see	O
the	O
following	O
:	O
(	O
1	O
)	O
NoConflict	O
surpasses	O
the	O
Baseline	O
with	O
the	O
NYT	O
,	O
Twitter	O
,	O
and	O
Merged	O
data	O
in	O
both	O
Pearson	O
r	O
and	O
Spearman	O
ρ	O
,	O
and	O
(	O
2	O
)	O
EventMiner	O
and	O
HandShakes	O
surpasses	O
Baseline	O
with	O
Twitter	O
data	O
in	O
Pearson	O
r	O
(	O
both	O
systems	O
have	O
lower	O
Spearman	O
ρ	O
than	O
Baseline	O
)	O
.	O
Additionally	O
,	O
NoConflict	O
surpasses	O
the	O
NexusDdpl	O
system	O
(	O
using	O
NYT	O
,	O
Twitter	O
,	O
and	O
Merged	O
data	O
)	O
,	O
and	O
the	O
HandShakes	O
system	O
surpasses	O
the	O
NexusDdpl	O
system	O
using	O
Twitter	O
data	O
.	O
Table	O
2	O
reports	O
Pearson	O
r	O
,	O
Spearman	B-MetricName
correlation	I-MetricName
coefficient	O
ρ	O
,	O
and	O
Root	O
Mean	O
Squared	O
Error	B-MetricName
(	O
RMSE	B-MetricName
)	O
over	O
cell	O
-	O
day	O
event	O
counts	O
of	O
the	O
Baseline	O
and	O
participant	O
systems	O
with	O
respect	O
to	O
Gold	O
Standard	O
,	O
for	O
the	O
35	O
days	O
time	O
range	O
.	O
Here	O
the	O
variables	O
range	O
over	O
the	O
whole	O
set	O
of	O
PRIO	O
-	O
GRID	O
cells	O
included	O
in	O
the	O
US	O
territory	O
and	O
,	O
thus	O
,	O
shows	O
the	O
correlation	O
of	O
event	O
numbers	O
across	O
geo	O
-	O
cells	O
,	O
thus	O
evaluating	O
the	O
system	O
's	O
geolocation	O
capabilities	O
.	O
NoConflict	O
(	O
NYT	O
)	O
had	O
the	O
highest	O
Pearson	O
r	O
and	O
lowest	O
RMSE	B-MetricName
across	O
all	O
systems	O
,	O
as	O
well	O
as	O
the	O
highest	O
Spearman	O
ρ	O
(	O
with	O
the	O
Merged	O
data	O
)	O
.	O
Using	O
Twitter	O
data	O
alone	O
,	O
the	O
Baseline	O
and	O
NexusDdpl	O
systems	O
outperformed	O
all	O
others	O
in	O
terms	O
of	O
Pearson	O
r	O
,	O
however	O
NexusDdpl	O
had	O
a	O
higher	O
Spearman	O
ρ	O
.	O
However	O
,	O
when	O
looking	O
at	O
both	O
correlation	O
metrics	O
simultaneously	O
,	O
no	O
system	O
is	O
above	O
the	O
NexusDdpl	O
baseline	O
.	O
In	O
Figure	O
2	O
we	O
plot	O
the	O
time	B-TaskName
series	I-TaskName
of	O
total	O
daily	O
protest	O
cells	O
for	O
the	O
best	O
performing	O
instance	O
of	O
each	O
system	O
on	O
New	O
York	O
Times	O
(	O
left	O
)	O
and	O
Twitter	O
(	O
right	O
)	O
data	O
,	O
respectively	O
.	O
We	O
see	O
the	O
systems	O
evaluated	O
on	O
the	O
NYT	O
data	O
failing	O
to	O
pick	O
up	O
both	O
variation	O
in	O
the	O
temporal	O
patterns	O
(	O
i.e.	O
,	O
a	O
large	O
number	O
of	O
protests	O
early	O
in	O
late	O
May	O
and	O
early	O
June	O
,	O
which	O
gradually	O
declines	O
with	O
weekly	O
spikes	O
)	O
and	O
the	O
magnitude	O
of	O
the	O
events	O
(	O
i.e	O
,	O
most	O
systems	O
pick	O
up	O
less	O
than	O
100	O
events	O
per	O
day	O
)	O
.	O
Systems	O
evaluated	O
on	O
Twitter	O
data	O
pick	O
up	O
more	O
events	O
in	O
late	O
May	O
and	O
early	O
June	O
,	O
but	O
still	O
fail	O
to	O
pick	O
up	O
the	O
magnitude	O
of	O
the	O
events	O
.	O
A	O
more	O
lenient	O
representation	O
of	O
the	O
agreement	O
with	O
Gold	O
Standard	O
is	O
shown	O
in	O
Table	O
3	O
.	O
Here	O
we	O
report	O
the	O
confusion	O
matrix	O
between	O
grid	O
cells	O
that	O
Gold	O
Standard	O
and	O
system	O
runs	O
code	O
as	O
experiencing	O
at	O
least	O
a	O
protest	O
event	O
.	O
It	O
can	O
be	O
observed	O
that	O
only	O
few	O
of	O
the	O
cells	O
classified	O
as	O
Protest	O
by	O
Gold	O
Standard	O
are	O
detected	O
by	O
the	O
automatic	O
systems	O
,	O
which	O
on	O
the	O
other	O
hand	O
incorrectly	O
classified	O
as	O
Protest	O
several	O
additional	O
cells	O
.	O

The	O
goal	O
of	O
the	O
"	O
Discovering	O
Black	O
Lives	O
Matter	O
Events	O
"	O
Shared	O
Task	O
was	O
to	O
explore	O
novel	O
performance	O
evaluations	O
of	O
pretrained	O
event	B-TaskName
detection	I-TaskName
systems	O
.	O
These	O
systems	O
were	O
applied	O
to	O
large	O
noisy	O
,	O
multi	O
-	O
modal	O
text	O
data	O
sets	O
(	O
i.e.	O
,	O
news	O
articles	O
and	O
social	O
media	O
data	O
)	O
related	O
to	O
a	O
specific	O
protest	O
movement	O
,	O
namely	O
,	O
Black	O
Lives	O
Matter	O
.	O
Thus	O
,	O
the	O
systems	O
are	O
being	O
evaluated	O
out	O
-	O
of	O
-	O
domain	O
in	O
terms	O
of	O
both	O
data	O
type	O
(	O
i.e.	O
,	O
the	O
systems	O
are	O
trained	O
on	O
news	O
data	O
and	O
evaluated	O
on	O
both	O
news	O
and	O
social	O
media	O
)	O
and	O
protest	O
movement	O
context	O
(	O
i.e.	O
,	O
the	O
training	O
data	O
are	O
not	O
necessarily	O
related	O
to	O
BLM	O
)	O
.	O
Systems	O
are	O
evaluated	O
in	O
their	O
ability	O
to	O
identify	O
both	O
events	O
across	O
time	O
as	O
well	O
as	O
events	O
their	O
distribution	O
across	O
space	O
.	O
This	O
evaluation	O
scenario	O
proved	O
difficult	O
for	O
all	O
systems	O
participating	O
in	O
the	O
shared	O
task	O
.	O
A	O
major	O
problem	O
,	O
as	O
shown	O
on	O
(	O
Hettiarachchi	O
et	O
al	O
,	O
2021	O
)	O
and	O
NoConflict	O
(	O
Hu	O
and	O
Stoehr	O
,	O
2021	O
)	O
,	O
with	O
precisions	O
of	O
56.0	O
and	O
73.6	O
,	O
respectively	O
.	O
The	O
low	O
recall	O
at	O
this	O
years	O
shared	O
task	O
may	O
well	O
be	O
due	O
to	O
the	O
low	O
coverage	O
of	O
protest	O
events	O
of	O
the	O
highly	O
diffused	O
BLM	O
movement	O
both	O
in	O
the	O
NYT	O
and	O
Twitter	O
corpus	O
,	O
so	O
the	O
upper	O
bound	O
of	O
the	O
recall	O
may	O
turn	O
out	O
not	O
to	O
be	O
much	O
higher	O
than	O
the	O
system	O
performance	O
.	O
One	O
possible	O
explanation	O
for	O
this	O
is	O
that	O
a	O
significant	O
part	O
of	O
the	O
BLM	O
events	O
in	O
the	O
Gold	O
standard	O
are	O
located	O
in	O
small	O
towns	O
,	O
for	O
which	O
NYT	O
has	O
a	O
limited	O
coverage	O
and	O
also	O
they	O
were	O
not	O
in	O
the	O
focus	O
of	O
social	O
media	O
,	O
due	O
to	O
their	O
small	O
scale	O
.	O
NexusDdpl	O
turned	O
out	O
to	O
be	O
quite	O
high	O
both	O
in	O
terms	O
of	O
event	B-TaskName
detection	I-TaskName
accuracy	B-MetricName
,	O
as	O
well	O
as	O
geo	O
-	O
coding	O
correlation	O
.	O
While	O
no	O
single	O
system	O
outperformed	O
all	O
others	O
in	O
tracking	O
both	O
temporal	O
and	O
spatial	O
trends	O
,	O
NoConflict	O
had	O
a	O
clear	O
advantage	O
(	O
i.e.	O
,	O
the	O
highest	O
scoring	O
system	O
in	O
2	O
out	O
of	O
3	O
metrics	O
)	O
in	O
terms	O
of	O
tracking	O
daily	O
events	O
.	O
3	O
:	O
Confusion	O
matrix	O
of	O
grid	O
cells	O
experiencing	O
at	O
least	O
one	O
Protest	O
event	O
(	O
true	O
)	O
versus	O
inactive	O
cells	O
(	O
false	O
)	O
,	O
for	O
the	O
Gold	O
Standard	O
,	O
Baseline	O
and	O
participant	O
systems	O
.	O
Unless	O
denoted	O
by	O
a	O
superscript	O
,	O
all	O
systems	O
use	O
the	O
"	O
merged	O
"	O
version	O
(	O
i.e.	O
,	O
both	O
NYT	O
and	O
Twitter	O
data	O
sets	O
)	O
except	O
for	O
HandShakes	O
system	O
which	O
uses	O
only	O
Twitter	O
data	O
.	O

Based	O
on	O
the	O
SCM	O
,	O
this	O
section	O
describes	O
how	O
to	O
resolve	O
the	O
trigger	O
curse	O
via	O
causal	O
intervention	O
.	O
Context	O
Intervention	O
.	O
To	O
block	O
the	O
backdoor	O
path	O
,	O
we	O
intervene	O
on	O
the	O
context	O
C	O
and	O
the	O
new	O
context	O
-	O
intervened	O
SCM	O
is	O
shown	O
in	O
Figure	O
1	O
(	O
b	O
)	O
.	O
Given	O
support	O
set	O
s	O
,	O
event	O
set	O
e	O
of	O
s	O
,	O
context	O
set	O
C	O
of	O
s	O
and	O
query	O
instance	O
q	O
,	O
we	O
optimize	O
the	O
interventional	O
distribution	O
P	O
(	O
Y	O
|	O
do	O
(	O
C	O
=	O
C	O
)	O
,	O
E	O
=	O
e	O
,	O
Q	O
=	O
q	O
)	O
rather	O
than	O
P	O
(	O
Y	O
|	O
S	O
=	O
s	O
,	O
Q	O
=	O
q	O
)	O
,	O
where	O
do	O
(	O
)	O
denotes	O
causal	O
intervention	O
operation	O
.	O
By	O
interven	O
-	O
ing	O
,	O
the	O
learning	O
objective	O
of	O
models	O
changes	O
from	O
optimizing	O
correlation	O
to	O
optimizing	O
causality	O
.	O
Backdoor	O
Adjustment	O
.	O
Backdoor	O
adjustment	O
is	O
used	O
to	O
estimate	O
the	O
interventional	O
distribution	O
4	O
:	O
P	O
(	O
Y	O
|	O
do	O
(	O
C	O
=	O
C	O
)	O
,	O
E	O
=	O
e	O
,	O
Q	O
=	O
q	O
)	O
=	O
t	O
T	O
s	O
S	O
P	O
(	O
Y	O
|	O
s	O
,	O
q	O
)	O
P	O
(	O
s	O
|	O
C	O
,	O
t	O
)	O
P	O
(	O
t	O
|	O
e	O
)	O
,	O
(	O
1	O
)	O
where	O
P	O
(	O
s	O
|	O
C	O
,	O
t	O
)	O
denotes	O
the	O
generation	O
of	O
s	O
from	O
the	O
trigger	O
and	O
contexts	O
.	O
P	O
(	O
s	O
|	O
C	O
,	O
t	O
)	O
=	O
1/	O
|	O
C	O
|	O
if	O
and	O
only	O
if	O
the	O
context	O
of	O
s	O
in	O
C	O
and	O
the	O
trigger	O
of	O
s	O
is	O
t.	O
P	O
(	O
Y	O
|	O
s	O
,	O
q	O
)	O
∝	O
φ	O
(	O
s	O
,	O
q	O
;	O
θ	B-HyperparameterName
)	O
is	O
the	O
matching	O
model	O
between	O
q	O
and	O
s	O
parametrized	O
by	O
θ	B-HyperparameterName
.	O
Estimating	O
P	O
(	O
t	O
|	O
e	O
)	O
via	O
Contextualized	O
Prediction	O
.	O
The	O
confounder	O
distribution	O
P	O
(	O
t	O
|	O
e	O
)	O
is	O
unknown	O
because	O
E	O
is	O
a	O
hidden	O
variable	O
.	O
Since	O
the	O
event	O
argument	O
information	O
is	O
contained	O
in	O
C	O
,	O
we	O
argue	O
that	O
P	O
(	O
t	O
|	O
e	O
)	O
∝	O
M	O
(	O
t	O
|	O
C	O
)	O
where	O
M	O
(	O
|	O
C	O
)	O
indicates	O
a	O
masked	O
token	O
prediction	O
task	O
(	O
Taylor	O
,	O
1953	O
)	O
which	O
is	O
constructed	O
by	O
masking	O
triggers	O
in	O
the	O
support	O
set	O
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
masked	O
language	O
model	O
to	O
calculate	O
P	O
(	O
t	O
|	O
e	O
)	O
by	O
first	O
generating	O
a	O
set	O
of	O
candidate	O
triggers	O
through	O
the	O
context	O
:	O
T	O
c	O
=	O
{	O
t	O
i	O
|	O
i	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
}	O
∪	O
{	O
t	O
0	B-DatasetName
}	O
,	O
where	O
t	O
i	O
is	O
the	O
i	O
-	O
th	O
predicted	O
token	O
and	O
t	O
0	B-DatasetName
is	O
the	O
original	O
trigger	O
of	O
the	O
support	O
set	O
instance	O
,	O
then	O
P	O
(	O
t	O
|	O
e	O
)	O
is	O
estimated	O
by	O
averaging	O
logit	O
obtained	O
from	O
the	O
MLM	B-DatasetName
:	O
P	O
(	O
ti	O
|	O
e	O
)	O
=	O
	O
	O
	O
	O
λ	O
i	O
=	O
0	B-DatasetName
(	O
1	O
−	O
λ	O
)	O
exp	O
(	O
li	O
)	O
j	O
exp	O
(	O
lj	O
)	O
i	O
=	O
0	B-DatasetName
(	O
2	O
)	O
where	O
l	O
i	O
is	O
the	O
logit	O
for	O
the	O
i	O
th	O
token	O
.	O
To	O
reduce	O
the	O
noise	O
introduced	O
by	O
MLM	B-DatasetName
,	O
we	O
assign	O
an	O
additional	O
hyperparameter	O
λ	O
(	O
0	B-DatasetName
,	O
1	O
)	O
to	O
t	O
0	B-DatasetName
.	O
Optimizing	O
via	O
Representation	B-TaskName
Learning	I-TaskName
.	O
Given	O
the	O
interventional	O
distribution	O
,	O
FSED	O
model	O
can	O
be	O
learned	O
by	O
minimizing	O
the	O
loss	B-MetricName
function	O
on	O
it	O
:	O
L	O
(	O
θ	B-HyperparameterName
)	O
=	O
−	O
q	O
Q	O
f	O
(	O
P	O
(	O
Y	O
|	O
do	O
(	O
C	O
)	O
,	O
e	O
,	O
q	O
;	O
θ	B-HyperparameterName
)	O
)	O
=	O
−	O
q	O
Q	O
f	O
(	O
t	O
T	O
s	O
S	O
P	O
(	O
Y	O
|	O
s	O
,	O
q	O
;	O
θ	B-HyperparameterName
)	O
P	O
(	O
s	O
|	O
C	O
,	O
t	O
)	O
P	O
(	O
t	O
|	O
e	O
)	O
)	O
(	O
3	O
)	O
where	O
Q	O
is	O
training	O
queries	O
and	O
f	O
is	O
a	O
strict	O
monotonically	O
increasing	O
function	O
.	O
However	O
,	O
the	O
optimization	O
of	O
L	O
(	O
θ	B-HyperparameterName
)	O
needs	O
to	O
calculate	O
every	O
P	O
(	O
Y	O
|	O
s	O
,	O
q	O
;	O
θ	B-HyperparameterName
)	O
,	O
which	O
is	O
quite	O
time	O
-	O
consuming	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
a	O
surrogate	O
learning	O
criteria	O
L	O
SG	O
(	O
θ	B-HyperparameterName
)	O
to	O
optimize	O
the	O
causal	O
relation	O
based	O
on	O
representation	B-TaskName
learning	I-TaskName
:	O
4	O
The	O
proof	O
is	O
shown	O
in	O
Appendix	O
LSG	O
(	O
θ	B-HyperparameterName
)	O
=	O
−	O
q	O
Q	O
g	O
(	O
R	O
(	O
q	O
;	O
θ	B-HyperparameterName
)	O
,	O
t	O
T	O
s	O
S	O
P	O
(	O
s	O
|	O
C	O
,	O
t	O
)	O
P	O
(	O
t	O
|	O
e	O
)	O
R	O
(	O
s	O
;	O
θ	B-HyperparameterName
)	O
)	O
Here	O
R	O
is	O
a	O
representation	O
model	O
which	O
inputs	O
s	O
or	O
q	O
and	O
outputs	O
a	O
dense	O
representation	O
.	O
g	O
(	O
,	O
)	O
is	O
a	O
distance	B-HyperparameterName
metric	I-HyperparameterName
measuring	O
the	O
similarity	O
between	O
two	O
representations	O
.	O
Such	O
loss	B-MetricName
function	O
is	O
widely	O
used	O
in	O
many	O
metric	O
-	O
based	O
methods	O
(	O
e.g.	O
,	O
Prototypical	O
Networks	O
and	O
Relation	O
Networks	O
)	O
.	O
In	O
the	O
Appendix	O
,	O
we	O
prove	O
L	O
SG	O
(	O
θ	B-HyperparameterName
)	O
is	O
equivalent	O
to	O
L	O
(	O
θ	B-HyperparameterName
)	O
.	O
2	O
)	O
FS	O
-	O
ClusterLoss	O
(	O
Lai	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
add	O
two	O
auxiliary	O
loss	B-MetricName
functions	O
when	O
training	O
.	O
Furthermore	O
,	O
we	O
compare	O
our	O
method	O
with	O
models	O
finetuned	O
with	O
support	O
set	O
(	O
Finetune	O
)	O
and	O
pretrained	O
using	O
the	O
training	O
set	O
(	O
Pretrain	O
)	O
.	O
BERT	B-MethodName
base	O
(	O
uncased	O
)	O
is	O
used	O
as	O
the	O
encoder	O
for	O
all	O
models	O
and	O
MLM	B-DatasetName
for	O
trigger	O
collection	O
.	O

The	O
performance	O
of	O
our	O
method	O
and	O
all	O
baselines	O
is	O
shown	O
in	O
Table	O
1	O
.	O
We	O
can	O
see	O
that	O
:	O
1	O
)	O
By	O
intervening	O
on	O
the	O
context	O
in	O
SCM	O
and	O
using	O
backdoor	O
adjustment	O
during	O
training	O
,	O
our	O
method	O
can	O
effectively	O
learn	O
FSED	O
models	O
.	O
Compared	O
with	O
the	O
original	O
metric	O
-	O
based	O
models	O
,	O
our	O
method	O
achieves	O
8.7	O
%	O
and	O
1.6	O
%	O
micro	B-MetricName
-	I-MetricName
F1	I-MetricName
(	O
average	O
)	O
improvement	O
in	O
prototypical	O
network	O
and	O
relation	O
network	O
respectively	O
.	O
2	O
)	O
The	O
causal	O
theory	O
is	O
a	O
promising	O
technique	O
for	O
resolving	O
the	O
trigger	O
cruse	O
problem	O
.	O
Notice	O
that	O
FS	O
-	O
LexFree	O
can	O
not	O
achieve	O
the	O
competitive	O
performance	O
with	O
the	O
original	O
FS	O
models	O
,	O
which	O
indicates	O
that	O
trigger	O
information	O
is	O
import	O
and	O
underfitting	O
triggers	O
will	O
hurt	O
the	O
detection	O
performance	O
.	O
This	O
verifies	O
that	O
trigger	O
curse	O
is	O
very	O
challenging	O
and	O
causal	O
intervention	O
can	O
effectively	O
resolve	O
it	O
.	O
3	O
)	O
Our	O
method	O
can	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
FSED	O
performance	O
.	O
Compared	O
with	O
best	O
score	O
in	O
baselines	O
,	O
our	O
method	O
gains	O
7.5	O
%	O
,	O
1.0	O
%	O
,	O
and	O
2.0	O
%	O
micro	B-MetricName
-	I-MetricName
F1	I-MetricName
improvements	O
on	O
ACE05	O
,	O
MAVEN	B-DatasetName
and	O
KBP17	O
datasets	O
respectively	O
.	O

One	O
-	O
way	O
K	O
-	O
Shot	O
Settings	O
.	O
We	O
adopt	O
One	O
-	O
way	O
K	O
-	O
shot	O
setting	O
in	O
our	O
experiments	O
,	O
in	O
which	O
the	O
support	O
set	O
in	O
an	O
episode	O
contains	O
one	O
event	O
type	O
(	O
called	O
concerned	O
event	O
)	O
and	O
the	O
query	O
can	O
contain	O
any	O
event	O
type	O
.	O
The	O
model	O
aims	O
to	O
detect	O
triggers	O
of	O
the	O
concerned	O
event	O
in	O
query	O
and	O
all	O
types	O
will	O
be	O
evaluated	O
by	O
traversing	O
each	O
event	O
type	O
.	O
The	O
support	O
set	O
and	O
query	O
in	O
an	O
episode	O
can	O
be	O
formulated	O
as	O
follows	O
:	O
S	O
=	O
{	O
(	O
S	O
1	O
,	O
E	O
,	O
Y	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
S	O
K	O
,	O
E	O
,	O
Y	O
K	O
)	O
}	O
where	O
S	O
is	O
the	O
support	O
set	O
,	O
E	O
is	O
the	O
concerned	O
event	O
,	O
S	O
i	O
=	O
{	O
s	O
i	O
1	O
,	O
s	O
i	O
2	O
,	O
.	O
.	O
.	O
,	O
s	O
i	O
n	O
i	O
}	O
is	O
the	O
i	O
-	O
th	O
sentence	O
in	O
support	O
,	O
s	O
i	O
j	O
is	O
the	O
j	O
-	O
th	O
token	O
in	O
S	O
i	O
,	O
Y	O
i	O
=	O
{	O
y	O
i	O
1	O
,	O
y	O
i	O
2	O
,	O
.	O
.	O
.	O
,	O
y	O
i	O
n	O
}	O
is	O
the	O
labels	O
of	O
tokens	O
in	O
S	O
i	O
and	O
y	O
i	O
j	O
=	O
1	O
only	O
if	O
t	O
i	O
is	O
the	O
trigger	O
(	O
or	O
part	O
of	O
trigger	O
)	O
of	O
concerned	O
event	O
,	O
otherwise	O
y	O
i	O
j	O
=	O
0	B-DatasetName
.	O
Q	O
=	O
{	O
Q	O
1	O
,	O
Q	O
2	O
,	O
.	O
.	O
.	O
,	O
Q	O
M	O
}	O
where	O
Q	O
is	O
the	O
set	O
of	O
query	O
and	O
Q	O
i	O
=	O
{	O
q	O
i	O
1	O
,	O
q	O
i	O
2	O
,	O
.	O
.	O
.	O
,	O
q	O
i	O
m	O
i	O
}	O
is	O
the	O
i	O
-	O
th	O
query	O
sentence	O
and	O
q	O
i	O
j	O
is	O
the	O
j	O
-	O
th	O
token	O
in	O
Q	O
i	O
The	O
model	O
is	O
expected	O
to	O
output	O
the	O
concerned	O
event	O
in	O
Q	O
:	O
O	O
Q	O
=	O
{	O
(	O
Q	O
1	O
,	O
E	O
,	O
T	O
1	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
Q	O
1	O
,	O
E	O
,	O
T	O
1	O
n	O
1	O
)	O
,	O
(	O
Q	O
2	O
,	O
E	O
,	O
T	O
2	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
Q	O
2	O
,	O
E	O
,	O
T	O
2	O
n	O
2	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
Q	O
M	O
,	O
E	O
,	O
T	O
M	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
Q	O
M	O
,	O
E	O
,	O
T	O
M	O
n	O
M	O
)	O
}	O
where	O
O	O
Q	O
is	O
the	O
set	O
of	O
triggers	O
of	O
concerned	O
event	O
detected	O
in	O
Q	O
,	O
T	O
i	O
k	O
is	O
the	O
k	O
-	O
th	O
trigger	O
of	O
concerned	O
event	O
in	O
sentence	O
Q	O
i	O
and	O
n	O
i	O
≥	O
0	B-DatasetName
means	O
the	O
number	O
of	O
triggers	O
of	O
concerned	O
event	O
in	O
Q	O
i	O
.	O
Evaluation	O
We	O
improve	O
the	O
traditional	O
episode	O
evaluation	O
setting	O
by	O
evaluating	O
the	O
full	O
test	O
set	O
.	O
For	O
each	O
event	O
type	O
in	O
test	O
set	O
,	O
we	O
randomly	O
sample	O
K	O
instances	O
as	O
support	O
set	O
and	O
all	O
other	O
instances	O
are	O
used	O
as	O
query	O
.	O
Following	O
previous	O
event	B-TaskName
detection	I-TaskName
works	O
(	O
Chen	O
et	O
al	O
,	O
2015	O
)	O
,	O
the	O
predicted	O
trigger	O
is	O
correct	O
if	O
its	O
event	O
type	O
and	O
offsets	O
match	O
those	O
of	O
a	O
gold	O
trigger	O
.	O
We	O
evaluate	O
all	O
methods	O
using	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
and	O
micro	B-MetricName
-	I-MetricName
F1	I-MetricName
scores	O
,	O
and	O
micro	B-MetricName
-	I-MetricName
F1	I-MetricName
is	O
taken	O
as	O
the	O
primary	O
measure	O
.	O

We	O
use	O
two	O
metric	O
-	O
base	O
methods	O
in	O
our	O
experiments	O
:	O
Prototypical	O
network	O
(	O
Snell	O
et	O
al	O
,	O
2017	O
)	O
and	O
Relation	O
network	O
(	O
Sung	O
et	O
al	O
,	O
2018	O
)	O
,	O
which	O
contain	O
an	O
encoder	O
component	O
and	O
a	O
classifier	O
component	O
.	O
Encoder	O
We	O
use	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
to	O
encoder	O
the	O
support	O
set	O
and	O
the	O
query	O
.	O
Given	O
a	O
sentece	O
X	O
=	O
{	O
x	O
1	O
,	O
x	O
2	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
}	O
,	O
BERT	B-MethodName
encodes	O
the	O
sequence	O
and	O
output	O
the	O
represent	O
of	O
each	O
token	O
in	O
X	O
:	O
R	O
=	O
{	O
r	O
1	O
,	O
r	O
2	O
,	O
.	O
.	O
.	O
,	O
r	O
n	O
}	O
.	O
After	O
obtaining	O
the	O
feature	O
representation	O
of	O
the	O
support	O
set	O
,	O
we	O
calculate	O
the	O
prototype	O
of	O
the	O
categories	O
(	O
concerned	O
event	O
and	O
other	O
)	O
:	O
p	O
i	O
=	O
1	O
|	O
R	O
i	O
|	O
r	O
R	O
i	O
r	O
,	O
i	O
=	O
0	B-DatasetName
,	O
1	O
where	O
p	O
i	O
is	O
the	O
prototype	O
of	O
category	O
i	O
,	O
R	O
i	O
is	O
the	O
set	O
of	O
feature	O
representation	O
of	O
tokens	O
that	O
labeled	O
with	O
y	O
=	O
i	O
in	O
support	O
set	O
.	O
Classifier	O
The	O
models	O
classify	O
each	O
token	O
in	O
query	O
based	O
on	O
its	O
similarity	O
to	O
the	O
prototype	O
.	O
We	O
first	O
calculate	O
the	O
similarity	O
between	O
prototype	O
and	O
token	O
in	O
query	O
.	O
s	O
i	O
,	O
j	O
,	O
k	B-HyperparameterName
=	I-HyperparameterName
g	O
(	O
p	O
k	O
,	O
q	O
i	O
j	O
)	O
,	O
k	B-HyperparameterName
=	I-HyperparameterName
0	B-DatasetName
,	O
1	O
(	O
6	O
)	O
where	O
g	O
(	O
x	O
,	O
y	O
)	O
measures	O
the	O
similarity	O
between	O
x	O
and	O
y	O
,	O
q	O
i	O
j	O
is	O
the	O
represent	O
of	O
j	O
-	O
th	O
token	O
in	O
i	O
-	O
th	O
query	O
sentence	O
.	O
Then	O
we	O
calculate	O
the	O
probability	O
distribution	O
of	O
token	O
q	O
i	O
j	O
:	O
P	O
(	O
Y	O
|	O
q	O
i	O
j	O
,	O
S	O
)	O
=	O
Softmax	B-MethodName
(	O
s	O
i	O
,	O
j	O
,	O
0	B-DatasetName
,	O
s	O
i	O
,	O
j	O
,	O
1	O
)	O
(	O
7	O
)	O
During	O
training	O
,	O
we	O
use	O
the	O
Cross	O
-	O
Entropy	O
loss	B-MetricName
on	O
each	O
token	O
of	O
query	O
.	O
And	O
the	O
support	O
set	O
and	O
the	O
query	O
are	O
randomly	O
sampled	O
from	O
the	O
training	O
set	O
.	O
When	O
evaluating	O
,	O
we	O
treat	O
the	O
labels	O
as	O
IO	O
tagging	O
schemes	O
,	O
and	O
adjacent	O
I	O
are	O
considered	O
to	O
be	O
the	O
same	O
trigger	O
so	O
that	O
we	O
can	O
handle	O
a	O
trigger	O
with	O
multiple	O
tokens	O
.	O
Similarity	O
Functions	O
For	O
prototypical	O
network	O
,	O
the	O
similarity	O
in	O
Equation	O
6	O
is	O
Euclidean	O
distance	O
.	O
For	O
relation	O
network	O
,	O
we	O
calculate	O
similarity	O
using	O
neural	O
networks	O
.	O
Unlike	O
the	O
original	O
paper	O
,	O
we	O
find	O
the	O
following	O
calculation	O
to	O
be	O
more	O
efficient	O
:	O
g	O
(	O
p	O
k	O
,	O
q	O
j	O
i	O
)	O
=	O
F	O
(	O
p	O
k	O
q	O
j	O
i	O
|	O
p	O
k	O
−	O
q	O
j	O
i	O
|	O
)	O
where	O
means	O
concatenation	O
vectors	O
and	O
F	O
is	O
two	O
-	O
layer	O
feed	O
-	O
forward	O
neural	O
networks	O
with	O
a	O
ReLU	B-MethodName
function	O
on	O
the	O
first	O
layer	O
.	O

Different	O
reviews	O
in	O
(	O
Arevalo	O
et	O
al	O
,	O
2017	O
;	O
Poria	O
et	O
al	O
,	O
2016Poria	O
et	O
al	O
,	O
,	O
2017bGhosal	O
et	O
al	O
,	O
2018	O
;	O
Morency	O
et	O
al	O
,	O
2011a	O
;	O
Zadeh	O
et	O
al	O
,	O
2018a	O
;	O
Mihalcea	O
,	O
2012	O
;	O
Lee	O
et	O
al	O
,	O
2018	O
;	O
Tsai	O
et	O
al	O
,	O
2018	O
)	O
suggest	O
that	O
multi	O
-	O
modal	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
are	O
relatively	O
new	O
areas	O
as	O
compared	O
to	O
uni	O
-	O
modal	O
analysis	O
.	O
Feature	B-MethodName
selection	I-MethodName
(	O
fusion	O
)	O
is	O
a	O
challenging	O
and	O
important	O
task	O
for	O
any	O
multi	O
-	O
modal	O
analysis	O
.	O
Poria	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
a	O
multi	O
-	O
kernel	O
learning	O
based	O
feature	B-MethodName
selection	I-MethodName
method	O
for	O
multimodal	O
sentiment	O
and	O
emotion	B-TaskName
recognition	I-TaskName
.	O
A	O
convolutional	O
deep	B-MethodName
belief	I-MethodName
network	I-MethodName
(	O
CDBN	O
)	O
is	O
proposed	O
in	O
(	O
Ranganathan	O
et	O
al	O
,	O
2016	O
)	O
to	O
learn	O
salient	O
multi	O
-	O
modal	O
features	O
of	O
low	O
-	O
intensity	O
expressions	O
of	O
emotions	O
,	O
whereas	O
Lee	O
et	O
al	O
(	O
2018	O
)	O
introduced	O
a	O
convolutional	O
attention	O
network	O
to	O
learn	O
multimodal	O
feature	O
representation	O
between	O
speech	O
and	O
text	O
data	O
for	O
multi	O
-	O
modal	O
emotion	B-TaskName
recognition	I-TaskName
.	O
A	O
feature	O
level	O
fusion	O
vector	O
was	O
built	O
,	O
and	O
then	O
a	O
Support	B-MethodName
Vector	I-MethodName
Machine	I-MethodName
(	O
SVM	B-MethodName
)	O
classifier	O
was	O
used	O
to	O
detect	O
the	O
emotional	O
duality	O
and	O
mixed	O
emotional	O
experience	O
in	O
(	O
Patwardhan	O
,	O
2017	O
)	O
.	O
Similar	O
work	O
on	O
feature	O
-	O
level	O
fusion	O
based	O
on	O
self	O
-	O
attention	O
mechanism	O
is	O
reported	O
in	O
(	O
Hazarika	O
et	O
al	O
,	O
2018	O
)	O
.	O
Fu	O
et	O
al	O
(	O
2017	O
)	O
introduced	O
an	O
enhanced	O
sparse	O
local	O
discriminative	O
canoni	O
-	O
cal	O
correlation	O
analysis	O
approach	O
(	O
En	O
-	O
SLDCCA	O
)	O
to	O
learn	O
the	O
multi	O
-	O
modal	O
shared	O
feature	O
representation	O
.	O
Tzirakis	O
et	O
al	O
(	O
2017	O
)	O
introduced	O
a	O
Long	O
Short	O
Term	O
Memory	O
(	O
LSTM	B-MethodName
)	O
based	O
end	O
-	O
to	O
-	O
end	O
multi	O
-	O
modal	O
emotion	B-TaskName
recognition	I-TaskName
system	O
in	O
which	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
and	O
a	O
deep	O
residual	B-MethodName
network	I-MethodName
are	O
used	O
to	O
capture	O
the	O
emotional	O
content	O
for	O
various	O
styles	O
of	O
speaking	O
,	O
robust	O
features	O
.	O
Poria	O
et	O
al	O
(	O
2017a	O
)	O
presented	O
a	O
literature	O
survey	O
on	O
various	O
affect	O
dimensions	O
e.g.	O
,	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
emotion	B-DatasetName
analysis	O
,	O
etc	O
.	O
,	O
for	O
the	O
multi	O
-	O
modal	O
analysis	O
.	O
A	O
multi	O
-	O
modal	O
fusion	O
-	O
based	O
approach	O
is	O
proposed	O
in	O
(	O
Blanchard	O
et	O
al	O
,	O
2018	O
)	O
for	O
sentiment	O
classification	O
.	O
The	O
author	O
used	O
exclusively	O
high	O
-	O
level	O
fusion	O
of	O
visual	O
and	O
acoustic	O
features	O
to	O
classify	O
the	O
sentiment	O
.	O
Zadeh	O
et	O
al	O
(	O
2016	O
)	O
presented	O
the	O
multi	O
-	O
modal	O
dictionary	O
-	O
based	O
technique	O
to	O
capture	O
the	O
interaction	O
between	O
spoken	O
words	O
and	O
facial	O
expression	O
better	O
when	O
expressing	O
the	O
sentiment	O
.	O
In	O
another	O
work	O
,	O
proposed	O
a	O
Tensor	O
Fusion	O
Network	O
(	O
TFN	O
)	O
to	O
capture	O
the	O
inter	O
-	O
modality	O
and	O
intra	O
-	O
modality	O
dynamics	O
between	O
the	O
multi	O
-	O
modalities	O
(	O
i.e.	O
,	O
text	O
,	O
visual	O
,	O
and	O
acoustic	O
)	O
.	O
These	O
works	O
did	O
not	O
take	O
contextual	O
information	O
into	O
account	O
.	O
Poria	O
et	O
al	O
(	O
2017b	O
)	O
introduced	O
an	O
Long	O
Short	O
Term	O
Memory	O
(	O
LSTM	B-MethodName
)	O
based	O
framework	O
for	O
sentiment	O
classification	O
which	O
uses	O
contextual	O
information	O
to	O
capture	O
interrelationships	O
between	O
the	O
utterances	O
.	O
In	O
another	O
work	O
,	O
Poria	O
et	O
al	O
(	O
2017c	O
)	O
proposed	O
a	O
user	O
opinion	O
based	O
framework	O
to	O
combine	O
all	O
the	O
multi	O
-	O
modal	O
inputs	O
(	O
i.e.	O
,	O
visual	O
,	O
acoustic	O
,	O
and	O
textual	O
)	O
by	O
applying	O
a	O
multi	O
-	O
kernel	O
learning	O
-	O
based	O
approach	O
.	O
Contextual	O
inter	O
-	O
modal	O
attention	O
mechanism	O
was	O
not	O
explored	O
in	O
much	O
details	O
until	O
recently	O
.	O
Zadeh	O
et	O
al	O
(	O
2018a	O
)	O
introduced	O
a	O
multi	O
-	O
attention	O
blocks	O
based	O
model	O
for	O
multi	O
-	O
modal	O
sentiment	O
classification	O
but	O
did	O
not	O
account	O
for	O
contextual	O
information	O
,	O
whereas	O
Ghosal	O
et	O
al	O
(	O
2018	O
)	O
proposed	O
a	O
contextual	O
inter	O
-	O
modal	O
attention	O
based	O
framework	O
for	O
multi	O
-	O
modal	O
sentiment	O
classification	O
.	O
Recently	O
,	O
Zadeh	O
et	O
al	O
(	O
2018c	O
)	O
introduced	O
the	O
largest	O
multimodal	O
dataset	O
namely	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
for	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
.	O
Author	O
effectively	O
fused	O
the	O
multi	O
-	O
modality	O
inputs	O
i.e.	O
,	O
text	O
,	O
visual	O
,	O
and	O
acoustic	O
through	O
a	O
dynamic	O
fusion	O
graph	O
and	O
reported	O
competitive	O
performance	O
w.r.t	O
.	O
various	O
state	O
-	O
ofthe	O
-	O
art	O
systems	O
for	O
both	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
.	O
Very	O
recently	O
,	O
Akhtar	O
et	O
al	O
(	O
2019	O
)	O
in	O
-	O
troduced	O
an	O
attention	O
based	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
framework	O
for	O
sentiment	O
and	O
emotion	B-TaskName
classification	I-TaskName
on	O
the	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
dataset	O
.	O
In	O
comparison	O
to	O
the	O
existing	O
systems	O
,	O
our	O
proposed	O
approach	O
aims	O
to	O
exploits	O
the	O
interaction	O
between	O
the	O
input	O
modalities	O
through	O
an	O
autoencoder	B-MethodName
based	O
inter	O
-	O
modal	O
interactive	O
module	O
.	O
The	O
interactive	O
module	O
learns	O
the	O
joint	O
representation	O
for	O
the	O
participating	O
modalities	O
,	O
which	O
are	O
further	O
utilized	O
to	O
capture	O
the	O
contributing	O
contextual	O
utterances	O
in	O
a	O
context	O
-	O
aware	O
attention	O
module	O
.	O
3	O
Context	O
-	O
aware	O
Interactive	O
Attention	O
(	O
CIA	O
)	O
Affect	O
Analysis	O
In	O
this	O
section	O
,	O
we	O
describe	O
our	O
proposed	O
approach	O
for	O
the	O
effective	O
fusion	O
of	O
multi	O
-	O
modal	O
input	O
sources	O
.	O
We	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
Contextaware	O
Interactive	O
Attention	O
(	O
CIA	O
)	O
based	O
recurrent	O
neural	O
network	O
for	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
.	O
As	O
discussed	O
earlier	O
,	O
one	O
of	O
the	O
main	O
challenges	O
for	O
multi	O
-	O
modal	O
information	O
analysis	O
is	O
to	O
exploit	O
the	O
interaction	O
among	O
the	O
input	O
modalities	O
.	O
Therefore	O
,	O
we	O
introduce	O
an	O
Inter	O
-	O
modal	O
Interactive	O
Module	O
(	O
IIM	O
)	O
that	O
aims	O
to	O
learn	O
the	O
interaction	O
between	O
any	O
two	O
modalities	O
through	O
an	O
auto	O
-	O
encoder	O
like	O
structure	O
.	O
For	O
the	O
text	O
-	O
acoustic	O
pair	O
of	O
modalities	O
,	O
we	O
aim	O
to	O
decode	O
the	O
acoustic	O
representation	O
through	O
the	O
encoded	O
textual	O
representation	O
.	O
After	O
training	O
of	O
IIM	O
,	O
we	O
extract	O
the	O
encoded	O
representation	O
for	O
further	O
processing	O
.	O
We	O
argue	O
that	O
the	O
encoded	O
representation	O
learns	O
the	O
interaction	O
between	O
the	O
text	O
and	O
acoustic	O
modalities	O
.	O
Similarly	O
,	O
we	O
compute	O
the	O
interaction	O
among	O
all	O
the	O
other	O
pairs	O
(	O
i.e.	O
,	O
acoustic	O
-	O
text	O
,	O
text	O
-	O
visual	O
,	O
visualtext	O
,	O
acoustic	O
-	O
visual	O
,	O
and	O
visual	O
-	O
acoustic	O
)	O
.	O
Next	O
,	O
we	O
extract	O
the	O
sequential	O
pattern	O
of	O
the	O
utterances	O
through	O
a	O
Bi	O
-	O
directional	O
Gated	B-MethodName
Recurrent	I-MethodName
Unit	I-MethodName
(	O
Bi	O
-	O
GRU	B-MethodName
)	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
)	O
)	O
.	O
For	O
each	O
pair	O
of	O
modalities	O
,	O
the	O
two	O
representations	O
denoting	O
the	O
interactions	O
between	O
them	O
are	O
combined	O
through	O
a	O
mean	O
operation	O
.	O
For	O
an	O
instance	O
,	O
we	O
compute	O
the	O
mean	O
of	O
the	O
text	O
-	O
acoustic	O
and	O
acoustic	O
-	O
text	O
representations	O
for	O
text	O
and	O
acoustic	O
modalities	O
.	O
The	O
mean	O
operation	O
ensures	O
that	O
the	O
network	O
utilizes	O
the	O
two	O
distinct	O
representations	O
by	O
keeping	O
the	O
minimal	O
dimension	O
.	O
In	O
our	O
network	O
,	O
we	O
,	O
additionally	O
,	O
learn	O
the	O
interaction	O
among	O
the	O
modalities	O
through	O
a	O
feedforward	B-MethodName
network	I-MethodName
.	O
At	O
first	O
,	O
all	O
the	O
three	O
modalities	O
are	O
passed	O
through	O
a	O
separate	O
Bi	O
-	O
GRU	B-MethodName
.	O
Then	O
,	O
pair	O
-	O
Algorithm	O
1	O
Inter	O
-	O
modal	O
Interactive	O
Module	O
for	O
Multi	O
-	O
modal	O
Sentiment	O
and	O
Emotion	B-TaskName
Recognition	I-TaskName
(	O
IIM	O
-	O
MMSE	O
)	O
procedure	O
IIM	O
-	O
MMSE	O
(	O
t	O
,	O
v	O
,	O
a	O
)	O
for	O
i	O
1	O
,	O
...	O
,	O
K	O
do	O
K	B-HyperparameterName
=	I-HyperparameterName
#	O
modalities	O
for	O
j	O
1	O
,	O
...	O
,	O
K	O
do	O
∀x	O
,	O
y	O
[	O
T	O
,	O
V	O
,	O
A	O
]	O
,	O
x	O
=	O
y	O
and	O
i	O
≤	O
j	O
C	O
x	O
i	O
y	O
j	O
IIM	O
(	O
x	O
i	O
,	O
y	O
j	O
)	O
C	O
x	O
i	O
y	O
j	O
biGRU	B-MethodName
(	O
C	O
x	O
i	O
y	O
j	O
)	O
C	O
x	O
i	O
biGRU	B-MethodName
(	O
x	O
i	O
)	O
for	O
i	O
,	O
j	O
1	O
,	O
...	O
,	O
K	O
do	O
∀x	O
,	O
y	O
[	O
T	O
,	O
V	O
,	O
A	O
]	O
,	O
and	O
x	O
=	O
y	O
M	O
x	O
i	O
,	O
y	O
j	O
M	O
ean	O
(	O
C	O
x	O
i	O
y	O
j	O
,	O
C	O
y	O
i	O
x	O
j	O
)	O
cat	O
x	O
i	O
,	O
y	O
j	O
Concatenate	O
(	O
C	O
x	O
i	O
,	O
C	O
y	O
j	O
)	O
BI	O
x	O
i	O
,	O
y	O
j	O
F	O
ullyConnected	O
(	O
cat	O
x	O
i	O
,	O
y	O
j	O
)	O
A	O
x	O
i	O
,	O
y	O
j	O
CAM	B-MethodName
(	O
M	O
x	O
i	O
,	O
y	O
j	O
,	O
BI	O
x	O
i	O
,	O
y	O
j	O
)	O
Rep	O
[	O
A	O
T	O
V	O
,	O
A	O
T	O
A	O
,	O
A	O
AV	O
]	O
polarity	O
Sent	O
(	O
Rep	O
)	O
/Emo	O
(	O
Rep	O
)	O
return	O
polarity	O
Algorithm	O
2	O
Inter	O
-	O
Modal	O
Interactive	O
Module	O
(	O
IIM	O
)	O
procedure	O
IIM	O
(	O
X	O
,	O
Y	O
)	O
C	O
XY	O
IIM	O
Encoder	O
(	O
X	O
,	O
Y	O
)	O
Y	O
IIM	O
Decoder	O
(	O
C	O
XY	O
)	O
loss	B-MetricName
cross	O
entropy	O
(	O
Y	O
,	O
Y	O
)	O
Backpropagation	O
to	O
update	O
the	O
weights	O
return	O
C	O
XY	O
Algorithm	O
3	O
Context	O
-	O
aware	O
Attention	O
Module	O
(	O
CAM	B-MethodName
)	O
procedure	O
CAM	B-MethodName
(	O
M	O
,	O
BI	O
)	O
P	O
M.BI	O
T	O
Cross	O
product	O
for	O
i	O
,	O
j	O
1	O
,	O
...	O
,	O
u	O
do	O
u	O
=	O
#	O
utterances	O
N	O
(	O
i	O
,	O
j	O
)	O
e	O
P	O
(	O
i	O
,	O
j	O
)	O
u	O
k=1	O
e	O
P	O
(	O
i	O
,	O
k	O
)	O
O	O
N.BI	O
A	O
O	O
M	O
Multiplicative	O
gating	O
.	O
return	O
A	O
wise	O
concatenation	O
is	O
performed	O
over	O
the	O
output	O
of	O
Bi	O
-	O
GRU	B-MethodName
and	O
passed	O
through	O
a	O
fully	O
-	O
connected	O
layer	O
to	O
extract	O
the	O
bi	O
-	O
modal	O
interaction	O
(	O
BI	O
)	O
.	O
Further	O
,	O
we	O
employ	O
a	O
Context	O
-	O
aware	O
Attention	O
Module	O
(	O
CAM	B-MethodName
)	O
to	O
exploit	O
the	O
correspondence	O
among	O
the	O
neighboring	O
utterances	O
.	O
The	O
inputs	O
to	O
the	O
CAM	B-MethodName
are	O
the	O
two	O
representations	O
for	O
each	O
pair	O
of	O
modalities	O
,	O
e.g.	O
,	O
mean	O
representation	O
M	O
T	O
A	O
and	O
bi	O
-	O
modal	O
interaction	O
BI	O
T	O
A	O
for	O
the	O
text	O
-	O
acoustic	O
pair	O
.	O
The	O
attention	O
module	O
assists	O
the	O
network	O
in	O
attending	O
the	O
contributing	O
features	O
by	O
putting	O
weights	O
to	O
the	O
current	O
and	O
the	O
neighboring	O
utterances	O
in	O
a	O
video	O
.	O
In	O
the	O
end	O
,	O
the	O
pair	O
-	O
wise	O
(	O
i.e.	O
,	O
text	O
-	O
acoustic	O
,	O
text	O
-	O
visual	O
,	O
and	O
acoustic	O
-	O
visual	O
)	O
attended	O
representations	O
are	O
concatenated	O
and	O
fed	O
to	O
an	O
output	O
layer	O
for	O
the	O
prediction	O
.	O
We	O
depict	O
and	O
summarize	O
the	O
proposed	O
approach	O
in	O
Figure	O
1	O
and	O
Algorithm	O
1	O
,	O
2	O
,	O
and	O
3	O
.	O
The	O
source	O
code	O
is	O
available	O
at	O
http://www.iitp	O
.	O
ac.in/˜ai	O
-	O
nlp	O
-	O
ml	O
/	O
resources.html	O
.	O

For	O
the	O
evaluation	O
of	O
our	O
proposed	O
approach	O
,	O
we	O
employ	O
five	O
multi	O
-	O
modal	O
benchmark	O
datasets	O
1	O
covering	O
two	O
affect	O
analysis	O
tasks	O
,	O
i.e.	O
,	O
sentiment	O
and	O
emotion	B-DatasetName
.	O
Table	O
1	O
:	O
Results	O
of	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
for	O
the	O
proposed	O
approach	O
.	O
T	O
:	O
Text	O
,	O
V	O
:	O
Visual	O
,	O
A	O
:	O
Acoustic	O
.	O
Weighted	O
accuracy	B-MetricName
as	O
a	O
metric	O
is	O
chosen	O
due	O
to	O
unbalanced	O
samples	O
across	O
various	O
emotions	O
and	O
it	O
is	O
also	O
in	O
line	O
with	O
the	O
other	O
existing	O
works	O
(	O
Zadeh	O
et	O
al	O
,	O
2018c	O

We	O
implement	O
our	O
proposed	O
model	O
on	O
the	O
Pythonbased	O
Keras	O
deep	O
learning	O
library	O
.	O
As	O
the	O
evaluation	O
metric	O
,	O
we	O
employ	O
accuracy	B-MetricName
(	O
weighted	O
accu	O
-	O
racy	O
(	O
Tong	O
et	O
al	O
,	O
2017	O
)	O
)	O
and	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
for	O
the	O
classification	O
problems	O
,	O
while	O
for	O
the	O
intensity	O
prediction	O
task	O
,	O
we	O
compute	O
Pearson	B-MetricName
correlation	I-MetricName
scores	O
and	O
mean	O
-	O
absolute	O
-	O
error	O
(	O
MAE	B-MetricName
)	O
.	O
We	O
evaluate	O
our	O
proposed	O
CIA	O
model	O
on	O
five	O
benchmark	O
datasets	O
i.e.	O
,	O
MOUD	O
,	O
MOSI	B-DatasetName
,	O
YouTube	O
,	O
ICT	O
-	O
MMMO	O
,	O
and	O
MOSEI	O
.	O
For	O
all	O
the	O
datasets	O
,	O
we	O
perform	O
grid	O
search	O
to	O
find	O
the	O
optimal	O
hyperparameters	O
(	O
c.f	O
.	O
Table	O
4	O
)	O
.	O
Though	O
we	O
push	O
for	O
a	O
generic	O
hyper	O
-	O
parameter	O
configuration	O
for	O
all	O
datasets	O
,	O
in	O
some	O
cases	O
,	O
a	O
different	O
choice	O
of	O
the	O
parameter	O
has	O
a	O
significant	O
effect	O
.	O
Therefore	O
,	O
we	O
choose	O
different	O
parameters	O
for	O
different	O
datasets	O
for	O
our	O
experiments	O
.	O
Details	O
of	O
hyper	O
-	O
parameters	O
for	O
different	O
datasets	O
are	O
depicted	O
in	O
Table	O
4	O
.	O
We	O
use	O
different	O
activation	O
functions	O
for	O
the	O
various	O
modules	O
in	O
our	O
model	O
.	O
We	O
use	O
tanh	O
as	O
the	O
activation	B-HyperparameterName
function	I-HyperparameterName
for	O
the	O
inter	O
-	O
modal	O
interactive	O
module	O
(	O
IIM	O
)	O
,	O
while	O
we	O
employ	O
ReLu	B-MethodName
for	O
the	O
context	O
-	O
aware	O
attention	O
module	O
.	O
For	O
each	O
dataset	O
,	O
we	O
use	O
Adam	B-MethodName
as	O
optimizer	B-HyperparameterName
.	O
In	O
this	O
paper	O
,	O
we	O
address	O
three	O
multi	O
-	O
modal	O
affective	O
analysis	O
problems	O
,	O
namely	O
i.e.	O
,	O
sentiment	O
classification	O
(	O
S	O
C	O
)	O
,	O
sentiment	O
intensity	O
(	O
S	O
I	O
)	O
and	O
emotion	B-TaskName
classification	I-TaskName
(	O
E	O
C	O
)	O
.	O
We	O
use	O
softmax	B-MethodName
as	O
a	O
classifier	O
for	O
sentiment	O
classification	O
,	O
while	O
optimizing	O
the	O
categorical	O
cross	O
-	O
entropy	O
as	O
a	O
loss	B-MetricName
function	O
.	O
In	O
comparison	O
,	O
we	O
use	O
sigmoid	O
for	O
prediction	O
and	O
binary	O
cross	O
-	O
entropy	O
as	O
the	O
loss	B-MetricName
function	O
for	O
the	O
emotion	B-TaskName
classification	I-TaskName
.	O
As	O
the	O
emotions	O
in	O
the	O
dataset	O
are	O
multi	O
-	O
labeled	O
,	O
we	O
apply	O
a	O
threshold	O
over	O
the	O
predicted	O
sigmoid	O
outputs	O
for	O
each	O
emotion	B-DatasetName
and	O
consider	O
all	O
the	O
emotions	O
as	O
present	O
whose	O
respective	O
values	O
are	O
above	O
the	O
threshold	O
.	O
We	O
cross	O
-	O
validate	O
and	O
optimize	O
both	O
We	O
evaluate	O
our	O
proposed	O
approach	O
for	O
all	O
the	O
possible	O
input	O
combinations	O
i.e.	O
,	O
uni	O
-	O
modal	O
(	O
T	O
,	O
A	O
,	O
V	O
)	O
,	O
bi	O
-	O
modal	O
(	O
T+V	O
,	O
T+A	O
,	O
A+V	O
)	O
and	O
tri	O
-	O
modal	O
(	O
T+V+A	O
)	O
.	O
We	O
depict	O
our	O
obtained	O
results	O
in	O
Table	O
1	O
.	O
For	O
MOSEI	O
dataset	O
,	O
with	O
tri	O
-	O
modal	O
inputs	O
,	O
our	O
proposed	O
system	O
reports	O
79.02	O
%	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
and	O
62.97	O
%	O
weighted	O
-	O
accuracy	B-MetricName
for	O
emotion	B-TaskName
classification	I-TaskName
.	O
For	O
sentiment	O
classification	O
,	O
we	O
obtain	O
78.23	O
%	O
,	O
80.37	O
%	O
,	O
49.15	O
%	O
and	O
50.14	O
%	O
as	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
for	O
two	O
-	O
class	O
,	O
five	O
-	O
class	O
and	O
seven	O
-	O
class	O
,	O
respectively	O
.	O
For	O
sentiment	O
intensity	O
prediction	O
task	O
,	O
our	O
proposed	O
system	O
yields	O
MAE	B-MetricName
and	O
Pearson	O
score	O
of	O
0.683	O
and	O
0.594	O
,	O
respectively	O
.	O
We	O
also	O
observe	O
that	O
the	O
proposed	O
approach	O
yields	O
better	O
performance	O
for	O
the	O
tri	O
-	O
modal	O
inputs	O
than	O
the	O
bi	O
-	O
modal	O
and	O
uni	O
-	O
modal	O
input	O
combinations	O
.	O
This	O
improvement	O
implies	O
that	O
our	O
proposed	O
CIA	O
architecture	O
utilizes	O
the	O
interaction	O
among	O
the	O
input	O
modalities	O
very	O
effectively	O
.	O
Furthermore	O
,	O
for	O
the	O
other	O
datasets	O
,	O
i.e.	O
,	O
MOSI	B-DatasetName
,	O
ICT	O
-	O
MMMO	O
,	O
YouTube	O
,	O
and	O
MOUD	O
,	O
we	O
also	O
observe	O
a	O
similar	O
phenomenon	O
as	O
well	O
(	O
c.f	O
.	O
Table	O
1	O
)	O
.	O
To	O
show	O
that	O
our	O
proposed	O
IIM	O
module	O
,	O
indeed	O
,	O
learns	O
the	O
interaction	O
among	O
the	O
distinct	O
modalities	O
,	O
we	O
also	O
perform	O
an	O
ablation	O
study	O
of	O
the	O
proposed	O
CIA	O
architecture	O
.	O
Consequently	O
,	O
we	O
omit	O
the	O
IIM	O
module	O
from	O
our	O
architecture	O
and	O
compute	O
the	O
self	O
-	O
attention	O
on	O
the	O
pair	O
-	O
wise	O
fully	O
-	O
connected	O
representations	O
for	O
the	O
prediction	O
.	O
We	O
observe	O
that	O
,	O
for	O
all	O
the	O
datasets	O
,	O
the	O
performance	O
of	O
this	O
modified	O
architecture	O
(	O
i.e.	O
,	O
CIA	O
-	O
IIM	O
)	O
is	O
constantly	O
inferior	O
(	O
with	O
1	O
%	O
to	O
7	O
%	O
F	O
-	O
score	O
points	O
)	O
to	O
the	O
proposed	O
CIA	O
architecture	O
.	O
This	O
performance	O
degradation	O
suggests	O
that	O
the	O
IIM	O
module	O
is	O
,	O
indeed	O
,	O
an	O
important	O
component	O
of	O
our	O
proposed	O
architecture	O
.	O
In	O
Table	O
3	O
,	O
we	O
depict	O
the	O
evaluation	O
results	O
for	O
both	O
-	O
with	O
and	O
without	O
IIM	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
our	O
comparative	O
studies	O
against	O
several	O
existing	O
and	O
recent	O
state	O
-	O
ofthe	O
-	O
art	O
systems	O
.	O
For	O
each	O
dataset	O
,	O
we	O
report	O
three	O
best	O
systems	O
for	O
the	O
comparisons	O
2	O
.	O
In	O
particular	O
,	O
we	O
compare	O
with	O
the	O
following	O
systems	O
:	O
Bag	O
of	O
Feature	O
-	O
Multimodal	B-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName
(	O
BoF	O
-	O
MSA	O
)	O
(	O
Blanchard	O
et	O
al	O
,	O
2018	O
)	O
,	O
Memory	O
Fusion	O
Network	O
(	O
MFN	O
)	O
(	O
Zadeh	O
et	O
al	O
,	O
2018b	O
)	O
(	O
Zadeh	O
et	O
al	O
,	O
2018c	O
)	O
,	O
Tensor	O
Fusion	O
Network	O
(	O
TFN	O
)	O
,	O
Random	O
Forest	O
(	O
RF	O
)	O
(	O
Breiman	O
,	O
2001	O
)	O
,	O
Support	B-MethodName
Vector	I-MethodName
Machine	I-MethodName
(	O
Zadeh	O
et	O
al	O
,	O
2016	O
)	O
,	O
Multi	O
-	O
Attention	O
Recurrent	O
Network	O
(	O
MARN	O
)	O
(	O
Zadeh	O
et	O
al	O
,	O
2018a	O
)	O
,	O
Dynamic	O
Fusion	O
Graph	O
(	O
DFG	O
)	O
(	O
Zadeh	O
et	O
al	O
,	O
2018c	O
)	O
,	O
Multi	O
Modal	O
Multi	O
Utterance	O
-	O
Bimodal	O
Attention	O
(	O
MMMU	O
-	O
BA	B-DatasetName
)	O
(	O
Ghosal	O
et	O
al	O
,	O
2018	O
)	O
,	O
Bi	O
-	O
directional	O
Contextual	O
LSTM	B-MethodName
(	O
BC	O
-	O
LSTM	B-MethodName
)	O
(	O
Poria	O
et	O
al	O
,	O
2017b	O
)	O
and	O
Multimodal	O
Factorization	O
Model	O
(	O
MFM	O
)	O
(	O
Tsai	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
show	O
the	O
comparative	O
results	O
in	O
Table	O
5a	O
and	O
Table	O
5b	O
for	O
emotion	B-DatasetName
and	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
respectively	O
.	O
We	O
observe	O
that	O
the	O
proposed	O
CIA	O
framework	O
yields	O
better	O
performance	O
against	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
all	O
the	O
cases	O
.	O
For	O
emotion	B-TaskName
classification	I-TaskName
,	O
our	O
proposed	O
approach	O
achieves	O
approximately	O
3	O
and	O
0.6	O
percentage	O
higher	O
F1	B-MetricName
-	O
(	O
Zadeh	O
et	O
al	O
,	O
2018c	O
)	O
.	O
†	O
Values	O
are	O
taken	O
from	O
(	O
Tsai	O
et	O
al	O
,	O
2018	O
)	O
.	O
Significance	O
T	O
-	O
test	O
(	O
<	O
0.05	O
)	O
signifies	O
that	O
the	O
obtained	O
results	O
are	O
statistically	O
significant	O
over	O
the	O
existing	O
systems	O
with	O
95	O
%	O
confidence	O
score	O
.	O
score	O
and	O
weighted	O
accuracy	B-MetricName
,	O
respectively	O
,	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
DFG	O
(	O
Zadeh	O
et	O
al	O
,	O
2018c	O
)	O
system	O
.	O
Furthermore	O
,	O
we	O
also	O
see	O
improvements	O
for	O
most	O
of	O
the	O
individual	O
emotion	B-DatasetName
classes	O
as	O
well	O
.	O
In	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
c.f	O
.	O
Table	O
5b	O
)	O
,	O
for	O
all	O
the	O
five	O
datasets	O
and	O
different	O
experimental	O
setups	O
,	O
the	O
proposed	O
CIA	O
framework	O
obtains	O
the	O
improved	O
accuracies	O
for	O
the	O
classification	O
tasks	O
.	O
For	O
intensity	O
prediction	O
,	O
our	O
proposed	O
framework	O
yields	O
lesser	O
mean	O
-	O
absolute	O
-	O
error	O
with	O
high	O
Pearson	B-MetricName
correlation	I-MetricName
scores	O
.	O
On	O
average	O
,	O
we	O
observe	O
1	O
to	O
5	O
%	O
improvement	O
in	O
accuracy	B-MetricName
values	O
in	O
comparison	O
to	O
the	O
next	O
best	O
systems	O
.	O
Similarly	O
,	O
for	O
the	O
intensity	O
prediction	O
task	O
,	O
we	O
report	O
approximately	O
0.03	O
and	O
0.04	O
points	O
improvement	O
in	O
mean	O
-	O
absolute	O
-	O
error	O
and	O
Pearson	O
score	O
,	O
respectively	O
.	O
We	O
perform	O
statistical	O
significance	O
test	O
(	O
paired	O
T	O
-	O
test	O
)	O
on	O
the	O
obtained	O
results	O
and	O
observe	O
that	O
performance	O
improvement	O
in	O
the	O
proposed	O
model	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
is	O
significant	O
with	O
95	O
%	O
confidence	O
(	O
i.e.	O
,	O
p	O
-	O
value	O
<	O
0.05	O
)	O
.	O

Inspired	B-DatasetName
by	O
Gui	O
et	O
al	O
(	O
2020	O
)	O
,	O
we	O
successively	O
introduce	O
two	O
classifiers	O
-	O
initial	O
classifier	O
and	O
iterative	O
classifier	O
to	O
construct	O
IRSE	O
-	O
Graph	O
.	O
Both	O
classifiers	O
are	O
constructed	O
using	O
slightly	O
adapted	O
GRN	O
and	O
utilized	O
to	O
deal	O
with	O
different	O
scenarios	O
,	O
respectively	O
.	O
In	O
this	O
way	O
,	O
we	O
can	O
fully	O
exploit	O
the	O
potential	O
of	O
iterative	O
classifier	O
to	O
predict	O
better	O
pairwise	O
orderings	O
.	O
We	O
will	O
give	O
a	O
detail	O
introduction	O
to	O
the	O
slightly	O
adapted	O
GRN	O
in	O
Section	O
4.3	O
.	O
To	O
better	O
understand	O
the	O
procedure	O
of	O
constructing	O
IRSE	O
-	O
Graph	O
,	O
we	O
provide	O
the	O
details	O
in	O
Algorithm	O
1	O
.	O
During	O
this	O
procedure	O
,	O
pairwise	O
orderings	O
are	O
iteratively	O
predicted	O
and	O
gradually	O
incorporated	O
to	O
refine	O
IRSE	O
-	O
Graph	O
.	O
Here	O
we	O
introduce	O
a	O
set	O
VP	O
(	O
k	O
)	O
to	O
collect	O
sentence	O
node	O
pairs	O
with	O
uncertain	O
pairwise	O
orderings	O
at	O
the	O
k	O
-	O
th	O
iteration	O
.	O
First	O
,	O
we	O
bulid	O
an	O
initial	O
classifier	O
based	O
on	O
the	O
initial	O
IRSE	O
-	O
Graph	O
,	O
where	O
the	O
learned	O
sentence	O
representations	O
are	O
used	O
to	O
predict	O
pairwise	O
orderings	O
between	O
any	O
two	O
linked	O
sentences	O
only	O
once	O
(	O
Lines	O
2	O
-	O
6	O
)	O
.	O
Note	O
that	O
in	O
the	O
initial	O
IRSE	O
-	O
Graph	O
,	O
all	O
weights	O
of	O
ss	O
-	O
edges	O
are	O
set	O
to	O
0.5	O
.	O
In	O
this	O
case	O
,	O
IRSE	O
-	O
Graph	O
degrades	O
to	O
the	O
conventional	O
SE	O
-	O
Graph	O
.	O
Concretely	O
,	O
for	O
any	O
two	O
linked	O
sentence	O
nodes	O
v	O
i	O
and	O
v	O
i	O
,	O
we	O
concatenate	O
their	O
vector	O
representations	O
κ	O
i	O
and	O
κ	O
i	O
as	O
[	O
κ	O
i	O
;	O
κ	O
i	O
]	O
and	O
[	O
κ	O
i	O
;	O
κ	O
i	O
]	O
,	O
which	O
are	O
fed	O
into	O
an	O
MLP	B-DatasetName
classifier	O
to	O
obtain	O
two	O
probabilities	O
.	O
Then	O
,	O
we	O
normalize	O
and	O
convert	O
these	O
two	O
probabilities	O
into	O
ss	O
-	O
edge	O
weights	O
w	O
i	O
,	O
i	O
and	O
w	O
i	O
,	O
i	O
.	O
If	O
both	O
w	O
i	O
,	O
i	O
and	O
w	O
i	O
,	O
i	O
are	O
within	O
a	O
prefixed	O
interval	O
[	O
δ	B-HyperparameterName
min	O
,	O
δ	B-HyperparameterName
max	O
]	O
,	O
we	O
consider	O
(	O
v	O
i	O
,	O
v	O
i	O
)	O
as	O
a	O
sentence	O
node	O
pair	O
with	O
uncertain	O
pairwise	O
ordering	O
and	O
add	O
it	O
into	O
VP	O
(	O
0	B-DatasetName
)	O
.	O
Moreover	O
,	O
we	O
replace	O
both	O
w	O
i	O
,	O
i	O
and	O
w	O
i	O
,	O
i	O
with	O
0.5	O
,	O
indicating	O
that	O
they	O
will	O
be	O
repredicted	O
in	O
the	O
next	O
iteration	O
.	O
In	O
the	O
following	O
,	O
we	O
also	O
construct	O
an	O
iterative	O
classifier	O
based	O
on	O
IRSE	O
-	O
Graph	O
.	O
However	O
,	O
in	O
an	O
easy	O
-	O
to	O
-	O
hard	O
manner	O
,	O
we	O
use	O
iterative	O
classifier	O
to	O
perform	O
pairwise	O
ordering	O
predictions	O
,	O
where	O
the	O
ss	O
-	O
edge	O
weights	O
of	O
IRSE	O
-	O
Graph	O
are	O
continously	O
updated	O
with	O
previously	O
-	O
predicted	O
pairwise	O
orderings	O
with	O
high	O
confidence	O
(	O
Lines	O
13	O
-	O
26	O
)	O
.	O
By	O
doing	O
so	O
,	O
graph	O
representations	O
can	O
be	O
continously	O
refined	O
for	O
better	O
subsequent	O
predictions	O
.	O
More	O
specifically	O
,	O
the	O
k	O
-	O
th	O
iteration	O
of	O
this	O
classifier	O
mainly	O
involve	O
three	O
steps	O
:	O
In	O
Step	O
1	O
,	O
based	O
on	O
the	O
current	O
IRSE	O
-	O
Graph	O
,	O
we	O
employ	O
the	O
adapted	O
GRN	O
to	O
conduct	O
graph	O
encoding	O
to	O
learn	O
sentence	O
representations	O
(	O
Line	O
15	O
)	O
.	O
In	O
Step	O
2	O
,	O
on	O
the	O
top	O
of	O
learned	O
sentence	O
representations	O
,	O
we	O
stack	O
an	O
MLP	B-DatasetName
classifier	O
to	O
predict	O
pairwise	O
orderings	O
for	O
sentence	O
node	O
pairs	O
in	O
VP	O
(	O
k	O
)	O
(	O
Lines	O
16	O
-	O
19	O
)	O
.	O
Likewise	O
,	O
we	O
collect	O
sentence	O
node	O
pairs	O
with	O
uncertain	O
pairwise	O
orderings	O
to	O
form	O
VP	O
(	O
k+1	O
)	O
,	O
and	O
reassign	O
their	O
corresponding	O
ss	O
-	O
edge	O
weights	O
as	O
0.5	O
,	O
so	O
as	O
to	O
avoid	O
the	O
negative	O
effect	O
of	O
these	O
uncertain	O
ss	O
-	O
edge	O
weights	O
during	O
the	O
next	O
Algorithm	O
1	O
The	O
procedure	O
of	O
constructing	O
IRSE	O
-	O
Graph	O
Input	O
:	O
the	O
initial	O
IRSE	O
-	O
Graph	O
:	O
G=	O
(	O
V	O
,	O
E	O
,	O
W	O
)	O
with	O
all	O
w	O
i	O
,	O
i	O
=	O
0	B-DatasetName
;	O
two	O
thresholds	O
:	O
δmin	O
,	O
δmax	O
Output	O
:	O
the	O
final	O
IRSE	O
-	O
Graph	O
:	O
G	O
=	O
(	O
V	O
,	O
E	O
,	O
W	O
)	O
1	O
:	O
VP	O
(	O
0	B-DatasetName
)	O
2	O
:	O
{	O
κi	O
}	O
I	O
i=1	O
GRN	O
(	O
G	O
)	O
3	O
:	O
for	O
any	O
linked	O
sentence	O
node	O
pair	O
(	O
vi	O
,	O
v	O
i	O
)	O
&	O
&	O
i	O
<	O
i	O
do	O
4	O
:	O
w	O
i	O
,	O
i	O
InitialClassifer	O
(	O
[	O
κi	O
;	O
κ	O
i	O
]	O
)	O
5	O
:	O
w	O
i	O
,	O
i	O
InitialClassifer	O
(	O
[	O
κ	O
i	O
;	O
κi	O
]	O
)	O
6	O
:	O
w	O
i	O
,	O
i	O
,	O
w	O
i	O
,	O
i	O
Normalize	O
(	O
w	O
i	O
,	O
i	O
,	O
w	O
i	O
,	O
i	O
)	O
7	O
:	O
if	O
δmin	O
≤	O
w	O
i	O
,	O
i	O
≤	O
δmax	O
then	O
8	O
:	O
VP	O
(	O
0	B-DatasetName
)	O
VP	O
(	O
0	B-DatasetName
)	O
∪	O
{	O
(	O
vi	O
,	O
v	O
i	O
)	O
}	O
9	O
:	O
w	O
i	O
,	O
i	O
0.5	O
,	O
w	O
i	O
,	O
i	O
0.5	O
10	O
:	O
end	O
if	O
11	O
:	O
end	O
for	O
12	O
:	O
k	O
0	B-DatasetName
13	O
:	O
repeat	O
14	O
:	O
iteration	O
(	O
Lines	O
20	O
-	O
23	O
)	O
.	O
VP	O
(	O
k+1	O
)	O
15	O
:	O
{	O
κi	O
}	O
I	O
i=1	O
GRN	O
(	O
G	O
)	O
16	O
:	O
for	O
(	O
vi	O
,	O
v	O
i	O
)	O
VP	O
(	O
k	O
)	O
do	O
17	O
:	O
w	O
i	O
,	O
i	O
IterativeClassifer	O
(	O
[	O
κi	O
;	O
κ	O
i	O
]	O
)	O
18	O
:	O
w	O
i	O
,	O
i	O
IterativeClassifer	O
(	O
[	O
κ	O
i	O
;	O
κi	O
]	O
)	O
19	O
:	O
w	O
i	O
,	O
i	O
,	O
w	O
i	O
,	O
i	O
Normalize	O
(	O
w	O
i	O
,	O
i	O
,	O
w	O
i	O
,	O
i	O
)	O
20	O
:	O
if	O
δmin	O
≤	O
w	O
i	O
,	O
i	O
≤	O
δmax	O
then	O
21	O
:	O
VP	O
(	O
k+1	O
)	O
VP	O
(	O
k+1	O
)	O
∪	O
{	O
(	O
vi	O
,	O
v	O
i	O
)	O
}	O
22	O
:	O
w	O
i	O
,	O
i	O
0.5	O
,	O
w	O
i	O
,	O
i	O
0.5	O
23	O
:	O
end	O
if	O
24	O
:	O
end	O
for	O
25	O
:	O
k	O
k	O
+	O
1	O
26	O
:	O
until	O
VP	O
(	O
k+1	O
)	O
=	O
=	O
VP	O
(	O
k	O
)	O
|	O
|	O
VP	O
(	O
k	O
)	O
=	O
=	O
27	O
:	O
return	O
G	O
In	O
Step	O
3	O
,	O
if	O
VP	O
(	O
k+1	O
)	O
is	O
equal	O
to	O
VP	O
(	O
k	O
)	O
or	O
empty	O
,	O
we	O
believe	O
the	O
learning	O
of	O
IRSE	O
-	O
Graph	O
G	O
has	O
converged	O
and	O
thus	O
return	O
it	O
(	O
Lines	O
26	O
-	O
27	O
)	O
.	O
Although	O
both	O
of	O
our	O
classifiers	O
are	O
constructed	O
using	O
IRSE	O
-	O
Graph	O
,	O
their	O
training	O
procedures	O
are	O
slightly	O
different	O
.	O
As	O
for	O
initial	O
classifier	O
,	O
we	O
directly	O
train	O
it	O
on	O
the	O
initial	O
IRSE	O
-	O
Graph	O
without	O
any	O
pairwise	O
ordering	O
information	O
(	O
all	O
ss	O
-	O
edge	O
weights	O
are	O
set	O
to	O
0.5	O
)	O
.	O
By	O
contrast	O
,	O
we	O
train	O
iterative	O
classifier	O
on	O
IRSE	O
-	O
Graph	O
with	O
partial	O
pairwise	O
orderings	O
.	O
To	O
enable	O
iterative	O
classifier	O
generalizable	O
to	O
any	O
IRSE	O
-	O
Graph	O
with	O
partial	O
predicted	O
pairwise	O
orderings	O
,	O
we	O
first	O
set	O
al	O
ss	O
-	O
edge	O
weights	O
to	O
1	O
or	O
0	B-DatasetName
according	O
to	O
their	O
ground	O
-	O
truth	O
pairwise	O
orderings	O
,	O
and	O
then	O
train	O
the	O
classifier	O
to	O
correctly	O
predict	O
pari	O
-	O
wise	O
orderings	O
for	O
other	O
pairs	O
.	O
Concretely	O
,	O
if	O
s	O
i	O
appears	O
before	O
s	O
i	O
,	O
we	O
set	O
w	O
i	O
,	O
i	O
=	O
1	O
and	O
w	O
i	O
,	O
i	O
=	O
0	B-DatasetName
,	O
vice	O
versa	O
.	O
For	O
example	O
,	O
in	O
the	O
left	O
part	O
of	O
Figure	O
3	O
,	O
the	O
ground	O
-	O
truth	O
sentence	O
sequence	O
is	O
s	O
1	O
,	O
s	O
2	O
,	O
s	O
3	O
,	O
s	O
4	O
,	O
and	O
thus	O
we	O
assign	O
the	O
ss	O
-	O
edge	O
weights	O
of	O
linked	O
sentence	O
node	O
pairs	O
(	O
v	O
1	O
,	O
v	O
2	O
)	O
,	O
(	O
v	O
3	O
,	O
v	O
2	O
)	O
,	O
(	O
v	O
3	O
,	O
v	O
4	O
)	O
,	O
(	O
v	O
2	O
,	O
v	O
4	O
)	O
as	O
follows	O
:	O
w	O
1	O
,	O
2	O
=	O
1	O
,	O
w	O
2	O
,	O
3	O
=	O
1	O
,	O
w	O
2	O
,	O
4	O
=	O
1	O
,	O
w	O
3	O
,	O
4	O
=	O
1	O
,	O
and	O
w	O
2	O
,	O
1	O
=	O
0	B-DatasetName
,	O
w	O
3	O
,	O
2	O
=	O
0	B-DatasetName
,	O
w	O
4	O
,	O
2	O
=	O
0	B-DatasetName
,	O
w	O
4	O
,	O
3	O
=	O
0	B-DatasetName
.	O
Moreover	O
,	O
to	O
enhance	O
the	O
robustness	O
of	O
the	O
iterative	O
classifier	O
,	O
we	O
randomly	O
select	O
a	O
certain	O
ratio	O
η	O
of	O
sentence	O
pairs	O
and	O
assign	O
their	O
ss	O
-	O
edges	O
with	O
incorrect	O
weights	O
.	O
Let	O
us	O
revisit	O
Figure	O
3	O
,	O
for	O
the	O
randomly	O
selected	O
sentence	O
node	O
pair	O
(	O
v	O
1	O
,	O
v	O
2	O
)	O
,	O
we	O
assign	O
ss	O
-	O
edges	O
weights	O
w	O
1	O
,	O
2	O
and	O
w	O
2	O
,	O
1	O
with	O
randomly	O
generated	O
noisy	O
values	O
0.3	O
and	O
0.7	O
respectively	O
.	O
In	O
this	O
way	O
,	O
we	O
expect	O
that	O
iterative	O
classifier	O
can	O
conduct	O
correct	O
predictions	O
even	O
given	O
incorrect	O
previously	O
-	O
predicted	O
pairwise	O
orderings	O
.	O

Datasets	O
.	O
Following	O
previous	O
work	O
(	O
Yin	O
et	O
al	O
,	O
2020	O
;	O
Cui	O
et	O
al	O
,	O
2018	O
;	O
Yin	O
et	O
al	O
,	O
2021	O
)	O
,	O
we	O
carry	O
out	O
experiments	O
on	O
five	O
benchmark	O
datasets	O
:	O
SIND	B-DatasetName
,	O
ROCStory	O
.	O
SIND	B-DatasetName
is	O
a	O
visual	B-TaskName
storytelling	I-TaskName
dataset	O
and	O
ROCStory	O
(	O
Mostafazadeh	O
et	O
al	O
,	O
2016	O
)	O
is	O
about	O
commonsense	O
stories	O
.	O
Both	O
two	O
datasets	O
are	O
composed	O
of	O
5	O
-	O
sentence	O
stories	O
and	O
randomly	O
split	O
by	O
8:1:1	O
for	O
the	O
training	O
/	O
validation	O
/	O
test	O
sets	O
.	O
NIPS	O
Abstract	O
,	O
AAN	O
Abstract	O
,	O
arXiv	B-DatasetName
Abstract	O
.	O
These	O
three	O
datasets	O
consist	O
of	O
abstracts	O
from	O
research	O
papers	O
,	O
which	O
are	O
collected	O
from	O
NIPS	O
,	O
ACL	O
anthology	O
and	O
arXiv	B-DatasetName
,	O
respectively	O
(	O
Radev	O
et	O
al	O
,	O
2016	O
;	O
(	O
Yin	O
et	O
al	O
,	O
2021	O
)	O
for	O
our	O
model	O
and	O
its	O
variants	O
.	O
Specifically	O
,	O
we	O
apply	O
100	O
-	O
dimensional	O
GloVe	B-MethodName
word	B-TaskName
embeddings	I-TaskName
,	O
and	O
set	O
the	O
sizes	O
of	O
Bi	O
-	O
LSTM	B-MethodName
hidden	O
states	O
,	O
sentence	O
node	O
states	O
,	O
and	O
entity	O
node	O
states	O
as	O
512	O
,	O
512	O
and	O
150	O
,	O
respectively	O
.	O
The	O
recurrent	O
step	O
of	O
GRN	O
is	O
3	O
.	O
We	O
empirically	O
set	O
thresholds	O
δ	B-HyperparameterName
min	O
and	O
δ	B-HyperparameterName
max	O
as	O
0.2	O
and	O
0.8	O
,	O
and	O
set	O
η	O
as	O
20	O
%	O
,	O
15	O
%	O
,	O
25	O
%	O
,	O
15	O
%	O
,	O
15	O
%	O
according	O
to	O
accuracies	O
of	O
initial	O
classifier	O
on	O
validation	O
sets	O
.	O
Besides	O
,	O
we	O
individually	O
set	O
the	O
coefficient	O
λ	O
(	O
See	O
Equation	O
18	O
in	O
(	O
Yin	O
et	O
al	O
,	O
2020	O
)	O
)	O
as	O
0.5	O
,	O
0.5	O
,	O
0.2	O
,	O
0.4	O
,	O
0.5	O
on	O
the	O
five	O
datasets	O
.	O
We	O
adopt	O
Adadelta	B-MethodName
(	O
Zeiler	O
,	O
2012	O
)	O
with	O
=	O
10	O
−6	O
,	O
ρ	O
=	O
0.95	O
and	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
1.0	O
as	O
the	O
optimizer	B-HyperparameterName
.	O
We	O
employ	O
L2	O
weight	B-MethodName
decay	I-MethodName
with	O
coefficient	O
10	O
−5	O
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	O
and	O
dropout	O
rate	O
of	O
0.5	O
.	O
When	O
constructing	O
our	O
model	O
based	O
on	O
BERT	B-MethodName
,	O
we	O
use	O
the	O
same	O
settings	O
as	O
(	O
Cui	O
et	O
al	O
,	O
2020	O
)	O
.	O
Concretely	O
,	O
we	O
set	O
sizes	O
of	O
hidden	O
states	O
and	O
node	O
states	O
to	O
768	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
BERT	B-MethodName
as	O
3e	O
-	O
3	O
,	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
as	O
16	O
,	O
32	O
,	O
128	O
,	O
128	O
,	O
64	O
for	O
the	O
five	O
datasets	O
.	O
Baselines	O
.	O
To	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
model	O
(	O
IRSE	O
-	O
GRN	O
)	O
,	O
we	O
compare	O
it	O
with	O
SE	O
-	O
GRN	O
(	O
Yin	O
et	O
al	O
,	O
2021	O
)	O
.	O
Besides	O
,	O
we	O
report	O
the	O
performance	O
of	O
following	O
sentence	B-TaskName
ordering	I-TaskName
models	O
:	O
1	O
)	O
Pairwise	O
models	O
:	O
Pairwise	O
Model	O
,	O
RankTxNet	O
(	O
Kumar	B-DatasetName
et	O
al	O
,	O
2020	O
)	O
,	O
andB	O
-	O
TSort	O
(	O
Prabhumoye	O
et	O
al	O
,	O
2020	O
)	O
,	O
ConsGraph	O
(	O
Zhu	O
et	O
al	O
,	O
2021	O
)	O
;	O
2	O
)	O
Set	O
-	O
to	O
-	O
sequence	O
models	O
:	O
HAN	O
(	O
Wang	O
and	O
Wan	O
,	O
2019	O
)	O
,	O
LSTM+PtrNet	O
(	O
Gong	O
et	O
al	O
,	O
2016	O
)	O
,	O
V	O
-	O
LSTM+PtrNet	O
(	O
Logeswaran	O
et	O
al	O
,	O
2018	O
)	O
,	O
ATTOrderNet	O
(	O
Cui	O
et	O
al	O
,	O
2018	O
)	O
,	O
TGCM	O
(	O
Oh	O
et	O
al	O
,	O
2019	O
)	O
,	O
SE	O
-	O
GRN	O
(	O
Yin	O
et	O
al	O
,	O
2019	O
)	O
,	O
SE	O
-	O
GRN	O
(	O
Yin	O
et	O
al	O
,	O
2021	O
)	O
2020	O
)	O
and	O
BERSON	O
(	O
Cui	O
et	O
al	O
,	O
2020	O
)	O
.	O
Furthermore	O
,	O
to	O
examine	O
the	O
compatibility	O
of	O
other	O
technologies	O
with	O
our	O
model	O
,	O
we	O
report	O
the	O
performance	O
of	O
IRSE	O
-	O
GRN	O
equipped	O
with	O
some	O
effective	O
components	O
:	O
1	O
)	O
IRSE	O
-	O
GRN+FHDecoder	O
.	O
In	O
this	O
variant	O
,	O
we	O
equip	O
our	O
model	O
with	O
FHDecoder	O
(	O
Yin	O
et	O
al	O
,	O
2020	O
)	O
,	O
where	O
pairwise	O
ordering	O
information	O
is	O
incorporated	O
;	O
2	O
)	O
IRSE	O
-	O
GRN+BERT+FHDecoder	O
.	O
In	O
addition	O
to	O
FHDecoder	O
,	O
we	O
construct	O
the	O
sentence	O
encoder	O
based	O
on	O
BERT	B-MethodName
,	O
where	O
the	O
mean	O
-	O
pooling	O
outputs	O
of	O
all	O
learned	O
word	O
representations	O
are	O
used	O
to	O
initialize	O
sentence	O
nodes	O
.	O
Evaluation	O
Metrics	O
.	O
Following	O
previous	O
work	O
(	O
Oh	O
et	O
al	O
,	O
2019	O
;	O
Cui	O
et	O
al	O
,	O
2020	O
;	O
Prabhumoye	O
et	O
al	O
,	O
2020	O
;	O
Zhu	O
et	O
al	O
,	O
2021	O
;	O
Yin	O
et	O
al	O
,	O
2021	O
)	O
,	O
we	O
use	O
the	O
following	O
three	O
metrics	O
:	O
1	O
)	O
Kendall	O
's	O
Tau	O
(	O
τ	O
)	O
:	O
Formally	O
,	O
this	O
metric	O
is	O
calculated	O
as	O
1	O
-	O
2×	O
(	O
number	O
of	O
inversions	O
)	O
/	O
I	O
2	O
,	O
where	O
I	O
denotes	O
the	O
sequence	O
length	O
and	O
number	O
of	O
inversions	O
is	O
the	O
number	O
of	O
pairs	O
in	O
the	O
predicted	O
sequence	O
with	O
incorrect	O
relative	O
order	O
(	O
Lapata	O
,	O
2003	O
)	O
;	O
2	O
)	O
Perfect	O
Match	O
Ratio	O
(	O
PMR	O
)	O
:	O
This	O
metric	O
calculates	O
the	O
ratio	O
of	O
samples	O
where	O
the	O
entire	O
sequence	O
is	O
correctly	O
predicted	O
;	O
3	O
)	O
Accuracy	B-MetricName
(	O
Acc	B-MetricName
)	O
:	O
This	O
metric	O
measures	O
the	O
percentage	O
of	O
sentences	O
,	O
whose	O
absolute	O
positions	O
are	O
correctly	O
predicted	O
(	O
Logeswaran	O
et	O
al	O
,	O
2018	O
)	O
.	O

Modern	O
e	O
-	O
commerce	O
catalogs	O
contain	O
millions	O
of	O
references	O
,	O
associated	O
with	O
textual	O
and	O
visual	O
information	O
that	O
is	O
of	O
paramount	O
importance	O
for	O
the	O
products	O
to	O
be	O
found	O
via	O
search	O
or	O
browsing	O
.	O
Of	O
particular	O
significance	O
is	O
the	O
book	O
category	O
,	O
where	O
the	O
author	O
name	O
(	O
s	O
)	O
field	O
poses	O
a	O
significant	O
challenge	O
.	O
Indeed	O
,	O
books	O
written	O
by	O
a	O
given	O
author	O
might	O
be	O
listed	O
with	O
different	O
authors	O
'	O
names	O
due	O
to	O
abbreviations	O
,	O
spelling	O
variants	O
and	O
mistakes	O
,	O
among	O
others	O
.	O
To	O
solve	O
this	O
problem	O
at	O
scale	O
,	O
we	O
design	O
a	O
composite	O
system	O
involving	O
open	O
data	O
sources	O
for	O
books	O
,	O
as	O
well	O
as	O
deep	O
learning	O
components	O
,	O
such	O
as	O
approximate	O
match	O
with	O
Siamese	O
networks	O
and	O
name	O
correction	O
with	O
sequence	O
-	O
tosequence	O
networks	O
.	O
We	O
evaluate	O
this	O
approach	O
on	O
product	O
data	O
from	O
the	O
e	O
-	O
commerce	O
website	O
Rakuten	O
France	O
,	O
and	O
find	O
that	O
the	O
top	O
proposal	O
of	O
the	O
system	O
is	O
the	O
normalized	O
author	O
name	O
with	O
72	O
%	O
accuracy	B-MetricName
.	O

We	O
want	O
to	O
learn	O
a	O
mapping	O
that	O
assigns	O
a	O
similarity	O
score	O
to	O
a	O
pair	O
of	O
author	O
names	O
such	O
that	O
name	O
variants	O
of	O
the	O
same	O
entity	O
will	O
have	O
high	O
similarity	O
,	O
and	O
names	O
that	O
belong	O
to	O
different	O
entities	O
will	O
have	O
low	O
similarity	O
.	O
Once	O
learned	O
,	O
this	O
mapping	O
will	O
enable	O
us	O
to	O
assign	O
an	O
entity	O
to	O
any	O
given	O
name	O
.	O
To	O
this	O
end	O
,	O
we	O
might	O
use	O
a	O
classical	O
string	O
metric	O
such	O
as	O
the	O
Levenshtein	O
distance	O
or	O
the	O
n	O
-	O
gram	O
distance	O
(	O
Kondrak	O
,	O
2005	O
)	O
.	O
However	O
,	O
those	O
are	O
not	O
specific	O
to	O
people	O
's	O
names	O
,	O
and	O
might	O
return	O
a	O
large	O
distance	O
(	O
low	O
similarity	O
)	O
in	O
cases	O
such	O
as	O
the	O
inversion	O
between	O
first	O
name	O
and	O
last	O
name	O
or	O
the	O
abbreviation	O
of	O
the	O
first	O
name	O
to	O
an	O
initial	O
.	O
Thus	O
,	O
we	O
want	O
to	O
use	O
the	O
dataset	O
of	O
name	O
entities	O
to	O
learn	O
a	O
specialized	O
notion	O
of	O
similarity	O
-	O
this	O
is	O
known	O
as	O
distance	B-HyperparameterName
metric	I-HyperparameterName
learning	O
(	O
Kulis	O
et	O
al	O
,	O
2013	O
)	O
.	O
To	O
this	O
purpose	O
,	O
we	O
use	O
a	O
pair	O
of	O
neural	O
networks	O
with	O
shared	O
weights	O
,	O
or	O
Siamese	O
neural	O
network	O
(	O
Bromley	O
et	O
al	O
,	O
1994	O
)	O
.	O
Each	O
network	O
is	O
a	O
recurrent	O
neural	O
network	O
(	O
RNN	O
)	O
composed	O
of	O
a	O
character	O
-	O
level	O
embedding	O
layer	O
with	O
256	O
units	O
,	O
a	O
bidirectional	O
long	O
shortterm	O
memory	O
(	O
LSTM	B-MethodName
)	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
with	O
2	O
×	O
128	O
units	O
,	O
and	O
a	O
dense	O
layer	O
with	O
256	O
units	O
.	O
Each	O
network	O
takes	O
a	O
name	O
as	O
input	O
and	O
outputs	O
a	O
representation	O
-	O
the	O
two	O
representations	O
are	O
then	O
compared	O
using	O
cosine	O
similarity	O
with	O
a	O
target	O
value	O
equal	O
to	O
1	O
for	O
name	O
variants	O
of	O
the	O
same	O
entity	O
,	O
and	O
to	O
0	B-DatasetName
otherwise	O
.	O
We	O
preprocess	O
the	O
input	O
by	O
representing	O
all	O
characters	O
in	O
ASCII	O
and	O
lowercase	O
.	O
We	O
consider	O
a	O
sequence	O
length	O
of	O
32	O
using	O
zero	O
padding	O
.	O
The	O
Siamese	B-MethodName
network	I-MethodName
is	O
trained	O
with	O
contrastive	O
loss	B-MetricName
(	O
Hadsell	O
et	O
al	O
,	O
2006	O
)	O
in	O
order	O
to	O
push	O
the	O
similarity	O
towards	O
1	O
for	O
similar	O
pairs	O
,	O
and	O
below	O
a	O
certain	O
margin	O
(	O
that	O
we	O
set	O
to	O
0	B-DatasetName
)	O
for	O
dissimilar	O
pairs	O
.	O
The	O
optimization	O
is	O
done	O
using	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
,	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
10	O
−3	O
and	O
a	O
gradient	B-MethodName
clipping	I-MethodName
value	O
of	O
5	O
.	O
We	O
use	O
batches	O
of	O
512	O
samples	O
,	O
consider	O
a	O
negative	O
to	O
positive	O
pairs	O
ratio	O
of	O
4	O
:	O
1	O
,	O
and	O
randomly	O
generate	O
new	O
negative	O
pairs	O
at	O
every	O
epoch	O
.	O
At	O
test	O
time	O
,	O
we	O
search	O
for	O
the	O
canonical	O
name	O
whose	O
representation	O
is	O
closest	O
to	O
that	O
of	O
the	O
query	O
,	O
using	O
only	O
the	O
high	O
-	O
quality	O
name	O
entities	O
from	O
DBpedia	B-DatasetName
,	O
BnF	O
,	O
and	O
JRC	O
-	O
names	O
.	O
To	O
this	O
end	O
,	O
we	O
do	O
approximate	O
nearest	O
neighbor	O
search	O
using	O
Annoy	O
7	O
.	O

We	O
use	O
a	O
generative	O
model	O
to	O
correct	O
and	O
normalize	O
authors	O
'	O
names	O
directly	O
.	O
The	O
dataset	O
of	O
name	O
entities	O
is	O
again	O
employed	O
to	O
train	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
(	O
seq2seq	B-MethodName
)	O
model	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
to	O
produce	O
the	O
canonical	O
form	O
of	O
a	O
name	O
from	O
one	O
of	O
its	O
variants	O
.	O
The	O
dataset	O
is	O
further	O
augmented	O
by	O
including	O
additional	O
variants	O
where	O
the	O
first	O
name	O
is	O
abbreviated	O
to	O
an	O
initial	O
.	O
The	O
seq2seq	B-MethodName
model	O
is	O
an	O
encoder	O
-	O
decoder	O
using	O
RNNs	O
,	O
with	O
a	O
character	O
embedding	O
layer	O
,	O
as	O
in	O
the	O
case	O
of	O
the	O
Siamese	B-MethodName
network	I-MethodName
.	O
The	O
encoder	O
is	O
a	O
bi	O
-	O
directional	O
LSTM	B-MethodName
with	O
2	O
×	O
256	O
units	O
,	O
while	O
the	O
decoder	O
is	O
a	O
plain	O
LSTM	B-MethodName
with	O
512	O
units	O
connected	O
to	O
a	O
softmax	B-MethodName
layer	O
that	O
computes	O
a	O
probability	O
distribution	O
over	O
the	O
characters	O
.	O
The	O
training	O
is	O
performed	O
by	O
minimizing	O
the	O
categorical	O
cross	O
-	O
entropy	O
loss	B-MetricName
,	O
using	O
teacher	O
forcing	O
(	O
Williams	O
and	O
Zipser	O
,	O
1989	O
)	O
.	O
The	O
optimization	O
setting	O
is	O
identical	O
to	O
that	O
of	O
the	O
Siamese	O
nework	O
,	O
with	O
batches	O
of	O
1024	O
samples	O
.	O
For	O
inference	O
,	O
we	O
collect	O
the	O
10	O
output	O
sequences	O
with	O
highest	O
probability	O
using	O
beam	O
search	O
.	O

The	O
three	O
machine	O
learning	O
components	O
discussed	O
in	O
the	O
previous	O
section	O
have	O
been	O
individually	O
evaluated	O
on	O
their	O
specific	O
task	O
.	O
Furthermore	O
the	O
final	O
system	O
has	O
been	O
evaluated	O
in	O
terms	O
of	O
correctly	O
normalized	O
book	O
authors	O
in	O
a	O
real	O
case	O
scenario	O
.	O
Siamese	O
approximate	O
name	O
matching	O
We	O
evaluate	O
the	O
Siamese	B-MethodName
network	I-MethodName
on	O
a	O
held	O
out	O
test	O
set	O
,	O
and	O
compare	O
it	O
to	O
an	O
n	O
-	O
gram	O
distance	O
,	O
by	O
checking	O
that	O
the	O
nearest	O
neighbor	O
of	O
a	O
name	O
variant	O
is	O
the	O
canonical	O
name	O
of	O
the	O
entity	O
to	O
which	O
it	O
belongs	O
.	O
We	O
find	O
an	O
accuracy	B-MetricName
of	O
79.8	O
%	O
for	O
the	O
Siamese	B-MethodName
network	I-MethodName
,	O
against	O
71.1	O
%	O
for	O
the	O
n	O
-	O
gram	O
baseline	O
with	O
n	O
=	O
3	O
.	O
We	O
have	O
also	O
checked	O
metrics	O
when	O
introducing	O
a	O
threshold	O
distance	O
above	O
which	O
we	O
consider	O
that	O
no	O
matching	O
entity	O
is	O
found	O
,	O
and	O
found	O
systematic	O
improvement	O
over	O
the	O
baseline	O
.	O
In	O
the	O
final	O
system	O
,	O
we	O
set	O
the	O
threshold	O
to	O
infinity	O
.	O
Siamese	O
networks	O
are	O
more	O
effective	O
than	O
simpler	O
rule	O
-	O
based	O
approaches	O
and	O
more	O
specifically	O
they	O
perform	O
better	O
than	O
the	O
n	O
-	O
gram	O
baseline	O
on	O
the	O
following	O
cases	O
:	O
Vittorio	O
Hugo	O
Victor	O
Hugo	O
:	O
capturing	O
name	O
variants	O
in	O
different	O
languages	O
;	O
Bill	O
Shakespeare	O
William	O
Shakespeare	O
:	O
capturing	O
common	O
nicknames	O
Name	O
correction	O
with	O
seq2seq	B-MethodName
networks	O
Similarly	O
to	O
the	O
previous	O
approach	O
,	O
the	O
seq2seq	B-MethodName
network	O
is	O
evaluated	O
on	O
a	O
held	O
out	O
test	O
set	O
by	O
checking	O
that	O
one	O
of	O
the	O
generated	O
name	O
variants	O
is	O
the	O
canonical	O
name	O
of	O
the	O
entity	O
to	O
which	O
it	O
belongs	O
.	O
As	O
expected	O
,	O
name	O
normalization	O
using	O
seq2seq	B-MethodName
network	O
gives	O
poorer	O
performances	O
than	O
approximate	O
matching	O
within	O
a	O
dataset	O
of	O
known	O
authors	O
,	O
but	O
constitutes	O
a	O
complementary	O
approach	O
that	O
is	O
useful	O
in	O
case	O
of	O
formatting	O
issues	O
or	O
incomplete	O
names	O
.	O
This	O
approach	O
alone	O
reaches	O
a	O
top	O
-	O
10	O
accuracy	B-MetricName
of	O
42	O
%	O
on	O
the	O
entire	O
test	O
set	O
,	O
26	O
%	O
on	O
a	O
test	O
set	O
containing	O
only	O
names	O
with	O
initials	O
,	O
and	O
53	O
%	O
on	O
a	O
test	O
set	O
containing	O
only	O
minor	O
spelling	O
mistakes	O
.	O
Some	O
examples	O
where	O
seq2seq	B-MethodName
performs	O
better	O
than	O
the	O
other	O
methods	O
are	O
as	O
follows	O
:	O
V.	O
Hugo	O
Victor	O
Hugo	O
:	O
first	O
name	O
prediction	O
for	O
authors	O
we	O
do	O
n't	O
have	O
in	O
the	O
canonical	O
database	O
;	O
Vicor	O
Hugo	O
Victor	O
Hugo	O
:	O
misspelling	O
correction	O
for	O
authors	O
we	O
do	O
n't	O
have	O
in	O
the	O
canonical	O
database	O
.	O
Ranking	O
of	O
the	O
proposals	O
With	O
a	O
decision	O
threshold	O
of	O
p	O
=	O
0.5	O
,	O
the	O
trained	O
classifier	O
has	O
an	O
accuracy	B-MetricName
of	O
93	O
%	O
for	O
both	O
positive	O
and	O
negative	O
candidates	O
in	O
the	O
test	O
set	O
.	O
The	O
coefficients	O
of	O
the	O
estimator	O
reveal	O
the	O
importance	O
of	O
the	O
features	O
and	O
,	O
thus	O
,	O
of	O
the	O
related	O
components	O
.	O
The	O
three	O
most	O
important	O
contributions	O
are	O
the	O
match	O
with	O
the	O
Siamese	B-MethodName
network	I-MethodName
,	O
the	O
match	O
via	O
ISBN	O
in	O
Babelio	O
,	O
and	O
the	O
similarity	O
with	O
the	O
input	O
catalog	O
name	O
,	O
confirming	O
the	O
relevance	O
of	O
a	O
multi	O
-	O
approach	O
design	O
choice	O
.	O
Global	O
system	O
In	O
order	O
to	O
reflect	O
the	O
actual	O
use	O
of	O
the	O
global	O
system	O
on	O
e	O
-	O
commerce	O
catalog	O
data	O
,	O
the	O
final	O
evaluation	O
is	O
performed	O
at	O
the	O
book	O
level	O
,	O
by	O
considering	O
all	O
the	O
proposals	O
provided	O
by	O
the	O
different	O
components	O
for	O
a	O
given	O
book	O
.	O
The	O
metric	O
used	O
is	O
the	O
top	O
-	O
k	O
accuracy	B-MetricName
on	O
the	O
ranked	O
list	O
of	O
proposals	O
for	O
each	O
book	O
;	O
results	O
are	O
summarized	O
in	O
Table	O
2	O
.	O
We	O
find	O
that	O
72	O
%	O
of	O
the	O
books	O
have	O
the	O
author	O
's	O
name	O
normalized	O
by	O
the	O
highest	O
ranked	O
proposal	O
.	O
Excluding	O
from	O
the	O
evaluation	O
books	O
where	O
the	O
ground	O
truth	O
for	O
the	O
author	O
's	O
name	O
equals	O
the	O
catalog	O
value	O
,	O
this	O
accuracy	B-MetricName
drops	O
to	O
49	O
%	O
.	O
In	O
the	O
case	O
of	O
books	O
without	O
ISBN	O
or	O
that	O
do	O
not	O
match	O
on	O
any	O
of	O
the	O
bibliographic	O
resources	O
,	O
thus	O
relying	O
on	O
machine	O
learning	O
-	O
based	O
components	O
only	O
,	O
we	O
find	O
that	O
50	O
%	O
of	O
the	O
books	O
are	O
normalized	O
by	O
the	O
top	O
proposal	O
.	O
Finally	O
,	O
for	O
the	O
combination	O
of	O
the	O
above	O
two	O
restrictions	O
,	O
we	O
find	O
a	O
top	O
-	O
1	O
accuracy	B-MetricName
of	O
35	O
%	O
.	O

There	O
is	O
a	O
long	O
line	O
of	O
work	O
on	O
author	O
name	O
disambiguation	O
for	O
the	O
case	O
of	O
bibliographic	O
citation	O
records	O
(	O
Hussain	O
and	O
Asghar	O
,	O
2017	O
)	O
.	O
While	O
related	O
,	O
this	O
problem	O
differs	O
from	O
the	O
one	O
of	O
book	O
authors	O
.	O
Indeed	O
,	O
unlike	O
most	O
books	O
,	O
research	O
publications	O
usually	O
have	O
several	O
authors	O
,	O
each	O
of	O
them	O
having	O
published	O
papers	O
with	O
other	O
researchers	O
.	O
The	O
relationships	O
among	O
authors	O
,	O
which	O
can	O
be	O
represented	O
as	O
a	O
graph	O
,	O
may	O
be	O
used	O
to	O
help	O
disambiguate	O
the	O
bibliographic	O
citations	O
.	O
Named	O
entity	B-TaskName
linking	I-TaskName
(	O
Shen	O
et	O
al	O
,	O
2015	O
)	O
,	O
where	O
one	O
aims	O
at	O
determining	O
the	O
identity	O
of	O
entities	O
(	O
such	O
as	O
a	O
person	O
's	O
name	O
)	O
mentioned	O
in	O
text	O
,	O
is	O
another	O
related	O
problem	O
.	O
The	O
crucial	O
difference	O
with	O
the	O
disambiguation	O
of	O
book	O
authors	O
is	O
that	O
entity	B-TaskName
linking	I-TaskName
systems	O
leverage	O
the	O
context	O
of	O
the	O
named	O
entity	O
mention	O
to	O
link	O
unambiguously	O
to	O
an	O
entity	O
in	O
a	O
pre	O
-	O
populated	O
knowledge	O
base	O
.	O
The	O
conformity	O
of	O
truth	O
in	O
web	O
resources	O
is	O
also	O
a	O
related	O
problem	O
,	O
addressed	O
in	O
the	O
literature	O
by	O
TruthFinder	O
(	O
Yin	O
et	O
al	O
,	O
2008	O
)	O
algorithms	O
.	O
Similarly	O
,	O
the	O
proposed	O
global	O
model	O
in	O
which	O
we	O
combine	O
sources	O
learns	O
to	O
some	O
extent	O
the	O
level	O
of	O
trust	O
of	O
the	O
different	O
sources	O
.	O
Unlike	O
our	O
technique	O
,	O
the	O
TruthFinder	O
approach	O
needs	O
to	O
start	O
from	O
a	O
book	O
we	O
can	O
unambiguously	O
identify	O
in	O
several	O
sources	O
and	O
,	O
thus	O
,	O
needs	O
its	O
ISBN	O
.	O
Distance	B-HyperparameterName
metric	I-HyperparameterName
learning	O
with	O
neural	O
networks	O
has	O
been	O
used	O
for	O
merging	O
datasets	O
on	O
names	O
(	O
Srinivas	O
et	O
al	O
,	O
2018	O
)	O
,	O
for	O
normalizing	O
job	O
titles	O
(	O
Neculoiu	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
for	O
the	O
disambiguation	O
of	O
researchers	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Sequence	O
-	O
to	O
-	O
sequence	O
learning	O
has	O
been	O
used	O
for	O
the	O
more	O
general	O
task	O
of	O
text	O
normalization	O
(	O
Sproat	O
and	O
Jaitly	O
,	O
2016	O
)	O
,	O
and	O
for	O
sentence	O
-	O
level	O
grammar	O
error	O
identification	O
(	O
Schmaltz	O
et	O
al	O
,	O
2016	O
)	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
problem	O
of	O
normalization	O
of	O
book	O
authors	O
name	O
has	O
not	O
been	O
tackled	O
in	O
the	O
previous	O
literature	O
,	O
except	O
for	O
a	O
work	O
on	O
named	O
entity	B-TaskName
linking	I-TaskName
for	O
French	O
writers	O
(	O
Frontini	O
et	O
al	O
,	O
2015	O
)	O
.	O

One	O
way	O
to	O
evaluate	O
the	O
robustness	O
of	O
a	O
machine	O
learning	O
model	O
is	O
to	O
search	O
for	O
inputs	O
that	O
produce	O
incorrect	O
outputs	O
.	O
Inputs	O
intentionally	O
designed	O
to	O
fool	O
deep	O
learning	O
models	O
are	O
referred	O
to	O
as	O
adversarial	O
examples	O
(	O
Goodfellow	O
et	O
al	O
,	O
2017	O
)	O
.	O
Adversarial	O
examples	O
have	O
successfully	O
tricked	O
deep	O
neural	O
networks	O
for	O
image	B-TaskName
classification	I-TaskName
:	O
two	O
images	O
that	O
look	O
exactly	O
the	O
same	O
to	O
a	O
human	O
receive	O
⇤	O
*	O
Equal	O
contribution	O
1	O
Our	O
code	O
and	O
datasets	O
are	O
available	O
here	O
.	O
Figure	O
1	O
:	O
An	O
adversarial	O
example	O
generated	O
by	O
TFAD	O
-	O
JUSTED	O
for	O
BERT	B-MethodName
fine	O
-	O
tuned	O
on	O
the	O
Rotten	O
Tomatoes	O
sentiment	B-TaskName
analysis	I-TaskName
dataset	O
.	O
Swapping	O
a	O
single	O
word	O
causes	O
the	O
prediction	O
to	O
change	O
from	O
positive	O
to	O
negative	O
.	O
completely	O
different	O
predictions	O
from	O
the	O
classifier	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
.	O
While	O
applicable	O
in	O
the	O
image	O
case	O
,	O
the	O
idea	O
of	O
an	O
indistinguishable	O
change	O
lacks	O
a	O
clear	O
analog	O
in	O
text	O
.	O
Unlike	O
images	O
,	O
two	O
different	O
sequences	O
of	O
text	O
are	O
never	O
entirely	O
indistinguishable	O
.	O
This	O
raises	O
the	O
question	O
:	O
if	O
indistinguishable	O
perturbations	O
are	O
not	O
possible	O
,	O
what	O
are	O
adversarial	O
examples	O
in	O
text	O
?	O
The	O
literature	O
contains	O
many	O
potential	O
answers	O
to	O
this	O
question	O
,	O
proposing	O
varying	O
definitions	O
for	O
successful	O
adversarial	O
examples	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
.	O
Even	O
attacks	O
with	O
similar	O
definitions	O
of	O
success	O
often	O
measure	O
it	O
in	O
different	O
ways	O
.	O
The	O
lack	O
of	O
a	O
consistent	O
definition	O
and	O
standardized	O
evaluation	O
has	O
hindered	O
the	O
use	O
of	O
adversarial	O
examples	O
to	O
understand	O
and	O
improve	O
NLP	O
models	O
.	O
2	O
Therefore	O
,	O
we	O
propose	O
a	O
unified	O
definition	O
for	O
successful	O
adversarial	O
examples	O
in	O
natural	O
language	O
:	O
perturbations	O
that	O
both	O
fool	O
the	O
model	O
and	O
fulfill	O
a	O
set	O
of	O
linguistic	O
constraints	O
.	O
In	O
Section	O
2	O
,	O
we	O
present	O
four	O
categories	O
of	O
constraints	O
NLP	O
adversarial	O
examples	O
may	O
follow	O
,	O
depending	O
on	O
the	O
context	O
:	O
semantics	O
,	O
grammaticality	O
,	O
overlap	O
,	O
and	O
non	O
-	O
suspicion	O
to	O
human	O
readers	O
.	O
By	O
explicitly	O
laying	O
out	O
categories	O
of	O
constraints	O
adversarial	O
examples	O
may	O
follow	O
,	O
we	O
introduce	O
a	O
shared	O
vocabulary	O
for	O
discussing	O
constraints	O
on	O
adversarial	O
attacks	O
.	O
In	O
Section	O
4	O
,	O
we	O
suggest	O
options	O
for	O
human	O
and	O
automatic	O
evaluation	O
methods	O
for	O
each	O
category	O
.	O
We	O
use	O
these	O
methods	O
to	O
evaluate	O
two	O
SOTA	O
synonym	O
substitution	O
attacks	O
:	O
GENETICATTACK	O
by	O
Alzantot	O
et	O
al	O
(	O
2018	O
)	O
and	O
TEXTFOOLER	O
by	O
Jin	O
et	O
al	O
(	O
2019	O
)	O
.	O
Human	O
surveys	O
show	O
that	O
the	O
perturbed	O
examples	O
often	O
fail	O
to	O
fulfill	O
semantics	O
and	O
non	O
-	O
suspicion	O
constraints	O
.	O
Additionally	O
,	O
a	O
grammar	O
checker	O
detects	O
39	O
%	O
more	O
errors	O
in	O
the	O
perturbed	O
examples	O
than	O
in	O
the	O
original	O
inputs	O
,	O
including	O
many	O
types	O
of	O
errors	O
humans	O
almost	O
never	O
make	O
.	O
In	O
Section	O
5	O
,	O
we	O
produce	O
TFADJUSTED	O
,	O
an	O
attack	O
with	O
the	O
same	O
search	O
process	O
as	O
TEXTFOOLER	O
,	O
but	O
with	O
constraint	O
enforcement	O
tuned	O
to	O
generate	O
higher	O
quality	O
adversarial	O
examples	O
.	O
To	O
enforce	O
semantic	O
preservation	O
,	O
we	O
tighten	O
the	O
thresholds	O
on	O
the	O
cosine	O
similarity	O
between	O
embeddings	O
of	O
swapped	O
words	O
and	O
between	O
the	O
sentence	O
encodings	O
of	O
original	O
and	O
perturbed	O
sentences	O
.	O
To	O
enforce	O
grammaticality	O
,	O
we	O
validate	O
perturbations	O
with	O
a	O
grammar	O
checker	O
.	O
As	O
in	O
TEXTFOOLER	O
,	O
these	O
constraints	O
are	O
applied	O
at	O
each	O
step	O
of	O
the	O
search	O
.	O
Human	O
evaluation	O
shows	O
that	O
TFADJUSTED	O
generates	O
perturbations	O
that	O
better	O
preserve	O
semantics	O
and	O
are	O
less	O
noticeable	O
to	O
human	O
judges	O
.	O
However	O
,	O
with	O
stricter	O
constraints	O
,	O
the	O
attack	O
success	O
rate	O
decreases	O
from	O
over	O
80	O
%	O
to	O
under	O
20	O
%	O
.	O
When	O
used	O
for	O
adversarial	O
training	O
,	O
TEXTFOOLER	O
's	O
examples	O
decreased	O
model	O
accuracy	B-MetricName
,	O
but	O
TFADJUSTED	O
's	O
examples	O
did	O
not	O
.	O
Without	O
a	O
shared	O
vocabulary	O
for	O
discussing	O
constraints	O
,	O
past	O
work	O
has	O
compared	O
the	O
success	O
rate	O
of	O
search	O
methods	O
with	O
differing	O
constraint	O
application	O
techniques	O
.	O
Jin	O
et	O
al	O
(	O
2019	O
)	O
reported	O
a	O
higher	O
attack	O
success	O
rate	O
for	O
TEXTFOOLER	O
than	O
Alzantot	O
et	O
al	O
(	O
2018	O
)	O
did	O
for	O
GENETICATTACK	O
,	O
but	O
it	O
was	O
not	O
clear	O
whether	O
the	O
improvement	O
was	O
due	O
to	O
a	O
better	O
search	O
method	O
3	O
or	O
more	O
lenient	O
constraint	O
application	O
4	O
.	O
In	O
Section	O
6	O
we	O
compare	O
the	O
search	O
methods	O
with	O
constraint	O
application	O
held	O
constant	O
.	O
We	O
find	O
that	O
GENETICATTACK	O
's	O
search	O
method	O
is	O
more	O
successful	O
than	O
TEXTFOOLER	O
's	O
,	O
contrary	O
to	O
the	O
implications	O
of	O
Jin	O
et	O
al	O
(	O
2019	O
)	O
.	O
The	O
five	O
main	O
contributions	O
of	O
this	O
paper	O
are	O
:	O
A	O
definition	O
for	O
constraints	O
on	O
adversarial	O
perturbations	O
in	O
natural	O
language	O
and	O
suggest	O
evaluation	O
methods	O
for	O
each	O
constraint	O
.	O
Constraint	O
evaluations	O
of	O
two	O
SOTA	O
synonymsubstitution	O
attacks	O
,	O
revealing	O
that	O
their	O
perturbations	O
often	O
do	O
not	O
preserve	O
semantics	O
,	O
grammaticality	O
,	O
or	O
non	O
-	O
suspicion	O
.	O
Evidence	O
that	O
by	O
aligning	O
automatic	O
constraint	O
application	O
with	O
human	O
judgment	O
,	O
it	O
is	O
possible	O
for	O
attacks	O
to	O
produce	O
successful	O
,	O
valid	O
adversarial	O
examples	O
.	O
Demonstration	O
that	O
reported	O
differences	O
in	O
attack	O
success	O
between	O
TEXTFOOLER	O
and	O
GENET	B-MethodName
-	O
ICATTACK	O
are	O
the	O
result	O
of	O
more	O
lenient	O
constraint	O
enforcement	O
.	O
Our	O
framework	O
enables	O
fair	O
comparison	O
between	O
attacks	O
,	O
by	O
separating	O
effects	O
of	O
search	O
methods	O
from	O
effects	O
of	O
loosened	O
constraints	O
.	O

Overlap	O
constraints	O
restrict	O
the	O
similarity	O
between	O
x	O
and	O
x	O
adv	O
at	O
the	O
character	O
level	O
.	O
This	O
in	O
-	O
cludes	O
constraints	O
like	O
Levenshtein	O
distance	O
as	O
well	O
as	O
n	O
-	O
gram	O
based	O
measures	O
such	O
as	O
BLEU	B-MetricName
,	O
ME	O
-	O
TEOR	O
and	O
chRF	O
(	O
Papineni	O
et	O
al	O
,	O
2002	O
;	O
Denkowski	O
and	O
Lavie	O
,	O
2014	O
;	O
Popović	O
,	O
2015	O
)	O
.	O
Setting	O
a	O
maximum	O
edit	O
distance	O
is	O
useful	O
when	O
the	O
attacker	O
is	O
willing	O
to	O
introduce	O
misspellings	O
.	O
Additionally	O
,	O
the	O
edit	O
distance	O
constraint	O
is	O
sometimes	O
used	O
when	O
improving	O
the	O
robustness	O
of	O
models	O
.	O
For	O
example	O
,	O
Huang	O
et	O
al	O
(	O
2019	O
)	O
uses	O
Interval	O
Bound	O
Propagation	O
to	O
ensure	O
model	O
robustness	O
to	O
perturbations	O
within	O
some	O
edit	O
distance	O
of	O
the	O
input	O
.	O

Automatic	O
evaluation	O
of	O
semantic	B-TaskName
similarity	I-TaskName
is	O
a	O
well	O
-	O
studied	O
NLP	O
task	O
.	O
The	O
STS	B-DatasetName
Benchmark	I-DatasetName
is	O
used	O
as	O
a	O
common	O
measurement	O
(	O
Cer	O
et	O
al	O
,	O
2017	O
)	O
.	O
Michel	O
et	O
al	O
(	O
2019	O
)	O
explored	O
the	O
use	O
of	O
common	O
evaluation	O
metrics	O
for	O
machine	B-TaskName
translation	I-TaskName
as	O
a	O
proxy	O
for	O
semantic	B-TaskName
similarity	I-TaskName
in	O
the	O
attack	O
setting	O
.	O
While	O
n	O
-	O
gram	O
overlap	O
based	O
approaches	O
are	O
computationally	O
cheap	O
and	O
work	O
well	O
in	O
the	O
machine	B-TaskName
translation	I-TaskName
setting	O
,	O
they	O
do	O
not	O
correlate	O
with	O
human	O
judgment	O
as	O
well	O
as	O
sentence	O
encoders	O
.	O
Some	O
attacks	O
have	O
used	O
sentence	O
encoders	O
to	O
encode	O
two	O
sentences	O
into	O
a	O
pair	O
of	O
fixed	O
-	O
length	O
vectors	O
,	O
then	O
used	O
the	O
cosine	O
distance	O
between	O
the	O
vectors	O
as	O
a	O
proxy	O
for	O
semantic	B-TaskName
similarity	I-TaskName
.	O
TEXTFOOLER	O
uses	O
the	O
Universal	O
Sentence	O
Encoder	O
(	O
USE	B-MethodName
)	O
,	O
which	O
achieved	O
a	O
Pearson	B-MetricName
correlation	I-MetricName
score	O
of	O
0.782	O
on	O
the	O
STS	B-DatasetName
benchmark	I-DatasetName
(	O
Cer	O
et	O
al	O
,	O
2018	O
)	O
.	O
Another	O
option	O
is	O
BERT	B-MethodName
fine	O
-	O
tuned	O
for	O
semantic	B-TaskName
similarity	I-TaskName
,	O
which	O
achieved	O
a	O
score	O
of	O
0.865	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
.	O
Additionally	O
,	O
synonym	O
substitution	O
methods	O
,	O
including	O
TEXTFOOLER	O
and	O
GENETICATTACK	O
,	O
often	O
require	O
that	O
words	O
be	O
substituted	O
only	O
with	O
neighbors	O
in	O
the	O
counter	O
-	O
fitted	O
embedding	O
space	O
,	O
which	O
is	O
designed	O
to	O
push	O
synonyms	O
together	O
and	O
antonyms	O
apart	O
(	O
Mrksic	O
et	O
al	O
,	O
2016	O
)	O
.	O
These	O
automatic	O
metrics	O
of	O
similarity	O
produce	O
a	O
score	O
that	O
represents	O
the	O
similarity	O
between	O
x	O
and	O
x	O
adv	O
.	O
Attacks	O
depend	O
on	O
a	O
minimum	O
threshold	O
value	O
for	O
each	O
metric	O
to	O
determine	O
whether	O
the	O
changes	O
between	O
x	O
and	O
x	O
adv	O
preserve	O
semantics	O
.	O
Human	O
evaluation	O
is	O
needed	O
to	O
find	O
threshold	O
values	O
such	O
that	O
people	O
generally	O
"	O
agree	O
"	O
that	O
semantics	O
is	O
preserved	O
.	O

The	O
simplest	O
way	O
to	O
automatically	O
evaluate	O
grammatical	O
correctness	O
is	O
with	O
a	O
rule	O
-	O
based	O
grammar	O
checker	O
.	O
Free	O
grammar	O
checkers	O
are	O
available	O
online	O
in	O
many	O
languages	O
.	O
One	O
popular	O
checker	O
is	O
LanguageTool	O
,	O
an	O
open	O
-	O
source	O
proofreading	O
tool	O
(	O
Naber	O
,	O
2003	O
)	O
.	O
LanguageTool	O
ships	O
with	O
thousands	O
of	O
human	O
-	O
curated	O
rules	O
for	O
the	O
English	O
language	O
and	O
provides	O
an	O
interface	O
for	O
identifying	O
grammatical	O
errors	O
in	O
sentences	O
.	O
LanguageTool	O
uses	O
rules	O
to	O
detect	O
grammatical	O
errors	O
,	O
statistics	O
to	O
detect	O
uncommon	O
sequences	O
of	O
words	O
,	O
and	O
language	O
model	O
perplexity	B-MetricName
to	O
detect	O
commonly	O
confused	O
words	O
.	O

Automatic	O
evaluation	O
may	O
be	O
used	O
to	O
guess	O
whether	O
or	O
not	O
an	O
adversarial	O
example	O
is	O
suspicious	O
.	O
Models	O
can	O
be	O
trained	O
to	O
classify	O
passages	O
as	O
real	O
or	O
perturbed	O
,	O
just	O
as	O
human	O
judges	O
do	O
.	O
For	O
example	O
,	O
Warstadt	O
et	O
al	O
(	O
2018	O
)	O
trained	O
sentence	O
encoders	O
on	O
a	O
real	O
/	O
fake	O
task	O
as	O
a	O
proxy	O
for	O
evaluation	O
of	O
linguistic	B-TaskName
acceptability	I-TaskName
.	O
Recently	O
,	O
Zellers	O
et	O
al	O
(	O
2019	O
)	O
demonstrated	O
that	O
GROVER	O
,	O
a	O
transformer	O
-	O
based	O
text	B-TaskName
generation	I-TaskName
model	O
,	O
could	O
classify	O
its	O
own	O
generated	O
news	O
articles	O
as	O
human	O
or	O
machine	O
-	O
written	O
with	O
high	O
accuracy	B-MetricName
.	O

We	O
presented	O
a	O
shuffled	O
mix	O
of	O
real	O
and	O
perturbed	O
examples	O
to	O
human	O
judges	O
and	O
asked	O
if	O
they	O
were	O
real	O
or	O
computer	O
-	O
altered	O
.	O
As	O
this	O
is	O
a	O
time	O
-	O
consuming	O
task	O
for	O
long	O
documents	O
,	O
we	O
only	O
evaluated	O
adversarial	O
examples	O
generated	O
by	O
TEXTFOOLER	O
on	O
the	O
sentence	O
-	O
level	O
MR	B-DatasetName
dataset	O
.	O
If	O
all	O
generated	O
examples	O
were	O
non	O
-	O
suspicious	O
,	O
judges	O
would	O
average	O
50	O
%	O
accuracy	B-MetricName
,	O
as	O
they	O
would	O
not	O
be	O
able	O
to	O
distinguish	O
between	O
real	O
and	O
perturbed	O
examples	O
.	O
In	O
this	O
case	O
,	O
judges	O
achieved	O
69.2	O
%	O
accuracy	B-MetricName
.	O

Since	O
all	O
examples	O
produced	O
by	O
TFADJUSTED	O
are	O
checked	O
with	O
LanguageTool	O
,	O
no	O
perturbation	O
can	O
introduce	O
grammatical	O
errors	O
.	O
10	O
Non	O
-	O
suspicion	O
.	O
We	O
repeated	O
the	O
non	O
-	O
suspicion	O
study	O
from	O
Section	O
4.3	O
with	O
the	O
examples	O
generated	O
by	O
TFADJUSTED	O
.	O
Participants	O
were	O
able	O
to	O
guess	O
with	O
58.8	O
%	O
accuracy	B-MetricName
whether	O
inputs	O
were	O
computer	O
-	O
altered	O
.	O
The	O
accuracy	B-MetricName
is	O
over	O
10	O
%	O
lower	O
than	O
the	O
accuracy	B-MetricName
on	O
the	O
examples	O
generated	O
by	O
TEXTFOOLER	O
.	O
Attack	O
success	O
.	O
For	O
each	O
of	O
the	O
three	O
datasets	O
,	O
the	O
attack	O
success	O
rate	O
decreased	O
by	O
at	O
least	O
71	O
percentage	O
points	O
(	O
see	O
last	O
row	O
of	O
Table	O
5	O
)	O
.	O

Quality	O
Examples	O
Using	O
the	O
9	O
,	O
595	O
samples	O
in	O
the	O
MR	B-DatasetName
training	O
set	O
as	O
seed	O
inputs	O
,	O
TEXTFOOLER	O
generated	O
7	O
,	O
382	O
adversarial	O
examples	O
,	O
while	O
TFADJUSTED	O
generated	O
just	O
825	O
.	O
We	O
append	O
each	O
set	O
of	O
adversarial	O
examples	O
to	O
a	O
copy	O
of	O
the	O
original	O
MR	B-DatasetName
training	O
set	O
and	O
fine	O
-	O
tuned	O
a	O
pre	O
-	O
trained	O
BERT	B-MethodName
model	O
for	O
10	O
epochs	O
.	O
Figure	O
2	O
plots	O
the	O
test	O
accuracy	B-MetricName
over	O
10	O
training	O
epochs	O
,	O
averaged	O
over	O
5	O
random	O
seeds	B-DatasetName
per	O
dataset	O
.	O
While	O
neither	O
training	O
method	O
strongly	O
impacts	O
accuracy	B-MetricName
,	O
the	O
augmentation	O
using	O
TFADJUSTED	O
has	O
a	O
better	O
impact	O
than	O
that	O
of	O
TEXTFOOLER	O
.	O
We	O
then	O
re	O
-	O
ran	O
the	O
two	O
attacks	O
using	O
1000	O
examples	O
from	O
the	O
MR	B-DatasetName
test	O
set	O
as	O
seeds	B-DatasetName
.	O
Again	O
averaging	O
over	O
5	O
random	O
seeds	B-DatasetName
,	O
we	O
found	O
no	O
significant	O
change	O
in	O
robustness	O
.	O
That	O
is	O
,	O
models	O
trained	O
on	O
the	O
original	O
MR	B-DatasetName
dataset	O
were	O
approximately	O
as	O
robust	O
as	O
those	O
trained	O
on	O
the	O
datasets	O
augmented	O
with	O
TEXTFOOLER	O
and	O
TFADJUSTED	O
examples	O
.	O
This	O
corroborates	O
the	O
findings	O
of	O
Alzantot	O
et	O
al	O
(	O
2018	O
)	O
and	O
contradicts	O
those	O
of	O
Jin	O
et	O
al	O
(	O
2019	O
)	O
.	O
We	O
include	O
further	O
analysis	O
along	O
with	O
some	O
hypotheses	O
for	O
the	O
discrepancies	O
in	O
adversarial	O
training	O
results	O
in	O
A.4	O
.	O

A	O
case	O
-	O
based	O
reasoning	O
(	O
CBR	O
)	O
system	O
solves	O
a	O
new	O
problem	O
by	O
retrieving	O
'	O
cases	O
'	O
that	O
are	O
similar	O
to	O
the	O
given	O
problem	O
.	O
If	O
such	O
a	O
system	O
can	O
achieve	O
high	O
accuracy	B-MetricName
,	O
it	O
is	O
appealing	O
owing	O
to	O
its	O
simplicity	O
,	O
interpretability	O
,	O
and	O
scalability	O
.	O
In	O
this	O
paper	O
,	O
we	O
demonstrate	O
that	O
such	O
a	O
system	O
is	O
achievable	O
for	O
reasoning	O
in	O
knowledgebases	O
(	O
KBs	O
)	O
.	O
Our	O
approach	O
predicts	O
attributes	O
for	O
an	O
entity	O
by	O
gathering	O
reasoning	O
paths	O
from	O
similar	O
entities	O
in	O
the	O
KB	O
.	O
Our	O
probabilistic	O
model	O
estimates	O
the	O
likelihood	O
that	O
a	O
path	O
is	O
effective	O
at	O
answering	O
a	O
query	O
about	O
the	O
given	O
entity	O
.	O
The	O
parameters	O
of	O
our	O
model	O
can	O
be	O
efficiently	O
computed	O
using	O
simple	O
path	O
statistics	O
and	O
require	O
no	O
iterative	O
optimization	O
.	O
Our	O
model	O
is	O
non	O
-	O
parametric	O
,	O
growing	O
dynamically	O
as	O
new	O
entities	O
and	O
relations	O
are	O
added	O
to	O
the	O
KB	O
.	O
On	O
several	O
benchmark	O
datasets	O
our	O
approach	O
significantly	O
outperforms	O
other	O
rule	O
learning	O
approaches	O
and	O
performs	O
comparably	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
embedding	O
-	O
based	O
approaches	O
.	O
Furthermore	O
,	O
we	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
model	O
in	O
an	O
"	O
open	O
-	O
world	O
"	O
setting	O
where	O
new	O
entities	O
arrive	O
in	O
an	O
online	O
fashion	O
,	O
significantly	O
outperforming	O
state	O
-	O
ofthe	O
-	O
art	O
approaches	O
and	O
nearly	O
matching	O
the	O
best	O
offline	O
method	O
.	O
1	O

We	O
live	O
in	O
an	O
evolving	O
world	O
with	O
a	O
lot	O
of	O
heterogeneity	O
as	O
well	O
as	O
new	O
entities	O
being	O
created	O
continuously	O
.	O
For	O
example	O
,	O
scientific	O
papers	O
and	O
Wikipedia	O
pages	O
describing	O
facts	O
about	O
new	O
entities	O
,	O
are	O
being	O
constantly	O
added	O
(	O
e.g.	O
.	O
These	O
new	O
findings	O
further	O
trigger	O
the	O
inference	O
of	O
newer	O
facts	O
,	O
each	O
with	O
its	O
own	O
diverse	O
reasoning	O
.	O
We	O
are	O
interested	O
in	O
developing	O
such	O
automated	O
reasoning	O
systems	O
for	O
large	O
knowledge	O
-	O
bases	O
(	O
KBs	O
)	O
.	O
In	O
machine	O
learning	O
,	O
non	O
-	O
parametric	O
methods	O
hold	O
the	O
promise	O
of	O
handling	O
evolving	O
data	O
(	O
Cover	O
1	O
Code	O
available	O
at	O
https://github.com/	O
ameyagodbole	O
/	O
Prob	O
-	O
CBR	O
and	O
Hart	O
,	O
1967	O
;	O
Rasmussen	O
,	O
2000	O
)	O
.	O
Most	O
current	O
KG	O
completion	O
models	O
learn	O
low	O
dimensional	O
parametric	O
representation	O
of	O
entities	O
and	O
relations	O
via	O
tensor	O
factorization	O
or	O
sophisticated	O
neural	O
approaches	O
(	O
Nickel	O
et	O
al	O
,	O
2011	O
;	O
Bordes	O
et	O
al	O
,	O
2013	O
;	O
Socher	O
et	O
al	O
,	O
2013	O
;	O
Sun	O
et	O
al	O
,	O
2019	O
;	O
Vashishth	O
et	O
al	O
,	O
2020	O
)	O
.	O
Another	O
line	O
of	O
work	O
learns	O
Hornclause	O
style	O
reasoning	O
rules	O
from	O
the	O
KG	O
and	O
stores	O
them	O
in	O
its	O
parameters	O
Das	O
et	O
al	O
,	O
2018	O
;	O
Minervini	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
these	O
parametric	O
approaches	O
work	O
with	O
a	O
fixed	O
set	O
of	O
entities	O
and	O
it	O
is	O
unclear	O
how	O
these	O
models	O
will	O
adapt	O
to	O
new	O
entities	O
.	O
This	O
paper	O
presents	O
a	O
k	O
-	O
nearest	O
neighbor	O
(	O
KNN	O
)	O
based	O
approach	O
for	O
KG	O
reasoning	O
that	O
is	O
reminiscent	O
of	O
case	O
-	O
based	O
reasoning	O
(	O
CBR	O
)	O
in	O
classical	O
AI	O
.	O
A	O
CBR	O
system	O
solves	O
a	O
new	O
problem	O
by	O
retrieving	O
'	O
cases	O
'	O
that	O
are	O
similar	O
to	O
the	O
given	O
problem	O
,	O
revising	O
the	O
solution	O
to	O
retrieved	O
cases	O
(	O
if	O
necessary	O
)	O
and	O
reusing	O
it	O
for	O
the	O
new	O
problem	O
(	O
Schank	O
,	O
1982	O
;	O
Leake	O
,	O
1996	O
,	O
inter	O
-	O
alia	O
)	O
.	O
For	O
the	O
task	O
of	O
finding	O
a	O
target	O
entity	O
given	O
a	O
source	O
entity	O
and	O
binary	O
KG	O
relation	O
(	O
e.g.	O
(	O
JOHN	O
VON	O
NEU	O
-	O
MAN	O
,	O
PLACE	O
OF	O
DEATH	O
,	O
?	O
)	O
in	O
Figure	O
1	O
)	O
,	O
our	O
approach	O
first	O
retrieves	O
k	O
similar	O
entities	O
(	O
cases	O
)	O
to	O
the	O
query	O
entity	O
.	O
Next	O
,	O
for	O
each	O
retrieved	O
entity	O
,	O
it	O
finds	O
multiple	O
KG	O
paths	O
2	O
(	O
each	O
path	O
is	O
a	O
solution	O
to	O
retrieved	O
cases	O
)	O
to	O
the	O
entity	O
they	O
are	O
connected	O
by	O
the	O
query	O
relation	O
(	O
e.g.	O
paths	O
between	O
(	O
RICHARD	O
FEYNMAN	O
,	O
USA	O
)	O
)	O
.	O
However	O
,	O
one	O
solution	O
seldom	O
works	O
for	O
all	O
queries	O
.	O
For	O
example	O
,	O
even	O
though	O
the	O
path	O
'	O
BORN	O
IN	O
'	O
is	O
predictive	O
of	O
'	O
PLACE	O
OF	O
DEATH	O
'	O
for	O
US	O
-	O
born	O
scientists	O
(	O
figure	O
1	O
)	O
,	O
it	O
does	O
not	O
work	O
for	O
scientists	O
who	O
have	O
immigrated	O
to	O
USA	O
.	O
To	O
handle	O
this	O
,	O
we	O
present	O
a	O
probabilistic	O
CBR	O
approach	O
which	O
learns	O
to	O
weighs	O
paths	O
with	O
respect	O
to	O
an	O
estimate	O
of	O
its	O
prior	O
and	O
its	O
precision	O
,	O
given	O
the	O
query	O
.	O
The	O
prior	O
of	O
a	O
path	O
rep	O
-	O
Figure	O
1	O
:	O
Given	O
the	O
query	O
,	O
(	O
JON	O
VON	O
NEUMANN	O
,	O
PLACE	O
OF	O
DEATH	O
,	O
?	O
)	O
,	O
our	O
model	O
gathers	O
reasoning	O
paths	O
from	O
similar	O
entities	O
such	O
as	O
other	O
scientists	O
.	O
However	O
,	O
not	O
all	O
gathered	O
paths	O
work	O
for	O
a	O
query	O
e.g.	O
the	O
path	O
(	O
'	O
BORN	O
(	O
x	O
,	O
y	O
)	O
'	O
)	O
would	O
not	O
work	O
for	O
VON	O
NEUMANN	O
.	O
This	O
highlights	O
the	O
importance	O
of	O
learning	O
path	O
weights	O
for	O
clusters	O
of	O
similar	O
entities	O
.	O
Even	O
though	O
'	O
BORN	O
IN	O
'	O
could	O
be	O
a	O
reasonable	O
path	O
for	O
predicting	O
PLACE	O
OF	O
DEATH	O
,	O
this	O
does	O
not	O
apply	O
for	O
VON	O
NEUMANN	O
and	O
other	O
scientists	O
in	O
his	O
cluster	O
.	O
The	O
precision	O
parameter	O
of	O
the	O
path	O
given	O
the	O
cluster	O
helps	O
in	O
penalizing	O
the	O
'	O
BORN	O
IN	O
'	O
path	O
.	O
Note	O
that	O
the	O
node	O
USA	O
is	O
repeated	O
twice	O
in	O
the	O
figure	O
to	O
reduce	O
clutter	O
.	O
resents	O
its	O
frequency	O
while	O
the	O
precision	O
represents	O
the	O
likelihood	O
that	O
the	O
path	O
will	O
lead	O
to	O
a	O
correct	O
answer	O
entity	O
.	O
To	O
obtain	O
robust	O
estimates	O
of	O
the	O
path	O
parameters	O
,	O
we	O
cluster	O
similar	O
entities	O
together	O
and	O
compute	O
them	O
by	O
simple	O
count	O
statistics	O
(	O
2.2.3	O
)	O
.	O
Apart	O
from	O
computing	O
these	O
estimates	O
,	O
our	O
method	O
needs	O
no	O
further	O
training	O
.	O
Overall	O
,	O
our	O
simple	O
approach	O
outperforms	O
several	O
recent	O
parametric	O
rule	O
learning	O
methods	O
(	O
Das	O
et	O
al	O
,	O
2018	O
;	O
Minervini	O
et	O
al	O
,	O
2020	O
)	O
and	O
performs	O
competitively	O
with	O
various	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
KG	O
completion	O
approaches	O
(	O
Dettmers	O
et	O
al	O
,	O
2018	O
)	O
on	O
multiple	O
datasets	O
.	O
An	O
advantage	O
of	O
non	O
-	O
parametric	O
models	O
is	O
that	O
it	O
can	O
adapt	O
to	O
growing	O
data	O
by	O
adjusting	O
its	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
.	O
In	O
the	O
same	O
spirit	O
,	O
we	O
show	O
that	O
our	O
model	O
can	O
seamlessly	O
handle	O
an	O
'	O
open	O
-	O
world	O
'	O
setting	O
in	O
which	O
new	O
entities	O
arrive	O
in	O
the	O
KG	O
.	O
This	O
is	O
made	O
possible	O
by	O
several	O
design	O
choices	O
such	O
as	O
(	O
a	O
)	O
representing	O
entities	O
as	O
sparse	O
(	O
non	O
-	O
learned	O
)	O
vector	O
of	O
its	O
relation	O
types	O
(	O
2.2.1	O
)	O
,	O
(	O
b	O
)	O
our	O
use	O
of	O
an	O
online	O
non	O
-	O
parametric	O
hierarchical	O
clustering	O
algorithm	O
(	O
Monath	O
et	O
al	O
,	O
2019	O
)	O
that	O
can	O
efficiently	O
recompute	O
changes	O
in	O
cluster	O
assignments	O
because	O
of	O
the	O
newly	O
added	O
entity	O
(	O
2.3	O
)	O
,	O
(	O
c	O
)	O
and	O
a	O
simple	O
and	O
efficient	O
way	O
of	O
recomputing	O
the	O
prior	O
and	O
precision	O
parameters	O
for	O
paths	O
per	O
cluster	O
(	O
2.2.3	O
)	O
.	O
Current	O
models	O
for	O
KG	O
completion	O
that	O
learn	O
entity	O
representations	O
for	O
a	O
fixed	O
set	O
of	O
entities	O
can	O
not	O
handle	O
the	O
open	O
-	O
world	O
setting	O
.	O
In	O
fact	O
we	O
show	O
that	O
,	O
retraining	O
the	O
models	O
continually	O
with	O
new	O
data	O
leads	O
to	O
severe	O
degradation	O
of	O
the	O
model	O
performance	O
with	O
models	O
forgetting	O
what	O
it	O
had	O
learned	O
before	O
.	O
For	O
example	O
,	O
the	O
performance	O
(	O
MRR	B-MetricName
)	O
of	O
ROTATE	O
model	O
(	O
Sun	O
et	O
al	O
,	O
2019	O
)	O
drops	O
by	O
11	O
points	O
(	O
absolute	O
)	O
on	O
WN18RR	B-DatasetName
in	O
this	O
setting	O
(	O
3.4	O
)	O
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
show	O
that	O
with	O
new	O
data	O
,	O
the	O
performance	O
of	O
our	O
model	O
is	O
consistent	O
as	O
it	O
is	O
able	O
to	O
seamlessly	O
reason	O
with	O
the	O
newly	O
arrived	O
data	O
.	O
Our	O
work	O
is	O
most	O
closely	O
related	O
to	O
a	O
recent	O
concurrent	O
work	O
by	O
Das	O
et	O
al	O
(	O
2020	O
)	O
where	O
they	O
propose	O
a	O
model	O
that	O
gathers	O
paths	O
from	O
entities	O
similar	O
to	O
the	O
query	O
entity	O
.	O
However	O
,	O
Das	O
et	O
al	O
(	O
2020	O
)	O
encourages	O
path	O
that	O
occur	O
frequently	O
in	O
the	O
KG	O
and	O
does	O
not	O
learn	O
to	O
weigh	O
paths	O
differently	O
for	O
queries	O
.	O
This	O
often	O
leads	O
to	O
wrong	O
inference	O
leading	O
to	O
low	O
performance	O
.	O
For	O
example	O
,	O
on	O
the	O
test	O
-	O
II	O
evaluation	O
subset	O
of	O
FB122	B-DatasetName
where	O
all	O
triples	O
can	O
be	O
inferred	O
by	O
logical	O
rules	O
,	O
Das	O
et	O
al	O
(	O
2020	O
)	O
scores	O
quite	O
low	O
(	O
63	O
MRR	B-MetricName
)	O
because	O
of	O
learning	O
incorrect	O
rules	O
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
score	O
significantly	O
higher	O
(	O
94.83	O
MRR	B-MetricName
)	O
demonstrating	O
that	O
we	O
can	O
learn	O
more	O
effective	O
rules	O
.	O
In	O
fact	O
,	O
we	O
consistently	O
and	O
significantly	O
outperform	O
Das	O
et	O
al	O
(	O
2020	O
)	O
on	O
several	O
benchmark	O
datasets	O
.	O
Also	O
,	O
unlike	O
us	O
,	O
they	O
do	O
not	O
test	O
themselves	O
in	O
the	O
challenging	O
open	O
-	O
world	O
setting	O
.	O
The	O
contributions	O
of	O
this	O
paper	O
are	O
as	O
follows	O
:	O
(	O
a	O
)	O
We	O
present	O
a	O
KNN	O
based	O
approach	O
for	O
KG	O
completion	O
that	O
gathers	O
reasoning	O
paths	O
from	O
entities	O
that	O
are	O
similar	O
to	O
the	O
query	O
entity	O
.	O
Following	O
a	O
principled	O
probabilistic	O
approach	O
(	O
2.2	O
)	O
,	O
our	O
model	O
weighs	O
each	O
path	O
by	O
its	O
likelihood	O
of	O
reaching	O
a	O
correct	O
answer	O
which	O
penalizes	O
paths	O
that	O
are	O
spurious	O
in	O
nature	O
.	O
(	O
b	O
)	O
The	O
parameters	O
of	O
our	O
model	O
grow	O
with	O
data	O
and	O
can	O
be	O
estimated	O
efficiently	O
using	O
simple	O
count	O
statistics	O
(	O
2.3	O
)	O
.	O
Apart	O
from	O
this	O
,	O
our	O
approach	O
needs	O
no	O
training	O
.	O
We	O
show	O
that	O
our	O
simple	O
approach	O
significantly	O
outperforms	O
various	O
rule	O
learning	O
methods	O
(	O
Das	O
et	O
al	O
,	O
2018	O
;	O
Minervini	O
et	O
al	O
,	O
2020	O
;	O
Das	O
et	O
al	O
,	O
2020	O
)	O
on	O
many	O
benchmark	O
datasets	O
.	O
(	O
c	O
)	O
We	O
also	O
show	O
that	O
our	O
model	O
can	O
easily	O
handle	O
addition	O
of	O
facts	O
about	O
new	O
entities	O
and	O
is	O
able	O
to	O
seamlessly	O
integrate	O
and	O
reason	O
with	O
the	O
newly	O
added	O
data	O
significantly	O
outperforming	O
parametric	O
embedding	O
based	O
models	O
.	O
2	O
Non	O
-	O
parametric	O
Reasoning	O
in	O
KGs	O

A	O
great	O
benefit	O
of	O
non	O
-	O
parametric	O
models	O
is	O
that	O
it	O
can	O
seamlessly	O
handle	O
growing	O
data	O
by	O
adding	O
new	O
parameters	O
.	O
New	O
entities	O
constantly	O
arrive	O
in	O
the	O
world	O
(	O
e.g.	O
new	O
Wikipedia	O
articles	O
about	O
entities	O
are	O
frequently	O
created	O
)	O
.	O
We	O
consider	O
a	O
setting	O
(	O
Figure	O
2	O
)	O
in	O
which	O
new	O
entities	O
with	O
few	O
facts	O
(	O
edges	O
)	O
about	O
them	O
keep	O
getting	O
added	O
to	O
the	O
KG	O
.	O
This	O
setting	O
is	O
challenging	O
for	O
parametric	O
models	O
(	O
Das	O
et	O
al	O
,	O
2018	O
;	O
Sun	O
et	O
al	O
,	O
2019	O
)	O
as	O
it	O
is	O
unclear	O
how	O
these	O
models	O
can	O
incorporate	O
new	O
entities	O
without	O
retraining	O
from	O
scratch	O
.	O
However	O
,	O
retraining	O
to	O
obtain	O
entity	B-TaskName
embeddings	I-TaskName
on	O
industrial	O
scale	O
KGs	O
might	O
be	O
impractical	O
(	O
e.g.	O
consider	O
Facebook	O
social	O
graph	O
where	O
new	O
users	O
are	O
joining	O
continuously	O
)	O
.	O
Next	O
,	O
we	O
show	O
that	O
our	O
approach	O
can	O
handle	O
this	O
setting	O
efficiently	O
in	O
the	O
following	O
way	O
:	O
(	O
a	O
)	O
Adding	O
/	O
updating	O
entity	O
representations	O
:	O
First	O
we	O
need	O
to	O
create	O
entity	O
representations	O
for	O
the	O
newly	O
arrived	O
entities	O
.	O
Also	O
,	O
for	O
some	O
existing	O
entities	O
for	O
which	O
new	O
edges	O
were	O
added	O
(	O
e.g.	O
BILL	O
GATES	O
,	O
DURHAM	O
,	O
etc	O
.	O
in	O
figure	O
2	O
)	O
,	O
their	O
representations	O
need	O
to	O
be	O
updated	O
.	O
Recall	B-MetricName
,	O
that	O
we	O
represent	O
entities	O
as	O
a	O
sparse	O
vector	O
of	O
its	O
edge	O
types	O
and	O
hence	O
this	O
step	O
is	O
trivial	O
for	O
our	O
approach	O
.	O
(	O
b	O
)	O
Updating	O
cluster	O
assignments	O
:	O
Next	O
the	O
new	O
entities	O
needs	O
to	O
be	O
added	O
to	O
clusters	O
of	O
similar	O
entities	O
.	O
Also	O
,	O
the	O
cluster	O
assingments	O
of	O
entities	O
that	O
got	O
updated	O
can	O
also	O
change	O
as	O
well	O
and	O
their	O
change	O
can	O
further	O
trigger	O
changes	O
to	O
the	O
clustering	O
of	O
other	O
entities	O
.	O
To	O
handle	O
this	O
,	O
one	O
could	O
naively	O
cluster	O
all	O
entities	O
in	O
the	O
KG	O
,	O
however	O
that	O
could	O
be	O
wasteful	O
and	O
time	O
-	O
consuming	O
for	O
large	O
KGs	O
.	O
Instead	O
,	O
we	O
use	O
an	O
online	O
hierarchical	O
clustering	O
algorithm	O
-	O
GRINCH	O
(	O
Monath	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
has	O
shown	O
to	O
perform	O
as	O
well	O
as	O
agglomerative	O
clustering	O
in	O
the	O
online	O
setting	O
.	O
GRINCH	O
observes	O
one	O
entity	O
at	O
a	O
time	O
,	O
placing	O
it	O
next	O
to	O
its	O
nearest	O
neighbor	O
and	O
performing	O
local	O
re	O
-	O
arrangements	O
in	O
the	O
form	O
of	O
rotations	O
of	O
tree	O
nodes	O
and	O
global	O
rearrangments	O
in	O
the	O
form	O
of	O
grafting	O
a	O
subtrees	O
from	O
part	O
of	O
the	O
tree	O
to	O
another	O
.	O
Entities	O
can	O
be	O
deleted	O
from	O
a	O
hierarchy	O
by	O
simply	O
removing	O
the	O
corresponding	O
leaf	O
node	O
.	O
We	O
first	O
use	O
GRINCH	O
to	O
delete	O
the	O
entities	O
whose	O
representations	O
had	O
changed	O
because	O
of	O
the	O
addition	O
of	O
the	O
new	O
node	O
and	O
then	O
incrementally	O
add	O
those	O
entities	O
back	O
along	O
with	O
the	O
newly	O
added	O
entities	O
in	O
the	O
KG	O
.	O
We	O
extract	O
a	O
flat	O
clustering	O
from	O
the	O
hierarchical	O
clustering	O
built	O
(	O
c	O
)	O
Re	O
-	O
estimating	O
new	O
parameters	O
:	O
After	O
reassigning	O
clusters	O
,	O
the	O
final	O
step	O
is	O
to	O
estimate	O
the	O
per	O
-	O
cluster	O
parameters	O
.	O
This	O
computation	O
is	O
efficient	O
as	O
it	O
is	O
clear	O
from	O
equations	O
2	O
and	O
3	O
that	O
the	O
contribution	O
from	O
each	O
entity	O
in	O
a	O
cluster	O
can	O
be	O
computed	O
independently	O
(	O
and	O
hence	O
can	O
be	O
easily	O
parallelized	O
)	O
.	O
However	O
,	O
even	O
for	O
each	O
entity	O
,	O
this	O
computation	O
needs	O
path	O
traversal	O
in	O
the	O
KG	O
which	O
is	O
expensive	O
.	O
We	O
show	O
that	O
we	O
do	O
not	O
have	O
to	O
recompute	O
for	O
all	O
entities	O
in	O
the	O
clusters	O
.	O
Let	O
n	O
denote	O
the	O
maximum	O
length	O
of	O
a	O
reasoning	O
path	O
considered	O
by	O
our	O
model	O
.	O
For	O
every	O
new	O
entity	O
e	O
i	O
added	O
to	O
the	O
KG	O
,	O
we	O
need	O
to	O
recompute	O
statistics	O
for	O
entities	O
that	O
lie	O
within	O
cycles	O
of	O
length	O
up	O
to	O
(	O
n	O
+	O
1	O
)	O
starting	O
from	O
e	O
i	O
.	O
Please	O
refer	O
to	O
appendix	O
(	O
A.4	O
)	O
for	O
a	O
justification	O
of	O
this	O
result	O
.	O

Data	O
.	O
We	O
evaluate	O
on	O
the	O
following	O
KBC	O
datasets	O
:	O
NELL	B-DatasetName
-	O
995	O
,	O
FB122	B-DatasetName
(	O
Guo	O
et	O
al	O
,	O
2016	O
)	O
,	O
WN18RR	B-DatasetName
(	O
Dettmers	O
et	O
al	O
,	O
2018	O
)	O
.	O
FB122	B-DatasetName
is	O
a	O
subset	O
of	O
the	O
dataset	O
derived	O
from	O
Freebase	O
,	O
FB15	O
K	O
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
,	O
containing	O
122	O
relations	O
regarding	O
people	O
,	O
locations	O
,	O
and	O
sports	O
.	O
NELL	B-DatasetName
-	O
995	O
(	O
Xiong	O
et	O
al	O
,	O
2017	O
)	O
a	O
subset	O
of	O
the	O
NELL	B-DatasetName
derived	O
from	O
the	O
995th	O
iteration	O
of	O
the	O
system	O
.	O
WN18RR	B-DatasetName
was	O
created	O
by	O
Dettmers	O
et	O
al	O
(	O
2018	O
)	O
from	O
WN18	B-DatasetName
by	O
removing	O
inverse	O
relation	O
test	O
-	O
leakage	O
.	O
Evaluation	O
metrics	O
.	O
Following	O
previous	O
work	O
,	O
we	O
evaluate	O
our	O
method	O
using	O
HITS@N	O
and	O
mean	O
reciprocal	O
rank	O
(	O
MRR	B-MetricName
)	O
,	O
which	O
are	O
standard	O
metrics	O
for	O
evaluating	O
a	O
ranked	O
list	O
.	O

Figure	O
3	O
reports	O
the	O
result	O
for	O
this	O
task	O
.	O
We	O
report	O
results	O
on	O
the	O
RotatE	B-MethodName
model	O
with	O
randomly	O
initialized	O
embeddings	O
for	O
new	O
entities	O
(	O
RotatE	B-MethodName
)	O
and	O
the	O
model	O
with	O
systematic	O
initialization	O
of	O
new	O
entity	B-TaskName
embeddings	I-TaskName
(	O
RotatE+	O
)	O
.	O
We	O
experiment	O
with	O
m	O
=	O
{	O
10	O
%	O
,	O
30	O
%	O
}	O
of	O
previously	O
seen	O
edges	O
and	O
retrain	O
on	O
them	O
.	O
We	O
find	O
that	O
not	O
including	O
previously	O
seen	O
edges	O
leads	O
to	O
severe	O
degradation	O
of	O
overall	O
performance	O
due	O
to	O
the	O
model	O
forgetting	O
what	O
it	O
had	O
learned	O
in	O
the	O
past	O
.	O
We	O
also	O
report	O
results	O
with	O
freezing	O
the	O
already	O
seen	O
entity	O
representations	O
and	O
only	O
learning	O
representations	O
for	O
new	O
entities	O
(	O
RotatE	B-MethodName
-	O
Freeze	O
)	O
.	O
All	O
models	O
were	O
trained	O
till	O
the	O
validation	O
set	O
(	O
containing	O
both	O
new	O
and	O
old	O
triples	O
)	O
performance	O
stopped	O
improving	O
.	O
For	O
our	O
approach	O
,	O
we	O
also	O
report	O
results	O
for	O
an	O
oracle	O
setting	O
where	O
we	O
re	O
-	O
cluster	O
all	O
entities	O
as	O
new	O
data	O
arrives	O
and	O
re	O
-	O
estimate	O
all	O
parameters	O
from	O
scratch	O
(	O
instead	O
of	O
using	O
GRINCH	O
and	O
recomputing	O
only	O
required	O
parameters	O
(	O
2.3	O
)	O
.	O
For	O
both	O
datasets	O
,	O
the	O
offline	O
-	O
best	O
results	O
were	O
obtained	O
by	O
RotatE	B-MethodName
(	O
47.1	O
for	O
FB122	B-DatasetName
test	O
-	O
I	O
,	O
48	O
for	O
WN18RR	B-DatasetName
)	O
.	O
We	O
report	O
performance	O
on	O
the	O
entire	O
evaluation	O
set	O
(	O
full	O
)	O
and	O
also	O
on	O
the	O
set	O
containing	O
the	O
newly	O
added	O
edges	O
(	O
new	O
)	O
.	O
The	O
main	O
summary	O
of	O
the	O
results	O
are	O
(	O
i	O
)	O
RotatE	B-MethodName
model	O
converges	O
to	O
a	O
much	O
lower	O
performance	O
in	O
the	O
online	O
setting	O
losing	O
at	O
least	O
8	O
MRR	B-MetricName
points	O
in	O
FB122	B-DatasetName
and	O
at	O
least	O
11	O
points	O
in	O
WN18RR	B-DatasetName
.	O
On	O
FB122	B-DatasetName
,	O
we	O
observe	O
that	O
the	O
model	O
prefers	O
to	O
learn	O
new	O
information	O
more	O
by	O
sacrificing	O
previously	O
learned	O
facts	O
(	O
2nd	O
subfigure	O
in	O
figure	O
3	O
)	O
(	O
ii	O
)	O
In	O
the	O
freeze	O
setting	O
,	O
the	O
model	O
performance	O
deteriorates	O
quickly	O
after	O
a	O
certain	O
point	O
indicating	O
saturation	O
,	O
i.e.	O
it	O
becomes	O
hard	O
for	O
the	O
model	O
to	O
learn	O
new	O
information	O
about	O
arriving	O
entities	O
by	O
keeping	O
the	O
parameters	O
of	O
the	O
existing	O
entities	O
fixed	O
.	O
(	O
iii	O
)	O
On	O
the	O
full	O
evaluation	O
,	O
RotatE+	O
performs	O
better	O
than	O
RotatE	B-MethodName
showing	O
that	O
bad	O
initialization	O
deteriorates	O
performance	O
over	O
time	O
,	O
however	O
,	O
there	O
is	O
still	O
a	O
large	O
gap	O
between	O
the	O
best	O
performance	O
(	O
iv	O
)	O
Our	O
approach	O
almost	O
matches	O
our	O
performance	O
in	O
oracle	O
setting	O
indicating	O
the	O
effectiveness	O
of	O
the	O
online	O
clustering	O
and	O
fast	O
parameter	O
approximation	O
.	O
(	O
v	O
)	O
Lastly	O
,	O
we	O
perform	O
closest	O
to	O
the	O
offline	O
best	O
results	O
outperforming	O
all	O
variants	O
of	O
RotatE.	O

Computing	O
Infrastructure	O
:	O
All	O
our	O
experiments	O
were	O
run	O
on	O
a	O
Xeon	O
E5	O
-	O
2680	O
v4	O
@	O
2.40GHz	O
CPU	O
with	O
128	O
GB	O
RAM	B-MethodName
.	O
No	O
GPUs	O
were	O
needed	O
for	O
the	O
experiments	O
.	O
The	O
results	O
on	O
the	O
validation	O
set	O
are	O
reported	O
in	O
table	O
8	O
and	O
avg	O
.	O
of	O
3	O
runs	O
are	O
reported	O
in	O
table	O
9	O
.	O
The	O
NELL	B-DatasetName
-	O
995	O
does	O
not	O
come	O
with	O
a	O
validation	O
set	O
,	O
and	O
therefore	O
we	O
selected	O
3000	O
edges	O
randomly	O
from	O
the	O
full	O
NELL	B-DatasetName
KB	O
.	O
As	O
a	O
result	O
,	O
many	O
of	O
the	O
query	O
relations	O
were	O
different	O
from	O
what	O
was	O
present	O
in	O
the	O
splits	O
of	O
NELL	B-DatasetName
-	O
995	O
and	O
hence	O
is	O
not	O
a	O
good	O
representative	O
.	O
However	O
,	O
we	O
report	O
test	O
results	O
for	O
the	O
best	O
hyper	O
-	O
parameter	O
values	O
that	O
we	O
got	O
on	O
this	O
validation	O
set	O
.	O
The	O
fixed	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
in	O
our	O
model	O
are	O
essentially	O
the	O
sparse	O
non	O
-	O
learned	O
entity	O
vectors	O
(	O
which	O
can	O
be	O
easily	O
stored	O
in	O
COO	O
format	O
without	O
taking	O
much	O
space	O
)	O
.	O
Other	O
than	O
that	O
,	O
our	O
model	O
is	O
non	O
-	O
parametric	O
with	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
tied	O
to	O
the	O
data	O
.	O
For	O
experiments	O
on	O
WN18RR	B-DatasetName
:	O
Inference	O
time	O
:	O
18.9	O
queries	O
/	O
s	O
(	O
total	O
of	O
6268	O
queries	O
)	O
Train	O
time	O
:	O
around	O
20	O
mins	O
.	O
Best	O
Hyper	O
-	O
parameters	O
:	O
-	O
For	O
experiments	O
on	O
FB122	B-DatasetName
:	O
Inference	O
time	O
:	O

The	O
purpose	O
of	O
this	O
step	O
is	O
to	O
aggregate	O
semantic	O
information	O
at	O
the	O
inter	O
-	O
groups	O
level	O
.	O
The	O
forex	O
trade	O
data	O
and	O
the	O
finance	O
news	O
are	O
highly	O
relevant	O
:	O
the	O
trade	O
data	O
represents	O
the	O
history	O
movement	O
of	O
forex	O
,	O
and	O
the	O
finance	O
news	O
represents	O
the	O
environmental	O
variable	O
.	O
So	O
the	O
combination	O
of	O
them	O
can	O
help	O
us	O
model	O
the	O
forex	O
movement	O
better	O
.	O
In	O
a	O
certain	O
input	O
time	O
,	O
news	O
groups	O
have	O
different	O
impacts	O
on	O
forex	O
movement	O
.	O
So	O
we	O
employ	O
the	O
trade	O
data	O
as	O
a	O
query	O
to	O
calculate	O
the	O
attention	O
weights	O
of	O
news	O
groups	O
.	O
Then	O
the	O
weighted	O
sum	O
of	O
news	O
groups	O
and	O
the	O
trade	O
data	O
representation	O
are	O
finally	O
fused	O
to	O
predict	O
the	O
forex	O
movement	O
.	O
For	O
forex	O
trade	O
data	O
y	O
,	O
we	O
apply	O
a	O
3	O
-	O
layer	O
perceptron	O
to	O
access	O
the	O
trade	O
data	O
representation	O
R	O
t	O
,	O
and	O
each	O
layer	O
is	O
a	O
non	O
-	O
linear	O
transform	O
with	O
Relu	B-MethodName
activation	B-HyperparameterName
function	I-HyperparameterName
.	O
Then	O
we	O
calculate	O
the	O
attention	O
weight	O
between	O
R	O
t	O
and	O
G	O
i	O
:	O
g	O
(	O
i	O
)	O
=	O
Relu	B-MethodName
(	O
R	O
t	O
*	O
W	O
a	O
*	O
G	O
i	O
)	O
(	O
5	O
)	O
att	O
i	O
=	O
e	O
g	O
(	O
i	O
)	O
L	O
i=1	O
e	O
g	O
(	O
i	O
)	O
(	O
6	O
)	O
Where	O
att	O
(	O
i	O
)	O
is	O
the	O
i	O
-	O
th	O
news	O
group	O
's	O
attention	O
weight	O
to	O
trade	O
data	O
.	O
Then	O
we	O
sum	O
the	O
news	O
groups	O
representations	O
up	O
to	O
get	O
the	O
final	O
news	O
semantic	O
representation	O
R	O
s	O
:	O
R	O
s	O
=	O
L	O
i=1	O
G	O
i	O
*	O
att	O
i	O
(	O
7	O
)	O
To	O
fuse	O
the	O
news	O
semantic	O
and	O
trade	O
data	O
representations	O
effectively	O
,	O
we	O
choose	O
the	O
fusion	O
function	O
used	O
in	O
Mou	O
et	O
al	O
,	O
2016	O
)	O
to	O
fuse	O
R	O
s	O
and	O
R	O
t	O
and	O
predict	O
the	O
movement	O
:	O
R	O
=	O
[	O
R	O
t	O
;	O
R	O
s	O
;	O
R	O
t	O
−	O
R	O
s	O
;	O
R	O
t	O
R	O
s	O
]	O
(	O
8	O
)	O
p	O
(	O
f	O
|	O
x	O
,	O
y	O
)	O
=	O
softmax	B-MethodName
(	O
W	O
p	O
*	O
R	O
+	O
b	O
p	O
)	O
(	O
9	O
)	O
means	O
element	O
-	O
wise	O
multiplication	O
.	O

The	O
loss	B-MetricName
function	O
of	O
the	O
proposed	O
model	O
includes	O
two	O
parts	O
:	O
the	O
negative	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
training	O
loss	B-MetricName
and	O
the	O
L	O
2	O
regularization	O
item	O
:	O
Loss	O
=	O
−f	O
*	O
log	O
p	O
(	O
f	O
|	O
x	O
,	O
y	O
,	O
θ	B-HyperparameterName
)	O
+	O
λ	O
2	O
θ	B-HyperparameterName
2	O
2	O
(	O
10	O
)	O
θ	B-HyperparameterName
is	O
the	O
model	O
parameters	O
.	O
Experiments	O
show	O
that	O
the	O
performance	O
improves	O
after	O
adding	O
L	O
2	O
regularization	O
.	O
We	O
train	O
three	O
models	O
with	O
different	O
news	O
grouping	O
methods	O
:	O
time	O
,	O
topic	O
and	O
category	O
,	O
and	O
we	O
call	O
them	O
BHAM	O
-	O
Time	O
,	O
BHAM	O
-	O
Topic	O
,	O
BHAM	O
-	O
Category	O
,	O
respectively	O
.	O

The	O
experiment	O
dataset	O
is	O
accessed	O
from	O
the	O
professional	O
finance	O
news	O
providers	O
Reuters	O
2	O
.	O
We	O
collect	O
forex	O
trade	O
data	O
of	O
four	O
major	O
currency	O
pairs	O
(	O
USD	O
-	O
EUR	O
,	O
USD	O
-	O
JPY	O
,	O
USD	O
-	O
RMB	O
,	O
USD	O
-	O
GBP	O
)	O
from	O
2013	O
to	O
2017	O
.	O
We	O
collect	O
the	O
open	O
/	O
close	O
/	O
high	O
/	O
low	O
trade	O
price	O
for	O
each	O
trade	O
minute	O
.	O
As	O
for	O
the	O
finance	O
news	O
data	O
,	O
we	O
collect	O
all	O
the	O
English	O
news	O
happened	O
in	O
trade	O
time	O
released	O
by	O
Reuters	O
and	O
match	O
the	O
news	O
with	O
target	O
currency	O
pairs	O
according	O
to	O
news	O
region	O
.	O
For	O
example	O
,	O
we	O
match	O
USD	O
-	O
EUR	O
with	O
news	O
related	O
to	O
US	O
,	O
Europe	O
or	O
both	O
of	O
them	O
.	O
The	O
raw	O
data	O
contains	O
both	O
news	O
headline	O
and	O
body	O
,	O
and	O
we	O
utilize	O
the	O
headline	O
only	O
since	O
the	O
headline	O
contains	O
the	O
most	O
valuable	O
information	O
and	O
has	O
less	O
noise	O
.	O
The	O
forex	O
movement	O
label	O
f	O
is	O
decided	O
by	O
the	O
comparison	O
of	O
prediction	O
time	O
price	O
and	O
the	O
input	O
window	O
ending	O
price	O
.	O
We	O
design	O
the	O
symbol	O
USD	O
-	O
EUR	O
(	O
20	O
-	O
10	O
)	O
to	O
represent	O
the	O
prediction	O
for	O
the	O
USD	O
-	O
EUR	O
exchange	O
rate	O
with	O
20	O
minutes	O
input	O
time	O
and	O
10	O
minutes	O
prediction	O
delay	O
.	O
To	O
access	O
more	O
data	O
for	O
training	O
,	O
we	O
overlap	O
the	O
input	O
time	O
of	O
samples	O
.	O
For	O
example	O
,	O
when	O
overlap	O
-	O
rate	O
is	O
50	O
%	O
,	O
two	O
consecutive	O
samples	O
'	O
input	O
time	O
will	O
be	O
8:00	O
-	O
8:20	O
am	O
and	O
8:10	O
-	O
8:30	O
am	O
.	O
Then	O
the	O
data	O
samples	O
will	O
be	O
twice	O
as	O
large	O
as	O
no	O
overlap	O
condition	O
(	O
In	O
the	O
USD	O
-	O
EUR	O
(	O
20	O
-	O
10	O
)	O
dataset	O
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
will	O
increase	O
from	O
31k	O
to	O
62k	O
)	O
.	O
We	O
reserve	O
5k	O
samples	O
for	O
developing	O
and	O
5k	O
samples	O
for	O
testing	O
.	O
All	O
the	O
rest	O
of	O
samples	O
are	O
applied	O
for	O
training	O
.	O

We	O
choose	O
the	O
pytorch	O
-	O
pretrained	O
-	O
BERT	B-MethodName
3	O
as	O
BERT	B-MethodName
implement	O
and	O
choose	O
the	O
bert	O
-	O
baseuncased	O
version	O
in	O
which	O
there	O
are	O
12	O
layers	O
,	O
768	O
hidden	O
states	O
and	O
12	O
attention	O
heads	O
in	O
the	O
transformer	O
.	O
We	O
truncate	O
the	O
BERT	B-MethodName
input	O
to	O
256	O
tokens	O
and	O
fine	O
-	O
tune	O
the	O
BERT	B-MethodName
parameters	O
during	O
training	O
.	O
We	O
adopt	O
the	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
optimizer	B-HyperparameterName
with	O
the	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.001	O
.	O
We	O
apply	O
the	O
dropout	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
regularization	O
with	O
the	O
dropout	O
probability	O
of	O
0.2	O
to	O
reduce	O
over	O
-	O
fitting	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
32	O
.	O
The	O
training	O
epoch	O
is	O
60	O
with	O
early	O
stop	O
.	O
The	O
weight	O
of	O
L	O
2	O
regularization	O
is	O
0.015	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
begins	O
to	O
decay	O
after	O
10	O
epoch	O
.	O
The	O
overlap	O
rate	O
of	O
data	O
samples	O
is	O
50	O
%	O
,	O
and	O
the	O
number	O
of	O
selected	O
news	O
in	O
each	O
group	O
is	O
3	O
.	O
When	O
splitting	O
the	O
dataset	O
,	O
we	O
guarantee	O
that	O
the	O
samples	O
in	O
train	O
set	O
are	O
previous	O
to	O
samples	O
in	O
valid	O
set	O
and	O
test	O
set	O
to	O
avoid	O
the	O
possible	O
information	O
leakage	O
.	O
We	O
tune	O
the	O
hyper	O
-	O
parameters	O
on	O
the	O
development	O
set	O
and	O
test	O
model	O
on	O
the	O
test	O
set	O
.	O
The	O
forex	O
prediction	O
is	O
conducted	O
as	O
a	O
binary	O
classification	O
task	O
(	O
up	O
or	O
down	O
)	O
.	O
The	O
evaluation	O
metrics	O
are	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
and	O
Matthews	O
Correlation	O
Coefficient	O
(	O
MCC	O
)	O
.	O
MCC	O
is	O
often	O
reported	O
in	O
stock	O
movement	O
forecast	O
(	O
Xu	O
and	O
Cohen	O
,	O
2018	O
;	O
Ding	O
et	O
al	O
,	O
2016	O
)	O
because	O
it	O
can	O
overcome	O
the	O
data	O
imbalance	O
issue	O
.	O

In	O
this	O
section	O
,	O
we	O
analyze	O
the	O
influence	O
of	O
two	O
crucial	O
time	O
parameters	O
on	O
model	O
performance	O
,	O
which	O
are	O
input	O
time	O
and	O
predic	O
-	O
tion	O
delay	O
.	O
We	O
choose	O
the	O
input	O
time	O
{	O
10	O
,	O
20	O
,	O
30	O
,	O
40	O
,	O
50	O
,	O
60	O
}	O
(	O
minutes	O
)	O
,	O
the	O
prediction	O
delay	O
{	O
5	O
,	O
10	O
,	O
15	O
,	O
20	O
,	O
25	O
,	O
30	O
}	O
(	O
minutes	O
)	O
and	O
experiment	O
all	O
combinations	O
.	O
We	O
take	O
the	O
USD	O
-	O
JPY	O
for	O
example	O
to	O
analyze	O
the	O
time	O
effect	O
of	O
forex	O
trading	O
,	O
and	O
we	O
observe	O
similar	O
results	O
in	O
other	O
currency	O
pairs	O
.	O
The	O
Figure	O
4	O
shows	O
BHAM	O
-	O
Category	O
model	O
's	O
performances	O
(	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
%	O
)	O
on	O
USD	O
-	O
JPY	O
pair	O
under	O
different	O
combinations	O
of	O
input	O
time	O
and	O
prediction	O
delay	O
.	O
We	O
can	O
observe	O
that	O
with	O
the	O
increase	O
of	O
input	O
time	O
from	O
10	O
minutes	O
to	O
40	O
minutes	O
,	O
the	O
model	O
performance	O
improves	O
too	O
.	O
However	O
,	O
when	O
we	O
increase	O
the	O
input	O
time	O
continuously	O
,	O
the	O
model	O
performance	O
begins	O
to	O
decrease	O
.	O
Too	O
less	O
text	O
is	O
not	O
enough	O
to	O
support	O
the	O
prediction	O
,	O
but	O
too	O
many	O
texts	O
may	O
bring	O
much	O
noise	O
.	O
The	O
ideal	O
input	O
time	O
is	O
around	O
40	O
minutes	O
.	O
Besides	O
,	O
at	O
all	O
input	O
time	O
conditions	O
,	O
the	O
model	O
's	O
performances	O
decline	O
with	O
the	O
increase	O
of	O
prediction	O
delay	O
because	O
events	O
happened	O
in	O
the	O
prediction	O
delay	O
time	O
may	O
also	O
influence	O
the	O
forex	O
movement	O
.	O
We	O
can	O
also	O
conclude	O
that	O
forex	O
movement	O
pays	O
more	O
attention	O
to	O
the	O
latest	O
news	O
because	O
when	O
masking	O
the	O
latest	O
news	O
input	O
(	O
such	O
as	O
USD	O
-	O
JPY	O
(	O
40	O
-	O
05	O
)	O
and	O
USD	O
-	O
JPY	O
(	O
30	O
-	O
15	O
)	O
,	O
the	O
latter	O
one	O
can	O
be	O
seen	O
as	O
the	O
former	O
one	O
masking	O
the	O
lastest	O
10	O
minutes	O
input	O
)	O
,	O
the	O
model	O
performance	O
declines	O
obviously	O
at	O
almost	O
all	O
conditions	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
BERT	B-MethodName
-	O
based	O
Hierarchical	O
Aggregation	O
Model	O
to	O
summarize	O
a	O
large	O
amount	O
of	O
finance	O
news	O
for	O
forex	O
movement	O
prediction	O
.	O
Experiments	O
show	O
that	O
our	O
model	O
outperforms	O
all	O
the	O
baselines	O
by	O
a	O
large	O
margin	O
,	O
which	O
proves	O
the	O
effectiveness	O
of	O
the	O
proposed	O
framework	O
.	O
We	O
design	O
three	O
grouping	O
news	O
methods	O
:	O
time	O
,	O
topic	O
and	O
category	O
and	O
experiments	O
show	O
that	O
the	O
category	O
-	O
based	O
method	O
performs	O
best	O
,	O
which	O
shows	O
that	O
the	O
semantic	O
information	O
of	O
forex	O
related	O
news	O
is	O
mostly	O
aggregated	O
by	O
category	O
.	O
Experiments	O
about	O
time	O
effect	O
prove	O
that	O
the	O
proper	O
input	O
time	O
is	O
about	O
40	O
minutes	O
and	O
the	O
prediction	O
accuracy	B-MetricName
declines	O
with	O
the	O
increase	O
of	O
prediction	O
delay	O
.	O
Besides	O
,	O
we	O
analyze	O
the	O
influence	O
of	O
news	O
attributes	O
on	O
forex	O
trading	O
and	O
observe	O
some	O
interesting	O
conclusions	O
:	O
Business	O
Sectors	O
news	O
has	O
the	O
most	O
influence	O
on	O
USD	O
-	O
EUR	O
trading	O
and	O
Politics	O
/	O
International	O
Affairs	O
news	O
effects	O
USD	O
-	O
RMB	O
trading	O
most	O
.	O
Besides	O
,	O
both	O
USD	O
-	O
JPY	O
trading	O
and	O
USD	O
-	O
GBP	O
trading	O
pay	O
most	O
attention	O
to	O
news	O
from	O
US	O
.	O
All	O
these	O
influence	O
patterns	O
can	O
help	O
forex	O
traders	O
handle	O
different	O
news	O
more	O
wisely	O
and	O
make	O
better	O
decisions	O
.	O
To	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
to	O
utilize	O
the	O
advanced	O
NLP	O
pre	O
-	O
train	O
technology	O
in	O
the	O
enormous	O
forex	O
market	O
and	O
the	O
results	O
show	O
the	O
potential	O
of	O
this	O
research	O
area	O
.	O
Promising	O
future	O
studies	O
may	O
include	O
designing	O
more	O
suitable	O
grouping	O
methods	O
or	O
combining	O
news	O
grouping	O
and	O
market	O
predicting	O
in	O
an	O
end2end	O
model	O
.	O

In	O
our	O
first	O
model	O
,	O
we	O
use	O
the	O
lexical	O
translation	O
model	O
and	O
probability	O
function	O
p	O
t	O
in	O
Equation	O
1	O
as	O
the	O
weighting	O
function	O
,	O
which	O
can	O
be	O
learned	O
efficiently	O
off	O
-	O
line	O
using	O
the	O
EM	B-MetricName
algorithm	O
.	O
When	O
attempting	O
to	O
use	O
the	O
SSSP	O
procedure	O
to	O
compute	O
this	O
equation	O
for	O
a	O
given	O
source	O
input	O
x	O
,	O
we	O
immediately	O
have	O
the	O
problem	O
that	O
such	O
a	O
computation	O
requires	O
a	O
complete	O
component	O
representation	O
z	O
(	O
Knight	O
and	O
Al	O
-	O
Onaizan	O
,	O
1998	O
)	O
.	O
We	O
use	O
an	O
approximation	O
1	O
that	O
involves	O
ignoring	O
the	O
normalizer	O
|	O
A	O
|	O
and	O
exploiting	O
the	O
word	O
independence	O
assumption	O
of	O
the	O
model	O
,	O
which	O
allows	O
us	O
to	O
incrementally	O
compute	O
translation	O
scores	O
for	O
individual	O
source	O
words	O
given	O
output	O
translations	O
corresponding	O
to	O
shortest	O
paths	O
during	O
the	O
SSSP	O
search	O
.	O
The	O
full	O
decoding	O
algorithm	O
in	O
shown	O
in	O
Algorithm	O
1	O
,	O
where	O
the	O
red	O
highlights	O
the	O
adjustments	O
made	O
to	O
the	O
standard	O
SSSP	O
search	O
as	O
presented	O
in	O
Cormen	O
et	O
al	O
(	O
2009	O
)	O
.	O
The	O
main	O
modification	O
involves	O
adding	O
a	O
data	O
structure	O
s	O
R	O
|	O
V	O
|	O
×	O
|	O
x	O
|	O
(	O
initialized	O
as	O
0.0	O
|	O
V	O
|	O
×	O
|	O
x	O
|	O
at	O
line	O
2	O
)	O
that	O
stores	O
a	O
running	O
sum	O
of	O
source	O
word	O
scores	O
given	O
the	O
best	O
translations	O
at	O
each	O
node	O
,	O
which	O
can	O
be	O
used	O
for	O
computing	O
the	O
inner	O
sum	O
in	O
Equation	O
1	O
.	O
For	O
example	O
,	O
given	O
an	O
input	O
utterance	O
ceiling	O
function	O
,	O
s	O
6	O
in	O
Figure	O
2	O
contains	O
the	O
independent	O
translation	O
scores	O
for	O
words	O
ceiling	O
and	O
function	O
given	O
the	O
edge	O
label	O
numeric	O
and	O
p	O
t	O
.	O
Later	O
on	O
in	O
the	O
search	O
,	O
these	O
scores	O
are	O
used	O
to	O
compute	O
s	O
7	O
,	O
which	O
will	O
provide	O
translation	O
scores	O
for	O
each	O
word	O
given	O
the	O
edge	O
sequence	O
numeric	O
math	O
.	O
Taking	O
the	O
product	O
over	O
any	O
given	O
s	O
j	O
(	O
as	O
done	O
in	O
line	O
7	O
to	O
get	O
score	O
)	O
will	O
give	O
the	O
probability	O
of	O
the	O
shortest	O
path	O
translation	O
at	O
the	O
particular	O
point	O
j.	O
Here	O
,	O
the	O
transformation	O
into	O
−	O
log	O
space	O
is	O
used	O
to	O
find	O
the	O
minimum	O
incoming	O
path	O
.	O
Standardly	O
,	O
the	O
data	O
structure	O
π	O
can	O
be	O
used	O
to	O
retrieve	O
the	O
shortest	O
path	O
back	O
to	O
the	O
source	O
node	O
b	O
(	O
done	O
via	O
the	O
FINDPATH	O
method	O
)	O
.	O

The	O
second	O
component	O
is	O
a	O
decoder	O
network	O
,	O
which	O
directly	O
computes	O
the	O
conditional	O
distribution	O
p	O
(	O
z	O
|	O
x	O
)	O
as	O
follows	O
:	O
p	O
(	O
z	O
|	O
x	O
)	O
=	O
|	O
z	O
|	O
i=1	O
log	O
p	O
Θ	B-HyperparameterName
(	O
z	O
i	O
|	O
z	O
<	O
i	O
,	O
x	O
)	O
(	O
3	O
)	O
p	O
Θ	B-HyperparameterName
(	O
z	O
i	O
|	O
z	O
<	O
i	O
,	O
x	O
)	O
∼	O
softmax	B-MethodName
(	O
f	O
(	O
Θ	B-HyperparameterName
,	O
z	O
<	O
i	O
,	O
x	O
)	O
)	O
(	O
4	O
)	O
where	O
f	O
is	O
a	O
non	O
-	O
linear	O
function	O
that	O
encodes	O
information	O
about	O
the	O
sequence	O
z	O
<	O
i	O
and	O
the	O
input	O
x	O
given	O
the	O
model	O
parameters	O
Θ.	O
We	O
can	O
think	O
of	O
this	O
model	O
as	O
an	O
ordinary	O
recurrent	O
language	O
model	O
that	O
is	O
additionally	O
conditioned	O
on	O
the	O
input	O
x	O
using	O
information	O
from	O
our	O
encoder	O
.	O
We	O
implement	O
the	O
function	O
f	O
in	O
the	O
following	O
way	O
:	O
f	O
(	O
Θ	B-HyperparameterName
,	O
z	O
<	O
i	O
,	O
x	O
)	O
=	O
W	O
o	O
η	O
i	O
+	O
b	O
o	O
(	O
5	O
)	O
η	O
i	O
=	O
MLP	B-DatasetName
(	O
c	O
i	O
,	O
g	O
i	O
)	O
(	O
6	O
)	O
g	O
i	O
=	O
LSTM	B-MethodName
dec	O
(	O
g	O
i−1	O
,	O
E	O
out	O
z	O
i−1	O
,	O
c	O
i	O
)	O
(	O
7	O
)	O
where	O
MLP	B-DatasetName
is	O
a	O
multi	O
-	O
layer	O
perceptron	O
model	O
with	O
a	O
single	O
hidden	O
layer	O
,	O
E	O
out	O
R	O
|	O
Σ	O
dec	O
|	O
×e	O
is	O
a	O
randomly	O
initialized	O
embedding	O
matrix	O
,	O
g	O
i	O
is	O
the	O
decoder	O
's	O
hidden	O
state	O
at	O
step	O
i	O
,	O
and	O
c	O
i	O
is	O
a	O
contextvector	O
that	O
encodes	O
information	O
about	O
the	O
input	O
x	O
and	O
the	O
encoder	O
annotations	O
.	O
Each	O
context	O
vector	O
c	O
i	O
in	O
turn	O
is	O
a	O
weighted	O
sum	O
of	O
each	O
annotation	O
h	O
j	O
against	O
an	O
attention	O
vector	O
α	B-HyperparameterName
i	O
,	O
j	O
,	O
or	O
c	O
i	O
=	O
|	O
x	O
|	O
j=1	O
α	B-HyperparameterName
i	O
,	O
j	O
h	O
j	O
,	O
which	O
is	O
jointly	O
learned	O
using	O
an	O
additional	O
single	O
layered	O
multi	O
-	O
layer	O
perceptron	O
defined	O
in	O
the	O
following	O
way	O
:	O
α	B-HyperparameterName
i	O
,	O
j	O
∝	O
exp	O
(	O
e	O
i	O
,	O
j	O
)	O
;	O
e	O
i	O
,	O
j	O
=	O
MLP	B-DatasetName
(	O
g	O
i−1	O
,	O
h	O
j	O
)	O
(	O
8	O
)	O
Lexical	O
Bias	O
and	O
Copying	O
In	O
contrast	O
to	O
standard	O
MT	O
tasks	O
,	O
we	O
are	O
dealing	O
with	O
a	O
relatively	O
low	O
-	O
resource	O
setting	O
where	O
the	O
sparseness	O
of	O
the	O
target	O
vocabulary	O
is	O
an	O
issue	O
.	O
For	O
this	O
reason	O
,	O
we	O
experimented	O
with	O
integrating	O
lexical	O
translation	O
scores	O
using	O
a	O
biasing	O
technique	O
from	O
Arthur	O
et	O
al	O
(	O
2016	O
)	O
.	O
Their	O
method	O
is	O
based	O
on	O
the	O
following	O
computation	O
for	O
each	O
token	O
z	O
i	O
:	O
bias	O
i	O
=	O
	O
	O
p	O
t	O
(	O
1	O
:	O
d	O
[	O
V	O
[	O
G	O
]	O
]	O
,	O
d	O
[	O
b	O
]	O
o	O
,	O
π	O
[	O
V	O
[	O
G	O
]	O
]	O
N	O
il	O
2	O
:	O
s	O
[	O
V	O
[	O
G	O
]	O
]	O
N	O
il	O
Path	O
state	O
information	O
3	O
:	O
s	O
[	O
b	O
]	O
InitState	O
(	O
)	O
Initialize	O
source	O
state	O
4	O
:	O
for	O
each	O
vertex	O
u	O
≥	O
b	O
V	O
[	O
G	O
]	O
in	O
sorted	O
order	O
do	O
5	O
:	O
if	O
isinf	O
(	O
d	O
[	O
u	O
]	O
)	O
then	O
continue	O
6	O
:	O
p	O
s	O
[	O
u	O
]	O
Current	O
state	O
at	O
node	O
u	O
,	O
or	O
z	O
<	O
i	O
7	O
:	O
L	O
1	O
[	O
l	O
]	O
arg	O
max	O
(	O
v	O
1	O
,	O
...	O
,	O
v	O
k	O
)	O
Adj	O
[	O
u	O
]	O
softmax	B-MethodName
(	O
f	O
(	O
Θ	B-HyperparameterName
,	O
p	O
,	O
x	O
)	O
)	O

for	O
each	O
vertex	O
and	O
label	O
(	O
v	O
,	O
z	O
)	O
L	O
do	O
9	O
:	O
score	O
−	O
log	O
pΘ	O
(	O
z	O
|	O
p	O
,	O
x	O
)	O
+	O
d	O
[	O
u	O
]	O
10	O
:	O
if	O
d	O
[	O
v	O
]	O
>	O
score	O
then	O
11	O
:	O
d	O
[	O
v	O
]	O
score	O
,	O
π	O
[	O
v	O
]	O
u	O
12	O
:	O
s	O
[	O
v	O
]	O
UpdateState	O
(	O
p	O
,	O
z	O
)	O
13	O
:	O
return	O
FINDPATH	O
(	O
π	O
,	O
|	O
V	O
|	O
,	O
b	O
)	O
The	O
first	O
matrix	O
uses	O
the	O
inverse	O
(	O
p	O
t	O
)	O
of	O
the	O
lexical	O
translation	O
function	O
p	O
t	O
already	O
introduced	O
to	O
compute	O
the	O
probability	O
of	O
each	O
word	O
in	O
the	O
target	O
vocabulary	O
Σ	O
dec	O
(	O
the	O
columns	O
)	O
with	O
each	O
word	O
in	O
the	O
input	O
x	O
(	O
the	O
rows	O
)	O
,	O
which	O
is	O
then	O
weighted	O
by	O
the	O
attention	O
vector	O
from	O
Equation	O
8	O
.	O
bias	O
i	O
is	O
then	O
used	O
to	O
modify	O
Equation	O
5	O
in	O
the	O
following	O
way	O
:	O
f	O
bias	O
(	O
Θ	B-HyperparameterName
,	O
z	O
<	O
i	O
,	O
x	O
)	O
=	O
W	O
o	O
η	O
i	O
+	O
b	O
o	O
+	O
log	O
(	O
bias	O
i	O
+	O
)	O
where	O
is	O
a	O
hyper	O
-	O
parameter	O
that	O
helps	O
to	O
preserve	O
numerical	O
stability	O
and	O
biases	O
more	O
heavily	O
on	O
the	O
lexical	O
model	O
when	O
set	O
lower	O
.	O
We	O
also	O
experiment	O
with	O
the	O
copying	O
mechanism	O
from	O
Jia	O
and	O
Liang	O
(	O
2016	O
)	O
,	O
which	O
works	O
by	O
allowing	O
the	O
decoder	O
to	O
choose	O
from	O
a	O
set	O
of	O
latent	O
actions	O
,	O
a	O
j	O
,	O
that	O
includes	O
writing	O
target	O
words	O
according	O
to	O
Equation	O
5	O
,	O
as	O
done	O
standardly	O
,	O
or	O
copying	O
source	O
words	O
from	O
x	O
,	O
or	O
copy	O
[	O
x	O
i	O
]	O
according	O
to	O
the	O
attention	O
scores	O
in	O
Equation	O
8	O
.	O
A	O
distribution	O
is	O
then	O
computed	O
over	O
these	O
actions	O
using	O
a	O
softmax	B-MethodName
function	O
and	O
particular	O
actions	O
are	O
chosen	O
accordingly	O
during	O
training	O
and	O
decoding	O
.	O

The	O
full	O
decoding	O
procedure	O
is	O
shown	O
in	O
Algorithm	O
2	O
,	O
where	O
the	O
differences	O
with	O
the	O
standard	O
SSSP	O
are	O
again	O
shown	O
in	O
red	O
.	O
We	O
change	O
the	O
data	O
structure	O
s	O
to	O
contain	O
the	O
decoder	O
's	O
RNN	O
state	O
at	O
each	O
node	O
.	O
We	O
also	O
modify	O
the	O
scoring	O
(	O
line	O
7	O
,	O
which	O
uses	O
Equation	O
4	O
)	O
to	O
consider	O
only	O
the	O
top	O
l	O
edges	O
or	O
translations	O
at	O
that	O
point	O
,	O
as	O
opposed	O
to	O
imposing	O
a	O
full	O
search	O
.	O
When	O
l	O
is	O
set	O
to	O
1	O
,	O
for	O
example	O
,	O
the	O
procedure	O
does	O
a	O
greedy	O
search	O
through	O
the	O
graph	O
,	O
whereas	O
when	O
l	O
is	O
large	O
the	O
procedure	O
is	O
closer	O
to	O
a	O
full	O
search	O
.	O
In	O
general	O
terms	O
,	O
the	O
decoder	O
described	O
above	O
works	O
like	O
an	O
ordinary	O
neural	O
decoder	O
with	O
the	O
difference	O
that	O
each	O
decision	O
(	O
i.e.	O
,	O
new	O
target	O
-	O
side	O
word	B-TaskName
translation	I-TaskName
)	O
is	O
constrained	O
(	O
in	O
line	O
7	O
)	O
by	O
the	O
transitions	O
allowed	O
in	O
the	O
underlying	O
graph	O
in	O
order	O
to	O
ensure	O
wellformedness	O
of	O
each	O
component	O
output	O
.	O
Standardly	O
,	O
we	O
optimize	O
these	O
models	O
using	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
with	O
the	O
objective	O
of	O
finding	O
parametersΘ	O
that	O
minimize	O
the	O
negative	O
conditional	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
of	O
the	O
training	O
dataset	O
.	O

For	O
the	O
technical	O
datasets	O
,	O
the	O
goal	O
is	O
to	O
see	O
if	O
our	O
model	O
generates	O
correct	O
signature	O
representations	O
from	O
unobserved	O
descriptions	O
using	O
exact	B-MetricName
match	I-MetricName
.	O
We	O
follow	O
exactly	O
the	O
experimental	O
setup	O
and	O
data	O
splits	O
from	O
Richardson	O
and	O
Kuhn	O
(	O
2017b	O
)	O
,	O
and	O
measure	O
the	O
accuracy	B-MetricName
at	O
1	O
(	O
Acc@1	O
)	O
,	O
accuracy	B-MetricName
in	O
top	O
10	O
(	O
Acc@10	O
)	O
,	O
and	O
MRR	B-MetricName
.	O
For	O
the	O
GeoQuery	O
and	O
Sportscaster	O
experiments	O
,	O
the	O
goal	O
is	O
to	O
see	O
if	O
our	O
models	O
can	O
generate	O
correct	O
meaning	O
representations	O
for	O
unseen	O
input	O
.	O
For	O
GeoQuery	O
,	O
we	O
follow	O
Andreas	O
et	O
al	O
(	O
2013	O
)	O
in	O
evaluating	O
extrinsically	O
by	O
checking	O
that	O
each	O
representation	O
evaluates	O
to	O
the	O
same	O
answer	O
as	O
the	O
gold	O
representation	O
when	O
executed	O
against	O
the	O
Geobase	O
database	O
.	O
For	O
Sportscaster	O
,	O
we	O
evaluate	O
by	O
exact	B-MetricName
match	I-MetricName
to	O
a	O
gold	O
representation	O
.	O
Richardson	O
and	O
Kuhn	O
(	O
2017b	O
,	O
a	O
)	O
,	O
or	O
RK	O

Technical	O
Documentation	O
Results	O
Table	O
1	O
shows	O
the	O
results	O
for	O
Stdlib	O
and	O
Py27	O
.	O
In	O
the	O
monolingual	O
case	O
,	O
we	O
compare	O
against	O
the	O
best	O
performing	O
models	O
in	O
Richardson	O
and	O
Kuhn	O
(	O
2017b	O
,	O
a	O
)	O
.	O
As	O
summarized	O
in	O
Figure	O
3	O
,	O
our	O
experiments	O
show	O
that	O
training	O
polyglot	O
models	O
on	O
multiple	O
datasets	O
can	O
lead	O
to	O
large	O
improvements	O
over	O
training	O
individual	O
models	O
,	O
especially	O
on	O
the	O
Py27	O
datasets	O
where	O
using	O
a	O
polyglot	O
model	O
resulted	O
in	O
a	O
nearly	O
9	O
%	O
average	O
increase	O
in	O
accuracy	B-MetricName
@1	O
.	O
In	O
both	O
cases	O
,	O
however	O
,	O
the	O
best	O
performing	O
lexical	O
models	O
are	O
those	O
trained	O
only	O
on	O
the	O
datasets	O
they	O
are	O
evaluated	O
on	O
,	O
as	O
opposed	O
to	O
training	O
on	O
all	O
datasets	O
(	O
i.e.	O
,	O
+	O
more	O
)	O
.	O
This	O
is	O
surprising	O
given	O
that	O
training	O
on	O
all	O
datasets	O
doubles	O
the	O
size	O
of	O
the	O
training	O
data	O
,	O
and	O
shows	O
that	O
adding	O
more	O
data	O
does	O
not	O
necessarily	O
boost	O
performance	O
when	O
the	O
additional	O
data	O
is	O
from	O
another	O
distribution	O
.	O

Acc@1	O
Acc@10	O
UBL	O
(	O
Kwiatkowski	O
et	O
al	O
,	O
2010	O
)	O
74.2	O
-	O
TreeTrans	O
(	O
Jones	O
et	O
al	O
,	O
2012	O
)	O
76.8	O
-	O
nHT	O
(	O
Susanto	O
and	O
Lu	O
,	O
2017	O
)	O
83	O
.	O
The	O
neural	O
models	O
are	O
strongly	O
outperformed	O
by	O
all	O
other	O
models	O
both	O
in	O
the	O
monolingual	O
and	O
polyglot	O
case	O
(	O
only	O
the	O
latter	O
results	O
shown	O
)	O
,	O
even	O
when	O
lexical	O
biasing	O
is	O
applied	O
.	O
While	O
surprising	O
,	O
this	O
is	O
consistent	O
with	O
other	O
studies	O
on	O
lowresource	O
neural	O
MT	O
(	O
Zoph	O
et	O
al	O
,	O
2016	O
;	O
Östling	O
and	O
Tiedemann	O
,	O
2017	O
)	O
,	O
where	O
datasets	O
of	O
comparable	O
size	O
to	O
ours	O
(	O
e.g.	O
,	O
1	O
million	O
tokens	O
or	O
less	O
)	O
typically	O
fail	O
against	O
classical	O
SMT	O
models	O
.	O
This	O
result	O
has	O
also	O
been	O
found	O
in	O
relation	O
to	O
neural	O
AMR	O
semantic	B-TaskName
parsing	I-TaskName
,	O
where	O
similar	O
issues	O
of	O
sparsity	O
are	O
encountered	O
(	O
Peng	O
et	O
al	O
,	O
2017	O
)	O
.	O
Even	O
by	O
doubling	O
the	O
amount	O
of	O
training	O
data	O
by	O
training	O
on	O
all	O
datasets	O
(	O
results	O
not	O
shown	O
)	O
,	O
this	O
did	O
not	O
improve	O
the	O
accuracy	B-MetricName
,	O
suggesting	O
that	O
much	O
more	O
data	O
is	O
needed	O
(	O
more	O
discussion	O
below	O
)	O
.	O
Beyond	O
increases	O
in	O
accuracy	B-MetricName
,	O
our	O
polyglot	O
models	O
support	O
zero	O
-	O
shot	O
translation	O
as	O
shown	O
in	O
Figure	O
4	O
,	O
which	O
can	O
be	O
used	O
for	O
translating	O
between	O
unobserved	O
language	O
pairs	O
(	O
e.g.	O
,	O
(	O
es	O
,	O
Clojure	O
)	O
,	O
(	O
ru	O
,	O
Haskell	O
)	O
as	O
shown	O
in	O
1	O
-	O
2	O
)	O
,	O
or	O
for	O
finding	O
related	O
functionality	O
across	O
different	O
software	O
projects	O
(	O
as	O
shown	O
in	O
3	O
)	O
.	O
These	O
results	O
were	O
obtained	O
by	O
running	O
our	O
decoder	O
model	O
without	O
specifying	O
the	O
output	O
language	O
.	O
We	O
note	O
,	O
however	O
,	O
that	O
the	O
decoder	O
can	O
be	O
constrained	O
to	O
selectively	O
translate	O
to	O
any	O
specific	O
programming	O
language	O
or	O
project	O
(	O
e.g.	O
,	O
in	O
a	O
QA	O
setting	O
)	O
.	O
Future	O
work	O
will	O
further	O
investigate	O
the	O
decoder	O
's	O
polyglot	O
capabilities	O
,	O
which	O
is	O
currently	O
hard	O
to	O
evaluate	O
since	O
we	O
do	O
not	O
have	O
an	O
annotated	O
set	O
of	O
function	O
equivalences	O
between	O
different	O
APIs	O
.	O

Having	O
results	O
across	O
related	O
SP	O
tasks	O
allows	O
us	O
to	O
reflect	O
on	O
the	O
nature	O
of	O
the	O
main	O
technical	O
documentation	O
task	O
.	O
Consistent	O
with	O
recent	O
findings	O
(	O
Dong	O
and	O
Lapata	O
,	O
2016	O
)	O
,	O
we	O
show	O
that	O
relatively	O
simple	O
neural	O
sequence	O
models	O
are	O
competitive	O
with	O
,	O
and	O
in	O
some	O
cases	O
outperform	O
,	O
traditional	O
grammar	O
-	O
based	O
SP	O
methods	O
on	O
bench	O
-	O
mark	O
SP	O
tasks	O
.	O
However	O
,	O
this	O
result	O
is	O
not	O
observed	O
in	O
our	O
technical	O
documentation	O
task	O
,	O
in	O
part	O
because	O
this	O
problem	O
is	O
much	O
harder	O
for	O
neural	O
learners	O
given	O
the	O
sparseness	O
of	O
the	O
target	O
data	O
and	O
lack	O
of	O
redundancy	O
.	O
For	O
this	O
reason	O
,	O
we	O
believe	O
our	O
datasets	O
provide	O
new	O
challenges	O
for	O
neural	O
-	O
based	O
SP	O
,	O
and	O
serve	O
as	O
a	O
cautionary	O
tale	O
about	O
the	O
scalability	O
and	O
applicability	O
of	O
commonly	O
used	O
neural	O
models	O
to	O
lower	O
-	O
resource	O
SP	O
problems	O
.	O
In	O
general	O
,	O
we	O
believe	O
that	O
focusing	O
on	O
polyglot	O
and	O
mixed	O
language	O
decoding	O
is	O
not	O
only	O
of	O
interest	O
to	O
applications	O
(	O
e.g	O
,	O
mixed	O
language	O
API	O
QA	O
)	O
but	O
also	O
allows	O
for	O
new	O
forms	O
of	O
SP	O
evaluation	O
that	O
are	O
more	O
revealing	O
than	O
only	O
translation	O
accuracy	B-MetricName
.	O
When	O
comparing	O
the	O
accuracy	B-MetricName
of	O
the	O
best	O
monolingual	O
Geo	O
model	O
and	O
the	O
worst	O
performing	O
neural	O
polyglot	O
model	O
,	O
one	O
could	O
mistakingly	O
think	O
that	O
these	O
models	O
have	O
equal	O
abilities	O
,	O
though	O
the	O
polyglot	O
model	O
is	O
much	O
more	O
robust	O
and	O
general	O
.	O
Moving	O
forward	O
,	O
we	O
hope	O
that	O
our	O
work	O
helps	O
to	O
motivate	O
more	O
diverse	O
evaluations	O
of	O
this	O
type	O
.	O

To	O
accompany	O
our	O
text	O
editing	O
task	O
we	O
present	O
a	O
novel	O
dataset	O
of	O
nearly	O
12	O
million	O
sentence	O
-	O
level	O
edits	O
,	O
WikiDocEdits	B-DatasetName
.	O
These	O
edits	O
were	O
extracted	O
from	O
the	O
revision	O
histories	O
in	O
the	O
February	O
1	O
,	O
2020	O
dump	O
of	O
English	O
Wikipedia	O
.	O
3	O
For	O
a	O
given	O
Wikipedia	O
page	O
,	O
a	O
revision	O
consists	O
of	O
a	O
source	O
and	O
target	O
text	O
,	O
corresponding	O
to	O
the	O
old	O
and	O
new	O
versions	O
of	O
the	O
page	O
.	O
Each	O
revision	O
is	O
also	O
accompanied	O
by	O
an	O
editor	O
comment	O
,	O
which	O
we	O
will	O
use	O
as	O
a	O
proxy	O
for	O
the	O
user	O
command	O
.	O
For	O
a	O
given	O
revision	O
,	O
we	O
split	O
the	O
source	O
and	O
target	O
texts	O
into	O
sentences	O
and	O
then	O
attempt	O
to	O
match	O
the	O
sentences	O
between	O
source	O
and	O
target	O
.	O
For	O
efficiency	O
,	O
we	O
only	O
look	O
at	O
a	O
k	O
-	O
sentence	O
neighborhood	O
.	O
Unmatched	O
sentences	O
are	O
candidates	O
for	O
edits	O
.	O
A	O
source	O
sentence	O
s	O
and	O
target	O
sentence	O
t	O
form	O
an	O
edit	O
pair	O
s	O
−	O
t	O
if	O
f	O
(	O
s	O
,	O
t	O
)	O
>	O
,	O
where	O
f	O
is	O
sentencelevel	O
BLEU	B-MetricName
4	O
without	O
smoothing	O
and	O
=	O
0.1	O
in	O
our	O
case	O
.	O
If	O
an	O
unmatched	O
source	O
sentence	O
does	O
not	O
form	O
an	O
edit	O
pair	O
with	O
any	O
target	O
sentence	O
,	O
we	O
consider	O
it	O
to	O
be	O
a	O
sentence	O
deletion	O
.	O
This	O
can	O
also	O
be	O
thought	O
of	O
as	O
matching	O
to	O
an	O
empty	O
sentence	O
.	O
We	O
identify	O
sentence	O
insertions	O
in	O
an	O
analogous	O
manner	O
.	O
Importantly	O
,	O
we	O
only	O
consider	O
revisions	O
that	O
contain	O
a	O
single	O
sentence	O
-	O
level	O
edit	O
.	O
Otherwise	O
,	O
the	O
editor	O
comment	O
that	O
accompanies	O
each	O
revision	O
may	O
only	O
describe	O
one	O
of	O
the	O
possibly	O
many	O
sentence	O
-	O
level	O
edits	O
.	O
See	O
appendix	O
A	O
for	O
a	O
detailed	O
description	O
of	O
our	O
processing	O
pipeline	O
.	O

We	O
formalize	O
our	O
model	O
,	O
which	O
we	O
refer	O
to	O
as	O
Interactive	O
Editor	O
,	O
as	O
a	O
standard	O
auto	O
-	O
regressive	O
sequence	B-MethodName
to	I-MethodName
sequence	I-MethodName
model	O
.	O
Because	O
our	O
data	O
only	O
contains	O
single	O
-	O
sentence	O
edits	O
,	O
we	O
assume	O
that	O
the	O
sentence	O
to	O
be	O
edited	O
in	O
the	O
source	O
document	O
is	O
given	O
as	O
an	O
input	O
to	O
the	O
model	O
.	O
Given	O
a	O
source	O
sentence	O
s	O
D	O
,	O
the	O
context	O
around	O
s	O
,	O
which	O
we	O
will	O
refer	O
to	O
as	O
D	O
by	O
abuse	O
of	O
notation	O
,	O
a	O
user	O
command	O
q	O
,	O
a	O
grounding	O
corpus	O
G	O
,	O
and	O
a	O
candidate	O
target	O
sentence	O
s	O
,	O
the	O
model	O
,	O
f	O
,	O
computes	O
f	O
(	O
s	O
,	O
s	O
,	O
D	O
,	O
q	O
,	O
G	O
)	O
=	O
P	O
(	O
s	O
|	O
s	O
,	O
D	O
,	O
q	O
,	O
G	O
)	O
=	O
i	O
P	O
(	O
s	O
i	O
|	O
s	O
<	O
i	O
,	O
s	O
,	O
D	O
,	O
q	O
,	O
G	O
)	O
,	O
where	O
s	O
<	O
i	O
=	O
{	O
s	O
0	B-DatasetName
,	O
...	O
,	O
s	O
i−1	O
}	O
are	O
the	O
tokens	O
preceding	O
s	O
i	O
in	O
s	O
.	O
We	O
use	O
the	O
same	O
encoder	O
-	O
decoder	O
architecture	O
as	O
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
and	O
initialize	O
our	O
model	O
with	O
pretrained	O
language	O
model	O
weights	O
.	O
The	O
encoder	O
-	O
decoder	O
architecture	O
allows	O
us	O
to	O
perform	O
full	O
attention	O
over	O
the	O
inputs	O
s	O
,	O
D	O
,	O
q	O
,	O
and	O
G	O
,	O
while	O
the	O
decoder	O
allows	O
us	O
to	O
auto	O
-	O
regressively	O
generate	O
s	O
.	O
Meanwhile	O
,	O
initializing	O
with	O
pretrained	O
weights	O
has	O
been	O
shown	O
to	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
many	O
NLP	O
tasks	O
(	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
order	O
to	O
adapt	O
T5	B-MethodName
for	O
our	O
task	O
,	O
we	O
represent	O
all	O
our	O
inputs	O
as	O
sequences	O
of	O
tokens	O
.	O
We	O
then	O
concatenate	O
these	O
sequences	O
together	O
using	O
separator	O
tokens	O
,	O
truncating	O
and	O
padding	O
them	O
to	O
fixed	O
lengths	O
.	O
This	O
is	O
straightforward	O
since	O
all	O
our	O
inputs	O
are	O
text	O
.	O
See	O
fig	O
.	O
2	O
for	O
reference	O
.	O
We	O
also	O
use	O
the	O
standard	O
cross	O
-	O
entropy	O
loss	B-MetricName
to	O
train	O
.	O

We	O
train	O
our	O
model	O
on	O
a	O
subset	O
of	O
∼1	O
,	O
020	O
K	O
edits	O
from	O
WikiDocEdits	B-DatasetName
.	O
We	O
use	O
a	O
training	O
/	O
validation	O
/	O
test	O
split	O
of	O
1	O
,	O
000K/10K/10	O
K	O
edits	O
,	O
and	O
train	O
for	O
3	O
epochs	O
with	O
a	O
fixed	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.0001	O
,	O
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	O
.	O
We	O
use	O
the	O
T5	B-MethodName
-	O
base	O
implementation	O
from	O
Huggingface	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
finetune	O
all	O
weights	O
in	O
the	O
model	O
.	O
We	O
validate	O
every	O
200	O
steps	O
and	O
select	O
the	O
model	O
with	O
the	O
lowest	O
validation	O
loss	B-MetricName
.	O

For	O
inference	O
we	O
use	O
beam	O
search	O
with	O
a	O
beam	O
width	O
of	O
5	O
,	O
and	O
keep	O
the	O
5	O
highest	O
ranked	O
candidates	O
,	O
excluding	O
any	O
generation	O
that	O
parrots	O
the	O
source	O
as	O
this	O
corresponds	O
to	O
making	O
no	O
edits	O
.	O
Metrics	O
We	O
consider	O
several	O
metrics	O
to	O
evaluate	O
our	O
model	O
.	O
One	O
natural	O
metric	O
to	O
consider	O
is	O
BLEU	B-MetricName
(	O
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
)	O
.	O
BLEU	B-MetricName
shows	O
high	O
correlation	O
with	O
human	O
judgement	O
on	O
machine	B-TaskName
translation	I-TaskName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
;	O
Doddington	O
,	O
2002	O
)	O
.	O
While	O
this	O
should	O
not	O
a	O
priori	O
transfer	O
to	O
evaluating	O
different	O
tasks	O
,	O
our	O
task	O
in	O
fact	O
bears	O
a	O
high	O
similarity	O
to	O
machine	B-TaskName
translation	I-TaskName
because	O
of	O
how	O
the	O
output	O
is	O
constrained	O
by	O
the	O
inputs	O
.	O
If	O
,	O
for	O
example	O
,	O
the	O
source	O
sentence	O
in	O
an	O
English	O
to	O
German	O
translation	O
task	O
is	O
"	O
Sally	O
met	O
Lucy	O
"	O
,	O
the	O
German	O
translation	O
must	O
in	O
some	O
way	O
mention	O
Sally	O
and	O
Lucy	O
.	O
Similarly	O
,	O
in	O
our	O
task	O
,	O
if	O
the	O
source	O
sentence	O
is	O
"	O
Barack	O
Obama	O
was	O
the	O
44th	O
President	O
of	O
the	O
United	O
States	O
"	O
,	O
and	O
the	O
command	O
is	O
"	O
add	O
birth	O
date	O
"	O
,	O
the	O
edit	O
must	O
somehow	O
mention	O
a	O
birth	O
date	O
somewhere	O
.	O
Thus	O
,	O
in	O
our	O
setting	O
,	O
BLEU	B-MetricName
makes	O
sense	O
as	O
a	O
metric	O
since	O
in	O
principle	O
a	O
good	O
model	O
output	O
should	O
not	O
deviate	O
too	O
far	O
from	O
the	O
reference	O
.	O
We	O
use	O
macro	O
-	O
averaged	O
Comment	O
added	O
class	O
of	O
'	O
13	O

Entitled	O
"	O
It	O
Feels	O
Like	O
Home	O
(	O
Re	O
Invented	O
)	O
Tour	O
2011	O
"	O
,	O
it	O
included	O
many	O
remakes	O
of	O
Alliage	O
hits	O
as	O
well	O
as	O
some	O
of	O
his	O
newer	O
songs	O
.	O
sentence	O
-	O
level	O
BLEU	B-MetricName
with	O
epsilon	B-HyperparameterName
smoothing	O
and	O
equally	O
weighted	O
n	O
-	O
grams	O
,	O
with	O
n	O
up	O
to	O
4	O
.	O
One	O
issue	O
with	O
BLEU	B-MetricName
is	O
that	O
the	O
source	O
and	O
target	O
sentences	O
in	O
our	O
task	O
are	O
already	O
very	O
similar	O
,	O
so	O
a	O
model	O
that	O
simply	O
parrots	O
back	O
the	O
source	O
sentence	O
could	O
achieve	O
an	O
unduly	O
high	O
score	O
.	O
Therefore	O
,	O
we	O
also	O
evaluate	O
model	O
outputs	O
by	O
comparing	O
the	O
word	O
-	O
level	O
edits	O
made	O
by	O
the	O
model	O
against	O
the	O
reference	O
,	O
where	O
a	O
word	O
-	O
level	O
edit	O
is	O
a	O
tuple	O
of	O
an	O
operation	O
,	O
either	O
insertion	O
or	O
deletion	O
,	O
a	O
position	O
,	O
and	O
a	O
word	O
.	O
For	O
example	O
,	O
in	O
the	O
edit	O
"	O
Barack	O
Obama	O
was	O
the	O
44	O
th	O
President	O
of	O
the	O
United	O
States	O
"	O
−	O
"	O
Barack	O
Obama	O
,	O
born	O
August	O
4	O
th	O
1961	O
,	O
was	O
the	O
44	O
th	O
President	O
of	O
the	O
United	O
States	O
"	O
,	O
the	O
set	O
of	O
word	O
edits	O
would	O
look	O
like	O
{	O
(	O
insert	O
,	O
2	O
,	O
"	O
,	O
"	O
)	O
,	O
(	O
insert	O
,	O
3	O
,	O
"	O
born	O
"	O
)	O
,	O
...	O
}	O
.	O
Now	O
,	O
denote	O
the	O
set	O
of	O
word	O
edits	O
between	O
two	O
sentences	O
a	O
and	O
b	O
as	O
WE	O
(	O
a	O
,	O
b	O
)	O
.	O
Then	O
,	O
with	O
s	O
the	O
source	O
sentence	O
,	O
s	O
the	O
reference	O
target	O
sentence	O
and	O
h	O
the	O
target	O
sentence	O
generated	O
by	O
the	O
model	O
,	O
we	O
compute	O
the	O
precision	O
P	O
WE	O
(	O
s	O
,	O
h	O
,	O
s	O
)	O
=	O
|	O
WE	O
(	O
s	O
,	O
s	O
)	O
∩	O
WE	O
(	O
h	O
,	O
s	O
)	O
|	O
|	O
WE	O
(	O
h	O
,	O
s	O
)	O
|	O
,	O
recall	O
,	O
R	O
WE	O
(	O
s	O
,	O
h	O
,	O
s	O
)	O
=	O
|	O
WE	O
(	O
s	O
,	O
s	O
)	O
∩	O
WE	O
(	O
h	O
,	O
s	O
)	O
|	O
|	O
WE	O
(	O
s	O
,	O
s	O
)	O
|	O
,	O
and	O
F1	B-MetricName
score	I-MetricName
,	O
F	O
1	O
,	O
WE	O
(	O
s	O
,	O
h	O
,	O
s	O
)	O
=	O
2	O
P	O
WE	O
R	O
WE	O
P	O
WE	O
+	O
R	O
WE	O
.	O
Finally	O
,	O
we	O
compute	O
sentence	O
-	O
level	O
accuracy	B-MetricName
,	O
which	O
reports	O
the	O
proportion	O
of	O
edits	O
for	O
which	O
the	O
model	O
output	O
exactly	O
matched	O
the	O
reference	O
.	O
Baselines	O
We	O
use	O
two	O
baselines	O
to	O
compare	O
our	O
model	O
to	O
.	O
First	O
,	O
we	O
consider	O
the	O
parrot	B-MethodName
baseline	O
that	O
simply	O
outputs	O
the	O
source	O
sentence	O
as	O
is	O
.	O
The	O
second	O
baseline	O
attempts	O
to	O
delete	O
the	O
source	O
sentence	O
and	O
replace	O
it	O
with	O
a	O
new	O
sentence	O
.	O
We	O
use	O
a	O
pretrained	O
GPT	B-MethodName
-	O
2	O
model	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
that	O
generates	O
a	O
sentence	O
given	O
the	O
left	O
context	O
.	O

Table	O
5	O
presents	O
our	O
main	O
results	O
.	O
Notice	O
that	O
the	O
parrot	B-MethodName
baseline	O
is	O
able	O
to	O
achieve	O
a	O
considerably	O
high	O
BLEU	B-MetricName
score	I-MetricName
,	O
as	O
expected	O
,	O
while	O
the	O
GPT	B-MethodName
-	O
2	O
baseline	O
surprisingly	O
achieves	O
a	O
high	O
word	O
edit	O
recall	O
score	O
.	O
Our	O
interactive	O
neural	O
editor	O
model	O
is	O
able	O
to	O
beat	O
both	O
baselines	O
across	O
all	O
metrics	O
,	O
as	O
would	O
be	O
expected	O
.	O
Even	O
on	O
a	O
harsh	O
metric	O
like	O
accuracy	B-MetricName
our	O
model	O
achieves	O
a	O
nontrivial	O
score	O
,	O
although	O
we	O
suspect	O
most	O
of	O
the	O
edits	O
that	O
the	O
model	O
gets	O
exactly	O
right	O
are	O
fluency	O
edits	O
.	O
See	O
table	O
6	O
for	O
Comment	O
Added	O
more	O
marriage	O
info	O
.	O

They	O
are	O
more	O
frequent	O
than	O
primary	O
brain	O
tumors	O
.	O
Secondary	O
brain	O
tumors	O
are	O
more	O
frequent	O
than	O
primary	O
brain	O
tumors	O
.	O
a	O
breakdown	O
by	O
edit	O
type	O
,	O
and	O
table	O
4	O
for	O
example	O
model	O
outputs	O
.	O
Ablations	O
The	O
middle	O
rows	O
of	O
Table	O
5	O
show	O
the	O
results	O
for	O
three	O
ablations	O
of	O
our	O
model	O
.	O
The	O
first	O
ablation	O
removes	O
everything	O
but	O
the	O
source	O
sentence	O
s.	O
This	O
is	O
similar	O
to	O
the	O
paraphrase	O
setting	O
(	O
Gupta	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
the	O
editing	O
setting	O
in	O
Faruqui	O
et	O
al	O
(	O
2018	O
)	O
and	O
Yin	O
et	O
al	O
(	O
2018	O
)	O
.	O
We	O
can	O
see	O
that	O
including	O
the	O
context	O
,	O
grounding	O
,	O
and	O
command	O
as	O
additional	O
inputs	O
yields	O
significant	O
improvements	O
over	O
only	O
using	O
the	O
source	O
sentence	O
.	O
We	O
can	O
also	O
see	O
from	O
the	O
second	O
ablation	O
that	O
the	O
commands	O
are	O
a	O
crucial	O
element	O
in	O
the	O
model	O
's	O
performance	O
.	O
This	O
is	O
not	O
surprising	O
since	O
without	O
a	O
command	O
the	O
model	O
must	O
guess	O
what	O
type	O
of	O
edit	O
to	O
make	O
.	O
Similarly	O
,	O
the	O
model	O
without	O
grounding	O
performs	O
considerably	O
worse	O
than	O
the	O
full	O
model	O
,	O
showing	O
that	O
the	O
grounding	O
is	O
equally	O
important	O
as	O
the	O
command	O
.	O
Surprisingly	O
,	O
the	O
last	O
two	O
ablations	O
perform	O
only	O
marginally	O
better	O
than	O
the	O
first	O
,	O
meaning	O
that	O
removing	O
the	O
grounding	O
in	O
addition	O
to	O
the	O
commands	O
,	O
or	O
vice	O
-	O
versa	O
,	O
does	O
not	O
lead	O
to	O
a	O
large	O
drop	O
in	O
performance	O
.	O
This	O
seems	O
to	O
suggest	O
a	O
synergistic	O
effect	O
between	O
the	O
command	O
and	O
the	O
grounding	O
,	O
which	O
makes	O
sense	O
since	O
the	O
model	O
would	O
not	O
know	O
what	O
to	O
do	O
with	O
the	O
grounding	O
without	O
a	O
command	O
,	O
and	O
likewise	O
,	O
the	O
model	O
would	O
not	O
have	O
access	O
to	O
the	O
right	O
information	O
without	O
the	O
grounding	O
,	O
even	O
if	O
it	O
knew	O
what	O
to	O
edit	O
from	O
the	O
command	O
.	O
Breakdown	O
by	O
edit	O
type	O
The	O
results	O
of	O
our	O
full	O
model	O
are	O
broken	O
down	O
by	O
edit	O
intention	O
labels	O
in	O
Table	O
6	O
.	O
The	O
columns	O
report	O
the	O
same	O
metrics	O
as	O
in	O
our	O
main	O
table	O
of	O
results	O
,	O
with	O
the	O
exception	O
of	O
S	O
-	O
BLEU	B-MetricName
,	O
which	O
reports	O
the	O
BLEU	B-MetricName
score	I-MetricName
between	O
the	O
source	O
sentence	O
and	O
target	O
,	O
and	O
the	O
last	O
column	O
,	O
which	O
reports	O
the	O
number	O
of	O
test	O
edits	O
that	O
were	O
classified	O
into	O
each	O
category	O
.	O
With	O
the	O
caveat	O
that	O
intention	O
labels	O
come	O
from	O
an	O
automatic	O
classifier	O
and	O
not	O
human	O
annotation	O
,	O
we	O
can	O
observe	O
that	O
our	O
model	O
has	O
varying	O
performance	O
across	O
different	O
types	O
of	O
edits	O
.	O
The	O
model	O
performs	O
very	O
well	O
on	O
fluency	O
edits	O
,	O
but	O
worse	O
on	O
content	O
edits	O
.	O
This	O
comes	O
at	O
no	O
surprise	O
given	O
that	O
fluency	O
ed	O
-	O
its	O
should	O
be	O
easier	O
as	O
they	O
usually	O
correct	O
minor	O
mistakes	O
,	O
which	O
a	O
language	O
model	O
should	O
be	O
able	O
to	O
detect	O
from	O
pretraining	O
.	O
Content	O
edits	O
,	O
on	O
the	O
other	O
hand	O
,	O
require	O
pulling	O
the	O
correct	O
information	O
from	O
the	O
grounding	O
and	O
incorporating	O
it	O
in	O
the	O
correct	O
manner	O
into	O
the	O
sentence	O
.	O
The	O
S	O
-	O
BLEU	B-MetricName
scores	O
confirm	O
this	O
since	O
the	O
source	O
sentences	O
in	O
the	O
fluency	O
examples	O
are	O
much	O
more	O
similar	O
to	O
the	O
target	O
sentences	O
than	O
for	O
the	O
content	O
edits	O
.	O
In	O
fact	O
,	O
when	O
looking	O
at	O
the	O
absolute	O
improvement	O
of	O
the	O
BLEU	B-MetricName
over	O
the	O
S	O
-	O
BLEU	B-MetricName
scores	O
,	O
the	O
model	O
performs	O
equally	O
well	O
on	O
both	O
types	O
of	O
edits	O
.	O

We	O
also	O
evaluated	O
the	O
overall	O
quality	O
of	O
model	O
outputs	O
.	O
We	O
considered	O
our	O
full	O
model	O
,	O
and	O
our	O
ablated	O
model	O
that	O
only	O
takes	O
the	O
source	O
sentence	O
as	O
input	O
.	O
We	O
also	O
considered	O
showing	O
and	O
hiding	O
the	O
edit	O
commands	O
,	O
for	O
a	O
total	O
of	O
4	O
settings	O
.	O
For	O
a	O
given	O
setting	O
,	O
raters	O
were	O
asked	O
whether	O
they	O
found	O
each	O
of	O
the	O
top	O
3	O
model	O
outputs	O
satisfactory	O
.	O
Table	O
8	O
presents	O
the	O
results	O
for	O
the	O
top	O
model	O
outputs	O
,	O
with	O
bootstrapped	O
pvalues	O
for	O
pairwise	O
comparisons	O
.	O
We	O
use	O
a	O
Bonferroni	O
corrected	O
α	B-HyperparameterName
=	O
0.0125	O
to	O
determine	O
significance	O
.	O
Note	O
that	O
our	O
full	O
model	O
outperforms	O
our	O
ablated	O
model	O
in	O
the	O
first	O
two	O
comparisons	O
.	O
Inter	O
-	O
9	O
The	O
high	O
percentage	O
of	O
Neutral	O
judgments	O
here	O
may	O
be	O
partially	O
attributable	O
to	O
other	O
factors	O
.	O
Majority	O
Neutral	O
judgments	O
are	O
observed	O
for	O
approximately	O
65	O
%	O
of	O
those	O
examples	O
that	O
received	O
at	O
least	O
one	O
Neutral	O
judgment	O
.	O
This	O
suggests	O
many	O
commands	O
may	O
not	O
be	O
readily	O
interpretable	O
to	O
judges	O
.	O
10	O
Appendix	O
E	O
presents	O
some	O
additional	O
automatic	O
metrics	O
to	O
measure	O
the	O
faithfulness	O
of	O
the	O
model	O
to	O
the	O
grounding	O
.	O
estingly	O
,	O
the	O
difference	O
is	O
smaller	O
when	O
the	O
raters	O
are	O
not	O
shown	O
the	O
commands	O
.	O
Additionally	O
,	O
only	O
the	O
ablated	O
model	O
is	O
rated	O
differently	O
depending	O
on	O
whether	O
the	O
commands	O
are	O
shown	O
.	O
This	O
is	O
to	O
be	O
expected	O
since	O
the	O
ablated	O
model	O
is	O
not	O
likely	O
to	O
be	O
faithful	O
to	O
the	O
commands	O
.	O
In	O
addition	O
to	O
reporting	O
the	O
mean	O
scores	O
from	O
the	O
raters	O
,	O
we	O
can	O
also	O
look	O
at	O
the	O
number	O
of	O
examples	O
where	O
at	O
least	O
one	O
of	O
the	O
top	O
model	O
outputs	O
was	O
found	O
satisfactory	O
by	O
human	O
judges	O
(	O
i.e.	O
scored	O
higher	O
than	O
3	O
)	O
.	O
We	O
find	O
that	O
,	O
when	O
showing	O
the	O
edit	O
commands	O
,	O
at	O
least	O
one	O
of	O
the	O
outputs	O
from	O
our	O
full	O
model	O
was	O
satisfactory	O
in	O
85.83	O
%	O
of	O
cases	O
versus	O
60.17	O
%	O
for	O
the	O
ablated	O
model	O
.	O

In	O
addition	O
to	O
human	O
evaluations	O
,	O
we	O
also	O
used	O
automatic	O
metrics	O
to	O
evaluate	O
how	O
faithful	O
our	O
model	O
is	O
to	O
the	O
grounding	O
.	O
BERT	B-MethodName
Recall	B-MetricName
Similarly	O
to	O
the	O
coverage	O
analysis	O
in	O
appendix	O
D	O
,	O
we	O
can	O
use	O
R	O
BERT	B-MethodName
,	O
with	O
the	O
grounding	O
as	O
C	O
,	O
to	O
assess	O
how	O
well	O
each	O
word	O
inserted	O
by	O
the	O
model	O
is	O
supported	O
by	O
the	O
grounding	O
.	O
The	O
only	O
difference	O
is	O
that	O
the	O
model	O
output	O
now	O
replaces	O
the	O
reference	O
target	O
s	O
in	O
the	O
formula	O
for	O
R	O
BERT	B-MethodName
.	O
Table	O
14	O
gives	O
the	O
summary	O
statistics	O
for	O
R	O
BERT	B-MethodName
across	O
our	O
test	O
set	O
,	O
computed	O
on	O
the	O
outputs	O
of	O
our	O
full	O
model	O
,	O
and	O
the	O
ablated	O
model	O
without	O
grounding	O
.	O
Note	O
that	O
we	O
only	O
consider	O
edits	O
where	O
the	O
model	O
makes	O
at	O
least	O
one	O
insertion	O
.	O
The	O
ablated	O
model	O
serves	O
as	O
a	O
baseline	O
to	O
compare	O
the	O
grounded	O
model	O
to	O
.	O
This	O
baseline	O
achieves	O
a	O
high	O
R	O
BERT	B-MethodName
score	O
,	O
likely	O
because	O
of	O
spurious	O
matches	O
with	O
the	O
grounding	O
.	O
Nevertheless	O
,	O
our	O
grounded	O
model	O
is	O
still	O
more	O
faithful	O
to	O
the	O
grounding	O
,	O
as	O
expected	O
.	O
Grounding	O
Usage	O
While	O
R	O
BERT	B-MethodName
attempts	O
to	O
measure	O
how	O
faithful	O
the	O
model	O
is	O
to	O
the	O
grounding	O
(	O
i.e.	O
is	O
the	O
information	O
inserted	O
by	O
the	O
model	O
found	O
in	O
the	O
grounding	O
?	O
)	O
,	O
we	O
can	O
also	O
attempt	O
to	O
measure	O
how	O
much	O
the	O
grounding	O
is	O
used	O
(	O
i.e.	O
how	O
much	O
of	O
the	O
information	O
inserted	O
by	O
the	O
model	O
is	O
only	O
found	O
in	O
the	O
grounding	O
?	O
)	O
.	O
One	O
simple	O
approach	O
is	O
to	O
look	O
at	O
how	O
many	O
words	O
inserted	O
by	O
the	O
model	O
are	O
found	O
in	O
the	O
grounding	O
but	O
not	O
in	O
the	O
rest	O
of	O
the	O
inputs	O
.	O
While	O
this	O
is	O
n't	O
obvious	O
to	O
compute	O
similarities	O
between	O
BERT	B-MethodName
embeddings	O
,	O
we	O
can	O
use	O
exact	O
word	O
matches	O
instead	O
.	O
For	O
the	O
model	O
without	O
grounding	O
we	O
find	O
that	O
in	O
30.48	O
%	O
of	O
edits	O
in	O
the	O
test	O
set	O
(	O
with	O
at	O
least	O
one	O
insertion	O
)	O
,	O
at	O
least	O
one	O
of	O
the	O
words	O
inserted	O
by	O
the	O
model	O
is	O
found	O
in	O
the	O
grounding	O
but	O
not	O
in	O
the	O
rest	O
of	O
the	O
inputs	O
.	O
For	O
the	O
full	O
model	O
,	O
this	O
number	O
increases	O
to	O
48.66	O
%	O
as	O
expected	O
.	O
The	O
ablated	O
model	O
appears	O
to	O
insert	O
words	O
exclusive	O
to	O
the	O
grounding	O
in	O
a	O
high	O
proportion	O
of	O
edits	O
.	O
However	O
,	O
this	O
could	O
be	O
due	O
to	O
fluency	O
edits	O
,	O
where	O
the	O
model	O
might	O
insert	O
a	O
functional	O
word	O
that	O
happens	O
to	O
only	O
appear	O
in	O
the	O
grounding	O
.	O
If	O
we	O
restrict	O
our	O
attention	O
to	O
content	O
edits	O
,	O
as	O
defined	O
in	O
section	O
3.2	O
,	O
the	O
ablated	O
model	O
inserts	O
grounding	O
-	O
exclusive	O
words	O
in	O
only	O
36.85	O
%	O
of	O
edits	O
,	O
and	O
65.40	O
%	O
for	O
the	O
full	O
model	O
.	O

This	O
section	O
describes	O
our	O
pipeline	O
to	O
obtain	O
atomic	O
edits	O
from	O
Wikipedia	O
revisions	O
in	O
more	O
detail	O
.	O
We	O
start	O
by	O
filtering	O
the	O
revisions	O
in	O
the	O
data	O
.	O
In	O
particular	O
,	O
following	O
(	O
Zhang	O
et	O
al	O
,	O
2019b	O
)	O
,	O
we	O
only	O
keep	O
revisions	O
that	O
affect	O
a	O
single	O
section	O
,	O
and	O
we	O
exclude	O
revisions	O
that	O
do	O
not	O
contain	O
an	O
editor	O
comment	O
.	O
We	O
also	O
exclude	O
certain	O
page	O
types	O
like	O
talk	O
or	O
user	O
pages	O
.	O
We	O
then	O
strip	O
the	O
Wikipedia	O
markup	O
in	O
the	O
retrieved	O
text	O
,	O
using	O
the	O
WikiExtractor	O
script	O
(	O
Attardi	O
,	O
2015	O
)	O
.	O
This	O
removes	O
most	O
markup	O
and	O
Wikimedia	O
templates	O
from	O
the	O
text	O
.	O
Because	O
the	O
markup	O
language	O
used	O
on	O
Wikipedia	O
is	O
not	O
completely	O
formalized	O
11	O
,	O
and	O
because	O
malformed	O
markup	O
often	O
appears	O
in	O
intermediate	O
versions	O
of	O
Wikipedia	O
pages	O
,	O
there	O
is	O
no	O
guarantee	O
that	O
we	O
can	O
remove	O
all	O
the	O
markup	O
from	O
the	O
text	O
.	O
We	O
then	O
split	O
each	O
section	O
into	O
sentences	O
using	O
the	O
Punkt	O
sentence	O
tokenizer	O
(	O
Kiss	O
and	O
Strunk	O
,	O
2006	O
)	O
provided	O
in	O
the	O
NLTK	O
python	O
package	O
(	O
Bird	O
et	O
al	O
,	O
2009	O
)	O
.	O
After	O
splitting	O
into	O
sentences	O
,	O
we	O
attempt	O
to	O
match	O
the	O
sentences	O
from	O
the	O
pre	O
-	O
edit	O
(	O
source	O
)	O
document	O
to	O
the	O
sentences	O
in	O
the	O
post	O
-	O
edit	O
(	O
target	O
)	O
document	O
.	O
Unmatched	O
sentences	O
will	O
be	O
candidates	O
for	O
edits	O
.	O
Similarly	O
to	O
(	O
Faruqui	O
et	O
al	O
,	O
2018	O
)	O
,	O
for	O
each	O
sentence	O
s	O
i	O
in	O
the	O
source	O
document	O
,	O
we	O
only	O
look	O
at	O
the	O
target	O
sentences	O
{	O
t	O
i−k	O
,	O
...	O
,	O
t	O
i	O
,	O
...	O
,	O
t	O
i+k	O
}	O
,	O
with	O
k	B-HyperparameterName
=	I-HyperparameterName
20	O
.	O
This	O
avoids	O
the	O
quadratic	O
complexity	O
of	O
looking	O
at	O
all	O
matches	O
.	O
We	O
then	O
filter	O
out	O
revisions	O
that	O
contain	O
more	O
than	O
one	O
sentence	O
-	O
level	O
edit	O
to	O
ensure	O
that	O
the	O
comment	O
is	O
relevant	O
.	O
If	O
there	O
is	O
a	O
single	O
unmatched	O
source	O
,	O
respectively	O
target	O
,	O
sentence	O
,	O
we	O
consider	O
it	O
a	O
sentence	O
deletion	O
,	O
respectively	O
insertion	O
.	O
Because	O
we	O
do	O
not	O
look	O
at	O
all	O
matches	O
between	O
source	O
and	O
target	O
sentences	O
,	O
a	O
sentence	O
may	O
remain	O
unmatched	O
if	O
,	O
in	O
the	O
target	O
document	O
,	O
it	O
was	O
moved	O
more	O
than	O
k	O
sentences	O
away	O
compared	O
to	O
the	O
source	O
document	O
.	O
Thus	O
we	O
only	O
keep	O
a	O
sentence	O
insertion	O
or	O
deletion	O
if	O
the	O
total	O
number	O
of	O
source	O
and	O
target	O
sentences	O
differ	O
by	O
one	O
.	O
If	O
there	O
are	O
both	O
an	O
unmatched	O
source	O
sentence	O
s	O
and	O
target	O
sentence	O
t	O
,	O
we	O
consider	O
them	O
to	O
form	O
an	O
edit	O
s	O
−	O
t	O
if	O
f	O
(	O
s	O
,	O
t	O
)	O
>	O
,	O
where	O
f	O
is	O
the	O
BLEU	B-MetricName
score	I-MetricName
and	O
=	O
0.1	O
.	O
As	O
a	O
final	O
step	O
,	O
we	O
filter	O
out	O
edits	O
that	O
involve	O
sentences	O
with	O
markup	O
punctuation	O
.	O
We	O
have	O

We	O
employ	O
the	O
Transformer	B-MethodName
model	O
implemented	O
in	O
the	O
Sockeye	O
toolkit	O
.	O
The	O
number	O
of	O
layer	O
in	O
both	O
the	O
encoder	O
and	O
decoder	O
is	O
set	O
to	O
N=6	O
,	O
the	O
number	O
of	O
head	O
is	O
set	O
to	O
8	O
,	O
and	O
the	O
number	O
of	O
hidden	O
unit	O
in	O
the	O
feed	O
-	O
forward	O
network	O
is	O
set	O
to	O
1024	O
.	O
We	O
use	O
an	O
embedding	O
size	O
of	O
both	O
the	O
source	O
and	O
target	O
words	O
of	O
512	O
dimension	O
,	O
and	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	O
sentences	O
.	O
The	O
maximum	O
sentence	O
length	O
is	O
set	O
to	O
100	O
tokens	O
with	O
0.1	O
label	B-MethodName
smoothing	I-MethodName
.	O
We	O
apply	O
layer	B-MethodName
normalization	I-MethodName
and	O
add	O
dropout	O
to	O
the	O
embedding	O
and	O
transformer	O
layers	O
with	O
0.1	O
probability	O
.	O
Moreover	O
,	O
we	O
use	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.0002	O
,	O
and	O
save	O
the	O
checkpoint	O
every	O
1500	O
updates	O
.	O
Model	O
training	O
process	O
stops	O
after	O
8	O
checkpoints	O
without	O
improvements	O
on	O
the	O
validation	O
perplexity	B-MetricName
.	O
Following	O
Niu	O
et	O
al	O
(	O
2018a	O
)	O
,	O
we	O
select	O
the	O
4	O
best	O
checkpoint	O
based	O
on	O
the	O
validation	O
perplexity	B-MetricName
values	O
and	O
combine	O
them	O
in	O
a	O
linear	O
ensemble	O
for	O
decoding	O
.	O
Decoding	O
is	O
performed	O
by	O
using	O
beam	O
search	O
with	O
a	O
beam	O
size	O
of	O
5	O
.	O
We	O
evaluate	O
the	O
machine	B-TaskName
translation	I-TaskName
performance	O
by	O
using	O
the	O
case	O
-	O
sensitive	O
BLEU	B-MetricName
score	I-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
with	O
standard	O
tokenization	O
.	O

Table	O
4	O
shows	O
the	O
BLEU	B-MetricName
scores	O
of	O
the	O
general	O
NMT	O
model	O
and	O
baseline	O
NMT	O
model	O
on	O
machine	B-TaskName
translation	I-TaskName
task	O
.	O
We	O
can	O
observe	O
that	O
the	O
baseline	O
NMT	O
model	O
is	O
comparable	O
to	O
the	O
general	O
NMT	O
model	O
,	O
and	O
it	O
achieves	O
the	O
highest	O
BLEU	B-MetricName
scores	O
on	O
almost	O
all	O
the	O
test	O
datasets	O
in	O
both	O
directions	O
,	O
which	O
indicates	O
that	O
the	O
NMT	O
baseline	O
based	O
on	O
our	O
proposed	O
segmentation	O
method	O
is	O
competitive	O
.	O

Table	O
5	O
shows	O
the	O
BLEU	B-MetricName
scores	O
of	O
the	O
baseline	O
NMT	O
model	O
,	O
bi	O
-	O
directional	O
NMT	O
model	O
,	O
and	O
multi	O
-	O
task	O
neural	O
model	O
on	O
the	O
machine	B-TaskName
translation	I-TaskName
task	O
between	O
Turkish	O
and	O
English	O
.	O
The	O
table	O
shows	O
that	O
the	O
multi	O
-	O
task	O
neural	O
model	O
outperforms	O
both	O
the	O
baseline	O
NMT	O
model	O
and	O
bi	O
-	O
directional	O
NMT	O
model	O
,	O
and	O
it	O
achieves	O
the	O
highest	O
BLEU	B-MetricName
scores	O
on	O
almost	O
all	O
the	O
test	O
datasets	O
in	O
both	O
directions	O
,	O
which	O
suggests	O
that	O
the	O
multi	O
-	O
task	O
neural	O
model	O
is	O
capable	O
of	O
improving	O
the	O
bi	O
-	O
directional	O
translation	O
quality	O
on	O
agglutinative	O
languages	O
.	O
The	O
main	O
reason	O
is	O
that	O
compared	O
with	O
the	O
bi	O
-	O
directional	O
NMT	O
model	O
,	O
our	O
proposed	O
multi	O
-	O
task	O
neural	O
model	O
additionally	O
employs	O
the	O
stemming	O
task	O
for	O
the	O
agglutinative	O
language	O
,	O
which	O
is	O
effective	O
for	O
the	O
NMT	O
model	O
to	O
learn	O
both	O
the	O
source	O
-	O
side	O
semantic	O
information	O
and	O
the	O
target	O
-	O
side	O
language	O
modeling	O
.	O

The	O
university	O
was	O
emulating	O
its	O
lives	O
.	O
multi	O
-	O
task	O
The	O
university	O
was	O
imitating	O
life	O
.	O
The	O
function	O
of	O
epochs	O
and	O
perplexity	B-MetricName
values	O
on	O
the	O
validation	B-DatasetName
dataset	I-DatasetName
in	O
different	O
neural	O
translation	O
models	O
are	O
shown	O
in	O
Figure	O
3	O
.	O
We	O
can	O
see	O
that	O
the	O
perplexity	B-MetricName
values	O
are	O
consistently	O
lower	O
on	O
the	O
multi	O
-	O
task	O
neural	O
model	O
,	O
and	O
it	O
converges	O
rapidly	O
.	O
Table	O
6	O
shows	O
a	O
translation	O
example	O
for	O
the	O
different	O
models	O
on	O
Turkish	O
-	O
English	O
.	O
We	O
can	O
see	O
that	O
the	O
translation	O
result	O
of	O
the	O
multi	O
-	O
task	O
neural	O
model	O
is	O
more	O
accurate	O
.	O
The	O
Turkish	O
word	O
"	O
taklit	O
"	O
means	O
"	O
imitate	O
"	O
in	O
English	O
,	O
both	O
the	O
baseline	O
NMT	O
and	O
bi	O
-	O
directional	O
NMT	O
translate	O
it	O
into	O
a	O
synonym	O
"	O
emulate	O
"	O
.	O
However	O
,	O
they	O
are	O
not	O
able	O
to	O
express	O
the	O
meaning	O
of	O
the	O
sentence	O
correctly	O
.	O
The	O
main	O
reason	O
is	O
that	O
the	O
auxiliary	O
task	O
of	O
stemming	O
forces	O
the	O
proposed	O
model	O
to	O
focus	O
more	O
strongly	O
on	O
the	O
core	O
meaning	O
of	O
each	O
word	O
(	O
or	O
stem	O
)	O
,	O
therefore	O
helping	O
the	O
model	O
make	O
the	O
correct	O
lexical	O
choices	O
and	O
capture	O
the	O
in	O
-	O
depth	O
semantic	O
information	O
.	O

Moreover	O
,	O
we	O
evaluate	O
the	O
multi	O
-	O
task	O
neural	O
model	O
on	O
using	O
external	O
monolingual	O
data	O
for	O
Turkish	O
stemming	O
task	O
.	O
We	O
employ	O
the	O
parallel	O
sentences	O
and	O
the	O
monolingual	O
data	O
in	O
a	O
1	O
-	O
1	O
ratio	O
,	O
and	O
shuffle	O
them	O
randomly	O
before	O
each	O
training	O
epoch	O
.	O
More	O
details	O
about	O
the	O
data	O
are	O
shown	O
below	O
:	O
Table	O
7	O
shows	O
the	O
BLEU	B-MetricName
scores	O
of	O
the	O
proposed	O
multi	O
-	O
task	O
neural	O
model	O
on	O
using	O
different	O
external	O
monolingual	O
data	O
.	O
We	O
can	O
see	O
that	O
there	O
is	O
no	O
obvious	O
difference	O
on	O
Turkish	O
-	O
English	O
translation	O
performance	O
by	O
using	O
different	O
monolingual	O
data	O
,	O
whether	O
the	O
data	O
is	O
in	O
-	O
domain	O
or	O
out	O
-	O
of	O
-	O
domain	O
to	O
the	O
test	O
dataset	O
.	O
However	O
,	O
for	O
the	O
English	O
-	O
Turkish	O
machine	B-TaskName
translation	I-TaskName
task	O
,	O
which	O
can	O
be	O
seen	O
as	O
agglutinative	O
language	O
generation	O
task	O
,	O
using	O
the	O
mixed	O
data	O
of	O
talks	O
and	O
news	O
achieves	O
further	O
improvements	O
of	O
the	O
BLEU	B-MetricName
scores	O
on	O
almost	O
all	O
the	O
test	O
datasets	O
.	O
The	O
main	O
reason	O
is	O
that	O
the	O
proposed	O
multi	O
-	O
task	O
neural	O
model	O
incorporates	O
many	O
morphological	O
and	O
linguistic	O
information	O
of	O
Turkish	O
rather	O
than	O
that	O
of	O
English	O
,	O
which	O
mainly	O
pays	O
attention	O
to	O
the	O
source	O
-	O
side	O
representation	O
ability	O
on	O
agglutinative	O
languages	O
rather	O
than	O
the	O
target	O
-	O
side	O
language	O
modeling	O
.	O
We	O
also	O
evaluate	O
the	O
translation	O
performance	O
of	O
the	O
general	O
NMT	O
model	O
,	O
baseline	O
NMT	O
model	O
,	O
and	O
multi	O
-	O
task	O
neural	O
model	O
with	O
external	O
news	O
data	O
on	O
the	O
machine	B-TaskName
translation	I-TaskName
task	O
between	O
Uyghur	O
and	O
Chinese	O
.	O
The	O
experimental	O
results	O
are	O
shown	O
in	O
Table	O
8	O
.	O
The	O
results	O
indicate	O
that	O
the	O
multi	O
-	O
task	O
neural	O
model	O
achieves	O
the	O
highest	O
BLEU	B-MetricName
scores	O
on	O
the	O
test	O
dataset	O
by	O
utilizing	O
external	O
monolingual	O
data	O
for	O
the	O
stemming	O
task	O
on	O
Uyghur	O
sentences	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
multi	O
-	O
task	O
neural	O
model	O
for	O
translation	O
task	O
from	O
and	O
into	O
a	O
low	O
-	O
resource	O
and	O
morphologically	O
-	O
rich	O
agglutinative	O
language	O
.	O
The	O
model	O
jointly	O
learns	O
to	O
perform	O
bi	O
-	O
directional	O
translation	O
and	O
agglutinative	O
language	O
stemming	O
by	O
utilizing	O
the	O
shared	O
encoder	O
and	O
decoder	O
under	O
standard	O
NMT	O
framework	O
.	O
Extensive	O
experimental	O
results	O
show	O
that	O
the	O
proposed	O
model	O
is	O
beneficial	O
for	O
the	O
agglutinative	O
language	O
machine	B-TaskName
translation	I-TaskName
,	O
and	O
only	O
a	O
small	O
amount	O
of	O
the	O
agglutinative	O
data	O
can	O
improve	O
the	O
translation	O
performance	O
in	O
both	O
directions	O
.	O
Moreover	O
,	O
the	O
proposed	O
approach	O
with	O
external	O
monolingual	O
data	O
is	O
more	O
useful	O
for	O
translating	O
into	O
the	O
agglutinative	O
language	O
,	O
which	O
achieves	O
an	O
improvement	O
of	O
+1.42	O
BLEU	B-MetricName
points	O
for	O
translation	O
from	O
English	O
into	O
Turkish	O
and	O
+1.45	O
BLEU	B-MetricName
points	O
from	O
Chinese	O
into	O
Uyghur	O
.	O
In	O
future	O
work	O
,	O
we	O
plan	O
to	O
utilize	O
other	O
word	O
segmentation	O
methods	O
for	O
model	O
training	O
.	O
We	O
also	O
plan	O
to	O
combine	O
the	O
proposed	O
multi	O
-	O
task	O
neural	O
model	O
with	O
back	O
-	O
translation	O
method	O
to	O
enhance	O
the	O
ability	O
of	O
the	O
NMT	O
model	O
on	O
target	O
-	O
side	O
language	O
modeling	O
.	O

To	O
study	O
negative	O
interference	O
,	O
we	O
compare	O
multilingual	O
models	O
with	O
monolingual	O
baselines	O
.	O
Without	O
loss	B-MetricName
of	O
generality	O
,	O
we	O
focus	O
on	O
analyzing	O
bilingual	O
models	O
to	O
minimize	O
confounding	O
factors	O
.	O
For	O
two	O
languages	O
lg	O
1	O
and	O
lg	O
2	O
,	O
we	O
pretrain	O
a	O
single	O
bilingual	O
model	O
and	O
two	O
monolingual	O
models	O
.	O
We	O
then	O
assess	O
their	O
performance	O
on	O
downstream	O
tasks	O
using	O
two	O
different	O
settings	O
.	O
To	O
examine	O
negative	O
interference	O
,	O
we	O
evaluate	O
both	O
monolingual	O
and	O
multilingual	O
models	O
using	O
the	O
withinlanguage	O
monolingual	O
setting	O
,	O
such	O
that	O
the	O
pretrained	O
model	O
is	O
finetuned	O
and	O
tested	O
on	O
the	O
same	O
language	O
.	O
For	O
instance	O
,	O
if	O
the	O
monolingual	O
model	O
of	O
lg	O
1	O
outperforms	O
the	O
bilingual	O
model	O
on	O
lg	O
1	O
,	O
we	O
know	O
that	O
lg	O
2	O
induces	O
negative	O
impact	O
on	O
lg	O
1	O
in	O
the	O
bilingual	O
model	O
.	O
Besides	O
,	O
since	O
multilingual	O
models	O
are	O
trained	O
to	O
enable	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
,	O
we	O
also	O
report	O
their	O
performance	O
on	O
the	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
cross	I-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
setting	O
,	O
where	O
the	O
model	O
is	O
only	O
finetuned	O
on	O
the	O
source	O
language	O
,	O
say	O
lg	O
1	O
,	O
and	O
tested	O
on	O
the	O
target	O
language	O
lg	O
2	O
.	O
We	O
hypothesize	O
that	O
the	O
following	O
factors	O
play	O
important	O
roles	O
in	O
causing	O
negative	O
interference	O
and	O
study	O
each	O
individually	O
:	O
guages	O
,	O
we	O
hypothesize	O
that	O
it	O
can	O
also	O
occur	O
for	O
languages	O
with	O
less	O
resources	O
.	O
We	O
study	O
the	O
impact	O
of	O
training	O
data	O
size	O
per	O
language	O
on	O
negative	O
interference	O
.	O
We	O
subsample	O
a	O
high	O
-	O
resource	O
language	O
,	O
say	O
lg	O
1	O
,	O
to	O
create	O
a	O
"	O
low	O
-	O
resource	O
version	O
"	O
.	O
We	O
then	O
retrain	O
the	O
monolingual	O
and	O
bilingual	O
models	O
and	O
compare	O
with	O
results	O
of	O
their	O
high	O
-	O
source	O
counterparts	O
.	O
Particularly	O
,	O
we	O
test	O
if	O
reducing	O
lg	O
1	O
's	O
training	O
size	O
also	O
reduces	O
negative	O
interference	O
on	O
lg	O
2	O
.	O
Language	O
Similarity	O
Language	O
similarity	O
has	O
been	O
shown	O
important	O
for	O
effective	O
transfer	O
in	O
multilingual	O
models	O
.	O
shows	O
that	O
bilingual	O
models	O
trained	O
on	O
more	O
similar	O
language	O
pairs	O
result	O
in	O
better	O
zero	O
-	O
shot	O
transfer	O
performance	O
.	O
We	O
thus	O
expect	O
it	O
to	O
play	O
a	O
critical	O
role	O
in	O
negative	O
interference	O
as	O
well	O
.	O
For	O
a	O
specific	O
language	O
lg	O
1	O
,	O
we	O
pair	O
it	O
with	O
languages	O
that	O
are	O
closely	O
and	O
distantly	O
related	O
.	O
We	O
then	O
compare	O
these	O
bilingual	O
models	O
'	O
performance	O
on	O
lg	O
1	O
to	O
investigate	O
if	O
more	O
similar	O
languages	O
cause	O
less	O
severe	O
interference	O
.	O
In	O
addition	O
,	O
we	O
further	O
add	O
a	O
third	O
language	O
lg	O
3	O
that	O
is	O
similar	O
to	O
lg	O
1	O
and	O
train	O
a	O
trilingual	O
model	O
on	O
lg	O
1	O
-	O
lg	O
2	O
-	O
lg	O
3	O
.	O
We	O
compare	O
the	O
trilingual	O
model	O
with	O
the	O
bilingual	O
model	O
to	O
examine	O
if	O
adding	O
lg	O
3	O
can	O
mitigate	O
negative	O
interference	O
on	O
lg	O
1	O
.	O

State	O
-	O
of	O
-	O
the	O
-	O
art	O
multilingual	O
models	O
aim	O
to	O
share	O
as	O
many	O
parameters	O
as	O
possible	O
in	O
the	O
hope	O
of	O
learning	O
a	O
languageuniversal	O
model	O
for	O
all	O
languages	O
.	O
While	O
prior	O
studies	O
measure	O
the	O
latent	O
embedding	O
similarity	O
between	O
languages	O
,	O
we	O
instead	O
examine	O
model	O
parameters	O
directly	O
.	O
The	O
idea	O
is	O
to	O
test	O
whether	O
model	O
parameters	O
are	O
language	O
-	O
universal	O
or	O
language	O
-	O
specific	O
.	O
To	O
achieve	O
this	O
,	O
we	O
prune	O
multilingual	O
models	O
for	O
each	O
language	O
using	O
relaxed	O
L	O
0	B-DatasetName
norm	O
regularization	O
(	O
Louizos	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
compare	O
parameter	O
similarities	O
between	O
languages	O
.	O
Formally	O
,	O
for	O
a	O
model	O
f	O
(	O
;	O
θ	B-HyperparameterName
)	O
parameterized	O
by	O
θ	B-HyperparameterName
=	O
{	O
θ	B-HyperparameterName
i	O
}	O
n	O
i=1	O
where	O
each	O
θ	B-HyperparameterName
i	O
represents	O
an	O
individual	O
parameter	O
or	O
a	O
group	O
of	O
parameters	O
,	O
the	O
method	O
introduces	O
a	O
set	O
of	O
binary	O
masks	O
z	O
,	O
drawn	O
from	O
some	O
distribution	O
q	O
(	O
z	O
|	O
π	O
)	O
parametrized	O
by	O
π	O
,	O
and	O
learns	O
a	O
sparse	O
model	O
f	O
(	O
;	O
θ	B-HyperparameterName
z	O
)	O
by	O
optimizing	O
:	O
min	O
π	O
E	O
q	O
(	O
z	O
|	O
π	O
)	O
1	O
N	O
N	O
i=1	O
L	O
(	O
f	O
(	O
x	O
i	O
;	O
θ	B-HyperparameterName
)	O
,	O
y	O
i	O
)	O
+	O
λ	O
θ	B-HyperparameterName
0	B-DatasetName
s.t.θ	O
=	O
θ	B-HyperparameterName
z	O
,	O
(	O
1	O
)	O
where	O
is	O
the	O
Hadamard	O
(	O
elementwise	O
)	O
product	O
,	O
L	O
(	O
)	O
is	O
some	O
task	O
loss	B-MetricName
and	O
λ	O
is	O
a	O
hyper	O
-	O
parameter	O
.	O
We	O
follow	O
the	O
work	O
of	O
(	O
Louizos	O
et	O
al	O
,	O
2017	O
)	O
and	O
use	O
the	O
Hard	O
Concrete	O
distribution	O
for	O
the	O
binary	O
mask	O
z	O
,	O
such	O
that	O
the	O
above	O
objective	O
is	O
fully	O
differentiable	O
.	O
Then	O
,	O
for	O
each	O
bilingual	O
model	O
,	O
we	O
freeze	O
its	O
pretrained	O
parameter	O
weights	O
and	O
learn	O
binary	O
masks	O
z	O
for	O
each	O
language	O
independently	O
.	O
As	O
a	O
result	O
,	O
we	O
obtain	O
two	O
independent	O
sets	O
of	O
mask	O
parameters	O
π	O
which	O
can	O
be	O
used	O
to	O
determine	O
parameter	O
importance	O
.	O
Intuitively	O
,	O
for	O
each	O
parameter	O
group	O
,	O
it	O
is	O
language	O
-	O
universal	O
if	O
both	O
languages	O
consider	O
it	O
important	O
(	O
positive	O
π	O
values	O
)	O
.	O
On	O
the	O
other	O
hand	O
,	O
if	O
one	O
language	O
assigns	O
positive	O
value	O
while	O
the	O
other	O
assigns	O
negative	O
,	O
it	O
shows	O
that	O
the	O
parameter	O
group	O
is	O
language	O
-	O
specific	O
.	O
We	O
compare	O
them	O
across	O
languages	O
and	O
layers	O
to	O
analyze	O
parameter	O
similarity	O
in	O
multilingual	O
models	O
.	O

We	O
focus	O
on	O
standard	O
multilingual	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
used	O
in	O
mBERT	B-MethodName
and	O
XLM	B-MethodName
.	O
We	O
first	O
pretrain	O
models	O
and	O
then	O
evaluate	O
their	O
performance	O
on	O
four	O
NLP	O
benchmarks	O
.	O
For	O
pretraining	O
,	O
we	O
mainly	O
follow	O
the	O
setup	O
and	O
implementation	O
of	O
XLM	B-MethodName
(	O
Lample	O
and	O
Conneau	O
,	O
2019	O
)	O
.	O
We	O
focus	O
on	O
monolingual	O
and	O
bilingual	O
models	O
for	O
a	O
more	O
controllable	O
comparison	O
,	O
which	O
we	O
refer	O
to	O
as	O
Mono	O
and	O
JointPair	O
respectively	O
.	O
In	O
particular	O
,	O
we	O
always	O
include	O
English	O
(	O
En	O
)	O
in	O
bilingual	O
models	O
to	O
compare	O
on	O
zero	O
-	O
shot	O
transfer	O
settings	O
with	O
prior	O
work	O
.	O
Besides	O
,	O
we	O
consider	O
three	O
adpt	O
86.8	O
86.7	O
84.3	O
88.6	O
86.1	O
76.0	O
84.8	O
89.3	O
76.4	O
93.5	O
95.2	O
88.2	O
88	O
high	O
-	O
resource	O
languages	O
{	O
Arabic	O
(	O
Ar	O
)	O
,	O
French	O
(	O
Fr	O
)	O
,	O
Russian	O
(	O
Ru	O
)	O
}	O
and	O
three	O
low	O
-	O
resource	O
languages	O
{	O
Hindi	O
(	O
Hi	O
)	O
,	O
Swahili	O
(	O
Sw	O
)	O
,	O
Telugu	O
(	O
Te	O
)	O
}	O
(	O
see	O
Table	O
1	O
for	O
their	O
statistics	O
)	O
.	O
We	O
choose	O
these	O
six	O
languages	O
based	O
their	O
data	O
availability	O
in	O
downstream	O
tasks	O
.	O
We	O
use	O
Wikipedia	O
as	O
training	O
data	O
with	O
statistics	O
shown	O
in	O
Table	O
1	O
.	O
For	O
each	O
model	O
,	O
we	O
use	O
BPE	B-MethodName
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
to	O
learn	O
32k	O
subword	O
vocabulary	O
shared	O
between	O
languages	O
.	O
For	O
multilingual	O
models	O
,	O
we	O
sample	O
language	O
proportionally	O
to	O
P	O
i	O
=	O
(	O
L	O
i	O
j	O
L	O
j	O
)	O
1	O
T	O
,	O
where	O
L	O
i	O
is	O
the	O
size	O
of	O
the	O
training	O
corpus	O
for	O
i	O
-	O
th	O
language	O
pair	O
and	O
T	O
is	O
the	O
temperature	O
.	O
Each	O
model	O
is	O
a	O
standard	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
with	O
8	O
layers	O
,	O
12	O
heads	O
,	O
512	O
embedding	O
size	O
and	O
2048	B-DatasetName
hidden	O
dimension	O
for	O
the	O
feedforward	O
layer	O
.	O
Notice	O
that	O
we	O
specifically	O
consider	O
a	O
smaller	O
model	O
capacity	O
to	O
be	O
comparable	O
with	O
existing	O
models	O
with	O
larger	O
capacity	O
but	O
also	O
include	O
much	O
more	O
(	O
over	O
100	O
)	O
languages	O
.	O
We	O
use	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
and	O
exploit	O
the	O
same	O
learning	B-HyperparameterName
rate	I-HyperparameterName
schedule	O
as	O
Lample	O
and	O
Conneau	O
(	O
2019	O
)	O
.	O
We	O
train	O
each	O
model	O
with	O
4	O
NVIDIA	O
V100	O
GPUs	O
with	O
32	O
GB	O
of	O
memory	O
.	O
Using	O
mixed	O
precision	O
,	O
we	O
fit	O
a	O
batch	O
of	O
128	O
for	O
each	O
GPU	O
and	O
the	O
total	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
512	O
.	O
Each	O
epoch	O
contains	O
10k	O
steps	O
and	O
we	O
train	O
for	O
50	O
epochs	O
.	O
For	O
evaluation	O
,	O
we	O
consider	O
four	O
downstream	O
tasks	O
:	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
,	O
part	O
-	O
ofspeech	O
tagging	O
(	O
POS	O
)	O
,	O
question	B-TaskName
answering	I-TaskName
(	O
QA	O
)	O
,	O
and	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
NLI	O
)	O
.	O
(	O
See	O
Appendix	O
A	O
for	O
finetuning	O
details	O
.	O
)	O

We	O
use	O
the	O
WikiAnn	B-DatasetName
(	O
Pan	O
et	O
al	O
,	O
2017	O
)	O
dataset	O
,	O
which	O
is	O
a	O
sequence	O
labelling	O
task	O
built	O
automatically	O
from	O
Wikipedia	O
.	O
A	O
linear	B-MethodName
layer	I-MethodName
with	O
softmax	B-MethodName
classifier	O
is	O
added	O
on	O
top	O
of	O
pretrained	O
models	O
to	O
predict	O
the	O
label	O
for	O
each	O
word	O
based	O
on	O
its	O
first	O
subword	O
.	O
We	O
report	O
the	O
F1	B-MetricName
score	I-MetricName
.	O

Similar	O
to	O
NER	B-TaskName
,	O
POS	O
is	O
also	O
a	O
sequence	O
labelling	O
task	O
but	O
with	O
a	O
focus	O
on	O
synthetic	O
knowledge	O
.	O
In	O
particular	O
,	O
we	O
use	O
the	O
Universal	B-DatasetName
Dependencies	I-DatasetName
treebanks	O
(	O
Nivre	O
et	O
al	O
,	O
2018	O
)	O
.	O
Task	O
-	O
specific	O
layers	O
are	O
the	O
same	O
and	O
we	O
report	O
F1	B-MetricName
,	O
as	O
in	O
NER	B-TaskName
.	O

We	O
choose	O
to	O
use	O
the	O
TyDiQA	B-DatasetName
-	I-DatasetName
GoldP	I-DatasetName
dataset	O
(	O
Clark	O
et	O
al	O
,	O
2020	O
)	O
that	O
covers	O
typologically	O
diverse	O
languages	O
.	O
Similar	O
to	O
popular	O
QA	O
dataset	O
such	O
as	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2018	O
)	O
,	O
this	O
is	O
a	O
span	O
prediction	O
task	O
where	O
task	O
-	O
specific	O
linear	O
classifiers	O
are	O
used	O
to	O
predict	O
start	O
/	O
end	O
positions	O
of	O
the	O
answer	O
.	O
Standard	O
metrics	O
of	O
F1	B-MetricName
and	O
Exact	B-MetricName
Match	I-MetricName
(	O
EM	B-MetricName
)	O
are	O
reported	O
.	O
NLI	O
XNLI	B-DatasetName
(	O
Conneau	O
et	O
al	O
,	O
2018	O
)	O
is	O
probably	O
the	O
most	O
popular	O
cross	O
-	O
lingual	O
benchmark	O
.	O
Notice	O
that	O
the	O
original	O
dataset	O
only	O
contains	O
training	O
data	O
for	O
English	O
.	O
Consequently	O
,	O
we	O
only	O
evaluate	O
this	O
task	O
on	O
the	O
zero	O
-	O
shot	O
transfer	O
setting	O
while	O
we	O
consider	O
both	O
settings	O
for	O
the	O
rest	O
of	O
other	O
tasks	O
.	O

In	O
Table	O
2	O
and	O
3	O
gual	O
models	O
outperform	O
bilingual	O
models	O
for	O
all	O
languages	O
except	O
Swahili	O
on	O
all	O
three	O
tasks	O
.	O
In	O
fact	O
,	O
monolingual	O
models	O
even	O
perform	O
better	O
than	O
XLM	B-MethodName
on	O
four	O
out	O
of	O
six	O
languages	O
including	O
hi	O
and	O
te	O
,	O
despite	O
that	O
XLM	B-MethodName
is	O
much	O
larger	O
in	O
model	O
sizes	O
and	O
trained	O
with	O
much	O
more	O
resources	O
.	O
This	O
shows	O
that	O
negative	O
interference	O
can	O
occur	O
on	O
low	O
-	O
resource	O
languages	O
as	O
well	O
.	O
While	O
the	O
negative	O
impact	O
is	O
expected	O
to	O
be	O
more	O
prominent	O
on	O
high	O
-	O
resource	O
languages	O
,	O
we	O
demonstrate	O
that	O
it	O
may	O
occur	O
for	O
languages	O
with	O
resources	O
fewer	O
than	O
commonly	O
believed	O
.	O
The	O
existence	O
of	O
negative	O
interference	O
confirms	O
that	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
multilingual	O
models	O
can	O
not	O
generalize	O
equally	O
well	O
on	O
all	O
languages	O
,	O
and	O
there	O
is	O
still	O
a	O
gap	O
compared	O
to	O
monolingual	O
models	O
on	O
certain	O
languages	O
.	O
We	O
next	O
turn	O
to	O
dissect	O
negative	O
interference	O
by	O
studying	O
the	O
four	O
factors	O
described	O
in	O
Section	O
3.1	O
.	O
Training	O
Corpus	O
Size	O
By	O
comparing	O
the	O
validation	O
perplexity	B-MetricName
on	O
Swahili	O
and	O
Telugu	O
in	O
Figure	O
2	O
,	O
we	O
find	O
that	O
while	O
both	O
monolingual	O
models	O
outperform	O
bilingual	O
models	O
in	O
the	O
first	O
few	O
epochs	O
,	O
the	O
Swahili	O
model	O
's	O
perplexity	B-MetricName
starts	O
to	O
increase	O
and	O
is	O
eventually	O
surpassed	O
by	O
the	O
bilingual	O
model	O
in	O
later	O
epochs	O
.	O
This	O
matches	O
the	O
intuition	O
that	O
monolingual	O
models	O
may	O
overfit	O
when	O
training	O
data	O
size	O
is	O
small	O
.	O
To	O
verify	O
this	O
,	O
we	O
subsample	O
French	O
and	O
Russian	O
to	O
100k	O
sentences	O
to	O
create	O
a	O
"	O
low	O
-	O
resource	O
version	O
"	O
of	O
them	O
(	O
denoted	O
as	O
fr	O
l	O
/ru	O
l	O
)	O
.	O
As	O
shown	O
in	O
lingual	O
models	O
for	O
fr	O
l	O
/ru	O
l	O
,	O
in	O
contrast	O
for	O
fr	O
/	O
ru	O
.	O
This	O
suggests	O
that	O
multilingual	O
models	O
can	O
stimulate	O
positive	O
transfer	O
for	O
low	O
-	O
resource	O
languages	O
when	O
monolingual	O
models	O
overfit	O
.	O
On	O
the	O
other	O
hand	O
,	O
when	O
we	O
compare	O
bilingual	O
models	O
on	O
English	O
,	O
models	O
trained	O
using	O
different	O
sizes	O
of	O
fr	O
/	O
ru	O
data	O
obtain	O
similar	O
performance	O
,	O
indicating	O
that	O
the	O
training	O
size	O
of	O
the	O
source	O
language	O
has	O
little	O
impact	O
on	O
negative	O
interference	O
on	O
the	O
target	O
language	O
(	O
English	O
in	O
this	O
case	O
)	O
.	O
While	O
more	O
training	O
data	O
usually	O
implies	O
larger	O
vocabulary	O
and	O
more	O
diverse	O
linguistic	O
phenomena	O
,	O
negative	O
interference	O
seems	O
to	O
arise	O
from	O
more	O
fundamental	O
conflicts	O
contained	O
in	O
even	O
small	O
training	O
corpus	O
.	O
Language	O
Similarity	O
As	O
illustrated	O
by	O
Table	O
5	O
,	O
the	O
in	O
-	O
language	O
performance	O
on	O
English	O
drops	O
as	O
the	O
paired	O
language	O
becomes	O
more	O
distantly	O
related	O
(	O
French	O
vs	O
Russian	O
)	O
.	O
This	O
verifies	O
that	O
transferring	O
from	O
more	O
distant	O
languages	O
results	O
in	O
more	O
severe	O
negative	O
interference	O
.	O
It	O
is	O
therefore	O
natural	O
to	O
ask	O
if	O
adding	O
more	O
similar	O
languages	O
can	O
mitigate	O
negative	O
interference	O
,	O
especially	O
for	O
low	O
-	O
resource	O
languages	O
.	O
We	O
then	O
train	O
two	O
trilingual	O
models	O
,	O
adding	O
Marathi	O
to	O
English	O
-	O
Hindi	O
,	O
and	O
Kannada	O
to	O
English	O
-	O
Telugu	O
.	O
Compared	O
to	O
their	O
bilingual	O
counterparts	O
(	O
Table	O
4	O
)	O
,	O
trilingual	O
models	O
obtain	O
similar	O
within	O
-	O
language	O
performance	O
,	O
which	O
indicates	O
that	O
adding	O
similar	O
languages	O
can	O
not	O
mitigate	O
negative	O
interference	O
.	O
"	O
En	O
-	O
En	O
"	O
refers	O
to	O
gradients	O
of	O
two	O
English	O
batches	O
within	O
the	O
Ar	O
-	O
En	O
model	O
,	O
while	O
"	O
Ar	O
-	O
En	O
"	O
and	O
"	O
Fr	O
-	O
En	O
"	O
refer	O
to	O
gradients	O
of	O
two	O
batches	O
,	O
one	O
from	O
each	O
language	O
,	O
within	O
Ar	O
-	O
En	O
and	O
Fr	O
-	O
En	O
models	O
respectively	O
.	O
However	O
,	O
they	O
do	O
improve	O
zero	O
-	O
shot	O
cross	O
-	O
lingual	O
performance	O
.	O
One	O
possible	O
explanation	O
is	O
that	O
even	O
similar	O
languages	O
can	O
fight	O
for	O
language	O
-	O
specific	O
capacity	O
but	O
they	O
may	O
nevertheless	O
benefit	O
the	O
generalization	O
of	O
the	O
shared	O
knowledge	O
.	O
Gradient	O
Conflict	O
In	O
Figure	O
3	O
,	O
we	O
plot	O
the	O
gradient	O
cosine	O
similarity	O
between	O
Arabic	O
-	O
English	O
and	O
French	O
-	O
English	O
in	O
their	O
corresponding	O
bilingual	O
models	O
over	O
the	O
first	O
25	O
epochs	O
.	O
We	O
also	O
plot	O
the	O
similarity	O
within	O
English	O
,	O
measured	O
using	O
two	O
independently	O
sampled	O
batches	O
2	O
.	O
Specifically	O
,	O
gradients	O
between	O
two	O
different	O
languages	O
are	O
indeed	O
less	O
similar	O
than	O
those	O
within	O
the	O
same	O
language	O
.	O
The	O
gap	O
is	O
more	O
evident	O
in	O
the	O
early	O
few	O
epochs	O
,	O
where	O
we	O
observe	O
negative	O
gradient	O
similarities	O
for	O
Ar	O
-	O
En	O
and	O
Fr	O
-	O
En	O
while	O
those	O
for	O
En	O
-	O
En	O
are	O
positive	O
.	O
In	O
addition	O
,	O
gradients	O
in	O
Ar	O
-	O
En	O
are	O
less	O
similar	O
than	O
those	O
in	O
Fr	O
-	O
En	O
,	O
indicating	O
that	O
distant	O
language	O
pair	O
can	O
cause	O
more	O
severe	O
gradient	O
conflicts	O
.	O
These	O
results	O
confirm	O
that	O
gradient	O
conflict	O
exists	O
in	O
multilingual	O
models	O
and	O
is	O
correlated	O
to	O
per	O
language	O
performance	O
,	O
suggesting	O
it	O
may	O
introduce	O
optimization	O
challenge	O
that	O
results	O
in	O
negative	O
interference	O
.	O

The	O
existence	O
of	O
gradient	O
2	O
Notice	O
that	O
we	O
use	O
gradient	O
accumulation	O
to	O
sample	O
an	O
effectively	O
larger	O
batch	O
of	O
4096	O
sentences	O
to	O
calculate	O
the	O
gradient	O
similarity	O
.	O
conflicts	O
may	O
imply	O
that	O
languages	O
are	O
fighting	O
for	O
capacity	O
.	O
Thus	O
,	O
we	O
next	O
study	O
how	O
languageuniversal	O
these	O
multilingual	O
parameters	O
are	O
.	O
Figure	O
4a	O
shows	O
the	O
cosine	O
similarity	O
of	O
mask	O
parameters	O
π	O
across	O
different	O
layers	O
.	O
We	O
observe	O
that	O
within	O
-	O
language	O
similarity	O
(	O
En	O
-	O
En	O
)	O
is	O
near	O
perfect	O
,	O
which	O
validates	O
the	O
pruning	O
method	O
's	O
robustness	O
.	O
The	O
trend	O
shows	O
that	O
model	O
parameters	O
are	O
better	O
shared	O
in	O
the	O
bottom	O
layers	O
than	O
the	O
upper	O
ones	O
.	O
Besides	O
,	O
it	O
also	O
demonstrates	O
that	O
parameters	O
in	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
layers	O
obtain	O
higher	O
similarities	O
than	O
those	O
in	O
feedforward	O
layers	O
,	O
suggesting	O
that	O
attention	O
mechanism	O
might	O
be	O
more	O
languageuniversal	O
.	O
We	O
additionally	O
inspect	O
π	O
parameters	O
with	O
the	O
highest	O
absolute	O
values	O
and	O
plot	O
those	O
values	O
for	O
Ar	O
(	O
Figure	O
4b	O
)	O
,	O
together	O
with	O
their	O
En	O
counterparts	O
.	O
A	O
more	O
negative	O
value	O
indicates	O
that	O
the	O
parameter	O
is	O
more	O
likely	O
to	O
be	O
pruned	O
for	O
that	O
language	O
and	O
vice	O
versa	O
.	O
Interestingly	O
,	O
while	O
many	O
parameters	O
with	O
positive	O
values	O
(	O
on	O
the	O
right	O
)	O
are	O
language	O
-	O
universal	O
as	O
both	O
languages	O
assign	O
very	O
positive	O
values	O
,	O
parameters	O
with	O
negative	O
values	O
(	O
on	O
the	O
left	O
)	O
are	O
mostly	O
language	O
-	O
specific	O
for	O
Ar	O
as	O
En	O
assigns	O
positive	O
values	O
.	O
We	O
observe	O
similar	O
patterns	O
for	O
other	O
languages	O
as	O
well	O
.	O
These	O
results	O
demonstrate	O
that	O
language	O
-	O
specific	O
parameters	O
do	O
exist	O
in	O
multilingual	O
models	O
.	O
Having	O
language	O
-	O
specific	O
capacity	O
in	O
shared	O
parameters	O
is	O
sub	O
-	O
optimal	O
.	O
It	O
is	O
less	O
transferable	O
and	O
thus	O
can	O
hinder	O
cross	O
-	O
lingual	O
performance	O
.	O
Moreover	O
,	O
it	O
may	O
also	O
take	O
over	O
capacity	O
budgets	O
for	O
other	O
languages	O
and	O
degrade	O
their	O
within	O
-	O
language	O
performance	O
,	O
i.e.	O
,	O
causing	O
negative	O
interference	O
.	O
A	O
natural	O
next	O
question	O
is	O
whether	O
explicitly	O
adding	O
language	O
-	O
specific	O
capacity	O
into	O
multilingual	O
models	O
can	O
alleviate	O
negative	O
interference	O
.	O
We	O
thus	O
train	O
variants	O
of	O
bilingual	O
models	O
that	O
contain	O
language	O
-	O
specific	O
components	O
for	O
each	O
language	O
.	O
Particularly	O
,	O
we	O
consider	O
adding	O
language	O
-	O
specific	O
feedforward	O
layers	O
,	O
attention	B-HyperparameterName
layers	I-HyperparameterName
,	O
and	O
residual	O
adapter	O
layers	O
(	O
Rebuffi	O
et	O
al	O
,	O
2017	O
;	O
Houlsby	O
et	O
al	O
,	O
2019	O
)	O
,	O
denoted	O
as	O
ffn	O
,	O
attn	O
and	O
adpt	O
respectively	O
.	O
For	O
each	O
type	O
of	O
component	O
,	O
we	O
create	O
two	O
separate	O
copies	O
in	O
each	O
Transformer	B-MethodName
layer	O
,	O
one	O
designated	O
for	O
each	O
language	O
,	O
while	O
the	O
rest	O
of	O
the	O
network	O
remains	O
unchanged	O
.	O
As	O
shown	O
in	O
Table	O
2	O
and	O
3	O
,	O
adding	O
language	O
-	O
specific	O
capacity	O
does	O
mitigate	O
negative	O
interference	O
and	O
improve	O
monolingual	O
performance	O
.	O
We	O
also	O
find	O
that	O
language	O
-	O
specific	O
feedforward	O
layers	O
obtain	O
larger	O
performance	O
gains	O
compared	O
to	O
attention	B-HyperparameterName
layers	I-HyperparameterName
,	O
consistent	O
with	O
our	O
prior	O
analysis	O
.	O
However	O
,	O
these	O
gains	O
come	O
at	O
a	O
cost	O
of	O
cross	O
-	O
lingual	O
transferability	O
,	O
such	O
that	O
their	O
zeroshot	O
performance	O
drops	O
tremendously	O
.	O
Our	O
results	O
suggest	O
a	O
tension	O
between	O
addressing	O
interference	O
versus	O
improving	O
transferability	O
.	O
In	O
the	O
next	O
section	O
,	O
we	O
investigate	O
how	O
to	O
address	O
negative	O
interference	O
in	O
a	O
manner	O
that	O
can	O
improve	O
performance	O
on	O
both	O
within	O
-	O
language	O
tasks	O
and	O
cross	O
-	O
lingual	O
benchmarks	O
.	O
4	O
Mitigating	O
Negative	O
Interference	O
via	O
Meta	O
Learning	O

In	O
the	O
previous	O
section	O
,	O
we	O
demonstrated	O
that	O
while	O
explicitly	O
adding	O
language	O
-	O
specific	O
components	O
can	O
alleviate	O
negative	O
interference	O
,	O
it	O
can	O
also	O
hinder	O
cross	O
-	O
lingual	O
transferability	O
.	O
We	O
notice	O
that	O
a	O
critical	O
shortcoming	O
of	O
language	O
-	O
specific	O
capacity	O
is	O
that	O
they	O
are	O
agnostic	O
of	O
the	O
rest	O
of	O
other	O
languages	O
,	O
since	O
by	O
design	O
they	O
are	O
trained	O
on	O
the	O
designated	O
language	O
only	O
.	O
They	O
are	O
thus	O
more	O
likely	O
to	O
overfit	O
and	O
can	O
induce	O
optimization	O
challenges	O
for	O
shared	O
capacity	O
as	O
well	O
.	O
Inspired	B-DatasetName
by	O
recent	O
work	O
in	O
meta	O
learning	O
(	O
Flennerhag	O
et	O
al	O
,	O
2019	O
)	O
that	O
utilizes	O
meta	O
parameters	O
to	O
improve	O
gradient	O
geometry	O
of	O
the	O
base	O
network	O
,	O
we	O
propose	O
a	O
novel	O
meta	B-TaskName
-	I-TaskName
learning	I-TaskName
formulation	O
of	O
multilingual	O
models	O
that	O
exploits	O
language	O
-	O
specific	O
parameters	O
to	O
i	O
m	O
-	O
prove	O
generalization	O
of	O
shared	O
parameters	O
.	O
For	O
a	O
model	O
with	O
some	O
predefined	O
languagespecific	O
parameters	O
φ	O
=	O
{	O
φ	O
i	O
}	O
L	O
i=1	O
,	O
where	O
φ	O
i	O
is	O
designated	O
for	O
the	O
i	O
-	O
th	O
language	O
,	O
and	O
shared	O
parameters	O
θ	B-HyperparameterName
,	O
our	O
solution	O
is	O
to	O
treat	O
φ	O
as	O
meta	O
parameters	O
and	O
θ	B-HyperparameterName
as	O
base	O
parameters	O
.	O
Ideally	O
,	O
we	O
want	O
φ	O
to	O
store	O
non	O
-	O
transferable	O
language	O
-	O
specific	O
knowledge	O
to	O
resolve	O
conflicts	O
and	O
improve	O
generalization	O
of	O
θ	B-HyperparameterName
in	O
all	O
languages	O
(	O
a.k.a	O
.	O
mitigate	O
negative	O
interference	O
and	O
improve	O
cross	O
-	O
lingual	O
transferability	O
)	O
.	O
Therefore	O
,	O
we	O
train	O
φ	O
based	O
on	O
the	O
following	O
principle	O
:	O
if	O
θ	B-HyperparameterName
follows	O
the	O
gradients	O
on	O
training	O
data	O
for	O
a	O
given	O
φ	O
,	O
the	O
resulting	O
θ	B-HyperparameterName
should	O
obtain	O
a	O
good	O
validation	O
performance	O
on	O
all	O
languages	O
.	O
This	O
implies	O
a	O
bilevel	O
optimization	O
problem	O
(	O
Colson	O
et	O
al	O
,	O
2007	O
)	O
formally	O
written	O
as	O
:	O
min	O
φ	O
1	O
L	O
L	O
i=1	O
L	O
i	O
val	O
(	O
θ	B-HyperparameterName
*	O
,	O
φ	O
i	O
)	O
s.t	O
.	O
θ	B-HyperparameterName
*	O
=	O
arg	O
min	O
θ	B-HyperparameterName
1	O
L	O
L	O
i=1	O
L	O
i	O
train	O
(	O
θ	B-HyperparameterName
,	O
φ	O
i	O
)	O
,	O
(	O
2	O
)	O
where	O
L	O
i	O
val	O
and	O
L	O
i	O
train	O
denote	O
the	O
training	O
and	O
the	O
validation	O
MLM	B-DatasetName
loss	B-MetricName
for	O
the	O
i	O
-	O
th	O
language	O
.	O
Since	O
directly	O
solving	O
this	O
problem	O
can	O
be	O
prohibitive	O
due	O
to	O
the	O
expensive	O
inner	O
optimization	O
,	O
we	O
approximate	O
θ	B-HyperparameterName
*	O
by	O
adapting	O
the	O
current	O
θ	B-HyperparameterName
(	O
t	O
)	O
using	O
a	O
single	O
gradient	O
step	O
,	O
similar	O
to	O
techniques	O
used	O
in	O
prior	O
meta	B-TaskName
-	I-TaskName
learning	I-TaskName
methods	O
(	O
Finn	O
et	O
al	O
,	O
2017	O
)	O
.	O
This	O
results	O
in	O
a	O
two	O
-	O
phase	O
iterative	O
training	O
process	O
shown	O
in	O
Algorithm	O
1	O
(	O
See	O
Appendix	O
B	O
)	O
.	O
To	O
be	O
specific	O
,	O
at	O
each	O
training	O
step	O
t	O
on	O
the	O
ith	O
language	O
during	O
pretraining	O
,	O
we	O
first	O
adapt	O
a	O
gradient	O
step	O
on	O
θ	B-HyperparameterName
to	O
obtain	O
a	O
new	O
θ	B-HyperparameterName
and	O
update	O
φ	O
i	O
Update	O
language	O
-	O
specific	O
parameters	O
as	O
:	O
φ	O
(	O
t+1	O
)	O
i	O
GradientUpdate	O
(	O
φ	O
(	O
t	O
)	O
i	O
,	O
∇	O
φ	O
(	O
t	O
)	O
i	O
1	O
L	O
L	O
j=1	O
L	O
j	O
val	O
(	O
θ	B-HyperparameterName
(	O
t	O
)	O
i	O
−	O
β∇	O
θ	B-HyperparameterName
(	O
t	O
)	O
L	O
i	O
train	O
(	O
θ	B-HyperparameterName
(	O
t	O
)	O
,	O
φ	O
(	O
t	O
)	O
i	O
)	O
,	O
φ	O
(	O
t	O
)	O
j	O
)	O
)	O
7	O
:	O
Update	O
shared	O
parameters	O
as	O
:	O
t	O
)	O
,	O
φ	O
(	O
t+1	O
)	O
)	O
)	O
8	O
:	O
end	O
while	O
based	O
on	O
the	O
θ	B-HyperparameterName
's	O
validation	O
MLM	B-DatasetName
loss	B-MetricName
:	O
θ	B-HyperparameterName
(	O
t+1	O
)	O
GradientUpdate	O
(	O
θ	B-HyperparameterName
(	O
t	O
)	O
,	O
∇	O
θ	B-HyperparameterName
(	O
t	O
)	O
L	O
train	O
(	O
θ	B-HyperparameterName
(	O
φ	O
(	O
t+1	O
)	O
i	O
=	O
φ	O
(	O
t	O
)	O
i	O
−	O
α∇	O
φ	O
(	O
t	O
)	O
i	O
1	O
L	O
L	O
j=1	O
L	O
j	O
val	O
(	O
θ	B-HyperparameterName
,	O
φ	O
(	O
t	O
)	O
j	O
)	O
θ	B-HyperparameterName
=	O
θ	B-HyperparameterName
(	O
t	O
)	O
−	O
β∇	O
θ	B-HyperparameterName
(	O
t	O
)	O
L	O
i	O
train	O
(	O
θ	B-HyperparameterName
(	O
t	O
)	O
,	O
φ	O
(	O
t	O
)	O
i	O
)	O
,	O
(	O
3	O
)	O
where	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
are	O
learning	O
rates	O
.	O
Notice	O
that	O
θ	B-HyperparameterName
is	O
a	O
function	O
of	O
φ	O
(	O
t	O
)	O
i	O
and	O
thus	O
this	O
optimization	O
requires	O
computing	O
the	O
gradient	O
of	O
gradient	O
.	O
Particularly	O
,	O
by	O
applying	O
chain	O
rule	O
to	O
the	O
gradient	O
of	O
φ	O
(	O
t	O
)	O
i	O
,	O
we	O
can	O
observe	O
that	O
it	O
contains	O
a	O
higher	O
-	O
order	O
term	O
:	O
∇	O
2	O
φ	O
(	O
t	O
)	O
i	O
,	O
θ	B-HyperparameterName
(	O
t	O
)	O
L	O
i	O
train	O
(	O
θ	B-HyperparameterName
(	O
t	O
)	O
,	O
φ	O
(	O
t	O
)	O
i	O
)	O
∇	O
θ	B-HyperparameterName
1	O
L	O
L	O
j=1	O
L	O
j	O
val	O
(	O
θ	B-HyperparameterName
,	O
φ	O
(	O
t	O
)	O
j	O
)	O
(	O
4	O
)	O
This	O
is	O
important	O
,	O
since	O
it	O
shows	O
that	O
φ	O
i	O
can	O
obtain	O
information	O
from	O
other	O
languages	O
through	O
higherorder	O
gradients	O
.	O
In	O
other	O
words	O
,	O
language	O
-	O
specific	O
parameters	O
are	O
not	O
agnostic	O
of	O
other	O
languages	O
anymore	O
without	O
violating	O
the	O
language	O
-	O
specific	O
requirement	O
.	O
This	O
is	O
because	O
,	O
in	O
Eq	O
.	O
3	O
,	O
while	O
∇	O
θ	B-HyperparameterName
(	O
t	O
)	O
is	O
based	O
on	O
the	O
i	O
-	O
th	O
language	O
only	O
,	O
the	O
validation	O
loss	B-MetricName
is	O
computed	O
for	O
all	O
languages	O
.	O
Finally	O
,	O
in	O
the	O
second	O
phase	O
,	O
we	O
update	O
θ	B-HyperparameterName
based	O
on	O
the	O
new	O
φ	O
(	O
t+1	O
)	O
:	O
θ	B-HyperparameterName
(	O
t+1	O
)	O
=	O
θ	B-HyperparameterName
(	O
t	O
)	O
−	O
β∇	O
θ	B-HyperparameterName
(	O
t	O
)	O
L	O
train	O
(	O
θ	B-HyperparameterName
(	O
t	O
)	O
,	O
φ	O
(	O
t+1	O
)	O
)	O
(	O
5	O
)	O

While	O
our	O
method	O
is	O
generic	O
,	O
we	O
evaluate	O
it	O
applied	O
on	O
bilingual	O
models	O
with	O
adapter	O
networks	O
.	O
Adapters	O
have	O
been	O
effectively	O
utilized	O
in	O
multilingual	O
models	O
,	O
and	O
we	O
choose	O
them	O
for	O
practical	O
consideration	O
of	O
limiting	O
perlanguage	O
capacity	O
.	O
Unlike	O
prior	O
works	O
that	O
finetune	O
adapters	O
for	O
adaptation	O
,	O
we	O
train	O
them	O
jointly	O
with	O
shared	O
parameters	O
during	O
pretraining	O
.	O
We	O
follow	O
Houlsby	O
et	O
al	O
(	O
2019	O
)	O
and	O
insert	O
language	O
-	O
specific	O
adapters	O
after	O
attention	O
and	O
feedforward	O
layers	O
.	O
We	O
leave	O
a	O
more	O
thorough	O
investigation	O
of	O
how	O
to	O
better	O
pick	O
language	O
-	O
specific	O
structures	O
for	O
future	O
work	O
.	O
For	O
downstream	O
task	O
evaluation	O
,	O
we	O
finetune	O
all	O
layers	O
.	O
Notice	O
that	O
computing	O
the	O
gradient	O
of	O
gradient	O
in	O
Eq	O
.	O
3	O
doubles	O
the	O
memory	O
requirement	O
.	O
In	O
practice	O
,	O
we	O
utilize	O
the	O
finite	O
difference	O
approximation	O
(	O
Appendix	O
B	O
)	O
.	O
By	O
evaluating	O
their	O
performance	O
on	O
the	O
zeroshot	O
transfer	O
settings	O
(	O
Table	O
2	O
,	O
3	O
and	O
6	O
)	O
,	O
we	O
observe	O
that	O
our	O
method	O
,	O
denoted	O
as	O
meta	O
adpt	O
,	O
consistently	O
improves	O
the	O
performance	O
over	O
JointPair	O
baselines	O
,	O
while	O
ordinary	O
adapters	O
(	O
adpt	O
)	O
perform	O
worse	O
than	O
JointPair	O
.	O
This	O
shows	O
that	O
,	O
the	O
proposed	O
method	O
can	O
effectively	O
utilize	O
the	O
added	O
language	O
-	O
specific	O
adapters	O
to	O
improve	O
generalization	O
of	O
shared	O
parameters	O
across	O
languages	O
.	O
At	O
the	O
same	O
time	O
,	O
our	O
method	O
also	O
mitigates	O
negative	O
interference	O
and	O
outperforms	O
JointPair	O
on	O
withinlanguage	O
performance	O
,	O
closing	O
the	O
gap	O
with	O
monolingual	O
models	O
.	O
In	O
particular	O
,	O
it	O
performs	O
better	O
than	O
ordinary	O
adapters	O
in	O
both	O
settings	O
.	O
We	O
hypothesize	O
that	O
this	O
is	O
because	O
it	O
alleviates	O
language	O
conflicts	O
during	O
training	O
and	O
thus	O
converges	O
more	O
robustly	O
.	O
For	O
example	O
,	O
we	O
plot	O
training	O
loss	B-MetricName
in	O
the	O
early	O
stage	O
in	O
Figure	O
4c	O
,	O
which	O
shows	O
that	O
ordinary	O
adapters	O
converge	O
slower	O
than	O
JointPair	O
due	O
to	O
overfitting	O
of	O
language	O
-	O
specific	O
adapters	O
while	O
meta	O
adapters	O
converge	O
much	O
faster	O
.	O
For	O
ablation	O
studies	O
,	O
we	O
also	O
report	O
results	O
for	O
JointPair	O
trained	O
with	O
adapters	O
shared	O
between	O
two	O
languages	O
,	O
denoted	O
as	O
share	O
adpt	O
.	O
Unlike	O
language	O
-	O
specific	O
adapters	O
that	O
can	O
hinder	O
transferability	O
,	O
shared	O
adapters	O
improve	O
both	O
within	O
-	O
language	O
and	O
cross	O
-	O
lingual	O
performance	O
with	O
the	O
extra	O
capacity	O
.	O
However	O
,	O
meta	O
adapters	O
still	O
obtain	O
better	O
performance	O
.	O
These	O
results	O
show	O
that	O
mitigating	O
negative	O
interference	O
can	O
improve	O
multilingual	O
representations	O
.	O

Notice	O
that	O
XNLI	B-DatasetName
only	O
has	O
training	O
data	O
in	O
available	O
in	O
English	O
so	O
we	O
only	O
evaluate	O
zero	O
-	O
shot	O
crosslingual	O
performance	O
on	O
it	O
.	O
Following	O
(	O
Hu	O
et	O
al	O
,	O
2020	O
)	O
,	O
we	O
finetune	O
the	O
model	O
for	O
10	O
epochs	O
for	O
NER	B-TaskName
and	O
POS	O
,	O
2	O
epochs	O
for	O
QA	O
and	O
200	O
epochs	O
for	O
XNLI	B-DatasetName
.	O
For	O
NER	B-TaskName
,	O
POS	O
and	O
QA	O
,	O
we	O
search	O
the	O
following	O
hyperparameters	O
:	O
batch	B-HyperparameterName
size	I-HyperparameterName
{	O
16	O
,	O
32	O
}	O
;	O
learning	B-HyperparameterName
rate	I-HyperparameterName
{	O
2e	O
-	O
5	O
,	O
3e	O
-	O
5	O
,	O
5e	O
-	O
5	O
}	O
.	O
We	O
use	O
English	O
dev	O
set	O
for	O
zero	O
-	O
shot	O
cross	O
-	O
lingual	O
setting	O
and	O
the	O
target	O
language	O
dev	O
set	O
for	O
within	O
-	O
language	O
monolingual	O
setting	O
.	O
For	O
XNLI	B-DatasetName
,	O
we	O
search	O
for	O
:	O
batch	B-HyperparameterName
size	I-HyperparameterName
{	O
4	O
,	O
8	O
}	O
;	O
encoder	O
learning	B-HyperparameterName
rate	I-HyperparameterName
{	O
1e	O
-	O
6	O
,	O
5e	O
-	O
6	O
,	O
2e	O
-	O
5	O
}	O
;	O
classifier	O
learning	B-HyperparameterName
rate	I-HyperparameterName
{	O
5e	O
-	O
6	O
,	O
2e	O
-	O
5	O
,	O
5e	O
-	O
5	O
}	O
.	O
For	O
models	O
with	O
language	O
-	O
specific	O
components	O
,	O
we	O
test	O
freezing	O
these	O
components	O
or	O
finetuning	O
them	O
together	O
.	O
We	O
discover	O
that	O
finetuning	O
the	O
whole	O
network	O
always	O
yields	O
better	O
results	O
.	O
For	O
all	O
experiments	O
,	O
we	O
save	O
checkpoint	O
after	O
each	O
epoch	O
.	O

Let	O
z	O
i	O
be	O
the	O
output	O
of	O
the	O
i	O
-	O
th	O
layer	O
of	O
dimension	O
d.	O
The	O
residual	O
adapter	O
network	O
(	O
Rebuffi	O
et	O
al	O
,	O
2017	O
;	O
Houlsby	O
et	O
al	O
,	O
2019	O
;	O
)	O
is	O
a	O
bottleneck	O
layer	O
that	O
first	O
projects	O
z	O
i	O
to	O
an	O
inner	O
layer	O
with	O
dimension	O
b	O
:	O
h	O
i	O
=	O
g	O
(	O
W	O
z	O
i	O
z	O
i	O
)	O
(	O
6	O
)	O
where	O
W	O
z	O
i	O
R	O
d×b	O
and	O
g	O
is	O
some	O
activation	B-HyperparameterName
function	I-HyperparameterName
such	O
as	O
relu	B-MethodName
.	O
It	O
is	O
then	O
projected	O
back	O
to	O
the	O
original	O
input	O
dimension	O
d	O
with	O
a	O
residual	B-MethodName
connection	I-MethodName
:	O
o	O
i	O
=	O
W	O
h	O
i	O
h	O
i	O
+	O
z	O
i	O
(	O
7	O
)	O
where	O
W	O
h	O
i	O
R	O
b×d	O
.	O
In	O
our	O
experiments	O
,	O
we	O
fix	O
b	O
=	O
1	O
4	O
d.	O
For	O
a	O
bilingual	O
model	O
of	O
lg	O
1	O
and	O
lg	O
2	O
,	O
we	O
inject	O
two	O
langauge	O
-	O
specific	O
adapters	O
after	O
each	O
attention	O
and	O
feedforward	O
layer	O
,	O
one	O
for	O
each	O
language	O
.	O
For	O
example	O
,	O
if	O
the	O
input	O
text	O
is	O
in	O
lg	O
1	O
,	O
the	O
network	O
will	O
be	O
routed	O
to	O
adapters	O
designated	O
for	O
lg	O
1	O
.	O
The	O
rest	O
of	O
the	O
network	O
and	O
training	O
protocol	O
remain	O
unchanged	O
.	O
The	O
injected	O
adapter	O
layers	O
mimic	O
the	O
warp	O
layers	O
interleaved	O
between	O
base	O
network	O
layers	O
in	O
Flennerhag	O
et	O
al	O
(	O
2019	O
)	O
.	O
Warp	O
layers	O
are	O
meta	O
parameters	O
that	O
aim	O
to	O
improve	O
the	O
performance	O
of	O
the	O
base	O
network	O
.	O
They	O
precondition	O
base	O
network	O
gradients	O
to	O
obtain	O
better	O
gradient	O
geometry	O
.	O
In	O
our	O
experiments	O
,	O
we	O
treat	O
language	O
-	O
specific	O
adapters	O
as	O
meta	O
parameters	O
to	O
improve	O
generalization	O
of	O
the	O
shared	O
network	O
.	O
The	O
algorithm	O
is	O
outlined	O
in	O
Algorithm	O
1	O
.	O
The	O
adapters	O
are	O
updated	O
according	O
to	O
Eq	O
3	O
,	O
which	O
doubles	O
the	O
memory	O
requirement	O
.	O
In	O
particular	O
,	O
the	O
high	O
-	O
order	O
term	O
in	O
Eq	O
4	O
requires	O
computing	O
the	O
gradient	O
of	O
gradient	O
.	O
In	O
practice	O
,	O
we	O
approximate	O
this	O
term	O
using	O
the	O
finite	O
difference	O
approximation	O
as	O
:	O
∇	O
φ	O
(	O
t	O
)	O
i	O
L	O
i	O
train	O
(	O
θ	B-HyperparameterName
+	O
,	O
φ	O
(	O
t	O
)	O
i	O
)	O
−	O
∇	O
φ	O
(	O
t	O
)	O
i	O
L	O
i	O
train	O
(	O
θ	B-HyperparameterName
−	O
,	O
φ	O
(	O
t	O
)	O
i	O
)	O
2	O
(	O
8	O
)	O
where	O
θ	B-HyperparameterName
±	O
=	O
θ	B-HyperparameterName
(	O
t	O
)	O
±	O
∇	O
θ	B-HyperparameterName
1	O
L	O
L	O
j=1	O
L	O
j	O
val	O
(	O
θ	B-HyperparameterName
,	O
φ	O
(	O
t	O
)	O
j	O
)	O
and	O
is	O
a	O
small	O
scalar	O
.	O
We	O
use	O
the	O
same	O
value	O
for	O
learning	O
rates	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
in	O
Eq	O
3	O
,	O
to	O
be	O
consistent	O
with	O
standard	O
learning	B-HyperparameterName
rate	I-HyperparameterName
schedule	O
used	O
in	O
XLM	B-MethodName
(	O
Lample	O
and	O
Conneau	O
,	O
2019	O
)	O
.	O

The	O
goal	O
of	O
Semantic	O
Text	B-TaskName
Similarity	I-TaskName
(	O
STS	B-TaskName
)	O
is	O
to	O
find	O
the	O
degree	O
of	O
overlap	O
in	O
the	O
meaning	O
of	O
two	O
pieces	O
of	O
text	O
.	O
This	O
ranges	O
from	O
text	O
fragments	O
that	O
are	O
exact	O
semantic	O
equivalents	O
,	O
to	O
others	O
that	O
have	O
no	O
semantic	O
relation	O
.	O
STS	B-TaskName
has	O
a	O
wide	O
variety	O
of	O
applications	O
,	O
including	O
text	O
summarisation	O
(	O
Aliguliyev	O
,	O
2009	O
)	O
,	O
machine	B-TaskName
translation	I-TaskName
(	O
Kauchak	O
and	O
Barzilay	O
,	O
2006	O
)	O
,	O
and	O
search	O
optimisation	O
(	O
Sriram	O
et	O
al	O
,	O
2010	O
)	O
.	O
The	O
STS	B-TaskName
task	O
,	O
which	O
has	O
been	O
set	O
by	O
the	O
SemEval	O
conference	O
for	O
the	O
past	O
number	O
of	O
years	O
(	O
Agirre	O
et	O
al	O
,	O
2014	O
;	O
Agirre	O
et	O
al	O
,	O
2015	O
)	O
,	O
requires	O
that	O
submitted	O
systems	O
assign	O
a	O
score	O
between	O
0	B-DatasetName
(	O
the	O
sentences	O
are	O
on	O
different	O
topics	O
)	O
and	O
5	O
(	O
the	O
sentences	O
mean	O
exactly	O
the	O
same	O
thing	O
)	O
that	O
reflects	O
how	O
similar	O
two	O
sentences	O
are	O
.	O
Most	O
systems	O
that	O
tackled	O
SemEval	O
's	O
STS	B-TaskName
task	O
in	O
previous	O
years	O
have	O
involved	O
three	O
main	O
approaches	O
:	O
The	O
first	O
is	O
text	O
alignment	O
,	O
based	O
on	O
the	O
content	O
words	O
'	O
meaning	O
(	O
Sultan	O
et	O
al	O
,	O
2015	O
;	O
Sultan	O
et	O
al	O
,	O
2014b	O
)	O
.	O
The	O
second	O
represents	O
text	O
as	O
vectors	O
,	O
which	O
are	O
used	O
to	O
find	O
the	O
similarity	O
score	O
using	O
a	O
vector	O
similarity	O
metric	O
(	O
such	O
as	O
cosine	O
)	O
.	O
Third	O
,	O
machine	O
learning	O
approaches	O
are	O
used	O
to	O
compute	O
multiple	O
lexical	O
,	O
semantic	O
,	O
and	O
syntactic	O
features	O
to	O
classify	O
each	O
sentence	O
pair	O
's	O
similarity	O
.	O
We	O
make	O
use	O
of	O
both	O
text	O
alignments	O
and	O
vector	O
representations	O
,	O
while	O
limiting	O
comparisons	O
to	O
words	O
of	O
the	O
same	O
Type	O
(	O
Section	O
4.1	O
)	O
,	O
a	O
novel	O
concept	O
we	O
introduce	O
in	O
addition	O
to	O
methods	O
of	O
phrase	O
linking	O
(	O
Section	O
4.2	O
)	O
and	O
establishing	O
common	O
noun	O
importance	O
(	O
Section	O
4.3	O
)	O
.	O
These	O
,	O
combined	O
with	O
several	O
different	O
weight	O
combinations	O
we	O
pick	O
for	O
each	O
word	O
Type	O
,	O
provide	O
us	O
with	O
8	O
,	O
370	O
semantic	O
relation	O
measures	O
(	O
Section	O
5	O
)	O
.	O
The	O
overall	O
algorithm	O
for	O
generating	O
the	O
several	O
similarity	O
measures	O
is	O
presented	O
in	O
Algorithm	O
1	O
.	O
We	O
choose	O
a	O
subset	O
of	O
these	O
measures	O
using	O
methods	O
detailed	O
in	O
Section	O
6.1	O
,	O
combine	O
them	O
with	O
a	O
limited	O
set	O
of	O
features	O
and	O
use	O
Support	O
Vector	O
Regression	O
and	O
Kernel	O
Ridge	O
Regression	O
to	O
generate	O
a	O
Similarity	O
Score	B-MetricName
(	O
Section	O
6.2	O
)	O
.	O
Our	O
approach	O
also	O
handles	O
definitions	O
separately	O
from	O
arbitrary	O
sentences	O
,	O
as	O
we	O
observed	O
that	O
their	O
structure	O
is	O
significantly	O
different	O
.	O
Since	O
the	O
test	O
data	O
provided	O
this	O
year	O
did	O
not	O
contain	O
a	O
definition	O
data	O
set	O
,	O
this	O
paper	O
focuses	O
on	O
our	O
generic	O
approach	O
,	O
with	O
definition	O
similarity	O
discussed	O
briefly	O
in	O
Section	O
7	O
.	O

Algorithm	O
1	O
provides	O
an	O
overview	O
of	O
the	O
system	O
we	O
use	O
to	O
generate	O
the	O
various	O
Similarity	O
Scores	O
,	O
We	O
call	O
each	O
combination	O
that	O
generates	O
a	O
score	O
a	O
"	O
Method	O
"	O
.	O
We	O
use	O
thirty	O
weights	O
for	O
Types	O
3	O
,	O
while	O
providing	O
the	O
option	O
of	O
dividing	O
the	O
scores	O
by	O
the	O
number	O
of	O
WordNet	O
Synsets	O
(	O
UseSSToWeight	O
)	O
,	O
which	O
captures	O
any	O
dilution	O
due	O
to	O
a	O
word	O
's	O
different	O
senses	O
.	O
We	O
also	O
scale	O
word2vec	O
scores	O
by	O
different	O
values	O
.	O
This	O
gives	O
us	O
a	O
total	O
of	O
8	O
,	O
370	O
"	O
Methods	O
"	O
.	O
In	O
calculating	O
the	O
similarity	O
score	O
,	O
we	O
capture	O
the	O
fraction	O
of	O
each	O
Type	O
that	O
is	O
aligned	O
and	O
scale	O
it	O
by	O
the	O
weight	O
of	O
that	O
Type	O
.	O
This	O
is	O
captured	O
in	O
Equation	O
2	O
where	O
score	O
t	O
represents	O
the	O
Similarity	O
Score	B-MetricName
assigned	O
to	O
Type	O
t	O
by	O
either	O
of	O
the	O
measures	O
detailed	O
in	O
Section	O
3	O
,	O
count	O
t	O
represents	O
the	O
number	O
of	O
words	O
of	O
Type	O
t	O
in	O
both	O
sentences	O
,	O
w	O
t	O
the	O
weight	O
of	O
Type	O
t	O
in	O
the	O
current	O
iteration	O
,	O
and	O
T	O
is	O
the	O
total	O
number	O
of	O
Types	O
.	O

We	O
first	O
select	O
a	O
subset	O
of	O
the	O
Methods	O
,	O
which	O
are	O
then	O
passed	O
on	O
to	O
either	O
the	O
SVR	O
or	O
KRR	O
model	O
.	O
To	O
do	O
this	O
,	O
each	O
of	O
our	O
Methods	O
is	O
ranked	O
using	O
three	O
metrics	O
with	O
respect	O
to	O
the	O
training	O
set	O
:	O
The	O
first	O
is	O
by	O
use	O
of	O
the	O
Pearson	B-MetricName
Correlation	I-MetricName
(	O
a	O
criterion	O
we	O
call	O
"	O
Method	O
"	O
)	O
,	O
the	O
second	O
is	O
by	O
the	O
sum	O
of	O
the	O
absolute	O
error	O
between	O
Similarity	O
Scores	O
(	O
a	O
criterion	O
we	O
call	O
"	O
Error	B-MetricName
"	O
)	O
.	O
The	O
third	O
metric	O
aggregates	O
the	O
the	O
rankings	O
from	O
the	O
two	O
criterion	O
described	O
above	O
,	O
and	O
we	O
call	O
this	O
criterion	O
"	O
Combine	O
"	O
.	O
We	O
select	O
the	O
top	O
50	O
methods	O
using	O
one	O
of	O
the	O
three	O
selection	O
criterion	O
.	O

In	O
order	O
to	O
find	O
similarities	O
between	O
definitions	O
,	O
we	O
first	O
identify	O
the	O
word	O
that	O
a	O
definition	O
is	O
defining	O
.	O
We	O
achieve	O
this	O
by	O
use	O
of	O
OneLook	O
's	O
reverse	O
dictionary	O
search	O
5	O
,	O
which	O
returns	O
a	O
number	O
of	O
candidate	O
words	O
for	O
a	O
given	O
definition	O
.	O
For	O
each	O
definition	O
,	O
the	O
similarity	O
of	O
the	O
top	O
10	O
candidates	O
is	O
then	O
computed	O
using	O
Word2Vec	O
and	O
five	O
similarity	O
metrics	O
provided	O
by	O
WordNet	O
:	O
Path	O
distance	O
,	O
Leacock	O
-	O
Chodorow	O
,	O
Wu	O
and	O
Palmer	O
,	O
Jiang	O
-	O
Conrath	O
and	O
Lin	O
.	O
The	O
final	O
score	O
is	O
scaled	O
between	O
0	B-DatasetName
and	O
5	O
and	O
averaged	O
across	O
the	O
10	O
candidates	O
returned	O
by	O
OneLook	O
.	O
We	O
found	O
this	O
method	O
of	O
calculating	O
similarities	O
between	O
definitions	O
to	O
be	O
very	O
good	O
at	O
telling	O
if	O
two	O
definitions	O
refer	O
to	O
the	O
same	O
word	O
,	O
but	O
not	O
ideally	O
suited	O
for	O
measuring	O
how	O
similar	O
they	O
are	O
.	O
As	O
a	O
consequence	O
,	O
we	O
found	O
that	O
results	O
were	O
clustered	O
around	O
0	B-DatasetName
and	O
5	O
.	O
The	O
system	O
produced	O
a	O
Pearson	B-MetricName
correlation	I-MetricName
of	O
0.69	O
on	O
the	O
SemEval	O
2014	O
definitions	O
data	O
set	O
.	O
We	O
list	O
the	O
performance	O
of	O
our	O
system	O
in	O
Table	O
2	O
.	O
Our	O
system	O
's	O
poor	O
performance	O
on	O
the	O
ans	O
-	O
ans	O
and	O
ques	O
-	O
ques	O
datasets	O
can	O
be	O
attributed	O
to	O
our	O
choice	O
of	O
training	O
data	O
,	O
which	O
,	O
although	O
well	O
suited	O
for	O
previous	O
years	O
,	O
was	O
not	O
well	O
suited	O
for	O
these	O
datasets	O
.	O
However	O
,	O
our	O
system	O
produces	O
State	O
of	O
the	O
Art	O
results	O
on	O
the	O
2015	O
Test	O
Sets	O
.	O
A	O
breakdown	O
of	O
each	O
of	O
the	O
run	O
's	O
performance	O
against	O
the	O
2015	O
STS	B-TaskName
data	O
set	O
is	O
provided	O
in	O
Table	O
3	O
.	O
We	O
note	O
that	O
the	O
results	O
we	O
have	O
reported	O
for	O
previous	O
State	O
of	O
Art	O
for	O
individual	O
data	O
sources	O
are	O
not	O
the	O
results	O
from	O
just	O
the	O
winning	O
system	O
but	O
the	O
State	O
of	O
Art	O
across	O
all	O
Systems	O
for	O
that	O
data	O
source	O
.	O
Our	O
system	O
also	O
achieves	O
comparable	O
results	O
(	O
0.7793	O
)	O
to	O
that	O
presented	O
by	O
Sultan	O
et	O
al	O
(	O
2015	O
)	O
(	O
0.779	O
)	O
on	O
the	O
2014	O
STS	B-TaskName
dataset	O
.	O
The	O
weighted	O
mean	O
reported	O
by	O
us	O
does	O
not	O
include	O
definitions	O
as	O
we	O
decided	O
to	O
consider	O
them	O
independently	O
.	O
Table	O
4	O
on	O
their	O
2015	O
System	O
.	O

String	O
Kernels	O
(	O
Lodhi	O
et	O
al	O
,	O
2001	O
)	O
provide	O
a	O
way	O
of	O
comparing	O
two	O
documents	O
,	O
based	O
on	O
the	O
inner	O
product	O
generated	O
by	O
all	O
substrings	O
of	O
length	O
n	O
,	O
typically	O
known	O
as	O
character	O
n	O
-	O
grams	O
.	O
Being	O
relatively	O
simple	O
to	O
use	O
and	O
implement	O
,	O
this	O
technique	O
has	O
many	O
applications	O
according	O
to	O
the	O
literature	O
(	O
Cozma	O
et	O
al	O
,	O
2018	O
;	O
Giménez	O
-	O
Pérez	O
et	O
al	O
,	O
2017	O
;	O
Masala	O
et	O
al	O
,	O
2017	O
;	O
Ionescu	O
et	O
al	O
,	O
2014Popescu	O
and	O
Ionescu	O
,	O
2013	O
)	O
,	O
with	O
emphasis	O
on	O
dialect	B-TaskName
identification	I-TaskName
and	O
the	O
good	O
results	O
obtained	O
for	O
this	O
task	O
in	O
previous	O
VarDial	O
evaluation	O
campaigns	O
(	O
Butnaru	O
and	O
Ionescu	O
,	O
2018b	O
;	O
Ionescu	O
and	O
Butnaru	O
,	O
2017	O
;	O
.	O
Similar	O
to	O
our	O
last	O
year	O
's	O
submission	O
for	O
the	O
SMG	O
-	O
CH	O
subtask	O
(	O
Gȃman	O
and	O
Ionescu	O
,	O
2020	O
)	O
,	O
we	O
employ	O
the	O
string	O
kernels	O
computed	O
by	O
the	O
efficient	O
algorithm	O
introduced	O
by	O
Popescu	O
et	O
al	O
(	O
2017	O
)	O
.	O
This	O
gives	O
us	O
a	O
dual	O
representation	O
of	O
the	O
data	O
,	O
through	O
a	O
kernel	O
matrix	O
where	O
the	O
cell	O
on	O
row	O
i	O
and	O
column	O
j	O
represents	O
the	O
similarity	O
between	O
two	O
text	O
samples	O
x	O
i	O
and	O
x	O
j	O
.	O
In	O
our	O
experiments	O
,	O
we	O
consider	O
the	O
presence	O
bits	O
string	O
kernel	O
(	O
Popescu	O
and	O
Ionescu	O
,	O
2013	O
)	O
as	O
the	O
similarity	O
function	O
.	O
For	O
two	O
strings	O
x	O
i	O
and	O
x	O
j	O
over	O
a	O
set	O
of	O
characters	O
S	O
,	O
the	O
presence	O
bits	O
string	O
kernel	O
is	O
defined	O
as	O
follows	O
:	O
where	O
n	O
is	O
the	O
length	O
of	O
n	O
-	O
grams	O
and	O
#	O
(	O
x	O
,	O
g	O
)	O
is	O
a	O
function	O
that	O
returns	O
1	O
when	O
the	O
number	O
of	O
occurrences	O
of	O
n	O
-	O
gram	O
g	O
in	O
x	O
is	O
greater	O
than	O
1	O
,	O
and	O
0	B-DatasetName
otherwise	O
.	O
k	O
0/1	O
(	O
x	O
i	O
,	O
x	O
j	O
)	O
=	O
g	O
S	O
n	O
#	O
(	O
x	O
i	O
,	O
g	O
)	O
#	O
(	O
x	O
j	O
,	O
g	O
)	O
,	O
(	O
1	O
)	O
The	O
resulting	O
kernel	O
matrix	O
is	O
plugged	O
into	O
a	O
ν	O
-	O
Support	O
Vector	O
Regression	O
(	O
ν	O
-	O
SVR	O
)	O
model	O
.	O
SVR	O
(	O
Drucker	O
et	O
al	O
,	O
1997	O
)	O
is	O
a	O
modified	O
Support	O
Vector	O
Machines	O
(	O
SVM	B-MethodName
)	O
(	O
Cortes	O
and	O
Vapnik	O
,	O
1995	O
)	O
model	O
that	O
is	O
repurposed	O
for	O
regression	O
.	O
Similar	O
to	O
SVM	B-MethodName
,	O
SVR	O
uses	O
the	O
notion	O
of	O
support	O
vectors	O
and	O
margin	O
in	O
order	O
to	O
find	O
an	O
optimal	O
estimator	O
.	O
However	O
,	O
instead	O
of	O
a	O
separating	O
hyperplane	O
,	O
SVR	O
aims	O
to	O
find	O
a	O
hyperplane	O
that	O
estimates	O
the	O
data	O
points	O
(	O
support	O
vectors	O
)	O
within	O
the	O
margin	O
with	O
minimal	O
error	O
.	O
In	O
our	O
experiments	O
,	O
we	O
employ	O
an	O
equivalent	O
SVR	O
formulation	O
known	O
as	O
ν	O
-	O
SVR	O
(	O
Chang	O
and	O
Lin	O
,	O
2002	O
)	O
,	O
where	O
ν	O
is	O
the	O
configurable	O
proportion	O
of	O
support	O
vectors	O
to	O
keep	O
with	O
respect	O
to	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
in	O
the	O
data	O
set	O
.	O
Using	O
ν	O
-	O
SVR	O
,	O
the	O
optimal	O
solution	O
can	O
converge	O
to	O
a	O
sparse	O
model	O
,	O
with	O
only	O
a	O
few	O
support	O
vectors	O
.	O
This	O
is	O
especially	O
useful	O
in	O
our	O
case	O
,	O
as	O
the	O
data	O
set	O
provided	O
for	O
the	O
SMG	O
-	O
CH	O
subtask	O
does	O
not	O
contain	O
too	O
many	O
samples	O
.	O
Another	O
reason	O
to	O
employ	O
ν	O
-	O
SVR	O
in	O
our	O
regression	O
task	O
is	O
that	O
it	O
was	O
found	O
to	O
surpass	O
other	O
regression	O
methods	O
for	O
other	O
use	O
cases	O
,	O
such	O
as	O
complex	B-TaskName
word	I-TaskName
identification	I-TaskName
(	O
Butnaru	O
and	O
Ionescu	O
,	O
2018a	O
)	O
.	O

Characters	O
are	O
the	O
base	O
units	O
in	O
building	O
words	O
that	O
exist	O
in	O
the	O
vocabulary	O
of	O
most	O
languages	O
.	O
Among	O
the	O
advantages	O
of	O
working	O
at	O
the	O
character	O
level	O
,	O
we	O
enumerate	O
(	O
i	O
)	O
the	O
neutrality	O
with	O
respect	O
to	O
language	O
theory	O
(	O
independence	O
of	O
word	O
boundaries	O
,	O
semantic	O
structure	O
or	O
syntax	O
)	O
and	O
(	O
ii	O
)	O
the	O
robustness	O
to	O
spelling	O
errors	O
and	O
words	O
that	O
are	O
outside	O
the	O
vocabulary	O
(	O
Ballesteros	O
et	O
al	O
,	O
2015	O
)	O
.	O
These	O
explain	O
the	O
growing	O
interest	O
in	O
using	O
characters	O
as	O
features	O
in	O
various	O
language	O
modeling	O
setups	O
(	O
Al	O
-	O
Rfou	O
et	O
al	O
,	O
2019	O
;	O
Ballesteros	O
et	O
al	O
,	O
2015	O
;	O
Sutskever	O
et	O
al	O
,	O
2011	O
;	O
Wood	O
et	O
al	O
,	O
2009	O
;	O
Zhang	O
et	O
al	O
,	O
2015	O
)	O
.	O
Word	B-TaskName
embeddings	I-TaskName
are	O
vectorial	O
word	O
representations	O
that	O
associate	O
similar	O
vectors	O
to	O
semanti	O
-	O
cally	O
related	O
words	O
,	O
allowing	O
us	O
to	O
express	O
semantic	O
relations	O
mathematically	O
in	O
the	O
generated	O
embedding	O
space	O
.	O
From	O
the	O
initial	O
works	O
of	O
Bengio	O
et	O
al	O
(	O
2003	O
)	O
and	O
Schütze	O
(	O
1993	O
)	O
to	O
the	O
recent	O
improvements	O
in	O
the	O
quality	O
of	O
the	O
embedding	O
and	O
the	O
training	O
time	O
(	O
Collobert	O
and	O
Weston	O
,	O
2008	O
;	O
Mikolov	O
et	O
al	O
,	O
2013a	O
,	O
b	O
;	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
generating	O
meaningful	O
representations	O
of	O
words	O
became	O
a	O
hot	O
topic	O
in	O
the	O
NLP	O
research	O
community	O
.	O
These	O
improvements	O
,	O
and	O
many	O
others	O
not	O
mentioned	O
here	O
,	O
have	O
been	O
extensively	O
used	O
in	O
various	O
NLP	O
tasks	O
(	O
Garg	O
et	O
al	O
,	O
2018	O
;	O
Glorot	O
et	O
al	O
,	O
2011	O
;	O
Musto	O
et	O
al	O
,	O
2016	O
)	O
.	O
Considering	O
the	O
sometimes	O
orthogonal	O
benefits	O
of	O
character	O
and	O
word	B-TaskName
embeddings	I-TaskName
,	O
an	O
intuitive	O
idea	O
has	O
emerged	O
,	O
namely	O
that	O
of	O
combining	O
the	O
character	O
and	O
word	O
representations	O
,	O
which	O
should	O
complement	O
each	O
other	O
in	O
various	O
aspects	O
and	O
provide	O
better	O
meaningful	O
cues	O
in	O
the	O
learning	O
process	O
of	O
hybrid	O
neural	O
architectures	O
(	O
Liang	O
et	O
al	O
,	O
2017	O
)	O
.	O
Thus	O
,	O
throughout	O
the	O
experiments	O
performed	O
in	O
this	O
work	O
,	O
we	O
choose	O
to	O
employ	O
a	O
hybrid	O
convolutional	O
neural	O
network	O
working	O
at	O
both	O
the	O
character	O
level	O
(	O
Zhang	O
et	O
al	O
,	O
2015	O
)	O
and	O
the	O
word	O
level	O
(	O
Kim	O
,	O
2014	O
)	O
.	O
The	O
hybrid	O
architecture	O
concatenates	O
two	O
CNNs	O
,	O
out	O
of	O
which	O
one	O
is	O
equipped	O
with	O
a	O
character	O
embedding	O
layer	O
and	O
the	O
other	O
has	O
an	O
analogous	O
word	B-TaskName
embeddings	I-TaskName
layer	O
.	O
The	O
networks	O
are	O
able	O
to	O
automatically	O
learn	O
a	O
2D	O
representation	O
of	O
text	O
formed	O
of	O
either	O
character	O
or	O
word	O
embedding	O
vectors	O
,	O
that	O
are	O
further	O
processed	O
by	O
convolutional	O
and	O
fullyconnected	O
layers	O
.	O
The	O
last	O
convolutional	O
activation	O
maps	O
of	O
our	O
two	O
CNNs	O
sharing	O
similar	O
architectural	O
choices	O
are	O
concatenated	O
in	O
what	O
we	O
call	O
a	O
hybrid	O
network	O
(	O
Liang	O
et	O
al	O
,	O
2017	O
)	O
,	O
with	O
the	O
aim	O
of	O
accurately	O
and	O
simultaneously	O
predicting	O
the	O
two	O
location	O
components	O
required	O
for	O
the	O
geolocation	O
task	O
.	O
The	O
first	O
component	O
of	O
the	O
hybrid	O
network	O
is	O
a	O
characterlevel	O
CNN	O
,	O
which	O
takes	O
the	O
first	O
and	O
last	O
250	O
characters	O
in	O
the	O
input	O
and	O
encodes	O
each	O
character	O
with	O
its	O
position	O
in	O
the	O
alphabet	O
,	O
then	O
learns	O
end	O
-	O
to	O
-	O
end	O
embeddings	O
for	O
each	O
character	O
,	O
as	O
vectors	O
of	O
128	O
components	O
.	O
The	O
second	O
CNN	O
used	O
as	O
part	O
of	O
the	O
hybrid	O
network	O
operates	O
at	O
the	O
word	O
level	O
and	O
it	O
receives	O
as	O
input	O
each	O
sample	O
encoded	O
,	O
initially	O
,	O
as	O
an	O
array	O
of	O
100	O
indexes	O
,	O
corresponding	O
to	O
the	O
position	O
of	O
each	O
word	O
in	O
the	O
vocabulary	O
.	O
As	O
part	O
of	O
the	O
pre	O
-	O
processing	O
for	O
the	O
word	O
-	O
level	O
CNN	O
,	O
we	O
split	O
the	O
initial	O
text	O
into	O
words	O
,	O
keeping	O
the	O
first	O
50	O
words	O
and	O
the	O
last	O
50	O
words	O
in	O
the	O
sample	O
.	O
In	O
the	O
end	O
,	O
we	O
employ	O
the	O
German	O
Snowball	O
Stemmer	O
(	O
Weissweiler	O
and	O
Fraser	O
,	O
2018	O
)	O
to	O
reduce	O
each	O
word	O
to	O
its	O
stem	O
,	O
in	O
an	O
effort	O
to	O
reduce	O
the	O
vocabulary	O
size	O
by	O
mapping	O
variations	O
of	O
the	O
same	O
word	O
to	O
a	O
single	O
vocabulary	O
entry	O
.	O
The	O
word	O
-	O
level	O
CNN	O
is	O
also	O
equipped	O
with	O
an	O
embedding	O
layer	O
,	O
learning	O
end	O
-	O
to	O
-	O
end	O
word	O
representations	O
as	O
vectors	O
of	O
length	O
128	O
.	O
Each	O
of	O
the	O
two	O
CNNs	O
has	O
three	O
convolutional	O
(	O
conv	O
)	O
layers	O
placed	O
after	O
the	O
initial	O
embedding	O
layer	O
.	O
The	O
number	O
of	O
filters	O
decreases	O
from	O
1024	O
for	O
the	O
first	O
conv	O
layer	O
to	O
728	O
for	O
the	O
second	O
conv	O
layer	O
and	O
to	O
512	O
for	O
the	O
third	O
conv	O
layer	O
.	O
Each	O
conv	O
layer	O
is	O
equipped	O
with	O
Rectified	B-MethodName
Linear	I-MethodName
Units	I-MethodName
(	O
ReLU	B-MethodName
)	O
(	O
Nair	O
and	O
Hinton	O
,	O
2010	O
)	O
as	O
the	O
activation	B-HyperparameterName
function	I-HyperparameterName
.	O
The	O
convolutional	O
filter	O
sizes	O
differ	O
across	O
the	O
two	O
convolutional	O
architectures	O
.	O
Hence	O
,	O
we	O
use	O
kernels	O
of	O
sizes	O
9	O
,	O
7	O
and	O
7	O
for	O
the	O
char	O
CNN	O
.	O
In	O
the	O
same	O
time	O
,	O
we	O
choose	O
7	O
,	O
5	O
and	O
3	O
as	O
appropriate	O
filter	O
sizes	O
for	O
the	O
conv	O
layers	O
of	O
the	O
word	O
CNN	O
.	O
In	O
the	O
char	O
CNN	O
,	O
each	O
conv	O
layer	O
is	O
followed	O
by	O
a	O
max	O
-	O
pooling	O
layer	O
with	O
filters	O
of	O
size	O
3	O
.	O
In	O
the	O
word	O
CNN	O
,	O
we	O
add	O
max	O
-	O
pooling	O
layers	O
only	O
after	O
the	O
first	O
two	O
conv	O
layers	O
.	O
The	O
pooling	O
filter	O
sizes	O
are	O
3	O
for	O
the	O
first	O
pooling	O
layer	O
and	O
2	O
for	O
the	O
second	O
pooling	O
layer	O
.	O
The	O
activation	O
maps	O
resulting	O
after	O
the	O
last	O
conv	O
blocks	O
of	O
the	O
char	O
and	O
the	O
word	O
CNNs	O
are	O
concatenated	O
and	O
the	O
hybrid	O
network	O
continues	O
with	O
four	O
fully	O
-	O
connected	O
(	O
fc	O
)	O
layers	O
with	O
ReLU	B-MethodName
activations	O
.	O
The	O
fc	O
layers	O
are	O
formed	O
of	O
512	O
,	O
256	O
,	O
128	O
and	O
64	O
individual	O
neural	O
units	O
,	O
respectively	O
.	O

Gradient	O
tree	O
boosting	O
(	O
Friedman	O
,	O
2001	O
)	O
is	O
based	O
on	O
training	O
a	O
tree	O
ensemble	O
model	O
in	O
an	O
additive	O
fashion	O
.	O
This	O
technique	O
has	O
been	O
successfully	O
used	O
in	O
classification	O
(	O
Li	O
,	O
2010	O
)	O
and	O
ranking	O
(	O
Burges	O
,	O
2010	O
)	O
problems	O
,	O
obtaining	O
notable	O
results	O
in	O
reputed	O
competitions	O
such	O
as	O
the	O
Netflix	O
Challenge	O
(	O
Bennett	O
and	O
Lanning	O
,	O
2007	O
)	O
.	O
Furthermore	O
,	O
gradient	O
tree	O
boosting	O
is	O
the	O
ensemble	O
method	O
of	O
choice	O
in	O
some	O
real	O
-	O
world	O
pipelines	O
running	O
in	O
production	O
(	O
He	O
et	O
al	O
,	O
2014	O
)	O
.	O
XGBoost	O
(	O
Chen	O
and	O
Guestrin	O
,	O
2016	O
)	O
is	O
a	O
tree	O
boosting	O
model	O
targeted	O
at	O
solving	O
large	O
-	O
scale	O
tasks	O
with	O
limited	O
computational	O
resources	O
.	O
This	O
approach	O
aims	O
at	O
parallelizing	O
tree	O
learning	O
while	O
also	O
trying	O
to	O
handle	O
various	O
sparsity	O
patterns	O
.	O
Overfitting	O
is	O
addressed	O
through	O
shrinkage	O
and	O
column	O
subsampling	O
.	O
Shrinkage	O
acts	O
as	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
reducing	O
the	O
influence	O
of	O
each	O
individual	O
tree	O
.	O
Column	O
subsampling	O
is	O
borrowed	O
from	O
Random	O
Forests	O
(	O
Breiman	O
,	O
2001	O
)	O
,	O
bearing	O
the	O
advantage	O
of	O
speeding	O
up	O
the	O
computations	O
.	O
In	O
the	O
experiments	O
,	O
we	O
employ	O
XGBoost	O
as	O
a	O
metalearner	O
over	O
the	O
individual	O
predictions	O
of	O
each	O
of	O
the	O
models	O
described	O
above	O
.	O
We	O
opted	O
for	O
XG	O
-	O
Boost	O
in	O
detriment	O
of	O
average	O
voting	O
and	O
a	O
ν	O
-	O
SVR	O
meta	O
-	O
learner	O
,	O
both	O
providing	O
comparatively	O
lower	O
performance	O
levels	O
in	O
a	O
set	O
of	O
preliminary	O
ensemble	O
experiments	O
.	O

SVR	O
based	O
on	O
string	O
kernels	O
.	O
We	O
compute	O
the	O
presence	O
bits	O
string	O
kernel	O
using	O
the	O
efficient	O
algorithm	O
proposed	O
by	O
Popescu	O
et	O
al	O
(	O
2017	O
)	O
.	O
In	O
order	O
to	O
find	O
the	O
optimal	O
range	O
of	O
n	O
-	O
grams	O
,	O
we	O
experiment	O
with	O
multiple	O
blended	O
spectrum	O
string	O
kernels	O
based	O
on	O
various	O
n	O
-	O
gram	O
ranges	O
that	O
include	O
n	O
-	O
grams	O
from	O
3	O
to	O
7	O
characters	O
long	O
.	O
The	O
best	O
performance	O
in	O
terms	O
of	O
both	O
mean	O
absolute	O
error	O
(	O
MAE	B-MetricName
)	O
and	O
mean	O
squared	O
error	O
(	O
MSE	B-MetricName
)	O
was	O
attained	O
by	O
a	O
string	O
kernel	O
based	O
on	O
the	O
blended	O
spectrum	O
of	O
3	O
to	O
5	O
character	O
n	O
-	O
grams	O
.	O
These	O
results	O
are	O
consistent	O
with	O
those	O
reported	O
by	O
Ionescu	O
and	O
Butnaru	O
(	O
2017	O
)	O
and	O
Gȃman	O
and	O
Ionescu	O
(	O
2020	O
)	O
,	O
suggesting	O
that	O
the	O
3	O
-	O
5	O
n	O
-	O
gram	O
range	O
is	O
optimal	O
for	O
German	O
dialect	B-TaskName
identification	I-TaskName
.	O
The	O
resulting	O
kernel	O
matrix	O
is	O
used	O
as	O
input	O
for	O
two	O
ν	O
-	O
SVR	O
models	O
,	O
optimized	O
for	O
predicting	O
the	O
latitude	O
and	O
longitude	O
(	O
in	O
degrees	O
)	O
,	O
respectively	O
.	O
For	O
each	O
of	O
the	O
two	O
regressors	O
,	O
we	O
tune	O
the	O
regularization	O
penalty	O
C	O
,	O
in	O
a	O
range	O
of	O
values	O
from	O
10	O
−4	O
to	O
10	O
4	O
.	O
Similarly	O
,	O
for	O
the	O
proportion	O
of	O
support	O
vectors	O
ν	O
,	O
we	O
consider	O
10	O
values	O
uniformly	O
covering	O
the	O
interval	O
(	O
0	B-DatasetName
,	O
1	O
]	O
with	O
a	O
step	O
of	O
0.1	O
.	O
To	O
search	O
for	O
the	O
optimal	O
combination	O
of	O
hyperparameters	O
,	O
we	O
apply	O
grid	O
search	O
.	O
Consistent	O
with	O
our	O
previous	O
study	O
that	O
inspired	O
this	O
choice	O
of	O
model	O
and	O
features	O
(	O
Gȃman	O
and	O
Ionescu	O
,	O
2020	O
)	O
,	O
for	O
both	O
regression	O
models	O
,	O
the	O
best	O
hyperparameter	O
combination	O
is	O
C	O
=	O
10	O
and	O
ν	O
=	O
0.5	O
.	O
Hybrid	O
CNN	O
.	O
As	O
regularization	O
for	O
each	O
conv	O
block	O
of	O
the	O
two	O
CNNs	O
,	O
we	O
introduce	O
Gaussian	O
noise	O
and	O
spatial	O
dropout	O
.	O
We	O
try	O
many	O
different	O
magnitudes	O
in	O
the	O
two	O
regularization	O
techniques	O
,	O
obtaining	O
a	O
few	O
slightly	O
different	O
variations	O
of	O
the	O
same	O
model	O
.	O
These	O
are	O
compared	O
against	O
each	O
other	O
using	O
grid	O
search	O
,	O
selecting	O
the	O
best	O
model	O
to	O
be	O
included	O
in	O
our	O
final	O
ensemble	O
.	O
The	O
rates	O
used	O
for	O
the	O
spatial	O
dropout	O
(	O
Tompson	O
et	O
al	O
,	O
2015	O
)	O
are	O
in	O
the	O
range	O
[	O
0.001	O
,	O
0.2	O
]	O
,	O
with	O
a	O
step	O
of	O
0.001	O
,	O
with	O
the	O
best	O
dropout	O
rate	O
deemed	O
to	O
be	O
0.007	O
.	O
For	O
the	O
Gaussian	O
noise	O
,	O
we	O
consider	O
standard	O
deviations	O
in	O
the	O
range	O
[	O
0.0001	O
,	O
0.01	O
]	O
,	O
with	O
the	O
best	O
results	O
obtained	O
with	O
a	O
standard	O
deviation	O
of	O
0.001	O
.	O
After	O
each	O
of	O
the	O
four	O
fully	O
-	O
connected	O
layers	O
,	O
we	O
employ	O
plain	O
dropout	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
with	O
a	O
rate	O
of	O
0.005	O
,	O
deemed	O
the	O
best	O
choice	O
in	O
the	O
range	O
[	O
0.001	O
,	O
0.5	O
]	O
.	O
In	O
the	O
optimization	O
phase	O
,	O
common	O
evaluation	O
metrics	O
such	O
as	O
the	O
mean	O
squared	O
error	O
(	O
MSE	B-MetricName
)	O
and	O
the	O
mean	O
absolute	O
error	O
(	O
MAE	B-MetricName
)	O
are	O
typically	O
used	O
to	O
measure	O
performance	O
.	O
In	O
addition	O
to	O
these	O
two	O
metrics	O
,	O
we	O
consider	O
another	O
error	O
function	O
as	O
candidate	O
for	O
the	O
loss	B-MetricName
to	O
be	O
minimized	O
,	O
namely	O
the	O
Huber	O
loss	B-MetricName
.	O
Although	O
minimizing	O
the	O
Huber	O
loss	B-MetricName
tends	O
to	O
give	O
optimal	O
values	O
for	O
the	O
classical	O
evaluation	O
metrics	O
,	O
we	O
finally	O
use	O
MSE	B-MetricName
as	O
loss	B-MetricName
,	O
given	O
that	O
it	O
seems	O
to	O
converge	O
to	O
better	O
results	O
in	O
terms	O
of	O
the	O
median	O
distance	O
,	O
which	O
is	O
the	O
official	O
evaluation	O
metric	O
in	O
the	O
SMG	O
shared	O
task	O
.	O
We	O
optimize	O
the	O
hybrid	O
CNN	O
using	O
the	O
Adam	B-MethodName
optimization	O
algorithm	O
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
10	O
−3	O
,	O
chosen	O
from	O
the	O
range	O
[	O
10	O
−5	O
,	O
10	O
−2	O
]	O
,	O
a	O
weight	B-MethodName
decay	I-MethodName
of	O
10	O
−7	O
,	O
selected	O
in	O
the	O
initial	O
range	O
[	O
10	O
−9	O
,	O
10	O
−6	O
]	O
,	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
decay	O
of	O
0.999	O
.	O
We	O
train	O
the	O
hybrid	O
architecture	O
on	O
mini	O
-	O
batches	O
of	O
96	O
samples	O
for	O
1000	O
epochs	O
with	O
early	B-MethodName
stopping	I-MethodName
.	O
The	O
network	O
included	O
in	O
the	O
final	O
ensemble	O
converged	O
in	O
136	O
epochs	O
.	O
Fine	O
-	O
Tuned	O
German	O
BERT	B-MethodName
.	O
We	O
fine	O
-	O
tune	O
three	O
BERT	B-MethodName
models	O
in	O
slightly	O
different	O
setups	O
.	O
The	O
pretrained	O
base	O
model	O
used	O
as	O
starting	O
point	O
is	O
the	O
cased	O
German	O
BERT	B-MethodName
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
set	O
the	O
maximum	O
sequence	O
length	O
to	O
128	O
,	O
applying	O
zero	O
-	O
padding	O
to	O
reach	O
the	O
fixed	O
length	O
for	O
the	O
samples	O
that	O
are	O
shorter	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
,	O
unanimously	O
used	O
,	O
is	O
32	O
,	O
with	O
a	O
training	O
that	O
goes	O
on	O
for	O
a	O
maximum	O
of	O
50	O
epochs	O
.	O
We	O
optimize	O
the	O
models	O
using	O
Adam	B-MethodName
,	O
with	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
α	B-HyperparameterName
=	O
5	O
10	O
−5	O
.	O
The	O
division	O
by	O
zero	O
is	O
prevented	O
by	O
the	O
introduction	O
of	O
=	O
10	O
−8	O
.	O
Moving	O
to	O
the	O
particularities	O
of	O
each	O
of	O
the	O
three	O
fine	O
-	O
tuned	O
BERT	B-MethodName
models	O
,	O
we	O
note	O
a	O
difference	O
in	O
the	O
choice	O
of	O
the	O
loss	B-MetricName
function	O
,	O
such	O
that	O
the	O
first	O
model	O
employs	O
the	O
L	O
2	O
loss	B-MetricName
(	O
MSE	B-MetricName
)	O
,	O
while	O
the	O
other	O
two	O
models	O
use	O
the	O
L	O
1	O
loss	B-MetricName
(	O
MAE	B-MetricName
)	O
.	O
Another	O
variation	O
is	O
introduced	O
by	O
the	O
text	O
truncation	O
choices	O
,	O
such	O
that	O
for	O
the	O
first	O
and	O
third	O
models	O
,	O
we	O
do	O
not	O
enforce	O
truncation	O
,	O
while	O
for	O
the	O
second	O
model	O
,	O
all	O
the	O
samples	O
are	O
enforced	O
to	O
align	O
to	O
the	O
provided	O
maximum	O
sequence	O
length	O
.	O
These	O
differences	O
are	O
reflected	O
in	O
the	O
number	O
epochs	O
required	O
for	O
complete	O
convergence	O
:	O
20	O
,	O
18	O
and	O
33	O
,	O
respectively	O
.	O
We	O
monitor	O
the	O
median	O
distance	O
for	O
early	B-MethodName
stopping	I-MethodName
and	O
observe	O
that	O
the	O
best	O
performance	O
upon	O
convergence	O
is	O
obtained	O
by	O
the	O
third	O
model	O
,	O
with	O
a	O
median	O
distance	O
of	O
30.17	O
km	O
on	O
the	O
validation	O
set	O
,	O
followed	O
by	O
the	O
results	O
of	O
the	O
first	O
(	O
30.63	O
km	O
)	O
and	O
second	O
(	O
33.86	O
km	O
)	O
models	O
,	O
respectively	O
.	O
In	O
the	O
ensemble	O
,	O
we	O
include	O
only	O
the	O
top	O
scoring	O
BERT	B-MethodName
model	O
.	O
Extreme	O
Gradient	O
Boosting	O
.	O
We	O
employed	O
XG	O
-	O
Boost	O
as	O
a	O
meta	O
-	O
learner	O
,	O
training	O
it	O
over	O
the	O
predictions	O
of	O
all	O
the	O
other	O
models	O
described	O
in	O
Section	O
3	O
.	O
We	O
selected	O
XGBoost	O
as	O
the	O
submission	O
candidate	O
,	O
as	O
it	O
provided	O
the	O
best	O
results	O
.	O
The	O
XGBoost	O
regressor	O
,	O
deemed	O
optimal	O
by	O
the	O
hyperparameter	O
tuning	O
,	O
has	O
default	O
values	O
for	O
most	O
parameters	O
2	O
,	O
except	O
for	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
estimators	I-HyperparameterName
and	O
the	O
maximum	O
tree	O
depth	O
.	O
We	O
set	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
estimators	I-HyperparameterName
to	O
100	O
for	O
the	O
latitude	O
regressor	O
and	O
to	O
1000	O
for	O
the	O
longitude	O
regressor	O
.	O
Similarly	O
,	O
the	O
maximum	B-HyperparameterName
depth	I-HyperparameterName
of	O
the	O
trees	O
is	O
10	O
for	O
the	O
latitude	O
model	O
and	O
20	O
for	O
the	O
longitude	O
one	O
.	O

As	O
evaluation	O
metrics	O
we	O
use	O
the	O
Strict	O
F1	B-MetricName
score	I-MetricName
,	O
which	O
is	O
commonly	O
adopted	O
for	O
this	O
task	O
(	O
Segura	O
-	O
Bedmar	O
et	O
al	O
,	O
2013	O
)	O
.	O
It	O
is	O
computed	O
at	O
the	O
entity	O
level	O
,	O
and	O
assigns	O
a	O
hit	O
only	O
in	O
case	O
of	O
perfect	O
match	O
between	O
the	O
labels	O
assigned	O
by	O
the	O
model	O
and	O
the	O
labels	O
in	O
the	O
gold	O
annotation	O
.	O
In	O
CADEC	O
around	O
10	O
%	O
of	O
mentions	O
are	O
discontinuous	O
(	O
Dai	O
et	O
al	O
,	O
2020	O
)	O
and	O
it	O
is	O
possible	O
to	O
have	O
overlaps	O
and	O
intersections	O
of	O
discontinuous	O
spans	O
.	O
We	O
performed	O
data	O
tidying	O
by	O
merging	O
overlapping	O
ADE	O
mentions	O
,	O
keeping	O
only	O
the	O
longer	O
span	O
(	O
as	O
it	O
is	O
customary	O
in	O
the	O
literature	O
)	O
and	O
splitting	O
discontinuous	O
spans	O
in	O
multiple	O
continuous	O
spans	O
.	O

Apart	O
from	O
the	O
original	O
BERT	B-MethodName
,	O
we	O
experimented	O
with	O
SpanBERT	O
,	O
for	O
its	O
peculiar	O
pretraining	O
procedure	O
which	O
focuses	O
on	O
predicting	O
and	O
encoding	O
spans	O
instead	O
of	O
single	O
words	O
,	O
and	O
with	O
four	O
BERT	B-MethodName
variants	O
with	O
in	O
-	O
domain	O
knowledge	O
,	O
which	O
differ	O
from	O
each	O
other	O
both	O
for	O
the	O
corpus	O
they	O
were	O
trained	O
on	O
and	O
for	O
the	O
kind	O
of	O
pretraining	O
.	O
BERT	B-MethodName
Standard	O
model	O
,	O
pretrained	O
on	O
general	O
purpose	O
texts	O
(	O
Wikipedia	O
and	O
BookCorpus	B-DatasetName
)	O
.	O
SpanBERT	O
This	O
model	O
is	O
pretrained	O
using	O
the	O
same	O
corpus	O
as	O
the	O
original	O
BERT	B-MethodName
,	O
so	O
it	O
comes	O
with	O
no	O
in	O
-	O
domain	O
knowledge	O
.	O
But	O
the	O
pretraining	O
procedure	O
makes	O
its	O
embeddings	O
more	O
appropriate	O
for	O
NER	B-TaskName
-	O
like	O
tasks	O
.	O
as	O
it	O
introduces	O
an	O
additional	O
loss	B-MetricName
called	O
Span	O
Boundary	O
Objective	O
(	O
SBO	O
)	O
,	O
alongside	O
the	O
traditional	O
Masked	O
Language	B-TaskName
Modelling	I-TaskName
(	O
MLM	B-DatasetName
)	O
used	O
for	O
BERT	B-MethodName
.	O
Let	O
us	O
consider	O
a	O
sentence	O
S	O
=	O
[	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O
.	O
.	O
,	O
w	O
k	O
]	O
and	O
its	O
substring	O
S	O
m	O
:	O
n	O
=	O
[	O
w	O
m	O
,	O
.	O
.	O
.	O
,	O
w	O
n	O
]	O
.	O
w	O
m−1	O
and	O
w	O
n+1	O
are	O
the	O
boundaries	O
of	O
S	O
m	O
:	O
n	O
(	O
the	O
words	O
immediately	O
preceding	O
and	O
following	O
it	O
)	O
.	O
We	O
mask	O
S	O
by	O
replacing	O
all	O
the	O
words	O
in	O
S	O
m	O
:	O
n	O
with	O
the	O
[	O
MASK	O
]	O
token	O
.	O
SpanBERT	O
reads	O
the	O
masked	O
version	O
of	O
S	O
and	O
returns	O
an	O
embedding	O
for	O
each	O
word	O
.	O
The	O
MLM	B-DatasetName
loss	B-MetricName
measures	O
if	O
it	O
is	O
possible	O
to	O
reconstruct	O
each	O
original	O
word	O
w	O
i	O
S	O
m	O
:	O
n	O
from	O
the	O
corresponding	O
embedding	O
.	O
The	O
SBO	O
loss	B-MetricName
measures	O
if	O
it	O
is	O
possible	O
to	O
reconstruct	O
each	O
w	O
i	O
S	O
m	O
:	O
n	O
using	O
the	O
embeddings	O
of	O
the	O
boundary	O
words	O
w	O
m−1	O
and	O
w	O
n+1	O
.	O
BioBERT	O
(	O
Lee	O
et	O
al	O
,	O
2020	O
)	O
,	O
pretrained	O
from	O
a	O
BERT	B-MethodName
checkpoint	O
,	O
on	O
PubMed	O
abstracts	O
.	O
The	O
authors	O
of	O
BioBERT	O
provide	O
different	O
versions	O
of	O
the	O
model	O
,	O
pretrained	O
on	O
different	O
corpora	O
.	O
We	O
selected	O
the	O
version	O
which	O
seemed	O
to	O
have	O
the	O
greatest	O
advantage	O
on	O
this	O
task	O
,	O
according	O
to	O
the	O
results	O
by	O
Lee	O
et	O
al	O
(	O
2020	O
)	O
.	O
We	O
chose	O
BioBERT	O
v1.1	O
(	O
+	O
PubMed	O
)	O
,	O
which	O
outperformed	O
other	O
BioBERT	O
v1.0	O
versions	O
(	O
including	O
the	O
ones	O
trained	O
on	O
full	O
texts	O
)	O
in	O
NER	B-TaskName
tasks	O
involving	O
Diseases	O
and	O
Drugs	O
.	O
Preliminary	O
experiments	O
against	O
BioBERT	O
v.1.0	O
(	O
+	O
PubMed+PMC	O
)	O
confirmed	O
this	O
behaviour	O
(	O
see	O
Appendix	O
D	O
)	O
.	O
BioClinicalBERT	O
(	O
Alsentzer	O
et	O
al	O
,	O
2019	O
)	O
,	O
pretrained	O
from	O
a	O
BioBERT	O
checkpoint	O
,	O
on	O
clinical	O
texts	O
from	O
the	O
MIMIC	B-DatasetName
-	I-DatasetName
III	I-DatasetName
database	O
.	O
SciBERT	O
(	O
Beltagy	O
et	O
al	O
,	O
2019	O
)	O
,	O
pretrained	O
from	O
scratch	O
,	O
on	O
papers	O
retrieved	O
from	O
Semantic	B-DatasetName
Scholar	I-DatasetName
(	O
82	O
%	O
of	O
medical	B-DatasetName
domain	I-DatasetName
)	O
.	O
PubMedBERT	O
(	O
Gu	O
et	O
al	O
,	O
2020	O
)	O
,	O
pretrained	O
from	O
scratch	O
,	O
on	O
PubMed	O
abstracts	O
and	O
full	O
text	O
articles	O
from	O
PubMed	O
Central	O
.	O
This	O
model	O
was	O
created	O
to	O
prove	O
that	O
pretraining	O
from	O
scratch	O
on	O
a	O
single	O
domain	O
produces	O
substantial	O
gains	O
on	O
in	O
-	O
domain	O
downstream	O
tasks	O
.	O
Gu	O
et	O
al	O
(	O
2020	O
)	O
compared	O
it	O
with	O
various	O
other	O
models	O
pretrained	O
on	O
either	O
general	O
texts	O
,	O
mixed	O
-	O
domain	O
texts	O
or	O
in	O
-	O
domain	O
texts	O
starting	O
from	O
a	O
general	O
-	O
purpose	O
checkpoint	O
(	O
e.g.	O
BioBERT	O
)	O
,	O
showing	O
that	O
PubMedBERT	O
outperforms	O
them	O
on	O
several	O
tasks	O
based	O
on	O
medical	O
language	O
.	O
The	O
vocabulary	O
of	O
PubMedBERT	O
contains	O
more	O
in	O
-	O
domain	O
medical	O
words	O
than	O
any	O
other	O
model	O
under	O
consideration	O
.	O
However	O
,	O
it	O
should	O
be	O
kept	O
in	O
mind	O
that	O
ADE	O
detection	O
requires	O
an	O
understanding	O
of	O
both	O
medical	O
terms	O
and	O
colloquial	O
language	O
,	O
as	O
both	O
can	O
occur	O
in	O
social	O
media	O
text	O
.	O
Notice	O
that	O
two	O
in	O
-	O
domain	O
architectures	O
were	O
pretrained	O
from	O
scratch	O
(	O
SciBERT	O
and	O
PubMed	O
-	O
BERT	B-MethodName
)	O
,	O
meaning	O
that	O
they	O
have	O
a	O
unique	O
vocabulary	O
tailored	O
on	O
their	O
pretraining	O
corpus	O
,	O
and	O
include	O
specific	O
embeddings	O
for	O
in	O
-	O
domain	O
words	O
.	O
BioBERT	O
and	O
BioClinicalBERT	O
were	O
instead	O
pretrained	O
starting	O
from	O
a	O
BERT	B-MethodName
and	O
BioBERT	O
checkpoint	O
,	O
respectively	O
.	O
This	O
means	O
that	O
the	O
vocabularies	O
are	O
built	O
from	O
general	O
-	O
domain	O
texts	O
(	O
similarly	O
to	O
BERT	B-MethodName
)	O
and	O
the	O
embeddings	O
are	O
initialized	O
likewise	O
.	O

As	O
a	O
strong	O
baseline	O
,	O
we	O
used	O
the	O
TMRLeiden	O
architecture	O
(	O
Dirkson	O
and	O
Verberne	O
,	O
2019	O
)	O
,	O
which	O
achieved	O
the	O
2nd	O
best	O
Strict	O
F1	B-MetricName
-	I-MetricName
Score	I-MetricName
in	O
the	O
latest	O
SMM4H	B-DatasetName
shared	O
task	O
(	O
Weissenbacher	O
et	O
al	O
,	O
2019	O
)	O
and	O
is	O
composed	O
of	O
a	O
BiLSTM	B-MethodName
taking	O
as	O
input	O
a	O
concatenation	O
of	O
BERT	B-MethodName
and	O
Flair	O
embeddings	O
(	O
Akbik	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
chose	O
this	O
baseline	O
since	O
the	O
TMRLeiden	O
code	O
is	O
publicly	O
available	O
.	O

TMRLeiden	O
was	O
re	O
-	O
implemented	O
starting	O
from	O
its	O
the	O
original	O
code	O
1	O
and	O
trained	O
according	O
to	O
the	O
details	O
in	O
the	O
paper	O
.	O
As	O
for	O
the	O
Transformers	O
,	O
all	O
experiments	O
were	O
performed	O
using	O
the	O
TRANSFORMERS	O
library	O
(	O
Wolf	O
et	O
al	O
,	O
2019	O
)	O
(	O
see	O
Appendix	O
C	O
)	O
.	O
Parameter	O
-	O
tuning	O
was	O
done	O
via	O
grid	O
-	O
search	O
,	O
using	O
different	O
learning	O
rates	O
(	O
[	O
5e−4	O
,	O
5e−5	O
,	O
5e−6	O
]	O
)	O
and	O
dropout	O
rates	O
(	O
from	O
0.15	O
to	O
0.30	O
,	O
increments	O
of	O
0.05	O
)	O
.	O
All	O
the	O
architectures	O
were	O
trained	O
for	O
50	O
epochs	O
on	O
the	O
training	O
set	O
.	O
Learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
dropout	O
rate	O
and	O
maximum	O
epoch	O
were	O
chosen	O
evaluating	O
the	O
models	O
on	O
the	O
validation	O
set	O
.	O
During	O
evaluation	O
all	O
the	O
models	O
were	O
then	O
trained	O
using	O
the	O
best	O
hyperparameters	O
on	O
the	O
concatenation	O
of	O
the	O
training	O
set	O
and	O
the	O
validation	O
set	O
,	O
and	O
tested	O
on	O
the	O
test	O
set	O
.	O
This	O
procedure	O
was	O
repeated	O
five	O
times	O
with	O
different	O
random	O
seeds	B-DatasetName
,	O
and	O
finally	O
we	O
averaged	O
the	O
results	O
over	O
the	O
five	O
runs	O
.	O

The	O
results	O
for	O
the	O
two	O
datasets	O
are	O
shown	O
in	O
Table	O
1	O
(	O
we	O
focus	O
on	O
the	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
,	O
but	O
Precision	B-MetricName
and	O
Recall	B-MetricName
are	O
reported	O
in	O
Appendix	O
D	O
)	O
.	O
For	O
reference	O
,	O
we	O
reported	O
the	O
scores	O
of	O
the	O
best	O
architecture	O
by	O
Dai	O
et	O
al	O
(	O
2020	O
)	O
,	O
which	O
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
system	O
on	O
CADEC	O
.	O
At	O
a	O
glance	O
,	O
all	O
systems	O
perform	O
better	O
on	O
CADEC	O
,	O
whose	O
texts	O
belong	O
to	O
a	O
more	O
standardized	O
variety	O
of	O
language	O
.	O
SpanBERT	O
and	O
PubMedBERT	O
emerge	O
as	O
the	O
top	O
performing	O
models	O
,	O
with	O
close	O
F1	B-MetricName
-	O
scores	O
,	O
and	O
in	O
particular	O
,	O
the	O
SpanBERT	O
models	O
achieve	O
the	O
top	O
score	O
on	O
both	O
datasets	O
,	O
proving	O
that	O
modeling	O
spans	O
gives	O
an	O
important	O
advantage	O
for	O
the	O
identification	O
of	O
ADEs	O
.	O
For	O
both	O
models	O
,	O
the	O
addition	O
of	O
CRF	B-MethodName
generally	O
determines	O
a	O
slight	O
improvement	O
on	O
CADEC	O
,	O
while	O
it	O
is	O
detrimental	O
on	O
SMM4H.	O
On	O
SMM4H	B-DatasetName
,	O
the	O
F1	B-MetricName
-	O
scores	O
of	O
BioBERT	O
,	O
SciBERT	O
and	O
Bio	B-DatasetName
-	O
ClinicalBERT	O
consistently	O
improve	O
over	O
the	O
standard	O
BERT	B-MethodName
,	O
but	O
they	O
are	O
outperformed	O
by	O
its	O
CRFaugmented	O
version	O
,	O
while	O
on	O
CADEC	O
they	O
perform	O
closely	O
to	O
the	O
standard	O
model	O
.	O
The	O
results	O
suggest	O
that	O
in	O
-	O
domain	O
knowledge	O
is	O
consistently	O
useful	O
only	O
when	O
training	O
is	O
done	O
on	O
in	O
-	O
domain	O
text	O
from	O
scratch	O
,	O
instead	O
of	O
using	O
general	O
domain	O
text	O
first	O
.	O
SciBERT	O
is	O
also	O
trained	O
from	O
scratch	O
,	O
but	O
on	O
a	O
corpus	O
that	O
is	O
less	O
specific	O
for	O
the	O
biomedical	O
domain	O
than	O
the	O
PubMedBERT	O
one	O
(	O
Gu	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
models	O
are	O
also	O
being	O
compared	O
with	O
TM	O
-	O
RLeiden	O
:	O
we	O
can	O
notice	O
that	O
both	O
versions	O
of	O
SpanBERT	O
and	O
PubMedBERT	O
outperform	O
it	O
on	O
CADEC	O
(	O
the	O
differences	O
are	O
also	O
statistically	O
significant	O
for	O
the	O
McNemar	O
test	O
at	O
p	O
<	O
0.001	O
)	O
,	O
while	O
only	O
the	O
basic	O
versions	O
of	O
the	O
same	O
models	O
retain	O
an	O
advantage	O
on	O
it	O
on	O
SMM4H	B-DatasetName
(	O
also	O
in	O
this	O
case	O
,	O
the	O
difference	O
is	O
significant	O
at	O
p	O
<	O
0.001	O
)	O
.	O

We	O
analyzed	O
the	O
differences	O
between	O
the	O
ADE	O
entities	O
correctly	O
identified	O
by	O
the	O
models	O
and	O
those	O
that	O
were	O
missed	O
,	O
using	O
the	O
text	O
statistics	O
that	O
we	O
previously	O
extracted	O
with	O
TEXTSTAT	O
.	O
As	O
it	O
was	O
1	O
@hospitalpatient	O
have	O
been	O
on	O
humira	O
2years	O
now	O
n	O
get	O
on	O
off	O
chest	O
infections	O
that	O
sometimes	O
need	O
2diff	O
pills	O
2sort	O
out	O
should	O
i	O
b	O
worried	O
?	O
4	O
i	O
have	O
had	O
no	O
side	O
effects	O
been	O
taking	O
arthrotec	O
a	O
little	O
over	O
a	O
year	O
,	O
have	O
not	O
noticed	O
any	O
side	O
effects	O
.	O
it	O
does	O
help	O
alot	O
i	O
noticed	O
that	O
when	O
there	O
are	O
times	O
when	O
i	O
forget	O
to	O
take	O
it	O
i	O
ca	O
n't	O
stand	O
or	O
walk	O
for	O
any	O
lengths	O
of	O
time	O
.	O
2	O
had	O
a	O
great	O
few	O
hours	O
on	O
my	O
bike	O
but	O
exercise	O
drives	O
my	O
olanzapine	O
#	O
munchies	O
.	O
getting	O
fed	O
up	O
with	O
not	O
being	O
able	O
to	O
fit	O
into	O
summer	O
wardrobe	O
5	O
works	O
just	O
fine	O
.	O
if	O
there	O
are	O
any	O
side	O
effects	O
,	O
they	O
are	O
definitely	O
not	O
noticeable	O
.	O
what	O
's	O
with	O
all	O
these	O
older	O
people	O
(	O
70	O
's	O
)	O
complaining	O
about	O
the	O
lack	O
of	O
sex	O
drive	O
?	O
how	O
much	O
of	O
what	O
you	O
are	O
complaining	O
about	O
is	O
simply	O
related	O
to	O
getting	O
older	O
?	O
3	O
this	O
new	O
baccy	O
is	O
just	O
making	O
my	O
cough	O
so	O
much	O
worse	O
but	O
ahh	O
well	O
need	O
my	O
nicotine	O
6	O
what	O
a	O
great	O
store	O
@walmart	O
is	O
:	O
i	O
loss	B-MetricName
iq	O
points	O
,	O
gained	O
weight	O
&	O
got	O
addicted	O
to	O
nicotine	O
-	O
all	O
in	O
under	O
15	O
min	O
from	O
going	O
in	O
!	O
!	O
SpanBERT	O
(	O
underlined	O
in	O
red	O
)	O
.	O
Actual	O
ADEs	O
in	O
bold	O
with	O
gray	O
background	O
.	O
The	O
Samples	O
belong	O
to	O
SMM4H	B-DatasetName
(	O
1	O
-	O
3	O
,	O
6	O
)	O
and	O
CADEC	O
(	O
4	O
-	O
5	O
)	O
.	O
predictable	O
,	O
it	O
turns	O
out	O
that	O
longer	O
ADE	O
spans	O
are	O
more	O
difficult	O
to	O
identify	O
:	O
for	O
all	O
models	O
,	O
we	O
extracted	O
the	O
average	O
word	O
length	O
of	O
correct	O
and	O
missed	O
spans	O
and	O
we	O
compared	O
them	O
with	O
a	O
twotailed	O
Mann	O
-	O
Whitney	O
U	O
test	O
,	O
finding	O
that	O
the	O
latter	O
are	O
significantly	O
longer	O
(	O
Z	O
=	O
-	O
6.176	O
,	O
p	O
<	O
0.001	O
)	O
.	O
We	O
also	O
extracted	O
the	O
average	O
number	O
of	O
difficult	O
words	O
in	O
the	O
correct	O
and	O
in	O
the	O
missed	O
spans	O
,	O
defined	O
as	O
words	O
with	O
more	O
than	O
two	O
syllables	O
that	O
are	O
not	O
included	O
in	O
the	O
TEXTSTAT	O
list	O
of	O
words	O
of	O
common	O
usage	O
in	O
standard	O
English	O
.	O
We	O
took	O
this	O
as	O
an	O
approximation	O
of	O
the	O
number	O
of	O
"	O
technical	O
"	O
terms	O
in	O
the	O
dataset	O
instances	O
.	O
However	O
,	O
the	O
average	O
values	O
for	O
correct	O
and	O
missed	O
instances	O
do	O
not	O
differ	O
(	O
Z	O
=	O
0.109	O
,	O
p	O
>	O
0.1	O
)	O
,	O
suggesting	O
that	O
the	O
presence	O
of	O
difficult	O
or	O
technical	O
words	O
in	O
a	O
given	O
instance	O
does	O
not	O
represent	O
an	O
inherent	O
factor	O
of	O
difficulty	O
or	O
facilitation	O
.	O
Still	O
,	O
for	O
some	O
of	O
the	O
models	O
-	O
including	O
SpanBERT	O
,	O
PubMedBERT	O
and	O
TMRLeiden	O
-	O
this	O
difference	O
reaches	O
a	O
marginal	O
significance	O
(	O
p	O
<	O
0.05	O
)	O
exclusively	O
on	O
the	O
SMM4H	B-DatasetName
dataset	O
,	O
where	O
correctly	O
identified	O
spans	O
have	O
more	O
difficult	O
words	O
.	O
A	O
possible	O
interpretation	O
is	O
that	O
,	O
as	O
the	O
tweets	O
'	O
language	O
is	O
more	O
informal	O
,	O
such	O
words	O
represent	O
a	O
stronger	O
ADE	O
cue	O
,	O
compared	O
to	O
the	O
more	O
technical	O
language	O
of	O
the	O
CADEC	O
dataset	O
.	O
Finally	O
,	O
we	O
performed	O
a	O
qualitative	O
analysis	O
,	O
comparing	O
the	O
predictions	O
of	O
SpanBERT	O
and	O
Pub	O
-	O
MedBERT	O
.	O
We	O
selected	O
the	O
samples	O
on	O
which	O
one	O
of	O
the	O
architectures	O
performed	O
significantly	O
better	O
than	O
the	O
other	O
one	O
in	O
terms	O
of	O
F1	B-MetricName
-	I-MetricName
Score	I-MetricName
,	O
and	O
analyzed	O
them	O
manually	O
.	O
Some	O
significant	O
samples	O
can	O
be	O
found	O
in	O
Table	O
2	O
.	O
We	O
observed	O
that	O
most	O
of	O
the	O
samples	O
in	O
which	O
PubMedBERT	O
performed	O
better	O
than	O
SpanBERT	O
contained	O
medical	O
terms	O
,	O
which	O
SpanBERT	O
had	O
completely	O
ignored	O
(	O
e.g.	O
Sample	O
1	O
)	O
.	O
The	O
samples	O
in	O
which	O
SpanBERT	O
outperformed	O
the	O
in	O
-	O
domain	O
model	O
contained	O
instead	O
long	O
ADE	O
mentions	O
,	O
often	O
associated	O
with	O
informal	O
descriptions	O
(	O
e.g.	O
Samples	O
2	O
,	O
3	O
)	O
.	O
As	O
regards	O
false	O
positives	O
,	O
both	O
models	O
make	O
similar	O
errors	O
,	O
which	O
fit	O
into	O
two	O
broad	O
categories	O
:	O
(	O
1	O
)	O
extracting	O
diseases	O
or	O
symptoms	O
of	O
a	O
disease	O
(	O
e.g.	O
Samples	O
4	O
,	O
6	O
)	O
;	O
(	O
2	O
)	O
not	O
being	O
able	O
to	O
handle	O
general	O
mentions	O
,	O
hypothetical	O
language	O
,	O
negations	O
and	O
similar	O
linguistic	O
constructs	O
(	O
e.g.	O
Sample	O
5	O
)	O
.	O
While	O
the	O
second	O
kind	O
of	O
error	O
requires	O
a	O
deeper	O
reflection	O
,	O
the	O
first	O
one	O
might	O
be	O
addressed	O
by	O
training	O
the	O
model	O
to	O
extract	O
multiple	O
kinds	O
of	O
entities	O
(	O
e.g.	O
both	O
ADEs	O
and	O
Diseases	O
)	O
.	O

Since	O
there	O
is	O
little	O
research	O
on	O
determining	O
the	O
best	O
fitting	O
bias	O
for	O
stance	B-TaskName
detection	I-TaskName
,	O
we	O
explore	O
three	O
different	O
classifiers	O
for	O
the	O
stance	B-TaskName
classification	I-TaskName
,	O
support	O
vector	O
machines	O
(	O
SVM	B-MethodName
)	O
,	O
random	O
forest	O
,	O
and	O
gradient	O
boosting	O
decision	O
trees	O
(	O
GBDT	O
)	O
.	O
For	O
all	O
three	O
classifiers	O
,	O
we	O
use	O
the	O
implementations	O
in	O
Scikit	O
-	O
Learn	O
(	O
Pedregosa	O
et	O
al	O
,	O
2011	O
)	O
.	O
We	O
choose	O
SVM	B-MethodName
because	O
it	O
is	O
the	O
most	O
widely	O
used	O
machine	O
learning	O
model	O
for	O
text	B-TaskName
classification	I-TaskName
and	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
e.g.	O
,	O
(	O
Pilászy	O
,	O
2005	O
)	O
)	O
.	O
Additionally	O
,	O
it	O
has	O
been	O
shown	O
to	O
be	O
robust	O
with	O
high	O
dimensional	O
features	O
(	O
e.g.	O
,	O
(	O
Joachims	O
,	O
1998	O
)	O
)	O
.	O
Random	O
forest	O
is	O
adopted	O
because	O
of	O
its	O
capability	O
of	O
reducing	O
overfitting	O
by	O
performing	O
sampling	O
on	O
data	O
points	O
and	O
on	O
feature	O
subspaces	O
.	O
GBDT	O
is	O
selected	O
because	O
it	O
works	O
well	O
with	O
continuous	O
numerical	O
features	O
such	O
as	O
word	O
vectors	O
.	O
We	O
train	O
individual	O
classifiers	O
for	O
each	O
target	O
.	O
Parameters	O
are	O
optimized	O
in	O
a	O
5	O
-	O
fold	O
cross	O
-	O
validation	O
over	O
the	O
training	O
data	O
.	O
SVM	B-MethodName
and	O
random	O
forest	O
are	O
trained	O
on	O
different	O
numbers	O
of	O
selected	O
unigrams	O
for	O
each	O
target	O
:	O
1	O
,	O
700	O
for	O
Abortion	O
,	O
1	O
,	O
535	O
for	O
Atheism	O
,	O
1	O
,	O
381	O
for	O
Climate	O
,	O
1	O
,	O
749	O
for	O
Feminist	O
,	O
and	O
1	O
,	O
704	O
for	O
Hillary	O
.	O
GBDT	O
is	O
trained	O
on	O
the	O
word	O
vectors	O
:	O
300	O
dimensions	O
for	O
word2vec	O
and	O
250	O
dimensions	O
for	O
GloVe	B-MethodName
.	O
Additional	O
experiments	O
are	O
performed	O
with	O
a	O
standard	O
feed	O
-	O
forward	O
neural	O
network	O
on	O
word	O
vectors	O
.	O
These	O
showed	O
better	O
performance	O
on	O
the	O
training	O
set	O
for	O
some	O
targets	O
,	O
but	O
overall	O
,	O
GBDT	O
prove	O
to	O
be	O
more	O
reliable	O
.	O
SVM	B-MethodName
Our	O
initial	O
experiments	O
using	O
cross	O
validation	O
on	O
training	O
data	O
showed	O
that	O
linear	O
kernel	O
performed	O
better	O
than	O
non	O
-	O
linear	O
ones	O
,	O
and	O
that	O
the	O
LinearSVC	O
implementation	O
(	O
one	O
-	O
vs	O
-	O
rest	O
strategy	O
for	O
multi	O
-	O
class	O
)	O
outperformed	O
SVC	O
(	O
one	O
-	O
vs	O
-	O
one	O
strategy	O
)	O
.	O
The	O
optimal	O
parameters	O
differ	O
for	O
each	O
target	O
:	O
0.015	O
-	O
0.3	O
for	O
the	O
slack	O
variable	O
;	O
standard	O
hinge	O
or	O
squared	O
hinge	O
for	O
the	O
loss	B-MetricName
function	O
;	O
and	O
L2	O
norm	O
for	O
the	O
penalty	O
term	O
.	O

The	O
parameters	O
for	O
random	O
forest	O
are	O
:	O
50	O
,	O
70	O
,	O
or	O
90	O
for	O
the	O
number	O
of	O
trees	O
;	O
500	O
or	O
All	O
for	O
the	O
number	O
of	O
features	O
to	O
consider	O
when	O
looking	O
for	O
the	O
best	O
split	O
;	O
200	O
,	O
500	O
,	O
or	O
unlimited	O
for	O
the	O
maximum	B-HyperparameterName
depth	I-HyperparameterName
of	O
trees	O
.	O
GBDT	O
The	O
gradient	O
boosting	O
decision	O
trees	O
(	O
GBDT	O
)	O
classifier	O
is	O
used	O
in	O
combination	O
with	O
word	O
vector	O
features	O
.	O
Our	O
initial	O
experiments	O
showed	O
that	O
GBDT	O
handles	O
word	O
vector	O
features	O
better	O
than	O
SVM	B-MethodName
and	O
random	O
forest	O
.	O
The	O
optimal	O
parameter	O
range	O
for	O
different	O
targets	O
are	O
:	O
80	O
-	O
100	O
for	O
number	B-HyperparameterName
of	I-HyperparameterName
estimators	I-HyperparameterName
;	O
0.05	O
-	O
0.3	O
for	O
learning	B-HyperparameterName
rate	I-HyperparameterName
;	O
false	O
for	O
warm	O
start	O
;	O
and	O
0.5	O
-	O
1.0	O
for	O
subsample	O
ratio	O
.	O
Ensemble	O
Classifier	O
Since	O
initial	O
experiments	O
with	O
the	O
three	O
classifiers	O
showed	O
considerable	O
differences	O
across	O
targets	O
and	O
stances	O
,	O
we	O
investigate	O
whether	O
an	O
ensemble	O
classifier	O
would	O
benefit	O
from	O
aggregating	O
their	O
predictions	O
.	O
For	O
the	O
ensemble	O
classifier	O
,	O
we	O
choose	O
a	O
memory	O
-	O
based	O
learner	O
,	O
TiMBL	O
,	O
because	O
of	O
the	O
need	O
to	O
operate	O
on	O
a	O
small	O
set	O
of	O
rather	O
abstract	O
features	O
:	O
stance	O
predictions	O
and	O
confidence	O
scores	O
from	O
the	O
three	O
classifiers	O
along	O
with	O
the	O
global	O
features	O
(	O
see	O
section	O
2.2.3	O
)	O
.	O
We	O
use	O
TiMBL	O
(	O
Daelemans	O
et	O
al	O
,	O
2009	O
)	O
version	O
6.4.2	O
,	O
and	O
perform	O
5	O
-	O
fold	O
jackknifing	O
to	O
generate	O
the	O
training	O
set	O
for	O
this	O
ensemble	O
classifier	O
.	O
Parameter	O
optimization	O
is	O
performed	O
on	O
the	O
five	O
folds	O
.	O
The	O
best	O
parameters	O
are	O
different	O
in	O
each	O
target	O
:	O
7	O
-	O
29	O
for	O
the	O
number	O
of	O
neighbors	O
;	O
default	O
minority	O
voting	O
for	O
class	O
voting	O
in	O
most	O
cases	O
;	O
Modified	O
Value	O
Distance	O
,	O
Jeffrey	O
divergence	O
,	O
and	O
cosine	O
distance	O
for	O
distance	B-HyperparameterName
metric	I-HyperparameterName
;	O
and	O
gain	O
ratio	O
for	O
feature	O
weight	O
in	O
most	O
cases	O
.	O

While	O
the	O
official	O
scorer	O
averages	O
the	O
results	O
over	O
all	O
five	O
targets	O
,	O
we	O
are	O
interested	O
in	O
whether	O
our	O
classifiers	O
show	O
a	O
stable	O
performance	O
across	O
targets	O
,	O
and	O
why	O
the	O
ensemble	O
model	O
benefits	O
from	O
combining	O
all	O
individual	O
classifiers	O
.	O
For	O
this	O
reason	O
,	O
we	O
modified	O
the	O
scorer	O
so	O
that	O
it	O
would	O
calculate	O
accuracy	B-MetricName
,	O
precision	O
,	O
and	O
recall	O
for	O
individual	O
stances	O
per	O
target	O
separately	O
.	O
The	O
results	O
are	O
shown	O
in	O
table	O
5	O
.	O
The	O
official	O
metric	O
is	O
the	O
macro	O
-	O
averaged	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
on	O
Favor	O
and	O
Against	O
while	O
accuracy	B-MetricName
is	O
equivalent	O
to	O
the	O
micro	O
-	O
averaged	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
based	O
on	O
all	O
classes	O
.	O
The	O
results	O
show	O
a	O
more	O
diverse	O
picture	O
:	O
For	O
the	O
individual	O
classifiers	O
,	O
GBDT	O
reaches	O
the	O
highest	O
ac	O
-	O
curacies	O
for	O
the	O
targets	O
Climate	O
and	O
Feminist	O
,	O
random	O
forest	O
for	O
Atheism	O
and	O
Hillary	O
,	O
and	O
they	O
tie	O
for	O
Abortion	O
.	O
For	O
the	O
ensembles	O
,	O
the	O
version	O
without	O
global	O
features	O
reaches	O
higher	O
accuracies	O
for	O
Abortion	O
,	O
Climate	O
,	O
and	O
Hillary	O
,	O
the	O
version	O
with	O
global	O
features	O
has	O
a	O
higher	O
accuracy	B-MetricName
for	O
Feminist	O
,	O
and	O
they	O
tie	O
for	O
Atheism	O
.	O
EnsembleNG	O
,	O
which	O
reaches	O
the	O
best	O
score	O
across	O
all	O
targets	O
,	O
only	O
reaches	O
the	O
best	O
score	O
for	O
two	O
targets	O
:	O
Abortion	O
and	O
Feminist	O
.	O
It	O
reaches	O
lower	O
results	O
than	O
the	O
best	O
individual	O
classifier	O
for	O
3	O
targets	O
:	O
Atheism	O
,	O
Climate	O
,	O
and	O
Hillary	O
.	O
However	O
,	O
since	O
the	O
best	O
results	O
for	O
the	O
latter	O
3	O
targets	O
are	O
reached	O
by	O
different	O
individual	O
classifiers	O
(	O
random	O
forest	O
for	O
Atheism	O
and	O
Hillary	O
;	O
GBDT	O
for	O
Climate	O
)	O
,	O
we	O
assume	O
that	O
the	O
ensemble	O
provides	O
the	O
best	O
compromise	O
.	O
In	O
order	O
to	O
obtain	O
a	O
better	O
understanding	O
of	O
the	O
differences	O
in	O
performance	O
of	O
classifiers	O
across	O
targets	O
,	O
we	O
have	O
analyzed	O
the	O
distribution	O
of	O
stances	O
per	O
target	O
.	O
One	O
hypothesis	O
that	O
could	O
be	O
drawn	O
from	O
the	O
analysis	O
above	O
is	O
that	O
the	O
GBDT	O
model	O
is	O
better	O
suited	O
for	O
finding	O
examples	O
of	O
the	O
majority	O
classes	O
while	O
random	O
forest	O
is	O
better	O
at	O
finding	O
minority	O
class	O
examples	O
.	O
However	O
,	O
when	O
we	O
compare	O
the	O
targets	O
Abortion	O
and	O
Atheism	O
,	O
the	O
class	O
distribution	O
is	O
similar	O
,	O
but	O
the	O
performance	O
of	O
the	O
two	O
classifiers	O
is	O
vastly	O
different	O
:	O
For	O
Abortion	O
,	O
GBDT	O
reaches	O
higher	O
recall	O
for	O
the	O
majority	O
class	O
(	O
Against	O
)	O
and	O
higher	O
precision	O
for	O
Favor	O
.	O
For	O
Atheism	O
,	O
it	O
has	O
a	O
higher	O
precision	O
for	O
the	O
majority	O
class	O
and	O
a	O
higher	O
recall	O
for	O
Favor	O
.	O
The	O
reasons	O
for	O
these	O
different	O
behaviors	O
need	O
to	O
be	O
determined	O
in	O
future	O
work	O
.	O

This	O
paper	O
describes	O
our	O
contribution	O
to	O
SemEval	O
-	O
2021	O
Task	O
5	O
:	O
Toxic	O
Spans	O
Detection	O
.	O
Our	O
solution	O
is	O
built	O
upon	O
RoBERTa	B-MethodName
language	O
model	O
and	O
Conditional	O
Random	O
Fields	O
(	O
CRF	B-MethodName
)	O
.	O
We	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
on	O
Civil	O
Comments	O
dataset	O
,	O
enabling	O
it	O
to	O
create	O
better	O
contextual	O
representation	O
for	O
this	O
task	O
.	O
We	O
also	O
employed	O
the	O
semi	O
-	O
supervised	O
learning	O
technique	O
of	O
selftraining	O
,	O
which	O
allowed	O
us	O
to	O
extend	O
our	O
training	O
dataset	O
.	O
In	O
addition	O
to	O
these	O
,	O
we	O
also	O
identified	O
some	O
pre	O
-	O
processing	O
steps	O
that	O
significantly	O
improved	O
our	O
F1	B-MetricName
score	I-MetricName
.	O
Our	O
proposed	O
system	O
achieved	O
a	O
rank	O
of	O
41	O
with	O
an	O
F1	B-MetricName
score	I-MetricName
of	O
66.16	O
%	O
.	O

Toxic	O
comments	O
have	O
a	O
different	O
language	O
construct	O
from	O
the	O
general	O
language	O
.	O
Their	O
slang	O
and	O
obfuscated	O
content	O
(	O
van	O
Aken	O
et	O
al	O
,	O
2018	O
)	O
make	O
it	O
difficult	O
for	O
the	O
language	O
models	O
pre	O
-	O
trained	O
on	O
broader	O
datasets	O
to	O
understand	O
them	O
.	O
Similar	O
to	O
other	O
domain	O
-	O
specific	O
models	O
(	O
Beltagy	O
et	O
al	O
,	O
2019	O
;	O
Lee	O
et	O
al	O
,	O
2020	O
;	O
Paraschiv	O
et	O
al	O
,	O
2020	O
)	O
,	O
we	O
pretrained	O
the	O
RoBERTa	B-MethodName
-	O
base	O
model	O
on	O
the	O
Civil	O
comments	O
dataset	O
using	O
Masked	O
Language	B-TaskName
Modelling	I-TaskName
(	O
MLM	B-DatasetName
)	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
to	O
provide	O
the	O
necessary	O
domain	O
knowledge	O
and	O
created	O
our	O
model	O
RoBERTa	B-MethodName
(	O
p	O
)	O
.	O
The	O
original	O
weights	O
of	O
RoBERTabase	O
served	O
as	O
the	O
starting	O
point	O
for	O
the	O
pre	O
-	O
training	O
.	O
The	O
pre	O
-	O
training	O
was	O
done	O
for	O
0.2	O
million	O
steps	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	O
and	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e	O
-	O
5	O
.	O
1	O
https://github.com/jain	O
-	O
abhinav02/	O
Toxic_Spans_Detection	O

The	O
best	O
performing	O
model	O
on	O
the	O
manually	O
annotated	O
dataset	O
(	O
gold	O
dataset	O
)	O
was	O
used	O
to	O
generate	O
toxic	O
spans	O
for	O
the	O
unannotated	O
dataset	O
.	O
When	O
selecting	O
the	O
unannotated	O
data	O
,	O
we	O
followed	O
the	O
process	O
similar	O
to	O
the	O
one	O
used	O
for	O
creating	O
the	O
gold	O
dataset	O
(	O
Pavlopoulos	O
et	O
al	O
,	O
2021	O
)	O
that	O
is	O
,	O
filter	O
the	O
most	O
toxic	O
samples	O
(	O
toxicity	O
≥	O
0.80	O
)	O
from	O
the	O
Civil	O
Comments	O
dataset	O
and	O
select	O
a	O
random	O
set	O
of	O
10	O
,	O
000	O
samples	O
.	O
This	O
process	O
allowed	O
the	O
silver	O
data	O
to	O
have	O
similar	O
toxicity	O
distribution	O
as	O
the	O
gold	O
data	O
.	O
The	O
newly	O
generated	O
annotations	O
(	O
silver	O
dataset	O
)	O
were	O
then	O
used	O
along	O
with	O
the	O
gold	O
dataset	O
to	O
train	O
a	O
new	O
model	O
.	O
The	O
model	O
trained	O
on	O
the	O
combined	O
gold	O
and	O
silver	O
dataset	O
gave	O
better	O
performance	O
(	O
F1	B-MetricName
score	I-MetricName
:	O
66.13	O
%	O
)	O
than	O
the	O
one	O
trained	O
only	O
on	O
the	O
gold	O
dataset	O
(	O
F1	B-MetricName
score	I-MetricName
:	O
65.66	O
%	O
)	O
.	O
We	O
repeated	O
this	O
process	O
for	O
one	O
more	O
iteration	O
with	O
another	O
random	O
set	O
of	O
10	O
,	O
000	O
samples	O
(	O
F1	B-MetricName
score	I-MetricName
:	O
66.34	O
%	O
)	O
.	O
Figure	O
2	O
gives	O
a	O
simplistic	O
idea	O
of	O
selftraining	O
.	O
3	O
Experimental	O
Setup	O
Data	O
:	O
Each	O
training	O
example	O
consisted	O
of	O
a	O
text	O
sample	O
in	O
English	O
,	O
and	O
its	O
ground	O
truth	O
toxic	O
span	O
provided	O
as	O
a	O
list	O
of	O
character	O
offsets	O
(	O
possibly	O
empty	O
)	O
.	O
The	O
posts	O
were	O
sampled	O
from	O
the	O
publicly	O
available	O
Civil	O
Comments	O
dataset	O
.	O
The	O
training	O
set	O
consisted	O
of	O
7939	O
samples	O
.	O
We	O
randomly	O
sampled	O
20	O
%	O
of	O
it	O
as	O
the	O
development	O
set	O
.	O
The	O
test	O
set	O
for	O
the	O
evaluation	O
phase	O
had	O
2000	O
samples	O
.	O
In	O
the	O
training	O
dataset	O
,	O
sample	O
length	O
varies	O
from	O
1	O
to	O
421	O
tokens	O
,	O
with	O
an	O
average	O
length	O
of	O
47	O
tokens	O
when	O
tokenized	O
using	O
the	O
RoBERTa	B-MethodName
-	O
base	O
tokenizer	O
.	O
Nearly	O
10	O
%	O
of	O
all	O
tokens	O
in	O
the	O
training	O
dataset	O
are	O
marked	O
as	O
toxic	O
.	O
The	O
mean	O
span	O
length	O
is	O
17.5	O
characters	O
and	O
485	O
samples	O
have	O
empty	O
spans	O
.	O
Further	O
details	O
about	O
the	O
dataset	O
can	O
be	O
found	O
in	O
the	O
task	O
description	O
paper	O
(	O
Pavlopoulos	O
et	O
al	O
,	O
2021	O
)	O
.	O

The	O
evaluation	O
measure	O
for	O
a	O
sample	O
is	O
the	O
F1	B-MetricName
score	I-MetricName
between	O
the	O
predicted	O
spans	O
and	O
the	O
ground	O
truth	O
spans	O
as	O
defined	O
in	O
the	O
SemEval	O
-	O
2021	O
Task	O
5	O
paper	O
(	O
Pavlopoulos	O
et	O
al	O
,	O
2021	O
)	O
.	O
The	O
overall	O
score	O
is	O
obtained	O
by	O
taking	O
the	O
mean	O
of	O
the	O
F1	B-MetricName
score	I-MetricName
of	O
all	O
samples	O
in	O
the	O
test	O
set	O
.	O
by	O
Huggingface	O
2	O
.	O
The	O
RoBERTa	B-MethodName
model	O
was	O
followed	O
by	O
two	O
dense	O
layers	O
with	O
512	O
and	O
128	O
units	O
with	O
relu	B-MethodName
(	O
Agarap	O
,	O
2018	O
)	O
as	O
the	O
activation	B-HyperparameterName
function	I-HyperparameterName
and	O
a	O
dropout	O
rate	O
of	O
0.1	O
.	O
The	O
output	O
layer	O
had	O
two	O
or	O
three	O
labels	O
depending	O
on	O
the	O
tagging	O
scheme	O
.	O
We	O
applied	O
the	O
post	O
-	O
processing	O
steps	O
mentioned	O
in	O
section	O
2.2	O
for	O
all	O
the	O
model	O
variants	O
.	O

Table	O
1	O
shows	O
that	O
our	O
RoBERTa	B-MethodName
(	O
p	O
)	O
model	O
outperforms	O
the	O
original	O
RoBERTa	B-MethodName
model	O
.	O
As	O
suggested	O
earlier	O
,	O
domain	O
-	O
specific	O
pre	O
-	O
training	O
allows	O
the	O
model	O
to	O
understand	O
the	O
language	O
construct	O
of	O
toxic	O
comments	O
better	O
.	O
Additionally	O
,	O
we	O
observe	O
a	O
significant	O
increase	O
in	O
performance	O
by	O
adding	O
preprocessing	O
steps	O
as	O
it	O
makes	O
the	O
model	O
more	O
robust	O
to	O
the	O
noise	O
present	O
in	O
the	O
text	O
samples	O
.	O
Adding	O
the	O
CRF	B-MethodName
layer	O
further	O
improves	O
the	O
F1	B-MetricName
score	I-MetricName
by	O
eliminating	O
the	O
problem	O
of	O
independent	O
label	O
prediction	O
.	O
It	O
is	O
evident	O
from	O
table	O
1	O
that	O
the	O
BIO	O
tagging	O
scheme	O
performs	O
better	O
than	O
the	O
IO	O
tagging	O
scheme	O
when	O
working	O
with	O
CRF	B-MethodName
,	O
suggesting	O
it	O
can	O
better	O
understand	O
the	O
span	O
nature	O
of	O
the	O
output	O
.	O
Finally	O
,	O
using	O
two	O
rounds	O
of	O
self	O
-	O
training	O
helped	O
us	O
achieve	O
our	O
best	O
F1	B-MetricName
score	I-MetricName
,	O
66.34	O
%	O
3	O
.	O
One	O
interesting	O
observation	O
that	O
can	O
be	O
drawn	O
from	O
Table	O
1	O
is	O
that	O
for	O
almost	O
all	O
the	O
models	O
,	O
the	O
recall	O
remains	O
constant	O
and	O
improvement	O
in	O
F1	B-MetricName
is	O
due	O
to	O
improvement	O
in	O
precision	O
.	O
The	O
constancy	O
of	O
recall	O
indicates	O
that	O
few	O
spans	O
are	O
not	O
captured	O
as	O
toxic	O
by	O
any	O
of	O
the	O
models	O
.	O

Figure	O
3	O
shows	O
the	O
variation	O
of	O
the	O
F1	B-MetricName
score	I-MetricName
across	O
different	O
toxic	O
span	O
lengths	O
on	O
the	O
test	O
dataset	O
.	O
Our	O
model	O
achieved	O
a	O
very	O
high	O
F1	B-MetricName
score	I-MetricName
when	O
one	O
(	O
Span	O
Length	O
1	O
-	O
9	O
,	O
Mean	O
F1	B-MetricName
Score	I-MetricName
:	O
83.17	O
%	O
)	O
or	O
two	O
(	O
Span	O
Length	O
10	O
-	O
17	O
,	O
Mean	O
F1	B-MetricName
Score	I-MetricName
:	O
74.44	O
%	O
)	O
words	O
are	O
marked	O
as	O
toxic	O
in	O
a	O
text	O
sample	O
.	O
As	O
the	O
number	O
of	O
characters	O
marked	O
as	O
toxic	O
increases	O
,	O
the	O
F1	B-MetricName
score	I-MetricName
falls	O
drastically	O
,	O
reaching	O
as	O
low	O
as	O
24.82	O
%	O
when	O
more	O
than	O
58	O
characters	O
are	O
marked	O
as	O
toxic	O
.	O
There	O
are	O
two	O
main	O
reasons	O
for	O
this	O
.	O
First	O
,	O
it	O
is	O
easier	O
for	O
the	O
model	O
to	O
capture	O
short	O
-	O
term	O
dependencies	O
than	O
long	O
-	O
term	O
dependencies	O
.	O
Second	O
,	O
only	O
10	O
%	O
of	O
the	O
training	O
data	O
has	O
a	O
span	O
length	O
of	O
more	O
than	O
25	O
characters	O
making	O
the	O
model	O
less	O
equipped	O
to	O
capture	O
such	O
toxic	O
spans	O
.	O
To	O
investigate	O
our	O
model	O
's	O
most	O
problematic	O
cases	O
,	O
we	O
analysed	O
the	O
samples	O
for	O
which	O
our	O
model	O
gave	O
a	O
zero	O
F1	B-MetricName
score	I-MetricName
.	O
There	O
were	O
447	O
such	O
samples	O
,	O
of	O
which	O
349	O
samples	O
did	O
not	O
have	O
any	O
toxic	O
span	O
in	O
the	O
ground	O
truth	O
.	O
This	O
is	O
also	O
reflected	O
in	O
Figure	O
3	O
,	O
as	O
the	O
mean	O
F1	B-MetricName
score	I-MetricName
of	O
all	O
the	O
samples	O
with	O
zero	O
span	O
length	O
is	O
11.42	O
%	O
.	O
Further	O
analysis	O
revealed	O
that	O
our	O
model	O
tends	O
to	O
mark	O
those	O
tokens	O
as	O
toxic	O
,	O
which	O
were	O
frequently	O
found	O
to	O
be	O
toxic	O
elsewhere	O
.	O
A	O
few	O
samples	O
with	O
empty	O
toxic	O
spans	O
had	O
doubtful	O
gold	O
annotations	O
.	O
However	O
,	O
in	O
other	O
samples	O
,	O
our	O
model	O
failed	O
to	O
capture	O
the	O
sentence	O
's	O
context	O
precisely	O
and	O
predicts	O
tokens	O
that	O
were	O
not	O
used	O
in	O
a	O
toxic	O
sense	O
.	O
Table	O
2	O
shows	O
other	O
standard	O
errors	O
our	O
model	O
makes	O
.	O
It	O
seems	O
that	O
our	O
model	O
has	O
a	O
problem	O
with	O
small	O
sentences	O
.	O
More	O
often	O
than	O
not	O
,	O
it	O
misses	O
the	O
toxic	O
span	O
present	O
in	O
it	O
and	O
returns	O
an	O
empty	O
span	O
.	O
A	O
similar	O
case	O
occurs	O
when	O
it	O
encounters	O
text	O
samples	O
with	O
rare	O
toxic	O
words	O
.	O
These	O
words	O
may	O
be	O
present	O
in	O
very	O
few	O
examples	O
or	O
be	O
completely	O
absent	O
from	O
the	O
training	O
dataset	O
,	O
making	O
our	O
model	O
less	O
endowed	O
to	O
understand	O
them	O
.	O
Other	O
than	O
these	O
,	O
our	O
model	O
sometimes	O
misses	O
the	O
non	O
-	O
swear	O
words	O
in	O
a	O
toxic	O
span	O
.	O

This	O
paper	O
described	O
our	O
system	O
developed	O
for	O
SemEval	O
-	O
2021	O
Task	O
5	O
:	O
Toxic	O
Span	O
Detection	O
.	O
We	O
built	O
our	O
solution	O
on	O
the	O
RoBERTa	B-MethodName
language	O
model	O
and	O
Conditional	O
Random	O
Fields	O
(	O
CRF	B-MethodName
)	O
.	O
Though	O
RoBERTa	B-MethodName
alone	O
can	O
achieve	O
great	O
results	O
,	O
we	O
highlighted	O
the	O
benefits	O
of	O
using	O
external	O
datasets	O
and	O
the	O
performance	O
improvements	O
it	O
can	O
help	O
us	O
achieve	O
.	O
We	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
on	O
the	O
Civil	O
Comments	O
dataset	O
to	O
impart	O
domain	O
-	O
specific	O
knowledge	O
to	O
it	O
.	O
We	O
also	O
employed	O
the	O
semisupervised	O
learning	O
technique	O
of	O
self	O
-	O
training	O
to	O
extend	O
our	O
training	O
dataset	O
.	O
In	O
addition	O
to	O
these	O
,	O
we	O
also	O
discovered	O
some	O
pre	O
-	O
processing	O
steps	O
that	O
significantly	O
improved	O
our	O
F1	B-MetricName
score	I-MetricName
.	O
Experimenting	O
with	O
different	O
tagging	O
schemes	O
,	O
we	O
found	O
out	O
that	O
the	O
BIO	O
scheme	O
works	O
the	O
best	O
with	O
CRF	B-MethodName
.	O
In	O
future	O
,	O
we	O
plan	O
to	O
experiment	O
with	O
other	O
language	O
models	O
such	O
as	O
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
,	O
2019	O
)	O
,	O
XL	O
-	O
Net	O
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
and	O
DeBERTa	B-MethodName
(	O
He	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
system	O
could	O
also	O
benefit	O
from	O
the	O
addition	O
of	O
syntactic	O
and	O
semantic	O
features	O
at	O
the	O
word	O
and	O
sentence	O
level	O
.	O

Pre	O
-	O
training	O
has	O
improved	O
model	O
accuracy	B-MetricName
for	O
both	O
classification	O
and	O
generation	O
tasks	O
at	O
the	O
cost	O
of	O
introducing	O
much	O
larger	O
and	O
slower	O
models	O
.	O
Pruning	O
methods	O
have	O
proven	O
to	O
be	O
an	O
effective	O
way	O
of	O
reducing	O
model	O
size	O
,	O
whereas	O
distillation	O
methods	O
are	O
proven	O
for	O
speeding	O
up	O
inference	O
.	O
We	O
introduce	O
a	O
block	O
pruning	O
approach	O
targeting	O
both	O
small	O
and	O
fast	O
models	O
.	O
Our	O
approach	O
extends	O
structured	O
methods	O
by	O
considering	O
blocks	O
of	O
any	O
size	O
and	O
integrates	O
this	O
structure	O
into	O
the	O
movement	B-MethodName
pruning	I-MethodName
paradigm	O
for	O
fine	O
-	O
tuning	O
.	O
We	O
find	O
that	O
this	O
approach	O
learns	O
to	O
prune	O
out	O
full	O
components	O
of	O
the	O
underlying	O
model	O
,	O
such	O
as	O
attention	O
heads	O
.	O
Experiments	O
consider	O
classification	O
and	O
generation	O
tasks	O
,	O
yielding	O
among	O
other	O
results	O
a	O
pruned	O
model	O
that	O
is	O
a	O
2.4x	O
faster	O
,	O
74	O
%	O
smaller	O
BERT	B-MethodName
on	O
SQuAD	B-DatasetName
v1	O
,	O
with	O
a	O
1	O
%	O
drop	O
on	O
F1	B-MetricName
,	O
competitive	O
both	O
with	O
distilled	O
models	O
in	O
speed	O
and	O
pruned	O
models	O
in	O
size	O
.	O

Pre	O
-	O
trained	O
transformer	O
models	O
are	O
the	O
standard	O
for	O
NLP	O
tasks	O
in	O
both	O
classification	O
and	O
generation	O
tasks	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
recent	O
trend	O
is	O
for	O
models	O
to	O
continue	O
to	O
grow	O
in	O
size	O
while	O
yielding	O
improved	O
performance	O
on	O
standard	O
benchmarks	O
(	O
Rosset	O
,	O
2020	O
)	O
.	O
This	O
development	O
highlights	O
the	O
need	O
to	O
reduce	O
the	O
storage	O
size	O
and	O
increase	O
the	O
efficiency	O
of	O
pre	O
-	O
trained	O
models	O
.	O
Pruning	O
methods	O
have	O
shown	O
to	O
be	O
extremely	O
effective	O
at	O
reducing	O
the	O
storage	O
size	O
of	O
models	O
fine	O
-	O
tuned	O
for	O
a	O
specific	O
task	O
.	O
Approaches	O
such	O
as	O
magnitude	O
pruning	O
(	O
Han	O
et	O
al	O
,	O
2015	O
)	O
,	O
L0	O
regularization	O
(	O
Louizos	O
et	O
al	O
,	O
2018	O
)	O
,	O
lottery	O
ticket	O
hypothesis	O
(	O
Frankle	O
and	O
Carbin	O
,	O
2018	O
)	O
,	O
diff	O
pruning	O
(	O
Guo	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
movement	B-MethodName
pruning	I-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2020	O
)	O
have	O
demonstrated	O
remarkable	O
reductions	O
in	O
model	O
size	O
.	O
Movement	B-MethodName
pruning	I-MethodName
produces	O
77	O
%	O
savings	O
in	O
parameter	O
storage	O
for	O
a	O
1	O
%	O
drop	O
in	O
accuracy	B-MetricName
on	O
SQuAD	B-DatasetName
v1.1	O
.	O
However	O
,	O
these	O
models	O
yield	O
very	O
little	O
actual	O
efficiency	O
benefits	O
,	O
as	O
to	O
run	O
them	O
in	O
standard	O
hardware	O
often	O
requires	O
reconstructing	O
the	O
original	O
dense	O
shape	O
.	O
On	O
the	O
other	O
hand	O
distillation	O
methods	O
have	O
been	O
more	O
effective	O
at	O
producing	O
faster	O
models	O
as	O
has	O
been	O
shown	O
by	O
DistilBERT	B-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2019	O
)	O
,	O
TinyBERT	O
(	O
Jiao	O
et	O
al	O
,	O
2019	O
)	O
or	O
MobileBERT	B-MethodName
(	O
Sun	O
et	O
al	O
,	O
2020	O
)	O
.	O
These	O
approaches	O
utilize	O
targeted	O
distillation	O
to	O
produce	O
smaller	O
models	O
with	O
a	O
dense	O
structure	O
that	O
is	O
fast	O
on	O
standard	O
hardware	O
.	O
However	O
without	O
careful	O
engineering	O
and	O
size	O
selection	O
these	O
models	O
are	O
much	O
larger	O
than	O
pruned	O
ones	O
.	O
In	O
this	O
work	O
,	O
we	O
target	O
closing	O
this	O
gap	O
through	O
block	O
pruning	O
.	O
Unlike	O
pruning	O
individual	O
parameters	O
,	O
this	O
approach	O
encourages	O
pruning	O
that	O
can	O
be	O
optimized	O
on	O
dense	O
hardware	O
.	O
It	O
is	O
a	O
less	O
rigid	O
approach	O
than	O
row	O
or	O
column	O
-	O
based	O
pruning	O
typically	O
used	O
in	O
structured	O
approaches	O
(	O
McCarley	O
,	O
2019	O
)	O
,	O
which	O
have	O
been	O
difficult	O
to	O
apply	O
effectively	O
to	O
transformers	O
.	O
We	O
integrate	O
this	O
approach	O
with	O
Movement	B-MethodName
pruning	I-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2020	O
)	O
,	O
a	O
simple	O
method	O
for	O
pruning	O
pre	O
-	O
trained	O
models	O
during	O
fine	O
-	O
tuning	O
.	O
The	O
final	O
method	O
1	O
has	O
few	O
additional	O
hyperparameters	O
or	O
training	O
requirements	O
.	O
Experiments	O
consider	O
a	O
large	O
variety	O
of	O
different	O
benchmark	O
datasets	O
comparing	O
accuracy	B-MetricName
and	O
efficiency	O
.	O
We	O
find	O
a	O
surprising	O
result	O
that	O
despite	O
utilizing	O
sub	O
-	O
row	O
square	O
blocks	O
during	O
training	O
,	O
the	O
approach	O
learns	O
to	O
eliminate	O
full	O
components	O
of	O
the	O
model	O
,	O
effectively	O
dropping	O
a	O
large	O
number	O
of	O
attention	O
heads	O
.	O
This	O
effect	O
allows	O
the	O
model	O
to	O
achieve	O
speedups	O
even	O
beyond	O
standard	O
structured	O
pruning	O
of	O
feed	O
-	O
forward	O
layers	O
.	O
Results	O
show	O
a	O
2.4x	O
speedup	O
on	O
SQuAD	B-DatasetName
v1.1	O
with	O
a	O
1	O
%	O
drop	O
of	O
F1	B-MetricName
,	O
and	O
a	O
2.3x	O
speedup	O
on	O
QQP	B-DatasetName
with	O
a	O
1	O
%	O
loss	B-MetricName
of	O
F1	B-MetricName
.	O
Experiments	O
on	O
summarization	B-TaskName
also	O
show	O
a	O
1.39x	O
speedup	O
for	O
an	O
average	O
of	O
2	O
points	O
drop	O
on	O
all	O
ROUGE	O
metrics	O
on	O
CNN	O
/	O
DailyMail	O
,	O
and	O
for	O
a	O
reduction	O
of	O
decoder	O
weights	O
of	O
3.5x	O
.	O
1	O
Available	O
at	O
https://github.com/	O
huggingface	O
/	O
nn_pruning	O

Starting	O
with	O
a	O
transformer	O
model	O
with	O
parameters	O
θ	B-HyperparameterName
,	O
our	O
goal	O
is	O
to	O
produce	O
a	O
set	O
of	O
parameters	O
θ	B-HyperparameterName
that	O
are	O
both	O
fine	O
-	O
tuned	O
for	O
a	O
specific	O
end	O
-	O
task	O
and	O
smaller	O
in	O
such	O
a	O
way	O
that	O
inference	O
can	O
be	O
efficiently	O
computed	O
on	O
parallel	O
hardware	O
.	O
The	O
two	O
largest	O
lines	O
in	O
the	O
transformer	O
parameter	O
budget	O
are	O
the	O
feed	O
-	O
forward	O
network	O
sublayer	O
(	O
FFN	O
)	O
and	O
the	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
sub	O
-	O
layer	O
(	O
MHA	O
)	O
.	O
The	O
FFN	O
parameters	O
consist	O
of	O
two	O
matrices	O
(	O
W	O
1	O
and	O
W	O
2	O
)	O
of	O
transposed	O
shape	O
R	O
d	O
model	O
×d	O
ff	O
and	O
R	O
d	O
ff	O
×d	O
model	O
where	O
d	O
model	O
is	O
the	O
hidden	O
size	O
and	O
d	O
ff	O
d	O
model	O
is	O
the	O
inner	O
size	O
.	O
These	O
are	O
used	O
in	O
the	O
standard	O
fashion	O
by	O
the	O
network	O
.	O
The	O
MHA	O
parameters	O
consist	O
of	O
4	O
projection	O
matrices	O
(	O
W	O
q	O
,	O
W	O
k	O
,	O
W	O
v	O
and	O
W	O
o	O
)	O
of	O
size	O
R	O
d	O
model	O
×d	O
model	O
(	O
query	O
,	O
key	O
,	O
value	O
,	O
out	O
)	O
.	O
These	O
are	O
used	O
to	O
project	O
the	O
hidden	O
vector	O
to	O
and	O
from	O
the	O
component	O
attention	O
parts	O
.	O
In	O
implementations	O
,	O
this	O
projection	O
is	O
made	O
with	O
the	O
matrices	O
in	O
their	O
folded	O
tensor	O
form	O
In	O
standard	O
fine	O
-	O
tuning	O
,	O
starting	O
from	O
θ	B-HyperparameterName
,	O
we	O
optimize	O
the	O
loss	B-MetricName
L	O
(	O
for	O
instance	O
,	O
cross	O
-	O
entropy	O
for	O
classification	O
)	O
:	O
arg	O
min	O
θ	B-HyperparameterName
L	O
(	O
θ	B-HyperparameterName
)	O
Score	B-MetricName
-	O
based	O
pruning	O
methods	O
(	O
Ramanujan	O
et	O
al	O
,	O
2019	O
)	O
modify	O
the	O
model	O
by	O
introducing	O
score	O
parameters	O
S	O
for	O
each	O
parameter	O
i	O
and	O
replace	O
the	O
original	O
parameter	O
matrices	O
with	O
a	O
masked	O
version	O
W	O
=	O
W	O
M	O
(	O
S	O
)	O
.	O
For	O
instance	O
,	O
in	O
the	O
simplest	O
version	O
of	O
magnitude	O
pruning	O
,	O
the	O
mask	O
would	O
just	O
zero	O
-	O
out	O
parameters	O
with	O
low	O
absolute	O
values	O
.	O
Movement	B-MethodName
pruning	I-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2020	O
)	O
is	O
a	O
scorebased	O
pruning	O
approach	O
that	O
encourages	O
the	O
model	O
to	O
optimize	O
these	O
score	O
parameters	O
.	O
Specifically	O
,	O
we	O
focus	O
on	O
the	O
soft	O
-	O
movement	O
variant	O
of	O
movement	B-MethodName
pruning	I-MethodName
that	O
sets	O
M	O
(	O
S	O
)	O
=	O
1	O
(	O
S	O
>	O
τ	O
)	O
for	O
a	O
threshold	O
parameter	O
τ	O
,	O
and	O
optimizes	O
a	O
regularized	O
objective	O
,	O
arg	O
min	O
θ	B-HyperparameterName
,	O
S	O
L	O
(	O
θ	B-HyperparameterName
)	O
+	O
λ	O
σ	O
(	O
S	O
)	O
where	O
λ	O
is	O
a	O
hyper	O
-	O
parameter	O
,	O
A	O
=	O
i	O
,	O
j	O
A	O
i	O
,	O
j	O
and	O
σ	O
is	O
the	O
sigmoid	O
function	O
.	O
This	O
pruning	O
objective	O
encourages	O
the	O
model	O
to	O
fine	O
-	O
tune	O
the	O
parameters	O
while	O
lowering	O
the	O
scores	O
of	O
unimportant	O
parameters	O
and	O
thus	O
encouraging	O
more	O
sparsity	O
.	O
In	O
order	O
to	O
train	O
through	O
the	O
threshold	O
,	O
a	O
straight	O
-	O
through	O
estimator	O
(	O
Bengio	O
et	O
al	O
,	O
2013	O
)	O
is	O
used	O
.	O
Movement	B-MethodName
pruning	I-MethodName
,	O
combined	O
with	O
distillation	O
,	O
has	O
shown	O
to	O
be	O
a	O
very	O
effective	O
method	O
to	O
reduce	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
in	O
an	O
existing	O
model	O
yielding	O
94	O
%	O
pruning	O
in	O
our	O
tests	O
for	O
a	O
F1	B-MetricName
of	O
87.5	O
on	O
SQuAD	B-DatasetName
v1.1	O
(	O
BERT	B-MethodName
-	O
base	O
is	O
88.5	O
)	O
.	O
This	O
results	O
in	O
significantly	O
smaller	O
models	O
than	O
distillation	O
alone	O
.	O
However	O
,	O
even	O
with	O
this	O
sparsity	O
level	O
,	O
the	O
model	O
is	O
not	O
substantially	O
faster	O
when	O
run	O
on	O
most	O
standard	O
hardware	O
that	O
can	O
not	O
significantly	O
take	O
advantage	O
of	O
this	O
style	O
of	O
sparse	O
matrix	O
-	O
vector	O
product	O
.	O

We	O
conduct	O
experiments	O
on	O
five	O
(	O
English	O
)	O
tasks	O
commonly	O
used	O
to	O
evaluate	O
pre	O
-	O
trained	O
language	O
models	O
:	O
question	B-TaskName
answering	I-TaskName
(	O
SQuAD	B-DatasetName
v1.1	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
and	O
(	O
SQuAD	B-DatasetName
v2	O
Rajpurkar	O
et	O
al	O
,	O
2018	O
)	O
,	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
MNLI	B-DatasetName
Williams	O
et	O
al	O
,	O
2018	O
)	O
,	O
sentence	O
similarity	O
(	O
QQP	B-DatasetName
Chen	O
et	O
al	O
,	O
2018	O
)	O
,	O
sentiment	O
classification	O
(	O
SST	B-DatasetName
-	O
2	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
and	O
abstractive	O
summarization	B-TaskName
(	O
CNN	O
/	O
DailyMail	O
Hermann	O
et	O
al	O
,	O
2015	O
)	O
.	O
These	O
datasets	O
respectively	O
contain	O
87k	O
,	O
130k	O
,	O
392k	O
,	O
363k	O
,	O
67k	O
and	O
287k	O
training	O
examples	O
,	O
and	O
are	O
downloaded	O
from	O
the	O
Hugging	O
Face	O
datasets	O
hub	O
.	O
SQuAD	B-DatasetName
is	O
formulated	O
as	O
a	O
span	O
-	O
extraction	O
task	O
,	O
MNLI	B-DatasetName
and	O
QQP	B-DatasetName
are	O
sentence	O
pairs	O
classification	O
tasks	O
,	O
SST	B-DatasetName
-	O
2	O
is	O
a	O
sentence	B-TaskName
classification	I-TaskName
task	O
and	O
CNN	O
/	O
DailyMail	O
(	O
"	O
CNN	O
"	O
)	O
is	O
formulated	O
as	O
a	O
conditional	O
generation	O
task	O
.	O
We	O
report	O
the	O
performance	O
on	O
the	O
development	O
set	O
as	O
measured	O
by	O
the	O
accuracy	B-MetricName
for	O
MNLI	B-DatasetName
and	O
SST	B-DatasetName
-	O
2	O
,	O
F1	B-MetricName
for	O
QQP	B-DatasetName
,	O
the	O
exact	B-MetricName
match	I-MetricName
(	O
EM	B-MetricName
)	O
and	O
F1	B-MetricName
for	O
SQuAD	B-DatasetName
and	O
ROUGE	O
for	O
CNN	O
/	O
DailyMail	O
.	O
We	O
experiment	O
with	O
task	O
-	O
specific	O
pruning	O
of	O
transformer	O
language	O
models	O
.	O
We	O
use	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
We	O
compare	O
against	O
several	O
baselines	O
.	O
Movement	B-MethodName
pruning	I-MethodName
is	O
a	O
fully	O
unstructured	O
approach	O
and	O
gives	O
an	O
upper	O
bound	O
on	O
the	O
sparsity	O
trade	O
-	O
offs	O
we	O
hope	O
to	O
achieve	O
,	O
even	O
if	O
it	O
provides	O
little	O
speed	O
benefit	O
.	O
We	O
also	O
compare	O
our	O
results	O
against	O
state	O
-	O
ofthe	O
-	O
art	O
approaches	O
developed	O
for	O
fast	O
inference	O
of	O
transformer	O
-	O
based	O
language	O
models	O
.	O
DistilBERT	B-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2019	O
)	O
is	O
obtained	O
by	O
distilling	O
through	O
pre	O
-	O
training	O
a	O
pre	O
-	O
trained	O
BERT	B-MethodName
into	O
a	O
smaller	O
model	O
.	O
TinyBERT	O
(	O
Jiao	O
et	O
al	O
,	O
2019	O
)	O
distills	O
a	O
finetuned	O
model	O
while	O
using	O
data	B-TaskName
augmentation	I-TaskName
.	O
Mo	O
-	O
bileBERT	O
(	O
Sun	O
et	O
al	O
,	O
2020	O
)	O
is	O
the	O
result	O
of	O
a	O
large	O
architecture	O
search	O
.	O
dBART	O
(	O
Shleifer	O
and	O
Rush	O
,	O
2020	O
)	O
is	O
obtained	O
by	O
arbitrarily	O
copying	O
equally	O
spaced	O
layers	O
of	O
a	O
large	O
model	O
to	O
a	O
smaller	O
one	O
.	O
To	O
measure	O
inference	O
speed	O
on	O
GPU	O
,	O
we	O
use	O
a	O
24	O
GB	O
3090	O
RTX	O
and	O
an	O
Intel	O
i7	O
CPU	O
,	O
using	O
a	O
large	O
batch	B-HyperparameterName
size	I-HyperparameterName
(	O
128	O
)	O
for	O
evaluation	O
and	O
using	O
PyTorch	O
CUDA	O
timing	O
primitives	O
.	O
We	O
measure	O
the	O
speed	O
of	O
other	O
models	O
in	O
this	O
same	O
setup	O
.	O
Results	O
may	O
be	O
different	O
from	O
original	O
papers	O
,	O
as	O
latency	O
and	O
throughput	O
characteristics	O
are	O
different	O
for	O
each	O
platform	O
.	O
We	O
also	O
provide	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
in	O
the	O
linear	O
layers	O
of	O
the	O
Transformer	B-MethodName
layers	O
for	O
each	O
of	O
our	O
models	O
and	O
for	O
the	O
reference	O
ones	O
:	O
as	O
the	O
linear	O
layers	O
represent	O
most	O
of	O
the	O
FLOPS	B-MetricName
,	O
this	O
is	O
a	O
good	O
proxy	O
for	O
the	O
computation	O
required	O
and	O
to	O
some	O
extent	O
for	O
the	O
compute	O
time	O
,	O
when	O
the	O
model	O
characteristics	O
are	O
equivalent	O
.	O

The	O
pruning	O
approaches	O
are	O
shown	O
in	O
Table	O
1	O
.	O
Block	O
pruning	O
use	O
square	O
block	O
sizes	O
throughout	O
all	O
the	O
linear	O
layers	O
,	O
as	O
an	O
extension	O
of	O
the	O
original	O
movement	B-MethodName
pruning	I-MethodName
for	O
which	O
the	O
block	O
size	O
is	O
1	O
.	O
Hybrid	O
pruning	O
jointly	O
removes	O
hidden	O
dimensions	O
in	O
feed	O
-	O
forward	O
layers	O
W	O
1	O
and	O
W	O
2	O
,	O
using	O
movement	B-MethodName
pruning	I-MethodName
to	O
create	O
the	O
dimension	O
mask	O
.	O
This	O
corresponds	O
to	O
full	O
rows	O
or	O
columns	O
in	O
the	O
parameter	O
matrices	O
.	O
The	O
pruned	O
W	O
1	O
and	O
W	O
2	O
can	O
then	O
be	O
"	O
compacted	O
"	O
to	O
become	O
fully	O
dense	O
:	O
we	O
perform	O
dense	O
operations	O
on	O
cropped	O
matrices	O
.	O
For	O
the	O
attention	B-HyperparameterName
layers	I-HyperparameterName
,	O
pruning	O
only	O
some	O
rows	O
or	O
columns	O
in	O
W	O
q	O
,	O
W	O
k	O
,	O
W	O
v	O
and	O
W	O
o	O
can	O
not	O
be	O
practically	O
exploited	O
.	O
This	O
is	O
because	O
the	O
structure	O
of	O
the	O
computation	O
makes	O
the	O
additional	O
cost	O
of	O
resizing	O
the	O
tensor	O
inefficient	O
.	O
We	O
,	O
therefore	O
,	O
use	O
square	O
block	O
pruning	O
on	O
the	O
attention	O
layer	O
,	O
with	O
a	O
block	O
size	O
of	O
(	O
32	O
,	O
32	O
)	O
which	O
showed	O
the	O
best	O
tradeoff	O
between	O
performance	O
and	O
accuracy	B-MetricName
.	O
Struct	O
pruning	O
uses	O
the	O
same	O
methods	O
for	O
FFN	O
layers	O
but	O
aims	O
to	O
remove	O
model	O
attention	O
heads	O
directly	O
.	O
To	O
do	O
so	O
,	O
we	O
choose	O
a	O
block	O
size	O
on	O
attention	O
that	O
equals	O
the	O
head	O
size	O
while	O
still	O
using	O
the	O
same	O
soft	O
movement	B-MethodName
pruning	I-MethodName
strategy	O
.	O
For	O
this	O
approach	O
,	O
we	O
use	O
a	O
λ	O
att	O
equals	O
to	O
1/32	O
,	O
as	O
there	O
are	O
32	O
times	O
more	O
parameters	O
than	O
in	O
an	O
attention	O
block	O
than	O
in	O
a	O
feed	O
-	O
forward	O
dimension	O
.	O
When	O
Block	O
Pruning	O
does	O
not	O
fully	O
remove	O
a	O
component	O
such	O
as	O
an	O
attention	O
head	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
,	O
we	O
can	O
not	O
speed	O
up	O
the	O
model	O
.	O
But	O
we	O
can	O
reclaim	O
some	O
of	O
the	O
performance	O
at	O
no	O
speed	O
cost	O
and	O
at	O
marginal	O
cost	O
on	O
sparsity	O
by	O
making	O
use	O
of	O
those	O
zero	O
weights	O
.	O
Hybrid	O
Filled	O
pruning	O
allows	O
the	O
model	O
to	O
reinitialize	O
these	O
reclaimed	O
weights	O
uniformly	O
at	O
random	O
and	O
continue	O
fine	O
-	O
tuning	O
the	O
smaller	O
model	O
for	O
a	O
few	O
steps	O
.	O
We	O
also	O
explore	O
"	O
rewinding	O
"	O
(	O
Frankle	O
and	O
Carbin	O
,	O
2018	O
)	O
by	O
identifying	O
weights	O
that	O
should	O
not	O
be	O
pruned	O
(	O
because	O
they	O
are	O
part	O
of	O
a	O
non	O
-	O
empty	O
attention	O
head	O
)	O
and	O
re	O
-	O
fine	O
-	O
pruning	O
the	O
pre	O
-	O
trained	O
model	O
:	O
the	O
first	O
run	O
marks	O
the	O
attention	O
heads	O
that	O
were	O
not	O
pruned	O
,	O
and	O
the	O
second	O
uses	O
this	O
information	O
to	O
create	O
a	O
positive	O
mask	O
of	O
weights	O
that	O
are	O
protected	O
from	O
pruning	O
.	O
We	O
did	O
not	O
find	O
a	O
significant	O
difference	O
between	O
the	O
two	O
methods	O
.	O
The	O
results	O
presented	O
here	O
do	O
not	O
use	O
rewinding	O
.	O

Main	O
Results	O
We	O
begin	O
by	O
observing	O
the	O
highlevel	O
impact	O
of	O
the	O
different	O
pruning	O
methods	O
.	O
Figure	O
1	O
shows	O
the	O
effect	O
on	O
attention	O
and	O
feedforward	O
layers	O
for	O
the	O
different	O
block	O
pruning	O
methods	O
.	O
We	O
find	O
that	O
all	O
the	O
different	O
block	O
sizes	O
learn	O
to	O
prune	O
out	O
entire	O
dimensions	O
in	O
the	O
FFN	O
layers	O
.	O
Interestingly	O
we	O
find	O
that	O
the	O
block	O
methods	O
can	O
also	O
learn	O
to	O
remove	O
entire	O
heads	O
from	O
the	O
MHA	O
.	O
This	O
pruning	O
pattern	O
makes	O
it	O
possible	O
to	O
remove	O
entire	O
heads	O
from	O
the	O
model	O
during	O
inference	O
.	O
For	O
this	O
reason	O
,	O
we	O
focus	O
on	O
the	O
Hybrid	O
approach	O
as	O
our	O
main	O
method	O
,	O
which	O
can	O
both	O
eliminate	O
feed	O
-	O
forward	O
dimensions	O
while	O
using	O
blocks	O
to	O
remove	O
attention	O
heads	O
gradually	O
.	O
Results	O
on	O
SQuAD	B-DatasetName
are	O
shown	O
in	O
Figure	O
2	O
,	O
which	O
compares	O
our	O
approach	O
for	O
speed	O
and	O
density	O
to	O
baseline	O
BERT	B-MethodName
-	O
Base	O
tuned	O
models	O
such	O
as	O
TinyBERT	O
-	O
6	O
and	O
DistilBERT	B-MethodName
(	O
MobileBERT	B-MethodName
is	O
discussed	O
below	O
)	O
.	O
The	O
main	O
result	O
is	O
that	O
the	O
Hybrid	O
Pruning	O
model	O
is	O
as	O
fast	O
as	O
the	O
baseline	O
and	O
approaches	O
the	O
same	O
accuracy	B-MetricName
while	O
at	O
the	O
same	O
time	O
producing	O
significantly	O
smaller	O
models	O
in	O
terms	O
of	O
density	O
.	O
Moving	O
to	O
the	O
Hybrid	O
Filled	O
model	O
leads	O
to	O
a	O
further	O
gain	O
in	O
speed	O
at	O
a	O
small	O
cost	O
in	O
model	O
density	O
.	O
For	O
instance	O
,	O
for	O
the	O
same	O
F1	B-MetricName
performance	O
of	O
87.5	O
,	O
Hybrid	O
Filled	O
models	O
display	O
a	O
2.5x	O
speedup	O
against	O
1.88	O
for	O
TinyBERT	O
.	O
TinyBERT	O
and	O
Distil	O
-	O
BERT	B-MethodName
have	O
50	O
%	O
of	O
BERT	B-MethodName
's	O
encoder	O
parameters	O
,	O
whereas	O
Hybrid	O
Filled	O
models	O
have	O
25	O
%	O
BERT	B-MethodName
parameters	O
for	O
the	O
same	O
level	O
of	O
accuracy	B-MetricName
.	O
The	O
figures	O
also	O
include	O
two	O
intrinsic	O
baselines	O
:	O
our	O
reimplementation	O
of	O
Movement	B-MethodName
pruning	I-MethodName
and	O
pure	O
Block	O
pruning	O
.	O
We	O
find	O
that	O
our	O
implementation	O
of	O
Movement	B-MethodName
pruning	I-MethodName
is	O
highly	O
effective	O
at	O
producing	O
sparse	O
models	O
(	O
even	O
leading	O
to	O
a	O
small	O
increase	O
in	O
accuracy	B-MetricName
)	O
but	O
does	O
not	O
produce	O
significant	O
speedups	O
.	O
Square	O
Block	O
pruning	O
does	O
better	O
,	O
but	O
not	O
as	O
well	O
as	O
hybrid	O
blocks	O
.	O
Table	O
2	O
gives	O
a	O
full	O
comparison	O
of	O
models	O
with	O
different	O
compression	O
rates	O
.	O
As	O
linear	O
layers	O
represent	O
a	O
very	O
large	O
part	O
of	O
the	O
flops	O
of	O
a	O
transformer	O
model	O
,	O
this	O
compression	O
rate	O
is	O
actually	O
a	O
good	O
measure	O
of	O
the	O
maximum	O
achievable	O
speedup	O
.	O
This	O
number	O
is	O
much	O
higher	O
than	O
the	O
actually	O
measured	O
speedup	O
.	O
This	O
indicates	O
that	O
our	O
setup	O
for	O
measur	O
-	O
BERT	B-MethodName
performance	O
on	O
other	O
tasks	O
.	O
Comparison	O
with	O
MobileBERT	B-MethodName
All	O
methods	O
can	O
be	O
improved	O
further	O
using	O
a	O
larger	O
teacher	O
model	O
.	O
For	O
these	O
experiments	O
,	O
we	O
compare	O
with	O
MobileBERT	B-MethodName
,	O
which	O
uses	O
a	O
BERT	B-MethodName
-	O
large	O
teacher	O
and	O
reaches	O
an	O
F1	B-MetricName
of	O
90.0	O
on	O
SQuAD	B-DatasetName
v1.1	O
on	O
its	O
fastest	O
version	O
.	O
It	O
should	O
be	O
noted	O
that	O
Mo	O
-	O
bileBERT	O
makes	O
use	O
of	O
additional	O
optimizations	O
not	O
present	O
in	O
the	O
original	O
BERT	B-MethodName
-	O
large	O
we	O
are	O
using	O
:	O
LayerNorms	O
are	O
replaced	O
by	O
purely	O
linear	O
NoNorms	O
,	O
and	O
GeLUs	O
are	O
replaced	O
by	O
ReLUs	O
.	O
For	O
these	O
experiments	O
,	O
we	O
use	O
a	O
BERT	B-MethodName
-	O
large	O
teacher	O
to	O
perform	O
meaningful	O
comparisons	O
,	O
using	O
our	O
best	O
method	O
Hybrid	O
Filled	O
.	O
Figure	O
2	O
shows	O
that	O
we	O
have	O
comparable	O
results	O
on	O
SQuAD	B-DatasetName
v1.1	O
,	O
with	O
a	O
simpler	O
optimization	O
approach	O
:	O
we	O
get	O
a	O
slightly	O
better	O
model	O
(	O
F1=90.3	O
)	O
for	O
the	O
same	O
speedup	O
of	O
1.6x	O
,	O
and	O
we	O
get	O
a	O
speedup	O
of	O
2.2x	O
at	O
BERT	B-MethodName
-	O
base	O
accuracy	B-MetricName
(	O
F1=88.5	O
)	O
.	O
We	O
observe	O
that	O
using	O
a	O
large	O
teacher	O
is	O
beneficial	O
even	O
at	O
high	O
levels	O
of	O
pruning	O
:	O
up	O
to	O
80	O
%	O
of	O
sparsity	O
,	O
the	O
resulting	O
student	O
model	O
has	O
better	O
accuracy	B-MetricName
for	O
the	O
same	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
when	O
using	O
a	O
BERTlarge	O
teacher	O
instead	O
of	O
a	O
base	O
one	O
.	O
This	O
trend	O
reverses	O
after	O
this	O
point	O
:	O
a	O
larger	O
teacher	O
is	O
detrimental	O
to	O
accuracy	B-MetricName
when	O
the	O
student	O
is	O
very	O
heavily	O
pruned	O
.	O
Encoder	O
-	O
Decoder	O
Finally	O
,	O
we	O
apply	O
these	O
methods	O
to	O
two	O
encoder	O
-	O
decoder	O
architectures	O
,	O
BARTbase	O
and	O
BART	B-MethodName
-	O
large	O
for	O
the	O
task	O
of	O
summarization	B-TaskName
.	O
For	O
these	O
architectures	O
,	O
the	O
decoder	O
parameters	O
are	O
responsible	O
for	O
a	O
majority	O
of	O
the	O
computational	O
costs	O
,	O
so	O
these	O
are	O
our	O
main	O
focus	O
.	O
Voita	O
et	O
al	O
(	O
2019	O
)	O
observed	O
that	O
for	O
machine	B-TaskName
translation	I-TaskName
models	O
,	O
encoder	O
heads	O
were	O
much	O
easier	O
to	O
prune	O
than	O
decoder	O
ones	O
.	O
We	O
found	O
similar	O
results	O
,	O
e.g.	O
for	O
identical	O
λ	O
att	O
and	O
λ	O
ffn	O
,	O
the	O
encoder	O
was	O
systematically	O
more	O
pruned	O
than	O
the	O
decoder	O
,	O
for	O
both	O
MHA	O
and	O
FFN	O
sub	O
-	O
layers	O
.	O
In	O
order	O
to	O
increase	O
speedup	O
gain	O
,	O
we	O
applied	O
twice	O
as	O
much	O
weight	O
on	O
the	O
decoder	O
compression	O
,	O
which	O
resulted	O
in	O
even	O
pruning	O
ratios	O
among	O
the	O
encoder	O
and	O
decoder	O
.	O
Table	O
4	O
shows	O
the	O
main	O
results	O
.	O
We	O
see	O
that	O
Hybrid	O
pruning	O
leads	O
to	O
large	O
decoder	O
compression	O
ratios	O
(	O
3.4	O
on	O
BART	B-MethodName
-	O
base	O
and	O
3.5	O
BART	B-MethodName
-	O
large	O
)	O
with	O
only	O
a	O
small	O
drop	O
in	O
ROUGE	O
score	O
.	O
Speedups	O
reach	O
1.4	O
times	O
of	O
the	O
original	O
speed	O
.	O
(	O
Given	O
the	O
large	O
decoder	O
compression	O
rates	O
,	O
we	O
would	O
expect	O
larger	O
speedups	O
to	O
be	O
possible	O
with	O
further	O
engineering	O
of	O
the	O
inference	O
.	O
)	O
There	O
is	O
less	O
comparable	O
work	O
for	O
pre	O
-	O
trained	O
encoder	O
-	O
decoders	O
.	O
We	O
compare	O
our	O
approach	O
with	O
a	O
distillation	O
-	O
based	O
approach	O
dBART	O
(	O
Shleifer	O
and	O
Rush	O
,	O
2020	O
)	O
.	O
This	O
approach	O
yields	O
a	O
similar	O
speedup	O
gain	O
with	O
a	O
smaller	O
drop	O
in	O
performance	O
but	O
less	O
sparsity	O
.	O
For	O
models	O
of	O
comparable	O
sizes	O
(	O
158	O
M	O
for	O
our	O
Hybrid	O
NT	O
vs	O
176	O
M	O
for	O
dBART	O
-	O
6	O
-	O
6	O
)	O
,	O
we	O
observe	O
a	O
drop	O
of	O
0.7	O
in	O
R2	O
and	O
0.4	O
in	O
RL	O
against	O
0.9	O
in	O
R2	O
and	O
1.3	O
in	O
RL	O
for	O
dBART	O
-	O
6	O
-	O
6	O
.	O
As	O
with	O
encoder	O
-	O
only	O
models	O
,	O
the	O
two	O
approaches	O
could	O
likely	O
be	O
combined	O
to	O
yield	O
even	O
faster	O
,	O
smaller	O
models	O
.	O
3	O

Large	O
Model	O
Pruning	O
To	O
test	O
that	O
this	O
approach	O
scales	O
to	O
large	O
models	O
,	O
we	O
apply	O
Hybrid	O
pruning	O
on	O
BERT	B-MethodName
-	O
large	O
on	O
SQuAD	B-DatasetName
v1.1	O
.	O
We	O
observe	O
similar	O
results	O
:	O
a	O
18	O
%	O
dense	O
BERT	B-MethodName
-	O
large	O
has	O
a	O
F1	B-MetricName
of	O
90.2	O
,	O
with	O
a	O
speedup	O
of	O
3.2x	O
compared	O
to	O
BERT	B-MethodName
-	O
large	O
with	O
a	O
F1	B-MetricName
of	O
93.2	O
.	O
This	O
pruned	O
model	O
is	O
actually	O
faster	O
than	O
a	O
BERT	B-MethodName
-	O
base	O
model	O
(	O
Table	O
5	O
)	O
.	O
We	O
can	O
compare	O
Hybrid	O
Pruning	O
of	O
SQuAD	B-DatasetName
v2	O
BERT	B-MethodName
-	O
large	O
models	O
with	O
the	O
results	O
of	O
the	O
structured	O
pruning	O
method	O
described	O
in	O
McCarley	O
(	O
2019	O
)	O
.	O
For	O
a	O
17	O
%	O
dense	O
model	O
,	O
we	O
obtain	O
a	O
F1	B-MetricName
of	O
82.6	O
,	O
whereas	O
structured	O
pruning	O
gets	O
a	O
25	O
%	O
dense	O
model	O
with	O
a	O
F1	B-MetricName
of	O
81.5	O
.	O
This	O
result	O
is	O
in	O
line	O
with	O
Li	O
et	O
al	O
(	O
2020	O
)	O
:	O
the	O
larger	O
the	O
model	O
,	O
the	O
more	O
pruning	O
is	O
effective	O
.	O
When	O
pruning	O
a	O
larger	O
model	O
,	O
the	O
final	O
model	O
is	O
actually	O
better	O
than	O
a	O
smaller	O
one	O
with	O
the	O
same	O
absolute	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
.	O
Block	O
Size	O
Influence	O
Figure	O
3	O
shows	O
the	O
impact	O
of	O
different	O
block	O
sizes	O
on	O
Block	O
pruning	O
:	O
pruning	O
is	O
done	O
on	O
attention	B-HyperparameterName
layers	I-HyperparameterName
and	O
FFNs	O
with	O
the	O
same	O
square	O
block	O
size	O
,	O
from	O
(	O
4	O
,	O
4	O
)	O
to	O
(	O
32	O
,	O
32	O
)	O
,	O
with	O
a	O
BERT	B-MethodName
-	O
base	O
teacher	O
.	O
We	O
can	O
see	O
that	O
we	O
reach	O
the	O
BERT	B-MethodName
-	O
base	O
original	O
F1	B-MetricName
for	O
all	O
block	O
sizes	O
from	O
4	O
to	O
32	O
,	O
but	O
with	O
a	O
speedup	O
that	O
increases	O
with	O
the	O
block	O
size	O
.	O
The	O
maximum	O
reachable	O
speedup	O
without	O
F1	B-MetricName
drop	O
is	O
1.3	O
for	O
a	O
block	O
size	O
of	O
32	O
.	O
But	O
when	O
some	O
drop	O
of	O
F1	B-MetricName
is	O
allowed	O
,	O
the	O
speedup	O
increases	O
quickly	O
with	O
the	O
block	O
size	O
and	O
plateau	O
when	O
reaching	O
16	O
.	O
We	O
then	O
reach	O
a	O
speedup	O
of	O
1.75	O
for	O
an	O
F1	B-MetricName
drop	O
of	O
2	O
%	O
and	O
a	O
block	O
size	O
of	O
32	O
.	O
We	O
also	O
note	O
that	O
,	O
with	O
the	O
original	O
Movement	B-MethodName
Pruning	I-MethodName
method	O
,	O
we	O
see	O
some	O
speedup	O
due	O
to	O
full	O
dimension	O
pruning	O
.	O
This	O
likely	O
comes	O
from	O
our	O
improved	O
set	O
of	O
hyper	O
-	O
parameters	O
(	O
compared	O
to	O
the	O
original	O
paper	O
)	O
,	O
allowing	O
us	O
to	O
remove	O
some	O
empty	O
rows	O
and	O
columns	O
in	O
the	O
FFN	O
layers	O
.	O
However	O
we	O
see	O
that	O
using	O
blocks	O
leads	O
to	O
a	O
significant	O
speed	O
improvement	O
compared	O
to	O
Movement	B-MethodName
Pruning	I-MethodName
.	O
Quantization	B-TaskName
Quantization	B-TaskName
is	O
often	O
of	O
critical	O
importance	O
for	O
practical	O
applications	O
.	O
We	O
,	O
therefore	O
,	O
wanted	O
to	O
check	O
that	O
our	O
networks	O
could	O
be	O
subjected	O
to	O
quantization	B-TaskName
without	O
significant	O
loss	B-MetricName
of	O
accuracy	B-MetricName
,	O
especially	O
when	O
considering	O
the	O
issues	O
that	O
could	O
arise	O
with	O
the	O
high	O
level	O
of	O
sparsity	O
of	O
some	O
FFNs	O
.	O
Table	O
6	O
shows	O
the	O
results	O
of	O
full	O
8	O
-	O
bit	O
quantization	B-TaskName
tests	O
on	O
our	O
models	O
.	O
These	O
indicate	O
that	O
the	O
method	O
is	O
compatible	O
with	O
quantization	B-TaskName
,	O
and	O
the	O
models	O
using	O
quantization	B-TaskName
on	O
top	O
of	O
our	O
pruning	O
method	O
achieve	O
very	O
high	O
gains	O
in	O
terms	O
of	O
size	O
(	O
as	O
well	O
as	O
speed	O
)	O
.	O

Compress	O
EM	B-MetricName
F1	B-MetricName
As	O
shown	O
in	O
Table	O
7	O
,	O
combining	O
hybrid	O
pruning	O
with	O
distillation	O
always	O
performs	O
better	O
than	O
pruning	O
alone	O
,	O
but	O
that	O
it	O
is	O
not	O
critical	O
for	O
the	O
approach	O
to	O
work	O
.	O
The	O
distillation	O
effect	O
is	O
larger	O
for	O
smaller	O
datasets	O
such	O
as	O
SST	B-DatasetName
-	O
2	O
,	O
which	O
are	O
prone	O
to	O
over	O
-	O
fitting	O
.	O
We	O
believe	O
that	O
the	O
regularization	O
brought	O
by	O
pruning	O
and	O
distillation	O
counters	O
overfitting	O
caused	O
by	O
the	O
additional	O
number	O
of	O
steps	O
needed	O
for	O
pruning	O
.	O

We	O
show	O
here	O
the	O
effect	O
of	O
the	O
pattern	O
on	O
the	O
head	O
number	O
reduction	O
:	O
using	O
block	O
instead	O
of	O
row	O
/	O
column	O
pruning	O
leads	O
to	O
a	O
much	O
larger	O
number	O
of	O
pruned	O
heads	O
while	O
improving	O
accuracy	B-MetricName
,	O
here	O
on	O
the	O
SST	B-DatasetName
-	O
2	O
task	O
.	O
We	O
are	O
using	O
Block	O
Movement	B-MethodName
pruning	I-MethodName
for	O
each	O
model	O
,	O
with	O
different	O
block	O
patterns	O
,	O
pruning	O
only	O
the	O
attention	B-HyperparameterName
layers	I-HyperparameterName
.	O
Compression	O
measures	O
the	O
reduction	O
of	O
the	O
number	O
of	O
non	O
-	O
zero	O
parameters	O
in	O
attention	O
linear	O
layers	O
,	O
whereas	O
head	O
compression	O
measures	O
the	O
reduction	O
of	O
the	O
number	O
of	O
complete	O
non	O
-	O
zero	O
heads	O
.	O

We	O
select	O
speed	O
as	O
our	O
main	O
metric	O
to	O
compare	O
with	O
other	O
techniques	O
,	O
as	O
it	O
is	O
the	O
major	O
practical	O
measure	O
of	O
inference	O
efficiency	O
.	O
On	O
this	O
metric	O
,	O
we	O
decided	O
to	O
compare	O
our	O
models	O
to	O
the	O
best	O
models	O
available	O
i.e.	O
the	O
distilled	O
models	O
(	O
MobileBERT	B-MethodName
,	O
TinyBERT	O
)	O
,	O
even	O
though	O
the	O
method	O
is	O
different	O
,	O
as	O
they	O
are	O
the	O
strongest	O
"	O
speed	O
/	O
accuracy	B-MetricName
"	O
baseline	O
available	O
.	O
In	O
Table	O
9	O
we	O
compare	O
with	O
TinyBERT	O
(	O
Jiao	O
et	O
al	O
,	O
2019	O
)	O
and	O
MobileBERT	B-MethodName
(	O
Sun	O
et	O
al	O
,	O
2020	O
We	O
compare	O
as	O
well	O
to	O
Hybrid	O
pruning	O
,	O
with	O
and	O
without	O
a	O
teacher	O
,	O
with	O
the	O
unstructured	O
methods	O
from	O
Sanh	O
et	O
al	O
(	O
2020	O
)	O
(	O
the	O
original	O
Movement	B-MethodName
Pruning	I-MethodName
method	O
we	O
are	O
using	O
)	O
and	O
Gordon	O
et	O
al	O
(	O
2020	O
)	O

Automatic	O
evaluation	O
scores	O
do	O
not	O
reflect	O
the	O
quality	O
improvements	O
.	O
APE	B-DatasetName
for	O
MT	O
has	O
been	O
using	O
automatic	O
metrics	O
,	O
such	O
as	O
BLEU	B-MetricName
,	O
to	O
benchmark	O
progress	O
(	O
Libovickỳ	O
et	O
al	O
,	O
2016	O
)	O
.	O
However	O
,	O
classic	O
automatic	O
evaluation	O
metrics	O
fail	O
to	O
capture	O
the	O
signal	O
in	O
human	O
judgments	O
for	O
the	O
proposed	O
visual	O
story	O
post	O
-	O
editing	O
task	O
.	O
We	O
first	O
use	O
the	O
human	O
-	O
edited	O
stories	O
as	O
references	O
,	O
but	O
all	O
the	O
automatic	O
evaluation	O
metrics	O
generate	O
lower	O
scores	O
when	O
human	O
judges	O
give	O
a	O
higher	O
rating	O
(	O
Table	O
3	O
We	O
then	O
switch	O
to	O
use	O
the	O
human	O
-	O
written	O
stories	O
(	O
VIST	B-DatasetName
test	O
set	O
)	O
as	O
references	O
,	O
but	O
again	O
,	O
all	O
the	O
automatic	O
evaluation	O
metrics	O
generate	O
lower	O
scores	O
even	O
when	O
the	O
editing	O
was	O
done	O
by	O
human	O
(	O
Table	O
4	O
.	O
)	O
Table	O
5	O
further	O
shows	O
the	O
Spearman	O
rank	O
-	O
order	O
correlation	O
ρ	O
between	O
the	O
automatic	O
evaluation	O
scores	O
(	O
sum	O
of	O
all	O
six	O
aspects	O
)	O
and	O
human	O
judgment	O
calculated	O
using	O
different	O
data	O
combination	O
.	O
In	O
row	O
{	O
of	O
Table	O
5	O
,	O
the	O
reported	O
correlation	O
ρ	O
of	O
METEOR	B-DatasetName
is	O
consistent	O
with	O
the	O
findings	O
in	O
Huang	O
et	O
al	O
(	O
2016	O
)	O
,	O
which	O
suggests	O
that	O
METEOR	B-DatasetName
could	O
be	O
useful	O
when	O
comparing	O
among	O
stories	O
generated	O
by	O
the	O
same	O
visual	B-TaskName
storytelling	I-TaskName
model	O
.	O
However	O
,	O
when	O
comparing	O
among	O
machine	O
-	O
edited	O
stories	O
(	O
row	O
y	O
and	O
|	O
)	O
,	O
among	O
pre	O
-	O
and	O
post	O
-	O
edited	O
stories	O
(	O
row	O
z	O
and	O
}	O
)	O
,	O
or	O
among	O
any	O
combinations	O
of	O
them	O
(	O
row~	O
,	O
and	O
)	O
,	O
all	O
metrics	O
result	O
in	O
weak	O
correlations	O
with	O
human	O
judgments	O
.	O
These	O
results	O
strongly	O
suggest	O
the	O
need	O
of	O
a	O
new	O
automatic	O
evaluation	O
metric	O
for	O
visual	O
story	O
postediting	O
task	O
.	O
Some	O
new	O
metrics	O
have	O
recently	O
been	O
introduced	O
using	O
linguistic	O
(	O
Roemmele	O
and	O
Gor	O
-	O
don	O
,	O
2018a	O
)	O
or	O
story	O
features	O
(	O
Purdy	O
et	O
al	O
,	O
2018	O
)	O
to	O
evaluate	O
story	O
automatically	O
.	O
More	O
research	O
is	O
needed	O
to	O
examine	O
whether	O
these	O
metrics	O
are	O
useful	O
for	O
story	O
post	O
-	O
editing	O
tasks	O
too	O
.	O

The	O
approach	O
we	O
used	O
to	O
build	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
is	O
inspired	O
from	O
the	O
one	O
proposed	O
in	O
(	O
Bengio	O
and	O
Heigold	O
,	O
2014	O
)	O
.	O
The	O
deep	O
neural	O
architecture	O
depicted	O
in	O
figure	O
1	O
is	O
used	O
to	O
train	O
the	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
.	O
It	O
relies	O
on	O
a	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
classifier	O
over	O
words	O
and	O
on	O
a	O
deep	O
neural	O
network	O
(	O
DNN	O
)	O
trained	O
by	O
using	O
a	O
triplet	O
ranking	O
loss	B-MetricName
(	O
Bengio	O
and	O
Heigold	O
,	O
2014	O
;	O
Wang	O
et	O
al	O
,	O
2014	O
;	O
Weston	O
et	O
al	O
,	O
2011	O
)	O
.	O
The	O
two	O
architectures	O
are	O
trained	O
using	O
different	O
inputs	O
:	O
speech	O
signal	O
and	O
orthographic	O
representation	O
of	O
the	O
word	O
,	O
which	O
are	O
detailed	O
as	O
follows	O
.	O
The	O
convolutional	O
neural	O
network	O
classifier	O
is	O
trained	O
independently	O
to	O
predict	O
a	O
word	O
given	O
a	O
speech	O
signal	O
as	O
input	O
.	O
It	O
is	O
composed	O
of	O
convolution	B-MethodName
and	O
pooling	O
layers	O
,	O
followed	O
by	O
fully	O
connected	O
layers	O
which	O
feed	O
the	O
final	O
softmax	B-MethodName
layer	O
.	O
The	O
embedding	O
layer	O
is	O
the	O
fully	O
connected	O
layer	O
just	O
below	O
the	O
softmax	B-MethodName
one	O
,	O
named	O
s	O
in	O
the	O
figure	O
1	O
.	O
This	O
representation	O
contains	O
a	O
compact	O
representation	O
of	O
the	O
acoustic	O
signal	O
.	O
It	O
tends	O
to	O
preserve	O
acoustic	O
similarity	O
between	O
words	O
,	O
such	O
that	O
words	O
are	O
close	O
in	O
this	O
space	O
if	O
they	O
sound	O
alike	O
.	O
The	O
feedforward	O
neural	O
network	O
(	O
DNN	O
)	O
is	O
used	O
with	O
the	O
purpose	O
to	O
build	O
an	O
acoustic	O
word	O
embedding	O
for	O
a	O
word	O
not	O
observed	O
in	O
the	O
audio	O
training	O
corpus	O
,	O
based	O
on	O
its	O
orthographic	O
representation	O
.	O
It	O
is	O
trained	O
using	O
the	O
triplet	O
ranking	O
loss	B-MetricName
function	O
in	O
order	O
to	O
project	O
orthographic	O
word	O
representations	O
to	O
the	O
same	O
space	O
as	O
the	O
acoustic	O
embeddings	O
s.	O
The	O
orthographic	O
word	O
representation	O
consists	O
in	O
a	O
bag	O
of	O
n	O
-	O
grams	O
(	O
n	O
≤	O
3	O
)	O
of	O
letters	O
,	O
with	O
additional	O
special	O
symbols	O
[	O
and	O
]	O
to	O
specify	O
the	O
start	O
and	O
the	O
end	O
of	O
a	O
word	O
.	O
The	O
size	O
of	O
this	O
bag	O
of	O
ngrams	O
vector	O
is	O
reduced	O
using	O
an	O
auto	O
-	O
encoder	O
.	O
During	O
the	O
training	O
process	O
,	O
this	O
model	O
takes	O
as	O
inputs	O
acoustic	O
embeddings	O
s	O
selected	O
randomly	O
from	O
the	O
training	O
set	O
and	O
,	O
for	O
each	O
signal	O
acoustic	O
embedding	O
,	O
the	O
orthographic	O
representation	O
of	O
the	O
matching	O
word	O
o	O
+	O
,	O
and	O
the	O
orthographic	O
representation	O
of	O
a	O
randomly	O
selected	O
word	O
different	O
to	O
the	O
first	O
word	O
o	O
−	O
.	O
These	O
two	O
orthographic	O
representations	O
supply	O
shared	O
parameters	O
in	O
the	O
DNN	O
.	O
The	O
resulting	O
DNN	O
model	O
can	O
then	O
be	O
used	O
to	O
build	O
an	O
acoustic	O
word	O
embedding	O
(	O
w	O
+	O
)	O
from	O
any	O
word	O
,	O
as	O
long	O
as	O
one	O
can	O
extract	O
an	O
orthographic	O
representation	O
from	O
it	O
.	O
This	O
acoustic	O
word	O
embedding	O
can	O
be	O
perceived	O
as	O
a	O
canonical	O
acoustic	O
representation	O
for	O
a	O
word	O
,	O
since	O
different	O
pronunciations	O
imply	O
different	O
signal	O
embeddings	O
s.	O

In	O
the	O
literature	O
(	O
Kamper	O
et	O
al	O
,	O
2015	O
;	O
Levin	O
et	O
al	O
,	O
2013	O
;	O
Carlin	O
et	O
al	O
,	O
2011	O
)	O
,	O
a	O
word	O
discrimination	O
task	O
was	O
used	O
to	O
evaluate	O
acoustic	O
embeddings	O
s.	O
Given	O
a	O
pair	O
of	O
acoustic	O
segments	O
,	O
this	O
task	O
consists	O
on	O
deciding	O
whether	O
the	O
segments	O
correspond	O
to	O
the	O
same	O
words	O
or	O
not	O
.	O
This	O
evalua	O
-	O
tion	O
task	O
can	O
be	O
performed	O
on	O
many	O
ways	O
,	O
for	O
example	O
through	O
the	O
use	O
of	O
a	O
dynamic	B-MethodName
time	I-MethodName
warping	I-MethodName
(	O
DTW	B-MethodName
)	O
to	O
quantify	O
the	O
similarity	O
between	O
two	O
segments	O
when	O
using	O
frame	O
level	O
embeddings	O
(	O
Thiolliere	O
et	O
al	O
,	O
2015	O
)	O
,	O
or	O
by	O
using	O
the	O
euclidean	O
distance	O
or	O
the	O
cosine	O
similarity	O
between	O
embeddings	O
representing	O
the	O
segments	O
.	O
In	O
(	O
Kamper	O
et	O
al	O
,	O
2015	O
)	O
the	O
evaluation	O
was	O
conducted	O
on	O
two	O
collections	O
of	O
words	O
(	O
train	O
and	O
test	O
)	O
coming	O
from	O
the	O
Switchboard	O
English	O
corpus	O
.	O
After	O
training	O
the	O
model	O
on	O
the	O
training	O
corpus	O
,	O
the	O
cosine	O
similarity	O
is	O
computed	O
between	O
the	O
embeddings	O
of	O
each	O
pair	O
of	O
words	O
in	O
the	O
test	O
set	O
.	O
These	O
pairs	O
are	O
classified	O
as	O
similar	O
or	O
different	O
by	O
applying	O
a	O
threshold	O
on	O
their	O
distance	O
,	O
and	O
a	O
precisionrecall	O
curve	O
is	O
obtained	O
by	O
varying	O
the	O
threshold	O
.	O
In	O
this	O
study	O
,	O
we	O
propose	O
two	O
approaches	O
to	O
evaluate	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
w	O
+	O
.	O
We	O
suggest	O
to	O
build	O
different	O
evaluation	O
sets	O
in	O
order	O
to	O
assess	O
the	O
acoustic	O
word	B-TaskName
embeddings	I-TaskName
(	O
w	O
+	O
)	O
performances	O
on	O
orthographic	O
and	O
phonetic	O
similarity	O
and	O
homophones	O
detection	O
tasks	O
.	O
We	O
remind	O
that	O
the	O
acoustic	O
word	O
embedding	O
w	O
+	O
is	O
a	O
projection	O
of	O
an	O
orthographic	O
word	O
representation	O
o	O
+	O
into	O
the	O
space	O
of	O
acoustic	O
signal	O
embeddings	O
s.	O
In	O
our	O
evaluation	O
,	O
we	O
would	O
like	O
to	O
measure	O
the	O
loss	B-MetricName
of	O
orthographic	O
information	O
carried	O
by	O
w	O
+	O
and	O
the	O
potential	O
gain	O
of	O
acoustic	O
information	O
due	O
to	O
this	O
projection	O
,	O
in	O
comparison	O
to	O
the	O
information	O
carried	O
by	O
o	O
+	O
.	O
The	O
evaluation	O
sets	O
are	O
built	O
as	O
follows	O
:	O
given	O
a	O
list	O
L	O
of	O
n	O
frequent	O
words	O
(	O
candidate	O
words	O
)	O
in	O
the	O
vocabulary	O
composed	O
of	O
m	O
words	O
,	O
a	O
list	O
of	O
n	O
×	O
m	O
word	O
pairs	O
was	O
created	O
.	O
Then	O
,	O
two	O
alignments	O
were	O
performed	O
between	O
each	O
word	O
pair	O
based	O
on	O
their	O
orthographic	O
(	O
letters	O
)	O
and	O
phonetic	O
(	O
phonemes	O
)	O
representations	O
,	O
using	O
the	O
sclite	O
1	O
tool	O
.	O
From	O
these	O
alignment	O
two	O
edition	O
distances	O
are	O
computed	O
with	O
respect	O
to	O
the	O
alignment	O
results	O
of	O
orthographic	O
and	O
phonetic	O
representations	O
.	O
The	O
Edition	O
distance	O
is	O
computed	O
as	O
follows	O
:	O
SER	O
=	O
#	O
In	O
+	O
#	O
Sub	O
+	O
#	O
Del	O
#	O
symbols	O
in	O
the	O
ref	O
erence	O
word	O
×	O
100	O
(	O
1	O
)	O
where	O
SER	O
stands	O
for	O
Symbol	O
Error	B-MetricName
rate	O
,	O
symbols	O
correspond	O
to	O
the	O
letters	O
for	O
orthographic	O
representations	O
,	O
and	O
to	O
the	O
phonemes	O
for	O
phonetic	O
ones	O
,	O
and	O
In	O
,	O
Sub	O
and	O
Del	O
correspond	O
respectively	O
to	O
insertion	O
,	O
substitution	O
and	O
deletion	O
.	O
Next	O
,	O
we	O
compute	O
two	O
similarity	O
scores	O
that	O
correspond	O
to	O
the	O
orthographic	O
and	O
phonetic	O
similarity	O
scores	O
sim	O
score	O
attributed	O
for	O
each	O
pair	O
of	O
words	O
,	O
which	O
are	O
defined	O
as	O
:	O
sim	O
score	O
=	O
10	O
−	O
min	O
(	O
10	O
,	O
SER/10	O
)	O
(	O
2	O
)	O
where	O
min	O
(	O
)	O
is	O
a	O
function	O
used	O
to	O
have	O
an	O
edition	O
distance	O
between	O
0	B-DatasetName
and	O
10	O
.	O
Then	O
,	O
for	O
each	O
candidate	O
word	O
in	O
the	O
list	O
L	O
we	O
extract	O
its	O
orthographically	O
and	O
phonetically	O
10	O
nearest	O
words	O
.	O
This	O
results	O
in	O
two	O
lists	O
for	O
orthographic	O
and	O
phonetic	O
similarity	O
tasks	O
.	O
For	O
each	O
candidate	O
word	O
in	O
the	O
list	O
L	O
,	O
the	O
Orthographic	O
list	O
contains	O
its	O
ten	O
closest	O
words	O
in	O
terms	O
of	O
orthographic	O
similarity	O
scores	O
and	O
the	O
Phonetic	O
list	O
contains	O
its	O
ten	O
closest	O
words	O
in	O
terms	O
of	O
phonetic	O
similarity	O
scores	O
.	O
Finally	O
,	O
the	O
Homophones	O
list	O
,	O
used	O
for	O
the	O
homophone	O
detection	O
task	O
,	O
contains	O
the	O
homophone	O
words	O
(	O
i.e.	O
sharing	O
the	O
same	O
phonetic	O
representation	O
)	O
.	O
Table	O
1	O
shows	O
an	O
example	O
of	O
the	O
content	O
of	O
the	O
three	O
lists	O
.	O

With	O
the	O
acquirement	O
of	O
the	O
compressed	O
local	O
and	O
global	O
hyperbolic	O
capsule	O
set	O
{	O
u	O
1	O
,	O
.	O
.	O
.	O
,	O
u	O
P	O
}	O
in	O
layer	O
,	O
let	O
{	O
v	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
Q	O
}	O
denote	O
the	O
label	O
-	O
aware	O
hyperbolic	O
capsule	O
set	O
in	O
the	O
next	O
layer	O
+1	O
,	O
where	O
Q	O
equals	O
to	O
the	O
number	O
of	O
labels	O
.	O
Following	O
(	O
Sabour	O
et	O
al	O
,	O
2017	O
)	O
,	O
the	O
compressed	O
hyperbolic	O
capsules	O
are	O
firstly	O
transformed	O
into	O
a	O
set	O
of	O
prediction	O
capsules	O
{	O
û	O
j	O
|	O
1	O
,	O
.	O
.	O
.	O
,	O
û	O
j	O
|	O
P	O
}	O
for	O
the	O
j	O
-	O
th	O
label	O
-	O
aware	O
capsule	O
,	O
each	O
of	O
them	O
is	O
calculated	O
byû	O
j	O
|	O
i	O
=	O
W	O
ij	O
⊗	O
u	O
i	O
B	O
d	O
,	O
(	O
10	O
)	O
where	O
W	O
ij	O
is	O
a	O
learnable	O
parameter	O
.	O
Then	O
v	O
j	O
is	O
calculated	O
as	O
a	O
weighted	O
Möbius	O
summation	O
over	O
all	O
the	O
prediction	O
capsules	O
by	O
v	O
j	O
=	O
M	O
u	O
j	O
|	O
i	O
{	O
û	O
j	O
|	O
1	O
,	O
...	O
,	O
û	O
j	O
|	O
P	O
}	O
c	O
ij	O
⊗û	O
j	O
|	O
i	O
,	O
(	O
11	O
)	O
where	O
c	O
ij	O
denotes	O
the	O
coupling	O
coefficient	O
that	O
indicates	O
the	O
connection	O
strength	O
betweenû	O
j	O
|	O
i	O
and	O
v	O
j	O
.	O
The	O
coupling	O
coefficient	O
c	O
ij	O
is	O
iteratively	O
updated	O
during	O
the	O
HDR	O
procedure	O
and	O
computed	O
by	O
the	O
routing	O
softmax	B-MethodName
c	O
ij	O
=	O
exp	O
(	O
b	O
ij	O
)	O
k	O
exp	O
(	O
b	O
ik	O
)	O
,	O
(	O
12	O
)	O
where	O
the	O
logits	O
b	O
ij	O
are	O
the	O
log	O
prior	O
probabilities	O
between	O
capsule	O
i	O
and	O
j	O
,	O
which	O
are	O
initialized	O
as	O
0	B-DatasetName
.	O
Once	O
the	O
label	O
-	O
aware	O
hyperbolic	O
capsules	O
are	O
produced	O
,	O
each	O
b	O
ij	O
is	O
then	O
updated	O
by	O
b	O
ij	O
=	O
b	O
ij	O
+	O
K	O
(	O
d	O
B	O
(	O
v	O
j	O
,	O
û	O
j	O
|	O
i	O
)	O
)	O
,	O
(	O
13	O
)	O
where	O
d	O
B	O
(	O
,	O
)	O
denotes	O
the	O
Poincaré	O
distance	O
,	O
which	O
can	O
be	O
written	O
as	O
d	O
B	O
(	O
u	O
,	O
v	O
)	O
=	O
cosh	O
−1	O
(	O
1	O
+	O
1	O
2	O
λ	O
u	O
λ	O
v	O
u	O
−	O
v	O
2	O
)	O
.	O
(	O
14	O
)	O
And	O
K	O
is	O
a	O
Epanechnikov	O
kernel	O
function	O
(	O
Wand	O
and	O
Jones	O
,	O
1994	O
)	O
with	O
K	B-HyperparameterName
=	I-HyperparameterName
γ	B-HyperparameterName
−	O
x	O
,	O
x	O
[	O
0	B-DatasetName
,	O
γ	B-HyperparameterName
)	O
0	B-DatasetName
,	O
x	O
≥	O
γ	B-HyperparameterName
(	O
15	O
)	O
where	O
γ	B-HyperparameterName
is	O
the	O
maximum	O
Poincaré	O
distance	O
between	O
two	O
points	O
in	O
the	O
Poincaré	O
ball	O
,	O
which	O
is	O
d	O
B	O
(	O
p	O
,	O
0	B-DatasetName
)	O
with	O
p	O
=	O
1	O
−	O
(	O
=	O
10	O
−5	O
)	O
to	O
avoid	O
numerical	O
errors	O
.	O
HDR	O
is	O
summarized	O
in	O
Algorithm	O
1	O
.	O
Different	O
from	O
the	O
routing	O
procedure	O
described	O
in	O
(	O
Sabour	O
et	O
al	O
,	O
2017	O
)	O
,	O
HDR	O
does	O
not	O
require	O
the	O
squashing	O
function	O
since	O
all	O
the	O
hyperbolic	O
capsules	O
are	O
constrained	O
in	O
the	O
Poincaré	O
ball	O
.	O

The	O
large	O
amount	O
of	O
labels	O
in	O
MLC	O
is	O
one	O
major	O
source	O
of	O
the	O
computational	O
complexity	O
for	O
the	O
routing	O
procedure	O
.	O
Since	O
most	O
of	O
the	O
labels	O
are	O
unrelated	O
to	O
a	O
document	O
,	O
calculating	O
the	O
label	O
-	O
aware	O
hyperbolic	O
capsules	O
for	O
all	O
the	O
unrelated	O
labels	O
is	O
redundant	O
.	O
Therefore	O
,	O
encoding	O
based	O
adaptive	O
routing	O
layer	O
is	O
used	O
to	O
efficiently	O
decide	O
the	O
candidate	O
labels	O
for	O
the	O
document	O
.	O
The	O
adaptive	O
routing	O
layer	O
produces	O
the	O
candidate	O
probability	O
of	O
each	O
label	O
by	O
Initialize	O
∀i	O
,	O
j	O
:	O
b	O
ij	O
0	B-DatasetName
c	O
=	O
σ	O
(	O
W	O
c	O
1	O
T	O
e	O
i	O
E	O
e	O
i	O
+	O
b	O
c	O
)	O
,	O
(	O
16	O
)	O
3	O
:	O
for	O
r	O
iterations	O
do	O
4	O
:	O
for	O
all	O
capsule	O
i	O
in	O
layer	O
and	O
capsule	O
j	O
in	O
layer	O
+	O
1	O
:	O
c	O
ij	O
softmax	B-MethodName
(	O
b	O
ij	O
)	O
Eq	O
.	O
12	O
5	O
:	O
for	O
all	O
capsule	O
j	O
in	O
layer	O
(	O
+	O
1	O
)	O
:	O
v	O
j	O
M	O
i	O
c	O
ij	O
⊗û	O
j	O
|	O
i	O
6	O
:	O
for	O
all	O
capsule	O
i	O
in	O
layer	O
and	O
capsule	O
j	O
in	O
layer	O
+	O
1	O
:	O
b	O
ij	O
b	O
ij	O
+	O
K	O
(	O
d	O
B	O
(	O
v	O
j	O
,	O
û	O
j	O
|	O
i	O
)	O
)	O
7	O
:	O
return	O
v	O
j	O
where	O
σ	O
denotes	O
the	O
Sigmoid	O
function	O
.	O
W	O
c	O
and	O
the	O
bias	O
b	O
c	O
are	O
learnable	O
parameters	O
updated	O
by	O
minimizing	O
the	O
binary	O
cross	O
-	O
entropy	O
loss	B-MetricName
(	O
Liu	O
et	O
al	O
,	O
2017	O
)	O
L	O
c	O
=	O
−	O
Q	O
j=1	O
y	O
j	O
log	O
(	O
c	O
j	O
)	O
+	O
(	O
1	O
−	O
y	O
j	O
)	O
log	O
(	O
1	O
−	O
c	O
j	O
)	O
,	O
(	O
17	O
)	O
where	O
c	O
j	O
[	O
0	B-DatasetName
,	O
1	O
]	O
is	O
the	O
j	O
-	O
th	O
element	O
in	O
c	O
and	O
y	O
j	O
{	O
0	B-DatasetName
,	O
1	O
}	O
denotes	O
the	O
ground	O
truth	O
about	O
label	O
j.	O
The	O
adaptive	O
routing	O
layer	O
selects	O
the	O
candidate	O
labels	O
during	O
test	O
.	O
Label	O
-	O
aware	O
hyperbolic	O
capsules	O
are	O
then	O
constructed	O
via	O
HDR	O
to	O
predict	O
probabilities	O
of	O
these	O
candidate	O
labels	O
.	O
During	O
the	O
training	O
process	O
,	O
negative	O
sampling	O
is	O
used	O
to	O
improve	O
the	O
the	O
scalability	O
of	O
HYPERCAPS	O
.	O
Let	O
N	O
+	O
denote	O
the	O
true	O
label	O
set	O
and	O
N	O
−	O
denote	O
the	O
set	O
of	O
randomly	O
selected	O
negative	O
labels	O
,	O
the	O
loss	B-MetricName
function	O
is	O
derived	O
as	O
L	O
f	O
=	O
−	O
j	O
N	O
+	O
log	O
(	O
a	O
j	O
)	O
+	O
j	O
N	O
−	O
log	O
(	O
1	O
−	O
a	O
j	O
)	O
,	O
(	O
18	O
)	O
where	O
a	O
j	O
=	O
σ	O
(	O
d	O
B	O
(	O
v	O
j	O
,	O
0	B-DatasetName
)	O
)	O
is	O
activations	O
of	O
the	O
j	O
-	O
th	O
label	O
-	O
aware	O
capsules	O
,	O
which	O
is	O
proportional	O
to	O
the	O
distance	O
from	O
the	O
origin	O
of	O
the	O
Poincaré	O
ball	O
.	O

The	O
proposed	O
HYPERCAPS	O
is	O
evaluated	O
on	O
the	O
four	O
benchmark	O
datasets	O
by	O
comparing	O
with	O
the	O
six	O
baselines	O
in	O
terms	O
of	O
P@k	O
and	O
nDCG@k	O
with	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
,	O
3	O
,	O
5	O
.	O
Results	O
on	O
all	O
the	O
labels	O
averaged	O
over	O
the	O
test	O
instances	O
are	O
shown	O
in	O
Table	O
2	O
.	O
nDCG@1	O
is	O
omitted	O
since	O
it	O
gives	O
the	O
same	O
value	O
as	O
P@1	B-MetricName
.	O
It	O
is	O
notable	O
that	O
HYPERCAPS	O
obtains	O
competitive	O
results	O
on	O
the	O
four	O
datasets	O
.	O
The	O
encoding	O
-	O
based	O
FASTTEXT	B-MethodName
is	O
generally	O
inferior	O
to	O
the	O
other	O
baselines	O
as	O
it	O
applies	O
the	O
average	B-MethodName
pooling	I-MethodName
on	O
word	O
vector	O
representations	O
,	O
which	O
ig	O
-	O
nores	O
word	O
order	O
for	O
the	O
construction	O
of	O
document	O
representations	O
.	O
The	O
typical	O
MLC	O
method	O
SLEEC	O
takes	O
advantage	O
of	O
label	O
correlations	O
by	O
embedding	O
the	O
label	O
co	O
-	O
occurrence	O
graph	O
.	O
However	O
,	O
SLEEC	O
uses	O
TF	O
-	O
IDF	O
vectors	O
to	O
represent	O
documents	O
,	O
thus	O
word	O
order	O
is	O
also	O
ignored	O
.	O
XML	O
-	O
CNN	O
uses	O
a	O
dynamic	O
pooling	O
technique	O
to	O
aggregate	O
the	O
local	O
contextual	O
features	O
extracted	O
by	O
CNN	O
,	O
while	O
SGM	O
uses	O
attention	O
mechanism	O
to	O
aggregate	O
the	O
global	O
contextual	O
features	O
extracted	O
by	O
LSTM	B-MethodName
.	O
REGGNN	O
is	O
generally	O
superior	O
to	O
both	O
of	O
them	O
as	O
it	O
combines	O
the	O
local	O
and	O
global	O
contextual	O
information	O
dynamically	O
and	O
takes	O
label	O
correlations	O
into	O
consideration	O
using	O
a	O
regularized	O
loss	B-MetricName
.	O
However	O
,	O
the	O
two	O
capsulebased	O
methods	O
NLP	O
-	O
CAP	B-DatasetName
and	O
HYPERCAPS	O
consistently	O
outperform	O
all	O
the	O
other	O
methods	O
owing	O
to	O
dynamic	O
routing	O
,	O
which	O
aggregates	O
the	O
fine	O
-	O
grained	O
capsule	O
features	O
in	O
a	O
label	O
-	O
aware	O
manner	O
.	O
Moreover	O
,	O
NLP	O
-	O
CAP	B-DatasetName
only	O
uses	O
CNN	O
to	O
extract	O
the	O
local	O
contextual	O
information	O
,	O
while	O
HYPER	O
-	O
CAPS	O
benefits	O
from	O
the	O
parallel	O
combination	O
of	O
local	O
and	O
global	O
contextual	O
information	O
.	O
In	O
addi	O
-	O
tion	O
,	O
NLP	O
-	O
CAP	B-DatasetName
applies	O
the	O
non	O
-	O
linear	O
squashing	O
function	O
for	O
capsules	O
in	O
the	O
Euclidean	O
space	O
,	O
while	O
HDR	O
is	O
designed	O
for	O
hyperbolic	O
capsules	O
,	O
which	O
take	O
advantage	O
of	O
the	O
representation	O
capacity	O
of	O
the	O
hyperbolic	O
space	O
.	O
Therefore	O
,	O
HYPERCAPS	O
outperforms	O
NLP	O
-	O
CAP	B-DatasetName
as	O
expected	O
.	O
This	O
result	O
further	O
confirms	O
that	O
the	O
proposed	O
HYPERCAPS	O
with	O
HDR	O
is	O
effective	O
to	O
learn	O
the	O
label	O
-	O
aware	O
hyperbolic	O
capsules	O
for	O
MLC	O
.	O

In	O
MLC	O
,	O
tail	O
labels	O
have	O
low	O
occurring	O
frequency	O
and	O
hence	O
are	O
hard	O
to	O
predict	O
compared	O
to	O
head	O
labels	O
.	O
The	O
performance	O
on	O
tail	O
labels	O
of	O
the	O
four	O
benchmark	O
datasets	O
is	O
evaluated	O
in	O
terms	O
of	O
nDCG@k	O
with	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
,	O
3	O
,	O
5	O
.	O
Figure	O
4	O
shows	O
the	O
results	O
of	O
the	O
five	O
deep	O
learning	O
based	O
MLC	O
methods	O
,	O
i.e.	O
XML	O
-	O
CNN	O
,	O
SGM	O
,	O
REGGNN	O
,	O
NLP	O
-	O
CAP	B-DatasetName
and	O
HYPERCAPS	O
.	O
nDCG@1	O
is	O
smaller	O
than	O
nDCG@3	O
on	O
AAPD	O
,	O
RCV1	B-DatasetName
and	O
ZHIHU	O
since	O
most	O
of	O
their	O
test	O
instances	O
contain	O
less	O
than	O
three	O
tail	O
labels	O
.	O
It	O
is	O
remarkable	O
that	O
HYPERCAPS	O
outperforms	O
all	O
the	O
other	O
methods	O
on	O
tail	O
labels	O
.	O
REGGNN	O
takes	O
advantage	O
of	O
the	O
local	O
and	O
global	O
contextual	O
information	O
and	O
label	O
correlations	O
,	O
thus	O
it	O
outperforms	O
XML	O
-	O
CNN	O
and	O
SGM	O
.	O
The	O
two	O
capsule	O
-	O
based	O
methods	O
NLP	O
-	O
CAP	B-DatasetName
and	O
HYPERCAPS	O
are	O
both	O
superior	O
to	O
the	O
other	O
methods	O
,	O
which	O
indicates	O
that	O
the	O
label	O
-	O
aware	O
dynamic	O
routing	O
is	O
effective	O
for	O
the	O
prediction	O
on	O
tail	O
labels	O
.	O
In	O
addition	O
,	O
the	O
fact	O
that	O
HYPERCAPS	O
significantly	O
improves	O
the	O
prediction	O
performance	O
compared	O
to	O
NLP	O
-	O
CAP	B-DatasetName
implies	O
that	O
the	O
representation	O
capacity	O
of	O
the	O
hyperbolic	O
space	O
and	O
the	O
combination	O
of	O
local	O
and	O
global	O
contextual	O
information	O
are	O
helpful	O
for	O
learning	O
on	O
tail	O
labels	O
.	O
The	O
results	O
demonstrate	O
the	O
superiority	O
of	O
the	O
proposed	O
HYPERCAPS	O
on	O
tail	O
labels	O
for	O
MLC	O
.	O

An	O
ablation	O
test	O
would	O
be	O
informative	O
to	O
analyze	O
the	O
effect	O
of	O
varying	O
different	O
components	O
of	O
the	O
proposed	O
HYPERCAPS	O
,	O
which	O
can	O
be	O
taken	O
apart	O
as	O
local	O
Euclidean	O
capsules	O
only	O
(	O
denoted	O
as	O
L	O
)	O
,	O
global	O
Euclidean	O
capsules	O
only	O
(	O
denoted	O
as	O
G	O
)	O
,	O
a	O
combination	O
of	O
the	O
local	O
and	O
global	O
Euclidean	O
capsules	O
(	O
denoted	O
as	O
L	O
+	O
G	O
)	O
,	O
and	O
a	O
combination	O
of	O
the	O
local	O
and	O
global	O
hyperbolic	O
capsules	O
(	O
denoted	O
as	O
L	O
+	O
G	O
+	O
H	O
)	O
.	O
Euclidean	O
capsules	O
(	O
in	O
L	O
,	O
G	O
and	O
L	O
+	O
G	O
)	O
are	O
aggregated	O
via	O
the	O
origin	O
dynamic	O
routing	O
(	O
Sabour	O
et	O
al	O
,	O
2017	O
)	O
,	O
while	O
hyperbolic	O
capsules	O
(	O
in	O
L	O
+	O
G	O
+	O
H	O
)	O
are	O
aggregated	O
via	O
our	O
HDR	O
.	O
Figure	O
5	O
shows	O
the	O
results	O
on	O
EUR	O
-	O
LEX57	O
K	O
in	O
terms	O
of	O
P@k	O
with	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
,	O
3	O
,	O
5	O
.	O
In	O
order	O
to	O
make	O
the	O
comparison	O
fair	O
,	O
the	O
number	O
of	O
total	O
compressed	O
capsules	O
is	O
equally	O
set	O
to	O
256	O
for	O
all	O
the	O
four	O
models	O
.	O
Adaptive	O
routing	O
is	O
also	O
applied	O
with	O
the	O
maximum	O
candidate	O
label	O
number	O
set	O
equally	O
to	O
200	O
.	O
Generally	O
,	O
the	O
proposed	O
combination	O
of	O
local	O
and	O
global	O
contextual	O
information	O
contributes	O
to	O
the	O
effectiveness	O
of	O
the	O
model	O
(	O
L	O
+	O
G	O
)	O
.	O
Therefore	O
,	O
it	O
is	O
practical	O
to	O
combine	O
the	O
local	O
and	O
global	O
contextual	O
information	O
via	O
dynamic	O
routing	O
.	O
HDR	O
furthermore	O
improves	O
the	O
performance	O
by	O
making	O
use	O
of	O
the	O
representation	O
capacity	O
of	O
the	O
hyperbolic	O
space	O
.	O
Overall	O
,	O
each	O
of	O
the	O
components	O
benefits	O
the	O
performance	O
of	O
HYPERCAPS	O
for	O
MLC	O
.	O
In	O
summary	O
,	O
extensive	O
experiments	O
are	O
carried	O
out	O
on	O
four	O
MLC	O
benchmark	O
datasets	O
with	O
various	O
scales	O
.	O
The	O
results	O
demonstrate	O
that	O
the	O
proposed	O
HYPERCAPS	O
can	O
achieve	O
competitive	O
performance	O
compared	O
with	O
the	O
baselines	O
.	O
In	O
particular	O
,	O
effectiveness	O
of	O
HYPERCAPS	O
is	O
shown	O
on	O
tail	O
labels	O
.	O
The	O
ablation	O
test	O
furthermore	O
confirms	O
that	O
the	O
combination	O
of	O
local	O
and	O
global	O
contextual	O
information	O
is	O
practical	O
and	O
HYPERCAPS	O
benefits	O
from	O
the	O
representation	O
capacity	O
of	O
the	O
hyperbolic	O
space	O
.	O
6	O
Related	O
Work	O

Multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
(	O
MLC	O
)	O
aims	O
at	O
assigning	O
multiple	O
relevant	O
labels	O
to	O
one	O
document	O
.	O
The	O
MLC	O
label	O
set	O
is	O
large	O
compared	O
to	O
Multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
(	O
MCC	O
)	O
.	O
Besides	O
,	O
the	O
correlations	O
of	O
labels	O
(	O
e.g.	O
hierarchical	O
label	O
structures	O
(	O
Banerjee	O
et	O
al	O
,	O
2019	O
)	O
)	O
and	O
the	O
existence	O
of	O
tail	O
labels	O
make	O
MLC	O
a	O
hard	O
task	O
(	O
Bhatia	O
et	O
al	O
,	O
2015	O
)	O
.	O
As	O
data	O
sparsity	O
and	O
scalability	O
issues	O
arise	O
with	O
the	O
large	O
number	O
of	O
labels	O
,	O
XML	O
-	O
CNN	O
(	O
Liu	O
et	O
al	O
,	O
2017	O
)	O
employs	O
CNN	O
as	O
efficient	O
feature	O
extractor	O
,	O
whereas	O
it	O
ignores	O
label	O
correlations	O
,	O
which	O
are	O
often	O
used	O
to	O
deal	O
with	O
tail	O
labels	O
.	O
The	O
traditional	O
MLC	O
method	O
SLEEC	O
(	O
Bhatia	O
et	O
al	O
,	O
2015	O
)	O
makes	O
use	O
of	O
label	O
correlations	O
by	O
embedding	O
the	O
label	O
co	O
-	O
occurrence	O
graph	O
.	O
The	O
seq2seq	B-MethodName
model	O
SGM	O
(	O
Yang	O
et	O
al	O
,	O
2018b	O
)	O
uses	O
the	O
attention	O
mechanism	O
to	O
consider	O
the	O
label	O
correlations	O
,	O
while	O
REGGNN	O
(	O
Xu	O
et	O
al	O
,	O
2019	O
)	O
applies	O
a	O
regularized	O
loss	B-MetricName
specified	O
for	O
label	O
co	O
-	O
occurrence	O
.	O
REGGNN	O
additionally	O
chooses	O
to	O
dynamically	O
combine	O
the	O
local	O
and	O
global	O
contextual	O
information	O
to	O
construct	O
document	O
representations	O
.	O

Capsule	O
networks	O
are	O
recently	O
proposed	O
to	O
address	O
the	O
representation	O
limitations	O
of	O
CNN	O
and	O
RNN	O
.	O
The	O
concept	O
of	O
capsule	O
is	O
first	O
introduced	O
by	O
(	O
Hinton	O
et	O
al	O
,	O
2011	O
)	O
.	O
(	O
Sabour	O
et	O
al	O
,	O
2017	O
)	O
replaces	O
the	O
scalar	O
output	O
features	O
of	O
CNN	O
with	O
vector	O
capsules	O
and	O
pooling	O
with	O
dynamic	O
routing	O
.	O
(	O
Hinton	O
et	O
al	O
,	O
2018	O
)	O
proposes	O
the	O
EM	B-MetricName
algorithm	O
based	O
routing	O
procedure	O
between	O
capsule	O
layers	O
.	O
(	O
Gong	O
et	O
al	O
,	O
2018	O
)	O
proposes	O
to	O
regard	O
dynamic	O
routing	O
as	O
an	O
information	O
aggregation	O
procedure	O
,	O
which	O
is	O
more	O
effective	O
than	O
pooling	O
.	O
(	O
Yang	O
et	O
al	O
,	O
2018a	O
)	O
and	O
(	O
Du	O
et	O
al	O
,	O
2019a	O
)	O
investigate	O
capsule	O
networks	O
for	O
text	B-TaskName
classification	I-TaskName
.	O
(	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
then	O
presents	O
a	O
capsule	O
compression	O
method	O
and	O
reformulates	O
the	O
routing	O
procedure	O
to	O
fit	O
for	O
MLC	O
.	O
Our	O
work	O
is	O
different	O
from	O
the	O
predecessors	O
as	O
we	O
design	O
the	O
Hyperbolic	O
Dynamic	O
Routing	O
(	O
HDR	O
)	O
to	O
aggregate	O
the	O
parallel	O
combination	O
of	O
local	O
and	O
global	O
contextual	O
information	O
in	O
form	O
of	O
hyperbolic	O
capsules	O
,	O
which	O
are	O
constrained	O
in	O
the	O
hyperbolic	O
space	O
without	O
the	O
requirement	O
of	O
non	O
-	O
linear	O
squashing	O
function	O
.	O
In	O
addition	O
,	O
adaptive	O
routing	O
is	O
proposed	O
to	O
improve	O
the	O
scalability	O
for	O
large	O
number	O
of	O
labels	O
.	O

Before	O
going	O
to	O
the	O
details	O
of	O
our	O
method	O
,	O
we	O
provide	O
some	O
background	O
on	O
the	O
RL	O
-	O
based	O
dialog	O
manager	O
in	O
this	O
section	O
.	O
Fig	O
.	O
2	O
shows	O
an	O
overview	O
of	O
such	O
dialog	O
manager	O
.	O
We	O
describe	O
each	O
of	O
the	O
parts	O
briefly	O
below	O
.	O
Feature	O
Extraction	O
At	O
the	O
t	O
-	O
th	O
turn	O
of	O
a	O
dialog	O
,	O
the	O
user	O
input	O
u	O
t	O
is	O
parsed	O
into	O
domain	O
specific	O
intents	O
and	O
slots	O
to	O
form	O
a	O
semantic	O
frame	O
a	O
u	O
t	O
by	O
a	O
language	O
understanding	O
(	O
LU	O
)	O
module	O
.	O
o	O
u	O
t	O
and	O
o	O
s	O
t−1	O
are	O
the	O
one	O
-	O
hot	O
representations	O
of	O
such	O
semantic	O
frames	O
for	O
the	O
current	O
user	O
input	O
and	O
the	O
last	O
system	O
output	O
respectively	O
.	O
Alternatively	O
,	O
o	O
u	O
t	O
can	O
be	O
a	O
simple	O
n	O
-	O
grams	O
representation	O
of	O
u	O
t	O
.	O
But	O
the	O
vocabulary	O
size	O
is	O
relatively	O
large	O
in	O
real	O
-	O
world	O
applications	O
.	O
It	O
will	O
yield	O
slow	O
convergence	O
in	O
the	O
absence	O
of	O
a	O
LU	O
module	O
.	O
Based	O
on	O
the	O
slot	O
-	O
value	O
pair	O
output	O
with	O
the	O
highest	O
probability	O
,	O
a	O
query	O
is	O
sent	O
to	O
a	O
database	O
to	O
retrieve	O
user	O
requested	O
information	O
.	O
o	O
db	O
t	O
is	O
the	O
one	O
-	O
hot	O
representation	O
of	O
the	O
database	O
result	O
.	O
As	O
a	O
result	O
,	O
the	O
observable	O
information	O
x	O
t	O
is	O
the	O
concatenation	O
of	O
o	O
u	O
t	O
,	O
o	O
s	O
t−1	O
and	O
o	O
db	O
t	O
.	O
State	O
Representation	O
Based	O
on	O
the	O
extracted	O
feature	O
vector	O
x	O
t	O
and	O
previous	O
internal	O
state	O
s	O
t−1	O
,	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
are	O
used	O
to	O
infer	O
the	O
latent	O
representation	O
of	O
dialog	O
state	O
s	O
t	O
at	O
step	O
t.	O
Current	O
state	O
s	O
t	O
can	O
be	O
interpreted	O
as	O
the	O
summary	O
of	O
dialog	O
history	O
h	O
t	O
up	O
to	O
current	O
step	O
.	O
Dialog	O
Policy	O
Next	O
,	O
the	O
dialog	O
state	O
representation	O
s	O
t	O
is	O
fed	O
into	O
a	O
policy	O
network	O
.	O
The	O
output	O
π	O
(	O
a	O
|	O
h	O
t	O
;	O
θ	B-HyperparameterName
)	O
of	O
the	O
policy	O
network	O
is	O
a	O
probability	O
distribution	O
over	O
a	O
predefined	O
system	O
action	O
set	O
A	O
s	O
.	O
Lastly	O
,	O
the	O
system	O
samples	O
an	O
action	O
a	O
s	O
t	O
A	O
s	O
based	O
on	O
π	O
(	O
a	O
|	O
h	O
t	O
;	O
θ	B-HyperparameterName
)	O
and	O
receives	O
a	O
new	O
observation	O
x	O
t+1	O
with	O
an	O
assigned	O
reward	O
r	O
t	O
.	O
The	O
policy	O
parameters	O
θ	B-HyperparameterName
can	O
be	O
learned	O
by	O
maximizing	O
the	O
expected	O
discounted	O
cumulative	O
rewards	O
:	O
J	O
(	O
θ	B-HyperparameterName
)	O
=	O
E	O
T	O
−t	O
k=0	O
γ	B-HyperparameterName
k	O
r	O
t+k	O
(	O
1	O
)	O
where	O
T	O
is	O
the	O
maximal	O
step	O
,	O
and	O
γ	B-HyperparameterName
is	O
the	O
discount	O
factor	O
.	O
Usually	O
the	O
parameters	O
θ	B-HyperparameterName
can	O
be	O
iteratively	O
updated	O
by	O
policy	O
gradient	O
(	O
Williams	O
,	O
1992	O
)	O
approach	O
.	O
The	O
policy	O
gradient	O
can	O
be	O
empirically	O
estimated	O
as	O
:	O
∇	O
θ	B-HyperparameterName
J	O
(	O
θ	B-HyperparameterName
)	O
≈	O
1	O
N	O
N	O
i=1	O
T	O
t=1	O
∇	O
θ	B-HyperparameterName
log	O
π	O
(	O
a	O
s	O
i	O
,	O
t	O
|	O
hi	O
,	O
t	O
;	O
θ	B-HyperparameterName
)	O
(	O
Gi	O
,	O
t	O
−b	O
)	O
(	O
2	O
)	O
where	O
N	O
is	O
the	O
number	O
of	O
sampled	O
episodes	O
in	O
a	O
batch	O
,	O
G	O
i	O
,	O
t	O
=	O
T	O
−t	O
k=0	O
γ	B-HyperparameterName
k	O
r	O
i	O
,	O
t+k	O
is	O
the	O
sum	O
of	O
discounted	O
reward	O
at	O
step	O
t	O
in	O
the	O
episode	O
i	O
,	O
and	O
b	O
is	O
a	O
baseline	O
to	O
estimate	O
the	O
average	O
reward	O
of	O
current	O
policy	O
.	O

Although	O
the	O
ontology	B-MethodName
of	O
the	O
new	O
system	O
is	O
different	O
from	O
the	O
original	O
one	O
,	O
the	O
extended	O
dialog	O
manager	O
can	O
still	O
reuse	O
dialog	O
policy	O
of	O
the	O
illconsidered	O
system	O
circuitously	O
.	O
Given	O
user	O
logs	O
D	O
and	O
the	O
original	O
dialog	O
manager	O
π	O
(	O
θ	B-HyperparameterName
)	O
,	O
we	O
define	O
a	O
loss	B-MetricName
L	O
(	O
θ	B-HyperparameterName
;	O
D	O
,	O
θ	B-HyperparameterName
)	O
to	O
minimize	O
the	O
difference	O
between	O
new	O
dialog	O
manager	O
π	O
(	O
θ	B-HyperparameterName
)	O
and	O
the	O
old	O
one	O
:	O
L	O
(	O
θ	B-HyperparameterName
;	O
D	O
,	O
θ	B-HyperparameterName
)	O
=	O
d	O
D	O
|	O
d	O
|	O
t=1	O
KL	O
(	O
π	O
(	O
a	O
|	O
ht	O
;	O
θ	B-HyperparameterName
)	O
|	O
|	O
π	O
(	O
a	O
|	O
ht	O
;	O
θ	B-HyperparameterName
)	O
)	O
(	O
3	O
)	O
where	O
π	O
(	O
a	O
|	O
h	O
t	O
;	O
θ	B-HyperparameterName
)	O
and	O
π	O
(	O
a	O
|	O
h	O
t	O
;	O
θ	B-HyperparameterName
)	O
are	O
the	O
policy	O
distributions	O
over	O
A	O
s	O
and	O
A	O
s	O
given	O
the	O
same	O
dialog	O
history	O
h	O
t	O
.	O
|	O
d	O
|	O
means	O
turns	O
of	O
a	O
specific	O
dialog	O
d	O
D.	O
To	O
deal	O
with	O
unsupported	O
user	O
actions	O
,	O
A	O
s	O
will	O
be	O
a	O
subset	O
of	O
A	O
s	O
.	O
As	O
a	O
result	O
,	O
the	O
KL	O
term	O
in	O
equation	O
(	O
3	O
)	O
can	O
be	O
defined	O
as	O
follows	O
:	O
KL	O
(	O
π	O
(	O
a	O
|	O
ht	O
;	O
θ	B-HyperparameterName
)	O
|	O
|	O
π	O
(	O
a	O
|	O
ht	O
;	O
θ	B-HyperparameterName
)	O
)	O
=	O
|	O
As	O
|	O
k=1	O
π	O
(	O
a	O
k	O
|	O
ht	O
;	O
θ	B-HyperparameterName
)	O
logπ	O
(	O
a	O
k	O
|	O
ht	O
;	O
θ	B-HyperparameterName
)	O
−	O
logπ	O
(	O
a	O
k	O
|	O
ht	O
;	O
θ	B-HyperparameterName
)	O
(	O
4	O
)	O
As	O
the	O
original	O
policy	O
parameters	O
θ	B-HyperparameterName
are	O
fixed	O
,	O
the	O
loss	B-MetricName
function	O
in	O
equation	O
(	O
3	O
)	O
can	O
be	O
rewritten	O
as	O
:	O
L	O
(	O
θ	B-HyperparameterName
;	O
D	O
,	O
θ	B-HyperparameterName
)	O
=	O
−	O
d	O
D	O
|	O
d	O
|	O
t=1	O
|	O
As	O
|	O
k=1	O
π	O
(	O
a	O
k	O
|	O
ht	O
;	O
θ	B-HyperparameterName
)	O
logπ	O
(	O
a	O
k	O
|	O
ht	O
;	O
θ	B-HyperparameterName
)	O
(	O
5	O
)	O
This	O
objective	O
will	O
transfer	O
knowledge	O
of	O
the	O
original	O
system	O
to	O
the	O
"	O
student	O
"	O
at	O
the	O
turn	O
level	O
.	O
Under	O
the	O
guidance	O
of	O
the	O
original	O
system	O
,	O
the	O
extended	O
system	O
will	O
be	O
equipped	O
with	O
the	O
primary	O
strategy	O
to	O
complete	O
a	O
task	O
.	O

It	O
's	O
easy	O
for	O
the	O
developers	O
to	O
give	O
logic	O
rules	O
on	O
the	O
system	O
responses	O
to	O
handle	O
new	O
user	O
actions	O
.	O
For	O
example	O
,	O
if	O
users	O
ask	O
to	O
confirm	O
a	O
slot	O
,	O
the	O
system	O
should	O
inform	O
the	O
value	O
of	O
that	O
slot	O
immediately	O
.	O
Note	O
that	O
these	O
system	O
actions	O
which	O
handle	O
new	O
user	O
actions	O
may	O
not	O
exist	O
in	O
the	O
old	O
model	O
.	O
It	O
means	O
the	O
architecture	O
of	O
the	O
new	O
system	O
is	O
different	O
from	O
the	O
old	O
one	O
.	O
We	O
define	O
a	O
set	O
of	O
logic	O
constraints	O
R	O
=	O
{	O
(	O
h	O
l	O
,	O
a	O
l	O
)	O
}	O
L	O
l=1	O
,	O
where	O
h	O
l	O
H	O
R	O
indicates	O
the	O
dialog	O
context	O
condition	O
in	O
the	O
l	O
-	O
th	O
rule	O
,	O
and	O
a	O
l	O
A	O
s	O
is	O
the	O
corresponding	O
system	O
action	O
.	O
The	O
number	O
of	O
logic	O
rules	O
L	O
is	O
equal	O
to	O
the	O
number	O
of	O
new	O
user	O
actions	O
.	O
These	O
rules	O
can	O
be	O
seen	O
as	O
triggers	O
:	O
if	O
dialog	O
context	O
h	O
t	O
in	O
current	O
turn	O
t	O
meets	O
the	O
context	O
condition	O
h	O
l	O
defined	O
in	O
logic	O
rules	O
,	O
then	O
the	O
system	O
should	O
execute	O
a	O
l	O
.	O
In	O
our	O
work	O
,	O
we	O
use	O
the	O
output	O
of	O
the	O
LU	O
module	O
to	O
judge	O
whether	O
the	O
current	O
dialog	O
context	O
meets	O
the	O
condition	O
defined	O
by	O
logic	O
rules	O
.	O
An	O
alternative	O
method	O
is	O
simple	O
rules	O
matching	O
.	O
To	O
distill	O
the	O
knowledge	O
of	O
rules	O
to	O
a	O
new	O
system	O
,	O
we	O
define	O
a	O
loss	B-MetricName
function	O
L	O
(	O
θ	B-HyperparameterName
;	O
D	O
,	O
R	O
)	O
to	O
embed	O
such	O
constraints	O
in	O
the	O
new	O
system	O
:	O
L	O
(	O
θ	B-HyperparameterName
;	O
D	O
,	O
R	O
)	O
=	O
−	O
d	O
D	O
|	O
d	O
|	O
t=1	O
h	O
l	O
H	O
R	O
1	O
{	O
ht	O
=	O
h	O
l	O
}	O
×	O
|	O
A	O
s	O
|	O
k=1	O
1	O
{	O
a	O
k	B-HyperparameterName
=	I-HyperparameterName
a	O
l	O
}	O
log	O
π	O
(	O
a	O
k	O
|	O
ht	O
;	O
θ	B-HyperparameterName
)	O
(	O
6	O
)	O
Where	O
1	O
{	O
}	O
is	O
the	O
indicate	O
function	O
.	O
Equation	O
(	O
6	O
)	O
suggests	O
the	O
new	O
dialog	O
manager	O
π	O
(	O
θ	B-HyperparameterName
)	O
will	O
be	O
penalized	O
if	O
it	O
violates	O
the	O
instructions	O
defined	O
by	O
the	O
dialog	O
rules	O
.	O
Note	O
that	O
,	O
for	O
simplicity	O
,	O
we	O
assume	O
these	O
rules	O
are	O
absolutely	O
correct	O
and	O
mutually	O
exclusive	O
.	O
Although	O
this	O
hypothesis	O
may	O
lead	O
to	O
a	O
non	O
-	O
optimal	O
dialog	O
system	O
,	O
these	O
rules	O
define	O
reasonable	O
system	O
actions	O
to	O
corresponding	O
dialog	O
contexts	O
.	O
It	O
implies	O
that	O
the	O
new	O
system	O
can	O
be	O
further	O
refined	O
by	O
reinforcement	O
learning	O
once	O
a	O
new	O
interaction	O
environment	O
is	O
available	O
.	O

In	O
the	O
absence	O
of	O
a	O
new	O
training	O
environment	O
,	O
learning	O
is	O
made	O
possible	O
by	O
exploiting	O
structure	O
that	O
holds	O
in	O
the	O
new	O
dialog	O
manager	O
.	O
On	O
one	O
hand	O
,	O
we	O
expect	O
the	O
new	O
system	O
can	O
complete	O
tasks	O
like	O
the	O
original	O
one	O
.	O
On	O
the	O
other	O
hand	O
,	O
it	O
should	O
satisfy	O
the	O
constraints	O
defined	O
by	O
dialog	O
rules	O
.	O
So	O
,	O
the	O
learning	O
objective	O
of	O
new	O
dialog	O
manager	O
π	O
(	O
θ	B-HyperparameterName
)	O
can	O
be	O
defined	O
as	O
follows	O
:	O
L	O
(	O
θ	B-HyperparameterName
;	O
D	O
,	O
θ	B-HyperparameterName
,	O
R	O
)	O
=	O
L	O
(	O
θ	B-HyperparameterName
;	O
D	O
,	O
R	O
)	O
if	O
ht	O
HR	O
;	O
L	O
(	O
θ	B-HyperparameterName
;	O
D	O
,	O
θ	B-HyperparameterName
)	O
else	O
(	O
7	O
)	O
When	O
the	O
dialog	O
context	O
h	O
t	O
in	O
the	O
t	O
-	O
th	O
turn	O
satisfies	O
a	O
condition	O
defined	O
in	O
H	O
R	O
,	O
we	O
distill	O
knowledge	O
of	O
rules	O
into	O
the	O
new	O
system	O
.	O
Otherwise	O
,	O
we	O
distill	O
knowledge	O
of	O
the	O
original	O
system	O
into	O
the	O
new	O
one	O
.	O
Instead	O
of	O
retraining	O
from	O
scratch	O
,	O
developers	O
can	O
extend	O
RL	O
-	O
based	O
systems	O
by	O
reusing	O
existing	O
resources	O
.	O

For	O
the	O
original	O
RL	O
-	O
based	O
dialog	O
system	O
,	O
a	O
feature	O
vector	O
x	O
t	O
of	O
size	O
191	O
is	O
extracted	O
.	O
This	O
vector	O
is	O
the	O
concatenation	O
of	O
encodings	O
of	O
LU	O
results	O
,	O
the	O
previous	O
system	O
reply	O
,	O
database	O
results	O
and	O
the	O
current	O
turn	O
number	O
.	O
The	O
LU	O
module	O
is	O
implemented	O
with	O
an	O
SVM	B-MethodName
5	O
for	O
intent	B-TaskName
detection	I-TaskName
and	O
a	O
CRF	B-MethodName
6	O
for	O
slot	B-TaskName
filling	I-TaskName
.	O
The	O
language	O
generation	O
module	O
is	O
implemented	O
by	O
a	O
rule	O
-	O
based	O
method	O
.	O
The	O
hidden	O
dialog	O
state	O
representation	O
is	O
inferred	O
by	O
a	O
GRU	B-MethodName
(	O
Chung	O
et	O
al	O
,	O
2014	O
)	O
.	O
We	O
set	O
the	O
hidden	O
states	O
of	O
the	O
GRU	B-MethodName
to	O
be	O
120	O
.	O
The	O
policy	O
network	O
is	O
implemented	O
as	O
a	O
Multilayer	O
Perceptron	O
(	O
MLP	B-DatasetName
)	O
with	O
one	O
hidden	O
layer	O
.	O
The	O
size	O
of	O
the	O
hidden	O
layer	O
is	O
80	O
.	O
The	O
output	O
dimension	O
of	O
policy	O
network	O
is	O
15	O
,	O
which	O
corresponds	O
to	O
the	O
number	O
of	O
system	O
actions	O
.	O
To	O
encourage	O
shorter	O
interaction	O
,	O
we	O
set	O
a	O
small	O
per	O
-	O
turn	O
negative	O
reward	O
R	O
turn	O
=	O
−1	O
.	O
The	O
maximal	O
turn	O
is	O
set	O
to	O
be	O
40	O
.	O
If	O
the	O
user	O
goal	O
is	O
satisfied	O
,	O
the	O
policy	O
will	O
be	O
encouraged	O
by	O
a	O
large	O
positive	O
reward	O
R	O
succ	O
=	O
10	O
;	O
otherwise	O
the	O
policy	O
will	O
be	O
penalized	O
by	O
a	O
negative	O
reward	O
R	O
f	O
ail	O
=	O
−10	O
.	O
Discounted	O
factor	O
γ	B-HyperparameterName
=	O
0.9	O
.	O
The	O
baseline	O
b	O
of	O
current	O
policy	O
is	O
estimated	O
on	O
sampled	O
episodes	O
in	O
a	O
batch	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
N	O
is	O
set	O
to	O
be	O
32	O
.	O
Adadelta	B-MethodName
(	O
Zeiler	O
,	O
2012	O
)	O
method	O
is	O
used	O
to	O
update	O
model	O
parameters	O
.	O
The	O
original	O
system	O
S	O
1	O
is	O
trained	O
by	O
interacting	O
with	O
Sim1	O
.	O
After	O
about	O
2400	O
interactions	O
,	O
the	O
performance	O
of	O
S	O
1	O
starts	O
to	O
converge	O
.	O

We	O
also	O
provide	O
a	O
count	O
for	O
the	O
number	O
of	O
items	O
in	O
which	O
(	O
within	O
tables	O
in	O
Appendix	O
Train	O
a	O
model	O
on	O
the	O
pseudo	O
-	O
parallel	O
data	O
;	O
5	O
Fine	O
-	O
tune	O
the	O
model	O
on	O
L	O
;	O
6	O
until	O
convergence	O
or	O
maximum	O
iteration	O
;	O
7	O
Reranking	O
with	O
reverse	O
models	O
In	O
the	O
syntax	O
-	O
semantics	O
interface	O
,	O
the	O
parsing	O
task	O
is	O
usually	O
to	O
build	O
a	O
correct	O
semantic	O
(	O
or	O
syntactic	O
)	O
representation	O
of	O
a	O
sentence	O
.	O
One	O
can	O
consider	O
this	O
task	O
with	O
respect	O
to	O
neural	O
networks	O
-	O
which	O
operate	O
on	O
sequences	O
-	O
straightforwardly	O
by	O
reversing	O
the	O
order	O
of	O
the	O
parallel	O
data	O
:	O
the	O
source	O
sequence	O
(	O
meaning	O
)	O
becomes	O
the	O
target	O
,	O
and	O
the	O
target	O
sequence	O
(	O
text	O
)	O
becomes	O
the	O
source	O
.	O
Following	O
the	O
terminology	O
of	O
Li	O
and	O
White	O
(	O
2020	O
)	O
,	O
we	O
call	O
such	O
models	O
reverse	O
models	O
,	O
while	O
models	O
that	O
generate	O
text	O
from	O
meaning	O
representations	O
are	O
forward	O
models	O
.	O
3	O
We	O
can	O
rerank	O
the	O
output	O
of	O
a	O
forward	O
model	O
with	O
the	O
help	O
of	O
its	O
corresponding	O
reverse	O
model	O
.	O
Given	O
several	O
outputs	O
of	O
a	O
beam	O
search	O
of	O
the	O
forward	O
model	O
,	O
we	O
select	O
the	O
one	O
that	O
makes	O
the	O
best	O
meaning	O
representation	O
if	O
it	O
is	O
given	O
to	O
a	O
reverse	O
model	O
as	O
an	O
input	O
.	O
Here	O
,	O
best	O
means	O
the	O
one	O
that	O
has	O
lowest	O
perplexity	B-MetricName
with	O
respect	O
to	O
forced	O
decoding	O
.	O
One	O
can	O
combine	O
self	O
-	O
training	O
and	O
reranking	O
:	O
Train	O
forward	O
and	O
reverse	O
models	O
on	O
the	O
parallel	O
data	O
and	O
then	O
train	O
forward	O
and	O
reverse	O
models	O
on	O
the	O
pseudo	O
-	O
parallel	O
data	O
.	O
Afterwards	O
finetune	O
them	O
again	O
on	O
the	O
initial	O
parallel	O
data	O
.	O
Subsequently	O
,	O
use	O
the	O
reverse	O
models	O
to	O
rerank	O
the	O
output	O
of	O
the	O
forward	O
models	O
.	O
Train	O
forward	O
and	O
reverse	O
models	O
on	O
the	O
pseudo	O
-	O
parallel	O
data	O
;	O
5	O
Fine	O
-	O
tune	O
both	O
models	O
on	O
L	O
;	O
6	O
until	O
convergence	O
or	O
maximum	O
iteration	O
;	O

On	O
the	O
challenge	O
test	O
,	O
no	O
model	O
achieved	O
perfect	O
accuracy	B-MetricName
.	O
The	O
best	O
performances	O
are	O
by	O
RST	O
-	O
SM	O
and	O
RST	O
-	O
LG	O
.	O
Their	O
performance	O
is	O
similar	O
.	O
It	O
is	O
worth	O
noting	O
that	O
in	O
the	O
case	O
of	O
FACT	O
-	O
SM	O
,	O
reranking	O
with	O
self	O
-	O
training	O
gave	O
results	O
comparable	O
to	O
RST	O
-	O
SM	O
(	O
there	O
is	O
no	O
significance	O
difference	O
in	O
terms	O
of	O
Fisher	O
's	O
test	O
with	O
significance	O
at	O
5	O
%	O
)	O
.	O
This	O
is	O
not	O
the	O
case	O
for	O
FACT	O
-	O
LG	O
and	O
RST	O
-	O
LG	O
models	O
.	O
RST	O
-	O
LG	O
-	O
ST	O
-	O
RMR	O
outperforms	O
the	O
best	O
FACT	O
-	O
LG	O
model	O
(	O
see	O
Fig	O
.	O
5	O
)	O
.	O
From	O
these	O
experiments	O
,	O
we	O
see	O
that	O
on	O
the	O
standard	O
test	O
set	O
,	O
RST	O
-	O
Large	O
and	O
RST	O
-	O
Small	O
models	O
performed	O
best	O
in	O
terms	O
of	O
producing	O
the	O
correct	O
discourse	O
connective	O
for	O
SIMILARITY	O
(	O
respectively	O
CONTRAST	O
)	O
.	O
While	O
errors	O
occurred	O
-	O
sometimes	O
matching	O
the	O
results	O
of	O
the	O
corresponding	O
FACT	O
models	O
-	O
RST	O
models	O
correctly	O
distinguish	O
between	O
producing	O
the	O
lexeme	O
for	O
SIMILARITY	O
versus	O
CONTRAST	O
,	O
while	O
FACT	O
models	O
sometimes	O
confuse	O
SIMILARITY	O
with	O
CONTRAST	O
.	O
On	O
the	O
challenge	O
data	O
every	O
model	O
made	O
errors	O
.	O
The	O
RST	O
models	O
outperformed	O
the	O
corresponding	O
FACT	O
models	O
,	O
significantly	O
in	O
the	O
case	O
of	O
RST	O
-	O
LG	O
over	O
RST	O
-	O
LG	O
,	O
as	O
seen	O
in	O
Fig	O
.	O
5	O
.	O
Though	O
the	O
RST	O
models	O
yielded	O
less	O
dramatic	O
improvements	O
on	O
comparisons	O
in	O
the	O
challenge	O
set	O
,	O
it	O
is	O
worth	O
emphasizing	O
that	O
the	O
RST	O
models	O
produce	O
significantly	O
fewer	O
repetitions	O
,	O
omissions	O
and	O
hallucination	O
compared	O
to	O
the	O
FACT	O
models	O
(	O
Figs	O
.	O
6	O
and	O
7	O
,	O
Appendix	O
C	O
)	O
,	O
further	O
supporting	O
the	O
conclusion	O
that	O
the	O
RST	O
input	O
produces	O
better	O
output	O
.	O
This	O
result	O
is	O
interesting	O
,	O
since	O
the	O
content	O
plans	O
in	O
the	O
FACT	O
models	O
are	O
shorter	O
than	O
those	O
in	O
RST	O
models	O
,	O
yet	O
still	O
prompt	O
the	O
former	O
models	O
to	O
produce	O
more	O
words	O
than	O
RST	O
models	O
do	O
.	O

This	O
paper	O
proposes	O
an	O
improved	O
custom	O
model	O
for	O
WNUT	O
task	O
2	O
:	O
Identification	O
of	O
Informative	O
COVID	O
-	O
19	O
English	O
Tweet	O
.	O
We	O
improve	O
experiment	O
with	O
the	O
effectiveness	O
of	O
fine	O
-	O
tuning	O
methodologies	O
for	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
language	O
model	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
make	O
a	O
preliminary	O
instantiation	O
of	O
this	O
formal	O
model	O
for	O
the	O
text	B-TaskName
classification	I-TaskName
approaches	O
.	O
With	O
appropriate	O
training	O
techniques	O
,	O
our	O
model	O
is	O
able	O
to	O
achieve	O
0.9218	O
F1score	O
on	O
public	O
validation	O
set	O
and	O
the	O
ensemble	O
version	O
settles	O
at	O
top	O
9	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
(	O
0.9005	O
)	O
and	O
top	O
2	O
Recall	B-MetricName
(	O
0.9301	O
)	O
on	O
private	O
test	O
set	O
.	O

Since	O
the	O
outbreak	O
of	O
COVID	O
-	O
19	O
pandemic	O
,	O
frequently	O
updated	O
information	O
becomes	O
a	O
huge	O
problem	O
of	O
concern	O
.	O
Social	O
media	O
platforms	O
consequently	O
become	O
real	O
-	O
time	O
sources	O
for	O
news	O
about	O
flare	O
-	O
up	O
data	O
.	O
In	O
any	O
case	O
,	O
the	O
flare	O
-	O
up	O
has	O
been	O
spreading	O
quickly	O
,	O
we	O
observe	O
a	O
monstrous	O
amount	O
of	O
information	O
on	O
social	O
networks	O
,	O
for	O
example	O
around	O
4	O
million	O
COVID	O
-	O
19	O
English	O
Tweets	O
every	O
day	O
on	O
Twitter	O
,	O
in	O
which	O
most	O
of	O
these	O
Tweets	O
are	O
uninformative	O
.	O
Therefore	O
,	O
it	O
is	O
crucial	O
to	O
collect	O
the	O
informative	O
ones	O
(	O
for	O
example	O
Corona	O
Virus	O
Tweets	O
identified	O
with	O
new	O
cases	O
or	O
dubious	O
cases	O
)	O
for	O
downstream	O
applications	O
.	O
In	O
any	O
case	O
,	O
manual	O
ways	O
to	O
deal	O
with	O
recognizing	O
useful	O
Tweets	O
require	O
critical	O
human	O
endeavors	O
,	O
and	O
hence	O
are	O
expensive	O
.	O
Based	O
on	O
the	O
dataset	O
provided	O
in	O
WNUT	O
-	O
2020	O
Task	O
2	O
:	O
Identification	O
of	O
informative	O
COVID	O
-	O
19	O
English	O
Tweets	O
(	O
Nguyen	O
et	O
al	O
,	O
2020	O
)	O
,	O
we	O
propose	O
a	O
fine	O
-	O
tuning	O
strategy	O
to	O
adopt	O
the	O
universal	O
language	O
model	O
RoBERTa	B-MethodName
as	O
an	O
backbone	O
model	O
for	O
text	B-TaskName
classification	I-TaskName
purposes	O
.	O
We	O
also	O
conduct	O
several	O
experiments	O
in	O
varied	O
fine	O
-	O
tuning	O
architectures	O
on	O
the	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
.	O
Our	O
best	O
model	O
results	O
in	O
a	O
high	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
0.9005	O
on	O
the	O
task	O
's	O
private	O
test	O
dataset	O
and	O
that	O
of	O
0.9218	O
on	O
the	O
public	O
validation	O
set	O
with	O
Multilayer	O
Perceptron	O
Head	O
.	O

One	O
of	O
the	O
most	O
important	O
parts	O
in	O
text	B-TaskName
classification	I-TaskName
problems	O
is	O
input	O
representation	O
.	O
Traditional	O
methods	O
construct	O
context	O
-	O
independent	O
embeddings	O
for	O
words	O
.	O
Mikolov	O
et	O
al	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
introduce	O
an	O
open	O
-	O
source	O
Word2Vec	O
,	O
which	O
consists	O
of	O
two	O
models	O
:	O
Continuous	O
Bag	O
of	O
Words	O
(	O
CBOW	O
)	O
and	O
Skip	O
-	O
gram	O
model	O
.	O
The	O
models	O
were	O
trained	O
on	O
1.6	O
billion	O
words	O
to	O
learn	O
linguistic	O
contexts	O
of	O
words	O
.	O
While	O
Word2Vec	O
is	O
a	O
selfsupervised	O
algorithm	O
,	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
is	O
trained	O
unsupervised	O
to	O
form	O
word	B-TaskName
embeddings	I-TaskName
.	O
GloVe	B-MethodName
factorizes	O
co	O
-	O
occurrence	O
matrix	O
of	O
words	O
,	O
resulting	O
in	O
dense	O
word	O
vectors	O
.	O
However	O
,	O
both	O
GloVe	B-MethodName
and	O
Word2Vec	O
fail	O
representing	O
rare	O
or	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
.	O
FastText	B-MethodName
(	O
Mikolov	O
et	O
al	O
,	O
2018	O
)	O
mitigates	O
this	O
problem	O
by	O
decomposing	O
words	O
as	O
a	O
sum	O
of	O
character	O
n	O
-	O
grams	O
.	O
This	O
handles	O
unseen	O
words	O
very	O
well	O
because	O
these	O
character	O
n	O
-	O
grams	O
may	O
still	O
occur	O
in	O
other	O
words	O
.	O
In	O
contrast	O
to	O
context	O
-	O
independent	O
embeddings	O
,	O
modern	O
language	O
models	O
encode	O
word	O
semantics	O
within	O
contexts	O
.	O
Word	O
vectors	O
obtained	O
from	O
these	O
methods	O
achieve	O
better	O
results	O
on	O
downstream	O
tasks	O
because	O
a	O
word	O
in	O
different	O
contexts	O
expresses	O
different	O
meanings	O
.	O
Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
,	O
or	O
BERT	B-MethodName
for	O
short	O
,	O
outperforms	O
the	O
previous	O
best	O
result	O
with	O
GLUE	B-DatasetName
score	O
of	O
80.4	O
%	O
,	O
which	O
is	O
7.6	O
%	O
improvement	O
.	O
There	O
are	O
two	O
variants	O
of	O
BERT	B-MethodName
:	O
base	O
and	O
large	O
;	O
the	O
large	O
model	O
is	O
a	O
stack	O
of	O
24	O
Transformers	O
'	O
encoders	O
for	O
a	O
total	O
of	O
340	O
M	O
parameters	O
while	O
the	O
base	O
one	O
has	O
only	O
12	O
encoders	O
.	O
GPT	B-MethodName
-	O
2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
by	O
OpenAI	O
is	O
a	O
gigantic	O
model	O
with	O
1.5	O
billion	O
parameters	O
and	O
48	O
layers	O
,	O
setting	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
7	O
out	O
of	O
8	O
datasets	O
.	O
Face	O
-	O
book	O
Research	O
team	O
improves	O
training	O
procedures	O
for	O
BERT	B-MethodName
,	O
introducing	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
improvements	O
include	O
extended	O
training	O
time	O
on	O
a	O
ten	O
-	O
times	O
bigger	O
dataset	O
,	O
increased	O
batch	B-HyperparameterName
size	I-HyperparameterName
,	O
using	O
byte	O
-	O
level	O
encoding	O
with	O
larger	O
vocabulary	O
,	O
excluding	O
next	O
sentence	O
predicting	O
task	O
,	O
and	O
dynamic	O
masking	O
pattern	O
modifying	O
.	O

Figure	O
1	O
illustrates	O
our	O
process	O
.	O
For	O
MLM	B-DatasetName
tuning	O
we	O
propose	O
hierarchical	O
tuning	O
process	O
that	O
consists	O
of	O
two	O
steps	O
:	O
Domain	B-TaskName
adaptation	I-TaskName
using	O
extra	O
COVID	O
data	O
and	O
Task	O
adaptation	O
using	O
the	O
given	O
training	O
data	O
.	O
After	O
MLM	B-DatasetName
Tuning	O
,	O
we	O
utilize	O
different	O
training	O
techniques	O
for	O
text	B-TaskName
classification	I-TaskName
such	O
as	O
back	O
translation	O
,	O
warm	O
-	O
up	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
layer	O
freezing	O
and	O
layer	O
-	O
wise	O
learning	O
rates	O
.	O
This	O
section	O
provides	O
details	O
of	O
this	O
pipeline	O
.	O

RoBERTa	B-MethodName
apparently	O
is	O
an	O
excellent	O
language	O
model	O
since	O
it	O
was	O
trained	O
on	O
a	O
huge	O
dataset	O
in	O
a	O
broad	O
domain	O
.	O
However	O
,	O
the	O
general	O
domain	O
is	O
also	O
a	O
drawback	O
when	O
it	O
comes	O
to	O
downstream	O
tasks	O
with	O
completely	O
different	O
domains	O
such	O
as	O
classifying	O
users	O
'	O
tweets	O
on	O
Twitter	O
.	O
Therefore	O
,	O
in	O
order	O
to	O
produce	O
high	O
-	O
quality	O
outputs	O
from	O
the	O
model	O
,	O
there	O
is	O
a	O
need	O
of	O
fine	O
-	O
tuning	O
MLM	B-DatasetName
task	O
on	O
the	O
task	O
dataset	O
for	O
RoBERTa	B-MethodName
.	O
This	O
adapts	O
the	O
universal	O
language	O
model	O
into	O
our	O
narrow	O
domain	O
,	O
giving	O
it	O
prior	O
knowledge	O
for	O
later	O
classification	O
training	O
.	O
Choosing	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
the	O
key	O
factor	O
for	O
the	O
convergence	O
.	O
If	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
too	O
small	O
,	O
the	O
model	O
may	O
converge	O
too	O
slow	O
causing	O
harder	O
to	O
fit	O
to	O
new	O
data	O
distribution	O
.	O
On	O
the	O
other	O
hand	O
,	O
large	O
learning	B-HyperparameterName
rate	I-HyperparameterName
can	O
lead	O
to	O
the	O
problem	O
of	O
useful	O
feature	O
forgetting	O
.	O
Hence	O
,	O
we	O
employ	O
warm	O
-	O
up	O
learning	B-HyperparameterName
rate	I-HyperparameterName
scheduler	O
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
to	O
help	O
the	O
model	O
converge	O
faster	O
while	O
preserving	O
its	O
good	O
initialization	O
.	O

Layer	O
freezing	O
helps	O
preserving	O
useful	O
knowledge	O
that	O
a	O
pre	O
-	O
trained	O
neural	O
network	O
has	O
learned	O
.	O
Since	O
RoBERTa	B-MethodName
has	O
been	O
trained	O
on	O
a	O
huge	O
dataset	O
,	O
we	O
would	O
not	O
want	O
the	O
model	O
to	O
derive	O
too	O
far	O
from	O
its	O
pre	O
-	O
train	O
weights	O
.	O
The	O
training	O
procedure	O
is	O
divided	O
into	O
2	O
steps	O
:	O
Step	O
1	O
:	O
We	O
freeze	O
RoBERTa	B-MethodName
to	O
train	O
the	O
classification	O
head	O
for	O
the	O
first	O
epoch	O
.	O
Warm	O
-	O
up	O
learning	B-HyperparameterName
rate	I-HyperparameterName
(	O
Section	O
3.2.1	O
)	O
is	O
also	O
applied	O
.	O
Because	O
RoBERTa	B-MethodName
's	O
weights	O
are	O
already	O
well	O
trained	O
,	O
this	O
step	O
helps	O
escape	O
from	O
narrow	O
local	O
optimum	O
.	O
Step	O
2	O
:	O
RoBERTa	B-MethodName
is	O
unfrozen	O
,	O
a	O
whole	O
network	O
is	O
trained	O
.	O
In	O
RoBERTa	B-MethodName
,	O
upper	O
layers	O
produce	O
embeddings	O
with	O
more	O
contextspecific	O
than	O
lower	O
layers	O
.	O
This	O
motivates	O
us	O
to	O
further	O
apply	O
layer	O
-	O
wise	O
learning	B-HyperparameterName
rate	I-HyperparameterName
:	O
set	O
a	O
small	O
learning	B-HyperparameterName
rate	I-HyperparameterName
for	O
the	O
shallowest	O
layer	O
,	O
increase	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
as	O
the	O
layer	O
goes	O
deeper	O
.	O

When	O
training	O
a	O
huge	O
neural	O
network	O
on	O
a	O
relatively	O
small	O
dataset	O
,	O
overconfidence	O
is	O
a	O
problem	O
leading	O
to	O
bad	O
behaviours	O
of	O
the	O
model	O
.	O
This	O
phenomenon	O
occurs	O
when	O
the	O
model	O
gives	O
predictions	O
with	O
confidence	O
higher	O
than	O
its	O
accuracy	B-MetricName
.	O
While	O
there	O
have	O
been	O
a	O
lot	O
of	O
studies	O
for	O
overfitting	O
reduction	O
,	O
overconfidence	O
problem	O
attracts	O
less	O
attention	O
from	O
researchers	O
.	O
In	O
this	O
study	O
,	O
we	O
employ	O
label	B-MethodName
smoothing	I-MethodName
(	O
Szegedy	O
et	O
al	O
,	O
2015	O
)	O
to	O
prevent	O
model	O
from	O
being	O
too	O
certain	O
about	O
its	O
predictions	O
.	O
Instead	O
of	O
assigning	O
"	O
hard	O
"	O
one	O
-	O
hot	O
encoded	O
ground	O
truth	O
,	O
label	B-MethodName
smoothing	I-MethodName
adds	O
a	O
small	O
perturbation	O
into	O
the	O
label	O
by	O
a	O
smoothing	O
parameter	O
α	B-HyperparameterName
.	O
y	O
k	B-HyperparameterName
=	I-HyperparameterName
y	O
k	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
+	O
α	B-HyperparameterName
/	O
K	O
,	O
where	O
y	O
k	O
is	O
output	O
probabilities	O
of	O
K	O
classes	O
.	O
Moreover	O
,	O
label	B-MethodName
smoothing	I-MethodName
also	O
helps	O
stabilize	O
the	O
training	O
process	O
.	O
When	O
using	O
cross	O
-	O
entropy	O
loss	B-MetricName
,	O
one	O
-	O
hot	O
encoded	O
labels	O
cause	O
numerical	O
instabilities	O
if	O
the	O
prediction	O
is	O
close	O
to	O
one	O
-	O
hot	O
form	O
.	O
In	O
that	O
case	O
,	O
the	O
loss	B-MetricName
will	O
become	O
1	O
log	O
0	B-DatasetName
=	O
−	O
.	O
By	O
setting	O
α	B-HyperparameterName
=	O
0	B-DatasetName
,	O
this	O
problem	O
can	O
be	O
solved	O
.	O

Our	O
set	O
-	O
up	O
is	O
proceeded	O
as	O
following	O
instruction	O
.	O
We	O
trained	O
our	O
networks	O
with	O
PyTorch	O
framework	O
on	O
GPU	O
GeForce	O
GTX	O
2080Ti	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
32	O
for	O
20	O
epochs	O
.	O
We	O
used	O
AdamW	B-MethodName
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2017	O
)	O
for	O
the	O
optimization	O
and	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e	O
−	O
5	O
,	O
decayed	O
0.01	O
except	O
for	O
LayerNorm	O
layers	O
.	O
Label	B-MethodName
smoothing	I-MethodName
hyperparameter	O
α	B-HyperparameterName
was	O
empirically	O
experimented	O
with	O
multiple	O
values	O
of	O
0	B-DatasetName
,	O
0.1	O
,	O
0.15	O
,	O
0.2	O
and	O
the	O
last	O
value	O
possessed	O
promising	O
results	O
.	O
The	O
numbers	O
of	O
hidden	O
units	O
of	O
MLP	B-DatasetName
Head	O
and	O
BiLSTM	B-MethodName
Head	O
to	O
768	O
and	O
256	O
respectively	O
.	O

Evaluation	O
metrics	O
for	O
assessing	O
are	O
Accuracy	B-MetricName
,	O
F1score	O
,	O
Recall	B-MetricName
and	O
Precision	B-MetricName
metrics	O
on	O
public	O
validation	O
set	O
.	O
Accuracy	B-MetricName
can	O
be	O
used	O
when	O
the	O
class	O
distribution	O
is	O
similar	O
while	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
is	O
a	O
better	O
choice	O
of	O
metric	O
when	O
there	O
are	O
imbalanced	O
classes	O
.	O
precision	O
=	O
T	O
P	O
T	O
P	O
+	O
F	O
P	O
recall	O
=	O
T	O
P	O
T	O
P	O
+	O
F	O
N	O
F	O
1	O
=	O
2	O
precision	O
−1	O
+	O
recall	O
−1	O
,	O
where	O
T	O
P	O
:	O
True	O
Positive	O
,	O
F	O
P	O
:	O
False	O
Positive	O
,	O
F	O
N	O
:	O
False	O
Negative	O

Table	O
1	O
compares	O
the	O
performance	O
of	O
multiple	O
trial	O
architectures	O
training	O
with	O
pre	O
-	O
trained	O
method	O
using	O
RoBERTa	B-MethodName
in	O
our	O
base	O
settings	O
.	O
The	O
original	O
RoBERTa	B-MethodName
with	O
MLP	B-DatasetName
Head	O
shows	O
the	O
better	O
result	O
than	O
LSTM	B-MethodName
head	O
,	O
but	O
the	O
difference	O
is	O
not	O
really	O
noticeable	O
(	O
0.9082	O
vs.	O
0.9061	O
)	O
.	O
When	O
applying	O
direct	O
tuning	O
MLM	B-DatasetName
and	O
label	B-MethodName
smoothing	I-MethodName
,	O
the	O
gap	O
has	O
been	O
widened	O
,	O
specifically	O
,	O
0.9218	O
for	O
MLP	B-DatasetName
Head	O
and	O
0.9170	O
for	O
LSTM	B-MethodName
Head	O
.	O
In	O
the	O
table	O
,	O
hierarchical	O
tuning	O
and	O
back	O
translation	O
method	O
did	O
not	O
yield	O
better	O
results	O
than	O
direct	O
tuning	O
without	O
back	O
translation	O
one	O
.	O
Nevertheless	O
,	O
we	O
expect	O
this	O
method	O
can	O
generalize	O
well	O
on	O
many	O
different	O
distributed	O
datasets	O
and	O
thus	O
,	O
we	O
ensembled	O
the	O
two	O
versions	O
with	O
voting	O
and	O
submitted	O
to	O
the	O
private	O
test	O
benchmark	O
.	O
We	O
ended	O
up	O
at	O
top	O
9	O
on	O
the	O
leaderboard	O
with	O
0.9005	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
and	O

While	O
automatic	O
evaluation	O
metrics	O
are	O
very	O
important	O
and	O
invaluable	O
tools	O
for	O
rapid	O
development	O
of	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	O
)	O
systems	O
,	O
they	O
are	O
only	O
a	O
substitution	O
for	O
human	O
assessment	O
of	O
translation	O
quality	O
.	O
Various	O
methods	O
have	O
been	O
proposed	O
and	O
used	O
for	O
the	O
human	O
evaluation	O
of	O
MT	O
quality	O
by	O
assigning	O
overall	O
scores	O
to	O
MT	O
outputs	O
,	O
such	O
as	O
(	O
AL	O
-	O
PAC	O
,	O
1966	O
;	O
White	O
et	O
al	O
,	O
1994	O
;	O
Koehn	O
and	O
Monz	O
,	O
2006	O
;	O
Callison	O
-	O
Burch	O
et	O
al	O
,	O
2007	O
;	O
Roturier	O
and	O
Bensadoun	O
,	O
2011	O
;	O
Graham	O
et	O
al	O
,	O
2013	O
;	O
Barrault	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
all	O
of	O
them	O
rely	O
on	O
at	O
least	O
one	O
of	O
the	O
three	O
translation	O
quality	O
criteria	O
:	O
comprehensibility	O
(	O
comprehension	O
,	O
intelligibility	O
)	O
,	O
adequacy	O
(	O
fidelity	O
,	O
semantic	O
accuracy	B-MetricName
)	O
,	O
and	O
fluency	O
(	O
grammaticality	O
)	O
.	O
Comprehensibility	O
reflects	O
the	O
degree	O
to	O
which	O
a	O
translated	O
text	O
can	O
be	O
understood	O
,	O
adequacy	O
reflects	O
the	O
degree	O
to	O
which	O
the	O
translation	O
conveys	O
the	O
meaning	O
of	O
the	O
original	O
text	O
in	O
the	O
source	O
language	O
,	O
and	O
fluency	O
reflect	O
the	O
grammar	O
of	O
the	O
translated	O
text	O
.	O
The	O
raters	O
are	O
usually	O
asked	O
to	O
assign	O
an	O
overall	O
score	O
for	O
the	O
given	O
translation	O
criterion	O
.	O
In	O
order	O
to	O
get	O
more	O
details	O
about	O
translation	O
performance	O
,	O
error	O
classification	O
and	O
analysis	O
emerged	O
in	O
the	O
field	O
of	O
MT	O
(	O
Vilar	O
et	O
al	O
,	O
2006	O
;	O
Lommel	O
et	O
al	O
,	O
2014	O
;	O
Klubička	O
et	O
al	O
,	O
2018	O
;	O
Van	O
Brussel	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
there	O
is	O
less	O
work	O
dealing	O
with	O
human	O
perception	O
of	O
MT	O
quality	O
and	O
errors	O
.	O
For	O
statistical	O
phrase	O
-	O
based	O
MT	O
systems	O
(	O
SMT	O
)	O
,	O
Kirchhoff	O
et	O
al	O
(	O
2014	O
)	O
and	O
Federico	O
et	O
al	O
(	O
2014	O
)	O
were	O
identifying	O
error	O
types	O
which	O
are	O
mostly	O
disliked	O
by	O
readers	O
.	O
In	O
the	O
last	O
five	O
years	O
,	O
systems	O
based	O
on	O
artificial	O
neural	O
networks	O
(	O
NMT	O
)	O
have	O
become	O
the	O
new	O
state	O
of	O
the	O
art	O
.	O
Several	O
evaluation	O
studies	O
,	O
such	O
as	O
(	O
Castilho	O
et	O
al	O
,	O
2017	O
;	O
Klubička	O
et	O
al	O
,	O
2018	O
;	O
Van	O
Brussel	O
et	O
al	O
,	O
2018	O
)	O
reported	O
that	O
these	O
systems	O
are	O
able	O
to	O
produce	O
more	O
fluent	O
and	O
readable	O
translations	O
,	O
but	O
that	O
they	O
are	O
still	O
sufferring	O
from	O
adequacy	O
issues	O
.	O
In	O
addition	O
,	O
many	O
participants	O
mentioned	O
that	O
good	O
fluency	O
of	O
NMT	O
outputs	O
makes	O
it	O
more	O
difficult	O
to	O
spot	O
adequacy	O
errors	O
such	O
as	O
omissions	O
or	O
mistranslations	O
.	O
Such	O
"	O
fluently	O
inadequate	O
"	O
errors	O
may	O
mislead	O
readers	O
into	O
trusting	O
the	O
content	O
based	O
on	O
fluency	O
alone	O
,	O
especially	O
when	O
surrounded	O
by	O
fluent	O
and	O
adequate	O
parts	O
of	O
a	O
text	O
(	O
Martindale	O
and	O
Carpuat	O
,	O
2018	O
)	O
.	O
Automatic	O
identification	O
of	O
such	O
errors	O
for	O
both	O
SMT	O
and	O
NMT	O
systems	O
has	O
been	O
investigated	O
in	O
(	O
Martindale	O
et	O
al	O
,	O
2019	O
)	O
and	O
it	O
is	O
confirmed	O
that	O
these	O
errors	O
appear	O
much	O
more	O
often	O
in	O
NMT	O
system	O
.	O
To	O
the	O
best	O
of	O
our	O
knowlegde	O
,	O
comprehensibility	O
,	O
while	O
being	O
a	O
very	O
important	O
translation	O
quality	O
factor	O
,	O
has	O
not	O
been	O
investigated	O
in	O
depth	O
yet	O
.	O
It	O
should	O
be	O
stressed	O
that	O
comprehensibility	O
is	O
very	O
different	O
from	O
fluency	O
-	O
a	O
fluent	O
text	O
can	O
be	O
incomprehensible	O
(	O
for	O
example	O
"	O
Colorless	O
green	O
ideas	O
sleep	O
furiously	O
.	O
"	O
)	O
,	O
and	O
vice	O
versa	O
(	O
for	O
example	O
"	O
All	O
these	O
experiment	O
was	O
carry	O
out	O
this	O
year	O
.	O
"	O
)	O
.	O
Our	O
main	O
research	O
questions	O
are	O
:	O
RQ1	O
Are	O
there	O
"	O
comprehensible	O
inadequate	O
"	O
translations	O
which	O
are	O
misleading	O
human	O
readers	O
so	O
that	O
they	O
fully	O
trust	O
the	O
MT	O
output	O
despide	O
adequacy	O
errors	O
?	O
In	O
other	O
words	O
:	O
how	O
many	O
adequacy	O
errots	O
are	O
perceived	O
as	O
comprehensible	O
?	O
RQ2	O
If	O
the	O
answer	O
to	O
the	O
RQ1	O
is	O
"	O
yes	O
"	O
,	O
which	O
types	O
of	O
translation	O
errors	O
are	O
mainly	O
related	O
to	O
these	O
translations	O
?	O
As	O
a	O
first	O
step	O
,	O
a	O
group	O
of	O
evaluators	O
annotated	O
problematic	O
parts	O
of	O
the	O
given	O
machine	O
translated	O
text	O
.	O
They	O
were	O
not	O
asked	O
to	O
assign	O
any	O
error	O
labels	O
,	O
only	O
to	O
mark	O
the	O
parts	O
of	O
the	O
text	O
which	O
they	O
perceived	O
as	O
problematic	O
for	O
the	O
given	O
translation	O
criterion	O
.	O
They	O
first	O
annotated	O
all	O
comprehensibility	O
issues	O
,	O
and	O
after	O
about	O
two	O
weeks	O
,	O
all	O
adequacy	O
issues	O
.	O
For	O
each	O
criterion	O
,	O
they	O
were	O
asked	O
to	O
distinguish	O
major	O
and	O
minor	O
issues	O
.	O
We	O
then	O
analysed	O
all	O
major	O
issues	O
in	O
order	O
to	O
examine	O
relations	O
between	O
comprehensibility	O
and	O
adequacy	O
and	O
identify	O
error	O
types	O
.	O
The	O
analysis	O
was	O
carried	O
out	O
on	O
English	O
user	O
reviews	O
(	O
as	O
a	O
case	O
of	O
"	O
mid	O
-	O
way	O
"	O
genre	O
between	O
formal	O
and	O
informal	O
written	O
language	O
)	O
translated	O
into	O
Croatian	O
and	O
Serbian	O
(	O
as	O
a	O
case	O
of	O
mid	O
-	O
size	O
less	O
-	O
resourced	O
morphologically	O
rich	O
European	O
languages	O
)	O
.	O
It	O
is	O
worth	O
noting	O
that	O
the	O
aim	O
of	O
this	O
work	O
is	O
not	O
to	O
compare	O
MT	O
systems	O
,	O
nor	O
to	O
estimate	O
their	O
overall	O
performance	O
for	O
the	O
given	O
language	O
pairs	O
and	O
domain	O
in	O
order	O
to	O
potentially	O
improve	O
them	O
.	O
The	O
aim	O
of	O
this	O
work	O
is	O
to	O
explore	O
relations	O
between	O
two	O
aspects	O
of	O
human	O
perception	O
of	O
translation	O
quality	O
.	O

We	O
propose	O
an	O
LSTM	B-MethodName
-	O
based	O
model	O
with	O
hierarchical	O
architecture	O
on	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
from	O
code	O
-	O
switching	O
Twitter	O
data	O
.	O
Our	O
model	O
uses	O
bilingual	O
character	O
representation	O
and	O
transfer	B-TaskName
learning	I-TaskName
to	O
address	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
.	O
In	O
order	O
to	O
mitigate	O
data	O
noise	O
,	O
we	O
propose	O
to	O
use	O
token	O
replacement	O
and	O
normalization	O
.	O
In	O
the	O
3rd	O
Workshop	O
on	O
Computational	O
Approaches	O
to	O
Linguistic	O
Code	O
-	O
Switching	O
Shared	O
Task	O
,	O
we	O
achieved	O
second	O
place	O
with	O
62.76	O
%	O
harmonic	O
mean	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
for	O
English	O
-	O
Spanish	O
language	O
pair	O
without	O
using	O
any	O
gazetteer	O
and	O
knowledge	O
-	O
based	O
information	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
our	O
model	O
architecture	O
and	O
hyper	O
-	O
parameters	O
setting	O
.	O
Bilingual	O
Char	O
-	O
RNN	O
:	O
This	O
is	O
one	O
of	O
the	O
approaches	O
to	O
learn	O
character	O
-	O
level	O
embeddings	O
without	O
needing	O
of	O
any	O
lexical	O
hand	O
-	O
crafted	O
features	O
.	O
We	O
use	O
an	O
RNN	O
for	O
representing	O
the	O
word	O
with	O
character	O
-	O
level	O
information	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
.	O
Figure	O
1	O
shows	O
the	O
model	O
architecture	O
.	O
The	O
inputs	O
are	O
characters	O
extracted	O
from	O
a	O
word	O
and	O
every	O
character	O
is	O
embedded	O
with	O
d	O
dimension	O
vector	O
.	O
Then	O
,	O
we	O
use	O
it	O
as	O
the	O
input	O
for	O
a	O
Bidirectional	B-MethodName
LSTM	I-MethodName
as	O
character	O
encoder	O
,	O
wherein	O
every	O
time	O
step	O
,	O
a	O
character	O
is	O
input	O
to	O
the	O
network	O
.	O
Consider	O
a	O
t	O
as	O
the	O
hidden	O
states	O
for	O
word	O
t.	O
a	O
t	O
=	O
(	O
a	O
1	O
1	O
,	O
a	O
2	O
t	O
,	O
...	O
,	O
a	O
V	O
t	O
)	O
where	O
V	O
is	O
the	O
character	O
length	O
.	O
The	O
representation	O
of	O
the	O
word	O
is	O
obtained	O
by	O
taking	O
a	O
V	O
t	O
which	O
is	O
the	O
last	O
hidden	O
state	O
.	O
Figure	O
2	O
presents	O
the	O
overall	O
architecture	O
of	O
the	O
system	O
.	O
The	O
input	O
layers	O
receive	O
word	O
and	O
character	O
-	O
level	O
representations	O
from	O
English	O
and	O
Spanish	O
pre	O
-	O
trained	O
Fast	O
-	O
Text	O
word	O
vectors	O
and	O
Bilingual	O
Char	O
-	O
RNN	O
.	O
Consider	O
X	O
as	O
the	O
input	O
sequence	O
:	O
X	O
=	O
(	O
x	O
1	O
,	O
x	O
2	O
,	O
...	O
,	O
x	O
N	O
)	O
where	O
N	O
is	O
the	O
length	O
of	O
the	O
sequence	O
.	O
We	O
fix	O
the	O
word	O
embedding	O
parameters	O
.	O
Then	O
,	O
we	O
concatenate	O
both	O
vectors	O
to	O
get	O
a	O
richer	O
word	O
representation	O
u	O
t	O
.	O
Afterwards	O
,	O
we	O
pass	O
the	O
vectors	O
to	O
bidirectional	B-MethodName
LSTM	I-MethodName
.	O
u	O
t	O
=	O
x	O
t	O
a	O
t	O
−	O
h	O
t	O
=	O
−	O
−−−	O
LSTM	B-MethodName
(	O
u	O
t	O
,	O
−	O
−	O
h	O
t−1	O
)	O
,	O
−	O
h	O
t	O
=	O
−−−	O
−	O
LSTM	B-MethodName
(	O
u	O
t	O
,	O
−	O
−	O
h	O
t−1	O
)	O
c	O
t	O
=	O
−	O
h	O
t	O
−	O
h	O
t	O
where	O
denotes	O
the	O
concatenation	O
operator	O
.	O
Dropout	B-MethodName
is	O
applied	O
to	O
the	O
recurrent	O
layer	O
.	O
At	O
each	O
time	O
step	O
we	O
make	O
a	O
prediction	O
for	O
the	O
entity	O
of	O
the	O
current	O
token	O
.	O
A	O
softmax	B-MethodName
function	O
is	O
used	O
to	O
calculate	O
the	O
probability	O
distribution	O
of	O
all	O
possible	O
named	O
-	O
entity	O
tags	O
.	O
y	O
t	O
=	O
e	O
ct	O
T	O
j=1	O
e	O
c	O
j	O
,	O
where	O
j	O
=	O
1	O
,	O
..	O
,	O
T	O
where	O
y	O
t	O
is	O
the	O
probability	O
distribution	O
of	O
tags	O
at	O
word	O
t	O
and	O
T	O
is	O
the	O
maximum	O
time	O
step	O
.	O
Since	O
there	O
is	O
a	O
variable	O
number	O
of	O
sequence	O
length	O
,	O
we	O
padded	O
the	O
sequence	O
and	O
applied	O
mask	O
when	O
calculating	O
cross	O
-	O
entropy	O
loss	B-MetricName
function	O
.	O
Our	O
model	O
does	O
not	O
use	O
any	O
gazetteer	O
and	O
knowledge	O
-	O
based	O
information	O
,	O
and	O
it	O
can	O
be	O
easily	O
adapted	O
to	O
another	O
language	O
pair	O
.	O

We	O
trained	O
our	O
LSTM	B-MethodName
models	O
with	O
a	O
hidden	O
size	O
of	O
200	O
.	O
We	O
used	O
batch	B-HyperparameterName
size	I-HyperparameterName
equals	O
to	O
64	O
.	O
The	O
sentences	O
were	O
sorted	O
by	O
length	O
in	O
descending	O
order	O
.	O
Our	O
embedding	O
size	O
is	O
300	O
for	O
word	O
and	O
150	O
for	O
characters	O
.	O
Dropout	B-MethodName
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
of	O
0.4	O
was	O
applied	O
to	O
all	O
LSTMs	O
.	O
Adam	B-MethodName
Optimizer	B-HyperparameterName
was	O
chosen	O
with	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.01	O
.	O
We	O
applied	O
time	O
-	O
based	O
decay	O
of	O
√	O
2	O
decay	B-HyperparameterName
rate	I-HyperparameterName
and	O
stop	O
after	O
two	O
consecutive	O
epochs	O
without	O
improvement	O
.	O
We	O
tuned	O
our	O
model	O
with	O
the	O
development	O
set	O
and	O
evaluated	O
our	O
best	O
model	O
with	O
the	O
test	O
set	O
using	O
harmonic	O
mean	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
metric	O
with	O
the	O
script	O
provided	O
by	O
Aguilar	O
et	O
al	O
(	O
2018	O
)	O
.	O

Our	O
criteria	O
and	O
basis	O
for	O
evaluating	O
measurements	O
of	O
originality	O
are	O
:	O
1	O
.	O
Can	O
we	O
tell	O
whether	O
a	O
generated	O
sentence	O
is	O
an	O
original	O
use	O
of	O
language	O
?	O
2	O
.	O
Can	O
we	O
tell	O
whether	O
the	O
sentence	O
contains	O
a	O
fragment	O
from	O
the	O
ground	O
truth	O
that	O
is	O
a	O
candidate	O
for	O
protection	O
as	O
intellectual	O
property	O
?	O
Therefore	O
,	O
when	O
measuring	O
generation	O
originality	O
by	O
comparing	O
the	O
generated	O
sentence	O
with	O
the	O
sentences	O
in	O
the	O
ground	O
truth	O
,	O
then	O
the	O
answers	O
to	O
numbers	O
1	O
and	O
2	O
above	O
are	O
binary	O
.	O
Either	O
the	O
generated	O
sentence	O
is	O
an	O
original	O
use	O
of	O
language	O
or	O
it	O
is	O
not	O
.	O
Either	O
the	O
generation	O
is	O
at	O
risk	O
of	O
plagiarism	O
or	O
it	O
is	O
not	O
.	O
However	O
,	O
if	O
we	O
consider	O
that	O
the	O
ground	O
truth	O
may	O
not	O
be	O
representative	O
of	O
all	O
the	O
sentences	O
that	O
have	O
ever	O
been	O
generated	O
,	O
then	O
there	O
is	O
a	O
measure	O
of	O
uncertainty	O
that	O
may	O
be	O
added	O
to	O
the	O
binary	O
outcome	O
.	O
There	O
are	O
no	O
standard	O
automatic	O
measures	O
for	O
novelty	O
and	O
originality	O
in	O
stylized	O
language	O
generation	O
(	O
Mou	O
and	O
Vechtomova	O
,	O
2020	O
)	O
.	O
High	O
perplexity	B-MetricName
(	O
PPL	O
)	O
and	O
a	O
low	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
score	O
may	O
suggest	O
novelty	O
,	O
but	O
they	O
are	O
not	O
sufficient	O
for	O
testing	O
for	O
originality	O
.	O
High	O
PPL	O
and	O
a	O
low	O
BLEU	B-MetricName
score	I-MetricName
may	O
be	O
achieved	O
when	O
there	O
is	O
little	O
overlap	O
between	O
the	O
generated	O
language	O
and	O
the	O
ground	O
truth	O
,	O
but	O
nonesense	O
and	O
off	O
-	O
topic	O
sentences	O
are	O
rewarded	O
.	O
While	O
nonesense	O
sentences	O
may	O
be	O
novel	O
,	O
they	O
may	O
be	O
grammatically	O
incorrect	O
,	O
and	O
sentences	O
that	O
are	O
grammatically	O
correct	O
will	O
likely	O
have	O
some	O
overlap	O
with	O
fragments	O
(	O
n	O
-	O
grams	O
)	O
in	O
the	O
ground	O
truth	O
,	O
such	O
as	O
using	O
phrases	O
like	O
"	O
she	O
said	O
that	O
"	O
.	O
So	O
,	O
we	O
want	O
a	O
generation	O
originality	O
test	O
that	O
does	O
n't	O
penalize	O
n	O
-	O
gram	O
overlap	O
.	O
(	O
An	O
original	O
use	O
of	O
language	O
may	O
combine	O
common	O
n	O
-	O
grams	O
in	O
a	O
new	O
way	O
.	O
)	O
We	O
also	O
want	O
a	O
generation	O
originality	O
test	O
that	O
flags	O
potential	O
plagiarism	O
of	O
original	O
fragments	O
in	O
the	O
ground	O
truth	O
,	O
which	O
neither	O
BLEU	B-MetricName
nor	O
PPL	O
does	O
.	O
We	O
propose	O
a	O
generation	O
originality	O
test	O
(	O
GOT	O
)	O
that	O
addresses	O
original	O
uses	O
of	O
language	O
.	O
It	O
does	O
not	O
necessarily	O
address	O
original	O
ideas	O
.	O
GOT	O
is	O
equally	O
appropriate	O
for	O
stylized	O
text	B-TaskName
generation	I-TaskName
,	O
where	O
novelty	O
is	O
desirable	O
,	O
and	O
for	O
other	O
generation	O
tasks	O
where	O
there	O
is	O
not	O
an	O
imposed	O
style	O
but	O
the	O
generation	O
is	O
open	O
-	O
ended	O
,	O
including	O
summarization	B-TaskName
tasks	O
.	O

The	O
following	O
complexity	O
analysis	O
is	O
with	O
respect	O
to	O
Algorithm	O
1	O
.	O
We	O
are	O
representing	O
F	O
and	O
O	O
with	O
balanced	O
binary	O
search	O
trees	O
(	O
e.g.	O
,	O
red	O
-	O
black	O
tree	O
(	O
Guibas	O
and	O
Sedgewick	O
,	O
1978	O
;	O
OKASAKI	O
,	O
1999	O
)	O
)	O
where	O
the	O
comparator	O
is	O
lexicographic	O
ordering	O
.	O
Searching	O
,	O
insertion	O
and	O
deletion	O
in	O
such	O
trees	O
take	O
θ	B-HyperparameterName
(	O
log	O
n	O
)	O
comparisons	O
.	O
Since	O
the	O
length	O
of	O
fragments	O
is	O
assumed	O
to	O
be	O
constant	O
on	O
average	O
,	O
then	O
each	O
comparison	O
takes	O
constant	O
time	O
,	O
implying	O
that	O
each	O
search	O
/	O
insert	O
/	O
delete	O
operation	O
in	O
O	O
and	O
F	O
take	O
θ	B-HyperparameterName
(	O
log	O
n	O
)	O
time	O
.	O
Given	O
our	O
representation	O
of	O
F	O
and	O
O	O
with	O
balanced	O
binary	O
search	O
trees	O
,	O
consider	O
the	O
following	O
time	O
complexity	O
analysis	O
:	O
Let	O
n	O
=	O
number	O
of	O
sentences	O
in	O
the	O
dataset	O
.	O
The	O
first	O
for	O
-	O
loop	O
(	O
line	O
1	O
)	O
iterates	O
n	O
times	O
.	O
Let	O
c	O
=	O
the	O
average	O
length	O
(	O
i.e.	O
,	O
number	O
of	O
tokens	O
)	O
of	O
a	O
sentence	O
in	O
our	O
ground	O
truth	O
.	O
We	O
found	O
that	O
c	O
=	O
25	O
,	O
a	O
fairly	O
small	O
constant	O
.	O
Therefore	O
,	O
the	O
two	O
for	O
-	O
loops	O
in	O
Steps	O
4	O
and	O
5	O
iterate	O
on	O
average	O
a	O
constant	O
number	O
of	O
times	O
.	O
The	O
binary	O
search	O
in	O
F	O
(	O
line	O
10	O
)	O
has	O
a	O
runtime	O
complexity	O
of	O
θ	B-HyperparameterName
(	O
log	O
n	O
)	O
.	O
Depending	O
on	O
the	O
result	O
of	O
the	O
binary	O
search	O
of	O
F	O
(	O
line	O
10	O
)	O
there	O
may	O
be	O
an	O
insertion	O
to	O
F	O
(	O
line	O
14	O
)	O
which	O
has	O
a	O
runtime	O
complexity	O
of	O
θ	B-HyperparameterName
(	O
log	O
n	O
)	O
.	O
Then	O
the	O
number	O
of	O
calculations	O
in	O
lines	O
1	O
-	O
20	O
is	O
the	O
following	O
function	O
of	O
n	O
:	O
2c	O
2	O
n	O
log	O
n.	O
The	O
code	O
segment	O
of	O
lines	O
21	O
-	O
26	O
takes	O
θ	B-HyperparameterName
(	O
n	O
)	O
time	O
because	O
the	O
number	O
of	O
wl	O
-	O
token	O
fragments	O
in	O
the	O
ground	O
truth	O
dataset	O
(	O
of	O
n	O
sentences	O
where	O
each	O
sentence	O
consists	O
of	O
c	O
tokens	O
on	O
average	O
)	O
is	O
at	O
most	O
cn	O
.	O
Therefore	O
,	O
the	O
runtime	O
complexity	O
is	O
:	O
θ	B-HyperparameterName
(	O
n	O
log	O
n	O
)	O
.	O
This	O
algorithm	O
would	O
be	O
executed	O
before	O
generation	O
tasks	O
,	O
but	O
may	O
also	O
be	O
executed	O
whenever	O
the	O
Algorithm	O
1	O
Find	O
Original	O
Fragments	O
in	O
the	O
Ground	O
Truth	O
Require	O
:	O
Input	O
S	O
,	O
the	O
sentences	O
in	O
the	O
ground	O
truth	O
to	O
evaluate	O
Require	O
:	O
Input	O
F	O
,	O
list	O
of	O
fragments	O
already	O
discovered	O
,	O
may	O
be	O
empty	O
set	O
;	O
Require	O
:	O
Input	O
CountP	O
erF	O
rag	O
(	O
f	O
)	O
,	O
for	O
all	O
f	O
F	O
Require	O
:	O
O	O
,	O
list	O
of	O
original	O
fragments	O
Count	O
per	O
o	O
O	O
should	O
always	O
be	O
1	O
1	O
:	O
for	O
each	O
s	O
S	O
do	O
2	O
:	O
l	O
=	O
number	O
of	O
tokens	O
in	O
sentence	O
s	O
3	O
:	O
sentP	O
arts	O
=	O
set	O
of	O
tokens	O
in	O
s	O
4	O
:	O
for	O
each	O
wl	O
in	O
range	O
2	O
to	O
l	O
do	O
wl	O
=	O
length	O
of	O
window	O
5	O
:	O
for	O
each	O
i	O
in	O
range	O
0	B-DatasetName
to	O
l	O
−	O
wl	O
+	O
1	O
do	O
assume	O
zero	O
-	O
based	O
indexing	O
6	O
:	O
if	O
sentP	O
arts	O
[	O
i	O
]	O
or	O
sentP	O
arts	O
[	O
i	O
+	O
wl	O
−	O
1	O
]	O
=	O
special	O
token	O
2	O
then	O
7	O
:	O
Continue	O
to	O
next	O
i	O
8	O
:	O
else	O
9	O
:	O
f	O
rag	O
=	O
sentP	O
arts	O
[	O
i	O
:	O
i	O
+	O
wl	O
]	O
10	O
:	O
if	O
f	O
rag	O
F	O
then	O
binary	O
search	O
of	O
F	O
11	O
:	O
CountP	O
erF	O
rag	O
[	O
f	O
rag	O
]	O
=	O
CountP	O
erF	O
rag	O
[	O
f	O
rag	O
]	O
+	O
1	O
12	O
:	O
Break	O
from	O
for	O
-	O
loop	O
in	O
line	O
5	O
13	O
:	O
else	O
frag	O
was	O
not	O
found	O
in	O
F	O
14	O
:	O
Add	O
f	O
rag	O
to	O
F	O
15	O
:	O
CountP	O
erF	O
rag	O
[	O
f	O
rag	O
]	O
=	O
1	O
16	O
:	O
end	O
if	O
17	O
:	O
end	O
if	O
18	O
:	O
end	O
for	O
19	O
:	O
end	O
for	O
20	O
:	O
end	O
for	O
21	O
:	O
Set	O
O	O
to	O
the	O
empty	O
set	O
;	O
22	O
:	O
for	O
each	O
f	O
rag	O
in	O
F	O
do	O
23	O
:	O
if	O
CountP	O
erF	O
rag	O
[	O
f	O
rag	O
]	O
=	O
=	O
1	O
then	O
24	O
:	O
Add	O
f	O
rag	O
to	O
O	O
;	O
25	O
:	O
end	O
if	O
26	O
:	O
end	O
for	O
reference	O
set	O
changes	O
or	O
is	O
updated	O
(	O
for	O
example	O
,	O
based	O
on	O
generated	O
language	O
)	O
.	O

Question	O
Difficulty	O
Estimation	O
(	O
QDE	O
)	O
,	O
also	O
referred	O
to	O
as	O
"	O
calibration	O
"	O
,	O
is	O
a	O
crucial	O
task	O
in	O
education	O
.	O
Indeed	O
,	O
since	O
question	O
difficulty	O
can	O
be	O
leveraged	O
to	O
assess	O
the	O
skill	O
level	O
of	O
students	O
under	O
examination	O
,	O
wrongly	O
calibrated	O
questions	O
are	O
cause	O
of	O
erroneous	O
estimations	O
.	O
Also	O
,	O
an	O
accurate	O
calibration	O
enables	O
to	O
detect	O
and	O
discard	O
questions	O
that	O
are	O
too	O
easy	O
or	O
too	O
difficult	O
for	O
certain	O
students	O
.	O
Traditionally	O
,	O
QDE	O
is	O
performed	O
manually	O
or	O
with	O
pretesting	O
.	O
Manual	O
calibration	O
is	O
intrinsically	O
subjective	O
and	O
inconsistent	O
.	O
Pretesting	O
leads	O
indeed	O
to	O
an	O
accurate	O
and	O
consistent	O
calibration	O
,	O
but	O
i	O
)	O
introduces	O
a	O
long	O
delay	O
between	O
question	B-TaskName
generation	I-TaskName
and	O
when	O
the	O
question	O
can	O
be	O
used	O
to	O
score	O
students	O
,	O
and	O
ii	O
)	O
requires	O
to	O
deploy	O
the	O
new	O
questions	O
before	O
actually	O
using	O
them	O
for	O
scoring	O
.	O
To	O
address	O
the	O
issues	O
of	O
the	O
traditional	O
approaches	O
to	O
calibration	O
,	O
recent	O
research	O
tried	O
to	O
leverage	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
techniques	O
to	O
perform	O
QDE	O
from	O
question	O
text	O
:	O
the	O
idea	O
is	O
to	O
use	O
some	O
pretested	O
questions	O
to	O
train	O
a	O
model	O
that	O
performs	O
QDE	O
from	O
text	O
and	O
thus	O
eliminates	O
(	O
or	O
at	O
least	O
reduces	O
)	O
the	O
need	O
for	O
pretesting	O
of	O
new	O
questions	O
.	O
Although	O
such	O
works	O
showed	O
promising	O
results	O
,	O
none	O
of	O
them	O
experimented	O
with	O
the	O
latest	O
NLP	O
research	O
,	O
such	O
as	O
Transformer	B-MethodName
-	O
based	O
models	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
aim	O
at	O
filling	O
that	O
gap	O
,	O
studying	O
how	O
different	O
Transformers	O
,	O
also	O
trained	O
with	O
different	O
approaches	O
,	O
compare	O
with	O
the	O
current	O
state	O
of	O
the	O
art	O
.	O
We	O
evaluate	O
several	O
models	O
built	O
upon	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
DistilBERT	B-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2019	O
)	O
language	O
models	O
on	O
the	O
publicly	O
available	O
ASSISTments	O
dataset	O
and	O
the	O
private	O
CloudAcademy	O
dataset	O
.	O
By	O
using	O
data	O
from	O
two	O
different	O
domains	O
,	O
we	O
add	O
to	O
the	O
growing	O
body	O
of	O
evidence	O
showing	O
that	O
item	O
text	O
can	O
be	O
used	O
to	O
perform	O
QDE	O
.	O
More	O
precisely	O
,	O
we	O
show	O
that	O
-	O
after	O
being	O
fine	O
-	O
tuned	O
on	O
the	O
task	O
of	O
QDE	O
from	O
text	O
-	O
Transformer	B-MethodName
models	O
are	O
capable	O
of	O
calibrating	O
newly	O
generated	O
questions	O
more	O
accurately	O
than	O
previously	O
proposed	O
approaches	O
.	O
On	O
top	O
of	O
that	O
,	O
we	O
explore	O
the	O
possibility	O
of	O
leveraging	O
additional	O
textual	O
information	O
which	O
might	O
be	O
available	O
(	O
e.g.	O
transcript	O
of	O
video	O
lectures	O
)	O
to	O
perform	O
an	O
additional	O
pre	O
-	O
training	O
of	O
the	O
Transformers	O
before	O
fine	O
-	O
tuning	O
them	O
for	O
QDE	O
,	O
and	O
show	O
that	O
such	O
approach	O
can	O
be	O
used	O
to	O
further	O
improve	O
the	O
accuracy	B-MetricName
.	O
Overall	O
,	O
Transformer	B-MethodName
-	O
based	O
models	O
are	O
capable	O
of	O
reducing	O
the	O
RMSE	B-MetricName
by	O
up	O
tp	O
6.5	O
%	O
,	O
with	O
respect	O
to	O
previous	O
approaches	O
.	O
Lastly	O
,	O
we	O
perform	O
an	O
analysis	O
of	O
the	O
best	O
performing	O
model	O
to	O
under	O
-	O
stand	O
whether	O
some	O
question	O
characteristics	O
(	O
e.g.	O
text	O
length	O
,	O
question	O
type	O
)	O
particularly	O
affect	O
its	O
performance	O
.	O
Code	O
is	O
available	O
at	O
https://github	O
.	O
com	O
/	O
aradelli	O
/	O
transformers	O
-	O
for	O
-	O
qde	O
.	O

There	O
is	O
a	O
large	O
interest	O
in	O
understanding	O
how	O
textual	O
features	O
affect	O
item	O
difficulty	O
(	O
El	O
Masri	O
et	O
al	O
,	O
2017	O
;	O
Hickendorff	O
,	O
2013	O
)	O
,	O
and	O
this	O
is	O
not	O
limited	O
to	O
the	O
educational	O
domain	O
:	O
for	O
instance	O
Wang	O
et	O
al	O
(	O
2014	O
)	O
focus	O
on	O
difficulty	O
estimation	O
in	O
community	B-TaskName
question	I-TaskName
answering	I-TaskName
systems	O
.	O
The	O
first	O
works	O
addressing	O
QDE	O
from	O
text	O
focused	O
on	O
MCQs	O
and	O
used	O
deterministic	O
approaches	O
based	O
on	O
bag	O
of	O
words	O
and	O
similarities	O
between	O
question	O
,	O
answer	O
,	O
and	O
distractors	O
(	O
Alsubait	O
et	O
al	O
,	O
2013	O
;	O
Yaneva	O
et	O
al	O
,	O
2018	O
;	O
Kurdi	O
et	O
al	O
,	O
2016	O
)	O
.	O
Recent	O
works	O
mostly	O
use	O
machine	O
learning	O
approaches	O
.	O
Huang	O
et	O
al	O
(	O
2017	O
)	O
proposed	O
a	O
neural	O
network	O
for	O
QDE	O
of	O
"	O
reading	O
"	O
problems	O
,	O
in	O
which	O
the	O
answer	O
has	O
to	O
be	O
found	O
in	O
a	O
text	O
provided	O
together	O
with	O
the	O
question	O
.	O
The	O
model	O
receives	O
as	O
input	O
both	O
the	O
text	O
of	O
the	O
question	O
and	O
the	O
document	O
,	O
thus	O
it	O
actually	O
estimates	O
the	O
difficulty	O
of	O
finding	O
the	O
correct	O
answer	O
in	O
the	O
provided	O
document	O
.	O
This	O
is	O
a	O
major	O
difference	O
from	O
all	O
other	O
works	O
,	O
in	O
which	O
the	O
difficulty	O
depends	O
on	O
the	O
questions	O
only	O
.	O
Yaneva	O
et	O
al	O
(	O
2019	O
)	O
introduced	O
a	O
model	O
to	O
estimate	O
the	O
p	O
-	O
value	O
of	O
questions	O
from	O
text	O
.	O
The	O
p	O
-	O
value	O
of	O
a	O
question	O
is	O
defined	O
as	O
the	O
fraction	O
of	O
students	O
who	O
correctly	O
answered	O
it	O
and	O
does	O
not	O
account	O
for	O
different	O
skill	O
levels	O
.	O
This	O
model	O
was	O
trained	O
using	O
the	O
text	O
of	O
questions	O
and	O
a	O
large	O
dataset	O
of	O
medical	O
documents	O
,	O
thus	O
it	O
can	O
not	O
be	O
used	O
if	O
an	O
analogous	O
dataset	O
is	O
not	O
available	O
.	O
Similarly	O
,	O
the	O
model	O
proposed	O
in	O
(	O
Qiu	O
et	O
al	O
,	O
2019	O
)	O
requires	O
for	O
training	O
a	O
dataset	O
of	O
medical	O
documents	O
in	O
addition	O
to	O
the	O
question	O
texts	O
.	O
The	O
model	O
is	O
made	O
of	O
two	O
neural	O
architectures	O
to	O
estimate	O
the	O
wrongness	O
(	O
i.e.	O
1	O
−	O
p	O
-	O
value	O
)	O
of	O
newly	O
-	O
generated	O
MCQs	O
,	O
considering	O
it	O
as	O
made	O
of	O
two	O
components	O
which	O
indicate	O
i	O
)	O
how	O
difficult	O
it	O
is	O
to	O
choose	O
between	O
the	O
possible	O
choices	O
,	O
and	O
ii	O
)	O
how	O
difficult	O
it	O
is	O
to	O
recall	O
the	O
knowledge	O
required	O
to	O
answer	O
the	O
question	O
.	O
Benedetto	O
et	O
al	O
(	O
2020b	O
)	O
proposed	O
R2DE	O
,	O
a	O
model	O
that	O
estimates	O
the	O
difficulty	O
and	O
discrimination	O
,	O
as	O
defined	O
in	O
Item	O
Response	O
Theory	O
(	O
IRT	O
)	O
(	O
Hambleton	O
et	O
al	O
,	O
1991	O
)	O
,	O
of	O
newly	O
generated	O
MCQs	O
.	O
R2DE	O
computes	O
TF	O
-	O
IDF	O
features	O
from	O
the	O
text	O
of	O
the	O
questions	O
and	O
the	O
text	O
of	O
the	O
possible	O
choices	O
,	O
and	O
feeds	O
two	O
Random	O
Forest	O
regressors	O
with	O
such	O
features	O
.	O
The	O
performance	O
of	O
R2DE	O
was	O
improved	O
in	O
(	O
Benedetto	O
et	O
al	O
,	O
2020a	O
)	O
,	O
with	O
the	O
addition	O
of	O
readability	O
and	O
linguistics	O
features	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
(	O
Xue	O
et	O
al	O
,	O
2020	O
)	O
is	O
the	O
first	O
work	O
that	O
explored	O
the	O
effects	O
of	O
transfer	B-TaskName
learning	I-TaskName
for	O
QDE	O
from	O
text	O
.	O
Specifically	O
,	O
the	O
authors	O
fine	O
-	O
tune	O
pre	O
-	O
trained	O
ELMo	B-MethodName
embeddings	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
for	O
the	O
task	O
of	O
response	O
time	O
prediction	O
,	O
and	O
subsequently	O
perform	O
a	O
second	O
finetuning	O
for	O
the	O
task	O
of	O
p	O
-	O
value	O
estimation	O
.	O
Differently	O
from	O
all	O
the	O
other	O
models	O
,	O
this	O
one	O
and	O
R2DE	O
can	O
be	O
trained	O
using	O
only	O
question	O
text	O
,	O
without	O
needing	O
an	O
additional	O
dataset	O
of	O
related	O
documents	O
.	O
Our	O
approach	O
differs	O
from	O
previous	O
research	O
in	O
several	O
ways	O
.	O
First	O
of	O
all	O
,	O
we	O
adopt	O
transfer	B-TaskName
learning	I-TaskName
on	O
Transformers	O
,	O
which	O
are	O
yet	O
to	O
be	O
explored	O
for	O
QDE	O
from	O
text	O
.	O
Secondly	O
,	O
similarly	O
to	O
(	O
Benedetto	O
et	O
al	O
,	O
2020b	O
,	O
a	O
)	O
,	O
we	O
perform	O
the	O
estimation	O
of	O
the	O
IRT	O
difficulty	O
which	O
,	O
differently	O
from	O
wrongness	O
and	O
p	O
-	O
value	O
,	O
models	O
students	O
'	O
skill	O
levels	O
as	O
well	O
.	O
Specifically	O
,	O
we	O
consider	O
the	O
one	O
-	O
parameter	O
model	O
(	O
Rasch	O
,	O
1960	O
)	O
,	O
a	O
logistic	O
model	O
that	O
associates	O
a	O
skill	O
level	O
to	O
each	O
student	O
and	O
a	O
difficulty	O
level	O
to	O
each	O
question	O
(	O
both	O
represented	O
as	O
scalars	O
)	O
.	O
A	O
brief	O
introduction	O
to	O
the	O
one	O
-	O
parameter	O
IRT	O
model	O
is	O
given	O
in	O
Appendix	O
A.	O
Thirdly	O
,	O
the	O
Transformer	B-MethodName
models	O
presented	O
here	O
do	O
not	O
necessarily	O
require	O
an	O
additional	O
dataset	O
of	O
documents	O
from	O
the	O
same	O
topics	O
as	O
the	O
questions	O
,	O
and	O
they	O
can	O
be	O
trained	O
using	O
only	O
question	O
texts	O
;	O
however	O
,	O
if	O
such	O
dataset	O
is	O
available	O
,	O
it	O
can	O
be	O
leveraged	O
to	O
further	O
improve	O
the	O
accuracy	B-MetricName
of	O
calibration	O
.	O
Lastly	O
,	O
each	O
previous	O
work	O
experimented	O
on	O
one	O
private	O
dataset	O
;	O
we	O
experiment	O
on	O
two	O
datasets	O
(	O
one	O
being	O
publicly	O
available	O
)	O
,	O
showing	O
that	O
Transformers	O
can	O
be	O
successfully	O
used	O
for	O
QDE	O
in	O
different	O
domains	O
.	O

BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
is	O
a	O
pre	O
-	O
trained	O
language	O
model	O
that	O
reached	O
state	O
of	O
the	O
art	O
performance	O
in	O
many	O
language	O
tasks	O
.	O
Its	O
key	O
technical	O
innovation	O
was	O
the	O
application	O
of	O
the	O
Transformer	B-MethodName
,	O
a	O
popular	O
self	O
-	O
attention	O
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
,	O
to	O
language	O
modeling	O
.	O
BERT	B-MethodName
is	O
originally	O
trained	O
to	O
address	O
two	O
tasks	O
:	O
Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
and	O
Next	O
Sentence	O
Prediction	O
(	O
NSP	O
)	O
.	O
MLM	B-DatasetName
consists	O
in	O
removing	O
one	O
word	O
from	O
the	O
input	O
text	O
and	O
asking	O
the	O
model	O
to	O
fill	O
the	O
gap	O
,	O
while	O
in	O
NSP	O
the	O
model	O
is	O
asked	O
-	O
given	O
two	O
input	O
sentences	O
-	O
to	O
tell	O
whether	O
the	O
second	O
sentence	O
is	O
a	O
reasonable	O
continuation	O
to	O
the	O
first	O
one	O
.	O
Crucially	O
,	O
BERT	B-MethodName
can	O
be	O
used	O
for	O
many	O
different	O
downstream	O
tasks	O
,	O
as	O
we	O
do	O
here	O
for	O
QDE	O
:	O
starting	O
from	O
the	O
pre	O
-	O
trained	O
model	O
,	O
it	O
is	O
sufficient	O
to	O
stack	O
an	O
additional	O
layer	O
on	O
top	O
of	O
the	O
original	O
network	O
and	O
then	O
retrain	O
it	O
on	O
the	O
desired	O
task	O
(	O
process	O
named	O
"	O
finetuning	O
"	O
)	O
.	O
During	O
fine	O
-	O
tuning	O
,	O
the	O
internal	O
weights	O
of	O
the	O
pre	O
-	O
trained	O
model	O
are	O
updated	O
and	O
adapted	O
to	O
the	O
desired	O
task	O
(	O
together	O
with	O
the	O
weights	O
of	O
the	O
added	O
layer	O
)	O
,	O
which	O
is	O
both	O
more	O
efficient	O
than	O
training	O
the	O
whole	O
network	O
from	O
scratch	O
and	O
capable	O
of	O
better	O
results	O
,	O
as	O
the	O
knowledge	O
of	O
the	O
pre	O
-	O
trained	O
model	O
is	O
not	O
discarded	O
.	O
BERT	B-MethodName
is	O
a	O
large	O
model	O
and	O
therefore	O
requires	O
many	O
resources	O
for	O
training	O
and	O
fine	O
-	O
tuning	O
.	O
For	O
this	O
reason	O
,	O
we	O
also	O
experiment	O
with	O
DistilBERT	B-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
is	O
a	O
language	O
model	O
obtained	O
by	O
distilling	O
BERT	B-MethodName
.	O
Knowledge	B-MethodName
distillation	I-MethodName
is	O
a	O
compression	O
technique	O
in	O
which	O
a	O
small	O
model	O
is	O
trained	O
to	O
reproduce	O
the	O
full	O
output	O
distribution	O
of	O
a	O
larger	O
model	O
(	O
Hinton	O
et	O
al	O
,	O
2015	O
)	O
.	O
With	O
this	O
approach	O
,	O
DistilBERT	B-MethodName
is	O
able	O
to	O
retain	O
95	O
%	O
of	O
BERT	B-MethodName
's	O
performance	O
on	O
several	O
language	O
understanding	O
tasks	O
using	O
about	O
half	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
of	O
BERT	B-MethodName
.	O
Similarly	O
to	O
BERT	B-MethodName
,	O
Distil	O
-	O
BERT	B-MethodName
can	O
be	O
fine	O
-	O
tuned	O
on	O
downstream	O
tasks	O
and	O
it	O
is	O
therefore	O
worth	O
exploring	O
for	O
QDE	O
from	O
text	O
.	O

Before	O
comparing	O
the	O
Transformer	B-MethodName
models	O
with	O
the	O
state	O
of	O
the	O
art	O
,	O
we	O
show	O
here	O
the	O
performance	O
of	O
the	O
different	O
configurations	O
,	O
both	O
with	O
and	O
without	O
the	O
additional	O
pre	O
-	O
training	O
on	O
MLM	B-DatasetName
.	O
Table	O
1	O
displays	O
the	O
Mean	O
Absolute	O
Error	B-MetricName
(	O
MAE	B-MetricName
)	O
and	O
the	O
Root	O
Mean	O
Squared	O
Error	B-MetricName
(	O
RMSE	B-MetricName
)	O
of	O
the	O
different	O
configurations	O
;	O
the	O
error	O
is	O
the	O
difference	O
between	O
the	O
IRT	O
difficulty	O
and	O
the	O
estimation	O
of	O
the	O
Transformer	B-MethodName
model	O
.	O
The	O
input	O
configurations	O
are	O
the	O
ones	O
pre	O
-	O
:	O
i	O
)	O
Q	O
only	O
,	O
ii	O
)	O
Q+correct	O
,	O
iii	O
)	O
Q+all	O
.	O
We	O
show	O
the	O
mean	O
of	O
the	O
errors	O
,	O
measured	O
over	O
three	O
independent	O
runs	O
with	O
different	O
random	O
initializations	O
.	O
The	O
results	O
shown	O
here	O
are	O
obtained	O
on	O
Q	O
TEST	O
for	O
CloudAcademy	O
;	O
indeed	O
,	O
for	O
ASSISTments	O
the	O
text	O
of	O
the	O
possible	O
choices	O
is	O
not	O
avail	O
-	O
able	O
and	O
therefore	O
the	O
only	O
possible	O
input	O
configuration	O
is	O
Q	O
only	O
.	O
Even	O
though	O
there	O
is	O
not	O
one	O
model	O
configuration	O
which	O
clearly	O
outperforms	O
all	O
the	O
others	O
,	O
it	O
can	O
be	O
seen	O
that	O
both	O
the	O
additional	O
pre	O
-	O
training	O
and	O
the	O
textual	O
information	O
of	O
the	O
possible	O
choices	O
are	O
helpful	O
in	O
improving	O
the	O
accuracy	B-MetricName
of	O
the	O
estimation	O
.	O
For	O
DistilBERT	B-MethodName
without	O
the	O
additional	O
MLM	B-DatasetName
pre	O
-	O
training	O
the	O
best	O
configuration	O
is	O
Q+all	O
,	O
while	O
in	O
all	O
the	O
other	O
cases	O
the	O
best	O
input	O
configuration	O
is	O
Q+correct	O
.	O

As	O
baselines	O
,	O
we	O
use	O
:	O
i	O
)	O
ZeroR	O
:	O
predicts	O
for	O
all	O
the	O
questions	O
the	O
average	O
difficulty	O
of	O
the	O
training	O
questions	O
.	O
ii	O
)	O
R2DE	O
(	O
Benedetto	O
et	O
al	O
,	O
2020b	O
)	O
:	O
performs	O
difficulty	O
estimation	O
in	O
two	O
steps	O
:	O
the	O
input	O
texts	O
are	O
converted	O
into	O
feature	O
arrays	O
with	O
TF	O
-	O
IDF	O
and	O
then	O
the	O
feature	O
arrays	O
are	O
given	O
as	O
input	O
to	O
a	O
Random	O
Forest	O
regression	O
model	O
,	O
that	O
performs	O
the	O
actual	O
estimation	O
.	O
We	O
implemented	O
R2DE	O
using	O
the	O
available	O
code	O
9	O
.	O
iii	O
)	O
ELMo	B-MethodName
(	O
Xue	O
et	O
al	O
,	O
2020	O
)	O
:	O
we	O
re	O
-	O
implement	O
this	O
model	O
to	O
adapt	O
it	O
to	O
our	O
experimental	O
datasets	O
,	O
and	O
show	O
here	O
the	O
results	O
obtained	O
with	O
all	O
the	O
input	O
configurations	O
(	O
on	O
our	O
dataset	O
,	O
the	O
best	O
performing	O
input	O
configuration	O
is	O
different	O
than	O
the	O
one	O
in	O
the	O
original	O
paper	O
)	O
.	O
Table	O
2	O
and	O
Table	O
3	O
show	O
the	O
results	O
of	O
the	O
experiments	O
on	O
QDE	O
for	O
CloudAcademy	O
and	O
ASSISTments	O
,	O
by	O
displaying	O
the	O
MAE	B-MetricName
and	O
the	O
RMSE	B-MetricName
obtained	O
on	O
Q	O
TEST	O
by	O
the	O
Transformer	B-MethodName
models	O
and	O
the	O
chosen	O
baselines	O
(	O
all	O
the	O
possible	O
input	O
configurations	O
are	O
considered	O
for	O
the	O
baselines	O
)	O
.	O
For	O
each	O
Transformer	B-MethodName
model	O
,	O
only	O
one	O
input	O
configuration	O
is	O
shown	O
,	O
as	O
obtained	O
in	O
subsection	O
7.1	O
.	O
In	O
order	O
to	O
understand	O
how	O
well	O
the	O
models	O
estimate	O
the	O
difficulty	O
of	O
very	O
easy	O
and	O
very	O
challenging	O
questions	O
,	O
we	O
also	O
show	O
the	O
MAE	B-MetricName
and	O
RMSE	B-MetricName
measured	O
on	O
the	O
questions	O
whose	O
difficulty	O
b	O
is	O
such	O
that	O
|	O
b	O
|	O
>	O
2	O
(	O
also	O
referred	O
to	O
as	O
"	O
extreme	O
"	O
questions	O
)	O
.	O
The	O
results	O
shown	O
are	O
the	O
mean	O
and	O
the	O
standard	O
deviation	O
of	O
the	O
errors	O
over	O
three	O
independent	O
runs	O
.	O
Table	O
2	O
shows	O
that	O
also	O
R2DE	O
and	O
ELMo	B-MethodName
are	O
able	O
to	O
leverage	O
the	O
text	O
of	O
the	O
possible	O
choices	O
to	O
improve	O
the	O
accuracy	B-MetricName
of	O
the	O
estimation	O
;	O
indeed	O
,	O
the	O
best	O
performing	O
input	O
configuration	O
is	O
Q+all	O
for	O
R2DE	O
and	O
Q+correct	O
for	O
ELMo	B-MethodName
.	O
The	O
table	O
shows	O
that	O
the	O
Transformer	B-MethodName
models	O
generally	O
outperform	O
the	O
proposed	O
baselines	O
on	O
both	O
metrics	O
,	O
both	O
with	O
and	O
without	O
the	O
additional	O
pre	O
-	O
training	O
on	O
MLM	B-DatasetName
.	O
The	O
model	O
that	O
consistently	O
and	O
significantly	O
outperforms	O
all	O
the	O
others	O
is	O
BERT	B-MethodName
with	O
Q+correct	O
and	O
the	O
additional	O
MLM	B-DatasetName
pre	O
-	O
training	O
.	O
It	O
is	O
also	O
interesting	O
to	O
remark	O
that	O
ELMo	B-MethodName
seems	O
to	O
perform	O
estimations	O
a	O
bit	O
biased	O
towards	O
high	O
and	O
low	O
difficulties	O
:	O
indeed	O
,	O
considering	O
overall	O
MAE	B-MetricName
and	O
RMSE	B-MetricName
it	O
performs	O
at	O
the	O
same	O
level	O
of	O
R2DE	O
but	O
it	O
is	O
better	O
for	O
the	O
estimation	O
of	O
"	O
extreme	O
"	O
questions	O
.	O
This	O
might	O
also	O
be	O
the	O
reason	O
why	O
ELMo	B-MethodName
performs	O
better	O
than	O
DistilBERT	B-MethodName
without	O
MLM	B-DatasetName
on	O
"	O
extreme	O
"	O
questions	O
but	O
worse	O
overall	O
.	O
All	O
models	O
have	O
larger	O
errors	O
on	O
"	O
extreme	O
"	O
questions	O
than	O
on	O
general	O
ones	O
,	O
but	O
the	O
increase	O
is	O
different	O
for	O
each	O
of	O
them	O
:	O
the	O
best	O
performing	O
model	O
has	O
an	O
increase	O
in	O
the	O
MAE	B-MetricName
of	O
1.19	O
,	O
which	O
is	O
lower	O
than	O
the	O
other	O
Transformers	O
(	O
from	O
1.28	O
to	O
1.41	O
)	O
,	O
ELMo	B-MethodName
(	O
1.37	O
)	O
and	O
R2DE	O
(	O
1.52	O
)	O
.	O
Results	O
are	O
similar	O
for	O
the	O
RMSE	B-MetricName
:	O
the	O
increase	O
is	O
1.19	O
for	O
the	O
best	O
model	O
,	O
between	O
1.19	O
and	O
1.30	O
for	O
the	O
other	O
Transformers	O
,	O
1.23	O
for	O
ELMo	B-MethodName
(	O
Q+correct	O
)	O
and	O
1.37	O
for	O
R2DE	O
(	O
Q+all	O
)	O
.	O
Table	O
3	O
shows	O
results	O
similar	O
to	O
Table	O
2	O
:	O
BERT	B-MethodName
is	O
the	O
best	O
performing	O
model	O
and	O
both	O
Transformer	B-MethodName
models	O
outperform	O
the	O
baselines	O
.	O
However	O
,	O
we	O
can	O
see	O
that	O
the	O
errors	O
are	O
larger	O
than	O
in	O
the	O
previous	O
experiment	O
,	O
thus	O
suggesting	O
that	O
all	O
the	O
models	O
are	O
less	O
capable	O
at	O
estimating	O
the	O
difficulty	O
of	O
the	O
questions	O
in	O
ASSISTments	O
.	O
There	O
could	O
be	O
several	O
reasons	O
for	O
this	O
,	O
but	O
we	O
believe	O
that	O
this	O
limitation	O
is	O
mainly	O
due	O
to	O
two	O
aspects	O
:	O
i	O
)	O
the	O
platform	O
allows	O
the	O
creation	O
of	O
question	O
with	O
images	O
(	O
not	O
available	O
to	O
us	O
)	O
;	O
ii	O
)	O
the	O
language	O
used	O
in	O
the	O
dataset	O
is	O
"	O
less	O
natural	O
"	O
than	O
in	O
CloudAcademy	O
(	O
e.g.	O
many	O
questions	O
are	O
equations	O
with	O
no	O
additional	O
text	O
)	O
.	O

We	O
analyze	O
here	O
some	O
of	O
the	O
characteristics	O
of	O
the	O
best	O
performing	O
model	O
(	O
i.e.	O
BERT	B-MethodName
)	O
,	O
trying	O
to	O
understand	O
whether	O
there	O
are	O
some	O
question	O
properties	O
which	O
particularly	O
influence	O
its	O
accuracy	B-MetricName
.	O
We	O
perform	O
the	O
same	O
analysis	O
for	O
R2DE	O
,	O
to	O
understand	O
whether	O
such	O
characteristics	O
are	O
a	O
peculiarity	O
of	O
BERT	B-MethodName
or	O
are	O
shared	O
among	O
different	O
models	O
;	O
the	O
choice	O
of	O
R2DE	O
is	O
motivated	O
by	O
the	O
fact	O
that	O
it	O
is	O
the	O
second	O
best	O
performing	O
model	O
on	O
the	O
CloudAcademy	O
dataset	O
(	O
excluding	O
the	O
other	O
Transformer	B-MethodName
models	O
)	O
and	O
it	O
uses	O
TF	O
-	O
IDF	O
to	O
create	O
the	O
features	O
,	O
thus	O
a	O
non	O
-	O
neural	O
approach	O
.	O
We	O
report	O
here	O
the	O
results	O
of	O
three	O
analyses	O
,	O
studying	O
possible	O
differences	O
in	O
the	O
accuracy	B-MetricName
of	O
QDE	O
depending	O
on	O
i	O
)	O
input	O
length	O
and	O
question	O
difficulty	O
,	O
ii	O
)	O
number	O
of	O
correct	O
choices	O
,	O
and	O
iii	O
)	O
whether	O
it	O
is	O
a	O
cloze	O
question	O
.	O
First	O
of	O
all	O
,	O
in	O
Figure	O
3	O
we	O
show	O
for	O
the	O
two	O
datasets	O
the	O
distribution	O
of	O
questions	O
depending	O
on	O
the	O
input	O
length	O
and	O
the	O
true	O
difficulty	O
;	O
the	O
number	O
of	O
questions	O
in	O
each	O
bin	O
is	O
represented	O
by	O
its	O
color	O
.	O
The	O
two	O
figures	O
show	O
that	O
the	O
distribution	O
is	O
far	O
for	O
uniform	O
and	O
in	O
many	O
areas	O
there	O
are	O
not	O
enough	O
questions	O
to	O
obtain	O
significant	O
results	O
from	O
this	O
analysis	O
.	O
We	O
do	O
not	O
show	O
here	O
the	O
distribution	O
of	O
CloudAcademy	O
for	O
the	O
Q+all	O
input	O
config	O
-	O
uration	O
,	O
which	O
is	O
the	O
one	O
used	O
by	O
R2DE	O
,	O
but	O
it	O
is	O
qualitatively	O
similar	O
to	O
the	O
ones	O
displayed	O
.	O
Considering	O
this	O
distribution	O
,	O
we	O
decided	O
to	O
focus	O
only	O
on	O
some	O
of	O
the	O
questions	O
while	O
analyzing	O
the	O
error	O
depending	O
on	O
the	O
input	O
length	O
and	O
the	O
true	O
difficulty	O
.	O
Specifically	O
,	O
we	O
keep	O
questions	O
i	O
)	O
with	O
|	O
b	O
|	O
<	O
3	O
and	O
len	O
<	O
=	O
110	O
for	O
CloudAcademy	O
and	O
BERT	B-MethodName
(	O
96.4	O
%	O
of	O
the	O
questions	O
)	O
;	O
ii	O
)	O
with	O
|	O
b	O
|	O
<	O
3	O
and	O
len	O
<	O
=	O
110	O
for	O
CloudAcademy	O
and	O
R2DE	O
(	O
96.9	O
%	O
)	O
;	O
iii	O
)	O
with	O
|	O
b	O
|	O
<	O
4	O
and	O
len	O
<	O
=	O
80	O
for	O
ASSISTments	O
(	O
94.3	O
%	O
)	O
,	O
there	O
is	O
no	O
difference	O
between	O
BERT	B-MethodName
and	O
R2DE	O
because	O
they	O
use	O
the	O
same	O
input	O
configuration	O
.	O
Figure	O
4	O
shows	O
how	O
the	O
estimation	O
error	O
(	O
represented	O
by	O
the	O
color	O
)	O
of	O
BERT	B-MethodName
and	O
R2DE	O
depends	O
on	O
the	O
input	O
length	O
and	O
the	O
target	O
difficulty	O
of	O
the	O
questions	O
.	O
The	O
error	O
heavily	O
depends	O
on	O
the	O
target	O
difficulty	O
:	O
this	O
suggests	O
that	O
the	O
two	O
models	O
tend	O
to	O
estimate	O
difficulties	O
closer	O
to	O
0	B-DatasetName
than	O
the	O
target	O
values	O
(	O
especially	O
R2DE	O
)	O
.	O
Indeed	O
,	O
we	O
have	O
observed	O
that	O
both	O
the	O
target	O
difficulties	O
and	O
the	O
estimated	O
difficulties	O
follow	O
a	O
Gaussian	O
distribution	O
,	O
with	O
higher	O
variance	O
for	O
the	O
target	O
difficulties	O
.	O
There	O
is	O
no	O
clear	O
correlation	O
between	O
error	O
and	O
input	O
length	O
,	O
but	O
in	O
some	O
cases	O
it	O
seems	O
that	O
the	O
error	O
increases	O
with	O
the	O
input	O
length	O
(	O
e.g.	O
row	O
b	O
=	O
−1	O
in	O
BERT	B-MethodName
)	O
.	O
Figure	O
5	O
shows	O
the	O
same	O
analysis	O
as	O
Figure	O
4	O
for	O
ASSIST	O
-	O
ments	O
.	O
The	O
findings	O
are	O
very	O
similar	O
,	O
but	O
it	O
is	O
even	O
more	O
evident	O
the	O
fact	O
that	O
R2DE	O
tends	O
to	O
perform	O
predictions	O
close	O
to	O
0	B-DatasetName
;	O
the	O
error	O
of	O
BERT	B-MethodName
depends	O
less	O
heavily	O
on	O
the	O
difficulty	O
.	O
Again	O
,	O
there	O
are	O
no	O
clear	O
correlations	O
between	O
the	O
input	O
length	O
and	O
the	O
accuracy	B-MetricName
of	O
the	O
estimation	O
.	O
In	O
CloudAcademy	O
,	O
there	O
are	O
i	O
)	O
cloze	O
questions	O
,	O
in	O
which	O
the	O
correct	O
choice	O
goes	O
in	O
place	O
of	O
an	O
underscore	O
in	O
the	O
text	O
,	O
and	O
ii	O
)	O
questions	O
with	O
a	O
question	O
mark	O
at	O
the	O
end	O
.	O
Of	O
the	O
1259	O
test	O
questions	O
,	O
222	O
are	O
cloze	O
questions	O
.	O
From	O
the	O
average	O
errors	O
for	O
the	O
different	O
types	O
of	O
questions	O
,	O
we	O
observed	O
that	O
both	O
BERT	B-MethodName
and	O
R2DE	O
perform	O
slightly	O
worse	O
on	O
cloze	O
questions	O
:	O
BERT	B-MethodName
's	O
MAE	B-MetricName
is	O
0.804	O
on	O
cloze	O
questions	O
and	O
0.756	O
on	O
the	O
other	O
questions	O
;	O
similarly	O
,	O
R2DE	O
's	O
MAE	B-MetricName
is	O
0.893	O
on	O
cloze	O
questions	O
and	O
0.794	O
on	O
the	O
other	O
questions	O
.	O
Lastly	O
,	O
we	O
looked	O
at	O
the	O
average	O
error	O
depending	O
on	O
the	O
number	O
of	O
correct	O
choices	O
:	O
we	O
compared	O
questions	O
with	O
multiple	O
correct	O
choices	O
(	O
there	O
are	O
141	O
of	O
them	O
in	O
the	O
test	O
set	O
)	O
and	O
the	O
questions	O
with	O
one	O
correct	O
choice	O
.	O
For	O
BERT	B-MethodName
,	O
the	O
overall	O
MAE	B-MetricName
is	O
0.774	O
and	O
on	O
the	O
questions	O
with	O
multiple	O
correct	O
choices	O
it	O
is	O
0.764	O
;	O
in	O
the	O
case	O
of	O
R2DE	O
,	O
the	O
MAE	B-MetricName
is	O
0.813	O
overall	O
and	O
0.750	O
for	O
questions	O
with	O
multiple	O
correct	O
choices	O
.	O
This	O
difference	O
between	O
the	O
two	O
models	O
might	O
be	O
due	O
to	O
the	O
nature	O
of	O
R2DE	O
itself	O
:	O
indeed	O
,	O
it	O
uses	O
a	O
bag	O
of	O
words	O
approach	O
thus	O
it	O
does	O
not	O
care	O
about	O
the	O
position	O
of	O
each	O
word	O
.	O
Instead	O
,	O
BERT	B-MethodName
uses	O
contextual	O
embeddings	O
which	O
depend	O
on	O
the	O
position	O
of	O
each	O
word	O
and	O
the	O
encoding	O
of	O
multiple	O
correct	O
choices	O
we	O
performed	O
(	O
i.e.	O
concatenation	O
)	O
might	O
not	O
be	O
the	O
better	O
choice	O
,	O
especially	O
considering	O
that	O
probably	O
there	O
are	O
not	O
enough	O
questions	O
to	O
learn	O
the	O
encoding	O
.	O

For	O
fine	O
-	O
tuning	O
on	O
QDE	O
from	O
text	O
we	O
select	O
the	O
hyperparameters	O
from	O
the	O
following	O
pool	O
of	O
candi	O
-	O
dates	O
:	O
batch	B-HyperparameterName
size	I-HyperparameterName
=	O
16	O
,	O
32	O
,	O
64	O
;	O
learning	B-HyperparameterName
rate	I-HyperparameterName
=	O
1e	O
-	O
5	O
,	O
2e	O
-	O
5	O
,	O
3	O
-	O
5	O
;	O
patience	O
early	B-MethodName
stopping	I-MethodName
=	O
10	O
epochs	O
;	O
dropout	O
additional	O
layer	O
=	O
0.1	O
,	O
0.2	O
,	O
0.3	O
,	O
0.4	O
,	O
0.5	O
;	O
internal	O
dropout	O
=	O
0.1	O
,	O
0.2	O
,	O
0.3	O
,	O
0.4	O
,	O
0.5	O
.	O
For	O
the	O
additional	O
pre	O
-	O
training	O
on	O
MLM	B-DatasetName
,	O
the	O
hyperparameters	O
are	O
selected	O
between	O
the	O
following	O
candidates	O
:	O
batch	B-HyperparameterName
size	I-HyperparameterName
=	O
64	O
;	O
learning	B-HyperparameterName
rate	I-HyperparameterName
=	O
1e	O
-	O
5	O
;	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
=	O
4	O
,	O
12	O
,	O
24	O
,	O
36	O
;	O
dropout	O
=	O
0.1	O
.	O
In	O
both	O
cases	O
we	O
use	O
sequence	O
length	O
=	O
256	O
and	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
.	O

We	O
model	O
question	O
difficulty	O
as	O
defined	O
in	O
the	O
oneparameter	O
IRT	O
model	O
(	O
also	O
named	O
Rasch	O
model	O
(	O
Rasch	O
,	O
1960	O
)	O
)	O
,	O
which	O
associates	O
a	O
skill	O
level	O
θ	B-HyperparameterName
to	O
each	O
student	O
and	O
a	O
difficulty	O
level	O
b	O
to	O
each	O
question	O
.	O
For	O
a	O
given	O
question	O
j	O
,	O
its	O
latent	O
trait	O
b	O
j	O
define	O
the	O
item	O
response	O
function	O
(	O
i.r.f	O
.	O
)	O
,	O
which	O
indicates	O
the	O
probability	O
(	O
P	O
C	O
)	O
that	O
a	O
student	O
i	O
with	O
skill	O
level	O
θ	B-HyperparameterName
i	O
correctly	O
answers	O
the	O
question	O
.	O
The	O
formula	O
of	O
the	O
i.r.f	O
.	O
is	O
as	O
follows	O
:	O
According	O
to	O
the	O
intuition	O
,	O
a	O
student	O
with	O
a	O
given	O
skill	O
θ	B-HyperparameterName
i	O
has	O
a	O
lower	O
probability	O
of	O
correctly	O
answering	O
more	O
difficult	O
questions	O
.	O
Also	O
,	O
if	O
a	O
question	O
is	O
too	O
difficult	O
or	O
too	O
easy	O
(	O
i.e.	O
b	O
j	O
or	O
b	O
j	O
−	O
)	O
,	O
all	O
the	O
students	O
will	O
answer	O
in	O
the	O
same	O
way	O
(	O
i.e.	O
P	O
C	O
0	B-DatasetName
or	O
P	O
C	O
1	O
)	O
regardless	O
of	O
θ	B-HyperparameterName
i	O
.	O
Given	O
some	O
students	O
'	O
answers	O
to	O
a	O
set	O
of	O
questions	O
,	O
with	O
IRT	O
it	O
is	O
possible	O
to	O
estimate	O
both	O
the	O

The	O
anchor	O
algorithm	O
computes	O
the	O
topic	O
matrix	O
A	O
,	O
where	O
A	O
v	O
,	O
k	O
is	O
the	O
conditional	O
probability	O
of	O
observing	O
word	O
v	O
given	O
topic	O
k	O
,	O
e.g.	O
,	O
the	O
probability	O
of	O
seeing	O
the	O
word	O
"	O
lens	O
"	O
given	O
the	O
camera	O
topic	O
in	O
a	O
corpus	O
of	O
Amazon	O
product	O
reviews	O
.	O
Arora	O
et	O
al	O
(	O
2012a	O
)	O
find	O
these	O
probabilities	O
by	O
assuming	O
that	O
every	O
topic	O
contains	O
at	O
least	O
one	O
'	O
anchor	O
'	O
word	O
which	O
has	O
a	O
non	O
-	O
zero	O
probability	O
only	O
in	O
that	O
topic	O
.	O
Anchor	O
words	O
make	O
computing	O
the	O
topic	O
matrix	O
A	O
tractable	O
because	O
the	O
occurrence	O
pattern	O
of	O
the	O
anchor	O
word	O
mirrors	O
the	O
occurrence	O
pattern	O
of	O
the	O
topic	O
itself	O
.	O
To	O
recover	O
the	O
topic	O
matrix	O
A	O
using	O
anchor	O
words	O
,	O
we	O
first	O
compute	O
a	O
V	O
×	O
V	O
cooccurrence	O
matrix	O
Q	O
,	O
where	O
Q	O
i	O
,	O
j	O
is	O
the	O
conditional	O
probability	O
p	O
(	O
w	O
j	O
|	O
w	O
i	O
)	O
of	O
seeing	O
word	O
type	O
w	O
j	O
after	O
having	O
seen	O
w	O
i	O
in	O
the	O
same	O
document	O
.	O
A	O
form	O
of	O
the	O
Gram	O
-	O
Schmidt	O
process	O
on	O
Q	O
finds	O
anchor	O
words	O
{	O
g	O
1	O
.	O
.	O
.	O
g	O
k	O
}	O
(	O
Arora	O
et	O
al	O
,	O
2013	O
)	O
.	O
Once	O
we	O
have	O
the	O
set	O
of	O
anchor	O
words	O
,	O
we	O
can	O
compute	O
the	O
probability	O
of	O
a	O
topic	O
given	O
a	O
word	O
(	O
the	O
inverse	O
of	O
the	O
conditioning	O
in	O
A	O
)	O
.	O
This	O
coefficient	O
matrix	O
C	O
is	O
defined	O
row	O
-	O
wise	O
for	O
each	O
word	O
i	O
C	O
*	O
i	O
,	O
=	O
argmin	O
C	O
i	O
,	O
D	O
KL	O
Q	O
i	O
,	O
K	O
k=1	O
C	O
i	O
,	O
k	O
Q	O
g	O
k	O
,	O
,	O
(	O
1	O
)	O
which	O
gives	O
the	O
best	O
reconstruction	O
(	O
based	O
on	O
Kullback	O
-	O
Leibler	O
divergence	O
D	O
KL	O
)	O
of	O
non	O
-	O
anchor	O
words	O
given	O
anchor	O
words	O
'	O
conditional	O
probabilities	O
.	O
For	O
example	O
,	O
in	O
our	O
product	O
review	O
data	O
,	O
a	O
word	O
such	O
as	O
"	O
battery	O
"	O
is	O
a	O
convex	O
combination	O
of	O
the	O
anchor	O
words	O
'	O
contexts	O
(	O
Q	O
g	O
k	O
,	O
)	O
such	O
as	O
"	O
camera	O
"	O
,	O
"	O
phone	O
"	O
,	O
and	O
"	O
car	O
"	O
.	O
Solving	O
each	O
row	O
of	O
C	O
is	O
fast	O
and	O
is	O
embarrassingly	O
parallel	O
.	O
Finally	O
,	O
we	O
apply	O
Bayes	O
'	O
rule	O
to	O
recover	O
the	O
topic	O
matrix	O
A	O
from	O
the	O
coefficient	O
matrix	O
C.	O
The	O
anchor	O
algorithm	O
can	O
be	O
orders	O
of	O
magnitude	O
faster	O
than	O
probabilistic	O
inference	O
(	O
Arora	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
construction	O
of	O
Q	O
has	O
a	O
runtime	O
of	O
O	O
(	O
DN	O
2	O
)	O
where	O
D	O
is	O
the	O
number	O
of	O
documents	O
and	O
N	O
is	O
the	O
average	O
number	O
of	O
tokens	O
per	O
document	O
.	O
This	O
computation	O
requires	O
only	O
a	O
single	O
pass	O
over	O
the	O
data	O
and	O
can	O
be	O
pre	O
-	O
computed	O
for	O
interactive	O
use	O
-	O
cases	O
.	O
Once	O
Q	O
is	O
constructed	O
,	O
topic	O
recovery	O
requires	O
O	O
(	O
KV	O
2	O
+	O
K	O
2	O
V	O
I	O
)	O
,	O
where	O
K	O
is	O
the	O
number	O
of	O
topics	O
,	O
V	O
is	O
the	O
vocabulary	O
size	O
,	O
and	O
I	O
is	O
the	O
average	O
number	B-HyperparameterName
of	I-HyperparameterName
iterations	I-HyperparameterName
(	O
typically	O
100	O
-	O
1000	O
)	O
.	O
In	O
contrast	O
,	O
traditional	O
topic	O
(	O
Hoffman	O
et	O
al	O
,	O
2010	O
)	O
or	O
Stochastic	O
Variation	O
Inference	O
(	O
Hoffman	O
et	O
al	O
,	O
2013	O
)	O
improves	O
this	O
to	O
a	O
single	O
pass	O
over	O
the	O
entire	O
data	O
.	O
However	O
,	O
from	O
Heaps	O
'	O
law	O
(	O
Heaps	O
,	O
1978	O
)	O
it	O
follows	O
that	O
V	O
2	O
DN	O
for	O
large	O
datasets	O
,	O
leading	O
to	O
much	O
faster	O
inference	O
times	O
for	O
anchor	O
methods	O
compared	O
to	O
probabilistic	O
topic	O
modeling	O
.	O
Further	O
,	O
even	O
if	O
online	O
were	O
to	O
be	O
adapted	O
to	O
incorporate	O
human	O
guidance	O
,	O
a	O
single	O
pass	O
is	O
not	O
tractable	O
for	O
interactive	O
use	O
.	O

Our	O
first	O
evaluation	O
is	O
a	O
classification	O
task	O
to	O
predict	O
documents	O
'	O
newsgroup	O
membership	O
.	O
Thus	O
,	O
we	O
do	O
not	O
aim	O
for	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	B-MetricName
,	O
2	O
but	O
the	O
experiment	O
shows	O
title	O
-	O
based	O
tandem	O
anchors	O
yield	O
topics	O
closer	O
to	O
the	O
underlying	O
classes	O
than	O
Gram	O
-	O
Schmidt	O
anchors	O
.	O
After	O
randomly	O
splitting	O
the	O
data	O
into	O
test	O
and	O
training	O
sets	O
we	O
learn	O
topics	O
from	O
the	O
test	O
data	O
using	O
both	O
the	O
title	O
-	O
based	O
tandem	O
anchors	O
and	O
the	O
Gram	O
-	O
Schmidt	O
single	O
word	O
anchors	O
.	O
3	O
For	O
multiword	O
anchors	O
,	O
we	O
use	O
each	O
of	O
the	O
combiner	O
functions	O
from	O
Section	O
2.2	O
.	O
The	O
anchor	O
algorithm	O
only	O
gives	O
the	O
topic	O
-	O
word	O
distributions	O
and	O
not	O
word	O
-	O
level	O
topic	O
assignments	O
,	O
so	O
we	O
infer	O
token	O
-	O
level	O
topic	O
assignments	O
using	O
LDA	B-MethodName
Latent	O
Dirichlet	O
Allocation	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
with	O
fixed	O
topics	O
discovered	O
by	O
the	O
anchor	O
method	O
.	O
We	O
use	O
our	O
own	O
implementation	O
of	O
Gibbs	O
sampling	O
with	O
fixed	O
topics	O
and	O
a	O
symmetric	O
documenttopic	O
Dirichlet	O
prior	O
with	O
concentration	O
α	B-HyperparameterName
=	O
.01	O
.	O
Since	O
the	O
topics	O
are	O
fixed	O
,	O
this	O
inference	O
is	O
very	O
fast	O
and	O
can	O
be	O
parallelized	O
on	O
a	O
per	O
-	O
document	O
basis	O
.	O
We	O
then	O
train	O
a	O
hinge	O
-	O
loss	B-MetricName
linear	O
classifier	O
on	O
the	O
newsgroup	O
labels	O
using	O
Vowpal	O
Wabbit	O
4	O
with	O
topic	O
-	O
word	O
pairs	O
as	O
features	O
.	O
Finally	O
,	O
we	O
infer	O
topic	O
assignments	O
in	O
the	O
test	O
data	O
and	O
evaluate	O
the	O
classification	O
using	O
those	O
topic	O
-	O
word	O
features	O
.	O
For	O
both	O
training	O
and	O
test	O
,	O
we	O
exclude	O
words	O
outside	O
the	O
LDA	B-MethodName
vocabulary	O
.	O
The	O
topics	O
created	O
from	O
multiword	O
anchor	O
facets	O
are	O
more	O
accurate	O
than	O
Gram	O
-	O
Schmidt	O
topics	O
(	O
Figure	O
1	O
)	O
.	O
This	O
is	O
true	O
regardless	O
of	O
the	O
combiner	O
function	O
.	O
However	O
,	O
harmonic	O
mean	O
is	O
more	O
accurate	O
than	O
the	O
other	O
functions	O
.	O
5	O
Since	O
20NEWS	O
has	O
twenty	O
classes	O
,	O
accuracy	B-MetricName
alone	O
does	O
not	O
capture	O
confusion	O
between	O
closely	O
related	O
newsgroups	O
.	O
For	O
example	O
,	O
accuracy	B-MetricName
penalizes	O
a	O
classifier	O
just	O
as	O
much	O
for	O
labeling	O
a	O
document	O
from	O
'	O
rec.sport.baseball	O
'	O
with	O
'	O
rec.sport.hockey	O
'	O
as	O
with	O
'	O
alt.atheism	O
'	O
despite	O
the	O
similarity	O
between	O
sports	O
newsgroups	O
.	O
Consequently	O
,	O
after	O
building	O
a	O
confusion	O
matrix	O
between	O
the	O
predicted	O
and	O
true	O
classes	O
,	O
external	O
clustering	O
metrics	O
reveal	O
confusion	O
between	O
classes	O
.	O
The	O
first	O
clustering	O
metric	O
is	O
the	O
adjusted	O
Rand	O
index	O
(	O
Yeung	O
and	O
Ruzzo	O
,	O
2001	O
)	O
,	O
which	O
is	O
akin	O
to	O
accuracy	B-MetricName
for	O
clustering	O
,	O
as	O
it	O
gives	O
the	O
percentage	O
of	O
correct	O
pairing	O
decisions	O
from	O
a	O
reference	O
clustering	O
.	O
Adjusted	O
Rand	O
index	O
(	O
ARI	B-MetricName
)	O
also	O
accounts	O
for	O
chance	O
groupings	O
of	O
documents	O
.	O
Next	O
we	O
use	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
,	O
which	O
also	O
considers	O
pairwise	O
groups	O
,	O
balancing	O
the	O
contribution	O
of	O
false	O
negatives	O
,	O
but	O
without	O
the	O
true	O
negatives	O
.	O
Finally	O
,	O
we	O
use	O
variation	O
of	O
information	O
(	O
VI	O
)	O
.	O
This	O
metric	O
measures	O
the	O
amount	O
of	O
information	O
lost	O
by	O
switching	O
from	O
the	O
gold	O
standard	O
labels	O
to	O
the	O
predicted	O
labels	O
(	O
Meilȃ	O
,	O
2003	O
)	O
.	O
Since	O
we	O
are	O
measuring	O
the	O
amount	O
of	O
information	O
lost	O
,	O
lower	O
variation	O
of	O
information	O
is	O
better	O
.	O
Based	O
on	O
these	O
clustering	O
metrics	O
,	O
tandem	O
anchors	O
can	O
yield	O
superior	O
topics	O
to	O
those	O
created	O
using	O
single	O
word	O
anchors	O
(	O
Figure	O
1	O
)	O
.	O
As	O
with	O
accuracy	B-MetricName
,	O
this	O
is	O
true	O
regardless	O
of	O
which	O
combination	O
function	O
we	O
use	O
.	O
Furthermore	O
,	O
harmonic	O
mean	O
produces	O
the	O
least	O
confusion	O
between	O
classes	O
.	O
5	O
The	O
final	O
evaluation	O
is	O
topic	O
coherence	O
by	O
Newman	O
et	O
al	O
(	O
2010	O
)	O
,	O
which	O
measures	O
whether	O
the	O
topics	O
make	O
sense	O
,	O
and	O
correlates	O
with	O
human	O
judgments	O
of	O
topic	O
quality	O
.	O
Given	O
V	O
,	O
the	O
set	O
of	O
the	O
n	O
most	O
probable	O
words	O
of	O
a	O
topic	O
,	O
coherence	O
is	O
v	O
1	O
,	O
v	O
2	O
V	O
log	O
D	O
(	O
v	O
1	O
,	O
v	O
2	O
)	O
+	O
D	O
(	O
v	O
2	O
)	O
(	O
7	O
)	O
where	O
D	O
(	O
v	O
1	O
,	O
v	O
2	O
)	O
is	O
the	O
co	O
-	O
document	O
frequency	O
of	O
5	O
Significant	O
at	O
p	O
<	O
0.01/4	O
when	O
using	O
two	O
-	O
tailed	O
t	O
-	O
tests	O
with	O
a	O
Bonferroni	O
correction	O
.	O
For	O
each	O
of	O
our	O
evaluations	O
,	O
we	O
verify	O
the	O
normality	O
of	O
our	O
data	O
(	O
D'Agostino	O
and	O
Pearson	O
,	O
1973	O
)	O
and	O
use	O
two	O
-	O
tailed	O
t	O
-	O
tests	O
with	O
Bonferroni	O
correction	O
to	O
determine	O
whether	O
the	O
differences	O
between	O
the	O
different	O
methods	O
are	O
significant	O
.	O
Figure	O
1	O
:	O
Using	O
metadata	O
can	O
improve	O
anchor	O
-	O
based	O
topic	B-TaskName
models	I-TaskName
.	O
For	O
all	O
metrics	O
,	O
the	O
unsupervised	O
Gram	O
-	O
Schmidt	O
anchors	O
do	O
worse	O
than	O
creating	O
anchors	O
based	O
on	O
Newsgroup	O
titles	O
(	O
for	O
all	O
metrics	O
except	O
VI	O
,	O
higher	O
is	O
better	O
)	O
.	O
For	O
coherence	O
,	O
Gram	O
-	O
Schmidt	O
does	O
better	O
than	O
two	O
functions	O
for	O
combining	O
anchor	O
words	O
,	O
but	O
not	O
the	O
element	O
-	O
wise	O
min	O
or	O
harmonic	O
mean	O
.	O
word	O
types	O
v	O
1	O
and	O
v	O
2	O
,	O
and	O
D	O
(	O
v	O
2	O
)	O
is	O
the	O
document	O
frequency	O
of	O
word	O
type	O
v	O
2	O
.	O
A	O
smoothing	O
parameter	O
prevents	O
zero	O
logarithms	O
.	O
Figure	O
1	O
also	O
shows	O
topic	O
coherence	O
.	O
Although	O
title	O
-	O
based	O
anchor	O
facets	O
produce	O
better	O
classification	O
features	O
,	O
topics	O
from	O
Gram	O
-	O
Schmidt	O
anchors	O
have	O
better	O
coherence	O
than	O
title	O
-	O
based	O
anchors	O
with	O
the	O
vector	O
average	O
or	O
the	O
or	O
-	O
operator	O
.	O
However	O
,	O
when	O
using	O
the	O
harmonic	O
mean	O
combiner	O
,	O
title	O
-	O
based	O
anchors	O
produce	O
the	O
most	O
human	O
interpretable	O
topics	O
.	O
6	O
Harmonic	O
mean	O
beats	O
other	O
combiner	O
functions	O
because	O
it	O
is	O
robust	O
to	O
ambiguous	O
or	O
irrelevant	O
term	O
cooccurrences	O
an	O
anchor	O
facet	O
.	O
Both	O
the	O
vector	O
average	O
and	O
the	O
or	O
-	O
operator	O
are	O
swayed	O
by	O
large	O
outliers	O
,	O
making	O
them	O
sensitive	O
to	O
ambiguous	O
terms	O
in	O
an	O
anchor	O
facet	O
.	O
Element	O
-	O
wise	O
min	O
also	O
has	O
this	O
robustness	O
,	O
but	O
harmonic	O
mean	O
is	O
also	O
able	O
to	O
better	O
characterize	O
anchor	O
facets	O
as	O
it	O
has	O
more	O
centralizing	O
tendency	O
than	O
the	O
min	O
.	O

We	O
now	O
validate	O
our	O
main	O
result	O
that	O
for	O
interactive	O
topic	O
modeling	O
,	O
tandem	O
anchors	O
yields	O
better	O
topics	O
than	O
single	O
word	O
anchors	O
.	O
Like	O
our	O
titlebased	O
experiments	O
in	O
Section	O
3	O
,	O
topics	O
generated	O
from	O
users	O
become	O
features	O
to	O
train	O
and	O
test	O
a	O
classifier	O
for	O
the	O
20NEWS	O
dataset	O
.	O
We	O
choose	O
this	O
dataset	O
for	O
easier	O
comparison	O
with	O
the	O
Interactive	O
Topic	O
Modeling	O
result	O
of	O
.	O
Basedsie	O
on	O
our	O
results	O
with	O
title	O
-	O
based	O
anchors	O
,	O
we	O
use	O
the	O
harmonic	O
mean	O
combiner	O
in	O
our	O
analysis	O
.	O
As	O
before	O
,	O
we	O
report	O
not	O
only	O
accuracy	B-MetricName
,	O
but	O
also	O
multiple	O
clustering	O
metrics	O
using	O
the	O
confusion	O
matrix	O
from	O
the	O
classification	O
task	O
.	O
Finally	O
,	O
we	O
report	O
topic	O
coherence	O
.	O
Figure	O
3	O
summarizes	O
the	O
results	O
of	O
our	O
quantitative	O
evaluation	O
.	O
While	O
we	O
only	O
compare	O
user	O
generated	O
anchors	O
in	O
our	O
analysis	O
,	O
we	O
include	O
the	O
unsupervised	O
Gram	O
-	O
Schmidt	O
anchors	O
as	O
a	O
baseline	O
.	O
Some	O
of	O
the	O
data	O
violate	O
assumptions	O
of	O
normality	O
.	O
Therefore	O
,	O
we	O
use	O
Wilcoxon	O
's	O
signed	O
-	O
rank	O
test	O
(	O
Wilcoxon	O
,	O
1945	O
)	O
to	O
determine	O
if	O
the	O
differences	O
between	O
multiword	O
anchors	O
and	O
single	O
word	O
anchors	O
are	O
significant	O
.	O
Topics	O
from	O
user	O
generated	O
multiword	O
anchors	O
yield	O
higher	O
classification	O
accuracy	B-MetricName
(	O
Figure	O
3	O
)	O
.	O
Not	O
only	O
is	O
our	O
approach	O
more	O
scalable	O
than	O
the	O
Interactive	O
Topic	O
Model	O
,	O
but	O
we	O
also	O
achieve	O
Figure	O
3	O
:	O
Classification	B-TaskName
accuracy	B-MetricName
and	O
coherence	O
using	O
topic	O
features	O
gleaned	O
from	O
user	O
provided	O
multiword	O
and	O
single	O
word	O
anchors	O
.	O
Grahm	O
-	O
Schmidt	O
anchors	O
are	O
provided	O
as	O
a	O
baseline	O
.	O
For	O
all	O
metrics	O
except	O
VI	O
,	O
higher	O
is	O
better	O
.	O
Except	O
for	O
coherence	O
,	O
multiword	O
anchors	O
are	O
best	O
.	O
higher	O
classification	O
accuracy	B-MetricName
than	O
Hu	O
et	O
al	O
(	O
2014	O
)	O
.	O
9	O
Tandem	O
anchors	O
also	O
improve	O
clustering	O
metrics	O
.	O
10	O
While	O
user	O
selected	O
tandem	O
anchors	O
produce	O
better	O
classification	O
features	O
than	O
single	O
word	O
anchors	O
,	O
users	O
selected	O
single	O
word	O
anchors	O
produce	O
topics	O
with	O
similar	O
topic	O
coherence	O
scores	O
.	O
11	O
To	O
understand	O
this	O
phenomenon	O
,	O
we	O
use	O
quality	O
metrics	O
(	O
AlSumait	O
et	O
al	O
,	O
2009	O
)	O
for	O
ranking	O
topics	O
by	O
their	O
correspondence	O
to	O
genuine	O
themes	O
in	O
the	O
data	O
.	O
Significant	O
topics	O
are	O
likely	O
skewed	O
towards	O
a	O
few	O
related	O
words	O
,	O
so	O
we	O
measure	O
the	O
distance	O
of	O
each	O
topic	O
-	O
word	O
distribution	O
from	O
the	O
uniform	O
distribution	O
over	O
words	O
.	O
Topics	O
which	O
are	O
close	O
to	O
the	O
underlying	O
word	O
distribution	O
of	O
the	O
entire	O
data	O
are	O
likely	O
to	O
be	O
vacuous	O
,	O
so	O
we	O
also	O
measure	O
the	O
distance	O
of	O
each	O
topic	O
-	O
word	O
distribution	O
from	O
the	O
underlying	O
word	O
distribution	O
.	O
Finally	O
,	O
background	O
topics	O
are	O
likely	O
to	O
appear	O
in	O
a	O
wide	O
range	O
of	O
documents	O
,	O
while	O
meaningful	O
topics	O
will	O
appear	O
in	O
a	O
smaller	O
subset	O
of	O
the	O
data	O
.	O
Figure	O
4	O
reports	O
our	O
topic	O
significance	O
findings	O
.	O
For	O
all	O
three	O
significance	O
metrics	O
,	O
multiword	O
anchors	O
produce	O
more	O
significant	O
topics	O
than	O
single	O
word	O
anchors	O
.	O
10	O
Topic	O
coherence	O
is	O
based	O
solely	O
on	O
the	O
top	O
n	O
words	O
of	O
a	O
topic	O
,	O
while	O
both	O
accuracy	B-MetricName
and	O
topic	O
significance	O
depend	O
on	O
the	O
entire	O
topicword	O
distributions	O
.	O
With	O
single	O
word	O
anchors	O
,	O
topics	O
with	O
good	O
coherence	O
may	O
still	O
be	O
too	O
general	O
.	O
Tandem	O
anchors	O
enables	O
users	O
to	O
produce	O
topics	O
with	O
more	O
specific	O
word	O
distributions	O
which	O
are	O
better	O
features	O
for	O
classification	O
.	O
Users	O
are	O
able	O
to	O
combine	O
words	O
to	O
create	O
more	O
specific	O
topics	O
with	O
tandem	O
anchors	O
.	O

Following	O
(	O
Augenstein	O
et	O
al	O
,	O
2017	O
;	O
Taillé	O
et	O
al	O
,	O
2020a	O
)	O
,	O
we	O
partition	O
the	O
entity	O
mentions	O
in	O
the	O
test	O
set	O
based	O
on	O
lexical	O
overlap	O
with	O
the	O
training	O
set	O
.	O
We	O
distinguish	O
Seen	O
and	O
Unseen	O
mentions	O
and	O
also	O
extend	O
this	O
partition	O
to	O
relations	O
.	O
We	O
denote	O
a	O
relation	O
as	O
an	O
Exact	B-MetricName
Match	I-MetricName
if	O
the	O
same	O
(	O
head	O
,	O
predicate	O
,	O
tail	O
)	O
triple	O
appears	O
in	O
the	O
train	O
set	O
;	O
as	O
a	O
Partial	O
1	O
More	O
implementation	O
details	O
in	O
Appendix	O
A	O
Match	O
if	O
one	O
of	O
its	O
arguments	O
appears	O
in	O
the	O
same	O
position	O
in	O
a	O
training	O
relation	O
of	O
same	O
type	O
;	O
and	O
as	O
New	O
otherwise	O
.	O
We	O
implement	O
a	O
naive	O
Retention	O
Heuristic	O
that	O
tags	O
an	O
entity	O
mention	O
or	O
a	O
relation	O
exactly	O
present	O
in	O
the	O
training	O
set	O
with	O
its	O
majority	O
label	O
.	O
We	O
report	O
micro	O
-	O
averaged	O
Precision	B-MetricName
,	O
Recall	B-MetricName
and	O
F1	B-MetricName
scores	O
for	O
both	O
NER	B-TaskName
and	O
RE	O
in	O
Table	O
1	O
.	O
An	O
entity	O
mention	O
is	O
considered	O
correct	O
if	O
both	O
its	O
boundaries	O
and	O
type	O
have	O
been	O
correctly	O
predicted	O
.	O
For	O
RE	O
,	O
we	O
report	O
scores	O
in	O
the	O
Boundaries	O
and	O
Strict	O
settings	O
(	O
Bekoulis	O
et	O
al	O
,	O
2018	O
;	O
Taillé	O
et	O
al	O
,	O
2020b	O
)	O
.	O
In	O
the	O
Boundaries	O
setting	O
,	O
a	O
relation	O
is	O
correct	O
if	O
its	O
type	O
is	O
correct	O
and	O
the	O
boundaries	O
of	O
its	O
arguments	O
are	O
correct	O
,	O
without	O
considering	O
the	O
detection	O
of	O
their	O
types	O
.	O
The	O
Strict	O
setting	O
adds	O
the	O
requirement	O
that	O
the	O
entity	O
type	O
of	O
both	O
argument	O
is	O
correct	O
.	O

As	O
expected	O
,	O
this	O
first	O
evaluation	O
setting	O
enables	O
to	O
expose	O
an	O
important	O
lexical	O
overlap	O
bias	O
,	O
already	O
SpERT	O
89.0	O
0.1	O
74.1	O
1.0	O
86.5	O
0.2	O
77.0	O
1.1	O
52.2	O
1.1	O
38.9	O
1.0	O
57.0	O
0.8	O
75.1	O
1.2	O
48.4	O
1.0	O
36.3	O
2.0	O
53.9	O
0.8	O
SpERT	O
89.4	O
0.2	O
74.2	O
0.8	O
86.8	O
0.2	O
84.8	O
0.8	O
59.6	O
0.7	O
42.3	O
1.1	O
64.0	O
0.6	O
82.6	O
0.8	O
55.6	O
0.7	O
38.4	O
1.1	O
60.6	O
0.5	O
TABTO	O
89.7	O
0.1	O
77.4	O
0.8	O
87.5	O
0.2	O
85.9	O
0.9	O
62.6	O
1.8	O
44.6	O
2.9	O
66.4	O
1.3	O
81.6	O
1.5	O
58.1	O
1.6	O
38.5	O
3.1	O
61.7	O
1.1	O
PURE	O
90.5	O
0.2	O
80.0	O
0.3	O
88.7	O
0.1	O
86.0	O
1.3	O
60.5	O
1.0	O
47.1	O
1.6	O
65.1	O
0.7	O
84.1	O
1.1	O
57.9	O
1.3	O
44.0	O
2.0	O
62.6	O
discussed	O
in	O
NER	B-TaskName
,	O
in	O
end	O
-	O
to	O
-	O
end	O
Relation	B-TaskName
Extraction	I-TaskName
.	O
On	O
every	O
dataset	O
and	O
for	O
every	O
model	O
micro	B-MetricName
F1	I-MetricName
scores	O
are	O
the	O
highest	O
for	O
Exact	B-MetricName
Match	I-MetricName
relations	O
,	O
then	O
Partial	O
Match	O
and	O
finally	O
totally	O
unseen	O
relations	O
.	O
This	O
is	O
a	O
first	O
confirmation	O
that	O
retention	O
plays	O
an	O
important	O
role	O
in	O
the	O
measured	O
overall	O
performance	O
of	O
end	O
-	O
to	O
-	O
end	O
RE	O
models	O
.	O

Several	O
works	O
on	O
generalization	O
of	O
NER	B-TaskName
models	O
mention	O
lexical	O
overlap	O
with	O
the	O
training	O
as	O
a	O
key	O
indicator	O
of	O
performance	O
.	O
Augenstein	O
et	O
al	O
(	O
2017	O
)	O
separate	O
mentions	O
in	O
the	O
test	O
set	O
as	O
seen	O
and	O
unseen	O
during	O
training	O
and	O
measure	O
out	O
-	O
of	O
-	O
domain	B-TaskName
generalization	I-TaskName
in	O
an	O
extensive	O
study	O
of	O
two	O
CRF	B-MethodName
based	O
models	O
and	O
SENNA	O
combining	O
a	O
Convolutional	O
Neural	O
Network	O
with	O
a	O
CRF	B-MethodName
(	O
Collobert	O
and	O
Weston	O
,	O
2011	O
)	O
.	O
Taillé	O
et	O
al	O
(	O
2020a	O
)	O
compare	O
the	O
effect	O
of	O
introducing	O
contextual	O
embeddings	O
in	O
the	O
classical	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
architecture	O
in	O
a	O
similar	O
setting	O
and	O
show	O
that	O
they	O
help	O
close	O
the	O
performance	O
gap	O
on	O
unseen	O
mentions	O
and	O
domains	O
.	O
Arora	O
et	O
al	O
(	O
2020	O
)	O
;	O
Fu	O
et	O
al	O
(	O
2020b	O
,	O
a	O
)	O
study	O
the	O
influence	O
of	O
several	O
properties	O
such	O
as	O
lexical	O
overlap	O
,	O
label	O
consistency	O
and	O
entity	O
length	O
on	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
performance	O
.	O
They	O
model	O
these	O
properties	O
as	O
continuous	O
scores	O
associated	O
to	O
each	O
mention	O
and	O
bucketized	O
for	O
evaluation	O
.	O
Lexical	O
overlap	O
has	O
also	O
been	O
mentioned	O
in	O
Coreference	B-TaskName
Resolution	I-TaskName
(	O
Moosavi	O
and	O
Strube	O
,	O
2017	O
)	O
where	O
coreferent	O
mentions	O
tend	O
to	O
co	O
-	O
occur	O
in	O
the	O
test	O
and	O
train	O
sets	O
.	O
In	O
this	O
line	O
of	O
works	O
,	O
the	O
impact	O
of	O
lexical	O
overlap	O
is	O
measured	O
either	O
by	O
separating	O
performance	O
depending	O
on	O
the	O
property	O
of	O
mentions	O
(	O
seen	O
or	O
unseen	O
)	O
or	O
with	O
outof	O
-	O
domain	O
evaluation	O
with	O
a	O
test	O
set	O
from	O
a	O
different	O
dataset	O
with	O
lower	O
lexical	O
overlap	O
with	O
the	O
train	O
set	O
.	O
Another	O
recently	O
proposed	O
method	O
for	O
finegrained	O
evaluation	O
of	O
NLP	O
models	O
beyond	O
a	O
single	O
benchmark	O
score	O
is	O
to	O
modify	O
the	O
test	O
sentences	O
in	O
a	O
controlled	O
manner	O
.	O
McCoy	O
et	O
al	O
(	O
2019	O
)	O
expose	O
lexical	O
overlap	O
as	O
a	O
shallow	O
heuristic	O
adopted	O
by	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Natural	B-TaskName
Language	I-TaskName
Inference	I-TaskName
models	O
,	O
especially	O
by	O
swapping	O
subject	O
and	O
object	O
of	O
verbs	O
in	O
the	O
hypothesis	O
of	O
some	O
examples	O
where	O
the	O
premise	O
entails	O
the	O
hypothesis	O
.	O
While	O
such	O
a	O
modification	O
changes	O
the	O
label	O
of	O
these	O
examples	O
to	O
non	O
-	O
entailment	O
,	O
all	O
models	O
tested	O
show	O
a	O
spectacular	O
drop	O
of	O
accuracy	B-MetricName
on	O
these	O
models	O
.	O
Ribeiro	O
et	O
al	O
(	O
2020	O
)	O
propose	O
a	O
broader	O
set	O
of	O
test	O
set	O
modifications	O
to	O
individually	O
test	O
robustness	O
of	O
NLP	O
models	O
to	O
several	O
patterns	O
such	O
as	O
the	O
introduction	O
of	O
negation	O
,	O
swapping	O
words	O
with	O
synonyms	O
,	O
changing	O
tense	O
and	O
much	O
more	O
.	O
In	O
pipeline	O
RE	O
where	O
ground	O
truth	O
candidate	O
arguments	O
are	O
given	O
,	O
models	O
often	O
use	O
intermediate	O
representations	O
based	O
on	O
entity	O
types	O
that	O
reduce	O
lexical	O
overlap	O
issues	O
.	O
However	O
,	O
Rosenman	O
et	O
al	O
(	O
2020	O
)	O
show	O
that	O
they	O
still	O
tend	O
to	O
adopt	O
shallow	O
heuristics	O
based	O
on	O
the	O
type	O
of	O
the	O
arguments	O
and	O
the	O
presence	O
of	O
triggers	O
indicative	O
of	O
the	O
presence	O
of	O
a	O
relation	O
.	O
They	O
propose	O
hard	O
cases	O
with	O
several	O
mentions	O
of	O
same	O
types	O
for	O
which	O
Relation	O
Classifiers	O
struggle	O
connecting	O
the	O
correct	O
pair	O
.	O
Concurrently	O
,	O
Peng	O
et	O
al	O
(	O
2020	O
)	O
confirm	O
that	O
RE	O
benchmarks	O
present	O
shallow	O
cues	O
such	O
as	O
the	O
type	O
of	O
the	O
candidate	O
arguments	O
that	O
can	O
be	O
used	O
alone	O
to	O
infer	O
the	O
relation	O
.	O
We	O
propose	O
to	O
extend	O
previous	O
work	O
on	O
NER	B-TaskName
and	O
RE	O
to	O
the	O
more	O
realistic	O
end	O
-	O
to	O
-	O
end	O
RE	O
setting	O
with	O
two	O
of	O
the	O
previously	O
described	O
approaches	O
:	O
1	O
)	O
separating	O
performance	O
by	O
lexical	O
overlap	O
of	O
mentions	O
or	O
argument	O
pairs	O
and	O
2	O
)	O
modifying	O
some	O
CoNLL04	O
test	O
examples	O
by	O
swapping	O
relations	O
heads	O
and	O
tails	O
.	O

For	O
every	O
model	O
,	O
we	O
use	O
the	O
original	O
code	O
associated	O
with	O
the	O
papers	O
with	O
the	O
default	O
best	O
performing	O
hyperparameters	O
unless	O
stated	O
otherwise	O
.	O
We	O
run	O
5	O
runs	O
on	O
a	O
single	O
NVIDIA	O
2080Ti	O
GPU	O
for	O
each	O
of	O
them	O
on	O
each	O
dataset	O
.	O
For	O
CoNLL04	O
and	O
ACE05	O
,	O
we	O
train	O
each	O
model	O
with	O
both	O
the	O
cased	O
and	O
uncased	O
versions	O
of	O
BERT	B-MethodName
BASE	B-MethodName
and	O
only	O
keep	O
the	O
best	O
performing	O
setting	O
.	O
PURE	O
(	O
Zhong	O
and	O
Chen	O
,	O
2021	O
)	O
1	O
We	O
use	O
the	O
approximation	O
model	O
and	O
limit	O
use	O
a	O
context	O
window	O
of	O
0	B-DatasetName
to	O
only	O
use	O
the	O
current	O
sentence	O
for	O
prediction	O
and	O
be	O
able	O
to	O
compare	O
with	O
other	O
models	O
.	O
For	O
ACE05	O
,	O
we	O
use	O
the	O
standard	O
bert	O
-	O
base	O
-	O
uncased	O
LM	O
but	O
use	O
the	O
bert	O
-	O
base	O
-	O
cased	O
version	O
on	O
CoNLL04	O
which	O
results	O
in	O
a	O
significant	O
+2.4	O
absolute	O
improvement	O
in	O
RE	O
Strict	O
micro	B-MetricName
F1	I-MetricName
score	O
.	O
SpERT	O
(	O
Eberts	O
and	O
Ulges	O
,	O
2020	O
)	O
2	O
We	O
use	O
the	O
original	O
implementation	O
as	O
is	O
with	O
bert	O
-	O
base	O
-	O
cased	O
for	O
both	O
ACE05	O
and	O
CoNLL04	O
since	O
the	O
uncased	O
version	O
is	O
not	O
beneficial	O
,	O
even	O
on	O
ACE05	O
where	O
there	O
are	O
fewer	O
proper	O
nouns	O
.	O
For	O
the	O
Ent	O
-	O
SpERT	O
ablation	O
,	O
we	O
simply	O
remove	O
the	O
max	O
-	O
pooled	O
context	O
representation	O
from	O
the	O
final	O
concatenation	O
in	O
the	O
RE	O
module	O
.	O
This	O
modifies	O
the	O
RE	O
classifier	O
's	O
input	O
dimension	O
from	O
the	O
original	O
2354	O
to	O
1586	O
.	O
Two	O
are	O
better	O
than	O
one	O
(	O
TABTO	O
)	O
(	O
Wang	O
and	O
Lu	O
,	O
2020	O
)	O
3	O
We	O
use	O
the	O
original	O
implementation	O
with	O
bert	O
-	O
base	O
-	O
uncased	O
for	O
both	O
ACE05	O
and	O
CoNLL04	O
since	O
the	O
cased	O
version	O
is	O
not	O
beneficial	O
on	O
CoNLL04	O
.	O

We	O
present	O
general	O
datasets	O
statistics	O
in	O
Table	O
4	O
.	O
We	O
also	O
compute	O
average	O
values	O
of	O
some	O
entity	O
and	O
relation	O
attributes	O
inspired	O
by	O
(	O
Fu	O
et	O
al	O
,	O
2020a	O
)	O
and	O
reported	O
in	O
Table	O
5	O
.	O
We	O
report	O
two	O
of	O
their	O
entity	O
attributes	O
:	O
entity	O
length	O
in	O
number	O
of	O
tokens	O
(	O
eLen	O
)	O
and	O
entity	O
label	O
consistency	O
(	O
eCon	O
)	O
.	O
Given	O
a	O
test	O
entity	O
mention	O
,	O
its	O
label	O
consistency	O
is	O
the	O
number	O
of	O
occurrences	O
in	O
the	O
training	O
set	O
with	O
the	O
same	O
type	O
divided	O
by	O
its	O
total	O
number	O
of	O
occurrences	O
.	O
It	O
is	O
zero	O
for	O
unseen	O
mentions	O
.	O
Because	O
eCon	O
reflects	O
both	O
the	O
ambiguity	O
of	O
labels	O
for	O
seen	O
entities	O
and	O
the	O
proportion	O
of	O
unseen	O
entities	O
,	O
we	O
propose	O
to	O
introduce	O
the	O
eCon	O
*	O
score	O
that	O
only	O
averages	O
label	O
consistency	O
of	O
seen	O
mentions	O
and	O
eLex	O
,	O
the	O
proportion	O
of	O
entities	O
with	O
lexical	O
overlap	O
with	O
the	O
train	O
set	O
.	O
We	O
introduce	O
similar	O
scores	O
for	O
relations	O
.	O
Relation	O
label	O
consistency	O
(	O
rCon	O
)	O
extends	O
label	O
consistency	O
for	O
triples	O
.	O
Argument	O
types	O
label	O
constitency	O
(	O
aCon	O
)	O
considers	O
the	O
labels	O
of	O
every	O
pair	O
of	O
mentions	O
of	O
corresponding	O
types	O
in	O
the	O
training	O
set	O
.	O
Because	O
pairs	O
of	O
types	O
are	O
all	O
seen	O
during	O
training	O
we	O
do	O
not	O
decompose	O
aCon	O
into	O
aCon	O
*	O
and	O
aLex	O
.	O
Argument	O
length	O
(	O
aLen	O
)	O
is	O
the	O
sum	O
of	O
the	O
lengths	O
of	O
the	O
head	O
and	O
tail	O
mentions	O
.	O
Argument	O
distance	O
(	O
aDist	O
)	O
is	O
the	O
number	O
of	O
tokens	O
between	O
the	O
head	O
and	O
the	O
tail	O
of	O
a	O
relation	O
.	O
We	O
present	O
a	O
more	O
complete	O
report	O
of	O
overall	O
Precision	B-MetricName
,	O
Recall	B-MetricName
and	O
F1	B-MetricName
scores	O
that	O
can	O
be	O
interpreted	O
in	O
light	O
of	O
these	O
statistics	O
in	O
Table	O
6	O
.	O

Relation	B-TaskName
extraction	I-TaskName
(	O
RE	O
)	O
is	O
an	O
essential	O
topic	O
in	O
natural	O
language	O
processing	O
and	O
has	O
attracted	O
extensive	O
attention	O
.	O
Current	O
RE	O
approaches	O
achieve	O
fantastic	O
results	O
on	O
common	O
datasets	O
,	O
while	O
they	O
still	O
struggle	O
on	O
practical	O
applications	O
.	O
In	O
this	O
paper	O
,	O
we	O
analyze	O
the	O
above	O
performance	O
gap	O
,	O
the	O
underlying	O
reason	O
of	O
which	O
is	O
that	O
practical	O
applications	O
intrinsically	O
have	O
more	O
hard	O
cases	O
.	O
To	O
make	O
RE	O
models	O
more	O
robust	O
on	O
such	O
practical	O
hard	O
cases	O
,	O
we	O
propose	O
a	O
case	O
-	O
oriented	O
construction	O
framework	O
to	O
build	O
a	O
Hard	O
Case	O
Relation	B-TaskName
Extraction	I-TaskName
Dataset	O
(	O
HacRED	O
)	O
.	O
The	O
proposed	O
HacRED	O
consists	O
of	O
65	O
,	O
225	O
relational	O
facts	O
annotated	O
from	O
9	O
,	O
231	O
documents	O
with	O
sufficient	O
and	O
diverse	O
hard	O
cases	O
.	O
Notably	O
,	O
HacRED	O
is	O
one	O
of	O
the	O
largest	O
Chinese	O
document	O
-	O
level	O
RE	O
datasets	O
and	O
achieves	O
a	O
high	O
96	O
%	O
F1	B-MetricName
score	I-MetricName
on	O
data	O
quality	O
.	O
Furthermore	O
,	O
we	O
apply	O
the	O
stateof	O
-	O
the	O
-	O
art	O
RE	O
models	O
on	O
this	O
dataset	O
and	O
conduct	O
a	O
thorough	O
evaluation	O
.	O
The	O
results	O
show	O
that	O
the	O
performance	O
of	O
these	O
models	O
is	O
far	O
lower	O
than	O
humans	O
,	O
and	O
RE	O
applying	O
on	O
practical	O
hard	O
cases	O
still	O
requires	O
further	O
efforts	O
.	O
Ha	O
-	O
cRED	O
is	O
publicly	O
available	O
at	O
https://github	O
.	O
com	O
/	O
qiaojiim	O
/	O
HacRED	O
.	O

Relation	B-TaskName
extraction	I-TaskName
(	O
RE	O
)	O
is	O
one	O
of	O
the	O
core	O
NLP	O
tasks	O
and	O
plays	O
an	O
increasingly	O
important	O
role	O
in	O
knowledge	B-TaskName
graph	I-TaskName
completion	I-TaskName
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
and	O
question	B-TaskName
answering	I-TaskName
(	O
Dong	O
et	O
al	O
,	O
2015	O
)	O
.	O
RE	O
aims	O
to	O
extract	O
structured	O
relational	O
facts	O
,	O
i.e.	O
,	O
triples	O
such	O
as	O
(	O
Bill	O
Gates	O
,	O
founder_of	O
,	O
Microsoft	O
)	O
from	O
plain	O
texts	O
.	O
Recently	O
,	O
various	O
models	O
(	O
Zeng	O
et	O
al	O
,	O
2018	O
;	O
Takanobu	O
et	O
al	O
,	O
2019	O
;	O
Fu	O
et	O
al	O
,	O
2019	O
;	O
have	O
been	O
proposed	O
to	O
identify	O
the	O
relational	O
facts	O
and	O
achieved	O
state	O
-	O
of	O
-	O
theart	O
(	O
SOTA	O
)	O
performance	O
,	O
among	O
which	O
the	O
latest	O
method	O
CasRel	O
achieves	O
notable	O
91.8	O
%	O
F1	B-MetricName
score	I-MetricName
on	O
WebNLG	B-DatasetName
(	O
Gardent	O
et	O
al	O
,	O
2017	O
)	O
and	O
89.6	O
%	O
on	O
NYT	O
(	O
Riedel	O
et	O
al	O
,	O
2010	O
)	O
.	O
However	O
,	O
can	O
these	O
seemingly	O
fantastic	O
results	O
prove	O
that	O
the	O
current	O
RE	O
models	O
are	O
powerful	O
enough	O
to	O
perform	O
well	O
in	O
practical	O
applications	O
.	O
To	O
answer	O
the	O
question	O
,	O
we	O
employ	O
CasRel	O
on	O
300	O
randomly	O
selected	O
samples	O
of	O
WebNLG	B-DatasetName
and	O
the	O
same	O
number	O
of	O
data	O
from	O
practical	O
DuIE	O
1	O
.	O
The	O
F1	B-MetricName
scores	O
under	O
these	O
scenarios	O
drop	O
significantly	O
from	O
89.3	O
%	O
to	O
62.8	O
%	O
.	O
As	O
illustrated	O
in	O
Figure	O
1	O
,	O
CasRel	O
extracts	O
correct	O
triples	O
(	O
Elliot	O
See	O
,	O
place_of_birth	O
,	O
Dallas	O
)	O
and	O
(	O
Elliot	O
See	O
,	O
place_of_death	O
,	O
St.	O
Louis	O
)	O
in	O
WebNLG	B-DatasetName
where	O
keywords	O
such	O
as	O
born	O
and	O
died	O
explicitly	O
express	O
the	O
relation	O
information	O
.	O
In	O
contrast	O
,	O
Cas	O
-	O
Rel	O
fails	O
to	O
extract	O
triples	O
such	O
as	O
(	O
Yang	O
Jima	O
,	O
graduate_from	O
,	O
Communication	O
University	O
of	O
China	O
)	O
where	O
no	O
keywords	O
like	O
graduate	O
are	O
mentioned	O
.	O
The	O
most	O
significant	O
reason	O
why	O
CasRel	O
performs	O
well	O
on	O
WebNLG	B-DatasetName
but	O
struggles	O
on	O
practical	O
data	O
is	O
that	O
more	O
challenging	O
instances	O
which	O
we	O
refer	O
to	O
as	O
hard	O
cases	O
exist	O
in	O
the	O
practical	O
applications	O
.	O
Moreover	O
,	O
according	O
to	O
the	O
statistics	O
of	O
entity	O
description	O
documents	O
in	O
CN	O
-	O
DBpedia	B-DatasetName
,	O
at	O
least	O
40.1	O
%	O
relational	O
facts	O
can	O
only	O
be	O
extracted	O
from	O
hard	O
cases	O
.	O
Therefore	O
,	O
relation	B-TaskName
extraction	I-TaskName
from	O
hard	O
cases	O
can	O
not	O
be	O
neglected	O
and	O
demands	O
more	O
attention	O
.	O
Although	O
many	O
datasets	O
(	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Yao	O
et	O
al	O
,	O
2019	O
)	O
have	O
been	O
proposed	O
for	O
RE	O
,	O
they	O
rarely	O
analyze	O
the	O
performance	O
gap	O
and	O
focus	O
on	O
the	O
hard	O
cases	O
.	O
In	O
order	O
to	O
make	O
models	O
robust	O
on	O
hard	O
cases	O
and	O
more	O
fit	O
practical	O
scenarios	O
,	O
in	O
this	O
paper	O
,	O
we	O
aim	O
to	O
build	O
a	O
RE	O
dataset	O
with	O
sufficient	O
hard	O
cases	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
a	O
case	O
-	O
oriented	O
construction	O
framework	O
based	O
on	O
the	O
challenging	O
instances	O
and	O
build	O
a	O
Hard	O
Case	O
Relation	B-TaskName
Extraction	I-TaskName
Dataset	O
(	O
HacRED	O
)	O
.	O
Specifically	O
,	O
we	O
first	O
obtain	O
general	O
,	O
massive	O
,	O
and	O
various	O
contexts	O
as	O
well	O
as	O
relational	O
facts	O
from	O
CN	O
-	O
DBpedia	B-DatasetName
to	O
construct	O
a	O
distantly	O
supervised	O
dataset	O
.	O
The	O
crucial	O
part	O
is	O
to	O
distinguish	O
hard	O
cases	O
from	O
abundant	O
data	O
.	O
Therefore	O
,	O
we	O
formulate	O
nine	O
indicators	O
through	O
systematic	O
analysis	O
of	O
hard	O
cases	O
to	O
quantify	O
them	O
.	O
Then	O
,	O
we	O
conduct	O
feature	B-TaskName
engineering	I-TaskName
based	O
on	O
the	O
valid	O
indicators	O
.	O
Afterwards	O
,	O
a	O
classifier	O
is	O
trained	O
for	O
distinguishing	O
the	O
desired	O
hard	O
cases	O
.	O
Finally	O
,	O
we	O
develop	O
a	O
crowdsourcing	O
platform	O
with	O
a	O
novel	O
three	O
-	O
stage	O
annotation	O
strategy	O
and	O
effective	O
aggregation	O
method	O
CrowdTruth2.0	O
(	O
Dumitrache	O
et	O
al	O
,	O
2018	O
)	O
to	O
guarantee	O
the	O
data	O
size	O
and	O
quality	O
.	O
In	O
total	O
,	O
HacRED	O
consists	O
of	O
9	O
,	O
231	O
instances	O
with	O
26	O
predefined	O
relations	O
and	O
9	O
types	O
of	O
entities	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
it	O
is	O
one	O
of	O
the	O
largest	O
document	O
-	O
level	O
RE	O
benchmark	O
.	O
Moreover	O
,	O
HacRED	O
contains	O
sufficient	O
and	O
diverse	O
hard	O
cases	O
in	O
line	O
with	O
practice	O
.	O
We	O
conduct	O
extensive	O
experiments	O
and	O
systematic	O
error	O
analysis	O
of	O
SOTA	O
models	O
on	O
HacRED	O
.	O
A	O
sharp	O
performance	O
drop	O
on	O
HacRED	O
compared	O
to	O
the	O
existing	O
benchmarks	O
proves	O
that	O
RE	O
in	O
practical	O
applications	O
remains	O
an	O
open	O
problem	O
and	O
still	O
requires	O
further	O
research	O
.	O
To	O
recap	O
,	O
our	O
main	O
contributions	O
are	O
three	O
-	O
fold	O
:	O
We	O
first	O
analyze	O
the	O
performance	O
gap	O
between	O
popular	O
datasets	O
and	O
practical	O
applications	O
,	O
and	O
therefore	O
construct	O
one	O
of	O
the	O
largest	O
Chinese	O
document	O
-	O
level	O
RE	O
dataset	O
which	O
contains	O
sufficient	O
and	O
diverse	O
hard	O
cases	O
to	O
improve	O
the	O
evaluation	O
for	O
complex	O
RE	O
tasks	O
.	O
We	O
propose	O
a	O
case	O
-	O
oriented	O
construction	O
framework	O
to	O
build	O
RE	O
dataset	O
toward	O
spe	O
-	O
cial	O
cases	O
.	O
Meanwhile	O
,	O
we	O
design	O
a	O
novel	O
three	O
-	O
stage	O
annotation	O
method	O
applicable	O
for	O
crowdsourcing	O
of	O
complex	O
RE	O
.	O
We	O
systematically	O
evaluate	O
the	O
current	O
mainstream	O
RE	O
models	O
on	O
HacRED	O
and	O
justify	O
its	O
effectiveness	O
in	O
depth	O
.	O
2	O
Related	O
Work	O

Recently	O
,	O
many	O
exciting	O
works	O
have	O
been	O
proposed	O
to	O
solve	O
the	O
RE	O
tasks	O
.	O
(	O
1	O
)	O
Joint	O
Model	O
:	O
NovelTagging	O
(	O
Zheng	O
et	O
al	O
,	O
2017	O
)	O
first	O
formulates	O
the	O
task	O
as	O
a	O
sequence	O
labeling	O
problem	O
and	O
presents	O
a	O
novel	O
tagging	O
schema	O
to	O
jointly	O
extract	O
entities	O
and	O
relations	O
.	O
CopyRE	O
(	O
Zeng	O
et	O
al	O
,	O
2018	O
)	O
extracts	O
triples	O
based	O
on	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
structure	O
and	O
integrates	O
the	O
copy	O
mechanism	O
for	O
entity	O
generation	O
.	O
GraphRel	O
(	O
Fu	O
et	O
al	O
,	O
2019	O
)	O
uses	O
graph	B-MethodName
convolutional	I-MethodName
network	I-MethodName
(	O
GCN	B-MethodName
)	O
to	O
capture	O
features	O
of	O
words	O
and	O
text	O
.	O
CasRel	O
is	O
different	O
from	O
the	O
past	O
and	O
is	O
able	O
to	O
extract	O
more	O
triples	O
by	O
learning	O
relationspecific	O
entity	O
taggers	O
.	O
(	O
2	O
)	O
Pipeline	O
Model	O
:	O
PURE	O
(	O
Zhong	O
and	O
Chen	O
,	O
2020	O
)	O
is	O
a	O
simple	O
pipelined	O
approach	O
which	O
learns	O
an	O
entity	O
model	O
and	O
a	O
relation	O
model	O
independently	O
.	O
DGCNN	B-MethodName
-	O
BERT	B-MethodName
is	O
a	O
powerful	O
pipeline	O
method	O
that	O
first	O
identifies	O
multiple	O
relations	O
and	O
then	O
labels	O
the	O
head	O
and	O
tail	O
entities	O
given	O
a	O
relation	O
.	O
It	O
achieves	O
89.3	O
F1	B-MetricName
scores	O
and	O
has	O
won	O
the	O
champion	O
in	O
the	O
Competition	O
of	O
DuIE	O
held	O
by	O
Baidu	O
Inc.	O
(	O
3	O
)	O
Document	O
-	O
level	O
Relation	B-TaskName
Classification	I-TaskName
Models	O
:	O
LSR	O
(	O
Nan	O
et	O
al	O
,	O
2020	O
)	O
is	O
a	O
model	O
that	O
empowers	O
the	O
relational	B-TaskName
reasoning	I-TaskName
across	O
sentences	O
by	O
automatically	O
inducing	O
the	O
latent	O
document	O
-	O
level	O
graph	O
.	O
GAIN	O
(	O
Zeng	O
et	O
al	O
,	O
2020	O
)	O
introduces	O
a	O
path	O
reasoning	O
mechanism	O
based	O
on	O
a	O
heterogeneous	O
mention	O
-	O
level	O
graph	O
and	O
an	O
entity	O
-	O
level	O
graph	O
.	O
ATLOP	O
proposes	O
two	O
techniques	O
,	O
adaptive	O
thresholding	O
and	O
localized	O
context	O
pooling	O
.	O
SSAN	O
(	O
Xu	O
et	O
al	O
,	O
2021	O
)	O
designs	O
several	O
transformations	O
to	O
incorporate	O
mention	O
structural	O
dependencies	O
for	O
document	O
-	O
level	O
relation	B-TaskName
classification	I-TaskName
(	O
DocRC	O
)	O
.	O

To	O
build	O
a	O
dataset	O
toward	O
practical	O
hard	O
cases	O
,	O
we	O
systematically	O
formulate	O
the	O
nine	O
indicators	O
of	O
hard	O
cases	O
(	O
refer	O
to	O
Section	O
3	O
)	O
and	O
introduce	O
measurements	O
to	O
quantify	O
them	O
.	O
For	O
example	O
,	O
we	O
calculate	O
the	O
Argument	O
Distance	O
as	O
the	O
number	O
of	O
tokens	O
between	O
the	O
head	O
and	O
tail	O
entity	O
mentions	O
in	O
the	O
text	O
.	O
More	O
details	O
of	O
feature	B-TaskName
engineering	I-TaskName
are	O
described	O
in	O
Appendix	O
A.	O
After	O
hard	O
case	O
oriented	O
feature	B-TaskName
engineering	I-TaskName
,	O
we	O
discard	O
the	O
instances	O
in	O
D	O
ds	O
without	O
any	O
indicator	O
of	O
hard	O
cases	O
.	O
The	O
remaining	O
part	O
forms	O
a	O
hard	O
case	O
candidate	O
dataset	O
D	O
with	O
about	O
108	O
thousand	O
instances	O
.	O
We	O
randomly	O
sample	O
3	O
,	O
500	O
instances	O
from	O
D	O
and	O
ask	O
experts	O
to	O
select	O
the	O
hard	O
cases	O
given	O
the	O
context	O
and	O
features	O
.	O
Specifically	O
,	O
if	O
an	O
instance	O
with	O
multiple	O
hard	O
case	O
indicators	O
or	O
with	O
only	O
one	O
indicator	O
but	O
selected	O
by	O
all	O
three	O
experts	O
based	O
on	O
their	O
expertise	O
,	O
it	O
is	O
regarded	O
as	O
a	O
hard	O
case	O
.	O
To	O
further	O
evaluate	O
the	O
quality	O
of	O
selected	O
hard	O
cases	O
,	O
we	O
utilize	O
DGCNN	B-MethodName
-	O
BERT	B-MethodName
to	O
test	O
the	O
selected	O
and	O
unselected	O
data	O
.	O
If	O
the	O
F1	B-MetricName
score	I-MetricName
drops	O
δ=10	O
%	O
on	O
the	O
hard	O
cases	O
,	O
we	O
reserve	O
the	O
data	O
to	O
constitute	O
the	O
high	O
quality	O
seeds	B-DatasetName
of	O
hard	O
case	O
D	O
p	O
.	O
The	O
remaining	O
data	O
is	O
easy	O
case	O
D	O
n	O
.	O
In	O
total	O
,	O
we	O
obtain	O
1	O
,	O
431	O
seeds	B-DatasetName
of	O
hard	O
cases	O
.	O

In	O
this	O
section	O
,	O
we	O
analyze	O
various	O
aspects	O
of	O
common	O
RE	O
datasets	O
and	O
HacRED	O
.	O
Data	O
Size	O
.	O
As	O
shown	O
in	O
Table	O
3	O
,	O
HacRED	O
has	O
a	O
greater	O
average	O
number	O
of	O
words	O
,	O
entities	O
,	O
and	O
triples	O
in	O
each	O
text	O
than	O
all	O
of	O
the	O
sentencelevel	O
datasets	O
.	O
Thus	O
we	O
regard	O
HacRED	O
as	O
a	O
document	O
-	O
level	O
RE	O
dataset	O
.	O
Compared	O
with	O
the	O
document	O
-	O
level	O
datasets	O
,	O
DocRED	B-DatasetName
aims	O
at	O
common	O
document	O
-	O
level	O
RE	O
but	O
not	O
consider	O
performance	O
gaps	O
and	O
various	O
hard	O
cases	O
in	O
practical	O
scenarios	O
.	O
BC5CDR	B-DatasetName
is	O
specially	O
designed	O
for	O
biomedical	O
domain	O
.	O
By	O
contrast	O
,	O
we	O
are	O
the	O
first	O
to	O
analyze	O
the	O
performance	O
gap	O
between	O
popular	O
datasets	O
and	O
practical	O
applications	O
,	O
and	O
propose	O
HacRED	O
which	O
focuses	O
on	O
different	O
kinds	O
of	O
hard	O
cases	O
in	O
general	O
domain	O
.	O
Besides	O
,	O
HacRED	O
is	O
larger	O
in	O
scale	O
and	O
contains	O
much	O
more	O
various	O
relational	O
facts	O
than	O
BC5CDR	B-DatasetName
and	O
DocRED	B-DatasetName
but	O
with	O
lower	O
duplicated	O
triples	O
ratio	O
.	O
Data	O
Distribution	O
.	O
We	O
calculate	O
three	O
global	O
statistic	O
metrics	O
about	O
data	O
distribution	O
of	O
common	O
datasets	O
and	O
HacRED	O
.	O
and	O
the	O
relation	O
while	O
low	O
-	O
frequency	O
mentions	O
are	O
neglected	O
.	O
All	O
these	O
three	O
aspects	O
reveal	O
the	O
unreasonable	O
data	O
distribution	O
of	O
common	O
datasets	O
.	O
In	O
comparison	O
,	O
we	O
observe	O
a	O
more	O
reasonable	O
data	O
distribution	O
in	O
HacRED	O
from	O
Table	O
4	O
and	O
Table	O
5	O
.	O
HacRED	O
has	O
a	O
low	O
ratio	O
of	O
duplicate	O
triples	O
and	O
contains	O
various	O
relational	O
facts	O
,	O
which	O
addresses	O
semantic	O
bias	O
.	O
No	O
biased	O
relation	O
existing	O
in	O
HacRED	O
reduces	O
the	O
risk	O
of	O
selection	B-TaskName
bias	I-TaskName
.	O
The	O
proportion	O
of	O
top	O
20	O
%	O
relations	O
promotes	O
the	O
alleviation	O
of	O
relation	O
bias	O
on	O
HacRED	O
.	O
The	O
more	O
comparison	O
of	O
overall	O
data	O
distribution	O
can	O
be	O
found	O
in	O
Appendix	O
E.	O
Data	O
Quality	O
.	O
We	O
evaluate	O
the	O
quality	O
of	O
HacRED	O
through	O
both	O
automatic	O
metrics	O
and	O
human	O
evaluation	O
.	O
Specifically	O
,	O
we	O
first	O
compute	O
the	O
average	O
unit	O
quality	O
score	O
(	O
UQS	O
)	O
,	O
annotation	O
quality	O
score	O
(	O
AQS	O
)	O
,	O
and	O
worker	O
quality	O
score	O
(	O
WQS	O
)	O
of	O
the	O
whole	O
9	O
,	O
231	O
instances	O
.	O
UQS	O
,	O
AQS	O
and	O
WQS	O
are	O
proposed	O
by	O
CrowdTruth2.0	O
(	O
Appendix	O
F	O
provides	O
more	O
calculation	O
details	O
)	O
.	O
The	O
closer	O
these	O
scores	O
are	O
to	O
1	O
,	O
the	O
higher	O
quality	O
of	O
the	O
crowdsourcing	O
is	O
.	O
Meanwhile	O
,	O
we	O
randomly	O
sample	O
400	O
instances	O
from	O
HacRED	O
and	O
compute	O
the	O
precision	O
,	O
recall	O
,	O
and	O
F1	B-MetricName
score	I-MetricName
with	O
annotations	O
based	O
on	O
the	O
revision	O
of	O
humans	O
.	O
The	O
evaluation	O
scores	O
are	O
reported	O
in	O
Table	O
7	O
.	O
From	O
this	O
table	O
,	O
our	O
Ha	O
-	O
cRED	O
achieves	O
a	O
considerable	O
annotation	O
quality	O
.	O
As	O
a	O
comparison	O
,	O
NYT	O
contains	O
about	O
31	O
%	O
noise	O
instances	O
(	O
Riedel	O
et	O
al	O
,	O
2010	O
)	O
and	O
TACRED	B-DatasetName
has	O
poor	O
annotation	O
quality	O
(	O
Alt	O
et	O
al	O
,	O
2020	O
)	O
.	O
Hard	O
Case	O
Types	O
.	O
We	O
group	O
the	O
randomly	O
sampled	O
400	O
instances	O
into	O
nine	O
categories	O
as	O
shown	O
in	O
Table	O
6	O
.	O
The	O
proportions	O
of	O
different	O
kinds	O
of	O
instances	O
reflect	O
that	O
HacRED	O
contains	O
a	O
various	O
range	O
of	O
hard	O
cases	O
,	O
which	O
evaluates	O
models	O
comprehensively	O
for	O
practical	O
applications	O
.	O

As	O
DGCNN	B-MethodName
-	O
BERT	B-MethodName
has	O
been	O
used	O
in	O
the	O
main	O
process	O
of	O
construction	O
,	O
we	O
evaluate	O
other	O
strong	O
RE	O
models	O
including	O
joint	O
RE	O
models	O
,	O
pipeline	O
RE	O
models	O
,	O
and	O
DocRC	O
models	O
on	O
HacRED	O
.	O
First	O
,	O
we	O
limit	O
the	O
relation	O
set	O
within	O
20	O
types	O
both	O
in	O
Ha	O
-	O
cRED	O
and	O
DuIE	O
,	O
and	O
then	O
separate	O
a	O
part	O
of	O
instances	O
in	O
DuIE	O
to	O
form	O
the	O
contrastive	O
easy	O
case	O
dataset	O
D	O
ec	O
.	O
We	O
carry	O
out	O
the	O
equivalent	O
substitution	O
of	O
hard	O
cases	O
in	O
HacRED	O
for	O
easy	O
ones	O
in	O
D	O
ec	O
in	O
different	O
proportions	O
.	O
Figure	O
3	O
shows	O
the	O
F1	B-MetricName
curve	O
of	O
the	O
performances	O
w.r	O
.	O
in	O
performance	O
.	O
The	O
SOTA	O
model	O
CasRel	O
still	O
outperforms	O
other	O
joint	O
models	O
and	O
achieves	O
great	O
F1	B-MetricName
on	O
100	O
%	O
D	O
ec	O
.	O
However	O
,	O
the	O
performance	O
drops	O
on	O
data	O
with	O
more	O
complex	O
instances	O
.	O
We	O
notice	O
that	O
F1	B-MetricName
value	O
of	O
easy	O
cases	O
is	O
generally	O
greater	O
than	O
that	O
of	O
hard	O
cases	O
in	O
different	O
substitution	O
ratio	O
settings	O
,	O
which	O
illustrates	O
that	O
RE	O
models	O
indeed	O
struggle	O
when	O
tackling	O
hard	O
cases	O
.	O
Note	O
that	O
by	O
combining	O
HacRED	O
with	O
easy	O
cases	O
in	O
existing	O
datasets	O
,	O
it	O
is	O
easy	O
to	O
simulate	O
diverse	O
practical	O
scenarios	O
.	O
In	O
addition	O
,	O
we	O
split	O
HacRED	O
into	O
train	O
,	O
dev	O
,	O
and	O
test	O
sets	O
with	O
6231	O
,	O
1500	O
,	O
1500	O
instances	O
respectively	O
.	O
The	O
precision	O
,	O
recall	O
,	O
and	O
F1	B-MetricName
score	I-MetricName
of	O
the	O
three	O
major	O
categories	O
of	O
models	O
are	O
shown	O
in	O
Table	O
8	O
.	O
The	O
joint	O
and	O
pipeline	O
learning	O
strategies	O
do	O
not	O
contribute	O
to	O
a	O
great	O
F1	B-MetricName
on	O
triple	O
extraction	O
.	O
For	O
the	O
NER	B-TaskName
task	O
,	O
PURE	O
has	O
a	O
separate	O
entity	O
model	O
but	O
results	O
in	O
a	O
30.61	O
%	O
F1	B-MetricName
when	O
all	O
entities	O
in	O
a	O
document	O
are	O
considered	O
,	O
including	O
entities	O
with	O
no	O
positive	O
relation	O
labels	O
.	O
This	O
also	O
reflects	O
the	O
challenge	O
to	O
obtain	O
complete	O
entity	O
information	O
in	O
practical	O
scenarios	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
relation	B-TaskName
classification	I-TaskName
performances	O
of	O
DocRC	O
models	O
are	O
far	O
from	O
satisfactory	O
.	O
The	O
results	O
suggest	O
that	O
existing	O
models	O
have	O
remarkably	O
poor	O
performance	O
on	O
HacRED	O
compared	O
with	O
humans	O
(	O
Table	O
9	O
)	O
,	O
which	O
indicates	O
that	O
RE	O
applicable	O
for	O
practical	O
hard	O
cases	O
still	O
requires	O
further	O
research	O
.	O

In	O
this	O
section	O
,	O
we	O
give	O
insight	O
into	O
the	O
abilities	O
of	O
current	O
mainstream	O
joint	O
models	O
when	O
tackling	O
different	O
kinds	O
of	O
hard	O
cases	O
and	O
propose	O
some	O
research	O
indications	O
as	O
well	O
.	O
As	O
it	O
is	O
hard	O
to	O
obtain	O
complete	O
entity	O
information	O
in	O
practical	O
scenarios	O
,	O
we	O
do	O
not	O
consider	O
DocRC	O
models	O
in	O
this	O
section	O
that	O
entity	O
information	O
is	O
provided	O
as	O
input	O
.	O
Multiple	O
Triples	O
.	O
Table	O
11	O
shows	O
the	O
F1	B-MetricName
score	I-MetricName
of	O
existing	O
models	O
when	O
extracting	O
from	O
texts	O
with	O
different	O
number	O
of	O
triples	O
.	O
The	O
performance	O
of	O
NovelTagging	O
and	O
CopyRE	O
decreases	O
as	O
the	O
number	O
of	O
triples	O
increases	O
,	O
which	O
indicates	O
that	O
the	O
novel	O
tagging	O
schema	O
and	O
multiple	O
decoder	O
mechanism	O
are	O
not	O
able	O
to	O
address	O
the	O
challenge	O
of	O
Multiple	O
Triples	O
.	O
Since	O
GraphRel	O
predicts	O
relations	O
for	O
all	O
word	O
pairs	O
and	O
CasRel	O
learns	O
separate	O
entity	O
tagger	O
for	O
different	O
relations	O
,	O
these	O
two	O
models	O
alleviate	O
this	O
problem	O
.	O
An	O
interesting	O
point	O
is	O
that	O
the	O
performance	O
of	O
GraphRel	O
and	O
CasRel	O
rises	O
as	O
the	O
number	O
of	O
triples	O
increases	O
when	O
the	O
triples	O
number	O
is	O
less	O
than	O
16	O
,	O
indicating	O
that	O
these	O
two	O
models	O
work	O
well	O
in	O
texts	O
with	O
number	O
of	O
triples	O
nearing	O
the	O
average	O
.	O
However	O
,	O
all	O
models	O
get	O
F1	B-MetricName
score	I-MetricName
below	O
average	O
when	O
text	O
mentions	O
have	O
more	O
than	O
16	O
triples	O
.	O
BiLSTM	B-MethodName
-	O
based	O
neural	O
models	O
like	O
NovelTagging	O
and	O
CopyRE	O
.	O
The	O
performance	O
improvement	O
on	O
CasRel	O
suggests	O
the	O
powerfulness	O
of	O
BERT	B-MethodName
encoder	O
in	O
the	O
long	O
-	O
distance	O
context	O
.	O
Homogeneous	O
Entities	O
and	O
Similar	O
Relations	O
.	O
Since	O
the	O
text	O
mentions	O
multiple	O
homogeneous	O
entities	O
and	O
semantically	O
similar	O
relations	O
,	O
models	O
are	O
required	O
to	O
distinguish	O
the	O
fine	O
-	O
grained	O
difference	O
of	O
the	O
context	O
to	O
extract	O
the	O
correct	O
triples	O
.	O
The	O
first	O
two	O
columns	O
in	O
Table	O
10	O
have	O
similar	O
results	O
,	O
which	O
indicates	O
that	O
the	O
contexts	O
with	O
homogeneous	O
entities	O
and	O
similar	O
relations	O
are	O
as	O
challenging	O
as	O
the	O
long	O
-	O
distance	O
contexts	O
.	O
Long	O
-	O
tail	O
Relations	O
.	O
We	O
observe	O
a	O
dramatic	O
decrease	O
on	O
the	O
instances	O
with	O
long	O
-	O
tail	O
relational	O
triples	O
.	O
As	O
long	O
-	O
tail	O
relations	O
are	O
common	O
in	O
realworld	O
scenarios	O
,	O
a	O
more	O
efficient	O
learning	O
method	O
is	O
required	O
to	O
make	O
RE	O
models	O
applicable	O
for	O
practical	O
applications	O
.	O
Overlapping	O
Triples	O
.	O
CasRel	O
achieves	O
a	O
better	O
performance	O
on	O
extracting	O
overlapping	O
triples	O
.	O
This	O
proves	O
the	O
effectiveness	O
of	O
cascade	O
binary	O
tagging	O
strategy	O
by	O
first	O
identifying	O
the	O
head	O
mention	O
and	O
then	O
extract	O
the	O
corresponding	O
tail	O
mention	O
given	O
a	O
relation	O
.	O
Specifically	O
,	O
the	O
F1	B-MetricName
scores	O
of	O
overlapping	O
head	O
and	O
tail	O
mentions	O
are	O
66.38	O
%	O
and	O
47.44	O
%	O
respectively	O
.	O
Similarly	O
,	O
results	O
of	O
the	O
two	O
above	O
metrics	O
in	O
CopyRE	O
are	O
13.31	O
%	O
and	O
3.57	O
%	O
.	O
The	O
relative	O
higher	O
performance	O
on	O
overlapping	O
head	O
mentions	O
than	O
tail	O
mentions	O
also	O
suggests	O
that	O
the	O
order	O
of	O
extracting	O
arguments	O
could	O
have	O
effect	O
on	O
the	O
results	O
.	O
Distractor	O
and	O
Reasoning	O
.	O
We	O
manually	O
select	O
instances	O
with	O
Distractor	O
and	O
Reasoning	O
indicators	O
in	O
HacRED	O
because	O
they	O
cooccur	O
frequently	O
in	O
corpus	O
.	O
As	O
illustrated	O
in	O
Table	O
10	O
,	O
we	O
observe	O
a	O
drop	O
of	O
the	O
F1	B-MetricName
.	O
This	O
suggests	O
that	O
models	O
are	O
vulnerable	O
to	O
this	O
kind	O
of	O
instances	O
.	O
However	O
,	O
there	O
are	O
lots	O
of	O
texts	O
with	O
distractions	O
or	O
implicit	O
expression	O
,	O
which	O
needs	O
reasoning	O
,	O
and	O
even	O
common	O
sense	O
.	O
The	O
model	O
design	O
should	O
take	O
the	O
reasoning	O
mechanism	O
into	O
consideration	O
in	O
the	O
future	O
work	O
.	O

A	O
decision	O
tree	O
is	O
learned	O
by	O
the	O
auxilliary	O
features	O
calculated	O
in	O
stage	O
2	O
.	O
For	O
deep	O
models	O
,	O
we	O
concatenate	O
multiple	O
embeddings	O
and	O
auxilliary	O
features	O
to	O
make	O
up	O
the	O
input	O
.	O
We	O
add	O
special	O
tokens	O
to	O
mark	O
the	O
border	O
of	O
each	O
entity	O
and	O
generate	O
the	O
representation	O
vector	O
as	O
recommended	O
in	O
Baldini	O
Soares	O
et	O
al	O
(	O
2019	O
)	O
.	O
We	O
assign	O
a	O
label	O
1	O
to	O
each	O
instance	O
in	O
D	O
p	O
and	O
−1	O
in	O
D	O
n	O
.	O
The	O
deep	O
models	O
output	O
the	O
probability	O
of	O
the	O
instance	O
belonging	O
to	O
hard	O
cases	O
and	O
are	O
optimized	O
with	O
the	O
binary	O
cross	O
entropy	O
loss	B-MetricName
objective	O
.	O
To	O
start	O
PU	O
learning	O
,	O
we	O
sample	O
from	O
D	O
to	O
form	O
a	O
unlabeled	O
dataset	O
D	O
u	O
and	O
set	O
the	O
hyperparameter	O
π	O
p	O
=	O
0.41	O
estimated	O
by	O
the	O
proportion	O
of	O
hard	O
cases	O
selected	O
by	O
experts	O
.	O
We	O
implement	O
nnPU	O
(	O
Kiryo	O
et	O
al	O
,	O
2017	O
)	O
which	O
is	O
efficient	O
for	O
massive	O
data	O
and	O
deep	O
learning	O
and	O
use	O
J	O
nnpu	O
as	O
the	O
optimized	O
objective	O
,	O
J	O
nnpu	O
=	O
π	O
p	O
E	O
p	O
(	O
x	O
|	O
y=1	O
)	O
[	O
l	O
(	O
g	O
(	O
x	O
)	O
)	O
]	O
+	O
max	O
{	O
0	B-DatasetName
,	O
E	O
p	O
(	O
x	O
)	O
[	O
l	O
(	O
−g	O
(	O
x	O
)	O
)	O
]	O
−	O
π	O
p	O
E	O
p	O
(	O
x	O
|	O
y=1	O
)	O
[	O
l	O
(	O
−g	O
(	O
x	O
)	O
)	O
]	O
}	O
(	O
1	O
)	O
where	O
π	O
p	O
=	O
p	O
(	O
y	O
=	O
1	O
)	O
,	O
g	O
is	O
decision	O
function	O
,	O
l	O
is	O
surrogate	O
loss	B-MetricName
function	O
.	O
We	O
choose	O
the	O
double	O
hinge	O
loss	B-MetricName
l	O
=	O
max	O
(	O
−z	O
,	O
max	O
(	O
0	B-DatasetName
,	O
1	O
2	O
−	O
1	O
2	O
z	O
)	O
)	O
proposed	O
by	O
(	O
du	O
Plessis	O
et	O
al	O
,	O
2015	O
)	O
.	O

Broadly	O
speaking	O
,	O
the	O
dialog	O
belief	O
tracking	O
algorithms	O
can	O
be	O
divided	O
into	O
three	O
families	O
:	O
1	O
)	O
hand	O
-	O
crafted	O
rules	O
2	O
)	O
generative	O
models	O
,	O
and	O
3	O
)	O
maximum	O
-	O
entropy	O
model	O
(	O
Metallinou	O
et	O
al	O
,	O
2013	O
)	O
.	O
Later	O
on	O
,	O
many	O
deep	O
learning	O
based	O
discriminative	O
models	O
have	O
surged	O
to	O
replace	O
the	O
traditional	O
strategies	O
(	O
Henderson	O
et	O
al	O
,	O
2014a	O
;	O
Williams	O
et	O
al	O
,	O
2016	O
)	O
and	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
various	O
datasets	O
.	O
Though	O
the	O
discriminative	O
models	O
are	O
reported	O
to	O
achieve	O
fairly	O
high	O
accuracy	B-MetricName
,	O
their	O
applications	O
are	O
heavily	O
restricted	O
by	O
the	O
domain	O
,	O
ontology	B-MethodName
,	O
and	O
language	O
.	O
Recently	O
,	O
a	O
pointer	B-MethodName
network	I-MethodName
based	O
algorithm	O
(	O
Xu	O
and	O
Hu	O
,	O
2018	O
)	O
and	O
another	O
multi	O
-	O
domain	O
algorithm	O
(	O
Rastogi	O
et	O
al	O
,	O
2017	O
)	O
have	O
been	O
proposed	O
to	O
break	O
the	O
ontology	B-MethodName
and	O
domain	O
boundary	O
.	O
Besides	O
,	O
(	O
Mrkšić	O
et	O
al	O
,	O
2017	O
)	O
has	O
proposed	O
an	O
algorithm	O
to	O
train	O
a	O
unified	O
framework	O
to	O
deal	O
with	O
multiple	O
languages	O
with	O
annotated	O
datasets	O
.	O
In	O
contrast	O
,	O
our	O
paper	O
focuses	O
on	O
breaking	O
the	O
language	O
boundary	O
and	O
transfer	O
DST	O
knowledge	O
from	O
one	O
language	O
into	O
other	O
zeroannotation	O
languages	O
.	O

We	O
design	O
our	O
cross	O
-	O
lingual	O
DST	O
on	O
top	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
Neural	O
Belief	O
Tracker	O
(	O
NBT	O
)	O
,	O
which	O
demonstrates	O
many	O
advantages	O
(	O
no	O
hand	O
-	O
crafted	O
lexicons	O
,	O
no	O
linguistic	O
knowledge	O
required	O
,	O
etc	O
)	O
.	O
These	O
nice	O
properties	O
are	O
essential	O
for	O
our	O
cross	O
-	O
lingual	O
DST	O
design	O
because	O
we	O
are	O
pursuing	O
a	O
general	O
and	O
simple	O
framework	O
regardless	O
of	O
the	O
language	O
properties	O
.	O
In	O
short	O
,	O
NBT	O
consists	O
of	O
a	O
neural	O
network	O
that	O
computes	O
the	O
matching	O
score	O
for	O
every	O
candidate	O
slot	O
-	O
value	O
pair	O
(	O
c	O
s	O
,	O
c	O
v	O
)	O
based	O
on	O
the	O
following	O
three	O
inputs	O
:	O
(	O
i	O
)	O
the	O
system	O
dialog	O
acts	O
a	O
t	O
=	O
(	O
t	O
q	O
,	O
t	O
s	O
,	O
t	O
v	O
)	O
,	O
2	O
(	O
ii	O
)	O
the	O
user	O
utterance	O
u	O
t	O
,	O
and	O
(	O
iii	O
)	O
the	O
candidate	O
slot	O
-	O
value	O
pair	O
.	O
And	O
it	O
identifies	O
the	O
user	O
intents	O
by	O
evaluating	O
the	O
scores	O
for	O
all	O
the	O
slot	O
-	O
value	O
pairs	O
(	O
see	O
Figure	O
3	O
)	O
.	O
With	O
a	O
slight	O
abuse	O
of	O
notation	O
,	O
we	O
still	O
use	O
c	O
s	O
,	O
c	O
v	O
,	O
t	O
s	O
,	O
t	O
v	O
,	O
t	O
q	O
R	O
H	O
to	O
denote	O
the	O
vector	O
representations	O
of	O
themselves	O
,	O
where	O
H	O
is	O
the	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
.	O
We	O
will	O
use	O
pre	O
-	O
trained	O
embedding	O
vectors	O
in	O
our	O
cross	O
-	O
lingual	O
NBT	O
,	O
just	O
like	O
the	O
original	O
NBT	O
and	O
they	O
will	O
be	O
fixed	O
during	O
training	O
.	O
To	O
enable	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
learning	O
,	O
we	O
first	O
re	O
-	O
interpret	O
the	O
architecture	O
of	O
the	O
original	O
NBT	O
by	O
decomposing	O
it	O
into	O
three	O
components	O
:	O
Utterance	O
Encoding	O
The	O
first	O
component	O
is	O
an	O
utterance	O
encoder	O
,	O
which	O
maps	O
the	O
utterance	O
u	O
t	O
=	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
,	O
w	O
N	O
}	O
of	O
a	O
particular	O
language	O
into	O
a	O
semantic	O
representation	O
vector	O
r	O
(	O
u	O
t	O
)	O
R	O
H	O
,	O
where	O
w	O
i	O
R	O
H	O
is	O
the	O
word	O
vector	O
for	O
the	O
i	O
-	O
th	O
token	O
and	O
N	O
is	O
the	O
length	O
of	O
the	O
utterance	O
.	O
Note	O
that	O
the	O
dimension	O
of	O
the	O
semantic	O
vector	O
r	O
(	O
u	O
t	O
)	O
is	O
the	O
same	O
as	O
that	O
of	O
the	O
word	O
vector	O
.	O
We	O
implement	O
.	O
the	O
encoder	O
using	O
the	O
same	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
as	O
the	O
original	O
NBT	O
,	O
with	O
a	O
slight	O
modification	O
of	O
adding	O
a	O
top	O
batch	B-MethodName
normalization	I-MethodName
layer	O
.	O
We	O
will	O
explain	O
this	O
change	O
in	O
section	O
5	O
.	O

We	O
are	O
given	O
a	O
well	O
-	O
trained	O
NBT	O
for	O
a	O
source	O
language	O
e	O
,	O
and	O
we	O
want	O
to	O
learn	O
an	O
NBT	O
for	O
a	O
target	O
language	O
f	O
without	O
any	O
annotated	O
training	O
data	O
.	O
Therefore	O
,	O
we	O
can	O
not	O
learn	O
the	O
target	O
-	O
side	O
NBT	O
from	O
standard	O
supervised	O
learning	O
.	O
Instead	O
,	O
we	O
use	O
a	O
teacher	O
-	O
student	O
framework	O
to	O
distill	O
the	O
knowledge	O
from	O
the	O
source	O
-	O
side	O
NBT	O
(	O
teacher	O
network	O
)	O
into	O
the	O
target	O
-	O
side	O
NBT	O
(	O
student	O
network	O
)	O
(	O
see	O
Figure	O
4	O
)	O
.	O
Let	O
x	O
e	O
(	O
c	O
e	O
s	O
,	O
c	O
e	O
v	O
,	O
u	O
e	O
t	O
,	O
a	O
e	O
t	O
)	O
be	O
the	O
input	O
to	O
the	O
teacher	O
network	O
and	O
let	O
x	O
f	O
(	O
c	O
f	O
s	O
,	O
c	O
f	O
v	O
,	O
u	O
f	O
t	O
,	O
a	O
f	O
t	O
)	O
be	O
the	O
associated	O
input	O
to	O
the	O
student	O
network	O
.	O
The	O
standard	O
teacher	O
-	O
student	O
framework	O
trains	O
the	O
student	O
network	O
by	O
minimizinĝ	O
)	O
.	O
However	O
,	O
the	O
target	O
-	O
side	O
inputs	O
(	O
c	O
f	O
s	O
,	O
c	O
f	O
v	O
,	O
u	O
f	O
t	O
,	O
a	O
f	O
t	O
)	O
parallel	O
to	O
(	O
c	O
e	O
s	O
,	O
c	O
e	O
v	O
,	O
u	O
e	O
t	O
,	O
a	O
e	O
t	O
)	O
are	O
usually	O
not	O
available	O
in	O
crosslingual	O
DST	O
,	O
and	O
,	O
even	O
worse	O
,	O
the	O
target	O
-	O
side	O
utterance	O
u	O
e	O
t	O
is	O
not	O
available	O
.	O
We	O
may	O
have	O
to	O
generate	O
synthetic	O
input	O
data	O
for	O
the	O
student	O
network	O
or	O
leverage	O
external	O
data	O
sources	O
.	O
It	O
is	O
relatively	O
easy	O
to	O
use	O
the	O
mapping	O
M	O
(	O
)	O
to	O
generate	O
(	O
c	O
f	O
s	O
,	O
c	O
f	O
v	O
,	O
a	O
f	O
t	O
)	O
)	O
(	O
i.e.	O
,	O
the	O
inputs	O
of	O
the	O
target	O
-	O
side	O
context	O
gate	O
)	O
from	O
the	O
(	O
c	O
e	O
s	O
,	O
c	O
e	O
v	O
,	O
a	O
e	O
t	O
)	O
.	O
But	O
it	O
is	O
more	O
challenging	O
to	O
obtain	O
the	O
parallel	O
utterance	O
data	O
u	O
f	O
t	O
from	O
u	O
e	O
t	O
)	O
.	O
Therefore	O
,	O
we	O
have	O
to	O
leverage	O
external	O
bilingual	O
data	O
sources	O
to	O
alleviate	O
the	O
problem	O
.	O
However	O
,	O
the	O
external	O
bilingual	O
data	O
are	O
usually	O
not	O
in	O
the	O
same	O
domain	O
as	O
the	O
utterance	O
,	O
and	O
hence	O
they	O
are	O
not	O
aligned	O
with	O
the	O
slot	O
-	O
value	O
pair	O
and	O
system	O
acts	O
(	O
i.e.	O
,	O
(	O
c	O
e	O
s	O
,	O
c	O
e	O
v	O
,	O
a	O
e	O
t	O
)	O
or	O
(	O
c	O
f	O
s	O
,	O
c	O
f	O
v	O
,	O
a	O
f	O
t	O
)	O
)	O
.	O
For	O
this	O
reason	O
,	O
we	O
can	O
not	O
perform	O
the	O
knowledge	O
transfer	O
by	O
minimizing	O
the	O
cost	O
(	O
5	O
)	O
.	O
Instead	O
,	O
we	O
need	O
to	O
develop	O
a	O
new	O
cost	O
function	O
where	O
the	O
utterance	O
is	O
not	O
required	O
to	O
be	O
aligned	O
with	O
the	O
slot	O
-	O
value	O
pair	O
and	O
the	O
system	O
acts	O
.	O
To	O
this	O
end	O
,	O
let	O
g	O
e	O
=	O
g	O
e	O
(	O
c	O
e	O
s	O
,	O
c	O
e	O
v	O
,	O
a	O
e	O
t	O
)	O
and	O
g	O
J1	O
=	O
xe	O
,	O
x	O
f	O
|	O
|	O
y	O
(	O
c	O
e	O
s	O
,	O
c	O
e	O
v	O
,	O
u	O
e	O
t	O
,	O
a	O
e	O
t	O
)	O
−	O
y	O
(	O
c	O
f	O
s	O
,	O
c	O
f	O
v	O
,	O
u	O
f	O
t	O
,	O
a	O
f	O
t	O
)	O
|	O
|	O
2	O
(	O
f	O
=	O
g	O
f	O
(	O
c	O
f	O
s	O
,	O
c	O
f	O
v	O
,	O
a	O
f	O
t	O
)	O
.	O
And	O
we	O
substitute	O
(	O
3	O
)	O
into	O
(	O
5	O
)	O
and	O
get	O
:	O
J1	O
≤	O
|	O
|	O
Wy	O
|	O
|	O
2	O
c	O
f	O
v	O
,	O
c	O
e	O
v	O
|	O
|	O
re	O
g	O
e	O
−	O
r	O
f	O
g	O
f	O
|	O
|	O
2	O
=	O
|	O
|	O
Wy	O
|	O
|	O
2	O
c	O
f	O
v	O
,	O
c	O
e	O
v	O
|	O
|	O
g	O
e	O
(	O
re	O
−	O
r	O
f	O
)	O
+	O
r	O
f	O
(	O
g	O
e	O
−	O
g	O
f	O
)	O
|	O
|	O
2	O
≤	O
|	O
|	O
Wy	O
|	O
|	O
2	O
c	O
f	O
v	O
,	O
c	O
e	O
v	O
|	O
|	O
g	O
e	O
|	O
|	O
2	O
|	O
|	O
re	O
−	O
r	O
f	O
|	O
|	O
2	O
+	O
|	O
|	O
r	O
f	O
|	O
|	O
2	O
|	O
|	O
g	O
e	O
−	O
g	O
f	O
|	O
|	O
2	O
where	O
r	O
e	O
=	O
r	O
e	O
(	O
u	O
e	O
t	O
)	O
and	O
r	O
f	O
=	O
r	O
f	O
(	O
u	O
f	O
t	O
)	O
.	O
As	O
we	O
mentioned	O
earlier	O
,	O
the	O
weight	O
W	O
y	O
in	O
the	O
slotvalue	O
decoder	O
is	O
shared	O
between	O
the	O
student	O
and	O
the	O
teacher	O
networks	O
and	O
will	O
not	O
be	O
updated	O
.	O
The	O
teacher	O
-	O
student	O
optimization	O
only	O
adjusts	O
the	O
weights	O
related	O
to	O
the	O
language	O
-	O
specific	O
parts	O
in	O
Figure	O
3	O
(	O
i.e.	O
,	O
utterance	O
encoding	O
and	O
context	O
gating	O
)	O
.	O
Therefore	O
,	O
the	O
shared	O
weight	O
|	O
|	O
W	O
y	O
|	O
|	O
is	O
seen	O
as	O
a	O
constant	O
.	O
Furthermore	O
,	O
c	O
f	O
v	O
,	O
c	O
e	O
v	O
|	O
|	O
g	O
e	O
|	O
|	O
2	O
can	O
be	O
seen	O
as	O
a	O
constant	O
since	O
the	O
teacher	O
gate	O
is	O
fixed	O
.	O
Since	O
we	O
use	O
batch	B-MethodName
normalization	I-MethodName
layer	O
to	O
normalize	O
the	O
encoder	O
output	O
(	O
described	O
in	O
Figure	O
3	O
)	O
,	O
|	O
|	O
r	O
f	O
(	O
u	O
f	O
t	O
)	O
|	O
|	O
2	O
can	O
also	O
be	O
treated	O
as	O
a	O
constant	O
C	O
2	O
.	O
Therefore	O
,	O
we	O
formally	O
write	O
the	O
upper	O
bound	O
of	O
J	O
1	O
as	O
our	O
surrogate	O
cost	O
function	O
J	O
:	O
J	O
=	O
C1	O
|	O
|	O
re	O
(	O
u	O
e	O
t	O
)	O
−	O
r	O
f	O
(	O
u	O
f	O
t	O
)	O
|	O
|	O
2	O
+	O
C2	O
c	O
f	O
v	O
,	O
c	O
e	O
v	O
|	O
|	O
g	O
e	O
−	O
g	O
f	O
|	O
|	O
2	O
(	O
6	O
)	O
The	O
surrogate	O
cost	O
has	O
successfully	O
decoupled	O
utterance	O
encoder	O
with	O
context	O
gate	O
,	O
and	O
we	O
use	O
J	O
r	O
and	O
J	O
g	O
to	O
measure	O
the	O
encoder	O
matching	O
cost	O
and	O
the	O
gate	O
matching	O
cost	O
,	O
respectively	O
.	O
Jr	O
=	O
|	O
|	O
re	O
(	O
u	O
e	O
t	O
)	O
−	O
r	O
f	O
(	O
u	O
f	O
t	O
)	O
|	O
|	O
2	O
Jg	O
=	O
c	O
f	O
v	O
,	O
c	O
e	O
v	O
|	O
|	O
g	O
e	O
−	O
g	O
f	O
|	O
|	O
2	O
(	O
7	O
)	O
The	O
encoder	O
cost	O
J	O
r	O
is	O
optimized	O
to	O
distill	O
the	O
knowledge	O
from	O
the	O
teacher	O
encoder	O
to	O
student	O
encoder	O
while	O
gate	O
cost	O
J	O
g	O
is	O
optimized	O
to	O
distill	O
the	O
knowledge	O
from	O
teacher	O
gate	O
to	O
student	O
gate	O
.	O
This	O
objective	O
function	O
successfully	O
decouples	O
the	O
optimization	O
of	O
encoder	O
and	O
gate	O
,	O
thus	O
we	O
are	O
able	O
to	O
optimize	O
J	O
r	O
and	O
J	O
g	O
separately	O
from	O
different	O
data	O
sources	O
.	O
Recall	B-MetricName
that	O
we	O
can	O
easily	O
simulate	O
the	O
target	O
-	O
side	O
system	O
acts	O
,	O
slot	O
-	O
value	O
pairs	O
(	O
c	O
f	O
s	O
,	O
c	O
f	O
v	O
,	O
a	O
f	O
)	O
by	O
using	O
the	O
ontology	B-MethodName
mapping	O
M	O
.	O
Therefore	O
,	O
optimizing	O
J	O
g	O
is	O
relatively	O
easy	O
.	O
Formally	O
,	O
we	O
write	O
the	O
gate	O
matching	O
cost	O
as	O
follows	O
:	O
However	O
,	O
exact	O
optimization	O
of	O
J	O
r	O
is	O
difficult	O
and	O
we	O
have	O
to	O
approximate	O
it	O
using	O
external	O
parallel	O
data	O
.	O
We	O
consider	O
two	O
kinds	O
of	O
external	O
resources	O
(	O
bilingual	O
corpus	O
and	O
bilingual	O
dictionary	O
)	O
in	O
the	O
sections	O
5.2	O
-	O
5.3	O
(	O
see	O
Figure	O
5	O
for	O
the	O
main	O
idea	O
)	O
.	O
Jg	O
=	O

In	O
our	O
first	O
scenario	O
,	O
we	O
assume	O
there	O
exists	O
a	O
parallel	O
corpus	O
D	O
p	O
consisting	O
of	O
sentence	O
pairs	O
from	O
the	O
source	O
language	O
and	O
the	O
target	O
language	O
.	O
In	O
this	O
case	O
,	O
the	O
cost	O
function	O
(	O
6	O
)	O
is	O
approximated	O
by	O
J	O
=	O
E	O
(	O
me	O
,	O
m	O
f	O
)	O
Dp	O
|	O
|	O
re	O
(	O
me	O
)	O
−	O
r	O
f	O
(	O
m	O
f	O
)	O
|	O
|	O
2	O
+	O
αJg	O
(	O
9	O
)	O
where	O
α	B-HyperparameterName
is	O
the	O
balancing	O
factor	O
and	O
J	O
g	O
is	O
defined	O
in	O
(	O
6	O
)	O
.	O
The	O
cost	O
function	O
(	O
9	O
)	O
is	O
minimized	O
by	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
.	O
At	O
test	O
time	O
,	O
we	O
switch	O
the	O
encoder	O
to	O
receive	O
target	O
language	O
inputs	O
.	O

In	O
the	O
second	O
scenario	O
,	O
we	O
assume	O
there	O
exists	O
no	O
parallel	O
corpus	O
but	O
a	O
bilingual	O
dictionary	O
D	O
B	O
that	O
defines	O
the	O
correspondence	O
between	O
source	O
words	O
and	O
target	O
words	O
(	O
a	O
one	O
-	O
to	O
-	O
many	O
mapping	O
{	O
w	O
:	O
M	O
D	O
(	O
w	O
)	O
}	O
)	O
.	O
Likewise	O
,	O
it	O
is	O
infeasible	O
to	O
optimize	O
the	O
exact	O
encoder	O
cost	O
J	O
r	O
due	O
to	O
the	O
lack	O
of	O
target	O
-	O
side	O
utterances	O
.	O
We	O
propose	O
a	O
word	O
replacement	O
strategy	O
(	O
to	O
be	O
described	O
later	O
)	O
to	O
generate	O
synthetic	O
parallel	O
sentenceû	O
f	O
t	O
of	O
"	O
mixed	O
"	O
language	O
.	O
Then	O
,	O
we	O
use	O
the	O
generated	O
target	O
parallel	O
sentences	O
to	O
approximate	O
the	O
cost	O
(	O
6	O
)	O
by	O
Jr	O
=	O
E	O
u	O
t	O
D	O
|	O
|	O
re	O
(	O
u	O
e	O
t	O
)	O
−	O
r	O
f	O
(	O
û	O
f	O
t	O
)	O
|	O
|	O
2	O
+	O
αJg	O
(	O
10	O
)	O
where	O
α	B-HyperparameterName
is	O
the	O
balancing	O
factor	O
.	O
For	O
word	O
replacement	O
,	O
we	O
first	O
decide	O
the	O
number	O
of	O
words	O
N	O
w	O
to	O
be	O
replaced	O
,	O
then	O
we	O
draw	O
N	O
w	O
positions	O
randomly	O
from	O
the	O
source	O
utterance	O
and	O
substitute	O
the	O
corresponding	O
word	O
w	O
i	O
with	O
their	O
target	O
word	O
synonym	O
from	O
M	O
D	O
(	O
w	O
)	O
based	O
on	O
the	O
context	O
as	O
follows	O
:	O
jp	O
(	O
Nw	O
=	O
i	O
)	O
=	O
exp	O
(	O
−i	O
/	O
τ	O
)	O
i	O
<	O
N	O
exp	O
(	O
−i	O
/τ	O
)	O
p	O
(	O
ŵ	O
)	O
=	O
ŵ	O
hŵ	O
w	O
M	O
(	O
w	O
i	O
)	O
w	O
hŵ	O
(	O
11	O
)	O
where	O
hŵ	O
=	O
2	O
k=−2	O
:	O
k	B-HyperparameterName
=	I-HyperparameterName
0	B-DatasetName
w	O
i+k	O
represents	O
the	O
context	O
vector	O
and	O
N	O
denotes	O
the	O
utterance	O
length	O
.	O
The	O
context	O
similarity	O
of	O
context	O
and	O
the	O
targetside	O
synonym	O
can	O
better	O
help	O
us	O
in	O
choosing	O
the	O
most	O
appropriate	O
candidate	O
from	O
the	O
list	O
.	O
In	O
our	O
following	O
experiments	O
,	O
we	O
adjust	O
the	O
temperature	O
of	O
τ	O
to	O
control	O
the	O
aggressiveness	O
of	O
replacement	O
.	O

Here	O
we	O
highlight	O
the	O
baselines	O
we	O
use	O
to	O
compare	O
with	O
our	O
cross	O
-	O
lingual	O
algorithm	O
as	O
follows	O
:	O
(	O
1	O
)	O
Supervised	B-DatasetName
:	I-DatasetName
this	O
baseline	O
algorithm	O
assumes	O
the	O
existence	O
of	O
annotated	O
dialog	O
belief	O
tracking	O
datasets	O
,	O
and	O
it	O
determines	O
the	O
upper	O
bound	O
of	O
the	O
DST	O
model	O
.	O
(	O
2	O
)	O
w/o	O
Transfer	O
:	O
this	O
algorithm	O
trains	O
an	O
English	O
NBT	O
,	O
and	O
then	O
directly	O
feeds	O
target	O
language	O
into	O
the	O
embedding	O
level	O
as	O
inputs	O
during	O
test	O
time	O
to	O
evaluate	O
the	O
performance	O
.	O
(	O
3	O
)	O
Ontology	B-MethodName
-	O
match	O
:	O
this	O
algorithm	O
directly	O
uses	O
exact	O
string	O
matching	O
against	O
the	O
utterance	O
to	O
discover	O
the	O
perceived	O
slot	O
-	O
value	O
pairs	O
,	O
it	O
directly	O
assigns	O
a	O
high	O
score	O
to	O
the	O
appearing	O
candidates	O
.	O
(	O
4	O
)	O
Translation	B-TaskName
-	O
based	O
:	O
this	O
system	O
pre	O
-	O
trains	O
a	O
translator	O
on	O
the	O
external	O
bilingual	O
corpus	O
and	O
then	O
translates	O
the	O
English	O
dialog	O
and	O
ontology	B-MethodName
into	O
target	O
language	O
as	O
"	O
annotated	O
"	O
data	O
,	O
which	O
is	O
used	O
to	O
train	O
the	O
NBT	O
in	O
the	O
target	O
language	O
domain	O
(	O
more	O
details	O
about	O
the	O
implementation	O
,	O
performance	O
and	O
examples	O
are	O
listed	O
in	O
the	O
appendix	O
)	O
.	O
(	O
5	O
)	O
Word	O
-	O
By	O
-	O
Word	O
(	O
WBW	O
)	O
:	O
this	O
system	O
transforms	O
the	O
English	O
dialog	O
corpus	O
into	O
target	O
language	O
word	O
by	O
word	O
using	O
the	O
bilingual	O
dictionary	O
,	O
which	O
is	O
used	O
to	O
train	O
the	O
NBT	O
in	O
target	O
side	O
.	O
We	O
demonstrate	O
the	O
results	O
for	O
our	O
proposed	O
algorithms	O
and	O
other	O
competing	O
algorithms	O
in	O
Table	O
2	O
,	O
from	O
which	O
we	O
can	O
easily	O
conclude	O
that	O
that	O
(	O
i	O
)	O
our	O
Decoupled	O
NBT	O
does	O
not	O
affect	O
the	O
performance	O
,	O
and	O
(	O
ii	O
)	O
our	O
cross	O
-	O
lingual	O
NBT	O
framework	O
is	O
able	O
to	O
achieve	O
significantly	O
better	O
accuracy	B-MetricName
for	O
both	O
languages	O
in	O
both	O
parallel	O
-	O
resource	O
scenarios	O
.	O
Compare	O
with	O
Translator	O
/	O
WBW	O
.	O
With	O
bilingual	O
corpus	O
,	O
XL	O
-	O
NBT	O
-	O
C	O
with	O
pre	O
-	O
trained	O
bilingual	O
embedding	O
can	O
significantly	O
outperform	O
our	O
Translator	O
baseline	O
(	O
Klein	O
et	O
al	O
,	O
2017	O
)	O
.	O
This	O
is	O
intuitive	O
because	O
the	O
translation	O
model	O
requires	O
both	O
source	O
-	O
side	O
encoding	O
and	O
target	O
-	O
side	O
wordby	O
-	O
word	O
decoding	O
,	O
while	O
our	O
XL	O
-	O
NBT	O
only	O
needs	O
a	O
bilingual	O
source	O
-	O
encoding	O
to	O
align	O
two	O
vector	O
space	O
,	O
which	O
averts	O
the	O
compounded	O
decoding	O
errors	O
.	O
With	O
the	O
bilingual	O
dictionary	O
,	O
the	O
word	O
-	O
byword	O
translator	O
is	O
very	O
weak	O
and	O
leading	O
to	O
many	O
broken	O
target	O
sentences	O
,	O
which	O
poses	O
challenges	O
for	O
DST	O
training	O
.	O
In	O
comparison	O
,	O
our	O
XL	O
-	O
NBT	O
-	O
D	O
can	O
control	O
the	O
replacement	O
by	O
adjusting	O
its	O
temperature	O
to	O
maintain	O
the	O
stability	O
of	O
utterance	O
representation	O
.	O
Furthermore	O
,	O
for	O
both	O
cases	O
,	O
our	O
teacher	O
-	O
student	O
framework	O
can	O
make	O
use	O
of	O
the	O
knowledge	O
learned	O
in	O
source	O
-	O
side	O
NBT	O
to	O
assist	O
its	O
decision	B-TaskName
making	I-TaskName
,	O
while	O
translator	O
-	O
based	O
methods	O
learn	O
from	O
scratch	O
.	O

From	O
the	O
table	O
,	O
we	O
can	O
easily	O
observe	O
that	O
bilingual	O
corpus	O
is	O
obviously	O
a	O
more	O
informative	O
parallel	O
resource	O
to	O
perform	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
learning	O
.	O
The	O
accuracy	B-MetricName
of	O
XL	O
-	O
NBT	O
-	O
D	O
is	O
lower	O
than	O
XL	O
-	O
NBT	O
-	O
C.	O
We	O
conjecture	O
that	O
our	O
replace	O
-	O
ment	O
strategy	O
to	O
generate	O
"	O
mixed	O
"	O
language	O
utterance	O
can	O
sometimes	O
break	O
the	O
semantic	O
coherence	O
and	O
cause	O
additional	O
noises	O
during	O
the	O
transfer	O
process	O
,	O
which	O
remarkably	O
degrades	O
the	O
DST	O
performance	O
.	O
Monolingual	O
vs.	O
Bilingual	O
embedding	O
.	O
From	O
the	O
table	O
,	O
we	O
can	O
observe	O
that	O
the	O
bilingual	O
embedding	O
and	O
monolingual	O
embedding	O
does	O
not	O
make	O
much	O
difference	O
in	O
supervised	O
training	O
.	O
However	O
,	O
the	O
gap	O
in	O
the	O
bilingual	O
corpus	O
case	O
is	O
quite	O
obvious	O
.	O
Monolingual	O
embedding	O
even	O
causes	O
the	O
transfer	O
to	O
fail	O
in	O
a	O
bilingual	O
dictionary	O
case	O
.	O
We	O
conjecture	O
that	O
the	O
bilingual	O
word	O
embedding	O
already	O
contain	O
many	O
alignment	O
information	O
between	O
two	O
languages	O
,	O
which	O
largely	O
eases	O
the	O
training	O
of	O
encoder	O
matching	O
objective	O
.	O
German	O
vs.	O
Italian	O
As	O
can	O
be	O
seen	O
,	O
the	O
transfer	B-TaskName
learning	I-TaskName
results	O
for	O
Italian	O
are	O
remarkably	O
higher	O
than	O
German	O
,	O
especially	O
for	O
the	O
"	O
Goal	O
"	O
accuracy	B-MetricName
.	O
We	O
conjecture	O
that	O
it	O
is	O
due	O
to	O
German	O
declension	O
,	O
which	O
can	O
produce	O
many	O
word	O
forms	O
.	O
The	O
very	O
diverse	O
word	O
forms	O
present	O
great	O
challenges	O
for	O
DST	O
to	O
understand	O
its	O
intention	O
behind	O
.	O
Especially	O
for	O
the	O
bilingual	O
dictionary	O
,	O
German	O
tends	O
to	O
have	O
much	O
longer	O
replacement	O
candidate	O
lists	O
than	O
Italian	O
,	O
which	O
introduces	O
more	O
noises	O
to	O
the	O
replacement	O
procedure	O
.	O
Error	B-MetricName
Analysis	O
Here	O
we	O
showcase	O
the	O
most	O
frequent	O
error	O
types	O
in	O
subsection	O
6.1	O
.	O
From	O
our	O
observation	O
,	O
these	O
three	O
types	O
of	O
errors	O
distribute	O
evenly	O
in	O
the	O
test	O
dialogs	O
.	O
The	O
error	O
mainly	O
comes	O
from	O
the	O
unaligned	O
utterance	O
space	O
,	O
which	O
leads	O
to	O
failure	O
in	O
understanding	O
the	O
intention	O
of	O
human	O
utterance	O
in	O
the	O
target	O
language	O
.	O
This	O
can	O
lead	O
the	O
system	O
to	O
fail	O
in	O
modifying	O
the	O
dialog	O
state	O
or	O
maintaining	O
the	O
previous	O
dialog	O
states	O
.	O

Here	O
we	O
want	O
to	O
further	O
highlight	O
the	O
comparison	O
between	O
our	O
transfer	B-TaskName
learning	I-TaskName
algorithm	O
with	O
the	O
MT	O
-	O
based	O
approach	O
.	O
Though	O
our	O
approach	O
outperforms	O
the	O
standard	O
Translator	O
trained	O
on	O
IWSLT	O
-	O
2014	O
,	O
it	O
does	O
not	O
necessarily	O
claim	O
that	O
our	O
transfer	O
algorithm	O
outperforms	O
any	O
translation	O
methods	O
on	O
any	O
parallel	O
corpus	O
.	O
In	O
our	O
further	O
ablation	O
studies	O
,	O
we	O
found	O
that	O
using	O
Google	B-DatasetName
Translator	O
6	O
can	O
actually	O
achieve	O
a	O
better	O
score	O
than	O
our	O
transfer	O
algorithm	O
,	O
which	O
is	O
understandable	O
considering	O
the	O
complexity	O
of	O
Google	B-DatasetName
Translator	O
and	O
the	O
much	O
larger	O
parallel	O
corpus	O
it	O
leverages	O
.	O
By	O
leveraging	O
more	O
close	O
-	O
to	O
-	O
domain	O
corpus	O
and	O
comprehensive	O
entity	O
recognition	O
/	O
replacement	O
strategy	O
,	O
the	O
translator	O
model	O
is	O
able	O
to	O
achieve	O
a	O
higher	O
score	O
.	O
Apparently	O
,	O
we	O
need	O
to	O
trade	O
off	O
the	O
efficiency	O
for	O
the	O
accuracy	B-MetricName
.	O
For	O
DST	O
problem	O
,	O
it	O
is	O
an	O
overkill	O
to	O
introduce	O
a	O
more	O
complex	O
translation	O
algorithm	O
,	O
what	O
we	O
pursue	O
is	O
a	O
simple	O
yet	O
efficient	O
algorithm	O
to	O
achieve	O
promising	O
scores	O
.	O
It	O
is	O
also	O
worth	O
mentioning	O
that	O
our	O
XL	O
-	O
NBT	O
algorithm	O
only	O
takes	O
several	O
hours	O
to	O
achieve	O
the	O
reported	O
score	O
,	O
while	O
the	O
translator	O
model	O
takes	O
much	O
more	O
time	O
and	O
memory	O
to	O
train	O
depending	O
on	O
the	O
complexity	O
.	O
Thus	O
,	O
the	O
simplicity	O
and	O
efficiency	O
makes	O
our	O
model	O
a	O
better	O
fit	O
for	O
rarelanguage	O
and	O
limited	O
-	O
budget	O
scenarios	O
.	O

Here	O
we	O
investigate	O
the	O
effect	O
'	O
of	O
hyper	O
-	O
parameter	O
α	B-HyperparameterName
,	O
τ	O
on	O
the	O
evaluation	O
results	O
.	O
The	O
α	B-HyperparameterName
is	O
used	O
to	O
balance	O
the	O
optimization	O
of	O
encoder	O
constraint	O
and	O
gate	O
constraint	O
,	O
where	O
larger	O
α	B-HyperparameterName
means	O
more	O
optimization	O
on	O
gate	O
constraint	O
.	O
The	O
temperature	O
τ	O
is	O
used	O
to	O
control	O
the	O
aggressiveness	O
of	O
the	O
replacement	O
XL	O
-	O
NBT	O
-	O
D	O
,	O
where	O
smaller	O
τ	O
means	O
more	O
source	O
words	O
are	O
replaced	O
by	O
target	O
synonyms	O
.	O
From	O
the	O
table	O
experimental	O
results	O
are	O
not	O
very	O
sensitive	O
to	O
α	B-HyperparameterName
,	O
a	O
dramatic	O
change	O
of	O
α	B-HyperparameterName
will	O
not	O
harm	O
the	O
final	O
results	O
too	O
much	O
,	O
we	O
simply	O
choose	O
α	B-HyperparameterName
=	O
1	O
as	O
the	O
hyper	O
-	O
parameter	O
.	O
In	O
contrast	O
,	O
the	O
system	O
is	O
more	O
sensitive	O
to	O
temperature	O
.	O
Too	O
conservative	O
replacement	O
will	O
lead	O
to	O
weak	O
transfer	O
,	O
while	O
too	O
aggressive	O
replacement	O
will	O
destroy	O
the	O
utterance	O
representation	O
.	O
Therefore	O
,	O
we	O
choose	O
the	O
a	O
moderate	O
temperature	O
of	O
τ	O
=	O
0.1	O
throughout	O
our	O
experiments	O
.	O
We	O
also	O
draw	O
the	O
learning	O
curve	O
(	O
Precision	B-MetricName
vs.	O
Iteration	O
)	O
in	O
the	O
Appendix	O
for	O
both	O
XL	O
-	O
NBT	O
-	O
C	O
and	O
XL	O
-	O
NBT	O
-	O
D.	O
The	O
learning	O
curves	O
show	O
that	O
our	O
algorithm	O
is	O
stable	O
and	O
converges	O
quickly	O
,	O
and	O
the	O
reported	O
results	O
are	O
highly	O
reproducible	O
.	O

Rapid	O
progress	O
has	O
been	O
made	O
in	O
the	O
field	O
of	O
reading	B-TaskName
comprehension	I-TaskName
and	O
question	B-TaskName
answering	I-TaskName
,	O
where	O
several	O
systems	O
have	O
achieved	O
human	O
parity	O
in	O
some	O
simplified	O
settings	O
.	O
However	O
,	O
the	O
performance	O
of	O
these	O
models	O
degrades	O
significantly	O
when	O
they	O
are	O
applied	O
to	O
more	O
realistic	O
scenarios	O
,	O
where	O
answers	O
are	O
involved	O
with	O
various	O
types	O
,	O
multiple	O
text	O
strings	O
are	O
correct	O
answers	O
,	O
or	O
discrete	O
reasoning	O
abilities	O
are	O
required	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
the	O
Multi	O
-	O
Type	O
Multi	O
-	O
Span	O
Network	O
(	O
MTMSN	O
)	O
,	O
a	O
neural	O
reading	B-TaskName
comprehension	I-TaskName
model	O
that	O
combines	O
a	O
multi	O
-	O
type	O
answer	O
predictor	O
designed	O
to	O
support	O
various	O
answer	O
types	O
(	O
e.g.	O
,	O
span	O
,	O
count	O
,	O
negation	O
,	O
and	O
arithmetic	O
expression	O
)	O
with	O
a	O
multi	O
-	O
span	O
extraction	O
method	O
for	O
dynamically	O
producing	O
one	O
or	O
multiple	O
text	O
spans	O
.	O
In	O
addition	O
,	O
an	O
arithmetic	O
expression	O
reranking	O
mechanism	O
is	O
proposed	O
to	O
rank	O
expression	O
candidates	O
for	O
further	O
confirming	O
the	O
prediction	O
.	O
Experiments	O
show	O
that	O
our	O
model	O
achieves	O
79.9	O
F1	B-MetricName
on	O
the	O
DROP	B-DatasetName
hidden	O
test	O
set	O
,	O
creating	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O
Source	O
code	O
1	O
is	O
released	O
to	O
facilitate	O
future	O
work	O
.	O

This	O
paper	O
considers	O
the	O
reading	B-TaskName
comprehension	I-TaskName
task	O
in	O
which	O
some	O
discrete	O
-	O
reasoning	O
abilities	O
are	O
needed	O
to	O
correctly	O
answer	O
questions	O
.	O
Specifically	O
,	O
we	O
focus	O
on	O
a	O
new	O
reading	B-TaskName
comprehension	I-TaskName
dataset	O
called	O
DROP	B-DatasetName
(	O
Dua	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
requires	O
Discrete	O
Reasoning	O
Over	O
the	O
content	O
of	O
Paragraphs	O
to	O
obtain	O
the	O
final	O
answer	O
.	O
Unlike	O
previous	O
benchmarks	O
such	O
as	O
CNN	O
/	O
DM	O
(	O
Hermann	O
et	O
al	O
,	O
2015	O
)	O
and	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
that	O
have	O
been	O
well	O
solved	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
DROP	B-DatasetName
is	O
substantially	O
more	O
challenging	O
in	O
three	O
ways	O
.	O
First	O
,	O
the	O
answers	O
to	O
the	O
questions	O
involve	O
a	O
wide	O
range	O
of	O
types	O
such	O
as	O
numbers	O
,	O
dates	O
,	O
or	O
text	O
strings	O
.	O
Therefore	O
,	O
various	O
kinds	O
of	O
prediction	O
strategies	O
are	O
required	O
to	O
successfully	O
find	O
the	O
answers	O
.	O
Second	O
,	O
rather	O
than	O
restricting	O
the	O
answer	O
to	O
be	O
a	O
span	O
of	O
text	O
,	O
DROP	B-DatasetName
loosens	O
the	O
constraint	O
so	O
that	O
answers	O
may	O
be	O
a	O
set	O
of	O
multiple	O
text	O
strings	O
.	O
Third	O
,	O
for	O
questions	O
that	O
require	O
discrete	O
reasoning	O
,	O
a	O
system	O
must	O
have	O
a	O
more	O
comprehensive	O
understanding	O
of	O
the	O
context	O
and	O
be	O
able	O
to	O
perform	O
numerical	O
operations	O
such	O
as	O
addition	O
,	O
counting	O
,	O
or	O
sorting	O
.	O
Existing	O
approaches	O
,	O
when	O
applied	O
to	O
this	O
more	O
realistic	O
scenario	O
,	O
have	O
three	O
problems	O
.	O
First	O
,	O
to	O
produce	O
various	O
answer	O
types	O
,	O
Dua	O
et	O
al	O
(	O
2019	O
)	O
extend	O
previous	O
one	O
-	O
type	O
answer	O
prediction	O
(	O
Seo	O
et	O
al	O
,	O
2017	O
)	O
to	O
multi	O
-	O
type	B-TaskName
prediction	I-TaskName
that	O
supports	O
span	O
extraction	O
,	O
counting	O
,	O
and	O
addition	O
/	O
subtraction	O
.	O
However	O
,	O
they	O
have	O
not	O
fully	O
considered	O
all	O
potential	O
types	O
.	O
Take	O
the	O
question	O
"	O
What	O
percent	O
are	O
not	O
non	O
-	O
families	O
?	O
"	O
and	O
the	O
passage	O
snippet	O
"	O
39.9	O
%	O
were	O
non	O
-	O
families	O
"	O
as	O
an	O
example	O
,	O
a	O
negation	O
operation	O
is	O
required	O
to	O
infer	O
the	O
answer	O
.	O
Second	O
,	O
previous	O
reading	B-TaskName
comprehension	I-TaskName
models	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
;	O
Yu	O
et	O
al	O
,	O
2018	O
;	O
Hu	O
et	O
al	O
,	O
2018	O
)	O
are	O
designed	O
to	O
produce	O
one	O
single	O
span	O
as	O
the	O
answer	O
.	O
But	O
for	O
some	O
questions	O
such	O
as	O
"	O
Which	O
ancestral	O
groups	O
are	O
smaller	O
than	O
11	O
%	O
?	O
"	O
,	O
there	O
may	O
exist	O
several	O
spans	O
as	O
correct	O
answers	O
(	O
e.g.	O
,	O
"	O
Italian	O
"	O
,	O
"	O
English	O
"	O
,	O
and	O
"	O
Polish	O
"	O
)	O
,	O
which	O
can	O
not	O
be	O
well	O
handled	O
by	O
these	O
works	O
.	O
Third	O
,	O
to	O
support	O
numerical	O
reasoning	O
,	O
prior	O
work	O
(	O
Dua	O
et	O
al	O
,	O
2019	O
)	O
learns	O
to	O
predict	O
signed	O
numbers	O
for	O
obtaining	O
an	O
arithmetic	O
expression	O
that	O
can	O
be	O
executed	O
by	O
a	O
symbolic	O
system	O
.	O
Nevertheless	O
,	O
the	O
prediction	O
of	O
each	O
signed	O
number	O
is	O
isolated	O
,	O
and	O
the	O
expression	O
's	O
context	O
information	O
has	O
not	O
been	O
considered	O
.	O
As	O
a	O
result	O
,	O
obviously	O
-	O
wrong	O
expressions	O
,	O
such	O
as	O
all	O
predicted	O
signs	O
are	O
either	O
minus	O
or	O
zero	O
,	O
are	O
likely	O
produced	O
.	O
To	O
address	O
the	O
above	O
issues	O
,	O
we	O
introduce	O
the	O
Multi	O
-	O
Type	O
Multi	O
-	O
Span	O
Network	O
(	O
MTMSN	O
)	O
,	O
a	O
neural	O
reading	B-TaskName
comprehension	I-TaskName
model	O
for	O
predicting	O
various	O
types	O
of	O
answers	O
as	O
well	O
as	O
dynamically	O
extracting	O
one	O
or	O
multiple	O
spans	O
.	O
MTMSN	O
utilizes	O
a	O
series	O
of	O
pre	O
-	O
trained	O
Transformer	B-MethodName
blocks	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
to	O
obtain	O
a	O
deep	O
bidirectional	O
context	O
representation	O
.	O
On	O
top	O
of	O
it	O
,	O
a	O
multi	O
-	O
type	O
answer	O
predictor	O
is	O
proposed	O
to	O
not	O
only	O
support	O
previous	O
prediction	O
strategies	O
such	O
as	O
span	O
,	O
count	O
number	O
,	O
and	O
arithmetic	O
expression	O
,	O
but	O
also	O
add	O
a	O
new	O
type	O
of	O
logical	O
negation	O
.	O
This	O
results	O
in	O
a	O
wider	O
range	O
of	O
coverage	O
of	O
answer	O
types	O
,	O
which	O
turns	O
out	O
to	O
be	O
crucial	O
to	O
performance	O
.	O
Besides	O
,	O
rather	O
than	O
always	O
producing	O
one	O
single	O
span	O
,	O
we	O
present	O
a	O
multi	O
-	O
span	O
extraction	O
method	O
to	O
produce	O
multiple	O
answers	O
.	O
The	O
model	O
first	O
predicts	O
the	O
number	O
of	O
answers	O
,	O
and	O
then	O
extracts	O
non	O
-	O
overlapped	O
spans	O
to	O
the	O
specific	O
amount	O
.	O
In	O
this	O
way	O
,	O
the	O
model	O
can	O
learn	O
to	O
dynamically	O
extract	O
one	O
or	O
multiple	O
spans	O
,	O
thus	O
being	O
beneficial	O
for	O
multi	O
-	O
answer	O
cases	O
.	O
In	O
addition	O
,	O
we	O
propose	O
an	O
arithmetic	O
expression	O
reranking	O
mechanism	O
to	O
rank	O
expression	O
candidates	O
that	O
are	O
decoded	O
by	O
beam	O
search	O
,	O
so	O
that	O
their	O
context	O
information	O
can	O
be	O
considered	O
during	O
reranking	O
to	O
further	O
confirm	O
the	O
prediction	O
.	O
Our	O
MTMSN	O
model	O
outperforms	O
all	O
existing	O
approaches	O
on	O
the	O
DROP	B-DatasetName
hidden	O
test	O
set	O
by	O
achieving	O
79.9	O
F1	B-MetricName
score	I-MetricName
,	O
a	O
32.9	O
%	O
absolute	O
gain	O
over	O
prior	O
best	O
work	O
at	O
the	O
time	O
of	O
submission	O
.	O
To	O
make	O
a	O
fair	O
comparison	O
,	O
we	O
also	O
construct	O
a	O
baseline	O
that	O
uses	O
the	O
same	O
BERT	B-MethodName
-	O
based	O
encoder	O
.	O
Again	O
,	O
MTMSN	O
surpasses	O
it	O
by	O
obtaining	O
a	O
13.2	O
F1	B-MetricName
increase	O
on	O
the	O
development	O
set	O
.	O
We	O
also	O
provide	O
an	O
in	O
-	O
depth	O
ablation	O
study	O
to	O
show	O
the	O
effectiveness	O
of	O
our	O
proposed	O
methods	O
,	O
analyze	O
performance	O
breakdown	O
by	O
different	O
answer	O
types	O
,	O
and	O
give	O
some	O
qualitative	O
examples	O
as	O
well	O
as	O
error	O
analysis	O
.	O

Although	O
existing	O
reading	B-TaskName
comprehension	I-TaskName
tasks	O
focus	O
exclusively	O
on	O
finding	O
one	O
span	O
of	O
text	O
as	O
the	O
final	O
answer	O
,	O
DROP	B-DatasetName
loosens	O
the	O
restriction	O
so	O
that	O
the	O
answer	O
to	O
the	O
question	O
may	O
be	O
several	O
text	O
spans	O
.	O
Therefore	O
,	O
specific	O
adaption	O
should	O
be	O
made	O
to	O
extend	O
previous	O
single	O
-	O
span	O
extraction	O
to	O
multi	O
-	O
span	O
scenario	O
.	O
To	O
do	O
this	O
,	O
we	O
propose	O
directly	O
predicting	O
the	O
number	O
of	O
spans	O
and	O
model	O
it	O
as	O
a	O
classification	O
problem	O
.	O
This	O
is	O
achieved	O
by	O
computing	O
a	O
probability	O
distribution	O
on	O
span	O
amount	O
as	O
p	O
span	O
=	O
softmax	B-MethodName
(	O
FFN	O
(	O
[	O
h	O
Q	O
2	O
;	O
h	O
P	O
2	O
;	O
h	O
CLS	O
]	O
)	O
)	O
To	O
extract	O
non	O
-	O
overlapped	O
spans	O
to	O
the	O
specific	O
amount	O
,	O
we	O
adopt	O
the	O
non	O
-	O
maximum	O
suppression	O
(	O
NMS	O
)	O
algorithm	O
(	O
Rosenfeld	O
and	O
Thurston	O
,	O
1971	O
)	O
that	O
is	O
widely	O
used	O
in	O
computer	O
vision	O
for	O
pruning	O
redundant	O
bounding	O
boxes	O
,	O
as	O
shown	O
in	O
Algorithm	O
1	O
.	O
Concretely	O
,	O
the	O
model	O
first	O
proposes	O
a	O
set	O
of	O
top	O
-	O
K	O
spans	O
S	O
according	O
to	O
the	O
descending	O
order	O
of	O
the	O
span	O
score	O
,	O
which	O
is	O
computed	O
as	O
p	O
start	O
k	O
p	O
end	O
l	O
for	O
the	O
span	O
(	O
k	O
,	O
l	O
)	O
.	O
It	O
also	O
predicts	O
the	O
amount	O
of	O
extracted	O
spans	O
t	O
from	O
p	O
span	O
,	O
and	O
initializes	O
a	O
new	O
setS.	O
Next	O
,	O
we	O
add	O
the	O
span	O
s	O
i	O
that	O
possesses	O
the	O
maximum	O
span	O
score	O
to	O
the	O
setS	O
,	O
and	O
remove	O
it	O
from	O
S.	O
We	O
also	O
delete	O
any	O
remaining	O
span	O
s	O
j	O
that	O
overlaps	O
with	O
s	O
i	O
,	O
where	O
the	O
degree	O
of	O
overlap	O
is	O
measured	O
using	O
the	O
text	O
-	O
level	O
F1	B-MetricName
function	O
.	O
This	O
process	O
is	O
repeated	O
for	O
remaining	O
spans	O
in	O
S	O
,	O
until	O
S	O
is	O
empty	O
or	O
the	O
size	O
ofS	O
reaches	O
t.	O

Since	O
DROP	B-DatasetName
does	O
not	O
indicate	O
the	O
answer	O
type	O
but	O
only	O
provides	O
the	O
answer	O
string	O
,	O
we	O
therefore	O
adopt	O
the	O
weakly	O
supervised	O
annotation	O
scheme	O
,	O
as	O
suggested	O
in	O
Berant	O
et	O
al	O
(	O
2013	O
)	O
;	O
Dua	O
et	O
al	O
(	O
2019	O
)	O
.	O
We	O
find	O
all	O
possible	O
annotations	O
that	O
point	O
to	O
the	O
gold	O
answer	O
,	O
including	O
matching	O
spans	O
,	O
arithmetic	O
expressions	O
,	O
correct	O
count	O
numbers	O
,	O
negation	O
operations	O
,	O
and	O
the	O
number	O
of	O
spans	O
.	O
We	O
use	O
simple	O
rules	O
to	O
search	O
over	O
all	O
mentioned	O
numbers	O
to	O
find	O
potential	O
negations	O
.	O
That	O
is	O
,	O
if	O
100	O
minus	O
a	O
number	O
is	O
equal	O
to	O
the	O
answer	O
,	O
then	O
a	O
negation	O
occurs	O
on	O
this	O
number	O
.	O
Besides	O
,	O
we	O
only	O
search	O
the	O
addition	O
/	O
subtraction	O
of	O
three	O
numbers	O
at	O
most	O
due	O
to	O
the	O
exponential	O
search	O
space	O
.	O
To	O
train	O
our	O
model	O
,	O
we	O
propose	O
using	O
a	O
twostep	O
training	O
method	O
composed	O
of	O
an	O
inference	O
step	O
and	O
a	O
training	O
step	O
.	O
In	O
the	O
first	O
step	O
,	O
we	O
use	O
the	O
model	O
to	O
predict	O
the	O
probabilities	O
of	O
sign	O
assignments	O
for	O
numbers	O
.	O
If	O
there	O
exists	O
any	O
annotation	O
of	O
arithmetic	O
expressions	O
,	O
we	O
run	O
beam	O
search	O
to	O
produce	O
expression	O
candidates	O
and	O
label	O
them	O
as	O
either	O
correct	O
or	O
wrong	O
,	O
which	O
are	O
later	O
used	O
for	O
supervising	O
the	O
reranking	O
component	O
.	O
In	O
the	O
second	O
step	O
,	O
we	O
adopt	O
the	O
marginal	O
likelihood	O
objective	O
function	O
(	O
Clark	O
and	O
Gardner	O
,	O
2018	O
)	O
,	O
which	O
sums	O
over	O
the	O
probabilities	O
of	O
all	O
possible	O
annotations	O
including	O
the	O
above	O
labeled	O
expressions	O
.	O
Notice	O
that	O
there	O
are	O
two	O
objective	O
functions	O
for	O
the	O
multi	O
-	O
span	O
component	O
:	O
one	O
is	O
a	O
distantly	O
-	O
supervised	O
loss	B-MetricName
that	O
maximizes	O
the	O
probabilities	O
of	O
all	O
matching	O
spans	O
,	O
and	O
the	O
other	O
is	O
a	O
classification	O
loss	B-MetricName
that	O
maximizes	O
the	O
probability	O
on	O
span	O
amount	O
.	O
At	O
test	O
time	O
,	O
the	O
model	O
first	O
chooses	O
the	O
answer	O
type	O
and	O
then	O
performs	O
specific	O
prediction	O
strategies	O
.	O
For	O
the	O
span	O
type	O
,	O
we	O
use	O
Algorithm	O
1	O
for	O
decoding	O
.	O
If	O
the	O
type	O
is	O
addition	O
/	O
subtraction	O
,	O
arithmetic	O
expression	O
candidates	O
will	O
be	O
proposed	O
and	O
further	O
reranked	O
.	O
The	O
expression	O
with	O
the	O
maximum	O
product	O
of	O
cumulative	O
sign	O
probability	O
and	O
reranking	O
probability	O
is	O
chosen	O
.	O
As	O
for	O
the	O
counting	O
type	O
,	O
we	O
choose	O
the	O
number	O
that	O
has	O
the	O
maximum	O
counting	O
probability	O
.	O
Finally	O
,	O
if	O
the	O
type	O
is	O
negation	O
,	O
we	O
find	O
the	O
number	O
that	O
possesses	O
the	O
largest	O
negation	O
probability	O
,	O
and	O
then	O
output	O
the	O
answer	O
as	O
100	O
minus	O
this	O
number	O
.	O

Dataset	O
We	O
consider	O
the	O
reading	B-TaskName
comprehension	I-TaskName
benchmark	O
that	O
requires	O
Discrete	O
Reasoning	O
Over	O
Paragraphs	O
(	O
DROP	B-DatasetName
)	O
(	O
Dua	O
et	O
al	O
,	O
2019	O
)	O
prehensive	O
understanding	O
of	O
the	O
context	O
as	O
well	O
as	O
the	O
ability	O
of	O
numerical	O
reasoning	O
are	O
required	O
.	O
Model	O
settings	O
We	O
build	O
our	O
model	O
upon	O
two	O
publicly	O
available	O
uncased	O
versions	O
of	O
BERT	B-MethodName
:	O
BERT	B-MethodName
BASE	B-MethodName
and	O
BERT	B-MethodName
LARGE	O
2	O
,	O
and	O
refer	O
readers	O
to	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
for	O
details	O
on	O
model	O
sizes	O
.	O
We	O
use	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e	O
-	O
5	O
and	O
warmup	O
over	O
the	O
first	O
5	O
%	O
steps	O
to	O
train	O
.	O
The	O
maximum	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
is	O
set	O
to	O
10	O
for	O
base	O
models	O
and	O
5	O
for	O
large	O
models	O
,	O
while	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
12	O
or	O
24	O
respectively	O
.	O
A	O
dropout	O
probability	O
of	O
0.1	O
is	O
used	O
unless	O
stated	O
otherwise	O
.	O
The	O
number	O
of	O
counting	O
class	O
is	O
set	O
to	O
10	O
,	O
and	O
the	O
maximum	O
number	O
of	O
spans	O
is	O
8	O
.	O
The	O
beam	O
size	O
is	O
3	O
by	O
default	O
,	O
while	O
the	O
maximum	O
amount	O
of	O
signed	O
numbers	O
M	O
is	O
set	O
to	O
4	O
.	O
All	O
texts	O
are	O
tokenized	O
using	O
Word	O
-	O
Piece	O
vocabulary	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
truncated	O
to	O
sequences	O
no	O
longer	O
than	O
512	O
tokens	O
.	O
Baselines	O
Following	O
the	O
implementation	O
of	O
Augmented	O
QANet	O
(	O
NAQANet	O
)	O
(	O
Dua	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
introduce	O
a	O
similar	O
baseline	O
called	O
Augmented	O
BERT	B-MethodName
(	O
NABERT	O
)	O
.	O
The	O
main	O
difference	O
is	O
that	O
we	O
replace	O
the	O
encoder	O
of	O
QANet	O
(	O
Yu	O
et	O
al	O
,	O
2018	O
)	O
with	O
the	O
pre	O
-	O
trained	O
Transformer	B-MethodName
blocks	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
Moreover	O
,	O
it	O
also	O
supports	O
the	O
prediction	O
of	O
various	O
answer	O
types	O
such	O
as	O
span	O
,	O
arithmetic	O
expression	O
,	O
and	O
count	O
number	O
.	O

Two	O
metrics	O
,	O
namely	O
Exact	B-MetricName
Match	I-MetricName
(	O
EM	B-MetricName
)	O
and	O
F1	B-MetricName
score	I-MetricName
,	O
are	O
utilized	O
to	O
evaluate	O
models	O
.	O
We	O
use	O
the	O
official	O
script	O
to	O
compute	O
these	O
scores	O
.	O
Since	O
the	O
test	O
set	O
is	O
hidden	O
,	O
we	O
only	O
submit	O
the	O
best	O
single	O
model	O
to	O
obtain	O
test	O
results	O
.	O
Table	O
1	O
shows	O
the	O
performance	O
of	O
our	O
model	O
and	O
other	O
competitive	O
approaches	O
on	O
the	O
develop	O
-	O

Component	O
ablation	O
To	O
analyze	O
the	O
effect	O
of	O
the	O
proposed	O
components	O
,	O
we	O
conduct	O
ablation	O
studies	O
on	O
the	O
development	O
set	O
.	O
As	O
illustrated	O
in	O
Table	O
2	O
,	O
the	O
use	O
of	O
addition	O
and	O
subtraction	O
is	O
extremely	O
crucial	O
:	O
the	O
EM	B-MetricName
/	O
F1	B-MetricName
performance	O
of	O
both	O
the	O
base	O
and	O
large	O
models	O
drop	O
drastically	O
by	O
more	O
than	O
20	O
points	O
if	O
it	O
is	O
removed	O
.	O
Predicting	O
count	O
numbers	O
is	O
also	O
an	O
important	O
component	O
that	O
contributes	O
nearly	O
5	O
%	O
gain	O
on	O
both	O
metrics	O
.	O
Moreover	O
,	O
enhancing	O
the	O
model	O
with	O
the	O
negation	O
type	O
significantly	O
increases	O
the	O
F1	B-MetricName
by	O
roughly	O
9	O
percent	O
on	O
both	O
models	O
.	O
In	O
brief	O
,	O
the	O
above	O
results	O
show	O
that	O
multi	O
-	O
type	O
answer	O
prediction	O
is	O
vitally	O
important	O
for	O
handling	O
different	O
forms	O
of	O
answers	O
,	O
especially	O
in	O
cases	O
where	O
discrete	O
reasoning	O
abilities	O
are	O
required	O
.	O
We	O
also	O
report	O
the	O
performance	O
after	O
removing	O
the	O
multi	O
-	O
span	O
extraction	O
method	O
.	O
The	O
results	O
reveal	O
that	O
it	O
has	O
a	O
more	O
negative	O
impact	O
on	O
the	O
F1	B-MetricName
score	I-MetricName
.	O
We	O
interpret	O
this	O
phenomenon	O
as	O
follows	O
:	O
producing	O
multiple	O
spans	O
that	O
are	O
partially	O
matched	O
with	O
ground	O
-	O
truth	O
answers	O
is	O
much	O
easier	O
than	O
generating	O
an	O
exactly	O
-	O
matched	O
set	O
of	O
multiple	O
answers	O
.	O
Hence	O
for	O
multi	O
-	O
span	O
scenarios	O
,	O
the	O
gain	O
of	O
our	O
method	O
on	O
F1	B-MetricName
is	O
relatively	O
easier	O
to	O
obtain	O
than	O
the	O
one	O
on	O
EM	B-MetricName
.	O
Finally	O
,	O
to	O
ablate	O
arithmetic	O
expression	O
reranking	O
,	O
we	O
simply	O
use	O
the	O
arithmetic	O
expression	O
that	O
has	O
the	O
maximum	O
cumulative	O
sign	O
probability	O
instead	O
.	O
We	O
find	O
that	O
our	O
reranking	O
mechanism	O
gives	O
1.8	O
%	O
gain	O
on	O
both	O
metrics	O
for	O
the	O
large	O
model	O
.	O
This	O
confirms	O
that	O
validating	O
expression	O
candidates	O
with	O
their	O
context	O
information	O
is	O
beneficial	O
for	O
filtering	O
out	O
highly	O
-	O
confident	O
but	O
wrong	O
predictions	O
.	O
Architecture	O
ablation	O
We	O
further	O
conduct	O
a	O
detailed	O
ablation	O
in	O
Table	O
3	O
to	O
evaluate	O
our	O
architecture	O
designs	O
.	O
First	O
,	O
we	O
investigate	O
the	O
effects	O
of	O
some	O
"	O
global	O
vectors	O
"	O
used	O
in	O
our	O
model	O
.	O
Specifically	O
,	O
we	O
find	O
that	O
removing	O
the	O
question	O
and	O
passage	O
vectors	O
from	O
all	O
involved	O
computation	O
leads	O
to	O
1.3	O
%	O
drop	O
on	O
F1	B-MetricName
.	O
Ablating	O
the	O
representation	O
of	O
[	O
CLS	O
]	O
token	O
leads	O
to	O
even	O
worse	O
results	O
.	O
We	O
also	O
try	O
to	O
use	O
the	O
last	O
hidden	O
representation	O
(	O
denoted	O
as	O
M	O
3	O
)	O
to	O
calculate	O
question	O
and	O
passage	O
vectors	O
,	O
but	O
find	O
that	O
does	O
not	O
work	O
.	O
Next	O
,	O
we	O
remove	O
the	O
gating	O
mechanism	O
used	O
during	O
span	O
prediction	O
,	O
and	O
observe	O
a	O
nearly	O
0.8	O
%	O
decline	O
on	O
both	O
metrics	O
.	O
Finally	O
,	O
we	O
share	O
parameters	O
between	O
the	O
arithmetic	O
expression	O
component	O
and	O
the	O
negation	O
component	O
,	O
and	O
find	O
the	O
performance	O
drops	O
by	O
1.1	O
%	O
on	O
F1	B-MetricName
.	O

Performance	O
breakdown	O
We	O
now	O
provide	O
a	O
quantitative	O
analysis	O
by	O
showing	O
performance	O
breakdown	O
on	O
the	O
development	O
set	O
.	O
Table	O
4	O
shows	O
that	O
our	O
gains	O
mainly	O
come	O
from	O
the	O
most	O
frequent	O
number	O
type	O
,	O
which	O
requires	O
various	O
types	O
of	O
symbolic	O
,	O
discrete	O
reasoning	O
operations	O
.	O
Moreover	O
,	O
significant	O
improvements	O
are	O
also	O
obtained	O
in	O
the	O
multi	O
-	O
span	O
category	O
,	O
where	O
the	O
F1	B-MetricName
score	I-MetricName
increases	O
by	O
more	O
than	O
40	O
points	O
.	O
This	O
result	O
further	O
proves	O
the	O
validity	O
of	O
our	O
multi	O
-	O
span	O
extraction	O
method	O
.	O
We	O
also	O
give	O
the	O
performance	O
statistics	O
that	O
are	O
categorized	O
according	O
to	O
the	O
predicted	O
answer	O
types	O
in	O
Table	O
5	O
.	O
As	O
shown	O
in	O
the	O
Table	O
,	O
the	O
main	O
improvements	O
are	O
due	O
to	O
the	O
addition	O
/	O
subtraction	O
and	O
negation	O
types	O
.	O
We	O
conjecture	O
that	O
there	O
are	O
two	O
reasons	O
for	O
these	O
improvements	O
.	O
First	O
,	O
our	O
proposed	O
expression	O
reranking	O
mechanism	O
helps	O
validate	O
candidate	O
expressions	O
.	O
Second	O
,	O
a	O
new	O
inductive	O
bias	O
that	O
enables	O
the	O
model	O
to	O
perform	O
logical	O
negation	O
has	O
been	O
introduced	O
.	O
The	O
impressive	O
performance	O
on	O
the	O
negation	O
type	O
confirms	O
our	O
judgement	O
,	O
and	O
suggests	O
that	O
the	O
model	O
is	O
able	O
to	O
find	O
most	O
of	O
negation	O
operations	O
.	O
In	O
addition	O
,	O
we	O
also	O
observe	O
promising	O
gains	O
brought	O
by	O
the	O
span	O
and	O
count	O
types	O
.	O
We	O
think	O
the	O
gains	O
are	O
mainly	O
due	O
to	O
the	O
multi	O
-	O
span	O
extraction	O
method	O
as	O
well	O
as	O
architecture	O
designs	O
.	O
Effect	O
of	O
maximum	O
number	O
of	O
spans	O
To	O
investigate	O
the	O
effect	O
of	O
maximum	O
number	O
of	O
spans	O
on	O
multi	O
-	O
span	O
extraction	O
,	O
we	O
conduct	O
an	O
experiment	O
on	O
the	O
dev	O
set	O
and	O
show	O
the	O
curves	O
in	O
Figure	O
3	O
.	O
We	O
vary	O
the	O
value	O
from	O
2	O
to	O
12	O
,	O
increased	O
by	O
2	O
,	O
and	O
also	O
include	O
the	O
extreme	O
value	O
1	O
.	O
According	O
to	O
the	O
Figure	O
,	O
the	O
best	O
results	O
are	O
obtained	O
at	O
8	O
.	O
A	O
higher	O
value	O
could	O
potentially	O
increase	O
the	O
answer	O
recall	O
but	O
damage	O
the	O
precision	O
by	O
making	O
more	O
predictions	O
,	O
and	O
a	O
smaller	O
value	O
may	O
force	O
the	O
model	O
to	O
produce	O
limited	O
number	O
of	O
answers	O
,	O
resulting	O
in	O
high	O
precision	O
but	O
low	O
recall	O
.	O
Therefore	O
,	O
a	O
value	O
of	O
8	O
turns	O
out	O
to	O
be	O
a	O
good	O
trade	O
-	O
off	O
between	O
recall	O
and	O
precision	O
.	O
Moreover	O
,	O
when	O
the	O
value	O
decreases	O
to	O
1	O
,	O
the	O
multi	O
-	O
span	O
extraction	O
degrades	O
to	O
previous	O
single	O
-	O
span	O
scenario	O
,	O
and	O
the	O
performance	O
drops	O
significantly	O
.	O
Effect	O
of	O
beam	O
size	O
and	O
M	O
We	O
further	O
investigate	O
the	O
effect	O
of	O
beam	O
size	O
and	O
maximum	O
amount	O
of	O
signed	O
numbers	O
in	O
Figure	O
4	O
.	O
As	O
we	O
can	O
see	O
,	O
a	O
beam	O
size	O
of	O
3	O
leads	O
to	O
the	O
best	O
performance	O
,	O
likely	O
because	O
a	O
larger	O
beam	O
size	O
might	O
confuse	O
the	O
model	O
as	O
too	O
many	O
candidates	O
are	O
ranked	O
,	O
on	O
the	O
other	O
hand	O
,	O
a	O
small	O
size	O
could	O
be	O
not	O
sufficient	O
to	O
cover	O
the	O
correct	O
expression	O
.	O
In	O
addition	O
,	O
we	O
find	O
that	O
the	O
performance	O
constantly	O
decreases	O
as	O
the	O
maximum	O
threshold	O
M	O
increases	O
,	O
suggesting	O
that	O
most	O
of	O
expressions	O
only	O
contain	O
two	O
or	O
three	O
signed	O
numbers	O
,	O
and	O
setting	O
a	O
larger	O
threshold	O
could	O
bring	O
in	O
additional	O
distractions	O
.	O

We	O
list	O
the	O
annotation	O
statistics	O
on	O
the	O
DROP	B-DatasetName
train	O
set	O
in	O
Table	O
6	O
.	O
As	O
we	O
can	O
see	O
,	O
only	O
annotating	O
matching	O
spans	O
results	O
in	O
a	O
labeled	O
ratio	O
of	O
56.4	O
%	O
,	O
indicating	O
that	O
DROP	B-DatasetName
includes	O
various	O
answer	O
types	O
beyond	O
text	O
spans	O
.	O
By	O
further	O
considering	O
the	O
arithmetic	O
expression	O
,	O
the	O
ratio	O
increase	O
sharply	O
to	O
91.7	O
%	O
,	O
suggesting	O
more	O
than	O
35	O
%	O
answers	O
need	O
to	O
be	O
inferred	O
with	O
numeral	O
reasoning	O
.	O
Continuing	O
adding	O
counting	O
leads	O
to	O
a	O
percentage	O
of	O
94.4	O
%	O
,	O
and	O
a	O
final	O
97.9	O
%	O
coverage	O
is	O
achieved	O
by	O
additionally	O
taking	O
negation	O
into	O
account	O
.	O
More	O
importantly	O
,	O
the	O
F1	B-MetricName
score	I-MetricName
constantly	O
increases	O
as	O
more	O
answer	O
types	O
are	O
considered	O
.	O
This	O
result	O
is	O
consistent	O
with	O
our	O
observations	O
in	O
ablation	O
study	O
.	O
Error	B-MetricName
analysis	O
Finally	O
,	O
to	O
better	O
understand	O
the	O
remaining	O
challenges	O
,	O
we	O
randomly	O
sample	O
100	O
incorrectly	O
predicted	O
examples	O
based	O
on	O
EM	B-MetricName
and	O
categorize	O
them	O
into	O
7	O
classes	O
.	O
38	O
%	O
of	O
errors	O
are	O
incorrect	O
arithmetic	O
computations	O
,	O
18	O
%	O
require	O
sorting	O
over	O
multiple	O
entities	O
,	O
13	O
%	O
are	O
due	O
to	O
mistakes	O
on	O
multi	O
-	O
span	O
extraction	O
,	O
10	O
%	O
are	O
singlespan	O
extraction	O
problems	O
,	O
8	O
%	O
involve	O
miscounting	O
,	O
another	O
8	O
%	O
are	O
wrong	O
predictions	O
on	O
span	O
number	O
,	O
the	O
rest	O
(	O
5	O
%	O
)	O
are	O
due	O
to	O
various	O
reasons	O
such	O
as	O
incorrect	O
preprocessing	O
,	O
negation	O
error	O
,	O
and	O
so	O
on	O
.	O
See	O
Appendix	O
for	O
some	O
examples	O
of	O
the	O
above	O
error	O
cases	O
.	O

We	O
introduce	O
MTMSN	O
,	O
a	O
multi	O
-	O
type	O
multi	O
-	O
span	O
network	O
for	O
reading	B-TaskName
comprehension	I-TaskName
that	O
requires	O
discrete	O
reasoning	O
over	O
the	O
content	O
of	O
paragraphs	O
.	O
We	O
enhance	O
a	O
multi	O
-	O
type	O
answer	O
predictor	O
to	O
support	O
logical	O
negation	O
,	O
propose	O
a	O
multi	O
-	O
span	O
extraction	O
method	O
for	O
producing	O
multiple	O
answers	O
,	O
and	O
design	O
an	O
arithmetic	O
expression	O
reranking	O
mechanism	O
to	O
further	O
confirm	O
the	O
prediction	O
.	O
Our	O
model	O
achieves	O
79.9	O
F1	B-MetricName
on	O
the	O
DROP	B-DatasetName
hidden	O
test	O
set	O
,	O
creating	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O
As	O
future	O
work	O
,	O
we	O
would	O
like	O
to	O
consider	O
handling	O
additional	O
types	O
such	O
as	O
sorting	O
or	O
multiplication	O
/	O
division	O
.	O
We	O
also	O
plan	O
to	O
explore	O
more	O
advanced	O
methods	O
for	O
performing	O
complex	O
numerical	O
reasoning	O
.	O

Text	B-TaskName
summarization	I-TaskName
is	O
considered	O
as	O
a	O
challenging	O
task	O
in	O
the	O
NLP	O
community	O
.	O
The	O
availability	O
of	O
datasets	O
for	O
the	O
task	O
of	O
multilingual	O
text	B-TaskName
summarization	I-TaskName
is	O
rare	O
,	O
and	O
such	O
datasets	O
are	O
difficult	O
to	O
construct	O
.	O
In	O
this	O
work	O
,	O
we	O
build	O
an	O
abstract	O
text	O
summarizer	O
for	O
the	O
German	O
language	O
text	O
using	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
"	O
Transformer	B-MethodName
"	O
model	O
.	O
We	O
propose	O
an	O
iterative	O
data	B-TaskName
augmentation	I-TaskName
approach	O
which	O
uses	O
synthetic	O
data	O
along	O
with	O
the	O
real	O
summarization	B-TaskName
data	O
for	O
the	O
German	O
language	O
.	O
To	O
generate	O
synthetic	O
data	O
,	O
the	O
Common	B-DatasetName
Crawl	I-DatasetName
(	O
German	O
)	O
dataset	O
is	O
exploited	O
,	O
which	O
covers	O
different	O
domains	O
.	O
The	O
synthetic	O
data	O
is	O
effective	O
for	O
the	O
low	O
resource	O
condition	O
and	O
is	O
particularly	O
helpful	O
for	O
our	O
multilingual	O
scenario	O
where	O
availability	O
of	O
summarizing	O
data	O
is	O
still	O
a	O
challenging	O
issue	O
.	O
The	O
data	O
are	O
also	O
useful	O
in	O
deep	O
learning	O
scenarios	O
where	O
the	O
neural	O
models	O
require	O
a	O
large	O
amount	O
of	O
training	O
data	O
for	O
utilization	O
of	O
its	O
capacity	O
.	O
The	O
obtained	O
summarization	B-TaskName
performance	O
is	O
measured	O
in	O
terms	O
of	O
ROUGE	O
and	O
BLEU	B-MetricName
score	I-MetricName
.	O
We	O
achieve	O
an	O
absolute	O
improvement	O
of	O
+1.5	O
and	O
+16.0	O
in	O
ROUGE1	O
F1	B-MetricName
(	O
R1	O
F1	B-MetricName
)	O
on	O
the	O
development	O
and	O
test	O
sets	O
,	O
respectively	O
,	O
compared	O
to	O
the	O
system	O
which	O
does	O
not	O
rely	O
on	O
data	B-TaskName
augmentation	I-TaskName
.	O

The	O
Transformer	B-MethodName
model	O
is	O
implemented	O
in	O
OpenNMT	O
-	O
py	O
.	O
To	O
train	O
the	O
model	O
,	O
we	O
use	O
a	O
single	O
GPU	O
.	O
To	O
fit	O
the	O
model	O
to	O
the	O
GPU	O
cluster	O
,	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
equal	O
to	O
4	O
,	O
096	O
is	O
selected	O
for	O
training	O
.	O
The	O
validation	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
8	O
.	O
We	O
use	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2	O
,	O
drop	O
out	O
of	O
0.2	O
and	O
8	O
,	O
000	O
warm	O
-	O
up	O
steps	O
.	O
Decoding	O
uses	O
a	O
beam	O
size	O
of	O
10	O
and	O
we	O
did	O
not	O
set	O
any	O
minimum	O
length	O
of	O
output	O
summary	O
.	O

We	O
evaluate	O
the	O
results	O
for	O
every	O
10	O
,	O
000	O
iterations	O
on	O
the	O
dev	O
and	O
test	O
set	O
.	O
The	O
automatic	O
evaluation	O
results	O
based	O
on	O
the	O
dev	O
and	O
test	O
set	O
are	O
shown	O
in	O
Table	O
2	O
with	O
sample	O
summaries	O
in	O
Table	O
3	O
.	O
To	O
evaluate	O
the	O
proposed	O
algorithms	O
,	O
we	O
use	O
ROUGE	O
(	O
Recall	B-MetricName
-	O
Oriented	O
Understudy	O
for	O
Gisting	O
Evaluation	O
)	O
score	O
,	O
which	O
is	O
a	O
popular	O
metric	O
for	O
text	B-TaskName
summarization	I-TaskName
task	O
,	O
and	O
has	O
several	O
variants	O
like	O
ROUGE	O
-	O
N	O
,	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
,	O
which	O
measure	O
the	O
overlap	O
of	O
n	O
-	O
grams	O
between	O
the	O
system	O
and	O
reference	O
summary	O
(	O
LIN	O
,	O
2004	O
)	O
.	O
We	O
use	O
ROUGE	O
1	O
F1	B-MetricName
(	O
R1	O
F1	B-MetricName
)	O
,	O
ROUGE	O
2	O
F1	B-MetricName
(	O
R2	O
F1	B-MetricName
)	O
,	O
and	O
ROUGE	O
L	O
F1	B-MetricName
(	O
RL	O
F1	B-MetricName
)	O
for	O
scoring	O
the	O
generated	O
summary	O
.	O
In	O
addition	O
,	O
we	O
also	O
use	O
the	O
SacreBLEU	B-MetricName
4	O
evaluation	O
metric	O
(	O
Post	O
,	O
2018	O
)	O
.	O
Figure	O
3	O
presents	O
the	O
learning	O
curves	O
for	O
the	O
models	O
(	O
S1	O
and	O
S2	O
)	O
on	O
the	O
development	O
set	O
.	O
It	O
can	O
be	O
seen	O
that	O
there	O
is	O
a	O
variance	O
(	O
e.g.	O
word	O
selection	O
,	O
summary	O
length	O
)	O
for	O
model	O
S2	O
generated	O
summary	O
as	O
compared	O
with	O
model	O
S1	O
.	O
During	O
manual	O
verification	O
,	O
we	O
found	O
that	O
the	O
summaries	O
generated	O
without	O
a	O
minimum	O
length	O
constraint	O
appear	O
better	O
compared	O
to	O
summaries	O
with	O
minimum	O
length	O
constraint	O
.	O
Although	O
we	O
do	O
not	O
explicitly	O
specify	O
a	O
minimum	O
length	O
parameter	O
for	O
generating	O
summaries	O
for	O
the	O
models	O
,	O
the	O
average	O
length	O
of	O
words	O
generated	O
by	O
model	O
S2	O
(	O
e.g.	O
41.42	O
words	O
)	O
is	O
longer	O
than	O
the	O
model	O
S1	O
(	O
e.g.	O
39.81	O
words	O
)	O
.	O
Some	O
data	O
(	O
e.g.	O
name	O
,	O
year	O
)	O
were	O
found	O
inconsistent	O
during	O
a	O
comparison	O
of	O
the	O
generated	O
summary	O
with	O
respect	O
to	O
the	O
reference	O
.	O
There	O
is	O
a	O
variance	O
in	O
summaries	O
generated	O
by	O
model	O
S3	O
as	O
compared	O
to	O
S2	O
and	O
S1	O
.	O
In	O
terms	O
of	O
Rouge	O
score	O
model	O
S3	O
outperforms	O
model	O
S1	O
but	O
perform	O
worse	O
than	O
model	O
S2	O
(	O
see	O
Table	O
2	O
)	O
.	O

Visual	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
(	O
VQA	B-TaskName
)	O
(	O
Antol	O
et	O
al	O
,	O
2015	O
)	O
,	O
the	O
task	O
of	O
answering	O
questions	O
about	O
visual	O
content	O
,	O
was	O
proposed	O
to	O
facilitate	O
the	O
development	O
of	O
models	O
with	O
human	O
-	O
like	O
visual	O
and	O
linguistic	O
understanding	O
.	O
However	O
,	O
existing	O
VQA	B-TaskName
models	O
often	O
exploit	O
superficial	O
statistical	O
biases	O
to	O
produce	O
responses	O
,	O
instead	O
of	O
producing	O
the	O
right	O
answers	O
for	O
the	O
right	O
reasons	O
(	O
Kafle	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
VQA	B-DatasetName
-	I-DatasetName
CP	I-DatasetName
dataset	O
showcases	O
this	O
phenomenon	O
by	O
incorporating	O
different	O
question	O
type	O
/	O
answer	O
distributions	O
in	O
the	O
train	O
and	O
test	O
sets	O
.	O
Since	O
the	O
linguistic	O
priors	O
in	O
the	O
train	O
and	O
test	O
sets	O
differ	O
,	O
models	O
that	O
exploit	O
these	O
priors	O
fail	O
on	O
the	O
test	O
set	O
.	O
To	O
tackle	O
this	O
issue	O
,	O
recent	O
works	O
have	O
endeavored	O
to	O
enforce	O
proper	O
visual	B-TaskName
grounding	I-TaskName
,	O
where	O
the	O
goal	O
is	O
to	O
make	O
models	O
produce	O
answers	O
by	O
looking	O
at	O
relevant	O
visual	O
regions	O
(	O
Gan	O
et	O
al	O
,	O
2017	O
;	O
Selvaraju	O
et	O
al	O
,	O
Figure	O
1	O
:	O
We	O
find	O
that	O
existing	O
visual	O
sensitivity	O
enhancement	O
methods	O
improve	O
performance	O
on	O
VQA	B-TaskName
-	O
CPv2	O
through	O
regularization	O
as	O
opposed	O
to	O
proper	O
visual	B-TaskName
grounding	I-TaskName
.	O
2019	O
;	O
Wu	O
and	O
Mooney	O
,	O
2019	O
)	O
,	O
instead	O
of	O
exploiting	O
linguistic	O
priors	O
.	O
These	O
approaches	O
rely	O
on	O
additional	O
annotations	O
/	O
cues	O
such	O
as	O
human	O
-	O
based	O
attention	O
maps	O
(	O
Das	O
et	O
al	O
,	O
2017	O
)	O
,	O
textual	O
explanations	O
(	O
Huk	O
Park	O
et	O
al	O
,	O
2018	O
)	O
and	O
object	O
label	O
predictions	O
(	O
Ren	O
et	O
al	O
,	O
2015	O
)	O
to	O
identify	O
relevant	O
regions	O
,	O
and	O
train	O
the	O
model	O
to	O
base	O
its	O
predictions	O
on	O
those	O
regions	O
,	O
showing	O
large	O
improvements	O
(	O
8	O
-	O
10	O
%	O
accuracy	B-MetricName
)	O
on	O
the	O
VQA	B-TaskName
-	O
CPv2	O
dataset	O
.	O
Here	O
,	O
we	O
study	O
these	O
methods	O
.	O
We	O
find	O
that	O
their	O
improved	O
accuracy	B-MetricName
does	O
not	O
actually	O
emerge	O
from	O
proper	O
visual	B-TaskName
grounding	I-TaskName
,	O
but	O
from	O
regularization	O
effects	O
,	O
where	O
the	O
model	O
forgets	O
the	O
linguistic	O
priors	O
in	O
the	O
train	O
set	O
,	O
thereby	O
performing	O
better	O
on	O
the	O
test	O
set	O
.	O
To	O
support	O
these	O
claims	O
,	O
we	O
first	O
show	O
that	O
it	O
is	O
possible	O
to	O
achieve	O
such	O
gains	O
even	O
when	O
the	O
model	O
is	O
trained	O
to	O
look	O
at	O
:	O
a	O
)	O
irrelevant	O
visual	O
regions	O
,	O
and	O
b	O
)	O
random	O
visual	O
regions	O
.	O
Second	O
,	O
we	O
show	O
that	O
differences	O
in	O
the	O
predictions	O
from	O
the	O
variants	O
trained	O
with	O
relevant	O
,	O
irrelevant	O
and	O
random	O
visual	O
regions	O
are	O
not	O
statistically	O
significant	O
.	O
Third	O
,	O
we	O
show	O
that	O
these	O
methods	O
degrade	O
performance	O
when	O
the	O
priors	O
remain	O
intact	O
and	O
instead	O
work	O
on	O
VQA	B-TaskName
-	O
CPv2	O
by	O
hurting	O
its	O
train	O
accuracy	B-MetricName
.	O
Based	O
on	O
these	O
observations	O
,	O
we	O
hypothesize	O
that	O
controlled	O
degradation	O
on	O
the	O
train	O
set	O
al	O
ows	O
models	O
to	O
forget	O
the	O
training	O
priors	O
to	O
improve	O
test	O
accuracy	B-MetricName
.	O
To	O
test	O
this	O
hypothesis	O
,	O
we	O
introduce	O
a	O
simple	O
regularization	O
scheme	O
that	O
zeros	O
out	O
the	O
ground	O
truth	O
answers	O
,	O
thereby	O
always	O
penalizing	O
the	O
model	O
,	O
whether	O
the	O
predictions	O
are	O
correct	O
or	O
incorrect	O
.	O
We	O
find	O
that	O
this	O
approach	O
also	O
achieves	O
near	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
(	O
48.9	O
%	O
on	O
VQA	B-TaskName
-	O
CPv2	O
)	O
,	O
providing	O
further	O
support	O
for	O
our	O
claims	O
.	O
While	O
we	O
agree	O
that	O
visual	B-TaskName
grounding	I-TaskName
is	O
a	O
useful	O
direction	O
to	O
pursue	O
,	O
our	O
experiments	O
show	O
that	O
the	O
community	O
requires	O
better	O
ways	O
to	O
test	O
if	O
systems	O
are	O
actually	O
visually	O
grounded	O
.	O
We	O
make	O
some	O
recommendations	O
in	O
the	O
discussion	O
section	O
.	O

Some	O
recent	O
approaches	O
employ	O
a	O
question	O
-	O
only	O
branch	O
as	O
a	O
control	O
model	O
to	O
discover	O
the	O
questions	O
most	O
affected	O
by	O
linguistic	O
correlations	O
.	O
The	O
question	O
-	O
only	O
model	O
is	O
either	O
used	O
to	O
perform	O
adversarial	O
regularization	O
(	O
Grand	O
and	O
Belinkov	O
,	O
2019	O
;	O
Ramakrishnan	O
et	O
al	O
,	O
2018	O
)	O
or	O
to	O
re	O
-	O
scale	O
the	O
loss	B-MetricName
based	O
on	O
the	O
difficulty	O
of	O
the	O
question	O
(	O
Cadene	O
et	O
al	O
,	O
2019	O
)	O
.	O
However	O
,	O
when	O
these	O
ideas	O
are	O
applied	O
to	O
the	O
UpDn	O
model	O
(	O
Anderson	O
et	O
al	O
,	O
2018	O
)	O
,	O
which	O
attempts	O
to	O
learn	O
correct	O
visual	B-TaskName
grounding	I-TaskName
,	O
these	O
approaches	O
achieve	O
4	O
-	O
7	O
%	O
lower	O
accuracy	B-MetricName
compared	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

Both	O
Human	O
Importance	O
Aware	O
Network	O
Tuning	O
(	O
HINT	O
)	O
(	O
Selvaraju	O
et	O
al	O
,	O
2019	O
)	O
and	O
Self	O
Critical	O
Reasoning	O
(	O
SCR	O
)	O
(	O
Wu	O
and	O
Mooney	O
,	O
2019	O
)	O
,	O
train	O
the	O
network	O
to	O
be	O
more	O
sensitive	O
towards	O
salient	O
image	O
regions	O
by	O
improving	O
the	O
alignment	O
between	O
visual	O
cues	O
and	O
gradient	O
-	O
based	O
sensitivity	O
scores	O
.	O
HINT	O
proposes	O
a	O
ranking	O
loss	B-MetricName
between	O
humanbased	O
importance	O
scores	O
(	O
Das	O
et	O
al	O
,	O
2016	O
)	O
and	O
the	O
gradient	O
-	O
based	O
sensitivities	O
.	O
In	O
contrast	O
,	O
SCR	O
does	O
not	O
require	O
exact	O
saliency	O
ranks	O
.	O
Instead	O
,	O
it	O
penalizes	O
the	O
model	O
if	O
correct	O
answers	O
are	O
more	O
sensitive	O
towards	O
non	O
-	O
important	O
regions	O
as	O
compared	O
to	O
important	O
regions	O
,	O
and	O
if	O
incorrect	O
answers	O
are	O
more	O
sensitive	O
to	O
important	O
regions	O
than	O
correct	O
answers	O
.	O

To	O
reduce	O
the	O
reliance	O
on	O
linguistic	O
priors	O
,	O
visual	O
sensitivity	O
enhancement	O
methods	O
attempt	O
to	O
train	O
the	O
model	O
to	O
be	O
more	O
sensitive	O
to	O
relevant	O
visual	O
regions	O
when	O
answering	O
questions	O
.	O
Following	O
(	O
Wu	O
and	O
Mooney	O
,	O
2019	O
)	O
,	O
we	O
define	O
the	O
sensitivity	O
of	O
an	O
answer	O
a	O
with	O
respect	O
to	O
a	O
visual	O
region	O
v	O
i	O
as	O
:	O
S	O
(	O
a	O
,	O
v	O
i	O
)	O
:	O
=	O
(	O
∇	O
v	O
i	O
P	O
(	O
a	O
|	O
I	O
,	O
Q	O
)	O
)	O
T	O
1	O
.	O
(	O
2	O
)	O
Existing	O
methods	O
propose	O
the	O
following	O
training	O
objectives	O
to	O
improve	O
grounding	O
using	O
S	O
:	O
HINT	O
uses	O
a	O
ranking	O
loss	B-MetricName
,	O
which	O
penalizes	O
the	O
model	O
if	O
the	O
pair	O
-	O
wise	O
rankings	O
of	O
the	O
sensitivities	O
of	O
visual	O
regions	O
towards	O
ground	O
truth	O
answers	O
a	O
gt	O
are	O
different	O
from	O
the	O
ranks	O
computed	O
from	O
the	O
human	O
-	O
based	O
attention	O
maps	O
.	O
SCR	O
divides	O
the	O
region	O
proposals	O
into	O
influential	O
and	O
non	O
-	O
influential	O
regions	O
and	O
penalizes	O
the	O
model	O
if	O
:	O
1	O
)	O
S	O
(	O
a	O
gt	O
)	O
of	O
a	O
non	O
-	O
influential	O
region	O
is	O
higher	O
than	O
an	O
influential	O
region	O
,	O
and	O
2	O
)	O
the	O
region	O
most	O
influential	O
for	O
the	O
correct	O
answer	O
has	O
even	O
higher	O
sensitivity	O
for	O
incorrect	O
answers	O
.	O
Both	O
methods	O
improve	O
baseline	O
accuracy	B-MetricName
by	O
8	O
-	O
10	O
%	O
.	O
Is	O
this	O
actually	O
due	O
to	O
better	O
visual	B-TaskName
grounding	I-TaskName
?	O
4	O
Why	O
Did	O
the	O
Performance	O
Improve	O
?	O
We	O
probe	O
the	O
reasons	O
behind	O
the	O
performance	O
improvements	O
of	O
HINT	O
and	O
SCR	O
.	O
We	O
first	O
analyze	O
if	O
the	O
results	O
improve	O
even	O
when	O
the	O
visual	O
cues	O
are	O
irrelevant	O
(	O
Sec	O
.	O
4.2	O
)	O
or	O
random	O
(	O
Sec	O
.	O
4.3	O
)	O
and	O
examine	O
if	O
their	O
differences	O
are	O
statistically	O
significant	O
(	O
Sec	O
.	O
4.4	O
)	O
.	O
Then	O
,	O
we	O
analyze	O
the	O
regularization	O
effects	O
by	O
evaluating	O
the	O
performance	O
on	O
VQA	B-TaskName
-	O
CPv2	O
's	O
train	O
split	O
(	O
Sec	O
.	O
4.5	O
)	O
and	O
the	O
behavior	O
on	O
a	O
dataset	O
without	O
changing	O
priors	O
(	O
Sec	O
.	O
4.6	O
)	O
.	O
We	O
present	O
a	O
new	O
metric	O
to	O
assess	O
visual	B-TaskName
grounding	I-TaskName
in	O
Sec	O
.	O
4.7	O
and	O
describe	O
our	O
regularization	O
method	O
in	O
Sec	O
.	O
5	O
.	O

To	O
test	O
if	O
the	O
changes	O
in	O
results	O
were	O
statistically	O
significant	O
,	O
we	O
performed	O
Welch	O
's	O
t	O
-	O
tests	O
(	O
Welch	O
,	O
1938	O
)	O
on	O
the	O
predictions	O
of	O
the	O
variants	O
trained	O
on	O
relevant	O
,	O
irrelevant	O
and	O
random	O
cues	O
.	O
We	O
pick	O
Welch	O
's	O
t	O
-	O
test	O
over	O
the	O
Student	O
's	O
t	O
-	O
test	O
,	O
because	O
the	O
latter	O
assumes	O
equal	O
variances	O
for	O
predictions	O
from	O
different	O
variants	O
.	O
To	O
perform	O
the	O
tests	O
,	O
we	O
first	O
randomly	O
sample	O
5000	O
subsets	O
of	O
non	O
-	O
overlapping	O
test	O
instances	O
.	O
We	O
then	O
average	O
the	O
accuracy	B-MetricName
of	O
each	O
subset	O
across	O
5	O
runs	O
,	O
obtaining	O
5000	O
values	O
.	O
Next	O
,	O
we	O
run	O
the	O
t	O
-	O
tests	O
for	O
HINT	O
and	O
SCR	O
separately	O
on	O
the	O
subset	O
accuracies	O
.	O
As	O
shown	O
in	O
Table	O
2	O
,	O
the	O
p	O
-	O
values	O
across	O
the	O
variants	O
of	O
HINT	O
and	O
SCR	O
are	O
greater	O
than	O
or	O
equal	O
to	O
0.3	O
.	O
Using	O
a	O
confidence	O
level	O
of	O
95	O
%	O
(	O
α	B-HyperparameterName
=	O
0.05	O
)	O
,	O
we	O
fail	O
to	O
reject	O
the	O
null	O
hypothesis	O
that	O
the	O
mean	O
difference	O
between	O
the	O
paired	O
values	O
is	O
0	B-DatasetName
,	O
showing	O
that	O
the	O
variants	O
are	O
not	O
statistically	O
significantly	O
different	O
from	O
each	O
other	O
.	O
We	O
also	O
compare	O
the	O
predictions	O
of	O
HINT	O
/	O
SCR	O
against	O
baseline	O
,	O
and	O
find	O
that	O
p	O
-	O
values	O
are	O
all	O
zeros	O
,	O
showing	O
that	O
the	O
differences	O
have	O
statistical	O
significance	O
.	O
Percentage	O
of	O
Overlaps	O
:	O
To	O
further	O
check	O
if	O
the	O
variants	O
trained	O
on	O
irrelevant	O
or	O
random	O
regions	O
gain	O
performance	O
in	O
a	O
manner	O
similar	O
to	O
the	O
models	O
trained	O
on	O
relevant	O
regions	O
,	O
we	O
compute	O
the	O
overlap	O
between	O
their	O
predictions	O
on	O
VQA	B-TaskName
-	O
CPv2	O
's	O
test	O
set	O
.	O
The	O
percentage	O
of	O
overlap	O
is	O
defined	O
as	O
:	O
%	O
Overlap	O
=	O
n	O
same	O
n	O
total	O
×	O
100	O
%	O
,	O
where	O
,	O
n	O
same	O
denotes	O
the	O
number	O
of	O
instances	O
where	O
either	O
both	O
variants	O
were	O
correct	O
or	O
both	O
were	O
incorrect	O
and	O
n	O
total	O
denotes	O
the	O
total	O
number	O
of	O
test	O
instances	O
.	O
As	O
shown	O
in	O
Table	O
2	O
,	O
we	O
compare	O
%	O
Overlap	O
between	O
different	O
variants	O
of	O
HINT	O
/	O
SCR	O
with	O
baseline	O
and	O
against	O
each	O
other	O
.	O
We	O
find	O
89.7	O
−	O
91.9	O
%	O
and	O
89.5	O
−	O
92.0	O
%	O
overlaps	O
for	O
different	O
variants	O
of	O
HINT	O
and	O
SCR	O
respectively	O
.	O
These	O
high	O
overlaps	O
suggest	O
that	O
the	O
variants	O
are	O
not	O
working	O
in	O
fundamentally	O
different	O
manners	O
.	O

We	O
compare	O
the	O
training	O
accuracies	O
to	O
analyze	O
the	O
regularization	O
effects	O
.	O
As	O
shown	O
in	O
Table	O
1	O
,	O
the	O
baseline	O
method	O
has	O
the	O
highest	O
training	O
results	O
,	O
while	O
the	O
other	O
methods	O
cause	O
6.0	O
−	O
14.0	O
%	O
and	O
3.3−10.5	O
%	O
drops	O
in	O
the	O
training	O
accuracy	B-MetricName
on	O
VQA	B-TaskName
-	O
CPv2	O
and	O
VQAv2	O
,	O
respectively	O
.	O
We	O
hypothesize	O
that	O
degrading	O
performance	O
on	O
the	O
train	O
set	O
helps	O
forget	O
linguistic	O
biases	O
,	O
which	O
in	O
turn	O
helps	O
accuracy	B-MetricName
on	O
VQA	B-TaskName
-	O
CPv2	O
's	O
test	O
set	O
but	O
hurts	O
accuracy	B-MetricName
on	O
VQAv2	O
's	O
val	O
set	O
.	O

The	O
usage	O
of	O
visual	O
cues	O
and	O
sensitivities	O
in	O
existing	O
methods	O
is	O
superfluous	O
because	O
the	O
results	O
indicate	O
that	O
performance	O
improves	O
through	O
degradation	O
of	O
training	O
accuracy	B-MetricName
.	O
We	O
hypothesize	O
that	O
simple	O
regularization	O
that	O
does	O
not	O
rely	O
on	O
cues	O
or	O
sensitivities	O
can	O
also	O
achieve	O
large	O
performance	O
gains	O
for	O
VQA	B-DatasetName
-	I-DatasetName
CP	I-DatasetName
.	O
To	O
test	O
this	O
hypothesis	O
,	O
we	O
devise	O
a	O
simple	O
loss	B-MetricName
function	O
which	O
continuously	O
degrades	O
the	O
training	O
accuracy	B-MetricName
by	O
training	O
the	O
network	O
to	O
always	O
predict	O
a	O
score	O
of	O
zero	O
for	O
all	O
possible	O
answers	O
i.e.	O
produce	O
a	O
zero	O
vector	O
(	O
0	B-DatasetName
)	O
.	O
The	O
overall	O
loss	B-MetricName
function	O
can	O
be	O
written	O
as	O
:	O
L	O
:	O
=	O
BCE	O
(	O
P	O
(	O
A	O
)	O
,	O
A	O
gt	O
)	O
+	O
λBCE	O
(	O
P	O
(	O
A	O
)	O
,	O
0	B-DatasetName
)	O
,	O
where	O
,	O
BCE	O
refers	O
to	O
the	O
binary	O
cross	O
entropy	O
loss	B-MetricName
and	O
P	O
(	O
A	O
)	O
is	O
a	O
vector	O
consisting	O
of	O
predicted	O
scores	O
for	O
all	O
possible	O
answers	O
.	O
The	O
first	O
term	O
is	O
the	O
binary	O
cross	O
entropy	O
loss	B-MetricName
between	O
model	O
predictions	O
and	O
ground	O
truth	O
answer	O
vector	O
(	O
A	O
gt	O
)	O
,	O
and	O
the	O
second	O
term	O
is	O
our	O
regularizer	O
with	O
a	O
coefficient	O
of	O
λ	O
=	O
1	O
.	O
Note	O
that	O
this	O
regularizer	O
continually	O
penalizes	O
the	O
model	O
during	O
the	O
course	O
of	O
the	O
training	O
,	O
whether	O
its	O
predictions	O
are	O
correct	O
or	O
incorrect	O
.	O
As	O
shown	O
in	O
Table	O
1	O
,	O
we	O
present	O
results	O
when	O
this	O
loss	B-MetricName
is	O
used	O
on	O
:	O
a	O
)	O
Fixed	O
subset	O
covering	O
1	O
%	O
of	O
the	O
dataset	O
,	O
b	O
)	O
Varying	O
subset	O
covering	O
1	O
%	O
of	O
the	O
dataset	O
,	O
where	O
a	O
new	O
random	O
subset	O
is	O
sampled	O
every	O
epoch	O
and	O
c	O
)	O
100	O
%	O
of	O
the	O
dataset	O
.	O
Confirming	O
our	O
hypothesis	O
,	O
all	O
variants	O
of	O
our	O
model	O
achieve	O
near	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
,	O
solidifying	O
our	O
claim	O
that	O
the	O
performance	O
gains	O
for	O
recent	O
methods	O
come	O
from	O
regularization	O
effects	O
.	O
It	O
is	O
also	O
interesting	O
to	O
note	O
that	O
the	O
drop	O
in	O
training	O
accuracy	B-MetricName
is	O
lower	O
with	O
this	O
regularization	O
scheme	O
as	O
compared	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O
Of	O
course	O
,	O
if	O
any	O
model	O
was	O
actually	O
visually	O
grounded	O
,	O
then	O
we	O
would	O
expect	O
it	O
to	O
improve	O
performances	O
on	O
both	O
train	O
and	O
test	O
sets	O
.	O
We	O
do	O
not	O
observe	O
such	O
behavior	O
in	O
any	O
of	O
the	O
methods	O
,	O
indicating	O
that	O
they	O
are	O
not	O
producing	O
right	O
answers	O
for	O
the	O
right	O
reasons	O
.	O

While	O
our	O
results	O
indicate	O
that	O
current	O
visual	B-TaskName
grounding	I-TaskName
based	O
bias	O
mitigation	O
approaches	O
do	O
not	O
suffice	O
,	O
we	O
believe	O
this	O
is	O
still	O
a	O
good	O
research	O
direction	O
.	O
However	O
,	O
future	O
methods	O
must	O
seek	O
to	O
verify	O
that	O
performance	O
gains	O
are	O
not	O
stemming	O
from	O
spurious	O
sources	O
by	O
using	O
an	O
experimental	O
setup	O
similar	O
to	O
that	O
presented	O
in	O
this	O
paper	O
.	O
We	O
recommend	O
that	O
both	O
train	O
and	O
test	O
accuracy	B-MetricName
be	O
reported	O
,	O
because	O
a	O
model	O
truly	O
capable	O
of	O
visual	B-TaskName
grounding	I-TaskName
would	O
not	O
cause	O
drastic	O
drops	O
in	O
training	O
accuracy	B-MetricName
to	O
do	O
well	O
on	O
the	O
test	O
sets	O
.	O
Finally	O
,	O
we	O
advocate	O
for	O
creating	O
a	O
dataset	O
with	O
ground	O
truth	O
grounding	O
available	O
for	O
100	O
%	O
of	O
the	O
instances	O
using	O
synthetically	O
generated	O
datasets	O
Kafle	O
et	O
al	O
,	O
2018	O
;	O
Acharya	O
et	O
al	O
,	O
2019b	O
;	O
Hudson	O
and	O
Manning	O
,	O
2019	O
;	O
Johnson	O
et	O
al	O
,	O
2017	O
)	O
,	O
enabling	O
the	O
community	O
to	O
evaluate	O
if	O
their	O
methods	O
are	O
able	O
to	O
focus	O
on	O
relevant	O
information	O
.	O
Another	O
alternative	O
is	O
to	O
use	O
tasks	O
that	O
explicitly	O
test	O
grounding	O
,	O
e.g.	O
,	O
in	O
visual	O
query	O
detection	O
an	O
agent	B-DatasetName
must	O
output	O
boxes	O
around	O
any	O
regions	O
of	O
a	O
scene	O
that	O
match	O
the	O
natural	O
language	O
query	O
(	O
Acharya	O
et	O
al	O
,	O
2019a	O
)	O
.	O

We	O
compare	O
four	O
different	O
variants	O
of	O
HINT	O
and	O
SCR	O
to	O
study	O
the	O
causes	O
behind	O
the	O
improvements	O
including	O
the	O
models	O
that	O
are	O
fine	O
-	O
tuned	O
on	O
:	O
1	O
)	O
relevant	O
regions	O
(	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
)	O
2	O
)	O
irrelevant	O
regions	O
3	O
)	O
fixed	O
random	O
regions	O
and	O
4	O
)	O
variable	O
random	O
regions	O
.	O
For	O
all	O
variants	O
,	O
we	O
fine	O
-	O
tune	O
a	O
pretrained	O
UpDn	O
,	O
which	O
was	O
trained	O
on	O
either	O
VQA	B-TaskName
-	O
CPv2	O
or	O
VQAv2	O
for	O
40	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
10	O
−3	O
.	O
When	O
fine	O
-	O
tuning	O
with	O
HINT	O
,	O
SCR	O
or	O
our	O
method	O
,	O
we	O
also	O
use	O
the	O
main	O
binary	O
cross	O
entropy	O
VQA	B-TaskName
loss	B-MetricName
,	O
whose	O
weight	O
is	O
set	O
to	O
1	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
384	O
for	O
all	O
of	O
the	O
experiments	O
.	O

Following	O
(	O
Selvaraju	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
train	O
HINT	O
on	O
the	O
subset	O
with	O
human	O
-	O
based	O
attention	O
maps	O
(	O
Das	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
are	O
available	O
for	O
9	O
%	O
of	O
the	O
VQA	B-TaskName
-	O
CPv2	O
train	O
and	O
test	O
sets	O
.	O
The	O
same	O
subset	O
is	O
used	O
for	O
VQAv2	O
too	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
2	O
×	O
10	O
−5	O
and	O
the	O
weight	O
for	O
the	O
HINT	O
loss	B-MetricName
is	O
set	O
to	O
2	O
.	O

Since	O
(	O
Wu	O
and	O
Mooney	O
,	O
2019	O
)	O
reported	O
that	O
humanbased	O
textual	O
explanations	O
(	O
Huk	O
Park	O
et	O
al	O
,	O
2018	O
)	O
gave	O
better	O
results	O
than	O
human	O
-	O
based	O
attention	O
maps	O
for	O
SCR	O
,	O
we	O
train	O
all	O
of	O
the	O
SCR	O
variants	O
on	O
the	O
subset	O
containing	O
textual	O
explanation	O
-	O
based	O
cues	O
.	O
SCR	O
is	O
trained	O
in	O
two	O
phases	O
.	O
For	O
the	O
first	O
phase	O
,	O
which	O
strengthens	O
the	O
influential	O
objects	O
,	O
we	O
use	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5	O
×	O
10	O
−5	O
,	O
loss	B-MetricName
weight	O
of	O
3	O
and	O
train	O
the	O
model	O
to	O
a	O
maximum	O
of	O
12	O
epochs	O
.	O
Then	O
,	O
following	O
(	O
Wu	O
and	O
Mooney	O
,	O
2019	O
)	O
,	O
for	O
the	O
second	O
phase	O
,	O
we	O
use	O
the	O
best	O
performing	O
model	O
from	O
the	O
first	O
phase	O
to	O
train	O
the	O
second	O
phase	O
,	O
which	O
criticizes	O
incorrect	O
dominant	O
answers	O
.	O
For	O
the	O
second	O
phase	O
,	O
we	O
use	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
10	O
−4	O
and	O
weight	O
of	O
1000	O
,	O
which	O
is	O
applied	O
alongside	O
the	O
loss	B-MetricName
term	O
used	O
in	O
the	O
first	O
phase	O
.	O
The	O
specified	O
hyperparameters	O
worked	O
better	O
for	O
us	O
than	O
the	O
values	O
provided	O
in	O
the	O
original	O
paper	O
.	O
Our	O
Zero	O
-	O
Out	O
Regularizer	O
Our	O
regularization	O
method	O
,	O
which	O
is	O
a	O
binary	O
cross	O
entropy	O
loss	B-MetricName
between	O
the	O
model	O
predictions	O
and	O
a	O
zero	O
vector	O
,	O
does	O
not	O
use	O
additional	O
cues	O
or	O
sensitivities	O
and	O
yet	O
achieves	O
near	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
VQA	B-TaskName
-	O
CPv2	O
.	O
We	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
:	O
2×10	O
−6	O
r	O
,	O
where	O
r	O
is	O
the	O
ratio	O
of	O
the	O
training	O
instances	O
used	O
for	O
fine	O
-	O
tuning	O
.	O
The	O
weight	O
for	O
the	O
loss	B-MetricName
is	O
set	O
to	O
2	O
.	O
We	O
report	O
the	O
performance	O
obtained	O
at	O
the	O
8	O
th	O
epoch	O
.	O

We	O
test	O
our	O
regularization	O
method	O
on	O
random	O
subsets	O
of	O
varying	O
sizes	O
.	O
Fig	O
.	O
A6	O
shows	O
the	O
results	O
when	O
we	O
apply	O
our	O
loss	B-MetricName
to	O
1	O
−	O
100	O
%	O
of	O
the	O
training	O
instances	O
.	O
Clearly	O
,	O
the	O
ability	O
to	O
regularize	O
the	O
model	O
does	O
not	O
vary	O
much	O
with	O
respect	O
to	O
the	O
size	O
of	O
the	O
train	O
subset	O
,	O
with	O
the	O
best	O
performance	O
occurring	O
when	O
our	O
loss	B-MetricName
is	O
applied	O
to	O
1	O
%	O
of	O
the	O
training	O
instances	O
.	O
These	O
results	O
support	O
our	O
claims	O
that	O
it	O
is	O
possible	O
to	O
improve	O
performance	O
without	O
actually	O
performing	O
visual	B-TaskName
grounding	I-TaskName
.	O
Q	O
:	O
Is	O
this	O
food	O
sweet	O
?	O
A	O
:	O
yes	O
Remarks	O
:	O
The	O
most	O
sensitive	O
regions	O
for	O
irrelevant	O
/	O
random	O
variants	O
do	O
not	O
contain	O
food	O
,	O
yet	O
their	O
answers	O
are	O
correct	O
.	O

HINT	O
trained	O
on	O
relevant	O
cues	O
HINT	O
trained	O
on	O
irrelevant	O
cues	O
HINT	O
trained	O
on	O
random	O
cues	O
Q	O
:	O
Has	O
the	O
boy	O
worn	O
out	O
his	O
jeans	O
?	O
A	O
:	O
yes	O
Remarks	O
:	O
All	O
of	O
the	O
variants	O
look	O
at	O
both	O
relevant	O
and	O
irrelevant	O
regions	O
to	O
produce	O
correct	O
answer	O
.	O
Q	O
:	O
Is	O
the	O
sport	O
being	O
played	O
tennis	O
or	O
volleyball	O
?	O
A	O
:	O
tennis	O
Remarks	O
:	O
None	O
of	O
the	O
variants	O
look	O
at	O
relevant	O
regions	O
,	O
and	O
yet	O
produce	O
correct	O
answer	O
.	O
Q	O
:	O
What	O
is	O
the	O
swimmer	O
doing	O
?	O
A	O
:	O
surfing	O
Remarks	O
:	O
Models	O
trained	O
on	O
irrelevant	O
/	O
random	O
cues	O
do	O
not	O
look	O
at	O
the	O
swimmer	O
at	O
all	O
,	O
yet	O
produce	O
correct	O
answer	O
.	O
We	O
pick	O
samples	O
where	O
all	O
variants	O
produce	O
correct	O
response	O
to	O
the	O
question	O
.	O
The	O
first	O
column	O
shows	O
ground	O
truth	O
regions	O
and	O
columns	O
2	O
-	O
4	O
show	O
visualizations	O
from	O
HINT	O
trained	O
on	O
relevant	O
,	O
irrelevant	O
and	O
fixed	O
random	O
regions	O
respectively	O
.	O
Figure	O
A5	O
:	O
%	O
CPIG	O
for	O
baseline	O
and	O
different	O
variants	O
of	O
SCR	O
and	O
our	O
method	O
,	O
computed	O
using	O
ground	O
truth	O
relevant	O
regions	O
taken	O
from	O
textual	O
explanations	O
(	O
txt	O
)	O
.	O
Train	O
Test	O
Figure	O
A6	O
:	O
The	O
regularization	O
effect	O
of	O
our	O
loss	B-MetricName
is	O
invariant	O
with	O
respect	O
to	O
the	O
dataset	O
size	O
.	O

Following	O
the	O
first	O
treebanking	O
efforts	O
,	O
in	O
English	O
(	O
Marcus	O
et	O
al	O
,	O
1993	O
)	O
,	O
Chinese	O
(	O
Xue	O
et	O
al	O
,	O
2005	O
)	O
,	O
and	O
Arabic	O
(	O
Maamouri	O
and	O
Bies	O
,	O
2004	O
)	O
,	O
and	O
with	O
the	O
surge	O
of	O
interest	O
in	O
developing	O
statistical	O
,	O
broad	O
-	O
coverage	O
,	O
parsing	O
models	O
,	O
Sima'an	O
et	O
al	O
(	O
2001	O
)	O
introduced	O
a	O
pilot	O
treebanking	O
study	O
and	O
a	O
Hebrew	O
treebank	O
(	O
HTB	O
)	O
,	O
which	O
included	O
500	O
sentences	O
from	O
the	O
Hebrew	O
newspaper	O
ha'aretz	O
,	O
morphologically	O
segmented	O
and	O
morpho	O
-	O
syntactically	O
annotated	O
with	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tags	O
,	O
morphological	O
features	O
,	O
and	O
labeled	O
phrase	O
-	O
structure	O
trees	O
.	O
Following	O
the	O
annotation	O
practices	O
at	O
the	O
time	O
,	O
much	O
of	O
the	O
tagging	O
and	O
labeling	O
scheme	O
was	O
adopted	O
almost	O
as	O
is	O
from	O
the	O
UPenn	O
Treebank	O
(	O
Marcus	O
et	O
al	O
,	O
1993	O
)	O
.	O
However	O
,	O
due	O
to	O
its	O
rich	O
morphology	O
and	O
Semitic	O
phenomena	O
,	O
several	O
annotation	O
decisions	O
in	O
the	O
HTB	O
diverged	O
from	O
these	O
practices	O
.	O
Firstly	O
,	O
the	O
basic	O
units	O
that	O
appear	O
as	O
leaves	O
of	O
the	O
trees	O
are	O
not	O
space	O
-	O
delimited	O
tokens	O
,	O
but	O
segmented	O
units	O
that	O
we	O
call	O
morphemes	O
.	O
1	O
Various	O
prefixes	O
that	O
mark	O
independent	O
function	O
words	O
,	O
including	O
2	O
B	O
(	O
in	O
)	O
,	O
L	O
(	O
to	O
)	O
,	O
M	O
(	O
from	O
)	O
,	O
F	O
(	O
that	O
)	O
,	O
KF	O
(	O
when	O
)	O
and	O
H	O
(	O
definite	O
article	O
)	O
are	O
segmented	O
away	O
from	O
their	O
host	O
.	O
In	O
addition	O
,	O
pronominal	O
suffixes	O
that	O
appear	O
on	O
top	O
of	O
function	O
words	O
are	O
also	O
segmented	O
away	O
.	O
So	O
,	O
the	O
tokens	O
FLW	O
(	O
of	O
him	O
)	O
,	O
LK	O
(	O
to	O
you	O
)	O
,	O
and	O
AITM	O
(	O
with	O
them	O
)	O
,	O
are	O
segmented	O
into	O
FL	O
(	O
of	O
)	O
+	O
HWA	O
(	O
he	O
)	O
,	O
L	O
(	O
to	O
)	O
+	O
ATH	O
(	O
you	O
)	O
,	O
EM	B-MetricName
(	O
with	O
)	O
+	O
HM	O
(	O
them	O
)	O
respectively	O
.	O
3	O
The	O
POS	O
tags	O
labeling	O
scheme	O
in	O
the	O
HTB	O
includes	O
quite	O
a	O
few	O
changes	O
from	O
PTB	B-DatasetName
,	O
including	O
the	O
addition	O
of	O
special	O
tags	O
lexicalizing	O
important	O
functional	O
elements	O
in	O
Hebrew	O
:	O
AT	O
(	O
for	O
the	O
accusative	O
marker	O
)	O
,	O
H	O
(	O
the	O
definite	O
article	O
)	O
,	O
POSS	O
(	O
the	O
possesive	O
marker	O
)	O
,	O
and	O
HAM	B-DatasetName
(	O
the	O
yes	O
/	O
no	O
question	O
marker	O
)	O
.	O
In	O
addition	O
,	O
the	O
HTB	O
introduces	O
the	O
NNT	O
,	O
JJT	O
,	O
CDT	O
labels	O
,	O
marking	O
the	O
constructstate	O
variants	O
of	O
NN	O
,	O
JJ	O
,	O
CD	O
in	O
the	O
PTB	B-DatasetName
,	O
and	O
a	O
specific	O
tag	O
MOD	B-DatasetName
that	O
tags	O
modifier	O
words	O
which	O
is	O
neither	O
an	O
adjective	O
nor	O
an	O
adverb	O
.	O
On	O
top	O
of	O
that	O
,	O
all	O
open	O
class	O
POS	O
tags	O
as	O
well	O
as	O
auxiliaries	O
have	O
been	O
marked	O
for	O
their	O
inflectional	O
features	O
(	O
gender	O
,	O
number	O
,	O
person	O
,	O
time	O
)	O
,	O
yielding	O
in	O
total	O
hundreds	O
of	O
possible	O
fine	O
-	O
grained	O
POS	O
categories	O
.	O
The	O
syntactic	O
labels	O
in	O
the	O
phrase	O
structure	O
trees	O
of	O
the	O
HTB	O
were	O
adopted	O
from	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
(	O
PTB	B-DatasetName
)	O
almost	O
as	O
is	O
,	O
with	O
the	O
addition	O
of	O
a	O
PREDP	O
label	O
for	O
marking	O
verbless	O
predicates	O
.	O
The	O
syntactic	O
trees	O
themselves	O
looked	O
superficially	O
like	O
the	O
PTB	B-DatasetName
but	O
they	O
differ	O
in	O
several	O
aspects	O
.	O
Due	O
to	O
word	O
-	O
order	O
freedom	O
at	O
the	O
clause	O
level	O
,	O
S	O
-	O
level	O
categories	O
present	O
a	O
flat	O
structure	O
,	O
where	O
the	O
positions	O
of	O
the	O
arguments	O
do	O
not	O
entail	O
anything	O
about	O
their	O
grammatical	O
function	O
.	O
The	O
HTB	O
provided	O
3	O
types	O
of	O
manually	O
verified	O
function	O
tags	O
to	O
indicate	O
such	O
functions	O
:	O
SUBJect	O
,	O
OBJect	O
,	O
and	O
COMplement	O
,	O
the	O
latter	O
marking	O
obligatory	O
arguments	O
of	O
the	O
verb	O
.	O
Finally	O
the	O
HTB	O
defined	O
three	O
types	O
of	O
null	O
elements	O
:	O
*	O
T	O
*	O
marking	O
phonologically	O
empty	O
anaphors	O
,	O
*	O
PRO	O
*	O
for	O
pro	O
-	O
drop	O
subjects	O
,	O
and	O
*	O
NONE	O
*	O
for	O
elliptical	O
elements	O
.	O
The	O
work	O
of	O
Guthmann	O
et	O
al	O
(	O
2008	O
)	O
extended	O
the	O
HTB	O
to	O
6501	O
sentences	O
,	O
in	O
a	O
manuallyvalidated	O
automatic	O
process	O
.	O
4	O
During	O
this	O
process	O
they	O
further	O
added	O
a	O
systematic	O
marking	O
of	O
mother	O
-	O
daughter	O
dependencies	O
.	O
That	O
is	O
-	O
due	O
to	O
feature	O
-	O
spreading	O
in	O
Hebrew	O
,	O
morphological	O
features	O
of	O
phrases	O
may	O
be	O
contributed	O
by	O
different	O
daughters	O
,	O
and	O
not	O
necessarily	O
via	O
a	O
single	O
head	O
.	O
So	O
they	O
marked	O
each	O
daughter	O
with	O
the	O
role	O
it	O
plays	O
in	O
determining	O
its	O
mothers	O
'	O
features	O
(	O
gender	O
,	O
number	O
,	O
tense	O
,	O
etc	O
.	O
)	O
.	O
Using	O
these	O
feature	O
-	O
based	O
dependencies	O
,	O
they	O
performed	O
feature	O
-	O
percolation	O
from	O
daughter	O
to	O
mother	O
,	O
so	O
that	O
phrasal	O
nodes	O
are	O
also	O
marked	O
with	O
their	O
morphological	O
signatures	O
.	O
5	O
Still	O
,	O
the	O
phrase	O
-	O
structure	O
trees	O
yielded	O
by	O
HTBtrained	O
parsers	O
were	O
not	O
useful	O
for	O
downstream	O
applications	O
in	O
Hebrew	O
.	O
This	O
is	O
because	O
Hebrew	O
is	O
a	O
relatively	O
-	O
free	O
word	O
order	O
language	O
,	O
where	O
the	O
position	O
of	O
a	O
constituent	O
does	O
not	O
entail	O
its	O
grammatical	O
function	O
or	O
semantic	O
role	O
.	O
This	O
in	O
particular	O
precludes	O
the	O
use	O
of	O
well	O
known	O
'	O
head	O
tables	O
'	O
for	O
selecting	O
a	O
single	O
head	O
and	O
deriving	O
labeled	O
and	O
unlabeled	O
dependencies	O
.	O
To	O
overcome	O
this	O
,	O
Tsarfaty	O
(	O
2010	O
)	O
devised	O
a	O
set	O
of	O
rules	O
based	O
on	O
the	O
daughter	O
-	O
dependencies	O
,	O
function	O
tags	O
and	O
empty	O
elements	O
,	O
to	O
automatically	O
derive	O
the	O
relationalrealizational	O
(	O
RR	B-DatasetName
)	O
version	O
of	O
the	O
HTB	O
.	O
In	O
the	O
RR	B-DatasetName
HTB	O
,	O
each	O
node	O
is	O
marked	O
with	O
its	O
relational	O
network	O
(	O
an	O
unordered	O
set	O
of	O
grammatical	O
functions	O
)	O
mapped	O
to	O
the	O
ordered	O
syntactic	O
constituents	O
.	O
The	O
RR	B-DatasetName
HTB	O
retained	O
the	O
morphological	O
conventions	O
and	O
core	O
non	O
-	O
core	O
distinction	O
of	O
the	O
original	O
HTB	O
.	O
In	O
a	O
parallel	O
effort	O
,	O
and	O
with	O
the	O
surge	O
of	O
interest	O
in	O
dependency	B-TaskName
parsing	I-TaskName
(	O
Buchholz	O
and	O
Marsi	O
,	O
2006	O
;	O
Nivre	O
et	O
al	O
,	O
2007	O
)	O
,	O
6	O
Goldberg	O
(	O
2011	O
)	O
automatically	O
converted	O
the	O
HTB	O
into	O
its	O
first	O
,	O
unlabeled	O
,	O
dependency	O
version	O
.	O
The	O
automatic	O
conversion	O
procedure	O
assumed	O
that	O
heads	O
are	O
functional	O
rather	O
than	O
lexical	O
.	O
As	O
a	O
result	O
,	O
the	O
coordination	O
marker	O
would	O
head	O
coordination	O
structures	O
,	O
the	O
accusative	O
marker	O
would	O
head	O
direct	O
object	O
phrases	O
,	O
and	O
so	O
on	O
.	O
On	O
top	O
of	O
that	O
,	O
in	O
order	O
to	O
remain	O
compatible	O
with	O
the	O
wide	O
-	O
coverage	O
lexicon	O
of	O
Itai	O
and	O
Wintner	O
(	O
2008	O
)	O
,	O
this	O
version	O
of	O
the	O
HTB	O
adopted	O
the	O
POS	O
tags	O
scheme	O
of	O
Adler	O
(	O
2007	O
)	O
,	O
rather	O
than	O
the	O
POS	O
tags	O
of	O
Sima'an	O
et	O
al	O
(	O
2001	O
)	O
Based	O
on	O
this	O
version	O
,	O
Goldberg	O
and	O
Elhadad	O
(	O
2009	O
)	O
presented	O
the	O
first	O
Hebrew	O
dependency	B-TaskName
parsing	I-TaskName
results	O
,	O
only	O
unlabeled	O
attachment	O
scores	O
(	O
UAS	O
)	O
at	O
this	O
point	O
.	O
Here	O
too	O
,	O
as	O
with	O
the	O
phrasestructure	O
trees	O
,	O
it	O
was	O
impossible	O
to	O
devise	O
an	O
external	O
procedure	O
that	O
would	O
infer	O
dependency	O
labels	O
for	O
the	O
unlabeled	O
arcs	O
-	O
and	O
there	O
were	O
no	O
labeled	O
dependencies	O
to	O
train	O
such	O
a	O
labeler	O
on	O
.	O
At	O
that	O
point	O
,	O
where	O
the	O
need	O
for	O
Hebrew	O
labeled	O
dependencies	O
had	O
become	O
pressing	O
,	O
Tsarfaty	O
(	O
2013	O
)	O
presented	O
the	O
Unified	O
-	O
Stanford	O
Dependencies	O
(	O
Unified	O
-	O
SD	O
)	O
version	O
of	O
the	O
HTB	O
,	O
extending	O
the	O
Stanford	O
dependencies	O
(	O
SD	O
)	O
scheme	O
to	O
cover	O
both	O
morphological	O
and	O
syntactic	O
phenomena	O
.	O
Similar	O
to	O
SD	O
,	O
U	O
-	O
SD	O
assumed	O
a	O
labeling	O
hierarchy	O
,	O
with	O
several	O
changes	O
:	O
the	O
hierarchy	O
now	O
included	O
branches	O
for	O
head	O
-	O
types	O
(	O
hd	O
)	O
,	O
dependency	O
types	O
(	O
dep	O
)	O
,	O
and	O
functional	O
types	O
(	O
func	O
)	O
.	O
In	O
particular	O
,	O
dependencies	O
in	O
the	O
func	O
branch	O
mark	O
syntactic	O
functions	O
that	O
are	O
in	O
fact	O
interchangeable	O
with	O
morphology	O
,	O
when	O
considering	O
these	O
functions	O
from	O
a	O
typological	O
perspective	O
.	O
Tsarfaty	O
used	O
the	O
U	O
-	O
SD	O
labels	O
to	O
edit	O
three	O
versions	O
of	O
the	O
HTB	O
:	O
(	O
i	O
)	O
to	O
mark	O
the	O
original	O
phrasestructure	O
trees	O
in	O
the	O
HTB	O
with	O
the	O
labels	O
as	O
dashfeatures	O
,	O
(	O
ii	O
)	O
to	O
relabel	O
the	O
relational	O
networks	O
in	O
RR	B-DatasetName
trees	O
with	O
U	O
-	O
SD	O
labels	O
,	O
and	O
(	O
iii	O
)	O
to	O
derive	O
a	O
labeled	O
dependencies	O
version	O
of	O
the	O
HTB	O
.	O
As	O
with	O
the	O
unlabeled	O
dependencies	O
of	O
Goldberg	O
,	O
the	O
U	O
-	O
SD	O
HTB	O
assumed	O
functional	O
heads	O
across	O
the	O
board	O
,	O
and	O
the	O
POS	O
tags	O
layer	O
was	O
again	O
changed	O
to	O
comply	O
with	O
the	O
wide	O
-	O
coverage	O
lexicon	O
(	O
HEBLEX	O
)	O
of	O
Itai	O
and	O
Wintner	O
(	O
2008	O
)	O
.	O
The	O
labeled	O
dependencies	O
treebank	O
of	O
U	O
-	O
SD	O
then	O
provided	O
the	O
Hebrew	O
section	O
of	O
the	O
SPMRL	O
shared	O
tasks	O
(	O
Seddah	O
et	O
al	O
,	O
2013	O
(	O
Seddah	O
et	O
al	O
,	O
,	O
2014	O
.	O
3	O
The	O
Hebrew	O
UD	B-DatasetName
Treebank	O

The	O
present	O
version	O
of	O
UDv2	O
thus	O
results	O
from	O
a	O
sequence	O
of	O
automatic	O
and	O
semi	O
-	O
automatic	O
conversions	O
on	O
the	O
trees	O
of	O
Guthmann	O
et	O
al	O
(	O
2008	O
)	O
.	O
In	O
order	O
to	O
validate	O
the	O
current	O
UDv2	O
trees	O
,	O
we	O
reviewed	O
the	O
list	O
of	O
UD	B-DatasetName
POS	O
tags	O
,	O
relation	O
labels	O
and	O
features	O
,	O
and	O
for	O
each	O
of	O
these	O
items	O
we	O
identified	O
the	O
dependency	O
structures	O
in	O
the	O
HTB	O
dev	O
set	O
that	O
contain	O
them	O
.	O
At	O
this	O
point	O
,	O
for	O
each	O
item	O
,	O
a	O
linguist	O
characterized	O
the	O
role	O
such	O
item	O
actually	O
fulfills	O
in	O
the	O
Hebrew	O
grammatical	O
structures	O
,	O
(	O
as	O
opposed	O
to	O
the	O
role	O
it	O
was	O
designed	O
to	O
fulfill	O
in	O
the	O
UD	B-DatasetName
scheme	O
)	O
.	O
During	O
this	O
process	O
the	O
linguist	O
documented	O
errors	O
and	O
inconsistencies	O
that	O
were	O
found	O
,	O
either	O
between	O
the	O
realistic	O
use	O
of	O
a	O
function	O
in	O
the	O
UDv2	O
HTB	O
and	O
the	O
UDv2	O
guidelines	O
,	O
or	O
simply	O
attesting	O
insufficient	O
or	O
incorrect	O
coverage	O
of	O
the	O
linguistic	O
structure	O
that	O
this	O
particular	O
label	O
,	O
tag	O
or	O
feature	O
is	O
supposed	O
to	O
describe	O
.	O
This	O
validation	O
process	O
8	O
was	O
conducted	O
on	O
the	O
entire	O
HTB	O
UDv2	O
dev	O
set	O
9	O
and	O
it	O
was	O
followed	O
by	O
a	O
sequence	O
of	O
discussions	O
in	O
which	O
our	O
research	O
team	O
,	O
consisting	O
of	O
two	O
linguists	O
,	O
two	O
NLP	O
specialists	O
,	O
and	O
a	O
senior	O
NLP	O
researcher	O
,	O
discussed	O
possible	O
solutions	O
for	O
each	O
error	O
.	O
The	O
discussions	O
were	O
focused	O
on	O
explicitly	O
assessing	O
the	O
merits	O
of	O
each	O
solution	O
alternative	O
according	O
to	O
the	O
six	O
criteria	O
of	O
the	O
Mannings	O
Law	O
.	O
That	O
is	O
:	O
linguistically	O
adequate	O
,	O
typologically	O
adequate	O
,	O
suitable	O
for	O
rapid	O
,	O
consistent	O
annotation	O
,	O
suitable	O
for	O
parsing	O
with	O
high	O
accuracy	B-MetricName
,	O
easily	O
comprehended	O
by	O
non	O
-	O
linguists	O
,	O
and	O
provides	O
good	O
support	O
for	O
downstream	O
NLP	O
tasks	O
.	O
10	O
After	O
narrowing	O
down	O
the	O
list	O
of	O
adequate	O
solutions	O
,	O
the	O
final	O
decision	O
about	O
which	O
revisions	O
to	O
make	O
leaned	O
on	O
their	O
importance	O
and	O
feasibility	O
.	O
For	O
example	O
,	O
a	O
very	O
important	O
,	O
yet	O
easily	O
executable	O
revision	O
was	O
to	O
simply	O
replace	O
all	O
instances	O
of	O
prepositional	O
iobj	O
with	O
obl	O
.	O
Just	O
as	O
important	O
,	O
but	O
far	O
more	O
complex	O
,	O
was	O
to	O
switch	O
between	O
a	O
head	O
and	O
a	O
dependent	O
in	O
the	O
case	O
of	O
structures	O
containing	O
auxiliaries	O
(	O
e.g.	O
,	O
modals	O
,	O
as	O
we	O
illustrate	O
shortly	O
)	O
.	O
All	O
revisions	O
were	O
made	O
with	O
the	O
python	O
Pandas	O
package	O
,	O
and	O
they	O
were	O
applied	O
to	O
all	O
,	O
dev	O
,	O
train	O
and	O
test	O
,	O
sets	O
.	O
Revisions	O
were	O
made	O
with	O
respect	O
to	O
linguistic	O
patterns	O
that	O
refer	O
to	O
existing	O
labels	O
,	O
tags	O
or	O
features	O
,	O
with	O
no	O
consideration	O
of	O
any	O
particular	O
(	O
lexicalized	O
)	O
Hebrew	O
words	O
.	O
Furthermore	O
,	O
we	O
refrained	O
from	O
manual	O
changes	O
of	O
specific	O
errors	O
,	O
considering	O
that	O
their	O
source	O
might	O
be	O
a	O
vaster	O
problem	O
,	O
to	O
be	O
dealt	O
with	O
in	O
the	O
future	O
.	O
As	O
an	O
example	O
for	O
simple	O
edits	O
,	O
consider	O
adding	O
a	O
label	O
compound	O
:	O
affix	O
.	O
For	O
this	O
,	O
all	O
rows	O
containing	O
the	O
feature	O
'	O
Prefix	O
=	O
Yes	O
'	O
had	O
to	O
be	O
retrieved	O
,	O
and	O
the	O
label	O
was	O
changed	O
to	O
compound	O
:	O
affix	O
.	O
As	O
a	O
more	O
complex	O
case	O
,	O
consider	O
the	O
case	O
involving	O
modality	O
mentioned	O
above	O
.	O
Here	O
,	O
all	O
rows	O
with	O
the	O
xcomp	O
label	O
were	O
retrieved	O
.	O
For	O
each	O
row	O
,	O
if	O
the	O
head	O
had	O
a	O
morphological	O
feature	O
'	O
Verb	O
-	O
Type	O
=	O
Mod	O
'	O
,	O
the	O
head	O
's	O
label	O
was	O
relabeled	O
'	O
aux	O
'	O
,	O
the	O
row	O
itself	O
was	O
relabeled	O
with	O
the	O
original	O
label	O
of	O
the	O
head	O
,	O
and	O
the	O
numbers	O
were	O
changed	O
respectively	O
in	O
the	O
'	O
HEAD	O
'	O
column	O
(	O
see	O
Table	O
1	O
)	O
.	O

Eliminating	O
acl	O
:	O
inf	O
to	O
acl	O
.	O
The	O
automatic	O
conversion	O
to	O
UD	B-DatasetName
has	O
kept	O
fine	O
-	O
grained	O
labels	O
as	O
subrelations	O
,	O
resulting	O
with	O
the	O
language	O
-	O
specific	O
label	O
acl	O
:	O
inf	O
.	O
Since	O
the	O
UD	B-DatasetName
guidelines	O
permit	O
infinitive	O
structures	O
in	O
acl	O
,	O
it	O
is	O
unnecessary	O
to	O
mark	O
infinity	O
as	O
a	O
sub	O
-	O
relation	O
.	O
Moreover	O
,	O
all	O
cases	O
of	O
acl	O
:	O
inf	O
bear	O
the	O
feature	O
'	O
VerbForm	O
=	O
Inf	O
'	O
.	O
So	O
eliminating	O
the	O
morphological	O
feature	O
inf	O
from	O
the	O
subrelation	O
acl	O
:	O
inf	O
does	O
not	O
lead	O
to	O
any	O
information	O
loss	B-MetricName
.	O
Adding	O
compound	O
:	O
affix	O
This	O
new	O
relation	O
is	O
dedicated	O
to	O
non	O
-	O
standalone	O
words	O
,	O
which	O
function	O
semantically	O
like	O
affixes	O
,	O
but	O
syntactically	O
surface	O
as	O
separate	O
words	O
,	O
at	O
times	O
separated	O
by	O
a	O
hyphen	O
and	O
in	O
others	O
by	O
white	O
-	O
space	O
.	O
A	O
subset	O
of	O
these	O
words	O
are	O
loan	O
words	O
(	O
mainly	O
from	O
English	O
,	O
like	O
'	O
non	O
'	O
,	O
'	O
multi	O
'	O
etc	O
.	O
)	O
where	O
originally	O
they	O
surface	O
syntactically	O
as	O
affixes	O
.	O
In	O
UDv2	O
these	O
items	O
were	O
marked	O
by	O
the	O
feature	O
Prefix	O
=	O
Yes	O
.	O
However	O
,	O
since	O
they	O
mark	O
a	O
certain	O
type	O
of	O
Hebrew	O
compounds	O
,	O
we	O
used	O
sub	O
-	O
typing	O
to	O
indicate	O
it	O
.	O
11	O
In	O
"	O
KLL	O
-	O
EWLMIT	O
"	O
for	O
example	O
,	O
KLL	O
'	O
uni	O
-	O
'	O
is	O
semantically	O
a	O
prefix	O
to	O
EWLMIT	O
'	O
worldly	O
'	O
,	O
but	O
in	O
Hebrew	O
the	O
two	O
are	O
separate	O
words	O
.	O

The	O
Words	O
-	O
as	O
-	O
Classifiers	O
(	O
WAC	O
)	O
approach	O
to	O
grounded	O
semantics	O
is	O
quite	O
simple	O
:	O
train	O
a	O
binary	O
classifier	O
for	O
each	O
word	O
in	O
a	O
corpus	O
where	O
the	O
features	O
to	O
that	O
classifier	O
are	O
derived	O
from	O
images	O
(	O
Kennington	O
and	O
Schlangen	O
,	O
2015	O
)	O
.	O
Each	O
classifier	O
is	O
given	O
positive	O
and	O
negative	O
examples	O
of	O
visual	O
denotations	O
of	O
each	O
word	O
by	O
the	O
images	O
and	O
learns	O
a	O
"	O
fitness	O
"	O
score	O
by	O
the	O
classifier	O
.	O
For	O
example	O
,	O
the	O
red	O
classifier	O
is	O
given	O
images	O
of	O
objects	O
that	O
are	O
referred	O
to	O
as	O
red	O
in	O
a	O
corpus	O
,	O
and	O
randomly	O
assigned	O
negative	O
examples	O
of	O
things	O
that	O
are	O
not	O
referred	O
to	O
as	O
red	O
,	O
as	O
depicted	O
in	O
Figure	O
1	O
.	O
We	O
follow	O
Kiros	O
et	O
al	O
(	O
2018	O
)	O
and	O
use	O
Google	B-DatasetName
Image	O
Search	O
to	O
find	O
images	O
using	O
the	O
BERT	B-MethodName
vocabulary	O
,	O
resulting	O
in	O
27	O
,	O
152	O
words	O
and	O
corresponding	O
images	O
(	O
some	O
words	O
did	O
not	O
result	O
in	O
images	O
,	O
and	O
we	O
did	O
not	O
download	O
images	O
for	O
filler	O
words	O
)	O
.	O
For	O
each	O
word	O
,	O
we	O
perform	O
an	O
image	O
search	O
and	O
download	O
the	O
top	O
100	O
images	O
.	O
We	O
then	O
follow	O
Schlangen	O
et	O
al	O
(	O
2016	O
)	O
and	O
process	O
each	O
image	O
by	O
passing	O
them	O
through	O
the	O
recent	O
CLIP	B-MethodName
(	O
Jia	O
et	O
al	O
,	O
2021	O
)	O
convolutional	O
neural	O
network	O
(	O
trained	O
on	O
ImageNet	B-DatasetName
,	O
using	O
CLIP	B-MethodName
's	O
ViT	O
-	O
B/32	O
model	O
)	O
,	O
yielding	O
a	O
vector	O
of	O
size	O
512	O
for	O
each	O
image	O
.	O
We	O
use	O
the	O
100	O
images	O
as	O
positive	O
examples	O
for	O
each	O
term	O
in	O
our	O
vocabulary	O
and	O
randomly	O
select	O
three	O
negative	O
examples	O
for	O
each	O
positive	O
example	O
.	O
We	O
then	O
use	O
a	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
(	O
C=0.25	O
,	O
max	O
iterations=1000	O
)	O
,	O
one	O
for	O
each	O
word	O
,	O
trained	O
on	O
the	O
images	O
for	O
each	O
word	O
.	O
After	O
training	O
,	O
we	O
then	O
follow	O
Moro	O
et	O
al	O
(	O
2019	O
)	O
and	O
extract	O
the	O
coefficients	O
to	O
arrive	O
at	O
a	O
vector	O
of	O
size	O
513	O
(	O
all	O
coefficients	O
plus	O
the	O
bias	O
term	O
)	O
which	O
we	O
use	O
in	O
our	O
evaluations	O
below	O
.	O
We	O
call	O
these	O
the	O
WAC	O
vectors	O
.	O
The	O
WAC	O
model	O
is	O
useful	O
because	O
,	O
as	O
explained	O
in	O
Kennington	O
and	O
Schlangen	O
(	O
2015	O
)	O
,	O
the	O
classifiers	O
can	O
actually	O
identify	O
objects	O
(	O
something	O
that	O
language	O
models	O
can	O
not	O
do	O
on	O
their	O
own	O
)	O
,	O
the	O
coefficients	O
represent	O
a	O
computed	O
word	O
intension	O
,	O
new	O
words	O
in	O
a	O
vocabulary	O
can	O
easily	O
be	O
added	O
without	O
retraining	O
all	O
other	O
classifiers	O
including	O
adjectives	O
like	O
red	O
which	O
are	O
often	O
missing	O
from	O
pre	O
-	O
trained	O
object	O
classifiers	O
,	O
and	O
the	O
classifiers	O
are	O
effectively	O
learned	O
with	O
only	O
a	O
few	O
examples	O
,	O
making	O
it	O
effective	O
for	O
fast	O
learning	O
of	O
concrete	O
,	O
grounded	O
concepts	O
.	O
However	O
,	O
the	O
WAC	O
model	O
suffers	O
from	O
two	O
assumptions	O
:	O
first	O
,	O
that	O
all	O
words	O
have	O
concrete	O
,	O
visual	O
denotations	O
even	O
though	O
many	O
abstract	O
words	O
like	O
utopia	O
clearly	O
do	O
not	O
,	O
and	O
that	O
all	O
words	O
are	O
independent	O
of	O
each	O
other	O
in	O
terms	O
of	O
linguistic	O
context	O
.	O
We	O
hypothesize	O
in	O
both	O
experiments	O
below	O
that	O
these	O
coefficients	O
used	O
as	O
vectorized	O
embeddings	O
will	O
be	O
useful	O
to	O
a	O
text	O
-	O
only	O
language	O
model	O
because	O
they	O
add	O
necessary	O
visual	O
information	O
;	O
the	O
language	O
model	O
complements	O
WAC	O
by	O
using	O
linguistic	O
context	O
(	O
i.e.	O
,	O
text	O
)	O
for	O
training	O
,	O
overcoming	O
WAC	O
's	O
assumptions	O
.	O
4	O
Experiment	O
1	O
:	O
Tying	O
embedding	O
weights	O
and	O
pre	O
-	O
training	O
ELECTRA	B-MethodName
,	O
fine	O
-	O
tuning	O
on	O
GLUE	B-DatasetName
In	O
this	O
experiment	O
,	O
and	O
crucially	O
for	O
our	O
ongoing	O
work	O
that	O
aligns	O
with	O
child	O
-	O
inspired	O
language	B-TaskName
acquisition	I-TaskName
,	O
we	O
use	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
,	O
2020	O
)	O
as	O
a	O
language	O
model	O
because	O
it	O
has	O
been	O
shown	O
to	O
be	O
trainable	O
with	O
smaller	O
amounts	O
of	O
data	O
than	O
other	O
language	O
models	O
,	O
yet	O
yield	O
respectable	O
results	O
and	O
can	O
be	O
trained	O
using	O
a	O
single	O
GPU	O
.	O
Task	O
&	O
Procedure	O
Wang	O
et	O
al	O
(	O
2018	O
)	O
introduced	O
the	O
GLUE	B-DatasetName
benchmark	O
which	O
consists	O
of	O
nine	O
English	O
sentence	O
understanding	O
tasks	O
covering	O
several	O
domains	O
(	O
e.g.	O
,	O
movie	O
reviews	O
and	O
online	O
question	B-TaskName
answering	I-TaskName
)	O
.	O
We	O
opt	O
for	O
this	O
benchmark	O
because	O
of	O
its	O
coverage	O
over	O
several	O
domains	O
and	O
to	O
show	O
that	O
adding	O
multimodal	O
knowledge	O
improves	O
tasks	O
that	O
are	O
based	O
on	O
text	O
.	O
2	O
Our	O
aim	O
is	O
to	O
achieve	O
improved	O
results	O
over	O
the	O
text	O
-	O
only	O
baseline	O
with	O
a	O
specified	O
number	O
of	O
training	O
steps	O
using	O
the	O
openwebtext	B-DatasetName
data	O
for	O
training	O
.	O
3	O
We	O
report	O
results	O
on	O
the	O
development	O
set	O
,	O
as	O
done	O
in	O
Wu	O
et	O
al	O
(	O
2021	O
)	O
.	O
We	O
only	O
report	O
the	O
results	O
for	O
the	O
MRPC	B-DatasetName
(	O
a	O
paraphrase	O
task	O
that	O
uses	O
accuracy	B-MetricName
and	O
f1	O
metrics	O
)	O
,	O
COLA	B-MethodName
(	O
a	O
grammatical	O
acceptibility	O
task	O
;	O
uses	O
Matthew	O
's	O
Correlation	O
)	O
,	O
and	O
WNLI	B-DatasetName
(	O
ambiguity	O
resolution	O
;	O
uses	O
an	O
accuracy	B-MetricName
metric	O
)	O
tasks	O
because	O
they	O
are	O
sufficient	O
to	O
illustrate	O
the	O
utility	O
of	O
our	O
method	O
when	O
applied	O
to	O
ELECTRA	B-MethodName
.	O
To	O
give	O
ELECTRA	B-MethodName
knowledge	O
about	O
additional	O
modalities	O
from	O
the	O
Lancaster	O
and	O
WAC	O
vectors	O
,	O
we	O
tie	O
the	O
vectors	O
to	O
the	O
the	O
weights	O
of	O
the	O
generator	O
and	O
discriminator	O
of	O
ELECTRA	B-MethodName
depicted	O
in	O
Figure	O
2	O
,	O
and	O
vary	O
whether	O
the	O
embeddings	O
are	O
frozen	O
or	O
not	O
during	O
pre	O
-	O
training	O
,	O
then	O
train	O
for	O
100	O
,	O
000	O
steps	O
.	O
4	O
We	O
then	O
fine	O
-	O
tune	O
the	O
resulting	O
ELECTRA	B-MethodName
model	O
on	O
the	O
GLUE	B-DatasetName
tasks	O
using	O
the	O
multimodal	O
vectors	O
following	O
standard	O
fine	O
-	O
tuning	O
protocols	O
;	O
that	O
is	O
,	O
we	O
add	O
a	O
linear	B-MethodName
layer	I-MethodName
with	O
a	O
softmax	B-MethodName
to	O
the	O
pre	O
-	O
trained	O
model	O
and	O
use	O
the	O
ADAM	B-DatasetName
solver	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e	O
-	O
5	O
for	O
3	O
epochs	O
.	O
As	O
the	O
WAC	O
vectors	O
were	O
larger	O
than	O
ELECTRA	B-MethodName
's	O
expected	O
embedding	O
size	O
of	O
128	O
,	O
we	O
applied	O
UMAP	O
to	O
reduce	O
the	O
dimensionality	O
to	O
128	O
;	O
similarly	O
for	O
the	O
WAC	O
and	O
Lancaster	O
concatenated	O
embeddings	O
.	O
For	O
cases	O
where	O
there	O
was	O
no	O
vector	O
for	O
a	O
word	O
(	O
e.g.	O
,	O
the	O
[	O
unmapped	O
]	O
words	O
or	O
words	O
outside	O
of	O
the	O
vocabulary	O
of	O
the	O
Lancaster	O
vectors	O
)	O
,	O
we	O
simply	O
used	O
zero	O
vectors	O
.	O
For	O
Lancaster	O
vectors	O
,	O
we	O
set	O
the	O
ELECTRA	B-MethodName
embedding	O
size	O
to	O
39	O
.	O
We	O
explored	O
freezing	O
the	O
embeddings	O
;	O
our	O
hypothesis	O
is	O
that	O
not	O
freezing	O
the	O
embeddings	O
will	O
lead	O
to	O
better	O
results	O
because	O
the	O
training	O
regime	O
can	O
overpower	O
the	O
embeddings	O
,	O
but	O
retain	O
the	O
multimodal	O
knowledge	O
.	O
For	O
a	O
broader	O
comparison	O
,	O
we	O
also	O
compared	O
to	O
GloVE	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
and	O
several	O
ablations	O
where	O
we	O
concatenate	O
multimodal	O
vectors	O
with	O
the	O
GloVe	B-MethodName
vectors	O
(	O
we	O
used	O
the	O
evaluation	O
script	O
for	O
GloVE	O
provided	O
by	O
Wang	O
et	O
al	O
(	O
2018	O
)	O
)	O
.	O
We	O
also	O
use	O
the	O
same	O
training	O
and	O
evaluation	O
regime	O
for	O
the	O
WAC	O
and	O
Lancaster	O
vectors	O
,	O
and	O
a	O
concatenation	O
of	O
the	O
two	O
,	O
on	O
their	O
own	O
treating	O
them	O
as	O
word	O
-	O
level	O
embeddings	O
similar	O
to	O
GloVe	B-MethodName
.	O
Results	O
Table	O
1	O
shows	O
the	O
results	O
on	O
the	O
GLUE	B-DatasetName
benchmark	O
.	O
The	O
word	O
-	O
level	O
embeddings	O
of	O
GloVe	B-MethodName
,	O
WAC	O
,	O
and	O
Lancaster	O
are	O
shown	O
in	O
the	O
top	O
5	O
rows	O
of	O
the	O
table	O
.	O
As	O
expected	O
,	O
these	O
word	O
-	O
level	O
embeddings	O
are	O
not	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
but	O
we	O
notice	O
that	O
both	O
Lancaster	O
and	O
WAC	O
vectors	O
perform	O
comparably	O
against	O
the	O
GloVE	O
vectors	O
despite	O
only	O
being	O
trained	O
on	O
images	O
(	O
WAC	O
)	O
or	O
derived	O
from	O
the	O
Lancaster	O
norms	O
.	O
Of	O
note	O
is	O
a	O
significant	O
advantage	O
of	O
using	O
the	O
Lancaster	O
vectors	O
alone	O
compared	O
to	O
using	O
any	O
other	O
embedding	O
or	O
combination	O
for	O
the	O
WNLI	B-DatasetName
task	O
which	O
is	O
co	O
-	O
reference	O
and	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
for	O
fiction	O
books	O
.	O
This	O
suggests	O
that	O
inference	O
on	O
fiction	O
is	O
helped	O
by	O
knowing	O
which	O
modalities	O
affect	O
each	O
word	O
.	O
Interestingly	O
,	O
the	O
best	O
performing	O
model	O
for	O
COLA	B-MethodName
was	O
GloVE	O
and	O
Lancaster	O
word	O
-	O
level	O
embeddings	O
;	O
COLA	B-MethodName
is	O
a	O
grammaticality	O
test	O
,	O
which	O
is	O
important	O
in	O
language	O
understanding	O
,	O
but	O
arguably	O
less	O
critical	O
in	O
early	O
-	O
stage	O
child	O
language	B-TaskName
acquisition	I-TaskName
.	O
All	O
other	O
rows	O
show	O
the	O
ELECTRA	B-MethodName
baseline	O
and	O
ELECTRA	B-MethodName
that	O
uses	O
some	O
variation	O
of	O
WAC	O
,	O
Lancaster	O
,	O
or	O
both	O
as	O
embeddings	O
(	O
denoted	O
with	O
the	O
EL	O
-	O
prefix	O
)	O
.	O
The	O
bottom	O
part	O
of	O
the	O
table	O
compares	O
ELECTRA	B-MethodName
with	O
a	O
variant	O
of	O
ELECTRA	B-MethodName
that	O
uses	O
WAC	O
embeddings	O
(	O
both	O
with	O
and	O
without	O
freezing	O
the	O
embedding	O
weights	O
)	O
,	O
ELECTRA	B-MethodName
with	O
lancaster	O
embeddings	O
and	O
ELECTRA	B-MethodName
with	O
WAC	O
embeddings	O
concatenated	O
with	O
the	O
Lancaster	O
embeddings	O
(	O
where	O
the	O
length	O
of	O
the	O
WAC	O
embeddings	O
plus	O
the	O
size	O
of	O
ELECTRA	B-MethodName
is	O
128	O
)	O
.	O
Contrary	O
to	O
our	O
hypothesis	O
,	O
we	O
observe	O
that	O
when	O
ELECTRA	B-MethodName
uses	O
WAC	O
with	O
frozen	O
weights	O
,	O
the	O
performance	O
on	O
the	B-DatasetName
benchmark	I-DatasetName
performs	O
better	O
than	O
all	O
others	O
,	O
including	O
the	O
ELECTRA	B-MethodName
baseline	O
.	O
This	O
could	O
suggest	O
that	O
ELECTRA	B-MethodName
can	O
make	O
effective	O
use	O
of	O
the	O
visual	O
and	O
Lancaster	O
embeddings	O
by	O
adjusting	O
weights	O
in	O
the	O
other	O
layers	O
of	O
the	O
model	O
.	O
The	O
EL	O
-	O
lan	O
-	O
wac	O
variant	O
performed	O
well	O
above	O
the	O
ELECTRA	B-MethodName
baseline	O
,	O
substantiating	O
the	O
hypothesis	O
that	O
enriching	O
the	O
model	O
with	O
multimodal	O
knowledge	O
can	O
improve	O
results	O
.	O
Taken	O
together	O
,	O
we	O
find	O
the	O
results	O
encouraging	O
because	O
the	O
relatively	O
short	O
training	O
regime	O
still	O
yielded	O
respectable	O
results	O
,	O
suggesting	O
that	O
ELECTRA	B-MethodName
with	O
a	O
visual	O
or	O
other	O
multimodal	O
embedding	O
can	O
be	O
useful	O
with	O
less	O
training	O
as	O
is	O
the	O
case	O
when	O
children	O
learn	O
language	O
.	O

The	O
evaluation	O
in	O
Experiment	O
1	O
was	O
made	O
up	O
of	O
text	O
-	O
based	O
tasks	O
.	O
In	O
this	O
experiment	O
,	O
we	O
use	O
an	O
evaluation	O
that	O
requires	O
knowledge	O
of	O
the	O
visual	O
world	O
by	O
evaluating	O
the	O
Lancaster	O
and	O
WAC	O
vectors	O
on	O
the	O
Visual	B-TaskName
Dialog	I-TaskName
task	O
,	O
termed	O
visdial	B-DatasetName
.	O
Moreover	O
,	O
Experiment	O
1	O
used	O
pre	O
-	O
training	O
on	O
a	O
subset	O
of	O
the	O
data	O
for	O
only	O
100	O
,	O
000	O
steps	O
.	O
In	O
this	O
experiment	O
,	O
we	O
evaluate	O
using	O
a	O
fully	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
model	O
by	O
replacing	O
its	O
embeddings	O
with	O
the	O
WAC	O
and	O
Lancaster	O
vectors	O
.	O
Task	O
Following	O
Murahari	O
et	O
al	O
(	O
2019	O
)	O
,	O
given	O
an	O
image	O
,	O
dialogue	O
history	O
consisting	O
of	O
questionanswer	O
pairs	O
,	O
and	O
a	O
follow	O
-	O
up	O
question	O
about	O
the	O
image	O
,	O
the	O
task	O
of	O
visdial	B-DatasetName
is	O
to	O
predict	O
a	O
free	O
-	O
form	O
natural	O
language	O
answer	O
to	O
the	O
question	O
.	O
The	O
visdial	B-DatasetName
dataset	O
introduced	O
in	O
Das	O
et	O
al	O
(	O
2019	O
)	O
also	O
includes	O
evaluation	O
metrics	O
and	O
human	O
-	O
annotated	O
answers	O
to	O
the	O
natural	O
language	O
queries	O
about	O
the	O
image	O
.	O
Five	O
human	O
annotators	O
identified	O
which	O
responses	O
out	O
of	O
100	O
candidates	O
could	O
be	O
considered	O
correct	O
.	O
This	O
allows	O
multiple	O
answers	O
to	O
be	O
correct	O
(	O
e.g.	O
,	O
yes	O
and	O
yeah	O
are	O
semantically	O
identical	O
)	O
.	O
Metrics	O
We	O
report	O
the	O
following	O
metrics	O
:	O
R@1	B-MetricName
Rate	O
of	O
times	O
the	O
top	O
-	O
ranked	O
answer	O
is	O
a	O
correct	O
one	O
;	O
i.e.	O
,	O
accuracy	B-MetricName
.	O
R@5	B-MetricName
Rate	O
of	O
times	O
correct	O
answers	O
are	O
in	O
the	O
top	O
-	O
five	O
ranked	O
answers	O
.	O
MRR	B-MetricName
Mean	O
Reciprocal	O
Rank	O
is	O
the	O
multiplicative	O
inverse	O
of	O
the	O
rank	O
of	O
the	O
first	O
correct	O
answer	O
.	O
NDCG	O
Normalized	O
Discount	O
Accumulative	O
Gain	O
is	O
a	O
measure	O
of	O
ranking	O
quality	O
that	O
takes	O
the	O
top	O
K	O
ranked	O
options	O
,	O
where	O
K	O
is	O
the	O
number	O
of	O
answers	O
marked	O
as	O
correct	O
by	O
a	O
least	O
one	O
annotator	O
;	O
in	O
this	O
measure	O
,	O
the	O
fraction	O
of	O
annotators	O
that	O
marked	O
a	O
particular	O
answer	O
as	O
correct	O
is	O
taken	O
into	O
account	O
.	O
Baseline	O
and	O
Procedure	O
We	O
report	O
the	O
values	O
for	O
the	O
model	O
described	O
in	O
Murahari	O
et	O
al	O
(	O
2019	O
)	O
for	O
our	O
baseline	O
-	O
work	O
which	O
builds	O
on	O
VilBERT	B-MethodName
(	O
Lu	O
et	O
al	O
,	O
2019	O
)	O
,	O
a	O
parallel	O
model	O
of	O
vision	O
and	O
language	O
used	O
for	O
the	O
visual	O
dialogue	O
task	O
-	O
and	O
leverage	O
their	O
model	O
with	O
our	O
custom	O
,	O
multimodal	O
embeddings	O
.	O
Their	O
model	O
uses	O
two	O
transformers	O
,	O
one	O
for	O
the	O
language	O
modality	O
and	O
one	O
for	O
the	O
visual	O
modality	O
.	O
As	O
explained	O
in	O
Lu	O
et	O
al	O
(	O
2019	O
)	O
,	O
the	O
interaction	O
between	O
the	O
two	O
transformers	O
is	O
mediated	O
by	O
two	O
co	O
-	O
attention	B-HyperparameterName
layers	I-HyperparameterName
where	O
attention	O
in	O
one	O
modality	O
is	O
conditioned	O
on	O
inputs	O
from	O
the	O
other	O
modality	O
.	O
Murahari	O
et	O
al	O
(	O
2019	O
)	O
adapted	O
the	O
VilBERT	B-MethodName
model	O
for	O
the	O
visdial	B-DatasetName
task	O
by	O
using	O
a	O
pre	O
-	O
trained	O
language	O
model	O
trained	O
on	O
English	O
Wikipedia	O
and	O
the	O
BooksCorpus	O
(	O
Zhu	O
et	O
al	O
,	O
2015	O
)	O
using	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
and	O
next	O
sentence	O
prediction	O
losses	O
.	O
They	O
then	O
frame	O
the	O
task	O
as	O
a	O
next	O
-	O
sentence	O
prediction	O
task	O
(	O
whereas	O
the	O
original	O
VilBERT	B-MethodName
was	O
modeled	O
to	O
generate	O
descriptions	O
of	O
images	O
)	O
.	O
They	O
then	O
use	O
the	O
Conceptual	B-DatasetName
Captions	I-DatasetName
(	O
Sharma	O
et	O
al	O
,	O
2018	O
)	O
and	O
Visual	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
(	O
VQA	B-TaskName
)	O
(	O
Antol	O
et	O
al	O
,	O
2015	O
)	O
datasets	O
(	O
using	O
masked	O
image	O
region	O
,	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
,	O
and	O
next	O
sentence	O
prediction	O
losses	O
)	O
to	O
train	O
the	O
two	O
transformers	O
.	O
They	O
then	O
fine	O
-	O
tune	O
on	O
the	O
visdial	B-DatasetName
task	O
(	O
also	O
using	O
masked	O
image	O
region	O
,	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
,	O
and	O
next	O
sentence	O
prediction	O
losses	O
)	O
.	O
The	O
underlying	O
architecture	O
uses	O
a	O
pre	O
-	O
trained	O
BERT	B-MethodName
language	O
model	O
(	O
i.e.	O
,	O
bert	O
-	O
base	O
-	O
uncased	O
)	O
as	O
a	O
starting	O
point	O
before	O
training	O
on	O
the	O
Wikipedia	O
,	O
BooksCorpus	O
,	O
Conceptual	B-DatasetName
Captions	I-DatasetName
,	O
and	O
VQA	B-TaskName
datasets	O
.	O
This	O
constitutes	O
our	O
baseline	O
.	O
We	O
do	O
n't	O
consider	O
the	O
dense	O
representations	O
from	O
Murahari	O
et	O
al	O
(	O
2019	O
)	O
due	O
to	O
hardware	O
limitations	O
.	O
We	O
altered	O
their	O
architecture	O
by	O
replacing	O
the	O
RoBERTa	B-MethodName
pre	O
-	O
trained	O
embedding	O
layer	O
with	O
the	O
Lancaster	O
and	O
WAC	O
vectors	O
,	O
as	O
depicted	O
in	O
Figure	O
3	O
.	O
We	O
then	O
fine	O
-	O
tuned	O
on	O
the	O
visdial	B-DatasetName
task	O
using	O
their	O
training	O
regime	O
.	O
5	O
We	O
explain	O
how	O
we	O
made	O
vectors	O
compatible	O
with	O
their	O
architecture	O
.	O
Vocabulary	O
:	O
RoBERTa	B-MethodName
&	O
AoA	O
Abstract	O
words	O
do	O
not	O
have	O
concrete	O
,	O
visual	O
denotations	O
,	O
such	O
as	O
utopia	O
or	O
justice	O
,	O
so	O
it	O
does	O
not	O
make	O
theoretical	O
sense	O
to	O
include	O
a	O
WAC	O
embedding	O
for	O
words	O
that	O
are	O
clearly	O
abstract	O
because	O
whatever	O
set	O
of	O
images	O
represents	O
those	O
concepts	O
may	O
not	O
have	O
useful	O
semantic	O
information	O
.	O
Moreover	O
,	O
children	O
learning	O
their	O
first	O
language	O
learn	O
concrete	O
concepts	O
before	O
they	O
learn	O
more	O
abstract	O
concepts	O
(	O
Borghi	O
et	O
al	O
,	O
2019	O
;	O
Ponari	O
et	O
al	O
,	O
2018	O
)	O
.	O
To	O
explore	O
if	O
RoBERTa	B-MethodName
could	O
make	O
use	O
of	O
a	O
WAC	O
embedding	O
that	O
uses	O
words	O
that	O
are	O
more	O
aimed	O
at	O
a	O
child	O
vocabulary	O
,	O
we	O
report	O
results	O
of	O
filtering	O
out	O
words	O
not	O
in	O
the	O
the	O
Age	O
-	O
of	O
-	O
Acquisition	O
(	O
AoA	O
)	O
list	O
(	O
Kuperman	O
et	O
al	O
,	O
2012b	O
)	O
.	O
AoA	O
a	O
list	O
of	O
30	O
,	O
000	O
English	O
words	O
rated	O
for	O
the	O
average	O
age	O
when	O
children	O
first	O
speak	O
those	O
words	O
(	O
avg	O
11	O
years	O
;	O
std	O
3.0	O
,	O
most	O
common	O
words	O
are	O
for	O
ages	O
2	O
-	O
14	O
)	O
.	O
This	O
resulted	O
in	O
9	O
,	O
627	O
remaining	O
words	O
in	O
the	O
vocabulary	O
;	O
all	O
other	O
words	O
were	O
set	O
to	O
an	O
embedding	O
of	O
zeros	O
.	O
Lancaster	O
vectors	O
Similar	O
to	O
AoA	O
,	O
the	O
Lancaster	O
Norms	O
has	O
a	O
predefined	O
vocabulary	O
,	O
which	O
,	O
when	O
compared	O
to	O
the	O
RoBERTa	B-MethodName
vocabulary	O
results	O
in	O
11	O
,	O
402	O
words	O
in	O
both	O
.	O
For	O
each	O
word	O
in	O
the	O
RoBERTa	B-MethodName
vocabulary	O
that	O
was	O
also	O
in	O
the	O
Lancaster	O
norms	O
,	O
we	O
replaced	O
the	O
RoBERTa	B-MethodName
embedding	O
with	O
the	O
Lancaster	O
vector	O
for	O
that	O
word	O
;	O
otherwise	O
words	O
retained	O
the	O
original	O
RoBERTa	B-MethodName
embedding	O
.	O
As	O
their	O
model	O
expects	O
vectors	O
of	O
size	O
768	O
(	O
the	O
embedding	O
size	O
for	O
RoBERTa	B-MethodName
)	O
,	O
but	O
the	O
Lancaster	O
vectors	O
are	O
only	O
size	O
39	O
,	O
we	O
padded	O
the	O
rest	O
of	O
the	O
vector	O
with	O
zeros	O
.	O
WAC	O
vectors	O
We	O
use	O
the	O
vocabulary	O
from	O
the	O
RoBERTa	B-MethodName
tokenizer	O
as	O
with	O
the	O
Lancaster	O
Vectors	O
,	O
which	O
results	O
in	O
a	O
a	O
27	O
,	O
152	O
-	O
word	O
overlap	O
with	O
the	O
WAC	O
vectors	O
.	O
As	O
the	O
WAC	O
vectors	O
have	O
a	O
dimensionality	O
of	O
513	O
,	O
smaller	O
than	O
the	O
required	O
size	O
of	O
RoBERTa	B-MethodName
's	O
768	O
,	O
we	O
padded	O
zeros	O
after	O
each	O
vector	O
.	O
All	O
vectors	O
that	O
did	O
not	O
exist	O
in	O
the	O
WAC	O
set	O
were	O
zero	O
vectors	O
of	O
size	O
768	O
.	O
We	O
followed	O
a	O
training	O
regime	O
for	O
two	O
settings	O
:	O
no	O
-	O
freeze	O
The	O
embedding	O
layer	O
was	O
not	O
frozen	O
so	O
as	O
to	O
allow	O
weight	O
changes	O
during	O
training	O
.	O
2	O
-	O
freeze	O
The	O
embedding	O
layer	O
was	O
frozen	O
for	O
two	O
epochs	O
,	O
then	O
the	O
weights	O
were	O
unfrozen	O
for	O
the	O
rest	O
of	O
training	O
;	O
prior	O
work	O
has	O
shown	O
that	O
freezing	O
layers	O
after	O
a	O
certain	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
can	O
improve	O
results	O
(	O
Liu	O
et	O
al	O
,	O
2021	O
)	O
;	O
we	O
opt	O
for	O
two	O
because	O
it	O
still	O
allows	O
the	O
finetuning	O
to	O
overpower	O
the	O
exiting	O
embeddings	O
if	O
needed	O
and	O
preliminary	O
results	O
showed	O
that	O
freezing	O
the	O
weights	O
for	O
all	O
epochs	O
resulted	O
in	O
poor	O
model	O
performance	O
.	O
We	O
trained	O
each	O
model	O
for	O
20	O
epochs	O
in	O
total	O
,	O
which	O
is	O
the	O
default	O
training	O
setting	O
for	O
this	O
task	O
.	O
We	O
used	O
the	O
settings	O
that	O
were	O
used	O
to	O
train	O
the	O
baseline	O
model	O
(	O
i.e.	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e	O
-	O
5	O
)	O
.	O
We	O
report	O
the	O
results	O
of	O
the	O
baseline	O
model	O
and	O
the	O
variants	O
of	O
our	O
above	O
changes	O
.	O
6	O
Compared	O
to	O
Experiment	O
1	O
with	O
the	O
GLUE	B-DatasetName
benchmark	O
,	O
the	O
approach	O
taken	O
in	O
this	O
section	O
fundamentally	O
changes	O
how	O
the	O
Lancaster	O
and	O
WAC	O
embeddings	O
are	O
applied	O
to	O
RoBERTa	B-MethodName
;	O
here	O
the	O
Lancaster	O
and	O
WAC	O
embeddings	O
are	O
used	O
on	O
a	O
pre	O
-	O
trained	O
model	O
.	O
We	O
hy	O
-	O
6	O
Note	O
that	O
the	O
baseline	O
we	O
are	O
comparing	O
to	O
here	O
is	O
lower	O
than	O
what	O
is	O
reported	O
on	O
the	O
leaderboard	O
https://eval.ai/web/challenges/	O
challenge	O
-	O
page/518	O
/	O
leaderboard	O
.	O
This	O
is	O
partially	O
due	O
to	O
the	O
fact	O
that	O
our	O
training	O
regime	O
was	O
altered	O
due	O
to	O
hardware	O
limitations	O
(	O
i.e.	O
,	O
we	O
could	O
only	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8	O
on	O
a	O
single	O
12	O
GB	O
GPU	O
)	O
.	O
no	O
freeze	O
MRR	B-MetricName
R@1	B-MetricName
R@5	B-MetricName
NDCG	O
pothesize	O
that	O
RoBERTa	B-MethodName
will	O
improve	O
with	O
WAC	O
embeddings	O
,	O
as	O
well	O
as	O
the	O
Lancaster	O
concatenated	O
to	O
WAC	O
(	O
denoted	O
lanwac	O
)	O
,	O
though	O
the	O
Lancaster	O
embedding	O
on	O
its	O
own	O
may	O
be	O
too	O
small	O
to	O
make	O
a	O
difference	O
.	O
As	O
words	O
that	O
are	O
learned	O
earlier	O
in	O
a	O
child	O
's	O
life	O
are	O
generally	O
more	O
concrete	O
,	O
we	O
hypothesize	O
that	O
RoBERTa	B-MethodName
will	O
improve	O
when	O
WAC	O
only	O
uses	O
words	O
from	O
the	O
AoA	O
data	O
as	O
more	O
abstract	O
terms	O
are	O
represented	O
by	O
zero	O
vectors	O
.	O
Results	O
Table	O
2	O
shows	O
the	O
results	O
for	O
the	O
visdial	B-DatasetName
task	O
.	O
Though	O
it	O
is	O
clear	O
that	O
RoBERTa	B-MethodName
is	O
doing	O
the	O
heavy	O
lifting	O
,	O
when	O
added	O
to	O
RoBERTa	B-MethodName
,	O
the	O
Lancaster	O
and	O
WAC	O
vectors	O
show	O
improvements	O
over	O
the	O
RoBERTa	B-MethodName
baseline	O
for	O
some	O
metrics	O
.	O
As	O
noted	O
in	O
Murahari	O
et	O
al	O
(	O
2019	O
)	O
,	O
the	O
NDCG	O
metric	O
is	O
actually	O
counter	O
to	O
MRR	B-MetricName
,	O
but	O
is	O
important	O
because	O
it	O
takes	O
multiple	O
dialogue	O
response	O
annotations	O
into	O
account	O
.	O
For	O
cases	O
where	O
the	O
Lancaster	O
and	O
WAC	O
models	O
yield	O
better	O
performance	O
,	O
these	O
results	O
suggest	O
that	O
a	O
pre	O
-	O
trained	O
language	O
model	O
can	O
make	O
use	O
of	O
adding	O
multimodal	O
knowledge	O
in	O
the	O
form	O
of	O
vectors	O
derived	O
from	O
multimodal	O
knowledge	O
(	O
Lancaster	O
)	O
and	O
visual	O
(	O
WAC	O
)	O
for	O
the	O
visdial	B-DatasetName
task	O
.	O
RoBERTA	O
that	O
uses	O
the	O
WAC	O
embedding	O
especially	O
shows	O
respectable	O
results	O
in	O
the	O
visdial	B-DatasetName
task	O
,	O
particularly	O
when	O
the	O
embedding	O
uses	O
the	O
AoA	O
vocabulary	O
(	O
we	O
only	O
considered	O
AoA	O
for	O
WAC	O
because	O
WAC	O
peformed	O
better	O
than	O
lanwac	O
in	O
this	O
experiment	O
)	O
.	O
The	O
WAC	O
vectors	O
were	O
trained	O
on	O
very	O
noisy	O
data	O
,	O
yet	O
despite	O
the	O
noise	O
and	O
the	O
parsimonious	O
model	O
,	O
there	O
is	O
some	O
information	O
about	O
the	O
visual	O
world	O
that	O
enriches	O
the	O
baseline	O
model	O
.	O
The	O
variant	O
trained	O
with	O
frozen	O
weights	O
for	O
2	O
epochs	O
,	O
then	O
unfrozen	O
for	O
the	O
remaining	O
18	O
epochs	O
had	O
respectable	O
performance	O
across	O
all	O
metrics	O
.	O
7	O
7	O
As	O
a	O
sanity	O
check	O
,	O
we	O
also	O
evaluated	O
using	O
randomly	O
generated	O
embeddings	O
which	O
performed	O
only	O
slightly	O
worse	O

In	O
order	O
to	O
evaluate	O
the	O
adaptability	O
hypothesis	O
where	O
the	O
prediction	O
of	O
upcoming	O
verb	O
is	O
driven	O
by	O
local	O
nominal	O
arguments	O
,	O
we	O
implement	O
an	O
ngram	O
language	O
model	O
using	O
the	O
data	O
discussed	O
in	O
Section	O
3.2	O
.	O
Such	O
models	O
are	O
typically	O
used	O
to	O
compute	O
the	O
surprisal	O
metric	O
(	O
Hale	O
,	O
2001	O
;	O
Levy	O
,	O
2008	O
)	O
given	O
local	O
context	O
(	O
e.g.	O
,	O
Levy	O
et	O
al	O
,	O
2012	O
)	O
.	O
Recall	B-MetricName
that	O
we	O
have	O
at	O
most	O
3	O
NPs	O
as	O
the	O
preverbal	O
context	O
,	O
and	O
therefore	O
,	O
we	O
use	O
a	O
4	O
-	O
gram	O
model	O
so	O
that	O
the	O
model	O
has	O
access	O
to	O
the	O
complete	O
context	O
in	O
a	O
given	O
condition	O
to	O
make	O
a	O
verbal	O
prediction	O
.	O
Unlike	O
the	O
models	O
discussed	O
in	O
Section	O
5	O
,	O
the	O
preverbal	O
context	O
in	O
this	O
model	O
is	O
free	O
of	O
noise	O
.	O

|	O
ε	B-HyperparameterName
D	O
contains	O
all	O
balanced	O
strings	O
of	O
parentheses	O
and	O
square	O
brackets	O
.	O
Since	O
D	O
is	O
often	O
viewed	O
as	O
a	O
canonical	O
example	O
of	O
a	O
context	O
-	O
free	O
language	O
(	O
Chomsky	O
and	O
Schützenberger	O
,	O
1959	O
)	O
,	O
several	O
recent	O
studies	O
,	O
including	O
Sennhauser	O
and	O
Berwick	O
(	O
2018	O
)	O
,	O
Bernardy	O
(	O
2018	O
)	O
,	O
Skachkova	O
et	O
al	O
(	O
2018	O
)	O
,	O
and	O
Yu	O
et	O
al	O
(	O
2019	O
)	O
,	O
have	O
used	O
D	O
to	O
evaluate	O
whether	O
LSTMs	O
can	O
learn	O
hierarchical	O
dependencies	O
implemented	O
by	O
pushdown	O
automata	O
.	O
Here	O
,	O
we	O
consider	O
the	O
bracket	O
prediction	O
task	O
proposed	O
by	O
Sennhauser	O
and	O
Berwick	O
(	O
2018	O
)	O
.	O
Task	O
5	O
(	O
Bracket	O
Prediction	O
Task	O
)	O
.	O
Given	O
a	O
prefix	O
p	O
of	O
some	O
string	O
in	O
D	O
,	O
identify	O
the	O
next	O
valid	O
closing	O
bracket	O
for	O
p.	O
In	O
heatmaps	O
for	O
the	O
bracket	O
prediction	O
task	O
,	O
we	O
expect	O
the	O
last	O
unclosed	O
bracket	O
to	O
receive	O
the	O
highest	O
-	O
magnitude	O
relevance	O
score	O
.	O

We	O
consider	O
two	O
types	O
of	O
automata	O
-	O
based	O
networks	O
:	O
one	O
that	O
implements	O
a	O
finite	O
-	O
state	O
automaton	O
(	O
FSA	O
)	O
for	O
the	O
SP	O
task	O
,	O
and	O
one	O
that	O
implements	O
a	O
pushdown	O
automaton	O
(	O
PDA	O
)	O
for	O
the	O
bracket	O
prediction	O
task	O
.	O
Our	O
FSA	O
construction	O
is	O
similar	O
to	O
Korsky	O
and	O
Berwick	O
's	O
(	O
2019	O
)	O
FSA	O
construction	O
for	O
simple	O
recurrent	O
networks	O
.	O
Consider	O
a	O
deterministic	O
FSA	O
A	O
with	O
states	O
Q	O
and	O
alphabet	O
Σ.	O
To	O
simulate	O
A	O
using	O
an	O
LSTM	B-MethodName
,	O
we	O
use	O
|	O
Q	O
|	O
|	O
Σ	O
|	O
hidden	O
units	O
,	O
with	O
the	O
following	O
interpretation	O
.	O
Suppose	O
that	O
A	O
transitions	O
to	O
state	O
q	O
after	O
reading	O
input	O
x	O
(	O
1	O
)	O
,	O
x	O
(	O
2	O
)	O
,	O
.	O
.	O
.	O
,	O
x	O
(	O
t	O
)	O
.	O
The	O
hidden	O
state	O
h	O
(	O
t	O
)	O
is	O
a	O
onehot	O
representation	O
of	O
the	O
pair	O
⟨	O
q	O
,	O
x	O
(	O
t	O
)	O
⟩	O
,	O
which	O
encodes	O
both	O
the	O
current	O
state	O
of	O
A	O
and	O
the	O
most	O
recent	O
input	O
symbol	O
.	O
Since	O
the	O
FSA	O
undergoes	O
a	O
state	O
transition	O
with	O
each	O
input	O
symbol	O
,	O
the	O
forget	O
gate	O
always	O
clears	O
c	O
(	O
t	O
)	O
,	O
so	O
that	O
information	O
written	O
to	O
the	O
cell	O
state	O
does	O
not	O
persist	O
beyond	O
a	O
single	O
time	O
step	O
.	O
The	O
output	O
layer	O
simply	O
detects	O
whether	O
or	O
not	O
the	O
FSA	O
is	O
in	O
an	O
accepting	O
state	O
.	O
Details	O
are	O
provided	O
in	O
Appendix	O
A.3	O
.	O
Next	O
,	O
we	O
describe	O
how	O
to	O
implement	O
a	O
PDA	O
for	O
the	O
bracket	O
prediction	O
task	O
.	O
We	O
use	O
a	O
stack	O
containing	O
all	O
unclosed	O
brackets	O
observed	O
in	O
the	O
input	O
string	O
,	O
and	O
make	O
predictions	O
based	O
on	O
the	O
top	O
item	O
of	O
the	O
stack	O
.	O
We	O
represent	O
a	O
bounded	O
stack	O
of	O
size	O
k	O
using	O
2k	O
+	O
1	O
hidden	O
units	O
.	O
The	O
first	O
k	O
−	O
1	O
positions	O
contain	O
all	O
stack	O
items	O
except	O
the	O
top	O
item	O
,	O
with	O
(	O
represented	O
by	O
the	O
value	O
1	O
,	O
[	O
represented	O
by	O
−1	O
,	O
and	O
empty	O
positions	O
represented	O
by	O
0	B-DatasetName
.	O
The	O
kth	B-DatasetName
position	O
contains	O
the	O
top	O
item	O
of	O
the	O
stack	O
.	O
The	O
next	O
k	O
positions	O
contain	O
the	O
height	O
of	O
the	O
stack	O
in	O
unary	O
notation	O
,	O
and	O
the	O
last	O
position	O
contains	O
a	O
bit	O
indicating	O
whether	O
or	O
not	O
the	O
stack	O
is	O
empty	O
.	O
For	O
example	O
,	O
after	O
reading	O
the	O
input	O
(	O
[	O
(	O
(	O
)	O
with	O
a	O
stack	O
of	O
size	O
4	O
,	O
the	O
stack	O
contents	O
(	O
[	O
(	O
are	O
represented	O
by	O
c	O
(	O
5	O
)	O
=	O
[	O
1	O
−1	O
0	B-DatasetName
1	O
1	O
1	O
1	O
0	B-DatasetName
0	B-DatasetName
]	O
⊤	O
.	O
The	O
1	O
in	O
position	O
4	O
indicates	O
that	O
the	O
top	O
item	O
of	O
the	O
stack	O
is	O
(	O
,	O
and	O
the	O
1	O
,	O
−1	O
,	O
and	O
0	B-DatasetName
in	O
positions	O
1	O
-	O
3	O
indicate	O
that	O
the	O
remainder	O
of	O
the	O
stack	O
is	O
(	O
[	O
.	O
The	O
three	O
1s	O
in	O
positions	O
5	O
-	O
8	O
indicate	O
that	O
the	O
stack	O
height	O
is	O
3	O
,	O
and	O
the	O
0	B-DatasetName
in	O
position	O
9	O
indicates	O
that	O
the	O
stack	O
is	O
not	O
empty	O
.	O
When	O
:	O
k−1	O
,	O
pushing	O
the	O
opening	O
bracket	O
to	O
the	O
stack	O
.	O
The	O
empty	O
stack	O
bit	O
is	O
then	O
set	O
to	O
0	B-DatasetName
,	O
marking	O
the	O
stack	O
as	O
non	O
-	O
empty	O
.	O
When	O
the	O
current	O
input	O
symbol	O
is	O
a	O
closing	O
bracket	O
,	O
the	O
highest	O
item	O
of	O
positions	O
1	O
through	O
k	O
−	O
1	O
is	O
deleted	O
and	O
copied	O
to	O
position	O
k	O
,	O
popping	O
the	O
top	O
item	O
from	O
the	O
stack	O
.	O
Because	O
the	O
PDA	O
network	O
is	O
quite	O
complex	O
,	O
we	O
focus	O
here	O
on	O
describing	O
how	O
the	O
top	O
stack	O
item	O
in	O
position	O
k	O
is	O
determined	O
,	O
and	O
leave	O
other	O
details	O
for	O
Appendix	O
A.4	O
.	O
Let	O
α	B-HyperparameterName
(	O
t	O
)	O
be	O
1	O
if	O
x	O
(	O
t	O
)	O
is	O
(	O
or	O
[	O
,	O
it	O
is	O
copied	O
to	O
c	O
(	O
t	O
)	O
Name	O
Formula	O
Saliency	O
R	O
(	O
c	O
)	O
t	O
,	O
i	O
(	O
X	O
)	O
=	O
∂ŷc	O
∂x	O
(	O
t	O
)	O
i	O
x	O
(	O
t	O
)	O
i	O
=	O
X	O
t	O
,	O
i	O
G	O
×	O
I	O
R	O
(	O
c	O
)	O
t	O
,	O
i	O
(	O
X	O
)	O
=	O
Xt	O
,	O
i	O
∂ŷc	O
∂x	O
(	O
t	O
)	O
i	O
x	O
(	O
t	O
)	O
i	O
=	O
X	O
t	O
,	O
i	O
IG	O
R	O
(	O
c	O
)	O
t	O
,	O
i	O
(	O
X	O
)	O
=	O
Xt	O
,	O
i	O
∫	O
1	O
0	B-DatasetName
∂ŷc	O
∂x	O
(	O
t	O
)	O
i	O
x	O
(	O
t	O
)	O
i	O
=	O
αX	O
t	O
,	O
i	O
dα	O
x	O
(	O
t	O
)	O
=	O
(	O
,	O
−1	O
if	O
x	O
(	O
t	O
)	O
=	O
[	O
,	O
and	O
0	B-DatasetName
otherwise	O
.	O
At	O
each	O
time	O
step	O
,	O
g	O
(	O
t	O
)	O
k	B-HyperparameterName
=	I-HyperparameterName
tanh	O
(	O
m	O
u	O
(	O
t	O
)	O
)	O
,	O
where	O
m	O
≫	O
0	B-DatasetName
and	O
u	O
(	O
t	O
)	O
=	O
2	O
k	O
α	B-HyperparameterName
(	O
t	O
)	O
+	O
k−1	O
∑	O
j=1	O
2	O
j−1	O
h	O
(	O
t−1	O
)	O
j	O
.	O
(	O
1	O
)	O
Observe	O
that	O
m	O
u	O
(	O
t	O
)	O
≫	O
0	B-DatasetName
when	O
α	B-HyperparameterName
(	O
t	O
)	O
=	O
1	O
,	O
and	O
m	O
u	O
(	O
t	O
)	O
≪	O
0	B-DatasetName
when	O
α	B-HyperparameterName
(	O
t	O
)	O
=	O
−1	O
.	O
Thus	O
,	O
g	O
(	O
t	O
)	O
k	O
contains	O
the	O
stack	O
encoding	O
of	O
the	O
current	O
input	O
symbol	O
if	O
it	O
is	O
an	O
opening	O
bracket	O
.	O
If	O
the	O
current	O
input	O
symbol	O
is	O
a	O
closing	O
bracket	O
,	O
then	O
α	B-HyperparameterName
(	O
t	O
)	O
=	O
0	B-DatasetName
,	O
so	O
the	O
sign	O
of	O
u	O
(	O
t	O
)	O
is	O
determined	O
by	O
the	O
highest	O
item	O
of	O
h	O
(	O
t−1	O
)	O
:	O
k−1	O
.	O

Decomposition	O
-	O
based	O
methods	O
are	O
methods	O
that	O
satisfy	O
the	O
relation	O
y	O
c	O
=	O
R	O
(	O
c	O
)	O
bias	O
+	O
∑	O
t	O
,	O
i	O
R	O
(	O
c	O
)	O
t	O
,	O
i	O
(	O
X	O
)	O
,	O
(	O
2	O
)	O
where	O
R	O
(	O
c	O
)	O
bias	O
is	O
a	O
relevance	O
score	O
assigned	O
to	O
the	O
bias	O
units	O
of	O
the	O
network	O
.	O
The	O
interpretation	O
of	O
equation	O
(	O
2	O
)	O
is	O
that	O
the	O
logit	O
scoreŷ	O
c	O
is	O
"	O
distributed	O
"	O
among	O
the	O
input	O
features	O
and	O
the	O
bias	O
units	O
,	O
so	O
that	O
the	O
relevance	O
scores	O
form	O
a	O
"	O
decomposition	O
"	O
ofŷ	O
c	O
.	O
The	O
one	O
decomposition	O
-	O
based	O
method	O
we	O
consider	O
is	O
LRP	O
,	O
which	O
computes	O
scores	O
using	O
a	O
backpropagation	O
algorithm	O
that	O
distributes	O
scores	O
layer	O
by	O
layer	O
.	O
The	O
scores	O
of	O
the	O
output	O
layer	O
are	O
initialized	O
to	O
r	O
(	O
c	O
,	O
output	O
)	O
i	O
=	O
{	O
ŷ	O
i	O
,	O
i	O
=	O
c	O
0	B-DatasetName
,	O
otherwise	O
.	O
For	O
each	O
layer	O
l	O
with	O
activation	O
z	O
(	O
l	O
)	O
,	O
activation	B-HyperparameterName
function	I-HyperparameterName
f	O
(	O
l	O
)	O
,	O
and	O
output	O
a	O
(	O
l	O
)	O
=	O
f	O
(	O
l	O
)	O
(	O
z	O
(	O
l	O
)	O
)	O
,	O
the	O
relevance	O
r	O
(	O
c	O
,	O
l	O
)	O
of	O
a	O
(	O
l	O
)	O
is	O
determined	O
by	O
the	O
following	O
propagation	O
rule	O
:	O
r	O
(	O
c	O
,	O
l	O
)	O
i	O
=	O
∑	O
l	O
′	O
∑	O
j	O
r	O
(	O
c	O
,	O
l	O
′	O
)	O
j	O
W	O
(	O
l	O
′	O
l	O
)	O
j	O
,	O
i	O
a	O
(	O
l	O
)	O
i	O
z	O
(	O
l	O
′	O
)	O
j	O
+	O
sign	O
(	O
z	O
(	O
l	O
′	O
)	O
j	O
)	O
ε	B-HyperparameterName
,	O
where	O
l	O
′	O
ranges	O
over	O
all	O
layers	O
to	O
which	O
l	O
has	O
a	O
forward	O
connection	O
via	O
W	O
(	O
l	O
′	O
l	O
)	O
and	O
ε	B-HyperparameterName
>	O
0	B-DatasetName
is	O
a	O
stabilizing	O
constant	O
.	O
3	O
For	O
the	O
LSTM	B-MethodName
gate	O
interactions	O
,	O
we	O
follow	O
Arras	O
et	O
al	O
(	O
2017b	O
)	O
in	O
treating	O
multiplicative	O
connections	O
of	O
the	O
form	O
a	O
(	O
l	O
1	O
)	O
a	O
(	O
l	O
2	O
)	O
as	O
activation	O
functions	O
of	O
the	O
form	O
a	O
(	O
l	O
1	O
)	O
f	O
(	O
l	O
2	O
)	O
(	O
)	O
,	O
where	O
a	O
(	O
l	O
1	O
)	O
is	O
f	O
(	O
t	O
)	O
,	O
i	O
(	O
t	O
)	O
,	O
or	O
o	O
(	O
t	O
)	O
.	O
The	O
final	O
attribution	O
scores	O
are	O
given	O
by	O
the	O
values	O
propagated	O
to	O
the	O
input	O
layer	O
:	O
R	O
(	O
c	O
)	O
t	O
,	O
i	O
(	O
X	O
)	O
=	O
r	O
(	O
c	O
,	O
input	O
t	O
)	O
i	O
.	O

As	O
mentioned	O
in	O
the	O
previous	O
section	O
,	O
network	O
saturation	O
causes	O
gradients	O
to	O
be	O
approximately	O
0	B-DatasetName
when	O
using	O
sigmoid	O
or	O
tanh	B-MethodName
activation	I-MethodName
functions	O
.	O
1.000	O
−6.68	O
×	O
10	O
−5	O
100.0	O
69.9	O
10	O
1.000	O
−2.46	O
×	O
10	O
−5	O
100.0	O
92.3	O
11	O
1.000	O
−9.05	O
×	O
10	O
−6	O
100.0	O
98.7	O
12	O
1.000	O
−3.33	O
×	O
10	O
−6	O
100.0	O
99.8	O
by	O
saturation	O
,	O
Table	O
4	O
shows	O
heatmaps	O
for	O
the	O
input	O
accb	O
generated	O
by	O
gradient	O
-	O
based	O
methods	O
for	O
different	O
instantiations	O
of	O
the	O
counter	O
-	O
based	O
SP	O
network	O
with	O
varying	O
degrees	O
of	O
saturation	O
.	O
Recall	B-MetricName
from	O
Section	O
4	O
that	O
counter	O
values	O
for	O
this	O
network	O
are	O
expressed	O
in	O
multiples	O
of	O
the	O
scaling	O
factor	O
v.	O
We	O
control	O
the	O
saturation	O
of	O
the	O
network	O
via	O
the	O
parameter	O
u	O
=	O
tanh	O
−1	O
(	O
v	O
)	O
.	O
For	O
all	O
three	O
gradient	O
-	O
based	O
methods	O
,	O
scores	O
for	O
a	O
decrease	O
and	O
scores	O
for	O
b	O
increase	O
as	O
u	O
increases	O
.	O
Additionally	O
,	O
saliency	O
scores	O
for	O
the	O
first	O
c	O
decrease	O
when	O
u	O
increases	O
.	O
When	O
u	O
=	O
8	O
,	O
v	O
is	O
almost	O
completely	O
saturated	O
,	O
causing	O
G	O
×	O
I	O
to	O
produce	O
all	O
-	O
zero	O
heatmaps	O
.	O
On	O
the	O
other	O
hand	O
,	O
IG	O
is	O
still	O
able	O
to	O
produce	O
nonzero	O
heatmaps	O
even	O
at	O
u	O
=	O
64	O
.	O
Thus	O
,	O
IG	O
is	O
much	O
more	O
resistant	O
to	O
the	O
effects	O
of	O
saturation	O
than	O
G	O
×	O
I.	O
According	O
to	O
Sundararajan	O
et	O
al	O
(	O
2017	O
)	O
,	O
gradient	O
-	O
based	O
methods	O
satisfy	O
the	O
axiom	O
of	O
implementation	O
invariance	O
:	O
they	O
produce	O
the	O
same	O
heatmaps	O
for	O
any	O
two	O
networks	O
that	O
compute	O
the	O
same	O
function	O
.	O
This	O
formal	O
property	O
is	O
seemingly	O
at	O
odds	O
with	O
the	O
diverse	O
array	O
of	O
heatmaps	O
appearing	O
in	O
Table	O
4	O
,	O
which	O
are	O
produced	O
for	O
networks	O
that	O
all	O
yield	O
identical	O
classifiers	O
.	O
In	O
particular	O
,	O
the	O
networks	O
with	O
u	O
=	O
8	O
,	O
16	O
,	O
and	O
64	O
yield	O
qualitatively	O
different	O
heatmaps	O
,	O
despite	O
the	O
fact	O
that	O
the	O
three	O
networks	O
are	O
distinguished	O
only	O
by	O
differences	O
in	O
v	O
of	O
less	O
than	O
0.001	O
.	O
Because	O
the	O
three	O
functions	O
are	O
technically	O
not	O
equal	O
,	O
implementation	O
invariance	O
is	O
not	O
violated	O
in	O
theory	O
;	O
but	O
the	O
fact	O
that	O
IG	O
produces	O
different	O
heatmaps	O
for	O
three	O
nearly	O
identical	O
networks	O
shows	O
that	O
the	O
intuition	O
described	O
by	O
implementation	O
invariance	O
is	O
not	O
borne	O
out	O
in	O
practice	O
.	O
Besides	O
the	O
gradient	O
-	O
based	O
methods	O
,	O
LRP	O
is	O
also	O
susceptible	O
to	O
problems	O
arising	O
from	O
saturation	O
.	O
Recall	B-MetricName
from	O
heatmaps	O
#	O
3	O
and	O
#	O
4	O
of	O
Table	O
3	O
that	O
for	O
the	O
counting	O
task	O
network	O
,	O
LRP	O
assigns	O
scores	O
of	O
0	B-DatasetName
to	O
prefixes	O
with	O
equal	O
numbers	O
of	O
as	O
and	O
bs	O
.	O
We	O
hypothesize	O
that	O
this	O
phenomenon	O
is	O
related	O
to	O
the	O
fact	O
c	O
(	O
t	O
)	O
=	O
0	B-DatasetName
after	O
reading	O
such	O
prefixes	O
,	O
since	O
the	O
counter	O
has	O
been	O
incremented	O
and	O
decremented	O
in	O
equal	O
amounts	O
.	O
Accordingly	O
,	O
we	O
test	O
whether	O
this	O
phenomenon	O
can	O
be	O
mitigated	O
by	O
desaturating	O
the	O
gates	O
so	O
that	O
c	O
(	O
t	O
)	O
does	O
not	O
exactly	O
reach	O
0	B-DatasetName
.	O
Recall	B-MetricName
that	O
the	O
white	O
-	O
box	O
LSTM	B-MethodName
gates	O
approximate	O
1	O
≈	O
σ	O
(	O
m	O
)	O
using	O
a	O
constant	O
m	O
≫	O
0	B-DatasetName
.	O
We	O
construct	O
networks	O
with	O
varying	O
values	O
of	O
m	O
and	O
compute	O
LRP	O
scores	O
on	O
a	O
randomly	O
generated	O
testing	O
set	O
of	O
1000	O
strings	O
,	O
each	O
of	O
which	O
contains	O
at	O
least	O
one	O
prefix	O
with	O
equal	O
numbers	O
of	O
as	O
and	O
bs	O
.	O
In	O
Table	O
5	O
we	O
report	O
the	O
percentage	O
of	O
examples	O
for	O
which	O
such	O
prefixes	O
receive	O
LRP	O
scores	O
of	O
0	B-DatasetName
,	O
along	O
with	O
the	O
network	O
's	O
accuracy	B-MetricName
on	O
this	O
testing	O
set	O
and	O
the	O
average	O
value	O
of	O
c	O
(	O
t	O
)	O
when	O
the	O
counter	O
reaches	O
0	B-DatasetName
.	O
Indeed	O
,	O
the	O
percentage	O
of	O
prefixes	O
receiving	O
scores	O
of	O
0	B-DatasetName
increases	O
as	O
the	O
approximation	O
c	O
(	O
t	O
)	O
≈	O
0	B-DatasetName
becomes	O
more	O
exact	O
.	O

As	O
described	O
in	O
Subsection	O
4.1	O
,	O
the	O
network	O
for	O
the	O
counting	O
task	O
simply	O
sets	O
g	O
(	O
t	O
)	O
to	O
v	O
=	O
tanh	O
(	O
u	O
)	O
when	O
x	O
(	O
t	O
)	O
=	O
a	O
and	O
−v	O
when	O
x	O
(	O
t	O
)	O
=	O
b.	O
All	O
gates	O
are	O
fixed	O
to	O
1	O
.	O
The	O
output	O
layer	O
uses	O
h	O
(	O
t	O
)	O
=	O
tanh	O
(	O
c	O
(	O
t	O
)	O
)	O
as	O
the	O
score	O
for	O
the	O
True	O
class	O
and	O
v/2	O
as	O
the	O
score	O
for	O
the	O
False	O
class	O
.	O
g	O
(	O
t	O
)	O
=	O
tanh	O
(	O
u	O
[	O
1	O
−1	O
]	O
x	O
(	O
t	O
)	O
)	O
f	O
(	O
t	O
)	O
=	O
σ	O
(	O
m	O
)	O
i	O
(	O
t	O
)	O
=	O
σ	O
(	O
m	O
)	O
o	O
(	O
t	O
)	O
=	O
σ	O
(	O
m	O
)	O
y	O
(	O
t	O
)	O
=	O
[	O
1	O
0	B-DatasetName
]	O
h	O
(	O
t	O
)	O
+	O
[	O
0	B-DatasetName
v/2	O
]	O
A.2	O
SP	O
Task	O
Network	O
(	O
Counter	O
-	O
Based	O
)	O
The	O
seven	O
counters	O
for	O
the	O
SP	O
task	O
are	O
implemented	O
as	O
follows	O
.	O
First	O
,	O
we	O
compute	O
g	O
(	O
t	O
)	O
under	O
the	O
assumption	O
that	O
one	O
of	O
the	O
first	O
four	O
counters	O
is	O
always	O
incremented	O
,	O
and	O
one	O
of	O
the	O
last	O
three	O
counters	O
is	O
always	O
incremented	O
as	O
long	O
as	O
x	O
(	O
t	O
)	O
̸	O
=	O
a.	O
g	O
(	O
t	O
)	O
=	O
tanh	O
	O
	O
	O
	O
u	O
	O
	O
	O
	O
I	O
4	O
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
1	O
	O
	O
	O
	O
x	O
(	O
t	O
)	O
	O
	O
	O
	O
Then	O
,	O
we	O
use	O
the	O
input	O
gate	O
to	O
condition	O
the	O
last	O
three	O
counters	O
on	O
the	O
value	O
of	O
the	O
first	O
four	O
counters	O
.	O
For	O
example	O
,	O
if	O
h	O
(	O
t−1	O
)	O
1	O
=	O
0	B-DatasetName
,	O
then	O
no	O
as	O
have	O
been	O
encountered	O
in	O
the	O
input	O
string	O
before	O
time	O
t.	O
In	O
that	O
case	O
,	O
the	O
input	O
gate	O
for	O
counter	O
#	O
5	O
,	O
which	O
represents	O
subsequences	O
ending	O
with	O
b	O
,	O
is	O
set	O
to	O
i	O
(	O
t	O
)	O
5	O
=	O
σ	O
(	O
−m	O
)	O
≈	O
0	B-DatasetName
.	O
This	O
is	O
because	O
a	O
b	O
encountered	O
at	O
time	O
t	O
would	O
not	O
form	O
part	O
of	O
a	O
subsequence	O
if	O
no	O
as	O
have	O
been	O
encountered	O
so	O
far	O
,	O
so	O
counter	O
#	O
5	O
should	O
not	O
be	O
incremented	O
.	O
i	O
(	O
t	O
)	O
=	O
σ	O
	O
	O
	O
	O
2	O
m	O
	O
	O
	O
	O
0	B-DatasetName
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
1	O
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
	O
	O
	O
	O
h	O
(	O
t−1	O
)	O
+	O
m	O
[	O
1	O
1	O
1	O
1	O
−1	O
−1	O
−1	O
]	O
⊤	O
)	O
All	O
other	O
gates	O
are	O
fixed	O
to	O
1	O
.	O
The	O
output	O
layer	O
sets	O
the	O
score	O
of	O
the	O
True	O
class	O
to	O
h	O
(	O
t	O
)	O
5	O
+	O
h	O
(	O
t	O
)	O
6	O
+	O
h	O
(	O
t	O
)	O
7	O
and	O
the	O
score	O
of	O
the	O
False	O
class	O
to	O
v/2	O
.	O
f	O
(	O
t	O
)	O
=	O
σ	O
(	O
m1	O
)	O
o	O
(	O
t	O
)	O
=	O
σ	O
(	O
m1	O
)	O
y	O
(	O
t	O
)	O
=	O
[	O
0	B-DatasetName
1	O
1	O
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
]	O
h	O
(	O
t	O
)	O
+	O
[	O
0	B-DatasetName
v/2	O
]	O
A.3	O
FSA	O
Network	O
Here	O
we	O
describe	O
a	O
general	O
construction	O
of	O
an	O
LSTM	B-MethodName
simulating	O
an	O
FSA	O
with	O
states	O
Q	O
,	O
accepting	O
states	O
Q	O
F	O
⊆	O
Q	O
,	O
alphabet	O
Σ	O
,	O
and	O
transition	O
function	O
δ	B-HyperparameterName
:	O
Q	O
×	O
Σ	O
Q.	O
Recall	B-MetricName
that	O
h	O
(	O
t	O
)	O
contains	O
a	O
one	O
-	O
hot	O
representation	O
of	O
pairs	O
in	O
Q	O
×	O
Σ	O
encoding	O
the	O
current	O
state	O
of	O
the	O
FSA	O
and	O
the	O
most	O
recent	O
input	O
symbol	O
.	O
The	O
initial	O
state	O
h	O
(	O
0	B-DatasetName
)	O
=	O
0	B-DatasetName
represents	O
the	O
starting	O
configuration	O
of	O
the	O
FSA	O
.	O
At	O
a	O
high	O
level	O
,	O
the	O
state	O
transition	O
system	O
works	O
as	O
follows	O
.	O
First	O
,	O
g	O
(	O
t	O
)	O
first	O
marks	O
all	O
the	O
positions	O
corresponding	O
to	O
the	O
current	O
input	O
x	O
(	O
t	O
)	O
.	O
6	O
g	O
(	O
t	O
)	O
⟨q	O
,	O
x⟩	O
=	O
{	O
v	O
,	O
x	O
=	O
x	O
(	O
t	O
)	O
0	B-DatasetName
,	O
otherwise	O
The	O
input	O
gate	O
then	O
filters	O
out	O
any	O
positions	O
that	O
do	O
not	O
represent	O
valid	O
transitions	O
from	O
the	O
previous	O
state	O
q	O
′	O
,	O
which	O
is	O
recovered	O
from	O
h	O
(	O
t−1	O
)	O
.	O
i	O
(	O
t	O
)	O
⟨q	O
,	O
x⟩	O
=	O
{	O
1	O
,	O
δ	B-HyperparameterName
(	O
q	O
′	O
,	O
x	O
)	O
=	O
q	O
0	B-DatasetName
,	O
otherwise	O
Now	O
,	O
we	O
describe	O
how	O
this	O
behavior	O
is	O
implemented	O
in	O
our	O
LSTM	B-MethodName
.	O
The	O
cell	O
state	O
update	O
is	O
straightforwardly	O
implemented	O
as	O
follows	O
:	O
g	O
(	O
t	O
)	O
=	O
tanh	O
(	O
uW	O
(	O
c	O
,	O
x	O
)	O
x	O
(	O
t	O
)	O
)	O
,	O
where	O
W	O
(	O
c	O
,	O
x	O
)	O
⟨q	O
,	O
x⟩	O
,	O
j	O
=	O
{	O
1	O
,	O
j	O
is	O
the	O
index	O
for	O
x	O
0	B-DatasetName
,	O
otherwise	O
.	O
Observe	O
that	O
the	O
matrix	O
W	O
(	O
c	O
,	O
x	O
)	O
essentially	O
contains	O
a	O
copy	O
of	O
I	O
4	O
for	O
each	O
state	O
,	O
such	O
that	O
each	O
copy	O
is	O
distributed	O
across	O
the	O
different	O
cell	O
state	O
units	O
designated	O
for	O
that	O
state	O
.	O
The	O
input	O
gate	O
is	O
more	O
complex	O
.	O
First	O
,	O
the	O
bias	O
term	O
handles	O
the	O
case	O
where	O
the	O
current	O
case	O
is	O
the	O
starting	O
state	O
q	O
0	B-DatasetName
.	O
This	O
is	O
necessary	O
because	O
the	O
initial	O
configuration	O
of	O
the	O
network	O
is	O
represented	O
by	O
h	O
(	O
0	B-DatasetName
)	O
=	O
0	B-DatasetName
.	O
b	O
(	O
i	O
)	O
⟨q	O
,	O
x⟩	O
=	O
{	O
m	O
,	O
δ	B-HyperparameterName
(	O
q	O
0	B-DatasetName
,	O
x	O
)	O
=	O
q	O
−m	O
,	O
otherwise	O
The	O
bias	O
vector	O
sets	O
i	O
(	O
t	O
)	O
⟨q	O
,	O
x⟩	O
to	O
be	O
1	O
if	O
the	O
FSA	O
transitions	O
from	O
q	O
0	B-DatasetName
to	O
q	O
after	O
reading	O
x	O
,	O
and	O
0	B-DatasetName
otherwise	O
.	O
We	O
replicate	O
this	O
behavior	O
for	O
other	O
values	O
6	O
We	O
use	O
v	O
=	O
tanh	O
(	O
1	O
)	O
≈	O
0.762	O
.	O
of	O
h	O
(	O
t−1	O
)	O
by	O
using	O
the	O
weight	O
matrix	O
W	O
(	O
i	O
,	O
h	O
)	O
,	O
taking	O
the	O
bias	O
vector	O
into	O
account	O
:	O
i	O
(	O
t	O
)	O
=	O
σ	O
(	O
W	O
(	O
i	O
,	O
h	O
)	O
h	O
(	O
t−1	O
)	O
+	O
b	O
(	O
i	O
)	O
)	O
,	O
where	O
W	O
(	O
i	O
)	O
⟨q	O
,	O
x⟩	O
,	O
⟨q	O
′	O
,	O
x	O
′	O
⟩	O
=	O
{	O
m	O
−	O
b	O
(	O
i	O
)	O
⟨q	O
,	O
x⟩	O
,	O
δ	B-HyperparameterName
(	O
q	O
′	O
,	O
x	O
)	O
=	O
q	O
−m	O
−	O
b	O
(	O
i	O
)	O
⟨q	O
,	O
x⟩	O
,	O
otherwise	O
.	O
The	O
forget	O
gate	O
is	O
fixed	O
to	O
−1	O
,	O
since	O
the	O
state	O
needs	O
to	O
be	O
updated	O
at	O
every	O
time	O
step	O
.	O
The	O
output	O
gate	O
is	O
fixed	O
to	O
1	O
.	O
f	O
(	O
t	O
)	O
=	O
σ	O
(	O
−m1	O
)	O
o	O
(	O
t	O
)	O
=	O
σ	O
(	O
m1	O
)	O
The	O
output	O
layer	O
simply	O
selects	O
hidden	O
units	O
that	O
represent	O
accepting	O
and	O
rejecting	O
states	O
:	O
y	O
(	O
t	O
)	O
=	O
W	O
h	O
(	O
t	O
)	O
,	O
where	O
W	O
c	O
,	O
⟨q	O
,	O
x⟩	O
=	O
	O
	O
	O
	O
1	O
,	O
c	O
=	O
True	O
and	O
q	O
Q	O
F	O
1	O
,	O
c	O
=	O
False	O
and	O
q	O
/	O
Q	O
F	O
0	B-DatasetName
,	O
otherwise	O
.	O

Finally	O
,	O
we	O
describe	O
how	O
the	O
PDA	O
network	O
for	O
the	O
bracket	O
prediction	O
task	O
is	O
implemented	O
.	O
Of	O
the	O
four	O
networks	O
,	O
this	O
one	O
is	O
the	O
most	O
intricate	O
.	O
Recall	B-MetricName
from	O
Subsection	O
4.2	O
that	O
we	O
implement	O
a	O
bounded	O
stack	O
of	O
size	O
k	O
using	O
2k	O
+	O
1	O
hidden	O
units	O
,	O
with	O
the	O
following	O
interpretation	O
:	O
c	O
2k+1	O
is	O
a	O
bit	O
,	O
which	O
is	O
set	O
to	O
be	O
positive	O
if	O
the	O
stack	O
is	O
empty	O
and	O
nonpositive	O
otherwise	O
.	O
We	O
represent	O
the	O
brackets	O
(	O
,	O
[	O
,	O
)	O
,	O
and	O
]	O
in	O
onehot	O
encoding	O
with	O
the	O
indices	O
1	O
,	O
2	O
,	O
3	O
,	O
and	O
4	O
,	O
respectively	O
.	O
The	O
opening	O
brackets	O
(	O
and	O
[	O
are	O
represented	O
on	O
the	O
stack	O
by	O
1	O
and	O
−1	O
,	O
respectively	O
.	O
T	O
We	O
begin	O
by	O
describing	O
g	O
(	O
t	O
)	O
.	O
Due	O
to	O
the	O
complexity	O
of	O
the	O
network	O
,	O
we	O
describe	O
the	O
weights	O
and	O
biases	O
individually	O
,	O
which	O
are	O
combined	O
as	O
follows	O
.	O
g	O
(	O
t	O
)	O
=	O
tanh	O
(	O
m	O
(	O
z	O
(	O
g	O
,	O
t	O
)	O
)	O
)	O
,	O
where	O
z	O
(	O
g	O
,	O
t	O
)	O
=	O
W	O
(	O
c	O
,	O
x	O
)	O
x	O
(	O
t	O
)	O
+	O
W	O
(	O
c	O
,	O
h	O
)	O
h	O
(	O
t−1	O
)	O
+	O
b	O
(	O
c	O
)	O
First	O
,	O
the	O
bias	O
vector	O
sets	O
c	O
(	O
t	O
)	O
2k+1	O
to	O
be	O
1	O
,	O
indicating	O
that	O
the	O
stack	O
is	O
empty	O
.	O
This	O
ensures	O
that	O
the	O
initial	O
hidden	O
state	O
h	O
(	O
t	O
)	O
=	O
0	B-DatasetName
is	O
treated	O
as	O
an	O
empty	O
stack	O
.	O
b	O
(	O
c	O
)	O
=	O
[	O
0	B-DatasetName
2	O
]	O
W	O
(	O
c	O
,	O
x	O
)	O
serves	O
three	O
functions	O
when	O
x	O
(	O
t	O
)	O
is	O
an	O
open	O
bracket	O
,	O
and	O
does	O
nothing	O
when	O
x	O
(	O
t	O
)	O
is	O
a	O
closing	O
bracket	O
.	O
First	O
,	O
it	O
pushes	O
x	O
(	O
t	O
)	O
to	O
the	O
top	O
of	O
the	O
stack	O
,	O
represented	O
by	O
c	O
k+1:2k	O
to	O
1	O
in	O
order	O
to	O
increment	O
the	O
unary	O
counter	O
for	O
the	O
height	O
of	O
the	O
stack	O
.	O
Later	O
,	O
we	O
will	O
see	O
that	O
the	O
input	O
gate	O
filters	O
out	O
all	O
positions	O
except	O
for	O
the	O
top	O
of	O
the	O
stack	O
.	O
Finally	O
,	O
W	O
(	O
c	O
,	O
x	O
)	O
sets	O
the	O
empty	O
stack	O
indicator	O
to	O
−1	O
,	O
indicating	O
that	O
the	O
stack	O
is	O
not	O
empty	O
.	O
W	O
(	O
c	O
,	O
x	O
)	O
(	O
c	O
,	O
h	O
)	O
performs	O
two	O
functions	O
.	O
First	O
,	O
it	O
completes	O
equation	O
(	O
1	O
)	O
for	O
c	O
(	O
t	O
)	O
k	O
,	O
setting	O
it	O
to	O
be	O
the	O
secondhighest	O
stack	O
item	O
from	O
the	O
previous	O
time	O
step	O
.	O
Second	O
,	O
it	O
copies	O
the	O
top	O
of	O
the	O
stack	O
to	O
the	O
first	O
k	O
−	O
1	O
positions	O
,	O
with	O
the	O
input	O
gate	O
filtering	O
out	O
all	O
but	O
the	O
highest	O
position	O
.	O
=	O
	O
	O
	O
	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
2	O
k	O
−2	O
k	O
0	B-DatasetName
0	B-DatasetName
1	O
1	O
0	B-DatasetName
0	B-DatasetName
−2	O
−2	O
0	B-DatasetName
0	B-DatasetName
	O
	O
	O
	O
W	O
W	O
(	O
c	O
,	O
h	O
)	O
=	O
	O
	O
	O
	O
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
2	O
4	O
2	O
k−1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
−1	O
0	B-DatasetName
	O
	O
	O
	O
Finally	O
,	O
the	O
−1s	O
serve	O
to	O
decrease	O
the	O
empty	O
stack	O
indicator	O
by	O
an	O
amount	O
proportional	O
to	O
the	O
stack	O
height	O
at	O
time	O
t	O
−	O
1	O
.	O
Observe	O
that	O
if	O
x	O
(	O
t	O
)	O
is	O
a	O
closing	O
bracket	O
and	O
h	O
(	O
t−1	O
)	O
represents	O
a	O
stack	O
with	O
only	O
one	O
item	O
,	O
then	O
=	O
−1	O
+	O
2	O
=	O
1	O
,	O
so	O
the	O
empty	O
stack	O
indicator	O
is	O
set	O
to	O
1	O
,	O
indicating	O
that	O
the	O
stack	O
is	O
empty	O
.	O
Otherwise	O
,	O
W	O
(	O
c	O
,	O
x	O
)	O
2k+1	O
,	O
:	O
x	O
(	O
t	O
)	O
+	O
W	O
(	O
c	O
,	O
h	O
)	O
2k+1	O
,	O
:	O
h	O
(	O
t−1	O
)	O
≤	O
−2	O
,	O
so	O
the	O
empty	O
stack	O
indicator	O
is	O
nonpositive	O
.	O
Now	O
,	O
we	O
describe	O
the	O
input	O
gate	O
,	O
given	O
by	O
the	O
following	O
.	O
W	O
(	O
c	O
,	O
x	O
)	O
i	O
(	O
t	O
)	O
=	O
σ	O
(	O
m	O
(	O
z	O
(	O
i	O
,	O
t	O
)	O
)	O
)	O
z	O
(	O
i	O
,	O
t	O
)	O
=	O
W	O
(	O
i	O
,	O
x	O
)	O
x	O
(	O
t	O
)	O
+	O
W	O
(	O
i	O
,	O
h	O
)	O
h	O
(	O
t−1	O
)	O
+	O
b	O
(	O
i	O
)	O
W	O
(	O
i	O
,	O
x	O
)	O
sets	O
the	O
input	O
gate	O
for	O
the	O
first	O
k	O
−	O
1	O
positions	O
to	O
0	B-DatasetName
when	O
x	O
(	O
t	O
)	O
is	O
a	O
closing	O
bracket	O
.	O
In	O
that	O
case	O
,	O
an	O
item	O
needs	O
to	O
be	O
popped	O
from	O
the	O
stack	O
,	O
so	O
nothing	O
can	O
be	O
copied	O
to	O
these	O
hidden	O
units	O
.	O
When	O
x	O
(	O
t	O
)	O
is	O
an	O
opening	O
bracket	O
,	O
W	O
(	O
i	O
,	O
x	O
)	O
sets	O
i	O
(	O
t	O
)	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
,	O
so	O
that	O
the	O
bracket	O
can	O
be	O
copied	O
to	O
the	O
top	O
of	O
the	O
stack	O
.	O
(	O
i	O
,	O
h	O
)	O
uses	O
a	O
matrix	O
T	O
n	O
R	O
n×n	O
,	O
defined	O
below	O
.	O
Suppose	O
v	O
represents	O
the	O
number	O
s	O
in	O
unary	O
notation	O
:	O
v	O
j	O
is	O
1	O
if	O
j	O
≤	O
s	O
and	O
0	B-DatasetName
otherwise	O
.	O
T	O
n	O
has	O
the	O
special	O
property	O
that	O
T	O
n	O
v	O
is	O
a	O
one	O
-	O
hot	O
vector	O
for	O
s.	O
W	O
(	O
i	O
,	O
x	O
)	O
=	O
2	O
	O
	O
0	B-DatasetName
0	B-DatasetName
−1	O
−1	O
1	O
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
	O
	O
W	O
Based	O
on	O
this	O
,	O
W	O
(	O
i	O
,	O
h	O
)	O
is	O
defined	O
as	O
follows	O
.	O
W	O
(	O
i	O
,	O
h	O
)	O
=	O
2	O
	O
	O
	O
	O
0	B-DatasetName
(	O
T	O
k	O
)	O
:	O
k−1	O
,	O
:	O
0	B-DatasetName
(	O
T	O
k	O
)	O
:	O
k−1	O
,	O
:	O
0	B-DatasetName
0	B-DatasetName
	O
	O
	O
	O
W	O
(	O
i	O
,	O
h	O
)	O
:	O
k−1	O
,	O
k+1:2k	O
contains	O
T	O
k	O
,	O
with	O
the	O
last	O
row	O
truncated	O
.	O
This	O
portion	O
of	O
the	O
matrix	O
converts	O
h	O
(	O
t−1	O
)	O
k+1:2k	O
,	O
which	O
contains	O
a	O
unary	O
encoding	O
of	O
the	O
stack	O
height	O
,	O
to	O
a	O
one	O
-	O
hot	O
vector	O
marking	O
the	O
position	O
of	O
the	O
top	O
of	O
the	O
stack	O
.	O
This	O
ensures	O
that	O
,	O
when	O
pushing	O
to	O
the	O
stack	O
,	O
the	O
top	O
stack	O
item	O
from	O
time	O
t	O
−	O
1	O
is	O
only	O
copied	O
to	O
the	O
appropriate	O
position	O
of	O
h	O
(	O
t	O
)	O
:	O
k−1	O
.	O
The	O
other	O
copy	O
of	O
T	O
k	O
,	O
again	O
with	O
the	O
last	O
row	O
omitted	O
,	O
occurs	O
in	O
W	O
(	O
i	O
,	O
h	O
)	O
k+2:2k	O
,	O
k+1:2k	O
.	O
This	O
copy	O
of	O
T	O
k	O
ensures	O
that	O
when	O
the	O
unary	O
counter	O
for	O
the	O
stack	O
height	O
is	O
incremented	O
,	O
only	O
the	O
appropriate	O
position	O
is	O
updated	O
.	O
Finally	O
,	O
the	O
bias	O
vector	O
ensures	O
that	O
the	O
top	O
stack	O
item	O
and	O
the	O
empty	O
stack	O
indicator	O
are	O
always	O
updated	O
.	O
b	O
(	O
i	O
)	O
=	O
	O
	O
	O
	O
−1	O
1	O
−1	O
1	O
	O
	O
	O
	O
The	O
forget	O
gate	O
is	O
responsible	O
for	O
deleting	O
portions	O
of	O
memory	O
when	O
stack	O
items	O
are	O
popped	O
.	O
f	O
(	O
t	O
)	O
=	O
σ	O
(	O
m	O
(	O
z	O
(	O
f	O
,	O
t	O
)	O
)	O
)	O
z	O
(	O
f	O
,	O
t	O
)	O
=	O
W	O
(	O
f	O
,	O
x	O
)	O
x	O
(	O
t	O
)	O
+	O
W	O
(	O
f	O
,	O
h	O
)	O
h	O
(	O
t−1	O
)	O
+	O
b	O
(	O
f	O
)	O
W	O
(	O
f	O
,	O
x	O
)	O
first	O
ensures	O
that	O
no	O
stack	O
items	O
are	O
deleted	O
when	O
an	O
item	O
is	O
pushed	O
to	O
the	O
stack	O
.	O
W	O
(	O
f	O
,	O
x	O
)	O
=	O
2	O
	O
	O
	O
	O
1	O
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
1	O
1	O
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
0	B-DatasetName
	O
	O
	O
	O
Next	O
,	O
W	O
(	O
f	O
,	O
h	O
)	O
marks	O
the	O
second	O
highest	O
stack	O
position	O
and	O
the	O
top	O
of	O
the	O
unary	O
counter	O
for	O
deletion	O
,	O
in	O
case	O
an	O
item	O
needs	O
to	O
be	O
popped	O
.	O
W	O
(	O
f	O
,	O
h	O
)	O
=	O
2	O
	O
	O
	O
	O
0	B-DatasetName
−	O
(	O
T	O
k	O
)	O
2	O
:	O
,	O
:	O
0	B-DatasetName
−T	O
k	O
0	B-DatasetName
0	B-DatasetName
	O
	O
	O
	O
Finally	O
,	O
the	O
bias	O
term	O
ensures	O
that	O
the	O
top	O
stack	O
item	O
and	O
empty	O
stack	O
indicator	O
are	O
always	O
cleared	O
.	O
b	O
(	O
i	O
)	O
=	O
	O
	O
	O
	O
1	O
−1	O
1	O
−1	O
	O
	O
	O
	O
To	O
complete	O
the	O
construction	O
,	O
we	O
fix	O
the	O
output	O
gate	O
to	O
1	O
,	O
and	O
have	O
the	O
output	O
layer	O
read	O
the	O
top	O
stack	O
position	O
:	O
o	O
(	O
t	O
)	O
=	O
σ	O
(	O
m1	O
)	O
y	O
(	O
t	O
)	O
=	O
W	O
h	O
(	O
t	O
)	O
,	O
where	O
W	O
c	O
,	O
j	O
=	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
1	O
,	O
c	O
=	O
)	O
and	O
j	O
=	O
k	O
−1	O
,	O
c	O
=	O
]	O
and	O
j	O
=	O
k	O
1	O
,	O
c	O
=	O
None	O
and	O
j	O
=	O
2k	O
+	O
1	O
0	B-DatasetName
,	O
otherwise	O
.	O

Before	O
detailing	O
the	O
annotation	O
process	O
,	O
we	O
give	O
a	O
formal	O
introduction	O
to	O
the	O
"	O
topic	O
"	O
mentioned	O
in	O
this	O
paper	O
.	O
In	O
topic	O
modeling	O
,	O
a	O
topic	O
is	O
usually	O
viewed	O
as	O
a	O
probability	O
distribution	O
over	O
a	O
fixed	O
vocabulary	O
(	O
Liu	O
et	O
al	O
,	O
2016	O
)	O
.	O
In	O
addition	O
,	O
previous	O
studies	O
on	O
argument	B-TaskName
mining	I-TaskName
usually	O
manually	O
define	O
some	O
coarse	O
-	O
grain	O
topic	O
categories	O
for	O
either	O
topic	O
-	O
dependent	O
argument	O
classification	O
or	O
clustering	O
(	O
Reimers	O
et	O
al	O
,	O
2019	O
)	O
.	O
Different	O
from	O
previous	O
work	O
,	O
topics	O
in	O
this	O
study	O
refer	O
to	O
fine	O
-	O
grained	O
topic	O
categories	O
that	O
fit	O
the	O
context	O
.	O
For	O
example	O
,	O
given	O
the	O
sentence	O
"	O
House	O
prices	O
are	O
expected	O
to	O
be	O
fragile	O
.	O
"	O
,	O
the	O
coarse	O
-	O
grained	O
topic	O
label	O
of	O
it	O
could	O
be	O
"	O
economics	O
"	O
and	O
the	O
fine	O
-	O
grained	O
label	O
is	O
"	O
house	O
price	O
"	O
.	O
Comparing	O
the	O
two	O
kinds	O
of	O
labels	O
,	O
the	O
first	O
one	O
seems	O
more	O
like	O
the	O
theme	O
of	O
an	O
article	O
which	O
is	O
useful	O
in	O
text	O
-	O
clustering	O
or	O
text	O
-	O
tilling	O
,	O
and	O
the	O
second	O
one	O
gives	O
us	O
more	O
detailed	O
description	O
on	O
the	O
topic	O
itself	O
which	O
is	O
more	O
practical	O
in	O
discourselevel	O
topic	O
chaining	O
.	O
For	O
better	O
understanding	O
of	O
our	O
annotation	O
,	O
we	O
present	O
some	O
preliminary	O
definitions	O
as	O
following	O
:	O
Discourse	O
Topic	O
Unit	O
(	O
DTU	B-DatasetName
)	O
refers	O
to	O
the	O
elementary	O
topic	O
unit	O
in	O
our	O
annotated	O
DTC	O
structure	O
.	O
In	O
the	O
literature	O
,	O
Xi	O
and	O
Zhou	O
(	O
2017	O
)	O
hold	O
the	O
view	O
that	O
each	O
sentence	O
is	O
composed	O
of	O
multiple	O
DTUs	O
with	O
different	O
sub	O
-	O
topics	O
which	O
they	O
refer	O
to	O
as	O
elementary	O
discourse	O
topic	O
unit	O
(	O
EDTU	O
)	O
.	O
Different	O
from	O
them	O
,	O
we	O
study	O
macro	O
DTC	O
structures	O
in	O
this	O
work	O
where	O
each	O
sentence	O
is	O
taken	O
as	O
an	O
independent	O
DTU	B-DatasetName
1	O
.	O
It	O
is	O
worth	O
mentioning	O
that	O
not	O
all	O
the	O
1	O
Although	O
we	O
built	O
the	O
corpus	O
based	O
on	O
RST	B-DatasetName
-	I-DatasetName
DT	I-DatasetName
,	O
it	O
remains	O
DTUs	O
are	O
topic	O
-	O
bearing	O
,	O
there	O
are	O
also	O
some	O
sentences	O
with	O
no	O
topic	O
meaning	O
,	O
e.g.	O
,	O
the	O
sentence	O
"	O
Oops	O
!	O
"	O
.	O
Topic	O
Object	O
(	O
TO	O
)	O
could	O
be	O
subject	O
,	O
object	O
,	O
or	O
other	O
noun	O
or	O
noun	O
phrase	O
in	O
the	O
DTU	B-DatasetName
which	O
can	O
provide	O
a	O
certain	O
basis	O
for	O
topic	O
chain	O
parsing	O
.	O
Usually	O
,	O
each	O
TO	O
is	O
closely	O
related	O
to	O
the	O
topic	O
of	O
its	O
DTU	B-DatasetName
,	O
and	O
each	O
DTU	B-DatasetName
maintains	O
an	O
independent	O
TO	O
set	O
.	O
Notably	O
,	O
the	O
"	O
TO	O
"	O
mentioned	O
here	O
is	O
not	O
directly	O
equivalent	O
to	O
the	O
"	O
entity	O
"	O
in	O
co	O
-	O
reference	O
resolution	O
,	O
the	O
judgment	O
of	O
TO	O
requires	O
a	O
comprehensive	O
consideration	O
of	O
document	O
context	O
.	O
For	O
example	O
,	O
given	O
the	O
DTU	B-DatasetName
"	O
Drexel	O
Burnham	O
Lambert	O
Inc.	O
is	O
the	O
adviser	O
on	O
the	O
transaction	O
.	O
"	O
,	O
if	O
the	O
surrounding	O
context	O
of	O
the	O
DTU	B-DatasetName
is	O
mainly	O
about	O
the	O
company	O
,	O
then	O
we	O
choose	O
"	O
Drexel	O
Burnham	O
Lambert	O
Inc.	O
"	O
as	O
a	O
TO	O
;	O
if	O
the	O
context	O
is	O
mainly	O
about	O
the	O
transaction	O
,	O
then	O
we	O
choose	O
"	O
transaction	O
"	O
as	O
a	O
TO	O
,	O
and	O
we	O
can	O
also	O
select	O
both	O
of	O
them	O
if	O
necessary	O
.	O
It	O
is	O
worth	O
mentioning	O
that	O
the	O
TOs	O
could	O
also	O
be	O
implicit	O
ones	O
which	O
require	O
human	O
judgments	O
.	O
Topic	O
Event	O
(	O
TE	O
)	O
refers	O
to	O
the	O
main	O
phrase	O
which	O
most	O
clearly	O
expresses	O
an	O
event	O
occurrence	O
or	O
a	O
description	O
of	O
the	O
TOs	O
in	O
the	O
DTU	B-DatasetName
.	O
For	O
the	O
DTU	B-DatasetName
u4	O
in	O
Figure	O
1	O
,	O
we	O
select	O
"	O
develop	O
vaccines	O
against	O
the	O
virus	O
"	O
as	O
the	O
topic	O
event	O
of	O
the	O
DTU	B-DatasetName
.	O
With	O
the	O
above	O
-	O
mentioned	O
definitions	O
in	O
mind	O
,	O
we	O
argue	O
that	O
each	O
DTU	B-DatasetName
is	O
composed	O
of	O
a	O
set	O
of	O
TOs	O
and	O
a	O
core	O
TE	O
.	O
Based	O
on	O
this	O
concept	O
,	O
we	O
give	O
the	O
following	O
four	O
annotation	O
suggestions	O
:	O
Given	O
two	O
adjacent	O
DTUs	O
in	O
a	O
topic	O
chain	O
,	O
their	O
TO	O
sets	O
should	O
have	O
an	O
intersection	O
in	O
the	O
topic	O
space	O
.	O
For	O
the	O
two	O
DTUs	O
u3	O
and	O
u4	O
in	O
Figure	O
1	O
,	O
although	O
the	O
two	O
corresponding	O
TO	O
sets	O
,	O
{	O
WHO	O
,	O
risky	O
to	O
directly	O
take	O
each	O
elementary	O
discourse	O
unit	O
(	O
EDU	O
)	O
as	O
a	O
DTU	B-DatasetName
since	O
there	O
are	O
many	O
competing	O
hypotheses	O
about	O
what	O
constitutes	O
an	O
EDU	O
but	O
without	O
"	O
topic	O
"	O
.	O
Previous	O
work	O
on	O
topic	O
-	O
dependent	O
argument	B-TaskName
mining	I-TaskName
usually	O
take	O
each	O
independent	O
sentence	O
as	O
an	O
elementary	O
unit	O
,	O
and	O
this	O
work	O
is	O
inspired	O
by	O
these	O
researches	O
.	O
global	O
expert	O
networks	O
}	O
and	O
{	O
researchers	O
in	O
various	O
countries	O
,	O
vaccines	O
}	O
,	O
have	O
no	O
vocabulary	O
intersection	O
,	O
they	O
are	O
highly	O
related	O
in	O
the	O
topic	O
space	O
on	O
"	O
international	O
response	O
"	O
.	O
In	O
a	O
sense	O
,	O
the	O
relationship	O
between	O
TO	O
sets	O
is	O
similar	O
to	O
that	O
between	O
mentions	O
in	O
co	O
-	O
reference	O
resolution	O
or	O
tokens	O
in	O
lexical	O
chains	O
.	O
The	O
difference	O
is	O
that	O
DTC	O
parsing	O
requires	O
not	O
only	O
the	O
correlation	O
between	O
TO	O
sets	O
but	O
also	O
the	O
topic	O
transitivity	O
between	O
DTUs	O
.	O
Therefore	O
,	O
for	O
any	O
two	O
adjacent	O
DTUs	O
on	O
a	O
topic	O
chain	O
,	O
the	O
TE	O
in	O
the	O
second	O
DTU	B-DatasetName
should	O
evolve	O
from	O
the	O
TEs	O
in	O
the	O
established	O
chain	O
where	O
the	O
first	O
DTU	B-DatasetName
is	O
located	O
.	O
Sometimes	O
,	O
a	O
DTU	B-DatasetName
may	O
have	O
topic	O
relevance	O
to	O
multiple	O
subsequent	O
DTUs	O
,	O
we	O
only	O
opt	O
for	O
the	O
closest	O
and	O
most	O
relevant	O
one	O
for	O
annotation	O
.	O
To	O
achieve	O
this	O
,	O
we	O
follow	O
two	O
principles	O
to	O
build	O
each	O
arc	O
in	O
a	O
topic	O
chain	O
:	O
(	O
1	O
)	O
For	O
each	O
DTU	B-DatasetName
,	O
we	O
search	O
its	O
topic	O
-	O
related	O
DTU	B-DatasetName
from	O
near	O
to	O
far	O
;	O
(	O
2	O
)	O
We	O
label	O
topic	O
links	O
for	O
DTUs	O
in	O
order	O
and	O
the	O
annotated	O
DTC	O
structure	O
is	O
dynamically	O
optimized	O
during	O
the	O
human	O
annotation	O
process	O
.	O
For	O
example	O
,	O
when	O
comparing	O
the	O
current	O
DTU	B-DatasetName
(	O
U	O
-	O
j	O
)	O
with	O
previous	O
ones	O
,	O
we	O
directly	O
replace	O
the	O
previously	O
annotated	O
arc	O
(	O
U	O
-	O
i	O
,	O
U	O
-	O
k	O
)	O
with	O
(	O
U	O
-	O
i	O
,	O
U	O
-	O
j	O
)	O
if	O
the	O
topic	O
relevancy	O
between	O
U	O
-	O
i	O
and	O
U	O
-	O
j	O
obviously	O
surpasses	O
that	O
between	O
U	O
-	O
i	O
and	O
U	O
-	O
k.	O
In	O
other	O
words	O
,	O
we	O
do	O
not	O
require	O
all	O
topic	O
chains	O
to	O
be	O
labeled	O
,	O
but	O
we	O
try	O
to	O
ensure	O
the	O
accuracy	B-MetricName
of	O
the	O
annotated	O
chains	O
as	O
much	O
as	O
possible	O
.	O
This	O
labeling	O
strategy	O
can	O
enhance	O
the	O
value	O
of	O
this	O
small	O
-	O
scale	O
corpus	O
to	O
some	O
extent	O
.	O
In	O
news	O
articles	O
,	O
many	O
DTUs	O
are	O
organized	O
in	O
an	O
overview	O
-	O
example	O
format	O
where	O
similarities	O
among	O
the	O
examples	O
do	O
exist	O
but	O
the	O
evolution	O
of	O
topics	O
is	O
unseen	O
.	O
In	O
this	O
study	O
,	O
we	O
do	O
not	O
consider	O
simple	O
juxtapositions	O
like	O
this	O
.	O
Taking	O
wsj_2349	O
for	O
example	O
,	O
"	O
u1	O
:	O
The	O
following	O
issues	O
were	O
recently	O
filed	O
with	O
the	O
Securities	O
and	O
Exchange	O
Commission	O
:	O
u2	O
:	O
American	O
Cyanamid	O
Co.	O
,	O
offering	O
of	O
1	O
,	O
250	O
,	O
000	O
common	O
shares	O
,	O
via	O
Merrill	O
Lynch	O
Capital	O
Markets	O
.	O
u3	O
:	O
Limited	O
Inc.	O
,	O
offering	O
of	O
up	O
to	O
$	O
300	O
million	O
of	O
debt	O
securities	O
.	O
...	O
u8	O
:	O
Trans	O
World	O
Airlines	O
Inc.	O
,	O
offering	O
of	O
...	O
"	O
.	O
There	O
is	O
a	O
certain	O
textual	O
structure	O
in	O
between	O
the	O
DTUs	O
from	O
u2	O
to	O
u8	O
(	O
e.g.	O
,	O
they	O
share	O
the	O
multinuclear	O
relation	O
List	O
in	O
the	O
RST	O
theory	O
(	O
Mann	O
and	O
Thompson	O
,	O
1988	O
)	O
)	O
,	O
but	O
the	O
topic	O
transitivity	O
is	O
weak	O
.	O
Therefore	O
,	O
we	O
do	O
not	O
mark	O
any	O
topic	O
chains	O
among	O
the	O
DTUs	O
.	O
Due	O
to	O
the	O
principle	O
of	O
saving	O
words	O
and	O
avoiding	O
repetitions	O
,	O
ellipsis	O
and	O
co	O
-	O
reference	O
occur	O
frequently	O
.	O
Under	O
this	O
condition	O
,	O
we	O
need	O
to	O
manually	O
fill	O
in	O
the	O
ellipsis	O
and	O
clarify	O
the	O
co	O
-	O
reference	O
for	O
better	O
annotation	O
.	O
Here	O
we	O
take	O
the	O
example	O
in	O
Figure	O
1	O
to	O
illustrate	O
the	O
annotation	O
process	O
.	O
Simply	O
put	O
,	O
the	O
annotation	O
process	O
is	O
also	O
the	O
process	O
of	O
comparing	O
the	O
TO	O
and	O
TEs	O
of	O
the	O
current	O
DTU	B-DatasetName
with	O
that	O
of	O
the	O
previous	O
ones	O
.	O
According	O
to	O
the	O
annotation	O
instructions	O
,	O
we	O
do	O
the	O
comparison	O
from	O
near	O
to	O
far	O
aiming	O
to	O
obtain	O
the	O
closest	O
path	O
for	O
two	O
adjacent	O
DTUs	O
on	O
the	O
chain	O
.	O
For	O
the	O
DTU	B-DatasetName
u1	O
,	O
its	O
TO	O
set	O
contains	O
two	O
topic	O
objects	O
,	O
i.e.	O
,	O
"	O
coronavirus	O
"	O
and	O
"	O
COVID	O
-	O
19	O
"	O
,	O
and	O
its	O
core	O
topic	O
event	O
can	O
be	O
sketched	O
as	O
"	O
coronavirus	O
outbreak	O
in	O
Wuhan	O
"	O
.	O
Correspondingly	O
,	O
the	O
TO	O
set	O
of	O
u2	O
contains	O
a	O
pronoun	O
object	O
"	O
it	O
"	O
,	O
referring	O
to	O
"	O
coronavirus	O
"	O
,	O
and	O
its	O
core	O
TE	O
is	O
manually	O
detected	O
as	O
"	O
there	O
is	O
still	O
no	O
idea	O
how	O
to	O
beat	O
it	O
"	O
.	O
Obviously	O
,	O
the	O
two	O
TO	O
sets	O
have	O
an	O
intersection	O
(	O
i.e.	O
,	O
"	O
coronavirus	O
"	O
)	O
and	O
the	O
TE	O
in	O
u2	O
does	O
evolve	O
from	O
that	O
in	O
u1	O
.	O
Consequently	O
,	O
we	O
mark	O
a	O
topic	O
link	O
between	O
the	O
two	O
DTUs	O
.	O
For	O
u3	O
,	O
both	O
the	O
TO	O
set	O
and	O
TE	O
do	O
not	O
meet	O
our	O
annotation	O
requirements	O
,	O
so	O
we	O
neither	O
link	O
it	O
to	O
u1	O
nor	O
u2	O
.	O
For	O
u4	O
,	O
the	O
TO	O
set	O
is	O
relevant	O
to	O
that	O
of	O
u3	O
as	O
international	O
institutions	O
and	O
the	O
two	O
TEs	O
are	O
also	O
interrelated	O
,	O
we	O
therefore	O
build	O
a	O
link	O
between	O
them	O
.	O
In	O
this	O
way	O
,	O
the	O
overall	O
vein	O
of	O
topic	O
chains	O
will	O
be	O
built	O
after	O
several	O
rounds	O
of	O
comparison	O
.	O
Notably	O
,	O
from	O
the	O
resulting	O
graph	O
we	O
find	O
that	O
the	O
topic	O
chain	O
with	O
u1	O
,	O
u2	O
,	O
u5	O
,	O
and	O
u	O
-	O
k	O
on	O
it	O
does	O
provide	O
rich	O
and	O
low	O
-	O
noise	O
information	O
about	O
the	O
evolution	O
of	O
COVID	O
-	O
19	O
,	O
which	O
reflects	O
the	O
practical	O
value	O
of	O
our	O
annotated	O
DTCs	O
.	O

Recent	O
years	O
have	O
witnessed	O
the	O
great	O
effects	O
of	O
pre	O
-	O
trained	O
language	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
Cui	O
et	O
al	O
,	O
2020	O
)	O
on	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
.	O
Following	O
previous	O
work	O
,	O
we	O
introduce	O
a	O
Bert	O
-	O
based	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
method	O
in	O
our	O
baseline	O
system	O
.	O
Given	O
a	O
discourse	O
with	O
k	O
-	O
1	O
DTUs	O
,	O
we	O
use	O
the	O
pretrained	O
Bert	O
3	O
model	O
to	O
encode	O
the	O
entire	O
discourse	O
where	O
each	O
DTU	B-DatasetName
is	O
surrounded	O
by	O
the	O
[	O
CLS	O
]	O
and	O
[	O
SEP	O
]	O
tokens	O
.	O
And	O
we	O
take	O
the	O
Bert	O
output	O
corresponding	O
to	O
[	O
CLS	O
]	O
as	O
our	O
DTU	B-DatasetName
representation	O
.	O
Following	O
previous	O
work	O
,	O
we	O
also	O
fine	O
-	O
tuned	O
the	O
pre	O
-	O
trained	O
language	O
model	O
parameters	O
during	O
the	O
training	O
process	O
.	O
For	O
the	O
convenience	O
of	O
calculation	O
,	O
a	O
zero	O
-	O
initialized	O
vector	O
u	O
z	O
is	O
added	O
at	O
the	O
end	O
of	O
the	O
DTU	B-DatasetName
sequence	O
for	O
the	O
tail	O
DTUs	O
of	O
the	O
topic	O
chains	O
or	O
the	O
isolated	O
DTUs	O
to	O
point	O
to	O
,	O
obtaining	O
U	O
=	O
(	O
u	O
1	O
,	O
.	O
.	O
.	O
,	O
u	O
k−1	O
,	O
u	O
z	O
)	O
.	O
For	O
dependency	B-TaskName
parsing	I-TaskName
,	O
we	O
simply	O
build	O
a	O
bi	O
-	O
linear	O
function	O
between	O
U	O
and	O
its	O
duplicate	O
to	O
achieve	O
it	O
,	O
as	O
following	O
:	O
U	O
α	B-HyperparameterName
=	O
W	O
α	B-HyperparameterName
U	O
+	O
b	O
α	B-HyperparameterName
U	O
β	B-HyperparameterName
=	O
W	O
β	B-HyperparameterName
U	O
+	O
b	O
β	B-HyperparameterName
s	O
=	O
U	O
T	O
α	B-HyperparameterName
WU	O
β	B-HyperparameterName
where	O
U	O
α	B-HyperparameterName
and	O
U	O
β	B-HyperparameterName
are	O
(	O
D×k	O
)	O
matrices	O
representing	O
U	O
and	O
its	O
duplicate	O
,	O
W	O
R	O
D×D	O
denotes	O
the	O
parameters	O
of	O
the	O
bilinear	O
term	O
,	O
and	O
s	O
R	O
k×k	O
refers	O
to	O
the	O
scores	O
for	O
each	O
DTU	B-DatasetName
upon	O
its	O
candidate	O
successor	O
DTUs	O
.	O
The	O
detailed	O
system	O
configuration	O
is	O
presented	O
in	O
the	O
Appendix	O
.	O
We	O
measure	O
the	O
micro	O
-	O
averaged	O
F1	B-MetricName
scores	O
of	O
both	O
topic	O
links	O
and	O
chains	O
for	O
performance	O
,	O
and	O
we	O
do	O
not	O
take	O
those	O
isolated	O
DTUs	O
into	O
consideration	O
to	O
avoid	O
the	O
overestimation	O
of	O
performance	O
.	O
For	O
human	O
performance	O
,	O
we	O
asked	O
5	O
other	O
researchers	O
majoring	O
in	O
human	O
language	O
analysis	O
to	O
manually	O
annotate	O
the	O
test	O
set	O
and	O
took	O
the	O
averaged	O
F1	B-MetricName
scores	O
as	O
human	O
performance	O
.	O
Experimental	O
results	O
in	O
Table	O
3	O
show	O
that	O
fine	O
-	O
tuning	O
the	O
contextualized	O
Bert	O
model	O
can	O
achieve	O
a	O
great	O
performance	O
close	O
to	O
human	O
level	O
.	O
By	O
observing	O
the	O
model	O
outputs	O
(	O
sampled	O
in	O
Appendix	O
)	O
,	O
we	O
find	O
that	O
the	O
automatically	O
parsed	O
chain	O
structures	O
are	O
highly	O
consistent	O
with	O
the	O
manual	O
annotations	O
,	O
which	O
indicates	O
the	O
high	O
reliability	O
of	O
our	O
corpus	O
.	O
Notably	O
,	O
the	O
obtained	O
system	O
has	O
good	O
generalization	O
and	O
robustness	O
,	O
and	O
can	O
be	O
easily	O
migrated	O
to	O
other	O
NLP	O
tasks	O
for	O
DTC	O
structure	O
incorporation	O
.	O

We	O
used	O
the	O
768D	O
Bert	O
-	O
base	O
and	O
1024D	O
Bert	O
-	O
large	O
model	O
for	O
DTU	B-DatasetName
representation	O
.	O
In	O
order	O
to	O
prevent	O
memory	O
overflow	O
,	O
we	O
segment	O
each	O
article	O
according	O
to	O
the	O
maximum	O
length	O
of	O
64	O
,	O
and	O
encode	O
the	O
segmented	O
text	O
fragments	O
in	O
turn	O
.	O
We	O
manually	O
set	O
the	O
dropout	O
rate	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
L2	B-HyperparameterName
regularization	I-HyperparameterName
value	O
by	O
0.2	O
,	O
1e	O
-	O
5	O
,	O
and	O
1e	O
-	O
5	O
,	O
respectively	O
,	O
according	O
to	O
their	O
contributions	O
to	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
,	O
and	O
the	O
number	O
of	O
hyper	O
-	O
parameter	O
search	O
trials	O
was	O
around	O
15	O
.	O
We	O
trained	O
the	O
models	O
iteratively	O
on	O
the	O
training	O
corpus	O
for	O
20	O
rounds	O
with	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
set	O
to	O
1	O
(	O
document	O
)	O
,	O
and	O
we	O
got	O
the	O
best	O
model	O
around	O
the	O
18	O
-	O
th	O
round	O
.	O
We	O
implemented	O
the	O
codes	O
based	O
on	O
the	O
PyTorch	O
framework	O
,	O
and	O
all	O
the	O
experiments	O
were	O
conducted	O
on	O
the	O
NVIDIA	O
Tesla	O
P40	O
GPUs	O
with	O
the	O
random	O
seed	O
set	O
to	O
2	O
.	O
The	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
in	O
each	O
model	O
and	O
the	O
runtime	O
time	O
of	O
each	O
system	O
are	O
shown	O
in	O
the	O
table	O
below	O
.	O

The	O
decoder	O
network	O
is	O
similar	O
to	O
GNMT	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
.	O
At	O
each	O
time	O
-	O
step	O
t	O
,	O
let	O
y	O
0	B-DatasetName
t−1	O
R	O
e	O
denotes	O
the	O
word	O
embedding	O
of	O
y	O
t−1	O
and	O
y	O
1	O
t−1	O
R	O
h	O
denotes	O
the	O
output	O
of	O
bottom	O
LSTM	B-MethodName
from	O
previous	O
time	O
-	O
step	O
.	O
The	O
attention	O
network	O
calculates	O
the	O
context	O
vector	O
a	O
t	O
as	O
the	O
weighted	O
sum	O
of	O
source	O
annotation	O
vectors	O
:	O
a	O
t	O
=	O
S	O
i=1	O
α	B-HyperparameterName
t	O
,	O
i	O
x	O
i	O
(	O
6	O
)	O
Different	O
from	O
GNMT	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
,	O
we	O
use	O
the	O
concatenation	O
of	O
y	O
0	B-DatasetName
t−1	O
and	O
y	O
1	O
t−1	O
as	O
the	O
query	O
vector	O
for	O
attention	O
network	O
,	O
as	O
described	O
follows	O
:	O
h	O
t	O
=	O
[	O
y	O
0	B-DatasetName
t−1	O
T	O
;	O
y	O
1	O
t−1	O
T	O
]	O
T	O
(	O
7	O
)	O
e	O
t	O
,	O
i	O
=	O
v	O
T	O
a	O
tanh	O
(	O
W	O
a	O
h	O
t	O
+	O
U	O
a	O
x	O
i	O
)	O
(	O
8	O
)	O
α	B-HyperparameterName
t	O
,	O
i	O
=	O
exp	O
(	O
e	O
t	O
,	O
i	O
)	O
S	O
j=1	O
exp	O
(	O
e	O
t	O
,	O
j	O
)	O
This	O
approach	O
is	O
also	O
used	O
in	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
context	O
vector	O
a	O
t	O
is	O
then	O
fed	O
to	O
all	O
decoder	O
LSTMs	O
.	O
The	O
probability	O
of	O
the	O
next	O
word	O
y	O
t	O
is	O
simply	O
modeled	O
using	O
a	O
softmax	B-MethodName
layer	O
on	O
the	O
output	O
of	O
top	O
LSTM	B-MethodName
:	O
p	O
(	O
y	O
t	O
|	O
x	O
,	O
y	O
<	O
t	O
)	O
=	O
softmax	B-MethodName
(	O
y	O
t	O
,	O
y	O
L	O
dec	O
t	O
)	O
(	O
10	O
)	O
We	O
set	O
L	O
dec	O
to	O
8	O
in	O
all	O
our	O
experiments	O
.	O
3	O
Experimental	O
Features	O

For	O
all	O
our	O
models	O
,	O
we	O
adopt	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
(	O
β	B-HyperparameterName
1	O
=	O
0.9	O
,	O
β	B-HyperparameterName
2	O
=	O
0.999	O
and	O
=	O
1×	O
10	O
−8	O
)	O
as	O
the	O
optimizer	B-HyperparameterName
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
5	O
×	O
10	O
−4	O
.	O
We	O
gradually	O
halve	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
during	O
the	O
training	O
process	O
.	O
As	O
a	O
common	O
way	O
to	O
train	O
RNNs	O
,	O
we	O
clip	O
the	O
norm	O
of	O
gradient	O
to	O
a	O
predefined	O
value	O
5.0	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
128	O
.	O
We	O
use	O
dropout	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
to	O
avoid	O
overfitting	O
with	O
a	O
keep	O
probability	O
of	O
0.8	O
.	O

Test	O
(	O
BLEU	B-MetricName
)	O
Baseline	O
25.7	O
+	O
Synthetic	O
26.1	O
+	O
Ensemble	O
26.7	O
Table	O
1	O
:	O
English	O
-	O
German	O
translation	O
results	O
on	O
newstest2017	O
.	O
Table	O
1	O
show	O
the	O
results	O
of	O
English	O
-	O
German	O
Translation	B-TaskName
.	O
The	O
baseline	O
system	O
is	O
trained	O
on	O
preprocessed	O
parallel	O
data	O
4	O
.	O
For	O
synthetic	O
data	O
,	O
we	O
randomly	O
sample	O
10	O
M	O
German	O
sentences	O
from	O
NewsCrawl2016	O
and	O
translate	O
them	O
back	O
to	O
English	O
using	O
an	O
German	O
-	O
English	O
model	O
.	O
However	O
,	O
we	O
found	O
random	O
sampling	O
do	O
not	O
work	O
well	O
.	O
As	O
a	O
result	O
,	O
for	O
Chinese	O
-	O
English	O
translation	O
,	O
we	O
select	O
monolingual	O
data	O
according	O
to	O
development	O
set	O
.	O
We	O
first	O
train	O
one	O
baseline	O
model	O
and	O
continue	O
to	O
train	O
4	O
models	O
on	O
synthetic	O
data	O
with	O
different	O
shuffles	O
.	O
Next	O
we	O
ensemble	O
4	O
models	O
and	O
get	O
the	O
final	O
results	O
.	O
We	O
found	O
this	O
approach	O
do	O
not	O
lead	O
to	O
substantial	O
improvements	O
.	O

Test	O
(	O
BLEU	B-MetricName
)	O
We	O
use	O
all	O
training	O
data	O
(	O
CWMT	O
Corpus	O
,	O
UN	O
Parallel	O
Corpus	O
and	O
News	O
Commentary	O
)	O
to	O
train	O
a	O
baseline	O
system	O
.	O
The	O
Chinese	O
sentences	O
are	O
segmented	O
using	O
Stanford	O
Segmenter	O
5	O
.	O
For	O
English	O
sentences	O
,	O
we	O
use	O
the	O
moses	O
tokenizer	O
6	O
.	O
We	O
filter	O
bad	O
sentences	O
according	O
to	O
the	O
alignment	O
score	O
obtained	O
by	O
fast	O
-	O
align	O
toolkit	O
7	O
and	O
remove	O
duplications	O
in	O
the	O
training	O
data	O
.	O
The	O
preprocessed	O
training	O
data	O
consists	O
of	O
19	O
M	O
bilingual	O
pairs	O
.	O
As	O
noted	O
earlier	O
,	O
the	O
monolingual	O
data	O
is	O
selected	O
using	O
newsdev2017	O
.	O
We	O
first	O
train	O
4	O
L2R	O
models	O
and	O
one	O
R2L	O
model	O
on	O
training	O
data	O
,	O
then	O
we	O
finetune	O
our	O
model	O
on	O
a	O
mixture	O
of	O
2.5	O
M	O
synthetic	O
bilingual	O
pairs	O
and	O
2.5	O
M	O
bilingual	O
pairs	O
sampled	O
from	O
CWMT	O
corpus	O
.	O
As	O
shown	O
in	O
Table	O
3	O
show	O
the	O
results	O
of	O
English	O
-	O
Chinese	O
Translation	B-TaskName
.	O
We	O
use	O
our	O
reimplementation	O
of	O
DL4MT	O
to	O
train	O
English	O
-	O
Chinese	O
models	O
on	O
CWMT	O
and	O
UN	O
parallel	O
corpus	O
.	O
The	O
preprocessing	O
steps	O
,	O
including	O
word	O
segmentation	O
,	O
tokenization	O
,	O
and	O
sentence	O
filtering	O
,	O
are	O
almost	O
the	O
same	O
as	O
Section	O
4.2	O
,	O
except	O
that	O
we	O
limited	O
the	O
vocabulary	O
size	O
to	O
50	O
K	O
and	O
split	O
all	O
target	O
side	O
OOVs	O
into	O
characters	O
.	O
For	O
synthetic	O
parallel	O
data	O
,	O
we	O
use	O
SRILM	O
8	O
to	O
train	O
a	O
5	O
-	O
gram	O
KN	O
language	O
model	O
on	O
XinhuaNet2011	O
and	O
select	O
2.5	O
M	O
sentences	O
from	O
XinhuaNet2011	O
according	O
to	O
their	O
perplexities	O
.	O
We	O
obtained	O
+3.9	O
BLEU	B-MetricName
score	I-MetricName
when	O
tuning	O
the	O
single	O
best	O
model	O
on	O
a	O
mixture	O
of	O
2.5	O
M	O
synthetic	O
bilingual	O
pairs	O
and	O
2.5	O
M	O
bilingual	O
pairs	O
selected	O
from	O
CWMT	O
parallel	O
data	O
randomly	O
.	O
We	O
further	O
gain	O
+1.5	O
BLEU	B-MetricName
score	I-MetricName
when	O
ensembling	O
4	O
models	O
.	O

Figure	O
1	O
:	O
Examples	O
of	O
text	O
with	O
rigid	O
formats	O
.	O
In	O
lyrics	O
,	O
the	O
syllables	O
of	O
the	O
lyric	O
words	O
must	O
align	O
with	O
the	O
tones	O
of	O
the	O
notation	O
.	O
In	O
SongCi	O
and	O
Sonnet	O
,	O
there	O
are	O
strict	O
rhyming	O
schemes	O
and	O
the	O
rhyming	O
words	O
are	O
labeled	O
in	O
red	O
color	O
and	O
italic	O
font	O
.	O
2014	O
;	O
Gehring	O
et	O
al	O
,	O
2017	O
)	O
,	O
Transformer	B-MethodName
and	O
its	O
variants	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
;	O
,	O
pre	O
-	O
trained	O
auto	O
-	O
regressive	O
language	O
models	O
such	O
as	O
XLNet	B-MethodName
and	O
GPT2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
,	O
etc	O
.	O
Performance	O
has	O
been	O
improved	O
significantly	O
in	O
lots	O
of	O
tasks	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
,	O
dialogue	O
systems	O
(	O
Vinyals	O
and	O
Le	O
,	O
2015	O
;	O
Shang	O
et	O
al	O
,	O
2015	O
;	O
Li	O
,	O
2020	O
)	O
,	O
text	B-TaskName
summarization	I-TaskName
(	O
Rush	O
et	O
al	O
,	O
2015	O
;	O
Li	O
et	O
al	O
,	O
2017	O
;	O
See	O
et	O
al	O
,	O
2017	O
)	O
,	O
story	O
telling	O
(	O
Fan	O
et	O
al	O
,	O
2018	O
;	O
See	O
et	O
al	O
,	O
2019	O
)	O
,	O
poetry	O
writing	O
(	O
Zhang	O
and	O
Lapata	O
,	O
2014	O
;	O
Lau	O
et	O
al	O
,	O
2018	O
;	O
Liao	O
et	O
al	O
,	O
2019	O
)	O
,	O
etc	O
.	O
Generally	O
,	O
most	O
of	O
the	O
above	O
mentioned	O
tasks	O
can	O
be	O
regarded	O
as	O
free	O
text	B-TaskName
generation	I-TaskName
,	O
which	O
means	O
that	O
no	O
constraints	O
on	O
the	O
format	O
and	O
structure	O
,	O
say	O
the	O
number	O
of	O
words	O
and	O
rhyming	O
rules	O
.	O
Note	O
that	O
tasks	O
of	O
dialogue	B-TaskName
generation	I-TaskName
and	O
story	O
telling	O
are	O
almost	O
in	O
an	O
open	O
-	O
ending	O
generation	O
style	O
as	O
long	O
as	O
the	O
generated	O
content	O
is	O
relevant	O
with	O
the	O
conditional	O
input	O
text	O
.	O
Although	O
there	O
are	O
formats	O
constraints	O
on	O
the	O
poetry	O
text	O
,	O
the	O
proposed	O
models	O
just	O
treat	O
the	O
formats	O
as	O
kind	O
of	O
latent	O
information	O
and	O
let	O
the	O
model	O
capture	O
this	O
feature	O
implicitly	O
during	O
training	O
(	O
Liao	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
model	O
trained	O
on	O
the	O
five	O
-	O
character	O
quatrain	O
corpus	O
can	O
not	O
generate	O
seven	O
-	O
character	O
verses	O
.	O
Moreover	O
,	O
it	O
is	O
impossible	O
to	O
trigger	O
these	O
models	O
to	O
generate	O
satisfying	O
results	O
according	O
to	O
arbitrary	O
new	O
defined	O
formats	O
.	O
In	O
practice	O
we	O
will	O
confront	O
some	O
special	O
text	O
paradigms	O
such	O
as	O
Lyrics	O
(	O
assume	O
the	O
music	O
score	O
is	O
given	O
)	O
,	O
Sonnet	O
(	O
say	O
Shakespeare	O
's	O
Sonnets	O
(	O
Shakespeare	O
,	O
2000	O
)	O
)	O
,	O
SongCi	O
(	O
a	O
kind	O
of	O
Ci	O
.	O
Ci	O
is	O
a	O
type	O
of	O
lyric	O
poetry	O
in	O
the	O
tradition	O
of	O
Classical	O
Chinese	O
poetry	O
.	O
2	O
,	O
SongCi	O
is	O
the	O
Ci	O
created	O
during	O
Song	O
dynasty	O
)	O
,	O
etc	O
.	O
,	O
and	O
some	O
examples	O
are	O
illustrated	O
in	O
Figure	O
1	O
.	O
The	O
typical	O
characteristics	O
of	O
these	O
text	O
can	O
be	O
categorized	O
into	O
three	O
folds	O
:	O
(	O
1	O
)	O
The	O
assembling	O
of	O
text	O
must	O
comply	O
fully	O
with	O
the	O
predefined	O
rigid	O
formats	O
.	O
Assume	O
that	O
the	O
music	O
score	O
is	O
composed	O
,	O
then	O
the	O
lyricist	O
must	O
fill	O
the	O
lyric	O
content	O
strictly	O
tally	O
with	O
the	O
schemes	O
lie	O
in	O
the	O
notation	O
.	O
Take	O
partial	O
of	O
song	O
"	O
Edelweiss	O
"	O
as	O
shown	O
in	O
the	O
first	O
row	O
of	O
Figure	O
1	O
as	O
example	O
,	O
the	O
syllables	O
of	O
the	O
lyric	O
words	O
must	O
align	O
with	O
the	O
tones	O
of	O
the	O
notation	O
.	O
The	O
second	O
row	O
of	O
Figure	O
1	O
depicts	O
the	O
content	O
of	O
a	O
SongCi	O
created	O
based	O
on	O
the	O
CiPai	O
of	O
"	O
Bu	O
Suan	O
Zi	O
"	O
.	O
Given	O
the	O
CiPai	O
,	O
the	O
number	O
of	O
characters	O
and	O
the	O
syntactical	O
structure	O
of	O
the	O
content	O
are	O
also	O
defined	O
(	O
e.g.	O
,	O
the	O
number	O
of	O
characters	O
of	O
each	O
clause	O
:	O
5	O
,	O
5	O
.	O
7	O
,	O
5	O
.	O
5	O
,	O
5	O
.	O
7	O
,	O
5	O
.	O
)	O
.	O
(	O
2	O
)	O
The	O
arrangement	O
of	O
the	O
content	O
must	O
obey	O
the	O
defined	O
rhyming	O
schemes	O
.	O
For	O
example	O
,	O
all	O
the	O
final	O
words	O
(	O
words	O
in	O
red	O
color	O
and	O
italic	O
font	O
)	O
of	O
the	O
SongCi	O
content	O
in	O
Figure1	O
are	O
rhyming	O
(	O
the	O
spelling	O
of	O
each	O
word	O
is	O
:	O
"	O
zhu	O
"	O
,	O
"	O
yu	O
"	O
,	O
"	O
du	O
"	O
,	O
and	O
"	O
gu	O
"	O
.	O
)	O
.	O
The	O
example	O
in	O
the	O
third	O
row	O
of	O
Figure	O
1	O
comes	O
from	O
Shakespeare	O
's	O
"	O
Sonnet	O
116	O
"	O
(	O
Shakespeare	O
,	O
2000	O
)	O
,	O
the	O
first	O
four	O
sentences	O
.	O
Usually	O
,	O
the	O
rhyming	O
schemes	O
of	O
Shakespeare	O
's	O
Sonnets	O
is	O
"	O
ABAB	O
CDCD	O
EFEF	O
GG	O
"	O
3	O
.	O
In	O
the	O
example	O
,	O
the	O
rhyming	O
words	O
in	O
scheme	O
"	O
ABAB	O
"	O
are	O
"	O
minds	O
"	O
,	O
"	O
love	O
"	O
,	O
"	O
finds	O
"	O
,	O
and	O
"	O
remove	O
"	O
.	O
(	O
3	O
)	O
Even	O
though	O
the	O
format	O
is	O
rigid	O
,	O
the	O
sentence	O
integrity	O
must	O
always	O
be	O
guaranteed	O
.	O
Incomplete	O
sentence	O
such	O
as	O
"	O
love	O
is	O
not	O
the	O
"	O
is	O
inappropriate	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
text	B-TaskName
generation	I-TaskName
based	O
on	O
the	O
predefined	O
rigid	O
formats	O
constraints	O
has	O
not	O
been	O
well	O
investigated	O
yet	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
simple	O
and	O
elegant	O
framework	O
named	O
SongNet	B-MethodName
to	O
address	O
this	O
challenging	O
problem	O
.	O
The	O
backbone	O
of	O
the	O
framework	O
is	O
a	O
Transformer	B-MethodName
-	O
based	O
auto	O
-	O
regressive	O
language	O
model	O
.	O
Considering	O
the	O
three	O
folds	O
characteristics	O
mentioned	O
above	O
,	O
we	O
introduce	O
sets	O
of	O
tailor	O
-	O
designed	O
indicating	O
symbols	O
to	O
improve	O
the	O
modeling	O
performance	O
,	O
especially	O
for	O
the	O
robustness	O
of	O
the	O
format	O
,	O
rhyme	O
,	O
as	O
well	O
as	O
sentence	O
integrity	O
.	O
We	O
improve	O
the	O
attention	O
mechanism	O
to	O
impel	O
the	O
model	O
to	O
capture	O
the	O
future	O
information	O
on	O
the	O
format	O
to	O
further	O
enhance	O
sentence	O
integrity	O
.	O
Inspired	B-DatasetName
by	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
GPT	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2018	O
(	O
Radford	O
et	O
al	O
,	O
,	O
2019	O
,	O
a	O
pretraining	O
and	O
fine	O
-	O
tuning	O
framework	O
is	O
designed	O
to	O
further	O
improve	O
the	O
generation	O
quality	O
.	O
To	O
verify	O
the	O
performance	O
of	O
our	O
framework	O
,	O
we	O
collect	O
two	O
corpora	O
,	O
SongCi	O
and	O
Sonnet	O
,	O
in	O
Chinese	O
and	O
English	O
respectively	O
.	O
Extensive	O
experiments	O
on	O
the	O
collected	O
datasets	O
demonstrate	O
that	O
our	O
proposed	O
framework	O
can	O
generate	O
satisfying	O
results	O
in	O
terms	O
of	O
both	O
the	O
tailor	O
-	O
designed	O
automatic	O
metrics	O
including	O
format	O
accuracy	B-MetricName
,	O
rhyming	O
accuracy	B-MetricName
,	O
sentence	O
integrity	O
,	O
as	O
well	O
as	O
the	O
human	O
evaluation	O
results	O
on	O
relevance	O
,	O
fluency	O
,	O
and	O
style	O
.	O
In	O
summary	O
,	O
our	O
contributions	O
are	O
as	O
follows	O
:	O
We	O
propose	O
to	O
tackle	O
a	O
new	O
challenging	O
task	O
:	O
rigid	O
formats	O
controlled	O
text	B-TaskName
generation	I-TaskName
.	O
A	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
framework	O
named	O
SongNet	B-MethodName
is	O
designed	O
to	O
address	O
the	O
problem	O
.	O
Sets	O
of	O
symbols	O
are	O
tailor	O
-	O
designed	O
to	O
improve	O
the	O
modeling	O
performance	O
.	O
We	O
improve	O
the	O
attention	O
mechanism	O
to	O
impel	O
the	O
model	O
to	O
capture	O
the	O
future	O
information	O
to	O
further	O
enhance	O
the	O
sentence	O
integrity	O
.	O
To	O
verify	O
the	O
performance	O
of	O
our	O
framework	O
SongNet	B-MethodName
,	O
we	O
collect	O
two	O
corpora	O
,	O
SongCi	O
and	O
Sonnet	O
,	O
in	O
Chinese	O
and	O
English	O
respectively	O
.	O
We	O
design	O
several	O
automatic	O
evaluation	O
metrics	O
and	O
human	O
evaluation	O
metrics	O
to	O
conduct	O
the	O
performance	O
evaluation	O
.	O
Extensive	O
experiments	O
conducted	O
on	O
two	O
collected	O
corpora	O
demonstrate	O
that	O
our	O
proposed	O
framework	O
generates	O
significantly	O
better	O
results	O
given	O
arbitrary	O
formats	O
,	O
including	O
the	O
cold	O
-	O
start	O
formats	O
or	O
even	O
the	O
formats	O
newly	O
defined	O
by	O
ourselves	O
.	O

The	O
task	O
of	O
rigid	O
formats	O
controlled	O
text	B-TaskName
generation	I-TaskName
is	O
defined	O
as	O
follows	O
:	O
Input	O
:	O
a	O
rigid	O
format	O
C	O
C	O
:	O
C	O
=	O
{	O
c	O
0	B-DatasetName
c	O
1	O
c	O
2	O
c	O
3	O
,	O
c	O
0	B-DatasetName
c	O
1	O
c	O
2	O
c	O
3	O
c	O
4	O
c	O
5	O
.	O
}	O
(	O
1	O
)	O
where	O
C	O
is	O
the	O
set	O
of	O
all	O
possible	O
formats	O
.	O
Note	O
that	O
we	O
can	O
define	O
arbitrary	O
new	O
formats	O
not	O
restricted	O
to	O
the	O
ones	O
pre	O
-	O
defined	O
in	O
the	O
corpus	O
,	O
thus	O
|	O
C	O
|	O
.	O
Format	O
token	O
c	O
i	O
denotes	O
a	O
place	O
-	O
holder	O
symbol	O
of	O
C	O
which	O
need	O
to	O
be	O
translated	O
into	O
a	O
real	O
word	O
token	O
.	O
Format	O
C	O
contains	O
10	O
words	O
plus	O
two	O
extra	O
punctuation	O
characters	O
"	O
,	O
"	O
and	O
"	O
.	O
"	O
Output	O
:	O
a	O
natural	O
language	O
sentence	O
Y	O
Y	O
which	O
tally	O
with	O
the	O
defined	O
format	O
C	O
:	O
Y	O
=	O
love	O
is	O
not	O
love	O
,	O
bends	O
with	O
the	O
remover	O
to	O
remove	O
.	O
where	O
the	O
example	O
sentences	O
are	O
extracted	O
from	O
the	O
Shakespeare	O
's	O
Sonnets	O
(	O
Shakespeare	O
,	O
2000	O
)	O
.	O
From	O
the	O
result	O
Y	O
we	O
can	O
observe	O
that	O
the	O
count	O
of	O
words	O
is	O
10	O
which	O
is	O
consistent	O
with	O
the	O
format	O
C.	O
The	O
punctuation	O
characters	O
"	O
,	O
"	O
and	O
"	O
.	O
"	O
are	O
also	O
correct	O
.	O
Thus	O
,	O
we	O
claim	O
that	O
it	O
is	O
a	O
100	O
%	O
format	O
accuracy	B-MetricName
result	O
.	O
Also	O
,	O
since	O
the	O
two	O
clause	O
sentences	O
are	O
complete	O
,	O
we	O
can	O
get	O
a	O
good	O
sentence	O
integrity	O
score	O
.	O
If	O
C	O
is	O
defined	O
on	O
the	O
literary	O
genres	O
of	O
SongCi	O
or	O
Sonnet	O
which	O
have	O
rhyming	O
constraints	O
,	O
the	O
rhyming	O
performance	O
should	O
be	O
evaluated	O
as	O
well	O
.	O
Recall	B-MetricName
that	O
C	O
can	O
be	O
arbitrary	O
and	O
flexible	O
,	O
thus	O
we	O
can	O
rebuild	O
a	O
new	O
format	O
C	O
based	O
on	O
the	O
generated	O
result	O
Y	O
by	O
masking	O
partial	O
content	O
,	O
say	O
C	O
=	O
{	O
c	O
0	B-DatasetName
c	O
1	O
c	O
2	O
love	O
,	O
c	O
0	B-DatasetName
c	O
1	O
c	O
2	O
c	O
3	O
c	O
4	O
remove	O
.	O
}	O
,	O
then	O
we	O
may	O
obtain	O
better	O
results	O
by	O
re	O
-	O
generating	O
based	O
on	O
C	O
.	O
We	O
name	O
this	O
operation	O
as	O
polishing	O
.	O
Finally	O
,	O
the	O
target	O
of	O
this	O
problem	O
is	O
to	O
find	O
a	O
mapping	O
function	O
G	O
to	O
conduct	O
the	O
rigid	O
formats	O
controlled	O
text	B-TaskName
generation	I-TaskName
:	O
Y	O
=	O
G	O
(	O
C	O
)	O
(	O
2	O
)	O
3	O
Framework	O
Description	O

As	O
shown	O
in	O
Figure	O
2	O
,	O
the	O
backbone	O
of	O
our	O
framework	O
is	O
a	O
Transformer	B-MethodName
-	O
based	O
auto	O
-	O
regressive	O
language	O
model	O
.	O
The	O
input	O
can	O
be	O
the	O
whole	O
token	O
sequences	O
of	O
samples	O
from	O
SongCi	O
or	O
Sonnet	O
.	O
We	O
tailor	O
-	O
design	O
several	O
sets	O
of	O
indicating	O
symbols	O
to	O
enhance	O
the	O
performance	O
in	O
terms	O
of	O
accuracy	B-MetricName
on	O
format	O
,	O
rhyme	O
,	O
and	O
sentence	O
integrity	O
.	O
Specifically	O
,	O
symbols	O
C	O
=	O
{	O
c	O
i	O
}	O
are	O
introduced	O
for	O
format	O
and	O
rhyming	O
modeling	O
;	O
Intra	O
-	O
position	O
symbols	O
P	O
=	O
{	O
p	O
i	O
}	O
are	O
designed	O
to	O
represent	O
the	O
local	O
positions	O
of	O
the	O
tokens	O
within	O
each	O
sentence	O
aiming	O
to	O
improve	O
the	O
rhyming	O
performance	O
and	O
the	O
sentence	O
integrity	O
.	O
Segment	O
symbols	O
S	O
=	O
{	O
s	O
i	O
}	O
are	O
employed	O
to	O
identify	O
the	O
sentence	O
border	O
to	O
further	O
improve	O
the	O
sentence	O
quality	O
.	O
Attention	O
mechanism	O
is	O
improved	O
to	O
impel	O
the	O
model	O
to	O
capture	O
the	O
future	O
format	O
information	O
such	O
as	O
the	O
sentence	O
ending	O
markers	O
.	O
Similar	O
to	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
GPT	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2018	O
(	O
Radford	O
et	O
al	O
,	O
,	O
2019	O
,	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
paradigm	O
is	O
utilized	O
to	O
boost	O
the	O
performance	O
of	O
the	O
original	O
models	O
.	O

We	O
use	O
two	O
sentences	O
(	O
as	O
shown	O
in	O
Figure	O
1	O
)	O
"	O
love	O
is	O
not	O
love	O
,	O
...	O
,	O
bends	O
with	O
the	O
remover	O
to	O
remove	O
"	O
extracted	O
from	O
the	O
Shakespeare	O
's	O
Sonnets	O
(	O
Shakespeare	O
,	O
2000	O
)	O
as	O
examples	O
to	O
describe	O
the	O
details	O
of	O
our	O
framework	O
SongNet	B-MethodName
.	O
Since	O
our	O
basic	O
model	O
is	O
a	O
Transformer	B-MethodName
-	O
based	O
auto	O
-	O
regressive	O
language	O
model	O
,	O
during	O
training	O
,	O
the	O
input	O
is	O
"	O
bos	O
love	O
is	O
not	O
love	O
,	O
/s	O
...	O
,	O
bends	O
with	O
the	O
remover	O
to	O
remove	O
.	O
/s	O
"	O
,	O
and	O
the	O
corresponding	O
output	O
is	O
a	O
left	O
-	O
shifting	O
version	O
of	O
the	O
input	O
(	O
tokenized	O
,	O
and	O
we	O
ignore	O
"	O
...	O
"	O
for	O
convenience	O
and	O
clarity	O
)	O
:	O
love	O
is	O
not	O
love	O
,	O
/s	O
bends	O
with	O
the	O
remover	O
to	O
remove	O
.	O
/s	O
eos	O
where	O
/s	O
denotes	O
the	O
clause	O
or	O
sentence	O
separator	O
,	O
and	O
eos	O
is	O
the	O
ending	O
marker	O
of	O
the	O
whole	O
sequence	O
.	O
The	O
target	O
of	O
our	O
framework	O
is	O
to	O
conduct	O
the	O
formats	O
controlled	O
text	B-TaskName
generation	I-TaskName
.	O
Therefore	O
,	O
the	O
indicating	O
symbols	O
for	O
format	O
and	O
rhyme	O
as	O
well	O
as	O
the	O
sentence	O
integrity	O
are	O
designed	O
based	O
on	O
the	O
target	O
output	O
sequence	O
.	O
Format	O
and	O
Rhyme	O
Symbols	O
:	O
C	O
=	O
{	O
c	O
0	B-DatasetName
,	O
c	O
0	B-DatasetName
,	O
c	O
0	B-DatasetName
,	O
c	O
2	O
,	O
c	O
1	O
,	O
/s	O
c	O
0	B-DatasetName
,	O
c	O
0	B-DatasetName
,	O
c	O
0	B-DatasetName
,	O
c	O
0	B-DatasetName
,	O
c	O
0	B-DatasetName
,	O
c	O
2	O
,	O
c	O
1	O
,	O
/s	O
,	O
eos	O
}	O
(	O
3	O
)	O
where	O
we	O
use	O
{	O
c	O
0	B-DatasetName
}	O
to	O
represent	O
the	O
general	O
tokens	O
;	O
{	O
c	O
1	O
}	O
depict	O
the	O
punctuation	O
characters	O
;	O
{	O
c	O
2	O
}	O
represent	O
the	O
rhyming	O
tokens	O
"	O
love	O
"	O
and	O
"	O
remove	O
"	O
.	O
/s	O
and	O
eos	O
are	O
kept	O
.	O
Intra	O
-	O
Position	O
Symbols	O
:	O
P	O
=	O
{	O
p	O
4	O
,	O
p	O
3	O
,	O
p	O
2	O
,	O
p	O
1	O
,	O
p	O
0	B-DatasetName
,	O
/s	O
p	O
6	O
,	O
p	O
5	O
,	O
p	O
4	O
,	O
p	O
3	O
,	O
p	O
2	O
,	O
p	O
1	O
,	O
p	O
0	B-DatasetName
,	O
/s	O
,	O
eos	O
}	O
(	O
4	O
)	O
{	O
p	O
i	O
}	O
denote	O
the	O
local	O
positions	O
of	O
tokens	O
within	O
the	O
same	O
clause	O
or	O
sentence	O
.	O
Note	O
that	O
we	O
align	O
the	O
position	O
symbol	O
indices	O
in	O
a	O
descending	O
order	O
.	O
The	O
aim	O
is	O
to	O
improve	O
the	O
sentence	O
integrity	O
by	O
impelling	O
the	O
symbols	O
capture	O
the	O
sentence	O
dynamic	O
information	O
,	O
precisely	O
,	O
the	O
sense	O
to	O
end	O
a	O
sequence	O
.	O
For	O
example	O
,	O
{	O
p	O
0	B-DatasetName
}	O
usually	O
denote	O
punctuation	O
characters	O
,	O
thus	O
{	O
p	O
1	O
}	O
should	O
be	O
the	O
ending	O
words	O
of	O
sentences	O
.	O
Segment	O
Symbols	O
:	O
S	O
=	O
{	O
s	O
0	B-DatasetName
,	O
s	O
0	B-DatasetName
,	O
s	O
0	B-DatasetName
,	O
s	O
0	B-DatasetName
,	O
s	O
0	B-DatasetName
,	O
/s	O
s	O
1	O
,	O
s	O
1	O
,	O
s	O
1	O
,	O
s	O
1	O
,	O
s	O
1	O
,	O
s	O
1	O
,	O
s	O
1	O
,	O
/s	O
,	O
eos	O
}	O
(	O
5	O
)	O
where	O
s	O
i	O
is	O
the	O
symbol	O
index	O
for	O
sentence	O
i.	O
The	O
purpose	O
is	O
to	O
enhance	O
the	O
interactions	O
between	O
different	O
sentences	O
in	O
different	O
positions	O
by	O
defining	O
the	O
sentence	O
index	O
features	O
.	O
During	O
training	O
,	O
all	O
the	O
symbols	O
as	O
well	O
as	O
the	O
input	O
tokens	O
are	O
fed	O
into	O
the	O
transformer	O
-	O
based	O
language	O
model	O
.	O
Contrast	O
to	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
GPT2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
modify	O
the	O
traditional	O
attention	O
strategies	O
slightly	O
to	O
fit	O
our	O
problem	O
.	O
Specifically	O
,	O
for	O
the	O
input	O
,	O
we	O
first	O
obtain	O
the	O
representations	O
by	O
summing	O
all	O
the	O
embeddings	O
of	O
the	O
input	O
tokens	O
and	O
symbols	O
,	O
as	O
shown	O
in	O
the	O
red	O
solid	O
box	O
of	O
Figure	O
2	O
:	O
H	O
0	B-DatasetName
t	O
=	O
E	O
wt	O
+	O
E	O
ct	O
+	O
E	O
pt	O
+	O
E	O
st	O
+	O
E	O
gt	O
(	O
6	O
)	O
where	O
0	B-DatasetName
is	O
the	O
layer	O
index	O
and	O
t	O
is	O
the	O
state	O
index	O
.	O
E	O
*	O
is	O
the	O
embedding	O
vector	O
for	O
input	O
*	O
.	O
w	O
t	O
is	O
the	O
real	O
token	O
at	O
position	O
t.	O
c	O
,	O
p	O
,	O
and	O
s	O
are	O
three	O
pre	O
-	O
defined	O
symbols	O
.	O
g	O
is	O
the	O
global	O
position	O
index	O
same	O
as	O
position	O
symbols	O
used	O
in	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Moreover	O
,	O
the	O
state	O
at	O
time	O
t	O
need	O
to	O
know	O
some	O
future	O
information	O
to	O
grasp	O
the	O
global	O
sequence	O
dynamic	O
information	O
.	O
For	O
example	O
,	O
the	O
model	O
may	O
want	O
to	O
know	O
if	O
it	O
should	O
close	O
the	O
decoding	O
progress	O
by	O
generating	O
the	O
last	O
word	O
and	O
a	O
punctuation	O
character	O
to	O
end	O
the	O
sentence	O
.	O
To	O
represent	O
the	O
global	O
dynamic	O
information	O
,	O
we	O
introduce	O
another	O
variable	O
F	O
0	B-DatasetName
by	O
only	O
summing	O
the	O
pre	O
-	O
defined	O
symbols	O
as	O
shown	O
in	O
the	O
blue	O
dash	O
box	O
of	O
Figure	O
2	O
:	O
F	O
0	B-DatasetName
t	O
=	O
E	O
ct	O
+	O
E	O
pt	O
+	O
E	O
st	O
(	O
7	O
)	O
After	O
processing	O
the	O
input	O
,	O
two	O
blocks	O
of	O
attention	O
mechanisms	O
are	O
introduced	O
to	O
conduct	O
the	O
feature	O
learning	O
procedure	O
.	O
The	O
first	O
block	O
is	O
a	O
masking	O
multi	O
-	O
head	O
self	O
-	O
attention	O
component	O
,	O
and	O
the	O
second	O
block	O
is	O
named	O
global	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
.	O
Masking	O
Multi	O
-	O
Head	O
Self	O
-	O
Attention	O
:	O
C	O
1	O
t	O
=	O
LN	O
FFN	O
(	O
C	O
1	O
t	O
)	O
+	O
C	O
1	O
t	O
C	O
1	O
t	O
=	O
LN	O
SLF	O
-	O
ATT	O
(	O
Q	O
0	B-DatasetName
t	O
,	O
K	O
0	B-DatasetName
≤t	O
,	O
V	O
0	B-DatasetName
≤t	O
)	O
+	O
H	O
0	B-DatasetName
t	O
Q	O
0	B-DatasetName
=	O
H	O
0	B-DatasetName
W	O
Q	O
K	O
0	B-DatasetName
,	O
V	O
0	B-DatasetName
=	O
H	O
0	B-DatasetName
W	O
K	O
,	O
H	O
0	B-DatasetName
W	O
V	O
(	O
8	O
)	O
where	O
SLF	O
-	O
ATT	O
(	O
)	O
,	O
LN	O
(	O
)	O
,	O
and	O
FFN	O
(	O
)	O
represent	O
self	O
-	O
attention	O
mechanism	O
,	O
layer	B-MethodName
normalization	I-MethodName
,	O
and	O
feed	O
-	O
forward	O
network	O
respectively	O
.	O
Note	O
that	O
we	O
only	O
use	O
the	O
states	O
whose	O
indices	O
≤	O
t	O
as	O
the	O
attention	O
context	O
.	O
After	O
obtaining	O
C	O
1	O
t	O
from	O
Equation	O
(	O
8	O
)	O
,	O
we	O
feed	O
it	O
into	O
the	O
second	O
attention	O
block	O
to	O
capture	O
the	O
global	O
dynamic	O
information	O
from	O
F	O
0	B-DatasetName
.	O
Global	O
Multi	B-MethodName
-	I-MethodName
Head	I-MethodName
Attention	I-MethodName
:	O
H	O
1	O
t	O
=	O
LN	O
FFN	O
(	O
H	O
1	O
t	O
)	O
+	O
H	O
1	O
t	O
H	O
1	O
t	O
=	O
LN	O
GLOBAL	O
-	O
ATT	O
(	O
Q	O
1	O
t	O
,	O
K	O
1	O
,	O
V	O
1	O
)	O
+	O
C	O
1	O
t	O
Q	O
1	O
=	O
C	O
1	O
W	O
Q	O
K	O
1	O
,	O
V	O
1	O
=	O
F	O
0	B-DatasetName
W	O
K	O
,	O
F	O
0	B-DatasetName
W	O
V	O
(	O
9	O
)	O
We	O
can	O
observe	O
that	O
all	O
the	O
context	O
information	O
from	O
F	O
0	B-DatasetName
are	O
considered	O
.	O
This	O
is	O
the	O
reason	O
why	O
we	O
name	O
it	O
as	O
"	O
global	O
attention	O
"	O
and	O
why	O
the	O
input	O
real	O
token	O
information	O
E	O
wt	O
is	O
NOT	O
considered	O
.	O
Then	O
the	O
calculation	O
of	O
the	O
unified	O
first	O
model	O
layer	O
is	O
finished	O
.	O
We	O
can	O
iteratively	O
apply	O
these	O
two	O
attention	O
blocks	O
on	O
the	O
whole	O
L	O
model	O
layers	O
until	O
obtain	O
the	O
final	O
representations	O
H	O
L	O
.	O
Note	O
that	O
H	O
is	O
renewed	O
layerly	O
,	O
however	O
the	O
global	O
variable	O
F	O
0	B-DatasetName
is	O
fixed	O
.	O
Finally	O
,	O
the	O
training	O
objective	O
is	O
to	O
minimize	O
the	O
negative	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
over	O
the	O
whole	O
sequence	O
:	O
L	O
nll	B-MetricName
=	O
−	O
n	O
t=1	O
log	O
P	O
(	O
y	O
t	O
|	O
y	O
<	O
t	O
)	O
(	O
10	O
)	O

Although	O
our	O
framework	O
can	O
be	O
trained	O
purely	O
on	O
the	O
training	O
dataset	O
of	O
the	O
target	O
corpus	O
,	O
usually	O
the	O
scale	O
of	O
the	O
corpus	O
is	O
limited	O
.	O
For	O
example	O
,	O
there	O
are	O
only	O
about	O
150	O
samples	O
in	O
the	O
corpus	O
of	O
Shakespeare	O
's	O
Sonnets	O
(	O
Shakespeare	O
,	O
2000	O
)	O
.	O
Therefore	O
,	O
we	O
also	O
design	O
a	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
framework	O
to	O
further	O
improve	O
the	O
generation	O
quality	O
.	O
Recall	B-MetricName
that	O
in	O
the	O
task	O
definition	O
in	O
Section	O
2	O
,	O
we	O
claim	O
that	O
our	O
model	O
owns	O
the	O
ability	O
of	O
refining	O
and	O
polishing	O
.	O
To	O
achieve	O
this	O
goal	O
,	O
we	O
adjust	O
the	O
masking	O
strategy	O
used	O
in	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
to	O
our	O
framework	O
according	O
to	O
our	O
definitions	O
.	O
Specifically	O
,	O
we	O
randomly	O
(	O
say	O
20	O
%	O
)	O
select	O
partial	O
of	O
the	O
original	O
content	O
and	O
keep	O
them	O
not	O
changed	O
when	O
building	O
the	O
format	O
symbols	O
C.	O
For	O
example	O
,	O
we	O
will	O
get	O
a	O
new	O
symbol	O
set	O
C	O
for	O
the	O
example	O
sentences	O
:	O
C	O
=	O
{	O
c0	O
,	O
c0	O
,	O
c0	O
,	O
love	O
,	O
c1	O
,	O
/s	O
bends	O
,	O
c0	O
,	O
c0	O
,	O
c0	O
,	O
c0	O
,	O
remove	O
,	O
c1	O
,	O
/s	O
,	O
eos	O
}	O
where	O
"	O
love	O
"	O
,	O
"	O
bends	O
"	O
and	O
"	O
remove	O
"	O
are	O
kept	O
in	O
the	O
format	O
C	O
.	O
After	O
the	O
pre	O
-	O
training	O
stage	O
,	O
we	O
can	O
conduct	O
the	O
fine	O
-	O
tuning	O
procedure	O
directly	O
on	O
the	O
target	O
corpus	O
without	O
adjusting	O
any	O
model	O
structure	O
.	O

The	O
parameter	O
size	O
of	O
our	O
model	O
are	O
fixed	O
in	O
both	O
the	O
pre	O
-	O
training	O
stage	O
and	O
the	O
fine	O
-	O
tuning	O
stage	O
.	O
The	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
L	O
=	O
12	O
,	O
and	O
hidden	O
size	O
is	O
768	O
.	O
We	O
employ	O
12	O
heads	O
in	O
both	O
the	O
masking	O
multihead	O
self	O
-	O
attention	O
block	O
and	O
the	O
global	O
attention	O
block	O
.	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
optimization	O
method	O
with	O
Noam	O
learning	O
-	O
rate	O
decay	O
strategy	O
and	O
10	O
,	O
000	O
warmup	O
steps	O
is	O
employed	O
to	O
conduct	O
the	O
pre	O
-	O
training	O
.	O

Besides	O
PPL	O
and	O
Distinct	O
(	O
Li	O
et	O
al	O
,	O
2016	O
)	O
,	O
we	O
also	O
tailor	O
-	O
design	O
several	O
metrics	O
for	O
our	O
task	O
to	O
conduct	O
the	O
evaluation	O
for	O
format	O
,	O
rhyme	O
,	O
and	O
sentence	O
integrity	O
.	O
Format	O
Assume	O
that	O
there	O
are	O
m	O
sentences	O
defined	O
in	O
the	O
format	O
C	O
=	O
{	O
C	O
s	O
1	O
,	O
C	O
s	O
2	O
,	O
...	O
,	O
C	O
s	O
m	O
}	O
,	O
and	O
the	O
generated	O
results	O
Y	O
contains	O
n	O
sentences	O
Y	O
=	O
{	O
Y	O
s	O
1	O
,	O
Y	O
s	O
2	O
,	O
...	O
,	O
Y	O
s	O
n	O
}	O
.	O
Without	O
loss	B-MetricName
of	O
generality	O
,	O
we	O
align	O
C	O
and	O
Y	O
from	O
the	O
beginning	O
,	O
and	O
calculate	O
the	O
format	O
quality	O
according	O
to	O
the	O
following	O
rules	O
:	O
(	O
1	O
)	O
the	O
length	O
difference	O
|	O
|	O
C	O
s	O
i	O
|	O
−	O
|	O
Y	O
s	O
i	O
|	O
|	O
≤	O
δ	B-HyperparameterName
;	O
(	O
2	O
)	O
the	O
punctuation	O
characters	O
must	O
be	O
same	O
.	O
For	O
SongCi	O
,	O
we	O
let	O
δ	B-HyperparameterName
=	O
0	B-DatasetName
and	O
rule	O
(	O
2	O
)	O
must	O
be	O
conforming	O
.	O
For	O
Sonnet	O
,	O
we	O
relax	O
the	O
condition	O
where	O
we	O
let	O
δ	B-HyperparameterName
=	O
1	O
and	O
ignore	O
rule	O
(	O
2	O
)	O
.	O
Assume	O
that	O
the	O
number	O
of	O
format	O
-	O
correct	O
sentences	O
is	O
n	O
,	O
then	O
we	O
can	O
obtain	O
Precision	B-MetricName
p	O
=	O
n	O
/n	O
,	O
Recall	B-MetricName
r	O
=	O
n	O
/m	O
,	O
and	O
F1	B-MetricName
-	O
measure	O
.	O
We	O
report	O
both	O
the	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
and	O
Micro	B-MetricName
-	I-MetricName
F1	I-MetricName
in	O
the	O
results	O
tables	O
.	O
Model	O
PPL↓	O
Diversity	O
(	O
Distinct	O
)	O
↑	O
VAL	O
TEST	O
MA	O
-	O
D	O
-	O
1	O
MI	O
-	O
D	O
-	O
1	O
MA	O
-	O
D	O
-	O
2	O
MI	O
-	O
D	O
-	O
2	O
GPT2	O
w/	O
Rhyme	O
For	O
SongCi	O
,	O
usually	O
,	O
there	O
is	O
only	O
one	O
group	O
of	O
rhyming	O
words	O
in	O
one	O
sample	O
.	O
As	O
the	O
example	O
shown	O
in	O
Table	O
1	O
,	O
the	O
pronunciation	O
of	O
the	O
red	O
rhyming	O
words	O
are	O
"	O
zhu	O
"	O
,	O
"	O
yü	O
"	O
,	O
"	O
du	O
"	O
,	O
and	O
"	O
gu	O
"	O
respectively	O
,	O
and	O
the	O
rhyming	O
phoneme	O
is	O
"	O
u	O
"	O
.	O
For	O
the	O
generated	O
samples	O
,	O
we	O
first	O
use	O
the	O
tool	O
pinyin	O
4	O
to	O
get	O
the	O
pronunciations	O
(	O
PinYin	O
)	O
of	O
the	O
words	O
in	O
the	O
rhyming	O
positions	O
,	O
and	O
then	O
conduct	O
the	O
evaluation	O
.	O
For	O
Shakespeare	O
's	O
Sonnets	O
corpus	O
,	O
the	O
rhyming	O
rule	O
is	O
clear	O
"	O
ABAB	O
CDCD	O
EFEF	O
GG	O
"	O
and	O
there	O
are	O
7	O
groups	O
of	O
rhyming	O
tokens	O
.	O
For	O
the	O
generated	O
samples	O
,	O
we	O
employ	O
the	O
CMU	O
Pronouncing	O
Dictionary	O
5	O
(	O
Speech@CMU	O
,	O
1998	O
)	O
to	O
obtain	O
the	O
phonemes	O
of	O
the	O
words	O
in	O
the	O
rhyming	O
positions	O
.	O
For	O
example	O
,	O
the	O
phonemes	O
for	O
word	O
"	O
asleep	O
"	O
and	O
"	O
steep	O
"	O
are	O
[	O
'	O
AH0	O
'	O
,	O
'S	O
'	O
,	O
'	O
L	O
'	O
,	O
'	O
IY1	O
'	O
,	O
'	O
P	O
'	O
]	O
and	O
[	O
'S	O
'	O
,	O
'	O
T	O
'	O
,	O
'	O
IY1	O
'	O
,	O
'	O
P	O
'	O
]	O
respectively	O
.	O
And	O
then	O
we	O
can	O
conduct	O
the	O
evaluation	O
by	O
counting	O
the	O
overlapping	O
units	O
from	O
both	O
the	O
original	O
words	O
and	O
the	O
extracted	O
phonemes	O
group	O
by	O
group	O
.	O
We	O
report	O
the	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
and	O
Micro	B-MetricName
-	I-MetricName
F1	I-MetricName
numbers	O
in	O
the	O
results	O
tables	O
as	O
well	O
.	O
Integrity	O
Since	O
the	O
format	O
in	O
our	O
task	O
is	O
strict	O
and	O
rigid	O
,	O
thus	O
the	O
number	O
of	O
words	O
to	O
be	O
predicted	O
is	O
also	O
pre	O
-	O
defined	O
.	O
Our	O
model	O
must	O
organize	O
the	O
language	O
using	O
the	O
limited	O
positions	O
,	O
thus	O
sentence	O
integrity	O
may	O
become	O
a	O
serious	O
issue	O
.	O
For	O
example	O
,	O
the	O
integrity	O
of	O
"	O
love	O
is	O
not	O
love	O
.	O
/s	O
"	O
is	O
much	O
better	O
than"love	O
is	O
not	O
the	O
.	O
/s	O
"	O
.	O
To	O
conduct	O
the	O
evaluation	O
of	O
sentence	O
integrity	O
,	O
we	O
design	O
a	O
straightforward	O
method	O
by	O
calculating	O
the	O
prediction	O
probability	O
of	O
the	O
punctuation	O
characters	O
before	O
/s	O
given	O
the	O
prefix	O
tokens	O
:	O
Model	O
PPL↓	O
Diversity	O
(	O
Distinct	O
)	O
↑	O
VAL	O
TEST	O
MA	O
-	O
D	O
-	O
1	O
MI	O
-	O
D	O
-	O
1	O
MA	O
-	O
D	O
-	O
2	O
MI	O
-	O
D	O
-	O
2	O
Integrity	O
=	O
2	O
−	O
1	O
|	O
Y	O
|	O
|	O
Y	O
|	O
i=1	O
log	O
(	O
P	O
(	O
y	O
i	O
punc	O
|	O
y	O
i	O
0	B-DatasetName
,	O
y	O
i	O
1	O
,	O
...	O
,	O
y	O
i	O
<	O
punc	O
)	O
)	O
(	O
11	O
)	O
where	O
Y	O
is	O
the	O
generated	O
sequence	O
of	O
sentences	O
.	O
Smaller	O
integrity	O
metric	O
value	O
indicates	O
higher	O
sentence	O
quality	O
.	O
To	O
achieve	O
this	O
goal	O
,	O
we	O
conduct	O
pre	O
-	O
trainings	O
for	O
two	O
GPT2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
models	O
on	O
the	O
large	O
scale	O
Chinese	O
corpus	O
and	O
English	O
corpus	O
respectively	O
.	O
Then	O
we	O
utilize	O
the	O
GPT2	O
models	O
to	O
conduct	O
the	O
evaluation	O
for	O
sentence	O
integrity	O
.	O
Human	O
Evaluations	O
For	O
SongCi	O
,	O
we	O
sampled	O
50	O
samples	O
for	O
25	O
CiPais	O
.	O
For	O
Sonnet	O
,	O
the	O
whole	O
27	O
samples	O
in	O
the	O
test	O
set	O
are	O
selected	O
for	O
human	O
evaluation	O
.	O
We	O
recruit	O
three	O
helpers	O
to	O
score	O
the	O
Relevance	O
,	O
Fluency	O
,	O
and	O
Style	O
.	O
The	O
rating	O
criteria	O
are	O
as	O
follows	O
:	O
Relevance	O
:	O
+2	O
:	O
all	O
the	O
sentences	O
are	O
relevant	O
to	O
the	O
same	O
topic	O
;	O
+1	O
:	O
partial	O
sentences	O
are	O
relevant	O
;	O
0	B-DatasetName
:	O
not	O
relevant	O
at	O
all	O
.	O
Fluency	O
:	O
+2	O
:	O
fluent	O
;	O
+1	O
:	O
readable	O
but	O
with	O
some	O
grammar	O
mistakes	O
;	O
0	B-DatasetName
:	O
unreadable	O
.	O
Style	O
:	O
+2	O
:	O
match	O
with	O
SongCi	O
or	O
Sonnet	O
genres	O
;	O
+1	O
:	O
partially	O
match	O
;	O
0	B-DatasetName
:	O
mismatch	O
.	O

Please	O
note	O
that	O
we	O
mainly	O
employ	O
top	O
-	O
k	O
sampling	O
method	O
(	O
Fan	O
et	O
al	O
,	O
2018	O
;	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
to	O
conduct	O
the	O
generation	O
,	O
and	O
we	O
let	O
k	B-HyperparameterName
=	I-HyperparameterName
32	O
here	O
.	O
The	O
parameter	O
tuning	O
of	O
k	O
is	O
described	O
in	O
Section	O
5.3	O
.	O
on	O
SongCi	O
.	O
The	O
main	O
reason	O
is	O
that	O
Sonnet	O
only	O
contains	O
100	O
samples	O
in	O
the	O
training	O
set	O
as	O
shown	O
in	O
Table	O
3	O
.	O
Therefore	O
,	O
the	O
model	O
can	O
not	O
capture	O
sufficient	O
useful	O
features	O
especially	O
for	O
the	O
rhyming	O
issue	O
.	O

Since	O
we	O
employ	O
top	O
-	O
k	O
sampling	O
as	O
our	O
main	O
decoding	O
strategy	O
,	O
thus	O
we	O
design	O
several	O
experiments	O
to	O
conduct	O
the	O
parameter	O
tuning	O
on	O
k.	O
We	O
let	O
k	O
to	O
be	O
1	O
,	O
5	O
,	O
10	O
,	O
20	O
,	O
50	O
,	O
500	O
respectively	O
.	O
We	O
also	O
provide	O
the	O
beam	O
-	O
search	O
(	O
beam=5	O
)	O
results	O
for	O
comparing	O
and	O
reference	O
.	O
The	O
parameter	O
tuning	O
results	O
are	O
depicted	O
in	O
Figure	O
3	O
.	O
From	O
the	O
results	O
we	O
can	O
observe	O
that	O
large	O
k	O
can	O
increase	O
the	O
diversity	O
of	O
the	O
results	O
significantly	O
.	O
But	O
the	O
Rhyme	O
accuracy	B-MetricName
and	O
the	O
sentence	O
integrity	O
will	O
drop	O
simultaneously	O
.	O
Therefore	O
,	O
in	O
the	O
experiments	O
we	O
let	O
k	B-HyperparameterName
=	I-HyperparameterName
32	O
to	O
obtain	O
a	O
trade	O
-	O
off	O
between	O
the	O
diversity	O
and	O
the	O
general	O
quality	O
.	O

In	O
this	O
phase	O
,	O
instances	O
for	O
evaluation	O
are	O
released	O
.	O
3	O
Each	O
participating	O
system	O
generated	O
predictions	O
for	O
the	O
test	O
instances	O
,	O
for	O
up	O
to	O
N	O
models	O
.	O
4	O
Predictions	O
are	O
submitted	O
to	O
CodaLab	O
5	O
and	O
evaluated	O
automatically	O
against	O
the	O
goldstandard	O
labels	O
.	O
Submissions	O
were	O
anonymized	O
.	O
The	O
only	O
statistics	O
displayed	O
were	O
the	O
highest	O
score	O
of	O
all	O
systems	O
per	O
day	O
.	O
The	O
total	O
allowable	O
number	O
of	O
system	O
submissions	O
per	O
day	O
was	O
limited	O
to	O
5	O
per	O
team	O
per	O
track	O
.	O
The	O
metric	O
used	O
for	O
evaluation	O
is	O
the	O
F1	B-MetricName
score	I-MetricName
(	O
least	O
frequent	O
class	O
/	O
label	O
,	O
which	O
is	O
"	O
metaphor	O
"	O
)	O
with	O
Precision	B-MetricName
and	O
Recall	B-MetricName
also	O
available	O
via	O
the	O
detailed	O
results	O
link	O
in	O
CodaLab	O
.	O
The	O
shared	O
task	O
started	O
on	O
January	O
12	O
,	O
2020	O
when	O
the	O
training	O
data	O
was	O
made	O
available	O
to	O
registered	O
participants	O
.	O
On	O
February	O
14	O
,	O
2020	O
,	O
the	O
testing	O
data	O
was	O
released	O
.	O
Submissions	O
were	O
accepted	O
until	O
April	O
17	O
,	O
2020	O
.	O
Table	O
3	O
shows	O
the	O
submission	O
statistics	O
for	O
systems	O
with	O
a	O
system	O
paper	O
.	O
Generally	O
,	O
there	O
were	O
more	O
participants	O
in	O
the	O
VUA	O
tracks	O
than	O
in	O
TOEFL	O
tracks	O
,	O
and	O
in	O
All	O
POS	O
tracks	O
than	O
in	O
Verbs	O
tracks	O
.	O
In	O
total	O
,	O
13	O
system	O
papers	O
were	O
submitted	O
describing	O
methods	O
for	O
generating	O
metaphor	O
/	O
non	O
-	O
metaphor	O
predictions	O
.	O

The	O
clearest	O
trend	O
in	O
the	O
2020	O
submissions	O
is	O
the	O
use	O
of	O
deep	O
learning	O
architectures	O
based	O
on	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
-	O
more	O
than	O
half	O
of	O
the	O
participating	O
systems	O
used	O
BERT	B-MethodName
or	O
its	O
variant	O
.	O
The	O
usefulness	O
of	O
BERT	B-MethodName
for	O
metaphor	O
detection	O
has	O
been	O
shown	O
by	O
Mao	O
et	O
al	O
(	O
2019	O
)	O
,	O
where	O
a	O
BERT	B-MethodName
-	O
based	O
system	O
posted	O
F1	B-MetricName
=	O
0.717	O
on	O
VUA	O
AllPOS	O
,	O
hence	O
our	O
use	O
of	O
a	O
BERT	B-MethodName
-	O
based	O
system	O
as	O
Baseline	O
3	O
.	O
Beyond	O
explorations	O
of	O
neural	O
architectures	O
,	O
we	O
also	O
observe	O
usage	O
of	O
new	O
lexical	O
,	O
grammatical	O
,	O
and	O
morphological	O
information	O
,	O
such	O
as	O
finegrained	O
POS	O
,	O
spell	O
-	O
corrected	O
variants	O
of	O
words	O
(	O
for	O
TOEFL	O
data	O
)	O
,	O
sub	O
-	O
word	O
level	O
information	O
(	O
e.g.	O
,	O
character	O
embeddings	O
)	O
,	O
idioms	O
,	O
sensorimotor	O
and	O
embodiment	O
-	O
related	O
information	O
.	O

Since	O
the	O
same	O
VUA	O
dataset	O
was	O
used	O
in	O
2020	O
shared	O
task	O
as	O
in	O
the	O
2018	O
shared	O
task	O
,	O
we	O
can	O
directly	O
compare	O
the	O
performance	O
of	O
the	O
best	O
systems	O
to	O
observe	O
the	O
extent	O
of	O
the	O
improvement	O
.	O
The	O
best	O
system	O
in	O
2018	O
performed	O
at	O
F1	B-MetricName
=	O
0.651	O
;	O
the	O
best	O
performance	O
in	O
2020	O
is	O
more	O
than	O
10	O
points	O
better	O
-	O
F1	B-MetricName
=	O
0.769	O
.	O
Indeed	O
,	O
the	O
2018	O
best	O
performing	O
system	O
would	O
have	O
earned	O
the	O
rank	O
of	O
11	O
in	O
the	O
2020	O
All	O
POS	O
track	O
,	O
suggesting	O
that	O
the	O
field	O
has	O
generally	O
moved	O
to	O
more	O
effective	O
models	O
than	O
those	O
proposed	O
for	O
the	O
2018	O
competitions	O
.	O
The	O
best	O
results	O
posted	O
for	O
the	O
2020	O
shared	O
task	O
are	O
the	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
both	O
VUA	O
7	O
and	O
TOEFL	O
corpora	O
.	O
7	O
While	O
a	O
number	O
of	O
recent	O
systems	O
were	O
evaluted	O
on	O
VUA	O
data	O
(	O
Le	O
et	O
al	O
,	O
2020	O
;	O
Dankers	O
et	O
al	O
,	O
2019	O
;	O
Mao	O
et	O
al	O
,	O
2019	O
;	O
Gao	O
et	O
al	O
,	O
2018	O
)	O
,	O
their	O
results	O
are	O
not	O
directly	O
comparable	O
to	O
the	O
shared	O
task	O
,	O
since	O
they	O
evaluated	O
on	O
all	O
parts	O
of	O
speech	O
,	O
including	O
function	O
words	O
.	O
See	O
Dankers	O
et	O
al	O
(	O
2020	O
)	O
for	O
a	O
discussion	O
.	O

Table	O
6	O
shows	O
performance	O
by	O
genre	O
for	O
the	O
VUA	O
data	O
All	O
POS	O
track	O
.	O
The	O
patterns	O
are	O
highly	O
consistent	O
across	O
systems	O
,	O
and	O
replicate	O
those	O
observed	O
for	O
the	O
2018	O
shared	O
task	O
-	O
Academic	O
and	O
News	O
genres	O
are	O
substantially	O
easier	O
to	O
handle	O
than	O
Fiction	O
and	O
Conversation	O
.	O
The	O
gap	O
between	O
the	O
best	O
and	O
worst	O
performance	O
across	O
genres	O
for	O
the	O
same	O
system	O
remains	O
wide	O
-	O
between	O
11.4	O
F1	B-MetricName
points	O
and	O
24.3	O
F1	B-MetricName
points	O
.	O
Somewhat	O
encouragingly	O
,	O
the	O
gap	O
is	O
narrower	O
for	O
the	O
better	O
performing	O
systems	O
-	O
the	O
top	O
6	O
systems	O
show	O
the	O
smallest	O
gaps	O
between	O
best	O
and	O
worst	O
genres	O
(	O
11.4	O
-	O
14.0	O
)	O
.	O

Table	O
7	O
shows	O
performance	O
and	O
ranks	O
of	O
the	O
best	O
systems	O
for	O
teams	O
that	O
participated	O
in	O
both	O
VUA	O
and	O
TOEFL	O
AllPOS	O
tracks	O
,	O
along	O
with	O
baselines	O
.	O
Overall	O
,	O
the	O
relative	O
performance	O
rankings	O
are	O
consistent	O
-	O
F1	B-MetricName
scores	O
are	O
correlated	O
at	O
r	O
=	O
.92	O
and	O
team	O
ranks	O
are	O
correlated	O
at	O
r	O
=	O
0.95	O
across	O
the	O
two	O
datasets	O
.	O
All	O
teams	O
posted	O
better	O
performance	O
on	O
the	O
VUA	O
data	O
than	O
on	O
the	O
TOEFL	O
data	O
;	O
the	O
difference	O
(	O
see	O
column	O
4	O
in	O
Table	O
6	O
:	O
VUA	O
Dataset	O
:	O
Performance	O
(	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
)	O
of	O
the	O
best	O
systems	O
submitted	O
to	O
All	O
-	O
POS	O
track	O
by	O
genre	O
subsets	O
of	O
the	O
test	O
data	O
.	O
In	O
parentheses	O
,	O
we	O
show	O
the	O
rank	O
of	O
the	O
given	O
genre	O
within	O
all	O
genres	O
for	O
the	O
system	O
.	O
The	O
last	O
column	O
shows	O
the	O
overall	O
drop	O
in	O
performance	O
from	O
best	O
genre	O
(	O
ranked	O
1	O
)	O
to	O
worst	O
(	O
ranked	O
4	O
)	O
.	O
The	O
top	O
three	O
performances	O
for	O
a	O
given	O
genre	O
are	O
boldfaced	O
.	O
F1	B-MetricName
point	O
(	O
zhengchang	O
)	O
to	O
5	O
F1	B-MetricName
points	O
(	O
DeepMet	O
,	O
umd	O
bilstm	B-MethodName
,	O
Zenith	O
)	O
.	O
The	O
BERT	B-MethodName
baseline	O
posted	O
a	O
relatively	O
large	O
difference	O
of	O
9	O
F1	B-MetricName
points	O
;	O
this	O
could	O
be	O
because	O
BNC	O
data	O
is	O
more	O
similar	O
to	O
the	O
data	O
on	O
which	O
BERT	B-MethodName
has	O
been	O
pre	O
-	O
trained	O
than	O
TOEFL	O
data	O
.	O
We	O
note	O
,	O
however	O
,	O
that	O
participating	O
systems	O
that	O
used	O
BERT	B-MethodName
showed	O
a	O
smaller	O
performance	O
gap	O
between	O
VUA	O
and	O
TOEFL	O
data	O
;	O
in	O
zhengchang	O
the	O
gap	O
is	O
all	O
but	O
eliminated	O
.	O
This	O
suggests	O
that	O
a	O
BERT	B-MethodName
-	O
based	O
system	O
with	O
parameters	O
optimized	O
for	O
performance	O
on	O
TOEFL	O
data	O
can	O
close	O
this	O
gap	O
.	O
Considering	O
TOEFL	O
data	O
as	O
an	O
additional	O
genre	O
,	O
along	O
with	O
the	O
four	O
genres	O
represented	O
in	O
VUA	O
,	O
we	O
observe	O
that	O
it	O
is	O
generally	O
harder	O
than	O
Academic	O
and	O
News	O
,	O
and	O
is	O
commensurate	O
with	O
Fiction	O
in	O
terms	O
of	O
performance	O
,	O
for	O
the	O
three	O
systems	O
with	O
best	O
VUA	O
All	O
POS	O
performance	O
(	O
Deep	O
-	O
Met	B-DatasetName
:	O
0.72	O
both	O
,	O
Go	O
Figure	O
!	O
:	O
0.69	O
both	O
,	O
illiniMet	O
:	O
0.69	O
for	O
VUA	O
Fiction	O
,	O
.70	O
for	O
TOEFL	O
)	O
;	O
a	O
caveat	O
to	O
this	O
observation	O
is	O
that	O
the	O
difference	O
between	O
VUA	O
and	O
TOEFL	O
is	O
not	O
only	O
in	O
genre	O
but	O
in	O
the	O
metaphor	O
annotation	O
guidelines	O
as	O
well	O
.	O

Table	O
8	O
shows	O
performance	O
for	O
All	O
POS	O
track	O
on	O
the	O
TOEFL	O
data	O
by	O
the	O
writer	O
's	O
proficiency	O
level	O
-	O
high	O
or	O
medium	O
.	O
We	O
note	O
that	O
the	O
quality	O
of	O
the	O
human	O
annotations	O
does	O
not	O
appear	O
to	O
differ	O
substantially	O
by	O
proficiency	O
:	O
The	O
average	O
inter	O
-	O
annotator	O
agreement	O
for	O
the	O
high	O
proficiency	O
essays	O
was	O
κ	O
=	O
0.619	O
,	O
while	O
it	O
was	O
κ	O
=	O
0.613	O
for	O
the	O
medium	O
proficiency	O
essays	O
.	O
We	O
observe	O
that	O
generally	O
systems	O
tend	O
to	O
perform	O
better	O
on	O
the	O
higher	O
proficiency	O
essays	O
,	O
although	O
two	O
of	O
the	O
12	O
systems	O
posted	O
better	O
performance	O
on	O
the	O
medium	O
proficiency	O
data	O
.	O
However	O
,	O
even	O
though	O
the	O
medium	O
proficiency	O
essays	O
might	O
have	O
deficiencies	O
in	O
grammar	O
,	O
spelling	O
,	O
coherence	O
and	O
other	O
properties	O
of	O
the	O
essay	O
that	O
could	O
interfere	O
with	O
metaphor	O
detection	O
,	O
we	O
generally	O
observe	O
relatively	O
small	O
differences	O
in	O
performance	O
by	O
proficiency	O
-	O
up	O
to	O
3.5	O
F1	B-MetricName
points	O
,	O
with	O
a	O
few	O
ex	O
-	O
ceptions	O
(	O
zhengchang	O
,	O
Go	O
Figure	O
!	O
)	O
.	O
Interestingly	O
,	O
automatic	O
correction	O
of	O
spelling	O
errors	O
does	O
not	O
seem	O
to	O
guarantee	O
a	O
smaller	O
gap	O
in	O
performance	O
(	O
see	O
,	O
Go	O
Figure	O
!	O
)	O
.	O
In	O
parentheses	O
,	O
we	O
show	O
the	O
rank	O
of	O
the	O
given	O
proficiency	O
level	O
within	O
all	O
levels	O
for	O
the	O
system	O
.	O
The	O
last	O
column	O
shows	O
the	O
overall	O
drop	O
in	O
performance	O
from	O
best	O
proficiency	O
level	O
(	O
ranked	O
1	O
)	O
to	O
worst	O
(	O
ranked	O
4	O
)	O
.	O
The	O
top	O
three	O
performances	O
for	O
a	O
given	O
genre	O
are	O
boldfaced	O
.	O

Table	O
9	O
shows	O
the	O
performance	O
of	O
the	O
systems	O
submitted	O
to	O
the	O
All	O
POS	O
tracks	O
for	O
VUA	O
and	O
TOEFL	O
data	O
broken	O
down	O
by	O
part	O
of	O
speech	O
(	O
Verbs	O
,	O
Nouns	O
,	O
Adjectives	O
,	O
Adverbs	O
)	O
.	O
As	O
can	O
be	O
observed	O
both	O
from	O
the	O
All	O
POS	O
vs	O
Verbs	O
tracks	O
(	O
Tables	O
4	O
and	O
5	O
)	O
and	O
from	O
Table	O
9	O
,	O
performance	O
on	O
Verbs	O
is	O
generally	O
better	O
than	O
on	O
All	O
POS	O
.	O
8	O
For	O
VUA	O
data	O
,	O
all	O
but	O
one	O
systems	O
perform	O
best	O
on	O
Verbs	O
,	O
followed	O
by	O
Adjectives	O
and	O
Nouns	O
,	O
with	O
the	O
worst	O
performance	O
generally	O
observed	O
for	O
Adverbs	O
.	O
These	O
results	O
replicate	O
the	O
findings	O
from	O
the	O
2018	O
shared	O
task	O
and	O
follow	O
the	O
proportions	O
of	O
metaphors	O
in	O
the	O
respective	O
parts	O
of	O
speech	O
,	O
led	O
by	O
Verbs	O
(	O
30	O
%	O
)	O
,	O
Adjectives	O
(	O
18	O
%	O
)	O
,	O
Nouns	O
(	O
13	O
%	O
)	O
,	O
Adverbs	O
(	O
8	O
%	O
)	O
.	O
The	O
average	O
gap	O
between	O
best	O
and	O
worst	O
POS	O
performance	O
has	O
also	O
stayed	O
similar	O
-	O
11	O
F1	B-MetricName
points	O
(	O
it	O
was	O
9	O
%	O
in	O
2018	O
)	O
.	O
For	O
the	O
TOEFL	O
data	O
,	O
the	O
situation	O
is	O
quite	O
different	O
.	O
Adjectives	O
lead	O
the	O
scoreboard	O
for	O
all	O
but	O
3	O
systems	O
,	O
with	O
Adverbs	O
and	O
Verbs	O
coming	O
next	O
,	O
while	O
Nouns	O
proved	O
to	O
be	O
the	O
most	O
challenging	O
category	O
for	O
all	O
participating	O
systems	O
.	O
Furthermore	O
,	O
the	O
gap	O
between	O
best	O
and	O
worst	O
POS	O
performance	O
is	O
large	O
-	O
17	O
F1	B-MetricName
points	O
on	O
average	O
,	O
ranging	O
between	O
11	O
and	O
22	O
points	O
.	O
The	O
best	O
performance	O
on	O
Nouns	O
is	O
only	O
F1	B-MetricName
=	O
0.641	O
;	O
it	O
would	O
have	O
ranked	O
10th	O
out	O
of	O
12	O
on	O
Adjectives	O
.	O
The	O
proportions	O
of	O
metaphorically	O
used	O
Verbs	O
(	O
13	O
%	O
)	O
,	O
Adjectives	O
(	O
8	O
%	O
)	O
,	O
Nouns	O
(	O
4	O
%	O
)	O
,	O
and	O
Adverbs	O
(	O
3	O
%	O
)	O
(	O
based	O
on	O
training	O
data	O
)	O
perhaps	O
offer	O
some	O
explanation	O
of	O
the	O
difficulty	O
with	O
nouns	O
,	O
since	O
nominal	O
metaphors	O
seem	O
to	O
be	O
quite	O
rare	O
.	O
Stemle	O
and	O
Onysko	O
(	O
2020	O
)	O
observed	O
that	O
metaphors	O
occur	O
more	O
frequently	O
in	O
responses	O
to	O
some	O
essay	O
prompts	O
that	O
to	O
others	O
among	O
the	O
8	O
prompts	O
covered	O
in	O
the	O
TOEFL	O
dataset	O
;	O
moreover	O
,	O
for	O
some	O
prompts	O
,	O
a	O
metaphor	O
is	O
suggested	O
in	O
the	O
prompt	O
itself	O
and	O
occurs	O
frequently	O
in	O
responses	O
(	O
e.g.	O
whether	O
broad	O
knowledge	O
is	O
better	O
than	O
specialized	O
knowledge	O
)	O
.	O
It	O
is	O
possible	O
that	O
prompt	O
-	O
based	O
patterns	O
interact	O
with	O
POS	O
patterns	O
in	O
ways	O
that	O
affect	O
relative	O
ease	O
or	O
difficulty	O
of	O
POS	O
for	O
metaphor	O
identification	O
.	O
9	O
:	O
VUA	O
and	O
TOEFL	O
Datasets	O
by	O
POS	O
:	O
Performance	O
(	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
)	O
of	O
the	O
best	O
systems	O
submitted	O
to	O
All	O
-	O
POS	O
track	O
by	O
POS	O
subsets	O
of	O
the	O
test	O
data	O
.	O
In	O
parentheses	O
,	O
we	O
show	O
the	O
rank	O
of	O
the	O
given	O
POS	O
within	O
all	O
POS	O
for	O
the	O
system	O
.	O
The	O
last	O
column	O
shows	O
the	O
overall	O
drop	O
in	O
performance	O
from	O
best	O
POS	O
(	O
ranked	O
1	O
)	O
to	O
worst	O
(	O
ranked	O
4	O
)	O
.	O

Our	O
query	O
-	O
assisted	O
summarization	B-TaskName
model	O
,	O
M	O
Summ	O
,	O
is	O
autoregressive	O
,	O
outputting	O
the	O
requested	O
number	O
of	O
summary	O
sentences	O
one	O
-	O
by	O
-	O
one	O
.	O
At	O
time	O
step	O
t	O
,	O
a	O
sentence	O
e	O
out	O
t	O
is	O
output	O
according	O
to	O
the	O
current	O
query	O
and	O
an	O
encoding	O
of	O
the	O
summary	O
-	O
so	O
-	O
far	O
E	O
t	O
=	O
{	O
e	O
in	O
1	O
,	O
...	O
,	O
e	O
in	O
k	O
,	O
e	O
out	O
1	O
,	O
...	O
,	O
e	O
out	O
t−1	O
}	O
to	O
prevent	O
information	O
repetition	O
.	O
At	O
inference	O
time	O
,	O
M	O
Summ	O
outputs	O
the	O
summary	O
sentences	O
with	O
the	O
given	O
query	O
and	O
history	O
(	O
possibly	O
empty	O
)	O
.	O
At	O
train	O
time	O
,	O
we	O
emulate	O
a	O
session	O
by	O
invoking	O
M	O
Summ	O
with	O
a	O
sequence	O
of	O
differing	O
queries	O
,	O
Q	O
=	O
{	O
q	O
1	O
,	O
q	O
2	O
,	O
...	O
,	O
q	O
m	O
}	O
,	O
for	O
which	O
to	O
generate	O
the	O
corresponding	O
sequence	O
of	O
output	O
sentences	O
.	O
I.e.	O
,	O
output	O
sentence	O
e	O
out	O
t	O
is	O
biased	O
on	O
query	O
q	O
t	O
and	O
the	O
summary	O
-	O
so	O
-	O
far	O
E	O
t	O
at	O
time	O
step	O
t.	O
We	O
next	O
describe	O
the	O
architecture	O
3	O
of	O
M	O
Summ	O
,	O
also	O
illustrated	O
in	O
Figure	O
2	O
.	O
Sentence	O
encoding	O
.	O
The	O
first	O
step	O
of	O
the	O
model	O
is	O
hierarchically	O
encoding	O
the	O
sentences	O
of	O
the	O
document	O
set	O
D	O
to	O
obtain	O
contextualized	O
representation	O
c	O
j	O
for	O
sentence	O
s	O
j	O
∀j	O
.	O
A	O
CNN	O
(	O
Kim	O
,	O
2014	O
)	O
encodes	O
s	O
j	O
on	O
the	O
sentence	O
level	O
and	O
then	O
a	O
bi	O
-	O
LSTM	B-MethodName
(	O
Huang	O
et	O
al	O
,	O
2015	O
)	O
forms	O
representation	O
c	O
j	O
on	O
the	O
document	O
level	O
,	O
given	O
the	O
CNN	O
encodings	O
.	O
Query	O
encoding	O
.	O
Additionally	O
,	O
at	O
each	O
time	O
step	O
t	O
we	O
prepare	O
sentence+query	O
representations	O
c	O
t	O
j	O
=	O
c	O
j	O
CNN	O
(	O
q	O
t	O
)	O
,	O
i.e.	O
,	O
obtained	O
by	O
concatenating	O
a	O
sentence	O
representation	O
and	O
the	O
CNN	O
-	O
encoding	O
of	O
the	O
current	O
query	O
.	O
This	O
sentence+query	O
represen	O
-	O
Contextual	O
sentence	B-TaskName
embeddings	I-TaskName
are	O
concatenated	O
to	O
the	O
current	O
query	O
embedding	O
.	O
The	O
sentence+query	O
representation	O
is	O
softly	O
attended	O
with	O
a	O
transformed	O
query	O
-	O
focused	O
MMR	O
score	O
,	O
and	O
a	O
sentence	O
selection	O
distribution	O
is	O
obtained	O
with	O
a	O
two	O
-	O
hop	O
attention	O
mechanism	O
,	O
considering	O
a	O
summary	O
-	O
so	O
-	O
far	O
representation	O
.	O
A	O
dual	O
-	O
reward	O
mechanism	O
,	O
using	O
the	O
reference	O
summaries	O
and	O
query	O
,	O
optimizes	O
a	O
policy	O
to	O
train	O
the	O
model	O
for	O
summary	O
content	O
quality	O
and	O
sentence	O
-	O
to	O
-	O
query	O
resemblance	O
.	O
At	O
inference	O
time	O
,	O
an	O
initial	O
summary	O
is	O
generated	O
with	O
empty	O
E	O
in	O
and	O
q	O
t	O
-	O
s	O
,	O
while	O
for	O
an	O
expansion	O
they	O
are	O
not	O
empty	O
.	O
tation	O
influences	O
the	O
relevance	O
of	O
a	O
sentence	O
with	O
respect	O
to	O
the	O
current	O
input	O
query	O
.	O
Query	O
-	O
MMR	O
score	O
weighting	O
.	O
MMR	O
has	O
been	O
shown	O
to	O
be	O
effective	O
in	O
MDS	O
,	O
where	O
information	O
repeats	O
across	O
documents	O
.	O
It	O
aims	O
to	O
select	O
a	O
salient	O
sentence	O
for	O
a	O
summary	O
,	O
that	O
is	O
non	O
-	O
redundant	O
to	O
previous	O
summary	O
sentences	O
.	O
We	O
extend	O
standard	O
MMR	O
so	O
that	O
the	O
importance	O
of	O
the	O
sentence	O
is	O
in	O
regards	O
to	O
both	O
the	O
document	O
set	O
and	O
the	O
query	O
.	O
Formally	O
,	O
the	O
query	O
-	O
focused	O
MMR	O
function	O
defines	O
a	O
score	O
m	O
t	O
j	O
for	O
each	O
s	O
j	O
at	O
time	O
step	O
t	O
as	O
follows	O
:	O
m	O
t	O
j	O
=	O
λ	O
BISIM	O
(	O
s	O
j	O
,	O
D	O
,	O
q	O
t	O
)	O
−	O
(	O
1	O
−	O
λ	O
)	O
max	O
e	O
Et	O
SIM	O
(	O
s	O
j	O
,	O
e	O
)	O
(	O
1	O
)	O
BISIM	O
(	O
s	O
j	O
,	O
D	O
,	O
q	O
t	O
)	O
=	O
β	B-HyperparameterName
SIM	O
(	O
s	O
j	O
,	O
D	O
)	O
+	O
(	O
1	O
−	O
β	B-HyperparameterName
)	O
SIM	O
(	O
s	O
j	O
,	O
q	O
t	O
)	O
(	O
2	O
)	O
where	O
λ	O
[	O
0	B-DatasetName
,	O
1	O
]	O
balances	O
salience	O
and	O
redundancy	O
and	O
β	B-HyperparameterName
[	O
0	B-DatasetName
,	O
1	O
]	O
balances	O
a	O
sentence	O
's	O
salience	O
within	O
its	O
document	O
set	O
and	O
its	O
resemblance	O
to	O
the	O
current	O
query	O
.	O
SIM	O
(	O
x	O
,	O
y	O
)	O
measures	O
the	O
similarity	O
of	O
texts	O
x	O
and	O
y	O
,	O
and	O
D	O
is	O
a	O
fully	O
concatenated	O
version	O
of	O
document	O
set	O
D.	O
Following	O
findings	O
of	O
,	O
SIM	O
computes	O
cosine	O
similarity	O
between	O
the	O
two	O
compared	O
texts	O
'	O
TF	O
-	O
IDF	O
vectors	O
.	O
Redundancy	O
to	O
previous	O
sentences	O
is	O
computed	O
as	O
the	O
highest	O
similarity	O
-	O
score	O
against	O
any	O
of	O
the	O
previous	O
sentences	O
.	O
We	O
set	O
λ	O
=	O
0.6	O
(	O
following	O
Lebanoff	O
et	O
al	O
,	O
2018	O
)	O
and	O
β	B-HyperparameterName
=	O
0.5	O
(	O
see	O
Appendix	O
B.3	O
)	O
.	O
The	O
query	O
-	O
focused	O
MMR	O
scores	O
are	O
incorporated	O
into	O
M	O
Summ	O
by	O
softly	O
attending	O
on	O
the	O
sentence	O
representations	O
with	O
their	O
respective	O
translated	O
query	O
-	O
focused	O
MMR	O
scores	O
:	O
µ	O
t	O
=	O
softmax	B-MethodName
(	O
MLP	B-DatasetName
(	O
m	O
t	O
)	O
)	O
(	O
3	O
)	O
c	O
t	O
j	O
=	O
µ	O
t	O
j	O
c	O
t	O
j	O
(	O
4	O
)	O
State	O
representation	O
.	O
At	O
time	O
t	O
,	O
a	O
representation	O
z	O
t	O
of	O
the	O
summary	O
-	O
so	O
-	O
far	O
is	O
computed	O
by	O
applying	O
an	O
LSTM	B-MethodName
encoder	O
on	O
{	O
c	O
idx	O
(	O
e	O
in	O
1	O
)	O
,	O
...	O
,	O
c	O
idx	O
(	O
e	O
in	O
k	O
)	O
,	O
c	O
idx	O
(	O
e	O
out	O
1	O
)	O
,	O
...	O
,	O
c	O
idx	O
(	O
e	O
out	O
t−1	O
)	O
}	O
,	O
i.e.	O
,	O
on	O
the	O
plain	O
sentence	O
representations	O
of	O
E	O
t	O
,	O
where	O
idx	O
(	O
e	O
)	O
is	O
the	O
index	O
of	O
sentence	O
e.	O
Then	O
,	O
a	O
state	O
representation	O
g	O
t	O
considers	O
z	O
t	O
and	O
all	O
sentence	O
representations	O
with	O
the	O
glimpse	O
operation	O
(	O
Vinyals	O
et	O
al	O
,	O
2016	O
)	O
:	O
a	O
t	O
j	O
=	O
v	O
1	O
tanh	O
(	O
W	O
1ĉ	O
t	O
j	O
+	O
W	O
2	O
z	O
t	O
)	O
(	O
5	O
)	O
α	B-HyperparameterName
t	O
=	O
softmax	B-MethodName
(	O
a	O
t	O
)	O
(	O
6	O
)	O
g	O
t	O
=	O
j	O
α	B-HyperparameterName
t	O
j	O
W	O
1ĉ	O
t	O
j	O
(	O
7	O
)	O
where	O
v	O
1	O
,	O
W	O
1	O
and	O
W	O
2	O
are	O
model	O
parameters	O
,	O
and	O
a	O
t	O
represents	O
the	O
vector	O
composed	O
of	O
a	O
t	O
j	O
.	O
Finally	O
,	O
a	O
sentence	O
s	O
j	O
at	O
time	O
t	O
is	O
assigned	O
a	O
selection	O
probability	O
softmax	B-MethodName
(	O
p	O
t	O
)	O
j	O
such	O
that	O
:	O
p	O
t	O
j	O
=	O
v2	O
tanh	O
(	O
W	O
3ĉ	O
t	O
j	O
+	O
W	O
4	O
g	O
t	O
)	O
if	O
sj	O
/	O
Et	O
−	O
otherwise	O
(	O
8	O
)	O
where	O
v	O
2	O
,	O
W	O
3	O
and	O
W	O
4	O
are	O
model	O
parameters	O
.	O
Reinforcement	O
learning	O
.	O
As	O
M	O
Summ	O
's	O
goal	O
is	O
to	O
incrementally	O
generate	O
a	O
query	O
-	O
assisted	O
summary	O
,	O
it	O
should	O
strive	O
to	O
optimize	O
(	O
1	O
)	O
nonredundant	O
salient	O
-	O
sentence	O
extraction	O
and	O
(	O
2	O
)	O
queryto	O
-	O
sentence	O
similarity	O
,	O
that	O
can	O
be	O
appraised	O
with	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
and	O
text	O
-	O
similarity	O
metrics	O
,	O
respectively	O
.	O
A	O
policy	O
gradient	O
-	O
based	O
RL	O
approach	O
(	O
Williams	O
,	O
1992	O
)	O
allows	O
optimizing	O
on	O
such	O
nondifferentiable	O
metrics	O
.	O
Specifically	O
,	O
we	O
adopt	O
the	O
Advantage	O
Actor	O
Critic	O
method	O
(	O
Mnih	O
et	O
al	O
,	O
2016	O
)	O
for	O
policy	O
learning	O
,	O
and	O
a	O
dual	O
-	O
reward	O
procedure	O
(	O
Pasunuru	O
and	O
Bansal	O
,	O
2018	O
)	O
to	O
alternate	O
between	O
the	O
summary	O
and	O
query	O
-	O
similarity	O
rewards	O
.	O
At	O
time	O
step	O
t	O
,	O
for	O
selected	O
sentence	O
e	O
out	O
t	O
(	O
based	O
on	O
softmax	B-MethodName
(	O
p	O
t	O
)	O
)	O
,	O
reward	O
r	O
t	O
is	O
computed	O
and	O
weighted	O
into	O
M	O
Summ	O
's	O
loss	B-MetricName
function	O
.	O
The	O
reward	O
function	O
alternates	O
,	O
from	O
one	O
train	O
batch	O
to	O
the	O
next	O
,	O
between	O
ROUGE	O
∆	O
(	O
e	O
out	O
t	O
,	O
E	O
t	O
,	O
R	O
)	O
and	O
QSIM	O
(	O
e	O
out	O
t	O
,	O
q	O
t	O
)	O
.	O
The	O
former	O
computes	O
the	O
ROUGE	O
difference	O
before	O
adding	O
e	O
t	O
to	O
E	O
t	O
and	O
after	O
:	O
ROUGE∆	O
(	O
e	O
out	O
t	O
,	O
Et	O
,	O
R	O
)	O
=	O
ROUGE	O
(	O
(	O
Et	O
∪	O
e	O
out	O
t	O
)	O
,	O
R	O
)	O
−	O
ROUGE	O
(	O
E	O
t	O
,	O
R	O
)	O
(	O
9	O
)	O
A	O
larger	O
ROUGE	O
∆	O
value	O
implies	O
that	O
e	O
t	O
concisely	O
adds	O
more	O
information	O
onto	O
E	O
t	O
,	O
with	O
respect	O
to	O
topic	O
reference	O
summaries	O
R.	O
We	O
use	O
ROUGE	O
-	O
1	O
F	O
1	O
as	O
the	O
ROUGE	O
function	O
here	O
.	O
The	O
querysimilarity	O
reward	O
function	O
QSIM	O
(	O
e	O
out	O
t	O
,	O
q	O
t	O
)	O
=	O
avg	O
(	O
SEMSIM	O
(	O
e	O
out	O
t	O
,	O
q	O
t	O
)	O
,	O
LEXSIM	O
(	O
e	O
out	O
t	O
,	O
q	O
t	O
)	O
)	O
(	O
10	O
)	O
computes	O
an	O
average	O
of	O
semantic	O
and	O
lexical	O
similarities	O
between	O
the	O
selected	O
sentence	O
and	O
corresponding	O
query	O
.	O
SEMSIM	O
computes	O
the	O
cosine	O
similarity	O
between	O
the	O
average	O
of	O
word	B-TaskName
embeddings	I-TaskName
(	O
spaCy	O
:	O
Honnibal	O
and	O
Montani	O
,	O
2021	O
)	O
of	O
e	O
out	O
t	O
and	O
that	O
of	O
q	O
t	O
.	O
For	O
lexical	O
similarity	O
,	O
LEXSIM	O
(	O
e	O
out	O
t	O
,	O
q	O
t	O
)	O
=	O
avg	O
(	O
R	O
p	O
1	O
(	O
e	O
out	O
t	O
,	O
q	O
t	O
)	O
,	O
R	O
p	O
2	O
(	O
e	O
out	O
t	O
,	O
q	O
t	O
)	O
,	O
R	O
p	O
L	O
(	O
e	O
out	O
t	O
,	O
q	O
t	O
)	O
)	O
(	O
11	O
)	O
is	O
the	O
average	O
of	O
ROUGE	O
-	O
1	O
,	O
2	O
and	O
L	O
precision	O
scores	O
between	O
sentence	O
and	O
query	O
.	O
By	O
alternating	O
between	O
the	O
two	O
rewards	O
,	O
we	O
train	O
a	O
sentenceselection	O
policy	O
in	O
M	O
Summ	O
to	O
balance	O
summary	O
informativeness	O
and	O
adherence	O
to	O
queries	O
.	O
Overall	O
system	O
.	O
Our	O
M	O
Summ	O
model	O
adopts	O
its	O
base	O
architecture	O
from	O
(	O
for	O
generic	O
MDS	O
)	O
.	O
Chiefly	O
,	O
we	O
modify	O
their	O
model	O
for	O
handling	O
an	O
input	O
query	O
-	O
sequence	O
and	O
a	O
sentence	O
history	O
,	O
and	O
employ	O
a	O
different	O
summarization	B-TaskName
reward	O
function	O
.	O
The	O
query	O
is	O
incorporated	O
in	O
the	O
sentence	O
representation	O
,	O
in	O
the	O
new	O
query	O
-	O
focused	O
MMR	O
function	O
and	O
in	O
the	O
dual	O
-	O
reward	O
mechanism	O
.	O

We	O
adopt	O
and	O
adjust	O
the	O
architecture	O
in	O
3.2	O
for	O
this	O
subtask	O
.	O
Similar	O
to	O
M	O
Summ	O
,	O
M	O
Sugg	O
selects	O
input	O
units	O
one	O
-	O
by	O
-	O
one	O
considering	O
a	O
history	O
,	O
with	O
the	O
main	O
difference	O
being	O
the	O
absence	O
of	O
query	O
injection	O
.	O
Additionally	O
,	O
inputs	O
and	O
outputs	O
are	O
processed	O
on	O
the	O
phrase	O
-	O
rather	O
than	O
the	O
sentence	O
level	O
.	O
Phrase	O
and	O
state	O
representation	O
.	O
For	O
the	O
given	O
document	O
set	O
,	O
all	O
noun	O
phrases	O
are	O
extracted	O
using	O
a	O
standard	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
regular	O
expression	O
method	O
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
;	O
Wan	O
and	O
Xiao	O
,	O
2008	O
)	O
.	O
We	O
obtain	O
document	O
-	O
level	O
contextual	O
phrase	O
embeddings	O
,	O
c	O
j	O
for	O
phrase	O
ρ	O
j	O
,	O
with	O
the	O
CNN	O
and	O
bi	O
-	O
LSTM	B-MethodName
networks	O
,	O
and	O
softly	O
attend	O
the	O
embeddings	O
with	O
a	O
standard	O
MMR	O
score	O
:	O
m	O
t	O
j	O
=	O
λ	O
SIM	O
(	O
ρ	O
j	O
,	O
D	O
)	O
−	O
(	O
1	O
−	O
λ	O
)	O
max	O
e	O
Et	O
SIM	O
(	O
ρ	O
j	O
,	O
e	O
)	O
(	O
12	O
)	O
The	O
MMR	O
-	O
based	O
phrase	O
representations	O
then	O
pass	O
through	O
the	O
glimpse	O
attention	O
procedure	O
,	O
which	O
culminates	O
in	O
the	O
phrase	O
probability	O
distribution	O
for	O
selecting	O
the	O
next	O
output	O
phrase	O
.	O
Reinforcement	O
learning	O
.	O
The	O
policy	O
in	O
M	O
Sugg	O
is	O
trained	O
with	O
a	O
single	O
reward	O
function	O
that	O
measures	O
how	O
prominent	O
the	O
selected	O
phrase	O
is	O
within	O
the	O
reference	O
summaries	O
,	O
and	O
how	O
different	O
it	O
is	O
from	O
previously	O
seen	O
phrases	O
.	O
Formally	O
,	O
at	O
time	O
step	O
t	O
,	O
the	O
reward	O
r	O
t	O
of	O
selected	O
phrase	O
e	O
out	O
t	O
is	O
:	O
r	O
t	O
=	O
PF	O
(	O
e	O
out	O
t	O
,	O
R	O
)	O
−	O
γ	B-HyperparameterName
1	O
PFMAX	O
(	O
e	O
out	O
t	O
,	O
E	O
in	O
,	O
R	O
)	O
−	O
γ	B-HyperparameterName
2	O
PFMAX	O
(	O
e	O
out	O
t	O
,	O
E	O
t	O
\	O
E	O
in	O
,	O
R	O
)	O
(	O
13	O
)	O
PF	O
(	O
e	O
out	O
t	O
,	O
R	O
)	O
=	O
avg	O
r	O
R	O
(	O
avg	O
w	O
e	O
out	O
t	O
TF	O
(	O
w	O
,	O
r	O
)	O
)	O
(	O
14	O
)	O
PFMAX	O
(	O
e	O
out	O
t	O
,	O
L	O
,	O
R	O
)	O
=	O
max	O
e	O
L	O
PF	O
(	O
e	O
out	O
t	O
∩e	O
,	O
R	O
)	O
(	O
15	O
)	O
where	O
TF	O
(	O
w	O
,	O
r	O
)	O
is	O
the	O
relative	O
frequency	O
of	O
word	O
w	O
in	O
reference	O
summary	O
r.	O
Namely	O
,	O
PF	O
computes	O
the	O
average	O
term	O
frequency	O
of	O
a	O
phrase	O
over	O
its	O
words	O
and	O
across	O
the	O
reference	O
summaries	O
,	O
as	O
an	O
estimate	O
of	O
the	O
phrase	O
importance	O
within	O
the	O
topic	O
.	O
PFMAX	O
computes	O
the	O
highest	O
PF	O
against	O
a	O
list	O
of	O
phrases	O
,	O
which	O
is	O
used	O
to	O
lower	O
the	O
reward	O
of	O
a	O
phrase	O
that	O
is	O
redundant	O
to	O
phrases	O
used	O
earlier	O
.	O
Different	O
weights	O
are	O
given	O
to	O
the	O
PFMAX	O
against	O
the	O
input	O
history	O
(	O
γ	B-HyperparameterName
1	O
)	O
and	O
that	O
of	O
the	O
phrases	O
output	O
so	O
far	O
(	O
γ	B-HyperparameterName
2	O
)	O
.	O

Similarly	O
to	O
M	O
Summ	O
,	O
we	O
first	O
pre	O
-	O
train	O
the	O
base	O
model	O
to	O
get	O
a	O
warm	O
start	O
on	O
embedding	O
formation	O
and	O
salience	O
detection	O
.	O
The	O
reduced	O
architecture	O
of	O
M	O
Summ	O
and	O
M	O
Sugg	O
for	O
pre	O
-	O
training	O
are	O
identical	O
.	O
We	O
use	O
the	O
same	O
DUC	B-DatasetName
2007	I-DatasetName
training	O
data	O
,	O
with	O
document	O
sets	O
and	O
reference	O
summaries	O
,	O
and	O
additionally	O
prepare	O
three	O
"	O
histories	O
"	O
per	O
topic	O
:	O
one	O
empty	O
and	O
two	O
non	O
-	O
empty	O
.	O
An	O
empty	O
history	O
mimics	O
generating	O
a	O
session	O
's	O
initial	O
list	O
of	O
suggested	O
queries	O
,	O
while	O
a	O
non	O
-	O
empty	O
history	O
trains	O
the	O
model	O
to	O
consider	O
previously	O
known	O
information	O
.	O
Training	O
with	O
two	O
non	O
-	O
empty	O
histories	O
per	O
topic	O
prepares	O
a	O
model	O
for	O
varying	O
informational	O
states	O
.	O
These	O
are	O
curated	O
from	O
a	O
generic	O
summary	O
(	O
from	O
a	O
trained	O
M	O
Summ	O
model	O
)	O
that	O
is	O
truncated	O
at	O
two	O
random	O
sentence	O
-	O
lengths	O
between	O
1	O
and	O
12	O
.	O
Overall	O
,	O
the	O
model	O
is	O
trained	O
on	O
three	O
versions	O
of	O
each	O
topic	O
,	O
each	O
time	O
with	O
a	O
different	O
history	O
.	O
Similarly	O
to	O
M	O
Summ	O
,	O
validation	O
is	O
guided	O
by	O
the	O
average	O
normalized	O
area	O
under	O
the	O
recall	O
curve	O
.	O
Here	O
,	O
the	O
accumulating	O
r	O
t	O
scores	O
from	O
Equation	O
13are	O
used	O
as	O
the	O
recall	O
of	O
the	O
expanding	O
suggested	O
queries	O
list	O
.	O
I.e.	O
,	O
a	O
higher	O
reward	O
means	O
better	O
suggested	O
queries	O
are	O
output	O
earlier	O
.	O
The	O
AUC	B-MetricName
is	O
normalized	O
with	O
the	O
total	O
token	O
-	O
length	O
of	O
all	O
suggested	O
queries	O
to	O
mitigate	O
for	O
lengthy	O
phrase	O
extractions	O
.	O

We	O
collect	O
real	O
user	O
sessions	O
via	O
controlled	O
crowdsourcing	O
(	O
which	O
provides	O
high	O
quality	O
work	O
,	O
see	O
Appendix	O
D	O
)	O
with	O
the	O
use	O
of	O
an	O
INTSUMM	O
web	O
application	O
5	O
running	O
either	O
our	O
M	O
Summ	O
+	O
M	O
Sugg	O
models	O
or	O
the	O
S	O
2	O
baseline	O
algorithms	O
,	O
enabling	O
a	O
comparative	O
assessment	O
of	O
the	O
two	O
systems	O
.	O
Notably	O
,	O
our	O
algorithms	O
have	O
the	O
low	O
latency	O
required	O
for	O
the	O
interactive	O
setting	O
(	O
Attig	O
et	O
al	O
,	O
2017	O
)	O
,	O
i.e.	O
,	O
responding	O
almost	O
immediately	O
.	O
6	O
Using	O
the	O
DUC	B-DatasetName
2006	I-DatasetName
INTSUMM	O
test	O
set	O
,	O
we	O
prepared	O
two	O
complementing	O
user	O
sets	O
of	O
20	O
topics	O
,	O
each	O
with	O
10	O
of	O
the	O
topics	O
to	O
be	O
run	O
on	O
our	O
system	O
and	O
the	O
other	O
10	O
on	O
the	O
baseline	O
.	O
We	O
apply	O
the	O
evaluation	O
metrics	O
of	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
area	O
under	O
the	O
sessions	O
'	O
ROUGE	O
recall	O
curves	O
,	O
in	O
a	O
common	O
word	O
-	O
length	O
interval	O
across	O
all	O
sessions	O
and	O
topics	O
,	O
which	O
demonstrates	O
how	O
fast	O
salient	O
information	O
is	O
exposed	O
in	O
sessions	O
.	O
(	O
2	O
)	O
ROUGE	O
F	O
1	O
at	O
the	O
initial	O
summary	O
and	O
at	O
250	O
tokens	O
,	O
that	O
indicate	O
how	O
effectively	O
the	O
interactive	O
system	O
can	O
generate	O
summaries	O
at	O
pre	O
-	O
specified	O
,	O
comparable	O
lengths	O
.	O
(	O
3	O
)	O
Manually	O
assigned	O
query	O
-	O
responsiveness	O
score	O
(	O
1	O
to	O
5	O
scale	O
)	O
,	O
which	O
expresses	O
how	O
well	O
users	O
think	O
the	O
system	O
responded	O
to	O
their	O
requests	O
.	O
And	O
(	O
4	O
)	O
manual	O
UMUX	O
-	O
Lite	O
(	O
Lewis	O
et	O
al	O
,	O
2013	O
)	O
score	O
for	O
system	O
usability	O
(	O
effectiveness	O
and	O
ease	O
of	O
use	O
)	O
,	O
where	O
68	O
is	O
considered	O
"	O
acceptable	O
"	O
and	O
80.3	O
is	O
considered	O
"	O
excellent	O
"	O
.	O
We	O
also	O
measure	O
automatic	O
query	O
-	O
responsiveness	O
with	O
QSIM	O
.	O
7	O
We	O
conducted	O
two	O
such	O
comparative	O
collection	O
and	O
assessment	O
experiments	O
,	O
either	O
employing	O
M	O
Summ	O
configuration	O
v	O
or	O
i	O
,	O
namely	O
the	O
best	O
of	O
the	O
two	O
configuration	O
sets	O
.	O
In	O
both	O
cases	O
,	O
the	O
M	O
Sugg	O
model	O
used	O
was	O
set	O
with	O
γ	B-HyperparameterName
1	O
=	O
0.5	O
and	O
γ	B-HyperparameterName
2	O
=	O
0.9	O
after	O
some	O
hyperparameter	O
tuning	O
(	O
Appendix	O
B.4	O
)	O
.	O
The	O
first	O
experiment	O
(	O
with	O
configuration	O
v	O
)	O
is	O
described	O
here	O
,	O
and	O
the	O
other	O
in	O
Appendix	O
E.1	O
.	O
We	O
hired	O
6	O
qualified	O
workers	O
using	O
the	O
controlled	O
crowdsourcing	O
procedure	O
,	O
and	O
collected	O
2	O
-	O
3	O
sessions	O
per	O
topic	O
per	O
system	O
(	O
111	O
total	O
sessions	O
)	O
.	O
In	O
the	O
sessions	O
,	O
users	O
explore	O
their	O
given	O
topic	O
by	O
submitting	O
queries	O
with	O
a	O
common	O
generic	O
informational	O
goal	O
in	O
mind	O
(	O
Appendix	O
D	O
)	O
.	O
Overall	O
system	O
assessment	O
.	O
Table	O
2	O
,	O
presenting	O
average	O
scores	O
over	O
the	O
collected	O
sessions	O
,	O
shows	O
that	O
our	O
system	O
is	O
significantly	O
more	O
effective	O
for	O
exposing	O
salient	O
information	O
,	O
as	O
depicted	O
in	O
the	O
first	O
three	O
rows	O
.	O
Users	O
indicate	O
a	O
slight	O
degradation	O
in	O
query	O
-	O
responsiveness	O
of	O
our	O
system	O
,	O
consistent	O
with	O
QSIM	O
scores	O
(	O
row	O
4	O
-	O
5	O
)	O
.	O
Note	O
that	O
the	O
observed	O
difference	O
in	O
QSIM	O
scores	O
,	O
between	O
simulations	O
and	O
user	O
sessions	O
,	O
partly	O
stems	O
from	O
the	O
fact	O
that	O
they	O
were	O
computed	O
over	O
different	O
sets	O
of	O
queries	O
.	O
The	O
varying	O
queries	O
issued	O
by	O
the	O
users	O
in	O
user	O
sessions	O
form	O
a	O
less	O
stable	O
query	O
responsiveness	O
comparison	O
than	O
the	O
one	O
in	O
Table	O
1	O
,	O
where	O
QSIM	O
scores	O
are	O
computed	O
using	O
consistent	O
queries	O
for	O
all	O
systems	O
.	O
Despite	O
the	O
gap	O
in	O
QSIM	O
scores	O
between	O
our	O
system	O
and	O
S	O
2	O
in	O
Table	O
2	O
,	O
the	O
overall	O
usability	O
scores	O
are	O
slightly	O
better	O
(	O
last	O
row	O
)	O
.	O
This	O
may	O
suggest	O
that	O
users	O
appreciate	O
the	O
informativeness	O
of	O
the	O
produced	O
summary	O
even	O
when	O
they	O
are	O
aware	O
that	O
the	O
summary	O
is	O
less	O
biased	O
on	O
their	O
queries	O
;	O
thus	O
our	O
system	O
improves	O
informativeness	O
while	O
still	O
providing	O
a	O
favorable	O
user	O
experience	O
.	O

Datasets	O
.	O
The	O
DUC	B-DatasetName
2006	I-DatasetName
and	O
2007	O
datasets	O
were	O
obtained	O
according	O
to	O
the	O
DUC	O
website	O
(	O
duc	O
.	O
nist.gov	O
)	O
requirements	O
.	O
It	O
was	O
not	O
possible	O
for	O
others	O
to	O
reconstruct	O
the	O
document	O
sets	O
and	O
reference	O
summaries	O
of	O
the	O
dataset	O
from	O
the	O
crowdsourcing	O
tasks	O
.	O
The	O
datasets	O
are	O
composed	O
of	O
new	O
articles	O
mainly	O
from	O
the	O
late	O
1990s	O
from	O
large	O
news	O
outlets	O
,	O
compiled	O
by	O
NIST	O
.	O
All	O
data	O
exposed	O
by	O
our	O
systems	O
are	O
directly	O
extracted	O
from	O
those	O
articles	O
.	O
For	O
extraction	O
,	O
we	O
do	O
not	O
intentionally	O
add	O
in	O
any	O
rules	O
for	O
ignoring	O
or	O
boosting	O
certain	O
information	O
due	O
to	O
an	O
opinion	O
.	O
Crowdsourcing	O
.	O
Due	O
to	O
the	O
need	O
for	O
English	O
speaking	O
workers	O
,	O
a	O
location	O
filter	O
was	O
set	O
on	O
the	O
Amazon	O
Mechanical	O
Turk	O
(	O
https://www	O
.	O
mturk.com	O
)	O
tasks	O
for	O
the	O
US	O
,	O
UK	O
and	O
Australia	O
.	O
All	O
tasks	O
paid	O
according	O
to	O
a	O
$	O
10	O
per	O
hour	O
wage	O
,	O
according	O
to	O
the	O
estimated	O
required	O
time	O
of	O
each	O
task	O
.	O
The	O
payment	O
was	O
either	O
paid	O
per	O
assignment	O
,	O
or	O
as	O
a	O
combination	O
with	O
a	O
bonus	O
.	O
Compute	O
resources	O
.	O
Our	O
M	O
Summ	O
and	O
M	O
Sugg	O
models	O
required	O
between	O
2	O
and	O
20	O
hours	O
of	O
training	O
(	O
usually	O
around	O
4	O
hours	O
)	O
,	O
depending	O
on	O
the	O
configuration	O
.	O
We	O
trained	O
on	O
one	O
NVIDIA	O
GeForce	O
GTX	O
1080	O
Ti	O
GPU	O
with	O
11	O
GB	O
memory	O
.	O
The	O
pretrained	O
base	O
model	O
was	O
trained	O
once	O
and	O
reused	O
in	O
all	O
subsequent	O
training	O
.	O
Outputting	O
at	O
inference	O
time	O
is	O
computationally	O
cheap	O
:	O
M	O
Summ	O
runs	O
upto	O
about	O
1	O
second	O
,	O
but	O
mostly	O
in	O
a	O
few	O
hundred	O
milliseconds	O
,	O
and	O
M	O
Sugg	O
runs	O
upto	O
about	O
7	O
seconds	O
,	O
but	O
mostly	O
in	O
under	O
4	O
seconds	O
.	O
Training	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8	O
used	O
about	O
3	O
GB	O
GPU	O
memory	O
for	O
M	O
Summ	O
,	O
and	O
about	O
9	O
GB	O
memory	O
for	O
M	O
Sugg	O
(	O
since	O
there	O
are	O
many	O
more	O
input	O
units	O
per	O
document	O
set	O
,	O
i.e.	O
,	O
all	O
noun	O
phrases	O
versus	O
sentences	O
)	O
.	O

To	O
provide	O
a	O
warm	O
start	O
for	O
training	O
M	O
Summ	O
and	O
M	O
Sugg	O
,	O
a	O
reduced	O
version	O
of	O
the	O
models	O
,	O
which	O
is	O
the	O
same	O
for	O
both	O
,	O
is	O
first	O
pre	O
-	O
trained	O
for	O
generic	O
extractive	O
single	O
-	O
document	B-TaskName
summarization	I-TaskName
using	O
the	O
CNN	B-DatasetName
/	I-DatasetName
Daily	I-DatasetName
Mail	I-DatasetName
corpus	O
(	O
Hermann	O
et	O
al	O
,	O
2015	O
)	O
with	O
about	O
287k	O
samples	O
,	O
as	O
proposed	O
by	O
Chen	O
and	O
Bansal	O
(	O
2018	O
)	O
.	O
In	O
this	O
reduced	O
model	O
,	O
ĉ	O
t	O
j	O
is	O
replaced	O
by	O
c	O
j	O
in	O
Equations	O
5	O
,	O
7	O
and	O
8	O
.	O
Further	O
-	O
more	O
,	O
there	O
is	O
a	O
single	O
reward	O
function	O
for	O
learning	O
the	O
policy	O
,	O
computed	O
per	O
selected	O
sentence	O
e	O
out	O
t	O
as	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
F	O
1	O
w.r.t	O
.	O
the	O
(	O
single	O
)	O
reference	O
summary	O
's	O
sentence	O
at	O
index	O
t.	O
The	O
reduced	O
model	O
pre	O
-	O
trains	O
the	O
full	O
model	O
for	O
contextual	O
sentence	O
representation	O
and	O
for	O
salient	O
-	O
sentence	O
selection	O
in	O
the	O
single	O
-	O
document	O
generic	O
setting	O
.	O
This	O
allows	O
training	O
M	O
Summ	O
and	O
M	O
Sugg	O
with	O
a	O
relatively	O
small	O
dataset	O
for	O
their	O
final	O
purposes	O
.	O

Following	O
,	O
the	O
pre	O
-	O
trained	O
base	O
model	O
is	O
the	O
rnn	O
-	O
ext	O
+	O
RL	O
model	O
from	O
Chen	O
and	O
Bansal	O
(	O
2018	O
)	O
,	O
and	O
is	O
trained	O
like	O
in	O
Lebanoff	O
et	O
al	O
(	O
2018	O
)	O
.	O
Both	O
M	O
Summ	O
and	O
M	O
Sugg	O
are	O
further	O
trained	O
on	O
our	O
adjusted	O
DUC	B-DatasetName
2007	I-DatasetName
data	O
using	O
an	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e	O
-	O
4	O
and	O
no	O
weight	B-MethodName
decay	I-MethodName
.	O
A	O
discount	O
factor	O
of	O
0.99	O
is	O
used	O
for	O
the	O
reinforcement	O
learning	O
rewards	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
was	O
8	O
.	O
Training	O
was	O
halted	O
once	O
30	O
consecutive	O
epochs	O
did	O
not	O
improve	O
the	O
validation	O
score	O
.	O
The	O
MMR	O
function	O
within	O
our	O
models	O
uses	O
TF	O
-	O
IDF	O
vector	O
cosine	O
similarity	O
for	O
all	O
SIM	O
instances	O
(	O
in	O
Equations	O
1	O
and	O
12	O
)	O
.	O
The	O
TF	O
-	O
IDF	O
vectorizer	O
is	O
initialized	O
with	O
the	O
document	O
set	O
on	O
which	O
the	O
MMR	O
score	O
is	O
computed	O
.	O
As	O
is	O
commonly	O
practiced	O
,	O
selection	O
of	O
an	O
output	O
sentence	O
/	O
phrase	O
e	O
out	O
t	O
is	O
done	O
by	O
sampling	O
probability	O
distribution	O
p	O
t	O
(	O
in	O
Equation	O
8	O
)	O
at	O
train	O
time	O
,	O
and	O
by	O
extracting	O
the	O
maximum	O
scoring	O
sentence	O
/	O
phrase	O
at	O
inference	O
time	O
.	O
The	O
MLP	B-DatasetName
in	O
Equation	O
3	O
transforms	O
the	O
MMR	O
score	O
with	O
a	O
feed	O
-	O
forward	O
network	O
with	O
one	O
-	O
hidden	O
layer	O
of	O
dimension	O
80	O
following	O
.	O

Model	O
configurations	O
.	O
The	O
architecture	O
of	O
the	O
M	O
Summ	O
model	O
and	O
its	O
training	O
allowed	O
for	O
much	O
creativity	O
in	O
the	O
configuration	O
process	O
.	O
Other	O
than	O
the	O
combinations	O
mentioned	O
in	O
the	O
paper	O
in	O
Table	O
1	O
,	O
we	O
also	O
experimented	O
with	O
other	O
components	O
.	O
We	O
list	O
here	O
many	O
of	O
the	O
experiments	O
,	O
without	O
formal	O
results	O
.	O
Anecdotes	O
are	O
taken	O
by	O
looking	O
at	O
validation	O
scores	O
and	O
some	O
eyeballing	O
.	O
(	O
1	O
)	O
The	O
β	B-HyperparameterName
value	O
in	O
the	O
query	O
-	O
focused	O
MMR	O
function	O
in	O
Equation	O
2	O
,	O
that	O
impacts	O
the	O
weight	O
of	O
the	O
query	O
on	O
a	O
sentence	O
versus	O
the	O
document	O
set	O
on	O
the	O
sentence	O
.	O
We	O
tried	O
out	O
a	O
few	O
β	B-HyperparameterName
values	O
and	O
mainly	O
noticed	O
that	O
a	O
value	O
of	O
0.5	O
kept	O
validation	O
results	O
more	O
stable	O
across	O
configurations	O
,	O
or	O
kept	O
training	O
time	O
shorter	O
.	O
In	O
our	O
experiments	O
,	O
to	O
cancel	O
out	O
this	O
component	O
(	O
both	O
at	O
training	O
and	O
inference	O
time	O
)	O
,	O
we	O
simply	O
set	O
β	B-HyperparameterName
=	O
1	O
so	O
that	O
the	O
query	O
is	O
not	O
considered	O
.	O
(	O
2	O
)	O
Different	O
summary	O
reward	O
functions	O
.	O
ROUGE	O
∆	O
recall	O
(	O
instead	O
of	O
F	O
1	O
)	O
was	O
also	O
a	O
good	O
alternative	O
,	O
but	O
gave	O
somewhat	O
less	O
stable	O
results	O
across	O
configurations	O
.	O
ROUGE	O
(	O
not	O
as	O
∆	O
)	O
was	O
also	O
less	O
stable	O
with	O
recall	O
and	O
F	O
1	O
,	O
and	O
gave	O
too	O
short	O
and	O
irrelevant	O
sentences	O
with	O
precision	O
.	O
We	O
also	O
tried	O
sentence	O
level	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
,	O
like	O
in	O
,	O
eventually	O
outputting	O
sentences	O
that	O
were	O
much	O
less	O
compliant	O
to	O
queries	O
.	O
(	O
3	O
)	O
Using	O
only	O
the	O
query	O
similarity	O
reward	O
instead	O
of	O
the	O
dual	O
reward	O
mechanism	O
worked	O
surprisingly	O
well	O
.	O
This	O
may	O
be	O
due	O
to	O
the	O
queries	O
on	O
which	O
the	O
model	O
was	O
trained	O
on	O
.	O
These	O
queries	O
were	O
very	O
relevant	O
to	O
the	O
gold	O
reference	O
summaries	O
,	O
hence	O
possibly	O
implicitly	O
providing	O
a	O
strong	O
signal	O
to	O
salient	O
sentences	O
within	O
the	O
document	O
set	O
.	O
Still	O
,	O
this	O
was	O
less	O
productive	O
than	O
our	O
final	O
choice	O
of	O
reward	O
.	O
(	O
4	O
)	O
Adding	O
training	O
data	O
(	O
additional	O
DUC	O
MDS	O
datasets	O
)	O
did	O
not	O
impact	O
the	O
results	O
.	O
Importantly	O
,	O
since	O
DUC	B-DatasetName
2007	I-DatasetName
is	O
most	O
similar	O
to	O
the	O
test	O
DUC	B-DatasetName
2006	I-DatasetName
set	O
,	O
it	O
seems	O
to	O
be	O
more	O
beneficial	O
to	O
include	O
DUC	B-DatasetName
2007	I-DatasetName
in	O
the	O
training	O
set	O
.	O
(	O
5	O
)	O
We	O
also	O
tried	O
representing	O
the	O
query	O
in	O
the	O
input	O
by	O
concatenating	O
it	O
's	O
raw	O
text	O
to	O
each	O
input	O
sentence	O
before	O
get	O
the	O
sentence	O
representations	O
.	O
(	O
6	O
)	O
To	O
represent	O
the	O
sentences	O
,	O
we	O
also	O
tried	O
using	O
average	O
w2v	O
vectors	O
(	O
Honnibal	O
and	O
Montani	O
,	O
2021	O
)	O
and	O
Sentence	O
-	O
BERT	B-MethodName
(	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
instead	O
of	O
the	O
CNN	O
network	O
.	O
These	O
did	O
not	O
show	O
any	O
apparent	O
improvements	O
,	O
and	O
were	O
notably	O
expensive	O
in	O
terms	O
of	O
execution	O
time	O
.	O
(	O
7	O
)	O
For	O
the	O
sentence	O
similarity	O
in	O
the	O
query	O
-	O
MMR	O
component	O
,	O
we	O
tried	O
w2v	O
and	O
Sentence	O
-	O
BERT	B-MethodName
representations	O
instead	O
of	O
TF	O
-	O
IDF	O
vectors	O
.	O
Similarly	O
to	O
(	O
6	O
)	O
,	O
they	O
did	O
not	O
show	O
improvements	O
over	O
using	O
TF	O
-	O
IDF	O
,	O
and	O
were	O
very	O
time	O
-	O
costly	O
.	O
(	O
8	O
)	O
Instead	O
of	O
the	O
dual	O
-	O
reward	O
mechanism	O
that	O
alternates	O
between	O
the	O
two	O
rewards	O
from	O
batch	O
to	O
batch	O
,	O
we	O
also	O
considered	O
using	O
a	O
weighted	O
average	O
of	O
the	O
two	O
rewards	O
,	O
consistently	O
over	O
all	O
batches	O
.	O
Further	O
experimentation	O
is	O
required	O
on	O
this	O
technique	O
for	O
a	O
more	O
conclusive	O
judgment	O
.	O
Queries	O
used	O
for	O
training	O
.	O
The	O
queries	O
used	O
for	O
training	O
the	O
M	O
Summ	O
model	O
can	O
affect	O
the	O
way	O
it	O
learns	O
to	O
respond	O
to	O
a	O
query	O
.	O
Seemingly	O
,	O
the	O
most	O
natural	O
approach	O
would	O
be	O
to	O
train	O
the	O
model	O
as	O
close	O
as	O
possible	O
to	O
the	O
model	O
's	O
use	O
at	O
inference	O
time	O
.	O
This	O
would	O
mean	O
training	O
M	O
Summ	O
with	O
queries	O
from	O
real	O
sessions	O
.	O
However	O
,	O
a	O
session	O
's	O
queries	O
are	O
dependent	O
on	O
outputs	O
previously	O
produced	O
by	O
the	O
used	O
system	O
.	O
It	O
is	O
therefore	O
not	O
certain	O
that	O
the	O
sequence	O
of	O
queries	O
from	O
a	O
different	O
system	O
's	O
usage	O
would	O
necessarily	O
benefit	O
the	O
training	O
process	O
when	O
compared	O
to	O
a	O
synthesized	O
sequence	O
of	O
queries	O
.	O
I.e.	O
,	O
it	O
's	O
not	O
actually	O
possible	O
to	O
train	O
with	O
"	O
real	O
sessions	O
"	O
in	O
a	O
conventional	O
way	O
.	O
Also	O
,	O
as	O
stated	O
in	O
3.3	O
,	O
the	O
synthetic	O
queries	O
we	O
eventually	O
used	O
direct	O
the	O
model	O
to	O
select	O
salient	O
sentences	O
,	O
which	O
can	O
support	O
our	O
dual	O
-	O
objectives	O
:	O
to	O
get	O
a	O
sentence	O
that	O
is	O
both	O
globally	O
salient	O
to	O
the	O
topic	O
,	O
as	O
well	O
as	O
responsive	O
to	O
the	O
query	O
.	O
We	O
tried	O
training	O
on	O
other	O
query	O
types	O
,	O
synthesized	O
with	O
various	O
keyphrase	B-TaskName
extraction	I-TaskName
techniques	O
,	O
and	O
found	O
that	O
our	O
final	O
choice	O
of	O
queries	O
more	O
consistently	O
gave	O
good	O
results	O
overall	O
.	O
Sentence	O
length	O
.	O
We	O
segmented	O
the	O
sentences	O
in	O
the	O
document	O
sets	O
with	O
the	O
NLTK	O
8	O
sentence	O
tokenizer	O
,	O
and	O
removed	O
sentences	O
that	O
contain	O
quotes	O
in	O
them	O
or	O
do	O
not	O
end	O
with	O
a	O
period	O
.	O
During	O
training	O
we	O
did	O
not	O
constrain	O
the	O
input	O
sentences	O
in	O
any	O
way	O
.	O
Some	O
of	O
the	O
configuration	O
experiments	O
described	O
above	O
were	O
done	O
to	O
check	O
how	O
the	O
configuration	O
might	O
influence	O
the	O
length	O
of	O
the	O
selected	O
sentences	O
.	O
The	O
best	O
configurations	O
,	O
including	O
the	O
one	O
we	O
eventually	O
used	O
in	O
our	O
tests	O
,	O
tended	O
to	O
output	O
somewhat	O
longer	O
sentences	O
.	O
Very	O
long	O
sentences	O
are	O
usually	O
tedious	O
for	O
human	O
readers	O
,	O
and	O
we	O
hence	O
limited	O
the	O
sentences	O
to	O
30	O
tokens	O
at	O
inference	O
time	O
.	O
We	O
found	O
that	O
this	O
length	O
constraint	O
caused	O
a	O
slight	O
degradation	O
in	O
simulation	O
score	O
results	O
of	O
our	O
models	O
,	O
however	O
still	O
gave	O
superior	O
informativeness	O
results	O
compared	O
to	O
the	O
baseline	O
system	O
.	O
Initial	O
summary	O
length	O
.	O
Sentences	O
are	O
accumulated	O
until	O
surpassing	O
75	O
tokens	O
.	O
Therefore	O
summaries	O
are	O
not	O
shorter	O
than	O
75	O
tokens	O
,	O
but	O
mostly	O
not	O
much	O
longer	O
than	O
that	O
.	O

Model	O
configurations	O
.	O
We	O
experimented	O
with	O
different	O
configurations	O
and	O
hyper	O
-	O
parameter	O
finetuning	O
in	O
the	O
M	O
Sugg	O
model	O
as	O
well	O
.	O
Tuning	O
was	O
performed	O
in	O
accordance	O
to	O
the	O
validation	O
scores	O
and	O
generic	O
keyphrase	B-TaskName
extraction	I-TaskName
scores	O
on	O
the	O
MK	O
-	O
DUC	O
-	O
01	O
multi	O
-	O
document	O
keyphrase	B-TaskName
extraction	I-TaskName
dataset	O
of	O
Shapira	O
et	O
al	O
(	O
2021a	O
)	O
.	O
(	O
1	O
)	O
In	O
the	O
reward	O
function	O
in	O
Equation	O
13	O
,	O
we	O
set	O
γ	B-HyperparameterName
1	O
=	O
0.5	O
and	O
γ	B-HyperparameterName
2	O
=	O
0.9	O
,	O
i.e.	O
,	O
the	O
preceding	O
output	O
phrases	O
are	O
more	O
strongly	O
accounted	O
for	O
than	O
the	O
phrases	O
in	O
the	O
session	O
history	O
.	O
We	O
tested	O
several	O
values	O
between	O
0	B-DatasetName
and	O
1	O
for	O
both	O
hyper	O
-	O
parameters	O
.	O
(	O
2	O
)	O
We	O
implemented	O
altered	O
versions	O
of	O
the	O
reward	O
function	O
in	O
Equation	O
13	O
.	O
Instead	O
of	O
phrase	O
unigram	O
-	O
level	O
frequency	O
,	O
we	O
tried	O
computing	O
the	O
full	O
phrase	O
frequency	O
and	O
computing	O
partial	O
phrase	O
frequency	O
,	O
i.e.	O
,	O
a	O
maximal	O
phrase	O
template	O
match	O
within	O
a	O
reference	O
summary	O
.	O
All	O
functions	O
tested	O
were	O
adequate	O
overall	O
,	O
though	O
our	O
final	O
choice	O
of	O
reward	O
function	O
was	O
closest	O
to	O
the	O
keyphrase	B-TaskName
extraction	I-TaskName
task	O
unigram	O
overlap	O
metric	O
,	O
and	O
gave	O
best	O
results	O
overall	O
.	O
(	O
3	O
)	O
We	O
also	O
attempted	O
noun	O
phrase	O
extraction	O
with	O
the	O
spaCy	O
9	O
noun	O
chunker	O
and	O
named	O
entity	O
recognizer	O
.	O
This	O
combined	O
approach	O
misses	O
some	O
noun	O
phrases	O
within	O
the	O
text	O
,	O
but	O
mainly	O
is	O
also	O
more	O
computationally	O
heavy	O
than	O
the	O
simple	O
POS	O
regex	O
search	O
that	O
we	O
use	O
.	O

Controlled	O
crowdsourcing	O
protocol	O
.	O
We	O
followed	O
the	O
controlled	O
crowdsourcing	O
protocol	O
of	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
,	O
which	O
includes	O
three	O
steps	O
:	O
(	O
1	O
)	O
a	O
trap	O
task	O
for	O
finding	O
qualified	O
workers	O
;	O
(	O
2	O
)	O
practice	O
tasks	O
for	O
explaining	O
the	O
interface	O
and	O
the	O
purpose	O
,	O
as	O
well	O
as	O
reiterating	O
the	O
generic	O
information	O
goal	O
(	O
see	O
below	O
)	O
during	O
exploration	O
;	O
(	O
3	O
)	O
the	O
session	O
collection	O
tasks	O
.	O
We	O
used	O
the	O
Amazon	O
Mechanical	O
Turk	O
HITs	O
prepared	O
by	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
.	O
Process	O
cost	O
.	O
We	O
paid	O
$	O
0.40	O
for	O
a	O
trap	O
task	O
assignment	O
,	O
with	O
400	O
assignments	O
released	O
,	O
and	O
$	O
0.90	O
for	O
a	O
practice	O
task	O
assignment	O
,	O
with	O
28	O
assignments	O
completed	O
.	O
The	O
session	O
collection	O
assignment	O
paid	O
$	O
0.70	O
,	O
and	O
a	O
bonus	O
mainly	O
according	O
to	O
the	O
length	O
of	O
interaction	O
and	O
additional	O
comments	O
provided	O
.	O
The	O
bonus	O
was	O
between	O
$	O
0.15	O
and	O
$	O
0.35	O
.	O
A	O
total	O
of	O
111	O
sessions	O
were	O
recorded	O
from	O
6	O
high	O
quality	O
workers	O
.	O
The	O
full	O
process	O
cost	O
about	O
$	O
385	O
in	O
total	O
(	O
including	O
the	O
Mechanical	O
Turk	O
fees	O
)	O
for	O
the	O
experiment	O
including	O
configuration	O
v	O
in	O
Table	O
1	O
.	O
The	O
second	O
round	O
of	O
experiments	O
done	O
on	O
another	O
variant	O
of	O
our	O
system	O
(	O
configuration	O
i	O
)	O
also	O
included	O
28	O
practice	O
tasks	O
and	O
compiled	O
10	O
final	O
workers	O
for	O
a	O
total	O
of	O
180	O
collected	O
sessions	O
.	O
Bonuses	O
ranged	O
from	O
$	O
0.10	O
and	O
$	O
0.40	O
on	O
the	O
session	O
collection	O
task	O
.	O
The	O
full	O
process	O
cost	O
of	O
the	O
second	O
experiment	O
was	O
about	O
$	O
475	O
in	O
total	O
(	O
including	O
the	O
Mechanical	O
Turk	O
fees	O
)	O
.	O
Session	O
collection	O
data	O
preparation	O
.	O
We	O
used	O
the	O
same	O
20	O
test	O
topics	O
as	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
,	O
and	O
created	O
2	O
batches	O
of	O
tasks	O
.	O
For	O
the	O
first	O
batch	O
,	O
in	O
alternating	O
order	O
of	O
topics	O
,	O
10	O
topics	O
were	O
paired	O
with	O
our	O
system	O
,	O
and	O
the	O
other	O
10	O
were	O
paired	O
with	O
the	O
S	O
2	O
baseline	O
.	O
The	O
other	O
batch	O
consisted	O
of	O
the	O
complementing	O
topic	O
-	O
system	O
pairings	O
.	O
The	O
workers	O
were	O
assigned	O
a	O
batch	O
to	O
work	O
on	O
such	O
that	O
half	O
of	O
the	O
workers	O
would	O
work	O
on	O
each	O
batch	O
.	O
User	O
informational	O
goal	O
.	O
Since	O
all	O
sessions	O
on	O
a	O
topic	O
are	O
evaluated	O
against	O
the	O
same	O
reference	O
summaries	O
,	O
it	O
is	O
important	O
that	O
users	O
aim	O
to	O
explore	O
similar	O
information	O
.	O
Following	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
,	O
during	O
practice	O
tasks	O
all	O
users	O
received	O
a	O
common	O
informational	O
goal	O
to	O
follow	O
,	O
so	O
that	O
the	O
sessions	O
are	O
comparable	O
.	O
The	O
emphasized	O
description	O
was	O
:	O
"	O
produce	O
an	O
informative	O
summary	O
draft	O
text	O
which	O
a	O
journalist	O
could	O
use	O
to	O
best	O
produce	O
an	O
overview	O
of	O
the	O
topic	O
"	O
.	O
Sessions	O
filtering	O
.	O
In	O
the	O
first	O
experiment	O
,	O
we	O
filtered	O
out	O
7	O
sessions	O
that	O
accumulated	O
less	O
than	O
250	O
tokens	O
(	O
from	O
2	O
different	O
workers	O
)	O
.	O
In	O
the	O
second	O
experiment	O
,	O
9	O
of	O
the	O
10	O
workers	O
completed	O
at	O
least	O
19	O
of	O
the	O
20	O
topics	O
One	O
worker	O
completed	O
only	O
3	O
tasks	O
and	O
we	O
disregarded	O
those	O
sessions	O
.	O
We	O
also	O
threw	O
away	O
9	O
sessions	O
that	O
accumulated	O
less	O
than	O
250	O
tokens	O
.	O
INTSUMM	O
user	O
interface	O
.	O
We	O
used	O
the	O
same	O
user	O
interface	O
developed	O
by	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
with	O
a	O
small	O
change	O
to	O
enable	O
suggested	O
query	O
list	O
updates	O
after	O
each	O
interaction	O
(	O
the	O
interface	O
was	O
designed	O
for	O
the	O
baselines	O
,	O
where	O
the	O
suggestedquery	O
list	O
is	O
static	O
)	O
.	O
To	O
refrain	O
from	O
any	O
possible	O
user	O
experience	O
bias	O
,	O
we	O
made	O
the	O
UI	O
change	O
as	O
least	O
apparent	O
as	O
possible	O
.	O
System	O
response	O
time	O
.	O
M	O
Summ	O
is	O
able	O
to	O
generate	O
summaries	O
mostly	O
in	O
under	O
a	O
second	O
,	O
and	O
M	O
Sugg	O
prepares	O
the	O
list	O
in	O
a	O
few	O
seconds	O
.	O
The	O
summary	O
expansion	O
is	O
hence	O
presented	O
to	O
the	O
user	O
almost	O
immediately	O
after	O
query	O
submission	O
,	O
and	O
the	O
suggested	O
queries	O
list	O
is	O
shown	O
shortly	O
afterwords	O
,	O
before	O
the	O
user	O
finishes	O
reading	O
the	O
expansion	O
.	O
The	O
small	O
delay	O
in	O
suggested	O
query	O
updating	O
is	O
hence	O
almost	O
unnoticed	O
.	O
The	O
baseline	O
summarizer	O
responds	O
similarly	O
fast	O
to	O
M	O
Summ	O
,	O
making	O
response	O
-	O
time	O
difference	O
unperceivable	O
between	O
the	O
systems	O
.	O
User	O
feedback	O
.	O
Many	O
of	O
the	O
users	O
provided	O
feedback	O
about	O
the	O
session	O
collection	O
tasks	O
after	O
finishing	O
their	O
assignment	O
batch	O
.	O
The	O
overall	O
impression	O
was	O
that	O
there	O
was	O
no	O
strong	O
preference	O
for	O
either	O
system	O
.	O
For	O
example	O
,	O
one	O
user	O
wrote	O
:	O
"	O
I	O
did	O
not	O
discern	O
a	O
consistent	O
difference	O
between	O
the	O
two	O
systems	O
that	O
would	O
result	O
in	O
having	O
a	O
clear	O
preference	O
.	O
"	O
This	O
kind	O
of	O
comment	O
was	O
repeated	O
by	O
several	O
users	O
.	O
Generally	O
,	O
there	O
were	O
no	O
explicit	O
comments	O
about	O
the	O
difference	O
in	O
quality	O
of	O
the	O
summary	O
outputs	O
,	O
and	O
topics	O
were	O
mostly	O
scored	O
or	O
commented	O
on	O
similarly	O
between	O
the	O
two	O
systems	O
since	O
the	O
complexity	O
of	O
the	O
topic	O
influenced	O
the	O
ability	O
of	O
the	O
systems	O
to	O
comply	O
to	O
the	O
user	O
.	O
A	O
comment	O
in	O
favor	O
of	O
updating	O
suggested	O
queries	O
during	O
interaction	O
said	O
:	O
"	O
It	O
was	O
nice	O
to	O
have	O
a	O
new	O
list	O
as	O
you	O
progressed	O
through	O
the	O
task	O
,	O
it	O
helped	O
me	O
think	O
of	O
where	O
to	O
go	O
next	O
if	O
I	O
got	O
stuck	O
...	O
"	O
This	O
specific	O
comment	O
was	O
written	O
by	O
a	O
user	O
that	O
explored	O
topics	O
quite	O
deeply	O
.	O
On	O
the	O
other	O
hand	O
,	O
a	O
user	O
that	O
explored	O
more	O
shallow	O
liked	O
that	O
used	O
suggested	O
queries	O
in	O
the	O
static	O
list	O
were	O
marked	O
:	O
"	O
I	O
did	O
notice	O
...	O
the	O
red	O
font	O
color	O
on	O
the	O
used	O
queries	O
.	O
That	O
was	O
helpful	O
.	O
"	O
It	O
therefore	O
seems	O
that	O
updating	O
suggested	O
queries	O
are	O
more	O
useful	O
for	O
lengthy	O
exploration	O
,	O
but	O
for	O
quick	O
navigation	O
,	O
the	O
static	O
list	O
might	O
naturally	O
be	O
enough	O
.	O
We	O
conducted	O
two	O
comparative	O
session	O
collection	O
and	O
analysis	O
experiments	O
,	O
one	O
using	O
M	O
Summ	O
model	O
configuration	O
v	O
(	O
from	O
Table	O
1	O
)	O
,	O
as	O
presented	O
in	O
5.3	O
and	O
Table	O
2	O
,	O
and	O
another	O
with	O
M	O
Summ	O
model	O
configuration	O
i.	O
As	O
explained	O
in	O
5.2	O
,	O
these	O
two	O
configurations	O
performed	O
best	O
,	O
on	O
simulations	O
,	O
out	O
of	O
their	O
respective	O
configuration	O
sets	O
.	O
We	O
show	O
here	O
results	O
of	O
the	O
second	O
experiment	O
,	O
where	O
we	O
used	O
M	O
Summ	O
model	O
configuration	O
i	O
,	O
with	O
the	O
same	O
M	O
Sugg	O
model	O
as	O
in	O
the	O
first	O
experiment	O
.	O
The	O
S	O
2	O
baseline	O
was	O
similarly	O
used	O
for	O
comparison	O
.	O
We	O
also	O
kept	O
the	O
same	O
AUC	B-MetricName
length	O
limits	O
(	O
106	O
to	O
250	O
tokens	O
)	O
for	O
easy	O
comparability	O
to	O
Table	O
2	O
.	O
Table	O
3	O
shows	O
the	O
results	O
.	O
Here	O
too	O
,	O
while	O
less	O
substantially	O
,	O
informativeness	O
is	O
improved	O
with	O
our	O
system	O
without	O
significantly	O
harming	O
the	O
user	O
experience	O
.	O
Overall	O
,	O
it	O
seems	O
that	O
users	O
were	O
somewhat	O
more	O
satisfied	O
with	O
the	O
INTSUMM	O
system	O
that	O
uses	O
M	O
Summ	O
configuration	O
v	O
than	O
configuration	O
i.	O
Interestingly	O
,	O
it	O
seems	O
the	O
users	O
may	O
have	O
appreciated	O
the	O
slightly	O
better	O
informativeness	O
of	O
configuration	O
v	O
even	O
if	O
the	O
query	O
-	O
responsiveness	O
was	O
not	O
as	O
good	O
as	O
in	O
configuration	O
i	O
,	O
as	O
shown	O
through	O
the	O
QSIM	O
score	O
.	O
In	O
addition	O
,	O
we	O
see	O
that	O
absolute	O
manual	O
scores	O
in	O
Table	O
3	O
are	O
lower	O
than	O
in	O
Table	O
2	O
,	O
but	O
trends	O
are	O
generally	O
similar	O
.	O
It	O
is	O
common	O
that	O
scaling	O
of	O
manually	O
supplied	O
scores	O
can	O
fluctuate	O
(	O
e.g.	O
Gillick	O
and	O
Liu	O
,	O
2010	O
)	O
.	O
Figures	O
3	O
and	O
4	O
show	O
the	O
averaged	O
(	O
per	O
topic	O
and	O
then	O
over	O
all	O
topics	O
)	O
recall	O
curves	O
of	O
the	O
collected	O
sessions	O
in	O
the	O
experiment	O
described	O
in	O
5.3	O
and	O
above	O
,	O
respectively	O
.	O
The	O
x	O
-	O
axis	O
is	O
the	O
accumulating	O
token	O
-	O
length	O
of	O
the	O
session	O
,	O
and	O
the	O
y	O
-	O
axis	O
is	O
the	O
ROUGE	O
-	O
1	O
recall	O
.	O
The	O
points	O
on	O
the	O
curve	O
are	O
the	O
average	O
interpolated	O
values	O
from	O
all	O
the	O
sessions	O
.	O
The	O
vertical	O
dashed	O
lines	O
are	O
the	O
intersecting	O
bounds	O
of	O
the	O
sessions	O
,	O
from	O
106	O
tokens	O
to	O
250	O
.	O
The	O
area	O
under	O
the	O
curve	O
(	O
AUC	B-MetricName
)	O
is	O
computed	O
for	O
each	O
of	O
the	O
curves	O
,	O
and	O
reported	O
in	O
the	O
first	O
row	O
of	O
Tables	O
2	O
and	O
3	O
.	O
The	O
higher	O
AUC	B-MetricName
scores	O
obtained	O
from	O
the	O
recall	O
curves	O
of	O
our	O
models	O
,	O
compared	O
to	O
those	O
of	O
the	O
S	O
2	O
baseline	O
,	O
highlight	O
the	O
ability	O
to	O
expose	O
more	O
salient	O
information	O
earlier	O
in	O
the	O
session	O
.	O

The	O
normalized	O
AUC	B-MetricName
score	O
for	O
the	O
validation	O
metric	O
(	O
explained	O
in	O
3.3	O
)	O
is	O
computed	O
over	O
the	O
recall	O
curve	O
produced	O
from	O
the	O
accumulating	O
summary	O
expansions	O
.	O
Each	O
point	O
on	O
the	O
curve	O
marks	O
an	O
accumulating	O
token	O
-	O
length	O
(	O
x	O
-	O
axis	O
)	O
and	O
an	O
accumulating	O
recall	O
score	O
(	O
y	O
-	O
axis	O
)	O
of	O
an	O
interactive	O
state	O
,	O
as	O
depicted	O
in	O
Figures	O
3	O
and	O
4	O
(	O
although	O
these	O
figures	O
show	O
the	O
averaged	O
session	O
recall	O
curves	O
with	O
bounds	O
,	O
whereas	O
during	O
validation	O
the	O
curve	O
is	O
for	O
a	O
single	O
session	O
and	O
there	O
are	O
no	O
bounds	O
set	O
)	O
.	O
By	O
computing	O
the	O
area	O
under	O
the	O
full	O
curve	O
,	O
and	O
dividing	O
by	O
the	O
full	O
length	O
,	O
the	O
normalized	O
AUC	B-MetricName
score	O
is	O
obtained	O
.	O
The	O
normalization	O
gives	O
an	O
approximate	O
absolute	O
value	O
that	O
can	O
be	O
compared	O
at	O
different	O
lengths	O
(	O
although	O
at	O
large	O
length	O
differences	O
this	O
is	O
not	O
comparable	O
due	O
to	O
the	O
decaying	O
slope	O
of	O
the	O
curve	O
)	O
.	O
The	O
manual	O
query	O
-	O
responsiveness	O
score	O
,	O
reported	O
in	O
Tables	O
2	O
and	O
3	O
,	O
is	O
obtained	O
by	O
asking	O
users	O
,	O
at	O
the	O
end	O
of	O
a	O
session	O
,	O
"	O
During	O
the	O
interactive	O
stage	O
,	O
how	O
well	O
did	O
the	O
responses	O
respond	O
to	O
your	O
queries	O
?	O
"	O
,	O
for	O
which	O
they	O
rate	O
on	O
a	O
1	O
-	O
to	O
-	O
5	O
scale	O
.	O
The	O
scores	O
are	O
averaged	O
over	O
the	O
topic	O
and	O
then	O
over	O
all	O
topics	O
.	O
This	O
follows	O
the	O
evaluation	O
defined	O
in	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
.	O
The	O
UMUX	O
-	O
Lite	O
score	O
(	O
Lewis	O
et	O
al	O
,	O
2013	O
)	O
,	O
reported	O
in	O
Tables	O
2	O
and	O
3	O
,	O
is	O
obtained	O
by	O
asking	O
users	O
to	O
rate	O
(	O
1	O
-	O
to	O
-	O
5	O
)	O
two	O
statements	O
at	O
the	O
end	O
of	O
a	O
session	O
:	O
(	O
1	O
)	O
"	O
The	O
system	O
's	O
capabilities	O
meet	O
the	O
need	O
to	O
efficiently	O
collect	O
useful	O
information	O
for	O
a	O
journalistic	O
overview	O
"	O
and	O
(	O
2	O
)	O
"	O
The	O
system	O
is	O
easy	O
to	O
use	O
"	O
.	O
The	O
first	O
question	O
refers	O
to	O
the	O
users	O
'	O
informational	O
goal	O
that	O
they	O
received	O
,	O
in	O
order	O
to	O
follow	O
a	O
consistent	O
objective	O
goal	O
during	O
their	O
exploration	O
.	O
The	O
final	O
score	O
is	O
a	O
function	O
of	O
these	O
two	O
scores	O
,	O
and	O
is	O
used	O
as	O
a	O
replacement	O
for	O
the	O
popular	O
SUS	O
metric	O
(	O
Brooke	O
,	O
1996	O
)	O
(	O
with	O
a	O
much	O
longer	O
questionnaire	O
)	O
,	O
to	O
which	O
it	O
shows	O
very	O
high	O
correlation	O
,	O
thus	O
offering	O
a	O
cheaper	O
alternative	O
.	O
This	O
also	O
follows	O
the	O
evaluation	O
defined	O
in	O
Shapira	O
et	O
al	O
(	O
2021b	O
)	O
.	O
All	O
confidence	O
intervals	O
in	O
Tables	O
1	O
,	O
2	O
and	O
3	O
are	O
computed	O
as	O
margins	O
-	O
of	O
-	O
error	O
,	O
on	O
the	O
topiclevel	O
,	O
over	O
the	O
standard	O
error	O
of	O
the	O
mean	O
with	O
95	O
%	O
confidence	O
.	O
10	O
The	O
token	O
-	O
length	O
values	O
in	O

A	O
policy	O
gradient	O
-	O
based	O
reinforcement	O
learning	O
approach	O
(	O
Williams	O
,	O
1992	O
)	O
allows	O
optimizing	O
on	O
nondifferentiable	O
metrics	O
,	O
and	O
eliminates	O
the	O
exposure	O
bias	O
that	O
occurs	O
with	O
traditional	O
training	O
methods	O
,	O
like	O
cross	O
-	O
entropy	O
,	O
on	O
generation	O
tasks	O
(	O
Ranzato	O
et	O
al	O
,	O
2016	O
)	O
.	O
Specifically	O
,	O
we	O
use	O
the	O
Advantage	O
Actor	O
Critic	O
(	O
A2C	B-MethodName
)	O
policy	O
gradient	O
training	O
method	O
.	O
See	O
technical	O
explanations	O
in	O
the	O
appendix	O
of	O
(	O
Chen	O
and	O
Bansal	O
,	O
2018	O
)	O
.	O
At	O
a	O
high	O
level	O
,	O
an	O
output	O
reward	O
(	O
subtracted	O
by	O
a	O
baseline	O
reward	O
-	O
computed	O
on	O
a	O
version	O
of	O
the	O
model	O
without	O
MMR	O
attention	O
)	O
is	O
used	O
to	O
weight	O
the	O
output	O
selection	O
in	O
the	O
loss	B-MetricName
function	O
.	O
In	O
so	O
,	O
outputs	O
with	O
higher	O
rewards	O
increase	O
the	O
likelihood	O
of	O
those	O
outputs	O
and	O
lower	O
rewards	O
decrease	O
the	O
likelihood	O
.	O
Since	O
the	O
reward	O
function	O
is	O
not	O
differentiable	O
,	O
it	O
is	O
used	O
as	O
a	O
weight	O
on	O
the	O
probability	O
of	O
the	O
selected	O
output	O
,	O
which	O
is	O
then	O
given	O
to	O
the	O
loss	B-MetricName
function	O
.	O

As	O
a	O
pilot	O
study	O
,	O
we	O
consider	O
the	O
task	O
of	O
binary	O
text	B-TaskName
classification	I-TaskName
.	O
The	O
training	O
set	O
is	O
denoted	O
as	O
D	O
train	O
=	O
{	O
(	O
x	O
1	O
,	O
l	O
1	O
)	O
,	O
...	O
,	O
(	O
x	O
n	O
,	O
l	O
n	O
)	O
}	O
,	O
where	O
x	O
i	O
is	O
the	O
i	O
-	O
th	O
example	O
and	O
l	O
i	O
{	O
0	B-DatasetName
,	O
1	O
}	O
is	O
the	O
corresponding	O
label	O
.	O
We	O
fit	O
a	O
model	O
f	O
∶	O
(	O
x	O
;	O
θ	B-HyperparameterName
)	O
↦	O
{	O
0	B-DatasetName
,	O
1	O
}	O
with	O
parameters	O
θ	B-HyperparameterName
on	O
the	O
training	O
data	O
.	O
A	O
textual	O
perturbation	O
is	O
a	O
transformation	O
g	O
∶	O
(	O
x	O
;	O
β	B-HyperparameterName
)	O
x	O
*	O
that	O
injects	O
a	O
specific	O
type	O
of	O
noise	O
into	O
an	O
example	O
x	O
with	O
parameters	O
β	B-HyperparameterName
and	O
the	O
resulting	O
perturbed	O
example	O
is	O
x	O
*	O
.	O
We	O
design	O
several	O
experiment	O
settings	O
(	O
Table	O
1	O
)	O
to	O
answer	O
our	O
research	O
questions	O
.	O
Experiment	O
0	B-DatasetName
in	O
Table	O
1	O
is	O
the	O
standard	O
learning	O
setup	O
,	O
where	O
we	O
train	O
and	O
evaluate	O
a	O
model	O
on	O
the	O
original	O
dataset	O
.	O
Below	O
we	O
detail	O
other	O
experiment	O
settings	O
.	O

Robustness	O
.	O
We	O
apply	O
the	O
perturbations	O
to	O
test	O
examples	O
and	O
measure	O
the	O
robustness	O
of	O
model	O
to	O
said	O
perturbations	O
as	O
the	O
decrease	O
in	O
accuracy	B-MetricName
.	O
In	O
Table	O
1	O
,	O
Experiment	O
1	O
is	O
related	O
to	O
robustness	O
measurement	O
,	O
where	O
we	O
train	O
a	O
model	O
on	O
unperturbed	O
dataset	O
and	O
test	O
it	O
on	O
perturbed	O
examples	O
.	O
We	O
denote	O
the	O
test	O
accuracy	B-MetricName
of	O
a	O
model	O
f	O
(	O
⋅	O
)	O
on	O
examples	O
perturbed	O
by	O
g	O
(	O
⋅	O
)	O
in	O
Experiment	O
1	O
as	O
A	O
1	O
(	O
f	O
,	O
g	O
,	O
D	O
*	O
test	O
)	O
.	O
Similarly	O
,	O
the	O
test	O
accuracy	B-MetricName
in	O
Experiment	O
0	B-DatasetName
is	O
A	O
0	B-DatasetName
(	O
f	O
,	O
D	O
test	O
)	O
.	O
Consequently	O
,	O
the	O
robustness	O
is	O
calculated	O
as	O
the	O
difference	O
of	O
test	O
accuracies	O
:	O
robustness	O
(	O
f	O
,	O
g	O
,	O
D	O
)	O
=	O
A	O
1	O
(	O
f	O
,	O
g	O
,	O
D	O
*	O
test	O
)	O
−A	O
0	B-DatasetName
(	O
f	O
,	O
D	O
test	O
)	O
.	O
(	O
1	O
)	O
Models	O
usually	O
suffer	O
a	O
performance	O
drop	O
when	O
encountering	O
perturbations	O
,	O
therefore	O
the	O
robustness	O
is	O
usually	O
negative	O
,	O
where	O
lower	O
values	O
indicate	O
decreased	O
robustness	O
.	O
Improvement	O
by	O
Data	B-TaskName
Augmentation	I-TaskName
(	O
Post	O
Augmentation	O
∆	O
)	O
.	O
To	O
improve	O
robust	O
accuracy	B-MetricName
(	O
Tu	O
et	O
al	O
,	O
2020	O
)	O
(	O
i.e.	O
,	O
accuracy	B-MetricName
on	O
the	O
perturbed	O
test	O
set	O
)	O
,	O
it	O
is	O
a	O
common	O
practice	O
to	O
leverage	O
data	B-TaskName
augmentation	I-TaskName
(	O
Li	O
and	O
Specia	O
,	O
2019	O
;	O
Min	O
et	O
al	O
,	O
2020	O
;	O
Tan	O
and	O
Joty	O
,	O
2021	O
)	O
.	O
We	O
simulate	O
the	O
data	B-TaskName
augmentation	I-TaskName
process	O
by	O
appending	O
perturbed	O
data	O
to	O
the	O
training	O
set	O
(	O
Experiment	O
2	O
of	O
Table	O
1	O
)	O
.	O
We	O
calculate	O
the	O
improvement	O
on	O
performance	O
after	O
data	B-TaskName
augmentation	I-TaskName
as	O
the	O
difference	O
of	O
test	O
accuracies	O
:	O
∆	O
post_aug	O
(	O
f	O
,	O
g	O
,	O
D	O
)	O
=	O
A	O
2	O
(	O
f	O
,	O
g	O
,	O
D	O
*	O
test	O
)	O
−A	O
1	O
(	O
f	O
,	O
g	O
,	O
D	O
*	O
test	O
)	O
.	O
(	O
)	O
2	O
where	O
A	O
2	O
(	O
f	O
,	O
g	O
,	O
D	O
*	O
test	O
)	O
denotes	O
the	O
test	O
accuracy	B-MetricName
of	O
Experiment	O
2	O
.	O
∆	O
post_aug	O
is	O
the	O
higher	O
the	O
better	O
.	O
Learnability	O
.	O
We	O
want	O
to	O
compare	O
perturbations	O
in	O
terms	O
of	O
how	O
well	O
the	O
model	O
learns	O
to	O
identify	O
them	O
with	O
a	O
small	O
amount	O
of	O
evidence	O
.	O
We	O
cast	O
learnability	O
estimation	O
as	O
a	O
perturbation	O
classification	O
task	O
,	O
where	O
a	O
model	O
is	O
trained	O
to	O
identify	O
the	O
perturbation	O
in	O
an	O
example	O
.	O
We	O
define	O
that	O
the	O
learnability	O
estimation	O
consists	O
of	O
three	O
steps	O
,	O
namely	O
①	O
assigning	O
random	O
labels	O
,	O
②	O
perturbing	O
with	O
probabilities	O
,	O
and	O
③	O
estimating	O
model	O
performance	O
.	O
Below	O
we	O
introduce	O
the	O
procedure	O
and	O
intuition	O
for	O
each	O
step	O
.	O
This	O
estimation	O
framework	O
is	O
further	O
grounded	O
in	O
concepts	O
from	O
the	O
causality	O
literature	O
in	O
Section	O
3	O
,	O
which	O
justifies	O
our	O
motivations	O
.	O
We	O
summarize	O
our	O
estimation	O
approach	O
formally	O
in	O
Algorithm	O
1	O
(	O
Appendix	O
A	O
)	O
.	O
①	O
Assigning	O
Random	O
Labels	O
.	O
We	O
randomly	O
assign	O
pseudo	O
labels	O
to	O
each	O
training	O
example	O
regardless	O
of	O
its	O
original	O
label	O
.	O
Each	O
data	O
point	O
has	O
equal	O
probability	O
of	O
being	O
assigned	O
to	O
positive	O
(	O
l	O
′	O
=	O
1	O
)	O
or	O
negative	O
(	O
l	O
′	O
=	O
0	B-DatasetName
)	O
pseudo	O
label	O
.	O
This	O
results	O
in	O
a	O
randomly	O
labeled	O
dataset	O
D	O
′	O
train	O
=	O
{	O
(	O
x	O
1	O
;	O
l	O
′	O
1	O
)	O
,	O
...	O
,	O
(	O
x	O
n	O
,	O
l	O
′	O
n	O
)	O
}	O
,	O
where	O
L	O
′	O
∼	O
Bernoulli	O
(	O
1	O
,	O
0.5	O
)	O
.	O
In	O
this	O
way	O
,	O
we	O
ensure	O
that	O
there	O
is	O
no	O
difference	O
between	O
the	O
two	O
pseudo	O
groups	O
since	O
the	O
data	O
are	O
randomly	O
split	O
.	O
②	O
Perturbing	O
with	O
Probabilities	O
.	O
We	O
apply	O
the	O
perturbation	O
g	O
(	O
⋅	O
)	O
to	O
each	O
training	O
example	O
in	O
one	O
of	O
the	O
pseudo	O
groups	O
(	O
e.g.	O
,	O
l	O
′	O
=	O
1	O
in	O
Algorithm	O
1	O
)	O
1	O
.	O
In	O
this	O
way	O
,	O
we	O
create	O
a	O
correlation	O
between	O
the	O
existence	O
of	O
perturbation	O
and	O
label	O
(	O
i.e.	O
,	O
the	O
perturbation	O
occurrence	O
is	O
predictive	O
of	O
the	O
label	O
)	O
.	O
We	O
control	O
the	O
perturbation	O
probability	O
p	O
[	O
0	B-DatasetName
,	O
1	O
]	O
,	O
i.e.	O
,	O
an	O
example	O
has	O
a	O
specific	O
probability	O
p	O
of	O
being	O
perturbed	O
.	O
This	O
results	O
in	O
a	O
perturbed	O
training	O
set	O
D	O
′	O
*	O
train	O
=	O
{	O
(	O
x	O
*	O
1	O
,	O
l	O
′	O
1	O
)	O
,	O
...	O
,	O
(	O
x	O
*	O
n	O
,	O
l	O
′	O
n	O
)	O
}	O
,	O
where	O
the	O
perturbed	O
example	O
x	O
*	O
i	O
is	O
:	O
Z	O
∼	O
U	O
(	O
0	B-DatasetName
,	O
1	O
)	O
,	O
∀i	O
{	O
1	O
,	O
2	O
,	O
...	O
,	O
n	O
}	O
x	O
*	O
i	O
=	O
⎧	O
⎪	O
⎪	O
⎨	O
⎪	O
⎪	O
⎩	O
g	O
(	O
x	O
i	O
)	O
l	O
′	O
i	O
=	O
1	O
z	O
<	O
p	O
,	O
x	O
i	O
otherwise	O
.	O
(	O
)	O
3	O
Here	O
Z	O
is	O
a	O
random	O
variable	O
drawn	O
from	O
a	O
uniform	O
distribution	O
U	O
(	O
0	B-DatasetName
,	O
1	O
)	O
.	O
Due	O
to	O
randomization	O
in	O
the	O
formal	O
step	O
,	O
now	O
the	O
only	O
difference	O
between	O
the	O
two	O
pseudo	O
groups	O
is	O
the	O
occurrence	O
of	O
perturbation	O
.	O
③	O
Estimating	O
Model	O
Performance	O
.	O
We	O
train	O
a	O
model	O
on	O
the	O
randomly	O
labeled	O
dataset	O
with	O
per	O
-	O
1	O
Because	O
the	O
training	O
data	O
is	O
randomly	O
split	O
into	O
two	O
pseudo	O
groups	O
,	O
applying	O
perturbations	O
to	O
any	O
one	O
of	O
the	O
groups	O
should	O
yield	O
same	O
result	O
.	O
We	O
assume	O
that	O
we	O
always	O
perturb	O
into	O
the	O
first	O
group	O
(	O
l	O
′	O
=	O
1	O
)	O
hereafter	O
.	O
turbed	O
examples	O
.	O
Since	O
the	O
only	O
difference	O
between	O
the	O
two	O
pseudo	O
groups	O
is	O
the	O
existence	O
of	O
the	O
perturbation	O
,	O
the	O
model	O
is	O
trained	O
to	O
identify	O
the	O
perturbation	O
.	O
The	O
original	O
test	O
examples	O
D	O
test	O
are	O
also	O
assigned	O
random	O
labels	O
and	O
become	O
D	O
′	O
test	O
.	O
We	O
perturb	O
all	O
of	O
the	O
test	O
examples	O
in	O
one	O
pseudo	O
group	O
(	O
e.g.	O
,	O
l	O
′	O
=	O
1	O
,	O
as	O
in	O
step	O
2.1	O
)	O
to	O
produce	O
a	O
perturbed	O
test	O
set	O
D	O
′	O
*	O
test	O
.	O
Finally	O
,	O
the	O
perturbation	O
learnability	O
is	O
calculated	O
as	O
the	O
difference	O
of	O
accuracies	O
on	O
D	O
′	O
*	O
test	O
and	O
D	O
′	O
test	O
,	O
which	O
indicates	O
how	O
much	O
the	O
model	O
learns	O
from	O
the	O
perturbation	O
's	O
co	O
-	O
occurrence	O
with	O
pseudo	O
label	O
:	O
learnability	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
)	O
=	O
A	O
3	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
*	O
test	O
)	O
−A	O
4	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
test	O
)	O
.	O
(	O
4	O
)	O
A	O
4	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
*	O
test	O
)	O
and	O
A	O
3	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
test	O
)	O
are	O
accuracies	O
measured	O
by	O
Experiment	O
4	O
and	O
3	O
of	O
Table	O
1	O
,	O
respectively	O
.	O
We	O
observe	O
that	O
the	O
learnability	O
depends	O
on	O
perturbation	O
probability	O
p.	O
For	O
each	O
modelperturbation	O
pair	O
,	O
we	O
obtain	O
multiple	O
learnability	O
estimates	O
by	O
varying	O
the	O
perturbation	O
probability	O
(	O
Figure	O
3	O
)	O
.	O
However	O
,	O
we	O
expect	O
that	O
learnability	O
of	O
the	O
perturbation	O
(	O
as	O
a	O
concept	O
)	O
should	O
be	O
independent	O
of	O
perturbation	O
probability	O
.	O
To	O
this	O
end	O
,	O
we	O
use	O
the	O
log	O
AU	O
C	O
(	O
area	O
under	O
the	O
curve	O
in	O
log	O
scale	O
)	O
of	O
the	O
p	O
−	O
learnability	O
curve	O
(	O
Figure	O
3	O
)	O
,	O
termed	O
as	O
"	O
average	O
learnability	O
"	O
,	O
which	O
summarizes	O
the	O
overall	O
learnability	O
across	O
different	O
perturbation	O
probabilities	O
p	O
1	O
,	O
...	O
,	O
p	O
t	O
:	O
avg_learnability	O
(	O
f	O
,	O
g	O
,	O
D	O
)	O
∶=	O
log	O
AU	O
C	O
(	O
{	O
(	O
p	O
i	O
,	O
learnability	O
(	O
f	O
,	O
g	O
,	O
p	O
i	O
,	O
D	O
)	O
)	O
|	O
i	O
{	O
1	O
,	O
2	O
,	O
...	O
,	O
t	O
}	O
}	O
)	O
.	O
(	O
5	O
)	O
We	O
use	O
log	O
AU	O
C	O
rather	O
than	O
AU	O
C	O
because	O
we	O
empirically	O
find	O
that	O
the	O
learnability	O
varies	O
substantially	O
between	O
perturbations	O
when	O
p	O
is	O
small	O
,	O
and	O
a	O
log	O
scale	O
can	O
better	O
capture	O
this	O
nuance	O
.	O
We	O
also	O
introduce	O
learnability	O
at	O
a	O
specific	O
perturbation	O
probability	O
(	O
Learnability	O
@	O
p	O
)	O
as	O
an	O
alternate	O
summary	O
metric	O
and	O
provide	O
a	O
comparison	O
of	O
this	O
metric	O
against	O
log	O
AU	O
C	O
in	O
Appendix	O
D.	O

We	O
identify	O
learnability	O
as	O
a	O
causal	O
estimand	O
.	O
In	O
causality	O
,	O
the	O
term	O
"	O
identification	O
"	O
refers	O
to	O
the	O
process	O
of	O
moving	O
from	O
a	O
causal	O
estimand	O
(	O
Average	O
Treatment	O
Effect	O
,	O
ATE	O
)	O
to	O
an	O
equivalent	O
statistical	O
estimand	O
.	O
We	O
show	O
that	O
the	O
difference	O
of	O
accuracies	O
on	O
D	O
′	O
*	O
test	O
and	O
D	O
′	O
test	O
is	O
actually	O
a	O
causal	O
estimand	O
.	O
We	O
define	O
the	O
outcome	O
Y	O
of	O
a	O
test	O
example	O
x	O
i	O
as	O
the	O
correctness	O
of	O
the	O
predicted	O
label	O
:	O
Y	O
i	O
(	O
0	B-DatasetName
)	O
∶=	O
1	O
{	O
f	O
(	O
x	O
i	O
)	O
=	O
l	O
′	O
i	O
}	O
.	O
(	O
6	O
)	O
where	O
1	O
{	O
⋅	O
}	O
is	O
the	O
indicator	O
function	O
.	O
Similarly	O
,	O
the	O
outcome	O
Y	O
of	O
a	O
perturbed	O
test	O
example	O
x	O
*	O
i	O
is	O
:	O
Y	O
i	O
(	O
1	O
)	O
∶=	O
1	O
{	O
f	O
(	O
x	O
*	O
i	O
)	O
=	O
l	O
′	O
i	O
}	O
.	O
(	O
7	O
)	O
According	O
to	O
the	O
definition	O
of	O
Individual	O
Treatment	O
Effect	O
(	O
ITE	O
,	O
see	O
Equation	O
9of	O
Appendix	O
B	O
)	O
,	O
we	O
have	O
IT	O
E	O
i	O
=	O
1	O
{	O
f	O
(	O
x	O
*	O
i	O
)	O
=	O
l	O
′	O
i	O
}	O
−1	O
{	O
f	O
(	O
x	O
i	O
)	O
=	O
l	O
′	O
i	O
}	O
.	O
We	O
then	O
take	O
the	O
average	O
over	O
all	O
the	O
perturbed	O
test	O
examples	O
(	O
half	O
of	O
the	O
test	O
set	O
)	O
3	O
.	O
This	O
is	O
our	O
Average	O
Treatment	O
Effect	O
(	O
ATE	O
)	O
:	O
AT	O
E	O
=	O
E	O
[	O
Y	O
(	O
1	O
)	O
]	O
−	O
E	O
[	O
Y	O
(	O
0	B-DatasetName
)	O
]	O
=	O
E	O
[	O
1	O
{	O
f	O
(	O
x	O
*	O
)	O
=	O
l	O
′	O
}	O
]	O
−	O
E	O
[	O
1	O
{	O
f	O
(	O
x	O
)	O
=	O
l	O
′	O
}	O
]	O
=	O
P	O
(	O
f	O
(	O
x	O
*	O
)	O
=	O
l	O
′	O
)	O
−	O
P	O
(	O
f	O
(	O
x	O
)	O
=	O
l	O
′	O
)	O
=	O
A	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
*	O
test	O
)	O
−	O
A	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
test	O
)	O
.	O
(	O
8	O
)	O
Perturbation	O
Example	O
Sentence	O
None	O
His	O
quiet	O
and	O
straightforward	O
demeanor	O
was	O
rare	O
then	O
and	O
would	O
be	O
today	O
.	O
duplicate_punctuations	O
His	O
quiet	O
and	O
straightforward	O
demeanor	O
was	O
rare	O
then	O
and	O
would	O
be	O
today	O
..	O
butter_fingers_perturbation	O
His	O
quiet	O
and	O
straightforward	O
demeanor	O
was	O
rarw	O
then	O
and	O
would	O
be	O
today	O
.	O
shuffle_word	O
quiet	O
would	O
and	O
was	O
be	O
and	O
straightforward	O
then	O
demeanor	O
His	O
today	O
.	O
rare	O
random_upper_transformation	O
His	O
quiEt	O
and	O
straightForwARd	O
Demeanor	O
was	O
rare	O
TheN	O
and	O
would	O
be	O
today	O
.	O
insert_abbreviation	O
His	O
quiet	O
and	O
straightforward	O
demeanor	O
wuz	O
rare	O
then	O
and	O
would	O
b	O
today	O
.	O
whitespace_perturbation	O
His	O
quiet	O
and	O
straightforward	O
demean	O
or	O
wa	O
s	O
rare	O
thenand	O
would	O
be	O
today	O
.	O
visual_attack_letters	O
Hiṩ	O
qủiẽt	O
ầռd	O
strḁighṭḟorwẳrȡ	O
dԑmeanoŕ	O
wȃṣ	O
rȧre	O
tḫen	O
and	O
wouᶅd	O
ϸә	O
tອḏầȳ	O
.	O
leet_letters	O
His	O
qui3	O
t	O
and	O
strai9htfor3ard	O
d3m3an0r	O
3as	O
rar3	O
t43n	O
and	O
30uld	O
63	O
t0da4	O
.	O
where	O
A	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
)	O
is	O
the	O
accuracy	B-MetricName
of	O
model	O
f	O
(	O
⋅	O
)	O
trained	O
with	O
perturbation	O
g	O
(	O
⋅	O
)	O
at	O
perturbation	O
probability	O
p	O
on	O
test	O
set	O
D.	O
Therefore	O
,	O
we	O
show	O
that	O
ATE	O
is	O
exactly	O
the	O
difference	O
of	O
accuracy	B-MetricName
on	O
the	O
perturbed	O
and	O
unperturbed	O
test	O
sets	O
with	O
random	O
labels	O
.	O
And	O
the	O
difference	O
is	O
learnability	O
according	O
to	O
Equation	O
4	O
.	O
We	O
discuss	O
another	O
means	O
of	O
identification	O
of	O
ATE	O
in	O
Appendix	O
C	O
,	O
based	O
on	O
the	O
prediction	O
probability	O
.	O
We	O
compare	O
between	O
the	O
probability	O
-	O
based	O
and	O
accuracy	B-MetricName
-	O
based	O
metrics	O
there	O
.	O
We	O
find	O
that	O
our	O
accuracy	B-MetricName
-	O
based	O
metric	O
yields	O
better	O
resolution	O
,	O
so	O
we	O
report	O
this	O
metric	O
in	O
the	O
main	O
text	O
of	O
this	O
paper	O
.	O

Potential	O
Impacts	O
.	O
Our	O
findings	O
seem	O
intuitive	O
but	O
are	O
non	O
-	O
trivial	O
.	O
The	O
NLP	O
models	O
were	O
not	O
trained	O
on	O
perturbed	O
examples	O
when	O
measuring	O
robustness	O
,	O
but	O
still	O
they	O
display	O
a	O
strong	O
correlation	O
with	O
perturbation	O
learnability	O
.	O
Understanding	O
these	O
findings	O
are	O
important	O
for	O
a	O
more	O
principled	O
evaluation	O
of	O
and	O
control	O
over	O
NLP	O
models	O
.	O
Specifically	O
,	O
the	O
learnability	O
metric	O
complements	O
to	O
the	O
evaluation	O
of	O
newly	O
designed	O
perturbations	O
by	O
revealing	O
model	O
weaknesses	O
in	O
a	O
clean	O
setup	O
.	O
Reducing	O
perturbation	O
learnability	O
is	O
promising	O
for	O
improving	O
robustness	O
of	O
models	O
.	O
Contrastive	B-MethodName
learning	I-MethodName
(	O
Gao	O
et	O
al	O
,	O
2021	O
;	O
Yan	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2021	O
)	O
that	O
pulls	O
the	O
representations	O
of	O
the	O
original	O
and	O
perturbed	O
text	O
together	O
,	O
makes	O
it	O
difficult	O
for	O
the	O
model	O
to	O
identify	O
the	O
perturbation	O
(	O
reducing	O
learnability	O
)	O
and	O
thus	O
may	O
help	O
improve	O
robustness	O
.	O
Perturbation	O
can	O
also	O
be	O
viewed	O
as	O
injecting	O
spurious	O
feature	O
into	O
the	O
examples	O
,	O
so	O
the	O
learnability	O
metric	O
also	O
helps	O
to	O
interpret	O
robustness	O
to	O
spurious	O
correlation	O
(	O
Sagawa	O
et	O
al	O
,	O
2020	O
)	O
.	O
Moreover	O
,	O
learnability	O
may	O
facilitate	O
the	O
development	O
of	O
model	O
architectures	O
with	O
explicit	O
inductive	O
biases	O
(	O
Warstadt	O
and	O
Bowman	O
,	O
2020	O
;	O
to	O
avoid	O
sensitivity	O
to	O
noisy	O
perturbations	O
.	O
Grounding	O
the	O
learnability	O
within	O
the	O
causality	O
framework	O
inspires	O
future	O
researchers	O
to	O
incorporate	O
the	O
causal	O
perspective	O
into	O
model	O
design	O
(	O
Zhang	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
make	O
the	O
model	O
robust	O
to	O
different	O
types	O
of	O
perturbations	O
.	O
Limitations	O
.	O
In	O
this	O
work	O
,	O
we	O
focus	O
on	O
the	O
robust	O
accuracy	B-MetricName
(	O
Section	O
2.1	O
)	O
,	O
which	O
is	O
accuracy	B-MetricName
on	O
the	O
perturbed	O
test	O
set	O
.	O
We	O
do	O
not	O
assume	O
that	O
the	O
test	O
accuracy	B-MetricName
of	O
the	O
original	O
test	O
set	O
,	O
a.k.a	O
in	O
-	O
distribution	O
accuracy	B-MetricName
,	O
is	O
invariant	O
invariant	O
against	O
training	O
with	O
augmentation	O
or	O
not	O
.	O
It	O
would	O
be	O
interesting	O
to	O
investigate	O
the	O
trade	O
-	O
off	O
between	O
robust	O
accuracy	B-MetricName
and	O
in	O
-	O
distribution	O
accuracy	B-MetricName
in	O
the	O
future	O
.	O
We	O
also	O
note	O
that	O
this	O
work	O
has	O
not	O
established	O
that	O
the	O
relationship	O
between	O
learnability	O
and	O
robustness	O
is	O
causal	O
.	O
This	O
could	O
be	O
explored	O
with	O
other	O
approaches	O
in	O
causal	B-MethodName
inference	I-MethodName
for	O
deconfounding	O
besides	O
simulation	O
on	O
randomized	O
control	O
trial	O
,	O
such	O
as	O
working	O
with	O
real	O
data	O
but	O
stratifying	O
it	O
(	O
Frangakis	O
and	O
Rubin	O
,	O
2002	O
)	O
,	O
to	O
bring	O
the	O
learnability	O
experiment	O
closer	O
to	O
more	O
naturalistic	O
settings	O
.	O
Although	O
we	O
restrict	O
to	O
balanced	O
,	O
binary	O
classification	O
for	O
simplicity	O
in	O
this	O
pilot	O
study	O
,	O
our	O
framework	O
can	O
also	O
be	O
extended	O
to	O
imbalanced	O
,	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
.	O
We	O
are	O
aware	O
that	O
computing	O
average	O
learnability	O
is	O
expensive	O
for	O
large	O
models	O
and	O
datasets	O
,	O
which	O
is	O
further	O
discussed	O
in	O
Section	O
8	O
.	O
We	O
provide	O
a	O
greener	O
solution	O
in	O
Appendix	O
D.	O
We	O
could	O
further	O
verify	O
our	O
assumptions	O
for	O
perturbations	O
with	O
a	O
user	O
study	O
(	O
Moradi	O
and	O
Samwald	O
,	O
2021	O
)	O
which	O
investigates	O
how	O
understandable	O
the	O
perturbed	O
texts	O
are	O
to	O
humans	O
.	O

Algorithm	O
1	O
Learnability	O
Estimation	O
Input	O
:	O
training	O
set	O
D	O
train	O
=	O
{	O
(	O
x	O
1	O
,	O
l	O
1	O
)	O
,	O
...	O
,	O
(	O
x	O
n	O
,	O
l	O
n	O
)	O
}	O
,	O
test	O
set	O
D	O
test	O
=	O
{	O
(	O
x	O
n+1	O
,	O
l	O
n+1	O
)	O
,	O
...	O
,	O
(	O
x	O
n+m	O
,	O
l	O
n+m	O
)	O
}	O
,	O
D	O
=	O
D	O
train	O
∪	O
D	O
test	O
,	O
model	O
f	O
∶	O
(	O
x	O
;	O
θ	B-HyperparameterName
)	O
↦	O
{	O
0	B-DatasetName
,	O
1	O
}	O
,	O
perturbation	O
g	O
∶	O
(	O
x	O
;	O
β	B-HyperparameterName
)	O
x	O
*	O
,	O
perturbation	O
probability	O
p	O
Output	O
:	O
learnability	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
)	O
1	O
:	O
//	O
①	O
assigning	O
random	O
labels	O
2	O
:	O
Initialize	O
an	O
empty	O
dataset	O
D	O
′	O
3	O
:	O
for	O
i	O
in	O
{	O
1	O
,	O
2	O
,	O
...	O
,	O
n	O
+	O
m	O
}	O
do	O
4	O
:	O
l	O
′	O
i	O
randint	O
[	O
0	B-DatasetName
,	O
1	O
]	O
5	O
:	O
D	O
′	O
D	O
′	O
∪	O
{	O
(	O
x	O
i	O
,	O
l	O
′	O
i	O
)	O
}	O
6	O
:	O
end	O
for	O
7	O
:	O
//	O
②	O
perturbing	O
with	O
probabilities	O
8	O
:	O
Initialize	O
an	O
empty	O
dataset	O
D	O
′	O
*	O
9	O
:	O
for	O
i	O
in	O
{	O
1	O
,	O
2	O
,	O
...	O
,	O
n	O
+	O
m	O
}	O
do	O
10	O
:	O
z	O
rand	O
(	O
0	B-DatasetName
,	O
1	O
)	O
11	O
:	O
x	O
*	O
i	O
x	O
i	O
12	O
:	O
if	O
l	O
′	O
i	O
=	O
1	O
z	O
<	O
p	O
then	O
13	O
:	O
x	O
*	O
i	O
g	O
(	O
x	O
i	O
)	O
14	O
:	O
end	O
if	O
15	O
:	O
D	O
′	O
*	O
D	O
′	O
*	O
∪	O
{	O
(	O
x	O
*	O
i	O
,	O
l	O
′	O
i	O
)	O
D	O
′	O
train	O
,	O
D	O
′	O
test	O
D	O
′	O
[	O
1	O
∶	O
n	O
]	O
,	O
D	O
′	O
[	O
n	O
+	O
1	O
∶	O
n	O
+	O
m	O
]	O
19	O
:	O
D	O
′	O
*	O
train	O
,	O
D	O
′	O
*	O
test	O
D	O
′	O
*	O
[	O
1	O
∶	O
n	O
]	O
,	O
D	O
′	O
*	O
[	O
n+1	O
∶	O
n+m	O
]	O
20	O
:	O
fit	O
the	O
model	O
f	O
(	O
⋅	O
)	O
on	O
D	O
′	O
*	O
train	O
21	O
:	O
A	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
*	O
test	O
)	O
f	O
(	O
⋅	O
)	O
accuracy	B-MetricName
on	O
D	O
′	O
*	O
test	O
22	O
:	O
A	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
test	O
)	O
f	O
(	O
⋅	O
)	O
accuracy	B-MetricName
on	O
D	O
′	O
test	O
23	O
:	O
return	O
A	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
*	O
test	O
)	O
−	O
A	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
test	O
)	O

In	O
Section	O
3.2	O
,	O
we	O
propose	O
an	O
accuracy	B-MetricName
-	O
based	O
identification	O
of	O
ATE	O
.	O
Now	O
we	O
discuss	O
another	O
probability	O
-	O
based	O
identification	O
and	O
compare	O
between	O
them	O
.	O
We	O
can	O
also	O
define	O
the	O
outcome	O
Y	O
of	O
a	O
test	O
example	O
x	O
i	O
as	O
the	O
predicted	O
probability	O
of	O
(	O
pseudo	O
)	O
true	O
label	O
given	O
by	O
the	O
trained	O
model	O
f	O
(	O
⋅	O
)	O
:	O
Y	O
i	O
(	O
0	B-DatasetName
)	O
∶=	O
P	O
f	O
(	O
L	O
′	O
=	O
l	O
′	O
i	O
|	O
X	O
=	O
x	O
i	O
)	O
(	O
0	B-DatasetName
,	O
1	O
)	O
.	O
(	O
11	O
)	O
Similarly	O
,	O
the	O
performance	O
outcome	O
Y	O
of	O
a	O
perturbed	O
test	O
data	O
point	O
x	O
*	O
i	O
is	O
:	O
Y	O
i	O
(	O
1	O
)	O
∶=	O
P	O
f	O
(	O
L	O
′	O
=	O
l	O
′	O
i	O
|	O
X	O
=	O
x	O
*	O
i	O
)	O
(	O
0	B-DatasetName
,	O
1	O
)	O
.	O
(	O
12	O
)	O
For	O
example	O
,	O
for	O
a	O
test	O
example	O
(	O
x	O
i	O
,	O
l	O
′	O
i	O
)	O
which	O
receives	O
treatment	O
(	O
l	O
′	O
i	O
=	O
1	O
)	O
,	O
the	O
trained	O
model	O
f	O
(	O
⋅	O
)	O
predicts	O
its	O
label	O
as	O
1	O
with	O
only	O
a	O
small	O
probability	O
0.1	O
before	O
treatment	O
(	O
it	O
has	O
not	O
been	O
perturbed	O
yet	O
)	O
,	O
and	O
0.9	O
after	O
treatment	O
.	O
So	O
the	O
Individual	O
Treatment	O
Effect	O
(	O
ITE	O
,	O
see	O
Equation	O
9	O
)	O
of	O
this	O
example	O
is	O
calculated	O
as	O
IT	O
E	O
i	O
=	O
Y	O
i	O
(	O
1	O
)	O
−	O
Y	O
i	O
(	O
0	B-DatasetName
)	O
=	O
0.9	O
−	O
0.1	O
=	O
0.8	O
.	O
We	O
then	O
take	O
an	O
average	O
over	O
all	O
the	O
perturbed	O
test	O
examples	O
(	O
half	O
of	O
the	O
test	O
set	O
)	O
as	O
Average	O
Treatment	O
Effect	O
(	O
ATE	O
,	O
see	O
Equation	O
10	O
)	O
,	O
which	O
is	O
exactly	O
the	O
learnability	O
of	O
a	O
perturbation	O
for	O
a	O
model	O
.	O
To	O
clarify	O
,	O
the	O
two	O
operands	O
in	O
Equation	O
10	O
are	O
defined	O
as	O
follows	O
:	O
E	O
[	O
Y	O
(	O
1	O
)	O
]	O
∶=	O
P	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
*	O
test	O
)	O
.	O
(	O
13	O
)	O
It	O
means	O
the	O
average	O
predicted	O
probability	O
of	O
(	O
pseudo	O
)	O
true	O
label	O
given	O
by	O
the	O
trained	O
model	O
f	O
(	O
⋅	O
)	O
on	O
the	O
perturbed	O
test	O
set	O
D	O
′	O
*	O
test	O
.	O
E	O
[	O
Y	O
(	O
0	B-DatasetName
)	O
]	O
∶=	O
P	O
(	O
f	O
,	O
g	O
,	O
p	O
,	O
D	O
′	O
test	O
)	O
.	O
(	O
14	O
)	O
Similarly	O
,	O
this	O
is	O
the	O
average	O
predicted	O
probability	O
on	O
the	O
randomly	O
labeled	O
test	O
set	O
D	O
′	O
test	O
.	O
Notice	O
that	O
the	O
accuracy	B-MetricName
-	O
based	O
definition	O
of	O
outcome	O
Y	O
(	O
Equation	O
6	O
)	O
can	O
also	O
be	O
written	O
in	O
a	O
similar	O
form	O
to	O
the	O
probability	O
-	O
based	O
one	O
(	O
Equation	O
11	O
)	O
:	O
Y	O
i	O
(	O
0	B-DatasetName
)	O
∶=	O
1	O
{	O
f	O
(	O
x	O
i	O
)	O
=	O
l	O
′	O
i	O
}	O
=	O
1	O
{	O
P	O
f	O
(	O
L	O
′	O
=	O
l	O
′	O
i	O
|	O
X	O
=	O
x	O
i	O
)	O
>	O
0.5	O
}	O
{	O
0	B-DatasetName
,	O
1	O
}	O
.	O
(	O
15	O
)	O
because	O
the	O
correctness	O
of	O
the	O
prediction	O
is	O
equal	O
to	O
whether	O
the	O
predicted	O
probability	O
of	O
true	O
(	O
pseudo	O
)	O
label	O
exceeds	O
a	O
certain	O
threshold	O
(	O
i.e.	O
,	O
0.5	O
)	O
.	O
The	O
major	O
difference	O
is	O
that	O
,	O
accuracy	B-MetricName
-	O
based	O
IT	O
E	O
is	O
a	O
discrete	O
variable	O
falling	O
in	O
{	O
−1	O
,	O
0	B-DatasetName
,	O
1	O
}	O
,	O
while	O
probability	O
-	O
based	O
IT	O
E	O
is	O
a	O
continuous	O
one	O
ranging	O
from	O
-	O
1	O
to	O
1	O
.	O
For	O
example	O
,	O
if	O
a	O
model	O
learns	O
to	O
identify	O
a	O
perturbation	O
and	O
thus	O
changes	O
its	O
prediction	O
from	O
wrong	O
(	O
before	O
perturbation	O
)	O
to	O
correct	O
(	O
after	O
perturbation	O
)	O
,	O
accuracy	B-MetricName
-	O
based	O
IT	O
E	O
will	O
be	O
1	O
−	O
0	B-DatasetName
=	O
1	O
while	O
probability	O
-	O
based	O
IT	O
E	O
will	O
be	O
less	O
than	O
1	O
.	O
That	O
is	O
to	O
say	O
,	O
accuracy	B-MetricName
-	O
based	O
AT	O
E	O
tends	O
to	O
vary	O
more	O
drastically	O
than	O
probability	O
-	O
based	O
if	O
inconsistent	O
predictions	O
occur	O
more	O
often	O
,	O
and	O
thus	O
can	O
better	O
capture	O
the	O
nuance	O
of	O
perturbation	O
learnability	O
.	O
Empirically	O
,	O
we	O
find	O
that	O
accuracy	B-MetricName
-	O
based	O
average	O
learnability	O
varies	O
greatly	O
(	O
σ	O
=	O
0.375	O
,	O
Table	O
4	O
)	O
and	O
thus	O
can	O
better	O
distinguish	O
between	O
different	O
model	O
-	O
perturbation	O
pairs	O
than	O
probabilitybased	O
one	O
(	O
σ	O
=	O
0.288	O
,	O
Table	O
4	O
)	O
.	O
As	O
a	O
result	O
,	O
we	O
choose	O
accuracy	B-MetricName
-	O
based	O
ATE	O
as	O
the	O
primary	O
measurement	O
of	O
learnability	O
in	O
this	O
paper	O
.	O

Inspired	B-DatasetName
by	O
Precision	B-MetricName
@	O
K	O
in	O
Information	B-TaskName
Retrieval	I-TaskName
(	O
IR	O
)	O
,	O
we	O
propose	O
a	O
similar	O
metric	O
dubbed	O
Learnability	O
@	O
p	O
,	O
which	O
is	O
the	O
learnability	O
of	O
a	O
perturbation	O
for	O
a	O
model	O
at	O
a	O
specific	O
perturbation	O
probability	O
p.	O
We	O
are	O
primarily	O
interested	O
in	O
whether	O
a	O
selected	O
p	O
can	O
represent	O
the	O
learnability	O
over	O
different	O
perturbation	O
probabilities	O
and	O
correlates	O
well	O
with	O
robustness	O
and	O
post	O
data	B-TaskName
augmentation	I-TaskName
∆.	O
We	O
calculate	O
the	O
standard	O
deviation	O
(	O
σ	O
)	O
of	O
Learnability	O
@	O
p	O
and	O
average	O
learnability	O
(	O
log	O
AU	O
C	O
)	O
over	O
all	O
model	O
-	O
perturbation	O
pairs	O
to	O
measure	O
how	O
well	O
it	O
can	O
distinguish	O
between	O
different	O
models	O
and	O
perturbations	O
.	O
Table	O
4	O
shows	O
that	O
average	O
learnability	O
is	O
more	O
diversified	O
than	O
all	O
Learnability	O
@	O
p	O
and	O
diversity	O
(	O
σ	O
)	O
peaks	O
at	O
p	O
=	O
0.01	O
for	O
accuracybased	O
/	O
probability	O
-	O
based	O
measurement	O
.	O
Accuracybased	O
Learnability	O
@	O
p	O
is	O
generally	O
more	O
diversified	O
across	O
models	O
and	O
perturbations	O
than	O
its	O
counterpart	O
.	O
To	O
investigate	O
the	O
strength	O
of	O
the	O
correlations	O
,	O
we	O
also	O
calculate	O
Spearman	O
ρ	O
between	O
accuracy	B-MetricName
-	O
based	O
/	O
probability	O
-	O
based	O
learnability	O
@	O
p	O
vs.	O
average	O
learnability	O
/	O
robustness	O
/	O
post	O
data	B-TaskName
augmentation	I-TaskName
∆	O
over	O
all	O
model	O
-	O
perturbation	O
pairs	O
.	O
Table	O
4	O
shows	O
that	O
generally	O
average	O
learnability	O
has	O
stronger	O
correlation	O
than	O
Learnability	O
@	O
p.	O
Correlations	O
with	O
both	O
robustness	O
and	O
post	O
data	B-TaskName
augmentation	I-TaskName
∆	O
peak	O
at	O
p	O
=	O
0.02	O
for	O
accuracybased	O
/	O
probability	O
-	O
based	O
measurements	O
,	O
and	O
the	O
correlations	O
with	O
average	O
learnability	O
(	O
0.816*/0.886	O
*	O
)	O
are	O
also	O
strong	O
at	O
these	O
perturbation	O
probabilities	O
.	O
Overall	O
,	O
Learnability	O
@	O
p	O
with	O
higher	O
standard	O
deviation	O
correlates	O
better	O
with	O
average	O
learnability	O
,	O
robustness	O
and	O
post	O
data	B-TaskName
augmentation	I-TaskName
∆.	O
Our	O
analysis	O
shows	O
that	O
if	O
p	O
is	O
carefully	O
selected	O
by	O
σ	O
,	O
Learnability	O
@	O
p	O
is	O
also	O
a	O
promising	O
metric	O
,	O
though	O
not	O
as	O
accurate	O
as	O
average	O
learnability	O
.	O
One	O
advantage	O
of	O
Learnability	O
@	O
p	O
over	O
average	O
learnability	O
is	O
that	O
it	O
costs	O
less	O
time	O
to	O
obtain	O
learnability	O
at	O
a	O
single	O
perturbation	O
probability	O
.	O
3	O
)	O
of	O
each	O
model	O
-	O
perturbation	O
pair	O
on	O
QQP	B-DatasetName
dataset	O
.	O
Rows	O
are	O
sorted	O
by	O
average	O
values	O
over	O
all	O
models	O
.	O
The	O
perturbation	O
for	O
which	O
a	O
model	O
is	O
most	O
learnable	O
is	O
highlighted	O
in	O
bold	O
while	O
the	O
following	O
one	O
is	O
underlined	O
.	O

Pre	O
-	O
trained	O
sequence	O
-	O
to	O
-	O
sequence	O
language	O
models	O
have	O
led	O
to	O
widespread	O
success	O
in	O
many	O
natural	O
language	O
generation	O
tasks	O
.	O
However	O
,	O
there	O
has	O
been	O
relatively	O
less	O
work	O
on	O
analyzing	O
their	O
ability	O
to	O
generate	O
structured	O
outputs	O
such	O
as	O
graphs	O
.	O
Unlike	O
natural	O
language	O
,	O
graphs	O
have	O
distinct	O
structural	O
and	O
semantic	O
properties	O
in	O
the	O
context	O
of	O
a	O
downstream	O
NLP	O
task	O
,	O
e.g.	O
,	O
generating	O
a	O
graph	O
that	O
is	O
connected	O
and	O
acyclic	O
can	O
be	O
attributed	O
to	O
its	O
structural	O
constraints	O
,	O
while	O
the	O
semantics	O
of	O
a	O
graph	O
can	O
refer	O
to	O
how	O
meaningfully	O
an	O
edge	O
represents	O
the	O
relation	O
between	O
two	O
node	O
concepts	O
.	O
In	O
this	O
work	O
,	O
we	O
study	O
pre	O
-	O
trained	O
language	O
models	O
that	O
generate	O
explanation	O
graphs	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
and	O
analyze	O
their	O
ability	O
to	O
learn	O
the	O
structural	O
constraints	O
and	O
semantics	O
of	O
such	O
graphs	O
.	O
We	O
first	O
show	O
that	O
with	O
limited	O
supervision	O
,	O
pre	O
-	O
trained	O
language	O
models	O
often	O
generate	O
graphs	O
that	O
either	O
violate	O
these	O
constraints	O
or	O
are	O
semantically	O
incoherent	O
.	O
Since	O
curating	O
large	O
amount	O
of	O
humanannotated	O
graphs	O
is	O
expensive	O
and	O
tedious	O
,	O
we	O
propose	O
simple	O
yet	O
effective	O
ways	O
of	O
graph	O
perturbations	O
via	O
node	O
and	O
edge	O
edit	O
operations	O
that	O
lead	O
to	O
structurally	O
and	O
semantically	O
positive	O
and	O
negative	O
graphs	O
.	O
Next	O
,	O
we	O
leverage	O
these	O
graphs	O
in	O
different	O
contrastive	B-MethodName
learning	I-MethodName
models	O
with	O
Max	O
-	O
Margin	O
and	O
InfoNCE	B-MethodName
losses	O
.	O
Our	O
methods	O
lead	O
to	O
significant	O
improvements	O
in	O
both	O
structural	O
and	O
semantic	O
accuracy	B-MetricName
of	O
explanation	O
graphs	O
and	O
also	O
generalize	O
to	O
other	O
similar	O
graph	B-TaskName
generation	I-TaskName
tasks	O
.	O
Lastly	O
,	O
we	O
show	O
that	O
human	O
errors	O
are	O
the	O
best	O
negatives	O
for	O
contrastive	B-MethodName
learning	I-MethodName
and	O
also	O
that	O
automatically	O
generating	O
more	O
such	O
human	O
-	O
like	O
negative	O
graphs	O
can	O
lead	O
to	O
further	O
improvements	O
.	O
1	O

Pre	O
-	O
trained	O
sequence	O
-	O
to	O
-	O
sequence	O
language	O
models	O
(	O
PLMs	O
)	O
like	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
and	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
showing	O
the	O
belief	O
,	O
argument	O
,	O
stance	O
,	O
gold	O
explanation	O
graph	O
,	O
and	O
T5	B-MethodName
-	O
generated	O
explanation	O
graph	O
.	O
The	O
dashed	O
nodes	O
represent	O
commonsense	O
nodes	O
and	O
the	O
dashed	O
edges	O
are	O
incorrect	O
edges	O
.	O
The	O
first	O
generated	O
graph	O
is	O
structurally	O
incorrect	O
and	O
the	O
second	O
graph	O
is	O
semantically	O
incorrect	O
.	O
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
have	O
led	O
to	O
significant	O
advances	O
in	O
many	O
natural	O
language	O
generation	O
tasks	O
like	O
text	B-TaskName
summarization	I-TaskName
and	O
machine	B-TaskName
translation	I-TaskName
.	O
The	O
models	O
are	O
pre	O
-	O
trained	O
on	O
massive	O
amounts	O
of	O
text	O
data	O
with	O
self	O
-	O
supervision	O
,	O
thus	O
enabling	O
them	O
to	O
construct	O
coherent	O
natural	O
language	O
sentences	O
for	O
downstream	O
tasks	O
.	O
This	O
then	O
raises	O
the	O
question	O
whether	O
pre	O
-	O
trained	O
language	O
models	O
,	O
trained	O
on	O
free	O
-	O
form	O
natural	O
language	O
data	O
,	O
can	O
also	O
adapt	O
themselves	O
to	O
generate	O
structured	O
outputs	O
like	O
graphs	O
.	O
Graphs	O
are	O
common	O
in	O
NLP	O
tasks	O
that	O
involve	O
representing	O
structured	O
knowledge	O
in	O
the	O
form	O
of	O
knowledge	O
bases	O
(	O
Guarino	O
and	O
Giaretta	O
,	O
1995	O
)	O
,	O
constructing	O
event	O
chains	O
from	O
documents	O
(	O
Chambers	O
and	O
Jurafsky	O
,	O
2009	O
)	O
,	O
or	O
more	O
recent	O
work	O
on	O
encoding	O
reasoning	O
chains	O
,	O
explanations	O
,	O
or	O
deductive	O
proofs	O
(	O
Saha	O
et	O
al	O
,	O
2020	O
;	O
.	O
Graphs	O
differ	O
from	O
free	O
-	O
form	O
natural	O
language	O
.	O
In	O
the	O
context	O
of	O
NLP	O
,	O
natural	O
language	O
graphs	O
(	O
consisting	O
of	O
textual	O
nodes	O
and	O
edges	O
)	O
can	O
have	O
distinct	O
structural	O
and	O
semantic	O
properties	O
.	O
For	O
example	O
,	O
consider	O
a	O
recently	O
proposed	O
commonsense	O
explanation	O
graph	B-TaskName
generation	I-TaskName
task	O
shown	O
in	O
Fig	O
.	O
1	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
.	O
Each	O
example	O
shows	O
a	O
belief	O
,	O
an	O
argument	O
and	O
an	O
explanation	O
graph	O
explaining	O
how	O
the	O
argument	O
supports	O
or	O
refutes	O
the	O
belief	O
.	O
These	O
explanation	O
graphs	O
encode	O
structured	O
knowledge	O
(	O
augmented	O
with	O
commonsense	O
)	O
and	O
consist	O
of	O
concepts	O
as	O
nodes	O
and	O
relations	O
from	O
ConceptNet	B-DatasetName
(	O
Liu	O
and	O
Singh	O
,	O
2004	O
)	O
as	O
edges	O
.	O
For	O
example	O
,	O
the	O
second	O
graph	O
encodes	O
the	O
knowledge	O
that	O
"	O
both	O
salads	O
and	O
fast	O
food	O
are	O
part	O
of	O
mcdonalds	O
and	O
hence	O
mcdonalds	O
is	O
not	O
greasy	O
and	O
fattening	O
"	O
,	O
thus	O
explicitly	O
refuting	O
the	O
belief	O
.	O
From	O
prior	O
work	O
,	O
the	O
structural	O
constraints	O
enforce	O
the	O
graphs	O
to	O
be	O
connected	O
directed	O
acyclic	O
and	O
the	O
nodes	O
to	O
contain	O
at	O
least	O
two	O
concepts	O
from	O
the	O
belief	O
and	O
two	O
from	O
the	O
argument	O
.	O
The	O
semantic	O
aspect	O
deals	O
with	O
commonsense	O
and	O
evaluates	O
whether	O
each	O
edge	O
expresses	O
coherent	O
relational	O
knowledge	O
and	O
if	O
the	O
whole	O
graph	O
explains	O
the	O
stance	O
.	O
Following	O
Saha	O
et	O
al	O
(	O
2021b	O
)	O
,	O
we	O
represent	O
graphs	O
as	O
strings	O
composed	O
of	O
concatenated	O
edges	O
and	O
fine	O
-	O
tune	O
T5	B-MethodName
to	O
generate	O
graphs	O
in	O
an	O
autoregressive	O
manner	O
.	O
We	O
observe	O
that	O
while	O
moderate	O
amount	O
of	O
supervision	O
enables	O
the	O
model	O
to	O
learn	O
valid	O
graph	O
encodings	O
,	O
the	O
graphs	O
frequently	O
violate	O
task	O
-	O
specific	O
structural	O
constraints	O
(	O
like	O
connectivity	O
)	O
.	O
For	O
instance	O
,	O
the	O
first	O
example	O
in	O
Fig	O
.	O
1	O
shows	O
a	O
graph	O
generated	O
by	O
T5	B-MethodName
that	O
is	O
disconnected	O
and	O
hence	O
structurally	O
incorrect	O
.	O
Moreover	O
,	O
for	O
the	O
fraction	O
of	O
graphs	O
that	O
are	O
structurally	O
correct	O
,	O
the	O
model	O
also	O
makes	O
commonsense	O
mistakes	O
,	O
a	O
type	O
of	O
semantic	O
error	O
,	O
by	O
inferring	O
wrong	O
or	O
incoherent	O
relations	O
between	O
concepts	O
.	O
Both	O
T5	B-MethodName
-	O
generated	O
graphs	O
shown	O
in	O
Fig	O
.	O
1	O
contain	O
incoherent	O
or	O
noncommonsensical	O
edges	O
(	O
marked	O
by	O
dashed	O
arrows	O
)	O
like	O
"	O
fast	O
food	O
;	O
has	O
context	O
;	O
salads	O
"	O
.	O
Based	O
on	O
these	O
observations	O
,	O
we	O
study	O
PLMs	O
that	O
generate	O
explanation	O
graphs	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
and	O
analyze	O
their	O
ability	O
to	O
learn	O
the	O
structural	O
constraints	O
as	O
well	O
as	O
the	O
semantics	O
of	O
such	O
graphs	O
.	O
While	O
a	O
general	O
recipe	O
towards	O
improving	O
the	O
structural	O
and	O
semantic	O
aspects	O
of	O
graph	B-TaskName
generation	I-TaskName
can	O
be	O
via	O
large	O
-	O
scale	O
training	O
with	O
more	O
humanannotated	O
graphs	O
,	O
it	O
is	O
prohibitive	O
under	O
most	O
practical	O
scenarios	O
because	O
of	O
the	O
cognitive	O
load	O
associated	O
with	O
a	O
complex	O
data	O
creation	O
task	O
like	O
graph	O
annotation	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
.	O
Hence	O
,	O
we	O
propose	O
simple	O
yet	O
effective	O
methods	O
of	O
graph	O
perturbations	O
that	O
perform	O
various	O
kinds	O
of	O
node	O
and	O
edge	O
addition	O
,	O
deletion	O
,	O
and	O
replacement	O
operations	O
to	O
construct	O
structurally	O
and	O
semantically	O
positive	O
(	O
correct	O
)	O
and	O
negative	O
(	O
incorrect	O
)	O
graphs	O
.	O
Overall	O
,	O
we	O
leverage	O
three	O
types	O
of	O
negative	O
graphs	O
(	O
synthetic	O
structural	O
,	O
synthetic	O
semantic	O
,	O
and	O
human	O
-	O
created	O
semantic	O
)	O
and	O
develop	O
multiple	O
contrastive	B-MethodName
learning	I-MethodName
models	O
(	O
Hjelm	O
et	O
al	O
,	O
2018	O
;	O
Chen	O
et	O
al	O
,	O
2020a	O
;	O
Khosla	O
et	O
al	O
,	O
2020	O
;	O
Gunel	O
et	O
al	O
,	O
2020	O
)	O
for	O
effectively	O
distinguishing	O
between	O
correct	O
and	O
incorrect	O
graphs	O
.	O
Our	O
first	O
method	O
is	O
a	O
Generate	O
-	O
and	O
-	O
Refine	O
model	O
that	O
first	O
generates	O
an	O
initial	O
graph	O
and	O
further	O
refines	O
it	O
using	O
another	O
T5	B-MethodName
model	O
.	O
Next	O
,	O
we	O
propose	O
two	O
improved	O
modelsone	O
that	O
uses	O
the	O
negative	O
graphs	O
in	O
a	O
max	O
-	O
margin	O
formulation	O
and	O
another	O
that	O
uses	O
both	O
positive	O
and	O
negative	O
graphs	O
with	O
a	O
InfoNCE	B-MethodName
(	O
van	O
den	O
Oord	O
et	O
al	O
,	O
2018	O
)	O
contrastive	O
loss	B-MetricName
.	O
On	O
two	O
real	O
-	O
world	O
tasks	O
of	O
explanation	O
graph	B-TaskName
generation	I-TaskName
and	O
temporal	O
graph	B-TaskName
generation	I-TaskName
,	O
with	O
varied	O
node	O
and	O
edge	O
semantics	O
,	O
we	O
observe	O
that	O
our	O
proposed	O
methods	O
and	O
graph	O
perturbation	O
techniques	O
generalize	O
well	O
and	O
lead	O
to	O
improvements	O
in	O
both	O
structural	O
and	O
semantic	O
accuracy	B-MetricName
of	O
graphs	O
.	O
Further	O
analysis	O
of	O
different	O
types	O
of	O
negative	O
graphs	O
reveal	O
that	O
the	O
human	O
-	O
error	O
graphs	O
are	O
the	O
hardest	O
,	O
most	O
diverse	O
,	O
and	O
hence	O
the	O
best	O
type	O
of	O
negatives	O
to	O
learn	O
from	O
in	O
contrastive	B-MethodName
learning	I-MethodName
.	O
Hence	O
,	O
we	O
also	O
develop	O
methods	O
to	O
automatically	O
generate	O
more	O
such	O
human	O
-	O
like	O
semantic	O
negative	O
graphs	O
,	O
which	O
leads	O
to	O
further	O
improvements	O
.	O
We	O
summarize	O
our	O
contributions	O
as	O
follows	O
.	O
We	O
present	O
a	O
detailed	O
analysis	O
of	O
graph	O
structure	O
and	O
semantics	O
for	O
end	O
-	O
to	O
-	O
end	O
explanation	O
graph	B-TaskName
generation	I-TaskName
via	O
pre	O
-	O
trained	O
language	O
models	O
.	O
We	O
propose	O
simple	O
yet	O
effective	O
graph	O
perturbation	O
techniques	O
for	O
constructing	O
positive	O
and	O
negative	O
graphs	O
and	O
use	O
them	O
in	O
different	O
graph	O
contrastive	B-MethodName
learning	I-MethodName
models	O
.	O
Our	O
methods	O
lead	O
to	O
significant	O
improvements	O
in	O
both	O
structural	O
and	O
semantic	O
accuracy	B-MetricName
of	O
explanation	O
graphs	O
and	O
also	O
generalize	O
to	O
other	O
similar	O
graph	B-TaskName
generation	I-TaskName
tasks	O
.	O

Our	O
primary	O
task	O
of	O
interest	O
is	O
a	O
recently	O
proposed	O
commonsense	O
explanation	O
graph	B-TaskName
generation	I-TaskName
task	O
called	O
ExplaGraphs	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
.	O
In	O
Sec	O
.	O
6.4	O
,	O
we	O
also	O
experiment	O
with	O
another	O
related	O
task	O
of	O
temporal	O
graph	B-TaskName
generation	I-TaskName
(	O
Madaan	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
both	O
these	O
tasks	O
,	O
the	O
structural	O
aspect	O
deals	O
with	O
satisfying	O
certain	O
task	O
-	O
specific	O
constraints	O
on	O
the	O
graph	O
(	O
like	O
connectivity	O
)	O
and	O
the	O
semantic	O
aspect	O
deals	O
with	O
the	O
construction	O
of	O
meaningful	O
edges	O
(	O
that	O
adhere	O
to	O
commonsense	O
)	O
.	O
Below	O
we	O
discuss	O
ExplaGraphs	O
briefly	O
and	O
analyze	O
pre	O
-	O
trained	O
language	O
models	O
for	O
their	O
ability	O
to	O
generate	O
explanation	O
graphs	O
.	O
ExplaGraphs	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
.	O
In	O
this	O
task	O
,	O
given	O
a	O
belief	O
and	O
an	O
argument	O
,	O
an	O
agent	B-DatasetName
has	O
to	O
perform	O
two	O
sub	O
-	O
tasks	O
-	O
predict	O
the	O
stance	O
(	O
support	O
/	O
counter	O
)	O
and	O
also	O
generate	O
an	O
explanation	O
graph	O
explaining	O
the	O
stance	O
.	O
Explanation	O
graphs	O
are	O
structured	O
explanations	O
that	O
capture	O
explicit	O
reasoning	O
chains	O
between	O
the	O
belief	O
and	O
the	O
argument	O
,	O
thereby	O
making	O
models	O
more	O
interpretable	O
.	O
Formally	O
,	O
an	O
explanation	O
graph	O
is	O
a	O
connected	O
DAG	O
with	O
nodes	O
as	O
concepts	O
and	O
edges	O
as	O
commonsense	O
relations	O
between	O
two	O
concepts	O
(	O
See	O
Fig	O
.	O
1	O
)	O
.	O
The	O
concepts	O
are	O
either	O
part	O
of	O
the	O
belief	O
or	O
the	O
argument	O
(	O
represented	O
with	O
solid	O
boxes	O
)	O
or	O
any	O
external	O
commonsense	O
phrase	O
(	O
represented	O
with	O
dashed	O
boxes	O
)	O
.	O
Each	O
edge	O
in	O
the	O
graph	O
forms	O
a	O
coherent	O
sentence	O
and	O
the	O
graph	O
,	O
when	O
read	O
as	O
a	O
whole	O
,	O
forms	O
reasoning	O
structures	O
explaining	O
why	O
the	O
argument	O
supports	O
or	O
refutes	O
the	O
belief	O
.	O
Saha	O
et	O
al	O
(	O
2021b	O
)	O
evaluate	O
explanation	O
graphs	O
by	O
defining	O
two	O
accuracy	B-MetricName
metrics	O
-	O
(	O
1	O
)	O
Structural	O
Correctness	O
Accuracy	B-MetricName
(	O
StCA	O
)	O
:	O
Fraction	O
of	O
graphs	O
that	O
satisfy	O
all	O
structural	O
constraints	O
,	O
and	O
(	O
2	O
)	O
Semantic	O
Correctness	O
Accuracy	B-MetricName
(	O
SeCA	O
)	O
:	O
Fraction	O
of	O
graphs	O
that	O
are	O
both	O
structurally	O
and	O
semantically	O
correct	O
.	O
A	O
graph	O
is	O
considered	O
structurally	O
correct	O
if	O
it	O
satisfies	O
the	O
following	O
constraints	O
:	O
(	O
1	O
)	O
it	O
is	O
connected	O
,	O
(	O
2	O
)	O
it	O
is	O
a	O
DAG	O
,	O
(	O
3	O
)	O
the	O
edge	O
relations	O
belong	O
to	O
a	O
pre	O
-	O
defined	O
list	O
,	O
(	O
4	O
)	O
there	O
are	O
at	O
least	O
two	O
concepts	O
from	O
the	O
belief	O
and	O
two	O
from	O
the	O
argument	O
.	O
If	O
all	O
these	O
constraints	O
are	O
satisfied	O
,	O
the	O
graph	O
is	O
next	O
evaluated	O
for	O
semantic	O
correctness	O
by	O
a	O
model	O
-	O
based	O
metric	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
.	O
It	O
works	O
on	O
the	O
principle	O
that	O
an	O
explanation	O
graph	O
is	O
semantically	O
correct	O
if	O
the	O
stance	O
inferred	O
from	O
the	O
belief	O
and	O
the	O
graph	O
matches	O
the	O
gold	O
stance	O
.	O
Refer	O
to	O
Appendix	O
A	O
for	O
a	O
detailed	O
description	O
of	O
all	O
evaluation	O
metrics	O
.	O
Baseline	O
T5	B-MethodName
Model	O
.	O
Following	O
prior	O
work	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
,	O
we	O
generate	O
explanation	O
graphs	O
as	O
post	O
-	O
hoc	O
explanations	O
by	O
conditioning	O
on	O
the	O
belief	O
,	O
argument	O
and	O
the	O
predicted	O
stance	O
.	O
2	O
The	O
stance	O
prediction	O
model	O
is	O
a	O
fine	O
-	O
tuned	O
RoBERTa	B-MethodName
model	O
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
which	O
we	O
keep	O
unaltered	O
from	O
prior	O
work	O
and	O
focus	O
on	O
the	O
graph	B-TaskName
generation	I-TaskName
sub	O
-	O
task	O
.	O
We	O
generate	O
graphs	O
as	O
linearized	O
strings	O
in	O
an	O
endto	O
-	O
end	O
manner	O
by	O
leveraging	O
an	O
encoder	O
-	O
decoder	O
pre	O
-	O
trained	O
language	O
model	O
,	O
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
input	O
to	O
the	O
model	O
is	O
the	O
concatenated	O
belief	O
,	O
argument	O
and	O
the	O
stance	O
along	O
with	O
a	O
prefix	O
"	O
Generate	O
an	O
Explanation	O
Graph	O
for	O
"	O
.	O
The	O
graphs	O
are	O
encoded	O
as	O
concatenated	O
bracketed	O
edges	O
,	O
in	O
which	O
the	O
edges	O
are	O
ordered	O
according	O
to	O
the	O
Depth	O
First	O
Search	O
(	O
DFS	O
)	O
order	O
of	O
the	O
nodes	O
.	O
While	O
we	O
choose	O
T5	B-MethodName
because	O
of	O
its	O
superior	O
performance	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
,	O
we	O
do	O
not	O
make	O
any	O
model	O
-	O
specific	O
assumptions	O
and	O
graphs	O
can	O
be	O
generated	O
via	O
any	O
encoder	O
-	O
decoder	O
style	O
pre	O
-	O
trained	O
language	O
model	O
(	O
e.g.	O
,	O
see	O
Appendix	O
E	O
for	O
results	O
with	O
BART	B-MethodName
)	O
.	O
Analysis	O
of	O
T5	B-MethodName
Baseline	O
.	O
We	O
analyze	O
the	O
quality	O
of	O
the	O
explanation	O
graphs	O
generated	O
by	O
T5	B-MethodName
in	O
Table	O
1	O
.	O
We	O
vary	O
the	O
amount	O
of	O
training	O
data	O
from	O
500	O
to	O
2368	O
samples	O
(	O
all	O
)	O
and	O
report	O
StCA	O
and	O
SeCA	O
along	O
with	O
other	O
metrics	O
like	O
Graph	O
-	O
BertScore	O
(	O
G	O
-	O
BS	O
)	O
introduced	O
in	O
prior	O
work	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
.	O
While	O
the	O
structural	O
accuracy	B-MetricName
improves	O
with	O
increase	O
in	O
training	O
data	O
,	O
the	O
gain	O
saturates	O
quickly	O
and	O
even	O
after	O
training	O
on	O
the	O
entire	O
data	O
,	O
we	O
find	O
a	O
significant	O
fraction	O
of	O
graphs	O
to	O
violate	O
the	O
structural	O
constraints	O
.	O
We	O
note	O
that	O
a	O
high	O
91	O
%	O
of	O
T5	B-MethodName
's	O
generations	O
are	O
valid	O
graph	O
encodings	O
i.e.	O
,	O
the	O
generated	O
strings	O
can	O
be	O
parsed	O
into	O
graphical	O
structures	O
(	O
without	O
any	O
post	O
-	O
processing	O
)	O
,	O
suggesting	O
that	O
T5	B-MethodName
is	O
able	O
to	O
learn	O
the	O
graph	O
encoding	O
from	O
a	O
fairly	O
small	O
amount	O
of	O
supervision	O
.	O
However	O
,	O
it	O
fails	O
to	O
satisfy	O
the	O
various	O
structural	O
constraints	O
-	O
(	O
1	O
)	O
20	O
%	O
of	O
the	O
graphs	O
are	O
disconnected	O
,	O
(	O
2	O
)	O
6	O
%	O
of	O
the	O
graphs	O
contain	O
cycles	O
,	O
and	O
(	O
3	O
)	O
14	O
%	O
of	O
the	O
graphs	O
have	O
less	O
than	O
two	O
concepts	O
from	O
the	O
belief	O
or	O
from	O
the	O
argument	O
.	O
Note	O
that	O
these	O
constraints	O
are	O
not	O
encoded	O
in	O
the	O
model	O
,	O
thus	O
making	O
them	O
fairly	O
hard	O
to	O
learn	O
from	O
limited	O
supervision	O
.	O
On	O
the	O
fraction	O
of	O
structurally	O
correct	O
graphs	O
,	O
the	O
model	O
makes	O
further	O
semantic	O
errors	O
and	O
a	O
lower	O
SeCA	O
of	O
35	O
%	O
demonstrates	O
that	O
.	O
In	O
Fig	O
.	O
1	O
,	O
we	O
show	O
examples	O
of	O
structurally	O
incorrect	O
and	O
semantically	O
incorrect	O
graphs	O
generated	O
by	O
T5	B-MethodName
.	O
Overall	O
,	O
these	O
results	O
indicate	O
that	O
there	O
is	O
a	O
significant	O
scope	O
for	O
improvement	O
both	O
on	O
graph	O
structure	O
and	O
semantics	O
,	O
thus	O
motivating	O
us	O
to	O
develop	O
methods	O
with	O
design	O
choices	O
aimed	O
at	O
improving	O
both	O
aspects	O
.	O

One	O
simple	O
method	O
to	O
augment	O
existing	O
training	O
data	O
is	O
to	O
create	O
synthetic	O
positive	O
graphs	O
.	O
These	O
graphs	O
should	O
be	O
created	O
such	O
that	O
all	O
the	O
taskspecific	O
constraints	O
continue	O
to	O
hold	O
upon	O
perturbations	O
.	O
E.g.	O
,	O
removing	O
a	O
node	O
that	O
makes	O
the	O
graph	O
disconnected	O
is	O
a	O
prohibitive	O
action	O
.	O
Hence	O
,	O
we	O
choose	O
nodes	O
(	O
concepts	O
)	O
that	O
are	O
not	O
part	O
of	O
the	O
belief	O
or	O
the	O
argument	O
(	O
also	O
termed	O
as	O
commonsense	O
nodes	O
)	O
and	O
replace	O
them	O
with	O
phrases	O
that	O
are	O
synonymous	O
to	O
the	O
original	O
phrases	O
.	O
To	O
do	O
so	O
,	O
we	O
select	O
words	O
from	O
the	O
concept	O
with	O
POS	O
tags	O
of	O
Adjective	O
,	O
Noun	O
,	O
Adverb	O
,	O
or	O
Verb	O
and	O
replace	O
them	O
with	O
that	O
synonym	O
from	O
Wordnet	O
(	O
Miller	O
,	O
1995	O
)	O
for	O
which	O
the	O
cosine	O
similarity	O
of	O
their	O
word2vec	O
representations	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
is	O
the	O
highest	O
.	O
3	O
Fig	O
.	O
2	O
shows	O
an	O
example	O
of	O
a	O
positive	O
graph	O
perturbation	O
where	O
the	O
node	O
"	O
loss	B-MetricName
of	O
jobs	O
"	O
is	O
replaced	O
with	O
"	O
going	O
of	O
business	O
"	O
.	O
Note	O
that	O
our	O
node	O
replacement	O
operations	O
will	O
always	O
lead	O
to	O
structurally	O
similar	O
graphs	O
.	O
Automatically	O
constructing	O
structurally	O
diverse	O
positive	O
graphs	O
is	O
a	O
challenging	O
problem	O
and	O
we	O
leave	O
that	O
for	O
future	O
work	O
.	O

We	O
also	O
construct	O
semantically	O
incorrect	O
negative	O
explanation	O
graphs	O
.	O
While	O
the	O
previous	O
category	O
of	O
negative	O
graphs	O
(	O
SySt	O
)	O
captures	O
structural	O
constraints	O
,	O
SySe	O
captures	O
the	O
relational	O
knowledge	O
in	O
graphs	O
.	O
Semantic	O
incorrectness	O
typically	O
arises	O
from	O
inappropriate	O
relations	O
that	O
do	O
not	O
adhere	O
to	O
human	O
commonsense	O
(	O
"	O
loss	B-MetricName
of	O
jobs	O
;	O
is	O
a	O
;	O
humane	O
"	O
)	O
.	O
We	O
create	O
such	O
negative	O
graphs	O
by	O
selecting	O
a	O
random	O
number	O
of	O
edges	O
and	O
then	O
replacing	O
the	O
relations	O
with	O
some	O
other	O
relations	O
.	O
Fig	O
.	O
2	O
shows	O
a	O
semantic	O
negative	O
graph	O
in	O
which	O
the	O
relations	O
marked	O
with	O
dashed	O
lines	O
are	O
perturbed	O
.	O
Human	O
-	O
created	O
&	O
Semantic	O
Negative	O
Graphs	O
(	O
HuSe	O
)	O
.	O
The	O
space	O
of	O
semantically	O
incorrect	O
graphs	O
is	O
fairly	O
large	O
and	O
in	O
order	O
to	O
augment	O
our	O
synthetic	O
negative	O
graphs	O
with	O
harder	O
structurallydiverse	O
negatives	O
,	O
we	O
make	O
use	O
of	O
human	O
-	O
created	O
incorrect	O
graphs	O
from	O
prior	O
work	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
.	O
4	O
Humans	O
make	O
subtle	O
errors	O
,	O
thus	O
making	O
them	O
ideal	O
negative	O
candidates	O
for	O
contrastive	B-MethodName
learning	I-MethodName
.	O
ExplaGraphs	O
was	O
constructed	O
via	O
an	O
iterative	O
framework	O
in	O
which	O
the	O
graphs	O
are	O
iteratively	O
refined	O
(	O
up	O
to	O
two	O
times	O
)	O
until	O
they	O
are	O
verified	O
as	O
correct	O
.	O
We	O
treat	O
these	O
refined	O
graphs	O
as	O
negatives	O
.	O
Specifically	O
,	O
in	O
two	O
rounds	O
,	O
if	O
an	O
initial	O
graph	O
G	O
1	O
is	O
refined	O
into	O
graphs	O
G	O
2	O
and	O
G	O
3	O
successively	O
,	O
then	O
G	O
1	O
and	O
G	O
2	O
are	O
considered	O
as	O
negative	O
graphs	O
.	O
Unlike	O
SySe	O
which	O
only	O
perturb	O
the	O
relations	O
,	O
these	O
negatives	O
are	O
structurally	O
diverse	O
(	O
see	O
Fig	O
.	O
2	O
)	O
and	O
capture	O
semantics	O
not	O
just	O
at	O
the	O
level	O
of	O
each	O
edge	O
but	O
for	O
the	O
graph	O
as	O
a	O
whole	O
(	O
e.g.	O
,	O
a	O
graph	O
might	O
be	O
refined	O
because	O
it	O
does	O
not	O
explain	O
the	O
stance	O
)	O
.	O
Note	O
that	O
human	O
-	O
created	O
graphs	O
can	O
only	O
be	O
semantically	O
incorrect	O
,	O
since	O
their	O
structural	O
correctness	O
is	O
already	O
ensured	O
during	O
construction	O
.	O

Our	O
next	O
model	O
leverages	O
the	O
negatively	O
perturbed	O
graphs	O
in	O
a	O
max	O
-	O
margin	O
formulation	O
.	O
During	O
training	O
,	O
given	O
a	O
(	O
belief	O
,	O
argument	O
,	O
stance	O
)	O
context	O
x	O
,	O
a	O
ground	O
truth	O
graph	O
G	O
(	O
g	O
)	O
and	O
a	O
negative	O
graph	O
G	O
(	O
n	O
)	O
,	O
linearized	O
into	O
a	O
sequence	O
of	O
words	O
{	O
y	O
(	O
g	O
)	O
i	O
}	O
k	O
i=1	O
and	O
{	O
y	O
(	O
n	O
)	O
i	O
}	O
l	O
i=1	O
respectively	O
,	O
we	O
define	O
the	O
loss	B-MetricName
function	O
L	O
as	O
a	O
linear	O
combination	O
of	O
the	O
standard	O
crossentropy	O
loss	B-MetricName
L	O
CE	O
and	O
a	O
max	O
-	O
margin	O
loss	B-MetricName
L	O
MM	O
,	O
defined	O
between	O
a	O
word	O
y	O
(	O
g	O
)	O
i	O
of	O
the	O
positive	O
graph	O
and	O
a	O
word	O
y	O
(	O
n	O
)	O
i	O
of	O
the	O
negative	O
graph	O
.	O
L	O
CE	O
=	O
i	O
−logP	O
θ	B-HyperparameterName
(	O
y	O
(	O
g	O
)	O
i	O
|	O
y	O
(	O
g	O
)	O
<	O
i	O
,	O
x	O
)	O
L	O
MM	O
=	O
i	O
max	O
(	O
0	B-DatasetName
,	O
logP	O
θ	B-HyperparameterName
(	O
y	O
(	O
g	O
)	O
i	O
|	O
y	O
(	O
g	O
)	O
<	O
i	O
,	O
x	O
)	O
−	O
log	O
P	O
θ	B-HyperparameterName
(	O
y	O
(	O
n	O
)	O
i	O
|	O
y	O
(	O
n	O
)	O
<	O
i	O
,	O
x	O
)	O
+	O
β	B-HyperparameterName
)	O
L	O
=	O
L	O
CE	O
+	O
αL	O
MM	O
where	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
(	O
margin	O
)	O
are	O
hyperparameters	O
.	O
As	O
noted	O
earlier	O
,	O
the	O
baseline	O
model	O
often	O
makes	O
commonsense	O
mistakes	O
in	O
distinguishing	O
between	O
positive	O
and	O
negative	O
relations	O
(	O
"	O
causes	O
"	O
vs	O
"	O
not	O
causes	O
"	O
)	O
and	O
our	O
relation	O
perturbing	O
negative	O
graphs	O
and	O
the	O
max	O
-	O
margin	O
loss	B-MetricName
component	O
facilitate	O
learning	O
a	O
better	O
boundary	O
between	O
them	O
.	O

Our	O
Contrastive	O
Graph	B-TaskName
Generation	I-TaskName
Model	O
(	O
Fig	O
.	O
2	O
)	O
also	O
leverages	O
both	O
positive	O
and	O
negative	O
graphs	O
but	O
instead	O
of	O
doing	O
so	O
in	O
a	O
2	O
-	O
stage	O
Generate	O
&	O
Refine	O
model	O
,	O
uses	O
a	O
contrastive	B-MethodName
learning	I-MethodName
framework	O
(	O
Khosla	O
et	O
al	O
,	O
2020	O
;	O
Gunel	O
et	O
al	O
,	O
2020	O
)	O
.	O
Given	O
a	O
ground	O
-	O
truth	O
graph	O
G	O
(	O
g	O
)	O
,	O
a	O
positive	O
graph	O
G	O
(	O
p	O
)	O
and	O
a	O
set	O
of	O
negative	O
graphs	O
{	O
G	O
(	O
n	O
)	O
i	O
}	O
M	O
i=1	O
,	O
contrastive	B-MethodName
learning	I-MethodName
aims	O
to	O
learn	O
the	O
graph	O
representations	O
such	O
that	O
the	O
gold	O
graph	O
's	O
representation	O
is	O
close	O
to	O
that	O
of	O
the	O
synthetic	O
positive	O
graph	O
while	O
being	O
distant	O
from	O
those	O
of	O
the	O
negative	O
graphs	O
.	O
Similar	O
to	O
Cao	O
and	O
Wang	O
(	O
2021	O
)	O
,	O
we	O
use	O
the	O
last	O
layer	O
of	O
the	O
decoder	O
in	O
T5	B-MethodName
as	O
the	O
representation	O
of	O
each	O
token	O
in	O
the	O
graph	O
and	O
obtain	O
the	O
graph	O
representation	O
by	O
averaging	O
over	O
the	O
constituent	O
token	O
representations	O
.	O
Let	O
the	O
graph	O
representations	O
be	O
denoted	O
by	O
h	O
(	O
g	O
)	O
,	O
h	O
(	O
p	O
)	O
and	O
{	O
h	O
(	O
n	O
)	O
i	O
}	O
M	O
i=1	O
.	O
Given	O
H	O
(	O
g	O
)	O
=	O
{	O
h	O
(	O
p	O
)	O
}	O
{	O
h	O
(	O
n	O
)	O
i	O
}	O
M	O
i=1	O
,	O
our	O
overall	O
loss	B-MetricName
combines	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
L	O
CE	O
and	O
the	O
InfoNCE	B-MethodName
contrastive	O
loss	B-MetricName
(	O
van	O
den	O
Oord	O
et	O
al	O
,	O
2018	O
)	O
L	O
CL	O
as	O
shown	O
below	O
.	O
(	O
Efron	O
and	O
Tibshirani	O
,	O
1994	O
)	O
)	O
with	O
p	O
<	O
0.005	O
.	O
L	O
CL	O
=	O
−	O
log	O
exp	O
(	O
sim	O
(	O
h	O
(	O
g	O
)	O
,	O
h	O
(	O
p	O
)	O
)	O
/τ	O
)	O
h	O
i	O
H	O
(	O
g	O
)	O
exp	O
(	O
sim	O
(	O
h	O
(	O
g	O
)	O
,	O
h	O
i	O
)	O
/τ	O
)	O
L	O
=	O
L	O
CE	O
+	O
where	O
α	B-HyperparameterName
and	O
the	O
temperature	O
τ	O
are	O
the	O
hyperparameters	O
and	O
sim	O
(	O
)	O
denotes	O
the	O
cosine	O
similarity	O
function	O
between	O
the	O
graph	O
representations	O
.	O
6	O
Experiments	O

In	O
Table	O
2	O
,	O
we	O
compare	O
the	O
various	O
modeling	O
techniques	O
described	O
in	O
Sec	O
.	O
5	O
and	O
their	O
effect	O
on	O
the	O
structural	O
and	O
semantic	O
correctness	O
of	O
the	O
generated	O
graphs	O
.	O
While	O
our	O
primary	O
metrics	O
of	O
interest	O
are	O
Graph	O
Structural	O
Accuracy	B-MetricName
(	O
StCA	O
)	O
and	O
Semantic	O
Accuracy	B-MetricName
(	O
SeCA	O
)	O
,	O
following	O
prior	O
work	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
,	O
we	O
also	O
report	O
Stance	O
Accuracy	B-MetricName
(	O
SA	O
)	O
,	O
Graph	O
-	O
BertScore	O
(	O
G	O
-	O
BS	O
)	O
,	O
Graph	O
Edit	O
Distance	O
(	O
GED	B-DatasetName
)	O
and	O
Edge	O
Accuracy	B-MetricName
(	O
EA	O
)	O
.	O
We	O
observe	O
that	O
using	O
a	O
larger	O
T5	B-MethodName
model	O
improves	O
StCA	O
by	O
12	O
%	O
and	O
SeCA	O
by	O
16	O
%	O
.	O
This	O
finding	O
is	O
in	O
line	O
with	O
other	O
commonsense	O
reasoning	O
tasks	O
(	O
Lourie	O
et	O
al	O
,	O
2021	O
;	O
Elazar	O
et	O
al	O
,	O
2021	O
)	O
which	O
also	O
show	O
that	O
fine	O
-	O
tuning	O
a	O
larger	O
language	O
model	O
typically	O
leads	O
to	O
better	O
performance	O
.	O
Together	O
with	O
the	O
results	O
reported	O
in	O
Table	O
1	O
,	O
we	O
conclude	O
that	O
much	O
of	O
the	O
improvement	O
in	O
explanation	O
graph	B-TaskName
generation	I-TaskName
comes	O
from	O
increasing	O
the	O
training	O
data	O
and	O
using	O
a	O
larger	O
model	O
.	O
Given	O
its	O
superior	O
performance	O
,	O
we	O
build	O
our	O
proposed	O
models	O
on	O
T5	B-MethodName
-	O
large	O
.	O

We	O
test	O
the	O
generalizability	O
of	O
constructing	O
structurally	O
and	O
semantically	O
perturbed	O
graphs	O
for	O
contrastive	B-MethodName
learning	I-MethodName
by	O
also	O
experimenting	O
on	O
a	O
temporal	O
graph	B-TaskName
generation	I-TaskName
task	O
(	O
Madaan	O
and	O
Yang	O
,	O
2021	O
)	O
that	O
requires	O
constructing	O
a	O
temporal	O
graph	O
from	O
a	O
document	O
.	O
The	O
nodes	O
in	O
the	O
graph	O
are	O
events	O
from	O
the	O
document	O
and	O
the	O
edges	O
are	O
temporal	O
relations	O
between	O
events	O
(	O
"	O
before	O
"	O
,	O
"	O
after	O
"	O
,	O
etc	O
)	O
.	O
Following	O
our	O
overall	O
goal	O
of	O
improving	O
graph	B-TaskName
generation	I-TaskName
with	O
limited	O
data	O
,	O
we	O
randomly	O
sample	O
1.3	O
%	O
of	O
the	O
overall	O
corpus	O
(	O
∼9.5k	O
samples	O
)	O
as	O
the	O
training	O
data	O
such	O
that	O
all	O
graphs	O
are	O
connected	O
DAGs	O
.	O
Similar	O
to	O
ExplaGraphs	O
,	O
we	O
create	O
structurally	O
negative	O
graphs	O
with	O
disconnected	O
and	O
cyclic	O
graphs	O
and	O
semantic	O
negative	O
graphs	O
by	O
perturbating	O
the	O
temporal	O
relations	O
.	O
E.g.	O
,	O
if	O
an	O
edge	O
relation	O
is	O
"	O
before	O
"	O
,	O
we	O
replace	O
it	O
with	O
"	O
after	O
"	O
.	O
We	O
construct	O
positive	O
graphs	O
by	O
replacing	O
edges	O
like	O
"	O
A	O
before	O
B	O
"	O
with	O
"	O
B	O
after	O
A	O
"	O
(	O
more	O
details	O
in	O
Appendix	O
C	O
)	O
.	O
In	O
6.5	O
Analysis	O
of	O
Generated	O
Graphs	O
Fig	O
.	O
3	O
shows	O
an	O
example	O
of	O
the	O
graphs	O
generated	O
by	O
different	O
models	O
(	O
more	O
examples	O
in	O
Appendix	O
F	O
)	O
.	O
Unlike	O
T5	B-MethodName
,	O
our	O
models	O
'	O
graphs	O
are	O
both	O
structurally	O
and	O
semantically	O
correct	O
with	O
diverse	O
commonsense	O
nodes	O
(	O
"	O
Groupthink	O
"	O
,	O
"	O
Good	O
Thing	O
"	O
)	O
.	O
While	O
our	O
models	O
generate	O
more	O
correct	O
graphs	O
,	O
they	O
lack	O
in	O
structural	O
diversity	O
-	O
the	O
Contrastive	O
model	O
generates	O
77	O
%	O
of	O
linear	O
graphs	O
(	O
i.e.	O
,	O
the	O
nodes	O
are	O
in	O
a	O
linear	O
chain	O
)	O
which	O
is	O
comparable	O
to	O
75	O
%	O
in	O
the	O
T5	B-MethodName
model	O
.	O
This	O
can	O
be	O
attributed	O
to	O
our	O
structurally	O
similar	O
positive	O
graphs	O
as	O
the	O
model	O
does	O
not	O
obtain	O
enough	O
supervision	O
to	O
generate	O
diverse	O
graphs	O
.	O
Structural	O
diversity	O
is	O
not	O
a	O
measure	O
of	O
graph	O
correctness	O
;	O
however	O
,	O
like	O
diverse	O
text	B-TaskName
generation	I-TaskName
(	O
Vijayakumar	O
et	O
al	O
,	O
2018	O
)	O
,	O
generating	O
diverse	O
graphs	O
is	O
an	O
interesting	O
direction	O
for	O
future	O
work	O
.	O
6.6	O
Generating	O
Human	O
-	O
like	O
Semantic	O
Negatives	O
(	O
HuSe	O
-	O
Gen	O
)	O
In	O
ExplaGraphs	O
,	O
human	O
-	O
created	O
negatives	O
account	O
for	O
38	O
%	O
of	O
the	O
samples	O
for	O
which	O
the	O
initially	O
constructed	O
graph	O
was	O
incorrect	O
and	O
was	O
refined	O
.	O
Moreover	O
,	O
we	O
see	O
in	O
the	O
previous	O
section	O
that	O
humanerror	O
graphs	O
are	O
the	O
best	O
negative	O
candidates	O
for	O
contrastive	B-MethodName
learning	I-MethodName
(	O
which	O
is	O
intuitive	O
since	O
tricky	O
and	O
subtle	O
errors	O
made	O
by	O
expert	O
human	O
annotators	O
would	O
make	O
for	O
some	O
of	O
the	O
hardest	O
negatives	O
/	O
distractors	O
for	O
a	O
contrastive	B-MethodName
learning	I-MethodName
model	O
to	O
learn	O
from	O
)	O
.	O
Hence	O
,	O
in	O
this	O
final	O
section	O
,	O
we	O
further	O
explore	O
whether	O
it	O
is	O
also	O
possible	O
to	O
automatically	O
imitate	O
and	O
generate	O
more	O
of	O
such	O
harder	O
humanlike	O
incorrect	O
graphs	O
for	O
the	O
remaining	O
samples	O
as	O
well	O
.	O
Our	O
method	O
consists	O
of	O
the	O
following	O
steps	O
.	O
Human	O
-	O
like	O
Negative	O
Edge	O
Generation	O
.	O
We	O
first	O
fine	O
-	O
tune	O
a	O
T5	B-MethodName
model	O
that	O
conditions	O
on	O
the	O
belief	O
,	O
argument	O
and	O
the	O
stance	O
to	O
generate	O
a	O
set	O
of	O
incorrect	O
edges	O
(	O
which	O
is	O
the	O
set	O
of	O
edges	O
that	O
are	O
present	O
in	O
the	O
incorrect	O
graph	O
and	O
not	O
in	O
the	O
refined	O
graph	O
)	O
.	O
Human	O
-	O
like	O
Negative	O
Graph	B-TaskName
Construction	I-TaskName
.	O
This	O
generated	O
set	O
of	O
incorrect	O
edges	O
is	O
then	O
added	O
to	O
the	O
correct	O
graph	O
to	O
construct	O
the	O
incorrect	O
graph	O
,	O
such	O
that	O
it	O
is	O
structurally	O
correct	O
and	O
hence	O
representative	O
of	O
human	O
-	O
like	O
erroneous	O
graphs	O
.	O
Filtering	O
High	O
-	O
quality	O
Negative	O
Graphs	O
.	O
Con	O
-	O
trastive	O
models	O
will	O
only	O
benefit	O
from	O
these	O
negatives	O
if	O
the	O
negative	O
edge	O
generation	O
model	O
is	O
accurate	O
and	O
generates	O
edges	O
that	O
are	O
actually	O
incorrect	O
.	O
Hence	O
,	O
we	O
control	O
the	O
quality	O
of	O
the	O
generated	O
incorrect	O
graphs	O
by	O
the	O
following	O
two	O
techniques	O
-	O
(	O
a	O
)	O
Thresholding	O
via	O
fraction	O
of	O
Acceptable	O
Edges	O
(	O
AE	B-MethodName
)	O
:	O
We	O
say	O
that	O
a	O
generated	O
incorrect	O
edge	O
is	O
acceptable	O
if	O
it	O
is	O
not	O
part	O
of	O
the	O
correct	O
graph	O
and	O
can	O
be	O
added	O
to	O
the	O
correct	O
graph	O
without	O
violating	O
any	O
structural	O
constraints	O
.	O
We	O
compute	O
the	O
fraction	O
of	O
acceptable	O
edges	O
for	O
every	O
generated	O
negative	O
graph	O
and	O
choose	O
only	O
those	O
graphs	O
with	O
AE	B-MethodName
above	O
a	O
certain	O
threshold	O
δ	B-HyperparameterName
.	O
Intuitively	O
,	O
this	O
ensures	O
that	O
a	O
high	O
fraction	O
of	O
the	O
generated	O
edges	O
are	O
actually	O
incorrect	O
and	O
hence	O
when	O
added	O
to	O
the	O
correct	O
graph	O
,	O
will	O
lead	O
to	O
a	O
sufficiently	O
different	O
(	O
human	O
-	O
like	O
)	O
incorrect	O
graph	O
.	O
(	O
b	O
)	O
Thresholding	O
via	O
Incorrect	O
Probability	O
of	O
a	O
graph	O
(	O
IP	O
)	O
:	O
We	O
use	O
our	O
SeCA	O
metric	O
model	O
(	O
that	O
classifies	O
a	O
graph	O
into	O
support	O
,	O
counter	O
,	O
or	O
incorrect	O
class	O
)	O
to	O
compute	O
the	O
probability	O
of	O
the	O
generated	O
graph	O
being	O
incorrect	O
and	O
choose	O
those	O
graphs	O
that	O
are	O
above	O
a	O
certain	O
threshold	O
γ	B-HyperparameterName
of	O
incorrect	O
probability	O
.	O
We	O
set	O
δ	B-HyperparameterName
=	O
0.4	O
and	O
γ	B-HyperparameterName
=	O
0.5	O
(	O
tuned	O
on	O
the	O
dev	O
set	O
)	O
and	O
train	O
the	O
Max	O
-	O
margin	O
model	O
using	O
these	O
additionally	O
generated	O
human	O
-	O
like	O
negative	O
graphs	O
.	O
As	O
shown	O
in	O
Table	O
5	O
both	O
thresholding	O
approaches	O
lead	O
to	O
further	O
improvements	O
over	O
using	O
just	O
the	O
human	O
-	O
created	O
negative	O
graphs	O
.	O
These	O
initial	O
promising	O
results	O
for	O
emulating	O
hard	O
/	O
tricky	O
human	O
errors	O
as	O
strong	O
negatives	O
for	O
contrastive	B-MethodName
learning	I-MethodName
will	O
hopefully	O
lead	O
to	O
further	O
future	O
work	O
in	O
this	O
interesting	O
direction	O
.	O

From	O
an	O
ethics	O
standpoint	O
,	O
we	O
provide	O
a	O
brief	O
overview	O
and	O
show	O
samples	O
from	O
the	O
datasets	O
that	O
our	O
models	O
are	O
trained	O
on	O
throughout	O
the	O
paper	O
and	O
also	O
in	O
the	O
Appendix	O
.	O
Explanation	O
graph	B-TaskName
generation	I-TaskName
improves	O
the	O
interpretability	O
of	O
neural	O
commonsense	O
reasoning	O
systems	O
and	O
could	O
prove	O
to	O
be	O
effective	O
in	O
understanding	O
and	O
debugging	O
such	O
models	O
.	O
Hence	O
we	O
do	O
not	O
foresee	O
any	O
major	O
risks	O
or	O
negative	O
societal	O
impact	O
of	O
our	O
work	O
.	O
However	O
,	O
like	O
any	O
other	O
ML	O
model	O
,	O
the	O
graphs	O
generated	O
by	O
our	O
models	O
may	O
not	O
always	O
be	O
completely	O
accurate	O
and	O
hence	O
should	O
be	O
used	O
with	O
caution	O
for	O
real	O
-	O
world	O
applications	O
.	O
et	O
al	O
,	O
2019	O
)	O
classifier	O
that	O
given	O
a	O
belief	O
and	O
a	O
generated	O
explanation	O
graph	O
,	O
infers	O
whether	O
the	O
graph	O
supports	O
the	O
belief	O
,	O
counters	O
the	O
belief	O
or	O
is	O
incorrect	O
(	O
because	O
of	O
incoherent	O
edges	O
)	O
.	O
If	O
it	O
predicts	O
support	O
or	O
counter	O
and	O
this	O
stance	O
matches	O
the	O
gold	O
stance	O
,	O
then	O
the	O
graph	O
is	O
considered	O
semantically	O
correct	O
.	O
In	O
essense	O
,	O
SeCA	O
works	O
on	O
the	O
principle	O
that	O
an	O
explanation	O
graph	O
is	O
semantically	O
correct	O
if	O
a	O
stance	O
can	O
be	O
unambiguously	O
inferred	O
from	O
it	O
(	O
by	O
a	O
model	O
in	O
this	O
case	O
or	O
a	O
human	O
)	O
and	O
that	O
stance	O
is	O
the	O
same	O
as	O
the	O
gold	O
stance	O
.	O
Note	O
that	O
SeCA	O
is	O
a	O
reference	O
-	O
free	O
metric	O
(	O
does	O
not	O
use	O
the	O
groundtruth	O
graph	O
)	O
and	O
hence	O
is	O
invariant	O
to	O
structural	O
variations	O
in	O
explanation	O
graphs	O
.	O
Graph	O
-	O
BertScore	O
(	O
G	O
-	O
BS	O
)	O
.	O
Graph	O
-	O
BertScore	O
is	O
an	O
extension	O
of	O
BertScore	O
for	O
computing	O
the	O
degree	O
of	O
match	O
between	O
the	O
predicted	O
graphs	O
and	O
the	O
ground	O
-	O
truth	O
graphs	O
.	O
It	O
treats	O
a	O
graph	O
as	O
a	O
set	O
of	O
edges	O
and	O
computes	O
the	O
best	O
match	O
between	O
the	O
gold	O
edges	O
and	O
the	O
predicted	O
edges	O
,	O
where	O
the	O
matching	O
score	O
between	O
a	O
pair	O
of	O
edges	O
is	O
given	O
by	O
the	O
BertScore	O
F1	B-MetricName
.	O
Graph	O
Edit	O
Distance	O
(	O
GED	B-DatasetName
)	O
.	O
GED	B-DatasetName
is	O
the	O
standard	O
Graph	O
Edit	O
Distance	O
for	O
graphs	O
,	O
measuring	O
the	O
number	O
of	O
edit	O
operations	O
(	O
addition	O
,	O
deletion	O
,	O
and	O
replacement	O
of	O
nodes	O
and	O
edges	O
)	O
to	O
transform	O
one	O
graph	O
to	O
the	O
other	O
and	O
further	O
normalized	O
by	O
an	O
appropriate	O
normalizing	O
constant	O
.	O
Edge	O
Accuracy	B-MetricName
(	O
EA	O
)	O
.	O
The	O
final	O
metric	O
,	O
Edge	O
Accuracy	B-MetricName
(	O
EA	O
)	O
measures	O
the	O
fraction	O
of	O
edges	O
in	O
the	O
graph	O
that	O
are	O
important	O
.	O
An	O
edge	O
is	O
considered	O
important	O
if	O
removing	O
it	O
from	O
the	O
graph	O
leads	O
to	O
a	O
drop	O
in	O
the	O
gold	O
stance	O
prediction	O
confidence	O
.	O

Table	O
7	O
shows	O
the	O
number	O
of	O
train	O
,	O
validation	O
and	O
test	O
samples	O
of	O
the	O
two	O
datasets	O
we	O
experiment	O
with	O
.	O
We	O
build	O
our	O
models	O
on	O
top	O
of	O
the	O
Hugging	O
Face	O
transformers	O
library	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O
6	O
All	O
models	O
for	O
the	O
ExplaGraphs	O
dataset	O
7	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
are	O
trained	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8	O
and	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3	O
*	O
10	O
−5	O
for	O
a	O
maximum	O
of	O
15	O
epochs	O
.	O
The	O
maximum	O
input	O
and	O
output	O
sequence	O
lengths	O
are	O
both	O
set	O
to	O
150	O
.	O
For	O
the	O
max	O
-	O
margin	O
graph	B-TaskName
generation	I-TaskName
model	O
,	O
we	O
set	O
both	O
the	O
hyperparameters	O
α	B-HyperparameterName
(	O
mixing	O
ratio	O
)	O
and	O
β	B-HyperparameterName
(	O
margin	O
)	O
to	O
1.0	O
while	O
for	O
the	O
contrastive	O
graph	B-TaskName
generation	I-TaskName
model	O
,	O
we	O
set	O
α	B-HyperparameterName
to	O
0.1	O
.	O
For	O
the	O
temporal	O
graph	B-TaskName
generation	I-TaskName
task	O
8	O
(	O
Madaan	O
and	O
Yang	O
,	O
2021	O
)	O
,	O
we	O
train	O
all	O
models	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4	O
and	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3	O
*	O
10	O
−5	O
for	O
a	O
maximum	O
of	O
10	O
epochs	O
.	O
The	O
maximum	O
input	O
and	O
output	O
sequence	O
lengths	O
are	O
set	O
to	O
512	O
and	O
256	O
respectively	O
.	O
On	O
this	O
task	O
,	O
the	O
hyperparameters	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
for	O
the	O
max	O
-	O
margin	O
model	O
are	O
again	O
set	O
to	O
1.0	O
while	O
for	O
the	O
contrastive	O
graph	B-TaskName
generation	I-TaskName
model	O
,	O
we	O
set	O
α	B-HyperparameterName
to	O
0.2	O
.	O
Across	O
all	O
models	O
and	O
tasks	O
,	O
graphs	O
are	O
generated	O
using	O
beam	O
search	O
decoding	O
with	O
a	O
beam	O
size	O
of	O
4	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
and	O
learning	B-HyperparameterName
rate	I-HyperparameterName
are	O
manually	O
tuned	O
in	O
the	O
range	O
{	O
4	O
,	O
8	O
,	O
16	O
}	O
and	O
{	O
10	O
−5	O
,	O
2	O
*	O
10	O
−5	O
,	O
3	O
*	O
10	O
−5	O
}	O
respectively	O
and	O
the	O
best	O
models	O
are	O
chosen	O
based	O
on	O
the	O
respective	O
validation	O
set	O
performance	O
.	O
Similarly	O
,	O
the	O
mixing	O
ratio	O
hyperparameter	O
α	B-HyperparameterName
is	O
manually	O
tuned	O
in	O
the	O
range	O

Below	O
we	O
provide	O
brief	O
descriptions	O
of	O
the	O
evaluation	O
metrics	O
used	O
for	O
the	O
ExplaGraphs	O
task	O
.	O
For	O
further	O
details	O
,	O
we	O
refer	O
readers	O
to	O
prior	O
work	O
(	O
Saha	O
et	O
al	O
,	O
2021b	O
)	O
.	O
Structural	O
Correctness	O
Accuracy	B-MetricName
of	O
Graphs	O
(	O
StCA	O
)	O
.	O
It	O
computes	O
the	O
fraction	O
of	O
graphs	O
where	O
all	O
the	O
structural	O
constraints	O
are	O
satisfied	O
.	O
(	O
SeCA	O
)	O
.	O
SeCA	O
is	O
a	O
model	O
-	O
based	O
metric	O
that	O
computes	O
the	O
fraction	O
of	O
graphs	O
that	O
are	O
both	O
structurally	O
and	O
semantically	O
correct	O
.	O
For	O
computing	O
SeCA	O
,	O
prior	O
work	O
trains	O
a	O
3	O
-	O
way	O
RoBERTa	B-MethodName
(	O
Liu	O
Figure	O
6	O
:	O
Example	O
of	O
explanation	O
graphs	O
generated	O
by	O
different	O
models	O
.	O
The	O
baseline	O
T5	B-MethodName
-	O
generated	O
graph	O
is	O
semantically	O
incorrect	O
(	O
incoherent	O
relations	O
marked	O
in	O
dashed	O
red	O
)	O
while	O
our	O
proposed	O
models	O
generate	O
both	O
structurally	O
and	O
semantically	O
correct	O
graphs	O
.	O

During	O
the	O
time	O
of	O
writing	O
this	O
paper	O
the	O
dataset	O
included	O
25	O
,	O
512	O
,	O
320	O
abstracts	O
from	O
PubMed	O
and	O
1	O
,	O
350	O
,	O
119	O
full	O
articles	O
from	O
PMCOA	O
,	O
resulting	O
in	O
155	O
,	O
356	O
,	O
970	O
and	O
232	O
,	O
838	O
,	O
618	O
sentences	O
respectively	O
.	O
These	O
numbers	O
are	O
not	O
identical	O
to	O
the	O
ones	O
reported	O
by	O
NCBI	O
for	O
couple	O
of	O
reasons	O
.	O
Firstly	O
,	O
at	O
the	O
moment	O
,	O
we	O
do	O
not	O
process	O
the	O
deletion	O
updates	O
nor	O
do	O
we	O
remove	O
the	O
old	O
versions	O
of	O
PM	O
-	O
COA	O
articles	O
if	O
they	O
are	O
revised	O
,	O
i.e.	O
our	O
dataset	O
may	O
include	O
articles	O
,	O
which	O
have	O
been	O
retracted	O
and	O
an	O
article	O
may	O
be	O
included	O
multiple	O
times	O
if	O
The	O
main	O
processing	O
steps	O
of	O
the	O
pipeline	O
.	O
First	O
,	O
the	O
articles	O
are	O
downloaded	O
from	O
the	O
source	O
and	O
filtered	O
to	O
prevent	O
reprocessing	O
old	O
documents	O
.	O
The	O
documents	O
are	O
then	O
converted	O
to	O
plain	O
text	O
format	O
.	O
This	O
text	O
data	O
is	O
split	O
to	O
independent	O
sentences	O
,	O
tokenized	O
and	O
tagged	O
with	O
POS	O
labels	O
and	O
syntactic	O
dependencies	O
.	O
In	O
addition	O
,	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
for	O
several	O
entity	O
types	O
is	O
carried	O
out	O
.	O
the	O
content	O
has	O
been	O
modified	O
.	O
We	O
plan	O
to	O
take	O
the	O
deletions	O
into	O
account	O
in	O
near	O
future	O
.	O
Secondly	O
,	O
the	O
external	O
tools	O
in	O
our	O
pipeline	O
may	O
occasionally	O
fail	O
,	O
in	O
which	O
case	O
some	O
of	O
the	O
articles	O
are	O
not	O
processed	O
.	O
Since	O
the	O
pipeline	O
processes	O
the	O
input	O
data	O
in	O
batches	O
,	O
a	O
critical	O
error	O
may	O
lead	O
to	O
a	O
whole	O
batch	O
not	O
being	O
processed	O
.	O
We	O
are	O
currently	O
improving	O
the	O
pipeline	O
to	O
automatically	O
reprocess	O
the	O
failed	O
batches	O
with	O
the	O
problematic	O
articles	O
excluded	O
to	O
minimize	O
the	O
loss	B-MetricName
of	O
data	O
.	O
Running	O
the	O
parsing	O
pipeline	O
,	O
including	O
tokenization	O
,	O
POS	O
tagging	O
and	O
conversion	O
to	O
the	O
collapsed	O
Stanford	O
scheme	O
,	O
is	O
the	O
most	O
time	O
consuming	O
part	O
of	O
the	O
whole	O
pipeline	O
.	O
Execution	O
of	O
this	O
step	O
has	O
taken	O
84	O
,	O
552	O
CPU	O
hours	O
(	O
9.6	O
CPU	O
years	O
)	O
for	O
the	O
currently	O
available	O
data	O
.	O
Unfortunately	O
we	O
do	O
not	O
have	O
exact	O
processing	O
time	O
statistics	O
for	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
and	O
thus	O
estimate	O
its	O
computational	O
requirements	O
by	O
extrapolating	O
from	O
a	O
smaller	O
test	O
run	O
.	O
Based	O
on	O
this	O
experiment	O
NER	B-TaskName
has	O
demanded	O
4	O
,	O
100	O
CPU	O
hours	O
thus	O
far	O
.	O
The	O
text	O
preprocessing	O
and	O
sentence	O
splitting	O
steps	O
are	O
negligible	O
and	O
thus	O
the	O
overall	O
processing	O
time	O
required	O
is	O
approximately	O
10	O
CPU	O
years	O
.	O
In	O
total	O
,	O
our	O
processing	O
pipeline	O
has	O
detected	O
526	O
,	O
175	O
,	O
528	O
named	O
entities	O
.	O
GGPs	O
are	O
the	O
most	O
common	O
entities	O
,	O
covering	O
36.2	O
%	O
of	O
all	O
entity	O
mentions	O
,	O
whereas	O
the	O
cell	O
lines	O
are	O
the	O
most	O
infrequent	O
,	O
forming	O
only	O
1.3	O
%	O
of	O
the	O
data	O
.	O
The	O
entity	O
type	O
specific	O
statistics	O
along	O
with	O
the	O
most	O
common	O
entity	O
spans	O
are	O
listed	O
in	O
Table	O
2	O
.	O

The	O
normalization	O
of	O
Microorganism	O
entities	O
component	O
of	O
our	O
system	O
is	O
based	O
on	O
exact	O
matching	O
against	O
the	O
names	O
and	O
synonyms	O
of	O
the	O
concepts	O
in	O
the	O
NCBI	O
taxonomy	O
.	O
Error	B-MetricName
analysis	O
on	O
the	O
training	O
and	O
developments	O
data	O
sets	O
revealed	O
that	O
applying	O
some	O
rules	O
may	O
improve	O
the	O
results	O
.	O
For	O
instance	O
,	O
"	O
Escherichia	O
coli	O
"	O
has	O
an	O
exact	B-MetricName
match	I-MetricName
that	O
can	O
be	O
successfully	O
normalized	O
to	O
the	O
referent	O
concept	O
with	O
an	O
ID	O
"	O
562	O
"	O
in	O
the	O
NCBI	O
taxonomy	O
.	O
In	O
the	O
following	O
parts	O
of	O
the	O
document	O
,	O
although	O
the	O
"	O
E.	O
coli	O
"	O
mention	O
indicates	O
a	O
clear	O
reference	O
to	O
the	O
same	O
concept	O
,	O
it	O
can	O
not	O
be	O
normalized	O
to	O
the	O
"	O
Escherichia	O
coli	O
"	O
concept	O
with	O
an	O
exact	O
matching	O
approach	O
.	O
In	O
this	O
kind	O
of	O
cases	O
,	O
if	O
an	O
exact	B-MetricName
match	I-MetricName
does	O
not	O
exist	O
,	O
the	O
previously	O
mentioned	O
similar	O
entities	O
in	O
the	O
text	O
are	O
searched	O
.	O
If	O
a	O
match	O
is	O
found	O
,	O
the	O
same	O
concept	O
is	O
assigned	O
as	O
the	O
normalized	O
concept	O
for	O
the	O
corresponding	O
mention	O
"	O
E.	O
coli	O
"	O
.	O
If	O
there	O
does	O
not	O
exist	O
a	O
match	O
with	O
the	O
previously	O
normalized	O
concepts	O
,	O
the	O
root	O
concept	O
with	O
an	O
ID	O
"	O
2	O
"	O
is	O
assigned	O
.	O
4	O
Relation	B-TaskName
Extraction	I-TaskName

In	O
the	O
BioNLP	O
Shared	O
Task	O
2019	O
Bacteria	O
Biotopes	O
normalization	O
sub	O
-	O
task	O
,	O
entities	O
are	O
given	O
with	O
their	O
boundaries	O
in	O
the	O
text	O
and	O
the	O
participants	O
are	O
required	O
to	O
predict	O
the	O
normalization	O
of	O
the	O
entities	O
.	O
In	O
the	O
official	O
evaluation	O
,	O
for	O
each	O
normalized	O
Habitat	O
/	O
Phenotype	O
entity	O
,	O
Wang	O
similarity	O
W	O
(	O
Wang	O
et	O
al	O
,	O
2007	O
)	O
is	O
calculated	O
to	O
measure	O
the	O
similarity	O
between	O
the	O
reference	O
concept	O
and	O
the	O
predicted	O
concept	O
for	O
the	O
normalization	O
.	O
The	O
performances	O
of	O
the	O
submitted	O
systems	O
are	O
evaluated	O
with	O
their	O
Precision	B-MetricName
values	O
,	O
which	O
are	O
calculated	O
as	O
:	O
Precision	B-MetricName
=	O
S	O
p	O
/	O
N	O
(	O
2	O
)	O
where	O
S	O
p	O
indicates	O
the	O
total	O
Wang	O
similarity	O
W	O
for	O
all	O
predictions	O
(	O
Deleger	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
N	O
is	O
the	O
number	O
of	O
predicted	O
entities	O
.	O
In	O
the	O
BioNLP	O
Shared	O
Task	O
2019	O
Bacteria	O
Biotopes	O
relation	B-TaskName
extraction	I-TaskName
sub	O
-	O
task	O
,	O
entities	O
are	O
given	O
with	O
their	O
boundaries	O
in	O
the	O
text	O
and	O
the	O
participants	O
are	O
asked	O
to	O
predict	O
the	O
relations	O
between	O
the	O
entities	O
.	O
The	O
performances	O
of	O
the	O
submitted	O
systems	O
are	O
evaluated	O
with	O
their	O
F1	B-MetricName
(	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
)	O
,	O
recall	O
and	O
precision	O
values	O
.	O

The	O
official	O
results	O
obtained	O
by	O
our	O
system	O
and	O
the	O
other	O
participants	O
for	O
the	O
BB	O
-	O
norm	O
sub	O
-	O
task	O
are	O
shown	O
in	O
Table	O
3	O
.	O
Our	O
system	O
(	O
BOUN	O
-	O
ISIK	O
-	O
2	O
)	O
achieved	O
the	O
best	O
performance	O
with	O
67.9	O
%	O
Precision	B-MetricName
in	O
the	O
BB	O
-	O
norm	O
sub	O
-	O
task	O
(	O
Entity	O
Normalization	O
)	O
.	O

We	O
use	O
area	O
under	O
the	O
ROC	O
curve	O
(	O
AUC	B-MetricName
)	O
as	O
the	O
primary	O
evaluation	O
metric	O
for	O
SLA	O
modeling	O
(	O
Fawcett	O
,	O
2006	O
)	O
.	O
AUC	B-MetricName
is	O
a	O
common	O
measure	O
of	O
ranking	O
quality	O
in	O
classification	O
tasks	O
,	O
and	O
can	O
be	O
interpreted	O
as	O
the	O
probability	O
that	O
the	O
system	O
will	O
rank	O
a	O
randomly	O
-	O
chosen	O
error	O
above	O
a	O
randomlychosen	O
non	O
-	O
error	O
.	O
We	O
argue	O
that	O
this	O
notion	O
of	O
ranking	O
quality	O
is	O
particularly	O
useful	O
for	O
evaluating	O
systems	O
that	O
might	O
be	O
used	O
for	O
personalized	O
learning	O
,	O
e.g.	O
,	O
if	O
we	O
wish	O
to	O
prioritize	O
words	O
or	O
exercises	O
for	O
an	O
individual	O
learner	O
's	O
review	O
based	O
on	O
how	O
likely	O
they	O
are	O
to	O
have	O
forgotten	O
or	O
make	O
errors	O
at	O
a	O
given	O
point	O
in	O
time	O
.	O
We	O
also	O
report	O
F1	B-MetricName
score	I-MetricName
-	O
the	O
harmonic	O
mean	O
of	O
precision	O
and	O
recall	O
-	O
as	O
a	O
secondary	O
metric	O
,	O
since	O
it	O
is	O
more	O
common	O
in	O
similar	O
skewed	O
-	O
class	O
labeling	O
tasks	O
(	O
e.g.	O
,	O
Ng	O
et	O
al	O
,	O
2013	O
)	O
.	O
Note	O
,	O
however	O
,	O
that	O
F1	B-MetricName
can	O
be	O
significantly	O
improved	O
simply	O
by	O
tuning	O
the	O
classification	B-HyperparameterName
threshold	I-HyperparameterName
(	O
fixed	O
at	O
0.5	O
for	O
our	O
evaluations	O
)	O
without	O
affecting	O
AUC	B-MetricName
.	O

A	O
total	O
of	O
15	O
teams	O
participated	O
in	O
the	O
task	O
,	O
of	O
which	O
13	O
responded	O
to	O
a	O
brief	O
survey	O
about	O
their	O
approach	O
,	O
and	O
11	O
submitted	O
system	O
description	O
papers	O
.	O
All	O
but	O
two	O
of	O
these	O
teams	O
submitted	O
predictions	O
for	O
all	O
three	O
language	O
tracks	O
.	O
Official	O
shared	O
task	O
results	O
are	O
reported	O
in	O
Table	O
2	O
.	O
System	O
ranks	O
are	O
determined	O
by	O
sorting	O
teams	O
according	O
to	O
AUC	B-MetricName
,	O
and	O
using	O
DeLong	O
's	O
test	O
(	O
DeLong	O
et	O
al	O
,	O
1988	O
)	O
to	O
identify	O
statistical	O
ties	O
.	O
For	O
the	O
remainder	O
of	O
this	O
section	O
,	O
we	O
provide	O
a	O
summary	O
of	O
each	O
team	O
's	O
approach	O
,	O
ordered	O
by	O
the	O
team	O
's	O
average	O
rank	O
across	O
all	O
three	O
tracks	O
.	O
Certain	O
teams	O
are	O
marked	O
with	O
modeling	O
choice	O
indicators	O
(	O
♢	O
,	O
♣	O
,	O
‡	O
)	O
,	O
which	O
we	O
discuss	O
further	O
in	O
5	O
.	O
SanaLabs	O
(	O
Nilsson	O
et	O
al	O
,	O
2018	O
)	O
used	O
a	O
combination	O
of	O
recurrent	O
neural	O
network	O
(	O
RNN	O
)	O
predictions	O
with	O
those	O
of	O
a	O
Gradient	O
Boosted	O
Decision	O
Tree	O
(	O
GBDT	O
)	O
ensemble	O
,	O
trained	O
independently	O
for	O
each	O
track	O
.	O
This	O
was	O
motivated	O
by	O
the	O
observation	O
that	O
RNNs	O
work	O
well	O
for	O
sequence	O
data	O
,	O
while	O
GBDTs	O
are	O
often	O
the	O
best	O
-	O
performing	O
non	O
-	O
neural	O
model	O
for	O
shared	O
tasks	O
using	O
tabular	O
data	O
.	O
They	O
also	O
engineered	O
several	O
token	O
context	O
features	O
,	O
and	O
learner	O
/	O
token	O
history	O
features	O
such	O
as	O
number	O
of	O
times	O
seen	O
,	O
time	O
since	O
last	O
practice	O
,	O
etc	O
.	O
singsound	O
(	O
Xu	O
et	O
al	O
,	O
2018	O
)	O
used	O
an	O
RNN	O
architecture	O
using	O
four	O
types	O
of	O
encoders	O
,	O
representing	O
different	O
types	O
of	O
features	O
:	O
token	O
context	O
,	O
linguistic	O
information	O
,	O
user	O
data	O
,	O
and	O
exercise	O
format	O
.	O
The	O
RNN	O
decoder	O
integrated	O
information	O
from	O
all	O
four	O
encoders	O
.	O
Ablation	O
experiments	O
revealed	O
the	O
context	O
encoder	O
(	O
representing	O
the	O
token	O
)	O
contributed	O
the	O
most	O
to	O
model	O
performance	O
,	O
while	O
the	O
linguistic	O
encoder	O
(	O
representing	O
grammatical	O
information	O
)	O
contributed	O
the	O
least	O
.	O
NYU	O
(	O
Rich	O
et	O
al	O
,	O
2018	O
)	O
used	O
an	O
ensemble	O
of	O
GBDTs	O
with	O
features	O
engineered	O
based	O
on	O
psychological	O
theories	O
of	O
cognition	O
.	O
Predictions	O
for	O
each	O
track	O
were	O
averaged	O
between	O
a	O
track	O
-	O
specific	O
model	O
and	O
a	O
unified	O
model	O
(	O
trained	O
on	O
data	O
from	O
all	O
three	O
tracks	O
)	O
.	O
In	O
addition	O
to	O
the	O
word	O
,	O
user	O
,	O
and	O
exercise	O
features	O
provided	O
,	O
the	O
authors	O
included	O
word	O
lemmas	O
,	O
corpus	O
frequency	O
,	O
L1	O
-	O
L2	O
cognates	O
,	O
and	O
features	O
indicating	O
user	O
motivation	O
and	O
diligence	O
(	O
derived	O
from	O
usage	O
patterns	O
)	O
,	O
and	O
others	O
.	O
Ablation	O
studies	O
indicated	O
that	O
most	O
of	O
the	O
performance	O
was	O
due	O
to	O
the	O
user	O
and	O
token	O
features	O
.	O
TMU	O
(	O
Kaneko	O
et	O
al	O
,	O
2018	O
)	O
used	O
a	O
combination	O
of	O
two	O
bidirectional	O
RNNs	O
-	O
the	O
first	O
to	O
predict	O
potential	O
user	O
errors	O
at	O
a	O
given	O
token	O
,	O
and	O
a	O
second	O
to	O
track	O
the	O
history	O
of	O
previous	O
answers	O
by	O
each	O
user	O
.	O
These	O
networks	O
were	O
jointly	O
trained	O
through	O
a	O
unified	O
objective	O
function	O
.	O
The	O
authors	O
did	O
not	O
engineer	O
any	O
additional	O
features	O
,	O
but	O
did	O
train	O
a	O
single	O
model	O
for	O
all	O
three	O
tracks	O
(	O
using	O
a	O
track	O
ID	O
feature	O
to	O
distinguish	O
among	O
them	O
)	O
.	O
CECL	O
(	O
Bestgen	O
,	O
2018	O
)	O
used	O
a	O
logistic	B-MethodName
regression	I-MethodName
approach	O
.	O
The	O
base	O
feature	O
set	O
was	O
expanded	O
to	O
include	O
many	O
feature	O
conjunctions	O
,	O
including	O
word	O
n	O
-	O
grams	O
crossed	O
with	O
the	O
token	O
,	O
user	O
,	O
format	O
,	O
and	O
session	O
features	O
provided	O
with	O
the	O
data	O
set	O
.	O
Cambridge	B-DatasetName
(	O
Yuan	O
,	O
2018	O
)	O
trained	O
two	O
RNNsa	O
sequence	O
labeler	O
,	O
and	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
taking	O
into	O
account	O
previous	O
answers	O
-	O
and	O
found	O
that	O
averaging	O
their	O
predictions	O
yielded	O
the	O
best	O
results	O
.	O
They	O
focused	O
on	O
the	O
English	O
track	O
,	O
experimenting	O
with	O
additional	O
features	O
derived	O
from	O
other	O
English	O
learner	O
corpora	O
.	O
Hyper	O
-	O
parameters	O
were	O
tuned	O
for	O
English	O
and	O
used	O
as	O
-	O
is	O
for	O
other	O
tracks	O
,	O
with	O
comparable	O
results	O
.	O
UCSD	O
(	O
Tomoschuk	O
and	O
Lovelett	O
,	O
2018	O
)	O
used	O
a	O
random	O
forest	O
classifier	O
with	O
a	O
set	O
of	O
engineered	O
features	O
motivated	O
by	O
previous	O
research	O
in	O
memory	O
and	O
linguistic	O
effects	O
in	O
SLA	O
,	O
including	O
"	O
word	O
neighborhoods	O
,	O
"	O
corpus	O
frequency	O
,	O
cognates	O
,	O
and	O
repetition	O
/	O
experience	O
with	O
a	O
given	O
word	O
.	O
The	O
system	O
also	O
included	O
features	O
specific	O
to	O
each	O
user	O
,	O
such	O
as	O
mean	O
and	O
variance	O
of	O
error	O
rates	O
.	O
LambdaLab	O
used	O
GBDT	O
models	O
independently	O
for	O
each	O
track	O
,	O
deriving	O
their	O
features	O
from	O
confirmatory	O
analysis	O
of	O
psychologically	O
-	O
motivated	O
hypotheses	O
on	O
the	O
TRAIN	O
set	O
.	O
These	O
include	O
proxies	O
for	O
student	O
engagement	O
,	O
spacing	O
effect	O
,	O
response	O
time	O
,	O
etc	O
.	O
nihalnayak	O
(	O
Nayak	O
and	O
Rao	O
,	O
2018	O
)	O
used	O
a	O
logistic	B-MethodName
regression	I-MethodName
model	O
similar	O
to	O
the	O
baseline	O
,	O
but	O
added	O
features	O
inspired	O
by	O
research	O
in	O
codemixed	O
language	O
-	O
learning	O
where	O
context	O
plays	O
an	O
important	O
role	O
.	O
In	O
particular	O
,	O
they	O
included	O
word	O
,	O
part	O
of	O
speech	O
,	O
and	O
metaphone	O
features	O
for	O
previous	O
:	O
current	O
and	O
current	O
:	O
next	O
token	O
pairs	O
.	O
Grotoco	O
(	O
Klerke	O
et	O
al	O
,	O
2018	O
)	O
also	O
used	O
logistic	B-MethodName
regression	I-MethodName
,	O
including	O
word	O
lemmas	O
,	O
frequency	O
,	O
cognates	O
,	O
and	O
user	O
-	O
specific	O
features	O
such	O
as	O
word	O
error	O
rate	O
.	O
Interestingly	O
,	O
the	O
authors	O
found	O
that	O
ignoring	O
each	O
user	O
's	O
first	O
day	O
of	O
exercise	O
data	O
improved	O
their	O
predictions	O
,	O
suggesting	O
that	O
learners	O
first	O
needed	O
to	O
familiarize	O
themselves	O
with	O
app	O
before	O
their	O
data	O
were	O
reliable	O
for	O
modeling	O
.	O
jilljenn	O
(	O
Vie	O
,	O
2018	O
)	O
used	O
a	O
deep	O
factorization	O
machine	O
(	O
DeepFM	O
)	O
,	O
a	O
neural	O
architecture	O
developed	O
for	O
click	B-TaskName
-	I-TaskName
through	I-TaskName
rate	I-TaskName
prediction	I-TaskName
in	O
recommender	O
systems	O
.	O
This	O
model	O
allows	O
learning	O
from	O
both	O
lower	O
-	O
order	O
and	O
higher	O
-	O
order	O
induced	O
features	O
and	O
their	O
interactions	O
.	O
The	O
DeepFM	O
outperformed	O
a	O
simple	O
logistic	B-MethodName
regression	I-MethodName
baseline	O
without	O
much	O
additional	O
feature	B-TaskName
engineering	I-TaskName
.	O
Other	O
teams	O
did	O
not	O
submit	O
system	O
description	O
papers	O
.	O
However	O
,	O
according	O
to	O
a	O
task	O
organizer	O
survey	O
ymatusevych	O
used	O
a	O
linear	O
model	O
with	O
multilingual	O
word	B-TaskName
embeddings	I-TaskName
,	O
corpus	O
frequency	O
,	O
and	O
several	O
L1	O
-	O
L2	O
features	O
such	O
as	O
cognates	O
.	O
Additionally	O
,	O
simplelinear	O
used	O
an	O
ensemble	O
of	O
some	O
sort	O
(	O
for	O
the	O
French	O
track	O
only	O
)	O
.	O
renhk	O
and	O
zlb241	O
provided	O
no	O
details	O
about	O
their	O
systems	O
.	O
SLAM_baseline	O
is	O
the	O
baseline	O
system	O
provided	O
by	O
the	O
task	O
organizers	O
.	O
It	O
is	O
a	O
simple	O
logistic	B-MethodName
regression	I-MethodName
using	O
data	O
set	O
features	O
,	O
trained	O
separately	O
for	O
each	O
track	O
using	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
on	O
the	O
TRAIN	O
set	O
only	O
.	O

Here	O
we	O
attempt	O
to	O
answer	O
the	O
question	O
of	O
whether	O
particular	O
machine	O
learning	O
algorithms	O
have	O
a	O
significant	O
impact	O
on	O
task	O
performance	O
.	O
For	O
example	O
,	O
the	O
results	O
in	O
Table	O
2	O
suggest	O
that	O
the	O
algorithmic	O
choices	O
indicated	O
by	O
(	O
♢	O
,	O
♣	O
,	O
‡	O
)	O
are	O
particularly	O
effective	O
.	O
Is	O
this	O
actually	O
the	O
case	O
?	O
To	O
answer	O
this	O
question	O
,	O
we	O
partitioned	O
the	O
TEST	O
set	O
into	O
6.4k	O
subsets	O
(	O
one	O
for	O
each	O
learner	O
)	O
,	O
and	O
computed	O
per	O
-	O
user	O
AUC	B-MetricName
scores	O
for	O
each	O
team	O
's	O
predictions	O
(	O
83.9k	O
observations	O
total	O
)	O
.	O
We	O
also	O
coded	O
each	O
team	O
with	O
indicator	O
variables	O
to	O
describe	O
their	O
algorithmic	O
approach	O
,	O
and	O
used	O
a	O
regression	O
analysis	O
to	O
determine	O
if	O
these	O
algorithmic	O
variations	O
had	O
any	O
significant	O
effects	O
on	O
learnerspecific	O
AUC	B-MetricName
scores	O
.	O
To	O
analyze	O
this	O
properly	O
,	O
however	O
,	O
we	O
need	O
to	O
determine	O
whether	O
the	O
differences	O
among	O
modeling	O
choices	O
are	O
actually	O
meaningful	O
,	O
or	O
can	O
simply	O
be	O
explained	O
by	O
sampling	O
error	O
due	O
to	O
random	O
variations	O
among	O
users	O
,	O
teams	O
,	O
or	O
tracks	O
.	O
To	O
do	O
this	O
,	O
we	O
use	O
a	O
linear	O
mixed	O
-	O
effects	O
model	O
(	O
cf	O
.	O
,	O
Baayen	O
,	O
2008	O
,	O
Ch	O
.	O
7	O
)	O
.	O
In	O
addition	O
to	O
modeling	O
the	O
fixed	O
effects	O
of	O
the	O
various	O
learning	O
algorithms	O
,	O
we	O
can	O
also	O
model	O
the	O
random	O
effects	O
represented	O
by	O
the	O
user	O
ID	O
(	O
learners	O
may	O
vary	O
by	O
ability	O
)	O
,	O
the	O
team	O
ID	O
(	O
teams	O
may	O
differ	O
in	O
other	O
aspects	O
not	O
captured	O
by	O
our	O
schema	O
,	O
e.g.	O
,	O
the	O
hardware	O
used	O
)	O
,	O
and	O
the	O
track	O
ID	O
(	O
tracks	O
may	O
vary	O
inherently	O
in	O
difficulty	O
)	O
.	O
Table	O
3	O
presents	O
a	O
mixed	O
-	O
effects	O
analysis	O
for	O
the	O
algorithm	O
variations	O
used	O
by	O
at	O
least	O
3	O
teams	O
.	O
The	O
intercept	O
can	O
be	O
interpreted	O
as	O
the	O
"	O
average	O
"	O
AUC	B-MetricName
of	O
.786	O
.	O
Controlling	O
for	O
the	O
random	O
effects	O
of	O
user	O
(	O
which	O
exhibits	O
a	O
wide	O
standard	O
deviation	O
of	O
±.086	O
AUC	B-MetricName
)	O
,	O
team	O
(	O
±.013	O
)	O
,	O
and	O
track	O
(	O
±.011	O
)	O
,	O
three	O
of	O
the	O
algorithmic	O
choices	O
are	O
at	O
least	O
marginally	O
significant	O
(	O
p	O
<	O
.1	O
)	O
.	O
For	O
example	O
,	O
we	O
might	O
expect	O
a	O
system	O
that	O
uses	O
RNNs	O
to	O
model	O
learner	O
mastery	O
over	O
time	O
would	O
add	O
+	O
.028	O
to	O
learner	O
-	O
specific	O
AUC	B-MetricName
(	O
all	O
else	O
being	O
equal	O
)	O
.	O
Note	O
that	O
most	O
teams	O
'	O
systems	O
that	O
were	O
not	O
based	O
on	O
RNNs	O
or	O
tree	O
ensembles	O
used	O
logistic	B-MethodName
regression	I-MethodName
,	O
hence	O
the	O
"	O
linear	O
model	O
"	O
effect	O
is	O
negligible	O
(	O
effectively	O
treated	O
as	O
a	O
control	O
condition	O
in	O
the	O
analysis	O
)	O
.	O
These	O
results	O
suggest	O
two	O
key	O
insights	O
for	O
SLA	O
modeling	O
.	O
First	O
,	O
non	O
-	O
linear	O
algorithms	O
are	O
particularly	O
desirable	O
4	O
,	O
and	O
second	O
,	O
multitask	O
learning	O
approaches	O
that	O
share	O
information	O
across	O
tracks	O
(	O
i.e.	O
,	O
languages	O
)	O
are	O
also	O
effective	O
.	O

Another	O
interesting	O
research	O
question	O
is	O
:	O
what	O
is	O
the	O
upper	O
-	O
bound	O
for	O
this	O
task	O
?	O
This	O
can	O
be	O
estimated	O
by	O
treating	O
each	O
team	O
's	O
best	O
submission	O
as	O
an	O
independent	O
system	O
,	O
and	O
combining	O
the	O
results	O
using	O
ensemble	O
methods	O
in	O
a	O
variety	O
of	O
ways	O
.	O
Such	O
analyses	O
have	O
been	O
previously	O
applied	O
to	O
other	O
shared	O
task	O
challenges	O
and	O
meta	O
-	O
analyses	O
(	O
e.g.	O
,	O
Malmasi	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
oracle	O
system	O
is	O
meant	O
to	O
be	O
an	O
upperbound	O
:	O
for	O
each	O
token	O
in	O
the	O
TEST	O
set	O
,	O
the	O
oracle	O
outputs	O
the	O
team	O
prediction	O
with	O
the	O
lowest	O
error	O
for	O
that	O
particular	O
token	O
.	O
We	O
also	O
experiment	O
with	O
stacking	O
(	O
Wolpert	O
,	O
1992	O
)	O
by	O
training	O
a	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
using	O
each	O
team	O
's	O
prediction	O
as	O
an	O
input	O
feature	O
6	O
.	O
Finally	O
,	O
we	O
also	O
pool	O
system	O
predictions	O
together	O
by	O
taking	O
their	O
average	O
(	O
mean	O
)	O
.	O
Table	O
5	O
reports	O
AUC	B-MetricName
for	O
various	O
ensemble	O
methods	O
as	O
well	O
as	O
some	O
of	O
the	O
top	O
performing	O
team	O
systems	O
for	O
all	O
three	O
tracks	O
.	O
Interestingly	O
,	O
the	O
oracle	O
is	O
exceptionally	O
accurate	O
(	O
>	O
.993	O
AUC	B-MetricName
and	O
>	O
.884	O
F1	B-MetricName
,	O
not	O
shown	O
)	O
.	O
This	O
indicates	O
that	O
the	O
potential	O
upper	O
limit	O
of	O
performance	O
on	O
this	O
task	O
is	O
quite	O
high	O
,	O
since	O
there	O
exists	O
a	O
near	O
-	O
perfect	O
ranking	O
of	O
tokens	O
in	O
the	O
TEST	O
set	O
based	O
only	O
on	O
predictions	O
from	O
these	O
15	O
diverse	O
participating	O
teams	O
.	O
The	O
stacking	O
classifier	O
produces	O
significantly	O
better	O
rankings	O
than	O
any	O
of	O
the	O
constituent	O
systems	O
alone	O
,	O
while	O
the	O
average	O
(	O
over	O
all	O
teams	O
)	O
ranked	O
between	O
the	O
3rd	O
and	O
4th	O
best	O
system	O
in	O
all	O
three	O
tracks	O
.	O
Inspection	O
of	O
stacking	O
model	O
weights	O
revealed	O
that	O
it	O
largely	O
learned	O
to	O
trust	O
the	O
topperforming	O
systems	O
,	O
so	O
we	O
also	O
tried	O
simply	O
averaging	O
the	O
top	O
3	O
systems	O
for	O
each	O
track	O
,	O
and	O
this	O
method	O
was	O
statistically	O
tied	O
with	O
stacking	O
for	O
the	O
English	O
and	O
French	O
tracks	O
(	O
p	O
=	O
0.002	O
for	O
Spanish	O
)	O
.	O
Interestingly	O
,	O
the	O
highest	O
-	O
weighted	O
team	O
in	O
each	O
track	O
's	O
stacking	O
model	O
was	O
singsound	O
(	O
+2.417	O
on	O
average	O
across	O
the	O
three	O
models	O
)	O
,	O
followed	O
teams	O
and	O
learning	O
algorithms	O
.	O
It	O
would	O
be	O
interesting	O
to	O
revisit	O
these	O
ideas	O
using	O
a	O
more	O
diverse	O
and	O
longitudinal	O
data	O
set	O
in	O
the	O
future	O
.	O
To	O
support	O
ongoing	O
research	O
in	O
SLA	O
modeling	O
,	O
current	O
and	O
future	O
releases	O
of	O
our	O
data	O
set	O
will	O
be	O
publicly	O
maintained	O
online	O
at	O
:	O
https://doi.org/10.7910/DVN/8SWHNO	O
.	O

Examining	O
the	O
parallels	O
between	O
human	O
and	O
machine	O
learning	O
is	O
a	O
natural	O
way	O
for	O
us	O
to	O
better	O
understand	O
the	O
former	O
and	O
track	O
our	O
progress	O
in	O
the	O
latter	O
.	O
The	O
"	O
black	O
box	O
"	O
aspect	O
of	O
neural	O
networks	O
has	O
recently	O
inspired	O
a	O
large	O
body	O
of	O
work	O
related	O
to	O
interpretability	O
,	O
i.e.	O
understanding	O
of	O
representations	O
that	O
such	O
models	O
learn	O
.	O
In	O
NLP	O
,	O
this	O
push	O
has	O
been	O
largely	O
motivated	O
by	O
linguistic	O
questions	O
,	O
such	O
as	O
:	O
what	O
linguistic	O
properties	O
are	O
captured	O
by	O
neural	O
networks	O
?	O
and	O
to	O
what	O
extent	O
do	O
decisions	O
made	O
by	O
neural	O
models	O
reflect	O
established	O
linguistic	O
theories	O
?	O
Given	O
the	O
relative	O
recency	O
of	O
such	O
questions	O
,	O
much	O
work	O
in	O
the	O
domain	O
so	O
far	O
has	O
been	O
focused	O
on	O
the	O
context	O
of	O
models	O
in	O
isolation	O
(	O
e.g.	O
what	O
does	O
model	O
X	O
learn	O
about	O
linguistic	O
phenomenon	O
Y	O
?	O
)	O
In	O
order	O
to	O
more	O
broadly	O
understand	O
models	O
'	O
representational	O
tendencies	O
,	O
however	O
,	O
it	O
is	O
vital	O
that	O
such	O
questions	O
be	O
formed	O
not	O
only	O
with	O
other	O
models	O
in	O
mind	O
,	O
but	O
also	O
other	O
rep	O
-	O
resentational	O
methods	O
and	O
modalities	O
(	O
e.g.	O
behavioral	O
data	O
,	O
fMRI	O
measurements	O
,	O
etc	O
.	O
)	O
.	O
In	O
context	O
of	O
the	O
latter	O
concern	O
,	O
the	O
present	O
-	O
day	O
interpretability	O
toolkit	O
has	O
not	O
yet	O
been	O
able	O
to	O
afford	O
a	O
practical	O
way	O
of	O
reconciling	O
this	O
.	O
In	O
this	O
work	O
,	O
we	O
employ	O
Representational	O
Similarity	O
Analysis	O
(	O
RSA	O
)	O
as	O
a	O
simple	O
method	O
of	O
interpreting	O
neural	O
models	O
'	O
representational	O
spaces	O
as	O
they	O
relate	O
to	O
other	O
models	O
and	O
modalities	O
.	O
In	O
particular	O
,	O
we	O
conduct	O
an	O
experiment	O
wherein	O
we	O
investigate	O
the	O
correspondence	O
between	O
human	O
processing	O
difficulty	O
(	O
as	O
reflected	O
by	O
gaze	O
fixation	O
measurements	O
)	O
and	O
the	O
representations	O
induced	O
by	O
popular	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
.	O
In	O
our	O
experiments	O
,	O
we	O
hypothesize	O
that	O
there	O
exists	O
an	O
overlap	O
between	O
the	O
sentences	O
which	O
are	O
difficult	O
for	O
humans	O
to	O
process	O
and	O
those	O
for	O
which	O
per	O
-	O
layer	O
encoder	O
representations	O
are	O
least	O
correlated	O
.	O
Our	O
intuition	O
is	O
that	O
such	O
sentences	O
may	O
exhibit	O
factors	O
such	O
as	O
low	O
-	O
frequency	O
vocabulary	O
,	O
lexical	O
ambiguity	O
,	O
and	O
syntactic	O
complexity	O
(	O
e.g.	O
multiple	O
embedded	O
clauses	O
)	O
,	O
etc	O
.	O
that	O
are	O
uncommon	O
in	O
both	O
standard	O
language	O
and	O
,	O
relatedly	O
,	O
the	O
corpora	O
employed	O
in	O
training	O
large	O
-	O
scale	O
language	O
models	O
.	O
In	O
the	O
case	O
of	O
a	O
human	O
reader	O
,	O
encountering	O
such	O
a	O
sentence	O
may	O
result	O
in	O
a	O
number	O
of	O
processing	O
delays	O
,	O
e.g.	O
longer	O
aggregate	O
gaze	O
duration	O
.	O
In	O
the	O
case	O
of	O
a	O
sentence	O
encoder	O
,	O
an	O
uncommon	O
sentence	O
may	O
lead	O
to	O
a	O
degradation	O
of	O
representations	O
in	O
the	O
encoder	O
's	O
layers	O
,	O
wherein	O
a	O
lower	O
layer	O
might	O
learn	O
to	O
encode	O
vastly	O
different	O
information	O
than	O
a	O
higher	O
one	O
.	O
Similarly	O
,	O
different	O
models	O
'	O
representations	O
may	O
emphasize	O
different	O
aspects	O
of	O
these	O
more	O
complex	O
sentences	O
and	O
therefore	O
diverge	O
from	O
each	O
other	O
.	O
With	O
this	O
in	O
mind	O
,	O
our	O
hypothesis	O
is	O
that	O
sentences	O
which	O
are	O
difficult	O
for	O
humans	O
to	O
process	O
are	O
likely	O
to	O
have	O
divergent	O
representations	O
within	O
models	O
'	O
internal	O
layers	O
and	O
between	O
different	O
models	O
'	O
layers	O
.	O
Understanding	O
and	O
analysing	O
language	O
encoders	O
In	O
recent	O
years	O
,	O
some	O
prominent	O
efforts	O
towards	O
interpreting	O
neural	O
networks	O
for	O
NLP	O
have	O
included	O
:	O
developing	O
suites	O
that	O
evaluate	O
network	O
representations	O
through	O
performance	O
on	O
downstream	O
tasks	O
(	O
Conneau	O
et	O
al	O
,	O
2017a	O
;	O
Wang	O
et	O
al	O
,	O
2018	O
;	O
McCann	O
et	O
al	O
,	O
2018	O
)	O
;	O
analyzing	O
network	O
predictions	O
on	O
carefully	O
curated	O
datasets	O
(	O
Linzen	O
et	O
al	O
,	O
2016	O
;	O
Marvin	O
and	O
Linzen	O
,	O
2018	O
;	O
Gulordava	O
et	O
al	O
,	O
2018	O
;	O
Loula	O
et	O
al	O
,	O
2018	O
;	O
Dasgupta	O
et	O
al	O
,	O
2018	O
;	O
Tenney	O
et	O
al	O
,	O
2018	O
)	O
;	O
and	O
employing	O
diagnostic	O
classifiers	O
to	O
assess	O
whether	O
certain	O
classes	O
of	O
information	O
are	O
encoded	O
in	O
a	O
model	O
's	O
(	O
intermediate	O
)	O
representations	O
(	O
Adi	O
et	O
al	O
,	O
2016	O
;	O
Chrupała	O
et	O
al	O
,	O
2017	O
;	O
Hupkes	O
et	O
al	O
,	O
2017	O
;	O
Belinkov	O
et	O
al	O
,	O
2017	O
)	O
.	O
While	O
these	O
approaches	O
provide	O
valuable	O
insights	O
into	O
how	O
neural	O
networks	O
process	O
a	O
large	O
variety	O
of	O
phenomena	O
,	O
they	O
rely	O
on	O
decoding	O
accuracy	B-MetricName
as	O
a	O
probe	O
for	O
encoded	O
linguistic	O
information	O
.	O
If	O
properly	O
biased	O
,	O
this	O
means	O
that	O
they	O
can	O
detect	O
whether	O
information	O
is	O
encoded	O
in	O
a	O
representation	O
or	O
not	O
.	O
However	O
,	O
they	O
do	O
not	O
allow	O
for	O
a	O
direct	O
comparison	O
of	O
representational	O
structure	O
between	O
models	O
.	O
Consider	O
a	O
toy	O
dataset	O
of	O
five	O
sentences	O
of	O
interest	O
and	O
three	O
encodings	O
derived	O
from	O
quite	O
different	O
processing	O
models	O
;	O
a	O
hidden	O
state	O
of	O
a	O
trained	O
neural	O
language	O
model	O
,	O
a	O
tf	O
-	O
idf	O
weighted	O
bag	O
-	O
of	O
-	O
words	O
representation	O
,	O
and	O
measurements	O
of	O
fixation	O
duration	O
from	O
an	O
eyetracking	O
device	O
.	O
Probing	O
methods	O
do	O
not	O
allow	O
us	O
to	O
quantify	O
or	O
visualise	O
,	O
for	O
each	O
of	O
these	O
encoding	O
strategies	O
,	O
how	O
the	O
encoder	O
's	O
responses	O
to	O
the	O
five	O
sentences	O
relate	O
to	O
each	O
other	O
.	O
Moreover	O
,	O
probing	O
methods	O
would	O
not	O
directly	O
reveal	O
whether	O
the	O
fixations	O
from	O
the	O
eye	O
-	O
tracking	O
device	O
aligned	O
more	O
closely	O
with	O
the	O
tf	O
-	O
idf	O
representation	O
or	O
the	O
states	O
of	O
the	O
neural	O
language	O
model	O
.	O
In	O
short	O
,	O
while	O
probing	O
classifier	O
methods	O
can	O
establish	O
if	O
phenomena	O
are	O
separable	O
based	O
on	O
the	O
provided	O
representations	O
,	O
they	O
do	O
not	O
tell	O
us	O
about	O
the	O
overall	O
geometry	O
of	O
the	O
representational	O
spaces	O
.	O
RSA	O
,	O
on	O
the	O
other	O
hand	O
,	O
provides	O
a	O
basis	O
for	O
higher	O
-	O
order	O
comparisons	O
between	O
spaces	O
of	O
representations	O
,	O
and	O
a	O
way	O
to	O
visualise	O
and	O
quantify	O
the	O
extent	O
to	O
which	O
they	O
are	O
isomorphic	O
.	O
Indeed	O
,	O
RSA	O
has	O
seen	O
a	O
modest	O
introduction	O
within	O
interpretable	O
NLP	O
in	O
recent	O
years	O
.	O
For	O
example	O
,	O
Chrupała	O
et	O
al	O
(	O
2017	O
)	O
employed	O
RSA	O
as	O
a	O
means	O
of	O
correlating	O
encoder	O
representations	O
of	O
speech	O
,	O
text	O
,	O
and	O
images	O
in	O
a	O
post	O
-	O
hoc	O
analysis	O
of	O
a	O
multi	O
-	O
task	O
neural	O
pipeline	O
.	O
Similarly	O
,	O
Bouchacourt	O
and	O
Baroni	O
(	O
2018	O
)	O
used	O
the	O
framework	O
to	O
measure	O
the	O
similarity	O
between	O
input	O
image	O
embeddings	O
and	O
the	O
representations	O
of	O
the	O
same	O
image	O
by	O
an	O
agent	B-DatasetName
in	O
an	O
language	O
game	O
setting	O
.	O
More	O
recently	O
,	O
Chrupała	O
and	O
Alishahi	O
(	O
2019	O
)	O
correlated	O
activation	O
patterns	O
of	O
sentence	O
encoders	O
with	O
symbolic	O
representations	O
,	O
such	O
as	O
syntax	O
trees	O
.	O
Lastly	O
,	O
similar	O
to	O
our	O
work	O
here	O
,	O
Abnar	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
an	O
extension	O
to	O
RSA	O
that	O
enables	O
the	O
comparison	O
of	O
a	O
single	O
model	O
in	O
the	O
face	O
of	O
isolated	O
,	O
changing	O
parameters	O
,	O
and	O
employed	O
this	O
metric	O
along	O
with	O
RSA	O
to	O
correlate	O
NLP	O
models	O
'	O
and	O
human	O
brains	O
'	O
respective	O
representations	O
of	O
language	O
.	O
We	O
hope	O
to	O
position	O
our	O
work	O
among	O
this	O
brief	O
survey	O
and	O
further	O
demonstrate	O
the	O
flexibility	O
of	O
RSA	O
across	O
several	O
levels	O
of	O
abstraction	O
.	O

RSA	O
was	O
proposed	O
by	O
Kriegeskorte	O
et	O
al	O
(	O
2008	O
)	O
as	O
a	O
method	O
of	O
relating	O
the	O
different	O
representational	O
modalities	O
employed	O
in	O
neuroscientific	O
studies	O
.	O
Due	O
to	O
the	O
lack	O
of	O
correspondence	O
between	O
the	O
activity	O
patterns	O
of	O
disparate	O
measurement	O
modalities	O
(	O
e.g.	O
brain	O
activity	O
via	O
fMRI	O
,	O
behavioural	O
responses	O
)	O
,	O
RSA	O
aims	O
to	O
abstract	O
away	O
from	O
the	O
activity	O
patterns	O
themselves	O
and	O
instead	O
compute	O
representational	O
dissimilarity	O
matrices	O
(	O
RDMs	O
)	O
,	O
which	O
characterize	O
the	O
information	O
carried	O
by	O
a	O
given	O
representation	O
method	O
through	O
dissimilarity	O
structure	O
.	O
Given	O
a	O
set	O
of	O
representational	O
methods	O
(	O
e.g.	O
,	O
pretrained	O
encoders	O
)	O
M	O
and	O
a	O
set	O
of	O
experimental	O
conditions	O
(	O
sentences	O
)	O
N	O
,	O
we	O
can	O
construct	O
RDMs	O
for	O
each	O
method	O
in	O
M	O
.	O
Each	O
cell	O
in	O
an	O
RDM	O
corresponds	O
to	O
the	O
dissimilarity	O
between	O
the	O
activity	O
patterns	O
associated	O
with	O
pairs	O
of	O
experimental	O
conditions	O
n	O
i	O
,	O
n	O
j	O
N	O
,	O
say	O
,	O
a	O
pair	O
of	O
sentences	O
.	O
When	O
n	O
i	O
=	O
n	O
j	O
,	O
the	O
dissimilarity	O
between	O
an	O
experimental	O
condition	O
and	O
itself	O
is	O
intuitively	O
0	B-DatasetName
,	O
thus	O
making	O
the	O
N	O
×	O
N	O
RDM	O
symmetric	O
along	O
a	O
diagonal	O
of	O
zeros	O
(	O
Kriegeskorte	O
et	O
al	O
,	O
2008	O
)	O
.	O
The	O
RDMs	O
of	O
the	O
different	O
representational	O
methods	O
in	O
M	O
can	O
then	O
be	O
directly	O
compared	O
in	O
a	O
Representational	O
Similarity	O
Matrix	O
(	O
RSM	O
)	O
.	O
This	O
comparison	O
of	O
RDMs	O
is	O
known	O
as	O
second	O
-	O
order	O
analysis	O
,	O
which	O
is	O
broadly	O
based	O
on	O
the	O
idea	O
of	O
a	O
second	O
-	O
order	O
isomorphism	O
(	O
Shepard	O
and	O
Chipman	O
,	O
1970	O
)	O
.	O
In	O
such	O
an	O
analysis	O
,	O
the	O
principal	O
point	O
of	O
comparison	O
is	O
the	O
match	O
between	O
the	O
dissimilarity	O
structure	O
of	O
the	O
different	O
representa	O
-	O
tional	O
methods	O
.	O
Intuitively	O
,	O
this	O
can	O
be	O
expressed	O
through	O
the	O
notion	O
of	O
distance	O
between	O
distances	O
,	O
and	O
is	O
thus	O
related	O
to	O
Earth	O
Mover	O
's	O
Distance	O
(	O
Rubner	O
et	O
al	O
,	O
2000	O
)	O
.	O
1	O
Figure	O
1	O
shows	O
an	O
illustration	O
of	O
the	O
first	O
and	O
second	O
order	O
analyses	O
for	O
pretrained	O
language	O
encoders	O
.	O
Note	O
that	O
RSA	O
is	O
meaningfully	O
different	O
from	O
,	O
and	O
complementary	O
to	O
,	O
methods	O
that	O
employ	O
saturating	O
functions	O
of	O
representation	O
distances	O
(	O
e.g.	O
decoding	O
accuracy	B-MetricName
,	O
mutual	O
information	O
)	O
,	O
which	O
suffer	O
from	O
(	O
a	O
)	O
a	O
ceiling	O
effect	O
:	O
being	O
able	O
to	O
distinguish	O
experimental	O
phenomenon	O
A	O
from	O
B	O
with	O
with	O
an	O
accuracy	B-MetricName
of	O
100	O
%	O
and	O
experimental	O
phenomenon	O
C	O
from	O
D	O
with	O
an	O
accuracy	B-MetricName
of	O
100	O
%	O
does	O
not	O
mean	O
that	O
the	O
distance	O
between	O
A	O
and	O
B	O
is	O
the	O
same	O
as	O
that	O
between	O
C	O
and	O
D	O
;	O
and	O
(	O
b	O
)	O
discretization	O
(	O
Nili	O
et	O
al	O
,	O
2014	O
)	O
.	O
We	O
follow	O
Kriegeskorte	O
et	O
al	O
(	O
2008	O
)	O
in	O
using	O
the	O
correlation	O
distance	O
of	O
experimental	O
condition	O
pairs	O
n	O
i	O
,	O
n	O
j	O
N	O
as	O
a	O
dissimilarity	O
measure	O
,	O
wheren	O
i	O
is	O
the	O
mean	O
of	O
n	O
i	O
's	O
elements	O
,	O
is	O
the	O
dot	O
product	O
,	O
and	O
is	O
the	O
l	O
2	O
norm	O
:	O
corr	O
(	O
x	O
)	O
=	O
1	O
−	O
(	O
n	O
i	O
−n	O
i	O
)	O
(	O
n	O
j	O
−n	O
j	O
)	O
(	O
n	O
i	O
−n	O
i	O
2	O
(	O
n	O
j	O
−n	O
j	O
2	O
.	O
Compared	O
to	O
other	O
measures	O
,	O
correlation	O
distance	O
is	O
preferable	O
as	O
it	O
normalizes	O
both	O
the	O
mean	O
and	O
variance	O
of	O
activity	O
patterns	O
over	O
experimental	O
conditions	O
.	O
Other	O
popular	O
measures	O
include	O
the	O
Euclidean	O
distance	O
and	O
the	O
Malahanobis	O
distance	O
(	O
Kriegeskorte	O
et	O
al	O
,	O
2006	O
)	O
.	O

We	O
use	O
two	O
product	O
categories	O
,	O
namely	O
,	O
Home&Kitchen	O
and	O
Sports&Outdoors	O
for	O
our	O
analysis	O
from	O
the	O
dataset	O
mentioned	O
in	O
Section	O
2.1	O
after	O
combining	O
the	O
question	O
-	O
answer	O
and	O
review	O
dataset	O
with	O
the	O
Product	O
IDs	O
.	O
We	O
will	O
denote	O
the	O
two	O
categories	O
as	O
Home	O
and	O
Sports	O
,	O
respectively	O
.	O
We	O
use	O
the	O
same	O
data	O
split	O
from	O
OAAG	O
1	O
to	O
retrain	O
the	O
models	O
.	O
Since	O
there	O
is	O
no	O
validation	B-DatasetName
dataset	I-DatasetName
,	O
we	O
take	O
the	O
10	O
%	O
of	O
the	O
train	O
data	O
as	O
validation	O
data	O
.	O
Table	O
A.1	O
in	O
the	O
Appendix	O
shows	O
the	O
details	O
of	O
training	O
,	O
validation	O
,	O
and	O
test	O
split	O
.	O
We	O
keep	O
all	O
the	O
hyper	O
-	O
parameters	O
the	O
same	O
as	O
the	O
OAAG	O
and	O
CHIME	O
.	O
We	O
train	O
all	O
the	O
OAAG	O
models	O
for	O
20	O
epochs	O
and	O
CHIME	O
models	O
for	O
3	O
epochs	O
,	O
and	O
the	O
model	O
that	O
performs	O
the	O
best	O
on	O
the	O
validation	O
set	O
is	O
used	O
to	O
evaluate	O
the	O
test	O
set	O
.	O
We	O
evaluate	O
the	O
model	O
with	O
ROUGE	O
metric	O
and	O
report	O
the	O
F1	B-MetricName
scores	O
for	O
ROUGE	O
-	O
1	O
(	O
R1	O
)	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
(	O
RL	O
)	O
,	O
which	O
measure	O
the	O
word	O
overlap	O
and	O
the	O
longest	O
common	O
sequence	O
between	O
the	O
reference	O
answer	O
and	O
the	O
generated	O
answer	O
,	O
respectively	O
.	O
We	O
obtain	O
the	O
ROUGE	O
scores	O
using	O
rouge	O
-	O
score	O
2	O
package	O
.	O
Both	O
the	O
models	O
use	O
the	O
BM25	O
algorithm	O
to	O
retrieve	O
relevant	O
reviews	O
using	O
the	O
questions	O
in	O
the	O
test	O
dataset	O
.	O
We	O
refer	O
to	O
this	O
test	O
setting	O
as	O
BM25Q.	O

In	O
this	O
paper	O
,	O
we	O
extensively	O
analyze	O
the	O
generative	O
approach	O
of	O
question	O
-	O
answering	O
in	O
e	O
-	O
commerce	O
using	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
OAAG	O
model	O
(	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2020	O
)	O
and	O
CHIME	O
model	O
(	O
Lu	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
find	O
many	O
shortcomings	O
which	O
need	O
to	O
be	O
addressed	O
for	O
a	O
reliable	O
PQA	O
system	O
.	O
We	O
try	O
to	O
address	O
four	O
re	O
-	O
search	O
questions	O
related	O
to	O
the	O
generative	O
approach	O
for	O
PQA	O
,	O
such	O
as	O
how	O
the	O
models	O
utilize	O
the	O
reviews	O
,	O
how	O
it	O
performs	O
on	O
diverse	O
question	O
types	O
,	O
whether	O
it	O
is	O
biased	O
toward	O
frequent	O
phrases	O
in	O
training	O
data	O
,	O
and	O
the	O
correctness	O
of	O
the	O
generated	O
response	O
.	O
We	O
hope	O
that	O
our	O
analysis	O
will	O
lead	O
to	O
more	O
rigorous	O
PQA	O
research	O
.	O
A.2	O
Generated	O
Answers	O
with	O
High	O
R1	O
Score	B-MetricName
R1	O
score	O
.	O
In	O
Table	O
A.2	O
,	O
in	O
the	O
first	O
and	O
the	O
second	O
example	O
,	O
the	O
generated	O
answers	O
are	O
exactly	O
the	O
opposite	O
of	O
the	O
reference	O
answers	O
.	O
In	O
the	O
third	O
example	O
,	O
the	O
question	O
was	O
about	O
sweating	O
of	O
the	O
bottle	O
and	O
straw	O
cover	O
,	O
but	O
the	O
answer	O
does	O
not	O
address	O
any	O
of	O
these	O
.	O
In	O
the	O
fourth	O
example	O
,	O
the	O
answer	O
is	O
ambivalent	O
.	O
The	O
last	O
example	O
contains	O
a	O
frequently	O
occurring	O
phrase	O
"	O
I	O
do	O
n't	O
know	O
"	O
with	O
a	O
very	O
high	O
R1	O
score	O
.	O

Total	O
Retained	O
A	O
comparison	O
with	O
the	O
organizer	O
-	O
provided	O
parallel	O
training	O
data	O
used	O
in	O
our	O
WMT18	O
system	O
(	O
which	O
is	O
largely	O
the	O
same	O
as	O
the	O
provided	O
parallel	O
data	O
for	O
WMT19	O
in	O
the	O
Russian	O
-	O
English	O
language	O
pair	O
)	O
on	O
baseline	O
Marian	O
transformer	O
systems	O
with	O
identical	O
training	O
conditions	O
show	O
that	O
aggressive	O
language	O
ID	O
based	O
filtering	O
yields	O
an	O
approximate	O
+1	O
BLEU	B-MetricName
point	O
improvement	O
as	O
measured	O
by	O
SacreBLEU	B-MetricName
(	O
Post	O
,	O
2018	O
)	O
.	O
These	O
results	O
are	O
shown	O
in	O
Table	O
2	O
.	O

As	O
with	O
last	O
year	O
's	O
efforts	O
,	O
we	O
train	O
multiple	O
Marian	O
(	O
Junczys	O
-	O
Dowmunt	O
et	O
al	O
,	O
2018	O
)	O
models	O
with	O
both	O
University	O
of	O
Edinburgh	O
's	O
"	O
bi	O
-	O
deep	O
"	O
and	O
Google	B-DatasetName
's	O
transformer	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
architectures	O
.	O
Network	O
hyperparameters	O
are	O
the	O
same	O
as	O
detailed	O
in	O
Gwinnup	O
et	O
al	O
(	O
2018	O
)	O
.	O
We	O
again	O
use	O
newstest2014	O
as	O
the	O
validation	O
set	O
during	O
training	O
.	O
Utilizing	O
the	O
best	O
-	O
performing	O
BPE	B-MethodName
parameters	O
from	O
Section	O
2.2	O
,	O
we	O
first	O
trained	O
a	O
baseline	O
system	O
in	O
each	O
of	O
the	O
two	O
network	O
architectures	O
,	O
noting	O
the	O
Transformer	B-MethodName
system	O
's	O
better	O
performance	O
of	O
+0.82	O
BLEU	B-MetricName
on	O
average	O
across	O
decoded	O
test	O
sets	O
.	O
An	O
additional	O
six	O
distinct	O
transformer	O
models	O
were	O
then	O
independently	O
3	O
trained	O
for	O
use	O
in	O
ensemble	O
decoding	O
.	O
We	O
then	O
ensemble	O
decoded	O
test	O
sets	O
with	O
all	O
eight	O
models	O
.	O
Marian	O
typically	O
assigns	O
each	O
model	O
used	O
in	O
ensemble	O
decoding	O
a	O
feature	O
weight	O
of	O
1.0	O
;	O
thus	O
each	O
model	O
contributes	O
equally	O
to	O
the	O
decoding	O
process	O
.	O
Borrowing	O
from	O
our	O
Moses	O
training	O
approach	O
,	O
we	O
utilize	O
a	O
multi	O
-	O
iteration	O
decode	O
and	O
optimize	O
feature	O
weights	O
using	O
the	O
"	O
Expected	O
Corpus	O
BLEU	B-MetricName
"	O
(	O
ECB	O
)	O
metric	O
with	O
the	O
Drem	O
optimizer	B-HyperparameterName
(	O
Erdmann	O
and	O
Gwinnup	O
,	O
2015	O
)	O
.	O
We	O
experimented	O
using	O
newstest2014	O
and	O
newstest2017	O
as	O
tuning	O
sets	O
-	O
2017	O
did	O
not	O
help	O
performance	O
,	O
but	O
using	O
2014	O
did	O
improve	O
performance	O
by	O
up	O
to	O
+0.9	O
BLEU	B-MetricName
4	O
over	O
the	O
non	O
-	O
tuned	O
ensemble	O
.	O
Scores	O
for	O
all	O
the	O
above	O
-	O
mentioned	O
systems	O
are	O
shown	O
in	O
Table	O
4	O
.	O
The	O
best	O
-	O
performing	O
ensemble	O
(	O
ensemble	O
tune14	O
)	O
was	O
used	O
in	O
system	O
combination	O
.	O

As	O
in	O
previous	O
years	O
,	O
we	O
trained	O
a	O
phrase	O
-	O
based	O
Moses	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
system	O
with	O
the	O
same	O
data	O
as	O
the	O
Marian	O
system	O
outlined	O
in	O
Section	O
3.1	O
in	O
order	O
to	O
provide	O
diversity	O
for	O
system	O
combination	O
.	O
This	O
system	O
employed	O
a	O
hierarchical	O
reordering	O
model	O
(	O
Galley	O
and	O
Manning	O
,	O
2008	O
)	O
and	O
5	O
-	O
gram	O
operation	O
sequence	O
model	O
(	O
Durrani	O
et	O
al	O
,	O
2011	O
)	O
.	O
The	O
5	O
-	O
gram	O
English	O
language	O
model	O
was	O
trained	O
with	O
KenLM	O
on	O
all	O
permissable	O
monolingual	O
English	O
news	O
-	O
crawl	O
data	O
.	O
The	O
BPE	B-MethodName
model	O
used	O
was	O
applied	O
to	O
both	O
the	O
parallel	O
training	O
data	O
and	O
the	O
language	O
modeling	O
corpus	O
.	O
System	O
weights	O
were	O
tuned	O
with	O
the	O
Drem	O
(	O
Erdmann	O
and	O
Gwinnup	O
,	O
2015	O
)	O
optimizer	B-HyperparameterName
using	O
the	O
"	O
Expected	O
Corpus	O
BLEU	B-MetricName
"	O
(	O
ECB	O
)	O
metric	O
.	O

Jane	O
system	O
combination	O
(	O
Freitag	O
et	O
al	O
,	O
2014	O
)	O
was	O
employed	O
to	O
combine	O
outputs	O
from	O
the	O
best	O
systems	O
from	O
each	O
approach	O
outlined	O
above	O
.	O
Individual	O
component	O
system	O
and	O
final	O
combination	O
scores	O
are	O
shown	O
in	O
Table	O
6	O
for	O
cased	O
,	O
detokenized	O
BLEU	B-MetricName
and	O
BEER	O
2.0	O
(	O
Stanojević	O
and	O
Sima'an	O
,	O
2014	O
)	O
.	O

The	O
dataset	O
comprises	O
randomly	O
chosen	O
comments	O
from	O
the	O
Globe	O
and	O
Mail	O
news	O
site	O
(	O
sampled	O
from	O
the	O
SFU	O
Opinion	O
and	O
Comment	O
Corpus	O
dataset	O
)	O
(	O
Kolhatkar	O
et	O
al	O
,	O
2019	O
)	O
,	O
of	O
250	O
characters	O
or	O
less	O
.	O
Comment	O
scores	O
were	O
crowdsourced	O
using	O
Figure	O
Eight	O
(	O
now	O
Appen	O
)	O
.	O
The	O
annotation	O
job	O
consisted	O
of	O
588	O
crowdworkers	O
(	O
annotators	O
)	O
providing	O
244468	O
judgements	O
on	O
44355	O
comments	O
.	O
2	O
Each	O
annotator	O
was	O
asked	O
to	O
identify	O
for	O
each	O
comment	O
whether	O
it	O
was	O
healthy	O
and	O
if	O
any	O
of	O
the	O
attributes	O
were	O
present	O
,	O
in	O
the	O
form	O
of	O
a	O
standard	O
questionnaire	O
(	O
see	O
Appendix	O
A	O
)	O
.	O
Annotators	O
were	O
not	O
given	O
any	O
wider	O
context	O
or	O
additional	O
information	O
about	O
where	O
a	O
comment	O
was	O
posted	O
or	O
how	O
it	O
was	O
engaged	O
with	O
by	O
other	O
users	O
.	O
To	O
both	O
accommodate	O
and	O
attempt	O
to	O
resolve	O
meaningful	O
disagreement	O
,	O
we	O
applied	O
a	O
dynamic	O
judgement	O
method	O
which	O
requests	O
additional	O
annotations	O
for	O
those	O
comments	O
on	O
which	O
there	O
was	O
insufficient	O
consensus	O
(	O
either	O
yes	O
or	O
no	O
with	O
a	O
confidence	O
of	O
less	O
than	O
75	O
%	O
)	O
.	O
All	O
comments	O
were	O
annotated	O
at	O
least	O
three	O
times	O
,	O
and	O
more	O
annotators	O
were	O
added	O
,	O
up	O
to	O
a	O
limit	O
of	O
five	O
annotators	O
per	O
comment	O
until	O
sufficient	O
consensus	O
was	O
reached	O
.	O
Annotation	O
Job	O
Refinement	O
.	O
The	O
inherent	O
subtlety	O
,	O
subjectivity	O
,	O
and	O
frequent	O
ambiguity	O
of	O
the	O
attributes	O
covered	O
in	O
this	O
dataset	O
make	O
crowdsourcing	O
quality	O
attribute	O
labels	O
an	O
unavoidably	O
difficult	O
process	O
.	O
Typically	O
the	O
goal	O
in	O
an	O
annotation	O
task	O
would	O
simply	O
be	O
to	O
maximise	O
agreement	O
between	O
the	O
multiple	O
annotators	O
of	O
each	O
comment	O
.	O
However	O
,	O
when	O
the	O
annotation	O
task	O
is	O
inherently	O
subjective	O
and	O
meaningful	O
difference	O
of	O
opinion	O
is	O
itself	O
valuable	O
data	O
,	O
the	O
goal	O
becomes	O
instead	O
to	O
maximise	O
common	O
understanding	O
of	O
the	O
task	O
across	O
annotators	O
.	O
This	O
entails	O
tailoring	O
the	O
phrasing	O
of	O
the	O
questions	O
put	O
to	O
annotators	O
,	O
so	O
as	O
to	O
create	O
as	O
common	O
an	O
understanding	O
as	O
possible	O
of	O
what	O
each	O
question	O
is	O
really	O
asking	O
.	O
This	O
way	O
,	O
disagreement	O
between	O
annotators	O
reflected	O
in	O
the	O
dataset	O
will	O
represent	O
different	O
reasonable	O
readings	O
of	O
the	O
same	O
comment	O
which	O
are	O
themselves	O
important	O
to	O
capture	O
.	O
In	O
research	O
on	O
irony	O
and	O
sarcasm	O
,	O
for	O
example	O
,	O
Filatova	O
noted	O
the	O
difficulty	O
even	O
among	O
expert	O
researchers	O
in	O
formally	O
defining	O
these	O
terms	O
(	O
Filatova	O
,	O
2012	O
)	O
.	O
For	O
the	O
other	O
attributes	O
included	O
in	O
this	O
dataset	O
which	O
are	O
as	O
(	O
if	O
not	O
more	O
)	O
ambiguous	O
and	O
subtle	O
than	O
sarcasm	O
,	O
we	O
expect	O
this	O
to	O
hold	O
true	O
as	O
well	O
.	O
The	O
exact	O
wording	O
of	O
each	O
question	O
on	O
the	O
questionnaire	O
went	O
through	O
multiple	O
iterations	O
,	O
tested	O
by	O
smaller	O
scale	O
experiments	O
to	O
evaluate	O
effectiveness	O
.	O
The	O
quality	O
of	O
the	O
resulting	O
data	O
was	O
evaluated	O
manually	O
by	O
our	O
team	O
,	O
calculating	O
the	O
proportion	O
of	O
perceived	O
mistaken	O
annotations	O
and	O
their	O
'	O
severity	O
'	O
:	O
to	O
what	O
extent	O
a	O
judgement	O
was	O
'	O
obviously	O
wrong	O
'	O
,	O
as	O
opposed	O
to	O
an	O
understandable	O
alternative	O
reading	O
of	O
a	O
comment	O
.	O
We	O
found	O
that	O
providing	O
annotators	O
with	O
precise	O
and	O
more	O
comprehensive	O
definitions	O
of	O
each	O
attribute	O
was	O
not	O
more	O
likely	O
to	O
produce	O
interannotator	O
agreement	O
or	O
better	O
quality	O
data	O
.	O
Neither	O
,	O
however	O
,	O
were	O
best	O
results	O
produced	O
by	O
asking	O
simple	O
,	O
'	O
yes	O
or	O
no	O
'	O
questions	O
such	O
as	O
'	O
Is	O
this	O
comment	O
dismissive	O
?	O
'	O
for	O
all	O
attributes	O
.	O
The	O
best	O
results	O
were	O
achieved	O
by	O
relying	O
primarily	O
on	O
annotators	O
implicit	O
understandings	O
of	O
and	O
intuitions	O
about	O
the	O
attributes	O
,	O
aided	O
by	O
brief	O
inline	O
explanations	O
.	O
We	O
added	O
explanations	O
to	O
avoid	O
mistakes	O
for	O
those	O
attributes	O
which	O
are	O
more	O
ambiguous	O
,	O
and	O
for	O
which	O
our	O
smaller	O
tests	O
had	O
indicated	O
required	O
further	O
guidance	O
.	O
These	O
can	O
be	O
seen	O
in	O
the	O
questionnaire	O
included	O
as	O
Appendix	O
A.	O
To	O
ensure	O
that	O
disagreement	O
reflects	O
reasonable	O
difference	O
of	O
opinion	O
,	O
rather	O
than	O
inattention	O
or	O
misunderstanding	O
of	O
the	O
task	O
,	O
it	O
is	O
necessary	O
to	O
apply	O
a	O
method	O
of	O
quality	O
control	O
.	O
The	O
attempt	O
to	O
create	O
a	O
labeled	O
dataset	O
is	O
premised	O
on	O
the	O
assumption	O
of	O
some	O
'	O
ground	O
truth	O
'	O
;	O
that	O
it	O
is	O
possible	O
for	O
comments	O
to	O
have	O
labels	O
and	O
confidence	O
scores	O
accurately	O
representing	O
the	O
presence	O
of	O
one	O
or	O
more	O
attributes	O
to	O
some	O
extent	O
.	O
However	O
,	O
the	O
extent	O
to	O
which	O
a	O
comment	O
displays	O
one	O
or	O
more	O
attribute	O
is	O
subjective	O
,	O
and	O
the	O
scores	O
would	O
be	O
unhelpful	O
if	O
they	O
did	O
not	O
capture	O
what	O
a	O
wider	O
and	O
more	O
diverse	O
audience	O
than	O
our	O
team	O
of	O
authors	O
would	O
understand	O
the	O
comments	O
to	O
mean	O
.	O
Our	O
process	O
of	O
quality	O
control	O
therefore	O
aimed	O
to	O
reduce	O
the	O
number	O
of	O
'	O
bad	O
'	O
annotators	O
,	O
those	O
who	O
either	O
do	O
not	O
understand	O
or	O
appropriately	O
engage	O
with	O
the	O
task	O
,	O
while	O
still	O
allowing	O
for	O
differences	O
of	O
opinion	O
.	O
Our	O
primary	O
quality	O
control	O
mechanism	O
was	O
to	O
collate	B-DatasetName
a	O
set	O
of	O
'	O
test	O
comments	O
'	O
,	O
for	O
which	O
we	O
had	O
manually	O
established	O
the	O
correct	O
answers	O
.	O
Annotators	O
encountered	O
one	O
test	O
comment	O
per	O
batch	O
of	O
seven	O
comments	O
they	O
reviewed	O
,	O
without	O
knowing	O
which	O
of	O
the	O
seven	O
was	O
the	O
test	O
comment	O
,	O
and	O
their	O
running	O
accuracy	B-MetricName
on	O
these	O
test	O
comments	O
was	O
defined	O
as	O
their	O
'	O
trustworthiness	O
score	O
'	O
.	O
The	O
task	O
required	O
that	O
annotators	O
maintain	O
a	O
trustworthiness	O
score	O
of	O
more	O
than	O
78	O
%	O
.	O
If	O
an	O
annotator	O
dropped	O
below	O
this	O
level	O
,	O
they	O
were	O
removed	O
from	O
the	O
annotator	O
pool	O
for	O
this	O
task	O
,	O
and	O
all	O
of	O
their	O
prior	O
annotations	O
were	O
discarded	O
3	O
.	O
The	O
removed	O
'	O
bad	O
'	O
annotator	O
judgements	O
were	O
replaced	O
by	O
newly	O
collected	O
trusted	O
judgements	O
as	O
necessary	O
.	O
We	O
restricted	O
our	O
test	O
comments	O
to	O
what	O
were	O
(	O
in	O
our	O
view	O
)	O
clear	O
and	O
definitive	O
examples	O
of	O
the	O
attributes	O
,	O
such	O
that	O
one	O
would	O
fail	O
on	O
the	O
test	O
comments	O
only	O
if	O
one	O
has	O
an	O
incorrect	O
understanding	O
of	O
what	O
is	O
meant	O
by	O
a	O
particular	O
attribute	O
.	O
In	O
the	O
course	O
of	O
our	O
preliminary	O
small	O
-	O
scale	O
refining	O
iterations	O
of	O
the	O
questionnaire	O
,	O
analysis	O
of	O
responses	O
revealed	O
some	O
recurring	O
misunderstandings	O
or	O
mistakes	O
.	O
For	O
example	O
,	O
a	O
common	O
error	O
was	O
to	O
label	O
all	O
non	O
-	O
sarcastic	O
humour	O
as	O
sarcasm	O
,	O
or	O
to	O
conflate	O
polite	O
disagreement	O
with	O
dismissiveness	O
.	O
As	O
a	O
result	O
,	O
we	O
identified	O
and	O
included	O
specific	O
test	O
comments	O
,	O
drawn	O
from	O
real	O
examples	O
,	O
aimed	O
at	O
reducing	O
these	O
common	O
errors	O
.	O
We	O
included	O
very	O
few	O
test	O
comments	O
for	O
the	O
higher	O
level	O
question	O
on	O
whether	O
a	O
comment	O
belongs	O
in	O
a	O
healthy	O
conversation	O
.	O
Any	O
test	O
questions	O
on	O
this	O
topic	O
were	O
very	O
extreme	O
examples	O
,	O
such	O
as	O
highly	O
abusive	O
explicit	O
comments	O
,	O
to	O
ensure	O
that	O
annotators	O
were	O
not	O
randomly	O
answering	O
that	O
question	O
.	O
We	O
had	O
two	O
reasons	O
for	O
minimising	O
the	O
use	O
of	O
test	O
comments	O
for	O
this	O
question	O
.	O
Firstly	O
,	O
since	O
this	O
was	O
in	O
our	O
view	O
the	O
most	O
open	O
-	O
ended	O
question	O
,	O
it	O
is	O
difficult	O
to	O
establish	O
tests	O
on	O
the	O
basis	O
of	O
which	O
to	O
exclude	O
annotators	O
.	O
Secondly	O
,	O
allowing	O
greater	O
annotator	O
discretion	O
on	O
this	O
question	O
provides	O
insight	O
on	O
whether	O
there	O
is	O
a	O
correlation	O
between	O
the	O
six	O
attributes	O
and	O
being	O
labelled	O
as	O
unhealthy	O
.	O
4	O

The	O
dataset	O
comprises	O
a	O
total	O
of	O
44355	O
comments	O
labelled	O
'	O
yes	O
'	O
or	O
'	O
no	O
'	O
for	O
each	O
attribute	O
,	O
along	O
with	O
a	O
confidence	O
score	O
for	O
each	O
label	O
.	O
The	O
labels	O
and	O
corresponding	O
confidence	O
scores	O
for	O
each	O
attribute	O
are	O
based	O
on	O
an	O
aggregation	O
of	O
the	O
answers	O
given	O
by	O
different	O
annotators	O
,	O
weighted	O
by	O
their	O
respective	O
'	O
trustworthiness	O
'	O
scores	O
.	O
As	O
an	O
example	O
to	O
demonstrate	O
this	O
process	O
,	O
consider	O
a	O
comment	O
annotated	O
by	O
5	O
annotators	O
with	O
trustworthiness	O
scores	O
0.78	O
,	O
0.85	O
,	O
0.9	O
,	O
1.0	O
,	O
and	O
0.95	O
,	O
who	O
judge	O
a	O
comment	O
for	O
a	O
particular	O
attribute	O
with	O
judgements	O
'	O
yes	O
'	O
,	O
'	O
yes	O
'	O
,	O
'	O
yes	O
'	O
,	O
'	O
no	O
'	O
,	O
'	O
yes	O
'	O
respectively	O
.	O
Let	O
T	O
be	O
the	O
sum	O
of	O
their	O
trustworthiness	O
scores	O
,	O
and	O
T	O
y	O
,	O
T	O
n	O
the	O
sum	O
of	O
the	O
trustworthiness	O
scores	O
of	O
those	O
who	O
answered	O
'	O
yes	O
'	O
and	O
'	O
no	O
'	O
respectively	O
.	O
The	O
label	O
is	O
then	O
determined	O
by	O
which	O
of	O
T	O
y	O
or	O
T	O
n	O
is	O
larger	O
,	O
in	O
this	O
case	O
it	O
is	O
T	O
y	O
,	O
and	O
the	O
confidence	O
score	O
is	O
T	O
y	O
/T	O
,	O
in	O
this	O
case	O
0.78	O
.	O
The	O
proportion	O
of	O
comments	O
that	O
contain	O
each	O
attribute	O
is	O
shown	O
in	O
Table	O
1	O
As	O
the	O
comments	O
were	O
sampled	O
from	O
the	O
SFU	O
Opinion	O
and	O
Comment	O
Corpus	O
dataset	O
,	O
the	O
prevalence	O
for	O
each	O
attribute	O
is	O
inevitably	O
low	O
.	O
Despite	O
the	O
label	O
imbalance	O
,	O
the	O
dataset	O
represents	O
an	O
important	O
contribution	O
to	O
identification	O
of	O
this	O
wider	O
variety	O
of	O
subtle	O
attributes	O
,	O
with	O
thousands	O
of	O
positive	O
examples	O
for	O
each	O
.	O
Our	O
manual	O
analysis	O
during	O
initial	O
iterations	O
of	O
the	O
annotation	O
job	O
indicated	O
that	O
4	O
There	O
remains	O
a	O
clear	O
methodological	O
issue	O
with	O
using	O
this	O
data	O
for	O
comparing	O
the	O
set	O
of	O
comments	O
classed	O
as	O
'	O
unhealthy	O
'	O
with	O
those	O
classed	O
as	O
one	O
or	O
more	O
of	O
the	O
other	O
attributes	O
:	O
having	O
been	O
asked	O
all	O
questions	O
as	O
part	O
of	O
the	O
same	O
questionnaire	O
,	O
annotators	O
may	O
have	O
been	O
primed	O
to	O
associate	O
the	O
attributes	O
with	O
'	O
unhealthiness	O
'	O
,	O
even	O
if	O
they	O
would	O
not	O
have	O
done	O
so	O
otherwise	O
.	O
(	O
Rosenblatt	O
et	O
al	O
,	O
1956	O
)	O
of	O
confidence	O
scores	O
for	O
each	O
attribute	O
.	O
Figure	O
2a	O
shows	O
confidence	O
scores	O
for	O
those	O
comments	O
labelled	O
as	O
'	O
no	O
'	O
for	O
each	O
unhealthy	O
attribute	O
,	O
while	O
Figure	O
2b	O
represents	O
those	O
of	O
comments	O
labelled	O
'	O
yes	O
'	O
.	O
these	O
final	O
proportions	O
are	O
roughly	O
representative	O
of	O
the	O
prevalence	O
of	O
these	O
attributes	O
in	O
similar	O
live	O
contexts	O
,	O
such	O
as	O
North	O
American	O
online	O
newspaper	O
comment	O
sections	O
.	O
There	O
are	O
specific	O
attributes	O
,	O
notably	O
sarcasm	O
,	O
for	O
which	O
it	O
can	O
be	O
possible	O
to	O
collate	B-DatasetName
a	O
corpus	O
of	O
self	O
-	O
labelled	O
data	O
,	O
for	O
example	O
by	O
scraping	O
tweets	O
with	O
'	O
#	O
sarcastic	O
'	O
from	O
Twitter	O
,	O
or	O
comments	O
followed	O
by	O
'	O
/s	O
'	O
on	O
Reddit	B-DatasetName
(	O
Khodak	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
these	O
specific	O
circumstances	O
,	O
the	O
avoidance	O
of	O
the	O
need	O
to	O
crowdsource	O
and	O
pay	O
for	O
annotations	O
can	O
permit	O
much	O
larger	O
and	O
more	O
balanced	O
datasets	O
.	O
However	O
,	O
for	O
all	O
other	O
attributes	O
we	O
consider	O
,	O
and	O
in	O
fora	O
like	O
the	O
comment	O
sections	O
of	O
news	O
sites	O
,	O
relying	O
on	O
self	O
-	O
labelled	O
data	O
is	O
not	O
possible	O
.	O
For	O
these	O
attributes	O
,	O
crowdsourcing	O
is	O
the	O
only	O
feasible	O
way	O
to	O
obtain	O
high	O
quality	O
data	O
,	O
and	O
as	O
such	O
we	O
would	O
expect	O
proportions	O
reflecting	O
those	O
observed	O
in	O
similar	O
contexts	O
.	O
Inspection	O
of	O
random	O
subsets	O
of	O
the	O
new	O
UCC	B-DatasetName
dataset	O
reveals	O
that	O
the	O
data	O
is	O
generally	O
of	O
a	O
high	O
quality	O
,	O
and	O
captures	O
important	O
nuances	O
,	O
accurately	O
identifying	O
these	O
subtle	O
attributes	O
,	O
both	O
when	O
they	O
overlap	O
(	O
as	O
is	O
common	O
)	O
,	O
and	O
also	O
when	O
they	O
do	O
not	O
(	O
see	O
Figure	O
3	O
for	O
examples	O
)	O
.	O
Figure	O
4	O
shows	O
the	O
correlations	O
between	O
attributes	O
,	O
calculated	O
based	O
on	O
the	O
pool	O
of	O
comments	O
which	O
are	O
labelled	O
as	O
one	O
or	O
more	O
of	O
the	O
six	O
unhealthy	O
attributes	O
.	O
The	O
figure	O
highlights	O
two	O
important	O
facts	O
.	O
First	O
,	O
the	O
relatively	O
low	O
correlation	O
between	O
most	O
attributes	O
indicates	O
that	O
the	O
dataset	O
succeeds	O
in	O
differentiating	O
between	O
these	O
different	O
types	O
of	O
subtle	O
unhealthy	O
attributes	O
.	O
As	O
expected	O
,	O
there	O
is	O
significant	O
correlation	O
between	O
antagonistic	O
and	O
hostile	O
comments	O
.	O
There	O
is	O
some	O
correlation	O
between	O
the	O
often	O
more	O
subtle	O
attributes	O
like	O
dismissiveness	O
/	O
condescension	O
and	O
antagonism	O
,	O
while	O
these	O
are	O
less	O
correlated	O
with	O
hostility	O
.	O
We	O
also	O
include	O
correlations	O
with	O
the	O
'	O
toxicity	O
'	O
scores	O
produced	O
by	O
Jigsaw	B-MethodName
's	O
Perspective	O
API	O
(	O
perspectiveapi.com	O
)	O
,	O
which	O
again	O
confirms	O
that	O
our	O
attributes	O
,	O
in	O
particular	O
those	O
other	O
than	O
antagonistic	O
and	O
hostile	O
,	O
capture	O
something	O
distinct	O
from	O
overt	O
toxicity	O
.	O
A	O
notable	O
feature	O
of	O
Figure	O
4	O
is	O
the	O
slightly	O
negative	O
correlations	O
between	O
sarcasm	O
and	O
other	O
attributes	O
,	O
indicating	O
that	O
annotators	O
generally	O
did	O
not	O
associate	O
sarcasm	O
with	O
other	O
unhealthy	O
attributes	O
.	O
Secondly	O
,	O
'	O
unhealthy	O
'	O
correlates	O
significantly	O
with	O
antagonism	O
and	O
hostility	O
,	O
but	O
very	O
little	O
with	O
the	O
other	O
attributes	O
,	O
indicating	O
a	O
fairly	O
broad	O
general	O
notion	O
of	O
healthy	O
conversation	O
on	O
the	O
part	O
of	O
the	O
annotators	O
,	O
which	O
mostly	O
includes	O
dismissive	O
,	O
condescending	O
,	O
sarcastic	O
and	O
generalising	O
comments	O
.	O
Despite	O
its	O
generally	O
high	O
quality	O
,	O
the	O
nature	O
of	O
the	O
task	O
and	O
the	O
annotation	O
method	O
entails	O
some	O
level	O
of	O
noise	O
in	O
the	O
dataset	O
.	O
This	O
noise	O
is	O
particularly	O
difficult	O
to	O
quantify	O
given	O
the	O
need	O
to	O
distinguish	O
between	O
different	O
but	O
reasonable	O
interpretations	O
of	O
a	O
comment	O
,	O
and	O
simply	O
incorrect	O
annotations	O
caused	O
by	O
a	O
lack	O
of	O
understanding	O
or	O
care	O
on	O
the	O
part	O
of	O
an	O
annotator	O
(	O
for	O
example	O
,	O
one	O
comment	O
reading	O
"	O
You	O
are	O
an	O
ignorant	O
*	O
sshole	O
"	O
was	O
judged	O
not	O
to	O
be	O
needlessly	O
hostile	O
,	O
an	O
obvious	O
error	O
)	O
.	O
This	O
highlights	O
the	O
difficulties	O
of	O
using	O
traditional	O
reliability	O
metrics	O
like	O
Krippendorff	O
's	O
α	B-HyperparameterName
for	O
crowdsourced	O
annotations	O
on	O
subjective	O
tasks	O
(	O
D'Arcey	O
et	O
al	O
,	O
2019	O
)	O
.	O
Krippendorff	O
's	O
α	B-HyperparameterName
is	O
a	O
number	O
between	O
0	B-DatasetName
and	O
1	O
intended	O
to	O
indicate	O
the	O
extent	O
to	O
which	O
annotators	O
agree	O
compared	O
with	O
what	O
would	O
have	O
happened	O
if	O
they	O
guessed	O
randomly	O
.	O
The	O
base	O
assumption	O
then	O
is	O
that	O
all	O
disagreement	O
between	O
annotators	O
decreases	O
reliability	O
,	O
which	O
is	O
not	O
necessarily	O
the	O
case	O
for	O
subjective	O
attributes	O
(	O
Salminen	O
et	O
al	O
,	O
2018b	O
;	O
Swanson	O
et	O
al	O
,	O
2014	O
)	O
.	O
Despite	O
the	O
above	O
caveat	O
,	O
we	O
conduct	O
analysis	O
using	O
Krippendorff	O
's	O
α	B-HyperparameterName
(	O
K	O
-	O
α	B-HyperparameterName
)	O
for	O
two	O
reasons	O
.	O
Firstly	O
,	O
to	O
allow	O
for	O
comparison	O
with	O
other	O
literature	O
in	O
the	O
field	O
,	O
we	O
report	O
the	O
K	O
-	O
α	B-HyperparameterName
for	O
judgements	O
on	O
each	O
attribute	O
in	O
Table	O
2	O
.	O
They	O
range	O
from	O
0.31	O
-	O
0.39	O
,	O
which	O
is	O
comparable	O
with	O
other	O
datasets	O
labelling	O
'	O
similar	O
'	O
phenomenon	O
,	O
such	O
as	O
sarcasm	O
(	O
0.24	O
-	O
0.38	O
)	O
(	O
Swanson	O
et	O
al	O
,	O
2014	O
;	O
Justo	O
et	O
al	O
,	O
2018	O
;	O
D'Arcey	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
hate	B-DatasetName
speech	I-DatasetName
with	O
sub	O
-	O
attributes	O
from	O
Figure	O
Eight	O
annotators	O
(	O
0.21	O
)	O
(	O
Lazaridou	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
one	O
exception	O
is	O
the	O
set	O
of	O
judgements	O
on	O
whether	O
a	O
comment	O
has	O
a	O
place	O
in	O
a	O
healthy	O
conversation	O
,	O
with	O
a	O
lower	O
K	O
-	O
α	B-HyperparameterName
of	O
0.26	O
.	O
Given	O
that	O
this	O
is	O
a	O
more	O
open	O
-	O
ended	O
question	O
,	O
this	O
is	O
not	O
necessarily	O
surprising	O
.	O
Secondly	O
,	O
to	O
the	O
extent	O
that	O
K	O
-	O
α	B-HyperparameterName
is	O
an	O
important	O
reliability	O
metric	O
for	O
this	O
form	O
of	O
data	O
,	O
it	O
supports	O
our	O
use	O
of	O
'	O
trustworthiness	O
'	O
scores	O
when	O
aggregating	O
judgements	O
on	O
a	O
given	O
comment	O
to	O
decide	O
labels	O
and	O
confidence	O
scores	O
.	O
Specifically	O
,	O
as	O
shown	O
in	O
Figure	O
5	O
,	O
we	O
see	O
that	O
as	O
we	O
increase	O
the	O
trustworthiness	O
threshold	O
for	O
annotators	O
whose	O
judgements	O
are	O
included	O
,	O
the	O
resulting	O
K	O
-	O
α	B-HyperparameterName
steadily	O
increase	O
.	O
This	O
provides	O
some	O
indication	O
that	O
our	O
trustworthiness	O
scores	O
do	O
capture	O
the	O
reliability	O
of	O
our	O
annotators	O
,	O
and	O
thus	O
that	O
their	O
judgements	O
ought	O
to	O
be	O
weighted	O
more	O
highly	O
in	O
the	O
final	O
confidence	O
in	O
a	O
comment	O
's	O
labels	O
.	O

Use	O
of	O
a	O
pre	O
-	O
trained	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
fine	O
-	O
tuning	O
on	O
this	O
dataset	O
produces	O
classifiers	O
with	O
modest	O
performance	O
(	O
Figure	O
6	O
)	O
,	O
compared	O
to	O
the	O
state	O
of	O
the	O
art	O
for	O
sequence	O
classification	O
.	O
The	O
best	O
performing	O
attributes	O
,	O
'	O
hostile	O
'	O
and	O
'	O
antagonistic	O
'	O
are	O
also	O
those	O
most	O
similar	O
to	O
the	O
types	O
of	O
attributes	O
typically	O
annotated	O
in	O
comment	O
classification	O
work	O
.	O
The	O
other	O
attributes	O
seem	O
to	O
cluster	O
together	O
,	O
with	O
the	O
'	O
sarcastic	O
'	O
label	O
particularly	O
noteworthy	O
for	O
its	O
low	O
performance	O
.	O
To	O
give	O
context	O
to	O
the	O
model	O
performance	O
,	O
we	O
follow	O
(	O
Wulczyn	O
et	O
al	O
,	O
2017	O
)	O
and	O
compare	O
our	O
performance	O
with	O
human	O
workers	O
.	O
For	O
each	O
comment	O
,	O
we	O
hold	O
out	O
one	O
annotator	O
to	O
act	O
as	O
our	O
'	O
human	O
model	O
'	O
and	O
use	O
the	O
aggregated	O
score	O
of	O
the	O
other	O
annotators	O
as	O
the	O
ground	O
truth	O
to	O
compute	O
the	O
ROC	B-MetricName
AUC	I-MetricName
.	O
To	O
stabilize	O
our	O
results	O
,	O
this	O
procedure	O
is	O
repeated	O
five	O
times	O
and	O
the	O
average	O
reported	O
.	O
We	O
use	O
the	O
same	O
test	O
sets	O
to	O
compute	O
the	O
ROC	B-MetricName
AUC	I-MetricName
of	O
the	O
trained	O
BERT	B-MethodName
model	O
and	O
average	O
those	O
scores	O
as	O
well	O
.	O
As	O
we	O
can	O
see	O
,	O
for	O
all	O
attributes	O
other	O
than	O
'	O
sarcastic	O
'	O
the	O
BERT	B-MethodName
model	O
outperforms	O
a	O
randomly	O
selected	O
human	O
annotator	O
,	O
indicating	O
that	O
it	O
has	O
sufficiently	O
captured	O
the	O
semantic	O
and	O
syntactic	O
structures	O
for	O
these	O
attributes	O
.	O
For	O
'	O
sarcastic	O
'	O
,	O
the	O
gap	O
between	O
the	O
BERT	B-MethodName
model	O
and	O
human	O
annotators	O
indicates	O
a	O
rich	O
area	O
for	O
studying	O
whether	O
model	O
performance	O
can	O
be	O
improved	O
.	O

We	O
found	O
that	O
CNN	O
-	O
based	O
systems	O
obtained	O
the	O
best	O
results	O
for	O
some	O
error	O
types	O
,	O
likely	O
due	O
to	O
some	O
characteristics	O
derived	O
from	O
CNN	O
.	O
We	O
trained	O
four	O
CNN	O
-	O
based	O
ensemble	O
systems	O
,	O
using	O
the	O
model	O
architecture	O
in	O
(	O
Chollampatt	O
and	O
Ng	O
,	O
2018	O
)	O
,	O
but	O
without	O
reranking	O
.	O
Four	O
best	O
combinations	O
to	O
build	O
the	O
ensemble	O
systems	O
were	O
selected	O
.	O
Unlike	O
(	O
Chollampatt	O
and	O
Ng	O
,	O
2018	O
)	O
,	O
we	O
did	O
not	O
use	O
fastText	B-MethodName
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
to	O
initialize	O
word	B-TaskName
embeddings	I-TaskName
because	O
we	O
found	O
no	O
improvement	O
on	O
the	O
development	O
set	O
by	O
doing	O
that	O
.	O
We	O
tuned	O
parameters	O
for	O
the	O
system	O
,	O
such	O
as	O
batch	B-HyperparameterName
size	I-HyperparameterName
,	O
word	B-HyperparameterName
embedding	I-HyperparameterName
dimension	I-HyperparameterName
,	O
etc	O
.	O

We	O
expect	O
to	O
combine	O
these	O
models	O
trained	O
above	O
into	O
a	O
more	O
powerful	O
system	O
through	O
effective	O
ensemble	O
methods	O
.	O
Our	O
ensemble	O
work	O
mainly	O
focuses	O
on	O
rule	O
-	O
based	O
solutions	O
.	O
We	O
will	O
introduce	O
two	O
main	O
modules	O
first	O
.	O
Confidence	O
Table	O
We	O
can	O
obtain	O
the	O
precision	O
and	O
F	O
0.5	O
metric	O
on	O
each	O
error	O
type	O
through	O
sentence	O
alignment	O
and	O
error	O
type	O
classification	O
by	O
Errant	O
(	O
Bryant	O
et	O
al	O
,	O
2017	O
)	O
.	O
Errant	O
provides	O
performance	O
statistics	O
based	O
on	O
55	O
error	O
types	O
and	O
is	O
also	O
the	O
tool	O
used	O
to	O
evaluate	O
this	O
GEC	O
shared	O
task	O
,	O
thus	O
we	O
use	O
the	O
result	O
of	O
operation	O
and	O
error	O
type	O
span	O
-	O
level	O
(	O
Bryant	O
et	O
al	O
,	O
2017	O
)	O
for	O
a	O
model	O
or	O
system	O
as	O
the	O
confidence	O
table	O
.	O
Conflict	O
Solver	O
We	O
often	O
encounter	O
GEC	O
error	O
conflicts	O
when	O
combining	O
multiple	O
models	O
or	O
systems	O
.	O
For	O
example	O
,	O
We	O
love	O
played	O
soccer	O
.	O
One	O
system	O
corrects	O
played	O
to	O
playing	O
,	O
while	O
another	O
system	O
may	O
correct	O
played	O
to	O
to	O
play	O
.	O
When	O
two	O
different	O
corrections	O
occur	O
in	O
the	O
same	O
place	O
,	O
we	O
need	O
to	O
consider	O
which	O
one	O
to	O
choose	O
.	O
We	O
solve	O
this	O
problem	O
in	O
a	O
unified	O
pipeline	O
,	O
which	O
can	O
also	O
be	O
seen	O
as	O
an	O
ensemble	O
way	O
:	O
(	O
1	O
)	O
We	O
sort	O
each	O
group	O
of	O
conflicting	O
corrections	O
proposed	O
by	O
all	O
the	O
systems	O
in	O
a	O
reverse	O
order	O
of	O
location	O
index	O
and	O
confidence	O
.	O
(	O
2	O
)	O
We	O
apply	O
three	O
sub	O
-	O
strategies	O
:	O
When	O
combining	O
outcomes	O
from	O
different	O
systems	O
,	O
we	O
treat	O
the	O
precision	O
in	O
a	O
confidence	O
table	O
as	O
the	O
confidence	O
.	O
Each	O
correction	O
has	O
its	O
confidence	O
obtained	O
by	O
looking	O
up	O
the	O
precision	O
of	O
the	O
corresponding	O
type	O
of	O
the	O
correction	O
in	O
the	O
table	O
.	O
If	O
two	O
conflicting	O
corrections	O
are	O
the	O
same	O
,	O
we	O
merge	O
them	O
and	O
add	O
α	B-HyperparameterName
to	O
the	O
confidence	O
of	O
the	O
correction	O
;	O
otherwise	O
,	O
the	O
correction	O
with	O
a	O
lower	O
confidence	O
will	O
be	O
discarded	O
.	O
After	O
combining	O
outcomes	O
,	O
if	O
the	O
confidence	O
of	O
a	O
correction	O
is	O
lower	O
than	O
β	B-HyperparameterName
,	O
the	O
correction	O
is	O
discarded	O
.	O
γ	B-HyperparameterName
is	O
used	O
to	O
distinguish	O
when	O
it	O
is	O
more	O
important	O
to	O
focus	O
on	O
the	O
precision	O
or	O
F	O
0.5	O
of	O
a	O
correction	O
.	O
When	O
we	O
move	O
to	O
the	O
final	O
ensemble	O
with	O
confidence	O
tables	O
of	O
existing	O
systems	O
,	O
if	O
the	O
confidence	O
is	O
larger	O
than	O
γ	B-HyperparameterName
,	O
we	O
select	O
the	O
correction	O
proposed	O
by	O
the	O
system	O
that	O
has	O
the	O
best	O
F	O
0.5	O
on	O
the	O
type	O
of	O
this	O
correction	O
.	O
Otherwise	O
,	O
the	O
correction	O
by	O
a	O
system	O
with	O
the	O
best	O
precision	O
is	O
selected	O
.	O
In	O
Figure	O
1	O
,	O
⊗	O
means	O
the	O
outcome	O
is	O
obtained	O
by	O
combining	O
two	O
systems	O
represented	O
by	O
intersecting	O
lines	O
of	O
two	O
different	O
colours	O
.	O
If	O
there	O
are	O
multiple	O
⊗	O
on	O
a	O
line	O
,	O
it	O
means	O
the	O
ensemble	O
is	O
over	O
all	O
of	O
these	O
⊗	O
on	O
this	O
line	O
.	O
Figure	O
1	O
displays	O
three	O
types	O
of	O
ensemble	O
methods	O
based	O
on	O
all	O
of	O
the	O
CNN	O
-	O
based	O
and	O
Transformer	B-MethodName
-	O
based	O
translation	O
models	O
.	O
Combine	O
each	O
CNN	O
-	O
based	O
ensemble	O
model	O
with	O
each	O
of	O
the	O
selected	O
five	O
of	O
the	O
Transformer	B-MethodName
-	O
based	O
models	O
.	O
This	O
is	O
noted	O
as	O
'	O
ensemble	O
-	O
by	O
-	O
2	O
'	O
.	O
Perform	O
ensemble	O
over	O
all	O
of	O
the	O
ensemble	O
models	O
relating	O
to	O
either	O
CNN	O
ensemble	O
1	O
or	O
CNN	O
ensemble	O
2	O
,	O
noted	O
as	O
EoE	O
(	O
Ensemble	O
over	O
Ensemble	O
)	O
1	O
and	O
2	O
.	O
Ensemble	O
each	O
CNN	O
ensemble	O
model	O
with	O
some	O
selected	O
combinations	O
of	O
Transformer	B-MethodName
-	O
based	O
models	O
to	O
produce	O
16	O
strong	O
ensemble	O
system	O
outcomes	O
,	O
represented	O
as	O
'	O
Hybrid	O
Ensemble	O
'	O
in	O
Figure	O
1	O
.	O
It	O
is	O
where	O
multiple	O
lines	O
of	O
the	O
same	O
color	O
are	O
merged	O
into	O
one	O
line	O
in	O
Figure	O
1	O
.	O
After	O
getting	O
all	O
of	O
the	O
ensemble	O
outcomes	O
,	O
we	O
will	O
do	O
the	O
final	O
ensemble	O
step	O
:	O
select	O
the	O
best	O
confidence	O
for	O
each	O
type	O
from	O
each	O
single	O
or	O
ensemble	O
system	O
to	O
form	O
the	O
strongest	O
final	O
outcome	O
.	O
In	O
this	O
ensemble	O
step	O
,	O
we	O
use	O
the	O
last	O
aforementioned	O
sub	O
-	O
strategy	O
,	O
and	O
discard	O
the	O
error	O
types	O
with	O
very	O
low	O
confidence	O
to	O
boost	O
the	O
final	O
performance	O
.	O

We	O
trained	O
eight	O
Transformer	B-MethodName
-	O
based	O
translation	O
models	O
in	O
different	O
combinations	O
of	O
error	O
adaptation	O
,	O
domain	B-TaskName
adaptation	I-TaskName
,	O
and	O
GPU	O
set	O
.	O
In	O
Table	O
4	O
,	O
we	O
notice	O
that	O
a	O
smaller	O
error	O
weight	O
yields	O
higher	O
precision	O
and	O
a	O
slight	O
decrease	O
in	O
recall	O
.	O
We	O
set	O
the	O
copy	O
number	O
as	O
8	O
,	O
10	O
and	O
15	O
,	O
and	O
find	O
that	O
domain	B-TaskName
adaptation	I-TaskName
has	O
no	O
significant	O
effect	O
on	O
the	O
results	O
.	O
4	O
GPU	O
is	O
obviously	O
better	O
than	O
2	O
GPU	O
sets	O
,	O
which	O
is	O
probably	O
because	O
of	O
the	O
larger	O
batch	B-HyperparameterName
size	I-HyperparameterName
accumulation	O
for	O
gradient	O
calculation	O
.	O

As	O
described	O
in	O
Section	O
2.1.3	O
,	O
we	O
need	O
to	O
ensemble	O
all	O
of	O
the	O
CNN	O
-	O
based	O
and	O
Transformer	B-MethodName
-	O
based	O
translation	O
models	O
.	O
We	O
have	O
already	O
introduced	O
the	O
configuration	O
of	O
the	O
single	O
models	O
in	O
Section	O
3.2.1	O
and	O
Section	O
3.2.2	O
.	O
Next	O
we	O
will	O
describe	O
the	O
configuration	O
of	O
the	O
ensemble	O
system	O
.	O
For	O
the	O
three	O
ensemble	O
types	O
:	O
Ensemble	O
-	O
by	O
-	O
2	O
,	O
EoE	O
and	O
Hybrid	O
Ensemble	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
,	O
we	O
used	O
different	O
parameters	O
in	O
the	O
conflict	O
solver	O
.	O
We	O
did	O
a	O
small	O
-	O
scale	O
grid	O
search	O
for	O
the	O
parameters	O
in	O
Table	O
5	O
.	O
When	O
combining	O
two	O
models	O
that	O
are	O
not	O
strong	O
,	O
we	O
expect	O
a	O
higher	O
recall	O
so	O
β	B-HyperparameterName
was	O
not	O
high	O
.	O
For	O
EoE	O
and	O
hybrid	O
ensemble	O
,	O
we	O
expect	O
a	O
higher	O
precision	O
so	O
that	O
they	O
can	O
provide	O
high	O
quality	O
single	O
type	O
performance	O
.	O
Corrections	O
proposed	O
by	O
multiple	O
models	O
are	O
given	O
higher	O
weights	O
(	O
controlled	O
by	O
α	B-HyperparameterName
)	O
.	O
If	O
the	O
confidence	O
of	O
a	O
correction	O
finally	O
reaches	O
β	B-HyperparameterName
,	O
the	O
correction	O
will	O
be	O
adopted	O
.	O
In	O
the	O
final	O
ensemble	O
,	O
we	O
select	O
the	O
best	O
performance	O
on	O
each	O
type	O
from	O
each	O
single	O
system	O
or	O
ensemble	O
system	O
and	O
discard	O
the	O
corrections	O
with	O
low	O
precision	O
(	O
controlled	O
by	O
β	B-HyperparameterName
)	O
.	O
To	O
get	O
higher	O
F	O
0.5	O
,	O
in	O
the	O
case	O
where	O
the	O
precision	O
is	O
greater	O
than	O
a	O
predefined	O
threshold	O
(	O
controlled	O
by	O
γ	B-HyperparameterName
)	O
,	O
we	O
will	O
choose	O
the	O
model	O
with	O
the	O
highest	O
F	O
0.5	O
for	O
the	O
corresponding	O
error	O
type	O
.	O
The	O
final	O
outcome	O
of	O
the	O
data	O
set	O
is	O
then	O
fed	O
through	O
the	O
translation	O
models	O
and	O
ensemble	O
systems	O
again	O
to	O
do	O
a	O
second	O
pass	O
correction	O
.	O

Almost	O
all	O
argument	B-TaskName
mining	I-TaskName
frameworks	O
proposed	O
so	O
far	O
employ	O
a	O
pipeline	O
of	O
stages	O
,	O
each	O
of	O
which	O
is	O
addressing	O
a	O
sub	O
-	O
task	O
of	O
the	O
argument	B-TaskName
mining	I-TaskName
problem	O
(	O
Lippi	O
and	O
Torroni	O
,	O
2015a	O
)	O
.	O
The	O
segmentation	O
of	O
text	O
into	O
argumentative	O
units	O
is	O
typically	O
the	O
first	O
sub	O
-	O
task	O
encountered	O
in	O
such	O
an	O
argument	B-TaskName
mining	I-TaskName
pipeline	O
,	O
aiming	O
to	O
segment	O
texts	O
into	O
argumentative	O
and	O
non	O
-	O
argumentative	O
text	O
units	O
(	O
i.e.	O
segments	O
that	O
do	O
contain	O
or	O
do	O
not	O
contain	O
argument	O
components	O
,	O
such	O
as	O
claims	O
or	O
premises	O
)	O
.	O
The	O
granularity	O
of	O
argument	O
components	O
is	O
text	O
-	O
dependant	O
.	O
For	O
example	O
,	O
in	O
Wikipedia	O
articles	O
studied	O
in	O
(	O
Rinott	O
et	O
al	O
,	O
2015	O
)	O
,	O
argument	O
components	O
spanned	O
from	O
less	O
than	O
a	O
sentence	O
to	O
more	O
than	O
a	O
paragraph	O
,	O
although	O
90	O
%	O
of	O
the	O
cases	O
was	O
up	O
to	O
3	O
sentences	O
,	O
with	O
95	O
%	O
of	O
components	O
being	O
comprised	O
of	O
whole	O
sentences	O
.	O
Several	O
approaches	O
address	O
the	O
identification	O
of	O
argumentative	O
units	O
at	O
the	O
sentence	O
level	O
,	O
a	O
subtask	O
known	O
as	O
"	O
argumentative	O
sentence	O
detection	O
"	O
,	O
which	O
typically	O
models	O
the	O
task	O
as	O
a	O
binary	O
classification	O
problem	O
.	O
Employing	O
machine	O
learning	O
and	O
a	O
set	O
of	O
features	O
representing	O
sentences	O
,	O
the	O
goal	O
is	O
to	O
discard	O
sentences	O
that	O
are	O
not	O
part	O
(	O
or	O
do	O
not	O
contain	O
a	O
component	O
)	O
of	O
an	O
argument	O
.	O
As	O
reported	O
also	O
by	O
Lippi	O
and	O
Torroni	O
(	O
2015a	O
)	O
,	O
the	O
vast	O
majority	O
of	O
existing	O
approaches	O
employ	O
"	O
classic	O
,	O
off	O
-	O
the	O
-	O
self	O
"	O
classifiers	O
,	O
while	O
most	O
of	O
the	O
effort	O
is	O
devoted	O
to	O
highly	O
engineered	O
features	O
.	O
A	O
plethora	O
of	O
learning	O
algorithms	O
have	O
been	O
applied	O
on	O
the	O
task	O
,	O
including	O
Naive	O
Bayes	O
(	O
Moens	O
et	O
al	O
,	O
2007	O
;	O
Park	O
and	O
Cardie	O
,	O
2014	O
)	O
,	O
Support	O
Vector	O
Machines	O
(	O
SVM	B-MethodName
)	O
(	O
Mochales	O
and	O
Moens	O
,	O
2011	O
;	O
Rooney	O
et	O
al	O
,	O
2012	O
;	O
Park	O
and	O
Cardie	O
,	O
2014	O
;	O
Stab	O
and	O
Gurevych	O
,	O
2014b	O
;	O
Lippi	O
and	O
Torroni	O
,	O
2015b	O
)	O
,	O
Maximum	O
Entropy	O
(	O
Mochales	O
and	O
Moens	O
,	O
2011	O
)	O
,	O
Logistic	B-MethodName
Regression	I-MethodName
(	O
Goudas	O
et	O
al	O
,	O
2014	O
(	O
Goudas	O
et	O
al	O
,	O
,	O
2015Levy	O
et	O
al	O
,	O
2014	O
)	O
,	O
Decision	O
Trees	O
and	O
Random	O
Forests	O
(	O
Goudas	O
et	O
al	O
,	O
2014	O
(	O
Goudas	O
et	O
al	O
,	O
,	O
2015Stab	O
and	O
Gurevych	O
,	O
2014b	O
)	O
.	O
However	O
,	O
approaches	O
addressing	O
this	O
task	O
in	O
a	O
semi	O
-	O
supervised	O
or	O
unsupervised	O
manner	O
are	O
still	O
scarce	O
.	O
In	O
(	O
Petasis	O
and	O
Karkaletsis	O
,	O
2016	O
)	O
an	O
unsupervised	O
approach	O
is	O
presented	O
,	O
which	O
addresses	O
the	O
sub	O
-	O
task	O
of	O
identifying	O
the	O
main	O
claim	O
in	O
a	O
document	O
by	O
exploiting	O
evidence	O
from	O
an	O
extractive	B-TaskName
summarization	I-TaskName
algorithm	O
,	O
TextRank	O
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
)	O
.	O
In	O
an	O
attempt	O
to	O
study	O
the	O
overlap	O
between	O
graph	O
-	O
based	O
approaches	O
and	O
approaches	O
targeting	O
extractive	B-TaskName
summarization	I-TaskName
with	O
argument	B-TaskName
mining	I-TaskName
,	O
evaluation	O
results	O
suggest	O
a	O
positive	O
effect	O
on	O
the	O
sub	O
-	O
task	O
,	O
achieving	O
an	O
accuracy	B-MetricName
of	O
50	O
%	O
on	O
the	O
corpus	O
compiled	O
by	O
Hasan	O
and	O
Ng	O
(	O
2014	O
)	O
from	O
online	O
debate	O
forums	O
and	O
on	O
a	O
corpus	O
of	O
persuasive	O
essays	O
(	O
Stab	O
and	O
Gurevych	O
,	O
2014a	O
)	O
.	O
Regarding	O
semi	O
-	O
supervised	O
approaches	O
,	O
Habernal	O
and	O
Gurevych	O
(	O
2015	O
)	O
propose	O
new	O
unsupervised	O
features	O
that	O
exploit	O
clustering	O
of	O
unlabeled	O
argumentative	O
data	O
from	O
debate	O
portals	O
based	O
on	O
word	B-TaskName
embeddings	I-TaskName
,	O
outperforming	O
several	O
baselines	O
.	O
This	O
work	O
employs	O
also	O
topic	O
modeling	O
as	O
one	O
of	O
its	O
features	O
,	O
by	O
including	O
as	O
features	O
the	O
distributions	O
of	O
sentences	O
from	O
LDA	B-MethodName
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
.	O
Topic	O
modeling	O
has	O
been	O
mainly	O
exploited	O
for	O
identification	O
of	O
argumentative	O
relations	O
and	O
for	O
extraction	O
of	O
argument	O
and	O
domain	O
lexicons	O
.	O
In	O
Lawrence	O
et	O
al	O
(	O
2014	O
)	O
,	O
LDA	B-MethodName
is	O
used	O
to	O
decide	O
whether	O
a	O
proposition	O
can	O
be	O
attached	O
to	O
its	O
previous	O
proposition	O
in	O
order	O
to	O
identify	O
non	O
directional	O
relations	O
among	O
propositions	O
detected	O
through	O
classifiers	O
based	O
on	O
words	O
and	O
part	O
-	O
ofspeech	O
tags	O
.	O
LDA	B-MethodName
has	O
been	O
also	O
used	O
to	O
mine	O
lexicons	O
of	O
argument	O
(	O
words	O
that	O
are	O
topic	O
independent	O
)	O
and	O
domain	O
words	O
(	O
Nguyen	O
and	O
Litman	O
,	O
2015	O
)	O
,	O
by	O
post	O
-	O
processing	O
document	O
topics	O
generated	O
by	O
LDA	B-MethodName
.	O
These	O
lexicons	O
have	O
been	O
used	O
as	O
features	O
for	O
supervised	O
approaches	O
for	O
argument	B-TaskName
mining	I-TaskName
(	O
Nguyen	O
and	O
Litman	O
,	O
2016a	O
,	O
b	O
)	O
.	O
However	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
no	O
prior	O
approach	O
has	O
applied	O
topic	O
modeling	O
to	O
argumentative	O
sentence	O
detection	O
in	O
an	O
unsupervised	O
setting	O
,	O
which	O
is	O
the	O
featuring	O
aspect	O
of	O
the	O
proposed	O
A2	O
T	O
approach	O
presented	O
in	O
the	O
following	O
.	O

Sentence	O
Index	O
(	O
S	O
)	O
Figure	O
1	O
:	O
Schema	O
of	O
the	O
A2	O
T	O
approach	O
s	O
i	O
is	O
a	O
sentence	O
containing	O
an	O
argumentative	O
unit	O
,	O
c	O
is	O
the	O
text	O
containing	O
s	O
,	O
and	O
l	O
is	O
the	O
argumentative	O
role	O
expressed	O
by	O
the	O
unit	O
(	O
e.g.	O
,	O
major	O
claim	O
,	O
claim	O
,	O
premise	O
)	O
.	O
The	O
A2	O
T	O
approach	O
is	O
articulated	O
in	O
the	O
following	O
activities	O
:	O
Sentence	O
extraction	O
.	O
A2	O
T	O
approach	O
is	O
characterized	O
by	O
the	O
use	O
of	O
topic	O
modeling	O
at	O
sentencelevel	O
granularity	O
.	O
For	O
this	O
reason	O
,	O
a	O
pre	O
-	O
processing	O
step	O
of	O
the	O
corpus	O
C	O
is	O
enforced	O
based	O
on	O
conventional	O
techniques	O
for	O
sentence	O
tokenization	O
,	O
words	O
tokenization	O
,	O
normalization	O
,	O
and	O
indexing	O
(	O
Manning	O
et	O
al	O
,	O
2008	O
)	O
.	O
The	O
result	O
is	O
a	O
sentence	O
set	O
S	O
=	O
{	O
−	O
s	O
1	O
,	O
c	O
,	O
pos	O
1	O
,	O
.	O
.	O
.	O
,	O
−	O
s	O
m	O
,	O
c	O
,	O
pos	O
m	O
}	O
,	O
where	O
−	O
s	O
i	O
is	O
the	O
vector	O
representation	O
of	O
the	O
sentence	O
s	O
i	O
and	O
c	O
,	O
pos	O
are	O
text	O
and	O
position	O
in	O
the	O
text	O
where	O
the	O
sentence	O
appears	O
,	O
respectively	O
.	O
The	O
sentence	O
set	O
is	O
stored	O
in	O
a	O
sentence	O
index	O
for	O
efficient	O
access	O
of	O
S	O
elements	O
.	O
Topic	O
modeling	O
.	O
The	O
set	O
of	O
extracted	O
sentences	O
S	O
is	O
used	O
as	O
the	O
document	O
corpus	O
on	O
which	O
topic	O
modeling	O
is	O
applied	O
.	O
The	O
result	O
of	O
this	O
activity	O
is	O
twofold	O
.	O
First	O
,	O
topic	O
modeling	O
returns	O
a	O
set	O
of	O
topics	O
T	O
=	O
{	O
t	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
t	O
k	O
}	O
representing	O
the	O
latent	O
variables	O
that	O
are	O
most	O
representative	O
for	O
the	O
sentences	O
S.	O
Second	O
,	O
topic	O
modeling	O
returns	O
a	O
distribution	O
of	O
sentences	O
over	O
topics	O
θ	B-HyperparameterName
=	O
{	O
θ	B-HyperparameterName
s	O
1	O
,	O
.	O
.	O
.	O
,	O
θ	B-HyperparameterName
sm	O
}	O
.	O
In	O
particular	O
,	O
θ	B-HyperparameterName
s	O
i	O
=	O
[	O
p	O
(	O
t	O
0	B-DatasetName
|	O
s	O
i	O
)	O
,	O
.	O
.	O
.	O
,	O
p	O
(	O
t	O
k	O
|	O
s	O
i	O
)	O
]	O
is	O
the	O
probability	O
distribution	O
of	O
the	O
sentence	O
s	O
i	O
over	O
the	O
set	O
of	O
topics	O
T	O
,	O
where	O
p	O
(	O
t	O
j	O
|	O
s	O
i	O
)	O
represents	O
the	O
probability	O
of	O
the	O
topic	O
t	O
j	O
given	O
the	O
sentence	O
s	O
i	O
(	O
i.e.	O
,	O
the	O
so	O
-	O
called	O
assignment	O
value	O
of	O
s	O
i	O
to	O
t	O
j	O
)	O
.	O
Attraction	O
evaluation	O
.	O
The	O
notion	O
of	O
attraction	O
is	O
introduced	O
to	O
measure	O
the	O
degree	O
of	O
focus	O
that	O
characterizes	O
sentences	O
with	O
respect	O
to	O
the	O
emerged	O
topics	O
.	O
To	O
this	O
end	O
,	O
the	O
distribution	O
of	O
sentences	O
over	O
topics	O
θ	B-HyperparameterName
is	O
exploited	O
with	O
the	O
aim	O
at	O
determining	O
the	O
best	O
topic	O
assignment	O
for	O
each	O
sentence	O
of	O
S.	O
The	O
result	O
is	O
an	O
attraction	O
set	O
A	O
=	O
{	O
s	O
1	O
,	O
a	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
m	O
,	O
a	O
m	O
}	O
where	O
s	O
i	O
is	O
a	O
sentence	O
of	O
S	O
and	O
a	O
i	O
is	O
its	O
corresponding	O
attraction	O
value	O
.	O
Sentence	O
labeling	O
.	O
By	O
exploiting	O
the	O
attraction	O
set	O
A	O
,	O
labeling	O
has	O
the	O
goal	O
to	O
determine	O
the	O
sentences	O
of	O
S	O
that	O
are	O
more	O
focused	O
on	O
a	O
specific	O
topic	O
,	O
according	O
to	O
the	O
hypothesis	O
that	O
those	O
sentences	O
are	O
the	O
argumentative	O
units	O
.	O
In	O
a	O
basic	O
scenario	O
,	O
labeling	O
consists	O
in	O
distinguishing	O
between	O
sentences	O
that	O
are	O
argumentative	O
units	O
(	O
l	O
=	O
au	O
)	O
and	O
sentences	O
that	O
are	O
not	O
argumentative	O
units	O
(	O
l	O
=	O
au	O
)	O
.	O
In	O
a	O
more	O
articulated	O
scenario	O
,	O
labeling	O
consists	O
in	O
assigning	O
a	O
role	O
to	O
sentences	O
that	O
are	O
recognized	O
as	O
argumentative	O
units	O
.	O
For	O
instance	O
,	O
it	O
is	O
possible	O
to	O
distinguish	O
argumentative	O
-	O
unit	O
sentences	O
that	O
are	O
claims	O
(	O
l	O
=	O
cl	O
)	O
,	O
major	O
claims	O
(	O
l	O
=	O
mc	O
)	O
,	O
or	O
premises	O
(	O
l	O
=	O
pr	O
)	O
.	O
A	O
sentence	O
s	O
recognized	O
as	O
argumentative	O
unit	O
is	O
inserted	O
in	O
the	O
final	O
set	O
U	O
with	O
the	O
assigned	O
label	O
and	O
it	O
is	O
returned	O
as	O
a	O
result	O
of	O
A2	O
T	O
.	O

In	O
A2	O
T	O
,	O
the	O
sentence	O
extraction	O
step	O
is	O
enforced	O
by	O
relying	O
on	O
standard	O
techniques	O
for	O
representing	O
documents	O
in	O
terms	O
of	O
feature	O
vectors	O
and	O
bag	O
of	O
words	O
(	O
using	O
tf	O
-	O
idf	O
as	O
weighting	O
scheme	O
)	O
(	O
Castano	O
et	O
al	O
,	O
2017	O
)	O
.	O
Probabilistic	O
topic	O
modeling	O
is	O
exploited	O
to	O
enforce	O
the	O
subsequent	O
topic	O
modeling	O
step	O
.	O
Probabilistic	O
topic	B-TaskName
models	I-TaskName
are	O
a	O
suite	O
of	O
algorithms	O
whose	O
aim	O
is	O
to	O
discover	O
the	O
hidden	O
thematic	O
structure	O
in	O
large	O
archives	O
of	O
documents	O
,	O
namely	O
sentences	O
in	O
A2	O
T	O
.	O
The	O
idea	O
is	O
that	O
documents	O
are	O
represented	O
as	O
random	O
mixtures	O
over	O
latent	O
topics	O
,	O
where	O
each	O
topic	O
is	O
characterized	O
by	O
a	O
distribution	O
over	O
words	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
.	O
Probabilistic	O
topic	O
modeling	O
algorithms	O
infer	O
the	O
distribution	O
θ	B-HyperparameterName
of	O
documents	O
over	O
topics	O
and	O
the	O
distribution	O
φ	O
of	O
words	O
over	O
topics	O
,	O
by	O
sampling	O
from	O
the	O
bag	O
of	O
words	O
of	O
each	O
document	O
.	O
In	O
our	O
approach	O
,	O
we	O
choose	O
to	O
exploit	O
the	O
Hierarchical	O
Dirichlet	O
Process	O
(	O
HDP	O
)	O
.	O
With	O
respect	O
to	O
other	O
algorithms	O
(	O
such	O
as	O
LDA	B-MethodName
)	O
,	O
HDP	O
has	O
the	O
advantage	O
to	O
provide	O
the	O
optimal	O
number	O
of	O
topics	O
instead	O
of	O
requiring	O
to	O
set	O
such	O
a	O
number	O
as	O
input	O
(	O
Teh	O
et	O
al	O
,	O
2006	O
)	O
.	O
Attraction	O
evaluation	O
.	O
The	O
notion	O
of	O
attraction	O
is	O
introduced	O
in	O
A2	O
T	O
to	O
capture	O
the	O
intuition	O
that	O
argumentative	O
units	O
are	O
related	O
to	O
the	O
distribution	O
of	O
sentences	O
over	O
topics	O
.	O
Consider	O
a	O
set	O
of	O
sentences	O
S	O
and	O
the	O
distribution	O
θ	B-HyperparameterName
of	O
sentences	O
over	O
the	O
set	O
of	O
topics	O
T	O
.	O
The	O
more	O
the	O
distribution	O
θ	B-HyperparameterName
s	O
i	O
of	O
a	O
sentence	O
s	O
i	O
over	O
the	O
topics	O
is	O
unequal	O
,	O
the	O
more	O
s	O
i	O
is	O
focused	O
on	O
a	O
topic	O
,	O
thus	O
suggesting	O
s	O
i	O
as	O
a	O
possible	O
argumentative	O
unit	O
.	O
A	O
further	O
feature	O
that	O
attraction	O
aims	O
to	O
capture	O
is	O
that	O
argumentative	O
units	O
often	O
appear	O
either	O
at	O
the	O
beginning	O
or	O
at	O
the	O
end	O
of	O
texts	O
.	O
The	O
attraction	O
a	O
i	O
of	O
a	O
sentence	O
s	O
i	O
is	O
calculated	O
as	O
follows	O
:	O
a	O
i	O
=	O
Kϕ	O
s	O
i	O
+	O
(	O
1	O
−	O
K	O
)	O
ρ	O
s	O
i	O
s	O
j	O
c	O
ρ	O
s	O
j	O
,	O
ϕ	O
s	O
i	O
=	O
max	O
(	O
θ	B-HyperparameterName
s	O
i	O
)	O
is	O
a	O
measure	O
of	O
how	O
much	O
s	O
i	O
is	O
focused	O
on	O
a	O
topic	O
and	O
ρ	O
s	O
i	O
=	O
αf	O
(	O
pos	O
i	O
)	O
2	O
+	O
βf	O
(	O
pos	O
i	O
)	O
+	O
γ	B-HyperparameterName
is	O
a	O
parabolic	O
function	O
over	O
the	O
position	O
of	O
the	O
sentence	O
in	O
c.	O
In	O
particular	O
,	O
given	O
L	O
(	O
c	O
)	O
as	O
the	O
number	O
of	O
sentences	O
in	O
c	O
,	O
f	O
(	O
pos	O
i	O
)	O
=	O
L	O
(	O
c	O
)	O
2	O
−	O
pos	O
i	O
such	O
that	O
f	O
(	O
pos	O
i	O
)	O
is	O
higher	O
when	O
s	O
i	O
appears	O
either	O
at	O
the	O
beginning	O
or	O
at	O
the	O
end	O
of	O
c.	O
The	O
parameters	O
α	B-HyperparameterName
,	O
β	B-HyperparameterName
,	O
γ	B-HyperparameterName
determine	O
the	O
shape	O
of	O
ρ	O
s	O
i	O
.	O
K	O
[	O
0	B-DatasetName
,	O
1	O
]	O
is	O
a	O
constant	O
value	O
used	O
to	O
balance	O
the	O
role	O
of	O
focus	O
and	O
position	O
in	O
calculating	O
the	O
attraction	O
.	O
The	O
attraction	O
a	O
i	O
can	O
be	O
interpreted	O
as	O
the	O
probability	O
of	O
a	O
sentence	O
s	O
i	O
to	O
contain	O
an	O
argumentative	O
unit	O
.	O
According	O
to	O
this	O
interpretation	O
,	O
given	O
s	O
i	O
,	O
also	O
the	O
contiguous	O
sentences	O
s	O
i−1	O
and	O
s	O
i+1	O
have	O
a	O
chance	O
to	O
be	O
argumentative	O
units	O
.	O
As	O
a	O
result	O
,	O
given	O
the	O
calculated	O
attraction	O
set	O
A	O
,	O
we	O
update	O
the	O
attraction	O
values	O
a	O
i	O
through	O
an	O
interpolation	O
mechanism	O
based	O
on	O
the	O
Savitzky	O
-	O
Golay	O
smoothing	O
filter	O
(	O
SGF	O
)	O
(	O
Savitzky	O
and	O
Golay	O
,	O
1964	O
)	O
,	O
so	O
that	O
A	O
:	O
=	O
SGF	O
(	O
A	O
)	O
.	O
In	O
Figure	O
2	O
,	O
an	O
example	O
of	O
attraction	O
evaluation	O
is	O
provided	O
by	O
showing	O
the	O
values	O
of	O
ϕ	O
,	O
ρ	O
,	O
attraction	O
,	O
and	O
interpolated	O
attraction	O
for	O
all	O
the	O
sentences	O
within	O
one	O
considered	O
student	O
essays	O
included	O
in	O
the	O
corpus	O
from	O
(	O
Stab	O
and	O
Gurevych	O
,	O
2014a	O
)	O
(	O
see	O
Section	O
4	O
)	O
.	O
Sentence	O
labeling	O
.	O
Sentence	O
labeling	O
has	O
the	O
goal	O
to	O
turn	O
attraction	O
values	O
into	O
labeled	O
categories	O
.	O
Consider	O
a	O
set	O
of	O
possible	O
labels	O
L	O
=	O
{	O
l	O
1	O
,	O
.	O
.	O
.	O
,	O
l	O
g	O
}	O
,	O
each	O
one	O
denoting	O
a	O
possible	O
argumentative	O
role	O
that	O
can	O
be	O
assigned	O
to	O
a	O
sentence	O
.	O
Given	O
a	O
set	O
of	O
attraction	O
values	O
A	O
,	O
a	O
thresholdbased	O
mechanism	O
is	O
enforced	O
to	O
assign	O
labels	O
to	O
sentences	O
according	O
to	O
the	O
following	O
scheme	O
:	O
a	O
i	O
<	O
τ	O
1	O
:	O
s	O
i	O
l	O
1	O
τ	O
1	O
≤	O
a	O
i	O
<	O
τ	O
2	O
:	O
s	O
i	O
l	O
2	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
a	O
i	O
≥	O
τ	O
g−1	O
:	O
s	O
i	O
l	O
g	O
where	O
τ	O
1	O
<	O
τ	O
2	O
<	O
...	O
<	O
τ	O
g−1	O
(	O
τ	O
1	O
,	O
.	O
.	O
.	O
τ	O
g−1	O
(	O
0	B-DatasetName
,	O
1	O
]	O
)	O
are	O
prefixed	O
threshold	O
values	O
.	O
The	O
result	O
of	O
sentence	O
labeling	O
is	O
a	O
partition	O
of	O
S	O
into	O
g	O
categories	O
with	O
associated	O
labels	O
.	O
In	O
the	O
experiments	O
,	O
we	O
discuss	O
two	O
different	O
strategies	O
for	O
sentence	O
labeling	O
.	O
The	O
first	O
one	O
is	O
a	O
two	O
-	O
class	O
labeling	O
strategy	O
where	O
the	O
possible	O
labels	O
for	O
a	O
sentence	O
are	O
argumentative	O
unit	O
(	O
au	O
)	O
and	O
non	O
-	O
argumentative	O
unit	O
(	O
au	O
.	O
The	O
second	O
strategy	O
is	O
a	O
multi	O
-	O
class	O
labeling	O
in	O
which	O
the	O
possible	O
labels	O
of	O
a	O
sentence	O
are	O
non	O
-	O
argumentative	O
unit	O
au	O
,	O
premise	O
(	O
pr	O
)	O
,	O
claim	O
(	O
cl	O
)	O
,	O
and	O
major	O
claim	O
(	O
mc	O
)	O
.	O

For	O
evaluation	O
of	O
the	O
proposed	O
A2	O
T	O
approach	O
,	O
we	O
have	O
used	O
two	O
English	O
corpora	O
.	O
The	O
first	O
corpus	O
(	O
C1	O
in	O
the	O
following	O
)	O
is	O
a	O
collection	O
of	O
90	O
student	O
persuasive	O
essays	O
(	O
Stab	O
and	O
Gurevych	O
,	O
2014a	O
)	O
which	O
has	O
been	O
manually	O
annotated	O
with	O
major	O
claims	O
(	O
one	O
per	O
essay	O
)	O
,	O
claims	O
and	O
premises	O
at	O
the	O
clause	O
level	O
.	O
In	O
addition	O
,	O
the	O
corpus	O
contains	O
manual	O
annotations	O
of	O
argumentative	O
relations	O
,	O
where	O
the	O
claims	O
and	O
premises	O
are	O
linked	O
,	O
while	O
claims	O
are	O
linked	O
to	O
the	O
major	O
claim	O
either	O
with	O
a	O
support	O
or	O
an	O
attack	O
relation	O
.	O
Interannotation	O
agreement	O
has	O
been	O
measured	O
to	O
unitized	O
alpha	B-HyperparameterName
(	O
Krippendorff	O
,	O
2004	O
)	O
α	B-HyperparameterName
U	O
=	O
0.724	O
.	O
These	O
90	O
essays	O
consist	O
of	O
a	O
total	O
of	O
1	O
,	O
675	O
sentences	O
(	O
from	O
which	O
19.3	O
%	O
contain	O
no	O
argument	O
components	O
)	O
,	O
with	O
an	O
average	O
length	O
of	O
18.61	O
±	O
7	O
sentences	O
per	O
essay	O
,	O
while	O
the	O
5.4	O
%	O
of	O
sentences	O
contain	O
a	O
major	O
claim	O
,	O
26.4	O
%	O
contain	O
a	O
claim	O
,	O
and	O
61.1	O
%	O
contain	O
a	O
premise	O
.	O
The	O
second	O
corpus	O
(	O
C2	O
in	O
the	O
following	O
)	O
has	O
been	O
compiled	O
and	O
manually	O
annotated	O
as	O
described	O
in	O
(	O
Habernal	O
and	O
Gurevych	O
,	O
2017	O
)	O
.	O
This	O
corpus	O
focuses	O
on	O
user	O
generated	O
content	O
,	O
including	O
user	O
comments	O
,	O
forum	O
posts	O
,	O
blogs	O
,	O
and	O
newspaper	O
articles	O
,	O
covering	O
several	O
thematic	O
domains	O
from	O
educational	O
controversies	O
,	O
such	O
as	O
homeschooling	O
,	O
private	O
vs.	O
public	O
schools	O
,	O
or	O
singlesex	O
education	O
.	O
Containing	O
in	O
total	O
340	O
documents	O
,	O
the	O
corpus	O
has	O
been	O
manually	O
annotated	O
with	O
an	O
argument	O
scheme	O
based	O
on	O
extended	O
Toulmin	O
's	O
model	O
,	O
involving	O
claims	O
,	O
premises	O
,	O
and	O
backing	O
,	O
rebuttal	O
,	O
refutation	O
argument	O
units	O
.	O
The	O
corpus	O
contains	O
documents	O
of	O
various	O
sizes	O
,	O
with	O
a	O
mean	O
size	O
of	O
11.44	O
±	O
11.70	O
sentences	O
per	O
document	O
,	O
while	O
the	O
inter	O
-	O
annotator	O
agreement	O
was	O
measured	O
as	O
α	B-HyperparameterName
U	O
=	O
0.48	O
.	O
The	O
corpus	O
consists	O
of	O
3	O
,	O
899	O
sentences	O
,	O
from	O
which	O
2	O
,	O
214	O
sentences	O
(	O
57	O
%	O
)	O
contain	O
no	O
argument	O
components	O
.	O
Both	O
corpora	O
have	O
been	O
preprocessed	O
with	O
NLTK	O
(	O
Loper	O
and	O
Bird	O
,	O
2002	O
)	O
in	O
order	O
to	O
identify	O
tokens	O
and	O
sentences	O
.	O
Then	O
,	O
each	O
sentence	O
was	O
annotated	O
as	O
argumentative	O
or	O
non	O
-	O
argumentative	O
,	O
depending	O
on	O
whether	O
it	O
contained	O
an	O
argument	O
unit	O
(	O
i.e.	O
a	O
text	O
fragment	O
annotated	O
as	O
major	O
claim	O
,	O
claim	O
,	O
or	O
premise	O
)	O
.	O
In	O
addition	O
,	O
each	O
argumentative	O
sentence	O
was	O
further	O
annotated	O
with	O
one	O
of	O
major	O
claim	O
,	O
claim	O
,	O
and	O
premise	O
,	O
based	O
on	O
the	O
type	O
of	O
the	O
contained	O
argumentative	O
unit	O
.	O
For	O
the	O
second	O
corpus	O
,	O
which	O
utilizes	O
a	O
richer	O
argument	O
scheme	O
,	O
we	O
have	O
considered	O
backing	O
,	O
rebuttal	O
and	O
refutation	O
units	O
as	O
premises	O
.	O
This	O
second	O
corpus	O
does	O
not	O
contain	O
units	O
annotated	O
as	O
major	O
claims	O
.	O
The	O
following	O
three	O
tasks	O
have	O
been	O
executed	O
:	O
Task	O
1	O
:	O
Argumentative	O
sentence	O
identification	O
-	O
given	O
a	O
sentence	O
,	O
classify	O
whether	O
or	O
not	O
it	O
contains	O
an	O
argument	O
component	O
.	O
Task	O
2	O
:	O
Major	O
claim	O
identification	O
-	O
given	O
a	O
argumentative	O
sentence	O
,	O
classify	O
whether	O
or	O
not	O
it	O
contains	O
a	O
major	O
claim	O
.	O
Task	O
3	O
:	O
Argumentative	O
sentence	B-TaskName
classification	I-TaskName
-	O
given	O
a	O
sentence	O
,	O
classify	O
the	O
sentence	O
as	O
major	O
claim	O
,	O
claim	O
,	O
premise	O
,	O
or	O
nonargumentative	O
.	O
Baseline	O
.	O
As	O
a	O
baseline	O
for	O
comparison	O
against	O
our	O
approach	O
,	O
we	O
created	O
a	O
probabilistic	O
classifier	O
of	O
sentences	O
which	O
evaluates	O
the	O
probability	O
p	O
(	O
l	O
=	O
au	O
|	O
s	O
i	O
)	O
as	O
follows	O
.	O
Given	O
the	O
text	O
c	O
containing	O
L	O
(	O
c	O
)	O
sentences	O
s	O
i	O
,	O
let	O
be	O
ζ	O
c	O
∼	O
Dir	O
(	O
α	B-HyperparameterName
)	O
the	O
probability	O
distribution	O
of	O
the	O
sentences	O
in	O
c	O
,	O
such	O
that	O
ζ	O
s	O
i	O
c	O
∼	O
p	O
(	O
l	O
=	O
au	O
|	O
s	O
i	O
)	O
.	O
The	O
L	O
(	O
c	O
)	O
parameters	O
α	B-HyperparameterName
used	O
to	O
generate	O
ζ	O
c	O
are	O
defined	O
such	O
that	O
α	B-HyperparameterName
i	O
=	O
L	O
(	O
c	O
)	O
2	O
−	O
pos	O
i	O
.	O
The	O
rationale	O
of	O
this	O
procedure	O
is	O
to	O
bias	O
the	O
random	O
assignment	O
of	O
a	O
sentence	O
to	O
the	O
au	O
label	O
in	O
favor	O
of	O
sentences	O
appearing	O
either	O
in	O
the	O
beginning	O
or	O
in	O
the	O
end	O
of	O
a	O
text	O
.	O
This	O
bias	O
attempts	O
to	O
model	O
empirical	O
evi	O
-	O
dence	O
that	O
in	O
several	O
types	O
of	O
documents	O
,	O
the	O
density	O
of	O
argumentative	O
units	O
in	O
various	O
sections	O
of	O
documents	O
depends	O
on	O
the	O
structure	O
of	O
documents	O
.	O
The	O
beginning	O
and	O
end	O
of	O
a	O
document	O
are	O
expected	O
to	O
contain	O
argumentative	O
units	O
in	O
structured	O
documents	O
like	O
news	O
,	O
scientific	O
publications	O
,	O
or	O
argumentative	O
essays	O
(	O
Stab	O
and	O
Gurevych	O
,	O
2017	O
)	O
,	O
where	O
major	O
claims	O
and	O
supporting	O
premises	O
are	O
frequently	O
found	O
in	O
the	O
beginning	O
of	O
documents	O
,	O
with	O
documents	O
frequently	O
ending	O
with	O
repeating	O
the	O
major	O
claims	O
and	O
supporting	O
evidence	O
.	O

The	O
goal	O
of	O
Task	O
1	O
is	O
to	O
associate	O
each	O
sentence	O
of	O
the	O
corpora	O
to	O
a	O
label	O
in	O
L	O
=	O
{	O
au	O
,	O
au	O
}	O
by	O
following	O
a	O
two	O
-	O
class	O
labeling	O
strategy	O
(	O
see	O
Section	O
3	O
)	O
.	O
As	O
a	O
first	O
experiment	O
,	O
we	O
performed	O
sentence	O
labeling	O
with	O
different	O
threshold	O
ranging	O
from	O
0	B-DatasetName
to	O
1	O
with	O
step	O
0.05	O
.	O
In	O
Figure	O
3	O
,	O
we	O
report	O
the	O
precision	O
,	O
recall	O
,	O
and	O
F1	B-MetricName
-	O
measure	O
for	O
A2	O
T	O
and	O
for	O
the	O
baseline	O
.	O
In	O
addition	O
,	O
we	O
report	O
also	O
the	O
results	O
of	O
applying	O
sentence	O
labeling	O
based	O
on	O
ϕ	O
and	O
ρ	O
(	O
the	O
components	O
of	O
attraction	O
)	O
separately	O
.	O
The	O
parameter	O
K	O
for	O
attraction	O
calculation	O
has	O
been	O
set	O
to	O
0.5	O
.	O
Since	O
A2	O
T	O
is	O
an	O
unsupervised	O
method	O
,	O
there	O
is	O
no	O
easy	O
way	O
to	O
define	O
the	O
threshold	O
parameter	O
τ	O
,	O
which	O
has	O
been	O
empirically	O
defined	O
to	O
τ	O
=	O
0.3	O
.	O
The	O
different	O
behavior	O
of	O
A2	O
T	O
with	O
respect	O
to	O
the	O
baseline	O
is	O
shown	O
in	O
the	O
confusion	O
matrices	O
reported	O
in	O
Figures	O
4	O
and	O
5	O
.	O
From	O
Figure	O
3	O
,	O
we	O
can	O
see	O
that	O
A2	O
T	O
is	O
significantly	O
better	O
than	O
the	O
baseline	O
,	O
especially	O
for	O
the	O
C1	O
corpus	O
.	O
A	O
characteristic	O
of	O
this	O
corpus	O
is	O
that	O
argumentative	O
units	O
are	O
frequently	O
located	O
in	O
the	O
introduction	O
or	O
the	O
conclusion	O
of	O
an	O
essay	O
,	O
which	O
is	O
also	O
reflected	O
by	O
the	O
baseline	O
that	O
achieved	O
an	O
F1	B-MetricName
-	O
measure	O
of	O
0.35	O
for	O
a	O
threshold	O
of	O
τ	O
=	O
0.05	O
(	O
with	O
the	O
baseline	O
being	O
particularly	O
precise	O
,	O
suggesting	O
that	O
argumentative	O
units	O
are	O
very	O
frequently	O
at	O
the	O
beginning	O
and	O
end	O
of	O
essays	O
)	O
.	O
Both	O
components	O
of	O
attraction	O
(	O
ϕ	O
and	O
ρ	O
)	O
perform	O
well	O
,	O
with	O
the	O
topic	O
component	O
ϕ	O
being	O
slightly	O
better	O
than	O
position	O
information	O
ρ	O
,	O
both	O
in	O
precision	O
and	O
recall	O
.	O
The	O
results	O
are	O
similar	O
for	O
corpus	O
C2	O
,	O
with	O
A2	O
T	O
surpassing	O
the	O
baseline	O
,	O
although	O
A2	O
T	O
advantage	O
in	O
precision	O
is	O
smaller	O
.	O
As	O
shown	O
in	O
the	O
confusion	O
matrix	O
of	O
Figure	O
5	O
,	O
the	O
main	O
source	O
of	O
error	O
is	O
the	O
large	O
number	O
of	O
false	O
positives	O
for	O
the	O
au	O
class	O
,	O
proposing	O
more	O
argumentative	O
units	O
than	O
what	O
have	O
been	O
manu	O
-	O
ally	O
identified	O
in	O
corpus	O
C2	O
.	O
This	O
can	O
be	O
attributed	O
to	O
the	O
sparseness	O
of	O
argumentative	O
units	O
in	O
the	O
C2	O
corpus	O
,	O
with	O
almost	O
60	O
%	O
of	O
the	O
sentences	O
being	O
non	O
-	O
argumentative	O
.	O

The	O
goal	O
of	O
Task	O
3	O
is	O
to	O
associate	O
each	O
sentence	O
of	O
the	O
corpora	O
to	O
a	O
label	O
in	O
L	O
=	O
{	O
au	O
,	O
pr	O
,	O
cl	O
,	O
mc	O
}	O
by	O
following	O
a	O
multi	O
-	O
class	O
labeling	O
strategy	O
(	O
see	O
Section	O
3	O
)	O
.	O
In	O
particular	O
,	O
we	O
adopted	O
the	O
thresholds	O
[	O
0.1	O
,	O
0.3	O
,	O
0.5	O
]	O
.	O
This	O
task	O
is	O
challenging	O
since	O
it	O
is	O
required	O
to	O
distinguish	O
the	O
different	O
role	O
played	O
in	O
argumentation	O
by	O
sentences	O
that	O
are	O
often	O
very	O
similar	O
from	O
the	O
terminological	O
point	O
of	O
view	O
.	O
The	O
confusion	O
matrix	O
for	O
corpus	O
C1	O
is	O
shown	O
in	O
Figure	O
6	O
,	O
while	O
Figure	O
7	O
shows	O
the	O
confusion	O
matrix	O
for	O
corpus	O
C2	O
.	O
Both	O
A2	O
T	O
and	O
the	O
baseline	O
achieve	O
low	O
results	O
,	O
but	O
the	O
accuracy	B-MetricName
of	O
A2	O
T	O
is	O
0.3	O
against	O
the	O
0.1	O
of	O
the	O
baseline	O
.	O
From	O
Figure	O
6	O
we	O
see	O
that	O
A2	O
T	O
achieved	O
good	O
results	O
for	O
premises	O
,	O
and	O
quite	O
good	O
results	O
for	O
claims	O
,	O
although	O
distinguishing	O
between	O
claims	O
and	O
premises	O
is	O
challenging	O
for	O
the	O
A2	O
T	O
approach	O
.	O
In	O
particular	O
,	O
the	O
role	O
it	O
may	O
be	O
useful	O
to	O
work	O
also	O
on	O
semantic	O
relations	O
holding	O
among	O
sentences	O
.	O
This	O
is	O
actually	O
one	O
of	O
the	O
future	O
tasks	O
in	O
our	O
research	O
work	O
.	O
Another	O
specific	O
challenge	O
emerges	O
when	O
we	O
consider	O
the	O
corpus	O
C2	O
.	O
Indeed	O
,	O
C2	O
contains	O
a	O
limited	O
number	O
of	O
argumentative	O
sentences	O
with	O
respect	O
to	O
the	O
corpus	O
size	O
.	O
In	O
this	O
case	O
,	O
since	O
we	O
analyze	O
all	O
the	O
sentences	O
according	O
to	O
their	O
bag	O
of	O
words	O
,	O
we	O
tend	O
to	O
overestimate	O
the	O
number	O
of	O
argumentative	O
units	O
,	O
collecting	O
a	O
relatively	O
high	O
number	O
of	O
false	O
positives	O
.	O

Monolingual	O
training	O
of	O
a	O
dependency	O
parser	O
has	O
been	O
successful	O
when	O
relatively	O
large	O
treebanks	O
are	O
available	O
(	O
Kiperwasser	O
and	O
Goldberg	O
,	O
2016	O
;	O
Dozat	O
and	O
Manning	O
,	O
2017	O
)	O
.	O
However	O
,	O
for	O
many	O
languages	O
,	O
treebanks	O
are	O
either	O
too	O
small	O
or	O
unavailable	O
.	O
Therefore	O
,	O
multilingual	O
models	O
leveraging	O
Universal	O
Dependency	O
annotations	O
(	O
Nivre	O
et	O
al	O
,	O
2018	O
)	O
have	O
drawn	O
serious	O
attention	O
(	O
Zhang	O
and	O
Barzilay	O
,	O
2015	O
;	O
Ammar	O
et	O
al	O
,	O
2016	O
;	O
Kondratyuk	O
and	O
Straka	O
,	O
2019	O
)	O
.	O
Multilingual	O
approaches	O
learn	O
generalizations	O
across	O
languages	O
and	O
share	O
information	O
between	O
them	O
,	O
making	O
it	O
possible	O
to	O
parse	O
a	O
target	O
language	O
without	O
supervision	O
in	O
that	O
language	O
.	O
Moreover	O
,	O
multilingual	O
models	O
can	O
be	O
faster	O
to	O
train	O
and	O
easier	O
to	O
maintain	O
than	O
a	O
large	O
set	O
of	O
monolingual	O
models	O
.	O
However	O
,	O
scaling	O
a	O
multilingual	O
model	O
over	O
a	O
high	O
number	O
of	O
languages	O
can	O
lead	O
to	O
sub	O
-	O
optimal	O
results	O
,	O
especially	O
if	O
the	O
training	O
languages	O
are	O
typologically	O
diverse	O
.	O
Often	O
,	O
multilingual	O
neural	O
models	O
have	O
been	O
found	O
to	O
outperform	O
their	O
monolingual	O
counterparts	O
on	O
low	O
-	O
and	O
zero	O
-	O
resource	O
languages	O
due	O
to	O
positive	O
transfer	O
effects	O
,	O
but	O
underperform	O
for	O
high	O
-	O
resource	O
languages	O
(	O
Johnson	O
et	O
al	O
,	O
2017	O
;	O
Arivazhagan	O
et	O
al	O
,	O
2019	O
;	O
Conneau	O
et	O
al	O
,	O
2020	O
)	O
,	O
a	O
problem	O
also	O
known	O
as	O
"	O
the	O
curse	O
of	O
multilinguality	O
"	O
.	O
Generally	O
speaking	O
,	O
a	O
multilingual	O
model	O
without	O
language	O
-	O
specific	O
supervision	O
is	O
likely	O
to	O
suffer	O
from	O
over	O
-	O
generalization	O
and	O
perform	O
poorly	O
on	O
high	O
-	O
resource	O
languages	O
due	O
to	O
limited	O
capacity	O
compared	O
to	O
the	O
monolingual	O
baselines	O
,	O
as	O
verified	O
by	O
our	O
experiments	O
on	O
parsing	O
.	O
In	O
this	O
paper	O
,	O
we	O
strike	O
a	O
good	O
balance	O
between	O
maximum	O
sharing	O
and	O
language	O
-	O
specific	O
capacity	O
in	O
multilingual	O
dependency	B-TaskName
parsing	I-TaskName
.	O
Inspired	B-DatasetName
by	O
recently	O
introduced	O
parameter	O
sharing	O
techniques	O
(	O
Platanios	O
et	O
al	O
,	O
2018	O
;	O
Houlsby	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
propose	O
a	O
new	O
multilingual	O
parser	O
,	O
UDapter	O
,	O
that	O
learns	O
to	O
modify	O
its	O
language	O
-	O
specific	O
parameters	O
including	O
the	O
adapter	O
modules	O
,	O
as	O
a	O
function	O
of	O
language	O
embeddings	O
.	O
This	O
allows	O
the	O
model	O
to	O
share	O
parameters	O
across	O
languages	O
,	O
ensuring	O
generalization	O
and	O
transfer	O
ability	O
,	O
but	O
also	O
enables	O
language	O
-	O
specific	O
parameterization	O
in	O
a	O
single	O
multilingual	O
model	O
.	O
Furthermore	O
,	O
we	O
propose	O
not	O
to	O
learn	O
language	O
embeddings	O
from	O
scratch	O
,	O
but	O
to	O
leverage	O
a	O
mix	O
of	O
linguistically	O
curated	O
and	O
predicted	O
typological	O
features	O
as	O
obtained	O
from	O
the	O
URIEL	O
language	O
typology	O
database	O
which	O
supports	O
3718	O
languages	O
including	O
all	O
languages	O
represented	O
in	O
UD	B-DatasetName
.	O
While	O
the	O
importance	O
of	O
typological	O
features	O
for	O
cross	O
-	O
lingual	O
parsing	O
is	O
known	O
for	O
both	O
non	O
-	O
neural	O
(	O
Naseem	O
et	O
al	O
,	O
2012	O
;	O
Zhang	O
and	O
Barzilay	O
,	O
2015	O
)	O
and	O
neural	O
approaches	O
(	O
Ammar	O
et	O
al	O
,	O
2016	O
;	O
Scholivet	O
et	O
al	O
,	O
2019	O
)	O
,	O
we	O
are	O
the	O
first	O
to	O
use	O
them	O
effectively	O
as	O
direct	O
input	O
to	O
a	O
neural	O
parser	O
,	O
without	O
manual	O
selection	O
,	O
over	O
a	O
large	O
number	O
of	O
languages	O
in	O
the	O
context	O
of	O
zero	O
-	O
shot	O
parsing	O
where	O
gold	O
POS	O
labels	O
are	O
not	O
given	O
at	O
test	O
time	O
.	O
In	O
our	O
model	O
,	O
typological	O
features	O
are	O
crucial	O
,	O
leading	O
to	O
a	O
substantial	O
LAS	O
increase	O
on	O
zero	O
-	O
shot	O
languages	O
and	O
no	O
loss	B-MetricName
on	O
high	O
-	O
resource	O
languages	O
when	O
compared	O
to	O
the	O
language	O
embeddings	O
learned	O
from	O
scratch	O
.	O
We	O
train	O
and	O
test	O
our	O
model	O
on	O
the	O
13	O
syntactically	O
diverse	O
high	O
-	O
resource	O
languages	O
that	O
were	O
used	O
by	O
Kulmizev	O
et	O
al	O
(	O
2019	O
)	O
,	O
and	O
also	O
evaluate	O
it	O
on	O
30	O
genuinely	O
low	O
-	O
resource	O
languages	O
.	O
Results	O
show	O
that	O
UDapter	O
significantly	O
outperforms	O
stateof	O
-	O
the	O
-	O
art	O
monolingual	O
(	O
Straka	O
,	O
2018	O
)	O
and	O
multilingual	O
(	O
Kondratyuk	O
and	O
Straka	O
,	O
2019	O
)	O
parsers	O
on	O
most	O
high	O
-	O
resource	O
languages	O
and	O
achieves	O
overall	O
promising	O
improvements	O
on	O
zero	O
-	O
shot	O
languages	O
.	O
Contributions	O
We	O
conduct	O
several	O
experiments	O
on	O
a	O
large	O
set	O
of	O
languages	O
and	O
perform	O
thorough	O
analyses	O
of	O
our	O
model	O
.	O
Accordingly	O
,	O
we	O
make	O
the	O
following	O
contributions	O
:	O
1	O
)	O
We	O
apply	O
the	O
idea	O
of	O
adapter	O
tuning	O
(	O
Rebuffi	O
et	O
al	O
,	O
2018	O
;	O
Houlsby	O
et	O
al	O
,	O
2019	O
)	O
to	O
the	O
task	O
of	O
universal	O
dependency	B-TaskName
parsing	I-TaskName
.	O
2	O
)	O
We	O
combine	O
adapters	O
with	O
the	O
idea	O
of	O
contextual	O
parameter	O
generation	O
(	O
Platanios	O
et	O
al	O
,	O
2018	O
)	O
,	O
leading	O
to	O
a	O
novel	O
language	O
adaptation	O
approach	O
with	O
state	O
-	O
of	O
-	O
the	O
art	O
UD	B-DatasetName
parsing	O
results	O
.	O
3	O
)	O
We	O
provide	O
a	O
simple	O
but	O
effective	O
method	O
for	O
conditioning	O
the	O
language	O
adaptation	O
on	O
existing	O
typological	O
language	O
features	O
,	O
which	O
we	O
show	O
is	O
crucial	O
for	O
zero	O
-	O
shot	O
performance	O
.	O

This	O
section	O
presents	O
the	O
background	O
of	O
our	O
approach	O
.	O
Multilingual	O
Neural	O
Networks	O
Early	O
models	O
in	O
multilingual	O
neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
designed	O
dedicated	O
architectures	O
(	O
Dong	O
et	O
al	O
,	O
2015	O
;	O
Firat	O
et	O
al	O
,	O
2016	O
)	O
whilst	O
subsequent	O
models	O
,	O
from	O
Johnson	O
et	O
al	O
(	O
2017	O
)	O
onward	O
,	O
added	O
a	O
simple	O
language	O
identifier	O
to	O
the	O
models	O
with	O
the	O
same	O
architecture	O
as	O
their	O
monolingual	O
counterparts	O
.	O
More	O
recently	O
,	O
multilingual	O
NMT	O
models	O
have	O
focused	O
on	O
maximizing	O
transfer	O
accuracy	B-MetricName
for	O
low	O
-	O
resource	O
language	O
pairs	O
,	O
while	O
preserving	O
high	O
-	O
resource	O
language	O
accuracy	B-MetricName
(	O
Platanios	O
et	O
al	O
,	O
2018	O
;	O
Neubig	O
and	O
Hu	O
,	O
2018	O
;	O
Aharoni	O
et	O
al	O
,	O
2019	O
;	O
Arivazhagan	O
et	O
al	O
,	O
2019	O
)	O
,	O
known	O
as	O
the	O
(	O
positive	O
)	O
transfer	O
-	O
(	O
negative	O
)	O
interference	O
trade	O
-	O
off	O
.	O
Another	O
line	O
of	O
work	O
builds	O
massively	O
multilingual	O
pre	O
-	O
trained	O
language	O
mod	O
-	O
els	O
to	O
produce	O
contextual	O
representation	O
to	O
be	O
used	O
in	O
downstream	O
tasks	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Conneau	O
et	O
al	O
,	O
2020	O
)	O
.	O
As	O
the	O
leading	O
model	O
,	O
multilingual	O
BERT	B-MethodName
(	O
mBERT	B-MethodName
)	O
2	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
which	O
is	O
a	O
deep	O
self	B-MethodName
-	I-MethodName
attention	I-MethodName
network	I-MethodName
,	O
was	O
trained	O
without	O
language	O
-	O
specific	O
signals	O
on	O
the	O
104	O
languages	O
with	O
the	O
largest	O
Wikipedias	O
.	O
It	O
uses	O
a	O
shared	O
vocabulary	O
of	O
110	O
K	O
WordPieces	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
has	O
been	O
shown	O
to	O
facilitate	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
in	O
several	O
applications	O
(	O
Pires	O
et	O
al	O
,	O
2019	O
;	O
Wu	O
and	O
Dredze	O
,	O
2019	O
)	O
.	O
Concurrently	O
to	O
our	O
work	O
,	O
Pfeiffer	O
et	O
al	O
(	O
2020	O
)	O
have	O
proposed	O
to	O
combine	O
language	O
and	O
task	O
adapters	O
,	O
small	O
bottleneck	O
layers	O
(	O
Rebuffi	O
et	O
al	O
,	O
2018	O
;	O
Houlsby	O
et	O
al	O
,	O
2019	O
)	O
,	O
to	O
address	O
the	O
capacity	O
issue	O
which	O
limits	O
multilingual	O
pre	O
-	O
trained	O
models	O
for	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
.	O
Cross	O
-	O
Lingual	O
Dependency	B-TaskName
Parsing	I-TaskName
The	O
availability	O
of	O
consistent	O
dependency	O
treebanks	O
in	O
many	O
languages	O
Nivre	O
et	O
al	O
,	O
2018	O
)	O
has	O
provided	O
an	O
opportunity	O
for	O
the	O
study	O
of	O
cross	O
-	O
lingual	O
parsing	O
.	O
Early	O
studies	O
trained	O
a	O
delexicalized	O
parser	O
(	O
Zeman	O
and	O
Resnik	O
,	O
2008	O
;	O
on	O
one	O
or	O
more	O
source	O
languages	O
by	O
using	O
either	O
gold	O
or	O
predicted	O
POS	O
labels	O
(	O
Tiedemann	O
,	O
2015	O
)	O
and	O
applied	O
it	O
to	O
target	O
languages	O
.	O
Building	O
on	O
this	O
,	O
later	O
work	O
used	O
additional	O
features	O
such	O
as	O
typological	O
language	O
properties	O
(	O
Naseem	O
et	O
al	O
,	O
2012	O
)	O
,	O
syntactic	O
embeddings	O
(	O
Duong	O
et	O
al	O
,	O
2015	O
)	O
,	O
and	O
cross	O
-	O
lingual	O
word	O
clusters	O
(	O
Täckström	O
et	O
al	O
,	O
2012	O
)	O
.	O
Among	O
lexicalized	O
approaches	O
,	O
Vilares	O
et	O
al	O
(	O
2016	O
)	O
learns	O
a	O
bilingual	O
parser	O
on	O
a	O
corpora	O
obtained	O
by	O
merging	O
harmonized	O
treebanks	O
.	O
Ammar	O
et	O
al	O
(	O
2016	O
)	O
trains	O
a	O
multilingual	O
parser	O
using	O
multilingual	O
word	B-TaskName
embeddings	I-TaskName
,	O
token	O
-	O
level	O
language	O
information	O
,	O
language	O
typology	O
features	O
and	O
fine	O
-	O
grained	O
POS	O
tags	O
.	O
More	O
recently	O
,	O
based	O
on	O
mBERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
zero	O
-	O
shot	O
transfer	O
in	O
dependency	B-TaskName
parsing	I-TaskName
was	O
investigated	O
(	O
Wu	O
and	O
Dredze	O
,	O
2019	O
;	O
Tran	O
and	O
Bisazza	O
,	O
2019	O
)	O
.	O
Finally	O
Kondratyuk	O
and	O
Straka	O
(	O
2019	O
)	O
trained	O
a	O
multilingual	O
parser	O
on	O
the	O
concatenation	O
of	O
all	O
available	O
UD	B-DatasetName
treebanks	O
.	O
Language	O
Embeddings	O
and	O
Typology	O
Conditioning	O
a	O
multilingual	O
model	O
on	O
the	O
input	O
language	O
is	O
studied	O
in	O
NMT	O
(	O
Ha	O
et	O
al	O
,	O
2016	O
;	O
Johnson	O
et	O
al	O
,	O
2017	O
)	O
,	O
syntactic	O
parsing	O
(	O
Ammar	O
et	O
al	O
,	O
2016	O
)	O
and	O
language	O
modeling	O
(	O
Östling	O
and	O
Tiedemann	O
,	O
2017	O
)	O
.	O
The	O
goal	O
is	O
to	O
embed	O
language	O
information	O
in	O
real	O
-	O
valued	O
vectors	O
in	O
order	O
to	O
enrich	O
internal	O
representations	O
with	O
input	O
language	O
for	O
multilingual	O
models	O
.	O
In	O
dependency	B-TaskName
parsing	I-TaskName
,	O
several	O
previous	O
studies	O
(	O
Naseem	O
et	O
al	O
,	O
2012	O
;	O
Zhang	O
and	O
Barzilay	O
,	O
2015	O
;	O
Ammar	O
et	O
al	O
,	O
2016	O
;	O
Scholivet	O
et	O
al	O
,	O
2019	O
)	O
have	O
suggested	O
that	O
typological	O
features	O
are	O
useful	O
for	O
the	O
selective	O
sharing	O
of	O
transfer	O
information	O
.	O
Results	O
,	O
however	O
,	O
are	O
mixed	O
and	O
often	O
limited	O
to	O
a	O
handful	O
of	O
manually	O
selected	O
features	O
(	O
Fisch	O
et	O
al	O
,	O
2019	O
;	O
Ponti	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
the	O
most	O
similar	O
work	O
to	O
ours	O
,	O
Ammar	O
et	O
al	O
(	O
2016	O
)	O
uses	O
typological	O
features	O
to	O
learn	O
language	O
embeddings	O
as	O
part	O
of	O
training	O
,	O
by	O
augmenting	O
each	O
input	O
token	O
and	O
parsing	O
action	O
representation	O
.	O
Unfortunately	O
though	O
,	O
this	O
technique	O
is	O
found	O
to	O
underperform	O
the	O
simple	O
use	O
of	O
randomly	O
initialized	O
language	O
embeddings	O
(	O
'	O
language	O
IDs	O
'	O
)	O
.	O
Authors	O
also	O
reported	O
that	O
language	O
embeddings	O
hurt	O
the	O
performance	O
of	O
the	O
parser	O
in	O
zero	O
-	O
shot	O
experiments	O
(	O
Ammar	O
et	O
al	O
,	O
2016	O
,	O
footnote	O
30	O
)	O
.	O
Our	O
work	O
instead	O
demonstrates	O
that	O
typological	O
features	O
can	O
be	O
very	O
effective	O
if	O
used	O
with	O
the	O
right	O
adaptation	O
strategy	O
in	O
both	O
supervised	O
and	O
zero	O
-	O
shot	O
settings	O
.	O
Finally	O
,	O
Lin	O
et	O
al	O
(	O
2019	O
)	O
use	O
typological	O
features	O
,	O
along	O
with	O
properties	O
of	O
the	O
training	O
data	O
,	O
to	O
choose	O
optimal	O
transfer	O
languages	O
for	O
various	O
tasks	O
,	O
including	O
UD	B-DatasetName
parsing	O
,	O
in	O
a	O
hard	O
manner	O
.	O
By	O
contrast	O
,	O
we	O
focus	O
on	O
a	O
soft	O
parameter	O
sharing	O
approach	O
to	O
maximize	O
generalizations	O
within	O
a	O
single	O
universal	O
model	O
.	O

To	O
obtain	O
contextualized	O
word	O
representations	O
,	O
UDapter	O
uses	O
mBERT	B-MethodName
.	O
For	O
a	O
token	O
i	O
in	O
sentence	O
S	O
,	O
BERT	B-MethodName
builds	O
an	O
input	O
representation	O
w	O
i	O
composed	O
by	O
summing	O
a	O
WordPiece	B-MethodName
embedding	O
x	O
i	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
and	O
a	O
position	O
embedding	O
f	O
i	O
.	O
Each	O
w	O
i	O
S	O
is	O
then	O
passed	O
to	O
a	O
stacked	O
self	O
-	O
attention	B-HyperparameterName
layers	I-HyperparameterName
(	O
SA	O
)	O
to	O
generate	O
the	O
final	O
encoder	O
representation	O
r	O
i	O
:	O
w	O
i	O
=	O
x	O
i	O
+	O
f	O
i	O
(	O
4	O
)	O
r	O
i	O
=	O
SA	O
(	O
w	O
i	O
;	O
Θ	B-HyperparameterName
(	O
ad	O
)	O
)	O
(	O
5	O
)	O
where	O
Θ	B-HyperparameterName
(	O
ad	O
)	O
denotes	O
the	O
adapter	O
modules	O
.	O
During	O
training	O
,	O
instead	O
of	O
fine	O
-	O
tuning	O
the	O
whole	O
encoder	O
network	O
together	O
with	O
the	O
task	O
-	O
specific	O
top	O
layer	O
,	O
we	O
use	O
adapter	O
modules	O
(	O
Rebuffi	O
et	O
al	O
,	O
2018	O
;	O
Stickland	O
and	O
Murray	O
,	O
2019	O
;	O
Houlsby	O
et	O
al	O
,	O
2019	O
)	O
,	O
or	O
simply	O
adapters	O
,	O
to	O
capture	O
both	O
task	O
-	O
specific	O
and	O
language	O
-	O
specific	O
information	O
.	O
Adapters	O
are	O
small	O
modules	O
added	O
between	O
layers	O
of	O
a	O
pre	O
-	O
trained	O
network	O
.	O
In	O
adapter	O
tuning	O
,	O
the	O
weights	O
of	O
the	O
original	O
network	O
are	O
kept	O
frozen	O
,	O
whilst	O
the	O
adapters	O
are	O
trained	O
for	O
a	O
downstream	O
task	O
.	O
Tuning	O
with	O
adapters	O
was	O
mainly	O
suggested	O
for	O
parameter	O
efficiency	O
but	O
they	O
also	O
act	O
as	O
an	O
information	O
module	O
for	O
the	O
task	O
or	O
the	O
language	O
to	O
be	O
adapted	O
(	O
Pfeiffer	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
way	O
,	O
the	O
original	O
network	O
serves	O
as	O
a	O
memory	O
for	O
the	O
language	O
(	O
s	O
)	O
.	O
In	O
UDapter	O
,	O
following	O
Houlsby	O
et	O
al	O
(	O
2019	O
)	O
,	O
two	O
bottleneck	O
adapters	O
with	O
two	O
feedforward	O
projections	O
and	O
a	O
GELU	B-MethodName
nonlinearity	O
(	O
Hendrycks	O
and	O
Gimpel	O
,	O
2016	O
)	O
are	O
inserted	O
into	O
each	O
transformer	O
layer	O
as	O
shown	O
in	O
Figure	O
1	O
.	O
We	O
apply	O
adapter	O
tuning	O
for	O
two	O
reasons	O
:	O
1	O
)	O
Each	O
adapter	O
module	O
consists	O
of	O
only	O
few	O
parameters	O
and	O
allows	O
to	O
use	O
contextual	O
parameter	O
generation	O
(	O
CPG	O
;	O
see	O
3.3	O
)	O
with	O
a	O
reasonable	O
number	O
of	O
trainable	O
parameters	O
.	O
3	O
2	O
)	O
Adapters	O
enable	O
taskspecific	O
as	O
well	O
as	O
language	O
-	O
specific	O
adaptation	O
via	O
CPG	O
since	O
it	O
keeps	O
backbone	O
multilingual	O
representations	O
as	O
memory	O
for	O
all	O
languages	O
in	O
pre	O
-	O
training	O
,	O
which	O
is	O
important	O
for	O
multilingual	O
transfer	O
.	O
F	O
1	O
1	O
0	B-DatasetName
0	B-DatasetName
1	O
0	B-DatasetName
0	B-DatasetName
1	O
1	O
0	B-DatasetName
1	O
0	B-DatasetName
1	O
1	O
Biaffine	O
Attention	O
(	O
for	O

To	O
control	O
the	O
amount	O
of	O
sharing	O
across	O
languages	O
,	O
we	O
generate	O
trainable	O
parameters	O
of	O
the	O
model	O
using	O
a	O
contextual	O
parameter	O
generator	O
(	O
CPG	O
)	O
function	O
inspired	O
by	O
Platanios	O
et	O
al	O
(	O
2018	O
)	O
.	O
CPG	O
enables	O
UDapter	O
to	O
retain	O
high	O
multilingual	O
quality	O
without	O
losing	O
performance	O
on	O
a	O
single	O
language	O
,	O
during	O
multi	O
-	O
language	O
training	O
.	O
We	O
define	O
CPG	O
as	O
a	O
function	O
of	O
language	O
embeddings	O
.	O
Since	O
we	O
only	O
train	O
adapters	O
and	O
the	O
biaffine	O
attention	O
(	O
i.e.	O
adapter	O
tuning	O
)	O
,	O
the	O
parameter	O
generator	O
is	O
formalized	O
as	O
{	O
θ	B-HyperparameterName
(	O
ad	O
)	O
,	O
θ	B-HyperparameterName
(	O
bf	O
)	O
}	O
g	O
(	O
m	O
)	O
(	O
l	O
e	O
)	O
where	O
g	O
(	O
m	O
)	O
denotes	O
the	O
parameter	O
generator	O
with	O
language	O
embedding	O
l	O
e	O
,	O
and	O
θ	B-HyperparameterName
(	O
ad	O
)	O
and	O
θ	B-HyperparameterName
(	O
bf	O
)	O
denote	O
the	O
parameters	O
of	O
adapters	O
and	O
biaffine	O
attention	O
respectively	O
.	O
We	O
implement	O
CPG	O
as	O
a	O
simple	O
linear	O
transform	O
of	O
a	O
language	O
embedding	O
,	O
similar	O
to	O
Platanios	O
et	O
al	O
(	O
2018	O
)	O
,	O
so	O
that	O
weights	O
of	O
adapters	O
in	O
the	O
encoder	O
and	O
biaffine	O
attention	O
are	O
generated	O
by	O
the	O
dot	O
product	O
of	O
language	O
embeddings	O
:	O
g	O
(	O
m	O
)	O
(	O
l	O
e	O
)	O
=	O
(	O
W	O
(	O
ad	O
)	O
,	O
W	O
(	O
bf	O
)	O
)	O
l	O
e	O
(	O
6	O
)	O
3	O
Due	O
to	O
CPG	O
,	O
the	O
number	O
of	O
adapter	O
parameters	O
is	O
multiplied	O
by	O
language	O
embedding	O
size	O
,	O
resulting	O
in	O
a	O
larger	O
model	O
compared	O
to	O
the	O
baseline	O
(	O
more	O
details	O
in	O
Appendix	O
A.1	O
)	O
.	O
where	O
l	O
e	O
R	O
M	O
,	O
W	O
(	O
ad	O
)	O
R	O
P	O
(	O
ad	O
)	O
×M	O
,	O
W	O
(	O
bf	O
)	O
R	O
P	O
(	O
bf	O
)	O
×M	O
,	O
M	O
is	O
the	O
language	O
embedding	O
size	O
,	O
P	O
(	O
ad	O
)	O
and	O
P	O
(	O
bf	O
)	O
are	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
for	O
adapters	O
and	O
biaffine	O
attention	O
respectively	O
.	O
4	O
An	O
important	O
advantage	O
of	O
CPG	O
is	O
the	O
easy	O
integration	O
of	O
existing	O
task	O
or	O
language	O
features	O
.	O

Soft	O
sharing	O
via	O
CPG	O
enables	O
our	O
model	O
to	O
modify	O
its	O
parsing	O
decisions	O
depending	O
on	O
a	O
language	O
embedding	O
.	O
While	O
this	O
allows	O
UDapter	O
to	O
perform	O
well	O
on	O
the	O
languages	O
in	O
training	O
,	O
even	O
if	O
they	O
are	O
typologically	O
diverse	O
,	O
information	O
sharing	O
is	O
still	O
a	O
problem	O
for	O
languages	O
not	O
seen	O
during	O
training	O
(	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
)	O
as	O
a	O
language	O
embedding	O
is	O
not	O
available	O
.	O
Inspired	B-DatasetName
by	O
Naseem	O
et	O
al	O
(	O
2012	O
)	O
and	O
Ammar	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
address	O
this	O
problem	O
by	O
defining	O
language	O
embeddings	O
as	O
a	O
function	O
of	O
a	O
large	O
set	O
of	O
language	O
typological	O
features	O
,	O
including	O
syntactic	O
and	O
phonological	O
features	O
.	O
We	O
use	O
a	O
multi	O
-	O
layer	O
perceptron	O
MLP	B-DatasetName
(	O
lang	O
)	O
with	O
two	O
feedforward	O
layers	O
and	O
a	O
ReLU	B-MethodName
nonlinear	O
activation	O
to	O
compute	O
a	O
language	O
embedding	O
l	O
e	O
:	O
l	O
e	O
=	O
MLP	B-DatasetName
(	O
lang	O
)	O
(	O
l	O
t	O
)	O
(	O
7	O
)	O
where	O
l	O
t	O
is	O
a	O
typological	O
feature	O
vector	O
for	O
a	O
language	O
consisting	O
of	O
all	O
103	O
syntactic	O
,	O
28	O
phonological	O
and	O
158	O
phonetic	O
inventory	O
features	O
from	O
the	O
URIEL	O
language	O
typology	O
database	O
(	O
Lewis	O
et	O
al	O
,	O
2015	O
)	O
and	O
Glottolog	O
(	O
Hammarström	O
et	O
al	O
,	O
2020	O
)	O
.	O
As	O
many	O
feature	O
values	O
are	O
not	O
available	O
for	O
each	O
language	O
,	O
we	O
use	O
the	O
values	O
predicted	O
by	O
using	O
a	O
k	B-MethodName
-	I-MethodName
nearest	I-MethodName
neighbors	I-MethodName
approach	O
based	O
on	O
average	O
of	O
genetic	O
,	O
geographical	O
and	O
feature	O
distances	O
between	O
languages	O
.	O
For	O
the	O
encoder	O
,	O
we	O
use	O
BERT	B-MethodName
-	O
multilingualcased	O
together	O
with	O
its	O
WordPiece	B-MethodName
tokenizer	O
.	O
Since	O
dependency	O
annotations	O
are	O
between	O
words	O
,	O
we	O
pass	O
the	O
BERT	B-MethodName
output	O
corresponding	O
to	O
the	O
first	O
wordpiece	B-MethodName
per	O
word	O
to	O
the	O
biaffine	O
parser	O
.	O
We	O
apply	O
the	O
same	O
hyper	O
-	O
parameter	O
settings	O
as	O
Kondratyuk	O
and	O
Straka	O
(	O
2019	O
)	O
.	O
Additionally	O
,	O
we	O
use	O
256	O
and	O
32	O
for	O
adapter	O
size	O
and	O
language	O
embedding	O
size	O
respectively	O
.	O
In	O
our	O
approach	O
,	O
pre	O
-	O
trained	O
BERT	B-MethodName
weights	O
are	O
frozen	O
,	O
and	O
only	O
adapters	O
and	O
biaffine	O
attention	O
are	O
trained	O
,	O
thus	O
we	O
use	O
the	O
same	O
learning	B-HyperparameterName
rate	I-HyperparameterName
for	O
the	O
whole	O
network	O
by	O
applying	O
an	O
inverse	O
square	O
root	O
learning	B-HyperparameterName
rate	I-HyperparameterName
decay	O
with	O
linear	B-MethodName
warmup	I-MethodName
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
.	O
Appendix	O
A.1	O
gives	O
the	O
hyper	O
-	O
parameter	O
details	O
.	O
Baselines	O
We	O
compare	O
UDapter	O
to	O
the	O
current	O
state	O
of	O
the	O
art	O
in	O
UD	B-DatasetName
parsing	O
:	O
[	O
1	O
]	O
UUparser+BERT	O
(	O
Kulmizev	O
et	O
al	O
,	O
2019	O
)	O
,	O
a	O
graph	O
-	O
based	O
BLSTM	O
parser	O
(	O
de	O
Lhoneux	O
et	O
al	O
,	O
2017	O
;	O
Smith	O
et	O
al	O
,	O
2018	O
)	O
using	O
mBERT	B-MethodName
embeddings	O
as	O
additional	O
features	O
.	O
[	O
2	O
]	O
UDpipe	O
(	O
Straka	O
,	O
2018	O
)	O
,	O
a	O
monolingually	O
trained	O
multi	O
-	O
task	O
parser	O
that	O
uses	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
and	O
character	O
representations	O
.	O
[	O
3	O
]	O
UDify	O
(	O
Kondratyuk	O
and	O
Straka	O
,	O
2019	O
)	O
,	O
the	O
mBERTbased	O
multi	O
-	O
task	O
UD	B-DatasetName
parser	O
on	O
which	O
our	O
UDapter	O
is	O
based	O
,	O
but	O
originally	O
trained	O
on	O
all	O
language	O
treebanks	O
from	O
UD	B-DatasetName
.	O
UDPipe	O
scores	O
are	O
taken	O
from	O
Kondratyuk	O
and	O
Straka	O
(	O
2019	O
)	O
.	O
To	O
enable	O
a	O
direct	O
comparison	O
,	O
we	O
also	O
re	O
-	O
train	O
UDify	O
on	O
our	O
set	O
of	O
13	O
high	O
-	O
resource	O
languages	O
both	O
monolingually	O
(	O
one	O
treebank	O
at	O
a	O
time	O
;	O
monoudify	O
)	O
and	O
multilingually	O
(	O
on	O
the	O
concatenation	O
of	O
languages	O
;	O
multi	O
-	O
udify	O
)	O
.	O
Finally	O
,	O
we	O
evaluate	O
two	O
variants	O
of	O
our	O
model	O
:	O
1	O
)	O
Adapter	B-MethodName
-	O
only	O
has	O
only	O
task	O
-	O
specific	O
adapter	O
modules	O
and	O
no	O
languagespecific	O
adaptation	O
,	O
i.e.	O
no	O
contextual	O
parameter	O
generator	O
;	O
and	O
2	O
)	O
UDapter	O
-	O
proxy	O
is	O
trained	O
without	O
typology	O
features	O
:	O
a	O
separate	O
language	O
embedding	O
is	O
learnt	O
from	O
scratch	O
for	O
each	O
in	O
-	O
training	O
language	O
,	O
and	O
for	O
low	O
-	O
resource	O
languages	O
we	O
use	O
one	O
from	O
the	O
same	O
language	O
family	O
,	O
if	O
available	O
,	O
as	O
proxy	O
representation	O
.	O
Importantly	O
,	O
all	O
baselines	O
are	O
either	O
trained	O
for	O
a	O
single	O
language	O
,	O
or	O
multilingually	O
without	O
any	O
language	O
-	O
specific	O
adaptation	O
.	O
By	O
comparing	O
UDapter	O
to	O
these	O
parsers	O
,	O
we	O
highlight	O
its	O
unique	O
character	O
that	O
enables	O
language	O
specific	O
parameterization	O
by	O
typological	O
features	O
within	O
a	O
multilingual	O
framework	O
for	O
both	O
supervised	O
and	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
setup	O
.	O

We	O
start	O
by	O
analyzing	O
the	O
projection	O
weights	O
assigned	O
to	O
different	O
typological	O
features	O
by	O
the	O
first	O
layer	O
of	O
the	O
language	O
embedding	O
network	O
(	O
see	O
eq	O
.	O
7	O
)	O
.	O
Figure	O
4b	O
shows	O
the	O
averages	O
of	O
normalized	O
syntactic	O
,	O
phonological	O
and	O
phonetic	O
inventory	O
feature	O
weights	O
.	O
Although	O
dependency	B-TaskName
parsing	I-TaskName
is	O
a	O
syntactic	O
task	O
,	O
the	O
network	O
does	O
not	O
only	O
utilize	O
syntactic	O
features	O
,	O
as	O
also	O
observed	O
by	O
Lin	O
et	O
al	O
(	O
2019	O
)	O
,	O
but	O
exploits	O
all	O
available	O
typological	O
features	O
to	O
learn	O
its	O
representations	O
.	O
Next	O
,	O
we	O
plot	O
the	O
language	O
representations	O
learned	O
in	O
UDapter	O
by	O
using	O
t	O
-	O
SNE	O
(	O
van	O
der	O
Maaten	O
and	O
Hinton	O
,	O
2008	O
)	O
,	O
which	O
is	O
similar	O
to	O
the	O
analysis	O
carried	O
out	O
by	O
Ponti	O
et	O
al	O
(	O
2019	O
,	O
figure	O
8	O
)	O
using	O
the	O
language	O
vectors	O
learned	O
by	O
Malaviya	O
et	O
al	O
(	O
2017	O
)	O
.	O
Figure	O
5	O
illustrates	O
2D	O
vector	O
spaces	O
generated	O
for	O
the	O
typological	O
feature	O
vectors	O
l	O
t	O
(	O
A	O
)	O
and	O
the	O
language	O
embeddings	O
l	O
e	O
learned	O
by	O
UDapter	O
with	O
or	O
without	O
typological	O
features	O
(	O
B	O
and	O
C	O
respectively	O
)	O
.	O
The	O
benefits	O
of	O
using	O
typological	O
features	O
can	O
be	O
understood	O
by	O
comparing	O
A	O
and	O
B	O
:	O
During	O
training	O
,	O
UDapter	O
learns	O
to	O
project	O
URIEL	O
features	O
to	O
language	O
embeddings	O
in	O
a	O
way	O
that	O
is	O
optimal	O
for	O
in	O
-	O
training	O
language	O
parsing	O
quality	O
.	O
This	O
leads	O
to	O
a	O
different	O
placement	O
of	O
the	O
high	O
-	O
resource	O
languages	O
(	O
red	O
points	O
)	O
in	O
the	O
space	O
,	O
where	O
many	O
linguistic	O
similarities	O
are	O
preserved	O
(	O
e.g.	O
Hebrew	O
and	O
Arabic	O
;	O
European	O
languages	O
except	O
Basque	O
)	O
but	O
others	O
are	O
overruled	O
(	O
Japanese	O
drifting	O
away	O
from	O
Korean	O
)	O
.	O
Looking	O
at	O
the	O
low	O
-	O
resource	O
languages	O
(	O
blue	O
points	O
)	O
we	O
find	O
that	O
typologically	O
similar	O
languages	O
tend	O
to	O
have	O
similar	O
embeddings	O
to	O
the	O
closest	O
highresource	O
language	O
in	O
both	O
A	O
and	O
B.	O
In	O
fact	O
,	O
most	O
groupings	O
of	O
genetically	O
related	O
languages	O
,	O
such	O
as	O
the	O
Indian	O
languages	O
(	O
hi	O
-	O
cluster	O
)	O
or	O
the	O
Uralic	O
ones	O
(	O
fi	O
-	O
cluster	O
)	O
are	O
largely	O
preserved	O
across	O
these	O
two	O
spaces	O
.	O
Comparing	O
B	O
and	O
C	O
where	O
language	O
embeddings	O
are	O
learned	O
from	O
scratch	O
,	O
the	O
absence	O
of	O
typological	O
features	O
leads	O
to	O
a	O
seemingly	O
random	O
space	O
with	O
no	O
linguistic	O
similarities	O
(	O
e.g.	O
Arabic	O
far	O
away	O
from	O
Hebrew	O
,	O
Korean	O
closer	O
to	O
English	O
than	O
to	O
Japanese	O
,	O
etc	O
.	O
)	O
and	O
,	O
therefore	O
,	O
no	O
principled	O
way	O
to	O
represent	O
additional	O
languages	O
.	O
Taken	O
together	O
with	O
the	O
parsing	O
results	O
of	O
4.1	O
,	O
these	O
plots	O
suggest	O
that	O
UDapter	O
embeddings	O
strike	O
a	O
good	O
balance	O
between	O
a	O
linguistically	O
motivated	O
representation	O
space	O
and	O
one	O
solely	O
optimized	O
for	O
in	O
-	O
training	O
language	O
accuracy	B-MetricName
.	O

In	O
section	O
4.1	O
we	O
observed	O
that	O
adapter	O
tuning	O
alone	O
(	O
that	O
is	O
,	O
without	O
CPG	O
)	O
improved	O
the	O
multilingual	O
baseline	O
in	O
the	O
high	O
-	O
resource	O
languages	O
,	O
but	O
worsened	O
it	O
considerably	O
in	O
the	O
zero	O
-	O
shot	O
setup	O
.	O
By	O
contrast	O
,	O
the	O
addition	O
of	O
CPG	O
with	O
typological	O
features	O
led	O
to	O
the	O
best	O
results	O
over	O
all	O
languages	O
.	O
But	O
could	O
we	O
have	O
obtained	O
similar	O
results	O
by	O
simply	O
increasing	O
the	O
adapter	O
size	O
?	O
For	O
instance	O
,	O
in	O
multilingual	O
MT	O
,	O
increasing	O
overall	O
model	O
capacity	O
of	O
an	O
already	O
very	O
large	O
and	O
deep	O
architecture	O
can	O
be	O
a	O
powerful	O
alternative	O
to	O
more	O
sophisticated	O
parameter	O
sharing	O
approaches	O
(	O
Arivazhagan	O
et	O
al	O
,	O
2019	O
)	O
.	O
To	O
answer	O
this	O
question	O
we	O
train	O
another	O
adapteronly	O
model	O
with	O
doubled	O
size	O
(	O
2048	B-DatasetName
instead	O
of	O
the	O
1024	O
used	O
in	O
the	O
main	O
experiments	O
)	O
.	O
As	O
seen	O
in	O
3a	O
,	O
increase	O
in	O
model	O
size	O
brings	O
a	O
slight	O
gain	O
to	O
the	O
high	O
-	O
resource	O
languages	O
,	O
but	O
actually	O
leads	O
to	O
a	O
small	O
loss	B-MetricName
in	O
the	O
zero	O
-	O
shot	O
setup	O
.	O
This	O
shows	O
that	O
adapters	O
enlarge	O
the	O
per	O
-	O
language	O
capacity	O
for	O
in	O
-	O
training	O
languages	O
,	O
but	O
at	O
the	O
same	O
time	O
they	O
hurt	O
generalization	O
and	O
zero	O
-	O
shot	O
transfer	O
.	O
By	O
contrast	O
,	O
UDapter	O
including	O
CPG	O
which	O
increases	O
the	O
model	O
size	O
by	O
language	O
embeddings	O
(	O
see	O
Appendix	O
A.1	O
for	O
details	O
)	O
,	O
outperforms	O
both	O
adapter	O
-	O
only	O
models	O
,	O
confirming	O
once	O
more	O
the	O
importance	O
of	O
this	O
component	O
.	O
For	O
our	O
last	O
analysis	O
(	O
Fig	O
.	O
3b	O
)	O
,	O
we	O
study	O
soft	O
parameter	O
sharing	O
via	O
CPG	O
on	O
different	O
portions	O
of	O
the	O
network	O
,	O
namely	O
:	O
only	O
on	O
the	O
adapter	O
modules	O
'	O
cpg	O
(	O
adapters	O
)	O
'	O
versus	O
on	O
both	O
adapters	O
and	O
biaffine	O
attention	O
'	O
cpg	O
(	O
adap.+biaf	O
.	O
)	O
'	O
corresponding	O
to	O
the	O
full	O
UDapter	O
.	O
Results	O
show	O
that	O
most	O
of	O
the	O
gain	O
in	O
the	O
high	O
-	O
resource	O
languages	O
is	O
obtained	O
by	O
only	O
applying	O
CPG	O
on	O
the	O
multilingual	O
encoder	O
.	O
On	O
the	O
other	O
hand	O
,	O
for	O
the	O
low	O
-	O
resource	O
languages	O
,	O
typological	O
feature	O
based	O
parameter	O
sharing	O
is	O
most	O
important	O
in	O
the	O
biaffine	O
attention	O
layer	O
.	O
We	O
leave	O
further	O
investigation	O
of	O
this	O
result	O
to	O
future	O
work	O
.	O

We	O
have	O
presented	O
UDapter	O
,	O
a	O
multilingual	O
dependency	B-TaskName
parsing	I-TaskName
model	O
that	O
learns	O
to	O
adapt	O
languagespecific	O
parameters	O
on	O
the	O
basis	O
of	O
adapter	O
modules	O
(	O
Rebuffi	O
et	O
al	O
,	O
2018	O
;	O
Houlsby	O
et	O
al	O
,	O
2019	O
)	O
and	O
the	O
contextual	O
parameter	O
generation	O
(	O
CPG	O
)	O
method	O
(	O
Platanios	O
et	O
al	O
,	O
2018	O
)	O
which	O
is	O
in	O
principle	O
applicable	O
to	O
a	O
range	O
of	O
multilingual	B-TaskName
NLP	I-TaskName
tasks	O
.	O
While	O
adapters	O
provide	O
a	O
more	O
general	O
tasklevel	O
adaptation	O
,	O
CPG	O
enables	O
language	O
-	O
specific	O
adaptation	O
,	O
defined	O
as	O
a	O
function	O
of	O
language	O
embeddings	O
projected	O
from	O
linguistically	O
curated	O
typological	O
features	O
.	O
In	O
this	O
way	O
,	O
the	O
model	O
retains	O
high	O
per	O
-	O
language	O
performance	O
in	O
the	O
training	O
data	O
and	O
achieves	O
better	O
zero	O
-	O
shot	O
transfer	O
.	O
UDapter	O
,	O
trained	O
on	O
a	O
concatenation	O
of	O
typologically	O
diverse	O
languages	O
(	O
Kulmizev	O
et	O
al	O
,	O
2019	O
)	O
,	O
outperforms	O
strong	O
monolingual	O
and	O
multilingual	O
baselines	O
on	O
the	O
majority	O
of	O
both	O
high	O
-	O
resource	O
and	O
low	O
-	O
resource	O
(	O
zero	O
-	O
shot	O
)	O
languages	O
,	O
which	O
reflects	O
its	O
strong	O
balance	O
between	O
per	O
-	O
language	O
capacity	O
and	O
maximum	O
sharing	O
.	O
Finally	O
,	O
the	O
analyses	O
we	O
performed	O
on	O
the	O
underlying	O
characteristics	O
of	O
our	O
model	O
show	O
that	O
typological	O
features	O
are	O
crucial	O
for	O
zero	O
-	O
shot	O
languages	O
.	O
(	O
Kondratyuk	O
and	O
Straka	O
,	O
2019	O
)	O
for	O
all	O
low	O
-	O
resource	O
languages	O
.	O
'	O
*	O
'	O
shows	O
languages	O
not	O
present	O
in	O
mBERT	B-MethodName
training	O
data	O
.	O
Additionally	O
,	O
(	O
†	O
)	O
indicates	O
languages	O
where	O
no	O
significant	O
difference	O
between	O
UDapter	O
and	O
multi	O
-	O
udify	O
by	O
significance	O
testing	O
.	O
For	O
udapter	O
-	O
proxy	O
,	O
chosen	O
proxy	O
language	O
is	O
given	O
between	O
brackets	O
.	O
CTR	O
means	O
centroid	O
language	O
embedding	O
.	O
models	O
are	O
trained	O
separately	O
so	O
the	O
total	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
for	O
13	O
languages	O
is	O
2.5B	O
(	O
13x191	O
M	O
)	O
.	O

We	O
train	O
the	O
model	O
to	O
minimize	O
the	O
negative	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
loss	B-MetricName
using	O
back	O
-	O
propagation	O
with	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
and	O
a	O
mini	B-HyperparameterName
-	I-HyperparameterName
batch	I-HyperparameterName
size	I-HyperparameterName
of	O
16	O
.	O
To	O
monitor	O
model	O
performance	O
,	O
we	O
use	O
the	O
train	O
/	O
validation	O
split	O
provided	O
by	O
the	O
organizers	O
.	O
For	O
optimization	O
,	O
we	O
use	O
the	O
AdamW	B-MethodName
optimizer	B-HyperparameterName
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2019	O
)	O
with	O
gradient	B-MethodName
clipping	I-MethodName
(	O
Pascanu	O
et	O
al	O
,	O
2013	O
)	O
and	O
a	O
linear	O
scheduler	O
with	O
no	O
warm	O
-	O
up	O
.	O
We	O
use	O
FP	O
-	O
16	O
mixed	O
precision	O
(	O
Micikevicius	O
et	O
al	O
,	O
2018	O
)	O
training	O
(	O
and	O
inference	O
)	O
in	O
order	O
to	O
afford	O
a	O
larger	O
batch	B-HyperparameterName
size	I-HyperparameterName
and	O
increased	O
training	O
speed	O
.	O
To	O
optimize	O
GPU	O
use	O
by	O
minimizing	O
the	O
amount	O
of	O
memory	O
allocated	O
for	O
padding	O
tokens	O
,	O
we	O
use	O
dynamic	O
padding	O
and	O
length	O
-	O
based	O
batching	O
in	O
the	O
sense	O
of	O
(	O
Skinner	O
,	O
2018	O
)	O
.	O
Finally	O
,	O
we	O
employ	O
label	B-MethodName
smoothing	I-MethodName
(	O
Szegedy	O
et	O
al	O
,	O
2016	O
)	O
with	O
a	O
smoothing	O
factor	O
of	O
0.1	O
.	O

As	O
mentioned	O
in	O
Section	O
[	O
1	O
]	O
,	O
we	O
do	O
not	O
experiment	O
with	O
hyperparameter	O
tuning	O
but	O
rather	O
keep	O
the	O
default	O
parameters	O
of	O
the	O
Trainer	O
API	O
in	O
the	O
Hugging	O
Face	O
transformers	O
library	O
.	O
More	O
specifically	O
,	O
we	O
use	O
β	B-HyperparameterName
1	O
=	O
0.9	O
,	O
β	B-HyperparameterName
2	O
=	O
0.999	O
and	O
ϵ	O
=	O
10	O
−8	O
for	O
the	O
AdamW	B-MethodName
optimizer	B-HyperparameterName
parameter	O
values	O
and	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.00005	O
.	O
We	O
train	O
the	O
models	O
for	O
a	O
maximum	O
of	O
25	O
epochs	O
with	O
an	O
early	B-MethodName
stopping	I-MethodName
patience	O
level	O
set	O
to	O
0.001	O
for	O
3	O
epochs	O
.	O
Finally	O
,	O
we	O
set	O
a	O
maximum	O
sequence	O
length	O
of	O
128	O
since	O
input	O
sentences	O
are	O
generally	O
short	O
and	O
we	O
would	O
like	O
to	O
avoid	O
consuming	O
GPU	O
memory	O
for	O
padding	O
tokens	O
.	O

Table	O
[	O
3	O
]	O
summarizes	O
the	O
performance	O
of	O
our	O
approach	O
in	O
the	O
validation	O
set	O
for	O
each	O
sub	O
-	O
task	O
.	O
Note	O
that	O
in	O
this	O
set	O
of	O
experiments	O
,	O
the	O
validation	O
set	O
was	O
used	O
both	O
during	O
training	O
(	O
e.g.	O
for	O
early	B-MethodName
stopping	I-MethodName
or	O
selection	O
of	O
batch	B-HyperparameterName
size	I-HyperparameterName
)	O
as	O
well	O
as	O
for	O
the	O
reporting	O
of	O
the	O
systems	O
'	O
performance	O
.	O
Table	O
[	O
4	O
]	O
summarizes	O
the	O
performance	O
of	O
our	O
approach	O
in	O
the	O
evaluation	O
set	O
for	O
each	O
sub	O
-	O
task	O
.	O
The	O
organizers	O
chose	O
to	O
disclose	O
to	O
each	O
team	O
only	O
their	O
respective	O
score	O
along	O
with	O
the	O
average	O
score	O
of	O
all	O
submitted	O
systems	O
.	O
Our	O
system	O
performed	O
considerably	O
better	O
than	O
the	O
average	O
in	O
sub	O
-	O
task	O
1a	O
and	O
surpassed	O
the	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
0.63	O
reported	O
in	O
(	O
Magge	O
et	O
al	O
,	O
2021	O
)	O
.	O
Performance	O
was	O
considerably	O
poorer	O
for	O
sub	O
-	O
tasks	O
3	O
and	O
8	O
.	O
As	O
mentioned	O
in	O
4	O
:	O
Results	O
on	O
the	O
evaluation	O
set	O
for	O
sub	O
-	O
tasks	O
1a	O
,	O
3	O
and	O
8	O
.	O
Average	O
score	O
of	O
all	O
participating	O
systems	O
in	O
parentheses	O
.	O
Metric	O
is	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
for	O
class	O
1	O
.	O
Section	O
[	O
1	O
]	O
,	O
our	O
main	O
efforts	O
were	O
dedicated	O
to	O
subtask	O
1a	O
and	O
the	O
system	O
developed	O
did	O
not	O
transfer	O
well	O
to	O
the	O
remaining	O
sub	O
-	O
tasks	O
.	O

We	O
demonstrated	O
that	O
a	O
RoBERTa	B-MethodName
model	O
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
pretrained	O
on	O
approximately	O
128	O
million	O
tweets	O
performs	O
very	O
competitively	O
when	O
finetuned	O
on	O
English	O
tweet	O
classification	O
for	O
ADRs	O
.	O
Using	O
only	O
a	O
standard	O
finetuning	O
approach	O
,	O
our	O
model	O
obtained	O
competitive	O
results	O
,	O
outperforming	O
the	O
average	O
of	O
all	O
submissions	O
for	O
sub	O
-	O
task	O
1	O
by	O
a	O
9	O
%	O
absolute	O
difference	O
in	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
.	O
This	O
constitutes	O
yet	O
another	O
testament	O
of	O
the	O
fact	O
that	O
large	O
pre	O
-	O
trained	O
language	O
models	O
have	O
rightfully	O
become	O
the	O
default	O
approach	O
in	O
virtually	O
all	O
NLP	O
tasks	O
.	O
With	O
respect	O
to	O
potential	O
future	O
work	O
,	O
there	O
is	O
a	O
large	O
collection	O
of	O
available	O
options	O
.	O
Text	B-TaskName
classification	I-TaskName
and	O
,	O
more	O
generally	O
,	O
binary	O
classification	O
is	O
one	O
of	O
the	O
oldest	O
and	O
most	O
widely	O
researched	O
topics	O
in	O
NLP	O
.	O
Most	O
approaches	O
aiming	O
to	O
improve	O
performance	O
of	O
classification	O
models	O
can	O
be	O
broadly	O
categorized	O
into	O
three	O
groups	O
,	O
depending	O
on	O
the	O
segment	O
of	O
the	O
machine	O
learning	O
workflow	O
that	O
they	O
are	O
targeting	O
.	O
Data	B-TaskName
augmentation	I-TaskName
methods	O
typically	O
target	O
the	O
initial	O
part	O
of	O
the	O
workflow	O
,	O
the	O
data	O
,	O
aiming	O
to	O
increase	O
the	O
quantity	O
,	O
quality	O
and	O
diversity	O
of	O
the	O
training	O
dataset	O
to	O
ensure	O
that	O
model	O
performance	O
is	O
robust	O
to	O
small	O
syntactic	O
or	O
semantic	O
perturbations	O
in	O
the	O
inputs	O
.	O
Transformations	O
acting	O
directly	O
on	O
strings	O
,	O
such	O
random	O
token	O
insertions	O
or	O
deletions	O
,	O
synonym	O
/	O
antonym	O
replacements	O
and	O
related	O
techniques	O
(	O
Wei	O
and	O
Zou	O
,	O
2019	O
;	O
Karimi	O
et	O
al	O
,	O
2021	O
,	O
inter	O
alia	O
)	O
have	O
shown	O
significant	O
performance	O
improvements	O
,	O
especially	O
in	O
lowresource	O
scenarios	O
much	O
like	O
the	O
one	O
in	O
this	O
shared	O
task	O
.	O
A	O
second	O
approach	O
,	O
evidently	O
a	O
natural	O
extension	O
of	O
the	O
previous	O
technique	O
,	O
would	O
be	O
to	O
target	O
vector	O
encodings	O
of	O
the	O
tokens	O
and/or	O
documents	O
that	O
are	O
produced	O
by	O
the	O
various	O
layers	O
of	O
the	O
neural	O
networks	O
.	O
We	O
can	O
distinguish	O
two	O
different	O
approaches	O
here	O
:	O
(	O
i	O
)	O
improve	O
the	O
language	O
model	O
backbone	O
during	O
the	O
pretraining	O
phase	O
,	O
or	O
(	O
ii	O
)	O
improve	O
the	O
weights	O
of	O
the	O
language	O
model	O
backbone	O
during	O
finetuning	O
.	O
The	O
research	O
community	O
has	O
devoted	O
intensive	O
efforts	O
in	O
the	O
former	O
approach	O
,	O
as	O
can	O
be	O
observed	O
by	O
the	O
ever	O
-	O
increasing	O
list	O
of	O
transformerbased	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Joshi	O
et	O
al	O
,	O
2020	O
;	O
Kitaev	O
et	O
al	O
,	O
2020	O
;	O
Raffel	O
et	O
al	O
,	O
2020	O
;	O
Brown	O
et	O
al	O
,	O
2020	O
,	O
inter	O
multi	O
alia	O
)	O
released	O
.	O
Model	O
size	O
,	O
in	O
terms	O
of	O
total	O
number	O
of	O
trainable	O
parameters	O
,	O
has	O
been	O
consistently	O
shown	O
to	O
correlate	O
strongly	O
with	O
downstream	O
performance	O
,	O
so	O
opting	O
for	O
a	O
larger	O
pretrained	O
model	O
would	O
be	O
a	O
reasonable	O
first	O
steps	O
towards	O
more	O
transferable	O
vector	O
representations	O
(	O
and	O
hence	O
improved	O
performance	O
)	O
in	O
the	O
downstream	O
task	O
.	O
The	O
latter	O
approach	O
would	O
include	O
domain	B-TaskName
adaptation	I-TaskName
techniques	O
,	O
such	O
as	O
continued	O
self	O
-	O
supervised	O
pretraining	O
followed	O
by	O
supervised	O
finetuning	O
,	O
which	O
has	O
been	O
shown	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
to	O
consistently	O
lead	O
to	O
superior	O
results	O
relative	O
to	O
direct	O
finetuning	O
.	O
Finally	O
,	O
one	O
could	O
aim	O
to	O
improve	O
performance	O
by	O
modifying	O
aspects	O
of	O
the	O
objective	O
function	O
.	O
(	O
Hui	O
and	O
Belkin	O
,	O
2021	O
)	O
,	O
in	O
an	O
extensive	O
series	O
of	O
experiments	O
,	O
show	O
that	O
the	O
established	O
practice	O
of	O
using	O
a	O
cross	O
-	O
entropy	O
loss	B-MetricName
for	O
classification	O
is	O
not	O
well	O
-	O
founded	O
and	O
show	O
through	O
a	O
variety	O
of	O
diverse	O
experiments	O
that	O
a	O
square	O
loss	B-MetricName
can	O
,	O
in	O
many	O
cases	O
,	O
significantly	O
improve	O
performance	O
.	O

Most	O
deep	O
learning	O
approaches	O
for	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
generation	O
are	O
limited	O
to	O
the	O
WikiSQL	B-DatasetName
dataset	O
,	O
which	O
only	O
supports	O
very	O
simple	O
queries	O
over	O
a	O
single	O
table	O
.	O
We	O
focus	O
on	O
the	O
Spider	O
dataset	O
,	O
a	O
complex	O
and	O
crossdomain	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
task	O
,	O
which	O
includes	O
complex	O
queries	O
over	O
multiple	O
tables	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
SQL	O
clause	O
-	O
wise	O
decoding	O
neural	O
architecture	O
with	O
a	O
self	O
-	O
attention	O
based	O
database	O
schema	O
encoder	O
to	O
address	O
the	O
Spider	O
task	O
.	O
Each	O
of	O
the	O
clause	O
-	O
specific	O
decoders	O
consists	O
of	O
a	O
set	O
of	O
sub	O
-	O
modules	O
,	O
which	O
is	O
defined	O
by	O
the	O
syntax	O
of	O
each	O
clause	O
.	O
Additionally	O
,	O
our	O
model	O
works	O
recursively	O
to	O
support	O
nested	O
queries	O
.	O
When	O
evaluated	O
on	O
the	O
Spider	O
dataset	O
,	O
our	O
approach	O
achieves	O
4.6	O
%	O
and	O
9.8	O
%	O
accuracy	B-MetricName
gain	O
in	O
the	O
test	O
and	O
dev	O
sets	O
,	O
respectively	O
.	O
In	O
addition	O
,	O
we	O
show	O
that	O
our	O
model	O
is	O
significantly	O
more	O
effective	O
at	O
predicting	O
complex	O
and	O
nested	O
queries	O
than	O
previous	O
work	O
.	O

Text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
generation	O
is	O
the	O
task	O
of	O
translating	O
a	O
natural	O
language	O
question	O
into	O
the	O
corresponding	O
SQL	O
.	O
Recently	O
,	O
various	O
deep	O
learning	O
approaches	O
have	O
been	O
proposed	O
based	O
on	O
the	O
WikiSQL	B-DatasetName
dataset	O
(	O
Zhong	O
et	O
al	O
,	O
2017	O
)	O
.	O
However	O
,	O
because	O
WikiSQL	B-DatasetName
contains	O
only	O
very	O
simple	O
queries	O
over	O
just	O
a	O
single	O
table	O
,	O
these	O
approaches	O
(	O
Xu	O
et	O
al	O
,	O
2017	O
;	O
Huang	O
et	O
al	O
,	O
2018	O
;	O
Yu	O
et	O
al	O
,	O
2018a	O
;	O
Dong	O
and	O
Lapata	O
,	O
2018	O
)	O
can	O
not	O
be	O
applied	O
directly	O
to	O
generate	O
complex	O
queries	O
containing	O
elements	O
such	O
as	O
JOIN	O
,	O
GROUP	O
BY	O
,	O
and	O
nested	O
queries	O
.	O
To	O
overcome	O
this	O
limitation	O
,	O
Yu	O
et	O
al	O
(	O
2018c	O
)	O
introduced	O
Spider	O
,	O
a	O
new	O
complex	O
and	O
crossdomain	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
dataset	O
.	O
It	O
contains	O
a	O
large	O
number	O
of	O
complex	O
queries	O
over	O
different	O
databases	O
with	O
multiple	O
tables	O
.	O
It	O
also	O
requires	O
a	O
model	O
to	O
generalize	O
to	O
unseen	O
database	O
schema	O
as	O
different	O
databases	O
are	O
used	O
for	O
training	O
and	O
testing	O
.	O
Therefore	O
,	O
a	O
model	O
should	O
understand	O
not	O
only	O
the	O
natural	O
language	O
question	O
but	O
also	O
the	O
schema	O
of	O
the	O
corresponding	O
database	O
to	O
predict	O
the	O
correct	O
SQL	O
query	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
SQL	O
-	O
specific	O
clause	O
-	O
wise	O
decoding	O
neural	O
network	O
model	O
to	O
address	O
the	O
Spider	O
task	O
.	O
We	O
first	O
predict	O
a	O
sketch	O
for	O
each	O
SQL	O
clause	O
(	O
e.g.	O
,	O
SELECT	O
,	O
WHERE	O
)	O
with	O
text	B-TaskName
classification	I-TaskName
modules	O
.	O
Then	O
,	O
clause	O
-	O
specific	O
decoders	O
find	O
the	O
columns	O
and	O
corresponding	O
operators	O
based	O
on	O
the	O
sketches	O
.	O
Our	O
contributions	O
are	O
summarized	O
as	O
follows	O
.	O
We	O
decompose	O
the	O
clause	O
-	O
wise	O
SQL	O
decoding	O
process	O
.	O
We	O
also	O
modularize	O
each	O
of	O
the	O
clause	O
-	O
specific	O
decoders	O
into	O
sub	O
-	O
modules	O
based	O
on	O
the	O
syntax	O
of	O
each	O
clause	O
.	O
Our	O
architecture	O
enables	O
the	O
model	O
to	O
learn	O
clausedependent	O
context	O
and	O
also	O
ensures	O
the	O
syntactic	O
correctness	O
of	O
the	O
predicted	O
SQL	O
.	O
Our	O
model	O
works	O
recursively	O
so	O
that	O
it	O
can	O
predict	O
nested	O
queries	O
.	O
We	O
also	O
introduce	O
a	O
self	O
-	O
attention	O
based	O
database	O
schema	O
encoder	O
that	O
enables	O
our	O
model	O
to	O
generalize	O
to	O
unseen	O
databases	O
.	O
In	O
the	O
experiment	O
on	O
the	O
Spider	O
dataset	O
,	O
we	O
achieve	O
24.3	O
%	O
and	O
28.8	O
%	O
exact	O
SQL	O
matching	O
accuracy	B-MetricName
on	O
the	O
test	O
and	O
dev	O
set	O
respectively	O
,	O
which	O
outperforms	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approach	O
(	O
Yu	O
et	O
al	O
,	O
2018b	O
)	O
by	O
4.6	O
%	O
and	O
9.8	O
%	O
.	O
In	O
addition	O
,	O
we	O
show	O
that	O
our	O
approach	O
is	O
significantly	O
more	O
effective	O
compared	O
to	O
previous	O
work	O
at	O
predicting	O
not	O
only	O
simple	O
SQL	O
queries	O
,	O
but	O
also	O
complex	O
and	O
nested	O
queries	O
.	O

We	O
encode	O
a	O
natural	O
language	O
question	O
with	O
a	O
bidirectional	B-MethodName
LSTM	I-MethodName
.	O
We	O
denote	O
H	O
Q	O
R	O
d×	O
|	O
X	O
|	O
as	O
the	O
question	O
encoding	O
,	O
where	O
d	O
is	O
the	O
number	O
of	O
LSTM	B-MethodName
units	O
and	O
|	O
X	O
|	O
is	O
the	O
number	O
of	O
tokens	O
in	O
the	O
question	O
.	O
To	O
encode	O
a	O
database	O
schema	O
,	O
we	O
consider	O
each	O
column	O
in	O
its	O
tables	O
as	O
a	O
concatenated	O
sequence	O
of	O
words	O
from	O
the	O
table	O
name	O
and	O
column	O
name	O
with	O
a	O
separation	O
token	O
.	O
(	O
e.g.	O
,	O
[	O
student	O
,	O
[	O
SEP	O
]	O
,	O
first	O
,	O
name	O
]	O
)	O
.	O
First	O
,	O
we	O
apply	O
bi	O
-	O
directional	O
LSTM	B-MethodName
over	O
this	O
sequence	O
for	O
each	O
column	O
.	O
Then	O
,	O
we	O
apply	O
the	O
self	O
-	O
attention	O
mechanism	O
(	O
Lin	O
et	O
al	O
,	O
2017	O
)	O
over	O
the	O
LSTM	B-MethodName
outputs	O
to	O
form	O
a	O
summarized	O
fixedsize	O
vector	O
for	O
each	O
column	O
.	O
For	O
the	O
ith	O
column	O
,	O
its	O
encoding	O
h	O
(	O
i	O
)	O
col	O
R	O
d	O
is	O
computed	O
by	O
a	O
weighted	O
sum	O
of	O
the	O
LSTM	B-MethodName
output	O
o	O
(	O
i	O
)	O
col	O
R	O
d×	O
|	O
L	O
|	O
as	O
follows	O
:	O
α	B-HyperparameterName
=	O
softmax	B-MethodName
(	O
w	O
T	O
tanh	O
(	O
o	O
(	O
i	O
)	O
col	O
)	O
)	O
(	O
1	O
)	O
h	O
(	O
i	O
)	O
col	O
=	O
o	O
(	O
i	O
)	O
col	O
α	B-HyperparameterName
T	O
(	O
2	O
)	O
where	O
|	O
L	O
|	O
is	O
the	O
number	O
of	O
tokens	O
in	O
the	O
column	O
and	O
w	O
R	O
d	O
is	O
a	O
trainable	O
parameter	O
.	O
We	O
denote	O
H	O
col	O
=	O
[	O
h	O
(	O
1	O
)	O
col	O
,	O
...	O
h	O
(	O
|	O
C	O
|	O
)	O
col	O
]	O
as	O
columns	O
encoding	O
where	O
|	O
C	O
|	O
is	O
the	O
number	O
of	O
columns	O
in	O
the	O
database	O
.	O

We	O
predict	O
the	O
clause	O
-	O
wise	O
sketch	O
via	O
8	O
different	O
text	B-TaskName
classification	I-TaskName
modules	O
that	O
include	O
the	O
number	O
of	O
SQL	O
expressions	O
in	O
each	O
clause	O
,	O
the	O
presence	O
of	O
LIMIT	O
clause	O
,	O
and	O
the	O
presence	O
of	O
INTERSECT	O
/	O
UNION	O
/	O
EXCEPT	O
as	O
described	O
in	O
Figure	O
1	O
.	O
All	O
of	O
them	O
share	O
the	O
same	O
model	O
architecture	O
but	O
are	O
trained	O
separately	O
.	O
For	O
the	O
classification	O
,	O
we	O
applied	O
attention	O
-	O
based	O
bi	O
-	O
directional	O
LSTM	B-MethodName
following	O
Zhou	O
et	O
al	O
(	O
2016	O
)	O
.	O
First	O
,	O
we	O
compute	O
sentence	O
representation	O
r	O
s	O
R	O
d	O
by	O
a	O
weighted	O
sum	O
of	O
question	O
encoding	O
H	O
Q	O
R	O
d×	O
|	O
X	O
|	O
.	O
Then	O
we	O
apply	O
the	O
softmax	B-MethodName
classifier	O
to	O
choose	O
the	O
sketch	O
as	O
follows	O
:	O
α	B-HyperparameterName
s	O
=	O
softmax	B-MethodName
(	O
w	O
T	O
s	O
tanh	O
(	O
H	O
Q	O
)	O
)	O
(	O
3	O
)	O
r	O
s	O
=	O
H	O
Q	O
α	B-HyperparameterName
T	O
s	O
(	O
4	O
)	O
P	O
sketch	O
=	O
softmax	B-MethodName
(	O
W	O
s	O
r	O
s	O
+	O
b	O
s	O
)	O
(	O
5	O
)	O
where	O
w	O
s	O
R	O
d	O
,	O
W	O
s	O
R	O
ns×d	O
,	O
b	O
s	O
R	O
ns	O
are	O
trainable	O
parameters	O
and	O
n	O
s	O
is	O
the	O
number	O
of	O
possible	O
sketches	O
.	O

To	O
predict	O
columns	O
and	O
operators	O
,	O
we	O
use	O
the	O
LSTM	B-MethodName
decoder	O
with	O
the	O
attention	O
mechanism	O
(	O
Luong	O
et	O
al	O
,	O
2015	O
)	O
such	O
that	O
the	O
number	O
of	O
decoding	O
steps	O
are	O
decided	O
by	O
the	O
sketch	O
prediction	O
module	O
.	O
We	O
train	O
5	O
different	O
column	O
prediction	O
modules	O
separately	O
for	O
each	O
SQL	O
clause	O
,	O
but	O
they	O
share	O
the	O
same	O
architecture	O
.	O
In	O
the	O
column	O
prediction	O
module	O
,	O
the	O
hidden	O
state	O
of	O
the	O
decoder	O
at	O
the	O
t	O
-	O
th	O
decoding	O
step	O
is	O
computed	O
as	O
d	O
(	O
t	O
)	O
col	O
(	O
R	O
d	O
)	O
=	O
LSTM	B-MethodName
(	O
d	O
(	O
t−1	O
)	O
col	O
,	O
h	O
(	O
t−1	O
)	O
col	O
)	O
,	O
where	O
h	O
(	O
t−1	O
)	O
col	O
R	O
d	O
is	O
an	O
encoding	O
of	O
the	O
predicted	O
column	O
in	O
the	O
previous	O
decoding	O
step	O
.	O
The	O
context	O
vector	O
r	O
(	O
t	O
)	O
is	O
computed	O
by	O
a	O
weighted	O
sum	O
of	O
question	O
encodings	O
H	O
Q	O
R	O
d×	O
|	O
X	O
|	O
based	O
on	O
attention	O
weight	O
as	O
follows	O
:	O
α	B-HyperparameterName
(	O
t	O
)	O
=	O
softmax	B-MethodName
(	O
d	O
(	O
t	O
)	O
col	O
T	O
H	O
Q	O
)	O
(	O
6	O
)	O
r	O
(	O
t	O
)	O
=	O
H	O
Q	O
α	B-HyperparameterName
(	O
t	O
)	O
T	O
(	O
7	O
)	O
Then	O
,	O
the	O
attentional	O
output	O
of	O
the	O
t	O
-	O
th	O
decoding	O
step	O
a	O
(	O
t	O
)	O
col	O
is	O
computed	O
as	O
a	O
linear	O
combination	O
of	O
d	O
(	O
t	O
)	O
col	O
R	O
d	O
and	O
r	O
(	O
t	O
)	O
R	O
d	O
followed	O
by	O
tanh	B-MethodName
activation	I-MethodName
.	O
a	O
(	O
t	O
)	O
col	O
=	O
tanh	O
(	O
W	O
1	O
d	O
(	O
t	O
)	O
col	O
+	O
W	O
2	O
r	O
(	O
t	O
)	O
)	O
(	O
8	O
)	O
where	O
W	O
1	O
,	O
W	O
2	O
R	O
d×d	O
are	O
trainable	O
parameters	O
.	O
Finally	O
,	O
the	O
probability	O
for	O
each	O
column	O
at	O
the	O
tth	O
decoding	O
step	O
is	O
computed	O
as	O
a	O
dot	O
product	O
between	O
a	O
(	O
t	O
)	O
col	O
R	O
d	O
and	O
the	O
encoding	O
of	O
each	O
column	O
in	O
H	O
col	O
R	O
d×	O
|	O
C	O
|	O
followed	O
by	O
softmax	B-MethodName
.	O
P	O
(	O
t	O
)	O
col	O
=	O
softmax	B-MethodName
(	O
a	O
(	O
t	O
)	O
col	O
T	O
H	O
col	O
)	O
(	O
9	O
)	O
To	O
predict	O
corresponding	O
operators	O
for	O
each	O
predicted	O
column	O
,	O
we	O
use	O
a	O
decoder	O
of	O
the	O
same	O
architecture	O
as	O
in	O
the	O
column	O
prediction	O
module	O
.	O
The	O
only	O
difference	O
is	O
that	O
a	O
decoder	O
input	O
at	O
the	O
t	O
-	O
th	O
decoding	O
step	O
is	O
an	O
encoding	O
of	O
the	O
t	O
-	O
th	O
predicted	O
column	O
from	O
the	O
column	O
prediction	O
module	O
.	O
d	O
(	O
t	O
)	O
op	O
=	O
LSTM	B-MethodName
(	O
d	O
(	O
t−1	O
)	O
op	O
,	O
h	O
(	O
t	O
)	O
col	O
)	O
(	O
10	O
)	O
Attentional	O
output	O
a	O
(	O
t	O
)	O
op	O
R	O
d	O
is	O
computed	O
identically	O
to	O
Eq	O
.	O
(	O
8	O
)	O
.	O
Then	O
,	O
the	O
probability	O
for	O
operators	O
corresponding	O
to	O
the	O
t	O
-	O
th	O
predicted	O
column	O
is	O
computed	O
by	O
the	O
softmax	B-MethodName
classifier	O
as	O
follows	O
:	O
P	O
(	O
t	O
)	O
op	O
=	O
softmax	B-MethodName
(	O
W	O
o	O
a	O
(	O
t	O
)	O
op	O
+	O
b	O
o	O
)	O
(	O
11	O
)	O
where	O
W	O
o	O
R	O
no×d	O
and	O
b	O
o	O
R	O
no	O
are	O
trainable	O
parameters	O
and	O
n	O
o	O
is	O
the	O
number	O
of	O
possible	O
operators	O
.	O

We	O
evaluate	O
our	O
model	O
with	O
Spider	O
(	O
Yu	O
et	O
al	O
,	O
2018c	O
)	O
,	O
a	O
large	O
-	O
scale	O
,	O
complex	O
and	O
cross	O
-	O
domain	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
SQL	I-TaskName
dataset	O
.	O
We	O
follow	O
the	O
same	O
database	O
split	O
as	O
Yu	O
et	O
al	O
(	O
2018c	O
)	O
,	O
which	O
ensures	O
that	O
any	O
database	O
schema	O
that	O
appears	O
in	O
the	O
training	O
set	O
does	O
not	O
appear	O
in	O
the	O
dev	O
or	O
test	O
set	O
.	O
Through	O
this	O
split	O
,	O
we	O
examine	O
how	O
well	O
our	O
model	O
can	O
be	O
generalized	O
to	O
unseen	O
databases	O
.	O
Because	O
the	O
test	O
set	O
is	O
not	O
opened	O
to	O
the	O
public	O
,	O
we	O
use	O
the	O
dev	O
set	O
for	O
the	O
ablation	O
analysis	O
.	O
For	O
the	O
evaluation	O
metrics	O
,	O
we	O
use	O
1	O
)	O
accuracy	B-MetricName
of	O
exact	O
SQL	O
matching	O
and	O
2	O
)	O
F1	B-MetricName
score	I-MetricName
of	O
SQL	O
component	O
matching	O
,	O
proposed	O
by	O
(	O
Yu	O
et	O
al	O
,	O
2018c	O
)	O
.	O
We	O
also	O
follow	O
their	O
query	O
hardness	O
criteria	O
to	O
understand	O
the	O
model	O
performance	O
on	O
different	O
levels	O
of	O
queries	O
.	O
Our	O
model	O
and	O
all	O
the	O
baseline	O
models	O
are	O
trained	O
based	O
on	O
only	O
the	O
Spider	O
dataset	O
without	O
data	B-TaskName
augmentation	I-TaskName
.	O

We	O
use	O
the	O
same	O
hyperparameters	O
for	O
every	O
module	O
.	O
For	O
the	O
word	O
embedding	O
,	O
we	O
apply	O
deep	O
contextualized	O
word	O
representations	O
(	O
ELMO	B-MethodName
)	O
from	O
Peters	O
et	O
al	O
(	O
2018	O
)	O
and	O
allow	O
them	O
to	O
be	O
fine	O
-	O
tuned	O
during	O
the	O
training	O
.	O
For	O
the	O
question	O
and	O
column	O
encoders	O
,	O
we	O
use	O
a	O
1	O
-	O
layer	O
512	O
-	O
unit	O
bi	O
-	O
directional	O
LSTM	B-MethodName
.	O
For	O
the	O
decoders	O
in	O
the	O
columns	O
and	O
operators	O
prediction	O
modules	O
,	O
we	O
use	O
a	O
1	O
-	O
layer	O
1024unit	O
uni	O
-	O
directional	O
LSTM	B-MethodName
.	O
For	O
the	O
training	O
,	O
we	O
use	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	O
-	O
4	O
and	O
use	O
early	B-MethodName
stopping	I-MethodName
with	O
50	O
epochs	O
.	O
Additionally	O
,	O
we	O
use	O
dropout	O
(	O
Hinton	O
et	O
al	O
,	O
2012	O
)	O
with	O
a	O
rate	O
of	O
0.2	O
for	O
the	O
regularization	O
.	O

Table	O
1	O
shows	O
the	O
exact	O
SQL	O
matching	O
accuracy	B-MetricName
of	O
our	O
model	O
and	O
previous	O
models	O
.	O
We	O
achieve	O
24.3	O
%	O
and	O
28.8	O
%	O
on	O
the	O
test	O
and	O
dev	O
sets	O
respectively	O
,	O
which	O
outperforms	O
the	O
previous	O
best	O
model	O
SyntaxSQLNet	O
(	O
Yu	O
et	O
al	O
,	O
2018b	O
)	O
by	O
4.6	O
%	O
and	O
9.8	O
%	O
.	O
Moreover	O
,	O
our	O
model	O
outperforms	O
previous	O
models	O
on	O
all	O
different	O
query	O
hardness	O
levels	O
.	O
To	O
examine	O
how	O
each	O
technique	O
contributes	O
to	O
the	O
performance	O
,	O
we	O
conduct	O
an	O
ablation	O
analysis	O
of	O
three	O
aspects	O
:	O
1	O
)	O
without	O
recursion	O
,	O
2	O
)	O
without	O
self	O
-	O
attention	O
for	O
database	O
schema	O
encoding	O
,	O
and	O
3	O
)	O
without	O
sketch	O
prediction	O
modules	O
that	O
decide	O
the	O
number	O
of	O
decoding	O
steps	O
.	O
Without	O
recursive	O
sub	O
-	O
query	O
generation	O
,	O
the	O
accuracy	B-MetricName
drops	O
by	O
5.7	O
%	O
and	O
3.6	O
%	O
for	O
hard	O
and	O
extra	O
hard	O
queries	O
,	O
respectively	O
.	O
This	O
result	O
shows	O
that	O
the	O
recursion	O
we	O
use	O
enables	O
the	O
model	O
to	O
predict	O
nested	O
queries	O
.	O
When	O
using	O
the	O
final	O
LSTM	B-MethodName
hidden	O
state	O
as	O
in	O
Yu	O
et	O
al	O
(	O
2018b	O
)	O
instead	O
of	O
using	O
self	O
-	O
attention	O
for	O
schema	O
encoding	O
,	O
the	O
accuracy	B-MetricName
drops	O
by	O
4.0	O
%	O
on	O
all	O
queries	O
.	O
Finally	O
,	O
when	O
using	O
only	O
an	O
encoder	O
-	O
decoder	O
architecture	O
without	O
sketch	O
generation	O
for	O
columns	O
prediction	O
,	O
the	O
accuracy	B-MetricName
drops	O
by	O
4.7	O
%	O
.	O
For	O
the	O
component	O
matching	O
result	O
for	O
each	O
SQL	O
clause	O
,	O
our	O
model	O
outperforms	O
previous	O
approaches	O
for	O
all	O
of	O
the	O
SQL	O
components	O
by	O
a	O
significant	O
margin	O
,	O
as	O
shown	O
in	O
Table	O
2	O
.	O
Examples	O
of	O
predicted	O
SQL	O
from	O
different	O
models	O
are	O
shown	O
in	O
Appendix	O
A.	O

Classification	B-TaskName
Few	O
studies	O
to	O
date	O
have	O
addressed	O
sentence	O
-	O
level	O
readability	O
for	O
English	O
.	O
Napoles	O
&	O
Dredze	O
(	O
2010	O
)	O
built	O
their	O
own	O
corpus	O
with	O
documents	O
from	O
English	O
and	O
Simple	O
English	O
Wikipedia	O
to	O
train	O
both	O
document	O
-	O
and	O
sentence	O
-	O
level	O
classifiers	O
.	O
Using	O
bag	O
-	O
of	O
-	O
words	O
features	O
,	O
unigram	O
and	O
bigram	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
features	O
,	O
type	O
-	O
token	O
ratio	O
,	O
the	O
proportion	O
of	O
words	O
appearing	O
on	O
a	O
list	O
of	O
easier	O
words	O
,	O
and	O
parse	O
features	O
similar	O
to	O
Petersen	O
's	O
,	O
their	O
binary	O
classifier	O
achieved	O
an	O
accuracy	B-MetricName
of	O
80.8	O
%	O
on	O
this	O
task	O
.	O
The	O
structure	O
of	O
this	O
task	O
,	O
however	O
,	O
is	O
not	O
suited	O
to	O
text	B-TaskName
simplification	I-TaskName
applications	O
,	O
because	O
the	O
sentences	O
are	O
not	O
controlled	O
for	O
meaning	O
.	O
Classifying	O
a	O
sentence	O
in	O
isolation	O
as	O
more	O
likely	O
to	O
be	O
from	O
Simple	O
Wikipedia	O
or	O
English	O
Wikipedia	O
is	O
not	O
as	O
useful	O
as	O
a	O
model	O
trained	O
to	O
differentiate	O
sentences	O
carrying	O
the	O
same	O
meaning	O
.	O
This	O
work	O
is	O
not	O
directly	O
comparable	O
to	O
that	O
of	O
Vajjala	O
&	O
Meurers	O
(	O
2012	O
;	O
or	O
subsequent	O
work	O
on	O
ranking	O
sentences	O
by	O
their	O
complexity	O
due	O
to	O
the	O
differences	O
in	O
choice	O
of	O
corpora	O
and	O
task	O
structure	O
.	O
In	O
the	O
medical	B-DatasetName
domain	I-DatasetName
,	O
Kauchak	O
et	O
al	O
(	O
2014	O
)	O
also	O
looked	O
at	O
sentence	O
-	O
level	O
classification	O
,	O
identifying	O
sentences	O
as	O
being	O
either	O
simple	O
or	O
difficult	O
.	O
Their	O
features	O
included	O
word	O
length	O
,	O
sentences	O
length	O
,	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
counts	O
,	O
average	O
unigram	O
frequencies	O
and	O
standard	O
deviation	O
,	O
and	O
the	O
proportion	O
of	O
words	O
not	O
on	O
a	O
list	O
of	O
the	O
five	O
thousand	O
most	O
frequent	O
words	O
as	O
well	O
as	O
three	O
domainspecific	O
features	O
based	O
on	O
an	O
ontology	B-MethodName
of	O
medical	O
terminology	O
.	O
Ranking	O
Vajjala	O
&	O
Meurers	O
(	O
Vajjala	O
and	O
Meurers	O
,	O
2014	O
;	O
Vajjala	O
,	O
2015	O
)	O
were	O
the	O
first	O
to	O
look	O
at	O
ranking	O
sentences	O
rather	O
than	O
classifying	O
them	O
,	O
having	O
observed	O
that	O
the	O
distributions	O
of	O
predicted	O
reading	O
levels	O
across	O
the	O
two	O
subcorpora	O
of	O
the	O
Parallel	O
Wikipedia	O
corpus	O
(	O
Zhu	O
et	O
al	O
,	O
2010	O
,	O
PWKP	O
)	O
were	O
different	O
.	O
While	O
the	O
Simple	O
English	O
portion	O
of	O
the	O
corpus	O
was	O
clearly	O
skewed	O
toward	O
the	O
lower	O
grade	O
levels	O
,	O
it	O
appears	O
that	O
the	O
English	O
portion	O
of	O
the	O
corpus	O
was	O
evenly	O
distributed	O
across	O
all	O
grade	O
levels	O
,	O
making	O
binary	O
-	O
classification	O
difficult	O
.	O
This	O
led	O
Vajjala	O
&	O
Meurers	O
to	O
develop	O
a	O
ranking	O
model	O
using	O
the	O
predicted	O
reading	O
levels	O
from	O
a	O
multiclass	O
classifier	O
trained	O
on	O
whole	O
documents	O
.	O
For	O
each	O
sentence	O
pair	O
,	O
they	O
assumed	O
that	O
the	O
English	O
Wikipedia	O
sentence	O
should	O
be	O
classified	O
at	O
a	O
higher	O
level	O
than	O
the	O
Simple	O
English	O
Wikipedia	O
sentence	O
.	O
Using	O
a	O
hard	O
cut	O
-	O
off	O
(	O
i.e.	O
rank	O
(	O
sent	O
english	O
)	O
>	O
rank	O
(	O
sent	O
simple	O
)	O
)	O
,	O
their	O
model	O
achieved	O
about	O
59	O
%	O
accuracy	B-MetricName
,	O
although	O
this	O
improved	O
to	O
70	O
%	O
by	O
relaxing	O
the	O
inequality	O
constraint	O
to	O
include	O
equality	O
.	O
Based	O
on	O
the	O
finding	O
that	O
30	O
%	O
of	O
sentence	O
pairs	O
from	O
the	O
PWKP	O
corpus	O
are	O
incorrectly	O
ranked	O
despite	O
lying	O
within	O
one	O
reading	O
level	O
of	O
each	O
other	O
,	O
we	O
hypothesize	O
that	O
finer	O
-	O
grained	O
distinctions	O
may	O
be	O
necessary	O
to	O
tease	O
apart	O
the	O
differences	O
in	O
related	O
pairs	O
of	O
sentences	O
.	O
Offline	O
Psycholinguistic	O
Features	O
While	O
Vajjala	O
&	O
Meurers	O
(	O
2012	O
;	O
2014	O
)	O
do	O
use	O
some	O
psycholinguistically	O
-	O
motivated	O
features	O
,	O
their	O
features	O
are	O
primarily	O
lexical	O
in	O
nature	O
and	O
therefore	O
complementary	O
to	O
ours	O
,	O
which	O
depend	O
on	O
the	O
sentence	O
processing	O
context	O
.	O
They	O
drew	O
psycholinguistic	O
features	O
from	O
the	O
MRC	O
psycholinguistic	O
database	O
(	O
Wilson	O
,	O
1988	O
)	O
,	O
including	O
word	O
familiarity	O
,	O
concreteness	O
,	O
imageability	O
,	O
meaningfulness	O
,	O
and	O
age	O
of	O
acquisition	O
.	O
These	O
features	O
were	O
coupled	O
with	O
a	O
second	O
age	O
of	O
acquisition	O
database	O
and	O
values	O
related	O
to	O
the	O
average	O
number	O
of	O
senses	O
per	O
word	O
.	O
Towards	O
online	O
considerations	O
More	O
recently	O
,	O
Ambati	O
et	O
al	O
(	O
2016	O
)	O
used	O
an	O
incremental	O
parser	O
to	O
extend	O
Vajjala	O
&	O
Meurers	O
work	O
.	O
Since	O
human	O
processing	O
is	O
incremental	O
,	O
they	O
reasoned	O
,	O
features	O
from	O
an	O
incremental	O
parser	O
might	O
be	O
more	O
informative	O
than	O
features	O
extracted	O
from	O
a	O
nonincremental	O
parser	O
.	O
To	O
this	O
end	O
,	O
they	O
used	O
the	O
incremental	O
derivations	O
from	O
a	O
combinatory	O
categorial	O
grammar	O
(	O
CCG	O
)	O
parser	O
.	O
Ambati	O
et	O
al	O
ran	O
several	O
models	O
on	O
the	O
English	O
and	O
Simple	O
English	O
Wikipedia	O
data	O
set	O
(	O
Hwang	O
et	O
al	O
,	O
2015	O
,	O
ESEW	O
)	O
:	O
one	O
using	O
only	O
the	O
syntactic	O
features	O
from	O
(	O
Vajjala	O
and	O
Meurers	O
,	O
2014	O
)	O
;	O
another	O
(	O
INCCCG	O
)	O
using	O
only	O
features	O
from	O
the	O
incremental	O
parser	O
;	O
and	O
INCCCG+	O
,	O
incorporating	O
morpho	O
-	O
syntactic	O
and	O
psycholinguistic	O
features	O
from	O
(	O
Vajjala	O
and	O
Meurers	O
,	O
2014	O
)	O
.	O
At	O
the	O
sentence	O
level	O
,	O
they	O
include	O
sentence	O
length	O
,	O
number	O
of	O
CCG	O
constituents	O
in	O
the	O
final	O
parse	O
,	O
and	O
the	O
depth	O
of	O
the	O
CCG	O
derivation	O
.	O
They	O
also	O
use	O
count	O
features	O
for	O
the	O
number	O
of	O
times	O
each	O
CCG	O
derivation	O
rule	O
is	O
applied	O
(	O
e.g.	O
forward	O
application	O
,	O
type	O
-	O
raising	O
)	O
.	O
Finally	O
,	O
they	O
include	O
counts	O
of	O
different	O
CCG	O
syntactic	O
categories	O
as	O
well	O
as	O
the	O
average	O
'	O
complexity	O
'	O
of	O
the	O
syntactic	O
categories	O
.	O
While	O
the	O
parser	O
they	O
use	O
is	O
inspired	O
by	O
human	O
behavior	O
,	O
in	O
that	O
it	O
is	O
an	O
incremental	O
parser	O
,	O
these	O
features	O
do	O
not	O
re	O
-	O
late	O
to	O
any	O
specific	O
linguistic	O
theory	O
of	O
sentence	O
processing	O
.	O
The	O
work	O
presented	O
here	O
is	O
most	O
comparable	O
to	O
that	O
of	O
Vajjala	O
&	O
Meurers	O
and	O
Ambati	O
et	O
al	O
,	O
as	O
we	O
all	O
address	O
the	O
problem	O
of	O
ranking	O
sentences	O
according	O
to	O
their	O
linguistic	O
complexity	O
.	O
Our	O
study	O
is	O
the	O
only	O
one	O
of	O
the	O
three	O
to	O
examine	O
features	O
based	O
on	O
theories	O
of	O
online	O
sentence	O
processing	O
.	O
Ambati	O
et	O
al	O
(	O
2016	O
)	O
provide	O
accuracy	B-MetricName
information	O
for	O
their	O
own	O
features	O
as	O
well	O
as	O
Vajjala	O
&	O
Meurers	O
'	O
(	O
2014	O
)	O
features	O
on	O
the	O
English	O
and	O
Simple	O
English	O
Wikipedia	O
corpus	O
(	O
ESEW	O
)	O
which	O
we	O
use	O
,	O
but	O
used	O
a	O
60	O
-	O
20	O
-	O
20	O
training	O
-	O
dev	O
-	O
test	O
split	O
where	O
we	O
used	O
10	O
-	O
fold	O
cross	O
-	O
validation	O
,	O
making	O
the	O
results	O
not	O
directly	O
comparable	O
.	O

It	O
is	O
a	O
work	O
-	O
hard	O
,	O
play	O
-	O
hard	O
ethic	O
that	O
many	O
of	O
the	O
world	O
's	O
billionaires	O
might	O
subscribe	O
to	O
but	O
it	O
would	O
be	O
a	O
huge	O
change	O
for	O
most	O
workers	O
and	O
their	O
employers	O
.	O
2	O
It	O
is	O
a	O
'	O
work	O
-	O
hard	O
,	O
play	O
-	O
hard	O
'	O
way	O
of	O
thinking	O
that	O
many	O
of	O
the	O
world	O
's	O
billionaires	O
might	O
agree	O
with	O
but	O
it	O
would	O
be	O
a	O
huge	O
change	O
for	O
most	O
workers	O
and	O
their	O
employers	O
.	O
1	O
Many	O
of	O
the	O
world	O
's	O
billionaires	O
might	O
agree	O
with	O
this	O
way	O
of	O
thinking	O
but	O
it	O
would	O
be	O
a	O
very	O
big	O
change	O
for	O
most	O
workers	O
and	O
their	O
employers	O
.	O
difficult	O
-	O
to	O
-	O
integrate	O
point	O
in	O
the	O
sentence	O
.	O
These	O
features	O
comprise	O
the	O
INTEGRATIONCOST	O
model	O
.	O
Finally	O
,	O
we	O
use	O
a	O
modified	O
version	O
of	O
the	O
IDD3	O
library	O
from	O
Andre	O
Cunha	O
(	O
Cunha	O
et	O
al	O
,	O
2015	O
)	O
to	O
extract	O
idea	O
density	O
decomposed	O
across	O
three	O
types	O
of	O
propositional	O
idea	O
:	O
predications	O
,	O
modifications	O
,	O
and	O
connections	O
.	O
10	O
Here	O
we	O
use	O
only	O
averaged	O
features	O
,	O
as	O
the	O
crucial	O
measure	O
is	O
the	O
idea	O
density	O
rather	O
than	O
the	O
raw	O
number	O
of	O
ideas	O
being	O
expressed	O
.	O
These	O
features	O
comprise	O
the	O
IDEAD	O
-	O
ENSITY	O
model	O
.	O
As	O
a	O
point	O
of	O
comparison	O
for	O
these	O
models	O
,	O
we	O
created	O
a	O
BASELINE	O
which	O
used	O
only	O
sentence	O
length	O
and	O
the	O
average	O
word	O
length	O
as	O
features	O
.	O
We	O
also	O
created	O
models	O
based	O
on	O
features	O
grouped	O
by	O
the	O
parser	O
used	O
to	O
extract	O
them	O
:	O
SUR	O
-	O
PRISAL+EMBED	O
for	O
the	O
ModelBlocks	O
parser	O
and	O
IDEA+INTEGRATION	O
for	O
the	O
Stanford	O
parser	O
.	O
While	O
ModelBlocks	O
achieves	O
competitive	O
accuracies	O
,	O
it	O
is	O
much	O
slower	O
than	O
other	O
state	O
-	O
of	O
-	O
theart	O
parsers	O
available	O
today	O
.	O
Therefore	O
we	O
wanted	O
to	O
provide	O
a	O
point	O
of	O
comparison	O
regarding	O
the	O
relative	O
utility	O
of	O
these	O
parsers	O
:	O
grouping	O
features	O
by	O
parser	O
allows	O
us	O
to	O
assess	O
the	O
trade	O
-	O
off	O
between	O
model	O
accuracy	B-MetricName
and	O
the	O
time	O
necessary	O
for	O
feature	O
extraction	O
.	O
Finally	O
,	O
we	O
considered	O
combinations	O
of	O
the	O
parser	O
-	O
grouped	O
features	O
with	O
the	O
baseline	O
(	O
BASE+SURPRISAL+EMBED	O
and	O
BASE+IDEA+INTEGRATION	O
)	O
and	O
a	O
10	O
Code	O
available	O
at	O
:	O
https://github.com/	O
dmhowcroft	O
/	O
idd3	O
.	O
FULLMODEL	O
using	O
the	O
baseline	O
features	O
and	O
all	O
of	O
the	O
psycholinguistic	O
features	O
.	O
Replication	O
The	O
scripts	O
required	O
for	O
replication	O
are	O
available	O
at	O
https://github.com/	O
dmhowcroft	O
/	O
eacl2017	O
-	O
replication	O
.	O
This	O
includes	O
pointers	O
to	O
the	O
corpora	O
,	O
preprocessing	O
scripts	O
and	O
settings	O
for	O
the	O
parsers	O
,	O
as	O
well	O
as	O
scripts	O
for	O
feature	O
extraction	O
and	O
running	O
the	O
averaged	O
perceptron	O
model	O
.	O

The	O
feature	O
sets	O
for	O
individual	O
psycholinguistic	O
theories	O
only	O
achieve	O
accuracies	O
between	O
55	O
%	O
and	O
65	O
%	O
(	O
see	O
the	O
first	O
4	O
columns	O
of	O
Fig	O
.	O
1	O
)	O
.	O
Combining	O
all	O
of	O
these	O
features	O
into	O
the	O
PSYCHOLIN	O
-	O
GUISTIC	O
model	O
improves	O
performance	O
to	O
nearly	O
70	O
%	O
(	O
column	O
5	O
)	O
.	O
Looking	O
at	O
the	O
feature	O
sets	O
grouped	O
by	O
parser	O
(	O
columns	O
6	O
and	O
7	O
)	O
,	O
we	O
see	O
that	O
the	O
combination	O
of	O
surprisal	O
and	O
embedding	O
depth	O
(	O
from	O
the	O
ModelBlocks	O
parser	O
)	O
significantly	O
outperforms	O
the	O
combination	O
of	O
integration	O
cost	O
and	O
idea	O
density	O
(	O
from	O
the	O
Stanford	O
Parser	O
)	O
.	O
However	O
,	O
the	O
strength	O
of	O
the	O
features	O
derived	O
from	O
ModelBlocks	O
seems	O
to	O
be	O
primarily	O
driven	O
by	O
the	O
EMBEDDING	O
features	O
,	O
while	O
the	O
strength	O
of	O
the	O
dependency	O
-	O
parse	O
-	O
derived	O
features	O
appears	O
to	O
stem	O
from	O
INTEGRATIONCOST	O
.	O
Moving	O
to	O
Figure	O
2	O
,	O
we	O
see	O
that	O
our	O
BASE	B-MethodName
-	O
LINE	B-MethodName
features	O
achieved	O
an	O
accuracy	B-MetricName
of	O
71.24	O
%	O
,	O
despite	O
using	O
only	O
average	O
word	O
length	O
and	O
sentence	O
length	O
.	O
This	O
is	O
1.48	O
percentage	O
points	O
higher	O
than	O
the	O
69.76	O
%	O
accuracy	B-MetricName
of	O
the	O
PSYCHOLINGUIS	O
-	O
TIC	O
model	O
,	O
which	O
includes	O
surprisal	O
,	O
embedding	O
depth	O
,	O
integration	O
cost	O
,	O
and	O
idea	O
density	O
.	O
However	O
,	O
the	O
FULL	O
model	O
(	O
column	O
4	O
)	O
outperforms	O
the	O
BASELINE	O
by	O
a	O
statistically	O
significant	O
11	O
1.98	O
percentage	O
points	O
(	O
p	O
0.01	O
)	O
.	O
This	O
confirms	O
our	O
primary	O
hypothesis	O
:	O
psycholinguistic	O
features	O
based	O
on	O
online	O
sentence	O
processing	O
can	O
improve	O
models	O
of	O
sentence	O
complexity	O
beyond	O
a	O
simple	O
baseline	O
.	O
To	O
address	O
the	O
secondary	O
hypothesis	O
,	O
we	O
turn	O
to	O
the	O
OSE	O
data	O
in	O
Figure	O
3	O
.	O
The	O
best	O
model	O
for	O
this	O
corpus	O
uses	O
the	O
baseline	O
features	O
combined	O
with	O
embedding	O
depth	O
and	O
surprisal	O
features	O
extracted	O
from	O
ModelBlocks	O
.	O
In	O
both	O
OSE	O
f	O
ar	O
and	O
OSE	O
near	O
we	O
gain	O
about	O
3	O
points	O
over	O
the	O
baseline	O
when	O
adding	O
these	O
features	O
(	O
3.18	O
and	O
3.25	O
points	O
,	O
respectively	O
)	O
,	O
which	O
is	O
similar	O
to	O
the	O
gains	O
for	O
the	O
FULL	O
model	O
over	O
the	O
baseline	O
.	O
The	O
fact	O
that	O
the	O
increase	O
in	O
performance	O
between	O
the	O
BASELINE	O
model	O
and	O
the	O
best	O
performing	O
model	O
does	O
not	O
differ	O
between	O
the	O
OSE	O
near	O
and	O
the	O
OSE	O
f	O
ar	O
datasets	O
suggests	O
a	O
lack	O
of	O
support	O
for	O
our	O
secondary	O
hypothesis	O
that	O
these	O
features	O
are	O
espe	O
-	O
cially	O
helpful	O
for	O
distinguishing	O
items	O
of	O
similar	O
difficulty	O
levels	O
.	O
These	O
results	O
warrant	O
a	O
full	O
comparison	O
to	O
the	O
work	O
of	O
Ambati	O
et	O
al	O
(	O
2016	O
)	O
,	O
despite	O
the	O
differences	O
in	O
our	O
evaluation	O
sets	O
.	O
Ambati	O
et	O
al	O
found	O
that	O
their	O
features	O
based	O
on	O
incremental	O
CCG	O
derivations	O
achieved	O
an	O
accuracy	B-MetricName
of	O
72.12	O
%	O
,	O
while	O
the	O
offline	O
psycholinguistic	O
features	O
of	O
Vajjala	O
&	O
Meurers	O
came	O
in	O
at	O
74.58	O
%	O
,	O
1.36	O
percentage	O
points	O
better	O
than	O
our	O
73.22	O
%	O
.	O
Finally	O
,	O
a	O
model	O
combining	O
all	O
of	O
Vajjala	O
&	O
Meurers	O
featurs	O
with	O
the	O
incremental	O
CCG	O
features	O
achieved	O
a	O
performance	O
of	O
78.87	O
%	O
.	O
Since	O
the	O
features	O
examined	O
in	O
our	O
study	O
are	O
complementary	O
to	O
those	O
proposed	O
by	O
these	O
two	O
previous	O
studies	O
,	O
a	O
model	O
combining	O
all	O
of	O
these	O
features	O
should	O
further	O
improve	O
in	O
accuracy	B-MetricName
.	O

It	O
is	O
obvious	O
that	O
all	O
proposed	O
procedure	O
candidates	O
are	O
co	O
-	O
related	O
to	O
each	O
other	O
.	O
In	O
order	O
to	O
encode	O
this	O
interaction	O
,	O
we	O
follow	O
the	O
method	O
in	O
(	O
Zhou	O
et	O
al	O
,	O
2018a	O
)	O
which	O
uses	O
an	O
LSTM	B-MethodName
model	O
to	O
predict	O
a	O
sequence	O
from	O
the	O
K	O
×	O
T	O
generated	O
procedure	O
proposal	O
.	O
The	O
input	O
of	O
the	O
recurrent	O
prediction	O
model	O
for	O
each	O
time	O
step	O
consists	O
of	O
three	O
parts	O
:	O
frame	O
features	O
,	O
the	O
position	O
embedding	O
,	O
the	O
plausibility	O
score	O
feature	O
.	O
Frame	O
Features	O
For	O
a	O
generated	O
procedure	O
proposal	O
,	O
the	O
corresponding	O
feature	O
vectors	O
F	O
(	O
k	O
,	O
t	O
)	O
are	O
calculated	O
as	O
follows	O
:	O
F	O
(	O
k	O
,	O
t	O
)	O
=	O
f	O
C	O
(	O
k	O
,	O
t	O
)	O
−L	O
(	O
k	O
,	O
t	O
)	O
,	O
,	O
f	O
C	O
(	O
k	O
,	O
t	O
)	O
+	O
L	O
(	O
k	O
,	O
t	O
)	O
(	O
1	O
)	O
C	O
(	O
k	O
,	O
t	O
)	O
=	O
t	O
+	O
k	O
(	O
k	O
)	O
×	O
M	O
(	O
k	O
,	O
t	O
)	O
m	O
(	O
2	O
)	O
L	O
(	O
k	O
,	O
t	O
)	O
=	O
k	O
(	O
k	O
)	O
×	O
M	O
(	O
k	O
,	O
t	O
)	O
l	O
2	O
(	O
3	O
)	O
where	O
k	B-HyperparameterName
=	I-HyperparameterName
{	O
k	O
1	O
,	O
,	O
k	O
K	O
}	O
is	O
a	O
list	O
of	O
different	O
ker	O
-	O
nel	O
sizes	O
.	O
The	O
M	O
(	O
k	O
,	O
t	O
)	O
m	O
and	O
M	O
(	O
k	O
,	O
t	O
)	O
l	O
represent	O
the	O
midpoint	O
and	O
length	O
offset	O
of	O
the	O
span	O
for	O
k	O
-	O
th	O
kernel	O
and	O
t	O
-	O
th	O
frame	O
respectively	O
and	O
k	O
(	O
k	O
)	O
is	O
the	O
length	O
of	O
the	O
k	O
-	O
th	O
kernel	O
.	O

We	O
treat	O
all	O
possible	O
positions	O
as	O
a	O
list	O
of	O
tokens	O
and	O
use	O
an	O
embedding	O
layer	O
to	O
get	O
a	O
continuous	O
representation	O
.	O
The	O
[	O
BOS	O
]	O
and	O
[	O
EOS	O
]	O
,	O
i.e.	O
the	O
begin	O
of	O
sentence	O
and	O
the	O
end	O
of	O
sentence	O
,	O
are	O
also	O
added	O
into	O
the	O
vocabulary	O
for	O
sequence	O
prediction	O
.	O
Score	B-MetricName
Feature	O
The	O
score	O
feature	O
is	O
a	O
flatten	O
of	O
matrix	O
M	O
s	O
,	O
i.e.	O
s	O
R	O
K	O
T	O
×1	O
.	O
The	O
input	O
embedding	O
of	O
each	O
time	O
step	O
is	O
the	O
concatenation	O
of	O
:	O
1	O
.	O
The	O
averaged	O
features	O
of	O
the	O
proposal	O
predicted	O
in	O
the	O
previous	O
step	O
t	O
:	O
F	O
(	O
k	O
,	O
t	O
)	O
=	O
1	O
2L	O
(	O
k	O
,	O
t	O
)	O
L	O
(	O
k	O
,	O
t	O
)	O
t	O
=	O
−L	O
(	O
k	O
,	O
t	O
)	O
f	O
C	O
(	O
k	O
,	O
t	O
)	O
+	O
t	O
(	O
4	O
)	O
2	O
.	O
The	O
position	O
embedding	O
of	O
the	O
proposal	O
.	O
3	O
.	O
The	O
score	O
feature	O
s.	O
Specifically	O
,	O
for	O
the	O
first	O
step	O
,	O
the	O
input	O
frame	O
feature	O
is	O
the	O
averaged	O
frame	O
features	O
of	O
the	O
entire	O
video	O
.	O
F	O
=	O
1	O
T	O
T	O
t=1	O
f	O
t	O
and	O
the	O
position	O
embedding	O
is	O
the	O
encoding	O
of	O
[	O
BOS	O
]	O
.	O
The	O
procedure	O
extraction	O
finishes	O
when	O
[	O
EOS	O
]	O
is	O
predicted	O
,	O
and	O
the	O
output	O
of	O
this	O
module	O
is	O
a	O
sequence	O
of	O
indexes	O
of	O
frames	O
:	O
P	O
=	O
{	O
p	O
1	O
p	O
L	O
}	O
where	O
L	O
is	O
the	O
maximum	O
count	O
of	O
the	O
predicted	O
proposals	O
.	O

The	O
target	O
of	O
the	O
model	O
is	O
to	O
extract	O
procedures	O
and	O
generate	O
captions	O
.	O
The	O
loss	B-MetricName
function	O
consists	O
of	O
four	O
parts	O
:	O
(	O
1	O
)	O
L	O
s	O
:	O
a	O
binary	O
cross	O
-	O
entropy	O
loss	B-MetricName
of	O
each	O
generated	O
positive	O
and	O
negative	O
procedure	O
;	O
(	O
2	O
)	O
L	O
r	O
:	O
the	O
regression	O
loss	B-MetricName
with	O
a	O
smooth	O
l1	O
-	O
loss	B-MetricName
(	O
Ren	O
et	O
al	O
,	O
2015	O
)	O
of	O
a	O
time	O
span	O
between	O
the	O
extracted	O
and	O
the	O
ground	O
-	O
truth	O
procedure	O
.	O
(	O
3	O
)	O
L	O
p	O
:	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
of	O
each	O
proposed	O
procedure	O
in	O
the	O
predicted	O
sequence	O
of	O
proposals	O
.	O
(	O
4	O
)	O
L	O
c	O
:	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
of	O
each	O
token	O
in	O
the	O
generated	O
procedure	O
captions	O
.	O
Here	O
are	O
the	O
formulations	O
:	O
L	O
=	O
α	B-HyperparameterName
s	O
L	O
s	O
+	O
α	B-HyperparameterName
r	O
L	O
r	O
+	O
α	B-HyperparameterName
p	O
L	O
p	O
+	O
α	B-HyperparameterName
c	O
L	O
c	O
(	O
5	O
)	O
L	O
s	O
=	O
−	O
1	O
C	O
P	O
C	O
P	O
i=1	O
log	O
(	O
M	O
P	O
s	O
)	O
−	O
1	O
C	O
N	O
C	O
N	O
i=1	O
log	O
(	O
1	O
−	O
M	O
N	O
s	O
)	O
(	O
6	O
)	O
L	O
r	O
=	O
1	O
C	O
P	O
C	O
P	O
i=1	O
|	O
|	O
B	O
pred	O
i	O
−	O
B	O
gt	O
i	O
|	O
|	O
s−l1	O
(	O
7	O
)	O
L	O
p	O
=	O
−	O
1	O
L	O
L	O
l=1	O
log	O
(	O
p	O
l	O
1	O
(	O
gt	O
l	O
)	O
l	O
)	O
(	O
8	O
)	O
L	O
c	O
=	O
−	O
1	O
L	O
L	O
l=1	O
1	O
|	O
W	O
l	O
|	O
w	O
W	O
l	O
log	O
(	O
w1	O
(	O
gt	O
w	O
)	O
)	O
(	O
9	O
)	O
where	O
M	O
P	O
s	O
and	O
M	O
N	O
s	O
are	O
the	O
scoring	O
matrix	O
of	O
positive	O
and	O
negative	O
samples	O
in	O
a	O
video	O
,	O
and	O
C	O
P	O
and	O
C	O
N	O
represent	O
the	O
count	O
separately	O
.	O
Here	O
we	O
regard	O
a	O
sample	O
as	O
positive	O
if	O
its	O
IoU	B-MetricName
(	O
Intersection	O
of	O
Union	O
)	O
with	O
any	O
ground	O
-	O
truth	O
procedure	O
is	O
more	O
than	O
0.8	O
.	O
If	O
the	O
IoU	B-MetricName
is	O
less	O
than	O
0.2	O
,	O
we	O
treat	O
it	O
as	O
negative	O
.	O
The	O
loss	B-MetricName
L	O
s	O
aims	O
to	O
enlarge	O
the	O
score	O
of	O
all	O
positive	O
samples	O
and	O
decrease	O
the	O
score	O
otherwise	O
.	O
The	O
B	O
pred	O
i	O
and	O
B	O
gt	O
i	O
represent	O
the	O
boundary	O
(	O
calculated	O
by	O
the	O
offset	O
of	O
midpoint	O
and	O
length	O
)	O
of	O
the	O
positive	O
sample	O
and	O
ground	O
-	O
truth	O
procedure	O
separately	O
.	O
We	O
only	O
take	O
positive	O
samples	O
into	O
account	O
and	O
conduct	O
the	O
regression	O
with	O
L	O
r	O
to	O
shorten	O
the	O
distance	O
between	O
all	O
positive	O
samples	O
and	O
the	O
ground	O
-	O
truth	O
procedures	O
.	O
The	O
p	O
l	O
is	O
the	O
classification	O
result	O
of	O
the	O
procedure	O
extraction	O
module	O
and	O
the	O
value	O
of	O
1	O
will	O
be	O
1	O
if	O
the	O
predicted	O
class	O
of	O
extracted	O
procedure	O
proposal	O
is	O
identical	O
to	O
the	O
class	O
of	O
the	O
groundtruth	O
proposal	O
with	O
the	O
maximal	O
IoU	B-MetricName
and	O
0	B-DatasetName
otherwise	O
.	O
The	O
cross	O
-	O
entropy	O
loss	B-MetricName
L	O
p	O
aims	O
to	O
exploit	O
the	O
model	O
to	O
correctly	O
select	O
the	O
most	O
similar	O
proposal	O
of	O
each	O
ground	O
-	O
truth	O
procedure	O
from	O
many	O
positive	O
samples	O
.	O
Finally	O
,	O
W	O
stores	O
all	O
decoded	O
captions	O
of	O
procedures	O
of	O
a	O
video	O
.	O
The	O
L	O
c	O
is	O
designed	O
for	O
the	O
captioning	O
module	O
based	O
on	O
the	O
extracted	O
procedures	O
.	O

We	O
separately	O
evaluate	O
the	O
procedure	O
extraction	O
and	O
captioning	O
module	O
.	O
For	O
procedure	O
extraction	O
,	O
we	O
adopt	O
the	O
widely	O
used	O
mJacc	O
(	O
mean	O
of	O
Jaccard	O
)	O
(	O
Bojanowski	O
et	O
al	O
,	O
2014	O
)	O
and	O
mIoU	B-MetricName
(	O
mean	O
of	O
IoU	B-MetricName
)	O
metrics	O
for	O
evaluating	O
the	O
procedure	O
proposition	O
.	O
The	O
Jaccard	O
calculates	O
the	O
intersection	O
of	O
the	O
predicted	O
and	O
ground	O
-	O
truth	O
procedure	O
proposals	O
over	O
the	O
length	O
of	O
the	O
latter	O
.	O
The	O
IoU	B-MetricName
replaces	O
the	O
denominator	O
part	O
with	O
the	O
union	O
of	O
predicted	O
and	O
ground	O
-	O
truth	O
procedures	O
.	O
For	O
procedure	O
captioning	O
,	O
we	O
adopt	O
BLEU	B-MetricName
-	O
4	O
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
and	O
METEOR	B-DatasetName
(	O
Banerjee	O
and	O
Lavie	O
,	O
2005	O
)	O
as	O
the	O
metrics	O
to	O
evaluate	O
the	O
performance	O
on	O
the	O
result	O
of	O
captioning	O
based	O
on	O
both	O
extracted	O
and	O
ground	O
-	O
truth	O
procedures	O
.	O

For	O
the	O
procedure	O
extraction	O
module	O
,	O
we	O
follow	O
the	O
method	O
in	O
(	O
Zhou	O
et	O
al	O
,	O
2018a	O
)	O
to	O
use	O
16	O
different	O
kernel	O
sizes	O
for	O
the	O
temporal	O
convolutional	O
layer	O
,	O
i.e.	O
from	O
3	O
to	O
123	O
with	O
the	O
interval	O
step	O
of	O
8	O
,	O
which	O
can	O
cover	O
the	O
different	O
lengths	O
.	O
We	O
also	O
used	O
a	O
max	O
-	O
pooling	O
layer	O
with	O
a	O
kernel	O
of	O
[	O
8	O
,	O
5	O
]	O
after	O
the	O
convolutional	O
layer	O
.	O
We	O
extract	O
at	O
most	O
16	O
procedures	O
for	O
each	O
video	O
,	O
and	O
the	O
maximum	O
caption	O
length	O
of	O
each	O
extracted	O
procedure	O
is	O
50	O
.	O
The	O
hidden	O
size	O
of	O
all	O
recurrent	O
model	O
(	O
LSTM	B-MethodName
)	O
is	O
512	O
and	O
we	O
conduct	O
a	O
dropout	O
for	O
each	O
layer	O
with	O
a	O
probability	O
of	O
0.5	O
.	O
We	O
use	O
two	O
transformer	O
models	O
with	O
2048	B-DatasetName
inner	O
hidden	O
sizes	O
,	O
8	O
heads	O
,	O
and	O
6	O
layers	O
to	O
encode	O
context	O
-	O
aware	O
transcripts	O
and	O
video	O
frame	O
features	O
separately	O
.	O
We	O
adopt	O
an	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
a	O
starting	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.000025	O
and	O
α	B-HyperparameterName
=	O
0.8	O
and	O
β	B-HyperparameterName
=	O
0.999	O
to	O
train	O
the	O
model	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
training	O
is	O
4	O
for	O
each	O
GPU	O
and	O
we	O
use	O
4	O
GPUs	O
to	O
train	O
our	O
model	O
so	O
the	O
overall	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
16	O
.	O
We	O
demonstrate	O
the	O
result	O
of	O
the	O
procedure	O
extraction	O
model	O
by	O
Table	O
1	O
.	O
We	O
compare	O
our	O
model	O
with	O
several	O
baseline	O
methods	O
:	O
(	O
1	O
)	O
SCNN	O
-	O
prop	O
(	O
Shou	O
et	O
al	O
,	O
2016	O
)	O
(	O
2	O
)	O
vsLSTM	O
is	O
an	O
LSTM	B-MethodName
based	O
video	B-TaskName
summarization	I-TaskName
model	O
;	O
(	O
3	O
)	O
Proc	O
-	O
Nets	O
(	O
Zhou	O
et	O
al	O
,	O
2018a	O
)	O
which	O
is	O
the	O
previous	O
SOTA	O
method	O
.	O

For	O
evaluating	O
procedure	O
captioning	O
,	O
we	O
consider	O
two	O
baseline	O
models	O
:	O
(	O
1	O
)	O
Bi	O
-	O
LSTM	B-MethodName
with	O
temporal	B-MethodName
attention	I-MethodName
(	O
Yao	O
et	O
al	O
,	O
2015	O
)	O
(	O
2	O
)	O
an	O
end	O
-	O
to	O
-	O
end	O
transformer	O
based	O
video	O
dense	O
captioning	O
model	O
proposed	O
in	O
(	O
Zhou	O
et	O
al	O
,	O
2018b	O
)	O
.	O
We	O
evaluate	O
the	O
performance	O
of	O
captioning	O
on	O
two	O
different	O
procedures	O
:	O
(	O
1	O
)	O
the	O
ground	O
-	O
truth	O
procedure	O
;	O
(	O
2	O
)	O
the	O
procedure	O
extracted	O
by	O
models	O
.	O
In	O
Table	O
2	O
,	O
we	O
demonstrate	O
that	O
using	O
ground	O
-	O
truth	O
procedures	O
can	O
generate	O
better	O
captions	O
.	O
Additionally	O
,	O
our	O
model	O
achieves	O
the	O
SOTA	O
result	O
on	O
BLEU	B-MetricName
-	O
4	O
and	O
METEOR	B-DatasetName
metrics	O
when	O
using	O
the	O
ground	O
-	O
truth	O
procedures	O
as	O
well	O
as	O
the	O
extracted	O
procedures	O
.	O

We	O
study	O
several	O
captioning	O
results	O
and	O
find	O
that	O
the	O
Caption	O
by	O
Video	O
Model	O
tends	O
to	O
generate	O
general	O
descriptions	O
such	O
as	O
"	O
add	O
...	O
"	O
for	O
all	O
steps	O
.	O
Nonetheless	O
,	O
our	O
model	O
tends	O
to	O
generate	O
various	O
fine	O
-	O
grained	O
captions	O
.	O
Motivated	O
by	O
this	O
,	O
we	O
conduct	O
another	O
experiment	O
to	O
use	O
cherry	O
picked	O
sentence	O
like	O
add	O
the	O
chicken	O
(	O
or	O
beef	O
,	O
carrot	O
,	O
onion	O
,	O
etc	O
.	O
)	O
to	O
the	O
pan	O
and	O
stir	O
or	O
add	O
pepper	O
and	O
salt	O
to	O
the	O
bowl	O
as	O
the	O
captions	O
for	O
all	O
procedures	O
and	O
can	O
still	O
achieve	O
a	O
good	O
result	O
on	O
BLEU	B-MetricName
(	O
4.0	O
+	O
)	O
and	O
METEOR	B-DatasetName
(	O
16.0	O
+	O
)	O
.	O
We	O
find	O
that	O
the	O
distribution	O
of	O
captions	O
in	O
this	O
dataset	O
is	O
biased	O
because	O
there	O
are	O
many	O
similar	O
procedure	O
descriptions	O
even	O
in	O
different	O
recipes	O
.	O

This	O
paper	O
proposes	O
a	O
new	O
method	O
of	O
instance	O
-	O
based	O
domain	B-TaskName
adaptation	I-TaskName
for	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
Sentiment	B-TaskName
analysis	I-TaskName
involves	O
judging	O
a	O
polarity	O
,	O
positive	O
or	O
negative	O
,	O
of	O
a	O
review	O
such	O
as	O
a	O
movie	O
review	O
.	O
This	O
is	O
one	O
of	O
the	O
document	B-TaskName
classification	I-TaskName
tasks	O
and	O
supervised	O
learning	O
can	O
be	O
used	O
to	O
solve	O
it	O
.	O
However	O
,	O
if	O
the	O
domain	O
of	O
the	O
test	O
data	O
is	O
different	O
from	O
the	O
domain	O
for	O
the	O
learning	O
data	O
(	O
for	O
example	O
,	O
book	O
reviews	O
)	O
,	O
the	O
accuracy	B-MetricName
of	O
the	O
classifier	O
obtained	O
through	O
standard	O
supervised	O
learning	O
reduced	O
.	O
This	O
is	O
the	O
problem	O
with	O
domain	O
shift	O
.	O
The	O
solution	O
to	O
this	O
problem	O
is	O
domain	B-TaskName
adaptation	I-TaskName
.	O
Domain	B-TaskName
adaptation	I-TaskName
can	O
be	O
roughly	O
divided	O
into	O
two	O
categories	O
:	O
feature	O
-	O
based	O
and	O
instance	O
-	O
based	O
(	O
Pan	O
and	O
Yang	O
,	O
2010	O
)	O
.	O
In	O
summary	O
,	O
both	O
are	O
weightedlearning	O
methods	O
,	O
but	O
feature	O
-	O
based	O
gives	O
weights	O
to	O
features	O
and	O
instance	O
-	O
based	O
gives	O
weight	O
to	O
instance	O
.	O
Here	O
,	O
we	O
present	O
a	O
new	O
instance	O
-	O
based	O
method	O
.	O
Generally	O
,	O
the	O
instance	O
-	O
based	O
method	O
assumes	O
a	O
covariate	O
shift	O
,	O
and	O
gives	O
the	O
weight	O
based	O
on	O
the	O
probability	O
density	O
ratio	O
between	O
target	O
domain	O
and	O
source	O
domain	O
.	O
However	O
,	O
the	O
computational	O
cost	O
for	O
the	O
instance	O
-	O
based	O
method	O
is	O
too	O
high	O
.	O
The	O
method	O
presented	O
here	O
is	O
simple	O
and	O
its	O
effect	O
is	O
better	O
than	O
methods	O
using	O
a	O
typical	O
probability	O
density	O
ratio	O
.	O
Our	O
method	O
first	O
defines	O
l	O
w	O
,	O
the	O
likelihood	O
of	O
the	O
keyword	O
of	O
the	O
word	O
w	O
using	O
the	O
IDF	O
in	O
the	O
target	O
domain	O
.	O
Using	O
l	O
w	O
,	O
the	O
weight	O
of	O
a	O
review	O
x	O
in	O
the	O
source	O
domain	O
is	O
set	O
as	O
the	O
keyword	O
content	O
rate	O
w	O
x	O
.	O
After	O
that	O
,	O
weighted	O
-	O
learning	O
is	O
performed	O
by	O
giving	O
w	O
x	O
to	O
each	O
document	O
in	O
the	O
source	O
domain	O
x	O
to	O
overcome	O
the	O
domain	O
shift	O
.	O
In	O
the	O
experiment	O
,	O
we	O
used	O
Amazon	O
dataset	O
(	O
Blitzer	O
et	O
al	O
,	O
2007	O
)	O
,	O
and	O
compared	O
our	O
proposed	O
method	O
with	O
two	O
typical	O
instance	O
-	O
based	O
methods	O
:	O
unconstrained	O
least	O
squares	O
importance	O
fitting	O
(	O
uL	O
-	O
SIF	O
)	O
(	O
Yamada	O
et	O
al	O
,	O
2011	O
)	O
using	O
the	O
probability	O
density	O
ratio	O
and	O
the	O
method	O
defining	O
weight	O
through	O
Naive	O
Bayes	O
model	O
(	O
Shinnou	O
and	O
Sasaki	O
,	O
2014	O
)	O
,	O
to	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
method	O
.	O

NONE	O
in	O
Table1	O
is	O
compared	O
with	O
the	O
case	O
weighting	O
method	O
(	O
uLSIF	O
,	O
NB	O
,	O
and	O
our	O
method	O
)	O
.	O
It	O
is	O
clear	O
that	O
NONE	O
has	O
a	O
high	O
positive	O
solution	O
rate	O
.	O
For	O
theae	O
data	O
only	O
,	O
the	O
instance	O
-	O
based	O
method	O
has	O
no	O
effect	O
on	O
domain	B-TaskName
adaptation	I-TaskName
.	O
However	O
,	O
feature	O
-	O
based	O
and	O
instance	O
-	O
based	O
methods	O
are	O
easy	O
to	O
combine	O
.	O
Here	O
,	O
the	O
four	O
domains	O
applied	O
in	O
paper	O
are	O
adapted	O
to	O
B	O
E	O
,	O
D	O
B	O
,	O
E	O
K	O
and	O
K	O
D	O
,	O
and	O
SCL	O
conversion	O
training	O
is	O
used	O
first	O
.	O
The	O
prime	O
vector	O
of	O
the	O
data	O
,	O
then	O
experiment	O
with	O
the	O
weighted	O
-	O
learning	O
in	O
this	O
transformed	O
vector	O
is	O
performed	O
using	O
the	O
proposed	O
method	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
3	O
.	O
The	O
CORAL	O
in	O
Table	O
3	O
is	O
taken	O
from	O
.	O
From	O
Table	O
3	O
,	O
it	O
can	O
be	O
seen	O
that	O
SCL	O
has	O
no	O
effect	O
.	O
After	O
SCL	O
is	O
combined	O
with	O
our	O
method	O
,	O
the	O
accuracy	B-MetricName
is	O
not	O
high	O
enough	O
.	O
However	O
,	O
when	O
SCL	O
is	O
combined	O
with	O
the	O
proposed	O
method	O
,	O
the	O
precision	O
for	O
SCL	O
alone	O
is	O
improved	O
.	O
The	O
positive	O
effect	O
of	O
the	O
combined	O
the	O
instance	O
-	O
based	O
and	O
feature	O
-	O
based	O
methods	O
can	O
be	O
confirmed	O
.	O
There	O
are	O
many	O
ways	O
in	O
useing	O
the	O
feature	O
-	O
based	O
method	O
,	O
in	O
addition	O
to	O
SCL	O
,	O
so	O
it	O
can	O
be	O
improved	O
by	O
combining	O
these	O
techniques	O
with	O
the	O
proposed	O
method	O
.	O
In	O
addition	O
,	O
although	O
the	O
weighted	O
-	O
learning	O
SVM	B-MethodName
is	O
used	O
in	O
this	O
paper	O
,	O
the	O
loss	B-MetricName
value	O
of	O
the	O
loss	B-MetricName
function	O
in	O
the	O
neural	O
network	O
is	O
multiplied	O
by	O
the	O
weight	O
,	O
and	O
it	O
is	O
easier	O
to	O
realize	O
weighted	O
-	O
learning	O
as	O
the	O
loss	B-MetricName
value	O
.	O
There	O
are	O
many	O
options	O
for	O
using	O
the	O
domain	B-TaskName
adaptation	I-TaskName
method	O
on	O
deep	O
learning	O
,	O
and	O
these	O
solutions	O
combined	O
with	O
instance	O
-	O
based	O
are	O
easier	O
to	O
compute	O
.	O
In	O
a	O
simple	O
example	O
,	O
we	O
used	O
the	O
AutoEncoder	B-MethodName
(	O
AE	B-MethodName
)	O
as	O
the	O
feature	O
-	O
based	O
method	O
.	O
Using	O
AE	B-MethodName
,	O
the	O
dimension	O
of	O
the	O
data	O
in	O
the	O
source	O
and	O
target	O
domain	O
was	O
reduced	O
,	O
that	O
is	O
,	O
encoded	O
.	O
In	O
learning	O
and	O
testing	O
,	O
we	O
used	O
the	O
connected	O
data	O
of	O
the	O
orig	O
-	O
inal	O
data	O
and	O
the	O
encoded	O
data	O
instead	O
of	O
the	O
original	O
data	O
.	O
In	O
learning	O
,	O
as	O
described	O
above	O
,	O
the	O
value	O
of	O
the	O
loss	B-MetricName
function	O
was	O
multiplied	O
by	O
the	O
weight	O
obtained	O
by	O
our	O
method	O
and	O
was	O
taken	O
as	O
the	O
loss	B-MetricName
value	O
FIG	O
.	O
1	O
.	O
Only	O
the	O
experiment	O
of	O
B	O
E	O
is	O
performed	O
,	O
and	O
the	O
results	O
in	O
Table	O
4	O
FIG	O
.	O
2	O
were	O
obtained	O
.	O
In	O
addition	O
,	O
in	O
this	O
experiment	O
,	O
neural	O
network	O
learning	O
was	O
ended	O
in	O
50	O
epochs	O
,	O
and	O
the	O
correct	O
rate	O
was	O
the	O
result	O
from	O
evaluating	O
the	O
model	O
obtained	O
by	O
learning	O
data	O
after	O
50	O
epochs	O
.	O
Moreover	O
,	O
the	O
dimension	O
was	O
reduced	O
to	O
200	O
.	O
Every	O
method	O
used	O
the	O
same	O
multi	O
-	O
layer	O
perceptron	O
,	O
which	O
has	O
three	O
layers	O
.	O

This	O
paper	O
describes	O
the	O
system	O
of	O
team	O
LeisureX	O
in	O
the	O
CoNLL	O
2018	O
Shared	O
Task	O
:	O
Multilingual	O
Parsing	O
from	O
Raw	O
Text	O
to	O
Universal	B-DatasetName
Dependencies	I-DatasetName
.	O
Our	O
system	O
predicts	O
the	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tag	O
and	O
dependency	O
tree	O
jointly	O
.	O
For	O
the	O
basic	O
tasks	O
,	O
including	O
tokenization	O
,	O
lemmatization	B-TaskName
and	O
morphology	O
prediction	O
,	O
we	O
employ	O
the	O
official	O
baseline	O
model	O
(	O
UDPipe	O
)	O
.	O
To	O
train	O
the	O
low	O
-	O
resource	O
languages	O
,	O
we	O
adopt	O
a	O
sampling	O
method	O
based	O
on	O
other	O
richresource	O
languages	O
.	O
Our	O
system	O
achieves	O
a	O
macro	O
-	O
average	O
of	O
68.31	O
%	O
LAS	O
F1	B-MetricName
score	I-MetricName
,	O
with	O
an	O
improvement	O
of	O
2.51	O
%	O
compared	O
with	O
the	O
UDPipe	O
.	O

The	O
goal	O
of	O
Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
UD	B-DatasetName
)	O
(	O
Nivre	O
et	O
al	O
,	O
2016	O
;	O
Zeman	O
et	O
al	O
,	O
2017	O
)	O
is	O
to	O
develop	O
multilingual	O
treebank	O
,	O
whose	O
annotations	O
of	O
morphology	O
and	O
syntax	O
are	O
cross	O
-	O
linguistically	O
consistent	O
.	O
In	O
this	O
paper	O
,	O
we	O
describe	O
our	O
system	O
for	O
the	O
CoNLL	O
2018	O
Shared	O
Task	O
:	O
Multilingual	O
Parsing	O
from	O
Raw	O
Text	O
to	O
Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
Zeman	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
we	O
focus	O
only	O
on	O
the	O
subtasks	O
of	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
(	O
POS	O
)	O
tagging	O
and	O
dependency	B-TaskName
parsing	I-TaskName
.	O
For	O
the	O
intermediate	O
steps	O
,	O
including	O
tokenization	O
,	O
lemmatization	B-TaskName
and	O
morphology	O
prediction	O
,	O
we	O
tackle	O
them	O
by	O
the	O
official	O
baseline	O
model	O
(	O
UDPipe	O
)	O
1	O
.	O
Dependency	B-TaskName
parsing	I-TaskName
that	O
aims	O
to	O
predict	O
the	O
existence	O
and	O
type	O
of	O
linguistic	O
dependency	O
relations	O
between	O
words	O
,	O
is	O
a	O
fundamental	O
part	O
in	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
tasks	O
(	O
Li	O
et	O
al	O
,	O
2018c	O
;	O
.	O
Many	O
referential	O
natural	O
language	O
processing	O
studies	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
;	O
Cai	O
et	O
al	O
,	O
2018	O
;	O
Li	O
et	O
al	O
,	O
2018b	O
;	O
Wang	O
et	O
al	O
,	O
2018	O
;	O
Qin	O
et	O
al	O
,	O
2017	O
)	O
can	O
also	O
contribute	O
to	O
the	O
universal	O
dependency	B-TaskName
parsing	I-TaskName
system	O
.	O
Universal	O
dependency	B-TaskName
parsing	I-TaskName
focuses	O
on	O
learning	O
syntactic	O
dependency	O
structure	O
over	O
many	O
typologically	O
different	O
languages	O
,	O
even	O
low	O
-	O
resource	O
languages	O
in	O
a	O
real	O
-	O
world	O
setting	O
.	O
Within	O
the	O
dependency	B-TaskName
parsing	I-TaskName
literature	O
,	O
there	O
are	O
two	O
dominant	O
techniques	O
,	O
graph	O
-	O
based	O
(	O
McDonald	O
et	O
al	O
,	O
2005	O
;	O
Ma	O
and	O
Zhao	O
,	O
2012	O
;	O
Kiperwasser	O
and	O
Goldberg	O
,	O
2016	O
;	O
and	O
transition	O
-	O
based	O
parsing	O
(	O
Nivre	O
,	O
2003	O
;	O
Dyer	O
et	O
al	O
,	O
2015	O
;	O
.	O
Graph	O
-	O
based	O
dependency	O
parsers	O
enjoy	O
the	O
advantage	O
of	O
the	O
global	O
search	O
which	O
learns	O
the	O
scoring	O
functions	O
for	O
all	O
possible	O
parsing	O
trees	O
to	O
find	O
the	O
globally	O
highest	O
scoring	O
one	O
while	O
transition	O
-	O
based	O
dependency	O
parsers	O
build	O
dependency	O
trees	O
from	O
left	O
to	O
right	O
incrementally	O
,	O
which	O
makes	O
the	O
series	O
of	O
multiple	O
choice	O
decisions	O
locally	O
.	O
In	O
our	O
system	O
,	O
we	O
adopt	O
the	O
transition	B-TaskName
-	I-TaskName
based	I-TaskName
dependency	I-TaskName
parsing	I-TaskName
in	O
view	O
of	O
its	O
relatively	O
lower	O
time	O
complexity	O
.	O
Our	O
system	O
implements	O
universal	O
dependency	B-TaskName
parsing	I-TaskName
based	O
on	O
the	O
stack	O
-	O
pointer	O
networks	O
(	O
STACKPTR	O
)	O
parser	O
introduced	O
by	O
(	O
Ma	O
et	O
al	O
,	O
2018	O
)	O
.	O
Furthermore	O
,	O
previous	O
work	O
(	O
Straka	O
et	O
al	O
,	O
2016	O
;	O
Nguyen	O
et	O
al	O
,	O
2017	O
)	O
showed	O
that	O
POS	O
tags	O
are	O
helpful	O
to	O
dependency	B-TaskName
parsing	I-TaskName
.	O
In	O
particular	O
,	O
(	O
Nguyen	O
et	O
al	O
,	O
2017	O
)	O
pointed	O
out	O
that	O
parsing	O
performance	O
could	O
be	O
improved	O
by	O
the	O
merit	O
of	O
accurate	O
POS	O
tags	O
and	O
the	O
context	O
of	O
syntactic	O
parse	O
tree	O
could	O
help	O
resolve	O
POS	O
ambiguities	O
.	O
Therefore	O
,	O
we	O
seek	O
to	O
jointly	O
learn	O
POS	O
tagging	O
and	O
dependency	B-TaskName
parsing	I-TaskName
.	O
As	O
Long	B-MethodName
short	I-MethodName
-	I-MethodName
term	I-MethodName
memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
networks	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
have	O
shown	O
significant	O
representational	O
effectiveness	O
to	O
a	O
wide	O
range	O
of	O
NLP	O
tasks	O
,	O
we	O
leverage	O
bidirectional	O
LSTMs	O
(	O
BiLSTM	B-MethodName
)	O
to	O
learn	O
shared	O
representations	O
for	O
both	O
POS	O
tagging	O
and	O
dependency	B-TaskName
parsing	I-TaskName
.	O
In	O
addition	O
,	O
to	O
train	O
the	O
low	O
-	O
resource	O
languages	O
,	O
we	O
adopt	O
a	O
sampling	O
method	O
based	O
on	O
other	O
richresource	O
languages	O
.	O
In	O
terms	O
of	O
all	O
the	O
above	O
model	O
improvement	O
,	O
compared	O
to	O
the	O
UDPipe	O
baseline	O
,	O
our	O
system	O
achieves	O
a	O
macro	O
-	O
average	O
of	O
68.31	O
%	O
LAS	O
F1	B-MetricName
score	I-MetricName
,	O
with	O
an	O
improvement	O
of	O
2.51	O
%	O
in	O
this	O
task	O
.	O

The	O
universal	O
dependency	B-TaskName
parsing	I-TaskName
component	O
of	O
our	O
system	O
is	O
built	O
on	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approach	O
STACKPTR	O
,	O
which	O
combines	O
pointer	O
networks	O
(	O
Vinyals	O
et	O
al	O
,	O
2015	O
)	O
with	O
an	O
internal	O
stack	O
for	O
tracking	O
the	O
status	O
of	O
depth	O
-	O
first	O
search	O
.	O
It	O
benefits	O
from	O
the	O
global	O
information	O
of	O
the	O
sentence	O
and	O
all	O
previously	O
derived	O
subtree	O
structures	O
,	O
and	O
removes	O
the	O
left	O
-	O
to	O
-	O
right	O
restriction	O
in	O
classical	O
transition	O
-	O
based	O
parsers	O
.	O
The	O
STACKPTR	O
parser	O
mainly	O
consists	O
of	O
two	O
parts	O
:	O
encoder	O
and	O
decoder	O
.	O
The	O
encoder	O
based	O
on	O
BiLSTM	B-MethodName
-	O
CNNs	O
architecture	O
takes	O
the	O
sequence	O
of	O
tokens	O
and	O
their	O
POS	O
tags	O
as	O
input	O
,	O
then	O
encodes	O
it	O
into	O
encoder	O
hidden	O
state	O
s	O
i	O
.	O
The	O
internal	O
stack	O
σ	O
is	O
initialized	O
with	O
dummy	O
ROOT	O
.	O
For	O
decoder	O
(	O
a	O
uni	O
-	O
directional	O
RNN	O
)	O
,	O
it	O
receives	O
the	O
input	O
from	O
last	O
step	O
and	O
outputs	O
decoder	O
hidden	O
state	O
h	O
t	O
.	O
The	O
pointer	O
neural	O
network	O
takes	O
the	O
top	O
element	O
w	O
h	O
in	O
the	O
stack	O
σ	O
at	O
each	O
timestep	O
t	O
as	O
current	O
head	O
to	O
select	O
a	O
specific	O
child	O
w	O
c	O
with	O
biaffine	O
attention	O
mechanism	O
for	O
attention	O
score	O
function	O
in	O
all	O
possible	O
head	O
-	O
dependent	O
pairs	O
.	O
Then	O
the	O
child	O
w	O
c	O
will	O
be	O
pushed	O
onto	O
the	O
stack	O
σ	O
for	O
next	O
step	O
when	O
c	O
=	O
h	O
,	O
otherwise	O
it	O
indicates	O
that	O
all	O
children	O
of	O
the	O
current	O
head	O
h	O
have	O
been	O
selected	O
,	O
therefore	O
the	O
head	O
w	O
h	O
will	O
be	O
popped	O
out	O
of	O
the	O
stack	O
σ	O
.	O
The	O
attention	O
scoring	O
function	O
used	O
is	O
given	O
as	O
follows	O
and	O
the	O
pointer	O
neural	O
network	O
uses	O
a	O
t	O
as	O
pointer	O
to	O
select	O
the	O
child	O
element	O
:	O
e	O
t	O
i	O
=	O
h	O
T	O
t	O
Ws	O
i	O
+	O
U	O
T	O
h	O
t	O
+	O
V	O
T	O
s	O
i	O
+	O
b	O
a	O
t	O
=	O
sof	B-DatasetName
tmax	O
(	O
e	O
t	O
)	O
More	O
specifically	O
,	O
the	O
decoder	O
maintains	O
a	O
list	O
of	O
available	O
words	O
in	O
test	O
phase	O
.	O
For	O
each	O
head	O
h	O
at	O
each	O
decoding	O
step	O
,	O
the	O
selected	O
child	O
will	O
be	O
removed	O
from	O
the	O
list	O
to	O
make	O
sure	O
that	O
it	O
can	O
not	O
be	O
selected	O
as	O
a	O
child	O
of	O
other	O
head	O
words	O
.	O
Given	O
a	O
dependency	O
tree	O
,	O
there	O
may	O
be	O
multiple	O
children	O
for	O
a	O
specific	O
head	O
.	O
This	O
results	O
in	O
more	O
than	O
one	O
valid	O
selection	O
for	O
each	O
time	O
step	O
,	O
which	O
might	O
confuse	O
the	O
decoder	O
.	O
To	O
address	O
this	O
problem	O
,	O
the	O
parser	O
introduces	O
an	O
inside	O
-	O
outside	O
order	O
to	O
utilize	O
second	O
-	O
order	O
sibling	O
information	O
,	O
which	O
has	O
been	O
proven	O
to	O
be	O
an	O
important	O
feature	O
for	O
parsing	O
process	O
(	O
McDonald	O
and	O
Pereira	O
,	O
2006	O
;	O
Koo	O
and	O
Collins	O
,	O
2010	O
)	O
.	O
To	O
utilize	O
the	O
secondorder	O
information	O
,	O
the	O
parser	O
replaces	O
the	O
input	O
of	O
decoder	O
from	O
s	O
i	O
as	O
follows	O
:	O
β	B-HyperparameterName
i	O
=	O
s	O
s	O
s	O
h	O
s	O
i	O
where	O
s	O
and	O
h	O
indicate	O
the	O
sibling	O
and	O
head	O
index	O
of	O
node	O
i	O
,	O
is	O
the	O
element	O
-	O
wise	O
sum	O
operation	O
to	O
ensure	O
no	O
additional	O
model	O
parameters	O
.	O

Our	O
system	O
focuses	O
on	O
three	O
targets	O
:	O
the	O
UPOS	O
tag	O
,	O
dependency	O
arc	O
and	O
dependency	O
relation	O
.	O
Therefore	O
,	O
we	O
rely	O
on	O
the	O
UDPipe	O
model	O
(	O
Straka	O
Treebank	O
Sampling	O
Breton	O
KEB	O
English	O
,	O
Irish	O
Czech	O
PUD	O
Czech	O
PDT	O
English	O
PUD	O
English	O
EWT	O
Faroese	O
OFT	O
Norwegian	O
,	O
English	O
,	O
Danish	O
,	O
Swedish	O
,	O
German	O
,	O
Dutch	O
Finnish	O
PUD	O
Finnish	O
TDT	O
Japanese	O
Modern	O
Japanese	O
GSD	O
Naija	O
NSC	O
English	O
Swedish	O
PUD	O
Swedish	O
Talbanken	O
Thai	O
PUD	O
English	O
,	O
Chinese	O
,	O
Hindi	O
,	O
Vietnamese	O
For	O
treebanks	O
with	O
non	O
-	O
empty	O
training	O
dataset	O
(	O
including	O
treebanks	O
whose	O
training	O
set	O
is	O
very	O
small	O
)	O
,	O
we	O
utilize	O
the	O
baseline	O
model	O
UDPipe	O
trained	O
on	O
corresponding	O
treebank	O
,	O
which	O
has	O
been	O
provided	O
by	O
the	O
organizer	O
.	O
For	O
the	O
remaining	O
nine	O
treebanks	O
without	O
training	O
data	O
,	O
we	O
construct	O
the	O
train	O
dataset	O
by	O
sampling	O
from	O
the	O
other	O
training	O
datasets	O
according	O
to	O
the	O
language	O
similarity	O
inspired	O
by	O
(	O
Zhao	O
et	O
al	O
,	O
2009	O
(	O
Zhao	O
et	O
al	O
,	O
,	O
2010Wang	O
et	O
al	O
,	O
,	O
2016	O
,	O
as	O
detailed	O
in	O
Table	O
1	O
.	O
Our	O
system	O
adopts	O
the	O
hyper	O
-	O
parameter	O
configuration	O
in	O
(	O
Ma	O
et	O
al	O
,	O
2018	O
)	O
,	O
with	O
a	O
few	O
exceptions	O
.	O
We	O
initialize	O
word	O
vectors	O
with	O
50	O
-	O
dimensional	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
,	O
100	O
-	O
dimensional	O
tag	O
embeddings	O
and	O
512	O
-	O
dimensional	O
recurrent	O
states	O
(	O
in	O
each	O
direction	O
)	O
.	O
Our	O
system	O
drops	O
embeddings	O
and	O
hidden	O
states	O
independently	O
with	O
33	O
%	O
probability	O
.	O
We	O
optimize	O
with	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
,	O
setting	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
1e	O
−3	O
and	O
β	B-HyperparameterName
1	O
=	O
β	B-HyperparameterName
2	O
=	O
0.9	O
.	O
Moreover	O
,	O
we	O
train	O
models	O
for	O
up	O
to	O
100	O
epochs	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
32	O
on	O
3	O
NVIDIA	O
GeForce	O
GTX	O
1080Ti	O
GPUs	O
with	O
200	O
to	O
500	O
sentences	O
per	O
second	O
and	O
occupying	O
2	O
to	O
3	O
GB	O
graphic	O
memory	O
each	O
model	O
.	O
A	O
full	O
run	O
over	O
the	O
test	O
datasets	O
on	O
the	O
TIRA	O
virtual	O
machine	O
(	O
Potthast	O
et	O
al	O
,	O
2014	O
)	O
takes	O
about	O
12	O
hours	O
.	O
with	O
absolute	O
gains	O
(	O
1.28	O
-	O
3.08	O
%	O
)	O
on	O
average	O
LAS	O
,	O
UAS	O
,	O
MLAS	O
and	O
CLAS	O
.	O
These	O
results	O
show	O
that	O
our	O
joint	O
model	O
could	O
improve	O
the	O
performance	O
of	O
universal	O
dependency	B-TaskName
parsing	I-TaskName
.	O
Surprisingly	O
,	O
in	O
the	O
case	O
of	O
POS	O
tagging	O
,	O
our	O
joint	O
model	O
obtains	O
lower	O
averaged	O
accuracy	B-MetricName
in	O
both	O
UPOS	O
and	O
XPOS	O
.	O
The	O
possible	O
reason	O
for	O
performance	O
degradation	O
may	O
be	O
that	O
we	O
select	O
all	O
hyper	O
-	O
parameters	O
based	O
on	O
English	O
and	O
do	O
not	O
tune	O
them	O
individually	O
.	O
Furthermore	O
,	O
we	O
also	O
compare	O
the	O
performance	O
of	O
our	O
system	O
with	O
the	O
baseline	O
and	O
the	O
best	O
scorer	O
on	O
big	O
treebanks	O
(	O
Table	O
3	O
)	O
,	O
PUD	O
treebanks	O
(	O
Table	O
4	O
)	O
,	O
low	O
-	O
resource	O
languages	O
(	O
Table	O
5	O
)	O
,	O
respectively	O
.	O

Since	O
our	O
model	O
applies	O
the	O
baseline	O
model	O
for	O
tokenization	O
and	O
segmentation	O
,	O
we	O
show	O
all	O
results	O
of	O
focused	O
metrics	O
on	O
each	O
treebank	O
in	O
Table	O
6	O
.	O
In	O
addition	O
,	O
we	O
compare	O
our	O
model	O
with	O
the	O
best	O
and	O
the	O
average	O
results	O
of	O
top	O
ten	O
models	O
on	O
each	O
treebank	O
,	O
using	O
LAS	O
F1	B-MetricName
for	O
the	O
evaluation	O
metric	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O

Summarization	B-TaskName
is	O
the	O
task	O
of	O
condensing	O
a	O
piece	O
of	O
text	O
to	O
a	O
shorter	O
version	O
that	O
contains	O
the	O
main	O
information	O
from	O
the	O
original	O
.	O
There	O
are	O
two	O
broad	O
approaches	O
to	O
summarization	B-TaskName
:	O
extractive	O
and	O
abstractive	O
.	O
Extractive	O
methods	O
assemble	O
summaries	O
exclusively	O
from	O
passages	O
(	O
usually	O
whole	O
sentences	O
)	O
taken	O
directly	O
from	O
the	O
source	O
text	O
,	O
while	O
abstractive	O
methods	O
may	O
generate	O
novel	O
words	O
and	O
phrases	O
not	O
featured	O
in	O
the	O
source	O
text	O
-	O
as	O
a	O
human	O
-	O
written	O
abstract	O
usually	O
does	O
.	O
The	O
extractive	O
approach	O
is	O
easier	O
,	O
because	O
copying	O
large	O
Original	O
Text	O
(	O
truncated	O
)	O
:	O
lagos	O
,	O
nigeria	O
(	O
cnn	O
)	O
a	O
day	O
after	O
winning	O
nigeria	O
's	O
presidency	O
,	O
muhammadu	O
buhari	O
told	O
cnn	O
's	O
christiane	O
amanpour	O
that	O
he	O
plans	O
to	O
aggressively	O
fight	O
corruption	O
that	O
has	O
long	O
plagued	O
nigeria	O
and	O
go	O
after	O
the	O
root	O
of	O
the	O
nation	O
's	O
unrest	O
.	O
buhari	O
said	O
he	O
'll	O
"	O
rapidly	O
give	O
attention	O
"	O
to	O
curbing	O
violence	O
in	O
the	O
northeast	O
part	O
of	O
nigeria	O
,	O
where	O
the	O
terrorist	O
group	O
boko	O
haram	O
operates	O
.	O
by	O
cooperating	O
with	O
neighboring	O
nations	O
chad	O
,	O
cameroon	O
and	O
niger	O
,	O
he	O
said	O
his	O
administration	O
is	O
confident	O
it	O
will	O
be	O
able	O
to	O
thwart	O
criminals	O
and	O
others	O
contributing	O
to	O
nigeria	O
's	O
instability	O
.	O
for	O
the	O
first	O
time	O
in	O
nigeria	O
's	O
history	O
,	O
the	O
opposition	O
defeated	O
the	O
ruling	O
party	O
in	O
democratic	O
elections	O
.	O
buhari	O
defeated	O
incumbent	O
goodluck	O
jonathan	O
by	O
about	O
2	O
million	O
votes	O
,	O
according	O
to	O
nigeria	O
's	O
independent	O
national	O
electoral	O
commission	O
.	O
the	O
win	O
comes	O
after	O
a	O
long	O
history	O
of	O
military	O
rule	O
,	O
coups	O
and	O
botched	O
attempts	O
at	O
democracy	O
in	O
africa	O
's	O
most	O
populous	O
nation	O
.	O
Baseline	O
Seq2Seq	B-MethodName
+	O
Attention	O
:	O
UNK	O
UNK	O
says	O
his	O
administration	O
is	O
confident	O
it	O
will	O
be	O
able	O
to	O
destabilize	O
nigeria	O
's	O
economy	O
.	O
UNK	O
says	O
his	O
administration	O
is	O
confident	O
it	O
will	O
be	O
able	O
to	O
thwart	O
criminals	O
and	O
other	O
nigerians	O
.	O
he	O
says	O
the	O
country	O
has	O
long	O
nigeria	O
and	O
nigeria	O
's	O
economy	O
.	O
Pointer	O
-	O
Gen	O
:	O
muhammadu	O
buhari	O
says	O
he	O
plans	O
to	O
aggressively	O
fight	O
corruption	O
in	O
the	O
northeast	O
part	O
of	O
nigeria	O
.	O
he	O
says	O
he	O
'll	O
"	O
rapidly	O
give	O
attention	O
"	O
to	O
curbing	O
violence	O
in	O
the	O
northeast	O
part	O
of	O
nigeria	O
.	O
he	O
says	O
his	O
administration	O
is	O
confident	O
it	O
will	O
be	O
able	O
to	O
thwart	O
criminals	O
.	O
Pointer	O
-	O
Gen	O
+	O
Coverage	O
:	O
muhammadu	O
buhari	O
says	O
he	O
plans	O
to	O
aggressively	O
fight	O
corruption	O
that	O
has	O
long	O
plagued	O
nigeria	O
.	O
he	O
says	O
his	O
administration	O
is	O
confident	O
it	O
will	O
be	O
able	O
to	O
thwart	O
criminals	O
.	O
the	O
win	O
comes	O
after	O
a	O
long	O
history	O
of	O
military	O
rule	O
,	O
coups	O
and	O
botched	O
attempts	O
at	O
democracy	O
in	O
africa	O
's	O
most	O
populous	O
nation	O
.	O
Figure	O
1	O
:	O
Comparison	O
of	O
output	O
of	O
3	O
abstractive	O
summarization	B-TaskName
models	O
on	O
a	O
news	O
article	O
.	O
The	O
baseline	O
model	O
makes	O
factual	O
errors	O
,	O
a	O
nonsensical	O
sentence	O
and	O
struggles	O
with	O
OOV	O
words	O
muhammadu	O
buhari	O
.	O
The	O
pointer	O
-	O
generator	O
model	O
is	O
accurate	O
but	O
repeats	O
itself	O
.	O
Coverage	O
eliminates	O
repetition	O
.	O
The	O
final	O
summary	O
is	O
composed	O
from	O
several	O
fragments	O
.	O
chunks	O
of	O
text	O
from	O
the	O
source	O
document	O
ensures	O
baseline	O
levels	O
of	O
grammaticality	O
and	O
accuracy	B-MetricName
.	O
On	O
the	O
other	O
hand	O
,	O
sophisticated	O
abilities	O
that	O
are	O
crucial	O
to	O
high	O
-	O
quality	O
summarization	B-TaskName
,	O
such	O
as	O
paraphrasing	O
,	O
generalization	O
,	O
or	O
the	O
incorporation	O
of	O
real	O
-	O
world	O
knowledge	O
,	O
are	O
possible	O
only	O
in	O
an	O
abstractive	O
framework	O
(	O
see	O
Figure	O
5	O
)	O
.	O
Due	O
to	O
the	O
difficulty	O
of	O
abstractive	O
summarization	B-TaskName
,	O
the	O
great	O
majority	O
of	O
past	O
work	O
has	O
been	O
extractive	O
(	O
Kupiec	O
et	O
al	O
,	O
1995	O
;	O
Paice	O
,	O
1990	O
;	O
Saggion	O
and	O
Poibeau	O
,	O
2013	O
)	O
.	O
However	O
,	O
the	O
recent	O
success	O
of	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
(	O
Sutskever	O
...	O
et	O
al	O
,	O
2014	O
)	O
,	O
in	O
which	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
both	O
read	O
and	O
freely	O
generate	O
text	O
,	O
has	O
made	O
abstractive	O
summarization	B-TaskName
viable	O
Rush	O
et	O
al	O
,	O
2015	O
;	O
Zeng	O
et	O
al	O
,	O
2016	O
)	O
.	O
Though	O
these	O
systems	O
are	O
promising	O
,	O
they	O
exhibit	O
undesirable	O
behavior	O
such	O
as	O
inaccurately	O
reproducing	O
factual	O
details	O
,	O
an	O
inability	O
to	O
deal	O
with	O
out	O
-	O
of	O
-	O
vocabulary	O
(	O
OOV	O
)	O
words	O
,	O
and	O
repeating	O
themselves	O
(	O
see	O
Figure	O
1	O
)	O
.	O
In	O
this	O
paper	O
we	O
present	O
an	O
architecture	O
that	O
addresses	O
these	O
three	O
issues	O
in	O
the	O
context	O
of	O
multi	O
-	O
sentence	O
summaries	O
.	O
While	O
most	O
recent	O
abstractive	O
work	O
has	O
focused	O
on	O
headline	O
generation	O
tasks	O
(	O
reducing	O
one	O
or	O
two	O
sentences	O
to	O
a	O
single	O
headline	O
)	O
,	O
we	O
believe	O
that	O
longer	O
-	O
text	B-TaskName
summarization	I-TaskName
is	O
both	O
more	O
challenging	O
(	O
requiring	O
higher	O
levels	O
of	O
abstraction	O
while	O
avoiding	O
repetition	O
)	O
and	O
ultimately	O
more	O
useful	O
.	O
Therefore	O
we	O
apply	O
our	O
model	O
to	O
the	O
recently	O
-	O
introduced	O
CNN/	O
Daily	O
Mail	O
dataset	O
(	O
Hermann	O
et	O
al	O
,	O
2015	O
;	O
,	O
which	O
contains	O
news	O
articles	O
(	O
39	O
sentences	O
on	O
average	O
)	O
paired	O
with	O
multi	O
-	O
sentence	O
summaries	O
,	O
and	O
show	O
that	O
we	O
outperform	O
the	O
stateof	O
-	O
the	O
-	O
art	O
abstractive	O
system	O
by	O
at	O
least	O
2	O
ROUGE	O
points	O
.	O
Our	O
hybrid	O
pointer	O
-	O
generator	O
network	O
facilitates	O
copying	O
words	O
from	O
the	O
source	O
text	O
via	O
pointing	O
(	O
Vinyals	O
et	O
al	O
,	O
2015	O
)	O
,	O
which	O
improves	O
accuracy	B-MetricName
and	O
handling	O
of	O
OOV	O
words	O
,	O
while	O
retaining	O
the	O
ability	O
to	O
generate	O
new	O
words	O
.	O
The	O
network	O
,	O
which	O
can	O
be	O
viewed	O
as	O
a	O
balance	O
between	O
extractive	O
and	O
abstractive	O
approaches	O
,	O
is	O
similar	O
to	O
Gu	O
et	O
al	O
's	O
(	O
2016	O
)	O
CopyNet	O
and	O
Miao	O
and	O
Blunsom	O
's	O
(	O
2016	O
)	O
Forced	O
-	O
Attention	O
Sentence	B-DatasetName
Compression	I-DatasetName
,	O
that	O
were	O
applied	O
to	O
short	O
-	O
text	B-TaskName
summarization	I-TaskName
.	O
We	O
propose	O
a	O
novel	O
variant	O
of	O
the	O
coverage	O
vector	O
(	O
Tu	O
et	O
al	O
,	O
2016	O
)	O
from	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName
,	O
which	O
we	O
use	O
to	O
track	O
and	O
control	O
coverage	O
of	O
the	O
source	O
document	O
.	O
We	O
show	O
that	O
coverage	O
is	O
remarkably	O
effective	O
for	O
eliminating	O
repetition	O
.	O

Our	O
baseline	O
model	O
is	O
similar	O
to	O
that	O
of	O
,	O
and	O
is	O
depicted	O
in	O
Figure	O
2	O
.	O
The	O
tokens	O
of	O
the	O
article	O
w	O
i	O
are	O
fed	O
one	O
-	O
by	O
-	O
one	O
into	O
the	O
encoder	O
(	O
a	O
single	O
-	O
layer	O
bidirectional	B-MethodName
LSTM	I-MethodName
)	O
,	O
producing	O
a	O
sequence	O
of	O
encoder	O
hidden	O
states	O
h	O
i	O
.	O
On	O
each	O
step	O
t	O
,	O
the	O
decoder	O
(	O
a	O
single	O
-	O
layer	O
unidirectional	O
LSTM	B-MethodName
)	O
receives	O
the	O
word	O
embedding	O
of	O
the	O
previous	O
word	O
(	O
while	O
training	O
,	O
this	O
is	O
the	O
previous	O
word	O
of	O
the	O
reference	O
summary	O
;	O
at	O
test	O
time	O
it	O
is	O
the	O
previous	O
word	O
emitted	O
by	O
the	O
decoder	O
)	O
,	O
and	O
has	O
decoder	O
state	O
s	O
t	O
.	O
The	O
attention	O
distribution	O
a	O
t	O
is	O
calculated	O
as	O
in	O
Bahdanau	O
et	O
al	O
(	O
2015	O
)	O
:	O
e	O
t	O
i	O
=	O
v	O
T	O
tanh	O
(	O
W	O
h	O
h	O
i	O
+	O
W	O
s	O
s	O
t	O
+	O
b	O
attn	O
)	O
(	O
1	O
)	O
a	O
t	O
=	O
softmax	B-MethodName
(	O
e	O
t	O
)	O
(	O
2	O
)	O
where	O
v	O
,	O
W	O
h	O
,	O
W	O
s	O
and	O
b	O
attn	O
are	O
learnable	O
parameters	O
.	O
The	O
attention	O
distribution	O
can	O
be	O
viewed	O
as	O
For	O
each	O
decoder	O
timestep	O
a	O
generation	O
probability	O
p	O
gen	O
[	O
0	B-DatasetName
,	O
1	O
]	O
is	O
calculated	O
,	O
which	O
weights	O
the	O
probability	O
of	O
generating	O
words	O
from	O
the	O
vocabulary	O
,	O
versus	O
copying	O
words	O
from	O
the	O
source	O
text	O
.	O
The	O
vocabulary	O
distribution	O
and	O
the	O
attention	O
distribution	O
are	O
weighted	O
and	O
summed	O
to	O
obtain	O
the	O
final	O
distribution	O
,	O
from	O
which	O
we	O
make	O
our	O
prediction	O
.	O
Note	O
that	O
out	O
-	O
of	O
-	O
vocabulary	O
article	O
words	O
such	O
as	O
2	O
-	O
0	B-DatasetName
are	O
included	O
in	O
the	O
final	O
distribution	O
.	O
Best	O
viewed	O
in	O
color	O
.	O
a	O
probability	O
distribution	O
over	O
the	O
source	O
words	O
,	O
that	O
tells	O
the	O
decoder	O
where	O
to	O
look	O
to	O
produce	O
the	O
next	O
word	O
.	O
Next	O
,	O
the	O
attention	O
distribution	O
is	O
used	O
to	O
produce	O
a	O
weighted	O
sum	O
of	O
the	O
encoder	O
hidden	O
states	O
,	O
known	O
as	O
the	O
context	O
vector	O
h	O
*	O
t	O
:	O
h	O
*	O
t	O
=	O
∑	O
i	O
a	O
t	O
i	O
h	O
i	O
(	O
3	O
)	O
The	O
context	O
vector	O
,	O
which	O
can	O
be	O
seen	O
as	O
a	O
fixedsize	O
representation	O
of	O
what	O
has	O
been	O
read	O
from	O
the	O
source	O
for	O
this	O
step	O
,	O
is	O
concatenated	O
with	O
the	O
decoder	O
state	O
s	O
t	O
and	O
fed	O
through	O
two	O
linear	O
layers	O
to	O
produce	O
the	O
vocabulary	O
distribution	O
P	O
vocab	O
:	O
P	O
vocab	O
=	O
softmax	B-MethodName
(	O
V	O
(	O
V	O
[	O
s	O
t	O
,	O
h	O
*	O
t	O
]	O
+	O
b	O
)	O
+	O
b	O
)	O
(	O
4	O
)	O
where	O
V	O
,	O
V	O
,	O
b	O
and	O
b	O
are	O
learnable	O
parameters	O
.	O
P	O
vocab	O
is	O
a	O
probability	O
distribution	O
over	O
all	O
words	O
in	O
the	O
vocabulary	O
,	O
and	O
provides	O
us	O
with	O
our	O
final	O
distribution	O
from	O
which	O
to	O
predict	O
words	O
w	O
:	O
P	O
(	O
w	O
)	O
=	O
P	O
vocab	O
(	O
w	O
)	O
(	O
5	O
)	O
During	O
training	O
,	O
the	O
loss	B-MetricName
for	O
timestep	O
t	O
is	O
the	O
negative	O
log	O
likelihood	O
of	O
the	O
target	O
word	O
w	O
*	O
t	O
for	O
that	O
timestep	O
:	O
loss	B-MetricName
t	O
=	O
−	O
log	O
P	O
(	O
w	O
*	O
t	O
)	O
(	O
6	O
)	O
and	O
the	O
overall	O
loss	B-MetricName
for	O
the	O
whole	O
sequence	O
is	O
:	O
loss	B-MetricName
=	O
1	O
T	O
∑	O
T	O
t=0	O
loss	B-MetricName
t	O
(	O
7	O
)	O

Our	O
pointer	O
-	O
generator	O
network	O
is	O
a	O
hybrid	O
between	O
our	O
baseline	O
and	O
a	O
pointer	B-MethodName
network	I-MethodName
(	O
Vinyals	O
et	O
al	O
,	O
2015	O
)	O
,	O
as	O
it	O
allows	O
both	O
copying	O
words	O
via	O
pointing	O
,	O
and	O
generating	O
words	O
from	O
a	O
fixed	O
vocabulary	O
.	O
In	O
the	O
pointer	O
-	O
generator	O
model	O
(	O
depicted	O
in	O
Figure	O
3	O
)	O
the	O
attention	O
distribution	O
a	O
t	O
and	O
context	O
vector	O
h	O
*	O
t	O
are	O
calculated	O
as	O
in	O
section	O
2.1	O
.	O
In	O
addition	O
,	O
the	O
generation	O
probability	O
p	O
gen	O
[	O
0	B-DatasetName
,	O
1	O
]	O
for	O
timestep	O
t	O
is	O
calculated	O
from	O
the	O
context	O
vector	O
h	O
*	O
t	O
,	O
the	O
decoder	O
state	O
s	O
t	O
and	O
the	O
decoder	O
input	O
x	O
t	O
:	O
p	O
gen	O
=	O
σ	O
(	O
w	O
T	O
h	O
*	O
h	O
*	O
t	O
+	O
w	O
T	O
s	O
s	O
t	O
+	O
w	O
T	O
x	O
x	O
t	O
+	O
b	O
ptr	B-DatasetName
)	O
(	O
8	O
)	O
where	O
vectors	O
w	O
h	O
*	O
,	O
w	O
s	O
,	O
w	O
x	O
and	O
scalar	O
b	O
ptr	B-DatasetName
are	O
learnable	O
parameters	O
and	O
σ	O
is	O
the	O
sigmoid	O
function	O
.	O
Next	O
,	O
p	O
gen	O
is	O
used	O
as	O
a	O
soft	O
switch	O
to	O
choose	O
between	O
generating	O
a	O
word	O
from	O
the	O
vocabulary	O
by	O
sampling	O
from	O
P	O
vocab	O
,	O
or	O
copying	O
a	O
word	O
from	O
the	O
input	O
sequence	O
by	O
sampling	O
from	O
the	O
attention	O
distribution	O
a	O
t	O
.	O
For	O
each	O
document	O
let	O
the	O
extended	O
vocabulary	O
denote	O
the	O
union	O
of	O
the	O
vocabulary	O
,	O
and	O
all	O
words	O
appearing	O
in	O
the	O
source	O
document	O
.	O
We	O
obtain	O
the	O
following	O
probability	O
distribution	O
over	O
the	O
extended	O
vocabulary	O
:	O
P	O
(	O
w	O
)	O
=	O
p	O
gen	O
P	O
vocab	O
(	O
w	O
)	O
+	O
(	O
1	O
−	O
p	O
gen	O
)	O
∑	O
i	O
:	O
w	O
i	O
=	O
w	O
a	O
t	O
i	O
(	O
9	O
)	O
Note	O
that	O
if	O
w	O
is	O
an	O
out	O
-	O
of	O
-	O
vocabulary	O
(	O
OOV	O
)	O
word	O
,	O
then	O
P	O
vocab	O
(	O
w	O
)	O
is	O
zero	O
;	O
similarly	O
if	O
w	O
does	O
not	O
appear	O
in	O
the	O
source	O
document	O
,	O
then	O
∑	O
i	O
:	O
w	O
i	O
=	O
w	O
a	O
t	O
i	O
is	O
zero	O
.	O
The	O
ability	O
to	O
produce	O
OOV	O
words	O
is	O
one	O
of	O
the	O
primary	O
advantages	O
of	O
pointer	O
-	O
generator	O
models	O
;	O
by	O
contrast	O
models	O
such	O
as	O
our	O
baseline	O
are	O
restricted	O
to	O
their	O
pre	O
-	O
set	O
vocabulary	O
.	O
The	O
loss	B-MetricName
function	O
is	O
as	O
described	O
in	O
equations	O
(	O
6	O
)	O
and	O
(	O
7	O
)	O
,	O
but	O
with	O
respect	O
to	O
our	O
modified	O
probability	O
distribution	O
P	O
(	O
w	O
)	O
given	O
in	O
equation	O
(	O
9	O
)	O
.	O

Repetition	O
is	O
a	O
common	O
problem	O
for	O
sequenceto	O
-	O
sequence	O
models	O
(	O
Tu	O
et	O
al	O
,	O
2016	O
;	O
,	O
and	O
is	O
especially	O
pronounced	O
when	O
generating	O
multi	O
-	O
sentence	O
text	O
(	O
see	O
Figure	O
1	O
)	O
.	O
We	O
adapt	O
the	O
coverage	O
model	O
of	O
Tu	O
et	O
al	O
(	O
2016	O
)	O
to	O
solve	O
the	O
problem	O
.	O
In	O
our	O
coverage	O
model	O
,	O
we	O
maintain	O
a	O
coverage	O
vector	O
c	O
t	O
,	O
which	O
is	O
the	O
sum	O
of	O
attention	O
distributions	O
over	O
all	O
previous	O
decoder	O
timesteps	O
:	O
c	O
t	O
=	O
∑	O
t−1	O
t	O
=	O
0	B-DatasetName
a	O
t	O
(	O
10	O
)	O
Intuitively	O
,	O
c	O
t	O
is	O
a	O
(	O
unnormalized	O
)	O
distribution	O
over	O
the	O
source	O
document	O
words	O
that	O
represents	O
the	O
degree	O
of	O
coverage	O
that	O
those	O
words	O
have	O
received	O
from	O
the	O
attention	O
mechanism	O
so	O
far	O
.	O
Note	O
that	O
c	O
0	B-DatasetName
is	O
a	O
zero	O
vector	O
,	O
because	O
on	O
the	O
first	O
timestep	O
,	O
none	O
of	O
the	O
source	O
document	O
has	O
been	O
covered	O
.	O
The	O
coverage	O
vector	O
is	O
used	O
as	O
extra	O
input	O
to	O
the	O
attention	O
mechanism	O
,	O
changing	O
equation	O
(	O
1	O
)	O
to	O
:	O
e	O
t	O
i	O
=	O
v	O
T	O
tanh	O
(	O
W	O
h	O
h	O
i	O
+	O
W	O
s	O
s	O
t	O
+	O
w	O
c	O
c	O
t	O
i	O
+	O
b	O
attn	O
)	O
(	O
11	O
)	O
where	O
w	O
c	O
is	O
a	O
learnable	O
parameter	O
vector	O
of	O
same	O
length	O
as	O
v.	O
This	O
ensures	O
that	O
the	O
attention	O
mechanism	O
's	O
current	O
decision	O
(	O
choosing	O
where	O
to	O
attend	O
next	O
)	O
is	O
informed	O
by	O
a	O
reminder	O
of	O
its	O
previous	O
decisions	O
(	O
summarized	O
in	O
c	O
t	O
)	O
.	O
This	O
should	O
make	O
it	O
easier	O
for	O
the	O
attention	O
mechanism	O
to	O
avoid	O
repeatedly	O
attending	O
to	O
the	O
same	O
locations	O
,	O
and	O
thus	O
avoid	O
generating	O
repetitive	O
text	O
.	O
We	O
find	O
it	O
necessary	O
(	O
see	O
section	O
5	O
)	O
to	O
additionally	O
define	O
a	O
coverage	O
loss	B-MetricName
to	O
penalize	O
repeatedly	O
attending	O
to	O
the	O
same	O
locations	O
:	O
covloss	O
t	O
=	O
∑	O
i	O
min	O
(	O
a	O
t	O
i	O
,	O
c	O
t	O
i	O
)	O
(	O
12	O
)	O
Note	O
that	O
the	O
coverage	O
loss	B-MetricName
is	O
bounded	O
;	O
in	O
particular	O
covloss	O
t	O
≤	O
∑	O
i	O
a	O
t	O
i	O
=	O
1	O
.	O
Equation	O
(	O
12	O
)	O
differs	O
from	O
the	O
coverage	O
loss	B-MetricName
used	O
in	O
Machine	B-TaskName
Translation	I-TaskName
.	O
In	O
MT	O
,	O
we	O
assume	O
that	O
there	O
should	O
be	O
a	O
roughly	O
oneto	O
-	O
one	O
translation	O
ratio	O
;	O
accordingly	O
the	O
final	O
coverage	O
vector	O
is	O
penalized	O
if	O
it	O
is	O
more	O
or	O
less	O
than	O
1	O
.	O
Our	O
loss	B-MetricName
function	O
is	O
more	O
flexible	O
:	O
because	O
summarization	B-TaskName
should	O
not	O
require	O
uniform	O
coverage	O
,	O
we	O
only	O
penalize	O
the	O
overlap	O
between	O
each	O
attention	O
distribution	O
and	O
the	O
coverage	O
so	O
far	O
-	O
preventing	O
repeated	O
attention	O
.	O
Finally	O
,	O
the	O
coverage	O
loss	B-MetricName
,	O
reweighted	O
by	O
some	O
hyperparameter	O
λ	O
,	O
is	O
added	O
to	O
the	O
primary	O
loss	B-MetricName
function	O
to	O
yield	O
a	O
new	O
composite	O
loss	B-MetricName
function	O
:	O
loss	B-MetricName
t	O
=	O
−	O
log	O
P	O
(	O
w	O
*	O
t	O
)	O
+	O
λ	O
∑	O
i	O
min	O
(	O
a	O
t	O
i	O
,	O
c	O
t	O
i	O
)	O
(	O
13	O
)	O
3	O
Related	O
Work	O
Neural	O
abstractive	O
summarization	B-TaskName
.	O
Rush	O
et	O
al	O
(	O
2015	O
)	O
were	O
the	O
first	O
to	O
apply	O
modern	O
neural	O
networks	O
to	O
abstractive	B-TaskName
text	I-TaskName
summarization	I-TaskName
,	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
DUC	O
-	O
2004	O
and	O
Gigaword	O
,	O
two	O
sentence	O
-	O
level	O
summarization	B-TaskName
datasets	O
.	O
Their	O
approach	O
,	O
which	O
is	O
centered	O
on	O
the	O
attention	O
mechanism	O
,	O
has	O
been	O
augmented	O
with	O
recurrent	O
decoders	O
,	O
Abstract	O
Meaning	O
Representations	O
(	O
Takase	O
et	O
al	O
,	O
2016	O
)	O
,	O
hierarchical	O
networks	O
,	O
variational	O
autoencoders	B-MethodName
(	O
Miao	O
and	O
Blunsom	O
,	O
2016	O
)	O
,	O
and	O
direct	O
optimization	O
of	O
the	O
performance	O
metric	O
(	O
Ranzato	O
et	O
al	O
,	O
2016	O
)	O
,	O
further	O
improving	O
performance	O
on	O
those	O
datasets	O
.	O
However	O
,	O
large	O
-	O
scale	O
datasets	O
for	O
summarization	B-TaskName
of	O
longer	O
text	O
are	O
rare	O
.	O
adapted	O
the	O
DeepMind	O
question	O
-	O
answering	O
dataset	O
(	O
Hermann	O
et	O
al	O
,	O
2015	O
)	O
for	O
summarization	B-TaskName
,	O
resulting	O
in	O
the	O
CNN	B-DatasetName
/	I-DatasetName
Daily	I-DatasetName
Mail	I-DatasetName
dataset	O
,	O
and	O
provided	O
the	O
first	O
abstractive	O
baselines	O
.	O
The	O
same	O
authors	O
then	O
published	O
a	O
neural	O
extractive	O
approach	O
(	O
Nallapati	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
uses	O
hierarchical	O
RNNs	O
to	O
select	O
sentences	O
,	O
and	O
found	O
that	O
it	O
significantly	O
outperformed	O
their	O
abstractive	O
result	O
with	O
respect	O
to	O
the	O
ROUGE	O
metric	O
.	O
To	O
our	O
knowledge	O
,	O
these	O
are	O
the	O
only	O
two	O
published	O
results	O
on	O
the	O
full	O
dataset	O
.	O
Prior	O
to	O
modern	O
neural	O
methods	O
,	O
abstractive	O
summarization	B-TaskName
received	O
less	O
attention	O
than	O
extractive	B-TaskName
summarization	I-TaskName
,	O
but	O
Jing	O
(	O
2000	O
)	O
explored	O
cutting	O
unimportant	O
parts	O
of	O
sentences	O
to	O
create	O
summaries	O
,	O
and	O
Cheung	O
and	O
Penn	O
(	O
2014	O
)	O
explore	O
sentence	B-TaskName
fusion	I-TaskName
using	O
dependency	O
trees	O
.	O
Pointer	O
-	O
generator	O
networks	O
.	O
The	O
pointer	B-MethodName
network	I-MethodName
(	O
Vinyals	O
et	O
al	O
,	O
2015	O
)	O
is	O
a	O
sequence	O
-	O
tosequence	O
model	O
that	O
uses	O
the	O
soft	O
attention	O
distribution	O
of	O
Bahdanau	O
et	O
al	O
(	O
2015	O
)	O
to	O
produce	O
an	O
output	O
sequence	O
consisting	O
of	O
elements	O
from	O
the	O
input	O
sequence	O
.	O
The	O
pointer	B-MethodName
network	I-MethodName
has	O
been	O
used	O
to	O
create	O
hybrid	O
approaches	O
for	O
NMT	O
(	O
Gulcehre	O
et	O
al	O
,	O
2016	O
)	O
,	O
language	O
modeling	O
(	O
Merity	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
summarization	B-TaskName
(	O
Gu	O
et	O
al	O
,	O
2016	O
;	O
Gulcehre	O
et	O
al	O
,	O
2016	O
;	O
Miao	O
and	O
Blunsom	O
,	O
2016	O
;	O
Zeng	O
et	O
al	O
,	O
2016	O
)	O
.	O
Our	O
approach	O
is	O
close	O
to	O
the	O
Forced	O
-	O
Attention	O
Sentence	B-DatasetName
Compression	I-DatasetName
model	O
of	O
Miao	O
and	O
Blunsom	O
(	O
2016	O
)	O
and	O
the	O
CopyNet	O
model	O
of	O
Gu	O
et	O
al	O
(	O
2016	O
)	O
,	O
with	O
some	O
small	O
differences	O
:	O
(	O
i	O
)	O
We	O
calculate	O
an	O
explicit	O
switch	O
probability	O
p	O
gen	O
,	O
whereas	O
Gu	O
et	O
al	O
induce	O
competition	O
through	O
a	O
shared	O
softmax	B-MethodName
function	O
.	O
(	O
ii	O
)	O
We	O
recycle	O
the	O
attention	O
distribution	O
to	O
serve	O
as	O
the	O
copy	O
distribution	O
,	O
but	O
Gu	O
et	O
al	O
use	O
two	O
separate	O
distributions	O
.	O
(	O
iii	O
)	O
When	O
a	O
word	O
appears	O
multiple	O
times	O
in	O
the	O
source	O
text	O
,	O
we	O
sum	O
probability	O
mass	O
from	O
all	O
corresponding	O
parts	O
of	O
the	O
attention	O
distribution	O
,	O
whereas	O
Miao	O
and	O
Blunsom	O
do	O
not	O
.	O
Our	O
reasoning	O
is	O
that	O
(	O
i	O
)	O
calculating	O
an	O
explicit	O
p	O
gen	O
usefully	O
enables	O
us	O
to	O
raise	O
or	O
lower	O
the	O
probability	O
of	O
all	O
generated	O
words	O
or	O
all	O
copy	O
words	O
at	O
once	O
,	O
rather	O
than	O
individually	O
,	O
(	O
ii	O
)	O
the	O
two	O
distributions	O
serve	O
such	O
similar	O
purposes	O
that	O
we	O
find	O
our	O
simpler	O
approach	O
suffices	O
,	O
and	O
(	O
iii	O
)	O
we	O
observe	O
that	O
the	O
pointer	O
mechanism	O
often	O
copies	O
a	O
word	O
while	O
attending	O
to	O
multiple	O
occurrences	O
of	O
it	O
in	O
the	O
source	O
text	O
.	O
Our	O
approach	O
is	O
considerably	O
different	O
from	O
that	O
of	O
Gulcehre	O
et	O
al	O
(	O
2016	O
)	O
and	O
.	O
Those	O
works	O
train	O
their	O
pointer	O
components	O
to	O
activate	O
only	O
for	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
or	O
named	O
entities	O
(	O
whereas	O
we	O
allow	O
our	O
model	O
to	O
freely	O
learn	O
when	O
to	O
use	O
the	O
pointer	O
)	O
,	O
and	O
they	O
do	O
not	O
mix	O
the	O
probabilities	O
from	O
the	O
copy	O
distribution	O
and	O
the	O
vocabulary	O
distribution	O
.	O
We	O
believe	O
the	O
mixture	O
approach	O
described	O
here	O
is	O
better	O
for	O
abstractive	O
summarization	B-TaskName
-	O
in	O
section	O
6	O
we	O
show	O
that	O
the	O
copy	O
mechanism	O
is	O
vital	O
for	O
accurately	O
reproducing	O
rare	O
but	O
in	O
-	O
vocabulary	O
words	O
,	O
and	O
in	O
section	O
7.2	O
we	O
observe	O
that	O
the	O
mixture	O
model	O
enables	O
the	O
language	O
model	O
and	O
copy	O
mechanism	O
to	O
work	O
together	O
to	O
perform	O
abstractive	O
copying	O
.	O
Coverage	O
.	O
Originating	O
from	O
Statistical	O
Machine	B-TaskName
Translation	I-TaskName
(	O
Koehn	O
,	O
2009	O
)	O
,	O
coverage	O
was	O
adapted	O
for	O
NMT	O
by	O
Tu	O
et	O
al	O
(	O
2016	O
)	O
and	O
,	O
who	O
both	O
use	O
a	O
GRU	B-MethodName
to	O
update	O
the	O
coverage	O
vector	O
each	O
step	O
.	O
We	O
find	O
that	O
a	O
simpler	O
approach	O
-	O
summing	O
the	O
attention	O
distributions	O
to	O
obtain	O
the	O
coverage	O
vector	O
-	O
suffices	O
.	O
In	O
this	O
respect	O
our	O
approach	O
is	O
similar	O
to	O
Xu	O
et	O
al	O
(	O
2015	O
)	O
,	O
who	O
apply	O
a	O
coverage	O
-	O
like	O
method	O
to	O
image	O
cap	O
-	O
tioning	O
,	O
and	O
Chen	O
et	O
al	O
(	O
2016	O
)	O
,	O
who	O
also	O
incorporate	O
a	O
coverage	O
mechanism	O
(	O
which	O
they	O
call	O
'	O
distraction	O
'	O
)	O
as	O
described	O
in	O
equation	O
(	O
11	O
)	O
into	O
neural	O
summarization	B-TaskName
of	O
longer	O
text	O
.	O
Temporal	B-MethodName
attention	I-MethodName
is	O
a	O
related	O
technique	O
that	O
has	O
been	O
applied	O
to	O
NMT	O
(	O
Sankaran	O
et	O
al	O
,	O
2016	O
)	O
and	O
summarization	B-TaskName
.	O
In	O
this	O
approach	O
,	O
each	O
attention	O
distribution	O
is	O
divided	O
by	O
the	O
sum	O
of	O
the	O
previous	O
,	O
which	O
effectively	O
dampens	O
repeated	O
attention	O
.	O
We	O
tried	O
this	O
method	O
but	O
found	O
it	O
too	O
destructive	O
,	O
distorting	O
the	O
signal	O
from	O
the	O
attention	O
mechanism	O
and	O
reducing	O
performance	O
.	O
We	O
hypothesize	O
that	O
an	O
early	O
intervention	O
method	O
such	O
as	O
coverage	O
is	O
preferable	O
to	O
a	O
post	O
hoc	O
method	O
such	O
as	O
temporal	B-MethodName
attention	I-MethodName
-	O
it	O
is	O
better	O
to	O
inform	O
the	O
attention	O
mechanism	O
to	O
help	O
it	O
make	O
better	O
decisions	O
,	O
than	O
to	O
override	O
its	O
decisions	O
altogether	O
.	O
This	O
theory	O
is	O
supported	O
by	O
the	O
large	O
boost	O
that	O
coverage	O
gives	O
our	O
ROUGE	O
scores	O
(	O
see	O
Table	O
1	O
)	O
,	O
compared	O
to	O
the	O
smaller	O
boost	O
given	O
by	O
temporal	B-MethodName
attention	I-MethodName
for	O
the	O
same	O
task	O
.	O

For	O
all	O
experiments	O
,	O
our	O
model	O
has	O
256dimensional	O
hidden	O
states	O
and	O
128	O
-	O
dimensional	O
word	B-TaskName
embeddings	I-TaskName
.	O
For	O
the	O
pointer	O
-	O
generator	O
models	O
,	O
we	O
use	O
a	O
vocabulary	O
of	O
50k	O
words	O
for	O
both	O
source	O
and	O
target	O
-	O
note	O
that	O
due	O
to	O
the	O
pointer	B-MethodName
network	I-MethodName
's	O
ability	O
to	O
handle	O
OOV	O
words	O
,	O
we	O
can	O
use	O
For	O
the	O
baseline	O
model	O
,	O
we	O
also	O
try	O
a	O
larger	O
vocabulary	O
size	O
of	O
150k	O
.	O
Note	O
that	O
the	O
pointer	O
and	O
the	O
coverage	O
mechanism	O
introduce	O
very	O
few	O
additional	O
parameters	O
to	O
the	O
network	O
:	O
for	O
the	O
models	O
with	O
vocabulary	O
size	O
50k	O
,	O
the	O
baseline	O
model	O
has	O
21	O
,	O
499	O
,	O
600	O
parameters	O
,	O
the	O
pointer	O
-	O
generator	O
adds	O
1153	O
extra	O
parameters	O
(	O
w	O
h	O
*	O
,	O
w	O
s	O
,	O
w	O
x	O
and	O
b	O
ptr	B-DatasetName
in	O
equation	O
8	O
)	O
,	O
and	O
coverage	O
adds	O
512	O
extra	O
parameters	O
(	O
w	O
c	O
in	O
equation	O
11	O
)	O
.	O
Unlike	O
,	O
we	O
do	O
not	O
pretrain	O
the	O
word	B-TaskName
embeddings	I-TaskName
-	O
they	O
are	O
learned	O
from	O
scratch	O
during	O
training	O
.	O
We	O
train	O
using	O
Adagrad	B-MethodName
(	O
Duchi	O
et	O
al	O
,	O
2011	O
)	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.15	O
and	O
an	O
initial	O
accumulator	O
value	O
of	O
0.1	O
.	O
(	O
This	O
was	O
found	O
to	O
work	O
best	O
of	O
Stochastic	B-MethodName
Gradient	I-MethodName
Descent	I-MethodName
,	O
Adadelta	B-MethodName
,	O
Momentum	O
,	O
Adam	B-MethodName
and	O
RM	O
-	O
SProp	O
)	O
.	O
We	O
use	O
gradient	B-MethodName
clipping	I-MethodName
with	O
a	O
maximum	O
gradient	O
norm	O
of	O
2	O
,	O
but	O
do	O
not	O
use	O
any	O
form	O
of	O
regularization	O
.	O
We	O
use	O
loss	B-MetricName
on	O
the	O
validation	O
set	O
to	O
implement	O
early	B-MethodName
stopping	I-MethodName
.	O
During	O
training	O
and	O
at	O
test	O
time	O
we	O
truncate	O
the	O
article	O
to	O
400	O
tokens	O
and	O
limit	O
the	O
length	O
of	O
the	O
summary	O
to	O
100	O
tokens	O
for	O
training	O
and	O
120	O
tokens	O
at	O
test	O
time	O
.	O
3	O
This	O
is	O
done	O
to	O
expedite	O
training	O
and	O
testing	O
,	O
but	O
we	O
also	O
found	O
that	O
truncating	O
the	O
article	O
can	O
raise	O
the	O
performance	O
of	O
the	O
model	O
(	O
see	O
section	O
7.1	O
for	O
more	O
details	O
)	O
.	O
For	O
training	O
,	O
we	O
found	O
it	O
efficient	O
to	O
start	O
with	O
highly	O
-	O
truncated	O
sequences	O
,	O
then	O
raise	O
the	O
maximum	O
length	O
once	O
converged	O
.	O
We	O
train	O
on	O
a	O
single	O
Tesla	O
K40	O
m	O
GPU	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	O
.	O
At	O
test	O
time	O
our	O
summaries	O
are	O
produced	O
using	O
beam	O
search	O
with	O
beam	O
size	O
4	O
.	O
We	O
trained	O
both	O
our	O
baseline	O
models	O
for	O
about	O
600	O
,	O
000	O
iterations	O
(	O
33	O
epochs	O
)	O
-	O
this	O
is	O
similar	O
to	O
the	O
35	O
epochs	O
required	O
by	O
Nallapati	O
et	O
al	O
's	O
(	O
2016	O
)	O
best	O
model	O
.	O
Training	O
took	O
4	O
days	O
and	O
14	O
hours	O
for	O
the	O
50k	O
vocabulary	O
model	O
,	O
and	O
8	O
days	O
21	O
hours	O
for	O
the	O
150k	O
vocabulary	O
model	O
.	O
We	O
found	O
the	O
pointer	O
-	O
generator	O
model	O
quicker	O
to	O
train	O
,	O
requiring	O
less	O
than	O
230	O
,	O
000	O
training	O
iterations	O
(	O
12.8	O
epochs	O
)	O
;	O
a	O
total	O
of	O
3	O
days	O
and	O
4	O
hours	O
.	O
In	O
particular	O
,	O
the	O
pointer	O
-	O
generator	O
model	O
makes	O
much	O
quicker	O
progress	O
in	O
the	O
early	O
phases	O
of	O
training	O
.	O
To	O
obtain	O
our	O
final	O
coverage	O
model	O
,	O
we	O
added	O
the	O
coverage	O
mechanism	O
with	O
coverage	O
loss	B-MetricName
weighted	O
to	O
λ	O
=	O
1	O
(	O
as	O
described	O
in	O
equation	O
13	O
)	O
,	O
and	O
trained	O
for	O
a	O
further	O
3000	O
iterations	O
(	O
about	O
2	O
hours	O
)	O
.	O
In	O
this	O
time	O
the	O
coverage	O
loss	B-MetricName
converged	O
to	O
about	O
0.2	O
,	O
down	O
from	O
an	O
initial	O
value	O
of	O
about	O
0.5	O
.	O
We	O
also	O
tried	O
a	O
more	O
aggressive	O
value	O
of	O
λ	O
=	O
2	O
;	O
this	O
reduced	O
coverage	O
loss	B-MetricName
but	O
increased	O
the	O
primary	O
loss	B-MetricName
function	O
,	O
thus	O
we	O
did	O
not	O
use	O
it	O
.	O
We	O
tried	O
training	O
the	O
coverage	O
model	O
without	O
the	O
loss	B-MetricName
function	O
,	O
hoping	O
that	O
the	O
attention	O
mechanism	O
may	O
learn	O
by	O
itself	O
not	O
to	O
attend	O
repeatedly	O
to	O
the	O
same	O
locations	O
,	O
but	O
we	O
found	O
this	O
to	O
be	O
ineffective	O
,	O
with	O
no	O
discernible	O
reduction	O
in	O
repetition	O
.	O
We	O
also	O
tried	O
training	O
with	O
coverage	O
from	O
the	O
first	O
iteration	O
rather	O
than	O
as	O
a	O
separate	O
training	O
phase	O
,	O
but	O
found	O
that	O
in	O
the	O
early	O
phase	O
of	O
training	O
,	O
the	O
coverage	O
objective	O
interfered	O
with	O
the	O
main	O
objective	O
,	O
reducing	O
overall	O
performance	O
.	O

Our	O
results	O
are	O
given	O
in	O
Table	O
1	O
.	O
We	O
evaluate	O
our	O
models	O
with	O
the	O
standard	O
ROUGE	O
metric	O
(	O
Lin	O
,	O
2004b	O
)	O
,	O
reporting	O
the	O
F	O
1	O
scores	O
for	O
ROUGE	O
-	O
1	O
,	O
ROUGE	O
-	O
2	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
(	O
which	O
respectively	O
measure	O
the	O
word	O
-	O
overlap	O
,	O
bigram	O
-	O
overlap	O
,	O
and	O
longest	O
common	O
sequence	O
between	O
the	O
reference	O
summary	O
and	O
the	O
summary	O
to	O
be	O
evaluated	O
)	O
.	O
We	O
obtain	O
our	O
ROUGE	O
scores	O
using	O
the	O
pyrouge	O
package	O
.	O
4	O
We	O
also	O
evaluate	O
with	O
the	O
METEOR	B-DatasetName
metric	O
(	O
Denkowski	O
and	O
Lavie	O
,	O
2014	O
)	O
,	O
both	O
in	O
exact	B-MetricName
match	I-MetricName
mode	O
(	O
rewarding	O
only	O
exact	O
matches	O
between	O
words	O
)	O
and	O
full	O
mode	O
(	O
which	O
additionally	O
rewards	O
matching	O
stems	O
,	O
synonyms	O
and	O
paraphrases	O
)	O
.	O
5	O
In	O
addition	O
to	O
our	O
own	O
models	O
,	O
we	O
also	O
report	O
the	O
lead	O
-	O
3	O
baseline	O
(	O
which	O
uses	O
the	O
first	O
three	O
sentences	O
of	O
the	O
article	O
as	O
a	O
summary	O
)	O
,	O
and	O
compare	O
to	O
the	O
only	O
existing	O
abstractive	O
and	O
extractive	O
(	O
Nallapati	O
et	O
al	O
,	O
2017	O
)	O
models	O
on	O
the	O
full	O
dataset	O
.	O
The	O
output	O
of	O
our	O
models	O
is	O
available	O
online	O
.	O
6	O
Given	O
that	O
we	O
generate	O
plain	O
-	O
text	O
summaries	O
but	O
2017	O
)	O
generate	O
anonymized	O
summaries	O
(	O
see	O
Section	O
4	O
)	O
,	O
our	O
ROUGE	O
scores	O
are	O
not	O
strictly	O
comparable	O
.	O
There	O
is	O
evidence	O
to	O
suggest	O
that	O
the	O
original	O
-	O
text	O
dataset	O
may	O
result	O
in	O
higher	O
ROUGE	O
scores	O
in	O
general	O
than	O
the	O
anonymized	O
dataset	O
-	O
the	O
lead	O
-	O
3	O
baseline	O
is	O
higher	O
on	O
the	O
former	O
than	O
the	O
latter	O
.	O
One	O
possible	O
explanation	O
is	O
that	O
multi	O
-	O
word	O
named	O
entities	O
lead	O
to	O
a	O
higher	O
rate	O
of	O
n	O
-	O
gram	O
overlap	O
.	O
Unfortunately	O
,	O
ROUGE	O
is	O
the	O
only	O
available	O
means	O
of	O
comparison	O
with	O
Nallapati	O
et	O
al	O
's	O
work	O
.	O
Nevertheless	O
,	O
given	O
that	O
the	O
disparity	O
in	O
the	O
lead	O
-	O
3	O
scores	O
is	O
(	O
+1.1	O
ROUGE	O
-	O
1	O
,	O
+2.0	O
ROUGE	O
-	O
2	O
,	O
+1.1	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
)	O
points	O
respectively	O
,	O
and	O
our	O
best	O
model	O
scores	O
exceed	O
by	O
(	O
+4.07	O
ROUGE	O
-	O
1	O
,	O
+3.98	O
ROUGE	O
-	O
2	O
,	O
+3.73	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
)	O
points	O
,	O
we	O
may	O
estimate	O
that	O
we	O
outperform	O
the	O
only	O
previous	O
abstractive	O
system	O
by	O
at	O
least	O
2	O
ROUGE	O
points	O
allround	O
.	O

We	O
have	O
shown	O
that	O
our	O
pointer	O
mechanism	O
makes	O
our	O
abstractive	O
system	O
more	O
reliable	O
,	O
copying	O
factual	O
details	O
correctly	O
more	O
often	O
.	O
But	O
does	O
the	O
ease	O
of	O
copying	O
make	O
our	O
system	O
any	O
less	O
abstractive	O
?	O
Figure	O
6	O
shows	O
that	O
our	O
final	O
model	O
's	O
summaries	O
contain	O
a	O
much	O
lower	O
rate	O
of	O
novel	O
n	O
-	O
grams	O
(	O
i.e.	O
,	O
those	O
that	O
do	O
n't	O
appear	O
in	O
the	O
article	O
)	O
than	O
the	O
reference	O
summaries	O
,	O
indicating	O
a	O
lower	O
degree	O
of	O
abstraction	O
.	O
Note	O
that	O
the	O
baseline	O
model	O
produces	O
novel	O
n	O
-	O
grams	O
more	O
frequently	O
-	O
however	O
,	O
this	O
statistic	O
includes	O
all	O
the	O
incorrectly	O
copied	O
words	O
,	O
UNK	O
tokens	O
and	O
fabrications	O
alongside	O
the	O
good	O
instances	O
of	O
abstraction	O
.	O
Figure	O
6	O
:	O
Although	O
our	O
best	O
model	O
is	O
abstractive	O
,	O
it	O
does	O
not	O
produce	O
novel	O
n	O
-	O
grams	O
(	O
i.e.	O
,	O
n	O
-	O
grams	O
that	O
do	O
n't	O
appear	O
in	O
the	O
source	O
text	O
)	O
as	O
often	O
as	O
the	O
reference	O
summaries	O
.	O
The	O
baseline	O
model	O
produces	O
more	O
novel	O
n	O
-	O
grams	O
,	O
but	O
many	O
of	O
these	O
are	O
erroneous	O
(	O
see	O
section	O
7.2	O
)	O
.	O
Article	O
:	O
andy	O
murray	O
(	O
...	O
)	O
is	O
into	O
the	O
semi	O
-	O
finals	O
of	O
the	O
miami	O
open	O
,	O
but	O
not	O
before	O
getting	O
a	O
scare	O
from	O
21	O
year	O
-	O
old	O
austrian	O
dominic	O
thiem	O
,	O
who	O
pushed	O
him	O
to	O
4	O
-	O
4	O
in	O
the	O
second	O
set	O
before	O
going	O
down	O
3	O
-	O
6	O
6	O
-	O
4	O
,	O
6	O
-	O
1	O
in	O
an	O
hour	O
and	O
three	O
quarters	O
.	O
(	O
...	O
)	O
Summary	O
:	O
andy	O
murray	O
defeated	O
dominic	O
thiem	O
3	O
-	O
6	O
6	O
-	O
4	O
,	O
6	O
-	O
1	O
in	O
an	O
hour	O
and	O
three	O
quarters	O
.	O
Article	O
:	O
(	O
...	O
)	O
wayne	O
rooney	O
smashes	O
home	O
during	O
manchester	O
united	O
's	O
3	O
-	O
1	O
win	O
over	O
aston	O
villa	O
on	O
saturday	O
.	O
(	O
...	O
)	O
Summary	O
:	O
manchester	O
united	O
beat	O
aston	O
villa	O
3	O
-	O
1	O
at	O
old	O
trafford	O
on	O
saturday	O
.	O
Figure	O
7	O
:	O
Examples	O
of	O
abstractive	O
summaries	O
produced	O
by	O
our	O
model	O
(	O
bold	O
denotes	O
novel	O
words	O
)	O
.	O
In	O
particular	O
,	O
Figure	O
6	O
shows	O
that	O
our	O
final	O
model	O
copies	O
whole	O
article	O
sentences	O
35	O
%	O
of	O
the	O
time	O
;	O
by	O
comparison	O
the	O
reference	O
summaries	O
do	O
so	O
only	O
1.3	O
%	O
of	O
the	O
time	O
.	O
This	O
is	O
a	O
main	O
area	O
for	O
improvement	O
,	O
as	O
we	O
would	O
like	O
our	O
model	O
to	O
move	O
beyond	O
simple	O
sentence	O
extraction	O
.	O
However	O
,	O
we	O
observe	O
that	O
the	O
other	O
65	O
%	O
encompasses	O
a	O
range	O
of	O
abstractive	O
techniques	O
.	O
Article	O
sentences	O
are	O
truncated	O
to	O
form	O
grammatically	O
-	O
correct	O
shorter	O
versions	O
,	O
and	O
new	O
sentences	O
are	O
composed	O
by	O
stitching	O
together	O
fragments	O
.	O
Unnecessary	O
interjections	O
,	O
clauses	O
and	O
parenthesized	O
phrases	O
are	O
sometimes	O
omitted	O
from	O
copied	O
passages	O
.	O
Some	O
of	O
these	O
abilities	O
are	O
demonstrated	O
in	O
Figure	O
1	O
,	O
and	O
the	O
supplementary	B-DatasetName
material	I-DatasetName
contains	O
more	O
examples	O
.	O
Figure	O
7	O
shows	O
two	O
examples	O
of	O
more	O
impressive	O
abstraction	O
-	O
both	O
with	O
similar	O
structure	O
.	O
The	O
dataset	O
contains	O
many	O
sports	O
stories	O
whose	O
summaries	O
follow	O
the	O
X	O
beat	O
Y	O
score	O
on	O
day	O
tem	O
-	O
plate	O
,	O
which	O
may	O
explain	O
why	O
our	O
model	O
is	O
most	O
confidently	O
abstractive	O
on	O
these	O
examples	O
.	O
In	O
general	O
however	O
,	O
our	O
model	O
does	O
not	O
routinely	O
produce	O
summaries	O
like	O
those	O
in	O
Figure	O
7	O
,	O
and	O
is	O
not	O
close	O
to	O
producing	O
summaries	O
like	O
in	O
Figure	O
5	O
.	O
The	O
value	O
of	O
the	O
generation	O
probability	O
p	O
gen	O
also	O
gives	O
a	O
measure	O
of	O
the	O
abstractiveness	O
of	O
our	O
model	O
.	O
During	O
training	O
,	O
p	O
gen	O
starts	O
with	O
a	O
value	O
of	O
about	O
0.30	O
then	O
increases	O
,	O
converging	O
to	O
about	O
0.53	O
by	O
the	O
end	O
of	O
training	O
.	O
This	O
indicates	O
that	O
the	O
model	O
first	O
learns	O
to	O
mostly	O
copy	O
,	O
then	O
learns	O
to	O
generate	O
about	O
half	O
the	O
time	O
.	O
However	O
at	O
test	O
time	O
,	O
p	O
gen	O
is	O
heavily	O
skewed	O
towards	O
copying	O
,	O
with	O
a	O
mean	O
value	O
of	O
0.17	O
.	O
The	O
disparity	O
is	O
likely	O
due	O
to	O
the	O
fact	O
that	O
during	O
training	O
,	O
the	O
model	O
receives	O
word	O
-	O
by	O
-	O
word	O
supervision	O
in	O
the	O
form	O
of	O
the	O
reference	O
summary	O
,	O
but	O
at	O
test	O
time	O
it	O
does	O
not	O
.	O
Nonetheless	O
,	O
the	O
generator	O
module	O
is	O
useful	O
even	O
when	O
the	O
model	O
is	O
copying	O
.	O
We	O
find	O
that	O
p	O
gen	O
is	O
highest	O
at	O
times	O
of	O
uncertainty	O
such	O
as	O
the	O
beginning	O
of	O
sentences	O
,	O
the	O
join	O
between	O
stitched	O
-	O
together	O
fragments	O
,	O
and	O
when	O
producing	O
periods	O
that	O
truncate	O
a	O
copied	O
sentence	O
.	O
Our	O
mixture	O
model	O
allows	O
the	O
network	O
to	O
copy	O
while	O
simultaneously	O
consulting	O
the	O
language	O
model	O
-	O
enabling	O
operations	O
like	O
stitching	O
and	O
truncation	O
to	O
be	O
performed	O
with	O
grammaticality	O
.	O
In	O
any	O
case	O
,	O
encouraging	O
the	O
pointer	O
-	O
generator	O
model	O
to	O
write	O
more	O
abstractively	O
,	O
while	O
retaining	O
the	O
accuracy	B-MetricName
advantages	O
of	O
the	O
pointer	O
module	O
,	O
is	O
an	O
exciting	O
direction	O
for	O
future	O
work	O
.	O

In	O
text	B-TaskName
summarization	I-TaskName
,	O
manual	O
evaluation	O
,	O
as	O
exemplified	O
by	O
the	O
Pyramid	O
method	O
(	O
Nenkova	O
and	O
Passonneau	O
,	O
2004	O
)	O
,	O
is	O
the	O
gold	O
-	O
standard	O
in	O
evaluation	O
.	O
However	O
,	O
due	O
to	O
time	O
required	O
and	O
relatively	O
high	O
cost	O
of	O
annotation	O
,	O
the	O
great	O
majority	O
of	O
research	O
papers	O
on	O
summarization	B-TaskName
use	O
exclusively	O
automatic	O
evaluation	O
metrics	O
,	O
such	O
as	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
,	O
JS	O
-	O
2	O
(	O
Louis	O
and	O
Nenkova	O
,	O
2013	O
)	O
,	O
S3	O
(	O
Peyrard	O
et	O
al	O
,	O
2017	O
)	O
,	O
BERTScore	O
(	O
Zhang	O
et	O
al	O
,	O
2020	O
)	O
,	O
Mover	O
-	O
Score	B-MetricName
(	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
etc	O
.	O
Among	O
these	O
metrics	O
,	O
ROUGE	O
is	O
by	O
far	O
the	O
most	O
popular	O
,	O
and	O
there	O
is	O
relatively	O
little	O
discussion	O
of	O
how	O
ROUGE	O
may	O
deviate	O
from	O
human	O
judgment	O
and	O
the	O
potential	O
for	O
this	O
deviation	O
to	O
change	O
conclusions	O
drawn	O
regarding	O
relative	O
merit	O
of	O
baseline	O
and	O
proposed	O
methods	O
.	O
To	O
characterize	O
the	O
relative	O
goodness	O
of	O
evaluation	O
metrics	O
,	O
it	O
is	O
necessary	O
to	O
perform	O
metaevaluation	O
(	O
Graham	O
,	O
2015	O
;	O
Lin	O
and	O
Och	O
,	O
2004	O
)	O
,	O
where	O
a	O
dataset	O
annotated	O
with	O
human	O
judgments	O
(	O
e.g.	O
TAC	O
1	O
2008	O
(	O
Dang	O
and	O
Owczarzak	O
,	O
2008	O
)	O
)	O
is	O
used	O
to	O
test	O
the	O
degree	O
to	O
which	O
automatic	O
metrics	O
correlate	O
therewith	O
.	O
However	O
,	O
the	O
classic	O
TAC	O
meta	O
-	O
evaluation	O
datasets	O
are	O
now	O
6	O
-	O
12	O
years	O
old	O
2	O
and	O
it	O
is	O
not	O
clear	O
whether	O
conclusions	O
found	O
there	O
will	O
hold	O
with	O
modern	O
systems	O
and	O
summarization	B-TaskName
tasks	O
.	O
Two	O
earlier	O
works	O
exemplify	O
this	O
disconnect	O
:	O
(	O
1	O
)	O
Peyrard	O
(	O
2019	O
)	O
observed	O
that	O
the	O
human	O
-	O
annotated	O
summaries	O
in	O
the	O
TAC	O
dataset	O
are	O
mostly	O
of	O
lower	O
quality	O
than	O
those	O
produced	O
by	O
modern	O
systems	O
and	O
that	O
various	O
automated	O
evaluation	O
metrics	O
strongly	O
disagree	O
in	O
the	O
higher	O
-	O
scoring	O
range	O
in	O
which	O
current	O
systems	O
now	O
operate	O
.	O
(	O
2	O
)	O
Rankel	O
et	O
al	O
(	O
2013	O
)	O
observed	O
that	O
the	O
correlation	O
between	O
ROUGE	O
and	O
human	O
judgments	O
in	O
the	O
TAC	O
dataset	O
decreases	O
when	O
looking	O
at	O
the	O
best	O
systems	O
only	O
,	O
even	O
for	O
systems	O
from	O
eight	O
years	O
ago	O
,	O
which	O
are	O
far	O
from	O
today	O
's	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O
Constrained	O
by	O
few	O
existing	O
human	O
judgment	O
datasets	O
,	O
it	O
remains	O
unknown	O
how	O
existing	O
metrics	O
behave	O
on	O
current	O
top	O
-	O
scoring	O
summarization	B-TaskName
systems	O
.	O
In	O
this	O
paper	O
,	O
we	O
ask	O
the	O
question	O
:	O
does	O
the	O
rapid	O
progress	O
of	O
model	O
development	O
in	O
summarization	B-TaskName
models	O
require	O
us	O
to	O
re	O
-	O
evaluate	O
the	O
evaluation	O
process	O
used	O
for	O
text	B-TaskName
summarization	I-TaskName
?	O
To	O
this	O
end	O
,	O
we	O
create	O
and	O
release	O
a	O
large	O
benchmark	O
for	O
meta	O
-	O
evaluating	O
summarization	B-TaskName
metrics	O
including	O
:	O
Outputs	O
from	O
25	O
top	O
-	O
scoring	O
extractive	O
and	O
abstractive	O
summarization	B-TaskName
systems	O
on	O
the	O
CNN	O
/	O
DailyMail	O
dataset	O
.	O
Automatic	O
evaluations	O
from	O
several	O
evaluation	O
metrics	O
including	O
traditional	O
metrics	O
(	O
e.g.	O
ROUGE	O
)	O
and	O
modern	O
semantic	O
matching	O
metrics	O
(	O
e.g.	O
BERTScore	O
,	O
MoverScore	O
)	O
.	O
Ability	O
of	O
metrics	O
to	O
Observations	O
on	O
existing	O
human	O
judgments	O
(	O
TAC	O
)	O
Observations	O
on	O
new	O
human	O
judgments	O
(	O
CNNDM	O
)	O
Exp	O
-	O
I	O
:	O
evaluate	O
all	O
systems	O
?	O
(	O
Sec	O
.	O
4.1	O
)	O
MoverScore	O
and	O
JS	O
-	O
2	O
outperform	O
all	O
other	O
metrics	O
.	O
ROUGE	O
-	O
2	O
outperforms	O
all	O
other	O
metrics	O
.	O
MoverScore	O
and	O
JS	O
-	O
2	O
performs	O
worse	O
both	O
in	O
extractive	O
(	O
only	O
achieved	O
nearly	O
0.1	O
Pearson	B-MetricName
correlation	I-MetricName
)	O
and	O
abstractive	O
summaries	O
.	O
Exp	O
-	O
II	O
:	O
evaluate	O
top	O
-	O
k	O
systems	O
?	O
(	O
Sec	O
.	O
4.2	O
)	O
As	O
k	O
becomes	O
smaller	O
,	O
ROUGE	O
-	O
2	O
de	O
-	O
correlates	O
with	O
humans	O
.	O
For	O
extractive	O
and	O
abstractive	O
systems	O
,	O
ROUGE	O
-	O
2	O
highly	O
correlates	O
with	O
humans	O
.	O
For	O
evaluating	O
a	O
mix	O
of	O
extractive	O
and	O
abstractive	O
systems	O
,	O
all	O
metrics	O
de	O
-	O
correlate	O
.	O
Exp	O
-	O
III	O
:	O
compare	O
2	O
systems	O
?	O
(	O
Sec	O
.	O
4.3	O
)	O
MoverScore	O
and	O
JS	O
-	O
2	O
outperform	O
all	O
other	O
metrics	O
.	O
ROUGE	O
-	O
2	O
is	O
the	O
most	O
reliable	O
for	O
abstractive	O
systems	O
while	O
ROUGE	O
-	O
1	O
is	O
most	O
reliable	O
for	O
extractive	O
systems	O
.	O
Exp	O
-	O
IV	O
:	O
evaluate	O
summaries	O
?	O
(	O
Sec	O
.	O
4.4	O
)	O
(	O
1	O
)	O
MoverScore	O
and	O
JS	O
-	O
2	O
outperform	O
all	O
other	O
metrics	O
.	O
(	O
2	O
)	O
Metrics	O
have	O
much	O
lower	O
correlations	O
when	O
evaluating	O
summaries	O
than	O
systems	O
.	O
(	O
1	O
)	O
ROUGE	O
metrics	O
outperform	O
all	O
other	O
metrics	O
.	O
(	O
2	O
)	O
For	O
extractive	O
summaries	O
,	O
most	O
metrics	O
are	O
better	O
at	O
evaluating	O
summaries	O
than	O
systems	O
.	O
For	O
abstractive	O
summaries	O
,	O
some	O
metrics	O
are	O
better	O
at	O
summary	O
level	O
,	O
others	O
are	O
better	O
at	O
system	O
level	O
.	O
Manual	O
evaluations	O
using	O
the	O
lightweight	O
pyramids	O
method	O
(	O
Shapira	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
we	O
use	O
as	O
a	O
gold	O
-	O
standard	O
to	O
evaluate	O
summarization	B-TaskName
systems	O
as	O
well	O
as	O
automated	O
metrics	O
.	O
Using	O
this	O
benchmark	O
,	O
we	O
perform	O
an	O
extensive	O
analysis	O
,	O
which	O
indicates	O
the	O
need	O
to	O
re	O
-	O
examine	O
our	O
assumptions	O
about	O
the	O
evaluation	O
of	O
automatic	O
summarization	B-TaskName
systems	O
.	O
Specifically	O
,	O
we	O
conduct	O
four	O
experiments	O
analyzing	O
the	O
correspondence	O
between	O
various	O
metrics	O
and	O
human	O
evaluation	O
.	O
Somewhat	O
surprisingly	O
,	O
we	O
find	O
that	O
many	O
of	O
the	O
previously	O
attested	O
properties	O
of	O
metrics	O
found	O
on	O
the	O
TAC	O
dataset	O
demonstrate	O
different	O
trends	O
on	O
our	O
newly	O
collected	O
CNNDM	O
dataset	O
,	O
as	O
shown	O
in	O
Tab	O
.	O
1	O
.	O
For	O
example	O
,	O
MoverScore	O
is	O
the	O
best	O
performing	O
metric	O
for	O
evaluating	O
summaries	O
on	O
dataset	O
TAC	O
,	O
but	O
it	O
is	O
significantly	O
worse	O
than	O
ROUGE	O
-	O
2	O
on	O
our	O
collected	O
CNNDM	O
set	O
.	O
Additionally	O
,	O
many	O
previous	O
works	O
(	O
Novikova	O
et	O
al	O
,	O
2017	O
;	O
Peyrard	O
et	O
al	O
,	O
2017	O
;	O
Chaganty	O
et	O
al	O
,	O
2018	O
)	O
show	O
that	O
metrics	O
have	O
much	O
lower	O
correlations	O
at	O
comparing	O
summaries	O
than	O
systems	O
.	O
For	O
extractive	O
summaries	O
on	O
CNNDM	O
,	O
however	O
,	O
most	O
metrics	O
are	O
better	O
at	O
comparing	O
summaries	O
than	O
systems	O
.	O
Calls	O
for	O
Future	O
Research	O
These	O
observations	O
demonstrate	O
the	O
limitations	O
of	O
our	O
current	O
bestperforming	O
metrics	O
,	O
highlighting	O
(	O
1	O
)	O
the	O
need	O
for	O
future	O
meta	O
-	O
evaluation	O
to	O
(	O
i	O
)	O
be	O
across	O
multiple	O
datasets	O
and	O
(	O
ii	O
)	O
evaluate	O
metrics	O
on	O
different	O
application	O
scenarios	O
,	O
e.g.	O
summary	O
level	O
vs.	O
system	O
level	O
(	O
2	O
)	O
the	O
need	O
for	O
more	O
systematic	O
metaevaluation	O
of	O
summarization	B-TaskName
metrics	O
that	O
updates	O
with	O
our	O
ever	O
-	O
evolving	O
systems	O
and	O
datasets	O
,	O
and	O
(	O
3	O
)	O
the	O
potential	O
benefit	O
to	O
the	O
summarization	B-TaskName
community	O
of	O
a	O
shared	O
task	O
similar	O
to	O
the	O
WMT	O
3	O
Metrics	O
Task	O
in	O
Machine	B-TaskName
Translation	I-TaskName
,	O
where	O
systems	O
and	O
metrics	O
co	O
-	O
evolve	O
.	O
3	O
http://www.statmt.org/wmt20/	O

We	O
examine	O
eight	O
metrics	O
that	O
measure	O
the	O
agreement	O
between	O
two	O
texts	O
,	O
in	O
our	O
case	O
,	O
between	O
the	O
system	O
summary	O
and	O
reference	O
summary	O
.	O
BERTScore	O
(	O
BScore	O
)	O
measures	O
soft	O
overlap	O
between	O
contextual	O
BERT	B-MethodName
embeddings	O
of	O
tokens	O
between	O
the	O
two	O
texts	O
4	O
(	O
Zhang	O
et	O
al	O
,	O
2020	O
)	O
.	O
MoverScore	O
(	O
MScore	O
)	O
applies	O
a	O
distance	O
measure	O
to	O
contextualized	O
BERT	B-MethodName
and	O
ELMo	B-MethodName
word	B-TaskName
embeddings	I-TaskName
5	O
(	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
.	O
Sentence	O
Mover	O
Similarity	O
(	O
SMS	O
)	O
applies	O
minimum	O
distance	O
matching	O
between	O
text	O
based	O
on	O
sentence	B-TaskName
embeddings	I-TaskName
(	O
Clark	O
et	O
al	O
,	O
2019	O
)	O
.	O
Word	O
Mover	O
Similarity	O
(	O
WMS	O
)	O
measures	O
similarity	O
using	O
minimum	O
distance	O
matching	O
between	O
texts	O
which	O
are	O
represented	O
as	O
a	O
bag	O
of	O
word	B-TaskName
embeddings	I-TaskName
6	O
(	O
Kusner	O
et	O
al	O
,	O
2015	O
)	O
.	O
JS	O
divergence	O
(	O
JS	O
-	O
2	O
)	O
measures	O
Jensen	O
-	O
Shannon	O
divergence	O
between	O
the	O
two	O
text	O
's	O
bigram	O
distributions	O
7	O
(	O
Lin	O
et	O
al	O
,	O
2006	O
)	O
.	O
ROUGE	O
-	O
1	O
and	O
ROUGE	O
-	O
2	O
measure	O
overlap	O
of	O
unigrams	O
and	O
bigrams	O
respectively	O
8	O
(	O
Lin	O
,	O
2004	O
)	O
.	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
measures	O
overlap	O
of	O
the	O
longest	O
common	O
subsequence	O
between	O
two	O
texts	O
(	O
Lin	O
,	O
2004	O
)	O
.	O
We	O
use	O
the	O
recall	O
variant	O
of	O
all	O
metrics	O
(	O
since	O
the	O
Pyramid	O
method	O
of	O
human	O
evaluations	O
is	O
inherently	O
recall	O
based	O
)	O
except	O
MScore	O
which	O
has	O
no	O
specific	O
recall	O
variant	O
.	O

Pearson	B-MetricName
Correlation	I-MetricName
is	O
a	O
measure	O
of	O
linear	O
correlation	O
between	O
two	O
variables	O
and	O
is	O
popular	O
in	O
metaevaluating	O
metrics	O
at	O
the	O
system	O
level	O
(	O
Lee	O
Rodgers	O
,	O
1988	O
)	O
.	O
We	O
use	O
the	O
implementation	O
given	O
by	O
Virtanen	O
et	O
al	O
(	O
2020	O
)	O
.	O
William	O
's	O
Significance	O
Test	O
is	O
a	O
means	O
of	O
calculating	O
the	O
statistical	O
significance	O
of	O
differences	O
in	O
correlations	O
for	O
dependent	O
variables	O
(	O
Williams	O
,	O
1959	O
;	O
Graham	O
and	O
Baldwin	O
,	O
2014	O
)	O
.	O
This	O
is	O
useful	O
for	O
us	O
since	O
metrics	O
evaluated	O
on	O
the	O
same	O
dataset	O
are	O
not	O
independent	O
of	O
each	O
other	O
.	O

In	O
text	B-TaskName
summarization	I-TaskName
,	O
a	O
"	O
good	O
"	O
summary	O
should	O
represent	O
as	O
much	O
relevant	O
content	O
from	O
the	O
input	O
document	O
as	O
possible	O
,	O
within	O
the	O
acceptable	O
length	O
limits	O
.	O
Many	O
human	O
evaluation	O
methods	O
have	O
been	O
proposed	O
to	O
capture	O
this	O
desideratum	O
(	O
Nenkova	O
and	O
Passonneau	O
,	O
2004	O
;	O
Chaganty	O
et	O
al	O
,	O
2018	O
;	O
Fan	O
et	O
al	O
,	O
2018	O
;	O
Shapira	O
et	O
al	O
,	O
2019	O
)	O
.	O
Among	O
these	O
,	O
Pyramid	O
(	O
Nenkova	O
and	O
Passonneau	O
,	O
2004	O
)	O
is	O
a	O
reliable	O
and	O
widely	O
used	O
method	O
,	O
that	O
evaluates	O
content	O
selection	O
by	O
(	O
1	O
)	O
exhaustively	O
obtaining	O
Semantic	O
Content	O
Units	O
(	O
SCUs	O
)	O
from	O
reference	O
summaries	O
,	O
(	O
2	O
)	O
weighting	O
them	O
based	O
on	O
the	O
number	O
of	O
times	O
they	O
are	O
mentioned	O
and	O
(	O
3	O
)	O
scoring	O
a	O
system	O
summary	O
based	O
on	O
which	O
SCUs	O
can	O
be	O
inferred	O
.	O
Recently	O
,	O
Shapira	O
et	O
al	O
(	O
2019	O
)	O
extended	O
Pyramid	O
to	O
a	O
lightweight	O
,	O
crowdsourcable	O
method	O
-	O
LitePyramids	O
,	O
which	O
uses	O
Amazon	O
Mechanical	O
Turk	O
10	O
(	O
AMT	O
)	O
for	O
gathering	O
human	O
annotations	O
.	O
LitePyramids	O
simplifies	O
Pyramid	O
by	O
(	O
1	O
)	O
allowing	O
crowd	O
workers	O
to	O
extract	O
a	O
subset	O
of	O
all	O
possible	O
SCUs	O
and	O
(	O
2	O
)	O
eliminating	O
the	O
difficult	O
task	O
of	O
merging	O
duplicate	O
SCUs	O
from	O
different	O
reference	O
summaries	O
,	O
instead	O
using	O
SCU	O
sampling	O
to	O
simulate	O
frequency	O
-	O
based	O
weighting	O
.	O
Both	O
Pyramid	O
and	O
LitePyramid	O
rely	O
on	O
the	O
presence	O
of	O
multiple	O
references	O
per	O
document	O
to	O
assign	O
importance	O
weights	O
to	O
SCUs	O
.	O
However	O
in	O
the	O
CNNDM	O
dataset	O
there	O
is	O
only	O
one	O
reference	O
summary	O
per	O
document	O
.	O
We	O
therefore	O
adapt	O
the	O
LitePyramid	O
method	O
for	O
the	O
single	O
-	O
reference	O
setting	O
as	O
follows	O
.	O
SCU	O
Extraction	O
The	O
LitePyramids	O
annotation	O
instructions	O
define	O
a	O
Semantic	O
Content	O
Unit	O
(	O
SCU	O
)	O
as	O
a	O
sentence	O
containing	O
a	O
single	O
fact	O
written	O
as	O
briefly	O
and	O
clearly	O
as	O
possible	O
.	O
Instead	O
,	O
we	O
focus	O
on	O
shorter	O
,	O
more	O
fine	O
-	O
grained	O
SCUs	O
that	O
contain	O
at	O
most	O
2	O
-	O
3	O
entities	O
.	O
This	O
allows	O
for	O
partial	O
content	O
overlap	O
between	O
a	O
generated	O
and	O
reference	O
summary	O
,	O
and	O
also	O
makes	O
the	O
task	O
easy	O
for	O
workers	O
.	O
Tab	O
.	O
2	O
gives	O
an	O
example	O
.	O
We	O
exhaustively	O
extract	O
(	O
up	O
to	O
16	O
)	O
SCUs	O
11	O
from	O
each	O
reference	O
summary	O
.	O
Requiring	O
the	O
set	O
of	O
SCUs	O
to	O
be	O
exhaustive	O
increases	O
the	O
complexity	O
of	O
the	O
SCU	O
generation	O
task	O
,	O
and	O
hence	O
instead	O
of	O
relying	O
on	O
crowd	O
-	O
workers	O
,	O
we	O
create	O
SCUs	O
from	O
reference	O
summaries	O
ourselves	O
.	O
In	O
the	O
end	O
,	O
we	O
obtained	O
nearly	O
10.5	O
SCUs	O
on	O
average	O
from	O
each	O
reference	O
summary	O
.	O
System	O
Evaluation	O
During	O
system	O
evaluation	O
the	O
full	O
set	O
of	O
SCUs	O
is	O
presented	O
to	O
crowd	O
workers	O
.	O
Workers	O
are	O
paid	O
similar	O
to	O
Shapira	O
et	O
al	O
(	O
2019	O
)	O
,	O
scaling	O
the	O
rates	O
for	O
fewer	O
SCUs	O
and	O
shorter	O
summary	O
texts	O
.	O
For	O
abstractive	O
systems	O
,	O
we	O
pay	O
$	O
0.20	O
per	O
summary	O
and	O
for	O
extractive	O
systems	O
,	O
we	O
pay	O
$	O
0.15	O
per	O
summary	O
since	O
extractive	O
summaries	O
are	O
more	O
readable	O
and	O
might	O
precisely	O
overlap	O
with	O
SCUs	O
.	O
We	O
post	O
-	O
process	O
system	O
output	O
summaries	O
before	O
presenting	O
them	O
to	O
annotators	O
by	O
true	O
-	O
casing	O
the	O
text	O
using	O
Stanford	O
CoreNLP	O
(	O
Manning	O
et	O
al	O
,	O
2014	O
)	O
and	O
replacing	O
"	O
unknown	O
"	O
tokens	O
with	O
a	O
special	O
symbol	O
"	O
2	O
"	O
(	O
Chaganty	O
et	O
al	O
,	O
2018	O
)	O
.	O
Tab	O
.	O
2	O
depicts	O
an	O
example	O
reference	O
summary	O
,	O
system	O
summary	O
,	O
SCUs	O
extracted	O
from	O
the	O
reference	O
summary	O
,	O
and	O
annotations	O
obtained	O
in	O
evaluating	O
the	O
system	O
summary	O
.	O
Annotation	O
Scoring	O
For	O
robustness	O
(	O
Shapira	O
et	O
al	O
,	O
2019	O
)	O
,	O
each	O
system	O
summary	O
is	O
evaluated	O
by	O
4	O
crowd	O
workers	O
.	O
Each	O
worker	O
annotates	O
up	O
to	O
16	O
SCUs	O
by	O
marking	O
an	O
SCU	O
"	O
present	O
"	O
if	O
it	O
can	O
be	O
(	O
a	O
)	O
Reference	O
Summary	O
:	O
Bayern	O
Munich	O
beat	O
Porto	O
6	O
-	O
1	O
in	O
the	O
Champions	O
League	O
on	O
Tuesday	O
.	O
Pep	O
Guardiola	O
's	O
side	O
progressed	O
7	O
-	O
4	O
on	O
aggregate	O
to	O
reach	O
semi	O
-	O
finals	O
.	O
Thomas	O
Muller	O
scored	O
27th	O
Champions	O
League	O
goal	O
to	O
pass	O
Mario	O
Gomez	O
.	O
Muller	O
is	O
now	O
the	O
leading	O
German	O
scorer	O
in	O
the	O
competition	O
.	O
After	O
game	O
Muller	O
led	O
the	O
celebrations	O
with	O
supporters	O
using	O
a	O
megaphone	O
.	O
(	O
b	O
)	O
System	O
Summary	O
(	O
BART	B-MethodName
,	O
Lewis	O
et	O
al	O
(	O
2019	O
)	O
)	O
:	O
Bayern	O
Munich	O
beat	O
Porto	O
6	O
-	O
1	O
at	O
the	O
Allianz	O
Arena	O
on	O
Tuesday	O
night	O
.	O
Thomas	O
Muller	O
scored	O
his	O
27th	O
Champions	O
League	O
goal	O
.	O
The	O
25	O
-	O
year	O
-	O
old	O
became	O
the	O
highest	O
-	O
scoring	O
German	O
since	O
the	O
tournament	O
took	O
its	O
current	O
shape	O
in	O
1992	O
.	O
Bayern	O
players	O
remained	O
on	O
the	O
pitch	O
for	O
some	O
time	O
as	O
they	O
celebrated	O
with	O
supporters	O
.	O
(	O
c	O
)	O
SCUs	O
with	O
corresponding	O
evaluations	O
:	O
Bayern	O
Munich	O
beat	O
Porto	O
.	O
Bayern	O
Munich	O
won	O
6	O
-	O
1	O
.	O
Bayern	O
Munich	O
won	O
in	O
Champions	O
League	O
.	O
inferred	O
from	O
the	O
system	O
summary	O
or	O
"	O
not	O
present	O
"	O
otherwise	O
.	O
We	O
obtain	O
a	O
total	O
of	O
10	O
,	O
000	O
human	O
annotations	O
(	O
100	O
documents	O
×	O
25	O
systems	O
×	O
4	O
workers	O
)	O
.	O
For	O
each	O
document	O
,	O
we	O
identify	O
a	O
"	O
noisy	O
"	O
worker	O
as	O
one	O
who	O
disagrees	O
with	O
the	O
majority	O
(	O
i.e.	O
marks	O
an	O
SCU	O
as	O
"	O
present	O
"	O
when	O
majority	O
thinks	O
"	O
not	O
present	O
"	O
or	O
vice	O
-	O
versa	O
)	O
,	O
on	O
the	O
largest	O
number	O
of	O
SCUs	O
.	O
We	O
remove	O
the	O
annotations	O
of	O
noisy	O
workers	O
and	O
retain	O
7	O
,	O
742	O
annotations	O
of	O
the	O
10	O
,	O
000	O
.	O
After	O
this	O
filtering	O
,	O
we	O
obtain	O
an	O
average	O
inter	O
-	O
annotator	O
agreement	O
(	O
Krippendorff	O
's	O
alpha	B-HyperparameterName
(	O
Krippendorff	O
,	O
2011	O
)	O
)	O
of	O
0.66	O
.	O
12	O
Finally	O
,	O
we	O
use	O
the	O
majority	O
vote	O
to	O
mark	O
the	O
presence	O
of	O
an	O
SCU	O
in	O
a	O
system	O
summary	O
,	O
breaking	O
ties	O
by	O
the	O
class	O
,	O
"	O
not	O
present	O
"	O
.	O

Automatic	O
metrics	O
are	O
widely	O
used	O
to	O
determine	O
where	O
a	O
new	O
system	O
may	O
rank	O
against	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
.	O
Thus	O
,	O
in	O
meta	O
-	O
evaluation	O
studies	O
,	O
calculating	O
correlation	O
of	O
automatic	O
metrics	O
with	O
human	O
judgments	O
at	O
the	O
system	O
level	O
is	O
a	O
commonly	O
-	O
used	O
setting	O
(	O
Novikova	O
et	O
al	O
,	O
2017	O
;	O
Bojar	O
et	O
al	O
,	O
2016	O
;	O
Graham	O
,	O
2015	O
)	O
.	O
We	O
follow	O
this	O
setting	O
and	O
specifically	O
,	O
ask	O
two	O
questions	O
:	O
Can	O
metrics	O
reliably	O
compare	O
different	O
systems	O
?	O
To	O
answer	O
this	O
we	O
observe	O
the	O
Pearson	B-MetricName
correlation	I-MetricName
between	O
different	O
metrics	O
and	O
human	O
judgments	O
in	O
Fig	O
.	O
2	O
,	O
finding	O
that	O
:	O
(	O
1	O
)	O
MoverScore	O
and	O
JS	O
-	O
2	O
,	O
which	O
were	O
the	O
best	O
performing	O
metrics	O
on	O
TAC	O
,	O
have	O
poor	O
correlations	O
with	O
humans	O
in	O
comparing	O
CNNDM	O
Ext	O
systems	O
.	O
(	O
2	O
)	O
Most	O
metrics	O
have	O
high	O
correlations	O
on	O
the	O
TAC	O
-	O
2008	O
dataset	O
but	O
many	O
suffer	O
on	O
TAC	O
-	O
2009	O
,	O
especially	O
ROUGE	O
based	O
metrics	O
.	O
However	O
,	O
ROUGE	O
metrics	O
consistently	O
perform	O
well	O
on	O
the	O
collected	O
CNNDM	O
datasets	O
.	O
Are	O
some	O
metrics	O
significantly	O
better	O
than	O
others	O
in	O
comparing	O
systems	O
?	O
Since	O
automated	O
metrics	O
calculated	O
on	O
the	O
same	O
data	O
are	O
not	O
independent	O
,	O
we	O
must	O
perform	O
the	O
William	O
's	O
test	O
(	O
Williams	O
,	O
1959	O
)	O
to	O
establish	O
if	O
the	O
difference	O
in	O
correlations	O
between	O
metrics	O
is	O
statistically	O
significant	O
(	O
Graham	O
and	O
Baldwin	O
,	O
2014	O
)	O
.	O
In	O
Fig	O
.	O
1	O
we	O
report	O
the	O
pvalues	O
of	O
William	O
's	O
test	O
.	O
We	O
find	O
that	O
Figure	O
1	O
:	O
p	O
-	O
value	O
of	O
William	O
's	O
Significance	O
Test	O
for	O
the	O
hypothesis	O
"	O
Is	O
the	O
system	O
on	O
left	O
(	O
y	O
-	O
axis	O
)	O
significantly	O
better	O
than	O
system	O
on	O
top	O
(	O
x	O
-	O
axis	O
)	O
"	O
.	O
'	O
BScore	O
'	O
refers	O
to	O
BERTScore	O
and	O
'	O
MScore	O
'	O
refers	O
to	O
MoverScore	O
.	O
A	O
dark	O
green	O
value	O
in	O
cell	O
(	O
i	O
,	O
j	O
)	O
denotes	O
metric	O
m	O
i	O
has	O
a	O
significantly	O
higher	O
Pearson	B-MetricName
correlation	I-MetricName
with	O
human	O
scores	O
compared	O
to	O
metric	O
m	O
j	O
(	O
p	O
-	O
value	O
<	O
0.05	O
)	O
.	O
13	O
'	O
-	O
'	O
in	O
cell	O
(	O
i	O
,	O
j	O
)	O
refers	O
to	O
the	O
case	O
when	O
Pearson	B-MetricName
correlation	I-MetricName
of	O
m	O
i	O
with	O
human	O
scores	O
is	O
less	O
that	O
of	O
m	O
j	O
(	O
Sec	O
.	O
4.1	O
)	O
.	O
(	O
1	O
)	O
MoverScore	O
and	O
JS	O
-	O
2	O
are	O
significantly	O
better	O
than	O
other	O
metrics	O
in	O
correlating	O
with	O
human	O
judgments	O
on	O
the	O
TAC	O
datasets	O
.	O
(	O
2	O
)	O
However	O
,	O
on	O
CNNDM	O
Abs	O
and	O
CNNDM	O
Mix	O
,	O
R	O
-	O
2	O
significantly	O
outperforms	O
all	O
others	O
whereas	O
on	O
CNNDM	O
Ext	O
none	O
of	O
the	O
metrics	O
show	O
significant	O
improvements	O
over	O
others	O
.	O
Takeaway	O
:	O
These	O
results	O
suggest	O
that	O
metrics	O
run	O
the	O
risk	O
of	O
overfitting	O
to	O
some	O
datasets	O
,	O
highlighting	O
the	O
need	O
to	O
meta	O
-	O
evaluate	O
metrics	O
for	O
modern	O
datasets	O
and	O
systems	O
.	O
Additionally	O
,	O
there	O
is	O
no	O
one	O
-	O
size	O
-	O
fits	O
-	O
all	O
metric	O
that	O
can	O
outperform	O
others	O
on	O
all	O
datasets	O
.	O
This	O
suggests	O
the	O
utility	O
of	O
using	O
different	O
metrics	O
for	O
different	O
datasets	O
to	O
evaluate	O
systems	O
e.g.	O
MoverScore	O
on	O
TAC	O
-	O
2008	O
,	O
JS	O
-	O
2	O
on	O
TAC	O
-	O
2009	O
and	O
R	O
-	O
2	O
on	O
CNNDM	O
datasets	O
.	O

Instead	O
of	O
comparing	O
many	O
systems	O
(	O
Sec	O
.	O
4.1	O
,	O
4.2	O
)	O
ranking	O
two	O
systems	O
aims	O
to	O
test	O
the	O
discriminative	O
power	O
of	O
a	O
metric	O
,	O
i.e.	O
,	O
the	O
degree	O
to	O
which	O
the	O
metric	O
can	O
capture	O
statistically	O
significant	O
differences	O
between	O
two	O
summarization	B-TaskName
systems	O
.	O
We	O
analyze	O
the	O
reliability	O
of	O
metrics	O
along	O
a	O
useful	O
dimension	O
:	O
can	O
metrics	O
reliably	O
say	O
if	O
one	O
system	O
is	O
significantly	O
better	O
than	O
another	O
?	O
Since	O
we	O
only	O
have	O
100	O
annotated	O
summaries	O
to	O
compare	O
any	O
two	O
systems	O
,	O
sys	O
1	O
and	O
sys	O
2	O
,	O
we	O
use	O
paired	O
bootstrap	O
resampling	O
,	O
to	O
test	O
with	O
statistical	O
sig	B-MethodName
-	O
nificance	O
if	O
sys	O
1	O
is	O
better	O
than	O
sys	O
2	O
according	O
to	O
metric	O
m	O
(	O
Koehn	O
,	O
2004	O
;	O
Dror	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
take	O
all	O
J	O
2	O
pairs	O
of	O
systems	O
and	O
compare	O
their	O
mean	O
human	O
score	O
(	O
Eqn	O
.	O
3	O
)	O
using	O
paired	O
bootstrap	O
resampling	O
.	O
We	O
assign	O
a	O
label	O
y	O
true	O
=	O
1	O
if	O
sys	O
1	O
is	O
better	O
than	O
sys	O
2	O
with	O
95	O
%	O
confidence	O
,	O
y	O
true	O
=	O
2	O
for	O
viceversa	O
and	O
y	O
true	O
=	O
0	B-DatasetName
if	O
the	O
confidence	O
is	O
below	O
95	O
%	O
.	O
We	O
treat	O
this	O
as	O
the	O
ground	O
truth	O
label	O
of	O
the	O
pair	O
(	O
sys	O
1	O
,	O
sys	O
2	O
)	O
.	O
This	O
process	O
is	O
then	O
repeated	O
for	O
all	O
metrics	O
,	O
to	O
get	O
a	O
"	O
prediction	O
"	O
,	O
y	O
m	O
pred	O
from	O
each	O
metric	O
m	O
for	O
the	O
same	O
J	O
2	O
pairs	O
.	O
If	O
m	O
is	O
a	O
good	O
proxy	O
for	O
human	O
judgments	O
,	O
the	O
F1	B-MetricName
score	I-MetricName
(	O
Goutte	O
and	O
Gaussier	O
,	O
2005	O
)	O
between	O
y	O
m	O
pred	O
and	O
y	O
true	O
should	O
be	O
high	O
.	O
We	O
calculate	O
the	O
weighted	O
macro	B-MetricName
F1	I-MetricName
score	O
for	O
all	O
metrics	O
and	O
view	O
them	O
in	O
Fig	O
.	O
4	O
.	O
We	O
find	O
that	O
ROUGE	O
based	O
metrics	O
perform	O
moderately	O
well	O
in	O
this	O
task	O
.	O
R	O
-	O
2	O
performs	O
the	O
best	O
on	O
CNNDM	O
datasets	O
.	O
While	O
on	O
the	O
TAC	O
2009	O
dataset	O
,	O
JS	O
-	O
2	O
achieves	O
the	O
highest	O
F1	B-MetricName
score	I-MetricName
,	O
its	O
performance	O
is	O
low	O
on	O
CNNDM	O
Ext	O
.	O
Takeaway	O
:	O
Different	O
metrics	O
are	O
better	O
suited	O
for	O
different	O
datasets	O
.	O
For	O
example	O
,	O
on	O
the	O
CNNDM	O
datasets	O
,	O
we	O
recommend	O
using	O
R	O
-	O
2	O
while	O
,	O
on	O
the	O
TAC	O
datasets	O
,	O
we	O
recommend	O
using	O
JS	O
-	O
2	O
.	O

In	O
addition	O
to	O
comparing	O
systems	O
,	O
real	O
-	O
world	O
application	O
scenarios	O
also	O
require	O
metrics	O
to	O
reliably	O
compare	O
multiple	O
summaries	O
of	O
a	O
document	O
.	O
For	O
example	O
,	O
top	O
-	O
scoring	O
reinforcement	O
learning	O
based	O
summarization	B-TaskName
systems	O
(	O
Böhm	O
et	O
al	O
,	O
2019	O
)	O
and	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
extractive	O
system	O
(	O
Zhong	O
et	O
al	O
,	O
2020	O
)	O
heavily	O
rely	O
on	O
summary	O
-	O
level	O
reward	O
scores	O
to	O
guide	O
the	O
optimization	O
process	O
.	O
In	O
this	O
experiment	O
,	O
we	O
ask	O
the	O
question	O
:	O
how	O
well	O
do	O
different	O
metrics	O
perform	O
at	O
the	O
summary	O
level	O
,	O
i.e.	O
in	O
comparing	O
system	O
summaries	O
generated	O
from	O
the	O
same	O
document	O
?	O
We	O
use	O
Eq	O
.	O
1	O
to	O
calculate	O
Pearson	B-MetricName
correlation	I-MetricName
between	O
different	O
metrics	O
and	O
human	O
judgments	O
for	O
different	O
datasets	O
and	O
collected	O
system	O
outputs	O
.	O
Our	O
observations	O
are	O
summarized	O
in	O
Fig	O
.	O
5	O
.	O
We	O
find	O
that	O
:	O
(	O
1	O
)	O
As	O
compared	O
to	O
semantic	O
matching	O
metrics	O
,	O
R	O
-	O
1	O
,	O
R	O
-	O
2	O
and	O
R	O
-	O
L	O
have	O
lower	O
correlations	O
on	O
the	O
TAC	O
datasets	O
but	O
are	O
strong	O
indicators	O
of	O
good	O
summaries	O
especially	O
for	O
extractive	O
summaries	O
on	O
the	O
CNNDM	O
dataset	O
.	O
(	O
2	O
)	O
Notably	O
,	O
BERTScore	O
,	O
WMS	O
,	O
R	O
-	O
1	O
and	O
R	O
-	O
L	O
have	O
negative	O
correlations	O
on	O
TAC	O
-	O
2009	O
but	O
perform	O
moderately	O
well	O
on	O
other	O
datasets	O
including	O
CNNDM	O
.	O
(	O
3	O
)	O
Previous	O
meta	O
-	O
evaluation	O
studies	O
(	O
Novikova	O
et	O
al	O
,	O
2017	O
;	O
Peyrard	O
et	O
al	O
,	O
2017	O
;	O
Chaganty	O
et	O
al	O
,	O
2018	O
)	O
conclude	O
that	O
automatic	O
metrics	O
tend	O
to	O
correlate	O
well	O
with	O
humans	O
at	O
the	O
system	O
level	O
but	O
have	O
poor	O
correlations	O
at	O
the	O
instance	O
(	O
here	O
summary	O
)	O
level	O
.	O
We	O
find	O
this	O
observation	O
only	O
holds	O
on	O
TAC	O
-	O
2008	O
.	O
Some	O
metrics	O
'	O
summary	O
-	O
level	O
correlations	O
can	O
outperform	O
system	O
-	O
level	O
on	O
the	O
CNNDM	O
dataset	O
as	O
shown	O
in	O
Fig	O
.	O
7b	O
(	O
bins	O
below	O
y	O
=	O
0	B-DatasetName
)	O
.	O
Notably	O
,	O
MoverScore	O
has	O
a	O
correlation	O
of	O
only	O
0.05	O
on	O
CNNDM	O
Ext	O
at	O
the	O
system	O
level	O
but	O
0.74	O
at	O
the	O
summary	O
level	O
.	O
Takeaway	O
:	O
Meta	O
-	O
evaluations	O
of	O
metrics	O
on	O
the	O
old	O
TAC	O
datasets	O
show	O
significantly	O
different	O
trends	O
than	O
meta	O
-	O
evaluation	O
on	O
modern	O
systems	O
and	O
datasets	O
.	O
Even	O
though	O
some	O
metrics	O
might	O
be	O
good	O
at	O
comparing	O
summaries	O
,	O
they	O
may	O
point	O
in	O
the	O
wrong	O
direction	O
when	O
comparing	O
systems	O
.	O
Moreover	O
,	O
some	O
metrics	O
show	O
poor	O
generalization	O
ability	O
to	O
different	O
datasets	O
(	O
e.g.	O
BERTScore	O
on	O
TAC	O
-	O
2009	O
vs	O
other	O
datasets	O
)	O
.	O
This	O
highlights	O
the	O
need	O
for	O
empirically	O
testing	O
the	O
efficacy	O
of	O
different	O
automatic	O
metrics	O
in	O
evaluating	O
summaries	O
on	O
multiple	O
datasets	O
.	O

The	O
effectiveness	O
of	O
different	O
automatic	O
metrics	O
-	O
ROUGE	O
-	O
2	O
(	O
Lin	O
,	O
2004	O
)	O
,	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
(	O
Lin	O
,	O
2004	O
)	O
,	O
ROUGE	O
-	O
WE	O
(	O
Ng	O
and	O
Abrecht	O
,	O
2015	O
)	O
,	O
JS	O
-	O
2	O
(	O
Louis	O
and	O
Nenkova	O
,	O
2013	O
)	O
and	O
S3	O
(	O
Peyrard	O
et	O
al	O
,	O
2017	O
)	O
is	O
commonly	O
evaluated	O
based	O
on	O
their	O
correlation	O
with	O
human	O
judgments	O
(	O
e.g.	O
,	O
on	O
the	O
TAC	O
-	O
2008	O
(	O
Dang	O
and	O
Owczarzak	O
,	O
2008	O
)	O
and	O
TAC	O
-	O
2009	O
(	O
Dang	O
andOwczarzak	O
,	O
2009	O
)	O
datasets	O
)	O
.	O
As	O
an	O
important	O
supplementary	O
technique	O
to	O
metaevaluation	O
,	O
Graham	O
(	O
2015	O
)	O
advocate	O
for	O
the	O
use	O
of	O
a	O
significance	O
test	O
,	O
William	O
's	O
test	O
(	O
Williams	O
,	O
1959	O
)	O
,	O
to	O
measure	O
the	O
improved	O
correlations	O
of	O
a	O
metric	O
with	O
human	O
scores	O
and	O
show	O
that	O
the	O
popular	O
variant	O
of	O
ROUGE	O
(	O
mean	O
ROUGE	O
-	O
2	O
score	O
)	O
is	O
sub	O
-	O
optimal	O
.	O
Unlike	O
these	O
works	O
,	O
instead	O
of	O
proposing	O
a	O
new	O
metric	O
,	O
in	O
this	O
paper	O
,	O
we	O
upgrade	O
the	O
meta	O
-	O
evaluation	O
environment	O
by	O
introducing	O
a	O
sizeable	O
human	O
judgment	O
dataset	O
evaluating	O
current	O
top	O
-	O
scoring	O
systems	O
and	O
mainstream	O
datasets	O
.	O
And	O
then	O
,	O
we	O
re	O
-	O
evaluate	O
diverse	O
metrics	O
at	O
both	O
systemlevel	O
and	O
summary	O
-	O
level	O
settings	O
.	O
(	O
Novikova	O
et	O
al	O
,	O
2017	O
)	O
also	O
analyzes	O
existing	O
metrics	O
,	O
but	O
they	O
only	O
focus	O
on	O
dialog	O
generation	O
.	O

The	O
identification	O
of	O
PHI	O
is	O
a	O
type	O
of	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
task	O
where	O
sensitive	O
named	O
entities	O
specifically	O
are	O
identified	O
.	O
The	O
first	O
study	O
with	O
CRF	B-MethodName
-	O
based	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
for	O
Swedish	O
was	O
on	O
the	O
gold	O
standard	O
Stockholm	O
EPR	O
PHI	O
Corpus	O
.	O
The	O
distribution	O
of	O
PHIs	O
is	O
shown	O
in	O
Table	O
1	O
.	O
In	O
this	O
instance	O
,	O
manual	O
annotation	O
with	O
expert	O
consensus	O
was	O
used	O
to	O
create	O
the	O
gold	O
standard	O
(	O
Dalianis	O
and	O
Velupillai	O
,	O
2010	O
)	O
.	O
De	B-TaskName
-	I-TaskName
identification	I-TaskName
tasks	O
based	O
on	O
the	O
CRF	B-MethodName
machine	O
learning	O
algorithm	O
has	O
been	O
carried	O
out	O
on	O
this	O
data	O
set	O
previously	O
with	O
precision	O
scores	O
ranging	O
between	O
85	O
%	O
and	O
95	O
%	O
,	O
recalls	O
ranging	O
between	O
71	O
%	O
and	O
87	O
%	O
and	O
F1	B-MetricName
-	O
scores	O
between	O
0.76	O
and	O
0.91	O
(	O
Dalianis	O
and	O
Velupillai	O
,	O
2010	O
;	O
Berg	O
and	O
Dalianis	O
,	O
2019	O
)	O
.	O
One	O
approach	O
previously	O
used	O
for	O
concealing	O
the	O
training	O
set	O
's	O
sensitive	O
data	O
was	O
carried	O
out	O
by	O
,	O
using	O
the	O
Stockholm	O
EPR	O
PHI	O
Corpus	O
.	O
In	O
the	O
study	O
,	O
the	O
textual	O
part	O
of	O
the	O
data	O
were	O
used	O
to	O
create	O
14	O
different	O
features	O
and	O
part	O
of	O
speech	O
tags	O
.	O
The	O
textual	O
part	O
was	O
then	O
removed	O
,	O
and	O
only	O
the	O
features	O
and	O
part	O
of	O
speech	O
tags	O
were	O
used	O
for	O
training	O
a	O
Random	O
Forest	O
model	O
.	O
Fairly	O
high	O
precision	O
of	O
89.1	O
%	O
was	O
obtained	O
,	O
but	O
with	O
a	O
recall	O
of	O
54.3	O
%	O
and	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
64.8	O
.	O
In	O
contrast	O
to	O
using	O
only	O
the	O
sensitive	O
EHR	O
data	O
for	O
training	O
,	O
McMurry	O
et	O
al	O
(	O
2013	O
)	O
integrated	O
both	O
publicly	O
available	O
scientific	O
,	O
medical	O
publications	O
and	O
private	O
sensitive	O
clinical	O
notes	O
to	O
develop	O
a	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
system	O
.	O
While	O
considering	O
the	O
term	O
frequencies	O
and	O
part	O
of	O
speech	O
tags	O
between	O
the	O
two	O
data	O
sources	O
,	O
they	O
used	O
both	O
rule	O
lists	O
and	O
decision	O
trees	O
for	O
their	O
system	O
.	O
This	O
was	O
an	O
interesting	O
approach	O
since	O
it	O
raised	O
the	O
prospect	O
of	O
using	O
non	O
-	O
sensitive	O
data	O
in	O
building	O
useful	O
deidentification	O
models	O
.	O
However	O
,	O
it	O
is	O
not	O
clear	O
whether	O
medical	O
journals	O
have	O
significant	O
advantages	O
over	O
any	O
other	O
public	O
text	O
,	O
like	O
news	O
corpora	O
,	O
for	O
detecting	O
PHI	O
.	O
A	O
study	O
similar	O
to	O
Mc	O
-	O
Murry	O
et	O
al	O
(	O
2013	O
)	O
,	O
by	O
Berg	O
and	O
Dalianis	O
(	O
2019	O
)	O
,	O
showed	O
few	O
benefits	O
of	O
combining	O
non	O
-	O
medical	O
public	O
text	O
and	O
sensitive	O
clinical	O
notes	O
to	O
build	O
a	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
system	O
for	O
medical	O
records	O
.	O
More	O
recently	O
,	O
deep	O
learning	O
approaches	O
using	O
recurrent	O
neural	O
networks	O
seem	O
to	O
yield	O
significant	O
improvements	O
over	O
traditional	O
rules	O
-	O
based	O
methods	O
or	O
statistical	O
machine	O
learning	O
(	O
Dernoncourt	O
et	O
al	O
,	O
2017	O
)	O
.	O
Still	O
,	O
recent	O
studies	O
indicate	O
that	O
combining	O
several	O
approaches	O
will	O
yield	O
the	O
best	O
results	O
.	O
For	O
instance	O
,	O
the	O
best	O
system	O
in	O
a	O
recent	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
shared	O
task	O
was	O
a	O
combination	O
of	O
bidirectional	B-MethodName
LSTM	I-MethodName
,	O
CRF	B-MethodName
and	O
a	O
rule	O
-	O
based	O
subsystem	O
(	O
Liu	O
et	O
al	O
,	O
2017	O
)	O
.	O
Significant	O
domain	O
variation	O
,	O
such	O
as	O
a	O
different	O
language	O
,	O
is	O
an	O
important	O
factor	O
that	O
was	O
not	O
considered	O
in	O
the	O
discussed	O
shared	O
task	O
.	O
Domain	O
differences	O
were	O
cited	O
as	O
the	O
reason	O
for	O
poor	O
performance	O
on	O
psychiatric	O
notes	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
(	O
Stubbs	O
et	O
al	O
,	O
2017	O
)	O
,	O
compared	O
with	O
the	O
previous	O
de	B-TaskName
-	I-TaskName
identification	I-TaskName
task	O
on	O
general	O
clinical	O
narratives	O
(	O
Stubbs	O
et	O
al	O
,	O
2015	O
)	O
.	O
Within	O
the	O
same	O
language	O
and	O
similar	O
clinical	O
settings	O
,	O
the	O
change	O
of	O
domain	O
is	O
likely	O
not	O
substantial	O
.	O
While	O
in	O
future	O
research	O
it	O
may	O
be	O
worth	O
considering	O
domain	O
adaption	O
techniques	O
to	O
work	O
towards	O
a	O
system	O
meant	O
to	O
be	O
used	O
between	O
hospitals	O
,	O
they	O
were	O
not	O
considered	O
in	O
this	O
study	O
,	O
beyond	O
the	O
use	O
of	O
non	O
-	O
sensitive	O
dictionaries	O
for	O
names	O
and	O
location	O
.	O

The	O
results	O
of	O
the	O
experimental	O
work	O
are	O
summarised	O
in	O
Figure	O
2	O
.	O
As	O
can	O
be	O
observed	O
in	O
the	O
figure	O
,	O
the	O
CRF	B-MethodName
algorithm	O
seems	O
to	O
generally	O
outperform	O
the	O
LSTM	B-MethodName
algorithm	O
on	O
all	O
metrics	O
;	O
precision	O
,	O
recall	O
and	O
F1	B-MetricName
measure	O
.	O
This	O
result	O
is	O
not	O
consistent	O
with	O
repeated	O
reports	O
in	O
the	O
literature	O
,	O
where	O
deep	O
learning	O
apsklearn	O
-	O
crfsuite.readthedocs.io/en/	O
latest/	O
5	O
word2vec	O
,	O
https://github.com/tmikolov/	O
word2vec	O
proaches	O
such	O
as	O
LSTM	B-MethodName
have	O
been	O
shown	O
to	O
out	O
-	O
perform	O
most	O
other	O
methods	O
,	O
including	O
CRF	B-MethodName
.	O
Since	O
deep	O
learning	O
approaches	O
normally	O
require	O
very	O
large	O
amounts	O
of	O
data	O
,	O
one	O
explanation	O
for	O
this	O
result	O
could	O
be	O
that	O
the	O
word	B-TaskName
embeddings	I-TaskName
used	O
in	O
this	O
study	O
did	O
not	O
contain	O
sufficient	O
context	O
variations	O
required	O
for	O
more	O
robust	O
performance	O
or	O
an	O
insufficient	O
training	O
set	O
of	O
annotated	O
data	O
.	O
The	O
ability	O
to	O
identify	O
date	O
part	O
and	O
age	O
entities	O
are	O
similar	O
when	O
training	O
on	O
pseudonymised	O
data	O
and	O
real	O
data	O
for	O
the	O
CRF	B-MethodName
.	O
In	O
contrast	O
,	O
Location	O
,	O
Health	O
Care	O
Unit	O
and	O
Full	O
Date	O
were	O
negatively	O
affected	O
when	O
using	O
pseudonymised	O
training	O
data	O
regardless	O
of	O
using	O
a	O
CRF	B-MethodName
or	O
LSTM	B-MethodName
model	O
.	O

Experimental	O
results	O
of	O
the	O
CRF	B-MethodName
algorithm	O
are	O
shown	O
in	O
Table	O
3	O
.	O
Not	O
presented	O
in	O
the	O
table	O
is	O
the	O
combination	O
of	O
training	O
on	O
real	O
data	O
and	O
evaluation	O
of	O
pseudo	O
data	O
(	O
Real	O
-	O
Pseudo	O
)	O
,	O
but	O
the	O
results	O
of	O
this	O
combination	O
gave	O
a	O
precision	O
of	O
86.37	O
and	O
recall	O
of	O
77.80	O
%	O
and	O
an	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
81.86	O
.	O

The	O
experimental	O
results	O
of	O
the	O
LSTM	B-MethodName
algorithm	O
are	O
shown	O
in	O
Table	O
4	O
and	O
again	O
,	O
not	O
presented	O
in	O
the	O
table	O
is	O
the	O
combination	O
of	O
training	O
on	O
real	O
data	O
and	O
evaluation	O
of	O
pseudo	O
data	O
(	O
Real	O
-	O
Pseudo	O
)	O
.	O
The	O
result	O
of	O
this	O
combination	O
is	O
a	O
precision	O
of	O
65.83	O
%	O
and	O
recall	O
of	O
74.79	O
%	O
and	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
70.03	O
.	O

In	O
a	O
world	O
abounding	O
in	O
constant	O
protests	O
resulting	O
from	O
events	O
like	O
a	O
global	O
pandemic	O
,	O
climate	O
change	O
,	O
religious	O
or	O
political	O
conflicts	O
,	O
there	O
has	O
always	O
been	O
a	O
need	O
to	O
detect	O
events	O
/	O
protests	O
before	O
getting	O
amplified	O
by	O
news	O
media	O
or	O
social	O
media	O
.	O
This	O
paper	O
demonstrates	O
our	O
work	O
on	O
the	O
sentence	B-TaskName
classification	I-TaskName
subtask	O
of	O
multilingual	O
protest	O
detection	O
in	O
CASE@ACL	O
-	O
IJCNLP	O
2021	O
.	O
We	O
approached	O
this	O
task	O
by	O
employing	O
various	O
multilingual	O
pre	O
-	O
trained	O
transformer	O
models	O
to	O
classify	O
if	O
any	O
sentence	O
contains	O
information	O
about	O
an	O
event	O
that	O
has	O
transpired	O
or	O
not	O
.	O
Furthermore	O
,	O
we	O
performed	O
soft	O
voting	O
over	O
the	O
models	O
,	O
achieving	O
the	O
best	O
results	O
among	O
the	O
models	O
,	O
accomplishing	O
a	O
macro	O
F1	B-MetricName
-	I-MetricName
Score	I-MetricName
of	O
0.8291	O
,	O
0.7578	O
,	O
and	O
0.7951	O
in	O
English	O
,	O
Spanish	O
,	O
and	O
Portuguese	O
,	O
respectively	O
.	O
The	O
source	O
codes	O
for	O
our	O
systems	O
are	O
published	O
1	O
.	O

Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
(	O
BERT	B-MethodName
)	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
is	O
a	O
pretrained	O
language	O
model	O
which	O
was	O
created	O
with	O
the	O
objective	O
that	O
fine	O
-	O
tuning	O
a	O
pretrained	O
model	O
yields	O
better	O
performance	O
.	O
BERT	B-MethodName
's	O
pretraining	O
phase	O
includes	O
two	O
tasks	O
.	O
Firstly	O
,	O
Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
is	O
where	O
certain	O
words	O
are	O
randomly	O
masked	O
in	O
a	O
sequence	O
.	O
About	O
15	O
%	O
of	O
the	O
words	O
in	O
a	O
sequence	O
is	O
masked	O
.	O
The	O
model	O
then	O
attempts	O
to	O
predict	O
the	O
masked	O
words	O
.	O
Secondly	O
,	O
Next	O
Sentence	O
Prediction	O
(	O
NSP	O
)	O
,	O
where	O
the	O
model	O
has	O
an	O
additional	O
loss	B-MetricName
function	O
,	O
NSP	O
loss	B-MetricName
,	O
indicates	O
if	O
the	O
second	O
sequence	O
follows	O
the	O
first	O
one	O
.	O
Around	O
50	O
%	O
of	O
the	O
inputs	O
are	O
a	O
pair	O
,	O
and	O
they	O
randomly	O
chose	O
the	O
other	O
50	O
.	O
Here	O
,	O
we	O
use	O
a	O
bert	O
-	O
base	O
-	O
multilingual	O
-	O
cased	O
(	O
Pires	O
et	O
al	O
,	O
2019	O
)	O
trained	O
on	O
top	O
of	O
104	O
languages	O
in	O
the	O
largest	O
Wikipedia	O
corpus	O
.	O
This	O
model	O
has	O
12	O
layers	O
,	O
12	O
Attention	O
heads	O
with	O
over	O
179	O
million	O
parameters	O
.	O

DistilBERT	B-MethodName
(	O
Sanh	O
et	O
al	O
,	O
2019	O
)	O
is	O
the	O
distilled	O
version	O
of	O
BERT	B-MethodName
.	O
DistilBERT	B-MethodName
employs	O
a	O
triple	O
loss	B-MetricName
language	B-TaskName
modelling	I-TaskName
,	O
where	O
it	O
integrates	O
cosine	O
distance	O
loss	B-MetricName
with	O
knowledge	B-MethodName
distillation	I-MethodName
.	O
DistilBERT	B-MethodName
has	O
40	O
%	O
fewer	O
parameters	O
than	O
BERT	B-MethodName
but	O
still	O
promises	O
97	O
%	O
of	O
the	O
latter	O
's	O
performance	O
.	O
It	O
is	O
also	O
60	O
%	O
faster	O
than	O
BERT	B-MethodName
.	O
In	O
this	O
system	O
,	O
we	O
used	O
a	O
cased	O
multilingual	O
DistilBERT	B-MethodName
model	O
as	O
they	O
are	O
three	O
different	O
languages	O
.	O
For	O
our	O
cause	O
,	O
we	O
finetune	O
distilbert	B-MethodName
-	O
base	O
-	O
multilingual	O
-	O
cased	O
,	O
which	O
is	O
distilled	O
from	O
the	O
mBERT	B-MethodName
checkpoint	O
.	O
The	O
model	O
has	O
6	O
layers	O
,	O
768	O
dimensions	O
,	O
and	O
12	O
Attention	O
heads	O
,	O
totalizing	O
about	O
134	O
million	O
parameters	O
.	O

For	O
our	O
system	O
,	O
we	O
fine	O
-	O
tune	O
the	O
pretrained	O
models	O
discussed	O
in	O
Section	O
4.1	O
,	O
4.2	O
,	O
and	O
4.3	O
.	O
We	O
combine	O
the	O
three	O
datasets	O
as	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
for	O
Spanish	O
and	O
Portuguese	O
are	O
quite	O
low	O
.	O
After	O
combining	O
the	O
models	O
,	O
we	O
split	O
the	O
validation	O
set	O
accordingly	O
,	O
maintaining	O
the	O
split	O
's	O
ratio	O
and	O
tabulating	O
the	O
results	O
on	O
the	O
concatenated	O
dataset	O
in	O
Table3	O
.	O
The	O
embeddings	O
are	O
extracted	O
from	O
these	O
models	O
to	O
be	O
fed	O
as	O
input	O
to	O
the	O
LSTM	B-MethodName
layer	O
,	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
as	O
shown	O
in	O
Figure1	O
.	O
The	O
resulting	O
output	O
is	O
fed	O
into	O
a	O
global	B-MethodName
average	I-MethodName
pooling	I-MethodName
layer	O
(	O
Lin	O
et	O
al	O
,	O
2014	O
)	O
and	O
then	O
passed	O
into	O
fully	O
connected	O
layers	O
,	O
followed	O
by	O
a	O
sigmoid	B-MethodName
activation	I-MethodName
function	O
to	O
obtain	O
the	O
resulting	O
probability	O
score	O
for	O
the	O
input	O
sentences	O
.	O
The	O
same	O
parameters	O
are	O
used	O
for	O
all	O
three	O
models	O
.	O
A	O
dropout	O
layer	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
is	O
also	O
added	O
in	O
between	O
the	O
fully	O
connected	O
layers	O
for	O
regularization	O
.	O
Refer	O
Table	O
4	O
for	O
the	O
parameters	O
used	O
in	O
the	O
model	O
.	O
English	O
(	O
22	O
,	O
825	O
)	O
.	O
We	O
also	O
believe	O
that	O
our	O
approach	O
of	O
combining	O
datasets	O
could	O
have	O
influenced	O
the	O
performance	O
of	O
the	O
low	O
support	O
datasets	O
.	O

The	O
need	O
to	O
develop	O
automated	O
systems	O
to	O
detect	O
any	O
event	O
is	O
an	O
active	O
protest	O
has	O
constantly	O
been	O
increasing	O
because	O
of	O
the	O
escalation	O
of	O
social	O
media	O
users	O
and	O
several	O
platforms	O
to	O
support	O
them	O
.	O
In	O
this	O
paper	O
,	O
we	O
have	O
explored	O
several	O
multilingual	O
language	O
models	O
to	O
classify	O
if	O
a	O
given	O
sentence	O
talks	O
about	O
an	O
event	O
that	O
has	O
happened	O
(	O
Event	O
)	O
or	O
not	O
(	O
Not	O
-	O
event	O
)	O
in	O
three	O
languages	O
.	O
Our	O
work	O
primarily	O
focuses	O
on	O
fine	O
-	O
tuning	O
language	O
models	O
and	O
feeding	O
them	O
to	O
an	O
architecture	O
we	O
created	O
.	O
We	O
also	O
observe	O
that	O
the	O
problem	O
of	O
class	O
imbalance	O
has	O
had	O
a	O
significant	O
impact	O
on	O
the	O
performance	O
of	O
the	O
models	O
.	O
The	O
soft	O
voting	O
approach	O
has	O
achieved	O
macro	B-MetricName
F1	I-MetricName
-	O
Scores	O
of	O
0.8291	O
,	O
0.7578	O
,	O
and	O
0.7951	O
for	O
English	O
,	O
Spanish	O
,	O
and	O
Portuguese	O
,	O
respectively	O
.	O
For	O
future	O
work	O
,	O
we	O
intend	O
to	O
explore	O
class	O
weighting	O
techniques	O
and	O
semi	O
-	O
supervised	O
approaches	O
to	O
improve	O
our	O
performance	O
.	O

Accurate	O
detection	O
of	O
emotion	B-DatasetName
from	O
natural	O
language	O
has	O
applications	O
ranging	O
from	O
building	O
emotional	O
chatbots	O
to	O
better	O
understanding	O
individuals	O
and	O
their	O
lives	O
.	O
However	O
,	O
progress	O
on	O
emotion	B-DatasetName
detection	O
has	O
been	O
hampered	O
by	O
the	O
absence	O
of	O
large	O
labeled	O
datasets	O
.	O
In	O
this	O
work	O
,	O
we	O
build	O
a	O
very	O
large	O
dataset	O
for	O
fine	O
-	O
grained	O
emotions	O
and	O
develop	O
deep	O
learning	O
models	O
on	O
it	O
.	O
We	O
achieve	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
24	O
fine	O
-	O
grained	O
types	O
of	O
emotions	O
(	O
with	O
an	O
average	B-MetricName
accuracy	I-MetricName
of	O
87.58	O
%	O
)	O
.	O
We	O
also	O
extend	O
the	O
task	O
beyond	O
emotion	B-DatasetName
types	O
to	O
model	O
Robert	O
Plutchik	O
's	O
8	O
primary	O
emotion	B-DatasetName
dimensions	O
,	O
acquiring	O
a	O
superior	O
accuracy	B-MetricName
of	O
95.68	O
%	O
.	O

The	O
SemEval	O
-	O
2007	O
Affective	B-DatasetName
Text	I-DatasetName
task	O
(	O
Strapparava	O
and	O
Mihalcea	O
,	O
2007	O
)	O
[	O
SEM07	O
]	O
focused	O
on	O
classification	O
of	O
emotion	B-DatasetName
and	O
valence	O
(	O
i.e.	O
,	O
positive	O
and	O
negative	O
texts	O
)	O
in	O
news	O
headlines	O
.	O
A	O
total	O
of	O
1	O
,	O
250	O
headlines	O
were	O
manually	O
labeled	O
with	O
the	O
6	O
basic	O
emotions	O
of	O
Ekman	O
(	O
Ekman	O
,	O
1972	O
)	O
and	O
made	O
available	O
to	O
participants	O
.	O
Similarly	O
,	O
(	O
Aman	O
and	O
Szpakowicz	O
,	O
2007	O
)	O
describe	O
an	O
emotion	B-DatasetName
annotation	O
task	O
of	O
identifying	O
emotion	B-DatasetName
category	O
,	O
emotion	B-DatasetName
intensity	O
and	O
the	O
words	O
/	O
phrases	O
that	O
indicate	O
emotion	B-DatasetName
in	O
blog	O
post	O
data	O
of	O
4	O
,	O
090	O
sentences	O
and	O
a	O
system	O
exploiting	O
the	O
data	O
.	O
Our	O
work	O
differs	O
from	O
both	O
that	O
of	O
SEM07	O
(	O
Strapparava	O
and	O
Mihalcea	O
,	O
2007	O
)	O
and	O
(	O
Aman	O
and	O
Szpakowicz	O
,	O
2007	O
)	O
in	O
that	O
we	O
focus	O
on	O
a	O
different	O
genre	O
(	O
i.e.	O
,	O
Twitter	O
)	O
and	O
investigate	O
distant	O
supervision	O
as	O
a	O
way	O
to	O
acquire	O
a	O
significantly	O
larger	O
labeled	O
dataset	O
.	O
Our	O
work	O
is	O
similar	O
to	O
(	O
Mohammad	O
,	O
2012	O
;	O
Mohammad	O
and	O
Kiritchenko	O
,	O
2015	O
)	O
,	O
(	O
Wang	O
et	O
al	O
,	O
2012	O
)	O
,	O
and	O
(	O
Volkova	O
and	O
Bachrach	O
,	O
2016	O
)	O
who	O
use	O
distant	O
supervision	O
to	O
acquire	O
Twitter	O
data	O
with	O
emotion	B-DatasetName
hashtags	O
and	O
report	O
analyses	O
and	O
experiments	O
to	O
validate	O
the	O
utility	O
of	O
this	O
approach	O
.	O
For	O
example	O
,	O
(	O
Mohammad	O
,	O
2012	O
)	O
shows	O
that	O
by	O
using	O
a	O
simple	O
domain	B-TaskName
adaptation	I-TaskName
method	O
to	O
train	O
a	O
classifier	O
on	O
their	O
data	O
they	O
are	O
able	O
to	O
improve	O
both	O
precision	O
and	O
recall	O
on	O
the	O
SemEval	O
-	O
2007	O
(	O
Strapparava	O
andMihalcea	O
,	O
2007	O
)	O
dataset	O
.	O
As	O
the	O
author	O
points	O
out	O
,	O
this	O
is	O
another	O
premise	O
that	O
the	O
selflabeled	O
hashtags	O
acquired	O
from	O
Twitter	O
are	O
consistent	O
,	O
to	O
some	O
degree	O
,	O
with	O
the	O
emotion	B-DatasetName
labels	O
given	O
by	O
the	O
trained	O
human	O
judges	O
who	O
labeled	O
the	O
SemEval	O
-	O
2007	O
data	O
.	O
As	O
pointed	O
out	O
earlier	O
,	O
(	O
Wang	O
et	O
al	O
,	O
2012	O
)	O
randomly	O
sample	O
a	O
set	O
of	O
400	O
tweets	O
from	O
their	O
data	O
and	O
human	O
-	O
label	O
as	O
relevant	O
/	O
irrelevant	O
,	O
as	O
a	O
way	O
to	O
verify	O
the	O
distant	O
supervision	O
approach	O
with	O
the	O
quality	O
assurance	O
heuristics	O
they	O
employ	O
.	O
The	O
authors	O
found	O
that	O
the	O
precision	O
on	O
a	O
test	O
set	O
is	O
93.16	O
%	O
,	O
thus	O
confirming	O
the	O
utility	O
of	O
the	O
heuristics	O
.	O
(	O
Wang	O
et	O
al	O
,	O
2012	O
)	O
provide	O
a	O
number	O
of	O
important	O
observations	O
,	O
as	O
conclusions	O
based	O
on	O
their	O
work	O
.	O
These	O
include	O
that	O
since	O
they	O
are	O
provided	O
by	O
the	O
tweets	O
'	O
writers	O
,	O
the	O
emotion	B-DatasetName
hashtags	O
are	O
more	O
natural	O
and	O
reliable	O
than	O
the	O
emotion	B-DatasetName
labels	O
traditionally	O
assigned	O
by	O
annotators	O
to	O
data	O
by	O
a	O
few	O
annotators	O
.	O
This	O
is	O
the	O
case	O
since	O
in	O
the	O
lab	O
-	O
condition	O
method	O
annotators	O
need	O
to	O
infer	O
the	O
writers	O
emotions	O
from	O
text	O
,	O
which	O
may	O
not	O
be	O
accurate	O
.	O
Additionally	O
,	O
(	O
Volkova	O
and	O
Bachrach	O
,	O
2016	O
)	O
follow	O
the	O
same	O
distant	O
supervision	O
approach	O
and	O
find	O
correlations	O
of	O
users	O
'	O
emotional	O
tone	O
and	O
the	O
perceived	O
demographics	O
of	O
these	O
users	O
'	O
social	O
networks	O
exploiting	O
the	O
emotion	B-DatasetName
hashtag	O
-	O
labeled	O
data	O
.	O
Our	O
dataset	O
is	O
more	O
than	O
an	O
order	O
of	O
magnitude	O
larger	O
than	O
(	O
Mohammad	O
,	O
2012	O
)	O
and	O
(	O
Volkova	O
and	O
Bachrach	O
,	O
2016	O
)	O
and	O
the	O
range	O
of	O
emotions	O
we	O
target	O
is	O
much	O
more	O
fine	O
grained	O
than	O
(	O
Mohammad	O
,	O
2012	O
;	O
Wang	O
et	O
al	O
,	O
2012	O
;	O
Volkova	O
and	O
Bachrach	O
,	O
2016	O
)	O
since	O
we	O
model	O
24	O
emotion	B-DatasetName
types	O
,	O
rather	O
than	O
focus	O
on	O
≤	O
7	O
basic	O
emotions	O
.	O
(	O
Yan	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2016	O
;	O
Yan	O
and	O
Turtle	O
,	O
2016a	O
,	O
b	O
)	O
develop	O
a	O
dataset	O
of	O
15	O
,	O
553	O
tweets	O
labeled	O
with	O
28	O
emotion	B-DatasetName
types	O
and	O
so	O
target	O
a	O
fine	O
-	O
grained	O
range	O
as	O
we	O
do	O
.	O
The	O
authors	O
instruct	O
human	O
annotators	O
under	O
lab	O
conditions	O
to	O
assign	O
any	O
emotion	B-DatasetName
they	O
feel	O
is	O
expressed	O
in	O
the	O
data	O
,	O
allowing	O
them	O
to	O
assign	O
more	O
than	O
one	O
emotion	B-DatasetName
to	O
a	O
given	O
tweet	O
.	O
A	O
set	O
of	O
28	O
chosen	O
emotions	O
was	O
then	O
decided	O
upon	O
and	O
further	O
annotations	O
were	O
performed	O
using	O
Amazon	O
Mechanical	O
Turk	O
(	O
AMT	O
)	O
.	O
The	O
authors	O
cite	O
an	O
agreement	O
of	O
0.50	O
Krippendorff	O
's	O
alpha	B-HyperparameterName
(	O
α	B-HyperparameterName
)	O
between	O
the	O
lab	O
/	O
expert	O
annotators	O
,	O
and	O
an	O
(	O
α	B-HyperparameterName
)	O
of	O
0.28	O
between	O
experts	O
and	O
AMT	O
workers	O
.	O
EmoTweet	O
-	O
28	O
is	O
a	O
useful	O
resource	O
.	O
However	O
,	O
the	O
agreement	O
between	O
annotators	O
is	O
not	O
high	O
and	O
the	O
set	O
of	O
assigned	O
labels	O
do	O
not	O
adhere	O
to	O
a	O
specific	O
theory	O
of	O
emotion	B-DatasetName
.	O
We	O
use	O
a	O
much	O
larger	O
dataset	O
and	O
report	O
an	O
accuracy	B-MetricName
of	O
the	O
hashtag	O
approach	O
at	O
90	O
%	O
based	O
on	O
human	O
judgement	O
as	O
reported	O
in	O
Section	O
4	O
.	O

In	O
their	O
work	O
,	O
(	O
Wang	O
et	O
al	O
,	O
2012	O
)	O
manually	O
label	O
a	O
random	O
sample	O
of	O
400	O
tweets	O
extracted	O
with	O
hash	O
-	O
tags	O
in	O
a	O
similar	O
way	O
as	O
we	O
acquire	O
our	O
data	O
and	O
find	O
that	O
human	O
annotators	O
agree	O
93	O
%	O
of	O
the	O
time	O
with	O
the	O
hashtag	O
emotion	B-DatasetName
type	O
if	O
the	O
hashtag	O
occurs	O
as	O
the	O
last	O
word	O
in	O
the	O
tweet	O
.	O
We	O
wanted	O
to	O
validate	O
our	O
use	O
of	O
hashtags	O
in	O
a	O
similar	O
fashion	O
and	O
on	O
a	O
bigger	O
random	O
sample	O
.	O
We	O
had	O
human	O
annotators	O
label	O
a	O
random	O
sample	O
of	O
5	O
,	O
600	O
tweets	O
that	O
satisfy	O
our	O
preprocessing	O
pipeline	O
.	O
Manual	O
inspection	O
during	O
annotation	O
resulted	O
in	O
further	O
removing	O
a	O
negligible	O
16	O
tweets	O
that	O
were	O
found	O
to	O
have	O
problems	O
.	O
For	O
each	O
of	O
the	O
remaining	O
5	O
,	O
584	O
tweets	O
,	O
the	O
annotators	O
assign	O
a	O
binary	O
tag	O
from	O
the	O
set	O
{	O
relevant	O
,	O
irrelevant	O
}	O
to	O
indicate	O
whether	O
a	O
tweet	O
carries	O
an	O
emotion	B-DatasetName
category	O
as	O
assigned	O
using	O
our	O
distant	O
supervision	O
method	O
or	O
not	O
.	O
Annotators	O
assigned	O
61.37	O
%	O
(	O
n	O
=	O
3	O
,	O
427	O
)	O
"	O
relevant	O
"	O
tags	O
and	O
38.63	O
%	O
(	O
n	O
=	O
2	O
,	O
157	O
)	O
"	O
irrelevant	O
"	O
tags	O
.	O
Our	O
analysis	O
of	O
this	O
manually	O
labeled	O
dataset	O
al	O
o	O
supports	O
the	O
findings	O
of	O
(	O
Wang	O
et	O
al	O
,	O
2012	O
)	O
:	O
When	O
we	O
limit	O
position	O
of	O
the	O
emotion	B-DatasetName
hashtag	O
to	O
the	O
end	O
of	O
a	O
tweet	O
,	O
we	O
acquire	O
90.57	O
%	O
relevant	O
data	O
.	O
We	O
also	O
find	O
that	O
if	O
we	O
relax	O
the	O
constraint	O
on	O
the	O
hashtag	O
position	O
such	O
that	O
we	O
allow	O
the	O
hashtag	O
to	O
occur	O
in	O
the	O
last	O
quarter	O
of	O
a	O
tweet	O
(	O
based	O
on	O
a	O
total	O
tweet	O
character	O
count	O
)	O
,	O
we	O
acquire	O
85.43	O
%	O
relevant	O
tweets	O
.	O
We	O
also	O
find	O
that	O
only	O
23.20	O
%	O
(	O
n	O
=	O
795	O
out	O
of	O
3	O
,	O
427	O
)	O
of	O
the	O
emotion	B-DatasetName
carrying	O
tweets	O
have	O
the	O
emotion	B-DatasetName
hashtags	O
occurring	O
in	O
final	O
position	O
,	O
whereas	O
31.75	O
%	O
(	O
n	O
=	O
1	O
,	O
088	O
out	O
of	O
3	O
,	O
427	O
)	O
of	O
the	O
tweets	O
have	O
the	O
emotion	B-DatasetName
hashtags	O
in	O
the	O
last	O
quarter	O
of	O
the	O
tweet	O
string	O
.	O
This	O
shows	O
how	O
enforcing	O
a	O
final	O
hashtag	O
location	O
results	O
in	O
loss	B-MetricName
of	O
a	O
considerable	O
number	O
of	O
emotion	B-DatasetName
tweets	O
.	O
As	O
shown	O
in	O
Table	O
2	O
,	O
only	O
1	O
,	O
608	O
,	O
233	O
tweets	O
out	O
of	O
a	O
total	O
of	O
6	O
,	O
851	O
,	O
955	O
tweets	O
(	O
%	O
=	O
23	O
,	O
47	O
)	O
in	O
our	O
bigger	O
dataset	O
have	O
emotion	B-DatasetName
hashtags	O
occurring	O
in	O
final	O
position	O
.	O
Overall	O
,	O
we	O
agree	O
with	O
(	O
Mohammad	O
,	O
2012	O
;	O
Wang	O
et	O
al	O
,	O
2012	O
)	O
that	O
the	O
accuracy	B-MetricName
acquired	O
by	O
enforcing	O
a	O
strict	O
pipeline	O
and	O
limiting	O
to	O
emotion	B-DatasetName
hashtags	O
to	O
final	O
position	O
is	O
a	O
reasonable	O
measure	O
for	O
warranting	O
good	O
-	O
quality	O
data	O
for	O
training	O
supervised	O
systems	O
,	O
an	O
assumption	O
we	O
have	O
also	O
validated	O
with	O
our	O
empirical	O
findings	O
here	O
.	O
One	O
advantage	O
of	O
using	O
distant	O
supervision	O
under	O
these	O
conditions	O
for	O
labeling	O
emotion	B-DatasetName
data	O
,	O
as	O
(	O
Wang	O
et	O
al	O
,	O
2012	O
)	O
also	O
notes	O
,	O
is	O
that	O
the	O
label	O
is	O
assigned	O
by	O
the	O
writer	O
of	O
the	O
tweet	O
himself	O
/	O
herself	O
rather	O
than	O
an	O
annotator	O
who	O
could	O
wrongly	O
decide	O
what	O
category	O
a	O
tweet	O
is	O
.	O
After	O
all	O
,	O
emotion	B-DatasetName
is	O
a	O
fuzzy	O
concept	O
and	O
>	O
90	O
%	O
agreement	O
as	O
we	O
report	O
here	O
is	O
higher	O
than	O
the	O
human	O
agreement	O
usually	O
acquired	O
on	O
many	O
NLP	O
tasks	O
.	O
Another	O
advantage	O
of	O
this	O
method	O
is	O
obviously	O
that	O
it	O
enables	O
us	O
to	O
acquire	O
a	O
sufficiently	O
large	O
training	O
set	O
to	O
use	O
deep	O
learning	O
.	O
We	O
now	O
turn	O
to	O
describing	O
our	O
deep	O
learning	O
methods	O
.	O

As	O
explained	O
earlier	O
,	O
Plutchik	O
organizes	O
the	O
24	O
emotion	B-DatasetName
types	O
in	O
the	O
3	O
main	O
circles	O
that	O
we	O
will	O
refer	O
to	O
as	O
plutchik	O
-	O
1	O
,	O
plutchik	O
-	O
2	O
,	O
and	O
plutchik	O
-	O
3	O
.	O
Emotion	O
Qadir	O
(	O
2013	O
)	O
Roberts	O
(	O
2012	O
)	O
MD	O
(	O
2015	O
We	O
model	O
the	O
set	O
of	O
emotions	O
belonging	O
to	O
each	O
of	O
the	O
3	O
circles	O
independently	O
,	O
thus	O
casting	O
each	O
as	O
an	O
8	O
-	O
way	O
classification	O
task	O
.	O
Inspired	B-DatasetName
by	O
observations	O
from	O
the	O
literature	O
and	O
our	O
own	O
annotation	O
study	O
,	O
we	O
limit	O
our	O
data	O
to	O
tweets	O
of	O
at	O
least	O
5	O
words	O
with	O
an	O
emotional	O
hashtag	O
occurring	O
at	O
the	O
end	O
.	O
We	O
then	O
split	O
the	O
data	O
representing	O
each	O
of	O
the	O
3	O
circles	O
into	O
80	O
%	O
training	O
(	O
TRAIN	O
)	O
,	O
10	O
%	O
development	O
(	O
DEV	O
)	O
,	O
and	O
10	O
%	O
testing	O
(	O
TEST	O
)	O
.	O
As	O
mentioned	O
above	O
,	O
we	O
run	O
experiments	O
with	O
a	O
range	O
of	O
online	O
,	O
out	O
-	O
of	O
-	O
core	O
classifiers	O
as	O
well	O
as	O
the	O
GRNNs	O
.	O
To	O
train	O
the	O
GRNNs	O
,	O
we	O
optimize	O
the	O
hyper	O
-	O
parameters	O
of	O
the	O
network	O
on	O
a	O
development	O
set	O
as	O
we	O
describe	O
below	O
,	O
choosing	O
a	O
vocabulary	O
size	O
of	O
80	O
K	O
words	O
(	O
a	O
vocabulary	O
size	O
we	O
also	O
use	O
for	O
the	O
out	O
-	O
of	O
-	O
core	O
classifiers	O
)	O
,	O
a	O
word	O
embedding	O
vector	O
of	O
size	O
300	O
dimensions	O
learnt	O
directly	O
from	O
the	O
training	O
data	O
,	O
an	O
input	O
maximum	O
length	O
of	O
30	O
words	O
,	O
7	O
epochs	O
,	O
and	O
the	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
optimizer	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.001	O
.	O
We	O
use	O
3	O
dense	O
layers	O
each	O
with	O
1	O
,	O
000	O
units	O
.	O
We	O
use	O
dropout	O
(	O
Hinton	O
et	O
al	O
,	O
2012	O
)	O
for	O
regularization	O
,	O
with	O
a	O
dropout	O
rate	O
of	O
0.5	O
.	O
For	O
our	O
loss	B-MetricName
function	O
,	O
we	O
use	O
categorical	O
cross	O
-	O
entropy	O
.	O
We	O
use	O
a	O
minibatch	O
(	O
Cotter	O
et	O
al	O
,	O
2011	O
)	O
size	O
of	O
128	O
.	O
We	O
found	O
this	O
architecture	O
to	O
work	O
best	O
with	O
almost	O
all	O
the	O
settings	O
and	O
so	O
we	O
fix	O
it	O
across	O
the	O
board	O
for	O
all	O
experiments	O
with	O
GRNNs	O
.	O
Results	O
with	O
Traditional	O
Classifiers	O
Results	O
with	O
the	O
online	O
classifiers	O
are	O
presented	O
in	O
terms	O
of	O
F	O
-	O
score	O
in	O
Table	O
3	O
.	O
As	O
the	O
table	O
shows	O
,	O
among	O
this	O
group	O
of	O
classifiers	O
,	O
the	O
Passive	O
Agressive	O
classifier	O
(	O
PAC	O
)	O
acquires	O
the	O
best	O
performance	O
.	O
PAC	O
achieves	O
an	O
overall	O
F	O
-	O
score	O
of	O
64.86	O
%	O
on	O
plutchik	O
-	O
1	O
,	O
53.30	O
%	O
on	O
plutchik	O
-	O
2	O
,	O
and	O
68.14	O
%	O
on	O
plutchik	O
-	O
3	O
,	O
two	O
of	O
which	O
are	O
higher	O
than	O
an	O
arbitrary	O
baseline	O
3	O
of	O
60	O
%	O
.	O
Results	O
with	O
GRNNs	O
Table	O
4	O
presents	O
results	O
with	O
GRNNs	O
,	O
compared	O
with	O
the	O
best	O
results	O
using	O
the	O
traditional	O
classifiers	O
as	O
acquired	O
with	O
PAC	O
.	O
As	O
the	O
table	O
shows	O
,	O
the	O
GRNN	O
models	O
are	O
very	O
successful	O
across	O
all	O
the	O
3	O
classification	O
tasks	O
.	O
With	O
GRNNs	O
,	O
we	O
acquire	O
an	O
overall	O
F	O
-	O
scores	O
of	O
:	O
91.21	O
%	O
on	O
plutchik	O
-	O
1	O
,	O
82.32	O
%	O
on	O
plutchik	O
-	O
2	O
,	O
and	O
87.47	O
%	O
on	O
plutchik	O
-	O
3	O
.	O
These	O
results	O
are	O
26.35	O
%	O
,	O
29.02	O
%	O
,	O
and	O
25.37	O
%	O
higher	O
than	O
PAC	O
,	O
respectively	O
.	O
Negative	O
Results	O
We	O
experiment	O
with	O
aug	O
-	O
menting	O
training	O
data	O
reported	O
here	O
in	O
two	O
ways	O
:	O
1	O
)	O
For	O
each	O
emotion	B-DatasetName
type	O
,	O
we	O
concatenate	O
the	O
training	O
data	O
with	O
training	O
data	O
of	O
tweets	O
that	O
are	O
more	O
(	O
or	O
less	O
)	O
intense	O
from	O
the	O
same	O
sector	O
/	O
dimension	O
in	O
the	O
wheel	O
,	O
and	O
2	O
)	O
for	O
each	O
emotion	B-DatasetName
type	O
,	O
we	O
add	O
tweets	O
where	O
emotion	B-DatasetName
hashtags	O
occur	O
in	O
the	O
last	O
quarter	O
of	O
a	O
tweet	O
(	O
which	O
were	O
originally	O
filtered	O
out	O
from	O
TRAIN	O
)	O
.	O
However	O
,	O
we	O
gain	O
no	O
improvements	O
based	O
on	O
either	O
of	O
these	O
methods	O
,	O
thus	O
reflecting	O
the	O
importance	O
of	O
using	O
high	O
-	O
quality	O
training	O
data	O
and	O
the	O
utility	O
of	O
our	O
strict	O
pipeline	O
.	O

We	O
now	O
investigate	O
the	O
task	O
of	O
predicting	O
each	O
of	O
the	O
8	O
primary	O
emotion	B-DatasetName
dimensions	O
represented	O
by	O
the	O
sectors	O
of	O
the	O
wheel	O
(	O
where	O
the	O
three	O
degrees	O
of	O
intensity	O
of	O
a	O
given	O
emotion	B-DatasetName
are	O
reduced	O
to	O
a	O
single	O
emotion	B-DatasetName
dimension	O
[	O
e.g.	O
,	O
{	O
ecstasy	O
,	O
joy	O
,	O
serenity	O
}	O
are	O
reduced	O
to	O
the	O
joy	O
dimension	O
]	O
(	O
Volkova	O
and	O
Bachrach	O
,	O
2016	O
)	O
's	O
model	O
.	O
(	O
TRAIN	O
-	O
ALL	O
)	O
,	O
the	O
10	O
%	O
DEV	O
to	O
form	O
DEV	O
-	O
ALL	O
,	O
and	O
the	O
10	O
%	O
TEST	O
to	O
form	O
TEST	O
-	O
ALL	O
.	O
We	O
test	O
a	O
number	O
of	O
hyper	O
-	O
parameters	O
on	O
DEV	O
and	O
find	O
the	O
ones	O
we	O
have	O
identified	O
on	O
the	O
fine	O
-	O
grained	O
prediction	O
to	O
work	O
best	O
and	O
so	O
we	O
adopt	O
them	O
as	O
is	O
with	O
the	O
exception	O
of	O
limiting	O
to	O
only	O
2	O
epochs	O
.	O
We	O
believe	O
that	O
with	O
a	O
wider	O
exploration	O
of	O
hyperparameters	O
,	O
improvements	O
could	O
be	O
possible	O
.	O
As	O
Table	O
5	O
shows	O
,	O
we	O
are	O
able	O
to	O
model	O
the	O
8	O
dimensions	O
with	O
an	O
overall	O
superior	O
accuracy	B-MetricName
of	O
95.68	O
%	O
.	O
As	O
far	O
as	O
we	O
know	O
,	O
this	O
is	O
the	O
first	O
work	O
on	O
modeling	O
these	O
dimensions	O
.	O

We	O
compare	O
our	O
results	O
on	O
the	O
8	O
basic	O
emotions	O
to	O
the	O
published	O
literature	O
.	O
As	O
Table	O
6	O
shows	O
,	O
on	O
this	O
subset	O
of	O
emotions	O
,	O
our	O
system	O
is	O
4.53	O
%	O
(	O
acc	B-MetricName
)	O
higher	O
than	O
the	O
best	O
published	O
results	O
(	O
Volkova	O
and	O
Bachrach	O
,	O
2016	O
)	O
,	O
facilitated	O
by	O
the	O
fact	O
that	O
we	O
have	O
an	O
order	O
of	O
magnitude	O
more	O
training	O
data	O
.	O
As	O
shown	O
in	O
Table	O
7	O
,	O
we	O
also	O
apply	O
(	O
Volkova	O
and	O
Bachrach	O
,	O
2016	O
)	O
's	O
pre	O
-	O
trained	O
model	O
on	O
our	O
test	O
set	O
of	O
the	O
6	O
emotions	O
they	O
predict	O
(	O
which	O
belong	O
to	O
plutchik	O
-	O
2	O
)	O
,	O
and	O
acquire	O
an	O
overall	B-MetricName
accuracy	I-MetricName
of	O
26.95	O
%	O
,	O
which	O
is	O
significantly	O
lower	O
than	O
our	O
accuracy	B-MetricName
.	O

The	O
size	O
of	O
the	O
resulting	O
lexicons	O
(	O
a	O
complete	O
list	O
is	O
provided	O
in	O
Table	O
8	O
in	O
the	O
Appendix	O
)	O
ranges	O
from	O
roughly	O
100k	O
to	O
more	O
than	O
2	O
M	O
entries	O
mainly	O
depending	O
on	O
the	O
vocabulary	O
of	O
the	O
respective	O
embeddings	O
.	O
We	O
want	O
to	O
point	O
out	O
that	O
not	O
every	O
single	O
entry	O
should	O
be	O
considered	O
meaningful	O
because	O
of	O
noise	O
in	O
the	O
embedding	O
vocabulary	O
caused	O
by	O
typos	O
and	O
tokenization	O
errors	O
.	O
However	O
,	O
choosing	O
the	O
"	O
best	O
"	O
size	O
for	O
an	O
emotion	B-DatasetName
lexicon	O
necessarily	O
translates	O
into	O
a	O
quality	O
-	O
coverage	O
trade	O
-	O
off	O
for	O
which	O
there	O
is	O
no	O
general	O
solution	O
.	O
Instead	O
,	O
we	O
release	O
the	O
full	O
-	O
size	O
lexicons	O
and	O
leave	O
it	O
to	O
prospective	O
users	O
to	O
apply	O
any	O
sort	O
of	O
filtering	O
they	O
deem	O
appropriate	O
.	O
Silver	O
Evaluation	O
.	O
Figure	O
2	O
displays	O
the	O
results	O
of	O
our	O
silver	O
evaluation	O
.	O
Languages	O
(	O
x	O
-	O
axis	O
)	O
are	O
sorted	O
by	O
their	O
average	O
performance	O
over	O
all	O
variables	O
(	O
not	O
shown	O
in	O
the	O
plot	O
;	O
tabular	O
data	O
given	O
in	O
the	O
Appendix	O
)	O
.	O
As	O
can	O
be	O
seen	O
,	O
the	O
evaluation	O
results	O
for	O
English	O
are	O
markedly	O
better	O
than	O
for	O
any	O
other	O
language	O
.	O
This	O
is	O
not	O
surprising	O
since	O
no	O
(	O
potentially	O
error	O
-	O
prone	O
)	O
machine	B-TaskName
translation	I-TaskName
was	O
performed	O
.	O
Apart	O
from	O
that	O
,	O
performance	O
remains	O
relatively	O
stable	O
across	O
most	O
of	O
the	O
languages	O
and	O
starts	O
degrading	O
more	O
quickly	O
only	O
for	O
the	O
last	O
third	O
of	O
them	O
.	O
In	O
particular	O
,	O
for	O
Valence	O
-	O
typically	O
the	O
easiest	O
variable	O
to	O
predict	O
-	O
we	O
achieve	O
a	O
strong	O
performance	O
of	O
r	O
>	O
.7	O
for	O
56	O
languages	O
.	O
On	O
the	O
other	O
hand	O
,	O
for	O
Arousal	O
-	O
typically	O
,	O
the	O
most	O
difficult	O
one	O
to	O
predict	O
-	O
we	O
achieve	O
a	O
solid	O
performance	O
of	O
r	O
>	O
.5	O
for	O
55	O
languages	O
.	O
Dominance	O
and	O
the	O
discrete	O
emotion	B-DatasetName
variables	O
show	O
performance	O
trajectories	O
swinging	O
between	O
these	O
two	O
extremes	O
.	O
We	O
assume	O
that	O
the	O
main	O
factors	O
for	O
explaining	O
performance	O
differences	O
between	O
languages	O
are	O
the	O
quality	O
of	O
the	O
translation	O
and	O
embedding	O
models	O
which	O
,	O
in	O
turn	O
,	O
both	O
depend	O
on	O
the	O
amount	O
of	O
available	O
text	O
data	O
(	O
parallel	O
or	O
monolingual	O
,	O
respectively	O
)	O
.	O
Comparing	O
MTLFFN	O
and	O
ridge	O
baseline	O
,	O
we	O
find	O
that	O
the	O
neural	O
network	O
reliably	O
outperforms	O
the	O
linear	O
model	O
.	O
On	O
average	O
over	O
all	O
languages	O
and	O
variables	O
,	O
the	O
MTL	O
models	O
achieve	O
6.7	O
%	O
-	O
points	O
higher	O
Pearson	B-MetricName
correlation	I-MetricName
.	O
Conversely	O
,	O
ridge	O
regression	O
outperforms	O
MTLFFN	O
in	O
only	O
15	O
of	O
the	O
total	O
728	O
cases	O
(	O
91	O
languages	O
×	O
8	O
variables	O
)	O
.	O
Gold	O
Evaluation	O
.	O
Results	O
for	O
VAD	O
variables	O
on	O
gold	O
data	O
are	O
given	O
in	O
Table	O
3	O
.	O
As	O
can	O
be	O
seen	O
,	O
our	O
lexicons	O
show	O
a	O
good	O
correlation	O
with	O
human	O
judgment	O
and	O
do	O
so	O
robustly	O
,	O
even	O
for	O
less	O
-	O
resourced	O
languages	O
,	O
such	O
as	O
Indonesian	O
(	O
i	O
d	O
)	O
,	O
Turkish	O
(	O
tr	O
)	O
,	O
or	O
Croatian	O
(	O
hr	O
)	O
,	O
and	O
across	O
affective	O
variables	O
.	O
Perhaps	O
the	O
strongest	O
negative	O
outliers	O
are	O
the	O
Arousal	O
results	O
for	O
the	O
two	O
Chinese	O
datasets	O
(	O
zh	O
)	O
,	O
which	O
are	O
likely	O
to	O
result	O
from	O
the	O
low	O
reliability	O
of	O
the	O
gold	O
ratings	O
(	O
see	O
below	O
)	O
.	O
Buechel	O
and	O
Hahn	O
(	O
2018b	O
)	O
.	O
Shared	O
words	O
between	O
TargetGold	O
and	O
TargetPred	O
-	O
test	O
;	O
(	O
%	O
)	O
:	O
percentage	O
relative	O
to	O
TargetGold	O
;	O
Mn	O
(	O
all	O
)	O
:	O
mean	O
over	O
all	O
datasets	O
;	O
Mn	O
(	O
vs.	O
monolingual	O
)	O
:	O
mean	O
over	O
datasets	O
with	O
comparative	O
results	O
.	O
We	O
compare	O
these	O
results	O
against	O
those	O
from	O
Buechel	O
and	O
Hahn	O
(	O
2018b	O
)	O
which	O
were	O
acquired	O
on	O
the	O
respective	O
TargetGold	O
dataset	O
in	O
a	O
monolingual	O
fashion	O
using	O
10	O
-	O
fold	O
cross	O
-	O
validation	O
(	O
10	O
-	O
CV	O
)	O
.	O
We	O
admit	O
that	O
those	O
results	O
are	O
not	O
fully	O
comparable	O
to	O
those	O
presented	O
here	O
because	O
we	O
use	O
fixed	O
splits	O
rather	O
than	O
10	O
-	O
CV	O
.	O
Nevertheless	O
,	O
we	O
find	O
that	O
the	O
results	O
of	O
our	O
cross	O
-	O
lingual	O
set	O
-	O
up	O
are	O
more	O
than	O
competitive	O
,	O
outperforming	O
the	O
monolingual	O
results	O
from	O
Buechel	O
and	O
Hahn	O
(	O
2018b	O
)	O
in	O
17	O
out	O
of	O
30	O
cases	O
(	O
mainly	O
for	O
Valence	O
and	O
Dominance	O
,	O
less	O
often	O
for	O
Arousal	O
)	O
.	O
This	O
is	O
surprising	O
since	O
we	O
use	O
an	O
otherwise	O
identical	O
model	O
and	O
training	O
procedure	O
.	O
We	O
conjecture	O
that	O
the	O
large	O
size	O
of	O
the	O
English	O
Source	O
lexicon	O
,	O
compared	O
to	O
most	O
TargetGold	O
lexicons	O
,	O
more	O
than	O
compensates	O
for	O
error	O
-	O
prone	O
machine	B-TaskName
translation	I-TaskName
.	O
Table	O
4	O
shows	O
the	O
results	O
for	O
BE5	O
datasets	O
which	O
are	O
in	O
line	O
with	O
the	O
VAD	O
results	O
.	O
Regarding	O
the	O
ordering	O
of	O
the	O
emotional	O
variables	O
,	O
again	O
,	O
we	O
find	O
Valence	O
to	O
be	O
the	O
easiest	O
one	O
to	O
predict	O
,	O
Arousal	O
the	O
hardest	O
,	O
whereas	O
basic	O
emotions	O
and	O
Dominance	O
take	O
a	O
middle	O
ground	O
.	O
Comparison	O
against	O
Human	O
Reliability	O
.	O
We	O
base	O
this	O
analysis	O
on	O
inter	O
-	O
study	O
reliability	O
(	O
ISR	O
)	O
,	O
a	O
rather	O
strong	O
criterion	O
for	O
human	O
performance	O
.	O
ISR	O
is	O
computed	O
,	O
per	O
variable	O
,	O
as	O
the	O
correlation	O
between	O
the	O
ratings	O
from	O
two	O
distinct	O
annotation	O
studies	O
(	O
Warriner	O
et	O
al	O
,	O
2013	O
)	O
.	O
Hence	O
,	O
this	O
analysis	O
is	O
restricted	O
to	O
languages	O
where	O
more	O
than	O
one	O
gold	O
lexicon	O
exists	O
per	O
emotion	B-DatasetName
format	O
.	O
We	O
intersect	O
the	O
entries	O
from	O
both	O
gold	O
standards	O
as	O
well	O
as	O
the	O
respective	O
TargetPred	O
-	O
test	O
set	O
and	O
compute	O
the	O
correlation	O
between	O
all	O
three	O
pairs	O
of	O
lexicons	O
.	O
If	O
our	O
lexicon	O
agrees	O
more	O
with	O
one	O
of	O
the	O
gold	O
standards	O
than	O
the	O
two	O
gold	O
standards	O
agree	O
with	O
each	O
other	O
,	O
we	O
consider	O
this	O
as	O
an	O
indicator	O
for	O
superhuman	O
reliability	O
(	O
Buechel	O
and	O
Hahn	O
,	O
2018b	O
)	O
.	O
As	O
shown	O
in	O
Table	O
5	O
,	O
our	O
lexicons	O
are	O
often	O
competitive	O
with	O
human	O
reliability	O
for	O
Valence	O
(	O
especially	O
for	O
English	O
and	O
Chinese	O
)	O
,	O
but	O
outperform	O
human	O
reliability	O
in	O
4	O
out	O
of	O
6	O
cases	O
for	O
Arousal	O
,	O
and	O
in	O
the	O
single	O
test	O
case	O
for	O
Dominance	O
.	O
There	O
are	O
no	O
cases	O
of	O
overlapping	O
gold	O
standards	O
for	O
BE5	O
.	O
G	O
o	O
l	O
d	O
1	O
G	O
o	O
l	O
d	O
2	O
S	O
h	O
a	O
r	O
e	O
d	O
E	O
m	O
o	O
G	O
1	O
v	O
s	O
G	O
2	O
G	O
1	O
v	O
s	O
P	O
r	O
G	O
2	O
v	O

This	O
section	O
investigates	O
patterns	O
in	O
prediction	O
quality	O
across	O
languages	O
,	O
validating	O
design	O
decisions	O
of	O
our	O
methodology	O
.	O
Translation	B-TaskName
vs.	O
Prediction	O
.	O
Is	O
it	O
beneficial	O
to	O
predict	O
new	O
ratings	O
for	O
the	O
words	O
in	O
TargetMT	O
rather	O
than	O
using	O
them	O
as	O
final	O
lexicon	O
entries	O
straight	O
away	O
?	O
For	O
each	O
TargetGold	O
lexicon	O
(	O
cf	O
.	O
Table	O
2	O
)	O
,	O
we	O
intersect	O
its	O
word	O
material	O
with	O
that	O
in	O
TargetMT	O
and	O
TargetPred	O
.	O
Then	O
,	O
we	O
compute	O
the	O
correlation	O
between	O
TargetPred	O
and	O
TargetMT	O
with	O
the	O
gold	O
standard	O
.	O
This	O
analysis	O
was	O
done	O
on	O
the	O
respective	O
train	O
sets	O
because	O
using	O
TargetMT	O
rather	O
than	O
TargetPred	O
is	O
only	O
an	O
option	O
for	O
entries	O
known	O
at	O
training	O
time	O
.	O
Table	O
6	O
depicts	O
the	O
results	O
of	O
this	O
comparison	O
averaged	O
over	O
all	O
gold	O
lexicons	O
.	O
As	O
hypothesized	O
,	O
the	O
TargetPred	O
lexicons	O
agree	O
,	O
on	O
average	O
,	O
more	O
with	O
human	O
judgment	O
than	O
the	O
TargetMT	O
lexicons	O
,	O
suggesting	O
that	O
the	O
word	O
emotion	B-DatasetName
model	O
acts	O
as	O
a	O
value	O
-	O
adding	O
post	O
-	O
processor	O
,	O
partly	O
mitigating	O
rating	O
inconsistencies	O
introduced	O
by	O
mere	O
translation	O
of	O
the	O
lexicons	O
.	O
The	O
observation	O
holds	O
for	O
each	O
individual	O
emotion	B-DatasetName
variable	O
with	O
particularly	O
large	O
benefits	O
for	O
Arousal	O
,	O
where	O
the	O
postprocessed	O
TargetPred	O
lexicons	O
are	O
on	O
average	O
14	O
%	O
-	O
points	O
better	O
compared	O
to	O
the	O
translation	O
-	O
only	O
TargetMT	O
lexicons	O
.	O
This	O
seems	O
to	O
indicate	O
that	O
lexical	O
Arousal	O
is	O
less	O
consistent	O
between	O
translational	O
equivalents	O
compared	O
to	O
other	O
emotional	O
meaning	O
components	O
like	O
Valence	O
and	O
Sadness	O
,	O
which	O
appear	O
to	O
be	O
more	O
robust	O
against	O
translation	O
.	O
V	O
a	O
l	O
A	O
r	O
o	O
D	O
o	O
m	O
J	O
o	O
y	O
A	O
n	O
g	O
S	O
a	O
d	O
F	O
e	O
a	O
D	O
i	O
Gold	O
vs.	O
Silver	O
Evaluation	O
.	O
How	O
meaningful	O
is	O
silver	O
evaluation	O
without	O
gold	O
data	O
?	O
We	O
compute	O
the	O
Pearson	B-MetricName
correlation	I-MetricName
between	O
gold	O
and	O
silver	O
evaluation	O
results	O
across	O
languages	O
per	O
emotion	B-DatasetName
variable	O
.	O
For	O
languages	O
where	O
we	O
consider	O
multiple	O
datasets	O
during	O
gold	O
evaluation	O
,	O
we	O
first	O
average	O
the	O
gold	O
evaluation	O
results	O
for	O
each	O
emotion	B-DatasetName
variable	O
.	O
As	O
can	O
be	O
seen	O
from	O
Table	O
7	O
,	O
the	O
correlation	O
values	O
range	O
between	O
r	O
=	O
.91	O
for	O
Joy	O
and	O
r	O
=	O
.27	O
for	O
Disgust	O
.	O
This	O
relatively	O
large	O
dispersion	O
is	O
not	O
surprising	O
when	O
we	O
take	O
into	O
account	O
that	O
we	O
correlate	O
very	O
small	O
data	O
series	O
(	O
for	O
Valence	O
and	O
Arousal	O
there	O
are	O
just	O
12	O
languages	O
for	O
which	O
both	O
gold	O
and	O
silver	O
evaluation	O
results	O
are	O
available	O
;	O
for	O
BE5	O
there	O
are	O
only	O
5	O
such	O
languages	O
)	O
.	O
However	O
,	O
the	O
mean	O
over	O
all	O
correlation	O
values	O
in	O
Table	O
7	O
is	O
.64	O
,	O
indicating	O
that	O
there	O
is	O
a	O
relatively	O
strong	O
correlation	O
between	O
both	O
types	O
of	O
evaluation	O
.	O
This	O
suggests	O
that	O
the	O
silver	O
evaluation	O
may	O
be	O
used	O
as	O
a	O
rather	O
reliable	O
proxy	O
of	O
lexicon	O
quality	O
even	O
in	O
the	O
absence	O
of	O
language	O
-	O
specific	O
gold	O
data	O
.	O
Table	O
7	O
:	O
Agreement	O
between	O
gold	O
and	O
silver	O
evaluation	O
across	O
languages	O
in	O
Pearson	O
's	O
r	O
relative	O
to	O
the	O
number	O
of	O
applicable	O
languages	O
(	O
"	O
#	O
Lg	O
"	O
)	O
.	O

Training	O
of	O
the	O
MTLFFN	O
model	O
closely	O
followed	O
the	O
procedure	O
specified	O
by	O
Buechel	O
and	O
Hahn	O
(	O
2018b	O
)	O
:	O
For	O
each	O
language	O
,	O
the	O
model	O
was	O
trained	O
for	O
roughly	O
15k	O
iterations	O
(	O
exactly	O
168	O
epochs	O
)	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	O
using	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
10	O
−3	O
,	O
and	O
.5	O
dropout	O
on	O
the	O
hidden	O
layers	O
and	O
.2	O
on	O
the	O
input	O
layer	O
.	O
As	O
nonlinear	O
activation	B-HyperparameterName
function	I-HyperparameterName
we	O
used	O
leaky	B-MethodName
ReLU	I-MethodName
with	O
"	O
leakage	O
"	O
of	O
0.01	O
.	O
Embedding	O
vectors	O
are	O
the	O
only	O
model	O
input	O
.	O
They	O
have	O
300	O
dimensions	O
for	O
every	O
language	O
,	O
independent	O
of	O
their	O
respective	O
training	O
data	O
size	O
(	O
Grave	O
et	O
al	O
,	O
2018	O
)	O
.	O
Since	O
the	O
automatic	O
translation	O
of	O
Source	O
is	O
not	O
guaranteed	O
to	O
result	O
in	O
single	O
-	O
word	O
translations	O
,	O
we	O
use	O
the	O
following	O
workaround	O
to	O
derive	O
embedding	O
vectors	O
for	O
multi	O
-	O
token	O
translations	O
:	O
If	O
the	O
translation	O
as	O
a	O
whole	O
can	O
not	O
be	O
found	O
in	O
the	O
embedding	O
model	O
,	O
the	O
multi	O
-	O
token	O
term	O
gets	O
split	O
up	O
into	O
its	O
constituent	O
parts	O
,	O
using	O
spaces	O
,	O
apostrophes	O
or	O
hyphens	O
as	O
separators	O
.	O
Each	O
substring	O
is	O
looked	O
up	O
in	O
the	O
embedding	O
model	O
,	O
the	O
averaged	O
vector	O
is	O
taken	O
as	O
input	O
.	O
If	O
no	O
substring	O
is	O
recognized	O
,	O
we	O
use	O
the	O
zero	O
vector	O
instead	O
.	O
We	O
also	O
use	O
the	O
zero	O
vector	O
for	O
single	O
-	O
token	O
entries	O
in	O
TargetMT	O
that	O
are	O
missing	O
in	O
the	O
embeddings	O
.	O
Since	O
Buechel	O
and	O
Hahn	O
(	O
2018b	O
)	O
considered	O
only	O
VAD	O
but	O
not	O
BE5	O
datasets	O
,	O
we	O
conducted	O
a	O
development	O
experiment	O
on	O
the	O
TargetMT	O
-	O
dev	O
sets	O
for	O
all	O
91	O
languages	O
where	O
we	O
assessed	O
whether	O
MTL	O
is	O
advantageous	O
for	O
BE5	O
variables	O
as	O
well	O
,	O
or	O
for	O
a	O
combination	O
of	O
VAD	O
and	O
BE5	O
variables	O
.	O
We	O
found	O
that	O
MTL	O
improved	O
performance	O
when	O
applied	O
separately	O
among	O
all	O
VAD	O
and	O
BE5	O
variables	O
.	O
Yet	O
,	O
when	O
jointly	O
learning	O
all	O
eight	O
emotion	B-DatasetName
variables	O
,	O
the	O
results	O
were	O
somewhat	O
inconclusive	O
.	O
Performance	O
increased	O
for	O
BE5	O
,	O
but	O
decreased	O
for	O
VAD	O
.	O
Hence	O
,	O
for	O
lexicon	O
creation	O
,	O
we	O
took	O
a	O
cautious	O
approach	O
and	O
trained	O
two	O
separate	O
models	O
per	O
language	O
,	O
one	O
for	O
VAD	O
,	O
the	O
other	O
for	O
BE5	O
.	O
An	O
analysis	O
of	O
MTL	O
across	O
VAD	O
and	O
BE5	O
is	O
left	O
for	O
future	O
work	O
.	O
The	O
MTLFFN	O
model	O
is	O
implemented	O
in	O
PY	O
-	O
TORCH	O
,	O
adapting	O
part	O
of	O
the	O
TENSORFLOW	O
code	O
from	O
Buechel	O
and	O
Hahn	O
(	O
2018b	O
)	O
.	O
The	O
ridge	O
regression	O
baseline	O
model	O
is	O
implemented	O
with	O
SCIKIT	O
-	O
LEARN	O
(	O
Pedregosa	O
et	O
al	O
,	O
2011	O
)	O

Two	O
general	O
types	O
of	O
evaluation	O
metrics	O
are	O
commonly	O
used	O
to	O
evaluate	O
paraphrase	B-TaskName
generation	I-TaskName
:	O
automatic	O
evaluation	O
and	O
human	O
evaluation	O
.	O
Automatic	O
Evaluation	O
Several	O
automatic	O
evaluation	O
metrics	O
are	O
used	O
for	O
the	O
evaluation	O
of	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
The	O
widely	O
-	O
used	O
metrics	O
include	O
(	O
1	O
)	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
,	O
which	O
was	O
originally	O
developed	O
to	O
evaluate	O
machine	B-TaskName
translation	I-TaskName
systems	O
;	O
(	O
2	O
)	O
METEOR	B-DatasetName
(	O
Denkowski	O
and	O
Lavie	O
,	O
2014	O
)	O
,	O
which	O
aims	O
to	O
address	O
BLEU	B-MetricName
's	O
weakness	O
of	O
being	O
unable	O
to	O
measure	O
semantic	O
equivalents	O
when	O
applied	O
to	O
low	O
-	O
resource	O
languages	O
and	O
has	O
a	O
better	O
correlation	O
with	O
human	O
judgment	O
at	O
the	O
sentence	O
/	O
segment	O
level	O
than	O
BLEU	B-MetricName
;	O
(	O
3	O
)	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
,	O
a	O
recall	O
-	O
based	O
evaluation	O
metric	O
originally	O
developed	O
for	O
text	B-TaskName
summarization	I-TaskName
,	O
has	O
also	O
been	O
used	O
to	O
evaluate	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
Its	O
versions	O
,	O
ROUGE	O
-	O
N	O
(	O
computing	O
the	O
n	O
-	O
gram	O
recall	O
)	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
(	O
focusing	O
on	O
the	O
longest	O
common	O
subsequence	O
)	O
are	O
mostly	O
used	O
.	O
(	O
4	O
)	O
TER	O
(	O
Snover	O
et	O
al	O
,	O
2006	O
)	O
,	O
which	O
was	O
also	O
developed	O
to	O
evaluate	O
machine	B-TaskName
translation	I-TaskName
.	O
It	O
measures	O
the	O
number	O
of	O
edits	O
that	O
a	O
human	O
translator	O
would	O
have	O
to	O
perform	O
to	O
change	O
a	O
translation	O
so	O
it	O
exactly	O
matches	O
a	O
reference	O
translation	O
.	O
A	O
TER	O
score	O
is	O
a	O
value	O
in	O
the	O
range	O
of	O
0	B-DatasetName
-	O
1	O
,	O
but	O
is	O
frequently	O
presented	O
as	O
a	O
percentage	O
,	O
where	O
lower	O
is	O
better	O
.	O
Human	O
Evaluation	O
Due	O
to	O
the	O
fact	O
that	O
automatic	O
evaluation	O
metrics	O
mainly	O
focus	O
on	O
the	O
ngram	O
overlaps	O
instead	O
of	O
meaning	O
,	O
human	O
evaluation	O
is	O
used	O
to	O
provide	O
a	O
more	O
accurate	O
and	O
qualitative	O
evaluation	O
of	O
the	O
generated	O
output	O
.	O
In	O
human	O
evaluation	O
,	O
human	O
annotators	O
are	O
asked	O
to	O
score	O
generated	O
paraphrases	O
along	O
multiple	O
dimensions	O
of	O
quality	O
such	O
as	O
similarity	O
,	O
clarity	O
,	O
and	O
fluency	O
.	O
Owing	O
to	O
the	O
manual	O
annotation	O
efforts	O
,	O
human	O
evaluation	O
is	O
naturally	O
more	O
costly	O
compared	O
to	O
automatic	O
evaluation	O
,	O
but	O
more	O
representative	O
of	O
the	O
quality	O
of	O
the	O
generated	O
output	O
.	O

Model	O
-	O
focused	O
improvements	O
only	O
aim	O
to	O
utilize	O
various	O
mechanisms	O
to	O
enhance	O
the	O
encoder	O
or	O
the	O
decoder	O
without	O
paying	O
special	O
attention	O
to	O
the	O
attributes	O
of	O
the	O
generated	O
paraphrases	O
(	O
e.g.	O
,	O
granularity	O
level	O
such	O
as	O
word	O
-	O
level	O
,	O
phrase	O
-	O
level	O
and	O
sentence	O
-	O
level	O
)	O
.	O
Attention	O
The	O
Attention	O
mechanism	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
enables	O
the	O
decoder	O
to	O
focus	O
on	O
some	O
words	O
/	O
phrases	O
that	O
are	O
of	O
high	O
relevance	O
when	O
generating	O
a	O
word	O
.	O
First	O
,	O
a	O
weight	O
for	O
each	O
token	O
in	O
the	O
source	O
sequence	O
in	O
each	O
timestep	O
is	O
computed	O
to	O
indicate	O
the	O
importance	O
,	O
emphasizing	O
the	O
important	O
information	O
from	O
the	O
input	O
and	O
de	O
-	O
emphasizing	O
the	O
unimportant	O
information	O
.	O
Given	O
the	O
weight	O
distribution	O
over	O
all	O
the	O
tokens	O
in	O
the	O
source	O
sequence	O
,	O
this	O
extra	O
input	O
vector	O
,	O
the	O
context	O
vector	O
,	O
is	O
provided	O
to	O
the	O
decoder	O
..	O
Copy	O
To	O
counter	O
the	O
effect	O
of	O
rare	O
and	O
outof	O
-	O
vocabulary	O
words	O
in	O
neural	O
sequence	O
models	O
,	O
(	O
Vinyals	O
et	O
al	O
,	O
2015	O
)	O
proposed	O
a	O
pointer	B-MethodName
network	I-MethodName
.	O
A	O
pointer	B-MethodName
network	I-MethodName
copies	O
an	O
element	O
from	O
the	O
input	O
sequence	O
directly	O
into	O
the	O
output	O
.	O
Similarly	O
,	O
copy	O
mechanism	O
copies	O
a	O
span	O
of	O
elements	O
from	O
the	O
input	O
sequence	O
decided	O
by	O
attention	O
mechanism	O
directly	O
into	O
the	O
output	O
.	O
With	O
copy	O
mechanism	O
,	O
the	O
decoder	O
is	O
able	O
to	O
determine	O
whether	O
a	O
generate	O
mode	O
or	O
a	O
copy	O
mode	O
should	O
be	O
used	O
at	O
each	O
timestep	O
.	O
First	O
introduced	O
by	O
Gu	O
et	O
al	O
(	O
2016	O
)	O
for	O
abstractive	O
summarization	B-TaskName
,	O
Cao	O
et	O
al	O
(	O
2017	O
)	O
haev	O
also	O
applied	O
the	O
copy	O
mechanism	O
to	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
Despite	O
the	O
advantage	O
of	O
generating	O
well	O
-	O
formed	O
paraphrases	O
by	O
using	O
the	O
copy	O
mechanism	O
,	O
it	O
leads	O
to	O
the	O
undesirable	O
consequence	O
of	O
making	O
a	O
paraphrase	O
contain	O
many	O
of	O
the	O
phrases	O
in	O
the	O
original	O
sentence	O
and	O
limits	O
diversity	O
.	O
This	O
calls	O
for	O
a	O
controlled	O
use	O
of	O
the	O
copy	O
mechanism	O
during	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
Gupta	O
et	O
al	O
(	O
2017	O
)	O
implemented	O
the	O
encoder	O
and	O
decoder	O
with	O
LSTMs	O
,	O
whereas	O
the	O
transformer	O
is	O
utilized	O
by	O
Roy	O
and	O
Grangier	O
(	O
2019	O
)	O
.	O
Reinforcement	O
Learning	O
As	O
pointed	O
out	O
by	O
(	O
Ranzato	O
et	O
al	O
,	O
2015	O
)	O
,	O
a	O
well	O
-	O
known	O
problem	O
of	O
the	O
encoder	O
-	O
decoder	O
architecture	O
is	O
exposure	O
bias	O
:	O
the	O
decoding	O
of	O
current	O
word	O
is	O
conditioned	O
on	O
the	O
gold	O
references	O
during	O
training	O
but	O
on	O
the	O
generated	O
output	O
from	O
the	O
last	O
timestep	O
during	O
testing	O
.	O
Therefore	O
,	O
the	O
error	O
might	O
be	O
accumulated	O
and	O
propagated	O
when	O
testing	O
.	O
Another	O
problem	O
lies	O
in	O
the	O
mismatch	O
between	O
the	O
training	O
goal	O
and	O
the	O
evaluation	O
metrics	O
.	O
While	O
the	O
generated	O
paraphrases	O
are	O
finally	O
evaluated	O
automatically	O
using	O
the	O
previously	O
mentioned	O
metrics	O
,	O
the	O
network	O
is	O
trained	O
to	O
maximize	O
the	O
probability	O
of	O
generating	O
the	O
reference	O
paraphrases	O
.	O
Therefore	O
,	O
minimizing	O
the	O
training	O
loss	B-MetricName
might	O
not	O
correspond	O
to	O
optimizing	O
the	O
evaluation	O
metric	O
.	O
To	O
address	O
this	O
limitation	O
,	O
reinforcement	O
learning	O
(	O
RL	O
)	O
is	O
leveraged	O
.	O
RL	O
aims	O
to	O
train	O
an	O
agent	B-DatasetName
to	O
interact	O
with	O
the	O
environment	O
with	O
the	O
goal	O
of	O
maximizing	O
its	O
reward	O
.	O
Toward	O
finding	O
an	O
optimal	O
policy	O
,	O
RL	O
can	O
be	O
used	O
to	O
maximize	O
the	O
reward	O
indicated	O
as	O
a	O
desired	O
evaluation	O
metric	O
or	O
a	O
combination	O
of	O
multiple	O
desired	O
metrics	O
.	O
Rather	O
than	O
minimizing	O
loss	B-MetricName
(	O
the	O
conventional	O
approach	O
)	O
,	O
first	O
utilized	O
RL	O
to	O
maximize	O
the	O
reward	O
given	O
by	O
an	O
evaluator	O
which	O
outputs	O
a	O
real	O
value	O
to	O
represent	O
the	O
matching	O
degree	O
between	O
two	O
sentences	O
as	O
paraphrases	O
of	O
each	O
other	O
.	O
Other	O
reward	O
functions	O
have	O
been	O
explored	O
by	O
researchers	O
,	O
including	O
ROUGE	O
score	O
,	O
perplexity	B-MetricName
score	O
and	O
language	O
fluency	O
(	O
Siddique	O
et	O
al	O
,	O
2020	O
;	O
.	O
Generative	O
adversarial	O
networks	O
(	O
GAN	B-MethodName
)	O
Proposed	O
by	O
Goodfellow	O
et	O
al	O
(	O
2014	O
)	O
,	O
GANs	O
consist	O
of	O
generators	O
and	O
discriminators	O
,	O
where	O
generators	O
try	O
to	O
generate	O
realistic	O
outputs	O
that	O
match	O
the	O
real	O
distribution	O
and	O
discriminators	O
try	O
to	O
distinguish	O
between	O
the	O
samples	O
generated	O
by	O
generators	O
and	O
the	O
samples	O
that	O
are	O
real	O
.	O
GAN	B-MethodName
is	O
originally	O
trained	O
by	O
minimax	O
optimization	O
proposed	O
in	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
.	O
However	O
,	O
when	O
GAN	B-MethodName
is	O
applied	O
in	O
text	B-TaskName
generation	I-TaskName
,	O
the	O
traditional	O
training	O
method	O
can	O
not	O
be	O
used	O
because	O
generating	O
discrete	O
words	O
is	O
non	O
-	O
differentiable	O
.	O
Therefore	O
,	O
the	O
idea	O
of	O
policy	O
gradient	O
(	O
Sutton	O
et	O
al	O
,	O
1999	O
)	O
is	O
leveraged	O
to	O
solve	O
this	O
problem	O
(	O
Yu	O
et	O
al	O
,	O
2017	O
)	O
.	O
With	O
policy	O
gradient	O
applied	O
,	O
discriminators	O
act	O
like	O
the	O
reward	O
function	O
in	O
RL	O
.	O
Moreover	O
,	O
different	O
discriminators	O
can	O
provide	O
different	O
desired	O
rewards	O
and	O
thus	O
equip	O
the	O
model	O
with	O
the	O
capacity	O
to	O
generating	O
text	O
with	O
different	O
conditions	O
.	O
Here	O
,	O
a	O
model	O
is	O
usually	O
trained	O
in	O
an	O
adversarial	O
way	O
:	O
generators	O
and	O
discriminators	O
are	O
first	O
pretrained	O
,	O
then	O
generators	O
are	O
trained	O
to	O
maximize	O
the	O
loss	B-MetricName
of	O
the	O
fixed	O
discriminators	O
,	O
then	O
generators	O
are	O
fixed	O
and	O
discriminators	O
are	O
again	O
trained	O
to	O
minimize	O
the	O
loss	B-MetricName
by	O
provided	O
the	O
real	O
samples	O
and	O
the	O
samples	O
generated	O
by	O
the	O
fixed	O
generators	O
.	O
For	O
the	O
task	O
of	O
paraphrase	B-TaskName
generation	I-TaskName
,	O
different	O
discriminators	O
are	O
designed	O
to	O
distinguish	O
between	O
generated	O
samples	O
and	O
real	O
samples	O
,	O
paraphrases	O
and	O
non	O
-	O
paraphrases	O
(	O
Yang	O
et	O
al	O
,	O
2019	O
;	O
Vizcarra	O
and	O
Ochoa	O
-	O
Luna	O
,	O
2020	O
)	O
.	O

For	O
attribute	O
-	O
focused	O
improvements	O
,	O
their	O
purpose	O
is	O
to	O
improve	O
the	O
quality	O
of	O
generated	O
paraphrases	O
in	O
some	O
specific	O
aspects	O
such	O
as	O
diversity	O
and	O
also	O
provide	O
control	O
over	O
some	O
attributes	O
of	O
generated	O
paraphrases	O
such	O
as	O
syntax	O
and	O
granularity	O
level	O
.	O
These	O
attribute	O
-	O
focused	O
works	O
usually	O
use	O
the	O
previously	O
mentioned	O
models	O
as	O
their	O
backbone	O
models	O
.	O
Based	O
on	O
the	O
backbone	O
models	O
,	O
different	O
mechanisms	O
are	O
applied	O
for	O
different	O
focuses	O
.	O
Diversity	O
Attempts	O
focusing	O
on	O
diversity	O
aim	O
to	O
generate	O
multiple	O
diverse	O
paraphrases	O
for	O
a	O
given	O
sentence	O
.	O
Some	O
works	O
control	O
diversity	O
by	O
provid	O
-	O
ing	O
control	O
signals	O
to	O
the	O
decoder	O
.	O
Random	O
pattern	O
embeddings	O
are	O
used	O
by	O
.	O
(	O
Kumar	B-DatasetName
et	O
al	O
,	O
2019	O
)	O
utilized	O
a	O
submodular	O
mechanism	O
to	O
maximize	O
submodular	O
functions	O
measuring	O
fidelity	O
and	O
diversity	O
.	O
,	O
and	O
(	O
Cao	O
and	O
Wan	O
,	O
2020	O
)	O
all	O
generate	O
diverse	O
paraphrases	O
by	O
providing	O
the	O
decoder	O
with	O
different	O
latent	O
patterns	O
as	O
control	O
signal	O
.	O
Furthermore	O
,	O
(	O
Cao	O
and	O
Wan	O
,	O
2020	O
)	O
also	O
incorporated	O
their	O
model	O
with	O
a	O
diversity	O
loss	B-MetricName
to	O
control	O
diversity	O
.	O
use	O
RL	O
with	O
multiple	O
reward	O
functions	O
to	O
generate	O
diverse	O
paraphrases	O
.	O
One	O
of	O
the	O
reward	O
functions	O
computes	O
ROUGE	O
score	O
between	O
a	O
generated	O
sentence	O
and	O
original	O
sentence	O
,	O
which	O
can	O
focus	O
on	O
the	O
word	O
variations	O
and	O
diversity	O
.	O
Acting	O
like	O
a	O
reward	O
function	O
in	O
RL	O
,	O
discriminators	O
naturally	O
can	O
be	O
used	O
to	O
provide	O
control	O
over	O
some	O
desired	O
attributes	O
.	O
(	O
Qian	O
et	O
al	O
,	O
2019	O
)	O
utilized	O
multiple	O
generators	O
in	O
GAN	B-MethodName
to	O
generate	O
multiple	O
diverse	O
paraphrases	O
.	O
A	O
generator	O
discriminator	O
is	O
used	O
to	O
distinguish	O
sentences	O
generated	O
by	O
different	O
generators	O
and	O
guarantee	O
the	O
generated	O
paraphrases	O
are	O
diverse	O
enough	O
.	O
Word	O
-	O
Level	O
Works	O
on	O
word	O
-	O
level	O
paraphrasing	O
mainly	O
focus	O
on	O
generating	O
paraphrases	O
by	O
replacing	O
original	O
words	O
in	O
the	O
source	O
texts	O
with	O
synonyms	O
.	O
Some	O
works	O
leveraged	O
external	O
linguistic	O
knowledge	O
(	O
Cao	O
et	O
al	O
,	O
2017	O
;	O
Lin	O
et	O
al	O
,	O
2020	O
)	O
.	O
(	O
Cao	O
et	O
al	O
,	O
2017	O
)	O
utilized	O
an	O
alignment	O
table	O
capturing	O
many	O
synonym	O
mappings	O
based	O
on	O
the	O
IBM	O
Model	O
(	O
Chahuneau	O
et	O
al	O
,	O
2013	O
)	O
.	O
(	O
Lin	O
et	O
al	O
,	O
2020	O
)	O
utilized	O
WordNet	O
(	O
Miller	O
,	O
1995	O
to	O
retrieve	O
synonyms	O
.	O
Other	O
works	O
instead	O
proposed	O
special	O
mechanisms	O
to	O
learn	O
a	O
mapping	O
of	O
synonyms	O
(	O
Ma	O
et	O
al	O
,	O
2018	O
;	O
Fu	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
example	O
,	O
(	O
Ma	O
et	O
al	O
,	O
2018	O
)	O
utilized	O
retrieved	O
-	O
based	O
method	O
to	O
learn	O
such	O
a	O
mapping	O
.	O
(	O
Fu	O
et	O
al	O
,	O
2019	O
)	O
incorporates	O
a	O
novel	O
latent	O
bag	O
-	O
of	O
-	O
word	O
mechanism	O
into	O
seq2seq	B-MethodName
model	O
for	O
content	O
planning	O
,	O
which	O
mainly	O
provides	O
candidate	O
synonyms	O
for	O
words	O
in	O
the	O
source	O
texts	O
.	O
However	O
,	O
generating	O
paraphrases	O
only	O
on	O
a	O
word	O
-	O
level	O
makes	O
the	O
quality	O
and	O
diversity	O
of	O
generated	O
paraphrases	O
limited	O
.	O
Therefore	O
,	O
paraphrasing	O
has	O
also	O
been	O
studied	O
on	O
other	O
granularity	O
level	O
,	O
e.g.	O
syntax	O
level	O
.	O
Syntax	O
Works	O
in	O
this	O
category	O
explore	O
methods	O
to	O
provide	O
control	O
over	O
the	O
syntax	O
of	O
generated	O
paraphrases	O
.	O
Basically	O
,	O
all	O
the	O
methods	O
used	O
by	O
previous	O
works	O
can	O
be	O
split	O
into	O
two	O
classes	O
:	O
1	O
.	O
Explicit	O
Control	O
and	O
2	O
.	O
Implicit	O
Control	O
.	O
Methods	O
in	O
the	O
first	O
class	O
first	O
encode	O
the	O
syntax	O
tree	O
of	O
an	O
exemplar	O
sentence	O
into	O
a	O
list	O
of	O
vector	O
representations	O
and	O
then	O
feed	O
them	O
into	O
decoder	O
at	O
each	O
timestep	O
when	O
decoding	O
(	O
Iyyer	O
et	O
al	O
,	O
2018	O
;	O
Chen	O
et	O
al	O
,	O
2019	O
;	O
Goyal	O
and	O
Durrett	O
,	O
2020	O
;	O
Kumar	B-DatasetName
et	O
al	O
,	O
2020	O
)	O
.	O
These	O
methods	O
can	O
provide	O
explicit	O
control	O
over	O
the	O
syntax	O
of	O
generated	O
paraphrases	O
and	O
thus	O
has	O
better	O
interpretability	O
.	O
The	O
second	O
class	O
of	O
methods	O
will	O
first	O
learn	O
a	O
distribution	O
over	O
syntax	O
information	O
by	O
VAE	B-MethodName
.	O
Then	O
a	O
latent	O
syntax	O
variable	O
sampled	O
from	O
the	O
learned	O
distribution	O
will	O
be	O
fed	O
into	O
decoder	O
at	O
each	O
decoding	O
step	O
.	O
Although	O
the	O
control	O
provided	O
by	O
this	O
method	O
is	O
implicit	O
,	O
it	O
does	O
not	O
require	O
exemplar	O
sentences	O
and	O
also	O
can	O
group	O
multiple	O
related	O
syntax	O
under	O
the	O
same	O
latent	O
assignment	O
.	O
Multi	O
-	O
Level	O
Focusing	O
on	O
a	O
single	O
granularity	O
level	O
of	O
paraphrasing	O
still	O
makes	O
generated	O
paraphrases	O
limited	O
.	O
Therefore	O
,	O
researchers	O
also	O
explore	O
methods	O
to	O
combine	O
multiple	O
granularity	O
levels	O
together	O
.	O
Such	O
attempts	O
equip	O
their	O
model	O
with	O
the	O
capacity	O
of	O
generating	O
synonyms	O
,	O
substituting	O
phrases	O
and	O
also	O
rearrange	O
sentential	O
structures	O
(	O
Li	O
et	O
al	O
,	O
2019	O
;	O
Huang	O
et	O
al	O
,	O
2019	O
;	O
Kazemnejad	O
et	O
al	O
,	O
2020	O
)	O
.	O
By	O
using	O
multiple	O
encoders	O
,	O
(	O
Li	O
et	O
al	O
,	O
2019	O
)	O
and	O
(	O
Kazemnejad	O
et	O
al	O
,	O
2020	O
)	O
both	O
enable	O
their	O
models	O
to	O
capture	O
paraphrasing	O
patterns	O
on	O
different	O
granularity	O
levels	O
.	O
(	O
Huang	O
et	O
al	O
,	O
2019	O
)	O
instead	O
utilized	O
the	O
help	O
of	O
external	O
linguistic	O
knowledge	O
from	O
the	O
paraphrase	O
database	O
(	O
Ganitkevitch	O
et	O
al	O
,	O
2013	O
)	O
to	O
retrieve	O
and	O
learn	O
word	O
-	O
level	O
and	O
phraselevel	O
paraphrases	O
.	O
With	O
different	O
methods	O
,	O
both	O
of	O
them	O
successfully	O
combine	O
multiple	O
granularity	O
levels	O
together	O
when	O
generating	O
paraphrases	O
.	O
6	O
State	O
-	O
of	O
-	O
the	O
-	O
Art	O
Performance	O
Table	O
3	O
shows	O
the	O
ROUGE	O
and	O
BLEU	B-MetricName
scores	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
some	O
most	O
frequently	O
used	O
evaluation	O
corpus	O
in	O
recent	O
years	O
:	O
MSCOCO	B-DatasetName
and	O
Quora	O
.	O
Due	O
to	O
the	O
facts	O
that	O
different	O
metrics	O
are	O
used	O
in	O
different	O
works	O
,	O
different	O
datasets	O
are	O
used	O
in	O
different	O
works	O
and	O
many	O
of	O
them	O
did	O
not	O
release	O
their	O
codes	O
,	O
Table	O
3	O
is	O
not	O
fully	O
filled	O
.	O
However	O
,	O
with	O
most	O
of	O
the	O
table	O
filled	O
,	O
we	O
can	O
still	O
have	O
some	O
observations	O
worth	O
mentioning	O
.	O
First	O
,	O
the	O
use	O
of	O
attention	O
mechanism	O
achieves	O
a	O
close	O
performance	O
on	O
Quora	O
but	O
has	O
a	O
better	O
performance	O
on	O
MSCOCO	B-DatasetName
(	O
row	O
1	O
and	O
2	O
)	O
.	O
Similarly	O
,	O
the	O
simple	O
application	O
of	O
VAE	B-MethodName
also	O
achieves	O
a	O
close	O
performance	O
on	O
Quora	O
but	O
further	O
improves	O
the	O
performance	O
on	O
MSCOCO	B-DatasetName
(	O
row4	O
)	O
.	O
With	O
the	O
copy	O
mechanism	O
,	O
the	O
Seq2Seq	B-MethodName
model	O
is	O
able	O
to	O
retain	O
some	O
words	O
and	O
thus	O
yields	O
a	O
much	O
better	O
results	O
(	O
row	O
3	O
)	O
.	O
Transformer	B-MethodName
(	O
row	O
5	O
)	O
outperforms	O
all	O
the	O
Seq2Seq	B-MethodName
-	O
based	O
models	O
without	O
copy	O
mechanism	O
(	O
row	O
1	O
,	O
2	O
,	O
4	O
,	O
6	O
)	O
,	O
which	O
shows	O
the	O
advantages	O
of	O
Transformer	B-MethodName
and	O
meanwhile	O
also	O
proves	O
the	O
effectiveness	O
of	O
copy	O
mechanism	O
.	O
Second	O
,	O
a	O
model	O
that	O
employs	O
RL	O
(	O
row	O
7	O
)	O
has	O
a	O
great	O
advantage	O
for	O
generating	O
better	O
paraphrases	O
because	O
of	O
the	O
reward	O
provided	O
.	O
Therefore	O
,	O
a	O
well	O
designed	O
optimization	O
goal	O
plays	O
an	O
important	O
role	O
in	O
the	O
task	O
of	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
Third	O
,	O
a	O
novel	O
decoding	O
algorithm	O
based	O
on	O
large	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
helps	O
to	O
generate	O
better	O
paraphrases	O
at	O
the	O
word	O
level	O
(	O
row	O
8	O
)	O
because	O
of	O
the	O
strength	O
of	O
large	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
and	O
the	O
synonyms	O
learned	O
by	O
decoding	O
algorithm	O
.	O
Fourth	O
,	O
the	O
attempts	O
to	O
improve	O
paraphrase	B-TaskName
generation	I-TaskName
with	O
a	O
special	O
focus	O
on	O
combining	O
multiple	O
granularity	O
levels	O
also	O
yield	O
good	O
performance	O
(	O
row	O
9	O
,	O
10	O
)	O
.	O
When	O
learning	O
to	O
generate	O
paraphrase	O
in	O
word	O
level	O
,	O
phrase	O
level	O
and	O
sentence	O
level	O
at	O
the	O
same	O
time	O
,	O
their	O
models	O
improve	O
the	O
performance	O
on	O
multiple	O
metrics	O
compared	O
with	O
their	O
backbone	O
Transformer	B-MethodName
model	O
(	O
row	O
5	O
)	O
.	O
Finally	O
,	O
incorporating	O
syntax	O
control	O
into	O
paraphrase	B-TaskName
generation	I-TaskName
will	O
also	O
yield	O
better	O
results	O
at	O
word	O
level	O
and	O
sentence	O
level	O
(	O
row	O
11	O
,	O
12	O
)	O
.	O
Compared	O
with	O
implicit	O
control	O
(	O
row	O
11	O
)	O
,	O
explicit	O
control	O
has	O
a	O
much	O
better	O
performance	O
(	O
row	O
12	O
)	O
based	O
on	O
Quora	O
.	O
It	O
should	O
be	O
noted	O
that	O
most	O
of	O
the	O
works	O
utilize	O
two	O
datasets	O
for	O
experiments	O
(	O
as	O
shown	O
in	O
Table	O
4	O
)	O
with	O
one	O
of	O
them	O
focusing	O
on	O
question	O
paraphrases	O
and	O
the	O
other	O
focusing	O
on	O
general	O
sentence	O
paraphrases	O
.	O
Quora	O
is	O
the	O
most	O
popular	O
dataset	O
for	O
question	O
paraphrases	O
.	O
However	O
,	O
for	O
corpus	O
focusing	O
on	O
general	O
sentnece	O
paraphrases	O
,	O
different	O
works	O
have	O
different	O
choices	O
among	O
MSCOCO	B-DatasetName
,	O
Twitter	O
URL	O
and	O
ParaNMT	O
.	O
MSCOCO	B-DatasetName
is	O
more	O
preferred	O
for	O
less	O
noise	O
compared	O
with	O
Twitter	O
URL	O
and	O
ParaNMT	O
.	O
Therefore	O
,	O
a	O
combination	O
of	O
MSCOCO	B-DatasetName
and	O
Quora	O
is	O
more	O
reasonable	O
.	O
For	O
evaluation	O
metrics	O
,	O
BLEU	B-MetricName
is	O
the	O
most	O
frequently	O
used	O
one	O
.	O
However	O
,	O
as	O
proposed	O
by	O
(	O
Niu	O
et	O
al	O
,	O
2020	O
)	O
(	O
Niu	O
et	O
al	O
,	O
2020	O
)	O
.	O
because	O
of	O
"	O
curse	O
of	O
BLEU	B-MetricName
on	O
paraphrase	O
evaluation	O
"	O
.	O
As	O
shown	O
in	O
Table	O
5	O
,	O
examples	O
with	O
low	O
BLEU	B-MetricName
scores	O
might	O
include	O
both	O
relatively	O
good	O
and	O
bad	O
paraphrasing	O
because	O
BLEU	B-MetricName
scores	O
only	O
measure	O
the	O
overlap	O
between	O
outputs	O
and	O
references	O
.	O
However	O
,	O
a	O
generated	O
paraphrase	O
might	O
still	O
be	O
a	O
good	O
paraphrase	O
even	O
it	O
is	O
not	O
same	O
with	O
the	O
reference	O
.	O
Therefore	O
,	O
for	O
evaluation	O
,	O
it	O
is	O
better	O
to	O
combine	O
automatic	O
evaluation	O
metrics	O
and	O
human	O
evaluation	O
together	O
for	O
a	O
more	O
comprehensive	O
evaluation	O
.	O

Most	O
recent	O
works	O
on	O
multi	O
-	O
level	O
paraphrase	B-TaskName
generation	I-TaskName
only	O
focus	O
on	O
word	O
-	O
level	O
paraphrasing	O
and	O
phrase	O
-	O
level	O
paraphrasing	O
.	O
However	O
,	O
more	O
granularity	O
levels	O
can	O
be	O
incorporated	O
.	O
We	O
believe	O
it	O
is	O
worthwhile	O
to	O
study	O
the	O
combination	O
of	O
various	O
levels	O
,	O
including	O
word	O
-	O
level	O
,	O
phrase	O
-	O
level	O
,	O
syntaxlevel	O
and	O
sentence	O
-	O
level	O
.	O
Transfer	B-TaskName
learning	I-TaskName
With	O
the	O
goal	O
of	O
generating	O
different	O
surfaces	O
of	O
given	O
sentences	O
while	O
preserving	O
the	O
meaning	O
,	O
text	B-TaskName
summarization	I-TaskName
,	O
text	B-TaskName
simplification	I-TaskName
and	O
paraphrase	B-TaskName
generation	I-TaskName
are	O
essentially	O
similar	O
.	O
Therefore	O
,	O
one	O
could	O
utilize	O
transfer	B-TaskName
learning	I-TaskName
of	O
these	O
three	O
tasks	O
to	O
improve	O
the	O
performance	O
.	O
Stylistic	O
paraphrase	B-TaskName
generation	I-TaskName
Currently	O
,	O
word	O
-	O
and	O
phrase	O
-	O
substitution	O
in	O
paraphrase	O
gener	O
-	O
ation	O
can	O
not	O
be	O
carefully	O
controlled	O
.	O
Therefore	O
,	O
it	O
is	O
hard	O
to	O
control	O
the	O
style	O
of	O
generated	O
paraphrases	O
.	O
We	O
believe	O
it	O
is	O
worthwhile	O
to	O
explore	O
methods	O
of	O
incorporating	O
specific	O
styles	O
into	O
generated	O
paraphrases	O
.	O
For	O
instance	O
,	O
by	O
controlling	O
the	O
types	O
of	O
words	O
and	O
phrases	O
,	O
we	O
can	O
incorporate	O
metaphor	O
and	O
idiomatic	O
expressions	O
into	O
paraphrases	O
(	O
Zhou	O
et	O
al	O
,	O
2021b	O
,	O
a	O
)	O
,	O
which	O
could	O
also	O
help	O
to	O
enhance	O
creativity	O
and	O
diversity	O
of	O
generated	O
paraphrases	O
.	O
Evaluation	O
metrics	O
As	O
stated	O
in	O
Section	O
6	O
,	O
BLEU	B-MetricName
scores	O
and	O
other	O
automatic	O
evaluation	O
metrics	O
based	O
on	O
similar	O
principle	O
are	O
not	O
good	O
enough	O
to	O
evaluate	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
Thus	O
there	O
is	O
a	O
need	O
for	O
better	O
automatic	O
evaluation	O
methods	O
.	O
One	O
possible	O
method	O
is	O
to	O
utilize	O
paraphrase	B-TaskName
identification	I-TaskName
in	O
the	O
automatic	O
evaluation	O
metrics	O
to	O
explicitly	O
provide	O
an	O
evaluation	O
of	O
if	O
the	O
generated	O
sentence	O
and	O
input	O
sentence	O
are	O
paraphrases	O
.	O

The	O
vast	O
majority	O
of	O
modern	O
Hebrew	O
texts	O
are	O
written	O
in	O
a	O
letter	O
-	O
only	O
version	O
of	O
the	O
Hebrew	O
script	O
,	O
one	O
which	O
omits	O
the	O
diacritics	O
present	O
in	O
the	O
full	O
diacritized	O
,	O
or	O
dotted	O
variant	O
.	O
1	O
Since	O
most	O
vowels	O
are	O
encoded	O
via	O
diacritics	O
,	O
the	O
pronunciation	O
of	O
words	O
in	O
the	O
text	O
is	O
left	O
underspecified	O
,	O
and	O
a	O
considerable	O
mass	O
of	O
tokens	O
becomes	O
ambiguous	O
.	O
This	O
ambiguity	O
forces	O
readers	O
and	O
learners	O
to	O
infer	O
the	O
intended	O
reading	O
using	O
syntactic	O
and	O
semantic	O
context	O
,	O
as	O
well	O
as	O
common	O
sense	O
(	O
Bentin	O
and	O
Frost	O
,	O
1987	O
;	O
Abu	O
-	O
Rabia	O
,	O
2001	O
)	O
.	O
In	O
NLP	O
systems	O
,	O
recovering	O
such	O
signals	O
is	O
difficult	O
,	O
and	O
indeed	O
their	O
performance	O
on	O
Hebrew	O
tasks	O
is	O
adversely	O
affected	O
by	O
the	O
presence	O
of	O
undotted	O
text	O
(	O
Shacham	O
and	O
Wintner	O
,	O
2007	O
;	O
Goldberg	O
and	O
Elhadad	O
,	O
2010	O
;	O
Tsarfaty	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
an	O
example	O
,	O
the	O
sentence	O
in	O
Table	O
1	O
(	O
a	O
)	O
will	O
be	O
resolved	O
by	O
a	O
typical	O
reader	O
as	O
(	O
b	O
)	O
in	O
most	O
reasonable	O
contexts	O
,	O
knowing	O
that	O
the	O
word	O
"	O
softly	O
"	O
may	O
characterize	O
landings	O
.	O
In	O
contrast	O
,	O
an	O
automatic	O
system	O
processing	O
Hebrew	O
text	O
may	O
not	O
be	O
as	O
sensitive	O
to	O
this	O
kind	O
of	O
grammatical	O
knowledge	O
and	O
instead	O
interpret	O
the	O
undotted	O
token	O
as	O
the	O
more	O
'	O
The	O
plane	O
landed	O
congratulations	O
'	O
Table	O
1	O
:	O
An	O
example	O
of	O
an	O
undotted	O
Hebrew	O
text	O
(	O
a	O
)	O
(	O
written	O
right	O
to	O
left	O
)	O
which	O
can	O
be	O
interpreted	O
in	O
at	O
least	O
two	O
different	O
ways	O
(	O
b	O
,	O
c	O
)	O
,	O
dotted	O
and	O
pronounced	O
differently	O
,	O
but	O
only	O
(	O
b	O
)	O
makes	O
grammatical	O
sense	O
.	O
frequent	O
word	O
in	O
(	O
c	O
)	O
,	O
harming	O
downstream	O
performance	O
.	O
One	O
possible	O
way	O
to	O
overcome	O
this	O
problem	O
is	O
by	O
adding	O
diacritics	O
to	O
undotted	O
text	O
,	O
or	O
dotting	O
,	O
implemented	O
using	O
data	O
-	O
driven	O
algorithms	O
trained	O
on	O
dotted	O
text	O
.	O
Obtaining	O
such	O
data	O
is	O
not	O
trivial	O
,	O
even	O
given	O
correct	O
pronunciation	O
:	O
the	O
standard	O
Tiberian	O
diacritic	O
system	O
contains	O
several	O
sets	O
of	O
identicallyvocalized	O
forms	O
,	O
so	O
while	O
most	O
Hebrew	O
speakers	O
easily	O
read	O
dotted	O
text	O
,	O
they	O
are	O
unable	O
to	O
produce	O
it	O
.	O
Moreover	O
,	O
the	O
process	O
of	O
manually	O
adding	O
diacritics	O
in	O
either	O
handwritten	O
script	O
or	O
through	O
digital	O
input	O
devices	O
is	O
mechanically	O
cumbersome	O
.	O
Thus	O
,	O
the	O
overwhelming	O
majority	O
of	O
modern	O
Hebrew	O
text	O
is	O
undotted	O
,	O
and	O
manually	O
dotting	O
it	O
requires	O
expertise	O
.	O
The	O
resulting	O
scarcity	O
of	O
available	O
dotted	O
text	O
in	O
modern	O
Hebrew	O
contrasts	O
with	O
Biblical	O
and	O
Rabbinical	O
texts	O
which	O
,	O
while	O
dotted	O
,	O
manifest	O
a	O
very	O
different	O
language	O
register	O
.	O
This	O
state	O
of	O
affairs	O
allows	O
individuals	O
and	O
companies	O
to	O
offer	O
dotting	O
as	O
paid	O
services	O
,	O
either	O
by	O
experts	O
or	O
automatically	O
,	O
e.g.	O
the	O
Morfix	O
engine	O
by	O
Melingo	O
.	O
2	O
Such	O
usage	O
practices	O
also	O
force	O
a	O
disconnect	O
in	O
the	O
NLP	O
pipeline	O
,	O
requiring	O
an	O
API	O
call	O
into	O
an	O
external	O
service	O
whose	O
parameters	O
can	O
not	O
be	O
updated	O
.	O
Existing	O
computational	O
approaches	O
to	O
dotting	O
are	O
manifested	O
as	O
complex	O
,	O
multi	O
-	O
resourced	O
systems	O
which	O
perform	O
morphological	B-TaskName
analysis	I-TaskName
on	O
the	O
undotted	O
text	O
and	O
look	O
undotted	O
words	O
up	O
in	O
handcrafted	O
dictionaries	O
as	O
part	O
of	O
the	O
dotting	O
process	O
.	O
Dicta	O
's	O
Nakdan	O
(	O
Shmidman	O
et	O
al	O
,	O
2020	O
)	O
,	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
,	O
applies	O
such	O
methods	O
in	O
addition	O
to	O
applying	O
multiple	O
neural	O
networks	O
over	O
different	O
levels	O
of	O
the	O
text	O
,	O
requiring	O
manual	O
annotation	O
not	O
only	O
for	O
dotting	O
but	O
also	O
for	O
morphology	O
.	O
Among	O
the	O
resources	O
it	O
uses	O
are	O
a	O
diacritized	O
corpus	O
of	O
3	O
M	O
tokens	O
and	O
a	O
POS	O
-	O
tagged	O
corpus	O
of	O
300	O
K	O
tokens	O
.	O
Training	O
the	O
model	O
takes	O
several	O
weeks	O
.	O
3	O
In	O
this	O
work	O
,	O
we	O
set	O
out	O
to	O
simplify	O
the	O
dotting	O
task	O
as	O
much	O
as	O
possible	O
to	O
standard	O
modules	O
.	O
We	O
introduce	O
a	O
large	O
corpus	O
of	O
semi	O
-	O
automatically	O
dotted	O
Hebrew	O
,	O
collected	O
from	O
various	O
sources	O
,	O
and	O
use	O
it	O
to	O
train	O
an	O
RNN	O
-	O
based	O
model	O
.	O
Our	O
system	O
,	O
NAKDIMON	O
,	O
accepts	O
the	O
undotted	O
character	O
sequence	O
as	O
its	O
input	O
,	O
consults	O
no	O
external	O
resources	O
or	O
lexical	O
components	O
,	O
and	O
produces	O
diacritics	O
for	O
each	O
character	O
,	O
resulting	O
in	O
dotted	O
text	O
whose	O
quality	O
is	O
comparable	O
to	O
that	O
of	O
the	O
commercial	O
Morfix	O
,	O
on	O
both	O
character	O
-	O
level	O
and	O
word	O
-	O
level	O
accuracy	B-MetricName
.	O
Our	O
model	O
is	O
easy	O
to	O
integrate	O
within	O
larger	O
systems	O
that	O
perform	O
end	O
-	O
to	O
-	O
end	O
Hebrew	O
processing	O
tasks	O
,	O
as	O
opposed	O
to	O
the	O
existing	O
proprietary	O
dotters	O
.	O
To	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
attempt	O
at	O
a	O
"	O
light	O
"	O
model	O
for	O
Hebrew	O
dotting	O
since	O
early	O
HMM	O
-	O
based	O
systems	O
(	O
Kontorovich	O
,	O
2001	O
;	O
Gal	O
,	O
2002	O
)	O
.	O
We	O
introduce	O
a	O
novel	O
test	O
set	O
for	O
Modern	O
Hebrew	O
dotting	O
,	O
derived	O
from	O
larger	O
and	O
more	O
diverse	O
sources	O
than	O
existing	O
datasets	O
.	O
In	O
experiments	O
over	O
our	O
dataset	O
,	O
we	O
show	O
that	O
our	O
system	O
is	O
particularly	O
useful	O
in	O
the	O
main	O
use	O
case	O
of	O
modern	O
dotting	O
,	O
which	O
is	O
to	O
convey	O
the	O
desired	O
pronunciation	O
to	O
a	O
reader	O
,	O
and	O
that	O
the	O
errors	O
it	O
makes	O
should	O
be	O
more	O
easily	O
detectable	O
by	O
non	O
-	O
professionals	O
than	O
Dicta	O
's	O
.	O
4	O
2	O
Task	O
and	O
Datasets	O

Dotted	O
modern	O
Hebrew	O
text	O
is	O
scarce	O
,	O
since	O
speakers	O
usually	O
read	O
and	O
write	O
undotted	O
text	O
,	O
with	O
the	O
occasional	O
diacritic	O
added	O
for	O
disambiguation	O
when	O
context	O
does	O
not	O
suffice	O
.	O
As	O
we	O
are	O
unaware	O
of	O
legally	O
-	O
obtainable	O
dotted	O
modern	O
corpora	O
,	O
we	O
use	O
a	O
combination	O
of	O
dotted	O
pre	O
-	O
modern	O
texts	O
as	O
well	O
as	O
automatically	O
and	O
semi	O
-	O
automatically	O
dotted	O
modern	O
sources	O
to	O
train	O
NAKDIMON	O
:	O
The	O
PRE	O
-	O
MODERN	B-MethodName
portion	O
is	O
obtained	O
from	O
two	O
main	O
sources	O
:	O
A	O
combination	O
of	O
late	O
pre	O
-	O
modern	O
text	O
from	O
Project	O
Ben	O
-	O
Yehuda	O
,	O
mostly	O
texts	O
from	O
the	O
late	O
19th	O
century	O
and	O
the	O
early	O
20th	O
century	O
;	O
5	O
rabbinical	O
texts	O
from	O
the	O
medieval	O
period	O
,	O
the	O
most	O
important	O
of	O
which	O
is	O
Mishneh	O
Torah	O
(	O
obtained	O
from	O
Project	O
Mamre	O
)	O
;	O
6	O
and	O
23	O
short	O
stories	O
from	O
the	O
short	O
story	O
project	O
.	O
7	O
This	O
portion	O
contains	O
roughly	O
1.81	O
M	O
Hebrew	O
tokens	O
,	O
most	O
of	O
which	O
are	O
dotted	O
,	O
with	O
a	O
varying	O
level	O
of	O
accuracy	B-MetricName
,	O
varying	O
dotting	O
styles	O
,	O
and	O
varying	O
degree	O
of	O
similarity	O
to	O
Modern	O
Hebrew	O
.	O
The	O
AUTOMATIC	O
portion	O
contains	O
547	O
short	O
stories	O
taken	O
from	O
the	O
short	O
story	O
project	O
.	O
The	O
stories	O
are	O
dotted	O
using	O
Dicta	O
without	O
manual	O
validation	O
.	O
The	O
corpus	O
contains	O
roughly	O
1.27	O
M	O
Hebrew	O
tokens	O
.	O
Lastly	O
,	O
the	O
MODERN	B-MethodName
portion	O
contains	O
manually	O
collected	O
text	O
in	O
Modern	O
Hebrew	O
,	O
mostly	O
from	O
undotted	O
sources	O
,	O
which	O
we	O
dot	O
using	O
Dicta	O
and	O
follow	O
up	O
by	O
manually	O
fixing	O
errors	O
,	O
either	O
using	O
Dicta	O
's	O
API	O
or	O
via	O
automated	O
scripts	O
which	O
catch	O
common	O
mistakes	O
.	O
We	O
made	O
an	O
effort	O
to	O
collect	O
a	O
diverse	O
set	O
of	O
sources	O
:	O
news	O
,	O
opinion	O
columns	O
,	O
paragraphs	O
from	O
books	O
,	O
short	O
stories	O
,	O
Wikipedia	O
articles	O
,	O
governmental	O
publications	O
,	O
blog	O
posts	O
and	O
forums	O
expressing	O
various	O
domains	O
and	O
voices	O
,	O
and	O
more	O
.	O
Our	O
MODERN	B-MethodName
corpus	O
contains	O
roughly	O
326	O
K	O
Hebrew	O
tokens	O
,	O
and	O
is	O
much	O
more	O
consistent	O
and	O
similar	O
to	O
the	O
expectation	O
of	O
a	O
native	O
Hebrew	O
speaker	O
than	O
the	O
PRE	O
-	O
MODERN	B-MethodName
or	O
the	O
AUTOMATIC	O
corpora	O
,	O
and	O
more	O
accurately	O
dotted	O
than	O
the	O
AU	O
-	O
TOMATIC	O
corpus	O
.	O
The	O
sources	O
and	O
statistics	O
of	O
this	O
dataset	O
are	O
presented	O
in	O
Table	O
2	O
.	O

Shmidman	O
et	O
al	O
(	O
2020	O
)	O
provide	O
a	O
benchmark	O
dataset	O
for	O
dotting	O
modern	O
Hebrew	O
documents	O
.	O
However	O
,	O
it	O
is	O
relatively	O
small	O
and	O
non	O
-	O
diverse	O
:	O
all	O
22	O
documents	O
in	O
the	O
dataset	O
originate	O
in	O
a	O
single	O
source	O
,	O
namely	O
Hebrew	O
Wikipedia	O
articles	O
.	O
Therefore	O
,	O
we	O
created	O
a	O
new	O
test	O
set	O
8	O
from	O
a	O
larger	O
variety	O
of	O
texts	O
,	O
including	O
high	O
-	O
quality	O
Wikipedia	O
articles	O
and	O
edited	O
news	O
stories	O
,	O
as	O
well	O
as	O
user	O
-	O
generated	O
blog	O
posts	O
.	O
This	O
set	O
consists	O
of	O
ten	O
documents	O
from	O
each	O
of	O
eleven	O
sources	O
(	O
5x	O
Dicta	O
's	O
test	O
set	O
)	O
,	O
and	O
totals	O
20	O
,	O
474	O
Hebrew	O
tokens	O
,	O
roughly	O
3.5x	O
Dicta	O
's	O
.	O
We	O
use	O
the	O
same	O
technique	O
and	O
style	O
for	O
dotting	O
this	O
corpus	O
as	O
we	O
do	O
for	O
the	O
MODERN	B-MethodName
corpus	O
(	O
2.2	O
)	O
,	O
but	O
the	O
documents	O
were	O
3	O
Nakdimon	O
NAKDIMON	O
embeds	O
the	O
input	O
characters	O
and	O
passes	O
them	O
through	O
a	O
two	O
-	O
layer	O
Bi	O
-	O
LSTM	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
.	O
The	O
LSTM	B-MethodName
output	O
is	O
fed	O
into	O
a	O
single	O
linear	B-MethodName
layer	I-MethodName
,	O
which	O
then	O
feeds	O
three	O
linear	O
layers	O
,	O
one	O
for	O
each	O
diacritic	O
category	O
(	O
see	O
2	O
)	O
.	O
Each	O
character	O
then	O
receives	O
a	O
prediction	O
for	O
each	O
category	O
independently	O
and	O
all	O
predicted	O
marks	O
are	O
added	O
to	O
it	O
as	O
output	O
.	O
Decoding	O
is	O
performed	O
greedily	O
,	O
with	O
no	O
validation	O
of	O
readability	O
or	O
any	O
other	O
dependence	O
between	O
character	O
-	O
level	O
decisions	O
.	O
The	O
input	O
is	O
pre	O
-	O
processed	O
by	O
removing	O
all	O
but	O
Hebrew	O
characters	O
,	O
spaces	O
and	O
punctuation	O
;	O
digits	O
are	O
converted	O
to	O
a	O
dedicated	O
symbol	O
,	O
as	O
are	O
Latin	O
characters	O
.	O
All	O
existing	O
diacritic	O
marks	O
are	O
stripped	O
,	O
and	O
each	O
document	O
is	O
split	O
into	O
chunks	O
bounded	O
at	O
whitespace	O
,	O
ignoring	O
sentence	O
boundaries	O
.	O
We	O
train	O
NAKDIMON	O
first	O
over	O
PRE	O
-	O
MODERN	B-MethodName
,	O
then	O
over	O
the	O
AUTOMATIC	O
corpus	O
,	O
and	O
then	O
by	O
over	O
the	O
MODERN	B-MethodName
corpus	O
.	O
During	O
training	O
,	O
the	O
loss	B-MetricName
is	O
the	O
sum	O
of	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
from	O
all	O
three	O
categories	O
.	O
Trivial	O
decisions	O
,	O
such	O
as	O
the	O
label	O
for	O
the	O
shin	O
/	O
sin	O
diacritic	O
for	O
any	O
non	O
-	O
‫ש‬	O
letter	O
,	O
are	O
masked	O
.	O
Tuning	O
experiments	O
are	O
detailed	O
in	O
Appendix	O
B	O
;	O
an	O
evaluation	O
of	O
a	O
preliminary	O
version	O
of	O
NAKDI	O
-	O
MON	O
over	O
the	O
Dicta	O
test	O
set	O
is	O
in	O
Appendix	O
C	O
,	O
and	O
Hyperparameters	O
are	O
detailed	O
in	O
Appendix	O
D.	O

We	O
compare	O
the	O
performance	O
of	O
NAKDIMON	O
on	O
our	O
new	O
test	O
set	O
(	O
2.3	O
)	O
against	O
Dicta	O
,	O
9	O
Snopi	O
,	O
10	O
and	O
Morfix	O
(	O
Kamir	O
et	O
al	O
,	O
2002	O
)	O
.	O
as	O
well	O
as	O
a	O
MA	O
-	O
JORITY	O
baseline	O
which	O
returns	O
the	O
most	O
common	O
dotting	O
for	O
each	O
word	O
seen	O
in	O
our	O
full	O
training	O
set	O
.	O
Metrics	O
We	O
report	O
four	O
metrics	O
:	O
decision	O
accuracy	B-MetricName
(	O
DEC	O
)	O
is	O
computed	O
over	O
the	O
entire	O
set	O
of	O
individual	O
possible	O
decisions	O
:	O
dagesh	O
/	O
mappiq	O
for	O
letters	O
that	O
allow	O
it	O
,	O
sin	O
/	O
shin	O
dot	O
for	O
the	O
letter	O
‫	O
,	O
ש‬	O
and	O
all	O
other	O
diacritics	O
for	O
letters	O
that	O
allow	O
them	O
;	O
character	O
accuracy	B-MetricName
(	O
CHA	O
)	O
is	O
the	O
portion	O
of	O
characters	O
in	O
the	O
text	O
that	O
end	O
up	O
in	O
their	O
intended	O
final	O
form	O
(	O
which	O
may	O
combine	O
two	O
or	O
three	O
decisions	O
,	O
e.g.	O
dagesh	O
+	O
vowel	O
)	O
;	O
word	O
accuracy	B-MetricName
(	O
WOR	O
)	O
is	O
the	O
portion	O
of	O
words	O
with	O
no	O
mistakes	O
;	O
and	O
vocalization	O

We	O
provide	O
document	O
-	O
level	O
macro	O
-	O
averaged	O
accuracy	B-MetricName
percentage	O
results	O
for	O
a	O
single	O
run	O
over	O
our	O
test	O
set	O
in	O
Table	O
3	O
.	O
All	O
systems	O
,	O
except	O
Snopi	O
,	O
substantially	O
outperform	O
the	O
majority	O
-	O
dotting	O
baseline	O
on	O
all	O
metrics	O
.	O
NAKDIMON	O
outperforms	O
Morfix	O
on	O
character	O
-	O
level	O
metrics	O
but	O
not	O
on	O
word	O
-	O
level	O
metrics	O
,	O
mostly	O
since	O
Morfix	O
ignores	O
certain	O
words	O
altogether	O
,	O
incurring	O
errors	O
on	O
multiple	O
characters	O
.	O
We	O
note	O
the	O
substantial	O
improvement	O
our	O
model	O
achieves	O
on	O
the	O
VOC	O
metric	O
compared	O
to	O
the	O
WOR	O
metric	O
:	O
18.43	O
%	O
of	O
word	O
-	O
level	O
errors	O
are	O
attributable	O
to	O
vocalization	O
-	O
agnostic	O
dotting	O
,	O
compared	O
to	O
13.80	O
%	O
for	O
Dicta	O
and	O
10.41	O
%	O
for	O
Snopi	O
(	O
but	O
20.91	O
%	O
for	O
Morfix	O
)	O
.	O
Considering	O
that	O
the	O
central	O
use	O
case	O
for	O
dotting	O
modern	O
Hebrew	O
text	O
is	O
to	O
facilitate	O
pronunciation	O
to	O
learners	O
and	O
for	O
reading	O
,	O
and	O
that	O
undotted	O
homograph	O
ambiguity	O
typically	O
comes	O
with	O
pronunciation	O
differences	O
,	O
we	O
believe	O
this	O
measure	O
to	O
be	O
no	O
less	O
important	O
than	O
WOR	O
.	O
Results	O
on	O
Dicta	O
's	O
test	O
set	O
(	O
Shmidman	O
et	O
al	O
,	O
2020	O
)	O
are	O
presented	O
in	O
Appendix	O
C.	O

Existing	O
work	O
on	O
diacritizing	O
Hebrew	O
is	O
not	O
common	O
,	O
and	O
all	O
efforts	O
build	O
on	O
word	O
-	O
level	O
features	O
.	O
Kontorovich	O
(	O
2001	O
)	O
trains	O
an	O
HMM	O
on	O
a	O
vocalized	O
and	O
morphologically	O
-	O
tagged	O
portion	O
of	O
the	O
Hebrew	O
Bible	O
containing	O
30	O
,	O
743	O
words	O
,	O
and	O
evaluates	O
the	O
result	O
on	O
a	O
test	O
set	O
containing	O
2	O
,	O
852	O
words	O
,	O
achieving	O
81	O
%	O
WOR	O
accuracy	B-MetricName
.	O
Note	O
that	O
Biblical	O
Hebrew	O
is	O
very	O
different	O
from	O
Modern	O
Hebrew	O
in	O
both	O
vocabulary	O
,	O
grammatical	O
structure	O
,	O
and	O
diacritization	O
,	O
and	O
also	O
has	O
many	O
words	O
with	O
unique	O
diacritization	O
.	O
In	O
our	O
system	O
,	O
we	O
exclude	O
the	O
Bible	O
altogether	O
from	O
the	O
training	O
set	O
,	O
as	O
its	O
inclusion	O
actively	O
hurts	O
performance	O
on	O
the	O
validation	O
set	O
,	O
which	O
consists	O
of	O
Modern	O
Hebrew	O
.	O
Tomer	O
(	O
2012	O
)	O
designs	O
a	O
diacritization	O
system	O
for	O
Hebrew	O
verbs	O
consisting	O
of	O
a	O
combination	O
of	O
a	O
verb	O
inflection	O
system	O
,	O
a	O
syllable	O
boundary	O
detector	O
,	O
and	O
an	O
SVM	B-MethodName
model	O
for	O
classifying	O
verb	O
inflection	O
paradigms	O
.	O
The	O
focus	O
on	O
verbs	O
in	O
a	O
type	O
-	O
level	O
setup	O
makes	O
this	O
work	O
incomparable	O
to	O
ours	O
or	O
to	O
others	O
in	O
this	O
survey	O
.	O
In	O
Arabic	O
,	O
diacritization	O
serves	O
a	O
comparable	O
purpose	O
to	O
that	O
in	O
Hebrew	O
,	O
but	O
not	O
exclusively	O
:	O
most	O
diacritic	O
marks	O
differentiate	O
consonantal	O
phonemes	O
from	O
each	O
other	O
,	O
e.g.	O
/b/	O
vs.	O
/t/	O
(	O
which	O
only	O
the	O
sin	O
/	O
shin	O
dot	O
does	O
in	O
Hebrew	O
)	O
,	O
whereas	O
vocalization	O
marks	O
are	O
in	O
a	O
one	O
-	O
to	O
-	O
one	O
relationship	O
with	O
their	O
phonetic	O
realizations	O
,	O
e.g.	O
only	O
the	O
fatha	O
as	O
in	O
/ba/	O
encodes	O
the	O
/a/	O
vowel	O
.	O
Dictionary	O
-	O
less	O
Arabic	O
diacritization	O
has	O
been	O
attempted	O
using	O
a	O
3	O
-	O
layer	O
Bi	O
-	O
LSTM	B-MethodName
(	O
Belinkov	O
and	O
Glass	O
,	O
2015	O
)	O
.	O
Abandah	O
et	O
al	O
(	O
2015	O
)	O
use	O
a	O
Bi	O
-	O
LSTM	B-MethodName
where	O
characters	O
are	O
assigned	O
either	O
one	O
or	O
more	O
diacritic	O
symbols	O
.	O
Our	O
system	O
differs	O
from	O
theirs	O
by	O
virtue	O
of	O
separating	O
the	O
diacritization	O
categories	O
.	O
Mubarak	O
et	O
al	O
(	O
2019	O
)	O
tackled	O
Arabic	O
diacritization	O
as	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
problem	O
,	O
tasking	O
the	O
model	O
with	O
reproducing	O
the	O
characters	O
as	O
well	O
as	O
the	O
marks	O
.	O
Zalmout	O
and	O
Habash	O
(	O
2017	O
)	O
have	O
made	O
the	O
case	O
against	O
RNN	O
-	O
only	O
systems	O
,	O
arguing	O
for	O
the	O
importance	O
of	O
morphological	O
analyzers	O
in	O
Arabic	O
NLP	O
systems	O
.	O
We	O
concede	O
that	O
well	O
-	O
curated	O
systems	O
may	O
perform	O
better	O
than	O
uncurated	O
ones	O
,	O
particularly	O
on	O
low	O
-	O
resource	O
languages	O
such	O
as	O
Hebrew	O
,	O
but	O
we	O
note	O
that	O
they	O
are	O
difficult	O
to	O
train	O
for	O
individual	O
use	O
cases	O
and	O
are	O
burdensome	O
to	O
incorporate	O
within	O
larger	O
systems	O
.	O
Diacritics	O
restoration	O
in	O
Latin	O
-	O
based	O
scripts	O
,	O
applicable	O
mostly	O
to	O
European	O
languages	O
,	O
forms	O
a	O
substantially	O
different	O
problem	O
from	O
the	O
one	O
in	O
Hebrew	O
given	O
the	O
highly	O
lexicalized	O
nature	O
of	O
diacritic	O
usage	O
in	O
these	O
languages	O
and	O
the	O
very	O
low	O
rate	O
of	O
characters	O
requiring	O
diacritics	O
.	O
The	O
state	O
-	O
of	O
-	O
theart	O
systems	O
in	O
such	O
languages	O
employ	O
transformer	O
models	O
in	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
setup	O
(	O
Náplava	O
et	O
al	O
,	O
2021	O
;	O
Stankevičius	O
et	O
al	O
,	O
2022	O
)	O
,	O
supplanting	O
character	O
-	O
RNN	O
sequence	O
prediction	O
architectures	O
reminiscent	O
of	O
ours	O
(	O
Náplava	O
et	O
al	O
,	O
2018	O
)	O
.	O
Indeed	O
,	O
the	O
authors	O
of	O
this	O
latter	O
work	O
note	O
the	O
only	O
non	O
-	O
European	O
in	O
their	O
dataset	O
,	O
Vietnamese	O
,	O
as	O
a	O
special	O
outlier	O
.	O

We	O
tried	O
to	O
further	O
improve	O
NAKDIMON	O
by	O
initializing	O
its	O
parameters	O
from	O
a	O
language	O
model	O
trained	O
to	O
predict	O
masked	O
characters	O
in	O
a	O
large	O
undotted	O
Wikipedia	O
corpus	O
(	O
440	O
MB	O
,	O
30	O
%	O
mask	O
rate	O
)	O
,	O
but	O
were	O
only	O
able	O
to	O
achieve	O
an	O
improvement	O
of	O
0.07	O
%	O
.	O
Attempted	O
architectural	O
modifications	O
,	O
including	O
substituting	O
a	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
for	O
the	O
LSTM	B-MethodName
;	O
adding	O
a	O
CRF	B-MethodName
layer	O
to	O
the	O
decoding	O
process	O
;	O
and	O
adding	O
a	O
residual	B-MethodName
connection	I-MethodName
between	O
the	O
character	O
LSTM	B-MethodName
layers	O
,	O
yielded	O
no	O
substantial	O
benefits	O
in	O
these	O
experiments	O
.	O
Similarly	O
,	O
varying	O
the	O
number	O
of	O
LSTM	B-MethodName
layers	O
between	O
2	O
and	O
5	O
(	O
keeping	O
the	O
total	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
roughly	O
constant	O
,	O
close	O
to	O
the	O
5	O
,	O
313	O
,	O
223	O
parameters	O
of	O
our	O
final	O
model	O
)	O
had	O
little	O
to	O
no	O
impact	O
on	O
the	O
accuracy	B-MetricName
on	O
the	O
validation	O
set	O
.	O

Speech	O
translation	O
(	O
ST	O
)	O
involves	O
translating	O
the	O
acoustic	O
speech	O
signals	O
in	O
the	O
source	O
language	O
into	O
the	O
words	O
in	O
the	O
target	O
language	O
.	O
We	O
use	O
it	O
to	O
evaluate	O
the	O
semantic	O
capability	O
of	O
SSL	B-DatasetName
models	O
,	O
and	O
how	O
they	O
benefit	O
the	O
translation	O
task	O
.	O
We	O
use	O
the	O
CoVoST2	O
EnÑDe	O
(	O
Wang	O
et	O
al	O
,	O
2020	O
)	O
dataset	O
(	O
CC0	O
Licensed	O
)	O
with	O
their	O
official	O
train	O
,	O
validation	O
,	O
and	O
test	O
splits	O
while	O
removing	O
all	O
the	O
samples	O
containing	O
"	O
REMOVE	O
"	O
,	O
resulting	O
in	O
425.8	O
,	O
25.9	O
and	O
24.5	O
hours	O
respectively	O
.	O
For	O
text	O
,	O
we	O
keep	O
original	O
case	O
,	O
normalize	O
punctuation	O
,	O
and	O
build	O
character	O
vocabulary	O
with	O
100	O
%	O
train	O
-	O
set	O
coverage	O
.	O
We	O
report	O
case	O
-	O
sensitive	O
de	O
-	O
tokenized	O
BLEU	B-MetricName
using	O
sacreBLEU	B-MetricName
(	O
Post	O
,	O
2018	O
)	O
.	O
Our	O
downstream	O
model	O
has	O
an	O
encoder	O
-	O
decoder	O
architecture	O
with	O
3	O
layers	O
of	O
Transformers	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
each	O
with	O
hidden	O
dimension	O
of	O
512	O
.	O
A	O
convolutional	O
subsampler	O
is	O
used	O
to	O
reduce	O
the	O
sequence	O
length	O
of	O
the	O
input	O
before	O
feeding	O
it	O
to	O
the	O
encoder	O
.	O
We	O
train	O
our	O
model	O
with	O
label	O
-	O
smoothing	O
using	O
a	O
probability	O
of	O
0.1	O
.	O
A	O
beam	O
size	O
of	O
20	O
is	O
used	O
for	O
inference	O
.	O

Although	O
an	O
ASR	O
is	O
included	O
in	O
SUPERB	O
,	O
it	O
only	O
examines	O
SSL	B-DatasetName
models	O
on	O
read	O
English	O
corpus	O
Lib	O
-	O
riSpeech	O
(	O
Panayotov	O
et	O
al	O
,	O
2015	O
)	O
.	O
Therefore	O
,	O
we	O
introduce	O
out	O
-	O
of	O
-	O
domain	O
ASR	O
(	O
OOD	O
-	O
ASR	O
)	O
,	O
which	O
aims	O
to	O
evaluate	O
the	O
models	O
'	O
capabilities	O
across	O
languages	O
,	O
and	O
out	O
-	O
of	O
-	O
domain	O
scenarios	O
.	O
The	O
OOD	O
-	O
ASR	O
tasks	O
are	O
categorized	O
into	O
cross	O
-	O
lingual	O
and	O
spontaneous	O
speech	O
tasks	O
.	O
For	O
the	O
cross	O
-	O
lingual	O
tasks	O
,	O
we	O
choose	O
the	O
Mexican	O
Spanish	O
(	O
es	O
)	O
,	O
Mandarin	O
(	O
zh	O
)	O
,	O
and	O
Arabic	O
(	O
ar	O
)	O
subsets	O
from	O
Common	B-DatasetName
Voice	I-DatasetName
7.0	I-DatasetName
(	O
Ardila	O
et	O
al	O
,	O
2020	O
)	O
(	O
CC0	O
Licensed	O
)	O
containing	O
21.5	O
,	O
31.2	O
,	O
and	O
30.7	O
hours	O
of	O
training	O
data	O
respectively	O
.	O
The	O
validation	O
set	O
sizes	O
are	O
1.2	O
hours	O
,	O
14.4	O
hours	O
and	O
12.24	O
hours	O
,	O
and	O
the	O
test	O
set	O
sizes	O
are	O
0.6	O
hour	O
,	O
15.3	O
hours	O
and	O
12.5	O
hours	O
for	O
es	O
,	O
zh	O
and	O
ar	O
respectively	O
.	O
For	O
the	O
spontaneous	O
speech	O
task	O
(	O
spon	O
)	O
,	O
we	O
use	O
the	O
Santa	O
Barbara	O
Corpus	O
of	O
Spoken	O
American	O
English	O
(	O
SBCSAE	O
)	O
(	O
Du	O
Bois	O
et	O
al	O
,	O
2000	O
-	O
2005	O
(	O
CC	O
BY	O
-	O
ND	O
3.0	O
Licensed	O
)	O
,	O
consisting	O
of	O
60	O
conversations	O
over	O
different	O
topics	O
spanning	O
16.7	O
hours	O
of	O
data	O
.	O
The	O
validation	O
and	O
test	O
set	O
sizes	O
are	O
1.6	O
hours	O
and	O
2.2	O
hours	O
respectively	O
.	O
For	O
evaluation	O
,	O
we	O
use	O
word	B-MetricName
error	I-MetricName
rate	I-MetricName
(	I-MetricName
WER	I-MetricName
)	I-MetricName
as	O
the	O
metric	O
except	O
for	O
Mandarin	O
which	O
character	O
error	O
rate	O
(	O
CER	O
)	O
is	O
used	O
.	O
The	O
error	O
rates	O
are	O
averaged	O
across	O
all	O
sub	O
-	O
tasks	O
to	O
offer	O
an	O
overall	O
score	O
.	O
The	O
ASR	O
model	O
is	O
a	O
2	O
-	O
layer	O
BLSTM	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
with	O
hidden	O
states	O
of	O
1024	O
dimension	O
.	O
The	O
training	O
objective	O
is	O
to	O
minimize	O
the	O
Connectionist	O
Temporal	O
Classification	B-TaskName
(	O
CTC	B-DatasetName
)	O
loss	B-MetricName
(	O
Graves	O
et	O
al	O
,	O
2006	O
)	O
.	O
During	O
inference	O
,	O
we	O
use	O
CTC	B-DatasetName
greedy	O
decoding	O
without	O
language	O
model	O
re	O
-	O
scoring	O
to	O
simplify	O
the	O
process	O
and	O
to	O
highlight	O
the	O
impact	O
of	O
the	O
learned	O
acoustic	O
representations	O
.	O

For	O
voice	B-TaskName
conversion	I-TaskName
(	O
VC	O
)	O
,	O
we	O
consider	O
the	O
intralingual	O
VC	O
task	O
in	O
VCC2020	O
(	O
Zhao	O
et	O
al	O
,	O
2020	O
)	O
For	O
the	O
pretraining	O
methods	O
,	O
we	O
abbreviate	O
"	O
vector	O
quantization	B-TaskName
"	O
as	O
VQ	O
,	O
"	O
future	O
"	O
as	O
F	O
,	O
"	O
masked	O
"	O
as	O
M	O
,	O
"	O
generation	O
"	O
as	O
G	O
,	O
"	O
contrastive	O
discrimination	O
"	O
as	O
C	O
,	O
and	O
"	O
token	O
prediction	O
/	O
classification	O
"	O
as	O
P.	O
Parameters	O
for	O
both	O
pretraining	O
and	O
inference	O
are	O
counted	O
.	O
(	O
ODbL	O
Licensed	O
)	O
under	O
the	O
any	O
-	O
to	O
-	O
one	O
(	O
A2O	O
)	O
setting	O
.	O
A2O	O
VC	O
aims	O
to	O
convert	O
speech	O
from	O
any	O
arbitrary	O
speaker	O
into	O
that	O
of	O
a	O
predefined	O
target	O
speaker	O
.	O
We	O
use	O
the	O
task	O
to	O
evaluate	O
the	O
speaker	O
transferability	O
as	O
well	O
as	O
the	O
generalizability	O
of	O
the	O
SSL	B-DatasetName
models	O
.	O
We	O
use	O
60	O
utterances	O
from	O
the	O
target	O
speaker	O
that	O
spans	O
5	O
minutes	O
for	O
training	O
,	O
and	O
25	O
utterances	O
for	O
testing	O
that	O
span	O
2	O
minutes	O
.	O
No	O
validation	O
set	O
was	O
used	O
.	O
We	O
use	O
the	O
commonly	O
used	O
mel	O
-	O
cepstrum	O
distortion	O
(	O
MCD	O
)	O
,	O
word	B-MetricName
error	I-MetricName
rate	I-MetricName
(	I-MetricName
WER	I-MetricName
)	I-MetricName
and	O
automatic	O
speaker	B-TaskName
verification	I-TaskName
(	O
ASV	O
)	O
accept	O
rate	O
from	O
off	O
-	O
the	O
-	O
shelf	O
ASR	O
and	O
ASV	O
models	O
as	O
evaluation	O
metrics	O
.	O
The	O
downstream	O
model	O
is	O
trained	O
to	O
reconstruct	O
the	O
acoustic	O
feature	O
from	O
the	O
upstream	O
representations	O
in	O
a	O
target	O
-	O
speaker	O
-	O
dependent	O
manner	O
.	O
In	O
the	O
conversion	O
phase	O
,	O
given	O
the	O
representations	O
extracted	O
by	O
the	O
upstream	O
,	O
the	O
model	O
generates	O
the	O
converted	O
acoustic	O
features	O
,	O
which	O
are	O
then	O
sent	O
to	O
a	O
neural	O
vocoder	O
to	O
synthesize	O
the	O
converted	O
waveform	O
.	O
We	O
adopted	O
Tacotron2	B-MethodName
(	O
Shen	O
et	O
al	O
,	O
2018	O
)	O
as	O
the	O
downstream	O
model	O
,	O
which	O
is	O
an	O
autoregressive	O
network	O
consisting	O
of	O
convolutional	O
and	O
LSTM	B-MethodName
layers	O
.	O
For	O
the	O
neural	O
vocoder	O
,	O
we	O
used	O
the	O
Hifi	B-MethodName
-	I-MethodName
GAN	I-MethodName
(	O
Kong	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
follow	O
an	O
implementation	O
described	O
in	O
(	O
Huang	O
et	O
al	O
,	O
2021b	O
)	O
.	O

For	O
each	O
task	O
,	O
2	O
additional	O
downstream	O
architectures	O
are	O
created	O
by	O
modifying	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
and	O
the	O
hidden	O
dimensions	O
compared	O
to	O
our	O
default	O
setting	O
.	O
We	O
create	O
small	O
and	O
large	O
models	O
that	O
are	O
roughly	O
the	O
half	O
and	O
twice	O
of	O
default	O
in	O
terms	O
of	O
the	O
number	O
of	O
trainable	O
parameters	O
.	O
A	O
detailed	O
comparison	O
of	O
the	O
downstream	O
architectures	O
is	O
shown	O
in	O
Table	O
5	O
.	O
The	O
results	O
are	O
shown	O
in	O
Table	O
6	O
.	O
We	O
show	O
that	O
the	O
ranking	O
of	O
the	O
upstream	O
models	O
is	O
almost	O
fixed	O
when	O
the	O
model	O
sizes	O
are	O
varied	O
.	O
As	O
expected	O
,	O
the	O
small	O
architecture	O
has	O
worse	O
perfor	O
-	O
mance	O
than	O
default	O
,	O
while	O
large	O
has	O
better	O
.	O
Moreover	O
,	O
the	O
scores	O
causing	O
the	O
change	O
in	O
ranking	O
are	O
negligible	O
,	O
e.g.	O
,	O
TERA	O
/	O
CPC	O
in	O
SS	O
and	O
wav2vec	O
2.0	O
Base	O
/	O
HuBERT	O
Base	O
in	O
OOD	O
-	O
ASR	O
with	O
large	O
.	O
The	O
results	O
show	O
that	O
the	O
relative	O
performance	O
achieved	O
by	O
different	O
upstream	O
models	O
is	O
agnostic	O
to	O
the	O
downstream	O
architecture	O
,	O
confirming	O
the	O
robustness	O
of	O
the	O
framework	O
used	O
in	O
SUPERB	O
-	O
SG	O
.	O

Yes	O
,	O
we	O
cited	O
those	O
artifacts	O
properly	O
in	O
Section	O
3	O
.	O
B.4.2	O
Did	O
you	O
discuss	O
the	O
license	O
or	O
terms	O
for	O
use	O
and/or	O
distribution	O
of	O
any	O
artifacts	O
?	O
Yes	O
,	O
the	O
licenses	O
of	O
the	O
artifacts	O
are	O
clearly	O
indicated	O
in	O
Section	O
3	O
.	O
B.4.3	O
Did	O
you	O
discuss	O
if	O
your	O
use	O
of	O
existing	O
artifact	O
(	O
s	O
)	O
was	O
consistent	O
with	O
their	O
intended	O
use	O
,	O
provided	O
that	O
it	O
was	O
specified	O
?	O
For	O
the	O
artifacts	O
you	O
create	O
,	O
do	O
you	O
specify	O
intended	O
use	O
and	O
whether	O
that	O
is	O
compatible	O
with	O
the	O
original	O
access	O
conditions	O
(	O
in	O
particular	O
,	O
derivatives	O
of	O
data	O
accessed	O
for	O
research	O
purposes	O
should	O
not	O
be	O
used	O
outside	O
of	O
research	O
contexts	O
)	O
?	O
Yes	O
,	O
we	O
use	O
the	O
official	O
implementations	O
of	O
the	O
upstream	O
models	O
in	O
Table	O
1	O
and	O
followed	O
their	O
public	O
API	O
to	O
access	O
the	O
models	O
.	O
For	O
the	O
datasets	O
,	O
we	O
also	O
follow	O
their	O
licenses	O
.	O
B.4.4	O
Did	O
you	O
discuss	O
the	O
steps	O
taken	O
to	O
check	O
whether	O
the	O
data	O
that	O
was	O
collected	O
/	O
used	O
contains	O
any	O
information	O
that	O
names	O
or	O
uniquely	O
identifies	O
individual	O
people	O
or	O
offensive	O
content	O
,	O
and	O
the	O
steps	O
taken	O
to	O
protect	O
/	O
anonymize	O
it	O
?	O
No	O
,	O
there	O
were	O
no	O
data	O
collection	O
involved	O
in	O
this	O
work	O
.	O
We	O
used	O
the	O
widely	O
-	O
used	O
public	O
datasets	O
and	O
followed	O
the	O
common	O
data	O
preprocessing	O
steps	O
.	O
B.4.5	O
Did	O
you	O
provide	O
documentation	O
of	O
the	O
artifacts	O
,	O
e.g.	O
,	O
coverage	O
of	O
domains	O
,	O
languages	O
,	O
and	O
linguistic	O
phenomena	O
,	O
demographic	O
groups	O
represented	O
,	O
etc	O
.	O
?	O
Yes	O
,	O
the	O
properties	O
of	O
the	O
artifacts	O
were	O
indicated	O
in	O
Section	O
3	O
.	O
Yes	O
.	O
B.5.1	O
Did	O
you	O
report	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
in	O
the	O
models	O
used	O
,	O
the	O
total	O
computational	O
budget	O
(	O
e.g.	O
,	O
GPU	O
hours	O
)	O
,	O
and	O
computing	O
infrastructure	O
used	O
?	O
We	O
reported	O
the	O
number	O
of	O
the	O
parameters	O
in	O
Section	O
5.3.1	O
.	O
The	O
computational	O
budget	O
and	O
computing	O
infrastructures	O
are	O
reported	O
in	O
Table	O
12	O
.	O
B.5.2	O
Did	O
you	O
discuss	O
the	O
experimental	O
setup	O
,	O
including	O
hyperparameter	O
search	O
and	O
best	O
-	O
found	O
hyperparameter	O
values	O
?	O
No	O
,	O
we	O
did	O
n't	O
do	O
the	O
hyperparameter	O
searching	O
in	O
a	O
unified	O
way	O
.	O
Some	O
hyperparameters	O
came	O
from	O
the	O
official	O
implementation	O
or	O
related	O
works	O
and	O
some	O
were	O
searched	O
by	O
ourselves	O
.	O
However	O
,	O
the	O
hyperparameters	O
we	O
used	O
are	O
public	O
available	O
1	O
.	O
B.5.3	O
Did	O
you	O
report	O
descriptive	O
statistics	O
about	O
your	O
results	O
(	O
e.g.	O
,	O
error	O
bars	O
around	O
results	O
,	O
summary	O
statistics	O
from	O
sets	O
of	O
experiments	O
)	O
,	O
and	O
is	O
it	O
transparent	O
whether	O
you	O
are	O
reporting	O
the	O
max	O
,	O
mean	O
,	O
etc	O
.	O
or	O
just	O
a	O
single	O
run	O
?	O
Yes	O
,	O
we	O
indicated	O
that	O
in	O
Section	O
4	O
.	O
B.5.4	O
If	O
you	O
used	O
existing	O
packages	O
(	O
e.g.	O
,	O
for	O
preprocessing	O
,	O
for	O
normalization	O
,	O
or	O
for	O
evaluation	O
)	O
,	O
did	O
you	O
report	O
the	O
implementation	O
,	O
model	O
,	O
and	O
parameter	O
settings	O
used	O
(	O
e.g.	O
,	O
NLTK	O
,	O
Spacy	O
,	O
ROUGE	O
,	O
etc	O
.	O
)	O
?	O
Yes	O
,	O
we	O
reported	O
them	O
in	O
Section	O
3	O
.	O
B.6	O
Did	O
you	O
use	O
human	O
annotators	O
(	O
e.g.	O
,	O
crowdworkers	O
)	O
or	O
research	O
with	O
human	O
subjects	O
?	O
No	O
.	O

The	O
input	O
consists	O
of	O
dialogue	O
context	O
of	O
t−1	O
turns	O
,	O
each	O
including	O
a	O
pair	O
of	O
user	O
utterance	O
U	O
and	O
system	O
response	O
R	O
,	O
(	O
U	O
1	O
,	O
R	O
1	O
)	O
,	O
...	O
,	O
(	O
U	O
t−1	O
,	O
R	O
t−1	O
)	O
,	O
and	O
the	O
user	O
utterance	O
at	O
current	O
turn	O
U	O
t	O
.	O
A	O
taskoriented	O
dialogue	O
system	O
aims	O
to	O
generate	O
the	O
next	O
response	O
R	O
t	O
.	O
The	O
information	O
for	O
responses	O
is	O
typically	O
queried	O
from	O
a	O
database	O
based	O
on	O
the	O
user	O
's	O
provided	O
information	O
i.e.	O
inform	O
slots	O
tracked	O
by	O
a	O
DST	O
.	O
We	O
assume	O
access	O
to	O
a	O
database	O
of	O
all	O
domains	O
with	O
each	O
column	O
corresponding	O
to	O
a	O
specific	O
slot	O
being	O
tracked	O
.	O
We	O
denote	O
the	O
intermediate	O
output	O
,	O
including	O
the	O
dialogue	O
state	O
of	O
current	O
turn	O
B	O
t	O
and	O
dialogue	O
act	O
as	O
A	O
t	O
.	O
We	O
denote	O
the	O
list	O
of	O
all	O
domains	O
D	O
=	O
(	O
d	O
1	O
,	O
d	O
2	O
,	O
...	O
)	O
,	O
all	O
slots	O
S	O
=	O
(	O
s	O
1	O
,	O
s	O
2	O
,	O
...	O
)	O
,	O
and	O
all	O
acts	O
A	O
=	O
(	O
a	O
1	O
,	O
a	O
2	O
,	O
...	O
)	O
.	O
We	O
also	O
denote	O
the	O
list	O
of	O
all	O
(	O
domain	O
,	O
slot	O
)	O
pairs	O
as	O
DS	O
=	O
(	O
ds	O
1	O
,	O
ds	O
2	O
,	O
...	O
)	O
.	O
Note	O
that	O
DS	O
≤	O
D	O
×	O
S	O
as	O
some	O
slots	O
might	O
not	O
be	O
applicable	O
in	O
all	O
domains	O
.	O
Given	O
the	O
current	O
dialogue	O
turn	O
t	O
,	O
we	O
represent	O
each	O
text	O
input	O
as	O
a	O
sequence	O
of	O
tokens	O
,	O
each	O
of	O
which	O
is	O
a	O
unique	O
token	O
index	O
from	O
a	O
vocabulary	O
set	O
V	O
:	O
dialogue	O
context	O
X	O
ctx	O
,	O
current	O
user	O
utterance	O
X	O
utt	O
,	O
and	O
target	O
system	O
response	O
X	O
res	O
.	O
Similarly	O
,	O
we	O
also	O
represent	O
the	O
list	O
of	O
domains	O
as	O
X	O
D	O
and	O
the	O
list	O
of	O
slots	O
as	O
X	O
S	O
.	O
In	O
DST	O
,	O
we	O
consider	O
the	O
raw	O
text	O
form	O
of	O
dialogue	O
state	O
of	O
the	O
previous	O
turn	O
B	O
t−1	O
,	O
similarly	O
as	O
(	O
Lei	O
et	O
al	O
,	O
2018	O
;	O
Budzianowski	O
and	O
Vulić	O
,	O
2019	O
)	O
.	O
In	O
the	O
context	O
-	O
to	O
-	O
text	O
setting	O
,	O
we	O
assume	O
access	O
to	O
the	O
ground	O
-	O
truth	O
dialogue	O
states	O
of	O
current	O
turn	O
B	O
t	O
.	O
The	O
dialogue	O
state	O
of	O
the	O
previous	O
and	O
current	O
turn	O
can	O
then	O
be	O
represented	O
as	O
a	O
sequence	O
of	O
tokens	O
X	O
prev	O
st	O
and	O
X	O
curr	O
st	O
respectively	O
.	O
For	O
a	O
fair	O
comparison	O
with	O
current	O
approaches	O
,	O
during	O
inference	O
,	O
we	O
use	O
the	O
model	O
predicted	O
dialogue	O
statesX	O
prev	O
st	O
and	O
do	O
not	O
use	O
X	O
curr	O
st	O
in	O
DST	O
and	O
end	O
-	O
to	O
-	O
end	O
tasks	O
.	O
Following	O
(	O
Wen	O
et	O
al	O
,	O
2015	O
;	O
,	O
we	O
consider	O
the	O
delexicalized	O
target	O
response	O
X	O
dl	O
res	O
by	O
replacing	O
tokens	O
of	O
slot	O
values	O
by	O
their	O
corresponding	O
generic	O
tokens	O
to	O
allow	O
learning	O
valueindependent	O
parameters	O
.	O
Our	O
model	O
consists	O
of	O
3	O
major	O
components	O
(	O
See	O
Figure	O
1	O
)	O
.	O
First	O
,	O
Encoders	O
encode	O
all	O
text	O
input	O
into	O
continuous	O
representations	O
.	O
To	O
make	O
it	O
consistent	O
,	O
we	O
encode	O
all	O
input	O
with	O
the	O
same	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
.	O
Secondly	O
,	O
our	O
Bi	O
-	O
level	O
State	O
Tracker	O
(	O
BDST	O
)	O
is	O
used	O
to	O
detect	O
contextual	O
dependencies	O
to	O
generate	O
dialogue	O
states	O
.	O
The	O
DST	O
includes	O
2	O
modules	O
for	O
slot	O
-	O
level	O
and	O
domain	O
-	O
level	O
representation	B-TaskName
learning	I-TaskName
.	O
Each	O
module	O
comprises	O
attention	B-HyperparameterName
layers	I-HyperparameterName
to	O
project	O
domain	O
or	O
slot	O
representations	O
and	O
incorporate	O
important	O
information	O
from	O
dialogue	O
context	O
,	O
dialogue	O
state	O
of	O
the	O
previous	O
turn	O
,	O
and	O
current	O
user	O
utterance	O
.	O
The	O
outputs	O
are	O
combined	O
as	O
a	O
context	O
-	O
aware	O
vector	O
to	O
decode	O
the	O
corresponding	O
inform	O
or	O
request	O
slots	O
in	O
each	O
domain	O
.	O
Lastly	O
,	O
our	O
Joint	O
Dialogue	O
Act	O
and	O
Response	O
Generator	O
(	O
DARG	O
)	O
projects	O
the	O
target	O
system	O
response	O
representations	O
and	O
enhances	O
them	O
with	O
information	O
from	O
various	O
dialogue	O
components	O
.	O
Our	O
response	O
generator	O
can	O
also	O
learn	O
a	O
latent	O
representation	O
to	O
generate	O
dialogue	O
acts	O
,	O
which	O
condition	O
all	O
target	O
tokens	O
during	O
each	O
generation	O
step	O
.	O

An	O
encoder	O
encodes	O
a	O
text	O
sequence	O
X	O
to	O
a	O
sequence	O
of	O
continuous	O
representation	O
Z	O
R	O
L	O
X	O
×d	O
.	O
L	O
X	O
is	O
the	O
length	O
of	O
sequence	O
X	O
and	O
d	O
is	O
the	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
.	O
Each	O
encoder	O
includes	O
a	O
token	O
-	O
level	O
embedding	O
layer	O
.	O
The	O
embedding	O
layer	O
is	O
a	O
trainable	O
embedding	O
matrix	O
E	O
R	O
V	O
×d	O
.	O
Each	O
row	O
represents	O
a	O
token	O
in	O
the	O
vocabulary	O
set	O
V	O
as	O
a	O
d	O
-	O
dimensional	O
vector	O
.	O
We	O
denote	O
E	O
(	O
X	O
)	O
as	O
the	O
embedding	O
function	O
that	O
transform	O
the	O
sequence	O
X	O
by	O
looking	O
up	O
the	O
respective	O
token	O
index	O
:	O
Z	O
emb	O
=	O
E	O
(	O
X	O
)	O
R	O
L	O
X	O
×d	O
.	O
We	O
inject	O
the	O
positional	O
attribute	O
of	O
each	O
token	O
as	O
similarly	O
adopted	O
in	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
positional	O
encoding	O
is	O
denoted	O
as	O
P	O
E.	O
The	O
final	O
embedding	O
is	O
the	O
element	O
-	O
wise	O
summation	O
between	O
token	O
-	O
embedded	O
representations	O
and	O
positional	O
encoded	O
representations	O
with	O
layer	B-MethodName
normalization	I-MethodName
(	O
Ba	O
et	O
al	O
,	O
2016	O
)	O
:	O
Z	O
=	O
LayerNorm	O
(	O
Z	O
emb	O
+	O
P	O
E	O
(	O
X	O
)	O
)	O
R	O
L	O
X	O
×d	O
.	O
The	O
encoder	O
outputs	O
include	O
representations	O
of	O
dialogue	O
context	O
Z	O
ctx	O
,	O
current	O
user	O
utterance	O
Z	O
utt	O
,	O
and	O
target	O
response	O
Z	O
dl	O
res	O
.	O
We	O
also	O
encode	O
the	O
dialogue	O
states	O
of	O
the	O
previous	O
turn	O
and	O
current	O
turn	O
and	O
obtain	O
Z	O
prev	O
st	O
and	O
Z	O
curr	O
st	O
respectively	O
.	O
We	O
encode	O
X	O
S	O
and	O
X	O
D	O
using	O
only	O
token	O
-	O
level	O
embedding	O
layer	O
:	O
Z	O
S	O
=	O
LayerNorm	O
(	O
E	O
(	O
X	O
S	O
)	O
)	O
and	O
Z	O
D	O
=	O
LayerNorm	O
(	O
E	O
(	O
X	O
D	O
)	O
)	O
.	O
During	O
training	O
,	O
we	O
shift	O
the	O
target	O
response	O
by	O
one	O
position	O
to	O
the	O
left	O
side	O
to	O
allow	O
auto	O
-	O
regressive	O
prediction	O
in	O
each	O
generation	O
step	O
.	O
We	O
share	O
the	O
embedding	O
matrix	O
E	O
to	O
encode	O
all	O
text	O
tokens	O
except	O
for	O
tokens	O
of	O
target	O
responses	O
as	O
the	O
delexicalized	O
outputs	O
contain	O
different	O
semantics	O
from	O
natural	O
language	O
inputs	O
.	O

The	O
representations	O
Z	O
dst	O
are	O
used	O
as	O
context	O
-	O
aware	O
representations	O
to	O
decode	O
individual	O
dialogue	O
states	O
.	O
Given	O
a	O
domain	O
index	O
i	O
and	O
slot	O
index	O
j	O
,	O
the	O
feature	O
vector	O
Z	O
dst	O
[	O
i	O
,	O
j	O
,	O
:	O
]	O
R	O
d	O
is	O
used	O
to	O
generate	O
value	O
of	O
the	O
corresponding	O
(	O
domain	O
,	O
slot	O
)	O
pair	O
.	O
The	O
vector	O
is	O
used	O
as	O
an	O
initial	O
hidden	O
state	O
for	O
an	O
RNN	O
decoder	O
to	O
decode	O
an	O
inform	O
slot	O
value	O
.	O
Given	O
the	O
k	O
-	O
th	O
(	O
domain	O
,	O
slot	O
)	O
pair	O
and	O
decoding	O
step	O
l	O
,	O
the	O
output	O
hidden	O
state	O
in	O
each	O
recurrent	O
step	O
h	O
kl	O
is	O
passed	O
through	O
a	O
linear	O
transformation	O
with	O
softmax	B-MethodName
to	O
obtain	O
output	O
distribution	O
over	O
vocabulary	O
set	O
V	O
:	O
P	O
inf	O
kl	O
=	O
Softmax	B-MethodName
(	O
h	O
kl	O
W	O
inf	O
)	O
R	O
V	O
where	O
W	O
inf	O
dst	O
R	O
drnn×	O
V	O
.	O
For	O
request	O
slot	O
of	O
k	O
-	O
th	O
(	O
domain	O
,	O
slot	O
)	O
pair	O
,	O
we	O
pass	O
the	O
corresponding	O
vector	O
Z	O
dst	O
vector	O
through	O
a	O
linear	B-MethodName
layer	I-MethodName
with	O
sigmoid	B-MethodName
activation	I-MethodName
to	O
predict	O
a	O
value	O
of	O
0	B-DatasetName
or	O
1	O
.	O
P	O
req	O
k	B-HyperparameterName
=	I-HyperparameterName
Sigmoid	O
(	O
Z	O
dst	O
k	O
W	O
req	O
)	O
.	O
Optimization	O
.	O
The	O
DST	O
is	O
optimized	O
by	O
the	O
crossentropy	O
loss	B-MetricName
functions	O
of	O
inform	O
and	O
request	O
slots	O
:	O
L	O
dst	O
=	O
L	O
inf	O
+	O
L	O
req	O
=	O
DS	O
k=1	O
Y	O
k	O
l=1	O
−	O
log	O
(	O
P	O
inf	O
kl	O
(	O
y	O
kl	O
)	O
)	O
+	O
DS	O
k=1	O
−y	O
k	O
log	O
(	O
P	O
req	O
k	O
)	O
−	O
(	O
1	O
−	O
y	O
k	O
)	O
(	O
1	O
−	O
log	O
(	O
P	O
req	O
k	O
)	O
)	O
3.3	O
Joint	O
Dialogue	O
Act	O
and	O
Response	O
Generator	O
(	O
DARG	O
)	O
Database	O
Representations	O
.	O
Following	O
,	O
we	O
create	O
a	O
one	O
-	O
hot	O
vector	O
for	O
each	O
domain	O
d	O
:	O
x	O
d	O
db	O
{	O
0	B-DatasetName
,	O
1	O
}	O
6	O
and	O
6	O
i	O
x	O
d	O
db	O
,	O
i	O
=	O
1	O
.	O
Each	O
position	O
of	O
the	O
vector	O
indicates	O
a	O
number	O
or	O
a	O
range	O
of	O
entities	O
.	O
The	O
vectors	O
of	O
all	O
domains	O
are	O
concatenated	O
to	O
create	O
a	O
multi	O
-	O
domain	O
vector	O
X	O
db	O
R	O
6×	O
D	O
.	O
We	O
embed	O
this	O
vector	O
as	O
described	O
in	O
Section	O
3.1	O
.	O
Response	B-TaskName
Generation	I-TaskName
.	O
We	O
adopt	O
a	O
stackedattention	O
architecture	O
that	O
sequentially	O
learns	O
dependencies	O
between	O
each	O
token	O
in	O
target	O
responses	O
with	O
each	O
dialogue	O
component	O
representation	O
.	O
First	O
,	O
we	O
obtain	O
Z	O
gen	O
res	O
=	O
Att	O
(	O
Z	O
res	O
,	O
Z	O
res	O
)	O
R	O
Lres×d	O
.	O
This	O
attention	O
layer	O
can	O
learn	O
semantics	O
within	O
the	O
target	O
response	O
to	O
construct	O
a	O
more	O
semantically	O
structured	O
sequence	O
.	O
We	O
then	O
use	O
attention	O
to	O
capture	O
dependencies	O
in	O
background	O
information	O
contained	O
in	O
dialogue	O
context	O
and	O
user	O
utterance	O
.	O
The	O
outputs	O
are	O
Z	O
gen	O
ctx	O
=	O
Att	O
(	O
Z	O
ctx	O
,	O
Z	O
gen	O
res	O
)	O
R	O
Lres×d	O
and	O
Z	O
gen	O
utt	O
=	O
Att	O
(	O
Z	O
utt	O
,	O
Z	O
gen	O
ctx	O
)	O
R	O
Lres×d	O
sequentially	O
.	O
To	O
incorporate	O
information	O
of	O
dialogue	O
states	O
and	O
DB	O
results	O
,	O
we	O
apply	O
attention	O
steps	O
to	O
capture	O
dependencies	O
between	O
each	O
response	O
token	O
representation	O
and	O
state	O
or	O
DB	O
representation	O
.	O
Specifically	O
,	O
we	O
first	O
obtain	O
Z	O
gen	O
dst	O
=	O
Att	O
(	O
Z	O
dst	O
,	O
Z	O
gen	O
utt	O
)	O
R	O
Lres×d	O
.	O
In	O
the	O
context	O
-	O
to	O
-	O
text	O
setting	O
,	O
as	O
we	O
directly	O
use	O
the	O
ground	O
-	O
truth	O
dialogue	O
states	O
,	O
we	O
simply	O
replace	O
Z	O
dst	O
with	O
Z	O
curr	O
st	O
.	O
Then	O
we	O
obtain	O
Z	O
gen	O
db	O
=	O
Att	O
(	O
Z	O
db	O
,	O
Z	O
gen	O
dst	O
)	O
R	O
Lres×d	O
.	O
These	O
attention	B-HyperparameterName
layers	I-HyperparameterName
capture	O
the	O
information	O
needed	O
to	O
generate	O
tokens	O
that	O
are	O
towards	O
task	O
completion	O
and	O
supplement	O
the	O
contextual	O
cues	O
obtained	O
in	O
previous	O
attention	B-HyperparameterName
layers	I-HyperparameterName
.	O
We	O
let	O
the	O
models	O
to	O
progressively	O
capture	O
these	O
dependencies	O
for	O
N	O
gen	O
times	O
and	O
denote	O
the	O
final	O
output	O
as	O
Z	O
gen	O
.	O
The	O
final	O
output	O
is	O
passed	O
to	O
a	O
linear	B-MethodName
layer	I-MethodName
with	O
softmax	B-MethodName
activation	O
to	O
decode	O
system	O
responses	O
auto	O
-	O
regressively	O
:	O
P	O
res	O
=	O
Softmax	B-MethodName
(	O
Z	O
gen	O
W	O
gen	O
)	O
R	O
Lres×	O
Vres	O
Dialogue	O
Act	O
Modeling	O
.	O
We	O
couple	O
response	B-TaskName
generation	I-TaskName
with	O
dialogue	O
act	O
modeling	O
by	O
learning	O
a	O
latent	O
variable	O
Z	O
act	O
R	O
d	O
.	O
We	O
place	O
the	O
vector	O
in	O
the	O
first	O
position	O
of	O
Z	O
res	O
,	O
resulting	O
in	O
Z	O
res+act	O
R	O
(	O
Lres+1	O
)	O
×d	O
.	O
We	O
then	O
pass	O
this	O
tensor	O
to	O
the	O
same	O
stacked	O
attention	B-HyperparameterName
layers	I-HyperparameterName
as	O
above	O
.	O
By	O
adding	O
the	O
latent	O
variable	O
in	O
the	O
first	O
position	O
,	O
we	O
allow	O
our	O
model	O
to	O
semantically	O
condition	O
all	O
downstream	O
tokens	O
from	O
second	O
position	O
,	O
i.e.	O
all	O
tokens	O
in	O
the	O
target	O
response	O
,	O
on	O
this	O
latent	O
variable	O
.	O
The	O
output	O
representation	O
of	O
the	O
latent	O
vector	O
i.e.	O
by	O
domain	O
first	O
row	O
in	O
Z	O
gen	O
,	O
incorporates	O
contextual	O
signals	O
accumulated	O
from	O
all	O
attention	B-HyperparameterName
layers	I-HyperparameterName
and	O
is	O
used	O
to	O
predict	O
dialogue	O
acts	O
.	O
We	O
denote	O
this	O
representation	O
as	O
Z	O
gen	O
act	O
and	O
pass	O
it	O
through	O
a	O
linear	B-MethodName
layer	I-MethodName
to	O
obtain	O
a	O
multi	O
-	O
hot	O
encoded	O
tensor	O
.	O
We	O
apply	O
Sigmoid	O
on	O
this	O
tensor	O
to	O
classify	O
each	O
dialogue	O
act	O
as	O
0	B-DatasetName
or	O
1	O
:	O
P	O
act	O
=	O
Sigmoid	O
(	O
Z	O
gen	O
act	O
W	O
act	O
)	O
R	O
A	O
.	O
Optimization	O
.	O
The	O
response	O
generator	O
is	O
jointly	O
trained	O
by	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
functions	O
of	O
generated	O
responses	O
and	O
dialogue	O
acts	O
:	O
L	O
gen	O
=	O
L	O
res	O
+	O
L	O
act	O
=	O
Yres	O
l=1	O
−	O
log	O
(	O
P	O
res	O
l	O
(	O
y	O
l	O
)	O
)	O
+	O
A	O
a=1	O
−y	O
a	O
log	O
(	O
P	O
act	O
a	O
)	O
−	O
(	O
1	O
−	O
y	O
a	O
)	O
(	O
1	O
−	O
log	O
(	O
P	O
act	O
a	O
)	O
)	O
4	O
Experiments	O

Joint	O
Acc	B-MetricName
.	O
HJST	O
(	O
Eric	O
et	O
al	O
,	O
2019	O
)	O
35.55	O
%	O
DST	O
Reader	O
36.40	O
%	O
TSCP	O
(	O
Lei	O
et	O
al	O
,	O
2018	O
)	O
37.12	O
%	O
FJST	O
(	O
Eric	O
et	O
al	O
,	O
2019	O
)	O
38.00	O
%	O
HyST	O
38.10	O
%	O
TRADE	O
(	O
Wu	O
et	O
al	O
,	O
2019a	O
)	O
45.60	O
%	O
NADST	O
(	O
Le	O
et	O
al	O
,	O
2020	O
)	O
49.04	O
%	O
DSTQA	O
(	O
Zhou	O
and	O
Small	O
,	O
2019	O
)	O
51.17	O
%	O
SOM	B-MethodName
-	O
DST	O
(	O
Kim	O
et	O
al	O
,	O
2020	O
)	O
53.01	O
%	O
BDST	O
(	O
Ours	O
)	O
49.55	O
%	O
71.29	O
%	O
60.96	O
%	O
18.80	O
TokenMoE	O
(	O
Pei	O
et	O
al	O
,	O
2019	O
)	O
75.30	O
%	O
59.70	O
%	O
16.81	O
HDSA	O
82.90	O
%	O
68.90	O
%	O
23.60	O
Structured	O
Fusion	O
(	O
Mehri	O
et	O
al	O
,	O
2019	O
)	O
82.70	O
%	O
72.10	O
%	O
16.34	O
LaRL	O
(	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
82.78	O
%	O
79.20	O
%	O
12.80	O
GPT2	O
(	O
Budzianowski	O
and	O
Vulić	O
,	O
2019	O
)	O
We	O
adopt	O
a	O
teacher	O
-	O
forcing	O
training	O
strategy	O
by	O
simply	O
using	O
the	O
ground	O
-	O
truth	O
inputs	O
of	O
dialogue	O
state	O
of	O
the	O
previous	O
turn	O
and	O
the	O
gold	O
DB	O
representations	O
.	O
During	O
inference	O
in	O
DST	O
and	O
end	O
-	O
to	O
-	O
end	O
tasks	O
,	O
we	O
decode	O
system	O
responses	O
sequentially	O
turn	O
by	O
turn	O
,	O
using	O
the	O
previously	O
decoded	O
state	O
as	O
input	O
in	O
the	O
current	O
turn	O
,	O
and	O
at	O
each	O
turn	O
,	O
using	O
the	O
new	O
predicted	O
state	O
to	O
query	O
DBs	O
.	O
We	O
train	O
all	O
networks	O
with	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
and	O
a	O
decaying	O
learning	B-HyperparameterName
rate	I-HyperparameterName
schedule	O
.	O
All	O
models	O
are	O
trained	O
up	O
to	O
30	O
epochs	O
and	O
the	O
best	O
models	O
are	O
selected	O
based	O
on	O
validation	O
loss	B-MetricName
.	O
We	O
used	O
a	O
greedy	O
approach	O
to	O
decode	O
all	O
slots	O
and	O
a	O
beam	O
search	O
with	O
beam	O
size	O
5	O
.	O
To	O
evaluate	O
the	O
models	O
,	O
we	O
use	O
the	O
following	O
metrics	O
:	O
Joint	O
Accuracy	B-MetricName
and	O
Slot	O
Accuracy	B-MetricName
(	O
Henderson	O
et	O
al	O
,	O
2014b	O
)	O
,	O
Inform	O
and	O
Success	O
(	O
Wen	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
BLEU	B-MetricName
score	I-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
.	O
As	O
suggested	O
by	O
Liu	O
et	O
al	O
(	O
2016	O
)	O
,	O
human	O
evaluation	O
,	O
even	O
though	O
popular	O
in	O
dialogue	O
research	O
,	O
might	O
not	O
be	O
necessary	O
in	O
tasks	O
with	O
domain	O
constraints	O
such	O
as	O
MultiWOZ	B-DatasetName
.	O
We	O
implemented	O
all	O
models	O
using	O
Pytorch	O
and	O
will	O
release	O
our	O
code	O
on	O
github	O
1	O
.	O

DST	O
.	O
We	O
test	O
our	O
state	O
tracker	O
(	O
i.e.	O
using	O
only	O
L	O
dst	O
)	O
and	O
compare	O
the	O
performance	O
with	O
the	O
baseline	O
models	O
in	O
(	O
Peng	O
et	O
al	O
,	O
2019	O
)	O
,	O
our	O
single	O
model	O
generates	O
better	O
responses	O
in	O
all	O
domains	O
without	O
relying	O
on	O
multiple	O
domainspecific	O
teacher	O
models	O
.	O
We	O
also	O
noted	O
that	O
the	O
performance	O
of	O
DST	O
improves	O
in	O
contrast	O
to	O
the	O
previous	O
DST	O
task	O
.	O
This	O
can	O
be	O
explained	O
as	O
additional	O
supervision	O
from	O
system	O
responses	O
not	O
only	O
contributes	O
to	O
learn	O
a	O
natural	O
response	O
but	O
also	O
positively	O
impact	O
the	O
DST	O
component	O
.	O
Other	O
baseline	O
models	O
such	O
as	O
Wu	O
et	O
al	O
,	O
2019b	O
)	O
present	O
challenges	O
in	O
the	O
MultiWOZ	B-DatasetName
benchmark	O
as	O
the	O
models	O
could	O
not	O
fully	O
optimize	O
due	O
to	O
the	O
large	O
scale	O
entity	O
memory	O
.	O
For	O
example	O
,	O
following	O
GLMP	O
(	O
Wu	O
et	O
al	O
,	O
2019b	O
)	O
,	O
the	O
restaurant	O
domain	O
alone	O
has	O
over	O
1	O
,	O
000	O
memory	O
tuples	O
of	O
(	O
Subject	O
,	O
Relation	O
,	O
Object	O
)	O
.	O
Ablation	O
.	O
We	O
conduct	O
a	O
comprehensive	O
ablation	O
analysis	O
with	O
several	O
model	O
variants	O
in	O
Table	O
6	O
and	O
have	O
the	O
following	O
observations	O
:	O
The	O
model	O
variant	O
with	O
a	O
single	O
-	O
level	O
DST	O
(	O
by	O
considering	O
S	O
=	O
DS	O
and	O
N	O
dst	O
D	O
=	O
0	B-DatasetName
)	O
(	O
Row	O
A2	O
)	O
performs	O
worse	O
than	O
the	O
Bi	O
-	O
level	O
DST	O
(	O
Row	O
A1	O
)	O
.	O
In	O
addition	O
,	O
using	O
the	O
dual	O
architecture	O
also	O
improves	O
the	O
latency	O
in	O
each	O
attention	B-HyperparameterName
layers	I-HyperparameterName
as	O
typically	O
D	O
+	O
S	O
DS	O
.	O
The	O
performance	O
gap	O
also	O
indicates	O
the	O
potential	O
of	O
separating	O
global	O
and	O
local	O
dialogue	O
state	O
dependencies	O
by	O
domain	O
and	O
slot	O
level	O
.	O
Using	O
B	O
t−1	O
and	O
only	O
the	O
last	O
user	O
utterance	O
as	O
the	O
dialogue	O
context	O
(	O
Row	O
A1	O
and	O
B1	O
)	O
performs	O
as	O
well	O
as	O
using	O
B	O
t−1	O
and	O
a	O
full	O
-	O
length	O
dialogue	O
history	O
(	O
Row	O
A5	O
and	O
B3	O
)	O
.	O
This	O
demonstrates	O
that	O
the	O
information	O
from	O
the	O
last	O
dialogue	O
state	O
is	O
sufficient	O
to	O
represent	O
the	O
dialogue	O
history	O
up	O
to	O
the	O
last	O
user	O
utterance	O
.	O
One	O
benefit	O
from	O
not	O
using	O
the	O
full	O
dialogue	O
history	O
is	O
that	O
it	O
reduces	O
the	O
memory	O
cost	O
as	O
the	O
number	O
of	O
tokens	O
in	O
a	O
full	O
-	O
length	O
dialogue	O
history	O
is	O
much	O
larger	O
than	O
that	O
of	O
a	O
dialogue	O
state	O
(	O
particularly	O
as	O
the	O
conversation	O
evolves	O
over	O
many	O
turns	O
)	O
.	O
We	O
note	O
that	O
removing	O
the	O
loss	B-MetricName
function	O
to	O
learn	O
the	O
dialogue	O
act	O
latent	O
variable	O
(	O
Row	O
B2	O
)	O
can	O
hurt	O
the	O
generation	O
performance	O
,	O
especially	O
by	O
the	O
task	O
completion	O
metrics	O
Inform	O
and	O
Success	O
.	O
This	O
is	O
interesting	O
as	O
we	O
expect	O
dialogue	O
acts	O
affect	O
the	O
general	O
semantics	O
of	O
output	O
sentences	O
,	O
indicated	O
by	O
BLEU	B-MetricName
score	I-MetricName
,	O
rather	O
than	O
the	O
model	O
ability	O
to	O
retrieve	O
correct	O
entities	O
.	O
This	O
reveals	O
the	O
benefit	O
of	O
our	O
approach	O
.	O
By	O
enforcing	O
a	O
semantic	O
condition	O
on	O
each	O
token	O
of	O
the	O
target	O
response	O
,	O
the	O
model	O
can	O
facility	O
the	O
dialogue	O
flow	O
towards	O
successful	O
task	O
completion	O
.	O
In	O
both	O
state	O
tracker	O
and	O
response	O
generator	O
modules	O
,	O
we	O
note	O
that	O
learning	O
feature	O
representations	O
through	O
deeper	O
attention	O
networks	O
can	O
improve	O
the	O
quality	O
of	O
predicted	O
states	O
and	O
system	O
responses	O
.	O
This	O
is	O
consistent	O
with	O
our	O
DST	O
performance	O
as	O
compared	O
to	O
baseline	O
models	O
of	O
shallow	O
networks	O
.	O
Lastly	O
,	O
in	O
the	O
end	O
-	O
to	O
-	O
end	O
task	O
,	O
our	O
model	O
achieves	O
better	O
performance	O
as	O
the	O
number	O
of	O
attention	O
heads	O
increases	O
,	O
by	O
learning	O
more	O
high	O
-	O
resolution	O
dependencies	O
.	O

Baseline	O
.	O
provides	O
a	O
baseline	O
for	O
this	O
setting	O
by	O
following	O
the	O
sequenceto	O
-	O
sequence	O
model	O
.	O
The	O
source	O
sequence	O
is	O
all	O
past	O
dialogue	O
turns	O
and	O
the	O
target	O
sequence	O
is	O
the	O
system	O
response	O
.	O
The	O
initial	O
hidden	O
state	O
of	O
the	O
RNN	O
decoder	O
is	O
incorporated	O
with	O
additional	O
signals	O
from	O
the	O
dialogue	O
states	O
and	O
database	O
representations	O
.	O
TokenMoE	O
(	O
Pei	O
et	O
al	O
,	O
2019	O
)	O
.	O
TokenMoE	O
refers	O
to	O
Token	O
-	O
level	O
Mixture	O
-	O
of	O
-	O
Expert	O
model	O
.	O
The	O
model	O
follows	O
a	O
modularized	O
approach	O
by	O
separating	O
different	O
components	O
known	O
as	O
expert	O
bots	O
for	O
different	O
dialogue	O
scenarios	O
.	O
A	O
dialogue	O
scenario	O
can	O
be	O
dependent	O
on	O
a	O
domain	O
,	O
a	O
type	O
of	O
dialogue	O
act	O
,	O
etc	O
.	O
A	O
chair	O
bot	O
is	O
responsible	O
for	O
controlling	O
expert	O
bots	O
to	O
dynamically	O
generate	O
dialogue	O
responses	O
.	O
HDSA	O
.	O
This	O
is	O
the	O
current	O
stateof	O
-	O
the	O
-	O
art	O
in	O
terms	O
of	O
Inform	O
and	O
BLEU	B-MetricName
score	I-MetricName
in	O
the	O
context	O
-	O
to	O
-	O
text	B-TaskName
generation	I-TaskName
setting	O
in	O
MultiWOZ2.0	O
.	O
HDSA	O
leverages	O
the	O
structure	O
of	O
dialogue	O
acts	O
to	O
build	O
a	O
multi	O
-	O
layer	O
hierarchical	O
graph	O
.	O
The	O
graph	O
is	O
incorporated	O
as	O
an	O
inductive	O
bias	O
in	O
a	O
self	B-MethodName
-	I-MethodName
attention	I-MethodName
network	I-MethodName
to	O
improve	O
the	O
semantic	O
quality	O
of	O
generated	O
dialogue	O
responses	O
.	O
Structured	O
Fusion	O
(	O
Mehri	O
et	O
al	O
,	O
2019	O
)	O
.	O
This	O
approach	O
follows	O
a	O
traditional	O
modularized	O
dialogue	O
system	O
architecture	O
,	O
including	O
separate	O
components	O
for	O
NLU	O
,	O
DM	O
,	O
and	O
NLG	O
.	O
These	O
compo	O
-	O
nents	O
are	O
pre	O
-	O
trained	O
and	O
combined	O
into	O
an	O
end	O
-	O
toend	O
system	O
.	O
Each	O
component	O
output	O
is	O
used	O
as	O
a	O
structured	O
input	O
to	O
other	O
components	O
.	O
LaRL	O
(	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
.	O
This	O
model	O
uses	O
a	O
latent	O
dialogue	O
action	O
framework	O
instead	O
of	O
handcrafted	O
dialogue	O
acts	O
.	O
The	O
latent	O
variables	O
are	O
learned	O
using	O
unsupervised	O
learning	O
with	O
stochastic	O
variational	B-MethodName
inference	I-MethodName
.	O
The	O
model	O
is	O
trained	O
in	O
a	O
reinforcement	O
learning	O
framework	O
whereby	O
the	O
parameters	O
are	O
trained	O
to	O
yield	O
a	O
better	O
Success	O
rate	O
.	O
The	O
model	O
is	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
terms	O
of	O
Success	O
metric	O
.	O
GPT2	O
(	O
Budzianowski	O
and	O
Vulić	O
,	O
2019	O
)	O
.	O
Unsupervised	B-TaskName
pre	I-TaskName
-	I-TaskName
training	I-TaskName
language	O
models	O
have	O
significantly	O
improved	O
machine	O
learning	O
performance	O
in	O
many	O
NLP	O
tasks	O
.	O
This	O
baseline	O
model	O
leverages	O
the	O
power	O
of	O
a	O
pre	O
-	O
trained	O
model	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
and	O
adapts	O
to	O
the	O
context	O
-	O
to	O
-	O
text	B-TaskName
generation	I-TaskName
setting	O
in	O
task	O
-	O
oriented	O
dialogues	O
.	O
All	O
input	O
components	O
,	O
including	O
dialogue	O
state	O
and	O
database	O
state	O
,	O
are	O
transformed	O
into	O
raw	O
text	O
format	O
and	O
concatenated	O
as	O
a	O
single	O
sequence	O
.	O
The	O
sequence	O
is	O
used	O
as	O
input	O
to	O
a	O
pre	O
-	O
trained	O
GPT	B-MethodName
-	O
2	O
model	O
which	O
is	O
then	O
fine	O
-	O
tuned	O
with	O
MultiWOZ	B-DatasetName
data	O
.	O
DAMD	O
.	O
This	O
is	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
for	O
context	O
-	O
to	O
-	O
text	B-TaskName
generation	I-TaskName
task	O
in	O
MultiWOZ	B-DatasetName
2.1	I-DatasetName
.	O
This	O
approach	O
augments	O
training	O
data	O
with	O
multiple	O
responses	O
of	O
similar	O
context	O
.	O
Each	O
dialogue	O
state	O
is	O
mapped	O
to	O
multiple	O
valid	O
dialogue	O
acts	O
to	O
create	O
additional	O
state	O
-	O
act	O
pairs	O
.	O
B.3	O
End	O
-	O
to	O
-	O
End	O
TSCP	O
(	O
Lei	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
addition	O
to	O
the	O
DST	O
task	O
,	O
we	O
evaluate	O
TSCP	O
as	O
an	O
end	O
-	O
to	O
-	O
end	O
dialogue	O
system	O
that	O
can	O
do	O
both	O
DST	O
and	O
NLG	O
.	O
We	O
adapt	O
the	O
models	O
to	O
the	O
multi	O
-	O
domain	O
DST	O
setting	O
as	O
described	O
in	O
Section	O
B.1	O
and	O
keep	O
the	O
original	O
response	O
decoder	O
.	O
Similar	O
to	O
the	O
DST	O
component	O
,	O
the	O
response	O
generator	O
of	O
TSCP	O
also	O
adopts	O
a	O
pointer	B-MethodName
network	I-MethodName
to	O
generate	O
tokens	O
of	O
the	O
target	O
system	O
responses	O
by	O
copying	O
tokens	O
from	O
source	O
sequences	O
.	O
In	O
this	O
setting	O
,	O
we	O
test	O
TSCP	O
with	O
two	O
settings	O
of	O
the	O
maximum	O
length	O
of	O
the	O
output	O
dialogue	O
state	O
sequence	O
:	O
L	O
=	O
8	O
and	O
L	O
=	O
20	O
.	O
HRED	O
-	O
TS	B-MethodName
(	O
Peng	O
et	O
al	O
,	O
2019	O
)	O
.	O
This	O
model	O
adopts	O
a	O
teacher	O
-	O
student	O
framework	O
to	O
address	O
multidomain	O
task	O
-	O
oriented	O
dialogues	O
.	O
Multiple	O
teacher	O
networks	O
are	O
trained	O
for	O
different	O
domains	O
and	O
intermediate	O
representations	O
of	O
dialogue	O
acts	O
and	O
output	O
responses	O
are	O
used	O
to	O
guide	O
a	O
universal	O
student	O
network	O
.	O
The	O
student	O
network	O
uses	O
these	O
representations	O
to	O
directly	O
generate	O
responses	O
from	O
dialogue	O
context	O
without	O
predicting	O
dialogue	O
states	O
.	O

We	O
examine	O
an	O
example	O
of	O
dialogue	O
in	O
the	O
test	O
data	O
and	O
compare	O
our	O
predicted	O
outputs	O
with	O
the	O
baseline	O
TSCP	O
(	O
L	O
=	O
20	O
)	O
(	O
Lei	O
et	O
al	O
,	O
2018	O
)	O
and	O
the	O
ground	O
truth	O
.	O
From	O
Figure	O
4	O
,	O
we	O
observe	O
that	O
both	O
our	O
predicted	O
dialogue	O
state	O
and	O
system	O
response	O
are	O
more	O
correct	O
than	O
the	O
baseline	O
.	O
Specifically	O
,	O
our	O
dialogue	O
state	O
can	O
detect	O
the	O
correct	O
type	O
slot	O
in	O
the	O
attraction	O
domain	O
.	O
As	O
our	O
dialogue	O
state	O
is	O
correctly	O
predicted	O
,	O
the	O
queried	O
results	O
from	O
DB	O
is	O
also	O
more	O
correct	O
,	O
resulting	O
in	O
better	O
response	O
with	O
the	O
right	O
information	O
(	O
i.e.	O
'	O
no	O
attraction	O
available	O
'	O
)	O
.	O
In	O
Figure	O
5	O
,	O
we	O
show	O
the	O
visualization	O
of	O
domain	O
-	O
level	O
and	O
slot	O
-	O
level	O
attention	O
on	O
the	O
user	O
utterance	O
.	O
We	O
notice	O
important	O
tokens	O
of	O
the	O
text	O
sequences	O
,	O
i.e.	O
'	O
entertainment	O
'	O
and	O
'	O
close	O
to	O
'	O
,	O
are	O
attended	O
with	O
higher	O
attention	O
scores	O
.	O
Besides	O
,	O
at	O
domain	O
-	O
level	O
attention	O
,	O
we	O
find	O
a	O
potential	O
additional	O
signal	O
from	O
the	O
token	O
'	O
restaurant	O
'	O
,	O
which	O
is	O
also	O
the	O
domain	O
from	O
the	O
previous	O
dialogue	O
turn	O
.	O
We	O
also	O
observe	O
that	O
attention	O
is	O
more	O
refined	O
throughout	O
the	O
neural	O
network	O
layers	O
.	O
For	O
example	O
,	O
in	O
the	O
domain	O
-	O
level	O
processing	O
,	O
compared	O
to	O
the	O
2	O
nd	O
layer	O
,	O
the	O
4	O
th	O
layer	O
attention	O
is	O
more	O
clustered	O
around	O
specific	O
tokens	O
of	O
the	O
user	O
utterance	O
.	O
In	O
Table	O
10	O
and	O
11	O
,	O
we	O
reported	O
the	O
complete	O
output	O
of	O
this	O
example	O
dialogue	O
.	O
Overall	O
,	O
our	O
dialogue	O
agent	B-DatasetName
can	O
carry	O
a	O
proper	O
dialogue	O
with	O
the	O
user	O
throughout	O
the	O
dialogue	O
steps	O
.	O
Specifically	O
,	O
we	O
observed	O
that	O
our	O
model	O
can	O
detect	O
new	O
domains	O
at	O
dialogue	O
steps	O
where	O
the	O
domains	O
are	O
introduced	O
e.g.	O
attraction	O
domain	O
at	O
the	O
5	O
th	O
turn	O
and	O
taxi	O
domain	O
at	O
the	O
8	O
th	O
turn	O
.	O
The	O
dialogue	O
agent	B-DatasetName
can	O
also	O
detect	O
some	O
of	O
the	O
co	O
-	O
references	O
among	O
the	O
domains	O
.	O
For	O
example	O
,	O
at	O
the	O
5	O
th	O
turn	O
,	O
the	O
dialogue	O
agent	B-DatasetName
can	O
infer	O
the	O
slot	O
area	O
for	O
the	O
new	O
domain	O
attraction	O
as	O
the	O
user	O
mentioned	O
'	O
close	O
the	O
restaurant	O
'	O
.	O
We	O
noticed	O
that	O
that	O
at	O
later	O
dialogue	O
steps	O
such	O
as	O
the	O
6	O
th	O
turn	O
,	O
our	O
decoded	O
dialogue	O
state	O
is	O
not	O
correct	O
possibly	O
due	O
to	O
the	O
incorrect	O
decoded	O
dialogue	O
state	O
in	O
the	O
previous	O
turn	O
,	O
i.e.	O
5	O
th	O
turn	O
.	O
In	O
Figure	O
2	O
and	O
3	O
,	O
we	O
plotted	O
the	O
Joint	O
Goal	O
Accuracy	B-MetricName
and	O
BLEU	B-MetricName
metrics	O
of	O
our	O
model	O
by	O
dialogue	O
turn	O
.	O
As	O
we	O
expected	O
,	O
the	O
Joint	O
Accuracy	B-MetricName
metric	O
tends	O
to	O
decrease	O
as	O
the	O
dialogue	O
history	O
extends	O
over	O
time	O
.	O
The	O
dialogue	O
agent	B-DatasetName
achieves	O
the	O
highest	O
accuracy	B-MetricName
in	O
state	O
tracking	O
at	O
the	O
1	O
st	O
turn	O
and	O
gradually	O
reduces	O
to	O
zero	O
accuracy	B-MetricName
at	O
later	O
dialogue	O
steps	O
,	O
i.e.	O
15	O
th	O
to	O
18	O
th	O
turns	O
.	O
For	O
response	B-TaskName
generation	I-TaskName
performance	O
,	O
the	O
trend	O
of	O
BLEU	B-MetricName
score	I-MetricName
is	O
less	O
obvious	O
.	O
The	O
dialogue	O
agent	B-DatasetName
obtains	O
the	O
highest	O
BLEU	B-MetricName
scores	O
at	O
the	O
3	O
rd	O
turn	O
and	O
fluctuates	O
between	O
the	O
2	O
nd	O
and	O
13	O
th	O
turn	O
.	O
10	O
.	O
Each	O
turn	O
includes	O
the	O
input	O
of	O
past	O
system	O
response	O
S	O
t−1	O
and	O
current	O
user	O
utterance	O
U	O
t	O
,	O
and	O
the	O
predicted	O
dialogue	O
dialogue	O
state	O
BS	O
t	O
and	O
system	O
response	O
S	O
t	O
.	O
The	O
dialogue	O
consists	O
of	O
11	O
turns	O
in	O
total	O
and	O
extends	O
across	O
3	O
domains	O
sequentially	O
:	O
restaurant	O
,	O
attraction	O
,	O
and	O
taxi	O
.	O

Our	O
model	O
is	O
conducted	O
in	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
manner	O
which	O
consists	O
of	O
a	O
shared	O
encoder	O
,	O
a	O
predictor	O
,	O
and	O
a	O
pair	O
of	O
counterfactual	O
decoders	O
.	O
The	O
predictor	O
and	O
the	O
decoders	O
take	O
the	O
output	O
of	O
the	O
encoder	O
as	O
input	O
.	O
Our	O
model	O
looks	O
like	O
SHAPED	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
(	O
several	O
decoders	O
with	O
a	O
classifier	O
)	O
,	O
but	O
the	O
motivations	O
and	O
mechanisms	O
behind	O
the	O
model	O
are	O
different	O
.	O
Claim	O
-	O
aware	O
Encoder	O
Intuitively	O
,	O
the	O
plaintiff	O
's	O
claim	O
c	O
and	O
the	O
fact	O
description	O
f	O
are	O
sequences	O
of	O
words	O
.	O
Therefore	O
,	O
the	O
encoder	O
firstly	O
transforms	O
the	O
words	O
to	O
embeddings	O
.	O
Then	O
the	O
embedding	O
sequences	O
are	O
fed	O
to	O
the	O
Bi	O
-	O
LSTM	B-MethodName
,	O
producing	O
two	O
sequences	O
of	O
hidden	O
states	O
h	O
c	O
,	O
h	O
f	O
corresponding	O
to	O
the	O
plaintiff	O
's	O
claim	O
and	O
the	O
fact	O
description	O
respectively	O
.	O
After	O
that	O
,	O
we	O
use	O
a	O
claim	O
-	O
aware	O
attention	O
mechanism	O
to	O
fuse	O
h	O
c	O
and	O
h	O
f	O
.	O
For	O
each	O
hidden	O
state	O
h	O
f	O
i	O
in	O
h	O
f	O
,	O
e	O
i	O
k	O
is	O
its	O
attention	O
weight	O
on	O
h	O
c	O
k	O
,	O
and	O
the	O
attention	O
distribution	O
q	O
i	O
is	O
calculated	O
as	O
follow	O
:	O
e	O
i	O
k	B-HyperparameterName
=	I-HyperparameterName
v	O
T	O
tanh	O
(	O
W	O
c	O
h	O
c	O
k	O
+	O
W	O
f	O
h	O
f	O
i	O
+	O
b	O
attn	O
)	O
(	O
4	O
)	O
q	O
i	O
=	O
sof	B-DatasetName
tmax	O
(	O
e	O
i	O
)	O
(	O
5	O
)	O
where	O
v	O
,	O
W	O
c	O
,	O
W	O
f	O
,	O
b	O
attn	O
are	O
learnable	O
parameters	O
.	O
The	O
attention	O
distribution	O
can	O
be	O
regarded	O
as	O
the	O
importance	O
of	O
each	O
word	O
in	O
the	O
plaintiff	O
's	O
claim	O
for	O
a	O
word	O
in	O
fact	O
description	O
.	O
Next	O
,	O
the	O
new	O
representation	O
of	O
fact	O
description	O
is	O
produced	O
as	O
follows	O
:	O
h	O
f	O
*	O
i	O
=	O
h	O
f	O
i	O
+	O
k	O
q	O
i	O
k	O
h	O
c	O
k	O
(	O
6	O
)	O
After	O
feeding	O
to	O
another	O
Bi	O
-	O
LSTM	B-MethodName
layer	O
,	O
we	O
get	O
the	O
claim	O
-	O
aware	O
representation	O
of	O
fact	O
h.	O
Judgment	O
Predictor	O
Given	O
the	O
claim	O
-	O
aware	O
representation	O
of	O
fact	O
h	O
,	O
the	O
judgment	O
predictor	O
produces	O
the	O
probability	O
of	O
support	O
P	O
sup	O
through	O
a	O
fully	O
connected	O
layer	O
and	O
a	O
sigmoid	O
operation	O
.	O
The	O
prediction	O
result	O
j	O
is	O
obtained	O
as	O
follow	O
:	O
j	O
=	O
1	O
P	O
sup	O
>	O
0.5	O
0	B-DatasetName
P	O
sup	O
<	O
=	O
0.5	O
(	O
7	O
)	O
where	O
1	O
means	O
support	O
,	O
and	O
0	B-DatasetName
means	O
non	O
-	O
support	O
.	O
Counterfactual	O
Decoder	O
To	O
eliminate	O
the	O
effect	O
of	O
data	O
bias	O
,	O
here	O
we	O
use	O
a	O
pair	O
of	O
counterfactual	O
decoders	O
,	O
which	O
contains	O
two	O
decoders	O
,	O
one	O
is	O
for	O
supported	O
cases	O
,	O
and	O
the	O
other	O
is	O
for	O
non	O
-	O
supported	O
cases	O
.	O
The	O
two	O
decoders	O
have	O
the	O
same	O
structure	O
but	O
aim	O
to	O
generate	O
the	O
court	O
's	O
view	O
with	O
different	O
judgments	O
.	O
We	O
name	O
them	O
as	O
counterfactual	O
decoders	O
because	O
every	O
time	O
there	O
is	O
only	O
one	O
of	O
the	O
two	O
generated	O
court	O
's	O
views	O
correct	O
.	O
Still	O
,	O
we	O
apply	O
the	O
attention	O
-	O
mechanism	O
.	O
At	O
each	O
step	O
t	O
,	O
given	O
the	O
encoder	O
's	O
output	O
h	O
,	O
and	O
the	O
decode	O
state	O
s	O
t	O
,	O
the	O
attention	O
distribution	O
a	O
t	O
is	O
calculated	O
the	O
same	O
way	O
as	O
q	O
i	O
in	O
Eq	O
.	O
5	O
,	O
but	O
with	O
different	O
parameters	O
.	O
The	O
context	O
vector	O
h	O
*	O
t	O
is	O
then	O
a	O
weighted	O
sum	O
of	O
h	O
:	O
h	O
*	O
t	O
=	O
i	O
a	O
t	O
i	O
h	O
i	O
(	O
8	O
)	O
The	O
context	O
vector	O
h	O
*	O
t	O
,	O
which	O
can	O
be	O
regarded	O
as	O
a	O
representation	O
of	O
the	O
input	O
for	O
this	O
step	O
,	O
is	O
concatenated	O
with	O
the	O
decode	O
state	O
s	O
t	O
and	O
fed	O
to	O
linear	O
layers	O
to	O
produce	O
the	O
vocabulary	O
distribution	O
p	O
vocab	O
:	O
p	O
vocab	O
=	O
sof	B-DatasetName
tmax	O
(	O
V	O
(	O
V	O
[	O
s	O
t	O
,	O
h	O
*	O
t	O
]	O
)	O
+	O
b	O
)	O
+	O
b	O
)	O
(	O
9	O
)	O
where	O
V	O
,	O
V	O
,	O
b	O
,	O
b	O
are	O
all	O
learnable	O
parameters	O
.	O
Then	O
we	O
add	O
a	O
generation	O
probability	O
(	O
See	O
et	O
al	O
,	O
2017	O
)	O
to	O
solve	O
the	O
OOV	O
problem	O
.	O
Given	O
the	O
context	O
h	O
*	O
t	O
,	O
the	O
decode	O
state	O
s	O
t	O
and	O
the	O
decoder	O
's	O
input	O
(	O
the	O
word	O
embedding	O
of	O
the	O
previous	O
word	O
)	O
x	O
t	O
,	O
the	O
generation	O
probability	O
p	O
gen	O
can	O
be	O
calculated	O
:	O
P	O
gen	O
=	O
σ	O
(	O
w	O
T	O
h	O
*	O
h	O
*	O
t	O
+	O
w	O
T	O
s	O
s	O
t	O
+	O
w	O
T	O
x	O
x	O
t	O
+	O
b	O
ptr	B-DatasetName
)	O
(	O
10	O
)	O
where	O
w	O
h	O
*	O
,	O
w	O
s	O
,	O
w	O
x	O
and	O
b	O
ptr	B-DatasetName
are	O
learnable	O
,	O
and	O
σ	O
is	O
the	O
sigmoid	O
function	O
.	O
The	O
final	O
probability	O
for	O
a	O
word	O
w	O
in	O
time	O
step	O
is	O
obtained	O
:	O
Training	O
For	O
predictor	O
,	O
we	O
use	O
cross	O
-	O
entropy	O
as	O
the	O
loss	B-MetricName
function	O
:	O
P	O
(	O
w	O
)	O
=	O
P	O
gen	O
*	O
p	O
vocab	O
(	O
w	O
)	O
+	O
(	O
1	O
−	O
P	O
gen	O
)	O
i	O
:	O
w	O
i	O
=	O
w	O
a	O
t	O
i	O
(	O
L	O
pred	O
=	O
−ĵlog	O
(	O
P	O
sup	O
)	O
−	O
(	O
1	O
−ĵ	O
)	O
log	O
(	O
1	O
−	O
P	O
sup	O
)	O
(	O
12	O
)	O
whereĵ	O
is	O
the	O
real	O
judgment	O
.	O
For	O
decoders	O
,	O
the	O
previous	O
word	O
in	O
training	O
is	O
the	O
word	O
in	O
real	O
court	O
's	O
view	O
,	O
and	O
the	O
loss	B-MetricName
for	O
timestep	O
t	O
is	O
the	O
negative	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
of	O
the	O
target	O
word	O
w	O
*	O
t	O
:	O
L	O
t	O
=	O
−logP	O
(	O
w	O
*	O
t	O
)	O
(	O
13	O
)	O
and	O
the	O
overall	O
generation	O
loss	B-MetricName
is	O
:	O
L	O
gen	O
=	O
1	O
T	O
T	O
t=0	O
L	O
t	O
(	O
14	O
)	O
where	O
T	O
is	O
the	O
length	O
of	O
real	O
court	O
's	O
view	O
.	O
Since	O
we	O
aim	O
to	O
make	O
the	O
two	O
decoders	O
generate	O
two	O
different	O
court	O
's	O
views	O
,	O
we	O
take	O
a	O
mask	O
operation	O
when	O
calculating	O
the	O
loss	B-MetricName
of	O
each	O
decoder	O
.	O
The	O
exact	O
loss	B-MetricName
for	O
the	O
support	O
decoder	O
is	O
:	O
L	O
sup	O
=	O
L	O
genĵ	O
=	O
1	O
0ĵ	O
=	O
0	B-DatasetName
(	O
15	O
)	O
the	O
loss	B-MetricName
for	O
the	O
non	O
-	O
support	O
decoder	O
L	O
nsup	O
is	O
obtained	O
by	O
the	O
opposite	O
way	O
.	O
Thus	O
,	O
the	O
total	O
loss	B-MetricName
is	O
:	O
L	O
total	O
=	O
L	O
sup	O
+	O
L	O
nsup	O
+	O
λL	O
pred	O
(	O
16	O
)	O
where	O
we	O
set	O
λ	O
to	O
0.1	O
in	O
our	O
model	O
.	O
Inference	O
In	O
inference	O
,	O
the	O
counterfactual	O
decoders	O
apply	O
beam	O
search	O
to	O
generate	O
two	O
court	O
's	O
views	O
,	O
and	O
one	O
of	O
them	O
will	O
be	O
selected	O
as	O
the	O
final	O
output	O
,	O
depending	O
on	O
the	O
result	O
of	O
the	O
predictor	O
j.	O

Since	O
there	O
is	O
no	O
publicly	O
available	O
court	O
's	O
view	O
generation	O
dataset	O
in	O
civil	O
cases	O
,	O
we	O
build	O
a	O
dataset	O
based	O
on	O
raw	O
civil	O
legal	O
documents	O
3	O
.	O
Specifically	O
,	O
we	O
choose	O
private	O
lending	O
,	O
which	O
is	O
the	O
most	O
frequent	O
category	O
in	O
civil	O
cases	O
,	O
to	O
construct	O
the	O
dataset	O
.	O
We	O
process	O
the	O
legal	O
documents	O
as	O
following	O
steps	O
:	O
1	O
)	O
Split	O
legal	O
documents	O
into	O
three	O
parts	O
:	O
plaintiff	O
's	O
claim	O
,	O
facts	O
description	O
,	O
and	O
court	O
's	O
view	O
,	O
which	O
can	O
be	O
objectively	O
split	O
by	O
keywords	O
(	O
subtitles	O
)	O
.	O
2	O
)	O
Human	O
annotation	O
.	O
We	O
employ	O
experts	O
with	O
legal	O
backgrounds	O
to	O
annotate	O
the	O
judgment	O
(	O
defined	O
in	O
Sec	O
.	O
3	O
)	O
on	O
the	O
court	O
's	O
view	O
.	O
3	O
)	O
Annotation	O
verification	O
.	O
We	O
use	O
random	O
sampling	O
test	O
to	O
ensure	O
that	O
the	O
annotation	O
accuracy	B-MetricName
is	O
over	O
95	O
%	O
.	O
After	O
that	O
,	O
we	O
get	O
the	O
dataset	O
as	O
shown	O
in	O
Tab	O
.	O
1	O
.	O
We	O
randomly	O
separate	O
the	O
dataset	O
into	O
a	O
training	O
set	O
,	O
a	O
validation	O
set	O
,	O
and	O
a	O
test	O
set	O
according	O
to	O
a	O
ratio	O
of	O
8	O
:	O
1	O
:	O
1	O
,	O
the	O
ratio	O
of	O
supported	O
cases	O
is	O
about	O
76	O
%	O
in	O
each	O
set	O
.	O

ROUGE	O
4	O
is	O
a	O
set	O
of	O
metrics	O
used	O
in	O
the	O
NLP	O
task	O
.	O
We	O
keep	O
the	O
results	O
of	O
ROUGE	O
-	O
1	O
,	O
ROUGE	O
-	O
2	O
,	O
and	O
ROUGE	O
-	O
L.	O
ROUGE	O
-	O
1	O
and	O
ROUGE	O
-	O
2	O
refer	O
to	O
the	O
overlap	O
of	O
unigram	O
and	O
bigram	O
between	O
the	O
generated	O
and	O
reference	O
documents	O
,	O
respectively	O
.	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
is	O
a	O
Longest	O
Common	O
Subsequence	O
(	O
LCS	O
)	O
based	O
statistics	O
.	O
BLEU	B-MetricName
5	O
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
is	O
a	O
method	O
of	O
au	O
-	O
tomatic	O
text	O
-	O
generation	O
evaluation	O
that	O
highly	O
correlates	O
with	O
human	O
evaluation	O
.	O
We	O
use	O
BLEU	B-MetricName
-	O
1	O
,	O
BLEU	B-MetricName
-	O
2	O
to	O
evaluate	O
from	O
the	O
perspectives	O
of	O
unigram	O
,	O
bigram	O
.	O
BLEU	B-MetricName
-	O
N	O
is	O
an	O
average	O
of	O
BLEU	B-MetricName
-	O
1	O
,	O
BLEU2	O
,	O
BLEU	B-MetricName
-	O
3	O
,	O
BLEU	B-MetricName
-	O
4	O
.	O
BERT	B-MethodName
SCORE	O
6	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
computes	O
a	O
similarity	O
score	O
by	O
using	O
contextual	O
embedding	O
of	O
the	O
tokens	O
.	O
We	O
calculate	O
the	O
precision	O
(	O
p	O
)	O
,	O
recall	O
(	O
r	O
)	O
and	O
f	O
1	O
-	O
score	O
to	O
evaluate	O
the	O
information	O
matching	O
degree	O
.	O
Accuracy	B-MetricName
of	O
judgment	O
prediction	O
To	O
evaluate	O
the	O
performance	O
of	O
the	O
predictor	O
,	O
we	O
calculate	O
the	O
precision	O
(	O
p	O
)	O
,	O
recall	O
(	O
r	O
)	O
and	O
,	O
f	O
1	O
-	O
score	O
of	O
supported	O
and	O
non	O
-	O
supported	O
cases	O
,	O
respectively	O
.	O

Tab	O
.	O
2	O
demonstrates	O
the	O
results	O
of	O
court	O
's	O
view	O
generation	O
with	O
ROUGE	O
,	O
BLEU	B-MetricName
,	O
and	O
BERT	B-MethodName
SCORE	O
.	O
Also	O
,	O
we	O
report	O
the	O
results	O
on	O
the	O
judgment	O
prediction	O
of	O
our	O
predictor	O
component	O
with	O
precision	O

The	O
court	O
concluded	O
that	O
:	O
The	O
subject	O
of	O
the	O
private	O
lending	O
relationship	O
between	O
Plaintiff	O
A	O
and	O
Defendant	O
B	O
was	O
qualified	O
,	O
the	O
content	O
was	O
legal	O
,	O
and	O
the	O
meaning	O
was	O
true	O
.	O
It	O
should	O
be	O
deemed	O
valid	O
.	O
Defendant	O
should	O
repay	O
the	O
plaintiff	O
's	O
loan	O
within	O
a	O
reasonable	O
period	O
after	O
the	O
plaintiff	O
urged	O
.	O
Therefore	O
,	O
Defendant	O
B	O
should	O
bear	O
the	O
civil	O
liability	O
of	O
returning	O
the	O
plaintiff	O
's	O
loan	O
of	O
$	O
495	O
,	O
000	O
and	O
paying	O
overdue	O
interest	O
Acceptance	O
.	O
The	O
court	O
did	O
not	O
support	O
the	O
plaintiff	O
's	O
claim	O
requesting	O
the	O
defendant	O
C	O
to	O
return	O
the	O
loan	O
together	O
because	O
the	O
evidence	O
was	O
insufficient	O
(	O
p	O
)	O
,	O
recall	O
(	O
r	O
)	O
,	O
and	O
f	O
1	O
-	O
score	O
(	O
f1	O
)	O
in	O
Tab	O
.	O
3	O
.	O
To	O
demonstrate	O
that	O
our	O
method	O
is	O
de	O
-	O
biased	O
on	O
judgment	O
generation	O
,	O
we	O
report	O
the	O
result	O
of	O
human	O
evaluation	O
in	O
Tab	O
.	O
4	O
.	O
Results	O
of	O
court	O
's	O
view	O
generation	O
:	O
From	O
Tab	O
.	O
2	O
,	O
we	O
can	O
conclude	O
that	O
:	O
(	O
1	O
)	O
S2S	O
tends	O
to	O
repeat	O
words	O
,	O
which	O
makes	O
it	O
get	O
high	O
BLEU	B-MetricName
but	O
low	O
BERT	B-MethodName
SCORE	O
.	O
(	O
2	O
)	O
Oversampling	O
strategy	O
does	O
n't	O
benefit	O
the	O
models	O
,	O
hence	O
,	O
it	O
can	O
not	O
address	O
the	O
confounding	O
bias	O
.	O
(	O
3	O
)	O
With	O
claim	O
-	O
aware	O
encoder	O
and	O
backdoor	O
-	O
inspired	O
counterfactual	O
decoders	O
,	O
our	O
AC	O
-	O
NLG	O
achieves	O
better	O
performance	O
on	O
court	O
's	O
view	O
generation	O
compared	O
with	O
baselines	O
.	O
(	O
4	O
)	O
The	O
performance	O
gap	O
between	O
AC	O
-	O
NLGw	O
/	O
oCA	O
and	O
AC	O
-	O
NLG	O
demonstrates	O
the	O
effectiveness	O
of	O
our	O
proposed	O
claim	O
-	O
aware	O
encoder	O
,	O
and	O
the	O
gap	O
between	O
AC	O
-	O
NLGw	O
/	O
oBA	O
and	O
AC	O
-	O
NLG	O
illustrates	O
the	O
superiority	O
of	O
our	O
counterfactual	O
decoders	O
.	O
Results	O
of	O
judgment	O
prediction	O
:	O
From	O
Tab	O
.	O
3	O
,	O
we	O
have	O
the	O
following	O
observations	O
:	O
(	O
1	O
)	O
The	O
counterfactual	O
decoders	O
in	O
our	O
model	O
can	O
significantly	O
eliminate	O
the	O
confounding	O
bias	O
,	O
hence	O
,	O
achieve	O
remarkable	O
improvement	O
on	O
the	O
non	O
-	O
supported	O
cases	O
,	O
for	O
example	O
boosting	O
f	O
1	O
from	O
49.8	O
%	O
to	O
76.9	O
%	O
.	O
(	O
2	O
)	O
The	O
proposed	O
claim	O
-	O
aware	O
encoder	O
has	O
a	O
limited	O
effect	O
on	O
judgment	O
prediction	O
since	O
it	O
's	O
designed	O
for	O
improving	O
the	O
quality	O
of	O
generation	O
as	O
shown	O
in	O
Tab	O
.	O
2	O
.	O
(	O
3	O
)	O
Still	O
,	O
oversampling	O
brings	O
no	O
improvement	O
to	O
the	O
model	O
.	O
Results	O
of	O
human	O
evaluation	O
:	O
From	O
Tab	O
.	O
4	O
,	O
we	O
have	O
the	O
following	O
observations	O
:	O
(	O
1	O
)	O
due	O
to	O
the	O
confounding	O
bias	O
in	O
data	O
,	O
the	O
performance	O
of	O
judgment	O
generation	O
in	O
PGN	O
is	O
poor	O
for	O
non	O
-	O
supported	O
cases	O
,	O
and	O
its	O
performance	O
gap	O
between	O
supported	O
and	O
non	O
-	O
supported	O
cases	O
is	O
huge	O
(	O
1.56	O
)	O
.	O
(	O
2	O
)	O
By	O
debiasing	O
with	O
backdoor	O
-	O
inspired	O
counterfactual	O
decoders	O
,	O
our	O
AC	O
-	O
NLG	O
significantly	O
improves	O
the	O
performance	O
of	O
judgment	O
generation	O
,	O
especially	O
for	O
non	O
-	O
supported	O
cases	O
,	O
and	O
achieves	O
a	O
smaller	O
performance	O
gap	O
(	O
only	O
0.28	O
)	O
between	O
the	O
supported	O
and	O
non	O
-	O
supported	O
cases	O
.	O
(	O
3	O
)	O
With	O
a	O
claim	O
-	O
aware	O
encoder	O
,	O
our	O
AC	O
-	O
NLG	O
also	O
achieves	O
better	O
performance	O
on	O
the	O
generation	O
of	O
rational	O
and	O
generated	O
court	O
's	O
view	O
fluency	O
.	O
(	O
4	O
)	O
Kappa	O
coefficient	O
κ	O
is	O
more	O
than	O
0.8	O
between	O
any	O
two	O
judges	O
,	O
which	O
proves	O
the	O
validation	O
of	O
human	O
evaluation	O
.	O
Overall	O
,	O
thanks	O
to	O
the	O
proposed	O
claim	O
-	O
aware	O
encoder	O
,	O
counterfactual	O
decoders	O
,	O
and	O
a	O
synergistic	O
judgment	O
predictor	O
,	O
our	O
model	O
achieves	O
better	O
performance	O
than	O
single	O
-	O
task	O
baselines	O
on	O
the	O
task	O
of	O
judgment	O
prediction	O
,	O
judgment	O
generation	O
in	O
court	O
's	O
view	O
and	O
court	O
's	O
view	O
generation	O
.	O

While	O
AI	O
is	O
gaining	O
adoption	O
in	O
legal	O
justice	O
(	O
Lin	O
et	O
al	O
,	O
2012	O
;	O
Zhong	O
et	O
al	O
,	O
2018	O
;	O
Hu	O
et	O
al	O
,	O
2018	O
;	O
Jiang	O
et	O
al	O
,	O
2018	O
;	O
Chalkidis	O
et	O
al	O
,	O
2019	O
)	O
,	O
any	O
subtle	O
statistical	O
miscalculation	O
may	O
trigger	O
serious	O
consequences	O
.	O
From	O
a	O
fairness	O
perspective	O
,	O
prior	O
studies	O
suggested	O
that	O
global	O
(	O
statistical	O
)	O
optimization	O
=	O
individual	O
(	O
demographic	O
)	O
fairness	O
(	O
Zemel	O
et	O
al	O
,	O
2013	O
)	O
,	O
and	O
this	O
ethical	O
concern	O
should	O
be	O
further	O
investigated	O
.	O
In	O
this	O
section	O
,	O
we	O
explore	O
the	O
following	O
ethical	O
issues	O
.	O
Target	O
User	O
:	O
According	O
to	O
the	O
report	O
of	O
statistics	O
,	O
a	O
typical	O
active	O
trial	O
judge	O
closed	O
around	O
250	O
cases	O
in	O
a	O
year	O
.	O
Trial	O
judges	O
suffering	O
from	O
'	O
daunting	O
workload	O
'	O
is	O
becoming	O
an	O
critical	O
issue	O
(	O
Duan	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
proposed	O
algorithm	O
is	O
designed	O
for	O
generating	O
the	O
court	O
's	O
view	O
draft	O
for	O
assisting	O
the	O
trial	O
judges	O
for	O
decision	B-TaskName
making	I-TaskName
.	O
This	O
work	O
is	O
an	O
algorithmic	O
investigation	O
,	O
but	O
such	O
algorithm	O
should	O
never	O
'	O
replace	O
'	O
human	O
judges	O
.	O
Human	O
knowledge	O
/	O
judgment	O
should	O
be	O
the	O
final	O
safeguard	O
to	O
protect	O
social	O
justice	O
and	O
individual	O
fairness	O
.	O
Potential	O
Error	B-MetricName
:	O
The	O
potential	O
error	O
would	O
be	O
as	O
follows	O
:	O
a	O
)	O
generating	O
a	O
wrong	O
judgment	O
and	O
b	O
)	O
generating	O
a	O
wrong	O
rationale	O
.	O
The	O
goal	O
of	O
our	O
algorithm	O
is	O
to	O
generate	O
a	O
draft	O
of	O
court	O
's	O
view	O
for	O
trail	O
judge	O
as	O
a	O
reference	O
,	O
and	O
judges	O
need	O
to	O
proofread	O
the	O
content	O
generated	O
from	O
algorithm	O
.	O
Demographic	O
Bias	O
:	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
addressing	O
the	O
bias	O
problem	O
from	O
the	O
data	O
generation	O
by	O
treating	O
the	O
variable	O
of	O
data	O
generation	O
as	O
confounder	O
in	O
back	O
-	O
door	O
adjustment	O
.	O
The	O
model	O
adoption	O
can	O
face	O
potential	O
demographic	O
bias	O
/	O
unfairness	O
challenges	O
,	O
such	O
as	O
gender	O
and	O
race	O
bias	O
in	O
the	O
training	O
data	O
.	O
To	O
further	O
ensure	O
the	O
model	O
fairness	O
,	O
in	O
the	O
future	O
,	O
algorithm	O
adoption	O
should	O
be	O
empowered	O
with	O
de	O
-	O
biased	O
legal	O
content	O
pretraining	O
,	O
which	O
could	O
avoid	O
potential	O
demographic	O
bias	O
.	O
For	O
instance	O
,	O
in	O
order	O
to	O
remove	O
gender	O
/	O
race	O
bias	O
,	O
system	O
could	O
use	O
(	O
Bolukbasi	O
et	O
al	O
,	O
2016	O
)	O
algorithm	O
to	O
debias	O
the	O
sensitive	O
gender	O
/	O
race	O
information	O
,	O
e.g.	O
,	O
replace	O
'	O
he	O
/	O
she	O
'	O
and	O
'	O
asian	O
/	O
hispanic	O
'	O
with	O
gender	O
/	O
race	O
neutral	O
words	O
for	O
pretraining	O
,	O
which	O
can	O
be	O
vital	O
for	O
legal	O
domain	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
Attentional	O
and	O
Counterfactual	O
based	O
Natural	O
Language	O
Genera	O
-	O
tion	O
(	O
AC	O
-	O
NLG	O
)	O
method	O
to	O
solve	O
the	O
task	O
of	O
court	O
's	O
view	O
generation	O
in	O
civil	O
cases	O
and	O
ensure	O
the	O
fairness	O
of	O
the	O
judgment	O
.	O
We	O
design	O
a	O
claim	O
-	O
aware	O
encoder	O
to	O
represent	O
the	O
fact	O
description	O
which	O
emphasizes	O
on	O
the	O
plaintiff	O
's	O
claim	O
,	O
as	O
well	O
as	O
a	O
pair	O
of	O
backdoor	O
-	O
inspired	O
counterfactual	O
decoders	O
to	O
generate	O
judgment	O
-	O
discriminative	O
court	O
's	O
views	O
(	O
both	O
supportive	O
and	O
non	O
-	O
supportive	O
views	O
)	O
and	O
to	O
eliminate	O
the	O
bias	O
that	O
arose	O
from	O
the	O
data	O
generation	O
mechanism	O
by	O
connecting	O
with	O
a	O
synergistic	O
judgment	O
predictive	O
model	O
.	O
The	O
experimental	O
results	O
show	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O
Based	O
on	O
the	O
AC	O
-	O
NLG	O
method	O
,	O
in	O
the	O
future	O
,	O
we	O
can	O
explore	O
the	O
following	O
directions	O
:	O
(	O
1	O
)	O
Improve	O
the	O
accuracy	B-MetricName
of	O
judgment	O
on	O
a	O
claim	O
-	O
level	O
.	O
(	O
2	O
)	O
Add	O
external	O
knowledge	O
(	O
e.g.	O
a	O
logic	O
graph	O
)	O
to	O
the	O
predictor	O
for	O
the	O
interpretability	O
of	O
the	O
model	O
.	O

1	O
.	O
The	O
defendants	O
B	O
and	O
C	O
jointly	O
repaid	O
the	O
loan	O
principal	O
of	O
$	O
20	O
,	O
000	O
and	O
the	O
interest	O
loss	B-MetricName
(	O
calculated	O
from	O
the	O
bank	O
's	O
loan	O
interest	O
rate	O
at	O
the	O
same	O
period	O
from	O
the	O
date	O
of	O
prosecution	O
to	O
the	O
date	O
when	O
the	O
judgment	O
is	O
confirmed	O
)	O
.	O
2.The	O
litigation	O
costs	O
in	O
this	O
case	O
are	O
paid	O
by	O
the	O
two	O
defendants	O
.	O

The	O
court	O
concluded	O
that	O
:	O
The	O
private	O
lending	O
relationship	O
between	O
the	O
plaintiff	O
and	O
the	O
defendant	O
is	O
established	O
and	O
effective	O
,	O
and	O
shall	O
be	O
protected	O
according	O
to	O
law	O
.	O
The	O
defendant	O
should	O
repay	O
the	O
loan	O
after	O
receiving	O
it	O
,	O
but	O
now	O
he	O
did	O
not	O
repay	O
,	O
it	O
is	O
obviously	O
a	O
breach	O
of	O
contract	O
.	O
Therefore	O
,	O
this	O
court	O
supports	O
the	O
claim	O
of	O
the	O
plaintiff	O
that	O
the	O
defendant	O
should	O
return	O
the	O
loan	O
of	O
$	O
20	O
,	O
000	O
and	O
the	O
corresponding	O
loss	B-MetricName
of	O
interest	O
Acceptance	O
.	O
The	O
plaintiff	O
claimed	O
that	O
the	O
defendant	O
should	O
pay	O
interest	O
,	O
but	O
did	O
not	O
provide	O
evidence	O
to	O
prove	O
that	O
both	O
of	O
them	O
clearly	O
agreed	O
on	O
the	O
interest	O
,	O
so	O
the	O
court	O
does	O
not	O
support	O
the	O
plaintiff	O
's	O
claim	O
for	O
interest	O
Rejection	O
.	O
The	O
plaintiff	O
withdrew	O
some	O
of	O
the	O
claims	O
in	O
the	O
court	O
hearing	O
,	O
and	O
this	O
court	O
permitted	O
it	O
.	O

To	O
start	O
with	O
,	O
the	O
input	O
vector	O
of	O
each	O
word	O
is	O
generated	O
by	O
utilizing	O
a	O
word	O
embedding	O
lookup	O
table	O
L	O
w	O
R	O
r×dw	O
and	O
a	O
positional	O
embedding	O
lookup	O
table	O
L	O
p	O
R	O
n×dp	O
,	O
where	O
d	O
w	O
is	O
the	O
dimension	O
of	O
word	B-TaskName
embeddings	I-TaskName
,	O
r	O
is	O
the	O
vocabulary	O
size	O
,	O
and	O
d	O
p	O
is	O
the	O
dimension	O
of	O
positional	O
embeddings	O
.	O
These	O
embedding	O
lookup	O
tables	O
will	O
map	O
s	O
=	O
{	O
w	O
1	O
,	O
...	O
,	O
w	O
n	O
}	O
to	O
{	O
e	O
1	O
w	O
,	O
...	O
,	O
e	O
n	O
w	O
}	O
and	O
{	O
e	O
1	O
p	O
,	O
...	O
,	O
e	O
n	O
p	O
}	O
,	O
respectively	O
.	O
For	O
our	O
base	O
models	O
(	O
not	O
using	O
a	O
pre	O
-	O
trained	O
language	O
model	O
)	O
,	O
e	O
i	O
w	O
will	O
be	O
projected	O
to	O
a	O
low	O
dimensional	O
vector	O
e	O
i	O
low	O
which	O
is	O
calculated	O
as	O
follows	O
:	O
e	O
i	O
low	O
=	O
σ	O
(	O
W	O
e	O
e	O
i	O
w	O
)	O
,	O
where	O
W	O
e	O
R	O
d	O
low	O
×dw	O
(	O
d	O
low	O
<	O
d	O
w	O
)	O
denotes	O
the	O
matrix	O
of	O
projection	O
and	O
σ	O
(	O
)	O
is	O
the	O
activation	B-HyperparameterName
function	I-HyperparameterName
.	O
In	O
this	O
case	O
,	O
t	O
i	O
in	O
the	O
input	O
T	O
=	O
{	O
t	O
1	O
,	O
...	O
,	O
t	O
n	O
}	O
is	O
rep	O
-	O
resented	O
by	O
[	O
e	O
i	O
low	O
;	O
e	O
i	O
p	O
]	O
and	O
d	O
model	O
=	O
d	O
low	O
+	O
d	O
p	O
.	O
For	O
a	O
pre	O
-	O
trained	O
language	O
model	O
like	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
t	O
i	O
equals	O
the	O
sum	O
of	O
e	O
i	O
w	O
,	O
e	O
i	O
p	O
,	O
and	O
e	O
i	O
s	O
,	O
where	O
e	O
s	O
=	O
{	O
e	O
1	O
s	O
,	O
...	O
,	O
e	O
n	O
s	O
}	O
(	O
the	O
dimension	O
of	O
e	O
i	O
s	O
is	O
d	O
p	O
)	O
represents	O
segment	O
embeddings	O
,	O
and	O
d	O
model	O
=	O
d	O
p	O
=	O
d	O
w	O
.	O
Then	O
,	O
the	O
input	O
vector	O
T	O
is	O
passed	O
to	O
multihead	O
self	O
-	O
attention	O
modules	O
,	O
where	O
a	O
feed	O
-	O
forward	O
network	O
and	O
an	O
add	O
-	O
norm	O
network	O
are	O
combined	O
in	O
sequence	O
to	O
generate	O
the	O
context	O
representation	O
of	O
each	O
layer	O
H	O
=	O
{	O
H	O
1	O
,	O
...	O
,	O
H	O
l	O
}	O
,	O
where	O
l	O
is	O
the	O
number	O
of	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
layers	O
and	O
H	O
i	O
=	O
{	O
H	O
1	O
i	O
,	O
...	O
,	O
H	O
n	O
i	O
}	O
.	O
H	O
i	O
can	O
be	O
calculated	O
as	O
follows	O
:	O
O	O
i	O
=	O
M	O
H	O
(	O
H	O
i−1	O
,	O
h	O
)	O
,	O
(	O
4	O
)	O
F	O
F	O
N	O
i	O
=	O
max	O
(	O
0	B-DatasetName
,	O
O	O
i	O
W	O
i	O
1	O
+	O
b	O
i	O
1	O
)	O
W	O
i	O
2	O
+	O
b	O
i	O
2	O
,	O
(	O
5	O
)	O
H	O
i	O
=	O
LN	O
(	O
H	O
i−1	O
+	O
F	O
F	O
N	O
i	O
)	O
,	O
(	O
6	O
)	O
where	O
h	O
is	O
the	O
number	O
of	O
attention	O
heads	O
,	O
H	O
0	B-DatasetName
=	O
T	O
,	O
the	O
matrices	O
W	O
i	O
1	O
R	O
d	O
model	O
×d	O
f	O
f	O
and	O
W	O
i	O
2	O
R	O
d	O
f	O
f	O
×d	O
model	O
represent	O
mappings	O
from	O
d	O
model	O
to	O
d	O
f	O
f	O
and	O
back	O
to	O
d	O
model	O
.	O
LN	O
(	O
)	O
is	O
a	O
layer	B-MethodName
normalization	I-MethodName
method	O
applying	O
to	O
sequential	O
data	O
(	O
Ba	O
et	O
al	O
,	O
2016	O
)	O
.	O
Finally	O
,	O
the	O
output	O
of	O
the	O
encoder	O
is	O
H	O
l	O
,	O
i.e.	O
,	O
the	O
last	O
layer	O
of	O
H.	O

The	O
single	O
-	O
task	O
version	O
of	O
our	O
approaches	O
is	O
TSMSA	O
.	O
Given	O
a	O
predicted	O
label	O
sequence	O
Y	O
and	O
a	O
sequential	O
representation	O
H	O
l	O
,	O
the	O
score	O
function	O
S	O
(	O
H	O
l	O
,	O
Y	O
)	O
can	O
be	O
defined	O
as	O
follows	O
:	O
S	O
(	O
H	O
l	O
,	O
Y	O
)	O
=	O
n	O
i=1	O
Q	O
y	O
i−1	O
,	O
y	O
i	O
+	O
n	O
i=1	O
P	O
i	O
,	O
y	O
i	O
,	O
(	O
7	O
)	O
P	O
=	O
H	O
l	O
W	O
p	O
+	O
b	O
p	O
,	O
(	O
8	O
)	O
where	O
the	O
matrix	O
Q	O
R	O
k×k	O
captures	O
the	O
relation	O
of	O
adjacent	O
labels	O
,	O
the	O
matrix	O
P	O
R	O
n×k	O
learns	O
the	O
relation	O
of	O
tokens	O
and	O
labels	O
,	O
and	O
the	O
matrices	O
W	O
p	O
R	O
d	O
model	O
×k	O
and	O
b	O
p	O
R	O
n×k	O
indicate	O
a	O
projection	O
operation	O
from	O
dimension	O
d	O
model	O
to	O
dimension	O
k.	O
In	O
the	O
above	O
,	O
k	O
means	O
the	O
dimension	O
of	O
the	O
label	O
space	O
.	O
Then	O
,	O
the	O
linear	O
-	O
chain	O
CRF	B-MethodName
is	O
exploited	O
to	O
calculate	O
the	O
conditional	O
probability	O
of	O
the	O
predicted	O
sequence	O
Y	O
as	O
follows	O
:	O
p	O
(	O
Y	O
|	O
H	O
l	O
)	O
=	O
exp	O
(	O
S	O
(	O
H	O
l	O
,	O
Y	O
)	O
)	O
Ỹ	O
Y	O
all	O
exp	O
(	O
S	O
(	O
H	O
l	O
,	O
Ỹ	O
)	O
)	O
,	O
(	O
9	O
)	O
where	O
Y	O
all	O
denotes	O
the	O
set	O
of	O
all	O
possible	O
sequential	O
labels	O
.	O
So	O
the	O
loss	B-MetricName
of	O
a	O
sentence	O
can	O
be	O
calculated	O
by	O
the	O
negative	O
log	O
likelihood	O
as	O
follows	O
:	O
L	O
(	O
s	O
)	O
=	O
−	O
log	O
p	O
(	O
Y	O
|	O
H	O
l	O
)	O
.	O
(	O
10	O
)	O

By	O
integrating	O
aspect	O
and	O
opinion	O
term	B-TaskName
extraction	I-TaskName
(	O
task	O
0	B-DatasetName
)	O
and	O
TOWE	O
(	O
task	O
1	O
)	O
into	O
a	O
multi	O
-	O
task	O
architecture	O
,	O
we	O
propose	O
a	O
MT	O
-	O
TSMSA	O
method	O
for	O
AOPE	O
.	O
MT	O
-	O
TSMSA	O
can	O
be	O
defined	O
as	O
using	O
a	O
sentence	O
H	O
l	O
and	O
a	O
task	O
i	O
d	O
{	O
0	B-DatasetName
,	O
1	O
}	O
to	O
calculate	O
the	O
conditional	O
probability	O
p	O
(	O
Y	O
|	O
H	O
l	O
,	O
i	O
d	O
)	O
.	O
When	O
the	O
task	O
i	O
d	O
equals	O
0	B-DatasetName
,	O
it	O
means	O
aspect	O
and	O
opinion	O
term	B-TaskName
extraction	I-TaskName
.	O
For	O
TOWE	O
,	O
the	O
task	O
i	O
d	O
is	O
1	O
.	O
Some	O
examples	O
are	O
shown	O
in	O
Figure	O
1	O
(	O
d	O
)	O
.	O
Aiming	O
at	O
handling	O
different	O
tasks	O
,	O
different	O
score	O
functions	O
S	O
0	B-DatasetName
(	O
H	O
l	O
,	O
Y	O
0	B-DatasetName
)	O
and	O
S	O
1	O
(	O
H	O
l	O
,	O
Y	O
1	O
)	O
are	O
defined	O
,	O
where	O
S	O
0	B-DatasetName
(	O
)	O
and	O
S	O
1	O
(	O
)	O
have	O
different	O
parameter	O
matrices	O
,	O
Y	O
0	B-DatasetName
(	O
Y	O
0	B-DatasetName
i	O
{	O
B	O
-	O
ASP	O
,	O
I	O
-	O
ASP	O
,	O
B	O
-	O
OP	O
,	O
I	O
-	O
OP	O
O	O
}	O
)	O
and	O
Y	O
1	O
(	O
Y	O
1	O
i	O
{	O
B	O
,	O
I	O
,	O
O	O
,	O
[	O
SEP	O
]	O
}	O
)	O
represent	O
the	O
sequential	O
labels	O
of	O
aspect	O
and	O
opinion	O
term	B-TaskName
extraction	I-TaskName
,	O
and	O
TOWE	O
,	O
respectively	O
.	O
So	O
the	O
conditional	O
probabilities	O
of	O
the	O
predicted	O
sequences	O
Y	O
0	B-DatasetName
and	O
Y	O
1	O
can	O
be	O
calculated	O
as	O
follows	O
:	O
p	O
(	O
Y	O
0	B-DatasetName
|	O
H	O
l	O
,	O
i	O
d	O
=	O
0	B-DatasetName
)	O
=	O
exp	O
(	O
S	O
0	B-DatasetName
(	O
H	O
l	O
,	O
Y	O
0	B-DatasetName
)	O
)	O
Ỹ	O
Y	O
0	B-DatasetName
all	O
exp	O
(	O
S	O
0	B-DatasetName
(	O
H	O
l	O
,	O
Ỹ	O
)	O
)	O
,	O
(	O
11	O
)	O
p	O
(	O
Y	O
1	O
|	O
H	O
l	O
,	O
i	O
d	O
=	O
1	O
)	O
=	O
exp	O
(	O
S	O
1	O
(	O
H	O
l	O
,	O
Y	O
1	O
)	O
)	O
Ỹ	O
Y	O
1	O
all	O
exp	O
(	O
S	O
1	O
(	O
H	O
l	O
,	O
Ỹ	O
)	O
)	O
,	O
(	O
12	O
)	O
where	O
Y	O
0	B-DatasetName
all	O
denotes	O
the	O
set	O
of	O
all	O
possible	O
sequential	O
labels	O
of	O
task	O
0	B-DatasetName
and	O
Y	O
1	O
all	O
represents	O
the	O
set	O
of	O
all	O
possible	O
sequential	O
labels	O
of	O
task	O
1	O
.	O
The	O
loss	B-MetricName
of	O
a	O
sentence	O
is	O
also	O
calculated	O
by	O
the	O
negative	O
log	O
likelihood	O
as	O
follows	O
:	O
L	O
(	O
s	O
,	O
i	O
d	O
)	O
=	O
−	O
log	O
p	O
(	O
Y	O
|	O
H	O
l	O
,	O
i	O
d	O
)	O
.	O
(	O
13	O
)	O
Given	O
M	O
sentences	O
S	O
=	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
...	O
,	O
s	O
M	O
}	O
with	O
i	O
d	O
=	O
{	O
i	O
d	O
1	O
,	O
...	O
,	O
i	O
d	O
M	O
}	O
,	O
we	O
can	O
minimize	O
the	O
loss	B-MetricName
for	O
training	O
:	O
J	O
(	O
θ	B-HyperparameterName
)	O
=	O
M	O
k=1	O
(	O
(	O
1	O
−	O
i	O
d	O
k	O
)	O
λ	O
+	O
i	O
d	O
k	O
)	O
L	O
(	O
s	O
k	O
,	O
i	O
d	O
k	O
)	O
,	O
(	O
14	O
)	O
where	O
λ	O
is	O
the	O
hyper	O
-	O
parameter	O
used	O
to	O
balance	O
these	O
two	O
tasks	O
.	O

For	O
the	O
TOWE	O
task	O
,	O
Fan	O
et	O
al	O
(	O
2019	O
)	O
utilize	O
300dimension	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
vectors	O
which	O
are	O
pre	O
-	O
trained	O
on	O
unlabeled	O
data	O
of	O
840	O
billion	O
tokens	O
to	O
initialize	O
word	O
embedding	O
vectors	O
in	O
IOG	O
.	O
The	O
word	B-TaskName
embeddings	I-TaskName
are	O
fixed	O
at	O
the	O
stage	O
of	O
training	O
.	O
For	O
fair	O
comparison	O
,	O
we	O
use	O
the	O
same	O
fixed	O
word	B-TaskName
embeddings	I-TaskName
in	O
TSMSA	O
(	O
Base	O
)	O
.	O
We	O
randomly	O
select	O
20	O
%	O
of	O
the	O
training	O
set	O
as	O
the	O
development	O
set	O
for	O
adjusting	O
all	O
hyper	O
-	O
parameters	O
.	O
The	O
value	O
of	O
d	O
model	O
is	O
128	O
,	O
and	O
the	O
numbers	O
of	O
attention	O
heads	O
and	O
layers	O
are	O
4	O
and	O
6	O
,	O
respectively	O
.	O
In	O
addition	O
,	O
the	O
dropout	O
rate	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
and	O
maximal	O
sequence	O
length	O
are	O
set	O
to	O
0.5	O
,	O
0.001	O
,	O
and	O
100	O
,	O
respectively	O
.	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
is	O
adopted	O
to	O
optimize	O
our	O
model	O
.	O
Pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
like	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
can	O
be	O
applied	O
to	O
our	O
methods	O
,	O
and	O
we	O
adopt	O
BERT	B-MethodName
-	O
base	O
3	O
model	O
,	O
where	O
d	O
model	O
is	O
768	O
and	O
the	O
number	O
of	O
attention	O
heads	O
and	O
layers	O
are	O
both	O
12	O
.	O
Other	O
hyper	O
-	O
parameters	O
include	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
BERT	B-MethodName
and	O
CRF	B-MethodName
,	O
the	O
maximal	O
sequence	O
length	O
,	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
.	O
Based	O
on	O
the	O
development	O
set	O
,	O
these	O
hyper	O
-	O
parameters	O
are	O
set	O
to	O
5e	O
-	O
5	O
,	O
2e	O
-	O
4	O
,	O
100	O
,	O
and	O
8	O
,	O
respectively	O
.	O
Unless	O
otherwise	O
mentioned	O
,	O
λ	O
is	O
set	O
to	O
1	O
.	O
To	O
be	O
consistent	O
with	O
various	O
baselines	O
(	O
Fan	O
et	O
al	O
,	O
2019	O
;	O
Chen	O
et	O
al	O
,	O
2020	O
;	O
,	O
the	O
term	O
-	O
level	O
F1	B-MetricName
score	I-MetricName
is	O
used	O
as	O
the	O
evaluation	O
metric	O
for	O
both	O
TOWE	O
and	O
AOPE	O
tasks	O
.	O
Term	O
-	O
level	O
means	O
that	O
the	O
boundaries	O
of	O
the	O
span	O
are	O
the	O
same	O
as	O
the	O
ground	O
-	O
truth	O
.	O
For	O
the	O
AOPE	O
task	O
,	O
the	O
consistency	O
of	O
a	O
predicted	O
aspect	O
-	O
opinion	O
pair	O
with	O
the	O
labeled	O
pair	O
indicates	O
the	O
correctness	O
of	O
prediction	O
.	O
based	O
methods	O
are	O
poor	O
because	O
the	O
rules	O
only	O
cover	O
a	O
small	O
number	O
of	O
cases	O
.	O
By	O
utilizing	O
BiL	O
-	O
STM	O
or	O
BERT	B-MethodName
as	O
the	O
encoder	O
to	O
extract	O
opinion	O
terms	O
,	O
the	O
BiLSTM	B-MethodName
/	O
BERT	B-MethodName
+	O
Distance	O
-	O
rule	O
perform	O
much	O
better	O
than	O
other	O
rule	O
-	O
based	O
methods	O
.	O
However	O
,	O
these	O
methods	O
can	O
not	O
deal	O
with	O
the	O
one	O
-	O
to	O
-	O
many	O
case	O
.	O
Secondly	O
,	O
TC	O
-	O
BiLSTM	B-MethodName
and	O
TF	O
-	O
BERT	B-MethodName
extract	O
static	O
word	B-TaskName
embeddings	I-TaskName
for	O
aspects	O
and	O
then	O
incorporate	O
them	O
into	O
sentence	O
representation	O
by	O
concatenation	O
or	O
addition	O
.	O
Nevertheless	O
,	O
the	O
results	O
of	O
TC	O
-	O
BiLSTM	B-MethodName
and	O
TF	O
-	O
BERT	B-MethodName
are	O
still	O
over	O
10	O
%	O
lower	O
than	O
IOG	O
/	O
TSMSA	O
(	O
Base	O
)	O
and	O
SDRN	O
/	O
TSMSA	O
(	O
BERT	B-MethodName
)	O
,	O
respectively	O
.	O
It	O
reveals	O
that	O
the	O
static	O
word	O
embedding	O
is	O
not	O
a	O
good	O
representation	O
of	O
the	O
aspect	O
and	O
the	O
concatenation	O
/	O
addition	O
operation	O
is	O
not	O
good	O
enough	O
to	O
represent	O
the	O
specific	O
aspect	O
.	O
Finally	O
,	O
IOG	O
is	O
a	O
state	O
-	O
ofthe	O
-	O
art	O
baseline	O
method	O
for	O
TOWE	O
and	O
the	O
performance	O
of	O
TSMSA	O
(	O
Base	O
)	O
trained	O
by	O
the	O
same	O
word	O
embedding	O
is	O
similar	O
to	O
IOG	O
,	O
which	O
indicates	O
the	O
effectiveness	O
in	O
capturing	O
the	O
representation	O
of	O
a	O
specific	O
aspect	O
with	O
the	O
symbol	O
"	O
[	O
SEP	O
]	O
"	O
.	O
Furthermore	O
,	O
the	O
pre	O
-	O
trained	O
language	O
model	O
BERT	B-MethodName
can	O
be	O
applied	O
to	O
our	O
basic	O
method	O
.	O
The	O
F1	B-MetricName
score	I-MetricName
of	O
TSMSA	O
(	O
BERT	B-MethodName
)	O
is	O
in	O
average	O
8	O
%	O
higher	O
than	O
TSMSA	O
(	O
Base	O
)	O
and	O
IOG	O
.	O
SDRN	O
,	O
which	O
also	O
exploits	O
BERT	B-MethodName
as	O
the	O
encoder	O
,	O
passes	O
the	O
information	O
of	O
the	O
aspect	O
through	O
a	O
synchronization	O
unit	O
and	O
utilizes	O
supervised	O
self	O
-	O
attention	O
to	O
capture	O
this	O
information	O
.	O
Nevertheless	O
,	O
it	O
represents	O
the	O
specific	O
aspect	O
implicitly	O
,	O
which	O
might	O
have	O
an	O
negative	O
impact	O
on	O
capturing	O
the	O
information	O
of	O
targets	O
.	O
In	O
average	O
,	O
the	O
performance	O
of	O
SDRN	O
is	O
2	O
%	O
lower	O
than	O
TSMSA	O
(	O
BERT	B-MethodName
)	O
.	O
The	O
overall	O
results	O
reveal	O
that	O
our	O
proposed	O
method	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
TOWE	O
.	O

The	O
results	O
of	O
convergence	O
and	O
sensitivity	O
studies	O
are	O
shown	O
in	O
Figure	O
2	O
.	O
Figure	O
2	O
(	O
a	O
)	O
reveals	O
that	O
our	O
model	O
gradually	O
converges	O
as	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
increases	O
.	O
Although	O
the	O
dropout	O
rate	O
is	O
set	O
to	O
0.5	O
,	O
it	O
also	O
converges	O
smoothly	O
.	O
Figure	O
2	O
(	O
b	O
)	O
shows	O
the	O
effect	O
of	O
the	O
number	O
of	O
attention	O
heads	O
.	O
When	O
the	O
number	O
of	O
attention	O
heads	O
is	O
4	O
,	O
TSMSA	O
(	O
Base	O
)	O
achieves	O
stable	O
and	O
good	O
performance	O
,	O
and	O
as	O
the	O
value	O
increased	O
,	O
the	O
performance	O
might	O
be	O
better	O
.	O
Figure	O
2	O
(	O
c	O
)	O
shows	O
that	O
the	O
best	O
performance	O
is	O
achieved	O
when	O
the	O
number	O
of	O
multi	O
-	O
head	O
self	O
-	O
attention	B-HyperparameterName
layers	I-HyperparameterName
is	O
6	O
,	O
and	O
as	O
the	O
number	O
increased	O
,	O
the	O
model	O
might	O
be	O
confronted	O
with	O
overfitting	O
.	O
Figure	O
2	O
(	O
d	O
)	O
indicates	O
the	O
impact	O
of	O
λ	O
on	O
our	O
model	O
which	O
influences	O
the	O
learning	O
of	O
different	O
tasks	O
.	O
Stable	O
and	O
good	O
results	O
can	O
be	O
obtained	O
when	O
λ	O
=	O
1	O
,	O
and	O
better	O
performance	O
can	O
be	O
achieved	O
when	O
the	O
value	O
is	O
set	O
to	O
0.5	O
or	O
2	O
.	O
Compared	O
with	O
other	O
hyper	O
-	O
parameters	O
,	O
the	O
results	O
also	O
indicate	O
that	O
λ	O
has	O
a	O
relatively	O
small	O
impact	O
on	O
the	O
model	O
performance	O
.	O

To	O
further	O
compare	O
our	O
MT	O
-	O
TSMSA	O
(	O
BERT	B-MethodName
)	O
with	O
the	O
best	O
-	O
performing	O
baseline	O
of	O
SDRN	O
,	O
we	O
here	O
conduct	O
a	O
case	O
study	O
by	O
following	O
(	O
Chen	O
et	O
al	O
,	O
2020	O
)	O
.	O
As	O
shown	O
in	O
Table	O
6	O
,	O
both	O
SDRN	O
and	O
MT	O
-	O
TSMSA	O
(	O
BERT	B-MethodName
)	O
perform	O
well	O
in	O
extracting	O
aspectopinion	O
pairs	O
from	O
complicated	O
relations	O
.	O
But	O
in	O
some	O
cases	O
like	O
Case	O
4	O
,	O
SDRN	O
misses	O
the	O
pair	O
of	O
(	O
watching	O
videos	O
,	O
hot	O
)	O
.	O
The	O
reason	O
may	O
be	O
that	O
the	O
massive	O
hyper	O
-	O
parameters	O
in	O
SDRN	O
have	O
a	O
great	O
impact	O
on	O
the	O
effect	O
.	O
For	O
example	O
,	O
the	O
threshold	O
β	B-HyperparameterName
in	O
the	O
relation	O
synchronization	O
mechanism	O
of	O
SDRN	O
will	O
largely	O
affect	O
the	O
results	O
of	O
the	O
model	O
.	O
On	O
the	O
other	O
hand	O
,	O
our	O
method	O
can	O
extract	O
all	O
the	O
pairs	O
because	O
it	O
introduces	O
fewer	O
hyper	O
-	O
parameters	O
,	O
which	O
leads	O
to	O
stable	O
results	O
.	O
However	O
,	O
in	O
Case	O
5	O
,	O
our	O
method	O
can	O
not	O
extract	O
the	O
pair	O
.	O
The	O
rea	O
-	O
son	O
is	O
that	O
task	O
0	B-DatasetName
of	O
MT	O
-	O
TSMSA	O
(	O
BERT	B-MethodName
)	O
fails	O
to	O
extract	O
the	O
aspect	O
term	O
"	O
log	O
into	O
the	O
system	O
"	O
.	O
Moreover	O
,	O
the	O
in	O
-	O
depth	O
reason	O
is	O
that	O
for	O
the	O
aspect	O
term	B-TaskName
extraction	I-TaskName
task	O
,	O
the	O
performance	O
of	O
SDRN	O
(	O
i.e.	O
,	O
83.67	O
%	O
,	O
89.49	O
%	O
,	O
and	O
74.05	O
%	O
)	O
is	O
better	O
than	O
that	O
of	O
MT	O
-	O
TSMSA	O
(	O
BERT	B-MethodName
)	O
,	O
i.e.	O
,	O
83.11	O
%	O
,	O
84.85	O
%	O
,	O
and	O
72.69	O
%	O
on	O
the	O
datasets	O
from	O
(	O
Chen	O
et	O
al	O
,	O
2020	O
)	O
.	O

We	O
formulate	O
frame	O
detection	O
as	O
a	O
multilabel	O
classification	O
problem	O
for	O
each	O
of	O
the	O
three	O
typologies	O
,	O
using	O
our	O
dataset	O
to	O
train	O
supervised	O
models	O
.	O
Experimental	O
Setup	O
Our	O
proposed	O
model	O
is	O
a	O
RoBERTa	B-MethodName
model	O
(	O
Liu	O
et	O
al	O
,	O
2019b	O
)	O
trained	O
using	O
binary	O
cross	O
-	O
entropy	O
on	O
the	O
CLS	O
token	O
.	O
We	O
consider	O
both	O
(	O
i	O
)	O
a	O
model	O
trained	O
using	O
the	O
roberta	O
-	O
base	O
parameters	O
and	O
(	O
ii	O
)	O
a	O
second	O
model	O
that	O
has	O
first	O
been	O
fine	O
-	O
tuned	O
on	O
our	O
full	O
set	O
of	O
immigration	O
tweets	O
using	O
masked	O
-	O
language	O
modeling	O
.	O
Fine	O
tuning	O
was	O
performed	O
for	O
60	O
epochs	O
.	O
In	O
both	O
models	O
,	O
early	B-MethodName
stopping	I-MethodName
is	O
used	O
to	O
avoid	O
overfitting	O
.	O
Models	O
are	O
compared	O
with	O
two	O
baselines	O
:	O
random	O
prediction	O
,	O
and	O
logistic	B-MethodName
regression	I-MethodName
with	O
unigram	O
and	O
bigram	O
features	O
.	O
Each	O
model	O
was	O
trained	O
five	O
times	O
with	O
different	O
random	O
seeds	B-DatasetName
and	O
we	O
report	O
bootstrapped	O
mean	O
performance	O
.	O
Results	O
The	O
fine	O
-	O
tuned	O
RoBERTa	B-MethodName
model	O
significantly	O
outperforms	O
all	O
baselines	O
(	O
Table	O
2	O
)	O
.	O
RoBERTa	B-MethodName
has	O
the	O
most	O
substantial	O
gains	O
over	O
logistic	B-MethodName
regression	I-MethodName
for	O
low	O
-	O
frequency	O
frames	O
(	O
Supplementary	B-DatasetName
Material	I-DatasetName
C	O
,	O
Figure	O
8	O
)	O
.	O
These	O
gains	O
for	O
rare	O
frames	O
are	O
essential	O
for	O
analyzing	O
immigra	O
-	O
tion	O
discourse	O
on	O
social	O
media	O
in	O
order	O
to	O
capture	O
diverse	O
perspectives	O
and	O
arguments	O
.	O
Table	O
3	O
shows	O
several	O
evaluation	O
metrics	O
separated	O
by	O
frame	O
type	O
.	O
Precision	B-MetricName
,	O
recall	O
,	O
and	O
F1	B-MetricName
are	O
calculated	O
as	O
unweighted	O
averages	O
over	O
all	O
frames	O
belonging	O
to	O
each	O
category	O
.	O
Overall	O
,	O
issue	O
-	O
generic	O
policy	O
and	O
narrative	O
frames	O
can	O
be	O
detected	O
more	O
effectively	O
than	O
issue	O
-	O
specific	O
frames	O
.	O
This	O
difference	O
reflects	O
that	O
issue	O
-	O
specific	O
frames	O
were	O
sparser	O
in	O
the	O
training	O
data	O
,	O
but	O
also	O
that	O
detecting	O
these	O
frames	O
is	O
inherently	O
more	O
challenging	O
because	O
it	O
requires	O
jointly	O
reasoning	O
about	O
immigration	O
-	O
related	O
topics	O
and	O
how	O
these	O
topics	O
affect	O
immigrants	O
.	O
For	O
example	O
,	O
tweets	O
about	O
immigrants	O
committing	O
crimes	O
and	O
tweets	O
about	O
hate	O
crimes	O
committed	O
against	O
immigrants	O
have	O
distinct	O
issue	O
-	O
specific	O
frames	O
(	O
threat	O
:	O
public	O
order	O
and	O
victim	O
:	O
discrimination	O
)	O
,	O
even	O
though	O
these	O
texts	O
can	O
be	O
linguistically	O
quite	O
similar	O
.	O
Given	O
some	O
thematic	O
similarities	O
between	O
typologies	O
,	O
we	O
tested	O
an	O
additional	O
model	O
that	O
jointly	O
predicted	O
frames	O
from	O
all	O
three	O
typologies	O
using	O
the	O
fine	O
-	O
tuned	O
RoBERTa	B-MethodName
model	O
;	O
however	O
,	O
the	O
resulting	O
model	O
offered	O
worse	O
performance	O
than	O
any	O
single	O
-	O
typology	O
model	O
,	O
suggesting	O
minimal	O
benefits	O
of	O
cross	O
-	O
typology	O
learning	O
.	O
Supplementary	O
Section	O
C	O
contains	O
additional	O
model	O
performance	O
analyses	O
by	O
frame	O
and	O
region	O
.	O
Hegemonic	O
Framing	O
Conservative	O
media	O
's	O
framing	O
of	O
political	O
issues	O
is	O
known	O
to	O
be	O
more	O
consistent	O
,	O
coordinated	O
,	O
and	O
hegemonic	O
than	O
mainstream	O
media	O
,	O
which	O
has	O
been	O
vital	O
to	O
the	O
success	O
of	O
the	O
American	O
conservative	O
movement	O
(	O
Hemmer	O
,	O
2016	O
;	O
Speakman	O
and	O
Funk	O
,	O
2020	O
)	O
.	O
If	O
the	O
same	O
pattern	O
holds	O
for	O
social	O
media	O
,	O
we	O
would	O
expect	O
automated	O

Overgeneralizing	O
highly	O
-	O
correlated	O
features	O
Many	O
words	O
and	O
phrases	O
do	O
not	O
directly	O
cue	O
frames	O
,	O
but	O
are	O
highly	O
-	O
correlated	O
.	O
The	O
model	O
makes	O
erroneous	O
predictions	O
when	O
such	O
features	O
are	O
used	O
in	O
different	O
contexts	O
(	O
e.g.	O
violence	O
against	O
immigrants	O
,	O
rather	O
than	O
immigrants	O
being	O
violent	O
)	O
Lunaria	O
's	O
figures	O
from	O
2018	O
recorded	O
12	O
shootings	O
,	O
two	O
murders	O
and	O
33	O
physical	O
assaults	O
against	O
migrants	O
in	O
the	O
first	O
two	O
months	O
since	O
Salvini	O
entered	O
government	O
.	O
Model	O
missed	O
Victim	O
:	O
Humanitarian	O
frame	O
Pronoun	O
ambiguity	O
Coreference	B-TaskName
resolution	I-TaskName
is	O
often	O
not	O
possible	O
and	O
annotators	O
avoided	O
making	O
assumptions	O
to	O
resolve	O
ambiguities	O
.	O
For	O
example	O
,	O
"	O
you	O
"	O
can	O
be	O
used	O
to	O
discuss	O
individuals	O
'	O
experiences	O
(	O
episodic	O
)	O
but	O
its	O
impersonal	O
sense	O
can	O
be	O
in	O
broad	O
generalizations	O
(	O
thematic	O
)	O
.	O
It	O
's	O
worse	O
when	O
you	O
have	O
immigrant	O
parents	O
who	O
do	O
n't	O
speak	O
the	O
language	O
cause	O
you	O
have	O
to	O
deal	O
with	O
all	O
the	O
paperwork	O
,	O
be	O
the	O
translator	O
for	O
them	O
whenever	O
they	O
go	O
(	O
...	O
)	O
its	O
tiring	O
but	O
someone	O
has	O
to	O
Model	O
predicted	O
Episodic	O
but	O
referent	O
is	O
unclear	O
frame	O
detection	O
to	O
achieve	O
higher	O
performance	O
on	O
conservative	O
tweets	O
due	O
to	O
more	O
linguistic	O
regularities	O
across	O
messages	O
.	O
Indeed	O
,	O
we	O
find	O
that	O
issuegeneric	O
and	O
issue	O
-	O
specific	O
classifiers	O
achieve	O
higher	O
F1	B-MetricName
scores	O
on	O
tweets	O
written	O
by	O
conservative	O
authors	O
compared	O
to	O
liberal	O
authors	O
(	O
Figure	O
1	O
)	O
,	O
even	O
though	O
there	O
are	O
fewer	O
conservative	O
tweets	O
in	O
the	O
training	O
data	O
(	O
334	O
conservative	O
vs	O
385	O
liberal	O
tweets	O
)	O
.	O
Higher	O
model	O
performance	O
on	O
conservative	O
tweets	O
suggests	O
that	O
,	O
like	O
political	O
and	O
media	O
elites	O
,	O
conservatives	O
on	O
social	O
media	O
are	O
more	O
consistent	O
than	O
liberals	O
in	O
their	O
linguistic	O
framing	O
of	O
immigration	O
.	O
Error	B-MetricName
Analysis	O
We	O
identify	O
classification	O
errors	O
by	O
qualitatively	O
analyzing	O
a	O
random	O
sample	O
of	O
200	O
tweets	O
that	O
misclassified	O
at	O
least	O
one	O
frame	O
.	O
Table	O
4	O
shows	O
the	O
most	O
common	O
categories	O
of	O
errors	O
.	O

We	O
are	O
interested	O
in	O
the	O
successful	O
classification	O
of	O
aggressive	O
posts	O
only	O
and	O
therefore	O
,	O
rather	O
than	O
reporting	O
precision	O
,	O
recall	O
and	O
F	O
-	O
measures	O
,	O
we	O
re	O
-	O
port	O
accuracy	B-MetricName
as	O
in	O
equation	O
(	O
1	O
)	O
:	O
Accuracy	B-MetricName
=	O
true	O
positives	O
true	O
positives	O
+	O
false	O
negatives	O
(	O
1	O
)	O

All	O
test	O
and	O
training	O
texts	O
were	O
lower	O
-	O
cased	O
and	O
transformed	O
into	O
document	O
-	O
term	O
matrices	O
using	O
the	O
text2vec	O
package	O
for	O
R	O
(	O
Selivanov	O
and	O
Wang	O
,	O
2017	O
)	O
.	O
For	O
each	O
value	O
of	O
threshold	O
t	O
from	O
1	O
to	O
10	O
,	O
the	O
training	O
texts	O
were	O
assigned	O
true	O
and	O
false	O
labels	O
according	O
to	O
their	O
attack	O
score	O
s	O
where	O
aggression	O
is	O
true	O
if	O
s	O
≥	O
t.	O
We	O
trained	O
an	O
extreme	O
gradient	O
boosting	O
(	O
XG	O
-	O
Boost	O
)	O
classifier	O
with	O
the	O
R	O
package	O
xgboost	O
(	O
Chen	O
et	O
al	O
,	O
2018	O
)	O
.	O
Boosting	O
is	O
an	O
additive	O
technique	O
whereby	O
new	O
models	O
are	O
added	O
to	O
correct	O
the	O
errors	O
made	O
by	O
existing	O
models	O
thus	O
far	O
:	O
models	O
are	O
added	O
sequentially	O
until	O
no	O
further	O
improvements	O
can	O
be	O
made	O
.	O
In	O
gradient	O
boosting	O
,	O
new	O
models	O
predict	O
the	O
residuals	O
or	O
errors	O
of	O
prior	O
models	O
using	O
a	O
gradient	O
descent	O
algorithm	O
.	O
XG	O
-	O
Boost	O
is	O
known	O
to	O
work	O
well	O
with	O
sparse	O
matrices	O
,	O
which	O
is	O
the	O
kind	O
of	O
input	O
associated	O
with	O
textual	O
data	O
,	O
and	O
in	O
NLP	O
terms	O
has	O
been	O
shown	O
to	O
perform	O
competitively	O
in	O
sentiment	B-TaskName
analysis	I-TaskName
shared	O
tasks	O
(	O
Nasim	O
,	O
2017	O
;	O
Jabreel	O
and	O
Moreno	O
,	O
2018	O
)	O
.	O
To	O
avoid	O
over	O
-	O
fitting	O
we	O
set	O
parameters	O
fairly	O
conservatively	O
,	O
with	O
a	O
maximum	O
tree	O
depth	O
of	O
6	O
,	O
the	O
number	O
of	O
rounds	O
at	O
10	O
and	O
early	B-MethodName
stopping	I-MethodName
set	O
to	O
5	O
,	O
gamma	B-HyperparameterName
at	O
1	O
,	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
at	O
0.3	O
.	O
We	O
report	O
classifier	O
accuracy	B-MetricName
according	O
to	O
equation	O
(	O
1	O
)	O
on	O
gold	O
aggression	O
:	O
true	O
labels	O
in	O
our	O
CrimeBB	O
test	O
corpus	O
.	O
Recall	B-MetricName
that	O
we	O
do	O
not	O
compare	O
XGBoost	O
with	O
other	O
classifiers	O
,	O
as	O
our	O
focus	O
is	O
on	O
the	O
training	O
data	O
rather	O
than	O
performance	O
.	O
In	O
future	O
work	O
we	O
can	O
investigate	O
other	O
models	O
including	O
neural	O
networks	O
,	O
though	O
logistic	B-MethodName
regression	I-MethodName
has	O
in	O
some	O
cases	O
out	O
-	O
performed	O
neural	O
nets	O
in	O
the	O
detection	O
of	O
abusive	B-TaskName
language	I-TaskName
(	O
Park	O
and	O
Fung	O
,	O
2017	O
)	O
.	O
As	O
the	O
value	O
of	O
t	O
increases	O
the	O
size	O
of	O
the	O
aggression	O
:	O
true	O
dataset	O
decreases	O
,	O
as	O
seen	O
in	O
Table	O
1	O
.	O
To	O
ensure	O
any	O
change	O
in	O
accuracy	B-MetricName
is	O
not	O
due	O
to	O
the	O
decrease	O
in	O
aggression	O
:	O
true	O
training	O
instances	O
,	O
we	O
run	O
a	O
second	O
experiment	O
in	O
which	O
for	O
all	O
values	O
of	O
t	O
both	O
label	O
subsets	O
(	O
aggression	O
:	O
true	O
and	O
aggression	O
:	O
false	O
)	O
are	O
randomly	O
reduced	O
to	O
3223	O
instances	O
-	O
the	O
size	O
of	O
the	O
smallest	O
attack	O
score	O
subcorpus	O
(	O
per	O
the	O
cumulative	O
n.posts	O
column	O
in	O
Table	O
1	O
)	O
.	O
For	O
this	O
latter	O
experiment	O
we	O
report	O
accuracies	O
averaged	O
over	O
one	O
hundred	O
runs	O
to	O
smooth	O
variation	O
in	O
the	O
random	O
sampling	O
process	O
(	O
identified	O
as	O
'	O
Acc	B-MetricName
.	O
Control	O
'	O
in	O
Table	O
3	O
)	O
.	O

Classification	B-TaskName
accuracies	O
are	O
shown	O
in	O
Table	O
3	O
5	O
.	O
It	O
is	O
apparent	O
that	O
in	O
both	O
training	O
data	O
settings	O
-	O
controlled	O
and	O
non	O
-	O
controlled	O
(	O
'	O
all	O
'	O
)	O
-	O
the	O
accuracy	B-MetricName
of	O
aggression	B-TaskName
identification	I-TaskName
reduces	O
as	O
the	O
true	O
/	O
false	O
cut	O
-	O
off	O
threshold	O
t	O
increases	O
.	O
In	O
the	O
case	O
of	O
the	O
controlled	O
training	O
data	O
setting	O
there	O
is	O
at	O
first	O
a	O
small	O
increase	O
in	O
accuracy	B-MetricName
as	O
t	O
rises	O
from	O
1	O
to	O
3	O
.	O
This	O
result	O
suggests	O
that	O
the	O
levels	O
in	O
the	O
Wiki	O
-	O
Comments	O
Corpus	O
most	O
closely	O
matching	O
the	O
aggressive	O
posts	O
on	O
HackForums	O
are	O
those	O
in	O
the	O
attack	O
score	O
range	O
1	O
to	O
5	O
,	O
and	O
that	O
the	O
optimal	O
value	O
of	O
t	O
is	O
between	O
2	O
and	O
3	O
.	O
To	O
illustrate	O
the	O
rise	O
and	O
fall	O
in	O
classification	O
accuracy	B-MetricName
as	O
t	O
increases	O
,	O
we	O
plot	O
accuracies	O
as	O
boxplots	O
for	O
the	O
100	O
runs	O
in	O
the	O
controlled	O
training	O
data	O
setting	O
(	O
Figure	O
4.3	O
)	O
.	O
The	O
boxplots	O
show	O
medians	O
(	O
the	O
thick	O
horizontal	O
bars	O
)	O
,	O
first	O
and	O
third	O
quar	O
-	O
tiles	O
(	O
Q1	O
,	O
Q3	O
,	O
shown	O
by	O
the	O
hinges	O
)	O
,	O
and	O
whiskers	O
extending	O
as	O
far	O
as	O
1.5	O
*	O
IQR	B-DatasetName
where	O
IQR	B-DatasetName
is	O
the	O
inter	O
-	O
quartile	O
range	O
between	O
Q1	O
and	O
Q3	O
.	O
Datapoints	O
beyond	O
the	O
whiskers	O
are	O
outliers	O
and	O
are	O
plotted	O
individually	O
.	O

We	O
have	O
shown	O
that	O
abusive	B-TaskName
language	I-TaskName
in	O
an	O
online	O
hacking	O
forum	O
is	O
relatively	O
mild	O
compared	O
to	O
that	O
found	O
in	O
Wikipedia	O
page	O
edit	O
comments	O
.	O
We	O
propose	O
that	O
the	O
tendency	O
of	O
forum	O
users	O
to	O
on	O
the	O
whole	O
engage	O
in	O
constructive	O
and	O
informative	O
discourse	O
results	O
in	O
positive	O
behaviour	O
and	O
non	O
-	O
toxic	O
language	O
.	O
WikiComments	O
,	O
on	O
the	O
other	O
hand	O
,	O
is	O
made	O
up	O
of	O
debates	O
about	O
the	O
rights	O
and	O
wrongs	O
of	O
page	O
edits	O
,	O
and	O
perhaps	O
inevitably	O
this	O
adversarial	O
set	O
up	O
allows	O
more	O
aggressive	O
behaviours	O
to	O
manifest	O
themselves	O
in	O
writing	O
.	O
In	O
future	O
work	O
we	O
evidently	O
need	O
to	O
annotate	O
more	O
data	O
so	O
that	O
we	O
have	O
more	O
than	O
100	O
examples	O
of	O
abusive	B-TaskName
language	I-TaskName
from	O
CrimeBB	O
.	O
Due	O
to	O
the	O
low	O
hit	O
rate	O
for	O
abusive	B-TaskName
language	I-TaskName
in	O
CrimeBB	O
texts	O
(	O
100	O
in	O
4123	O
,	O
for	O
instance	O
)	O
we	O
can	O
investigate	O
automatic	O
annotation	O
of	O
further	O
chunks	O
of	O
the	O
data	O
,	O
along	O
with	O
supervised	O
sampling	O
from	O
those	O
new	O
annotations	O
to	O
check	O
their	O
quality	O
.	O
These	O
labelled	O
data	O
on	O
a	O
larger	O
scale	O
will	O
allow	O
us	O
to	O
analyse	O
more	O
general	O
patterns	O
of	O
behaviour	O
such	O
as	O
individual	O
and	O
community	O
-	O
wide	O
trends	O
over	O
time	O
,	O
how	O
aggression	O
surfaces	O
and	O
is	O
dealt	O
with	O
by	O
moderators	O
,	O
and	O
linguistic	O
facets	O
of	O
aggressive	O
behaviour	O
such	O
as	O
homophobia	O
,	O
racism	O
,	O
misogyny	O
and	O
so	O
on	O
.	O
We	O
can	O
also	O
investigate	O
other	O
Internet	O
domains	O
such	O
as	O
social	O
media	O
,	O
other	O
forums	O
and	O
potentially	O
the	O
Dark	O
Web	O
,	O
but	O
also	O
other	O
sections	O
of	O
CrimeBB	O
,	O
such	O
as	O
the	O
reputation	O
voting	O
area	O
within	O
HackForums	O
in	O
which	O
we	O
might	O
expect	O
to	O
find	O
more	O
vitriolic	O
interactions	O
given	O
that	O
votes	O
can	O
be	O
both	O
positive	O
and	O
negative	O
and	O
accompanied	O
by	O
review	O
-	O
like	O
texts	O
.	O
Finally	O
,	O
we	O
are	O
also	O
interested	O
in	O
applications	O
of	O
our	O
research	O
,	O
including	O
the	O
questions	O
of	O
desired	O
accuracy	B-MetricName
of	O
any	O
deployed	O
system	O
,	O
the	O
appropriate	O
actions	O
to	O
take	O
,	O
and	O
the	O
ethics	O
of	O
data	O
collection	O
,	O
analysis	O
and	O
intervention	O
(	O
Kennedy	O
et	O
al	O
,	O
2017	O
;	O
.	O
One	O
option	O
could	O
be	O
to	O
create	O
an	O
alert	O
system	O
for	O
forum	O
moderators	O
,	O
thereby	O
offering	O
real	O
-	O
world	O
impact	O
for	O
our	O
work	O
while	O
allowing	O
the	O
appropriate	O
authorities	O
to	O
take	O
action	O
when	O
necessary	O
(	O
Kumar	B-DatasetName
et	O
al	O
,	O
2018	O
)	O
.	O

Knowledge	O
Bases	O
(	O
KBs	O
)	O
can	O
be	O
created	O
in	O
many	O
different	O
ways	O
depending	O
on	O
the	O
availability	O
of	O
external	O
resources	O
and	O
specific	O
application	O
needs	O
.	O
Recently	O
,	O
much	O
work	O
in	O
Natural	O
Language	O
Processing	O
focused	O
on	O
Knowledge	B-TaskName
Base	I-TaskName
Completion	I-TaskName
(	O
Nickel	O
et	O
al	O
,	O
2016a	O
,	O
KBC	O
)	O
,	O
the	O
task	O
of	O
enriching	O
and	O
refining	O
existing	O
KBs	O
.	O
Many	O
different	O
methods	O
have	O
been	O
explored	O
for	O
KBC	O
,	O
including	O
exploitation	O
of	O
resources	O
such	O
as	O
text	O
corpora	O
(	O
Snow	O
et	O
al	O
,	O
2006	O
;	O
Mintz	O
et	O
al	O
,	O
2009	O
;	O
Aprosio	O
et	O
al	O
,	O
2013	O
)	O
or	O
other	O
KBs	O
Bryl	O
and	O
Bizer	O
,	O
2014	O
)	O
for	O
acquiring	O
additional	O
knowledge	O
.	O
Alternative	O
approaches	O
,	O
in	O
contrast	O
,	O
primarily	O
rely	O
on	O
existing	O
information	O
from	O
the	O
KB	O
itself	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
;	O
Nickel	O
et	O
al	O
,	O
2016b	O
)	O
used	O
as	O
ground	O
-	O
truth	O
to	O
simultaneously	O
learn	O
continuous	O
representations	O
of	O
KB	O
concepts	O
and	O
relations	O
,	O
which	O
are	O
used	O
to	O
infer	O
additional	O
KB	O
relations	O
.	O
Finally	O
,	O
Open	B-TaskName
Information	I-TaskName
Extraction	I-TaskName
methods	O
looked	O
at	O
ways	O
to	O
extract	O
large	O
amounts	O
of	O
facts	O
from	O
Web	O
-	O
scale	O
corpora	O
in	O
order	O
to	O
acquire	O
open	O
-	O
domain	O
KBs	O
Faruqui	O
and	O
Kumar	B-DatasetName
,	O
2015	O
,	O
inter	O
alia	O
)	O
;	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
a	O
different	O
,	O
yet	O
complementary	O
task	O
,	O
which	O
is	O
a	O
necessary	O
step	O
when	O
inducing	O
novel	O
KBs	O
from	O
scratch	O
,	O
namely	O
extracting	O
clean	O
taxonomies	O
from	O
noisy	O
knowl	O
-	O
edge	O
graphs	O
.	O
State	O
-	O
of	O
-	O
the	O
-	O
art	O
algorithms	O
differ	O
by	O
the	O
amount	O
of	O
human	O
supervision	O
required	O
and	O
their	O
ability	O
to	O
respect	O
some	O
topological	O
properties	O
while	O
pruning	O
.	O
Approaches	O
like	O
those	O
of	O
Kozareva	O
and	O
Hovy	O
(	O
2010	O
)	O
,	O
Velardi	O
et	O
al	O
(	O
2013	O
)	O
and	O
Kapanipathi	O
et	O
al	O
(	O
2014	O
)	O
,	O
for	O
instance	O
,	O
apply	O
different	O
topological	O
pruning	O
strategies	O
that	O
require	O
to	O
specify	O
the	O
root	O
and	O
leaf	O
concept	O
nodes	O
of	O
the	O
KB	O
in	O
advance	O
-	O
i.e.	O
,	O
a	O
predefined	O
set	O
of	O
abstract	O
toplevel	O
concepts	O
and	O
lower	O
terminological	O
nodes	O
,	O
respectively	O
.	O
The	O
approach	O
of	O
avoids	O
such	O
supervision	O
on	O
the	O
basis	O
of	O
an	O
iterative	O
method	O
that	O
uses	O
an	O
efficient	O
variant	O
of	O
topological	O
sorting	O
(	O
Tarjan	O
,	O
1972	O
)	O
for	O
cycle	O
pruning	O
.	O
Such	O
lack	O
of	O
supervision	O
,	O
however	O
,	O
comes	O
at	O
the	O
cost	O
of	O
not	O
being	O
able	O
to	O
preserve	O
the	O
original	O
connectivity	O
between	O
the	O
top	O
(	O
abstract	O
)	O
and	O
the	O
bottom	O
(	O
instance	O
)	O
concepts	O
.	O
Random	O
edge	O
removal	O
,	O
in	O
fact	O
,	O
can	O
lead	O
to	O
disconnected	O
components	O
,	O
a	O
problem	O
shared	O
with	O
the	O
OntoLearn	O
Reloaded	O
approach	O
(	O
Velardi	O
et	O
al	O
,	O
2013	O
)	O
,	O
which	O
can	O
not	O
ensure	O
such	O
property	O
when	O
used	O
to	O
approximate	O
the	O
solution	O
for	O
a	O
large	O
noisy	O
graph	O
.	O
Our	O
work	O
goes	O
one	O
step	O
beyond	O
the	O
previous	O
contributions	O
by	O
presenting	O
a	O
new	O
efficient	O
algorithm	O
that	O
is	O
able	O
to	O
extract	O
a	O
clean	O
taxonomy	O
from	O
a	O
noisy	O
knowledge	O
graph	O
without	O
needing	O
to	O
know	O
in	O
advance	O
-	O
that	O
is	O
,	O
having	O
to	O
manually	O
specifythe	O
top	O
-	O
level	O
and	O
leaf	O
concepts	O
of	O
the	O
taxonomy	O
,	O
while	O
preserving	O
the	O
overall	O
connectivity	O
of	O
the	O
graph	O
.	O
We	O
achieve	O
this	O
by	O
projecting	O
the	O
information	O
from	O
a	O
reference	O
KB	O
such	O
as	O
,	O
for	O
instance	O
,	O
WordNet	O
(	O
Fellbaum	O
,	O
1998	O
)	O
,	O
onto	O
the	O
input	O
noisy	O
KB	O
on	O
the	O
basis	O
of	O
pre	O
-	O
existing	O
KB	O
links	O
-	O
which	O
in	O
turn	O
can	O
be	O
automatically	O
generated	O
with	O
high	O
precision	O
using	O
any	O
of	O
the	O
existing	O
solutions	O
for	O
KB	O
mapping	O
(	O
Navigli	O
and	O
Ponzetto	O
,	O
2012	O
;	O
Faralli	O
et	O
al	O
,	O
2016	O
,	O
inter	O
alia	O
)	O
or	O
by	O
relying	O
on	O
ground	O
truth	O
information	O
from	O
the	O
Linguistic	O
Linked	O
Open	O
Data	O
cloud	O
(	O
Chiarcos	O
et	O
al	O
,	O
2012	O
)	O
.	O
Some	O
aspects	O
of	O
the	O
proposed	O
approachnamely	O
,	O
the	O
propagation	O
of	O
the	O
nodes	O
'	O
weights	O
through	O
the	O
graph	O
,	O
which	O
we	O
metaphorically	O
represent	O
as	O
the	O
flow	O
of	O
a	O
contrast	O
medium	O
across	O
nodes	O
(	O
Section	O
3.3	O
)	O
-	O
are	O
somewhat	O
similar	O
in	O
spirit	O
to	O
spreading	O
activation	O
(	O
Collins	O
and	O
Loftus	O
,	O
1975	O
)	O
and	O
random	O
walks	O
on	O
graphs	O
(	O
Lovász	O
,	O
1993	O
)	O
approaches	O
.	O
However	O
,	O
in	O
contrast	O
to	O
spreading	O
activation	O
approaches	O
we	O
leverage	O
the	O
graph	O
directionality	O
in	O
order	O
to	O
reach	O
all	O
the	O
possible	O
nodes	O
within	O
the	O
same	O
connected	O
components	O
.	O
More	O
-	O
over	O
,	O
in	O
contrast	O
to	O
random	O
walks	O
on	O
graphs	O
our	O
method	O
is	O
deterministic	O
in	O
nature	O
.	O
Here	O
,	O
we	O
argue	O
for	O
the	O
choice	O
of	O
a	O
deterministic	O
approach	O
,	O
like	O
ours	O
,	O
that	O
does	O
not	O
require	O
tuning	O
of	O
parameters	O
:	O
its	O
termination	O
is	O
guaranteed	O
by	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
iterations	I-HyperparameterName
,	O
which	O
we	O
bind	O
by	O
the	O
maximal	O
diameter	O
|	O
E	O
|	O
for	O
a	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
.	O
Generally	O
,	O
random	O
walk	O
algorithms	O
would	O
provide	O
an	O
approximation	O
that	O
may	O
lead	O
to	O
a	O
less	O
precise	O
estimation	O
of	O
the	O
order	O
induced	O
by	O
the	O
contrast	O
medium	O
level	O
.	O
3	O
The	O
ContrastMedium	O
Algorithm	O

We	O
benchmark	O
ContrastMedium	O
using	O
a	O
variety	O
of	O
metrics	O
that	O
are	O
meant	O
to	O
capture	O
structural	O
properties	O
of	O
the	O
output	O
taxonomies	O
(	O
to	O
describe	O
the	O
impact	O
of	O
pruning	O
on	O
the	O
original	O
NKGs	O
)	O
,	O
as	O
well	O
as	O
an	O
estimation	O
of	O
their	O
overall	O
quality	O
.	O
Edge	O
compression	O
:	O
the	O
ratio	O
of	O
the	O
number	O
of	O
pruned	O
edges	O
over	O
the	O
total	O
number	O
of	O
edges	O
:	O
C	O
E	O
G	O
,	O
G	O
=	O
|	O
E	O
G	O
|	O
−	O
|	O
E	O
G	O
|	O
|	O
E	O
G	O
|	O
where	O
E	O
G	O
and	O
E	O
G	O
represent	O
the	O
number	O
of	O
edges	O
found	O
within	O
the	O
input	O
(	O
G	O
)	O
and	O
pruned	O
(	O
G	O
)	O
taxonomy	O
,	O
respectively	O
.	O
Pruning	O
accuracy	B-MetricName
:	O
the	O
performance	O
on	O
a	O
3	O
-	O
way	O
classification	O
task	O
to	O
automatically	O
detect	O
the	O
level	O
of	O
granularity	O
of	O
a	O
concept	O
as	O
a	O
proxy	O
to	O
quantify	O
the	O
overall	O
quality	O
of	O
the	O
output	O
taxonomies	O
.	O
Pruning	O
accuracy	B-MetricName
is	O
estimated	O
using	O
gold	O
-	O
standard	O
annotations	O
that	O
are	O
created	O
from	O
a	O
random	O
sample	O
of	O
1	O
,	O
000	O
nodes	O
for	O
each	O
NKG	O
.	O
Two	O
annotators	O
with	O
previous	O
experience	O
in	O
knowledge	O
acquisition	O
and	O
engineering	O
were	O
asked	O
to	O
provide	O
for	O
each	O
concept	O
whether	O
it	O
can	O
be	O
classified	O
as	O
:	O
i	O
)	O
a	O
root	O
,	O
top	O
-	O
level	O
abstract	O
concept	O
-	O
i.e.	O
,	O
any	O
of	O
entity	O
,	O
object	O
,	O
etc	O
.	O
and	O
more	O
in	O
general	O
nodes	O
that	O
correspond	O
to	O
abstract	O
concepts	O
that	O
we	O
can	O
expect	O
to	O
be	O
part	O
of	O
a	O
core	O
ontology	B-MethodName
such	O
as	O
,	O
for	O
instance	O
,	O
DOLCE	O
(	O
Gangemi	O
et	O
al	O
,	O
2002	O
)	O
;	O
ii	O
)	O
a	O
leaf	O
terminological	O
node	O
(	O
i.e.	O
,	O
instances	O
such	O
as	O
Lady	O
Gaga	O
or	O
Porsche	O
911	O
)	O
;	O
iii	O
)	O
or	O
a	O
middle	O
-	O
level	O
concept	O
(	O
e.g.	O
,	O
celebrity	O
or	O
cars	O
,	O
concepts	O
not	O
fitting	O
into	O
any	O
of	O
the	O
previous	O
classes	O
)	O
.	O
An	O
adjudication	O
procedure	O
was	O
used	O
to	O
resolve	O
any	O
discrepancy	O
between	O
the	O
two	O
annotators	O
:	O
the	O
inter	O
-	O
annotator	O
agreement	O
after	O
adjudication	O
is	O
κ	O
=	O
0.657	O
(	O
Fleiss	O
,	O
1971	O
)	O
,	O
with	O
most	O
disagreement	O
occurring	O
on	O
the	O
identification	O
of	O
abstract	O
,	O
core	O
ontology	B-MethodName
concepts	O
.	O
A	O
local	O
3	O
-	O
way	O
classification	O
task	O
provides	O
a	O
rather	O
crude	O
way	O
to	O
estimate	O
the	O
performance	O
on	O
inducing	O
hierarchical	O
structures	O
like	O
taxonomies	O
.	O
Here	O
,	O
we	O
use	O
it	O
primarily	O
to	O
benchmark	O
how	O
well	O
ContrastMedium	O
compares	O
against	O
other	O
,	O
structure	O
-	O
agnostic	O
algorithms	O
used	O
within	O
state	O
-	O
ofthe	O
-	O
art	O
solutions	O
such	O
as	O
,	O
for	O
instance	O
,	O
Tarjan	O
's	O
topological	O
sorting	O
(	O
Section	O
2	O
)	O
,	O
which	O
only	O
break	O
cycles	O
in	O
a	O
random	O
fashion	O
.	O
Given	O
ground	O
-	O
truth	O
concept	O
granularity	O
judgements	O
,	O
we	O
compute	O
standard	O
accuracy	B-MetricName
for	O
each	O
of	O
the	O
three	O
classes	O
.	O
That	O
is	O
,	O
we	O
compare	O
the	O
system	O
outputs	O
against	O
the	O
gold	O
standards	O
and	O
obtain	O
three	O
accuracy	B-MetricName
measures	O
:	O
one	O
for	O
the	O
root	O
nodes	O
(	O
A	O
R	O
)	O
,	O
one	O
for	O
the	O
nodes	O
'	O
in	O
the	O
middle	O
'	O
(	O
A	O
M	O
)	O
and	O
finally	O
one	O
for	O
the	O
leaf	O
nodes	O
(	O
A	O
L	O
)	O
.	O
For	O
example	O
a	O
true	O
positive	O
root	O
node	O
is	O
a	O
node	O
annotated	O
as	O
a	O
root	O
node	O
in	O
the	O
gold	O
standard	O
and	O
having	O
no	O
incoming	O
edges	O
in	O
the	O
pruned	O
graph	O
.	O
Error	B-MetricName
Reduction	O
(	O
ER	O
)	O
:	O
finally	O
,	O
we	O
compute	O
the	O
relative	O
error	O
reduction	O
of	O
ContrastMedium	O
against	O
other	O
,	O
baseline	O
approaches	O
as	O
:	O
Baseline	O
|	O
V	O
G	O
|	O
|	O
E	O
G	O
|	O
C	O
E	O
G	O
,	O
G	O
|	O
V	O
G	O
|	O
|	O
E	O
G	O
|	O
C	O
E	O
G	O
,	O
G	O
A	O
R	O
A	O
M	O
A	O
L	O
A	O
R	O
A	O
M	O
A	O
L	O
news	O
-	O
p1	O
.6	O
170k	O
1.536k	O
0.15	O
%	O
170k	O
1.535k	O
0.18	O
%	O
98.9	O
98.3	O
99.3	O
93.3	O
94.6	O
95.3	O
0.62	O
news	O
-	O
p2.3	O
225k	O
1.867k	O
0.19	O
%	O
225k	O
1.866k	O
0.23	O
%	O
98.7	O
98.7	O
99.9	O
95.7	O
94.7	O
95.6	O
0.50	O
wiki	O
-	O
p1.8	O
183k	O
1.165k	O
0.18	O
%	O
183k	O
1.164k	O
0.22	O
%	O
97.6	O
94.7	O
97.3	O
93.1	O
87.3	O
94.1	O
0.41	O
wiki	O
-	O
p6.0	O
394k	O
1.897k	O
0.18	O
%	O
394k	O
1.896k	O
0.21	O
%	O
95.9	O
94.3	O
98.3	O
89.5	O
90.1	O
92.8	O
0.50	O
(	O
2015	O
)	O
based	O
on	O
Tarjan	O
's	O
topological	O
sorting	O
(	O
Section	O
2	O
)	O
,	O
which	O
iteratively	O
searches	O
for	O
a	O
cycle	O
(	O
until	O
no	O
cycle	O
can	O
be	O
found	O
)	O
and	O
randomly	O
removes	O
an	O
edge	O
from	O
it	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
only	O
algorithm	O
that	O
we	O
can	O
fairly	O
compare	O
with	O
,	O
since	O
alternative	O
solutions	O
all	O
need	O
to	O
know	O
the	O
sets	O
of	O
root	O
and	O
leaf	O
nodes	O
in	O
advance	O
.	O

Table	O
3	O
summarizes	O
the	O
performance	O
of	O
Con	O
-	O
trastMedium	O
on	O
the	O
four	O
automatically	O
acquired	O
NKGs	O
.	O
The	O
results	O
show	O
that	O
the	O
pruning	O
impact	O
of	O
our	O
approach	O
is	O
lower	O
than	O
that	O
of	O
the	O
baseline	O
(	O
an	O
average	O
of	O
1	O
K	O
edges	O
of	O
difference	O
,	O
cf	O
.	O
columns	O
3	O
and	O
6	O
)	O
,	O
which	O
also	O
determines	O
higher	O
edge	O
compression	O
C	O
E	O
G	O
,	O
G	O
values	O
for	O
the	O
baseline	O
method	O
.	O
Despite	O
being	O
less	O
aggressive	O
in	O
terms	O
of	O
the	O
number	O
of	O
edges	O
pruned	O
,	O
ContrastMedium	O
outperforms	O
the	O
Tarjan	O
-	O
based	O
algorithm	O
on	O
all	O
datasets	O
in	O
terms	O
of	O
accuracy	B-MetricName
.	O
Thanks	O
to	O
our	O
method	O
,	O
in	O
fact	O
,	O
we	O
are	O
able	O
to	O
achieve	O
,	O
even	O
despite	O
the	O
baseline	O
already	O
reaching	O
very	O
high	O
performance	O
levels	O
(	O
well	O
above	O
90	O
%	O
accuracy	B-MetricName
)	O
,	O
improvements	O
of	O
up	O
to	O
6	O
points	O
,	O
with	O
an	O
overall	O
error	O
reduction	O
between	O
around	O
40	O
%	O
and	O
60	O
%	O
.	O
To	O
provide	O
an	O
intuition	O
of	O
why	O
ContrastMedium	O
clearly	O
outperforms	O
the	O
baseline	O
approach	O
,	O
we	O
provide	O
in	O
Figure	O
2	O
an	O
exemplified	O
depiction	O
of	O
a	O
typical	O
case	O
on	O
which	O
the	O
baseline	O
fails	O
(	O
based	O
on	O
a	O
manually	O
inspected	O
random	O
sample	O
)	O
.	O
In	O
our	O
example	O
,	O
the	O
Tarjan	O
baseline	O
first	O
detects	O
the	O
cycle	O
C	O
1	O
=	O
(	O
lion	O
animal	O
feline	O
lion	O
)	O
and	O
randomly	O
decides	O
to	O
break	O
it	O
by	O
removing	O
the	O
edge	O
(	O
animal	O
feline	O
)	O
.	O
Next	O
,	O
it	O
detects	O
the	O
cycle	O
C	O
2	O
=	O
(	O
animal	O
great	O
apes	O
animal	O
)	O
and	O
randomly	O
decides	O
to	O
break	O
it	O
by	O
removing	O
the	O
edge	O
(	O
animal	O
great	O
apes	O
)	O
.	O
ContrastMedium	O
,	O
instead	O
,	O
after	O
the	O
shaking	O
of	O
the	O
graph	O
can	O
leverage	O
the	O
partial	O
ordering	O
of	O
the	O
nodes	O
(	O
based	O
on	O
the	O
concept	O
granularity	O
of	O
the	O
corresponding	O
concepts	O
)	O
to	O
select	O
the	O
edges	O
(	O
animal	O
,	O
feline	O
)	O
,	O
(	O
feline	O
,	O
lion	O
)	O
and	O
(	O
animal	O
,	O
great	O
apes	O
)	O
,	O
while	O
removing	O
all	O
remaining	O
wrong	O
and	O
redundant	O
edges	O
.	O

Our	O
experiments	O
are	O
designed	O
in	O
leave	O
-	O
one	O
-	O
topicout	O
cross	O
-	O
validation	O
setting	O
:	O
From	O
the	O
10	O
topicstance	O
pairs	O
in	O
the	O
dataset	O
,	O
we	O
use	O
nine	O
for	O
training	O
and	O
the	O
last	O
as	O
the	O
test	O
fold	O
,	O
and	O
we	O
repeat	O
this	O
once	O
for	O
each	O
possible	O
fold	O
.	O
This	O
way	O
,	O
no	O
topic	O
-	O
specific	O
knowledge	O
can	O
be	O
used	O
in	O
the	O
synthesis	O
process	O
.	O
For	O
each	O
given	O
basic	O
rhetorical	O
strategy	O
(	O
logosoriented	O
and	O
pathos	O
-	O
oriented	O
)	O
,	O
we	O
train	O
one	O
model	O
each	O
for	O
the	O
selection	O
,	O
the	O
arrangement	O
,	O
and	O
the	O
"	O
phrasing	O
"	O
of	O
argumentative	O
discourse	O
units	O
(	O
ADUs	O
)	O
on	O
the	O
nine	O
training	O
folds	O
.	O
The	O
arguments	O
synthesized	O
by	O
their	O
combination	O
are	O
then	O
evaluated	O
against	O
the	O
human	O
-	O
generated	O
arguments	O
in	O
the	O
test	O
folds	O
.	O
The	O
evaluation	O
covers	O
all	O
three	O
models	O
as	O
well	O
as	O
the	O
final	O
generated	O
argument	O
for	O
each	O
strategy	O
.	O
We	O
report	O
the	O
average	B-MetricName
accuracy	I-MetricName
across	O
all	O
ten	O
folds	O
for	O
each	O
of	O
the	O
models	O
.	O

In	O
each	O
training	O
/	O
test	O
experiment	O
for	O
one	O
of	O
the	O
two	O
strategies	O
,	O
we	O
first	O
abstract	O
all	O
ADUs	O
across	O
all	O
strategy	O
-	O
specific	O
topic	O
-	O
stance	O
pairs	O
by	O
extracting	O
the	O
LIWC	O
,	O
NRC	O
,	O
and	O
MPQA	B-DatasetName
features	O
,	O
as	O
described	O
in	O
Section	O
4.1	O
.	O
Then	O
,	O
we	O
cluster	O
the	O
given	O
training	O
set	O
using	O
standard	O
k	O
-	O
means	O
(	O
Ostrovsky	O
et	O
al	O
,	O
2012	O
)	O
.	O
After	O
some	O
initial	O
experiments	O
,	O
we	O
decide	O
to	O
set	O
k	O
to	O
6	O
,	O
because	O
this	O
best	O
balanced	O
the	O
distribution	O
of	O
arguments	O
over	O
clusters	O
,	O
and	O
showed	O
clear	O
strategyspecific	O
differences	O
.	O
3	O
Using	O
the	O
resulting	O
clustering	O
model	O
,	O
we	O
predicted	O
the	O
type	O
A	O
-	O
F	O
of	O
each	O
ADU	O
in	O
the	O
test	O
set	O
(	O
the	O
tenth	O
topic	O
)	O
.	O
Given	O
the	O
ADU	O
types	O
,	O
we	O
next	O
converted	O
the	O
human	O
-	O
generated	O
training	O
and	O
test	O
arguments	O
from	O
a	O
sequence	O
of	O
ADUs	O
to	O
a	O
sequence	O
of	O
ADU	O
types	O
.	O
After	O
that	O
,	O
we	O
trained	O
one	O
2	O
-	O
gram	O
and	O
one	O
3	O
-	O
gram	O
selection	O
language	O
.	O
4	O
In	O
Table	O
3	O
,	O
we	O
report	O
the	O
mean	O
perplexity	B-MetricName
of	O
the	O
models	O
for	O
both	O
strategies	O
.	O
As	O
shown	O
,	O
the	O
2	O
-	O
gram	O
perplexity	B-MetricName
is	O
lower	O
than	O
the	O
3	O
-	O
gram	O
perplexity	B-MetricName
in	O
both	O
cases	O
.	O
We	O
assume	O
that	O
the	O
reason	O
lies	O
in	O
the	O
limited	O
size	O
of	O
the	O
dataset	O
and	O
the	O
narrow	O
setting	O
:	O
Only	O
117	O
sentences	O
(	O
ADUs	O
)	O
are	O
given	O
per	O
strategy	O
for	O
training	O
,	O
with	O
a	O
vocabulary	O
size	O
of	O
6	O
(	O
number	O
of	O
ADU	O
types	O
)	O
.	O
Based	O
on	O
the	O
results	O
,	O
we	O
decided	O
to	O
use	O
the	O
2	O
-	O
gram	O
selection	O
language	O
model	O
to	O
generate	O
candidate	O
arguments	O
.	O

To	O
train	O
arrangement	O
as	O
described	O
in	O
Section	O
4.2	O
,	O
we	O
took	O
all	O
arguments	O
of	O
the	O
nine	O
training	O
topics	O
in	O
each	O
experiment	O
.	O
We	O
converted	O
each	O
argument	O
from	O
a	O
sequence	O
of	O
ADUs	O
to	O
a	O
sequence	O
of	O
ADU	O
roles	O
(	O
thesis	O
,	O
pro	O
,	O
and	O
con	O
)	O
.	O
After	O
that	O
,	O
we	O
trained	O
a	O
2	O
-	O
gram	O
and	O
3	O
-	O
gram	O
language	O
model	O
for	O
each	O
strategy	O
.	O
Table	O
4	O
lists	O
the	O
mean	O
perplexity	B-MetricName
values	O
over	O
the	O
10	O
folds	O
.	O
Here	O
,	O
the	O
perplexity	B-MetricName
is	O
lower	O
for	O
3	O
-	O
grams	O
than	O
for	O
2	O
-	O
grams	O
,	O
which	O
can	O
be	O
expected	O
to	O
yield	O
better	O
performance	O
.	O
Therefore	O
,	O
we	O
used	O
the	O
3	O
-	O
gram	O
language	O
model	O
to	O
filter	O
the	O
set	O
of	O
candidate	O
arguments	O
.	O

For	O
phrasing	O
(	O
in	O
terms	O
of	O
choosing	O
the	O
best	O
ADU	O
sequence	O
)	O
,	O
we	O
first	O
extracted	O
features	O
from	O
each	O
candidate	O
,	O
as	O
described	O
in	O
Section	O
4.3	O
.	O
Then	O
,	O
we	O
calculated	O
the	O
semantic	O
similarities	O
between	O
each	O
pair	O
of	O
adjacent	O
ADUs	O
as	O
follows	O
:	O
1	O
.	O
We	O
obtained	O
a	O
300	O
-	O
dimensional	O
word	O
embedding	O
for	O
each	O
word	O
in	O
an	O
ADU	O
using	O
the	O
pre	O
-	O
trained	O
GloVe	B-MethodName
common	O
-	O
crawl	O
model	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
.	O
5	O
2	O
.	O
We	O
averaged	O
the	O
embeddings	O
of	O
all	O
words	O
in	O
an	O
ADU	O
,	O
resulted	O
in	O
one	O
vector	O
representing	O
the	O
ADU	O
.	O
3	O
.	O
For	O
each	O
adjacent	O
pair	O
of	O
ADUs	O
,	O
we	O
computed	O
the	O
cosine	O
similarity	O
of	O
their	O
vectors	O
.	O
Figure	O
4	O
shows	O
a	O
histogram	O
of	O
the	O
distribution	O
of	O
the	O
cosine	O
similarities	O
of	O
each	O
adjacent	O
pair	O
of	O
5	O
The	O
used	O
model	O
can	O
be	O
found	O
here	O
:	O
http://nlp	O
.	O
stanford.edu/data/glove.42B.300d.zip	O
.	O
ADUs	O
(	O
i.e.	O
,	O
each	O
ADU	O
2	O
-	O
gram	O
)	O
in	O
logos	O
-	O
oriented	O
arguments	O
and	O
in	O
pathos	O
-	O
oriented	O
arguments	O
.	O
We	O
observe	O
a	O
generally	O
high	O
similarity	O
between	O
neighboring	O
ADUs	O
for	O
both	O
strategies	O
,	O
with	O
logos	O
-	O
oriented	O
2	O
-	O
grams	O
being	O
slightly	O
more	O
similar	O
on	O
average	O
.	O
Given	O
the	O
ADU	O
2	O
-	O
grams	O
,	O
we	O
train	O
a	O
linear	B-MethodName
regression	I-MethodName
model	O
that	O
predicts	O
the	O
sum	O
of	O
ADU	O
2	O
-	O
gram	O
probabilities	O
in	O
each	O
argument	O
.	O
In	O
case	O
of	O
the	O
logos	O
strategy	O
,	O
the	O
model	O
has	O
a	O
mean	O
squared	O
error	O
(	O
MSE	B-MetricName
)	O
of	O
0.05	O
.	O
In	O
case	O
of	O
pathos	O
the	O
MSE	B-MetricName
is	O
0.03	O
.	O

Up	O
to	O
this	O
point	O
,	O
we	O
trained	O
all	O
selection	O
,	O
arrangement	O
,	O
and	O
phrasing	O
models	O
10	O
times	O
.	O
Combining	O
the	O
three	O
models	O
for	O
each	O
strategy	O
,	O
we	O
finally	O
generated	O
one	O
argument	O
per	O
strategy	O
for	O
the	O
topic	O
-	O
stance	O
pair	O
left	O
out	O
in	O
each	O
experiments	O
.	O
Hence	O
,	O
we	O
ended	O
up	O
with	O
10	O
computationally	O
synthesized	O
arguments	O
per	O
strategy	O
in	O
total	O
.	O
We	O
evaluate	O
each	O
of	O
these	O
arguments	O
by	O
checking	O
whether	O
it	O
matches	O
any	O
of	O
the	O
13	O
humangenerated	O
ground	O
-	O
truth	O
arguments	O
given	O
per	O
topicstance	O
pair	O
.	O
The	O
matching	O
is	O
quantified	O
in	O
terms	O
of	O
n	O
-	O
gram	O
overlap	O
with	O
n	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
5	O
}	O
.	O
For	O
comparison	O
,	O
we	O
consider	O
a	O
baseline	O
that	O
randomly	O
generates	O
arguments	O
for	O
each	O
topic	O
-	O
stance	O
pair	O
as	O
follows	O
:	O
1	O
.	O
Select	O
a	O
random	O
thesis	O
unit	O
from	O
t	O
1	O
to	O
t	O
4	O
.	O
2	O
.	O
Select	O
a	O
random	O
con	O
unit	O
from	O
c	O
1	O
to	O
c	O
4	O
.	O
3	O
.	O
Select	O
three	O
random	O
pro	O
units	O
from	O
p	O
1	O
to	O
p	O
12	O
.	O
4	O
.	O
Randomly	O
arrange	O
the	O
selected	O
units	O
.	O
Table	O
5	O
:	O
Accuracy	B-MetricName
of	O
n	O
-	O
gram	O
overlaps	O
between	O
the	O
human	O
-	O
generated	O
arguments	O
for	O
each	O
strategy	O
and	O
the	O
arguments	O
computationally	O
synthesized	O
by	O
our	O
model	O
and	O
the	O
baseline	O
.	O
In	O
the	O
sequential	O
case	O
,	O
the	O
ordering	O
is	O
considered	O
,	O
in	O
the	O
non	O
-	O
sequential	O
case	O
,	O
it	O
is	O
ignored	O
.	O
The	O
better	O
result	O
in	O
each	O
experiment	O
is	O
marked	O
bold	O
,	O
if	O
any	O
.	O
Strategy	O
ID	O
Argument	O
Computationally	O
Synthesized	O
from	O
Five	O
Argumentative	O
Discourse	O
Units	O

Natural	B-TaskName
language	I-TaskName
inference	I-TaskName
is	O
the	O
task	O
of	O
predicting	O
,	O
given	O
two	O
fragments	O
of	O
text	O
,	O
whether	O
the	O
meaning	O
of	O
one	O
(	O
premise	O
)	O
entails	O
the	O
other	O
(	O
hypothesis	O
)	O
(	O
Dagan	O
et	O
al	O
,	O
2006	O
)	O
.	O
The	O
task	O
is	O
formulated	O
as	O
a	O
3	O
-	O
way	O
classification	O
problem	O
,	O
in	O
which	O
the	O
premise	O
and	O
hypothesis	O
pairs	O
are	O
labeled	O
as	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
,	O
if	O
their	O
relationship	O
could	O
not	O
be	O
directly	O
inferred	O
(	O
Bowman	O
et	O
al	O
,	O
2015b	O
)	O
.	O
NLI	O
has	O
been	O
widely	O
used	O
as	O
an	O
evaluation	O
task	O
for	O
language	O
understanding	O
,	O
and	O
there	O
have	O
been	O
a	O
large	O
number	O
of	O
challenging	O
datasets	O
,	O
which	O
have	O
been	O
used	O
to	O
further	O
our	O
understanding	O
of	O
the	O
capabilities	O
of	O
language	O
models	O
(	O
Wang	O
et	O
al	O
,	O
2018	O
.	O
Paired	O
data	O
for	O
figurative	O
language	O
is	O
relatively	O
sparse	O
,	O
and	O
there	O
is	O
a	O
gap	O
in	O
the	O
diagnostic	O
datasets	O
used	O
for	O
NLI	O
in	O
this	O
area	O
.	O
Previous	O
work	O
includes	O
the	O
literal	O
/	O
metaphoric	O
paraphrases	O
of	O
Mohammad	O
et	O
al	O
(	O
2016	O
)	O
and	O
Bizzoni	O
and	O
Lappin	O
(	O
2018	O
)	O
,	O
although	O
both	O
contain	O
only	O
hundreds	O
of	O
samples	O
,	O
insufficient	O
for	O
proper	O
model	O
training	O
and	O
evaluation	O
.	O
With	O
regard	O
to	O
NLI	O
,	O
early	O
work	O
proposed	O
the	O
task	O
of	O
textual	O
entailment	O
as	O
a	O
way	O
of	O
understanding	O
metaphor	O
processing	O
capabilities	O
Agerri	O
,	O
2008	O
)	O
.	O
Poliak	O
et	O
al	O
(	O
2018	O
)	O
build	O
a	O
dataset	O
for	O
diverse	O
NLI	O
,	O
which	O
includes	O
some	O
creative	O
language	O
such	O
as	O
puns	O
,	O
albeit	O
making	O
no	O
claims	O
with	O
regard	O
to	O
figurativeness	O
.	O
Zhou	O
et	O
al	O
(	O
2021	O
)	O
build	O
a	O
dataset	O
consisting	O
of	O
paired	O
idiomatic	O
and	O
literal	O
expressions	O
.	O
They	O
begin	O
with	O
a	O
set	O
of	O
823	O
idiomatic	O
expressions	O
yielding	O
5	O
,	O
170	O
sentences	O
,	O
and	O
had	O
annotators	O
manually	O
rewrite	O
sentences	O
containing	O
these	O
idioms	O
as	O
literal	O
expressions	O
.	O
We	O
expand	O
on	O
this	O
methodology	O
by	O
having	O
annotators	O
only	O
correct	O
definitions	O
for	O
the	O
idioms	O
themselves	O
and	O
use	O
these	O
definitions	O
to	O
automatically	O
generate	O
the	O
literal	O
interpretations	O
of	O
the	O
idioms	O
by	O
replacing	O
them	O
into	O
appropriate	O
contexts	O
:	O
this	O
allows	O
us	O
to	O
scale	O
up	O
to	O
over	O
24k	O
silver	O
sentences	O
.	O
We	O
also	O
expand	O
beyond	O
paraphrasing	O
by	O
incorporating	O
both	O
entailment	O
and	O
non	O
-	O
entailment	O
Their	O
dataset	O
consists	O
of	O
figurative	O
/	O
literal	O
pairs	O
recast	O
from	O
previously	O
developed	O
simile	O
and	O
metaphor	O
datasets	O
,	O
along	O
with	O
a	O
parallel	O
dataset	O
between	O
ironic	O
and	O
non	O
-	O
ironic	O
rephrasing	O
.	O
This	O
sets	O
the	O
groundwork	O
for	O
figurative	O
NLI	O
,	O
but	O
the	O
dataset	O
is	O
relatively	O
small	O
outside	O
of	O
the	O
irony	O
domain	O
,	O
and	O
the	O
non	O
-	O
entailments	O
are	O
generated	O
purely	O
by	O
replacing	O
words	O
with	O
their	O
antonyms	O
,	O
restricting	O
the	O
novelty	O
of	O
the	O
hypotheses	O
.	O
Their	O
dataset	O
is	O
relatively	O
easy	O
for	O
NLI	O
models	O
;	O
here	O
we	O
show	O
that	O
figurative	O
language	O
can	O
be	O
challenging	O
,	O
particularly	O
with	O
regard	O
to	O
non	O
-	O
entailments	O
.	O
Zhou	O
et	O
al	O
(	O
2021	O
)	O
and	O
Chakrabarty	O
et	O
al	O
(	O
2021a	O
)	O
provide	O
invaluable	O
resources	O
for	O
figurative	O
NLI	O
;	O
our	O
works	O
aims	O
to	O
covers	O
gaps	O
in	O
a	O
number	O
of	O
areas	O
.	O
First	O
,	O
we	O
generate	O
a	O
large	O
number	O
of	O
both	O
entailment	O
and	O
non	O
-	O
entailment	O
pairs	O
,	O
allowing	O
for	O
better	O
evaluation	O
of	O
adversarial	O
non	O
-	O
entailing	O
examples	O
.	O
Second	O
,	O
our	O
silver	O
methods	O
allow	O
for	O
rapid	O
development	O
of	O
larger	O
scale	O
data	O
,	O
allowing	O
for	O
model	O
training	O
and	O
evaluation	O
.	O
We	O
show	O
that	O
while	O
entailment	O
pairs	O
are	O
relatively	O
easy	O
(	O
accuracy	B-MetricName
scores	O
ranging	O
from	O
.86	O
to	O
.89	O
)	O
,	O
the	O
non	O
-	O
entailment	O
pairs	O
are	O
exceedingly	O
challenging	O
,	O
with	O
the	O
roberta	O
-	O
large	O
model	O
achieving	O
accuracy	B-MetricName
scores	O
ranging	O
from	O
.311	O
to	O
.539	O
.	O

We	O
obtain	O
baseline	O
NLI	O
models	O
by	O
fine	O
-	O
tuning	O
roberta	O
-	O
base	O
and	O
roberta	O
-	O
large	O
models	O
on	O
the	O
MNLI	B-DatasetName
dataset	O
(	O
Williams	O
et	O
al	O
,	O
2018	O
)	O
,	O
with	O
entailments	O
as	O
the	O
positive	O
class	O
and	O
all	O
others	O
as	O
the	O
negative	O
and	O
evaluate	O
them	O
on	O
their	O
original	O
test	O
sets	O
as	O
well	O
as	O
IMPLI	O
.	O
5	O
Due	O
to	O
variance	O
in	O
neural	O
model	O
performance	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2017	O
)	O
,	O
we	O
take	O
the	O
mean	O
score	O
over	O
5	O
runs	O
using	O
different	O
seeds	B-DatasetName
.	O
We	O
report	O
results	O
in	O
Table	O
5	O
.	O
We	O
observe	O
that	O
idiomatic	O
entailments	O
are	O
relatively	O
easy	O
to	O
classify	O
,	O
with	O
accuracy	B-MetricName
scores	O
over	O
.84	O
.	O
Non	O
-	O
entailments	O
were	O
much	O
more	O
challenging	O
.	O
Silver	O
pairs	O
generated	O
through	O
adversarial	O
definitions	O
were	O
especially	O
difficult	O
:	O
the	O
pairs	O
contain	O
high	O
lexical	O
overlap	O
,	O
and	O
in	O
many	O
cases	O
the	O
premise	O
and	O
hypotheses	O
are	O
semantically	O
similar	O
.	O
The	O
replacement	O
into	O
literal	O
samples	O
were	O
easier	O
,	O
as	O
the	O
idiomatic	O
definition	O
clashes	O
more	O
starkly	O
with	O
the	O
original	O
premise	O
,	O
making	O
non	O
-	O
entailment	O
predictions	O
more	O
likely	O
.	O
Consistent	O
with	O
Chakrabarty	O
et	O
al	O
(	O
2021a	O
)	O
's	O
work	O
in	O
metaphors	O
,	O
non	O
-	O
entailment	O
through	O
antonym	O
replacement	O
is	O
easiest	O
for	O
idioms	O
:	O
the	O
antonymic	O
relationship	O
can	O
be	O
a	O
marker	O
for	O
non	O
-	O
entailment	O
,	O
despite	O
the	O
high	O
word	O
overlap	O
.	O
With	O
regard	O
to	O
metaphors	O
,	O
silver	O
entailment	O
pairs	O
are	O
relatively	O
easy	O
.	O
Manual	O
pairs	O
are	O
more	O
challenging	O
but	O
are	O
still	O
much	O
easier	O
than	O
idioms	O
.	O
This	O
is	O
supported	O
by	O
the	O
fact	O
that	O
metaphors	O
are	O
common	O
in	O
everyday	O
language	O
:	O
these	O
models	O
have	O
likely	O
seen	O
the	O
same	O
(	O
or	O
similar	O
)	O
metaphors	O
in	O
training	O
.	O
Our	O
findings	O
show	O
that	O
in	O
fact	O
metaphoricity	O
may	O
not	O
be	O
particularly	O
challenging	O
for	O
deep	O
pre	O
-	O
trained	O
models	O
,	O
as	O
they	O
are	O
able	O
to	O
effectively	O
capture	O
the	O
metaphoric	O
entailment	O
relations	O
.	O
The	O
roberta	O
-	O
large	O
model	O
performs	O
better	O
for	O
metaphoric	O
expressions	O
than	O
roberta	O
-	O
base	O
,	O
but	O
the	O
difference	O
on	O
other	O
partitions	O
is	O
relatively	O
small	O
.	O
6	O
We	O
also	O
find	O
that	O
lexical	O
overlap	O
plays	O
a	O
significant	O
role	O
here	O
as	O
noted	O
by	O
previous	O
work	O
(	O
McCoy	O
et	O
al	O
,	O
2019	O
)	O
:	O
sentences	O
with	O
high	O
overlap	O
tend	O
to	O
be	O
classified	O
as	O
entailments	O
regardless	O
of	O
the	O
true	O
label	O
(	O
for	O
more	O
,	O
see	O
Appendix	O
B	O
)	O
.	O
We	O
note	O
that	O
the	O
manual	O
pairs	O
tend	O
to	O
be	O
more	O
difficult	O
for	O
both	O
idioms	O
and	O
metaphors	O
:	O
these	O
pairs	O
can	O
be	O
more	O
flexible	O
and	O
creative	O
,	O
whereas	O
the	O
silver	O
pairs	O
are	O
restricted	O
to	O
more	O
regular	O
patterns	O
.	O

We	O
use	O
a	O
fixed	O
set	O
of	O
hyperparameters	O
for	O
all	O
NLI	O
fine	O
-	O
tuning	O
experiments	O
:	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	O
−5	O
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
32	O
,	O
and	O
maximum	O
input	O
length	O
of	O
128	O
tokens	O
.	O
The	O
models	O
are	O
trained	O
for	O
3	O
epochs	O
.	O
We	O
used	O
the	O
HuggingFace	O
implementation	O
of	O
the	O
models	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O

This	O
paper	O
presents	O
our	O
system	O
details	O
and	O
results	O
of	O
participation	O
in	O
the	O
RDoC	O
Tasks	O
of	O
BioNLP	O
-	O
OST	B-DatasetName
2019	O
.	O
Research	O
Domain	O
Criteria	O
(	O
RDoC	O
)	O
construct	O
is	O
a	O
multi	O
-	O
dimensional	O
and	O
broad	O
framework	O
to	O
describe	O
mental	O
health	O
disorders	O
by	O
combining	O
knowledge	O
from	O
genomics	O
to	O
behaviour	O
.	O
Non	O
-	O
availability	O
of	O
RDoC	O
labelled	O
dataset	O
and	O
tedious	O
labelling	O
process	O
hinders	O
the	O
use	O
of	O
RDoC	O
framework	O
to	O
reach	O
its	O
full	O
potential	O
in	O
Biomedical	O
research	O
community	O
and	O
Healthcare	O
industry	O
.	O
Therefore	O
,	O
Task	O
-	O
1	O
aims	O
at	O
retrieval	O
and	O
ranking	O
of	O
PubMed	O
abstracts	O
relevant	O
to	O
a	O
given	O
RDoC	O
construct	O
and	O
Task	O
-	O
2	O
aims	O
at	O
extraction	O
of	O
the	O
most	O
relevant	O
sentence	O
from	O
a	O
given	O
PubMed	O
abstract	O
.	O
We	O
investigate	O
(	O
1	O
)	O
attention	O
based	O
supervised	O
neural	O
topic	O
model	O
and	O
SVM	B-MethodName
for	O
retrieval	O
and	O
ranking	O
of	O
PubMed	O
abstracts	O
and	O
,	O
further	O
utilize	O
BM25	O
and	O
other	O
relevance	O
measures	O
for	O
re	O
-	O
ranking	O
,	O
(	O
2	O
)	O
supervised	O
and	O
unsupervised	O
sentence	O
ranking	O
models	O
utilizing	O
multi	O
-	O
view	O
representations	O
comprising	O
of	O
query	O
-	O
aware	O
attention	O
-	O
based	O
sentence	O
representation	O
(	O
QAR	O
)	O
,	O
bag	O
-	O
of	O
-	O
words	O
(	O
BoW	O
)	O
and	O
TF	O
-	O
IDF	O
.	O
Our	O
best	O
systems	O
achieved	O
1st	O
rank	O
and	O
scored	O
0.86	O
mAP	B-MetricName
and	O
0.58	O
macro	O
average	B-MetricName
accuracy	I-MetricName
in	O
Task	O
-	O
1	O
and	O
Task	O
-	O
2	O
respectively	O
.	O

RDoc	O
-	O
IR	O
Task	O
-	O
1	O
:	O
The	O
task	O
aims	O
at	O
retrieving	O
and	O
ranking	O
the	O
PubMed	O
abstracts	O
(	O
within	O
each	O
of	O
the	O
eight	O
clusters	O
)	O
that	O
are	O
relevant	O
for	O
the	O
RDoC	O
construct	O
(	O
i.e	O
,	O
a	O
query	O
)	O
related	O
to	O
the	O
cluster	O
in	O
the	O
abstract	O
appears	O
.	O
The	O
training	O
data	O
consists	O
of	O
abstracts	O
(	O
title	O
+	O
sentences	O
)	O
each	O
annotated	O
with	O
one	O
or	O
more	O
RDoC	O
constructs	O
.	O
Test	O
data	O
consists	O
of	O
abstracts	O
without	O
annotation	O
and	O
the	O
goal	O
is	O
to	O
submit	O
a	O
ranked	O
lists	O
of	O
relevant	O
articles	O
for	O
each	O
medical	B-DatasetName
domain	I-DatasetName
RDoC	O
construct	O
.	O
RDoc	O
-	O
IE	O
Task	O
-	O
2	O
The	O
task	O
aims	O
at	O
extracting	O
the	O
most	O
relevant	O
sentence	O
from	O
each	O
PubMed	O
abstract	O
for	O
the	O
corresponding	O
RDoC	O
construct	O
.	O
The	O
input	O
consists	O
of	O
an	O
abstract	O
(	O
title	O
t	O
and	O
sentences	O
s	O
)	O
for	O
an	O
RDoC	O
construct	O
q.	O
The	O
training	O
data	O
consists	O
of	O
abstracts	O
each	O
annotated	O
with	O
one	O
RDoC	O
construct	O
and	O
the	O
most	O
relevant	O
sentence	O
.	O
Test	O
data	O
contains	O
abstracts	O
relevant	O
for	O
RDoC	O
constructs	O
and	O
the	O
goal	O
is	O
to	O
submit	O
a	O
list	O
of	O
predicted	O
most	O
relevant	O
sentence	O
for	O
each	O
abstract	O
.	O
Our	O
Contributions	O
:	O
Following	O
are	O
our	O
multifold	O
contributions	O
in	O
this	O
paper	O
:	O
(	O
1	O
)	O
RDoC	O
-	O
IR	O
Task	O
-	O
1	O
:	O
We	O
perform	O
document	O
(	O
or	O
abstract	O
)	O
ranking	O
in	O
two	O
steps	O
,	O
first	O
using	O
supervised	O
neural	O
topic	O
model	O
and	O
SVM	B-MethodName
.	O
Moreover	O
,	O
we	O
have	O
introduced	O
attentions	O
in	O
supervised	O
neural	O
topic	O
model	O
,	O
along	O
with	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
from	O
several	O
sources	O
.	O
Then	O
,	O
we	O
re	O
-	O
rank	O
documents	O
using	O
BM25	O
and	O
similarity	O
scores	O
between	O
query	O
and	O
query	O
-	O
aware	O
attention	O
-	O
based	O
document	O
representation	O
.	O
Comparing	O
with	O
other	O
participating	O
systems	O
in	O
the	O
shared	O
task	O
,	O
our	O
submission	O
is	O
ranked	O
1	O
st	O
with	O
a	O
mAP	B-MetricName
score	O
of	O
0.86	O
.	O
(	O
2	O
)	O
RDoC	O
-	O
IE	O
Task	O
-	O
2	O
:	O
We	O
have	O
addressed	O
the	O
sentence	O
ranking	O
task	O
by	O
introducing	O
unsupervised	O
and	O
supervised	O
sentence	O
ranking	O
schemes	O
.	O
Moreover	O
,	O
we	O
have	O
employed	O
multi	O
-	O
view	O
representations	O
consisting	O
of	O
bag	O
-	O
of	O
-	O
words	O
,	O
TF	O
-	O
IDF	O
and	O
query	O
-	O
aware	O
attention	O
-	O
based	O
sentence	O
representation	O
via	O
enhanced	O
query	O
-	O
sentence	O
interactions	O
.	O
We	O
have	O
also	O
investigated	O
relevance	O
of	O
title	O
with	O
the	O
sentences	O
and	O
coined	O
ways	O
to	O
incorporate	O
both	O
query	O
-	O
sentence	O
and	O
title	O
-	O
sentence	O
relevance	O
scores	O
in	O
ranking	O
sentences	O
with	O
an	O
abstract	O
.	O
Comparing	O
with	O
other	O
participating	O
systems	O
in	O
the	O
shared	O
task	O
,	O
our	O
submission	O
is	O
ranked	O
1	O
st	O
with	O
a	O
macro	O
average	B-MetricName
accuracy	I-MetricName
of	O
0.58	O
.	O
Our	O
code	O
is	O
available	O
at	O
https://github.com/	O
YatinChaudhary	O
/	O
RDoC_Task	O
.	O

In	O
this	O
paper	O
,	O
we	O
deal	O
with	O
texts	O
of	O
different	O
lengths	O
in	O
form	O
of	O
query	O
,	O
sentence	O
and	O
document	O
.	O
In	O
this	O
section	O
,	O
we	O
describe	O
the	O
way	O
we	O
represent	O
the	O
different	O
texts	O
.	O
Bag	O
-	O
of	O
-	O
words	O
(	O
BoW	O
)	O
and	O
Term	O
frequencyinverse	O
document	O
frequency	O
(	O
TF	O
-	O
IDF	O
)	O
:	O
We	O
use	O
two	O
the	O
local	O
representation	O
schemes	O
:	O
BoW	O
and	O
TF	O
-	O
IDF	O
(	O
Manning	O
et	O
al	O
,	O
2008	O
)	O
to	O
compute	O
sentence	O
/	O
document	O
vectors	O
.	O
ꞈ	O
v	O
2	O
v	O
1	O
v	O
2	O
v	O
i	O
ꞈ	O
v	O
D	O
h	O
(	O
v	O
)	O
...	O
h	O
e	O
(	O
v	O
)	O
h	O
2	O
(	O
v	O
<	O
2	O
)	O
h	O
i	O
(	O
v	O
<	O
i	O
)	O
U	O
v	O
...	O
α	B-HyperparameterName
D	O
α	B-HyperparameterName
2	O
α	B-HyperparameterName
1	O
V	O
i	O
=	O
p	O
(	O
V	O
i	O
|	O
v	O
<	O
i	O
)	O
ꞈ	O
.	O
Embedding	O
Sum	O
Representation	O
(	O
ESR	O
)	O
:	O
Word	B-TaskName
embeddings	I-TaskName
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
;	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
have	O
been	O
successfully	O
used	O
in	O
computing	O
distributed	O
representation	O
of	O
text	O
snippets	O
(	O
short	O
or	O
long	O
)	O
.	O
In	O
ESR	O
scheme	O
,	O
we	O
employ	O
the	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
from	O
FastText	B-MethodName
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
and	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
To	O
represent	O
a	O
text	O
(	O
query	O
,	O
sentence	O
or	O
document	O
)	O
,	O
we	O
compute	O
the	O
sum	O
of	O
(	O
pre	O
-	O
trained	O
)	O
word	O
vectors	O
of	O
each	O
word	O
in	O
the	O
text	O
.	O
E.g.	O
,	O
ESR	O
for	O
a	O
document	O
d	O
with	O
D	O
words	O
can	O
be	O
computed	O
as	O
:	O
ESR	O
(	O
d	O
)	O
=	O
d	O
=	O
D	O
i=1	O
e	O
(	O
d	O
i	O
)	O
where	O
,	O
e	O
R	O
E	O
is	O
the	O
pre	O
-	O
trained	O
embedding	O
vector	O
of	O
dimension	O
E	O
for	O
the	O
word	O
d	O
i	O
.	O
Query	O
-	O
aware	O
Attention	O
-	O
based	O
Representation	O
(	O
QAR	O
)	O
for	O
Documents	O
and	O
Sentences	O
:	O
Unlike	O
ESR	O
,	O
we	O
reward	O
the	O
maximum	O
matches	O
between	O
a	O
query	O
and	O
document	O
by	O
computing	O
density	O
of	O
matches	O
between	O
them	O
,	O
similar	O
to	O
McDonald	O
et	O
al	O
(	O
2018	O
)	O
.	O
In	O
doing	O
so	O
,	O
we	O
introduce	O
a	O
weighted	O
sum	O
of	O
word	O
vectors	O
from	O
pre	O
-	O
trained	O
embeddings	O
and	O
therefore	O
,	O
incorporate	O
importance	O
/	O
attention	O
of	O
certain	O
words	O
in	O
document	O
(	O
or	O
sentence	O
)	O
that	O
appear	O
in	O
the	O
query	O
text	O
.	O
For	O
an	O
enhanced	O
query	O
-	O
aware	O
attention	O
based	O
document	O
(	O
or	O
sentence	O
)	O
representation	O
,	O
we	O
first	O
compute	O
an	O
histogram	O
a	O
i	O
(	O
d	O
)	O
R	O
D	O
of	O
attention	O
weights	O
for	O
each	O
word	O
k	O
in	O
the	O
document	O
d	O
(	O
or	O
sentence	O
s	O
)	O
relative	O
to	O
the	O
ith	O
query	O
word	O
q	O
i	O
,	O
using	O
cosine	O
similarity	O
:	O
a	O
i	O
(	O
d	O
)	O
=	O
[	O
a	O
i	O
,	O
k	O
]	O
D	O
k=1	O
where	O
,	O
a	O
i	O
,	O
k	B-HyperparameterName
=	I-HyperparameterName
e	O
(	O
q	O
i	O
)	O
T	O
e	O
(	O
d	O
k	O
)	O
|	O
|	O
e	O
(	O
q	O
i	O
)	O
|	O
|	O
|	O
|	O
e	O
(	O
d	O
k	O
)	O
|	O
|	O
for	O
each	O
kth	B-DatasetName
word	O
in	O
the	O
document	O
d.	O
Here	O
,	O
e	O
(	O
w	O
)	O
refers	O
to	O
an	O
embedding	O
vector	O
of	O
the	O
word	O
w.	O
We	O
then	O
compute	O
an	O
query	O
-	O
aware	O
attentionbased	O
representation	O
Φ	O
i	O
(	O
d	O
)	O
of	O
document	O
d	O
from	O
the	O
viewpoint	O
of	O
ith	O
query	O
word	O
by	O
summing	O
the	O
word	O
vectors	O
of	O
the	O
document	O
,	O
weighted	O
by	O
their	O
attention	O
scores	O
a	O
i	O
(	O
d	O
)	O
:	O
Φ	O
i	O
(	O
d	O
)	O
=	O
D	O
k=1	O
a	O
i	O
,	O
k	O
(	O
d	O
)	O
e	O
(	O
d	O
k	O
)	O
=	O
a	O
i	O
(	O
d	O
)	O
[	O
e	O
(	O
d	O
k	O
)	O
]	O
D	O
k=1	O
where	O
is	O
an	O
element	O
-	O
wise	O
multiplication	O
operator	O
.	O
Next	O
,	O
we	O
compute	O
density	O
of	O
matches	O
between	O
several	O
words	O
in	O
query	O
and	O
the	O
document	O
by	O
summing	O
each	O
of	O
the	O
attention	O
histograms	O
a	O
i	O
for	O
all	O
the	O
query	O
terms	O
i.	O
Therefore	O
,	O
the	O
query	O
-	O
aware	O
document	O
representation	O
for	O
a	O
document	O
(	O
or	O
sentence	O
)	O
relative	O
to	O
all	O
query	O
words	O
in	O
q	O
is	O
given	O
by	O
:	O
QAR	O
(	O
d	O
)	O
=	O
Φ	O
q	O
(	O
d	O
)	O
=	O
|	O
q	O
|	O
i	O
Φ	O
i	O
(	O
d	O
)	O
(	O
1	O
)	O
Similarly	O
,	O
a	O
query	O
-	O
aware	O
sentence	O
representation	O
Φ	O
q	O
(	O
s	O
)	O
and	O
query	O
-	O
aware	O
title	O
representation	O
Φ	O
q	O
(	O
t	O
)	O
can	O
be	O
computed	O
for	O
the	O
sentence	O
s	O
and	O
document	O
title	O
t	O
,	O
respectively	O
.	O
For	O
query	O
representation	O
,	O
we	O
use	O
ESR	O
scheme	O
as	O
q	O
=	O
|	O
q	O
|	O
i=1	O
e	O
(	O
w	O
i	O
)	O
.	O
Figure	O
2	O
illustrates	O
the	O
computation	O
of	O
queryaware	O
attention	O
-	O
based	O
sentence	O
representation	O
.	O

Topic	B-TaskName
models	I-TaskName
(	O
TMs	O
)	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
have	O
shown	O
to	O
capture	O
thematic	O
structures	O
,	O
i.e.	O
,	O
topics	O
appearing	O
within	O
the	O
document	O
collection	O
.	O
Beyond	O
interpretability	O
,	O
topic	B-TaskName
models	I-TaskName
can	O
extract	O
latent	O
document	O
representation	O
that	O
is	O
used	O
to	O
perform	O
document	O
retrieval	O
.	O
Recently	O
,	O
Gupta	O
et	O
al	O
(	O
2019a	O
)	O
and	O
Gupta	O
et	O
al	O
(	O
2019b	O
)	O
have	O
shown	O
that	O
the	O
neural	O
network	O
-	O
based	O
topic	B-TaskName
models	I-TaskName
(	O
NTM	O
)	O
outperform	O
LDA	B-MethodName
-	O
based	O
topic	B-TaskName
models	I-TaskName
(	O
Blei	O
et	O
al	O
,	O
2003	O
;	O
Srivastava	O
and	O
Sutton	O
,	O
2017	O
)	O
in	O
terms	O
of	O
generalization	O
,	O
interpretability	O
and	O
document	O
retrieval	O
.	O
In	O
order	O
to	O
perform	O
document	B-TaskName
classification	I-TaskName
and	O
retrieval	O
,	O
we	O
have	O
employed	O
supervised	O
version	O
of	O
neural	O
topic	O
model	O
with	O
extra	O
features	O
and	O
further	O
introduced	O
word	O
-	O
level	O
attention	O
in	O
a	O
neural	O
topic	O
model	O
,	O
i.e.	O
in	O
DocNADE	O
(	O
Larochelle	O
and	O
Lauly	O
,	O
2012	O
;	O
Gupta	O
et	O
al	O
,	O
2019a	O
)	O
.	O
Supervised	O
NTM	O
(	O
SupDocNADE	O
)	O
:	O
Document	O
Neural	O
Autoregressive	O
Distribution	O
Estimator	O
(	O
DocNADE	O
)	O
is	O
a	O
neural	O
network	O
based	O
topic	O
model	O
that	O
works	O
on	O
bag	O
-	O
of	O
-	O
words	O
(	O
BoW	O
)	O
representation	O
to	O
model	O
a	O
document	O
collection	O
in	O
a	O
language	O
modeling	O
fashion	O
.	O
Consider	O
a	O
document	O
d	O
,	O
represented	O
as	O
v	O
=	O
[	O
v	O
1	O
,	O
...	O
,	O
v	O
i	O
,	O
...	O
,	O
v	O
D	O
]	O
of	O
size	O
D	O
,	O
where	O
v	O
i	O
{	O
1	O
,	O
...	O
,	O
Z	O
}	O
is	O
the	O
index	O
of	O
ith	O
word	O
in	O
the	O
vocabulary	O
and	O
Z	O
is	O
the	O
vocabulary	O
size	O
.	O
DocNADE	O
models	O
the	O
joint	O
distribution	O
p	O
(	O
v	O
)	O
of	O
document	O
v	O
by	O
decomposing	O
p	O
(	O
v	O
)	O
into	O
autoregressive	O
conditional	O
of	O
each	O
word	O
v	O
i	O
in	O
the	O
document	O
,	O
i.e.	O
,	O
p	O
(	O
v	O
)	O
=	O
D	O
i=1	O
p	O
(	O
v	O
i	O
|	O
v	O
<	O
i	O
)	O
,	O
where	O
v	O
<	O
i	O
{	O
v	O
1	O
,	O
...	O
,	O
v	O
i−1	O
}	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
(	O
left	O
)	O
,	O
DocNADE	O
computes	O
each	O
autoregressive	O
conditional	O
p	O
(	O
v	O
i	O
|	O
v	O
<	O
i	O
)	O
using	O
a	O
feed	O
forward	O
neural	O
network	O
for	O
i	O
{	O
1	O
,	O
...	O
,	O
D	O
}	O
as	O
,	O
p	O
(	O
v	O
i	O
=	O
w	O
|	O
v	O
<	O
i	O
)	O
=	O
exp	O
(	O
b	O
w	O
+	O
U	O
w	O
,	O
:	O
h	O
(	O
v	O
<	O
i	O
)	O
)	O
w	O
exp	O
(	O
b	O
w	O
+	O
U	O
w	O
,	O
:	O
h	O
(	O
v	O
<	O
i	O
)	O
)	O
h	O
i	O
(	O
v	O
<	O
i	O
)	O
=	O
f	O
(	O
c	O
+	O
j	O
<	O
i	O
W	O
:	O
,	O
v	O
j	O
)	O
where	O
,	O
f	O
(	O
)	O
is	O
a	O
non	O
-	O
linear	O
activation	B-HyperparameterName
function	I-HyperparameterName
,	O
W	O
R	O
H×Z	O
and	O
U	O
R	O
Z×H	O
are	O
encoding	O
and	O
decoding	O
matrices	O
,	O
c	O
R	O
H	O
and	O
b	O
R	O
Z	O
are	O
encoding	O
and	O
decoding	O
biases	O
,	O
H	O
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
units	I-HyperparameterName
in	O
latent	O
representation	O
h	O
i	O
(	O
v	O
<	O
i	O
)	O
.	O
Here	O
,	O
h	O
i	O
(	O
v	O
<	O
i	O
)	O
contains	O
information	O
of	O
words	O
preceding	O
the	O
word	O
v	O
i	O
.	O
For	O
a	O
document	O
v	O
,	O
the	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
L	O
(	O
v	O
)	O
and	O
latent	O
representation	O
h	O
(	O
v	O
)	O
are	O
given	O
as	O
,	O
L	O
unsup	O
(	O
v	O
)	O
=	O
D	O
i=1	O
log	O
p	O
(	O
v	O
i	O
|	O
v	O
<	O
i	O
)	O
(	O
2	O
)	O
h	O
(	O
v	O
)	O
=	O
f	O
(	O
c	O
+	O
D	O
i=1	O
W	O
:	O
,	O
v	O
i	O
)	O
(	O
3	O
)	O
Here	O
,	O
L	O
(	O
v	O
)	O
is	O
used	O
to	O
optimize	O
the	O
topic	O
model	O
in	O
unsupervised	O
fashion	O
and	O
h	O
(	O
v	O
)	O
encodes	O
the	O
topic	O
proportion	O
.	O
See	O
Gupta	O
et	O
al	O
(	O
2019a	O
)	O
for	O
further	O
details	O
on	O
training	O
unsupervised	O
DocNADE	O
.	O
Here	O
,	O
we	O
extend	O
the	O
unsupervised	O
version	O
to	O
DocNADE	O
with	O
a	O
hybrid	O
cost	O
L	O
hybrid	O
(	O
v	O
)	O
,	O
consisting	O
of	O
a	O
(	O
supervised	O
)	O
discriminative	O
training	O
cost	O
p	O
(	O
y	O
=	O
q	O
|	O
v	O
)	O
along	O
with	O
an	O
unsupervised	O
generative	O
cost	O
p	O
(	O
v	O
)	O
for	O
a	O
given	O
query	O
q	O
and	O
associated	O
document	O
v	O
:	O
L	O
hybrid	O
(	O
v	O
)	O
=	O
L	O
sup	O
(	O
v	O
)	O
+	O
λ	O
L	O
unsup	O
(	O
v	O
)	O
(	O
4	O
)	O
where	O
λ	O
[	O
0	B-DatasetName
,	O
1	O
]	O
.	O
The	O
supervised	O
cost	O
is	O
given	O
by	O
:	O
L	O
sup	O
(	O
v	O
)	O
=	O
p	O
(	O
y	O
=	O
q	O
|	O
v	O
)	O
=	O
softmax	B-MethodName
(	O
d	O
+	O
S	O
h	O
(	O
v	O
)	O
)	O
Here	O
,	O
S	O
R	O
L×H	O
and	O
d	O
R	O
L	O
are	O
output	O
matrix	O
and	O
bias	O
,	O
L	O
is	O
the	O
total	O
number	O
of	O
unique	O
RDoC	O
constructs	O
(	O
i.e.	O
,	O
unique	O
query	O
labels	O
)	O
.	O
Supervised	O
Attention	O
-	O
based	O
NTM	O
(	O
a	O
-	O
SupDocNADE	O
)	O
:	O
Observe	O
in	O
equation	O
3	O
that	O
the	O
DocNADE	O
computes	O
document	O
representation	O
h	O
(	O
v	O
)	O
via	O
aggregation	O
of	O
word	O
embedding	O
vectors	O
without	O
considering	O
attention	O
over	O
certain	O
words	O
.	O
However	O
,	O
certain	O
content	O
words	O
own	O
high	O
important	O
,	O
especially	O
in	O
classification	O
task	O
.	O
Therefore	O
,	O
we	O
have	O
introduced	O
attention	O
-	O
based	O
embedding	O
aggregation	O
in	O
supDocNADE	O
(	O
Figure	O
1	O
,	O
left	O
)	O
:	O
h	O
(	O
v	O
)	O
=	O
f	O
(	O
c	O
+	O
D	O
i=1	O
α	B-HyperparameterName
i	O
W	O
:	O
,	O
v	O
i	O
)	O
(	O
5	O
)	O
Here	O
,	O
α	B-HyperparameterName
i	O
is	O
an	O
attention	O
score	O
of	O
each	O
word	O
i	O
in	O
the	O
document	O
v	O
,	O
learned	O
via	O
supervised	O
training	O
.	O
Additionally	O
,	O
we	O
incorporate	O
extra	O
word	O
features	O
,	O
such	O
as	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
from	O
several	O
sources	O
:	O
FastText	B-MethodName
(	O
E	O
f	O
ast	O
)	O
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
and	O
word2vec	O
(	O
E	O
word2vec	O
)	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
We	O
introduce	O
these	O
features	O
by	O
concatenating	O
h	O
e	O
(	O
v	O
)	O
with	O
h	O
(	O
v	O
)	O
in	O
the	O
supervised	O
portion	O
of	O
the	O
a	O
-	O
supDocNADE	O
model	O
,	O
as	O
Therefore	O
,	O
the	O
classification	O
portion	O
of	O
a	O
-	O
supDocNADE	O
with	O
additional	O
features	O
is	O
given	O
by	O
:	O
h	O
e	O
(	O
v	O
)	O
=	O
f	O
c	O
+	O
D	O
i=1	O
α	B-HyperparameterName
i	O
(	O
E	O
f	O
ast	O
:	O
,	O
v	O
i	O
+	O
E	O
word2vec	O
:	O
,	O
v	O
i	O
)	O
(	O
6	O
p	O
(	O
q	O
|	O
v	O
)	O
=	O
softmax	B-MethodName
(	O
d	O
+	O
S	O
concat	O
(	O
h	O
(	O
v	O
)	O
,	O
h	O
e	O
(	O
v	O
)	O
)	O
)	O
where	O
,	O
S	O
R	O
H	O
×L	O
and	O
H	O
=	O
H	O
+	O
E	O
f	O
ast	O
+	O
E	O
word2vec	O
.	O

BM25	O
:	O
A	O
ranking	O
function	O
proposed	O
by	O
Robertson	O
and	O
Zaragoza	O
(	O
2009	O
)	O
is	O
used	O
to	O
estimate	O
the	O
relevance	O
of	O
a	O
document	O
for	O
a	O
given	O
query	O
.	O
BM25	O
-	O
Extra	O
:	O
The	O
relevance	O
score	O
of	O
BM	O
-	O
25	O
is	O
combined	O
with	O
four	O
extra	O
features	O
:	O
(	O
1	O
)	O
percentage	O
of	O
query	O
words	O
with	O
exact	B-MetricName
match	I-MetricName
in	O
the	O
document	O
,	O
(	O
2	O
)	O
percentage	O
of	O
query	O
words	O
bigrams	O
matched	O
in	O
the	O
document	O
,	O
(	O
3	O
)	O
IDF	O
weighted	O
document	O
vector	O
for	O
feature	O
#	O
1	O
,	O
and	O
(	O
4	O
)	O
IDF	O
weighted	O
document	O
vector	O
for	O
feature	O
#	O
2	O
.	O
Therefore	O
,	O
BM25	O
-	O
Extra	O
returns	O
a	O
vector	O
of	O
5	O
scores	O
.	O

RDoC	O
Task	O
-	O
1	O
aims	O
at	O
retrieving	O
and	O
ranking	O
of	O
PubMed	O
abstracts	O
(	O
title	O
and	O
content	O
)	O
that	O
are	O
relevant	O
for	O
8	O
RDoC	O
constructs	O
.	O
Participants	O
are	O
provided	O
with	O
8	O
clusters	O
,	O
each	O
with	O
a	O
RDoC	O
construct	O
label	O
and	O
required	O
to	O
rank	O
abstracts	O
within	O
each	O
cluster	O
based	O
on	O
their	O
relevance	O
to	O
the	O
corresponding	O
cluster	O
label	O
.	O
Each	O
cluster	O
contains	O
abstracts	O
relevant	O
to	O
its	O
RDoC	O
construct	O
,	O
while	O
some	O
(	O
or	O
most	O
)	O
of	O
the	O
abstracts	O
are	O
noisy	O
in	O
the	O
sense	O
that	O
they	O
belong	O
to	O
a	O
different	O
RDoC	O
construct	O
.	O
Ideally	O
,	O
the	O
participants	O
are	O
required	O
to	O
rank	O
abstracts	O
in	O
each	O
of	O
the	O
clusters	O
by	O
determining	O
their	O
relevance	O
with	O
the	O
RDoC	O
construct	O
of	O
the	O
cluster	O
in	O
which	O
they	O
appear	O
.	O
To	O
address	O
the	O
RDoc	O
Task	O
-	O
1	O
,	O
we	O
learn	O
a	O
mapping	O
function	O
between	O
latent	O
representation	O
h	O
(	O
v	O
)	O
of	O
a	O
document	O
(	O
i.e	O
..	O
,	O
abstract	O
)	O
v	O
and	O
its	O
RDoC	O
construct	O
,	O
i.e.	O
,	O
query	O
words	O
q	O
in	O
a	O
supervised	O
fashion	O
.	O
In	O
doing	O
so	O
,	O
we	O
have	O
employed	O
supervised	O
classifiers	O
,	O
especially	O
supervised	O
neural	O
topic	O
model	O
a	O
-	O
supDocNADE	O
(	O
section	O
3.2	O
)	O
for	O
document	B-TaskName
ranking	I-TaskName
.	O
We	O
treat	O
q	O
as	O
label	O
and	O
maximize	O
p	O
(	O
q	O
|	O
v	O
)	O
leading	O
to	O
maximize	O
L	O
hybrid	O
(	O
v	O
)	O
in	O
a	O
-	O
supDocNADE	O
model	O
.	O
As	O
demonstrated	O
in	O
Figure	O
1	O
(	O
right	O
)	O
,	O
we	O
perform	O
document	B-TaskName
ranking	I-TaskName
in	O
two	O
steps	O
:	O
(	O
1	O
)	O
Document	O
Relevance	O
Ranking	O
:	O
We	O
build	O
a	O
supervised	O
classifier	O
using	O
all	O
the	O
training	O
documents	O
and	O
their	O
corresponding	O
labels	O
(	O
RDoC	O
constructs	O
)	O
,	O
provided	O
with	O
the	O
training	O
set	O
.	O
At	O
the	O
test	O
time	O
,	O
we	O
compute	O
prediction	O
probability	O
score	O
p	O
(	O
CID	B-DatasetName
=	O
q	O
|	O
v	O
test	O
(	O
CID	B-DatasetName
)	O
)	O
)	O
of	O
the	O
label	O
=	O
CID	B-DatasetName
for	O
each	O
test	O
document	O
v	O
test	O
(	O
CID	B-DatasetName
)	O
in	O
the	O
cluster	O
,	O
CID	B-DatasetName
.	O
This	O
prediction	O
probability	O
(	O
or	O
confidence	O
score	O
)	O
is	O
treated	O
as	O
a	O
relevance	O
score	O
of	O
the	O
document	O
for	O
the	O
RDoC	O
construct	O
of	O
the	O
cluster	O
.	O
Figure	O
1	O
(	O
right	O
)	O
shows	O
that	O
we	O
perform	O
document	B-TaskName
ranking	I-TaskName
using	O
the	O
probability	O
scores	O
(	O
col	O
-	O
2	O
)	O
of	O
the	O
RDoC	O
construct	O
(	O
e.g.	O
loss	B-MetricName
)	O
within	O
the	O
cluster	O
C1	O
.	O
Observe	O
that	O
a	O
test	O
document	O
with	O
least	O
confidence	O
for	O
a	O
cluster	O
are	O
ranked	O
lower	O
within	O
the	O
cluster	O
and	O
thus	O
,	O
improving	O
mean	O
average	B-MetricName
precision	I-MetricName
(	O
mAP	B-MetricName
)	O
.	O
Additionally	O
,	O
we	O
also	O
show	O
the	O
predicted	O
RDoC	O
construct	O
in	O
col	O
-	O
1	O
by	O
the	O
supervised	O
classifier	O
.	O
(	O
2	O
)	O
Document	O
Relevance	O
Re	O
-	O
ranking	O
:	O
Secondly	O
,	O
we	O
re	O
-	O
ranked	O
each	O
document	O
v	O
(	O
ti	O
-	O
tle+abstract	O
)	O
within	O
each	O
cluster	O
(	O
with	O
label	O
q	O
)	O
using	O
unsupervised	O
ranking	O
,	O
where	O
the	O
relevance	O
scores	O
are	O
computed	O
as	O
:	O
(	O
a	O
)	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
:	O
sum	O
each	O
of	O
the	O
5	O
relevance	O
scores	O
to	O
get	O
the	O
final	O
relevance	O
,	O
and	O
(	O
b	O
)	O
reRank	O
(	O
QAR	O
)	O
:	O
cosine	O
-	O
similarity	O
(	O
QAR	O
(	O
v	O
)	O
,	O
q	O
)	O
.	O

Beyond	O
unsupervised	O
ranking	O
,	O
we	O
further	O
investigate	O
sentence	O
ranking	O
in	O
supervised	O
paradigm	O
by	O
introducing	O
a	O
distance	B-HyperparameterName
metric	I-HyperparameterName
between	O
the	O
query	O
(	O
or	O
title	O
)	O
and	O
sentence	O
vectors	O
.	O
Figure	O
2	O
describes	O
the	O
computation	O
of	O
relevance	O
score	O
for	O
a	O
sentence	O
s	O
j	O
using	O
a	O
supervised	O
sentence	O
ranker	O
scheme	O
.	O
Like	O
the	O
unsupervised	O
ranker	O
(	O
section	O
3.5.1	O
)	O
,	O
the	O
supervised	O
ranker	O
also	O
employs	O
vector	O
representations	O
:	O
Φ	O
q	O
(	O
s	O
j	O
)	O
,	O
t	O
and	O
q.	O
Using	O
the	O
projection	O
matrix	O
G	O
,	O
we	O
then	O
apply	O
a	O
projection	O
to	O
each	O
of	O
the	O
representation	O
to	O
obtain	O
Φ	O
p	O
q	O
(	O
s	O
j	O
)	O
,	O
t	O
p	O
and	O
q	O
p	O
.	O
Here	O
,	O
the	O
operator	O
⊗	O
performs	O
concatenation	O
of	O
the	O
projected	O
vector	O
with	O
its	O
input	O
via	O
residual	B-MethodName
connection	I-MethodName
.	O
Next	O
,	O
we	O
apply	O
a	O
Manhattan	O
distance	B-HyperparameterName
metric	I-HyperparameterName
to	O
compute	O
similarity	O
(	O
or	O
relevance	O
)	O
scores	O
,	O
following	O
:	O
r	O
sup	O
=	O
exp	O
−	O
|	O
|	O
(	O
Φ	O
p	O
q	O
(	O
s	O
j	O
)	O
,	O
q	O
p	O
)	O
+	O
β	B-HyperparameterName
(	O
Φ	O
p	O
q	O
(	O
s	O
j	O
)	O
,	O
t	O
p	O
)	O
|	O
|	O
2	O
where	O
β	B-HyperparameterName
[	O
0	B-DatasetName
,	O
1	O
]	O
controls	O
the	O
relevance	O
of	O
title	O
,	O
determined	O
by	O
cross	O
-	O
validation	O
.	O
A	O
final	O
relevance	O
score	O
r	O
sup	O
f	O
[	O
0	B-DatasetName
,	O
1	O
]	O
is	O
computed	O
by	O
feeding	O
a	O
vector	O
[	O
r	O
sup	O
,	O
r	O
sup	O
siamese	O
,	O
BM25	O
-	O
extra	O
]	O
into	O
a	O
supervised	O
linear	B-MethodName
regression	I-MethodName
,	O
which	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
by	O
minimizing	O
mean	O
squared	O
error	O
between	O
the	O
r	O
sup	O
f	O
and	O
{	O
0	B-DatasetName
,	O
1	O
}	O
,	O
i.e.	O
,	O
1	O
when	O
the	O
sentence	O
s	O
j	O
is	O
relevant	O
to	O
query	O
q.	O
Here	O
,	O
r	O
sup	O
siamese	O
refers	O
to	O
a	O
relevance	O
Best	O
mAP	B-MetricName
score	O
for	O
each	O
model	O
is	O
marked	O
in	O
bold	O
.	O
(	O
reRank	O
#	O
1	O
:	O
"	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
"	O
;	O
reRank	O
#	O
2	O
:	O
"	O
reRank	O
(	O
QAR	O
)	O
"	O
;	O
reRank	O
#	O
3	O
:	O
"	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
+	O
reRank	O
(	O
QAR	O
)	O
"	O
)	O
score	O
computed	O
between	O
q	O
and	O
s	O
j	O
via	O
Siamese	O
-	O
LSTM	B-MethodName
.	O
To	O
perform	O
sentence	O
ranking	O
within	O
an	O
abstract	O
for	O
a	O
given	O
RDoC	O
construct	O
q	O
,	O
the	O
relevance	O
score	O
r	O
sup	O
f	O
(	O
or	O
r	O
unsup	O
f	O
)	O
is	O
computed	O
for	O
all	O
the	O
sentences	O
and	O
a	O
sentence	O
with	O
the	O
highest	O
score	O
is	O
extracted	O
.	O

Dataset	O
Description	O
:	O
Dataset	O
for	O
RDoC	O
Tasks	O
contains	O
a	O
total	O
of	O
266	O
PubMed	O
abstracts	O
labelled	O
with	O
8	O
RDoC	O
constructs	O
in	O
a	O
single	O
label	O
fashion	O
.	O
Number	O
of	O
abstracts	O
for	O
each	O
RDoC	O
construct	O
is	O
described	O
in	O
Table	O
2	O
,	O
where	O
first	O
row	O
describes	O
the	O
statistics	O
for	O
all	O
abstracts	O
and	O
second	O
&	O
third	O
row	O
shows	O
the	O
split	O
of	O
those	O
abstracts	O
into	O
training	O
and	O
development	O
sets	O
maintaining	O
a	O
80	O
-	O
20	O
ratio	O
for	O
each	O
RDoC	O
construct	O
.	O
For	O
Task	O
-	O
1	O
,	O
each	O
PubMed	O
abstract	O
contains	O
its	O
associated	O
title	O
,	O
PubMed	O
ID	O
(	O
PMID	O
)	O
and	O
label	O
(	O
RDoC	O
construct	O
)	O
.	O
In	O
addition	O
for	O
Task	O
-	O
2	O
,	O
each	O
PubMed	O
abstract	O
also	O
contains	O
a	O
list	O
of	O
most	O
relevant	O
sentences	O
from	O
that	O
abstract	O
.	O
Final	O
evaluation	O
test	O
data	O
for	O
Task	O
-	O
1	O
&	O
Task	O
-	O
2	O
contains	O
999	O
&	O
244	O
abstracts	O
respectively	O
.	O
We	O
use	O
"	O
RegexpTokenizer	O
"	O
from	O
scikit	O
-	O
learn	O
to	O
tokenize	O
abstracts	O
and	O
lower	O
-	O
cased	O
all	O
tokens	O
.	O
After	O
this	O
,	O
we	O
remove	O
those	O
tokens	O
which	O
occur	O
in	O
less	O
than	O
3	O
abstracts	O
and	O
also	O
remove	O
stopwords	O
(	O
using	O
nltk	O
)	O
.	O
For	O
computing	O
BM25	O
-	O
Extra	O
relevance	O
score	O
,	O
we	O
use	O
unprocessed	O
raw	O
text	O
of	O
sentences	O
and	O
titles	O
.	O
Experimental	O
Setup	O
:	O
As	O
the	O
training	O
dataset	O
labelled	O
with	O
RDoC	O
constructs	O
is	O
very	O
small	O
,	O
we	O
use	O
an	O
external	O
source	O
of	O
semantical	O
knowledge	O
by	O
incorporating	O
pretrained	O
distributional	O
word	B-TaskName
embeddings	I-TaskName
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
from	O
FastText	B-MethodName
model	O
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
trained	O
on	O
the	O
entire	O
corpus	O
of	O
PubMed	O
and	O
MIMIC	O
III	O
Clinical	O
notes	O
(	O
Johnson	O
et	O
al	O
,	O
2016	O
)	O
.	O
Similarly	O
,	O
we	O
also	O
use	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
(	O
Moen	O
and	O
Ananiadou	O
,	O
2013	O
)	O
from	O
word2vec	O
model	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
trained	O
on	O
PubMed	O
and	O
PMC	O
abstracts	O
.	O
We	O
create	O
3	O
folds	O
*	O
of	O
train	O
/	O
dev	O
splits	O
for	O
cross	O
-	O
validation	O
.	O
RDoC	O
Task	O
-	O
1	O
:	O
For	O
DocNADE	O
topic	O
model	O
,	O
we	O
use	O
latent	O
representation	O
of	O
size	O
50	O
.	O
We	O
use	O
pretrained	O
FastText	B-MethodName
embeddings	O
of	O
size	O
300	O
and	O
pretrained	O
word2vec	O
embeddings	O
of	O
size	O
200	O
.	O
For	O
SVM	B-MethodName
,	O
we	O
use	O
Bag	O
-	O
of	O
-	O
words	O
(	O
BoW	O
)	O
representation	O
of	O
abstracts	O
with	O
radial	O
basis	O
kernel	O
function	O
.	O
PubMed	O
abstracts	O
are	O
provided	O
in	O
eight	O
different	O
clusters	O
,	O
one	O
for	O
each	O
RDoC	O
construct	O
,	O
for	O
final	O
test	O
set	O
evaluation	O
.	O
RDoC	O
Task	O
-	O
2	O
:	O
We	O
use	O
pretrained	O
FastText	B-MethodName
embeddings	O
to	O
compute	O
query	O
-	O
aware	O
sentence	O
representation	O
of	O
a	O
sentence	O
(	O
Φ	O
q	O
(	O
s	O
j	O
)	O
)	O
,	O
title	O
(	O
t	O
)	O
and	O
query	O
(	O
q	O
)	O
representations	O
.	O
We	O
also	O
train	O
Replicated	O
-	O
Siamese	O
-	O
LSTM	B-MethodName
model	O
with	O
input	O
as	O
sentence	O
and	O
query	O
pair	O
i.e.	O
,	O
(	O
s	O
j	O
,	O
q	O
)	O
and	O
label	O
as	O
1	O
if	O
s	O
j	O
is	O
relevant	O
otherwise	O
0	B-DatasetName
.	O
We	O
use	O
β	B-HyperparameterName
{	O
0	B-DatasetName
,	O
1	O
}	O
.	O

Table	O
3	O
shows	O
the	O
performance	O
of	O
supervised	O
Document	O
Ranker	O
models	O
i.e	O
,	O
a	O
-	O
supDocNADE	O
and	O
SVM	B-MethodName
,	O
for	O
Task	O
-	O
1	O
.	O
SVM	B-MethodName
achieves	O
a	O
classification	O
accuracy	B-MetricName
of	O
0.947	O
and	O
mean	O
average	B-MetricName
precision	I-MetricName
.	O
It	O
shows	O
that	O
an	O
intruder	O
/	O
noisy	O
abstract	O
(	O
Gold	O
Label	O
:	O
Loss	O
)	O
is	O
assigned	O
higher	O
probability	O
than	O
the	O
abstracts	O
with	O
same	O
Gold	O
Label	O
as	O
the	O
cluster	O
.	O
But	O
,	O
using	O
re	O
-	O
ranking	O
with	O
BM25	O
-	O
Extra	O
(	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
)	O
relevance	O
score	O
assigns	O
lowest	O
relevance	O
to	O
the	O
intruder	O
abstract	O
.	O
(	O
mAP	B-MetricName
)	O
of	O
0.992	O
by	O
ranking	O
the	O
abstracts	O
in	O
their	O
respective	O
clusters	O
using	O
the	O
supervised	O
prediction	O
probabilities	O
(	O
p	O
(	O
q	O
|	O
v	O
)	O
)	O
.	O
After	O
that	O
,	O
we	O
use	O
three	O
different	O
relevance	O
scores	O
:	O
(	O
1	O
)	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
,	O
(	O
2	O
)	O
reRank	O
(	O
QAR	O
)	O
and	O
(	O
3	O
)	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
+	O
reRank	O
(	O
QAR	O
)	O
,	O
for	O
re	O
-	O
ranking	O
of	O
the	O
abstracts	O
in	O
their	O
respective	O
clusters	O
.	O
It	O
is	O
to	O
be	O
noted	O
that	O
the	O
ranking	O
mAP	B-MetricName
of	O
the	O
clusters	O
using	O
prediction	O
probabilities	O
is	O
already	O
the	O
best	O
possible	O
i.e.	O
,	O
the	O
intruder	O
abstracts	O
(	O
abstracts	O
with	O
different	O
label	O
(	O
RDoC	O
construct	O
)	O
than	O
the	O
cluster	O
label	O
)	O
are	O
at	O
the	O
bottom	O
of	O
the	O
ranked	O
clusters	O
.	O
Therefore	O
,	O
re	O
-	O
ranking	O
of	O
these	O
clusters	O
would	O
not	O
achieve	O
a	O
better	O
score	O
.	O
Similarly	O
,	O
we	O
train	O
a	O
-	O
supDocNADE	O
model	O
with	O
three	O
different	O
settings	O
:	O
(	O
1	O
)	O
random	O
weight	O
initialization	O
,	O
(	O
2	O
)	O
incorporating	O
FastText	B-MethodName
embeddings	O
(	O
h	O
e	O
(	O
v	O
)	O
)	O
and	O
(	O
3	O
)	O
incorporating	O
Fast	O
-	O
Text	O
and	O
word2vec	O
embeddings	O
(	O
h	O
e	O
(	O
v	O
)	O
)	O
.	O
By	O
using	O
the	O
pretrained	O
embeddings	O
,	O
the	O
classification	O
accuracy	B-MetricName
increases	O
from	O
0.912	O
to	O
0.965	O
,	O
this	O
shows	O
that	O
distributional	O
pretrained	O
embeddings	O
carry	O
significant	O
semantic	O
knowledge	O
.	O
Furthermore	O
,	O
re	O
-	O
ranking	O
using	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
and	O
reRank	O
(	O
QAR	O
)	O
further	O
results	O
in	O
the	O
improvement	O
of	O
mAP	B-MetricName
score	O
(	O
0.994	O
vs	O
0.983	O
)	O
by	O
shifting	O
the	O
intruder	O
documents	O
at	O
the	O
bottom	O
of	O
each	O
impure	O
cluster	O
.	O

Table	O
4	O
shows	O
an	O
impure	O
"	O
Potential	O
Threat	O
Anxiety	O
"	O
cluster	O
of	O
abstracts	O
containing	O
an	O
intruder	O
abstract	O
with	O
label	O
(	O
RDoC	O
construct	O
)	O
"	O
Loss	O
"	O
.	O
When	O
this	O
cluster	O
is	O
ranked	O
on	O
the	O
basis	O
of	O
predic	O
-	O
tion	O
probabilities	O
(	O
p	O
(	O
q	O
|	O
v	O
)	O
)	O
,	O
then	O
"	O
Loss	O
"	O
abstract	O
is	O
ranked	O
third	O
from	O
the	O
bottom	O
and	O
it	O
degrades	O
the	O
mAP	B-MetricName
score	O
of	O
the	O
retrieval	O
system	O
.	O
But	O
after	O
re	O
-	O
ranking	O
this	O
cluster	O
using	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
relevance	O
score	O
,	O
the	O
"	O
Loss	O
"	O
abstract	O
is	O
ranked	O
at	O
the	O
bottom	O
,	O
thus	O
maximizing	O
the	O
mAP	B-MetricName
score	O
.	O
Therefore	O
,	O
re	O
-	O
ranking	O
with	O
BM25	O
-	O
Extra	O
on	O
top	O
of	O
ranking	O
with	O
p	O
(	O
q	O
|	O
v	O
)	O
is	O
,	O
evidently	O
,	O
a	O
robust	O
abstract	O
/	O
document	B-TaskName
ranking	I-TaskName
technique	O
.	O

Table	O
5	O
shows	O
results	O
for	O
Task	O
-	O
2	O
using	O
three	O
unsupervised	O
and	O
two	O
supervised	O
sentence	O
ranker	O
models	O
.	O
For	O
unsupervised	O
model	O
,	O
using	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
relevance	O
score	O
between	O
a	O
query	O
(	O
q	O
)	O
,	O
label	O
(	O
RDoC	O
construct	O
)	O
of	O
an	O
abstract	O
,	O
and	O
all	O
the	O
sentences	O
(	O
s	O
j	O
)	O
in	O
an	O
abstract	O
,	O
we	O
get	O
an	O
macroaverage	O
accuracy	B-MetricName
(	O
MAA	O
)	O
of	O
0.631	O
.	O
However	O
,	O
using	O
version1	O
and	O
version2	O
models	O
(	O
see	O
Fig	O
2	O
)	O
,	O
we	O
achieve	O
a	O
MAA	O
score	O
of	O
0.701	O
and	O
0.526	O
respectively	O
.	O
Higher	O
accuracy	B-MetricName
of	O
version1	O
model	O
suggests	O
that	O
title	O
(	O
t	O
)	O
of	O
an	O
abstract	O
also	O
contains	O
the	O
essential	O
information	O
regarding	O
the	O
most	O
relevant	O
sentence	O
.	O
For	O
supervised	O
model	O
,	O
we	O
get	O
an	O
MAA	O
score	O
of	O
0.772	O
and	O
0.737	O
by	O
setting	O
β	B-HyperparameterName
=	O
0	B-DatasetName
&	O
1	O
in	O
supervised	O
relevance	O
score	O
(	O
r	O
sup	O
f	O
)	O
equation	O
in	O
section	O
3.5.2	O
.	O
Hence	O
,	O
for	O
supervised	O
sentence	O
ranker	O
model	O
,	O
title	O
(	O
t	O
)	O
is	O
playing	O
a	O
negative	O
influence	O
in	O
correctly	O
identifying	O
the	O
relevance	O
(	O
r	O
sup	O
f	O
)	O
of	O
different	O
sentences	O
.	O
Furthermore	O
,	O
we	O
combine	O
the	O
knowledge	O
of	O
unsupervised	O
and	O
supervised	O
sentence	O
rankers	O
by	O
creating	O
multiple	O
ensembles	O
(	O
majority	O
voting	O
)	O
of	O
the	O
predictions	O
from	O
different	O
models	O
.	O
We	O
achieve	O
the	O
highest	O
MAA	O
score	O
of	O
0.789	O
by	O
combining	O
the	O
predictions	O
of	O
(	O
1	O
)	O
reRank	O
(	O
BM25	O
-	O
Extra	O
)	O
,	O
(	O
2	O
)	O
version1	O
,	O
and	O
(	O
3	O
)	O
r	O
sup	O
f	O
with	O
β	B-HyperparameterName
=	O
0	B-DatasetName
.	O
Notice	O
that	O
all	O
the	O
proposed	O
supervised	O
and	O
unsupervised	O
sentence	O
ranking	O
mod	O
-	O
-	O
-	O
We	O
found	O
that	O
nurses	O
experience	O
a	O
grieving	O
process	O
similar	O
to	O
those	O
directly	O
suffering	O
from	O
perinatal	O
loss	B-MetricName
.	O

Table	O
6	O
shows	O
the	O
final	O
evaluation	O
scores	O
of	O
different	O
competing	O
systems	O
for	O
both	O
the	O
RDoC	O
Task	O
-	O
1	O
&	O
Task	O
-	O
2	O
on	O
final	O
test	O
set	O
.	O
Observe	O
that	O
our	O
submission	O
(	O
MIC	O
-	O
CIS	O
)	O
scored	O
a	O
mAP	B-MetricName
score	O
of	O
0.86	O
and	O
MAA	O
of	O
0.58	O
in	O
Task	O
-	O
1	O
and	O
Task	O
-	O
2	O
,	O
respectively	O
.	O
Notice	O
that	O
we	O
outperform	O
the	O
second	O
best	O
system	O
by	O
20.83	O
%	O
(	O
0.58	O
vs	O
0.48	O
)	O
margin	O
in	O
Task2	O
.	O

In	O
conclusion	O
,	O
both	O
supervised	O
neural	O
topic	O
model	O
and	O
SVM	B-MethodName
can	O
effectively	O
perform	O
ranking	O
of	O
PubMed	O
abstracts	O
in	O
a	O
given	O
cluster	O
based	O
on	O
the	O
prediction	O
probabilities	O
.	O
However	O
,	O
a	O
further	O
reranking	O
using	O
BM25	O
-	O
Extra	O
or	O
query	O
-	O
aware	O
sentence	O
representation	O
(	O
QAR	O
)	O
has	O
proven	O
to	O
maximize	O
the	O
mAP	B-MetricName
score	O
by	O
correctly	O
assigning	O
the	O
lowest	O
relevance	O
score	O
to	O
the	O
intruder	O
abstracts	O
.	O
Also	O
,	O
unsupervised	O
and	O
supervised	O
sentence	O
ranker	O
models	O
using	O
query	O
-	O
title	O
-	O
sentence	O
interactions	O
outperform	O
the	O
traditional	O
BM25	O
-	O
Extra	O
based	O
ranking	O
model	O
by	O
a	O
significant	O
margin	O
.	O
In	O
future	O
,	O
we	O
would	O
like	O
to	O
introduce	O
complementary	O
feature	O
representation	O
via	O
hidden	O
vectors	O
of	O
LSTM	B-MethodName
jointly	O
with	O
topic	B-TaskName
models	I-TaskName
and	O
would	O
like	O
to	O
further	O
investigate	O
the	O
interpretability	O
(	O
Gupta	O
et	O
al	O
,	O
2015	O
;	O
of	O
the	O
proposed	O
neural	O
ranking	O
models	O
in	O
the	O
sense	O
that	O
one	O
can	O
extract	O
salient	O
patterns	O
determining	O
relationship	O
between	O
query	O
and	O
text	O
.	O
Another	O
promising	O
direction	O
would	O
be	O
introduce	O
abstract	O
information	O
,	O
such	O
as	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
and	O
named	O
entity	O
tags	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Gupta	O
et	O
al	O
,	O
2016	O
)	O
to	O
augment	O
information	B-TaskName
retrieval	I-TaskName
(	O
IR	O
)	O
.	O

In	O
this	O
section	O
we	O
compare	O
HCNs	O
to	O
existing	O
approaches	O
on	O
the	O
public	O
"	O
bAbI	O
dialog	O
"	O
dataset	O
(	O
Bordes	O
and	O
Weston	O
,	O
2016	O
)	O
.	O
This	O
dataset	O
includes	O
two	O
end	O
-	O
to	O
-	O
end	O
dialog	B-TaskName
learning	I-TaskName
tasks	O
,	O
in	O
the	O
restaurant	O
domain	O
,	O
called	O
task5	O
and	O
task6	O
.	O
2	O
Task5	O
consists	O
of	O
synthetic	O
,	O
simulated	O
dialog	O
data	O
,	O
with	O
highly	O
regular	O
user	O
behavior	O
and	O
constrained	O
vocabulary	O
.	O
Dialogs	O
include	O
a	O
database	O
access	O
action	O
which	O
retrieves	O
relevant	O
restaurants	O
from	O
a	O
database	O
,	O
with	O
results	O
included	O
in	O
the	O
dialog	O
transcript	O
.	O
We	O
test	O
on	O
the	O
"	O
OOV	O
"	O
variant	O
of	O
Task5	O
,	O
which	O
includes	O
entity	O
values	O
not	O
observed	O
in	O
the	O
training	O
set	O
.	O
Task6	O
draws	O
on	O
human	O
-	O
computer	O
dialog	O
data	O
from	O
the	O
second	O
dialog	O
state	O
tracking	O
challenge	O
(	O
DSTC2	O
)	O
,	O
where	O
usability	O
subjects	O
(	O
crowd	O
-	O
workers	O
)	O
interacted	O
with	O
several	O
variants	O
of	O
a	O
spoken	O
dialog	O
system	O
(	O
Henderson	O
et	O
al	O
,	O
2014a	O
)	O
.	O
Since	O
the	O
database	O
from	O
DSTC2	O
was	O
not	O
provided	O
,	O
database	O
calls	O
have	O
been	O
inferred	O
from	O
the	O
data	O
and	O
inserted	O
into	O
the	O
dialog	O
transcript	O
.	O
Example	O
dialogs	O
are	O
provided	O
in	O
the	O
Appendix	O
Sections	O
A.2	O
and	O
A.3	O
.	O
To	O
apply	O
HCNs	O
,	O
we	O
wrote	O
simple	O
domain	O
-	O
specific	O
software	O
,	O
as	O
follows	O
.	O
First	O
,	O
for	O
entity	O
extraction	O
(	O
step	O
4	O
in	O
Figure	O
1	O
)	O
,	O
we	O
used	O
a	O
simple	O
string	O
match	O
,	O
with	O
a	O
pre	O
-	O
defined	O
list	O
of	O
entity	O
names	O
-	O
i.e.	O
,	O
the	O
list	O
of	O
restaurants	O
available	O
in	O
the	O
database	O
.	O
Second	O
,	O
in	O
the	O
context	O
update	O
(	O
step	O
5	O
)	O
,	O
we	O
wrote	O
simple	O
logic	O
for	O
tracking	O
entities	O
:	O
when	O
an	O
entity	O
is	O
recognized	O
in	O
the	O
user	O
input	O
,	O
it	O
is	O
retained	O
by	O
the	O
software	O
,	O
over	O
-	O
writing	O
any	O
previously	O
stored	O
value	O
.	O
For	O
example	O
,	O
if	O
the	O
price	O
"	O
cheap	O
"	O
is	O
recognized	O
in	O
the	O
first	O
turn	O
,	O
it	O
is	O
retained	O
as	O
price	O
=	O
cheap	O
.	O
If	O
"	O
expensive	O
"	O
is	O
then	O
recognized	O
in	O
the	O
third	O
turn	O
,	O
it	O
over	O
-	O
writes	O
"	O
cheap	O
"	O
so	O
the	O
code	O
now	O
holds	O
price	O
=	O
expensive	O
.	O
Third	O
,	O
system	O
actions	O
were	O
templatized	O
:	O
for	O
example	O
,	O
system	O
actions	O
of	O
the	O
form	O
"	O
prezzo	O
is	O
a	O
nice	O
restaurant	O
in	O
the	O
west	O
of	O
town	O
in	O
the	O
moderate	O
price	O
range	O
"	O
all	O
map	O
to	O
the	O
template	O
"	O
<	O
name	O
>	O
is	O
a	O
nice	O
restaurant	O
in	O
the	O
<	O
location	O
>	O
of	O
town	O
in	O
the	O
<	O
price	O
>	O
price	O
range	O
"	O
.	O
This	O
results	O
in	O
16	O
templates	O
for	O
Task5	O
and	O
58	O
for	O
Task6	O
.	O
3	O
Fourth	O
,	O
when	O
database	O
results	O
are	O
received	O
into	O
the	O
entity	O
state	O
,	O
they	O
are	O
sorted	O
by	O
rating	O
.	O
Finally	O
,	O
an	O
action	O
mask	O
was	O
created	O
which	O
encoded	O
common	O
-	O
sense	O
dependencies	O
.	O
These	O
are	O
implemented	O
as	O
simple	O
if	O
-	O
then	O
rules	O
based	O
on	O
the	O
presence	O
of	O
entity	O
values	O
:	O
for	O
example	O
,	O
only	O
allow	O
an	O
API	O
call	O
if	O
pre	O
-	O
conditions	O
are	O
met	O
;	O
only	O
offer	O
a	O
restaurant	O
if	O
database	O
results	O
have	O
already	O
been	O
received	O
;	O
do	O
not	O
ask	O
for	O
an	O
entity	O
if	O
it	O
is	O
already	O
known	O
;	O
etc	O
.	O
For	O
Task6	O
,	O
we	O
noticed	O
that	O
the	O
system	O
can	O
say	O
that	O
no	O
restaurants	O
match	O
the	O
current	O
query	O
without	O
consulting	O
the	O
database	O
(	O
for	O
an	O
example	O
dialog	O
,	O
see	O
Section	O
A.3	O
in	O
the	O
Appendix	O
)	O
.	O
In	O
a	O
practical	O
system	O
this	O
information	O
would	O
be	O
retrieved	O
from	O
the	O
database	O
and	O
not	O
encoded	O
in	O
the	O
RNN	O
.	O
So	O
,	O
we	O
mined	O
the	O
training	O
data	O
and	O
built	O
a	O
table	O
of	O
search	O
queries	O
known	O
to	O
yield	O
no	O
results	O
.	O
We	O
also	O
added	O
context	O
features	O
that	O
indicated	O
the	O
state	O
of	O
the	O
database	O
-	O
for	O
example	O
,	O
whether	O
there	O
were	O
any	O
restaurants	O
matching	O
the	O
current	O
query	O
.	O
The	O
complete	O
set	O
of	O
context	O
features	O
is	O
given	O
in	O
Appendix	O
Section	O
A.4	O
.	O
Altogether	O
this	O
code	O
consisted	O
of	O
about	O
250	O
lines	O
of	O
Python	O
.	O
We	O
then	O
trained	O
an	O
HCN	O
on	O
the	O
training	O
set	O
,	O
employing	O
the	O
domain	O
-	O
specific	O
software	O
described	O
above	O
.	O
We	O
selected	O
an	O
LSTM	B-MethodName
for	O
the	O
recurrent	O
layer	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
,	O
with	O
the	O
AdaDelta	B-MethodName
optimizer	B-HyperparameterName
(	O
Zeiler	O
,	O
2012	O
)	O
.	O
We	O
used	O
the	O
development	O
set	O
to	O
tune	O
the	O
number	O
of	O
hid	O
-	O
den	O
units	O
(	O
128	O
)	O
,	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
(	O
12	O
)	O
.	O
Utterance	O
embeddings	O
were	O
formed	O
by	O
averaging	O
word	B-TaskName
embeddings	I-TaskName
,	O
using	O
a	O
publicly	O
available	O
300dimensional	O
word	O
embedding	O
model	O
trained	O
using	O
word2vec	O
on	O
web	O
data	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
4	O
The	O
word	B-TaskName
embeddings	I-TaskName
were	O
static	O
and	O
not	O
updated	O
during	O
LSTM	B-MethodName
training	O
.	O
In	O
training	O
,	O
each	O
dialog	O
formed	O
one	O
minibatch	O
,	O
and	O
updates	O
were	O
done	O
on	O
full	O
rollouts	O
(	O
i.e.	O
,	O
non	O
-	O
truncated	O
back	O
propagation	O
through	O
time	O
)	O
.	O
The	O
training	O
loss	B-MetricName
was	O
categorical	O
cross	O
-	O
entropy	O
.	O
Further	O
low	O
-	O
level	O
implementation	O
details	O
are	O
in	O
the	O
Appendix	O
Section	O
A.1	O
.	O
We	O
ran	O
experiments	O
with	O
four	O
variants	O
of	O
our	O
model	O
:	O
with	O
and	O
without	O
the	O
utterance	O
embeddings	O
,	O
and	O
with	O
and	O
without	O
the	O
action	O
mask	O
(	O
Figure	O
1	O
,	O
steps	O
3	O
and	O
6	O
respectively	O
)	O
.	O
Following	O
past	O
work	O
,	O
we	O
report	O
average	O
turn	O
accuracy	B-MetricName
-	O
i.e.	O
,	O
for	O
each	O
turn	O
in	O
each	O
dialog	O
,	O
present	O
the	O
(	O
true	O
)	O
history	O
of	O
user	O
and	O
system	O
actions	O
to	O
the	O
network	O
and	O
obtain	O
the	O
network	O
's	O
prediction	O
as	O
a	O
string	O
of	O
characters	O
.	O
The	O
turn	O
is	O
correct	O
if	O
the	O
string	O
matches	O
the	O
reference	O
exactly	O
,	O
and	O
incorrect	O
if	O
not	O
.	O
We	O
also	O
report	O
dialog	O
accuracy	B-MetricName
,	O
which	O
indicates	O
if	O
all	O
turns	O
in	O
a	O
dialog	O
are	O
correct	O
.	O
We	O
compare	O
to	O
four	O
past	O
end	O
-	O
to	O
-	O
end	O
approaches	O
(	O
Bordes	O
and	O
Weston	O
,	O
2016	O
;	O
Liu	O
and	O
Perez	O
,	O
2016	O
;	O
Eric	O
and	O
Manning	O
,	O
2017	O
;	O
Seo	O
et	O
al	O
,	O
2016	O
)	O
.	O
We	O
emphasize	O
that	O
past	O
approaches	O
have	O
applied	O
purely	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
,	O
or	O
(	O
as	O
a	O
baseline	O
)	O
purely	O
programmed	O
rules	O
(	O
Bordes	O
and	O
Weston	O
,	O
2016	O
)	O
.	O
By	O
contrast	O
,	O
Hybrid	O
Code	O
Networks	O
are	O
a	O
hybrid	O
of	O
hand	O
-	O
coded	O
rules	O
and	O
learned	O
models	O
.	O
Results	O
are	O
shown	O
in	O
Table	O
1	O
.	O
Since	O
Task5	O
is	O
synthetic	O
data	O
generated	O
using	O
rules	O
,	O
it	O
is	O
possible	O
to	O
obtain	O
perfect	O
accuracy	B-MetricName
using	O
rules	O
(	O
line	O
1	O
)	O
.	O
The	O
addition	O
of	O
domain	O
knowledge	O
greatly	O
simplifies	O
the	O
learning	O
task	O
and	O
enables	O
HCNs	O
to	O
also	O
attain	O
perfect	O
accuracy	B-MetricName
.	O
On	O
Task6	O
,	O
rules	O
alone	O
fare	O
poorly	O
,	O
whereas	O
HCNs	O
outperform	O
past	O
learned	O
models	O
.	O
We	O
next	O
examined	O
learning	O
curves	O
,	O
training	O
with	O
increasing	O
numbers	O
of	O
dialogs	O
.	O
To	O
guard	O
against	O
bias	O
in	O
the	O
ordering	O
of	O
the	O
training	O
set	O
,	O
we	O
averaged	O
over	O
5	O
runs	O
,	O
randomly	O
permuting	O
the	O
order	O
of	O
the	O
training	O
dialogs	O
in	O
each	O
run	O
.	O
Results	O
are	O
in	O
Figure	O
2	O
.	O
In	O
Task5	O
,	O
the	O
action	O
mask	O
and	O
utterance	O
embeddings	O
substantially	O
reduce	O
the	O
number	O
of	O
training	O
dialogs	O
required	O
(	O
note	O
the	O
horizontal	O
axis	O
scale	O
is	O
logarithmic	O
)	O
.	O
For	O
Task6	O
,	O
the	O
bene	O
-	O
(	O
Bordes	O
and	O
Weston	O
,	O
2016	O
)	O
.	O
Results	O
for	O
"	O
Rules	O
"	O
taken	O
from	O
Bordes	O
and	O
Weston	O
(	O
2016	O
)	O
.	O
Note	O
that	O
,	O
unlike	O
cited	O
past	O
work	O
,	O
HCNs	O
make	O
use	O
of	O
domainspecific	O
procedural	O
knowledge	O
.	O
Figure	O
2	O
:	O
Training	O
dialog	O
count	O
vs.	O
turn	O
accuracy	B-MetricName
for	O
bAbI	O
dialog	O
Task5	O
-	O
OOV	O
and	O
Task6	O
.	O
"	O
embed	O
"	O
indicates	O
whether	O
utterance	O
embeddings	O
were	O
included	O
;	O
"	O
mask	O
"	O
indicates	O
whether	O
the	O
action	O
masking	O
code	O
was	O
active	O
.	O
fits	O
of	O
the	O
utterance	O
embeddings	O
are	O
less	O
clear	O
.	O
An	O
error	O
analysis	O
showed	O
that	O
there	O
are	O
several	O
systematic	O
differences	O
between	O
the	O
training	O
and	O
testing	O
sets	O
.	O
Indeed	O
,	O
DSTC2	O
intentionally	O
used	O
different	O
dialog	O
policies	O
for	O
the	O
training	O
and	O
test	O
sets	O
,	O
whereas	O
our	O
goal	O
is	O
to	O
mimic	O
the	O
policy	O
in	O
the	O
training	O
set	O
.	O
Nonetheless	O
,	O
these	O
tasks	O
are	O
the	O
best	O
public	O
benchmark	O
we	O
are	O
aware	O
of	O
,	O
and	O
HCNs	O
exceed	O
performance	O
of	O
existing	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
.	O
In	O
addition	O
,	O
they	O
match	O
performance	O
of	O
past	O
models	O
using	O
an	O
order	O
of	O
magnitude	O
less	O
data	O
(	O
200	O
vs.	O
1618	O
dialogs	O
)	O
,	O
which	O
is	O
crucial	O
in	O
practical	O
settings	O
where	O
collecting	O
realistic	O
dialogs	O
for	O
a	O
new	O
domain	O
can	O
be	O
expensive	O
.	O

We	O
now	O
turn	O
to	O
comparing	O
with	O
purely	O
handcrafted	O
approaches	O
.	O
To	O
do	O
this	O
,	O
we	O
obtained	O
logs	O
from	O
our	O
company	O
's	O
text	O
-	O
based	O
customer	O
support	O
dialog	O
system	O
,	O
which	O
uses	O
a	O
sophisticated	O
rulebased	O
dialog	O
manager	O
.	O
Data	O
from	O
this	O
system	O
is	O
attractive	O
for	O
evaluation	O
because	O
it	O
is	O
used	O
by	O
real	O
customers	O
-	O
not	O
usability	O
subjects	O
-	O
and	O
because	O
its	O
rule	O
-	O
based	O
dialog	O
manager	O
was	O
developed	O
by	O
customer	O
support	O
professionals	O
at	O
our	O
company	O
,	O
and	O
not	O
the	O
authors	O
.	O
This	O
data	O
is	O
not	O
publicly	O
available	O
,	O
but	O
we	O
are	O
unaware	O
of	O
suitable	O
humancomputer	O
dialog	O
data	O
in	O
the	O
public	O
domain	O
which	O
uses	O
rules	O
.	O
Customers	O
start	O
using	O
the	O
dialog	O
system	O
by	O
entering	O
a	O
brief	O
description	O
of	O
their	O
problem	O
,	O
such	O
as	O
"	O
I	O
need	O
to	O
update	O
my	O
operating	O
system	O
"	O
.	O
They	O
are	O
then	O
routed	O
to	O
one	O
of	O
several	O
hundred	O
domains	O
,	O
where	O
each	O
domain	O
attempts	O
to	O
resolve	O
a	O
particular	O
problem	O
.	O
In	O
this	O
study	O
,	O
we	O
collected	O
humancomputer	O
transcripts	O
for	O
the	O
high	O
-	O
traffic	O
domains	O
"	O
reset	O
password	O
"	O
and	O
"	O
can	O
not	O
access	O
account	O
"	O
.	O
We	O
labeled	O
the	O
dialog	O
data	O
as	O
follows	O
.	O
First	O
,	O
we	O
enumerated	O
unique	O
system	O
actions	O
observed	O
in	O
the	O
data	O
.	O
Then	O
,	O
for	O
each	O
dialog	O
,	O
starting	O
from	O
the	O
beginning	O
,	O
we	O
examined	O
each	O
system	O
action	O
,	O
and	O
determined	O
whether	O
it	O
was	O
"	O
correct	O
"	O
.	O
Here	O
,	O
correct	O
means	O
that	O
it	O
was	O
the	O
most	O
appropriate	O
action	O
among	O
the	O
set	O
of	O
existing	O
system	O
actions	O
,	O
given	O
the	O
history	O
of	O
that	O
dialog	O
.	O
If	O
multiple	O
actions	O
were	O
arguably	O
appropriate	O
,	O
we	O
broke	O
ties	O
in	O
favor	O
of	O
the	O
existing	O
rule	O
-	O
based	O
dialog	O
manager	O
.	O
Example	O
dialogs	O
are	O
provided	O
in	O
the	O
Appendix	O
Sections	O
A.5	O
and	O
A.6	O
.	O
If	O
a	O
system	O
action	O
was	O
labeled	O
as	O
correct	O
,	O
we	O
left	O
it	O
as	O
-	O
is	O
and	O
continued	O
to	O
the	O
next	O
system	O
action	O
.	O
If	O
the	O
system	O
action	O
was	O
not	O
correct	O
,	O
we	O
replaced	O
it	O
with	O
the	O
correct	O
system	O
action	O
,	O
and	O
discarded	O
the	O
rest	O
of	O
the	O
dialog	O
,	O
since	O
we	O
do	O
not	O
know	O
how	O
the	O
user	O
would	O
have	O
replied	O
to	O
this	O
new	O
system	O
action	O
.	O
The	O
resulting	O
dataset	O
contained	O
a	O
mixture	O
of	O
complete	O
and	O
partial	O
dialogs	O
,	O
containing	O
only	O
correct	O
system	O
actions	O
.	O
We	O
partitioned	O
this	O
set	O
into	O
training	O
and	O
test	O
dialogs	O
.	O
Basic	O
statistics	O
of	O
the	O
data	O
are	O
shown	O
in	O
Table	O
2	O
.	O
In	O
this	O
domain	O
,	O
no	O
entities	O
were	O
relevant	O
to	O
the	O
control	O
flow	O
,	O
and	O
there	O
was	O
no	O
obvious	O
mask	O
logic	O
since	O
any	O
question	O
could	O
follow	O
any	O
question	O
.	O
Therefore	O
,	O
we	O
wrote	O
no	O
domain	O
-	O
specific	O
software	O
for	O
this	O
instance	O
of	O
the	O
HCN	O
,	O
and	O
relied	O
purely	O
on	O
the	O
recurrent	O
neural	O
network	O
to	O
drive	O
the	O
conversation	O
.	O
The	O
architecture	O
and	O
training	O
of	O
the	O
RNN	O
was	O
the	O
same	O
as	O
in	O
Section	O
4	O
,	O
except	O
that	O
here	O
we	O
did	O
not	O
have	O
enough	O
data	O
for	O
a	O
validation	O
set	O
,	O
so	O
we	O
instead	O
trained	O
until	O
we	O
either	O
achieved	O
100	O
%	O
accuracy	B-MetricName
on	O
the	O
training	O
set	O
or	O
reached	O
200	O
epochs	O
.	O
To	O
evaluate	O
,	O
we	O
observe	O
that	O
conventional	O
measures	O
like	O
average	O
dialog	O
accuracy	B-MetricName
unfairly	O
penalize	O
the	O
system	O
used	O
to	O
collect	O
the	O
dialogs	O
-	O
in	O
our	O
case	O
,	O
the	O
rule	O
-	O
based	O
system	O
.	O
If	O
the	O
system	O
used	O
for	O
collection	O
makes	O
an	O
error	O
at	O
turn	O
t	O
,	O
the	O
labeled	O
dialog	O
only	O
includes	O
the	O
sub	O
-	O
dialog	O
up	O
to	O
turn	O
t	O
,	O
and	O
the	O
system	O
being	O
evaluated	O
off	O
-	O
line	O
is	O
only	O
evaluated	O
on	O
that	O
sub	O
-	O
dialog	O
.	O
In	O
other	O
words	O
,	O
in	O
our	O
case	O
,	O
reporting	O
dialog	O
accuracy	B-MetricName
would	O
favor	O
the	O
HCN	O
because	O
it	O
would	O
be	O
evaluated	O
on	O
fewer	O
turns	O
than	O
the	O
rule	O
-	O
based	O
system	O
.	O
We	O
therefore	O
,	O
where	O
C	O
(	O
HCN	O
-	O
win	O
)	O
is	O
the	O
number	O
of	O
test	O
dialogs	O
where	O
the	O
rule	O
-	O
based	O
approach	O
output	O
a	O
wrong	O
action	O
before	O
the	O
HCN	O
;	O
C	O
(	O
rule	O
-	O
win	O
)	O
is	O
the	O
number	O
of	O
test	O
dialogs	O
where	O
the	O
HCN	O
output	O
a	O
wrong	O
action	O
before	O
the	O
rulebased	O
approach	O
;	O
and	O
C	O
(	O
all	O
)	O
is	O
the	O
number	O
of	O
dialogs	O
in	O
the	O
test	O
set	O
.	O
When	O
∆P	O
>	O
0	B-DatasetName
,	O
there	O
are	O
more	O
dialogs	O
in	O
which	O
HCNs	O
produce	O
longer	O
continuous	O
sequences	O
of	O
correct	O
actions	O
starting	O
from	O
the	O
beginning	O
of	O
the	O
dialog	O
.	O
We	O
run	O
all	O
experiments	O
5	O
times	O
,	O
each	O
time	O
shuffling	O
the	O
order	O
of	O
the	O
training	O
set	O
.	O
Results	O
are	O
in	O
Figure	O
3	O
.	O
HCNs	O
exceed	O
performance	O
of	O
the	O
existing	O
rule	O
-	O
based	O
system	O
after	O
about	O
30	O
dialogs	O
.	O
In	O
these	O
domains	O
,	O
we	O
have	O
a	O
further	O
source	O
of	O
knowledge	O
:	O
the	O
rule	O
-	O
based	O
dialog	O
managers	O
themselves	O
can	O
be	O
used	O
to	O
generate	O
example	O
"	O
sunnyday	O
"	O
dialogs	O
,	O
where	O
the	O
user	O
provides	O
purely	O
expected	O
inputs	O
.	O
From	O
each	O
rule	O
-	O
based	O
controller	O
,	O
synthetic	O
dialogs	O
were	O
sampled	O
to	O
cover	O
each	O
expected	O
user	O
response	O
at	O
least	O
once	O
,	O
and	O
added	O
to	O
the	O
set	O
of	O
labeled	O
real	O
dialogs	O
.	O
This	O
resulted	O
in	O
75	O
dialogs	O
for	O
the	O
"	O
Forgot	O
password	O
"	O
domain	O
,	O
and	O
325	O
for	O
the	O
"	O
Ca	O
n't	O
access	O
account	O
"	O
domain	O
.	O
Training	O
was	O
repeated	O
as	O
described	O
above	O
.	O
Results	O
are	O
also	O
included	O
in	O
Figure	O
3	O
,	O
with	O
the	O
suffix	O
"	O
sampled	O
"	O
.	O
In	O
the	O
"	O
Ca	O
n't	O
access	O
account	O
"	O
domain	O
,	O
the	O
sampled	O
dialogs	O
yield	O
a	O
large	O
improvement	O
,	O
probably	O
because	O
the	O
flow	O
chart	O
for	O
this	O
domain	O
is	O
large	O
,	O
so	O
the	O
sampled	O
dialogs	O
increase	O
coverage	O
.	O
The	O
gain	O
in	O
the	O
"	O
forgot	O
password	O
"	O
domain	O
is	O
present	O
but	O
smaller	O
.	O
In	O
summary	O
,	O
HCNs	O
can	O
out	O
-	O
perform	O
Figure	O
3	O
:	O
Training	O
dialogs	O
vs.	O
∆P	O
,	O
where	O
∆P	O
is	O
the	O
fraction	O
of	O
test	O
dialogs	O
where	O
HCNs	O
produced	O
longer	O
initial	O
correct	O
sequences	O
of	O
system	O
actions	O
than	O
the	O
rules	O
,	O
minus	O
the	O
fraction	O
where	O
rules	O
produced	O
longer	O
initial	O
correct	O
sequences	O
than	O
the	O
HCNs	O
.	O
"	O
embed	O
"	O
indicates	O
whether	O
utterance	O
embeddings	O
were	O
included	O
;	O
"	O
sampled	O
"	O
indicates	O
whether	O
dialogs	O
sampled	O
from	O
the	O
rule	O
-	O
based	O
controller	O
were	O
included	O
in	O
the	O
training	O
set	O
.	O
production	O
-	O
grade	O
rule	O
-	O
based	O
systems	O
with	O
a	O
reasonable	O
number	O
of	O
labeled	O
dialogs	O
,	O
and	O
adding	O
synthetic	O
"	O
sunny	O
-	O
day	O
"	O
dialogs	O
improves	O
performance	O
further	O
.	O
Moreover	O
,	O
unlike	O
existing	O
pipelined	O
approaches	O
to	O
dialog	O
management	O
that	O
rely	O
on	O
an	O
explicit	O
state	O
tracker	O
,	O
this	O
HCN	O
used	O
no	O
explicit	O
state	O
tracker	O
,	O
highlighting	O
an	O
advantage	O
of	O
the	O
model	O
.	O

In	O
the	O
previous	O
sections	O
,	O
supervised	O
learning	O
(	O
SL	O
)	O
was	O
applied	O
to	O
train	O
the	O
LSTM	B-MethodName
to	O
mimic	O
dialogs	O
provided	O
by	O
the	O
system	O
developer	O
.	O
Once	O
a	O
system	O
operates	O
at	O
scale	O
,	O
interacting	O
with	O
a	O
large	O
number	O
of	O
users	O
,	O
it	O
is	O
desirable	O
for	O
the	O
system	O
to	O
continue	O
to	O
learn	O
autonomously	O
using	O
reinforcement	O
learning	O
(	O
RL	O
)	O
.	O
With	O
RL	O
,	O
each	O
turn	O
receives	O
a	O
measurement	O
of	O
goodness	O
called	O
a	O
reward	O
;	O
the	O
agent	B-DatasetName
explores	O
different	O
sequences	O
of	O
actions	O
in	O
different	O
situations	O
,	O
and	O
makes	O
adjustments	O
so	O
as	O
to	O
maximize	O
the	O
expected	O
discounted	O
sum	O
of	O
rewards	O
,	O
which	O
is	O
called	O
the	O
return	O
,	O
denoted	O
G.	O
For	O
optimization	O
,	O
we	O
selected	O
a	O
policy	O
gradient	O
approach	O
(	O
Williams	O
,	O
1992	O
)	O
,	O
which	O
has	O
been	O
successfully	O
applied	O
to	O
dialog	O
systems	O
(	O
Jurčíček	O
et	O
al	O
,	O
2011	O
)	O
,	O
robotics	O
(	O
Kohl	O
and	O
Stone	O
,	O
2004	O
)	O
,	O
and	O
the	O
board	O
game	O
Go	O
(	O
Silver	O
et	O
al	O
,	O
2016	O
)	O
.	O
In	O
policy	O
gradient	O
-	O
based	O
RL	O
,	O
a	O
model	O
π	O
is	O
parameterized	O
by	O
w	O
and	O
outputs	O
a	O
distribution	O
from	O
which	O
actions	O
are	O
sampled	O
at	O
each	O
timestep	O
.	O
At	O
the	O
end	O
of	O
a	O
trajectory	O
-	O
in	O
our	O
case	O
,	O
dialog	O
-	O
the	O
return	O
G	O
for	O
that	O
trajectory	O
is	O
computed	O
,	O
and	O
the	O
gradients	O
of	O
the	O
probabilities	O
of	O
the	O
actions	O
taken	O
with	O
respect	O
to	O
the	O
model	O
weights	O
are	O
computed	O
.	O
The	O
weights	O
are	O
then	O
adjusted	O
by	O
taking	O
a	O
gradient	O
step	O
proportional	O
to	O
the	O
return	O
:	O
w	O
w+α	O
(	O
t	O
w	O
log	O
π	O
(	O
a	O
t	O
|	O
h	O
t	O
;	O
w	O
)	O
)	O
(	O
G−b	O
)	O
(	O
1	O
)	O
where	O
α	B-HyperparameterName
is	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
;	O
a	O
t	O
is	O
the	O
action	O
taken	O
at	O
timestep	O
t	O
;	O
h	O
t	O
is	O
the	O
dialog	O
history	O
at	O
time	O
t	O
;	O
G	O
is	O
the	O
return	O
of	O
the	O
dialog	O
;	O
x	O
F	O
denotes	O
the	O
Jacobian	O
of	O
F	O
with	O
respect	O
to	O
x	O
;	O
b	O
is	O
a	O
baseline	O
described	O
below	O
;	O
and	O
π	O
(	O
a	O
|	O
h	O
;	O
w	O
)	O
is	O
the	O
LSTM	B-MethodName
-	O
i.e.	O
,	O
a	O
stochastic	O
policy	O
which	O
outputs	O
a	O
distribution	O
over	O
a	O
given	O
a	O
dialog	O
history	O
h	O
,	O
parameterized	O
by	O
weights	O
w.	O
The	O
baseline	O
b	O
is	O
an	O
estimate	O
of	O
the	O
average	B-MetricName
return	I-MetricName
of	O
the	O
current	O
policy	O
,	O
estimated	O
on	O
the	O
last	O
100	O
dialogs	O
using	O
weighted	O
importance	O
sampling	O
.	O
5	O
Intuitively	O
,	O
"	O
better	O
"	O
dialogs	O
receive	O
a	O
positive	O
gradient	O
step	O
,	O
making	O
the	O
actions	O
selected	O
more	O
likely	O
;	O
and	O
"	O
worse	O
"	O
dialogs	O
receive	O
a	O
negative	O
gradient	O
step	O
,	O
making	O
the	O
actions	O
selected	O
less	O
likely	O
.	O
SL	O
and	O
RL	O
correspond	O
to	O
different	O
methods	O
of	O
updating	O
weights	O
,	O
so	O
both	O
can	O
be	O
applied	O
to	O
the	O
same	O
network	O
.	O
However	O
,	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
optimal	O
RL	O
policy	O
will	O
agree	O
with	O
the	O
SL	O
training	O
set	O
;	O
therefore	O
,	O
after	O
each	O
RL	O
gradient	O
step	O
,	O
we	O
check	O
whether	O
the	O
updated	O
policy	O
reconstructs	O
the	O
training	O
set	O
.	O
If	O
not	O
,	O
we	O
re	O
-	O
run	O
SL	O
gradient	O
steps	O
on	O
the	O
training	O
set	O
until	O
the	O
model	O
reproduces	O
the	O
training	O
set	O
.	O
Note	O
that	O
this	O
approach	O
allows	O
new	O
training	O
dialogs	O
to	O
be	O
added	O
at	O
any	O
time	O
during	O
RL	O
optimization	O
.	O
We	O
illustrate	O
RL	O
optimization	O
on	O
a	O
simulated	O
dialog	O
task	O
in	O
the	O
name	O
dialing	O
domain	O
.	O
In	O
this	O
system	O
,	O
a	O
contact	O
's	O
name	O
may	O
have	O
synonyms	O
(	O
"	O
Michael	O
"	O
may	O
also	O
be	O
called	O
"	O
Mike	O
"	O
)	O
,	O
and	O
a	O
contact	O
may	O
have	O
more	O
than	O
one	O
phone	O
number	O
,	O
such	O
as	O
"	O
work	O
"	O
or	O
"	O
mobile	O
"	O
,	O
which	O
may	O
in	O
turn	O
have	O
synonyms	O
like	O
"	O
cell	O
"	O
for	O
"	O
mobile	O
"	O
.	O
This	O
domain	O
has	O
a	O
database	O
of	O
names	O
and	O
phone	O
numbers	O
taken	O
from	O
the	O
Microsoft	O
personnel	O
directory	O
,	O
5	O
entity	O
typesfirstname	O
,	O
nickname	O
,	O
lastname	O
,	O
phonenumber	O
,	O
and	O
phonetype	O
-	O
and	O
14	O
actions	O
,	O
including	O
2	O
API	O
call	O
actions	O
.	O
Simple	O
entity	O
logic	O
was	O
coded	O
,	O
which	O
retains	O
the	O
most	O
recent	O
copy	O
of	O
recognized	O
entities	O
.	O
A	O
simple	O
action	O
mask	O
suppresses	O
impossible	O
actions	O
,	O
such	O
as	O
placing	O
a	O
phonecall	O
before	O
a	O
phone	O
number	O
has	O
been	O
retrieved	O
from	O
the	O
database	O
.	O
Example	O
dialogs	O
are	O
provided	O
in	O
Appendix	O
Section	O
A.7	O
.	O
To	O
perform	O
optimization	O
,	O
we	O
created	O
a	O
simulated	O
user	O
.	O
At	O
the	O
start	O
of	O
a	O
dialog	O
,	O
the	O
simulated	O
user	O
randomly	O
selected	O
a	O
name	O
and	O
phone	O
type	O
,	O
including	O
names	O
and	O
phone	O
types	O
not	O
covered	O
by	O
the	O
dialog	O
system	O
.	O
When	O
speaking	O
,	O
the	O
simulated	O
user	O
can	O
use	O
the	O
canonical	O
name	O
or	O
a	O
nickname	O
;	O
usually	O
answers	O
questions	O
but	O
can	O
ignore	O
the	O
system	O
;	O
can	O
provide	O
additional	O
information	O
not	O
requested	O
;	O
and	O
can	O
give	O
up	O
.	O
The	O
simulated	O
user	O
was	O
parameterized	O
by	O
around	O
10	O
probabilities	O
,	O
set	O
by	O
hand	O
.	O
We	O
defined	O
the	O
reward	O
as	O
being	O
1	O
for	O
successfully	O
completing	O
the	O
task	O
,	O
and	O
0	B-DatasetName
otherwise	O
.	O
A	O
discount	O
of	O
0.95	O
was	O
used	O
to	O
incentivize	O
the	O
system	O
to	O
complete	O
dialogs	O
faster	O
rather	O
than	O
slower	O
,	O
yielding	O
return	O
0	B-DatasetName
for	O
failed	O
dialogs	O
,	O
and	O
G	O
=	O
0.95	O
T	O
−1	O
for	O
successful	O
dialogs	O
,	O
where	O
T	O
is	O
the	O
number	O
of	O
system	O
turns	O
in	O
the	O
dialog	O
.	O
Finally	O
,	O
we	O
created	O
a	O
set	O
of	O
21	O
labeled	O
dialogs	O
,	O
which	O
will	O
be	O
used	O
for	O
supervised	O
learning	O
.	O
For	O
the	O
RNN	O
in	O
the	O
HCN	O
,	O
we	O
again	O
used	O
an	O
LSTM	B-MethodName
with	O
AdaDelta	B-MethodName
,	O
this	O
time	O
with	O
32	O
hidden	O
units	O
.	O
RL	O
policy	O
updates	O
are	O
made	O
after	O
each	O
dialog	O
.	O
Since	O
a	O
simulated	O
user	O
was	O
employed	O
,	O
we	O
did	O
not	O
have	O
real	O
user	O
utterances	O
,	O
and	O
instead	O
relied	O
on	O
context	O
features	O
,	O
omitting	O
bag	O
-	O
of	O
-	O
words	O
and	O
utterance	O
embedding	O
features	O
.	O
We	O
first	O
evaluate	O
RL	O
by	O
randomly	O
initializing	O
an	O
LSTM	B-MethodName
,	O
and	O
begin	O
RL	O
optimization	O
.	O
After	O
10	O
RL	O
updates	O
,	O
we	O
freeze	O
the	O
policy	O
,	O
and	O
run	O
500	O
dialogs	O
with	O
the	O
user	O
simulation	O
to	O
measure	O
task	O
completion	O
.	O
We	O
repeat	O
all	O
of	O
this	O
for	O
100	O
runs	O
,	O
and	O
report	O
average	O
performance	O
.	O
In	O
addition	O
,	O
we	O
also	O
report	O
results	O
by	O
initializing	O
the	O
LSTM	B-MethodName
using	O
supervised	O
learning	O
on	O
the	O
training	O
set	O
,	O
consisting	O
of	O
1	O
,	O
2	O
,	O
5	O
,	O
or	O
10	O
dialogs	O
sampled	O
randomly	O
from	O
the	O
training	O
set	O
,	O
then	O
running	O
RL	O
as	O
described	O
above	O
.	O
Results	O
are	O
in	O
Figure	O
4	O
.	O
Although	O
RL	O
alone	O
can	O
find	O
a	O
good	O
policy	O
,	O
pre	O
-	O
training	O
with	O
just	O
a	O
handful	O
of	O
labeled	O
dialogs	O
improves	O
learning	O
speed	O
dramatically	O
.	O
Additional	O
experiments	O
,	O
not	O
shown	O
for	O
space	O
,	O
found	O
that	O
ablating	O
the	O
action	O
mask	O
slowed	O
training	O
,	O
agreeing	O
with	O
Williams	O
(	O
2008	O
)	O
.	O
Finally	O
,	O
we	O
conduct	O
a	O
further	O
experiment	O
where	O
we	O
sample	O
10	O
training	O
dialogs	O
,	O
then	O
add	O
one	O
to	O
the	O
training	O
set	O
just	O
before	O
RL	O
dialog	O
0	B-DatasetName
,	O
100	O
,	O
200	O
,	O
...	O
,	O
900	O
.	O
Results	O
are	O
shown	O
in	O
Figure	O
4	O
.	O
This	O
shows	O
that	O
SL	O
dialogs	O
can	O
be	O
introduced	O
as	O
RL	O
is	O
in	O
progress	O
-	O
i.e.	O
,	O
that	O
it	O
is	O
possible	O
to	O
interleave	O
RL	O
and	O
SL	O
.	O
This	O
is	O
an	O
attractive	O
property	O
for	O
practical	O
systems	O
:	O
if	O
a	O
dialog	O
error	O
is	O
spotted	O
by	O
a	O
developer	O
while	O
RL	O
is	O
in	O
progress	O
,	O
it	O
is	O
natural	O
to	O
add	O
a	O
training	O
dialog	O
to	O
the	O
training	O
set	O
.	O

Discourse	O
elements	O
in	O
argumentative	O
essays	O
are	O
sensitive	O
to	O
their	O
positions	O
.	O
For	O
example	O
,	O
introduction	O
mostly	O
comes	O
before	O
thesis	O
or	O
main	O
ideas	O
and	O
main	O
ideas	O
may	O
occur	O
more	O
often	O
at	O
the	O
beginnings	O
or	O
endings	O
of	O
paragraphs	O
.	O
Figure	O
2	O
shows	O
an	O
essay	O
with	O
7	O
sentences	O
and	O
4	O
paragraphs	O
as	O
an	O
example	O
.	O
We	O
consider	O
three	O
types	O
of	O
sentence	O
positions	O
for	O
positional	O
encoding	O
.	O
Global	O
position	O
:	O
The	O
index	O
of	O
a	O
sentence	O
is	O
used	O
to	O
describe	O
its	O
position	O
where	O
we	O
assume	O
sentences	O
in	O
an	O
essay	O
form	O
a	O
sequence	O
.	O
Paragraph	O
position	O
:	O
An	O
essay	O
has	O
multiple	O
paragraphs	O
.	O
The	O
position	O
of	O
the	O
paragraph	O
that	O
contains	O
the	O
sentence	O
is	O
also	O
important	O
.	O
Local	O
position	O
:	O
The	O
position	O
of	O
the	O
sentence	O
in	O
its	O
paragraph	O
is	O
informative	O
as	O
well	O
.	O
We	O
adopt	O
a	O
relative	O
positional	O
encoding	O
approach	O
.	O
We	O
compute	O
the	O
relative	O
positions	O
for	O
the	O
above	O
three	O
position	O
types	O
.	O
For	O
example	O
,	O
the	O
relative	O
global	O
position	O
of	O
the	O
i	O
-	O
th	O
(	O
i	O
≥	O
1	O
)	O
sentence	O
in	O
an	O
essay	O
E	O
is	O
pos	O
global	O
(	O
i	O
)	O
=	O
i	O
|	O
E	O
|	O
,	O
(	O
4	O
)	O
where	O
|	O
E	O
|	O
is	O
the	O
number	O
of	O
sentences	O
.	O
To	O
integrate	O
with	O
sentence	O
representations	O
,	O
we	O
expand	O
pos	O
global	O
(	O
i	O
)	O
to	O
a	O
vector	O
of	O
the	O
same	O
dimension	O
d	O
of	O
the	O
distributed	O
sentence	O
representations	O
by	O
duplicating	O
its	O
value	O
to	O
every	O
dimension	O
,	O
noted	O
as	O
pos	O
global	O
(	O
i	O
)	O
R	O
d	O
.	O
The	O
relative	O
paragraph	O
position	O
representation	O
pos	O
para	O
(	O
i	O
)	O
and	O
relative	O
local	O
position	O
representation	O
pos	O
local	O
(	O
i	O
)	O
are	O
computed	O
in	O
the	O
same	O
way	O
.	O
The	O
final	O
position	O
representation	O
pos	O
(	O
i	O
)	O
is	O
formulated	O
as	O
a	O
liner	O
combination	O
of	O
the	O
three	O
relative	O
position	O
representations	O
,	O
i.e.	O
,	O
pos	O
(	O
i	O
)	O
=	O
t	O
{	O
global	O
,	O
local	O
,	O
para	O
}	O
β	B-HyperparameterName
t	O
pos	O
t	O
(	O
i	O
)	O
,	O
(	O
5	O
)	O
where	O
{	O
β	B-HyperparameterName
t	O
}	O
are	O
parameters	O
to	O
be	O
learnt	O
in	O
training	O
.	O
The	O
element	O
representation	O
of	O
the	O
i	O
-	O
th	O
sentence	O
is	O
e	O
i	O
=	O
tanh	O
(	O
BiLSTM	B-MethodName
(	O
C	O
i	O
+	O
pos	O
(	O
i	O
)	O
)	O
)	O
.	O
(	O
6	O
)	O

Self	O
-	O
Attention	O
relates	O
elements	O
at	O
different	O
positions	O
by	O
computing	O
attention	O
between	O
every	O
pair	O
of	O
elements	O
.	O
An	O
attention	B-HyperparameterName
function	I-HyperparameterName
is	O
to	O
map	O
a	O
query	O
and	O
a	O
set	O
of	O
key	O
-	O
value	O
pairs	O
to	O
an	O
output	O
.	O
The	O
queries	O
Q	O
,	O
keys	O
K	O
and	O
values	O
V	O
are	O
vectors	O
.	O
We	O
define	O
Q	O
,	O
K	O
R	O
d	O
k	O
×n	O
and	O
d	O
k	O
is	O
the	O
dimension	O
.	O
The	O
attention	O
is	O
computed	O
as	O
α	B-HyperparameterName
=	O
Attn	O
(	O
Q	O
,	O
K	O
)	O
=	O
softmax	B-MethodName
(	O
QK	O
T	O
√	O
d	O
k	O
)	O
.	O
(	O
7	O
)	O
The	O
output	O
is	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
values	O
,	O
i.e.	O
,	O
αV.	O
Here	O
,	O
we	O
are	O
interested	O
in	O
the	O
attention	O
vectors	O
rather	O
than	O
the	O
weighted	O
output	O
,	O
because	O
an	O
attention	O
vector	O
reflects	O
the	O
relatedness	O
of	O
a	O
given	O
sentence	O
to	O
every	O
other	O
sentence	O
.	O
We	O
propose	O
the	O
inter	O
-	O
sentence	O
attention	O
(	O
ISA	O
)	O
by	O
applying	O
self	O
-	O
attention	O
to	O
sentence	O
semantic	O
representations	O
C	O
and	O
discourse	O
element	O
representations	O
E	O
=	O
{	O
e	O
i	O
}	O
.	O
Element	O
Self	O
-	O
Attention	O
(	O
ElemSA	O
)	O
:	O
ElemSA	O
models	O
relations	O
among	O
discourse	O
elements	O
.	O
We	O
use	O
E	O
to	O
get	O
Q	O
and	O
K	O
,	O
Q	O
=	O
EW	O
Q	O
,	O
K	B-HyperparameterName
=	I-HyperparameterName
EW	O
K	O
,	O
where	O
W	O
Q	O
,	O
W	O
K	O
R	O
d×d	O
k	O
.	O
We	O
do	O
not	O
use	O
the	O
normalized	O
attention	O
vectors	O
as	O
shown	O
in	O
Equation	O
7to	O
capture	O
relative	O
relatedness	O
.	O
Instead	O
,	O
we	O
use	O
α	B-HyperparameterName
e	O
=	O
tanh	O
(	O
QK	O
T	O
√	O
d	O
k	O
)	O
as	O
attention	O
vectors	O
.	O
Content	O
Self	O
-	O
Attention	O
(	O
ContSA	O
)	O
:	O
ContSA	O
explores	O
content	O
relatedness	O
to	O
model	O
sentence	O
interactions	O
.	O
Similarly	O
to	O
ElemSA	O
,	O
we	O
use	O
the	O
sentence	O
semantic	O
representations	O
C	O
to	O
compute	O
the	O
ContSA	O
vector	O
α	B-HyperparameterName
c	O
.	O
The	O
parameters	O
are	O
independent	O
from	O
ElemSA	O
.	O
Adaptive	O
Maxpooling	O
Different	O
essays	O
have	O
different	O
number	O
of	O
sentences	O
.	O
To	O
have	O
a	O
fixed	O
-	O
length	O
attention	O
vector	O
,	O
we	O
borrow	O
the	O
idea	O
of	O
spatial	B-MethodName
pyramid	I-MethodName
pooling	I-MethodName
from	O
image	O
processing	O
(	O
He	O
et	O
al	O
,	O
2015	O
)	O
.	O
It	O
can	O
maintain	O
relatedness	O
information	O
by	O
maxpooling	O
α	B-HyperparameterName
e	O
and	O
α	B-HyperparameterName
c	O
in	O
local	O
bins	O
.	O
These	O
bins	O
have	O
sizes	O
proportional	O
to	O
the	O
number	O
of	O
an	O
essay	O
's	O
sentences	O
so	O
that	O
the	O
number	O
of	O
bins	O
is	O
fixed	O
regardless	O
of	O
the	O
essay	O
length	O
.	O
We	O
set	O
the	O
number	O
of	O
bins	O
to	O
1	O
,	O
2	O
,	O
4	O
and	O
8	O
,	O
respectively	O
.	O
The	O
resulted	O
representations	O
can	O
be	O
seen	O
as	O
descriptions	O
of	O
the	O
relatedness	O
of	O
a	O
sentence	O
to	O
different	O
zones	O
of	O
its	O
essay	O
.	O
These	O
representations	O
are	O
concatenated	O
so	O
that	O
the	O
dimension	O
of	O
the	O
pooled	O
attention	O
vectors	O
α	B-HyperparameterName
c	O
,	O
α	B-HyperparameterName
e	O
is	O
1	O
+	O
2	O
+	O
4	O
+	O
8=15	O
.	O
Finally	O
,	O
the	O
prediction	O
is	O
made	O
according	O
to	O
Y	O
=	O
softmax	B-MethodName
linear	O
(	O
[	O
α	B-HyperparameterName
e	O
;	O
α	B-HyperparameterName
c	O
;	O
E	O
]	O
)	O
,	O
(	O
8	O
)	O
where	O
α	B-HyperparameterName
c	O
,	O
α	B-HyperparameterName
e	O
and	O
E	O
are	O
concatenated	O
.	O

The	O
thesis	O
express	O
the	O
central	O
claim	O
of	O
an	O
author	O
with	O
respect	O
to	O
the	O
essay	O
's	O
topic	O
.	O
Main	O
Idea	O
The	O
ideas	O
establish	O
foundational	O
ideas	O
or	O
aspects	O
that	O
are	O
related	O
to	O
the	O
thesis	O
.	O
Evidence	O
The	O
evidence	O
elements	O
provide	O
examples	O
or	O
other	O
evidence	O
that	O
are	O
used	O
to	O
support	O
main	O
ideas	O
and	O
thesis	O
.	O
Elaboration	O
The	O
elaboration	O
elements	O
further	O
explain	O
main	O
ideas	O
or	O
provide	O
reasons	O
,	O
but	O
contain	O
no	O
examples	O
or	O
other	O
evidence	O
.	O
Conclusion	O
The	O
conclusion	O
sentence	O
is	O
the	O
extension	O
of	O
the	O
central	O
argument	O
,	O
summarizes	O
the	O
full	O
text	O
,	O
and	O
echos	O
the	O
thesis	O
of	O
the	O
essay	O
.	O
Other	O
Other	O
elements	O
refer	O
to	O
the	O
ones	O
that	O
do	O
not	O
match	O
the	O
above	O
classes	O
.	O
The	O
dataset	O
has	O
1	O
,	O
230	O
argumentative	O
essays	O
written	O
by	O
high	O
school	O
students	O
,	O
covering	O
diverse	O
topics	O
.	O
These	O
essays	O
were	O
collected	O
from	O
a	O
website	O
LeleKetang	O
.	O
1	O
We	O
asked	O
two	O
annotators	O
from	O
the	O
literal	O
arts	O
college	O
to	O
assign	O
discourse	O
elements	O
to	O
sentences	O
from	O
these	O
essays	O
according	O
to	O
a	O
manual	O
.	O
The	O
annotators	O
discussed	O
to	O
reach	O
a	O
consensus	O
and	O
refined	O
the	O
manual	O
for	O
several	O
rounds	O
.	O
We	O
use	O
one	O
annotator	O
's	O
annotation	O
as	O
the	O
gold	O
answer	O
,	O
and	O
the	O
other	O
's	O
annotation	O
as	O
the	O
prediction	O
,	O
and	O
compute	O
the	O
F1	B-MetricName
scores	O
to	O
measure	O
the	O
agreement	O
,	O
which	O
is	O
shown	O
in	O
Figure	O
3	O
.	O
Table	O
1	O
shows	O
the	O
basic	O
statistics	O
of	O
the	O
dataset	O
.	O
The	O
distribution	O
of	O
discourse	O
elements	O
is	O
imbalanced	O
.	O
Elaboration	O
and	O
evidence	O
sentences	O
are	O
[	O
To	O
conclude	O
,	O
art	O
could	O
play	O
an	O
active	O
role	O
in	O
improving	O
the	O
quality	O
of	O
people	O
's	O
lives	O
,	O
]	O
s	O
1	O
[	O
but	O
I	O
think	O
that	O
governments	O
should	O
attach	O
heavier	O
weight	O
to	O
other	O
social	O
issues	O
such	O
as	O
education	O
and	O
housing	O
needs	O
]	O
s	O
2	O
[	O
because	O
those	O
are	O
the	O
most	O
essential	O
ways	O
enable	O
to	O
make	O
people	O
a	O
decent	O
life	O
.	O
]	O
s	O
3	O
.	O
many	O
more	O
than	O
thesis	O
and	O
main	O
idea	O
sentences	O
.	O
The	O
type	O
of	O
other	O
sentence	O
accounts	O
for	O
a	O
very	O
small	O
percentage	O
of	O
the	O
dataset	O
.	O
The	O
test	O
dataset	O
is	O
10	O
%	O
of	O
the	O
whole	O
dataset	O
.	O

The	O
max	O
length	O
of	O
sentences	O
is	O
set	O
to	O
40	O
words	O
.	O
Sentences	O
are	O
padded	O
or	O
truncated	O
according	O
to	O
this	O
length	O
.	O
The	O
Tencent	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
(	O
Song	O
et	O
al	O
,	O
2018	O
)	O
were	O
used	O
for	O
experiments	O
on	O
the	O
Chinese	O
dataset	O
.	O
The	O
dimension	O
of	O
word	B-TaskName
embeddings	I-TaskName
is	O
200	O
.	O
The	O
Bert	O
tokenizer	O
and	O
embeddings	O
were	O
used	O
for	O
experiments	O
on	O
the	O
English	O
dataset	O
.	O
The	O
dimension	O
of	O
all	O
the	O
BiLSTM	B-MethodName
hidden	O
layers	O
is	O
256	O
on	O
Chinese	O
dataset	O
,	O
and	O
128	O
on	O
English	O
dataset	O
.	O
So	O
is	O
the	O
dimension	O
of	O
d	O
k	O
.	O
The	O
dimension	O
of	O
the	O
attention	O
vectors	O
is	O
15	O
.	O
The	O
optimizer	B-HyperparameterName
is	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
(	O
SGD	B-MethodName
)	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.1	O
.	O
The	O
best	O
models	O
were	O
selected	O
for	O
all	O
settings	O
based	O
on	O
the	O
results	O
on	O
the	O
validation	O
data	O
,	O
which	O
is	O
10	O
%	O
of	O
the	O
training	O
data	O
.	O
We	O
use	O
accuracy	B-MetricName
(	O
Acc	B-MetricName
.	O
)	O
and	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
as	O
evaluation	O
metrics	O
.	O

Table	O
4	O
shows	O
the	O
performance	O
of	O
the	O
baselines	O
and	O
DiSA	O
.	O
We	O
can	O
see	O
that	O
HBiLSTM	O
performs	O
even	O
worse	O
than	O
feature	O
-	O
based	O
approach	O
.	O
HBiL	O
-	O
STM	O
has	O
a	O
low	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
score	O
,	O
indicating	O
that	O
it	O
has	O
difficulties	O
in	O
identifying	O
particular	O
discourse	O
elements	O
.	O
The	O
two	O
end	O
-	O
to	O
-	O
end	O
models	O
do	O
not	O
consider	O
position	O
information	O
and	O
interactions	O
among	O
sentences	O
.	O
The	O
performance	O
of	O
BERT	B-MethodName
is	O
worse	O
than	O
HBiLSTM	O
.	O
This	O
verifies	O
that	O
sequence	O
modeling	O
is	O
more	O
proper	O
than	O
single	O
sentence	B-TaskName
classification	I-TaskName
for	O
this	O
task	O
.	O
DiSA	O
achieves	O
the	O
best	O
performance	O
on	O
all	O
metrics	O
,	O
with	O
a	O
large	O
improvement	O
compared	O
with	O
the	O
baselines	O
.	O
Figure	O
3	O
further	O
illustrates	O
system	O
performance	O
on	O
identifying	O
specific	O
discourse	O
elements	O
.	O
The	O
human	O
performance	O
is	O
also	O
measured	O
by	O
considering	O
one	O
annotator	O
's	O
annotation	O
as	O
the	O
answer	O
,	O
and	O
the	O
other	O
one	O
's	O
as	O
the	O
prediction	O
.	O
The	O
discourse	O
elements	O
that	O
HBiLSTM	O
is	O
unable	O
to	O
accurately	O
identify	O
are	O
thesis	O
and	O
main	O
idea	O
.	O
Despite	O
their	O
importance	O
for	O
understanding	O
a	O
text	O
,	O
their	O
scale	O
is	O
obviously	O
smaller	O
than	O
other	O
discourse	O
elements	O
,	O
which	O
may	O
bring	O
in	O
obstacles	O
for	O
datadriven	O
approaches	O
.	O
Feature	O
-	O
based	O
method	O
performs	O
better	O
than	O
HBiLSTM	O
on	O
identifying	O
thesis	O
and	O
main	O
idea	O
.	O
But	O
it	O
heavily	O
relies	O
on	O
feature	O
-	O
engineering	O
such	O
as	O
manually	O
collected	O
discourse	O
markers	O
and	O
cue	O
words	O
.	O
It	O
does	O
not	O
perform	O
well	O
on	O
identifying	O
evidence	O
due	O
to	O
the	O
difficulties	O
in	O
designing	O
related	O
features	O
.	O
DiSA	O
is	O
also	O
an	O
end	O
-	O
to	O
-	O
end	O
model	O
the	O
same	O
as	O
HBiLSTM	O
but	O
performs	O
much	O
better	O
.	O
We	O
will	O
discuss	O
the	O
impacts	O
of	O
positional	O
encoding	O
and	O
intersentence	O
attention	O
in	O
Section	O
6.3.2	O
and	O
6.3.3	O
.	O
Compared	O
with	O
the	O
feature	O
-	O
based	O
method	O
,	O
DiSA	O
has	O
comparable	O
performance	O
on	O
identifying	O
thesis	O
but	O
has	O
superior	O
results	O
on	O
identifying	O
main	O
idea	O
(	O
9	O
%	O
higher	O
in	O
F1	B-MetricName
score	I-MetricName
)	O
and	O
evidence	O
(	O
21	O
%	O
higher	O
in	O
F1	B-MetricName
score	I-MetricName
)	O
.	O

This	O
part	O
investigates	O
the	O
effect	O
of	O
sentence	O
positional	O
encodings	O
.	O
We	O
compare	O
our	O
relative	O
sentence	O
positional	O
encoding	O
(	O
relativeSPE	O
)	O
with	O
two	O
other	O
encoding	O
strategies	O
which	O
are	O
previously	O
used	O
for	O
word	O
sequences	O
.	O
Sinusoidal	O
indicates	O
the	O
sinusoidal	O
positional	O
encoding	O
which	O
is	O
used	O
in	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
PosEmbedding	O
uses	O
a	O
distributed	O
vector	O
to	O
represent	O
an	O
absolute	O
position	O
.	O
The	O
position	O
embeddings	O
are	O
learned	O
during	O
training	O
.	O
Each	O
of	O
the	O
above	O
three	O
strategies	O
is	O
applied	O
for	O
modeling	O
global	O
position	O
,	O
local	O
position	O
and	O
paragraph	O
position	O
,	O
which	O
are	O
then	O
combined	O
according	O
to	O
Equation	O
5	O
.	O
Table	O
5	O
lists	O
the	O
results	O
of	O
using	O
different	O
SPEs	O
and	O
modeling	O
different	O
positions	O
.	O
RelativeSPE	O
performs	O
best	O
with	O
improvements	O
of	O
2	O
-	O
3	O
%	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
score	O
compared	O
with	O
Sinusoidal	O
and	O
PosEmbedding	O
.	O
Without	O
SPE	O
,	O
the	O
metrics	O
drop	O
at	O
least	O
6.2	O
%	O
compared	O
with	O
using	O
any	O
SPE	O
strategy	O
,	O
and	O
8.6	O
%	O
compared	O
with	O
relativeSPE	O
.	O
If	O
we	O
explicitly	O
add	O
only	O
pos	O
global	O
,	O
the	O
results	O
even	O
decrease	O
.	O
Perhaps	O
recurrent	O
neural	O
networks	O
such	O
as	O
LSTM	B-MethodName
naturally	O
capture	O
sequential	O
positional	O
information	O
.	O
However	O
,	O
encoding	O
paragraph	O
position	O
(	O
pos	O
para	O
)	O
and	O
local	O
position	O
(	O
pos	O
local	O
)	O
largely	O
improves	O
the	O
performance	O
.	O
This	O
indicates	O
that	O
proper	O
structural	O
positional	O
encodings	O
can	O
exploit	O
richer	O
discourse	O
structures	O
than	O
sequential	O
structures	O
.	O

Table	O
6	O
shows	O
the	O
effects	O
of	O
removing	O
intersentence	O
attention	O
(	O
ISA	O
)	O
components	O
from	O
DiSA	O
.	O
We	O
can	O
see	O
that	O
both	O
ElemSA	O
and	O
ContSA	O
can	O
make	O
contributions	O
,	O
and	O
ElemSA	O
seems	O
to	O
have	O
a	O
larger	O
effect	O
on	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
score	O
.	O
Removing	O
ISA	O
,	O
the	O
accuracy	B-MetricName
and	O
the	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
score	O
decreases	O
1.8	O
%	O
and	O
2.2	O
%	O
.	O
Remind	O
that	O
ISA	O
uses	O
attention	O
vectors	O
as	O
representations	O
rather	O
than	O
the	O
final	O
output	O
αV	O
in	O
the	O
self	O
-	O
attention	O
module	O
.	O
The	O
result	O
is	O
not	O
good	O
.	O
This	O
indicates	O
that	O
semantic	O
relation	O
among	O
sentences	O
is	O
more	O
important	O
for	O
DEI	O
than	O
the	O
specific	O
meaning	O
of	O
sentences	O
.	O
We	O
further	O
analyze	O
ISA	O
's	O
impact	O
on	O
specific	O
discourse	O
elements	O
.	O
As	O
shown	O
in	O
Table	O
7	O
,	O
ISA	O
affects	O
the	O
identification	O
of	O
the	O
minority	O
discourse	O
element	O
thesis	O
most	O
.	O
It	O
also	O
benefits	O
identifying	O
evidence	O
which	O
is	O
not	O
a	O
minority	O
discourse	O
element	O
.	O
Thesis	O
sentences	O
often	O
relate	O
to	O
other	O
sentences	O
from	O
different	O
essay	O
zones	O
,	O
while	O
evidence	O
sentences	O
mainly	O
provide	O
facts	O
or	O
examples	O
so	O
they	O
often	O
relate	O
to	O
local	O
context	O
in	O
content	O
.	O
ISA	O
helps	O
capture	O
such	O
patterns	O
.	O
The	O
performance	O
on	O
other	O
types	O
also	O
increases	O
with	O
different	O
degrees	O
.	O
Anyway	O
,	O
ISA	O
provides	O
a	O
way	O
to	O
build	O
useful	O
representations	O
by	O
exploiting	O
relations	O
between	O
sentences	O
in	O
the	O
same	O
text	O
without	O
any	O
extra	O
burden	O
.	O

Table	O
8	O
and	O
Table	O
9	O
show	O
main	O
experimental	O
results	O
on	O
the	O
English	O
dataset	O
.	O
The	O
second	O
column	O
of	O
Table	O
8	O
shows	O
the	O
results	O
on	O
distinguishing	O
four	O
component	O
types	O
(	O
i.e.	O
,	O
major	O
claim	O
,	O
claim	O
,	O
premise	O
,	O
other	O
)	O
.	O
DiSA	O
outperforms	O
the	O
baselines	O
with	O
a	O
large	O
margin	O
on	O
both	O
accuracy	B-MetricName
and	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
.	O
Again	O
,	O
removing	O
SPE	O
leads	O
to	O
a	O
large	O
performance	O
decrease	O
.	O
,	O
where	O
Joint	O
-	O
Best	O
incorporates	O
relation	O
identification	O
as	O
an	O
auxiliary	O
task	O
.	O
The	O
third	O
column	O
of	O
Table	O
8	O
shows	O
the	O
comparison	O
to	O
the	O
best	O
results	O
from	O
.	O
DiSA	O
does	O
not	O
perform	O
competitively	O
based	O
on	O
the	O
distributed	O
representation	O
only	O
,	O
because	O
the	O
baseline	O
uses	O
some	O
strong	O
hand	O
-	O
crafted	O
features	O
,	O
especially	O
the	O
component	O
position	O
features	O
,	O
which	O
rely	O
on	O
the	O
correct	O
argument	O
component	O
information	O
.	O
Thus	O
we	O
build	O
a	O
feature	O
vector	O
by	O
incorporating	O
the	O
indicator	O
features	O
and	O
a	O
component	O
position	O
feature	O
:	O
number	O
of	O
preceding	O
and	O
following	O
components	O
in	O
paragraph	O
,	O
out	O
of	O
8	O
categories	O
of	O
features	O
introduced	O
in	O
.	O
The	O
vector	O
is	O
concatenated	O
with	O
the	O
distributed	O
representation	O
.	O
This	O
combination	O
obtains	O
improvements	O
,	O
outperforms	O
Single	O
-	O
Best	O
results	O
,	O
and	O
achieves	O
close	O
performance	O
compared	O
with	O
Joint	O
-	O
Best	O
,	O
which	O
considers	O
argumentative	O
relation	O
identification	O
as	O
an	O
auxiliary	O
task	O
.	O
We	O
also	O
attempt	O
to	O
apply	O
the	O
same	O
strategy	O
for	O
the	O
Chinese	O
task	O
.	O
But	O
the	O
improvement	O
is	O
negligible	O
.	O
The	O
reason	O
may	O
be	O
that	O
the	O
indicator	O
phrases	O
used	O
in	O
Chinese	O
essays	O
is	O
much	O
less	O
than	O
in	O
English	O
essays	O
.	O
The	O
English	O
dataset	O
heavily	O
relies	O
on	O
phrases	O
signaling	O
beliefs	O
or	O
argumentative	O
discourse	O
connectors	O
.	O
Table	O
9	O
shows	O
the	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
scores	O
of	O
DiSA	O
on	O
identifying	O
specific	O
argument	O
components	O
.	O
Without	O
the	O
ISA	O
module	O
,	O
the	O
identification	O
of	O
major	O
claims	O
and	O
claims	O
would	O
decline	O
by	O
3	O
%	O
and	O
1.4	O
%	O
absolute	O
F1	B-MetricName
score	I-MetricName
,	O
respectively	O
.	O
This	O
is	O
consistent	O
with	O
the	O
experimental	O
results	O
on	O
the	O
Chinese	O
dataset	O
.	O
As	O
a	O
result	O
,	O
the	O
effectiveness	O
of	O
the	O
SPE	O
and	O
ISA	O
can	O
be	O
verified	O
on	O
both	O
the	O
Chinese	O
and	O
the	O
English	O
datasets	O
.	O

Text	B-TaskName
classification	I-TaskName
assumes	O
a	O
dataset	O
D	O
=	O
{	O
x	O
i	O
,	O
y	O
i	O
}	O
N	O
i=1	O
which	O
associates	O
an	O
input	O
text	O
x	O
i	O
to	O
its	O
corresponding	O
class	O
label	O
y	O
i	O
.	O
We	O
will	O
omit	O
the	O
index	O
i	O
when	O
dealing	O
with	O
a	O
single	O
input	O
sample	O
.	O
Let	O
the	O
input	O
sequence	O
of	O
word	O
features	O
(	O
e.g.	O
,	O
embeddings	O
)	O
be	O
denoted	O
as	O
x	O
=	O
{	O
w	O
t	O
}	O
T	O
t=1	O
,	O
where	O
T	O
is	O
the	O
length	O
of	O
the	O
sequence	O
.	O
The	O
sequence	O
of	O
hidden	O
states	O
produced	O
by	O
an	O
encoding	O
function	O
f	O
φ	O
with	O
learnable	O
parameters	O
φ	O
is	O
then	O
h	O
=	O
{	O
h	O
t	O
}	O
T	O
t=1	O
.	O
Formally	O
,	O
f	O
φ	O
:	O
x	O
(	O
h	O
,	O
α	B-HyperparameterName
)	O
,	O
where	O
attention	O
weightsα	O
=	O
{	O
α	B-HyperparameterName
t	O
}	O
T	O
t=1	O
indicate	O
a	O
probability	O
distribution	O
over	O
the	O
hidden	O
states	O
(	O
Zou	O
et	O
al	O
,	O
2018	O
;	O
Yang	O
et	O
al	O
,	O
2016	O
)	O
.	O
Finally	O
,	O
the	O
hidden	O
representations	O
are	O
fed	O
into	O
a	O
function	O
g	O
θ	B-HyperparameterName
:	O
(	O
h	O
,	O
α	B-HyperparameterName
)	O
ŷ	O
with	O
learnable	O
parameters	O
θ	B-HyperparameterName
and	O
a	O
softmax	B-MethodName
layer	O
that	O
predicts	O
the	O
probabilitiesŷ	O
over	O
classes	O
:	O
y	O
=	O
Softmax	B-MethodName
(	O
W	O
h	O
+	O
b	O
)	O
,	O
θ	B-HyperparameterName
=	O
{	O
W	O
,	O
b	O
}	O
(	O
1	O
)	O
whereh	O
=	O
h	O
t	O
hαt	O
h	O
t	O
and	O
Softmax	B-MethodName
(	O
z	O
i	O
)	O
=	O
e	O
z	O
i	O
/	O
j	O
e	O
z	O
j	O
.	O
The	O
parameters	O
φ	O
and	O
θ	B-HyperparameterName
are	O
trained	O
to	O
minimize	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
L	O
task	O
(	O
ŷ	O
,	O
y	O
)	O
between	O
the	O
predicted	O
labelŷ	O
and	O
the	O
ground	O
-	O
truth	O
label	O
y.	O

Attention	O
can	O
be	O
treated	O
as	O
output	O
variables	O
,	O
so	O
that	O
humans	O
can	O
supervise	O
.	O
Given	O
an	O
input	O
sample	O
x	O
,	O
let	O
α	B-HyperparameterName
andα	O
be	O
the	O
attention	O
labels	O
(	O
provided	O
by	O
human	O
annotators	O
)	O
and	O
the	O
trained	O
attention	O
weights	O
.	O
Then	O
,	O
the	O
loss	B-MetricName
for	O
attention	O
supervision	O
is	O
defined	O
as	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
L	O
att	O
(	O
α	B-HyperparameterName
,	O
α	B-HyperparameterName
)	O
betweenα	O
and	O
α	B-HyperparameterName
.	O
Finally	O
,	O
the	O
parameters	O
of	O
the	O
text	B-TaskName
classification	I-TaskName
network	O
with	O
attention	O
supervision	O
are	O
trained	O
to	O
minimize	O
both	O
loss	B-MetricName
terms	O
together	O
as	O
follows	O
:	O
L	O
=	O
L	O
task	O
(	O
ŷ	O
,	O
y	O
)	O
+	O
µ	O
L	O
att	O
(	O
α	B-HyperparameterName
,	O
α	B-HyperparameterName
)	O
(	O
2	O
)	O
where	O
µ	O
is	O
a	O
preference	O
weight	O
.	O
Requiring	O
humans	O
to	O
explicitly	O
annotate	O
soft	O
labels	O
α	B-HyperparameterName
has	O
been	O
considered	O
unrealistic	O
(	O
Barrett	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
often	O
delegated	O
to	O
implicit	O
signals	O
such	O
as	O
eye	O
gaze	O
.	O
As	O
an	O
alternative	O
to	O
asking	O
humans	O
to	O
annotate	O
,	O
important	O
words	O
for	O
the	O
given	O
sample	O
and	O
class	O
label	O
have	O
been	O
typically	O
annotated	O
as	O
rationale	O
(	O
Bao	O
et	O
al	O
,	O
2018	O
;	O
Zhao	O
et	O
al	O
,	O
2018	O
)	O
.	O
Formally	O
,	O
given	O
an	O
input	O
sample	O
x	O
and	O
its	O
class	O
label	O
y	O
,	O
let	O
A	O
{	O
0	B-DatasetName
,	O
1	O
}	O
T	O
be	O
a	O
binary	O
vector	O
of	O
selecting	O
words	O
in	O
x	O
,	O
i.e.	O
,	O
∀w	O
t	O
x	O
:	O
A	O
(	O
w	O
t	O
)	O
{	O
0	B-DatasetName
,	O
1	O
}	O
.	O
Then	O
,	O
we	O
convert	O
the	O
attention	O
annotation	O
A	O
into	O
a	O
soft	O
distribution	O
of	O
target	O
attention	O
labels	O
α	B-HyperparameterName
using	O
softmax	B-MethodName
:	O
α	B-HyperparameterName
t	O
=	O
exp	O
(	O
λ	O
A	O
(	O
w	O
t	O
)	O
)	O
T	O
t	O
=	O
1	O
exp	O
(	O
λ	O
A	O
(	O
w	O
t	O
)	O
)	O
(	O
3	O
)	O
where	O
λ	O
is	O
a	O
positive	O
hyper	O
-	O
parameter	O
that	O
controls	O
the	O
variance	O
of	O
scores	O
:	O
when	O
λ	O
increases	O
,	O
the	O
distribution	O
of	O
α	B-HyperparameterName
becomes	O
more	O
skewed	O
,	O
guiding	O
to	O
attend	O
a	O
few	O
of	O
more	O
important	O
words	O
.	O
To	O
illustrate	O
a	O
rationale	O
,	O
when	O
given	O
the	O
aforementioned	O
review	O
sample	O
in	O
Sec	O
.	O
1	O
,	O
possible	O
annotations	O
for	O
the	O
negative	O
label	O
are	O
either	O
"	O
this	O
place	O
is	O
small	O
and	O
crowded	O
but	O
the	O
service	O
is	O
quick	O
"	O
or	O
"	O
this	O
place	O
is	O
small	O
and	O
crowded	O
but	O
the	O
service	O
is	O
quick	O
"	O
,	O
where	O
the	O
underlines	O
indicate	O
the	O
hard	O
selection	O
by	O
human	O
.	O
Then	O
,	O
we	O
can	O
translate	O
them	O
into	O
the	O
sample	O
-	O
level	O
anno	O
-	O
tation	O
A	O
=	O
[	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
or	O
A	O
=	O
[	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
1	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
.	O

Our	O
key	O
idea	O
is	O
to	O
leverage	O
causal	O
signals	O
(	O
Johansson	O
et	O
al	O
,	O
2016	O
)	O
from	O
human	O
annotation	O
A	O
(	O
or	O
attention	O
labels	O
α	B-HyperparameterName
)	O
of	O
an	O
input	O
sample	O
x	O
to	O
its	O
corresponding	O
model	O
predictionŷ	O
.	O
More	O
specifically	O
,	O
we	O
test	O
whether	O
two	O
different	O
attentions	O
(	O
one	O
is	O
original	O
and	O
the	O
other	O
is	O
counterfactual	O
)	O
on	O
the	O
same	O
input	O
sample	O
x	O
lead	O
to	O
different	O
prediction	O
resultŝ	O
y.	O
If	O
high	O
(	O
original	O
)	O
and	O
low	O
(	O
counterfactual	O
)	O
attention	O
weights	O
for	O
an	O
word	O
w	O
t	O
yield	O
the	O
same	O
(	O
or	O
very	O
similar	O
)	O
prediction	O
,	O
it	O
provides	O
evidence	O
to	O
edit	O
the	O
importance	O
of	O
word	O
w	O
t	O
in	O
A	O
into	O
a	O
lower	O
value	O
.	O
Formally	O
,	O
letα	O
andᾱ	O
be	O
the	O
original	O
and	O
counterfactual	O
attention	O
weights	O
,	O
respectively	O
,	O
and	O
let	O
y	O
andȳ	O
t	O
be	O
the	O
original	O
prediction	O
and	O
its	O
counterfactual	O
prediction	O
with	O
attention	O
change	O
(	O
i.e.	O
,	O
from	O
α	B-HyperparameterName
t	O
toᾱ	O
t	O
)	O
on	O
w	O
t	O
x	O
,	O
respectively	O
.	O
Then	O
,	O
knowing	O
the	O
quantity	O
|	O
ŷ	O
−ȳ	O
t	O
|	O
,	O
measured	O
as	O
the	O
individualized	O
treatment	O
effect	O
(	O
ITE	O
)	O
,	O
enables	O
measuring	O
how	O
Algorithm	O
1	O
SANA	O
Input	O
:	O
Training	O
dataset	O
D	O
,	O
Task	O
-	O
level	O
annotation	O
A	O
Output	O
:	O
Model	O
parameters	O
{	O
φ	O
,	O
θ	B-HyperparameterName
}	O
Initialize	O
attention	O
labels	O
α	B-HyperparameterName
from	O
A	O
Using	O
Eq	O
(	O
3	O
)	O
{	O
φ	O
,	O
θ	B-HyperparameterName
}	O
argmin	O
φ	O
,	O
θ	B-HyperparameterName
L	O
(	O
D	O
,	O
α	B-HyperparameterName
;	O
φ	O
,	O
θ	B-HyperparameterName
)	O
Using	O
Eq	O
(	O
2	O
)	O
for	O
z	O
=	O
1	O
to	O
z	O
max	O
do	O
for	O
each	O
(	O
x	O
,	O
y	O
)	O
D	O
do	O
h	O
,	O
α	B-HyperparameterName
f	O
φ	O
(	O
x	O
)	O
y	O
g	O
θ	B-HyperparameterName
(	O
h	O
,	O
α	B-HyperparameterName
)	O
for	O
each	O
w	O
t	O
x	O
do	O
if	O
A	O
(	O
w	O
t	O
)	O
>	O
0	B-DatasetName
then	O
α	B-HyperparameterName
Counterfactuals	O
(	O
α	B-HyperparameterName
,	O
w	O
t	O
)	O
y	O
t	O
g	O
θ	B-HyperparameterName
(	O
h	O
,	O
ᾱ	O
)	O
if	O
T	O
V	O
D	O
(	O
ŷ	O
,	O
ȳ	O
t	O
)	O
<	O
then	O
A	O
(	O
w	O
t	O
)	O
γ	B-HyperparameterName
A	O
(	O
w	O
t	O
)	O
end	O
end	O
end	O
end	O
λ	O
γ	B-HyperparameterName
−1	O
λ	O
In	O
Eq	O
(	O
3	O
)	O
Update	O
attention	O
labels	O
α	B-HyperparameterName
from	O
A	O
Using	O
Eq	O
(	O
3	O
)	O
{	O
φ	O
,	O
θ	B-HyperparameterName
}	O
argmin	O
φ	O
,	O
θ	B-HyperparameterName
L	O
(	O
D	O
,	O
α	B-HyperparameterName
;	O
φ	O
,	O
θ	B-HyperparameterName
)	O
Using	O
Eq	O
(	O
2	O
)	O
end	O
return	O
{	O
φ	O
,	O
θ	B-HyperparameterName
}	O
much	O
the	O
word	O
w	O
t	O
contributes	O
to	O
the	O
original	O
prediction	O
via	O
attention	O
mechanism	O
.	O
For	O
this	O
measurement	O
,	O
we	O
adopt	O
the	O
Total	O
Variance	O
Distance	O
(	O
Jain	O
and	O
Wallace	O
,	O
2019	O
)	O
between	O
the	O
two	O
predictions	O
,	O
which	O
is	O
defined	O
as	O
follows	O
:	O
T	O
V	O
D	O
(	O
ŷ	O
,	O
ȳ	O
t	O
)	O
=	O
1	O
2	O
C	O
c=1	O
|	O
ŷ	O
c	O
−ȳ	O
c	O
t	O
|	O
(	O
4	O
)	O
where	O
c	O
is	O
the	O
class	O
index	O
.	O
If	O
T	O
V	O
D	O
value	O
is	O
too	O
low	O
,	O
we	O
can	O
give	O
a	O
penalty	O
by	O
decaying	O
the	O
human	O
annotation	O
A	O
(	O
w	O
t	O
)	O
with	O
a	O
factor	O
of	O
γ	B-HyperparameterName
,	O
which	O
we	O
empirically	O
set	O
as	O
0.5	O
,	O
to	O
update	O
the	O
attention	O
labels	O
.	O

Based	O
on	O
TVD	O
,	O
we	O
propose	O
a	O
simple	O
yet	O
effective	O
approach	O
,	O
Sample	O
-	O
level	O
AttentioN	O
Adaptation	O
(	O
SANA	O
)	O
,	O
to	O
derive	O
the	O
sample	O
-	O
level	O
machine	O
attention	O
from	O
the	O
task	O
-	O
level	O
human	O
annotation	O
.	O
As	O
described	O
in	O
Alg	O
.	O
1	O
,	O
SANA	O
starts	O
with	O
the	O
classification	O
model	O
trained	O
with	O
the	O
initial	O
attention	O
labels	O
α	B-HyperparameterName
.	O
Based	O
on	O
φ	O
and	O
θ	B-HyperparameterName
,	O
we	O
run	O
the	O
classification	O
inference	O
several	O
times	O
for	O
an	O
input	O
sample	O
:	O
one	O
for	O
obtaining	O
the	O
original	O
attention	O
weightsα	O
and	O
the	O
others	O
for	O
counterfactual	O
attention	O
weights	O
α	B-HyperparameterName
.	O
More	O
specifically	O
,	O
we	O
first	O
store	O
the	O
hidden	O
representations	O
h	O
and	O
the	O
attention	O
weightsα	O
from	O
f	O
φ	O
,	O
and	O
the	O
original	O
predictionŷ	O
.	O
Then	O
,	O
for	O
each	O
word	O
w	O
t	O
,	O
Counterfactuals	O
returns	O
the	O
counterfactual	O
attention	O
weightsᾱ	O
,	O
by	O
1	O
)	O
copyingα	O
but	O
2	O
)	O
assigning	O
zero	O
to	O
the	O
t	O
-	O
th	O
dimension	O
and	O
3	O
)	O
renormalizing	O
as	O
probability	O
distribution	O
,	O
and	O
we	O
obtain	O
its	O
corresponding	O
prediction	O
resultȳ	O
t	O
by	O
re	O
-	O
using	O
h.	O
Note	O
that	O
,	O
since	O
the	O
hidden	O
representation	O
at	O
time	O
step	O
t	O
contextualizes	O
a	O
word	O
w	O
t	O
with	O
surrounding	O
words	O
,	O
we	O
adopt	O
perturbing	O
only	O
single	O
words	O
in	O
SANA	O
,	O
not	O
multiple	O
words	O
at	O
the	O
same	O
time	O
,	O
also	O
enjoying	O
the	O
computational	O
advantage	O
.	O
Finally	O
,	O
based	O
onŷ	O
andȳ	O
t	O
,	O
as	O
defined	O
in	O
Eq	O
(	O
4	O
)	O
,	O
we	O
compute	O
T	O
V	O
D	O
and	O
update	O
the	O
human	O
annotation	O
A	O
by	O
threshold	O
and	O
decay	O
ratio	O
γ	B-HyperparameterName
.	O
Once	O
an	O
iteration	O
1	O
is	O
completed	O
over	O
the	O
whole	O
training	O
corpus	O
,	O
we	O
re	O
-	O
train	O
the	O
network	O
with	O
the	O
updated	O
attention	O
annotation	O
and	O
labels	O
.	O
For	O
the	O
stable	O
update	O
,	O
we	O
observe	O
that	O
increasing	O
the	O
coefficient	O
λ	O
in	O
Eq	O
(	O
3	O
)	O
is	O
crucial	O
,	O
as	O
T	O
V	O
D	O
is	O
not	O
an	O
optimal	O
metric	O
,	O
preventing	O
α	B-HyperparameterName
from	O
being	O
flattened	O
.	O

In	O
an	O
extreme	O
scenario	O
without	O
any	O
human	O
annotator	O
and	O
public	O
resources	O
,	O
inspired	O
by	O
self	O
knowledge	B-MethodName
distillation	I-MethodName
(	O
Furlanello	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
report	O
results	O
for	O
using	O
the	O
attention	O
weights	O
of	O
the	O
unsupervised	O
model	O
as	O
a	O
supervision	O
.	O
Note	O
,	O
however	O
,	O
this	O
is	O
highly	O
unlikely	O
in	O
practice	O
,	O
but	O
reported	O
as	O
a	O
lower	O
bound	O
accuracy	B-MetricName
,	O
when	O
unsupervised	O
attention	O
noise	O
is	O
propagated	O
through	O
distillation	O
supervision	O
.	O
Using	O
SANA	O
is	O
even	O
more	O
critical	O
in	O
this	O
noisy	O
annotation	O
scenario	O
,	O
to	O
denoise	O
attention	O
supervision	O
from	O
counterfactual	O
reasoning	O
,	O
which	O
we	O
empirically	O
analyze	O
this	O
in	O
the	O
subsequent	O
section	O
.	O
1	O
O	O
(	O
|	O
D	O
|	O
T	O
)	O
,	O
where	O
T	O
is	O
the	O
maximum	O
sequence	O
length	O
4	O
Experiment	O
Setup	O

For	O
all	O
datasets	O
,	O
we	O
use	O
skip	O
-	O
gram	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
(	O
official	O
GoogleNews	O
-	O
vectors	O
-	O
negative300	O
)	O
word	B-TaskName
embeddings	I-TaskName
with	O
300	O
dimensions	O
.	O
We	O
use	O
1layered	O
GRU	B-MethodName
for	O
each	O
direction	O
with	O
hidden	O
size	O
of	O
150	O
for	O
both	O
SST2	B-DatasetName
and	O
IMDB	B-DatasetName
,	O
and	O
300	O
for	O
20NG	O
dataset	O
,	O
with	O
g	O
θ	B-HyperparameterName
of	O
300	O
dimension	O
with	O
0.5	O
dropout	O
rate	O
.	O
For	O
attention	O
mechanism	O
,	O
the	O
size	O
of	O
trainable	O
context	O
vector	O
is	O
set	O
to	O
100	O
for	O
SST2	B-DatasetName
and	O
300	O
for	O
IMDB	B-DatasetName
and	O
20NG	O
.	O
For	O
attention	O
supervision	O
,	O
we	O
use	O
the	O
balancing	O
coefficient	O
µ	O
=	O
1.0	O
for	O
SST2	B-DatasetName
and	O
IMDB	B-DatasetName
,	O
and	O
µ	O
=	O
2.0	O
for	O
20NG	O
.	O
Contrary	O
to	O
Zou	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
observe	O
a	O
larger	O
µ	O
is	O
more	O
effective	O
for	O
the	O
smaller	O
dataset	O
.	O
We	O
set	O
the	O
contrasting	O
coefficient	O
λ	O
=	O
3	O
except	O
λ	O
=	O
5	O
for	O
20NG	O
dataset	O
.	O
In	O
Alg	O
.	O
1	O
,	O
we	O
use	O
decay	O
ratio	O
γ	B-HyperparameterName
=	O
2.0	O
and	O
TVD	O
threshold	O
=	O
0.3	O
.	O
In	O
our	O
experiments	O
,	O
the	O
decay	O
ratio	O
is	O
not	O
significantly	O
correlated	O
with	O
the	O
final	O
accuracy	B-MetricName
,	O
but	O
correlated	O
more	O
with	O
the	O
convergence	O
period	O
.	O
Setting	O
γ	B-HyperparameterName
=	O
2.0	O
leads	O
to	O
the	O
reported	O
performance	O
within	O
z	O
max	O
=	O
5	O
.	O
For	O
BERT	B-MethodName
,	O
we	O
train	O
BERT	B-MethodName
-	O
base	O
architecture	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4	O
over	O
3	O
epochs	O
.	O
We	O
used	O
Adam	B-MethodName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
6.25e	O
-	O
5	O
and	O
PiecewiseLinear	O
scheduler	O
.	O
All	O
parameters	O
are	O
optimized	O
until	O
convergence	O
,	O
using	O
Adam	B-MethodName
optimizer	B-HyperparameterName
of	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.001	O
.	O
The	O
learning	O
parameters	O
were	O
chosen	O
by	O
the	O
best	O
performance	O
on	O
the	O
validation	O
set	O
.	O
In	O
Alg	O
.	O
1	O
,	O
the	O
models	O
are	O
additionally	O
fine	O
-	O
tuned	O
over	O
10	O
epochs	O
for	O
each	O
iteration	O
.	O
Note	O
that	O
learning	O
time	O
longer	O
than	O
our	O
setting	O
does	O
not	O
contribute	O
to	O
improving	O
the	O
model	O
accuracy	B-MetricName
.	O

We	O
now	O
proceed	O
to	O
empirically	O
validate	O
the	O
effectiveness	O
of	O
SANA	O
,	O
compared	O
to	O
unsupervised	O
attention	O
,	O
and	O
attention	O
supervision	O
approaches	O
using	O
either	O
task	O
-	O
level	O
or	O
sample	O
-	O
level	O
annotations	O
as	O
baselines	O
(	O
shortly	O
,	O
unsupervised	O
,	O
task	O
-	O
level	O
,	O
and	O
sample	O
)	O
.	O
For	O
task	O
-	O
level	O
annotations	O
(	O
e.g.	O
,	O
in	O
SANA	O
)	O
,	O
we	O
adopt	O
pre	O
-	O
annotated	O
task	O
-	O
level	O
annotations	O
without	O
any	O
additional	O
human	O
efforts	O
:	O
for	O
the	O
two	O
sentiment	O
tasks	O
,	O
we	O
use	O
SentiWordNet	O
(	O
Esuli	O
and	O
Sebastiani	O
,	O
2006	O
)	O
,	O
and	O
for	O
20NG	O
task	O
,	O
we	O
use	O
entities	O
recognized	O
by	O
AllenNLP	O
NER	B-TaskName
(	O
Peters	O
et	O
al	O
,	O
2017	O
)	O
.	O
We	O
thus	O
present	O
the	O
empirical	O
findings	O
for	O
the	O
following	O
four	O
research	O
questions	O
:	O
RQ1	O
:	O
Does	O
SANA	O
improve	O
model	O
accuracy	B-MetricName
?	O
RQ2	O
:	O
Does	O
SANA	O
improve	O
model	O
robustness	O
?	O
RQ3	O
:	O
Is	O
SANA	O
effective	O
for	O
data	O
-	O
scarce	O
cases	O
?	O
RQ4	O
:	O
Does	O
SANA	O
improve	O
attention	O
explainability	O
?	O

The	O
main	O
objective	O
of	O
this	O
work	O
is	O
to	O
improve	O
attention	O
supervisions	O
for	O
the	O
purpose	O
of	O
better	O
text	B-TaskName
classification	I-TaskName
.	O
Thus	O
,	O
we	O
evaluate	O
the	O
three	O
attention	O
methods	O
by	O
their	O
contribution	O
to	O
the	O
classification	O
performance	O
.	O
Tab	O
.	O
2	O
shows	O
the	O
classification	O
accuracy	B-MetricName
for	O
three	O
classification	O
datasets	O
.	O
In	O
the	O
table	O
,	O
we	O
can	O
observe	O
the	O
proposed	O
approach	O
,	O
SANA	O
with	O
task	O
-	O
level	O
annotation	O
,	O
outperforms	O
all	O
baselines	O
in	O
all	O
the	O
datasets	O
.	O
Among	O
the	O
results	O
,	O
SANA	O
achieves	O
the	O
largest	O
improvement	O
over	O
in	O
20NG	O
dataset	O
,	O
which	O
has	O
the	O
smallest	O
training	O
data	O
.	O
This	O
suggests	O
that	O
SANA	O
can	O
also	O
provide	O
effective	O
attention	O
supervisions	O
in	O
data	O
-	O
scarce	O
environments	O
.	O
To	O
discuss	O
this	O
issue	O
further	O
,	O
we	O
will	O
repeat	O
this	O
comparison	O
over	O
the	O
varying	O
size	O
of	O
training	O
data	O
for	O
RQ3	O
.	O
Our	O
study	O
also	O
confirms	O
two	O
additional	O
observations	O
to	O
our	O
advantage	O
-	O
counterfactual	O
1	O
)	O
is	O
effective	O
even	O
in	O
model	O
distillation	O
setting	O
and	O
2	O
)	O
meaningfully	O
contributes	O
to	O
performance	O
gains	O
.	O
More	O
specifically	O
,	O
1	O
)	O
SANA	O
achieves	O
84.35	O
%	O
in	O
SST2	B-DatasetName
dataset	O
which	O
is	O
higher	O
than	O
the	O
distillation	O
only	O
model	O
,	O
but	O
lower	O
than	O
task	O
-	O
level	O
supervised	O
model	O
.	O
2	O
)	O
this	O
model	O
gets	O
88.23	O
%	O
in	O
20NG	O
dataset	O
,	O
which	O
outperforms	O
even	O
task	O
-	O
level	O
supervised	O
model	O
with	O
1.04	O
point	O
gains	O
.	O
This	O
also	O
suggests	O
the	O
limitation	O
of	O
model	O
distillation	O
as	O
supervision	O
signals	O
and	O
supervision	O
by	O
public	O
resources	O
can	O
provide	O
better	O
initial	O
point	O
for	O
SANA	O
than	O
model	O
distillation	O
.	O
Our	O
key	O
contribution	O
is	O
to	O
show	O
zero	O
-	O
cost	O
attention	O
supervision	O
can	O
improve	O
a	O
simple	O
model	O
closer	O
to	O
a	O
highly	O
sophisticated	O
model	O
,	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
requiring	O
more	O
layers	O
and	O
data	O
.	O
This	O
motivates	O
us	O
to	O
supervise	O
attention	O
for	O
BERT	B-MethodName
,	O
though	O
understanding	O
of	O
BERT	B-MethodName
internals	O
,	O
such	O
as	O
(	O
Rogers	O
et	O
al	O
,	O
2020	O
)	O
,	O
is	O
mostly	O
observational	O
at	O
this	O
stage	O
-	O
Intervening	O
with	O
attention	O
would	O
be	O
an	O
interesting	O
future	O
work	O
.	O
Our	O
experimental	O
results	O
show	O
that	O
SANA	O
works	O
well	O
in	O
diverse	O
scenarios	O
,	O
but	O
we	O
observe	O
that	O
the	O
effectiveness	O
is	O
reduced	O
when	O
the	O
length	O
of	O
target	O
text	O
increases	O
(	O
Figure	O
2	O
)	O
or	O
token	O
identifiability	O
decreases	O
(	O
e.g.	O
,	O
complex	O
architecture	O
)	O
:	O
SANA	O
more	O
effectively	O
works	O
when	O
the	O
token	O
identifiability	O
is	O
improved	O
(	O
by	O
adding	O
residual	B-MethodName
connection	I-MethodName
between	O
two	O
recurrent	O
layers	O
)	O
,	O
achieving	O
0.83	O
point	O
gain	O
from	O
89.14	O
%	O
,	O
which	O
is	O
larger	O
gap	O
than	O
0.47	O
point	O
gain	O
without	O
residual	B-MethodName
connection	I-MethodName
.	O

Having	O
tested	O
for	O
the	O
overall	O
performance	O
with	O
the	O
original	O
datasets	O
,	O
we	O
evaluate	O
the	O
robustness	O
of	O
SANA	O
with	O
the	O
the	O
adversarial	O
datasets	O
.	O
Recently	O
,	O
adversarial	O
examples	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
have	O
been	O
employed	O
as	O
an	O
evaluation	O
tool	O
for	O
model	O
robustness	O
:	O
while	O
the	O
adversarial	O
example	O
conveys	O
very	O
similar	O
semantics	O
of	O
its	O
original	O
sample	O
,	O
but	O
with	O
small	O
and	O
intentional	O
feature	O
perturbations	O
to	O
cause	O
classification	O
models	O
to	O
make	O
false	O
predictions	O
.	O
For	O
robustness	O
analysis	O
,	O
we	O
thus	O
test	O
whether	O
the	O
attention	O
models	O
can	O
keep	O
the	O
original	O
predictions	O
from	O
adversarial	O
examples	O
.	O
This	O
experiment	O
consists	O
of	O
the	O
following	O
steps	O
:	O
First	O
,	O
based	O
on	O
the	O
original	O
training	O
data	O
,	O
we	O
set	O
a	O
basic	O
BiGRU	B-MethodName
model	O
(	O
without	O
attention	O
mechanism	O
)	O
as	O
threat	O
model	O
,	O
which	O
an	O
adversarial	B-TaskName
attack	I-TaskName
method	O
aims	O
to	O
deceive	O
.	O
Second	O
,	O
based	O
on	O
the	O
original	O
test	O
data	O
,	O
we	O
generate	O
paraphrase	O
texts	O
by	O
using	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
attack	O
method	O
(	O
Alzantot	O
et	O
al	O
,	O
2018	O
)	O
with	O
word	O
-	O
level	O
perturbations	O
.	O
Third	O
,	O
we	O
randomly	O
select	O
almost	O
500	O
paraphrase	O
texts	O
,	O
which	O
succeed	O
in	O
changing	O
the	O
prediction	O
of	O
threat	O
model	O
,	O
i.e.	O
,	O
adversarial	O
examples	O
.	O
Finally	O
,	O
we	O
report	O
the	O
accuracy	B-MetricName
of	O
the	O
three	O
attention	O
models	O
over	O
both	O
adversarial	O
examples	O
and	O
their	O
corresponding	O
original	O
samples	O
,	O
respectively	O
.	O
Tab	O
.	O
3	O
presents	O
the	O
results	O
of	O
adversarial	O
attacks	O
.	O
3	O
In	O
the	O
table	O
,	O
we	O
can	O
find	O
that	O
SANA	O
is	O
more	O
robust	O
,	O
showing	O
the	O
smallest	O
gap	O
of	O
the	O
classification	O
accuracy	B-MetricName
between	O
the	O
original	O
and	O
adversarial	O
samples	O
.	O
It	O
demonstrates	O
that	O
,	O
when	O
the	O
network	O
is	O
attending	O
to	O
the	O
words	O
having	O
causal	O
signals	O
to	O
the	O
model	O
prediction	O
,	O
the	O
network	O
becomes	O
more	O
robust	O
against	O
adversarial	O
attacks	O
,	O
which	O
is	O
consistent	O
with	O
the	O
experimental	O
results	O
in	O
Lai	O
et	O
al	O
(	O
2019	O
)	O
.	O
In	O
addition	O
to	O
that	O
,	O
we	O
observe	O
similar	O
results	O
against	O
the	O
white	O
-	O
box	O
adversarial	O
examples	O
(	O
Tsai	O
et	O
al	O
,	O
2019	O
)	O
,	O
where	O
SANA	O
improves	O
3.20	O
and	O
1.80	O
point	O
gains	O
from	O
both	O
unsupervised	O
and	O
supervised	O
attentions	O
.	O

This	O
section	O
compares	O
models	O
over	O
the	O
varying	O
amount	O
of	O
training	O
samples	O
in	O
IMDB	B-DatasetName
dataset	O
,	O
as	O
a	O
stress	O
test	O
for	O
data	O
-	O
scarce	O
scenarios	O
.	O
For	O
this	O
experiment	O
,	O
we	O
collect	O
the	O
samplespecific	O
annotations	O
from	O
human	O
workers	O
.	O
First	O
,	O
we	O
randomly	O
select	O
500	O
training	O
samples	O
from	O
IMDB	B-DatasetName
dataset	O
,	O
and	O
ask	O
the	O
worker	O
to	O
underline	O
the	O
apparent	O
rationales	O
for	O
the	O
sentiment	O
class	O
,	O
guided	O
by	O
the	O
definition	O
of	O
rationale	O
in	O
Zhang	O
et	O
al	O
(	O
2016	O
)	O
.	O
The	O
data	O
collection	O
is	O
conducted	O
using	O
an	O
open	O
annotation	O
tool	O
(	O
Yang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Then	O
,	O
we	O
build	O
an	O
additional	O
method	O
,	O
named	O
sample	O
,	O
which	O
is	O
trained	O
with	O
the	O
collected	O
sample	O
-	O
specific	O
annotations	O
.	O
The	O
results	O
are	O
presented	O
in	O
Fig	O
.	O
1	O
.	O
We	O
notice	O
that	O
SANA	O
and	O
sample	O
show	O
much	O
stronger	O
performance	O
when	O
the	O
training	O
data	O
is	O
scarce	O
,	O
where	O
similar	O
results	O
are	O
reported	O
in	O
(	O
Bao	O
et	O
al	O
,	O
2018	O
)	O
.	O
As	O
we	O
expected	O
,	O
the	O
attention	O
supervision	O
using	O
the	O
sample	O
-	O
specific	O
annotations	O
gets	O
a	O
higher	O
accuracy	B-MetricName
than	O
that	O
using	O
the	O
task	O
-	O
level	O
annotations	O
,	O
but	O
can	O
not	O
be	O
scaled	O
-	O
up	O
above	O
500	O
training	O
samples	O
,	O
which	O
is	O
represented	O
by	O
the	O
red	O
reference	O
line	O
.	O
In	O
contrast	O
,	O
SANA	O
improves	O
accuracy	B-MetricName
with	O
≥	O
1000	O
samples	O
and	O
its	O
scalability	O
.	O
This	O
result	O
demonstrates	O
that	O
our	O
counterfactual	O
inferences	O
successfully	O
augment	O
one	O
annotation	O
into	O
multiple	O
(	O
counterfactual	O
)	O
attention	O
supervisions	O
,	O
better	O
regularizing	O
from	O
limited	O
samples	O
.	O

As	O
for	O
human	O
consuming	O
attention	O
as	O
explanation	O
,	O
there	O
has	O
been	O
criticism	O
that	O
unsupervised	O
attention	O
weights	O
are	O
too	O
poorly	O
correlated	O
with	O
the	O
contribution	O
of	O
each	O
word	O
for	O
machine	O
decision	O
(	O
or	O
,	O
unfaithful	O
)	O
(	O
Jain	O
and	O
Wallace	O
,	O
2019	O
;	O
Serrano	O
and	O
Smith	O
,	O
2019	O
;	O
Pruthi	O
et	O
al	O
,	O
2019	O
)	O
.	O
Meanwhile	O
,	O
(	O
Wiegreffe	O
and	O
Pinter	O
,	O
2019	O
)	O
develops	O
diagnostics	O
to	O
decide	O
when	O
attention	O
is	O
good	O
enough	O
as	O
explanation	O
.	O
As	O
for	O
improving	O
human	O
consumption	O
,	O
one	O
direction	O
focuses	O
on	O
better	O
aligning	O
models	O
to	O
human	O
,	O
another	O
on	O
improving	O
annotation	O
quality	O
.	O
First	O
,	O
identifiability	O
(	O
Brunner	O
et	O
al	O
,	O
2020	O
)	O
explains	O
human	O
-	O
machine	O
discrepancy	O
,	O
where	O
tokenlevel	O
information	O
is	O
lost	O
in	O
model	O
hidden	O
states	O
.	O
For	O
better	O
alignment	O
,	O
(	O
Tutek	O
andŠnajder	O
,	O
2020	O
)	O
utilizes	O
masked	O
language	O
model	O
(	O
MLM	B-DatasetName
)	O
loss	B-MetricName
and	O
(	O
Mohankumar	O
et	O
al	O
,	O
2020	O
)	O
invents	O
orthogonal	O
LSTM	B-MethodName
representations	O
.	O
Second	O
,	O
toward	O
the	O
direction	O
of	O
improving	O
annotation	O
,	O
(	O
Barrett	O
et	O
al	O
,	O
2018	O
;	O
Zhong	O
et	O
al	O
,	O
2019	O
;	O
Bao	O
et	O
al	O
,	O
2018	O
)	O
adopts	O
sample	O
-	O
specific	O
human	O
annotations	O
.	O
In	O
addition	O
to	O
rationales	O
,	O
(	O
Zhao	O
et	O
al	O
,	O
2018	O
)	O
uses	O
event	O
trigger	O
words	O
and	O
(	O
Kim	O
and	O
Kim	O
,	O
2018	O
)	O
leverages	O
user	O
authenticated	O
domains	O
to	O
narrow	O
down	O
the	O
scope	O
of	O
attentions	O
.	O
(	O
Strubell	O
et	O
al	O
,	O
2018	O
)	O
injects	O
word	O
dependency	O
relations	O
to	O
recognize	O
the	O
semantic	O
roles	O
in	O
text	O
.	O
Such	O
annotation	O
overhead	O
can	O
be	O
replaced	O
by	O
existing	O
pre	O
-	O
annotated	O
resources	O
:	O
(	O
Zou	O
et	O
al	O
,	O
2018	O
)	O
considers	O
sentiment	O
lexicon	O
dictionary	O
for	O
a	O
related	O
task	O
.	O
We	O
pursue	O
the	O
second	O
direction	O
,	O
but	O
without	O
incurring	O
additional	O
human	O
annotation	O
,	O
by	O
exploring	O
the	O
counterfactual	O
augmentation	O
,	O
originated	O
from	O
self	O
-	O
supervision	O
signals	O
,	O
contributing	O
towards	O
both	O
accuracy	B-MetricName
and	O
robustness	O
of	O
the	O
model	O
.	O

Machine	O
consuming	O
attention	O
for	O
higher	O
accuracy	B-MetricName
is	O
the	O
most	O
classical	O
target	O
scenario	O
.	O
(	O
Yang	O
et	O
al	O
,	O
2016	O
)	O
proposes	O
hierarchical	O
attention	O
for	O
document	B-TaskName
classification	I-TaskName
,	O
(	O
Chen	O
et	O
al	O
,	O
2016	O
)	O
personalizes	O
classification	O
to	O
user	O
and	O
product	O
attributes	O
.	O
(	O
Margatina	O
et	O
al	O
,	O
2019	O
)	O
incorporates	O
knowledge	O
information	O
to	O
the	O
self	O
-	O
attention	O
module	O
,	O
i.e.	O
,	O
lexicon	O
features	O
.	O
Alternatively	O
,	O
machine	O
may	O
mine	O
or	O
augment	O
attention	O
supervision	O
:	O
(	O
Tang	O
et	O
al	O
,	O
2019	O
)	O
automatically	O
mines	O
attention	O
supervision	O
by	O
masking	O
-	O
out	O
highly	O
attentive	O
words	O
in	O
a	O
progressive	O
manner	O
.	O
(	O
Choi	O
et	O
al	O
,	O
2019	O
)	O
augments	O
counterfactual	O
observations	O
to	O
debias	O
human	O
attention	O
supervision	O
via	O
instance	O
similarity	O
.	O
Our	O
work	O
is	O
of	O
combining	O
the	O
strength	O
of	O
the	O
two	O
works	O
:	O
we	O
automatically	O
improve	O
attention	O
supervision	O
via	O
self	O
-	O
supervision	O
signals	O
,	O
but	O
we	O
build	O
it	O
with	O
free	O
task	O
-	O
level	O
resources	O
.	O

We	O
studied	O
the	O
problem	O
of	O
attention	O
supervision	O
,	O
and	O
showed	O
that	O
requiring	O
sample	O
-	O
level	O
human	O
supervision	O
is	O
often	O
less	O
effective	O
than	O
task	O
-	O
level	O
alternative	O
with	O
lower	O
(	O
and	O
often	O
zero	O
-	O
)	O
overhead	O
.	O
Specifically	O
,	O
we	O
proposed	O
a	O
counterfactual	O
signal	O
for	O
self	O
-	O
supervision	O
,	O
to	O
augment	O
task	O
-	O
level	O
human	O
annotation	O
,	O
into	O
sample	O
-	O
level	O
machine	O
attention	O
supervision	O
,	O
to	O
increase	O
both	O
the	O
accuracy	B-MetricName
and	O
robustness	O
of	O
the	O
model	O
.	O
We	O
hope	O
future	O
research	O
to	O
explore	O
scenarios	O
where	O
human	O
intuition	O
is	O
not	O
working	O
as	O
well	O
as	O
text	B-TaskName
classification	I-TaskName
,	O
such	O
as	O
graph	O
attention	O
(	O
Veličković	O
et	O
al	O
,	O
2017	O
)	O
.	O

Tables	O
3	O
-	O
5	O
show	O
the	O
results	O
.	O
Target	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
is	O
calculated	O
using	O
the	O
subset	O
metric	O
(	O
similar	O
to	O
metrics	O
used	O
by	O
Yang	O
and	O
Cardie	O
(	O
2013	O
)	O
,	O
Irsoy	O
and	O
Cardie	O
(	O
2014	O
)	O
)	O
;	O
if	O
either	O
the	O
predicted	O
or	O
gold	O
target	O
tokens	O
are	O
a	O
subset	O
of	O
the	O
other	O
,	O
the	O
match	O
is	O
counted	O
when	O
computing	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
.	O
Overlapping	O
matches	O
that	O
are	O
not	O
subsets	O
do	O
not	O
count	O
(	O
e.g	O
Egypt	O
's	O
position	O
and	O
Israel	O
's	O
position	O
do	O
not	O
match	O
.	O
)	O
.	O
For	O
this	O
task	O
,	O
in	O
the	O
case	O
of	O
multiple	O
mentions	O
of	O
the	O
same	O
entity	O
in	O
the	O
post	O
,	O
any	O
mention	O
will	O
be	O
considered	O
correct	O
if	O
the	O
subset	O
matches	O
4	O
(	O
e.g	O
if	O
Palestine	O
is	O
a	O
gold	O
target	O
,	O
and	O
state	O
of	O
Palestine	O
is	O
predicted	O
at	O
a	O
different	O
position	O
in	O
the	O
post	O
,	O
it	O
is	O
still	O
correct	O
)	O
.	O
This	O
evaluation	O
is	O
driven	O
from	O
the	O
sentiment	O
summarization	B-TaskName
perspective	O
:	O
we	O
want	O
to	O
predict	O
the	O
overall	O
opinion	O
in	O
the	O
post	O
towards	O
an	O
entity	O
.	O
F	O
-	O
pos	O
,	O
F	O
-	O
neg	O
,	O
and	O
Acc	B-MetricName
-	O
sent	O
show	O
the	O
performance	O
of	O
the	O
sentiment	O
model	O
on	O
only	O
the	O
correctly	O
predicted	O
targets	O
5	O
.	O
Since	O
the	O
target	O
and	O
sentiment	O
models	O
are	O
trained	O
separately	O
,	O
this	O
is	O
meant	O
to	O
give	O
an	O
idea	O
of	O
how	O
the	O
sentiment	O
model	O
would	O
perform	O
in	O
standalone	O
mode	O
,	O
if	O
targets	O
were	O
already	O
provided	O
.	O
F	O
-	O
all	O
shows	O
the	O
overall	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
showing	O
the	O
performance	O
of	O
correctly	O
predicted	O
targets	O
with	O
correct	O
sentiment	O
compared	O
to	O
the	O
total	O
number	O
of	O
polar	O
targets	O
.	O
This	O
evaluates	O
the	O
end	O
-	O
to	O
-	O
end	O
scenario	O
of	O
both	O
important	O
target	O
and	O
sentiment	O
prediction	O
.	O
Best	O
results	O
are	O
shown	O
in	O
bold	O
.	O
Significance	O
thresholds	O
are	O
calculated	O
for	O
the	O
best	O
performing	O
systems	O
(	O
Tables	O
4	O
-	O
5	O
)	O
using	O
the	O
approximate	O
randomization	O
test	O
(	O
Yeh	O
,	O
2000	O
)	O
for	O
target	O
recall	O
,	O
precision	O
,	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
,	O
Acc	B-MetricName
-	O
sent	O
and	O
F	O
-	O
all	O
.	O
Significance	O
over	O
the	O
method	O
in	O
the	O
previous	O
row	O
is	O
indicated	O
by	O
*	O
(	O
p	O
<	O
0.05	O
)	O
,	O
*	O
*	O
(	O
p	O
<	O
0.005	O
)	O
,	O
*	O
*	O
(	O
p	O
<	O
0.0005	O
)	O
.	O
A	O
confidence	O
interval	O
of	O
almost	O
four	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
points	O
is	O
required	O
to	O
obtain	O
p	O
<	O
0.05	O
.	O
Our	O
dataset	O
is	O
small	O
;	O
nonetheless	O
we	O
get	O
significant	O
results	O
.	O
Comparing	O
Sentiment	O
Lexicons	O
Table	O
3	O
shows	O
the	O
results	O
comparing	O
the	O
different	O
baselines	O
.	O
All	O
targets	O
are	O
retrieved	O
using	O
all	O
-	O
NP	O
;	O
sentiment	O
is	O
determined	O
using	O
the	O
lexical	O
baselines	O
.	O
As	O
expected	O
,	O
the	O
all	O
-	O
NP	O
baseline	O
shows	O
near	O
perfect	O
recall	O
and	O
low	O
precision	O
in	O
predicting	O
important	O
targets	O
.	O
We	O
observe	O
that	O
the	O
gloss	O
-	O
translated	O
MPQA	B-DatasetName
lexicon	O
outperforms	O
the	O
two	O
other	O
Arabic	O
lexicons	O
among	O
the	O
sentiment	O
baselines	O
.	O
We	O
believe	O
that	O
the	O
hit	O
rate	O
of	O
MPQA	B-DatasetName
is	O
higher	O
than	O
that	O
of	O
the	O
smaller	O
,	O
manually	O
-	O
labeled	O
SIFAAT	O
,	O
and	O
it	O
is	O
more	O
precise	O
than	O
the	O
automatically	O
generated	O
WordNet	O
-	O
based	O
lexicon	O
ArSenL.	O
The	O
performance	O
of	O
MPQA	B-DatasetName
is	O
,	O
however	O
,	O
reliant	O
on	O
the	O
availability	O
of	O
high	O
-	O
quality	O
English	O
glosses	O
.	O
We	O
found	O
MPQA	B-DatasetName
to	O
consistently	O
outperform	O
in	O
the	O
model	O
results	O
,	O
so	O
in	O
our	O
best	O
-	O
linguistic	O
models	O
,	O
we	O
only	O
show	O
results	O
using	O
the	O
MPQA	B-DatasetName
lexicon	O
.	O

Looking	O
at	O
table	O
4	O
,	O
we	O
can	O
see	O
that	O
using	O
the	O
lemma	B-DatasetName
representation	O
easily	O
outperforms	O
the	O
sparser	O
surface	O
word	O
,	O
and	O
that	O
adding	O
tokenized	O
clitics	O
as	O
separate	O
tokens	O
outperforms	O
representations	O
which	O
only	O
use	O
the	O
word	O
form	O
.	O
Moreover	O
,	O
upon	O
using	O
the	O
D3	B-DatasetName
decliticization	O
method	O
,	O
we	O
observe	O
a	O
significant	O
increase	O
in	O
recall	O
of	O
targets	O
over	O
the	O
ATB	O
representation	O
.	O
This	O
shows	O
that	O
the	O
presence	O
of	O
the	O
Arabic	O
definite	O
article	O
Al+	O
is	O
an	O
important	O
indicator	O
of	O
a	O
target	O
entity	O
;	O
thus	O
,	O
even	O
if	O
an	O
entity	O
is	O
not	O
named	O
,	O
Al+	O
indicates	O
that	O
it	O
is	O
a	O
known	O
entity	O
and	O
is	O
likely	O
more	O
salient	O
.	O
The	O
more	O
tokens	O
are	O
split	O
off	O
,	O
the	O
more	O
targets	O
are	O
recalled	O
,	O
although	O
this	O
comes	O
at	O
the	O
cost	O
of	O
a	O
decrease	O
in	O
sentiment	O
performance	O
,	O
where	O
the	O
lemma	B-DatasetName
representation	O
has	O
the	O
highest	O
sentiment	O
score	O
and	O
the	O
D3	B-DatasetName
representation	O
has	O
the	O
lowest	O
af	O
-	O
ter	O
surface	O
word	O
.	O
We	O
believe	O
the	O
addition	O
of	O
extra	O
tokens	O
in	O
the	O
sequence	O
(	O
which	O
are	O
function	O
words	O
and	O
have	O
not	O
much	O
bearing	O
on	O
semantics	O
)	O
generates	O
noise	O
with	O
respect	O
to	O
the	O
sentiment	O
model	O
.	O
All	O
models	O
significantly	O
improve	O
the	O
baselines	O
on	O
Fmeasure	O
;	O
for	O
Acc	B-MetricName
-	O
sent	O
,	O
the	O
surface	O
word	O
CRF	B-MethodName
does	O
not	O
significantly	O
outperform	O
the	O
MPQA	B-DatasetName
baseline	O
.	O
Effect	O
of	O
Word	O
Clusters	O
Figures	O
2	O
-	O
5	O
show	O
the	O
performance	O
of	O
different	O
morphological	O
representations	O
when	O
varying	O
the	O
number	O
of	O
word	O
vector	O
clusters	O
k.	O
(	O
Higher	O
k	O
means	O
more	O
clusters	O
and	O
fewer	O
entities	O
per	O
semantic	O
cluster	O
.	O
)	O
Adding	O
cluster	O
features	O
tends	O
to	O
further	O
boost	O
the	O
recall	O
of	O
important	O
targets	O
for	O
all	O
morphological	O
schemes	O
,	O
while	O
more	O
or	O
less	O
maintaining	O
precision	O
.	O
The	O
difference	O
in	O
different	O
schemes	O
is	O
consistent	O
with	O
the	O
results	O
of	O
Table	O
4	O
;	O
the	O
D3	B-DatasetName
representation	O
maintains	O
the	O
highest	O
recall	O
of	O
targets	O
,	O
while	O
the	O
opposite	O
is	O
true	O
for	O
identifying	O
sentiment	O
towards	O
the	O
targets	O
.	O
The	O
ATB	O
representation	O
shows	O
the	O
best	O
overall	O
Fmeasure	O
,	O
peaking	O
at	O
41.5	O
using	O
k=250	O
(	O
compare	O
with	O
38.2	O
using	O
no	O
clusters	O
)	O
;	O
however	O
,	O
it	O
recalls	O
much	O
fewer	O
targets	O
than	O
the	O
D3	B-DatasetName
representation	O
.	O
The	O
effect	O
of	O
clusters	O
on	O
sentiment	O
is	O
less	O
clear	O
;	O
it	O
seems	O
to	O
benefit	O
the	O
D3	B-DatasetName
and	O
ATB	O
schemes	O
more	O
than	O
lemma	B-DatasetName
(	O
significant	O
boosts	O
in	O
sentiment	O
accuracy	B-MetricName
)	O
.	O
The	O
improvements	O
in	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
and	O
F	O
-	O
all	O
observed	O
by	O
using	O
the	O
best	O
value	O
of	O
k	O
is	O
statistically	O
significant	O
for	O
all	O
schemes	O
(	O
k=10	O
for	O
lemma	B-DatasetName
,	O
k=250	O
for	O
lemma+ATB	O
,	O
k=500	O
for	O
lemma+D3	O
,	O
with	O
F	O
-	O
all	O
values	O
of	O
40.7	O
,	O
41.5	O
,	O
and	O
39.1	O
respectively	O
)	O
.	O
In	O
general	O
,	O
the	O
cluster	O
performances	O
tend	O
to	O
peak	O
at	O
a	O
certain	O
value	O
of	O
k	O
which	O
balances	O
the	O
reduced	O
sparsity	O
of	O
the	O
model	O
(	O
fewer	O
clusters	O
)	O
with	O
the	O
semantic	O
closeness	O
of	O
entities	O
within	O
a	O
cluster	O
(	O
more	O
clusters	O
)	O
.	O
Performance	O
of	O
Best	O
Linguistic	O
Model	O
Table	O
5	O
shows	O
the	O
performance	O
of	O
our	O
best	O
-	O
linguistic	O
model	O
,	O
which	O
in	O
addition	O
to	O
the	O
word	O
form	O
and	O
part	O
of	O
speech	O
,	O
contains	O
named	O
entity	O
and	O
base	O
phrase	O
chunks	O
,	O
the	O
syntactic	O
dependency	O
features	O
,	O
and	O
the	O
sentiment	O
lexicon	O
features	O
.	O
The	O
best	O
linguistic	O
model	O
is	O
run	O
using	O
both	O
ATB	O
and	O
D3	B-DatasetName
tokenization	O
schemes	O
,	O
and	O
then	O
using	O
a	O
combined	O
ATB+D3	O
scheme	O
where	O
we	O
use	O
D3	B-DatasetName
for	O
the	O
target	O
model	O
and	O
remove	O
the	O
extra	O
clitics	O
before	O
piping	O
in	O
the	O
output	O
to	O
the	O
sentiment	O
model	O
.	O
This	O
combined	O
scheme	O
results	O
in	O
the	O
best	O
results	O
overall	O
:	O
F	O
-	O
score	O
of	O
61.4	O
for	O
targets	O
,	O
accuracy	B-MetricName
of	O
75.4	O
for	O
sentiment	O
and	O
overall	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
of	O
43.1	O
.	O
Adding	O
the	O
richer	O
linguistic	O
resources	O
results	O
in	O
both	O
improved	O
target	O
precision	O
,	O
recall	O
,	O
and	O
sentiment	O
scores	O
,	O
with	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
for	O
positive	O
targets	O
reaching	O
67.7	O
for	O
positive	O
targets	O
and	O
80	O
for	O
negative	O
targets	O
.	O
Performance	O
exceeds	O
that	O
of	O
the	O
simpler	O
models	O
which	O
use	O
only	O
POS	O
and	O
word	O
clusters	O
,	O
but	O
it	O
is	O
worth	O
noting	O
that	O
using	O
only	O
the	O
basic	O
model	O
with	O
the	O
word	O
clusters	O
can	O
achieve	O
significant	O
boosts	O
in	O
recall	O
and	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
bringing	O
it	O
closer	O
to	O
the	O
rich	O
linguistic	O
model	O
.	O
The	O
last	O
row	O
shows	O
the	O
best	O
linguistic	O
model	O
D3+ATB	O
combined	O
with	O
the	O
clusters	O
(	O
best	O
result	O
for	O
k=8000	O
,	O
or	O
about	O
30	O
words	O
per	O
cluster	O
)	O
.	O
Adding	O
the	O
clusters	O
improves	O
target	O
and	O
Fmeasure	O
scores	O
,	O
although	O
this	O
result	O
is	O
not	O
statistically	O
significant	O
.	O
We	O
observe	O
that	O
it	O
becomes	O
more	O
difficult	O
to	O
improve	O
on	O
the	O
rich	O
linguistic	O
model	O
using	O
word	O
clusters	O
,	O
which	O
are	O
more	O
beneficial	O
for	O
low	O
resource	O
scenarios	O
.	O
Our	O
results	O
are	O
comparable	O
to	O
published	O
work	O
for	O
most	O
similar	O
tasks	O
in	O
English	O
:	O
e.g	O
Yang	O
and	O
Cardie	O
(	O
2013	O
)	O
who	O
reported	O
target	O
subset	O
Fmeasure	O
of~65	O
,	O
Pontiki	O
et	O
al	O
(	O
2014	O
)	O
where	O
best	O
Figure	O
6	O
:	O
Overall	O
F	O
-	O
score	O
vs	O
clusters	O
.	O
performing	O
SemEval	O
systems	O
reported	O
70	O
-	O
80	O
%	O
for	O
sentiment	O
given	O
defined	O
aspects	O
,	O
and	O
(	O
Mitchell	O
et	O
al	O
,	O
2013	O
;	O
Deng	O
and	O
Wiebe	O
,	O
2015	O
)	O
for	O
overall	O
Fmeasure	O
;	O
we	O
note	O
that	O
our	O
tasks	O
differ	O
as	O
described	O
in	O
section	O
2	O
.	O
Results	O
on	O
blind	O
test	O
Table	O
6	O
shows	O
the	O
results	O
on	O
unseen	O
test	O
data	O
for	O
best	O
-	O
linguistic	O
using	O
D3	B-DatasetName
,	O
D3+ATB	O
and	O
with	O
clusters	O
using	O
k=8000	O
.	O
The	O
results	O
are	O
similar	O
to	O
what	O
was	O
observed	O
in	O
the	O
development	O
data	O
.	O

The	O
core	O
of	O
our	O
system	O
is	O
based	O
upon	O
computing	O
semantic	B-TaskName
similarity	I-TaskName
of	O
sentence	O
chunks	O
.	O
More	O
precisely	O
,	O
we	O
are	O
looking	O
for	O
the	O
best	O
estimation	O
of	O
sim	O
(	O
CH	O
a	O
i	O
,	O
CH	O
b	O
j	O
)	O
.	O
The	O
sim	O
score	O
should	O
describe	O
semantic	B-TaskName
similarity	I-TaskName
of	O
a	O
given	O
chunk	O
pairthe	O
higher	O
score	O
the	O
more	O
easily	O
both	O
chunks	O
can	O
be	O
replaced	O
with	O
each	O
other	O
without	O
chaining	O
the	O
meaning	O
of	O
both	O
sentences	O
.	O
The	O
similarity	O
score	O
ranges	O
from	O
0	B-DatasetName
to	O
5	O
,	O
where	O
0	B-DatasetName
is	O
the	O
lowest	O
similarity	O
and	O
5	O
is	O
the	O
highest	O
similarity	O
.	O
Eg	O
.	O
the	O
sim	O
(	O
"	O
a	O
new	O
laptop	O
"	O
,	O
"	O
a	O
new	O
notebook	O
"	O
)	O
=	O
5	O
and	O
sim	O
(	O
"	O
a	O
new	O
laptop	O
"	O
,	O
"	O
an	O
old	O
rock	O
"	O
)	O
=	O
0	B-DatasetName
.	O
We	O
use	O
the	O
chunk	O
similarity	O
as	O
a	O
feature	O
in	O
our	O
machine	O
learning	O
approach	O
(	O
Section	O
3	O
)	O
and	O
as	O
a	O
metric	O
in	O
our	O
unsupervised	O
approach	O
(	O
Section	O
4	O
)	O
.	O
Our	O
attempts	O
to	O
estimate	O
the	O
sim	O
function	O
are	O
based	O
upon	O
estimating	O
semantic	B-TaskName
similarity	I-TaskName
of	O
individual	O
words	O
and	O
compiling	O
them	O
into	O
one	O
number	O
for	O
a	O
given	O
chunk	O
pair	O
.	O
We	O
experiment	O
with	O
Word2Vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
and	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
for	O
estimating	O
similarity	O
of	O
words	O
.	O
We	O
compile	O
all	O
the	O
word	O
similarities	O
in	O
one	O
number	O
that	O
reflects	O
semantic	B-TaskName
similarity	I-TaskName
of	O
whole	O
chunks	O
via	O
the	O
following	O
methods	O
:	O
1	O
)	O
the	O
vector	O
composition	O
method	O
and	O
2	O
)	O
an	O
adapted	O
method	O
for	O
constructing	O
vectors	O
called	O
lexical	O
semantic	O
vectors	O
.	O
Vector	O
composition	O
requires	O
that	O
the	O
semantics	O
of	O
words	O
is	O
described	O
by	O
vectors	O
.	O
E.g.	O
we	O
have	O
vectors	O
for	O
all	O
words	O
m	O
i	O
:	O
∀w	O
i	O
CH	O
a	O
j	O
and	O
n	O
i	O
:	O
∀w	O
i	O
CH	O
b	O
k	O
in	O
two	O
given	O
chunks	O
CH	O
a	O
j	O
and	O
CH	O
b	O
k	O
.	O
The	O
vectors	O
for	O
words	O
in	O
each	O
chunk	O
are	O
summed	O
(	O
or	O
averaged	O
)	O
to	O
obtain	O
one	O
vector	O
for	O
each	O
chunk	O
:	O
m	O
=	O
i	O
(	O
m	O
i	O
)	O
and	O
n	O
=	O
j	O
(	O
n	O
j	O
)	O
.	O
The	O
vectors	O
are	O
then	O
compared	O
with	O
cosine	O
distance	O
:	O
sim	O
(	O
CH	O
a	O
j	O
,	O
CH	O
b	O
k	O
)	O
=	O
cos	O
(	O
θ	B-HyperparameterName
)	O
=	O
m	O
n	O
m	O
|	O
n	O
|	O
.	O
Lexical	O
semantic	O
vectors	O
were	O
originally	O
introduced	O
in	O
(	O
Li	O
et	O
al	O
,	O
2006	O
)	O
.	O
We	O
have	O
made	O
two	O
modifications	O
.	O
We	O
do	O
not	O
weight	O
words	O
with	O
their	O
information	O
content	O
and	O
we	O
use	O
methods	O
for	O
distributional	O
semantics	O
(	O
Word2Vec	O
and	O
GloVe	B-MethodName
)	O
rather	O
than	O
semantic	O
networks	O
.	O
The	O
modified	O
method	O
is	O
explained	O
here	O
.	O
First	O
of	O
all	O
,	O
we	O
create	O
a	O
combined	O
vocabulary	O
of	O
all	O
unique	O
words	O
from	O
chunks	O
CH	O
a	O
k	O
and	O
CH	O
b	O
l	O
:	O
L	O
=	O
unique	O
(	O
CH	O
a	O
k	O
∪	O
CH	O
b	O
l	O
)	O
.	O
Then	O
we	O
take	O
all	O
words	O
from	O
vocabulary	O
L	O
:	O
w	O
i	O
L	O
and	O
look	O
for	O
maximal	O
similarities	O
with	O
words	O
from	O
chunks	O
a	O
and	O
b	O
,	O
respectively	O
.	O
This	O
way	O
we	O
get	O
vectors	O
m	O
and	O
n	O
containing	O
maximal	O
similarities	O
of	O
chunk	O
words	O
and	O
words	O
from	O
the	O
combined	O
vocabulary	O
:	O
m	O
i	O
=	O
max	O
j:1≤j≤	O
|	O
CH	O
a	O
k	O
|	O
sim	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
:	O
∀w	O
i	O
L	O
n	O
i	O
=	O
max	O
j:1≤j≤	O
|	O
CH	O
b	O
l	O
|	O
sim	O
(	O
w	O
i	O
,	O
w	O
j	O
)	O
:	O
∀w	O
i	O
L	O
(	O
1	O
)	O
where	O
m	O
i	O
and	O
n	O
i	O
are	O
elements	O
of	O
vectors	O
m	O
and	O
n.	O
In	O
order	O
to	O
obtain	O
similarity	O
of	O
a	O
chunk	O
pair	O
we	O
compare	O
their	O
respective	O
vectors	O
with	O
the	O
cosine	O
similarity	O
similarly	O
to	O
the	O
previous	O
approach	O
.	O
The	O
principle	O
of	O
the	O
method	O
is	O
illustrated	O
by	O
the	O
example	O
in	O
figure	O
1	O
.	O
iDF	O
weighting	O
.	O
We	O
assume	O
that	O
some	O
words	O
are	O
more	O
important	O
than	O
others	O
.	O
In	O
order	O
to	O
reflect	O
this	O
assumption	O
,	O
we	O
try	O
to	O
weight	O
the	O
vectors	O
with	O
iDF	O
weighting	O
.	O
We	O
compute	O
the	O
iDF	O
weights	O
on	O
the	O
articles	O
from	O
English	O
wikipedia	O
text	O
data	O
(	O
Wikipedia	O
dump	O
from	O
March	O
7	O
,	O
2015	O
)	O
.	O

The	O
main	O
effort	O
of	O
our	O
team	O
was	O
focused	O
on	O
the	O
machine	O
learning	O
approach	O
to	O
the	O
task	O
.	O
We	O
divided	O
the	O
task	O
into	O
to	O
three	O
classification	O
/	O
regression	O
tasks	O
:	O
Alignment	O
binary	O
classification	O
-	O
we	O
decide	O
whether	O
two	O
given	O
chunks	O
should	O
be	O
aligned	O
with	O
each	O
other	O
.	O
Score	B-MetricName
classification	O
/	O
regression	O
-	O
we	O
experiment	O
with	O
both	O
classification	O
and	O
regression	O
of	O
the	O
chunks	O
similarity	O
score	O
.	O
Type	O
classification	O
-	O
we	O
classify	O
all	O
aligned	O
pairs	O
of	O
chunks	O
into	O
a	O
predefined	O
set	O
of	O
types	O
-	O
see	O
Section	O
1.1	O
.	O

Machine	O
learning	O
approach	O
We	O
employ	O
the	O
following	O
classifiers	O
and	O
classification	O
frameworks	O
:	O
Alignment	O
binary	O
classification	O
-	O
Voted	O
perceptron	O
(	O
Weka	O
)	O
.	O
Score	B-MetricName
classification	O
-	O
Maximum	O
entropy	O
(	O
Brainy	O
)	O
.	O
Type	O
classification	O
-	O
Support	O
vector	O
machines	O
(	O
Brainy	O
)	O
.	O
These	O
classifiers	O
perform	O
best	O
on	O
the	O
evaluation	O
datasets	O
.	O
We	O
achieved	O
the	O
best	O
results	O
for	O
estimating	O
chunk	O
similarity	O
with	O
Word2Vec	O
and	O
the	O
modified	O
lexical	O
semantic	O
vectors	O
-	O
see	O
Section	O
2.2	O
.	O
We	O
experimented	O
with	O
reduced	O
feature	O
set	O
(	O
word	O
overlap	O
,	O
word	O
positions	O
difference	O
,	O
POS	O
tags	O
difference	O
,	O
semantic	B-TaskName
similarity	I-TaskName
,	O
global	O
semantic	B-TaskName
similarity	I-TaskName
,	O
paraphrase	O
database	O
)	O
-	O
run	O
1	O
and	O
with	O
all	O
featuresrun	O
3	O
.	O
The	O
run	O
1	O
contains	O
the	O
optimal	O
combination	O
of	O
features	O
.	O
Since	O
this	O
combination	O
is	O
established	O
on	O
evaluation	O
datasets	O
it	O
does	O
not	O
need	O
to	O
be	O
optimal	O
for	O
the	O
test	O
datasets	O
.	O
To	O
increase	O
our	O
chances	O
in	O
the	O
completion	O
,	O
we	O
also	O
run	O
the	O
system	O
with	O
all	O
features	O
-	O
run	O
3	O
.	O
We	O
use	O
the	O
provided	O
annotated	O
evaluation	O
dataset	O
(	O
Images	O
,	O
Headlines	O
,	O
Answer	O
students	O
)	O
for	O
training	O
the	O
models	O
.	O
We	O
train	O
three	O
models	O
,	O
each	O
for	O
one	O
dataset	O
.	O
For	O
development	O
,	O
we	O
use	O
the	O
10	O
-	O
fold	O
crossvalidation	O
.	O
For	O
final	O
test	O
runs	O
,	O
we	O
train	O
the	O
three	O
models	O
on	O
evaluation	O
datasets	O
and	O
run	O
the	O
system	O
on	O
the	O
corresponding	O
test	O
datasets	O
(	O
e.g.	O
Images	O
evaluation	O
dataset	O
based	O
model	O
is	O
used	O
to	O
annotate	O
Images	O
test	O
data	O
)	O
.	O
We	O
do	O
not	O
neither	O
modify	O
the	O
original	O
datasets	O
nor	O
annotate	O
any	O
additional	O
data	O
.	O
Rule	O
-	O
based	O
approach	O
There	O
are	O
little	O
options	O
in	O
this	O
approach	O
.	O
Again	O
,	O
we	O
have	O
achieved	O
the	O
best	O
results	O
for	O
estimating	O
chunk	O
similarity	O
with	O
Word2Vec	O
and	O
the	O
modified	O
lexical	O
semantic	O
vectors	O
-	O
see	O
Section	O
2.2	O
.	O
We	O
set	O
the	O
threshold	O
for	O
the	O
similarity	O
score	O
to	O
2.5	O
.	O
All	O
lower	O
values	O
are	O
set	O
to	O
0	B-DatasetName
.	O
This	O
is	O
the	O
run	O
2	O
.	O
Individual	O
setting	O
for	O
different	O
dataset	O
We	O
restrained	O
from	O
setting	O
individual	O
configurations	O
for	O
different	O
datasets	O
.	O
The	O
setup	O
is	O
completely	O
identical	O
for	O
all	O
datasets	O
.	O

In	O
this	O
section	O
,	O
we	O
summarize	O
the	O
official	O
results	O
for	O
the	O
SemEval	O
2016	O
competition	O
-	O
see	O
table	O
1	O
.	O
The	O
results	O
are	O
calculated	O
for	O
the	O
following	O
dataset	O
:	O
Headlines	O
,	O
Images	O
and	O
Answer	O
students	O
.	O
The	O
results	O
show	O
F1	B-MetricName
scores	O
for	O
chunk	O
alignment	O
(	O
Ali	O
)	O
,	O
determination	O
of	O
the	O
relation	O
type	O
(	O
Type	O
)	O
,	O
chunk	O
similarity	O
score	O
(	O
Score	B-MetricName
)	O
and	O
combination	O
of	O
relation	O
type	O
and	O
score	O
similarity	O
(	O
T+S	O
)	O
.	O
The	O
bold	O
numbers	O
are	O
the	O
overall	O
best	O
scores	O
.	O
We	O
participated	O
only	O
in	O
the	O
gold	O
standard	O
chunk	O
scenario	O
.	O
The	O
results	O
clearly	O
show	O
that	O
the	O
unsupervised	O
run	O
2	O
perform	O
much	O
worse	O
than	O
the	O
supervised	O
runs	O
1	O
and	O
3	O
.	O
We	O
expected	O
that	O
.	O
However	O
,	O
it	O
is	O
worth	O
of	O
noticing	O
that	O
the	O
unsupervised	O
alignment	O
algorithm	O
inspired	O
by	O
machine	B-TaskName
translation	I-TaskName
alignment	O
placed	O
quite	O
well	O
.	O
In	O
fact	O
,	O
it	O
is	O
newer	O
looses	O
more	O
than	O
3	O
%	O
from	O
the	O
best	O
alignment	O
score	O
in	O
all	O
datasets	O
.	O
The	O
overall	O
rank	O
of	O
the	O
run	O
2	O
places	O
in	O
the	O
top	O
half	O
among	O
all	O
system	O
with	O
exception	O
of	O
the	O
answer	O
student	O
dataset	O
.	O
The	O
poor	O
performance	O
of	O
the	O
run	O
2	O
on	O
this	O
dataset	O
is	O
most	O
likely	O
caused	O
by	O
the	O
fact	O
that	O
the	O
hand	O
-	O
crafted	O
rules	O
were	O
prepared	O
for	O
the	O
images	O
and	O
headlines	O
datasets	O
and	O
they	O
are	O
clearly	O
not	O
applicable	O
on	O
the	O
answer	O
student	O
which	O
is	O
substantially	O
different	O
.	O
The	O
runs	O
1	O
and	O
3	O
perform	O
very	O
similarly	O
.	O
The	O
optimized	O
feature	O
set	O
of	O
the	O
run	O
1	O
helps	O
especially	O
in	O
the	O
answer	O
student	O
dataset	O
.	O
However	O
,	O
the	O
differences	O
between	O
these	O
runs	O
are	O
too	O
small	O
and	O
they	O
can	O
be	O
caused	O
by	O
chance	O
.	O
It	O
is	O
worth	O
of	O
noticing	O
that	O
the	O
run	O
1	O
is	O
not	O
the	O
best	O
one	O
in	O
any	O
of	O
the	O
datasets	O
and	O
it	O
still	O
wins	O
in	O
the	O
overall	O
results	O
table	O
.	O
The	O
reason	O
is	O
that	O
it	O
provides	O
the	O
most	O
consistent	O
results	O
among	O
all	O
other	O
runs	O
of	O
all	O
systems	O
in	O
the	O
competition	O
.	O
In	O
order	O
to	O
provide	O
additional	O
information	O
about	O
the	O
features	O
effectiveness	O
,	O
we	O
have	O
evaluated	O
them	O
on	O
the	O
final	O
test	O
datasets	O
.	O
In	O
many	O
cases	O
,	O
the	O
obtained	O
results	O
are	O
not	O
conclusive	O
.	O
On	O
some	O
datasets	O
,	O
the	O
features	O
help	O
slightly	O
on	O
others	O
they	O
even	O
decrease	O
the	O
performance	O
.	O
However	O
,	O
the	O
following	O
three	O
features	O
have	O
significant	O
influence	O
on	O
the	O
final	O
results	O
:	O
modified	O
lexical	O
semantic	O
vectors	O
(	O
+3	O
%	O
of	O
the	O
mean	O
of	O
T+S	O
F1	B-MetricName
scores	O
)	O
,	O
shared	O
words	O
(	O
+2	O
%	O
)	O
,	O
POS	O
tags	O
difference	O
(	O
+2	O
%	O
)	O
.	O
The	O
modified	O
lexical	O
semantic	O
vectors	O
method	O
performed	O
better	O
than	O
vector	O
composition	O
by	O
1	O
%	O
for	O
the	O
machine	O
learning	O
approach	O
and	O
by	O
2	O
%	O
for	O
the	O
rule	O
-	O
based	O
approach	O
in	O
average	O
.	O
By	O
optimizing	O
the	O
feature	O
set	O
,	O
we	O
were	O
able	O
to	O
increase	O
the	O
mean	O
score	O
to	O
0.6484	O
of	O
T+S	O
F1	B-MetricName
measure	O
.	O

Given	O
an	O
event	O
phrase	O
,	O
our	O
models	O
aim	O
to	O
generate	O
three	O
entity	O
-	O
specific	O
pragmatic	O
inferences	O
:	O
Per	O
-	O
sonX	O
's	O
intent	O
,	O
PersonX	O
's	O
reaction	O
,	O
and	O
others	O
'	O
reactions	O
.	O
The	O
general	O
outline	O
of	O
our	O
model	O
architecture	O
is	O
illustrated	O
in	O
Figure	O
3	O
.	O
The	O
input	O
to	O
our	O
model	O
is	O
an	O
event	O
pattern	O
described	O
through	O
free	O
-	O
form	O
text	O
with	O
typed	O
variables	O
such	O
as	O
PersonX	O
gives	O
PersonY	O
as	O
a	O
gift	O
.	O
For	O
notation	O
purposes	O
,	O
we	O
describe	O
each	O
event	O
pattern	O
E	O
as	O
a	O
sequence	O
of	O
word	B-TaskName
embeddings	I-TaskName
he	O
1	O
,	O
e	O
2	O
,	O
.	O
.	O
.	O
,	O
e	O
n	O
i	O
2	O
R	O
n	O
⇥	O
D	O
.	O
This	O
input	O
is	O
encoded	O
as	O
a	O
vector	O
h	O
E	O
2	O
R	O
H	O
that	O
will	O
be	O
used	O
for	O
predicting	O
output	O
.	O
The	O
output	O
of	O
the	O
model	O
is	O
its	O
hypotheses	O
about	O
PersonX	O
's	O
intent	O
,	O
PersonX	O
's	O
reaction	O
,	O
and	O
others	O
'	O
reactions	O
(	O
v	O
i	O
,	O
v	O
x	O
,	O
and	O
v	O
o	O
,	O
respectively	O
)	O
.	O
We	O
experiment	O
with	O
representing	O
the	O
From	O
an	O
encoded	O
event	O
,	O
our	O
model	O
predicts	O
intents	O
and	O
reactions	O
in	O
a	O
multitask	O
setting	O
.	O
output	O
in	O
two	O
decoding	O
set	O
-	O
ups	O
:	O
three	O
vectors	O
interpretable	O
as	O
discrete	O
distributions	O
over	O
words	O
and	O
phrases	O
(	O
n	O
-	O
gram	O
reranking	O
)	O
or	O
three	O
sequences	O
of	O
words	O
(	O
sequence	O
decoding	O
)	O
.	O
Encoding	O
events	O
The	O
input	O
event	O
phrase	O
E	O
is	O
compressed	O
into	O
an	O
H	O
-	O
dimensional	O
embedding	O
h	O
E	O
via	O
an	O
encoding	O
function	O
f	O
:	O
R	O
n	O
⇥	O
D	O
!	O
R	O
H	O
:	O
h	O
E	O
=	O
f	O
(	O
e	O
1	O
,	O
.	O
.	O
.	O
,	O
e	O
n	O
)	O
We	O
experiment	O
with	O
several	O
ways	O
for	O
defining	O
f	O
,	O
inspired	O
by	O
standard	O
techniques	O
in	O
sentence	O
and	O
phrase	O
classification	O
(	O
Kim	O
,	O
2014	O
)	O
.	O
First	O
,	O
we	O
experiment	O
with	O
max	O
-	O
pooling	O
and	O
mean	O
-	O
pooling	O
over	O
the	O
word	O
vectors	O
{	O
e	O
i	O
}	O
n	O
i=1	O
.	O
We	O
also	O
consider	O
a	O
convolutional	O
neural	O
network	O
(	O
ConvNet	O
;	O
LeCun	O
et	O
al	O
,	O
1998	O
)	O
taking	O
the	O
last	O
layer	O
of	O
the	O
network	O
as	O
the	O
encoded	O
version	O
of	O
the	O
event	O
.	O
Lastly	O
,	O
we	O
encode	O
the	O
event	O
phrase	O
with	O
a	O
bi	O
-	O
directional	O
RNN	O
(	O
specifically	O
,	O
a	O
GRU	B-MethodName
;	O
Cho	O
et	O
al	O
,	O
2014	O
)	O
,	O
concatenating	O
the	O
final	O
hidden	O
states	O
of	O
the	O
forward	O
and	O
backward	O
cells	O
as	O
the	O
encoding	O
:	O
h	O
E	O
=	O
[	O
!	O
h	O
n	O
;	O
h	O
1	O
]	O
.	O
For	O
hyperparameters	O
and	O
other	O
details	O
,	O
we	O
refer	O
the	O
reader	O
to	O
Appendix	O
B.	O
Though	O
the	O
event	O
sequences	O
are	O
typically	O
rather	O
short	O
(	O
4.6	O
tokens	O
on	O
average	O
)	O
,	O
our	O
model	O
still	O
benefits	O
from	O
the	O
ConvNet	O
and	O
BiRNN	O
's	O
ability	O
to	O
compose	O
words	O
.	O
Pragmatic	O
inference	O
decoding	O
We	O
use	O
three	O
decoding	O
modules	O
that	O
take	O
the	O
event	O
phrase	O
embedding	O
h	O
E	O
and	O
output	O
distributions	O
of	O
possible	O
PersonX	O
's	O
intent	O
(	O
v	O
i	O
)	O
,	O
PersonX	O
's	O
reactions	O
(	O
v	O
x	O
)	O
,	O
and	O
others	O
'	O
reactions	O
(	O
v	O
o	O
)	O
.	O
We	O
experiment	O
with	O
two	O
different	O
decoder	O
set	O
-	O
ups	O
.	O
First	O
,	O
we	O
experiment	O
with	O
n	O
-	O
gram	O
re	O
-	O
ranking	O
,	O
considering	O
the	O
|	O
V	O
|	O
most	O
frequent	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
grams	O
in	O
our	O
annotations	O
.	O
Each	O
decoder	O
projects	O
the	O
event	O
phrase	O
embedding	O
h	O
E	O
into	O
a	O
|	O
V	O
|	O
dimensional	O
vector	O
,	O
which	O
is	O
then	O
passed	O
through	O
a	O
softmax	B-MethodName
function	O
.	O
For	O
instance	O
,	O
the	O
distribution	O
over	O
descriptions	O
of	O
PersonX	O
's	O
intent	O
is	O
given	O
by	O
:	O
v	O
i	O
=	O
softmax	B-MethodName
(	O
W	O
i	O
h	O
E	O
+	O
b	O
i	O
)	O
Second	O
,	O
we	O
experiment	O
with	O
sequence	O
generation	O
,	O
using	O
RNN	O
decoders	O
to	O
generate	O
the	O
textual	O
description	O
.	O
The	O
event	O
phrase	O
embedding	O
h	O
E	O
is	O
set	O
as	O
the	O
initial	O
state	O
h	O
dec	O
of	O
three	O
decoder	O
RNNs	O
(	O
using	O
GRU	B-MethodName
cells	O
)	O
,	O
which	O
then	O
output	O
the	O
intent	O
/	O
reactions	O
one	O
word	O
at	O
a	O
time	O
(	O
using	O
beam	O
-	O
search	O
at	O
test	O
time	O
)	O
.	O
For	O
example	O
,	O
an	O
event	O
's	O
intent	O
sequence	O
(	O
v	O
i	O
=	O
v	O
(	O
0	B-DatasetName
)	O
i	O
v	O
(	O
1	O
)	O
i	O
.	O
.	O
.	O
)	O
is	O
computed	O
as	O
follows	O
:	O
v	O
(	O
t+1	O
)	O
i	O
=	O
softmax	B-MethodName
(	O
W	O
i	O
RNN	O
(	O
v	O
(	O
t	O
)	O
i	O
,	O
h	O
(	O
t	O
)	O
i	O
,	O
dec	O
)	O
+	O
b	O
i	O
)	O
Training	O
objective	O
We	O
minimize	O
the	O
crossentropy	O
between	O
the	O
predicted	O
distribution	O
over	O
words	O
and	O
phrases	O
,	O
against	O
the	O
one	O
actually	O
observed	O
in	O
our	O
dataset	O
.	O
Further	O
,	O
we	O
employ	O
multitask	O
learning	O
,	O
simultaneously	O
minimizing	O
the	O
loss	B-MetricName
for	O
all	O
three	O
decoders	O
at	O
each	O
iteration	O
.	O
Training	O
details	O
We	O
fix	O
our	O
input	O
embeddings	O
,	O
using	O
300	O
-	O
dimensional	O
skip	O
-	O
gram	O
word	B-TaskName
embeddings	I-TaskName
trained	O
on	O
Google	B-DatasetName
News	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
For	O
decoding	O
,	O
we	O
consider	O
a	O
vocabulary	O
of	O
size	O
|	O
V	O
|	O
=	O
14	O
,	O
034	O
in	O
the	O
n	O
-	O
gram	O
re	O
-	O
ranking	O
setup	O
.	O
For	O
the	O
sequence	O
decoding	O
setup	O
,	O
we	O
only	O
consider	O
the	O
unigrams	O
in	O
V	O
,	O
yielding	O
an	O
output	O
space	O
of	O
7	O
,	O
110	O
at	O
each	O
time	O
step	O
.	O
We	O
randomly	O
divided	O
our	O
set	O
of	O
24	O
,	O
716	O
unique	O
events	O
(	O
57	O
,	O
094	O
annotations	O
)	O
into	O
a	O
training	O
/	O
dev./test	O
set	O
using	O
an	O
80/10/10	O
%	O
split	O
.	O
Some	O
annotations	O
have	O
multiple	O
responses	O
(	O
i.e.	O
,	O
a	O
crowdworker	O
gave	O
multiple	O
possible	O
intents	O
and	O
reactions	O
)	O
,	O
in	O
which	O
case	O
we	O
take	O
each	O
of	O
the	O
combinations	O
of	O
their	O
responses	O
as	O
a	O
separate	O
training	O
example	O
.	O

Neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
is	O
one	O
of	O
the	O
core	O
topics	O
in	O
natural	O
language	O
processing	O
,	O
which	O
aims	O
to	O
generate	O
sequences	O
of	O
words	O
in	O
the	O
target	O
language	O
conditioned	O
on	O
the	O
source	O
inputs	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
;	O
Cho	O
et	O
al	O
,	O
2014	O
;	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
In	O
the	O
common	O
supervised	O
setting	O
,	O
the	O
training	O
objective	O
is	O
to	O
learn	O
a	O
transformation	O
from	O
the	O
source	O
space	O
to	O
the	O
target	O
space	O
X	O
Y	O
:	O
f	O
(	O
y	O
|	O
x	O
;	O
Θ	B-HyperparameterName
)	O
with	O
the	O
usage	O
of	O
parallel	O
data	O
.	O
In	O
this	O
way	O
,	O
NMT	O
models	O
are	O
expected	O
to	O
1	O
The	O
core	O
codes	O
are	O
contained	O
in	O
Appendix	O
E.	O
be	O
capable	O
of	O
generalizing	O
to	O
unseen	O
instances	O
with	O
the	O
help	O
of	O
large	O
scale	O
training	O
data	O
,	O
which	O
poses	O
a	O
big	O
challenge	O
for	O
scenarios	O
with	O
limited	O
resources	O
.	O
To	O
address	O
this	O
problem	O
,	O
various	O
methods	O
have	O
been	O
developed	O
to	O
leverage	O
abundant	O
unlabeled	O
data	O
for	O
augmenting	O
limited	O
labeled	O
data	O
(	O
Sennrich	O
et	O
al	O
,	O
2016a	O
;	O
Cheng	O
et	O
al	O
,	O
2016	O
;	O
Hoang	O
et	O
al	O
,	O
2018	O
;	O
Song	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
example	O
,	O
backtranslation	O
(	O
BT	O
)	O
(	O
Sennrich	O
et	O
al	O
,	O
2016a	O
)	O
makes	O
use	O
of	O
the	O
monolingual	O
data	O
on	O
the	O
target	O
side	O
to	O
synthesize	O
large	O
scale	O
pseudo	O
parallel	O
data	O
,	O
which	O
is	O
further	O
combined	O
with	O
the	O
real	O
parallel	O
corpus	O
in	O
machine	B-TaskName
translation	I-TaskName
task	O
.	O
Another	O
line	O
of	O
research	O
is	O
to	O
introduce	O
adversarial	O
inputs	O
to	O
improve	O
the	O
generalization	O
of	O
NMT	O
models	O
towards	O
small	O
perturbations	O
(	O
Iyyer	O
et	O
al	O
,	O
2015	O
;	O
Fadaee	O
et	O
al	O
,	O
2017	O
;	O
Wang	O
et	O
al	O
,	O
2018	O
;	O
Cheng	O
et	O
al	O
,	O
2018	O
;	O
Gao	O
et	O
al	O
,	O
2019	O
)	O
.	O
While	O
these	O
methods	O
lead	O
to	O
significant	O
boosts	O
in	O
translation	O
quality	O
,	O
we	O
argue	O
that	O
augmenting	O
the	O
observed	O
training	O
data	O
in	O
the	O
discrete	O
space	O
inherently	O
has	O
two	O
major	O
limitations	O
.	O
First	O
,	O
augmented	O
training	O
instances	O
in	O
discrete	O
space	O
are	O
lack	O
diversity	O
.	O
We	O
still	O
take	O
BT	O
as	O
an	O
example	O
,	O
it	O
typically	O
uses	O
beam	O
search	O
(	O
Sennrich	O
et	O
al	O
,	O
2016a	O
)	O
or	O
greedy	O
search	O
(	O
Lample	O
et	O
al	O
,	O
2018a	O
,	O
c	O
)	O
to	O
generate	O
synthetic	O
source	O
sentences	O
for	O
each	O
target	O
monolingual	O
sentence	O
.	O
The	O
above	O
two	O
search	O
strategies	O
are	O
approximate	O
algorithms	O
to	O
identify	O
the	O
maximum	O
a	O
-	O
posteriori	O
(	O
MAP	B-DatasetName
)	O
output	O
,	O
and	O
thus	O
favor	O
the	O
most	O
frequent	O
one	O
in	O
case	O
of	O
ambiguity	O
.	O
proposed	O
a	O
sampling	O
strategy	O
from	O
the	O
output	O
distribution	O
to	O
alleviate	O
this	O
issue	O
,	O
but	O
this	O
method	O
typically	O
yields	O
synthesized	O
data	O
with	O
low	O
quality	O
.	O
While	O
some	O
extensions	O
(	O
Wang	O
et	O
al	O
,	O
2018	O
;	O
Imamura	O
et	O
al	O
,	O
2018	O
;	O
Khayrallah	O
et	O
al	O
,	O
2020	O
;	O
Nguyen	O
et	O
al	O
,	O
2020	O
)	O
augment	O
each	O
training	O
instance	O
with	O
multiple	O
literal	O
forms	O
,	O
they	O
still	O
fail	O
to	O
cover	O
adequate	O
variants	O
under	O
the	O
same	O
meaning	O
.	O
Second	O
,	O
it	O
is	O
difficult	O
for	O
augmented	O
texts	O
in	O
dis	O
-	O
crete	O
space	O
to	O
preserve	O
their	O
original	O
meanings	O
.	O
In	O
the	O
context	O
of	O
natural	O
language	O
processing	O
,	O
discrete	O
manipulations	O
such	O
as	O
adds	O
,	O
drops	O
,	O
reorders	O
,	O
and/or	O
replaces	O
words	O
in	O
the	O
original	O
sentences	O
often	O
result	O
in	O
significant	O
changes	O
in	O
semantics	O
.	O
To	O
address	O
this	O
issue	O
,	O
Gao	O
et	O
al	O
(	O
2019	O
)	O
and	O
Cheng	O
et	O
al	O
(	O
2020	O
)	O
instead	O
replace	O
words	O
with	O
other	O
words	O
that	O
are	O
predicted	O
using	O
language	O
model	O
under	O
the	O
same	O
context	O
,	O
by	O
interpolating	O
their	O
embeddings	O
.	O
Although	O
being	O
effective	O
,	O
these	O
techniques	O
are	O
limited	O
to	O
word	O
-	O
level	O
manipulation	O
and	O
are	O
unable	O
to	O
perform	O
the	O
whole	O
sentence	O
transformation	O
,	O
such	O
as	O
producing	O
another	O
sentence	O
by	O
rephrasing	O
the	O
original	O
one	O
so	O
that	O
they	O
have	O
the	O
same	O
meaning	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
Continuous	O
Semantic	O
Augmentation	O
(	O
CSANMT	O
)	O
,	O
a	O
novel	O
data	B-TaskName
augmentation	I-TaskName
paradigm	O
for	O
NMT	O
,	O
to	O
alleviate	O
both	O
limitations	O
mentioned	O
above	O
.	O
The	O
principle	O
of	O
CSANMT	O
is	O
to	O
produce	O
diverse	O
training	O
data	O
from	O
a	O
semantically	O
-	O
preserved	O
continuous	O
space	O
.	O
Specifically	O
,	O
(	O
1	O
)	O
we	O
first	O
train	O
a	O
semantic	O
encoder	O
via	O
a	O
tangential	O
contrast	O
,	O
which	O
encourages	O
each	O
training	O
instance	O
to	O
support	O
an	O
adjacency	O
semantic	O
region	O
in	O
continuous	O
space	O
and	O
treats	O
the	O
tangent	O
points	O
of	O
the	O
region	O
as	O
the	O
critical	O
states	O
of	O
semantic	O
equivalence	O
.	O
This	O
is	O
motivated	O
by	O
the	O
intriguing	O
observation	O
made	O
by	O
recent	O
work	O
showing	O
that	O
the	O
vectors	O
in	O
continuous	O
space	O
can	O
easily	O
cover	O
adequate	O
variants	O
under	O
the	O
same	O
meaning	O
(	O
Wei	O
et	O
al	O
,	O
2020a	O
)	O
.	O
(	O
2	O
)	O
We	O
then	O
introduce	O
a	O
Mixed	O
Gaussian	O
Recurrent	O
Chain	O
(	O
MGRC	O
)	O
algorithm	O
to	O
sample	O
a	O
cluster	O
of	O
vectors	O
from	O
the	O
adjacency	O
semantic	O
region	O
.	O
(	O
3	O
)	O
Each	O
of	O
the	O
sampled	O
vectors	O
is	O
finally	O
incorporated	O
into	O
the	O
decoder	O
by	O
developing	O
a	O
broadcasting	O
integration	O
network	O
,	O
which	O
is	O
agnostic	O
to	O
model	O
architectures	O
.	O
As	O
a	O
consequence	O
,	O
transforming	O
discrete	O
sentences	O
into	O
the	O
continuous	O
space	O
can	O
effectively	O
augment	O
the	O
training	O
data	O
space	O
and	O
thus	O
improve	O
the	O
generalization	O
capability	O
of	O
NMT	O
models	O
.	O
We	O
evaluate	O
our	O
framework	O
on	O
a	O
variety	O
of	O
machine	B-TaskName
translation	I-TaskName
tasks	O
,	O
including	O
WMT14	B-DatasetName
English	O
-	O
German	O
/	O
French	O
,	O
NIST	O
Chinese	O
-	O
English	O
and	O
multiple	O
IWSLT	O
tasks	O
.	O
Specifically	O
,	O
CSANMT	O
sets	O
the	O
new	O
state	O
of	O
the	O
art	O
among	O
existing	O
augmentation	O
techniques	O
on	O
the	O
WMT14	B-DatasetName
English	O
-	O
German	O
task	O
with	O
30.94	O
BLEU	B-MetricName
score	I-MetricName
.	O
In	O
addition	O
,	O
our	O
approach	O
could	O
achieve	O
comparable	O
performance	O
with	O
the	O
baseline	O
model	O
with	O
the	O
usage	O
of	O
only	O
25	O
%	O
of	O
training	O
data	O
.	O
This	O
reveals	O
that	O
CSANMT	O
has	O
great	O
potential	O
to	O
achieve	O
good	O
results	O
with	O
very	O
few	O
data	O
.	O
Furthermore	O
,	O
CSANMT	O
demonstrates	O
consistent	O
improvements	O
over	O
strong	O
baselines	O
in	O
low	O
resource	O
scenarios	O
,	O
such	O
as	O
IWSLT14	O
English	O
-	O
German	O
and	O
IWSLT17	O
English	O
-	O
French	O
.	O

Problem	O
Definition	O
Supposing	O
X	O
and	O
Y	O
are	O
two	O
data	O
spaces	O
that	O
cover	O
all	O
possible	O
sequences	O
of	O
words	O
in	O
source	O
and	O
target	O
languages	O
,	O
respectively	O
.	O
We	O
denote	O
(	O
x	O
,	O
y	O
)	O
(	O
X	O
,	O
Y	O
)	O
as	O
a	O
pair	O
of	O
two	O
sentences	O
with	O
the	O
same	O
meaning	O
,	O
where	O
x	O
=	O
{	O
x	O
1	O
,	O
x	O
2	O
,	O
...	O
,	O
x	O
T	O
}	O
is	O
the	O
source	O
sentence	O
with	O
T	O
tokens	O
,	O
and	O
y	O
=	O
{	O
y	O
1	O
,	O
y	O
2	O
,	O
...	O
,	O
y	O
T	O
′	O
}	O
is	O
the	O
target	O
sentence	O
with	O
T	O
′	O
tokens	O
.	O
A	O
sequence	O
-	O
tosequence	O
model	O
is	O
usually	O
applied	O
to	O
neural	O
machine	B-TaskName
translation	I-TaskName
,	O
which	O
aims	O
to	O
learn	O
a	O
transformation	O
from	O
the	O
source	O
space	O
to	O
the	O
target	O
space	O
X	O
Y	O
:	O
f	O
(	O
y	O
|	O
x	O
;	O
Θ	B-HyperparameterName
)	O
with	O
the	O
usage	O
of	O
parallel	O
data	O
.	O
Formally	O
,	O
given	O
a	O
set	O
of	O
observed	O
sentence	O
pairs	O
C	O
=	O
{	O
(	O
x	O
(	O
n	O
)	O
,	O
y	O
(	O
n	O
)	O
)	O
}	O
N	O
n=1	O
,	O
the	O
training	O
objective	O
is	O
to	O
maximize	O
the	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
:	O
J	O
mle	O
(	O
Θ	B-HyperparameterName
)	O
=	O
E	O
(	O
x	O
,	O
y	O
)	O
∼C	O
log	O
P	O
(	O
y	O
|	O
x	O
;	O
Θ	B-HyperparameterName
)	O
.	O
(	O
1	O
)	O
The	O
log	O
-	O
probability	O
is	O
typically	O
decomposed	O
as	O
:	O
log	O
P	O
(	O
y	O
|	O
x	O
;	O
Θ	B-HyperparameterName
)	O
=	O
T	O
′	O
t=1	O
log	O
P	O
(	O
y	O
t	O
|	O
y	O
<	O
t	O
,	O
x	O
;	O
Θ	B-HyperparameterName
)	O
,	O
where	O
Θ	B-HyperparameterName
is	O
a	O
set	O
of	O
trainable	O
parameters	O
and	O
y	O
<	O
t	O
is	O
a	O
partial	O
sequence	O
before	O
time	O
-	O
step	O
t.	O
However	O
,	O
there	O
is	O
a	O
major	O
problem	O
in	O
the	O
common	O
supervised	O
setting	O
for	O
neural	O
machine	B-TaskName
translation	I-TaskName
,	O
that	O
is	O
the	O
number	O
of	O
training	O
instances	O
is	O
very	O
limited	O
because	O
of	O
the	O
cost	O
in	O
acquiring	O
parallel	O
data	O
.	O
This	O
makes	O
it	O
difficult	O
to	O
learn	O
an	O
NMT	O
model	O
generalized	O
well	O
to	O
unseen	O
instances	O
.	O
Traditional	O
data	B-TaskName
augmentation	I-TaskName
methods	O
generate	O
more	O
training	O
samples	O
by	O
applying	O
discrete	O
manipulations	O
to	O
unlabeled	O
(	O
or	O
labeled	O
)	O
data	O
,	O
such	O
as	O
back	O
-	O
translation	O
or	O
randomly	O
replacing	O
a	O
word	O
with	O
another	O
one	O
,	O
which	O
usually	O
suffer	O
from	O
the	O
problems	O
of	O
semantic	O
deviation	O
and	O
the	O
lack	O
of	O
diversity	O
.	O

We	O
propose	O
a	O
novel	O
data	B-TaskName
augmentation	I-TaskName
paradigm	O
for	O
neural	O
machine	B-TaskName
translation	I-TaskName
,	O
termed	O
continuous	O
semantic	O
augmentation	O
(	O
CSANMT	O
)	O
,	O
to	O
better	O
generalize	O
the	O
model	O
's	O
capability	O
to	O
unseen	O
instances	O
.	O
We	O
adopt	O
the	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
model	O
as	O
a	O
backbone	O
,	O
and	O
the	O
framework	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
In	O
this	O
architecture	O
,	O
an	O
extra	O
semantic	O
encoder	O
translates	O
the	O
source	O
x	O
and	O
the	O
target	O
sentence	O
y	O
to	O
real	O
-	O
value	O
vectors	O
r	O
x	O
=	O
ψ	O
(	O
x	O
;	O
Θ	B-HyperparameterName
′	O
)	O
and	O
r	O
y	O
=	O
ψ	O
(	O
y	O
;	O
Θ	B-HyperparameterName
′	O
)	O
respectively	O
,	O
where	O
ψ	O
(	O
;	O
Θ	B-HyperparameterName
′	O
)	O
is	O
the	O
forward	O
function	O
of	O
the	O
semantic	O
encoder	O
parameterized	O
by	O
Θ	B-HyperparameterName
′	O
(	O
parameters	O
other	O
than	O
Θ	B-HyperparameterName
)	O
.	O
∀	O
(	O
x	O
,	O
y	O
)	O
(	O
X	O
,	O
Y	O
)	O
:	O
r	O
x	O
=	O
r	O
y	O
.	O
Besides	O
,	O
an	O
adjacency	O
semantic	O
region	O
ν	O
(	O
r	O
x	O
,	O
r	O
y	O
)	O
in	O
the	O
semantic	O
space	O
describes	O
adequate	O
variants	O
of	O
literal	O
expression	O
centered	O
around	O
each	O
observed	O
sentence	O
pair	O
(	O
x	O
,	O
y	O
)	O
.	O
In	O
our	O
scenario	O
,	O
we	O
first	O
sample	O
a	O
series	O
of	O
vectors	O
(	O
denoted	O
by	O
R	O
)	O
from	O
the	O
adjacency	O
semantic	O
region	O
to	O
augment	O
the	O
current	O
training	O
instance	O
,	O
that	O
is	O
R	O
=	O
{	O
r	O
(	O
1	O
)	O
,	O
r	O
(	O
2	O
)	O
,	O
...	O
,	O
r	O
(	O
K	O
)	O
}	O
,	O
wherer	O
(	O
k	O
)	O
∼	O
ν	O
(	O
r	O
x	O
,	O
r	O
y	O
)	O
.	O
K	O
is	O
the	O
hyperparameter	O
that	O
determines	O
the	O
number	O
of	O
sampled	O
vectors	O
.	O
Each	O
sampler	O
(	O
k	O
)	O
is	O
then	O
integrated	O
into	O
the	O
generation	O
process	O
through	O
a	O
broadcasting	O
integration	O
network	O
:	O
ot	O
=	O
W1r	O
(	O
k	O
)	O
+	O
W2ot	O
+	O
b	O
,	O
(	O
2	O
)	O
where	O
o	O
t	O
is	O
the	O
output	O
of	O
the	O
self	O
-	O
attention	O
module	O
at	O
position	O
t.	O
Finally	O
,	O
the	O
training	O
objective	O
in	O
Eq	O
.	O
(	O
1	O
)	O
can	O
be	O
improved	O
as	O
J	O
mle	O
(	O
Θ	B-HyperparameterName
)	O
=	O
E	O
(	O
x	O
,	O
y	O
)	O
∼C	O
,	O
r	O
(	O
k	O
)	O
R	O
log	O
P	O
(	O
y	O
|	O
x	O
,	O
r	O
(	O
k	O
)	O
;	O
Θ	B-HyperparameterName
)	O
.	O
(	O
3	O
)	O
By	O
augmenting	O
the	O
training	O
instance	O
(	O
x	O
,	O
y	O
)	O
with	O
diverse	O
samples	O
from	O
the	O
adjacency	O
semantic	O
region	O
,	O
the	O
model	O
is	O
expected	O
to	O
generalize	O
to	O
more	O
unseen	O
instances	O
.	O
To	O
this	O
end	O
,	O
we	O
must	O
consider	O
such	O
two	O
problems	O
:	O
(	O
1	O
)	O
How	O
to	O
optimize	O
the	O
semantic	O
encoder	O
so	O
that	O
it	O
produces	O
a	O
meaningful	O
adjacency	O
semantic	O
region	O
for	O
each	O
observed	O
training	O
pair	O
.	O
(	O
i	O
)	O
,	O
y	O
(	O
i	O
)	O
)	O
.	O
(	O
2	O
)	O
How	O
to	O
obtain	O
samples	O
from	O
the	O
adjacency	O
semantic	O
region	O
in	O
an	O
efficient	O
and	O
effective	O
way	O
.	O
In	O
the	O
rest	O
part	O
of	O
this	O
section	O
,	O
we	O
introduce	O
the	O
resolutions	O
of	O
these	O
two	O
problems	O
,	O
respectively	O
.	O
Tangential	O
Contrastive	B-MethodName
Learning	I-MethodName
We	O
start	O
from	O
analyzing	O
the	O
geometric	O
interpretation	O
of	O
adjacency	O
semantic	O
regions	O
.	O
The	O
schematic	O
diagram	O
is	O
illustrated	O
in	O
Figure	O
2	O
.	O
Let	O
(	O
x	O
(	O
i	O
)	O
,	O
y	O
(	O
i	O
)	O
)	O
and	O
(	O
x	O
(	O
j	O
)	O
,	O
y	O
(	O
j	O
)	O
)	O
are	O
two	O
instances	O
randomly	O
sampled	O
from	O
the	O
training	O
corpora	O
.	O
For	O
(	O
x	O
(	O
i	O
)	O
,	O
y	O
(	O
i	O
)	O
)	O
,	O
the	O
adjacency	O
semantic	O
region	O
ν	O
(	O
r	O
x	O
(	O
i	O
)	O
,	O
r	O
y	O
(	O
i	O
)	O
)	O
is	O
defined	O
as	O
the	O
union	O
of	O
two	O
closed	O
balls	O
that	O
are	O
centered	O
by	O
r	O
x	O
(	O
i	O
)	O
and	O
r	O
y	O
(	O
i	O
)	O
,	O
respectively	O
.	O
The	O
radius	O
of	O
both	O
balls	O
is	O
d	O
=	O
∥	O
r	O
x	O
(	O
i	O
)	O
−	O
r	O
y	O
(	O
i	O
)	O
∥	O
2	O
,	O
which	O
is	O
also	O
considered	O
as	O
a	O
slack	O
variable	O
for	O
determining	O
semantic	O
equivalence	O
.	O
The	O
underlying	O
interpretation	O
is	O
that	O
vectors	O
whose	O
distances	O
from	O
r	O
x	O
(	O
i	O
)	O
(	O
or	O
r	O
y	O
(	O
i	O
)	O
)	O
do	O
not	O
exceed	O
d	O
,	O
are	O
semantically	O
-	O
equivalent	O
to	O
both	O
r	O
x	O
(	O
i	O
)	O
and	O
r	O
y	O
(	O
i	O
)	O
.	O
To	O
make	O
ν	O
(	O
r	O
x	O
(	O
i	O
)	O
,	O
r	O
y	O
(	O
i	O
)	O
)	O
conform	O
to	O
the	O
interpretation	O
,	O
we	O
employ	O
a	O
similar	O
method	O
as	O
in	O
(	O
Zheng	O
et	O
al	O
,	O
2019	O
;	O
to	O
optimize	O
the	O
semantic	O
encoder	O
with	O
the	O
tangential	O
contrast	O
.	O
Specifically	O
,	O
we	O
construct	O
negative	O
samples	O
by	O
applying	O
the	O
convex	O
interpolation	O
between	O
the	O
current	O
instance	O
and	O
other	O
ones	O
in	O
the	O
same	O
training	O
batch	O
for	O
instance	O
comparison	O
.	O
And	O
the	O
tangent	O
points	O
(	O
i.e.	O
,	O
the	O
points	O
on	O
the	O
boundary	O
)	O
are	O
considered	O
as	O
the	O
critical	O
states	O
of	O
semantic	O
equivalence	O
.	O
The	O
training	O
objective	O
is	O
formulated	O
as	O
:	O
J	O
ctl	O
(	O
Θ	B-HyperparameterName
′	O
)	O
=	O
E	O
(	O
x	O
(	O
i	O
)	O
,	O
y	O
(	O
i	O
)	O
)	O
∼B	O
log	O
e	O
s	O
r	O
x	O
(	O
i	O
)	O
,	O
r	O
y	O
(	O
i	O
)	O
e	O
s	O
r	O
x	O
(	O
i	O
)	O
,	O
r	O
y	O
(	O
i	O
)	O
+	O
ξ	O
,	O
ξ	O
=	O
|	O
B	O
|	O
j&j̸	O
=	O
i	O
e	O
s	O
r	O
y	O
(	O
i	O
)	O
,	O
r	O
y	O
′	O
(	O
j	O
)	O
+	O
e	O
s	O
r	O
x	O
(	O
i	O
)	O
,	O
r	O
x	O
′	O
(	O
j	O
)	O
,	O
(	O
4	O
)	O
where	O
B	O
indicates	O
a	O
batch	O
of	O
sentence	O
pairs	O
randomly	O
selected	O
from	O
the	O
training	O
corpora	O
C	O
,	O
and	O
s	O
(	O
)	O
is	O
the	O
score	O
function	O
that	O
computes	O
the	O
cosine	O
similarity	O
between	O
two	O
vectors	O
.	O
The	O
negative	O
samples	O
r	O
x	O
′	O
(	O
j	O
)	O
and	O
r	O
y	O
′	O
(	O
j	O
)	O
are	O
designed	O
as	O
the	O
following	O
interpolation	O
:	O
r	O
x	O
′	O
(	O
j	O
)	O
=	O
r	O
x	O
(	O
i	O
)	O
+	O
λx	O
(	O
r	O
x	O
(	O
j	O
)	O
−	O
r	O
x	O
(	O
i	O
)	O
)	O
,	O
λx	O
(	O
d	O
d	O
′	O
x	O
,	O
1	O
]	O
,	O
r	O
y	O
′	O
(	O
j	O
)	O
=	O
r	O
y	O
(	O
i	O
)	O
+	O
λy	O
(	O
r	O
y	O
(	O
j	O
)	O
−	O
r	O
y	O
(	O
i	O
)	O
)	O
,	O
λy	O
(	O
d	O
d	O
′	O
y	O
,	O
1	O
]	O
,	O
(	O
5	O
)	O
where	O
d	O
′	O
x	O
=	O
∥	O
r	O
x	O
(	O
i	O
)	O
−	O
r	O
x	O
(	O
j	O
)	O
∥	O
and	O
d	O
′	O
y	O
=	O
∥	O
r	O
y	O
(	O
i	O
)	O
−	O
r	O
y	O
(	O
j	O
)	O
∥.	O
The	O
two	O
equations	O
in	O
Eq	O
.	O
(	O
5	O
)	O
set	O
up	O
when	O
d	O
′	O
x	O
and	O
d	O
′	O
y	O
are	O
larger	O
than	O
d	O
respectively	O
,	O
or	O
else	O
r	O
x	O
′	O
(	O
j	O
)	O
=	O
r	O
x	O
(	O
j	O
)	O
and	O
r	O
y	O
′	O
(	O
j	O
)	O
=	O
r	O
y	O
(	O
j	O
)	O
.	O
According	O
to	O
this	O
design	O
,	O
an	O
adjacency	O
semantic	O
region	O
for	O
the	O
i	O
-	O
th	O
training	O
instance	O
can	O
be	O
fully	O
established	O
by	O
interpolating	O
various	O
instances	O
in	O
the	O
same	O
training	O
batch	O
.	O
We	O
follow	O
to	O
adaptively	O
adjust	O
the	O
value	O
of	O
λ	O
x	O
(	O
or	O
λ	O
y	O
)	O
during	O
the	O
training	O
process	O
,	O
and	O
refer	O
to	O
the	O
original	O
paper	O
for	O
details	O
.	O
MGRC	O
Sampling	O
To	O
obtain	O
augmented	O
data	O
from	O
the	O
adjacency	O
semantic	O
region	O
for	O
the	O
training	O
instance	O
(	O
x	O
,	O
y	O
)	O
,	O
we	O
introduce	O
a	O
Mixed	O
Gaussian	O
Recurrent	O
Chain	O
(	O
denoted	O
by	O
MGRC	O
)	O
algorithm	O
to	O
design	O
an	O
efficient	O
and	O
effective	O
sampling	O
strategy	O
.	O
As	O
illustrated	O
in	O
Figure	O
3	O
,	O
we	O
first	O
transform	O
the	O
bias	O
vectorr	O
=	O
r	O
y	O
−	O
r	O
x	O
according	O
to	O
a	O
predefined	O
scale	O
vector	O
ω	O
,	O
that	O
is	O
ω	O
r	O
,	O
where	O
is	O
the	O
element	O
-	O
wise	O
product	O
operation	O
.	O
Then	O
,	O
we	O
construct	O
a	O
novel	O
sampler	O
=	O
r	O
+	O
ω	O
r	O
for	O
augmenting	O
the	O
current	O
instance	O
,	O
in	O
which	O
r	O
is	O
either	O
r	O
x	O
or	O
r	O
y	O
.	O
As	O
a	O
consequence	O
,	O
the	O
goal	O
of	O
the	O
sampling	O
strategy	O
turns	O
into	O
find	O
a	O
set	O
of	O
scale	O
vectors	O
,	O
i.e.	O
{	O
ω	O
(	O
1	O
)	O
,	O
ω	O
(	O
2	O
)	O
,	O
...	O
,	O
ω	O
(	O
K	O
)	O
}	O
.	O
Intuitively	O
,	O
we	O
can	O
assume	O
that	O
ω	O
follows	O
a	O
distribution	O
with	O
universal	O
or	O
Gaussian	O
forms	O
,	O
despite	O
the	O
latter	O
demonstrates	O
better	O
results	O
in	O
our	O
experience	O
.	O
Formally	O
,	O
we	O
design	O
a	O

Input	O
:	O
The	O
representations	O
of	O
the	O
training	O
instance	O
(	O
x	O
,	O
y	O
)	O
,	O
i.e.	O
rx	O
and	O
ry	O
.	O
Output	O
:	O
A	O
set	O
of	O
augmented	O
samples	O
R	O
=	O
{	O
r	O
(	O
1	O
)	O
,	O
r	O
(	O
2	O
)	O
,	O
...	O
,	O
r	O
(	O
K	O
)	O
}	O
1	O
:	O
Normalizing	O
the	O
importance	O
of	O
each	O
element	O
inr	O
=	O
ry	O
−	O
rx	O
:	O
Wr	O
=	O
|	O
r	O
|	O
−min	O
(	O
|	O
r	O
|	O
)	O
max	O
(	O
|	O
r	O
|	O
)	O
−min	O
(	O
|	O
r	O
|	O
)	O
2	O
:	O
Set	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
,	O
ω	O
(	O
1	O
)	O
∼	O
N	O
(	O
0	B-DatasetName
,	O
diag	O
(	O
W	O
2	O
r	O
)	O
)	O
,	O
r	O
(	O
1	O
)	O
=	O
r	O
+	O
ω	O
(	O
1	O
)	O
(	O
ry	O
−	O
rx	O
)	O
3	O
:	O
Initialize	O
the	O
set	O
of	O
samples	O
as	O
R	O
=	O
{	O
r	O
(	O
1	O
)	O
}	O
.	O
4	O
:	O
while	O
k	O
≤	O
(	O
K	O
−	O
1	O
)	O
do	O
5	O
:	O
k	O
k	O
+	O
1	O
6	O
:	O
Calculate	O
the	O
current	O
scale	O
vector	O
:	O
ω	O
(	O
k	O
)	O
∼	O
p	O
(	O
ω	O
|	O
ω	O
(	O
1	O
)	O
,	O
ω	O
(	O
2	O
)	O
,	O
...	O
,	O
ω	O
(	O
k−1	O
)	O
according	O
to	O
Eq	O
.	O
(	O
6	O
)	O
.	O
7	O
:	O
Calculate	O
the	O
current	O
sample	O
:	O
r	O
(	O
k	O
)	O
=	O
r	O
+	O
ω	O
(	O
k	O
)	O
(	O
ry	O
−	O
rx	O
)	O
.	O

R	O
R	O
{	O
r	O
(	O
k	O
)	O
}	O
.	O
9	O
:	O
end	O
while	O
mixed	O
Gaussian	O
distribution	O
as	O
follow	O
:	O
ω	O
(	O
k	O
)	O
∼	O
p	O
(	O
ω	O
|	O
ω	O
(	O
1	O
)	O
,	O
ω	O
(	O
2	O
)	O
,	O
...	O
,	O
ω	O
(	O
k−1	O
)	O
)	O
,	O
p	O
=	O
ηN	O
0	B-DatasetName
,	O
diag	O
(	O
W	O
2	O
r	O
)	O
+	O
(	O
1.0	O
−	O
η	O
)	O
N	O
1	O
k	O
−	O
1	O
k−1	O
i=1	O
ω	O
(	O
i	O
)	O
,	O
1	O
.	O
(	O
6	O
)	O
This	O
framework	O
unifies	O
the	O
recurrent	O
chain	O
and	O
the	O
rejection	O
sampling	O
mechanism	O
.	O
Concretely	O
,	O
we	O
first	O
normalize	O
the	O
importance	O
of	O
each	O
dimension	O
inr	O
as	O
W	O
r	O
=	O
|	O
r	O
|	O
−min	O
(	O
|	O
r	O
|	O
)	O
max	O
(	O
|	O
r	O
|	O
)	O
−min	O
(	O
|	O
r	O
|	O
)	O
,	O
the	O
operation	O
|	O
|	O
takes	O
the	O
absolute	O
value	O
of	O
each	O
element	O
in	O
the	O
vector	O
,	O
which	O
means	O
the	O
larger	O
the	O
value	O
of	O
an	O
element	O
is	O
the	O
more	O
informative	O
it	O
is	O
.	O
Thus	O
N	O
(	O
0	B-DatasetName
,	O
diag	O
(	O
W	O
2	O
r	O
)	O
)	O
limits	O
the	O
range	O
of	O
sampling	O
to	O
a	O
subspace	O
of	O
the	O
adjacency	O
semantic	O
region	O
,	O
and	O
rejects	O
to	O
conduct	O
sampling	O
from	O
the	O
uninformative	O
dimensions	O
.	O
Moreover	O
,	O
N	O
(	O
1	O
k−1	O
k−1	O
i=1	O
ω	O
(	O
i	O
)	O
,	O
1	O
)	O
simulates	O
a	O
recurrent	O
chain	O
that	O
generates	O
a	O
sequence	O
of	O
reasonable	O
vectors	O
where	O
the	O
current	O
one	O
is	O
dependent	O
on	O
the	O
prior	O
vectors	O
.	O
The	O
reason	O
for	O
this	O
design	O
is	O
that	O
we	O
expect	O
that	O
p	O
in	O
Eq	O
.	O
(	O
6	O
)	O
can	O
become	O
a	O
stationary	O
distribution	O
with	O
the	O
increase	O
of	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
,	O
which	O
describes	O
the	O
fact	O
that	O
the	O
diversity	O
of	O
each	O
training	O
instance	O
is	O
not	O
infinite	O
.	O
η	O
is	O
a	O
hyperparameter	O
to	O
balance	O
the	O
importance	O
of	O
the	O
above	O
two	O
Gaussian	O
forms	O
.	O
For	O
a	O
clearer	O
presentation	O
,	O
Algorithm	O
1	O
summarizes	O
the	O
sampling	O
process	O
.	O

The	O
training	O
objective	O
in	O
our	O
approach	O
is	O
a	O
combination	O
of	O
J	O
mle	O
(	O
Θ	B-HyperparameterName
)	O
in	O
Eq	O
.	O
(	O
3	O
)	O
and	O
J	O
ctl	O
(	O
Θ	B-HyperparameterName
′	O
)	O
in	O
Eq	O
.	O
(	O
4	O
)	O
.	O
In	O
practice	O
,	O
we	O
introduce	O
a	O
two	O
-	O
phase	O
training	O
procedure	O
with	O
mini	O
-	O
batch	O
losses	O
.	O
Firstly	O
,	O
we	O
train	O
the	O
semantic	O
encoder	O
from	O
scratch	O
using	O
the	O
task	O
-	O
specific	O
data	O
,	O
i.e.	O
Θ	B-HyperparameterName
′	O
*	O
=	O
argmax	O
Θ	B-HyperparameterName
′	O
J	O
ctl	O
(	O
Θ	B-HyperparameterName
′	O
)	O
.	O
Secondly	O
,	O
we	O
optimize	O
the	O
encoder	O
-	O
decoder	O
model	O
by	O
maximizing	O
the	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
,	O
i.e.	O
Θ	B-HyperparameterName
*	O
=	O
argmax	O
Θ	B-HyperparameterName
J	O
mle	O
(	O
Θ	B-HyperparameterName
)	O
,	O
and	O
fine	O
-	O
tune	O
the	O
semantic	O
encoder	O
with	O
a	O
small	O
learning	B-HyperparameterName
rate	I-HyperparameterName
at	O
the	O
same	O
time	O
.	O
During	O
inference	O
,	O
the	O
sequence	O
of	O
target	O
words	O
is	O
generated	O
auto	O
-	O
regressively	O
,	O
which	O
is	O
almost	O
the	O
same	O
as	O
the	O
vanilla	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
A	O
major	O
difference	O
is	O
that	O
our	O
method	O
involves	O
the	O
semantic	O
vector	O
of	O
the	O
input	O
sequence	O
for	O
generation	O
:	O
y	O
*	O
t	O
=	O
argmax	O
yt	O
P	O
(	O
|	O
y	O
<	O
t	O
,	O
x	O
,	O
r	O
x	O
;	O
Θ	B-HyperparameterName
)	O
,	O
where	O
r	O
x	O
=	O
ψ	O
(	O
x	O
;	O
Θ	B-HyperparameterName
′	O
)	O
.	O
This	O
module	O
is	O
plug	O
-	O
in	O
-	O
use	O
as	O
well	O
as	O
is	O
agnostic	O
to	O
model	O
architectures	O
.	O

We	O
first	O
apply	O
CSANMT	O
to	O
NIST	O
Chinese	O
-	O
English	O
(	O
Zh	O
En	O
)	O
,	O
WMT14	B-DatasetName
English	O
-	O
German	O
(	O
En	O
De	O
)	O
and	O
English	O
-	O
French	O
(	O
En	O
Fr	O
)	O
tasks	O
,	O
and	O
conduct	O
extensive	O
analyses	O
for	O
better	O
understanding	O
the	O
proposed	O
method	O
.	O
And	O
then	O
we	O
generalize	O
the	O
capability	O
of	O
our	O
method	O
to	O
low	O
-	O
resource	O
IWSLT	O
tasks	O
.	O
Training	O
Details	O
.	O
We	O
implement	O
our	O
approach	O
on	O
top	O
of	O
the	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
semantic	O
encoder	O
is	O
a	O
4	O
-	O
layer	O
transformer	O
encoder	O
with	O
the	O
same	O
hidden	O
size	O
as	O
the	O
backbone	O
model	O
.	O
Following	O
sentence	O
-	O
bert	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
,	O
we	O
average	O
the	O
outputs	O
of	O
all	O
positions	O
as	O
the	O
sequence	O
-	O
level	O
representation	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
for	O
finetuning	O
the	O
semantic	O
encoder	O
at	O
the	O
second	O
training	O
stage	O
is	O
set	O
as	O
1e	O
−	O
5	O
.	O
All	O
experiments	O
are	O
performed	O
on	O
8	O
V100	O
GPUs	O
.	O
We	O
accumulate	O
the	O
gradient	O
of	O
8	O
iterations	O
and	O
update	O
the	O
models	O
with	O
a	O
batch	O
of	O
about	O
65	O
K	O
tokens	O
.	O
The	O
hyperparameters	O
K	O
and	O
η	O
in	O
MGRC	O
sampling	O
are	O
tuned	O
on	O
the	O
validation	O
set	O
with	O
the	O
range	O
of	O
K	O
{	O
10	O
,	O
20	O
,	O
40	O
,	O
80	O
}	O
and	O
η	O
{	O
0.15	O
,	O
0.30	O
,	O
0.45	O
,	O
0.6	O
,	O
0.75	O
,	O
0.90	O
}	O
.	O
We	O
use	O
the	O
default	O
setup	O
of	O
K	B-HyperparameterName
=	I-HyperparameterName
40	O
for	O
all	O
three	O
tasks	O
,	O
η	O
=	O
0.6	O
for	O
both	O
Zh	O
En	O
and	O
En	O
De	O
while	O
η	O
=	O
0.45	O
for	O
En	O
Fr	O
.	O
For	O
evaluation	O
,	O
the	O
beam	O
size	O
and	O
length	O
penalty	O
are	O
set	O
to	O
4	O
and	O
0.6	O
for	O
the	O
En	O
De	O
as	O
well	O
as	O
En	O
Fr	O
,	O
while	O
5	O
and	O
1.0	O
for	O
the	O
Zh	O
En	O
task	O
.	O

Results	O
of	O
Zh	O
En	O
.	O
Table	O
1	O
shows	O
the	O
results	O
on	O
the	O
Chinese	O
-	O
to	O
-	O
English	O
translation	O
task	O
.	O
From	O
the	O
results	O
,	O
we	O
can	O
conclude	O
that	O
our	O
approach	O
outperforms	O
existing	O
augmentation	O
strategies	O
such	O
as	O
back	O
-	O
translation	O
(	O
Sennrich	O
et	O
al	O
,	O
2016a	O
;	O
Wei	O
et	O
al	O
,	O
2020a	O
)	O
and	O
switchout	O
(	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
by	O
a	O
large	O
margin	O
(	O
up	O
to	O
3.63	O
BLEU	B-MetricName
)	O
,	O
which	O
verifies	O
that	O
augmentation	O
in	O
continuous	O
space	O
is	O
more	O
effective	O
than	O
methods	O
with	O
discrete	O
manipulations	O
.	O
Compared	O
to	O
the	O
approaches	O
that	O
replace	O
words	O
in	O
the	O
embedding	O
space	O
(	O
Cheng	O
et	O
al	O
,	O
2020	O
)	O
,	O
our	O
approach	O
also	O
demonstrates	O
superior	O
performance	O
,	O
which	O
reveals	O
that	O
sentence	O
-	O
level	O
augmentation	O
with	O
continuous	O
semantics	O
works	O
better	O
on	O
generalizing	O
to	O
unseen	O
instances	O
.	O
Moreover	O
,	O
compared	O
to	O
the	O
vanilla	O
Transformer	B-MethodName
,	O
our	O
approach	O
consistently	O
achieves	O
promising	O
improvements	O
on	O
five	O
test	O
sets	O
.	O
Results	O
of	O
En	O
De	O
and	O
En	O
Fr	O
.	O
From	O
Table	O
2	O
,	O
our	O
approach	O
consistently	O
performs	O
better	O
than	O
existing	O
methods	O
(	O
Sennrich	O
et	O
al	O
,	O
2016a	O
;	O
Wang	O
et	O
al	O
,	O
2018	O
;	O
Wei	O
et	O
al	O
,	O
2020a	O
;	O
Cheng	O
et	O
al	O
,	O
2020	O
)	O
,	O
yielding	O
significant	O
gains	O
(	O
0.65∼1.76	O
BLEU	B-MetricName
)	O
on	O
the	O
En	O
De	O
and	O
En	O
Fr	O
tasks	O
.	O
An	O
exception	O
is	O
that	O
Nguyen	O
et	O
al	O
(	O
2020	O
)	O
achieved	O
comparable	O
results	O
with	O
ours	O
via	O
multiple	O
forward	O
and	O
backward	O
NMT	O
models	O
,	O
thus	O
data	O
diversification	O
intuitively	O
demonstrates	O
lower	O
training	O
efficiency	O
.	O
Moreover	O
,	O
we	O
observe	O
that	O
CSANMT	O
gives	O
30.16	O
BLEU	B-MetricName
on	O
the	O
En	O
De	O
task	O
with	O
the	O
base	O
setting	O
,	O
significantly	O
outperforming	O
the	O
vanilla	O
Transformer	B-MethodName
by	O
2.49	O
BLEU	B-MetricName
points	O
.	O
Our	O
approach	O
yields	O
a	O
further	O
improvement	O
of	O
0.68	O
BLEU	B-MetricName
by	O
equipped	O
with	O
the	O
wider	O
architecture	O
,	O
demonstrating	O
superiority	O
over	O
the	O
standard	O
Transformer	B-MethodName
by	O
2.15	O
BLEU	B-MetricName
.	O
Similar	O
observations	O
can	O
be	O
drawn	O
for	O
the	O
En	O
Fr	O
task	O
.	O

Effects	O
of	O
K	O
and	O
η	O
.	O
Figure	O
4	O
illustrates	O
how	O
the	O
hyper	O
-	O
parameters	O
K	O
and	O
η	O
in	O
MGRC	O
sampling	O
affect	O
the	O
translation	O
quality	O
.	O
From	O
Figures	O
4	O
(	O
a	O
)	O
-	O
4	O
(	O
c	O
)	O
,	O
we	O
can	O
observe	O
that	O
gradually	O
increasing	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
significantly	O
improves	O
BLEU	B-MetricName
scores	O
,	O
which	O
demonstrates	O
large	O
gaps	O
between	O
K	B-HyperparameterName
=	I-HyperparameterName
10	O
and	O
K	B-HyperparameterName
=	I-HyperparameterName
40	O
.	O
However	O
,	O
assigning	O
larger	O
values	O
(	O
e.g.	O
,	O
80	O
)	O
to	O
K	O
does	O
not	O
result	O
in	O
further	O
improvements	O
among	O
all	O
three	O
tasks	O
.	O
We	O
conjecture	O
that	O
the	O
reasons	O
are	O
two	O
folds	O
:	O
(	O
1	O
)	O
it	O
is	O
fact	O
that	O
the	O
diversity	O
of	O
each	O
training	O
instance	O
is	O
not	O
infinite	O
and	O
thus	O
MGRC	O
gets	O
saturated	O
is	O
inevitable	O
with	O
K	O
increasing	O
.	O
(	O
2	O
)	O
MGRC	O
sampling	O
with	O
a	O
scaled	O
item	O
(	O
i.e.	O
,	O
W	O
r	O
)	O
may	O
degenerate	O
to	O
traverse	O
in	O
the	O
same	O
place	O
.	O
This	O
prompts	O
us	O
to	O
design	O
more	O
sophisticated	O
algorithms	O
in	O
future	O
work	O
.	O
In	O
our	O
experiments	O
,	O
we	O
default	O
set	O
K	B-HyperparameterName
=	I-HyperparameterName
40	O
to	O
achieve	O
a	O
balance	O
between	O
the	O
training	O
efficiency	O
and	O
translation	O
quality	O
.	O
Figure	O
4	O
(	O
d	O
)	O
shows	O
the	O
effect	O
of	O
η	O
on	O
validation	O
sets	O
,	O
which	O
balances	O
the	O
importance	O
of	O
two	O
Gaussian	O
forms	O
during	O
the	O
sampling	O
process	O
.	O
The	O
setting	O
of	O
η	O
=	O
0.6	O
achieves	O
the	O
best	O
results	O
on	O
both	O
the	O
Zh	O
En	O
and	O
En	O
De	O
tasks	O
,	O
and	O
η	O
=	O
0.45	O
consistently	O
outperforms	O
other	O
values	O
on	O
the	O
En	O
Fr	O
task	O
.	O

We	O
demonstrate	O
both	O
the	O
lexical	O
diversity	O
(	O
measured	O
by	O
TTR=	O
num	O
.	O
of	O
types	O
num	O
.	O
of	O
tokens	O
)	O
of	O
various	O
trans	O
-	O
lations	O
and	O
the	O
semantic	O
faithfulness	O
of	O
machine	O
translated	O
ones	O
(	O
measured	O
by	O
BLEURT	O
with	O
considering	O
human	O
translations	O
as	O
the	O
references	O
)	O
in	O
Table	O
4	O
.	O
It	O
is	O
clear	O
that	O
CSANMT	O
substantially	O
bridge	O
the	O
gap	O
of	O
the	O
lexical	O
diversity	O
between	O
translations	O
produced	O
by	O
human	O
and	O
machine	O
.	O
Meanwhile	O
,	O
CSANMT	O
shows	O
a	O
better	O
capability	O
on	O
preserving	O
the	O
semantics	O
of	O
the	O
generated	O
translations	O
than	O
Transformer	B-MethodName
.	O
We	O
intuitively	O
attribute	O
the	O
significantly	O
increases	O
of	O
BLEU	B-MetricName
scores	O
on	O
all	O
datasets	O
to	O
these	O
two	O
factors	O
.	O
We	O
also	O
have	O
studied	O
the	O
robustness	O
of	O
CSANMT	O
towards	O
noisy	O
inputs	O
and	O
the	O
translationese	O
effect	O
,	O
see	O
Appendix	O
D	O
for	O
details	O
.	O
Effect	O
of	O
the	O
semantic	O
encoder	O
.	O
We	O
introduce	O
two	O
variants	O
of	O
the	O
semantic	O
encoder	O
to	O
investigate	O
its	O
performance	O
on	O
En	O
De	O
validation	O
set	O
.	O
Specifically	O
,	O
(	O
1	O
)	O
we	O
remove	O
the	O
extra	O
semantic	O
encoder	O
and	O
construct	O
the	O
sentence	O
-	O
level	O
representations	O
by	O
averaging	O
the	O
sequence	O
of	O
outputs	O
of	O
the	O
vanilla	O
sentence	O
encoder	O
.	O
(	O
2	O
)	O
We	O
replace	O
the	O
default	O
4	O
-	O
layer	O
semantic	O
encoder	O
with	O
a	O
large	O
pre	O
-	O
trained	O
model	O
(	O
PTM	O
)	O
(	O
i.e.	O
,	O
XLM	B-MethodName
-	O
R	O
(	O
Conneau	O
et	O
al	O
,	O
2020	O
)	O
)	O
.	O
The	O
results	O
are	O
reported	O
in	O
Table	O
3	O
.	O
Comparing	O
line	O
2	O
with	O
line	O
3	O
,	O
we	O
can	O
conclude	O
that	O
an	O
extra	O
semantic	O
encoder	O
is	O
necessary	O
for	O
constructing	O
the	O
universal	O
continuous	O
space	O
among	O
different	O
languages	O
.	O
Moreover	O
,	O
when	O
the	O
large	O
PTM	O
is	O
incorporated	O
,	O
our	O
approach	O
yields	O
further	O
improvements	O
,	O
but	O
it	O
causes	O
massive	O
computational	O
overhead	O
.	O
Comparison	O
between	O
discrete	O
and	O
continuous	O
augmentations	O
.	O
To	O
conduct	O
detailed	O
compar	O
-	O
isons	O
between	O
different	O
augmentation	O
methods	O
,	O
we	O
asymptotically	O
increase	O
the	O
training	O
data	O
to	O
analyze	O
the	O
performance	O
of	O
them	O
on	O
the	O
En	O
De	O
translation	O
.	O
As	O
in	O
Figure	O
5	O
,	O
our	O
approach	O
significantly	O
outperforms	O
the	O
back	O
-	O
translation	O
method	O
on	O
each	O
subset	O
,	O
whether	O
or	O
not	O
extra	O
monolingual	O
data	O
(	O
Sennrich	O
et	O
al	O
,	O
2016a	O
)	O
is	O
introduced	O
.	O
These	O
results	O
demonstrate	O
the	O
stronger	O
ability	O
of	O
our	O
approach	O
than	O
discrete	O
augmentation	O
methods	O
on	O
generalizing	O
to	O
unseen	O
instances	O
with	O
the	O
same	O
set	O
of	O
observed	O
data	O
points	O
.	O
Encouragingly	O
,	O
our	O
approach	O
achieves	O
comparable	O
performance	O
with	O
the	O
baseline	O
model	O
with	O
only	O
25	O
%	O
of	O
training	O
data	O
,	O
which	O
indicates	O
that	O
our	O
approach	O
has	O
great	O
potential	O
to	O
achieve	O
good	O
results	O
with	O
very	O
few	O
data	O
.	O
Effect	O
of	O
MGRC	O
sampling	O
and	O
tangential	O
contrastive	B-MethodName
learning	I-MethodName
.	O
To	O
better	O
understand	O
the	O
effectiveness	O
of	O
the	O
MGRC	O
sampling	O
and	O
the	O
tangential	O
contrastive	B-MethodName
learning	I-MethodName
,	O
we	O
conduct	O
detailed	O
ablation	O
studies	O
in	O
Table	O
5	O
.	O
The	O
details	O
of	O
four	O
variants	O
with	O
different	O
objectives	O
or	O
sampling	O
strategies	O
are	O
shown	O
in	O
Appendix	O
C.	O
From	O
the	O
results	O
,	O
we	O
can	O
observe	O
that	O
both	O
removing	O
the	O
recurrent	O
dependence	O
and	O
replacing	O
the	O
Gaussian	O
forms	O
with	O
uniform	O
distributions	O
make	O
the	O
translation	O
quality	O
decline	O
,	O
but	O
the	O
former	O
demonstrates	O
more	O
drops	O
.	O
We	O
also	O
have	O
tried	O
the	O
training	O
objectives	O
with	O
other	O
forms	O
,	O
such	O
as	O
variational	B-MethodName
inference	I-MethodName
and	O
cosine	O
similarity	O
,	O
to	O
optimize	O
the	O
semantic	O
encoder	O
.	O
However	O
,	O
the	O
BLEU	B-MetricName
score	I-MetricName
drops	O
significantly	O
.	O
Training	O
Cost	O
and	O
Convergence	O
.	O
shows	O
the	O
evolution	O
of	O
BLEU	B-MetricName
scores	O
during	O
training	O
.	O
It	O
is	O
obvious	O
that	O
our	O
method	O
performs	O
consistently	O
better	O
than	O
both	O
the	O
vanilla	O
Transformer	B-MethodName
and	O
the	O
back	O
-	O
translation	O
method	O
at	O
each	O
iteration	O
(	O
except	O
for	O
the	O
first	O
10	O
K	O
warm	O
-	O
up	O
iterations	O
,	O
where	O
the	O
former	O
one	O
has	O
access	O
to	O
less	O
unique	O
training	O
data	O
than	O
the	O
latter	O
two	O
due	O
to	O
the	O
K	O
times	O
over	O
-	O
sampling	O
)	O
.	O
For	O
the	O
vanilla	O
Transformer	B-MethodName
,	O
the	O
BLEU	B-MetricName
score	I-MetricName
reaches	O
its	O
peak	O
at	O
about	O
52	O
K	O
iterations	O
.	O
In	O
comparison	O
,	O
both	O
CSANMT	O
and	O
the	O
back	O
-	O
translation	O
method	O
require	O
75	O
K	O
updates	O
for	O
convergence	O
.	O
In	O
other	O
words	O
,	O
CSANMT	O
spends	O
44	O
%	O
more	O
training	O
costs	O
than	O
the	O
vanilla	O
Transformer	B-MethodName
,	O
due	O
to	O
the	O
longer	O
time	O
to	O
make	O
the	O
NMT	O
model	O
converge	O
with	O
augmented	O
training	O
instances	O
.	O
This	O
is	O
the	O
same	O
as	O
the	O
back	O
-	O
translation	O
method	O
.	O
Word	O
prediction	O
accuracy	B-MetricName
.	O
Figure	O
7	O
illustrates	O
the	O
prediction	O
accuracy	B-MetricName
of	O
both	O
frequent	O
and	O
rare	O
words	O
.	O
As	O
expected	O
,	O
CSANMT	O
generalizes	O
to	O
rare	O
words	O
better	O
than	O
the	O
vanilla	O
Transformer	B-MethodName
,	O
and	O
the	O
gap	O
of	O
word	O
prediction	O
accuracy	B-MetricName
is	O
as	O
large	O
as	O
16	O
%	O
.	O
This	O
indicates	O
that	O
the	O
NMT	O
model	O
alleviates	O
the	O
probability	O
under	O
-	O
estimation	O
of	O
rare	O
words	O
via	O
continuous	O
semantic	O
augmentation	O
.	O
Baselines	O
.	O
In	O
contrast	O
to	O
the	O
vanilla	O
Transformer	B-MethodName
,	O
CSANMT	O
involves	O
with	O
approximate	O
20	O
%	O
additional	O
parameters	O
.	O
In	O
this	O
section	O
,	O
we	O
further	O
compare	O
against	O
the	O
baselines	O
with	O
increased	O
amounts	O
of	O
parameters	O
,	O
and	O
investigate	O
the	O
performance	O
of	O
CSANMT	O
equipped	O
with	O
much	O
stronger	O
baselines	O
(	O
e.g.	O
deep	O
and	O
scale	O
Transformers	O
Wei	O
et	O
al	O
,	O
2020b	O
)	O
)	O
.	O
From	O
the	O
results	O
on	O
WMT14	B-DatasetName
testsets	O
in	O
Table	O
6	O
,	O
we	O
can	O
observe	O
that	O
CSANMT	O
still	O
outperforms	O
the	O
vanilla	O
Transformer	B-MethodName
(	O
by	O
more	O
than	O
1.2	O
BLEU	B-MetricName
)	O
under	O
the	O
same	O
amount	O
of	O
parameters	O
,	O
which	O
shows	O
that	O
the	O
additional	O
parameters	O
are	O
not	O
the	O
key	O
to	O
the	O
improvement	O
.	O
Moreover	O
,	O
CSANMT	O
yields	O
at	O
least	O
0.9	O
BLEU	B-MetricName
gains	O
equipped	O
with	O
much	O
stronger	O
baselines	O
.	O
For	O
example	O
,	O
the	O
scale	O
Transformer	B-MethodName
,	O
which	O
originally	O
gives	O
29.3	O
BLEU	B-MetricName
in	O
the	O
En	O
De	O
task	O
,	O
now	O
gives	O
31.37	O
BLEU	B-MetricName
with	O
our	O
continuous	O
semantic	O
augmentation	O
strategy	O
.	O
It	O
is	O
important	O
to	O
mention	O
that	O
our	O
method	O
can	O
help	O
models	O
to	O
achieve	O
further	O
improvement	O
,	O
even	O
if	O
they	O
are	O
strong	O
enough	O
.	O

We	O
further	O
generalize	O
the	O
capability	O
of	O
the	O
proposed	O
CSANMT	O
to	O
various	O
low	O
-	O
resource	O
machine	B-TaskName
translation	I-TaskName
tasks	O
,	O
including	O
IWSLT14	O
English	O
-	O
German	O
and	O
IWSLT17	O
English	O
-	O
French	O
.	O
The	O
details	O
of	O
the	O
datasets	O
and	O
model	O
configurations	O
can	O
be	O
found	O
in	O
Appendix	O
B.	O
Table	O
7	O
shows	O
the	O
results	O
of	O
different	O
models	O
.	O
Compared	O
to	O
the	O
vanilla	O
Transformer	B-MethodName
,	O
the	O
proposed	O
CSANMT	O
improve	O
the	O
BLEU	B-MetricName
scores	O
of	O
the	O
two	O
tasks	O
by	O
2.7	O
and	O
2.9	O
points	O
,	O
respectively	O
.	O
This	O
result	O
indicates	O
that	O
the	O
claiming	O
of	O
the	O
continuous	O
semantic	O
augmentation	O
enriching	O
the	O
training	O
corpora	O
with	O
very	O
limited	O
observed	O
instances	O
.	O

We	O
propose	O
a	O
novel	O
data	B-TaskName
augmentation	I-TaskName
paradigm	O
CSANMT	O
,	O
which	O
involves	O
with	O
an	O
adjacency	O
semantic	O
region	O
as	O
the	O
vicinity	O
manifold	O
for	O
each	O
training	O
instance	O
.	O
This	O
method	O
is	O
expected	O
to	O
make	O
more	O
unseen	O
instances	O
under	O
generalization	O
with	O
very	O
limited	O
training	O
data	O
.	O
The	O
main	O
components	O
of	O
CSANMT	O
consists	O
of	O
the	O
tangential	O
contrastive	B-MethodName
learning	I-MethodName
and	O
the	O
Mixed	O
Gaussian	O
Recurrent	O
Chain	O
(	O
MGRC	O
)	O
sampling	O
.	O
Experiments	O
on	O
both	O
rich	O
-	O
and	O
low	O
-	O
resource	O
machine	B-TaskName
translation	I-TaskName
tasks	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O
In	O
the	O
future	O
work	O
,	O
we	O
would	O
like	O
to	O
further	O
study	O
the	O
vicinal	O
risk	O
minimization	O
with	O
the	O
combination	O
of	O
multi	O
-	O
lingual	O
aligned	O
scenarios	O
and	O
large	O
-	O
scale	O
monolingual	O
data	O
,	O
and	O
development	O
it	O
as	O
a	O
pure	O
data	O
augmentator	O
merged	O
into	O
the	O
vanilla	O
Transformer	B-MethodName
.	O
We	O
use	O
the	O
Stanford	O
segmenter	O
(	O
Tseng	O
et	O
al	O
,	O
2005	O
)	O
for	O
Chinese	B-TaskName
word	I-TaskName
segmentation	I-TaskName
and	O
apply	O
the	O
script	O
tokenizer.pl	O
of	O
Moses	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
for	O
English	O
,	O
German	O
and	O
French	O
tokenization	O
.	O
We	O
measure	O
the	O
performance	O
with	O
the	O
4gram	O
BLEU	B-MetricName
score	I-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
.	O
Both	O
the	O
case	O
-	O
sensitive	O
tokenized	O
BLEU	B-MetricName
(	O
compued	O
by	O
multi	O
-	O
bleu.pl	O
)	O
and	O
the	O
detokenized	O
sacrebleu	B-MetricName
3	O
(	O
Post	O
,	O
2018	O
)	O
are	O
reported	O
on	O
the	O
En	O
De	O
and	O
En	O
Fr	O
tasks	O
.	O
The	O
case	O
-	O
insensitive	O
BLEU	B-MetricName
is	O
reported	O
on	O
the	O
Zh	O
En	O
task	O
.	O

For	O
the	O
low	O
-	O
resource	O
scenario	O
,	O
we	O
choose	O
the	O
IWSLT14	O
English	O
-	O
German	O
(	O
En	O
De	O
)	O
and	O
IWSLT17	O
English	O
-	O
French	O
(	O
En	O
Fr	O
)	O
tasks	O
.	O
Datasets	O
.	O
For	O
IWSLT14	O
En	O
De	O
,	O
there	O
are	O
160k	O
sentence	O
pairs	O
for	O
training	O
and	O
7584	O
sentence	O
pairs	O
for	O
validation	O
.	O
As	O
in	O
previous	O
work	O
(	O
Ranzato	O
et	O
al	O
,	O
2016	O
;	O
Zhu	O
et	O
al	O
,	O
2020	O
)	O
,	O
the	O
concatenation	O
of	O
dev2010	O
,	O
dev2012	O
,	O
test2010	O
,	O
test2011	O
and	O
test2012	O
is	O
used	O
as	O
the	O
test	O
set	O
.	O
For	O
IWSLT17	O
En	O
Fr	O
,	O
there	O
are	O
236k	O
sentence	O
pairs	O
for	O
training	O
and	O
10263	O
for	O
validation	O
.	O
The	O
concatenation	O
of	O
test2010	O
,	O
test2011	O
,	O
test2012	O
,	O
test2013	O
,	O
test2014	O
and	O
test2015	O
is	O
used	O
as	O
the	O
test	O
set	O
.	O
We	O
use	O
a	O
joint	O
source	O
and	O
target	O
vocabulary	O
with	O
10k	O
byte	O
-	O
pair	O
-	O
encoding	O
(	O
BPE	B-MethodName
)	O
types	O
(	O
Sennrich	O
et	O
al	O
,	O
2016b	O
)	O
for	O
above	O
two	O
tasks	O
.	O
Model	O
Settings	O
.	O
The	O
model	O
configuration	O
is	O
transformer_iwslt	O
,	O
representing	O
a	O
6	O
-	O
layer	O
model	O
with	O
embedding	O
size	O
512	O
and	O
FFN	O
layer	O
dimension	O
1024	O
.	O
We	O
train	O
all	O
models	O
using	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
adaptive	O
learning	B-HyperparameterName
rate	I-HyperparameterName
schedule	O
(	O
warm	O
-	O
up	O
step	O
with	O
4	O
K	O
)	O
as	O
in	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
During	O
inference	O
,	O
we	O
use	O
beam	O
search	O
with	O
a	O
beam	O
size	O
of	O
5	O
and	O
length	O
penalty	O
of	O
1.0	O
.	O

Dialogue	O
systems	O
that	O
aim	O
to	O
acquire	O
user	O
models	O
through	O
interactions	O
with	O
users	O
need	O
to	O
have	O
interviewing	O
functionality	O
.	O
In	O
this	O
study	O
,	O
we	O
propose	O
a	O
method	O
to	O
generate	O
interview	O
dialogues	O
to	O
build	O
a	O
dialogue	O
system	O
that	O
acquires	O
user	O
preferences	O
for	O
food	O
.	O
First	O
,	O
we	O
collected	O
118	O
text	O
-	O
based	O
dialogues	O
between	O
the	O
interviewer	O
and	O
customer	O
and	O
annotated	O
the	O
communicative	O
function	O
and	O
semantic	O
content	O
of	O
the	O
utterances	O
.	O
Next	O
,	O
using	O
the	O
corpus	O
as	O
training	O
data	O
,	O
we	O
created	O
a	O
classification	O
model	O
for	O
the	O
communicative	O
function	O
of	O
the	O
interviewer	O
's	O
next	O
utterance	O
and	O
a	O
generative	O
model	O
that	O
predicts	O
the	O
semantic	O
content	O
of	O
the	O
utterance	O
based	O
on	O
the	O
dialogue	O
history	O
.	O
By	O
representing	O
semantic	O
content	O
as	O
a	O
sequence	O
of	O
tokens	O
,	O
we	O
evaluated	O
the	O
semantic	O
content	O
prediction	O
model	O
using	O
BLEU	B-MetricName
.	O
The	O
results	O
demonstrated	O
that	O
the	O
semantic	O
content	O
produced	O
by	O
the	O
proposed	O
method	O
was	O
closer	O
to	O
the	O
ground	O
truth	O
than	O
the	O
semantic	O
content	O
transformed	O
from	O
the	O
output	O
text	O
generated	O
by	O
the	O
retrieval	O
model	O
and	O
GPT	B-MethodName
-	O
2	O
.	O
Further	O
,	O
we	O
present	O
some	O
examples	O
of	O
dialogue	B-TaskName
generation	I-TaskName
by	O
applying	O
model	O
outputs	O
to	O
template	O
-	O
based	O
sentence	O
generation	O
.	O

As	O
part	O
of	O
the	O
interviewing	O
system	O
,	O
we	O
created	O
a	O
Semantic	O
Content	O
Generation	O
(	O
SCG	O
)	O
model	O
that	O
generates	O
the	O
semantic	O
content	O
of	O
the	O
interviewer	O
's	O
next	O
sentence	O
.	O
The	O
model	O
takes	O
the	O
history	O
of	O
messages	O
of	O
both	O
the	O
interviewer	O
and	O
customer	O
as	O
input	O
and	O
predicts	O
the	O
semantic	O
content	O
of	O
the	O
last	O
sentence	O
in	O
the	O
next	O
interviewer	O
's	O
message	O
1	O
.	O
The	O
representation	O
of	O
semantic	O
content	O
follows	O
the	O
annotation	O
scheme	O
described	O
in	O
Section	O
3.2	O
.	O
To	O
train	O
the	O
SCG	O
model	O
,	O
we	O
used	O
a	O
pre	O
-	O
trained	O
Japanese	O
language	O
model	O
2	O
of	O
the	O
Transformerbased	O
GPT	B-MethodName
-	O
2	O
model	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
is	O
commonly	O
used	O
for	O
conversation	O
generation	O
and	O
fine	O
-	O
tuned	O
it	O
using	O
our	O
own	O
small	O
dataset	O
described	O
in	O
Section	O
3.1	O
.	O
Figure	O
3	O
illustrates	O
GTP	O
-	O
2	O
fine	O
tuning	O
to	O
create	O
the	O
SCG	O
model	O
.	O
Each	O
sample	O
of	O
the	O
training	O
data	O
is	O
a	O
pair	O
of	O
dialogue	O
context	O
and	O
semantic	O
content	O
of	O
the	O
interviewer	O
's	O
next	O
sentence	O
.	O
As	O
the	O
dialogue	O
context	O
,	O
messages	O
preceding	O
the	O
prediction	O
target	O
sentence	O
are	O
concatenated	O
.	O
The	O
end	O
of	O
each	O
context	O
message	O
is	O
indicated	O
by	O
[	O
SEP	O
]	O
special	O
token	O
.	O
The	O
maximum	O
number	O
of	O
context	O
messages	O
is	O
five	O
.	O
This	O
1	O
When	O
the	O
next	O
interviewer	O
message	O
consists	O
of	O
multiple	O
sentences	O
,	O
the	O
semantic	O
content	O
of	O
the	O
last	O
sentence	O
is	O
used	O
as	O
the	O
prediction	O
target	O
.	O
This	O
is	O
because	O
the	O
main	O
assertion	O
of	O
the	O
message	O
is	O
often	O
made	O
in	O
the	O
last	O
sentence	O
.	O
2	O
japanese	O
-	O
gpt2	O
-	O
small	O
:	O
https://huggingface.co/rinna/japanese	O
-	O
gpt2	O
-	O
small	O
sequence	O
is	O
concatenated	O
with	O
the	O
semantic	O
content	O
of	O
the	O
prediction	O
target	O
(	O
the	O
interviewer	O
's	O
sentence	O
)	O
and	O
fed	O
to	O
GPT	B-MethodName
-	O
2	O
.	O
The	O
semantic	O
content	O
is	O
represented	O
as	O
a	O
sequence	O
of	O
tokens	O
:	O
verb	O
,	O
object	O
-	O
features	O
,	O
and	O
evaluation	O
description	O
if	O
necessary	O
.	O
Example	O
-	O
1	O
in	O
Figure	O
3	O
shows	O
an	O
example	O
of	O
object	O
-	O
features	O
consisting	O
of	O
ObjectAttribute	O
and	O
AttributeValue	O
,	O
in	O
which	O
the	O
semantic	O
content	O
of	O
the	O
interviewer	O
's	O
next	O
sentence	O
is	O
"	O
[	O
like	O
,	O
[	O
(	O
Dish	O
,	O
pasta	O
,	O
type	O
-	O
of	O
,	O
?	O
)	O
]	O
]	O
"	O
(	O
original	O
sentence	O
:	O
"	O
What	O
kind	O
of	O
pasta	O
do	O
you	O
like	O
?	O
"	O
)	O
.	O
The	O
verb	O
,	O
ObjectType	O
,	O
ObjectName	O
,	O
Objec	O
-	O
tAttribute	O
,	O
and	O
AttributeValue	O
are	O
concatenated	O
into	O
a	O
sequence	O
.	O
Each	O
of	O
these	O
is	O
separated	O
by	O
a	O
[	O
SEP	O
]	O
.	O
Additionally	O
,	O
the	O
<	O
s	O
>	O
and	O
<	O
/s	O
>	O
tokens	O
indicate	O
the	O
beginning	O
and	O
end	O
of	O
each	O
sample	O
,	O
respectively	O
.	O
In	O
Example	O
-	O
2	O
,	O
the	O
semantic	O
content	O
contains	O
the	O
evaluation	O
part	O
:	O
"	O
[	O
think	O
,	O
[	O
(	O
Dish	O
,	O
steak	O
)	O
]	O
,	O
[	O
Evaluation	O
,	O
good	O
]	O
]	O
"	O
(	O
original	O
sentence	O
:	O
"	O
Steak	O
is	O
good	O
.	O
"	O
)	O
,	O
where	O
the	O
second	O
argument	O
[	O
Evaluation	O
,	O
good	O
]	O
is	O
added	O
.	O
Each	O
input	O
sequence	O
is	O
tokenized	O
by	O
the	O
tokenizer	O
,	O
and	O
GPT	B-MethodName
-	O
2	O
optimizes	O
the	O
model	O
weights	O
by	O
minimizing	O
the	O
negative	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
for	O
the	O
next	O
-	O
token	O
prediction	O
.	O

We	O
compared	O
the	O
proposed	O
models	O
with	O
two	O
baseline	O
models	O
:	O
the	O
retrieval	O
model	O
and	O
text	B-TaskName
generation	I-TaskName
model	O
.	O
Retrieval	O
Model	O
:	O
We	O
simply	O
applied	O
a	O
technique	O
used	O
in	O
information	B-TaskName
retrieval	I-TaskName
to	O
a	O
response	O
selection	O
,	O
as	O
proposed	O
in	O
(	O
Ritter	O
et	O
al	O
,	O
2011	O
;	O
Sordoni	O
et	O
al	O
,	O
2015b	O
)	O
.	O
The	O
customer	O
's	O
message	O
and	O
the	O
interviewer	O
's	O
response	O
to	O
it	O
were	O
paired	O
as	O
an	O
input	O
-	O
response	O
pair	O
.	O
In	O
the	O
response	O
selection	O
process	O
,	O
among	O
all	O
pairs	O
,	O
the	O
one	O
whose	O
input	O
sentence	O
had	O
the	O
highest	O
similarity	O
to	O
the	O
customer	O
's	O
input	O
was	O
selected	O
,	O
and	O
the	O
response	O
part	O
of	O
this	O
pair	O
was	O
used	O
as	O
the	O
system	O
's	O
(	O
interviewer	O
's	O
)	O
response	O
.	O
The	O
sentence	O
vector	O
was	O
a	O
hidden	O
representation	O
of	O
the	O
[	O
CLS	O
]	O
token	O
obtained	O
from	O
BERT	B-MethodName
,	O
and	O
cosine	O
similarity	O
was	O
used	O
to	O
calculate	O
the	O
sentence	O
similarity	O
.	O
Text	B-TaskName
Generation	I-TaskName
Model	O
:	O
A	O
GPT	B-MethodName
-	O
2	O
language	O
model	O
was	O
trained	O
using	O
pairs	O
of	O
dialogue	O
context	O
and	O
the	O
next	O
interviewer	O
's	O
sentence	O
.	O
The	O
difference	O
from	O
the	O
SCG	O
model	O
is	O
that	O
the	O
dialogue	O
context	O
was	O
paired	O
with	O
the	O
text	O
(	O
not	O
the	O
semantic	O
content	O
)	O
of	O
the	O
interviewer	O
's	O
response	O
.	O
Therefore	O
,	O
this	O
model	O
generated	O
an	O
interviewer	O
's	O
response	O
text	O
rather	O
than	O
were	O
excluded	O
from	O
the	O
dataset	O
.	O
Table	O
3	O
:	O
Average	O
BLEU	B-MetricName
-	O
4	O
scores	O
.	O
Numbers	O
in	O
parentheses	O
indicate	O
the	O
length	O
of	O
the	O
dialogue	O
history	O
in	O
the	O
best	O
model	O
using	O
the	O
validation	B-DatasetName
dataset	I-DatasetName
.	O
In	O
the	O
retrieval	O
model	O
,	O
the	O
length	O
of	O
the	O
dialogue	O
history	O
was	O
set	O
to	O
one	O
.	O

BLEU	B-MetricName
-	O
4	O
score	O
(	O
standard	O
deviation	O
)	O
Retrieval	O
11.5	O
(	O
20.6	O
)	O
Text	B-TaskName
Generation	I-TaskName
(	O
N=4	O
)	O
13.0	O
(	O
22.3	O
)	O
SCG	O
(	O
Proposed	O
)	O
(	O
N=3	O
)	O
17.3	O
(	O
24.7	O
)	O
the	O
semantic	O
content	O
of	O
the	O
sentence	O
.	O

To	O
evaluate	O
the	O
output	O
produced	O
by	O
the	O
models	O
,	O
we	O
conducted	O
an	O
automated	O
evaluation	O
using	O
the	O
BLEU	B-MetricName
with	O
respect	O
to	O
the	O
semantic	O
content	O
.	O
For	O
this	O
purpose	O
,	O
we	O
treated	O
the	O
semantic	O
content	O
of	O
the	O
target	O
interviewer	O
's	O
sentence	O
as	O
a	O
sequence	O
of	O
words	O
(	O
e.g.	O
,	O
"	O
like	O
[	O
SEP	O
]	O
Dish	O
[	O
SEP	O
]	O
pasta	O
[	O
SEP	O
]	O
typeof	O
[	O
SEP	O
]	O
?	O
"	O
)	O
and	O
used	O
it	O
as	O
the	O
ground	O
truth	O
.	O
For	O
the	O
SCG	O
model	O
,	O
the	O
BLEU	B-MetricName
score	I-MetricName
was	O
calculated	O
by	O
comparing	O
the	O
generated	O
semantic	O
content	O
with	O
the	O
ground	O
truth	O
.	O
For	O
the	O
retrieval	O
model	O
,	O
the	O
semantic	O
content	O
annotation	O
for	O
the	O
response	O
part	O
was	O
compared	O
to	O
the	O
ground	O
truth	O
.	O
For	O
the	O
text	B-TaskName
generation	I-TaskName
model	O
,	O
the	O
semantic	O
content	O
was	O
assigned	O
by	O
annotating	O
the	O
generated	O
message	O
and	O
comparing	O
it	O
with	O
the	O
ground	O
truth	O
to	O
calculate	O
the	O
BLEU	B-MetricName
score	I-MetricName
.	O
As	O
an	O
evaluation	O
of	O
semantic	O
content	O
consisting	O
of	O
a	O
combination	O
of	O
the	O
verb	O
and	O
object	O
-	O
features	O
,	O
we	O
show	O
the	O
average	O
of	O
BLEU	B-MetricName
scores	O
using	O
4grams	O
in	O
the	O
test	O
set	O
in	O
Table	O
3	O
.	O
The	O
proposed	O
model	O
achieved	O
the	O
highest	O
BLEU	B-MetricName
score	I-MetricName
.	O
We	O
changed	O
the	O
dialogue	O
context	B-HyperparameterName
length	I-HyperparameterName
from	O
1	O
to	O
5	O
and	O
found	O
that	O
a	O
model	O
with	O
a	O
dialogue	O
context	B-HyperparameterName
length	I-HyperparameterName
of	O
three	O
achieved	O
the	O
best	O
performance	O
in	O
the	O
validation	B-DatasetName
dataset	I-DatasetName
.	O
These	O
results	O
suggest	O
that	O
the	O
proposed	O
SCG	O
model	O
performed	O
the	O
best	O
in	O
reproducing	O
the	O
semantic	O
content	O
of	O
the	O
interviewer	O
's	O
message	O
.	O

We	O
evaluated	O
the	O
performance	O
of	O
the	O
CFP	O
model	O
by	O
setting	O
the	O
length	O
of	O
the	O
context	O
to	O
three	O
as	O
this	O
setting	O
performed	O
best	O
in	O
the	O
SCG	O
model	O
.	O
The	O
results	O
showed	O
that	O
the	O
model	O
performance	O
for	O
the	O
seven	O
-	O
classes	O
classification	O
was	O
0.39	O
in	O
accuracy	B-MetricName
and	O
0.30	O
in	O
weighted	O
average	O
of	O
the	O
F1	B-MetricName
score	I-MetricName
.	O

In	O
this	O
study	O
,	O
we	O
created	O
a	O
dialogue	O
model	O
to	O
interview	O
the	O
food	O
preferences	O
of	O
users	O
.	O
Text	O
-	O
based	O
dialogues	O
between	O
an	O
interviewer	O
and	O
customer	O
were	O
collected	O
,	O
and	O
the	O
communicative	O
function	O
and	O
semantic	O
content	O
of	O
the	O
interviewer	O
's	O
utterances	O
were	O
annotated	O
.	O
Using	O
this	O
dataset	O
,	O
we	O
created	O
models	O
to	O
predict	O
the	O
communicative	O
function	O
of	O
the	O
interviewer	O
's	O
utterances	O
and	O
generate	O
semantic	O
content	O
.	O
The	O
outputs	O
of	O
these	O
two	O
models	O
were	O
then	O
applied	O
to	O
template	O
-	O
based	O
response	B-TaskName
generation	I-TaskName
to	O
produce	O
a	O
response	O
.	O
In	O
the	O
model	O
evaluation	O
for	O
generating	O
semantic	O
content	O
,	O
the	O
proposed	O
model	O
outperformed	O
the	O
two	O
baseline	O
models	O
,	O
retrieval	O
and	O
generative	O
,	O
in	O
the	O
automatic	O
evaluation	O
using	O
BLEU	B-MetricName
-	O
4	O
.	O
As	O
future	O
work	O
,	O
we	O
will	O
improve	O
the	O
response	B-TaskName
generation	I-TaskName
mechanism	O
to	O
generate	O
a	O
variety	O
of	O
expressions	O
because	O
the	O
current	O
template	O
-	O
based	O
response	B-TaskName
generation	I-TaskName
may	O
not	O
be	O
sufficient	O
in	O
its	O
expressiveness	O
.	O
For	O
example	O
,	O
one	O
of	O
the	O
ideas	O
would	O
be	O
presenting	O
candidates	O
such	O
as	O
Japanese	O
,	O
Chinese	O
,	O
and	O
Italian	O
when	O
asking	O
about	O
preferences	O
for	O
a	O
genre	O
and	O
asking	O
the	O
user	O
to	O
select	O
one	O
.	O
It	O
would	O
also	O
be	O
useful	O
to	O
predict	O
the	O
user	O
's	O
preference	O
based	O
on	O
the	O
dialog	O
history	O
and	O
user	O
information	O
and	O
generate	O
questions	O
such	O
as	O
"	O
Do	O
you	O
prefer	O
Chinese	O
to	O
Italian	O
?	O
Thus	O
,	O
by	O
using	O
question	O
content	O
(	O
e.g.	O
,	O
genre	O
)	O
and	O
related	O
vocabulary	O
and	O
knowledge	O
(	O
Chinese	O
and	O
Italian	O
as	O
examples	O
of	O
genre	O
)	O
,	O
the	O
question	O
variation	O
can	O
be	O
increased	O
.	O
Another	O
possibility	O
is	O
to	O
automatically	O
extract	O
or	O
determine	O
the	O
response	O
templates	O
through	O
machine	O
learning	O
,	O
but	O
this	O
is	O
a	O
challenging	O
task	O
.	O
Further	O
,	O
a	O
user	O
study	O
should	O
be	O
conducted	O
,	O
as	O
it	O
is	O
known	O
that	O
automatic	O
evaluation	O
using	O
BLEU	B-MetricName
does	O
not	O
always	O
correlate	O
with	O
human	O
evaluation	O
(	O
Liu	O
et	O
al	O
,	O
2016	O
)	O
.	O
In	O
the	O
user	O
study	O
,	O
users	O
interact	O
with	O
the	O
system	O
,	O
and	O
then	O
they	O
evaluate	O
the	O
quality	O
of	O
the	O
responses	O
generated	O
from	O
the	O
system	O
,	O
and	O
judge	O
whether	O
the	O
system	O
effectively	O
elicits	O
information	O
from	O
the	O
user	O
.	O

We	O
treat	O
the	O
task	O
of	O
categorizing	O
narratives	O
related	O
to	O
sexual	O
abuse	O
disclosure	O
-	O
Stance	O
,	O
Hate	B-DatasetName
Speech	I-DatasetName
,	O
3	O
Implementation	O
used	O
for	O
BERTweet	O
is	O
available	O
here	O
Sarcasm	O
and	O
Dialogue	O
Acts	O
,	O
independently	O
.	O
Each	O
STL	O
model	O
is	O
given	O
an	O
input	O
representation	O
e	O
(	O
Equation	O
1	O
)	O
.	O
Within	O
the	O
proposed	O
tasks	O
for	O
classifying	O
sexual	O
abuse	O
disclosure	O
narrative	O
for	O
the	O
tweets	O
related	O
to	O
the	O
#	O
MeToo	O
movement	O
(	O
Section	O
3	O
)	O
,	O
we	O
use	O
sigmoid	B-MethodName
activation	I-MethodName
for	O
Sarcasm	B-TaskName
detection	I-TaskName
(	O
whose	O
classification	O
outputs	O
are	O
binary	O
)	O
and	O
softmax	B-MethodName
activation	O
for	O
all	O
other	O
tasks	O
for	O
the	O
final	O
output	O
layer	O
.	O
Model	O
Optimization	O
To	O
account	O
for	O
the	O
imbalance	O
present	O
among	O
the	O
labels	O
,	O
we	O
use	O
classbalanced	O
focal	B-MethodName
loss	I-MethodName
as	O
the	O
optimization	O
loss	B-MetricName
function	O
(	O
Cui	O
et	O
al	O
,	O
2019	O
)	O
,	O
as	O
formulated	O
in	O
Equation	O
5	O
.	O
Given	O
a	O
sample	O
class	O
i	O
containing	O
n	O
i	O
samples	O
in	O
total	O
,	O
it	O
adds	O
a	O
weighting	O
factor	O
of	O
(	O
1−β	O
)	O
(	O
1−β	O
n	O
i	O
)	O
with	O
parameters	O
β	B-HyperparameterName
[	O
0	B-DatasetName
,	O
1	O
)	O
,	O
where	O
n	O
y	O
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
in	O
the	O
ground	O
truth	O
class	O
y.	O
The	O
proposed	O
class	O
-	O
balanced	O
term	O
is	O
model	O
agnostic	O
.	O
p	O
represents	O
predicted	O
class	O
probabilities	O
and	O
L	O
represents	O
the	O
choice	O
of	O
the	O
loss	B-MetricName
function	O
(	O
binary	O
cross	O
entropy	O
for	O
Sarcasm	O
and	O
categorical	O
cross	O
entropy	O
for	O
others	O
)	O
.	O
CB	O
(	O
p	O
,	O
y	O
)	O
=	O
1	O
−	O
β	B-HyperparameterName
1	O
−	O
β	B-HyperparameterName
ny	O
L	O
(	O
p	O
,	O
y	O
)	O
(	O
5	O
)	O
As	O
for	O
the	O
multilabel	O
emotion	B-TaskName
classification	I-TaskName
task	O
,	O
the	O
unnormalized	O
output	O
(	O
assuming	O
one	O
or	O
more	O
of	O
11	O
different	O
emotions	O
)	O
is	O
subjected	O
to	O
a	O
Sigmoid	B-MethodName
activation	I-MethodName
,	O
and	O
the	O
network	O
is	O
optimized	O
using	O
binary	O
cross	O
-	O
entropy	O
(	O
BCE	O
)	O
as	O
:	O
LBCE	O
=	O
−	O
1	O
N	O
N	O
i=1	O
yi.log	O
(	O
p	O
(	O
yi	O
)	O
)	O
+	O
(	O
1	O
−	O
yi	O
)	O
.log	O
(	O
1	O
−	O
p	O
(	O
yi	O
)	O
)	O
(	O
6	O
)	O
where	O
N	O
is	O
the	O
number	O
of	O
training	O
samples	O
,	O
y	O
and	O
p	O
(	O
y	O
)	O
denotes	O
true	O
and	O
predicted	O
labels	O
respectively	O
.	O

For	O
our	O
MTL	O
approach	O
,	O
we	O
use	O
two	O
optimization	O
objectives	O
:	O
one	O
for	O
the	O
primary	O
task	O
,	O
which	O
can	O
be	O
any	O
of	O
the	O
proposed	O
tasks	O
for	O
classifying	O
tweets	O
related	O
to	O
#	O
MeToo	O
movement	O
(	O
Section	O
3	O
)	O
,	O
and	O
other	O
for	O
the	O
auxiliary	O
task	O
,	O
which	O
can	O
be	O
either	O
a	O
task	O
related	O
to	O
classifying	O
sexual	O
abuse	O
disclosure	O
for	O
#	O
MeToo	O
movement	O
(	O
Homogeneous	O
MTL	O
)	O
or	O
emotion	B-TaskName
classification	I-TaskName
task	O
(	O
Heterogeneous	O
MTL	O
)	O
.	O
The	O
two	O
objectives	O
are	O
weighted	O
by	O
a	O
parameter	O
γ	B-HyperparameterName
,	O
which	O
controls	O
the	O
importance	O
placed	O
on	O
the	O
auxiliary	O
task	O
(	O
1	O
−	O
γ	B-HyperparameterName
for	O
the	O
primary	O
task	O
)	O
.	O
Multitask	O
learning	O
frameworks	O
are	O
generally	O
built	O
using	O
either	O
of	O
these	O
two	O
approaches	O
:	O
hard	O
parameter	O
sharing	O
or	O
soft	O
parameter	O
sharing	O
.	O
In	O
a	O
hard	O
parameter	O
sharing	O
model	O
(	O
Caruana	O
,	O
1997	O
)	O
,	O
both	O
the	O
primary	O
and	O
auxiliary	O
tasks	O
have	O
a	O
shared	O
encoder	O
followed	O
by	O
separate	O
task	O
-	O
specific	O
network	O
branches	O
,	O
and	O
the	O
shared	O
encoder	O
is	O
updated	O
by	O
both	O
the	O
tasks	O
alternately	O
.	O
On	O
the	O
other	O
hand	O
,	O
in	O
the	O
soft	O
parameter	O
sharing	O
approach	O
,	O
tasks	O
have	O
different	O
encoders	O
with	O
independent	O
parameters	O
,	O
and	O
the	O
distance	O
between	O
their	O
parameters	O
is	O
regularized	O
using	O
a	O
regularization	O
constraint	O
(	O
Duong	O
et	O
al	O
,	O
2015	O
;	O
Yang	O
and	O
Hospedales	O
,	O
2016	O
)	O
,	O
to	O
encourage	O
the	O
parameters	O
to	O
be	O
similar	O
.	O
Flexible	O
Cross	O
-	O
Stitched	O
Parameter	O
Sharing	O
Architecture	O
:	O
We	O
design	O
our	O
model	O
so	O
that	O
the	O
task	O
-	O
agnostic	O
textual	O
feature	O
representations	O
benefit	O
from	O
hard	O
sharing	O
while	O
the	O
regularization	O
of	O
the	O
task	O
-	O
specific	O
features	O
can	O
be	O
learned	O
according	O
to	O
task	O
pair	O
settings	O
.	O
We	O
call	O
our	O
approach	O
flexible	O
cross	O
-	O
stitched	O
parameter	O
sharing	O
,	O
presented	O
in	O
Figure	O
2	O
.	O
Specifically	O
,	O
we	O
train	O
two	O
separate	O
models	O
(	O
one	O
for	O
each	O
task	O
)	O
in	O
tandem	O
while	O
also	O
having	O
a	O
shared	O
encoder	O
that	O
is	O
updated	O
by	O
both	O
of	O
them	O
and	O
weighted	O
joint	O
learning	O
of	O
primary	O
task	O
decoder	O
parameters	O
that	O
are	O
tuned	O
specifically	O
for	O
the	O
task	O
.	O
This	O
allows	O
both	O
the	O
models	O
to	O
have	O
their	O
own	O
set	O
of	O
parameters	O
while	O
also	O
encouraging	O
knowledge	O
transfer	O
via	O
the	O
shared	O
encoder	O
weights	O
.	O
For	O
each	O
training	O
pass	O
of	O
the	O
primary	O
task	O
,	O
the	O
input	O
representation	O
e	O
(	O
p	O
)	O
is	O
passed	O
through	O
(	O
a	O
)	O
stacked	O
BiLSTM	B-MethodName
encoder	O
and	O
(	O
b	O
)	O
stacked	O
shared	O
BiLSTM	B-MethodName
encoder	O
.	O
This	O
results	O
in	O
two	O
contextualized	O
word	O
representations	O
(	O
h	O
(	O
p	O
)	O
1	O
,	O
h	O
(	O
p	O
)	O
2	O
,	O
...	O
h	O
(	O
p	O
)	O
n	O
)	O
and	O
(	O
h	O
(	O
s	O
)	O
1	O
,	O
h	O
(	O
s	O
)	O
2	O
,	O
...	O
h	O
(	O
s	O
)	O
n	O
)	O
,	O
where	O
superscript	O
(	O
p	O
)	O
is	O
used	O
to	O
denote	O
the	O
representations	O
resulting	O
from	O
encoder	O
in	O
the	O
primary	O
task	O
model	O
and	O
superscript	O
(	O
s	O
)	O
is	O
used	O
to	O
denote	O
the	O
ones	O
from	O
shared	O
encoder	O
.	O
We	O
calculate	O
the	O
weighted	O
summation	O
of	O
these	O
two	O
representations	O
-	O
h	O
(	O
p	O
)	O
,	O
using	O
two	O
learnable	O
parameters	O
,	O
α	B-HyperparameterName
(	O
p	O
)	O
and	O
α	B-HyperparameterName
(	O
s	O
)	O
(	O
where	O
α	B-HyperparameterName
(	O
p	O
)	O
+	O
α	B-HyperparameterName
(	O
s	O
)	O
=	O
1	O
)	O
,	O
as	O
formulated	O
in	O
Equation	O
7to	O
regulate	O
the	O
information	O
resulting	O
from	O
the	O
two	O
encoders	O
(	O
Figure	O
2	O
)	O
.	O
h	O
(	O
p	O
)	O
=	O
α	B-HyperparameterName
(	O
p	O
)	O
h	O
(	O
p	O
)	O
+	O
α	B-HyperparameterName
(	O
s	O
)	O
h	O
(	O
s	O
)	O
(	O
7	O
)	O
Such	O
an	O
approach	O
to	O
aggregate	O
information	O
flow	O
from	O
two	O
encoders	O
has	O
facilitated	O
success	O
in	O
prior	O
Multitask	O
learning	O
settings	O
as	O
well	O
(	O
Rajamanickam	O
et	O
al	O
,	O
2020	O
;	O
Dankers	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
for	O
our	O
auxiliary	O
task	O
,	O
we	O
pass	O
the	O
embeddings	O
e	O
(	O
a	O
)	O
through	O
only	O
the	O
shared	O
encoder	O
(	O
h	O
(	O
a	O
)	O
=	O
h	O
(	O
s	O
)	O
)	O
,	O
followed	O
by	O
a	O
dropout	O
layer	O
.	O
We	O
use	O
this	O
architecture	O
for	O
Heterogeneous	O
MTL	O
experiments	O
.	O
For	O
Homogeneous	O
MTL	O
ones	O
,	O
we	O
employ	O
hard	O
parameter	O
sharing	O
model	O
due	O
to	O
statistical	O
out	O
-	O
performance	O
in	O
this	O
scenario	O
.	O
This	O
technique	O
consists	O
of	O
a	O
single	O
stacked	O
encoder	O
that	O
is	O
shared	O
and	O
updated	O
by	O
both	O
tasks	O
related	O
to	O
identifying	O
narratives	O
related	O
to	O
sexual	O
abuse	O
disclosures	O
within	O
#	O
MeToo	O
movement	O
,	O
followed	O
by	O
task	O
-	O
specific	O
branches	O
.	O
The	O
shared	O
representations	O
from	O
the	O
encoder	O
are	O
passed	O
through	O
the	O
dropout	O
layer	O
.	O
These	O
output	O
representations	O
(	O
in	O
the	O
case	O
of	O
both	O
Homogeneous	O
and	O
Heterogeneous	O
experiments	O
)	O
are	O
passed	O
through	O
respective	O
BiLSTM	B-MethodName
decoders	O
and	O
dropout	O
layers	O
to	O
get	O
the	O
final	O
representation	O
m	O
(	O
p	O
)	O
and	O
m	O
(	O
a	O
)	O
,	O
respectively	O
for	O
both	O
the	O
tasks	O
.	O
The	O
auxiliary	O
network	O
branch	O
is	O
optimized	O
using	O
either	O
Equation	O
5	O
(	O
Class	O
Balanced	O
Focal	B-MethodName
Loss	I-MethodName
)	O
or	O
Equation	O
6	O
(	O
Binary	O
Cross	O
Entropy	O
)	O
,	O
depending	O
upon	O
whether	O
the	O
auxiliary	O
task	O
is	O
associated	O
with	O
identifying	O
sexual	O
abuse	O
disclosure	O
narratives	O
or	O
emotions	O
.	O
These	O
output	O
representations	O
m	O
(	O
p	O
)	O
and	O
m	O
(	O
a	O
)	O
are	O
passed	O
through	O
a	O
linear	O
output	O
layer	O
to	O
get	O
unnormalized	O
outputs	O
o	O
(	O
p	O
)	O
and	O
o	O
(	O
a	O
)	O
respectively	O
.	O
Sigmoid	B-MethodName
activation	I-MethodName
function	O
is	O
used	O
for	O
Sarcasm	B-TaskName
detection	I-TaskName
and	O
the	O
emotion	B-TaskName
classification	I-TaskName
task	O
,	O
and	O
Softmax	B-MethodName
activation	O
for	O
others	O
.	O

MTL	O
framework	O
traditionally	O
improves	O
generalization	O
by	O
leveraging	O
the	O
domain	O
-	O
specific	O
information	O
due	O
to	O
the	O
relatedness	O
of	O
the	O
tasks	O
present	O
in	O
the	O
training	O
signals	O
(	O
Caruana	O
,	O
1997	O
)	O
;	O
hence	O
we	O
use	O
two	O
publicly	O
available	O
datasets	O
mined	O
from	O
Twitter	O
:	O
n	O
)	O
identify	O
BERTweet	O
word	O
-	O
level	O
embeddings	O
for	O
the	O
primary	O
and	O
auxiliary	O
task	O
respectively	O
.	O
The	O
different	O
arrows	O
are	O
used	O
to	O
indicate	O
the	O
alternate	O
passes	O
of	O
the	O
primary	O
task	O
(	O
solid	O
arrows	O
)	O
and	O
auxiliary	O
task	O
(	O
dotted	O
arrows	O
)	O
.	O
Two	O
controllable	O
parameters	O
α	B-HyperparameterName
(	O
p	O
)	O
and	O
α	B-HyperparameterName
(	O
s	O
)	O
are	O
used	O
to	O
control	O
information	O
flow	O
from	O
task	O
-	O
specific	O
and	O
shared	O
encoder	O
respectively	O
,	O
for	O
the	O
primary	O
task	O
.	O
Sexual	O
Abuse	O
Disclosures	O
-	O
#	B-DatasetName
MeTooMA	I-DatasetName
This	O
dataset	O
4	O
has	O
9	O
,	O
973	O
tweets	O
and	O
covers	O
different	O
mutually	O
non	O
-	O
exclusive	O
linguistic	O
annotations	O
related	O
to	O
the	O
#	O
MeToo	O
movement	O
.	O
The	O
distribution	O
and	O
statistics	O
about	O
various	O
labels	O
are	O
present	O
in	O
Table	O
1	O
and	O
Section	O
3	O
.	O
We	O
present	O
an	O
instance	O
associated	O
with	O
each	O
of	O
the	O
proposed	O
tasks	O
in	O
Table	O
1	O
.	O
For	O
our	O
experiments	O
,	O
we	O
focus	O
only	O
on	O
tweets	O
that	O
are	O
annotated	O
as	O
relevant	O
to	O
the	O
#	O
MeToo	O
movement	O
.	O
Emotions	O
-	O
SemEval18	O
This	O
dataset	O
5	O
has	O
been	O
taken	O
from	O
SemEval	O
-	O
2018	O
Task	O
-	O
1	O
(	O
Mohammad	O
et	O
al	O
,	O
2018	O
)	O
and	O
covers	O
emotion	B-DatasetName
-	O
specific	O
labels	O
representing	O
the	O
mental	O
state	O
of	O
the	O
authors	O
of	O
the	O
tweets	O
.	O
It	O
consists	O
of	O
10	O
,	O
986	O
tweets	O
distributed	O
across	O
11	O
emotion	B-DatasetName
labels	O
-	O
(	O
anger	O
,	O
disgust	O
,	O
anticipation	O
,	O
fear	O
,	O
joy	O
,	O
love	O
,	O
optimism	O
,	O
pessimism	O
,	O
sadness	O
,	O
surprise	O
and	O
trust	O
)	O
,	O
each	O
being	O
a	O
binary	O
label	O
to	O
indicate	O
the	O
presence	O
of	O
a	O
particular	O
emotion	B-DatasetName
.	O

Preprocessing	O
We	O
pre	O
-	O
process	O
tweet	O
text	O
by	O
(	O
i	O
)	O
normalizing	O
user	O
mentions	O
and	O
URLs	O
,	O
and	O
(	O
ii	O
)	O
translating	O
the	O
emoticon	O
into	O
text	O
(	O
Hutto	O
and	O
Gilbert	O
,	O
2014	O
)	O
.	O
For	O
tokenization	O
,	O
we	O
use	O
Tweet	O
Tokenizer	O
from	O
NLTK	O
.	O
6	O
Hyperparameters	O
For	O
our	O
model	O
7	O
hyperparameters	O
were	O
tuned	O
on	O
the	O
validation	O
set	O
to	O
find	O
the	O
best	O
configurations	O
.	O
We	O
use	O
a	O
pre	O
-	O
trained	O
BERTweet	O
model	O
to	O
extract	O
768	O
-	O
dimensional	O
token	O
-	O
level	O
embeddings	O
.	O
Grid	O
search	O
was	O
performed	O
to	O
find	O
the	O
optimal	O
value	O
of	O
hyperparameters	O
and	O
their	O
range	O
is	O
summarized	O
as	O
:	O
size	O
of	O
BiLSTM	B-MethodName
and	O
dense	O
layers	O
{	O
128	O
,	O
256	O
,	O
512	O
}	O
,	O
embedding	O
size	O
d	O
{	O
100	O
,	O
200	O
,	O
300	O
}	O
,	O
dropout	O
δ	B-HyperparameterName
{	O
0.1	O
,	O
0.2	O
,	O
0.3	O
,	O
0.4	O
,	O
0.5.0.6	O
}	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
λ	O
{	O
10	O
−5	O
,	O
10	O
−4	O
,	O
10	O
−3	O
,	O
10	O
−2	O
,	O
10	O
−1	O
}	O
,	O
weight	B-MethodName
decay	I-MethodName
ω	O
{	O
10	O
−6	O
,	O
10	O
−5	O
,	O
10	O
−4	O
,	O
10	O
−3	O
}	O
,	O
optimizer	B-HyperparameterName
{	O
Adam	B-MethodName
,	O
Adadelta	B-MethodName
}	O
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
b	O
{	O
32	O
,	O
64	O
,	O
128	O
}	O
and	O
epochs	O
(	O
<	O
100	O
)	O
.	O
For	O
the	O
MTL	O
experiments	O
,	O
we	O
tune	O
the	O
weightage	O
of	O
the	O
auxiliary	O
task	O
(	O
γ	B-HyperparameterName
[	O
0.1	O
,	O
0.9	O
]	O
with	O
intervals	O
of	O
0.1	O
)	O
for	O
each	O
task	O
pair	O
.	O
For	O
each	O
task	O
associated	O
with	O
identifying	O
narratives	O
pertaining	O
to	O
the	O
#	O
MeToo	O
movement	O
in	O
the	O
MTL	O
setup	O
,	O
its	O
value	O
is	O
considered	O
as	O
the	O
one	O
where	O
the	O
model	O
performance	O
improved	O
the	O
most	O
and	O
for	O
both	O
the	O
tasks	O
.	O
For	O
instance	O
,	O
we	O
find	O
the	O
optimal	O
value	O
of	O
γ	B-HyperparameterName
for	O
hate	B-DatasetName
speech	I-DatasetName
(	O
as	O
the	O
auxiliary	O
task	O
)	O
to	O
be	O
0.4	O
in	O
all	O
Homogeneous	O
task	O
cases	O
and	O
of	O
emotion	B-DatasetName
detection	O
to	O
be	O
0.2	O
for	O
the	O
Heterogeneous	O
tasks	O
.	O
For	O
the	O
MTL	O
experiments	O
,	O
α	B-HyperparameterName
p	O
and	O
α	B-HyperparameterName
s	O
are	O
learnable	O
and	O
tuned	O
on	O
the	O
validation	O
loss	B-MetricName
.	O
The	O
encoders	O
consist	O
of	O
two	O
stacked	O
BiLSTM	B-MethodName
's	O
with	O
hidden	O
size	O
=	O
128	O
.	O
BiLSTM	B-MethodName
classifier	O
has	O
hidden	O
size	O
=	O
256	O
,	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
units	I-HyperparameterName
in	O
the	O
penultimate	O
dense	O
layer	O
is	O
128	O
.	O
Dropout	B-MethodName
is	O
set	O
to	O
0.3	O
.	O
For	O
all	O
our	O
experiments	O
,	O
we	O
use	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
and	O
initialize	O
model	O
weights	O
using	O
Xavier	B-MethodName
initialization	I-MethodName
(	O
Glorot	O
and	O
Bengio	O
,	O
2010	O
)	O
.	O
We	O
set	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
to	O
128	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
1e	O
−	O
3	O
.	O
Training	O
All	O
models	O
were	O
trained	O
until	O
convergence	O
for	O
both	O
primary	O
and	O
auxiliary	O
tasks	O
.	O
For	O
our	O
MTL	O
experiments	O
,	O
the	O
training	O
process	O
involves	O
alternating	O
between	O
primary	O
and	O
auxiliary	O
task	O
steps	O
,	O
with	O
each	O
task	O
having	O
its	O
own	O
loss	B-MetricName
function	O
.	O
All	O
experiments	O
are	O
run	O
using	O
stratified	O
5	O
-	O
fold	O
crossvalidation	O
.	O
We	O
report	O
the	O
average	O
macro	B-MetricName
F1	I-MetricName
scores	O
across	O
the	O
5	O
folds	O
to	O
account	O
for	O
imbalance	O
,	O
as	O
previously	O
used	O
in	O
multi	O
-	O
label	O
settings	O
(	O
Zhang	O
and	O
Zhou	O
,	O
2013	O
6	O
Results	O
and	O
Discussion	O

Learning	O
the	O
affective	O
states	O
in	O
the	O
#	B-DatasetName
MeTooMA	I-DatasetName
dataset	O
is	O
challenging	O
due	O
to	O
the	O
inherently	O
subjective	O
nature	O
of	O
the	O
tweets	O
coupled	O
with	O
limitations	O
on	O
the	O
data	O
's	O
size	O
.	O
Multitask	O
learning	O
achieves	O
significant	O
performance	O
gains	O
in	O
terms	O
of	O
macro	B-MetricName
F1	I-MetricName
score	O
,	O
as	O
shown	O
in	O
Homogeneous	O
MTL	O
with	O
row	O
identifying	O
primary	O
task	O
and	O
columns	O
denoting	O
auxiliary	O
task	O
.	O
The	O
higher	O
performance	O
of	O
Homogeneous	O
MTL	O
can	O
be	O
inferred	O
to	O
be	O
indicative	O
of	O
better	O
generalization	O
when	O
pairs	O
of	O
tasks	O
are	O
jointly	O
modeled	O
.	O
Interestingly	O
,	O
these	O
tasks	O
show	O
their	O
best	O
performance	O
with	O
the	O
selective	O
counterparts	O
in	O
the	O
Homogeneous	O
MTL	O
setup	O
.	O
Stance	B-TaskName
detection	I-TaskName
is	O
strongly	O
coupled	O
with	O
Sarcasm	O
labeling	O
,	O
and	O
the	O
same	O
is	O
seen	O
to	O
be	O
true	O
for	O
Hate	B-DatasetName
Speech	I-DatasetName
classification	O
and	O
Stance	O
identification	O
.	O
This	O
selective	O
out	O
-	O
performance	O
of	O
specific	O
pairs	O
of	O
tasks	O
can	O
be	O
attributed	O
to	O
the	O
high	O
correlation	O
between	O
the	O
tasks	O
themselves	O
(	O
Frenda	O
,	O
2018	O
;	O
.	O
For	O
instance	O
,	O
the	O
offensive	O
text	O
is	O
often	O
strongly	O
coupled	O
with	O
sarcasm	O
,	O
as	O
wit	O
is	O
a	O
common	O
linguistic	O
denominator	O
for	O
understanding	O
the	O
intended	O
meaning	O
of	O
phrases	O
related	O
to	O
anger	O
(	O
Badlani	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
further	O
detail	O
this	O
through	O
examples	O
in	O
Section	O
6.4	O
.	O

Overnight	O
The	O
Overnight	O
semantic	B-TaskName
parsing	I-TaskName
dataset	O
(	O
Wang	O
et	O
al	O
,	O
2015	O
)	O
consists	O
of	O
13	O
,	O
682	O
natural	O
utterance	O
,	O
canonical	O
form	O
,	O
meaning	O
representation	O
triples	O
split	O
across	O
eight	O
domains	O
.	O
To	O
simulate	O
low	O
-	O
resource	O
splits	O
of	O
this	O
dataset	O
,	O
we	O
follow	O
Shin	O
et	O
al	O
and	O
create	O
randomly	O
subsampled	O
splits	O
of	O
200	O
training	O
examples	O
for	O
each	O
domain	O
,	O
using	O
20	O
%	O
of	O
the	O
remaining	O
data	O
for	O
validation	O
.	O
We	O
measure	O
and	O
report	O
denotation	O
accuracy	B-MetricName
by	O
evaluating	O
all	O
predicted	O
queries	O
using	O
the	O
SEMPRE	O
toolkit	O
(	O
Berant	O
et	O
al	O
,	O
2013	O
)	O
.	O
We	O
repeat	O
each	O
experiment	O
on	O
Overnight	O
with	O
five	O
different	O
random	O
splits	O
.	O
TOPv2	B-DatasetName
Chen	O
et	O
al	O
(	O
2020	O
)	O
introduce	O
the	O
TOPv2	B-DatasetName
dataset	O
,	O
a	O
task	O
-	O
oriented	O
semantic	B-TaskName
parsing	I-TaskName
dataset	O
with	O
eight	O
domains	O
,	O
two	O
of	O
which	O
come	O
with	O
predefined	O
low	O
-	O
resource	O
splits	O
.	O
The	O
authors	O
propose	O
a	O
principled	O
way	O
of	O
constructing	O
low	O
-	O
resource	O
training	O
sets	O
,	O
samples	O
per	O
intent	O
and	O
slot	O
(	O
SPIS	O
)	O
,	O
intended	O
to	O
ensure	O
equal	O
exposure	O
to	O
ontology	B-MethodName
labels	O
across	O
domains	O
of	O
varying	O
complexity	O
.	O
We	O
experiment	O
with	O
the	O
weather	O
and	O
reminder	O
domains	O
at	O
the	O
10	O
,	O
25	O
,	O
and	O
500	O
SPIS	O
resource	O
splits	O
,	O
performing	O
five	O
runs	O
on	O
each	O
model	O
varying	O
the	O
random	O
seed	O
.	O
The	O
reminder	O
domain	O
is	O
the	O
most	O
challenging	O
with	O
19	O
intent	O
labels	O
,	O
32	O
slot	O
labels	O
,	O
and	O
with	O
21	O
%	O
of	O
the	O
programs	O
having	O
a	O
depth	O
greater	O
than	O
2	O
.	O
Weather	O
in	O
comparison	O
has	O
7	O
intent	O
labels	O
,	O
11	O
slot	O
labels	O
,	O
and	O
no	O
programs	O
with	O
depth	O
greater	O
than	O
2	O
.	O

Prompt	O
tuning	O
,	O
as	O
proposed	O
by	O
Lester	O
et	O
al	O
(	O
2021	O
)	O
,	O
prepends	O
a	O
sequence	O
of	O
continuous	O
embeddings	O
p	O
=	O
(	O
p	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
K	O
)	O
to	O
the	O
sequence	O
input	O
embeddings	O
e	O
(	O
u	O
)	O
=	O
(	O
e	O
(	O
u	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
e	O
(	O
u	O
N	O
)	O
)	O
before	O
feeding	O
it	O
to	O
a	O
language	O
model	O
with	O
parameters	O
θ	B-HyperparameterName
.	O
During	O
prompt	O
tuning	O
we	O
optimize	O
the	O
prompt	O
embeddings	O
(	O
p	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
K	O
)	O
exclusively	O
,	O
keeping	O
the	O
language	O
model	O
parameters	O
θ	B-HyperparameterName
and	O
the	O
pretrained	O
vocabulary	O
embeddings	O
fixed	O
.	O
Note	O
that	O
this	O
process	O
still	O
requires	O
backpropagating	O
gradients	O
through	O
the	O
full	O
language	O
model	O
.	O
Like	O
fine	O
-	O
tuning	O
models	O
,	O
we	O
maximize	O
the	O
likelihood	O
of	O
generating	O
the	O
output	O
sequence	O
z.	O

We	O
find	O
that	O
prompt	O
tuning	O
improves	O
over	O
finetuning	O
for	O
all	O
large	O
model	O
configurations	O
and	O
target	O
representations	O
.	O
On	O
Overnight	O
,	O
prompt	O
tuned	O
denotation	O
accuracy	B-MetricName
exceeds	O
fine	O
-	O
tuned	O
counterparts	O
by	O
up	O
to	O
5	O
points	O
with	O
T5	B-MethodName
-	O
large	O
and	O
T5	B-MethodName
-	O
xl	O
.	O
For	O
T5	B-MethodName
-	O
small	O
and	O
T5	B-MethodName
-	O
base	O
,	O
prompt	O
tuning	O
remains	O
competitive	O
(	O
within	O
1	O
%	O
average	B-MetricName
accuracy	I-MetricName
)	O
with	O
fine	O
-	O
tuning	O
when	O
predicting	O
canonical	O
forms	O
.	O
On	O
TOPv2	B-DatasetName
,	O
prompt	O
tuning	O
achieves	O
an	O
absolute	O
improvement	O
of	O
15	O
%	O
mean	O
accuracy	B-MetricName
over	O
fine	O
-	O
tuning	O
on	O
the	O
lowest	O
SPIS	O
split	O
.	O
This	O
performance	O
disparity	O
lessens	O
when	O
training	O
data	O
increases	O
;	O
however	O
,	O
prompt	O
tuned	O
T5	B-MethodName
-	O
large	O
continues	O
to	O
beat	O
its	O
finetuned	O
counterpart	O
by	O
5	O
points	O
at	O
500	O
SPIS	O
and	O
the	O
BART	B-MethodName
-	O
CopyPtr	O
model	O
by	O
1.4	O
points	O
.	O
Our	O
prompt	O
tuning	O
models	O
outperform	O
previously	O
reported	O
results	O
on	O
these	O
datasets	O
.	O
On	O
Overnight	O
,	O
our	O
best	O
model	O
-	O
T5	B-MethodName
-	O
xl	O
PT	O
with	O
canonical	O
representations	O
and	O
constrained	O
decodingoutperforms	O
the	O
BART	B-MethodName
FT	O
model	O
of	O
Shin	O
et	O
al	O
(	O
2021	O
)	O
by	O
5	O
accuracy	B-MetricName
points	O
,	O
and	O
GPT	B-MethodName
-	O
3	O
by	O
more	O
than	O
2	O
points	O
.	O
On	O
the	O
25	O
SPIS	O
split	O
of	O
TOPv2	B-DatasetName
,	O
we	O
see	O
an	O
average	O
improvement	O
of	O
more	O
than	O
5	O
points	O
compared	O
to	O
the	O
BART	B-MethodName
-	O
CopyPTR	O
of	O
Chen	O
et	O
al	O
(	O
2020	O
)	O
.	O

Here	O
we	O
provide	O
all	O
model	O
details	O
and	O
hyperparameters	O
to	O
reproduce	O
our	O
results	O
.	O
We	O
experiment	O
with	O
BART	B-MethodName
and	O
T5	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
;	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
,	O
two	O
large	O
pre	O
-	O
trained	O
encoder	O
-	O
decoder	O
language	O
models	O
.	O
BART	B-MethodName
is	O
trained	O
on	O
the	O
same	O
160	O
GB	O
text	O
dataset	O
used	O
to	O
train	O
RoBERTa	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
(	O
Paszke	O
et	O
al	O
,	O
2019	O
;	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O
Fine	O
-	O
tuning	O
baseline	O
We	O
compare	O
against	O
baselines	O
that	O
fine	O
-	O
tune	O
all	O
parameters	O
of	O
BART	B-MethodName
and	O
T5	B-MethodName
.	O
We	O
train	O
the	O
T5	B-MethodName
models	O
with	O
AdaFactor	B-MethodName
(	O
Shazeer	O
and	O
Stern	O
,	O
2018	O
)	O
and	O
BART	B-MethodName
with	O
Adam	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2020	O
;	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
.	O
On	O
TOPv2	B-DatasetName
,	O
we	O
use	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
10	O
−4	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	O
.	O
On	O
Overnight	O
,	O
we	O
use	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
10	O
−3	O
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	O
across	O
all	O
sizes	O
of	O
T5	B-MethodName
.	O
On	O
both	O
datasets	O
,	O
we	O
train	O
for	O
5000	O
epochs	O
and	O
perform	O
model	B-TaskName
selection	I-TaskName
by	O
early	B-MethodName
stopping	I-MethodName
on	O
the	O
validation	O
set	O
.	O
Prompt	O
tuning	O
We	O
follow	O
the	O
prompt	O
tuning	O
procedure	O
proposed	O
by	O
Lester	O
et	O
al	O
for	O
T5	B-MethodName
.	O
We	O
use	O
150	O
prompt	O
tokens	O
for	O
all	O
model	O
sizes	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.3	O
optimized	O
with	O
AdaFactor	B-MethodName
.	O
We	O
train	O
for	O
5000	O
epochs	O
on	O
most	O
domains	O
,	O
although	O
it	O
sometimes	O
took	O
as	O
many	O
as	O
20000	O
epochs	O
to	O
converge	O
on	O
the	O
low	O
-	O
resource	O
splits	O
.	O
Like	O
the	O
fine	O
-	O
tuned	O
baseline	O
,	O
we	O
perform	O
model	B-TaskName
selection	I-TaskName
with	O
best	O
exact	B-MetricName
match	I-MetricName
accuracy	B-MetricName
on	O
the	O
validation	O
set	O
.	O
We	O
apply	O
the	O
same	O
method	O
to	O
BART	B-MethodName
and	O
found	O
that	O
it	O
did	O
not	O
converge	O
under	O
a	O
number	O
of	O
hyperparameter	O
configurations	O
.	O
We	O
therefore	O
exclude	O
prompt	O
tuned	O
BART	B-MethodName
models	O
from	O
our	O
results	O
1	O
.	O
Constrained	O
Decoding	O
We	O
implement	O
grammarconstrained	O
decoding	O
by	O
building	O
a	O
prefix	O
tree	O
containing	O
all	O
canonical	O
or	O
meaning	O
representations	O
in	O
the	O
dataset	O
as	O
in	O
Shin	O
et	O
al	O
(	O
2021	O
)	O
.	O
When	O
doing	O
constrained	O
decoding	O
we	O
perform	O
a	O
beam	O
search	O
with	O
10	O
beams	O
and	O
use	O
the	O
prefix	O
tree	O
to	O
look	O
up	O
valid	O
single	O
token	O
continuations	O
of	O
the	O
decoded	O
sequence	O
.	O

Our	O
objective	O
naturally	O
rises	O
from	O
the	O
model	O
we	O
described	O
in	O
the	O
text	O
.	O
Furthermore	O
,	O
as	O
our	O
experiments	O
show	O
,	O
it	O
is	O
easy	O
to	O
generalize	O
this	O
objective	O
,	O
to	O
a	O
"	O
semi	O
-	O
supervised	O
"	O
setting	O
,	O
in	O
which	O
the	O
learner	O
has	O
access	O
to	O
only	O
a	O
few	O
fully	O
labeled	O
examples	O
and	O
additional	O
partially	O
labeled	O
examples	O
.	O
E.g.	O
,	O
if	O
only	O
segmentation	O
is	O
annotated	O
but	O
the	O
type	O
information	O
is	O
missing	O
.	O
The	O
loss	B-MetricName
function	O
is	O
a	O
linear	O
combination	O
of	O
the	O
negative	O
log	O
probability	O
of	O
each	O
sub	O
-	O
tasks	O
,	O
together	O
with	O
the	O
decision	O
module	O
:	O
J	O
=	O
−	O
N	O
i	O
log	O
P	O
(	O
y	O
i	O
|	O
x	O
i	O
)	O
+	O
α	B-HyperparameterName
log	O
P	O
(	O
y	O
seg	O
(	O
i	O
)	O
|	O
x	O
(	O
i	O
)	O
)	O
+	O
β	B-HyperparameterName
log	O
P	O
(	O
y	O
typ	O
(	O
i	O
)	O
|	O
x	O
(	O
i	O
)	O
)	O
,	O
(	O
1	O
)	O
where	O
N	O
is	O
the	O
number	O
of	O
examples	O
in	O
the	O
training	O
set	O
,	O
y	O
seg	O
and	O
y	O
typ	O
are	O
the	O
decomposed	O
segmentation	O
and	O
type	O
tags	O
corresponding	O
to	O
the	O
two	O
sub	O
-	O
task	O
modules	O
,	O
and	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
are	O
the	O
hyperparameters	O
controlling	O
the	O
importance	O
of	O
the	O
two	O
modules	O
contributions	O
respectively	O
.	O
If	O
the	O
training	O
example	O
is	O
fully	O
labeled	O
with	O
both	O
segmentation	O
and	O
type	O
annotated	O
,	O
training	O
is	O
straightforward	O
;	O
if	O
the	O
training	O
example	O
is	O
partially	O
labeled	O
,	O
e.g.	O
,	O
only	O
with	O
segmentation	O
but	O
without	O
type	O
,	O
we	O
can	O
set	O
the	O
log	O
probability	O
of	O
the	O
type	O
module	O
and	O
the	O
decision	O
module	O
0	B-DatasetName
and	O
only	O
train	O
the	O
segmentation	O
module	O
.	O
This	O
formulation	O
provides	O
extra	O
flexibility	O
of	O
using	O
partially	O
annotated	O
corpus	O
together	O
with	O
fully	O
annotated	O
corpus	O
to	O
improve	O
the	O
overall	O
performance	O
.	O

We	O
evaluated	O
our	O
models	O
over	O
three	O
different	O
sentiment	B-TaskName
analysis	I-TaskName
tasks	O
adapted	O
for	O
sequence	O
prediction	O
.	O
We	O
included	O
additional	O
results	O
for	O
multilingual	O
NER	B-TaskName
in	O
the	O
Appendix	O
for	O
reference	O
.	O
Target	O
Sentiment	O
Datasets	O
We	O
evaluated	O
our	O
models	O
on	O
the	O
targeted	O
sentiment	O
dataset	O
released	O
by	O
Mitchell	O
et	O
al	O
(	O
2013	O
)	O
,	O
which	O
consists	O
of	O
entity	O
and	O
sentiment	O
annotations	O
on	O
both	O
English	O
and	O
Spanish	O
tweets	O
.	O
Similar	O
to	O
previous	O
studies	O
(	O
Mitchell	O
et	O
al	O
,	O
2013	O
;	O
Zhang	O
et	O
al	O
,	O
2015	O
;	O
Li	O
and	O
Lu	O
,	O
2017	O
)	O
,	O
our	O
task	O
focuses	O
on	O
people	O
and	O
organizations	O
(	O
collapsed	O
into	O
volitional	O
named	O
entities	O
tags	O
)	O
and	O
the	O
sentiment	O
associated	O
with	O
their	O
description	O
in	O
tweets	O
.	O
After	O
this	O
processing	O
,	O
the	O
labels	O
of	O
each	O
tweets	O
are	O
composed	O
of	O
both	O
segmentation	O
(	O
entity	O
spans	O
)	O
and	O
types	O
(	O
sentiment	O
tags	O
)	O
.	O
We	O
used	O
the	O
original	O
10	O
-	O
fold	O
cross	O
validation	O
splits	O
to	O
calculate	O
averaged	O
F1	B-MetricName
score	I-MetricName
,	O
using	O
10	O
%	O
of	O
the	O
training	O
set	O
for	O
development	O
.	O
We	O
used	O
the	O
same	O
metrics	O
in	O
Zhang	O
et	O
al	O
(	O
2015	O
)	O
and	O
Li	O
and	O
Lu	O
(	O
2017	O
)	O
for	O
a	O
fair	O
comparison	O
.	O

Following	O
previous	O
studies	O
(	O
Ma	O
and	O
Hovy	O
,	O
2016	O
;	O
Liu	O
et	O
al	O
,	O
2018	O
)	O
showing	O
that	O
the	O
word	O
embedding	O
choice	O
can	O
significantly	O
influence	O
performance	O
,	O
we	O
used	O
the	O
pre	O
-	O
trained	O
GloVe	B-MethodName
100	O
dimension	O
Twitter	O
embeddings	O
only	O
for	O
all	O
tasks	O
in	O
the	O
main	O
text	O
.	O
All	O
the	O
words	O
not	O
contained	O
in	O
these	O
embeddings	O
(	O
OOV	O
,	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
)	O
are	O
treated	O
as	O
an	O
"	O
unknown	O
"	O
word	O
.	O
Our	O
models	O
were	O
deployed	O
with	O
minimal	O
hyper	O
parameters	O
tuning	O
,	O
and	O
can	O
be	O
briefly	O
summarized	O
as	O
:	O
the	O
character	O
embeddings	O
has	O
dimension	O
30	O
,	O
the	O
hidden	O
layer	O
dimension	O
of	O
the	O
character	O
level	O
LSTM	B-MethodName
is	O
25	O
,	O
and	O
the	O
hidden	O
layer	O
of	O
the	O
word	O
level	O
LSTM	B-MethodName
has	O
dimension	O
300	O
.	O
Similar	O
to	O
Liu	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
also	O
applied	O
highway	B-MethodName
networks	I-MethodName
(	O
Srivastava	O
et	O
al	O
,	O
2015	O
)	O
from	O
the	O
character	O
level	O
LSTM	B-MethodName
to	O
the	O
word	O
level	O
LSTM	B-MethodName
.	O
In	O
our	O
pilot	O
study	O
,	O
we	O
shrank	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
in	O
our	O
modular	O
architectures	O
to	O
around	O
one	O
third	O
such	O
that	O
the	O
total	O
number	O
of	O
parameter	O
is	O
similar	O
as	O
that	O
in	O
the	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
,	O
but	O
we	O
did	O
not	O
observe	O
a	O
significant	O
performance	O
change	O
so	O
we	O
kept	O
them	O
as	O
denoted	O
.	O
The	O
values	O
of	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
in	O
the	O
objective	O
function	O
were	O
always	O
set	O
to	O
1.0	O
.	O

We	O
used	O
BIOES	O
tagging	O
scheme	O
but	O
only	O
during	O
the	O
training	O
and	O
convert	O
them	O
back	O
to	O
BIO2	O
for	O
evaluation	O
for	O
all	O
tasks	O
3	O
.	O
Our	O
model	O
was	O
implemented	O
using	O
pytorch	O
(	O
Paszke	O
et	O
al	O
,	O
2017	O
)	O
.	O
To	O
help	O
improve	O
performance	O
we	O
parallelized	O
the	O
for	O
-	O
ward	O
algorithm	O
and	O
the	O
Viterbi	O
algorithm	O
on	O
the	O
GPU	O
.	O
All	O
the	O
experiments	O
were	O
run	O
on	O
NVIDIA	O
GPUs	O
.	O
We	O
used	O
the	O
Stochastic	B-MethodName
Gradient	I-MethodName
Descent	I-MethodName
(	O
SGD	B-MethodName
)	O
optimization	O
of	O
batch	B-HyperparameterName
size	I-HyperparameterName
10	O
,	O
with	O
a	O
momentum	O
0.9	O
to	O
update	O
the	O
model	O
parameters	O
,	O
with	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.01	O
,	O
the	O
decay	B-HyperparameterName
rate	I-HyperparameterName
0.05	O
;	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
decays	O
over	O
epochs	O
by	O
η/	O
(	O
1	O
+	O
e	O
*	O
ρ	O
)	O
,	O
where	O
η	O
is	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
e	O
is	O
the	O
epoch	B-HyperparameterName
number	I-HyperparameterName
,	O
and	O
ρ	O
is	O
the	O
decay	B-HyperparameterName
rate	I-HyperparameterName
.	O
We	O
used	O
gradient	O
clip	O
to	O
force	O
the	O
absolute	O
value	O
of	O
the	O
gradient	O
to	O
be	O
less	O
than	O
5.0	O
.	O
We	O
used	O
early	O
-	O
stop	O
to	O
prevent	O
over	O
-	O
fitting	O
,	O
with	O
a	O
patience	O
of	O
30	O
and	O
at	O
least	O
120	O
epochs	O
.	O
In	O
addition	O
to	O
dropout	O
,	O
we	O
used	O
Adversarial	O
Training	O
(	O
AT	O
)	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
,	O
to	O
regularize	O
our	O
model	O
as	O
the	O
parameter	O
numbers	O
increase	O
with	O
modules	O
.	O
AT	O
improves	O
robustness	O
to	O
small	O
worst	O
-	O
case	O
perturbations	O
by	O
computing	O
the	O
gradients	O
of	O
a	O
loss	B-MetricName
function	O
w.r.t	O
.	O
the	O
input	O
.	O
In	O
this	O
study	O
,	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
in	O
Eq	O
.	O
1	O
are	O
both	O
set	O
to	O
1.0	O
,	O
and	O
we	O
leave	O
other	O
tuning	O
choices	O
for	O
future	O
investigation	O
.	O

The	O
complete	O
results	O
of	O
our	O
experiments	O
on	O
the	O
target	O
sentiment	O
task	O
are	O
summarized	O
in	O
Tab	O
.	O
4	O
.	O
Our	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
-	O
TI	O
(	O
g	O
)	O
model	O
outperforms	O
all	O
the	O
other	O
competing	O
models	O
in	O
Precision	B-MetricName
,	O
Recall	B-MetricName
and	O
the	O
F1	B-MetricName
score	I-MetricName
.	O

The	O
proposed	O
twofold	O
modular	O
infusion	O
model	O
(	O
with	O
guided	O
gating	O
as	O
an	O
option	O
)	O
breaks	O
the	O
complex	O
learning	O
problem	O
into	O
several	O
sub	O
-	O
problems	O
and	O
then	O
integrate	O
them	O
using	O
joint	O
training	O
.	O
The	O
process	O
defined	O
by	O
this	O
formulation	O
has	O
more	O
parameters	O
and	O
requires	O
learning	O
multiple	O
objectives	O
jointly	O
.	O
Our	O
convergence	O
analysis	O
intends	O
to	O
evaluate	O
whether	O
the	O
added	O
complexity	O
leads	O
to	O
a	O
harder	O
learning	O
problem	O
(	O
i.e.	O
,	O
slower	O
to	O
converge	O
)	O
or	O
whether	O
the	O
tasks	O
constrain	O
each	O
other	O
and	O
as	O
a	O
result	O
can	O
be	O
efficiently	O
learned	O
.	O
We	O
compare	O
between	O
our	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
-	O
TI	O
(	O
g	O
)	O
model	O
and	O
recent	O
published	O
top	O
models	O
on	O
the	O
English	O
NER	B-TaskName
dataset	O
in	O
Figure	O
10	O
and	O
on	O
the	O
subjec	O
-	O
tive	O
polarity	O
disambiguation	O
datasets	O
in	O
Figure	O
11	O
.	O
The	O
curve	O
compares	O
convergence	O
speed	O
in	O
terms	O
of	O
learning	O
epochs	O
.	O
Our	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
-	O
TI	O
(	O
g	O
)	O
model	O
has	O
a	O
much	O
faster	O
convergence	O
rate	O
compared	O
to	O
the	O
other	O
models	O
.	O
Figure	O
11	O
:	O
Comparing	O
convergence	O
over	O
the	O
development	O
set	O
on	O
the	O
subjective	O
polarity	O
disambiguation	O
datasets	O
.	O
The	O
x	O
-	O
axis	O
is	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
and	O
the	O
y	O
-	O
axis	O
is	O
the	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
.	O

Aggression	O
and	O
related	O
activities	O
like	O
trolling	O
,	O
hate	B-DatasetName
speech	I-DatasetName
etc	O
.	O
involve	O
toxic	O
comments	O
in	O
various	O
forms	O
.	O
These	O
are	O
common	O
scenarios	O
in	O
today	O
's	O
time	O
and	O
websites	O
react	O
by	O
shutting	O
down	O
their	O
comment	O
sections	O
.	O
To	O
tackle	O
this	O
,	O
an	O
algorithmic	O
solution	O
is	O
preferred	O
to	O
human	O
moderation	O
which	O
is	O
slow	O
and	O
expensive	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
single	O
model	O
capsule	B-MethodName
network	I-MethodName
with	O
focal	B-MethodName
loss	I-MethodName
to	O
achieve	O
this	O
task	O
which	O
is	O
suitable	O
for	O
production	O
environment	O
.	O
Our	O
model	O
achieves	O
competitive	O
results	O
over	O
other	O
strong	O
baseline	O
methods	O
,	O
which	O
show	O
its	O
effectiveness	O
and	O
that	O
focal	B-MethodName
loss	I-MethodName
exhibits	O
significant	O
improvement	O
in	O
such	O
cases	O
where	O
class	O
imbalance	O
is	O
a	O
regular	O
issue	O
.	O
Additionally	O
,	O
we	O
show	O
that	O
the	O
problem	O
of	O
extensive	O
data	O
preprocessing	O
,	O
data	B-TaskName
augmentation	I-TaskName
can	O
be	O
tackled	O
by	O
capsule	O
networks	O
implicitly	O
.	O
We	O
achieve	O
an	O
overall	O
ROC	B-MetricName
AUC	I-MetricName
of	O
98.46	O
on	O
Kaggletoxic	O
comment	O
dataset	O
and	O
show	O
that	O
it	O
beats	O
other	O
architectures	O
by	O
a	O
good	O
margin	O
.	O
As	O
comments	O
tend	O
to	O
be	O
written	O
in	O
more	O
than	O
one	O
language	O
,	O
and	O
transliteration	O
is	O
a	O
common	O
problem	O
,	O
we	O
further	O
show	O
that	O
our	O
model	O
handles	O
this	O
effectively	O
by	O
applying	O
our	O
model	O
on	O
TRAC	O
shared	O
task	O
dataset	O
which	O
contains	O
comments	O
in	O
code	O
-	O
mixed	O
Hindi	O
-	O
English	O
.	O

In	O
today	O
's	O
time	O
,	O
with	O
an	O
ever	O
increasing	O
penetration	O
of	O
social	O
media	O
,	O
news	O
portals	O
,	O
blogs	O
,	O
QnA	O
forums	O
,	O
and	O
other	O
websites	O
that	O
allow	O
user	O
interaction	O
,	O
users	O
often	O
end	O
up	O
inviting	O
comments	O
that	O
are	O
nasty	O
,	O
harrasing	O
,	O
insulting	O
,	O
toxic	O
etc	O
.	O
This	O
can	O
have	O
adverse	O
effects	O
on	O
users	O
,	O
who	O
then	O
become	O
victims	O
of	O
cyberbullying	O
or	O
online	O
harrasment	O
.	O
An	O
online	O
survey	O
carried	O
out	O
by	O
the	O
Pew	O
Research	O
Centre	O
in	O
2017	O
states	O
that	O
4	O
in	O
10	O
Americans	O
have	O
personally	O
experienced	O
online	O
harrasment	O
.	O
Strikingly	O
,	O
1	O
in	O
5	O
Americans	O
have	O
witnessed	O
severe	O
form	O
of	O
online	O
harrasment	O
like	O
physical	O
threats	O
,	O
stalking	O
,	O
sexual	O
harrasment	O
etc	O
.	O
There	O
are	O
several	O
challenges	O
associated	O
with	O
solving	O
this	O
kind	O
of	O
problem	O
.	O
First	O
being	O
the	O
problem	O
of	O
class	O
imbalance	O
found	O
in	O
the	O
dataset	O
.	O
Since	O
such	O
type	O
of	O
comments	O
are	O
sparse	O
in	O
nature	O
,	O
they	O
introduce	O
skewness	O
in	O
the	O
dataset	O
.	O
There	O
are	O
several	O
ways	O
to	O
handle	O
this	O
problem	O
,	O
however	O
,	O
we	O
choose	O
a	O
more	O
recent	O
technique	O
which	O
modifies	O
the	O
standard	O
cross	O
entropy	O
loss	B-MetricName
function	O
known	O
as	O
Focal	B-MethodName
Loss	I-MethodName
(	O
Lin	O
et	O
al	O
,	O
2017	O
)	O
.	O
We	O
will	O
briefly	O
describe	O
how	O
it	O
helps	O
in	O
improving	O
classifier	O
performance	O
.	O
The	O
next	O
problem	O
we	O
want	O
to	O
address	O
is	O
that	O
of	O
data	O
preprocessing	O
.	O
This	O
is	O
the	O
most	O
time	O
consuming	O
task	O
and	O
requires	O
a	O
good	O
understanding	O
of	O
the	O
data	O
.	O
However	O
,	O
we	O
wish	O
to	O
minimise	O
this	O
process	O
so	O
as	O
to	O
have	O
a	O
good	O
model	O
with	O
minimal	O
preprocessing	O
of	O
the	O
data	O
.	O
Another	O
frequently	O
observed	O
challenge	O
is	O
transliteration	O
,	O
which	O
is	O
often	O
observed	O
,	O
especially	O
,	O
when	O
we	O
are	O
working	O
with	O
text	O
data	O
from	O
social	O
networking	O
websites	O
.	O
Users	O
tend	O
to	O
speak	O
in	O
more	O
than	O
one	O
language	O
in	O
the	O
same	O
statement	O
.	O
This	O
leads	O
to	O
several	O
out	O
of	O
vocabulary	O
or	O
OOV	O
words	O
for	O
which	O
the	O
model	O
would	O
not	O
have	O
any	O
word	O
embedding	O
.	O
We	O
use	O
randomly	O
initialised	O
word	B-TaskName
embeddings	I-TaskName
in	O
such	O
a	O
case	O
and	O
show	O
how	O
they	O
can	O
be	O
trained	O
during	O
model	O
training	O
procedure	O
such	O
that	O
it	O
results	O
in	O
clusters	O
of	O
OOV	O
words	O
which	O
have	O
similar	O
meaning	O
in	O
Hindi	O
.	O
We	O
propose	O
to	O
tackle	O
all	O
the	O
above	O
described	O
challenges	O
using	O
a	O
single	O
model	O
as	O
opposed	O
to	O
ensemble	O
of	O
several	O
other	O
models	O
,	O
which	O
is	O
a	O
common	O
practice	O
in	O
such	O
competitive	O
challenges	O
.	O
We	O
also	O
show	O
that	O
our	O
proposed	O
model	O
can	O
converge	O
really	O
quickly	O
,	O
hence	O
the	O
model	O
can	O
be	O
trained	O
in	O
lesser	O
time	O
.	O
This	O
is	O
essential	O
when	O
the	O
model	O
has	O
to	O
be	O
deployed	O
in	O
a	O
production	O
environment	O
where	O
it	O
requires	O
retraining	O
periodically	O
.	O

Proposed	O
Model	O
:	O
The	O
model	O
proposed	O
in	O
(	O
Zhao	O
et	O
al	O
,	O
2018	O
)	O
has	O
been	O
used	O
for	O
the	O
experimentation	O
with	O
an	O
inclusion	O
of	O
Focal	B-MethodName
Loss	I-MethodName
(	O
Lin	O
et	O
al	O
,	O
2017	O
)	O
as	O
a	O
loss	B-MetricName
function	O
to	O
address	O
the	O
class	O
imbalance	O
problem	O
.	O
In	O
our	O
experiments	O
we	O
have	O
compared	O
performances	O
of	O
CNNs	O
and	O
RNNs	O
as	O
feature	O
extractors	O
and	O
found	O
that	O
sentence	O
representation	O
obtained	O
from	O
RNNs	O
performs	O
better	O
than	O
representations	O
obtained	O
after	O
applying	O
convolution	B-MethodName
operation	O
,	O
although	O
CNNs	O
tends	O
to	O
perform	O
better	O
on	O
short	O
texts	O
.	O
The	O
model	O
consists	O
of	O
four	O
layers	O
:	O
(	O
i	O
)	O
Word	O
Embedding	O
Layer	O
:	O
We	O
represent	O
every	O
comment	O
x	O
i	O
,	O
as	O
a	O
sequence	O
of	O
one	O
-	O
hot	O
encoding	O
of	O
its	O
words	O
,	O
x	O
i	O
=	O
(	O
w	O
1	O
,	O
w	O
2	O
,	O
...	O
w	O
n	O
)	O
of	O
length	O
n	O
max	O
,	O
which	O
is	O
the	O
maximum	O
length	O
of	O
the	O
comment	O
,	O
with	O
zero	O
padding	O
.	O
Such	O
a	O
sequence	O
becomes	O
the	O
input	O
to	O
the	O
embedding	O
layer	O
.	O
To	O
represent	O
word	O
tokens	O
several	O
ideas	O
like	O
sparse	O
representation	O
or	O
dense	O
representation	O
(	O
Collobert	O
and	O
Weston	O
,	O
2008	O
;	O
Bengio	O
et	O
al	O
,	O
2003	O
)	O
have	O
been	O
proposed	O
.	O
(	O
ii	O
)	O
Feature	O
Extraction	O
Layer	O
:	O
This	O
layer	O
has	O
been	O
used	O
to	O
extract	O
either	O
n	O
-	O
grams	O
feature	O
at	O
different	O
position	O
of	O
a	O
sentence	O
through	O
different	O
filters	O
(	O
CNNs	O
)	O
or	O
long	O
term	O
temporal	O
dependencies	O
within	O
the	O
sentence	O
(	O
RNNs	O
)	O
.	O
We	O
use	O
RNNs	O
as	O
feature	O
extractors	O
in	O
our	O
final	O
model	O
.	O
(	O
iii	O
)	O
Capsule	O
Layer	O
:	O
The	O
Capsule	O
layer	O
is	O
primarily	O
composed	O
of	O
two	O
sub	O
-	O
layers	O
Primary	O
Capsule	O
Layer	O
and	O
Convolutional	O
Capsule	O
Layer	O
.	O
The	O
primary	O
capsule	O
layer	O
is	O
supposed	O
to	O
capture	O
the	O
instantiated	O
parameters	O
of	O
the	O
inputs	O
,	O
for	O
example	O
,	O
in	O
case	O
of	O
texts	O
local	O
order	O
of	O
words	O
and	O
their	O
semantic	O
representation	O
.	O
Suppose	O
we	O
haveê	O
number	O
of	O
feature	O
extractors	O
,	O
then	O
the	O
input	O
to	O
the	O
Primary	O
capsule	O
layer	O
will	O
be	O
Z	O
R	O
n×ê	O
(	O
where	O
n	O
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
timesteps	I-HyperparameterName
in	O
RNNs	O
)	O
.	O
The	O
primary	O
capsules	O
transform	O
a	O
scalar	O
-	O
output	O
feature	O
detector	O
to	O
vector	O
-	O
valued	O
capsules	O
to	O
capture	O
the	O
instantiated	O
features	O
.	O
Let	O
d	O
be	O
the	O
dimension	O
of	O
each	O
capsule	O
,	O
then	O
for	O
each	O
capsule	O
p	O
i	O
R	O
d	O
,	O
where	O
p	O
denotes	O
instantiated	O
parameters	O
set	O
of	O
a	O
capsule	O
(	O
Sabour	O
et	O
al	O
,	O
2017	O
)	O
,	O
we	O
have	O
p	O
i	O
=	O
g	O
(	O
W	O
Z	O
i	O
+	O
b	O
)	O
,	O
where	O
Z	O
i	O
is	O
captured	O
by	O
RNNs	O
in	O
the	O
feature	O
extractor	O
layer	O
.	O
Here	O
,	O
g	O
is	O
the	O
nonlinear	O
squash	O
function	O
which	O
shrinks	O
the	O
small	O
vectors	O
to	O
around	O
0	B-DatasetName
and	O
large	O
vectors	O
around	O
1	O
.	O
(	O
iv	O
)	O
The	O
Convolutional	O
Capsule	O
:	O
The	O
Conv	O
layers	O
capsules	O
output	O
a	O
local	O
grid	O
of	O
vectors	O
to	O
capsules	O
in	O
earlier	O
layers	O
using	O
different	O
transformation	O
matrices	O
for	O
each	O
capsule	O
and	O
grids	O
member	O
(	O
Sabour	O
et	O
al	O
,	O
2017	O
)	O
.Capsule	O
networks	O
are	O
trained	O
using	O
a	O
dynamic	O
routing	O
algorithm	O
that	O
overlooks	O
words	O
that	O
are	O
not	O
important	O
or	O
unrelated	O
in	O
the	O
text	O
,	O
like	O
stopwords	O
and	O
name	O
mentions	O
.	O
F	O
L	O
(	O
p	O
t	O
)	O
=	O
−α	O
t	O
(	O
1	O
−	O
p	O
t	O
)	O
γ	B-HyperparameterName
log	O
(	O
p	O
t	O
)	O
,	O
where	O
p	O
t	O
=	O
{	O
p	O
if	O
y	O
=	O
1	O
1	O
−	O
p	O
else	O
γ	B-HyperparameterName
is	O

For	O
all	O
our	O
experiments	O
we	O
have	O
used	O
pre	O
-	O
trained	O
embeddings	O
for	O
each	O
word	O
token	O
obtained	O
from	O
(	O
Joulin	O
et	O
al	O
,	O
2016	O
)	O
.	O
We	O
have	O
also	O
exploited	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
,	O
random	O
and	O
manually	O
trained	O
embeddings	O
for	O
initialization	O
.	O
After	O
experimentation	O
,	O
fasttext	B-MethodName
embeddings	O
with	O
dimension	O
of	O
300	O
were	O
found	O
to	O
perform	O
better	O
than	O
rest	O
of	O
the	O
initialization	O
process	O
.	O
In	O
our	O
experiments	O
we	O
observed	O
that	O
RMSProp	B-MethodName
(	O
Tieleman	O
and	O
Hinton	O
,	O
2012	O
)	O
and	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
as	O
an	O
optimizer	B-HyperparameterName
works	O
well	O
for	O
training	O
RNNs	O
and	O
CNNs	O
respectively	O
and	O
used	O
this	O
throughout	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
was	O
kept	O
between	O
[	O
.1	O
and	O
.001	O
]	O
.	O
For	O
CNNs	O
,	O
number	O
of	O
Kernels	O
was	O
chosen	O
from	O
the	O
range	O
[	O
128	O
,	O
256	O
,	O
512	O
]	O
and	O
the	O
LSTM	B-MethodName
units	O
were	O
selected	O
from	O
the	O
range	O
[	O
128	O
,	O
256	O
]	O
.	O
In	O
all	O
of	O
our	O
experiments	O
with	O
the	O
proposed	O
model	O
only	O
a	O
single	O
layer	O
for	O
feature	O
extraction	O
was	O
used	O
.	O
Number	O
of	O
capsules	O
was	O
varied	O
from	O
[	O
8	O
,	O
10	O
,	O
16	O
]	O
,	O
the	O
vector	O
length	O
of	O
8	O
for	O
each	O
capsule	O
was	O
found	O
to	O
be	O
the	O
best	O
,	O
and	O
the	O
dropout	O
values	O
for	O
RNNs	O
were	O
taken	O
as	O
per	O
suggestions	O
from	O
(	O
Zaremba	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
α	B-HyperparameterName
and	O
γ	B-HyperparameterName
values	O
in	O
focal	B-MethodName
loss	I-MethodName
were	O
experimented	O
for	O
[	O
1.5	O
,	O
2	O
,	O
2.5	O
,	O
3	O
,	O
3.5	O
]	O
and	O
[	O
.2	O
,	O
.25	O
,	O
.3	O
]	O
and	O
finally	O
α	B-HyperparameterName
=	O
2	O
and	O
γ=0.25	O
were	O
taken	O
.	O

The	O
proposed	O
CapsNet	B-MethodName
architecture	O
was	O
able	O
to	O
beat	O
other	O
strong	O
baseline	O
algorithms	O
with	O
reasonable	O
difference	O
in	O
accuracies	O
with	O
minimal	O
preprocessing	O
.	O
We	O
demonstrated	O
that	O
using	O
focal	B-MethodName
loss	I-MethodName
along	O
with	O
CapsNet	B-MethodName
gave	O
us	O
.25	O
raise	O
in	O
the	O
ROC	B-MetricName
-	I-MetricName
AUC	I-MetricName
for	O
Kaggle	O
's	O
toxic	B-TaskName
comment	I-TaskName
classification	I-TaskName
and	O
1.39	O
and	O
.80	O
gain	O
in	O
F1	B-MetricName
scores	O
on	O
TRAC	O
shared	O
task	O
dataset	O
in	O
English	O
,	O
from	O
Facebook	O
and	O
Twitter	O
comments	O
respectively	O
.	O
All	O
of	O
our	O
experiments	O
were	O
performed	O
on	O
NVIDIA	O
Quadro	O
M1200	O
4096	O
MB	O
GPU	O
system	O
with	O
32	O
GB	O
RAM	B-MethodName
and	O
Intel	O
i7	O
processor	O
.	O
The	O
model	O
took	O
almost	O
33	O
minutes	O
for	O
an	O
epoch	O
to	O
train	O
which	O
was	O
faster	O
in	O
comparison	O
with	O
other	O
models	O
,	O
with	O
exception	O
to	O
the	O
models	O
using	O
CNNs	O
as	O
feature	O
extractors	O
.	O
For	O
example	O
,	O
the	O
second	O
best	O
performing	O
model	O
,	O
which	O
uses	O
Pre	O
-	O
trained	O
LSTM	B-MethodName
embeddings	O
takes	O
more	O
than	O
a	O
day	O
for	O
the	O
autoencoder	B-MethodName
to	O
train	O
and	O
further	O
39	O
+	O
minutes	O
for	O
each	O
epoch	O
.	O
Hence	O
,	O
we	O
can	O
say	O
that	O
our	O
model	O
is	O
viable	O
for	O
production	O
environment	O
.	O
We	O
have	O
tested	O
the	O
capability	O
of	O
the	O
architecture	O
to	O
handle	O
the	O
OOV	O
words	O
or	O
misspelled	O
words	O
.	O
For	O
this	O
we	O
used	O
TRAC	O
shared	O
dataset	O
,	O
initialised	O
the	O
word	B-TaskName
embeddings	I-TaskName
randomly	O
and	O
trained	O
the	O
model	O
for	O
classification	O
process	O
.	O
Next	O
,	O
we	O
enabled	O
the	O
embeddings	O
to	O
be	O
changed	O
during	O
training	O
process	O
which	O
is	O
mentioned	O
as	O
dynamic	O
channel	O
in	O
(	O
Kim	O
,	O
2014	O
)	O
to	O
let	O
the	O
model	O
learn	O
new	O
embeddings	O
.	O
After	O
training	O
,	O
we	O
took	O
the	O
weights	O
of	O
embedding	O
layer	O
and	O
plotted	O
these	O
embedings	O
using	O
Tensorboard	O
(	O
Abadi	O
et	O
al	O
,	O
2015	O
)	O
.	O
From	O
figure	O
2	O
we	O
can	O
see	O
that	O
the	O
model	O
is	O
able	O
to	O
minimise	O
the	O
distance	O
between	O
the	O
misspelled	O
word	O
and	O
is	O
able	O
to	O
capture	O
the	O
relationship	O
between	O
transliterated	O
words	O
as	O
in	O
Table	O
:	O
2	O
.	O
We	O
found	O
that	O
total	O
of	O
3	O
clusters	O
were	O
formed	O
after	O
the	O
experiment	O
as	O
shown	O
in	O
Fig	O
:	O
2b	O
.	O
We	O
investigated	O
these	O
clusters	O
and	O
found	O
that	O
some	O
of	O
the	O
highly	O
used	O
words	O
in	O
the	O
comments	O
belonged	O
to	O
certain	O
classes	O
.	O
For	O
example	O
,	O
one	O
of	O
the	O
cluster	O
contained	O
more	O
of	O
neutral	O
words	O
,	O
another	O
cluster	O
contained	O
highly	O
aggressive	O
and	O
abusive	O
words	O
,	O
and	O
the	O
third	O
cluster	O
contained	O
some	O
toxic	O
words	O
along	O
with	O
place	O
and	O
country	O
names	O
related	O
to	O
one	O
's	O
origin	O
which	O
were	O
used	O
in	O
some	O
foul	O
comments	O
.	O
We	O
show	O
the	O
capability	O
of	O
our	O
model	O
to	O
tackle	O
the	O
problem	O
of	O
overfitting	O
,	O
as	O
observed	O
during	O
training	O
we	O
see	O
the	O
model	O
to	O
have	O
comparatively	O
lower	O
difference	O
in	O
training	O
and	O
validation	O
loss	B-MetricName
than	O
other	O
models	O
.	O
Same	O
can	O
be	O
seen	O
from	O
Fig	O
:	O
2a	O
the	O
loss	B-MetricName
margin	O
difference	O
does	O
n't	O
change	O
.	O
We	O
have	O
shown	O
that	O
,	O
not	O
only	O
our	O
model	O
has	O
performed	O
well	O
on	O
the	O
classification	O
task	O
,	O
it	O
also	O
has	O
ability	O
to	O
generalise	O
well	O
and	O
can	O
learn	O
good	O
representation	O
for	O
word	O
tokens	O
.	O

BERT	B-MethodName
is	O
one	O
of	O
the	O
key	O
innovations	O
in	O
the	O
recent	O
progress	O
of	O
contextualized	O
representation	B-TaskName
learning	I-TaskName
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Howard	O
and	O
Ruder	O
,	O
2018	O
;	O
Radford	O
et	O
al	O
,	O
2018a	O
;	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
idea	O
behind	O
the	O
progress	O
is	O
that	O
even	O
though	O
the	O
word	O
embedding	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
;	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
layer	O
(	O
in	O
a	O
typical	O
neural	O
network	O
for	O
NLP	O
)	O
is	O
trained	O
from	O
large	O
-	O
scale	O
corpora	O
,	O
training	O
a	O
wide	O
variety	O
of	O
neural	O
architectures	O
that	O
encode	O
contextual	O
representations	O
only	O
from	O
the	O
limited	O
supervised	O
data	O
on	O
end	O
tasks	O
is	O
insufficient	O
.	O
Unlike	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
and	O
ULMFiT	B-MethodName
Figure	O
1	O
:	O
Overview	O
of	O
BERT	B-MethodName
settings	O
for	O
review	O
reading	B-TaskName
comprehension	I-TaskName
(	O
RRC	O
)	O
,	O
aspect	B-TaskName
extraction	I-TaskName
(	O
AE	B-MethodName
)	O
and	O
aspect	O
sentiment	O
classification	O
(	O
ASC	O
)	O
.	O
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
that	O
are	O
intended	O
to	O
provide	O
additional	O
features	O
for	O
a	O
particular	O
architecture	O
that	O
bears	O
human	O
's	O
understanding	O
of	O
the	O
end	O
task	O
,	O
BERT	B-MethodName
adopts	O
a	O
fine	O
-	O
tuning	O
approach	O
that	O
requires	O
almost	O
no	O
specific	O
architecture	O
for	O
each	O
end	O
task	O
.	O
This	O
is	O
desired	O
as	O
an	O
intelligent	O
agent	B-DatasetName
should	O
minimize	O
the	O
use	O
of	O
prior	O
human	O
knowledge	O
in	O
the	O
model	O
design	O
.	O
Instead	O
,	O
it	O
should	O
learn	O
such	O
knowledge	O
from	O
data	O
.	O
BERT	B-MethodName
has	O
two	O
parameter	O
intensive	O
settings	O
:	O
BERT	B-MethodName
BASE	B-MethodName
:	O
12	O
layers	O
,	O
768	O
hidden	O
dimensions	O
and	O
12	O
attention	O
heads	O
(	O
in	O
transformer	O
)	O
with	O
the	O
total	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
,	O
110	O
M	O
;	O
BERT	B-MethodName
LARGE	O
:	O
24	O
layers	O
,	O
1024	O
hidden	O
dimensions	O
and	O
16	O
attention	O
heads	O
(	O
in	O
transformer	O
)	O
with	O
the	O
total	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
,	O
340M.	O
We	O
only	O
extend	O
BERT	B-MethodName
with	O
one	O
extra	O
taskspecific	O
layer	O
and	O
fine	O
-	O
tune	O
BERT	B-MethodName
on	O
each	O
end	O
task	O
.	O
We	O
focus	O
on	O
three	O
(	O
3	O
)	O
review	O
-	O
based	O
tasks	O
:	O
review	O
reading	B-TaskName
comprehension	I-TaskName
(	O
RRC	O
)	O
,	O
aspect	B-TaskName
extraction	I-TaskName
(	O
AE	B-MethodName
)	O
and	O
aspect	O
sentiment	O
classification	O
(	O
ASC	O
)	O
.	O
The	O
inputs	O
/	O
outputs	O
settings	O
are	O
depicted	O
in	O
Figure	O
1	O
and	O
detailed	O
in	O
the	O
following	O
subsections	O
.	O

Following	O
the	O
success	O
of	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
and	O
BERT	B-MethodName
's	O
SQuAD	B-DatasetName
implementation	O
,	O
we	O
design	O
review	O
reading	B-TaskName
comprehension	I-TaskName
as	O
follows	O
.	O
Given	O
a	O
question	O
q	O
=	O
(	O
q	O
1	O
,	O
.	O
.	O
.	O
,	O
q	O
m	O
)	O
asking	O
for	O
an	O
answer	O
from	O
a	O
review	O
d	O
=	O
(	O
d	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
n	O
)	O
,	O
we	O
formulate	O
the	O
input	O
as	O
a	O
sequence	O
x	O
=	O
(	O
[	O
CLS	O
]	O
,	O
q	O
1	O
,	O
.	O
.	O
.	O
,	O
q	O
m	O
,	O
[	O
SEP	O
]	O
,	O
d	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
n	O
,	O
[	O
SEP	O
]	O
)	O
,	O
where	O
[	O
CLS	O
]	O
is	O
a	O
dummy	O
token	O
not	O
used	O
for	O
RRC	O
and	O
[	O
SEP	O
]	O
is	O
intended	O
to	O
separate	O
q	O
and	O
d.	O
Let	O
BERT	B-MethodName
(	O
)	O
be	O
the	O
pre	O
-	O
trained	O
(	O
or	O
posttrained	O
as	O
in	O
the	O
next	O
section	O
)	O
BERT	B-MethodName
model	O
.	O
We	O
first	O
obtain	O
the	O
hidden	O
representation	O
as	O
h	O
=	O
BERT	B-MethodName
(	O
x	O
)	O
R	O
r	O
h	O
*	O
|	O
x	O
|	O
,	O
where	O
|	O
x	O
|	O
is	O
the	O
length	O
of	O
the	O
input	O
sequence	O
and	O
r	O
h	O
is	O
the	O
size	O
of	O
the	O
hidden	O
dimension	O
.	O
Then	O
the	O
hidden	O
representation	O
is	O
passed	O
to	O
two	O
separate	O
dense	O
layers	O
followed	O
by	O
softmax	B-MethodName
functions	O
:	O
Training	O
the	O
RRC	O
model	O
involves	O
minimizing	O
the	O
loss	B-MetricName
that	O
is	O
designed	O
as	O
the	O
averaged	O
cross	O
entropy	O
on	O
the	O
two	O
pointers	O
:	O
l	O
1	O
=	O
softmax	B-MethodName
(	O
W	O
1	O
h	O
+	O
b	O
1	O
)	O
and	O
l	O
2	O
=	O
softmax	B-MethodName
(	O
W	O
2	O
h	O
+	O
b	O
2	O
)	O
,	O
where	O
W	O
1	O
,	O
W	O
2	O
R	O
r	O
h	O
and	O
b	O
1	O
,	O
b	O
2	O
R.	O
L	O
RRC	O
=	O
−	O
log	O
l	O
1	O
I	O
(	O
s	O
)	O
+	O
log	O
l	O
2	O
I	O
(	O
e	O
)	O
2	O
,	O
where	O
I	O
(	O
s	O
)	O
and	O
I	O
(	O
e	O
)	O
are	O
one	O
-	O
hot	O
vectors	O
representing	O
the	O
ground	O
truths	O
of	O
pointers	O
.	O
RRC	O
may	O
suffer	O
from	O
the	O
prohibitive	O
cost	O
of	O
annotating	O
large	O
-	O
scale	O
training	O
data	O
covering	O
a	O
wide	O
range	O
of	O
domains	O
.	O
And	O
BERT	B-MethodName
severely	O
lacks	O
two	O
kinds	O
of	O
prior	O
knowledge	O
:	O
(	O
1	O
)	O
large	O
-	O
scale	O
domain	O
knowledge	O
(	O
e.g.	O
,	O
about	O
a	O
specific	O
product	O
category	O
)	O
,	O
and	O
(	O
2	O
)	O
task	O
-	O
awareness	O
knowledge	O
(	O
MRC	O
/	O
RRC	O
in	O
this	O
case	O
)	O
.	O
We	O
detail	O
the	O
technique	O
of	O
jointly	O
incorporating	O
these	O
two	O
types	O
of	O
knowledge	O
in	O
Sec	O
.	O
4	O
.	O

As	O
a	O
core	O
task	O
in	O
ABSA	O
,	O
aspect	B-TaskName
extraction	I-TaskName
(	O
AE	B-MethodName
)	O
aims	O
to	O
find	O
aspects	O
that	O
reviewers	O
have	O
expressed	O
opinions	O
on	O
(	O
Hu	O
and	O
Liu	O
,	O
2004	O
)	O
.	O
In	O
supervised	O
settings	O
,	O
it	O
is	O
typically	O
modeled	O
as	O
a	O
sequence	O
labeling	O
task	O
,	O
where	O
each	O
token	O
from	O
a	O
sentence	O
is	O
labeled	O
as	O
one	O
of	O
{	O
Begin	O
,	O
Inside	O
,	O
Outside	O
}	O
.	O
A	O
continuous	O
chunk	O
of	O
tokens	O
that	O
are	O
labeled	O
as	O
one	O
B	O
and	O
followed	O
by	O
zero	O
or	O
more	O
Is	O
forms	O
an	O
aspect	O
.	O
The	O
input	O
sentence	O
with	O
m	O
words	O
is	O
constructed	O
as	O
x	O
=	O
(	O
[	O
CLS	O
]	O
,	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
m	O
,	O
[	O
SEP	O
]	O
)	O
.	O
After	O
h	O
=	O
BERT	B-MethodName
(	O
x	O
)	O
,	O
we	O
apply	O
a	O
dense	O
layer	O
and	O
a	O
softmax	B-MethodName
for	O
each	O
position	O
of	O
the	O
sequence	O
:	O
l	O
3	O
=	O
softmax	B-MethodName
(	O
W	O
3	O
h+b	O
3	O
)	O
,	O
where	O
W	O
3	O
R	O
3	O
*	O
r	O
h	O
and	O
b	O
3	O
R	O
3	O
(	O
3	O
is	O
the	O
total	O
number	O
of	O
labels	O
(	O
BIO	O
)	O
)	O
.	O
Softmax	B-MethodName
is	O
applied	O
along	O
the	O
dimension	O
of	O
labels	O
for	O
each	O
position	O
and	O
l	O
3	O
[	O
0	B-DatasetName
,	O
1	O
]	O
3	O
*	O
|	O
x	O
|	O
.	O
The	O
labels	O
are	O
predicted	O
as	O
taking	O
argmax	O
function	O
at	O
each	O
position	O
of	O
l	O
3	O
and	O
the	O
loss	B-MetricName
function	O
is	O
the	O
averaged	O
cross	O
entropy	O
across	O
all	O
positions	O
of	O
a	O
sequence	O
.	O
AE	B-MethodName
is	O
a	O
task	O
that	O
requires	O
intensive	O
domain	O
knowledge	O
(	O
e.g.	O
,	O
knowing	O
that	O
"	O
screen	O
"	O
is	O
a	O
part	O
of	O
a	O
laptop	O
)	O
.	O
Previous	O
study	O
(	O
Xu	O
et	O
al	O
,	O
2018a	O
)	O
has	O
shown	O
that	O
incorporating	O
domain	O
word	B-TaskName
embeddings	I-TaskName
greatly	O
improve	O
the	O
performance	O
.	O
Adapting	O
BERT	B-MethodName
's	O
general	O
language	O
models	O
to	O
domain	O
reviews	O
is	O
crucial	O
for	O
AE	B-MethodName
,	O
as	O
shown	O
in	O
Sec	O
.	O
5	O
.	O

As	O
discussed	O
in	O
the	O
introduction	O
,	O
fine	O
-	O
tuning	O
BERT	B-MethodName
directly	O
on	O
the	O
end	O
task	O
that	O
has	O
limited	O
tuning	O
data	O
faces	O
both	O
domain	O
challenges	O
and	O
taskawareness	O
challenge	O
.	O
To	O
enhance	O
the	O
performance	O
of	O
RRC	O
(	O
and	O
also	O
AE	B-MethodName
and	O
ASC	O
)	O
,	O
we	O
may	O
need	O
to	O
reduce	O
the	O
bias	O
introduced	O
by	O
non	O
-	O
review	O
knowledge	O
(	O
e.g.	O
,	O
from	O
Wikipedia	O
corpora	O
)	O
and	O
fuse	O
domain	O
knowledge	O
(	O
DK	O
)	O
(	O
from	O
unsupervised	O
domain	O
data	O
)	O
and	O
task	O
knowledge	O
(	O
from	O
supervised	O
MRC	O
task	O
but	O
out	O
-	O
of	O
-	O
domain	O
data	O
)	O
.	O
Given	O
MRC	O
is	O
a	O
general	O
task	O
with	O
answers	O
of	O
questions	O
covering	O
almost	O
all	O
document	O
contents	O
,	O
a	O
large	O
-	O
scale	O
MRC	O
supervised	O
corpus	O
may	O
also	O
benefit	O
AE	B-MethodName
and	O
ASC	O
.	O
Eventually	O
,	O
we	O
aim	O
to	O
have	O
a	O
general	O
-	O
purpose	O
post	O
-	O
training	O
strategy	O
that	O
can	O
exploit	O
the	O
above	O
two	O
kinds	O
of	O
knowledge	O
for	O
end	O
tasks	O
.	O
To	O
post	O
-	O
train	O
on	O
domain	O
knowledge	O
,	O
we	O
leverage	O
the	O
two	O
novel	O
pre	O
-	O
training	O
objectives	O
from	O
BERT	B-MethodName
:	O
masked	O
language	O
model	O
(	O
MLM	B-DatasetName
)	O
and	O
next	O
sentence	O
7	O
prediction	O
(	O
NSP	O
)	O
.	O
The	O
former	O
predicts	O
randomly	O
masked	O
words	O
and	O
the	O
latter	O
detects	O
whether	O
two	O
sides	O
of	O
the	O
input	O
are	O
from	O
the	O
same	O
document	O
or	O
not	O
.	O
A	O
training	O
example	O
is	O
formulated	O
as	O
(	O
[	O
CLS	O
]	O
,	O
x	O
1	O
:	O
j	O
,	O
[	O
SEP	O
]	O
,	O
x	O
j+1	O
:	O
n	O
,	O
[	O
SEP	O
]	O
)	O
,	O
where	O
x	O
1	O
:	O
n	O
is	O
a	O
document	O
(	O
with	O
randomly	O
masked	O
words	O
)	O
split	O
into	O
two	O
sides	O
x	O
1	O
:	O
j	O
and	O
x	O
j+1	O
:	O
n	O
and	O
[	O
SEP	O
]	O
separates	O
those	O
two	O
.	O
MLM	B-DatasetName
is	O
crucial	O
for	O
injecting	O
review	O
domain	O
knowledge	O
and	O
for	O
alleviating	O
the	O
bias	O
of	O
the	O
knowledge	O
from	O
Wikipedia	O
.	O
For	O
example	O
,	O
in	O
the	O
Wikipedia	O
domain	O
,	O
BERT	B-MethodName
may	O
learn	O
to	O
guess	O
the	O
[	O
MASK	O
]	O
in	O
"	O
The	O
[	O
MASK	O
]	O
is	O
bright	O
"	O
as	O
"	O
sun	O
"	O
.	O
But	O
in	O
a	O
laptop	O
domain	O
,	O
it	O
could	O
be	O
"	O
screen	O
"	O
.	O
Further	O
,	O
if	O
the	O
[	O
MASK	O
]	O
ed	O
word	O
is	O
an	O
opinion	O
word	O
in	O
"	O
The	O
touch	O
screen	O
is	O
[	O
MASK	O
]	O
"	O
,	O
this	O
objective	O
challenges	O
BERT	B-MethodName
to	O
learn	O
the	O
representations	O
for	O
fine	O
-	O
grained	O
opinion	O
words	O
like	O
"	O
great	O
"	O
or	O
"	O
terrible	O
"	O
for	O
[	O
MASK	O
]	O
.	O
The	O
objective	O
of	O
NSP	O
further	O
encourages	O
BERT	B-MethodName
to	O
learn	O
contextual	O
representation	O
beyond	O
word	O
-	O
level	O
.	O
In	O
the	O
context	O
of	O
reviews	O
,	O
NSP	O
formulates	O
a	O
task	O
of	O
"	O
artificial	O
review	O
prediction	O
"	O
,	O
where	O
a	O
negative	O
example	O
is	O
an	O
original	O
review	O
but	O
a	O
positive	O
example	O
is	O
a	O
synthesized	O
fake	O
review	O
by	O
combining	O
two	O
different	O
reviews	O
.	O
This	O
task	O
exploits	O
the	O
rich	O
relationships	O
between	O
two	O
sides	O
in	O
the	O
input	O
,	O
such	O
as	O
whether	O
two	O
sides	O
of	O
texts	O
have	O
the	O
same	O
rating	O
or	O
not	O
(	O
when	O
two	O
reviews	O
with	O
different	O
ratings	O
are	O
combined	O
as	O
a	O
positive	O
example	O
)	O
,	O
or	O
whether	O
two	O
sides	O
are	O
targeting	O
the	O
same	O
product	O
or	O
not	O
(	O
when	O
two	O
reviews	O
from	O
different	O
products	O
are	O
merged	O
as	O
a	O
positive	O
example	O
)	O
.	O
In	O
summary	O
,	O
these	O
two	O
objectives	O
encourage	O
to	O
learn	O
a	O
myriad	O
of	O
fine	O
-	O
grained	O
features	O
for	O
potential	O
end	O
tasks	O
.	O
We	O
let	O
the	O
loss	B-MetricName
function	O
of	O
MLM	B-DatasetName
be	O
L	O
MLM	B-DatasetName
and	O
the	O
loss	B-MetricName
function	O
of	O
next	O
text	O
piece	O
prediction	O
be	O
L	O
NSP	O
,	O
the	O
total	O
loss	B-MetricName
of	O
the	O
domain	O
knowledge	O
posttraining	O
is	O
L	O
DK	O
=	O
L	O
MLM	B-DatasetName
+	O
L	O
NSP	O
.	O
To	O
post	O
-	O
train	O
BERT	B-MethodName
on	O
task	O
-	O
aware	O
knowledge	O
,	O
we	O
use	O
SQuAD	B-DatasetName
(	O
1.1	O
)	O
,	O
which	O
is	O
a	O
popular	O
largescale	O
MRC	O
dataset	O
.	O
Although	O
BERT	B-MethodName
gains	O
great	O
success	O
on	O
SQuAD	B-DatasetName
,	O
this	O
success	O
is	O
based	O
on	O
the	O
huge	O
amount	O
of	O
training	O
examples	O
of	O
SQuAD	B-DatasetName
(	O
100	O
,	O
000	O
+	O
)	O
.	O
This	O
amount	O
is	O
large	O
enough	O
to	O
ameliorate	O
the	O
flaws	O
of	O
BERT	B-MethodName
that	O
has	O
almost	O
no	O
questions	O
on	O
the	O
left	O
side	O
and	O
no	O
textual	O
span	O
predictions	O
based	O
on	O
both	O
the	O
question	O
and	O
the	O
document	O
on	O
the	O
right	O
side	O
.	O
However	O
,	O
a	O
small	O
amount	O
of	O
finetuning	O
examples	O
is	O
not	O
sufficient	O
to	O
turn	O
BERT	B-MethodName
to	O
be	O
more	O
task	O
-	O
aware	O
,	O
as	O
shown	O
in	O
Sec	O
.	O
5	O
.	O
We	O
let	O
the	O
loss	B-MetricName
on	O
SQuAD	B-DatasetName
be	O
L	O
MRC	O
,	O
which	O
is	O
in	O
a	O
similar	O
setting	O
as	O
the	O
loss	B-MetricName
L	O
RRC	O
for	O
RRC	O
.	O
As	O
a	O
result	O
,	O
the	O
joint	O
loss	B-MetricName
of	O
post	O
-	O
training	O
is	O
defined	O
as	O
L	O
=	O
L	O
DK	O
+	O
L	O
MRC	O
.	O
One	O
major	O
issue	O
of	O
post	O
-	O
training	O
on	O
such	O
a	O
loss	B-MetricName
is	O
the	O
prohibitive	O
cost	O
of	O
GPU	O
memory	O
usage	O
.	O
Instead	O
of	O
updating	O
parameters	O
over	O
a	O
batch	O
,	O
we	O
divide	O
a	O
batch	O
into	O
multiple	O
sub	O
-	O
batches	O
and	O
accumulate	O
gradients	O
on	O
those	O
sub	O
-	O
batches	O
before	O
parameter	O
updates	O
.	O
This	O
allows	O
for	O
a	O
smaller	O
subbatch	O
to	O
be	O
consumed	O
in	O
each	O
iteration	O
.	O
u	O
:	O
number	O
of	O
sub	O
-	O
batches	O
.	O
1	O
∇	O
Θ	B-HyperparameterName
L	O
0	B-DatasetName
2	O
{	O
D	O
DK	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
D	O
DK	O
,	O
u	O
}	O
Split	O
(	O
D	O
DK	O
,	O
u	O
)	O
3	O
{	O
D	O
MRC	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
D	O
MRC	O
,	O
u	O
}	O
Split	O
(	O
D	O
MRC	O
,	O
u	O
)	O
4	O
for	O
i	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
u	O
}	O
do	O
5	O
L	O
partial	O
L	O
DK	O
(	O
D	O
DK	O
,	O
i	O
)	O
+	O
L	O
MRC	O
(	O
D	O
MRC	O
,	O
i	O
)	O
u	O
6	O
∇	O
Θ	B-HyperparameterName
L	O
∇	O
Θ	B-HyperparameterName
L	O
+	O
BackProp	O
(	O
L	O
partial	O
)	O
7	O
end	O
8	O
Θ	B-HyperparameterName
ParameterUpdates	O
(	O
∇	O
Θ	B-HyperparameterName
L	O
)	O
to	O
be	O
able	O
to	O
fit	O
into	O
GPU	O
memory	O
.	O
In	O
line	O
5	O
,	O
it	O
computes	O
the	O
partial	O
joint	O
loss	B-MetricName
L	O
partial	O
of	O
two	O
subbatches	O
D	O
DK	O
,	O
i	O
and	O
D	O
MRC	O
,	O
i	O
from	O
the	O
i	O
-	O
th	O
iteration	O
through	O
forward	O
pass	O
.	O
Note	O
that	O
the	O
summation	O
of	O
two	O
sub	O
-	O
batches	O
'	O
losses	O
is	O
divided	O
by	O
u	O
,	O
which	O
compensate	O
the	O
scale	O
change	O
introduced	O
by	O
gradient	O
accumulation	O
in	O
line	O
6	O
.	O
Line	O
6	O
accumulates	O
the	O
gradients	O
produced	O
by	O
backpropagation	O
from	O
the	O
partial	O
joint	O
loss	B-MetricName
.	O
To	O
this	O
end	O
,	O
accumulating	O
the	O
gradients	O
u	O
times	O
is	O
equivalent	O
to	O
computing	O
the	O
gradients	O
on	O
the	O
whole	O
batch	O
once	O
.	O
But	O
the	O
subbatches	O
and	O
their	O
intermediate	O
hidden	O
representations	O
during	O
the	O
i	O
-	O
th	O
forward	O
pass	O
can	O
be	O
discarded	O
to	O
save	O
memory	O
space	O
.	O
Only	O
the	O
gradients	O
∇	O
Θ	B-HyperparameterName
are	O
kept	O
throughout	O
all	O
iterations	O
and	O
used	O
to	O
update	O
parameters	O
(	O
based	O
on	O
the	O
chosen	O
optimizer	B-HyperparameterName
)	O
in	O
line	O
8	O
.	O
We	O
detail	O
the	O
hyper	O
-	O
parameter	O
settings	O
of	O
this	O
algorithm	O
in	O
Sec	O
.	O
5.3	O
.	O

We	O
adopt	O
BERT	B-MethodName
BASE	B-MethodName
(	O
uncased	O
)	O
as	O
the	O
basis	O
for	O
all	O
experiments	O
10	O
.	O
Since	O
post	O
-	O
training	O
may	O
take	O
a	O
large	O
footprint	O
on	O
GPU	O
memory	O
(	O
as	O
BERT	B-MethodName
pretraining	O
)	O
,	O
we	O
leverage	O
FP16	O
computation	O
11	O
to	O
reduce	O
the	O
size	O
of	O
both	O
the	O
model	O
and	O
hidden	O
representations	O
of	O
data	O
.	O
We	O
set	O
a	O
static	O
loss	B-MetricName
scale	O
of	O
2	O
in	O
FP16	O
,	O
which	O
can	O
avoid	O
any	O
over	O
/	O
under	O
-	O
flow	O
of	O
floating	O
point	O
computation	O
.	O
The	O
maximum	O
length	O
of	O
post	O
-	O
training	O
is	O
set	O
to	O
320	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	O
for	O
each	O
type	O
of	O
knowledge	O
.	O
The	O
number	O
of	O
subbatch	O
u	O
is	O
set	O
to	O
2	O
,	O
which	O
is	O
good	O
enough	O
to	O
store	O
each	O
sub	O
-	O
batch	O
iteration	O
into	O
a	O
GPU	O
memory	O
of	O
11G.	O
We	O
use	O
Adam	B-MethodName
optimizer	B-HyperparameterName
and	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
be	O
3e	O
-	O
5	O
.	O
We	O
train	O
70	O
,	O
000	O
steps	O
for	O
the	O
laptop	O
domain	O
and	O
140	O
,	O
000	O
steps	O
for	O
the	O
restaurant	O
domain	O
,	O
which	O
roughly	O
have	O
one	O
pass	O
over	O
the	O
preprocessed	O
data	O
on	O
the	O
respective	O
domain	O
.	O

As	O
BERT	B-MethodName
outperforms	O
existing	O
open	O
source	O
MRC	O
baselines	O
by	O
a	O
large	O
margin	O
,	O
we	O
do	O
not	O
intend	O
to	O
exhaust	O
existing	O
implementations	O
but	O
focus	O
on	O
variants	O
of	O
BERT	B-MethodName
introduced	O
in	O
this	O
paper	O
.	O
DrQA	O
is	O
a	O
baseline	O
from	O
the	O
document	O
reader	O
12	O
of	O
DrQA	O
(	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
.	O
We	O
adopt	O
this	O
baseline	O
because	O
of	O
its	O
simple	O
implementation	O
for	O
reproducibility	O
.	O
We	O
run	O
the	O
document	O
reader	O
with	O
random	O
initialization	O
and	O
train	O
it	O
directly	O
on	O
ReviewRC	O
.	O
We	O
use	O
all	O
default	O
hyper	O
-	O
parameter	O
settings	O
for	O
this	O
baseline	O
except	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
,	O
which	O
is	O
set	O
as	O
60	O
for	O
better	O
convergence	O
.	O
DrQA+MRC	O
is	O
derived	O
from	O
the	O
above	O
baseline	O
with	O
official	O
pre	O
-	O
trained	O
weights	O
on	O
SQuAD	B-DatasetName
.	O
We	O
fine	O
-	O
tune	O
document	O
reader	O
with	O
ReviewRC	O
.	O
We	O
expand	O
the	O
vocabulary	O
of	O
the	O
embedding	O
layer	O
from	O
the	O
pre	O
-	O
trained	O
model	O
on	O
ReviewRC	O
since	O
reviews	O
may	O
have	O
words	O
that	O
are	O
rare	O
in	O
Wikipedia	O
and	O
keep	O
other	O
hyper	O
-	O
parameters	O
as	O
their	O
defaults	O
.	O
For	O
AE	B-MethodName
and	O
ASC	O
,	O
we	O
summarize	O
the	O
scores	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
arts	O
on	O
SemEval	O
(	O
based	O
the	O
best	O
of	O
our	O
knowledge	O
)	O
for	O
brevity	O
.	O
DE	O
-	O
CNN	O
(	O
Xu	O
et	O
al	O
,	O
2018a	O
)	O
reaches	O
the	O
state	O
-	O
ofthe	O
-	O
arts	O
for	O
AE	B-MethodName
by	O
leveraging	O
domain	O
embeddings	O
.	O
MGAN	O
(	O
Li	O
et	O
al	O
,	O
2018	O
)	O
reaches	O
the	O
state	O
-	O
of	O
-	O
theart	O
ASC	O
on	O
SemEval	O
2014	O
task	O
4	O
.	O
Lastly	O
,	O
to	O
answer	O
RQ1	O
,	O
RQ2	O
,	O
and	O
RQ3	O
,	O
we	O
have	O
the	O
following	O
BERT	B-MethodName
variants	O
.	O
BERT	B-MethodName
leverages	O
the	O
vanilla	O
BERT	B-MethodName
pre	O
-	O
trained	O
weights	O
and	O
fine	O
-	O
tunes	O
on	O
all	O
3	O
end	O
tasks	O
.	O
We	O
use	O
this	O
baseline	O
to	O
answer	O
RQ2	O
and	O
show	O
that	O
BERT	B-MethodName
's	O
pre	O
-	O
trained	O
weights	O
alone	O
have	O
limited	O
performance	O
gains	O
on	O
review	O
-	O
based	O
tasks	O
.	O
BERT	B-MethodName
-	O
DK	O
post	O
-	O
trains	O
BERT	B-MethodName
's	O
weights	O
only	O
on	O
domain	O
knowledge	O
(	O
reviews	O
)	O
and	O
fine	O
-	O
tunes	O
on	O
the	O
3	O
end	O
tasks	O
.	O
We	O
use	O
BERT	B-MethodName
-	O
DK	O
and	O
the	O
following	O
BERT	B-MethodName
-	O
MRC	O
to	O
answer	O
RQ3	O
.	O
BERT	B-MethodName
-	O
MRC	O
post	O
-	O
trains	O
BERT	B-MethodName
's	O
weights	O
on	O
SQuAD	B-DatasetName
1.1	O
and	O
then	O
fine	O
-	O
tunes	O
on	O
the	O
3	O
end	O
tasks	O
.	O
BERT	B-MethodName
-	O
PT	O
(	O
proposed	O
method	O
)	O
post	O
-	O
trains	O
BERT	B-MethodName
's	O
weights	O
using	O
the	O
joint	O
post	O
-	O
training	O
algorithm	O
in	O
Section	O
4	O
and	O
then	O
fine	O
-	O
tunes	O
on	O
the	O
3	O
end	O
tasks	O
.	O

To	O
be	O
consistent	O
with	O
existing	O
research	O
on	O
MRC	O
,	O
we	O
use	O
the	O
same	O
evaluation	O
script	O
from	O
SQuAD	B-DatasetName
1.1	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
for	O
RRC	O
,	O
which	O
reports	O
Exact	B-MetricName
Match	I-MetricName
(	O
EM	B-MetricName
)	O
and	O
F1	B-MetricName
scores	O
.	O
EM	B-MetricName
requires	O
the	O
answers	O
to	O
have	O
exact	O
string	O
match	O
with	O
human	O
annotated	O
answer	O
spans	O
.	O
F1	B-MetricName
score	I-MetricName
is	O
the	O
averaged	O
F1	B-MetricName
scores	O
of	O
individual	O
answers	O
,	O
which	O
is	O
typically	O
higher	O
than	O
EM	B-MetricName
and	O
is	O
the	O
major	O
metric	O
.	O
Each	O
individual	O
F1	B-MetricName
score	I-MetricName
is	O
the	O
harmonic	O
mean	O
of	O
individual	O
precision	O
and	O
recall	O
computed	O
based	O
on	O
the	O
number	O
of	O
overlapped	O
words	O
between	O
the	O
predicted	O
answer	O
and	O
human	O
annotated	O
answers	O
.	O
For	O
AE	B-MethodName
,	O
we	O
use	O
the	O
standard	O
evaluation	O
scripts	O
come	O
with	O
the	O
SemEval	O
datasets	O
and	O
report	O
the	O
F1	B-MetricName
score	I-MetricName
.	O
For	O
ASC	O
,	O
we	O
compute	O
both	O
accuracy	B-MetricName
and	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
over	O
3	O
classes	O
of	O
polarities	O
,	O
where	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
is	O
the	O
major	O
metric	O
as	O
the	O
imbalanced	O
classes	O
introduce	O
biases	O
on	O
accuracy	B-MetricName
.	O
To	O
be	O
consistent	O
with	O
existing	O
research	O
(	O
Tang	O
et	O
al	O
,	O
2016	O
)	O
,	O
examples	O
belonging	O
to	O
the	O
conflict	O
polarity	O
are	O
dropped	O
due	O
to	O
a	O
very	O
small	O
number	O
of	O
examples	O
.	O
We	O
set	O
the	O
maximum	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
to	O
4	O
for	O
BERT	B-MethodName
variants	O
,	O
though	O
most	O
runs	O
converge	O
just	O
within	O
2	O
epochs	O
.	O
Results	O
are	O
reported	O
as	O
averages	O
of	O
9	O
runs	O
(	O
9	O
different	O
random	O
seeds	B-DatasetName
for	O
random	O
batch	O
generation	O
)	O
.	O
13	O

The	O
neutralization	O
module	O
is	O
designed	O
to	O
filter	O
sentiment	O
information	O
out	O
from	O
the	O
input	O
text	O
.	O
For	O
example	O
,	O
the	O
input	O
hate	O
when	O
my	O
bus	O
is	O
late	O
should	O
be	O
filtered	O
to	O
produce	O
a	O
neutral	O
statement	O
like	O
bus	O
is	O
late	O
.	O
Neutralization	O
is	O
performed	O
by	O
a	O
neural	O
sentiment	O
classification	O
module	O
.	O
Each	O
word	O
in	O
the	O
input	O
sentence	O
of	O
length	O
N	O
(	O
x	O
=	O
{	O
x	O
1	O
,	O
x	O
2	O
,	O
...	O
,	O
x	O
N	O
}	O
)	O
(	O
in	O
one	O
-	O
hot	O
representation	O
,	O
padded	O
wherever	O
necessary	O
)	O
is	O
transformed	O
into	O
K	O
-	O
dimensional	O
embedding	O
.	O
The	O
embeddings	O
are	O
then	O
passed	O
through	O
a	O
layer	O
of	O
recurrent	O
units	O
such	O
as	O
Long	O
Short	O
Term	O
Memory	O
(	O
LSTM	B-MethodName
)	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
.	O
The	O
output	O
of	O
the	O
LSTM	B-MethodName
layers	O
are	O
then	O
sent	O
to	O
a	O
self	O
-	O
attention	O
layer	O
before	O
passing	O
through	O
a	O
softmax	B-MethodName
based	O
classifier	O
.	O
The	O
classifier	O
is	O
trained	O
with	O
the	O
supervision	O
from	O
sentiment	O
positive	O
/	O
negative	O
labels	O
using	O
corpora	O
P	O
and	O
N	O
.	O
During	O
testing	O
,	O
for	O
a	O
given	O
input	O
of	O
length	O
N	O
,	O
the	O
self	O
attention	O
vector	O
α	B-HyperparameterName
=	O
α	B-HyperparameterName
1	O
,	O
α	B-HyperparameterName
2	O
,	O
...	O
,	O
α	B-HyperparameterName
N	O
is	O
first	O
extracted	O
(	O
details	O
skipped	O
for	O
brevity	O
,	O
refer	O
Xu	O
et	O
al	O
(	O
2018	O
)	O
)	O
.	O
We	O
then	O
inverse	O
and	O
discretize	O
the	O
self	O
attention	O
vector	O
as	O
follows	O
:	O
α	B-HyperparameterName
i	O
=	O
0	B-DatasetName
,	O
if	O
α	B-HyperparameterName
i	O
>	O
µ	O
+	O
2σ	O
1	O
,	O
otherwise	O
(	O
1	O
)	O
where	O
α	B-HyperparameterName
i	O
is	O
the	O
attention	O
weight	O
for	O
the	O
i	O
th	O
word	O
,	O
µ	O
and	O
σ	O
are	O
the	O
mean	O
and	O
standard	O
deviation	O
for	O
the	O
attention	O
vector	O
.	O
For	O
each	O
word	O
in	O
the	O
input	O
,	O
if	O
the	O
discretized	O
attention	O
weight	O
is	O
0	B-DatasetName
,	O
it	O
is	O
filtered	O
out	O
.	O
The	O
motivation	O
behind	O
such	O
a	O
design	O
is	O
that	O
if	O
the	O
classifier	O
is	O
trained	O
well	O
,	O
the	O
attention	O
weights	O
will	O
represent	O
the	O
contribution	O
of	O
each	O
word	O
to	O
the	O
overall	O
sentiment	O
decision	O
(	O
Xu	O
et	O
al	O
,	O
2018	O
)	O
.	O
Sentiment	O
bearing	O
words	O
will	O
receive	O
higher	O
attention	O
whereas	O
neutral	O
words	O
will	O
have	O
lower	O
attention	O
weights	O
.	O
It	O
is	O
worth	O
noting	O
that	O
neutralization	O
can	O
be	O
done	O
in	O
several	O
other	O
ways	O
such	O
as	O
filtering	O
based	O
on	O
a	O
sentiment	O
lexicon	O
.	O
However	O
,	O
such	O
operations	O
would	O
require	O
additional	O
resources	O
such	O
as	O
sentiment	O
dictionary	O
,	O
sense	O
disambiguation	O
tools	O
,	O
whereas	O
the	O
neural	O
classification	O
based	O
filtering	O
can	O
only	O
work	O
with	O
binary	O
sentiment	O
labeled	O
data	O
.	O
Also	O
,	O
for	O
computing	O
word	O
-	O
level	O
sentiment	O
contributions	O
,	O
recent	O
techniques	O
(	O
such	O
as	O
gradient	O
-	O
based	O
methods	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
)	O
can	O
be	O
used	O
.	O
For	O
simplicity	O
,	O
we	O
use	O
attention	O
based	O
filtering	O
.	O

The	O
sarcasm	O
synthesis	O
module	O
is	O
a	O
sequence	O
-	O
tosequence	O
network	O
that	O
expects	O
a	O
set	O
of	O
keywords	O
related	O
to	O
positive	O
sentiment	O
and	O
negative	O
situation	O
phrases	O
.	O
For	O
training	O
this	O
module	O
,	O
the	O
sarcasm	O
corpus	O
S	O
is	O
used	O
.	O
To	O
prepare	O
the	O
input	O
,	O
we	O
implement	O
a	O
keyword	B-TaskName
extraction	I-TaskName
technique	O
based	O
on	O
POS	O
tagging	O
.	O
Sentences	O
in	O
S	O
are	O
POS	O
-	O
tagged	O
and	O
then	O
stopwords	O
are	O
removed	O
,	O
and	O
then	O
based	O
on	O
the	O
POS	O
tags	O
noun	O
,	O
verb	O
,	O
adjective	O
and	O
adverbs	O
are	O
retained	O
.	O
This	O
way	O
,	O
the	O
input	O
keywords	O
to	O
the	O
system	O
would	O
somewhat	O
be	O
similar	O
to	O
the	O
keywords	O
expected	O
in	O
real	O
time	O
scenario	O
.	O
The	O
module	O
follows	O
an	O
encode	O
-	O
attend	O
-	O
decode	O
style	O
architecture	O
like	O
the	O
positive	O
sentiment	O
induction	O
module	O
,	O
but	O
with	O
a	O
different	O
learning	O
objective	O
.	O
Keywords	O
in	O
the	O
input	O
(	O
in	O
one	O
-	O
hot	O
representation	O
,	O
padded	O
wherever	O
necessary	O
)	O
are	O
transformed	O
into	O
a	O
sequence	O
of	O
embeddings	O
and	O
then	O
encoded	O
by	O
layers	O
of	O
LSTMs	O
,	O
which	O
produces	O
a	O
hidden	O
representations	O
for	O
each	O
input	O
word	O
.	O
The	O
decoder	O
,	O
consisting	O
of	O
a	O
single	O
layer	O
of	O
LSTMs	O
stacked	O
on	O
the	O
top	O
of	O
a	O
decoder	O
embedding	O
layer	O
,	O
attend	O
over	O
the	O
encoded	O
hidden	O
representations	O
and	O
generate	O
target	O
words	O
.	O
In	O
general	O
,	O
for	O
T	O
training	O
instances	O
of	O
keywords	O
and	O
sarcastic	O
texts	O
,	O
{	O
x	O
i	O
,	O
y	O
i	O
}	O
T	O
i=1	O
,	O
the	O
training	O
objective	O
is	O
to	O
maximize	O
the	O
likelihood	O
of	O
a	O
target	O
y	O
i	O
given	O
x	O
i	O
,	O
which	O
is	O
similar	O
to	O
minimizing	O
the	O
cross	O
entropy	O
between	O
the	O
target	O
distribution	O
and	O
the	O
predicted	O
output	O
distribution	O
.	O
For	O
training	O
the	O
neural	O
network	O
,	O
the	O
crossentropy	O
loss	B-MetricName
is	O
back	O
propagated	O
.	O
In	O
other	O
words	O
,	O
the	O
gradient	O
of	O
the	O
negative	O
cross	O
-	O
entropy	O
loss	B-MetricName
is	O
considered	O
to	O
update	O
the	O
model	O
parameters	O
.	O
The	O
gradient	O
is	O
given	O
by	O
:	O
∇	O
θ	B-HyperparameterName
L	O
(	O
θ	B-HyperparameterName
)	O
=	O
∇	O
θ	B-HyperparameterName
T	O
i=1	O
y	O
i	O
log	O
P	O
M	O
θ	B-HyperparameterName
(	O
ŷ	O
i	O
|	O
x	O
i	O
)	O
(	O
3	O
)	O
where	O
L	O
is	O
the	O
loss	B-MetricName
function	O
and	O
M	O
θ	B-HyperparameterName
is	O
the	O
translation	O
system	O
with	O
parameter	O
θ.ŷ	O
i	O
is	O
the	O
predicted	O
sentence	O
.	O
In	O
our	O
setting	O
,	O
where	O
the	O
input	O
is	O
not	O
a	O
proper	O
sentence	O
,	O
the	O
problem	O
with	O
the	O
above	O
objective	O
is	O
that	O
it	O
does	O
not	O
strongly	O
enforce	O
the	O
decoder	O
to	O
learn	O
and	O
produce	O
sarcastic	O
output	O
.	O
We	O
speculate	O
that	O
minimizing	O
the	O
token	O
-	O
level	O
cross	O
entropy	O
loss	B-MetricName
in	O
Eq	O
.	O
3	O
may	O
help	O
produce	O
an	O
output	O
that	O
is	O
grammatically	O
correct	O
but	O
not	O
sarcastic	O
enough	O
.	O
For	O
instance	O
,	O
the	O
decoder	O
may	O
incur	O
insignificant	O
cross	O
-	O
entropy	O
loss	B-MetricName
after	O
generating	O
a	O
sentence	O
like	O
absolutely	O
loved	O
it	O
,	O
as	O
this	O
sentence	O
has	O
considerable	O
overlap	O
with	O
the	O
reference	O
sarcastic	O
text	O
that	O
provides	O
supervision	O
.	O
One	O
possible	O
idea	O
to	O
tackle	O
such	O
problems	O
is	O
to	O
employ	O
a	O
sarcasm	O
scorer	O
that	O
can	O
determine	O
the	O
sarcasm	O
content	O
in	O
the	O
generated	O
output	O
,	O
and	O
use	O
the	O
scores	O
given	O
by	O
the	O
sarcasm	O
scorer	O
for	O
better	O
training	O
of	O
the	O
generator	O
.	O
However	O
,	O
the	O
sarcasm	O
scorer	O
may	O
be	O
external	O
to	O
the	O
sequence	O
-	O
tosequence	O
pipeline	O
,	O
and	O
the	O
scoring	O
function	O
may	O
not	O
be	O
differentiable	O
with	O
respect	O
to	O
the	O
model	O
M	O
θ	B-HyperparameterName
.	O
For	O
this	O
,	O
we	O
apply	O
reinforcement	O
learning	O
which	O
considers	O
sarcasm	O
content	O
score	O
as	O
a	O
form	O
of	O
reward	O
and	O
use	O
it	O
to	O
fine	O
-	O
tune	O
the	O
learning	O
process	O
.	O
For	O
learning	O
,	O
the	O
policy	O
gradient	O
theorem	O
(	O
Williams	O
,	O
1992	O
)	O
is	O
used	O
.	O
The	O
system	O
is	O
trained	O
under	O
a	O
modified	O
learning	O
objective	O
i.e.	O
,	O
to	O
maximize	O
the	O
expected	O
reward	O
score	O
for	O
a	O
set	O
of	O
produced	O
candidate	O
sentences	O
.	O
The	O
generator	O
,	O
operating	O
with	O
a	O
policy	O
of	O
P	O
M	O
θ	B-HyperparameterName
(	O
ŷ	O
i	O
|	O
x	O
i	O
)	O
,	O
producing	O
an	O
outputŷ	O
i	O
with	O
an	O
expected	O
reward	O
score	O
computed	O
using	O
a	O
scorer	O
,	O
will	O
thus	O
have	O
the	O
following	O
gradient	O
:	O
∇	O
θ	B-HyperparameterName
RL	O
(	O
θ	B-HyperparameterName
)	O
=	O
∇	O
θ	B-HyperparameterName
Eŷ	O
i	O
∼P	O
M	O
θ	B-HyperparameterName
(	O
ŷ	O
i	O
|	O
x	O
i	O
)	O
[	O
R	O
(	O
ŷ	O
i	O
)	O
]	O
=	O
E	O
[	O
R	O
(	O
ŷ	O
i	O
)	O
∇	O
θ	B-HyperparameterName
log	O
(	O
P	O
M	O
θ	B-HyperparameterName
(	O
ŷ	O
i	O
|	O
x	O
i	O
)	O
)	O
]	O
(	O
4	O
)	O
where	O
RL	O
is	O
the	O
modified	O
learning	O
objective	O
which	O
has	O
to	O
be	O
maximized	O
and	O
R	O
is	O
a	O
reward	O
function	O
that	O
is	O
computed	O
using	O
an	O
external	O
scorer	O
.	O
In	O
practice	O
,	O
the	O
expected	O
reward	O
is	O
computed	O
by	O
(	O
a	O
)	O
sampling	O
candidate	O
outputs	O
from	O
the	O
policy	O
P	O
M	O
θ	B-HyperparameterName
,	O
(	O
b	O
)	O
computing	O
the	O
reward	O
score	O
for	O
each	O
candidate	O
and	O
(	O
c	O
)	O
averaging	O
the	O
rewards	O
so	O
obtained	O
.	O
In	O
typical	O
RL	O
settings	O
,	O
the	O
learner	O
is	O
typically	O
initialized	O
to	O
a	O
random	O
policy	O
distribution	O
.	O
However	O
,	O
in	O
our	O
case	O
,	O
since	O
some	O
supervision	O
is	O
already	O
available	O
in	O
the	O
form	O
of	O
target	O
sarcastic	O
sentences	O
,	O
we	O
pretrain	O
the	O
model	O
with	O
the	O
loss	B-MetricName
minimization	O
objective	O
given	O
in	O
Eq	O
.	O
3	O
and	O
then	O
fine	O
-	O
tune	O
the	O
model	O
based	O
on	O
the	O
policy	O
gradient	O
scheme	O
following	O
Eq	O
.	O
4	O
.	O
Thus	O
,	O
the	O
learner	O
gets	O
initialized	O
with	O
a	O
better	O
policy	O
distribution	O
.	O
For	O
reward	O
calculation	O
,	O
we	O
consider	O
the	O
confidence	O
score	O
of	O
a	O
sarcasm	O
classifier	O
(	O
probability	O
of	O
being	O
sarcastic	O
)	O
trained	O
using	O
S	O
as	O
positive	O
examples	O
and	O
P	O
and	O
N	O
taken	O
as	O
negative	O
examples	O
.	O
For	O
our	O
setting	O
,	O
the	O
classifier	O
is	O
analogous	O
to	O
the	O
classifier	O
used	O
for	O
neutralization	O
.	O
The	O
classifier	O
is	O
based	O
on	O
embedding	O
,	O
LSTM	B-MethodName
and	O
softmax	B-MethodName
layers	O
.	O
Since	O
the	O
input	O
to	O
the	O
system	O
is	O
a	O
list	O
of	O
words	O
,	O
it	O
may	O
seem	O
that	O
the	O
sarcasm	O
synthesis	O
module	O
may	O
not	O
require	O
sequence	B-MethodName
to	I-MethodName
sequence	I-MethodName
learning	O
,	O
and	O
a	O
much	O
simpler	O
approach	O
like	O
bag	O
-	O
of	O
-	O
words	O
to	O
sequence	O
generation	O
could	O
have	O
been	O
used	O
.	O
However	O
,	O
note	O
that	O
the	O
input	O
to	O
the	O
generator	O
is	O
obtained	O
after	O
dropping	O
words	O
during	O
neutralization	O
and	O
later	O
appending	O
the	O
negative	O
situation	O
phrase	O
.	O
The	O
sequentiality	O
is	O
,	O
thus	O
,	O
not	O
completely	O
lost	O
.	O
This	O
makes	O
sequence	B-MethodName
to	I-MethodName
sequence	I-MethodName
model	O
an	O
intuitive	O
choice	O
.	O
We	O
now	O
explain	O
our	O
experimental	O
setup	O
.	O

For	O
the	O
neutralization	O
module	O
,	O
the	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
size	O
is	O
set	O
to	O
128	O
,	O
two	O
layers	O
of	O
LSTMs	O
of	O
hidden	O
dimension	O
of	O
200	O
are	O
used	O
.	O
The	O
classifier	O
trains	O
for	O
10	O
epochs	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	O
,	O
and	O
achieves	O
a	O
validation	O
accuracy	B-MetricName
of	O
96	O
%	O
and	O
training	O
accuracy	B-MetricName
of	O
98	O
%	O
.	O
For	O
positive	O
sentiment	O
induction	O
module	O
,	O
the	O
embedding	O
dimensions	O
for	O
both	O
encoder	O
and	O
decoder	O
are	O
set	O
to	O
500	O
.	O
Both	O
the	O
encoder	O
and	O
decoder	O
have	O
only	O
one	O
layer	O
of	O
LSTM	B-MethodName
,	O
with	O
a	O
hidden	O
dimension	O
of	O
500	O
.	O
The	O
module	O
is	O
built	O
on	O
top	O
of	O
the	O
OpenNMT	O
(	O
Klein	O
et	O
al	O
,	O
2017	O
)	O
framework	O
.	O
Training	O
happens	O
in	O
100	O
,	O
000	O
iterations	O
and	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
64	O
.	O
The	O
positive	O
sentiment	O
induction	O
module	O
,	O
at	O
the	O
end	O
of	O
the	O
training	O
,	O
produces	O
a	O
bigram	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
score	O
of	O
62.25	O
%	O
.	O
For	O
bootstrapping	O
negative	O
situations	O
and	O
other	O
purposes	O
,	O
the	O
POS	O
tagger	O
from	O
Spacy	O
5	O
is	O
used	O
.	O
The	O
Lucene	O
-	O
IR	O
framework	O
is	O
set	O
up	O
to	O
retrieve	O
negative	O
situations	O
.	O
The	O
model	O
configuration	O
and	O
training	O
parameters	O
for	O
the	O
sarcasm	O
synthesizer	O
is	O
the	O
same	O
as	O
the	O
positive	O
sentiment	O
induction	O
module	O
.	O
For	O
the	O
RL	O
scheme	O
,	O
for	O
each	O
instance	O
,	O
the	O
expected	O
reward	O
is	O
computed	O
over	O
100	O
candidate	O
samples	O
.	O
At	O
the	O
end	O
of	O
the	O
training	O
,	O
the	O
bigram	O
BLEU	B-MetricName
score	I-MetricName
on	O
the	O
validation	O
set	O
turns	O
out	O
to	O
be	O
59.3	O
%	O
.	O
For	O
reward	O
computation	O
,	O
we	O
use	O
a	O
classifier	O
similar	O
to	O
the	O
one	O
used	O
for	O
neutralization	O
.	O
The	O
embedding	O
size	O
for	O
this	O
classifier	O
is	O
300	O
and	O
it	O
uses	O
two	O
layers	O
of	O
unidirectional	O
LSTMs	O
with	O
a	O
hidden	O
dimension	O
of	O
300	O
.	O
It	O
trains	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	O
and	O
produces	O
a	O
validation	O
accuracy	B-MetricName
of	O
78.3	O
%	O
.	O
The	O
probability	O
estimates	O
given	O
by	O
the	O
classifier	O
for	O
any	O
input	O
text	O
are	O
taken	O
as	O
reward	O
scores	O
.	O
For	O
optimization	O
,	O
cross	O
entropy	O
loss	B-MetricName
criterion	O
is	O
used	O
.	O

Absence	O
of	O
automatic	O
evaluation	O
metrics	O
capable	O
of	O
capturing	O
subtleties	O
of	O
sarcasm	O
makes	O
it	O
difficult	O
to	O
evaluate	O
sarcasm	O
generators	O
.	O
For	O
evaluation	O
,	O
we	O
still	O
use	O
the	O
popular	O
translation	O
and	O
summarization	B-TaskName
evaluation	O
metrics	O
METEOR	B-DatasetName
(	O
Banerjee	O
and	O
Lavie	O
,	O
2005	O
)	O
and	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
.	O
Additionally	O
,	O
to	O
check	O
the	O
semantic	O
relatedness	O
between	O
the	O
input	O
and	O
output	O
,	O
we	O
use	O
Skip	O
-	O
thought	O
sentence	O
similarity	O
metric	O
6	O
.	O
Note	O
that	O
using	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
will	O
be	O
futile	O
here	O
as	O
direct	O
n	O
-	O
gram	O
overlaps	O
between	O
the	O
predicted	O
and	O
goldstandard	O
sentences	O
are	O
not	O
expected	O
to	O
be	O
significantly	O
higher	O
for	O
such	O
a	O
task	O
.	O
We	O
still	O
include	O
it	O
as	O
an	O
evaluation	O
metric	O
for	O
completion	O
.	O
We	O
employ	O
an	O
additional	O
metric	O
to	O
judge	O
the	O
percentage	O
of	O
length	O
increment	O
(	O
abbreviated	O
as	O
WL	O
)	O
to	O
see	O
the	O
if	O
the	O
length	O
of	O
the	O
output	O
is	O
generally	O
more	O
than	O
that	O
of	O
the	O
input	O
(	O
for	O
the	O
reference	O
text	O
it	O
is	O
67	O
%	O
)	O
.	O
The	O
notion	O
behind	O
this	O
metric	O
is	O
that	O
sarcasm	O
typically	O
requires	O
more	O
context	O
than	O
its	O
literal	O
version	O
,	O
requiring	O
to	O
have	O
more	O
words	O
present	O
at	O
the	O
target	O
side	O
.	O

For	O
this	O
task	O
,	O
we	O
used	O
4	O
deep	O
learning	O
models	O
.	O
The	O
architecture	O
of	O
the	O
first	O
3	O
models	O
were	O
relatively	O
similar	O
,	O
differing	O
in	O
the	O
embedding	O
layer	O
.	O
The	O
first	O
model	O
involves	O
character	O
embedding	O
with	O
dimension	O
equal	O
to	O
the	O
total	O
number	O
of	O
unique	O
characters	O
in	O
training	O
set	O
including	O
emojis	O
.	O
The	O
output	O
of	O
this	O
layer	O
is	O
fed	O
to	O
a	O
series	O
of	O
6	O
convolutional	O
neural	O
network	O
layers	O
(	O
CNNs	O
)	O
with	O
ReLU	B-MethodName
activation	O
.	O
Each	O
CNN	O
used	O
256	O
filters	O
,	O
with	O
a	O
filter	O
size	O
of	O
7	O
for	O
the	O
first	O
two	O
layers	O
and	O
3	O
for	O
the	O
rest	O
.	O
Max	B-MethodName
pooling	I-MethodName
with	O
size	O
3	O
was	O
used	O
for	O
the	O
first	O
two	O
and	O
last	O
CNNs	O
.	O
The	O
CNNs	O
'	O
output	O
was	O
fed	O
into	O
a	O
bidirectional	B-MethodName
LSTM	I-MethodName
(	O
Bi	O
-	O
LSTM	B-MethodName
)	O
with	O
2	O
*	O
200	O
units	O
,	O
whose	O
output	O
was	O
flattened	O
to	O
feed	O
into	O
two	O
dense	O
layers	O
.	O
We	O
used	O
two	O
fully	O
connected	O
layers	O
with	O
1024	O
units	O
each	O
,	O
ReLU	B-MethodName
activation	O
,	O
and	O
dropout	O
of	O
0.5	O
.	O
Finally	O
,	O
we	O
used	O
a	O
dense	O
layer	O
with	O
size	O
two	O
and	O
softmax	B-MethodName
activation	O
.	O
We	O
used	O
Adam	B-MethodName
as	O
the	O
optimizer	B-HyperparameterName
and	O
binary	O
cross	O
-	O
entropy	O
as	O
the	O
loss	B-MetricName
function	O
.	O
The	O
model	O
was	O
trained	O
with	O
10	O
epochs	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	O
.	O
The	O
second	O
architecture	O
was	O
identical	O
to	O
the	O
first	O
,	O
except	O
the	O
first	O
layer	O
was	O
a	O
word	O
embedding	O
using	O
GloVe	B-MethodName
2	O
pre	O
-	O
trained	O
on	O
Twitter	O
data	O
with	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
of	O
100	O
.	O
The	O
third	O
model	O
was	O
a	O
concatenation	O
of	O
word	O
and	O
character	O
embeddings	O
.	O
We	O
combined	O
the	O
Bi	O
-	O
LSTM	B-MethodName
output	O
of	O
the	O
first	O
and	O
second	O
models	O
and	O
then	O
applied	O
dense	O
layers	O
as	O
before	O
.	O
After	O
building	O
the	O
above	O
models	O
,	O
we	O
tried	O
to	O
improve	O
the	O
outcomes	O
by	O
adding	O
layers	O
and	O
features	O
.	O
We	O
used	O
a	O
multi	O
-	O
head	O
self	O
-	O
attention	O
with	O
an	O
attention	O
width	O
of	O
15	O
and	O
ReLU	B-MethodName
activation	O
.	O
We	O
also	O
explored	O
the	O
effect	O
of	O
sentiment	O
features	O
.	O
Since	O
the	O
data	O
classes	O
were	O
imbalanced	O
,	O
we	O
tried	O
to	O
make	O
class	O
sizes	O
equal	O
by	O
downsampling	O
and	O
upsampling	O
.	O
In	O
downsampling	O
,	O
samples	O
from	O
the	O
majority	O
class	O
(	O
tweets	O
without	O
ADR	O
mentions	O
)	O
were	O
randomly	O
sampled	O
without	O
replacement	O
.	O
In	O
upsampling	O
we	O
did	O
the	O
opposite	O
,	O
adding	O
samples	O
from	O
the	O
minority	O
class	O
with	O
replacement	O
.	O
None	O
of	O
these	O
strategies	O
substantially	O
altered	O
our	O
baseline	O
results	O
.	O
In	O
our	O
final	O
model	O
,	O
we	O
used	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
(	O
Embeddings	O
from	O
Language	O
Models	O
)	O
with	O
1024	O
dimensions	O
.	O
In	O
contrast	O
to	O
traditional	O
word	B-TaskName
embeddings	I-TaskName
such	O
as	O
GloVe	B-MethodName
and	O
word2vec	O
,	O
ELMo	B-MethodName
assigns	O
each	O
word	O
to	O
a	O
vector	O
as	O
a	O
function	O
of	O
the	O
entire	O
sentence	O
containing	O
that	O
word	O
.	O
Therefore	O
,	O
the	O
same	O
word	O
can	O
have	O
different	O
embeddings	O
depending	O
on	O
its	O
context	O
.	O
Since	O
ELMo	B-MethodName
already	O
captures	O
character	O
-	O
level	O
information	O
under	O
the	O
hood	O
,	O
we	O
decided	O
to	O
encircle	O
the	O
complexity	O
inside	O
the	O
embedding	O
layer	O
and	O
used	O
only	O
two	O
additional	O
dense	O
layers	O
with	O
256	O
and	O
2	O
units	O
,	O
using	O
ReLU	B-MethodName
and	O
softmax	B-MethodName
activations	O
,	O
respectively	O
.	O

Among	O
all	O
architectures	O
,	O
the	O
best	O
results	O
came	O
from	O
ELMo	B-MethodName
embedding	O
(	O
F1	B-MetricName
=	O
0.64	O
)	O
.	O
Therefore	O
,	O
we	O
only	O
submitted	O
ELMo	B-MethodName
results	O
with	O
5	O
,	O
10	O
,	O
and	O
15	O
epochs	O
.	O
The	O
model	O
performed	O
less	O
well	O
for	O
the	O
validation	O
set	O
(	O
F1	B-MetricName
=	O
0.41	O
)	O
,	O
below	O
the	O
average	B-MetricName
F1	I-MetricName
score	O
of	O
0.50	O
among	O
all	O
teams	O
,	O
which	O
might	O
result	O
from	O
overfitting	O
.	O
Using	O
more	O
sophisticated	O
architecture	O
after	O
the	O
embedding	O
layer	O
might	O
improve	O
performance	O
.	O
Since	O
task	O
2	O
's	O
performance	O
depends	O
strongly	O
on	O
task	O
1	O
,	O
we	O
also	O
scored	O
lower	O
on	O
this	O
task	O
compared	O
to	O
the	O
team	O
average	O
(	O
0.40	O
vs.	O
0.54	O
)	O
.	O
Since	O
ADR	O
phrases	O
and	O
tweets	O
do	O
not	O
always	O
lexically	O
match	O
,	O
approaches	O
such	O
as	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
might	O
perform	O
better	O
.	O
Other	O
approaches	O
to	O
improve	O
performance	O
:	O
Task	O
1	O
:	O
Try	O
other	O
embeddings	O
such	O
as	O
BERT	B-MethodName
I	O
would	O
also	O
like	O
to	O
show	O
my	O
gratitude	O
to	O
Peter	O
Leimbigler	O
for	O
comments	O
that	O
greatly	O
improved	O
the	O
manuscript	O
.	O
Finally	O
,	O
special	O
thanks	O
go	O
to	O
Alfred	O
Whitehead	O
for	O
supporting	O
me	O
to	O
participate	O
in	O
this	O
challenge	O
.	O

For	O
ACSA	O
,	O
we	O
manually	O
create	O
templates	O
containing	O
one	O
slot	O
for	O
the	O
given_category	O
and	O
another	O
slot	O
for	O
the	O
polarity_type	O
label	O
.	O
We	O
set	O
a	O
category	O
word	O
set	O
A	O
=	O
{	O
a	O
1	O
,	O
.	O
.	O
.	O
,	O
a	O
|	O
C	O
|	O
}	O
,	O
|	O
C	O
|	O
is	O
the	O
category	O
type	O
size	O
(	O
e.g.	O
,	O
a	O
i	O
=	O
"	O
price	O
"	O
)	O
and	O
polarity	O
type	O
word	O
set	O
P	O
=	O
{	O
p	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
|	O
L	O
|	O
}	O
,	O
|	O
L	O
|	O
is	O
the	O
polarity	O
type	O
size	O
(	O
e.g.	O
,	O
p	O
k	B-HyperparameterName
=	I-HyperparameterName
"	O
positive	O
"	O
)	O
,	O
and	O
use	O
words	O
to	O
define	O
templates	O
T	O
a	O
i	O
,	O
p	O
k	O
(	O
e.g.	O
"	O
The	O
sentiment	O
polarity	O
of	O
price	O
is	O
positive	O
"	O
)	O
.	O
The	O
template	O
T	O
is	O
"	O
The	O
sentiment	O
polarity	O
of	O
a	O
i	O
is	O
p	O
k	O
"	O
.	O
For	O
a	O
given	O
category	O
a	O
i	O
,	O
we	O
can	O
obtain	O
a	O
list	O
of	O
templates	O
T	O
a	O
i	O
=	O
[	O
T	O
a	O
i	O
,	O
p	O
1	O
,	O
.	O
.	O
.	O
,	O
T	O
a	O
i	O
,	O
p	O
|	O
L	O
|	O
]	O
.	O
For	O
ACD	O
,	O
we	O
use	O
a	O
i	O
to	O
create	O
a	O
sentiment	O
template	O
T	O
+	O
a	O
i	O
for	O
an	O
existing	O
aspect	O
category	O
,	O
and	O
a	O
none	O
-	O
category	O
template	O
T	O
−	O
a	O
i	O
.	O
T	O
+	O
is	O
"	O
The	O
a	O
i	O
category	O
is	O
discussed	O
"	O
and	O
T	O
−	O
is	O
"	O
The	O
a	O
i	O
category	O
is	O
not	O
discussed	O
"	O
.	O

For	O
ACSA	O
,	O
we	O
first	O
enumerate	O
all	O
possible	O
polarities	O
for	O
the	O
given	O
category	O
of	O
the	O
sentence	O
X	O
and	O
fill	O
them	O
in	O
the	O
prepared	O
templates	O
,	O
and	O
then	O
use	O
the	O
fine	O
-	O
tuned	O
pre	O
-	O
trained	O
generative	O
language	O
model	O
to	O
assign	O
a	O
score	O
for	O
each	O
template	O
T	O
a	O
i	O
,	O
p	O
k	B-HyperparameterName
=	I-HyperparameterName
{	O
t	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
m	O
}	O
,	O
formulated	O
as	O
:	O
f	O
(	O
Ta	O
i	O
,	O
p	O
k	O
)	O
=	O
m	O
c=1	O
log	O
P	O
(	O
tc	O
|	O
t1	O
:	O
c−1	O
,	O
X	O
)	O
(	O
1	O
)	O
We	O
calculate	O
a	O
score	O
f	O
(	O
T	O
a	O
i	O
,	O
p	O
k	O
)	O
for	O
each	O
possible	O
polarity	O
by	O
employing	O
the	O
pre	O
-	O
trained	O
generative	O
language	O
model	O
(	O
i.e.	O
,	O
BART	B-MethodName
)	O
to	O
score	O
the	O
templates	O
,	O
and	O
then	O
choose	O
the	O
polarity	O
of	O
category	O
a	O
i	O
with	O
the	O
largest	O
score	O
.	O
For	O
ACD	O
,	O
we	O
first	O
create	O
templates	O
T	O
+	O
a	O
i	O
and	O
T	O
−	O
a	O
i	O
for	O
all	O
possible	O
categories	O
of	O
the	O
sentence	O
X	O
,	O
and	O
then	O
use	O
the	O
fine	O
-	O
tuned	O
pre	O
-	O
trained	O
generative	O
language	O
model	O
to	O
assign	O
a	O
score	O
for	O
each	O
template	O
T	O
a	O
i	O
=	O
{	O
t	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
m	O
}	O
,	O
in	O
a	O
similar	O
way	O
as	O
Equation	O
1	O
.	O
Also	O
,	O
we	O
decide	O
whether	O
the	O
a	O
i	O
category	O
is	O
discussed	O
or	O
not	O
in	O
the	O
input	O
sentence	O
according	O
to	O
the	O
higher	O
score	O
between	O
T	O
+	O
a	O
i	O
and	O
T	O
−	O
a	O
i	O
.	O

For	O
ACSA	O
,	O
suppose	O
that	O
the	O
polarity	O
type	O
of	O
a	O
i	O
is	O
p	O
k	O
.	O
We	O
fill	O
the	O
given	O
category	O
a	O
i	O
and	O
the	O
polarity	O
type	O
p	O
k	O
into	O
template	O
T	O
to	O
create	O
a	O
gold	O
target	O
output	O
T	O
a	O
i	O
,	O
p	O
k	O
.	O
Similarly	O
for	O
ACD	O
,	O
if	O
the	O
category	O
of	O
a	O
i	O
is	O
discussed	O
,	O
the	O
gold	O
target	O
T	O
+	O
a	O
i	O
is	O
obtained	O
by	O
filling	O
a	O
i	O
into	O
T	O
+	O
,	O
and	O
otherwise	O
is	O
T	O
−	O
a	O
i	O
.	O
For	O
ACSA	O
,	O
we	O
use	O
all	O
gold	O
polarities	O
in	O
the	O
training	O
set	O
to	O
construct	O
(	O
X	O
,	O
T	O
)	O
pairs	O
.	O
For	O
ACD	O
,	O
we	O
use	O
all	O
gold	O
categories	O
in	O
the	O
training	O
set	O
to	O
construct	O
(	O
X	O
,	O
T	O
+	O
)	O
pairs	O
,	O
and	O
additionally	O
create	O
negative	O
samples	O
(	O
X	O
,	O
T	O
−	O
)	O
by	O
sampling	O
all	O
none	O
existing	O
categories	O
in	O
the	O
input	O
.	O
Finally	O
,	O
we	O
obtain	O
{	O
(	O
X	O
,	O
T	O
)	O
}	O
=	O
{	O
(	O
X	O
,	O
T	O
+	O
)	O
∪	O
(	O
X	O
,	O
T	O
−	O
)	O
}	O
Given	O
a	O
sequence	O
pair	O
(	O
X	O
,	O
T	O
)	O
,	O
we	O
feed	O
the	O
input	O
X	O
=	O
x	O
1	O
:	O
n	O
to	O
the	O
BART	B-MethodName
encoder	O
,	O
obtaining	O
hidden	O
representations	O
of	O
the	O
sentence	O
:	O
h	O
enc	O
=	O
ENCODER	O
(	O
x1	O
:	O
n	O
)	O
(	O
2	O
)	O
At	O
the	O
c	O
th	O
step	O
of	O
the	O
decoder	O
,	O
h	O
enc	O
and	O
previous	O
output	O
tokens	O
t	O
1	O
:	O
c−1	O
are	O
then	O
as	O
inputs	O
,	O
yielding	O
a	O
representation	O
using	O
attention	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
h	O
dec	O
c	O
=	O
DECODER	O
(	O
h	O
enc	O
,	O
t1	O
:	O
c−1	O
)	O
(	O
3	O
)	O
The	O
conditional	O
probability	O
of	O
the	O
word	O
t	O
c	O
is	O
defined	O
as	O
:	O
P	O
(	O
tc	O
|	O
t1	O
:	O
c−1	O
,	O
X	O
)	O
=	O
SOFTMAX	B-MethodName
(	O
h	O
dec	O
c	O
W	O
lm	O
+	O
b	O
lm	O
)	O
,	O
(	O
4	O
)	O
where	O
W	O
lm	O
R	O
d	O
h	O
×	O
|	O
V	O
|	O
and	O
b	O
lm	O
R	O
|	O
V	O
|	O
,	O
|	O
V	O
|	O
represents	O
the	O
vocab	O
size	O
of	O
pre	O
-	O
trained	O
BART	B-MethodName
.	O
The	O
cross	O
-	O
entropy	O
between	O
the	O
decoder	O
's	O
output	O
and	O
the	O
original	O
template	O
is	O
used	O
as	O
the	O
loss	B-MetricName
function	O
:	O
L	O
=	O
−	O
m	O
c=1	O
log	O
P	O
(	O
tc	O
|	O
t1	O
,	O
c−1	O
,	O
X	O
)	O
(	O
5	O
)	O
4	O
Experiments	O
We	O
choose	O
the	O
SemEval	O
-	O
2014	O
restaurant	O
review	O
(	O
Rest14	O
)	O
(	O
Pontiki	O
et	O
al	O
,	O
2014a	O
)	O
,	O
a	O
variant	O
of	O
Rest14	O
(	O
Rest14	O
-	O
hard	O
)	O
(	O
Xue	O
and	O
Li	O
,	O
2018	O
)	O
and	O
the	O
multiaspect	O
multi	O
-	O
sentiment	O
(	O
MAMS	B-DatasetName
)	O
(	O
Jiang	O
et	O
al	O
,	O
2019	O
)	O
datasets	O
for	O
sentence	O
-	O
level	O
sentiment	O
,	O
the	O
Tri	O
-	O
pAdvisor	O
(	O
Wang	O
et	O
al	O
,	O
2010	O
)	O
and	O
BeerAdvocate	B-DatasetName
(	O
McAuley	O
et	O
al	O
,	O
2012	O
;	O
Lei	O
et	O
al	O
,	O
2016	O
)	O
datasets	O
for	O
document	O
-	O
level	O
sentiment	O
.	O
Standard	O
splits	O
of	O
training	O
/	O
development	O
/	O
testing	O
sets	O
are	O
adopted	O
following	O
previous	O
work	O
Tay	O
et	O
al	O
(	O
2018	O
)	O
,	O
the	O
details	O
of	O
which	O
are	O
shown	O
in	O
Appendix	O
A.	O
We	O
use	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
-	O
base	O
1	O
and	O
BARTbase	O
2	O
models	O
for	O
task	O
fine	O
-	O
tuning	O
.	O
We	O
select	O
the	O
fine	O
-	O
tuning	O
learning	B-HyperparameterName
rate	I-HyperparameterName
from	O
{	O
4e	O
-	O
5	O
,	O
2e	O
-	O
5	O
,	O
and	O
1e	O
-	O
5	O
}	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
from	O
{	O
8	O
,	O
16	O
,	O
24	O
}	O
for	O
different	O
models	O
.	O
The	O
dropout	O
probability	O
is	O
0.1	O
.	O
The	O
best	O
model	O
configuration	O
is	O
selected	O
according	O
to	O
the	O
highest	O
performance	O
on	O
the	O
development	O
set	O
.	O
The	O
details	O
of	O
settings	O
are	O
shown	O
in	O
Appendix	O
A.	O

Different	O
templates	O
can	O
be	O
used	O
for	O
expressing	O
the	O
same	O
meaning	O
.	O
For	O
instance	O
,	O
"	O
The	O
sentiment	O
polarity	O
of	O
given_category	O
is	O
positive	O
"	O
can	O
also	O
be	O
expressed	O
by	O
"	O
The	O
sentiment	O
is	O
positive	O
for	O
given_category	O
"	O
.	O
For	O
ACSA	O
,	O
we	O
investigate	O
the	O
impact	O
of	O
manual	O
templates	O
using	O
the	O
MAMS	B-DatasetName
development	O
set	O
.	O
Table	O
1	O
shows	O
the	O
impact	O
of	O
different	O
choice	O
of	O
templates	O
.	O
For	O
instance	O
,	O
"	O
The	O
given_category	O
category	O
has	O
a	O
polarity_type	O
label	O
"	O
and	O
"	O
The	O
sentiment	O
polarity	O
of	O
given_category	O
is	O
polarity_type	O
"	O
give	O
82.31	O
%	O
and	O
83.78	O
%	O
accuracy	B-MetricName
,	O
respectively	O
,	O
indicating	O
that	O
the	O
template	O
has	O
influence	O
on	O
the	O
final	O
performance	O
.	O
This	O
is	O
consistent	O
with	O
finds	O
of	O
Gao	O
et	O
al	O
(	O
2020	O
)	O
for	O
the	O
fewshot	O
task	O
.	O
Based	O
on	O
the	O
development	O
results	O
,	O
we	O
use	O
the	O
top	O
performing	O
template	O
"	O
The	O
sentiment	O
polarity	O
of	O
given_category	O
is	O
polarity_type	O
"	O
in	O
our	O
ACSA	O
experiments	O
.	O
For	O
ACD	O
,	O
we	O
investigate	O
the	O
impact	O
of	O
templates	O
using	O
the	O
Rest14	O
development	O
set	O
.	O
Table	O
2	O
shows	O
the	O
performance	O
impact	O
of	O
different	O
templates	O
.	O
We	O
use	O
the	O
top	O
performing	O
template	O
"	O
The	O
category_type	O
category	O
is	O
discussed	O
"	O
as	O
template	O
T	O
+	O
and	O
"	O
The	O
category_type	O
category	O
is	O
not	O
discussed	O
"	O
as	O
template	O
T	O
−	O
in	O
our	O
ACD	O
experiments	O
.	O

The	O
results	O
of	O
sentence	O
-	O
level	O
ACSA	O
are	O
shown	O
in	O
Table	O
3	O
.	O
We	O
can	O
see	O
that	O
,	O
first	O
,	O
the	O
performance	O
of	O
BERT	B-MethodName
MLM	B-DatasetName
and	O
BART	B-MethodName
MLM	B-DatasetName
is	O
better	O
than	O
BERT	B-MethodName
classification	O
and	O
BART	B-MethodName
classification	O
,	O
respectively	O
.	O
In	O
particular	O
,	O
BERT	B-MethodName
MLM	B-DatasetName
gives	O
a	O
strong	O
baseline	O
,	O
outperforming	O
all	O
non	O
-	O
BERT	B-MethodName
and	O
BERT	B-MethodName
classification	O
baselines	O
.	O
This	O
shows	O
that	O
making	O
use	O
of	O
pre	O
-	O
training	O
at	O
the	O
task	O
level	O
can	O
achieve	O
better	O
results	O
than	O
that	O
at	O
the	O
representation	O
level	O
.	O
Also	O
,	O
the	O
BART	B-MethodName
MLM	B-DatasetName
and	O
classification	O
models	O
perform	O
better	O
than	O
the	O
corresponding	O
BERT	B-MethodName
models	O
.	O
Second	O
,	O
BART	B-MethodName
generation	O
outperforms	O
all	O
baselines	O
on	O
all	O
three	O
datasets	O
,	O
which	O
indicates	O
that	O
our	O
model	O
can	O
better	O
detect	O
multiple	O
sentiment	O
polarities	O
in	O
one	O
sentence	O
toward	O
different	O
aspect	O
categories	O
.	O
Third	O
,	O
BART	B-MethodName
generation	O
performs	O
significantly	O
better	O
than	O
BART	B-MethodName
MLM	B-DatasetName
,	O
giving	O
absolutely	O
3.89	O
%	O
stronger	O
accuracy	B-MetricName
on	O
MAMS	B-DatasetName
,	O
demonstrating	O
the	O
effectiveness	O
of	O
the	O
generation	O
method	O
.	O
This	O
shows	O
the	O
strength	O
of	O
BART	B-MethodName
pre	O
-	O
training	O
for	O
generating	O
semantically	O
related	O
content	O
,	O
which	O
was	O
also	O
reflected	O
by	O
the	O
strong	O
performance	O
of	O
BART	B-MethodName
on	O
abstractive	O
sum	O
-	O
We	O
use	O
the	O
results	O
reported	O
in	O
XRCE	O
(	O
Brun	O
et	O
al	O
,	O
2014	O
)	O
,	O
NRC	O
-	O
Canada	O
(	O
Kiritchenko	O
et	O
al	O
,	O
2014	O
)	O
,	O
BERT	B-MethodName
-	O
pair	O
-	O
NLI	O
-	O
B	O
and	O
CNE	O
-	O
net	O
(	O
Dai	O
et	O
al	O
,	O
2020	O
)	O
.	O
marization	O
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
contrast	O
,	O
the	O
MLM	B-DatasetName
method	O
concatenates	O
the	O
input	O
and	O
output	O
into	O
one	O
sequence	O
,	O
and	O
thus	O
fails	O
to	O
model	O
their	O
correlation	O
in	O
encoder	O
-	O
decoder	O
pre	O
-	O
trainng	O
.	O
The	O
performances	O
of	O
our	O
model	O
on	O
documentlevel	O
ACSA	O
are	O
shown	O
in	O
Table	O
4	O
.	O
Compared	O
with	O
LSTM	B-MethodName
,	O
HAN	O
and	O
MR	B-DatasetName
,	O
BERT	B-MethodName
classification	O
and	O
BART	B-MethodName
classification	O
outperform	O
all	O
baselines	O
,	O
which	O
shows	O
the	O
effectiveness	O
of	O
pre	O
-	O
training	O
.	O
BERT	B-MethodName
MLM	B-DatasetName
and	O
BART	B-MethodName
MLM	B-DatasetName
surpass	O
BERT	B-MethodName
classification	O
and	O
BART	B-MethodName
classification	O
,	O
respectively	O
.	O
Our	O
BART	B-MethodName
generation	O
model	O
achieves	O
improvements	O
of	O
1.15	O
%	O
and	O
0.70	O
%	O
over	O
BART	B-MethodName
MLM	B-DatasetName
on	O
TripAdvisor	O
and	O
BeerAdvocate	B-DatasetName
,	O
respectively	O
,	O
demonstrating	O
that	O
the	O
generation	O
method	O
can	O
more	O
effectively	O
make	O
use	O
of	O
BART	B-MethodName
for	O
ACSA	O
.	O

Results	O
on	O
the	O
Rest14	O
ACD	O
subtask	O
are	O
presented	O
in	O
Table	O
5	O
.	O
Following	O
Pontiki	O
et	O
al	O
(	O
2014b	O
)	O
,	O
we	O
use	O
Micro	B-MetricName
-	I-MetricName
F1	I-MetricName
for	O
evaluating	O
.	O
Again	O
BART	B-MethodName
generation	O
achieves	O
better	O
results	O
than	O
BART	B-MethodName
classification	O
and	O
BART	B-MethodName
MLM	B-DatasetName
.	O
Our	O
model	O
outperforms	O
all	O
baselines	O
on	O
precision	O
and	O
F	O
-	O
1	O
score	O
.	O
In	O
particular	O
,	O
a	O
more	O
than	O
95	O
%	O
precision	O
score	O
is	O
achieved	O
,	O
which	O
shows	O
that	O
our	O
model	O
can	O
effectively	O
exclude	O
the	O
aspect	O
categories	O
not	O
mentioned	O
in	O
the	O
input	O
.	O
We	O
also	O
investigate	O
the	O
performance	O
on	O
the	O
MAMS	B-DatasetName
dataset	O
,	O
which	O
consists	O
of	O
at	O
least	O
two	O
unique	O
aspect	O
categories	O
with	O
different	O
sentiment	O
polarities	O
in	O
each	O
input	O
sentence	O
.	O
Table	O
7	O
shows	O
that	O
BART	B-MethodName
generation	O
outperforms	O
all	O
baselines	O
,	O
indicating	O
better	O
ability	O
of	O
our	O
model	O
to	O
detect	O
multiple	O
aspect	O
categories	O
in	O
one	O
sentence	O
.	O

The	O
generation	O
method	O
allows	O
us	O
to	O
build	O
a	O
straightforward	O
joint	O
model	O
by	O
extending	O
the	O
first	O
template	O
in	O
Table	O
1	O
,	O
using	O
"	O
The	O
sentiment	O
polarity	O
of	O
<	O
given_category	O
>	O
is	O
none	O
"	O
as	O
a	O
template	O
for	O
nonexisting	O
aspect	O
categories	O
.	O
The	O
results	O
on	O
Rest	O
-	O
14	O
and	O
MAMS	B-DatasetName
are	O
presented	O
in	O
Table	O
6	O
.	O
We	O
find	O
that	O
joint	O
BART	B-MethodName
generation	O
achieves	O
better	O
results	O
on	O
this	O
task	O
with	O
improvements	O
over	O
pipeline	O
BART	B-MethodName
generation	O
.	O
Joint	O
BART	B-MethodName
generation	O
outperforms	O
all	O
baselines	O
on	O
precision	O
,	O
recall	O
and	O
F	O
-	O
1	O
score	O
,	O
which	O
shows	O
the	O
advantage	O
of	O
joint	O
learning	O
.	O
(	O
10	O
,	O
20	O
,	O
50	O
,	O
100	O
,	O
200	O
,	O
500	O
instances	O
per	O
category	O
type	O
for	O
Rest14	O
and	O
MAMS	B-DatasetName
)	O
.	O
The	O
results	O
are	O
shown	O
in	O
Figure	O
4	O
,	O
where	O
the	O
methods	O
of	O
BERT	B-MethodName
classification	O
,	O
BART	B-MethodName
classification	O
and	O
BART	B-MethodName
MLM	B-DatasetName
are	O
also	O
compared	O
.	O
It	O
can	O
be	O
seen	O
that	O
on	O
all	O
the	O
datasets	O
,	O
our	O
model	O
outperforms	O
BERT	B-MethodName
classification	O
,	O
BART	B-MethodName
classification	O
and	O
BART	B-MethodName
MLM	B-DatasetName
,	O
especially	O
when	O
the	O
number	O
of	O
training	O
instances	O
is	O
small	O
.	O
For	O
example	O
,	O
when	O
there	O
are	O
only	O
10	O
training	O
instances	O
,	O
our	O
model	O
gives	O
accuracy	B-MetricName
scores	O
of	O
82.01	O
%	O
on	O
Rest14	O
,	O
as	O
compared	O
to	O
38.57	O
%	O
by	O
BERT	B-MethodName
classification	O
and	O
50.16	O
%	O
by	O
BART	B-MethodName
classification	O
.	O
When	O
the	O
number	O
of	O
instances	O
grows	O
as	O
large	O
as	O
500	O
,	O
our	O
model	O
gives	O
2.24	O
%	O
and	O
2.65	O
%	O
better	O
accuracies	O
than	O
BART	B-MethodName
MLM	B-DatasetName
on	O
Rest14	O
and	O
MAMS	B-DatasetName
,	O
respectively	O
.	O
One	O
possible	O
reason	O
is	O
that	O
our	O
method	O
makes	O
more	O
use	O
of	O
direct	O
sentiment	O
knowledge	O
in	O
the	O
pre	O
-	O
trained	O
language	O
model	O
by	O
directly	O
adopting	O
the	O
original	O
structure	O
of	O
BART	B-MethodName
mentioned	O
earlier	O
.	O
In	O
contrast	O
,	O
classification	O
methods	O
can	O
not	O
achieve	O
this	O
due	O
to	O
transferring	O
the	O
sentiment	O
bias	O
indirectly	O
.	O
The	O
results	O
of	O
our	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
experiments	O
are	O
in	O
Table	O
8	O
.	O
In	O
all	O
cases	O
,	O
our	O
method	O
outperforms	O
all	O
the	O
baselines	O
.	O
In	O
particular	O
,	O
the	O
model	O
trained	O
on	O
MAMS	B-DatasetName
has	O
a	O
better	O
performance	O
on	O
Rest14	O
than	O
the	O
reverse	O
zero	O
-	O
shot	O
setting	O
,	O
which	O
proves	O
that	O
the	O
MAMS	B-DatasetName
dataset	O
has	O
a	O
higher	O
challenge	O
.	O

Aspect	O
categories	O
can	O
be	O
implicit	O
and	O
do	O
not	O
necessarily	O
occur	O
as	O
terms	O
in	O
the	O
given	O
sentence	O
.	O
To	O
explore	O
the	O
correlation	O
between	O
ACSA	O
accuracy	B-MetricName
and	O
the	O
occurrence	O
frequency	O
of	O
a	O
given	O
category	O
,	O
we	O
split	O
the	O
eight	O
categories	O
in	O
the	O
MAMS	B-DatasetName
test	O
set	O
into	O
four	O
subsets	O
based	O
on	O
the	O
occurrence	O
frequency	O
.	O
The	O
category	O
(	O
i.e.	O
,	O
miscellaneous	O
)	O
that	O
never	O
occurs	O
in	O
the	O
given	O
sentence	O
is	O
put	O
into	O
the	O
zero	O
frequency	O
subset	O
,	O
the	O
15	O
%	O
least	O
frequent	O
(	O
i.e.	O
,	O
ambience	O
,	O
staff	O
)	O
are	O
put	O
into	O
low	O
frequency	O
subset	O
,	O
the	O
30	O
%	O
most	O
frequent	O
(	O
i.e.	O
,	O
menu	O
,	O
service	O
)	O
are	O
put	O
into	O
high	O
frequency	O
subset	O
,	O
and	O
the	O
remaining	O
(	O
i.e.	O
,	O
price	O
,	O
food	O
,	O
place	O
)	O
are	O
put	O
into	O
mid	O
frequency	O
subset	O
.	O
Figure	O
5	O
shows	O
the	O
accuracy	B-MetricName
of	O
BART	B-MethodName
classification	O
and	O
our	O
model	O
against	O
the	O
frequency	O
.	O
As	O
the	O
category	O
occurrence	O
frequency	O
decreases	O
,	O
the	O
relative	O
gap	O
of	O
accuracy	B-MetricName
between	O
the	O
two	O
models	O
increases	O
.	O
In	O
the	O
zero	O
frequency	O
,	O
our	O
method	O
gives	O
absolutely	O
8.03	O
%	O
stronger	O
accuracy	B-MetricName
than	O
BART	B-MethodName
classification	O
.	O
This	O
demonstrates	O
that	O
our	O
method	O
is	O
more	O
robust	O
in	O
summarizing	O
the	O
sentiment	O
polarity	O
of	O
abstract	O
or	O
rare	O
categories	O
.	O
Even	O
if	O
there	O
are	O
no	O
explicit	O
category	O
terms	O
in	O
the	O
sentence	O
,	O
the	O
generation	O
method	O
can	O
give	O
the	O
implicit	O
category	O
opinion	O
of	O
the	O
whole	O
sentence	O
according	O
to	O
the	O
context	O
.	O

Deep	O
learning	O
(	O
DL	O
)	O
is	O
being	O
used	O
extensively	O
for	O
text	B-TaskName
classification	I-TaskName
.	O
However	O
,	O
researchers	O
have	O
demonstrated	O
the	O
vulnerability	O
of	O
such	O
classifiers	O
to	O
adversarial	O
attacks	O
.	O
Attackers	O
modify	O
the	O
text	O
in	O
a	O
way	O
which	O
misleads	O
the	O
classifier	O
while	O
keeping	O
the	O
original	O
meaning	O
close	O
to	O
intact	O
.	O
State	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
SOTA	O
)	O
attack	O
algorithms	O
follow	O
the	O
general	O
principle	O
of	O
making	O
minimal	O
changes	O
to	O
the	O
text	O
so	O
as	O
to	O
not	O
jeopardize	O
semantics	O
.	O
Taking	O
advantage	O
of	O
this	O
we	O
propose	O
a	O
novel	O
and	O
intuitive	O
defense	O
strategy	O
called	O
Sample	O
Shielding	O
.	O
It	O
is	O
attacker	O
and	O
classifier	O
agnostic	O
,	O
does	O
not	O
require	O
any	O
reconfiguration	O
of	O
the	O
classifier	O
or	O
external	O
resources	O
and	O
is	O
simple	O
to	O
implement	O
.	O
Essentially	O
,	O
we	O
sample	O
subsets	O
of	O
the	O
input	O
text	O
,	O
classify	O
them	O
and	O
summarize	O
these	O
into	O
a	O
final	O
decision	O
.	O
We	O
shield	O
three	O
popular	O
DL	O
text	O
classifiers	O
with	O
Sample	O
Shielding	O
,	O
test	O
their	O
resilience	O
against	O
four	O
SOTA	O
attackers	O
across	O
three	O
datasets	O
in	O
a	O
realistic	O
threat	O
setting	O
.	O
Even	O
when	O
given	O
the	O
advantage	O
of	O
knowing	O
about	O
our	O
shielding	O
strategy	O
the	O
adversary	O
's	O
attack	O
success	O
rate	O
is	O
<	O
=	O
10	O
%	O
with	O
only	O
one	O
exception	O
and	O
often	O
<	O
5	O
%	O
.	O
Additionally	O
,	O
Sample	O
Shielding	O
maintains	O
near	O
original	O
accuracy	B-MetricName
when	O
applied	O
to	O
original	O
texts	O
.	O
Crucially	O
,	O
we	O
show	O
that	O
the	O
'	O
make	O
minimal	O
changes	O
'	O
approach	O
of	O
SOTA	O
attackers	O
leads	O
to	O
critical	O
vulnerabilities	O
that	O
can	O
be	O
defended	O
against	O
with	O
an	O
intuitive	O
sampling	O
strategy	O
.	O
1	O

Text	O
classifiers	O
have	O
become	O
ubiquitous	O
.	O
Unfortunately	O
,	O
they	O
are	O
subject	O
to	O
attacks	O
from	O
adversaries	O
,	O
typically	O
executed	O
using	O
machine	O
learning	O
methods	O
.	O
Attackers	O
work	O
by	O
making	O
small	O
modifications	O
to	O
the	O
text	O
that	O
mislead	O
the	O
classifier	O
.	O
Adversarial	O
attackers	O
are	O
now	O
a	O
growing	O
part	O
of	O
the	O
ecosystem	O
.	O
Like	O
classifiers	O
,	O
attack	O
algorithms	O
have	O
achieved	O
strong	O
success	O
due	O
to	O
advances	O
in	O
machine	O
learning	O
/	O
deep	O
learning	O
.	O
Current	O
text	O
attackers	O
,	O
like	O
1	O
Our	O
code	O
and	O
data	O
are	O
available	O
at	O
:	O
https://github.com/JonRusert/SampleShielding	O
TextFooler	O
and	O
Bert	O
-	O
Attack	O
(	O
Li	O
et	O
al	O
,	O
2020	O
)	O
,	O
are	O
able	O
to	O
reduce	O
near	O
perfect	O
classification	O
accuracy	B-MetricName
down	O
to	O
5	O
%	O
.	O
Additionally	O
,	O
these	O
attackers	O
achieve	O
this	O
while	O
perturbing	O
(	O
changing	O
)	O
only	O
a	O
small	O
amount	O
of	O
the	O
original	O
text	O
.	O
This	O
helps	O
preserve	O
the	O
original	O
meaning	O
so	O
that	O
humans	O
are	O
able	O
to	O
understand	O
the	O
original	O
message	O
even	O
though	O
classifiers	O
are	O
duped	O
.	O
As	O
a	O
counter	O
,	O
classifier	O
shielding	O
techniques	O
are	O
being	O
explored	O
.	O
One	O
such	O
approach	O
is	O
adversarial	O
training	O
where	O
the	O
classifier	O
,	O
assumed	O
to	O
have	O
access	O
to	O
the	O
attacker	O
,	O
uses	O
it	O
to	O
generate	O
perturbed	O
texts	O
-	O
these	O
are	O
added	O
to	O
the	O
classifier	O
's	O
training	O
data	O
.	O
While	O
this	O
leads	O
to	O
model	O
resilience	O
against	O
that	O
attacker	O
it	O
leaves	O
the	O
classifier	O
open	O
to	O
attacks	O
by	O
new	O
attackers	O
.	O
Other	O
defenses	O
involve	O
modifying	O
classifier	O
structure	O
to	O
reduce	O
the	O
information	O
an	O
attacker	O
can	O
glean	O
from	O
it	O
(	O
Goel	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
this	O
type	O
of	O
reconfiguration	O
will	O
not	O
be	O
possible	O
if	O
a	O
third	O
party	O
classifier	O
(	O
e.g.	O
Google	B-DatasetName
Perspective	O
)	O
is	O
leveraged	O
.	O
Even	O
other	O
approaches	O
involve	O
modifying	O
the	O
input	O
text	O
during	O
classification	O
time	O
,	O
but	O
are	O
currently	O
limited	O
to	O
classifiers	O
built	O
from	O
specific	O
masked	O
language	O
models	O
(	O
Zeng	O
et	O
al	O
,	O
2021	O
)	O
or	O
rely	O
on	O
external	O
synonym	O
datasets	O
(	O
Wang	O
et	O
al	O
,	O
2021a	O
)	O
.	O
We	O
propose	O
a	O
shielding	O
technique	O
which	O
is	O
attacker	O
-	O
agnostic	O
,	O
does	O
not	O
require	O
additional	O
training	O
/	O
reconfiguration	O
to	O
the	O
classifier	O
,	O
can	O
shield	O
any	O
classifier	O
,	O
does	O
not	O
require	O
an	O
external	O
data	O
source	O
,	O
and	O
can	O
be	O
used	O
in	O
a	O
more	O
realistic	O
threat	O
setting	O
.	O
We	O
refer	O
to	O
this	O
as	O
Sample	O
Shielding	O
.	O
Sample	O
Shielding	O
takes	O
advantage	O
of	O
current	O
constraints	O
in	O
SOTA	O
attacks	O
.	O
Mainly	O
,	O
to	O
preserve	O
original	O
meaning	O
,	O
these	O
make	O
the	O
minimal	O
changes	O
needed	O
to	O
deceive	O
the	O
classifier	O
.	O
For	O
example	O
,	O
BERT	B-MethodName
-	O
Attack	O
(	O
Li	O
et	O
al	O
,	O
2020	O
)	O
only	O
perturbs	O
up	O
to	O
16	O
%	O
of	O
text	O
,	O
and	O
often	O
far	O
less	O
(	O
e.g.	O
1.1	O
%	O
)	O
for	O
some	O
datasets	O
.	O
Thus	O
,	O
if	O
we	O
would	O
look	O
at	O
the	O
84	O
%	O
to	O
99	O
%	O
of	O
text	O
that	O
is	O
untouched	O
our	O
model	O
would	O
be	O
more	O
likely	O
to	O
classify	O
correctly	O
.	O
Hence	O
,	O
in	O
Sample	O
Shielding	O
we	O
take	O
many	O
samples	O
of	O
the	O
input	O
2	O
.	O
We	O
assess	O
Sample	O
Shielding	O
under	O
a	O
realistic	O
threat	O
model	O
where	O
the	O
attacker	O
can	O
not	O
query	O
a	O
website	O
's	O
classifier	O
hundreds	O
of	O
times	O
since	O
that	O
pattern	O
is	O
easily	O
detectable	O
by	O
the	O
website	O
.	O
We	O
run	O
experiments	O
under	O
two	O
conditions	O
,	O
when	O
the	O
attacker	O
has	O
knowledge	O
of	O
Sample	O
Shielding	O
and	O
when	O
it	O
does	O
not	O
.	O
In	O
both	O
cases	O
the	O
attacker	O
uses	O
a	O
local	O
copy	O
of	O
the	O
websites	O
'	O
classifier	O
.	O
This	O
is	O
an	O
optimistic	O
assumption	O
favouring	O
the	O
attacker	O
and	O
thus	O
provides	O
a	O
lower	O
bound	O
to	O
our	O
results	O
.	O
3	O
.	O
We	O
test	O
against	O
4	O
SOTA	O
text	O
attack	O
algorithms	O
,	O
3	O
text	O
datasets	O
and	O
3	O
classifiers	O
.	O
When	O
the	O
attacker	O
does	O
not	O
have	O
knowledge	O
of	O
Sample	O
Shielding	O
,	O
our	O
defense	O
reduces	O
attack	O
success	O
rate	O
from	O
near	O
total	O
decimation	O
90	O
-	O
100	O
%	O
down	O
to	O
13	O
-	O
36	O
%	O
,	O
while	O
still	O
maintaining	O
accuracy	B-MetricName
on	O
original	O
texts	O
.	O
When	O
the	O
attacker	O
has	O
knowledge	O
of	O
Sample	O
Shielding	O
,	O
our	O
defense	O
performs	O
even	O
better	O
,	O
reducing	O
attacks	O
down	O
to	O
1	O
-	O
10	O
%	O
success	O
rate	O
.	O
This	O
is	O
partially	O
due	O
to	O
Sample	O
Shielding	O
's	O
random	O
nature	O
providing	O
unreliable	O
feedback	O
to	O
attackers	O
.	O
Our	O
success	O
with	O
Sample	O
Shielding	O
is	O
good	O
news	O
for	O
classifiers	O
-	O
and	O
it	O
raises	O
the	O
bar	O
significantly	O
for	O
the	O
next	O
generation	O
attackers	O
.	O
We	O
share	O
code	O
and	O
our	O
perturbed	O
text	O
collections	O
for	O
future	O
research	O
.	O

We	O
run	O
experiments	O
on	O
the	O
combination	O
of	O
the	O
three	O
victim	O
classification	O
models	O
,	O
three	O
datasets	O
,	O
and	O
four	O
attack	O
algorithms	O
.	O
These	O
combinations	O
are	O
run	O
on	O
both	O
threat	O
model	O
conditions	O
(	O
attacker	O
is	O
aware/	O
not	O
aware	O
of	O
SampleShielding	O
)	O
.	O
This	O
leads	O
to	O
72	O
shielding	O
experiments	O
.	O
For	O
all	O
attacks	O
,	O
we	O
leverage	O
TextAttack	O
framework	O
7	O
which	O
provides	O
classification	O
algorithms	O
and	O
adversarial	B-TaskName
text	I-TaskName
generation	O
algorithms	O
implemented	O
as	O
specified	O
in	O
respective	O
papers	O
(	O
Morris	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
all	O
experiments	O
where	O
the	O
attacker	O
does	O
not	O
use	O
Sample	O
Shielding	O
4	O
We	O
share	O
the	O
original	O
and	O
perturbed	O
texts	O
for	O
replicability	O
.	O
We	O
note	O
that	O
replicability	O
of	O
previous	O
defenses	O
are	O
limited	O
because	O
the	O
identity	O
of	O
their	O
randomly	O
sampled	O
test	O
instances	O
are	O
not	O
provided	O
.	O
5	O
We	O
calibrated	O
classifier	O
accuracies	O
against	O
previous	O
research	O
(	O
Li	O
et	O
al	O
,	O
2020	O
;	O
6	O
huggingface.co/textattack	O
7	O
textattack.readthedocs.io/en/latest/index.html	O
we	O
set	O
k	B-HyperparameterName
=	I-HyperparameterName
100	O
and	O
p	O
=	O
0.3	O
.	O
While	O
better	O
performance	O
was	O
achieved	O
with	O
other	O
values	O
in	O
preliminary	O
experiments	O
,	O
we	O
chose	O
to	O
go	O
with	O
a	O
single	O
combination	O
of	O
p	O
and	O
k	O
for	O
simplicity	O
.	O
In	O
experiments	O
where	O
the	O
attacker	O
uses	O
Sample	O
Shielding	O
pre	O
-	O
processing	O
we	O
reduce	O
k	O
to	O
30	O
for	O
efficiency	O
.	O
Except	O
where	O
otherwise	O
noted	O
,	O
majority	O
voting	O
is	O
used	O
to	O
generate	O
results	O
.	O
Additionally	O
,	O
shifting	O
sampling	O
(	O
Section	O
2.2.1	O
)	O
shielding	O
typically	O
achieved	O
10	O
-	O
20	O
points	O
lower	O
accuracy	B-MetricName
compared	O
to	O
random	O
,	O
thus	O
we	O
do	O
not	O
include	O
it	O
in	O
the	O
results	O
.	O

We	O
examine	O
accuracy	B-MetricName
and	O
Attack	O
Success	O
Rate	O
:	O
accuracy	B-MetricName
=	O
#	O
examples_classif	O
ied_correctly	O
#	O
total_examples	O
(	O
1	O
)	O
ASR	O
=	O
Original	O
Acc	B-MetricName
.	O
−	O
Attacked	O
Acc	B-MetricName
.	O
Original	O
Acc	B-MetricName
.	O
(	O
2	O
)	O
4	O
Results	O
We	O
first	O
present	O
results	O
for	O
the	O
condition	O
where	O
the	O
attacker	O
is	O
not	O
aware	O
of	O
Sample	O
Shielding	O
based	O
pre	O
-	O
processing	O
and	O
then	O
the	O
results	O
for	O
when	O
the	O
attacker	O
also	O
employs	O
Sample	O
Shielding	O
.	O

Results	O
are	O
in	O
Table	O
1	O
.	O
BERT	B-MethodName
is	O
the	O
strongest	O
classifier	O
achieving	O
91	O
-	O
100	O
%	O
accuracy	B-MetricName
on	O
the	O
original	O
datasets	O
.	O
Attacks	O
are	O
highly	O
successful	O
against	O
unshielded	O
texts	O
.	O
TextFooler	O
and	O
Bert	O
-	O
Attack	O
are	O
the	O
most	O
successful	O
,	O
dropping	O
accuracies	O
to	O
0	B-DatasetName
-	O
5	O
%	O
generally	O
.	O
Attacks	O
were	O
able	O
to	O
achieve	O
strong	O
drops	O
with	O
minimal	O
amount	O
of	O
text	O
perturbed	O
(	O
about	O
10	O
%	O
)	O
.	O
Figure	O
3	O
shows	O
that	O
the	O
average	O
percent	O
of	O
words	O
perturbed	O
across	O
datasets	O
for	O
each	O
attack	O
are	O
about	O
equal	O
in	O
the	O
mid	O
regions	O
of	O
the	O
plots	O
.	O
For	O
AG	B-DatasetName
News	I-DatasetName
,	O
attacks	O
are	O
less	O
successful	O
against	O
BERT	B-MethodName
;	O
accuracy	B-MetricName
drops	O
to	O
19	O
%	O
in	O
the	O
strongest	O
attack	O
(	O
TextFooler	O
)	O
,	O
and	O
only	O
to	O
49	O
%	O
in	O
the	O
weakest	O
(	O
TextBugger	O

Increasing	O
p	O
raises	O
the	O
risk	O
of	O
samples	O
containing	O
increased	O
amounts	O
of	O
perturbed	O
text	O
.	O
Decreasing	O
k	O
raises	O
the	O
risk	O
of	O
not	O
covering	O
enough	O
of	O
the	O
unperturbed	O
portions	O
of	O
the	O
original	O
text	O
.	O
While	O
our	O
settings	O
of	O
p	O
=	O
0.3	O
and	O
k	B-HyperparameterName
=	I-HyperparameterName
100	O
for	O
our	O
main	O
results	O
are	O
reasonable	O
values	O
(	O
Table	O
1	O
,	O
Table	O
2	O
)	O
they	O
are	O
not	O
necessarily	O
optimal	O
.	O
Optimal	O
p.	O
Figure	O
4	O
shows	O
the	O
results	O
for	O
all	O
com	O
-	O
binations	O
of	O
attacks	O
against	O
LSTM	B-MethodName
on	O
IMDB	B-DatasetName
with	O
word	O
shielding	O
as	O
the	O
defense	O
,	O
k	O
fixed	O
at	O
100	O
.	O
As	O
we	O
increase	O
p	O
,	O
we	O
see	O
a	O
continued	O
drop	O
in	O
accuracy	B-MetricName
which	O
is	O
consistent	O
with	O
the	O
idea	O
that	O
a	O
higher	O
p	O
is	O
more	O
likely	O
to	O
capture	O
perturbed	O
text	O
.	O
The	O
optimal	O
value	O
range	O
appears	O
to	O
be	O
in	O
0.2	O
-	O
0.4	O
range	O
,	O
although	O
we	O
do	O
not	O
see	O
large	O
drops	O
until	O
0.6	O
onward	O
.	O
We	O
also	O
examined	O
the	O
same	O
combination	O
on	O
AG	B-DatasetName
News	I-DatasetName
(	O
Figure	O
5	O
)	O
since	O
it	O
's	O
texts	O
are	O
considerably	O
shorter	O
and	O
found	O
consistent	O
results	O
.	O
Optimal	O
k.	O
Figure	O
6	O
shows	O
results	O
for	O
all	O
attacks	O
against	O
LSTM	B-MethodName
on	O
IMDB	B-DatasetName
with	O
word	O
sampling	O
as	O
the	O
defense	O
,	O
p	O
fixed	O
at	O
0.3	O
.	O
The	O
optimal	O
k	O
is	O
not	O
as	O
clear	O
as	O
p.	O
We	O
see	O
clear	O
increases	O
after	O
30	O
samples	O
,	O
but	O
then	O
the	O
optimal	O
k	O
varies	O
depending	O
on	O
attack	O
.	O
However	O
,	O
we	O
see	O
a	O
leveling	O
off	O
around	O
90	O
samples	O
,	O
which	O
gives	O
some	O
credence	O
to	O
our	O
chosen	O
k	O
of	O
100	O
.	O
We	O
also	O
found	O
similar	O
results	O
when	O
examining	O
the	O
same	O
combination	O
on	O
AG	B-DatasetName
News	I-DatasetName
(	O
Figure	O
7	O
)	O
,	O
however	O
,	O
k	O
stabilized	O
lower	O
(	O
about	O
50	O
)	O
.	O

Sample	O
Shielding	O
,	O
an	O
intuitively	O
designed	O
defense	O
which	O
is	O
attacker	O
and	O
classifier	O
agnostic	O
,	O
protects	O
effectively	O
;	O
reducing	O
ASR	O
from	O
90	O
-	O
100	O
%	O
down	O
to	O
14	O
-	O
34	O
%	O
with	O
minimal	O
accuracy	B-MetricName
loss	B-MetricName
(	O
3	O
%	O
)	O
in	O
original	O
texts	O
.	O
The	O
randomness	O
(	O
through	O
sampling	O
)	O
provides	O
unreliable	O
feedback	O
for	O
attackers	O
,	O
thus	O
it	O
even	O
thwarts	O
attackers	O
who	O
have	O
query	O
access	O
to	O
classifiers	O
protected	O
with	O
Sample	O
Shielding	O
.	O
Attack	O
strategies	O
will	O
need	O
to	O
increase	O
the	O
amount	O
of	O
perturbation	O
to	O
make	O
sure	O
a	O
majority	O
of	O
samples	O
fail	O
at	O
classification	O
.	O
However	O
,	O
this	O
will	O
risk	O
semantic	O
integrity	O
.	O
Thus	O
,	O
we	O
expect	O
Sample	O
Shielding	O
to	O
cause	O
ripples	O
in	O
future	O
adversarial	B-TaskName
attack	I-TaskName
strategies	O
while	O
providing	O
text	O
classifiers	O
with	O
a	O
definite	O
advantage	O
.	O

In	O
this	O
paper	O
we	O
introduce	O
the	O
systems	O
IIE	O
submitted	O
for	O
the	O
WMT20	O
shared	O
task	O
on	O
German↔French	O
news	O
translation	O
.	O
Our	O
systems	O
are	O
based	O
on	O
the	O
Transformer	B-MethodName
architecture	O
with	O
some	O
effective	O
improvements	O
.	O
Multiscale	O
collaborative	O
deep	O
architecture	O
,	O
data	O
selection	O
,	O
back	O
translation	O
,	O
knowledge	B-MethodName
distillation	I-MethodName
,	O
domain	B-TaskName
adaptation	I-TaskName
,	O
model	O
ensemble	O
and	O
re	O
-	O
ranking	O
are	O
employed	O
and	O
proven	O
effective	O
in	O
our	O
experiments	O
.	O
Our	O
German	O
French	O
system	O
achieved	O
35.0	O
BLEU	B-MetricName
and	O
ranked	O
the	O
second	O
among	O
all	O
anonymous	O
submissions	O
,	O
and	O
our	O
French	O
German	O
system	O
achieved	O
36.6	O
BLEU	B-MetricName
and	O
ranked	O
the	O
fourth	O
in	O
all	O
anonymous	O
submissions	O
.	O

The	O
structure	O
of	O
NMT	O
models	O
has	O
evolved	O
quickly	O
,	O
such	O
as	O
RNN	O
-	O
based	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
,	O
CNN	O
-	O
based	O
(	O
Gehring	O
et	O
al	O
,	O
2017	O
)	O
and	O
attentionbased	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
systems	O
.	O
Deep	O
neural	O
networks	O
have	O
revolutionized	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
various	O
communities	O
,	O
from	O
computer	O
vision	O
to	O
natural	O
language	O
processing	O
.	O
We	O
adopt	O
the	O
deep	O
transformer	O
model	O
proposed	O
by	O
our	O
work	O
(	O
Wei	O
et	O
al	O
,	O
2020b	O
)	O
.	O
Instead	O
of	O
relying	O
on	O
the	O
whole	O
encoder	O
stack	O
to	O
directly	O
learn	O
a	O
desired	O
representation	O
,	O
we	O
let	O
each	O
encoder	O
block	O
learn	O
a	O
fine	O
-	O
grained	O
representation	O
and	O
enhance	O
it	O
by	O
encoding	O
spatial	O
dependencies	O
using	O
a	O
bottom	O
-	O
up	O
network	O
.	O
For	O
coordination	O
,	O
we	O
attend	O
each	O
block	O
of	O
the	O
decoder	O
to	O
both	O
the	O
corresponding	O
representation	O
of	O
the	O
encoder	O
and	O
the	O
contextual	O
representation	O
with	O
spatial	O
dependencies	O
.	O
This	O
not	O
only	O
shortens	O
the	O
path	O
of	O
error	O
propagation	O
,	O
but	O
also	O
helps	O
to	O
prevent	O
the	O
lower	O
level	O
information	O
from	O
being	O
forgotten	O
or	O
diluted	O
.	O
In	O
this	O
section	O
we	O
describe	O
the	O
details	O
(	O
as	O
illustrated	O
in	O
figure	O
1	O
)	O
of	O
our	O
deep	O
architectures	O
as	O
below	O
:	O
Block	O
-	O
Scale	O
Collaboration	O
.	O
An	O
intuitive	O
extension	O
of	O
naive	O
stacking	O
of	O
layers	O
is	O
to	O
group	O
few	O
stacked	O
layers	O
into	O
a	O
block	O
.	O
We	O
suppose	O
that	O
the	O
encoder	O
and	O
decoder	O
of	O
our	O
model	O
have	O
the	O
same	O
number	O
of	O
blocks	O
(	O
i.e.	O
,	O
N	O
)	O
.	O
Each	O
block	O
of	O
the	O
encoder	O
has	O
M	O
n	O
(	O
n	O
{	O
1	O
,	O
2	O
,	O
...	O
,	O
N	O
}	O
)	O
identical	O
layers	O
,	O
while	O
each	O
decoder	O
block	O
contains	O
one	O
layer	O
.	O
Thus	O
,	O
we	O
can	O
adjust	O
the	O
value	O
of	O
each	O
M	O
n	O
flexibly	O
to	O
increase	O
the	O
depth	O
of	O
the	O
encoder	O
.	O
Formally	O
,	O
for	O
the	O
n	O
-	O
th	O
block	O
of	O
the	O
encoder	O
:	O
B	O
n	O
e	O
=	O
BLOCK	O
e	O
(	O
B	O
n−1	O
e	O
)	O
,	O
(	O
1	O
)	O
where	O
BLOCK	O
e	O
(	O
)	O
is	O
the	O
block	O
function	O
,	O
in	O
which	O
the	O
layer	O
function	O
F	O
(	O
)	O
is	O
iterated	O
M	O
n	O
times	O
,	O
i.e.	O
where	O
l	O
{	O
1	O
,	O
2	O
,	O
...	O
,	O
M	O
n	O
}	O
,	O
H	O
n	O
,	O
l	O
e	O
and	O
Θ	B-HyperparameterName
n	O
,	O
l	O
e	O
are	O
the	O
representation	O
and	O
parameters	O
of	O
the	O
l	O
-	O
th	O
layer	O
in	O
the	O
n	O
-	O
th	O
block	O
,	O
respectively	O
.	O
The	O
decoder	O
works	O
in	O
a	O
similar	O
way	O
but	O
the	O
layer	O
function	O
G	O
(	O
)	O
is	O
iterated	O
only	O
once	O
in	O
each	O
block	O
,	O
B	O
n	O
d	O
=	O
BLOCK	O
d	O
(	O
B	O
n−1	O
d	O
,	O
B	O
n	O
e	O
)	O
=	O
G	O
(	O
B	O
n−1	O
d	O
,	O
B	O
n	O
e	O
;	O
Θ	B-HyperparameterName
n	O
d	O
)	O
+	O
B	O
n−1	O
d	O
.	O
(	O
3	O
)	O
Each	O
block	O
of	O
the	O
decoder	O
attends	O
to	O
the	O
corresponding	O
encoder	O
block	O
.	O
Contextual	O
Collaboration	O
.	O
To	O
model	O
long	O
-	O
term	O
spatial	O
dependencies	O
and	O
reuse	O
global	O
representations	O
,	O
we	O
define	O
a	O
GRU	B-MethodName
cell	O
Q	O
(	O
c	O
,	O
x	O
)	O
,	O
which	O
maps	O
a	O
hidden	O
state	O
c	O
and	O
an	O
additional	O
inputx	O
into	O
a	O
new	O
hidden	O
state	O
:	O
C	O
n	O
=	O
Q	O
(	O
C	O
n−1	O
,	O
B	O
n	O
e	O
)	O
,	O
n	O
[	O
1	O
,	O
N	O
]	O
C	O
0	B-DatasetName
=	O
E	O
e	O
,	O
(	O
4	O
)	O
where	O
E	O
e	O
is	O
the	O
embedding	O
matrix	O
of	O
the	O
source	O
input	O
x.	O
The	O
new	O
state	O
C	O
n	O
can	O
be	O
fused	O
with	O
each	O
layer	O
of	O
the	O
subsequent	O
blocks	O
in	O
both	O
the	O
encoder	O
and	O
the	O
decoder	O
.	O
Formally	O
,	O
B	O
n	O
e	O
in	O
Eq	O
.	O
(	O
1	O
)	O
can	O
be	O
re	O
-	O
calculated	O
in	O
the	O
following	O
way	O
:	O
B	O
n	O
e	O
=	O
H	O
n	O
,	O
Mn	O
e	O
,	O
H	O
n	O
,	O
l	O
e	O
=	O
F	O
(	O
H	O
n	O
,	O
l−1	O
e	O
,	O
C	O
n−1	O
;	O
Θ	B-HyperparameterName
n	O
,	O
l	O
e	O
)	O
+	O
H	O
n	O
,	O
l−1	O
e	O
,	O
H	O
n	O
,	O
0	B-DatasetName
e	O
=	O
B	O
n−1	O
e	O
.	O
(	O
5	O
)	O
Similarly	O
,	O
for	O
decoder	O
,	O
we	O
have	O
B	O
n	O
d	O
=	O
BLOCK	O
d	O
(	O
B	O
n−1	O
d	O
,	O
B	O
n	O
e	O
)	O
=	O
G	O
(	O
B	O
n−1	O
d	O
,	O
B	O
n	O
e	O
,	O
C	O
n	O
;	O
Θ	B-HyperparameterName
n	O
d	O
)	O
+	O
B	O
n−1	O
d	O
.	O
(	O
6	O
)	O

We	O
use	O
the	O
PyTorch	O
implementation	O
of	O
Transformer	B-MethodName
2	O
.	O
We	O
choose	O
the	O
Transformer	B-MethodName
base	O
setting	O
,	O
in	O
which	O
the	O
encoder	O
and	O
decoder	O
are	O
of	O
48	O
and	O
6	O
layers	O
,	O
respectively	O
.	O
The	O
dropout	O
rate	O
is	O
fixed	O
as	O
0.1	O
.	O
We	O
set	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
as	O
4096	O
and	O
the	O
parameter	O
-	O
-	O
update	O
-	O
freq	O
as	O
16	O
.	O

Results	O
and	O
ablations	O
for	O
De	O
Fr	O
Fr	O
De	O
are	O
shown	O
in	O
Table	O
1	O
and	O
2	O
,	O
respectively	O
.	O
We	O
report	O
case	O
-	O
sensitive	O
SacreBLEU	B-MetricName
scores	O
using	O
Sacre	O
-	O
BLEU	B-MetricName
(	O
Post	O
,	O
2018	O
)	O
3	O
,	O
using	O
international	O
tokenization	O
for	O
German↔French	O
.	O
German	O
French	O
For	O
De	O
Fr	O
,	O
iterative	O
BT	O
improves	O
our	O
baseline	O
performance	O
on	O
newstest	O
2019	O
by	O
about	O
2.5	O
BLEU	B-MetricName
.	O
The	O
addition	O
of	O
KD	O
and	O
model	O
ensemble	O
improves	O
single	O
model	O
performance	O
by	O
0.8	O
BLEU	B-MetricName
,	O
but	O
combining	O
this	O
with	O
fine	O
-	O
tuning	O
and	O
reranking	O
gives	O
us	O
a	O
total	O
of	O
2	O
BLEU	B-MetricName
.	O
Our	O
final	O
submission	O
for	O
WMT20	O
achieves	O
35.0	O
BLEU	B-MetricName
points	O
for	O
German	O
French	O
translation	O
(	O
ranked	O
in	O
the	O
second	O
place	O
)	O
.	O
French	O
German	O
For	O
Fr	O
De	O
,	O
we	O
see	O
similar	O
improvements	O
with	O
iterative	O
BT	O
by	O
about	O
2.3	O
BLEU	B-MetricName
.	O
KD	O
,	O
ensembling	O
,	O
and	O
fine	O
-	O
tuning	O
add	O
an	O
additional	O
1.4	O
BLEU	B-MetricName
,	O
with	O
reranking	O
contributing	O
0.9	O
BLEU	B-MetricName
.	O
Our	O
final	O
submission	O
for	O
WMT20	O
achieves	O
36.6	O
BLEU	B-MetricName
points	O
for	O
French	O
German	O
translation	O
(	O
ranked	O
in	O
the	O
fourth	O
among	O
anonymous	O
submissions	O
)	O
.	O

This	O
paper	O
describes	O
CAS	O
IIE	O
's	O
submission	O
to	O
the	O
WMT20	O
German↔French	O
news	O
translation	O
task	O
.	O
We	O
investigate	O
extremely	O
deep	O
models	O
(	O
with	O
48	O
layers	O
)	O
and	O
exploit	O
effective	O
strategies	O
to	O
better	O
utilize	O
parallel	O
data	O
as	O
well	O
as	O
monolingual	O
data	O
.	O
Finally	O
,	O
our	O
German	O
French	O
system	O
achieved	O
35.0	O
BLEU	B-MetricName
and	O
ranked	O
the	O
second	O
among	O
all	O
anonymous	O
submissions	O
,	O
and	O
our	O
French	O
German	O
system	O
achieved	O
36.6	O
BLEU	B-MetricName
and	O
ranked	O
the	O
fourth	O
in	O
all	O
anonymous	O
submissions	O
.	O

Following	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
,	O
we	O
consider	O
how	O
to	O
apply	O
the	O
pretrained	O
multilingual	O
BERT	B-MethodName
model	O
(	O
MBERT	B-MethodName
;	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
to	O
a	O
target	O
lowresource	O
language	O
,	O
for	O
which	O
both	O
labeled	O
and	O
unlabeled	O
data	O
is	O
scarce	O
.	O
This	O
model	O
has	O
produced	O
strong	O
CWRs	O
for	O
many	O
languages	O
(	O
Kondratyuk	O
and	O
Straka	O
,	O
2019	O
,	O
inter	O
alia	O
)	O
and	O
has	O
been	O
the	O
starting	O
model	O
for	O
many	O
studies	O
on	O
low	O
-	O
resource	O
languages	O
(	O
Muller	O
et	O
al	O
,	O
2021	O
;	O
Pfeiffer	O
et	O
al	O
,	O
2020	O
;	O
.	O
MBERT	B-MethodName
covers	O
the	O
languages	O
with	O
the	O
104	O
largest	O
Wikipedias	O
,	O
and	O
it	O
uses	O
this	O
data	O
to	O
con	O
-	O
struct	O
a	O
wordpiece	B-MethodName
vocabulary	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
and	O
train	O
its	O
transformer	O
-	O
based	O
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Although	O
low	O
-	O
resource	O
languages	O
are	O
slightly	O
oversampled	O
,	O
high	O
-	O
resource	O
languages	O
still	O
dominate	O
both	O
the	O
final	O
pretraining	O
data	O
and	O
the	O
vocabulary	O
(	O
Ács	O
,	O
2019	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
note	O
that	O
target	O
low	O
-	O
resource	O
languages	O
fall	O
into	O
three	O
categories	O
with	O
respect	O
to	O
MBERT	B-MethodName
's	O
pretraining	O
data	O
:	O
the	O
lowest	O
-	O
resource	O
languages	O
in	O
the	O
data	O
(	O
Type	O
1	O
)	O
,	O
completely	O
unseen	O
low	O
-	O
resource	O
languages	O
(	O
Type	O
2	O
)	O
,	O
and	O
low	O
-	O
resouce	O
languages	O
with	O
more	O
representation	O
(	O
Type	O
0	B-DatasetName
)	O
.	O
4	O
Due	O
to	O
their	O
poor	O
representation	O
in	O
the	O
vocabulary	O
,	O
Type	O
1	O
and	O
Type	O
2	O
languages	O
achieve	O
suboptimal	O
tokenization	O
and	O
higher	O
rates	O
of	O
the	O
"	O
unknown	O
"	O
wordpiece	B-MethodName
5	O
when	O
using	O
MBERT	B-MethodName
out	O
of	O
the	O
box	O
.	O
This	O
hinders	O
the	O
model	O
's	O
ability	O
to	O
capture	O
meaningful	O
patterns	O
in	O
the	O
data	O
,	O
resulting	O
in	O
reduced	O
data	O
efficiency	O
and	O
degraded	O
performance	O
.	O
We	O
note	O
that	O
this	O
challenge	O
is	O
exacerbated	O
when	O
modeling	O
languages	O
written	O
in	O
non	O
-	O
Latin	O
scripts	O
.	O
MBERT	B-MethodName
's	O
vocabulary	O
is	O
heavily	O
Latin	O
-	O
centric	O
(	O
Ács	O
,	O
2019	O
;	O
Muller	O
et	O
al	O
,	O
2021	O
)	O
,	O
resulting	O
in	O
a	O
significantly	O
larger	O
portion	O
of	O
non	O
-	O
Latin	O
scripts	O
being	O
represented	O
with	O
"	O
unknown	O
"	O
tokens	O
(	O
Pfeiffer	O
et	O
al	O
,	O
2021b	O
)	O
and	O
further	O
limiting	O
the	O
model	O
's	O
ability	O
to	O
generalize	O
.	O
In	O
effect	O
,	O
MBERT	B-MethodName
's	O
low	O
initial	O
performance	O
on	O
such	O
languages	O
can	O
be	O
attributed	O
to	O
its	O
inability	O
to	O
represent	O
the	O
script	O
itself	O
.	O
To	O
alleviate	O
the	O
problem	O
of	O
poor	O
tokenization	O
,	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
propose	O
to	O
specialize	O
MBERT	B-MethodName
using	O
Vocabulary	O
Augmentation	O
(	O
VA	O
)	O
.	O
Given	O
unlabeled	O
data	O
in	O
the	O
target	O
language	O
,	O
they	O
train	O
a	O
new	O
wordpiece	B-MethodName
vocabulary	O
on	O
the	O
data	O
,	O
then	O
select	O
the	O
99	O
most	O
common	O
wordpieces	O
in	O
the	O
new	O
vocabulary	O
that	O
replace	O
"	O
unknown	O
"	O
tokens	O
under	O
the	O
original	O
vocabulary	O
.	O
They	O
then	O
add	O
these	O
99	O
wordpieces	O
to	O
the	O
original	O
vocabulary	O
and	O
continue	O
pretraining	O
MBERT	B-MethodName
on	O
the	O
unlabeled	O
data	O
for	O
additional	O
steps	O
.	O
They	O
further	O
describe	O
a	O
tiered	O
variant	O
(	O
TVA	O
)	O
,	O
in	O
which	O
a	O
larger	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
used	O
for	O
the	O
embeddings	O
of	O
these	O
99	O
new	O
wordpieces	O
.	O
VA	O
yields	O
strong	O
gains	O
over	O
unadapted	O
multilingual	O
language	O
models	O
on	O
dependency	B-TaskName
parsing	I-TaskName
in	O
four	O
low	O
-	O
resource	O
languages	O
with	O
Latin	O
scripts	O
.	O
How	O
-	O
ever	O
,	O
no	O
evaluation	O
has	O
been	O
performed	O
on	O
other	O
tasks	O
or	O
on	O
languages	O
with	O
non	O
-	O
Latin	O
scripts	O
,	O
which	O
raises	O
our	O
first	O
research	O
question	O
:	O
RQ1	O
:	O
Do	O
the	O
conclusions	O
of	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
hold	O
for	O
other	O
tasks	O
and	O
for	O
languages	O
with	O
non	O
-	O
Latin	O
scripts	O
?	O
We	O
can	O
view	O
VA	O
and	O
TVA	O
as	O
an	O
instantation	O
of	O
a	O
more	O
general	O
framework	O
of	O
vocabulary	O
augmentation	O
,	O
shared	O
by	O
other	O
approaches	O
to	O
using	O
MBERT	B-MethodName
in	O
low	O
-	O
resource	O
settings	O
.	O
Given	O
a	O
new	O
vocabulary	O
V	O
,	O
number	O
of	O
wordpieces	O
n	O
,	O
and	O
learning	B-HyperparameterName
rate	I-HyperparameterName
multiplier	O
a	O
,	O
the	O
n	O
most	O
common	O
wordpieces	O
in	O
V	O
are	O
added	O
to	O
the	O
original	O
vocabulary	O
.	O
Additional	O
pretraining	O
is	O
then	O
performed	O
,	O
with	O
the	O
embeddings	O
of	O
the	O
n	O
wordpieces	O
taking	O
on	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
a	O
times	O
greater	O
than	O
the	O
overall	O
learning	B-HyperparameterName
rate	I-HyperparameterName
.	O
For	O
VA	O
,	O
we	O
set	O
n	O
=	O
99	O
and	O
a	O
=	O
1	O
,	O
while	O
we	O
treat	O
a	O
as	O
a	O
hyperparameter	O
for	O
TVA	O
.	O
The	O
related	O
E	O
-	O
MBERT	B-MethodName
method	O
of	O
sets	O
n	O
=	O
|	O
V	O
|	O
and	O
a	O
=	O
1	O
.	O
Investigating	O
various	O
other	O
instantiations	O
of	O
this	O
framework	O
is	O
an	O
interesting	O
research	O
direction	O
,	O
though	O
it	O
is	O
out	O
of	O
the	O
scope	O
of	O
this	O
work	O
.	O

To	O
pretrain	O
LAPT	O
and	O
VA	O
models	O
,	O
we	O
use	O
the	O
code	O
of	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
,	O
who	O
modify	O
the	O
pretraining	O
code	O
of	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
to	O
only	O
use	O
the	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
loss	B-MetricName
.	O
To	O
generate	O
VA	O
vocabularies	O
,	O
we	O
train	O
a	O
new	O
vocabulary	O
of	O
size	O
5000	O
and	O
select	O
the	O
99	O
wordpieces	O
that	O
replace	O
the	O
most	O
unknown	O
tokens	O
.	O
We	O
train	O
with	O
a	O
fixed	O
linear	B-MethodName
warmup	I-MethodName
of	O
1000	O
steps	O
.	O
To	O
pretrain	O
BERT	B-MethodName
models	O
,	O
we	O
use	O
the	O
HuggingFace	O
Transformers	O
library	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O
Following	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
,	O
we	O
train	O
a	O
half	O
-	O
sized	O
RoBERTa	B-MethodName
model	O
with	O
six	O
layers	O
and	O
12	O
attention	O
heads	O
.	O
We	O
use	O
a	O
byte	O
-	O
pair	O
vocabulary	O
of	O
size	O
52000	O
and	O
a	O
linear	B-MethodName
warmup	I-MethodName
of	O
1	O
epoch	O
.	O
For	O
LAPT	O
,	O
VA	O
,	O
and	O
BERT	B-MethodName
,	O
we	O
train	O
for	O
up	O
to	O
20	O
epochs	O
total	O
,	O
selecting	O
the	O
highest	O
-	O
performing	O
epoch	O
based	O
on	O
validation	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
loss	B-MetricName
.	O
FASTT	O
models	O
are	O
trained	O
with	O
the	O
skipgram	O
model	O
for	O
five	O
epochs	O
,	O
with	O
the	O
default	O
hyperparameters	O
of	O
Bojanowski	O
et	O
al	O
(	O
2017	O
)	O
.	O
Training	O
of	O
downstream	O
parsers	O
and	O
taggers	O
follows	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
and	O
Kondratyuk	O
and	O
Straka	O
(	O
2019	O
)	O
,	O
with	O
an	O
inverse	O
square	O
-	O
root	O
learning	B-HyperparameterName
rate	I-HyperparameterName
decay	O
and	O
linear	B-MethodName
warmup	I-MethodName
,	O
and	O
layer	O
-	O
wise	O
gradual	O
unfreezing	O
and	O
discriminative	O
finetuning	O
.	O
Models	O
are	O
trained	O
with	O
AllenNLP	O
,	O
version	O
2.1.0	O
,	O
for	O
up	O
to	O
200	O
epochs	O
with	O
early	B-MethodName
stopping	I-MethodName
based	O
on	O
validation	O
performance	O
.	O
We	O
choose	O
batch	O
sizes	O
to	O
be	O
the	O
maximum	O
that	O
allows	O
for	O
successful	O
training	O
on	O
one	O
GPU	O
.	O

We	O
perform	O
further	O
analysis	O
to	O
investigate	O
VA	O
's	O
patterns	O
of	O
success	O
.	O
Concretely	O
,	O
we	O
hypothesize	O
that	O
VA	O
significantly	O
improves	O
the	O
tokenizer	O
's	O
coverage	O
of	O
target	O
languages	O
where	O
it	O
is	O
most	O
successful	O
.	O
Inspired	B-DatasetName
by	O
Ács	O
(	O
2019	O
)	O
,	O
Chau	O
et	O
al	O
(	O
2020	O
)	O
,	O
and	O
Rust	O
et	O
al	O
(	O
2021	O
)	O
,	O
we	O
quantify	O
tokenizer	O
coverage	O
using	O
the	O
percentage	O
of	O
tokens	O
in	O
the	O
raw	O
text	O
that	O
yield	O
unknown	O
wordpieces	O
when	O
tokenized	O
with	O
a	O
given	O
vocabulary	O
(	O
"	O
UNK	O
token	O
percentage	O
"	O
)	O
.	O
These	O
are	O
tokens	O
whose	O
representations	O
contain	O
at	O
least	O
partial	O
ambiguity	O
due	O
to	O
the	O
inclusion	O
of	O
the	O
unknown	O
wordpiece	B-MethodName
.	O
Tab	O
.	O
3	O
presents	O
the	O
UNK	O
token	O
percentage	O
for	O
each	O
dataset	O
using	O
the	O
MBERT	B-MethodName
vocabulary	O
,	O
averaged	O
over	O
each	O
script	O
and	O
language	O
type	O
.	O
This	O
vocabulary	O
is	O
used	O
in	O
LAPT	O
and	O
represents	O
the	O
baseline	O
level	O
of	O
vocabulary	O
coverage	O
.	O
We	O
also	O
include	O
the	O
change	O
in	O
the	O
UNK	O
token	O
percentage	O
between	O
the	O
MBERT	B-MethodName
and	O
VA	O
vocabularies	O
,	O
which	O
quantifies	O
the	O
coverage	O
improvement	O
.	O
Both	O
sets	O
of	O
values	O
are	O
juxtaposed	O
against	O
the	O
average	O
change	O
in	O
task	O
-	O
specific	O
performance	O
from	O
LAPT	O
to	O
VA	O
,	O
representing	O
the	O
effect	O
of	O
augmenting	O
the	O
vocabulary	O
on	O
task	O
-	O
specific	O
performance	O
.	O
We	O
observe	O
that	O
off	O
-	O
the	O
-	O
shelf	O
MBERT	B-MethodName
already	O
at	O
-	O
tains	O
relatively	O
high	O
vocabulary	O
coverage	O
for	O
Type	O
0	B-DatasetName
and	O
1	O
languages	O
,	O
as	O
well	O
as	O
languages	O
written	O
in	O
Latin	O
and	O
Cyrillic	O
scripts	O
.	O
On	O
the	O
other	O
hand	O
,	O
up	O
to	O
one	O
-	O
fifth	O
of	O
the	O
tokens	O
in	O
Arabic	O
languages	O
and	O
one	O
-	O
sixth	O
of	O
those	O
in	O
Type	O
2	O
languages	O
yield	O
an	O
unknown	O
wordpiece	B-MethodName
.	O
For	O
these	O
languages	O
,	O
there	O
is	O
great	O
room	O
for	O
increasing	O
tokenizer	O
coverage	O
,	O
and	O
VA	O
indeed	O
addresses	O
this	O
more	O
tangible	O
need	O
.	O
This	O
aligns	O
with	O
the	O
task	O
-	O
specific	O
performance	O
improvements	O
for	O
each	O
group	O
and	O
helps	O
to	O
explain	O
our	O
results	O
in	O
2.3	O
.	O
It	O
is	O
notable	O
that	O
VA	O
does	O
not	O
always	O
eliminate	O
the	O
issue	O
of	O
unknown	O
wordpieces	O
,	O
even	O
in	O
languages	O
for	O
which	O
MBERT	B-MethodName
attains	O
high	O
vocabulary	O
coverage	O
.	O
This	O
suggests	O
that	O
the	O
remaining	O
unknown	O
wordpieces	O
in	O
these	O
languages	O
are	O
more	O
sparsely	O
distributed	O
(	O
i.e.	O
,	O
they	O
represent	O
low	O
frequency	O
sequences	O
)	O
,	O
while	O
the	O
unknown	O
wordpieces	O
in	O
languages	O
with	O
lower	O
vocabulary	O
coverage	O
represent	O
sequences	O
that	O
occur	O
more	O
commonly	O
.	O
As	O
a	O
result	O
,	O
augmenting	O
the	O
vocabulary	O
in	O
such	O
languages	O
quickly	O
improves	O
coverage	O
while	O
associating	O
these	O
commonly	O
occurring	O
sequences	O
with	O
each	O
other	O
,	O
which	O
benefits	O
the	O
overall	O
tokenization	O
quality	O
.	O
We	O
further	O
explore	O
the	O
association	O
between	O
the	O
improvements	O
in	O
vocabulary	O
coverage	O
and	O
taskspecific	O
performance	O
in	O
Fig	O
.	O
1	O
.	O
Although	O
we	O
do	O
not	O
find	O
that	O
languages	O
from	O
the	O
same	O
types	O
or	O
scripts	O
form	O
clear	O
clusters	O
,	O
we	O
nonetheless	O
observe	O
a	O
loose	O
correlation	O
between	O
the	O
two	O
factors	O
in	O
question	O
and	O
see	O
that	O
VA	O
delivers	O
greater	O
performance	O
gains	O
on	O
Type	O
2	O
and	O
Arabic	O
-	O
script	O
languages	O
compared	O
to	O
their	O
Type	O
0/1	O
and	O
Latin	O
-	O
script	O
counterparts	O
,	O
respectively	O
.	O
To	O
quantify	O
the	O
strength	O
of	O
this	O
association	O
,	O
we	O
also	O
compute	O
the	O
language	O
-	O
level	O
Spearman	B-MetricName
correlation	I-MetricName
between	O
the	O
change	O
in	O
UNK	O
token	O
percentage	O
on	O
the	O
unlabeled	O
dataset	O
7	O
from	O
the	O
MBERT	B-MethodName
to	O
VA	O
vocabulary	O
and	O
the	O
task	O
-	O
specific	O
performance	O
improvements	O
from	O
LAPT	O
to	O
VA	O
.	O
The	O
resulting	O
ρ	O
-	O
values	O
-	O
0.29	O
for	O
NER	B-TaskName
,	O
0.56	O
for	O
POS	O
tagging	O
,	O
and	O
0.81	O
for	O
UD	B-DatasetName
parsing	O
-	O
suggest	O
that	O
this	O
set	O
of	O
factors	O
is	O
meaningful	O
for	O
some	O
tasks	O
,	O
though	O
additional	O
and	O
more	O
fine	O
-	O
grained	O
analysis	O
in	O
future	O
work	O
should	O
give	O
a	O
more	O
complete	O
explanation	O
.	O
3	O
Mix	O
-	O
in	O
Specialization	O
:	O
VA	O
and	O
Transliteration	B-TaskName
We	O
now	O
expand	O
on	O
the	O
observation	O
made	O
in	O
2.3	O
regarding	O
the	O
difficulties	O
that	O
MBERT	B-MethodName
encounters	O
when	O
faced	O
with	O
unseen	O
low	O
-	O
resource	O
languages	O
in	O
non	O
-	O
Latin	O
scripts	O
because	O
of	O
its	O
inability	O
to	O
model	O
the	O
script	O
.	O
Having	O
observed	O
that	O
VA	O
is	O
beneficial	O
in	O
such	O
cases	O
,	O
we	O
now	O
investigate	O
the	O
interaction	O
between	O
this	O
method	O
and	O
another	O
specialization	O
approach	O
that	O
targets	O
this	O
problem	O
.	O
Specifically	O
,	O
we	O
consider	O
the	O
transliteration	O
methods	O
of	O
Muller	O
et	O
al	O
(	O
2021	O
)	O
,	O
in	O
which	O
unseen	O
low	O
-	O
resource	O
languages	O
in	O
non	O
-	O
Latin	O
scripts	O
are	O
transliterated	O
into	O
the	O
Latin	O
script	O
,	O
often	O
using	O
transliteration	O
schemes	O
inspired	O
by	O
the	O
Latin	O
orthographies	O
of	O
languages	O
related	O
to	O
the	O
target	O
language	O
.	O
They	O
hypothesize	O
that	O
the	O
increased	O
similarity	O
in	O
the	O
languages	O
'	O
writing	O
systems	O
,	O
combined	O
with	O
MBERT	B-MethodName
's	O
overall	O
Latin	O
-	O
centricity	O
,	O
provides	O
increased	O
opportunity	O
for	O
crosslingual	O
transfer	O
.	O
We	O
can	O
view	O
transliteration	O
as	O
a	O
inverted	O
form	O
of	O
vocabulary	O
augmentation	O
:	O
instead	O
of	O
adapting	O
the	O
model	O
to	O
the	O
needs	O
of	O
the	O
data	O
,	O
the	O
data	O
is	O
adjusted	O
to	O
meet	O
the	O
assumptions	O
of	O
the	O
model	O
.	O
Furthermore	O
,	O
the	O
transliteration	O
step	O
is	O
performed	O
prior	O
to	O
pretraining	O
MBERT	B-MethodName
on	O
additional	O
unlabeled	O
data	O
in	O
the	O
target	O
language	O
,	O
the	O
same	O
stage	O
at	O
which	O
VA	O
is	O
performed	O
.	O
In	O
both	O
cases	O
,	O
the	O
ultimate	O
goal	O
is	O
identical	O
:	O
improving	O
tokenization	O
and	O
more	O
effectively	O
using	O
available	O
data	O
.	O
We	O
can	O
thus	O
view	O
transliteration	O
and	O
VA	O
as	O
two	O
instantiations	O
of	O
a	O
more	O
general	O
mix	O
-	O
in	O
paradigm	O
for	O
model	O
specialization	O
,	O
whereby	O
various	O
transformations	O
(	O
mix	O
-	O
ins	O
)	O
are	O
applied	O
to	O
the	O
data	O
and/or	O
model	O
prior	O
to	O
performing	O
additional	O
pretraining	O
.	O
These	O
mix	O
-	O
ins	O
target	O
different	O
components	O
of	O
the	O
experimental	O
pipeline	O
,	O
which	O
naturally	O
raises	O
our	O
second	O
research	O
question	O
:	O
RQ2	O
:	O
How	O
do	O
the	O
VA	O
and	O
transliteration	O
mix	O
-	O
ins	O
for	O
MBERT	B-MethodName
compare	O
and	O
interact	O
?	O

Predictions	O
trained	O
with	O
Cross	O
Entropy	O
Each	O
phrase	O
is	O
a	O
sequence	O
of	O
word	B-TaskName
embeddings	I-TaskName
that	O
is	O
passed	O
through	O
an	O
LSTM	B-MethodName
to	O
produce	O
a	O
512d	O
vector	O
representation	O
for	O
the	O
premise	O
and	O
the	O
hypothesis	O
.	O
Both	O
vectors	O
are	O
used	O
to	O
compute	O
the	O
predicted	O
conditional	O
probability	O
and	O
calculate	O
the	O
loss	B-MetricName
.	O
can	O
not	O
express	O
P	O
(	O
X	O
)	O
=	O
0	B-DatasetName
exactly	O
,	O
but	O
can	O
get	O
arbitrarily	O
close	O
in	O
order	O
to	O
represent	O
the	O
probability	O
of	O
a	O
phrase	O
that	O
is	O
extremely	O
unlikely	O
.	O
6	O
Our	O
model	O
for	O
P	O
(	O
x	O
)	O
and	O
P	O
(	O
x	O
,	O
y	O
)	O
We	O
train	O
a	O
neural	O
network	O
model	O
to	O
predict	O
P	O
(	O
x	O
)	O
,	O
P	O
(	O
y	O
)	O
,	O
and	O
P	O
(	O
x	O
|	O
y	O
)	O
for	O
phrases	O
x	O
and	O
y.	O
This	O
model	O
consists	O
of	O
an	O
LSTM	B-MethodName
that	O
outputs	O
a	O
512d	O
vector	O
which	O
is	O
passed	O
through	O
an	O
additional	O
512d	O
layer	O
.	O
We	O
use	O
300d	O
GloVe	B-MethodName
vectors	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
trained	O
on	O
840B	O
tokens	O
as	O
the	O
word	O
embedding	O
input	O
to	O
the	O
LSTM	B-MethodName
.	O
We	O
use	O
the	O
same	O
model	O
to	O
represent	O
both	O
x	O
and	O
y	O
regardless	O
of	O
which	O
phrase	O
is	O
the	O
premise	O
or	O
the	O
hypothesis	O
.	O
Thus	O
,	O
we	O
pass	O
the	O
sequence	O
of	O
word	B-TaskName
embeddings	I-TaskName
for	O
phrase	O
x	O
through	O
the	O
model	O
to	O
get	O
x	O
,	O
and	O
we	O
do	O
the	O
same	O
for	O
phrase	O
y	O
to	O
get	O
y.	O
As	O
previously	O
described	O
,	O
we	O
sum	O
the	O
elements	O
of	O
x	O
and	O
y	O
to	O
get	O
the	O
predicted	O
denotational	O
probabilities	O
P	O
(	O
x	O
)	O
and	O
P	O
(	O
y	O
)	O
.	O
From	O
x	O
and	O
y	O
,	O
we	O
find	O
the	O
joint	O
vector	O
z	O
,	O
which	O
we	O
use	O
to	O
compute	O
the	O
predicted	O
denotational	O
conditional	O
probability	O
P	O
(	O
x	O
|	O
y	O
)	O
according	O
to	O
the	O
equation	O
in	O
Section	O
5	O
.	O
Figure	O
3	O
illustrates	O
the	O
structure	O
of	O
our	O
model	O
.	O
Our	O
training	O
data	O
consists	O
of	O
ordered	O
phrase	O
pairs	O
x	O
,	O
y	O
.	O
We	O
train	O
our	O
model	O
to	O
predict	O
the	O
denotational	O
probabilities	O
of	O
each	O
phrase	O
(	O
P	O
(	O
x	O
)	O
and	O
P	O
(	O
y	O
)	O
)	O
as	O
well	O
as	O
the	O
conditional	O
probability	O
P	O
(	O
x	O
|	O
y	O
)	O
.	O
Typically	O
the	O
pair	O
y	O
,	O
x	O
will	O
also	O
appear	O
in	O
the	O
training	O
data	O
.	O
Our	O
per	O
-	O
example	O
loss	B-MetricName
is	O
the	O
sum	O
of	O
the	O
cross	O
entropy	O
losses	O
for	O
P	O
(	O
x	O
)	O
,	O
P	O
(	O
y	O
)	O
,	O
and	O
P	O
(	O
x	O
|	O
y	O
)	O
:	O
L	O
=	O
−	O
P	O
(	O
x	O
)	O
log	O
Q	O
(	O
x	O
)	O
+	O
(	O
1−P	O
(	O
x	O
)	O
)	O
log	O
1−Q	O
(	O
x	O
)	O
−	O
P	O
(	O
y	O
)	O
log	O
Q	O
(	O
y	O
)	O
+	O
(	O
1−P	O
(	O
y	O
)	O
)	O
log	O
1−Q	O
(	O
y	O
)	O
−	O
P	O
(	O
x	O
|	O
y	O
)	O
log	O
Q	O
(	O
x	O
|	O
y	O
)	O
+	O
(	O
1−P	O
(	O
x	O
|	O
y	O
)	O
)	O
log	O
1−Q	O
(	O
x	O
|	O
y	O
)	O
We	O
use	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.001	O
,	O
and	O
a	O
dropout	O
rate	O
of	O
0.5	O
.	O
These	O
parameters	O
were	O
tuned	O
on	O
the	O
development	O
data	O
.	O
Numerical	O
issues	O
In	O
Section	O
5	O
,	O
we	O
described	O
the	O
probability	O
vectors	O
x	O
as	O
being	O
in	O
the	O
positive	O
orthant	O
.	O
However	O
,	O
in	O
our	O
implementation	O
,	O
we	O
use	O
unnormalized	O
log	O
probabilities	O
.	O
This	O
puts	O
all	O
of	O
our	O
vectors	O
in	O
the	O
negative	O
orthant	O
instead	O
,	O
but	O
it	O
prevents	O
the	O
gradients	O
from	O
becoming	O
too	O
small	O
during	O
training	O
.	O
To	O
ensure	O
that	O
the	O
vectors	O
are	O
in	O
R	O
N	O
−	O
,	O
we	O
clip	O
the	O
values	O
of	O
the	O
elements	O
of	O
x	O
so	O
that	O
x	O
i	O
≤	O
0	B-DatasetName
.	O
To	O
compute	O
log	O
P	O
(	O
x	O
)	O
,	O
we	O
sum	O
the	O
elements	O
of	O
x	O
and	O
clip	O
the	O
sum	O
to	O
the	O
range	O
(	O
log	O
(	O
10	O
−10	O
)	O
,	O
−0.0001	O
)	O
in	O
order	O
to	O
avoid	O
errors	O
caused	O
by	O
passing	O
log	O
(	O
0	B-DatasetName
)	O
values	O
to	O
the	O
loss	B-MetricName
function	O
.	O
The	O
conditional	O
log	O
probability	O
is	O
simply	O
log	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
log	O
P	O
(	O
x	O
,	O
y	O
)	O
−	O
log	O
P	O
(	O
y	O
)	O
,	O
where	O
log	O
P	O
(	O
x	O
,	O
y	O
)	O
is	O
now	O
the	O
element	O
-	O
wise	O
minimum	O
:	O
log	O
P	O
(	O
x	O
,	O
y	O
)	O
=	O
i	O
min	O
(	O
x	O
i	O
,	O
y	O
i	O
)	O
This	O
element	O
-	O
wise	O
minimum	O
is	O
a	O
standard	O
pooling	O
operation	O
(	O
we	O
take	O
the	O
minimum	O
instead	O
of	O
the	O
more	O
common	O
max	B-MethodName
pooling	I-MethodName
)	O
.	O
Note	O
that	O
if	O
x	O
i	O
>	O
y	O
i	O
,	O
neither	O
element	O
x	O
i	O
nor	O
y	O
i	O
is	O
updated	O
with	O
respect	O
to	O
the	O
P	O
(	O
x	O
|	O
y	O
)	O
loss	B-MetricName
.	O
Both	O
x	O
i	O
and	O
y	O
i	O
will	O
always	O
be	O
updated	O
with	O
respect	O
to	O
the	O
P	O
(	O
x	O
)	O
and	O
P	O
(	O
y	O
)	O
components	O
of	O
the	O
loss	B-MetricName
.	O

To	O
train	O
our	O
model	O
,	O
we	O
use	O
phrase	O
pairs	O
x	O
,	O
y	O
from	O
the	O
denotation	O
graph	O
generated	O
on	O
the	O
training	O
split	O
of	O
the	O
FLICKR30	O
K	O
corpus	O
(	O
Young	O
et	O
al	O
,	O
2014	O
)	O
.	O
We	O
consider	O
all	O
271	O
,	O
062	O
phrases	O
that	O
occur	O
with	O
at	O
least	O
10	O
images	O
in	O
the	O
training	O
split	O
of	O
the	O
graph	O
,	O
to	O
ensure	O
that	O
the	O
phrases	O
are	O
frequent	O
enough	O
that	O
their	O
computed	O
denotational	O
probabilities	O
are	O
reliable	O
.	O
Since	O
the	O
FLICKR30	O
K	O
captions	O
are	O
lemmatized	O
in	O
order	O
to	O
construct	O
the	O
denotation	O
graph	O
,	O
all	O
the	O
phrases	O
in	O
the	O
dataset	O
described	O
in	O
this	O
section	O
are	O
lemmatized	O
as	O
well	O
.	O
We	O
include	O
all	O
phrase	O
pairs	O
where	O
the	O
two	O
phrases	O
have	O
at	O
least	O
one	O
image	O
in	O
common	O
.	O
These	O
constitute	O
45	O
million	O
phrase	O
pairs	O
x	O
,	O
y	O
with	O
P	O
(	O
x	O
|	O
y	O
)	O
>	O
0	B-DatasetName
.	O
To	O
train	O
our	O
model	O
to	O
predict	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
0	B-DatasetName
,	O
we	O
include	O
phrase	O
pairs	O
x	O
,	O
y	O
that	O
have	O
no	O
images	O
in	O
common	O
if	O
N	O
×P	O
(	O
x	O
)	O
P	O
(	O
y	O
)	O
≥	O
N	O
−1	O
(	O
N	O
is	O
the	O
total	O
number	O
of	O
images	O
)	O
,	O
meaning	O
that	O
x	O
and	O
y	O
occur	O
frequently	O
enough	O
that	O
we	O
would	O
expect	O
them	O
to	O
co	O
-	O
occur	O
at	O
least	O
once	O
in	O
the	O
data	O
.	O
This	O
yields	O
2	O
million	O
pairs	O
where	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
0	B-DatasetName
.	O
For	O
additional	O
examples	O
of	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
1	O
,	O
we	O
include	O
phrase	O
pairs	O
that	O
have	O
an	O
ancestordescendant	O
relationship	O
in	O
the	O
denotation	O
graph	O
.	O
We	O
include	O
all	O
ancestor	O
-	O
descendant	O
pairs	O
where	O
each	O
phrase	O
occurs	O
with	O
at	O
least	O
2	O
images	O
,	O
for	O
an	O
additional	O
3	O
million	O
phrase	O
pairs	O
.	O
For	O
evaluation	O
purposes	O
,	O
we	O
first	O
assign	O
5	O
%	O
of	O
the	O
phrases	O
to	O
the	O
development	O
pool	O
and	O
5	O
%	O
to	O
the	O
test	O
pool	O
.	O
The	O
actual	O
test	O
data	O
then	O
consists	O
of	O
all	O
phrase	O
pairs	O
where	O
at	O
least	O
one	O
of	O
the	O
two	O
phrases	O
comes	O
from	O
the	O
test	O
pool	O
.	O
The	O
resulting	O
test	O
data	O
contains	O
10.6	O
%	O
unseen	O
phrases	O
by	O
type	O
and	O
51.2	O
%	O
unseen	O
phrases	O
by	O
token	O
.	O
All	O
phrase	O
pairs	O
in	O
the	O
test	O
data	O
contain	O
at	O
least	O
one	O
phrase	O
that	O
was	O
unseen	O
in	O
the	O
training	O
or	O
development	O
data	O
.	O
The	O
development	O
data	O
was	O
created	O
the	O
same	O
way	O
.	O
This	O
dataset	O
is	O
available	O
to	O
download	O
at	O
http://nlp.cs.illinois.edu/	O
HockenmaierGroup	O
/	O
data.html	O
.	O
We	O
train	O
our	O
model	O
on	O
the	O
training	O
data	O
(	O
42	O
million	O
phrase	O
pairs	O
)	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
512	O
for	O
10	O
epochs	O
,	O
and	O
use	O
the	O
mean	O
KL	O
divergence	O
on	O
the	O
conditional	O
probabilities	O
in	O
the	O
development	O
data	O
to	O
select	O
the	O
best	O
model	O
.	O
Since	O
P	O
(	O
x	O
|	O
y	O
)	O
is	O
a	O
Bernoulli	O
distribution	O
,	O
we	O
compute	O
the	O
KL	O
divergence	O
for	O
each	O
phrase	O
pair	O
x	O
,	O
y	O
as	O
D	O
KL	O
(	O
P	O
|	O
|	O
Q	O
)	O
=	O
P	O
(	O
x	O
|	O
y	O
)	O
log	O
P	O
(	O
x	O
|	O
y	O
)	O
Q	O
(	O
x	O
|	O
y	O
)	O
+	O
1	O
−	O
P	O
(	O
x	O
|	O
y	O
)	O
log	O
1	O
−	O
P	O
(	O
x	O
|	O
y	O
)	O
1	O
−	O
Q	O
(	O
x	O
|	O
y	O
)	O
where	O
Q	O
(	O
x	O
|	O
y	O
)	O
is	O
the	O
conditional	O
probability	O
predicted	O
by	O
our	O
model	O
.	O

We	O
evaluate	O
our	O
model	O
using	O
1	O
)	O
the	O
KL	O
divergences	O
D	O
KL	O
(	O
P	O
|	O
|	O
Q	O
)	O
of	O
the	O
gold	O
individual	O
and	O
conditional	O
probabilities	O
P	O
(	O
x	O
)	O
and	O
P	O
(	O
x	O
|	O
y	O
)	O
against	O
the	O
corresponding	O
predicted	O
probabilities	O
Q	O
,	O
and	O
2	O
)	O
the	O
Pearson	B-MetricName
correlation	I-MetricName
r	O
,	O
which	O
expresses	O
the	O
correlation	O
between	O
two	O
variables	O
(	O
the	O
per	O
-	O
item	O
gold	O
and	O
predicted	O
probabilities	O
)	O
as	O
a	O
value	O
between	O
−1	O
(	O
total	O
negative	O
correlation	O
)	O
and	O
1	O
(	O
total	O
positive	O
correlation	O
)	O
.	O
As	O
described	O
above	O
,	O
we	O
compute	O
the	O
KL	O
divergence	O
on	O
a	O
per	O
-	O
item	O
basis	O
,	O
and	O
report	O
the	O
mean	O
over	O
all	O
items	O
in	O
the	O
test	O
set	O
.	O
Table	O
1	O
shows	O
the	O
performance	O
of	O
our	O
trained	O
model	O
on	O
unseen	O
test	O
data	O
.	O
The	O
full	O
test	O
data	O
consists	O
of	O
4.6	O
million	O
phrase	O
pairs	O
,	O
all	O
of	O
which	O
contain	O
at	O
least	O
one	O
phrase	O
that	O
was	O
not	O
observed	O
in	O
either	O
the	O
training	O
or	O
development	O
data	O
.	O
Our	O
model	O
does	O
reasonably	O
well	O
at	O
predicting	O
these	O
conditional	O
probabilities	O
,	O
reaching	O
a	O
correlation	O
of	O
r	O
=	O
0.949	O
with	O
P	O
(	O
x	O
|	O
y	O
)	O
on	O
the	O
complete	O
test	O
data	O
.	O
On	O
the	O
subset	O
of	O
123	O
,	O
000	O
test	O
phrase	O
pairs	O
where	O
both	O
phrases	O
are	O
previously	O
unseen	O
,	O
the	O
model	O
's	O
predictions	O
are	O
almost	O
as	O
good	O
at	O
r	O
=	O
0.920	O
.	O
On	O
the	O
subset	O
of	O
3	O
,	O
100	O
test	O
phrase	O
pairs	O
where	O
at	O
We	O
also	O
analyze	O
our	O
model	O
's	O
accuracy	B-MetricName
on	O
phrase	O
pairs	O
where	O
the	O
gold	O
P	O
(	O
x	O
|	O
y	O
)	O
is	O
either	O
0	B-DatasetName
or	O
1	O
.	O
The	O
latter	O
case	O
reflects	O
an	O
important	O
property	O
of	O
the	O
denotation	O
graph	O
,	O
since	O
P	O
(	O
x	O
|	O
y	O
)	O
=	O
1	O
when	O
x	O
is	O
an	O
ancestor	O
of	O
y.	O
More	O
generally	O
,	O
we	O
can	O
interpret	O
P	O
(	O
h	O
|	O
p	O
)	O
=	O
1	O
as	O
a	O
confident	O
prediction	O
of	O
entailment	O
,	O
and	O
P	O
(	O
h	O
|	O
p	O
)	O
=	O
0	B-DatasetName
as	O
a	O
confident	O
prediction	O
of	O
contradiction	O
.	O
Figure	O
4	O
shows	O
the	O
distribution	O
of	O
predicted	O
conditional	O
probabilities	O
for	O
phrase	O
pairs	O
where	O
gold	O
P	O
(	O
h	O
|	O
p	O
)	O
=	O
0	B-DatasetName
(	O
top	O
)	O
and	O
gold	O
P	O
(	O
h	O
|	O
p	O
)	O
=	O
1	O
(	O
bottom	O
)	O
.	O
Our	O
model	O
's	O
predictions	O
on	O
unseen	O
phrase	O
pairs	O
(	O
gray	O
bars	O
)	O
are	O
nearly	O
as	O
accurate	O
as	O
its	O
predictions	O
on	O
the	O
full	O
test	O
data	O
(	O
black	O
bars	O
)	O
.	O

In	O
Section	O
7.2	O
,	O
we	O
trained	O
our	O
probability	O
model	O
on	O
both	O
short	O
phrase	O
pairs	O
for	O
which	O
we	O
had	O
gold	O
probabilities	O
and	O
longer	O
SNLI	B-DatasetName
sentence	O
pairs	O
for	O
which	O
we	O
estimated	O
probabilities	O
.	O
We	O
now	O
evaluate	O
the	O
effectiveness	O
of	O
this	O
model	O
for	O
textual	O
entailment	O
,	O
and	O
demonstrate	O
that	O
these	O
predicted	O
probabilities	O
are	O
informative	O
features	O
for	O
predicting	O
entailment	O
on	O
both	O
SICK	B-DatasetName
and	O
SNLI	B-DatasetName
.	O
Model	O
We	O
first	O
train	O
an	O
LSTM	B-MethodName
similar	O
to	O
the	O
100d	O
LSTM	B-MethodName
that	O
achieved	O
the	O
best	O
accuracy	B-MetricName
of	O
the	O
neural	O
models	O
in	O
Bowman	O
et	O
al	O
(	O
2015	O
)	O
.	O
It	O
takes	O
GloVe	B-MethodName
word	O
vectors	O
as	O
input	O
and	O
produces	O
100d	O
sentence	O
vectors	O
for	O
the	O
premise	O
and	O
hypothesis	O
.	O
200d	O
tanh	O
layers	O
and	O
a	O
softmax	B-MethodName
layer	O
for	O
3	O
-	O
class	O
entailment	O
classification	O
.	O
We	O
train	O
the	O
LSTM	B-MethodName
on	O
the	O
SNLI	B-DatasetName
training	O
data	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
512	O
for	O
10	O
epochs	O
.	O
We	O
use	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.001	O
and	O
a	O
dropout	O
rate	O
of	O
0.85	O
,	O
and	O
use	O
the	O
development	O
data	O
to	O
select	O
the	O
best	O
model	O
.	O
Next	O
,	O
we	O
take	O
the	O
output	O
vector	O
produced	O
by	O
the	O
LSTM	B-MethodName
for	O
each	O
sentence	O
pair	O
and	O
append	O
our	O
predicted	O
P	O
(	O
h	O
|	O
p	O
)	O
value	O
(	O
the	O
probability	O
of	O
the	O
hypothesis	O
given	O
the	O
premise	O
)	O
.	O
We	O
train	O
another	O
classifier	O
that	O
passes	O
this	O
201d	O
vector	O
through	O
two	O
tanh	O
layers	O
with	O
a	O
dropout	O
rate	O
of	O
0.5	O
and	O
a	O
final	O
3	O
-	O
class	O
softmax	B-MethodName
classification	O
layer	O
.	O
Holding	O
the	O
parameters	O
of	O
the	O
LSTM	B-MethodName
fixed	O
,	O
we	O
train	O
this	O
model	O
for	O
10	O
epochs	O
on	O
the	O
SNLI	B-DatasetName
training	O
data	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
512	O
.	O
Results	O
Table	O
2	O
contains	O
our	O
results	O
on	O
SNLI	B-DatasetName
.	O
Our	O
baseline	O
LSTM	B-MethodName
achieves	O
the	O
same	O
77.2	O
%	O
accuracy	B-MetricName
reported	O
by	O
Bowman	O
et	O
al	O
(	O
2015	O
)	O
,	O
whereas	O
a	O
classifier	O
that	O
combines	O
the	O
output	O
of	O
this	O
LSTM	B-MethodName
with	O
only	O
a	O
single	O
feature	O
from	O
the	O
output	O
of	O
our	O
probability	O
model	O
improves	O
to	O
78.2	O
%	O
accuracy	B-MetricName
.	O
We	O
use	O
the	O
same	O
approach	O
to	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
predictions	O
on	O
SICK	B-DatasetName
(	O
Table	O
3	O
)	O
.	O
SICK	B-DatasetName
does	O
not	O
have	O
enough	O
data	O
to	O
train	O
an	O
LSTM	B-MethodName
,	O
so	O
we	O
combine	O
the	O
SICK	B-DatasetName
and	O
SNLI	B-DatasetName
training	O
data	O
to	O
train	O
both	O
the	O
LSTM	B-MethodName
and	O
the	O
final	O
model	O
.	O
When	O
we	O
add	O
the	O
predicted	O
conditional	O
probability	O
as	O
a	O
single	O
feature	O
for	O
each	O
SICK	B-DatasetName
sentence	O
pair	O
,	O
performance	O
increases	O
from	O
81.5	O
%	O
to	O
82.7	O
%	O
accuracy	B-MetricName
.	O
This	O
approach	O
outperforms	O
the	O
transfer	B-TaskName
learning	I-TaskName
approach	O
of	O
Bowman	O
et	O
al	O
(	O
2015	O
)	O
,	O
which	O
was	O
also	O
trained	O
on	O
both	O
SICK	B-DatasetName
and	O
SNLI	B-DatasetName
.	O

The	O
knowledge	O
learned	O
through	O
model	O
-	O
based	O
RL	O
is	O
contributed	O
to	O
a	O
knowledge	O
base	O
that	O
can	O
be	O
used	O
for	O
many	O
tasks	O
.	O
So	O
our	O
KRR	O
-	O
RL	O
framework	O
enables	O
a	O
robot	O
to	O
dynamically	O
generate	O
partial	O
world	O
models	O
for	O
tasks	O
under	O
settings	O
that	O
were	O
never	O
experienced	O
.	O
For	O
example	O
,	O
an	O
agent	B-DatasetName
does	O
not	O
know	O
the	O
current	O
time	O
is	O
morning	O
or	O
noon	O
,	O
there	O
are	O
two	O
possible	O
values	O
for	O
variable	O
"	O
time	O
"	O
.	O
Consider	O
that	O
our	O
agent	B-DatasetName
has	O
learned	O
world	O
dynamics	O
under	O
the	O
times	O
of	O
morning	O
and	O
noon	O
.	O
Our	O
KRR	O
-	O
RL	O
framework	O
enables	O
the	O
robot	O
to	O
reason	O
about	O
the	O
two	O
transition	O
systems	O
under	O
the	O
two	O
settings	O
and	O
generate	O
a	O
new	O
transition	O
system	O
for	O
this	O
"	O
morning	O
-	O
or	O
-	O
noon	O
"	O
setting	O
.	O
Without	O
our	O
framework	O
,	O
an	O
agent	B-DatasetName
would	O
have	O
to	O
randomly	O
select	O
one	O
between	O
the	O
"	O
morning	O
"	O
and	O
"	O
noon	O
"	O
policies	O
.	O
To	O
evaluate	O
our	O
policies	O
dynamically	O
constructed	O
via	O
KRR	O
,	O
we	O
let	O
an	O
agent	B-DatasetName
learn	O
three	O
controllers	O
under	O
three	O
different	O
environment	O
settings	O
-	O
the	O
navigation	O
actions	O
have	O
decreasing	O
success	O
rates	O
under	O
the	O
settings	O
.	O
In	O
this	O
experiment	O
,	O
the	O
robot	O
does	O
not	O
know	O
which	O
setting	O
it	O
is	O
in	O
(	O
out	O
of	O
two	O
that	O
are	O
randomly	O
selected	O
)	O
.	O
The	O
baseline	O
does	O
not	O
have	O
the	O
KRR	O
capability	O
of	O
merging	O
knowledge	O
learned	O
from	O
different	O
settings	O
,	O
and	O
can	O
only	O
randomly	O
select	O
a	O
policy	O
from	O
the	O
two	O
(	O
each	O
corresponding	O
to	O
a	O
setting	O
)	O
.	O
Experimental	O
results	O
show	O
that	O
the	O
baseline	O
agent	B-DatasetName
achieved	O
an	O
average	O
of	O
26.8	O
%	O
success	O
rate	O
in	O
navigation	O
tasks	O
,	O
whereas	O
our	O
KRR	O
-	O
RL	O
agent	B-DatasetName
achieved	O
83.8	O
%	O
success	O
rate	O
on	O
average	O
.	O
Figure	O
7	O
shows	O
the	O
costs	O
in	O
a	O
box	O
plot	O
(	O
including	O
min	O
-	O
max	O
,	O
25	O
%	O
,	O
and	O
75	O
%	O
values	O
)	O
.	O
Thus	O
,	O
KRR	O
-	O
RL	O
enables	O
a	O
robot	O
to	O
effectively	O
apply	O
the	O
learned	O
knowledge	O
to	O
tasks	O
under	O
new	O
settings	O
.	O
Let	O
us	O
take	O
a	O
closer	O
look	O
at	O
the	O
"	O
time	O
"	O
variable	O
T	O
.	O
If	O
T	O
is	O
the	O
domain	O
of	O
T	O
,	O
the	O
RL	O
-	O
only	O
baseline	O
has	O
to	O
compute	O
a	O
total	O
of	O
2	O
|	O
T	O
|	O
world	O
models	O
to	O
account	O
for	O
all	O
possible	O
information	O
about	O
the	O
value	O
of	O
T	O
,	O
where	O
2	O
|	O
T	O
|	O
is	O
the	O
number	O
of	O
subsets	O
of	O
T	O
.	O
If	O
there	O
are	O
N	O
such	O
variables	O
,	O
the	O
number	O
of	O
world	O
models	O
grows	O
exponentially	O
to	O
2	O
|	O
T	O
|	O
N	O
.	O
In	O
comparison	O
,	O
the	O
KRR	O
-	O
RL	O
agent	B-DatasetName
needs	O
to	O
compute	O
only	O
|	O
T	O
|	O
N	O
world	O
models	O
,	O
which	O
dramatically	O
reduces	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
that	O
must	O
be	O
learned	O
through	O
RL	O
while	O
retaining	O
policy	O
quality	O
.	O

Reinforcement	O
learning	O
(	O
RL	O
)	O
,	O
or	O
specifically	O
,	O
policy	B-TaskName
gradient	I-TaskName
methods	I-TaskName
(	O
Williams	O
,	O
1992	O
)	O
,	O
have	O
been	O
frequently	O
adopted	O
to	O
both	O
task	O
-	O
oriented	O
dialogue	O
agents	O
(	O
Roman	O
Roman	O
et	O
al	O
,	O
2020	O
;	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2021	O
)	O
or	O
open	O
-	O
domain	O
chitchat	O
agents	O
(	O
Li	O
et	O
al	O
,	O
2016c	O
;	O
Saleh	O
et	O
al	O
,	O
2020	O
)	O
.	O
It	O
can	O
either	O
propagate	O
non	O
-	O
differentiable	O
loss	B-MetricName
(	O
Cai	O
et	O
al	O
,	O
2019a	O
)	O
or	O
optimize	O
an	O
expert	O
reward	O
such	O
as	O
ease	O
of	O
answering	O
(	O
Li	O
et	O
al	O
,	O
2016c	O
)	O
.	O
It	O
also	O
adopts	O
a	O
scenario	O
where	O
a	O
user	O
simulator	O
and	O
a	O
dialogue	O
agent	B-DatasetName
interact	O
,	O
and	O
an	O
Figure	O
1	O
:	O
An	O
example	O
of	O
the	O
inference	O
flow	O
that	O
shows	O
the	O
generated	O
partner	O
personas	O
and	O
the	O
incorporation	O
of	O
partner	O
personas	O
generation	O
into	O
response	B-TaskName
generation	I-TaskName
.	O
Figure	O
2	O
:	O
The	O
illustrated	O
reinforcement	O
learning	O
strategy	O
that	O
directly	O
backpropagates	O
the	O
response	O
-	O
related	O
rewards	O
from	O
the	O
critic	O
network	O
to	O
the	O
partner	O
personas	O
generator	O
and	O
the	O
dialogue	O
response	O
generator	O
.	O
expert	O
reward	O
function	O
can	O
be	O
defined	O
to	O
assign	O
the	O
goodness	O
to	O
each	O
response	O
generated	O
(	O
Roman	O
Roman	O
et	O
al	O
,	O
2020	O
)	O
.	O

We	O
employ	O
a	O
critic	O
network	O
to	O
compute	O
the	O
reinforcement	O
learning	O
rewards	O
for	O
our	O
generators	O
.	O
We	O
use	O
a	O
binary	O
classifier	O
as	O
critic	O
by	O
extracting	O
training	O
instances	O
(	O
s	O
,	O
r	O
,	O
L	O
=	O
1	O
)	O
,	O
3	O
(	O
s	O
A	O
,	O
r	O
A	O
,	O
L	O
=	O
1	O
)	O
and	O
(	O
s	O
B	O
,	O
r	O
B	O
,	O
L	O
=	O
1	O
)	O
.	O
Then	O
we	O
can	O
derive	O
two	O
negative	O
samples	O
as	O
:	O
(	O
s	O
A	O
,	O
r	O
B	O
,	O
L	O
=	O
0	B-DatasetName
)	O
and	O
(	O
s	O
B	O
,	O
r	O
A	O
,	O
L	O
=	O
0	B-DatasetName
)	O
.	O
Thereafter	O
,	O
we	O
fine	O
-	O
tune	O
on	O
a	O
binary	O
classifier	O
to	O
be	O
used	O
as	O
our	O
critic	O
in	O
RL	O
on	O
the	O
training	O
partition	O
by	O
minimizing	O
the	O
binary	O
cross	O
-	O
entropy	O
loss	B-MetricName
:	O
−Llog	O
(	O
P	O
(	O
L	O
|	O
s	O
,	O
r	O
)	O
)	O
−	O
(	O
1−L	O
)	O
log	O
(	O
1	O
−	O
P	O
(	O
L	O
|	O
s	O
,	O
r	O
)	O
)	O
,	O
where	O
the	O
binary	O
label	O
L	O
indicates	O
whether	O
the	O
response	O
is	O
relevant	O
to	O
the	O
personas	O
.	O
We	O
then	O
use	O
this	O
classifier	O
acting	O
as	O
a	O
critic	O
network	O
that	O
outputsL	O
,	O
conditioned	O
on	O
the	O
generated	O
partner	O
personasp	O
and	O
generated	O
responser	O
.	O
The	O
predicted	O
binary	O
labelL	O
is	O
then	O
converted	O
to	O
a	O
reward	O
R.	O
R	O
is	O
a	O
positive	O
reward	O
whenL	O
=	O
1	O
,	O
and	O
R	O
is	O
a	O
negative	O
reward	O
whenL	O
=	O
0	B-DatasetName
.	O
We	O
empirically	O
set	O
the	O
reward	O
R	O
for	O
RL	O
to	O
{	O
1	O
,	O
-	O
1	O
}	O
for	O
both	O
PPG	O
and	O
DRG	O
.	O
We	O
then	O
update	O
our	O
RL	O
agents	O
with	O
the	O
following	O
gradients	O
:	O
∆θ	O
PPG	O
=	O
−R	O
▽	O
θ	B-HyperparameterName
PPG	O
log	O
P	O
(	O
p	O
|	O
s	O
,	O
c	O
)	O
for	O
the	O
partner	O
personas	O
generator	O
(	O
PPG	O
)	O
,	O
and	O
for	O
the	O
dialogue	O
response	O
generator	O
(	O
DRG	O
)	O
:	O
∆θ	O
DRG	O
=	O
−R	O
▽	O
θ	B-HyperparameterName
DRG	O
log	O
P	O
(	O
r	O
|	O
s	O
,	O
p	O
,	O
c	O
)	O
By	O
formulating	O
a	O
reward	O
that	O
measures	O
the	O
relevance	O
between	O
generated	O
partner	O
personas	O
and	O
generated	O
dialogue	O
response	O
,	O
we	O
are	O
motivated	O
by	O
the	O
following	O
objectives	O
:	O
Further	O
fine	O
-	O
tune	O
the	O
partner	O
personas	O
generator	O
to	O
generate	O
personas	O
that	O
benefits	O
the	O
downstream	O
dialogue	O
response	B-TaskName
generation	I-TaskName
.	O
Further	O
fine	O
-	O
tune	O
the	O
dialogue	O
response	O
generator	O
trained	O
with	O
ground	O
-	O
truth	O
partner	O
personas	O
to	O
adapt	O
to	O
noisy	O
partner	O
personas	O
generated	O
by	O
the	O
partner	O
personas	O
generator	O
.	O
As	O
mentioned	O
in	O
Section	O
3.1	O
,	O
the	O
first	O
motivation	O
is	O
that	O
we	O
are	O
generating	O
the	O
complete	O
personas	O
profile	O
.	O
However	O
,	O
some	O
of	O
them	O
can	O
be	O
irrelevant	O
and	O
unhelpful	O
for	O
the	O
next	O
-	O
turn	O
dialogue	O
response	B-TaskName
generation	I-TaskName
.	O
It	O
could	O
be	O
challenging	O
for	O
the	O
partner	O
personas	O
generator	O
alone	O
to	O
identify	O
which	O
personas	O
could	O
be	O
helpful	O
.	O
Therefore	O
,	O
we	O
design	O
such	O
a	O
reward	O
to	O
train	O
the	O
personas	O
generator	O
to	O
learn	O
to	O
generate	O
a	O
set	O
of	O
personas	O
that	O
is	O
more	O
helpful	O
for	O
the	O
downstream	O
dialogue	O
response	B-TaskName
generation	I-TaskName
.	O
Our	O
second	O
motivation	O
is	O
that	O
the	O
dialogue	O
response	O
generator	O
has	O
not	O
been	O
exposed	O
to	O
the	O
generated	O
partner	O
personas	O
.	O
We	O
would	O
like	O
to	O
fine	O
-	O
tune	O
the	O
response	O
generator	O
to	O
mitigate	O
the	O
potential	O
traininginference	O
discrepancy	O
.	O
Experimental	O
results	O
indicate	O
that	O
our	O
design	O
empirically	O
works	O
well	O
.	O
The	O
previous	O
work	O
from	O
Cai	O
et	O
al	O
(	O
2019a	O
)	O
employed	O
critic	O
network	O
for	O
RL	O
loss	B-MetricName
backpropagation	O
.	O
The	O
major	O
difference	O
is	O
that	O
their	O
critic	O
is	O
trained	O
in	O
an	O
adversarial	O
manner	O
(	O
Li	O
et	O
al	O
,	O
2018	O
)	O
to	O
pick	O
up	O
the	O
gold	O
response	O
among	O
other	O
negative	O
candidates	O
.	O
Also	O
,	O
their	O
critic	O
network	O
conditions	O
only	O
on	O
the	O
dialogue	O
response	O
but	O
not	O
on	O
the	O
generated	O
skeleton	O
.	O
In	O
contrast	O
,	O
we	O
aim	O
for	O
improved	O
response	B-TaskName
generation	I-TaskName
with	O
a	O
classifier	O
conditioning	O
on	O
both	O
the	O
generated	O
personas	O
and	O
the	O
generated	O
response	O
.	O

For	O
both	O
PPG	O
and	O
DRG	O
,	O
perplexity	B-MetricName
(	O
PPL	O
)	O
is	O
reported	O
to	O
measure	O
the	O
intrinsic	O
performance	O
with	O
the	O
ground	O
truth	O
output	O
(	O
Roller	O
et	O
al	O
,	O
2021	O
)	O
.	O
We	O
adopt	O
well	O
-	O
known	O
sequence	O
evaluation	O
metrics	O
weighted	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
and	O
Fmeasure	O
for	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
(	O
Lin	O
,	O
2004	O
)	O
as	O
the	O
extrinsic	O
evaluations	O
.	O
For	O
PPG	O
,	O
we	O
also	O
report	O
Distinct	O
-	O
N	O
with	O
N=	O
{	O
1	O
,	O
2	O
}	O
to	O
measure	O
the	O
response	O
diversity	O
(	O
Li	O
et	O
al	O
,	O
2016a	O
;	O
Cai	O
et	O
al	O
,	O
2019b	O
;	O
Gao	O
et	O
al	O
,	O
2019	O
)	O
with	O
the	O
ratio	O
of	O
distinct	O
unigrams	O
/	O
bigrams	O
against	O
total	O
number	O
of	O
unigrams	O
/	O
bigrams	O
generated	O
.	O

We	O
conduct	O
experiments	O
on	O
the	O
PERSONACHAT	O
,	O
the	O
most	O
well	O
-	O
known	O
multiturn	O
dialogue	O
dataset	O
conditioned	O
on	O
personas	O
.	O
We	O
follow	O
the	O
train	O
/	O
valid	O
/	O
test	O
split	O
from	O
the	O
PARLAI	O
platform	O
(	O
Miller	O
et	O
al	O
,	O
2017	O
)	O
that	O
contains	O
about	O
65	O
,	O
000/7	O
,	O
800/7	O
,	O
500	O
instances	O
respectively	O
.	O
Each	O
instance	O
contains	O
about	O
8	O
utterances	O
on	O
average	O
and	O
about	O
4	O
traits	O
for	O
each	O
of	O
the	O
self	O
and	O
partner	O
personas	O
.	O
We	O
denote	O
the	O
dataset	O
with	O
this	O
original	O
personas	O
as	O
PERSONACHAT	O
-	O
ORI	O
.	O
Later	O
the	O
original	O
personas	O
have	O
been	O
manually	O
scrutinized	O
by	O
rephrasing	O
,	O
generalizing	O
or	O
specializing	O
,	O
which	O
we	O
denote	O
as	O
PERSONACHAT	O
-	O
REV	O
.	O
We	O
apply	O
the	O
same	O
preprocessing	O
operation	O
to	O
both	O
datasets	O
.	O
To	O
train	O
the	O
critic	O
for	O
RL	O
,	O
we	O
collected	O
about	O
130	O
,	O
000	O
instances	O
from	O
the	O
train	O
split	O
with	O
equally	O
distributed	O
positive	O
and	O
negative	O
samples	O
.	O
During	O
early	O
experiments	O
,	O
we	O
found	O
that	O
feeding	O
all	O
traits	O
yields	O
lower	O
performance	O
.	O
Retrieving	O
Top	O
-	O
3	O
relevant	O
partner	O
personas	O
using	O
BM25	O
(	O
Robertson	O
and	O
Walker	O
,	O
1994	O
)	O
yields	O
the	O
best	O
performance	O
on	O
the	O
original	O
personas	O
.	O
GPT	B-MethodName
-	O
2	O
This	O
is	O
a	O
comparison	O
model	O
fine	O
-	O
tuned	O
on	O
GPT	B-MethodName
-	O
2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
build	O
the	O
same	O
three	O
E2E	B-DatasetName
systems	O
described	O
above	O
,	O
and	O
the	O
best	O
model	O
is	O
selected	O
,	O
the	O
third	O
one	O
.	O
TRANSFERTRANSFO	O
A	O
comparison	O
model	O
built	O
with	O
a	O
Transformer	B-MethodName
-	O
based	O
model	O
pre	O
-	O
trained	O
on	O
gen	O
-	O
eral	O
domain	O
corpus	O
,	O
which	O
is	O
then	O
fine	O
-	O
tuned	O
on	O
PERSONACHAT	O
.	O
PERCVAE	O
This	O
is	O
a	O
comparison	O
model	O
that	O
employs	O
a	O
memory	O
-	O
augmented	O
architecture	O
incorporated	O
with	O
conditional	O
variational	B-MethodName
autoencoder	I-MethodName
that	O
exploits	O
persona	O
information	O
.	O
PAML	O
This	O
is	O
a	O
comparison	O
model	O
that	O
leverages	O
several	O
dialogues	O
collected	O
from	O
the	O
same	O
speaker	O
to	O
enhance	O
response	O
personality	O
via	O
metalearning	O
(	O
Madotto	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
the	O
authors	O
did	O
not	O
conduct	O
experiments	O
on	O
the	O
PERSONACHAT	O
-	O
REV	O
and	O
no	O
preprocessing	O
scripts	O
are	O
provided	O
for	O
the	O
revised	O
personas	O
,	O
we	O
only	O
report	O
the	O
results	O
of	O
their	O
model	O
on	O
the	O
PERSONACHAT	O
-	O
ORI	O
only	O
.	O
MTL	O
w/	O
Personas	O
Reconstruction	B-TaskName
This	O
is	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
MTL	O
)	O
comparison	O
model	O
(	O
Lee	O
et	O
al	O
,	O
2021	O
)	O
trained	O
to	O
maximise	O
the	O
objective	O
:	O
αL	O
PPG	O
+	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
L	O
DRG	O
,	O
where	O
L	O
PPG	O
represents	O
the	O
auxiliary	O
PPG	O
likelihood	O
,	O
and	O
L	O
DRG	O
represents	O
the	O
DRG	O
likelihood	O
.	O
α	B-HyperparameterName
is	O
weight	O
tuned	O
over	O
the	O
validation	O
set	O
,	O
and	O
both	O
tasks	O
condition	O
on	O
dialogue	O
context	O
and	O
self	O
personas	O
and	O
share	O
the	O
same	O
model	O
parameters	O
.	O

For	O
supervised	O
phase	O
,	O
we	O
set	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
as	O
our	O
optimizer	B-HyperparameterName
,	O
with	O
hyperparameters	O
η	O
=	O
5e−4	O
,	O
β	B-HyperparameterName
1	O
=	O
0.9	O
,	O
β	B-HyperparameterName
2	O
=	O
0.999	O
,	O
ϵ	O
=	O
1e−8	O
.	O
The	O
models	O
are	O
fine	O
-	O
tuned	O
for	O
2	O
epochs	O
.	O
For	O
RL	O
phase	O
,	O
we	O
set	O
Adam	B-MethodName
as	O
our	O
optimizer	B-HyperparameterName
,	O
with	O
η	O
=	O
5e−6	O
,	O
β	B-HyperparameterName
1	O
=	O
0.9	O
,	O
β	B-HyperparameterName
2	O
=	O
0.999	O
,	O
ϵ	O
=	O
1e−8	O
.	O
We	O
update	O
the	O
model	O
parameters	O
every	O
20	O
training	O
instances	O
and	O
validate	O
the	O
model	O
performance	O
every	O
50	O
updates	O
.	O
DistilBERT	B-MethodName
is	O
used	O
to	O
initialize	O
the	O
model	O
parameters	O
for	O
the	O
critic	O
network	O
.	O
We	O
set	O
Adam	B-MethodName
as	O
our	O
optimizer	B-HyperparameterName
,	O
with	O
hyperparameters	O
η	O
=	O
5e−6	O
,	O
β	B-HyperparameterName
1	O
=	O
0.9	O
,	O
β	B-HyperparameterName
2	O
=	O
0.999	O
,	O
ϵ	O
=	O
1e−8	O
.	O
We	O
fine	O
-	O
tune	O
the	O
critic	O
for	O
1	O
epoch	O
,	O
and	O
we	O
freeze	O
it	O
empirically	O
during	O
RL	O
.	O
All	O
the	O
experiments	O
are	O
conducted	O
based	O
on	O
the	O
TRANSFORMERS	O
library	O
from	O
HUGGINGFACE	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O

We	O
present	O
the	O
progressive	O
change	O
of	O
the	O
testing	O
perplexity	B-MetricName
for	O
DRG	O
and	O
PPG	O
on	O
PERSONACHAT	O
-	O
ORI	O
in	O
Figure	O
4	O
.	O
4	O
We	O
observe	O
that	O
they	O
improve	O
D	O
Human	O
Evaluation	O
Criteria	O
(	O
Appropriateness	O
)	O
:	O
"	O
Who	O
is	O
more	O
appropriate	O
given	O
the	O
previous	O
dialogue	O
context	O
?	O
"	O
(	O
Informativeness	O
)	O
:	O
"	O
Who	O
is	O
more	O
diverse	O
instead	O
of	O
null	O
answers	O
such	O
as	O
I	O
do	O
not	O
know	O
?	O
"	O
(	O
Engagingness	O
)	O
:	O
"	O
Who	O
would	O
you	O
prefer	O
to	O
talk	O
with	O
for	O
a	O
long	O
conversation	O
?	O
"	O
(	O
Human	O
-	O
likeness	O
)	O
:	O
"	O
Which	O
speaker	O
do	O
you	O
think	O
sounds	O
more	O
like	O
a	O
real	O
person	O
?	O
"	O
(	O
Coherence	O
)	O
:	O
"	O
Which	O
persona	O
contains	O
traits	O
that	O
are	O
more	O
coherent	O
to	O
each	O
other	O
?	O
"	O
(	O
Interestingness	B-DatasetName
)	O
:	O
"	O
Which	O
persona	O
is	O
more	O
interesting	O
and	O
diverse	O
?	O
"	O
The	O
first	O
four	O
are	O
from	O
the	O
existing	O
work	O
Zou	O
et	O
al	O
,	O
2021	O
)	O
and	O
we	O
propose	O
the	O
last	O
two	O
for	O
evaluating	O
PPG	O
.	O
We	O
report	O
the	O
first	O
four	O
for	O
DRG	O
,	O
and	O
we	O
report	O
the	O
last	O
four	O
for	O
PPG	O
.	O

We	O
run	O
all	O
our	O
experiments	O
on	O
a	O
single	O
NVIDIA	O
TI	O
-	O
TAN	O
RTX	O
with	O
24	O
GB	O
GPU	O
memory	O
.	O
Fine	O
-	O
tuning	O
the	O
generators	O
for	O
2	O
epochs	O
as	O
we	O
have	O
done	O
on	O
our	O
preprocessed	O
PERSONACHAT	O
train	O
split	O
consumes	O
about	O
3	O
-	O
4	O
hours	O
.	O
Fine	O
-	O
tuning	O
our	O
critic	O
classifier	O
for	O
1	O
epoch	O
consumes	O
about	O
1	O
hour	O
.	O
Our	O
RL	O
phase	O
consumes	O
about	O
15	O
hours	O
to	O
achieve	O
the	O
best	O
validation	O
loss	B-MetricName
before	O
being	O
early	O
stopped	O
.	O
We	O
report	O
averaged	O
results	O
from	O
3	O
runs	O
for	O
our	O
dialogue	O
response	B-TaskName
generation	I-TaskName
and	O
partner	O
personas	O
generation	O
results	O
reported	O
in	O
Table	O
1	O
,	O
Table	O
4	O
and	O
Table	O
7	O
.	O

The	O
Variational	B-MethodName
Autoencoder	I-MethodName
is	O
a	O
generative	O
model	O
first	O
introduced	O
by	O
Kingma	O
and	O
Welling	O
(	O
2013	O
)	O
.	O
Like	O
other	O
autoencoders	B-MethodName
,	O
VAEs	O
learn	O
a	O
mapping	O
q	O
θ	B-HyperparameterName
(	O
z	O
|	O
x	O
)	O
from	O
high	O
dimensional	O
input	O
x	O
to	O
a	O
low	O
dimensional	O
latent	O
variable	O
z.	O
Instead	O
of	O
doing	O
this	O
in	O
a	O
deterministic	O
way	O
,	O
the	O
encoder	O
learns	O
the	O
parameters	O
of	O
e.g.	O
a	O
normal	O
distribution	O
.	O
The	O
desired	O
effect	O
is	O
that	O
each	O
area	O
in	O
the	O
latent	O
space	O
has	O
a	O
semantic	O
meaning	O
and	O
thus	O
samples	O
from	O
p	O
(	O
z	O
)	O
can	O
be	O
decoded	O
in	O
a	O
meaningful	O
way	O
.	O
The	O
decoder	O
p	O
θ	B-HyperparameterName
(	O
x	O
|	O
z	O
)	O
,	O
also	O
referred	O
to	O
as	O
dec	O
(	O
z	O
)	O
,	O
is	O
trained	O
to	O
reconstruct	O
the	O
input	O
x	O
based	O
on	O
the	O
latent	O
variable	O
z.	O
In	O
order	O
to	O
approximate	O
θ	B-HyperparameterName
via	O
gradient	O
descent	O
the	O
reparametrization	O
trick	O
(	O
Kingma	O
and	O
Welling	O
,	O
2013	O
)	O
was	O
introduced	O
.	O
This	O
trick	O
allows	O
the	O
gradient	O
to	O
flow	O
through	O
non	O
-	O
deterministic	O
z	O
by	O
separating	O
the	O
discrete	O
sampling	O
operation	O
.	O
Let	O
µ	O
and	O
σ	O
be	O
deterministic	O
outputs	O
of	O
the	O
encoder	O
q	O
θ	B-HyperparameterName
(	O
µ	O
,	O
σ	O
|	O
x	O
)	O
:	O
z	O
=	O
µ	O
+	O
σ	O
where	O
∼	O
N	O
(	O
0	B-DatasetName
,	O
I	O
)	O
and	O
is	O
the	O
element	O
-	O
wise	O
product	O
.	O
To	O
prevent	O
the	O
model	O
from	O
pushing	O
σ	O
close	O
to	O
0	B-DatasetName
and	O
thus	O
falling	O
back	O
to	O
a	O
deterministic	O
autoencoder	B-MethodName
,	O
the	O
objective	O
is	O
extended	O
by	O
the	O
Kullback	O
-	O
Leibler	O
(	O
KL	O
)	O
diver	O
-	O
gence	O
between	O
prior	O
p	O
(	O
z	O
)	O
and	O
q	O
(	O
z	O
|	O
x	O
)	O
:	O
L	O
(	O
θ	B-HyperparameterName
;	O
x	O
)	O
=	O
−KL	O
(	O
q	O
θ	B-HyperparameterName
(	O
z	O
|	O
x	O
)	O
|	O
|	O
p	O
(	O
z	O
)	O
)	O
+	O
E	O
q	O
θ	B-HyperparameterName
(	O
z	O
|	O
x	O
)	O
[	O
logp	O
θ	B-HyperparameterName
(	O
x	O
|	O
z	O
)	O
]	O
.	O
(	O
2	O
)	O
Bowman	O
et	O
al	O
(	O
2016	O
)	O
apply	O
this	O
idea	O
for	O
sentence	O
generation	O
using	O
an	O
RNN	O
as	O
encoder	O
and	O
decoder	O
.	O
They	O
observe	O
that	O
a	O
strong	O
auto	O
-	O
regressive	O
language	O
modeling	O
ability	O
in	O
the	O
decoder	O
reduces	O
the	O
information	O
stored	O
in	O
the	O
latent	O
variable	O
,	O
right	O
up	O
to	O
a	O
complete	O
collapse	O
of	O
the	O
KL	O
term	O
.	O
They	O
explore	O
different	O
techniques	O
to	O
weaken	O
the	O
decoder	O
,	O
like	O
word	O
dropout	O
or	O
KL	O
term	O
weight	O
annealing	O
,	O
as	O
possible	O
solutions	O
.	O
This	O
guarantees	O
a	O
semantically	O
rich	O
latent	O
variable	O
and	O
good	O
sentence	O
generation	O
ability	O
.	O
Below	O
,	O
we	O
describe	O
how	O
to	O
combine	O
both	O
techniques	O
in	O
order	O
to	O
generate	O
meaningful	O
queries	O
for	O
Membership	O
Query	O
Synthesis	O
.	O

The	O
data	O
used	O
in	O
our	O
experiments	O
comes	O
from	O
two	O
sources	O
,	O
(	O
i	O
)	O
the	O
SST2	B-DatasetName
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
and	O
(	O
ii	O
)	O
SAR14	O
(	O
Nguyen	O
et	O
al	O
,	O
2014	O
)	O
.	O
We	O
limit	O
sentence	O
length	O
to	O
a	O
maximum	O
of	O
15	O
words	O
.	O
This	O
is	O
motivated	O
by	O
lower	O
training	O
times	O
and	O
the	O
tendency	O
of	O
vanilla	O
VAEs	O
not	O
to	O
perform	O
well	O
on	O
longer	O
sentences	O
(	O
Shen	O
et	O
al	O
,	O
2019	O
)	O
.	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
with	O
size	O
512	O
.	O
As	O
additional	O
regularization	O
we	O
set	O
weight	O
dropout	O
to	O
0.3	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
.	O
Input	O
embeddings	O
are	O
also	O
of	O
size	O
512	O
,	O
which	O
allows	O
us	O
to	O
share	O
the	O
embed	O
-	O
ding	O
weights	O
with	O
the	O
softmax	B-MethodName
weights	O
of	O
the	O
output	O
layer	O
(	O
Press	O
and	O
Wolf	O
,	O
2016	O
)	O
.	O
To	O
prevent	O
posterior	O
collapse	O
we	O
use	O
logistic	O
annealing	O
of	O
the	O
KL	O
term	O
weight	O
and	O
weaken	O
the	O
decoder	O
by	O
applying	O
word	O
dropout	O
with	O
probability	O
0.5	O
(	O
Bowman	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
model	O
is	O
trained	O
using	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.005	O
.	O
Once	O
the	O
KL	O
term	O
weight	O
is	O
close	O
to	O
1	O
,	O
the	O
learning	O
weight	O
is	O
linearly	O
decreased	O
to	O
0	B-DatasetName
.	O
The	O
training	O
stops	O
after	O
20	O
epochs	O
and	O
the	O
latent	O
variable	O
z	O
has	O
k	B-HyperparameterName
=	I-HyperparameterName
50	O
dimensions	O
.	O
The	O
trained	O
VAE	B-MethodName
achieves	O
a	O
reconstruction	O
loss	B-MetricName
of	O
45.3	O
and	O
KL	O
divergence	O
of	O
13.2	O
on	O
the	O
SST2	B-DatasetName
training	O
set	O
.	O
Learner	O
The	O
Learner	O
is	O
an	O
SVM	B-MethodName
1	O
with	O
linear	O
kernel	O
.	O
Each	O
instance	O
is	O
represented	O
as	O
the	O
latent	O
variable	O
z	O
learned	O
by	O
the	O
autoencoder	B-MethodName
.	O
The	O
latent	O
variable	O
is	O
a	O
vector	O
with	O
50	O
dimensions	O
and	O
the	O
SVM	B-MethodName
is	O
trained	O
on	O
this	O
representation	O
.	O
We	O
calculate	O
classification	O
performance	O
on	O
the	O
reduced	O
SST2	B-DatasetName
test	O
set	O
and	O
report	O
F1	B-MetricName
-	O
scores	O
.	O
Generator	O
The	O
generator	O
is	O
the	O
decoder	O
of	O
the	O
VAE	B-MethodName
described	O
above	O
.	O
Once	O
a	O
point	O
z	O
in	O
feature	O
space	O
is	O
selected	O
,	O
it	O
is	O
used	O
as	O
the	O
input	O
of	O
the	O
decoder	O
x	O
=	O
dec	O
(	O
z	O
)	O
which	O
generates	O
the	O
human	O
readable	O
sentence	O
x	O
in	O
an	O
autoregressive	O
way	O
.	O

The	O
instances	O
selected	O
or	O
generated	O
by	O
any	O
model	O
or	O
baseline	O
are	O
annotated	O
manually	O
by	O
one	O
human	O
coder	O
.	O
2	O
Although	O
the	O
pool	O
data	O
has	O
labels	O
on	O
the	O
review	O
level	O
,	O
we	O
do	O
not	O
use	O
these	O
labels	O
in	O
our	O
experiments	O
.	O
Positive	O
reviews	O
can	O
include	O
negative	O
sentences	O
and	O
vice	O
versa	O
.	O
This	O
means	O
that	O
using	O
document	O
-	O
level	O
labels	O
would	O
introduce	O
noise	O
and	O
might	O
impair	O
the	O
baselines	O
.	O
During	O
each	O
of	O
the	O
three	O
experimental	O
runs	O
,	O
all	O
models	O
and	O
baselines	O
are	O
annotated	O
simultaneously	O
by	O
the	O
same	O
person	O
.	O
The	O
annotator	O
is	O
presented	O
with	O
one	O
instance	O
at	O
a	O
time	O
and	O
has	O
no	O
information	O
which	O
of	O
the	O
models	O
has	O
produced	O
each	O
particular	O
instance	O
.	O
Once	O
a	O
label	O
is	O
selected	O
,	O
it	O
is	O
transmitted	O
to	O
the	O
corresponding	O
model	O
and	O
triggers	O
the	O
selection	O
/	O
generation	O
of	O
the	O
next	O
instance	O
.	O
Thus	O
,	O
at	O
any	O
given	O
time	O
there	O
is	O
one	O
unlabeled	O
instance	O
for	O
each	O
model	O
or	O
baseline	O
.	O
From	O
this	O
set	O
of	O
unlabeled	O
instances	O
,	O
one	O
instance	O
is	O
chosen	O
randomly	O
and	O
presented	O
to	O
the	O
annotator	O
.	O
This	O
procedure	O
is	O
repeated	O
until	O
500	O
instances	O
are	O
labeled	O
for	O
each	O
model	O
or	O
baseline	O
.	O
Hiding	O
the	O
instance	O
source	O
from	O
the	O
annotator	O
is	O
intended	O
to	O
prevent	O
any	O
bias	O
during	O
the	O
annotation	O
process	O
.	O
6	O
Results	O
and	O
Analysis	O
6.1	O
Classification	B-TaskName
Performance	O
F	O
-	O
scores	O
as	O
a	O
function	O
of	O
annotated	O
instances	O
Figure	O
3	O
shows	O
learning	O
curves	O
for	O
the	O
different	O
AL	O
strategies	O
and	O
baselines	O
as	O
a	O
function	O
of	O
the	O
number	O
of	O
annotation	O
instances	O
added	O
to	O
the	O
training	O
data	O
.	O
The	O
random	O
and	O
least	O
conf	O
baselines	O
perform	O
reasonably	O
well	O
.	O
Least	O
conf	O
struggles	O
in	O
the	O
beginning	O
,	O
likely	O
attributed	O
to	O
the	O
minimal	O
seed	O
set	O
.	O
Once	O
enough	O
instances	O
are	O
labeled	O
it	O
catches	O
up	O
.	O
Gen	O
uniform	O
has	O
a	O
strong	O
start	O
but	O
,	O
after	O
around	O
200	O
instances	O
,	O
is	O
outperformed	O
by	O
the	O
nearest	O
neighbor	O
approaches	O
which	O
yield	O
the	O
highest	O
F1	B-MetricName
-	O
scores	O
.	O
Among	O
the	O
nearest	O
neighbor	O
approaches	O
,	O
the	O
uniform	O
schedule	O
ranks	O
better	O
than	O
wang	O
.	O
The	O
same	O
behaviour	O
is	O
observed	O
for	O
the	O
generation	O
methods	O
,	O
although	O
gen	O
wang	O
produces	O
the	O
worst	O
results	O
overall	O
.	O
Overall	O
,	O
gen	O
uniform	O
is	O
competitive	O
with	O
respect	O
to	O
F1	B-MetricName
-	O
scores	O
and	O
shows	O
that	O
sentences	O
generated	O
from	O
points	O
in	O
the	O
feature	O
space	O
are	O
informative	O
and	O
useful	O
for	O
training	O
a	O
text	O
classifier	O
.	O
F	O
-	O
scores	O
as	O
a	O
function	O
of	O
annotation	O
time	O
AL	O
simulations	O
have	O
often	O
been	O
criticized	O
for	O
reporting	O
unrealistic	O
results	O
,	O
based	O
merely	O
on	O
the	O
number	O
of	O
annotated	O
instances	O
(	O
see	O
,	O
e.g.	O
,	O
Settles	O
(	O
2009	O
)	O
,	O
pp	O
.	O
37	O
ff	O
.	O
)	O
.	O
It	O
is	O
well	O
known	O
,	O
however	O
,	O
that	O
the	O
number	O
of	O
annotated	O
instances	O
is	O
often	O
not	O
a	O
good	O
predictor	O
for	O
the	O
real	O
annotation	O
costs	O
.	O
AL	O
strategies	O
tend	O
to	O
select	O
the	O
hard	O
nuts	O
for	O
human	O
annotators	O
and	O
it	O
is	O
not	O
unreasonable	O
to	O
assume	O
that	O
the	O
annotation	O
of	O
N	O
instances	O
in	O
an	O
AL	O
setup	O
might	O
take	O
longer	O
and	O
thus	O
might	O
be	O
more	O
expensive	O
than	O
annotating	O
the	O
same	O
number	O
of	O
randomly	O
selected	O
instances	O
.	O
Therefore	O
,	O
we	O
also	O
show	O
learning	O
curves	O
as	O
a	O
function	O
of	O
annotation	O
time	O
(	O
Figure	O
4	O
)	O
.	O
The	O
results	O
show	O
a	O
clear	O
advantage	O
for	O
the	O
generation	O
models	O
.	O
The	O
reduction	O
in	O
annotation	O
time	O
is	O
due	O
to	O
shorter	O
query	O
length	O
and	O
less	O
neutral	O
or	O
noisy	O
instances	O
,	O
as	O
shown	O
in	O
Table	O
2	O
.	O
This	O
speeds	O
up	O
the	O
annotation	O
by	O
a	O
significant	O
margin	O
while	O
providing	O
the	O
Learner	O
with	O
informative	O
instances	O
,	O
despite	O
their	O
short	O
length	O
.	O
Figure	O
5	O
shows	O
that	O
the	O
length	O
of	O
generated	O
instances	O
increase	O
over	O
time	O
and	O
further	O
exploration	O
also	O
hints	O
that	O
the	O
generated	O
length	O
is	O
correlated	O
with	O
the	O
length	O
of	O
the	O
sentences	O
in	O
the	O
seed	O
set	O
.	O
As	O
listed	O
in	O
Table	O
2	O
,	O
the	O
random	O
baseline	O
reveals	O
that	O
36.8	O
percent	O
of	O
sentences	O
in	O
the	O
pool	O
are	O
neutral	O
/	O
artifacts	O
and	O
positive	O
sentences	O
outweigh	O
negative	O
ones	O
by	O
a	O
factor	O
of	O
2.6	O
.	O
This	O
means	O
that	O
random	O
sampling	O
results	O
in	O
unbalanced	O
datasets	O
with	O
far	O
more	O
positive	O
examples	O
.	O
Our	O
generation	O
method	O
does	O
not	O
show	O
this	O
disadvantage	O
.	O
In	O
contrast	O
,	O
the	O
generated	O
instances	O
maintain	O
a	O
more	O
balanced	O
distribution	O
of	O
class	O
labels	O
and	O
are	O
less	O
likely	O
to	O
be	O
skipped	O
.	O
These	O
are	O
indicators	O
that	O
the	O
selected	O
points	O
are	O
close	O
to	O
the	O
hyperplane	O
and	O
the	O
VAE	B-MethodName
is	O
able	O
to	O
generate	O
coherent	O
and	O
highly	O
informative	O
sentences	O
from	O
them	O
.	O

NER	B-TaskName
(	O
Sundheim	O
,	O
1995	O
)	O
is	O
a	O
fundamental	O
task	O
in	O
natural	O
language	O
processing	O
.	O
The	O
task	O
has	O
a	O
lot	O
of	O
applications	O
in	O
various	O
domains	O
such	O
as	O
social	O
media	O
(	O
Derczynski	B-DatasetName
et	O
al	O
,	O
2017	O
)	O
,	O
news	O
(	O
Tjong	O
Kim	O
Sang	O
,	O
2002	O
;	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
,	O
Ecommerce	O
(	O
Fetahu	O
et	O
al	O
,	O
2021	O
;	O
Wang	O
et	O
al	O
,	O
2021b	O
)	O
,	O
and	O
medical	O
domains	O
(	O
Dogan	O
et	O
al	O
,	O
2014	O
;	O
Li	O
et	O
al	O
,	O
2016	O
)	O
.	O
Recently	O
,	O
pretrained	O
contextual	O
embeddings	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
XLM	B-MethodName
-	O
R	O
and	O
LUKE	O
(	O
Yamada	O
et	O
al	O
,	O
2020	O
)	O
have	O
significantly	O
improved	O
the	O
NER	B-TaskName
performance	O
.	O
The	O
embeddings	O
are	O
trained	O
on	O
large	O
-	O
scale	O
unlabeled	O
data	O
such	O
as	O
Wikipedia	O
,	O
which	O
can	O
significantly	O
improve	O
the	O
contextual	O
representations	O
of	O
named	O
entities	O
.	O
Recent	O
efforts	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Akbik	O
et	O
al	O
,	O
2018	O
;	O
Straková	O
et	O
al	O
,	O
2019	O
)	O
concatenate	O
different	O
kinds	O
of	O
pretrained	O
embeddings	O
to	O
form	O
stronger	O
token	O
representations	O
.	O
Moreover	O
,	O
the	O
embeddings	O
are	O
trained	O
over	O
long	O
documents	O
,	O
which	O
allows	O
the	O
model	O
to	O
easily	O
model	O
long	O
-	O
range	O
dependencies	O
to	O
disambiguate	O
complex	O
named	O
entities	O
in	O
the	O
sentence	O
.	O
Recently	O
,	O
a	O
lot	O
of	O
work	O
shows	O
that	O
utilizing	O
the	O
document	O
-	O
level	O
contexts	O
in	O
the	O
CoNLL	O
NER	B-TaskName
datasets	O
can	O
significantly	O
improve	O
token	O
representations	O
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
(	O
Yu	O
et	O
al	O
,	O
2020	O
;	O
Luoma	O
and	O
Pyysalo	O
,	O
2020	O
;	O
Yamada	O
et	O
al	O
,	O
2020	O
;	O
Wang	O
et	O
al	O
,	O
2021a	O
)	O
.	O
However	O
,	O
the	O
lack	O
of	O
context	O
in	O
the	O
MultiCoNER	B-DatasetName
datasets	O
means	O
the	O
embeddings	O
can	O
not	O
take	O
advantage	O
of	O
long	O
-	O
range	O
dependencies	O
for	O
entity	B-TaskName
disambiguation	I-TaskName
.	O
Recently	O
,	O
Wang	O
et	O
al	O
(	O
2021b	O
)	O
use	O
Google	B-DatasetName
search	O
to	O
retrieve	O
external	O
contexts	O
of	O
the	O
input	O
sentence	O
and	O
successfully	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
across	O
multiple	O
domains	O
.	O
We	O
adopt	O
this	O
idea	O
so	O
that	O
the	O
embeddings	O
can	O
utilize	O
the	O
related	O
knowledge	O
by	O
taking	O
the	O
advantage	O
of	O
long	O
-	O
range	O
dependencies	O
to	O
form	O
stronger	O
token	O
representations	O
.	O
Comparing	O
with	O
Wang	O
et	O
al	O
(	O
2021b	O
)	O
,	O
we	O
build	O
the	O
local	O
KB	O
based	O
on	O
Wikipedia	O
because	O
the	O
KB	O
matches	O
the	O
indomain	O
data	O
of	O
the	O
shared	O
task	O
and	O
is	O
fast	O
enough	O
to	O
meet	O
the	O
time	O
requirement	O
in	O
the	O
test	O
phase	O
2	O
.	O
Fine	O
-	O
tuning	O
pretrained	O
contextual	O
embeddings	O
is	O
a	O
useful	O
and	O
effective	O
approach	O
to	O
many	O
NLP	O
tasks	O
.	O
Recently	O
,	O
some	O
of	O
the	O
research	O
efforts	O
propose	O
to	O
further	O
train	O
the	O
fine	O
-	O
tuned	O
embeddings	O
with	O
specific	O
training	O
data	O
or	O
in	O
a	O
larger	O
model	O
architecture	O
to	O
improve	O
model	O
performance	O
.	O
Shi	O
and	O
Lee	O
(	O
2021	O
)	O
proposed	O
two	O
-	O
stage	O
fine	O
-	O
tuning	O
,	O
which	O
first	O
trains	O
a	O
general	O
multilingual	O
Enhanced	O
Universal	O
Dependency	O
(	O
Bouma	O
et	O
al	O
,	O
2021	O
)	O
parser	O
and	O
then	O
finetunes	O
on	O
each	O
specific	O
language	O
separately	O
.	O
Wang	O
et	O
al	O
(	O
2021a	O
)	O
proposed	O
to	O
train	O
models	O
through	O
concatenating	O
fine	O
-	O
tuned	O
embeddings	O
.	O
We	O
extend	O
these	O
ideas	O
as	O
multi	O
-	O
stage	O
fine	O
-	O
tuning	O
,	O
which	O
improves	O
the	O
accuracy	B-MetricName
of	O
monolingual	O
models	O
that	O
use	O
finetuned	O
multilingual	O
embeddings	O
as	O
initialization	O
in	O
training	O
.	O
Moreover	O
,	O
multi	O
-	O
stage	O
fine	O
-	O
tuning	O
can	O
accelerate	O
the	O
training	O
process	O
in	O
system	O
building	O
.	O

In	O
our	O
system	O
,	O
we	O
use	O
XLM	B-MethodName
-	O
R	O
large	O
as	O
the	O
embedding	O
for	O
all	O
the	O
tracks	O
.	O
It	O
is	O
a	O
multilingual	O
model	O
and	O
is	O
applicable	O
to	O
all	O
tracks	O
.	O
Given	O
the	O
input	O
sentence	O
x	O
and	O
the	O
retrieved	O
contexts	O
{	O
x	O
1	O
,	O
,	O
x	O
k	O
}	O
,	O
we	O
add	O
the	O
separator	O
token	O
(	O
i.e.	O
,	O
"	O
<	O
/s	O
>	O
"	O
in	O
XLM	B-MethodName
-	O
R	O
)	O
between	O
them	O
and	O
concatenated	O
them	O
together	O
to	O
form	O
the	O
inputx	O
of	O
the	O
NER	B-TaskName
module	O
.	O
We	O
chunk	O
retrieved	O
texts	O
to	O
avoid	O
the	O
amount	O
of	O
subtoken	O
in	O
the	O
sequence	O
exceeding	O
the	O
maximum	O
subtoken	O
length	O
in	O
XLM	B-MethodName
-	O
R	O
(	O
i.e.	O
,	O
512	O
in	O
XLM	B-MethodName
-	O
R	O
)	O
.	O
Our	O
system	O
regards	O
the	O
NER	B-TaskName
task	O
as	O
a	O
sequence	O
labeling	O
problem	O
.	O
The	O
embedding	O
layer	O
in	O
the	O
NER	B-TaskName
module	O
encode	O
the	O
concatenated	O
sequencẽ	O
x	O
and	O
output	O
the	O
corresponding	O
token	O
representa	O
-	O
tions	O
{	O
v	O
1	O
,	O
,	O
v	O
n	O
,	O
}	O
.	O
The	O
module	O
then	O
feeds	O
the	O
token	O
representations	O
{	O
v	O
1	O
,	O
,	O
v	O
n	O
}	O
of	O
the	O
input	O
sentence	O
into	O
a	O
linear	O
-	O
chain	O
CRF	B-MethodName
layer	O
to	O
obtain	O
the	O
conditional	O
probability	O
p	O
θ	B-HyperparameterName
(	O
y	O
|	O
x	O
)	O
:	O
ψ	O
(	O
y	O
′	O
,	O
y	O
,	O
v	O
i	O
)	O
=	O
exp	O
(	O
W	O
T	O
y	O
v	O
i	O
+	O
b	O
y	O
′	O
,	O
y	O
)	O
(	O
1	O
)	O
p	O
θ	B-HyperparameterName
(	O
y	O
|	O
x	O
)	O
=	O
n	O
i=1	O
ψ	O
(	O
y	O
i−1	O
,	O
y	O
i	O
,	O
v	O
i	O
)	O
y	O
′	O
Y	O
(	O
x	O
)	O
n	O
i=1	O
ψ	O
(	O
y	O
′	O
i−1	O
,	O
y	O
′	O
i	O
,	O
v	O
i	O
)	O
where	O
θ	B-HyperparameterName
represents	O
the	O
model	O
parameters	O
and	O
Y	O
(	O
x	O
)	O
denotes	O
the	O
set	O
of	O
all	O
possible	O
label	O
sequences	O
given	O
x.	O
In	O
the	O
potential	O
function	O
ψ	O
(	O
y	O
′	O
,	O
y	O
,	O
v	O
i	O
)	O
,	O
W	O
T	O
y	O
v	O
i	O
is	O
the	O
emission	O
score	O
and	O
b	O
y	O
′	O
,	O
y	O
is	O
the	O
transition	O
score	O
,	O
where	O
W	O
T	O
R	O
t×d	O
and	O
b	O
R	O
t×t	O
are	O
parameters	O
and	O
the	O
subscripts	O
y	O
′	O
and	O
y	O
are	O
the	O
indices	O
of	O
the	O
matrices	O
.	O
During	O
training	O
,	O
the	O
negative	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
loss	B-MetricName
L	O
NLL	B-MetricName
(	O
θ	B-HyperparameterName
)	O
=	O
−	O
log	O
p	O
θ	B-HyperparameterName
(	O
y	O
*	O
|	O
x	O
)	O
for	O
the	O
concatenated	O
input	O
sequence	O
with	O
gold	O
labels	O
y	O
*	O
is	O
used	O
.	O
During	O
inference	O
,	O
the	O
model	O
predictionŷ	O
θ	B-HyperparameterName
is	O
given	O
by	O
Viterbi	O
decoding	O
.	O

Given	O
predictions	O
{	O
ŷ	O
θ	B-HyperparameterName
1	O
,	O
,	O
ŷ	O
θm	O
}	O
from	O
m	O
models	O
with	O
different	O
random	O
seeds	B-DatasetName
,	O
we	O
use	O
majority	O
voting	O
to	O
generate	O
the	O
final	O
predictionŷ	O
.	O
We	O
convert	O
the	O
label	O
sequences	O
into	O
entity	O
spans	O
to	O
perform	O
majority	O
voting	O
.	O
Following	O
Yamada	O
et	O
al	O
(	O
2020	O
)	O
,	O
the	O
module	O
ranks	O
all	O
spans	O
in	O
the	O
predictions	O
by	O
the	O
number	O
of	O
votes	O
in	O
descending	O
order	O
and	O
selects	O
the	O
spans	O
with	O
more	O
than	O
50	O
%	O
votes	O
into	O
the	O
final	O
prediction	O
.	O
The	O
spans	O
with	O
more	O
votes	O
are	O
kept	O
if	O
the	O
selected	O
spans	O
have	O
overlaps	O
and	O
the	O
longer	O
spans	O
are	O
kept	O
if	O
the	O
spans	O
have	O
the	O
same	O
votes	O
.	O
(	O
Nguyen	O
et	O
al	O
,	O
2016	O
)	O
containing	O
a	O
lot	O
of	O
natural	O
language	O
questions	O
;	O
ORCAS	B-DatasetName
(	O
Search	O
Query	O
NER	B-TaskName
)	O
contains	O
user	O
queries	O
from	O
Microsoft	O
Bing	O
(	O
Craswell	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
MSQ	O
and	O
ORCAS	B-DatasetName
samples	O
are	O
taken	O
as	O
out	O
-	O
ofdomain	O
data	O
in	O
the	O
shared	O
task	O
.	O
The	O
training	O
and	O
development	O
sets	O
only	O
contain	O
a	O
small	O
collection	O
of	O
samples	O
of	O
these	O
two	O
domains	O
and	O
mainly	O
contain	O
data	O
from	O
the	O
LOWNER	O
domain	O
.	O
The	O
test	O
set	O
,	O
however	O
,	O
contains	O
much	O
more	O
MSQ	O
and	O
ORCAS	B-DatasetName
samples	O
to	O
assess	O
the	O
out	O
-	O
of	O
-	O
domain	O
performance	O
.	O
The	O
results	O
of	O
the	O
shared	O
task	O
are	O
evaluated	O
with	O
the	O
entity	O
-	O
level	O
macro	B-MetricName
F1	I-MetricName
scores	O
,	O
which	O
treat	O
all	O
the	O
labels	O
equally	O
.	O
In	O
comparison	O
,	O
most	O
of	O
the	O
publicly	O
available	O
NER	B-TaskName
datasets	O
(	O
e.g.	O
,	O
CoNLL	O
2002CoNLL	O
,	O
2003	O
are	O
evaluated	O
with	O
the	O
entity	O
-	O
level	O
micro	B-MetricName
F1	I-MetricName
scores	O
,	O
which	O
emphasize	O
common	O
labels	O
.	O

NER	B-TaskName
Model	O
Training	O
Before	O
building	O
the	O
final	O
system	O
,	O
we	O
compare	O
a	O
lot	O
of	O
variants	O
of	O
the	O
system	O
.	O
We	O
train	O
these	O
variant	O
models	O
on	O
the	O
training	O
set	O
for	O
3	O
times	O
each	O
with	O
different	O
random	O
seeds	B-DatasetName
and	O
compare	O
the	O
averaged	O
performance	O
of	O
the	O
models	O
.	O
According	O
to	O
the	O
dataset	O
sizes	O
,	O
we	O
train	O
the	O
models	O
for	O
5	O
epochs	O
,	O
10	O
epochs	O
and	O
100	O
epochs	O
for	O
multilingual	O
,	O
monolingual	O
and	O
code	O
-	O
mixed	O
models	O
respectively	O
.	O
Our	O
final	O
NER	B-TaskName
models	O
are	O
trained	O
on	O
the	O
combined	O
dataset	O
including	O
both	O
the	O
training	O
and	O
development	O
sets	O
on	O
each	O
track	O
to	O
fully	O
utilize	O
the	O
labeled	O
data	O
.	O
For	O
models	O
trained	O
on	O
the	O
training	O
set	O
,	O
we	O
use	O
the	O
best	O
macro	B-MetricName
F1	I-MetricName
on	O
the	O
development	O
set	O
during	O
training	O
to	O
select	O
the	O
best	O
model	O
checkpoint	O
.	O
For	O
models	O
trained	O
on	O
the	O
combined	O
dataset	O
,	O
Continue	O
Pretraining	O
To	O
make	O
XLM	B-MethodName
-	O
R	O
learn	O
the	O
data	O
distribution	O
of	O
the	O
shared	O
task	O
,	O
we	O
combine	O
the	O
training	O
and	O
development	O
sets	O
on	O
the	O
monolingual	O
tracks	O
to	O
build	O
a	O
corpus	O
to	O
continue	O
pretrain	O
XLM	B-MethodName
-	O
R.	O
Specifically	O
,	O
we	O
collocate	O
all	O
sentences	O
according	O
to	O
their	O
languages	O
,	O
then	O
cut	O
the	O
text	O
into	O
chunks	O
of	O
fixed	O
length	O
,	O
and	O
train	O
the	O
model	O
on	O
these	O
text	O
chunks	O
using	O
the	O
Masked	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
objective	O
.	O
We	O
continue	O
pretrain	O
XLM	B-MethodName
-	O
R	O
for	O
5	O
epochs	O
.	O
We	O
use	O
the	O
continue	O
pretrained	O
XLM	B-MethodName
-	O
R	O
model	O
as	O
the	O
initialization	O
of	O
the	O
multilingual	O
7	O
Please	O
refer	O
to	O
Appendix	O
A	O
for	O
detailed	O
settings	O
.	O
models	O
during	O
training	O
.	O

There	O
are	O
55	O
teams	O
that	O
participated	O
in	O
the	O
shared	O
task	O
.	O
Due	O
to	O
limited	O
space	O
,	O
we	O
only	O
compare	O
our	O
system	O
with	O
the	O
systems	O
from	O
teams	O
USTC	O
-	O
NELSLIP	O
,	O
RACAI	O
and	O
Sliced	O
10	O
.	O
In	O
the	O
postevaluation	O
phase	O
,	O
we	O
evaluate	O
a	O
baseline	O
system	O
without	O
using	O
the	O
knowledge	O
retrieval	O
module	O
to	O
further	O
show	O
the	O
effectiveness	O
of	O
our	O
knowledgebased	O
system	O
.	O
The	O
official	O
results	O
and	O
the	O
results	O
of	O
our	O
baseline	O
system	O
are	O
shown	O
in	O
Table	O
1	O
.	O
Our	O
system	O
performs	O
the	O
best	O
on	O
10	O
out	O
of	O
13	O
tracks	O
and	O
is	O
competitive	O
on	O
the	O
other	O
3	O
tracks	O
.	O
Moreover	O
,	O
our	O
system	O
outperforms	O
our	O
baseline	O
by	O
14.39	O
F1	B-MetricName
on	O
average	O
,	O
which	O
shows	O
the	O
knowledge	O
retrieval	O
module	O
is	O
extremely	O
helpful	O
for	O
disambiguating	O
complex	O
entities	O
leading	O
to	O
significant	O
improvement	O
on	O
model	O
performance	O
.	O

To	O
further	O
show	O
the	O
effectiveness	O
of	O
our	O
knowledgebased	O
system	O
,	O
we	O
show	O
the	O
relative	O
improvements	O
of	O
our	O
system	O
over	O
our	O
baseline	O
system	O
on	O
each	O
domain	O
in	O
Table	O
2	O
.	O
We	O
observe	O
that	O
in	O
most	O
of	O
the	O
cases	O
,	O
the	O
two	O
out	O
-	O
of	O
-	O
domain	O
test	O
sets	O
have	O
more	O
relative	O
improvements	O
than	O
the	O
in	O
-	O
domain	O
test	O
set	O
.	O
This	O
observation	O
shows	O
that	O
the	O
knowledge	O
from	O
Wikipedia	O
can	O
not	O
only	O
improve	O
the	O
performance	O
of	O
the	O
LOWNER	O
domain	O
which	O
is	O
the	O
same	O
domain	O
as	O
the	O
KB	O
,	O
but	O
also	O
has	O
very	O
strong	O
cross	O
-	O
domain	O
Table	O
2	O
:	O
Per	O
-	O
domain	O
macro	B-MetricName
F1	I-MetricName
score	O
on	O
the	O
test	O
set	O
of	O
our	O
system	O
and	O
our	O
baseline	O
system	O
for	O
each	O
language	O
.	O
∆	O
represents	O
the	O
relative	O
improvements	O
of	O
our	O
system	O
over	O
the	O
baseline	O
system	O
.	O
transferability	O
to	O
other	O
domains	O
such	O
as	O
web	O
questions	O
and	O
user	O
queries	O
.	O
According	O
to	O
the	O
baseline	O
performance	O
over	O
the	O
three	O
domains	O
,	O
the	O
ORCAS	B-DatasetName
domain	O
has	O
the	O
lowest	O
score	O
,	O
which	O
shows	O
the	O
challenges	O
in	O
recognizing	O
named	O
entities	O
in	O
user	O
queries	O
.	O
However	O
,	O
our	O
retrieved	O
documents	O
in	O
KB	O
can	O
significantly	O
ease	O
the	O
challenges	O
in	O
this	O
domain	O
and	O
results	O
in	O
the	O
highest	O
improvement	O
out	O
of	O
the	O
three	O
domains	O
.	O

To	O
evaluate	O
the	O
relevance	O
of	O
the	O
retrieval	O
results	O
to	O
the	O
query	O
,	O
we	O
define	O
a	O
character	O
-	O
level	O
relevance	O
metric	O
,	O
which	O
calculates	O
the	O
Intersectionover	O
-	O
Union	O
(	O
IoU	B-MetricName
)	O
between	O
the	O
characters	O
of	O
query	O
and	O
result	O
.	O
Assuming	O
that	O
the	O
character	O
sets	O
11	O
of	O
query	O
and	O
retrieval	O
result	O
are	O
A	O
and	O
B	O
respectively	O
,	O
then	O
the	O
character	O
-	O
level	O
IoU	B-MetricName
is	O
A∩B	O
A∪B	O
.	O
We	O
calculate	O
the	O
character	O
-	O
level	O
IoU	B-MetricName
of	O
the	O
sentence	O
and	O
its	O
top	O
-	O
1	O
retrieval	O
result	O
on	O
all	O
tracks	O
,	O
and	O
plot	O
its	O
distribution	O
on	O
the	O
training	O
,	O
development	O
and	O
test	O
set	O
in	O
Figure	O
3	O
.	O
We	O
have	O
the	O
following	O
observations	O
:	O
1	O
)	O
the	O
IoU	B-MetricName
values	O
are	O
concentrated	O
around	O
1.0	O
on	O
the	O
training	O
and	O
development	O
sets	O
of	O
EN	O
,	O
ES	O
,	O
NL	O
,	O
RU	O
,	O
TR	O
,	O
KO	O
,	O
FA	B-MethodName
,	O
which	O
indicates	O
that	O
most	O
of	O
the	O
samples	O
were	O
derived	O
from	O
Wikipedia	O
.	O
Therefore	O
,	O
by	O
retrieving	O
,	O
we	O
can	O
obtain	O
the	O
original	O
documents	O
for	O
these	O
samples	O
.	O
2	O
)	O
the	O
distribution	O
of	O
data	O
on	O
the	O
test	O
set	O
is	O
consistent	O
with	O
the	O
training	O
and	O
development	O
sets	O
for	O
most	O
languages	O
,	O
except	O
for	O
TR	O
.	O
On	O
TR	O
,	O
the	O
character	O
-	O
level	O
IoU	B-MetricName
values	O
of	O
the	O
samples	O
and	O
query	O
results	O
cluster	O
at	O
around	O
0.5	O
.	O
We	O
hypothesize	O
that	O
this	O
is	O
because	O
the	O
source	O
of	O
the	O
test	O
set	O
for	O
TR	O
is	O
different	O
from	O
the	O
training	O
set	O
.	O
However	O
,	O
the	O
model	O
still	O
performs	O
strongly	O
on	O
this	O
language	O
,	O
suggesting	O
that	O
the	O
model	O
can	O
mitigate	O
the	O
difficulties	O
caused	O
11	O
The	O
sets	O
take	O
repeat	O
characters	O
as	O
different	O
characters	O
.	O
by	O
inconsistent	O
data	O
distribution	O
by	O
retrieving	O
the	O
context	O
from	O
Wikipedia	O
.	O

As	O
we	O
mentioned	O
in	O
Section	O
3.1	O
,	O
there	O
are	O
three	O
context	O
processing	O
options	O
,	O
which	O
are	O
:	O
1	O
)	O
use	O
the	O
matched	O
paragraph	O
;	O
2	O
)	O
use	O
the	O
matched	O
sentence	O
;	O
3	O
)	O
use	O
the	O
matched	O
sentence	O
but	O
remove	O
the	O
wiki	O
anchors	O
.	O
We	O
denote	O
the	O
three	O
options	O
as	O
PARA	O
,	O
SENT	O
and	O
SENT	O
-	O
LINK	O
respectively	O
.	O
Entity	B-TaskName
Retrieval	I-TaskName
with	O
Gold	O
Entities	O
We	O
use	O
gold	O
entities	O
on	O
the	O
development	O
set	O
to	O
see	O
whether	O
the	O
model	O
performance	O
can	O
be	O
improved	O
.	O
This	O
can	O
be	O
seen	O
as	O
the	O
most	O
ideal	O
scenario	O
for	O
iterative	O
retrieval	O
.	O
We	O
denote	O
this	O
process	O
as	O
ITER	O
G	O
and	O
use	O
PARA	O
for	O
the	O
context	O
type	O
.	O
In	O
Table	O
3	O
,	O
we	O
can	O
observe	O
that	O
:	O
1	O
)	O
For	O
the	O
three	O
context	O
options	O
,	O
PARA	O
is	O
the	O
best	O
option	O
for	O
EN	O
,	O
ES	O
,	O
NL	O
,	O
RU	O
,	O
TR	O
,	O
KO	O
,	O
FA	B-MethodName
,	O
MIX	O
and	O
MULTI	O
.	O
SENT	O
-	O
LINK	O
is	O
the	O
best	O
option	O
for	O
HI	O
and	O
BN	O
.	O
For	O
DE	O
and	O
ZH	O
,	O
SENT	O
and	O
SENT	O
-	O
LINK	O
are	O
competitive	O
.	O
As	O
a	O
result	O
,	O
we	O
choose	O
SENT	O
for	O
the	O
two	O
languages	O
since	O
we	O
believe	O
the	O
wiki	O
anchors	O
from	O
the	O
Wikipedia	O
can	O
help	O
model	O
performance	O
;	O
2	O
)	O
Comparing	O
with	O
the	O
baseline	O
,	O
the	O
knowledge	O
from	O
Google	B-DatasetName
Search	O
can	O
improve	O
model	O
performance	O
.	O
Based	O
on	O
the	O
best	O
context	O
option	O
of	O
each	O
track	O
,	O
the	O
knowledge	O
from	O
Wikipedia	O
is	O
better	O
than	O
the	O
online	O
search	O
engine	O
;	O
3	O
)	O
For	O
ITER	O
G	O
,	O
we	O
can	O
find	O
that	O
the	O
context	O
can	O
further	O
Iterative	O
Entity	B-TaskName
Retrieval	I-TaskName
with	O
Predicted	O
Entities	O
Based	O
on	O
the	O
results	O
in	O
Table	O
3	O
,	O
we	O
further	O
analyze	O
how	O
the	O
predicted	O
entity	O
mentions	O
can	O
improve	O
the	O
retrieval	O
quality	O
.	O
We	O
denote	O
the	O
iterative	O
entity	B-TaskName
retrieval	I-TaskName
with	O
predicted	O
mentions	O
as	O
ITER	O
P	O
.	O
In	O
the	O
experiment	O
,	O
we	O
set	O
T	O
=	O
2	O
.	O
12	O
We	O
extract	O
the	O
predicted	O
mentions	O
of	O
the	O
development	O
sets	O
from	O
the	O
models	O
based	O
on	O
the	O
best	O
context	O
option	O
for	O
each	O
track	O
.	O
We	O
conduct	O
the	O
experiments	O
over	O
HI	O
,	O
BN	O
and	O
MIX	O
which	O
have	O
significant	O
improvement	O
with	O
ITER	O
G	O
.	O
In	O
Table	O
4	O
,	O
we	O
also	O
list	O
the	O
performance	O
of	O
ITER	O
G	O
for	O
reference	O
,	O
which	O
can	O
be	O
seen	O
as	O
using	O
the	O
predicted	O
mentions	O
with	O
100	O
%	O
accuracy	B-MetricName
.	O
From	O
the	O
results	O
,	O
we	O
observe	O
that	O
only	O
MIX	O
can	O
be	O
improved	O
.	O
Since	O
iterative	O
entity	B-TaskName
retrieval	I-TaskName
uses	O
predicted	O
mentions	O
as	O
a	O
part	O
of	O
retrieval	O
query	O
,	O
the	O
performance	O
of	O
mention	O
detection	O
directly	O
affects	O
the	O
retrieval	O
quality	O
.	O
To	O
further	O
analyze	O
the	O
observation	O
in	O
Table	O
4	O
,	O
we	O
evaluate	O
the	O
mention	O
F1	B-MetricName
score	I-MetricName
of	O
the	O
NER	B-TaskName
models	O
with	O
sentence	O
retrieval	O
.	O
For	O
comparison	O
with	O
mention	O
detection	O
performance	O
of	O
NER	B-TaskName
models	O
,	O
we	O
additionally	O
train	O
mention	O
detection	O
models	O
by	O
discarding	O
the	O
entity	O
labels	O
during	O
training	O
.	O
From	O
the	O
results	O
in	O
Table	O
5	O
,	O
we	O
suspect	O
the	O
low	O
mention	O
F1	B-MetricName
introduces	O
noises	O
in	O
the	O
knowledge	O
retrieval	O
module	O
for	O
BN	O
and	O
HI	O
,	O
which	O
lead	O
to	O
the	O
decline	O
of	O
performance	O
as	O
shown	O
in	O
Table	O
4	O
.	O
Moreover	O
,	O
the	O
mention	O
F1	B-MetricName
of	O
mention	O
detection	O
models	O
(	O
second	O
row	O
of	O
Table	O
5	O
)	O
only	O
outperform	O
that	O
of	O
the	O
NER	B-TaskName
models	O
(	O
first	O
row	O
of	O
Table	O
5	O
)	O
in	O
a	O
moderate	O
scale	O
.	O
Therefore	O
,	O
we	O
train	O
the	O
ITER	O
models	O
only	O
for	O
the	O
code	O
-	O
mixed	O
track	O
and	O
use	O
the	O
NER	B-TaskName
models	O
with	O
sentence	O
retrieval	O
to	O
predict	O
mentions	O
.	O

For	O
the	O
knowledge	O
retrieval	O
module	O
,	O
we	O
retrieve	O
top	O
-	O
10	O
related	O
results	O
from	O
the	O
KB	O
.	O
For	O
iterative	O
entity	B-TaskName
retrieval	I-TaskName
,	O
we	O
set	O
T	O
=	O
2	O
.	O
In	O
masked	O
language	O
model	O
pretraining	O
,	O
we	O
use	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5	O
×	O
10	O
−5	O
.	O
For	O
the	O
NER	B-TaskName
module	O
,	O
we	O
use	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5	O
×	O
10	O
−6	O
for	O
fine	O
-	O
tuning	O
the	O
XLM	B-MethodName
-	O
R	O
embeddings	O
and	O
use	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.05	O
to	O
update	O
the	O
parameters	O
in	O
the	O
CRF	B-MethodName
layer	O
following	O
Wang	O
et	O
al	O
(	O
2021b	O
)	O
.	O
Each	O
NER	B-TaskName
model	O
built	O
by	O
our	O
system	O
can	O
be	O
trained	O
and	O
evaluated	O
on	O
a	O
single	O
Tesla	O
V100	O
GPU	O
with	O
16	O
GB	O
memory	O
.	O
For	O
the	O
ensemble	O
module	O
,	O
we	O
train	O
about	O
10	O
models	O
for	O
each	O
track	O
.	O
A.	O
(	O
Akbik	O
et	O
al	O
,	O
2018	O
)	O
,	O
ELMo	B-MethodName
embeddings	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Che	O
et	O
al	O
,	O
2018	O
)	O
,	O
XLM	B-MethodName
-	O
R	O
embeddings	O
fine	O
-	O
tuned	O
on	O
the	O
whole	O
training	O
data	O
and	O
XLM	B-MethodName
-	O
R	O
embeddings	O
fine	O
-	O
tuned	O
on	O
the	O
language	O
data	O
by	O
multi	O
-	O
stage	O
finetuning	O
.	O
We	O
only	O
feed	O
the	O
knowledge	O
-	O
based	O
input	O
into	O
XLM	B-MethodName
-	O
R	O
embeddings	O
and	O
feed	O
the	O
original	O
input	O
into	O
other	O
embeddings	O
because	O
it	O
is	O
hard	O
for	O
the	O
other	O
embeddings	O
(	O
especially	O
for	O
LSTM	B-MethodName
-	O
based	O
embeddings	O
such	O
as	O
Flair	O
and	O
ELMo	B-MethodName
)	O
to	O
encode	O
such	O
a	O
long	O
input	O
.	O
We	O
use	O
Bi	O
-	O
LSTM	B-MethodName
encoder	O
to	O
encode	O
the	O
concatenated	O
embeddings	O
with	O
a	O
hidden	O
state	O
of	O
1	O
,	O
000	O
and	O
then	O
feed	O
the	O
output	O
token	O
representations	O
into	O
the	O
CRF	B-MethodName
layer	O
.	O
Following	O
most	O
of	O
the	O
previous	O
efforts	O
,	O
we	O
use	O
SGD	B-MethodName
optimizer	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.01	O
.	O
For	O
ACE	O
,	O
we	O
search	O
the	O
embedding	O
concatenation	O
for	O
30	O
episodes	O
.	O

Our	O
EN	O
-	O
PT	O
input	O
lexicon	O
has	O
931	O
,	O
568	O
manually	O
validated	O
translations	O
(	O
words	O
and	O
phrases	O
)	O
.	O
This	O
lexicon	O
has	O
been	O
compiled	O
in	O
a	O
long	O
term	O
effort	O
started	O
in	O
the	O
context	O
of	O
project	O
ISTRION	O
2	O
.	O
The	O
translations	O
were	O
extracted	O
automatically	O
from	O
several	O
corpora	O
,	O
including	O
Europarl	O
(	O
Koehn	O
and	O
Monz	O
,	O
2005	O
)	O
,	O
JRC	O
-	O
Acquis	O
(	O
Steinberger	O
et	O
al	O
,	O
2006	O
)	O
,	O
OPUS	O
EMEA	B-MethodName
(	O
Tiedemann	O
,	O
2009	O
)	O
and	O
others	O
,	O
using	O
a	O
combination	O
of	O
complementary	O
alignment	O
and	O
extraction	O
methods	O
:	O
GIZA	O
(	O
Och	O
and	O
Ney	O
,	O
2003	O
)	O
,	O
Anymalign	O
(	O
Lardilleux	O
and	O
Lepage	O
,	O
2009	O
)	O
,	O
spelling	O
similarity	O
measure	O
SpSim	O
(	O
Gomes	O
and	O
Lopes	O
,	O
2011	O
)	O
combined	O
with	O
co	O
-	O
occurrence	O
Dice	B-MetricName
measure	O
,	O
and	O
others	O
.	O
The	O
automatically	O
extracted	O
word	O
and	O
phrasal	O
translations	O
were	O
automatically	O
classified	O
,	O
prior	O
to	O
human	O
validation	O
,	O
using	O
an	O
SVM	B-MethodName
classifier	O
trained	O
on	O
previously	O
validated	O
translations	O
as	O
described	O
by	O
Mahesh	O
et	O
al	O
(	O
2015	O
)	O
.	O
The	O
automatic	O
classification	O
speeds	O
up	O
human	O
validation	O
because	O
very	O
few	O
translations	O
(	O
less	O
than	O
5	O
%	O
)	O
are	O
incorrectly	O
classified	O
,	O
and	O
only	O
those	O
need	O
to	O
be	O
manually	O
labeled	O
as	O
correct	O
or	O
incorrect	O
.	O
We	O
did	O
not	O
perform	O
any	O
extraction	O
or	O
validation	O
of	O
new	O
translations	O
from	O
the	O
corpus	O
provided	O
for	O
this	O
shared	O
task	O
.	O
We	O
did	O
,	O
however	O
,	O
complement	O
our	O
lexicon	O
with	O
cognate	O
and	O
homograph	O
alignments	O
using	O
the	O
SpSim	O
(	O
Gomes	O
and	O
Lopes	O
,	O
2011	O
)	O
spelling	O
similarity	O
measure	O
.	O

Since	O
no	O
development	O
data	O
was	O
supplied	O
,	O
we	O
took	O
the	O
initiative	O
to	O
prepare	O
some	O
development	O
sets	O
in	O
order	O
to	O
have	O
an	O
idea	O
of	O
the	O
most	O
promising	O
set	O
of	O
parameters	O
to	O
be	O
used	O
in	O
our	O
system	O
over	O
the	O
provided	O
data	O
to	O
produce	O
the	O
intended	O
translations	O
.	O
As	O
such	O
,	O
several	O
documents	O
were	O
removed	O
from	O
the	O
original	O
training	O
data	O
,	O
composed	O
by	O
the	O
medlinepubmed	O
,	O
biological	O
and	O
health	O
sets	O
,	O
applying	O
the	O
training	O
methods	O
on	O
the	O
remaining	O
documents	O
and	O
using	O
the	O
selected	O
ones	O
to	O
translate	O
and	O
compare	O
the	O
translations	O
against	O
their	O
originals	O
by	O
determining	O
their	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
scores	O
.	O
However	O
,	O
in	O
order	O
to	O
get	O
a	O
clearer	O
picture	O
of	O
the	O
type	O
of	O
results	O
that	O
could	O
be	O
expected	O
,	O
some	O
additional	O
tests	O
were	O
carried	O
out	O
including	O
the	O
selected	O
set	O
of	O
documents	O
in	O
the	O
training	O
data	O
.	O
Our	O
translation	O
model	O
supports	O
:	O
a	O
conservative	O
extraction	O
approach	O
,	O
which	O
is	O
more	O
restrictive	O
,	O
allowing	O
fewer	O
translation	O
equivalents	O
,	O
having	O
a	O
lower	O
recall	O
but	O
a	O
higher	O
precision	O
;	O
and	O
a	O
flexible	O
extraction	O
approach	O
,	O
which	O
is	O
more	O
permissive	O
,	O
allowing	O
a	O
larger	O
number	O
of	O
equivalents	O
but	O
at	O
the	O
cost	O
of	O
an	O
increase	O
of	O
incorrect	O
ones	O
.	O
We	O
were	O
interested	O
in	O
evaluating	O
the	O
impact	O
of	O
both	O
approaches	O
on	O
results	O
.	O
Table	O
4	O
shows	O
the	O
average	O
results	O
on	O
both	O
translation	O
directions	O
of	O
those	O
preliminary	O
tests	O
,	O
consisting	O
of	O
the	O
average	O
BLEU	B-MetricName
scores	O
for	O
the	O
conservative	O
(	O
cons	O
.	O
)	O
and	O
flexible	O
(	O
flex	O
.	O
)	O
approaches	O
,	O
as	O
well	O
as	O
the	O
average	O
times	O
taken	O
to	O
translate	O
the	O
documents	O
on	O
either	O
extraction	O
approaches	O
.	O
Those	O
results	O
concern	O
the	O
following	O
configurations	O
:	O
full	O
:	O
the	O
documents	O
used	O
for	O
testing	O
were	O
not	O
removed	O
from	O
the	O
training	O
set	O
(	O
medlinepubmed	O
,	O
biological	O
and	O
health	O
)	O
;	O
dev	O
:	O
the	O
documents	O
used	O
for	O
testing	O
were	O
removed	O
form	O
the	O
training	O
set	O
;	O
dev	O
-	O
europarl	O
:	O
the	O
same	O
as	O
dev	O
,	O
but	O
including	O
the	O
europarl	O
corpus	O
;	O
and	O
dev	O
-	O
europarl	O
-	O
low	O
:	O
the	O
same	O
as	O
dev	O
-	O
europarl	O
,	O
but	O
assigned	O
a	O
lower	O
relevance	O
to	O
the	O
europarl	O
corpus	O
.	O
These	O
preliminary	O
tests	O
have	O
shown	O
that	O
the	O
flexible	O
extraction	O
approach	O
produced	O
on	O
average	O
better	O
translation	O
results	O
when	O
the	O
reference	O
documents	O
were	O
not	O
included	O
in	O
the	O
test	O
set	O
,	O
which	O
is	O
the	O
normal	O
testing	O
situation	O
,	O
so	O
we	O
used	O
the	O
flexible	O
approach	O
.	O
The	O
Europarl	O
corpus	O
4	O
,	O
which	O
is	O
significantly	O
larger	O
(	O
54	O
,	O
543	O
,	O
044	O
words	O
in	O
English	O
and	O
60	O
,	O
375	O
,	O
477	O
words	O
in	O
Portuguese	O
)	O
,	O
was	O
tested	O
as	O
a	O
source	O
of	O
additional	O
term	O
coverage	O
,	O
which	O
allowed	O
a	O
translation	O
quality	O
improvement	O
lower	O
than	O
1	O
BLEU	B-MetricName
point	O
.	O
However	O
,	O
given	O
its	O
significant	O
increase	O
in	O
processing	O
time	O
because	O
of	O
its	O
large	O
size	O
,	O
a	O
time	O
increase	O
around	O
14	O
times	O
larger	O
,	O
we	O
had	O
to	O
drop	O
it	O
from	O
the	O
submission	O
tests	O
due	O
to	O
deadline	O
constraints	O
.	O
Additionally	O
,	O
these	O
results	O
show	O
that	O
assigning	O
a	O
lower	O
relevance	O
to	O
a	O
corpus	O
from	O
a	O
totally	O
different	O
domain	O
may	O
have	O
some	O
positive	O
impact	O
on	O
average	O
results	O
.	O
Once	O
we	O
have	O
decided	O
,	O
from	O
this	O
initial	O
testing	O
preparation	O
,	O
which	O
would	O
be	O
the	O
most	O
promising	O
and	O
interesting	O
features	O
to	O
use	O
in	O
the	O
final	O
runs	O
,	O
we	O
ran	O
the	O
training	O
processes	O
again	O
to	O
include	O
the	O
documents	O
that	O
have	O
been	O
left	O
out	O
,	O
this	O
way	O
using	O
the	O
full	O
data	O
provided	O
by	O
the	O
organizers	O
for	O
the	O
runs	O
to	O
be	O
submitted	O
.	O

This	O
last	O
run	O
shares	O
the	O
same	O
features	O
as	O
the	O
previous	O
run	O
(	O
assigning	O
higher	O
relevances	O
to	O
corresponding	O
corpora	O
)	O
but	O
this	O
time	O
our	O
bilingual	O
lexicon	O
and	O
named	O
entities	O
database	O
was	O
included	O
for	O
term	O
coverage	O
improvement	O
,	O
and	O
an	O
alignment	O
based	O
on	O
cognates	O
(	O
Gomes	O
and	O
Lopes	O
,	O
2011	O
)	O
is	O
used	O
.	O
About	O
our	O
bilingual	O
lexicon	O
,	O
considering	O
that	O
it	O
was	O
built	O
mainly	O
from	O
the	O
European	O
legislation	O
,	O
it	O
was	O
given	O
a	O
lower	O
relevance	O
because	O
past	O
experiences	O
have	O
shown	O
us	O
that	O
,	O
when	O
the	O
domain	O
is	O
not	O
shared	O
with	O
the	O
texts	O
to	O
be	O
translated	O
,	O
it	O
should	O
not	O
have	O
the	O
same	O
relevance	O
in	O
order	O
to	O
reduce	O
the	O
probability	O
of	O
using	O
inadequate	O
terms	O
for	O
the	O
intended	O
translation	O
domain	O
or	O
subject	O
.	O
Again	O
,	O
this	O
is	O
a	O
situation	O
that	O
has	O
also	O
been	O
confirmed	O
and	O
noted	O
in	O
Table	O
4	O
between	O
dev	O
-	O
europarl	O
and	O
deveuroparl	O
-	O
low	O
:	O
reducing	O
the	O
relevance	O
of	O
europarl	O
contributed	O
to	O
a	O
slight	O
score	O
increase	O
compared	O
to	O
when	O
the	O
relevance	O
is	O
the	O
same	O
.	O
As	O
a	O
side	O
note	O
,	O
translating	O
the	O
tests	O
took	O
nearly	O
14	O
hours	O
for	O
each	O
run	O
6	O
.	O
Had	O
we	O
included	O
europarl	O
,	O
judging	O
by	O
Table	O
4	O
,	O
we	O
would	O
have	O
taken	O
nearly	O
200	O
hours	O
,	O
which	O
is	O
more	O
than	O
a	O
week	O
,	O
expecting	O
to	O
simply	O
gain	O
0.75	O
BLEU	B-MetricName
points	O
,	O
on	O
average	O
,	O
so	O
we	O
had	O
no	O
other	O
option	O
than	O
leaving	O
it	O
out	O
.	O
Such	O
increase	O
in	O
translation	O
time	O
is	O
due	O
to	O
the	O
substantial	O
increase	O
of	O
translation	O
equivalents	O
available	O
for	O
decoding	O
from	O
such	O
a	O
large	O
corpus	O
.	O
The	O
decision	O
to	O
carry	O
out	O
the	O
alignment	O
based	O
on	O
cognates	O
was	O
taken	O
because	O
after	O
a	O
first	O
run	O
of	O
tests	O
we	O
realized	O
that	O
many	O
of	O
the	O
untranslated	O
terms	O
referred	O
to	O
medical	O
terms	O
and	O
diseases	O
,	O
which	O
shared	O
many	O
letters	O
between	O
both	O
languages	O
and	O
therefore	O
had	O
a	O
high	O
level	O
of	O
cognaticity	O
.	O
All	O
these	O
changes	O
allowed	O
a	O
significant	O
reduction	O
of	O
the	O
unique	O
untranslated	O
terms	O
to	O
a	O
total	O
of	O
4700	O
,	O
and	O
for	O
all	O
the	O
reasons	O
in	O
this	O
subsection	O
,	O
we	O
have	O
considered	O
this	O
run	O
as	O
being	O
our	O
best	O
.	O

Recent	O
years	O
have	O
seen	O
a	O
surge	O
of	O
interests	O
in	O
fine	O
-	O
grained	O
entity	B-TaskName
typing	I-TaskName
(	O
FET	O
)	O
as	O
it	O
serves	O
as	O
an	O
important	O
cornerstone	O
of	O
several	O
nature	O
language	O
processing	O
tasks	O
including	O
relation	B-TaskName
extraction	I-TaskName
(	O
Mintz	O
et	O
al	O
,	O
2009	O
)	O
,	O
entity	B-TaskName
linking	I-TaskName
(	O
Raiman	O
and	O
Raiman	O
,	O
2018	O
)	O
,	O
and	O
knowledge	B-TaskName
base	I-TaskName
completion	I-TaskName
(	O
Dong	O
et	O
al	O
,	O
2014	O
)	O
.	O
To	O
reduce	O
manual	O
efforts	O
in	O
labelling	O
training	O
data	O
,	O
distant	O
supervision	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
)	O
has	O
been	O
widely	O
adopted	O
by	O
recent	O
FET	O
systems	O
.	O
With	O
the	O
help	O
of	O
an	O
external	O
knowledge	O
base	O
(	O
KB	O
)	O
,	O
an	O
entity	O
mention	O
is	O
first	O
Figure	O
1	O
:	O
T	O
-	O
SNE	O
visualization	O
of	O
the	O
mention	O
embeddings	O
generated	O
by	O
NFETC	O
(	O
left	O
)	O
and	O
CLSC	O
(	O
right	O
)	O
on	O
the	O
BBN	O
dataset	O
.	O
Our	O
model	O
(	O
CLSC	O
)	O
clearly	O
groups	O
mentions	O
of	O
the	O
same	O
type	O
into	O
a	O
compact	O
cluster	O
.	O
linked	O
to	O
an	O
existing	O
entity	O
in	O
KB	O
,	O
and	O
then	O
labeled	O
with	O
all	O
possible	O
types	O
of	O
the	O
KB	O
entity	O
as	O
supervision	O
.	O
However	O
,	O
despite	O
its	O
efficiency	O
,	O
distant	O
supervision	O
also	O
brings	O
the	O
challenge	O
of	O
outof	O
-	O
context	O
noise	O
,	O
as	O
it	O
assigns	O
labels	O
in	O
a	O
context	O
agnostic	O
manner	O
.	O
Early	O
works	O
usually	O
ignore	O
such	O
noise	O
in	O
supervision	O
(	O
Ling	O
and	O
Weld	O
,	O
2012	O
;	O
Shimaoka	O
et	O
al	O
,	O
2016	O
)	O
,	O
which	O
dampens	O
the	O
performance	O
of	O
distantly	O
supervised	O
models	O
.	O
Towards	O
overcoming	O
out	O
-	O
of	O
-	O
context	O
noise	O
,	O
two	O
lines	O
of	O
work	O
have	O
been	O
proposed	O
to	O
distantly	O
supervised	O
FET	O
.	O
The	O
first	O
kind	O
of	O
work	O
try	O
to	O
filter	O
out	O
noisy	O
labels	O
using	O
heuristic	O
rules	O
(	O
Gillick	O
et	O
al	O
,	O
2014	O
)	O
.	O
However	O
,	O
such	O
heuristic	O
pruning	O
significantly	O
reduces	O
the	O
amount	O
of	O
training	O
data	O
,	O
and	O
thus	O
can	O
not	O
make	O
full	O
use	O
of	O
distantly	O
annotated	O
data	O
.	O
In	O
contrast	O
,	O
the	O
other	O
thread	O
of	O
works	O
try	O
to	O
incorporate	O
such	O
imperfect	O
annotation	O
by	O
partiallabel	O
loss	B-MetricName
(	O
PLL	O
)	O
.	O
The	O
basic	O
assumption	O
is	O
that	O
,	O
for	O
a	O
noisy	O
mention	O
,	O
the	O
maximum	O
score	O
associated	O
with	O
its	O
candidate	O
types	O
should	O
be	O
greater	O
than	O
the	O
scores	O
associated	O
with	O
any	O
other	O
non	O
-	O
candidate	O
types	O
(	O
Ren	O
et	O
al	O
,	O
2016a	O
;	O
Abhishek	O
et	O
al	O
,	O
2017	O
;	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
.	O
Despite	O
their	O
success	O
,	O
PLLbased	O
models	O
still	O
suffer	O
from	O
Confirmation	O
Bias	O
by	O
taking	O
its	O
own	O
prediction	O
as	O
optimization	O
objective	O
in	O
the	O
next	O
step	O
.	O
Specifically	O
,	O
given	O
an	O
entity	O
mention	O
,	O
if	O
the	O
typing	O
system	O
selected	O
a	O
wrong	O
type	O
with	O
the	O
maximum	O
score	O
among	O
all	O
candidates	O
,	O
it	O
will	O
try	O
to	O
further	O
maximize	O
the	O
score	O
of	O
the	O
wrong	O
type	O
in	O
following	O
optimization	O
epoches	O
(	O
in	O
order	O
to	O
minimize	O
PLL	O
)	O
,	O
thus	O
amplifying	O
the	O
confirmation	O
bias	O
.	O
Such	O
bias	O
starts	O
from	O
the	O
early	O
stage	O
of	O
training	O
,	O
when	O
the	O
typing	O
model	O
is	O
still	O
very	O
suboptimal	O
,	O
and	O
can	O
accumulate	O
in	O
training	O
process	O
.	O
Related	O
discussion	O
can	O
be	O
also	O
found	O
in	O
the	O
setting	O
of	O
semi	O
-	O
supervised	O
learning	O
(	O
Lee	O
et	O
al	O
,	O
2006	O
;	O
Laine	O
and	O
Aila	O
,	O
2017	O
;	O
Tarvainen	O
and	O
Valpola	O
,	O
2017	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
method	O
for	O
distantly	O
supervised	O
fine	O
-	O
grained	O
entity	B-TaskName
typing	I-TaskName
.	O
Enlightened	O
by	O
(	O
Kamnitsas	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
propose	O
to	O
effectively	O
utilize	O
imperfect	O
annotation	O
as	O
model	O
regularization	O
via	O
Compact	O
Latent	O
Space	O
Clustering	O
(	O
CLSC	O
)	O
.	O
More	O
specifically	O
,	O
our	O
model	O
encourages	O
the	O
feature	O
extractor	O
to	O
group	O
mentions	O
of	O
the	O
same	O
type	O
as	O
a	O
compact	O
cluster	O
(	O
dense	O
region	O
)	O
in	O
the	O
representation	O
space	O
,	O
which	O
leads	O
to	O
better	O
classification	O
performance	O
.	O
For	O
training	O
data	O
with	O
noisy	O
labels	O
,	O
instead	O
of	O
generating	O
pseudo	O
supervision	O
by	O
the	O
typing	O
model	O
itself	O
,	O
we	O
dynamically	O
construct	O
a	O
similarity	O
-	O
weighted	O
graph	O
between	O
clean	O
and	O
noisy	O
mentions	O
,	O
and	O
apply	O
label	O
propagation	O
on	O
the	O
graph	O
to	O
help	O
the	O
formation	O
of	O
compact	O
clusters	O
.	O
Figure	O
1	O
demonstrates	O
the	O
effectiveness	O
of	O
our	O
method	O
in	O
clustering	O
mentions	O
of	O
different	O
types	O
into	O
dense	O
regions	O
.	O
In	O
contrast	O
to	O
PLL	O
-	O
based	O
models	O
,	O
we	O
do	O
not	O
force	O
the	O
model	O
to	O
fit	O
pseudo	O
supervision	O
generated	O
by	O
itself	O
,	O
but	O
only	O
use	O
noisy	O
data	O
as	O
part	O
of	O
regularization	O
for	O
our	O
feature	O
extractor	O
layer	O
,	O
thus	O
avoiding	O
bias	O
accumulation	O
.	O
Extensive	O
experiments	O
on	O
standard	O
benchmarks	O
show	O
that	O
our	O
method	O
consistently	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O
Further	O
study	O
reveals	O
that	O
,	O
the	O
advantage	O
of	O
our	O
model	O
over	O
the	O
competitors	O
gets	O
even	O
more	O
significant	O
as	O
the	O
portion	O
of	O
noisy	O
data	O
rises	O
.	O

Overview	O
.	O
The	O
basic	O
assumptions	O
of	O
our	O
idea	O
are	O
:	O
(	O
1	O
)	O
all	O
mentions	O
belong	O
to	O
the	O
same	O
type	O
should	O
be	O
close	O
to	O
each	O
other	O
in	O
the	O
representation	O
space	O
because	O
they	O
should	O
have	O
similar	O
context	O
,	O
(	O
2	O
)	O
similar	O
contexts	O
lead	O
to	O
the	O
same	O
type	O
.	O
For	O
clean	O
data	O
,	O
we	O
compact	O
the	O
representation	O
space	O
of	O
the	O
same	O
type	O
to	O
comply	O
(	O
1	O
)	O
.	O
For	O
noisy	O
data	O
,	O
given	O
assumption	O
(	O
2	O
)	O
,	O
we	O
infer	O
the	O
their	O
type	O
distributions	O
via	O
label	O
propagation	O
and	O
candidate	O
types	O
constrain	O
.	O
Figure	O
2	O
shows	O
the	O
overall	O
framework	O
of	O
the	O
proposed	O
method	O
.	O
Clean	O
data	O
is	O
used	O
to	O
train	O
classifier	O
and	O
feature	O
extractor	O
end	O
-	O
to	O
-	O
endly	O
,	O
while	O
noisy	O
data	O
is	O
only	O
used	O
in	O
CLSC	O
regularization	O
.	O
Formally	O
,	O
given	O
a	O
batch	O
of	O
samples	O
{	O
(	O
m	O
i	O
,	O
c	O
i	O
,	O
Y	O
t	O
i	O
)	O
}	O
B	O
i=1	O
,	O
we	O
first	O
convert	O
each	O
sample	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
into	O
a	O
real	O
-	O
valued	O
vector	O
z	O
i	O
via	O
a	O
feature	O
extractor	O
z	O
(	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
;	O
θ	B-HyperparameterName
z	O
)	O
parameterized	O
by	O
θ	B-HyperparameterName
z	O
.	O
Then	O
a	O
type	O
classifier	O
g	O
(	O
z	O
i	O
;	O
θ	B-HyperparameterName
g	O
)	O
parameterized	O
by	O
θ	B-HyperparameterName
g	O
gives	O
the	O
posterior	O
P	O
(	O
y	O
|	O
z	O
i	O
;	O
θ	B-HyperparameterName
g	O
)	O
.	O
By	O
incorporating	O
CLSC	O
regularization	O
in	O
the	O
objective	O
function	O
,	O
we	O
encourage	O
the	O
feature	O
extractor	O
z	O
to	O
group	O
mentions	O
of	O
the	O
same	O
type	O
into	O
a	O
compact	O
cluster	O
,	O
which	O
facilitates	O
classification	O
as	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
Noisy	O
data	O
enhances	O
the	O
formation	O
of	O
compact	O
clusters	O
with	O
the	O
help	O
of	O
label	O
propagation	O
.	O

Figure	O
3	O
illustrates	O
our	O
feature	O
extractor	O
.	O
For	O
fair	O
comparison	O
,	O
we	O
adopt	O
the	O
same	O
feature	O
extraction	O
pipeline	O
as	O
used	O
in	O
(	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
.	O
The	O
feature	O
extractor	O
is	O
composed	O
of	O
an	O
embedding	O
layer	O
and	O
two	O
encoders	O
which	O
encode	O
mentions	O
and	O
contexts	O
respectively	O
.	O
Embedding	O
Layer	O
:	O
The	O
output	O
of	O
this	O
layer	O
is	O
a	O
concatenation	O
of	O
word	O
embedding	O
and	O
word	O
position	O
embedding	O
.	O
We	O
use	O
the	O
popular	O
300dimensional	O
word	O
embedding	O
supplied	O
by	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
to	O
capture	O
the	O
semantic	O
information	O
and	O
random	O
initialized	O
position	O
embedding	O
(	O
Zeng	O
et	O
al	O
,	O
2014	O
)	O
to	O
acquire	O
information	O
about	O
the	O
relation	O
between	O
words	O
and	O
the	O
mentions	O
.	O
Formally	O
,	O
Given	O
a	O
word	O
embedding	O
matrix	O
W	O
word	O
of	O
shape	O
d	O
w	O
×	O
|	O
V	O
|	O
,	O
where	O
V	O
is	O
the	O
vocabulary	O
and	O
d	O
w	O
is	O
the	O
size	O
of	O
word	O
embedding	O
,	O
each	O
column	O
of	O
W	O
word	O
represents	O
a	O
specific	O
word	O
w	O
in	O
V	O
.	O
We	O
map	O
each	O
word	O
w	O
j	O
in	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
to	O
a	O
word	O
embedding	O
w	O
d	O
j	O
R	O
dw	O
.	O
Analogously	O
,	O
we	O
get	O
the	O
word	O
position	O
embedding	O
w	O
p	O
j	O
R	O
dp	O
of	O
each	O
word	O
according	O
to	O
the	O
relative	O
distance	O
between	O
the	O
word	O
and	O
the	O
mention	O
,	O
we	O
only	O
use	O
a	O
fixed	O
length	O
context	O
here	O
.	O
The	O
final	O
embedding	O
of	O
the	O
j	O
-	O
th	O
word	O
is	O
w	O
E	O
j	O
=	O
[	O
w	O
d	O
j	O
,	O
w	O
p	O
j	O
]	O
.	O
Mention	O
Encoder	O
:	O
To	O
capture	O
lexical	O
level	O
information	O
of	O
mentions	O
,	O
an	O
averaging	O
mention	O
encoder	O
and	O
a	O
LSTM	B-MethodName
mention	O
encoder	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
is	O
applied	O
to	O
encode	O
mentions	O
.	O
Given	O
m	O
i	O
=	O
(	O
w	O
s	O
,	O
w	O
s+1	O
,	O
,	O
w	O
e	O
)	O
,	O
the	O
aver	O
-	O
aging	O
mention	O
representation	O
r	O
a	O
i	O
R	O
dw	O
is	O
:	O
r	O
a	O
i	O
=	O
1	O
e	O
−	O
s	O
+	O
1	O
e	O
j	O
=	O
s	O
w	O
d	O
j	O
(	O
1	O
)	O
By	O
applying	O
a	O
LSTM	B-MethodName
over	O
an	O
extended	O
mention	O
(	O
w	O
s−1	O
,	O
w	O
s	O
,	O
w	O
s+1	O
,	O
,	O
w	O
e	O
,	O
w	O
e+1	O
)	O
,	O
we	O
get	O
a	O
sequence	O
(	O
h	O
s−1	O
,	O
h	O
s	O
,	O
h	O
s+1	O
,	O
,	O
h	O
e	O
,	O
h	O
e+1	O
)	O
.	O
We	O
use	O
h	O
e+1	O
as	O
LSTM	B-MethodName
mention	O
representation	O
r	O
l	O
i	O
R	O
d	O
l	O
.	O
The	O
final	O
mention	O
representation	O
is	O
r	O
m	O
i	O
=	O
[	O
r	O
a	O
i	O
,	O
r	O
l	O
i	O
]	O
R	O
dw+d	O
l	O
.	O
Context	O
Encoder	O
:	O
A	O
bidirectional	B-MethodName
LSTM	I-MethodName
with	O
d	O
l	O
hidden	O
units	O
is	O
employed	O
to	O
encode	O
embedding	O
se	O
-	O
quence	O
(	O
w	O
E	O
s−W	O
,	O
w	O
E	O
s−W+1	O
,	O
,	O
w	O
E	O
e+W	O
)	O
:	O
−	O
h	O
j	O
=	O
LST	O
M	O
(	O
−−	O
h	O
j−1	O
,	O
w	O
E	O
j−1	O
)	O
−	O
h	O
j	O
=	O
LST	O
M	O
(	O
−−	O
h	O
j−1	O
,	O
w	O
E	O
j−1	O
)	O
h	O
j	O
=	O
[	O
−	O
h	O
j	O
−	O
h	O
j	O
]	O
(	O
2	O
)	O
where	O
denotes	O
element	O
-	O
wise	O
plus	O
.	O
Then	O
,	O
the	O
word	O
-	O
level	O
attention	O
mechanism	O
computes	O
a	O
score	O
β	B-HyperparameterName
i	O
,	O
j	O
over	O
different	O
word	O
j	O
in	O
the	O
context	O
c	O
i	O
to	O
get	O
the	O
final	O
context	O
representation	O
r	O
c	O
i	O
:	O
α	B-HyperparameterName
j	O
=	O
w	O
T	O
tanh	O
(	O
h	O
j	O
)	O
β	B-HyperparameterName
i	O
,	O
j	O
=	O
exp	O
(	O
α	B-HyperparameterName
j	O
)	O
k	O
exp	O
(	O
α	B-HyperparameterName
k	O
)	O
r	O
c	O
i	O
=	O
j	O
β	B-HyperparameterName
i	O
,	O
j	O
h	O
i	O
,	O
j	O
(	O
3	O
)	O
We	O
use	O
r	O
i	O
=	O
[	O
r	O
m	O
i	O
,	O
r	O
c	O
i	O
]	O
R	O
dz	O
=	O
R	O
dw+d	O
l	O
+	O
d	O
l	O
as	O
the	O
feature	O
representation	O
of	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
and	O
use	O
a	O
Neural	O
Networks	O
q	O
over	O
r	O
i	O
to	O
get	O
the	O
feature	O
vector	O
z	O
i	O
.	O
q	O
has	O
n	O
layers	O
with	O
h	O
n	O
hidden	O
units	O
and	O
use	O
ReLu	B-MethodName
activation	O
.	O

The	O
overview	O
of	O
CLSC	O
regularization	O
is	O
exhibited	O
in	O
Figure	O
4	O
,	O
which	O
includes	O
three	O
steps	O
:	O
dynamic	O
graph	B-TaskName
construction	I-TaskName
(	O
Figure	O
4c	O
)	O
,	O
label	O
propagation	O
(	O
Figure	O
4d	O
,	O
e	O
)	O
and	O
Markov	O
chains	O
(	O
Figure	O
4	O
g	O
)	O
.	O
The	O
idea	O
of	O
compact	O
clustering	O
for	O
semisupervised	O
learning	O
is	O
first	O
proposed	O
by	O
(	O
Kamnitsas	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
basic	O
idea	O
is	O
to	O
encourage	O
mentions	O
of	O
the	O
same	O
type	O
to	O
be	O
clustered	O
into	O
a	O
dense	O
region	O
in	O
the	O
embedding	O
space	O
.	O
We	O
introduce	O
more	O
details	O
of	O
CLSC	O
for	O
distantly	O
supervised	O
FET	O
in	O
following	O
sections	O
.	O
Dynamic	O
Graph	B-TaskName
Construction	I-TaskName
:	O
We	O
start	O
by	O
creating	O
a	O
fully	O
connected	O
graph	O
G	O
over	O
the	O
batch	O
of	O
samples	O
Z	O
=	O
{	O
z	O
i	O
}	O
B	O
i=1	O
,	O
as	O
shown	O
in	O
Figure	O
4c	O
1	O
.	O
Each	O
node	O
of	O
G	O
is	O
a	O
feature	O
representation	O
z	O
i	O
,	O
while	O
the	O
distance	O
between	O
nodes	O
is	O
defined	O
by	O
a	O
scaled	O
dot	O
-	O
product	O
distance	O
function	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
:	O
A	O
ij	O
=	O
exp	O
(	O
z	O
T	O
i	O
z	O
j	O
√	O
d	O
z	O
)	O
,	O
∀z	O
i	O
,	O
z	O
j	O
Z	O
A	O
=	O
exp	O
(	O
Z	O
T	O
Z	O
√	O
d	O
z	O
)	O
(	O
4	O
)	O
Each	O
entry	O
A	O
ij	O
measures	O
the	O
similarity	O
between	O
z	O
i	O
and	O
z	O
j	O
,	O
A	O
R	O
B×B	O
can	O
be	O
viewed	O
as	O
the	O
weighted	O
adjacency	O
matrix	O
of	O
G.	O
Label	O
Propagation	O
:	O
The	O
end	O
goal	O
of	O
CLSC	O
is	O
to	O
cluster	O
mentions	O
of	O
the	O
same	O
type	O
to	O
a	O
dense	O
region	O
.	O
For	O
mentions	O
which	O
have	O
more	O
than	O
one	O
labeled	O
types	O
,	O
we	O
apply	O
label	O
propagation	O
(	O
LP	O
)	O
on	O
G	O
to	O
estimate	O
their	O
type	O
distribution	O
.	O
Formally	O
,	O
we	O
denote	O
Φ	O
R	O
B×K	O
as	O
the	O
label	O
propagation	O
posterior	O
of	O
a	O
training	O
batch	O
.	O
The	O
original	O
label	O
propagation	O
proposed	O
by	O
(	O
Zhu	O
and	O
Ghahramani	O
,	O
2002	O
)	O
uses	O
a	O
transition	O
matrix	O
H	O
to	O
model	O
the	O
probability	O
of	O
a	O
node	O
i	O
propagating	O
its	O
type	O
posterior	O
φ	O
i	O
=	O
P	O
(	O
y	O
i	O
|	O
x	O
i	O
)	O
R	O
K	O
to	O
the	O
other	O
nodes	O
.	O
Each	O
entry	O
of	O
the	O
transition	O
matrix	O
H	O
R	O
B×B	O
is	O
defined	O
as	O
:	O
H	O
ij	O
=	O
A	O
ij	O
/	O
b	O
A	O
ib	O
(	O
5	O
)	O
The	O
original	O
label	O
propagation	O
algorithm	O
is	O
defined	O
as	O
:	O
1	O
.	O
Propagate	O
the	O
label	O
by	O
transition	O
matrix	O
H	O
,	O
Φ	O
(	O
t+1	O
)	O
=	O
HΦ	O
(	O
t	O
)	O
2	O
.	O
Clamp	O
the	O
labeled	O
data	O
to	O
their	O
true	O
labels	O
.	O
Repeat	O
from	O
step	O
1	O
until	O
Φ	O
converges	O
In	O
this	O
work	O
Φ	O
(	O
0	B-DatasetName
)	O
is	O
randomly	O
initialized	O
2	O
.	O
Unlike	O
unlabeled	O
data	O
in	O
semi	O
-	O
supervised	O
learning	O
,	O
distantly	O
labeled	O
mentions	O
in	O
FET	O
have	O
a	O
limited	O
set	O
of	O
candidate	O
types	O
.	O
Based	O
on	O
this	O
observation	O
,	O
We	O
assume	O
that	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
can	O
only	O
transmit	O
and	O
receive	O
probability	O
of	O
types	O
in	O
Y	O
t	O
i	O
no	O
matter	O
it	O
is	O
noisy	O
data	O
or	O
clean	O
data	O
.	O
Formally	O
,	O
define	O
a	O
B	O
×	O
K	O
indicator	O
matrix	O
M	O
R	O
B×K	O
,	O
where	O
M	O
ij	O
=	O
1	O
if	O
type	O
j	O
in	O
Y	O
t	O
i	O
otherwise	O
0	B-DatasetName
,	O
where	O
B	O
is	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
and	O
K	O
is	O
the	O
number	O
of	O
types	O
.	O
Our	O
clamping	O
step	O
relies	O
on	O
M	O
as	O
is	O
shown	O
in	O
Figure	O
4d	O
:	O
Φ	O
(	O
t+1	O
)	O
ij	O
Φ	O
(	O
t+1	O
)	O
ij	O
M	O
ij	O
/	O
k	O
Φ	O
(	O
t+1	O
)	O
ik	O
M	O
ik	O
(	O
6	O
)	O
For	O
convenience	O
,	O
we	O
iterate	O
through	O
these	O
two	O
steps	O
S	O
lp	O
times	O
,	O
S	O
lp	O
is	O
a	O
hyperparameter	O
.	O
Compact	O
Clustering	O
:	O
The	O
LP	O
posterior	O
Φ	O
=	O
Φ	O
(	O
S	O
lp	O
+1	O
)	O
is	O
used	O
to	O
judge	O
the	O
label	O
agreement	O
between	O
samples	O
.	O
In	O
the	O
desired	O
optimal	O
state	O
,	O
transition	O
probabilities	O
between	O
samples	O
should	O
be	O
uniform	O
inside	O
the	O
same	O
class	O
,	O
while	O
be	O
zero	O
between	O
different	O
classes	O
.	O
Based	O
on	O
this	O
assumption	O
,	O
the	O
desirable	O
transition	O
matrix	O
T	O
R	O
B×B	O
is	O
defined	O
as	O
:	O
T	O
ij	O
=	O
K	O
k=1	O
Φ	O
ik	O
Φ	O
jk	O
m	O
k	O
,	O
m	O
k	B-HyperparameterName
=	I-HyperparameterName
B	O
b=1	O
Φ	O
bk	O
(	O
7	O
)	O
m	O
k	O
is	O
a	O
normalization	O
term	O
for	O
class	O
k.	O
Transition	O
matrix	O
H	O
derived	O
from	O
z	O
(	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
;	O
θ	B-HyperparameterName
z	O
)	O
should	O
be	O
in	O
keeping	O
with	O
T	O
.	O
Thus	O
we	O
minimize	O
the	O
cross	O
entropy	O
between	O
T	O
and	O
H	O
:	O
L	O
1−step	O
=	O
−	O
1	O
B	O
2	O
B	O
i=1	O
B	O
j=1	O
T	O
ij	O
log	O
(	O
H	O
ij	O
)	O
(	O
8	O
)	O
For	O
instance	O
,	O
if	O
T	O
ij	O
is	O
close	O
to	O
1	O
,	O
H	O
ij	O
needs	O
to	O
be	O
bigger	O
,	O
which	O
results	O
in	O
the	O
growth	O
of	O
A	O
ij	O
and	O
finally	O
optimize	O
θ	B-HyperparameterName
z	O
(	O
Eq.4	O
)	O
.	O
The	O
loss	B-MetricName
L	O
1−step	O
has	O
largely	O
described	O
the	O
regularization	O
we	O
use	O
in	O
z	O
(	O
(	O
m	O
i	O
,	O
c	O
i	O
)	O
;	O
θ	B-HyperparameterName
z	O
)	O
for	O
compression	O
clustering	O
.	O
In	O
order	O
to	O
keep	O
the	O
structure	O
of	O
existing	O
clusters	O
,	O
(	O
Kamnitsas	O
et	O
al	O
,	O
2018	O
)	O
proposed	O
an	O
extension	O
of	O
L	O
1−step	O
to	O
the	O
case	O
of	O
Markov	O
chains	O
with	O
multiple	O
transitions	O
between	O
samples	O
,	O
which	O
should	O
remain	O
within	O
a	O
single	O
class	O
.	O
The	O
extension	O
maximizes	O
probability	O
of	O
paths	O
that	O
only	O
traverse	O
among	O
samples	O
belong	O
to	O
one	O
class	O
.	O
Define	O
E	O
R	O
B×B	O
as	O
:	O
E	O
=	O
Φ	O
T	O
Φ	O
(	O
9	O
)	O
E	O
ij	O
measures	O
the	O
label	O
similarities	O
between	O
z	O
i	O
and	O
z	O
j	O
,	O
which	O
is	O
used	O
to	O
mask	O
the	O
transition	O
between	O
different	O
clusters	O
.	O
The	O
extension	O
is	O
given	O
by	O
:	O
H	O
(	O
1	O
)	O
=	O
H	O
H	O
(	O
s	O
)	O
=	O
(	O
H	O
E	O
)	O
(	O
s−1	O
)	O
H	O
=	O
(	O
H	O
E	O
)	O
H	O
(	O
s−1	O
)	O
,	O
(	O
10	O
)	O
where	O
is	O
Hadamard	O
Product	O
,	O
and	O
H	O
(	O
s	O
)	O
ij	O
is	O
the	O
probability	O
of	O
a	O
Markov	O
process	O
to	O
transit	O
from	O
node	O
i	O
to	O
node	O
j	O
after	O
s	O
−	O
1	O
steps	O
within	O
the	O
same	O
class	O
.	O
The	O
extended	O
loss	B-MetricName
function	O
models	O
paths	O
of	O
different	O
length	O
s	O
between	O
samples	O
on	O
the	O
graph	O
:	O
L	O
clsc	O
=	O
−	O
1	O
S	O
m	O
1	O
B	O
2	O
Sm	O
s=1	O
B	O
i=1	O
B	O
j=1	O
T	O
ij	O
log	O
(	O
H	O
(	O
s	O
)	O
ij	O
)	O
.	O
(	O
11	O
)	O
For	O
S	O
m	O
=	O
1	O
,	O
L	O
clsc	O
=	O
L	O
1−step	O
.	O
By	O
minimizing	O
the	O
cross	O
entropy	O
between	O
T	O
and	O
H	O
(	O
s	O
)	O
(	O
Eq.11	O
)	O
,	O
L	O
clsc	O
compact	O
paths	O
of	O
different	O
length	O
between	O
samples	O
within	O
the	O
same	O
class	O
.	O
Here	O
,	O
S	O
m	O
is	O
a	O
hyper	O
-	O
parameter	O
to	O
control	O
the	O
maximum	O
length	O
of	O
Markov	O
chain	O
.	O
L	O
clsc	O
is	O
added	O
to	O
the	O
final	O
objective	O
function	O
as	O
regularization	O
to	O
encourage	O
compact	O
cluttering	O
.	O

Given	O
the	O
representation	O
of	O
a	O
mention	O
,	O
the	O
type	O
posterior	O
is	O
given	O
by	O
a	O
standard	O
softmax	B-MethodName
classifier	O
parameterized	O
by	O
θ	B-HyperparameterName
g	O
:	O
P	O
(	O
ŷ	O
i	O
|	O
z	O
i	O
;	O
θ	B-HyperparameterName
g	O
)	O
=	O
sof	B-DatasetName
tmax	O
(	O
W	O
c	O
z	O
i	O
+	O
b	O
c	O
)	O
,	O
(	O
12	O
)	O
where	O
W	O
c	O
R	O
K×dz	O
is	O
a	O
parameter	O
matrix	O
,	O
b	O
R	O
K	O
is	O
the	O
bias	O
vector	O
,	O
where	O
K	O
is	O
the	O
number	O
of	O
types	O
.	O
The	O
predicted	O
type	O
is	O
then	O
given	O
byt	O
i	O
=	O
argmax	O
y	O
i	O
P	O
(	O
ŷ	O
i	O
|	O
z	O
i	O
;	O
θ	B-HyperparameterName
g	O
)	O
.	O
Our	O
loss	B-MetricName
function	O
consists	O
of	O
two	O
parts	O
.	O
L	O
sup	O
is	O
supervision	O
loss	B-MetricName
defined	O
by	O
KL	O
divergence	O
:	O
L	O
sup	O
=	O
−	O
1	O
B	O
c	O
Bc	O
i=1	O
K	O
k=1	O
y	O
ik	O
log	O
(	O
P	O
(	O
y	O
i	O
|	O
z	O
i	O
;	O
θ	B-HyperparameterName
g	O
)	O
)	O
k	O
(	O
13	O
)	O
Here	O
B	O
c	O
is	O
the	O
number	O
of	O
clean	O
data	O
in	O
a	O
training	O
batch	O
,	O
K	O
is	O
the	O
number	O
of	O
target	O
types	O
.	O
The	O
regularization	O
term	O
is	O
given	O
by	O
L	O
clsc	O
.	O
Hence	O
,	O
the	O
overall	O
loss	B-MetricName
function	O
is	O
:	O
L	O
f	O
inal	O
=	O
L	O
sup	O
+	O
λ	O
clsc	O
×	O
L	O
clsc	O
(	O
14	O
)	O
λ	O
clsc	O
is	O
a	O
hyper	O
parameter	O
to	O
control	O
the	O
influence	O
of	O
CLSC	O
.	O

We	O
compare	O
the	O
proposed	O
method	O
with	O
several	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
FET	O
systems	O
3	O
:	O
Attentive	O
(	O
Shimaoka	O
et	O
al	O
,	O
2016	O
)	O
uses	O
an	O
attention	O
based	O
feature	O
extractor	O
and	O
does	O
n't	O
distinguish	O
clean	O
from	O
noisy	O
data	O
;	O
AFET	O
(	O
Ren	O
et	O
al	O
,	O
2016a	O
)	O
trains	O
label	O
embedding	O
with	O
partial	O
label	O
loss	B-MetricName
;	O
AAA	O
(	O
Abhishek	O
et	O
al	O
,	O
2017	O
)	O
learns	O
joint	O
representation	O
of	O
mentions	O
and	O
type	O
labels	O
;	O
PLE+HYENA	O
/	O
FIGER	B-DatasetName
(	O
Ren	O
et	O
al	O
,	O
2016b	O
)	O
proposes	O
heterogeneous	O
partial	O
-	O
label	O
embedding	O
for	O
label	O
noise	O
reduction	O
to	O
boost	O
typing	O
systems	O
.	O
We	O
compare	O
two	O
PLE	O
models	O
with	O
HYENA	O
(	O
Yogatama	O
et	O
al	O
,	O
2015	O
)	O
and	O
FIGER	B-DatasetName
(	O
Ling	O
and	O
Weld	O
,	O
2012	O
)	O
as	O
the	O
base	O
typing	O
system	O
respectively	O
;	O
NFETC	O
(	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
trains	O
neural	O
fine	O
-	O
grained	O
typing	O
system	O
with	O
hierarchy	O
-	O
aware	O
loss	B-MetricName
.	O
We	O
compare	O
the	O
performance	O
of	O
the	O
NFETC	O
model	O
with	O
two	O
different	O
loss	B-MetricName
functions	O
:	O
partial	O
-	O
label	O
loss	B-MetricName
and	O
PLL+hierarchical	O
loss	B-MetricName
.	O
We	O
denote	O
the	O
two	O
variants	O
as	O
NFETC	O
and	O
NFETC	O
hier	O
respectively	O
;	O
NFETC	O
-	O
CLSC	O
is	O
the	O
proposed	O
model	O
in	O
this	O
work	O
.	O
We	O
use	O
the	O
NFETC	O
model	O
as	O
our	O
base	O
model	O
,	O
based	O
on	O
which	O
we	O
apply	O
Compact	O
Latent	O
Space	O
Clustering	O
Regularization	O
as	O
described	O
in	O
Section	O
3.2	O
;	O
Similarly	O
,	O
we	O
report	O
results	O
produced	O
by	O
using	O
both	O
KLdivergense	O
-	O
based	O
loss	B-MetricName
(	O
NFETC	O
-	O
CLSC	O
)	O
and	O
KL+hierarchical	O
loss	B-MetricName
(	O
NFETC	O
-	O
CLSC	O
hier	O
)	O
.	O

For	O
evaluation	O
metrics	O
,	O
we	O
adopt	O
strict	O
accuracy	B-MetricName
,	O
loose	O
macro	O
,	O
and	O
loose	O
micro	O
F	O
-	O
scores	O
widely	O
used	O
in	O
the	O
FET	O
task	O
(	O
Ling	O
and	O
Weld	O
,	O
2012	O
)	O
.	O
To	O
fine	O
tuning	O
the	O
hyper	O
-	O
parameters	O
,	O
we	O
randomly	O
sampled	O
10	O
%	O
of	O
the	O
test	O
set	O
as	O
a	O
development	O
set	O
for	O
both	O
datasets	O
.	O
With	O
the	O
fine	O
-	O
tuned	O
hyperparameter	O
as	O
mentioned	O
in	O
4.4	O
,	O
we	O
run	O
the	O
model	O
five	O
times	O
and	O
report	O
the	O
average	O
strict	O
accuracy	B-MetricName
,	O
macro	B-MetricName
F1	I-MetricName
and	O
micro	B-MetricName
F1	I-MetricName
on	O
the	O
test	O
set	O
.	O

We	O
search	O
the	O
hyper	O
parameter	O
of	O
Ontonotes	B-DatasetName
and	O
BBN	O
respectively	O
via	O
Hyperopt	O
proposed	O
by	O
(	O
Bergstra	O
et	O
al	O
,	O
2013	O
)	O
.	O
Hyper	O
parameters	O
are	O
shown	O
in	O
Appendix	O
A.	O
We	O
optimize	O
the	O
model	O
via	O
Adam	B-MethodName
Optimizer	B-HyperparameterName
.	O
The	O
full	O
hyper	O
parameters	O
includes	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
lr	O
,	O
the	O
dimension	O
d	O
p	O
of	O
word	O
position	O
embedding	O
,	O
the	O
dimension	O
d	O
l	O
of	O
the	O
mention	O
encoder	O
's	O
output	O
(	O
equal	O
to	O
the	O
dimension	O
of	O
the	O
context	O
encoder	O
's	O
ourput	O
)	O
,	O
the	O
input	O
dropout	O
keep	O
probability	O
p	O
i	O
and	O
output	O
dropout	O
keep	O
probability	O
p	O
o	O
for	O
LSTM	B-MethodName
layers	O
(	O
in	O
context	O
encoder	O
and	O
LSTM	B-MethodName
mention	O
encoder	O
)	O
,	O
the	O
L2	B-HyperparameterName
regularization	I-HyperparameterName
parameter	O
λ	O
,	O
the	O
factor	O
of	O
hierarchical	O
loss	B-MetricName
normalization	O
α	B-HyperparameterName
(	O
α	B-HyperparameterName
>	O
0	B-DatasetName
means	O
use	O
the	O
normalization	O
)	O
,	O
BN	O
(	O
whether	O
using	O
Batch	B-MethodName
normalization	I-MethodName
)	O
,	O
the	O
max	O
step	O
S	O
lp	O
of	O
the	O
label	O
propagation	O
,	O
the	O
max	O
length	O
S	O
m	O
of	O
Markov	O
chain	O
,	O
the	O
influence	O
parameter	O
λ	O
clsc	O
of	O
CLSC	O
,	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
B	O
,	O
the	O
number	O
n	O
of	O
hidden	O
layers	O
in	O
q	O
and	O
the	O
number	O
h	O
n	O
of	O
hidden	O
units	O
of	O
the	O
hidden	O
layers	O
.	O
We	O
implement	O
all	O
models	O
using	O
Tensorflow	O
4	O
.	O

Table	O
1	O
shows	O
performance	O
comparison	O
between	O
the	O
proposed	O
CLSC	O
model	O
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
FET	O
systems	O
.	O
On	O
both	O
benchmarks	O
,	O
the	O
CLSC	O
model	O
achieves	O
the	O
best	O
performance	O
in	O
all	O
three	O
metrics	O
.	O
When	O
focusing	O
on	O
the	O
comparison	O
between	O
NFETC	O
and	O
CLSC	O
,	O
we	O
have	O
following	O
observation	O
:	O
Compact	O
Latent	O
Space	O
Clustering	O
shows	O
its	O
effectiveness	O
on	O
both	O
clean	O
data	O
and	O
noisy	O
data	O
.	O
By	O
applying	O
CLSC	O
regularization	O
on	O
the	O
basic	O
NFETC	O
model	O
,	O
we	O
observe	O
consistent	O
and	O
significant	O
performance	O
boost	O
;	O
Hierarchical	O
-	O
aware	O
loss	B-MetricName
shows	O
significant	O
advantage	O
on	O
the	O
OntoNotes	B-DatasetName
dataset	O
,	O
while	O
showing	O
insignificant	O
performance	O
boost	O
on	O
the	O
BBN	O
dataset	O
.	O
This	O
is	O
due	O
to	O
different	O
distribution	O
of	O
labels	O
on	O
the	O
test	O
set	O
.	O
The	O
proportion	O
of	O
terminal	O
types	O
of	O
the	O
test	O
set	O
is	O
69	O
%	O
for	O
the	O
BBN	O
dataset	O
,	O
while	O
is	O
only	O
33	O
%	O
on	O
the	O
OntoNotes	B-DatasetName
dataset	O
.	O
Thus	O
,	O
applying	O
hierarchical	O
-	O
aware	O
loss	B-MetricName
on	O
the	O
BBN	O
dataset	O
brings	O
little	O
improvement	O
;	O
Both	O
algorithms	O
are	O
able	O
to	O
utilize	O
noisy	O
data	O
to	O
improve	O
performance	O
,	O
so	O
we	O
would	O
like	O
to	O
further	O
study	O
their	O
performance	O
in	O
different	O
noisy	O
scenarios	O
in	O
following	O
discussions	O
.	O
By	O
principle	O
,	O
with	O
sufficient	O
amount	O
of	O
clean	O
training	O
data	O
,	O
most	O
typing	O
systems	O
can	O
achieve	O
satisfying	O
performance	O
.	O
To	O
further	O
study	O
the	O
robustness	O
of	O
the	O
methods	O
to	O
label	O
noise	O
,	O
we	O
compare	O
their	O
performance	O
with	O
the	O
presence	O
of	O
25	O
%	O
,	O
20	O
%	O
,	O
15	O
%	O
,	O
10	O
%	O
and	O
5	O
%	O
clean	O
training	O
data	O
and	O
all	O
noisy	O
training	O
data	O
.	O
Figure	O
5	O
shows	O
the	O
performance	O
curves	O
as	O
the	O
proportion	O
of	O
clean	O
data	O
drops	O
.	O
As	O
it	O
reveals	O
,	O
the	O
CLSC	O
model	O
consistently	O
wins	O
in	O
the	O
comparison	O
.	O
The	O
advantage	O
is	O
especially	O
clear	O
on	O
the	O
BBN	O
dataset	O
,	O
which	O
offers	O
less	O
amount	O
of	O
training	O
data	O
.	O
Note	O
that	O
,	O
with	O
only	O
27.9	O
%	O
of	O
training	O
data	O
(	O
when	O
only	O
leaving	O
5	O
%	O
clean	O
data	O
)	O
on	O
the	O
BBN	O
dataset	O
,	O
the	O
CLSC	O
model	O
yield	O
a	O
comparable	O
result	O
with	O
the	O
NFETC	O
model	O
trained	O
on	O
full	O
data	O
.	O
This	O
comparison	O
clearly	O
shows	O
the	O
superiority	O
of	O
our	O
approach	O
in	O
the	O
effectiveness	O
of	O
utilizing	O
noisy	O
data	O
.	O

Named	B-TaskName
entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
has	O
been	O
excavated	O
for	O
a	O
long	O
time	O
(	O
Collins	O
and	O
Singer	O
,	O
1999	O
;	O
,	O
which	O
classifies	O
coarsegrained	O
types	O
(	O
e.g.	O
person	O
,	O
location	O
)	O
.	O
Recently	O
,	O
(	O
Nagesh	O
and	O
Surdeanu	O
,	O
2018a	O
,	O
b	O
)	O
applied	O
ladder	O
network	O
(	O
Rasmus	O
et	O
al	O
,	O
2015	O
)	O
to	O
coarse	O
-	O
grained	O
entity	O
classification	O
in	O
a	O
semi	O
-	O
supervised	O
learning	O
fashion	O
.	O
(	O
Ling	O
and	O
Weld	O
,	O
2012	O
)	O
proposed	O
Fine	O
-	O
Grained	O
Entity	O
Recognition	O
(	O
FET	O
)	O
.	O
They	O
used	O
distant	O
supervision	O
to	O
get	O
training	O
corpus	O
for	O
FET	O
.	O
Embedding	O
techniques	O
was	O
applied	O
to	O
learn	O
feature	O
representations	O
since	O
(	O
Yogatama	O
et	O
al	O
,	O
2015	O
;	O
Dong	O
et	O
al	O
,	O
2015	O
)	O
.	O
(	O
Shimaoka	O
et	O
al	O
,	O
2016	O
)	O
introduced	O
attention	O
mechanism	O
for	O
FET	O
to	O
capture	O
informative	O
words	O
.	O
(	O
Xin	O
et	O
al	O
,	O
2018a	O
)	O
used	O
the	O
TransE	B-MethodName
entity	B-TaskName
embeddings	I-TaskName
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
as	O
the	O
query	O
vector	O
of	O
attention	O
.	O
Early	O
works	O
ignore	O
the	O
out	O
-	O
of	O
-	O
context	O
noise	O
,	O
(	O
Gillick	O
et	O
al	O
,	O
2014	O
)	O
proposed	O
context	O
dependent	O
FET	O
and	O
use	O
three	O
heuristics	O
to	O
clean	O
the	O
noisy	O
labels	O
with	O
the	O
side	O
effect	O
of	O
losing	O
training	O
data	O
.	O
To	O
utilize	O
noisy	O
data	O
,	O
(	O
Ren	O
et	O
al	O
,	O
2016a	O
)	O
distinguished	O
the	O
loss	B-MetricName
function	O
of	O
noisy	O
data	O
from	O
clean	O
data	O
via	O
partial	O
label	O
loss	B-MetricName
(	O
PLL	O
)	O
.	O
(	O
Abhishek	O
et	O
al	O
,	O
2017	O
;	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
proposed	O
variants	O
of	O
PLL	O
,	O
which	O
still	O
suffer	O
from	O
confirmation	O
bias	O
.	O
(	O
Xu	O
and	O
Barbosa	O
,	O
2018	O
)	O
proposed	O
hierarchical	O
loss	B-MetricName
to	O
handle	O
over	O
-	O
specific	O
noise	O
.	O
On	O
top	O
of	O
AFET	O
,	O
(	O
Ren	O
et	O
al	O
,	O
2016b	O
)	O
proposed	O
a	O
method	O
PLE	O
to	O
reduce	O
the	O
label	O
noise	O
,	O
which	O
lead	O
to	O
a	O
great	O
success	O
in	O
FET	O
.	O
Because	O
label	O
noise	O
reduction	O
is	O
separated	O
from	O
the	O
learning	O
of	O
FET	O
,	O
there	O
might	O
be	O
error	O
propagation	O
problem	O
.	O
Recently	O
,	O
(	O
Xin	O
et	O
al	O
,	O
2018b	O
)	O
proposed	O
utilizing	O
a	O
pretrained	O
language	O
model	O
measures	O
the	O
compatibility	O
between	O
context	O
and	O
type	O
names	O
,	O
and	O
use	O
it	O
to	O
repel	O
the	O
interference	O
of	O
noisy	O
labels	O
.	O
However	O
,	O
the	O
compatibility	O
got	O
by	O
language	O
model	O
may	O
not	O
be	O
right	O
and	O
type	O
information	O
is	O
defined	O
by	O
corpus	O
and	O
annotation	O
guidelines	O
rather	O
than	O
type	O
names	O
as	O
is	O
mentioned	O
in	O
(	O
Azad	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
addition	O
,	O
there	O
are	O
some	O
work	O
about	O
entity	O
-	O
level	O
typing	O
which	O
aim	O
to	O
figure	O
out	O
the	O
types	O
of	O
entities	O
in	O
KB	O
(	O
Yaghoobzadeh	O
and	O
Schütze	O
,	O
2015	O
;	O
Jin	O
et	O
al	O
,	O
2018	O
)	O
.	O

In	O
recent	O
years	O
,	O
significant	O
progress	O
has	O
been	O
made	O
in	O
the	O
field	O
of	O
open	B-TaskName
-	I-TaskName
domain	I-TaskName
question	I-TaskName
answering	I-TaskName
(	O
Chen	O
et	O
al	O
,	O
2017	O
;	O
Wang	O
et	O
al	O
,	O
2017Wang	O
et	O
al	O
,	O
,	O
2018Min	O
et	O
al	O
,	O
2018	O
;	O
Asai	O
et	O
al	O
,	O
2019	O
)	O
.	O
Very	O
recently	O
,	O
some	O
works	O
turn	O
to	O
deal	O
with	O
a	O
more	O
challenging	O
task	O
of	O
asking	O
complex	O
questions	O
(	O
Welbl	O
et	O
al	O
,	O
2018	O
;	O
Yang	O
et	O
al	O
,	O
2018	O
)	O
from	O
the	O
open	O
-	O
domain	O
text	O
corpus	O
.	O
In	O
the	O
open	O
-	O
domain	O
scenario	O
,	O
one	O
critical	O
challenge	O
raised	O
by	O
complex	O
questions	O
is	O
that	O
each	O
question	O
may	O
require	O
multiple	O
pieces	O
of	O
evidence	O
to	O
get	O
the	O
right	O
answer	O
,	O
while	O
the	O
evidence	O
usually	O
scatters	O
in	O
different	O
passages	O
.	O
Examples	O
in	O
Figure	O
1	O
shows	O
two	O
types	O
of	O
questions	O
that	O
require	O
evidence	O
from	O
multiple	O
passages	O
.	O
To	O
deal	O
with	O
the	O
challenging	O
multi	O
-	O
evidence	O
questions	O
,	O
an	O
open	O
-	O
domain	O
QA	O
system	O
should	O
be	O
able	O
to	O
(	O
1	O
)	O
efficiently	O
retrieve	O
a	O
small	O
number	O
of	O
passages	O
that	O
cover	O
the	O
full	O
evidence	O
;	O
and	O
(	O
2	O
)	O
accurately	O
extract	O
the	O
answer	O
by	O
jointly	O
considering	O
the	O
candidate	O
evidence	O
passages	O
.	O
While	O
there	O
have	O
been	O
several	O
prior	O
works	O
in	O
the	O
latter	O
direction	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
;	O
Figure	O
1	O
:	O
Examples	O
of	O
complex	O
questions	O
involving	O
two	O
facts	O
of	O
a	O
person	O
.	O
Different	O
facts	O
are	O
color	O
-	O
coded	O
.	O
P	O
#	O
are	O
all	O
relevant	O
passages	O
,	O
while	O
only	O
the	O
ones	O
with	O
solid	O
-	O
line	O
boxes	O
are	O
the	O
true	O
supporting	O
passages	O
.	O
Lin	O
et	O
al	O
,	O
2018	O
)	O
,	O
the	O
solutions	O
to	O
the	O
first	O
problem	O
still	O
rely	O
on	O
traditional	O
or	O
neural	O
information	B-TaskName
retrieval	I-TaskName
(	O
IR	O
)	O
approaches	O
,	O
which	O
solely	O
measure	O
the	O
relevance	O
between	O
the	O
question	O
and	O
each	O
individual	O
paragraph	O
,	O
and	O
will	O
highly	O
possibly	O
put	O
the	O
wrong	O
evidence	O
to	O
the	O
top	O
.	O
1	O
For	O
example	O
in	O
Figure	O
1	O
(	O
top	O
)	O
,	O
P1	O
and	O
P2	O
are	O
two	O
candidate	O
evidence	O
passages	O
that	O
are	O
closely	O
related	O
to	O
the	O
question	O
but	O
only	O
cover	O
the	O
same	O
unilateral	O
fact	O
required	O
by	O
the	O
question	O
,	O
therefore	O
leading	O
us	O
to	O
the	O
wrong	O
answer	O
Newton	O
.	O
This	O
paper	O
formulates	O
a	O
new	O
problem	O
of	O
complementary	O
evidence	O
identification	O
for	O
answering	O
complex	O
questions	O
.	O
The	O
key	O
idea	O
is	O
to	O
consider	O
the	O
problem	O
as	O
measuring	O
the	O
properties	O
of	O
the	O
selected	O
passages	O
,	O
more	O
than	O
the	O
individual	O
relevance	O
.	O
Specifically	O
,	O
we	O
hope	O
the	O
selected	O
passages	O
can	O
serve	O
as	O
a	O
set	O
of	O
spanning	O
bases	O
that	O
supports	O
the	O
question	O
.	O
The	O
selected	O
passage	O
set	O
thus	O
should	O
satisfy	O
the	O
properties	O
of	O
(	O
1	O
)	O
relevancy	O
,	O
i.e.	O
,	O
they	O
should	O
be	O
closely	O
related	O
to	O
the	O
question	O
;	O
(	O
2	O
)	O
diversity	O
,	O
i.e.	O
,	O
they	O
should	O
cover	O
diverse	O
information	O
given	O
the	O
coverage	O
property	O
is	O
satisfied	O
;	O
(	O
3	O
)	O
compactness	O
,	O
i.e.	O
,	O
the	O
number	O
of	O
passages	O
to	O
satisfy	O
the	O
above	O
properties	O
should	O
be	O
minimal	O
.	O
With	O
these	O
three	O
defined	O
properties	O
,	O
we	O
hope	O
to	O
both	O
improve	O
the	O
selective	O
accuracy	B-MetricName
and	O
encourage	O
the	O
interpretability	O
of	O
the	O
evidence	O
identification	O
.	O
Note	O
that	O
complementary	O
evidence	O
identification	O
in	O
QA	O
is	O
different	O
from	O
Search	O
Result	O
Diversification	O
(	O
SRD	B-DatasetName
)	O
in	O
IR	O
on	O
their	O
requirement	O
of	O
compactness	O
.	O
The	O
size	O
of	O
the	O
selected	O
set	O
is	O
constrained	O
in	O
QA	O
tasks	O
by	O
the	O
capability	O
of	O
downstream	O
reasoning	O
models	O
and	O
practically	O
needs	O
to	O
be	O
a	O
small	O
value	O
,	O
whereas	O
it	O
is	O
not	O
the	O
case	O
in	O
SRD	B-DatasetName
.	O
To	O
achieve	O
the	O
above	O
goals	O
,	O
a	O
straightforward	O
approach	O
is	O
to	O
train	O
a	O
model	O
that	O
evaluates	O
each	O
subset	O
of	O
the	O
candidate	O
passages	O
,	O
e.g.	O
,	O
by	O
concatenating	O
passages	O
in	O
any	O
subsets	O
.	O
However	O
,	O
this	O
approach	O
is	O
highly	O
inefficient	O
since	O
it	O
requires	O
to	O
encode	O
O	O
(	O
K	O
L	O
)	O
passage	O
subsets	O
,	O
where	O
K	O
is	O
the	O
total	O
number	O
of	O
candidates	O
and	O
L	O
is	O
the	O
maximum	O
size	O
of	O
subsets	O
.	O
Thus	O
,	O
a	O
practical	O
complementary	O
evidence	O
identification	O
method	O
needs	O
to	O
be	O
computationally	O
efficient	O
.	O
This	O
is	O
especially	O
critical	O
when	O
we	O
use	O
heavy	O
models	O
like	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
and	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
,	O
where	O
passage	O
encoding	O
is	O
time	O
and	O
memory	O
consuming	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
an	O
efficient	O
method	O
to	O
select	O
a	O
set	O
of	O
spanning	O
passages	O
that	O
is	O
sufficient	O
and	O
diverse	O
.	O
The	O
core	O
idea	O
is	O
to	O
represent	O
questions	O
and	O
passages	O
in	O
a	O
vector	O
space	O
and	O
define	O
the	O
measures	O
of	O
our	O
criterion	O
in	O
the	O
vector	O
space	O
.	O
For	O
example	O
,	O
in	O
the	O
vector	O
space	O
,	O
sufficiency	O
can	O
be	O
defined	O
as	O
a	O
similarity	O
between	O
the	O
question	O
vector	O
and	O
the	O
sum	O
of	O
selected	O
passage	O
vectors	O
,	O
measured	O
by	O
a	O
cosine	O
function	O
with	O
a	O
higher	O
score	O
indicating	O
a	O
closer	O
similarity	O
;	O
and	O
diversity	O
can	O
be	O
defined	O
as	O
1	O
distance	O
between	O
each	O
pair	O
of	O
passages	O
.	O
By	O
properly	O
training	O
the	O
passage	O
encoder	O
with	O
a	O
loss	B-MetricName
function	O
derived	O
by	O
the	O
above	O
terms	O
,	O
we	O
expect	O
the	O
resulted	O
vector	O
space	O
satisfies	O
the	O
property	O
that	O
the	O
complementary	O
evidence	O
passages	O
lead	O
to	O
large	O
scores	O
.	O
In	O
addition	O
,	O
our	O
method	O
only	O
encodes	O
each	O
passage	O
in	O
the	O
candidate	O
set	O
once	O
,	O
which	O
is	O
more	O
efficient	O
than	O
the	O
naive	O
solution	O
mentioned	O
above	O
.	O
To	O
evaluate	O
the	O
proposed	O
method	O
,	O
we	O
use	O
the	O
multi	O
-	O
hop	O
QA	O
dataset	O
HotpotQA	B-DatasetName
(	O
the	O
full	O
wiki	O
setting	O
)	O
since	O
the	O
ground	O
-	O
truth	O
of	O
evidence	O
passages	O
are	O
provided	O
.	O
Experiments	O
show	O
that	O
our	O
method	O
significantly	O
improves	O
the	O
accuracy	B-MetricName
of	O
complementary	O
evidence	O
selection	O
.	O

We	O
propose	O
a	O
new	O
supervised	O
training	O
objective	O
to	O
learn	O
the	O
BERT	B-MethodName
encoder	O
for	O
QA	O
that	O
optimizes	O
the	O
previous	O
conditions	O
.	O
Note	O
that	O
in	O
this	O
work	O
we	O
assume	O
a	O
set	O
of	O
labeled	O
training	O
examples	O
are	O
available	O
,	O
i.e.	O
,	O
the	O
ground	O
truth	O
annotations	O
contain	O
complementary	O
supporting	O
paragraphs	O
.	O
Recently	O
there	O
was	O
a	O
growing	O
in	O
such	O
datasets	O
(	O
Yang	O
et	O
al	O
,	O
2018	O
;	O
Yao	O
et	O
al	O
,	O
2019	O
)	O
,	O
due	O
to	O
the	O
increasing	O
interest	O
in	O
model	O
explainability	O
.	O
Also	O
,	O
such	O
supervision	O
signals	O
can	O
also	O
be	O
obtained	O
with	O
distant	O
supervision	O
.	O
For	O
each	O
training	O
instance	O
(	O
q	O
,	O
P	O
)	O
,	O
we	O
define	O
{	O
p	O
i	O
}	O
+	O
=	O
{	O
p	O
i	O
}	O
,	O
∀i	O
{	O
i	O
|	O
p	O
i	O
P	O
+	O
}	O
(	O
1	O
)	O
{	O
p	O
i	O
}	O
−	O
=	O
{	O
p	O
i	O
}	O
,	O
∀i	O
{	O
i	O
|	O
p	O
i	O
P	O
−	O
}	O
(	O
2	O
)	O
{	O
p	O
i	O
}	O
=	O
{	O
p	O
i	O
}	O
+	O
∪	O
{	O
p	O
i	O
}	O
−	O
(	O
3	O
)	O
Denoting	O
y	O
p	O
i	O
=	O
1	O
if	O
p	O
i	O
P	O
+	O
and	O
y	O
p	O
i	O
=	O
0	B-DatasetName
if	O
p	O
i	O
P	O
−	O
,	O
we	O
have	O
the	O
following	O
training	O
objective	O
function	O
:	O
L	O
(	O
{	O
pi	O
}	O
;	O
q	O
;	O
y	O
)	O
=	O
Lsup	O
(	O
{	O
pi	O
}	O
;	O
q	O
;	O
y	O
)	O
+	O
αL	O
d	O
(	O
{	O
pi	O
}	O
+	O
)	O
+	O
βLc	O
(	O
{	O
pi	O
}	O
;	O
q	O
;	O
y	O
)	O
(	O
4	O
)	O
where	O
Lsup	O
(	O
{	O
pi	O
}	O
;	O
q	O
;	O
y	O
)	O
=	O
−	O
i	O
yp	O
i	O
log	O
(	O
f	O
(	O
pi	O
)	O
)	O
,	O
(	O
5	O
)	O
L	O
d	O
(	O
{	O
pi	O
}	O
+	O
)	O
=	O
p	O
i	O
,	O
p	O
j	O
,	O
i	O
=	O
j	O
(	O
1	O
−	O
1	O
(	O
pi	O
,	O
p	O
j	O
)	O
)	O
.	O
(	O
6	O
)	O
Lc	O
(	O
{	O
pi	O
}	O
;	O
q	O
;	O
y	O
)	O
=	O
	O
	O
	O
	O
	O
	O
	O
	O
1	O
−	O
cos	O
(	O
q	O
,	O
i	O
pi	O
)	O
,	O
if	O
Πp	O
i	O
yp	O
i	O
=	O
1	O
max	O
(	O
0	B-DatasetName
,	O
cos	O
(	O
q	O
,	O
i	O
pi	O
)	O
−	O
γ	B-HyperparameterName
)	O
,	O
if	O
Πp	O
i	O
yp	O
i	O
=	O
0	B-DatasetName
(	O
7	O
)	O
where	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
are	O
the	O
hyperparameter	O
weights	O
and	O
1	O
(	O
,	O
)	O
denotes	O
L1	O
loss	B-MetricName
between	O
two	O
input	O
vectors	O
.	O
Eq	O
5	O
is	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
corresponding	O
to	O
relevance	O
condition	O
;	O
Eq	O
6	O
regularizes	O
the	O
diversity	O
condition	O
;	O
Eq	O
7	O
is	O
the	O
cosine	O
-	O
embedding	O
loss	B-MetricName
2	O
for	O
the	O
compactness	O
condition	O
and	O
γ	B-HyperparameterName
>	O
0	B-DatasetName
is	O
the	O
margin	O
to	O
encourage	O
data	O
samples	O
with	O
better	O
question	O
coverage	O
.	O
2	O
Refer	O
to	O
CosineEmbeddingLoss	O
in	O
PyTorch	O
.	O

Score	B-MetricName
Function	O
During	O
inference	O
,	O
we	O
use	O
the	O
following	O
score	O
function	O
to	O
find	O
the	O
best	O
paragraph	O
combination	O
:	O
g	O
(	O
P	O
sel	O
;	O
q	O
;	O
{	O
pi	O
}	O
)	O
=	O
p	O
i	O
P	O
(	O
pi	O
|	O
q	O
)	O
+	O
α	B-HyperparameterName
cos	O
(	O
p	O
i	O
pi	O
,	O
q	O
)	O
+	O
β	B-HyperparameterName
p	O
i	O
,	O
p	O
j	O
,	O
i	O
=	O
j	O
1	O
(	O
pi	O
,	O
pj	O
)	O
(	O
8	O
)	O
where	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
are	O
hyperparameters	O
similar	O
to	O
Eq	O
4	O
.	O
Note	O
that	O
our	O
approach	O
requires	O
to	O
encode	O
each	O
passage	O
in	O
P	O
only	O
once	O
for	O
each	O
question	O
,	O
resulting	O
in	O
an	O
O	O
(	O
K	O
)	O
time	O
complexity	O
of	O
encoding	O
(	O
K	B-HyperparameterName
=	I-HyperparameterName
|	O
P	O
|	O
)	O
;	O
and	O
the	O
subset	O
selection	O
is	O
performed	O
in	O
the	O
vector	O
space	O
,	O
which	O
is	O
much	O
more	O
efficient	O
than	O
selecting	O
subsets	O
before	O
encoding	O
.	O
Beam	O
Search	O
In	O
a	O
real	O
-	O
world	O
application	O
,	O
there	O
is	O
usually	O
a	O
large	O
candidate	O
set	O
of	O
P	O
,	O
e.g.	O
,	O
retrieved	O
passages	O
for	O
q	O
via	O
a	O
traditional	O
IR	O
system	O
.	O
Our	O
algorithm	O
requires	O
O	O
(	O
K	O
)	O
time	O
encoding	O
,	O
and	O
O	O
(	O
K	O
L	O
)	O
time	O
scoring	O
in	O
vector	O
space	O
when	O
ranking	O
all	O
the	O
combinations	O
in	O
L	O
candidates	O
.	O
Thus	O
when	O
K	O
becomes	O
large	O
,	O
it	O
is	O
still	O
inefficient	O
even	O
when	O
L	O
=	O
2	O
.	O
We	O
resort	O
to	O
beam	O
search	O
to	O
deal	O
with	O
scenarios	O
with	O
large	O
Ks	O
.	O
The	O
details	O
can	O
be	O
found	O
in	O
Appendix	O
A.	O
3	O
Experiments	O

Baseline	O
We	O
compare	O
with	O
the	O
BERT	B-MethodName
passage	O
ranker	O
(	O
Nie	O
et	O
al	O
,	O
2019	O
)	O
that	O
is	O
commonly	O
used	O
on	O
open	O
-	O
domain	O
QA	O
including	O
HotpotQA	B-DatasetName
.	O
The	O
baseline	O
uses	O
the	O
same	O
BERT	B-MethodName
architecture	O
as	O
our	O
approach	O
described	O
in	O
Section	O
2.2	O
,	O
but	O
is	O
trained	O
with	O
only	O
the	O
relevancy	O
loss	B-MetricName
(	O
Eq	O
5	O
)	O
and	O
therefore	O
only	O
consider	O
the	O
relevancy	O
when	O
selecting	O
evidence	O
.	O
We	O
also	O
compare	O
the	O
DRN	O
model	O
from	O
(	O
Harel	O
et	O
al	O
,	O
2019	O
)	O
which	O
is	O
designed	O
for	O
the	O
SRD	B-DatasetName
task	O
.	O
Their	O
ensemble	O
system	O
first	O
finds	O
the	O
most	O
relevant	O
evidence	O
to	O
the	O
given	O
question	O
,	O
and	O
then	O
select	O
the	O
second	O
diverse	O
evidence	O
using	O
their	O
score	O
function	O
.	O
The	O
major	O
differences	O
from	O
our	O
method	O
are	O
that	O
(	O
1	O
)	O
they	O
train	O
two	O
separate	O
models	O
for	O
evidence	O
selection	O
;	O
(	O
2	O
)	O
they	O
do	O
not	O
consider	O
the	O
compactness	O
among	O
the	O
evidences	O
.	O
It	O
is	O
worth	O
mentioning	O
that	O
we	O
replace	O
their	O
LSTM	B-MethodName
encoder	O
with	O
BERT	B-MethodName
encoder	O
for	O
fair	O
comparison	O
.	O
Metric	O
During	O
the	O
evaluation	O
we	O
make	O
each	O
method	O
output	O
its	O
top	O
2	O
ranked	O
results	O
3	O
(	O
i.e.	O
the	O
top	O
1	O
ranked	O
pair	O
from	O
our	O
method	O
)	O
as	O
the	O
prediction	O
.	O
The	O
final	O
performance	O
is	O
evaluated	O
by	O
exact	B-MetricName
match	I-MetricName
(	O
EM	B-MetricName
)	O
,	O
i.e.	O
,	O
whether	O
both	O
true	O
evidence	O
passages	O
are	O
covered	O
,	O
and	O
the	O
F1	B-MetricName
score	I-MetricName
on	O
the	O
test	O
sets	O
.	O

In	O
the	O
experiments	O
,	O
we	O
have	O
M	O
=	O
3	O
,	O
N	O
=	O
4	O
for	O
MNLI	B-DatasetName
-	O
12	O
and	O
M	O
=	O
4	O
,	O
N	O
=	O
5	O
for	O
HotpotQA	B-DatasetName
-	O
50	O
with	O
our	O
method	O
.	O
The	O
values	O
are	O
selected	O
according	O
to	O
development	O
performance	O
.	O
We	O
follow	O
the	O
settings	O
and	O
hyperparameters	O
used	O
in	O
(	O
Harel	O
et	O
al	O
,	O
2019	O
)	O
for	O
the	O
DRN	O
model	O
.	O
many	O
pieces	O
of	O
true	O
evidences	O
enclosed	O
by	O
the	O
complete	O
set	O
of	O
candidate	O
passages	O
where	O
our	O
proposed	O
ranker	O
selects	O
from	O
.	O
For	O
HotpotQA	B-DatasetName
dataset	O
,	O
we	O
use	O
a	O
bi	O
-	O
gram	O
BM25	O
ranker	O
to	O
collect	O
top	O
50	O
relevant	O
passages	O
and	O
build	O
the	O
basis	O
for	O
the	O
experiments	O
4	O
,	O
which	O
inevitably	O
leads	O
some	O
of	O
the	O
true	O
evidences	O
to	O
be	O
filtered	O
out	O
and	O
makes	O
its	O
upper	O
-	O
bound	O
less	O
than	O
100	O
%	O
.	O
For	O
the	O
artificial	O
MNLI	B-DatasetName
-	O
12	O
dataset	O
,	O
all	O
the	O
true	O
evidences	O
are	O
guaranteed	O
to	O
be	O
included	O
.	O
Table	O
1	O
shows	O
that	O
our	O
method	O
achieves	O
significant	O
improvements	O
on	O
both	O
datasets	O
.	O
On	O
HotpotQA	B-DatasetName
-	O
50	O
,	O
all	O
systems	O
have	O
low	O
EM	B-MetricName
scores	O
,	O
because	O
of	O
the	O
relatively	O
low	O
recall	O
of	O
the	O
BM25	O
retrieval	O
.	O
Only	O
35.49	O
%	O
of	O
the	O
samples	O
in	O
the	O
test	O
set	O
contain	O
both	O
ground	O
-	O
truth	O
evidence	O
passages	O
.	O
On	O
MNLI	B-DatasetName
-	O
12	O
,	O
the	O
EM	B-MetricName
score	O
is	O
around	O
50	O
%	O
.	O
This	O
is	O
mainly	O
because	O
the	O
segments	O
are	O
usually	O
much	O
shorter	O
than	O
a	O
paragraph	O
,	O
with	O
an	O
average	O
length	O
of	O
7	O
words	O
.	O
Therefore	O
it	O
is	O
more	O
challenging	O
in	O
matching	O
the	O
q	O
with	O
the	O
p	O
i	O
s.	O
Specifically	O
,	O
both	O
our	O
method	O
and	O
the	O
BERT	B-MethodName
baseline	O
surpass	O
the	O
DRN	O
model	O
on	O
all	O
datasets	O
and	O
metrics	O
,	O
which	O
results	O
from	O
our	O
question	O
-	O
conditioned	O
passage	O
encoding	O
approach	O
.	O
Our	O
defined	O
vector	O
space	O
proves	O
beneficial	O
to	O
model	O
the	O
complementation	O
among	O
the	O
evidence	O
with	O
respect	O
to	O
a	O
given	O
question	O
.	O
The	O
ablation	O
study	O
of	O
our	O
loss	B-MetricName
function	O
further	O
illustrates	O
that	O
the	O
diversity	O
and	O
the	O
compactness	O
terms	O
efficiently	O
bring	O
additional	O
20%/30	O
%	O
increase	O
in	O
EM	B-MetricName
score	O
on	O
two	O
datasets	O
and	O
consequently	O
raise	O
the	O
F1	B-MetricName
score	I-MetricName
by	O
about	O
8/6	O
absolute	O
points	O
.	O
Figure	O
2	O
gives	O
examples	O
about	O
how	O
our	O
model	O
improves	O
over	O
the	O
baseline	O
.	O
Our	O
method	O
can	O
successfully	O
select	O
complementary	O
passages	O
while	O
the	O
baselines	O
only	O
select	O
passages	O
that	O
look	O
similar	O
to	O
the	O
question	O
.	O
A	O
more	O
interesting	O
example	O
is	O
given	O
at	O
the	O
bottom	O
where	O
the	O
top	O
-	O
50	O
only	O
covers	O
one	O
supporting	O
passage	O
.	O
The	O
BERT	B-MethodName
baseline	O
selects	O
two	O
incorrect	O
passages	O
that	O
cover	O
identical	O
part	O
of	O
facts	O
required	O
by	O
the	O
question	O
and	O
similarly	O
the	O
DRN	O
baseline	O
select	O
a	O
relevant	O
evidence	O
and	O
an	O
irrelevant	O
evidence	O
,	O
while	O
our	O
method	O
scores	O
lower	O
the	O
second	O
passage	O
that	O
does	O
not	O
bring	O
new	O
information	O
,	O
and	O
reaches	O
a	O
supporting	O
selection	O
.	O
A	O
similar	O
situation	O
contributes	O
to	O
the	O
majority	O
of	O
improvement	O
on	O
one	O
-	O
supporting	O
-	O
evidence	O
data	O
sample	O
in	O
HotpotQA	B-DatasetName
-	O
50	O
.	O
Inference	O
Speed	O
Our	O
beam	O
search	O
with	O
score	O
function	O
brings	O
slight	O
overheads	O
to	O
the	O
running	O
time	O
.	O
On	O
HotpotQA	B-DatasetName
-	O
50	O
,	O
it	O
takes	O
1	O
,	O
990	O
milliseconds	O
(	O
ms	O
)	O
on	O
average	O
to	O
obtain	O
the	O
embeddings	O
of	O
all	O
passages	O
for	O
one	O
data	O
sample	O
whereas	O
our	O
vector	O
-	O
based	O
complementary	O
selection	O
only	O
adds	O
an	O
extra	O
2	O
ms	O
which	O
can	O
be	O
negligible	O
compared	O
to	O
the	O
encoding	O
time	O
.	O

The	O
latest	O
dense	O
retrieval	O
methods	O
(	O
Lee	O
et	O
al	O
,	O
2019	O
;	O
Karpukhin	O
et	O
al	O
,	O
2020	O
;	O
Guu	O
et	O
al	O
,	O
2020	O
)	O
show	O
promising	O
results	O
on	O
efficient	O
inference	O
on	O
the	O
full	O
set	O
of	O
Wikipedia	O
articles	O
,	O
which	O
allows	O
to	O
skip	O
the	O
initial	O
standard	O
BM25	O
retrieval	O
and	O
avoid	O
the	O
significant	O
loss	B-MetricName
during	O
the	O
pre	O
-	O
processing	O
step	O
.	O
Our	O
proposed	O
approach	O
is	O
able	O
to	O
directly	O
cooperate	O
with	O
these	O
methods	O
as	O
we	O
all	O
work	O
in	O
the	O
vector	O
space	O
.	O
Therefore	O
,	O
the	O
extension	O
to	O
dense	O
retrieval	O
can	O
be	O
naturally	O
the	O
next	O
step	O
of	O
our	O
work	O
.	O

In	O
the	O
paper	O
,	O
we	O
propose	O
a	O
new	O
problem	O
of	O
complementary	O
evidence	O
identification	O
and	O
define	O
the	O
criterion	O
of	O
complementary	O
evidence	O
in	O
vector	O
space	O
.	O
We	O
further	O
design	O
an	O
algorithm	O
and	O
a	O
loss	B-MetricName
function	O
to	O
support	O
efficient	O
training	O
and	O
inference	O
for	O
complementary	O
evidence	O
selection	O
.	O
Compared	O
to	O
the	O
baseline	O
,	O
our	O
approach	O
improves	O
more	O
than	O
20	O
%	O
and	O
remains	O
to	O
scale	O
well	O
to	O
the	O
computationally	O
complex	O
cases	O
.	O
In	O
both	O
examples	O
,	O
the	O
DRN	O
baseline	O
first	O
finds	O
the	O
most	O
relevant	O
evidence	O
to	O
the	O
question	O
(	O
left	O
)	O
and	O
then	O
select	O
a	O
diverse	O
one	O
(	O
right	O
)	O
;	O
the	O
BERT	B-MethodName
baseline	O
model	O
selected	O
the	O
top	O
-	O
2	O
most	O
relevant	O
passages	O
(	O
P1	O
,	O
P2	O
)	O
to	O
the	O
question	O
regardless	O
of	O
their	O
complementation	O
;	O
whereas	O
our	O
model	O
made	O
the	O
selection	O
(	O
P1	O
,	O
P3	B-DatasetName
)	O
with	O
consideration	O
of	O
both	O
relevance	O
and	O
evidence	O
sufficiency	O
.	O
Note	O
that	O
,	O
in	O
the	O
bottom	O
example	O
,	O
one	O
of	O
the	O
ground	O
-	O
truth	O
supporting	O
passages	O
and	O
the	O
answer	O
were	O
excluded	O
when	O
building	O
the	O
dataset	O
.	O

Advances	O
in	O
machine	O
learning	O
methods	O
and	O
the	O
release	O
of	O
annotated	O
datasets	O
of	O
clinical	O
texts	O
(	O
Uzuner	O
et	O
al	O
,	O
2011	O
;	O
Styler	O
IV	O
et	O
al	O
,	O
2014	O
)	O
in	O
the	O
past	O
decade	O
has	O
led	O
to	O
an	O
increase	O
of	O
available	O
clinical	O
NLP	O
systems	O
for	O
interesting	O
tasks	O
.	O
Recent	O
advances	O
in	O
pre	O
-	O
trained	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
have	O
made	O
ever	O
more	O
accurate	O
clinical	O
NLP	O
systems	O
possible	O
.	O
Unsupervised	B-TaskName
domain	I-TaskName
adaptation	I-TaskName
algorithms	O
(	O
e.g.	O
,	O
Ziser	O
and	O
Reichart	O
(	O
2019	O
)	O
)	O
have	O
made	O
it	O
possible	O
to	O
reduce	O
performance	O
degradation	O
when	O
applying	O
trained	O
models	O
to	O
new	O
domains	O
.	O
The	O
great	O
promise	O
of	O
these	O
developments	O
is	O
that	O
these	O
methods	O
can	O
be	O
combined	O
into	O
pipelines	O
that	O
allow	O
for	O
sophisticated	O
information	O
extraction	O
capabilities	O
for	O
downstream	O
clinical	O
use	O
cases	O
.	O
Rather	O
than	O
building	O
one	O
-	O
off	O
datasets	O
for	O
each	O
complex	O
downstream	O
task	O
that	O
arises	O
,	O
standard	O
NLP	O
components	O
could	O
potentially	O
be	O
used	O
as	O
"	O
Lego	O
"	O
-	O
style	O
building	O
blocks	O
that	O
allow	O
for	O
flexibly	O
approaching	O
new	O
tasks	O
as	O
they	O
arise	O
.	O
However	O
,	O
the	O
existence	O
of	O
the	O
building	O
blocks	O
alone	O
does	O
not	O
solve	O
this	O
problem	O
.	O
Combining	O
individual	O
components	O
into	O
NLP	O
pipelines	O
can	O
lead	O
to	O
cascading	O
errors	O
(	O
Finkel	O
et	O
al	O
,	O
2006	O
)	O
.	O
The	O
true	O
error	O
rate	O
for	O
structured	O
extraction	O
tasks	O
is	O
potentially	O
as	O
high	O
as	O
the	O
sum	O
of	O
the	O
component	O
tasks	O
'	O
errors	O
.	O
For	O
example	O
,	O
if	O
the	O
goal	O
is	O
to	O
extract	O
normalized	O
concepts	O
with	O
assertion	O
status	O
,	O
the	O
concept	O
error	O
can	O
come	O
from	O
normalization	O
error	O
,	O
negation	B-TaskName
detection	I-TaskName
error	O
,	O
uncertainty	O
detection	O
error	O
,	O
etc	O
,	O
and	O
the	O
errors	O
may	O
not	O
be	O
correlated	O
.	O
These	O
problems	O
are	O
exacerbated	O
in	O
the	O
common	O
case	O
where	O
individual	O
components	O
are	O
trained	O
on	O
data	O
from	O
different	O
domains	O
,	O
and	O
tested	O
on	O
data	O
from	O
yet	O
another	O
domain	O
.	O
In	O
this	O
work	O
,	O
we	O
quantitatively	O
examine	O
the	O
issues	O
described	O
above	O
in	O
the	O
context	O
of	O
extracting	O
drug	O
temporality	O
signatures	O
,	O
with	O
the	O
goal	O
of	O
understanding	O
drug	O
start	O
and	O
stop	O
events	O
.	O
We	O
approach	O
this	O
task	O
with	O
the	O
combination	O
of	O
three	O
sub	O
-	O
tasks	O
:	O
1	O
)	O
the	O
temporal	O
relation	O
of	O
these	O
mentions	O
to	O
the	O
document	O
creation	O
time	O
(	O
DocTimeRel	O
)	O
,	O
2	O
)	O
negation	O
status	O
of	O
the	O
mention	O
,	O
and	O
3	O
)	O
aspectual	O
link	O
relations	O
of	O
the	O
mention	O
(	O
e.g.	O
,	O
is	O
it	O
being	O
described	O
as	O
starting	O
or	O
stopping	O
)	O
.	O
Figure	O
1	O
shows	O
an	O
example	O
sentence	O
with	O
a	O
drug	O
mention	O
,	O
that	O
demonstrates	O
how	O
the	O
three	O
tasks	O
work	O
together	O
to	O
establish	O
the	O
status	O
of	O
that	O
drug	O
in	O
that	O
patient	O
.	O
Successfully	O
solving	O
this	O
task	O
is	O
beneficial	O
for	O
understanding	O
patient	O
treatment	O
course	O
,	O
and	O
enabling	O
more	O
causal	O
understanding	O
in	O
important	O
tasks	O
such	O
as	O
adverse	O
drug	O
event	B-TaskName
detection	I-TaskName
or	O
relating	O
medication	O
courses	O
to	O
outcomes	O
.	O
We	O
first	O
set	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
benchmarks	O
for	O
three	O
tasks	O
on	O
the	O
THYME	O
corpus	O
by	O
fine	O
-	O
tuning	O
large	O
pre	O
-	O
trained	O
transformer	O
models	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
then	O
examine	O
how	O
the	O
performance	O
of	O
individual	O
systems	O
degrades	O
when	O
moving	O
from	O
the	O
training	O
data	O
to	O
our	O
target	O
data	O
(	O
a	O
pediatric	O
cardiology	O
cohort	O
)	O
,	O
and	O
how	O
the	O
overall	O
system	O
performs	O
when	O
combining	O
multiple	O
systems	O
Additionally	O
,	O
as	O
the	O
patient	O
has	O
preserved	O
ejection	O
fraction	O
,	O
no	O
prior	O
history	O
of	O
embolic	O
phenomena	O
,	O
and	O
no	O
significant	O
valvular	O
disease	O
,	O
it	O
would	O
be	O
acceptable	O
for	O
him	O
to	O
remain	O
off	O
e	O
Coumadin	O
/e	O
for	O
the	O
interim	O
.	O
Figure	O
1	O
:	O
An	O
example	O
sentence	O
with	O
highlighted	O
drug	O
name	O
Coumadin	O
to	O
be	O
classified	O
for	O
all	O
three	O
tasks	O
.	O
The	O
gold	O
standard	O
has	O
this	O
drug	O
mention	O
classified	O
as	O
negated	O
,	O
with	O
DocTimeRel	O
=	O
OVERLAP	O
,	O
and	O
ALINK	O
=	O
CONTINUES	O
.	O
These	O
three	O
facts	O
can	O
be	O
used	O
to	O
understand	O
that	O
the	O
patient	O
is	O
not	O
on	O
the	O
drug	O
now	O
or	O
going	O
forward	O
,	O
and	O
was	O
likely	O
not	O
on	O
the	O
drug	O
prior	O
to	O
the	O
note	O
as	O
well	O
.	O
with	O
imperfect	O
performance	O
.	O
Despite	O
strong	O
individual	O
results	O
,	O
we	O
find	O
that	O
performance	O
suffers	O
immensely	O
due	O
to	O
both	O
out	O
-	O
of	O
-	O
domain	O
performance	O
losses	O
and	O
the	O
basic	O
combinatorial	O
math	O
of	O
integrating	O
outputs	O
from	O
multiple	O
systems	O
.	O
This	O
is	O
the	O
case	O
even	O
though	O
we	O
use	O
a	O
metric	O
,	O
accuracy	B-MetricName
,	O
that	O
is	O
forgiving	O
to	O
the	O
worst	O
-	O
performing	O
individual	O
model	O
.	O

It	O
is	O
both	O
formally	O
and	O
empirically	O
understood	O
that	O
classifiers	O
can	O
suffer	O
performance	O
loss	B-MetricName
when	O
the	O
test	O
data	O
is	O
drawn	O
from	O
a	O
different	O
distribution	O
than	O
the	O
training	O
data	O
(	O
sometimes	O
called	O
domain	O
shift	O
)	O
.	O
This	O
presents	O
a	O
difficult	O
challenge	O
in	O
clinical	O
NLP	O
because	O
data	O
-	O
sharing	O
limitations	O
make	O
it	O
difficult	O
to	O
create	O
large	O
and	O
diverse	O
training	O
corpora	O
.	O
As	O
a	O
result	O
,	O
domain	B-TaskName
adaptation	I-TaskName
approaches	O
have	O
been	O
applied	O
to	O
multiple	O
tasks	O
in	O
clinical	O
NLP	O
(	O
Miller	O
et	O
al	O
,	O
2017	O
;	O
Liu	O
et	O
al	O
,	O
2018	O
;	O
Hur	O
et	O
al	O
,	O
2020	O
)	O
.	O
Recent	O
work	O
in	O
the	O
general	O
domain	O
has	O
made	O
use	O
of	O
transfer	B-TaskName
learning	I-TaskName
,	O
which	O
can	O
attack	O
the	O
problem	O
of	O
domain	O
shift	O
,	O
but	O
by	O
a	O
different	O
mechanism	O
than	O
domain	B-TaskName
adaptation	I-TaskName
;	O
by	O
training	O
on	O
massive	O
corpora	O
,	O
large	O
pre	O
-	O
trained	O
models	O
both	O
learn	O
general	O
features	O
,	O
and	O
are	O
able	O
to	O
learn	O
from	O
smaller	O
new	O
datasets	O
without	O
overfitting	O
.	O
The	O
most	O
prominent	O
of	O
these	O
models	O
are	O
based	O
on	O
the	O
transformer	O
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
uses	O
a	O
transformer	O
encoder	O
,	O
and	O
has	O
shown	O
that	O
pre	O
-	O
training	O
with	O
massive	O
amounts	O
of	O
text	O
on	O
a	O
language	O
modeling	O
task	O
,	O
then	O
fine	O
-	O
tuning	O
on	O
a	O
supervised	O
task	O
of	O
interest	O
,	O
achieves	O
large	O
performance	O
gains	O
in	O
multiple	O
NLP	O
tasks	O
.	O
1	O
During	O
fine	O
-	O
tuning	O
for	O
sentence	B-TaskName
classification	I-TaskName
tasks	O
,	O
a	O
classification	O
head	O
with	O
randomly	O
initialized	O
weights	O
is	O
attached	O
to	O
a	O
special	O
sentenceinitial	O
token	O
.	O
Fine	O
-	O
tuning	O
then	O
proceeds	O
in	O
a	O
standard	O
supervised	O
learning	O
paradigm	O
,	O
with	O
the	O
goal	O
of	O
learning	O
the	O
weights	O
of	O
the	O
classification	O
head	O
,	O
but	O
where	O
the	O
weights	O
of	O
all	O
of	O
the	O
transformer	O
encoder	O
layers	O
can	O
also	O
be	O
updated	O
.	O
We	O
use	O
RoBERTabase	O
,	O
a	O
12	O
-	O
layer	O
transformer	O
encoder	O
that	O
provides	O
excellent	O
performance	O
but	O
manageable	O
memory	O
utilization	O
for	O
our	O
hardware	O
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
bigger	O
vision	O
of	O
our	O
current	O
work	O
is	O
extracting	O
temporally	O
-	O
aware	O
medication	O
mentions	O
from	O
electronic	O
health	O
records	O
data	O
.	O
This	O
would	O
enable	O
important	O
downstream	O
tasks	O
including	O
automatically	O
extracting	O
drug	O
timelines	O
to	O
correlate	O
with	O
treatments	O
,	O
or	O
extracting	O
better	O
causal	O
information	O
about	O
drugs	O
and	O
potential	O
adverse	O
events	O
.	O
Some	O
other	O
recent	O
work	O
has	O
also	O
examined	O
this	O
topic	O
(	O
Ramirez	O
et	O
al	O
,	O
2019	O
)	O
,	O
but	O
focused	O
on	O
a	O
single	O
drug	O
class	O
(	O
proton	O
pump	O
inhibitors	O
)	O
,	O
was	O
limited	O
to	O
the	O
problem	O
list	O
section	O
,	O
and	O
made	O
the	O
assumption	O
that	O
missing	O
drug	O
implied	O
drug	O
stoppage	O
.	O

This	O
is	O
the	O
task	O
of	O
finding	O
whether	O
a	O
given	O
event	O
is	O
being	O
negated	O
(	O
e.g.	O
,	O
statins	O
is	O
negated	O
in	O
not	O
currently	O
on	O
statins	O
)	O
.	O
We	O
model	O
this	O
as	O
a	O
spanin	O
-	O
context	O
classification	O
-	O
given	O
a	O
sentence	O
in	O
a	O
document	O
with	O
a	O
marked	O
event	O
span	O
,	O
classify	O
that	O
span	O
as	O
being	O
negated	O
or	O
not	O
negated	O
.	O
We	O
experiment	O
with	O
two	O
different	O
machine	O
learning	O
models	O
.	O
The	O
first	O
is	O
a	O
classical	O
feature	O
-	O
based	O
support	B-MethodName
vector	I-MethodName
machine	I-MethodName
that	O
is	O
the	O
default	O
model	O
of	O
Apache	O
cTAKES	O
(	O
Savova	O
et	O
al	O
,	O
2010	O
)	O
.	O
Features	O
include	O
bag	O
of	O
words	O
and	O
part	O
of	O
speech	O
tags	O
in	O
and	O
around	O
the	O
event	O
,	O
negation	O
cue	O
words	O
from	O
lists	O
and	O
their	O
relation	O
to	O
the	O
event	O
,	O
and	O
dependency	O
parse	O
features	O
that	O
relate	O
negation	O
cue	O
words	O
to	O
events	O
.	O
Details	O
of	O
this	O
system	O
were	O
presented	O
by	O
Wu	O
et	O
al	O
(	O
2014	O
)	O
.	O
For	O
comparison	O
we	O
train	O
a	O
RoBERTa	B-MethodName
-	O
based	O
system	O
,	O
where	O
the	O
input	O
representation	O
is	O
the	O
sentence	O
with	O
special	O
tokens	O
indicating	O
the	O
event	O
to	O
be	O
classified	O
.	O
We	O
put	O
a	O
binary	O
sigmoid	O
layer	O
as	O
the	O
output	O
,	O
with	O
the	O
"	O
[	O
CLS	O
]	O
"	O
token	O
representation	O
from	O
the	O
final	O
layer	O
as	O
the	O
classifier	O
input	O
,	O
and	O
fine	O
-	O
tune	O
the	O
entire	O
model	O
.	O
Hyperparameters	O
such	O
as	O
learning	B-HyperparameterName
rate	I-HyperparameterName
and	O
number	O
of	O
training	O
epochs	O
are	O
optimized	O
on	O
the	O
THYME	O
colon	O
development	O
set	O
.	O
Our	O
implementation	O
uses	O
the	O
Huggingface	O
Transformers	O
library	O
(	O
Wolf	O
et	O
al	O
,	O
2019	O
)	O
.	O

The	O
tasks	O
described	O
above	O
are	O
trained	O
on	O
a	O
single	O
source	O
dataset	O
,	O
and	O
must	O
be	O
combined	O
into	O
a	O
pipeline	O
that	O
will	O
run	O
on	O
data	O
from	O
a	O
different	O
target	O
distribution	O
.	O
To	O
adapt	O
to	O
the	O
target	O
domain	O
,	O
we	O
use	O
unsupervised	B-TaskName
domain	I-TaskName
adaptation	I-TaskName
methods	O
,	O
where	O
we	O
have	O
access	O
to	O
only	O
unlabeled	O
target	O
examples	O
.	O
Since	O
large	O
pre	O
-	O
trained	O
transformer	O
models	O
have	O
arrived	O
,	O
they	O
have	O
been	O
shown	O
to	O
be	O
quite	O
robust	O
to	O
out	O
-	O
of	O
-	O
distribution	O
examples	O
(	O
Hendrycks	O
et	O
al	O
,	O
2020	O
)	O
,	O
including	O
on	O
clinical	O
tasks	O
(	O
Lin	O
et	O
al	O
,	O
2020	O
)	O
,	O
where	O
it	O
was	O
shown	O
that	O
adding	O
domain	B-TaskName
adaptation	I-TaskName
layers	O
on	O
top	O
of	O
BERT	B-MethodName
was	O
no	O
better	O
than	O
BERT	B-MethodName
itself	O
for	O
negation	B-TaskName
detection	I-TaskName
.	O
One	O
of	O
the	O
few	O
effective	O
methods	O
for	O
improving	O
the	O
out	O
-	O
of	O
-	O
distribution	O
performance	O
of	O
pre	O
-	O
trained	O
transformer	O
models	O
has	O
been	O
to	O
continue	O
to	O
pre	O
-	O
train	O
the	O
language	O
modeling	O
objective	O
on	O
the	O
target	O
domain	O
data	O
,	O
before	O
any	O
fine	O
-	O
tuning	O
is	O
done	O
on	O
the	O
source	O
data	O
(	O
Han	O
and	O
Eisenstein	O
,	O
2019	O
;	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
focus	O
on	O
that	O
method	O
,	O
since	O
this	O
is	O
currently	O
the	O
most	O
promising	O
direction	O
for	O
adapting	O
large	O
pre	O
-	O
trained	O
transformers	O
.	O
Specifically	O
,	O
to	O
use	O
this	O
method	O
,	O
we	O
run	O
additional	O
masked	O
language	O
model	O
training	O
steps	O
on	O
the	O
target	O
training	O
data	O
from	O
the	O
RoBERTa	B-MethodName
-	O
base	O
checkpoint	O
,	O
before	O
fine	O
-	O
tuning	O
on	O
the	O
labeled	O
colon	O
cancer	O
data	O
,	O
and	O
then	O
testing	O
on	O
target	O
test	O
data	O
.	O
We	O
tune	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
for	O
the	O
language	O
model	O
pre	O
-	O
training	O
on	O
target	O
development	O
set	O
data	O
,	O
optimizing	O
for	O
perplexity	B-MetricName
.	O

For	O
the	O
three	O
tasks	O
of	O
interest	O
,	O
we	O
evaluate	O
indomain	O
(	O
THYME	O
colon	O
cancer	O
corpus	O
)	O
,	O
as	O
well	O
as	O
one	O
closely	O
related	O
out	O
-	O
of	O
-	O
domain	O
corpus	O
(	O
THYME	O
brain	O
cancer	O
corpus	O
)	O
.	O
We	O
also	O
use	O
a	O
second	O
out	O
-	O
ofdomain	O
corpus	O
,	O
an	O
internal	O
data	O
set	O
we	O
annotated	O
for	O
all	O
three	O
tasks	O
(	O
pulmonary	O
hypertension	O
[	O
PH	O
]	O
notes	O
)	O
.	O
This	O
annotation	O
was	O
performed	O
by	O
an	O
experienced	O
annotator	O
who	O
has	O
worked	O
on	O
clinical	O
annotation	O
projects	O
in	O
the	O
past	O
.	O
We	O
measure	O
performance	O
on	O
negation	O
with	O
F1score	O
,	O
on	O
DocTimeRel	O
with	O
accuracy	B-MetricName
(	O
because	O
the	O
classes	O
are	O
relatively	O
balanced	O
)	O
,	O
and	O
on	O
ALINK	O
extraction	O
with	O
the	O
average	B-MetricName
F1	I-MetricName
score	O
of	O
all	O
categories	O
,	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
(	O
because	O
the	O
high	O
frequency	O
NONE	O
label	O
makes	O
accuracy	B-MetricName
uninformative	O
)	O
.	O
In	O
addition	O
to	O
system	O
-	O
level	O
performance	O
,	O
we	O
report	O
an	O
evaluation	O
of	O
mention	O
-	O
level	O
accuracy	B-MetricName
:	O
an	O
event	O
is	O
counted	O
as	O
correct	O
if	O
all	O
three	O
systems	O
made	O
the	O
correct	O
prediction	O
,	O
and	O
we	O
report	O
the	O
percentage	O
of	O
events	O
that	O
were	O
correct	O
.	O
This	O
setting	O
estimates	O
how	O
usable	O
the	O
entire	O
pipeline	O
is	O
,	O
given	O
different	O
system	O
settings	O
.	O
The	O
"	O
Colon	O
"	O
columns	O
of	O
Table	O
1	O
show	O
results	O
on	O
the	O
THYME	O
colon	O
cancer	O
data	O
(	O
in	O
-	O
domain	O
)	O
.	O
RoBERTa	B-MethodName
performance	O
is	O
stronger	O
than	O
the	O
SVM	B-MethodName
on	O
all	O
three	O
tasks	O
.	O
Negation	O
performance	O
is	O
particularly	O
strong	O
,	O
though	O
we	O
are	O
not	O
aware	O
of	O
any	O
reported	O
results	O
on	O
this	O
dataset	O
to	O
compare	O
against	O
.	O
DocTimeRel	O
performance	O
is	O
3	O
points	O
better	O
than	O
the	O
best	O
result	O
of	O
Clinical	O
TempEval	O
2016	O
.	O
ALINK	O
scores	O
are	O
lower	O
than	O
the	O
other	O
tasks	O
,	O
though	O
again	O
there	O
are	O
no	O
published	O
comparisons	O
.	O
It	O
is	O
likely	O
this	O
is	O
a	O
more	O
difficult	O
task	O
,	O
in	O
particular	O
because	O
the	O
RE	O
-	O
INITIATES	O
category	O
has	O
relatively	O
few	O
examples	O
and	O
whose	O
low	O
performance	O
skews	O
the	O
averaging	O
of	O
the	O
macro	B-MetricName
-	I-MetricName
F1	I-MetricName
.	O
The	O
"	O
Brain	O
"	O
and	O
"	O
PH	O
"	O
columns	O
of	O
Table	O
1	O
show	O
out	O
-	O
of	O
-	O
domain	O
performance	O
of	O
the	O
same	O
systems	O
on	O
the	O
THYME	O
brain	O
cancer	O
and	O
our	O
internal	O
pulmonary	O
hypertension	O
data	O
,	O
respectively	O
.	O
On	O
THYME	O
brain	O
cancer	O
data	O
,	O
RoBERTa	B-MethodName
again	O
out	O
-	O
performs	O
SVM	B-MethodName
substantially	O
on	O
all	O
sub	O
-	O
tasks	O
,	O
but	O
surprisingly	O
the	O
SVM	B-MethodName
performs	O
better	O
on	O
PH	O
data	O
for	O
negation	O
and	O
DocTimeRel	O
.	O
Adapting	O
the	O
RoBERTa	B-MethodName
model	O
(	O
RoBERTa+LM	O
)	O
by	O
performing	O
additional	O
language	O
modeling	O
in	O
the	O
target	O
domain	O
before	O
fine	O
-	O
tuning	O
on	O
colon	O
cancer	O
data	O
leads	O
to	O
gains	O
only	O
on	O
DocTimeRel	O
for	O
the	O
PH	O
data	O
and	O
on	O
ALINK	O
for	O
both	O
corpora	O
.	O
However	O
,	O
the	O
improvement	O
to	O
DocTimeRel	O
from	O
adapting	O
RoBERTa	B-MethodName
still	O
leaves	O
it	O
worse	O
off	O
than	O
the	O
SVM	B-MethodName
.	O
Mention	O
level	O
accuracy	B-MetricName
(	O
"	O
All	O
"	O
column	O
)	O
is	O
good	O
for	O
the	O
in	O
-	O
domain	O
data	O
(	O
THYME	O
colon	O
cancer	O
)	O
,	O
but	O
drops	O
off	O
substantially	O
even	O
for	O
the	O
THYME	O
brain	O
cancer	O
corpus	O
from	O
the	O
same	O
institution	O
,	O
created	O
with	O
the	O
same	O
guidelines	O
and	O
using	O
the	O
same	O
annotators	O
.	O
The	O
mention	O
level	O
accuracy	B-MetricName
for	O
our	O
internal	O
PH	O
data	O
is	O
unusable	O
at	O
an	O
accuracy	B-MetricName
of	O
0.506	O
with	O
RoBERTa+LM	O
.	O
This	O
accuracy	B-MetricName
means	O
that	O
roughly	O
one	O
of	O
every	O
two	O
drug	O
mentions	O
will	O
have	O
at	O
least	O
one	O
of	O
its	O
attributes	O
classified	O
incorrectly	O
.	O

NER	B-TaskName
We	O
select	O
a	O
subset	O
of	O
the	O
following	O
two	O
NER	B-TaskName
tasks	O
,	O
CoNLL	O
-	O
2002NER	O
(	O
Sang	O
,	O
2002	O
and	O
CoNLL	O
-	O
2003	O
NER	B-TaskName
(	O
Sang	O
andDe	O
Meulder	O
,	O
2003	O
)	O
,	O
to	O
form	O
this	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
NER	I-TaskName
dataset	O
.	O
It	O
covers	O
4	O
languages	O
,	O
including	O
English	O
,	O
German	O
,	O
Spanish	O
and	O
Dutch	O
,	O
and	O
4	O
types	O
of	O
named	O
entities	O
,	O
including	O
Person	O
,	O
Location	O
,	O
Organization	O
and	O
Miscellaneous	B-TaskName
entities	O
that	O
do	O
not	O
belong	O
to	O
the	O
previous	O
three	O
types	O
.	O
F1	B-MetricName
score	I-MetricName
is	O
used	O
as	O
the	O
metric	O
.	O
POS	O
Tagging	O
(	O
POS	O
)	O
Following	O
(	O
Kim	O
et	O
al	O
,	O
2017	O
)	O
,	O
we	O
select	O
a	O
subset	O
of	O
Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
UD	B-DatasetName
)	O
Treebanks	O
(	O
v2.5	O
)	O
(	O
Zeman	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
covers	O
18	O
languages	O
.	O
Accuracy	B-MetricName
(	O
ACC	B-MetricName
)	O
of	O
the	O
predicted	O
POS	O
tags	O
is	O
used	O
as	O
the	O
metric	O
.	O
News	B-TaskName
Classification	I-TaskName
(	O
NC	O
)	O
This	O
task	O
aims	O
to	O
predict	O
the	O
category	O
given	O
a	O
news	O
article	O
.	O
It	O
covers	O
5	O
languages	O
,	O
including	O
English	O
,	O
Spanish	O
,	O
French	O
,	O
German	O
and	O
Russian	O
.	O
Each	O
labeled	O
instance	O
is	O
a	O
3	O
-	O
tuple	O
:	O
<	O
news	O
title	O
,	O
news	O
body	O
,	O
category	O
>	O
.	O
The	O
category	O
number	O
is	O
10	O
.	O
We	O
crawl	O
this	O
dataset	O
from	O
Microsoft	O
News	O
(	O
MSN	O
)	O
.	O
Accuracy	B-MetricName
(	O
ACC	B-MetricName
)	O
of	O
the	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
is	O
used	O
as	O
the	O
metric	O
.	O

MLQA	B-DatasetName
The	O
MLQA	B-DatasetName
(	O
Lewis	O
et	O
al	O
,	O
2019b	O
)	O
is	O
a	O
multilingual	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
task	O
,	O
which	O
contains	O
QA	O
annotations	O
labeled	O
in	O
7	O
languages	O
,	O
including	O
English	O
,	O
Arabic	O
,	O
German	O
,	O
Spanish	O
,	O
Hindi	O
,	O
Vietnamese	O
and	O
Chinese	O
.	O
F1	B-MetricName
score	I-MetricName
of	O
the	O
predicted	O
answers	O
is	O
used	O
as	O
the	O
metric	O
.	O
XNLI	B-DatasetName
We	O
reuse	O
the	O
original	O
XNLI	B-DatasetName
dataset	O
(	O
Conneau	O
et	O
al	O
,	O
2018	O
)	O
in	O
XGLUE	B-DatasetName
.	O
PAWS	B-DatasetName
-	I-DatasetName
X	I-DatasetName
The	O
PAWS	B-DatasetName
-	I-DatasetName
X	I-DatasetName
(	O
Yang	O
et	O
al	O
,	O
2019a	O
)	O
is	O
a	O
paraphrase	B-TaskName
identification	I-TaskName
dataset	O
,	O
which	O
extends	O
the	O
Wikipedia	O
portion	O
of	O
the	O
PAWS	B-DatasetName
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
evaluation	O
to	O
more	O
languages	O
.	O
We	O
select	O
4	O
languages	O
,	O
including	O
English	O
,	O
Spanish	O
,	O
French	O
and	O
German	O
,	O
from	O
the	O
original	O
dataset	O
and	O
use	O
them	O
in	O
XGLUE	B-DatasetName
.	O
Accuracy	B-MetricName
(	O
ACC	B-MetricName
)	O
of	O
the	O
binary	O
classification	O
is	O
used	O
as	O
the	O
metric	O
.	O
Query	O
-	O
Ad	O
Matching	O
(	O
QADSM	O
)	O
This	O
task	O
aims	O
to	O
predict	O
whether	O
an	O
advertisement	O
(	O
ad	O
)	O
is	O
relevant	O
to	O
an	O
input	O
query	O
.	O
It	O
covers	O
3	O
languages	O
,	O
including	O
English	O
,	O
French	O
and	O
German	O
.	O
Each	O
labeled	O
instance	O
is	O
a	O
4	O
-	O
tuple	O
:	O
<	O
query	O
,	O
ad	O
title	O
,	O
ad	O
description	O
,	O
label	O
>	O
.	O
The	O
label	O
indicates	O
whether	O
the	O
ad	O
is	O
relevant	O
to	O
the	O
query	O
(	O
Good	O
)	O
,	O
or	O
not	O
(	O
Bad	O
)	O
.	O
We	O
con	O
-	O
struct	O
this	O
dataset	O
based	O
on	O
Bing	O
.	O
Accuracy	B-MetricName
(	O
ACC	B-MetricName
)	O
of	O
the	O
binary	O
classification	O
is	O
used	O
as	O
the	O
metric	O
.	O
Web	O
Page	O
Ranking	O
(	O
WPR	O
)	O
This	O
task	O
aims	O
to	O
predict	O
whether	O
a	O
web	O
page	O
is	O
relevant	O
to	O
an	O
input	O
query	O
.	O
It	O
covers	O
7	O
languages	O
,	O
including	O
English	O
,	O
German	O
,	O
French	O
,	O
Spanish	O
,	O
Italian	O
,	O
Portuguese	O
and	O
Chinese	O
.	O
Each	O
labeled	O
instance	O
is	O
a	O
4	O
-	O
tuple	O
:	O
<	O
query	O
,	O
web	O
page	O
title	O
,	O
web	O
page	O
snippet	O
,	O
label	O
>	O
.	O
The	O
relevance	O
label	O
contains	O
5	O
ratings	O
:	O
Perfect	O
(	O
4	O
)	O
,	O
Excellent	O
(	O
3	O
)	O
,	O
Good	O
(	O
2	O
)	O
,	O
Fair	O
(	O
1	O
)	O
and	O
Bad	O
(	O
0	B-DatasetName
)	O
.	O
We	O
construct	O
this	O
dataset	O
based	O
on	O
Bing	O
.	O
Normalize	O
Discounted	O
Cumulative	O
Gain	O
(	O
nDCG	O
)	O
is	O
used	O
as	O
the	O
metric	O
.	O
QA	O
Matching	O
(	O
QAM	O
)	O
This	O
task	O
aims	O
to	O
predict	O
whether	O
a	O
<	O
question	O
,	O
passage	O
>	O
pair	O
is	O
a	O
QA	O
pair	O
.	O
It	O
covers	O
3	O
languages	O
,	O
including	O
English	O
,	O
French	O
and	O
German	O
.	O
Each	O
labeled	O
instance	O
is	O
a	O
3	O
-	O
tuple	O
:	O
<	O
question	O
,	O
passage	O
,	O
label	O
>	O
.	O
The	O
label	O
indicates	O
whether	O
the	O
passage	O
is	O
the	O
answer	O
of	O
the	O
question	O
(	O
1	O
)	O
,	O
or	O
not	O
(	O
0	B-DatasetName
)	O
.	O
We	O
construct	O
this	O
dataset	O
based	O
on	O
Bing	O
.	O
Accuracy	B-MetricName
(	O
ACC	B-MetricName
)	O
of	O
the	O
binary	O
classification	O
is	O
used	O
as	O
the	O
metric	O
.	O

Question	B-TaskName
Generation	I-TaskName
(	O
QG	O
)	O
This	O
task	O
aims	O
to	O
generate	O
a	O
question	O
for	O
a	O
given	O
passage	O
.	O
We	O
collect	O
<	O
passage	O
,	O
question	O
>	O
pairs	O
from	O
Bing	O
.	O
It	O
covers	O
6	O
languages	O
,	O
including	O
English	O
,	O
French	O
,	O
German	O
,	O
Spanish	O
,	O
Italian	O
and	O
Portuguese	O
.	O
BLEU	B-MetricName
-	O
4	O
score	O
is	O
used	O
as	O
the	O
metric	O
.	O
News	O
Title	O
Generation	O
(	O
NTG	O
)	O
This	O
task	O
aims	O
to	O
generate	O
a	O
proper	O
title	O
for	O
a	O
given	O
news	O
body	O
.	O
We	O
collect	O
<	O
news	O
body	O
,	O
news	O
title	O
>	O
pairs	O
from	O
Microsoft	O
News	O
(	O
MSN	O
)	O
.	O
It	O
covers	O
5	O
languages	O
,	O
including	O
German	O
,	O
English	O
,	O
French	O
,	O
Spanish	O
and	O
Russian	O
.	O
BLEU	B-MetricName
-	O
4	O
score	O
is	O
used	O
as	O
the	O
metric	O
.	O
3	O
Pre	O
-	O
train	O
Unicoder	O
for	O
Cross	O
-	O
lingual	O
Understanding	O
Tasks	O
We	O
select	O
Unicoder	O
as	O
the	O
backbone	O
model	O
.	O
Section	O
3	O
introduces	O
a	O
simplified	O
version	O
of	O
Unicoder	O
using	O
two	O
pre	O
-	O
training	O
tasks	O
(	O
MLN	O
and	O
TLM	O
)	O
for	O
cross	O
-	O
lingual	O
understanding	O
tasks	O
.	O
Section	O
4	O
describes	O
how	O
to	O
extend	O
Unicoder	O
to	O
cover	O
cross	O
-	O
lingual	O
generation	O
tasks	O
.	O
The	O
original	O
Unicoder	O
includes	O
more	O
pre	O
-	O
training	O
tasks	O
besides	O
MLM	B-DatasetName
and	O
TLM	O
.	O
But	O
to	O
keep	O
the	O
baseline	O
pre	O
-	O
trained	O
model	O
simple	O
and	O
to	O
reduce	O
the	O
experimental	O
cost	O
,	O
we	O
just	O
use	O
MLM	B-DatasetName
and	O
TLM	O
in	O
this	O
paper	O
.	O
It	O
means	O
for	O
understanding	O
tasks	O
,	O
Unicoder	O
is	O
almost	O
equal	O
to	O
XLM	B-MethodName
,	O
except	O
some	O
hyper	O
-	O
parameter	O
differences	O
.	O

Following	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
,	O
this	O
task	O
extends	O
the	O
masked	O
language	O
model	O
task	O
to	O
multiple	O
languages	O
.	O
At	O
each	O
iteration	O
,	O
a	O
batch	O
is	O
composed	O
of	O
sentences	O
sampled	O
from	O
different	O
languages	O
.	O
The	O
sampling	O
probability	O
of	O
a	O
language	O
l	O
i	O
is	O
defined	O
as	O
λ	O
l	O
i	O
=	O
p	O
α	B-HyperparameterName
l	O
i	O
/	O
l	O
i	O
p	O
α	B-HyperparameterName
l	O
i	O
,	O
where	O
p	O
l	O
i	O
is	O
the	O
percentage	O
of	O
the	O
language	O
l	O
i	O
in	O
the	O
entire	O
corpus	O
,	O
the	O
smoothing	O
factor	O
α	B-HyperparameterName
is	O
set	O
to	O
0.3	O
.	O
For	O
each	O
batch	O
,	O
we	O
randomly	O
sample	O
15	O
%	O
of	O
the	O
words	O
and	O
replace	O
them	O
with	O
(	O
i	O
)	O
a	O
special	O
symbol	O
[	O
MASK	O
]	O
,	O
(	O
ii	O
)	O
a	O
random	O
token	O
or	O
(	O
iii	O
)	O
keep	O
them	O
unchanged	O
with	O
probability	O
80	O
%	O
,	O
10	O
%	O
and	O
10	O
%	O
,	O
respectively	O
.	O
For	O
each	O
token	O
,	O
we	O
only	O
use	O
its	O
token	O
embedding	O
and	O
position	O
embedding	O
,	O
and	O
discard	O
segment	O
embedding	O
and	O
language	O
embedding	O
.	O

Following	O
Conneau	O
and	O
Lample	O
(	O
2019	O
)	O
,	O
this	O
task	O
extends	O
the	O
MLM	B-DatasetName
task	O
to	O
bilingual	O
corpus	O
.	O
Given	O
a	O
bilingual	O
sentence	O
pair	O
,	O
TLM	O
first	O
concatenates	O
them	O
into	O
a	O
single	O
sentence	O
,	O
and	O
then	O
masks	O
words	O
using	O
the	O
same	O
strategy	O
of	O
MLM	B-DatasetName
.	O
The	O
pre	O
-	O
trained	O
model	O
learns	O
to	O
recover	O
each	O
masked	O
word	O
based	O
on	O
the	O
bilingual	O
context	O
.	O
We	O
follow	O
MLM	B-DatasetName
to	O
sample	O
language	O
pairs	O
in	O
each	O
batch	O
with	O
α	B-HyperparameterName
=	O
0.3	O
.	O

Motivated	O
by	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2019a	O
)	O
,	O
xDAE	O
aims	O
to	O
predict	O
the	O
original	O
text	O
X	O
=	O
(	O
x	O
1	O
,	O
x	O
2	O
,	O
...	O
,	O
x	O
|	O
X	O
|	O
)	O
l	O
i	O
from	O
a	O
language	O
l	O
i	O
based	O
on	O
its	O
corrupted	O
form	O
c	O
(	O
X	O
)	O
,	O
where	O
c	O
(	O
X	O
)	O
is	O
a	O
noising	O
function	O
that	O
corrupts	O
an	O
input	O
text	O
X	O
as	O
its	O
output	O
.	O
Four	O
different	O
text	O
noising	O
strategies	O
for	O
c	O
(	O
)	O
are	O
explored	O
in	O
this	O
paper	O
.	O
(	O
1	O
)	O
Shuffle	O
the	O
input	O
text	O
X	O
by	O
adding	O
a	O
noise	O
α	B-HyperparameterName
∼	O
U	O
(	O
0	B-DatasetName
,	O
3	O
)	O
to	O
the	O
input	O
indices	O
and	O
then	O
re	O
-	O
ordering	O
X	O
based	O
on	O
the	O
rank	O
of	O
the	O
noised	O
indices	O
.	O
(	O
2	O
)	O
Drop	O
words	O
with	O
a	O
probability	O
of	O
0.1	O
.	O
(	O
3	O
)	O
Replace	O
10	O
%	O
of	O
the	O
input	O
words	O
in	O
X	O
with	O
the	O
[	O
MASK	O
]	O
symbol	O
.	O
(	O
4	O
)	O
Sample	O
a	O
number	O
of	O
token	O
spans	O
from	O
X	O
with	O
span	O
lengths	O
drawn	O
from	O
a	O
Poisson	O
distribution	O
(	O
λ	O
=	O
3	O
)	O
,	O
and	O
then	O
replace	O
each	O
token	O
span	O
with	O
a	O
single	O
[	O
MASK	O
]	O
token	O
.	O
Here	O
,	O
0	B-DatasetName
-	O
length	O
spans	O
correspond	O
to	O
the	O
insertion	O
of	O
[	O
MASK	O
]	O
tokens	O
.	O
Based	O
on	O
the	O
performance	O
of	O
different	O
noising	O
strategies	O
(	O
Table	O
10	O
)	O
,	O
we	O
select	O
(	O
4	O
)	O
and	O
use	O
it	O
in	O
pre	O
-	O
training	O
.	O
We	O
leave	O
finding	O
better	O
text	O
noising	O
strategies	O
for	O
future	O
work	O
.	O
We	O
train	O
Unicoder	O
using	O
this	O
task	O
by	O
maximizing	O
the	O
following	O
loss	B-MetricName
function	O
L	O
xDAE	O
:	O
L	O
xDAE	O
=	O
l	O
i	O
L	O
X	O
l	O
i	O
|	O
X	O
|	O
t=1	O
log	O
p	O
(	O
x	O
t	O
|	O
x	O
<	O
t	O
,	O
c	O
(	O
X	O
)	O
)	O
where	O
L	O
=	O
l	O
1	O
,	O
...	O
,	O
l	O
N	O
denotes	O
N	O
languages	O
,	O
X	O
is	O
an	O
instance	O
in	O
the	O
i	O
th	O
language	O
l	O
i	O
,	O
p	O
(	O
x	O
t	O
|	O
x	O
<	O
t	O
,	O
c	O
(	O
X	O
)	O
)	O
denotes	O
the	O
probability	O
of	O
generating	O
a	O
single	O
token	O
x	O
t	O
at	O
time	O
step	O
t	O
given	O
c	O
(	O
X	O
)	O
and	O
x	O
<	O
t	O
.	O

Motivated	O
by	O
ProphetNet	B-MethodName
(	O
Yan	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2020	O
)	O
,	O
xFNP	O
introduces	O
a	O
future	O
n	O
-	O
gram	O
prediction	O
mechanism	O
to	O
natural	O
language	O
generation	O
.	O
It	O
encourages	O
the	O
model	O
to	O
plan	O
for	O
the	O
future	O
tokens	O
explicitly	O
and	O
prevents	O
over	O
-	O
fitting	O
on	O
strong	O
local	O
correlations	O
.	O
Given	O
an	O
input	O
text	O
X	O
=	O
(	O
x	O
1	O
,	O
x	O
2	O
,	O
...	O
,	O
x	O
|	O
X	O
|	O
)	O
l	O
i	O
from	O
a	O
language	O
l	O
i	O
,	O
we	O
randomly	O
mask	O
k	O
token	O
spans	O
of	O
X	O
to	O
generate	O
the	O
masked	O
text	O
X	O
as	O
the	O
input	O
,	O
and	O
concatenate	O
all	O
masked	O
token	O
spans	O
into	O
Y	O
as	O
the	O
output	O
.	O
Details	O
of	O
this	O
mask	O
strategy	O
are	O
described	O
in	O
Section	O
6.1	O
.	O
After	O
this	O
,	O
xFNP	O
first	O
encodes	O
X	O
to	O
H	O
enc	O
with	O
the	O
encoder	O
:	O
H	O
enc	O
=	O
Encoder	O
(	O
X	O
)	O
Then	O
,	O
instead	O
of	O
predicting	O
the	O
next	O
token	O
only	O
at	O
each	O
time	O
step	O
,	O
xFNP	O
generates	O
n	O
future	O
tokens	O
simultaneously	O
at	O
time	O
step	O
t	O
with	O
the	O
decoder	O
:	O
p	O
(	O
y	O
t	O
|	O
y	O
<	O
t	O
,	O
X	O
)	O
,	O
...	O
,	O
p	O
(	O
y	O
t+n−1	O
|	O
y	O
<	O
t	O
,	O
X	O
)	O
=	O
Decoder	O
(	O
y	O
<	O
t	O
,	O
H	O
enc	O
)	O
Following	O
Yan	B-DatasetName
et	I-DatasetName
al	I-DatasetName
(	O
2020	O
)	O
,	O
we	O
set	O
n	O
=	O
2	O
.	O
We	O
train	O
Unicoder	O
using	O
this	O
task	O
by	O
maximizing	O
the	O
following	O
loss	B-MetricName
function	O
L	O
xF	O
N	O
P	O
:	O
L	O
xF	O
N	O
P	O
=	O
l	O
i	O
L	O
X	O
l	O
i	O
{	O
α	B-HyperparameterName
0	B-DatasetName
|	O
Y	O
|	O
t=1	O
log	O
p	O
(	O
y	O
t	O
|	O
y	O
<	O
t	O
,	O
X	O
)	O
+	O
α	B-HyperparameterName
1	O
|	O
Y	O
|	O
−1	O
t=1	O
log	O
p	O
(	O
y	O
t+1	O
|	O
y	O
<	O
t	O
,	O
X	O
)	O
}	O
where	O
X	O
and	O
Y	O
are	O
generated	O
from	O
X	O
based	O
on	O
the	O
method	O
mentioned	O
above	O
.	O
Following	O
Yan	B-DatasetName
et	I-DatasetName
al	I-DatasetName
(	O
2020	O
)	O
,	O
we	O
set	O
α	B-HyperparameterName
0	B-DatasetName
=	O
α	B-HyperparameterName
1	O
=	O
1	O
.	O

Understanding	O
Tasks	O
The	O
hyper	O
-	O
parameters	O
are	O
set	O
as	O
follows	O
:	O
768	O
hidden	O
units	O
,	O
12	O
heads	O
,	O
GELU	B-MethodName
activation	O
,	O
a	O
dropout	O
rate	O
of	O
0.1	O
,	O
512	O
max	O
input	O
length	O
,	O
12	O
layers	O
in	O
encoder	O
.	O
In	O
the	O
pre	O
-	O
training	O
stage	O
,	O
we	O
first	O
initialize	O
Unicoder	O
LC	O
with	O
XLM	B-MethodName
-	O
R	O
base	O
,	O
and	O
then	O
run	O
continue	O
pre	O
-	O
training	O
with	O
the	O
accumulated	O
8	O
,	O
192	O
batch	B-HyperparameterName
size	I-HyperparameterName
with	O
gradients	O
accumulation	O
.	O
We	O
use	O
Adam	B-MethodName
Optimizer	B-HyperparameterName
with	O
a	O
linear	O
warm	O
-	O
up	O
and	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
3e	O
-	O
5	O
.	O
We	O
select	O
different	O
understanding	O
tasks	O
randomly	O
in	O
different	O
batches	O
.	O
This	O
costed	O
12	O
days	O
on	O
16	O
V100	O
.	O
In	O
the	O
fine	O
-	O
tuning	O
stage	O
,	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
32	O
.	O
We	O
use	O
Adam	B-MethodName
Optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
warm	O
-	O
up	O
and	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
5e	O
-	O
6	O
.	O
For	O
all	O
sentence	B-TaskName
classification	I-TaskName
tasks	O
,	O
we	O
finetune	O
10	O
epochs	O
.	O
For	O
POS	O
Tagging	O
and	O
NER	B-TaskName
,	O
we	O
fine	O
-	O
tune	O
20	O
epochs	O
.	O
And	O
for	O
POS	O
Tagging	O
,	O
we	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
2e	O
-	O
5	O
.	O
For	O
MLQA	B-DatasetName
,	O
we	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
3e	O
-	O
5	O
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
to	O
12	O
and	O
train	O
2	O
epochs	O
following	O
BERT	B-MethodName
for	O
SQuAD	B-DatasetName
.	O
After	O
each	O
epoch	O
,	O
we	O
test	O
the	O
fine	O
-	O
tuned	O
model	O
on	O
the	O
dev	O
sets	O
of	O
all	O
languages	O
.	O
We	O
select	O
the	O
model	O
with	O
the	O
best	O
average	O
result	O
on	O
the	O
dev	O
sets	O
of	O
all	O
languages	O
.	O
,	O
the	O
hyper	O
-	O
parameters	O
are	O
set	O
as	O
follows	O
:	O
768	O
hidden	O
units	O
,	O
12	O
heads	O
,	O
GELU	B-MethodName
activation	O
,	O
a	O
dropout	O
rate	O
of	O
0.1	O
,	O
512	O
max	O
input	O
length	O
,	O
12	O
layers	O
in	O
encoder	O
,	O
12	O
layers	O
in	O
decoder	O
.	O

In	O
the	O
pre	O
-	O
training	O
stage	O
,	O
we	O
first	O
initialize	O
encoder	O
and	O
decoder	O
with	O
XLM	B-MethodName
-	O
R	O
,	O
and	O
then	O
run	O
continue	O
pre	O
-	O
training	O
with	O
1	O
,	O
024	O
batch	B-HyperparameterName
size	I-HyperparameterName
.	O
We	O
use	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
warm	O
-	O
up	O
and	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
2e	O
-	O
4	O
.	O
This	O
costed	O
10	O
days	O
on	O
16	O
V100	O
.	O
In	O
the	O
fine	O
-	O
tuning	O
stage	O
,	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
1024	O
.	O
We	O
use	O
Adam	B-MethodName
Optimizer	B-HyperparameterName
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
1e	O
-	O
5	O
and	O
warm	O
-	O
up	O
steps	O
2000	O
.	O
For	O
Unicoder	O
xF	O
N	O
P	O
SC	O
,	O
the	O
hyper	O
-	O
parameters	O
are	O
set	O
as	O
follows	O
:	O
1	O
,	O
024	O
hidden	O
size	O
,	O
12	O
layers	O
in	O
encoder	O
,	O
12	O
layers	O
in	O
decoder	O
,	O
512	O
max	O
input	O
length	O
.	O
In	O
the	O
pre	O
-	O
training	O
stage	O
,	O
we	O
pre	O
-	O
train	O
the	O
model	O
from	O
scratch	O
and	O
follow	O
ProphetNet	B-MethodName
(	O
Yan	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2020	O
)	O
to	O
randomly	O
mask	O
a	O
continuous	O
span	O
(	O
with	O
a	O
fixed	O
length	O
9	O
)	O
in	O
every	O
64	O
tokens	O
.	O
About	O
15	O
%	O
of	O
the	O
tokens	O
in	O
original	O
sequence	O
are	O
masked	O
in	O
this	O
step	O
.	O
We	O
use	O
a	O
special	O
symbol	O
[	O
MASK	O
]	O
to	O
replace	O
80	O
%	O
of	O
the	O
masked	O
tokens	O
,	O
keep	O
10	O
%	O
unchanged	O
,	O
and	O
random	O
replace	O
10	O
%	O
of	O
the	O
masked	O
tokens	O
.	O
We	O
set	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
to	O
1	O
,	O
024	O
,	O
training	O
steps	O
to	O
350	O
,	O
000	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
1e	O
-	O
4	O
.	O
We	O
set	O
the	O
number	O
of	O
future	O
tokens	O
n	O
to	O
2	O
.	O
In	O
the	O
fine	O
-	O
tuning	O
stage	O
,	O
we	O
use	O
Adam	B-MethodName
Optimizer	B-HyperparameterName
and	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
1e	O
-	O
4	O
.	O
We	O
set	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
to	O
64	O
and	O
the	O
warm	O
-	O
up	O
steps	O
to	O
1	O
,	O
000	O
.	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
XLM	B-MethodName
(	O
Conneau	O
and	O
Lample	O
,	O
2019	O
)	O
and	O
XLM	B-MethodName
-	O
R	O
base	O
All	O
models	O
are	O
(	O
12	O
-	O
layer	O
)	O
based	O
ones	O
.	O
Given	O
a	O
task	O
,	O
each	O
pre	O
-	O
trained	O
model	O
is	O
fine	O
-	O
tuned	O
using	O
its	O
English	O
training	O
set	O
only	O
,	O
and	O
then	O
applied	O
to	O
all	O
test	O
sets	O
in	O
different	O
languages	O
.	O
AVG	O
2	O
U	O
and	O
AVG	O
2	O
G	O
denote	O
the	O
average	O
score	O
of	O
the	O
average	O
scores	O
on	O
9	O
understanding	O
tasks	O
and	O
2	O
generation	O
tasks	O
,	O
respectively	O
.	O
12	O
-	O
layer	O
Unicoder	O
xF	O
N	O
P	O
SC	O
trained	O
on	O
Wikipedia	O
corpus	O
for	O
100	O
languages	O
.	O
Given	O
a	O
downstream	O
task	O
,	O
each	O
pre	O
-	O
trained	O
model	O
is	O
fine	O
-	O
tuned	O
using	O
its	O
English	O
training	O
set	O
and	O
then	O
applied	O
to	O
all	O
test	O
sets	O
in	O
different	O
languages	O
.	O
Note	O
that	O
,	O
all	O
results	O
are	O
reproduced	O
by	O
this	O
paper	O
,	O
except	O
the	O
XLM	B-MethodName
†	O
results	O
on	O
XNLI	B-DatasetName
are	O
from	O
Conneau	O
and	O
Lample	O
(	O
2019	O
)	O
.	O

We	O
reimplement	O
the	O
Transformer	B-MethodName
-	I-MethodName
XL	I-MethodName
model	O
in	O
Jax	O
(	O
Bradbury	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
our	O
experiments	O
,	O
we	O
employ	O
the	O
base	O
model	O
in	O
(	O
Dai	O
et	O
al	O
,	O
2019	O
)	O
,	O
except	O
that	O
we	O
increase	O
the	O
tail	O
shrinkage	O
factor	O
used	O
for	O
the	O
adaptive	B-MethodName
softmax	I-MethodName
and	O
input	O
representations	O
from	O
1	O
to	O
4	O
,	O
which	O
saves	O
63	O
%	O
of	O
the	O
parameters	O
without	O
compromising	O
the	O
performance	O
.	O
On	O
the	O
full	O
Wikitext	O
-	O
103	O
dataset	O
,	O
our	O
implementation	O
has	O
a	O
test	O
perplexity	B-MetricName
of	O
24.2	O
(	O
published	O
result	O
for	O
this	O
base	O
model	O
was	O
24.0	O
)	O
.	O
We	O
train	O
our	O
models	O
using	O
the	O
standard	O
likelihood	O
objective	O
for	O
language	O
models	O
with	O
a	O
total	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	O
on	O
8	O
V100	O
GPUs	O
.	O
Adam	B-MethodName
optimizer	B-HyperparameterName
is	O
used	O
with	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2.5	O
×	O
10	O
−4	O
,	O
which	O
decays	O
up	O
to	O
200k	O
steps	O
following	O
a	O
cosine	O
curve	O
.	O
During	O
training	O
,	O
we	O
use	O
text	O
segments	O
of	O
150	O
steps	O
and	O
a	O
memory	O
of	O
equal	O
size	O
.	O
When	O
evaluating	O
the	O
model	O
,	O
we	O
use	O
a	O
sequence	O
length	O
of	O
64	O
and	O
memory	O
size	O
640	O
.	O
Unless	O
further	O
noted	O
,	O
in	O
our	O
experiments	O
we	O
use	O
an	O
embedding	O
size	O
of	O
256	O
for	O
BoW	O
-	O
conditioned	O
models	O
.	O
For	O
other	O
models	O
,	O
we	O
project	O
each	O
node	O
or	O
edge	O
represented	O
by	O
BoW	O
to	O
an	O
embedding	O
space	O
of	O
size	O
128	O
.	O
The	O
default	O
GNN	O
we	O
use	O
has	O
a	O
single	O
linear	O
message	O
passing	O
layer	O
of	O
256	O
hidden	O
units	O
.	O

Our	O
first	O
task	O
is	O
text	B-TaskName
generation	I-TaskName
conditioned	O
on	O
the	O
graph	O
.	O
We	O
evaluate	O
model	O
performance	O
by	O
(	O
1	O
)	O
computing	O
model	O
perplexity	B-MetricName
on	O
held	O
-	O
out	O
text	O
and	O
(	O
2	O
)	O
drawing	O
samples	O
from	O
the	O
model	O
and	O
comparing	O
that	O
to	O
the	O
ground	O
truth	O
text	O
article	O
.	O
We	O
use	O
BLEU	B-MetricName
score	I-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
to	O
measure	O
the	O
similarity	O
of	O
our	O
generated	O
samples	O
to	O
the	O
ground	O
truth	O
.	O
Table	O
3	O
:	O
The	O
perplexity	B-MetricName
and	O
the	O
generated	O
text	O
reverse	O
-	O
BLEU	B-MetricName
score	I-MetricName
of	O
different	O
types	O
of	O
graph	O
-	O
conditioned	O
models	O
.	O
We	O
show	O
the	O
reverse	O
-	O
BLEU	B-MetricName
score	I-MetricName
with	O
or	O
without	O
prompting	O
the	O
original	O
title	O
at	O
the	O
start	O
of	O
the	O
text	B-TaskName
generation	I-TaskName
.	O
Unlike	O
previous	O
use	O
cases	O
for	O
BLEU	B-MetricName
score	I-MetricName
where	O
there	O
are	O
many	O
references	O
for	O
one	O
generated	O
sample	O
,	O
here	O
we	O
have	O
only	O
one	O
ground	O
truth	O
reference	O
but	O
we	O
can	O
generate	O
multiple	O
samples	O
.	O
We	O
therefore	O
simply	O
swapped	O
the	O
reference	O
with	O
the	O
samples	O
when	O
computing	O
the	O
score	O
,	O
which	O
we	O
term	O
as	O
the	O
reverse	O
-	O
BLEU	B-MetricName
(	O
rBLEU	O
)	O
.	O
We	O
have	O
also	O
tried	O
other	O
ways	O
of	O
computing	O
the	O
BLEU	B-MetricName
score	I-MetricName
and	O
find	O
that	O
they	O
do	O
n't	O
change	O
how	O
models	O
compare	O
against	O
each	O
other	O
.	O
Unless	O
explicitly	O
stated	O
,	O
we	O
let	O
the	O
model	O
sample	O
with	O
a	O
memory	O
size	O
of	O
640	O
,	O
and	O
condition	O
on	O
the	O
graphs	O
in	O
the	O
test	O
set	O
to	O
generate	O
text	O
for	O
up	O
to	O
512	O
tokens	O
per	O
sample	O
for	O
a	O
total	O
of	O
20	O
samples	O
per	O
graph	O
.	O
The	O
rBLEU	O
score	O
is	O
computed	O
based	O
on	O
these	O
samples	O
and	O
corresponding	O
ground	O
-	O
truth	O
texts	O
are	O
truncated	O
to	O
the	O
same	O
length	O
.	O
We	O
sample	O
the	O
texts	O
from	O
the	O
distribution	O
with	O
a	O
temperature	O
of	O
0.8	O
.	O
For	O
each	O
case	O
,	O
we	O
report	O
the	O
average	O
rBLEU	O
score	O
of	O
3	O
sampling	O
runs	O
.	O
We	O
find	O
the	O
variances	O
are	O
insignificant	O
which	O
do	O
not	O
affect	O
the	O
comparison	O
results	O
.	O
In	O
Appendix	O
A.3	O
we	O
also	O
report	O
results	O
for	O
generating	O
longer	O
samples	O
for	O
up	O
to	O
4096	O
tokens	O
.	O

In	O
Table	O
3	O
,	O
we	O
show	O
the	O
perplexity	B-MetricName
and	O
the	O
rBLEU	O
score	O
of	O
the	O
unconditional	O
,	O
BoW	O
,	O
nodes	O
-	O
only	O
,	O
and	O
GNN	O
conditioned	O
models	O
.	O
As	O
a	O
reference	O
,	O
a	O
standard	O
Transformer	B-MethodName
-	I-MethodName
XL	I-MethodName
model	O
trained	O
on	O
the	O
full	O
Wikitext	O
-	O
103	O
training	O
set	O
reaches	O
25.08	O
perplexity	B-MetricName
on	O
our	O
test	O
set	O
,	O
which	O
contains	O
71.7	O
%	O
of	O
the	O
original	O
test	O
articles	O
.	O
We	O
can	O
see	O
that	O
the	O
unconditional	O
,	O
i.e.	O
text	O
only	O
,	O
model	O
trained	O
on	O
our	O
dataset	O
gets	O
a	O
very	O
similar	O
performance	O
as	O
trained	O
on	O
the	O
full	O
set	O
.	O
This	O
is	O
strong	O
evidence	O
that	O
our	O
dataset	O
can	O
be	O
a	O
good	O
benchmark	O
for	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
text	O
generative	O
models	O
.	O
We	O
also	O
see	O
that	O
conditioned	O
on	O
the	O
graphs	O
,	O
model	O
perplexity	B-MetricName
did	O
n't	O
improve	O
,	O
but	O
the	O
relevance	O
of	O
the	O
samples	O
measured	O
by	O
the	O
BLEU	B-MetricName
scores	O
did	O
improve	O
significantly	O
.	O
This	O
indicates	O
that	O
the	O
graph	O
conditioned	O
models	O
can	O
indeed	O
steer	O
the	O
language	O
model	O
towards	O
more	O
relevant	O
topics	O
,	O
but	O
this	O
so	O
far	O
can	O
not	O
yet	O
improve	O
likelihood	O
metrics	O
.	O
To	O
make	O
the	O
evaluation	O
more	O
fair	O
to	O
the	O
text	O
-	O
only	O
model	O
,	O
we	O
also	O
tried	O
to	O
prompt	O
the	O
generation	O
with	O
the	O
title	O
of	O
the	O
article	O
,	O
such	O
that	O
the	O
text	O
-	O
only	O
model	O
also	O
has	O
some	O
context	O
.	O
In	O
this	O
setting	O
the	O
graph	O
models	O
are	O
still	O
better	O
,	O
showing	O
the	O
importance	O
of	O
modeling	O
the	O
structure	O
.	O
Lastly	O
,	O
among	O
all	O
the	O
3	O
graph	O
model	O
variants	O
,	O
we	O
observe	O
that	O
using	O
a	O
set	O
of	O
embeddings	O
from	O
the	O
nodes	O
model	O
is	O
better	O
than	O
using	O
a	O
single	O
embedding	O
from	O
the	O
BoW	O
model	O
,	O
and	O
fully	O
utilizing	O
the	O
graph	O
structure	O
through	O
the	O
GNN	O
model	O
is	O
consistently	O
better	O
than	O
ignoring	O
the	O
edges	O
as	O
in	O
the	O
nodes	O
model	O
.	O
However	O
the	O
differences	O
among	O
the	O
methods	O
are	O
relatively	O
small	O
.	O
For	O
visualizations	O
of	O
a	O
few	O
graphs	O
in	O
our	O
dataset	O
and	O
the	O
corresponding	O
samples	O
generated	O
based	O
on	O
them	O
please	O
refer	O
to	O
Appendix	O
A.	O

In	O
this	O
task	O
,	O
we	O
evaluate	O
the	O
possibility	O
of	O
retrieving	O
relevant	O
text	O
for	O
a	O
given	O
query	O
graph	O
.	O
We	O
pair	O
all	O
articles	O
with	O
all	O
graphs	O
in	O
the	O
test	O
set	O
,	O
resulting	O
in	O
43×43=1849	O
pairs	O
.	O
Then	O
the	O
trained	O
graphconditioned	O
language	O
models	O
are	O
used	O
to	O
produce	O
the	O
per	O
-	O
token	O
likelihood	O
of	O
each	O
pair	O
,	O
and	O
we	O
use	O
these	O
likelihood	O
scores	O
to	O
rank	O
the	O
text	O
articles	O
for	O
each	O
graph	O
.	O
We	O
expect	O
the	O
learned	O
models	O
can	O
rank	O
the	O
correct	O
pairs	O
higher	O
than	O
wrong	O
ones	O
.	O
To	O
measure	O
the	O
results	O
we	O
use	O
standard	O
ranking	O
metrics	O
including	O
recall@K	O
,	O
which	O
computes	O
the	O
fraction	O
of	O
times	O
the	O
correct	O
pair	O
is	O
included	O
in	O
the	O
top	O
K	O
predictions	O
,	O
as	O
well	O
as	O
mean	O
average	B-MetricName
precision	I-MetricName
(	O
mAP	B-MetricName
)	O
.	O
In	O
Table	O
5	O
,	O
it	O
is	O
observed	O
that	O
graph	O
-	O
conditioned	O
models	O
can	O
indeed	O
retrieve	O
more	O
relevant	O
texts	O
from	O
the	O
graph	O
than	O
the	O
unconditional	O
model	O
,	O
among	O
which	O
the	O
GNN	O
-	O
based	O
model	O
performs	O
the	O
best	O
,	O
and	O
the	O
unconditional	O
model	O
performs	O
close	O
to	O
a	O
random	O
guess	O
.	O

A	O
pattern	O
which	O
is	O
not	O
used	O
in	O
the	O
construction	O
of	O
the	O
Hearst	O
Corpus	O
is	O
used	O
here	O
:	O
Hyponym	O
Noun	O
Phrase	O
is	O
(	O
a	O
|	O
an	O
|	O
the	O
)	O
Hypernym	O
Noun	O
Phrase	O
.	O
Here	O
the	O
original	O
input	O
paragraph	O
is	O
searched	O
against	O
this	O
pattern	O
and	O
all	O
the	O
possible	O
matches	O
are	O
returned	O
in	O
the	O
form	O
of	O
hyponym	O
:	O
hypernym	O
.	O
a	O
fennel	O
is	O
a	O
plant	O
is	O
a	O
match	O
for	O
this	O
pattern	O
with	O
noun	O
phrases	O
a	O
fennel	O
and	O
plant	O
.	O
UMBC	O
Embedding	O
:	O
A	O
word	O
embedding	O
matrix	O
is	O
created	O
over	O
the	O
Normalized	O
Corpus	O
using	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
specifications	O
of	O
the	O
model	O
are	O
as	O
follows	O
:	O
(	O
a	O
)	O
Model	O
:	O
Continuous	O
Bag	O
of	O
Words	O
(	O
CBOW	O
)	O
-	O
a	O
term	O
's	O
embedding	O
value	O
is	O
determined	O
by	O
its	O
context	O
words	O
.	O
The	O
order	O
of	O
the	O
words	O
in	O
the	O
window	O
size	O
does	O
not	O
matter	O
.	O
(	O
b	O
)	O
Window	O
Size	O
:	O
10	O
.	O
The	O
context	O
window	O
size	O
for	O
a	O
term	O
which	O
determines	O
its	O
vector	O
value	O
.	O
(	O
c	O
)	O
Minimum	O
Frequency	O
Count	O
:	O
5	O
.	O
If	O
the	O
frequency	O
of	O
a	O
word	O
is	O
less	O
than	O
this	O
value	O
,	O
the	O
word	O
does	O
not	O
exist	O
in	O
the	O
embedding	O
.	O
(	O
d	O
)	O
Embedding	B-HyperparameterName
Dimension	I-HyperparameterName
Size	O
:	O
300	O
.	O
The	O
number	O
of	O
dimensions	O
for	O
the	O
embedding	O
matrix	O
.	O

Output	O
candidate	O
hypernym	O
lists	O
are	O
evaluated	O
against	O
gold	O
hypernym	O
lists	O
using	O
the	O
following	O
evaluation	O
criteria	O
:	O
Mean	O
Reciprocal	O
Rank	O
(	O
MRR	B-MetricName
)	O
,	O
Mean	O
Average	B-MetricName
Precision	I-MetricName
(	O
MAP	B-DatasetName
)	O
and	O
Precision	B-MetricName
At	O
k	O
(	O
P@k	O
)	O
,	O
where	O
k	O
is	O
1	O
,	O
3	O
,	O
5	O
and	O
15	O
.	O
We	O
ran	O
our	O
model	O
against	O
two	O
sets	O
of	O
data	O
training	O
data	O
and	O
test	O
data	O
with	O
1500	O
input	O
terms	O
each	O
.	O
These	O
results	O
are	O
shown	O
in	O
Tables	O
1	O
and	O
2	O
,	O
where	O
it	O
can	O
be	O
clearly	O
observed	O
that	O
our	O
system	O
performs	O
much	O
better	O
for	O
concepts	O
.	O
However	O
,	O
the	O
IS	B-DatasetName
-	I-DatasetName
A	I-DatasetName
module	O
seemed	O
to	O
fetch	O
good	O
candidates	O
for	O
both	O
entity	O
and	O
concept	O
data	O
.	O
The	O
gold	O
data	O
provided	O
with	O
the	O
task	O
does	O
not	O
always	O
consider	O
all	O
possible	O
word	O
senses	O
or	O
domains	O
of	O
an	O
input	O
term	O
.	O
As	O
a	O
result	O
,	O
we	O
observed	O
numerous	O
candidate	O
hypernyms	O
that	O
seem	O
to	O
be	O
plausible	O
solutions	O
that	O
are	O
not	O
considered	O
correct	O
when	O
compared	O
to	O
the	O
gold	O
data	O
.	O
For	O
example	O
,	O
the	O
input	O
concept	O
navigator	O
has	O
gold	O
standard	O
hypernyms	O
of	O
[	O
military	O
branch	O
,	O
ex	O
-	O
plorer	O
,	O
military	O
machine	O
,	O
travel	O
,	O
adventurer	O
,	O
seaman	O
]	O
.	O
Our	O
system	O
finds	O
candidate	O
hypernyms	O
[	O
browser	O
,	O
web	O
browser	O
,	O
website	O
,	O
application	O
]	O
.	O
We	O
also	O
noticed	O
that	O
due	O
to	O
our	O
normalization	O
decisions	O
(	O
i.e.	O
,	O
using	O
all	O
lower	O
-	O
case	O
characters	O
)	O
and	O
the	O
contents	O
of	O
the	O
corpus	O
,	O
Babbage	O
performs	O
poorly	O
in	O
some	O
cases	O
.	O
For	O
example	O
,	O
the	O
gold	O
hypernyms	O
for	O
input	O
entity	O
Hurricane	O
are	O
[	O
video	O
game	O
,	O
software	O
program	O
,	O
computer	O
program	O
]	O
but	O
our	O
system	O
produced	O
[	O
storm	O
,	O
windstorm	O
,	O
typhoon	O
,	O
tornado	O
,	O
cyclone	O
]	O
.	O
Clearly	O
,	O
our	O
system	O
did	O
not	O
differentiate	O
between	O
the	O
named	O
entity	O
Hurricane	O
and	O
the	O
common	O
noun	O
hurricane	O
while	O
training	O
the	O
wordembedding	O
models	O
.	O
On	O
the	O
positive	O
side	O
,	O
our	O
system	O
produced	O
promising	O
results	O
in	O
some	O
cases	O
.	O
Hyponym	O
liberalism	O
produced	O
[	O
theory	O
,	O
philosophy	O
,	O
economic	O
policy	O
]	O
which	O
is	O
very	O
similar	O
to	O
the	O
gold	O
data	O
[	O
economic	O
theory	O
,	O
theory	O
]	O
.	O
It	O
also	O
correctly	O
generated	O
the	O
hyponym	O
person	O
for	O
hypernyms	O
such	O
as	O
collector	O
,	O
moderator	O
,	O
director	O
,	O
senior	O
,	O
and	O
reporter	O
.	O
For	O
input	O
reporter	O
it	O
produced	O
[	O
writer	O
,	O
person	O
]	O
which	O
matches	O
the	O
gold	O
hypernym	O
set	O
.	O

Word	O
forms	O
are	O
ambiguous	O
,	O
and	O
derive	O
meaning	O
from	O
the	O
context	O
in	O
which	O
they	O
appear	O
.	O
For	O
example	O
,	O
the	O
form	O
"	O
bass	O
"	O
can	O
refer	O
to	O
a	O
musical	O
instrument	O
,	O
a	O
low	O
-	O
frequency	O
sound	O
,	O
a	O
type	O
of	O
voice	O
,	O
or	O
a	O
kind	O
of	O
fish	O
.	O
The	O
correct	O
reference	O
is	O
determined	O
by	O
the	O
surrounding	O
linguistic	O
context	O
.	O
Traditionally	O
,	O
this	O
kind	O
of	O
ambiguity	O
was	O
dealt	O
via	O
word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
(	O
WSD	O
)	O
,	O
a	O
task	O
that	O
disambiguates	O
word	O
forms	O
in	O
context	O
between	O
symbolic	O
sense	O
-	O
ids	O
from	O
a	O
sense	O
inventory	O
such	O
as	O
WordNet	O
(	O
Miller	O
,	O
1992	O
)	O
or	O
,	O
more	O
recently	O
,	O
BabelNet	O
(	O
Navigli	O
and	O
Ponzetto	O
,	O
2010	O
)	O
.	O
Such	O
sense	O
inventories	O
rely	O
heavily	O
on	O
manual	O
curation	O
,	O
are	O
labor	O
intensive	O
to	O
produce	O
,	O
are	O
not	O
available	O
in	O
specialized	O
domains	O
and	O
inherently	O
unsuitable	O
for	O
words	O
with	O
emerging	O
senses	O
.	O
1	O
This	O
can	O
be	O
remedied	O
by	O
word	B-TaskName
sense	I-TaskName
induction	I-TaskName
(	O
WSI	O
)	O
,	O
a	O
task	O
where	O
the	O
input	O
is	O
a	O
given	O
word	O
-	O
type	O
and	O
a	O
corpus	O
,	O
and	O
the	O
output	O
is	O
a	O
derived	O
sense	O
inventory	O
for	O
that	O
word	O
.	O
Then	O
,	O
sense	O
disambiguation	O
can	O
be	O
performed	O
over	O
the	O
WSI	O
-	O
derived	O
senses	O
.	O
The	O
introduction	O
of	O
large	O
-	O
scale	O
pre	O
-	O
trained	O
LMs	O
and	O
Masked	O
LMs	O
(	O
MLM	B-DatasetName
)	O
seemingly	O
made	O
WSI	O
/	O
WSD	O
tasks	O
obsolete	O
:	O
instead	O
of	O
representing	O
tokens	O
with	O
symbols	O
that	O
encode	O
sense	O
information	O
,	O
each	O
token	O
is	O
associated	O
with	O
a	O
contextualized	O
vector	O
embeddings	O
that	O
captures	O
various	O
aspects	O
of	O
its	O
in	O
-	O
context	O
semantics	O
,	O
including	O
the	O
word	O
-	O
sense	O
.	O
These	O
contextualized	O
vectors	O
proved	O
to	O
be	O
very	O
effective	O
as	O
features	O
for	O
downstream	O
NLP	O
tasks	O
.	O
However	O
,	O
contextualized	O
embeddings	O
also	O
have	O
some	O
major	O
shortcomings	O
:	O
most	O
notably	O
for	O
our	O
case	O
,	O
they	O
are	O
expensive	O
to	O
store	O
(	O
e.g.	O
BERT	B-MethodName
embeddings	O
are	O
768	O
or	O
1024	O
floating	O
point	O
numbers	O
for	O
each	O
token	O
)	O
,	O
and	O
are	O
hard	O
to	O
index	O
and	O
query	O
at	O
scale	O
.	O
Even	O
if	O
we	O
do	O
manage	O
to	O
store	O
and	O
query	O
them	O
,	O
they	O
are	O
not	O
interpretable	O
,	O
making	O
it	O
impossible	O
for	O
a	O
user	O
to	O
query	O
for	O
a	O
particular	O
sense	O
of	O
a	O
word	O
without	O
providing	O
a	O
full	O
disambiguating	O
context	O
for	O
that	O
word	O
.	O
For	O
example	O
,	O
consider	O
a	O
user	O
wishing	O
to	O
query	O
a	O
dataset	O
for	O
sentences	O
discussing	O
Oracle	O
in	O
the	O
mythology	O
-	O
prophet	O
sense	O
,	O
rather	O
than	O
the	O
tech	O
company	O
sense	O
.	O
It	O
is	O
not	O
clear	O
how	O
to	O
formulate	O
such	O
a	O
query	O
to	O
an	O
index	O
of	O
contextualized	O
word	O
vectors	O
.	O
However	O
,	O
it	O
is	O
trivial	O
to	O
do	O
for	O
an	O
index	O
that	O
annotates	O
each	O
token	O
with	O
its	O
derived	O
sense	O
-	O
i	O
d	O
(	O
in	O
terms	O
of	O
UI	O
,	O
after	O
a	O
user	O
issues	O
a	O
query	O
such	O
as	O
"	O
Oracle	O
"	O
,	O
the	O
system	O
may	O
show	O
a	O
prompt	O
such	O
as	O
"	O
did	O
you	O
mean	O
Oracle	O
related	O
to	O
IBM	O
;	O
Sun	O
;	O
Microsoft	O
,	O
or	O
to	O
Prophet	O
;	O
Temple	O
;	O
Queen	O
"	O
,	O
allowing	O
to	O
narrow	O
the	O
search	O
in	O
the	O
right	O
direction	O
)	O
.	O
Goldberg	O
(	O
2018	O
,	O
2019	O
)	O
show	O
how	O
contextualized	O
embeddings	O
can	O
be	O
used	O
for	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
WSI	O
results	O
.	O
The	O
core	O
idea	O
of	O
their	O
WSI	O
algorithm	O
is	O
based	O
on	O
the	O
intuition	O
,	O
first	O
proposed	O
by	O
Başkaya	O
et	O
al	O
(	O
2013	O
)	O
,	O
that	O
occurrences	O
of	O
a	O
word	O
that	O
share	O
a	O
sense	O
,	O
also	O
share	O
in	O
-	O
context	O
substitutes	O
.	O
An	O
MLM	B-DatasetName
is	O
then	O
used	O
to	O
derive	O
top	O
-	O
k	O
word	O
substitutes	O
for	O
each	O
word	O
,	O
and	O
these	O
substitutevectors	O
are	O
clustered	O
to	O
derive	O
word	O
senses	O
.	O
Our	O
main	O
contribution	O
in	O
this	O
work	O
is	O
proposing	O
a	O
method	O
that	O
scales	O
up	O
Amrami	O
and	O
Goldberg	O
(	O
2018	O
)	O
's	O
work	O
to	O
efficiently	O
annotate	O
all	O
tokens	O
in	O
a	O
large	O
corpus	O
(	O
e.g.	O
Wikipedia	O
)	O
with	O
automatically	O
derived	O
word	O
-	O
senses	O
.	O
This	O
combines	O
the	O
high	O
-	O
accuracy	B-MetricName
of	O
the	O
MLM	B-DatasetName
-	O
based	O
approach	O
,	O
with	O
the	O
symbolic	O
representation	O
provided	O
by	O
discrete	O
sense	O
annotations	O
.	O
The	O
discrete	O
annotations	O
are	O
interpretable	O
(	O
each	O
sense	O
is	O
represented	O
as	O
a	O
set	O
of	O
words	O
)	O
,	O
editable	O
,	O
indexable	O
and	O
searchable	O
using	O
standard	O
IR	O
techniques	O
.	O
We	O
show	O
two	O
applications	O
of	O
the	O
discrete	O
annotations	O
,	O
the	O
first	O
one	O
is	O
senseaware	O
information	B-TaskName
retrieval	I-TaskName
(	O
7	O
)	O
,	O
and	O
the	O
second	O
is	O
high	O
-	O
quality	O
senseful	O
static	O
word	B-TaskName
embeddings	I-TaskName
we	O
can	O
derive	O
by	O
training	O
a	O
static	O
embeddings	O
model	O
on	O
the	O
large	O
sense	O
annotated	O
corpus	O
(	O
8	O
)	O
.	O
We	O
first	O
show	O
how	O
the	O
method	O
proposed	O
by	O
Amrami	O
and	O
Goldberg	O
(	O
2018	O
)	O
can	O
be	O
adapted	O
from	O
deriving	O
senses	O
of	O
individual	O
lemmas	O
to	O
efficiently	O
and	O
cheaply	O
annotating	O
all	O
the	O
corpus	O
occurrences	O
of	O
all	O
the	O
words	O
in	O
a	O
large	O
vocabulary	O
(	O
3	O
)	O
.	O
Deriving	O
word	O
-	O
sense	O
clusters	O
for	O
all	O
of	O
English	O
Wikipedia	O
words	O
that	O
appear	O
as	O
single	O
-	O
token	O
words	O
in	O
BERT	B-MethodName
-	O
LARGE	O
's	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
vocabulary	O
,	O
and	O
assigning	O
a	O
sense	O
to	O
each	O
occurrence	O
in	O
the	O
corpus	O
,	O
required	O
100	O
hours	O
of	O
cheap	O
P100	O
GPUs	O
(	O
5	O
hours	O
of	O
wall	O
-	O
clock	O
time	O
on	O
20	O
single	O
GPU	O
machines	O
)	O
followed	O
by	O
roughly	O
4	O
hours	O
on	O
a	O
single	O
96	O
-	O
cores	O
CPU	O
machines	O
.	O
The	O
whole	O
process	O
requires	O
less	O
than	O
50	O
GB	O
of	O
disk	O
space	O
,	O
and	O
costs	O
less	O
than	O
150	O
$	O
on	O
Google	B-DatasetName
Cloud	O
platform	O
.	O
After	O
describing	O
the	O
clustering	O
algorithm	O
(	O
4	O
)	O
,	O
we	O
evaluate	O
the	O
quality	O
of	O
our	O
system	O
and	O
of	O
the	O
automatic	O
sense	O
tagging	O
using	O
SemEval	O
datasets	O
and	O
a	O
new	O
manually	O
annotated	O
dataset	O
we	O
created	O
(	O
5	O
)	O
.	O
We	O
show	O
that	O
with	O
the	O
produced	O
annotated	O
corpora	O
it	O
is	O
easy	O
to	O
serve	O
sense	O
-	O
aware	O
information	B-TaskName
retrieval	I-TaskName
applications	O
(	O
7	O
)	O
.	O
Another	O
immediate	O
application	O
is	O
feeding	O
the	O
sense	O
-	O
annotated	O
corpora	O
to	O
a	O
static	O
embedding	O
algorithm	O
such	O
as	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
,	O
for	O
deriving	O
sense	O
-	O
aware	O
static	O
embeddings	O
(	O
8	O
)	O
.	O
This	O
results	O
in	O
state	O
-	O
of	O
-	O
theart	O
sense	O
-	O
aware	O
embeddings	O
,	O
which	O
we	O
evaluate	O
both	O
on	O
an	O
existing	O
WiC	B-DatasetName
benchmark	O
(	O
Pilehvar	O
and	O
Camacho	O
-	O
Collados	O
,	O
2019	O
)	O
and	O
on	O
a	O
new	O
challenging	O
benchmark	O
which	O
we	O
create	O
(	O
9	O
)	O
.	O
In	O
contrast	O
to	O
WSD	O
which	O
relies	O
on	O
curated	O
sense	O
inventories	O
,	O
our	O
method	O
is	O
data	O
-	O
driven	O
,	O
therefore	O
resulting	O
senses	O
are	O
corpus	O
dependent	O
.	O
The	O
method	O
can	O
be	O
applied	O
to	O
any	O
domain	O
for	O
which	O
a	O
BERTlike	O
model	O
is	O
available	O
,	O
as	O
we	O
demonstrate	O
by	O
applying	O
it	O
to	O
the	O
PubMed	O
Abstracts	O
of	O
scientific	O
papers	O
,	O
using	O
SCIBERT	O
(	O
Beltagy	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
resulting	O
senses	O
cover	O
scientific	O
terms	O
which	O
are	O
not	O
typically	O
found	O
in	O
standard	O
sense	O
inventories	O
(	O
6	O
)	O
.	O
Figure	O
1	O
shows	O
examples	O
of	O
induced	O
senses	O
for	O
selected	O
words	O
from	O
the	O
English	O
Wikipedia	O
corpus	O
.	O
For	O
each	O
sense	O
we	O
list	O
5	O
communitybased	O
representatives	O
(	O
3	O
)	O
,	O
as	O
well	O
as	O
the	O
5	O
closest	O
neighbours	O
in	O
the	O
sense	O
-	O
aware	O
embedding	O
space	O
(	O
8	O
)	O
.	O
Additional	O
examples	O
are	O
available	O
in	O
Appendix	O
A.	O
Code	O
and	O
resources	O
are	O
available	O
in	O
github.com/allenai/WSIatScale	O
.	O

For	O
each	O
word	O
w	O
in	O
the	O
vocabulary	O
,	O
we	O
construct	O
a	O
graph	O
G	O
w	O
=	O
(	O
V	O
w	O
,	O
E	O
w	O
)	O
where	O
each	O
vertex	O
v	O
V	O
w	O
is	O
a	O
substitute	O
-	O
word	O
predicted	O
by	O
the	O
MLM	B-DatasetName
for	O
w	O
,	O
and	O
an	O
edge	O
(	O
u	O
,	O
v	O
)	O
E	O
w	O
connects	O
substitutes	O
that	O
are	O
predicted	O
for	O
the	O
same	O
instance	O
.	O
The	O
edge	O
is	O
weighted	O
by	O
the	O
number	O
of	O
instances	O
in	O
which	O
both	O
u	O
and	O
v	O
were	O
predicted	O
.	O
More	O
formally	O
,	O
let	O
X	O
=	O
{	O
x	O
i	O
w	O
}	O
n	O
i=1	O
bet	O
the	O
set	O
of	O
all	O
top	O
-	O
k	O
substitutes	O
for	O
n	O
instances	O
of	O
word	O
w	O
,	O
and	O
x	O
i	O
w	O
=	O
{	O
w	O
j	O
x	O
i	O
w	O
}	O
k	O
j=1	O
represents	O
the	O
k	O
top	O
substitutes	O
for	O
the	O
ith	O
instance	O
of	O
word	O
w.	O
The	O
graph	O
G	O
w	O
is	O
defined	O
as	O
follows	O
:	O
V	O
w	O
=	O
{	O
u	O
:	O
i	O
u	O
x	O
i	O
w	O
}	O
E	O
w	O
=	O
{	O
(	O
u	O
,	O
v	O
)	O
:	O
i	O
u	O
x	O
i	O
w	O
v	O
x	O
i	O
w	O
}	O
W	O
(	O
u	O
,	O
v	O
)	O
=	O
|	O
{	O
i	O
:	O
(	O
u	O
,	O
v	O
)	O
x	O
i	O
w	O
}	O
|	O
Community	B-TaskName
detection	I-TaskName
A	O
community	O
in	O
a	O
subgraph	O
corresponds	O
to	O
a	O
set	O
of	O
tokens	O
that	O
tend	O
to	O
co	O
-	O
occur	O
in	O
top	O
-	O
k	O
substitutes	O
of	O
many	O
instances	O
,	O
and	O
not	O
co	O
-	O
occur	O
with	O
top	O
-	O
k	O
substitutes	O
of	O
other	O
instances	O
.	O
This	O
corresponds	O
well	O
to	O
senses	O
and	O
we	O
take	O
community	O
's	O
nodes	O
as	O
sense	O
's	O
representatives	O
.	O
We	O
identify	O
communities	O
using	O
the	O
fast	O
"	O
Louvain	O
"	O
method	O
(	O
Blondel	O
et	O
al	O
,	O
2008	O
)	O
.	O
Briefly	O
,	O
Louvain	O
searches	O
for	O
an	O
assignment	O
of	O
nodes	O
to	O
clusters	O
such	O
that	O
the	O
modularity	O
score	O
Q	O
-	O
which	O
measures	O
the	O
density	O
of	O
edges	O
inside	O
communities	O
compared	O
to	O
edges	O
between	O
communities	O
-	O
is	O
maximized	O
:	O
Q	O
=	O
1	O
2	O
m	O
u	O
v	O
W	O
(	O
u	O
,	O
v	O
)	O
−	O
k	O
u	O
k	O
v	O
2	O
m	O
δ	B-HyperparameterName
(	O
c	O
u	O
,	O
c	O
v	O
)	O
m	O
is	O
the	O
sum	O
of	O
all	O
edge	O
weights	O
in	O
the	O
graph	O
,	O
k	O
u	O
=	O
v	O
W	O
(	O
u	O
,	O
v	O
)	O
is	O
the	O
sum	O
of	O
the	O
weights	O
of	O
the	O
edges	O
attached	O
to	O
node	O
u	O
,	O
c	O
u	O
is	O
the	O
community	O
to	O
which	O
u	O
is	O
assigned	O
,	O
and	O
δ	B-HyperparameterName
is	O
Kronecker	O
delta	O
function	O
.	O
This	O
objective	O
is	O
optimized	O
using	O
an	O
iterative	O
heuristic	O
process	O
.	O
For	O
details	O
,	O
see	O
Blondel	O
et	O
al	O
(	O
2008	O
)	O
.	O

We	O
start	O
by	O
intrinsically	O
evaluating	O
the	O
WSI	O
clustering	O
method	O
on	O
:	O
(	O
a	O
)	O
SemEval	O
2010	O
and	O
SemEval	B-DatasetName
2013	I-DatasetName
;	O
and	O
(	O
b	O
)	O
a	O
new	O
test	O
set	O
we	O
develop	O
for	O
largescale	O
WSI	O
.	O
In	O
section	O
9	O
,	O
we	O
additionally	O
extrinsically	O
evaluate	O
the	O
accuracy	B-MetricName
of	O
static	O
embeddings	O
derived	O
from	O
a	O
sense	O
-	O
induced	O
Wikipedia	O
dataset	O
.	O
When	O
collecting	O
word	O
-	O
substitutes	O
,	O
we	O
lemmatize	O
the	O
top	O
-	O
k	O
list	O
,	O
join	O
equivalent	O
lemmas	O
,	O
remove	O
stopwords	O
and	O
the	O
target	O
word	O
from	O
the	O
list	O
,	O
and	O
keep	O
the	O
top	O
-	O
5	O
remaining	O
lemmas	O
.	O

We	O
evaluate	O
the	O
community	O
-	O
based	O
WSI	O
algorithm	O
on	O
two	O
WSI	O
datasets	O
:	O
SemEval	O
2010	O
Task	O
14	O
and	O
SemEval	B-DatasetName
2013	I-DatasetName
Task	O
13	O
(	O
Jurgens	O
and	O
Klapaftis	O
,	O
2013	O
)	O
.	O
Table	O
2	O
compares	O
our	O
method	O
to	O
Goldberg	O
(	O
2018	O
,	O
2019	O
)	O
and	O
AutoSense	O
(	O
Amplayo	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
is	O
the	O
second	O
-	O
best	O
available	O
WSI	O
method	O
.	O
Bert	O
-	O
noDP	O
/	O
DP	O
are	O
taken	O
from	O
Amrami	O
and	O
Goldberg	O
(	O
2019	O
)	O
.	O
Bert	O
-	O
DP	O
uses	O
"	O
dynamic	O
patterns	O
"	O
which	O
precludes	O
widescale	O
application	O
.	O
We	O
follow	O
previous	O
work	O
Komninos	O
and	O
Manandhar	O
,	O
2016	O
;	O
Amrami	O
and	O
Goldberg	O
,	O
2019	O
)	O
and	O
evaluate	O
SemEval	O
2010	O
using	O
F	O
-	O
Score	B-MetricName
and	O
V	O
-	O
Measure	O
and	O
SemEval	B-DatasetName
2013	I-DatasetName
using	O
Fuzzy	O
Normalized	O
Mutual	O
Information	O
(	O
FNMI	O
)	O
and	O
Fuzzy	O
B	O
-	O
Cubed	O
(	O
FBC	O
)	O
as	O
well	O
as	O
their	O
geometric	O
mean	O
(	O
AVG	O
)	O
.	O
Our	O
method	O
performs	O
best	O
on	O
SemEval	O
2010	O
and	O
comparable	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
SemEval	B-DatasetName
2013	I-DatasetName
.	O
The	O
algorithm	O
performs	O
on	O
-	O
par	O
with	O
the	O
Bert	O
-	O
noDP	O
method	O
,	O
and	O
does	O
not	O
fall	O
far	O
behind	O
the	O
Bert	O
-	O
DP	O
method	O
.	O
We	O
now	O
turn	O
to	O
assess	O
the	O
end	O
-	O
to	O
-	O
end	O
induction	O
and	O
tagging	O
over	O
Wikipedia	O
.	O

We	O
used	O
a	O
list	O
of	O
20	O
ambiguous	O
words	O
from	O
CoarseWSD	O
-	O
20	O
(	O
Loureiro	O
et	O
al	O
,	O
2021	O
)	O
.	O
The	O
full	O
list	O
and	O
per	O
-	O
word	O
results	O
can	O
be	O
found	O
in	O
Appendix	O
C.	O
For	O
each	O
word	O
we	O
sampled	O
100	O
passages	O
from	O
English	O
Wikipedia	O
with	O
the	O
target	O
word	O
,	O
including	O
inflected	O
forms	O
(	O
case	O
insensitive	O
)	O
.	O
Unlike	O
CoarseWSD	O
-	O
20	O
,	O
we	O
sampled	O
examples	O
without	O
any	O
respect	O
to	O
a	O
predefined	O
set	O
of	O
senses	O
.	O
For	O
example	O
,	O
the	O
only	O
two	O
senses	O
that	O
appear	O
in	O
CoarseWSD	O
-	O
20	O
for	O
the	O
target	O
word	O
arm	O
are	O
arm	O
(	O
anatomy	O
)	O
,	O
and	O
arm	O
(	O
computing	O
)	O
,	O
leaving	O
out	O
instances	O
matching	O
senses	O
reflecting	O
weapons	O
,	O
subdivisions	O
,	O
mechanical	O
arms	O
etc	O
.	O
With	O
the	O
notion	O
that	O
word	B-TaskName
sense	I-TaskName
induction	I-TaskName
systems	O
should	O
be	O
robust	O
to	O
different	O
annotations	O
schemes	O
,	O
we	O
gave	O
two	O
fluent	O
English	O
speakers	O
100	O
sentences	O
for	O
each	O
of	O
the	O
20	O
ambiguous	O
words	O
from	O
CoarseWSD	O
-	O
20	O
.	O
Annotators	O
were	O
not	O
given	O
a	O
sense	O
inventory	O
.	O
Each	O
annotator	O
was	O
asked	O
to	O
label	O
each	O
instance	O
with	O
the	O
matching	O
sense	O
according	O
to	O
their	O
judgment	O
.	O
For	O
example	O
,	O
for	O
the	O
target	O
word	O
apple	O
in	O
the	O
sentence	O
"	O
The	O
iPhone	O
was	O
announced	O
by	O
Apple	O
CEO	O
.	O
"	O
,	O
annotators	O
can	O
label	O
the	O
target	O
sense	O
with	O
Apple	O
Inc.	O
,	O
Apple	O
The	O
Company	O
etc	O
.	O
Annotation	O
Guidelines	O
are	O
available	O
in	O
Appendix	O
B.	O
On	O
average	O
annotators	O
labeled	O
6.65	O
senses	O
per	O
word	O
(	O
5.85	O
and	O
7.45	O
average	O
clusters	O
per	O
word	O
for	O
the	O
two	O
annotators	O
)	O
.	O
This	O
is	O
more	O
than	O
the	O
2.65	O
average	O
senses	O
according	O
to	O
CoarseWSD	O
-	O
20	O
and	O
less	O
than	O
WordNet	O
's	O
9.85	O
.	O
Results	O
We	O
report	O
our	O
system	O
's	O
performance	O
alongside	O
two	O
additional	O
methods	O
:	O
A	O
strong	O
baseline	O
of	O
the	O
most	O
frequent	O
sense	O
(	O
MFS	O
)	O
,	O
and	O
Babelfy	O
(	O
Moro	O
et	O
al	O
,	O
2014	O
)	O
-	O
the	O
sense	O
disambiguation	O
system	O
used	O
in	O
BabelNet	O
(	O
Tested	O
using	O
Babelfy	O
live	O
version	O
April	O
2021	O
)	O
.	O
Differently	O
from	O
the	O
latter	O
,	O
our	O
system	O
does	O
not	O
disambiguates	O
but	O
induces	O
senses	O
,	O
therefore	O
,	O
clusters	O
are	O
not	O
labeled	O
with	O
a	O
sense	O
tag	O
from	O
a	O
sense	O
inventory	O
.	O
Instead	O
,	O
we	O
represent	O
senses	O
to	O
annotators	O
using	O
a	O
list	O
of	O
common	O
substitute	O
words	O
and	O
a	O
few	O
examples	O
.	O
Thus	O
,	O
after	O
annotating	O
the	O
Wikipedia	O
passages	O
,	O
we	O
additionally	O
asked	O
annotators	O
to	O
name	O
the	O
system	O
's	O
clusters	O
with	O
the	O
same	O
naming	O
convention	O
as	O
in	O
their	O
annotations	O
.	O
Given	O
a	O
similar	O
naming	O
convention	O
between	O
systems	O
and	O
annotators	O
,	O
we	O
report	O
F1	B-MetricName
scores	O
of	O
systems	O
'	O
tagging	O
accuracy	B-MetricName
with	O
respect	O
to	O
the	O
manual	O
annotations	O
.	O
We	O
report	O
F1	B-MetricName
averaged	O
over	O
words	O
in	O
Table	O
3	O
.	O
Our	O
system	O
outperforms	O
both	O
baselines	O
,	O
despite	O
Babelfy	O
having	O
access	O
to	O
a	O
list	O
of	O
predefined	O
word	O
senses	O
.	O
A	O
full	O
by	O
-	O
word	O
table	O
and	O
comprehensive	O
results	O
analysis	O
are	O
in	O
Appendix	O
C.	O
While	O
a	O
1	O
-	O
to	O
-	O
1	O
mapping	O
between	O
system	O
clusters	O
and	O
manual	O
senses	O
is	O
optimal	O
,	O
our	O
system	O
sometimes	O
splits	O
senses	O
into	O
smaller	O
clusters	O
,	O
thus	O
annotators	O
will	O
name	O
two	O
system	O
clusters	O
with	O
the	O
same	O
label	O
.	O
Therefore	O
it	O
is	O
also	O
important	O
to	O
report	O
the	O
number	O
of	O
clusters	O
produced	O
by	O
the	O
system	O
comparing	O
to	O
the	O
number	O
of	O
senses	O
after	O
the	O
annotators	O
merged	O
similar	O
clusters	O
.	O
Our	O
system	O
produced	O
7.25	O
clusters	O
with	O
2.25	O
clusters	O
on	O
average	O
merged	O
by	O
the	O
annotators	O
.	O
7	O
Additionally	O
,	O
in	O
rare	O
cases	O
our	O
system	O
encapsulates	O
a	O
few	O
senses	O
in	O
a	O
single	O
cluster	O
:	O
this	O
happened	O
3	O
and	O
5	O
times	O
for	O
both	O
annotators	O
across	O
all	O
the	O
dataset	O
.	O

Acc	B-MetricName
.	O
JBT	O
(	O
Pelevina	O
et	O
al	O
,	O
2016	O
)	O
53.6	O
Sense	O
-	O
aware	O
Embeddings	O
(	O
this	O
work	O
)	O
58.3	O
SW2V	O
*	O
(	O
Mancini	O
et	O
al	O
,	O
2017	O
)	O
58.1	O
DeConf	O
*	O
(	O
Pilehvar	O
and	O
Collier	O
,	O
2016	O
)	O
58.7	O
LessLex	O
*	O
(	O
Colla	O
et	O
al	O
,	O
2020	O
)	O
59.2	O
Our	O
method	O
is	O
the	O
following	O
:	O
Given	O
the	O
senseaware	O
embeddings	O
,	O
a	O
target	O
word	O
w	O
and	O
two	O
contexts	O
,	O
we	O
calculate	O
the	O
context	O
vector	O
as	O
the	O
average	O
of	O
the	O
context	O
words	O
.	O
The	O
matching	O
sense	O
vector	O
is	O
the	O
closest	O
out	O
of	O
all	O
w	O
embeddings	O
.	O
We	O
then	O
classify	O
the	O
contexts	O
as	O
corresponding	O
to	O
the	O
same	O
meaning	O
if	O
the	O
cosine	O
distance	O
of	O
the	O
found	O
sense	O
embedding	O
is	O
more	O
than	O
threshold	O
apart	O
.	O
We	O
do	O
not	O
use	O
the	O
train	O
set	O
.	O
The	O
threshold	O
is	O
optimized	O
over	O
the	O
development	O
set	O
and	O
fixed	O
to	O
0.68	O
.	O
This	O
task	O
has	O
a	O
few	O
tracks	O
,	O
we	O
compare	O
our	O
embeddings	O
systems	O
to	O
the	O
best	O
performing	O
methods	O
from	O
the	O
Sense	O
Representations	O
track	O
.	O
Of	O
these	O
,	O
JBT	O
(	O
Pelevina	O
et	O
al	O
,	O
2016	O
)	O
,	O
a	O
lexical	O
embedding	O
method	O
,	O
is	O
the	O
only	O
one	O
that	O
does	O
not	O
use	O
an	O
external	O
lexical	O
resource	O
(	O
induction	O
)	O
.	O
The	O
results	O
in	O
Table	O
4	O
show	O
accuracy	B-MetricName
on	O
this	O
task	O
.	O
We	O
outperform	O
the	O
induction	O
method	O
,	O
and	O
are	O
on	O
-	O
par	O
with	O
the	O
lexicon	O
-	O
based	O
methods	O
,	O
despite	O
not	O
using	O
any	O
external	O
lexical	O
resource	O
.	O

Another	O
setup	O
for	O
evaluating	O
word	B-TaskName
embeddings	I-TaskName
is	O
that	O
of	O
outlier	B-TaskName
detection	I-TaskName
:	O
given	O
a	O
set	O
of	O
words	O
,	O
identify	O
which	O
one	O
does	O
not	O
belong	O
to	O
the	O
set	O
(	O
Blair	O
et	O
al	O
,	O
2016	O
)	O
.	O
Outlier	B-TaskName
detection	I-TaskName
instances	O
are	O
composed	O
of	O
in	O
-	O
group	O
elements	O
and	O
a	O
set	O
of	O
outliers	O
from	O
a	O
related	O
semantic	O
space	O
.	O
In	O
each	O
evaluation	O
round	O
,	O
one	O
outlier	O
is	O
added	O
to	O
the	O
in	O
-	O
group	O
items	O
,	O
and	O
the	O
algorithm	O
is	O
tasked	O
with	O
finding	O
the	O
outlier	O
.	O
Existing	O
outlier	B-TaskName
detection	I-TaskName
datasets	O
either	O
did	O
not	O
explicitly	O
target	O
sense	O
-	O
ambiguous	O
words	O
(	O
8	O
-	O
8	O
-	O
8	O
(	O
Camacho	O
-	O
Collados	O
and	O
Navigli	O
,	O
2016	O
)	O
,	O
WikiSem500	B-DatasetName
(	O
Blair	O
et	O
al	O
,	O
2016	O
)	O
)	O
or	O
explicitly	O
removed	O
ambiguous	O
words	O
altogether	O
(	O
25	O
-	O
8	O
-	O
8	O
-	O
sem	O
(	O
Brink	O
Andersen	O
et	O
al	O
,	O
2020	O
)	O
)	O
.	O
Ambiguity	O
-	O
driven	O
Outlier	B-TaskName
Detection	I-TaskName
.	O
We	O
construct	O
a	O
challenge	O
set	O
for	O
outlier	B-TaskName
detection	I-TaskName
that	O
specifically	O
targets	O
ambiguous	O
cases	O
.	O
In	O
order	O
to	O
account	O
for	O
sense	O
ambiguity	O
,	O
we	O
add	O
a	O
distractor	O
to	O
each	O
of	O
the	O
in	O
-	O
group	O
sets	O
:	O
the	O
distractor	O
is	O
an	O
item	O
which	O
has	O
multiple	O
senses	O
,	O
where	O
the	O
most	O
salient	O
sense	O
does	O
not	O
belong	O
to	O
the	O
group	O
,	O
while	O
another	O
sense	O
does	O
belong	O
to	O
the	O
group	O
.	O
For	O
example	O
:	O
In	O
-	O
group	O
:	O
zeus	O
,	O
hades	O
,	O
poseidon	O
,	O
aphrodite	O
,	O
ares	O
,	O
athena	O
,	O
artemis	B-DatasetName
Outliers	O
:	O
mercury	O
,	O
odysseus	B-DatasetName
,	O
jesus	O
,	O
sparta	O
,	O
delphi	O
,	O
rome	O
,	O
wrath	O
,	O
atlanta	O
Distractor	O
:	O
nike	O
Here	O
,	O
a	O
model	O
which	O
does	O
not	O
explicitly	O
represent	O
the	O
greek	O
-	O
god	O
sense	O
of	O
nike	O
is	O
likely	O
to	O
place	O
it	O
far	O
away	O
from	O
the	O
in	O
-	O
group	O
instances	O
,	O
causing	O
it	O
to	O
be	O
mistakenly	O
marked	O
as	O
the	O
outlier	O
.	O
The	O
starting	O
point	O
for	O
our	O
dataset	O
is	O
25	O
-	O
8	O
-	O
8	O
-	O
Sem	O
(	O
Brink	O
Andersen	O
et	O
al	O
,	O
2020	O
)	O
.	O
This	O
dataset	O
contains	O
25	O
test	O
groups	O
,	O
each	O
with	O
8	O
in	O
-	O
group	O
elements	O
and	O
8	O
outliers	O
,	O
resulting	O
in	O
200	O
unique	O
test	O
cases	O
.	O
The	O
outliers	O
are	O
sorted	O
in	O
a	O
decreasing	O
degree	O
of	O
relatedness	O
to	O
the	O
in	O
-	O
group	O
elements	O
.	O
In	O
our	O
dataset	O
we	O
replace	O
one	O
of	O
the	O
in	O
-	O
group	O
elements	O
with	O
an	O
ambiguous	O
distractor	O
.	O
For	O
example	O
,	O
in	O
the	O
Greek	O
-	O
gods	O
case	O
above	O
,	O
we	O
replaced	O
the	O
original	O
8	O
th	O
item	O
(	O
"	O
hera	O
"	O
)	O
with	O
the	O
ambiguous	O
distractor	O
nike	O
.	O
12	O
The	O
dataset	O
consists	O
of	O
25	O
groups	O
of	O
7	O
non	O
ambiguous	O
group	O
elements	O
,	O
1	O
distractor	O
and	O
8	O
outliers	O
(	O
25	O
-	O
7	O
-	O
1	O
-	O
8	O
)	O
,	O
similarly	O
resulting	O
200	O
unique	O
test	O
cases	O
.	O
Method	O
Following	O
Camacho	O
-	O
Collados	O
and	O
Navigli	O
(	O
2016	O
)	O
,	O
we	O
rank	O
each	O
word	O
likelihood	O
of	O
being	O
the	O
outlier	O
by	O
the	O
average	O
of	O
all	O
pair	O
-	O
wise	O
semantic	O
similarities	O
of	O
the	O
words	O
in	O
W	O
\	O
{	O
w	O
}	O
.	O
Therefore	O
if	O
w	O
is	O
an	O
outlier	O
,	O
this	O
score	O
should	O
be	O
low	O
.	O
See	O
Appendix	O
D	O
for	O
additional	O
details	O
.	O
Metrics	O
Camacho	O
-	O
Collados	O
and	O
Navigli	O
(	O
2016	O
)	O
proposed	O
evaluating	O
outlier	B-TaskName
detection	I-TaskName
using	O
the	O
accuracy	B-MetricName
(	O
The	O
fraction	O
of	O
correctly	O
classified	O
outliers	O
among	O
the	O
total	O
cases	O
)	O
and	O
Outlier	O
Position	O
Percentage	O
(	O
OPP	O
)	O
metric	O
.	O
OPP	O
indicates	O
how	O
close	O
outliers	O
are	O
to	O
being	O
classified	O
correctly	O
:	O
OP	O
P	O
=	O
W	O
D	O
OP	O
(	O
W	O
)	O
|	O
W	O
|	O
−1	O
|	O
D	O
|	O
×	O
100	O
where	O
OP	O
(	O
W	O
)	O
is	O
the	O
position	O
of	O
the	O
outlier	O
according	O
to	O
the	O
algorithm	O
.	O
Results	O
In	O
Table	O
5	O
we	O
report	O
performance	O
of	O
on	O
the	O
25	O
-	O
7	O
-	O
1	O
-	O
8	O
set	O
.	O
Word2vec	O
and	O
GloVe	B-MethodName
accuracy	B-MetricName
scores	O
are	O
low	O
while	O
having	O
high	O
OPP	O
scores	O
.	O
This	O
is	O
the	O
expected	O
behaviour	O
for	O
embeddings	O
without	O
sense	O
awareness	O
.	O
These	O
will	O
position	O
the	O
distractor	O
and	O
the	O
outlier	O
furthest	O
away	O
from	O
the	O
group	O
items	O
while	O
not	O
designed	O
to	O
make	O
the	O
hard	O
decision	O
required	O
for	O
high	O
Accuracy	B-MetricName
.	O
Our	O
sense	O
-	O
aware	O
embeddings	O
strongly	O
outperform	O
GloVe	B-MethodName
and	O
word2vec	O
which	O
do	O
not	O
include	O
senses	O
.	O
Our	O
embeddings	O
also	O
outperform	O
the	O
word	B-TaskName
embeddings	I-TaskName
proposed	O
in	O
De	O
-	O
Conf	O
(	O
Pilehvar	O
and	O
Collier	O
,	O
2016	O
)	O
,	O
which	O
are	O
the	O
best	O
performing	O
sense	O
embeddings	O
on	O
WiC	B-DatasetName
which	O
are	O
also	O
publicly	O
available	O
.	O

In	O
table	O
6	O
we	O
report	O
a	O
by	O
-	O
word	O
analysis	O
of	O
our	O
manual	O
evaluation	O
results	O
.	O
For	O
each	O
word	O
we	O
detail	O
F1	B-MetricName
scores	O
of	O
the	O
most	O
frequent	O
sense	O
(	O
MFS	O
)	O
,	O
Babelfy	O
,	O
and	O
our	O
proposed	O
system	O
.	O
Similarly	O
to	O
Loureiro	O
et	O
al	O
(	O
2021	O
)	O
,	O
we	O
report	O
the	O
ratio	O
of	O
the	O
first	O
sense	O
with	O
respect	O
to	O
the	O
rest	O
(	O
F2R	O
)	O
and	O
normalized	O
entropy	O
17	O
to	O
reflect	O
sense	O
balance	O
.	O
All	O
of	O
which	O
are	O
reported	O
per	O
annotator	O
.	O
Analysis	O
Analysis	O
of	O
our	O
system	O
's	O
error	O
shows	O
that	O
for	O
some	O
words	O
the	O
system	O
could	O
not	O
create	O
a	O
matching	O
cluster	O
for	O
specific	O
senses	O
(	O
to	O
name	O
a	O
few	O
examples	O
,	O
"	O
yard	O
"	O
as	O
a	O
ship	O
identifier	O
and	O
"	O
impound	O
/	O
enclosure	O
"	O
sense	O
for	O
the	O
word	O
"	O
pound	O
"	O
)	O
.	O
It	O
appears	O
that	O
a	O
matching	O
cluster	O
was	O
not	O
created	O
due	O
to	O
the	O
low	O
tally	O
of	O
these	O
senses	O
in	O
the	O
English	O
Wikipedia	O
,	O
and	O
indeed	O
the	O
two	O
senses	O
appeared	O
only	O
two	O
and	O
three	O
times	O
respectively	O
in	O
the	O
100	O
for	O
all	O
words	O
17	O
Computed	O
as	O
−	O
k	O
i=1	O
c	O
i	O
n	O
log	O
c	O
i	O
n	O
log	O
(	O
k	O
)	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
annotated	O
senses	O
,	O
each	O
of	O
size	O
ci	O
and	O
n	O
is	O
the	O
size	O
of	O
annotated	O
examples	O
per	O
word	O
,	O
in	O
our	O
case	O
n	O
=	O
100	O
.	O
passages	O
sample	O
.	O
Additionally	O
,	O
annotator	O
2	O
annotated	O
in	O
a	O
more	O
fine	O
-	O
grained	O
manner	O
that	O
does	O
not	O
correspond	O
to	O
our	O
system	O
tendency	O
to	O
merge	O
capitalized	O
instances	O
of	O
the	O
target	O
word	O
into	O
a	O
sense	O
that	O
corresponds	O
to	O
"	O
part	O
of	O
named	O
entity	O
"	O
.	O
As	O
described	O
above	O
,	O
in	O
rare	O
cases	O
our	O
system	O
merged	O
two	O
senses	O
into	O
a	O
single	O
cluster	O
.	O
For	O
example	O
,	O
the	O
same	O
cluster	O
of	O
the	O
word	O
"	O
trunk	O
"	O
contained	O
occurrences	O
which	O
annotator	O
1	O
tagged	O
either	O
"	O
human	O
torso	O
"	O
or	O
"	O
tube	O
-	O
like	O
organs	O
"	O
(	O
like	O
the	O
pulmonary	O
trunk	O
)	O
.	O
While	O
such	O
annotation	O
was	O
uncommon	O
(	O
3	O
out	O
of	O
117	O
senses	O
for	O
annotator	O
1	O
and	O
5	O
out	O
of	O
149	O
senses	O
for	O
annotator	O
2	O
)	O
,	O
it	O
does	O
affect	O
our	O
system	O
's	O
micro	B-MetricName
F1	I-MetricName
score	O
for	O
the	O
better	O
.	O
In	O
case	O
we	O
do	O
not	O
allow	O
such	O
annotation	O
our	O
overall	O
score	O
drops	O
from	O
87.52	O
to	O
86.65	O
.	O
A	O
comparison	O
between	O
Babelfy	O
and	O
our	O
gold	O
annotation	O
shows	O
a	O
common	O
mistake	O
in	O
its	O
labeling	O
where	O
Babelfy	O
attributes	O
the	O
vast	O
majority	O
of	O
sentences	O
to	O
the	O
same	O
non	O
-	O
salient	O
sense	O
.	O
For	O
example	O
,	O
Babelfy	O
attributes	O
77	O
out	O
of	O
100	O
instances	O
of	O
hood	O
to	O
"	O
An	O
aggressive	O
and	O
violent	O
young	O
criminal	O
"	O
-	O
a	O
sense	O
that	O
was	O
not	O
found	O
even	O
once	O
in	O
the	O
manual	O
annotation	O
.	O
While	O
in	O
a	O
number	O
of	O
cases	O
Babelfy	O
used	O
finer	O
-	O
grained	O
sysnset	O
groups	O
than	O
in	O
our	O
annotations	O
we	O
took	O
into	O
account	O
any	O
senses	O
that	O
are	O
a	O
subset	O
of	O
our	O
annotated	O
senses	O
.	O
For	O
examples	O
,	O
Babelfy	O
's	O
"	O
United	O
States	O
writer	O
who	O
lived	O
in	O
Europe	O
;	O
strongly	O
influenced	O
the	O
development	O
of	O
modern	O
literature	O
(	O
1885	O
-	O
1972	O
)	O
"	O
synset	O
was	O
attribute	O
any	O
instances	O
from	O
the	O
senses	O
surname	O
that	O
refer	O
to	O
the	O
writer	O
Ezra	O
Pound	O
.	O

For	O
well	O
over	O
half	O
a	O
century	O
,	O
solutions	O
to	O
the	O
ad	O
hoc	O
retrieval	O
problem	O
-	O
where	O
the	O
system	O
's	O
task	O
is	O
return	O
a	O
list	O
of	O
top	O
k	O
texts	O
from	O
an	O
arbitrarily	O
large	O
corpus	O
D	O
that	O
maximizes	O
some	O
metric	O
of	O
quality	O
such	O
as	O
average	B-MetricName
precision	I-MetricName
or	O
NDCG	O
-	O
has	O
been	O
dominated	O
by	O
sparse	O
vector	O
representations	O
,	O
for	O
example	O
,	O
bag	O
-	O
of	O
-	O
words	O
BM25	O
.	O
Even	O
in	O
modern	O
multi	O
-	O
stage	O
ranking	O
architectures	O
,	O
which	O
take	O
advantage	O
of	O
large	O
pretrained	O
transformers	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
the	O
models	O
are	O
deployed	O
as	O
rerankers	O
over	O
initial	O
candidates	O
retrieved	O
based	O
on	O
sparse	O
vector	O
representations	O
;	O
this	O
is	O
sometimes	O
called	O
"	O
first	O
-	O
stage	O
retrieval	O
"	O
.	O
One	O
well	O
-	O
known	O
example	O
of	O
this	O
design	O
is	O
the	O
BERT	B-MethodName
-	O
based	O
reranker	O
of	O
Nogueira	O
and	O
Cho	O
(	O
2019	O
)	O
;	O
see	O
Lin	O
et	O
al	O
(	O
2020	O
)	O
for	O
a	O
recent	O
survey	O
.	O
multi	O
-	O
vector	O
bi	O
-	O
encoders	O
or	O
cross	O
-	O
encoders	O
.	O
Hence	O
,	O
improving	O
the	O
effectiveness	O
of	O
single	O
-	O
vector	O
biencoders	O
represents	O
an	O
important	O
problem	O
.	O
One	O
approach	O
to	O
improving	O
the	O
effectiveness	O
of	O
single	O
-	O
vector	O
bi	O
-	O
encoders	O
is	O
hard	O
negative	O
mining	O
,	O
by	O
training	O
with	O
carefully	O
selected	O
negative	O
examples	O
that	O
emphasize	O
discrimination	O
between	O
relevant	O
and	O
non	O
-	O
relevant	O
texts	O
.	O
There	O
are	O
several	O
approaches	O
to	O
accomplish	O
this	O
.	O
Karpukhin	O
et	O
al	O
(	O
2020	O
)	O
and	O
Qu	O
et	O
al	O
(	O
2020	O
)	O
leverage	O
large	O
in	O
-	O
batch	O
negatives	O
to	O
enrich	O
training	O
signals	O
.	O
Guu	O
et	O
al	O
(	O
2020	O
)	O
and	O
propose	O
to	O
mine	O
hard	O
negatives	O
using	O
the	O
trained	O
bi	O
-	O
encoder	O
itself	O
.	O
By	O
searching	O
for	O
global	O
negative	O
samples	O
from	O
an	O
asynchronously	O
updated	O
ANN	O
index	O
,	O
the	O
bi	O
-	O
encoder	O
can	O
learn	O
information	O
not	O
present	O
in	O
the	O
training	O
data	O
produced	O
by	O
sparse	O
representations	O
.	O
However	O
,	O
both	O
large	O
in	O
-	O
batch	O
negative	O
sampling	O
and	O
asynchronous	O
ANN	O
index	O
updates	O
are	O
computationally	O
demanding	O
.	O
The	O
later	O
is	O
especially	O
impractical	O
for	O
large	O
corpora	O
since	O
it	O
requires	O
periodic	O
inference	O
over	O
all	O
texts	O
in	O
the	O
corpus	O
to	O
ensure	O
that	O
the	O
best	O
negative	O
examples	O
are	O
retrieved	O
.	O
There	O
is	O
also	O
work	O
that	O
explores	O
knowledge	B-MethodName
distillation	I-MethodName
(	O
KD	O
)	O
(	O
Hinton	O
et	O
al	O
,	O
2015	O
)	O
to	O
enhance	O
retrieval	O
effectiveness	O
and	O
efficiency	O
.	O
Most	O
related	O
to	O
our	O
study	O
is	O
Hofstätter	O
et	O
al	O
(	O
2020	O
)	O
,	O
who	O
demonstrate	O
that	O
KD	O
using	O
a	O
cross	O
-	O
encoder	O
teacher	O
significantly	O
improves	O
the	O
effectiveness	O
of	O
bi	O
-	O
encoders	O
for	O
dense	O
retrieval	O
.	O
Similarly	O
,	O
Barkan	O
et	O
al	O
(	O
2020	O
)	O
investigate	O
the	O
effectiveness	O
of	O
distilling	O
a	O
trained	O
cross	O
-	O
encoder	O
into	O
a	O
bi	O
-	O
encoder	O
for	O
sentence	O
similarity	O
tasks	O
.	O
Gao	O
et	O
al	O
(	O
2020a	O
)	O
explore	O
KD	O
combinations	O
of	O
different	O
objectives	O
such	O
as	O
language	O
modeling	O
and	O
ranking	O
.	O
However	O
,	O
the	O
above	O
papers	O
use	O
computationally	O
expensive	O
cross	O
-	O
encoder	O
teacher	O
models	O
;	O
thus	O
,	O
combining	O
them	O
for	O
KD	O
with	O
more	O
advanced	O
negative	O
sampling	O
techniques	O
can	O
be	O
impractical	O
.	O
In	O
light	O
of	O
existing	O
work	O
on	O
hard	O
negative	O
mining	O
and	O
knowledge	B-MethodName
distillation	I-MethodName
,	O
we	O
propose	O
to	O
improve	O
the	O
effectiveness	O
of	O
single	O
-	O
vector	O
bi	O
-	O
encoders	O
with	O
a	O
more	O
efficient	O
KD	O
approach	O
:	O
in	O
-	O
batch	O
KD	O
using	O
a	O
bi	O
-	O
encoder	O
teacher	O
.	O
The	O
advantage	O
of	O
our	O
design	O
is	O
that	O
,	O
during	O
distillation	O
,	O
it	O
enables	O
the	O
efficient	O
exploitation	O
of	O
all	O
possible	O
query	O
-	O
passage	O
pairs	O
within	O
a	O
minibatch	O
,	O
which	O
we	O
call	O
tight	O
coupling	O
(	O
illustrated	O
in	O
Figure	O
1	O
)	O
.	O
This	O
is	O
a	O
key	O
difference	O
between	O
our	O
KD	O
approach	O
and	O
previous	O
methods	O
for	O
dense	O
retrieval	O
,	O
where	O
only	O
the	O
scores	O
of	O
given	O
query	O
-	O
passage	O
triplets	O
(	O
not	O
all	O
combinations	O
)	O
are	O

The	O
bi	O
-	O
encoder	O
design	O
has	O
been	O
widely	O
adopted	O
for	O
dense	O
retrieval	O
Guu	O
et	O
al	O
,	O
2020	O
;	O
Karpukhin	O
et	O
al	O
,	O
2020	O
;	O
Luan	O
et	O
al	O
,	O
2021	O
;	O
Qu	O
et	O
al	O
,	O
2020	O
;	O
,	O
where	O
queries	O
and	O
passages	O
are	O
encoded	O
in	O
a	O
low	O
-	O
dimensional	O
space	O
.	O
It	O
aims	O
to	O
learn	O
lowdimensional	O
representations	O
that	O
pull	O
queries	O
and	O
relevant	O
passages	O
together	O
and	O
push	O
queries	O
and	O
non	O
-	O
relevant	O
passages	O
apart	O
.	O
Following	O
the	O
work	O
of	O
Mnih	O
and	O
Kavukcuoglu	O
(	O
2013	O
)	O
,	O
we	O
formulate	O
a	O
common	O
objective	O
for	O
dense	O
representation	B-TaskName
learning	I-TaskName
for	O
passage	B-TaskName
retrieval	I-TaskName
.	O
Given	O
a	O
query	O
q	O
and	O
a	O
parameterized	O
scoring	O
function	O
φ	O
θ	B-HyperparameterName
that	O
computes	O
the	O
relevance	O
between	O
a	O
query	O
and	O
a	O
candidate	O
passage	O
p	O
,	O
we	O
define	O
a	O
probability	O
distribution	O
over	O
documents	O
in	O
a	O
corpus	O
D	O
with	O
respect	O
to	O
relevance	O
,	O
as	O
follows	O
:	O
P	O
q	O
θ	B-HyperparameterName
(	O
p	O
,	O
D	O
)	O
=	O
exp	O
(	O
φ	O
θ	B-HyperparameterName
(	O
q	O
,	O
p	O
)	O
)	O
p	O
D	O
exp	O
(	O
φ	O
θ	B-HyperparameterName
(	O
q	O
,	O
p	O
)	O
)	O
=	O
exp	O
(	O
h	O
q	O
h	O
p	O
)	O
p	O
D	O
exp	O
(	O
h	O
q	O
h	O
p	O
)	O
,	O
(	O
1	O
)	O
where	O
h	O
q	O
(	O
h	O
p	O
)	O
R	O
d	O
denotes	O
the	O
query	O
(	O
passage	O
)	O
representation	O
produced	O
by	O
the	O
bi	O
-	O
encoder	O
.	O
A	O
typical	O
bi	O
-	O
encoder	O
uses	O
a	O
simple	O
scoring	O
function	O
for	O
φ	O
θ	B-HyperparameterName
,	O
for	O
example	O
,	O
the	O
inner	O
product	O
of	O
two	O
vectors	O
,	O
as	O
shown	O
above	O
.	O
The	O
main	O
challenge	O
of	O
evaluating	O
and	O
computing	O
gradients	O
of	O
Eq	O
.	O
(	O
1	O
)	O
is	O
the	O
prohibitively	O
expensive	O
computation	O
cost	O
given	O
the	O
number	O
of	O
passages	O
in	O
the	O
corpus	O
D	O
,	O
typically	O
millions	O
(	O
or	O
even	O
more	O
)	O
.	O
This	O
is	O
already	O
setting	O
aside	O
the	O
cost	O
of	O
using	O
pretrained	O
transformers	O
such	O
as	O
BERT	B-MethodName
as	O
the	O
encoder	O
to	O
compute	O
h	O
q	O
and	O
h	O
p	O
.	O
Thus	O
,	O
previous	O
work	O
approximates	O
Eq	O
.	O
(	O
1	O
)	O
by	O
NCE	O
,	O
which	O
samples	O
p	O
D	O
+	O
from	O
training	O
data	O
and	O
p	O
D	O
=	O
{	O
D	O
+	O
∪	O
D	O
−	O
}	O
,	O
where	O
D	O
−	O
is	O
from	O
a	O
noisy	O
distribution	O
such	O
as	O
candidates	O
retrieved	O
by	O
BM25	O
(	O
Nogueira	O
and	O
Cho	O
,	O
2019	O
)	O
,	O
filtered	O
by	O
finetuned	O
transformers	O
(	O
Qu	O
et	O
al	O
,	O
2020	O
)	O
,	O
or	O
retrieved	O
by	O
an	O
asynchronously	O
updated	O
bi	O
-	O
encoder	O
model	O
itself	O
.	O
Another	O
simple	O
yet	O
effective	O
approach	O
is	O
in	O
-	O
batch	O
negative	O
sampling	O
,	O
as	O
used	O
by	O
Karpukhin	O
et	O
al	O
(	O
2020	O
)	O
,	O
which	O
takes	O
p	O
and	O
p	O
of	O
other	O
queries	O
within	O
a	O
minibatch	O
as	O
negative	O
examples	O
in	O
NCE	O
.	O

Other	O
than	O
designing	O
sophisticated	O
sampling	O
methods	O
for	O
p	O
,	O
training	O
bi	O
-	O
encoder	O
models	O
using	O
knowledge	B-MethodName
distillation	I-MethodName
(	O
KD	O
)	O
with	O
effective	O
teacher	O
models	O
is	O
another	O
promising	O
approach	O
(	O
Hofstätter	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
case	O
,	O
we	O
aim	O
to	O
make	O
the	O
bi	O
-	O
encoder	O
model	O
mimic	O
the	O
teacher	O
model	O
's	O
probability	O
distribution	O
as	O
follows	O
:	O
P	O
q	O
θ	B-HyperparameterName
;	O
student	O
(	O
p	O
,	O
D	O
)	O
=	O
exp	O
(	O
h	O
q	O
h	O
p	O
)	O
p	O
D	O
exp	O
(	O
h	O
q	O
h	O
p	O
)	O
≈	O
exp	O
(	O
φθ	O
(	O
q	O
,	O
p	O
)	O
/τ	O
)	O
p	O
D	O
exp	O
(	O
φθ	O
(	O
q	O
,	O
p	O
)	O
/τ	O
)	O
=	O
P	O
q	O
θ	B-HyperparameterName
;	O
teacher	O
(	O
p	O
,	O
D	O
)	O
,	O
(	O
2	O
)	O
where	O
φθ	O
denotes	O
the	O
relevance	O
score	O
estimated	O
by	O
a	O
pretrained	O
model	O
parameterized	O
byθ	O
and	O
τ	O
,	O
the	O
temperature	O
hyperparameter	O
used	O
in	O
the	O
KD	O
framework	O
.	O
To	O
improve	O
retrieval	O
effectiveness	O
,	O
one	O
can	O
leverage	O
pre	O
-	O
computed	O
scores	O
from	O
pretrained	O
models	O
such	O
as	O
cross	O
-	O
encoders	O
,	O
e.g.	O
,	O
BERT	B-MethodName
,	O
bi	O
-	O
encoders	O
,	O
e.g.	O
,	O
ColBERT	O
,	O
or	O
ensembled	O
scores	O
from	O
multiple	O
models	O
φθ	O
=	O
j	O
φθ	O
;	O
j	O
.	O
3	O
Our	O
Approach	O
arg	O
min	O
θ	B-HyperparameterName
q	O
Q	O
B	O
p	O
D	O
B	O
L	O
φ	O
θ	B-HyperparameterName
,	O
φθ	O
,	O
(	O
3	O
)	O
where	O
L	O
φ	O
θ	B-HyperparameterName
,	O
φθ	O
is	O
:	O
P	O
q	O
θ	B-HyperparameterName
;	O
teacher	O
(	O
p	O
,	O
D	O
B	O
)	O
log	O
P	O
q	O
θ	B-HyperparameterName
;	O
teacher	O
(	O
p	O
,	O
D	O
B	O
)	O
P	O
q	O
θ	B-HyperparameterName
;	O
student	O
(	O
p	O
,	O
D	O
B	O
)	O
.	O
(	O
4	O
)	O
Note	O
that	O
here	O
we	O
consider	O
all	O
pairwise	O
relationship	O
between	O
queries	O
and	O
passages	O
within	O
a	O
minibatch	O
that	O
contains	O
a	O
query	O
set	O
Q	O
B	O
and	O
a	O
passage	O
set	O
D	O
B	O
.	O

In	O
this	O
section	O
,	O
we	O
conduct	O
experiments	O
on	O
the	O
MS	B-DatasetName
MARCO	I-DatasetName
passage	O
and	O
document	O
corpora	O
.	O
For	O
passage	B-TaskName
ranking	I-TaskName
,	O
we	O
first	O
train	O
models	O
on	O
BM25	O
negatives	O
as	O
warm	O
-	O
up	O
and	O
compare	O
different	O
KD	O
methods	O
.	O
We	O
then	O
further	O
train	O
models	O
on	O
the	O
hard	O
negatives	O
retrieved	O
by	O
the	O
BM25	O
warmed	O
-	O
up	O
checkpoint	O
.	O
For	O
document	B-TaskName
ranking	I-TaskName
,	O
following	O
previous	O
work	O
Zhan	O
et	O
al	O
,	O
2020	O
;	O
Lu	O
et	O
al	O
,	O
2021	O
)	O
,	O
we	O
start	O
with	O
our	O
BM25	O
warmed	O
-	O
up	O
checkpoint	O
for	O
passage	B-TaskName
ranking	I-TaskName
and	O
conduct	O
additional	O
hard	O
negative	O
training	O
.	O
To	O
evaluate	O
output	O
quality	O
,	O
we	O
report	O
MRR@10	O
(	O
NDCG@10	B-MetricName
)	O
for	O
MARCO	O
Dev	O
(	O
TREC	B-DatasetName
-	O
DL	O
'	O
19	O
)	O
and	O
Recall@1	B-MetricName
K	O
,	O
denoted	O
as	O
R@1K.	O
To	O
compare	O
with	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
,	O
we	O
evaluate	O
our	O
design	O
,	O
TCT	O
-	O
ColBERT	O
,	O
under	O
two	O
approaches	O
for	O
negative	O
sampling	O
:	O
(	O
1	O
)	O
BM25	O
and	O
(	O
2	O
)	O
hard	O
negatives	O
retrieved	O
by	O
the	O
bi	O
-	O
encoder	O
itself	O
.	O

In	O
this	O
setting	O
,	O
models	O
are	O
trained	O
using	O
the	O
official	O
public	O
data	O
triples.train.small	O
,	O
where	O
negative	O
samples	O
are	O
produced	O
by	O
BM25	O
.	O
We	O
compare	O
different	O
bi	O
-	O
encoder	O
models	O
using	O
BERT	B-MethodName
-	O
base	O
as	O
the	O
backbone	O
,	O
which	O
uses	O
single	O
768	O
-	O
dim	O
vectors	O
to	O
represent	O
each	O
query	O
and	O
passage	O
:	O
1	O
.	O
Baseline	O
:	O
a	O
single	O
-	O
vector	O
bi	O
-	O
encoder	O
trained	O
with	O
in	O
-	O
batch	O
negatives	O
,	O
as	O
discussed	O
in	O
Section	O
2.1	O
,	O
which	O
is	O
similar	O
to	O
Karpukhin	O
et	O
al	O
(	O
2020	O
)	O
but	O
with	O
a	O
smaller	O
batch	B-HyperparameterName
size	I-HyperparameterName
.	O
2	O
.	O
Pairwise	O
KD	O
:	O
the	O
approach	O
of	O
Hofstätter	O
et	O
al	O
(	O
2020	O
)	O
,	O
who	O
improve	O
ranking	O
effectiveness	O
using	O
cross	O
-	O
encoders	O
with	O
pairwise	O
KD	O
.	O
We	O
also	O
compare	O
against	O
two	O
models	O
,	O
KD	O
-	O
T1	O
and	O
KD	O
-	O
T2	O
,	O
which	O
use	O
BERT	B-MethodName
-	O
base	O
bi	O
-	O
encoders	O
as	O
student	O
models	O
.	O
In	O
the	O
former	O
,	O
the	O
student	O
is	O
distilled	O
from	O
a	O
BERT	B-MethodName
-	O
base	O
cross	O
-	O
encoder	O
,	O
while	O
the	O
latter	O
is	O
distilled	O
from	O
ensembled	O
cross	O
-	O
encoders	O
comprising	O
BERT	B-MethodName
-	O
base	O
,	O
BERT	B-MethodName
-	O
large	O
,	O
and	O
ALBERTlarge	O
.	O
These	O
figures	O
reported	O
in	O
Table	O
2	O
are	O
copied	O
from	O
Hofstätter	O
et	O
al	O
(	O
2020	O
)	O
.	O
For	O
a	O
fair	O
comparison	O
with	O
our	O
models	O
based	O
on	O
KL	O
-	O
divergence	O
KD	O
,	O
we	O
also	O
implement	O
our	O
KD	O
-	O
T2	O
using	O
the	O
precomputed	O
pairwise	O
softmax	B-MethodName
probabilities	O
provided	O
by	O
Hofstätter	O
et	O
al	O
(	O
2020	O
)	O
(	O
who	O
use	O
MSE	B-MetricName
margin	O
loss	B-MetricName
for	O
KD	O
)	O
.	O
In	O
addition	O
,	O
we	O
adopt	O
pairwise	O
softmax	B-MethodName
probabilities	O
from	O
fine	O
-	O
tuned	O
ColBERT	O
to	O
train	O
KD	O
-	O
ColBERT	O
for	O
comparison	O
.	O
All	O
our	O
models	O
are	O
fine	O
-	O
tuned	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
96	O
and	O
learning	B-HyperparameterName
rate	I-HyperparameterName
7	O
×	O
10	O
−6	O
for	O
500	O
K	O
steps	O
on	O
a	O
single	O
TPU	O
-	O
V2	O
.	O
For	O
TCT	O
-	O
ColBERT	O
,	O
there	O
are	O
two	O
steps	O
in	O
our	O
training	O
procedure	O
:	O
(	O
1	O
)	O
finetune	O
φθ	O
;	O
MaxSim	O
as	O
our	O
teacher	O
model	O
,	O
(	O
2	O
)	O
freeze	O
φθ	O
;	O
MaxSim	O
and	O
distill	O
knowledge	O
into	O
our	O
student	O
model	O
φ	O
θ	B-HyperparameterName
.	O
We	O
keep	O
all	O
the	O
hyperparameter	O
settings	O
the	O
same	O
but	O
adjust	O
temperature	O
τ	O
=	O
0.25	O
for	O
KD	O
at	O
the	O
second	O
step	O
.	O
For	O
all	O
our	O
models	O
,	O
including	O
the	O
baseline	O
,	O
we	O
initialize	O
the	O
student	O
model	O
using	O
the	O
fine	O
-	O
tuned	O
weights	O
of	O
the	O
teacher	O
model	O
in	O
the	O
first	O
step	O
.	O
We	O
limit	O
the	O
input	O
tokens	O
to	O
32	O
(	O
150	O
)	O
for	O
queries	O
(	O
passages	O
)	O
.	O
To	O
evaluate	O
effectiveness	O
,	O
we	O
encode	O
all	O
passages	O
in	O
the	O
corpus	O
and	O
conduct	O
brute	O
force	O
search	O
over	O
the	O
vector	O
representations	O
.	O
Our	O
main	O
results	O
,	O
including	O
paired	O
t	O
-	O
test	O
for	O
significance	O
testing	O
,	O
are	O
shown	O
in	O
Table	O
2	O
.	O
In	O
addition	O
to	O
the	O
effectiveness	O
of	O
the	O
student	O
models	O
,	O
we	O
also	O
show	O
the	O
effectiveness	O
of	O
the	O
teacher	O
models	O
for	O
the	O
KD	O
methods	O
.	O
1	O
First	O
,	O
we	O
see	O
that	O
pairwise	O
KD	O
methods	O
show	O
significant	O
improvements	O
over	O
the	O
baseline	O
,	O
indicat	O
-	O
0	B-DatasetName
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
Index	O
Size	O
(	O
10	O
ing	O
that	O
information	O
from	O
BM25	O
negatives	O
can	O
not	O
be	O
fully	O
exploited	O
without	O
teacher	O
models	O
.	O
Second	O
,	O
although	O
KD	O
-	O
T2	O
improves	O
the	O
bi	O
-	O
encoder	O
's	O
effectiveness	O
over	O
KD	O
-	O
T1	O
,	O
it	O
is	O
not	O
consistently	O
better	O
than	O
KD	O
-	O
ColBERT	O
in	O
terms	O
of	O
students	O
'	O
effectiveness	O
.	O
We	O
suspect	O
that	O
they	O
have	O
comparable	O
capabilities	O
to	O
discriminate	O
most	O
paired	O
passages	O
(	O
BM25	O
negative	O
vs.	O
positive	O
samples	O
)	O
,	O
i.e.	O
,	O
Col	O
-	O
BERT	B-MethodName
is	O
good	O
enough	O
to	O
guide	O
bi	O
-	O
encoder	O
student	O
models	O
to	O
discriminate	O
them	O
.	O
On	O
the	O
other	O
hand	O
,	O
our	O
TCT	O
-	O
ColBERT	O
model	O
,	O
which	O
uses	O
only	O
one	O
teacher	O
model	O
and	O
adds	O
only	O
33	O
%	O
more	O
training	O
time	O
over	O
the	O
baseline	O
,	O
yields	O
the	O
best	O
effectiveness	O
,	O
demonstrating	O
the	O
advantages	O
of	O
our	O
proposed	O
inbatch	O
KD	O
-	O
exhaustive	O
exploitation	O
of	O
all	O
querydocument	O
combinations	O
in	O
a	O
minibatch	O
.	O
To	O
understand	O
why	O
TCT	O
-	O
ColBERT	O
yields	O
better	O
results	O
,	O
we	O
study	O
the	O
models	O
'	O
retrieval	O
effectiveness	O
against	O
carefully	O
selected	O
distractors	O
.	O
We	O
start	O
with	O
a	O
small	O
synthetic	O
corpus	O
composed	O
of	O
the	O
relevant	O
passages	O
and	O
the	O
top	O
-	O
1000	O
BM25	O
candidates	O
of	O
the	O
6980	O
(	O
43	O
)	O
queries	O
from	O
MARCO	O
Dev	O
(	O
TREC	B-DatasetName
-	O
DL	O
'	O
19	O
)	O
.	O
To	O
increase	O
the	O
corpus	O
size	O
,	O
we	O
gradually	O
add	O
passages	O
uniformly	O
sampled	O
from	O
the	O
corpus	O
without	O
replacement	O
.	O
From	O
Figure	O
2	O
,	O
we	O
see	O
that	O
the	O
three	O
KD	O
models	O
exhibit	O
nearly	O
the	O
same	O
effectiveness	O
when	O
the	O
corpus	O
only	O
contains	O
BM25	O
candidates	O
.	O
This	O
shows	O
that	O
the	O
bi	O
-	O
encoders	O
learn	O
to	O
discriminate	O
relevant	O
passages	O
from	O
the	O
BM25	O
negative	O
samples	O
well	O
.	O
However	O
,	O
as	O
the	O
index	O
size	O
increases	O
,	O
TCT	O
-	O
ColBERT	O
demonstrates	O
better	O
ranking	O
effectiveness	O
than	O
the	O
other	O
pairwise	O
KD	O
methods	O
,	O
indicating	O
that	O
the	O
learned	O
representations	O
are	O
more	O
robust	O
.	O
We	O
attribute	O
this	O
robustness	O
against	O
"	O
distractors	O
"	O
to	O
the	O
enriched	O
information	O
from	O
inbatch	O
KD	O
,	O
where	O
we	O
are	O
able	O
to	O
exploit	O
all	O
in	O
-	O
batch	O
query	O
-	O
document	O
combinations	O
.	O

In	O
this	O
subsection	O
,	O
we	O
evaluate	O
TCT	O
-	O
ColBERT	O
when	O
training	O
with	O
hard	O
negatives	O
(	O
HNs	O
)	O
.	O
We	O
compare	O
our	O
model	O
to	O
four	O
competitive	O
approaches	O
:	O
1	O
.	O
ANCE	O
is	O
the	O
most	O
representative	O
work	O
,	O
which	O
proposes	O
asynchronous	O
index	O
refreshes	O
to	O
mine	O
hard	O
negatives	O
.	O
The	O
model	O
is	O
trained	O
for	O
600	O
K	O
steps	O
with	O
index	O
refreshes	O
every	O
10	O
K	O
steps	O
.	O
ANCE	O
uses	O
RoBERTa	B-MethodName
-	O
base	O
as	O
its	O
backbone	O
.	O
2	O
.	O
LTRe	O
(	O
Zhan	O
et	O
al	O
,	O
2020	O
)	O
further	O
improves	O
from	O
an	O
ANCE	O
checkpoint	O
by	O
adding	O
more	O
training	O
steps	O
with	O
the	O
same	O
hard	O
negative	O
mining	O
approach	O
;	O
thus	O
,	O
the	O
computation	O
cost	O
of	O
index	O
refreshes	O
from	O
ANCE	O
can	O
not	O
be	O
neglected	O
.	O
LTRe	O
also	O
use	O
RoBERTa	B-MethodName
-	O
base	O
as	O
its	O
backbone	O
.	O
3	O
.	O
SEED	B-DatasetName
-	O
Encoder	O
(	O
Lu	O
et	O
al	O
,	O
2021	O
)	O
leverages	O
a	O
pretraining	O
strategy	O
to	O
enhance	O
the	O
capability	O
of	O
the	O
bi	O
-	O
encoder	O
,	O
which	O
is	O
further	O
fine	O
-	O
tuned	O
with	O
HNs	O
using	O
asynchronous	O
index	O
refreshes	O
.	O
4	O
.	O
RocketQA	O
(	O
Qu	O
et	O
al	O
,	O
2020	O
)	O
trains	O
a	O
bi	O
-	O
encoder	O
model	O
using	O
hard	O
negatives	O
denoised	O
by	O
a	O
crossencoder	O
,	O
ERNIE	O
-	O
2.0	O
-	O
Large	O
(	O
Sun	O
et	O
al	O
,	O
2019	O
)	O
.	O
It	O
further	O
demonstrates	O
that	O
training	O
bi	O
-	O
encoders	O
with	O
many	O
in	O
-	O
batch	O
negatives	O
(	O
batch	B-HyperparameterName
size	I-HyperparameterName
up	O
to	O
4096	O
)	O
significantly	O
improves	O
ranking	O
effectiveness	O
;	O
however	O
,	O
this	O
approach	O
is	O
computationally	O
expensive	O
(	O
the	O
authors	O
report	O
using	O
8×V100	O
GPUs	O
for	O
training	O
)	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
RocketQA	O
represents	O
the	O
state	O
of	O
the	O
art	O
in	O
single	O
-	O
vector	O
bi	O
-	O
encoders	O
for	O
dense	O
retrieval	O
.	O
For	O
a	O
more	O
fair	O
comparison	O
,	O
we	O
also	O
report	O
the	O
ranking	O
effectiveness	O
of	O
their	O
model	O
trained	O
with	O
a	O
smaller	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	O
.	O
For	O
all	O
the	O
approaches	O
above	O
,	O
we	O
directly	O
copy	O
the	O
reported	O
effectiveness	O
from	O
the	O
original	O
papers	O
.	O
For	O
our	O
TCT	O
-	O
ColBERT	O
model	O
,	O
following	O
the	O
settings	O
of	O
the	O
above	O
approaches	O
,	O
we	O
first	O
use	O
our	O
TCT	O
-	O
ColBERT	O
model	O
trained	O
on	O
BM25	O
negatives	O
as	O
a	O
warm	O
-	O
up	O
starting	O
point	O
and	O
index	O
all	O
8.8	O
M	O
MARCO	O
passages	O
.	O
Using	O
the	O
warmed	O
-	O
up	O
index	O
,	O
we	O
retrieve	O
top	O
-	O
200	O
passages	O
for	O
each	O
training	O
query	O
and	O
randomly	O
sample	O
(	O
with	O
replacement	O
)	O
hard	O
negatives	O
from	O
the	O
200	O
candidates	O
to	O
form	O
our	O
training	O
data	O
.	O
Note	O
that	O
due	O
to	O
resource	O
limitations	O
we	O
do	O
not	O
conduct	O
experiments	O
with	O
asynchronous	O
index	O
refreshes	O
since	O
multiple	O
V100	O
GPUs	O
are	O
required	O
for	O
such	O
a	O
model	O
training	O
scheme	O
.	O
2	O
In	O
this	O
experiment	O
,	O
all	O
the	O
hyperparameter	O
settings	O
are	O
the	O
same	O
as	O
the	O
ones	O
in	O
the	O
BM25	O
negative	O
training	O
,	O
except	O
for	O
training	O
steps	O
,	O
which	O
is	O
set	O
to	O
100	O
K	O
for	O
both	O
student	O
and	O
teacher	O
training	O
.	O
Table	O
3	O
reports	O
the	O
results	O
of	O
our	O
experiments	O
with	O
hard	O
negative	O
training	O
.	O
First	O
,	O
we	O
observe	O
that	O
our	O
TCT	O
-	O
ColBERT	O
model	O
trained	O
with	O
BM25	O
negatives	O
marginally	O
outperforms	O
the	O
other	O
models	O
trained	O
with	O
HNs	O
,	O
except	O
for	O
RocketQA	O
.	O
Comparing	O
the	O
different	O
training	O
strategies	O
discussed	O
in	O
Section	O
3.3	O
(	O
second	O
main	O
block	O
of	O
the	O
table	O
)	O
,	O
we	O
see	O
that	O
the	O
ranking	O
effectiveness	O
of	O
TCT	O
-	O
ColBERT	O
(	O
HN	O
)	O
degrades	O
when	O
training	O
on	O
hard	O
negatives	O
without	O
the	O
guide	O
of	O
a	O
teacher	O
.	O
This	O
is	O
consistent	O
with	O
the	O
findings	O
of	O
Qu	O
et	O
al	O
(	O
2020	O
)	O
that	O
hard	O
negatives	O
contain	O
noisy	O
information	O
(	O
i.e.	O
,	O
some	O
hard	O
negatives	O
may	O
actually	O
be	O
relevant	O
)	O
.	O
Also	O
,	O
show	O
that	O
training	O
bi	O
-	O
encoders	O
with	O
hard	O
negatives	O
can	O
be	O
unstable	O
:	O
hard	O
negatives	O
benefit	O
ranking	O
effectiveness	O
only	O
under	O
certain	O
hyperparameter	O
settings	O
.	O
In	O
contrast	O
,	O
hard	O
negative	O
training	O
using	O
Col	O
-	O
BERT	B-MethodName
's	O
in	O
-	O
batch	O
KD	O
further	O
boosts	O
ranking	O
effectiveness	O
,	O
especially	O
when	O
our	O
teacher	O
(	O
ColBERT	O
)	O
2	O
Re	O
-	O
encoding	O
the	O
entire	O
corpus	O
takes	O
∼10	O
hours	O
on	O
one	O
GPU	O
.	O
is	O
trained	O
with	O
the	O
same	O
hard	O
negative	O
samples	O
beforehand	O
.	O
It	O
is	O
also	O
worth	O
noting	O
that	O
our	O
TCT	O
-	O
ColBERT	O
(	O
w/	O
TCT	O
HN+	O
)	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
96	O
yields	O
competitive	O
ranking	O
effectiveness	O
compared	O
to	O
RocketQA	O
(	O
the	O
current	O
state	O
of	O
the	O
art	O
)	O
,	O
which	O
uses	O
batch	B-HyperparameterName
size	I-HyperparameterName
4096	O
.	O
These	O
results	O
demonstrate	O
the	O
advantages	O
of	O
our	O
TCT	O
design	O
:	O
our	O
approach	O
effectively	O
exploits	O
hard	O
negatives	O
in	O
a	O
computationally	O
efficient	O
manner	O
(	O
i.e.	O
,	O
without	O
the	O
need	O
for	O
large	O
batch	O
sizes	O
or	O
periodic	O
index	O
refreshes	O
)	O
.	O

To	O
validate	O
the	O
effectiveness	O
and	O
generality	O
of	O
our	O
training	O
strategy	O
,	O
we	O
conduct	O
further	O
experiments	O
on	O
document	O
retrieval	O
using	O
the	O
MS	B-DatasetName
MARCO	I-DatasetName
document	B-TaskName
ranking	I-TaskName
dataset	O
.	O
This	O
dataset	O
contains	O
3.2	O
M	O
web	O
pages	O
gathered	O
from	O
passages	O
in	O
the	O
MS	B-DatasetName
MARCO	I-DatasetName
passage	B-TaskName
ranking	I-TaskName
dataset	O
.	O
Similar	O
to	O
the	O
passage	O
condition	O
,	O
we	O
evaluate	O
model	O
effectiveness	O
on	O
two	O
test	O
sets	O
of	O
queries	O
:	O
Per	O
official	O
guidelines	O
,	O
we	O
report	O
different	O
metrics	O
for	O
the	O
two	O
query	O
sets	O
:	O
MRR@100	O
for	O
MARCO	O
Dev	O
and	O
NDCG@10	B-MetricName
for	O
TREC	B-DatasetName
-	O
DL	O
'	O
19	O
.	O
Following	O
the	O
FirstP	O
setting	O
for	O
document	O
retrieval	O
described	O
in	O
,	O
we	O
feed	O
the	O
first	O
512	O
tokens	O
of	O
each	O
document	O
for	O
encoding	O
,	O
and	O
start	O
with	O
the	O
warmed	O
-	O
up	O
checkpoint	O
for	O
our	O
encoder	O
's	O
parameters	O
trained	O
for	O
passage	B-TaskName
retrieval	I-TaskName
(	O
using	O
BM25	O
negatives	O
,	O
as	O
described	O
in	O
Section	O
4.1.1	O
)	O
.	O
The	O
settings	O
for	O
fine	O
-	O
tuning	O
our	O
warmed	O
-	O
up	O
encoder	O
Table	O
4	O
:	O
Document	O
retrieval	O
results	O
using	O
the	O
FirstP	O
approach	O
.	O
All	O
our	O
implemented	O
models	O
are	O
labeled	O
with	O
a	O
number	O
and	O
superscripts	O
represent	O
significant	O
improvements	O
over	O
the	O
labeled	O
model	O
(	O
paired	O
t	O
-	O
test	O
,	O
p	O
<	O
0.05	O
)	O
.	O

MARCO	O
Dev	O
TREC	B-DatasetName
-	O
DL	O
'	O
19	O
MRR@100	O
NDCG@10	B-MetricName
ANCE	O
.368	O
.614	O
LTRe	O
(	O
Zhan	O
et	O
al	O
,	O
2020	O
)	O
-	O
.634	O
SEED	B-DatasetName
-	O
Encoder	O
(	O
Lu	O
et	O
al	O
,	O
2021	O
Ranking	O
effectiveness	O
is	O
reported	O
in	O
Table	O
4	O
.	O
First	O
,	O
we	O
observe	O
that	O
TCT	O
-	O
ColBERT	O
(	O
our	O
warmedup	O
checkpoint	O
)	O
performs	O
far	O
worse	O
than	O
other	O
approaches	O
to	O
document	O
retrieval	O
using	O
the	O
FirstP	O
method	O
.	O
This	O
may	O
be	O
due	O
to	O
the	O
fact	O
that	O
FirstP	O
document	O
retrieval	O
is	O
very	O
different	O
from	O
passage	B-TaskName
retrieval	I-TaskName
,	O
making	O
zero	O
-	O
shot	O
transfer	O
ineffective	O
.	O
After	O
applying	O
HN	O
training	O
on	O
both	O
teacher	O
and	O
student	O
models	O
(	O
condition	O
2	O
)	O
,	O
the	O
ranking	O
effectiveness	O
increases	O
significantly	O
.	O
In	O
addition	O
,	O
we	O
find	O
that	O
another	O
iteration	O
of	O
training	O
with	O
an	O
index	O
refresh	O
(	O
condition	O
3	O
)	O
further	O
improves	O
ranking	O
effectiveness	O
.	O
To	O
sum	O
up	O
,	O
in	O
the	O
document	B-TaskName
ranking	I-TaskName
task	O
,	O
TCT	O
-	O
ColBERT	O
yields	O
competitive	O
effectiveness	O
with	O
a	O
one	O
-	O
time	O
index	O
refresh	O
and	O
outperforms	O
other	O
computationally	O
expensive	O
methods	O
with	O
one	O
additional	O
index	O
refresh	O
.	O

In	O
our	O
final	O
set	O
of	O
experiments	O
,	O
we	O
show	O
that	O
dense	O
retrieval	O
with	O
single	O
-	O
vector	O
representations	O
can	O
be	O
integrated	O
with	O
results	O
from	O
sparse	O
retrieval	O
to	O
further	O
increase	O
effectiveness	O
.	O
We	O
illustrate	O
the	O
endto	O
-	O
end	O
tradeoffs	O
in	O
terms	O
of	O
quality	O
,	O
time	O
,	O
and	O
space	O
of	O
different	O
dense	O
-	O
sparse	O
hybrid	O
combinations	O
on	O
the	O
passage	B-TaskName
retrieval	I-TaskName
tasks	O
.	O
Many	O
papers	O
(	O
Luan	O
et	O
al	O
,	O
2021	O
;	O
Gao	O
et	O
al	O
,	O
2020b	O
;	O
have	O
demonstrated	O
that	O
sparse	O
retrieval	O
can	O
complement	O
dense	O
retrieval	O
via	O
a	O
simple	O
linear	O
combination	O
of	O
their	O
scores	O
.	O
In	O
our	O
implementation	O
,	O
for	O
each	O
query	O
q	O
,	O
we	O
use	O
sparse	O
and	O
dense	O
techniques	O
to	O
retrieve	O
the	O
top	O
-	O
1000	O
passages	O
,	O
D	O
sp	O
and	O
D	O
ds	O
,	O
with	O
their	O
relevance	O
scores	O
,	O
φ	O
sp	O
(	O
q	O
,	O
p	O
D	O
sp	O
)	O
and	O
φ	O
ds	O
(	O
q	O
,	O
p	O
D	O
ds	O
)	O
,	O
respectively	O
.	O
Then	O
,	O
we	O
compute	O
the	O
final	O
relevance	O
score	O
for	O
each	O
retrieved	O
passage	O
φ	O
(	O
q	O
,	O
p	O
)	O
,	O
where	O
p	O
D	O
sp	O
∪	O
D	O
ds	O
,	O
as	O
follows	O
:	O
	O
	O
	O
	O
	O
	O
	O
	O
α	B-HyperparameterName
φ	O
sp	O
(	O
q	O
,	O
p	O
)	O
+	O
min	O
p	O
D	O
ds	O
φ	O
ds	O
(	O
q	O
,	O
p	O
)	O
,	O
if	O
p	O
/	O
D	O
ds	O
α	B-HyperparameterName
min	O
p	O
Dsp	O
φ	O
sp	O
(	O
q	O
,	O
p	O
)	O
+	O
φ	O
ds	O
(	O
q	O
,	O
p	O
)	O
,	O
if	O
p	O
/	O
D	O
sp	O
α	B-HyperparameterName
φ	O
sp	O
(	O
q	O
,	O
p	O
)	O
+	O
φ	O
ds	O
(	O
q	O
,	O
p	O
)	O
,	O
otherwise	O
.	O
This	O
technique	O
is	O
an	O
approximation	O
of	O
a	O
linear	O
combination	O
of	O
sparse	O
and	O
dense	O
retrieval	O
scores	O
.	O
Specifically	O
,	O
if	O
p	O
/	O
D	O
sp	O
(	O
or	O
D	O
ds	O
)	O
,	O
we	O
instead	O
use	O
the	O
minimum	O
score	O
of	O
φ	O
sp	O
(	O
q	O
,	O
p	O
D	O
sp	O
)	O
,	O
or	O
φ	O
ds	O
(	O
q	O
,	O
p	O
D	O
ds	O
)	O
as	O
a	O
substitute	O
.	O
For	O
the	O
sparse	O
and	O
dense	O
retrieval	O
combinations	O
,	O
we	O
tune	O
the	O
hyperparameter	O
α	B-HyperparameterName
on	O
6000	O
randomly	O
sampled	O
queries	O
from	O
the	O
MS	B-DatasetName
MARCO	I-DatasetName
training	O
set	O
.	O
We	O
conduct	O
dense	O
-	O
sparse	O
hybrid	O
experiments	O
with	O
sparse	O
retrieval	O
(	O
BM25	O
ranking	O
)	O
on	O
the	O
original	O
passages	O
(	O
denoted	O
BM25	O
)	O
and	O
on	O
passages	O
with	O
docTTTTTquery	O
document	O
expansion	O
(	O
Nogueira	O
and	O
Lin	O
,	O
2019	O
)	O
(	O
denoted	O
doc2query	O
-	O
T5	B-MethodName
)	O
.	O
To	O
characterize	O
end	O
-	O
to	O
-	O
end	O
effectiveness	O
and	O
efficiency	O
,	O
we	O
perform	O
sparse	O
retrieval	O
with	O
the	O
Pyserini	O
toolkit	O
and	O
dense	O
retrieval	O
with	O
Faiss	O
(	O
Johnson	O
et	O
al	O
,	O
2017	O
)	O
,	O
but	O
implement	O
the	O
score	O
combination	O
in	O
separate	O
custom	O
code	O
.	O
Table	O
5	O
shows	O
passage	B-TaskName
retrieval	I-TaskName
results	O
in	O
terms	O
of	O
ranking	O
effectiveness	O
,	O
query	O
latency	O
,	O
and	O
storage	O
requirements	O
(	O
i.e.	O
,	O
index	O
size	O
)	O
for	O
each	O
model	O
and	O
Table	O
6	O
reports	O
the	O
component	O
latencies	O
of	O
our	O
TCT	O
-	O
ColBERT	O
dense	O
-	O
sparse	O
hybrid	O
.	O
3	O
The	O
crossencoder	O
reranker	O
of	O
Nogueira	O
and	O
Cho	O
(	O
2019	O
)	O
provides	O
a	O
point	O
of	O
reference	O
for	O
multi	O
-	O
stage	O
reranking	O
designs	O
,	O
which	O
is	O
effective	O
but	O
slow	O
.	O
Generally	O
,	O
dense	O
retrieval	O
methods	O
(	O
whether	O
single	O
-	O
vector	O
or	O
multi	O
-	O
vector	O
)	O
are	O
more	O
effective	O
but	O
slower	O
than	O
sparse	O
retrieval	O
methods	O
,	O
which	O
rely	O
on	O
bag	O
-	O
of	O
-	O
words	O
querying	O
using	O
inverted	O
indexes	O
.	O
Single	O
-	O
vector	O
dense	O
models	O
also	O
require	O
more	O
space	O
than	O
sparse	O
retrieval	O
methods	O
.	O
Moving	O
(	O
Dai	O
and	O
Callan	O
,	O
2020	O
)	O
.243	O
.551	O
55	O
4	O
doc2query	O
-	O
T5	B-MethodName
(	O
Nogueira	O
and	O
Lin	O
,	O
2019	O
)	O
.277	O
.551	O
64	O
14	O
Dense	O
retrieval	O
:	O
single	O
-	O
vector	O
TAS	O
-	O
B	O
(	O
Hofstätter	O
et	O
al	O
,	O
2021	O
)	O
.343	O
.722	O
64	O
13	O
RocketQA	O
(	O
Qu	O
et	O
al	O
,	O
2020	O
)	O
.370	O
-	O
107	O
b	O
13	O
a	O
TCT	O
-	O
ColBERT	O
.344	O
.685	O
107	O
13	O
TCT	O
-	O
ColBERT	O
(	O
w/	O
TCT	O
HN+	O
)	O
.359	O
.719	O
107	O
13	O
Dense	O
retrieval	O
:	O
multi	O
-	O
vector	O
ME	O
-	O
BERT	B-MethodName
(	O
Luan	O
et	O
al	O
,	O
2021	O
)	O
.334	O
.687	O
-	O
96	O
ColBERT	O
(	O
Khattab	O
and	O
Zaharia	O
,	O
2020	O
)	O
.360	O
-	O
458	O
154	O
Hybrid	O
dense	O
+	O
sparse	O
CLEAR	B-DatasetName
(	O
Gao	O
et	O
al	O
,	O
2020b	O
)	O
.338	O
.699	O
-	O
17	O
a	O
ME	O
-	O
HYBRID	O
-	O
E	O
(	O
Luan	O
et	O
al	O
,	O
2021	O
)	O
.343	O
.706	O
-	O
100	O
TAS	O
-	O
B	O
+	O
doc2query	O
-	O
T5	B-MethodName
(	O
Hofstätter	O
et	O
al	O
,	O
2021	O
)	O
.	O
(	O
Hofstätter	O
et	O
al	O
,	O
2021	O
)	O
.421	O
.759	O
12800	O
27	O
a	O
RocketQA	O
with	O
reranking	O
(	O
Qu	O
et	O
al	O
,	O
2020	O
)	O
.439	O
-	O
-	O
13	O
a	O
a	O
We	O
estimate	O
dense	O
index	O
size	O
using	O
16	O
-	O
bit	O
floats	O
;	O
for	O
hybrid	O
,	O
we	O
add	O
the	O
sizes	O
of	O
sparse	O
and	O
dense	O
indexes	O
.	O
b	O
We	O
assume	O
latency	O
comparable	O
to	O
our	O
settings	O
.	O
from	O
single	O
-	O
vector	O
to	O
multi	O
-	O
vector	O
dense	O
models	O
,	O
we	O
see	O
that	O
ColBERT	O
exhibits	O
higher	O
effectiveness	O
but	O
is	O
slower	O
and	O
requires	O
much	O
more	O
storage	O
.	O
Finally	O
,	O
when	O
integrated	O
with	O
sparse	O
retrieval	O
methods	O
,	O
TCT	O
-	O
ColBERT	O
is	O
able	O
to	O
beat	O
a	O
basic	O
multi	O
-	O
stage	O
reranking	O
design	O
(	O
BM25	O
+	O
BERTlarge	O
)	O
,	O
but	O
with	O
much	O
lower	O
query	O
latency	O
,	O
although	O
at	O
the	O
cost	O
of	O
increased	O
storage	O
.	O
Hybrid	O
TCT	O
-	O
ColBERT	O
(	O
w/	O
TCT	O
HN+	O
)	O
+	O
doc2query	O
-	O
T5	B-MethodName
compares	O
favorably	O
with	O
a	O
recent	O
advanced	O
model	O
,	O
TAS	O
-	O
B	O
+	O
doc2query	O
-	O
T5	B-MethodName
(	O
Hofstätter	O
et	O
al	O
,	O
2021	O
)	O
,	O
which	O
introduces	O
topic	O
-	O
aware	O
sampling	O
and	O
dual	O
teachers	O
,	O
incorporating	O
part	O
of	O
our	O
TCT	O
-	O
ColBERT	O
work	O
.	O
Nevertheless	O
,	O
even	O
the	O
best	O
hybrid	O
variant	O
of	O
TCT	O
-	O
ColBERT	O
alone	O
,	O
without	O
further	O
reranking	O
,	O
remains	O
quite	O
some	O
distance	O
from	O
RocketQA	O
,	O
the	O
current	O
state	O
of	O
the	O
art	O
(	O
with	O
reranking	O
using	O
cross	O
-	O
encoders	O
)	O
.	O
This	O
suggests	O
that	O
there	O
remain	O
relevance	O
signals	O
that	O
require	O
full	O
attention	O
interactions	O
to	O
exploit	O
.	O

A	O
string	O
language	O
L	O
over	O
alphabet	O
Σ	O
is	O
strictly	O
local	O
(	O
SL	O
)	O
iff	O
there	O
is	O
some	O
k	O
N	O
such	O
that	O
L	O
is	O
generated	O
by	O
a	O
strictly	O
k	O
-	O
local	O
grammar	O
G.	O
Such	O
a	O
grammar	O
consists	O
of	O
a	O
finite	O
set	O
of	O
k	O
-	O
grams	O
,	O
each	O
one	O
of	O
which	O
describes	O
an	O
illicit	O
substring	O
.	O
More	O
precisely	O
,	O
given	O
a	O
string	O
w	O
Σ	O
*	O
,	O
letŵ	O
k	O
:	O
=	O
k	O
w	O
k	O
(	O
where	O
,	O
/	O
Σ	O
)	O
and	O
k	O
-	O
grams	O
(	O
w	O
)	O
:	O
=	O
{	O
s	O
|	O
s	O
is	O
a	O
substring	O
ofŵ	O
k−1	O
of	O
length	O
k	O
}	O
.	O
Then	O
G	O
generates	O
string	O
w	O
iff	O
k	O
-	O
grams	O
(	O
w	O
)	O
∩	O
G	O
=	O
.	O
That	O
is	O
to	O
say	O
,	O
G	O
generates	O
every	O
string	O
over	O
Σ	O
that	O
does	O
not	O
contain	O
an	O
illicit	O
substring	O
.	O
Most	O
phonological	O
dependencies	O
can	O
be	O
described	O
in	O
strictly	O
local	O
terms	O
-	O
see	O
Heinz	O
(	O
2015	O
)	O
for	O
numerous	O
examples	O
.	O
Consider	O
for	O
instance	O
the	O
well	O
-	O
known	O
process	O
of	O
word	O
-	O
final	O
obstruent	O
devoicing	O
that	O
forces	O
voiced	O
obstruents	O
at	O
the	O
end	O
of	O
the	O
word	O
to	O
be	O
realized	O
as	O
voiceless	O
:	O
moroz	O
[	O
maros	O
]	O
'	O
frost	O
'	O
in	O
Russian	O
,	O
Bad	O
[	O
bat	O
]	O
'	O
bath	O
'	O
in	O
German	O
)	O
.	O
If	O
one	O
considers	O
phonotactics	O
rather	O
than	O
mappings	O
from	O
underlying	O
representations	O
to	O
surface	O
forms	O
,	O
this	O
is	O
tantamount	O
to	O
a	O
ban	O
against	O
word	O
-	O
final	O
voiced	O
obstruents	O
.	O
Said	O
ban	O
,	O
in	O
turn	O
,	O
is	O
captured	O
by	O
a	O
strictly	O
2	O
-	O
local	O
grammar	O
G	O
that	O
contains	O
all	O
bigrams	O
of	O
the	O
form	O
v	O
,	O
with	O
v	O
a	O
voiced	O
obstruent	O
.	O
The	O
specification	O
of	O
SL	O
grammars	O
can	O
be	O
simplified	O
by	O
applying	O
mappings	O
.	O
In	O
the	O
case	O
at	O
hand	O
,	O
one	O
could	O
define	O
a	O
function	O
f	O
that	O
replaces	O
every	O
voiced	O
obstruent	O
by	O
the	O
designated	O
symbol	O
♦	O
so	O
that	O
the	O
grammar	O
G	O
can	O
be	O
reduced	O
to	O
the	O
single	O
bigram	O
♦	O
.	O
One	O
has	O
to	O
be	O
careful	O
,	O
though	O
.	O
The	O
SL	O
languages	O
are	O
not	O
closed	O
under	O
relabelings	O
,	O
in	O
fact	O
,	O
every	O
regular	O
language	O
is	O
the	O
image	O
of	O
a	O
strictly	O
2local	O
language	O
under	O
some	O
relabeling	O
.	O
However	O
,	O
the	O
directionality	O
of	O
the	O
♦	O
-	O
relabeling	O
above	O
is	O
the	O
opposite	O
:	O
first	O
the	O
relabeling	O
is	O
applied	O
,	O
and	O
then	O
the	O
grammar	O
filters	O
out	O
strings	O
in	O
the	O
image	O
of	O
that	O
relabeling	O
.	O
As	O
long	O
as	O
the	O
relabeling	O
is	O
a	O
manyto	O
-	O
one	O
map	O
between	O
alphabets	O
(	O
and	O
thus	O
does	O
not	O
introduce	O
distinctions	O
that	O
were	O
n't	O
already	O
part	O
of	O
the	O
original	O
alphabet	O
)	O
,	O
this	O
provably	O
does	O
not	O
increase	O
weak	O
generative	O
capacity	O
for	O
any	O
of	O
the	O
formalisms	O
discussed	O
in	O
this	O
paper	O
.	O
We	O
make	O
use	O
of	O
such	O
relabelings	O
in	O
the	O
following	O
sections	O
in	O
order	O
to	O
convert	O
natural	O
language	O
patterns	O
into	O
more	O
easily	O
described	O
formal	O
languages	O
.	O
For	O
morphotactics	O
,	O
though	O
,	O
this	O
raises	O
the	O
issue	O
how	O
the	O
size	O
of	O
the	O
atomic	O
units	O
should	O
be	O
chosen	O
.	O
One	O
could	O
posit	O
that	O
morphology	O
,	O
just	O
like	O
phonology	O
,	O
treats	O
every	O
phonological	O
segment	O
as	O
a	O
symbol	O
.	O
In	O
that	O
case	O
,	O
stems	O
and	O
morphemes	O
are	O
strings	O
of	O
symbols	O
.	O
Alternatively	O
,	O
one	O
may	O
treat	O
each	O
morpheme	O
,	O
including	O
stems	O
,	O
as	O
an	O
atomic	O
symbol	O
.	O
This	O
is	O
an	O
important	O
decision	O
when	O
it	O
comes	O
to	O
modeling	O
the	O
interactions	O
of	O
morphology	O
and	O
phonology	O
such	O
as	O
phonologically	O
conditioned	O
allomorphy	O
.	O
Fortunately	O
our	O
results	O
are	O
independent	O
of	O
this	O
choice	O
,	O
due	O
to	O
the	O
productive	O
nature	O
of	O
compounding	O
.	O
To	O
better	O
understand	O
why	O
different	O
representations	O
could	O
in	O
principle	O
affect	O
subregular	O
complexity	O
,	O
note	O
first	O
that	O
whether	O
a	O
stem	O
is	O
represented	O
as	O
a	O
single	O
,	O
atomic	O
symbol	O
or	O
as	O
a	O
sequence	O
of	O
phonological	O
segments	O
seems	O
to	O
determine	O
if	O
prefixes	O
and	O
suffixes	O
might	O
be	O
separated	O
by	O
an	O
unbounded	O
amount	O
of	O
symbols	O
.	O
Consider	O
a	O
circumfix	O
u	O
-	O
-	O
v	O
,	O
where	O
neither	O
part	O
of	O
the	O
affix	O
may	O
occur	O
without	O
the	O
other	O
.	O
A	O
concrete	O
example	O
is	O
the	O
nominalization	O
circumfix	O
ke	O
-	O
-	O
an	O
in	O
Indonesian	O
(	O
Mahdi	O
,	O
2012	O
;	O
Sneddon	O
,	O
1996	O
)	O
:	O
(	O
1	O
)	O
a.	O
tingii	O
high	O
b.	O
ke	O
-	O
NMN	O
-	O
tinggi	O
high	O
-	O
an	O
-	O
NMN	O
'	O
altitude	O
'	O
If	O
a	O
stem	O
is	O
a	O
single	O
symbol	O
x	O
,	O
then	O
x	O
and	O
uxv	O
are	O
well	O
-	O
formed	O
whereas	O
ux	O
and	O
xv	O
are	O
not	O
due	O
to	O
u	O
-	O
-	O
v	O
being	O
a	O
circumfix	O
whose	O
subparts	O
can	O
not	O
occur	O
in	O
isolation	O
.	O
This	O
generalization	O
is	O
easily	O
captured	O
by	O
the	O
strictly	O
3	O
-	O
local	O
grammar	O
{	O
xv	O
,	O
ux	O
}	O
.	O
However	O
,	O
if	O
stems	O
are	O
sequences	O
of	O
symbols	O
,	O
then	O
the	O
well	O
-	O
formed	O
patterns	O
are	O
of	O
the	O
form	O
x	O
+	O
or	O
ux	O
+	O
v	O
(	O
since	O
the	O
length	O
of	O
stems	O
is	O
in	O
principle	O
unbounded	O
)	O
.	O
The	O
illict	O
strings	O
,	O
on	O
the	O
other	O
hand	O
,	O
are	O
of	O
the	O
form	O
x	O
+	O
v	O
and	O
ux	O
+	O
.	O
But	O
no	O
strictly	O
local	O
grammar	O
can	O
generate	O
the	O
former	O
patterns	O
without	O
also	O
generating	O
the	O
latter	O
.	O
That	O
is	O
due	O
to	O
the	O
strictly	O
local	O
languages	O
being	O
closed	O
under	O
suffix	O
substitution	O
closure	O
.	O
Suffix	O
Substitution	O
Closure	O
Language	O
L	O
is	O
SL	O
iff	O
there	O
exists	O
a	O
k	O
N	O
such	O
that	O
for	O
all	O
strings	O
u	O
1	O
,	O
v	O
1	O
,	O
u	O
2	O
,	O
v	O
2	O
and	O
any	O
string	O
x	O
of	O
length	O
k	O
−	O
1	O
,	O
if	O
u	O
1	O
xv	O
1	O
,	O
u	O
2	O
xv	O
2	O
L	O
,	O
then	O
u	O
1	O
xv	O
2	O
L.	O
If	O
there	O
is	O
no	O
upper	O
bound	O
on	O
the	O
length	O
of	O
stems	O
,	O
then	O
we	O
can	O
infer	O
from	O
x	O
k	O
L	O
and	O
ux	O
k	O
v	O
L	O
that	O
both	O
x	O
k	O
v	O
L	O
and	O
ux	O
k	O
L.	O
It	O
seems	O
,	O
then	O
,	O
that	O
circumfixes	O
are	O
strictly	O
local	O
only	O
if	O
each	O
stem	O
is	O
an	O
atomic	O
symbol	O
.	O
But	O
this	O
line	O
of	O
reasoning	O
erroneously	O
assumes	O
that	O
the	O
circumfix	O
can	O
only	O
apply	O
to	O
individual	O
stems	O
,	O
which	O
ignores	O
the	O
availability	O
of	O
compounding	O
.	O
Returning	O
to	O
Indonesian	O
,	O
we	O
see	O
that	O
its	O
nominalization	O
marker	O
is	O
not	O
restricted	O
to	O
single	O
stems	O
and	O
can	O
also	O
apply	O
to	O
compounds	O
.	O
(	O
2	O
)	O
a.	O
maha	O
big	O
siswa	O
pupil	O
'	O
student	O
'	O
b.	O
ke	O
-	O
NMN	O
-	O
maha	O
big	O
siswa	O
pupil	O
-	O
an	O
-	O
NMN	O
'	O
student	O
affairs	O
'	O
Compounding	O
is	O
an	O
unbounded	O
process	O
,	O
so	O
even	O
if	O
each	O
stem	O
is	O
mapped	O
to	O
a	O
single	O
symbol	O
x	O
,	O
one	O
ends	O
up	O
with	O
the	O
same	O
patterns	O
as	O
with	O
the	O
segmental	O
mapping	O
approach	O
:	O
x	O
+	O
and	O
ux	O
+	O
v	O
are	O
well	O
-	O
formed	O
,	O
while	O
ux	O
+	O
and	O
x	O
+	O
v	O
are	O
ill	O
-	O
formed	O
.	O
Since	O
the	O
choice	O
of	O
representation	O
does	O
not	O
affect	O
the	O
subregular	O
complexity	O
results	O
,	O
we	O
opt	O
for	O
the	O
segmental	O
mapping	O
,	O
which	O
does	O
not	O
require	O
us	O
to	O
use	O
compounding	O
in	O
all	O
our	O
natural	O
language	O
data	O
points	O
.	O
The	O
details	O
of	O
the	O
segmental	O
mapping	O
are	O
as	O
follows	O
:	O
within	O
a	O
stem	O
,	O
all	O
segments	O
are	O
replaced	O
by	O
some	O
distinguished	O
symbol	O
.	O
We	O
choose	O
x	O
for	O
this	O
purpose	O
.	O
All	O
morphemes	O
,	O
on	O
the	O
other	O
hand	O
,	O
are	O
replaced	O
by	O
single	O
symbols	O
.	O
Symbols	O
are	O
chosen	O
to	O
maximize	O
clarity	O
of	O
exposition	O
,	O
so	O
that	O
we	O
sometimes	O
assign	O
each	O
morpheme	O
a	O
unique	O
symbol	O
and	O
sometimes	O
map	O
irrelevant	O
morphemes	O
to	O
a	O
randomly	O
chosen	O
filler	O
symbol	O
.	O
For	O
some	O
linguistic	O
phenomena	O
we	O
follow	O
linguistic	O
convention	O
in	O
assuming	O
that	O
the	O
underlying	O
representations	O
contain	O
additional	O
distinguished	O
symbols	O
to	O
mark	O
the	O
edges	O
of	O
the	O
stem	O
-	O
this	O
will	O
be	O
mentioned	O
explicitly	O
for	O
all	O
relevant	O
cases	O
.	O
The	O
preceding	O
discussion	O
yielded	O
as	O
a	O
nice	O
sideresult	O
that	O
circumfixation	O
is	O
not	O
SL	O
unless	O
each	O
part	O
of	O
the	O
circumfix	O
can	O
also	O
occur	O
on	O
its	O
own	O
.	O
Few	O
circumfixes	O
display	O
that	O
kind	O
of	O
freedom	O
,	O
wherefore	O
not	O
all	O
aspects	O
of	O
morphotactics	O
are	O
SL	O
.	O
However	O
,	O
large	O
swaths	O
of	O
morphology	O
still	O
are	O
,	O
with	O
a	O
couple	O
of	O
examples	O
from	O
English	O
given	O
below	O
:	O
(	O
3	O
)	O
a.	O
una	O
-	O
do	O
xx	O
b.	O
break	O
xxxxx	O
-	O
able	O
-	O
b	O
(	O
4	O
)	O
a.	O
punch	O
xxxxx	O
-	O
ed	O
-	O
c	O
b.	O
put	O
xxx	O
-	O
ε	B-HyperparameterName
-	O
c	O
Any	O
kind	O
of	O
affix	O
that	O
only	O
consists	O
of	O
one	O
part	O
and	O
whose	O
distribution	O
is	O
determined	O
within	O
a	O
locally	O
bounded	O
domain	O
is	O
part	O
of	O
strictly	O
local	O
morphotactics	O
.	O
Although	O
we	O
did	O
not	O
carry	O
out	O
any	O
rigorous	O
quantitative	O
comparisons	O
,	O
we	O
believe	O
the	O
majority	O
of	O
morphological	O
dependencies	O
to	O
belong	O
to	O
this	O
class	O
.	O

As	O
pointed	O
out	O
in	O
the	O
previous	O
section	O
,	O
the	O
Swahili	O
pattern	O
is	O
n't	O
too	O
dissimilar	O
from	O
the	O
phonological	O
requirement	O
that	O
no	O
word	O
has	O
more	O
than	O
one	O
primary	O
stress	O
.	O
However	O
,	O
the	O
distribution	O
of	O
primary	O
stress	O
is	O
more	O
specific	O
than	O
that	O
:	O
every	O
phonological	O
word	O
has	O
exactly	O
one	O
primary	O
stress	O
.	O
Ensuring	O
the	O
presence	O
of	O
at	O
least	O
one	O
primary	O
stress	O
is	O
beyond	O
the	O
capabilities	O
of	O
SP	O
grammars	O
-	O
once	O
again	O
this	O
holds	O
because	O
every	O
subsequence	O
of	O
an	O
ill	O
-	O
formed	O
word	O
without	O
primary	O
stress	O
is	O
also	O
a	O
subsequence	O
of	O
the	O
well	O
-	O
formed	O
counterpart	O
with	O
exactly	O
one	O
primary	O
stress	O
.	O
A	O
better	O
model	O
for	O
primary	O
stress	O
assignment	O
is	O
furnished	O
by	O
tier	O
-	O
based	O
strictly	O
local	O
(	O
TSL	O
;	O
Heinz	O
et	O
al	O
(	O
2011	O
)	O
)	O
grammars	O
,	O
which	O
also	O
happen	O
to	O
be	O
powerful	O
enough	O
for	O
circumfixation	O
.	O
A	O
TSL	O
grammar	O
is	O
an	O
SL	O
grammar	O
that	O
operates	O
over	O
a	O
tier	O
,	O
a	O
specific	O
substructure	O
of	O
the	O
string	O
.	O
Given	O
a	O
tier	O
-	O
alphabet	O
T	O
⊆	O
Σ	O
,	O
let	O
E	O
T	O
be	O
a	O
mapping	O
that	O
erases	O
all	O
symbols	O
in	O
a	O
string	O
that	O
do	O
not	O
belong	O
to	O
T	O
.	O
First	O
,	O
E	O
T	O
(	O
ε	B-HyperparameterName
)	O
=	O
ε	B-HyperparameterName
.	O
Then	O
for	O
a	O
Σ	O
and	O
w	O
Σ	O
*	O
,	O
E	O
T	O
(	O
aw	O
)	O
:	O
=	O
a	O
E	O
T	O
(	O
w	O
)	O
if	O
a	O
T	O
E	O
T	O
(	O
w	O
)	O
otherwise	O
The	O
T	O
-	O
tier	O
of	O
a	O
string	O
w	O
is	O
its	O
image	O
under	O
E	O
T	O
.	O
A	O
tier	O
-	O
based	O
strictly	O
k	O
-	O
local	O
grammar	O
G	O
is	O
a	O
pair	O
K	O
,	O
T	O
where	O
K	O
is	O
a	O
strictly	O
k	O
-	O
local	O
grammar	O
over	O
tier	O
-	O
alphabet	O
T	O
.	O
The	O
grammar	O
generates	O
the	O
language	O
L	O
(	O
G	O
)	O
:	O
=	O
{	O
w	O
|	O
E	O
T	O
(	O
w	O
)	O
L	O
(	O
K	O
)	O
}	O
.	O
Note	O
that	O
every	O
SL	O
language	O
is	O
a	O
TSL	O
language	O
with	O
T	O
=	O
Σ.	O
The	O
distribution	O
of	O
primary	O
stress	O
is	O
tier	O
-	O
based	O
strictly	O
2	O
-	O
local	O
.	O
Assuming	O
that	O
primary	O
stress	O
is	O
indicated	O
as	O
some	O
diacritic	O
on	O
symbols	O
,	O
the	O
tieralphabet	O
T	O
contains	O
all	O
symbols	O
with	O
this	O
diacritic	O
.	O
This	O
is	O
tantamount	O
to	O
projecting	O
a	O
tier	O
that	O
only	O
contains	O
segments	O
with	O
primary	O
stress	O
.	O
The	O
grammar	O
then	O
contains	O
the	O
bigram	O
to	O
block	O
words	O
with	O
an	O
empty	O
primary	O
stress	O
tier	O
,	O
i.e.	O
words	O
that	O
contain	O
no	O
primary	O
stress	O
.	O
In	O
addition	O
,	O
every	O
bigram	O
uv	O
for	O
u	O
,	O
v	O
T	O
is	O
added	O
to	O
rule	O
out	O
words	O
with	O
more	O
than	O
one	O
primary	O
stress	O
.	O
The	O
requirement	O
of	O
exactly	O
one	O
primary	O
stress	O
per	O
word	O
thus	O
boils	O
down	O
to	O
having	O
exactly	O
one	O
segment	O
on	O
the	O
primary	O
stress	O
tier	O
,	O
which	O
is	O
a	O
strictly	O
local	O
dependency	O
over	O
that	O
tier	O
.	O
The	O
Swahili	O
pattern	O
from	O
the	O
previous	O
section	O
can	O
also	O
be	O
analyzed	O
as	O
tier	O
-	O
based	O
strictly	O
local	O
,	O
and	O
the	O
same	O
is	O
true	O
for	O
circumfixation	O
.	O
For	O
Swahili	O
we	O
project	O
a	O
tier	O
that	O
contains	O
only	O
the	O
affix	O
vyo	O
,	O
and	O
we	O
do	O
not	O
allow	O
more	O
than	O
one	O
segment	O
on	O
this	O
tier	O
.	O
As	O
a	O
result	O
,	O
vyo	O
occurs	O
at	O
most	O
once	O
per	O
word	O
.	O
To	O
ensure	O
that	O
vyo	O
is	O
a	O
prefix	O
whenever	O
si	O
is	O
present	O
,	O
we	O
furthermore	O
postulate	O
a	O
marker	O
#	O
that	O
indicates	O
the	O
edges	O
of	O
the	O
stem	O
.	O
The	O
projected	O
tier	O
then	O
includes	O
all	O
instances	O
of	O
vyo	O
,	O
si	O
and	O
the	O
marker	O
#	O
(	O
of	O
which	O
there	O
are	O
exactly	O
two	O
)	O
.	O
On	O
this	O
tier	O
,	O
the	O
4	O
-	O
gram	O
si##vyo	O
correctly	O
excludes	O
all	O
ill	O
-	O
formed	O
cases	O
of	O
vyo	O
as	O
a	O
suffix	O
,	O
whereas	O
vyo	O
#	O
#	O
prevents	O
vyo	O
from	O
occurring	O
as	O
a	O
prefix	O
in	O
the	O
absence	O
of	O
si	O
.	O
Adapting	O
the	O
ban	O
against	O
two	O
instances	O
of	O
vyo	O
to	O
this	O
slightly	O
expanded	O
tier	O
is	O
left	O
as	O
an	O
exercise	O
to	O
the	O
reader	O
.	O
In	O
order	O
to	O
regulate	O
the	O
distribution	O
of	O
circumfixes	O
such	O
as	O
u	O
-	O
-	O
v	O
,	O
we	O
have	O
to	O
project	O
a	O
tier	O
that	O
contains	O
only	O
those	O
subparts	O
u	O
and	O
v.	O
If	O
the	O
affixes	O
can	O
never	O
occur	O
by	O
themselves	O
,	O
then	O
we	O
block	O
v	O
and	O
u	O
.	O
Removing	O
one	O
of	O
those	O
two	O
bigrams	O
creates	O
asymmetric	O
cases	O
where	O
one	O
of	O
the	O
affixesbut	O
not	O
the	O
other	O
-	O
is	O
sometimes	O
allowed	O
to	O
be	O
present	O
by	O
itself	O
.	O
We	O
also	O
add	O
uu	O
and	O
vv	O
to	O
block	O
strings	O
where	O
the	O
prefix	O
parts	O
outnumber	O
or	O
are	O
outnumbered	O
by	O
the	O
suffix	O
parts	O
of	O
the	O
circumfix	O
.	O
Note	O
that	O
this	O
has	O
the	O
side	O
effect	O
of	O
also	O
prohibiting	O
unbounded	O
circumfixation	O
,	O
a	O
point	O
we	O
return	O
to	O
in	O
Sec	O
.	O
3.2	O
.	O
At	O
this	O
point	O
,	O
we	O
can	O
safely	O
say	O
that	O
natural	O
language	O
morphotactics	O
is	O
at	O
least	O
TSL	O
(	O
barring	O
the	O
discovery	O
of	O
any	O
intermediate	O
classes	O
between	O
SL	O
and	O
TSL	O
,	O
or	O
SP	O
and	O
TSL	O
)	O
.	O
SL	O
is	O
sufficiently	O
powerful	O
for	O
large	O
parts	O
of	O
morphology	O
,	O
but	O
any	O
kind	O
of	O
dependency	O
that	O
involves	O
both	O
prefixes	O
and	O
suffixes	O
is	O
likely	O
not	O
SL	O
.	O
Some	O
patterns	O
that	O
are	O
not	O
SL	O
are	O
SP	O
,	O
but	O
these	O
also	O
turn	O
out	O
to	O
be	O
TSL	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
there	O
are	O
no	O
morphological	O
dependencies	O
that	O
are	O
SP	O
but	O
not	O
TSL	O
(	O
even	O
though	O
the	O
two	O
language	O
classes	O
are	O
incomparable	O
)	O
.	O
We	O
thus	O
put	O
forward	O
the	O
following	O
conjecture	O
:	O
Subregular	O
Morphotactics	O
All	O
morphotactic	O
dependencies	O
are	O
tier	O
-	O
based	O
strictly	O
local	O
.	O
As	O
any	O
universal	O
claim	O
about	O
the	O
real	O
world	O
,	O
our	O
conjecture	O
can	O
not	O
be	O
proved	O
conclusively	O
-	O
the	O
fact	O
that	O
no	O
counterexamples	O
have	O
been	O
encountered	O
does	O
not	O
guarantee	O
that	O
counterexamples	O
will	O
never	O
be	O
encountered	O
.	O
But	O
there	O
are	O
additional	O
reasons	O
to	O
consider	O
TSL	O
a	O
better	O
fit	O
for	O
morphotactics	O
than	O
one	O
of	O
the	O
more	O
powerful	O
classes	O
.	O
Moving	O
beyond	O
TSL	O
in	O
the	O
subregular	O
hierarchy	O
would	O
take	O
us	O
into	O
the	O
class	O
of	O
star	O
-	O
free	O
languages	O
,	O
which	O
are	O
equivalent	O
to	O
the	O
string	O
sets	O
definable	O
in	O
first	O
-	O
order	O
logic	O
with	O
the	O
transitive	O
closure	O
of	O
the	O
successor	O
relation	O
.	O
As	O
mentioned	O
before	O
,	O
every	O
language	O
that	O
is	O
generated	O
by	O
a	O
tier	O
-	O
based	O
strictly	O
k	O
-	O
local	O
grammar	O
can	O
be	O
identified	O
in	O
the	O
limited	O
from	O
positive	O
text	O
,	O
provided	O
the	O
learner	O
knows	O
the	O
value	O
of	O
k.	O
The	O
class	O
of	O
star	O
-	O
free	O
languages	O
,	O
on	O
the	O
other	O
hand	O
,	O
is	O
not	O
learnable	O
in	O
the	O
limit	O
from	O
positive	O
text	O
.	O
It	O
also	O
makes	O
largely	O
incorrect	O
typological	O
predictions	O
:	O
Unlike	O
TSL	O
,	O
the	O
class	O
of	O
starfree	O
languages	O
is	O
closed	O
under	O
union	O
and	O
relative	O
complement	O
.	O
But	O
the	O
union	O
or	O
relative	O
complement	O
of	O
two	O
morphotactic	O
systems	O
attested	O
in	O
natural	O
languages	O
rarely	O
yields	O
linguistically	O
plausible	O
morphotactics	O
.	O
Similarly	O
,	O
it	O
is	O
trivial	O
to	O
write	O
firstorder	O
formulas	O
for	O
highly	O
unnatural	O
patterns	O
,	O
e.g.	O
that	O
every	O
word	O
containing	O
two	O
instances	O
of	O
a	O
but	O
less	O
than	O
three	O
bs	O
must	O
contain	O
no	O
substring	O
of	O
the	O
form	O
cd	O
+	O
c.	O
These	O
points	O
show	O
that	O
moving	O
from	O
TSL	O
to	O
star	O
-	O
free	O
means	O
losing	O
essential	O
properties	O
of	O
natural	O
language	O
morphotactics	O
.	O
Future	O
work	O
may	O
of	O
course	O
identify	O
more	O
adequate	O
classes	O
in	O
the	O
vicinity	O
of	O
TSL	O
.	O
Given	O
our	O
current	O
,	O
more	O
limited	O
knowledge	O
of	O
the	O
subregular	O
hierarchy	O
,	O
however	O
,	O
the	O
strongest	O
empirically	O
defensible	O
stance	O
is	O
that	O
tier	O
-	O
based	O
strict	O
locality	O
is	O
both	O
sufficient	O
and	O
necessary	O
for	O
natural	O
language	O
morphotactics	O
.	O

We	O
introduce	O
a	O
new	O
language	O
representation	O
model	O
called	O
BERT	B-MethodName
,	O
which	O
stands	O
for	O
Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
.	O
Unlike	O
recent	O
language	O
representation	O
models	O
(	O
Peters	O
et	O
al	O
,	O
2018a	O
;	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
,	O
BERT	B-MethodName
is	O
designed	O
to	O
pretrain	O
deep	O
bidirectional	O
representations	O
from	O
unlabeled	O
text	O
by	O
jointly	O
conditioning	O
on	O
both	O
left	O
and	O
right	O
context	O
in	O
all	O
layers	O
.	O
As	O
a	O
result	O
,	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
model	O
can	O
be	O
finetuned	O
with	O
just	O
one	O
additional	O
output	O
layer	O
to	O
create	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
for	O
a	O
wide	O
range	O
of	O
tasks	O
,	O
such	O
as	O
question	B-TaskName
answering	I-TaskName
and	O
language	O
inference	O
,	O
without	O
substantial	O
taskspecific	O
architecture	O
modifications	O
.	O
BERT	B-MethodName
is	O
conceptually	O
simple	O
and	O
empirically	O
powerful	O
.	O
It	O
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
eleven	O
natural	O
language	O
processing	O
tasks	O
,	O
including	O
pushing	O
the	O
GLUE	B-DatasetName
score	O
to	O
80.5	O
%	O
(	O
7.7	O
%	O
point	O
absolute	O
improvement	O
)	O
,	O
MultiNLI	B-DatasetName
accuracy	B-MetricName
to	O
86.7	O
%	O
(	O
4.6	O
%	O
absolute	O
improvement	O
)	O
,	O
SQuAD	B-DatasetName
v1.1	O
question	B-TaskName
answering	I-TaskName
Test	O
F1	B-MetricName
to	O
93.2	O
(	O
1.5	O
point	O
absolute	O
improvement	O
)	O
and	O
SQuAD	B-DatasetName
v2.0	O
Test	O
F1	B-MetricName
to	O
83.1	O
(	O
5.1	O
point	O
absolute	O
improvement	O
)	O
.	O

Language	O
model	O
pre	O
-	O
training	O
has	O
been	O
shown	O
to	O
be	O
effective	O
for	O
improving	O
many	O
natural	O
language	O
processing	O
tasks	O
(	O
Dai	O
and	O
Le	O
,	O
2015	O
;	O
Peters	O
et	O
al	O
,	O
2018a	O
;	O
Radford	O
et	O
al	O
,	O
2018	O
;	O
Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
.	O
These	O
include	O
sentence	O
-	O
level	O
tasks	O
such	O
as	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
Bowman	O
et	O
al	O
,	O
2015	O
;	O
Williams	O
et	O
al	O
,	O
2018	O
)	O
and	O
paraphrasing	O
(	O
Dolan	O
and	O
Brockett	O
,	O
2005	O
)	O
,	O
which	O
aim	O
to	O
predict	O
the	O
relationships	O
between	O
sentences	O
by	O
analyzing	O
them	O
holistically	O
,	O
as	O
well	O
as	O
token	O
-	O
level	O
tasks	O
such	O
as	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
and	O
question	B-TaskName
answering	I-TaskName
,	O
where	O
models	O
are	O
required	O
to	O
produce	O
fine	O
-	O
grained	O
output	O
at	O
the	O
token	O
level	O
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
;	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
.	O
There	O
are	O
two	O
existing	O
strategies	O
for	O
applying	O
pre	O
-	O
trained	O
language	O
representations	O
to	O
downstream	O
tasks	O
:	O
feature	O
-	O
based	O
and	O
fine	O
-	O
tuning	O
.	O
The	O
feature	O
-	O
based	O
approach	O
,	O
such	O
as	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018a	O
)	O
,	O
uses	O
task	O
-	O
specific	O
architectures	O
that	O
include	O
the	O
pre	O
-	O
trained	O
representations	O
as	O
additional	O
features	O
.	O
The	O
fine	O
-	O
tuning	O
approach	O
,	O
such	O
as	O
the	O
Generative	O
Pre	O
-	O
trained	O
Transformer	B-MethodName
(	O
OpenAI	O
GPT	B-MethodName
)	O
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
,	O
introduces	O
minimal	O
task	O
-	O
specific	O
parameters	O
,	O
and	O
is	O
trained	O
on	O
the	O
downstream	O
tasks	O
by	O
simply	O
fine	O
-	O
tuning	O
all	O
pretrained	O
parameters	O
.	O
The	O
two	O
approaches	O
share	O
the	O
same	O
objective	O
function	O
during	O
pre	O
-	O
training	O
,	O
where	O
they	O
use	O
unidirectional	O
language	O
models	O
to	O
learn	O
general	O
language	O
representations	O
.	O
We	O
argue	O
that	O
current	O
techniques	O
restrict	O
the	O
power	O
of	O
the	O
pre	O
-	O
trained	O
representations	O
,	O
especially	O
for	O
the	O
fine	O
-	O
tuning	O
approaches	O
.	O
The	O
major	O
limitation	O
is	O
that	O
standard	O
language	O
models	O
are	O
unidirectional	O
,	O
and	O
this	O
limits	O
the	O
choice	O
of	O
architectures	O
that	O
can	O
be	O
used	O
during	O
pre	O
-	O
training	O
.	O
For	O
example	O
,	O
in	O
OpenAI	O
GPT	B-MethodName
,	O
the	O
authors	O
use	O
a	O
left	O
-	O
toright	O
architecture	O
,	O
where	O
every	O
token	O
can	O
only	O
attend	O
to	O
previous	O
tokens	O
in	O
the	O
self	O
-	O
attention	B-HyperparameterName
layers	I-HyperparameterName
of	O
the	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Such	O
restrictions	O
are	O
sub	O
-	O
optimal	O
for	O
sentence	O
-	O
level	O
tasks	O
,	O
and	O
could	O
be	O
very	O
harmful	O
when	O
applying	O
finetuning	O
based	O
approaches	O
to	O
token	O
-	O
level	O
tasks	O
such	O
as	O
question	B-TaskName
answering	I-TaskName
,	O
where	O
it	O
is	O
crucial	O
to	O
incorporate	O
context	O
from	O
both	O
directions	O
.	O
In	O
this	O
paper	O
,	O
we	O
improve	O
the	O
fine	O
-	O
tuning	O
based	O
approaches	O
by	O
proposing	O
BERT	B-MethodName
:	O
Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
.	O
BERT	B-MethodName
alleviates	O
the	O
previously	O
mentioned	O
unidirectionality	O
constraint	O
by	O
using	O
a	O
"	O
masked	O
language	O
model	O
"	O
(	O
MLM	B-DatasetName
)	O
pre	O
-	O
training	O
objective	O
,	O
inspired	O
by	O
the	O
Cloze	O
task	O
(	O
Taylor	O
,	O
1953	O
)	O
.	O
The	O
masked	O
language	O
model	O
randomly	O
masks	O
some	O
of	O
the	O
tokens	O
from	O
the	O
input	O
,	O
and	O
the	O
objective	O
is	O
to	O
predict	O
the	O
original	O
vocabulary	O
i	O
d	O
of	O
the	O
masked	O
word	O
based	O
only	O
on	O
its	O
context	O
.	O
Unlike	O
left	O
-	O
toright	O
language	O
model	O
pre	O
-	O
training	O
,	O
the	O
MLM	B-DatasetName
objective	O
enables	O
the	O
representation	O
to	O
fuse	O
the	O
left	O
and	O
the	O
right	O
context	O
,	O
which	O
allows	O
us	O
to	O
pretrain	O
a	O
deep	O
bidirectional	O
Transformer	B-MethodName
.	O
In	O
addition	O
to	O
the	O
masked	O
language	O
model	O
,	O
we	O
also	O
use	O
a	O
"	O
next	O
sentence	O
prediction	O
"	O
task	O
that	O
jointly	O
pretrains	O
text	O
-	O
pair	O
representations	O
.	O
The	O
contributions	O
of	O
our	O
paper	O
are	O
as	O
follows	O
:	O
We	O
demonstrate	O
the	O
importance	O
of	O
bidirectional	O
pre	O
-	O
training	O
for	O
language	O
representations	O
.	O
Unlike	O
Radford	O
et	O
al	O
(	O
2018	O
)	O
,	O
which	O
uses	O
unidirectional	O
language	O
models	O
for	O
pre	O
-	O
training	O
,	O
BERT	B-MethodName
uses	O
masked	O
language	O
models	O
to	O
enable	O
pretrained	O
deep	O
bidirectional	O
representations	O
.	O
This	O
is	O
also	O
in	O
contrast	O
to	O
Peters	O
et	O
al	O
(	O
2018a	O
)	O
,	O
which	O
uses	O
a	O
shallow	O
concatenation	O
of	O
independently	O
trained	O
left	O
-	O
to	O
-	O
right	O
and	O
right	O
-	O
to	O
-	O
left	O
LMs	O
.	O
We	O
show	O
that	O
pre	O
-	O
trained	O
representations	O
reduce	O
the	O
need	O
for	O
many	O
heavily	O
-	O
engineered	O
taskspecific	O
architectures	O
.	O
BERT	B-MethodName
is	O
the	O
first	O
finetuning	O
based	O
representation	O
model	O
that	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
large	O
suite	O
of	O
sentence	O
-	O
level	O
and	O
token	O
-	O
level	O
tasks	O
,	O
outperforming	O
many	O
task	O
-	O
specific	O
architectures	O
.	O
BERT	B-MethodName
advances	O
the	O
state	O
of	O
the	O
art	O
for	O
eleven	O
NLP	O
tasks	O
.	O
The	O
code	O
and	O
pre	O
-	O
trained	O
models	O
are	O
available	O
at	O
https://github.com/	O
google	O
-	O
research	O
/	O
bert	O
.	O

We	O
introduce	O
BERT	B-MethodName
and	O
its	O
detailed	O
implementation	O
in	O
this	O
section	O
.	O
There	O
are	O
two	O
steps	O
in	O
our	O
framework	O
:	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
.	O
During	O
pre	O
-	O
training	O
,	O
the	O
model	O
is	O
trained	O
on	O
unlabeled	O
data	O
over	O
different	O
pre	O
-	O
training	O
tasks	O
.	O
For	O
finetuning	O
,	O
the	O
BERT	B-MethodName
model	O
is	O
first	O
initialized	O
with	O
the	O
pre	O
-	O
trained	O
parameters	O
,	O
and	O
all	O
of	O
the	O
parameters	O
are	O
fine	O
-	O
tuned	O
using	O
labeled	O
data	O
from	O
the	O
downstream	O
tasks	O
.	O
Each	O
downstream	O
task	O
has	O
separate	O
fine	O
-	O
tuned	O
models	O
,	O
even	O
though	O
they	O
are	O
initialized	O
with	O
the	O
same	O
pre	O
-	O
trained	O
parameters	O
.	O
The	O
question	O
-	O
answering	O
example	O
in	O
Figure	O
1	O
will	O
serve	O
as	O
a	O
running	O
example	O
for	O
this	O
section	O
.	O
A	O
distinctive	O
feature	O
of	O
BERT	B-MethodName
is	O
its	O
unified	O
architecture	O
across	O
different	O
tasks	O
.	O
There	O
is	O
mini	O
-	O
mal	O
difference	O
between	O
the	O
pre	O
-	O
trained	O
architecture	O
and	O
the	O
final	O
downstream	O
architecture	O
.	O
Model	O
Architecture	O
BERT	B-MethodName
's	O
model	O
architecture	O
is	O
a	O
multi	O
-	O
layer	O
bidirectional	O
Transformer	B-MethodName
encoder	O
based	O
on	O
the	O
original	O
implementation	O
described	O
in	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
and	O
released	O
in	O
the	O
tensor2tensor	O
library	O
.	O
1	O
Because	O
the	O
use	O
of	O
Transformers	O
has	O
become	O
common	O
and	O
our	O
implementation	O
is	O
almost	O
identical	O
to	O
the	O
original	O
,	O
we	O
will	O
omit	O
an	O
exhaustive	O
background	O
description	O
of	O
the	O
model	O
architecture	O
and	O
refer	O
readers	O
to	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
as	O
well	O
as	O
excellent	O
guides	O
such	O
as	O
"	O
The	O
Annotated	O
Transformer	B-MethodName
.	O
"	O
2	O
In	O
this	O
work	O
,	O
we	O
denote	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
(	O
i.e.	O
,	O
Transformer	B-MethodName
blocks	O
)	O
as	O
L	O
,	O
the	O
hidden	O
size	O
as	O
H	O
,	O
and	O
the	O
number	O
of	O
self	O
-	O
attention	O
heads	O
as	O
A.	O
3	O
We	O
primarily	O
report	O
results	O
on	O
two	O
model	O
sizes	O
:	O
BERT	B-MethodName
BASE	B-MethodName
(	O
L=12	O
,	O
H=768	O
,	O
A=12	O
,	O
Total	O
Param	O
-	O
eters=110	O
M	O
)	O
and	O
BERT	B-MethodName
LARGE	O
(	O
L=24	O
,	O
H=1024	O
,	O
A=16	O
,	O
Total	O
Parameters=340	O
M	O
)	O
.	O
BERT	B-MethodName
BASE	B-MethodName
was	O
chosen	O
to	O
have	O
the	O
same	O
model	O
size	O
as	O
OpenAI	O
GPT	B-MethodName
for	O
comparison	O
purposes	O
.	O
Critically	O
,	O
however	O
,	O
the	O
BERT	B-MethodName
Transformer	B-MethodName
uses	O
bidirectional	O
self	O
-	O
attention	O
,	O
while	O
the	O
GPT	B-MethodName
Transformer	B-MethodName
uses	O
constrained	O
self	O
-	O
attention	O
where	O
every	O
token	O
can	O
only	O
attend	O
to	O
context	O
to	O
its	O
left	O
.	O
4	O
Input	O
/	O
Output	O
Representations	O
To	O
make	O
BERT	B-MethodName
handle	O
a	O
variety	O
of	O
down	O
-	O
stream	O
tasks	O
,	O
our	O
input	O
representation	O
is	O
able	O
to	O
unambiguously	O
represent	O
both	O
a	O
single	O
sentence	O
and	O
a	O
pair	O
of	O
sentences	O
(	O
e.g.	O
,	O
Question	O
,	O
Answer	O
)	O
in	O
one	O
token	O
sequence	O
.	O
Throughout	O
this	O
work	O
,	O
a	O
"	O
sentence	O
"	O
can	O
be	O
an	O
arbitrary	O
span	O
of	O
contiguous	O
text	O
,	O
rather	O
than	O
an	O
actual	O
linguistic	O
sentence	O
.	O
A	O
"	O
sequence	O
"	O
refers	O
to	O
the	O
input	O
token	O
sequence	O
to	O
BERT	B-MethodName
,	O
which	O
may	O
be	O
a	O
single	O
sentence	O
or	O
two	O
sentences	O
packed	O
together	O
.	O
We	O
use	O
WordPiece	B-MethodName
embeddings	O
(	O
Wu	O
et	O
al	O
,	O
2016	O
)	O
with	O
a	O
30	O
,	O
000	O
token	O
vocabulary	O
.	O
The	O
first	O
token	O
of	O
every	O
sequence	O
is	O
always	O
a	O
special	O
classification	O
token	O
(	O
[	O
CLS	O
]	O
)	O
.	O
The	O
final	O
hidden	O
state	O
corresponding	O
to	O
this	O
token	O
is	O
used	O
as	O
the	O
aggregate	O
sequence	O
representation	O
for	O
classification	O
tasks	O
.	O
Sentence	O
pairs	O
are	O
packed	O
together	O
into	O
a	O
single	O
sequence	O
.	O
We	O
differentiate	O
the	O
sentences	O
in	O
two	O
ways	O
.	O
First	O
,	O
we	O
separate	O
them	O
with	O
a	O
special	O
token	O
(	O
[	O
SEP	O
]	O
)	O
.	O
Second	O
,	O
we	O
add	O
a	O
learned	O
embedding	O
to	O
every	O
token	O
indicating	O
whether	O
it	O
belongs	O
to	O
sentence	O
A	O
or	O
sentence	O
B.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
we	O
denote	O
input	O
embedding	O
as	O
E	O
,	O
the	O
final	O
hidden	O
vector	O
of	O
the	O
special	O
[	O
CLS	O
]	O
token	O
as	O
C	O
R	O
H	O
,	O
and	O
the	O
final	O
hidden	O
vector	O
for	O
the	O
i	O
th	O
input	O
token	O
as	O
T	O
i	O
R	O
H	O
.	O
For	O
a	O
given	O
token	O
,	O
its	O
input	O
representation	O
is	O
constructed	O
by	O
summing	O
the	O
corresponding	O
token	O
,	O
segment	O
,	O
and	O
position	O
embeddings	O
.	O
A	O
visualization	O
of	O
this	O
construction	O
can	O
be	O
seen	O
in	O
Figure	O
2	O
.	O

Unlike	O
Peters	O
et	O
al	O
(	O
2018a	O
)	O
and	O
Radford	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
do	O
not	O
use	O
traditional	O
left	O
-	O
to	O
-	O
right	O
or	O
right	O
-	O
to	O
-	O
left	O
language	O
models	O
to	O
pre	O
-	O
train	O
BERT	B-MethodName
.	O
Instead	O
,	O
we	O
pre	O
-	O
train	O
BERT	B-MethodName
using	O
two	O
unsupervised	O
tasks	O
,	O
described	O
in	O
this	O
section	O
.	O
This	O
step	O
is	O
presented	O
in	O
the	O
left	O
part	O
of	O
Figure	O
1	O
.	O
Task	O
#	O
1	O
:	O
Masked	O
LM	O
Intuitively	O
,	O
it	O
is	O
reasonable	O
to	O
believe	O
that	O
a	O
deep	O
bidirectional	O
model	O
is	O
strictly	O
more	O
powerful	O
than	O
either	O
a	O
left	O
-	O
to	O
-	O
right	O
model	O
or	O
the	O
shallow	O
concatenation	O
of	O
a	O
left	O
-	O
toright	O
and	O
a	O
right	O
-	O
to	O
-	O
left	O
model	O
.	O
Unfortunately	O
,	O
standard	O
conditional	O
language	O
models	O
can	O
only	O
be	O
trained	O
left	O
-	O
to	O
-	O
right	O
or	O
right	O
-	O
to	O
-	O
left	O
,	O
since	O
bidirectional	O
conditioning	O
would	O
allow	O
each	O
word	O
to	O
indirectly	O
"	O
see	O
itself	O
"	O
,	O
and	O
the	O
model	O
could	O
trivially	O
predict	O
the	O
target	O
word	O
in	O
a	O
multi	O
-	O
layered	O
context	O
.	O
former	O
is	O
often	O
referred	O
to	O
as	O
a	O
"	O
Transformer	B-MethodName
encoder	O
"	O
while	O
the	O
left	O
-	O
context	O
-	O
only	O
version	O
is	O
referred	O
to	O
as	O
a	O
"	O
Transformer	B-MethodName
decoder	I-MethodName
"	O
since	O
it	O
can	O
be	O
used	O
for	O
text	B-TaskName
generation	I-TaskName
.	O
In	O
order	O
to	O
train	O
a	O
deep	O
bidirectional	O
representation	O
,	O
we	O
simply	O
mask	O
some	O
percentage	O
of	O
the	O
input	O
tokens	O
at	O
random	O
,	O
and	O
then	O
predict	O
those	O
masked	O
tokens	O
.	O
We	O
refer	O
to	O
this	O
procedure	O
as	O
a	O
"	O
masked	O
LM	O
"	O
(	O
MLM	B-DatasetName
)	O
,	O
although	O
it	O
is	O
often	O
referred	O
to	O
as	O
a	O
Cloze	O
task	O
in	O
the	O
literature	O
(	O
Taylor	O
,	O
1953	O
)	O
.	O
In	O
this	O
case	O
,	O
the	O
final	O
hidden	O
vectors	O
corresponding	O
to	O
the	O
mask	O
tokens	O
are	O
fed	O
into	O
an	O
output	O
softmax	B-MethodName
over	O
the	O
vocabulary	O
,	O
as	O
in	O
a	O
standard	O
LM	O
.	O
In	O
all	O
of	O
our	O
experiments	O
,	O
we	O
mask	O
15	O
%	O
of	O
all	O
WordPiece	B-MethodName
tokens	O
in	O
each	O
sequence	O
at	O
random	O
.	O
In	O
contrast	O
to	O
denoising	B-TaskName
auto	O
-	O
encoders	O
(	O
Vincent	O
et	O
al	O
,	O
2008	O
)	O
,	O
we	O
only	O
predict	O
the	O
masked	O
words	O
rather	O
than	O
reconstructing	O
the	O
entire	O
input	O
.	O
Although	O
this	O
allows	O
us	O
to	O
obtain	O
a	O
bidirectional	O
pre	O
-	O
trained	O
model	O
,	O
a	O
downside	O
is	O
that	O
we	O
are	O
creating	O
a	O
mismatch	O
between	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
,	O
since	O
the	O
[	O
MASK	O
]	O
token	O
does	O
not	O
appear	O
during	O
fine	O
-	O
tuning	O
.	O
To	O
mitigate	O
this	O
,	O
we	O
do	O
not	O
always	O
replace	O
"	O
masked	O
"	O
words	O
with	O
the	O
actual	O
[	O
MASK	O
]	O
token	O
.	O
The	O
training	O
data	O
generator	O
chooses	O
15	O
%	O
of	O
the	O
token	O
positions	O
at	O
random	O
for	O
prediction	O
.	O
If	O
the	O
i	O
-	O
th	O
token	O
is	O
chosen	O
,	O
we	O
replace	O
the	O
i	O
-	O
th	O
token	O
with	O
(	O
1	O
)	O
the	O
[	O
MASK	O
]	O
token	O
80	O
%	O
of	O
the	O
time	O
(	O
2	O
)	O
a	O
random	O
token	O
10	O
%	O
of	O
the	O
time	O
(	O
3	O
)	O
the	O
unchanged	O
i	O
-	O
th	O
token	O
10	O
%	O
of	O
the	O
time	O
.	O
Then	O
,	O
T	O
i	O
will	O
be	O
used	O
to	O
predict	O
the	O
original	O
token	O
with	O
cross	O
entropy	O
loss	B-MetricName
.	O
We	O
compare	O
variations	O
of	O
this	O
procedure	O
in	O
Appendix	O
C.2	O
.	O
Task	O
#	O
2	O
:	O
Next	O
Sentence	O
Prediction	O
(	O
NSP	O
)	O
Many	O
important	O
downstream	O
tasks	O
such	O
as	O
Question	B-TaskName
Answering	I-TaskName
(	O
QA	O
)	O
and	O
Natural	B-TaskName
Language	I-TaskName
Inference	I-TaskName
(	O
NLI	O
)	O
are	O
based	O
on	O
understanding	O
the	O
relationship	O
between	O
two	O
sentences	O
,	O
which	O
is	O
not	O
directly	O
captured	O
by	O
language	O
modeling	O
.	O
In	O
order	O
to	O
train	O
a	O
model	O
that	O
understands	O
sentence	O
relationships	O
,	O
we	O
pre	O
-	O
train	O
for	O
a	O
binarized	O
next	O
sentence	O
prediction	O
task	O
that	O
can	O
be	O
trivially	O
generated	O
from	O
any	O
monolingual	O
corpus	O
.	O
Specifically	O
,	O
when	O
choosing	O
the	O
sentences	O
A	O
and	O
B	O
for	O
each	O
pretraining	O
example	O
,	O
50	O
%	O
of	O
the	O
time	O
B	O
is	O
the	O
actual	O
next	O
sentence	O
that	O
follows	O
A	O
(	O
labeled	O
as	O
IsNext	O
)	O
,	O
and	O
50	O
%	O
of	O
the	O
time	O
it	O
is	O
a	O
random	O
sentence	O
from	O
the	O
corpus	O
(	O
labeled	O
as	O
NotNext	O
)	O
.	O
As	O
we	O
show	O
in	O
Figure	O
1	O
,	O
C	O
is	O
used	O
for	O
next	O
sentence	O
prediction	O
(	O
NSP	O
)	O
.	O
5	O
Despite	O
its	O
simplicity	O
,	O
we	O
demonstrate	O
in	O
Section	O
5.1	O
that	O
pre	O
-	O
training	O
towards	O
this	O
task	O
is	O
very	O
beneficial	O
to	O
both	O
QA	O
and	O
NLI	O
.	O
6	O
The	O
NSP	O
task	O
is	O
closely	O
related	O
to	O
representationlearning	O
objectives	O
used	O
in	O
Jernite	O
et	O
al	O
(	O
2017	O
)	O
and	O
Logeswaran	O
and	O
Lee	O
(	O
2018	O
)	O
.	O
However	O
,	O
in	O
prior	O
work	O
,	O
only	O
sentence	B-TaskName
embeddings	I-TaskName
are	O
transferred	O
to	O
down	O
-	O
stream	O
tasks	O
,	O
where	O
BERT	B-MethodName
transfers	O
all	O
parameters	O
to	O
initialize	O
end	O
-	O
task	O
model	O
parameters	O
.	O

The	O
General	B-DatasetName
Language	O
Understanding	O
Evaluation	O
(	O
GLUE	B-DatasetName
)	O
benchmark	O
(	O
Wang	O
et	O
al	O
,	O
2018a	O
)	O
is	O
a	O
collection	O
of	O
diverse	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
tasks	O
.	O
Detailed	O
descriptions	O
of	O
GLUE	B-DatasetName
datasets	O
are	O
included	O
in	O
Appendix	O
B.1	O
.	O
To	O
fine	O
-	O
tune	O
on	O
GLUE	B-DatasetName
,	O
we	O
represent	O
the	O
input	O
sequence	O
(	O
for	O
single	O
sentence	O
or	O
sentence	O
pairs	O
)	O
as	O
described	O
in	O
Section	O
3	O
,	O
and	O
use	O
the	O
final	O
hidden	O
vector	O
C	O
R	O
H	O
corresponding	O
to	O
the	O
first	O
input	O
token	O
(	O
[	O
CLS	O
]	O
)	O
as	O
the	O
aggregate	O
representation	O
.	O
The	O
only	O
new	O
parameters	O
introduced	O
during	O
fine	O
-	O
tuning	O
are	O
classification	O
layer	O
weights	O
W	O
R	O
K×H	O
,	O
where	O
K	O
is	O
the	O
number	O
of	O
labels	O
.	O
We	O
compute	O
a	O
standard	O
classification	O
loss	B-MetricName
with	O
C	O
and	O
W	O
,	O
i.e.	O
,	O
log	O
(	O
softmax	B-MethodName
(	O
CW	O
T	O
)	O
)	O
.	O
We	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	O
and	O
fine	O
-	O
tune	O
for	O
3	O
epochs	O
over	O
the	O
data	O
for	O
all	O
GLUE	B-DatasetName
tasks	O
.	O
For	O
each	O
task	O
,	O
we	O
selected	O
the	O
best	O
fine	O
-	O
tuning	O
learning	B-HyperparameterName
rate	I-HyperparameterName
(	O
among	O
5e	O
-	O
5	O
,	O
4e	O
-	O
5	O
,	O
3e	O
-	O
5	O
,	O
and	O
2e	O
-	O
5	O
)	O
on	O
the	O
Dev	O
set	O
.	O
Additionally	O
,	O
for	O
BERT	B-MethodName
LARGE	O
we	O
found	O
that	O
finetuning	O
was	O
sometimes	O
unstable	O
on	O
small	O
datasets	O
,	O
so	O
we	O
ran	O
several	O
random	O
restarts	O
and	O
selected	O
the	O
best	O
model	O
on	O
the	O
Dev	O
set	O
.	O
With	O
random	O
restarts	O
,	O
we	O
use	O
the	O
same	O
pre	O
-	O
trained	O
checkpoint	O
but	O
perform	O
different	O
fine	O
-	O
tuning	O
data	O
shuffling	O
and	O
classifier	O
layer	O
initialization	O
.	O
9	O
Results	O
are	O
presented	O
in	O
Table	O
1	O
.	O
Both	O
BERT	B-MethodName
BASE	B-MethodName
and	O
BERT	B-MethodName
LARGE	O
outperform	O
all	O
systems	O
on	O
all	O
tasks	O
by	O
a	O
substantial	O
margin	O
,	O
obtaining	O
4.5	O
%	O
and	O
7.0	O
%	O
respective	O
average	B-MetricName
accuracy	I-MetricName
improvement	O
over	O
the	O
prior	O
state	O
of	O
the	O
art	O
.	O
Note	O
that	O
BERT	B-MethodName
BASE	B-MethodName
and	O
OpenAI	O
GPT	B-MethodName
are	O
nearly	O
identical	O
in	O
terms	O
of	O
model	O
architecture	O
apart	O
from	O
the	O
attention	O
masking	O
.	O
For	O
the	O
largest	O
and	O
most	O
widely	O
reported	O
GLUE	B-DatasetName
task	O
,	O
MNLI	B-DatasetName
,	O
BERT	B-MethodName
obtains	O
a	O
4.6	O
%	O
absolute	O
accuracy	B-MetricName
improvement	O
.	O
On	O
the	O
official	O
GLUE	B-DatasetName
leaderboard	O
10	O
,	O
BERT	B-MethodName
LARGE	O
obtains	O
a	O
score	O
of	O
80.5	O
,	O
compared	O
to	O
OpenAI	O
GPT	B-MethodName
,	O
which	O
obtains	O
72.8	O
as	O
of	O
the	O
date	O
of	O
writing	O
.	O
We	O
find	O
that	O
BERT	B-MethodName
LARGE	O
significantly	O
outperforms	O
BERT	B-MethodName
BASE	B-MethodName
across	O
all	O
tasks	O
,	O
especially	O
those	O
with	O
very	O
little	O
training	O
data	O
.	O
The	O
effect	O
of	O
model	O
size	O
is	O
explored	O
more	O
thoroughly	O
in	O
Section	O
5.2	O
.	O

The	B-DatasetName
Stanford	I-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
SQuAD	B-DatasetName
v1.1	O
)	O
is	O
a	O
collection	O
of	O
100k	O
crowdsourced	O
question	O
/	O
answer	O
pairs	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
.	O
Given	O
a	O
question	O
and	O
a	O
passage	O
from	O
9	O
The	O
GLUE	B-DatasetName
data	O
set	O
distribution	O
does	O
not	O
include	O
the	O
Test	O
labels	O
,	O
and	O
we	O
only	O
made	O
a	O
single	O
GLUE	B-DatasetName
evaluation	O
server	O
submission	O
for	O
each	O
of	O
BERTBASE	O
and	O
BERTLARGE	O
.	O
10	O
https://gluebenchmark.com/leaderboard	O
Wikipedia	O
containing	O
the	O
answer	O
,	O
the	O
task	O
is	O
to	O
predict	O
the	O
answer	O
text	O
span	O
in	O
the	O
passage	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
in	O
the	O
question	B-TaskName
answering	I-TaskName
task	O
,	O
we	O
represent	O
the	O
input	O
question	O
and	O
passage	O
as	O
a	O
single	O
packed	O
sequence	O
,	O
with	O
the	O
question	O
using	O
the	O
A	O
embedding	O
and	O
the	O
passage	O
using	O
the	O
B	O
embedding	O
.	O
We	O
only	O
introduce	O
a	O
start	O
vector	O
S	O
R	O
H	O
and	O
an	O
end	O
vector	O
E	O
R	O
H	O
during	O
fine	O
-	O
tuning	O
.	O
The	O
probability	O
of	O
word	O
i	O
being	O
the	O
start	O
of	O
the	O
answer	O
span	O
is	O
computed	O
as	O
a	O
dot	O
product	O
between	O
T	O
i	O
and	O
S	O
followed	O
by	O
a	O
softmax	B-MethodName
over	O
all	O
of	O
the	O
words	O
in	O
the	O
paragraph	O
:	O
P	O
i	O
=	O
e	O
S	O
T	O
i	O
j	O
e	O
S	O
T	O
j	O
.	O
The	O
analogous	O
formula	O
is	O
used	O
for	O
the	O
end	O
of	O
the	O
answer	O
span	O
.	O
The	O
score	O
of	O
a	O
candidate	O
span	O
from	O
position	O
i	O
to	O
position	O
j	O
is	O
defined	O
as	O
S	O
T	O
i	O
+	O
E	O
T	O
j	O
,	O
and	O
the	O
maximum	O
scoring	O
span	O
where	O
j	O
≥	O
i	O
is	O
used	O
as	O
a	O
prediction	O
.	O
The	O
training	O
objective	O
is	O
the	O
sum	O
of	O
the	O
log	O
-	O
likelihoods	O
of	O
the	O
correct	O
start	O
and	O
end	O
positions	O
.	O
We	O
fine	O
-	O
tune	O
for	O
3	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e	O
-	O
5	O
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	O
.	O
Table	O
2	O
shows	O
top	O
leaderboard	O
entries	O
as	O
well	O
as	O
results	O
from	O
top	O
published	O
systems	O
(	O
Seo	O
et	O
al	O
,	O
2017	O
;	O
Clark	O
and	O
Gardner	O
,	O
2018	O
;	O
Peters	O
et	O
al	O
,	O
2018a	O
;	O
Hu	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
top	O
results	O
from	O
the	O
SQuAD	B-DatasetName
leaderboard	O
do	O
not	O
have	O
up	O
-	O
to	O
-	O
date	O
public	O
system	O
descriptions	O
available	O
,	O
11	O
and	O
are	O
allowed	O
to	O
use	O
any	O
public	O
data	O
when	O
training	O
their	O
systems	O
.	O
We	O
therefore	O
use	O
modest	O
data	B-TaskName
augmentation	I-TaskName
in	O
our	O
system	O
by	O
first	O
fine	O
-	O
tuning	O
on	O
TriviaQA	B-DatasetName
(	O
Joshi	O
et	O
al	O
,	O
2017	O
)	O
befor	O
fine	O
-	O
tuning	O
on	O
SQuAD	B-DatasetName
.	O
Our	O
best	O
performing	O
system	O
outperforms	O
the	O
top	O
leaderboard	O
system	O
by	O
+1.5	O
F1	B-MetricName
in	O
ensembling	O
and	O
+1.3	O
F1	B-MetricName
as	O
a	O
single	O
system	O
.	O
In	O
fact	O
,	O
our	O
single	O
BERT	B-MethodName
model	O
outperforms	O
the	O
top	O
ensemble	O
system	O
in	O
terms	O
of	O
F1	B-MetricName
score	I-MetricName
.	O
Without	O
TriviaQA	B-DatasetName
fine	O
-	O
tuning	O
data	O
,	O
we	O
only	O
lose	O
0.1	O
-	O
0.4	O
F1	B-MetricName
,	O
still	O
outperforming	O
all	O
existing	O
systems	O
by	O
a	O
wide	O
margin	O
.	O
12	O

The	O
SQuAD	B-DatasetName
2.0	O
task	O
extends	O
the	O
SQuAD	B-DatasetName
1.1	O
problem	O
definition	O
by	O
allowing	O
for	O
the	O
possibility	O
that	O
no	O
short	O
answer	O
exists	O
in	O
the	O
provided	O
paragraph	O
,	O
making	O
the	O
problem	O
more	O
realistic	O
.	O
We	O
use	O
a	O
simple	O
approach	O
to	O
extend	O
the	O
SQuAD	B-DatasetName
v1.1	O
BERT	B-MethodName
model	O
for	O
this	O
task	O
.	O
We	O
treat	O
questions	O
that	O
do	O
not	O
have	O
an	O
answer	O
as	O
having	O
an	O
answer	O
span	O
with	O
start	O
and	O
end	O
at	O
the	O
[	O
CLS	O
]	O
token	O
.	O
The	O
probability	O
space	O
for	O
the	O
start	O
and	O
end	O
answer	O
span	O
positions	O
is	O
extended	O
to	O
include	O
the	O
position	O
of	O
the	O
[	O
CLS	O
]	O
token	O
.	O
For	O
prediction	O
,	O
we	O
compare	O
the	O
score	O
of	O
the	O
no	O
-	O
answer	O
span	O
:	O
s	O
null	O
=	O
S	O
C	O
+	O
E	O
C	O
to	O
the	O
score	O
of	O
the	O
best	O
non	O
-	O
null	O
span	O
12	O
The	O
TriviaQA	B-DatasetName
data	O
we	O
used	O
consists	O
of	O
paragraphs	O
from	O
TriviaQA	B-DatasetName
-	O
Wiki	O
formed	O
of	O
the	O
first	O
400	O
tokens	O
in	O
documents	O
,	O
that	O
contain	O
at	O
least	O
one	O
of	O
the	O
provided	O
possible	O
answers	O
.	O
s	O
i	O
,	O
j	O
=	O
max	O
j≥i	O
S	O
T	O
i	O
+	O
E	O
T	O
j	O
.	O
We	O
predict	O
a	O
non	O
-	O
null	O
answer	O
whenŝ	O
i	O
,	O
j	O
>	O
s	O
null	O
+	O
τ	O
,	O
where	O
the	O
threshold	O
τ	O
is	O
selected	O
on	O
the	O
dev	O
set	O
to	O
maximize	O
F1	B-MetricName
.	O
We	O
did	O
not	O
use	O
TriviaQA	B-DatasetName
data	O
for	O
this	O
model	O
.	O
We	O
fine	O
-	O
tuned	O
for	O
2	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e	O
-	O
5	O
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
48	O
.	O
The	O
results	O
compared	O
to	O
prior	O
leaderboard	O
entries	O
and	O
top	O
published	O
work	O
(	O
Sun	O
et	O
al	O
,	O
2018	O
;	O
Wang	O
et	O
al	O
,	O
2018b	O
)	O
are	O
shown	O
in	O
Table	O
3	O
,	O
excluding	O
systems	O
that	O
use	O
BERT	B-MethodName
as	O
one	O
of	O
their	O
components	O
.	O
We	O
observe	O
a	O
+5.1	O
F1	B-MetricName
improvement	O
over	O
the	O
previous	O
best	O
system	O
.	O

The	O
Situations	O
With	O
Adversarial	O
Generations	O
(	O
SWAG	B-DatasetName
)	O
dataset	O
contains	O
113k	O
sentence	O
-	O
pair	O
completion	O
examples	O
that	O
evaluate	O
grounded	O
commonsense	O
inference	O
(	O
Zellers	O
et	O
al	O
,	O
2018	O
)	O
.	O
Given	O
a	O
sentence	O
,	O
the	O
task	O
is	O
to	O
choose	O
the	O
most	O
plausible	O
continuation	O
among	O
four	O
choices	O
.	O
When	O
fine	O
-	O
tuning	O
on	O
the	O
SWAG	B-DatasetName
dataset	O
,	O
we	O
construct	O
four	O
input	O
sequences	O
,	O
each	O
containing	O
the	O
concatenation	O
of	O
the	O
given	O
sentence	O
(	O
sentence	O
A	O
)	O
and	O
a	O
possible	O
continuation	O
(	O
sentence	O
B	O
)	O
.	O
The	O
only	O
task	O
-	O
specific	O
parameters	O
introduced	O
is	O
a	O
vector	O
whose	O
dot	O
product	O
with	O
the	O
[	O
CLS	O
]	O
token	O
representation	O
C	O
denotes	O
a	O
score	O
for	O
each	O
choice	O
which	O
is	O
normalized	O
with	O
a	O
softmax	B-MethodName
layer	O
.	O
We	O
fine	O
-	O
tune	O
the	O
model	O
for	O
3	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e	O
-	O
5	O
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	O
.	O
Results	O
are	O
presented	O
in	O
Table	O
4	O
.	O
BERT	B-MethodName
LARGE	O
outperforms	O
the	O
authors	O
'	O
baseline	O
ESIM+ELMo	O
system	O
by	O
+27.1	O
%	O
and	O
OpenAI	O
GPT	B-MethodName
by	O
8.3	O
%	O
.	O

In	O
this	O
section	O
,	O
we	O
explore	O
the	O
effect	O
of	O
model	O
size	O
on	O
fine	O
-	O
tuning	O
task	O
accuracy	B-MetricName
.	O
We	O
trained	O
a	O
number	O
of	O
BERT	B-MethodName
models	O
with	O
a	O
differing	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
,	O
hidden	O
units	O
,	O
and	O
attention	O
heads	O
,	O
while	O
otherwise	O
using	O
the	O
same	O
hyperparameters	O
and	O
training	O
procedure	O
as	O
described	O
previously	O
.	O
Results	O
on	O
selected	O
GLUE	B-DatasetName
tasks	O
are	O
shown	O
in	O
Table	O
6	O
.	O
In	O
this	O
table	O
,	O
we	O
report	O
the	O
average	O
Dev	O
Set	O
accuracy	B-MetricName
from	O
5	O
random	O
restarts	O
of	O
fine	O
-	O
tuning	O
.	O
We	O
can	O
see	O
that	O
larger	O
models	O
lead	O
to	O
a	O
strict	O
accuracy	B-MetricName
improvement	O
across	O
all	O
four	O
datasets	O
,	O
even	O
for	O
MRPC	B-DatasetName
which	O
only	O
has	O
3	O
,	O
600	O
labeled	O
training	O
examples	O
,	O
and	O
is	O
substantially	O
different	O
from	O
the	O
pre	O
-	O
training	O
tasks	O
.	O
It	O
is	O
also	O
perhaps	O
surprising	O
that	O
we	O
are	O
able	O
to	O
achieve	O
such	O
significant	O
improvements	O
on	O
top	O
of	O
models	O
which	O
are	O
already	O
quite	O
large	O
relative	O
to	O
the	O
existing	O
literature	O
.	O
For	O
example	O
,	O
the	O
largest	O
Transformer	B-MethodName
explored	O
in	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
is	O
(	O
L=6	O
,	O
H=1024	O
,	O
A=16	O
)	O
with	O
100	O
M	O
parameters	O
for	O
the	O
encoder	O
,	O
and	O
the	O
largest	O
Transformer	B-MethodName
we	O
have	O
found	O
in	O
the	O
literature	O
is	O
(	O
L=64	O
,	O
H=512	O
,	O
A=2	O
)	O
with	O
235	O
M	O
parameters	O
(	O
Al	O
-	O
Rfou	O
et	O
al	O
,	O
2018	O
)	O
.	O
By	O
contrast	O
,	O
BERT	B-MethodName
BASE	B-MethodName
contains	O
110	O
M	O
parameters	O
and	O
BERT	B-MethodName
LARGE	O
contains	O
340	O
M	O
parameters	O
.	O
It	O
has	O
long	O
been	O
known	O
that	O
increasing	O
the	O
model	O
size	O
will	O
lead	O
to	O
continual	O
improvements	O
on	O
large	O
-	O
scale	O
tasks	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
and	O
language	O
modeling	O
,	O
which	O
is	O
demonstrated	O
by	O
the	O
LM	O
perplexity	B-MetricName
of	O
held	O
-	O
out	O
training	O
data	O
shown	O
in	O
Table	O
6	O
.	O
However	O
,	O
we	O
believe	O
that	O
this	O
is	O
the	O
first	O
work	O
to	O
demonstrate	O
convincingly	O
that	O
scaling	O
to	O
extreme	O
model	O
sizes	O
also	O
leads	O
to	O
large	O
improvements	O
on	O
very	O
small	O
scale	O
tasks	O
,	O
provided	O
that	O
the	O
model	O
has	O
been	O
sufficiently	O
pre	O
-	O
trained	O
.	O
Peters	O
et	O
al	O
(	O
2018b	O
)	O
presented	O
mixed	O
results	O
on	O
the	O
downstream	O
task	O
impact	O
of	O
increasing	O
the	O
pre	O
-	O
trained	O
bi	O
-	O
LM	O
size	O
from	O
two	O
to	O
four	O
layers	O
and	O
Melamud	O
et	O
al	O
(	O
2016	O
)	O
mentioned	O
in	O
passing	O
that	O
increasing	O
hidden	B-HyperparameterName
dimension	I-HyperparameterName
size	I-HyperparameterName
from	O
200	O
to	O
600	O
helped	O
,	O
but	O
increasing	O
further	O
to	O
1	O
,	O
000	O
did	O
not	O
bring	O
further	O
improvements	O
.	O
Both	O
of	O
these	O
prior	O
works	O
used	O
a	O
featurebased	O
approach	O
-	O
we	O
hypothesize	O
that	O
when	O
the	O
model	O
is	O
fine	O
-	O
tuned	O
directly	O
on	O
the	O
downstream	O
tasks	O
and	O
uses	O
only	O
a	O
very	O
small	O
number	O
of	O
randomly	O
initialized	O
additional	O
parameters	O
,	O
the	O
taskspecific	O
models	O
can	O
benefit	O
from	O
the	O
larger	O
,	O
more	O
expressive	O
pre	O
-	O
trained	O
representations	O
even	O
when	O
downstream	O
task	O
data	O
is	O
very	O
small	O
.	O

All	O
of	O
the	O
BERT	B-MethodName
results	O
presented	O
so	O
far	O
have	O
used	O
the	O
fine	O
-	O
tuning	O
approach	O
,	O
where	O
a	O
simple	O
classification	O
layer	O
is	O
added	O
to	O
the	O
pre	O
-	O
trained	O
model	O
,	O
and	O
all	O
parameters	O
are	O
jointly	O
fine	O
-	O
tuned	O
on	O
a	O
downstream	O
task	O
.	O
However	O
,	O
the	O
feature	O
-	O
based	O
approach	O
,	O
where	O
fixed	O
features	O
are	O
extracted	O
from	O
the	O
pretrained	O
model	O
,	O
has	O
certain	O
advantages	O
.	O
First	O
,	O
not	O
all	O
tasks	O
can	O
be	O
easily	O
represented	O
by	O
a	O
Transformer	B-MethodName
encoder	O
architecture	O
,	O
and	O
therefore	O
require	O
a	O
task	O
-	O
specific	O
model	O
architecture	O
to	O
be	O
added	O
.	O
Second	O
,	O
there	O
are	O
major	O
computational	O
benefits	O
to	O
pre	O
-	O
compute	O
an	O
expensive	O
representation	O
of	O
the	O
training	O
data	O
once	O
and	O
then	O
run	O
many	O
experiments	O
with	O
cheaper	O
models	O
on	O
top	O
of	O
this	O
representation	O
.	O
In	O
this	O
section	O
,	O
we	O
compare	O
the	O
two	O
approaches	O
by	O
applying	O
BERT	B-MethodName
to	O
the	O
CoNLL	O
-	O
2003	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
task	O
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
.	O
In	O
the	O
input	O
to	O
BERT	B-MethodName
,	O
we	O
use	O
a	O
case	O
-	O
preserving	O
WordPiece	B-MethodName
model	O
,	O
and	O
we	O
include	O
the	O
maximal	O
document	O
context	O
provided	O
by	O
the	O
data	O
.	O
Following	O
standard	O
practice	O
,	O
we	O
formulate	O
this	O
as	O
a	O
tagging	O
task	O
but	O
do	O
not	O
use	O
a	O
CRF	B-MethodName
layer	O
in	O
the	O
output	O
.	O
We	O
use	O
the	O
representation	O
of	O
the	O
first	O
sub	O
-	O
token	O
as	O
the	O
input	O
to	O
the	O
token	O
-	O
level	O
classifier	O
over	O
the	O
NER	B-TaskName
label	O
set	O
.	O
To	O
ablate	O
the	O
fine	O
-	O
tuning	O
approach	O
,	O
we	O
apply	O
the	O
feature	O
-	O
based	O
approach	O
by	O
extracting	O
the	O
activations	O
from	O
one	O
or	O
more	O
layers	O
without	O
fine	O
-	O
tuning	O
any	O
parameters	O
of	O
BERT	B-MethodName
.	O
These	O
contextual	O
embeddings	O
are	O
used	O
as	O
input	O
to	O
a	O
randomly	O
initialized	O
two	O
-	O
layer	O
768	O
-	O
dimensional	O
BiLSTM	B-MethodName
before	O
the	O
classification	O
layer	O
.	O
Results	O
are	O
presented	O
in	O
Table	O
7	O
.	O
BERT	B-MethodName
LARGE	O
performs	O
competitively	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O
The	O
best	O
performing	O
method	O
concatenates	O
the	O
token	O
representations	O
from	O
the	O
top	O
four	O
hidden	O
layers	O
of	O
the	O
pre	O
-	O
trained	O
Transformer	B-MethodName
,	O
which	O
is	O
only	O
0.3	O
F1	B-MetricName
behind	O
fine	O
-	O
tuning	O
the	O
entire	O
model	O
.	O
This	O
demonstrates	O
that	O
BERT	B-MethodName
is	O
effective	O
for	O
both	O
finetuning	O
and	O
feature	O
-	O
based	O
approaches	O
.	O

Recent	O
empirical	O
improvements	O
due	O
to	O
transfer	B-TaskName
learning	I-TaskName
with	O
language	O
models	O
have	O
demonstrated	O
that	O
rich	O
,	O
unsupervised	B-TaskName
pre	I-TaskName
-	I-TaskName
training	I-TaskName
is	O
an	O
integral	O
part	O
of	O
many	O
language	O
understanding	O
systems	O
.	O
In	O
particular	O
,	O
these	O
results	O
enable	O
even	O
low	O
-	O
resource	O
tasks	O
to	O
benefit	O
from	O
deep	O
unidirectional	O
architectures	O
.	O
Our	O
major	O
contribution	O
is	O
further	O
generalizing	O
these	O
findings	O
to	O
deep	O
bidirectional	O
architectures	O
,	O
allowing	O
the	O
same	O
pre	O
-	O
trained	O
model	O
to	O
successfully	O
tackle	O
a	O
broad	O
set	O
of	O
NLP	O
tasks	O
.	O
To	O
generate	O
each	O
training	O
input	O
sequence	O
,	O
we	O
sample	O
two	O
spans	O
of	O
text	O
from	O
the	O
corpus	O
,	O
which	O
we	O
refer	O
to	O
as	O
"	O
sentences	O
"	O
even	O
though	O
they	O
are	O
typically	O
much	O
longer	O
than	O
single	O
sentences	O
(	O
but	O
can	O
be	O
shorter	O
also	O
)	O
.	O
The	O
first	O
sentence	O
receives	O
the	O
A	O
embedding	O
and	O
the	O
second	O
receives	O
the	O
B	O
embedding	O
.	O
50	O
%	O
of	O
the	O
time	O
B	O
is	O
the	O
actual	O
next	O
sentence	O
that	O
follows	O
A	O
and	O
50	O
%	O
of	O
the	O
time	O
it	O
is	O
a	O
random	O
sentence	O
,	O
which	O
is	O
done	O
for	O
the	O
"	O
next	O
sentence	O
prediction	O
"	O
task	O
.	O
They	O
are	O
sampled	O
such	O
that	O
the	O
combined	O
length	O
is	O
≤	O
512	O
tokens	O
.	O
The	O
LM	O
masking	O
is	O
applied	O
after	O
WordPiece	B-MethodName
tokenization	O
with	O
a	O
uniform	O
masking	O
rate	O
of	O
15	O
%	O
,	O
and	O
no	O
special	O
consideration	O
given	O
to	O
partial	O
word	O
pieces	O
.	O
We	O
train	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
256	O
sequences	O
(	O
256	O
sequences	O
*	O
512	O
tokens	O
=	O
128	O
,	O
000	O
tokens	O
/	O
batch	O
)	O
for	O
1	O
,	O
000	O
,	O
000	O
steps	O
,	O
which	O
is	O
approximately	O
40	O
epochs	O
over	O
the	O
3.3	O
billion	O
word	O
corpus	O
.	O
We	O
use	O
Adam	B-MethodName
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	O
-	O
4	O
,	O
β	B-HyperparameterName
1	O
=	O
0.9	O
,	O
β	B-HyperparameterName
2	O
=	O
0.999	O
,	O
L2	O
weight	B-MethodName
decay	I-MethodName
of	O
0.01	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
warmup	O
over	O
the	O
first	O
10	O
,	O
000	O
steps	O
,	O
and	O
linear	O
decay	O
of	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
.	O
We	O
use	O
a	O
dropout	O
probability	O
of	O
0.1	O
on	O
all	O
layers	O
.	O
We	O
use	O
a	O
gelu	B-MethodName
activation	O
(	O
Hendrycks	O
and	O
Gimpel	O
,	O
2016	O
)	O
rather	O
than	O
the	O
standard	O
relu	B-MethodName
,	O
following	O
OpenAI	O
GPT	B-MethodName
.	O
The	O
training	O
loss	B-MetricName
is	O
the	O
sum	O
of	O
the	O
mean	O
masked	O
LM	O
likelihood	O
and	O
the	O
mean	O
next	O
sentence	O
prediction	O
likelihood	O
.	O
Training	O
of	O
BERT	B-MethodName
BASE	B-MethodName
was	O
performed	O
on	O
4	O
Cloud	O
TPUs	O
in	O
Pod	O
configuration	O
(	O
16	O
TPU	O
chips	O
total	O
)	O
.	O
13	O
Training	O
of	O
BERT	B-MethodName
LARGE	O
was	O
performed	O
on	O
16	O
Cloud	O
TPUs	O
(	O
64	O
TPU	O
chips	O
total	O
)	O
.	O
Each	O
pretraining	O
took	O
4	O
days	O
to	O
complete	O
.	O
Longer	O
sequences	O
are	O
disproportionately	O
expensive	O
because	O
attention	O
is	O
quadratic	O
to	O
the	O
sequence	O
length	O
.	O
To	O
speed	O
up	O
pretraing	O
in	O
our	O
experiments	O
,	O
we	O
pre	O
-	O
train	O
the	O
model	O
with	O
sequence	O
length	O
of	O
128	O
for	O
90	O
%	O
of	O
the	O
steps	O
.	O
Then	O
,	O
we	O
train	O
the	O
rest	O
10	O
%	O
of	O
the	O
steps	O
of	O
sequence	O
of	O
512	O
to	O
learn	O
the	O
positional	O
embeddings	O
.	O

For	O
fine	O
-	O
tuning	O
,	O
most	O
model	O
hyperparameters	O
are	O
the	O
same	O
as	O
in	O
pre	O
-	O
training	O
,	O
with	O
the	O
exception	O
of	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
and	O
number	O
of	O
training	O
epochs	O
.	O
The	O
dropout	O
probability	O
was	O
always	O
kept	O
at	O
0.1	O
.	O
The	O
optimal	O
hyperparameter	O
values	O
are	O
task	O
-	O
specific	O
,	O
but	O
we	O
found	O
the	O
following	O
range	O
of	O
possible	O
values	O
to	O
work	O
well	O
across	O
all	O
tasks	O
:	O
Batch	B-HyperparameterName
size	I-HyperparameterName
:	O
16	O
,	O
32	O
We	O
also	O
observed	O
that	O
large	O
data	O
sets	O
(	O
e.g.	O
,	O
100k+	O
labeled	O
training	O
examples	O
)	O
were	O
far	O
less	O
sensitive	O
to	O
hyperparameter	O
choice	O
than	O
small	O
data	O
sets	O
.	O
Fine	O
-	O
tuning	O
is	O
typically	O
very	O
fast	O
,	O
so	O
it	O
is	O
reasonable	O
to	O
simply	O
run	O
an	O
exhaustive	O
search	O
over	O
the	O
above	O
parameters	O
and	O
choose	O
the	O
model	O
that	O
performs	O
best	O
on	O
the	O
development	O
set	O
.	O
A.4	O
Comparison	O
of	O
BERT	B-MethodName
,	O
ELMo	B-MethodName
,	O
and	O
OpenAI	O
GPT	B-MethodName
Here	O
we	O
studies	O
the	O
differences	O
in	O
recent	O
popular	O
representation	B-TaskName
learning	I-TaskName
models	O
including	O
ELMo	B-MethodName
,	O
OpenAI	O
GPT	B-MethodName
and	O
BERT	B-MethodName
.	O
The	O
comparisons	O
between	O
the	O
model	O
architectures	O
are	O
shown	O
visually	O
in	O
Figure	O
3	O
.	O
Note	O
that	O
in	O
addition	O
to	O
the	O
architecture	O
differences	O
,	O
BERT	B-MethodName
and	O
OpenAI	O
GPT	B-MethodName
are	O
finetuning	O
approaches	O
,	O
while	O
ELMo	B-MethodName
is	O
a	O
feature	O
-	O
based	O
approach	O
.	O
The	O
most	O
comparable	O
existing	O
pre	O
-	O
training	O
method	O
to	O
BERT	B-MethodName
is	O
OpenAI	O
GPT	B-MethodName
,	O
which	O
trains	O
a	O
left	O
-	O
to	O
-	O
right	O
Transformer	B-MethodName
LM	O
on	O
a	O
large	O
text	O
corpus	O
.	O
In	O
fact	O
,	O
many	O
of	O
the	O
design	O
decisions	O
in	O
BERT	B-MethodName
were	O
intentionally	O
made	O
to	O
make	O
it	O
as	O
close	O
to	O
GPT	B-MethodName
as	O
possible	O
so	O
that	O
the	O
two	O
methods	O
could	O
be	O
minimally	O
compared	O
.	O
The	O
core	O
argument	O
of	O
this	O
work	O
is	O
that	O
the	O
bi	O
-	O
directionality	O
and	O
the	O
two	O
pretraining	O
tasks	O
presented	O
in	O
Section	O
3.1	O
account	O
for	O
the	O
majority	O
of	O
the	O
empirical	O
improvements	O
,	O
but	O
we	O
do	O
note	O
that	O
there	O
are	O
several	O
other	O
differences	O
between	O
how	O
BERT	B-MethodName
and	O
GPT	B-MethodName
were	O
trained	O
:	O
GPT	B-MethodName
is	O
trained	O
on	O
the	O
BooksCorpus	O
(	O
800	O
M	O
words	O
)	O
;	O
BERT	B-MethodName
is	O
trained	O
on	O
the	O
BooksCorpus	O
(	O
800	O
M	O
words	O
)	O
and	O
Wikipedia	O
(	O
2	O
,	O
500	O
M	O
words	O
)	O
.	O
GPT	B-MethodName
uses	O
a	O
sentence	O
separator	O
(	O
GPT	B-MethodName
was	O
trained	O
for	O
1	O
M	O
steps	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	O
,	O
000	O
words	O
;	O
BERT	B-MethodName
was	O
trained	O
for	O
1	O
M	O
steps	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	O
,	O
000	O
words	O
.	O
GPT	B-MethodName
used	O
the	O
same	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e	O
-	O
5	O
for	O
all	O
fine	O
-	O
tuning	O
experiments	O
;	O
BERT	B-MethodName
chooses	O
a	O
task	O
-	O
specific	O
fine	O
-	O
tuning	O
learning	B-HyperparameterName
rate	I-HyperparameterName
which	O
performs	O
the	O
best	O
on	O
the	O
development	O
set	O
.	O
To	O
isolate	O
the	O
effect	O
of	O
these	O
differences	O
,	O
we	O
perform	O
ablation	O
experiments	O
in	O
Section	O
5.1	O
which	O
demonstrate	O
that	O
the	O
majority	O
of	O
the	O
improvements	O
are	O
in	O
fact	O
coming	O
from	O
the	O
two	O
pre	O
-	O
training	O
tasks	O
and	O
the	O
bidirectionality	O
they	O
enable	O
.	O

The	O
illustration	O
of	O
fine	O
-	O
tuning	O
BERT	B-MethodName
on	O
different	O
tasks	O
can	O
be	O
seen	O
in	O
Figure	O
4	O
.	O
Our	O
task	O
-	O
specific	O
models	O
are	O
formed	O
by	O
incorporating	O
BERT	B-MethodName
with	O
one	O
additional	O
output	O
layer	O
,	O
so	O
a	O
minimal	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
need	O
to	O
be	O
learned	O
from	O
scratch	O
.	O
Among	O
the	O
tasks	O
,	O
The	O
GLUE	B-DatasetName
benchmark	O
includes	O
the	O
following	O
datasets	O
,	O
the	O
descriptions	O
of	O
which	O
were	O
originally	O
summarized	O
in	O
Wang	O
et	O
al	O
(	O
2018a	O
)	O
:	O
MNLI	B-DatasetName
Multi	O
-	O
Genre	O
Natural	B-TaskName
Language	I-TaskName
Inference	I-TaskName
is	O
a	O
large	O
-	O
scale	O
,	O
crowdsourced	O
entailment	O
classification	O
task	O
(	O
Williams	O
et	O
al	O
,	O
2018	O
)	O
.	O
Given	O
a	O
pair	O
of	O
sentences	O
,	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
the	O
second	O
sentence	O
is	O
an	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
with	O
respect	O
to	O
the	O
first	O
one	O
.	O
QQP	B-DatasetName
Quora	B-DatasetName
Question	I-DatasetName
Pairs	I-DatasetName
is	O
a	O
binary	O
classification	O
task	O
where	O
the	O
goal	O
is	O
to	O
determine	O
if	O
two	O
questions	O
asked	O
on	O
Quora	O
are	O
semantically	O
equivalent	O
.	O
QNLI	B-DatasetName
Question	O
Natural	B-TaskName
Language	I-TaskName
Inference	I-TaskName
is	O
a	O
version	O
of	O
the	B-DatasetName
Stanford	I-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
which	O
has	O
been	O
converted	O
to	O
a	O
binary	O
classification	O
task	O
(	O
Wang	O
et	O
al	O
,	O
2018a	O
...	O
...	O
with	O
human	O
annotations	O
of	O
their	O
sentiment	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
.	O
CoLA	B-DatasetName
The	O
Corpus	O
of	O
Linguistic	B-TaskName
Acceptability	I-TaskName
is	O
a	O
binary	O
single	O
-	O
sentence	B-TaskName
classification	I-TaskName
task	O
,	O
where	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
an	O
English	O
sentence	O
is	O
linguistically	O
"	O
acceptable	O
"	O
or	O
not	O
(	O
Warstadt	O
et	O
al	O
,	O
2018	O
)	O
.	O

The	O
Semantic	B-TaskName
Textual	I-TaskName
Similarity	I-TaskName
Benchmark	O
is	O
a	O
collection	O
of	O
sentence	O
pairs	O
drawn	O
from	O
news	O
headlines	O
and	O
other	O
sources	O
(	O
Cer	O
et	O
al	O
,	O
2017	O
)	O
.	O
They	O
were	O
annotated	O
with	O
a	O
score	O
from	O
1	O
to	O
5	O
denoting	O
how	O
similar	O
the	O
two	O
sentences	O
are	O
in	O
terms	O
of	O
semantic	O
meaning	O
.	O
MRPC	B-DatasetName
Microsoft	O
Research	O
Paraphrase	O
Corpus	O
consists	O
of	O
sentence	O
pairs	O
automatically	O
extracted	O
from	O
online	O
news	O
sources	O
,	O
with	O
human	O
annotations	O
for	O
whether	O
the	O
sentences	O
in	O
the	O
pair	O
are	O
semantically	O
equivalent	O
(	O
Dolan	O
and	O
Brockett	O
,	O
2005	O
)	O
.	O
RTE	B-DatasetName
Recognizing	O
Textual	O
Entailment	O
is	O
a	O
binary	O
entailment	O
task	O
similar	O
to	O
MNLI	B-DatasetName
,	O
but	O
with	O
much	O
less	O
training	O
data	O
(	O
Bentivogli	O
et	O
al	O
,	O
2009	O
)	O
.	O
14	O
WNLI	B-DatasetName
Winograd	O
NLI	O
is	O
a	O
small	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
dataset	O
(	O
Levesque	O
et	O
al	O
,	O
2011	O
)	O
.	O
The	O
GLUE	B-DatasetName
webpage	O
notes	O
that	O
there	O
are	O
issues	O
with	O
the	O
construction	O
of	O
this	O
dataset	O
,	O
15	O
and	O
every	O
trained	O
system	O
that	O
's	O
been	O
submitted	O
to	O
GLUE	B-DatasetName
has	O
performed	O
worse	O
than	O
the	O
65.1	O
baseline	O
accuracy	B-MetricName
of	O
predicting	O
the	O
majority	O
class	O
.	O
We	O
therefore	O
exclude	O
this	O
set	O
to	O
be	O
fair	O
to	O
OpenAI	O
GPT	B-MethodName
.	O
For	O
our	O
GLUE	B-DatasetName
submission	O
,	O
we	O
always	O
predicted	O
the	O
majority	O
class	O
.	O

Our	O
evaluation	O
results	O
are	O
shown	O
in	O
Table	O
2	O
for	O
all	O
methods	O
.	O
This	O
includes	O
the	O
two	O
baseline	O
methods	O
,	O
and	O
the	O
machine	O
learning	O
methods	O
with	O
the	O
different	O
feature	O
sets	O
.	O
We	O
evaluate	O
the	O
machine	O
learning	O
methods	O
using	O
each	O
dataset	O
as	O
a	O
test	O
set	O
,	O
and	O
using	O
each	O
of	O
the	O
other	O
two	O
and	O
their	O
combination	O
as	O
the	O
training	O
set	O
.	O
The	O
First	O
Mentioned	O
baseline	O
is	O
very	O
weak	O
.	O
The	O
Most	O
Mentioned	O
baseline	O
is	O
much	O
stronger	O
.	O
In	O
almost	O
all	O
cases	O
machine	O
learning	O
methods	O
outperform	O
both	O
baselines	O
.	O
The	O
results	O
of	O
the	O
machine	O
learning	O
method	O
on	O
the	O
ASOIAF	O
and	O
SOC	B-DatasetName
are	O
very	O
strong	O
.	O
The	O
results	O
for	O
WOT	O
are	O
weaker	O
,	O
though	O
they	O
are	O
still	O
accurate	O
enough	O
to	O
be	O
useful	O
when	O
combined	O
with	O
manual	O
checking	O
.	O
It	O
is	O
surprising	O
that	O
using	O
the	O
combination	O
of	O
two	O
training	O
sets	O
does	O
not	O
always	O
out	O
-	O
perform	O
each	O
on	O
their	O
own	O
.	O
For	O
many	O
methods	O
training	O
on	O
just	O
one	O
dataset	O
resulted	O
in	O
better	O
results	O
.	O
We	O
believe	O
that	O
the	O
difference	O
between	O
the	O
top	O
result	O
for	O
a	O
method	O
and	O
the	O
result	O
using	O
the	O
combined	O
training	O
sets	O
is	O
too	O
small	O
to	O
be	O
meaningful	O
.	O
It	O
can	O
,	O
perhaps	O
,	O
be	O
attributed	O
to	O
a	O
coincidental	O
small	O
similarity	O
in	O
writing	O
style	O
of	O
one	O
of	O
the	O
training	O
books	O
to	O
the	O
testing	O
book	O
.	O
To	O
maximise	O
the	O
generalisability	O
of	O
the	O
NovelPerspective	O
prototype	O
(	O
see	O
Section	O
5	O
)	O
,	O
we	O
deploy	O
models	O
trained	O
on	O
all	O
three	O
datasets	O
combined	O
.	O
Almost	O
all	O
the	O
machine	O
learning	O
models	O
resulted	O
in	O
similarly	O
high	O
accuracy	B-MetricName
.	O
The	O
exception	O
to	O
this	O
is	O
word	O
embedding	O
features	O
based	O
model	O
trained	O
on	O
SOC	B-DatasetName
,	O
which	O
for	O
both	O
ASOIAF	O
and	O
WOT	O
test	O
sets	O
performed	O
much	O
worse	O
.	O
We	O
attribute	O
the	O
poor	O
performance	O
of	O
these	O
models	O
to	O
the	O
small	O
amount	O
of	O
training	O
data	O
.	O
SOC	B-DatasetName
has	O
only	O
91	O
chapters	O
to	O
generate	O
its	O
training	O
cases	O
from	O
,	O
and	O
the	O
word	O
embedding	O
feature	O
set	O
has	O
600	O
dimensions	O
.	O
It	O
is	O
thus	O
very	O
easily	O
to	O
over	O
-	O
fit	O
which	O
causes	O
these	O
poor	O
results	O
.	O
Table	O
3	O
shows	O
the	O
training	O
set	O
accuracy	B-MetricName
of	O
each	O
machine	O
learning	O
model	O
.	O
This	O
is	O
a	O
rough	O
upper	O
bound	O
for	O
the	O
possible	O
performance	O
of	O
these	O
models	O
on	O
each	O
test	O
set	O
,	O
as	O
imposed	O
by	O
the	O
classifier	O
and	O
the	O
feature	O
set	O
.	O
The	O
WOT	O
bound	O
is	O
much	O
lower	O
than	O
the	O
other	O
two	O
texts	O
.	O
This	O
likely	O
relates	O
to	O
WOT	O
being	O
written	O
in	O
a	O
style	O
that	O
closer	O
to	O
the	O
line	O
between	O
third	O
-	O
person	O
omniscient	O
,	O
than	O
the	O
more	O
clear	O
third	O
-	O
person	O
limited	O
POV	O
of	O
the	O
other	O
texts	O
.	O
We	O
believe	O
longer	O
range	O
features	O
are	O
required	O
to	O
improve	O
the	O
results	O
for	O
WOT	O
.	O
However	O
,	O
as	O
this	O
achieves	O
such	O
high	O
accuracy	B-MetricName
for	O
the	O
other	O
texts	O
,	O
further	O
features	O
would	O
not	O
improve	O
accuracy	B-MetricName
significantly	O
,	O
without	O
additional	O
more	O
difficult	O
training	O
data	O
(	O
and	O
may	O
cause	O
over	O
-	O
fitting	O
)	O
.	O
The	O
results	O
do	O
not	O
show	O
a	O
clear	O
advantage	O
to	O
either	O
machine	O
learning	O
feature	O
set	O
.	O
Both	O
the	O
classical	O
features	O
and	O
the	O
word	B-TaskName
embeddings	I-TaskName
work	O
well	O
.	O
Though	O
,	O
it	O
seems	O
that	O
the	O
classical	O
feature	O
are	O
more	O
robust	O
;	O
both	O
with	O
smaller	O
training	O
sets	O
(	O
like	O
SOC	B-DatasetName
)	O
,	O
and	O
with	O
more	O
difficult	O
test	O
sets	O
(	O
like	O
WOT	O
)	O
.	O

We	O
evaluated	O
the	O
tools	O
for	O
the	O
task	O
of	O
text	B-TaskName
similarity	I-TaskName
.	O
Therefore	O
,	O
we	O
calculated	O
the	O
similarity	O
between	O
the	O
input	O
and	O
candidate	O
documents	O
,	O
either	O
based	O
on	O
the	O
whole	O
text	O
or	O
on	O
selected	O
rhetorical	O
elements	O
as	O
provided	O
by	O
the	O
tools	O
.	O
When	O
utilizing	O
the	O
output	O
from	O
the	O
various	O
tools	O
,	O
we	O
built	O
a	O
pseudo	O
-	O
document	O
based	O
either	O
on	O
the	O
sentences	O
or	O
entities	O
that	O
we	O
obtained	O
.	O
For	O
the	O
zoning	O
tools	O
,	O
we	O
concatenated	O
the	O
sentences	O
to	O
form	O
a	O
single	O
text	O
,	O
while	O
we	O
printed	O
the	O
entities	O
(	O
one	O
per	O
line	O
)	O
for	O
the	O
entity	O
-	O
based	O
predictions	O
.	O
Similarly	O
,	O
when	O
evaluating	O
combination	O
of	O
various	O
labels	O
,	O
we	O
concatenated	O
the	O
text	O
from	O
various	O
labels	O
into	O
a	O
single	O
file	O
.	O
We	O
performed	O
text	B-TaskName
similarity	I-TaskName
using	O
the	O
TextFlow	O
tool	O
(	O
Mrabet	O
et	O
al	O
,	O
2017	O
)	O
and	O
utilized	O
these	O
similarity	O
scores	O
to	O
rank	O
the	O
candidate	O
documents	O
.	O
Subsequently	O
,	O
we	O
evaluated	O
the	O
ranked	O
list	O
with	O
regard	O
the	O
metrics	O
of	O
precision	O
,	O
recall	O
and	O
f	O
-	O
score	O
at	O
rank	O
10	O
,	O
i.e.	O
P@10	O
,	O
R@10	B-MetricName
and	O
F@10	O
.	O
P@10	O
is	O
the	O
rate	O
of	O
correct	O
positive	O
candidate	O
documents	O
in	O
the	O
top	O
10	O
highest	O
ranked	O
documents	O
,	O
i.e.	O
P	O
@10	O
=	O
T	O
P	O
@10	O
10	O
.	O
The	O
R@10	B-MetricName
corresponds	O
to	O
the	O
rate	O
of	O
positives	O
candidate	O
documents	O
in	O
the	O
top	O
10	O
over	O
the	O
total	O
of	O
all	O
positive	O
instances	O
,	O
i.e.	O
R@10	B-MetricName
=	O
T	O
P	O
@10	O
N	O
um	O
.	O
P	O
ositive	O
.	O
Finally	O
,	O
the	O
F@10	O
is	O
the	O
harmonic	O
average	O
of	O
the	O
P@10	O
and	O
R@10	B-MetricName
above	O
,	O
i.e.	O
F	O
@10	O
=	O
2	O
*	O
P	O
@10	O
*	O
R@10	B-MetricName
P	O
@10+R@10	O
.	O
We	O
considered	O
as	O
positive	O
examples	O
all	O
those	O
publications	O
manually	O
classified	O
by	O
our	O
expert	O
as	O
"	O
very	O
similar	O
"	O
or	O
"	O
similar	O
"	O
.	O
Given	O
the	O
few	O
of	O
these	O
instances	O
in	O
our	O
datasets	O
,	O
we	O
decided	O
to	O
make	O
no	O
distinction	O
between	O
both	O
categories	O
.	O
As	O
a	O
result	O
,	O
the	O
number	O
of	O
positive	O
examples	O
for	O
the	O
input	O
documents	O
in	O
Figure	O
1	O
are	O
4	O
,	O
10	O
,	O
16	O
,	O
11	O
,	O
8	O
,	O
23	O
and	O
6	O
,	O
respectively	O
.	O
We	O
evaluated	O
at	O
rank	O
10	O
due	O
to	O
the	O
reason	O
that	O
only	O
two	O
datasets	O
have	O
more	O
than	O
20	O
positive	O
instances	O
,	O
while	O
only	O
two	O
of	O
them	O
over	O
10	O
positive	O
instances	O
.	O
For	O
datasets	O
which	O
contain	O
more	O
than	O
10	O
positive	O
examples	O
,	O
we	O
considered	O
the	O
number	O
of	O
positive	O
instances	O
to	O
be	O
equal	O
to	O
10	O
in	O
the	O
equation	O
of	O
R@10	B-MetricName
.	O
For	O
the	O
final	O
comparison	O
between	O
the	O
various	O
tools	O
and	O
baselines	O
,	O
we	O
per	O
-	O
formed	O
an	O
average	O
of	O
the	O
metrics	O
over	O
the	O
seven	O
datasets	O
.	O
We	O
defined	O
two	O
baselines	O
for	O
comparison	O
:	O
(	O
i	O
)	O
the	O
original	O
order	O
of	O
the	O
candidate	O
documents	O
as	O
returned	O
by	O
PubMed	O
's	O
"	O
similar	O
articles	O
"	O
functionality	O
;	O
and	O
(	O
ii	O
)	O
string	O
similarity	O
based	O
on	O
the	O
whole	O
text	O
(	O
title	O
and	O
abstract	O
)	O
without	O
any	O
pre	O
-	O
processing	O
on	O
the	O
text	O
.	O
For	O
the	O
first	O
baseline	O
,	O
we	O
searched	O
in	O
PubMed	O
for	O
each	O
of	O
the	O
seven	O
PMIDs	O
and	O
downloaded	O
the	O
list	O
of	O
the	O
top	O
100	O
similar	O
articles	O
(	O
stand	O
of	O
March	O
13th	O
,	O
2019	O
)	O
.	O
Given	O
that	O
the	O
current	O
list	O
of	O
similar	O
articles	O
might	O
include	O
citations	O
not	O
present	O
at	O
the	O
time	O
when	O
our	O
corpus	O
was	O
annotated	O
,	O
we	O
dismissed	O
any	O
document	O
not	O
included	O
in	O
our	O
dataset	O
when	O
calculating	O
the	O
above	O
metrics	O
,	O
i.e.	O
,	O
we	O
did	O
not	O
consider	O
them	O
as	O
false	O
positives	O
.	O

We	O
compared	O
the	O
tools	O
based	O
on	O
the	O
metrics	O
of	O
P@10	O
,	O
R@10	B-MetricName
and	O
F@10	O
that	O
assess	O
the	O
performance	O
of	O
the	O
various	O
tools	O
for	O
the	O
ranking	O
task	O
.	O
We	O
performed	O
a	O
total	O
of	O
38	O
experiments	O
which	O
includes	O
the	O
four	O
tools	O
and	O
baselines	O
,	O
as	O
well	O
as	O
some	O
combinations	O
of	O
selected	O
labels	O
from	O
the	O
tools	O
.	O
The	O
combination	O
of	O
labels	O
were	O
decided	O
based	O
on	O
the	O
performance	O
of	O
the	O
single	O
labels	O
and	O
on	O
our	O
understanding	O
of	O
which	O
labels	O
are	O
more	O
relevant	O
for	O
our	O
use	O
case	O
.	O
Table	O
2	O
presents	O
the	O
results	O
for	O
our	O
two	O
baselines	O
and	O
the	O
best	O
results	O
for	O
each	O
tool	O
.	O
In	O
the	O
following	O
we	O
specify	O
the	O
labels	O
that	O
obtained	O
the	O
best	O
results	O
:	O
Achakulvisut	O
et	O
al	O
the	O
combination	O
of	O
all	O
labels	O
,	O
i.e.	O
"	O
Background	O
-	O
Conclusions	O
-	O
Methods	O
-	O
Objective	O
-	O
Results	O
"	O
ArguminSci	O
:	O
two	O
combinations	O
of	O
labels	O
were	O
equally	O
good	O
:	O
"	O
Background	O
-	O
Challenge	O
-	O
Table	O
2	O
:	O
Summary	O
of	O
the	O
results	O
from	O
the	O
two	O
baselines	O
(	O
two	O
first	O
rows	O
)	O
and	O
when	O
using	O
the	O
selected	O
tools	O
.	O
The	O
maximum	O
scores	O
represent	O
the	O
maximum	O
value	O
of	O
P@10	O
,	O
R@10	B-MetricName
and	O
F@10	O
that	O
could	O
have	O
been	O
obtained	O
by	O
any	O
of	O
the	O
approaches	O
.	O
The	O
minimum	O
scores	O
are	O
the	O
ones	O
obtained	O
when	O
randomly	O
selecting	O
10	O
candidates	O
in	O
each	O
dataset	O
,	O
averaged	O
over	O
1	O
,	O
000	O
experiments	O
.	O
Outcome	O
"	O
and	O
"	O
Background	O
-	O
Challenge	O
-	O
Outcome	O
-	O
FutureWork	O
"	O
.	O
MAZEA	O
:	O
the	O
combination	O
"	O
Method	O
-	O
Result	O
"	O
.	O
Prasad	O
and	O
Kan	O
:	O
the	O
combination	O
"	O
Process	O
-	O
Material	O
"	O
.	O
For	O
our	O
datasets	O
,	O
all	O
approaches	O
using	O
rhetorical	O
tools	O
obtained	O
a	O
better	O
performance	O
than	O
the	O
baseline	O
from	O
PubMed	O
.	O
Further	O
,	O
three	O
tools	O
scored	O
higher	O
than	O
our	O
strong	O
baseline	O
that	O
uses	O
TextFlow	O
over	O
the	O
whole	O
text	O
(	O
titles	O
and	O
abstracts	O
)	O
.	O
Two	O
of	O
the	O
tools	O
(	O
Achakulvisut	O
et	O
al	O
and	O
ArguminSci	O
)	O
address	O
zoning	O
elements	O
while	O
one	O
of	O
them	O
(	O
Prasad	O
and	O
Kan	O
)	O
returns	O
entity	O
-	O
level	O
annotations	O
.	O
However	O
,	O
none	O
of	O
the	O
tools	O
scored	O
close	O
the	O
maximum	O
possible	O
scores	O
.	O
Given	O
that	O
we	O
do	O
not	O
have	O
at	O
least	O
10	O
positive	O
instances	O
(	O
"	O
very	O
similar	O
"	O
or	O
"	O
similar	O
"	O
)	O
for	O
some	O
of	O
our	O
input	O
documents	O
,	O
our	O
maximum	O
P@10	O
is	O
of	O
0.83	O
instead	O
of	O
1.0	O
.	O
The	O
three	O
zoning	O
tools	O
rely	O
on	O
labels	O
that	O
can	O
be	O
mapped	O
to	O
one	O
another	O
,	O
as	O
shown	O
by	O
the	O
order	O
of	O
their	O
labels	O
in	O
Table	O
3	O
.	O
When	O
examining	O
the	O
performance	O
of	O
single	O
labels	O
,	O
only	O
the	O
"	O
Outcome	O
"	O
label	O
from	O
ArguminSci	O
tool	O
could	O
perform	O
close	O
our	O
strong	O
baseline	O
.	O
The	O
labels	O
that	O
we	O
expected	O
to	O
be	O
more	O
relevant	O
,	O
i.e.	O
the	O
ones	O
more	O
related	O
to	O
the	O
background	O
and	O
outcome	O
sections	O
and	O
less	O
with	O
the	O
methods	O
section	O
,	O
did	O
not	O
always	O
perform	O
better	O
in	O
the	O
ranking	O
task	O
.	O
For	O
instance	O
,	O
the	O
F@10	O
obtained	O
by	O
the	O
label	O
"	O
Approach	O
"	O
from	O
ArguminSci	O
performed	O
slightly	O
better	O
(	O
0.28	O
)	O
than	O
the	O
"	O
Background	O
"	O
(	O
0.24	O
)	O
and	O
"	O
Challenge	O
"	O
(	O
0.24	O
)	O
labels	O
.	O
Similarly	O
,	O
the	O
label	O
"	O
Method	O
"	O
from	O
MAZEA	O
performed	O
better	O
(	O
0.32	O
)	O
than	O
"	O
Background	O
"	O
(	O
0.25	O
)	O
and	O
"	O
Purpose	O
"	O
(	O
0.25	O
)	O
sections	O
.	O
We	O
wonder	O
whether	O
the	O
good	O
performance	O
of	O
methods	O
-	O
related	O
labels	O
were	O
actually	O
due	O
to	O
mistakes	O
in	O
the	O
classification	O
performed	O
by	O
the	O
tools	O
.	O
Our	O
experiments	O
showed	O
that	O
a	O
combination	O
of	O
labels	O
always	O
performed	O
better	O
than	O
the	O
single	O
ones	O
,	O
while	O
some	O
combinations	O
of	O
labels	O
performed	O
better	O
than	O
others	O
(	O
cf	O
.	O
Figure	O
3	O
)	O
.	O
We	O
could	O
not	O
find	O
any	O
difference	O
in	O
the	O
text	B-TaskName
similarity	I-TaskName
scores	O
(	O
as	O
computed	O
by	O
TextFlow	O
)	O
when	O
considering	O
different	O
order	O
of	O
the	O
same	O
labels	O
in	O
the	O
concatenation	O
of	O
the	O
text	O
.	O

The	O
noise	O
and	O
complexity	O
of	O
the	O
training	O
data	O
is	O
a	O
source	O
of	O
predictive	O
uncertainty	O
in	O
itself	O
,	O
referred	O
to	O
as	O
data	O
or	O
aleatoric	O
uncertainty	O
(	O
Kiureghian	O
and	O
Ditlevsen	O
,	O
2009	O
)	O
.	O
This	O
uncertainty	O
is	O
often	O
reflected	O
in	O
the	O
disagreement	O
between	O
human	O
annotations	O
for	O
the	O
same	O
sourcehypothesis	O
segment	O
(	O
Cohn	O
and	O
Specia	O
,	O
2013	O
;	O
Fornaciari	O
et	O
al	O
,	O
2021	O
)	O
.	O
We	O
hypothesize	O
that	O
the	O
direct	O
assessments	O
can	O
be	O
better	O
modelled	O
as	O
normally	O
distributed	O
scores	O
rather	O
than	O
a	O
single	O
score	O
,	O
and	O
that	O
a	O
model	O
trained	O
to	O
predict	O
this	O
distribution	O
(	O
mean	O
and	O
standard	O
deviation	O
)	O
could	O
provide	O
better	O
quality	O
estimates	O
2	O
.	O
We	O
formalise	O
this	O
as	O
a	O
KL	O
divergence	O
objective	O
,	O
using	O
the	O
closed	O
form	O
solution	O
to	O
estimate	O
the	O
KL	O
divergence	O
between	O
the	O
target	O
distribution	O
p	O
(	O
x	O
)	O
=	O
N	O
(	O
µ	O
1	O
,	O
σ	O
1	O
)	O
and	O
the	O
predicted	O
distribution	O
q	O
(	O
x	O
)	O
=	O
N	O
(	O
µ	O
2	O
,	O
σ	O
2	O
)	O
,	O
as	O
shown	O
in	O
Eq	O
.	O
1	O
.	O
KL	O
(	O
p	O
|	O
|	O
q	O
)	O
=	O
log	O
σ	O
2	O
σ	O
1	O
+	O
σ	O
2	O
1	O
+	O
(	O
µ	O
1	O
−	O
µ	O
2	O
)	O
2	O
2σ	O
2	O
2	O
−	O
1	O
2	O
(	O
1	O
)	O
where	O
we	O
take	O
the	O
mean	O
and	O
standard	O
deviation	O
(	O
std	O
)	O
of	O
the	O
direct	O
assessment	O
z_scores	O
as	O
the	O
target	O
(	O
ground	O
truth	O
proxy	O
)	O
values	O
p.	O
This	O
way	O
,	O
we	O
account	O
for	O
the	O
annotator	O
disagreement	O
(	O
reflected	O
in	O
the	O
std	O
value	O
)	O
during	O
learning	O
.	O
QE	O
epistemic	O
uncertainty	O
We	O
use	O
MC	O
dropout	O
(	O
Gal	O
and	O
Ghahramani	O
,	O
2016	O
)	O
to	O
account	O
for	O
the	O
uncertainty	O
of	O
the	O
QE	O
model	O
.	O
Specifically	O
,	O
we	O
enable	O
dropout	O
during	O
inference	O
and	O
run	O
multiple	O
forward	O
runs	O
over	O
each	O
test	O
instance	O
.	O
Thus	O
we	O
obtain	O
a	O
distribution	O
of	O
quality	O
predictions	O
for	O
each	O
instance	O
instead	O
of	O
a	O
single	O
point	O
estimate	O
.	O
We	O
use	O
the	O
estimated	O
mean	O
of	O
the	O
distribution	O
as	O
our	O
predicted	O
quality	O
estimate	O
.	O
MC	O
dropout	O
has	O
been	O
shown	O
to	O
improve	O
predictive	O
accuracy	B-MetricName
and	O
perform	O
on	O
par	O
or	O
even	O
better	O
compared	O
to	O
deep	B-MethodName
ensembles	I-MethodName
for	O
MT	O
evaluation	O
tasks	O
(	O
Glushkova	O
et	O
al	O
,	O
2021	O
)	O
.	O
It	O
thus	O
allows	O
us	O
to	O
simulate	O
ensembling	O
in	O
a	O
cheap	O
and	O
effective	O
way	O
,	O
without	O
the	O
need	O
to	O
train	O
multiple	O
checkpoints	O
.	O

The	O
QE	O
data	O
is	O
relatively	O
limited	O
,	O
making	O
it	O
harder	O
to	O
train	O
multilingual	O
models	O
with	O
a	O
large	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
without	O
over	O
-	O
fitting	O
.	O
Thus	O
,	O
as	O
explained	O
in	O
3.1	O
we	O
aimed	O
to	O
investigate	O
whether	O
we	O
could	O
obtain	O
models	O
that	O
generalise	O
better	O
and	O
are	O
more	O
robust	O
to	O
noise	O
and	O
out	O
-	O
of	O
-	O
distribution	O
data	O
by	O
training	O
the	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
model	O
first	O
on	O
a	O
larger	O
-	O
yet	O
noisier	O
and	O
out	O
-	O
of	O
-	O
domain	O
dataset	O
.	O
To	O
that	O
end	O
we	O
leverage	O
the	O
data	O
provided	O
for	O
the	O
past	O
Metrics	O
shared	O
tasks	O
,	O
which	O
covers	O
the	O
language	O
pairs	O
used	O
in	O
this	O
year	O
's	O
QE	O
task	O
,	O
including	O
the	O
blind	O
tests	O
for	O
which	O
we	O
had	O
no	O
in	O
-	O
domain	O
data	O
available	O
.	O
Altogether	O
,	O
it	O
encompasses	O
30	O
language	O
pairs	O
from	O
the	O
news	O
domain	O
(	O
versus	O
7	O
in	O
the	O
QE	O
dataset	O
)	O
.	O
We	O
provide	O
more	O
detailed	O
statistics	O
for	O
each	O
language	O
pair	O
of	O
the	O
Metrics	O
data	O
in	O
Appendix	O
C.	O
We	O
refer	O
to	O
experiments	O
using	O
the	O
model	O
initially	O
trained	O
on	O
the	O
Metrics	O
data	O
as	O
M1	O
M	O
-	O
.	O
We	O
also	O
show	O
that	O
using	O
the	O
trained	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
encoder	O
from	O
the	O
M1	O
M	O
model	O
can	O
prove	O
beneficial	O
for	O
the	O
predictions	O
on	O
post	O
-	O
edited	O
data	O
of	O
Task	O
2	O
(	O
see	O
Table	O
3	O
)	O
.	O

The	O
results	O
can	O
be	O
seen	O
in	O
Tables	O
1	O
and	O
2	O
.	O
In	O
line	O
with	O
the	O
shared	O
task	O
guidelines	O
we	O
treat	O
Pearson	O
r	O
as	O
the	O
primary	O
performance	O
metric	O
and	O
select	O
the	O
submitted	O
models	O
accordingly	O
.	O
We	O
can	O
observe	O
,	O
that	O
while	O
on	O
average	O
the	O
M1	O
model	O
and	O
its	O
variations	O
outperform	O
the	O
M2	O
model	O
,	O
their	O
performance	O
is	O
comparable	O
,	O
and	O
M2	O
-	O
KL	O
-	O
G	O
-	O
MCD	O
can	O
even	O
outperform	O
M1	O
M	O
-	O
ADAPT	O
for	O
specific	O
language	O
pairs	O
,	O
hence	O
it	O
made	O
sense	O
to	O
combine	O
them	O
in	O
the	O
final	O
ensemble	O
.	O
We	O
can	O
also	O
see	O
that	O
fine	O
-	O
tuning	O
the	O
M1	O
model	O
on	O
the	O
Metrics	O
data	O
,	O
results	O
in	O
performance	O
gains	O
for	O
the	O
majority	O
of	O
the	O
language	O
pairs	O
.	O
Specifically	O
,	O
even	O
applying	O
the	O
M1	O
M	O
directly	O
,	O
without	O
further	O
fine	O
-	O
tuning	O
on	O
QE	O
data	O
,	O
achieves	O
competitive	O
performance	O
for	O
most	O
pairs	O
,	O
which	O
further	O
improves	O
upon	O
fine	O
-	O
tuning	O
.	O
It	O
helps	O
in	O
increasing	O
the	O
performance	O
on	O
the	O
blind	O
sets	O
(	O
denoted	O
as	O
zeroshot	O
in	O
the	O
Appendix	O
B	O
assessments	O
for	O
each	O
segment	O
.	O
Thus	O
,	O
the	O
difference	O
in	O
target	O
score	O
range	O
and	O
distribution	O
could	O
affect	O
the	O
magnitude	O
of	O
predicted	O
scores	O
and	O
the	O
distance	O
to	O
the	O
ground	O
truth	O
values	O
,	O
which	O
is	O
reflected	O
in	O
the	O
MAE	B-MetricName
and	O
RMSE	B-MetricName
metrics	O
.	O
These	O
findings	O
,	O
further	O
supported	O
by	O
the	O
results	O
on	O
Task	O
2	O
,	O
is	O
a	O
first	O
step	O
in	O
exploring	O
the	O
underlying	O
connection	O
and	O
bridging	O
the	O
gap	O
between	O
the	O
Metrics	O
and	O
Quality	O
Estimation	O
shared	O
tasks	O
.	O

The	O
results	O
can	O
be	O
seen	O
in	O
Table	O
3	O
.	O
Similarly	O
to	O
Task	O
1	O
,	O
the	O
primary	O
evaluation	O
metric	O
for	O
the	O
sentence	O
level	O
sub	O
-	O
task	O
of	O
Task	O
2	O
is	O
the	O
Pearson	O
r	O
coefficient	O
,	O
2	O
:	O
Results	O
for	O
Task	O
1	O
with	O
the	O
M2	O
predictorestimator	O
(	O
mBART	B-MethodName
)	O
and	O
different	O
uncertainty	O
handling	O
additions	O
.	O
"	O
KL	O
"	O
signifies	O
the	O
incorporation	O
of	O
KL	O
loss	B-MetricName
,	O
"	O
G"the	O
incorporation	O
of	O
glass	O
-	O
box	O
features	O
and	O
MCD	O
the	O
addition	O
of	O
MC	O
dropout	O
.	O
ML	O
stands	O
for	O
MULTILIN	O
-	O
GUAL	O
,	O
showing	O
the	O
performance	O
averaged	O
over	O
all	O
language	O
pairs	O
.	O
Underlined	O
numbers	O
indicate	O
the	O
best	O
result	O
for	O
each	O
language	O
pair	O
and	O
evaluation	O
metric	O
.	O
Bold	O
systems	O
were	O
selected	O
for	O
the	O
final	O
ensemble	O
.	O
while	O
the	O
word	O
level	O
sub	O
-	O
task	O
is	O
evaluated	O
using	O
the	O
Matthews	O
correlation	O
coefficient	O
(	O
MCC	O
,	O
(	O
Matthews	O
,	O
1975	O
)	O
)	O
as	O
the	O
primary	O
performance	O
indicator	O
.	O
We	O
can	O
see	O
that	O
while	O
HTER	O
scores	O
do	O
not	O
always	O
correlate	O
highly	O
with	O
DAs	O
(	O
see	O
Table	O
4	O
)	O
,	O
the	O
use	O
of	O
the	O
M1	O
M	O
model	O
encoder	O
that	O
was	O
trained	O
on	O
large	O
data	O
with	O
direct	O
assessments	O
can	O
still	O
prove	O
useful	O
.	O
Indeed	O
,	O
when	O
fine	O
-	O
tuning	O
on	O
the	O
Task2	O
data	O
,	O
the	O
model	O
using	O
the	O
M1	O
M	O
encoder	O
(	O
M1	O
M	O
-	O
ADAPT	O
in	O
the	O
table	O
3	O
)	O
provides	O
a	O
performance	O
boost	O
for	O
the	O
Pearson	B-MetricName
correlation	I-MetricName
in	O
most	O
language	O
pairs	O
,	O
and	O
competitive	O
performance	O
for	O
the	O
rest	O
.	O
Based	O
on	O
these	O
results	O
,	O
we	O
deem	O
it	O
worthwhile	O
to	O
include	O
checkpoints	O
trained	O
with	O
this	O
configuration	O
in	O
the	O
ensemble	O
estimating	O
that	O
they	O
will	O
contribute	O
in	O
higher	O
performance	O
,	O
especially	O
on	O
the	O
blind	O
test	O
sets	O
.	O
This	O
can	O
be	O
further	O
confirmed	O
when	O

This	O
section	O
describes	O
how	O
to	O
construct	O
the	O
discourse	O
relation	O
graphs	O
and	O
action	O
graphs	O
.	O
Formally	O
,	O
for	O
a	O
given	O
conversation	O
C	O
=	O
{	O
u	O
0	B-DatasetName
,	O
...	O
,	O
u	O
m	O
}	O
with	O
m	O
utterances	O
,	O
we	O
construct	O
discourse	O
relation	O
graph	O
G	O
D	O
=	O
(	O
V	O
D	O
,	O
E	O
D	O
)	O
,	O
where	O
V	O
D	O
is	O
the	O
set	O
of	O
nodes	O
representing	O
Elementary	O
Discourse	O
Units	O
(	O
EDUs	O
)	O
,	O
and	O
E	O
D	O
is	O
the	O
adjacent	O
matrix	O
that	O
describes	O
the	O
relations	O
between	O
EDUs	O
,	O
and	O
action	O
graph	O
G	O
A	O
=	O
(	O
V	O
A	O
,	O
E	O
A	O
)	O
,	O
where	O
V	O
A	O
is	O
the	O
set	O
of	O
nodes	O
representing	O
"	O
WHO	O
"	O
,	O
"	O
DOING	O
"	O
and	O
"	O
WHAT	O
"	O
arguments	O
,	O
and	O
E	O
A	O
is	O
the	O
adjacent	O
matrix	O
to	O
link	O
"	O
WHO	O
-	O
DOING	O
-	O
WHAT	O
"	O
triples	O
.	O
Discourse	O
Relation	O
Graph	O
Utterances	O
from	O
different	O
speakers	O
do	O
not	O
occur	O
in	O
isolation	O
;	O
instead	O
,	O
they	O
are	O
related	O
within	O
the	O
context	O
of	O
discourse	O
(	O
Murray	O
et	O
al	O
,	O
2006	O
;	O
Qin	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
has	O
been	O
shown	O
effective	O
for	O
dialogue	B-TaskName
understanding	I-TaskName
like	O
identifying	O
the	O
decisions	O
in	O
multi	O
-	O
party	O
dialogues	O
(	O
Bui	O
et	O
al	O
,	O
2009	O
)	O
and	O
detecting	O
salient	O
content	O
in	O
email	O
conversations	O
(	O
McKeown	O
et	O
al	O
,	O
2007	O
)	O
.	O
Although	O
current	O
attention	O
-	O
based	O
neural	O
models	O
are	O
supposed	O
to	O
,	O
or	O
might	O
implicitly	O
,	O
learn	O
certain	O
relations	O
between	O
utterances	O
,	O
they	O
often	O
struggle	O
to	O
focus	O
on	O
many	O
informative	O
utterances	O
(	O
Chen	O
and	O
Yang	O
,	O
2020	O
;	O
Song	O
et	O
al	O
,	O
2020	O
)	O
and	O
fail	O
to	O
address	O
long	O
-	O
range	O
dependencies	O
(	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
,	O
especially	O
when	O
there	O
are	O
frequent	O
interruptions	O
.	O
As	O
a	O
result	O
,	O
explicitly	O
incorporating	O
the	O
discourse	O
relations	O
will	O
help	O
neural	O
summarization	B-TaskName
models	O
better	O
encode	O
the	O
unstructured	O
conversations	O
and	O
concentrate	O
on	O
the	O
most	O
salient	O
utterances	O
to	O
generate	O
more	O
informative	O
and	O
less	O
redundant	O
summaries	O
.	O
To	O
do	O
so	O
,	O
we	O
view	O
each	O
utterance	O
as	O
an	O
EDU	O
and	O
use	O
the	O
discourse	O
relation	O
types	O
defined	O
in	O
Asher	O
et	O
al	O
(	O
2016	O
)	O
.	O
We	O
first	O
pre	O
-	O
train	O
a	O
discourse	B-TaskName
parsing	I-TaskName
model	O
(	O
Shi	O
and	O
Huang	O
,	O
2019	O
)	O
on	O
a	O
humanannotated	O
multiparty	O
dialogue	O
corpus	O
(	O
Asher	O
et	O
al	O
,	O
2016	O
)	O
,	O
with	O
0.775	O
F1	B-MetricName
score	I-MetricName
on	O
link	O
predictions	O
and	O
0.557	O
F1	B-MetricName
score	I-MetricName
on	O
relation	O
classifications	O
,	O
which	O
are	O
comparable	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
(	O
Shi	O
and	O
Huang	O
,	O
2019	O
)	O
.	O
We	O
then	O
utilize	O
this	O
pre	O
-	O
trained	O
parser	O
to	O
predict	O
the	O
discourse	O
relations	O
within	O
conversations	O
in	O
our	O
SAMSum	B-DatasetName
corpus	I-DatasetName
(	O
Gliwa	O
et	O
al	O
,	O
2019	O
)	O
.	O
After	O
predictions	O
,	O
there	O
are	O
138	O
,	O
554	O
edges	O
identified	O
in	O
total	O
and	O
8.48	O
edges	O
per	O
conversation	O
.	O
The	O
distribution	O
of	O
these	O
predicted	O
discourse	O
relation	O
types	O
is	O
:	O
Comment	O
(	O
19.3	O
%	O
)	O
,	O
Clarification	O
Question	O
(	O
15.2	O
%	O
)	O
,	O
Elaboration	O
(	O
2.3	O
%	O
)	O
,	O
Acknowledgement	O
(	O
8.4	O
%	O
)	O
,	O
Continuation	O
(	O
10.1	O
%	O
)	O
,	O
Explanation	O
(	O
2.8	O
%	O
)	O
,	O
Conditional	O
(	O
0.2	O
%	O
)	O
,	O
Question	O
Answer	O
Pair	O
(	O
21.5	O
%	O
)	O
,	O
Alternation	O
(	O
0.3	O
%	O
)	O
,	O
Q	O
-	O
Elab	O
(	O
2.5	O
%	O
)	O
,	O
Result	O
(	O
5.5	O
%	O
)	O
,	O
Background	O
(	O
0.4	O
%	O
)	O
,	O
Narration	O
(	O
0.4	O
%	O
)	O
,	O
Correction	O
(	O
0.4	O
%	O
)	O
,	O
Parallel	O
(	O
0.9	O
%	O
)	O
,	O
and	O
Contrast	O
(	O
1.0	O
%	O
)	O
.	O
Then	O
for	O
each	O
conversation	O
,	O
we	O
construct	O
a	O
discourse	O
relation	O
graph	O
G	O
D	O
=	O
(	O
V	O
D	O
,	O
E	O
D	O
)	O
,	O
where	O
V	O
D	O
[	O
k	O
]	O
represents	O
the	O
k	O
-	O
th	O
utterance	O
.	O
E	O
D	O
[	O
i	O
]	O
[	O
j	O
]	O
=	O
r	O
if	O
there	O
is	O
a	O
link	O
from	O
the	O
i	O
-	O
th	O
utterance	O
to	O
the	O
j	O
-	O
th	O
one	O
with	O
discourse	O
relation	O
r.	O

Node	O
Initialization	O
For	O
discourse	O
relation	O
graph	O
,	O
we	O
employ	O
the	O
output	O
embeddings	O
of	O
the	O
special	O
tokens	O
x	O
i	O
,	O
0	B-DatasetName
from	O
the	O
utterance	O
encoder	O
,	O
i.e.	O
,	O
h	O
U	O
i	O
,	O
0	B-DatasetName
,	O
to	O
initialize	O
the	O
i	O
-	O
th	O
node	O
v	O
D	O
i	O
in	O
G	O
D	O
.	O
We	O
use	O
a	O
one	O
-	O
hot	O
embedding	O
layer	O
to	O
encode	O
the	O
relations	O
E	O
D	O
[	O
i	O
]	O
[	O
j	O
]	O
=	O
e	O
D	O
i	O
,	O
j	O
between	O
utterance	O
i	O
and	O
j.	O
For	O
action	O
graph	O
,	O
we	O
first	O
utilize	O
F	O
U	O
(	O
.	O
)	O
to	O
encode	O
each	O
token	O
in	O
nodes	O
v	O
A	O
i	O
and	O
then	O
average	O
their	O
output	O
embeddings	O
as	O
their	O
initial	O
representations	O
.	O
Structured	O
Graph	B-MethodName
Attention	I-MethodName
Network	I-MethodName
Based	O
on	O
Graph	B-MethodName
Attention	I-MethodName
Network	I-MethodName
(	O
Veličković	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
utilize	O
these	O
relations	O
between	O
nodes	O
to	O
encode	O
each	O
node	O
W	O
,	O
W	O
e	O
and	O
a	O
are	O
trainable	O
parameters	O
.	O
[	O
.	O
.	O
]	O
denotes	O
the	O
concatenation	O
of	O
two	O
vectors	O
.	O
σ	O
is	O
the	O
activation	B-HyperparameterName
function	I-HyperparameterName
,	O
N	O
i	O
is	O
the	O
set	O
containing	O
nodei	O
's	O
neighbours	O
in	O
G.	O
v	O
D	O
i	O
in	O
G	O
D	O
or	O
v	O
A	O
i	O
in	O
G	O
A	O
through	O
:	O
α	B-HyperparameterName
ij	O
=	O
exp	O
σ	O
a	O
T	O
[	O
Wv	O
i	O
Wv	O
j	O
W	O
e	O
e	O
i	O
,	O
j	O
]	O
k	O
N	O
i	O
exp	O
(	O
σ	O
(	O
a	O
T	O
[	O
Wv	O
i	O
Wv	O
k	O
W	O
e	O
e	O
i	O
,	O
k	O
]	O
)	O
)	O
h	O
i	O
=	O
σ	O
(	O
j	O
N	O
i	O
α	B-HyperparameterName
ij	O
Wv	O
j	O
)	O
Dataset	O
Split	O
#	O
Through	O
two	O
graph	O
encoders	O
F	O
D	O
(	O
.	O
,	O
.	O
)	O
and	O
F	O
A	O
(	O
.	O
,	O
.	O
)	O
,	O
we	O
then	O
obtain	O
the	O
hidden	O
representations	O
of	O
these	O
nodes	O
as	O
:	O
{	O
h	O
D	O
0	B-DatasetName
,	O
...	O
,	O
h	O
D	O
m	O
}	O
=	O
F	O
D	O
(	O
{	O
v	O
D	O
0	B-DatasetName
,	O
...	O
,	O
v	O
D	O
m	O
}	O
,	O
E	O
D	O
)	O
(	O
2	O
)	O
{	O
h	O
A	O
0	B-DatasetName
,	O
...	O
,	O
h	O
A	O
n	O
}	O
=	O
F	O
A	O
(	O
{	O
x	O
A	O
0	B-DatasetName
,	O
...	O
,	O
x	O
A	O
n	O
}	O
,	O
E	O
A	O
)	O
(	O
3	O
)	O

Different	O
levels	O
of	O
encoded	O
representations	O
are	O
then	O
aggregated	O
via	O
our	O
multi	O
-	O
granularity	O
decoder	O
to	O
generate	O
summaries	O
as	O
shown	O
in	O
Figure	O
2	O
(	O
b	O
)	O
.	O
With	O
s	O
−	O
1	O
previously	O
generated	O
tokens	O
y	O
1	O
,	O
...	O
,	O
y	O
s−1	O
,	O
our	O
decoder	O
G	O
(	O
.	O
)	O
predicts	O
the	O
l	O
-	O
th	O
token	O
via	O
:	O
y	O
=	O
G	O
(	O
y	O
1	O
:	O
s−1	O
,	O
F	O
U	O
(	O
C	O
)	O
,	O
F	O
D	O
(	O
G	O
D	O
)	O
,	O
F	O
A	O
(	O
G	O
A	O
)	O
)	O
(	O
4	O
)	O
P	O
(	O
ỹ	O
s	O
|	O
y	O
<	O
s	O
,	O
C	O
,	O
G	O
D	O
,	O
G	O
A	O
)	O
=	O
Softmax	B-MethodName
(	O
W	O
pŷ	O
)	O
(	O
5	O
)	O
To	O
better	O
incorporate	O
the	O
information	O
in	O
constructed	O
graphs	O
,	O
different	O
from	O
the	O
traditional	O
pretrained	O
BART	B-MethodName
model	O
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
,	O
we	O
improve	O
the	O
BART	B-MethodName
transformer	B-MethodName
decoder	I-MethodName
with	O
two	O
extra	O
cross	O
attentions	O
(	O
Discourse	O
Attention	O
and	O
Action	O
Attention	O
)	O
added	O
to	O
each	O
decoder	O
layer	O
,	O
which	O
attends	O
to	O
the	O
encoded	O
node	O
representations	O
in	O
discourse	O
relation	O
graphs	O
and	O
action	O
graphs	O
.	O
In	O
each	O
decoder	O
layer	O
,	O
after	O
performing	O
the	O
original	O
cross	O
attentions	O
over	O
every	O
token	O
in	O
utterances	O
{	O
h	O
U	O
i	O
,	O
0	B-DatasetName
:	O
l	O
}	O
and	O
getting	O
the	O
utterance	O
-	O
attended	O
representation	O
x	O
U	O
,	O
multi	O
-	O
granularity	O
decoder	O
then	O
conducts	O
cross	O
attentions	O
over	O
nodes	O
{	O
h	O
D	O
0	B-DatasetName
:	O
m	O
}	O
and	O
{	O
h	O
A	O
0	B-DatasetName
:	O
n	O
}	O
that	O
are	O
encoded	O
from	O
graph	O
encoders	O
in	O
parallel	O
,	O
to	O
obtain	O
the	O
discourse	O
-	O
attended	O
representation	O
x	O
D	O
and	O
action	O
-	O
attended	O
representation	O
x	O
A	O
.	O
These	O
two	O
attended	O
vectors	O
are	O
then	O
combined	O
into	O
a	O
structureaware	O
representation	O
x	O
S	O
,	O
through	O
a	O
feed	O
-	O
forward	O
network	O
for	O
further	O
forward	O
passing	O
in	O
the	O
decoder	O
.	O
To	O
alleviate	O
the	O
negative	O
impact	O
of	O
randomly	O
initialized	O
graph	O
encoders	O
and	O
cross	O
attentions	O
over	O
graphs	O
on	O
pre	O
-	O
trained	O
BART	B-MethodName
decoders	O
at	O
early	O
stages	O
and	O
accelerate	O
the	O
learning	O
of	O
newlyintroduced	O
modules	O
during	O
training	O
,	O
we	O
apply	O
ReZero	B-MethodName
(	O
Bachlechner	O
et	O
al	O
,	O
2020	O
)	O
to	O
the	O
residual	B-MethodName
connection	I-MethodName
after	O
attending	O
to	O
graphs	O
in	O
each	O
decoder	O
layer	O
:	O
x	O
S	O
=	O
x	O
U	O
+	O
αx	O
S	O
(	O
6	O
)	O
where	O
α	B-HyperparameterName
is	O
one	O
trainable	O
parameter	O
instead	O
of	O
a	O
fixed	O
value	O
1	O
,	O
which	O
modulates	O
updates	O
from	O
cross	O
attentions	O
over	O
graphs	O
.	O
Training	O
During	O
training	O
,	O
we	O
seek	O
to	O
minimize	O
the	O
cross	O
entropy	O
and	O
use	O
the	O
teacher	O
-	O
forcing	O
strategy	O
(	O
Bengio	O
et	O
al	O
,	O
2015	O
)	O
:	O
L	O
=	O
−	O
log	O
P	O
(	O
ỹ	O
l	O
|	O
y	O
<	O
l	O
,	O
C	O
,	O
G	O
D	O
,	O
G	O
A	O
)	O
(	O
7	O
)	O
4	O
Experiments	O

We	O
used	O
the	O
BART	B-MethodName
-	O
base	O
model	O
to	O
initialize	O
our	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
for	O
training	O
in	O
all	O
experiments	O
.	O
For	O
parameters	O
in	O
the	O
original	O
BART	B-MethodName
encoder	O
/	O
decoder	O
,	O
we	O
followed	O
the	O
default	O
settings	O
and	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
3e	O
-	O
5	O
with	O
120	O
warm	O
-	O
up	O
steps	O
.	O
For	O
graph	O
encoders	O
,	O
we	O
set	O
the	O
number	O
of	O
hidden	O
dimensions	O
as	O
768	O
,	O
the	O
number	O
of	O
attention	O
heads	O
as	O
2	O
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
as	O
2	O
,	O
and	O
the	O
dropout	O
rate	O
as	O
0.2	O
.	O
For	O
graph	O
cross	O
attentions	O
added	O
to	O
BART	B-MethodName
decoder	O
layers	O
,	O
we	O
set	O
the	O
number	O
of	O
attention	O
heads	O
as	O
2	O
.	O
The	O
weights	O
α	B-HyperparameterName
in	O
ReZero	B-MethodName
residual	O
connections	O
were	O
initialized	O
with	O
1	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
for	O
parameters	O
in	O
newly	O
added	O
modules	O
was	O
3e	O
-	O
4	O
with	O
60	O
warm	O
-	O
up	O
steps	O
.	O
All	O
experiments	O
were	O
performed	O
on	O
GeForce	O
RTX	O
2080Ti	O
(	O
11	O
GB	O
memory	O
)	O
.	O

Automatic	O
Evaluation	O
We	O
evaluated	O
all	O
the	O
models	O
with	O
the	O
widely	O
used	O
automatic	O
metric	O
,	O
All	O
model	O
variants	O
of	O
S	O
-	O
BART	B-MethodName
received	O
significantly	O
higher	O
ratings	O
than	O
BART	B-MethodName
(	O
student	O
t	O
-	O
test	O
,	O
p	O
<	O
0.05	O
)	O
.	O
ROUGE	O
scores	O
(	O
Lin	O
and	O
Och	O
,	O
2004	O
)	O
3	O
,	O
and	O
reported	O
ROUGE	O
-	O
1	O
,	O
ROUGE	O
-	O
2	O
,	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
in	O
Table	O
2	O
.	O
We	O
found	O
that	O
,	O
compared	O
to	O
simple	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
(	O
Pointer	O
Generator	O
and	O
Transformer	B-MethodName
)	O
,	O
incorporating	O
extra	O
information	O
such	O
as	O
commonsense	O
knowledge	O
from	O
ConceptNet	B-DatasetName
(	O
D	O
-	O
HGN	O
)	O
increased	O
the	O
ROUGE	O
metrics	O
.	O
When	O
equipped	O
with	O
pre	O
-	O
trained	O
models	O
and	O
simple	O
conversation	O
structures	O
such	O
as	O
topics	O
and	O
conversation	O
stages	O
,	O
Multi	O
-	O
View	O
Seq2Seq	B-MethodName
boosted	O
ROUGE	O
scores	O
.	O
Incorporating	O
discourse	O
relation	O
graphs	O
or	O
action	O
graphs	O
helped	O
the	O
performances	O
of	O
summarization	B-TaskName
,	O
suggesting	O
the	O
effectiveness	O
of	O
explicitly	O
modeling	O
relations	O
between	O
utterances	O
and	O
the	O
associations	O
between	O
speakers	O
and	O
actions	O
within	O
utterances	O
.	O
Combining	O
two	O
different	O
structured	O
graphs	O
produced	O
better	O
ROUGE	O
scores	O
compared	O
to	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
and	O
our	O
base	O
models	O
,	O
with	O
an	O
increase	O
of	O
2.0	O
%	O
on	O
ROUGE	O
-	O
1	O
,	O
4.3	O
%	O
on	O
ROUGE	O
-	O
2	O
,	O
and	O
1.2	O
%	O
on	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
compared	O
to	O
our	O
base	O
model	O
,	O
BART	B-MethodName
.	O
This	O
indicates	O
that	O
,	O
our	O
structure	O
-	O
aware	O
models	O
with	O
discourse	O
and	O
action	O
graphs	O
could	O
help	O
abstractive	O
conversation	O
summarization	B-TaskName
,	O
and	O
these	O
two	O
graphs	O
complemented	O
each	O
other	O
in	O
generating	O
better	O
summaries	O
.	O

To	O
investigate	O
the	O
generalizability	O
of	O
our	O
structureaware	O
models	O
,	O
we	O
then	O
tested	O
the	O
S	O
-	O
BART	B-MethodName
model	O
trained	O
on	O
SAMSum	B-DatasetName
corpus	I-DatasetName
directly	O
on	O
the	O
debate	O
summarization	B-TaskName
domain	O
(	O
ADSC	O
Corpus	O
(	O
Misra	O
et	O
al	O
,	O
2015	O
)	O
)	O
in	O
a	O
zero	O
-	O
shot	O
setting	O
.	O
Besides	O
the	O
differences	O
in	O
topics	O
,	O
utterances	O
in	O
debate	O
conversations	O
were	O
generally	O
longer	O
and	O
include	O
more	O
action	O
triples	O
(	O
37.20	O
vs	O
6.81	O
as	O
shown	O
in	O
Table	O
1	O
)	O
and	O
fewer	O
participants	O
.	O
The	O
distribution	O
of	O
discourse	O
relation	O
types	O
also	O
differed	O
a	O
lot	O
across	O
different	O
domains	O
4	O
(	O
e.g.	O
,	O
more	O
Contrast	O
in	O
debates	O
(	O
19.5	O
%	O
)	O
than	O
in	O
daily	O
conversations	O
(	O
1.0	O
%	O
)	O
)	O
.	O
As	O
shown	O
in	O
Table	O
3	O
,	O
our	O
single	O
graph	O
models	O
S	O
-	O
BART	B-MethodName
w.	O
Discourse	O
and	O
S	O
-	O
BART	B-MethodName
w.	O
Action	O
boosted	O
ROUGE	O
scores	O
compared	O
to	O
BART	B-MethodName
,	O
suggesting	O
that	O
utilizing	O
structures	O
can	O
also	O
increase	O
the	O
generalizability	O
of	O
conversation	O
summarization	B-TaskName
methods	O
.	O
However	O
,	O
contrary	O
to	O
in	O
-	O
domain	O
results	O
in	O
Table	O
2	O
,	O
action	O
graphs	O
led	O
to	O
much	O
more	O
gains	O
than	O
discourse	O
graphs	O
.	O
This	O
indicated	O
that	O
when	O
domain	O
shifts	O
,	O
action	O
triples	O
were	O
most	O
robust	O
in	O
terms	O
of	O
zero	O
-	O
shot	O
setups	O
;	O
differences	O
in	O
discourse	O
relation	O
distributions	O
could	O
limit	O
such	O
generalization	O
.	O
Consistent	O
with	O
in	O
-	O
domain	O
scenarios	O
,	O
our	O
S	O
-	O
BART	B-MethodName
w.	O
Discourse&Action	O
achieved	O
better	O
results	O
,	O
with	O
an	O
increase	O
of	O
66.2	O
%	O
on	O
ROUGE	O
-	O
1	O
,	O
373.4	O
%	O
on	O
ROUGE	O
-	O
2	O
,	O
and	O
82.2	O
%	O
on	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
over	O
BART	B-MethodName
.	O

This	O
part	O
conducted	O
ablation	O
studies	O
to	O
show	O
the	O
effectiveness	O
of	O
structured	O
graphs	O
in	O
our	O
S	O
-	O
BART	B-MethodName
.	O
The	O
Quality	O
of	O
Discourse	O
Relation	O
Graphs	O
We	O
showed	O
how	O
the	O
quality	O
of	O
discourse	O
relation	O
graphs	O
affected	O
the	O
performances	O
of	O
conversation	O
summarization	B-TaskName
in	O
Table	O
5	O
.	O
Specifically	O
,	O
we	O
compared	O
the	O
ROUGE	O
scores	O
of	O
S	O
-	O
BART	B-MethodName
using	O
our	O
constructed	O
discourse	O
relation	O
graphs	O
(	O
S	O
-	O
BART	B-MethodName
w.	O
Discourse	O
Graph	O
)	O
and	O
S	O
-	O
BART	B-MethodName
using	O
randomly	O
generated	O
discourse	O
relation	O
graphs	O
S	O
-	O
BART	B-MethodName
w.	O
Random	O
Graph	O
where	O
both	O
connections	O
between	O
nodes	O
and	O
relation	O
types	O
were	O
randomized	O
.	O
The	O
number	O
of	O
edges	O
in	O
two	O
graphs	O
was	O
kept	O
the	O
same	O
.	O
We	O
found	O
that	O
S	O
-	O
BART	B-MethodName
with	O
our	O
discourse	O
graphs	O
outperformed	O
6	O
.	O
Here	O
,	O
parallel	O
strategy	O
performed	O
cross	O
attentions	O
on	O
different	O
graphs	O
separately	O
and	O
then	O
combined	O
the	O
attended	O
results	O
with	O
feed	O
-	O
forward	O
networks	O
as	O
discussed	O
in	O
Section	O
3.3	O
;	O
sequential	O
strategy	O
performed	O
cross	O
attentions	O
on	O
two	O
graphs	O
in	O
a	O
specific	O
order	O
(	O
from	O
discourse	O
relation	O
graphs	O
to	O
actions	O
graphs	O
,	O
or	O
vice	O
versa	O
)	O
.	O
We	O
found	O
that	O
the	O
parallel	O
strategy	O
showed	O
better	O
performances	O
and	O
the	O
sequential	O
ones	O
did	O
not	O
introduce	O
gains	O
compared	O
to	O
S	O
-	O
BART	B-MethodName
with	O
single	O
graphs	O
.	O
This	O
demonstrates	O
that	O
discourse	O
relation	O
graphs	O
and	O
action	O
graphs	O
were	O
both	O
important	O
and	O
provided	O
different	O
signals	O
for	O
abstractive	O
conversation	O
summarization	B-TaskName
.	O
Visualizing	O
ReZero	B-MethodName
Weights	O
We	O
further	O
tested	O
our	O
structure	O
-	O
aware	O
BART	B-MethodName
with	O
two	O
ReZero	B-MethodName
settings	O
:	O
(	O
i	O
)	O
initializing	O
α	B-HyperparameterName
from	O
0	B-DatasetName
,	O
(	O
ii	O
)	O
initializing	O
α	B-HyperparameterName
from	O
1	O
,	O
and	O
found	O
initializing	O
α	B-HyperparameterName
from	O
1	O
would	O
bring	O
in	O
more	O
performance	O
gains	O
(	O
see	O
Appendix	O
)	O
.	O
We	O
then	O
visualized	O
the	O
average	O
α	B-HyperparameterName
over	O
different	O
decoder	O
layers	O
after	O
training	O
in	O
Figure	O
3	O
,	O
and	O
observed	O
that	O
(	O
i	O
)	O
when	O
α	B-HyperparameterName
was	O
initialized	O
with	O
1	O
,	O
the	O
final	O
α	B-HyperparameterName
was	O
much	O
larger	O
than	O
the	O
setting	O
where	O
α	B-HyperparameterName
was	O
initialized	O
with	O
0	B-DatasetName
,	O
which	O
might	O
because	O
randomly	O
initialized	O
modules	O
barely	O
received	O
supervisions	O
at	O
early	O
stages	O
and	O
therefore	O
contributes	O
less	O
to	O
BART	B-MethodName
.	O
(	O
ii	O
)	O
Compared	O
to	O
discourse	O
graphs	O
,	O
action	O
graphs	O
received	O
higher	O
α	B-HyperparameterName
weights	O
after	O
training	O
in	O
both	O
initializing	O
settings	O
,	O
suggesting	O
that	O
the	O
information	O
from	O
structured	O
action	O
graphs	O
might	O
be	O
harder	O
for	O
the	O
end	O
-	O
to	O
-	O
end	O
BART	B-MethodName
models	O
to	O
capture	O
.	O
(	O
iii	O
)	O
Utilizing	O
both	O
graphs	O
spontaneously	O
led	O
to	O
higher	O
ReZero	B-MethodName
weights	O
,	O
further	O
validating	O
the	O
effectiveness	O
of	O
combining	O
discourse	O
relation	O
graphs	O
and	O
action	O
graphs	O
and	O
their	O
complementary	O
properties	O
.	O

To	O
inspect	O
when	O
our	O
summarization	B-TaskName
models	O
could	O
help	O
the	O
conversations	O
summarization	B-TaskName
,	O
we	O
visualized	O
the	O
average	O
number	O
of	O
discourse	O
edges	O
and	O
the	O
average	O
number	O
of	O
action	O
triples	O
in	O
three	O
sets	O
of	O
conversations	O
in	O
examples	O
where	O
both	O
S	O
-	O
BART	B-MethodName
and	O
BART	B-MethodName
showed	O
low	O
ROUGE	O
scores	O
(	O
ROUGE	O
-	O
1	O
<	O
20.0	O
,	O
ROUGE	O
-	O
2	O
<	O
10.0	O
,	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
<	O
10.0	O
)	O
.	O
When	O
the	O
structures	O
in	O
conversations	O
were	O
simpler	O
(	O
fewer	O
discourse	O
edges	O
and	O
fewer	O
action	O
triples	O
than	O
the	O
average	O
)	O
,	O
BART	B-MethodName
showed	O
similar	O
performance	O
as	O
S	O
-	O
BART	B-MethodName
.	O
As	O
the	O
structures	O
of	O
conversations	O
become	O
more	O
complex	O
with	O
more	O
discourse	O
relations	O
and	O
more	O
action	O
mentions	O
,	O
S	O
-	O
BART	B-MethodName
outperformed	O
BART	B-MethodName
as	O
it	O
explicitly	O
incorporated	O
these	O
structured	O
graphs	O
.	O
However	O
,	O
both	O
BART	B-MethodName
and	O
S	O
-	O
BART	B-MethodName
struggled	O
when	O
there	O
were	O
much	O
more	O
interactions	O
beyond	O
certain	O
thresholds	O
,	O
calling	O
for	O
better	O
mechanisms	O
to	O
model	O
structures	O
in	O
conversations	O
for	O
generating	O
better	O
summaries	O
.	O

We	O
tested	O
our	O
structure	O
-	O
aware	O
BART	B-MethodName
(	O
S	O
-	O
BART	B-MethodName
w.	O
Discourse	O
/	O
Action	O
)	O
within	O
two	O
ReZero	B-MethodName
settings	O
:	O
(	O
i	O
)	O
initializing	O
α	B-HyperparameterName
from	O
0	B-DatasetName
,	O
(	O
ii	O
)	O
initializing	O
α	B-HyperparameterName
from	O
1	O
.	O
And	O
the	O
results	O
were	O
shown	O
in	O
Table	O
8	O
.	O
S	O
-	O
BART	B-MethodName
with	O
1	O
as	O
the	O
initialized	O
ReZero	B-MethodName
weight	O
outperformed	O

Many	O
NLP	O
tasks	O
can	O
be	O
formulated	O
as	O
sequence	O
labeling	O
problems	O
,	O
such	O
as	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
(	O
POS	O
)	O
tagging	O
(	O
Zheng	O
et	O
al	O
,	O
2013	O
)	O
,	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
)	O
,	O
and	O
event	B-TaskName
extraction	I-TaskName
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
.	O
Recently	O
,	O
neural	O
sequential	O
models	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Akbik	O
et	O
al	O
,	O
2018	O
;	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
have	O
shown	O
strong	O
performance	O
for	O
various	O
sequence	O
labeling	O
task	O
.	O
However	O
,	O
these	O
deep	O
neural	O
models	O
are	O
label	O
hungrythey	O
require	O
large	O
amounts	O
of	O
annotated	O
sequences	O
to	O
achieve	O
strong	O
performance	O
.	O
Obtaining	O
large	O
amounts	O
of	O
annotated	O
data	O
can	O
be	O
too	O
expensive	O
for	O
practical	O
sequence	O
labeling	O
tasks	O
,	O
due	O
to	O
tokenlevel	O
annotation	O
efforts	O
.	O
Active	B-TaskName
learning	I-TaskName
is	O
an	O
important	O
technique	O
for	O
sequence	O
labeling	O
in	O
low	O
-	O
resource	O
settings	O
.	O
Active	O
sequence	O
labeling	O
is	O
an	O
iterative	O
process	O
.	O
In	O
each	O
iteration	O
,	O
a	O
fixed	O
number	O
of	O
unlabeled	O
sequences	O
are	O
selected	O
by	O
a	O
query	O
policy	O
for	O
annotation	O
and	O
then	O
model	O
updating	O
,	O
in	O
hope	O
of	O
maximally	O
improving	O
model	O
performance	O
.	O
For	O
example	O
,	O
Tomanek	O
et	O
al	O
(	O
2007	O
)	O
;	O
Shen	O
et	O
al	O
(	O
2017	O
)	O
select	O
query	O
samples	O
based	O
on	O
data	O
uncertainties	O
;	O
Hazra	O
et	O
al	O
(	O
2019	O
)	O
compute	O
model	O
-	O
aware	O
similarity	O
to	O
eliminate	O
redundant	O
examples	O
and	O
improve	O
the	O
diversity	O
of	O
query	O
samples	O
;	O
and	O
Fang	O
et	O
al	O
(	O
2017	O
)	O
;	O
Liu	O
et	O
al	O
(	O
2018	O
)	O
use	O
reinforcement	O
learning	O
to	O
learn	O
query	O
policies	O
.	O
However	O
,	O
existing	O
methods	O
for	O
active	O
sequence	O
labeling	O
all	O
use	O
the	O
queried	O
samples	O
alone	O
in	O
each	O
iteration	O
.	O
We	O
argue	O
that	O
the	O
queried	O
samples	O
provide	O
limited	O
data	O
diversity	O
,	O
and	O
using	O
them	O
alone	O
for	O
model	O
updating	O
is	O
inefficient	O
in	O
terms	O
of	O
leveraging	O
human	O
annotation	O
efforts	O
.	O
We	O
study	O
the	O
problem	O
of	O
enhancing	O
active	O
sequence	O
labeling	O
via	O
data	B-TaskName
augmentation	I-TaskName
.	O
We	O
aim	O
to	O
generate	O
augmented	O
labeled	O
sequences	O
for	O
the	O
queried	O
samples	O
in	O
each	O
iteration	O
,	O
thereby	O
introducing	O
more	O
data	O
diversity	O
and	O
improve	O
model	O
generalization	O
.	O
However	O
,	O
data	B-TaskName
augmentation	I-TaskName
for	O
active	O
sequence	O
labeling	O
is	O
challenging	O
,	O
because	O
we	O
need	O
to	O
generate	O
sentences	O
and	O
token	O
-	O
level	O
labels	O
jointly	O
.	O
Prevailing	O
generative	O
models	O
(	O
Zhang	O
et	O
al	O
,	O
2016	O
;	O
Bowman	O
et	O
al	O
,	O
2016	O
)	O
are	O
inapplicable	O
because	O
they	O
can	O
only	O
generate	O
word	O
sequences	O
without	O
labels	O
.	O
It	O
is	O
also	O
infeasible	O
to	O
apply	O
heuristic	O
data	B-TaskName
augmentation	I-TaskName
methods	O
such	O
as	O
context	O
-	O
based	O
words	O
substitution	O
(	O
Kobayashi	O
,	O
2018	O
)	O
,	O
synonym	O
replacement	O
,	O
random	O
insertion	O
,	O
swap	O
,	O
and	O
deletion	O
(	O
Wei	O
and	O
Zou	O
,	O
2019	O
)	O
,	O
paraphrasing	O
(	O
Cho	O
et	O
al	O
,	O
2019	O
)	O
or	O
back	O
translation	O
,	O
because	O
label	O
composition	O
is	O
complex	O
for	O
sequence	O
labeling	O
.	O
Directly	O
using	O
these	O
techniques	O
to	O
manipulate	O
tokens	O
may	O
inject	O
incorrectly	O
labeled	O
sequences	O
into	O
training	O
data	O
and	O
harm	O
model	O
performance	O
.	O
We	O
propose	O
SeqMix	O
,	O
a	O
data	B-TaskName
augmentation	I-TaskName
method	O
for	O
generating	O
sub	O
-	O
sequences	O
along	O
with	O
their	O
labels	O
based	O
on	O
mixup	B-MethodName
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Under	O
the	O
active	O
sequence	O
labeling	O
framework	O
,	O
Se	O
-	O
qMix	O
is	O
capable	O
of	O
generating	O
plausible	O
pseudo	O
labeled	O
sequences	O
for	O
the	O
queried	O
samples	O
in	O
each	O
iteration	O
.	O
This	O
is	O
enabled	O
by	O
two	O
key	O
techniques	O
in	O
SeqMix	O
:	O
(	O
1	O
)	O
First	O
,	O
in	O
each	O
iteration	O
,	O
it	O
searches	O
for	O
pairs	O
of	O
eligible	O
sequences	O
and	O
mixes	O
them	O
both	O
in	O
the	O
feature	O
space	O
and	O
the	O
label	O
space	O
.	O
(	O
2	O
)	O
Second	O
,	O
it	O
has	O
a	O
discriminator	O
to	O
judge	O
if	O
the	O
generated	O
sequence	O
is	O
plausible	O
or	O
not	O
.	O
The	O
discriminator	O
is	O
designed	O
to	O
compute	O
the	O
perplexity	B-MetricName
scores	O
for	O
all	O
the	O
generated	O
candidate	O
sequences	O
and	O
select	O
the	O
low	O
-	O
perplexity	B-MetricName
sequences	O
as	O
plausible	O
ones	O
.	O
We	O
show	O
that	O
SeqMix	O
consistently	O
outperforms	O
standard	O
active	O
sequence	O
labeling	O
baselines	O
under	O
different	O
data	O
usage	O
percentiles	O
with	O
experiments	O
on	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
and	O
Event	B-TaskName
Detection	I-TaskName
tasks	O
.	O
On	O
average	O
,	O
it	O
achieves	O
2.95	O
%	O
,	O
2.27	O
%	O
,	O
3.75	O
%	O
F	O
1	O
improvements	O
on	O
the	O
CoNLL	O
-	O
2003	O
,	O
ACE05	O
and	O
WebPage	O
datasets	O
.	O
The	O
advantage	O
of	O
SeqMix	O
is	O
especially	O
prominent	O
in	O
low	O
-	O
resource	O
scenarios	O
,	O
achieving	O
12.06	O
%	O
,	O
8.86	O
%	O
,	O
16.49	O
%	O
F	O
1	O
improvements	O
to	O
the	O
original	O
active	B-TaskName
learning	I-TaskName
approach	O
on	O
the	O
above	O
three	O
datasets	O
.	O
Our	O
results	O
also	O
verify	O
the	O
proposed	O
mixup	B-MethodName
strategies	O
and	O
the	O
discriminator	O
are	O
vital	O
to	O
the	O
performance	O
of	O
SeqMix	O
.	O

Many	O
NLP	O
problems	O
can	O
be	O
formulated	O
as	O
sequence	O
labeling	O
problems	O
.	O
Given	O
an	O
input	O
sequence	O
,	O
the	O
task	O
is	O
to	O
annotate	O
it	O
with	O
token	O
-	O
level	O
labels	O
.	O
The	O
labels	O
often	O
consist	O
of	O
a	O
position	O
prefix	O
provided	O
by	O
a	O
labeling	O
schema	O
and	O
a	O
type	O
indicator	O
provided	O
by	O
the	O
specific	O
task	O
.	O
For	O
example	O
,	O
in	O
the	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
task	O
,	O
we	O
can	O
adopt	O
the	O
BIO	O
(	O
Beginning	O
,	O
Inside	O
,	O
Outside	O
)	O
tagging	O
scheme	O
(	O
Màrquez	O
et	O
al	O
,	O
2005	O
)	O
to	O
assign	O
labels	O
for	O
each	O
token	O
:	O
the	O
first	O
token	O
of	O
an	O
entity	O
mention	O
with	O
type	O
X	O
is	O
labeled	O
as	O
B	O
-	O
X	O
,	O
the	O
tokens	O
inside	O
that	O
mention	O
are	O
labeled	O
as	O
I	O
-	O
X	O
and	O
the	O
non	O
-	O
entity	O
tokens	O
are	O
labeled	O
as	O
O.	O
Consider	O
a	O
large	O
unlabeled	O
corpus	O
U	O
,	O
traditional	O
active	B-TaskName
learning	I-TaskName
starts	O
from	O
a	O
small	O
annotated	O
seed	O
set	O
L	O
,	O
and	O
utilizes	O
a	O
query	O
function	O
ψ	O
(	O
U	O
,	O
K	O
,	O
γ	B-HyperparameterName
(	O
)	O
)	O
to	O
obtain	O
K	O
most	O
informative	O
unlabeled	O
samples	O
X	O
=	O
{	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
K	O
}	O
along	O
with	O
their	O
labels	O
Y	O
=	O
{	O
y	O
1	O
,	O
,	O
y	O
K	O
}	O
,	O
where	O
γ	B-HyperparameterName
(	O
)	O
is	O
the	O
query	O
policy	O
.	O
Then	O
,	O
we	O
remove	O
X	O
from	O
the	O
unlabeled	O
data	O
U	O
and	O
repeat	O
the	O
above	O
procedure	O
until	O
the	O
satisfactory	O
performance	O
achieved	O
or	O
the	O
annotation	O
capacity	O
reached	O
.	O
In	O
SeqMix	O
,	O
we	O
aim	O
to	O
further	O
exploit	O
the	O
annotated	O
set	O
X	O
,	O
Y	O
to	O
generate	O
augmented	O
data	O
X	O
*	O
,	O
Y	O
*	O
.	O
Then	O
the	O
labeled	O
dataset	O
is	O
expanded	O
as	O
L	O
=	O
L	O
∪	O
X	O
,	O
Y	O
∪	O
X	O
*	O
,	O
Y	O
*	O
.	O
Formally	O
,	O
we	O
define	O
our	O
task	O
as	O
:	O
(	O
1	O
)	O
construct	O
a	O
generator	O
φ	O
(	O
)	O
to	O
implement	O
sequence	O
and	O
label	O
generation	O
based	O
on	O
the	O
actively	O
sampled	O
data	O
X	O
and	O
its	O
label	O
Y	O
,	O
(	O
2	O
)	O
set	O
a	O
discriminator	O
d	O
(	O
)	O
to	O
yield	O
the	O
filtered	O
generation	O
,	O
then	O
(	O
3	O
)	O
augment	O
the	O
labeled	O
set	O
as	O
L	O
=	O
L	O
∪	O
X	O
,	O
Y	O
∪	O
d	O
(	O
φ	O
(	O
X	O
,	O
Y	O
)	O
)	O
.	O

Active	O
sequence	O
labeling	O
selects	O
K	O
most	O
informative	O
instances	O
ψ	O
(	O
,	O
K	O
,	O
γ	B-HyperparameterName
(	O
)	O
)	O
in	O
each	O
iteration	O
,	O
with	O
the	O
hope	O
of	O
maximally	O
improving	O
model	O
performance	O
with	O
a	O
fixed	O
labeled	O
budget	O
.	O
With	O
the	O
input	O
sequence	O
x	O
of	O
length	O
T	O
,	O
we	O
denote	O
the	O
model	O
output	O
as	O
f	O
(	O
|	O
x	O
;	O
θ	B-HyperparameterName
)	O
.	O
Our	O
method	O
is	O
generic	O
to	O
any	O
query	O
policies	O
γ	B-HyperparameterName
(	O
)	O
.	O
Below	O
,	O
we	O
introduce	O
several	O
representative	O
policies	O
.	O
Least	O
Confidence	O
(	O
LC	O
)	O
Culotta	O
and	O
McCallum	O
(	O
2005	O
)	O
measure	O
the	O
uncertainty	O
of	O
sequence	O
models	O
by	O
the	O
most	O
likely	O
predicted	O
sequence	O
.	O
For	O
a	O
CRF	B-MethodName
model	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
,	O
we	O
calculate	O
γ	B-HyperparameterName
with	O
the	O
predicted	O
sequential	O
label	O
y	O
*	O
as	O
γ	B-HyperparameterName
LC	O
(	O
x	O
)	O
=	O
1	O
−	O
max	O
y	O
*	O
(	O
P	O
(	O
y	O
*	O
|	O
x	O
;	O
θ	B-HyperparameterName
)	O
,	O
(	O
1	O
)	O
where	O
y	O
*	O
is	O
the	O
Viterbi	O
parse	O
.	O
For	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
with	O
a	O
token	B-TaskName
classification	I-TaskName
head	O
,	O
we	O
adopt	O
a	O
variant	O
of	O
the	O
least	O
confidence	O
measure	O
:	O
γ	B-HyperparameterName
LC	O
'	O
(	O
x	O
)	O
=	O
T	O
t=1	O
(	O
1	O
−	O
max	O
yt	O
P	O
(	O
y	O
t	O
|	O
x	O
;	O
θ	B-HyperparameterName
)	O
)	O
,	O
(	O
2	O
)	O
where	O
P	O
(	O
y	O
t	O
|	O
x	O
;	O
θ	B-HyperparameterName
)	O
=	O
softmax	B-MethodName
(	O
f	O
(	O
y	O
t	O
|	O
x	O
;	O
θ	B-HyperparameterName
)	O
)	O
.	O
Normalized	O
Token	O
Entropy	O
(	O
NTE	O
)	O
Another	O
uncertainty	O
measure	O
for	O
the	O
query	O
policy	O
is	O
normalized	O
entropy	O
(	O
Settles	O
and	O
Craven	O
,	O
2008	O
)	O
,	O
defined	O
as	O
:	O
Disagreement	O
Sampling	O
Query	O
-	O
by	O
-	O
committee	O
(	O
QBC	O
)	O
(	O
Seung	O
et	O
al	O
,	O
1992	O
)	O
,	O
is	O
another	O
approach	O
for	O
specifying	O
the	O
policy	O
,	O
where	O
the	O
unlabeled	O
data	O
can	O
be	O
sampled	O
by	O
the	O
disagreement	O
of	O
the	O
base	O
models	O
.	O
The	O
disagreement	O
can	O
be	O
defined	O
in	O
several	O
ways	O
,	O
here	O
we	O
take	O
the	O
vote	O
entropy	O
proposed	O
by	O
(	O
Dagan	O
and	O
Engelson	O
,	O
1995	O
)	O
.	O
Given	O
a	O
committee	O
consist	O
of	O
C	O
models	O
,	O
the	O
vote	O
entropy	O
for	O
input	O
x	O
is	O
:	O
γ	B-HyperparameterName
TE	O
(	O
x	O
)	O
=	O
−	O
1	O
T	O
T	O
t=1	O
M	O
m=1	O
P	O
m	O
(	O
y	O
t	O
|	O
x	O
,	O
θ	B-HyperparameterName
)	O
log	O
P	O
m	O
(	O
y	O
t	O
|	O
x	O
,	O
θ	B-HyperparameterName
)	O
,	O
(	O
3	O
)	O
γ	B-HyperparameterName
VE	O
(	O
x	O
)	O
=	O
−	O
1	O
T	O
T	O
t=1	O
M	O
m=1	O
V	O
m	O
(	O
y	O
t	O
)	O
C	O
log	O
V	O
m	O
(	O
y	O
t	O
)	O
C	O
,	O
(	O
4	O
)	O
where	O
V	O
m	O
(	O
y	O
t	O
)	O
is	O
the	O
number	O
of	O
models	O
that	O
predict	O
the	O
t	O
-	O
th	O
token	O
x	O
t	O
as	O
the	O
label	O
m.	O
3	O
The	O
SeqMix	O
Method	O

Given	O
a	O
corpus	O
for	O
sequence	O
labeling	O
,	O
we	O
assume	O
the	O
dataset	O
contains	O
a	O
small	O
labeled	O
set	O
L	O
and	O
a	O
large	O
unlabeled	O
set	O
U	O
initially	O
.	O
We	O
start	O
from	O
augmenting	O
the	O
seed	O
set	O
L	O
with	O
SeqMix	O
.	O
First	O
,	O
we	O
adopt	O
a	O
pairing	O
function	O
ζ	O
(	O
)	O
to	O
find	O
paired	O
samples	O
by	O
traversing	O
L.	O
Next	O
,	O
we	O
generate	O
mixed	O
-	O
labeled	O
sequences	O
via	O
latent	O
space	O
linear	O
interpolation	O
with	O
one	O
of	O
the	O
approaches	O
mentioned	O
in	O
Section	O
3.2	O
.	O
To	O
ensure	O
the	O
semantic	O
quality	O
of	O
the	O
generated	O
sequences	O
,	O
we	O
use	O
a	O
discriminator	O
After	O
that	O
,	O
the	O
iterative	O
active	B-TaskName
learning	I-TaskName
procedure	O
begins	O
.	O
In	O
each	O
iteration	O
,	O
we	O
actively	O
select	O
instances	O
from	O
U	O
with	O
a	O
query	O
policy	O
γ	B-HyperparameterName
(	O
)	O
(	O
Section	O
2.2	O
)	O
to	O
obtain	O
the	O
top	O
K	O
samples	O
X	O
=	O
ψ	O
(	O
U	O
,	O
K	O
,	O
γ	B-HyperparameterName
(	O
)	O
)	O
.	O
The	O
newly	O
selected	O
samples	O
will	O
be	O
labeled	O
with	O
Y	O
,	O
and	O
the	O
batch	O
of	O
samples	O
X	O
,	O
Y	O
will	O
be	O
used	O
for	O
SeqMix	O
.	O
Again	O
,	O
we	O
generate	O
L	O
*	O
=	O
SeqMix	O
(	O
X	O
,	O
Y	O
,	O
α	B-HyperparameterName
,	O
ζ	O
(	O
)	O
,	O
d	O
(	O
)	O
)	O
and	O
expand	O
the	O
training	O
set	O
as	O
L	O
=	O
L	O
∪	O
L	O
*	O
.	O
Then	O
we	O
train	O
the	O
model	O
θ	B-HyperparameterName
on	O
the	O
newly	O
augmented	O
set	O
L.	O
The	O
iterative	O
active	B-TaskName
learning	I-TaskName
procedure	O
terminates	O
when	O
a	O
fixed	O
number	B-HyperparameterName
of	I-HyperparameterName
iterations	I-HyperparameterName
are	O
reached	O
.	O
We	O
summarize	O
the	O
above	O
procedure	O
in	O
Algorithm	O
1	O
.	O

Mixup	B-MethodName
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
is	O
a	O
data	B-TaskName
augmentation	I-TaskName
method	O
that	O
implements	O
linear	O
interpolation	O
in	O
the	O
input	O
space	O
.	O
Given	O
two	O
input	O
samples	O
x	O
i	O
,	O
x	O
j	O
along	O
//	O
active	B-TaskName
learning	I-TaskName
iterations	O
with	O
augmentation	O
for	O
round	O
in	O
active	B-TaskName
learning	I-TaskName
rounds	O
do	O
X	O
=	O
ψ	O
(	O
U	O
,	O
K	O
,	O
γ	B-HyperparameterName
(	O
)	O
)	O
U	O
=	O
U	O
−	O
X	O
Annotate	O
X	O
to	O
get	O
X	O
,	O
Y	O
L	O
*	O
=	O
SeqMix	O
(	O
X	O
,	O
Y	O
,	O
α	B-HyperparameterName
,	O
ζ	O
(	O
)	O
,	O
d	O
(	O
)	O
)	O
L	O
=	O
L	O
∪	O
X	O
,	O
Y	O
∪	O
L	O
*	O
θ	B-HyperparameterName
=	O
train	O
(	O
θ	B-HyperparameterName
,	O
L	O
)	O
end	O
Output	O
:	O
The	O
sequence	O
model	O
trained	O
with	O
active	O
data	B-TaskName
augmentation	I-TaskName
:	O
θ	B-HyperparameterName
with	O
the	O
labels	O
y	O
i	O
,	O
y	O
j	O
,	O
the	O
mixing	O
process	O
is	O
:	O
x	O
=	O
λx	O
i	O
+	O
(	O
1	O
−	O
λ	O
)	O
x	O
j	O
,	O
(	O
5	O
)	O
y	O
=	O
λy	O
i	O
+	O
(	O
1	O
−	O
λ	O
)	O
y	O
j	O
,	O
(	O
6	O
)	O
where	O
λ	O
∼	O
Beta	O
(	O
α	B-HyperparameterName
,	O
α	B-HyperparameterName
)	O
is	O
the	O
mixing	O
coefficient	O
.	O
Through	O
linear	O
combinations	O
on	O
the	O
input	O
level	O
of	O
paired	O
examples	O
and	O
their	O
labels	O
,	O
Mixup	B-MethodName
regularizes	O
the	O
model	O
to	O
present	O
linear	O
behavior	O
among	O
the	O
training	O
data	O
.	O
Mixup	B-MethodName
is	O
not	O
directly	O
applicable	O
to	O
generate	O
interpolated	O
samples	O
for	O
text	O
data	O
,	O
because	O
the	O
input	O
space	O
is	O
discrete	O
.	O
To	O
overcome	O
this	O
,	O
SeqMix	O
performs	O
token	O
-	O
level	O
interpolation	O
in	O
the	O
embedding	O
space	O
and	O
selects	O
a	O
token	O
closest	O
to	O
the	O
interpolated	O
embedding	O
.	O
Specifically	O
,	O
SeqMix	O
constructs	O
a	O
table	O
of	O
tokens	O
W	O
and	O
their	O
corresponding	O
contextual	O
embeddings	O
E	O
1	O
.	O
Given	O
two	O
sequences	O
x	O
i	O
=	O
{	O
w	O
1	O
i	O
,	O
,	O
w	O
T	O
i	O
}	O
and	O
x	O
j	O
=	O
{	O
w	O
1	O
j	O
,	O
,	O
w	O
T	O
j	O
}	O
with	O
their	O
embedding	O
representations	O
e	O
x	O
i	O
=	O
{	O
e	O
1	O
i	O
,	O
,	O
e	O
T	O
i	O
}	O
and	O
e	O
x	O
j	O
=	O
{	O
e	O
1	O
j	O
,	O
,	O
e	O
T	O
j	O
}	O
,	O
the	O
t	O
-	O
th	O
mixed	O
token	O
is	O
the	O
token	O
whose	O
embedding	O
e	O
t	O
is	O
closest	O
to	O
the	O
mixed	O
embedding	O
:	O
e	O
t	O
=	O
arg	O
min	O
e	O
E	O
e	O
−	O
(	O
λe	O
t	O
i	O
+	O
(	O
1	O
−	O
λ	O
)	O
e	O
t	O
j	O
)	O
2	O
.	O
(	O
7	O
)	O
To	O
get	O
the	O
corresponding	O
w	O
t	O
,	O
we	O
can	O
query	O
the	O
table	O
{	O
W	O
,	O
E	O
}	O
using	O
e	O
t	O
.	O
The	O
label	O
generation	O
is	O
straightforward	O
.	O
For	O
two	O
label	O
sequences	O
y	O
i	O
=	O
{	O
y	O
1	O
i	O
,	O
,	O
y	O
T	O
i	O
}	O
and	O
y	O
j	O
=	O
{	O
y	O
1	O
j	O
,	O
,	O
y	O
T	O
j	O
}	O
,	O
we	O
get	O
the	O
t	O
-	O
th	O
mixed	O
label	O
as	O
:	O
y	O
t	O
=	O
λy	O
t	O
i	O
+	O
(	O
1	O
−	O
λ	O
)	O
y	O
t	O
j	O
,	O
(	O
8	O
)	O
where	O
y	O
t	O
i	O
and	O
y	O
t	O
j	O
are	O
one	O
-	O
hot	O
encoded	O
labels	O
.	O
Along	O
with	O
the	O
above	O
sequence	O
mixup	B-MethodName
procedure	O
,	O
we	O
also	O
introduce	O
a	O
pairing	O
strategy	O
that	O
selects	O
sequences	O
for	O
mixup	B-MethodName
.	O
The	O
reason	O
is	O
that	O
,	O
in	O
many	O
sequence	O
labeling	O
tasks	O
,	O
the	O
labels	O
of	O
interest	O
are	O
scarce	O
.	O
For	O
example	O
,	O
in	O
the	O
NER	B-TaskName
and	O
event	B-TaskName
detection	I-TaskName
tasks	O
,	O
the	O
"	O
O	O
"	O
label	O
is	O
dominant	O
in	O
the	O
corpus	O
,	O
which	O
do	O
not	O
refer	O
to	O
any	O
entities	O
or	O
events	O
of	O
interest	O
.	O
We	O
thus	O
define	O
the	O
labels	O
of	O
interest	O
as	O
valid	O
labels	O
,	O
e.g.	O
,	O
the	O
non	O
-	O
"	O
O	O
"	O
labels	O
in	O
NER	B-TaskName
and	O
event	B-TaskName
detection	I-TaskName
,	O
and	O
design	O
a	O
sequence	O
pairing	O
function	O
to	O
select	O
more	O
informative	O
parent	O
sequences	O
for	O
mixup	B-MethodName
.	O
Specifically	O
,	O
the	O
sequence	O
pairing	O
function	O
ζ	O
(	O
)	O
is	O
designed	O
according	O
to	O
valid	O
label	O
density	O
.	O
For	O
a	O
sequence	O
,	O
its	O
valid	O
label	O
density	O
is	O
defined	O
as	O
η	O
=	O
n	O
s	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
valid	O
labels	O
and	O
s	O
is	O
the	O
length	O
of	O
the	O
sub	O
-	O
sequence	O
.	O
We	O
set	O
a	O
threshold	O
η	O
0	B-DatasetName
for	O
ζ	O
(	O
)	O
,	O
and	O
the	O
sequence	O
will	O
be	O
considered	O
as	O
an	O
eligible	O
candidate	O
for	O
mixup	B-MethodName
only	O
when	O
η	O
≥	O
η	O
0	B-DatasetName
.	O
Based	O
on	O
the	O
above	O
token	O
-	O
level	O
mixup	B-MethodName
procedure	O
and	O
the	O
sequence	O
pairing	O
function	O
,	O
we	O
propose	O
three	O
different	O
strategies	O
for	O
generating	O
interpolated	O
labeled	O
sequences	O
.	O
These	O
strategies	O
are	O
shown	O
in	O
Figure	O
1	O
and	O
described	O
below	O
:	O
Whole	O
-	O
sequence	O
mixup	B-MethodName
As	O
the	O
name	O
suggests	O
,	O
whole	O
-	O
sequence	O
mixup	B-MethodName
(	O
Figure	O
1	O
(	O
a	O
)	O
)	O
performs	O
sequence	O
mixing	O
at	O
the	O
whole	O
-	O
sequence	O
level	O
.	O
Given	O
two	O
sequences	O
x	O
i	O
,	O
y	O
i	O
,	O
x	O
j	O
,	O
y	O
j	O
L	O
,	O
they	O
must	O
share	O
the	O
same	O
length	O
without	O
counting	O
padding	O
words	O
.	O
Besides	O
,	O
the	O
paring	O
function	O
ζ	O
(	O
)	O
requires	O
that	O
both	O
the	O
two	O
sequences	O
satisfy	O
η	O
≥	O
η	O
0	B-DatasetName
.	O
Then	O
we	O
perform	O
mixup	B-MethodName
at	O
all	O
token	O
positions	O
,	O
by	O
employing	O
Equation	O
7to	O
generate	O
mixed	O
tokens	O
and	O
Equation	O
8	O
to	O
generate	O
mixed	O
labels	O
(	O
note	O
that	O
the	O
mixed	O
labels	O
are	O
soft	O
labels	O
)	O
.	O
Sub	O
-	O
sequence	O
mixup	B-MethodName
One	O
drawback	O
of	O
the	O
whole	O
-	O
sequence	O
mixup	B-MethodName
is	O
that	O
it	O
indiscriminately	O
mixes	O
over	O
all	O
tokens	O
,	O
which	O
may	O
include	O
incompatible	O
subsequences	O
and	O
generate	O
implausible	O
sequences	O
.	O
To	O
tackle	O
this	O
,	O
we	O
consider	O
sub	O
-	O
sequence	O
mixup	B-MethodName
(	O
Figure	O
1	O
x	O
i	O
,	O
y	O
i	O
,	O
x	O
j	O
,	O
y	O
j	O
,	O
(	O
i	O
=	O
j	O
)	O
in	O
L	O
do	O
if	O
ζ	O
(	O
x	O
i	O
,	O
y	O
i	O
,	O
x	O
j	O
,	O
y	O
j	O
)	O
then	O
λ	O
∼	O
Beta	O
(	O
α	B-HyperparameterName
,	O
α	B-HyperparameterName
)	O
//	O
mixup	B-MethodName
the	O
target	O
sub	O
-	O
sequences	O
for	O
t	O
=	O
1	O
,	O
,	O
T	O
do	O
Calculate	O
e	O
t	O
by	O
Eq	O
.	O
(	O
7	O
)	O
;	O
Get	O
corresponding	O
token	O
w	O
t	O
for	O
e	O
t	O
;	O
Calculate	O
y	O
t	O
by	O
Eq	O
.	O
(	O
8	O
)	O
.	O
end	O
x	O
sub	O
=	O
{	O
w	O
1	O
,	O
,	O
w	O
T	O
}	O
y	O
sub	O
=	O
{	O
y	O
1	O
,	O
,	O
y	O
T	O
}	O
//	O
replace	O
the	O
original	O
sequences	O
for	O
k	O
in	O
{	O
i	O
,	O
j	O
}	O
dõ	O
x	O
k	B-HyperparameterName
=	I-HyperparameterName
x	O
k	O
−	O
x	O
ksub	O
+	O
x	O
sub	O
y	O
k	B-HyperparameterName
=	I-HyperparameterName
y	O
k	O
−	O
y	O
ksub	O
+	O
ỹ	O
sub	O
if	O
d	O
(	O
x	O
k	O
)	O
then	O
L	O
*	O
=	O
L	O
*	O
∪	O
x	O
k	O
,	O
ỹ	O
k	O
end	O
if	O
|	O
L	O
*	O
|	O
≥	O
N	O
x	O
i	O
,	O
y	O
i	O
,	O
x	O
j	O
,	O
y	O
j	O
as	O
X	O
isub	O
=	O
x	O
1	O
isub	O
,	O
.	O
.	O
.	O
,	O
x	O
s	O
isub	O
,	O
X	O
jsub	O
=	O
x	O
1	O
jsub	O
,	O
.	O
.	O
.	O
,	O
x	O
s	O
jsub	O
.	O
If	O
x	O
isub	O
X	O
isub	O
,	O
x	O
jsub	O
X	O
jsub	O
,	O
such	O
that	O
their	O
η	O
≥	O
η	O
0	B-DatasetName
,	O
we	O
have	O
ζ	O
(	O
x	O
i	O
,	O
y	O
i	O
,	O
x	O
j	O
,	O
y	O
j	O
)	O
=	O
True	O
.	O
Then	O
the	O
subsequences	O
x	O
isub	O
and	O
x	O
jsub	O
are	O
mixed	O
as	O
Figure	O
1	O
(	O
b	O
)	O
.	O
The	O
mixed	O
sub	O
-	O
sequence	O
and	O
labels	O
will	O
replace	O
the	O
original	O
parts	O
of	O
the	O
parent	O
samples	O
,	O
and	O
the	O
other	O
parts	O
of	O
the	O
parent	O
samples	O
remain	O
unchanged	O
.	O
In	O
this	O
way	O
,	O
sub	O
-	O
sequence	O
mixup	B-MethodName
is	O
expected	O
to	O
keep	O
the	O
syntax	O
structure	O
of	O
the	O
original	O
sequence	O
,	O
while	O
providing	O
data	O
diversity	O
.	O
Label	O
-	O
constrained	O
sub	O
-	O
sequence	O
mixup	B-MethodName
can	O
be	O
considered	O
as	O
a	O
special	O
case	O
of	O
sub	O
-	O
sequence	O
mixup	B-MethodName
,	O
where	O
the	O
constraints	O
inherit	O
sub	O
-	O
sequence	O
mixup	B-MethodName
,	O
and	O
further	O
require	O
that	O
the	O
sub	O
-	O
sequence	O
labels	O
are	O
consistent	O
.	O
As	O
Figure	O
1	O
(	O
c	O
)	O
shows	O
,	O
after	O
mixing	O
such	O
paired	O
samples	O
,	O
the	O
generation	O
will	O
just	O
update	O
the	O
tokens	O
of	O
the	O
sub	O
-	O
sequences	O
while	O
keeping	O
the	O
labels	O
the	O
same	O
as	O
before	O
.	O
Hence	O
,	O
this	O
Generated	O
Sequence	O
Generated	O
Sequence	O
4	O
5	O
4	O
5	O
1	O
2	O
3	O
1	O
2	O
3	O
1	O
2	O
3	O
1	O
2	O
3	O

During	O
sequence	O
mixup	B-MethodName
,	O
the	O
mixing	O
coefficient	O
λ	O
determines	O
the	O
strength	O
of	O
interpolation	O
.	O
When	O
λ	O
approximates	O
0	B-DatasetName
or	O
1	O
,	O
the	O
generated	O
sequence	O
will	O
be	O
similar	O
to	O
one	O
of	O
the	O
parent	O
sequences	O
,	O
while	O
the	O
λ	O
around	O
0.5	O
produces	O
relatively	O
diverse	O
generation	O
.	O
However	O
,	O
generating	O
diverse	O
sequences	O
means	O
lowquality	O
sequences	O
can	O
be	O
generated	O
,	O
which	O
can	O
provide	O
noisy	O
contextual	O
information	O
and	O
hurt	O
model	O
performance	O
.	O
To	O
maintain	O
the	O
quality	O
of	O
mixed	O
sequences	O
,	O
we	O
set	O
a	O
discriminator	O
to	O
score	O
the	O
perplexity	B-MetricName
of	O
the	O
sequences	O
.	O
The	O
final	O
generated	O
sequences	O
will	O
consist	O
of	O
only	O
the	O
sequences	O
that	O
pass	O
the	O
sequence	O
quality	O
screening	O
.	O
For	O
the	O
screening	O
,	O
we	O
utilize	O
a	O
language	O
model	O
GPT	B-MethodName
-	O
2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
to	O
score	O
sequence	O
x	O
by	O
computing	O
its	O
perplexity	B-MetricName
:	O
Perplexity	B-MetricName
(	O
x	O
)	O
=	O
2	O
−	O
1	O
T	O
T	O
i=1	O
log	O
p	O
(	O
w	O
i	O
)	O
,	O
(	O
9	O
)	O
where	O
T	O
is	O
the	O
number	O
of	O
tokens	O
before	O
padding	O
,	O
w	O
i	O
is	O
the	O
i	O
-	O
th	O
token	O
of	O
sequence	O
x.	O
Based	O
on	O
the	O
perplexity	B-MetricName
and	O
a	O
score	O
range	O
[	O
s	O
1	O
,	O
s	O
2	O
]	O
,	O
the	O
discriminator	O
can	O
give	O
judgment	O
for	O
sequence	O
x	O
:	O
d	O
(	O
x	O
)	O
=	O
1	O
{	O
s	O
1	O
≤	O
Perplexity	B-MetricName
(	O
x	O
)	O
≤	O
s	O
2	O
}	O
.	O
(	O
10	O
)	O
The	O
lower	O
the	O
perplexity	B-MetricName
score	O
,	O
the	O
more	O
natural	O
the	O
sequence	O
.	O
However	O
,	O
the	O
discriminator	O
should	O
also	O
consider	O
the	O
regularization	O
effectiveness	O
and	O
the	O
generation	O
capacity	O
.	O
Hence	O
,	O
a	O
blind	O
low	O
perplexity	B-MetricName
setting	O
is	O
undesirable	O
.	O
The	O
overall	O
sequence	O
mixup	B-MethodName
and	O
selection	O
procedure	O
is	O
illustrated	O
in	O
Algorithm	O
2	O
.	O

Datasets	O
.	O
We	O
conduct	O
experiments	O
on	O
three	O
sequence	O
labeling	O
datasets	O
for	O
the	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
and	O
event	B-TaskName
detection	I-TaskName
tasks	O
.	O
(	O
1	O
)	O
CoNLL	O
-	O
03	O
(	O
Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder	O
,	O
2003	O
)	O
is	O
a	O
corpus	O
for	O
NER	B-TaskName
task	O
.	O
It	O
provides	O
four	O
named	O
entity	O
types	O
:	O
persons	O
,	O
locations	O
,	O
organizations	O
,	O
and	O
miscellaneous	O
.	O
2	O
(	O
2	O
)	O
ACE05	O
is	O
a	O
corpus	O
for	O
event	B-TaskName
detection	I-TaskName
.	O
It	O
provides	O
8	O
event	O
types	O
and	O
33	O
subtypes	O
.	O
We	O
study	O
the	O
event	O
trigger	O
detection	O
problem	O
,	O
which	O
aims	O
to	O
identify	O
trigger	O
tokens	O
in	O
a	O
sentence	O
.	O
(	O
3	O
)	O
Webpage	O
(	O
Ratinov	O
and	O
Roth	O
,	O
2009	O
)	O
is	O
a	O
NER	B-TaskName
corpus	O
with	O
20	O
webpages	O
related	O
to	O
computer	O
science	O
conference	O
and	O
academic	O
websites	O
.	O
It	O
inherits	O
the	O
entity	O
types	O
from	O
CoNLL	O
-	O
03	O
.	O
Data	O
Split	O
.	O
To	O
investigate	O
low	O
-	O
resource	O
sequence	O
labeling	O
,	O
we	O
randomly	O
take	O
700	O
labeled	O
sentences	O
from	O
the	O
original	O
CoNLL	O
-	O
03	O
dataset	O
as	O
the	O
training	O
set	O
.	O
For	O
ACE05	O
and	O
WebPage	O
dataset	O
,	O
the	O
annotation	O
is	O
sparse	O
,	O
so	O
we	O
conduct	O
experiments	O
on	O
their	O
original	O
dataset	O
without	O
further	O
slicing	O
.	O
We	O
set	O
6	O
data	O
usage	O
percentiles	O
for	O
the	O
training	O
set	O
in	O
each	O
corpus	O
.	O
The	O
sequence	O
model	O
is	O
initialed	O
on	O
a	O
small	O
seed	O
set	O
,	O
then	O
it	O
performs	O
five	O
iterates	O
of	O
active	B-TaskName
learning	I-TaskName
.	O
For	O
the	O
query	O
policy	O
,	O
we	O
use	O
random	O
sampling	O
and	O
the	O
three	O
active	B-TaskName
learning	I-TaskName
policies	O
mentioned	O
in	O
Section	O
2.2	O
.	O
The	O
machine	O
learning	O
performance	O
is	O
evaluated	O
by	O
F	O
1	O
score	O
for	O
each	O
data	O
usage	O
percentile	O
.	O
Parameters	O
.	O
We	O
use	O
BERT	B-MethodName
-	O
base	O
-	O
cased	O
for	O
the	O
NER	B-TaskName
task	O
as	O
the	O
underlying	O
model	O
,	O
and	O
BERT	B-MethodName
-	O
basemultilingual	O
-	O
cased	O
for	O
the	O
event	O
trigger	O
detection	O
task	O
.	O
We	O
set	O
the	O
max	O
length	O
as	O
128	O
to	O
pad	O
the	O
varying	O
-	O
length	O
sequences	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
the	O
underlying	O
model	O
is	O
5e	O
-	O
5	O
,	O
and	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
32	O
.	O
We	O
train	O
them	O
for	O
10	O
epochs	O
at	O
each	O
data	O
usage	O
percentile	O
.	O
For	O
the	O
parameters	O
of	O
SeqMix	O
,	O
we	O
set	O
α	B-HyperparameterName
=	O
8	O
to	O
sample	O
λ	O
from	O
Beta	O
(	O
α	B-HyperparameterName
,	O
α	B-HyperparameterName
)	O
.	O
We	O
use	O
the	O
sub	O
-	O
sequence	O
window	O
length	O
s	O
=	O
{	O
5	O
,	O
5	O
,	O
4	O
}	O
,	O
the	O
valid	O
label	O
density	O
η	O
0	B-DatasetName
=	O
{	O
0.6	O
,	O
0.2	O
,	O
0.5	O
}	O
for	O
CoNLL	O
-	O
03	O
,	O
ACE05	O
and	O
Webpage	O
,	O
respectively	O
.	O
The	O
augment	O
rate	O
is	O
set	O
as	O
0.2	O
,	O
and	O
the	O
discriminator	O
score	O
range	O
is	O
set	O
as	O
(	O
0	B-DatasetName
,	O
500	O
)	O
.	O
We	O
also	O
perform	O
a	O
detailed	O
parameter	O
study	O
in	O
Section	O
4.4	O
.	O

The	O
main	O
results	O
are	O
presented	O
in	O
Figure	O
2	O
,	O
where	O
we	O
use	O
NTE	O
sampling	O
as	O
the	O
default	O
active	B-TaskName
learning	I-TaskName
policy	O
.	O
From	O
the	O
result	O
,	O
it	O
is	O
clear	O
that	O
our	O
method	O
achieves	O
the	O
best	O
performance	O
consistently	O
at	O
each	O
data	O
usage	O
percentile	O
for	O
all	O
three	O
datasets	O
.	O
The	O
best	O
SeqMix	O
method	O
(	O
sub	O
-	O
sequence	O
mixup	B-MethodName
with	O
NTE	O
sampling	O
)	O
outperforms	O
the	O
strongest	O
active	B-TaskName
learning	I-TaskName
baselines	O
by	O
2.95	O
%	O
on	O
CoNLL	O
-	O
03	O
,	O
2.27	O
%	O
on	O
ACE05	O
and	O
3.75	O
%	O
on	O
WebPage	O
in	O
terms	O
of	O
F	O
1	O
score	O
on	O
average	O
.	O
Moreover	O
,	O
the	O
augmentation	O
advantage	O
is	O
especially	O
prominent	O
for	O
the	O
seed	O
set	O
initialization	O
stage	O
where	O
we	O
only	O
have	O
a	O
very	O
limited	O
number	O
of	O
labeled	O
data	O
.	O
Through	O
the	O
augmentation	O
,	O
we	O
improve	O
the	O
model	O
performance	O
from	O
68.65	O
%	O
to	O
80.71	O
%	O
,	O
where	O
the	O
seed	O
set	O
is	O
200	O
labeled	O
sequences	O
and	O
the	O
augmentation	O
provides	O
extra	O
40	O
data	O
points	O
for	O
CoNLL	O
-	O
03	O
.	O
The	O
improvement	O
is	O
also	O
significant	O
on	O
ACE05	O
(	O
40.65	O
%	O
to	O
49.51	O
%	O
)	O
,	O
and	O
WebPage	O
(	O
55.18	O
%	O
to	O
71.67	O
%	O
)	O
,	O
which	O
indicates	O
that	O
our	O
SeqMix	O
can	O
largely	O
resolve	O
the	O
label	O
scarcity	O
issue	O
in	O
low	O
-	O
resource	O
scenarios	O
.	O
We	O
also	O
perform	O
statistical	O
significance	O
tests	O
for	O
the	O
above	O
results	O
.	O
We	O
use	O
Wilcoxon	O
Signed	O
Rank	O
Test	O
(	O
Wilcoxon	O
,	O
1992	O
)	O
,	O
a	O
non	O
-	O
parametric	O
alternative	O
to	O
the	O
paired	O
t	O
-	O
test	O
.	O
This	O
significance	O
test	O
fits	O
our	O
task	O
as	O
F	O
-	O
score	O
is	O
generally	O
assumed	O
to	O
be	O
not	O
normally	O
distributed	O
(	O
Dror	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
nonparametric	O
significance	O
tests	O
should	O
be	O
used	O
in	O
such	O
a	O
case	O
.	O
The	O
results	O
show	O
that	O
sub	O
-	O
sequence	O
mixup	B-MethodName
and	O
label	O
-	O
constrained	O
sub	O
-	O
sequence	O
mixup	B-MethodName
can	O
provide	O
a	O
statistical	O
significance	O
(	O
the	O
confidence	O
level	O
α	B-HyperparameterName
=	O
0.05	O
and	O
the	O
number	O
of	O
data	O
points	O
N	O
=	O
6	O
)	O
for	O
all	O
the	O
comparisons	O
with	O
active	B-TaskName
learning	I-TaskName
baselines	O
on	O
used	O
datasets	O
.	O
The	O
whole	O
-	O
sequence	O
mixup	B-MethodName
passes	O
the	O
statistical	O
significance	O
test	O
with	O
α	B-HyperparameterName
=	O
0.1	O
and	O
N	O
=	O
6	O
on	O
CoNLL	O
-	O
03	O
and	O
WebPage	O
,	O
but	O
fails	O
on	O
ACE05	O
.	O
Among	O
all	O
the	O
three	O
SeqMix	O
variants	O
,	O
subsequence	O
mixup	B-MethodName
gives	O
the	O
overall	O
best	O
performance	O
(	O
label	O
-	O
constrained	O
sub	O
-	O
sequence	O
mixup	B-MethodName
achieves	O
very	O
close	O
performance	O
with	O
sub	O
-	O
sequence	O
mixup	B-MethodName
on	O
ACE05	O
dataset	O
)	O
,	O
but	O
whole	O
-	O
sequence	O
mixup	B-MethodName
does	O
not	O
yield	O
a	O
consistent	O
improvement	O
to	O
the	O
original	O
active	B-TaskName
learning	I-TaskName
method	O
.	O
This	O
is	O
because	O
the	O
whole	O
-	O
sequence	O
mixup	B-MethodName
may	O
generate	O
semantically	O
poor	O
new	O
sequences	O
.	O
Instead	O
,	O
the	O
sub	O
-	O
sequencelevel	O
process	O
reserves	O
the	O
original	O
context	O
information	O
between	O
the	O
sub	O
-	O
sequence	O
and	O
the	O
other	O
parts	O
of	O
the	O
whole	O
sequence	O
.	O
Meanwhile	O
,	O
the	O
updated	O
sub	O
-	O
sequences	O
inherit	O
the	O
original	O
local	O
informativeness	O
,	O
and	O
introduce	O
linguistic	O
diversity	O
to	O
enhance	O
the	O
model	O
's	O
generalization	O
ability	O
.	O
To	O
justify	O
that	O
SeqMix	O
can	O
provide	O
improvement	O
to	O
the	O
active	B-TaskName
learning	I-TaskName
framework	O
with	O
various	O
query	O
policies	O
,	O
we	O
employ	O
different	O
query	O
policies	O
with	O
SeqMix	O
augmentation	O
under	O
the	O
same	O
experiment	O
setting	O
as	O
Figure	O
2	O
(	O
a	O
)	O
.	O
From	O
Figure	O
3	O
,	O
we	O
find	O
that	O
there	O
is	O
a	O
consistent	O
performance	O
improvement	O
when	O
employing	O
SeqMix	O
with	O
different	O
query	O
policies	O
.	O
As	O
SeqMix	O
achieves	O
{	O
2.46	O
%	O
,	O
2.85	O
%	O
,	O
2.94	O
%	O
}	O
performance	O
gain	O
for	O
random	O
sampling	O
,	O
LC	O
sampling	O
and	O
NTE	O
sampling	O
respectively	O
.	O

To	O
verify	O
the	O
effectiveness	O
of	O
the	O
discriminator	O
,	O
we	O
conduct	O
the	O
ablation	O
study	O
on	O
a	O
subset	O
of	O
CoNLL	O
-	O
03	O
with	O
700	O
labeled	O
sequences	O
.	O
We	O
use	O
subsequence	O
mixup	B-MethodName
with	O
NTE	O
sampling	O
as	O
the	O
backbone	O
and	O
change	O
the	O
perplexity	B-MetricName
score	O
range	O
of	O
the	O
discriminator	O
.	O
We	O
start	O
from	O
the	O
seed	O
set	O
with	O
200	O
labeled	O
data	O
,	O
then	O
actively	O
query	O
100	O
data	O
in	O
each	O
learning	O
round	O
and	O
repeat	O
5	O
rounds	O
in	O
total	O
.	O
The	O
result	O
in	O
Table	O
1	O
demonstrates	O
the	O
discriminator	O
provides	O
a	O
stable	O
improvement	O
for	O
the	O
last	O
four	O
data	O
usage	O
percentiles	O
,	O
and	O
the	O
discriminator	O
with	O
score	O
range	O
(	O
0	B-DatasetName
,	O
500	O
)	O
can	O
boost	O
the	O
model	O
by	O
1.07	O
%	O
F	O
1	O
score	O
,	O
averaged	O
by	O
all	O
the	O
data	O
usage	O
percentiles	O
.	O
The	O
comparison	O
between	O
3	O
different	O
score	O
thresholds	O
demonstrates	O
the	O
lower	O
the	O
perplexity	B-MetricName
,	O
the	O
better	O
the	O
generation	O
quality	O
.	O
As	O
a	O
result	O
,	O
the	O
final	O
F	O
1	O
score	O
becomes	O
higher	O
with	O
the	O
better	O
generated	O
tokens	O
.	O
Actually	O
,	O
we	O
can	O
further	O
narrow	O
down	O
the	O
score	O
range	O
to	O
get	O
more	O
performance	O
improvement	O
in	O
return	O
,	O
but	O
the	O
too	O
strict	O
constraints	O
will	O
slow	O
down	O
the	O
generation	O
in	O
practice	O
and	O
reduce	O
the	O
number	O
of	O
generated	O
samples	O
.	O

In	O
this	O
subsection	O
,	O
we	O
study	O
the	O
effect	O
of	O
several	O
key	O
parameters	O
.	O
Augment	O
rate	O
r.	O
We	O
vary	O
the	O
augment	O
rate	O
r	O
=	O
|	O
L	O
*	O
|	O
|	O
ψ	O
(	O
U	O
,	O
K	O
,	O
γ	B-HyperparameterName
(	O
)	O
)	O
|	O
in	O
{	O
0	B-DatasetName
.2	O
,	O
0.4	O
,	O
0.6	O
,	O
0.8	O
,	O
1.0	O
}	O
and	O
keep	O
the	O
number	O
of	O
initial	O
data	O
usage	O
same	O
to	O
investigate	O
the	O
effect	O
of	O
augment	O
rate	O
for	O
data	B-TaskName
augmentation	I-TaskName
.	O
Table	O
2	O
shows	O
that	O
r	O
≤	O
0.6	O
can	O
provide	O
better	O
F	O
1	O
improvement	O
.	O
The	O
model	O
with	O
r	O
=	O
0.2	O
surpasses	O
the	O
model	O
with	O
r	O
=	O
1.0	O
by	O
0.73	O
%	O
,	O
evaluated	O
by	O
the	O
average	O
F	O
1	O
score	O
for	O
all	O
the	O
data	O
usage	O
percentiles	O
.	O
This	O
result	O
indicates	O
that	O
the	O
model	O
appreciates	O
moderate	O
augmentation	O
more	O
.	O
However	O
,	O
the	O
performance	O
variance	O
based	O
on	O
the	O
augment	O
rate	O
is	O
not	O
prominent	O
compared	O
to	O
the	O
improvement	O
provided	O
by	O
SeqMix	O
to	O
the	O
active	B-TaskName
learning	I-TaskName
framework	O
.	O
Valid	O
tag	O
density	O
η	O
0	B-DatasetName
.	O
We	O
search	O
the	O
valid	O
tag	O
density	O
η	O
0	B-DatasetName
as	O
Section	O
3.2	O
defined	O
by	O
varying	O
the	O
sub	O
-	O
sequence	O
window	O
length	O
s	O
and	O
the	O
required	O
number	O
of	O
valid	O
tag	O
n	O
within	O
the	O
window	O
.	O
The	O
results	O
in	O
Figure	O
4	O
(	O
a	O
)	O
illustrate	O
the	O
combination	O
(	O
s	O
=	O
5	O
,	O
n	O
=	O
3	O
)	O
outperforms	O
other	O
settings	O
.	O
When	O
s	O
is	O
too	O
small	O
,	O
the	O
window	O
usually	O
truncates	O
the	O
continuous	O
clause	O
,	O
thus	O
cutting	O
off	O
the	O
local	O
syntax	O
or	O
semantic	O
information	O
.	O
When	O
s	O
is	O
too	O
large	O
,	O
sub	O
-	O
sequence	O
mixup	B-MethodName
tends	O
to	O
behave	O
like	O
wholesequence	O
mixup	B-MethodName
,	O
where	O
the	O
too	O
long	O
sub	O
-	O
sequence	O
generation	O
can	O
hardly	O
maintain	O
the	O
rationality	O
of	O
syntax	O
and	O
semantics	O
as	O
before	O
.	O
The	O
high	O
η	O
0	B-DatasetName
with	O
long	O
window	O
length	O
may	O
result	O
in	O
an	O
insufficient	O
amount	O
of	O
eligible	O
parent	O
sequences	O
.	O
Actually	O
,	O
even	O
with	O
a	O
moderate	O
augment	O
rate	O
α	B-HyperparameterName
=	O
0.2	O
,	O
the	O
combination	O
(	O
s	O
=	O
6	O
,	O
n	O
=	O
5	O
)	O
has	O
been	O
unable	O
to	O
provide	O
enough	O
generation	O
.	O
Mixing	O
parameter	O
α	B-HyperparameterName
.	O
We	O
show	O
the	O
performance	O
with	O
different	O
α	B-HyperparameterName
in	O
Figure	O
4	O
(	O
b	O
)	O
.	O
The	O
parameter	O
α	B-HyperparameterName
decides	O
the	O
distribution	O
λ	O
∼	O
Beta	O
(	O
α	B-HyperparameterName
,	O
α	B-HyperparameterName
)	O
,	O
and	O
the	O
coefficient	O
λ	O
directly	O
involved	O
the	O
mixing	O
of	O
tokens	O
and	O
labels	O
.	O
Among	O
the	O
values	O
{	O
0.5	O
,	O
1	O
,	O
2	O
,	O
4	O
,	O
8	O
,	O
16	O
}	O
,	O
we	O
observed	O
α	B-HyperparameterName
=	O
8	O
presents	O
the	O
best	O
performance	O
.	O
It	O
outperforms	O
the	O
second	O
-	O
best	O
parameter	O
setting	O
0.49	O
%	O
by	O
average	O
.	O
From	O
the	O
perspective	O
of	O
Beta	O
distribution	O
,	O
larger	O
α	B-HyperparameterName
will	O
make	O
the	O
sampled	O
λ	O
more	O
concentrated	O
around	O
0.5	O
,	O
which	O
assigns	O
more	O
balance	O
weights	O
to	O
the	O
parent	O
samples	O
to	O
be	O
mixed	O
.	O
In	O
this	O
way	O
,	O
the	O
interpolation	O
produces	O
encoded	O
token	O
with	O
further	O
distance	O
to	O
both	O
the	O
parent	O
samples	O
,	O
thus	O
introduces	O
a	O
more	O
diverse	O
generation	O
.	O

Figure	O
5	O
presents	O
a	O
generation	O
example	O
via	O
subsequence	O
mixup	B-MethodName
.	O
For	O
the	O
convenience	O
of	O
presentation	O
,	O
we	O
set	O
the	O
length	O
of	O
sub	O
-	O
sequence	O
s	O
=	O
3	O
and	O
the	O
valid	O
label	O
density	O
threshold	O
η	O
0	B-DatasetName
=	O
2	O
3	O
.	O
The	O
two	O
input	O
sequences	O
get	O
paired	O
for	O
their	O
eligible	O
sub	O
-	O
sequences	O
"	O
COLORADO	O
10	O
St	O
"	O
and	O
"	O
Slovenia	O
,	O
Kwasniewski	O
"	O
.	O
The	O
subsequences	O
are	O
mixed	O
by	O
λ	O
=	O
0.39	O
in	O
this	O
case	O
,	O
which	O
is	O
sampled	O
from	O
Beta	O
(	O
α	B-HyperparameterName
,	O
α	B-HyperparameterName
)	O
.	O
Then	O
the	O
generated	O
sub	O
-	O
sequence	O
"	O
Ohio	O
(	O
novelist	O
"	O
replaces	O
the	O
original	O
parts	O
in	O
the	O
two	O
input	O
sequences	O
.	O
Among	O
the	O
generated	O
tokens	O
,	O
"	O
Ohio	O
"	O
inherits	O
the	O
label	O
B	O
-	O
ORG	O
from	O
"	O
COLORADO	O
"	O
and	O
the	O
label	O
B	O
-	O
LOC	O
from	O
"	O
Slovenia	O
"	O
,	O
and	O
the	O
distribution	O
Beta	O
(	O
α	B-HyperparameterName
,	O
α	B-HyperparameterName
)	O
assigns	O
the	O
two	O
labels	O
with	O
weights	O
λ	O
=	O
0.39	O
and	O
(	O
1	O
−	O
λ	O
)	O
=	O
0.61	O
.	O
The	O
open	O
parenthesis	O
is	O
produced	O
by	O
the	O
mixing	O
of	O
a	O
digit	O
and	O
a	O
punctuation	O
mark	O
,	O
and	O
keeps	O
the	O
label	O
O	O
shared	O
by	O
its	O
parents	O
.	O
Similarly	O
,	O
the	O
token	O
"	O
novelist	O
"	O
generated	O
by	O
"	O
St	O
"	O
and	O
"	O
Kwasniewski	O
"	O
gets	O
a	O
mixed	O
label	O
from	O
B	O
-	O
ORG	O
and	O
B	O
-	O
PER	O
.	O
The	O
discriminator	O
then	O
evaluates	O
the	O
two	O
generated	O
sequences	O
.	O
The	O
generated	O
sequence	O
i	O
is	O
not	O
reasonable	O
enough	O
intuitively	O
,	O
and	O
its	O
perplexity	B-MetricName
score	O
877	O
exceeds	O
the	O
threshold	O
,	O
so	O
it	O
is	O
not	O
added	O
into	O
the	O
training	O
set	O
.	O
The	O
generated	O
sequence	O
j	O
retains	O
the	O
original	O
syntax	O
and	O
semantic	O
structure	O
much	O
better	O
.	O
Although	O
the	O
open	O
parenthesis	O
seems	O
strange	O
,	O
it	O
plays	O
a	O
role	O
as	O
the	O
comma	O
in	O
the	O
original	O
sequence	O
to	O
separate	O
two	O
clauses	O
.	O
This	O
generation	O
behaves	O
closely	O
to	O
a	O
normal	O
sequence	O
and	O
earns	O
332	O
perplexity	B-MetricName
score	O
,	O
which	O
permits	O
its	O
incorporation	O
into	O
the	O
training	O
set	O
.	O

The	O
key	O
parameters	O
setting	O
in	O
our	O
framework	O
are	O
stated	O
here	O
:	O
(	O
1	O
)	O
The	O
number	O
of	O
active	B-TaskName
learning	I-TaskName
round	O
is	O
5	O
for	O
all	O
the	O
three	O
datasets	O
,	O
but	O
the	O
size	O
of	O
seed	O
set	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
in	O
each	O
round	O
differs	O
from	O
the	O
dataset	O
.	O
We	O
list	O
the	O
specific	O
numbers	O
as	O
Table	O
3	O
.	O
(	O
2	O
)	O
The	O
sub	O
-	O
sequence	O
window	O
length	O
s	O
and	O
the	O
valid	O
label	O
density	O
threshold	O
η	O
0	B-DatasetName
vary	O
from	O
the	O
datasets	O
.	O
For	O
CoNLL	O
-	O
03	O
,	O
s	O
=	O
5	O
,	O
η	O
0	B-DatasetName
=	O
0.6	O
;	O
for	O
ACE05	O
,	O
s	O
=	O
5	O
,	O
η	O
0	B-DatasetName
=	O
0.2	O
;	O
for	O
Web	O
-	O
Page	O
,	O
s	O
=	O
4	O
,	O
η	O
0	B-DatasetName
=	O
0.5	O
.	O
(	O
3	O
)	O
We	O
set	O
α	B-HyperparameterName
=	O
8	O
for	O
the	O
Beta	O
distribution	O
.	O
(	O
4	O
)	O
The	O
discriminator	O
score	O
range	O
is	O
set	O
as	O
(	O
0	B-DatasetName
,	O
500	O
)	O
for	O
all	O
the	O
datasets	O
.	O
(	O
5	O
)	O
For	O
BERT	B-MethodName
configuration	O
,	O
we	O
choose	O
5e	O
-	O
5	O
for	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
128	O
for	O
padding	O
length	O
,	O
32	O
for	O
batch	B-HyperparameterName
size	I-HyperparameterName
,	O
0.1	O
for	O
dropout	O
rate	O
,	O
1e	O
-	O
8	O
for	O
in	O
Adam	B-MethodName
.	O
At	O
each	O
data	O
usage	O
point	O
,	O
we	O
train	O
the	O
model	O
for	O
10	O
Epochs	O
.	O
(	O
6	O
)	O
We	O
set	O
C	O
=	O
3	O
for	O
the	O
QBC	O
query	O
policy	O
.	O

We	O
take	O
following	O
criteria	O
to	O
evaluate	O
the	O
sequence	O
labeling	O
task	O
.	O
A	O
named	O
entity	O
is	O
correct	O
only	O
if	O
it	O
is	O
an	O
exact	B-MetricName
match	I-MetricName
of	O
the	O
corresponding	O
entity	O
in	O
the	O
data	O
file	O
.	O
An	O
event	O
trigger	O
is	O
correct	O
only	O
if	O
the	O
span	O
and	O
type	O
match	O
with	O
golden	O
labels	O
.	O
Based	O
on	O
the	O
above	O
metric	O
,	O
we	O
evaluate	O
F	O
1	O
score	O
in	O
our	O
experiments	O
.	O

refers	O
to	O
the	O
number	O
of	O
labeled	O
data	O
,	O
excluding	O
the	O
augmentation	O
data	O
.	O
Sub	O
-	O
sequence	O
mixup	B-MethodName
is	O
trained	O
with	O
(	O
1+α	O
)	O
times	O
data	O
,	O
where	O
the	O
α	B-HyperparameterName
denotes	O
the	O
augment	O
rate	O
.	O
Note	O
that	O
WebPage	O
is	O
a	O
very	O
limited	O
dataset	O
,	O
there	O
is	O
a	O
big	O
difference	O
between	O
the	O
performance	O
on	O
the	O
validation	O
set	O
and	O
the	O
test	O
set	O
.	O
We	O
average	O
each	O
experiment	O
by	O
5	O
times	O
.	O

For	O
the	O
discriminator	O
score	O
range	O
,	O
we	O
first	O
examine	O
the	O
perplexity	B-MetricName
score	O
distribution	O
of	O
the	O
CoNLL	O
training	O
set	O
.	O
Then	O
determine	O
an	O
approximate	O
score	O
range	O
(	O
0	B-DatasetName
,	O
2000	O
)	O
first	O
.	O
We	O
linearly	O
split	O
score	O
ranges	O
below	O
2000	O
to	O
conduct	O
parameter	O
study	O
and	O
report	O
the	O
representative	O
ranges	O
in	O
Section	O
4.3	O
.	O
Given	O
the	O
consideration	O
to	O
the	O
generation	O
speed	O
and	O
the	O
augment	O
rate	O
setting	O
,	O
we	O
finally	O
choose	O
500	O
as	O
the	O
upper	O
limit	O
rather	O
than	O
a	O
too	O
narrow	O
score	O
range	O
setting	O
.	O
For	O
the	O
mixing	O
coefficient	O
λ	O
,	O
we	O
follow	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
to	O
sample	O
it	O
from	O
Beta	O
(	O
α	B-HyperparameterName
,	O
α	B-HyperparameterName
)	O
and	O
explore	O
α	B-HyperparameterName
ranging	O
from	O
[	O
0.5	O
,	O
16	O
]	O
.	O
We	O
present	O
this	O
parameter	O
study	O
in	O
Section	O
4.4	O
.	O
The	O
result	O
shows	O
different	O
α	B-HyperparameterName
did	O
not	O
influence	O
the	O
augmentation	O
performance	O
much	O
.	O
For	O
the	O
augment	O
rate	O
and	O
the	O
valid	O
tag	O
density	O
,	O
we	O
also	O
have	O
introduced	O
the	O
parameter	O
study	O
in	O
Section	O
4.4	O
.	O

[	O
name	O
]	O
Is	O
that	O
for	O
when	O
people	O
ca	O
n't	O
travel	O
due	O
to	O
your	O
staff	O
having	O
to	O
strike	O
to	O
keep	O
everyone	O
safe	O
?	O
Or	O
perhaps	O
short	O
formed	O
trains	O
that	O
you	O
ca	O
nt	O
get	O
on	O
.	O
sure	O
that	O
the	O
lexicon	O
in	O
T	O
r	O
is	O
different	O
from	O
that	O
of	O
C.	O
Namely	O
,	O
we	O
calculated	O
the	O
cosine	O
similarity	O
between	O
the	O
two	O
datasets	O
in	O
the	O
tf	O
-	O
idf	O
space	O
.	O
The	O
cosine	O
similarity	O
at	O
a	O
value	O
of	O
0.028	O
was	O
statistically	O
significant	O
with	O
a	O
Pearson	B-MetricName
correlation	I-MetricName
coefficient	O
value	O
0.012	O
(	O
p	O
-	O
value	O
0.0034	O
)	O
(	O
Schober	O
et	O
al	O
,	O
2018	O
)	O
.	O

We	O
trained	O
a	O
logistic	B-MethodName
regression	I-MethodName
model	O
for	O
complaint	O
detection	O
using	O
each	O
one	O
of	O
the	O
features	O
described	O
in	O
section	O
4.1	O
.	O
Table	O
3	O
summarizes	O
the	O
results	O
in	O
terms	O
of	O
accuracy	B-MetricName
and	O
macro	O
averaged	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
.	O
The	O
best	O
performing	O
model	O
is	O
based	O
on	O
unigrams	O
,	O
with	O
an	O
accuracy	B-MetricName
of	O
75.3	O
%	O
.	O
There	O
is	O
not	O
a	O
significant	O
difference	O
in	O
the	O
performance	O
of	O
different	O
sentiment	O
models	O
.	O
It	O
is	O
also	O
interesting	O
to	O
observe	O
that	O
simple	O
features	O
like	O
the	O
counts	O
of	O
different	O
pronoun	O
types	O
and	O
counts	O
of	O
intensifiers	O
have	O
strong	O
predictive	O
ability	O
.	O
Overall	O
,	O
we	O
observe	O
that	O
most	O
of	O
the	O
features	O
studied	O
here	O
have	O
some	O
ability	O
to	O
predict	O
complaints	O
.	O

This	O
paper	O
presents	O
our	O
submission	O
for	O
the	O
SemEval	O
shared	O
task	O
6	O
,	O
sub	O
-	O
task	O
A	O
on	O
the	O
identification	O
of	O
offensive	O
language	O
.	O
Our	O
proposed	O
model	O
,	O
C	O
-	O
BiGRU	B-MethodName
,	O
combines	O
a	O
Convolutional	O
Neural	O
Network	O
(	O
CNN	O
)	O
with	O
a	O
bidirectional	O
Recurrent	O
Neural	O
Network	O
(	O
RNN	O
)	O
.	O
We	O
utilize	O
word2vec	O
to	O
capture	O
the	O
semantic	O
similarities	O
between	O
words	O
.	O
This	O
composition	O
allows	O
us	O
to	O
extract	O
long	O
term	O
dependencies	O
in	O
tweets	O
and	O
distinguish	O
between	O
offensive	O
and	O
non	O
-	O
offensive	O
tweets	O
.	O
In	O
addition	O
,	O
we	O
evaluate	O
our	O
approach	O
on	O
a	O
different	O
dataset	O
and	O
show	O
that	O
our	O
model	O
is	O
capable	O
of	O
detecting	O
online	O
aggressiveness	O
in	O
both	O
English	O
and	O
German	O
tweets	O
.	O
Our	O
model	O
achieved	O
a	O
macro	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
79.40	O
%	O
on	O
the	O
SemEval	O
dataset	O
.	O

In	O
addition	O
to	O
Twitter	O
data	O
provided	O
by	O
the	O
organizers	O
of	O
the	O
SemEval	O
shared	O
task	O
,	O
we	O
further	O
evaluate	O
our	O
approach	O
on	O
German	O
tweets	O
from	O
the	O
GermEval	O
(	O
2018	O
)	O
shared	O
task	O
.	O
The	O
OLID	B-DatasetName
dataset	O
contains	O
13	O
,	O
240	O
tweets	O
,	O
with	O
4	O
,	O
400	O
offensive	O
and	O
8	O
,	O
840	O
non	O
-	O
offensive	O
tweets	O
(	O
66.77	O
%	O
offensive	O
,	O
33.23	O
%	O
non	O
-	O
offensive	O
)	O
.	O
Similarly	O
,	O
the	O
GermEval	O
dataset	O
contains	O
5	O
,	O
009	O
tweets	O
,	O
divided	O
into	O
1	O
,	O
688	O
offensive	O
and	O
3	O
,	O
321	O
non	O
-	O
offensive	O
tweets	O
(	O
66.30	O
%	O
offensive	O
,	O
33.70	O
%	O
non	O
-	O
offensive	O
)	O
.	O
To	O
compensate	O
for	O
the	O
imbalanced	O
class	O
distributions	O
and	O
weigh	O
each	O
class	O
equally	O
,	O
we	O
choose	O
the	O
macro	O
averaged	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
both	O
classes	O
as	O
our	O
main	O
evaluation	O
metric	O
.	O
From	O
both	O
data	O
sets	O
we	O
use	O
10	O
%	O
of	O
our	O
tweets	O
as	O
test	O
set	O
.	O
The	O
remaining	O
tweets	O
are	O
split	O
into	O
90	O
%	O
training	O
set	O
and	O
10	O
%	O
validation	O
set	O
.	O
We	O
conduct	O
a	O
stratified	O
10	O
-	O
fold	O
cross	O
-	O
validation	O
on	O
the	O
training	O
and	O
validation	O
set	O
to	O
prevent	O
overfitting	O
and	O
to	O
validate	O
our	O
model	O
.	O
The	O
pretrained	O
w2v	O
model	O
,	O
which	O
is	O
used	O
to	O
initialize	O
the	O
weights	O
of	O
our	O
embedding	O
layer	O
,	O
resulted	O
from	O
the	O
work	O
of	O
Godin	O
et	O
al	O
(	O
2015	O
)	O
.	O
The	O
w2v	O
model	O
for	O
the	O
GermEval	O
dataset	O
originates	O
from	O
our	O
previous	O
work	O
(	O
2018	O
)	O
.	O
For	O
comparison	O
to	O
our	O
proposed	O
model	O
,	O
a	O
token	O
bag	O
-	O
of	O
-	O
n	O
-	O
gram	O
model	O
composed	O
of	O
unigrams	O
,	O
bigrams	O
,	O
and	O
trigrams	O
weighted	O
by	O
their	O
TF	O
-	O
IDF	O
is	O
used	O
as	O
baseline	O
approach	O
.	O
We	O
subsequently	O
analyze	O
the	O
performance	O
of	O
different	O
classifiers	O
on	O
the	O
resulting	O
feature	O
space	O
.	O
We	O
have	O
used	O
the	O
packages	O
keras	O
,	O
scikit	O
-	O
learn	O
,	O
gensim	O
,	O
and	O
nltk	O
for	O
preprocessing	O
and	O
the	O
implementation	O
of	O
our	O
models	O
.	O

After	O
the	O
preprocessing	O
step	O
,	O
we	O
construct	O
a	O
dictionary	O
which	O
maps	O
all	O
unique	O
tokens	O
to	O
their	O
number	O
of	O
occurrences	O
in	O
the	O
respective	O
corpus	O
.	O
Tokens	O
which	O
appear	O
only	O
once	O
in	O
a	O
corpus	O
are	O
disregarded	O
and	O
treated	O
as	O
unknown	O
token	O
.	O
As	O
a	O
next	O
step	O
,	O
we	O
construct	O
the	O
weighting	O
matrix	O
W	O
m×dim	O
for	O
our	O
embedding	O
layer	O
,	O
where	O
dim	O
is	O
the	O
dimension	O
of	O
the	O
used	O
w2v	O
model	O
and	O
m	O
the	O
number	O
of	O
unique	O
tokens	O
t	O
i	O
,	O
i	O
{	O
1	O
,	O
...	O
,	O
m	O
}	O
.	O
The	O
word	O
vector	O
of	O
t	O
i	O
is	O
stored	O
in	O
W	O
if	O
the	O
token	O
is	O
represented	O
in	O
the	O
w2v	O
model	O
.	O
If	O
t	O
i	O
has	O
no	O
pretrained	O
word	O
vector	O
,	O
we	O
generate	O
a	O
random	O
vector	O
drawn	O
from	O
the	O
uniform	O
distribution	O
within	O
−	O
6	O
dim	O
,	O
6	O
dim	O
as	O
suggested	O
by	O
He	O
et	O
al	O
(	O
2015	O
)	O
.	O
We	O
fix	O
the	O
maximum	O
length	O
of	O
a	O
sentence	O
to	O
150	O
tokens	O
,	O
longer	O
sequences	O
are	O
clipped	O
at	O
the	O
end	O
and	O
shorter	O
sequences	O
are	O
padded	O
with	O
a	O
masking	O
token	O
.	O
The	O
convolutional	O
layer	O
of	O
our	O
classifier	O
consists	O
of	O
(	O
k	O
×	O
128	O
)	O
1	O
-	O
dimensional	O
filters	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
different	O
window	O
sizes	O
.	O
These	O
window	O
sizes	O
range	O
from	O
2	O
to	O
5	O
and	O
allow	O
the	O
extraction	O
of	O
n	O
-	O
gram	O
features	O
.	O
The	O
padding	O
of	O
the	O
input	O
is	O
kept	O
constant	O
,	O
resulting	O
in	O
the	O
same	O
output	O
sequence	O
length	O
as	O
the	O
input	O
.	O
We	O
further	O
choose	O
ReLu	B-MethodName
as	O
activation	B-HyperparameterName
function	I-HyperparameterName
.	O
The	O
resulting	O
feature	O
maps	O
are	O
concatenated	O
and	O
passed	O
towards	O
the	O
recurrent	O
layer	O
.	O
Gated	O
Recurrent	O
Units	O
(	O
GRU	B-MethodName
)	O
as	O
initially	O
proposed	O
by	O
are	O
used	O
in	O
RNNs	O
to	O
capture	O
long	O
-	O
term	O
dependencies	O
of	O
input	O
sequences	O
.	O
Similar	O
to	O
Long	B-MethodName
Short	I-MethodName
-	I-MethodName
Term	I-MethodName
Memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
units	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
GRU	B-MethodName
are	O
able	O
to	O
overcome	O
the	O
vanishing	O
gradient	O
problem	O
by	O
using	O
a	O
gating	O
mechanism	O
.	O
GRU	B-MethodName
have	O
shown	O
to	O
achieve	O
comparable	O
results	O
to	O
LSTM	B-MethodName
in	O
sequence	O
modeling	O
tasks	O
and	O
are	O
able	O
to	O
outperform	O
the	O
latter	O
on	O
smaller	O
data	O
sets	O
(	O
Chung	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
recurrent	O
layer	O
in	O
our	O
model	O
consists	O
of	O
a	O
bidirectional	B-MethodName
GRU	I-MethodName
,	O
where	O
the	O
concatenated	O
feature	O
maps	O
,	O
which	O
resulted	O
from	O
the	O
convolutional	O
layer	O
,	O
are	O
used	O
as	O
input	O
for	O
the	O
GRU	B-MethodName
layer	O
.	O
Simultaneously	O
,	O
the	O
reversed	O
copy	O
of	O
the	O
input	O
sequence	O
is	O
used	O
for	O
the	O
second	O
GRU	B-MethodName
layer	O
.	O
Both	O
GRU	B-MethodName
layers	O
return	O
a	O
hidden	O
state	O
for	O
each	O
processed	O
feature	O
map	O
.	O
The	O
output	O
of	O
both	O
layers	O
is	O
then	O
concatenated	O
.	O
We	O
set	O
the	O
length	O
of	O
the	O
returned	O
hidden	O
states	O
to	O
64	O
for	O
both	O
layers	O
,	O
resulting	O
in	O
an	O
output	O
space	O
of	O
(	O
150	O
×	O
128	O
)	O
neurons	O
.	O
Afterwards	O
,	O
a	O
global	O
max	B-MethodName
pooling	I-MethodName
layer	O
reduces	O
the	O
output	O
space	O
to	O
(	O
1	O
×	O
128	O
)	O
nodes	O
.	O
The	O
following	O
fully	O
-	O
connected	O
layer	O
consists	O
of	O
32	O
neurons	O
,	O
which	O
connect	O
to	O
a	O
single	O
output	O
neuron	O
.	O
The	O
output	O
neuron	O
utilizes	O
the	O
sigmoid	B-MethodName
activation	I-MethodName
function	O
.	O
To	O
additionally	O
prevent	O
overfitting	O
,	O
we	O
include	O
two	O
dropout	O
layers	O
with	O
a	O
dropout	O
rate	O
of	O
0.2	O
;	O
one	O
after	O
the	O
embedding	O
layer	O
and	O
another	O
one	O
after	O
the	O
fully	O
-	O
connected	O
layer	O
.	O
Furthermore	O
,	O
we	O
adopt	O
early	B-MethodName
stopping	I-MethodName
and	O
use	O
10	O
%	O
of	O
the	O
training	O
data	O
as	O
validation	O
split	O
.	O
We	O
use	O
cross	O
entropy	O
as	O
error	O
function	O
for	O
our	O
model	O
and	O
the	O
optimizer	B-HyperparameterName
'	O
adam	O
'	O
to	O
update	O
our	O
network	O
weights	O
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
for	O
the	O
gradient	O
update	O
is	O
set	O
to	O
32	O
.	O
A	O
schema	O
of	O
our	O
proposed	O
model	O
is	O
illustrated	O
in	O
Figure	O
1	O
.	O

For	O
the	O
comparison	O
model	O
,	O
the	O
SVM	B-MethodName
performs	O
best	O
on	O
the	O
OLID	B-DatasetName
dataset	O
with	O
an	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
70.22	O
%	O
averaged	O
over	O
a	O
10	O
-	O
fold	O
cross	O
-	O
validation	O
.	O
The	O
SVM	B-MethodName
also	O
shows	O
the	O
best	O
results	O
on	O
the	O
GermEval	O
dataset	O
with	O
an	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
66.61	O
%	O
.	O
The	O
evaluation	O
on	O
the	O
test	O
set	O
results	O
in	O
66.78	O
%	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
for	O
the	O
GermEval	O
gold	O
test	O
set	O
.	O
The	O
evaluation	O
of	O
the	O
baseline	O
model	O
for	O
the	O
OLID	B-DatasetName
gold	O
test	O
set	O
is	O
not	O
possible	O
at	O
the	O
time	O
of	O
writing	O
,	O
since	O
the	O
gold	O
test	O
data	O
have	O
not	O
yet	O
been	O
released	O
.	O
The	O
C	O
-	O
BiGRU	B-MethodName
achieved	O
a	O
76.28	O
%	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
on	O
the	O
OLID	B-DatasetName
and	O
a	O
71.13	O
%	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
on	O
the	O
GermEval	O
dataset	O
on	O
average	O
over	O
a	O
10	O
-	O
fold	O
cross	O
-	O
validation	O
.	O
On	O
the	O
OLID	B-DatasetName
gold	O
test	O
set	O
,	O
our	O
model	O
achieved	O
an	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
79.40	O
%	O
.	O
The	O
evaluation	O
on	O
the	O
GermEval	O
gold	O
test	O
data	O
resulted	O
in	O
a	O
72.41	O
%	O
F1score	O
.	O
An	O
overview	O
of	O
all	O
results	O
can	O
be	O
found	O
in	O
Table	O
1	O
.	O
Figure	O
2	O
shows	O
the	O
confusion	O
matrix	O
of	O
our	O
submitted	O
predictions	O
for	O
the	O
SemEval	O
shared	O
task	O
.	O

The	O
presented	O
model	O
continues	O
our	O
work	O
on	O
the	O
identification	O
of	O
offensive	O
German	O
tweets	O
(	O
2018	O
)	O
.	O
We	O
were	O
able	O
to	O
improve	O
our	O
proposed	O
model	O
by	O
adjusting	O
the	O
architecture	O
of	O
the	O
recurrent	O
layer	O
in	O
our	O
neural	O
network	O
.	O
By	O
using	O
a	O
bidirectional	B-MethodName
GRU	I-MethodName
instead	O
of	O
a	O
unidirectional	O
LSTM	B-MethodName
,	O
we	O
are	O
able	O
to	O
capture	O
past	O
and	O
future	O
information	O
about	O
the	O
input	O
sequence	O
and	O
exploit	O
the	O
better	O
performance	O
of	O
GRU	B-MethodName
networks	O
on	O
smaller	O
datasets	O
.	O
Furthermore	O
,	O
we	O
return	O
the	O
hidden	O
states	O
for	O
each	O
feature	O
map	O
instead	O
of	O
returning	O
only	O
the	O
last	O
hidden	O
state	O
.	O
This	O
allows	O
us	O
to	O
extract	O
higher	O
-	O
level	O
sequentially	O
dependent	O
features	O
from	O
each	O
concatenated	O
feature	O
map	O
.	O
Our	O
experiments	O
show	O
that	O
our	O
suggested	O
model	O
outperforms	O
the	O
baseline	O
model	O
on	O
both	O
datasets	O
.	O
The	O
difference	O
between	O
the	O
F1	B-MetricName
-	O
scores	O
for	O
the	O
English	O
and	O
German	O
dataset	O
might	O
be	O
attributed	O
to	O
the	O
smaller	O
size	O
of	O
the	O
German	O
training	O
set	O
,	O
which	O
contains	O
only	O
about	O
5	O
,	O
000	O
tweets	O
.	O
The	O
discrepancy	O
between	O
the	O
results	O
of	O
our	O
cross	O
-	O
validation	O
and	O
achieved	O
score	O
on	O
the	O
OLID	B-DatasetName
test	O
set	O
might	O
be	O
explained	O
by	O
the	O
small	O
amount	O
of	O
test	O
tweets	O
,	O
which	O
may	O
lead	O
to	O
imprecise	O
results	O
for	O
the	O
submitted	O
runs	O
.	O
By	O
utilizing	O
w2v	O
as	O
features	O
,	O
we	O
are	O
able	O
to	O
limit	O
extensive	O
and	O
language	O
specific	O
preprocessing	O
.	O
"	O
@USER	O
Lolol	O
God	O
he	O
is	O
such	O
an	O
a**hole	O
.	O
"	O
In	O
this	O
example	O
,	O
the	O
vector	O
representation	O
of	O
"	O
a**hole	O
"	O
has	O
a	O
high	O
cosine	O
similarity	O
(	O
0.63	O
)	O
to	O
the	O
vector	O
representation	O
of	O
"	O
asshole	O
"	O
,	O
which	O
allows	O
our	O
model	O
to	O
classify	O
this	O
tweet	O
as	O
offensive	O
.	O
On	O
the	O
contrary	O
,	O
our	O
approach	O
falls	O
short	O
when	O
confronted	O
with	O
indirect	O
insults	O
.	O
"	O
@USER	O
@USER	O
I	O
m	O
sure	O
the	O
air	O
that	O
he	O
is	O
breathing	O
is	O
also	O
bad	O
.	O
"	O
Our	O
model	O
wrongly	O
predicts	O
a	O
non	O
-	O
offensive	O
tweet	O
in	O
this	O
instance	O
.	O
The	O
detection	O
of	O
offensive	O
,	O
hateful	O
,	O
racist	O
,	O
and/or	O
sexist	O
user	O
behavior	O
in	O
social	O
media	O
still	O
proves	O
to	O
be	O
a	O
challenge	O
.	O
Even	O
for	O
humans	O
,	O
it	O
can	O
be	O
problematic	O
to	O
identify	O
offensive	O
microposts	O
,	O
since	O
these	O
posts	O
can	O
be	O
ambiguous	O
and	O
dependant	O
on	O
the	O
personal	O
mindset	O
of	O
a	O
reader	O
.	O
Ross	O
et	O
al	O
(	O
2017	O
)	O
show	O
that	O
it	O
can	O
be	O
difficult	O
to	O
measure	O
the	O
agreement	O
of	O
annotators	O
about	O
hate	B-DatasetName
speech	I-DatasetName
in	O
the	O
light	O
of	O
the	O
European	O
refugee	O
crisis	O
.	O
They	O
conclude	O
that	O
instead	O
of	O
a	O
classification	O
problem	O
,	O
a	O
regression	O
model	O
with	O
an	O
average	O
offensiveness	O
score	O
of	O
multiple	O
annotators	O
might	O
be	O
more	O
suitable	O
for	O
this	O
task	O
.	O
Furthermore	O
,	O
it	O
can	O
be	O
difficult	O
to	O
grasp	O
the	O
full	O
context	O
of	O
an	O
arbitrary	O
tweet	O
.	O
With	O
only	O
excerpts	O
of	O
a	O
conversation	O
,	O
the	O
context	O
and	O
true	O
intention	O
of	O
the	O
author	O
may	O
be	O
difficult	O
to	O
determine	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
our	O
submitted	O
model	O
for	O
the	O
SemEval	O
shared	O
task	O
6	O
and	O
evaluation	O
methods	O
for	O
the	O
identification	O
of	O
online	O
aggressiveness	O
in	O
social	O
media	O
microposts	O
.	O
Our	O
model	O
achieves	O
good	O
results	O
in	O
the	O
two	O
evaluated	O
datasets	O
.	O
For	O
the	O
OLID	B-DatasetName
dataset	O
which	O
contains	O
English	O
tweets	O
,	O
a	O
macro	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
79.40	O
%	O
is	O
reached	O
,	O
while	O
our	O
network	O
resulted	O
in	O
an	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
72.41	O
%	O
on	O
the	O
GermEval	O
dataset	O
,	O
which	O
consists	O
of	O
German	O
tweets	O
.	O
We	O
plan	O
to	O
evaluate	O
our	O
approach	O
on	O
more	O
datasets	O
to	O
further	O
investigate	O
the	O
potential	O
of	O
our	O
model	O
for	O
different	O
languages	O
.	O
One	O
such	O
set	O
is	O
the	O
TRAC	O
dataset	O
,	O
which	O
contains	O
aggressionannotated	O
Facebook	O
posts	O
and	O
comments	O
in	O
Hindi	O
.	O
Furthermore	O
,	O
we	O
want	O
to	O
examine	O
whether	O
additional	O
features	O
such	O
as	O
character	O
-	O
level	O
embeddings	O
or	O
POS	O
tagging	O
will	O
improve	O
our	O
results	O
.	O
Inclusion	O
of	O
figurative	O
language	O
detection	O
has	O
proved	O
to	O
enhance	O
many	O
NLP	O
tasks	O
,	O
such	O
as	O
argument	B-TaskName
mining	I-TaskName
and	O
so	O
-	O
called	O
hidden	O
hate	B-DatasetName
speech	I-DatasetName
(	O
Mitrović	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
is	O
also	O
one	O
of	O
our	O
future	O
directions	O
.	O

We	O
begin	O
by	O
creating	O
a	O
lexicon	O
of	O
words	O
that	O
are	O
commonly	O
used	O
to	O
refer	O
to	O
COVID	O
-	O
19	O
.	O
This	O
allows	O
us	O
to	O
determine	O
the	O
extent	O
to	O
which	O
users	O
in	O
each	O
subreddit	O
are	O
discussing	O
COVID	O
-	O
19	O
,	O
and	O
also	O
gives	O
us	O
a	O
clearer	O
idea	O
of	O
when	O
COVID	O
-	O
19	O
began	O
to	O
directly	O
affect	O
discussion	O
in	O
the	O
mental	O
health	O
subreddits	O
.	O
We	O
based	O
the	O
lexicon	O
on	O
a	O
set	O
of	O
twitter	O
search	O
keywords	O
from	O
Huang	O
et	O
al	O
(	O
2020	O
)	O
,	O
and	O
added	O
six	O
additional	O
words	O
that	O
we	O
believed	O
would	O
be	O
indicative	O
of	O
discussion	O
about	O
COVID	O
-	O
19	O
(	O
see	O
the	O
full	O
lexicon	O
in	O
Appendix	O
A	O
)	O
.	O
To	O
study	O
changes	O
in	O
the	O
number	O
of	O
users	O
seeking	O
mental	O
health	O
support	O
in	O
subreddits	O
,	O
we	O
record	O
the	O
author	O
usernames	O
for	O
each	O
post	O
in	O
our	O
dataset	O
.	O
Since	O
individuals	O
can	O
create	O
multiple	O
accounts	O
under	O
different	O
usernames	O
,	O
the	O
number	O
of	O
unique	O
usernames	O
associated	O
with	O
posts	O
is	O
likely	O
not	O
equal	O
to	O
the	O
true	O
number	O
of	O
unique	O
users	O
;	O
however	O
,	O
it	O
is	O
a	O
reasonable	O
proxy	O
.	O
To	O
study	O
changes	O
in	O
content	O
that	O
occur	O
during	O
the	O
pandemic	O
,	O
we	O
use	O
the	O
LIWC	O
lexicon	O
(	O
Pennebaker	O
et	O
al	O
,	O
2015	O
)	O
and	O
Latent	O
Dirichlet	O
Allocation	O
(	O
LDA	B-MethodName
)	O
topic	O
modeling	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
.	O
The	O
LIWC	O
lexicon	O
consists	O
of	O
seventy	O
-	O
three	O
hierarchical	O
psycholinguistic	O
word	O
categories	O
,	O
encapsulating	O
properties	O
including	O
linguistic	O
categories	O
(	O
e.g.	O
,	O
1st	O
person	O
plural	O
pronouns	O
,	O
verbs	O
)	O
,	O
emotions	O
(	O
e.g.	O
,	O
anxiety	O
,	O
sadness	O
)	O
,	O
time	O
(	O
e.g.	O
,	O
present	O
,	O
future	O
)	O
,	O
and	O
personal	O
concerns	O
(	O
e.g.	O
,	O
work	O
,	O
money	O
,	O
death	O
)	O
.	O
To	O
capture	O
the	O
discussion	O
topics	O
that	O
are	O
common	O
in	O
the	O
r	O
/	O
Anxiety	O
,	O
r	O
/	O
depression	O
,	O
and	O
r	O
/	O
SuicideWatch	O
subreddits	O
specifically	O
,	O
we	O
train	O
a	O
topic	O
model	O
on	O
posts	O
from	O
these	O
subreddits	O
.	O
We	O
ensure	O
that	O
discussions	O
from	O
each	O
of	O
the	O
subreddits	O
are	O
equally	O
represented	O
in	O
our	O
training	O
dataset	O
by	O
randomly	O
downsampling	O
the	O
posts	O
from	O
the	O
subreddits	O
with	O
more	O
data	O
.	O
We	O
use	O
the	O
implementation	O
of	O
LDA	B-MethodName
topic	O
modeling	O
provided	O
in	O
the	O
MALLET	O
toolkit	O
(	O
McCallum	O
,	O
2002	O
)	O
and	O
train	O
models	O
with	O
k	B-HyperparameterName
=	I-HyperparameterName
5	O
,	O
10	O
,	O
..	O
,	O
40	O
topics	O
.	O
We	O
select	O
a	O
single	O
model	O
to	O
use	O
in	O
our	O
analysis	O
by	O
examining	O
their	O
coherence	O
scores	O
,	O
a	O
measure	O
of	O
the	O
semantic	B-TaskName
similarity	I-TaskName
of	O
high	O
probability	O
words	O
within	O
each	O
topic	O
(	O
Mimno	O
et	O
al	O
,	O
2011	O
)	O
.	O
As	O
coherence	O
scores	O
tend	O
to	O
increase	O
with	O
increasing	O
k	O
,	O
we	O
select	O
k	O
as	O
the	O
first	O
local	O
maxima	O
of	O
coherence	O
scores	O
,	O
which	O
we	O
found	O
to	O
be	O
k	B-HyperparameterName
=	I-HyperparameterName
25	O
.	O
In	O
Appendix	O
B	O
,	O
we	O
show	O
the	O
25	O
topics	O
obtained	O
from	O
our	O
topic	O
model	O
,	O
along	O
with	O
the	O
highest	O
probability	O
words	O
associated	O
with	O
each	O
topic	O
.	O
We	O
also	O
provide	O
labels	O
that	O
summarize	O
the	O
essence	O
of	O
each	O
topic	O
,	O
which	O
we	O
created	O
by	O
examining	O
their	O
representative	O
words	O
.	O
Common	O
themes	O
of	O
discussion	O
include	O
:	O
daily	O
life	O
concerns	O
(	O
e.g.	O
,	O
school	O
,	O
work	O
,	O
sleep	O
and	O
routine	O
)	O
,	O
personal	O
relationships	O
(	O
e.g.	O
,	O
friends	O
,	O
family	O
,	O
relationships	O
)	O
,	O
and	O
mental	O
health	O
struggles	O
(	O
e.g.	O
,	O
anxiety	O
,	O
suicide	O
,	O
medical	O
treatment	O
)	O
.	O
When	O
using	O
text	O
from	O
posts	O
,	O
we	O
remove	O
special	O
characters	O
and	O
sequences	O
,	O
such	O
as	O
newlines	O
,	O
quotes	O
,	O
emails	O
,	O
and	O
tables	O
.	O
To	O
represent	O
the	O
text	O
of	O
a	O
post	O
,	O
we	O
concatenate	O
the	O
title	O
with	O
the	O
text	O
content	O
,	O
as	O
was	O
done	O
in	O
prior	O
work	O
(	O
Chakravorti	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
apply	O
additional	O
pre	O
-	O
processing	O
steps	O
for	O
our	O
topic	O
modeling	O
analysis	O
:	O
(	O
1	O
)	O
we	O
remove	O
a	O
set	O
of	O
common	O
stopwords	O
that	O
do	O
not	O
appear	O
in	O
the	O
LIWC	O
lexicon	O
(	O
we	O
kept	O
those	O
in	O
LIWC	O
as	O
they	O
have	O
been	O
found	O
to	O
have	O
psychological	O
meaning	O
)	O
,	O
(	O
2	O
)	O
we	O
form	O
bigrams	O
from	O
pairs	O
of	O
words	O
that	O
commonly	O
appear	O
together	O
,	O
and	O
(	O
3	O
)	O
we	O
lemmatize	O
each	O
word	O
.	O

We	O
report	O
the	O
ten	O
topics	O
with	O
the	O
highest	O
proportion	O
of	O
outliers	O
for	O
each	O
subreddit	O
during	O
the	O
COVID	O
-	O
19	O
period	O
(	O
March	O
to	O
May	O
2020	O
)	O
in	O
Table	O
3	O
.	O
One	O
notable	O
trend	O
is	O
an	O
increase	O
in	O
the	O
amount	O
of	O
discussion	O
related	O
to	O
family	O
;	O
we	O
find	O
that	O
the	O
FAM	O
-	O
ILY	O
AND	O
HOME	O
topic	O
increased	O
significantly	O
in	O
all	O
three	O
subreddits	O
and	O
the	O
FAMILY	O
AND	O
CHIL	O
-	O
DREN	O
topic	O
increased	O
significantly	O
in	O
r	O
/	O
depression	O
.	O
Figure	O
4b	O
shows	O
how	O
the	O
usage	O
of	O
the	O
FAMILY	O
AND	O
HOME	O
topic	O
has	O
changed	O
since	O
January	O
2019	O
within	O
each	O
subreddit	O
.	O
While	O
there	O
are	O
noticeable	O
increases	O
in	O
all	O
three	O
subreddits	O
,	O
we	O
a	O
see	O
particularly	O
large	O
spike	O
in	O
r	O
/	O
Anxiety	O
starting	O
around	O
mid	O
-	O
March	O
.	O
Within	O
all	O
subreddits	O
,	O
we	O
see	O
a	O
significant	O
decrease	O
in	O
the	O
TRANSPORT	O
AND	O
DAILY	O
LIFE	O
topic	O
(	O
see	O
Figure	O
4c	O
)	O
,	O
which	O
is	O
associated	O
with	O
words	O
such	O
"	O
drive	O
,	O
"	O
"	O
car	O
,	O
"	O
"	O
time	O
,	O
"	O
and	O
"	O
day	O
"	O
.	O
Mirroring	O
the	O
reduction	O
of	O
WORK	O
-	O
related	O
language	O
we	O
observed	O
in	O
Section	O
5.3.1	O
,	O
we	O
also	O
find	O
that	O
there	O
has	O
been	O
a	O
significant	O
decrease	O
in	O
discussion	O
of	O
the	O
SCHOOL	O
and	O
WORK	O
topics	O
within	O
the	O
r	O
/	O
Anxiety	O
and	O
r	O
/	O
depression	O
subreddits	O
.	O
We	O
observe	O
significant	O
changes	O
in	O
topics	O
that	O
are	O
explicitly	O
related	O
to	O
mental	O
health	O
.	O
One	O
of	O
the	O
most	O
prominent	O
trends	O
is	O
a	O
significant	O
increase	O
in	O
discussions	O
of	O
ANXIETY	O
and	O
its	O
symptoms	O
(	O
keywords	O
include	O
:	O
"	O
panic	O
,	O
"	O
"	O
heart	O
,	O
"	O
and	O
"	O
chest	O
"	O
)	O
.	O
As	O
seen	O
in	O
Figure	O
4a	O
,	O
we	O
see	O
a	O
spike	O
in	O
ANXIETY	O
in	O
mid	O
-	O
March	O
in	O
all	O
three	O
subreddits	O
;	O
however	O
,	O
whereas	O
we	O
see	O
a	O
return	O
to	O
a	O
typical	O
level	O
in	O
both	O
r	O
/	O
depression	O
and	O
r	O
/	O
SuicideWatch	O
,	O
within	O
the	O
r	O
/	O
Anxiety	O
,	O
ANX	O
-	O
IETY	O
discussion	O
rates	O
have	O
remained	O
abnormally	O
high	O
all	O
the	O
way	O
through	O
the	O
end	O
of	O
May	O
.	O
We	O
find	O
that	O
both	O
INFORMATION	O
SHARING	O
(	O
keywords	O
include	O
:	O
"	O
post	O
,	O
"	O
"	O
read	O
,	O
"	O
"	O
share	O
,	O
"	O
"	O
find	O
,	O
"	O
and	O
"	O
hope	O
"	O
)	O
and	O
COMMUNICATION	O
(	O
keywords	O
include	O
:	O
"	O
talk	O
,	O
"	O
"	O
call	O
,	O
"	O
and	O
"	O
message	O
"	O
)	O
have	O
become	O
more	O
frequent	O
topics	O
of	O
discussion	O
.	O
Discussion	O
Several	O
of	O
the	O
results	O
in	O
Table	O
3	O
seem	O
to	O
reflect	O
the	O
disruption	O
to	O
normal	O
daily	O
life	O
caused	O
by	O
COVID	O
-	O
19	O
and	O
the	O
resulting	O
quarantine	O
measures	O
.	O
This	O
includes	O
the	O
increase	O
in	O
the	O
FAMILY	O
AND	O
CHILDREN	O
topic	O
(	O
Figure	O
4b	O
)	O
,	O
which	O
is	O
largely	O
expected	O
,	O
as	O
quarantine	O
policies	O
implemented	O
to	O
help	O
contain	O
COVID	O
-	O
19	O
have	O
resulted	O
in	O
many	O
people	O
spending	O
more	O
time	O
at	O
home	O
and	O
with	O
family	O
than	O
they	O
previously	O
had	O
.	O
While	O
there	O
are	O
noticeable	O
increases	O
in	O
all	O
three	O
subreddits	O
,	O
we	O
see	O
a	O
particularly	O
large	O
spike	O
in	O
r	O
/	O
Anxiety	O
starting	O
around	O
mid	O
-	O
March	O
.	O
Prior	O
studies	O
on	O
disease	O
outbreaks	O
have	O
found	O
that	O
uncertainty	O
regarding	O
the	O
wellbeing	O
of	O
loved	O
ones	O
is	O
a	O
common	O
source	O
of	O
anxiety	O
during	O
epidemics	O
,	O
which	O
may	O
help	O
to	O
explain	O
this	O
finding	O
(	O
Chew	O
et	O
al	O
,	O
2020	O
)	O
.	O
Another	O
contributing	O
factor	O
may	O
be	O
the	O
emergence	O
of	O
new	O
family	O
responsibilities	O
,	O
such	O
as	O
childcare	O
and	O
home	O
-	O
schooling	O
,	O
that	O
many	O
people	O
have	O
had	O
to	O
take	O
on	O
in	O
the	O
face	O
of	O
closures	O
caused	O
by	O
the	O
pandemic	O
.	O
The	O
decrease	O
in	O
the	O
TRANSPORT	O
AND	O
DAILY	O
LIFE	O
topic	O
(	O
Figure	O
4c	O
)	O
is	O
intuitive	O
;	O
quarantine	O
practices	O
following	O
COVID	O
-	O
19	O
have	O
led	O
to	O
a	O
large	O
reduction	O
in	O
driving	O
and	O
other	O
forms	O
of	O
transportation	O
(	O
Domonoske	O
and	O
Adeline	O
,	O
2020	O
)	O
and	O
,	O
more	O
generally	O
,	O
to	O
a	O
disruption	O
in	O
daily	O
lifestyles	O
.	O
To	O
the	O
extent	O
that	O
these	O
results	O
indicate	O
an	O
abandonment	O
of	O
routine	O
,	O
they	O
are	O
somewhat	O
concerning	O
,	O
as	O
evidence	O
from	O
prior	O
outbreaks	O
suggests	O
that	O
getting	O
back	O
into	O
normal	O
routines	O
helps	O
to	O
reduce	O
loneliness	O
and	O
anxiety	O
during	O
quarantines	O
(	O
Huremović	O
,	O
2019	O
)	O
.	O
The	O
decreases	O
in	O
discussion	O
of	O
the	O
SCHOOL	O
and	O
WORK	O
topics	O
may	O
indicate	O
that	O
these	O
previously	O
common	O
sources	O
of	O
stress	O
have	O
now	O
become	O
secondary	O
concerns	O
compared	O
to	O
the	O
more	O
immediate	O
concerns	O
associated	O
with	O
COVID	O
-	O
19	O
.	O
The	O
increases	O
in	O
the	O
ANXITETY	O
topic	O
,	O
especially	O
on	O
r	O
/	O
Anxiety	O
,	O
are	O
aligned	O
with	O
existing	O
research	O
that	O
has	O
found	O
that	O
anxiety	O
and	O
the	O
somatic	O
symptoms	O
associated	O
with	O
it	O
are	O
common	O
psychological	O
responses	O
to	O
epidemics	O
(	O
Chew	O
et	O
al	O
,	O
2020	O
)	O
.	O
Further	O
,	O
studies	O
of	O
prior	O
epidemics	O
have	O
found	O
that	O
feelings	O
of	O
anxiety	O
and	O
fear	O
can	O
persist	O
even	O
after	O
the	O
disease	O
itself	O
has	O
been	O
contained	O
(	O
Usher	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
increase	O
in	O
the	O
INFORMATION	O
SHARING	O
and	O
COMMUNICATION	O
topics	O
may	O
be	O
tied	O
to	O
the	O
effects	O
of	O
social	O
distancing	O
measures	O
,	O
which	O
have	O
limited	O
in	O
-	O
person	O
interactions	O
and	O
led	O
people	O
to	O
increasingly	O
turn	O
to	O
digital	O
methods	O
of	O
communication	O
.	O
These	O
observations	O
may	O
also	O
reflect	O
a	O
desire	O
to	O
seek	O
out	O
information	O
related	O
to	O
COVID	O
-	O
19	O
;	O
individuals	O
who	O
experience	O
health	O
anxiety	O
are	O
more	O
likely	O
to	O
exhibit	O
online	O
health	O
information	O
seeking	O
behavior	O
(	O
McMullan	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
increase	O
in	O
mentions	O
of	O
words	O
related	O
to	O
social	O
media	O
(	O
e.g.	O
,	O
"	O
post	O
,	O
"	O
"	O
share	O
"	O
)	O
is	O
somewhat	O
worrisome	O
;	O
studies	O
of	O
disaster	O
events	O
have	O
found	O
that	O
both	O
more	O
frequent	O
social	O
media	O
use	O
and	O
exposure	O
to	O
conflicting	O
information	O
online	O
(	O
a	O
widely	O
acknowledged	O
issue	O
with	O
COVID	O
-	O
Table	O
3	O
:	O
Ten	O
topics	O
with	O
the	O
most	O
outliers	O
for	O
r	O
/	O
Anxiety	O
,	O
r	O
/	O
depression	O
,	O
and	O
r	O
/	O
SuicideWatch	O
.	O
Arrows	O
mark	O
the	O
direction	O
in	O
which	O
the	O
mean	O
of	O
the	O
outliers	O
shifted	O
from	O
the	O
predicted	O
mean	O
.	O
Topics	O
marked	O
with	O
*	O
have	O
a	O
statistically	O
significant	O
percentage	O
of	O
outliers	O
(	O
with	O
Bonferroni	O
correction	O
;	O
α	B-HyperparameterName
=	O
0.05	O
before	O
correction	O
)	O
.	O
19	O
(	O
Kouzy	O
et	O
al	O
,	O
2020	O
)	O
)	O
lead	O
to	O
higher	O
stress	O
levels	O
(	O
Torales	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
the	O
rise	O
of	O
the	O
INFORMATION	O
SHARING	O
topic	O
,	O
especially	O
in	O
it	O
's	O
relation	O
to	O
words	O
like	O
"	O
share	O
,	O
"	O
"	O
hope	O
,	O
"	O
and	O
"	O
story	O
,	O
"	O
could	O
also	O
be	O
indicative	O
of	O
a	O
collective	O
coping	O
process	O
,	O
in	O
which	O
individuals	O
come	O
together	O
for	O
social	O
support	O
.	O
As	O
noted	O
in	O
Section	O
5.3.1	O
,	O
this	O
type	O
of	O
coping	O
strategy	O
has	O
frequently	O
been	O
observed	O
during	O
past	O
disease	O
outbreaks	O
(	O
Chew	O
et	O
al	O
,	O
2020	O
)	O
and	O
may	O
also	O
be	O
reflected	O
by	O
the	O
increase	O
in	O
the	O
usage	O
of	O
WE	O
we	O
saw	O
for	O
discussions	O
in	O
r	O
/	O
Anxiety	O
.	O

We	O
apply	O
a	O
one	O
-	O
sample	O
proportion	O
test	O
to	O
assess	O
whether	O
the	O
proportion	O
of	O
observations	O
outside	O
of	O
the	O
prediction	O
interval	O
in	O
the	O
post	O
-	O
COVID	O
period	O
is	O
significantly	O
greater	O
than	O
5	O
%	O
.	O
This	O
test	O
assumes	O
that	O
the	O
observations	O
are	O
independent	O
;	O
however	O
,	O
we	O
find	O
that	O
there	O
is	O
order	O
-	O
1	O
autocorrelation	O
in	O
our	O
data	O
.	O
We	O
therefore	O
apply	O
a	O
correction	O
for	O
order	O
-	O
1	O
autocorrelation	O
(	O
Zwiers	O
and	O
Von	O
Storch	O
,	O
1995	O
)	O
when	O
computing	O
the	O
z	O
-	O
test	O
statistic	O
.	O
The	O
corrected	O
test	O
statistic	O
is	O
:	O
z	O
=	O
p	O
−	O
p	O
0	B-DatasetName
p	O
0	B-DatasetName
(	O
1	O
−	O
p	O
0	B-DatasetName
)	O
(	O
1	O
+	O
r	O
)	O
/n	O
(	O
1	O
−	O
r	O
)	O
(	O
2	O
)	O
wherep	O
is	O
the	O
proportion	O
of	O
observations	O
outside	O
of	O
the	O
prediction	O
interval	O
in	O
the	O
post	O
-	O
COVID	O
period	O
,	O
p	O
0	B-DatasetName
=	O
0.05	O
,	O
n	O
is	O
the	O
number	O
of	O
observations	O
in	O
the	O
post	O
-	O
COVID	O
period	O
,	O
and	O
r	O
is	O
the	O
lag	O
-	O
1	O
correlation	O
coefficient	O
of	O
the	O
pre	O
-	O
COVID	O
data	O
.	O
We	O
use	O
a	O
Bonferroni	O
correction	O
when	O
determining	O
statistical	O
significance	O
for	O
our	O
discussion	O
content	O
metrics	O
,	O
as	O
we	O
ran	O
almost	O
300	O
tests	O
.	O
M	O
=	O
294	O
,	O
which	O
is	O
the	O
number	O
of	O
LIWC	O
categories	O
and	O
topics	O
,	O
multiplied	O
by	O
the	O
number	O
of	O
subreddits	O
.	O
Our	O
corrected	O
α	B-HyperparameterName
=	O
0.05/294	O
=	O
1.7	O
×	O
10	O
−5	O
.	O

We	O
are	O
grateful	O
to	O
the	O
Michigan	O
AI	O
lab	O
for	O
discussions	O
that	O
led	O
to	O
this	O
project	O
,	O
and	O
to	O
the	O
statistical	O
consultants	O
at	O
CSCAR	O
who	O
helped	O
with	O
developing	O
our	O
process	O
for	O
hypothesis	O
testing	O
,	O
especially	O
Kerby	O
Shedden	O
and	O
Thomas	O
Fiore	O
.	O
This	O
material	O
is	O
based	O
in	O
part	O
on	O
work	O
supported	O
by	O
the	O
Precision	B-MetricName
Health	O
initiative	O
at	O
the	O
University	O
of	O
Michigan	O
,	O
the	O
NSF	O
(	O
grant	O
#	O
1815291	O
)	O
,	O
and	O
the	O
John	O
Templeton	O
Foundation	O
(	O
grant	O
#	O
61156	O
)	O
.	O
Any	O
opinions	O
,	O
findings	O
,	O
conclusions	O
,	O
or	O
recommendations	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
necessarily	O
reflect	O
the	O
views	O
of	O
the	O
Precision	B-MetricName
Health	O
initiative	O
,	O
the	O
NSF	O
,	O
or	O
the	O
John	O
Templeton	O
Foundation	O
.	O

We	O
measure	O
inter	O
-	O
annotator	O
agreement	O
with	O
Krippendorff	O
's	O
alpha	B-HyperparameterName
(	O
Krippendorff	O
,	O
2004	O
)	O
and	O
find	O
that	O
,	O
over	O
all	O
labels	O
,	O
there	O
are	O
substantial	O
levels	O
of	O
agreement	O
within	O
groups	O
of	O
annotators	O
:	O
α	B-HyperparameterName
=	O
0.79	O
for	O
trained	O
annotators	O
and	O
α	B-HyperparameterName
=	O
0.71	O
and	O
72	O
for	O
untrained	O
annotators	O
on	O
the	O
Yahoo	O
and	O
IAC	O
threads	O
,	O
respectively	O
.	O
However	O
,	O
there	O
is	O
lower	O
agreement	O
on	O
thread	O
labels	O
than	O
comment	O
labels	O
(	O
Table	O
2	O
)	O
.	O
The	O
agreement	O
of	O
thread	O
type	O
is	O
25	O
%	O
higher	O
for	O
the	O
Yahoo	O
threads	O
than	O
the	O
IAC	O
(	O
0.62	O
-	O
0.64	O
compared	O
to	O
0.48	O
)	O
.	O
The	O
less	O
subjective	O
comment	O
labels	O
(	O
i.e.	O
,	O
agreement	O
,	O
audience	O
,	O
and	O
topic	O
)	O
have	O
higher	O
agreement	O
than	O
persuasiveness	O
,	O
sentiment	O
,	O
and	O
tone	O
.	O
While	O
some	O
of	O
the	O
labels	O
have	O
only	O
moderate	O
agreement	O
(	O
0.5	O
<	O
α	B-HyperparameterName
<	O
0.6	O
)	O
,	O
we	O
find	O
these	O
results	O
satisfactory	O
as	O
the	O
agreement	O
levels	O
are	O
higher	O
than	O
those	O
reported	O
for	O
similarly	O
subjective	B-DatasetName
discourse	I-DatasetName
annotation	O
tasks	O
(	O
e.g.	O
,	O
Walker	O
et	O
al	O
(	O
2012	O
)	O
)	O
.	O
To	O
evaluate	O
the	O
untrained	O
annotators	O
,	O
we	O
compare	O
the	O
thread	O
-	O
level	O
annotations	O
made	O
on	O
300	O
Yahoo	O
threads	O
by	O
both	O
trained	O
and	O
untrained	O
coders	O
,	O
by	O
taking	O
the	O
majority	O
label	O
per	O
item	O
from	O
each	O
group	O
of	O
annotators	O
and	O
calculating	O
the	O
percent	O
of	O
exact	O
matches	O
(	O
Table	O
3	O
)	O
.	O
When	O
classifying	O
the	O
thread	O
type	O
,	O
multiple	O
labels	O
are	O
allowed	O
for	O
each	O
thread	O
,	O
so	O
we	O
convert	O
each	O
option	O
into	O
a	O
boolean	O
and	O
analyze	O
them	O
separately	O
.	O
Only	O
8	O
%	O
of	O
the	O
threads	O
have	O
no	O
majority	O
constructive	O
label	O
in	O
the	O
trained	O
and/or	O
untrained	O
annotations	O
,	O
and	O
20	O
%	O
have	O
no	O
majority	O
agreement	O
label	O
.	O
Within	O
both	O
annotation	O
groups	O
,	O
there	O
are	O
majority	O
labels	O
on	O
all	O
of	O
the	O
thread	O
type	O
labels	O
.	O
The	O
category	O
with	O
the	O
lowest	O
agreement	O
is	O
constructive	O
class	O
with	O
only	O
61	O
%	O
of	O
the	O
majority	O
labels	O
matching	O
,	O
followed	O
closely	O
by	O
agreement	O
(	O
only	O
62	O
%	O
matching	O
)	O
.	O
A	O
very	O
high	O
percent	O
of	O
the	O
thread	O
type	O
labels	O
(	O
81	O
%	O
)	O
.	O
The	O
strong	O
agreement	O
levels	O
between	O
trained	O
and	O
untrained	O
annotators	O
suggest	O
that	O
crowdsourcing	O
is	O
reliable	O
for	O
coding	O
thread	O
-	O
level	O
characteristics	O
.	O

There	O
has	O
recently	O
been	O
huge	O
interest	O
in	O
wellbeing	O
,	O
with	O
a	O
recent	O
review	O
arguing	O
that	O
psychological	O
well	O
-	O
being	O
plays	O
a	O
causal	O
role	O
in	O
promoting	O
job	O
success	O
,	O
physical	O
health	O
,	O
and	O
long	O
-	O
term	O
relationships	O
(	O
Lyubomirsky	O
et	O
al	O
,	O
2005	O
;	O
Kahneman	O
,	O
1999	O
)	O
.	O
In	O
this	O
paper	O
we	O
analyze	O
a	O
corpus	O
of	O
private	O
micro	O
-	O
blogs	O
from	O
a	O
well	O
-	O
being	O
application	O
called	O
ECHO	O
,	O
with	O
the	O
aim	O
to	O
detect	O
,	O
understand	O
,	O
and	O
fur	O
-	O
ECHO	O
initiates	O
user	O
-	O
written	O
reactions	O
to	O
daily	O
events	O
,	O
called	O
RECORDINGS	O
,	O
as	O
well	O
as	O
subsequent	O
REFLECTIONS	O
on	O
those	O
events	O
at	O
points	O
in	O
the	O
future	O
(	O
Isaacs	O
et	O
al	O
,	O
2013	O
)	O
.	O
1	O
Each	O
reaction	O
is	O
labelled	O
at	O
the	O
time	O
of	O
recording	O
or	O
reflection	O
by	O
the	O
user	O
,	O
the	O
first	O
-	O
person	O
experiencer	O
,	O
with	O
a	O
happiness	O
rating	O
from	O
1	O
and	O
9	O
.	O
Note	O
that	O
all	O
users	O
'	O
posts	O
and	O
ratings	O
are	O
private	O
,	O
distinguishing	O
this	O
corpus	O
from	O
public	O
sources	O
like	O
LiveJournal	O
,	O
where	O
the	O
content	O
of	O
posts	O
might	O
be	O
influenced	O
by	O
considerations	O
of	O
self	O
-	O
presentation	O
.	O
Figure	O
1	O
shows	O
a	O
RECORDING	O
and	O
REFLECTION	O
from	O
two	O
users	O
,	O
after	O
binning	O
the	O
happiness	O
ratings	O
into	O
positive	O
and	O
negative	O
.	O
Our	O
goal	O
is	O
to	O
ground	O
the	O
linguistic	O
descriptions	O
of	O
events	O
that	O
users	O
experience	O
,	O
such	O
as	O
those	O
in	O
Figure	O
1	O
,	O
in	O
theories	O
of	O
well	O
-	O
being	O
and	O
happiness	O
.	O
Without	O
such	O
a	O
grounding	O
,	O
it	O
is	O
difficult	O
for	O
the	O
ECHO	O
system	O
to	O
make	O
recommendations	O
to	O
users	O
to	O
improve	O
their	O
well	O
-	O
being	O
,	O
or	O
to	O
explain	O
the	O
relationships	O
between	O
different	O
event	O
types	O
and	O
well	O
-	O
being	O
,	O
or	O
to	O
develop	O
a	O
policy	O
that	O
can	O
do	O
a	O
good	O
job	O
of	O
selecting	O
events	O
for	O
targeted	O
reflection	O
(	O
Konrad	O
et	O
al	O
,	O
2015	O
;	O
Isaacs	O
et	O
al	O
,	O
2013	O
)	O
.	O
That	O
is	O
,	O
for	O
ECHO	O
's	O
purposes	O
,	O
we	O
need	O
techniques	O
that	O
not	O
only	O
reliably	O
categorize	O
a	O
user	O
's	O
scalar	O
happiness	O
level	O
,	O
but	O
are	O
explanatory	O
with	O
respect	O
to	O
the	O
sources	O
of	O
that	O
happiness	O
level	O
.	O
There	O
are	O
two	O
principal	O
challenges	O
to	O
this	O
goal	O
.	O
First	O
,	O
different	O
theories	O
posit	O
different	O
sources	O
for	O
feelings	O
of	O
well	O
-	O
being	O
and	O
happiness	O
.	O
Second	O
,	O
the	O
relevant	O
computational	O
resources	O
for	O
sentiment	O
or	O
mood	O
are	O
primarily	O
lexically	O
based	O
,	O
while	O
many	O
of	O
the	O
events	O
can	O
only	O
be	O
characterized	O
well	O
via	O
their	O
compositional	O
semantics	O
(	O
Reschke	O
and	O
Anand	O
,	O
2011	O
)	O
.	O
Other	O
research	O
also	O
shares	O
our	O
motivation	O
of	O
understanding	O
the	O
relationship	O
between	O
what	O
people	O
say	O
and	O
their	O
levels	O
of	O
happiness	O
and	O
related	O
moods	O
.	O
Mishne	O
(	O
2005	O
)	O
used	O
a	O
corpus	O
of	O
340	O
,	O
000	O
posts	O
from	O
Livejournal	O
that	O
were	O
self	O
-	O
annotated	O
with	O
the	O
40	O
most	O
common	O
moods	O
.	O
Lexical	O
features	O
alone	O
improved	O
classification	O
accuracy	B-MetricName
by	O
6	O
to	O
15	O
%	O
over	O
a	O
balanced	O
baseline	O
.	O
These	O
results	O
were	O
then	O
improved	O
considerably	O
(	O
Keshtkar	O
and	O
Inkpen	O
,	O
2009	O
)	O
.	O
Mihalcea	O
and	O
Liu	O
(	O
2006	O
)	O
experimented	O
with	O
the	O
subset	O
of	O
happy	O
/	O
sad	O
posts	O
,	O
and	O
used	O
conditional	O
probability	O
to	O
explore	O
the	O
"	O
happiness	O
factor	O
"	O
of	O
various	O
terms	O
,	O
and	O
the	O
relationship	O
of	O
these	O
terms	O
to	O
well	O
-	O
being	O
categories	O
such	O
as	O
human	O
-	O
centeredness	O
and	O
socialness	O
.	O
Schwartz	O
et	O
al	O
(	O
2016	O
)	O
extract	O
5	O
,	O
100	O
public	O
status	O
updates	O
on	O
Facebook	O
and	O
have	O
Turkers	O
annotate	O
them	O
using	O
Seligman	O
's	O
dimensions	O
for	O
well	O
-	O
being	O
:	O
Positive	O
Emotions	O
,	O
Engagement	O
,	O
Relationships	O
,	O
Meaning	O
,	O
and	O
Accomplish	O
(	O
Seligman	O
et	O
al	O
,	O
2006	O
;	O
Forgeard	O
et	O
al	O
,	O
2011	O
)	O
.	O
They	O
then	O
predict	O
each	O
dimension	O
with	O
lexical	O
and	O
LDA	B-MethodName
topic	O
features	O
.	O
A	O
related	O
line	O
of	O
work	O
builds	O
lexico	O
-	O
semantic	O
resources	O
for	O
sentiment	B-TaskName
analysis	I-TaskName
with	O
a	O
focus	O
on	O
how	O
the	O
participants	O
of	O
an	O
event	O
are	O
affected	O
by	O
it	O
.	O
Goyal	O
and	O
Riloff	O
(	O
2013	O
)	O
bootstrap	O
a	O
set	O
of	O
patientpolarity	O
verbs	O
from	O
narratives	O
and	O
Ding	O
and	O
Riloff	O
(	O
2016	O
)	O
extract	O
event	O
-	O
triples	O
from	O
blogs	O
that	O
reliably	O
indicate	O
positive	O
or	O
negative	O
affect	O
on	O
one	O
of	O
the	O
event	O
participants	O
.	O
Reed	O
et	O
al	O
(	O
2017	O
)	O
take	O
a	O
similar	O
approach	O
.	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
(	O
2013	O
)	O
annotate	O
how	O
participants	O
of	O
an	O
event	O
are	O
affected	O
,	O
and	O
Deng	O
&	O
Wiebe	O
(	O
2014	O
)	O
show	O
that	O
this	O
assists	O
inference	O
about	O
the	O
author	O
's	O
sentiment	O
towards	O
entities	O
or	O
events	O
.	O
Balahur	O
et	O
al	O
(	O
2012	O
)	O
use	O
the	O
narratives	O
produced	O
by	O
the	O
ISEAR	B-DatasetName
questionnaire	O
(	O
Scherer	O
et	O
al	O
,	O
1986	O
)	O
for	O
first	O
-	O
person	O
examples	O
of	O
particular	O
emotions	O
(	O
"	O
I	O
felt	O
angry	O
when	O
X	O
and	O
then	O
Y	O
happened	O
"	O
)	O
and	O
extract	O
sequences	O
of	O
subject	O
-	O
verbobject	O
triples	O
,	O
which	O
they	O
then	O
annotate	O
for	O
seven	O
basic	O
emotions	O
.	O
Choi	O
&	O
Wiebe	O
(	O
2014	O
)	O
use	O
Word	O
-	O
Net	O
to	O
try	O
to	O
learn	O
similar	O
patterns	O
,	O
and	O
Rupenhofer	O
&	O
Brandes	O
(	O
2015	O
)	O
annotate	O
synsets	O
in	O
Ger	O
-	O
maNet	O
based	O
on	O
an	O
event	O
decomposition	O
framework	O
.	O
Russo	O
et	O
al	O
(	O
2015	O
)	O
proposed	O
a	O
shared	O
task	O
for	O
recognition	O
of	O
a	O
set	O
of	O
pleasant	O
and	O
unpleasant	O
events	O
from	O
a	O
clinical	O
framework	O
for	O
well	O
-	O
being	O
(	O
MacPhillamy	O
and	O
Lewinsohn	O
,	O
1982	O
)	O
.	O
Work	O
on	O
AFINN	O
,	O
SentiWordNet	O
and	O
the	O
Connotation	O
Lexicon	O
also	O
aim	O
to	O
refine	O
existing	O
sentiment	O
resources	O
to	O
capture	O
more	O
subtle	O
notions	O
of	O
sentiment	O
(	O
Feng	O
et	O
al	O
,	O
2013	O
;	O
Kang	O
et	O
al	O
,	O
2014	O
;	O
Baccianella	O
et	O
al	O
,	O
2010	O
;	O
Nielsen	O
,	O
2011	O
)	O
.	O
Here	O
we	O
report	O
an	O
exploratory	O
study	O
where	O
we	O
synthesize	O
theoretical	O
constructs	O
associated	O
with	O
well	O
-	O
being	O
and	O
happiness	O
from	O
different	O
sources	O
.	O
We	O
then	O
develop	O
several	O
methods	O
for	O
characterizing	O
events	O
in	O
terms	O
of	O
these	O
theories	O
.	O
We	O
examine	O
the	O
extent	O
to	O
which	O
different	O
theoretical	O
accounts	O
can	O
explain	O
the	O
variance	O
in	O
the	O
happiness	O
scores	O
in	O
ECHO	O
.	O
We	O
show	O
that	O
each	O
theory	O
explains	O
a	O
part	O
of	O
the	O
variance	O
,	O
but	O
that	O
our	O
event	O
characterizations	O
need	O
to	O
be	O
more	O
fine	O
-	O
grained	O
.	O
We	O
show	O
that	O
several	O
recurrent	O
event	O
types	O
which	O
affect	O
people	O
's	O
feelings	O
of	O
well	O
-	O
being	O
,	O
such	O
as	O
OBLIGATION	O
and	O
INCOMPETENCE	O
,	O
are	O
not	O
captured	O
in	O
current	O
lexical	O
or	O
semantic	O
resources	O
.	O

ECHO	O
is	O
designed	O
to	O
encourage	O
users	O
to	O
react	O
to	O
daily	O
events	O
as	O
well	O
as	O
to	O
periodically	O
reflect	O
on	O
past	O
events	O
(	O
Isaacs	O
et	O
al	O
,	O
2013	O
)	O
.	O
Figure	O
2	O
depicts	O
the	O
user	O
interface	O
,	O
showing	O
a	O
RECORDING	O
from	O
today	O
,	O
as	O
well	O
as	O
prompts	O
to	O
reflect	O
on	O
events	O
from	O
the	O
past	O
.	O
ECHO	O
has	O
been	O
deployed	O
with	O
134	O
users	O
,	O
in	O
three	O
different	O
experiments	O
on	O
well	O
-	O
being	O
(	O
Konrad	O
et	O
al	O
,	O
2016b	O
,	O
a	O
)	O
.	O
The	O
total	O
corpus	O
consists	O
of	O
10354	O
posts	O
,	O
where	O
7573	O
are	O
RECORDINGS	O
and	O
2781	O
are	O
REFLECTIONS	O
.	O
While	O
the	O
corpus	O
could	O
be	O
considered	O
relatively	O
small	O
,	O
these	O
posts	O
provide	O
a	O
window	O
onto	O
users	O
'	O
private	O
thoughts	O
as	O
opposed	O
to	O
what	O
users	O
are	O
willing	O
to	O
make	O
public	O
on	O
social	O
media	O
.	O
In	O
addtion	O
,	O
the	O
annotations	O
for	O
happiness	O
are	O
provided	O
by	O
the	O
user	O
,	O
the	O
first	O
-	O
person	O
experiencer	O
,	O
and	O
not	O
by	O
a	O
third	O
party	O
.	O
Our	O
aim	O
is	O
to	O
explain	O
users	O
'	O
emotional	O
reactions	O
to	O
different	O
categories	O
of	O
events	O
mentioned	O
in	O
ECHO	O
posts	O
,	O
linking	O
the	O
user	O
reactions	O
directly	O
to	O
theories	O
of	O
well	O
-	O
being	O
as	O
exemplified	O
in	O
Table	O
1	O
.	O
Influential	O
accounts	O
such	O
as	O
Appraisal	O
Theory	O
(	O
Scherer	O
et	O
al	O
,	O
2001	O
(	O
Scherer	O
et	O
al	O
,	O
,	O
1986Ortony	O
et	O
al	O
,	O
1990	O
)	O
of	O
successfully	O
achieving	O
goals	O
.	O
Appraisal	O
theory	O
posits	O
that	O
goal	O
achievement	O
promotes	O
positive	O
affect	O
,	O
which	O
then	O
serves	O
to	O
reinforce	O
the	O
relevant	O
behavior	O
.	O
Row	O
2	O
provides	O
an	O
example	O
of	O
failing	O
to	O
achieve	O
an	O
important	O
personal	O
goal	O
,	O
which	O
is	O
posited	O
to	O
promote	O
negative	O
affect	O
,	O
motivating	O
people	O
to	O
modify	O
current	O
behaviors	O
to	O
change	O
that	O
negative	O
outcome	O
.	O
There	O
are	O
significant	O
critiques	O
of	O
the	O
adaptive	O
goal	O
-	O
based	O
account	O
espoused	O
in	O
Appraisal	O
theory	O
.	O
Appraisal	O
theory	O
focuses	O
on	O
short	O
-	O
term	O
personal	O
goals	O
,	O
but	O
Eudaimonic	O
psychologists	O
instead	O
focus	O
on	O
what	O
determines	O
long	O
-	O
term	O
happiness	O
.	O
Eudaimonic	O
theorists	O
suggest	O
that	O
certain	O
fundamental	O
psychological	O
needs	O
have	O
to	O
be	O
satisfied	O
for	O
people	O
to	O
experience	O
sustained	O
positive	O
long	O
-	O
term	O
emotions	O
.	O
Self	O
-	O
determination	O
theory	O
argues	O
that	O
there	O
are	O
3	O
basic	O
psychological	O
needs	O
:	O
AUTONOMY	O
,	O
COMPETENCE	O
and	O
CONNECTION	O
(	O
Deci	O
and	O
Ryan	O
,	O
2010	O
;	O
Ryan	O
and	O
Deci	O
,	O
2000	O
;	O
Bandura	O
,	O
1977	O
)	O
.	O
We	O
add	O
these	O
to	O
our	O
inventory	O
in	O
Table	O
1	O
in	O
Rows	O
3	O
to	O
8	O
.	O
According	O
to	O
self	O
-	O
determination	O
theory	O
,	O
satisfaction	O
of	O
these	O
basic	O
needs	O
results	O
in	O
positive	O
emotions	O
.	O
Row	O
3	O
describes	O
a	O
good	O
day	O
at	O
work	O
.	O
Row	O
5	O
describes	O
feeling	O
competent	O
because	O
hard	O
work	O
led	O
to	O
an	O
achievement	O
,	O
and	O
Row	O
7	O
describes	O
feeling	O
connected	O
with	O
family	O
.	O
On	O
the	O
other	O
hand	O
,	O
if	O
these	O
basic	O
needs	O
are	O
not	O
satisfied	O
,	O
then	O
negative	O
emotions	O
will	O
regularly	O
arise	O
.	O
For	O
example	O
,	O
obligations	O
to	O
do	O
things	O
one	O
does	O
not	O
feel	O
like	O
doing	O
(	O
Row	O
4	O
)	O
,	O
or	O
a	O
job	O
that	O
does	O
not	O
engage	O
personal	O
decision	B-TaskName
making	I-TaskName
or	O
involvement	O
(	O
lack	O
of	O
autonomy	O
)	O
can	O
make	O
one	O
feel	O
unhappy	O
.	O
Similarly	O
,	O
people	O
may	O
feel	O
unhappy	O
due	O
to	O
an	O
experience	O
where	O
the	O
demands	O
of	O
the	O
situation	O
outstrip	O
one	O
's	O
basic	O
abilities	O
,	O
such	O
as	O
doing	O
poorly	O
on	O
a	O
test	O
(	O
lack	O
of	O
competence	O
)	O
,	O
as	O
in	O
Row	O
6	O
.	O
In	O
addition	O
,	O
bad	O
things	O
happening	O
to	O
friends	O
(	O
Row	O
8	O
)	O
as	O
well	O
as	O
separation	O
from	O
family	O
or	O
friends	O
often	O
reduces	O
happiness	O
(	O
lack	O
of	O
connection	O
)	O
.	O
In	O
addition	O
,	O
there	O
is	O
strong	O
evidence	O
from	O
SAVOURING	O
theory	O
(	O
Jose	O
et	O
al	O
,	O
2012	O
;	O
Bryant	O
et	O
al	O
,	O
2011	O
)	O
arguing	O
that	O
people	O
often	O
experience	O
highly	O
positive	O
or	O
negative	O
emotions	O
arising	O
from	O
situations	O
that	O
are	O
n't	O
directly	O
goal	O
-	O
related	O
,	O
and	O
that	O
relate	O
more	O
directly	O
to	O
basic	O
drives	O
(	O
Maslow	O
,	O
1943	O
;	O
Elson	O
,	O
2012	O
)	O
.	O
For	O
example	O
,	O
experiences	O
such	O
as	O
eating	O
,	O
experiencing	O
nature	O
,	O
sex	O
and	O
physical	O
exercise	O
tend	O
to	O
engender	O
positive	O
emotions	O
,	O
whereas	O
pain	O
,	O
discomfort	O
and	O
inactivity	O
have	O
the	O
opposite	O
effects	O
,	O
and	O
these	O
are	O
documented	O
in	O
results	O
from	O
happiness	O
surveys	O
(	O
Kahneman	O
et	O
al	O
,	O
2004	O
;	O
Seligman	O
et	O
al	O
,	O
2006	O
)	O
.	O
Thus	O
while	O
experiences	O
such	O
as	O
eating	O
may	O
serve	O
the	O
survival	O
goal	O
of	O
preventing	O
starvation	O
,	O
avoiding	O
starvation	O
is	O
unlikely	O
to	O
be	O
a	O
direct	O
personal	O
goal	O
every	O
time	O
we	O
eat	O
,	O
suggesting	O
that	O
such	O
experiences	O
are	O
not	O
explained	O
by	O
Appraisal	O
theory	O
.	O
Similar	O
arguments	O
have	O
been	O
made	O
by	O
Lewinsohn	O
and	O
colleagues	O
who	O
have	O
shown	O
that	O
encouraging	O
people	O
to	O
engage	O
in	O
certain	O
simple	O
activities	O
(	O
shopping	O
,	O
mowing	O
the	O
lawn	O
,	O
driving	O
,	O
personal	O
hygiene	O
)	O
have	O
quite	O
predictable	O
effects	O
on	O
mood	O
without	O
engaging	O
significant	O
personal	O
goals	O
(	O
MacPhillamy	O
and	O
Lewinsohn	O
,	O
1982	O
;	O
Lewinsohn	O
et	O
al	O
,	O
1985	O
;	O
Lewinsohn	O
and	O
Amenson	O
,	O
1978	O
)	O
.	O
We	O
start	O
with	O
the	O
10354	O
posts	O
from	O
the	O
ECHO	O
corpus	O
and	O
map	O
happiness	O
scores	O
between	O
[	O
1	O
,	O
4	O
]	O
to	O
negative	O
,	O
and	O
scores	O
between	O
[	O
6	O
,	O
9	O
]	O
to	O
positive	O
.	O
For	O
posts	O
labelled	O
5	O
by	O
the	O
experiencer	O
,	O
we	O
categorize	O
it	O
as	O
negative	O
if	O
its	O
REFLECTION	O
score	O
decreases	O
to	O
lower	O
than	O
5	O
,	O
and	O
positive	O
if	O
its	O
REFLECTION	O
score	O
increases	O
.	O
We	O
label	O
the	O
rest	O
of	O
the	O
5s	O
as	O
neutral	O
,	O
and	O
leave	O
them	O
aside	O
.	O
We	O
then	O
have	O
5997	O
positive	O
posts	O
and	O
3573	O
negative	O
posts	O
.	O
We	O
randomly	O
sample	O
2868	O
posts	O
as	O
training	O
data	O
,	O
and	O
478	O
as	O
test	O
data	O
.	O
We	O
keep	O
the	O
rest	O
of	O
the	O
6224	O
posts	O
untouched	O
for	O
future	O
work	O
.	O
Then	O
we	O
split	O
the	O
posts	O
into	O
sentences	O
.	O
Table	O
2	O
shows	O
the	O
splits	O
for	O
each	O
class	O
.	O
We	O
first	O
test	O
the	O
separability	O
of	O
the	O
positive	O
and	O
negative	O
sentences	O
with	O
an	O
SVM	B-MethodName
classifier	O
from	O
Weka	O
3.8	O
,	O
using	O
as	O
baselines	O
only	O
unigrams	O
and	O
LIWC	O
(	O
Pennebaker	O
et	O
al	O
,	O
2001	O
)	O
illustrating	O
that	O
the	O
positive	O
and	O
negative	O
classes	O
can	O
be	O
separated	O
with	O
F1	B-MetricName
above	O
.70	O
,	O
and	O
that	O
both	O
unigrams	O
and	O
LIWC	O
perform	O
worse	O
on	O
the	O
negative	O
class	O
.	O
However	O
,	O
as	O
discussed	O
above	O
,	O
the	O
word	O
level	O
representations	O
of	O
the	O
features	O
in	O
the	O
baselines	O
do	O
not	O
help	O
us	O
with	O
our	O
goal	O
to	O
understand	O
how	O
linguistic	O
descriptions	O
of	O
events	O
that	O
affect	O
wellbeing	O
map	O
onto	O
theoretical	O
constructs	O
.	O
and	O
LIWC	O
categories	O
.	O
We	O
can	O
not	O
recommend	O
to	O
an	O
ECHO	O
user	O
that	O
they	O
should	O
for	O
example	O
,	O
try	O
to	O
use	O
the	O
word	O
why	O
less	O
(	O
Row	O
7	O
)	O
because	O
it	O
is	O
correlated	O
with	O
negative	O
feelings	O
,	O
or	O
try	O
to	O
use	O
less	O
negation	O
(	O
Rows	O
9	O
and	O
10	O
)	O
.	O
It	O
is	O
difficult	O
to	O
associate	O
these	O
features	O
with	O
well	O
-	O
being	O
classes	O
.	O
Even	O
in	O
cases	O
where	O
the	O
words	O
seem	O
to	O
be	O
strongly	O
related	O
to	O
a	O
well	O
-	O
being	O
category	O
,	O
a	O
single	O
word	O
typically	O
fails	O
to	O
provide	O
enough	O
information	O
,	O
e.g.	O
,	O
"	O
it	O
was	O
fun	O
talking	O
to	O
him	O
"	O
and	O
"	O
worked	O
on	O
a	O
fun	O
project	O
"	O
belong	O
to	O
different	O
well	O
-	O
being	O
classes	O
.	O
Moreover	O
,	O
the	O
mapping	O
of	O
LIWC	O
categories	O
to	O
words	O
are	O
many	O
-	O
to	O
-	O
many	O
,	O
e.g.	O
the	O
"	O
discrep	O
"	O
category	O
contains	O
words	O
related	O
to	O
both	O
Goals	O
and	O
Autonomy	O
.	O
We	O
posit	O
that	O
we	O
need	O
compositional	O
semantic	O
features	O
to	O
ground	O
our	O
a	O
Well	O
-	O
Being	O
classification	O
of	O
events	O
.	O
We	O
thus	O
explore	O
two	O
different	O
methods	O
for	O
mapping	O
these	O
well	O
-	O
being	O
event	O
categories	O
into	O
lexical	O
descriptions	O
,	O
one	O
of	O
which	O
is	O
top	O
-	O
down	O
and	O
the	O
other	O
which	O
is	O
bottom	O
-	O
up	O
.	O
Our	O
top	O
-	O
down	O
method	O
is	O
based	O
on	O
mapping	O
general	O
event	O
types	O
from	O
FrameNet	B-DatasetName
to	O
the	O
theoretical	O
categories	O
enumerated	O
in	O
Table	O
1	O
.	O
We	O
take	O
frame	O
specific	O
features	O
for	O
each	O
theoretical	O
category	O
from	O
the	O
lexical	O
units	O
for	O
each	O
frame	O
.	O
For	O
example	O
,	O
GOALS	O
are	O
often	O
dis	O
-	O
cussed	O
in	O
terms	O
of	O
specific	O
frames	O
from	O
the	O
Desiring	O
and	O
the	O
Intentionally	O
act	O
classes	O
,	O
as	O
shown	O
in	O
the	O
first	O
two	O
rows	O
of	O
Table	O
6	O
.	O
We	O
show	O
that	O
FrameNet	B-DatasetName
features	O
do	O
provide	O
an	O
interesting	O
level	O
of	O
generalization	O
but	O
much	O
of	O
the	O
compositional	O
semantics	O
of	O
events	O
is	O
still	O
missing	O
from	O
this	O
characterization	O
(	O
Section	O
4	O
)	O
.	O
Thus	O
,	O
our	O
bottom	O
-	O
up	O
method	O
applies	O
the	O
AutoSlog	O
linguistic	O
-	O
pattern	O
learner	O
to	O
induce	O
lexically	O
-	O
grounded	O
predicate	O
patterns	O
from	O
the	O
ECHO	O
data	O
(	O
Section	O
5	O
)	O
.	O
We	O
show	O
how	O
many	O
light	O
verbs	O
acquire	O
a	O
specific	O
semantics	O
with	O
their	O
arguments	O
,	O
and	O
how	O
common	O
events	O
like	O
"	O
Talking	O
"	O
are	O
separated	O
into	O
positive	O
and	O
negative	O
events	O
depending	O
on	O
whether	O
they	O
are	O
"	O
Talking	O
about	O
"	O
or	O
"	O
Talking	O
with	O
"	O
.	O

Table	O
6	O
provides	O
our	O
posited	O
mapping	O
from	O
frame	O
categories	O
to	O
the	O
appraisal	O
category	O
of	O
GOALS	O
as	O
well	O
as	O
to	O
the	O
eudaimonic	O
categories	O
of	O
AUTON	O
-	O
OMY	O
,	O
COMPETENCE	O
and	O
CONNECTION	O
,	O
and	O
to	O
the	O
hedonic	O
category	O
of	O
SAVOURING	O
.	O
To	O
develop	O
features	O
related	O
to	O
these	O
frame	O
categories	O
,	O
we	O
apply	O
SEMAFOR	O
(	O
Das	O
et	O
al	O
,	O
2013	O
)	O
to	O
label	O
the	O
ECHO	O
posts	O
with	O
their	O
corresponding	O
frames	O
using	O
FrameNet	B-DatasetName
1.5	O
(	O
Baker	O
et	O
al	O
,	O
2015	O
;	O
Baker	O
,	O
2014	O
)	O
.	O
We	O
partition	O
frame	O
features	O
into	O
subsets	O
corresponding	O
to	O
the	O
different	O
theoretical	O
constructs	O
as	O
defined	O
in	O
Table	O
6	O
.	O
We	O
acknowledge	O
that	O
our	O
mapping	O
may	O
not	O
be	O
perfect	O
,	O
and	O
that	O
some	O
frames	O
could	O
conceivably	O
be	O
categorized	O
as	O
both	O
goal	O
related	O
and	O
eudaimonic	O
.	O
We	O
train	O
an	O
SVM	B-MethodName
with	O
each	O
feature	O
subset	O
,	O
and	O
evaluate	O
the	O
models	O
on	O
our	O
test	O
set	O
,	O
with	O
results	O
in	O
Table	O
7	O
.	O
The	O
general	O
ALL	O
FRAME	O
feature	O
is	O
also	O
listed	O
for	O
comparison	O
.	O
The	O
.67	O
F1	B-MetricName
of	O
FRAME	O
is	O
slightly	O
lower	O
than	O
LIWC	O
in	O
Table	O
3	O
,	O
but	O
in	O
our	O
view	O
,	O
more	O
interpretable	O
.	O
In	O
addition	O
,	O
the	O
average	O
count	O
of	O
FRAME	O
features	O
per	O
sentence	O
is	O
an	O
order	O
of	O
magnitude	O
less	O
than	O
LIWC	O
features	O
(	O
hence	O
,	O
much	O
less	O
than	O
unigram	O
features	O
)	O
,	O
suggesting	O
the	O
targeted	O
power	O
of	O
these	O
features	O
.	O
See	O
Table	O
8	O
.	O
We	O
posit	O
that	O
FRAMES	O
are	O
thus	O
more	O
discriminative	O
than	O
LIWC	O
for	O
well	O
-	O
being	O
classes	O
,	O
and	O
that	O
FRAME	O
features	O
are	O
more	O
naturally	O
categorized	O
into	O
wellbeing	O
categories	O
at	O
a	O
semantic	O
level	O
.	O
The	O
Goals	O
section	O
of	O
Table	O
7	O
shows	O
that	O
Appraisal	O
theory	O
does	O
well	O
at	O
predicting	O
positive	O
events	O
,	O
but	O
performs	O
poorly	O
for	O
negative	O
events	O
,	O
primarily	O
due	O
to	O
low	O
recall	O
.	O
All	O
features	O
achieve	O
good	O
F1	B-MetricName
for	O
the	O
positive	O
class	O
,	O
but	O
not	O
the	O
negative	O
class	O
.	O
This	O
is	O
consistent	O
with	O
the	O
results	O
in	O
Table	O
3	O
.	O
The	O
EUDAIMONIC	O
features	O
include	O
Autonomy	O
&	O
Obligation	O
,	O
Competence	O
and	O
Connection	O
.	O
The	O
SVM	B-MethodName
trained	O
with	O
just	O
eudaimonic	O
features	O
produces	O
the	O
highest	O
F1	B-MetricName
score	I-MetricName
for	O
the	O
negative	O
class	O
,	O
highlighting	O
the	O
role	O
of	O
eudaimonic	O
related	O
events	O
in	O
negative	O
well	O
-	O
being	O
.	O
See	O
Table	O
7	O
.	O
The	O
results	O
for	O
an	O
breaking	O
eudaimonic	O
into	O
its	O
constituent	O
categories	O
is	O
in	O
Table	O
9	O
.	O
The	O
results	O
show	O
that	O
most	O
of	O
our	O
autonomy	O
categories	O
are	O
related	O
to	O
negative	O
autonomy	O
,	O
to	O
obligations	O
that	O
cause	O
feelings	O
of	O
negative	O
well	O
-	O
being	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
results	O
indicate	O
that	O
competence	O
and	O
connection	O
play	O
a	O
large	O
role	O
in	O
positive	O
well	O
-	O
being	O
.	O
The	O
top	O
25	O
most	O
informative	O
frame	O
features	O
are	O
illustrated	O
in	O
Table	O
10	O
(	O
out	O
of	O
639	O
instantiated	O
in	O
ECHO	O
)	O
.	O
These	O
illustrate	O
general	O
events	O
for	O
well	O
-	O
being	O
,	O
but	O
compositional	O
differences	O
,	O
such	O
as	O
"	O
spending	O
my	O
nights	O
by	O
the	O
side	O
of	O
my	O
textbook	O
"	O
and	O
"	O
spending	O
my	O
nights	O
with	O
friends	O
"	O
are	O
not	O
captured	O
.	O
The	O
first	O
"	O
spend	O
(	O
time	O
)	O
"	O
evokes	O
the	O
theoretical	O
construct	O
of	O
obligation	O
,	O
while	O
"	O
spend	O
(	O
time	O
with	O
)	O
"	O
is	O
related	O
to	O
connection	O
.	O

We	O
also	O
apply	O
Autoslog	O
-	O
TS	B-MethodName
,	O
a	O
weakly	O
supervised	O
linguistic	O
-	O
pattern	O
learner	O
as	O
a	O
way	O
of	O
learning	O
some	O
compositional	O
patterns	O
.	O
Autoslog	O
only	O
requires	O
training	O
documents	O
labeled	O
broadly	O
into	O
our	O
two	O
classes	O
of	O
POSITIVE	O
or	O
NEGATIVE	O
.	O
The	O
learner	O
uses	O
a	O
set	O
of	O
syntactic	O
templates	O
to	O
define	O
different	O
types	O
of	O
linguistic	O
expressions	O
.	O
In	O
general	O
,	O
this	O
method	O
tends	O
to	O
produce	O
high	O
precision	O
(	O
and	O
potentially	O
low	O
recall	O
)	O
markers	O
of	O
the	O
particular	O
classes	O
that	O
can	O
seed	O
further	O
hypothesizing	O
.	O
The	O
left	O
-	O
hand	O
side	O
of	O
a	O
specific	O
lexico	O
-	O
syntactic	O
pattern	O
(	O
in	O
bold	O
)	O
that	O
represents	O
an	O
instantiation	O
of	O
each	O
general	O
pattern	O
template	O
for	O
learning	O
well	O
-	O
being	O
patterns	O
in	O
our	O
data	O
.	O
2	O
In	O
order	O
to	O
enable	O
selection	O
of	O
particular	O
patterns	O
,	O
AutoSlog	O
-	O
TS	B-MethodName
computes	O
statistics	O
on	O
the	O
strength	O
of	O
association	O
of	O
each	O
pattern	O
with	O
each	O
class	O
,	O
i.e.	O
P	O
(	O
POSITIVE	O
|	O
p	O
)	O
and	O
P	O
(	O
NEGATIVE	O
|	O
p	O
)	O
,	O
along	O
with	O
the	O
pattern	O
's	O
overall	O
frequency	O
.	O
We	O
define	O
two	O
tuning	O
parameters	O
for	O
each	O
class	O
:	O
θ	B-HyperparameterName
f	O
,	O
the	O
frequency	O
with	O
which	O
a	O
pattern	O
occurs	O
,	O
θ	B-HyperparameterName
p	O
,	O
the	O
probability	O
with	O
which	O
a	O
pattern	O
is	O
associated	O
with	O
the	O
given	O
class	O
.	O
AutoSlog	O
lets	O
us	O
systematically	O
explore	O
tradeoffs	O
with	O
precision	O
and	O
recall	O
.	O
Here	O
we	O
select	O
θ	B-HyperparameterName
f	O
and	O
θ	B-HyperparameterName
p	O
to	O
optimize	O
F1	B-MetricName
on	O
our	O
test	O
set	O
.	O
For	O
more	O
detail	O
,	O
see	O
(	O
Riloff	O
,	O
1996	O
;	O
Oraby	O
et	O
al	O
,	O
2015	O
)	O
.	O
Our	O
primary	O
interest	O
here	O
is	O
Autoslog	O
's	O
ability	O
to	O
learn	O
compositional	O
patterns	O
.	O
Autoslog	O
can	O
,	O
in	O
principle	O
,	O
provide	O
three	O
kinds	O
of	O
information	O
:	O
i	O
)	O
it	O
can	O
provide	O
supplement	O
the	O
lexical	O
units	O
for	O
a	O
given	O
frame	O
;	O
ii	O
)	O
it	O
can	O
supplement	O
the	O
frames	O
in	O
a	O
well	O
-	O
being	O
category	O
;	O
and	O
iii	O
)	O
it	O
can	O
reveal	O
reliable	O
markers	O
of	O
mood	O
that	O
well	O
-	O
being	O
categories	O
do	O
not	O
capture	O
.	O
Because	O
our	O
interest	O
in	O
frames	O
is	O
ultimately	O
as	O
a	O
way	O
of	O
relating	O
well	O
-	O
being	O
categories	O
with	O
linguistic	O
signals	O
,	O
we	O
will	O
not	O
distinguish	O
(	O
i	O
)	O
and	O
(	O
ii	O
)	O
here	O
.	O
Here	O
we	O
discuss	O
all	O
patterns	O
with	O
a	O
θ	B-HyperparameterName
p	O
>	O
.7	O
Several	O
lexicosyntactic	O
patterns	O
fit	O
within	O
our	O
wellbeing	O
categories	O
but	O
are	O
not	O
captured	O
by	O
frames	O
,	O
while	O
as	O
expected	O
there	O
are	O
overlaps	O
between	O
FrameNet	B-DatasetName
and	O
Autoslog	O
as	O
well	O
.	O
Examples	O
are	O
listed	O
in	O
Table	O
12	O
.	O
One	O
large	O
class	O
includes	O
straightforward	O
lexical	O
patterns	O
:	O
FINISHED	O
,	O
FIN	B-DatasetName
-	O
ISH	O
,	O
and	O
FINALLY	O
which	O
we	O
associate	O
with	O
feelings	O
of	O
comptence	O
.	O
Verbal	O
patterns	O
with	O
EAT	O
and	O
ATE	O
indicate	O
savouring	O
,	O
with	O
NOT	O
EAT	O
reliably	O
marking	O
negative	O
sentences	O
.	O
The	O
frames	O
also	O
show	O
many	O
specific	O
types	O
of	O
food	O
(	O
cake	O
)	O
,	O
and	O
we	O
use	O
a	O
comprehensive	O
list	O
from	O
DBpedia	B-DatasetName
(	O
Lehmann	O
et	O
al	O
,	O
2014	O
)	O
to	O
collapse	O
all	O
these	O
to	O
the	O
general	O
type	O
FOOD	O
,	O
allowing	O
us	O
to	O
develop	O
patterns	O
such	O
as	O
MADE	O
FOOD	O
.	O
Autoslog	O
also	O
discovers	O
many	O
patterns	O
syntactically	O
linking	O
content	O
(	O
nouns	O
and	O
verbs	O
)	O
and	O
function	O
words	O
(	O
e.g.	O
,	O
prepositions	O
and	O
light	O
verbs	O
)	O
.	O
It	O
thus	O
furnishes	O
a	O
ready	O
source	O
for	O
multi	O
-	O
word	O
,	O
partially	O
compositional	O
expressions	O
of	O
positivity	O
or	O
negativity	O
.	O
In	O
what	O
follows	O
,	O
we	O
provide	O
some	O
examples	O
(	O
note	O
that	O
in	O
the	O
patterns	O
below	O
,	O
expressions	O
in	O
brackets	O
are	O
used	O
to	O
indicate	O
expressions	O
not	O
part	O
of	O
the	O
pattern	O
that	O
correlate	O
with	O
it	O
in	O
the	O
data	O
)	O
.	O
There	O
are	O
262	O
positive	O
patterns	O
of	O
the	O
form	O
Verb	O
/	O
Noun	O
+	O
"	O
with	O
"	O
,	O
e.g.	O
There	O
are	O
36	O
patterns	O
with	O
the	O
string	O
'	O
go	O
'	O
,	O
12	O
positive	O
(	O
16	O
items	O
)	O
and	O
24	O
negative	O
(	O
40	O
items	O
)	O
.	O
There	O
are	O
34	O
patterns	O
involving	O
the	O
past	O
tense	O
form	O
"	O
went	O
"	O
,	O
which	O
reverses	O
the	O
polarity	O
to	O
25	O
positive	O
patterns	O
(	O
273	O
items	O
)	O
and	O
9	O
negative	O
(	O
9	O
items	O
)	O
.	O
Across	O
the	O
two	O
versions	O
of	O
the	O
lemma	B-DatasetName
,	O
the	O
positive	O
patterns	O
provide	O
several	O
expressions	O
for	O
savouring	O
(	O
WENT	O
/	O
GO	O
ON	O
/	O
FOR	O
[	O
a	O
walk	O
,	O
a	O
hike	O
,	O
a	O
ride	O
]	O
,	O
WENT	O
/	O
GO	O
SHOPPING	O
/	O
SWIMMING	O
,	O
WENT	O
/	O
GO	O
TO	O
[	O
the	O
mall	O
,	O
a	O
movie	O
]	O
)	O
.	O
For	O
the	O
negative	O
,	O
the	O
predominance	O
of	O
'	O
go	O
'	O
comes	O
from	O
the	O
fact	O
that	O
they	O
are	O
largely	O
negated	O
(	O
NOT	O
GO	O
TO	O
[	O
the	O
movies	O
]	O
)	O
or	O
in	O
infinitive	O
contexts	O
that	O
suggest	O
obligation	O
(	O
[	O
HAVE	O
TO	O
]	O
GO	O
TO	O
[	O
class	O
]	O
,	O
[	O
HAVE	O
TO	O
]	O
GO	O
WORK	O
)	O
.	O
Similarly	O
,	O
the	O
positive	O
class	O
contains	O
9	O
patterns	O
with	O
'	O
bought	O
'	O
and	O
1	O
with	O
'	O
buy	O
'	O
(	O
ENTICED	O
[	O
TO	O
]	O
BUY	O
)	O
and	O
the	O
negative	O
class	O
has	O
6	O
patterns	O
with	O
'	O
bought	O
'	O
and	O
16	O
with	O
'	O
buy	O
'	O
,	O
all	O
emphasizing	O
buying	O
necessities	O
(	O
BUY	O
GROCERIES	O
/	O
TICKET	O
,	O
NEED	O
/	O
WANT	O
BUY	O
,	O
NOT	O
BUY	O
)	O
Thus	O
,	O
even	O
though	O
these	O
expressions	O
all	O
involve	O
the	O
same	O
verbs	O
and	O
prepositions	O
,	O
the	O
surrounding	O
environments	O
,	O
as	O
reflected	O
in	O
the	O
form	O
of	O
the	O
verb	O
,	O
split	O
between	O
positive	O
and	O
negative	O
sentence	O
classes	O
.	O
There	O
are	O
73	O
bigram	O
patterns	O
of	O
the	O
form	O
NEW	O
X	O
,	O
56	O
positive	O
(	O
83	O
items	O
)	O
and	O
17	O
negative	O
(	O
21	O
items	O
)	O
.	O
In	O
general	O
,	O
the	O
positive	O
ones	O
describe	O
new	O
objects	O
-	O
SHIRT	O
,	O
SHEETS	O
,	O
COMPUTER	O
,	O
CLOTHES	O
,	O
TEA	O
-	O
and	O
acquaintances	O
(	O
NEW	O
FRIEND	O
)	O
,	O
thus	O
encompassing	O
both	O
Connection	O
and	O
possibly	O
Savouring	O
.	O
In	O
contrast	O
,	O
the	O
negative	O
patterns	O
describe	O
changes	O
to	O
routines	O
-	O
HABITS	O
,	O
school	O
QUARTER	O
,	O
PROFESSOR	O
,	O
LIVING	O
[	O
conditions	O
]	O
,	O
or	O
SCHEDULE	O
-	O
which	O
are	O
likely	O
to	O
engender	O
a	O
sense	O
of	O
instability	O
,	O
and	O
hence	O
be	O
Eudaimonically	O
negative	O
.	O
Thus	O
,	O
these	O
patterns	O
illustrate	O
that	O
Autoslog	O
can	O
serve	O
as	O
a	O
high	O
-	O
precision	O
method	O
of	O
building	O
additional	O
patterns	O
-	O
especially	O
compositional	O
onesfor	O
a	O
given	O
well	O
-	O
being	O
category	O
.	O

The	O
intuition	O
of	O
weighted	O
clustering	O
is	O
based	O
on	O
the	O
formulation	O
of	O
classical	O
LDA	B-MethodName
which	O
models	O
the	O
probability	O
of	O
the	O
word	O
type	O
t	O
belonging	O
to	O
a	O
topic	O
i	O
as	O
N	O
t	O
,	O
i	O
+	O
βt	O
t	O
N	O
t	O
i	O
+	O
β	B-HyperparameterName
t	O
,	O
where	O
N	O
t	O
,	O
i	O
refers	O
to	O
the	O
number	O
of	O
times	O
word	O
type	O
t	O
has	O
been	O
assigned	O
to	O
topic	O
i	O
,	O
and	O
β	B-HyperparameterName
is	O
a	O
parameter	O
of	O
the	O
Dirichlet	O
prior	O
on	O
the	O
pertopic	O
word	O
distribution	O
.	O
In	O
our	O
case	O
,	O
illustrated	O
by	O
the	O
schematic	O
in	O
Fig	O
.	O
1	O
,	O
weighting	O
is	O
a	O
natural	O
way	O
to	O
account	O
for	O
the	O
frequency	O
effects	O
of	O
vocabulary	O
terms	O
during	O
clustering	O
.	O

We	O
apply	O
PCA	B-MethodName
to	O
the	O
word	B-TaskName
embeddings	I-TaskName
before	O
clustering	O
to	O
investigate	O
the	O
amount	O
of	O
redundancy	O
in	O
the	O
dimensions	O
of	O
large	O
embeddings	O
,	O
which	O
impact	O
clustering	O
complexity	O
(	O
4	O
)	O
.	O
With	O
reranking	O
,	O
the	O
dimensions	O
of	O
all	O
embeddings	O
can	O
be	O
reduced	O
by	O
more	O
than	O
80	O
%	O
(	O
Fig	O
.	O
2	O
)	O
.	O
We	O
observe	O
that	O
KM	O
w	O
r	O
can	O
consistently	O
reduce	O
the	O
number	O
of	O
dimensions	O
across	O
different	O
embedding	O
types	O
without	O
loss	B-MetricName
of	O
performance	O
.	O
Although	O
GMM	O
w	O
does	O
not	O
require	O
reranking	O
for	O
good	O
performance	O
,	O
it	O
's	O
cubic	O
complexity	O
indicates	O
that	O
KM	O
w	O
r	O
might	O
be	O
preferred	O
in	O
practical	O
settings	O
.	O

We	O
present	O
the	O
results	O
for	O
using	O
different	O
reranking	O
schemes	O
for	O
KM	O
(	O
Table	O
5	O
)	O
and	O
Weighted	O
KM	O
for	O
Frequency	O
(	O
Table	O
6	O
)	O
.	O
We	O
can	O
see	O
that	O
compared	O
to	O
the	O
TF	O
results	O
in	O
the	O
main	O
paper	O
,	O
other	O
schemes	O
for	O
reranking	O
such	O
as	O
aggregated	O
TF	O
-	O
IDF	O
and	O
TF	O
-	O
DF	O
improve	O
over	O
the	O
original	O
hard	O
clustering	O
,	O
but	O
fare	O
worse	O
in	O
comparison	O
with	O
reranking	O
with	O
TF	O
.	O
of	O
topics	O
due	O
to	O
the	O
greater	O
diversity	O
of	O
words	O
over	O
all	O
the	O
topics	O
.	O
Top	O
10	O
Word	O
for	O
Each	O
Topic	O
NPMI	O
dollar	O
rate	O
rates	O
exchange	O
currency	O
market	O
dealers	O
central	O
interest	O
point	O
0.369	O
year	O
growth	O
rise	O
government	O
economic	O
economy	O
expected	O
domestic	O
inflation	O
report	O
0.355	O
gold	O
reserves	O
year	O
tons	O
company	O
production	O
exploration	O
ounces	O
feet	O
mine	O
0.290	O
billion	O
year	O
rose	O
dlrs	O
fell	O
marks	O
earlier	O
figures	O
surplus	O
rise	O
-	O
0.005	O
year	O
tonnes	O
crop	O
production	O
week	O
grain	O
sugar	O
estimated	O
expected	O
area	O
0.239	O
dlrs	O
company	O
sale	O
agreement	O
unit	O
acquisition	O
assets	O
agreed	O
subsidiary	O
sell	O
-	O
0.043	O
bank	O
billion	O
banks	O
money	O
interest	O
market	O
funds	O
credit	O
debt	O
loans	O
0.239	O
tonnes	O
wheat	O
export	O
sugar	O
tonne	O
exports	O
sources	O
shipment	O
sales	O
week	O
0.218	O
plan	O
bill	O
industry	O
farm	O
proposed	O
government	O
administration	O
told	O
proposal	O
change	O
0.212	O
prices	O
production	O
price	O
crude	O
output	O
barrels	O
barrel	O
increase	O
demand	O
industry	O
0.339	O
group	O
company	O
investment	O
stake	O
firm	O
told	O
companies	O
capital	O
chairman	O
president	O
0.191	O
trade	O
countries	O
foreign	O
officials	O
told	O
official	O
world	O
government	O
imports	O
agreement	O
0.298	O
offer	O
company	O
shares	O
share	O
dlrs	O
merger	O
board	O
stock	O
tender	O
shareholders	O
0.074	O
shares	O
stock	O
share	O
common	O
dividend	O
company	O
split	O
shareholders	O
record	O
outstanding	O
0.277	O
dlrs	O
year	O
quarter	O
earnings	O
company	O
share	O
sales	O
reported	O
expects	O
results	O
-	O
0.037	O
market	O
analysts	O
time	O
added	O
long	O
analyst	O
term	O
noted	O
high	O
back	O
0.316	O
coffee	O
meeting	O
stock	O
producers	O
prices	O
export	O
buffer	O
quotas	O
market	O
price	O
0.170	O
loss	B-MetricName
dlrs	O
profit	O
shrs	O
includes	O
year	O
gain	O
share	O
mths	B-DatasetName
excludes	O
-	O
0.427	O
spokesman	O
today	O
government	O
strike	O
union	O
state	O
yesterday	O
workers	O
officials	O
told	O
0.201	O
program	O
corn	O
dlrs	O
prior	O
futures	O
price	O
loan	O
contract	O
contracts	O
cents	O
-	O
0.287	O
Top	O
10	O
Word	O
for	O
Each	O
Topic	O
NPMI	O
rise	O
increase	O
growth	O
fall	O
change	O
decline	O
drop	O
gains	O
cuts	O
rising	O
0.238	O
president	O
chairman	O
minister	O
house	O
baker	O
administration	O
secretary	O
executive	O
chief	O
washington	O
0.111	O
make	O
continue	O
result	O
include	O
reduce	O
open	O
support	O
work	O
raise	O
remain	O
0.101	O
january	O
march	O
february	O
april	O
december	O
june	O
september	O
october	O
july	O
friday	O
0.043	O
year	O
quarter	O
week	O
month	O
earlier	O
months	O
years	O
time	O
period	O
term	O
0.146	O
rose	O
fell	O
compared	O
reported	O
increased	O
estimated	O
revised	O
adjusted	O
unchanged	O
raised	O
0.196	O
today	O
major	O
made	O
announced	O
recent	O
full	O
previously	O
strong	O
final	O
additional	O
0.125	O
share	O
stock	O
shares	O
dividend	O
common	O
cash	O
stake	O
shareholders	O
outstanding	O
preferred	O
0.281	O
dlrs	O
billion	O
tonnes	O
marks	O
francs	O
barrels	O
cents	O
tonne	O
barrel	O
tons	O
-	O
0.364	O
sales	O
earnings	O
business	O
operations	O
companies	O
products	O
markets	O
assets	O
industries	O
operating	O
0.115	O
sale	O
acquisition	O
merger	O
sell	O
split	O
sold	O
owned	O
purchase	O
acquire	O
held	O
0.003	O
board	O
meeting	O
report	O
general	O
commission	O
annual	O
bill	O
committee	O
association	O
council	O
0.106	O
loss	B-MetricName
profit	O
revs	O
record	O
note	O
oper	O
prior	O
shrs	O
gain	O
includes	O
0.221	O
company	O
corp	O
group	O
unit	O
firm	O
management	O
subsidiary	O
trust	O
pacific	O
holdings	O
0.058	O
prices	O
price	O
current	O
total	O
lower	O
higher	O
surplus	O
system	O
high	O
average	O
0.198	O
offer	O
agreement	O
agreed	O
talks	O
tender	O
plan	O
terms	O
program	O
proposed	O
issue	O
0.138	O
bank	O
trade	O
market	O
rate	O
exchange	O
dollar	O
foreign	O
interest	O
rates	O
banks	O
0.327	O
told	O
official	O
added	O
department	O
analysts	O
officials	O
spokesman	O
sources	O
statement	O
reuters	O
0.181	O
production	O
export	O
exports	O
industry	O
wheat	O
sugar	O
imports	O
output	O
crude	O
domestic	O
0.262	O
japan	O
government	O
international	O
world	O
countries	O
american	O
japanese	O
national	O
states	O
united	O
0.251	O

PUBMEDBERT	O
MEL	O
addresses	O
this	O
problem	O
by	O
framing	O
it	O
as	O
a	O
task	O
of	O
mapping	O
entity	O
mentions	O
to	O
unified	O
concepts	O
in	O
a	O
medical	O
knowledge	O
graph	O
.	O
3	O
The	O
main	O
bottleneck	O
of	O
MEL	O
is	O
the	O
quality	O
of	O
the	O
entity	O
representations	O
(	O
Basaldella	O
et	O
al	O
,	O
2020	O
)	O
.	O
Prior	O
works	O
in	O
this	O
domain	O
have	O
adopted	O
very	O
sophisticated	O
text	O
pre	O
-	O
processing	O
heuristics	O
(	O
D'Souza	O
and	O
Ng	O
,	O
2015	O
;	O
Kim	O
et	O
al	O
,	O
2019	O
;	O
Ji	O
et	O
al	O
,	O
2020	O
;	O
Sung	O
et	O
al	O
,	O
2020	O
)	O
which	O
can	O
hardly	O
cover	O
all	O
the	O
variations	O
of	O
biomedical	O
names	O
.	O
In	O
parallel	O
,	O
self	B-TaskName
-	I-TaskName
supervised	I-TaskName
learning	I-TaskName
has	O
shown	O
tremendous	O
success	O
in	O
NLP	O
via	O
leveraging	O
the	O
masked	O
language	B-TaskName
modelling	I-TaskName
(	O
MLM	B-DatasetName
)	O
objective	O
to	O
learn	O
semantics	O
from	O
distributional	O
representations	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
.	O
Domain	O
-	O
specific	O
pretraining	O
on	O
biomedical	O
corpora	O
(	O
e.g.	O
BIOBERT	O
,	O
Lee	O
et	O
al	O
2020	O
and	O
BIOMEGA	O
-	O
TRON	O
,	O
Shin	O
et	O
al	O
2020	O
)	O
have	O
made	O
much	O
progress	O
in	O
biomedical	O
text	O
mining	O
tasks	O
.	O
Nonetheless	O
,	O
representing	O
medical	O
entities	O
with	O
the	O
existing	O
SOTA	O
pretrained	O
MLMs	O
(	O
e.g.	O
PUBMEDBERT	O
,	O
Gu	O
et	O
al	O
2020	O
)	O
as	O
suggested	O
in	O
Fig	O
.	O
1	O
(	O
left	O
)	O
does	O
not	O
lead	O
to	O
a	O
well	O
-	O
separated	O
representation	O
space	O
.	O
To	O
address	O
the	O
aforementioned	O
issue	O
,	O
we	O
propose	O
to	O
pretrain	O
a	O
Transformer	B-MethodName
-	O
based	O
language	O
model	O
on	O
the	O
biomedical	O
knowledge	O
graph	O
of	O
UMLS	B-DatasetName
(	O
Bodenreider	O
,	O
2004	O
)	O
,	O
the	O
largest	O
interlingua	O
of	O
biomedical	O
ontologies	O
.	O
UMLS	B-DatasetName
contains	O
a	O
comprehensive	O
collection	O
of	O
biomedical	O
synonyms	O
in	O
various	O
forms	O
(	O
UMLS	B-DatasetName
2020AA	O
has	O
4M+	O
concepts	O
and	O
10M+	O
synonyms	O
which	O
stem	O
from	O
over	O
150	O
controlled	O
vocabularies	O
including	O
MeSH	O
,	O
SNOMED	O
CT	O
,	O
RxNorm	O
,	O
Gene	O
Ontology	B-MethodName
and	O
OMIM	O
)	O
.	O
4	O
We	O
design	O
a	O
selfalignment	O
objective	O
that	O
clusters	O
synonyms	O
of	O
the	O
same	O
concept	O
.	O
To	O
cope	O
with	O
the	O
immense	O
size	O
of	O
UMLS	B-DatasetName
,	O
we	O
sample	O
hard	O
training	O
pairs	O
from	O
the	O
knowledge	O
base	O
and	O
use	O
a	O
scalable	O
metric	B-TaskName
learning	I-TaskName
loss	B-MetricName
.	O
We	O
name	O
our	O
model	O
as	O
Self	O
-	O
aligning	O
pretrained	O
BERT	B-MethodName
(	O
SAPBERT	O
)	O
.	O
Being	O
both	O
simple	O
and	O
powerful	O
,	O
SAPBERT	O
obtains	O
new	O
SOTA	O
performances	O
across	O
all	O
six	O
MEL	O
benchmark	O
datasets	O
.	O
In	O
contrast	O
with	O
the	O
current	O
systems	O
which	O
adopt	O
complex	O
pipelines	O
and	O
hybrid	O
components	O
(	O
Xu	O
et	O
al	O
,	O
2020	O
;	O
Ji	O
et	O
al	O
,	O
2020	O
;	O
Sung	O
et	O
al	O
,	O
2020	O
)	O
,	O
SAPBERT	O
applies	O
a	O
much	O
simpler	O
training	O
procedure	O
without	O
requiring	O
any	O
pre	O
-	O
or	O
post	O
-	O
processing	O
steps	O
.	O
At	O
test	O
time	O
,	O
a	O
simple	O
nearest	O
neighbour	O
's	O
search	O
is	O
sufficient	O
for	O
making	O
a	O
prediction	O
.	O
When	O
compared	O
with	O
other	O
domain	O
-	O
specific	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
(	O
e.g.	O
BIOBERT	O
and	O
SCIBERT	O
)	O
,	O
SAPBERT	O
also	O
brings	O
substantial	O
improvement	O
by	O
up	O
to	O
20	O
%	O
on	O
accuracy	B-MetricName
across	O
all	O
tasks	O
.	O
The	O
effectiveness	O
of	O
the	O
pretraining	O
in	O
SAP	O
-	O
BERT	B-MethodName
is	O
especially	O
highlighted	O
in	O
the	O
scientific	O
language	O
domain	O
where	O
SAPBERT	O
outperforms	O
previous	O
SOTA	O
even	O
without	O
fine	O
-	O
tuning	O
on	O
any	O
MEL	O
datasets	O
.	O
We	O
also	O
provide	O
insights	O
on	O
pretraining	O
's	O
impact	O
across	O
domains	O
and	O
explore	O
pretraining	O
with	O
fewer	O
model	O
parameters	O
by	O
using	O
a	O
recently	O
introduced	O
ADAPTER	O
module	O
in	O
our	O
training	O
scheme	O
.	O
Figure	O
2	O
:	O
The	O
distribution	O
of	O
similarity	O
scores	O
for	O
all	O
sampled	O
PUBMEDBERT	O
representations	O
in	O
a	O
minibatch	O
.	O
The	O
left	O
graph	O
shows	O
the	O
distribution	O
of	O
+	O
andpairs	O
which	O
are	O
easy	O
and	O
already	O
well	O
-	O
separated	O
.	O
The	O
right	O
graph	O
illustrates	O
larger	O
overlap	O
between	O
the	O
two	O
groups	O
generated	O
by	O
the	O
online	O
mining	O
step	O
,	O
making	O
them	O
harder	O
and	O
more	O
informative	O
for	O
learning	O
.	O

We	O
design	O
a	O
metric	B-TaskName
learning	I-TaskName
framework	O
that	O
learns	O
to	O
self	O
-	O
align	O
synonymous	O
biomedical	O
entities	O
.	O
The	O
framework	O
can	O
be	O
used	O
as	O
both	O
pretraining	O
on	O
UMLS	B-DatasetName
,	O
and	O
fine	O
-	O
tuning	O
on	O
task	O
-	O
specific	O
datasets	O
.	O
We	O
use	O
an	O
existing	O
BERT	B-MethodName
model	O
as	O
our	O
starting	O
point	O
.	O
In	O
the	O
following	O
,	O
we	O
introduce	O
the	O
key	O
components	O
of	O
our	O
framework	O
.	O
Formal	O
Definition	O
.	O
Let	O
(	O
x	O
,	O
y	O
)	O
X	O
×	O
Y	O
denote	O
a	O
tuple	O
of	O
a	O
name	O
and	O
its	O
categorical	O
label	O
.	O
For	O
the	O
self	O
-	O
alignment	O
pretraining	O
step	O
,	O
X	O
×	O
Y	O
is	O
the	O
set	O
of	O
all	O
(	O
name	O
,	O
CUI	O
5	O
)	O
pairs	O
in	O
UMLS	B-DatasetName
,	O
e.g.	O
(	O
Remdesivir	O
,	O
C4726677	O
)	O
;	O
while	O
for	O
the	O
finetuning	O
step	O
,	O
it	O
is	O
formed	O
as	O
an	O
entity	O
mention	O
and	O
its	O
corresponding	O
mapping	O
from	O
the	O
ontology	B-MethodName
,	O
e.g.	O
(	O
scratchy	O
throat	O
,	O
102618009	O
)	O
.	O
Given	O
any	O
pair	O
of	O
tuples	O
(	O
x	O
i	O
,	O
y	O
i	O
)	O
,	O
(	O
x	O
j	O
,	O
y	O
j	O
)	O
X	O
×	O
Y	O
,	O
the	O
goal	O
of	O
the	O
self	O
-	O
alignment	O
is	O
to	O
learn	O
a	O
function	O
f	O
(	O
;	O
θ	B-HyperparameterName
)	O
:	O
X	O
R	O
d	O
parameterised	O
by	O
θ	B-HyperparameterName
.	O
Then	O
,	O
the	O
similarity	O
f	O
(	O
x	O
i	O
)	O
,	O
f	O
(	O
x	O
j	O
)	O
(	O
in	O
this	O
work	O
we	O
use	O
cosine	O
similarity	O
)	O
can	O
be	O
used	O
to	O
estimate	O
the	O
resemblance	O
of	O
x	O
i	O
and	O
x	O
j	O
(	O
i.e.	O
,	O
high	O
if	O
x	O
i	O
,	O
x	O
j	O
are	O
synonyms	O
and	O
low	O
otherwise	O
)	O
.	O
We	O
model	O
f	O
by	O
a	O
BERT	B-MethodName
model	O
with	O
its	O
output	O
[	O
CLS	O
]	O
token	O
regarded	O
as	O
the	O
representation	O
of	O
the	O
input	O
.	O
6	O
During	O
the	O
learning	O
,	O
a	O
sampling	O
procedure	O
selects	O
the	O
informative	O
pairs	O
of	O
training	O
samples	O
and	O
uses	O
them	O
in	O
the	O
pairwise	O
metric	B-TaskName
learning	I-TaskName
loss	B-MetricName
function	O
(	O
introduced	O
shortly	O
)	O
.	O
Online	O
Hard	O
Pairs	O
Mining	O
.	O
We	O
use	O
an	O
online	O
hard	O
triplet	O
mining	O
condition	O
to	O
find	O
the	O
most	O
informative	O
training	O
examples	O
(	O
i.e.	O
hard	O
positive	O
/	O
negative	O
pairs	O
)	O
within	O
a	O
mini	O
-	O
batch	O
for	O
efficient	O
training	O
,	O
Fig	O
.	O
2	O
.	O
For	O
biomedical	O
entities	O
,	O
this	O
step	O
can	O
be	O
particularly	O
useful	O
as	O
most	O
examples	O
can	O
be	O
easily	O
classified	O
while	O
a	O
small	O
set	O
of	O
very	O
hard	O
ones	O
cause	O
the	O
most	O
challenge	O
to	O
representation	B-TaskName
learning	I-TaskName
.	O
7	O
We	O
start	O
from	O
constructing	O
all	O
possible	O
triplets	O
for	O
all	O
names	O
within	O
the	O
mini	O
-	O
batch	O
where	O
each	O
triplet	O
is	O
in	O
the	O
form	O
of	O
(	O
x	O
a	O
,	O
x	O
p	O
,	O
x	O
n	O
)	O
.	O
Here	O
x	O
a	O
is	O
called	O
anchor	O
,	O
an	O
arbitrary	O
name	O
in	O
the	O
minibatch	O
;	O
x	O
p	O
a	O
positive	O
match	O
of	O
x	O
a	O
(	O
i.e.	O
y	O
a	O
=	O
y	O
p	O
)	O
and	O
x	O
n	O
a	O
negative	O
match	O
of	O
x	O
a	O
(	O
i.e.	O
y	O
a	O
=	O
y	O
n	O
)	O
.	O
Among	O
the	O
constructed	O
triplets	O
,	O
we	O
select	O
out	O
all	O
triplets	O
that	O
violate	O
the	O
following	O
condition	O
:	O
f	O
(	O
x	O
a	O
)	O
−	O
f	O
(	O
x	O
p	O
)	O
2	O
<	O
f	O
(	O
x	O
a	O
)	O
−	O
f	O
(	O
x	O
n	O
)	O
2	O
+	O
λ	O
,	O
(	O
1	O
)	O
where	O
λ	O
is	O
a	O
pre	O
-	O
set	O
margin	O
.	O
In	O
other	O
words	O
,	O
we	O
only	O
consider	O
triplets	O
with	O
the	O
negative	O
sample	O
closer	O
to	O
the	O
positive	O
sample	O
by	O
a	O
margin	O
of	O
λ	O
.	O
These	O
are	O
the	O
hard	O
triplets	O
as	O
their	O
original	O
representations	O
were	O
very	O
far	O
from	O
correct	O
.	O
Every	O
hard	O
triplet	O
contributes	O
one	O
hard	O
positive	O
pair	O
(	O
x	O
a	O
,	O
x	O
p	O
)	O
and	O
one	O
hard	O
negative	O
pair	O
(	O
x	O
a	O
,	O
x	O
n	O
)	O
.	O
We	O
collect	O
all	O
such	O
positive	O
&	O
negative	O
pairs	O
and	O
denote	O
them	O
as	O
P	O
,	O
N	O
.	O
A	O
similar	O
but	O
not	O
identical	O
triplet	O
mining	O
condition	O
was	O
used	O
by	O
Schroff	O
et	O
al	O
(	O
2015	O
)	O
for	O
face	B-TaskName
recognition	I-TaskName
to	O
select	O
hard	O
negative	O
samples	O
.	O
Switching	O
-	O
off	O
this	O
mining	O
process	O
,	O
causes	O
a	O
drastic	O
performance	O
drop	O
(	O
see	O
Tab	O
.	O
2	O
)	O
.	O
Loss	O
Function	O
.	O
We	O
compute	O
the	O
pairwise	O
cosine	O
similarity	O
of	O
all	O
the	O
BERT	B-MethodName
-	O
produced	O
name	O
representations	O
and	O
obtain	O
a	O
similarity	O
matrix	O
S	O
R	O
|	O
X	O
b	O
|	O
×	O
|	O
X	O
b	O
|	O
where	O
each	O
entry	O
S	O
ij	O
corresponds	O
to	O
the	O
cosine	O
similarity	O
between	O
the	O
i	O
-	O
th	O
and	O
j	O
-	O
th	O
names	O
in	O
the	O
mini	O
-	O
batch	O
b.	O
We	O
adapted	O
the	O
Multi	O
-	O
Similarity	O
loss	B-MetricName
(	O
MS	O
loss	B-MetricName
,	O
Wang	O
et	O
al	O
2019	O
)	O
,	O
a	O
SOTA	O
metric	B-TaskName
learning	I-TaskName
objective	O
on	O
visual	O
recognition	O
,	O
for	O
learning	O
from	O
the	O
positive	O
and	O
negative	O
pairs	O
:	O
L	O
=	O
1	O
|	O
X	O
b	O
|	O
|	O
X	O
b	O
|	O
i=1	O
1	O
α	B-HyperparameterName
log	O
1	O
+	O
n	O
N	O
i	O
e	O
α	B-HyperparameterName
(	O
S	O
in	O
−	O
)	O
+	O
1	O
β	B-HyperparameterName
log	O
1	O
+	O
p	O
P	O
i	O
e	O
−β	O
(	O
S	O
ip	O
−	O
)	O
,	O
(	O
2	O
)	O
where	O
α	B-HyperparameterName
,	O
β	B-HyperparameterName
are	O
temperature	O
scales	O
;	O
is	O
an	O
offset	O
applied	O
on	O
the	O
similarity	O
matrix	O
;	O
P	O
i	O
,	O
N	O
i	O
are	O
indices	O
of	O
positive	O
and	O
negative	O
samples	O
of	O
the	O
anchor	O
i.	O
8	O
While	O
the	O
first	O
term	O
in	O
Eq	O
.	O
2	O
pushes	O
negative	O
pairs	O
away	O
from	O
each	O
other	O
,	O
the	O
second	O
term	O
pulls	O
positive	O
pairs	O
together	O
.	O
This	O
dynamic	O
allows	O
for	O
a	O
re	O
-	O
calibration	O
of	O
the	O
alignment	O
space	O
using	O
the	O
semantic	O
biases	O
of	O
synonymy	O
relations	O
.	O
The	O
MS	O
loss	B-MetricName
leverages	O
similarities	O
among	O
and	O
between	O
positive	O
and	O
negative	O
pairs	O
to	O
re	O
-	O
weight	O
the	O
importance	O
of	O
the	O
samples	O
.	O
The	O
most	O
informative	O
pairs	O
will	O
receive	O
more	O
gradient	O
signals	O
during	O
training	O
and	O
thus	O
can	O
better	O
use	O
the	O
information	O
stored	O
in	O
data	O
.	O

During	O
training	O
,	O
we	O
use	O
AdamW	B-MethodName
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2018	O
)	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e	O
-	O
5	O
and	O
weight	B-HyperparameterName
decay	I-HyperparameterName
rate	I-HyperparameterName
of	O
1e	O
-	O
2	O
.	O
Models	O
are	O
trained	O
on	O
the	O
prepared	O
pairwise	O
UMLS	B-DatasetName
data	O
for	O
1	O
epoch	O
(	O
approximately	O
50k	O
iterations	O
)	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
512	O
(	O
i.e.	O
,	O
256	O
pairs	O
per	O
mini	O
-	O
batch	O
)	O
.	O
We	O
train	O
with	O
Automatic	O
Mixed	O
Precision	B-MetricName
(	O
AMP	B-MethodName
)	O
10	O
provided	O
in	O
PyTorch	O
1.7.0	O
.	O
This	O
takes	O
approximately	O
5	O
hours	O
on	O
our	O
machine	O
(	O
con	O
-	O
4231	O
scientific	O
language	O
social	O
media	O
language	O
model	O
NCBI	O
BC5CDR	B-DatasetName
-	O
d	O
BC5CDR	B-DatasetName
-	O
c	O
MedMentions	B-DatasetName
AskAPatient	O
COMETA	B-DatasetName
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
@1	O
@5	O
the	O
improvement	O
comparing	O
to	O
the	O
base	O
model	O
(	O
the	O
deeper	O
the	O
more	O
)	O
.	O
Bottom	O
:	O
SAPBERT	O
vs.	O
SOTA	O
results	O
.	O
Blue	O
and	O
red	O
denote	O
unsupervised	O
and	O
supervised	O
models	O
.	O
Bold	O
and	O
underline	O
denote	O
the	O
best	O
and	O
second	O
best	O
results	O
in	O
the	O
column	O
.	O
"	O
†	O
"	O
denotes	O
statistically	O
significant	O
better	O
than	O
supervised	O
SOTA	O
(	O
T	O
-	O
test	O
,	O
ρ	O
<	O
0.05	O
)	O
.	O
On	O
COMETA	B-DatasetName
,	O
the	O
results	O
inside	O
the	O
parentheses	O
added	O
the	O
supervised	O
SOTA	O
's	O
dictionary	O
back	O
-	O
off	O
technique	O
(	O
Basaldella	O
et	O
al	O
,	O
2020	O
)	O
.	O
"	O
-	O
"	O
:	O
not	O
reported	O
in	O
the	O
SOTA	O
paper	O
.	O
"	O
OOM	O
"	O
:	O
out	O
-	O
of	O
-	O
memory	O
(	O
192GB+	O
)	O
.	O
figurations	O
specified	O
in	O
App	O
.	O
B.4	O
)	O
.	O
For	O
other	O
hyperparameters	O
used	O
,	O
please	O
view	O
App	O
.	O
C.2	O
.	O
Evaluation	O
Data	O
and	O
Protocol	O
.	O
We	O
experiment	O
on	O
6	O
different	O
English	O
MEL	O
datasets	O
:	O
4	O
in	O
the	O
scientific	O
domain	O
(	O
NCBI	O
,	O
Dogan	O
et	O
al	O
2014	O
;	O
BC5CDR	B-DatasetName
-	O
c	O
and	O
BC5CDR	B-DatasetName
-	O
d	O
,	O
Li	O
et	O
al	O
2016	O
;	O
MedMentions	B-DatasetName
,	O
Mohan	O
and	O
Li	O
2018	O
)	O
and	O
2	O
in	O
the	O
social	O
media	O
domain	O
(	O
COMETA	B-DatasetName
,	O
Basaldella	O
et	O
al	O
2020	O
andAskAPatient	O
,	O
Limsopatham	O
andCollier	O
2016	O
)	O
.	O
Descriptions	O
of	O
the	O
datasets	O
and	O
their	O
statistics	O
are	O
provided	O
in	O
App	O
.	O
A.	O
We	O
report	O
Acc	B-MetricName
@1	O
and	O
Acc	B-MetricName
@5	O
(	O
denoted	O
as	O
@1	O
and	O
@5	O
)	O
for	O
evaluating	O
performance	O
.	O
In	O
all	O
experiments	O
,	O
SAPBERT	O
denotes	O
further	O
pretraining	O
with	O
our	O
self	O
-	O
alignment	O
method	O
on	O
UMLS	B-DatasetName
.	O
At	O
the	O
test	O
phase	O
,	O
for	O
all	O
SAPBERT	O
models	O
we	O
use	O
nearest	O
neighbour	O
search	O
without	O
further	O
fine	O
-	O
tuning	O
on	O
task	O
data	O
(	O
unless	O
stated	O
otherwise	O
)	O
.	O
Except	O
for	O
numbers	O
reported	O
in	O
previous	O
papers	O
,	O
all	O
results	O
are	O
the	O
average	O
of	O
five	O
runs	O
with	O
different	O
random	O
seeds	B-DatasetName
.	O
Fine	O
-	O
Tuning	O
on	O
Task	O
Data	O
.	O
The	O
red	O
rows	O
in	O
Tab	O
.	O
1	O
are	O
results	O
of	O
models	O
(	O
further	O
)	O
fine	O
-	O
tuned	O
on	O
the	O
training	O
sets	O
of	O
the	O
six	O
MEL	O
datasets	O
.	O
Similar	O
to	O
pretraining	O
,	O
a	O
positive	O
pair	O
list	O
is	O
generated	O
through	O
traversing	O
the	O
combinations	O
of	O
mention	O
and	O
all	O
ground	O
truth	O
synonyms	O
where	O
mentions	O
are	O
from	O
the	O
training	O
set	O
and	O
ground	O
truth	O
synonyms	O
are	O
from	O
the	O
reference	O
ontology	B-MethodName
.	O
We	O
use	O
the	O
same	O
optimiser	O
and	O
learning	O
rates	O
but	O
train	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
256	O
(	O
to	O
accommodate	O
the	O
memory	O
of	O
1	O
GPU	O
)	O
.	O
On	O
scientific	O
language	O
datasets	O
,	O
we	O
train	O
for	O
3	O
epochs	O
while	O
on	O
AskAPatient	O
and	O
COMETA	B-DatasetName
we	O
train	O
for	O
15	O
and	O
10	O
epochs	O
respectively	O
.	O
For	O
BIOSYN	O
on	O
social	O
media	O
language	O
datasets	O
,	O
we	O
empirically	O
found	O
that	O
10	O
epochs	O
work	O
the	O
best	O
.	O
Other	O
configurations	O
are	O
the	O
same	O
as	O
the	O
original	O
BIOSYN	O
paper	O
.	O

*	O
BERT	B-MethodName
+	O
SAPBERT	O
(	O
Tab	O
.	O
1	O
,	O
top	O
)	O
.	O
We	O
illustrate	O
the	O
impact	O
of	O
SAPBERT	O
pretraining	O
over	O
7	O
existing	O
BERT	B-MethodName
-	O
based	O
models	O
(	O
*	O
BERT	B-MethodName
=	O
{	O
BIOBERT	O
,	O
PUBMEDBERT	O
,	O
...	O
}	O
)	O
.	O
SAPBERT	O
obtains	O
consistent	O
improvement	O
over	O
all	O
*	O
BERT	B-MethodName
models	O
across	O
all	O
datasets	O
,	O
with	O
larger	O
gains	O
(	O
by	O
up	O
to	O
31.0	O
%	O
absolute	O
Acc	B-MetricName
@1	O
increase	O
)	O
observed	O
in	O
the	O
social	O
media	O
domain	O
.	O
While	O
SCIBERT	O
is	O
the	O
leading	O
model	O
before	O
applying	O
SAPBERT	O
,	O
PUBMEDBERT+SAPBERT	O
performs	O
the	O
best	O
afterwards	O
.	O
SAPBERT	O
vs.	O
SOTA	O
(	O
Tab	O
.	O
1	O
,	O
bottom	O
)	O
.	O
We	O
take	O
PUBMEDBERT+SAPBERT	O
(	O
w	O
/	O
wo	O
fine	O
-	O
tuning	O
)	O
and	O
compare	O
against	O
various	O
published	O
SOTA	O
results	O
(	O
see	O
App	O
.	O
C.1	O
for	O
a	O
full	O
listing	O
of	O
10	O
baselines	O
)	O
which	O
all	O
require	O
task	O
supervision	O
.	O
For	O
the	O
scientific	O
language	O
domain	O
,	O
the	O
SOTA	O
is	O
BIOSYN	O
(	O
Sung	O
et	O
al	O
,	O
2020	O
)	O
.	O
For	O
the	O
social	O
media	O
domain	O
,	O
the	O
SOTA	O
are	O
Basaldella	O
et	O
al	O
(	O
2020	O
)	O
and	O
GEN	O
-	O
RANK	O
(	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
on	O
COMETA	B-DatasetName
and	O
AskAPatient	O
respectively	O
.	O
All	O
these	O
SOTA	O
methods	O
combine	O
BERT	B-MethodName
with	O
heuristic	O
modules	O
such	O
as	O
tf	O
-	O
idf	O
,	O
string	O
matching	O
and	O
information	B-TaskName
retrieval	I-TaskName
system	O
(	O
i.e.	O
Apache	O
Lucene	O
)	O
in	O
a	O
multi	O
-	O
stage	O
manner	O
.	O
Measured	O
by	O
Acc	B-MetricName
@1	O
,	O
SAPBERT	O
achieves	O
new	O
SOTA	O
with	O
statistical	O
significance	O
on	O
5	O
of	O
the	O
6	O
datasets	O
and	O
for	O
the	O
dataset	O
(	O
BC5CDR	B-DatasetName
-	O
c	O
)	O
where	O
SAPBERT	O
is	O
not	O
significantly	O
better	O
,	O
it	O
performs	O
on	O
par	O
with	O
SOTA	O
(	O
96.5	O
vs.	O
96.6	O
)	O
.	O
Interestingly	O
,	O
on	O
scientific	O
language	O
datasets	O
,	O
SAPBERT	O
outperforms	O
SOTA	O
without	O
any	O
task	O
supervision	O
(	O
fine	O
-	O
tuning	O
mostly	O
leads	O
to	O
overfitting	O
and	O
performance	O
drops	O
)	O
.	O
On	O
social	O
media	O
language	O
datasets	O
,	O
unsupervised	O
SAPBERT	O
lags	O
behind	O
supervised	O
SOTA	O
by	O
large	O
margins	O
,	O
highlighting	O
the	O
well	O
-	O
documented	O
complex	O
nature	O
of	O
social	O
media	O
language	O
(	O
Baldwin	O
et	O
al	O
,	O
2013	O
;	O
Collier	O
,	O
2015	O
,	O
2016	O
;	O
Basaldella	O
et	O
al	O
,	O
2020	O
;	O
Tutubalina	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
after	O
fine	O
-	O
tuning	O
on	O
the	O
social	O
media	O
datasets	O
(	O
using	O
the	O
MS	O
loss	B-MetricName
introduced	O
earlier	O
)	O
,	O
SAPBERT	O
outperforms	O
SOTA	O
significantly	O
,	O
indicating	O
that	O
knowledge	O
acquired	O
during	O
the	O
selfaligning	O
pretraining	O
can	O
be	O
adapted	O
to	O
a	O
shifted	O
domain	O
without	O
much	O
effort	O
.	O
The	O
ADAPTER	O
Variant	O
.	O
As	O
an	O
option	O
for	O
parameter	O
efficient	O
pretraining	O
,	O
we	O
explore	O
a	O
variant	O
of	O
SAPBERT	O
using	O
a	O
recently	O
introduced	O
training	O
module	O
named	O
ADAPTER	O
(	O
Houlsby	O
et	O
al	O
,	O
2019	O
)	O
.	O
While	O
maintaining	O
the	O
same	O
pretraining	O
scheme	O
with	O
the	O
same	O
SAPBERT	O
online	O
mining	O
+	O
MS	O
loss	B-MetricName
,	O
instead	O
of	O
training	O
from	O
the	O
full	O
model	O
of	O
PUBMEDBERT	O
,	O
we	O
insert	O
new	O
ADAPTER	O
layers	O
between	O
Transformer	B-MethodName
layers	O
of	O
the	O
fixed	O
PUBMEDBERT	O
,	O
and	O
only	O
train	O
the	O
weights	O
of	O
these	O
ADAPTER	O
layers	O
.	O
In	O
our	O
experiments	O
,	O
we	O
use	O
the	O
enhanced	O
ADAPTER	O
configuration	O
by	O
Pfeiffer	O
et	O
al	O
(	O
2020	O
)	O
.	O
We	O
include	O
two	O
variants	O
where	O
trained	O
parameters	O
are	O
13.22	O
%	O
and	O
1.09	O
%	O
of	O
the	O
full	O
SAPBERT	O
variant	O
.	O
The	O
ADAPTER	O
variant	O
of	O
SAPBERT	O
achieves	O
comparable	O
performance	O
to	O
full	O
-	O
model	O
-	O
tuning	O
in	O
scientific	O
datasets	O
but	O
lags	O
behind	O
in	O
social	O
media	O
datasets	O
,	O
Tab	O
.	O
1	O
.	O
The	O
results	O
indicate	O
that	O
more	O
parameters	O
are	O
needed	O
in	O
pretraining	O
for	O
knowledge	O
transfer	O
to	O
a	O
shifted	O
domain	O
,	O
in	O
our	O
case	O
,	O
the	O
social	O
media	O
datasets	O
.	O
The	O
Impact	O
of	O
Online	O
Mining	O
(	O
Eq	O
.	O
(	O
1	O
)	O
)	O
.	O
As	O
suggested	O
in	O
Tab	O
.	O
2	O
,	O
switching	O
off	O
the	O
online	O
hard	O
pairs	O
mining	O
procedure	O
causes	O
a	O
large	O
performance	O
drop	O
in	O
@1	O
and	O
a	O
smaller	O
but	O
still	O
significant	O
drop	O
in	O
@5	O
.	O
This	O
is	O
due	O
to	O
the	O
presence	O
of	O
many	O
easy	O
and	O
already	O
well	O
-	O
separated	O
samples	O
in	O
the	O
mini	O
-	O
batches	O
.	O
These	O
uninformative	O
training	O
examples	O
dominated	O
the	O
gradients	O
and	O
harmed	O
the	O
learning	O
process	O
.	O
Integrating	O
SAPBERT	O
in	O
Existing	O
Systems	O
.	O
SAPBERT	O
can	O
be	O
easily	O
inserted	O
into	O
existing	O
BERT	B-MethodName
-	O
based	O
MEL	O
systems	O
by	O
initialising	O
the	O
systems	O
with	O
SAPBERT	O
pretrained	O
weights	O
.	O
We	O
use	O
the	O
SOTA	O
scientific	O
language	O
system	O
,	O
BIOSYN	O
(	O
originally	O
initialised	O
with	O
BIOBERT	O
weights	O
)	O
,	O
as	O
an	O
example	O
and	O
show	O
the	O
performance	O
is	O
boosted	O
across	O
all	O
datasets	O
(	O
last	O
two	O
rows	O
,	O
Tab	O
.	O
1	O
)	O
.	O

We	O
use	O
COMETA	B-DatasetName
(	O
zeroshot	O
general	O
)	O
as	O
a	O
benchmark	O
for	O
selecting	O
learning	O
objectives	O
.	O
Note	O
that	O
this	O
split	O
of	O
COMETA	B-DatasetName
is	O
different	O
from	O
the	O
stratified	O
-	O
general	O
split	O
used	O
in	O
Tab	O
.	O
4	O
.	O
It	O
is	O
very	O
challenging	O
(	O
so	O
easy	O
to	O
see	O
the	O
difference	O
of	O
the	O
performance	O
)	O
and	O
also	O
does	O
not	O
directly	O
affect	O
the	O
model	O
's	O
performance	O
on	O
other	O
datasets	O
.	O
The	O
results	O
are	O
listed	O
in	O
Tab	O
.	O
6	O
.	O
Note	O
that	O
online	O
mining	O
is	O
switched	O
on	O
for	O
all	O
models	O
here	O
.	O
(	O
Basaldella	O
et	O
al	O
,	O
2020	O
)	O
64.6	O
74.6	O
NCA	O
loss	B-MetricName
(	O
Goldberger	O
et	O
al	O
,	O
2005	O
)	O
65.2	O
77.0	O
Lifted	O
-	O
Structure	O
loss	B-MetricName
(	O
Oh	O
Song	O
et	O
al	O
,	O
2016	O
)	O
62.0	O
72.1	O
InfoNCE	B-MethodName
(	O
Oord	O
et	O
al	O
,	O
2018	O
;	O
He	O
et	O
al	O
,	O
2020	O
)	O
63.3	O
74.2	O
Circle	O
loss	B-MetricName
(	O
Sun	O
et	O
al	O
,	O
2020	O
)	O
66.7	O
78.7	O
Multi	O
-	O
Similarity	O
loss	B-MetricName
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
67.2	O
80.3	O
Schumacher	O
et	O
al	O
(	O
2020	O
)	O
for	O
clinical	O
concept	O
linking	O
.	O
InfoNCE	B-MethodName
has	O
been	O
very	O
popular	O
in	O
selfsupervised	O
learning	O
and	O
contrastive	B-MethodName
learning	I-MethodName
(	O
Oord	O
et	O
al	O
,	O
2018	O
;	O
He	O
et	O
al	O
,	O
2020	O
)	O
.	O
Lifted	O
-	O
Structure	O
loss	B-MetricName
(	O
Oh	O
Song	O
et	O
al	O
,	O
2016	O
)	O
and	O
NCA	O
loss	B-MetricName
(	O
Goldberger	O
et	O
al	O
,	O
2005	O
)	O
are	O
two	O
very	O
classic	O
metric	B-TaskName
learning	I-TaskName
objectives	O
.	O
Multi	O
-	O
Similarity	O
loss	B-MetricName
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
and	O
Circle	O
loss	B-MetricName
(	O
Sun	O
et	O
al	O
,	O
2020	O
)	O
are	O
two	O
recently	O
proposed	O
metric	B-TaskName
learning	I-TaskName
objectives	O
and	O
have	O
been	O
considered	O
as	O
SOTA	O
on	O
large	O
-	O
scale	O
visual	O
recognition	O
benchmarks	O
.	O

In	O
Tab	O
.	O
7	O
we	O
list	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
trained	O
in	O
the	O
three	O
ADAPTER	O
variants	O
along	O
with	O
full	O
-	O
modeltuning	O
for	O
easy	O
comparison	O
.	O
BIOBERT	O
(	O
Lee	O
et	O
al	O
,	O
2020	O
)	O
https://huggingface.co/dmis	O
-	O
lab	O
/	O
biobert	O
-	O
v1.1	O
BLUEBERT	O
(	O
Peng	O
et	O
al	O
,	O
2019	O
)	O
https://huggingface.co/bionlp/bluebert_pubmed_mimic_uncased_L	O
-	O
12_H	O
-	O
768_A	O
-	O
12	O
CLINICALBERT	O
(	O
Alsentzer	O
et	O
al	O
,	O
2019	O
)	O
https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT	O
SCIBERT	O
(	O
Beltagy	O
et	O
al	O
,	O
2019	O
)	O
https://huggingface.co/allenai/scibert_scivocab_uncased	O
UMLSBERT	O
(	O
Michalopoulos	O
et	O
al	O
,	O
2020	O
)	O
https://www.dropbox.com/s/qaoq5gfen69xdcc/umlsbert.tar.xz?dl=0	O
PUBMEDBERT	O
(	O
Gu	O
et	O
al	O
,	O
2020	O
)	O
https://huggingface.co/microsoft/BiomedNLP	O
-	O
PubMedBERT	O
-	O
base	O
-	O
uncased	O
-	O
abstract	O
-	O
fulltext	O

Here	O
,	O
we	O
first	O
examine	O
the	O
relevance	O
of	O
our	O
proposal	O
to	O
reinstitute	O
summarization	B-TaskName
evaluation	O
over	O
multiple	O
summary	O
lengths	O
.	O
Then	O
,	O
we	O
investigate	O
our	O
research	O
question	O
of	O
whether	O
using	O
reference	O
summaries	O
of	O
a	O
single	O
length	O
suffices	O
for	O
evaluating	O
system	O
summaries	O
of	O
multiple	O
lengths	O
.	O
We	O
turn	O
to	O
the	O
DUC	O
2001	O
and	O
2002	O
multi	B-TaskName
-	I-TaskName
document	I-TaskName
summarization	I-TaskName
datasets	O
,	O
which	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
are	O
the	O
only	O
available	O
datasets	O
that	O
provide	O
the	O
necessary	O
requirements	O
for	O
this	O
analysis	O
(	O
see	O
Table	O
1	O
)	O
.	O
The	O
importance	O
of	O
evaluating	O
and	O
comparing	O
systems	O
at	O
several	O
lengths	O
is	O
demonstrated	O
with	O
the	O
observation	O
that	O
system	O
rankings	O
can	O
change	O
quite	O
significantly	O
at	O
different	O
summary	O
lengths	O
.	O
In	O
2001	O
,	O
the	O
Spearman	B-MetricName
correlation	I-MetricName
between	O
the	O
available	O
human	O
rankings	O
of	O
systems	O
at	O
the	O
50word	O
and	O
400	O
-	O
word	O
lengths	O
is	O
0.61	O
.	O
For	O
example	O
,	O
the	O
system	O
ranked	O
first	O
at	O
length	O
50	O
ranks	O
sixth	O
at	O
lengths	O
200	O
and	O
400	O
.	O
Even	O
for	O
the	O
human	O
system	O
ranking	O
at	O
the	O
100	O
-	O
word	O
length	O
,	O
which	O
deviates	O
the	O
least	O
from	O
human	O
rankings	O
at	O
the	O
other	O
lengths	O
,	O
the	O
correlation	O
with	O
system	O
ranking	O
at	O
the	O
400	O
length	O
is	O
only	O
0.73	O
.	O
Generally	O
,	O
the	O
larger	O
the	O
difference	O
between	O
a	O
pair	O
of	O
summary	O
lengths	O
,	O
the	O
greater	O
the	O
fluctuation	O
in	O
system	O
rankings	O
.	O
Similar	O
trends	O
were	O
observed	O
for	O
DUC	O
2002	O
,	O
and	O
when	O
comparing	O
system	O
rankings	O
by	O
automatic	O
ROUGE	O
scoring	O
(	O
both	O
rankings	O
are	O
elaborated	O
below	O
)	O
.	O
Obviously	O
,	O
such	O
performance	O
differences	O
are	O
overlooked	O
when	O
evaluating	O
systems	O
over	O
summaries	O
of	O
a	O
single	O
length	O
.	O
Next	O
,	O
we	O
turn	O
to	O
investigate	O
our	O
research	O
question	O
.	O
In	O
this	O
paper	O
,	O
we	O
examine	O
it	O
with	O
respect	O
to	O
automatic	O
summary	O
evaluation	O
,	O
which	O
has	O
become	O
most	O
common	O
for	O
system	O
development	O
and	O
evaluation	O
,	O
thanks	O
to	O
its	O
speed	O
and	O
low	O
cost	O
.	O
Specifically	O
,	O
we	O
use	O
several	O
variants	O
of	O
the	O
ROUGE	O
metric	O
(	O
Lin	O
,	O
2004	O
)	O
,	O
which	O
is	O
almost	O
exclusively	O
utilized	O
as	O
an	O
automatic	O
evaluation	O
metric	O
class	O
for	O
summarization	B-TaskName
.	O
ROUGE	O
variants	O
are	O
based	O
on	O
word	O
sequence	O
overlap	O
between	O
a	O
system	O
summary	O
and	O
a	O
reference	O
summary	O
,	O
where	O
each	O
variant	O
measures	O
a	O
different	O
aspect	O
of	O
text	O
comparison	O
.	O
Despite	O
its	O
pitfalls	O
,	O
ROUGE	O
has	O
shown	O
reasonable	O
correlation	O
of	O
its	O
system	O
scores	O
to	O
those	O
obtained	O
by	O
manual	O
evaluation	O
methods	O
(	O
Lin	O
,	O
2004	O
;	O
Over	O
and	O
James	O
,	O
2004	O
;	O
Over	O
et	O
al	O
,	O
2007	O
;	O
Nenkova	O
et	O
al	O
,	O
2007	O
;	O
Louis	O
and	O
Nenkova	O
,	O
2013	O
;	O
Peyrard	O
et	O
al	O
,	O
2017	O
)	O
,	O
such	O
as	O
SEE	O
(	O
Lin	O
,	O
2001	O
)	O
,	O
responsiveness	O
(	O
NIST	O
,	O
2006	O
)	O
and	O
Pyramid	O
(	O
Nenkova	O
et	O
al	O
,	O
2007	O
)	O
.	O
We	O
follow	O
the	O
same	O
methodology	O
of	O
assessing	O
the	O
reliability	O
of	O
automatic	O
evaluation	O
scores	O
by	O
measuring	O
their	O
correlation	O
to	O
human	O
evaluation	O
scores	O
.	O
In	O
our	O
case	O
,	O
DUC	O
2001	O
and	O
2002	O
applied	O
the	O
SEE	O
manual	O
evaluation	O
method	O
.	O
NIST	O
assessors	O
compared	O
systems	O
'	O
summaries	O
to	O
reference	O
summaries	O
,	O
which	O
were	O
all	O
decomposed	O
into	O
a	O
list	O
of	O
elementary	O
discourse	O
units	O
(	O
EDUs	O
)	O
.	O
Each	O
reference	O
EDU	O
was	O
marked	O
throughout	O
the	O
system	O
EDUs	O
and	O
was	O
scored	O
for	O
how	O
well	O
it	O
was	O
expressed	O
.	O
The	O
final	O
manually	O
evaluated	O
scores	O
,	O
called	O
the	O
human	O
mean	O
content	O
coverage	O
scores	O
,	O
are	O
provided	O
in	O
the	O
DUC	O
datasets	O
.	O
We	O
can	O
then	O
correlate	O
the	O
human	O
-	O
based	O
system	O
ranking	O
,	O
attained	O
from	O
these	O
provided	O
scores	O
,	O
to	O
the	O
system	O
ranking	O
attained	O
from	O
the	O
automatic	O
scores	O
that	O
we	O
calculate	O
using	O
our	O
proposed	O
methodology	O
.	O
As	O
a	O
baseline	O
,	O
we	O
consider	O
the	O
ROUGE	O
Recall	B-MetricName
scores	O
obtained	O
by	O
the	O
standard	O
reference	O
summary	O
configuration	O
(	O
Standard	O
,	O
first	O
row	O
in	O
Table	O
2	O
)	O
,	O
that	O
is	O
,	O
when	O
system	O
summaries	O
of	O
each	O
length	O
(	O
table	O
columns	O
)	O
are	O
evaluated	O
against	O
reference	O
summaries	O
of	O
the	O
same	O
length	O
.	O
This	O
is	O
the	O
same	O
configuration	O
used	O
by	O
Lin	O
(	O
2004	O
)	O
when	O
introducing	O
and	O
assessing	O
ROUGE	O
.	O
Then	O
,	O
looking	O
into	O
our	O
research	O
question	O
,	O
we	O
consider	O
reference	O
summary	O
configurations	O
in	O
which	O
system	O
summaries	O
of	O
all	O
lengths	O
are	O
evaluated	O
against	O
reference	O
summaries	O
of	O
a	O
single	O
chosen	O
length	O
(	O
OnlyNNN	O
,	O
subsequent	O
rows	O
of	O
Table	O
2	O
)	O
.	O
In	O
each	O
configuration	O
(	O
each	O
row	O
)	O
,	O
we	O
repeat	O
the	O
evaluation	O
twice	O
:	O
once	O
using	O
the	O
complete	O
set	O
of	O
available	O
reference	O
sum	O
-	O
2	O
)	O
for	O
different	O
ROUGE	O
variants	O
(	O
column	O
pairs	O
)	O
and	O
reference	O
summary	O
configurations	O
(	O
rows	O
)	O
,	O
when	O
using	O
1	O
reference	O
or	O
multiple	O
.	O
The	O
first	O
row	O
presents	O
absolute	O
correlations	O
,	O
with	O
relative	O
differences	O
in	O
subsequent	O
rows	O
.	O
maries	O
of	O
the	O
utilized	O
reference	O
length	O
,	O
and	O
once	O
with	O
just	O
one	O
randomly	O
chosen	O
reference	O
summary	O
from	O
that	O
set	O
(	O
the	O
3refs	O
and	O
1ref	O
sub	O
-	O
columns	O
)	O
.	O
For	O
each	O
reference	O
summary	O
configuration	O
,	O
we	O
compute	O
ROUGE	O
Recall	B-MetricName
system	O
scores	O
1	O
for	O
the	O
three	O
common	O
ROUGE	O
variants	O
R	O
-	O
1	O
,	O
R	O
-	O
2	O
and	O
R	O
-	O
L	O
,	O
which	O
compare	O
unigrams	O
,	O
bigrams	O
and	O
the	O
longest	O
common	O
subsequence	O
,	O
respectively	O
.	O
System	O
scores	O
,	O
per	O
summary	O
length	O
,	O
are	O
obtained	O
by	O
averaging	O
across	O
all	O
summarized	O
texts	O
.	O
We	O
then	O
calculate	O
their	O
Pearson	B-MetricName
correlation	I-MetricName
2	O
with	O
the	O
available	O
human	O
mean	O
content	O
coverage	O
scores	O
for	O
the	O
systems	O
.	O
The	O
first	O
row	O
of	O
Table	O
2	O
shows	O
these	O
correlations	O
,	O
considering	O
the	O
R	O
-	O
1	O
scores	O
for	O
the	O
DUC	O
2001	O
systems	O
,	O
per	O
summary	O
length	O
.	O
The	O
subsequent	O
rows	O
show	O
the	O
corresponding	O
figures	O
for	O
the	O
single	O
-	O
reference	O
-	O
length	O
configurations	O
.	O
For	O
readability	O
,	O
we	O
present	O
in	O
these	O
rows	O
the	O
relative	O
differences	O
to	O
the	O
Standard	O
baseline	O
row	O
.	O
Hence	O
,	O
positive	O
values	O
indicate	O
a	O
configuration	O
that	O
is	O
at	O
least	O
as	O
good	O
as	O
the	O
standard	O
configuration	O
.	O
Table	O
3	O
presents	O
correlations	O
averaged	O
over	O
all	O
summary	O
lengths	O
,	O
for	O
the	O
three	O
ROUGE	O
variants	O
over	O
both	O
datasets	O
.	O
We	O
see	O
in	O
the	O
tables	O
that	O
evaluating	O
system	O
summaries	O
of	O
all	O
lengths	O
against	O
references	O
of	O
a	O
single	O
length	O
often	O
performs	O
on	O
par	O
with	O
the	O
standard	O
configuration	O
.	O
In	O
particular	O
,	O
the	O
single	O
fixed	O
set	O
of	O
50	O
-	O
word	O
reference	O
summaries	O
performs	O
overall	O
as	O
well	O
as	O
the	O
standard	O
approach	O
,	O
and	O
,	O
although	O
not	O
substantially	O
,	O
is	O
the	O
most	O
effective	O
configuration	O
within	O
the	O
data	O
analyzed	O
.	O
In	O
other	O
words	O
,	O
in	O
this	O
dataset	O
,	O
the	O
50	O
-	O
word	O
reference	O
summaries	O
provide	O
a	O
"	O
test	O
sample	O
"	O
for	O
evaluating	O
the	O
longer	O
system	O
summaries	O
,	O
which	O
is	O
as	O
effective	O
as	O
the	O
same	O
length	O
references	O
used	O
by	O
the	O
standard	O
method	O
.	O
We	O
note	O
that	O
even	O
when	O
a	O
single	O
reference	O
summary	O
is	O
available	O
,	O
reasonable	O
correlations	O
with	O
human	O
scores	O
are	O
obtained	O
for	O
the	O
50	O
word	O
reference	O
.	O
This	O
suggests	O
that	O
it	O
may	O
be	O
possible	O
to	O
compare	O
system	O
summaries	O
of	O
multiple	O
lengths	O
even	O
against	O
a	O
single	O
reference	O
summary	O
,	O
of	O
a	O
relatively	O
short	O
length	O
.	O
This	O
observation	O
seems	O
to	O
deserve	O
further	O
assessment	O
over	O
recent	O
large	O
scale	O
datasets	O
,	O
such	O
as	O
CNN	O
/	O
DailyMail	O
,	O
which	O
provide	O
a	O
single	O
relatively	O
short	O
reference	O
for	O
each	O
summarized	O
text	O
.	O
In	O
addition	O
to	O
correlation	O
to	O
human	O
assessment	O
,	O
we	O
computed	O
the	O
correlations	O
between	O
system	O
rankings	O
calculated	O
by	O
Standard	O
and	O
those	O
calcu	O
-	O
(	O
Gillick	O
et	O
al	O
,	O
2008	O
)	O
.	O
lated	O
by	O
Only50	O
,	O
at	O
each	O
system	O
summary	O
length	O
.	O
We	O
find	O
very	O
high	O
correlations	O
(	O
above	O
0.95	O
for	O
all	O
system	O
summary	O
lengths	O
,	O
in	O
both	O
datasets	O
)	O
when	O
using	O
multiple	O
references	O
and	O
slightly	O
lower	O
(	O
0.85	O
to	O
0.9	O
)	O
with	O
one	O
reference	O
summary	O
.	O
These	O
figures	O
show	O
that	O
the	O
Only50	O
configuration	O
ranks	O
systems	O
very	O
similarly	O
to	O
Standard	O
.	O
To	O
further	O
verify	O
our	O
results	O
,	O
we	O
computed	O
correlations	O
in	O
two	O
additional	O
settings	O
.	O
First	O
,	O
we	O
conducted	O
the	O
same	O
analysis	O
,	O
excluding	O
2	O
-	O
3	O
of	O
the	O
worst	O
systems	O
,	O
which	O
might	O
artificially	O
boost	O
the	O
correlation	O
(	O
Rankel	O
et	O
al	O
,	O
2013	O
)	O
.	O
Second	O
,	O
we	O
computed	O
score	O
differences	O
between	O
all	O
pairs	O
of	O
systems	O
,	O
for	O
both	O
human	O
and	O
ROUGE	O
scores	O
,	O
and	O
computed	O
the	O
correlation	O
between	O
these	O
two	O
sets	O
of	O
differences	O
(	O
Rankel	O
et	O
al	O
,	O
2011	O
)	O
.	O
In	O
both	O
cases	O
we	O
observed	O
rather	O
consistent	O
results	O
,	O
assessing	O
that	O
a	O
single	O
set	O
of	O
short	O
reference	O
summaries	O
evaluates	O
system	O
summaries	O
of	O
different	O
lengths	O
just	O
as	O
well	O
as	O
the	O
standard	O
configuration	O
.	O

This	O
section	O
illustrates	O
how	O
system	O
performances	O
can	O
be	O
measured	O
and	O
compared	O
when	O
evaluating	O
them	O
on	O
outputs	O
of	O
varying	O
lengths	O
against	O
a	O
single	O
reference	O
point	O
.	O
Figure	O
1	O
presents	O
the	O
ROUGE	O
scores	O
of	O
the	O
Only50	O
configuration	O
for	O
three	O
DUC	O
-	O
01	O
submitted	O
systems	O
,	O
and	O
for	O
ICSISumm	O
(	O
Gillick	O
et	O
al	O
,	O
2008	O
)	O
,	O
a	O
later	O
competitive	O
system	O
.	O
As	O
expected	O
when	O
measuring	O
ROUGE	O
Recall	B-MetricName
against	O
a	O
fixed	O
reference	O
length	O
,	O
longer	O
system	O
summaries	O
typically	O
cover	O
more	O
of	O
the	O
reference	O
summaries	O
content	O
than	O
shorter	O
ones	O
,	O
yielding	O
higher	O
scores	O
.	O
Yet	O
,	O
it	O
can	O
be	O
noted	O
,	O
for	O
example	O
,	O
that	O
the	O
value	O
of	O
the	O
400	O
-	O
word	O
summary	O
of	O
system	O
R	O
in	O
the	O
figure	O
is	O
lower	O
than	O
that	O
of	O
the	O
200	O
-	O
word	O
summaries	O
of	O
the	O
other	O
systems	O
.	O
Such	O
a	O
compar	O
-	O
ison	O
is	O
impossible	O
in	O
the	O
standard	O
setup	O
,	O
as	O
each	O
system	O
length	O
is	O
evaluated	O
against	O
different	O
reference	O
summaries	O
.	O
We	O
note	O
that	O
similar	O
comparisons	O
are	O
embedded	O
in	O
the	O
evaluations	O
of	O
Steinberger	O
and	O
Jezek	O
(	O
2004	O
)	O
and	O
Kikuchi	O
et	O
al	O
(	O
2016	O
)	O
,	O
who	O
also	O
evaluated	O
multiple	O
summary	O
lengths	O
.	O
Further	O
,	O
one	O
can	O
define	O
the	O
marginal	O
value	O
of	O
longer	O
summaries	O
of	O
a	O
given	O
system	O
as	O
the	O
ROUGE	O
score	O
increase	O
per	O
number	O
of	O
additional	O
words	O
,	O
namely	O
the	O
graph	O
slope	O
.	O
This	O
denotation	O
allows	O
measuring	O
the	O
effectiveness	O
of	O
producing	O
longer	O
summaries	O
.	O
For	O
example	O
,	O
deploying	O
system	O
R	O
,	O
we	O
might	O
decide	O
to	O
output	O
only	O
summaries	O
no	O
longer	O
than	O
200	O
words	O
,	O
since	O
the	O
marginal	O
value	O
of	O
longer	O
summaries	O
becomes	O
too	O
small	O
.	O
The	O
other	O
systems	O
,	O
on	O
the	O
other	O
hand	O
,	O
seem	O
marginally	O
effective	O
also	O
in	O
400	O
word	O
summaries	O
.	O

The	O
dataset	O
construction	O
consist	O
of	O
(	O
1	O
)	O
crowdsourcing	O
the	O
dialogues	O
,	O
(	O
2	O
)	O
annotating	O
dialog	O
acts	O
and	O
entities	O
and	O
(	O
3	O
)	O
linking	O
utterances	O
into	O
grounded	O
knowledge	O
.	O
We	O
explain	O
these	O
three	O
steps	O
in	O
order	O
and	O
present	O
the	O
dataset	O
statistics	O
in	O
the	O
end	O
.	O
Dialogue	O
Crowd	O
-	O
sourcing	O
We	O
obtain	O
the	O
dialogue	O
dataset	O
through	O
a	O
two	O
-	O
phase	O
Wizard	O
-	O
of	O
-	O
Ozstyle	O
collection	O
(	O
Kelley	O
,	O
1984	O
;	O
Dahlbäck	O
et	O
al	O
,	O
1993	O
)	O
.	O
In	O
the	O
first	O
phase	O
,	O
we	O
run	O
small	O
-	O
scale	O
pilot	O
studies	O
and	O
examine	O
the	O
quality	O
of	O
collected	O
conversations	O
.	O
Based	O
on	O
the	O
examination	O
,	O
we	O
created	O
tutorials	O
and	O
qualification	O
tests	O
.	O
They	O
are	O
used	O
to	O
train	O
and	O
qualify	O
crowd	O
-	O
workers	O
for	O
the	O
second	O
phase	O
.	O
During	O
this	O
second	O
phase	O
,	O
we	O
consistently	O
monitor	O
the	O
collected	O
dialogue	O
datasets	O
and	O
perform	O
periodic	O
quality	O
check	O
on	O
samples	O
from	O
every	O
individual	O
work	O
pairs	O
.	O
If	O
more	O
than	O
5	O
%	O
from	O
one	O
pair	O
are	O
considered	O
invalid	O
,	O
their	O
collections	O
will	O
be	O
removed	O
.	O
Before	O
a	O
conversation	O
started	O
,	O
two	O
workers	O
are	O
paired	O
and	O
a	O
movie	O
is	O
chosen	O
agreed	O
by	O
both	O
4	O
.	O
We	O
constrain	O
at	O
least	O
one	O
of	O
them	O
to	O
have	O
watched	O
the	O
movie	O
to	O
make	O
sure	O
the	O
conversation	O
is	O
contentful	O
5	O
.	O
The	O
annotators	O
are	O
especially	O
instructed	O
to	O
(	O
1	O
)	O
behave	O
naturally	O
as	O
in	O
daily	O
life	O
,	O
(	O
2	O
)	O
avoid	O
dirty	O
words	O
and	O
(	O
3	O
)	O
talk	O
differently	O
in	O
each	O
conversation	O
.	O
Duplicate	O
conversations	O
will	O
be	O
removed	O
if	O
more	O
than	O
70	O
%	O
of	O
their	O
contents	O
are	O
overlapped	O
.	O
To	O
encourage	O
diverse	O
movies	O
,	O
we	O
further	O
set	O
an	O
upper	O
limit	O
to	O
forbid	O
one	O
movie	O
from	O
being	O
talked	O
about	O
for	O
more	O
than	O
100	O
times	O
.	O
The	O
whole	O
collecting	O
process	O
lasts	O
two	O
months	O
.	O
In	O
the	O
end	O
,	O
245	O
participants	O
are	O
involved	O
with	O
66	O
,	O
424	O
movies	O
being	O
talked	O
about	O
in	O
total	O
.	O
Dialogue	O
Act	O
and	O
Entity	O
Annotation	O
Following	O
prior	O
work	O
,	O
we	O
base	O
our	O
annotation	O
schema	O
on	O
the	O
ISO	O
24617	O
-	O
2	O
standard	O
(	O
Bunt	O
et	O
al	O
,	O
2010	O
(	O
Bunt	O
et	O
al	O
,	O
,	O
2012	O
.	O
Table	O
1	O
shows	O
our	O
annotation	O
schema	O
,	O
counts	O
,	O
descriptions	O
,	O
and	O
brief	O
examples	O
.	O
The	O
dialogue	O
acts	O
(	O
DAs	O
)	O
are	O
organized	O
in	O
a	O
hierarchical	O
structure	O
.	O
The	O
first	O
layer	O
makes	O
distinctions	O
on	O
three	O
concepts	O
:	O
objective	O
facts	O
,	O
recommendations	O
and	O
subjective	O
feelings	O
.	O
Each	O
concept	O
can	O
either	O
be	O
either	O
requested	O
or	O
informed	O
during	O
the	O
conversation	O
.	O
We	O
further	O
define	O
an	O
"	O
Other	O
"	O
class	O
to	O
include	O
actions	O
that	O
do	O
not	O
belong	O
to	O
any	O
of	O
the	O
three	O
concepts	O
,	O
like	O
some	O
general	O
non	O
-	O
contentful	O
greetings	O
or	O
echos	O
.	O
The	O
second	O
layer	O
includes	O
15	O
finer	O
-	O
grained	O
aspects	O
covering	O
most	O
popular	O
topics	O
being	O
discussed	O
.	O
Every	O
first	O
-	O
layer	O
DA	O
(	O
except	O
Other	O
)	O
will	O
be	O
further	O
group	O
it	O
into	O
one	O
of	O
these	O
15	O
aspects	O
,	O
e.g.	O
,	O
the	O
de	O
-	O
tailed	O
DA	O
of	O
the	O
first	O
example	O
in	O
Table	O
1	O
will	O
be	O
request	O
fact	O
director	O
.	O
If	O
one	O
utterance	O
contains	O
multiple	O
dialogue	O
acts	O
,	O
we	O
order	O
the	O
dialogue	O
acts	O
based	O
on	O
their	O
turn	O
of	O
appearance	O
in	O
the	O
utterance	O
.	O
As	O
for	O
the	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
,	O
we	O
labeled	O
5	O
kinds	O
of	O
entities	O
:	O
movie	O
names	O
,	O
director	O
,	O
actor	O
,	O
type	O
and	O
role	O
(	O
first	O
5	O
aspects	O
)	O
.	O
To	O
speed	O
up	O
the	O
annotation	O
process	O
,	O
we	O
first	O
define	O
a	O
set	O
of	O
handcrafted	O
regular	O
expressions	O
,	O
which	O
covers	O
most	O
frequent	O
patterns	O
at	O
each	O
class	O
,	O
to	O
train	O
a	O
DA	O
and	O
NER	B-TaskName
classifier	O
.	O
The	O
annotators	O
are	O
instructed	O
to	O
post	O
-	O
correct	O
the	O
auto	O
-	O
labeled	O
dialogues	O
instead	O
of	O
doing	O
everything	O
from	O
scratch	O
.	O
The	O
classifiers	O
are	O
trained	O
with	O
online	B-TaskName
learning	I-TaskName
(	O
Sahoo	O
et	O
al	O
,	O
2018	O
)	O
to	O
keep	O
improving	O
the	O
accuracy	B-MetricName
and	O
lower	O
down	O
the	O
frequency	O
of	O
post	O
-	O
correction	O
in	O
consequence	O
.	O
As	O
we	O
observe	O
,	O
this	O
semi	O
-	O
automated	O
way	O
significantly	O
speeds	O
up	O
the	O
labeling	O
process	O
.	O
All	O
the	O
dataset	O
is	O
finished	O
labeling	O
within	O
three	O
weeks	O
with	O
188	O
annotators	O
involved	O
.	O
Knowledge	O
Linkage	O
We	O
extract	O
fact	O
knowledge	O
from	O
the	O
structured	O
table	O
in	O
Douban	B-DatasetName
Movie	O
6	O
,	O
a	O
popular	O
Chinese	O
platform	O
for	O
movies	O
.	O
The	O
knowledge	O
is	O
organized	O
in	O
the	O
form	O
of	O
key	O
-	O
value	O
pairs	O
,	O
where	O
the	O
key	O
corresponds	O
to	O
the	O
15	O
aspects	O
defined	O
by	O
us	O
.	O
Some	O
aspects	O
,	O
like	O
lines	O
or	O
music	O
,	O
are	O
not	O
directly	O
available	O
from	O
the	O
structured	O
table	O
.	O
We	O
extract	O
these	O
missing	O
information	O
from	O
other	O
sources	O
and	O
combine	O
it	O
into	O
our	O
knowledge	O
base	O
.	O
For	O
utterances	O
labeld	O
as	O
inform	O
/	O
request	O
fact	O
,	O
we	O
link	O
them	O
to	O
the	O
key	O
-	O
value	O
pairs	O
from	O
the	O
same	O
aspect	O
.	O
Apart	O
from	O
the	O
objective	O
knowledge	O
,	O
we	O
also	O
crawl	O
movie	O
comments	O
from	O
Douban	B-DatasetName
Movie	O
to	O
support	O
the	O
generation	O
of	O
responses	O
expressing	O
subjective	O
feelings	O
.	O
These	O
comments	O
can	O
be	O
a	O
good	O
supplementary	O
to	O
provide	O
knowledge	O
that	O
can	O
be	O
hardly	O
organized	O
in	O
the	O
structured	O
form	O
(	O
Moghe	O
et	O
al	O
,	O
2018	O
)	O
.	O
For	O
utterances	O
labeled	O
as	O
inform	O
/	O
request	O
feeling	O
,	O
we	O
compare	O
them	O
with	O
Douban	B-DatasetName
comments	O
from	O
the	O
same	O
movie	O
and	O
compute	O
the	O
similarity	O
score	O
based	O
on	O
weighted	O
average	O
of	O
edit	O
distance	O
,	O
Jaccard	O
distance	O
,	O
tf	O
-	O
idf	O
,	O
sentence	O
vector	O
cosine	O
similarity	O
,	O
common	O
words	O
and	O
entities	O
.	O
Each	O
utterance	O
is	O
linked	O
to	O
the	O
most	O
similar	O
comment	O
with	O
a	O
threshold	O
cutoff	O
.	O
In	O
the	O
end	O
,	O
51.7	O
%	O
of	O
the	O
utterances	O
about	O
feelings	O
have	O
grounded	O
comments	O
.	O
For	O
utterances	O
about	O
recommendations	O
,	O
we	O
simply	O
ground	O
them	O
to	O
the	O
men	O
-	O
tioned	O
movie	O
entities	O
7	O
,	O
and	O
no	O
grounded	O
knowledge	O
is	O
linked	O
for	O
utterances	O
labeled	O
as	O
Other	O
.	O
An	O
example	O
of	O
our	O
annotation	O
is	O
presented	O
in	O
Table	O
1	O
.	O

The	O
intent	O
prediction	O
is	O
also	O
cast	O
as	O
a	O
sequence	O
prediction	O
task	O
.	O
Compared	O
with	O
the	O
traditional	O
way	O
of	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
,	O
casting	O
it	O
as	O
sequence	O
prediction	O
is	O
better	O
at	O
addressing	O
the	O
coexistence	O
of	O
multiple	O
DAs	O
and	O
capturing	O
the	O
sequential	O
dependencies	O
among	O
the	O
hierarchy	O
(	O
Raffel	O
et	O
al	O
,	O
2019	O
;	O
Vedula	O
et	O
al	O
,	O
2020	O
)	O
.	O
For	O
example	O
,	O
to	O
predict	O
the	O
DAs	O
of	O
the	O
4th	O
utterance	O
in	O
Figure	O
1	O
,	O
the	O
sequence	O
fed	O
to	O
the	O
language	O
model	O
will	O
be	O
"	O
[	O
context	O
]	O
dialogue	O
context	O
[	O
intent	O
]	O
inform	O
,	O
feeling	O
,	O
plot	O
,	O
request	O
,	O
fact	O
,	O
plot	O
"	O
.	O
By	O
this	O
means	O
,	O
before	O
predicting	O
a	O
DA	O
,	O
the	O
model	O
can	O
condition	O
on	O
both	O
the	O
dialogue	O
context	O
and	O
its	O
previous	O
DAs	O
to	O
improve	O
the	O
accuracy	B-MetricName
.	O

Automatic	O
Evaluation	O
In	O
Table	O
5	O
,	O
we	O
report	O
the	O
perplexity	B-MetricName
,	O
BLEU	B-MetricName
scores	O
and	O
distinct	O
uni	O
/	O
bigrams	O
for	O
three	O
model	O
sizes	O
.	O
To	O
investigate	O
the	O
effects	O
of	O
incorporating	O
annotations	O
and	O
pretraining	O
,	O
we	O
start	O
from	O
a	O
basic	O
model	O
which	O
trains	O
from	O
scratch	O
on	O
our	O
movie	O
corpus	O
.	O
At	O
each	O
time	O
,	O
we	O
add	O
one	O
more	O
condition	O
to	O
see	O
its	O
influence	O
.	O
The	O
results	O
show	O
a	O
clear	O
tendency	O
of	O
gradual	O
improvement	O
as	O
more	O
conditions	O
are	O
added	O
to	O
the	O
training	O
.	O
Adding	O
knowledge	O
especially	O
boosts	O
the	O
performance	O
,	O
which	O
is	O
understandable	O
considering	O
movie	O
-	O
domain	O
chats	O
usually	O
contain	O
many	O
movie	O
-	O
specific	O
rare	O
names	O
.	O
Without	O
knowledge	O
grounding	O
,	O
it	O
can	O
hardly	O
predict	O
the	O
correct	O
tokens	O
.	O
Pretraining	O
on	O
general	O
-	O
domain	O
conversations	O
can	O
improve	O
both	O
the	O
overlap	O
with	O
ground	O
truth	O
.	O
The	O
distinct	O
uni	O
/	O
bigrams	O
also	O
consistently	O
increase	O
,	O
implying	O
the	O
model	O
can	O
learn	O
useful	O
patterns	O
in	O
the	O
pretrained	O
corpus	O
to	O
enrich	O
its	O
generations	O
in	O
the	O
movie	O
domain	O
.	O
In	O
unseen	O
testset	O
,	O
the	O
performance	O
generally	O
drops	O
for	O
all	O
,	O
especially	O
for	O
models	O
without	O
knowledge	O
grounding	O
as	O
they	O
have	O
to	O
make	O
up	O
facts	O
and	O
comments	O
for	O
totally	O
unseen	O
movies	O
in	O
the	O
training	O
set	O
.	O
Table	O
6	O
measures	O
the	O
accuracy	B-MetricName
of	O
predicting	O
dialogue	O
act	O
(	O
DA	O
)	O
,	O
aspect	O
and	O
movie	O
tracker	O
of	O
our	O
model	O
.	O
Our	O
models	O
are	O
all	O
pretrained	O
with	O
general	O
-	O
domain	O
corpus	O
beforehand	O
.	O
Apart	O
from	O
being	O
trained	O
only	O
to	O
predict	O
the	O
individual	O
tasks	O
,	O
we	O
include	O
the	O
results	O
where	O
all	O
subtasks	O
are	O
cotrained	O
end	O
-	O
to	O
-	O
end	O
in	O
the	O
last	O
line	O
.	O
We	O
compare	O
our	O
models	O
with	O
the	O
Chinese	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
Table	O
7	O
measures	O
the	O
performance	O
of	O
retrieving	O
fact	O
knowledge	O
,	O
movie	O
comments	O
and	O
recommen	O
-	O
dation	O
respectively	O
.	O
We	O
report	O
the	O
hit@1	O
and	O
hit@5	O
scores	O
for	O
them	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
compare	O
our	O
model	O
with	O
a	O
random	O
baseline	O
,	O
bag	O
-	O
ofword	O
(	O
BOW	O
)	O
and	O
the	O
Bert	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
model	O
(	O
we	O
pass	O
sentences	O
through	O
Bert	O
and	O
derive	O
a	O
fixed	O
-	O
sized	O
vector	O
by	O
averaging	O
the	O
outputs	O
from	O
the	O
second	O
-	O
to	O
-	O
last	O
layer	O
(	O
May	O
et	O
al	O
,	O
2019	O
)	O
)	O
.	O
The	O
BOW	O
and	O
Bert	O
model	O
are	O
finetuned	O
with	O
our	O
knowledge	O
linkage	O
annotations	O
.	O
We	O
find	O
that	O
our	O
unified	O
model	O
again	O
outperforms	O
all	O
baseline	O
approaches	O
.	O
Adding	O
the	O
DA	O
as	O
a	O
condition	O
further	O
helps	O
.	O
Fact	O
retrieval	O
has	O
the	O
highest	O
hit	O
rate	O
as	O
it	O
is	O
well	O
structured	O
and	O
easy	O
to	O
match	O
.	O
Recommendation	O
,	O
on	O
the	O
other	O
hand	O
,	O
is	O
very	O
hard	O
to	O
predict	O
.	O
As	O
an	O
accurate	O
recommendation	O
system	O
is	O
clearly	O
beyond	O
the	O
scope	O
of	O
this	O
paper	O
,	O
it	O
is	O
understandable	O
that	O
our	O
simple	O
way	O
fails	O
to	O
provide	O
satisfying	O
recommendations	O
.	O
Human	O
Evaluation	O
Automatically	O
evaluating	O
dialogue	O
systems	O
are	O
known	O
to	O
be	O
extremely	O
hard	O
(	O
Liu	O
et	O
al	O
,	O
2016	O
;	O
.	O
We	O
further	O
conduct	O
a	O
set	O
of	O
static	O
and	O
interactive	O
human	O
evaluations	O
.	O
We	O
focus	O
on	O
evaluate	O
the	O
machine	O
-	O
generated	O
response	O
from	O
four	O
perspectives	O
.	O
Apart	O
from	O
the	O
oft	O
-	O
used	O
metrics	O
(	O
1	O
)	O
Sensibleness	O
(	O
Sens	O
)	O
and	O
(	O
2	O
)	O
Engagement	O
(	O
Enga	O
)	O
for	O
open	O
-	O
domain	O
chatbots	O
,	O
we	O
further	O
evaluate	O
on	O
(	O
3	O
)	O
Factuality	O
(	O
Fact	O
)	O
and	O
(	O
4	O
)	O
Informativeness	O
(	O
Info	O
)	O
to	O
see	O
if	O
models	O
can	O
actively	O
provide	O
informative	O
responses	O
based	O
on	O
movie	O
facts	O
.	O
Details	O
are	O
in	O
Appendix	O
B.	O
As	O
evaluating	O
factuality	O
requires	O
specific	O
movie	O
knowledge	O
,	O
this	O
metric	O
is	O
only	O
evaluated	O
by	O
the	O
same	O
person	O
who	O
produced	O
the	O
dialogue	O
.	O
The	O
other	O
metrics	O
are	O
evaluated	O
by	O
3	O
workers	O
each	O
.	O
Table	O
8	O
shows	O
the	O
agreement	O
scores	O
.	O
The	O
agreement	O
is	O
reasonable	O
considering	O
the	O
evaluations	O
are	O
subjective	O
.	O
The	O
results	O
are	O
the	O
majority	O
votes	O
of	O
the	O
binary	O
scores	O
.	O
In	O
the	O
static	O
evaluation	O
,	O
we	O
sample	O
300	O
responses	O
for	O
each	O
model	O
from	O
the	O
test	O
set	O
(	O
mixing	O
seen	O
and	O
unseen	O
)	O
.	O
The	O
responses	O
can	O
come	O
from	O
any	O
turn	O
in	O
a	O
conversation	O
.	O
We	O
show	O
the	O
results	O
in	O
Figure	O
2	O
.	O
Our	O
largest	O
model	O
with	O
762	O
M	O
is	O
clearly	O
preferred	O
by	O
human	O
evaluators	O
on	O
almost	O
all	O
metrics	O
and	O
approaches	O
human	O
performance	O
.	O
By	O
training	O
a	O
larger	O
model	O
and	O
increasing	O
the	O
training	O
size	O
,	O
the	O
gap	O
might	O
be	O
further	O
closed	O
.	O
In	O
the	O
interactive	O
evaluation	O
,	O
humans	O
can	O
chat	O
with	O
any	O
topic	O
but	O
restricted	O
in	O
the	O
movie	O
domain	O
.	O
We	O
conduct	O
an	O
online	O
Turing	O
test	O
where	O
one	O
side	O
is	O
always	O
a	O
human	O
participant	O
not	O
aware	O
whom	O
he	O
is	O
talking	O
with	O
.	O
The	O
other	O
side	O
could	O
be	O
either	O
Mitsuku	O
,	O
XiaoIce	O
13	O
,	O
our	O
model	O
(	O
762	O
M	O
with	O
pretraining	O
)	O
or	O
a	O
real	O
human	O
.	O
Mitsuku	O
interacts	O
in	O
English	O
,	O
so	O
we	O
hire	O
only	O
English	O
native	O
speakers	O
for	O
the	O
experiment	O
.	O
We	O
collect	O
100	O
conversations	O
for	O
all	O
models	O
.	O
Humans	O
can	O
stop	O
interacting	O
once	O
they	O
(	O
1	O
)	O
find	O
the	O
other	O
side	O
is	O
a	O
machine	O
or	O
(	O
2	O
)	O
reaches	O
the	O
maximum	O
turn	O
of	O
20	O
.	O
Responses	O
from	O
all	O
models	O
are	O
later	O
passed	O
to	O
the	O
third	O
party	O
to	O
judge	O
the	O
scores	O
.	O
The	O
results	O
are	O
shown	O
on	O
the	O
right	O
of	O
Figure	O
2	O
.	O
Our	O
model	O
outperforms	O
Mitsuku	O
and	O
XiaoIce	O
by	O
a	O
large	O
margin	O
.	O
As	O
Mitsuku	O
and	O
XiaoIce	O
are	O
designed	O
to	O
be	O
open	O
-	O
domain	O
chatbots	O
,	O
restricting	O
to	O
be	O
on	O
the	O
movie	O
domain	O
will	O
give	O
our	O
model	O
some	O
natural	O
advantage	O
.	O
We	O
can	O
also	O
notice	O
that	O
Mitsuku	O
and	O
XiaoIce	O
almost	O
never	O
produce	O
fake	O
facts	O
.	O
The	O
cost	O
is	O
the	O
extremely	O
low	O
ratio	O
of	O
informative	O
responses	O
since	O
they	O
tend	O
to	O
behave	O
over	O
-	O
safely	O
and	O
will	O
only	O
answer	O
it	O
when	O
they	O
are	O
100	O
%	O
sure	O
.	O
Our	O
model	O
is	O
closer	O
to	O
humans	O
in	O
that	O
sense	O
.	O
It	O
will	O
converse	B-DatasetName
actively	O
at	O
some	O
risk	O
of	O
containing	O
fact	O
errors	O
.	O
Figure	O
3	O
:	O
Change	O
of	O
SEA	O
and	O
FIA	O
as	O
the	O
turn	O
proceeds	O
.	O
13	O
We	O
use	O
its	O
chat	O
service	O
through	O
Weibo	B-DatasetName
.	O
It	O
will	O
sometimes	O
generate	O
responses	O
containing	O
keywords	O
like	O
"	O
XiaoIce	O
"	O
.	O
We	O
manually	O
replace	O
it	O
to	O
prevent	O
disclosing	O
its	O
identity	O
.	O
Distance	O
from	O
Human	O
Performance	O
In	O
the	O
interactive	O
evaluation	O
,	O
compared	O
with	O
human	O
performance	O
,	O
our	O
model	O
loses	O
a	O
bit	O
on	O
sensibleness	O
and	O
factuality	O
but	O
wins	O
on	O
the	O
other	O
two	O
.	O
To	O
investigate	O
where	O
our	O
model	O
fails	O
,	O
figure	O
3	O
visualizes	O
the	O
change	O
of	O
SSA	O
(	O
Sensibleness	O
-	O
Engagement	O
average	O
)	O
and	O
FIA	O
(	O
Factuality	O
-	O
Informativeness	O
average	O
)	O
when	O
the	O
conversational	O
turn	O
proceeds	O
.	O
A	O
good	O
chatbot	B-TaskName
should	O
balance	O
well	O
these	O
skills	O
(	O
Adiwardana	O
et	O
al	O
,	O
2020	O
)	O
.	O
SEA	O
can	O
reflect	O
how	O
it	O
behaves	O
as	O
a	O
general	O
chatbot	B-TaskName
while	O
FIA	O
can	O
better	O
test	O
its	O
capability	O
at	O
incorporating	O
domain	O
knowledge	O
.	O
We	O
can	O
see	O
a	O
clear	O
trend	O
of	O
decrease	O
for	O
all	O
models	O
.	O
As	O
for	O
human	O
performance	O
,	O
however	O
,	O
the	O
score	O
is	O
quite	O
consistent	O
across	O
turn	O
rounds	O
,	O
implying	O
a	O
large	O
improvement	O
space	O
for	O
current	O
models	O
to	O
deal	O
with	O
multi	O
-	O
turn	O
context	O
.	O
In	O
figure	O
4	O
,	O
we	O
further	O
show	O
the	O
"	O
dying	O
distribution	O
"	O
of	O
our	O
model	O
,	O
namely	O
,	O
in	O
which	O
DA	O
our	O
model	O
fails	O
to	O
pass	O
the	O
Turing	O
test	O
and	O
thereby	O
"	O
dies	O
"	O
.	O
Unsurprisingly	O
,	O
we	O
can	O
see	O
the	O
system	O
fails	O
mostly	O
when	O
informing	O
facts	O
or	O
feelings	O
.	O
Only	O
a	O
small	O
portion	O
are	O
from	O
non	O
-	O
grounded	O
chitchats	O
(	O
other	O
)	O
.	O
This	O
suggests	O
the	O
most	O
crucial	O
bottleneck	O
lies	O
in	O
the	O
interaction	O
with	O
movie	O
-	O
specific	O
knowledge	O
and	O
seamlessly	O
incorporating	O
it	O
into	O
the	O
response	B-TaskName
generation	I-TaskName
.	O
We	O
show	O
some	O
snippets	O
of	O
interactions	O
with	O
our	O
model	O
in	O
Table	O
9	O
.	O
The	O
first	O
two	O
are	O
failing	O
cases	O
labeled	O
by	O
humans	O
as	O
not	O
factual	O
and	O
sensible	O
.	O
We	O
can	O
see	O
the	O
model	O
struggles	O
at	O
replying	O
to	O
too	O
specific	O
facts	O
.	O
This	O
is	O
understandable	O
since	O
our	O
knowledge	O
base	O
only	O
provide	O
short	O
introductions	O
and	O
can	O
not	O
cover	O
all	O
what	O
happened	O
in	O
the	O
movie	O
.	O
The	O
second	O
case	O
shows	O
its	O
shortcoming	O
at	O
handing	O
long	O
-	O
range	O
consistency	O
.	O
It	O
still	O
recommends	O
the	O
current	O
movie	O
when	O
the	O
user	O
asks	O
about	O
"	O
which	O
other	O
movie	O
"	O
.	O
Employing	O
larger	O
knowledge	O
bases	O
and	O
explicitly	O
tracking	O
the	O
states	O
by	O
a	O
checklist	O
(	O
Kiddon	O
et	O
al	O
,	O
2016	O
)	O
might	O
potentially	O
alleviate	O
both	O
issue	O
.	O
We	O
also	O
provide	O
examples	O
for	O
controllable	O
generations	O
where	O
the	O
DA	O
and	O
aspect	O
are	O
manually	O
assigned	O
.	O
As	O
observed	O
,	O
the	O
model	O
shows	O
decent	O
performance	O
at	O
fitting	O
both	O
the	O
dialogue	O
con	O
-	O
text	O
and	O
specified	O
conditions	O
.	O
This	O
can	O
be	O
helpful	O
when	O
finer	O
-	O
grained	O
control	O
is	O
needed	O
.	O

Before	O
training	O
each	O
classifier	O
,	O
we	O
employ	O
the	O
best	O
performing	O
top	O
features	O
from	O
the	O
Figure	O
2	O
,	O
where	O
every	O
classifier	O
has	O
its	O
most	O
fitting	O
top	O
features	O
for	O
each	O
subtask	O
.	O
Next	O
,	O
we	O
construct	O
a	O
LOO	O
crossvalidation	O
framework	O
for	O
within	O
-	O
dataset	O
evaluations	O
.	O
1	O
It	O
is	O
important	O
to	O
note	O
that	O
,	O
in	O
each	O
step	O
of	O
the	O
LOO	O
,	O
we	O
choose	O
new	O
user	O
ids	O
for	O
evaluation	O
and	O
completely	O
exclude	O
all	O
of	O
their	O
tweets	O
from	O
the	O
training	O
sets	O
to	O
evade	O
ML	O
methods	O
potentially	O
learning	O
the	O
way	O
a	O
person	O
drafts	O
tweets	O
.	O
That	O
means	O
the	O
within	O
-	O
dataset	O
LOO	O
results	O
of	O
a	O
subtask	O
are	O
reported	O
for	O
all	O
users	O
of	O
the	O
labeled	O
set	O
.	O
Moreover	O
,	O
the	O
labeled	O
datasets	O
have	O
more	O
users	O
than	O
the	O
unlabeled	O
test	O
sets	O
per	O
subtask	O
(	O
e.g.	O
57	O
vs.	O
11	O
suicidal	O
users	O
in	O
subtask1	O
)	O
.	O
Ergo	O
,	O
we	O
expect	O
a	O
high	O
magnitudinal	O
difference	O
between	O
the	O
within	O
-	O
dataset	O
and	O
the	O
test	O
results	O
.	O
The	O
within	O
-	O
dataset	O
evaluation	O
results	O
of	O
the	O
selected	O
methods	O
are	O
in	O
Table	O
1	O
.	O
For	O
subtask	O
1	O
,	O
we	O
obtain	O
the	O
best	O
LOO	O
cross	O
-	O
validation	O
score	O
from	O
the	O
wEns	O
method	O
that	O
combines	O
the	O
results	O
of	O
four	O
ML	O
methods	O
(	O
LR	O
,	O
MNB	O
,	O
GNB	O
,	O
lSVM	O
)	O
in	O
a	O
way	O
that	O
improves	O
the	O
results	O
obtained	O
from	O
each	O
of	O
them	O
.	O
Meanwhile	O
,	O
GRU	B-MethodName
-	O
Bert	O
and	O
MNB	O
return	O
the	O
lowest	O
false	O
positive	O
rates	O
(	O
FPR	O
)	O
for	O
this	O
subtask	O
,	O
which	O
might	O
be	O
a	O
critical	O
rate	O
to	O
consider	O
in	O
real	O
-	O
life	O
applications	O
in	O
social	O
media	O
domains	O
.	O
LOO	O
results	O
of	O
subtask	O
2	O
in	O
Table	O
1	O
show	O
that	O
wEns	O
returns	O
the	O
best	O
scores	O
for	O
the	O
longer	O
-	O
spanning	O
dataset	O
as	O
well	O
,	O
where	O
LR	O
returns	O
the	O
best	O
FPR	O
,	O
and	O
GBN	O
returns	O
the	O
highest	O
true	O
positives	O
rate	O
(	O
TPR	O
)	O
.	O
Based	O
on	O
the	O
LOO	O
results	O
,	O
we	O
select	O
three	O
different	O
methods	O
we	O
were	O
allowed	O
to	O
submit	O
for	O
the	O
evaluation	O
of	O
the	O
test	O
set	O
:	O
LR	O
,	O
wEns	O
,	O
and	O
GRU	B-MethodName
-	O
Bert	O
.	O
We	O
choose	O
LR	O
and	O
wEns	O
for	O
their	O
high	O
performance	O
on	O
LOO	O
experiments	O
,	O
while	O
we	O
select	O
GRU	B-MethodName
-	O
Bert	O
for	O
measuring	O
how	O
a	O
DL	O
method	O
would	O
generalize	O
over	O
the	O
test	O
sets	O
.	O
The	O
baseline	O
classifier	O
provided	O
by	O
the	O
organizers	O
is	O
also	O
a	O
logistic	B-MethodName
regression	I-MethodName
.	O
However	O
,	O
it	O
performs	O
the	O
classification	O
over	O
merged	O
tweets	O
of	O
users	O
-	O
therefore	O
is	O
different	O
from	O
our	O
implementation	O
of	O
LR	O
.	O
In	O
Table	O
2	O
,	O
wEns	O
appears	O
to	O
provide	O
the	O
best	O
F1	B-MetricName
,	O
F2	O
,	O
and	O
TPR	O
scores	O
over	O
the	O
test	O
set	O
of	O
subtask	O
1	O
,	O
while	O
our	O
LR	O
outperforms	O
the	O
AUC	B-MetricName
of	O
the	O
baseline	O
method	O
.	O
While	O
these	O
methods	O
show	O
the	O
success	O
of	O
generalizability	O
on	O
the	O
30	O
-	O
days	O
test	O
set	O
,	O
the	O
results	O
are	O
not	O
that	O
successful	O
for	O
subtask	O
2	O
.	O
The	O
wEns	O
method	O
performs	O
the	O
same	O
as	O
the	O
baseline	O
in	O
terms	O
of	O
TPR	O
,	O
but	O
the	O
rest	O
of	O
the	O
scores	O
are	O
lower	O
than	O
the	O
baseline	O
results	O
.	O

Pretrained	B-TaskName
Language	I-TaskName
Models	I-TaskName
(	O
PLM	O
)	O
are	O
predominant	O
in	O
tackling	O
current	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
tasks	O
.	O
Most	O
PLMs	O
based	O
on	O
the	O
Transformer	B-MethodName
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
are	O
first	O
trained	O
on	O
massive	O
text	O
corpora	O
with	O
the	O
selfsupervised	O
objective	O
to	O
learn	O
word	O
representations	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
then	O
are	O
fine	O
-	O
tuned	O
for	O
a	O
specific	O
target	O
task	O
.	O
The	O
pretraining	O
and	O
fine	O
-	O
tuning	O
of	O
PLMs	O
achieves	O
state	O
-	O
ofthe	O
-	O
art	O
(	O
SOTA	O
)	O
performance	O
in	O
many	O
NLP	O
tasks	O
.	O
Inspired	B-DatasetName
by	O
the	O
benefits	O
of	O
pretraining	O
,	O
there	O
have	O
been	O
studies	O
demonstrate	O
the	O
effects	O
of	O
continued	O
pretraining	O
on	O
the	O
domain	O
of	O
a	O
target	O
task	O
or	O
the	O
target	O
task	O
dataset	O
(	O
Mitra	O
et	O
al	O
,	O
2020	O
;	O
Han	O
and	O
Eisenstein	O
,	O
2019	O
;	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
.	O
Gururangan	O
et	O
al	O
,	O
2020	O
adapt	O
PLMs	O
on	O
the	O
target	O
task	O
by	O
further	O
pretraining	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
on	O
the	O
target	O
text	O
corpus	O
before	O
it	O
is	O
fine	O
-	O
tuned	O
for	O
the	O
corresponding	O
task	O
and	O
showed	O
that	O
this	O
task	O
adaptation	O
consistently	O
improves	O
the	O
performance	O
for	O
text	B-TaskName
classification	I-TaskName
tasks	O
.	O
However	O
,	O
this	O
full	O
process	O
of	O
pretraining	O
and	O
then	O
fine	O
-	O
tuning	O
can	O
be	O
parameter	O
inefficient	O
for	O
recent	O
PLMs	O
that	O
have	O
millions	O
or	O
billions	O
of	O
parameters	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
.	O
This	O
parameter	O
inefficiency	O
becomes	O
even	O
worse	O
when	O
one	O
continues	O
pre	O
-	O
training	O
all	O
the	O
parameters	O
of	O
PLMs	O
on	O
the	O
task	O
-	O
specific	O
corpus	O
.	O
Furthermore	O
,	O
recent	O
PLMs	O
need	O
more	O
than	O
100s	O
of	O
MB	O
to	O
store	O
all	O
the	O
weights	O
(	O
Liu	O
et	O
al	O
,	O
2019	O
;	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
,	O
making	O
it	O
difficult	O
to	O
download	O
and	O
share	O
the	O
pre	O
-	O
trained	O
models	O
on	O
the	O
fly	O
.	O
Recently	O
,	O
adapters	O
have	O
been	O
proposed	O
as	O
an	O
alternative	O
approach	O
to	O
decrease	O
the	O
substantial	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
of	O
PLMs	O
in	O
the	O
fine	O
-	O
tuning	O
stage	O
(	O
Houlsby	O
et	O
al	O
,	O
2019	O
)	O
.	O
Finetuning	O
with	O
adapters	O
mostly	O
matches	O
the	O
performance	O
of	O
those	O
with	O
the	O
full	O
fine	O
-	O
tuning	O
strategy	O
on	O
many	O
NLP	O
tasks	O
including	O
GLUE	B-DatasetName
benchmark	O
(	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
and	O
reduces	O
the	O
size	O
of	O
the	O
model	O
from	O
100s	O
of	O
MB	O
to	O
the	O
order	O
of	O
MB	O
(	O
Pfeiffer	O
et	O
al	O
,	O
2020b	O
)	O
.	O
As	O
such	O
,	O
a	O
natural	O
question	O
arises	O
from	O
the	O
successes	O
of	O
the	O
adapter	O
approach	O
:	O
can	O
the	O
adapter	O
alone	O
adapt	O
PLMs	O
to	O
the	O
target	O
task	O
when	O
it	O
is	O
used	O
in	O
the	O
second	O
phase	O
of	O
the	O
pretraining	O
stage	O
and	O
thus	O
lead	O
to	O
the	O
improvement	O
of	O
the	O
performance	O
on	O
the	O
corresponding	O
task	O
?	O
In	O
this	O
paper	O
,	O
we	O
explore	O
task	O
-	O
adaptive	O
pretraining	O
,	O
termed	O
TAPT	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
,	O
with	O
adapters	O
to	O
address	O
this	O
question	O
and	O
overcome	O
the	O
limitations	O
of	O
the	O
conventional	O
full	O
pretraining	O
and	O
fine	O
-	O
tuning	O
.	O
We	O
only	O
train	O
the	O
adapter	O
modules	O
in	O
the	O
second	O
phase	O
of	O
pretraining	O
as	O
well	O
as	O
the	O
fine	O
-	O
tuning	O
stage	O
to	O
achieve	O
both	O
parameter	O
efficiency	O
and	O
the	O
benefits	O
of	O
continual	B-TaskName
pretraining	I-TaskName
and	O
compare	O
those	O
with	O
the	O
adapter	O
-	O
based	O
model	O
without	O
pretraining	O
.	O
Surprisingly	O
,	O
we	O
find	O
that	O
directly	O
fine	O
-	O
tuning	O
adapters	O
performs	O
mostly	O
on	O
par	O
with	O
the	O
pre	O
-	O
trained	O
adapter	O
model	O
and	O
outperforms	O
the	O
full	O
TAPT	O
,	O
contradicting	O
the	O
previously	O
proposed	O
benefits	O
of	O
continual	B-TaskName
pretraining	I-TaskName
in	O
the	O
full	O
pretraining	O
fine	O
-	O
tuning	O
scheme	O
.	O
As	O
directly	O
fine	O
-	O
tuning	O
adapters	O
skips	O
the	O
second	O
phase	O
of	O
pretraining	O
and	O
the	O
training	O
steps	O
of	O
adapters	O
are	O
faster	O
than	O
those	O
of	O
the	O
full	O
model	O
,	O
it	O
substantially	O
reduces	O
the	O
training	O
time	O
.	O
We	O
further	O
investigate	O
different	O
hyperparameter	O
settings	O
that	O
affect	O
the	O
effectiveness	O
of	O
pretraining	O
.	O

Our	O
implementation	O
is	O
based	O
on	O
HuggingFace	O
since	O
we	O
found	O
AllenNLP	O
(	O
Gardner	O
et	O
al	O
,	O
2018	O
)	O
used	O
in	O
Gururangan	O
et	O
al	O
,	O
2020	O
is	O
incompatible	O
with	O
adapter	O
-	O
transformer	O
(	O
Pfeiffer	O
et	O
al	O
,	O
2020b	O
)	O
.	O
We	O
follow	O
the	O
hyperparameters	O
setting	O
in	O
Gururangan	O
et	O
al	O
,	O
2020	O
,	O
and	O
each	O
model	O
in	O
the	O
pretraining	O
and	O
fine	O
-	O
tuning	O
stage	O
is	O
trained	O
on	O
a	O
single	O
GPU	O
(	O
NVIDIA	O
RTX	O
3090	O
)	O
.	O
Details	O
of	O
hyperparameters	O
are	O
described	O
in	O
Appendix	O
A.	O
Note	O
that	O
for	O
the	O
pretraining	O
step	O
,	O
we	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8	O
and	O
accumulate	O
the	O
gradient	O
for	O
every	O
32	O
steps	O
to	O
be	O
consistent	O
with	O
the	O
hyperparameter	O
setting	O
in	O
Gururangan	O
et	O
al	O
,	O
2020	O
.	O
We	O
perform	O
pretraining	O
with	O
the	O
self	O
-	O
supervised	O
objectives	O
,	O
which	O
are	O
randomly	O
masked	O
tokens	O
,	O
with	O
a	O
probability	O
of	O
15	O
%	O
for	O
each	O
epoch	O
and	O
we	O
do	O
not	O
apply	O
validation	O
to	O
pretraining	O
and	O
save	O
the	O
model	O
at	O
the	O
end	O
of	O
the	O
training	O
from	O
a	O
single	O
seed	O
.	O
For	O
TAPT	O
,	O
we	O
train	O
the	O
entire	O
parameters	O
of	O
the	O
RoBERTa	B-MethodName
via	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(	O
MLM	B-DatasetName
)	O
on	O
the	O
target	O
dataset	O
,	O
whereas	O
for	O
the	O
adapter	O
-	O
based	O
model	O
,	O
we	O
embed	O
the	O
language	O
adapters	O
in	O
each	O
transformer	O
layer	O
and	O
add	O
invertible	O
adapters	O
after	O
the	O
embedding	O
layers	O
to	O
perform	O
MLM	B-DatasetName
while	O
freezing	O
the	O
original	O
parameters	O
of	O
RoBERTa	B-MethodName
,	O
following	O
Pfeiffer	O
et	O
al	O
,	O
2020c	O
.	O
Fine	O
-	O
tuning	O
step	O
is	O
straightforward	O
.	O
We	O
perform	O
fine	O
-	O
tuning	O
parameters	O
that	O
are	O
pretrained	O
via	O
MLM	B-DatasetName
for	O
both	O
TAPT	O
and	O
the	O
adapter	O
model	O
.	O
Validation	O
is	O
performed	O
after	O
each	O
epoch	O
and	O
the	O
best	O
checkpoint	O
is	O
loaded	O
at	O
the	O
end	O
of	O
the	O
training	O
to	O
evaluate	O
the	O
performance	O
on	O
the	O
test	O
set	O
.	O

Experiments	O
cover	O
four	O
different	O
models	O
.	O
First	O
,	O
we	O
reproduce	O
the	O
performance	O
of	O
RoBERTa	B-MethodName
and	O
TAPT	O
in	O
Gururangan	O
et	O
al	O
,	O
2020	O
as	O
presented	O
in	O
Appendix	O
C.	O
Then	O
we	O
proceed	O
to	O
the	O
adapter	O
-	O
based	O
approach	O
.	O
To	O
investigate	O
the	O
benefits	O
of	O
task	O
-	O
adaptive	O
pretraining	O
with	O
adapters	O
,	O
we	O
compare	O
the	O
performance	O
of	O
the	O
pre	O
-	O
trained	O
adapter	O
model	O
with	O
the	O
model	O
without	O
pretraining	O
,	O
i.e.	O
,	O
directly	O
fine	O
-	O
tuning	O
adapters	O
in	O
RoBERTa	B-MethodName
on	O
the	O
target	O
task	O
.	O
For	O
the	O
adapter	O
-	O
based	O
approach	O
,	O
we	O
compare	O
the	O
adapter	O
-	O
based	O
model	O
with	O
the	O
second	O
phase	O
of	O
pretraining	O
and	O
the	O
model	O
without	O
the	O
pretraining	O
.	O
Since	O
the	O
weights	O
of	O
the	O
adapters	O
are	O
randomly	O
initialized	O
,	O
we	O
empirically	O
found	O
that	O
a	O
larger	O
learning	B-HyperparameterName
rate	I-HyperparameterName
worked	O
well	O
compared	O
to	O
the	O
full	O
fine	O
-	O
tuning	O
experiments	O
.	O
We	O
sweep	O
the	O
learning	O
rates	O
in	O
{	O
2e	O
-	O
5	O
,	O
1e	O
-	O
4	O
,	O
3e	O
-	O
4	O
,	O
6e	O
-	O
4	O
}	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
in	O
{	O
10	O
,	O
20	O
}	O
on	O
the	O
validation	O
set	O
and	O
report	O
the	O
test	O
score	O
that	O
performs	O
the	O
best	O
on	O
the	O
validation	O
set	O
.	O

The	O
results	O
are	O
summarized	O
in	O
Table	O
2	O
.	O
Surprisingly	O
,	O
for	O
the	O
average	O
F	O
1	O
score	O
,	O
the	O
adapter	O
-	O
based	O
model	O
without	O
task	O
-	O
adaptive	O
pretraining	O
performs	O
best	O
,	O
followed	O
by	O
the	O
other	O
adapter	O
with	O
the	O
pretraining	O
model	O
,	O
TAPT	O
,	O
and	O
the	O
baseline	O
RoBERTa	B-MethodName
.	O
Except	O
for	O
Hyperpartisan	O
news	O
,	O
the	O
adapter	O
model	O
without	O
pretraining	O
performs	O
mostly	O
on	O
par	O
with	O
the	O
counterpart	O
adapter	O
model	O
that	O
involves	O
pretraining	O
on	O
target	O
text	O
corpus	O
,	O
suggesting	O
that	O
the	O
benefits	O
of	O
additional	O
task	O
-	O
adaptive	O
pretraining	O
diminish	O
when	O
we	O
use	O
the	O
adapter	O
-	O
based	O
approach	O
.	O
Furthermore	O
,	O
directly	O
fine	O
-	O
tuned	O
adapter	O
model	O
only	O
trains	O
1.42	O
%	O
of	O
the	O
entire	O
parameters	O
which	O
leads	O
to	O
the	O
30	O
%	O
faster	O
-	O
training	O
step	O
than	O
the	O
full	O
model	O
and	O
skips	O
the	O
pretraining	O
stage	O
that	O
typically	O
expensive	O
to	O
train	O
than	O
the	O
fine	O
-	O
tuning	O
,	O
substantially	O
reducing	O
Figure	O
2	O
:	O
F	O
1	O
score	O
as	O
a	O
function	O
of	O
learning	B-HyperparameterName
rate	I-HyperparameterName
on	O
test	O
set	O
with	O
log	O
scale	O
on	O
x	O
-	O
axis	O
.	O
F	O
1	O
score	O
is	O
averaged	O
over	O
5	O
random	O
seeds	B-DatasetName
for	O
low	O
-	O
resource	O
tasks	O
(	O
CHEMPROT	B-DatasetName
,	O
ACL	B-DatasetName
-	I-DatasetName
ARC	I-DatasetName
,	O
SCIERC	B-DatasetName
,	O
HYPER	O
)	O
due	O
to	O
the	O
high	O
variance	O
.	O
For	O
high	O
-	O
resource	O
tasks	O
(	O
RCT	O
,	O
AGNEWS	O
,	O
HELPFULNESS	O
,	O
IMDB	B-DatasetName
)	O
,	O
we	O
report	O
the	O
F	O
1	O
score	O
from	O
a	O
single	O
random	O
seed	O
for	O
each	O
task	O
.	O
For	O
RoBERTa	B-MethodName
and	O
TAPT	O
,	O
we	O
follow	O
the	O
hyper	O
-	O
parameter	O
settings	O
in	O
Gururangan	O
et	O
al	O
,	O
2020	O
except	O
for	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
.	O
the	O
training	O
time	O
while	O
the	O
relative	O
speed	O
for	O
the	O
inference	O
only	O
decreases	O
by	O
2	O
%	O
to	O
the	O
full	O
model	O
.	O

We	O
analyze	O
how	O
the	O
adapter	O
alone	O
can	O
surpass	O
or	O
perform	O
on	O
par	O
with	O
both	O
the	O
full	O
model	O
and	O
adapter	O
model	O
with	O
task	O
-	O
adaptive	O
pretraining	O
.	O
Since	O
we	O
sweep	O
the	O
learning	O
rates	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
in	O
the	O
range	O
that	O
includes	O
larger	O
figures	O
compared	O
to	O
those	O
in	O
the	O
full	O
model	O
when	O
fine	O
-	O
tuning	O
adapters	O
and	O
kept	O
the	O
other	O
hyper	O
-	O
parameters	O
the	O
same	O
as	O
in	O
Gururangan	O
et	O
al	O
,	O
2020	O
,	O
we	O
the	O
larger	O
learning	B-HyperparameterName
rate	I-HyperparameterName
zeroes	O
out	O
the	O
benefits	O
of	O
pretraining	O
.	O
Figure	O
2	O
.	O
shows	O
the	O
average	O
F	O
1	O
score	O
across	O
all	O
tasks	O
as	O
a	O
function	O
of	O
learning	B-HyperparameterName
rate	I-HyperparameterName
.	O
The	O
adapter	O
model	O
without	O
a	O
second	O
phase	O
of	O
pretraining	O
consistently	O
outperforms	O
or	O
performs	O
on	O
par	O
with	O
the	O
adapter	O
model	O
with	O
pretraining	O
from	O
1e	O
-	O
4	O
to	O
6e	O
-	O
4	O
,	O
demonstrating	O
that	O
the	O
additional	O
pretraining	O
turns	O
out	O
to	O
be	O
ineffective	O
.	O
In	O
contrast	O
,	O
TAPT	O
outperforms	O
baseline	O
RoBERTa	B-MethodName
from	O
2e	O
-	O
5	O
,	O
where	O
both	O
TAPT	O
and	O
baseline	O
RoBERTa	B-MethodName
perform	O
best	O
.	O
The	O
results	O
show	O
that	O
different	O
learning	O
rates	O
used	O
in	O
the	O
fine	O
-	O
tuning	O
stage	O
can	O
affect	O
the	O
effectiveness	O
of	O
pretraining	O
and	O
demonstrate	O
that	O
directly	O
fine	O
-	O
tuning	O
a	O
fraction	O
of	O
parameters	O
can	O
provide	O
comparable	O
performance	O
to	O
the	O
full	O
-	O
model	O
as	O
well	O
as	O
the	O
adapter	O
model	O
with	O
pretraining	O
while	O
substantially	O
reducing	O
the	O
training	O
time	O
.	O
Inspired	B-DatasetName
by	O
the	O
results	O
of	O
the	O
adapter	O
models	O
,	O
we	O
perform	O
the	O
same	O
experiments	O
for	O
the	O
full	O
model	O
(	O
baseline	O
RoBERTa	B-MethodName
and	O
TAPT	O
)	O
on	O
our	O
implementation	O
by	O
sweeping	O
the	O
learning	O
rates	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
.	O
We	O
hypothesize	O
that	O
proper	O
hyperparameter	O
settings	O
such	O
as	O
a	O
larger	O
learning	B-HyperparameterName
rate	I-HyperparameterName
or	O
increasing	O
the	O
number	O
of	O
training	O
steps	O
in	O
the	O
fine	O
-	O
tuning	O
stage	O
can	O
improve	O
the	O
performance	O
of	O
baseline	O
RoBERTa	B-MethodName
,	O
making	O
pretraining	O
on	O
the	O
unlabeled	O
target	O
task	O
less	O
effective	O
.	O
We	O
sweep	O
the	O
learning	O
rates	O
in	O
{	O
1e	O
-	O
5	O
,	O
2e	O
-	O
5	O
,	O
3e	O
-	O
5	O
}	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
in	O
{	O
10	O
,	O
20	O
}	O
on	O
the	O
validation	O
set	O
and	O
report	O
the	O
test	O
score	O
that	O
performs	O
the	O
best	O
on	O
the	O
validation	O
set	O
.	O
Table	O
3	O
shows	O
the	O
best	O
performance	O
of	O
the	O
full	O
models	O
for	O
each	O
task	O
among	O
different	O
hyper	O
-	O
parameter	O
settings	O
.	O
The	O
average	O
F	O
1	O
score	O
of	O
baseline	O
RoBERTa	B-MethodName
greatly	O
increases	O
and	O
surprisingly	O
,	O
it	O
surpasses	O
the	O
performance	O
of	O
TAPT	O
in	O
some	O
tasks	O
.	O
The	O
results	O
ensure	O
that	O
although	O
pretraining	O
PLMs	O
on	O
the	O
target	O
task	O
results	O
in	O
better	O
performance	O
,	O
one	O
can	O
achieve	O
comparable	O
performance	O
by	O
simply	O
using	O
a	O
larger	O
learning	B-HyperparameterName
rate	I-HyperparameterName
or	O
increasing	O
training	O
steps	O
in	O
the	O
fine	O
-	O
tuning	O
stage	O
while	O
skipping	O
the	O
pretraining	O
step	O
that	O
is	O
computationally	O
demanding	O
compared	O
to	O
the	O
fine	O
-	O
tuning	O
.	O

We	O
provide	O
replication	O
results	O
of	O
Gururangan	O
et	O
al	O
,	O
2020	O
in	O
Table	O
9	O
.	O
Table	O
7	O
:	O
Validation	O
performance	O
of	O
adapter	O
experiments	O
.	O
Each	O
score	O
is	O
averaged	O
over	O
5	O
random	O
seeds	B-DatasetName
.	O
Evaluation	O
metric	O
is	O
macro	O
-	O
F	O
1	O
scores	O
for	O
each	O
task	O
except	O
for	O
CHMEPROT	O
and	O
RCT	O
which	O
use	O
micro	O
-	O
F	O
1	O
.	O
Figure	O
3	O
:	O
F	O
1	O
score	O
as	O
a	O
function	O
of	O
learning	B-HyperparameterName
rate	I-HyperparameterName
on	O
development	O
setwith	O
log	O
scale	O
on	O
x	O
-	O
axis	O
.	O
F	O
1	O
score	O
is	O
averaged	O
over	O
5	O
random	O
seeds	B-DatasetName
for	O
low	O
-	O
resource	O
tasks	O
(	O
CHEMPROT	B-DatasetName
,	O
ACL	B-DatasetName
-	I-DatasetName
ARC	I-DatasetName
,	O
SCIERC	B-DatasetName
,	O
HYPER	O
)	O
due	O
to	O
the	O
high	O
variance	O
.	O
For	O
high	O
-	O
resource	O
tasks	O
(	O
RCT	O
,	O
AGNEWS	O
,	O
HELPFULNESS	O
,	O
IMDB	B-DatasetName
)	O
,	O
we	O
report	O
the	O
F	O
1	O
score	O
from	O
a	O
single	O
random	O
seed	O
for	O
each	O
task	O
.	O
Here	O
we	O
sweep	O
the	O
learning	O
rates	O
in	O
{	O
1e	O
-	O
4	O
,	O
3e	O
-	O
4	O
,	O
6e	O
-	O
4	O
}	O
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
in	O
{	O
10	O
,	O
20	O
}	O
,	O
and	O
the	O
patience	O
factor	O
in	O
{	O
3	O
,	O
5	O
}	O
.	O

The	O
JANES	O
system	O
(	O
the	O
name	O
of	O
the	O
system	O
comes	O
from	O
the	O
Slovene	O
national	O
project	O
JANES	O
inside	O
which	O
the	O
system	O
was	O
developed	O
1	O
)	O
is	O
based	O
on	O
conditional	O
random	O
fields	O
(	O
CRFs	O
)	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
,	O
exploiting	O
the	O
following	O
handcrafted	O
features	O
:	O
lowercased	O
focus	O
token	O
(	O
token	O
for	O
which	O
features	O
are	O
being	O
extracted	O
)	O
lowercased	O
tokens	O
in	O
a	O
window	O
of	O
{	O
−3	O
,	O
−2	O
,	O
−1	O
,	O
1	O
,	O
2	O
,	O
3	O
}	O
form	O
the	O
focus	O
token	O
focus	O
token	O
suffixes	O
of	O
length	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
}	O
features	O
encoding	O
whether	O
the	O
focus	O
token	O
starts	O
with	O
http	O
(	O
link	O
)	O
,	O
#	O
(	O
hashtag	O
)	O
or	O
@	O
(	O
mention	O
)	O
Brown	O
cluster	O
binary	O
paths	O
for	O
the	O
focus	O
token	O
,	O
with	O
the	O
path	O
length	O
of	O
{	O
2	O
,	O
4	O
,	O
6	O
,	O
8	O
}	O
These	O
features	O
were	O
proven	O
to	O
yield	O
optimal	O
results	O
in	O
our	O
previous	O
work	O
on	O
tagging	O
non	O
-	O
standard	O
Slovene	O
(	O
Ljubešić	O
et	O
al	O
,	O
2017a	O
)	O
.	O
The	O
Brown	O
clusters	O
,	O
the	O
output	O
of	O
a	O
method	O
for	O
context	O
-	O
dependent	O
hierarchical	O
word	O
clustering	O
(	O
Brown	O
et	O
al	O
,	O
1992	O
)	O
,	O
were	O
calculated	O
from	O
the	O
web	O
data	O
that	O
were	O
made	O
available	O
through	O
the	O
shared	O
task	O
,	O
namely	O
the	O
slWaC	O
web	O
corpus	O
of	O
Slovene	O
and	O
the	O
hrWaC	O
and	O
srWaC	O
corpora	O
of	O
Croatian	O
and	O
Serbian	O
(	O
Ljubešić	O
and	O
Klubička	O
,	O
2014	O
)	O
.	O
We	O
have	O
used	O
default	O
parameters	O
for	O
calculating	O
Brown	O
clusters	O
,	O
except	O
for	O
the	O
minimum	O
occurrence	O
parameter	O
which	O
was	O
set	O
to	O
5	O
.	O
The	O
web	O
text	O
was	O
previously	O
lowercased	O
and	O
punctuations	O
and	O
newlines	O
were	O
removed	O
from	O
it	O
.	O
For	O
training	O
the	O
tagger	O
,	O
we	O
exploited	O
(	O
1	O
)	O
the	O
proximity	O
of	O
the	O
Croatian	O
and	O
Serbian	O
language	O
,	O
and	O
(	O
2	O
)	O
the	O
fact	O
that	O
we	O
have	O
much	O
more	O
standard	O
training	O
data	O
and	O
much	O
less	O
Twitter	O
training	O
data	O
.	O
We	O
sampled	O
our	O
final	O
training	O
data	O
for	O
each	O
language	O
in	O
the	O
following	O
manner	O
:	O
for	O
Slovene	O
:	O
we	O
added	O
to	O
the	O
Slovene	O
standard	O
training	O
data	O
ten	O
times	O
the	O
available	O
non	O
-	O
standard	O
data	O
,	O
thereby	O
reaching	O
a	O
similar	O
amount	O
of	O
standard	O
and	O
non	O
-	O
standard	O
data	O
in	O
our	O
training	O
set	O
;	O
from	O
previous	O
work	O
we	O
know	O
that	O
for	O
CRFs	O
oversampling	O
in	O
-	O
domain	O
data	O
is	O
the	O
simplest	O
and	O
most	O
effective	O
method	O
in	O
merging	O
out	O
-	O
domain	O
and	O
in	O
-	O
domain	O
training	O
data	O
(	O
Horsmann	O
and	O
Zesch	O
,	O
2015	O
;	O
Ljubešić	O
et	O
al	O
,	O
2017a	O
)	O
for	O
Croatian	O
:	O
we	O
merged	O
the	O
Croatian	O
and	O
the	O
Serbian	O
standard	O
language	O
training	O
datasets	O
,	O
added	O
to	O
it	O
ten	O
copies	O
of	O
the	O
Croatian	O
Twitter	O
training	O
dataset	O
and	O
two	O
copies	O
of	O
the	O
Serbian	O
training	O
dataset	O
,	O
thereby	O
putting	O
emphasis	O
on	O
the	O
Croatian	O
training	O
data	O
,	O
which	O
is	O
expected	O
to	O
be	O
closer	O
to	O
the	O
Croatian	O
test	O
data	O
for	O
Serbian	O
:	O
we	O
merged	O
the	O
Croatian	O
standard	O
training	O
data	O
,	O
two	O
copies	O
of	O
the	O
Serbian	O
standard	O
training	O
data	O
(	O
as	O
these	O
are	O
more	O
than	O
five	O
times	O
smaller	O
than	O
the	O
Croatian	O
ones	O
)	O
,	O
ten	O
copies	O
of	O
nonstandard	O
Croatian	O
training	O
data	O
,	O
and	O
four	O
copies	O
of	O
non	O
-	O
standard	O
Serbian	O
training	O
data	O
,	O
with	O
the	O
rationale	O
that	O
most	O
non	O
-	O
standard	O
elements	O
in	O
Croatian	O
are	O
present	O
in	O
non	O
-	O
standard	O
Serbian	O
as	O
well	O
,	O
but	O
with	O
lower	O
frequency	O
;	O
by	O
oversampling	O
non	O
-	O
standard	O
Croatian	O
in	O
the	O
Serbian	O
dataset	O
we	O
emphasize	O
the	O
non	O
-	O
standard	O
elements	O
in	O
the	O
Croatian	O
non	O
-	O
standard	O
training	O
data	O
as	O
the	O
Serbian	O
non	O
-	O
standard	O
data	O
is	O
much	O
closer	O
to	O
the	O
standard	O
language	O
(	O
Miličević	O
and	O
Ljubešić	O
,	O
2016	O
)	O
The	O
system	O
was	O
implemented	O
in	O
CRFSuite	O
(	O
Okazaki	O
,	O
2007	O
)	O
,	O
using	O
the	O
passive	O
aggressive	O
optimizer	B-HyperparameterName
and	O
10	O
epochs	O
,	O
a	O
setting	O
which	O
proved	O
to	O
yield	O
best	O
results	O
in	O
previous	O
experiments	O
(	O
Ljubešić	O
and	O
Erjavec	O
,	O
2016	O
)	O
.	O

The	O
first	O
group	O
of	O
results	O
considers	O
different	O
ways	O
of	O
pretraining	O
word	B-TaskName
embeddings	I-TaskName
.	O
The	O
word	B-TaskName
embeddings	I-TaskName
were	O
always	O
pretrained	O
on	O
the	O
web	O
data	O
available	O
for	O
each	O
language	O
.	O
We	O
considered	O
only	O
two	O
tools	O
for	O
pretraining	O
word	B-TaskName
embeddings	I-TaskName
:	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
and	O
fasttext	B-MethodName
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
two	O
architectures	O
,	O
CBOW	O
and	O
Skipgram	O
.	O
The	O
results	O
(	O
word2vec	O
cbow	O
vs.	O
word2vec	O
skipgram	O
)	O
show	O
for	O
Skipgram	O
to	O
be	O
significantly	O
better	O
suited	O
for	O
this	O
task	O
,	O
which	O
is	O
in	O
line	O
with	O
previous	O
results	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2017	O
)	O
.	O
Comparing	O
word2vec	O
and	O
fasttext	B-MethodName
(	O
word2vec	O
skipgram	O
vs.	O
fasttext	B-MethodName
skipgram	O
)	O
,	O
fasttext	B-MethodName
shows	O
a	O
slightly	O
better	O
performance	O
,	O
but	O
the	O
difference	O
gets	O
more	O
obvious	O
(	O
almost	O
half	O
a	O
point	O
in	O
token	O
accuracy	B-MetricName
)	O
once	O
fasttext	B-MethodName
is	O
used	O
to	O
generate	O
representations	O
for	O
the	O
words	O
not	O
present	O
in	O
the	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
(	O
fasttext	B-MethodName
skipgram	O
generated	O
)	O
.	O
3	O
setup	O
token	O
accuracy	B-MetricName
with	O
stdev	O
word2vec	O
cbow	O
0.8407	O
±	O
0.0025	O
word2vec	O
skipgram	O
0.8550	O
±	O
0.0041	O
fasttext	B-MethodName
skipgram	O
0.8578	O
±	O
0.0041	O
fasttext	B-MethodName
skipgram	O
generated	O
0.8596	O
±	O
0.0031	O
added	O
character	O
-	O
level	O
encoding	O
0.8780	O
±	O
0.0030	O
added	O
bidirectional	O
encoding	O
0.8790	O
±	O
0.0032	O
additionally	O
tuned	O
on	O
in	O
-	O
domain	O
data	O
0.8836	O
±	O
0.0026	O
pretrained	O
character	O
-	O
level	O
encoder	O
on	O
web	O
data	O
0.8855	O
±	O
0.0015	O
Table	O
2	O
:	O
Initial	O
experiments	O
on	O
the	O
JSI	O
system	O
,	O
performed	O
on	O
the	O
Slovene	O
dataset	O
.	O
The	O
standard	O
deviation	O
is	O
calculated	O
from	O
ten	O
evaluations	O
performed	O
during	O
the	O
last	O
epoch	O
.	O

The	O
second	O
group	O
of	O
experiments	O
considers	O
the	O
impact	O
of	O
adding	O
character	O
-	O
level	O
representations	O
of	O
each	O
token	O
to	O
the	O
word	O
representation	O
via	O
a	O
dedicated	O
character	O
-	O
level	O
BiLSTM	B-MethodName
.	O
Adding	O
the	O
character	O
-	O
level	O
representation	O
has	O
shown	O
the	O
biggest	O
impact	O
among	O
all	O
the	O
experiments	O
,	O
with	O
∼	O
2	O
accuracy	B-MetricName
points	O
increase	O
,	O
and	O
a	O
minor	O
difference	O
between	O
encoding	O
the	O
character	O
sequence	O
with	O
a	O
single	O
-	O
direction	O
or	O
a	O
bi	O
-	O
directional	O
LSTM	B-MethodName
.	O

Finally	O
,	O
in	O
the	O
last	O
set	O
of	O
experiments	O
we	O
investigated	O
whether	O
there	O
is	O
positive	O
impact	O
if	O
the	O
characterlevel	O
encoder	O
was	O
pretrained	O
on	O
a	O
inflectional	O
-	O
lexicon	O
-	O
like	O
resource	O
.	O
In	O
this	O
shared	O
task	O
the	O
web	O
data	O
were	O
automatically	O
tagged	O
with	O
a	O
CRF	B-MethodName
tagger	O
relying	O
on	O
a	O
lexicon	O
Ljubešić	O
and	O
Erjavec	O
,	O
2016	O
)	O
,	O
therefore	O
we	O
transformed	O
the	O
automatically	O
-	O
tagged	O
web	O
data	O
into	O
a	O
lexicon	O
by	O
(	O
1	O
)	O
picking	O
only	O
token	O
-	O
tag	O
pairs	O
occurring	O
at	O
least	O
100	O
times	O
in	O
the	O
web	O
data	O
and	O
(	O
2	O
)	O
selecting	O
only	O
the	O
most	O
frequent	O
token	O
-	O
tag	O
pair	O
per	O
token	O
.	O
With	O
the	O
second	O
criterion	O
we	O
lost	O
some	O
information	O
on	O
homonymous	O
words	O
,	O
but	O
also	O
got	O
rid	O
of	O
a	O
lot	O
of	O
wrong	O
automatic	O
annotations	O
of	O
frequent	O
words	O
.	O
The	O
results	O
on	O
pretraining	O
the	O
character	O
-	O
level	O
encoder	O
show	O
that	O
the	O
improvement	O
lies	O
below	O
half	O
an	O
accuracy	B-MetricName
point	O
,	O
but	O
this	O
improvement	O
showed	O
to	O
be	O
consistent	O
across	O
all	O
the	O
three	O
languages	O
.	O
4	O

In	O
this	O
section	O
we	O
report	O
the	O
results	O
of	O
the	O
final	O
setups	O
of	O
the	O
JANES	O
and	O
the	O
JSI	O
system	O
and	O
compare	O
it	O
to	O
the	O
HunPos	O
baseline	O
(	O
Halácsy	O
et	O
al	O
,	O
2007	O
)	O
defined	O
by	O
the	O
shared	O
task	O
organizers	O
.	O
Additionally	O
,	O
we	O
report	O
the	O
results	O
of	O
the	O
JANES	O
system	O
using	O
an	O
inflectional	O
lexicon	O
for	O
the	O
specific	O
language	O
,	O
namely	O
Sloleks	O
for	O
Slovene	O
(	O
Dobrovoljc	O
et	O
al	O
,	O
2015	O
)	O
,	O
hrLex	O
for	O
Croatian	O
(	O
Ljubešić	O
et	O
al	O
,	O
2016a	O
)	O
and	O
srLex	O
for	O
Serbian	O
(	O
Ljubešić	O
et	O
al	O
,	O
2016b	O
)	O
.	O
We	O
call	O
this	O
system	O
JANES	O
-	O
lex	O
.	O
We	O
compare	O
to	O
this	O
system	O
as	O
it	O
is	O
very	O
straightforward	O
to	O
add	O
information	O
from	O
an	O
inflectional	O
lexicon	O
as	O
additional	O
features	O
to	O
a	O
CRF	B-MethodName
-	O
based	O
system	O
.	O
Table	O
3	O
:	O
Results	O
of	O
the	O
two	O
systems	O
,	O
their	O
two	O
adaptations	O
and	O
the	O
baseline	O
on	O
the	O
test	O
data	O
.	O
Reported	O
metric	O
is	O
token	O
-	O
level	O
accuracy	B-MetricName
.	O
We	O
also	O
report	O
a	O
modification	O
of	O
the	O
JSI	O
system	O
that	O
we	O
implemented	O
after	O
the	O
shared	O
task	O
was	O
already	O
concluded	O
.	O
Namely	O
,	O
we	O
removed	O
the	O
fully	O
connected	O
layer	O
between	O
the	O
main	O
BiLSTM	B-MethodName
and	O
the	O
softmax	B-MethodName
layer	O
,	O
which	O
is	O
actually	O
the	O
most	O
frequent	O
setup	O
for	O
sequence	O
labeling	O
.	O
The	O
removed	O
layer	O
in	O
the	O
JSI	O
system	O
is	O
a	O
residue	O
from	O
the	O
tagger	O
we	O
based	O
our	O
implementation	O
on	O
5	O
.	O
We	O
call	O
the	O
simplified	O
tagger	O
JSI	O
-	O
simpler	O
.	O
The	O
results	O
of	O
the	O
two	O
taggers	O
and	O
the	O
two	O
variants	O
are	O
given	O
in	O
Table	O
3	O
.	O
The	O
reported	O
results	O
are	O
those	O
obtained	O
on	O
the	O
test	O
data	O
.	O
We	O
can	O
first	O
observe	O
that	O
(	O
1	O
)	O
all	O
the	O
systems	O
outperform	O
the	O
HunPos	O
baseline	O
by	O
a	O
wide	O
margin	O
and	O
that	O
(	O
2	O
)	O
the	O
results	O
of	O
the	O
four	O
remaining	O
systems	O
are	O
rather	O
close	O
.	O
The	O
largest	O
difference	O
that	O
can	O
be	O
observed	O
between	O
the	O
four	O
systems	O
are	O
2	O
accuracy	B-MetricName
points	O
on	O
Slovene	O
between	O
the	O
basic	O
CRF	B-MethodName
implementation	O
(	O
JANES	O
)	O
and	O
the	O
simplified	O
BiLSTM	B-MethodName
implementation	O
(	O
JSIsimpler	O
)	O
.	O
The	O
same	O
difference	O
is	O
not	O
to	O
be	O
observed	O
on	O
the	O
other	O
two	O
languages	O
,	O
with	O
the	O
same	O
systems	O
having	O
a	O
difference	O
of	O
0.5	O
points	O
on	O
Croatian	O
and	O
0.3	O
points	O
on	O
Serbian	O
.	O
The	O
reason	O
for	O
the	O
larger	O
difference	O
on	O
Slovene	O
data	O
lies	O
in	O
the	O
fact	O
that	O
the	O
Slovene	O
data	O
is	O
least	O
standard	O
(	O
17	O
%	O
tokens	O
being	O
nonstandard	O
)	O
,	O
followed	O
by	O
Croatian	O
(	O
13	O
%	O
non	O
-	O
standard	O
tokens	O
)	O
,	O
with	O
Serbian	O
data	O
deviating	O
the	O
least	O
from	O
the	O
norm	O
(	O
10	O
%	O
non	O
-	O
standard	O
tokens	O
)	O
(	O
Miličević	O
et	O
al	O
,	O
2017	O
)	O
as	O
more	O
complex	O
modeling	O
techniques	O
pay	O
off	O
more	O
as	O
the	O
language	O
deviates	O
stronger	O
from	O
the	O
norm	O
.	O
Adding	O
lexicon	O
information	O
to	O
the	O
JANES	O
system	O
(	O
JANES	O
vs.	O
JANES	O
-	O
lex	O
)	O
improves	O
the	O
results	O
on	O
all	O
three	O
languages	O
,	O
but	O
just	O
slightly	O
,	O
between	O
0.1	O
%	O
and	O
0.6	O
%	O
.	O
Previous	O
work	O
on	O
the	O
problem	O
(	O
Ljubešić	O
et	O
al	O
,	O
2017a	O
)	O
has	O
shown	O
that	O
Brown	O
clusters	O
already	O
provide	O
to	O
a	O
large	O
extent	O
the	O
information	O
that	O
was	O
traditionally	O
obtained	O
through	O
inflectional	O
lexicons	O
.	O
Comparing	O
the	O
JANES	O
and	O
JSI	O
results	O
by	O
using	O
the	O
McNemar	O
's	O
statistical	O
test	O
(	O
McNemar	O
,	O
1947	O
)	O
,	O
the	O
difference	O
on	O
Slovene	O
is	O
statistically	O
significant	O
at	O
the	O
p	O
<	O
0.001	O
level	O
,	O
with	O
an	O
absolute	O
difference	O
in	O
1.2	O
points	O
and	O
an	O
relative	O
error	O
reduction	O
of	O
9.3	O
%	O
.	O
The	O
differences	O
on	O
the	O
remaining	O
two	O
languages	O
are	O
not	O
statistically	O
significant	O
.	O
When	O
comparing	O
the	O
JSI	O
and	O
JSI	O
-	O
simpler	O
results	O
,	O
it	O
becomes	O
obvious	O
that	O
the	O
additional	O
layer	O
in	O
the	O
JSI	O
system	O
actually	O
deteriorates	O
the	O
results	O
.	O
On	O
all	O
the	O
three	O
languages	O
,	O
the	O
differences	O
are	O
statistically	O
significant	O
,	O
on	O
Slovene	O
and	O
Croatian	O
on	O
the	O
p	O
<	O
0.001	O
level	O
,	O
while	O
on	O
Serbian	O
it	O
is	O
on	O
the	O
p	O
<	O
0.05	O
level	O
.	O
The	O
level	O
of	O
significance	O
of	O
difference	O
between	O
the	O
JANES	O
and	O
JSI	O
-	O
simpler	O
systems	O
is	O
identical	O
to	O
that	O
of	O
between	O
JSI	O
and	O
JSI	O
-	O
simpler	O
.	O
The	O
most	O
interesting	O
observation	O
from	O
the	O
final	O
evaluation	O
of	O
the	O
submitted	O
and	O
modified	O
systems	O
is	O
that	O
the	O
difference	O
between	O
the	O
traditional	O
CRFs	O
and	O
the	O
(	O
probably	O
over	O
-	O
hyped	O
?	O
)	O
BiLSTMs	O
is	O
actually	O
quite	O
small	O
,	O
with	O
relative	O
error	O
reductions	O
being	O
15	O
%	O
on	O
Slovene	O
,	O
5	O
%	O
on	O
Croatian	O
and	O
only	O
3	O
%	O
on	O
Serbian	O
.	O
These	O
results	O
,	O
as	O
well	O
as	O
some	O
preliminary	O
results	O
on	O
standard	O
test	O
sets	O
,	O
suggest	O
that	O
there	O
would	O
be	O
no	O
significant	O
difference	O
in	O
the	O
results	O
between	O
CRFs	O
and	O
BiLSTMs	O
on	O
standard	O
training	O
and	O
test	O
data	O
.	O

In	O
this	O
paper	O
we	O
have	O
compared	O
two	O
popular	O
sequence	O
labeling	O
techniques	O
:	O
conditional	O
random	O
fields	O
(	O
CRFs	O
)	O
and	O
bidirectional	O
long	O
short	O
-	O
term	O
memories	O
(	O
BiLSTMs	O
)	O
on	O
the	O
task	O
of	O
morphosyntactic	O
annotation	O
of	O
tweets	O
written	O
in	O
three	O
closely	O
related	O
South	O
Slavic	O
languages	O
:	O
Slovene	O
,	O
Croatian	O
and	O
Serbian	O
.	O
We	O
have	O
shown	O
that	O
CRFs	O
with	O
well	O
defined	O
features	O
come	O
very	O
close	O
to	O
the	O
performance	O
of	O
the	O
stronger	O
BiLSTM	B-MethodName
models	O
,	O
the	O
difference	O
between	O
those	O
two	O
being	O
bigger	O
as	O
the	O
data	O
are	O
more	O
nonstandard	O
.	O
The	O
relative	O
error	O
reduction	O
between	O
those	O
two	O
systems	O
lies	O
between	O
15	O
%	O
for	O
Slovene	O
,	O
for	O
which	O
the	O
Twitter	O
variety	O
deviates	O
the	O
most	O
from	O
the	O
standard	O
,	O
and	O
3	O
%	O
for	O
Serbian	O
,	O
for	O
which	O
the	O
Twitter	O
variety	O
deviates	O
the	O
least	O
.	O
For	O
the	O
CRF	B-MethodName
system	O
,	O
we	O
have	O
shown	O
that	O
using	O
contextual	O
,	O
suffixal	O
and	O
distributional	O
features	O
gives	O
very	O
good	O
results	O
.	O
The	O
latter	O
make	O
an	O
inflectional	O
lexicon	O
mostly	O
obsolete	O
,	O
with	O
just	O
minor	O
improvements	O
in	O
accuracy	B-MetricName
if	O
features	O
from	O
large	O
inflectional	O
lexicons	O
are	O
added	O
.	O
For	O
the	O
BiLSTM	B-MethodName
system	O
,	O
we	O
have	O
shown	O
that	O
encoding	O
a	O
character	O
-	O
level	O
representation	O
of	O
a	O
word	O
is	O
the	O
single	O
most	O
useful	O
intervention	O
,	O
with	O
minor	O
improvements	O
obtained	O
through	O
proper	O
word	O
embedding	O
pretraining	O
,	O
fine	O
-	O
tuning	O
on	O
in	O
-	O
domain	O
data	O
and	O
pretraining	O
the	O
character	O
-	O
level	O
encoder	O
on	O
pairs	O
of	O
words	O
and	O
MSD	B-DatasetName
tags	O
from	O
a	O
large	O
automatically	O
tagged	O
web	O
corpus	O
.	O
With	O
an	O
error	O
analysis	O
we	O
have	O
shown	O
that	O
the	O
types	O
of	O
error	O
performed	O
by	O
each	O
of	O
the	O
systems	O
are	O
actually	O
very	O
similar	O
,	O
most	O
of	O
them	O
still	O
being	O
typical	O
tagger	O
errors	O
for	O
languages	O
with	O
a	O
rich	O
inflectional	O
morphology	O
.	O
However	O
,	O
there	O
is	O
evidence	O
that	O
BiLSTMs	O
resolve	O
long	O
-	O
range	O
dependencies	O
much	O
better	O
,	O
such	O
as	O
discriminating	O
between	O
masculinum	O
nouns	O
in	O
nominative	O
and	O
accusative	O
singular	O
,	O
but	O
yielding	O
slightly	O
more	O
mistakes	O
in	O
the	O
close	O
-	O
range	O
dependencies	O
such	O
as	O
the	O
case	O
of	O
prepositions	O
.	O

Alignment	O
Methods	O
.	O
Given	O
a	O
bitext	O
B	O
src	O
=	O
(	O
s	O
1	O
,	O
...	O
,	O
s	O
j	O
,	O
...	O
,	O
s	O
N	O
)	O
and	O
B	O
trg	O
=	O
(	O
t	O
1	O
,	O
...	O
,	O
t	O
i	O
,	O
...	O
,	O
t	O
M	O
)	O
where	O
B	O
src	O
is	O
a	O
sentence	O
in	O
the	O
source	O
language	O
and	O
B	O
trg	O
is	O
its	O
translation	O
in	O
the	O
target	O
language	O
,	O
an	O
alignment	O
A	O
is	O
a	O
mapping	O
of	O
words	O
between	O
B	O
src	O
and	O
B	O
trg	O
(	O
Tiedemann	O
,	O
2011	O
)	O
,	O
formally	O
defined	O
as	O
A	O
⊆	O
{	O
(	O
j	O
,	O
i	O
)	O
:	O
j	O
=	O
1	O
,	O
...	O
,	O
N	O
;	O
i	O
=	O
1	O
,	O
...	O
,	O
M	O
}	O
(	O
1	O
)	O
We	O
study	O
three	O
different	O
settings	O
:	O
(	O
a	O
)	O
standard	O
word	B-TaskName
alignment	I-TaskName
between	O
corresponding	O
words	O
,	O
(	O
b	O
)	O
alignments	O
between	O
all	O
target	O
words	O
and	O
the	O
language	O
label	O
in	O
the	O
input	O
string	O
,	O
and	O
(	O
c	O
)	O
the	O
union	O
between	O
the	O
former	O
two	O
.	O
Figure	O
1	O
shows	O
an	O
example	O
of	O
those	O
approaches	O
.	O
To	O
produce	O
word	O
alignments	O
between	O
parallel	O
sentences	O
,	O
i.e.	O
,	O
Figure	O
1	O
(	O
a	O
)	O
,	O
we	O
use	O
the	O
awesome	O
-	O
align	O
tool	O
(	O
Dou	O
and	O
Neubig	O
,	O
2021	O
)	O
,	O
a	O
recent	O
work	O
that	O
leverages	O
multilingual	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
to	O
extract	O
the	O
links	O
.	O
1	O
Models	O
.	O
To	O
train	O
Many	O
-	O
to	O
-	O
Many	O
MNMT	O
models	O
,	O
we	O
use	O
a	O
6	O
-	O
layer	O
Transformer	B-MethodName
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
,	O
prepending	O
a	O
language	O
label	O
in	O
the	O
input	O
to	O
indicate	O
the	O
target	O
language	O
(	O
Johnson	O
et	O
al	O
,	O
2017	O
)	O
.	O
Following	O
Garg	O
et	O
al	O
(	O
2019	O
)	O
,	O
given	O
an	O
alignment	O
matrix	O
AM	O
M	O
,	O
N	O
and	O
an	O
attention	O
matrix	O
computed	O
by	O
a	O
cross	O
attention	O
head	O
AH	O
M	O
,	O
N	O
,	O
for	O
each	O
target	O
word	O
i	O
,	O
we	O
use	O
the	O
following	O
cross	O
-	O
entropy	O
loss	B-MetricName
L	O
a	O
to	O
minimize	O
the	O
Kullback	O
-	O
Leibler	O
divergence	O
between	O
AH	O
and	O
AM	O
:	O
L	O
a	O
(	O
AH	O
,	O
AM	O
)	O
=	O
−	O
1	O
M	O
M	O
i=1	O
N	O
j=1	O
AM	O
i	O
,	O
j	O
log	O
(	O
AH	O
i	O
,	O
j	O
)	O
(	O
2	O
)	O
The	O
overall	O
loss	B-MetricName
L	O
is	O
:	O
L	O
=	O
L	O
t	O
+	O
γL	O
a	O
(	O
AH	O
,	O
AM	O
)	O
(	O
3	O
)	O
where	O
L	O
t	O
is	O
the	O
standard	O
NLL	B-MetricName
translation	O
loss	B-MetricName
,	O
and	O
γ	B-HyperparameterName
is	O
a	O
hyperparameter	O
.	O
We	O
use	O
γ	B-HyperparameterName
=	O
0.05	O
,	O
supervising	O
only	O
one	O
cross	O
attention	O
head	O
at	O
the	O
third	O
last	O
layer	O
.	O
2	O
Given	O
the	O
sparse	O
nature	O
of	O
the	O
alignments	O
,	O
we	O
replace	O
the	O
softmax	B-MethodName
operator	O
in	O
the	O
cross	O
attention	O
head	O
with	O
the	O
α	B-HyperparameterName
-	O
entmax	O
function	O
(	O
Peters	O
et	O
al	O
,	O
2019	O
;	O
Correia	O
et	O
al	O
,	O
2019	O
)	O
.	O
Entmax	O
allows	O
sparse	O
attention	O
weights	O
for	O
any	O
α	B-HyperparameterName
>	O
1	O
.	O
Following	O
Peters	O
et	O
al	O
(	O
2019	O
)	O
,	O
we	O
use	O
α=1.5	O
.	O

We	O
use	O
three	O
highly	O
multilingual	O
MT	O
benchmarks	O
:	O
TED	O
Talks	O
(	O
Qi	O
et	O
al	O
,	O
2018	O
)	O
.	O
An	O
Englishcentric	O
parallel	O
corpus	O
with	O
10	O
M	O
training	O
sentences	O
across	O
116	O
translation	O
directions	O
.	O
Following	O
Aharoni	O
et	O
al	O
(	O
2019	O
)	O
,	O
we	O
evaluate	O
on	O
a	O
total	O
of	O
16	O
language	O
directions	O
,	O
while	O
as	O
zeroshot	O
test	O
we	O
evaluate	O
on	O
4	O
language	O
pairs	O
.	O
WMT	O
-	O
2018	O
(	O
Bojar	O
et	O
al	O
,	O
2018	O
)	O
.	O
3	O
A	O
parallel	O
dataset	O
provided	O
by	O
the	O
WMT	O
-	O
2018	O
shared	O
task	O
on	O
news	O
translation	O
.	O
We	O
use	O
all	O
available	O
language	O
pairs	O
,	O
i.e.	O
14	O
,	O
up	O
to	O
5	O
M	O
training	O
sentences	O
for	O
each	O
language	O
pair	O
.	O
We	O
evaluate	O
the	O
models	O
on	O
the	O
test	O
sets	O
of	O
the	O
shared	O
task	O
,	O
i.e.	O
newstest2018	O
.	O
As	O
there	O
are	O
no	O
zero	O
-	O
shot	O
test	O
sets	O
provided	O
by	O
the	O
competition	O
,	O
we	O
use	O
the	O
test	O
portion	O
from	O
the	O
Tatoeba	B-DatasetName
-	O
challenge	O
(	O
Tiedemann	O
,	O
2020	O
)	O
,	O
4	O
in	O
all	O
possible	O
language	O
pair	O
combinations	O
included	O
in	O
the	O
challenge	O
.	O
OPUS	O
-	O
100	O
)	O
.	O
An	O
Englishcentric	O
multi	O
-	O
domain	O
benchmark	O
,	O
built	O
upon	O
the	O
OPUS	O
parallel	O
text	O
collection	O
(	O
Tiedemann	O
,	O
2012	O
)	O
Aharoni	O
et	O
al	O
(	O
2019	O
)	O
,	O
and	O
2	O
its	O
variant	O
with	O
a	O
1.5	O
-	O
entmax	O
function	O
on	O
the	O
cross	O
attention	O
heads	O
as	O
in	O
Correia	O
et	O
al	O
(	O
2019	O
)	O
.	O
The	O
labels	O
(	O
a	O
)	O
,	O
(	O
b	O
)	O
,	O
(	O
c	O
)	O
denote	O
the	O
use	O
of	O
different	O
alignment	O
supervision	O
(	O
see	O
Section	O
2	O
)	O
.	O
"	O
#	O
Param	O
.	O
"	O
:	O
trainable	O
parameter	O
number	O
.	O
"	O
EN	O
-	O
>	O
X	O
(	O
16	O
)	O
"	O
and	O
"	O
X	O
-	O
>	O
EN	O
(	O
16	O
)	O
"	O
:	O
average	O
BLEU	B-MetricName
scores	O
for	O
English	O
to	O
Non	O
-	O
English	O
languages	O
and	O
for	O
Non	O
-	O
English	O
languages	O
to	O
English	O
on	O
16	O
language	O
pairs	O
respectively	O
.	O
"	O
BLEU	B-MetricName
zero	O
(	O
4	O
)	O
"	O
and	O
"	O
ACC	B-MetricName
zero	O
(	O
4	O
)	O
"	O
:	O
average	O
BLEU	B-MetricName
scores	O
and	O
target	O
language	B-TaskName
identification	I-TaskName
accuracy	B-MetricName
over	O
4	O
zero	O
-	O
shot	O
language	O
directions	O
.	O
We	O
report	O
average	O
BLEU	B-MetricName
and	O
accuracy	B-MetricName
scores	O
,	O
plus	O
the	O
standard	O
deviation	O
over	O
3	O
training	O
runs	O
with	O
different	O
random	O
seeds	B-DatasetName
.	O
language	O
pair	O
.	O
It	O
provides	O
supervised	O
translation	O
test	O
data	O
for	O
188	O
language	O
pairs	O
,	O
and	O
zero	O
-	O
shot	O
evaluation	O
data	O
for	O
30	O
pairs	O
.	O
Following	O
related	O
work	O
(	O
Aharoni	O
et	O
al	O
,	O
2019	O
;	O
,	O
we	O
apply	O
joint	O
Byte	O
-	O
Pair	O
Encoding	O
(	O
BPE	B-MethodName
)	O
segmentation	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
;	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
,	O
with	O
a	O
shared	O
vocabulary	O
size	O
of	O
32	O
K	O
symbols	O
for	O
TED	O
Talks	O
and	O
64	O
K	O
for	O
WMT	O
-	O
2018	O
and	O
OPUS	O
-	O
100	O
.	O
As	O
evaluation	O
measure	O
,	O
we	O
use	O
tokenized	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
to	O
be	O
comparable	O
with	O
Aharoni	O
et	O
al	O
(	O
2019	O
)	O
for	O
the	O
TED	O
Talks	O
benchmark	O
,	O
and	O
SACREBLEU	B-MetricName
5	O
(	O
Post	O
,	O
2018	O
)	O
for	O
WMT	O
-	O
2018	O
and	O
OPUS	O
-	O
100	O
.	O
6	O
As	O
an	O
additional	O
evaluation	O
,	O
we	O
report	O
the	O
target	O
language	B-TaskName
identification	I-TaskName
accuracy	B-MetricName
score	O
for	O
the	O
zeroshot	O
cases	O
,	O
called	O
ACC	B-MetricName
zero	O
.	O
We	O
use	O
fasttext	B-MethodName
as	O
a	O
language	B-TaskName
identification	I-TaskName
tool	O
(	O
Joulin	O
et	O
al	O
,	O
2017	O
)	O
,	O
counting	O
how	O
many	O
times	O
the	O
translation	O
language	O
matches	O
the	O
reference	O
target	O
language	O
.	O
The	O
Transformer	B-MethodName
models	O
follow	O
the	O
base	O
setting	O
of	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
,	O
with	O
three	O
different	O
random	O
seeds	B-DatasetName
in	O
each	O
run	O
.	O
All	O
of	O
them	O
are	O
trained	O
on	O
the	O
Many	O
-	O
to	O
-	O
Many	O
English	O
-	O
centric	O
scenario	O
,	O
i.e.	O
,	O
on	O
the	O
concatenation	O
of	O
the	O
training	O
data	O
having	O
English	O
either	O
as	O
the	O
source	O
or	O
target	O
language	O
.	O
Details	O
about	O
data	O
and	O
model	O
settings	O
in	O
the	O
Appendix	O
.	O
5	O
Signature	O
:	O
BLEU+case.mixed+numrefs.1+smooth.exp+	O
tok	O
.	O
{	O
13a	O
,	O
ja	O
-	O
mecab	O
-	O
0.996	O
-	O
IPA	O
,	O
zh	O
}	O
+	O
version.1.5.0	O
6	O
We	O
report	O
average	O
BLEU	B-MetricName
over	O
all	O
test	O
sets	O
.	O
Scores	O
for	O
each	O
language	O
pair	O
are	O
available	O
in	O
the	O
supplementary	B-DatasetName
material	I-DatasetName
.	O

Throughout	O
this	O
section	O
we	O
refer	O
to	O
our	O
baseline	O
MNMT	O
models	O
by	O
the	O
labels	O
1	O
and	O
2	O
,	O
while	O
3	O
,	O
4	O
,	O
and	O
5	O
mark	O
the	O
models	O
trained	O
with	O
the	O
auxiliary	O
alignment	O
supervision	O
task	O
,	O
(	O
a	O
)	O
,	O
(	O
b	O
)	O
,	O
(	O
c	O
)	O
from	O
Figure	O
1	O
respectively	O
(	O
see	O
Section	O
2	O
)	O
.	O
TED	O
Talks	O
.	O
Table	O
1	O
shows	O
the	O
results	O
on	O
the	O
TED	O
Talks	O
benchmark	O
.	O
Regarding	O
translation	O
quality	O
on	O
the	O
language	O
pairs	O
seen	O
during	O
training	O
(	O
EN	O
X	O
and	O
X	O
EN	O
columns	O
)	O
,	O
average	O
BLEU	B-MetricName
scores	O
from	O
all	O
models	O
end	O
up	O
in	O
the	O
same	O
ballpark	O
.	O
In	O
contrast	O
,	O
zero	O
-	O
shot	O
results	O
vary	O
across	O
the	O
board	O
,	O
with	O
5	O
attaining	O
the	O
best	O
performance	O
,	O
with	O
almost	O
2	O
BLEU	B-MetricName
points	O
better	O
than	O
its	O
baseline	O
2	O
.	O
Moreover	O
,	O
5	O
considerably	O
improves	O
target	O
language	B-TaskName
identification	I-TaskName
accuracy	B-MetricName
(	O
ACC	B-MetricName
zero	O
)	O
,	O
with	O
more	O
stable	O
results	O
,	O
i.e.	O
lower	O
standard	O
deviation	O
,	O
than	O
counterparts	O
.	O
Surprisingly	O
,	O
the	O
addition	O
of	O
alignment	O
supervision	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
as	O
an	O
auxiliary	O
task	O
has	O
an	O
overall	O
detrimental	O
effect	O
on	O
the	O
zero	O
-	O
shot	O
performance	O
,	O
even	O
though	O
model	O
4	O
results	O
in	O
more	O
stable	O
results	O
than	O
2	O
.	O
WMT	O
-	O
2018	O
.	O
Table	O
2	O
reports	O
the	O
results	O
on	O
the	O
WMT	O
-	O
2018	O
benchmark	O
.	O
As	O
expected	O
,	O
in	O
a	O
highresource	O
scenario	O
bilingual	O
baselines	O
are	O
hard	O
to	O
beat	O
.	O
Among	O
multilingual	O
models	O
,	O
the	O
overall	O
performance	O
follows	O
a	O
similar	O
trend	O
as	O
before	O
.	O
Enriching	O
the	O
model	O
with	O
alignment	O
supervision	O
(	O
c	O
)	O
results	O
in	O
the	O
best	O
system	O
overall	O
,	O
with	O
an	O
improvement	O
of	O
more	O
than	O
3	O
BLEU	B-MetricName
points	O
in	O
the	O
zero	O
-	O
shot	O
(	O
2020	O
)	O
.	O
MATT	O
denotes	O
the	O
use	O
of	O
merged	O
attention	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
.	O
LALN	O
and	O
LALT	O
indicate	O
the	O
use	O
of	O
language	O
-	O
aware	O
components	O
.	O
Average	O
BLEU	B-MetricName
,	O
target	O
language	B-TaskName
identification	I-TaskName
accuracy	B-MetricName
and	O
standard	O
deviation	O
of	O
3	O
training	O
runs	O
.	O
testbed	O
compared	O
to	O
baseline	O
2	O
,	O
and	O
with	O
stable	O
results	O
across	O
three	O
training	O
runs	O
(	O
standard	O
deviations	O
of	O
0.12	O
and	O
0.82	O
)	O
.	O

As	O
one	O
can	O
see	O
from	O
Table	O
3	O
,	O
we	O
confirm	O
the	O
positive	O
effect	O
of	O
adding	O
the	O
alignment	O
strategy	O
(	O
c	O
)	O
both	O
as	O
translation	O
quality	O
and	O
as	O
a	O
mechanism	O
to	O
produce	O
stable	O
results	O
even	O
in	O
a	O
highly	O
multilingual	O
setup	O
,	O
i.e.	O
,	O
training	O
on	O
198	O
language	O
directions	O
.	O
The	O
average	O
score	O
over	O
30	O
zeroshot	O
language	O
pairs	O
is	O
low	O
but	O
the	O
individual	O
results	O
range	O
from	O
0.3	O
to	O
17.5	O
BLEU	B-MetricName
showing	O
the	O
potentials	O
of	O
multilingual	O
models	O
in	O
this	O
challenging	O
data	O
set	O
as	O
well	O
.	O
7	O
Even	O
though	O
the	O
results	O
from	O
our	O
best	O
model	O
still	O
lag	O
behind	O
models	O
with	O
languagespecific	O
components	O
,	O
i.e.	O
MATT+LALN+LALT	O
from	O
,	O
we	O
note	O
that	O
our	O
results	O
demonstrate	O
the	O
positive	O
effect	O
of	O
alignment	O
on	O
zero	O
-	O
shot	O
translation	O
.	O
8	O
Overall	O
,	O
our	O
experiments	O
show	O
consistent	O
results	O
across	O
different	O
benchmarks	O
,	O
providing	O
quantitative	O
evidence	O
on	O
the	O
utility	O
of	O
guided	O
alignment	O
in	O
highly	O
multilingual	O
MT	O
scenarios	O
.	O
Supervising	O
a	O
single	O
cross	O
attention	O
head	O
with	O
the	O
alignment	O
method	O
(	O
c	O
)	O
substantially	O
reduces	O
the	O
instability	O
between	O
training	O
runs	O
,	O
mitigating	O
the	O
off	O
-	O
target	O
translation	O
issue	O
in	O
the	O
zero	O
-	O
shot	O
evaluation	O
.	O
Zero	O
-	O
shot	O
improvements	O
,	O
i.e.	O
BLEU	B-MetricName
zero	O
and	O
ACC	B-MetricName
zero	O
,	O
are	O
large	O
in	O
two	O
benchmarks	O
out	O
of	O
three	O
,	O
i.e.	O
Ted	O
Talks	O
and	O
WMT	O
-	O
2018	O
,	O
and	O
with	O
a	O
similar	O
trend	O
in	O
OPUS	O
-	O
100	O
.	O
We	O
also	O
note	O
that	O
performance	O
differences	O
may	O
be	O
related	O
to	O
the	O
different	O
data	O
sizes	O
(	O
see	O
Appendix	O
A	O
)	O
.	O
TED	O
Talks	O
is	O
a	O
rather	O
small	O
and	O
imbalanced	O
multilingual	O
dataset	O
with	O
116	O
language	O
directions	O
with	O
a	O
total	O
of	O
10	O
M	O
training	O
sentences	O
,	O
while	O
WMT	O
-	O
2018	O
and	O
OPUS	O
-	O
100	O
comprise	O
14	O
language	O
pairs	O
for	O
a	O
total	O
of	O
47.8	O
M	O
training	O
sentences	O
,	O
and	O
110	O
M	O
training	O
sentences	O
for	O
198	O
language	O
pairs	O
,	O
respectively	O
.	O
We	O
plan	O
on	O
investigating	O
the	O
impact	O
of	O
the	O
training	O
size	O
and	O
the	O
resulting	O
alignments	O
on	O
the	O
zero	O
-	O
shot	O
test	O
sets	O
further	O
in	O
future	O
work	O
.	O
Limitations	O
Finally	O
,	O
we	O
highlight	O
that	O
we	O
have	O
focused	O
on	O
a	O
quantitative	O
evaluation	O
on	O
Englishcentric	O
MNMT	O
benchmarks	O
only	O
,	O
therefore	O
we	O
lack	O
a	O
comprehensive	O
evaluation	O
on	O
complete	O
MNMT	O
benchmarks	O
including	O
training	O
data	O
without	O
English	O
as	O
source	O
and	O
target	O
language	O
(	O
Freitag	O
and	O
Firat	O
,	O
2020	O
;	O
Rios	O
et	O
al	O
,	O
2020	O
;	O
Tiedemann	O
,	O
2020	O
;	O
Goyal	O
et	O
al	O
,	O
2021	O
)	O
.	O

We	O
use	O
the	O
OpenNMT	O
-	O
py	O
framework	O
(	O
Klein	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
the	O
Transformer	B-MethodName
base	O
model	O
setting	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Specifically	O
,	O
we	O
use	O
6	O
layers	O
for	O
the	O
encoder	O
and	O
the	O
decoder	O
,	O
512	O
as	O
model	O
dimension	O
,	O
and	O
2048	B-DatasetName
as	O
hidden	O
dimension	O
.	O
We	O
applied	O
0.1	O
as	O
dropout	O
for	O
both	O
residual	O
layers	O
and	O
attention	O
weights	O
,	O
using	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
β1	O
=	O
0.9	O
,	O
and	O
β2	O
=	O
0.998	O
,	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
set	O
at	O
3	O
and	O
40	O
K	O
warmup	O
steps	O
as	O
in	O
Aharoni	O
et	O
al	O
(	O
2019	O
)	O
.	O
We	O
train	O
the	O
models	O
with	O
three	O
random	O
seeds	B-DatasetName
each	O
,	O
for	O
200	O
K	O
training	O
steps	O
for	O
the	O
TED	O
Talks	O
and	O
WMT	O
-	O
2018	O
benchmarks	O
,	O
while	O
for	O
500	O
K	O
training	O
steps	O
for	O
the	O
OPUS	O
-	O
100	O
.	O
To	O
speed	O
up	O
training	O
,	O
we	O
use	O
halfprecision	O
,	O
i.e.	O
,	O
FP16	O
.	O

In	O
this	O
paper	O
,	O
we	O
discuss	O
our	O
submission	O
for	O
DialDoc	O
subtask	O
1	O
.	O
The	O
subtask	O
requires	O
systems	O
to	O
extract	O
knowledge	O
from	O
FAQ	O
-	O
type	O
documents	O
vital	O
to	O
reply	O
to	O
a	O
user	O
's	O
query	O
in	O
a	O
conversational	O
setting	O
.	O
We	O
experiment	O
with	O
pretraining	O
a	O
BERT	B-MethodName
-	O
based	O
question	O
-	O
answering	O
model	O
on	O
different	O
QA	O
datasets	O
from	O
MRQA	B-DatasetName
,	O
as	O
well	O
as	O
conversational	O
QA	O
datasets	O
like	O
CoQA	B-DatasetName
and	O
QuAC	B-DatasetName
.	O
Our	O
results	O
show	O
that	O
models	O
pretrained	O
on	O
CoQA	B-DatasetName
and	O
QuAC	B-DatasetName
perform	O
better	O
than	O
their	O
counterparts	O
that	O
are	O
pretrained	O
on	O
MRQA	B-DatasetName
datasets	O
.	O
Our	O
results	O
also	O
indicate	O
that	O
adding	O
more	O
pretraining	O
data	O
does	O
not	O
necessarily	O
result	O
in	O
improved	O
performance	O
.	O
Our	O
final	O
model	O
,	O
which	O
is	O
an	O
ensemble	O
of	O
AlBERT	O
-	O
XL	O
pretrained	O
on	O
CoQA	B-DatasetName
and	O
QuAC	B-DatasetName
independently	O
,	O
with	O
the	O
chosen	O
answer	O
having	O
the	O
highest	O
average	O
probability	O
score	O
,	O
achieves	O
an	O
F1	B-MetricName
-	I-MetricName
Score	I-MetricName
of	O
70.9	O
%	O
on	O
the	O
official	O
test	O
-	O
set	O
.	O

We	O
pass	O
the	O
pre	O
-	O
processed	O
training	O
data	O
through	O
a	O
QA	O
model	O
that	O
leverages	O
a	O
transformer	O
encoder	O
to	O
contextually	O
represent	O
the	O
question	O
(	O
dialogue	O
history	O
)	O
along	O
with	O
the	O
context	O
(	O
document	O
)	O
.	O
Since	O
the	O
grounding	O
document	O
is	O
often	O
longer	O
than	O
the	O
maximum	O
input	O
sequence	O
length	O
for	O
transformers	O
,	O
we	O
follow	O
(	O
Feng	O
et	O
al	O
,	O
2020	O
)	O
and	O
truncate	O
the	O
documents	O
in	O
sliding	O
windows	O
with	O
a	O
stride	O
.	O
The	O
document	O
trunk	O
and	O
the	O
dialogue	O
history	O
are	O
passed	O
through	O
the	O
transformer	O
encoder	O
to	O
create	O
contextual	O
representations	O
for	O
each	O
token	O
in	O
the	O
input	O
.	O
To	O
extract	O
the	O
beginning	O
and	O
the	O
ending	O
positions	O
of	O
the	O
answer	O
span	O
within	O
the	O
document	O
,	O
the	O
encoded	O
embeddings	O
are	O
sent	O
to	O
a	O
linear	B-MethodName
layer	I-MethodName
to	O
output	O
two	O
logits	O
that	O
correspond	O
to	O
the	O
probability	O
of	O
the	O
position	O
being	O
the	O
start	O
and	O
end	O
position	O
of	O
the	O
answer	O
span	O
.	O
The	O
training	O
loss	B-MetricName
is	O
computed	O
using	O
the	O
Cross	O
-	O
Entropy	O
loss	B-MetricName
function	O
.	O
We	O
use	O
the	O
huggingface	O
transformers	O
toolkit	O
in	O
all	O
of	O
our	O
experiments	O
.	O

Firstly	O
,	O
we	O
briefly	O
describe	O
the	O
different	O
datasets	O
used	O
for	O
the	O
continual	B-TaskName
pretraining	I-TaskName
of	O
our	O
transformer	O
-	O
based	O
QA	O
models	O
.	O
MRQA	B-DatasetName
-	O
19	O
Shared	O
task	O
(	O
Fisch	O
et	O
al	O
,	O
2019	O
)	O
CoQA	B-DatasetName
(	O
Reddy	O
et	O
al	O
,	O
2019	O
)	O
.	O
2	O
For	O
both	O
datasets	O
,	O
we	O
filter	O
out	O
samples	O
which	O
do	O
not	O
adhere	O
to	O
SQuADlike	O
extractive	O
QA	O
setup	O
(	O
e.g.	O
yes	O
/	O
no	O
questions	O
)	O
or	O
have	O
a	O
context	B-HyperparameterName
length	I-HyperparameterName
of	O
more	O
than	O
5000	O
characters	O
.	O
Table	O
1	O
presents	O
the	O
size	O
of	O
the	O
different	O
pretraining	O
datasets	O
after	O
the	O
removal	O
of	O
non	O
-	O
extractive	O
QA	O
samples	O
.	O

The	O
shared	O
-	O
task	O
relies	O
on	O
Exact	B-MetricName
Match	I-MetricName
(	O
EM	B-MetricName
)	O
and	O
F1	B-MetricName
metrics	O
to	O
evaluate	O
the	O
systems	O
on	O
subtask	O
1	O
.	O
To	O
compute	O
these	O
scores	O
,	O
we	O
use	O
the	O
metrics	O
for	O
SQuAD	B-DatasetName
from	O
huggingface	O
.	O
3	O

We	O
only	O
submitted	O
our	O
best	O
performing	O
models	O
on	O
the	O
official	O
test	O
set	O
due	O
to	O
a	O
constraint	O
on	O
the	O
number	O
of	O
submissions	O
.	O
Contrary	O
to	O
the	O
trends	O
for	O
testdev	O
phase	O
,	O
albert	O
-	O
xl	O
models	O
trained	O
on	O
conversational	O
QA	O
datasets	O
perform	O
the	O
best	O
.	O
albert	O
-	O
xl	O
+	O
QuAC	B-DatasetName
is	O
the	O
best	O
-	O
performing	O
single	O
model	O
according	O
to	O
the	O
EM	B-MetricName
metric	O
(	O
EM	B-MetricName
=	O
52.60	O
)	O
,	O
whereas	O
albert	O
-	O
xl	O
+	O
CoQA	B-DatasetName
performs	O
the	O
best	O
on	O
F1	B-MetricName
metric	O
(	O
F	O
1	O
=	O
69.48	O
)	O
on	O
the	O
test	O
set	O
.	O

We	O
perform	O
ensembling	O
over	O
the	O
outputs	O
of	O
the	O
model	O
variants	O
to	O
obtain	O
a	O
single	O
unified	O
ranked	O
list	O
.	O
For	O
a	O
given	O
question	O
Q	O
,	O
we	O
produce	O
20	O
candidate	O
spans	O
,	O
along	O
with	O
a	O
corresponding	O
probability	O
score	O
ps	O
.	O
We	O
compute	O
rank	O
-	O
scores	O
rs	O
for	O
the	O
answer	O
-	O
spans	O
at	O
rank	O
r	O
as	O
rs	O
=	O
1	O
log	O
2	O
(	O
r+1	O
)	O
.	O
We	O
then	O
aggregate	O
the	O
information	O
of	O
the	O
answer	O
spans	O
for	O
the	O
model	O
variants	O
using	O
the	O
following	O
techniques	O
.	O
Frequent	O
:	O
We	O
chose	O
the	O
answer	O
span	O
which	O
was	O
the	O
most	O
frequent	O
across	O
the	O
model	O
variants	O
.	O
Rank	O
Score	B-MetricName
:	O
We	O
chose	O
the	O
answer	O
span	O
which	O
was	O
the	O
highest	O
average	O
rank	O
score	O
.	O
Probability	O
Score	B-MetricName
:	O
We	O
chose	O
the	O
answer	O
span	O
which	O
was	O
the	O
highest	O
average	O
probability	O
score	O
.	O
We	O
observe	O
empirically	O
that	O
ensembling	O
using	O
the	O
probability	O
score	O
performs	O
the	O
best	O
and	O
hence	O
we	O
report	O
the	O
results	O
of	O
ensembling	O
using	O
the	O
probability	O
score	O
(	O
E	O
)	O
in	O
Table	O
3	O
.	O
We	O
observe	O
the	O
highest	O
gains	O
after	O
ensembling	O
the	O
outputs	O
of	O
all	O
the	O
5	O
model	O
variants	O
on	O
the	O
validation	O
test	O
and	O
test	O
-	O
dev	O
set	O
.	O
However	O
,	O
the	O
best	O
performance	O
on	O
the	O
test	O
set	O
was	O
achieved	O
by	O
ensembling	O
over	O
the	O
albert	O
-	O
xl	O
models	O
pre	O
-	O
trained	O
independently	O
on	O
CoQA	B-DatasetName
and	O
QuAC	B-DatasetName
(	O
EM	B-MetricName
=	O
53.5	O
,	O
F	O
1	O
=	O
70.9	O
)	O
.	O
This	O
was	O
the	O
final	O
submission	O
for	O
our	O
team	O
.	O

We	O
investigate	O
the	O
disparate	O
impact	O
of	O
pretraining	O
on	O
different	O
MRQA	B-DatasetName
-	O
19	O
datasets	O
on	O
the	O
Doc2Dial	B-DatasetName
shared	O
task	O
.	O
Specifically	O
,	O
we	O
explored	O
factors	O
such	O
as	O
answer	O
length	O
,	O
relative	O
position	O
of	O
the	O
answer	O
in	O
the	O
context	O
,	O
question	O
length	O
,	O
and	O
context	B-HyperparameterName
length	I-HyperparameterName
in	O
Table	O
4	O
.	O
We	O
observe	O
that	O
the	O
SQuAD	B-DatasetName
,	O
NewsQA	B-DatasetName
,	O
and	O
NaturalQuestions	O
(	O
NQ	B-DatasetName
)	O
has	O
compartaively	O
longer	O
answers	O
than	O
the	O
other	O
datasets	O
.	O
However	O
,	O
we	O
do	O
not	O
observe	O
a	O
noticeable	O
difference	O
in	O
terms	O
of	O
question	O
length	O
,	O
context	B-HyperparameterName
length	I-HyperparameterName
or	O
relative	O
position	O
of	O
the	O
answer	O
in	O
the	O
context	O
,	O
with	O
respect	O
to	O
the	O
other	O
datasets	O
.	O
We	O
also	O
use	O
the	O
dataset	O
of	O
Li	O
and	O
Roth	O
(	O
2002	O
)	O
to	O
train	O
a	O
BERT	B-MethodName
classifier	O
to	O
predict	O
answer	O
type	O
of	O
a	O
question	O
with	O
97	O
%	O
accuracy	B-MetricName
.	O
The	O
coarse	O
-	O
answer	O
types	O
are	O
DESC	O
(	O
Description	O
)	O
,	O
NUM	O
(	O
Numerical	O
)	O
,	O
ENT	O
(	O
Entity	O
)	O
,	O
HUM	O
(	O
Person	O
)	O
,	O
LOC	O
(	O
Location	O
)	O
and	O
ABBR	O
(	O
Abbreviation	O
)	O
.	O
We	O
use	O
the	O
classifier	O
to	O
gauge	O
the	O
distribution	O
of	O
answer	O
types	O
on	O
MRQA	B-DatasetName
datasets	O
and	O
Doc2Dial	B-DatasetName
.	O
We	O
observe	O
from	O
Figure	O
2	O
that	O
a	O
majority	O
of	O
questions	O
in	O
Doc2Dial	B-DatasetName
require	O
a	O
descriptive	O
answer	O
.	O
These	O
DESC	O
type	O
questions	O
are	O
more	O
prevelant	O
in	O
SQuAD	B-DatasetName
,	O
NewsQA	B-DatasetName
,	O
and	O
NQ	B-DatasetName
,	O
which	O
might	O
explain	O
their	O
efficacy	O
.	O
To	O
ascertain	O
the	O
benefit	O
of	O
intelligent	O
sampling	O
,	O
we	O
pretrain	O
on	O
a	O
much	O
smaller	O
subset	O
of	O
the	O
SQuAD	B-DatasetName
,	O
NewsQA	B-DatasetName
,	O
and	O
NaturalQuestions	O
dataset	O
,	O
which	O
we	O
obtain	O
via	O
intelligent	O
sampling	O
.	O
We	O
select	O
questions	O
which	O
satisfy	O
one	O
of	O
the	O
following	O
criteria	O
,	O
(	O
i	O
)	O
the	O
answer	O
length	O
of	O
the	O
question	O
is	O
≥	O
50	O
,	O
(	O
ii	O
)	O
the	O
question	O
includes	O
'	O
how	O
'	O
or	O
'	O
why	O
'	O
question	O
word	O
or	O
(	O
iii	O
)	O
the	O
answer	O
type	O
of	O
the	O
question	O
is	O
'	O
DESC	O
'	O
.	O
Overall	O
,	O
the	O
size	O
of	O
the	O
selected	O
sample	O
is	O
only	O
20	O
%	O
of	O
the	O
original	O
dataset	O
,	O
yet	O
achieves	O
a	O
higher	O
EM	B-MetricName
score	O
than	O
the	O
combined	O
dataset	O
as	O
seen	O
in	O
Table	O
2	O
.	O
Yet	O
,	O
surprisingly	O
,	O
the	O
performance	O
is	O
lower	O
than	O
each	O
of	O
the	O
individual	O
dataset	O
.	O

Our	O
submission	O
to	O
the	O
DialDoc	O
subtask	O
1	O
performs	O
continual	B-TaskName
pretraining	I-TaskName
of	O
a	O
transformer	O
-	O
based	O
encoder	O
on	O
out	O
-	O
of	O
-	O
domain	O
QA	O
datasets	O
.	O
Experiments	O
with	O
different	O
QA	O
datasets	O
suggest	O
that	O
conversational	O
QA	O
datasets	O
like	O
CoQA	B-DatasetName
and	O
QuAC	B-DatasetName
are	O
highly	O
beneficial	O
as	O
their	O
setup	O
is	O
substantially	O
similar	O
to	O
Doc2Dial	B-DatasetName
,	O
the	O
downstream	O
dataset	O
of	O
interest	O
.	O
Our	O
final	O
submission	O
ensembles	O
two	O
AlBERT	O
-	O
XL	O
models	O
independently	O
pretrained	O
on	O
CoQA	B-DatasetName
and	O
QuAC	B-DatasetName
and	O
achieves	O
an	O
F1	B-MetricName
-	I-MetricName
Score	I-MetricName
of	O
70.9	O
%	O
and	O
EM	B-MetricName
-	O
Score	B-MetricName
of	O
53.5	O
%	O
on	O
the	O
competition	O
test	O
-	O
set	O
.	O

Stance	B-TaskName
detection	I-TaskName
determines	O
whether	O
the	O
author	O
of	O
a	O
text	O
is	O
in	O
favor	O
of	O
,	O
against	O
or	O
neutral	O
to	O
a	O
specific	O
target	O
and	O
provides	O
valuable	O
insights	O
into	O
important	O
events	O
such	O
as	O
presidential	O
election	O
.	O
However	O
,	O
progress	O
on	O
stance	B-TaskName
detection	I-TaskName
has	O
been	O
hampered	O
by	O
the	O
absence	O
of	O
large	O
annotated	O
datasets	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
P	O
-	O
STANCE	O
,	O
a	O
large	O
stance	B-TaskName
detection	I-TaskName
dataset	O
in	O
the	O
political	O
domain	O
,	O
which	O
contains	O
21	O
,	O
574	O
labeled	O
tweets	O
.	O
We	O
provide	O
a	O
detailed	O
description	O
of	O
the	O
newly	O
created	O
dataset	O
and	O
develop	O
deep	O
learning	O
models	O
on	O
it	O
.	O
Our	O
best	O
model	O
achieves	O
a	O
macro	O
-	O
average	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
80.53	O
%	O
,	O
which	O
we	O
improve	O
further	O
by	O
using	O
semi	O
-	O
supervised	O
learning	O
.	O
Moreover	O
,	O
our	O
P	O
-	O
STANCE	O
dataset	O
can	O
facilitate	O
research	O
in	O
the	O
fields	O
of	O
cross	O
-	O
domain	O
stance	B-TaskName
detection	I-TaskName
such	O
as	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
where	O
a	O
classifier	O
is	O
adapted	O
from	O
a	O
different	O
but	O
related	O
target	O
.	O
We	O
publicly	O
release	O
our	O
dataset	O
and	O
code	O
.	O
1	O

Nowadays	O
,	O
people	O
often	O
express	O
their	O
stances	O
toward	O
specific	O
targets	O
(	O
e.g.	O
,	O
political	O
events	O
or	O
figures	O
,	O
religion	O
,	O
or	O
abortion	O
)	O
on	O
social	O
media	O
.	O
These	O
opinions	O
can	O
provide	O
valuable	O
insights	O
into	O
important	O
events	O
,	O
e.g.	O
,	O
presidential	O
election	O
.	O
The	O
goal	O
of	O
the	O
stance	B-TaskName
detection	I-TaskName
task	O
is	O
to	O
determine	O
whether	O
the	O
author	O
of	O
a	O
piece	O
of	O
text	O
is	O
in	O
favor	O
of	O
,	O
against	O
,	O
or	O
neutral	O
toward	O
a	O
specific	O
target	O
(	O
Mohammad	O
et	O
al	O
,	O
2016b	O
;	O
Küçük	O
and	O
Can	O
,	O
2020	O
;	O
ALDayel	O
and	O
Magdy	O
,	O
2021	O
)	O
.	O
Twitter	O
as	O
a	O
social	O
platform	O
has	O
produced	O
a	O
large	O
quantity	O
of	O
user	O
-	O
generated	O
content	O
,	O
which	O
has	O
become	O
a	O
rich	O
source	O
for	O
mining	O
useful	O
information	O
about	O
various	O
topics	O
such	O
as	O
presidential	O
election	O
.	O
Political	O
figures	O
,	O
who	O
usually	O
receive	O
considerable	O
attention	O
and	O
involve	O
themselves	O
in	O
a	O
large	O
number	O
of	O
political	O
events	O
,	O
are	O
great	O
targets	O
to	O
study	O
stance	B-TaskName
detection	I-TaskName
.	O
Therefore	O
,	O
detecting	O
the	O
1	O
https://github.com/chuchun8/PStance	O
stance	O
expressed	O
toward	O
political	O
figures	O
on	O
Twitter	O
has	O
drawn	O
a	O
lot	O
of	O
attention	O
in	O
the	O
NLP	O
community	O
(	O
Mohammad	O
et	O
al	O
,	O
2016a	O
;	O
Darwish	O
et	O
al	O
,	O
2017	O
)	O
.	O
Even	O
though	O
stance	B-TaskName
detection	I-TaskName
has	O
received	O
a	O
lot	O
of	O
attention	O
,	O
the	O
annotated	O
data	O
are	O
usually	O
limited	O
,	O
which	O
poses	O
strong	O
challenges	O
to	O
supervised	O
models	O
.	O
Moreover	O
,	O
a	O
limitation	O
of	O
existing	O
datasets	O
is	O
that	O
explicit	O
mentions	O
of	O
targets	O
and	O
surface	O
-	O
level	O
lexical	O
cues	O
that	O
may	O
expose	O
the	O
stance	O
can	O
be	O
widely	O
observed	O
in	O
the	O
data	O
(	O
Mohammad	O
et	O
al	O
,	O
2016a	O
;	O
Swami	O
et	O
al	O
,	O
2018	O
;	O
Darwish	O
et	O
al	O
,	O
2018	O
;	O
Conforti	O
et	O
al	O
,	O
2020b	O
;	O
Lai	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
means	O
a	O
model	O
can	O
detect	O
the	O
stance	O
without	O
extracting	O
effective	O
representations	O
for	O
the	O
meanings	O
of	O
sentences	O
(	O
i.e.	O
,	O
their	O
lexical	O
and	O
compositional	O
semantics	O
)	O
.	O
Another	O
limitation	O
of	O
existing	O
datasets	O
,	O
especially	O
the	O
datasets	O
built	O
on	O
social	O
media	O
,	O
is	O
that	O
the	O
average	O
length	O
of	O
tweets	O
is	O
short	O
,	O
which	O
indicates	O
that	O
the	O
data	O
in	O
these	O
previous	O
datasets	O
are	O
less	O
informative	O
and	O
thus	O
the	O
stance	O
can	O
be	O
detected	O
more	O
easily	O
.	O
In	O
an	O
effort	O
to	O
minimize	O
these	O
drawbacks	O
,	O
we	O
present	O
P	O
-	O
STANCE	O
,	O
a	O
dataset	O
for	O
stance	B-TaskName
detection	I-TaskName
whose	O
primary	O
goal	O
is	O
to	O
bridge	O
these	O
gaps	O
by	O
making	O
it	O
possible	O
to	O
run	O
large	O
-	O
scale	O
evaluations	O
that	O
require	O
a	O
deeper	O
semantic	O
understanding	O
.	O
This	O
large	O
annotated	O
dataset	O
is	O
composed	O
of	O
21	O
,	O
574	O
English	O
tweets	O
in	O
the	O
political	O
domain	O
and	O
each	O
tweet	O
is	O
annotated	O
with	O
a	O
stance	O
toward	O
one	O
of	O
three	O
different	O
targets	O
:	O
"	O
Donald	O
Trump	O
,	O
"	O
"	O
Joe	O
Biden	O
,	O
"	O
and	O
"	O
Bernie	O
Sanders	O
.	O
"	O
Examples	O
from	O
our	O
dataset	O
and	O
their	O
stance	O
labels	O
are	O
shown	O
in	O
Table	O
1	O
.	O
The	O
main	O
motivation	O
of	O
building	O
this	O
dataset	O
is	O
to	O
provide	O
a	O
new	O
benchmark	O
for	O
in	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
where	O
a	O
classifier	O
is	O
trained	O
and	O
validated	O
on	O
the	O
same	O
target	O
.	O
However	O
,	O
we	O
show	O
additional	O
interest	O
in	O
constructing	O
a	O
large	O
corpus	O
to	O
facilitate	O
research	O
on	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
where	O
a	O
classifier	O
is	O
adapted	O
from	O
different	O
but	O
related	O
target	O
.	O
More	O
interestingly	O
,	O
P	O
-	O
Stance	O
enables	O
a	O
new	O
task	O
in	O
stance	B-TaskName
detection	I-TaskName
,	O
which	O
is	O
cross	O
-	O
topic	O
stance	B-TaskName
detection	I-TaskName
where	O
a	O
classifier	O
is	O
adapted	O
from	O
the	O
same	O
target	O
but	O
with	O
different	O
topics	O
in	O
the	O
past	O
.	O
These	O
tasks	O
,	O
which	O
use	O
labeled	O
training	O
data	O
of	O
a	O
source	O
target	O
and	O
aim	O
to	O
train	O
a	O
model	O
that	O
generalizes	O
well	O
to	O
a	O
destination	O
target	O
with	O
a	O
shifted	O
distribution	O
,	O
hold	O
great	O
practical	O
value	O
.	O
Our	O
contributions	O
include	O
the	O
following	O
:	O
1	O
)	O
We	O
present	O
P	O
-	O
STANCE	O
,	O
a	O
large	O
dataset	O
for	O
stance	B-TaskName
detection	I-TaskName
composed	O
of	O
21	O
,	O
574	O
tweets	O
sampled	O
from	O
over	O
2.8	O
million	O
tweets	O
collected	O
from	O
Twitter	O
.	O
P	O
-	O
STANCE	O
is	O
more	O
than	O
three	O
times	O
larger	O
than	O
the	O
previous	O
benchmark	O
(	O
Mohammad	O
et	O
al	O
,	O
2016a	O
)	O
and	O
brings	O
additional	O
challenges	O
such	O
as	O
linguistic	O
complexities	O
.	O
We	O
provide	O
a	O
detailed	O
description	O
and	O
a	O
comprehensive	O
analysis	O
of	O
this	O
dataset	O
;	O
2	O
)	O
We	O
conduct	O
experiments	O
on	O
the	O
proposed	O
P	O
-	O
STANCE	O
dataset	O
and	O
establish	O
a	O
strong	O
baseline	O
based	O
on	O
BERTweet	O
(	O
Nguyen	O
et	O
al	O
,	O
2020	O
)	O
.	O
BERTweet	O
achieves	O
a	O
macro	O
-	O
average	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
80.53	O
%	O
,	O
which	O
we	O
improve	O
further	O
by	O
using	O
semisupervised	O
learning	O
;	O
3	O
)	O
The	O
union	O
of	O
P	O
-	O
STANCE	O
and	O
previous	O
benchmark	O
datasets	O
provides	O
more	O
opportunities	O
for	O
studying	O
other	O
stance	B-TaskName
detection	I-TaskName
tasks	O
,	O
e.g.	O
,	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
and	O
crosstopic	O
stance	B-TaskName
detection	I-TaskName
.	O

We	O
gathered	O
stance	O
annotations	O
of	O
three	O
targets	O
through	O
the	O
Amazon	O
Mechanical	O
Turk	O
(	O
AMT	O
)	O
crowdsourcing	O
platform	O
.	O
The	O
AMT	O
workers	O
were	O
asked	O
to	O
annotate	O
each	O
tweet	O
with	O
"	O
Favor	O
,	O
"	O
"	O
Against	O
,	O
"	O
"	O
None	O
,	O
"	O
or	O
"	O
I	O
do	O
n't	O
know	O
.	O
"	O
To	O
ensure	O
the	O
annotation	O
quality	O
,	O
we	O
employed	O
strict	O
requirements	O
for	O
the	O
annotators	O
:	O
1	O
)	O
Many	O
completed	O
tasks	O
(	O
>	O
500	O
)	O
;	O
2	O
)	O
To	O
reside	O
in	O
the	O
USA	O
;	O
3	O
)	O
A	O
high	O
acceptance	O
rate	O
(	O
>	O
95	O
%	O
)	O
.	O
Moreover	O
,	O
we	O
ran	O
the	O
annotation	O
process	O
in	O
several	O
batches	O
of	O
1000	O
examples	O
.	O
In	O
each	O
batch	O
,	O
we	O
include	O
100	O
internally	O
annotated	O
examples	O
to	O
measure	O
the	O
quality	O
of	O
the	O
annotators	O
.	O
If	O
an	O
annotator	O
mislabels	O
more	O
than	O
25	O
%	O
of	O
these	O
examples	O
,	O
we	O
discard	O
the	O
annotations	O
of	O
the	O
worker	O
completely	O
,	O
and	O
relabel	O
them	O
.	O
Interestingly	O
,	O
this	O
process	O
led	O
to	O
a	O
considerable	O
number	O
of	O
reannotations	O
,	O
amounting	O
for	O
more	O
than	O
20	O
%	O
of	O
the	O
data	O
.	O
Each	O
tweet	O
was	O
labeled	O
by	O
three	O
random	O
annotators	O
,	O
and	O
disagreements	O
in	O
the	O
labels	O
were	O
decided	O
by	O
the	O
majority	O
voting	O
among	O
the	O
three	O
annotators	O
.	O
After	O
obtaining	O
the	O
annotation	O
results	O
,	O
we	O
computed	O
Krippendorff	O
's	O
alpha	B-HyperparameterName
(	O
Krippendorff	O
,	O
2011	O
)	O
as	O
the	O
measure	O
of	O
inter	O
-	O
annotator	O
agreement	O
,	O
as	O
shown	O
in	O
Table	O
5	O
.	O
Tweets	O
that	O
were	O
annotated	O
with	O
label	O
"	O
I	O
do	O
n't	O
know	O
"	O
after	O
the	O
majority	O
voting	O
were	O
removed	O
from	O
the	O
dataset	O
.	O
We	O
observed	O
that	O
annotators	O
had	O
difficulties	O
in	O
reaching	O
an	O
agreement	O
on	O
tweets	O
with	O
label	O
"	O
None	O
"	O
and	O
the	O
average	O
of	O
Krippendorff	O
's	O
alpha	B-HyperparameterName
values	O
increases	O
from	O
0.60	O
to	O
0.81	O
when	O
we	O
consider	O
two	O
classes	O
:	O
"	O
Favor	O
"	O
and	O
"	O
Against	O
"	O
.	O
Similar	O
to	O
prior	O
work	O
(	O
Vamvas	O
and	O
Sennrich	O
,	O
2020	O
)	O
,	O
we	O
removed	O
the	O
label	O
"	O
None	O
"	O
from	O
the	O
dataset	O
in	O
our	O
experiments	O
.	O

Similar	O
to	O
Mohammad	O
et	O
al	O
(	O
2017	O
)	O
and	O
,	O
F	O
avg	O
and	O
macro	O
-	O
average	O
of	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
(	O
F	O
macro	O
)	O
are	O
adopted	O
to	O
evaluate	O
the	O
performance	O
of	O
our	O
baseline	O
models	O
.	O
First	O
,	O
the	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
label	O
"	O
Favor	O
"	O
and	O
"	O
Against	O
"	O
is	O
calculated	O
as	O
follows	O
:	O
F	O
f	O
avor	O
=	O
2P	O
f	O
avor	O
R	O
f	O
avor	O
P	O
f	O
avor	O
+	O
R	O
f	O
avor	O
(	O
1	O
)	O
F	O
against	O
=	O
2P	O
against	O
R	O
against	O
P	O
against	O
+	O
R	O
against	O
(	O
2	O
)	O
where	O
P	O
and	O
R	O
are	O
precision	O
and	O
recall	O
,	O
respectively	O
.	O
After	O
that	O
,	O
the	O
F	O
avg	O
is	O
calculated	O
as	O
:	O
F	O
avg	O
=	O
F	O
f	O
avor	O
+	O
F	O
against	O
2	O
(	O
3	O
)	O
We	O
compute	O
the	O
F	O
avg	O
for	O
each	O
target	O
.	O
F	O
macro	O
is	O
calculated	O
by	O
averaging	O
the	O
F	O
avg	O
across	O
all	O
targets	O
.	O

We	O
run	O
experiments	O
with	O
the	O
following	O
baselines	O
.	O
BiLSTM	B-MethodName
(	O
Schuster	O
and	O
Paliwal	O
,	O
1997	O
)	O
:	O
A	O
BiL	O
-	O
STM	O
model	O
that	O
takes	O
tweets	O
as	O
inputs	O
without	O
considering	O
the	O
target	O
information	O
.	O
CNN	O
(	O
Kim	O
,	O
2014	O
)	O
:	O
Similar	O
to	O
BiLSTM	B-MethodName
,	O
the	O
vanilla	O
CNN	O
only	O
takes	O
tweets	O
as	O
inputs	O
and	O
does	O
not	O
consider	O
the	O
target	O
information	O
.	O
TAN	O
(	O
Du	O
et	O
al	O
,	O
2017	O
)	O
:	O
TAN	O
is	O
an	O
attention	O
-	O
based	O
LSTM	B-MethodName
model	O
that	O
extracts	O
target	O
specific	O
features	O
.	O
BiCE	O
(	O
Augenstein	O
et	O
al	O
,	O
2016	O
)	O
:	O
A	O
BiLSTM	B-MethodName
that	O
uses	O
conditional	O
encoding	O
for	O
stance	B-TaskName
detection	I-TaskName
.	O
The	O
target	O
information	O
is	O
first	O
encoded	O
by	O
a	O
BiLSTM	B-MethodName
,	O
whose	O
hidden	O
representations	O
are	O
then	O
used	O
to	O
initialize	O
another	O
BiLSTM	B-MethodName
with	O
tweets	O
as	O
inputs	O
.	O
BiCE	O
is	O
also	O
a	O
strong	O
baseline	O
for	O
crosstarget	O
stance	B-TaskName
detection	I-TaskName
.	O
CrossNet	O
(	O
Xu	O
et	O
al	O
,	O
2018	O
)	O
:	O
CrossNet	O
is	O
another	O
model	O
for	O
cross	O
-	O
target	O
stance	B-TaskName
detection	I-TaskName
.	O
It	O
encodes	O
the	O
target	O
and	O
the	O
tweet	O
by	O
using	O
the	O
same	O
approach	O
with	O
BiCE	O
and	O
add	O
an	O
aspect	O
attention	O
layer	O
to	O
signal	O
the	O
core	O
part	O
of	O
a	O
stance	O
-	O
bearing	O
input	O
.	O
Cross	O
-	O
Net	O
improves	O
BiCE	O
in	O
many	O
cross	O
-	O
target	O
settings	O
.	O
GCAE	O
(	O
Xue	O
and	O
Li	O
,	O
2018	O
)	O
:	O
A	O
CNN	O
model	O
that	O
utilizes	O
a	O
gating	O
mechanism	O
to	O
block	O
targetunrelated	O
information	O
.	O
GCAE	O
is	O
a	O
strong	O
baseline	O
for	O
aspect	B-TaskName
-	I-TaskName
based	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
and	O
we	O
apply	O
it	O
to	O
our	O
stance	B-TaskName
detection	I-TaskName
task	O
.	O
PGCNN	O
(	O
Huang	O
and	O
Carley	O
,	O
2018	O
)	O
:	O
Similar	O
to	O
GCAE	O
,	O
PGCNN	O
is	O
based	O
on	O
gated	O
convolutional	O
networks	O
and	O
encodes	O
target	O
information	O
by	O
generating	O
target	O
-	O
sensitive	O
filters	O
.	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
:	O
A	O
pre	O
-	O
trained	O
language	O
model	O
that	O
predicts	O
the	O
stance	O
by	O
appending	O
a	O
linear	O
classification	O
layer	O
to	O
the	O
hidden	O
representation	O
of	O
[	O
CLS	O
]	O
token	O
.	O
We	O
fine	O
-	O
tune	O
the	O
BERT	B-MethodName
-	O
base	O
on	O
the	O
stance	B-TaskName
detection	I-TaskName
task	O
.	O
BERTweet	O
(	O
Nguyen	O
et	O
al	O
,	O
2020	O
)	O
:	O
BERTweet	O
is	O
another	O
pre	O
-	O
trained	O
language	O
model	O
following	O
the	O
training	O
procedure	O
of	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
.	O
Similar	O
to	O
BERT	B-MethodName
,	O
we	O
fine	O
-	O
tune	O
the	O
pretrained	O
BERTweet	O
to	O
predict	O
the	O
stance	O
by	O
appending	O
a	O
linear	O
classification	O
layer	O
to	O
the	O
hidden	O
representation	O
of	O
the	O
[	O
CLS	O
]	O
token	O
.	O
The	O
pre	O
-	O
trained	O
BERTweet	O
model	O
is	O
fine	O
-	O
tuned	O
under	O
the	O
PyTorch	O
framework	O
.	O
The	O
maximum	O
sequence	O
length	O
is	O
set	O
to	O
128	O
and	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
32	O
.	O
We	O
use	O
AdamW	B-MethodName
optimizer	B-HyperparameterName
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2019	O
)	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
2e	O
-	O
5	O
.	O

During	O
elections	O
,	O
there	O
is	O
a	O
considerable	O
amount	O
of	O
data	O
generated	O
by	O
users	O
expressing	O
their	O
opinions	O
about	O
candidates	O
,	O
out	O
of	O
which	O
only	O
a	O
small	O
amount	O
can	O
be	O
annotated	O
and	O
used	O
for	O
supervised	O
stance	B-TaskName
detection	I-TaskName
.	O
We	O
explore	O
the	O
potential	O
of	O
the	O
abundant	O
unlabeled	O
tweets	O
and	O
show	O
that	O
we	O
can	O
leverage	O
them	O
to	O
improve	O
the	O
performance	O
of	O
our	O
models	O
.	O
To	O
this	O
end	O
,	O
we	O
turn	O
to	O
semi	O
-	O
supervised	O
learning	O
,	O
and	O
leverage	O
techniques	O
such	O
as	O
Uncertainty	O
-	O
aware	O
Self	O
-	O
Training	O
(	O
UST	O
)	O
.	O
UST	O
(	O
Mukherjee	O
and	O
Awadallah	O
,	O
2020	O
)	O
is	O
a	O
semi	O
-	O
supervised	O
approach	O
which	O
uses	O
the	O
standard	O
teacher	O
-	O
student	O
self	O
-	O
training	O
framework	O
,	O
but	O
adds	O
a	O
few	O
powerful	O
changes	O
.	O
Concretely	O
,	O
UST	O
designs	O
different	O
techniques	O
which	O
leverage	O
the	O
uncertainty	O
of	O
the	O
teacher	O
model	O
to	O
select	O
the	O
unlabeled	O
set	O
of	O
examples	O
in	O
each	O
self	O
-	O
training	O
iteration	O
.	O
First	O
,	O
we	O
train	O
our	O
teacher	O
model	O
on	O
the	O
labeled	O
examples	O
.	O
Next	O
,	O
we	O
compute	O
uncertainty	O
estimates	O
of	O
our	O
teacher	O
model	O
on	O
the	O
set	O
of	O
unlabeled	O
examples	O
by	O
performing	O
a	O
few	O
forward	O
passes	O
with	O
dropout	O
enabled	O
.	O
Finally	O
,	O
we	O
incorporate	O
the	O
uncertainty	O
estimates	O
into	O
our	O
framework	O
as	O
follows	O
:	O
1	O
)	O
We	O
use	O
these	O
estimates	O
to	O
select	O
the	O
examples	O
for	O
which	O
the	O
teacher	O
is	O
most	O
or	O
least	O
confident	O
about	O
.	O
2	O
)	O
We	O
incorporate	O
the	O
teacher	O
confidence	O
in	O
the	O
student	O
loss	B-MetricName
by	O
penalizing	O
the	O
student	O
's	O
misclassified	O
examples	O
in	O
which	O
the	O
teacher	O
has	O
high	O
confidence	O
.	O
We	O
use	O
the	O
BERTweet	O
model	O
as	O
teacher	O
and	O
student	O
.	O
We	O
perform	O
various	O
experiments	O
to	O
show	O
the	O
benefits	O
of	O
using	O
a	O
large	O
amount	O
of	O
unlabeled	O
data	O
from	O
P	O
-	O
STANCE	O
-	O
EXT	O
alongside	O
our	O
UST	O
model	O
.	O
We	O
carry	O
out	O
three	O
barely	O
supervised	O
experiments	O
with	O
various	O
number	O
of	O
examples	O
in	O
the	O
training	O
set	O
.	O
Specifically	O
,	O
we	O
experiment	O
with	O
30	O
,	O
50	O
,	O
and	O
100	O
training	O
examples	O
.	O
Moreover	O
,	O
we	O
also	O
consider	O
an	O
experiment	O
using	O
the	O
whole	O
training	O
set	O
to	O
investigate	O
the	O
effect	O
of	O
the	O
unlabeled	O
examples	O
when	O
all	O
the	O
training	O
data	O
are	O
available	O
.	O
We	O
run	O
experiments	O
with	O
different	O
training	O
sets	O
,	O
and	O
report	O
the	O
F1	B-MetricName
-	O
scores	O
obtained	O
on	O
the	O
entire	O
testing	O
set	O
.	O
We	O
show	O
the	O
results	O
of	O
our	O
semi	O
-	O
supervised	O
ex	O
-	O

In	O
this	O
section	O
,	O
we	O
formally	O
define	O
the	O
GEC	O
task	O
discussed	O
in	O
this	O
paper	O
.	O
Let	O
D	O
be	O
the	O
GEC	O
training	O
data	O
that	O
comprise	O
pairs	O
of	O
an	O
ungrammatical	O
source	O
sentence	O
X	O
and	O
grammatical	O
target	O
sentence	O
Y	O
,	O
i.e.	O
,	O
D	O
=	O
{	O
(	O
X	O
n	O
,	O
Y	O
n	O
)	O
}	O
n	O
.	O
Here	O
,	O
|	O
D	O
|	O
denotes	O
the	O
number	O
of	O
sentence	O
pairs	O
in	O
the	O
dataset	O
D.	O
Let	O
Θ	B-HyperparameterName
represent	O
all	O
trainable	O
parameters	O
of	O
the	O
model	O
.	O
Our	O
objective	O
is	O
to	O
find	O
the	O
optimal	O
parameter	O
set	O
Θ	B-HyperparameterName
that	O
minimizes	O
the	O
following	O
objective	O
function	O
L	O
(	O
D	O
,	O
Θ	B-HyperparameterName
)	O
for	O
the	O
given	O
training	O
data	O
D	O
:	O
L	O
(	O
D	O
,	O
Θ	B-HyperparameterName
)	O
=	O
−	O
1	O
|	O
D	O
|	O
(	O
X	O
,	O
Y	O
)	O
D	O
log	O
(	O
p	O
(	O
Y	O
|	O
X	O
,	O
Θ	B-HyperparameterName
)	O
)	O
,	O
(	O
1	O
)	O
where	O
p	O
(	O
Y	O
|	O
X	O
,	O
Θ	B-HyperparameterName
)	O
denotes	O
the	O
conditional	O
probability	O
of	O
Y	O
given	O
X.	O
In	O
the	O
standard	O
supervised	O
learning	O
setting	O
,	O
the	O
parallel	O
data	O
D	O
comprise	O
only	O
"	O
genuine	O
"	O
parallel	O
data	O
D	O
g	O
(	O
i.e.	O
,	O
D	O
=	O
D	O
g	O
)	O
.	O
However	O
,	O
in	O
GEC	O
,	O
incorporating	O
pseudo	O
data	O
D	O
p	O
that	O
are	O
generated	O
from	O
grammatical	O
sentences	O
Y	O
T	O
,	O
where	O
T	O
represents	O
seed	O
corpus	O
(	O
i.e.	O
,	O
a	O
set	O
of	O
grammatical	O
sentences	O
)	O
,	O
is	O
common	O
(	O
Xie	O
et	O
al	O
,	O
2018	O
;	O
Zhao	O
et	O
al	O
,	O
2019	O
;	O
Grundkiewicz	O
et	O
al	O
,	O
2019	O
)	O
.	O
Our	O
interest	O
lies	O
in	O
the	O
following	O
three	O
nontrivial	O
aspects	O
of	O
Equation	O
1	O
.	O
Aspect	O
(	O
i	O
)	O
:	O
multiple	O
methods	O
for	O
generating	O
pseudo	O
data	O
D	O
p	O
are	O
available	O
(	O
Section	O
3	O
)	O
.	O
Aspect	O
(	O
ii	O
)	O
:	O
options	O
for	O
the	O
seed	O
corpus	O
T	O
are	O
numerous	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
how	O
the	O
seed	O
corpus	O
domain	O
affects	O
the	O
model	O
performance	O
is	O
yet	O
to	O
be	O
shown	O
.	O
We	O
compare	O
three	O
corpora	O
,	O
namely	O
,	O
Wikipedia	O
,	O
Simple	O
Wikipedia	O
(	O
SimpleWiki	O
)	O
and	O
English	O
Gigaword	O
,	O
as	O
a	O
first	O
trial	O
.	O
Wikipedia	O
and	O
SimpleWiki	O
have	O
similar	O
domains	O
,	O
but	O
different	O
grammatical	O
complexities	O
.	O
Therefore	O
,	O
we	O
can	O
investigate	O
how	O
grammatical	O
complexity	O
affects	O
model	O
performance	O
by	O
comparing	O
these	O
two	O
corpora	O
.	O
We	O
assume	O
that	O
Gigaword	O
contains	O
the	O
smallest	O
amount	O
of	O
noise	O
among	O
the	O
three	O
corpora	O
.	O
We	O
can	O
therefore	O
use	O
Gigaword	O
to	O
investigate	O
whether	O
clean	O
text	O
improves	O
model	O
performance	O
.	O
Aspect	O
(	O
iii	O
)	O
:	O
at	O
least	O
two	O
major	O
settings	O
for	O
incorporating	O
D	O
p	O
into	O
the	O
optimization	O
of	O
Equation	O
1	O
are	O
available	O
.	O
One	O
is	O
to	O
use	O
the	O
two	O
datasets	O
jointly	O
by	O
concatenating	O
them	O
as	O
D	O
=	O
D	O
g	O
∪	O
D	O
p	O
,	O
which	O
hereinafter	O
we	O
refer	O
to	O
as	O
JOINT	O
.	O
The	O
other	O
is	O
to	O
use	O
D	O
p	O
for	O
pretraining	O
,	O
namely	O
,	O
minimizing	O
L	O
(	O
D	O
p	O
,	O
Θ	B-HyperparameterName
)	O
to	O
acquire	O
Θ	B-HyperparameterName
,	O
and	O
then	O
fine	O
-	O
tuning	O
the	O
model	O
by	O
minimizing	O
L	O
(	O
D	O
g	O
,	O
Θ	B-HyperparameterName
)	O
;	O
hereinafter	O
,	O
we	O
refer	O
to	O
this	O
setting	O
as	O
PRETRAIN	O
.	O
We	O
investigate	O
these	O
aspects	O
through	O
our	O
extensive	O
experiments	O
(	O
Section	O
4	O
)	O
.	O

Backtranslation	O
for	O
the	O
EncDec	O
model	O
was	O
proposed	O
originally	O
by	O
Sennrich	O
et	O
al	O
(	O
2016b	O
)	O
.	O
In	O
backtranslation	O
,	O
a	O
reverse	O
model	O
,	O
which	O
generates	O
an	O
ungrammatical	O
sentence	O
from	O
a	O
given	O
grammatical	O
sentence	O
,	O
is	O
trained	O
.	O
The	O
output	O
of	O
the	O
reverse	O
model	O
is	O
paired	O
with	O
the	O
input	O
and	O
then	O
used	O
as	O
pseudo	O
data	O
.	O
BACKTRANS	O
(	O
NOISY	O
)	O
is	O
a	O
variant	O
of	O
backtranslation	O
that	O
was	O
proposed	O
by	O
Xie	O
et	O
al	O
(	O
2018	O
)	O
2	O
.	O
This	O
method	O
adds	O
rβ	O
random	O
to	O
the	O
score	O
of	O
each	O
hypothesis	O
in	O
the	O
beam	O
for	O
every	O
time	O
step	O
.	O
Here	O
,	O
noise	O
r	O
is	O
sampled	O
uniformly	O
from	O
the	O
interval	O
[	O
0	B-DatasetName
,	O
1	O
]	O
,	O
and	O
β	B-HyperparameterName
random	O
R	O
≥0	O
is	O
a	O
hyper	O
-	O
parameter	O
that	O
controls	O
the	O
noise	O
scale	O
.	O
If	O
we	O
set	O
β	B-HyperparameterName
random	O
=	O
0	B-DatasetName
,	O
then	O
BACK	O
-	O
TRANS	O
(	O
NOISY	O
)	O
is	O
identical	O
to	O
standard	O
backtranslation	O
.	O
BACKTRANS	O
(	O
SAMPLE	O
)	O
is	O
another	O
variant	O
of	O
backtranslation	O
,	O
which	O
was	O
proposed	O
by	O
Edunov	O
et	O
al	O
(	O
2018	O
)	O
for	O
MT	O
.	O
In	O
BACKTRANS	O
(	O
SAMPLE	O
)	O
,	O
sentences	O
are	O
decoded	O
by	O
sampling	O
from	O
the	O
distribution	O
of	O
the	O
reverse	O
model	O
.	O
DIRECTNOISE	O
Whereas	O
BACKTRANS	O
(	O
NOISY	O
)	O
and	O
BACKTRANS	O
(	O
SAMPLE	O
)	O
generate	O
ungrammatical	O
sentences	O
with	O
a	O
reverse	O
model	O
,	O
DIRECT	O
-	O
NOISE	O
injects	O
noise	O
"	O
directly	O
"	O
into	O
grammatical	O
sentences	O
(	O
Edunov	O
et	O
al	O
,	O
2018	O
;	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
.	O
Specifically	O
,	O
for	O
each	O
token	O
in	O
the	O
given	O
sentence	O
,	O
this	O
method	O
probabilistically	O
chooses	O
one	O
of	O
the	O
following	O
operations	O
:	O
(	O
i	O
)	O
masking	O
with	O
a	O
placeholder	O
token	O
mask	O
,	O
(	O
ii	O
)	O
deletion	O
,	O
(	O
iii	O
)	O
insertion	O
of	O
a	O
random	O
token	O
,	O
and	O
(	O
iv	O
)	O
keeping	O
the	O
original	O
3	O
.	O
For	O
each	O
token	O
,	O
the	O
choice	O
is	O
made	O
based	O
on	O
the	O
categorical	O
distribution	O
(	O
µ	O
mask	O
,	O
µ	O
deletion	O
,	O
µ	O
insertion	O
,	O
µ	O
keep	O
)	O
.	O

We	O
compare	O
the	O
effectiveness	O
of	O
the	O
BACK	O
-	O
TRANS	O
(	O
NOISY	O
)	O
,	O
BACKTRANS	O
(	O
SAMPLE	O
)	O
,	O
and	O
DIRECTNOISE	O
methods	O
for	O
generating	O
pseudo	O
data	O
.	O
In	O
DIRECTNOISE	O
,	O
we	O
set	O
(	O
µ	O
mask	O
,	O
µ	O
deletion	O
,	O
µ	O
insertion	O
,	O
µ	O
keep	O
)	O
=	O
(	O
0.5	O
,	O
0.15	O
,	O
0.15	O
,	O
0.2	O
)	O
11	O
.	O
We	O
use	O
β	B-HyperparameterName
random	O
=	O
6	O
for	O
BACKTRANS	O
(	O
NOISY	O
)	O
12	O
.	O
In	O
addition	O
,	O
we	O
use	O
(	O
i	O
)	O
the	O
JOINT	O
setting	O
and	O
(	O
ii	O
)	O
all	O
of	O
SimpleWiki	O
as	O
the	O
seed	O
corpus	O
T	O
throughout	O
this	O
section	O
.	O
The	O
results	O
are	O
summarized	O
in	O
Table	O
2	O
.	O
BACK	O
-	O
TRANS	O
(	O
NOISY	O
)	O
and	O
BACKTRANS	O
(	O
SAMPLE	O
)	O
show	O
competitive	O
values	O
of	O
F	O
0.5	O
.	O
Given	O
this	O
result	O
,	O
we	O
exclusively	O
use	O
BACKTRANS	O
(	O
NOISY	O
)	O
and	O
discard	O
BACKTRANS	O
(	O
SAMPLE	O
)	O
for	O
the	O
rest	O
of	O
the	O
experiments	O
.	O
The	O
advantage	O
of	O
BACKTRANS	O
(	O
NOISY	O
)	O
is	O
that	O
its	O
effectiveness	O
in	O
GEC	O
has	O
already	O
been	O
demonstrated	O
by	O
Xie	O
et	O
al	O
(	O
2018	O
)	O
.	O
In	O
addition	O
,	O
in	O
our	O
preliminary	O
experiment	O
,	O
BACKTRANS	O
(	O
NOISY	O
)	O
decoded	O
ungrammatical	O
sentence	O
1.2	O
times	O
faster	O
than	O
BACKTRANS	O
(	O
SAMPLE	O
)	O
did	O
.	O
We	O
also	O
use	O
DI	O
-	O
RECTNOISE	O
because	O
it	O
achieved	O
the	O
best	O
value	O
of	O
F	O
0.5	O
among	O
all	O
the	O
methods	O
.	O

The	O
present	O
experimental	O
results	O
show	O
that	O
the	O
following	O
configurations	O
are	O
effective	O
for	O
improving	O
the	O
model	O
performance	O
:	O
(	O
i	O
)	O
the	O
combination	O
of	O
JOINT	O
and	O
Gigaword	O
(	O
Section	O
4.3	O
)	O
,	O
(	O
ii	O
)	O
the	O
amount	O
of	O
pseudo	O
data	O
D	O
p	O
not	O
being	O
too	O
large	O
in	O
JOINT	O
(	O
Section	O
4.4	O
(	O
a	O
)	O
)	O
,	O
and	O
(	O
iii	O
)	O
PRETRAIN	O
with	O
BACK	O
-	O
TRANS	O
(	O
NOISY	O
)	O
using	O
large	O
pseudo	O
data	O
D	O
p	O
(	O
Section	O
4.4	O
(	O
b	O
)	O
)	O
.	O
We	O
summarize	O
these	O
findings	O
and	O
attempt	O
to	O
combine	O
PRETRAIN	O
and	O
JOINT	O
.	O
Specifically	O
,	O
we	O
pretrain	O
the	O
model	O
using	O
70	O
M	O
pseudo	O
data	O
of	O
BACKTRANS	O
(	O
NOISY	O
)	O
.	O
We	O
then	O
fine	O
-	O
tune	O
the	O
model	O
by	O
combining	O
BEA	O
-	O
train	O
and	O
relatively	O
small	O
DIRECTNOISE	O
pseudo	O
data	O
generated	O
from	O
Gigaword	O
(	O
we	O
set	O
|	O
D	O
p	O
|	O
=	O
250	O
K	O
)	O
.	O
However	O
,	O
the	O
performance	O
does	O
not	O
improve	O
on	O
BEA	O
-	O
valid	O
.	O
Therefore	O
,	O
the	O
best	O
approach	O
available	O
is	O
simply	O
to	O
pretrain	O
the	O
model	O
with	O
large	O
(	O
70	O
M	O
)	O
BACKTRANS	O
(	O
NOISY	O
)	O
pseudo	O
data	O
and	O
then	O
fine	O
-	O
tune	O
using	O
BEAtrain	O
,	O
which	O
hereinafter	O
we	O
refer	O
to	O
as	O
PRETLARGE	O
.	O
We	O
use	O
Gigaword	O
for	O
the	O
seed	O
corpus	O
T	O
because	O
it	O
has	O
the	O
best	O
performance	O
in	O
Table	O
3	O
.	O
We	O
evaluate	O
the	O
performance	O
of	O
PRETLARGE	O
on	O
test	O
sets	O
and	O
compare	O
the	O
scores	O
with	O
the	O
current	O
top	O
models	O
.	O
our	O
PRETLARGE	O
achieves	O
F	O
0.5	O
=	O
61.3	O
on	O
CoNLL	O
-	O
2014	O
.	O
This	O
result	O
outperforms	O
not	O
only	O
all	O
previous	O
single	O
-	O
model	O
results	O
but	O
also	O
all	O
ensemble	O
results	O
except	O
for	O
that	O
by	O
Grundkiewicz	O
et	O
al	O
(	O
2019	O
)	O
.	O
To	O
further	O
improve	O
the	O
performance	O
,	O
we	O
incorporate	O
the	O
following	O
techniques	O
that	O
are	O
widely	O
used	O
in	O
shared	O
tasks	O
such	O
as	O
BEA	O
-	O
2019	O
and	O
WMT	O
13	O
:	O
Synthetic	O
Spelling	O
Error	B-MetricName
(	O
SSE	B-MethodName
)	O
Lichtarge	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
the	O
method	O
of	O
probabilistically	O
injecting	O
character	O
-	O
level	O
noise	O
into	O
the	O
source	O
sentence	O
of	O
pseudo	O
data	O
D	O
p	O
.	O
Specifically	O
,	O
one	O
of	O
the	O
following	O
operations	O
is	O
applied	O
randomly	O
at	O
a	O
rate	O
of	O
0.003	O
per	O
character	O
:	O
deletion	O
,	O
insertion	O
,	O
replacement	O
,	O
or	O
transposition	O
of	O
adjacent	O
characters	O
.	O
Right	O
-	O
to	O
-	O
left	O
Re	O
-	O
ranking	O
(	O
R2L	O
)	O
Following	O
Sennrich	O
et	O
al	O
(	O
2016aSennrich	O
et	O
al	O
(	O
,	O
2017	O
;	O
Grundkiewicz	O
et	O
al	O
(	O
2019	O
)	O
,	O
we	O
train	O
four	O
right	O
-	O
to	O
-	O
left	O
models	O
.	O
The	O
ensemble	O
of	O
four	O
left	O
-	O
to	O
-	O
right	O
models	O
generate	O
n	O
-	O
best	O
candidates	O
and	O
their	O
corresponding	O
scores	O
(	O
i.e.	O
,	O
conditional	O
probabilities	O
)	O
.	O
We	O
then	O
pass	O
each	O
candidate	O
to	O
the	O
ensemble	O
of	O
the	O
four	O
right	O
-	O
to	O
-	O
left	O
models	O
and	O
compute	O
the	O
score	O
.	O
Finally	O
,	O
we	O
re	O
-	O
rank	O
the	O
n	O
-	O
best	O
candidates	O
based	O
on	O
the	O
sum	O
of	O
the	O
two	O
scores	O
.	O
Sentence	O
-	O
level	O
Error	B-MetricName
Detection	O
(	O
SED	O
)	O
SED	O
classifies	O
whether	O
a	O
given	O
sentence	O
contains	O
a	O
grammatical	O
error	O
.	O
Asano	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
incorporating	O
SED	O
into	O
the	O
evaluation	O
pipeline	O
and	O
reported	O
improved	O
precision	O
.	O
Here	O
,	O
the	O
GEC	O
model	O
is	O
applied	O
only	O
if	O
SED	O
detects	O
a	O
grammatical	O
error	O
in	O
the	O
given	O
source	O
sentence	O
.	O
The	O
motivation	O
is	O
that	O
SED	O
could	O
potentially	O
reduce	O
the	O
number	O
of	O
false	O
-	O
positive	O
errors	O
of	O
the	O
GEC	O
model	O
.	O
We	O
use	O
the	O
re	O
-	O
implementation	O
of	O
the	O
BERT	B-MethodName
-	O
based	O
SED	O
model	O
(	O
Asano	O
et	O
al	O
,	O
2019	O
)	O
.	O
Table	O
5	O
presents	O
the	O
results	O
of	O
applying	O
SSE	B-MethodName
,	O
13	O
http://www.statmt.org/wmt19/	O
R2L	O
,	O
and	O
SED	O
.	O
It	O
is	O
noteworthy	O
that	O
PRET	O
-	O
LARGE+SSE+R2L	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
both	O
CoNLL	O
-	O
2014	O
(	O
F	O
0.5	O
=	O
65.0	O
)	O
and	O
BEA	O
-	O
test	O
(	O
F	O
0.5	O
=	O
69.8	O
)	O
,	O
which	O
are	O
better	O
than	O
those	O
of	O
the	O
best	O
system	O
of	O
the	O
BEA	O
-	O
2019	O
shared	O
task	O
(	O
Grundkiewicz	O
et	O
al	O
,	O
2019	O
)	O
.	O
In	O
addition	O
,	O
PRET	O
-	O
LARGE+SSE+R2L+SED	O
can	O
further	O
improve	O
the	O
performance	O
on	O
BEA	O
-	O
test	O
(	O
F	O
0.5	O
=	O
70.2	O
)	O
.	O
However	O
,	O
unfortunately	O
,	O
incorporating	O
SED	O
decreased	O
the	O
performance	O
on	O
CoNLL	O
-	O
2014	O
and	O
JFLEG	B-DatasetName
.	O
This	O
fact	O
implies	O
that	O
SED	O
is	O
sensitive	O
to	O
the	O
domain	O
of	O
the	O
test	O
set	O
since	O
the	O
SED	O
model	O
is	O
fine	O
-	O
tuned	O
with	O
the	O
official	O
validation	O
split	O
of	O
BEA	O
dataset	O
.	O
We	O
leave	O
this	O
sensitivity	O
issue	O
as	O
our	O
future	O
work	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
our	O
systems	O
submitted	O
to	O
the	O
very	O
low	O
resource	O
supervised	O
translation	O
task	O
at	O
WMT20	O
.	O
We	O
participate	O
in	O
both	O
translation	O
directions	O
for	O
Upper	O
Sorbian	O
-	O
German	O
language	O
pair	O
.	O
Our	O
primary	O
submission	O
is	O
a	O
subword	O
-	O
level	O
Transformer	B-MethodName
-	O
based	O
neural	O
machine	B-TaskName
translation	I-TaskName
model	O
trained	O
on	O
original	O
training	O
bitext	O
.	O
We	O
also	O
conduct	O
several	O
experiments	O
with	O
backtranslation	O
using	O
limited	O
monolingual	O
data	O
in	O
our	O
postsubmission	O
work	O
and	O
include	O
our	O
results	O
for	O
the	O
same	O
.	O
In	O
one	O
such	O
experiment	O
,	O
we	O
observe	O
jumps	O
of	O
up	O
to	O
2.6	O
BLEU	B-MetricName
points	O
over	O
the	O
primary	O
system	O
by	O
pretraining	O
on	O
a	O
synthetic	O
,	O
backtranslated	O
corpus	O
followed	O
by	O
fine	O
-	O
tuning	O
on	O
the	O
original	O
parallel	O
training	O
data	O
.	O

The	O
Transformer	B-MethodName
model	O
is	O
the	O
dominant	O
architecture	O
within	O
current	O
NMT	O
models	O
due	O
to	O
its	O
superior	O
performance	O
on	O
several	O
language	O
pairs	O
.	O
While	O
still	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
)	O
model	O
composed	O
of	O
an	O
encoder	O
and	O
a	O
decoder	O
,	O
Transformer	B-MethodName
models	O
are	O
highly	O
parallelizable	O
thanks	O
to	O
being	O
composed	O
purely	O
of	O
feedforward	O
and	O
self	O
-	O
attention	B-HyperparameterName
layers	I-HyperparameterName
rather	O
than	O
recurrent	O
layers	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
;	O
Cho	O
et	O
al	O
,	O
2014b	O
)	O
.	O
The	O
reader	O
is	O
encouraged	O
to	O
read	O
the	O
original	O
paper	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
to	O
gain	O
a	O
deeper	O
understanding	O
of	O
the	O
model	O
.	O
We	O
adopt	O
the	O
Transformer	B-MethodName
base	O
architecture	O
available	O
under	O
the	O
fairseq	O
2	O
(	O
Ott	O
et	O
al	O
,	O
2019	O
)	O
library	O
for	O
all	O
our	O
models	O
.	O
However	O
,	O
NMT	O
models	O
are	O
known	O
to	O
be	O
datahungry	O
(	O
Koehn	O
and	O
Knowles	O
,	O
2017	O
)	O
;	O
their	O
performance	O
improves	O
sharply	O
with	O
the	O
availability	O
of	O
more	O
parallel	O
training	O
data	O
.	O
Except	O
for	O
a	O
few	O
language	O
pairs	O
(	O
e.g.	O
English	O
-	O
German	O
)	O
,	O
most	O
have	O
little	O
to	O
no	O
such	O
data	O
available	O
.	O
On	O
the	O
other	O
hand	O
,	O
a	O
far	O
greater	O
number	O
of	O
languages	O
have	O
a	O
decent	O
amount	O
of	O
monolingual	O
data	O
available	O
online	O
(	O
e.g.	O
Wikipedia	O
)	O
.	O
To	O
address	O
this	O
issue	O
of	O
lack	O
of	O
parallel	O
data	O
,	O
Sennrich	O
et	O
al	O
(	O
2016a	O
)	O
introduced	O
the	O
concept	O
of	O
backtranslation	O
.	O
It	O
involves	O
creating	O
a	O
synthetic	O
parallel	O
corpus	O
by	O
translating	O
sentences	O
from	O
the	O
target	O
-	O
side	O
monolingual	O
data	O
to	O
the	O
source	O
language	O
and	O
making	O
corresponding	O
pairs	O
.	O
A	O
baseline	O
target	O
source	O
model	O
(	O
PBSMT	O
or	O
NMT	O
)	O
,	O
trained	O
with	O
limited	O
data	O
,	O
is	O
generally	O
used	O
for	O
this	O
purpose	O
.	O
It	O
enables	O
the	O
use	O
of	O
large	O
corpora	O
of	O
monolingual	O
data	O
for	O
several	O
languages	O
,	O
the	O
size	O
of	O
which	O
is	O
typically	O
orders	O
of	O
magnitude	O
larger	O
than	O
any	O
corresponding	O
bitext	O
available	O
.	O
What	O
is	O
notable	O
is	O
that	O
only	O
the	O
sourceside	O
data	O
is	O
synthetic	O
in	O
such	O
a	O
scenario	O
and	O
the	O
target	O
-	O
side	O
still	O
corresponds	O
to	O
original	O
monolingual	O
data	O
.	O
Some	O
studies	O
(	O
Poncelas	O
et	O
al	O
,	O
2018	O
;	O
Popel	O
,	O
2018	O
)	O
have	O
investigated	O
the	O
effects	O
of	O
varying	O
the	O
amount	O
of	O
backtranslated	O
data	O
as	O
a	O
proportion	O
of	O
the	O
total	O
training	O
corpus	O
,	O
including	O
training	O
only	O
on	O
the	O
synthetic	O
dataset	O
as	O
a	O
standalone	O
corpus	O
.	O
We	O
follow	O
some	O
of	O
the	O
related	O
experiments	O
conducted	O
by	O
Kocmi	O
and	O
Bojar	O
(	O
2019	O
)	O
on	O
Gujarati	O
-	O
English	O
(	O
another	O
low	O
-	O
resource	O
pair	O
)	O
with	O
a	O
few	O
exceptions	O
.	O
Besides	O
,	O
we	O
also	O
report	O
performance	O
when	O
pretraining	O
solely	O
on	O
the	O
synthetic	O
corpus	O
following	O
by	O
finetuning	O
on	O
either	O
original	O
or	O
mixed	O
data	O
.	O
While	O
not	O
quite	O
the	O
same	O
,	O
one	O
could	O
think	O
of	O
this	O
approach	O
as	O
having	O
some	O
similarities	O
with	O
transfer	B-TaskName
learning	I-TaskName
(	O
Zoph	O
et	O
al	O
,	O
2016	O
)	O
as	O
well	O
as	O
domain	B-TaskName
adaptation	I-TaskName
(	O
Luong	O
and	O
Manning	O
,	O
2015	O
;	O
Freitag	O
and	O
Al	O
-	O
Onaizan	O
,	O
2016	O
)	O
for	O
machine	B-TaskName
translation	I-TaskName
.	O
There	O
has	O
also	O
been	O
work	O
on	O
using	O
sampling	O
(	O
Edunov	O
et	O
al	O
,	O
2018	O
)	O
for	O
generating	O
backtranslations	O
,	O
but	O
we	O
stick	O
to	O
using	O
beam	O
search	O
in	O
this	O
work	O
.	O

Our	O
primary	O
system	O
is	O
a	O
Transformer	B-MethodName
base	O
model	O
,	O
trained	O
on	O
the	O
parallel	O
training	O
corpus	O
for	O
both	O
translation	O
directions	O
till	O
60	O
epochs	O
.	O
We	O
keep	O
most	O
of	O
the	O
hyperparameters	O
to	O
their	O
default	O
values	O
in	O
fairseq	O
.	O
More	O
precisely	O
,	O
we	O
chose	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
as	O
the	O
optimizer	B-HyperparameterName
and	O
Adam	B-MethodName
betas	O
were	O
set	O
to	O
0.9	O
and	O
0.98	O
,	O
respectively	O
.	O
The	O
maximum	O
number	O
of	O
tokens	O
in	O
each	O
batch	O
was	O
set	O
to	O
4096	O
.	O
Learning	B-HyperparameterName
rate	I-HyperparameterName
was	O
set	O
to	O
0.0005	O
,	O
with	O
an	O
inverse	O
squared	O
root	O
decay	O
schedule	O
and	O
4000	O
steps	O
of	O
warmup	O
updates	O
.	O
Label	B-MethodName
smoothing	I-MethodName
was	O
set	O
to	O
0.1	O
and	O
dropout	O
to	O
0.3	O
.	O
Label	O
-	O
smoothed	O
cross	O
-	O
entropy	O
was	O
used	O
as	O
the	O
training	O
criterion	O
.	O
We	O
trained	O
all	O
our	O
models	O
for	O
a	O
fixed	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
,	O
determined	O
separately	O
for	O
each	O
system	O
,	O
and	O
chose	O
the	O
last	O
checkpoint	O
for	O
reporting	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
scores	O
on	O
the	O
test	O
sets	O
.	O
All	O
training	O
was	O
done	O
using	O
a	O
single	O
NVIDIA	O
P100	O
GPU	O
.	O
Due	O
to	O
the	O
small	O
amount	O
of	O
parallel	O
training	O
data	O
,	O
each	O
epoch	O
of	O
training	O
took	O
about	O
90	O
seconds	O
on	O
average	O
for	O
the	O
primary	O
system	O
.	O

In	O
this	O
section	O
,	O
we	O
report	O
our	O
post	O
-	O
submission	O
work	O
on	O
using	O
monolingual	O
data	O
for	O
backtranslation	O
.	O
We	O
took	O
the	O
raw	O
monolingual	O
data	O
that	O
we	O
describe	O
in	O
Section	O
3.1	O
and	O
backtranslated	O
with	O
our	O
primary	O
submission	O
models	O
for	O
the	O
respective	O
translation	O
directions	O
,	O
i.e.	O
,	O
hsb	O
de	O
for	O
Upper	O
Sorbian	O
data	O
and	O
de	O
hsb	O
for	O
German	O
data	O
.	O
We	O
used	O
fairseq	O
-	O
generate	O
function	O
with	O
a	O
beam	O
size	O
of	O
5	O
for	O
this	O
purpose	O
.	O
Once	O
again	O
,	O
we	O
limited	O
the	O
number	O
of	O
subwords	O
in	O
each	O
sentence	O
to	O
250	O
.	O
Finally	O
,	O
we	O
took	O
all	O
sentence	O
pairs	O
for	O
backtranslated	O
Upper	O
Sorbian	O
corpus	O
and	O
the	O
first	O
two	O
million	O
sentence	O
pairs	O
for	O
the	O
German	O
corpus	O
.	O
Table	O
1	O
indicates	O
the	O
size	O
of	O
the	O
backtranslated	O
corpora	O
by	O
original	O
language	O
.	O
For	O
further	O
experiments	O
,	O
we	O
name	O
the	O
datasets	O
as	O
follows	O
:	O
auth	O
:	O
Processed	O
original	O
training	O
data	O
.	O
synth	O
:	O
Backtranslated	O
de	O
hsb	O
and	O
hsb	O
de	O
corpora	O
.	O
mixed	O
:	O
Augmented	O
training	O
data	O
obtained	O
by	O
mixing	O
auth	O
with	O
a	O
portion	O
of	O
synth	O
in	O
1:1	O
ratio	O
,	O
providing	O
a	O
total	O
of	O
116	O
,	O
778	O
sentence	O
pairs	O
.	O
We	O
define	O
the	O
following	O
systems	O
for	O
making	O
use	O
of	O
the	O
backtranslated	O
data	O
.	O
Note	O
that	O
the	O
first	O
system	O
only	O
differs	O
from	O
the	O
primary	O
system	O
in	O
the	O
number	O
of	O
training	O
epochs	O
completed	O
.	O
auth	O
-	O
from	O
-	O
scratch	O
:	O
This	O
system	O
has	O
the	O
same	O
settings	O
as	O
the	O
primary	O
system	O
.	O
It	O
was	O
trained	O
on	O
the	O
auth	O
corpus	O
till	O
80	O
epochs	O
(	O
as	O
opposed	O
to	O
60	O
for	O
primary	O
)	O
.	O
mixed	O
-	O
from	O
-	O
scratch	O
:	O
We	O
trained	O
models	O
on	O
mixed	O
data	O
from	O
scratch	O
for	O
40	O
epochs	O
.	O
5	O
synth	O
-	O
from	O
-	O
scratch	O
:	O
Models	O
were	O
trained	O
only	O
on	O
the	O
synth	O
datasets	O
.	O
To	O
adjust	O
for	O
the	O
difference	O
in	O
the	O
size	O
of	O
the	O
respective	O
backtranslated	O
corpora	O
,	O
we	O
trained	O
hsb	O
de	O
system	O
for	O
10	O
epochs	O
and	O
de	O
hsb	O
system	O
for	O
30	O
epochs	O
.	O
synth	O
-	O
auth	O
-	O
finetune	O
:	O
We	O
took	O
the	O
models	O
trained	O
via	O
the	O
previous	O
system	O
and	O
fine	O
-	O
tuned	O
them	O
on	O
auth	O
data	O
for	O
20	O
epochs	O
in	O
each	O
translation	O
direction	O
.	O
synth	O
-	O
mixed	O
-	O
finetune	O
:	O
Same	O
as	O
the	O
last	O
model	O
,	O
except	O
that	O
fine	O
-	O
tuning	O
was	O
done	O
on	O
mixed	O
data	O
.	O
Fine	O
-	O
tuning	O
was	O
carried	O
out	O
by	O
loading	O
pretrained	O
checkpoints	O
and	O
adding	O
extra	O
training	O
flags	O
in	O
reset	O
-	O
optimizer	B-HyperparameterName
and	O
reset	O
-	O
lr	O
-	O
scheduler	O
.	O

The	O
systems	O
were	O
evaluated	O
on	O
the	O
blind	O
test	O
set	O
(	O
newstest2020	O
)	O
using	O
automated	O
metrics	O
;	O
no	O
human	O
evaluation	O
was	O
done	O
.	O
Table	O
2	O
shows	O
cased	O
BLEU	B-MetricName
scores	O
for	O
various	O
systems	O
.	O
Our	O
primary	O
systems	O
achieved	O
a	O
BLEU	B-MetricName
score	I-MetricName
of	O
47.6	O
for	O
Upper	O
Sorbian	O
German	O
and	O
45.2	O
for	O
German	O
Upper	O
Sorbian	O
translation	O
.	O
We	O
achieved	O
an	O
improvement	O
of	O
0.3	O
and	O
0.4	O
BLEU	B-MetricName
points	O
,	O
respectively	O
,	O
by	O
training	O
further	O
till	O
80	O
epochs	O
in	O
each	O
direction	O
.	O
We	O
also	O
evaluated	O
a	O
third	O
system	O
,	O
synth	O
-	O
auth	O
-	O
finetune	O
,	O
as	O
described	O
in	O
Section	O
4	O
,	O
which	O
provided	O
a	O
jump	O
of	O
2.6	O
points	O
in	O
BLEU	B-MetricName
score	I-MetricName
over	O
the	O
primary	O
system	O
for	O
Upper	O
Sorbian	O
German	O
and	O
2.5	O
for	O
German	O
Upper	O
Sorbian	O
.	O
In	O
addition	O
to	O
evaluating	O
on	O
blind	O
test	O
sets	O
,	O
we	O
also	O
report	O
BLEU	B-MetricName
scores	O
on	O
the	O
development	O
test	O
set	O
in	O
the	O
same	O
table	O
.	O
Two	O
outcomes	O
are	O
worth	O
highlighting	O
:	O
Model	O
trained	O
only	O
on	O
synth	O
data	O
for	O
German	O
Upper	O
Sorbian	O
translation	O
matched	O
the	O
performance	O
of	O
a	O
similar	O
model	O
trained	O
on	O
the	O
authentic	O
bitext	O
.	O
Best	O
results	O
were	O
obtained	O
by	O
fine	O
-	O
tuning	O
a	O
model	O
trained	O
on	O
synth	O
data	O
with	O
either	O
auth	O
or	O
mixed	O
.	O
The	O
second	O
result	O
is	O
notable	O
since	O
the	O
regime	O
of	O
pretraining	O
followed	O
by	O
fine	O
-	O
tuning	O
improves	O
the	O
BLEU	B-MetricName
scores	O
by	O
up	O
to	O
4	O
points	O
on	O
this	O
test	O
set	O
when	O
compared	O
to	O
training	O
only	O
on	O
the	O
original	O
bitext	O
.	O
Moreover	O
,	O
while	O
the	O
model	O
trained	O
on	O
synth	O
was	O
not	O
able	O
to	O
match	O
the	O
performance	O
of	O
that	O
trained	O
on	O
auth	O
for	O
Upper	O
Sorbian	O
German	O
,	O
it	O
still	O
provides	O
the	O
same	O
benefits	O
as	O
German	O
Upper	O
Sorbian	O
model	O
when	O
fine	O
-	O
tuned	O
further	O
.	O
Looking	O
at	O
the	O
small	O
improvements	O
achieved	O
by	O
using	O
only	O
the	O
mixed	O
corpus	O
for	O
training	O
,	O
increasing	O
its	O
size	O
by	O
combining	O
upsampled	O
auth	O
data	O
with	O
more	O
synth	O
data	O
might	O
lead	O
to	O
even	O
further	O
jumps	O
in	O
the	O
BLEU	B-MetricName
scores	O
.	O

In	O
this	O
paper	O
,	O
we	O
described	O
our	O
Transformer	B-MethodName
model	O
for	O
supervised	O
machine	B-TaskName
translation	I-TaskName
for	O
Upper	O
Sorbian	O
-	O
German	O
language	O
pair	O
.	O
We	O
take	O
note	O
of	O
relatively	O
high	O
BLEU	B-MetricName
scores	O
achieved	O
by	O
our	O
primary	O
systems	O
(	O
and	O
those	O
of	O
other	O
participants	O
)	O
on	O
this	O
low	O
-	O
resource	O
language	O
pair	O
,	O
which	O
could	O
relate	O
to	O
the	O
high	O
quality	O
of	O
the	O
training	O
corpus	O
.	O
We	O
also	O
report	O
results	O
and	O
takeaways	O
from	O
several	O
experiments	O
with	O
backtranslated	O
data	O
completed	O
post	O
the	O
shared	O
task	O
.	O
A	O
key	O
result	O
is	O
matching	O
the	O
performance	O
of	O
a	O
system	O
trained	O
on	O
the	O
original	O
bitext	O
with	O
one	O
trained	O
on	O
a	O
limited	O
amount	O
of	O
synthetic	O
,	O
backtranslated	O
data	O
.	O
Domain	O
mismatch	O
and	O
a	O
difference	O
in	O
the	O
quality	O
of	O
monolingual	O
corpus	O
might	O
have	O
prevented	O
the	O
system	O
from	O
achieving	O
a	O
similar	O
result	O
in	O
the	O
other	O
direction	O
.	O
We	O
notice	O
big	O
improvements	O
in	O
performance	O
over	O
the	O
primary	O
systems	O
by	O
following	O
a	O
"	O
pretraining	O
then	O
fine	O
-	O
tuning	O
"	O
regime	O
.	O
An	O
interesting	O
future	O
work	O
would	O
be	O
to	O
measure	O
the	O
applicability	O
of	O
this	O
approach	O
to	O
other	O
lowresource	O
language	O
pairs	O
.	O
Additional	O
systems	O
could	O
be	O
added	O
as	O
well	O
.	O
For	O
instance	O
,	O
models	O
trained	O
on	O
mixed	O
data	O
and	O
fine	O
-	O
tuned	O
on	O
auth	O
data	O
might	O
provide	O
a	O
meaningful	O
comparison	O
.	O
Prior	O
work	O
(	O
Ding	O
et	O
al	O
,	O
2019	O
)	O
has	O
shown	O
that	O
the	O
number	O
of	O
BPE	B-MethodName
merge	O
operations	O
has	O
a	O
significant	O
effect	O
on	O
the	O
performance	O
of	O
NMT	O
systems	O
.	O
This	O
work	O
was	O
pointed	O
out	O
during	O
the	O
review	O
process	O
and	O
should	O
be	O
an	O
avenue	O
for	O
further	O
improvement	O
of	O
the	O
model	O
performance	O
.	O

Recently	O
,	O
neural	O
twitter	O
sentiment	O
classification	O
has	O
become	O
one	O
of	O
state	O
-	O
of	O
-	O
thearts	O
,	O
which	O
requires	O
less	O
feature	B-TaskName
engineering	I-TaskName
work	O
compared	O
with	O
traditional	O
methods	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
simple	O
and	O
effective	O
ensemble	O
method	O
to	O
further	O
boost	O
the	O
performances	O
of	O
neural	O
models	O
.	O
We	O
collect	O
several	O
word	O
embedding	O
sets	O
which	O
are	O
publicly	O
released	O
(	O
often	O
are	O
learned	O
on	O
different	O
corpus	O
)	O
or	O
constructed	O
by	O
running	O
Skip	O
-	O
gram	O
on	O
released	O
largescale	O
corpus	O
.	O
We	O
make	O
an	O
assumption	O
that	O
different	O
word	B-TaskName
embeddings	I-TaskName
cover	O
different	O
words	O
and	O
encode	O
different	O
semantic	O
knowledge	O
,	O
thus	O
using	O
them	O
together	O
can	O
improve	O
the	O
generalizations	O
and	O
performances	O
of	O
neural	O
models	O
.	O
In	O
the	O
SemEval	O
2017	O
,	O
our	O
method	O
ranks	O
1st	O
in	O
Accuracy	B-MetricName
,	O
5th	O
in	O
AverageR.	O
Meanwhile	O
,	O
the	O
additional	O
comparisons	O
demonstrate	O
the	O
superiority	O
of	O
our	O
model	O
over	O
these	O
ones	O
based	O
on	O
only	O
one	O
word	O
embedding	O
set	O
.	O
We	O
release	O
our	O
code	O
1	O
for	O
the	O
method	O
replicability	O
.	O

We	O
use	O
4	O
embedding	O
sets	O
which	O
are	O
described	O
in	O
Table	O
1	O
.	O
Meanwhile	O
,	O
we	O
crawl	O
and	O
merge	O
all	O
annotated	O
datasets	O
of	O
previous	O
SemEvals	O
,	O
and	O
split	O
them	O
into	O
training	O
,	O
development	O
,	O
and	O
testing	O
sets	O
with	O
ratio	O
8:1:1	O
,	O
which	O
are	O
shown	O
in	O
Table	O
2	O
together	O
with	O
testing	O
set	O
of	O
SemEval	O
2017	O
.	O
From	O
the	O
table	O
,	O
we	O
can	O
see	O
that	O
testing	O
set	O
of	O
SemEval	O
2017	O
has	O
big	O
differences	O
on	O
the	O
category	O
ratio	O
(	O
negative	O
:	O
neutral	O
:	O
positive	O
)	O
,	O
compared	O
with	O
the	O
previous	O
SemEval	O
datasets	O
.	O
For	O
the	O
model	O
settings	O
,	O
all	O
RCNN	O
models	O
have	O
same	O
configurations	O
but	O
different	O
word	O
embedding	O
sets	O
.	O
We	O
set	O
dimensions	O
of	O
hidden	O
vectors	O
to	O
250	O
and	O
depths	O
d	O
to	O
2	O
.	O
To	O
avoid	O
model	O
over	O
-	O
fitting	O
,	O
we	O
use	O
dropout	O
and	O
regularization	O
as	O
follows	O
:	O
(	O
1	O
)	O
the	O
regularization	O
parameter	O
is	O
set	O
to	O
1e	O
-	O
5	O
;	O
(	O
2	O
)	O
the	O
dropout	O
rate	O
is	O
set	O
to	O
0.3	O
,	O
which	O
is	O
applied	O
in	O
the	O
final	O
text	O
representation	O
.	O
All	O
parameters	O
are	O
learned	O
by	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.001	O
.	O
Note	O
that	O
,	O
all	O
word	O
embedding	O
sets	O
are	O
fixed	O
when	O
training	O
.	O
All	O
models	O
are	O
tuned	O
by	O
the	O
development	O
set	O
in	O
Training	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
report	O
the	O
results	O
on	O
datasets	O
of	O
previous	O
SemEvals	O
,	O
which	O
are	O
described	O
in	O
Table	O
3	O
.	O
Then	O
,	O
we	O
report	O
the	O
performances	O
of	O
our	O
method	O
on	O
SemEval	O
2017	O
in	O
Table	O
4	O
.	O
From	O
the	O
Table	O
3	O
,	O
we	O
observe	O
that	O
gloveT	O
performs	O
worst	O
though	O
it	O
is	O
trained	O
on	O
in	O
-	O
domain	O
twitter	O
dataset	O
and	O
the	O
word2vecY	O
performs	O
best	O
though	O
it	O
is	O
derived	O
from	O
yelp	O
reviews	O
.	O
As	O
far	O
as	O
we	O
known	O
,	O
Yelp	O
data	O
is	O
constructed	O
by	O
carefully	O
filtering	O
and	O
is	O
high	O
-	O
quality	O
.	O
Thus	O
,	O
we	O
can	O
include	O
that	O
the	O
quality	O
of	O
corpus	O
is	O
also	O
important	O
as	O
the	O
size	O
of	O
corpus	O
and	O
domain	O
in	O
twitter	O
sentiment	O
classification	O
.	O
Additionally	O
,	O
we	O
can	O
infer	O
that	O
word2vecGN	O
outperforms	O
others	O
in	O
recall	O
of	O
negative	O
category	O
,	O
word2vecY	O
performs	O
best	O
in	O
recall	O
of	O
neutral	O
category	O
,	O
and	O
gloveT	O
is	O
best	O
in	O
recall	O
of	O
positive	O
category	O
.	O
Different	O
embedding	O
sets	O
propose	O
different	O
characteristics	O
.	O
Additionally	O
,	O
the	O
ensemble	O
method	O
obtains	O
a	O
significant	O
improvement	O
of	O
4	O
%	O
.	O
In	O
the	O
Table	O
4	O
,	O
we	O
compare	O
our	O
method	O
with	O
best	O
and	O
median	O
systems	O
in	O
SemEval	O
2017	O
,	O
and	O
report	O
the	O
results	O
of	O
individual	O
embedding	O
sets	O
.	O
Our	O
method	O
outperforms	O
other	O
systems	O
in	O
accuracy	B-MetricName
,	O
but	O
performs	O
worse	O
in	O
R	O
Average	O
,	O
especially	O
in	O
R	O
Negative	O
.	O
Compared	O
with	O
the	O
median	O
system	O
,	O
our	O
method	O
has	O
improvements	O
of	O
about	O
5	O
%	O
in	O
both	O
accuracy	B-MetricName
and	O
R	O
Average	O
.	O
Different	O
from	O
the	O
results	O
in	O
worse	O
among	O
these	O
embedding	O
sets	O
,	O
while	O
the	O
gloveT	O
obtains	O
best	O
performances	O
.	O
Additionally	O
,	O
we	O
can	O
observe	O
that	O
gloveT	O
performs	O
best	O
both	O
in	O
R	O
Negative	O
and	O
R	O
Positive	O
,	O
and	O
word2vecY	O
performs	O
best	O
in	O
R	O
Neutral	O
.	O
Compared	O
with	O
the	O
embedding	O
baselines	O
,	O
our	O
ensemble	O
method	O
obtains	O
improvements	O
of	O
2.7	O
%	O
and	O
1.5	O
%	O
in	O
accuracy	B-MetricName
and	O
R	O
Average	O
respectively	O
,	O
which	O
demonstrates	O
the	O
effectiveness	O
of	O
the	O
proposed	O
method	O
.	O

To	O
understand	O
the	O
meaning	O
of	O
a	O
sentence	O
or	O
a	O
text	O
,	O
it	O
is	O
essential	O
to	O
analyze	O
relations	O
between	O
a	O
predicate	O
and	O
its	O
arguments	O
.	O
Such	O
analysis	O
is	O
called	O
semantic	B-TaskName
role	I-TaskName
labeling	I-TaskName
(	O
SRL	O
)	O
or	O
predicate	O
-	O
argument	O
structure	O
(	O
PAS	O
)	O
analysis	O
.	O
For	O
English	O
,	O
the	O
accuracy	B-MetricName
of	O
SRL	O
has	O
reached	O
approximately	O
80	O
%	O
-	O
90	O
%	O
(	O
Ouchi	O
et	O
al	O
,	O
2018	O
;	O
Strubell	O
et	O
al	O
,	O
2018	O
;	O
Tan	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
there	O
are	O
many	O
omissions	O
of	O
arguments	O
in	O
Japanese	O
,	O
and	O
the	O
accuracy	B-MetricName
of	O
Japanese	O
PAS	O
analysis	O
on	O
omitted	O
arguments	O
is	O
still	O
around	O
50	O
%	O
-	O
60	O
%	O
(	O
Shibata	O
et	O
al	O
,	O
2016	O
;	O
Shibata	O
and	O
Kurohashi	O
,	O
2018	O
;	O
Kurita	O
et	O
al	O
,	O
2018	O
;	O
Ouchi	O
et	O
al	O
,	O
2017	O
)	O
.	O
A	O
reason	O
for	O
such	O
low	O
accuracy	B-MetricName
is	O
the	O
shortage	O
of	O
gold	O
datasets	O
and	O
knowledge	O
about	O
PAS	O
analysis	O
,	O
which	O
require	O
a	O
prohibitive	O
cost	O
of	O
creation	O
(	O
Iida	O
et	O
al	O
,	O
2007	O
;	O
Kawahara	O
et	O
al	O
,	O
2002	O
)	O
.	O
From	O
the	O
viewpoint	O
of	O
text	O
understanding	O
,	O
machine	O
comprehension	O
(	O
MC	O
)	O
has	O
been	O
actively	O
studied	O
in	O
recent	O
years	O
.	O
In	O
MC	O
studies	O
,	O
QA	O
datasets	O
consisting	O
of	O
triplets	O
of	O
a	O
document	O
,	O
a	O
question	O
and	O
*	O
The	O
current	O
affiliation	O
is	O
Yahoo	O
Japan	O
Corporation	O
.	O
its	O
answer	O
are	O
constructed	O
,	O
and	O
an	O
MC	O
model	O
is	O
trained	O
using	O
these	O
datasets	O
(	O
e.g.	O
,	O
Rajpurkar	O
et	O
al	O
(	O
2016	O
)	O
and	O
Trischler	O
et	O
al	O
(	O
2017	O
)	O
)	O
.	O
MC	O
has	O
made	O
remarkable	O
progress	O
in	O
the	O
last	O
couple	O
of	O
years	O
,	O
and	O
MC	O
models	O
have	O
even	O
exceeded	O
human	O
accuracy	B-MetricName
in	O
some	O
datasets	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
However	O
,	O
MC	O
accuracy	B-MetricName
is	O
not	O
necessarily	O
high	O
for	O
documents	O
that	O
contain	O
anaphoric	O
phenomena	O
and	O
those	O
that	O
need	O
external	O
knowledge	O
or	O
inference	O
(	O
Mihaylov	O
et	O
al	O
,	O
2018	O
;	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
Japanese	O
PAS	O
analysis	O
method	O
based	O
on	O
the	O
MC	O
framework	O
for	O
a	O
specific	O
domain	O
.	O
In	O
particular	O
,	O
we	O
focus	O
on	O
a	O
challenging	O
task	O
of	O
finding	O
an	O
antecedent	O
of	O
a	O
zero	O
pronoun	O
within	O
PAS	O
analysis	O
.	O
We	O
construct	O
a	O
widecoverage	O
QA	O
dataset	O
for	O
PAS	O
analysis	O
(	O
PAS	O
-	O
QA	O
)	O
in	O
the	O
domain	O
and	O
feed	O
it	O
to	O
an	O
MC	O
model	O
to	O
perform	O
PAS	O
analysis	O
.	O
We	O
also	O
construct	O
a	O
QA	O
dataset	O
for	O
reading	B-TaskName
comprehension	I-TaskName
(	O
RC	O
-	O
QA	O
)	O
in	O
the	O
same	O
domain	O
and	O
jointly	O
use	O
the	O
two	O
datasets	O
in	O
the	O
MC	O
model	O
to	O
improve	O
PAS	O
analysis	O
.	O
We	O
consider	O
the	O
domain	O
of	O
blogs	O
on	O
driving	O
because	O
of	O
the	O
following	O
two	O
reasons	O
.	O
Firstly	O
,	O
we	O
can	O
construct	O
high	O
-	O
quality	O
QA	O
datasets	O
in	O
a	O
short	O
time	O
using	O
crowdsourcing	O
.	O
Crowdworkers	O
can	O
interpret	O
driving	O
blog	O
articles	O
based	O
on	O
the	O
traffic	O
commonsense	O
shared	O
by	O
the	O
society	O
.	O
Secondly	O
,	O
if	O
computers	O
can	O
understand	O
driving	O
situations	O
correctly	O
by	O
extracting	O
driving	O
behavior	O
from	O
blogs	O
,	O
it	O
is	O
possible	O
to	O
predict	O
danger	O
and	O
warn	O
drivers	O
to	O
achieve	O
safer	O
transportation	O
.	O
Our	O
contributions	O
are	O
summarized	O
as	O
follows	O
.	O
FitzGerald	O
et	O
al	O
(	O
2018	O
)	O
and	O
constructed	O
QA	B-DatasetName
-	I-DatasetName
SRL	I-DatasetName
Bank	I-DatasetName
2.0	I-DatasetName
and	O
QAMRs	O
using	O
crowdsourcing	O
,	O
respectively	O
.	O
They	O
asked	O
crowdworkers	O
to	O
generate	O
question	O
-	O
answer	O
pairs	O
that	O
represent	O
a	O
PAS	O
.	O
These	O
datasets	O
are	O
similar	O
to	O
our	O
PAS	O
-	O
QA	O
dataset	O
,	O
but	O
different	O
in	O
that	O
we	O
focus	O
on	O
omitted	O
arguments	O
and	O
automatically	O
generate	O
questions	O
(	O
see	O
Section	O
3.1	O
)	O
.	O
Many	O
RC	O
-	O
QA	O
datasets	O
have	O
been	O
constructed	O
in	O
recent	O
years	O
.	O
For	O
example	O
,	O
Rajpurkar	O
et	O
al	O
(	O
2016	O
)	O
constructed	O
SQuAD	B-DatasetName
1.1	O
,	O
which	O
contains	O
100	O
K	O
crowdsourced	O
questions	O
and	O
answer	O
spans	O
in	O
a	O
Wikipedia	O
article	O
.	O
Rajpurkar	O
et	O
al	O
(	O
2018	O
)	O
updated	O
SQuAD	B-DatasetName
1.1	O
to	O
2.0	O
by	O
adding	O
unanswerable	O
questions	O
.	O
Some	O
RC	O
-	O
QA	O
datasets	O
have	O
been	O
built	O
in	O
a	O
specific	O
domain	O
(	O
Welbl	O
et	O
al	O
,	O
2017	O
;	O
Suster	O
and	O
Daelemans	O
,	O
2018	O
;	O
Pampari	O
et	O
al	O
,	O
2018	O
)	O
.	O

Many	O
MC	O
models	O
based	O
on	O
neural	O
networks	O
have	O
been	O
proposed	O
to	O
solve	O
RC	O
-	O
QA	O
datasets	O
.	O
For	O
example	O
,	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
an	O
MC	O
model	O
using	O
a	O
language	O
representation	O
model	O
,	O
BERT	B-MethodName
,	O
which	O
achieved	O
a	O
high	O
-	O
ranked	O
accuracy	B-MetricName
on	O
the	O
SQuAD	B-DatasetName
1.1	O
leaderboard	O
as	O
of	O
September	O
30	O
,	O
2019	O
.	O
As	O
a	O
previous	O
study	O
of	O
transfer	B-TaskName
learning	I-TaskName
of	O
MC	O
models	O
to	O
other	O
tasks	O
,	O
Pan	O
et	O
al	O
(	O
2018	O
)	O
pre	O
-	O
trained	O
an	O
MC	O
model	O
using	O
an	O
RC	O
-	O
QA	O
dataset	O
and	O
transfered	O
the	O
pre	O
-	O
trained	O
knowledge	O
to	O
sequence	O
-	O
tosequence	O
models	O
.	O
They	O
used	O
SQuAD	B-DatasetName
1.1	O
as	O
the	O
RC	O
-	O
QA	O
dataset	O
and	O
experimented	O
on	O
translation	O
and	O
summarization	B-TaskName
.	O
While	O
they	O
used	O
different	O
models	O
for	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
,	O
we	O
use	O
the	O
same	O
MC	O
model	O
by	O
constructing	O
PAS	O
-	O
QA	O
and	O
RC	O
-	O
QA	O
datasets	O
in	O
the	O
same	O
QA	O
form	O
.	O

We	O
conduct	O
PAS	O
analysis	O
experiments	O
of	O
our	O
MCsingle	O
/	O
merged	O
/	O
stepwise	O
methods	O
using	O
the	O
PAS	O
-	O
QA	O
and	O
RC	O
-	O
QA	O
datasets	O
.	O
We	O
also	O
compare	O
our	O
methods	O
with	O
the	O
neural	O
network	O
-	O
based	O
PAS	O
analysis	O
model	O
(	O
Shibata	O
and	O
Kurohashi	O
,	O
2018	O
)	O
(	O
hereafter	O
,	O
NN	O
-	O
PAS	O
)	O
,	O
which	O
achieved	O
the	O
state	O
-	O
of	O
-	O
theart	O
accuracy	B-MetricName
on	O
Japanese	O
PAS	O
analysis	O
.	O

We	O
adopt	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
as	O
an	O
MC	O
model	O
.	O
We	O
split	O
the	O
triplets	O
in	O
the	O
PAS	O
-	O
QA	O
dataset	O
as	O
shown	O
in	O
Table	O
4	O
.	O
All	O
sentences	O
in	O
these	O
datasets	O
are	O
preprocessed	O
using	O
the	O
Japanese	O
morphological	O
analyzer	O
,	O
JUMAN++	O
3	O
.	O
We	O
trained	O
a	O
Japanese	O
pre	O
-	O
trained	O
BERT	B-MethodName
model	O
using	O
Japanese	O
Wikipedia	O
,	O
which	O
consists	O
of	O
approximately	O
18	O
million	O
sentences	O
.	O
The	O
input	O
sentences	O
were	O
segmented	O
into	O
words	O
by	O
JUMAN++	O
,	O
and	O
words	O
were	O
broken	O
into	O
subwords	O
by	O
applying	O
BPE	B-MethodName
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
parameters	O
of	O
BERT	B-MethodName
are	O
the	O
same	O
as	O
English	O
BERT	B-MethodName
BASE	B-MethodName
.	O
The	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
for	O
the	O
pre	O
-	O
training	O
was	O
30	O
.	O
The	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baseline	O
PAS	O
analyzer	O
,	O
NN	O
-	O
PAS	O
,	O
was	O
trained	O
using	O
the	O
existing	O
PAS	O
dataset	O
,	O
KWDLC	O
4	O
(	O
Kyoto	O
University	O
Web	O
Document	O
Leads	O
Corpus	O
)	O
,	O
as	O
described	O
in	O
Shibata	O
and	O
Kurohashi	O
(	O
2018	O
)	O
.	O
We	O
also	O
trained	O
an	O
NN	O
-	O
PAS	O
model	O
using	O
the	O
PAS	O
-	O
QA	O
dataset	O
in	O
addition	O
to	O
KWDLC	O
(	O
hereafter	O
,	O
NN	O
-	O
PAS	O
′	O
)	O
.	O
For	O
this	O
training	O
,	O
the	O
PAS	O
-	O
QA	O
dataset	O
was	O
converted	O
to	O
the	O
same	O
format	O
as	O
KWDLC	O
,	O
where	O
questions	O
are	O
deleted	O
,	O
and	O
only	O
answers	O
are	O
used	O
.	O
The	O
PAS	O
-	O
QA	O
test	O
data	O
is	O
used	O
to	O
compare	O
the	O
baseline	O
methods	O
with	O
the	O
proposed	O
methods	O
.	O
As	O
an	O
evaluation	O
measure	O
,	O
EM	B-MetricName
(	O
Exact	B-MetricName
Match	I-MetricName
)	O
is	O
used	O
for	O
all	O
the	O
MC	O
models	O
.	O
EM	B-MetricName
is	O
defined	O
as	O
(	O
the	O
number	O
of	O
questions	O
in	O
which	O
the	O
system	O
answer	O
matches	O
the	O
gold	O
answer	O
in	O
the	O
dataset	O
)	O
/	O
(	O
the	O
number	O
of	O
questions	O
in	O
the	O
entire	O
dataset	O
)	O
.	O
For	O
each	O
experimental	O
condition	O
,	O
training	O
and	O
testing	O
were	O
conducted	O
five	O
times	O
,	O
and	O
the	O
average	O
scores	O
were	O
calculated	O
.	O
3	O
and	O
4	O
,	O
only	O
the	O
outputs	O
of	O
MC	O
-	O
stepwise	O
were	O
correct	O
.	O
We	O
found	O
some	O
cases	O
that	O
MC	O
-	O
stepwise	O
successfully	O
captured	O
knowledge	O
in	O
the	O
driving	O
domain	O
.	O
In	O
the	O
example	O
shown	O
in	O
Figure	O
4	O
,	O
the	O
correspondence	O
between	O
"	O
"	O
(	O
climb	O
up	O
the	O
slope	O
)	O
and	O
"	O
"	O
(	O
going	O
up	O
the	O
slope	O
)	O
can	O
be	O
recognized	O
.	O
MC	O
-	O
merged	O
's	O
answer	O
"	O
"	O
(	O
the	O
hill	O
road	O
)	O
,	O
which	O
has	O
a	O
coreference	O
relation	O
with	O
"	O

Multimodal	O
pre	O
-	O
training	O
has	O
propelled	O
great	O
advancement	O
in	O
vision	O
-	O
and	O
-	O
language	O
research	O
.	O
These	O
large	O
-	O
scale	O
pre	O
-	O
trained	O
models	O
,	O
although	O
successful	O
,	O
fatefully	O
suffer	O
from	O
slow	O
inference	O
speed	O
due	O
to	O
enormous	O
computation	O
cost	O
mainly	O
from	O
cross	O
-	O
modal	O
attention	O
in	O
Transformer	B-MethodName
architecture	O
.	O
When	O
applied	O
to	O
reallife	O
applications	O
,	O
such	O
latency	O
and	O
computation	O
demand	O
severely	O
deter	O
the	O
practical	O
use	O
of	O
pre	O
-	O
trained	O
models	O
.	O
In	O
this	O
paper	O
,	O
we	O
study	O
Image	O
-	O
text	O
retrieval	O
(	O
ITR	O
)	O
,	O
the	O
most	O
mature	O
scenario	O
of	O
V+L	O
application	O
,	O
which	O
has	O
been	O
widely	O
studied	O
even	O
prior	O
to	O
the	O
emergence	O
of	O
recent	O
pre	O
-	O
trained	O
models	O
.	O
We	O
propose	O
a	O
simple	O
yet	O
highly	O
effective	O
approach	O
,	O
LightningDOT	O
that	O
accelerates	O
the	O
inference	O
time	O
of	O
ITR	O
by	O
thousands	O
of	O
times	O
,	O
without	O
sacrificing	O
accuracy	B-MetricName
.	O
LightningDOT	O
removes	O
the	O
time	O
-	O
consuming	O
cross	O
-	O
modal	O
attention	O
by	O
pre	O
-	O
training	O
on	O
three	O
novel	O
learning	O
objectives	O
,	O
extracting	O
feature	O
indexes	O
offline	O
,	O
and	O
employing	O
instant	O
dot	O
-	O
product	O
matching	O
with	O
further	O
re	O
-	O
ranking	O
,	O
which	O
significantly	O
speeds	O
up	O
retrieval	O
process	O
.	O
In	O
fact	O
,	O
Light	O
-	O
ningDOT	O
achieves	O
new	O
state	O
of	O
the	O
art	O
across	O
multiple	O
ITR	O
benchmarks	O
such	O
as	O
Flickr30k	B-DatasetName
,	O
COCO	B-DatasetName
and	O
Multi30	O
K	O
,	O
outperforming	O
existing	O
pre	O
-	O
trained	O
models	O
that	O
consume	O
1000×	O
magnitude	O
of	O
computational	O
hours	O
.	O
1	O

V+L	O
Pre	O
-	O
training	O
Inspired	B-DatasetName
by	O
the	O
success	O
of	O
Transformer	B-MethodName
-	O
based	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
language	O
model	O
pre	O
-	O
training	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Yang	O
et	O
al	O
,	O
2019	O
;	O
Raffel	O
et	O
al	O
,	O
2020	O
;	O
Lan	O
et	O
al	O
,	O
2020	O
;	O
Clark	O
et	O
al	O
,	O
2020	O
)	O
,	O
vision	O
-	O
andlanguage	O
pre	O
-	O
training	O
(	O
Huang	O
et	O
al	O
,	O
2020b	O
;	O
Su	O
et	O
al	O
,	O
2020	O
;	O
Li	O
et	O
al	O
,	O
2020bLi	O
et	O
al	O
,	O
,	O
2019a	O
has	O
become	O
the	O
prevailing	O
paradigm	O
in	O
learning	O
multimodal	O
representations	O
,	O
with	O
strong	O
results	O
on	O
tasks	O
such	O
as	O
image	O
-	O
text	O
retrieval	O
(	O
Kiros	O
et	O
al	O
,	O
2014	O
)	O
,	O
visual	B-DatasetName
question	I-DatasetName
answering	I-DatasetName
(	O
Antol	O
et	O
al	O
,	O
2015	O
)	O
and	O
referring	B-TaskName
expression	I-TaskName
comprehension	I-TaskName
(	O
Yu	O
et	O
al	O
,	O
2016	O
)	O
.	O
Exemplary	O
works	O
include	O
two	O
-	O
stream	O
(	O
Tan	O
and	O
Bansal	O
,	O
2019	O
;	O
and	O
single	O
-	O
stream	O
models	O
Li	O
et	O
al	O
,	O
2020a	O
;	O
.	O
Multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
and	O
adversarial	O
training	O
are	O
also	O
explored	O
.	O
This	O
family	O
of	O
pre	O
-	O
training	O
methods	O
aims	O
for	O
general	O
-	O
purpose	O
V+L	O
without	O
computation	O
cost	O
consideration	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
our	O
work	O
is	O
the	O
first	O
known	O
effort	O
on	O
pre	O
-	O
training	O
visualsemantic	O
embedding	O
that	O
enables	O
low	O
-	O
latency	O
realtime	O
cross	B-TaskName
-	I-TaskName
modal	I-TaskName
retrieval	I-TaskName
.	O
Ours	O
is	O
concurrent	O
work	O
with	O
CLIP	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2021	O
)	O
.	O
Image	O
-	O
Text	O
Retrieval	O
Early	O
cross	O
-	O
modal	O
embedding	O
works	O
(	O
Kiros	O
et	O
al	O
,	O
2014	O
;	O
Faghri	O
et	O
al	O
,	O
2017	O
)	O
focus	O
on	O
using	O
a	O
twostream	O
model	O
to	O
learn	O
a	O
unified	O
visual	O
-	O
semantic	O
embedding	O
,	O
with	O
progressive	O
improvement	O
on	O
two	O
popular	O
benchmarks	O
:	O
Flickr30	O
K	O
(	O
Plummer	O
et	O
al	O
,	O
2015	O
)	O
and	O
COCO	B-DatasetName
(	O
Chen	O
et	O
al	O
,	O
2015	O
)	O
.	O
Later	O
methods	O
with	O
cross	O
-	O
attention	O
(	O
Lee	O
et	O
al	O
,	O
2018Wang	O
et	O
al	O
,	O
2019	O
;	O
become	O
more	O
popular	O
,	O
with	O
significant	O
performance	O
gain	O
.	O
Pre	O
-	O
trained	O
V+L	O
models	O
also	O
fall	O
into	O
this	O
category	O
.	O
By	O
exploiting	O
large	O
-	O
scale	O
image	O
-	O
text	O
datasets	O
,	O
pretrained	O
V+L	O
models	O
further	O
push	O
the	O
performance	O
on	O
Flickr30	O
K	O
and	O
COCO	B-DatasetName
.	O
Although	O
achieving	O
high	O
recall	O
,	O
cross	O
-	O
attention	O
requires	O
excessive	O
computation	O
cost	O
during	O
inference	O
that	O
can	O
not	O
be	O
overlooked	O
.	O
2	O
In	O
this	O
work	O
,	O
inspired	O
by	O
dense	O
retrieval	O
in	O
text	O
retrieval	O
domain	O
(	O
Guu	O
et	O
al	O
,	O
2020	O
;	O
Karpukhin	O
et	O
al	O
,	O
2020	O
;	O
Xiong	O
et	O
al	O
,	O
2020	O
;	O
Mao	O
et	O
al	O
,	O
2020	O
;	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
,	O
we	O
propose	O
a	O
more	O
efficient	O
attention	O
-	O
less	O
framework	O
.	O
With	O
pre	O
-	O
training	O
,	O
our	O
model	O
achieves	O
better	O
performance	O
while	O
being	O
significantly	O
faster	O
than	O
cross	O
-	O
modal	O
attention	O
methods	O
.	O
Note	O
that	O
the	O
proposed	O
approach	O
is	O
orthogonal	O
to	O
model	B-TaskName
compression	I-TaskName
techniques	O
that	O
reduce	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
/	O
parameters	O
(	O
Sun	O
et	O
al	O
,	O
2019	O
;	O
Jiao	O
et	O
al	O
,	O
2020	O
)	O
,	O
since	O
we	O
do	O
not	O
reduce	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
from	O
the	O
UNITER	B-MethodName
baseline	O
.	O
These	O
two	O
approaches	O
can	O
be	O
combined	O
to	O
further	O
boost	O
the	O
speed	O
,	O
which	O
is	O
an	O
interesting	O
future	O
work	O
direction	O
.	O

We	O
denote	O
the	O
Transformer	B-MethodName
-	O
based	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
image	O
encoder	O
and	O
language	O
encoder	O
by	O
f	O
θ	B-HyperparameterName
V	O
and	O
f	O
θ	B-HyperparameterName
L	O
,	O
respectively	O
(	O
θ	B-HyperparameterName
V	O
,	O
θ	B-HyperparameterName
L	O
are	O
learnable	O
parameters	O
)	O
.	O
Given	O
a	O
dataset	O
of	O
paired	O
image	O
and	O
text	O
{	O
(	O
i	O
,	O
t	O
)	O
}	O
,	O
we	O
first	O
extract	O
region	O
features	O
v	O
=	O
{	O
v	O
0	B-DatasetName
,	O
v	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
N	O
}	O
(	O
v	O
j	O
R	O
dv	O
,	O
N	O
is	O
the	O
number	O
of	O
regions	O
)	O
for	O
image	O
i	O
,	O
along	O
with	O
bounding	O
box	O
positions	O
of	O
regions	O
via	O
a	O
pre	O
-	O
trained	O
Faster	O
-	O
RCNN	O
(	O
Ren	O
et	O
al	O
,	O
2015	O
;	O
Anderson	O
et	O
al	O
,	O
2018	O
)	O
.	O
3	O
The	O
image	O
encoder	O
f	O
θ	B-HyperparameterName
V	O
encodes	O
this	O
sequence	O
of	O
image	O
regions	O
into	O
a	O
d	O
-	O
dimensional	O
space	O
f	O
θ	B-HyperparameterName
V	O
(	O
v	O
)	O
=	O
h	O
=	O
{	O
h	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
h	O
N	O
}	O
(	O
h	O
j	O
R	O
d	O
)	O
.	O
The	O
corresponding	O
text	O
t	O
is	O
tokenized	O
into	O
sub	O
-	O
word	O
units	O
and	O
projected	O
into	O
high	O
-	O
dimensional	O
feature	O
vectors	O
w	O
=	O
{	O
w	O
0	B-DatasetName
,	O
w	O
1	O
,	O
...	O
,	O
w	O
T	O
}	O
(	O
w	O
j	O
R	O
dw	O
,	O
T	O
is	O
the	O
number	O
of	O
tokens	O
)	O
following	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
.	O
4	O
Similarly	O
,	O
the	O
text	O
encoding	O
process	O
can	O
be	O
written	O
as	O
f	O
θ	B-HyperparameterName
L	O
(	O
w	O
)	O
=	O
z	O
=	O
{	O
z	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
z	O
T	O
}	O
(	O
z	O
j	O
R	O
d	O
)	O
.	O
We	O
regard	O
the	O
output	O
[	O
CLS	O
]	O
embedding	O
h	O
0	B-DatasetName
as	O
global	O
image	O
representation	O
,	O
and	O
z	O
0	B-DatasetName
as	O
global	O
text	O
representation	O
.	O
Following	O
sections	O
discuss	O
how	O
to	O
jointly	O
train	O
these	O
two	O
encoders	O
to	O
learn	O
strong	O
visual	O
-	O
semantic	O
embeddings	O
,	O
through	O
three	O
pre	O
-	O
training	O
objectives	O
.	O

L	O
MLM	B-DatasetName
(	O
t	O
)	O
=	O
−	O
log	O
P	O
θ	B-HyperparameterName
L	O
(	O
w	O
m	O
|	O
w	O
\m	O
)	O
=	O
−	O
1	O
M	O
M	O
k=1	O
log	O
P	O
θ	B-HyperparameterName
mlm	B-DatasetName
(	O
w	O
m	O
k	O
|	O
z	O
m	O
k	O
)	O
,	O
(	O
1	O
)	O
where	O
θ	B-HyperparameterName
mlm	B-DatasetName
is	O
the	O
additional	O
parameters	O
introduced	O
to	O
map	O
hidden	O
states	O
z	O
to	O
word	O
probabilities	O
.	O
Under	O
the	O
V+L	O
setting	O
,	O
the	O
textual	O
input	O
is	O
usually	O
highly	O
correlated	O
with	O
the	O
image	O
.	O
To	O
leverage	O
this	O
cross	O
-	O
modal	O
relation	O
,	O
we	O
propose	O
visualembedding	O
fused	O
MLM	B-DatasetName
(	O
VMLM	O
)	O
,	O
in	O
which	O
the	O
paired	O
image	O
i	O
is	O
considered	O
as	O
additional	O
input	O
when	O
training	O
the	O
model	O
to	O
reconstruct	O
masked	O
tokens	O
in	O
sentence	O
t.	O
The	O
loss	B-MetricName
function	O
of	O
VMLM	O
can	O
be	O
formulated	O
as	O
:	O
L	O
VMLM	O
(	O
t	O
,	O
i	O
)	O
=	O
−	O
log	O
P	O
θ	B-HyperparameterName
(	O
w	O
m	O
|	O
w	O
\m	O
,	O
i	O
)	O
=	O
−	O
1	O
M	O
M	O
k=1	O
log	O
P	O
θ	B-HyperparameterName
mlm	B-DatasetName
(	O
w	O
m	O
k	O
|	O
z	O
m	O
k	O
+	O
h	O
0	B-DatasetName
)	O
,	O
(	O
2	O
)	O
where	O
θ	B-HyperparameterName
=	O
{	O
θ	B-HyperparameterName
V	O
,	O
θ	B-HyperparameterName
L	O
}	O
and	O
the	O
word	O
probabilities	O
P	O
θ	B-HyperparameterName
are	O
conditioned	O
on	O
the	O
corresponding	O
image	O
i	O
via	O
the	O
global	O
image	O
representation	O
h	O
0	B-DatasetName
.	O
Although	O
VMLM	O
takes	O
a	O
similar	O
mathematical	O
form	O
to	O
the	O
MLM	B-DatasetName
task	O
proposed	O
in	O
UNITER	B-MethodName
,	O
they	O
differ	O
in	O
two	O
main	O
aspects	O
:	O
1	O
)	O
LightningDOT	O
uses	O
two	O
separate	O
encoders	O
(	O
h	O
0	B-DatasetName
is	O
computed	O
by	O
f	O
θ	B-HyperparameterName
V	O
)	O
;	O
and	O
2	O
)	O
visual	O
dependency	O
is	O
explicitly	O
injected	O
to	O
text	O
representations	O
(	O
z	O
m	O
k	O
+	O
h	O
0	B-DatasetName
)	O
,	O
instead	O
of	O
implicitly	O
learned	O
through	O
cross	O
-	O
modal	O
attention	O
.	O
Semantic	O
-	O
embedding	O
Fused	O
Masked	O
Region	O
Modeling	O
(	O
SMRM	O
)	O
Recent	O
works	O
on	O
V+L	O
pretraining	O
Tan	O
and	O
Bansal	O
,	O
2019	O
)	O
have	O
shown	O
that	O
mask	O
-	O
then	O
-	O
reconstruct	O
pre	O
-	O
training	O
on	O
image	O
regions	O
also	O
helps	O
image+text	O
embedding	O
learning	O
.	O
Similar	O
to	O
MLM	B-DatasetName
,	O
Masked	O
Region	O
Modeling	O
(	O
MRM	O
)	O
is	O
supervised	O
by	O
:	O
L	O
MRM	O
(	O
i	O
)	O
=	O
D	O
θmrm	O
(	O
v	O
m	O
,	O
f	O
θ	B-HyperparameterName
V	O
(	O
v	O
\m	O
)	O
)	O
=	O
1	O
M	O
M	O
k=1	O
D	O
θmrm	O
(	O
v	O
m	O
k	O
,	O
h	O
m	O
k	O
)	O
,	O
(	O
3	O
)	O
where	O
D	O
can	O
be	O
any	O
differentiable	O
distance	O
function	O
.	O
Among	O
the	O
variants	O
of	O
MRM	O
,	O
we	O
consider	O
Masked	O
Region	O
Feature	O
Regression	O
(	O
MRFR	O
)	O
with	O
L2	O
distance	O
and	O
Masked	O
Region	O
Classification	B-TaskName
with	O
KL	O
-	O
Divergence	O
(	O
MRC	O
-	O
kl	O
)	O
,	O
due	O
to	O
their	O
proven	O
success	O
in	O
learning	O
V+L	O
representations	O
.	O
6	O
In	O
MRFR	O
,	O
the	O
L	O
2	O
distance	O
between	O
two	O
feature	O
vectors	O
x	O
and	O
y	O
is	O
defined	O
as	O
:	O
D	O
θ	B-HyperparameterName
fr	O
(	O
x	O
,	O
y	O
)	O
=	O
k	O
x	O
k	O
−	O
g	O
θ	B-HyperparameterName
fr	O
(	O
y	O
k	O
)	O
2	O
2	O
,	O
where	O
2	O
denotes	O
L	O
2	O
-	O
norm	O
,	O
and	O
g	O
θ	B-HyperparameterName
fr	O
(	O
)	O
is	O
a	O
learnable	O
Multi	O
-	O
layer	O
Perceptron	O
(	O
MLP	B-DatasetName
)	O
with	O
parameters	O
θ	B-HyperparameterName
fr	O
.	O
The	O
KL	O
-	O
divergence	O
D	O
KL	O
in	O
MRC	O
-	O
kl	O
measures	O
distance	O
between	O
two	O
probability	O
distributions	O
:	O
D	O
θmrc	O
(	O
x	O
,	O
y	O
)	O
=	O
k	O
D	O
KL	O
(	O
c	O
(	O
x	O
k	O
)	O
|	O
|	O
g	O
θmrc	O
(	O
y	O
k	O
)	O
)	O
,	O
where	O
θ	B-HyperparameterName
mrc	O
is	O
the	O
parameters	O
of	O
a	O
trainable	O
MLP	B-DatasetName
that	O
maps	O
feature	O
vector	O
x	O
k	O
to	O
the	O
object	O
class	O
distribution	O
c	O
(	O
x	O
k	O
)	O
predicted	O
by	O
Faster	B-MethodName
R	I-MethodName
-	I-MethodName
CNN	I-MethodName
.	O
To	O
incorporate	O
language	O
information	O
encoded	O
in	O
the	O
paired	O
text	O
,	O
we	O
extend	O
MRM	O
to	O
Semanticembedding	O
fused	O
MRM	O
(	O
SMRM	O
)	O
,	O
where	O
the	O
global	O
text	O
representation	O
z	O
0	B-DatasetName
is	O
exploited	O
when	O
reconstructing	O
masked	O
regions	O
.	O
L	O
SMRM	O
(	O
i	O
,	O
t	O
)	O
=	O
D	O
θmrm	O
(	O
v	O
m	O
,	O
f	O
θ	B-HyperparameterName
V	O
(	O
v	O
\m	O
)	O
,	O
t	O
)	O
=	O
1	O
M	O
M	O
k=1	O
D	O
θmrm	O
(	O
v	O
m	O
k	O
,	O
h	O
m	O
k	O
+	O
z	O
0	B-DatasetName
)	O
.	O
(	O
4	O
)	O
The	O
specific	O
variants	O
SMRFR	O
and	O
SMRC	O
-	O
kl	O
can	O
be	O
derived	O
using	O
the	O
corresponding	O
distance	O
function	O
,	O
which	O
is	O
omitted	O
for	O
simplicity	O
.	O
Note	O
that	O
both	O
the	O
cross	O
-	O
modal	O
fusion	O
introduced	O
in	O
Eqn	O
.	O
(	O
2	O
)	O
and	O
Eqn	O
.	O
(	O
4	O
)	O
uses	O
simple	O
addition	O
without	O
introducing	O
extra	O
parameters	O
from	O
their	O
uni	O
-	O
modal	O
counterpart	O
.	O
Moreover	O
,	O
the	O
extra	O
parameters	O
θ	B-HyperparameterName
mlm	B-DatasetName
and	O
θ	B-HyperparameterName
mrm	O
is	O
not	O
needed	O
at	O
downstream	O
inference	O
so	O
will	O
not	O
slow	O
down	O
the	O
retrieval	O
.	O
Cross	B-TaskName
-	I-TaskName
modal	I-TaskName
Retrieval	I-TaskName
Objective	O
(	O
CMR	O
)	O
Beyond	O
image	O
or	O
text	O
focused	O
reconstructive	O
objectives	O
,	O
we	O
also	O
propose	O
a	O
new	O
pre	O
-	O
training	O
task	O
,	O
Cross	B-TaskName
-	I-TaskName
modal	I-TaskName
Retrieval	I-TaskName
(	O
CMR	O
)	O
,	O
to	O
leverage	O
the	O
paired	O
information	O
between	O
image	O
and	O
text	O
.	O
With	O
this	O
learning	O
objective	O
,	O
the	O
model	O
is	O
optimized	O
to	O
promote	O
high	O
similarity	O
score	O
for	O
a	O
matched	O
imagesentence	O
pair	O
(	O
i	O
,	O
t	O
)	O
and	O
vice	O
versa	O
.	O
The	O
similarity	O
score	O
between	O
query	O
t	O
and	O
image	O
i	O
is	O
defined	O
as	O
:	O
S	O
(	O
t	O
,	O
i	O
)	O
=	O
z	O
0	B-DatasetName
,	O
h	O
0	B-DatasetName
,	O
(	O
5	O
)	O
where	O
,	O
denotes	O
the	O
inner	O
product	O
between	O
two	O
vectors	O
,	O
and	O
h	O
0	B-DatasetName
and	O
z	O
0	B-DatasetName
are	O
the	O
output	O
[	O
CLS	O
]	O
embeddings	O
from	O
image	O
encoder	O
f	O
θ	B-HyperparameterName
V	O
and	O
language	O
encoder	O
f	O
θ	B-HyperparameterName
L	O
,	O
respectively	O
.	O
In	O
order	O
to	O
capture	O
both	O
image	O
-	O
retrieval	O
and	O
textretrieval	O
supervision	O
signals	O
in	O
a	O
single	O
forwardbackward	O
pass	O
,	O
we	O
propose	O
a	O
bi	O
-	O
directional	O
variant	O
of	O
contrastive	O
loss	B-MetricName
.	O
Given	O
any	O
matched	O
image	O
-	O
text	O
pair	O
(	O
i	O
,	O
t	O
)	O
,	O
we	O
treat	O
text	O
t	O
as	O
the	O
query	O
,	O
sample	O
n	O
−	O
1	O
negative	O
images	O
{	O
i	O
2	O
,	O
i	O
3	O
,	O
.	O
.	O
.	O
,	O
i	O
n	O
}	O
,	O
and	O
then	O
compute	O
the	O
objective	O
function	O
as	O
:	O
L	O
(	O
t	O
)	O
IR	O
=	O
−	O
log	O
e	O
S	O
(	O
t	O
,	O
i	O
1	O
)	O
n	O
k=1	O
e	O
S	O
(	O
t	O
,	O
i	O
k	O
)	O
,	O
where	O
t	O
1	O
:	O
=	O
t.	O
Similarly	O
,	O
we	O
take	O
image	O
i	O
as	O
query	O
(	O
i	O
1	O
:	O
=	O
i	O
)	O
,	O
sample	O
n	O
−	O
1	O
negative	O
text	O
,	O
and	O
compute	O
:	O
L	O
(	O
i	O
)	O
TR	O
=	O
−	O
log	O
e	O
S	O
(	O
i	O
,	O
t	O
1	O
)	O
n	O
k=1	O
e	O
S	O
(	O
i	O
,	O
t	O
k	O
)	O
to	O
optimize	O
for	O
text	O
retrieval	O
.	O
Following	O
Henderson	O
et	O
al	O
(	O
2017	O
)	O
;	O
Gillick	O
et	O
al	O
(	O
2019	O
)	O
;	O
Karpukhin	O
et	O
al	O
(	O
2020	O
)	O
,	O
we	O
use	O
in	O
-	O
batch	O
negatives	O
to	O
avoid	O
the	O
actual	O
sampling	O
of	O
a	O
negative	O
image	O
or	O
text	O
:	O
given	O
a	O
batch	O
of	O
n	O
positive	O
image	O
-	O
text	O
pairs	O
B	O
=	O
{	O
(	O
i	O
1	O
,	O
t	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
i	O
n	O
,	O
t	O
n	O
)	O
}	O
,	O
we	O
use	O
all	O
other	O
images	O
from	O
within	O
the	O
batch	O
as	O
negatives	O
(	O
{	O
i	O
j	O
}	O
,	O
where	O
j	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
and	O
j	O
=	O
k	O
)	O
for	O
every	O
positive	O
pair	O
(	O
i	O
k	O
,	O
t	O
k	O
)	O
,	O
and	O
vice	O
versa	O
for	O
negative	O
text	O
.	O
The	O
final	O
CMR	O
loss	B-MetricName
for	O
batch	O
B	O
is	O
:	O
L	O
CMR	O
(	O
B	O
)	O
=	O
1	O
2n	O
n	O
k=1	O
L	O
(	O
i	O
k	O
)	O
TR	O
+	O
L	O
(	O
t	O
k	O
)	O
IR	O
.	O
(	O
6	O
)	O
An	O
illustration	O
of	O
L	O
CMR	O
is	O
presented	O
in	O
Figure	O
3	O
.	O
7	O
Through	O
joint	O
pre	O
-	O
training	O
with	O
CMR	O
,	O
VMLM	O
and	O
SMRM	O
,	O
the	O
visual	O
-	O
semantic	O
embeddings	O
learned	O
from	O
image	O
encoder	O
and	O
language	O
encoder	O
can	O
be	O
readily	O
applied	O
to	O
downstream	O
tasks	O
.	O
During	O
finetuning	O
stage	O
,	O
we	O
directly	O
adopt	O
CMR	O
loss	B-MetricName
to	O
supervise	O
the	O
training	O
process	O
.	O
7	O
The	O
whole	O
similarity	O
matrix	O
can	O
be	O
computed	O
efficiently	O
with	O
one	O
batched	O
matrix	O
multiplication	O
call	O
.	O
This	O
operation	O
can	O
take	O
advantage	O
of	O
GPU	O
hardware	O
with	O
Tensor	O
Cores	O
for	O
faster	O
training	O
.	O

For	O
simplicity	O
,	O
we	O
take	O
text	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
image	I-TaskName
retrieval	I-TaskName
as	O
an	O
example	O
to	O
introduce	O
the	O
real	O
-	O
time	O
inference	O
pipeline	O
(	O
Figure	O
2	O
Offline	O
Feature	O
Extraction	O
Image	B-TaskName
retrieval	I-TaskName
task	O
requires	O
the	O
model	O
to	O
rank	O
every	O
image	O
i	O
in	O
an	O
image	O
database	O
I	O
based	O
on	O
its	O
similarity	O
to	O
a	O
text	O
query	O
t.	O
In	O
LightningDOT	O
,	O
we	O
first	O
apply	O
the	O
image	O
encoder	O
f	O
θ	B-HyperparameterName
V	O
to	O
all	O
images	O
in	O
I	O
,	O
and	O
cache	O
the	O
resulting	O
global	O
image	O
representations	O
{	O
h	O
(	O
Johnson	O
et	O
al	O
,	O
2019	O
)	O
in	O
memory	O
for	O
later	O
use	O
.	O
Note	O
that	O
the	O
entire	O
image	O
-	O
to	O
-	O
index	O
process	O
,	O
including	O
Faster	O
-	O
RCNN	O
feature	O
extraction	O
and	O
Transformer	B-MethodName
encoding	O
,	O
can	O
all	O
be	O
conducted	O
offline	O
.	O
Therefore	O
,	O
for	O
every	O
new	O
query	O
t	O
at	O
real	O
time	O
,	O
the	O
cached	O
index	O
can	O
be	O
reused	O
for	O
maximum	O
inference	O
time	O
saving	O
.	O
(	O
i	O
)	O
0	B-DatasetName
R	O
d	O
|	O
i	O
I	O
}	O
into	O
an	O
index	O
Online	O
Retrieval	O
During	O
inference	O
,	O
given	O
a	O
text	O
query	O
t	O
,	O
we	O
encode	O
it	O
with	O
the	O
language	O
encoder	O
θ	B-HyperparameterName
L	O
,	O
and	O
then	O
compute	O
its	O
similarity	O
score	O
to	O
the	O
embedding	O
of	O
every	O
image	O
in	O
I	O
(	O
stored	O
in	O
memory	O
index	O
)	O
via	O
Eqn	O
(	O
5	O
)	O
.	O
Finally	O
,	O
the	O
images	O
will	O
be	O
ranked	O
by	O
their	O
similarity	O
scores	O
,	O
from	O
the	O
highest	O
to	O
lowest	O
.	O
In	O
practice	O
,	O
people	O
are	O
more	O
interested	O
in	O
top	O
-	O
K	O
retrieval	O
,	O
with	O
a	O
list	O
of	O
K	O
images	O
I	O
t	O
satisfying	O
:	O
I	O
t	O
:	O
=	O
{	O
i	O
m	O
k	O
}	O
K	O
k=1	O
,	O
where	O
S	O
(	O
t	O
,	O
i	O
m	O
1	O
)	O
≥	O
S	O
(	O
t	O
,	O
i	O
m	O
2	O
)	O
≥	O
≥	O
S	O
(	O
t	O
,	O
i	O
m	O
K	O
)	O
and	O
S	O
(	O
t	O
,	O
i	O
m	O
K	O
)	O
≥	O
S	O
(	O
t	O
,	O
i	O
)	O
∀i	O
(	O
I	O
\	O
I	O
t	O
)	O
.	O
(	O
7	O
)	O
This	O
optimization	O
problem	O
has	O
been	O
well	O
studied	O
,	O
and	O
we	O
use	O
FAISS	O
(	O
Johnson	O
et	O
al	O
,	O
2019	O
)	O
to	O
solve	O
it	O
in	O
our	O
implementation	O
.	O
It	O
is	O
worth	O
noting	O
that	O
in	O
order	O
to	O
apply	O
fast	O
search	O
,	O
the	O
similarity	O
function	O
has	O
to	O
be	O
decomposable	O
.	O
Therefore	O
,	O
we	O
choose	O
the	O
simple	O
dot	O
product	O
as	O
S	O
instead	O
of	O
a	O
more	O
complicated	O
neural	O
network	O
function	O
.	O
Similarly	O
,	O
for	O
text	O
retrieval	O
,	O
the	O
same	O
architecture	O
can	O
be	O
applied	O
by	O
simply	O
pre	O
-	O
computing	O
the	O
embedding	O
for	O
all	O
sentences	O
and	O
using	O
an	O
image	O
as	O
query	O
instead	O
.	O
Re	O
-	O
ranking	O
To	O
further	O
improve	O
retrieval	O
accuracy	B-MetricName
,	O
we	O
propose	O
a	O
two	O
-	O
stage	O
approach	O
by	O
adopting	O
an	O
optional	O
re	O
-	O
ranking	O
model	O
.	O
In	O
the	O
first	O
stage	O
,	O
we	O
use	O
LightningDOT	O
to	O
retrieve	O
top	O
-	O
M	O
images	O
(	O
or	O
texts	O
)	O
,	O
where	O
M	O
is	O
an	O
integer	O
much	O
smaller	O
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
AR	O
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
AR	O

We	O
compare	O
the	O
proposed	O
approach	O
with	O
state	O
-	O
ofthe	O
-	O
art	O
methods	O
(	O
with	O
and	O
without	O
pre	O
-	O
training	O
)	O
and	O
report	O
the	O
results	O
in	O
Table	O
1	O
.	O
Without	O
crossattention	O
,	O
our	O
method	O
outperforms	O
non	O
-	O
pre	O
-	O
training	O
approaches	O
by	O
large	O
margins	O
on	O
all	O
metrics	O
.	O
Specifically	O
,	O
our	O
model	O
improves	O
over	O
CAAN	O
(	O
SOTA	O
method	O
with	O
cross	O
-	O
attention	O
)	O
by	O
3.3	O
%	O
(	O
73.5	O
vs.	O
70.2	O
)	O
on	O
COCO	B-DatasetName
and	O
9.5	O
%	O
(	O
89.3	O
vs.	O
79.8	O
)	O
on	O
Flickr30	O
K	O
in	O
terms	O
of	O
AR	O
.	O
When	O
compared	O
with	O
methods	O
without	O
cross	O
-	O
attention	O
(	O
VSE++	O
(	O
Faghri	O
et	O
al	O
,	O
2017	O
)	O
and	O
SCO	O
(	O
Huang	O
et	O
al	O
,	O
2018	O
)	O
)	O
,	O
LightningDOT	O
achieves	O
nearly	O
Model	O
COCO	B-DatasetName
Full	O
(	O
123	O
K	O
Images	O
)	O
Flickr30	O
K	O
Full	O
(	O
31	O
K	O
Images	O
)	O
Text	O
Retrieval	O
Image	B-TaskName
Retrieval	I-TaskName
Text	O
Retrieval	O
Image	B-TaskName
Retrieval	I-TaskName
R@5	B-MetricName
R@10	B-MetricName
R@20	O
R@5	B-MetricName
R@10	B-MetricName
R@20	O
AR	O
R@5	B-MetricName
R@10	B-MetricName
R@20	O
R@5	B-MetricName
R@10	B-MetricName
R@20	O
AR	O
(	O
Lee	O
et	O
al	O
,	O
2018	O
)	O
.	O
LightningDOT	O
with	O
/	O
without	O
UNITER	B-MethodName
-	O
base	O
re	O
-	O
ranker	O
is	O
significantly	O
faster	O
.	O
20	O
-	O
point	O
gain	O
on	O
AR	O
.	O
Although	O
LightningDOT	O
achieves	O
slightly	O
lower	O
AR	O
than	O
UNITER	B-MethodName
(	O
pretraining	O
method	O
with	O
cross	O
-	O
attention	O
)	O
,	O
with	O
3.5/1.1	O
points	O
drop	O
on	O
Flickr30K	O
/	O
COCO	O
,	B-DatasetName
it	O
is	O
600/1900	O
×	O
faster	O
than	O
UNITER	O
during	B-MethodName
inference	O
time	O
.	O
We	O
further	O
apply	O
second	O
-	O
stage	O
re	O
-	O
ranking	O
,	O
and	O
use	O
UNITER	O
to	B-MethodName
score	O
top	O
-	O
M	O
retrieved	O
image	O
-	O
text	O
pairs	O
from	O
LightningDOT	O
to	O
obtain	O
the	O
final	O
top	O
-	O
K	O
ranked	O
lists	O
.	O
With	O
re	O
-	O
ranking	O
,	O
LightningDOT	O
achieves	O
an	O
instant	O
performance	O
lift	O
,	O
surpassing	O
UNITER	O
on	B-MethodName
both	O
benchmarks	O
,	O
while	O
still	O
46	O
-	O
95	O
times	O
faster	O
than	O
UNITER	O
.	B-MethodName
With	O
an	O
even	O
stronger	O
re	O
-	O
ranker	O
OSCAR	O
,	B-MethodName
LightningDOT	O
achieves	O
similar	O
results	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
COCO	O
.	B-DatasetName

To	O
demonstrate	O
the	O
efficiency	O
of	O
LightningDOT	O
,	O
we	O
use	O
UNITER	B-MethodName
-	O
base	O
as	O
baseline	O
to	O
compare	O
inference	O
speed	O
.	O
We	O
also	O
compare	O
with	O
a	O
more	O
lightweight	O
cross	O
-	O
attention	O
method	O
SCAN	B-DatasetName
(	O
Lee	O
et	O
al	O
,	O
2018	O
)	O
,	O
which	O
uses	O
GRU	B-MethodName
(	O
Chung	O
et	O
al	O
,	O
2014	O
)	O
instead	O
of	O
a	O
12	O
-	O
layer	O
Transformer	B-MethodName
.	O
All	O
methods	O
are	O
tested	O
on	O
a	O
single	O
TITAN	B-DatasetName
RTX	O
GPU	O
,	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
400	O
.	O
As	O
shown	O
in	O
Table	O
3	O
,	O
SCAN	B-DatasetName
is	O
∼1.9×	O
faster	O
than	O
UNITER	B-MethodName
-	O
base	O
across	O
both	O
benchmarks	O
,	O
as	O
the	O
computational	O
cost	O
of	O
GRU	B-MethodName
is	O
much	O
cheaper	O
than	O
that	O
of	O
Transformer	B-MethodName
(	O
performance	O
drop	O
is	O
significant	O
though	O
)	O
.	O
However	O
,	O
the	O
speedup	O
from	O
SCAN	B-DatasetName
is	O
limited	O
,	O
as	O
it	O
computes	O
cross	O
-	O
attention	O
between	O
each	O
query	O
and	O
all	O
images	O
.	O
On	O
the	O
other	O
hand	O
,	O
LightningDOT	O
is	O
639×	O
faster	O
than	O
UNITER	B-MethodName
on	O
Flickr30K.	O
When	O
tested	O
with	O
5	O
times	O
more	O
i	O
m	O
-	O
ages	O
in	O
COCO	B-DatasetName
,	O
the	O
speedup	O
from	O
LightningDOT	O
is	O
1927×.	O
Even	O
with	O
re	O
-	O
ranking	O
,	O
LightningDOT	O
is	O
still	O
much	O
more	O
efficient	O
than	O
UNITER	B-MethodName
-	O
base	O
(	O
46×	O
faster	O
on	O
Flickr30	O
K	O
and	O
95×	O
faster	O
on	O
COCO	B-DatasetName
)	O
.	O
To	O
mimic	O
a	O
real	O
-	O
life	O
scenario	O
for	O
image	B-TaskName
retrieval	I-TaskName
,	O
where	O
the	O
candidate	O
pool	O
contains	O
hundreds	O
of	O
thousands	O
of	O
images	O
,	O
we	O
combine	O
all	O
images	O
from	O
training	O
,	O
validation	O
and	O
test	O
set	O
to	O
form	O
a	O
larger	O
candidate	O
pool	O
.	O
Note	O
that	O
models	O
are	O
still	O
trained	O
on	O
the	O
training	O
set	O
.	O
Although	O
the	O
number	O
of	O
text	O
queries	O
remain	O
the	O
same	O
,	O
the	O
number	O
of	O
candidate	O
images	O
scales	O
up	O
by	O
>	O
20×	O
,	O
where	O
cross	O
-	O
attention	O
methods	O
immediately	O
become	O
impractical	O
.	O
We	O
refer	O
this	O
setting	O
on	O
both	O
benchmarks	O
as	O
Flickr30k	B-DatasetName
-	O
full	O
(	O
31k	O
)	O
and	O
COCO	B-DatasetName
-	O
full	O
(	O
123k	O
)	O
.	O
Our	O
algorithm	O
is	O
6	O
,	O
591×	O
faster	O
on	O
Flickr30k	B-DatasetName
-	O
full	O
and	O
23	O
,	O
869×	O
faster	O
on	O
COCO	B-DatasetName
-	O
full	O
,	O
which	O
clearly	O
shows	O
the	O
advantage	O
of	O
LightningDOT	O
and	O
its	O
potential	O
in	O
real	O
-	O
world	O
applications	O
.	O
With	O
re	O
-	O
ranking	O
,	O
LightningDOT	O
is	O
still	O
more	O
than	O
1	O
,	O
000×	O
and	O
2	O
,	O
000×	O
faster	O
on	O
Flickr30kfull	O
and	O
COCO	B-DatasetName
-	O
full	O
,	O
respectively	O
.	O
In	O
general	O
,	O
for	O
other	O
re	O
-	O
rankers	O
such	O
as	O
OSCAR	B-MethodName
,	O
our	O
algorithm	O
can	O
approximately	O
speed	O
up	O
inference	O
by	O
N	O
images	O
/M	O
times	O
,	O
where	O
N	O
images	O
is	O
the	O
number	O
of	O
candidate	O
images	O
,	O
and	O
M	O
is	O
number	O
of	O
re	O
-	O
ranked	O
images	O
from	O
top	O
-	O
M	O
retrieved	O
results	O
by	O
LightningDOT	O
.	O
Similarly	O
,	O
we	O
construct	O
a	O
full	O
setting	O
for	O
text	O
retrieval	O
by	O
combining	O
all	O
text	O
queries	O
from	O
training	O
,	O
validation	O
and	O
test	O
set	O
.	O
Results	O
are	O
summarized	O
in	O
Table	O
2	O
.	O
Considering	O
the	O
size	O
of	O
candidate	O
pool	O
has	O
become	O
more	O
than	O
20×	O
larger	O
,	O
we	O
adopt	O
recall	O
at	O
top	O
5	O
,	O
10	O
,	O
50	O
as	O
evaluation	O
metrics	O
.	O
Our	O
method	O
achieves	O
reasonably	O
good	O
performance	O
,	O
with	O
AR	O
of	O
44.4	O
on	O
COCO	B-DatasetName
and	O
70.2	O
on	O
Flickr30K.	O
Re	O
-	O
ranking	O
further	O
lifts	O
AR	O
to	O
56.4	O
and	O
76.2	O
.	O
Results	O
from	O
UNITER	B-MethodName
or	O
SCAN	B-DatasetName
are	O
not	O
included	O
as	O
the	O
computation	O
of	O
pairwise	O
scores	O
is	O
extremely	O
expensive	O
,	O
given	O
the	O
excessive	O
amount	O
of	O
retrieval	O
candidates	O
.	O
While	O
LightningDOT	O
only	O
takes	O
minutes	O
to	O
evaluate	O
,	O
UNITER	B-MethodName
-	O
base	O
is	O
estimated	O
to	O
take	O
about	O
28	O
days	O
9	O
to	O
evaluate	O
under	O
the	O
full	O
setting	O
for	O
both	O
Text	O
Retrieval	O
Image	B-TaskName
Retrieval	I-TaskName
Method	O
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
AR	O

Image	B-TaskName
Retrieval	I-TaskName
LightningDOT	O
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
R@1	B-MetricName
R@5	B-MetricName
R@10	B-MetricName
AR	O
image	B-TaskName
retrieval	I-TaskName
and	O
text	O
retrieval	O
.	O
In	O
addition	O
,	O
We	O
compare	O
all	O
models	O
with	O
the	O
same	O
setting	O
:	O
cache	O
as	O
much	O
as	O
possible	O
for	O
fastest	O
speed	O
,	O
where	O
our	O
model	O
outperforms	O
others	O
in	O
both	O
speed	O
and	O
space	O
on	O
image	B-TaskName
retrieval	I-TaskName
.	O
The	O
proposed	O
algorithm	O
maps	O
each	O
image	O
to	O
a	O
768	O
-	O
dimensional	O
vector	O
,	O
which	O
only	O
consumes	O
about	O
300Mb	O
storage	O
space	O
for	O
the	O
whole	O
COCO	B-DatasetName
dataset	O
.	O
For	O
crossattention	O
models	O
such	O
as	O
SCAN	B-DatasetName
,	O
UNITER	B-MethodName
or	O
OS	O
-	O
CAR	B-DatasetName
,	O
they	O
also	O
need	O
to	O
cache	O
image	O
features	O
,	O
which	O
typically	O
requires	O
to	O
save	O
a	O
36	O
x	O
2048	B-DatasetName
dimensional	O
vector	O
per	O
image	O
,	O
and	O
it	O
consumes	O
about	O
28	O
GB	O
storage	O
space	O
for	O
COCO	B-DatasetName
dataset	O
.	O

We	O
further	O
report	O
results	O
on	O
multilingual	O
image	O
-	O
text	O
retrieval	O
tasks	O
.	O
Specially	O
,	O
we	O
evaluate	O
Lightning	O
-	O
DOT	O
under	O
the	O
translate	O
-	O
test	O
setting	O
,	O
which	O
is	O
to	O
translate	O
the	O
test	O
captions	O
in	O
other	O
languages	O
to	O
English	O
by	O
leveraging	O
Machine	B-TaskName
Translation	I-TaskName
(	O
MT	O
)	O
tool	O
.	O
10	O
Note	O
that	O
our	O
method	O
is	O
only	O
trained	O
on	O
English	O
captions	O
,	O
without	O
exploiting	O
the	O
original	O
or	O
translated	O
captions	O
from	O
multilingual	O
benchmarks	O
.	O
We	O
consider	O
two	O
benchmarks	O
:	O
Multi30	O
K	O
(	O
Elliott	O
et	O
al	O
,	O
2016	O
(	O
Elliott	O
et	O
al	O
,	O
,	O
2017Barrault	O
et	O
al	O
,	O
2018	O
)	O
with	O
captions	O
in	O
German	O
,	O
French	O
and	O
Czech	O
;	O
and	O
COCO	B-DatasetName
Japanese	O
(	O
Yoshikawa	O
et	O
al	O
,	O
2017	O
)	O
and	O
Chinese	O
(	O
Li	O
et	O
al	O
,	O
2019b	O
)	O
.	O
Average	O
Recall	B-MetricName
(	O
AR	O
)	O
is	O
used	O
as	O
the	O
evaluation	O
metric	O
.	O
Meta	O
-	O
Ave	O
,	O
the	O
average	O
of	O
AR	O
over	O
different	O
languages	O
across	O
two	O
benchmarks	O
,	O
is	O
used	O
as	O
a	O
global	O
metric	O
.	O
More	O
details	O
on	O
multilingual	O
ITR	O
benchmarks	O
are	O
included	O
in	O
Appendix	O
.	O
We	O
compare	O
LightningDOT	O
against	O
3	O
task	O
-	O
specific	O
methods	O
:	O
S	O
-	O
LIWE	O
(	O
Wehrmann	O
et	O
al	O
,	O
2019	O
)	O
,	O
MULE	O
and	O
SMALR	O
(	O
Burns	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
all	O
exploit	O
captions	O
in	O
different	O
languages	O
to	O
learn	O
multilingual	O
or	O
language	O
-	O
agnostic	O
word	B-TaskName
embeddings	I-TaskName
.	O
We	O
also	O
compare	O
with	O
a	O
pre	O
-	O
trained	O
model	O
M	O
3	O
P	O
(	O
Huang	O
et	O
al	O
,	O
2020a	O
)	O
,	O
which	O
is	O
alternatively	O
pre	O
-	O
trained	O
with	O
image	O
-	O
caption	O
pairs	O
labeled	O
in	O
English	O
and	O
cross	O
-	O
lingual	O
corpus	O
in	O
100	O
different	O
languages	O
.	O
Note	O
that	O
all	O
methods	O
discussed	O
above	O
are	O
trained	O
/	O
finetuned	O
on	O
captions	O
in	O
different	O
languages	O
.	O
For	O
fair	O
comparison	O
,	O
we	O
report	O
performance	O
of	O
UNITER	B-MethodName
under	O
the	O
same	O
translate	O
-	O
test	O
setting	O
,	O
which	O
is	O
finetuned	O
with	O
English	O
captions	O
only	O
and	O
tested	O
on	O
translated	O
captions	O
.	O
Table	O
6	O
shows	O
similar	O
trends	O
of	O
performance	O
improvements	O
as	O
on	O
English	O
benchmarks	O
.	O
Compared	O
to	O
both	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
task	O
-	O
specific	O
methods	O
and	O
pre	O
-	O
trained	O
models	O
,	O
LightningDOT	O
under	O
translatetest	O
setting	O
achieves	O
new	O
state	O
of	O
the	O
art	O
on	O
most	O
languages	O
and	O
establishes	O
a	O
strong	O
baseline	O
for	O
future	O
study	O
on	O
these	O
multilingual	O
benchmarks	O
.	O

To	O
further	O
facilitate	O
the	O
reproductivity	O
of	O
our	O
proposed	O
method	O
,	O
we	O
include	O
more	O
details	O
about	O
the	O
choice	O
of	O
model	O
size	O
and	O
hyper	O
-	O
parameters	O
for	O
both	O
pre	O
-	O
training	O
and	O
fine	O
-	O
tuning	O
.	O
The	O
model	O
dimensions	O
are	O
set	O
to	O
(	O
L=12	O
,	O
H=768	O
,	O
A=12	O
)	O
for	O
both	O
image	O
encoder	O
and	O
language	O
encoder	O
,	O
where	O
L	O
is	O
the	O
number	O
of	O
stacked	O
Transformer	B-MethodName
blocks	O
;	O
H	O
stands	O
for	O
hidden	O
activation	O
dimension	O
,	O
and	O
A	O
is	O
the	O
number	O
of	O
attention	O
heads	O
.	O
The	O
total	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
in	O
LightningDOT	O
is	O
220M.	O
Pre	O
-	O
training	O
and	O
finetuning	O
learn	O
the	O
parameters	O
of	O
both	O
encoders	O
.	O
During	O
inference	O
,	O
with	O
offline	O
representation	O
caching	O
,	O
only	O
the	O
forwarding	O
pass	O
with	O
one	O
encoder	O
from	O
the	O
query	O
modality	O
will	O
be	O
performed	O
online	O
.	O
For	O
both	O
pre	O
-	O
training	O
and	O
finetuning	O
,	O
AdamW	B-MethodName
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2019	O
)	O
is	O
used	O
to	O
optimize	O
the	O
model	O
training	O
,	O
with	O
β	B-HyperparameterName
1	O
=	O
0.9	O
,	O
β	B-HyperparameterName
2	O
=	O
0.98	O
.	O
We	O
adopt	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
warmup	O
strategy	O
,	O
where	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
linearly	O
increased	O
during	O
the	O
first	O
10	O
%	O
of	O
training	O
steps	O
,	O
followed	O
by	O
a	O
linear	O
decay	O
to	O
0	B-DatasetName
.	O
We	O
set	O
the	O
L2	O
weight	B-MethodName
decay	I-MethodName
to	O
be	O
0.01	O
.	O
During	O
pre	O
-	O
training	O
,	O
we	O
follow	O
UNITER	B-MethodName
to	O
randomly	O
sample	O
1	O
task	O
per	O
minibatch	O
update	O
.	O
11	O
Our	O
best	O
model	O
is	O
pre	O
-	O
trained	O
on	O
VMLM+SMRM+CRM	O
for	O
300	O
,	O
000	O
optimization	O
steps	O
.	O
We	O
set	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
to	O
10240	O
per	O
GPU	O
(	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
specified	O
by	O
#	O
tokens	O
+	O
#	O
regions	O
,	O
as	O
in	O
UNITER	B-MethodName
)	O
.	O
Pre	O
-	O
training	O
experiments	O
are	O
conducted	O
on	O
8×	O
V100	O
GPUs	O
with	O
6	O
-	O
step	O
gradient	O
accumulation	O
,	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
be	O
5e	O
-	O
5	O
.	O
For	O
ablation	O
studies	O
presented	O
in	O
Table	O
5	O
,	O
the	O
ablated	O
instances	O
of	O
our	O
model	O
are	O
pre	O
-	O
trained	O
for	O
30k	O
steps	O
on	O
COCO	B-DatasetName
dataset	O
(	O
Lin	O
et	O
al	O
,	O
2014	O
)	O
only	O
,	O
and	O
the	O
same	O
choice	O
of	O
learning	B-HyperparameterName
rate	I-HyperparameterName
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
are	O
applied	O
as	O
in	O
the	O
best	O
pre	O
-	O
training	O
setting	O
.	O
For	O
finetuning	O
,	O
we	O
set	O
batch	B-HyperparameterName
size	I-HyperparameterName
n	O
to	O
96	O
(	O
n	O
is	O
in	O
examples	O
,	O
instead	O
of	O
the	O
sequence	O
length	O
of	O
tokens	O
and	O
regions	O
)	O
,	O
and	O
search	O
learning	B-HyperparameterName
rate	I-HyperparameterName
from	O
{	O
1e	O
-	O
5	O
,	O
2e	O
-	O
5	O
,	O
5e	O
-	O
5	O
}	O
.	O
We	O
select	O
models	O
based	O
on	O
their	O
AR	O
on	O
the	O
validation	O
set	O
.	O
The	O
best	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
5e	O
-	O
5	O
for	O
COCO	B-DatasetName
and	O
1e	O
-	O
5	O
for	O
Flickr30K.	O
Our	O
models	O
are	O
trained	O
for	O
15	O
epochs	O
on	O
Flickr30k	B-DatasetName
,	O
and	O
20	O
epochs	O
on	O
COCO	B-DatasetName
.	O
For	O
re	O
-	O
ranking	O
,	O
we	O
choose	O
k	O
from	O
{	O
20	O
,	O
50	O
}	O
.	O

Although	O
the	O
room	O
size	O
is	O
awful	O
,	O
the	O
hotel	O
does	O
have	O
some	O
nice	O
touches	O
.	O
Another	O
benefit	O
of	O
the	O
hotel	O
is	O
that	O
...	O
Table	O
1	O
:	O
Review	O
ordering	O
from	O
a	O
functional	O
approach	O
.	O
Just	O
with	O
the	O
first	O
words	O
of	O
the	O
sentences	O
some	O
characteristic	O
features	O
can	O
be	O
appreciated	O
(	O
Verb	O
tenses	O
,	O
person	O
-	O
thirdfirst	O
-	O
,	O
...	O
)	O
Therefore	O
,	O
our	O
hypothesis	O
is	O
that	O
it	O
is	O
possible	O
to	O
characterise	O
subparts	O
of	O
a	O
discourse	O
(	O
related	O
to	O
a	O
genre	O
)	O
according	O
to	O
their	O
functionality	O
and	O
,	O
at	O
the	O
same	O
time	O
,	O
learn	O
about	O
its	O
(	O
flexible	O
)	O
ordering	O
.	O
Due	O
to	O
the	O
lack	O
of	O
annotated	O
corpora	O
with	O
discourse	O
information	O
about	O
that	O
communicative	O
purposes	O
,	O
we	O
propose	O
to	O
work	O
with	O
unsupervised	O
techniques	O
to	O
achieve	O
that	O
goal	O
.	O
We	O
expect	O
to	O
obtain	O
the	O
necessary	O
knowledge	O
to	O
produce	O
appropriate	O
document	O
plans	O
.	O
Taking	O
into	O
account	O
several	O
genres	O
that	O
normally	O
exhibit	O
a	O
pre	O
-	O
defined	O
or	O
known	O
-	O
in	O
-	O
advance	O
structure	O
,	O
such	O
as	O
the	O
case	O
of	O
news	O
,	O
Wikipedia	O
pages	O
,	O
or	O
scientific	O
article	O
,	O
we	O
would	O
be	O
able	O
to	O
validate	O
our	O
suggested	O
approach	O
in	O
other	O
textual	O
genres	O
that	O
lacks	O
such	O
well	O
-	O
defined	O
structure	O
a	O
priori	O
.	O
Our	O
methodology	O
relies	O
on	O
pattern	O
detection	O
techniques	O
.	O
Until	O
now	O
,	O
we	O
have	O
tried	O
clustering	O
that	O
does	O
not	O
require	O
previous	O
knowledge	O
of	O
the	O
number	O
of	O
clusters	O
.	O
Over	O
an	O
annotated	O
corpus	O
we	O
apply	O
an	O
Expectation	O
-	O
Maximisation	O
(	O
EM	B-MetricName
)	O
algorithm	O
,	O
having	O
included	O
within	O
the	O
features	O
linguistic	O
information	O
related	O
to	O
its	O
placement	O
.	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organised	O
as	O
follows	O
.	O
Section	O
2	O
summarises	O
the	O
related	O
work	O
concerning	O
text	B-TaskName
classification	I-TaskName
efforts	O
and	O
genre	O
studies	O
related	O
to	O
communication	O
objectives	O
.	O
Section	O
3	O
describes	O
the	O
kind	O
of	O
linguistic	O
lexical	O
features	O
that	O
we	O
have	O
been	O
using	O
in	O
our	O
experiments	O
until	O
now	O
.	O
After	O
that	O
,	O
section	O
4	O
describes	O
some	O
resources	O
coming	O
from	O
the	O
Semantic	O
Web	O
environment	O
that	O
could	O
complement	O
and	O
enrich	O
those	O
features	O
.	O
Finally	O
,	O
section	O
5	O
describes	O
the	O
experiments	O
already	O
performed	O
and	O
outlines	O
future	O
research	O
opportunities	O
.	O

Probing	O
LMs	O
for	O
Relational	O
Knowledge	O
Since	O
the	O
introduction	O
of	O
transformer	O
-	O
based	O
LMs	O
,	O
a	O
large	O
number	O
of	O
works	O
have	O
focused	O
on	O
analysing	O
the	O
capabilities	O
of	O
such	O
models	O
,	O
covering	O
the	O
extent	O
to	O
which	O
they	O
capture	O
syntax	O
(	O
Goldberg	O
,	O
2019	O
;	O
Saphra	O
and	O
Lopez	O
,	O
2019	O
;	O
Hewitt	O
and	O
Manning	O
,	O
2019	O
;	O
van	O
Schijndel	O
et	O
al	O
,	O
2019	O
;	O
Jawahar	O
et	O
al	O
,	O
2019	O
;	O
Tenney	O
et	O
al	O
,	O
2019	O
)	O
,	O
lexical	O
semantics	O
(	O
Ethayarajh	O
,	O
2019	O
;	O
Bommasani	O
et	O
al	O
,	O
2020	O
;	O
Vulic	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
various	O
forms	O
of	O
factual	O
and	O
commonsense	O
knowledge	O
(	O
Petroni	O
et	O
al	O
,	O
2019	O
;	O
Forbes	O
et	O
al	O
,	O
2019	O
;	O
Davison	O
et	O
al	O
,	O
2019	O
;	O
Zhou	O
et	O
al	O
,	O
2020	O
;	O
Talmor	O
et	O
al	O
,	O
2020	O
;	O
Roberts	O
et	O
al	O
,	O
2020	O
)	O
,	O
among	O
others	O
.	O
The	O
idea	O
of	O
extracting	O
relational	O
knowledge	O
from	O
LMs	O
,	O
in	O
particular	O
,	O
has	O
also	O
been	O
studied	O
.	O
For	O
instance	O
,	O
Petroni	O
et	O
al	O
(	O
2019	O
)	O
use	O
BERT	B-MethodName
for	O
link	B-TaskName
prediction	I-TaskName
.	O
To	O
this	O
end	O
,	O
they	O
use	O
a	O
manually	O
defined	O
prompt	O
for	O
each	O
relation	O
type	O
,	O
in	O
which	O
the	O
tail	O
entity	O
is	O
replaced	O
by	O
a	O
<	O
mask	O
>	O
token	O
.	O
To	O
complete	O
a	O
knowledge	O
graph	O
triple	O
such	O
as	O
(	O
Dante	O
,	O
born	O
-	O
in	O
,	O
?	O
)	O
they	O
create	O
the	O
input	O
"	O
Dante	O
was	O
born	O
in	O
<	O
mask	O
>	O
"	O
and	O
then	O
look	O
at	O
the	O
predictions	O
of	O
BERT	B-MethodName
for	O
the	O
masked	O
token	O
to	O
retrieve	O
the	O
correct	O
answer	O
.	O
It	O
is	O
notable	O
that	O
BERT	B-MethodName
is	O
thus	O
used	O
for	O
extracting	O
relational	O
knowledge	O
without	O
any	O
fine	O
-	O
tuning	O
.	O
This	O
clearly	O
shows	O
that	O
a	O
substantial	O
amount	O
of	O
factual	O
knowledge	O
is	O
encoded	O
in	O
the	O
parameters	O
of	O
pre	O
-	O
trained	O
LMs	O
.	O
Some	O
works	O
have	O
also	O
looked	O
at	O
how	O
such	O
knowledge	O
is	O
stored	O
.	O
Geva	O
et	O
al	O
(	O
2020	O
)	O
argue	O
that	O
the	O
feed	O
-	O
forward	O
layers	O
of	O
transformer	O
-	O
based	O
LMs	O
act	O
as	O
neural	O
memories	O
,	O
which	O
would	O
suggest	O
that	O
e.g.	O
"	O
the	O
place	O
where	O
Dante	O
is	O
born	O
"	O
is	O
stored	O
as	O
a	O
property	O
of	O
Florence	B-MethodName
.	O
Dai	O
et	O
al	O
(	O
2021	O
)	O
present	O
further	O
evidence	O
of	O
this	O
view	O
.	O
What	O
is	O
less	O
clear	O
,	O
then	O
,	O
is	O
whether	O
relations	O
themselves	O
have	O
an	O
explicit	O
representation	O
,	O
or	O
whether	O
transformer	O
models	O
essentially	O
store	O
a	O
propositionalised	O
knowledge	O
graph	O
.	O
The	O
results	O
we	O
present	O
in	O
this	O
paper	O
suggest	O
that	O
common	O
lexical	O
relations	O
(	O
e.g.	O
hypernymy	O
,	O
meronymy	O
,	O
has	O
-	O
attribute	O
)	O
,	O
at	O
least	O
,	O
must	O
have	O
some	O
kind	O
of	O
explicit	O
representation	O
,	O
although	O
it	O
remains	O
unclear	O
how	O
they	O
are	O
encoded	O
.	O
Another	O
notable	O
work	O
focusing	O
on	O
link	B-TaskName
prediction	I-TaskName
is	O
(	O
Bosselut	O
et	O
al	O
,	O
2019	O
)	O
,	O
where	O
GPT	B-MethodName
is	O
fine	O
-	O
tuned	O
to	O
complete	O
triples	O
from	O
commonsense	O
knowledge	B-TaskName
graphs	I-TaskName
,	O
in	O
particular	O
ConceptNet	B-DatasetName
(	O
Speer	O
et	O
al	O
,	O
2017	O
)	O
and	O
ATOMIC	B-DatasetName
.	O
While	O
their	O
model	O
was	O
able	O
to	O
generate	O
new	O
knowledge	O
graph	O
triples	O
,	O
it	O
is	O
unclear	O
to	O
what	O
extent	O
this	O
is	O
achieved	O
by	O
extracting	O
commonsense	O
knowledge	O
that	O
was	O
already	O
captured	O
by	O
the	O
pre	O
-	O
trained	O
GPT	B-MethodName
model	O
,	O
or	O
whether	O
this	O
rather	O
comes	O
from	O
the	O
ability	O
to	O
generalise	O
from	O
the	O
training	O
triples	O
.	O
For	O
the	O
ConceptNet	B-DatasetName
dataset	O
,	O
for	O
instance	O
,	O
Jastrzębski	O
et	O
al	O
(	O
2018	O
)	O
found	O
that	O
most	O
test	O
triples	O
are	O
in	O
fact	O
minor	O
variations	O
of	O
training	O
triples	O
.	O
In	O
this	O
paper	O
,	O
we	O
also	O
rely	O
on	O
fine	O
-	O
tuning	O
,	O
which	O
makes	O
it	O
harder	O
to	O
determine	O
to	O
what	O
extent	O
the	O
pre	O
-	O
trained	O
LM	O
already	O
captures	O
relational	O
knowledge	O
.	O
We	O
address	O
this	O
concern	O
by	O
including	O
relation	O
types	O
in	O
our	O
evaluation	O
which	O
are	O
different	O
from	O
the	O
ones	O
that	O
have	O
been	O
used	O
for	O
fine	O
-	O
tuning	O
.	O
Unsupervised	O
Relation	O
Discovery	O
Modelling	O
how	O
different	O
words	O
are	O
related	O
is	O
a	O
long	O
-	O
standing	O
challenge	O
in	O
NLP	O
.	O
An	O
early	O
approach	O
is	O
DIRT	O
(	O
Lin	O
and	O
Pantel	O
,	O
2001	O
)	O
,	O
which	O
encodes	O
the	O
relation	O
between	O
two	O
nouns	O
as	O
the	O
dependency	O
path	O
connecting	O
them	O
.	O
Their	O
view	O
is	O
that	O
two	O
such	O
dependency	O
paths	O
are	O
similar	O
if	O
the	O
sets	O
of	O
word	O
pairs	O
with	O
which	O
they	O
co	O
-	O
occur	O
are	O
similar	O
.	O
Hasegawa	O
et	O
al	O
(	O
2004	O
)	O
cluster	O
named	O
entity	O
pairs	O
based	O
on	O
the	O
bag	O
-	O
of	O
-	O
words	O
representations	O
of	O
the	O
contexts	O
in	O
which	O
they	O
appear	O
.	O
Along	O
the	O
same	O
lines	O
,	O
Yao	O
et	O
al	O
(	O
2011	O
)	O
proposed	O
a	O
generative	O
probabilistic	O
model	O
,	O
inspired	O
by	O
LDA	B-MethodName
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
,	O
in	O
which	O
relations	O
are	O
viewed	O
as	O
latent	O
variables	O
(	O
similar	O
to	O
topics	O
in	O
LDA	B-MethodName
)	O
.	O
Turney	O
(	O
2005	O
)	O
proposed	O
a	O
method	O
called	O
Latent	O
Relational	O
Analysis	O
(	O
LRA	B-DatasetName
)	O
,	O
which	O
uses	O
matrix	O
factorization	O
to	O
learn	O
relation	O
embeddings	O
based	O
on	O
co	O
-	O
occurrences	O
of	O
word	O
pairs	O
and	O
dependency	O
paths	O
.	O
Matrix	O
factorization	O
is	O
also	O
used	O
in	O
the	O
Universal	O
Schema	O
approach	O
from	O
Riedel	O
et	O
al	O
(	O
Riedel	O
et	O
al	O
,	O
2013	O
)	O
,	O
which	O
jointly	O
models	O
the	O
contexts	O
in	O
which	O
words	O
appear	O
in	O
a	O
corpus	O
with	O
a	O
given	O
set	O
of	O
relational	O
facts	O
.	O
The	O
aforementioned	O
works	O
essentially	O
represent	O
the	O
relation	O
between	O
two	O
words	O
by	O
summarising	O
the	O
contexts	O
in	O
which	O
these	O
words	O
co	O
-	O
occur	O
.	O
In	O
recent	O
years	O
,	O
a	O
number	O
of	O
strategies	O
based	O
on	O
distributional	O
models	O
have	O
been	O
explored	O
that	O
rely	O
on	O
similar	O
intuitions	O
but	O
go	O
beyond	O
simple	O
vector	O
operations	O
of	O
word	B-TaskName
embeddings	I-TaskName
.	O
2	O
For	O
instance	O
,	O
Jameel	O
et	O
al	O
(	O
2018	O
)	O
introduced	O
a	O
variant	O
of	O
the	O
GloVe	B-MethodName
word	O
embedding	O
model	O
,	O
in	O
which	O
relation	O
vectors	O
are	O
jointly	O
learned	O
with	O
word	O
vectors	O
.	O
In	O
SeVeN	O
(	O
Espinosa	O
-	O
Anke	O
and	O
Schockaert	O
,	O
2018	O
)	O
and	O
RELATIVE	O
(	O
Camacho	O
-	O
Collados	O
et	O
al	O
,	O
2019	O
)	O
,	O
relation	O
vectors	O
are	O
computed	O
by	O
averaging	O
the	O
embeddings	O
of	O
context	O
words	O
,	O
while	O
pair2vec	O
uses	O
an	O
LSTM	B-MethodName
to	O
summarise	O
the	O
contexts	O
in	O
which	O
two	O
given	O
words	O
occur	O
,	O
and	O
Washio	O
and	O
Kato	O
(	O
2018	O
)	O
learn	O
embeddings	O
of	O
dependency	O
paths	O
to	O
encode	O
word	O
pairs	O
.	O
Another	O
line	O
of	O
work	O
is	O
based	O
on	O
the	O
idea	O
that	O
relation	O
embeddings	O
should	O
facilitate	O
link	B-TaskName
prediction	I-TaskName
,	O
i.e.	O
given	O
the	O
first	O
word	O
and	O
a	O
relation	O
vector	O
,	O
we	O
should	O
be	O
able	O
to	O
predict	O
the	O
second	O
word	O
(	O
Marcheggiani	O
and	O
Titov	O
,	O
2016	O
;	O
Simon	O
et	O
al	O
,	O
2019	O
)	O
.	O
This	O
idea	O
also	O
lies	O
at	O
the	O
basis	O
of	O
the	O
approach	O
from	O
Soares	O
et	O
al	O
(	O
2019	O
)	O
,	O
who	O
train	O
a	O
relation	O
encoder	O
by	O
fine	O
-	O
tuning	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
with	O
a	O
link	B-TaskName
prediction	I-TaskName
loss	B-MetricName
.	O
However	O
,	O
it	O
should	O
be	O
noted	O
that	O
they	O
focus	O
on	O
learning	O
relation	O
vectors	O
from	O
individual	O
sentences	O
,	O
as	O
a	O
pre	O
-	O
training	O
task	O
for	O
applications	O
such	O
as	O
few	O
-	O
shot	O
relation	B-TaskName
extraction	I-TaskName
.	O
In	O
contrast	O
,	O
our	O
focus	O
in	O
this	O
paper	O
is	O
on	O
characterising	O
the	O
overall	O
relationship	O
between	O
two	O
words	O
.	O

Manual	O
Prompts	O
A	O
basic	O
prompt	O
generation	O
strategy	O
is	O
to	O
rely	O
on	O
manually	O
created	O
templates	O
,	O
2	O
Interestingly	O
,	O
Roller	O
and	O
Erk	O
(	O
2016	O
)	O
showed	O
that	O
the	O
direct	O
concatenation	O
of	O
distributional	O
word	O
vectors	O
in	O
isolation	O
can	O
effectively	O
identify	O
Hearst	O
Patterns	O
(	O
Hearst	O
,	O
1992	O
)	O
.	O
which	O
has	O
proven	O
effective	O
in	O
factual	O
knowledge	O
probing	O
(	O
Petroni	O
et	O
al	O
,	O
2019	O
)	O
and	O
text	B-TaskName
classification	I-TaskName
(	O
Schick	O
and	O
Schütze	O
,	O
2021	O
;	O
Tam	O
et	O
al	O
,	O
2021	O
;	O
Le	O
Scao	O
and	O
Rush	O
,	O
2021	O
)	O
,	O
among	O
many	O
others	O
.	O
To	O
test	O
whether	O
manually	O
generated	O
templates	O
can	O
be	O
effective	O
for	O
learning	O
relation	O
embeddings	O
,	O
we	O
will	O
consider	O
the	O
following	O
five	O
templates	O
:	O
(	O
Bouraoui	O
et	O
al	O
,	O
2020	O
;	O
Jiang	O
et	O
al	O
,	O
2020	O
)	O
.	O
Learned	O
Prompts	O
The	O
choice	O
of	O
prompt	O
can	O
have	O
a	O
significant	O
impact	O
on	O
an	O
LM	O
's	O
performance	O
.	O
Since	O
it	O
is	O
difficult	O
to	O
generate	O
manual	O
prompts	O
in	O
a	O
systematic	O
way	O
,	O
several	O
strategies	O
for	O
automated	O
generation	O
of	O
task	O
-	O
specific	O
prompts	O
have	O
been	O
proposed	O
,	O
e.g.	O
based	O
on	O
mining	O
patterns	O
from	O
a	O
corpus	O
(	O
Bouraoui	O
et	O
al	O
,	O
2020	O
)	O
,	O
paraphrasing	O
(	O
Jiang	O
et	O
al	O
,	O
2020	O
)	O
,	O
training	O
an	O
additional	O
LM	O
for	O
template	O
generation	O
(	O
Haviv	O
et	O
al	O
,	O
2021	O
;	O
Gao	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
prompt	O
optimization	O
(	O
Shin	O
et	O
al	O
,	O
2020	O
;	O
Liu	O
et	O
al	O
,	O
2021	O
)	O
.	O
In	O
our	O
work	O
,	O
we	O
focus	O
on	O
the	O
latter	O
strategy	O
,	O
given	O
its	O
conceptual	O
simplicity	O
and	O
its	O
strong	O
reported	O
performance	O
on	O
various	O
benchmarks	O
.	O
Specifically	O
,	O
we	O
consider	O
AutoPrompt	O
(	O
Shin	O
et	O
al	O
,	O
2020	O
)	O
and	O
P	O
-	O
tuning	O
(	O
Liu	O
et	O
al	O
,	O
2021	O
)	O
.	O
Note	O
that	O
both	O
methods	O
rely	O
on	O
training	O
data	O
.	O
We	O
will	O
use	O
the	O
same	O
training	O
data	O
and	O
loss	B-MetricName
function	O
that	O
we	O
use	O
for	O
fine	O
-	O
tuning	O
the	O
LM	O
;	O
see	O
Section	O
3.2	O
.	O
AutoPrompt	O
initializes	O
the	O
prompt	O
as	O
a	O
fixedlength	O
template	O
:	O
T	O
=	O
(	O
z	O
1	O
,	O
.	O
.	O
.	O
,	O
z	O
π	O
,	O
[	O
h	O
]	O
,	O
z	O
π+1	O
,	O
.	O
.	O
.	O
,	O
z	O
π+τ	O
,	O
[	O
t	O
]	O
,	O
z	O
π+τ	O
+1	O
,	O
.	O
.	O
.	O
,	O
z	O
π+τ	O
+	O
γ	B-HyperparameterName
)	O
(	O
1	O
)	O
where	O
π	O
,	O
τ	O
,	O
γ	B-HyperparameterName
are	O
hyper	O
-	O
parameters	O
which	O
determine	O
the	O
length	O
of	O
the	O
template	O
.	O
The	O
tokens	O
of	O
the	O
form	O
z	O
i	O
are	O
called	O
trigger	O
tokens	O
.	O
These	O
tokens	O
are	O
initialized	O
as	O
<	O
mask	O
>	O
.	O
The	O
method	O
then	O
iteratively	O
finds	O
the	O
best	O
token	O
to	O
replace	O
each	O
mask	O
,	O
based	O
on	O
the	O
gradient	O
of	O
the	O
task	O
-	O
specific	O
loss	B-MetricName
function	O
.	O
3	O
P	O
-	O
tuning	O
employs	O
the	O
same	O
template	O
initialization	O
as	O
AutoPrompt	O
but	O
its	O
trigger	O
tokens	O
are	O
newly	O
introduced	O
special	O
tokens	O
with	O
trainable	O
embeddingsê	O
1	O
:	O
π+τ	O
+	O
γ	B-HyperparameterName
,	O
which	O
are	O
learned	O
using	O
a	O
taskspecific	O
loss	B-MetricName
function	O
while	O
the	O
LM	O
's	O
weights	O
are	O
frozen	O
.	O

To	O
fine	O
-	O
tune	O
the	O
LM	O
,	O
we	O
need	O
training	O
data	O
and	O
a	O
loss	B-MetricName
function	O
.	O
As	O
training	O
data	O
,	O
we	O
assume	O
that	O
,	O
for	O
a	O
number	O
of	O
different	O
relation	O
types	O
r	O
,	O
we	O
have	O
access	O
to	O
examples	O
of	O
word	O
pairs	O
(	O
h	O
,	O
t	O
)	O
that	O
are	O
instances	O
of	O
that	O
relation	O
type	O
.	O
The	O
loss	B-MetricName
function	O
is	O
based	O
on	O
the	O
following	O
intuition	O
:	O
the	O
embeddings	O
of	O
word	O
pairs	O
that	O
belong	O
to	O
the	O
same	O
relation	O
type	O
should	O
be	O
closer	O
together	O
than	O
the	O
embeddings	O
of	O
pairs	O
that	O
belong	O
to	O
different	O
relations	O
.	O
In	O
particular	O
,	O
we	O
use	O
the	O
triplet	B-MethodName
loss	I-MethodName
from	O
Schroff	O
et	O
al	O
(	O
2015	O
)	O
and	O
the	O
classification	O
loss	B-MetricName
from	O
Reimers	O
and	O
Gurevych	O
(	O
2019	O
)	O
,	O
both	O
of	O
which	O
are	O
based	O
on	O
this	O
intuition	O
.	O
Triplet	B-MethodName
Loss	I-MethodName
We	O
draw	O
a	O
triplet	O
from	O
the	O
relation	O
dataset	O
by	O
selecting	O
an	O
anchor	O
pair	O
a	O
=	O
(	O
h	O
a	O
,	O
t	O
a	O
)	O
,	O
a	O
positive	O
example	O
p	O
=	O
(	O
h	O
p	O
,	O
t	O
p	O
)	O
and	O
a	O
negative	O
example	O
n	O
=	O
(	O
h	O
n	O
,	O
t	O
n	O
)	O
,	O
i.e.	O
we	O
select	O
word	O
pairs	O
a	O
,	O
p	O
,	O
n	O
such	O
that	O
a	O
and	O
p	O
belong	O
to	O
the	O
same	O
relation	O
type	O
while	O
n	O
belongs	O
to	O
a	O
different	O
relation	O
type	O
.	O
Let	O
us	O
write	O
x	O
a	O
,	O
x	O
p	O
,	O
x	O
n	O
for	O
the	O
corresponding	O
relation	O
embeddings	O
.	O
Each	O
relation	O
embedding	O
is	O
produced	O
by	O
the	O
same	O
LM	O
,	O
which	O
is	O
trained	O
to	O
make	O
the	O
distance	O
between	O
x	O
a	O
and	O
x	O
p	O
smaller	O
than	O
the	O
distance	O
between	O
x	O
a	O
and	O
x	O
n	O
.	O
Formally	O
,	O
this	O
is	O
accomplished	O
using	O
the	O
following	O
triplet	B-MethodName
loss	I-MethodName
function	O
:	O
L	O
t	O
=	O
max	O
0	B-DatasetName
,	O
x	O
a	O
−	O
x	O
p	O
−	O
x	O
a	O
−	O
x	O
n	O
+	O
ε	B-HyperparameterName
3	O
We	O
note	O
that	O
in	O
most	O
implementations	O
of	O
AutoPrompt	O
the	O
vocabulary	O
to	O
sample	O
trigger	O
tokens	O
is	O
restricted	O
to	O
that	O
of	O
the	O
training	O
data	O
.	O
However	O
,	O
given	O
the	O
nature	O
of	O
our	O
training	O
data	O
(	O
i.e.	O
,	O
pairs	O
of	O
words	O
and	O
not	O
sentences	O
)	O
,	O
we	O
consider	O
the	O
full	O
pre	O
-	O
trained	O
LM	O
's	O
vocabulary	O
.	O
where	O
ε	B-HyperparameterName
>	O
0	B-DatasetName
is	O
the	O
margin	O
and	O
is	O
the	O
l	O
2	O
norm	O
.	O
Classification	B-TaskName
Loss	O
Following	O
SBERT	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
,	O
we	O
use	O
a	O
classifier	O
to	O
predict	O
whether	O
two	O
word	O
pairs	O
belong	O
to	O
the	O
same	O
relation	O
.	O
The	O
classifier	O
is	O
jointly	O
trained	O
with	O
the	O
LM	O
using	O
the	O
negative	O
log	O
likelihood	O
loss	B-MetricName
function	O
:	O
L	O
c	O
=	O
−	O
log	O
(	O
g	O
(	O
x	O
a	O
,	O
x	O
p	O
)	O
)	O
−	O
log	O
(	O
1	O
−	O
g	O
(	O
x	O
a	O
,	O
x	O
n	O
)	O
)	O
where	O
g	O
(	O
u	O
,	O
v	O
)	O
=	O
sigmoid	O
(	O
W	O
[	O
u	O
v	O
|	O
v	O
−	O
u	O
|	O
]	O
T	O
)	O
with	O
W	O
R	O
3×d	O
,	O
u	O
,	O
v	O
R	O
d	O
,	O
|	O
|	O
the	O
element	O
-	O
wise	O
absolute	O
difference	O
,	O
and	O
concatenation	O
.	O

In	O
this	O
section	O
,	O
we	O
explain	O
our	O
experimental	O
setting	O
to	O
train	O
and	O
evaluate	O
RelBERT	O
.	O
triples	O
are	O
from	O
different	O
relations	O
)	O
.	O
Figure	O
2	O
illustrates	O
this	O
idea	O
.	O
Note	O
how	O
the	O
effective	O
batch	B-HyperparameterName
size	I-HyperparameterName
thus	O
increases	O
quadratically	O
,	O
while	O
the	O
number	O
of	O
vectors	O
that	O
needs	O
to	O
be	O
encoded	O
by	O
the	O
LM	O
remains	O
unchanged	O
.	O
In	O
our	O
setting	O
,	O
this	O
leads	O
to	O
an	O
additional	O
13500	O
triples	O
per	O
relation	O
.	O
Similar	O
in	O
-	O
batch	O
negative	O
sampling	O
has	O
been	O
shown	O
to	O
be	O
effective	O
in	O
information	B-TaskName
retrieval	I-TaskName
(	O
Karpukhin	O
et	O
al	O
,	O
2020	O
;	O
Gillick	O
et	O
al	O
,	O
2019	O
)	O
.	O
Third	O
,	O
we	O
also	O
construct	O
training	O
triples	O
by	O
considering	O
the	O
10	O
high	O
-	O
level	O
categories	O
as	O
relation	O
types	O
.	O
In	O
this	O
case	O
,	O
we	O
choose	O
two	O
positive	O
examples	O
from	O
different	O
relations	O
that	O
belong	O
to	O
the	O
same	O
category	O
,	O
along	O
with	O
a	O
positive	O
example	O
from	O
a	O
relation	O
from	O
a	O
different	O
category	O
.	O
We	O
add	O
5040	O
triples	O
of	O
this	O
kind	O
for	O
each	O
of	O
the	O
10	O
categories	O
.	O
Training	O
RelBERT	O
training	O
consists	O
of	O
two	O
phases	O
:	O
prompt	O
optimization	O
(	O
unless	O
a	O
manually	O
defined	O
prompt	O
is	O
used	O
)	O
and	O
language	O
model	O
finetuning	O
.	O
First	O
we	O
optimize	O
the	O
prompt	O
over	O
the	O
training	O
set	O
with	O
the	O
triplet	B-MethodName
loss	I-MethodName
L	O
t	O
while	O
the	O
parameters	O
of	O
the	O
LM	O
are	O
frozen	O
.	O
Subsequently	O
,	O
we	O
fine	O
-	O
tune	O
the	O
LM	O
with	O
the	O
resulting	O
prompt	O
,	O
using	O
the	O
sum	O
of	O
the	O
triplet	B-MethodName
loss	I-MethodName
L	O
t	O
and	O
the	O
classification	O
loss	B-MetricName
L	O
c	O
over	O
the	O
same	O
training	O
set	O
.	O
We	O
do	O
not	O
use	O
the	O
classification	O
loss	B-MetricName
during	O
the	O
prompt	O
optimisation	O
,	O
as	O
that	O
would	O
involve	O
training	O
the	O
classifier	O
while	O
optimizing	O
the	O
prompt	O
.	O
We	O
select	O
the	O
best	O
hyper	O
-	O
parameters	O
of	O
the	O
prompting	O
methods	O
based	O
on	O
the	O
final	O
loss	B-MetricName
over	O
the	O
validation	O
set	O
.	O
In	O
particular	O
,	O
when	O
manual	O
prompts	O
are	O
used	O
,	O
we	O
choose	O
the	O
best	O
template	O
among	O
the	O
five	O
candidates	O
described	O
in	O
Section	O
3.1	O
.	O
For	O
AutoPrompt	O
and	O
Ptuning	O
,	O
we	O
consider	O
all	O
combinations	O
of	O
π	O
{	O
8	O
,	O
9	O
}	O
,	O
τ	O
{	O
1	O
,	O
2	O
}	O
,	O
γ	B-HyperparameterName
{	O
1	O
,	O
2	O
}	O
.	O
We	O
use	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
as	O
our	O
main	O
LM	O
,	O
where	O
the	O
initial	O
weights	O
were	O
taken	O
from	O
the	O
roberta	O
-	O
large	O
model	O
checkpoint	O
shared	O
by	O
the	O
Huggingface	O
transformers	O
model	O
hub	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
use	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
learn	O
-	O
ing	O
rate	O
0.00002	O
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
64	O
and	O
we	O
fine	O
-	O
tune	O
the	O
model	O
for	O
1	O
epoch	O
.	O
For	O
AutoPrompt	O
,	O
the	O
top	O
-	O
50	O
tokens	O
are	O
considered	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
iterations	I-HyperparameterName
is	O
set	O
to	O
50	O
.	O
In	O
each	O
iteration	O
,	O
one	O
of	O
the	O
input	O
tokens	O
is	O
re	O
-	O
sampled	O
and	O
the	O
loss	B-MetricName
is	O
re	O
-	O
computed	O
across	O
the	O
entire	O
training	O
set	O
.	O
4	O
For	O
P	O
-	O
tuning	O
,	O
we	O
train	O
the	O
weights	O
that	O
define	O
the	O
trigger	O
embeddings	O
(	O
i.e.	O
the	O
weights	O
of	O
the	O
input	O
vectors	O
and	O
the	O
parameters	O
of	O
the	O
LSTM	B-MethodName
)	O
for	O
2	O
epochs	O
.	O
Note	O
that	O
we	O
do	O
not	O
tune	O
RelBERT	O
on	O
any	O
task	O
-	O
specific	O
training	O
or	O
validation	O
set	O
.	O
We	O
thus	O
use	O
the	O
same	O
relation	O
embeddings	O
across	O
all	O
the	O
considered	O
evaluation	O
tasks	O
.	O

We	O
evaluate	O
RelBERT	O
on	O
two	O
relation	O
-	O
centric	O
tasks	O
:	O
solving	O
analogy	O
questions	O
(	O
unsupervised	O
)	O
and	O
lexical	O
relation	B-TaskName
classification	I-TaskName
(	O
supervised	O
)	O
.	O
Analogy	O
Questions	O
We	O
consider	O
the	O
task	O
of	O
solving	O
word	O
analogy	O
questions	O
.	O
Given	O
a	O
query	O
word	O
pair	O
,	O
the	O
model	O
is	O
required	O
to	O
select	O
the	O
relationally	O
most	O
similar	O
word	O
pair	O
from	O
a	O
list	O
of	O
candidates	O
.	O
To	O
solve	O
this	O
task	O
,	O
we	O
simply	O
choose	O
the	O
candidate	O
whose	O
RelBERT	O
embedding	O
has	O
the	O
highest	O
cosine	O
similarity	O
with	O
the	O
RelBERT	O
embedding	O
of	O
the	O
query	O
pair	O
.	O
Note	O
that	O
this	O
task	O
is	O
completely	O
unsupervised	O
,	O
without	O
the	O
need	O
for	O
any	O
training	O
or	O
tuning	O
.	O
We	O
use	O
the	O
five	O
analogy	O
datasets	O
that	O
were	O
considered	O
by	O
Ushio	O
et	O
al	O
(	O
2021	O
)	O
:	O
the	O
SAT	O
analogies	O
dataset	O
(	O
Turney	O
et	O
al	O
,	O
2003	O
)	O
,	O
the	O
U2	O
and	O
U4	O
analogy	O
datasets	O
,	O
which	O
were	O
collected	O
from	O
an	O
educational	O
website	O
5	O
,	O
and	O
datasets	O
that	O
were	O
derived	O
6	O
from	O
BATS	O
(	O
Gladkova	O
et	O
al	O
,	O
2016	O
)	O
and	O
the	O
Google	B-DatasetName
analogy	O
dataset	O
(	O
Mikolov	O
et	O
al	O
,	O
2013b	O
)	O
.	O
These	O
five	O
datasets	O
consist	O
of	O
tuning	O
and	O
testing	O
fragments	O
.	O
In	O
particular	O
,	O
they	O
contain	O
37/337	O
(	O
SAT	O
)	O
,	O
24/228	O
(	O
U2	O
)	O
,	O
48/432	O
(	O
U4	O
)	O
,	O
50/500	O
(	O
Google	B-DatasetName
)	O
,	O
and	O
199/1799	O
(	O
BATS	O
)	O
questions	O
for	O
validation	O
/	O
testing	O
.	O
As	O
there	O
is	O
no	O
need	O
to	O
tune	O
RelBERT	O
on	O
task	O
-	O
specific	O
data	O
,	O
we	O
only	O
use	O
the	O
test	O
fragments	O
.	O
For	O
SAT	O
,	O
we	O
will	O
also	O
report	O
results	O
on	O
the	O
full	O
dataset	O
(	O
i.e.	O
the	O
testing	O
fragment	O
and	O
tuning	O
fragment	O
combined	O
)	O
,	O
as	O
this	O
allows	O
us	O
to	O
compare	O
the	O
performance	O
with	O
published	O
results	O
.	O
We	O
will	O
refer	O
to	O
this	O
full	O
version	O
of	O
the	O
SAT	O
dataset	O
as	O
SAT	O
†.	O
Lexical	O
Relation	B-TaskName
Classification	I-TaskName
We	O
consider	O
the	O
task	O
of	O
predicting	O
which	O
relation	O
a	O
given	O
word	O
pair	O
belongs	O
to	O
.	O
To	O
solve	O
this	O
task	O
,	O
we	O
train	O
a	O
multi	O
-	O
layer	O
perceptron	O
(	O
MLP	B-DatasetName
)	O
which	O
takes	O
the	O
(	O
frozen	O
)	O
RelBERT	O
embedding	O
of	O
the	O
word	O
pair	O
as	O
input	O
.	O
We	O
consider	O
the	O
following	O
widelyused	O
multi	O
-	O
class	O
relation	B-TaskName
classification	I-TaskName
benchmarks	O
:	O
K&H+N	O
(	O
Necşulescu	O
et	O
al	O
,	O
2015	O
)	O
,	O
BLESS	O
(	O
Baroni	O
and	O
Lenci	O
,	O
2011	O
)	O
,	O
ROOT09	O
(	O
Santus	O
et	O
al	O
,	O
2016b	O
)	O
,	O
EVALution	B-DatasetName
(	O
Santus	O
et	O
al	O
,	O
2015	O
)	O
,	O
and	O
CogALex	O
-	O
V	O
Subtask	O
2	O
(	O
Santus	O
et	O
al	O
,	O
2016a	O
)	O
.	O
Table	O
1	O
shows	O
the	O
size	O
of	O
the	O
training	O
,	O
validation	O
and	O
test	O
sets	O
for	O
each	O
of	O
the	O
relation	B-TaskName
classification	I-TaskName
dataset	O
.	O
The	O
hyperparameters	O
of	O
the	O
MLP	B-DatasetName
classifier	O
are	O
tuned	O
on	O
the	O
validation	O
set	O
of	O
each	O
dataset	O
.	O
Concretely	O
,	O
we	O
tune	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
from	O
[	O
0.001	O
,	O
0.0001	O
,	O
0.00001	O
]	O
and	O
the	O
hidden	B-HyperparameterName
layer	I-HyperparameterName
size	I-HyperparameterName
from	O
[	O
100	O
,	O
150	O
,	O
200	O
]	O
.	O
CogALex	O
-	O
V	O
only	O
has	O
testing	O
fragments	O
so	O
for	O
this	O
dataset	O
we	O
employ	O
the	O
default	O
configuration	O
of	O
Scikit	O
-	O
Learn	O
(	O
Pedregosa	O
et	O
al	O
,	O
2011	O
)	O
,	O
which	O
uses	O
a	O
100	O
-	O
dimensional	O
hidden	O
layer	O
and	O
is	O
optimized	O
using	O
Adam	B-MethodName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.001	O
.	O
These	O
datasets	O
focus	O
on	O
the	O
following	O
lexical	O
relations	O
:	O
co	O
-	O
hyponymy	O
(	O
cohyp	O
)	O
,	O
hypernymy	O
(	O
hyp	O
)	O
,	O
meronymy	O
(	O
mero	O
)	O
,	O
possession	O
(	O
poss	O
)	O
,	O
synonymy	O
(	O
syn	O
)	O
,	O
antonymy	O
(	O
ant	O
)	O
,	O
attribute	O
(	O
attr	O
)	O
,	O
event	O
,	O
and	O
random	O
(	O
rand	O
)	O
.	O

As	O
baselines	O
,	O
we	O
consider	O
two	O
standard	O
word	O
embedding	O
models	O
:	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
and	O
FastText	B-MethodName
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
,	O
where	O
word	O
pairs	O
are	O
represented	O
by	O
the	O
vector	O
difference	O
of	O
their	O
word	B-TaskName
embeddings	I-TaskName
(	O
diff	O
)	O
.	O
7	O
For	O
the	O
classification	O
experiments	O
,	O
we	O
also	O
consider	O
the	O
concatena	O
-	O
7	O
Vector	O
difference	O
is	O
the	O
most	O
common	O
method	O
for	O
encoding	O
relations	O
,	O
and	O
has	O
been	O
shown	O
to	O
be	O
the	O
most	O
reliable	O
in	O
the	O
context	O
of	O
word	O
analogies	O
(	O
Hakami	O
and	O
Bollegala	O
,	O
2017	O
)	O
.	O
tion	O
of	O
the	O
two	O
word	B-TaskName
embeddings	I-TaskName
(	O
cat	O
)	O
and	O
their	O
element	O
-	O
wise	O
multiplication	O
8	O
(	O
dot	O
)	O
.	O
We	O
furthermore	O
experiment	O
with	O
two	O
pre	O
-	O
trained	O
word	O
pair	O
embedding	O
models	O
:	O
pair2vec	O
(	O
pair	O
)	O
and	O
RELATIVE	O
(	O
Camacho	O
-	O
Collados	O
et	O
al	O
,	O
2019	O
)	O
(	O
rel	O
)	O
.	O
For	O
these	O
word	O
pair	O
embeddings	O
,	O
as	O
well	O
as	O
for	O
RelBERT	O
,	O
we	O
concatenate	O
the	O
embeddings	O
from	O
both	O
directions	O
,	O
i.e.	O
(	O
h	O
,	O
t	O
)	O
and	O
(	O
t	O
,	O
h	O
)	O
.	O
For	O
the	O
analogy	O
questions	O
,	O
two	O
simple	O
statistical	O
baselines	O
are	O
included	O
:	O
the	O
expected	O
random	O
performance	O
and	O
a	O
strategy	O
based	O
on	O
point	O
-	O
wise	O
mutual	O
information	O
(	O
PMI	O
)	O
Church	O
and	O
Hanks	O
(	O
1990	O
)	O
.	O
In	O
particular	O
,	O
the	O
PMI	O
score	O
of	O
a	O
word	O
pair	O
is	O
computed	O
using	O
the	O
English	O
Wikipedia	O
,	O
with	O
a	O
fixed	O
window	O
size	O
of	O
10	O
.	O
We	O
then	O
choose	O
the	O
candidate	O
pair	O
with	O
the	O
highest	O
PMI	O
as	O
the	O
prediction	O
.	O
Note	O
that	O
this	O
PMI	O
-	O
based	O
method	O
completely	O
ignores	O
the	O
query	O
pair	O
.	O
We	O
also	O
compare	O
with	O
the	O
published	O
results	O
from	O
Ushio	O
et	O
al	O
(	O
2021	O
)	O
,	O
where	O
a	O
strategy	O
is	O
proposed	O
to	O
solve	O
analogy	O
questions	O
by	O
using	O
LMs	O
to	O
compute	O
an	O
analogical	O
proportion	O
score	O
.	O
In	O
particular	O
,	O
a	O
four	O
-	O
word	O
tuple	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
)	O
is	O
encoded	O
using	O
a	O
custom	O
prompt	O
and	O
perplexity	B-MetricName
based	O
scoring	O
strategies	O
are	O
used	O
to	O
determine	O
whether	O
the	O
word	O
pair	O
(	O
a	O
,	O
b	O
)	O
has	O
the	O
same	O
relation	O
as	O
the	O
word	O
pair	O
(	O
c	O
,	O
d	O
)	O
.	O
Finally	O
,	O
for	O
the	O
SAT	O
†	O
dataset	O
,	O
we	O
compare	O
with	O
the	O
published	O
results	O
from	O
GPT	B-MethodName
-	O
3	O
(	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
,	O
LRA	B-DatasetName
(	O
Turney	O
,	O
2005	O
)	O
and	O
SuperSim	O
(	O
Turney	O
,	O
2013	O
)	O
;	O
for	O
relation	B-TaskName
classification	I-TaskName
we	O
report	O
the	O
published	O
results	O
of	O
the	O
LexNet	O
(	O
Shwartz	O
et	O
al	O
,	O
2016	O
)	O
and	O
SphereRE	O
relation	B-TaskName
classification	I-TaskName
models	O
,	O
taking	O
the	O
results	O
from	O
the	O
latter	O
publication	O
.	O
We	O
did	O
not	O
reproduce	O
these	O
latter	O
methods	O
in	O
similar	O
conditions	O
as	O
our	O
work	O
,	O
and	O
hence	O
they	O
are	O
not	O
fully	O
comparable	O
.	O
More	O
-	O
over	O
,	O
these	O
approaches	O
are	O
a	O
different	O
nature	O
,	O
as	O
the	O
aim	O
of	O
our	O
work	O
is	O
to	O
provide	O
universal	O
relation	O
embeddings	O
instead	O
of	O
task	O
-	O
specific	O
models	O
.	O

Table	O
2	O
shows	O
the	O
accuracy	B-MetricName
on	O
the	O
analogy	O
benchmarks	O
.	O
The	O
RelBERT	O
models	O
substantially	O
outperform	O
the	O
baselines	O
on	O
all	O
datasets	O
,	O
except	O
for	O
the	O
Google	B-DatasetName
analogy	O
dataset	O
.	O
9	O
Comparing	O
the	O
different	O
prompt	O
generation	O
approaches	O
,	O
we	O
can	O
see	O
that	O
,	O
surprisingly	O
,	O
the	O
manual	O
prompt	O
consistently	O
outperforms	O
the	O
automatically	O
-	O
learned	O
prompt	O
strategies	O
.	O
It	O
can	O
furthermore	O
be	O
noted	O
that	O
the	O
other	O
two	O
relation	O
embedding	O
methods	O
(	O
i.e.	O
pair2vec	O
and	O
REL	O
-	O
ATIVE	O
)	O
perform	O
poorly	O
in	O
this	O
unsupervised	O
task	O
.	O
The	O
analogical	O
proportion	O
score	O
from	O
Ushio	O
et	O
al	O
(	O
2021	O
)	O
also	O
underperforms	O
RelBERT	O
,	O
even	O
when	O
tuned	O
on	O
dataset	O
-	O
specific	O
tuning	O
data	O
.	O

Table	O
3	O
summarizes	O
the	O
results	O
of	O
the	O
lexical	O
relation	B-TaskName
classification	I-TaskName
experiments	O
,	O
in	O
terms	O
of	O
macro	O
and	O
micro	O
averaged	O
F1	B-MetricName
score	I-MetricName
.	O
The	O
RelBERT	O
models	O
achieve	O
the	O
best	O
results	O
on	O
all	O
datasets	O
except	O
for	O
BLESS	O
and	O
K&H+N	O
,	O
where	O
the	O
performance	O
of	O
all	O
models	O
is	O
rather	O
close	O
.	O
We	O
observe	O
a	O
particularly	O
large	O
improvement	O
over	O
the	O
word	O
embedding	O
and	O
SotA	O
models	O
on	O
the	O
EVALution	B-DatasetName
dataset	O
.	O
When	O
comparing	O
the	O
different	O
prompting	O
strategies	O
,	O
we	O
again	O
find	O
that	O
the	O
manual	O
prompts	O
perform	O
surprisingly	O
well	O
,	O
although	O
the	O
best	O
results	O
are	O
now	O
obtained	O
with	O
learned	O
prompts	O
in	O
a	O
few	O
cases	O
.	O
9	O
The	O
Google	B-DatasetName
analogy	O
dataset	O
has	O
been	O
shown	O
to	O
be	O
biased	O
toward	O
word	B-TaskName
similarity	I-TaskName
and	O
therefore	O
to	O
be	O
well	O
suited	O
to	O
word	B-TaskName
embeddings	I-TaskName
(	O
Linzen	O
,	O
2016	O
;	O
Rogers	O
et	O
al	O
,	O
2017	O

We	O
show	O
comparisons	O
of	O
versions	O
of	O
RelBERT	O
with	O
optimized	O
prompt	O
with	O
/	O
without	O
finetuning	O
.	O
Figure	O
4	O
shows	O
the	O
absolute	O
accuracy	B-MetricName
drop	O
from	O
RelBERT	O
(	O
i.e.	O
the	O
model	O
with	O
fine	O
-	O
tuning	O
)	O
to	O
the	O
vanilla	O
RoBERTa	B-MethodName
model	O
(	O
i.e.	O
without	O
fine	O
-	O
tuning	O
)	O
with	O
the	O
same	O
prompt	O
.	O
In	O
all	O
cases	O
,	O
the	O
accuracy	B-MetricName
drop	O
for	O
the	O
models	O
without	O
fine	O
-	O
tuning	O
is	O
substantial	O
.	O

We	O
use	O
RoBERTa	B-MethodName
in	O
our	O
main	O
experiments	O
and	O
here	O
we	O
train	O
RelBERT	O
with	O
ALBERT	B-MethodName
and	O
BERT	B-MethodName
instead	O
,	O
and	O
evaluate	O
them	O
on	O
both	O
of	O
the	O
analogy	O
and	O
relation	B-TaskName
classification	I-TaskName
tasks	O
.	O
Table	O
7	O
shows	O
the	O
accuracy	B-MetricName
on	O
the	O
analogy	O
questions	O
,	O
while	O
Table	O
8	O
shows	O
the	O
accuracy	B-MetricName
on	O
the	O
relation	B-TaskName
classification	I-TaskName
task	O
.	O
In	O
both	O
tasks	O
,	O
we	O
can	O
confirm	O
that	O
RoBERTa	B-MethodName
achieves	O
the	O
best	O
performance	O
within	O
the	O
LMs	O
,	O
by	O
a	O
relatively	O
large	O
margin	O
in	O
most	O
cases	O
.	O

Table	O
10	O
shows	O
the	O
best	O
prompt	O
configuration	O
based	O
on	O
the	O
validation	O
loss	B-MetricName
for	O
the	O
SemEval	O
2012	O
Task	O
2	O
dataset	O
in	O
our	O
main	O
experiments	O
using	O
RoBERTa	B-MethodName
.	O

All	O
the	O
trigger	O
tokens	O
are	O
initialized	O
by	O
mask	O
tokens	O
and	O
updated	O
based	O
on	O
the	O
gradient	O
of	O
a	O
loss	B-MetricName
function	O
L	O
t	O
.	O
Concretely	O
,	O
let	O
us	O
denote	O
the	O
loss	B-MetricName
value	O
with	O
template	O
T	O
as	O
L	O
t	O
(	O
T	O
)	O
.	O
The	O
candidate	O
set	O
for	O
the	O
j	O
th	O
trigger	O
is	O
derived	O
bỹ	O
W	O
j	O
=	O
top	O
-	O
k	O
w	O
W	O
e	O
T	O
w	O
∇	O
j	O
L	O
t	O
(	O
T	O
)	O
(	O
2	O
)	O
where	O
the	O
gradient	O
is	O
taken	O
with	O
respect	O
to	O
j	O
th	O
trigger	O
token	O
and	O
e	O
w	O
is	O
the	O
input	O
embedding	O
for	O
the	O
word	O
w.	O
Then	O
we	O
evaluate	O
each	O
token	O
based	O
on	O
the	O
loss	B-MetricName
function	O
as	O
z	O
j	O
=	O
argmin	O
w	O
W	O
j	O
L	O
t	O
(	O
rep	O
(	O
T	O
,	O
j	O
,	O
w	O
)	O
)	O
(	O
3	O
)	O
where	O
rep	O
(	O
T	O
,	O
j	O
,	O
w	O
)	O
replaces	O
the	O
j	O
th	O
token	O
in	O
T	O
by	O
w	O
and	O
j	O
is	O
randomly	O
chosen	O
.	O
We	O
ignore	O
any	O
candidates	O
that	O
do	O
not	O
improve	O
current	O
loss	B-MetricName
value	O
to	O
further	O
enhance	O
the	O
prompt	O
quality	O
.	O

Text	B-TaskName
classification	I-TaskName
aims	O
at	O
mapping	O
documents	O
into	O
a	O
set	O
of	O
predefined	O
categories	O
.	O
Supervised	O
machine	O
learning	O
models	O
have	O
shown	O
great	O
success	O
in	O
this	O
area	O
but	O
they	O
require	O
a	O
large	O
number	O
of	O
labeled	O
documents	O
to	O
reach	O
adequate	O
accuracy	B-MetricName
.	O
This	O
is	O
particularly	O
true	O
when	O
the	O
number	O
of	O
target	O
categories	O
is	O
in	O
the	O
tens	O
or	O
the	O
hundreds	O
.	O
In	O
this	O
work	O
,	O
we	O
explore	O
an	O
unsupervised	O
approach	O
to	O
classify	O
documents	O
into	O
categories	O
simply	O
described	O
by	O
a	O
label	O
.	O
The	O
proposed	O
method	O
is	O
inspired	O
by	O
the	O
way	O
a	O
human	O
proceeds	O
in	O
this	O
situation	O
:	O
It	O
draws	O
on	O
textual	O
similarity	O
between	O
the	O
most	O
relevant	O
words	O
in	O
each	O
document	O
and	O
a	O
dictionary	O
of	O
keywords	O
for	O
each	O
category	O
reflecting	O
its	O
semantics	O
and	O
lexical	O
field	O
.	O
The	O
novelty	O
of	O
our	O
method	O
hinges	O
on	O
the	O
enrichment	O
of	O
the	O
category	O
labels	O
through	O
a	O
combination	O
of	O
human	O
expertise	O
and	O
language	O
models	O
,	O
both	O
generic	O
and	O
domain	O
specific	O
.	O
Our	O
experiments	O
on	O
5	O
standard	O
corpora	O
show	O
that	O
the	O
proposed	O
method	O
increases	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
over	O
relying	O
solely	O
on	O
human	O
expertise	O
and	O
can	O
also	O
be	O
on	O
par	O
with	O
simple	O
supervised	O
approaches	O
.	O
It	O
thus	O
provides	O
a	O
practical	O
alternative	O
to	O
situations	O
where	O
lowcost	O
text	B-TaskName
categorization	I-TaskName
is	O
needed	O
,	O
as	O
we	O
illustrate	O
with	O
our	O
application	O
to	O
operational	O
risk	O
incidents	O
classification	O
.	O

In	O
this	O
review	O
of	O
relevant	O
work	O
,	O
we	O
focus	O
predominantly	O
on	O
techniques	O
that	O
have	O
been	O
proposed	O
to	O
overcome	O
the	O
requirement	O
of	O
having	O
a	O
large	O
number	O
of	O
annotated	O
data	O
for	O
standard	O
text	B-TaskName
classification	I-TaskName
techniques	O
.	O
Overall	O
,	O
the	O
majority	O
of	O
approaches	O
focus	O
on	O
generating	O
labeled	O
examples	O
without	O
full	O
manual	O
annotation	O
.	O
Those	O
include	O
semi	O
-	O
supervised	O
techniques	O
that	O
seek	O
to	O
leverage	O
a	O
small	O
set	O
of	O
labeled	O
documents	O
to	O
derive	O
labels	O
for	O
the	O
remainder	O
of	O
the	O
corpus	O
.	O
For	O
instance	O
,	O
Nigam	O
et	O
al	O
(	O
2000	O
)	O
propose	O
to	O
follow	O
the	O
Expectation	O
-	O
Maximization	O
(	O
EM	B-MetricName
)	O
algorithm	O
by	O
iteratively	O
using	O
the	O
set	O
of	O
labeled	O
data	O
to	O
obtain	O
probabilistically	O
-	O
weighted	O
class	O
labels	O
for	O
each	O
unlabeled	O
document	O
and	O
then	O
training	O
a	O
classifier	O
on	O
the	O
complete	O
corpus	O
based	O
on	O
those	O
annotations	O
.	O
This	O
process	O
is	O
repeated	O
until	O
convergence	O
of	O
the	O
log	O
likelihood	O
of	O
the	O
parameters	O
given	O
observed	O
data	O
.	O
Other	O
approaches	O
attempt	O
to	O
automatically	O
derive	O
labels	O
without	O
any	O
starting	O
set	O
of	O
annotations	O
.	O
For	O
instance	O
,	O
Turney	O
(	O
2002	O
)	O
classifies	O
a	O
review	O
as	O
recommended	O
or	O
not	O
recommended	O
by	O
computing	O
the	O
pointwise	O
mutual	O
information	O
of	O
the	O
words	O
in	O
the	O
review	O
with	O
a	O
positive	O
reference	O
word	O
(	O
excellent	O
)	O
and	O
with	O
a	O
negative	O
reference	O
word	O
(	O
poor	O
)	O
using	O
search	O
engine	O
results	O
as	O
a	O
proxy	O
for	O
a	O
reference	O
corpus	O
.	O
Another	O
example	O
is	O
Ko	O
and	O
Seo	O
(	O
2000	O
)	O
who	O
leverage	O
an	O
initial	O
set	O
of	O
manually	O
provided	O
keywords	O
for	O
each	O
target	O
category	O
to	O
derive	O
labels	O
.	O
Based	O
on	O
those	O
key	O
-	O
words	O
,	O
they	O
look	O
for	O
representative	O
sentences	O
in	O
the	O
corpus	O
to	O
support	O
label	O
assignment	O
.	O
Finally	O
,	O
Yang	O
et	O
al	O
(	O
2013	O
)	O
make	O
use	O
of	O
wikipedia	O
as	O
background	O
knowledge	O
to	O
assemble	O
representative	O
set	O
of	O
words	O
for	O
each	O
category	O
label	O
via	O
topic	O
modeling	O
and	O
use	O
them	O
to	O
annotate	O
the	O
unlabeled	O
documents	O
.	O
In	O
a	O
similar	O
way	O
,	O
Miller	O
et	O
al	O
(	O
2016	O
)	O
represent	O
each	O
target	O
category	O
as	O
a	O
TF	O
-	O
IDF	O
(	O
termfrequency	O
/	O
inverse	O
document	O
frequency	O
)	O
vector	O
obtained	O
from	O
Wikipedia	O
and	O
then	O
use	O
this	O
category	O
representation	O
as	O
an	O
informed	O
prior	O
to	O
Latent	O
Dirichlet	O
Allocation	O
(	O
LDA	B-MethodName
)	O
,	O
an	O
unsupervised	O
algorithm	O
that	O
finds	O
the	O
topics	O
that	O
best	O
satisfy	O
the	O
data	O
given	O
the	O
priors	O
.	O
The	O
occurrence	O
of	O
these	O
topics	O
in	O
a	O
document	O
can	O
be	O
used	O
as	O
a	O
noisy	O
label	O
for	O
that	O
document	O
.	O
Our	O
approach	O
differs	O
in	O
spirit	O
in	O
the	O
sense	O
that	O
our	O
objective	O
is	O
not	O
to	O
construct	O
surrogate	O
labels	O
so	O
that	O
we	O
can	O
apply	O
a	O
machine	O
learning	O
classifier	O
to	O
our	O
unlabeled	O
data	O
.	O
By	O
contrast	O
,	O
we	O
opted	O
for	O
a	O
fully	O
unsupervised	O
method	O
which	O
hinges	O
on	O
computing	O
a	O
similarity	O
metric	O
between	O
documents	O
and	O
target	O
categories	O
.	O
To	O
that	O
end	O
,	O
a	O
richer	O
representation	O
of	O
category	O
labels	O
is	O
derived	O
.	O
The	O
method	O
that	O
were	O
proposed	O
by	O
Yang	O
et	O
al	O
(	O
2013	O
)	O
;	O
Miller	O
et	O
al	O
(	O
2016	O
)	O
could	O
be	O
adapted	O
to	O
align	O
with	O
our	O
perspective	O
(	O
by	O
removing	O
the	O
classification	O
step	O
)	O
.	O
Other	O
examples	O
of	O
unsupervised	O
approach	O
include	O
Rao	O
et	O
al	O
(	O
2006	O
)	O
which	O
defined	O
the	O
label	O
of	O
documents	O
based	O
on	O
a	O
k	O
-	O
means	O
word	O
clustering	O
.	O
They	O
select	O
a	O
set	O
of	O
representative	O
words	O
from	O
each	O
cluster	O
as	O
a	O
label	O
and	O
derive	O
a	O
set	O
of	O
candidate	O
labels	O
.	O
An	O
input	O
document	O
vector	O
is	O
then	O
assigned	O
to	O
the	O
label	O
vector	O
that	O
maximizes	O
the	O
norm	O
of	O
the	O
dotproduct	O
.	O
While	O
this	O
approach	O
performs	O
well	O
when	O
there	O
are	O
no	O
categories	O
specified	O
as	O
input	O
,	O
e.g.	O
,	O
social	O
listening	O
,	O
trend	O
monitoring	O
,	O
topic	O
modeling	O
,	O
it	O
is	O
less	O
likely	O
to	O
do	O
so	O
with	O
a	O
set	O
of	O
predefined	O
target	O
categories	O
where	O
it	O
is	O
difficult	O
to	O
steer	O
word	O
clusters	O
to	O
categories	O
of	O
interest	O
and	O
,	O
critically	O
,	O
to	O
ensure	O
the	O
full	O
coverage	O
of	O
target	O
categories	O
(	O
new	O
internal	O
taxonomy	O
of	O
risk	O
in	O
our	O
practical	O
case	O
)	O
.	O
Finally	O
,	O
our	O
method	O
makes	O
use	O
of	O
word	B-TaskName
embeddings	I-TaskName
as	O
a	O
mean	O
to	O
enrich	O
category	O
label	O
via	O
semantic	O
expansion	O
.	O
As	O
far	O
as	O
we	O
know	O
,	O
word	B-TaskName
embeddings	I-TaskName
have	O
been	O
used	O
to	O
improve	O
text	B-TaskName
classification	I-TaskName
performance	O
through	O
their	O
application	O
as	O
a	O
document	O
representation	O
technique	O
.	O
In	O
Liu	O
et	O
al	O
(	O
2018	O
)	O
,	O
the	O
authors	O
show	O
that	O
task	O
oriented	O
embeddings	O
,	O
which	O
penalise	O
outputs	O
where	O
the	O
representative	O
words	O
of	O
a	O
category	O
are	O
close	O
to	O
the	O
representative	O
words	O
of	O
another	O
category	O
,	O
outperform	O
general	O
domain	O
embeddings	O
.	O
As	O
we	O
do	O
not	O
have	O
any	O
labeled	O
data	O
,	O
this	O
approach	O
is	O
not	O
directly	O
relevant	O
to	O
our	O
problem	O
setting	O
.	O

In	O
our	O
method	O
,	O
an	O
offline	O
process	O
is	O
used	O
to	O
extract	O
initial	O
keywords	O
from	O
category	O
labels	O
.	O
For	O
the	O
purpose	O
of	O
testing	O
our	O
approach	O
,	O
we	O
had	O
to	O
emulate	O
human	O
experts	O
ourselves	O
.	O
For	O
each	O
category	O
,	O
one	O
team	O
member	O
added	O
a	O
few	O
keywords	O
based	O
only	O
on	O
label	O
description	O
.	O
Then	O
,	O
we	O
randomly	O
selected	O
2	O
or	O
3	O
documents	O
for	O
each	O
label	O
that	O
were	O
read	O
by	O
two	O
team	O
members	O
who	O
used	O
them	O
to	O
identify	O
5	O
to	O
10	O
salient	O
words	O
to	O
be	O
added	O
to	O
each	O
dictionary	O
.	O
In	O
average	O
,	O
we	O
manually	O
added	O
9	O
words	O
per	O
label	O
for	O
20NewsGroup	O
,	O
17	O
words	O
for	O
AGs	O
Corpus	O
and	O
Google	B-DatasetName
-	O
Snippets	O
,	O
11	O
words	O
for	O
Yahoo	O
-	O
Answers	O
and	O
14	O
words	O
for	O
5AbstractsGroup	O
.	O
We	O
present	O
in	O
Table	O
2	O
,	O
the	O
output	O
of	O
that	O
process	O
for	O
the	O
AGs	O
Corpus	O
dataset	O
.	O
Once	O
we	O
identify	O
initial	O
keywords	O
,	O
we	O
make	O
the	O
series	O
of	O
enrichment	O
steps	O
described	O
in	O
section	O
3.2	O
.	O
For	O
every	O
word	O
in	O
the	O
set	O
of	O
initial	O
keywords	O
,	O
we	O
add	O
all	O
its	O
synonym	O
sets	O
from	O
WordNet	O
as	O
well	O
as	O
the	O
10	O
most	O
similar	O
words	O
from	O
Glove	O
,	O
CBOW	O
and	O
Skip	O
-	O
Gram	O
.	O
The	O
average	O
length	O
of	O
label	O
dictionaries	O
obtained	O
from	O
the	O
full	O
enrichment	O
pipeline	O
(	O
which	O
we	O
refer	O
to	O
as	O
all	O
keywords	O
)	O
is	O
428	O
words	O
.	O
We	O
use	O
the	O
word2vec	O
python	O
implementation	O
provided	O
by	O
gensim	O
(	O
Rehurek	O
and	O
Sojka	O
,	O
2010	O
)	O
.	O
For	O
Skip	O
-	O
gram	O
and	O
CBOW	O
,	O
a	O
10	O
-	O
word	O
window	O
size	O
is	O
used	O
to	O
provide	O
the	O
same	O
amount	O
of	O
raw	O
information	O
.	O
Also	O
words	O
appearing	O
3	O
times	O
or	O
fewer	O
are	O
filtered	O
out	O
,	O
10	O
workers	O
were	O
used	O
and	O
train	O
-	O
ing	O
was	O
performed	O
in	O
100	O
epochs	O
.	O
We	O
chose	O
300	O
for	O
the	O
size	O
of	O
all	O
word	B-TaskName
embeddings	I-TaskName
,	O
it	O
has	O
been	O
reported	O
to	O
perform	O
well	O
in	O
classification	O
tasks	O
(	O
Mikolov	O
et	O
al	O
,	O
2013a	O
)	O
.	O
Filtering	O
word	O
dictionaries	O
with	O
the	O
Functionaware	O
Component	O
(	O
FAC	O
)	O
allowed	O
to	O
keep	O
in	O
average	O
37	O
%	O
of	O
all	O
keywords	O
per	O
label	O
.	O
As	O
described	O
previously	O
,	O
once	O
different	O
versions	O
of	O
label	O
dictionaries	O
have	O
been	O
obtained	O
,	O
we	O
calculate	O
their	O
similarity	O
with	O
input	O
documents	O
using	O
LSA	O
and	O
Cosine	O
distance	O
.	O
The	O
optimal	O
dimension	O
(	O
k	O
)	O
of	O
the	O
latent	O
space	O
depends	O
on	O
the	O
dataset	O
.	O
Optimal	O
k	O
values	O
are	O
typically	O
in	O
the	O
range	O
of	O
100	O
-	O
300	O
dimensions	O
(	O
Harman	O
,	O
1993	O
;	O
Letsche	O
and	O
Berry	O
,	O
1997	O
)	O
.	O
In	O
this	O
work	O
,	O
for	O
each	O
dataset	O
,	O
we	O
set	O
a	O
range	O
of	O
100	O
-	O
300	O
values	O
,	O
and	O
we	O
determine	O
the	O
optimal	O
k	O
by	O
maximizing	O
the	O
topic	O
coherence	O
score	O
(	O
Röder	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
performance	O
was	O
evaluated	O
in	O
terms	O
of	O
precision	O
(	O
Prec	O
.	O
)	O
,	O
recall	O
(	O
Rec	O
.	O
)	O
and	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
(	O
F1	B-MetricName
)	O
.	O
All	O
measures	O
are	O
computed	O
based	O
on	O
a	O
weighted	O
average	O
of	O
each	O
class	O
using	O
the	O
number	O
of	O
true	O
instances	O
to	O
determine	O
the	O
weights	O
.	O

Table	O
3	O
summarizes	O
the	O
performance	O
of	O
each	O
of	O
the	O
methods	O
tested	O
on	O
the	O
five	O
corpora	O
that	O
we	O
considered	O
.	O
Overall	O
,	O
the	O
various	O
configurations	O
of	O
our	O
method	O
,	O
all	O
leveraging	O
embeddings	O
for	O
semantic	O
expansion	O
,	O
outperform	O
the	O
simple	O
unsupervised	O
baselines	O
,	O
leading	O
to	O
a	O
doubling	O
of	O
the	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
for	O
all	O
corpora	O
,	O
the	O
least	O
affected	O
being	O
the	O
5Abstracts	O
-	O
Group	O
where	O
F1	B-MetricName
goes	O
from	O
38.1	O
to	O
68.3	O
percent	O
,	O
comparing	O
with	O
the	O
all	O
keywords	O
variant	O
of	O
our	O
method	O
.	O
When	O
focusing	O
on	O
our	O
various	O
configurations	O
,	O
first	O
without	O
the	O
FAC	O
consolidation	O
,	O
we	O
observe	O
that	O
domain	O
specific	O
embeddings	O
alone	O
lead	O
to	O
better	O
performance	O
than	O
generic	O
embeddings	O
alone	O
and	O
this	O
across	O
all	O
corpora	O
and	O
all	O
metrics	O
,	O
except	O
for	O
the	O
Yahoo	O
-	O
Answers	O
dataset	O
.	O
The	O
difference	O
in	O
performance	O
however	O
is	O
not	O
very	O
large	O
,	O
with	O
the	O
exception	O
of	O
20NewsGroup	O
where	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
increases	O
from	O
52.6	O
with	O
generic	O
embeddings	O
to	O
61	O
with	O
domain	O
specific	O
ones	O
.	O
We	O
notice	O
also	O
that	O
combining	O
all	O
enrichments	O
(	O
All	O
keywords	O
)	O
provides	O
a	O
modest	O
increase	O
in	O
performance	O
over	O
embeddings	O
only	O
as	O
shown	O
by	O
the	O
results	O
for	O
Yahoo	O
-	O
Answers	O
,	O
5AbstractsGroup	O
and	O
Google	B-DatasetName
-	O
Snippets	O
.	O
Finally	O
the	O
use	O
of	O
the	O
consolidation	O
step	O
further	O
improves	O
performance	O
except	O
for	O
20NewsGroup	O
where	O
precision	O
increases	O
from	O
64.7	O
to	O
71.1	O
but	O
recall	O
decreases	O
from	O
57.8	O
to	O
35.6	O
.	O
Comparing	O
now	O
our	O
best	O
unsupervised	O
performance	O
with	O
the	O
supervised	O
baseline	O
,	O
we	O
observe	O
that	O
the	O
ratio	O
of	O
the	O
best	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
performance	O
over	O
the	O
supervised	O
baseline	O
performance	O
varies	O
from	O
0.71	O
to	O
1.11	O
with	O
two	O
datasets	O
yielding	O
ratios	O
above	O
1	O
.	O
Such	O
results	O
demonstrate	O
the	O
validity	O
of	O
the	O
unsupervised	O
approach	O
as	O
a	O
practical	O
alternative	O
to	O
investing	O
to	O
a	O
cognitively	O
and	O
timely	O
costly	O
annotation	O
effort	O
.	O

In	O
our	O
application	O
,	O
we	O
were	O
asked	O
to	O
map	O
both	O
internal	O
incidents	O
and	O
external	O
incidents	O
to	O
the	O
new	O
taxonomy	O
.	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
the	O
external	O
incidents	O
for	O
confidentiality	O
reasons	O
.	O
More	O
precisely	O
,	O
our	O
task	O
was	O
to	O
assign	O
a	O
unique	O
category	O
to	O
each	O
one	O
of	O
the	O
25	O
,	O
000	O
incidents	O
that	O
was	O
obtained	O
from	O
ORX	O
news	O
.	O
The	O
Operational	O
Risk	O
Exchange	O
(	O
ORX	O
)	O
is	O
a	O
consortium	O
of	O
financial	O
institutions	O
focused	O
on	O
operational	O
risk	O
information	O
sharing	O
.	O
The	O
ORX	O
news	O
service	O
provides	O
publicly	O
reported	O
operational	O
risk	O
loss	B-MetricName
data	O
to	O
its	O
institutional	O
members	O
.	O
An	O
incident	O
record	O
is	O
mostly	O
composed	O
of	O
an	O
incident	O
description	O
along	O
with	O
associated	O
metainformation	O
such	O
as	O
geographical	O
indicators	O
,	O
time	O
information	O
and	O
institution	O
affected	O
.	O
We	O
only	O
make	O
use	O
of	O
the	O
incident	O
descriptions	O
.	O
Their	O
average	O
length	O
is	O
2150	O
words	O
,	O
with	O
a	O
standard	O
deviation	O
of	O
1181	O
words	O
and	O
ranging	O
from	O
10	O
words	O
to	O
more	O
than	O
14000	O
words	O
.	O
The	O
target	O
taxonomy	O
is	O
composed	O
of	O
three	O
levels	O
.	O
The	O
first	O
one	O
contains	O
16	O
labels	O
and	O
indicates	O
at	O
a	O
very	O
high	O
level	O
the	O
domain	O
of	O
the	O
incidents	O
such	O
as	O
IT	O
,	O
legal	O
,	O
regulatory	O
.	O
The	O
second	O
and	O
third	O
levels	O
contain	O
respectively	O
69	O
and	O
264	O
levels	O
to	O
add	O
increasing	O
granularity	O
to	O
the	O
incident	O
classification	O
.	O
Figure	O
2	O
presents	O
an	O
extract	O
of	O
the	O
taxonomy	O
focused	O
on	O
ICT	O
risk	O
,	O
which	O
is	O
public	O
as	O
it	O
draws	O
upon	O
Article	O
107	O
(	O
3	O
)	O
of	O
Directive	O
2013/36	O
/	O
EU2	O
which	O
aim	O
to	O
ensure	O
the	O
convergence	O
of	O
supervisory	O
practices	O
in	O
the	O
assessment	O
of	O
the	O
information	O
and	O
communication	O
technology	O
(	O
ICT	O
)	O
risk	O
.	O
Before	O
discussing	O
the	O
results	O
,	O
we	O
thought	O
it	O
would	O
be	O
meaningful	O
to	O
point	O
out	O
some	O
of	O
the	O
characteristics	O
of	O
this	O
application	O
.	O
One	O
natural	O
challenge	O
in	O
real	O
world	O
cases	O
is	O
the	O
lack	O
of	O
unequivocal	O
ground	O
truth	O
.	O
Experts	O
can	O
often	O
identify	O
categories	O
that	O
do	O
not	O
correspond	O
to	O
the	O
input	O
but	O
in	O
the	O
end	O
,	O
they	O
can	O
not	O
ascertain	O
whether	O
one	O
category	O
should	O
prevail	O
over	O
another	O
unless	O
there	O
is	O
some	O
clear	O
guidelines	O
or	O
convention	O
at	O
the	O
level	O
of	O
the	O
organization	O
.	O
That	O
difficulty	O
is	O
further	O
compounded	O
in	O
our	O
case	O
as	O
most	O
documents	O
are	O
very	O
dense	O
in	O
term	O
of	O
information	O
and	O
become	O
ambiguous	O
.	O
For	O
instance	O
,	O
"	O
In	O
Japan	O
,	O
a	O
building	O
destruction	O
resulting	O
from	O
a	O
massive	O
earthquake	O
has	O
caused	O
power	O
outage	O
making	O
AMD	O
-	O
based	O
servers	O
unbootable	O
"	O
,	O
could	O
be	O
classified	O
as	O
Natural	O
Disaster	B-DatasetName
,	O
Dysfunctional	O
ICT	O
data	O
processing	O
or	O
handling	O
or	O
Destruction	O
/	O
loss	B-MetricName
of	O
physical	O
assets	O
among	O
others	O
.	O

Idioms	O
and	O
collocations	O
are	O
special	O
types	O
of	O
phrases	O
in	O
many	O
languages	O
.	O
An	O
idiom	O
is	O
a	O
phrase	O
whose	O
meaning	O
can	O
not	O
be	O
obtained	O
compositionally	O
,	O
i.e.	O
,	O
by	O
combining	O
the	O
meanings	O
of	O
the	O
words	O
that	O
compose	O
it	O
.	O
Collocations	O
are	O
phrases	O
in	O
which	O
there	O
is	O
a	O
semantic	O
association	O
between	O
the	O
component	O
words	O
and	O
some	O
restrictions	O
on	O
which	O
words	O
can	O
be	O
replaced	O
and	O
which	O
can	O
not	O
.	O
In	O
short	O
,	O
collocations	O
are	O
arbitrarily	O
restricted	O
lexeme	O
combinations	O
such	O
as	O
look	O
into	O
and	O
fully	O
aware	O
.	O
Many	O
scientists	O
from	O
diverse	O
fields	O
have	O
worked	O
on	O
the	O
challenging	O
tasks	O
of	O
automated	O
collocation	O
and	O
idiom	O
extraction	O
,	O
e.g.	O
,	O
see	O
(	O
Garg	O
and	O
Goyal	O
,	O
2014	O
;	O
Seretan	O
,	O
2013	O
;	O
Verma	O
and	O
Vuppuluri	O
,	O
2015	O
;	O
Verma	O
et	O
al	O
,	O
2016	O
)	O
and	O
the	O
references	O
contained	O
therein	O
,	O
yet	O
there	O
is	O
no	O
multi	O
-	O
purpose	O
,	O
ready	O
-	O
to	O
-	O
use	O
,	O
and	O
flexible	O
system	O
for	O
extracting	O
these	O
phrases	O
.	O
Collocation	O
and	O
its	O
special	O
forms	O
,	O
such	O
as	O
idioms	O
,	O
can	O
be	O
useful	O
in	O
many	O
important	O
tasks	O
,	O
e.g.	O
,	O
summarization	B-TaskName
(	O
Barrera	O
and	O
Verma	O
,	O
2012	O
)	O
,	O
question	O
-	O
answering	O
(	O
Barrera	O
et	O
al	O
,	O
2011	O
)	O
,	O
language	O
translation	O
,	O
topic	O
segmentation	O
,	O
authorial	O
style	O
,	O
and	O
so	O
on	O
.	O
As	O
a	O
result	O
,	O
a	O
tool	O
for	O
these	O
tasks	O
would	O
be	O
very	O
handy	O
.	O
To	O
tackle	O
this	O
void	O
,	O
we	O
introduce	O
a	O
feature	O
-	O
rich	O
system	O
called	O
ICE	O
(	O
short	O
for	O
Idiom	O
and	O
Collocation	O
Extractor	O
)	O
,	O
which	O
has	O
two	O
versions	O
:	O
one	O
is	O
flexible	O
and	O
pipelined	O
seamlessly	O
for	O
research	O
purposes	O
as	O
a	O
component	O
of	O
a	O
larger	O
system	O
such	O
as	O
a	O
question	B-TaskName
answering	I-TaskName
system	O
,	O
and	O
the	O
second	O
as	O
a	O
web	O
-	O
based	O
tool	O
for	O
educational	O
purposes	O
.	O
ICE	O
has	O
a	O
modular	O
architecture	O
and	O
also	O
includes	O
a	O
POS	O
tagger	O
,	O
which	O
can	O
be	O
used	O
alone	O
or	O
as	O
part	O
of	O
collocation	O
or	O
idiom	O
extraction	O
.	O
An	O
experiment	O
with	O
the	O
CoNLL	O
dataset	O
shows	O
that	O
ICE	O
's	O
POS	O
tagger	O
is	O
competitive	O
against	O
the	O
Stanford	O
POS	O
tagger	O
.	O
For	O
ease	O
of	O
use	O
in	O
research	O
,	O
we	O
provide	O
ICE	O
as	O
a	O
python	O
package	O
.	O
For	O
collocation	O
extraction	O
,	O
ICE	O
uses	O
the	O
IR	O
models	O
and	O
techniques	O
introduced	O
by	O
(	O
Verma	O
et	O
al	O
,	O
2016	O
)	O
.	O
These	O
methods	O
include	O
:	O
dictionary	O
search	O
,	O
online	O
dictionaries	O
,	O
a	O
substitution	O
method	O
that	O
compares	O
the	O
Bing	O
hit	O
counts	O
of	O
a	O
phrase	O
against	O
the	O
Bing	O
hit	O
counts	O
of	O
new	O
phrases	O
obtained	O
by	O
substituting	O
the	O
component	O
words	O
of	O
the	O
phrase	O
one	O
at	O
a	O
time	O
to	O
determine	O
the	O
"	O
adherence	O
factor	O
"	O
of	O
the	O
component	O
words	O
in	O
a	O
collocation	O
,	O
and	O
two	O
methods	O
that	O
try	O
to	O
measure	O
the	O
probability	O
of	O
association	O
of	O
the	O
component	O
words	O
again	O
using	O
hit	O
counts	O
.	O
In	O
(	O
Verma	O
et	O
al	O
,	O
2016	O
)	O
,	O
the	O
authors	O
created	O
a	O
gold	O
-	O
standard	O
dataset	O
of	O
collocations	O
by	O
taking	O
100	O
sentences	O
at	O
random	O
from	O
the	O
Wiki50	O
dataset	O
and	O
manually	O
annotating	O
them	O
for	O
collocations	O
(	O
including	O
idioms	O
)	O
using	O
eight	O
volunteers	O
,	O
who	O
used	O
the	O
Oxford	O
Dictionary	O
of	O
Collocations	O
and	O
Oxford	O
Dictionary	O
of	O
Idioms	O
.	O
Each	O
sentence	O
was	O
given	O
to	O
two	O
annotators	O
,	O
who	O
were	O
given	O
25	O
sentences	O
each	O
for	O
annotation	O
,	O
and	O
their	O
work	O
was	O
checked	O
and	O
corrected	O
afterwards	O
by	O
two	O
other	O
people	O
.	O
In	O
creating	O
this	O
dataset	O
,	O
even	O
with	O
the	O
assistance	O
of	O
dictionaries	O
,	O
human	O
performance	O
varied	O
from	O
an	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
about	O
39	O
%	O
to	O
70	O
%	O
for	O
the	O
collocation	O
task	O
.	O
A	O
comparison	O
showed	O
that	O
their	O
combination	O
schemes	O
outperformed	O
existing	O
techniques	O
,	O
such	O
as	O
MWEToolkit	O
(	O
Ramisch	O
et	O
al	O
,	O
2010	O
)	O
and	O
Text	O
-	O
NSP	O
(	O
Banerjee	O
and	O
Pedersen	O
,	O
2003	O
)	O
,	O
with	O
the	O
best	O
method	O
achieving	O
an	O
F1score	O
of	O
around	O
40	O
%	O
on	O
the	O
gold	O
-	O
standard	O
dataset	O
,	O
which	O
is	O
within	O
the	O
range	O
of	O
human	O
performance	O
.	O
For	O
idiom	O
extraction	O
,	O
ICE	O
uses	O
the	O
semanticsbased	O
methods	O
introduced	O
by	O
(	O
Verma	O
and	O
Vuppuluri	O
,	O
2015	O
)	O
.	O
The	O
salient	O
feature	O
of	O
these	O
methods	O
is	O
that	O
they	O
use	O
Bing	O
search	O
for	O
the	O
definition	O
of	O
a	O
given	O
phrase	O
and	O
then	O
check	O
the	O
compositionality	O
of	O
the	O
phrase	O
definition	O
against	O
combinations	O
of	O
the	O
words	O
obtained	O
when	O
a	O
define	O
word	O
query	O
is	O
issued	O
,	O
where	O
the	O
word	O
belongs	O
to	O
the	O
phrase	O
.	O
If	O
there	O
is	O
a	O
difference	O
in	O
the	O
meaning	O
,	O
that	O
phrase	O
is	O
considered	O
an	O
idiom	O
.	O
In	O
(	O
Verma	O
and	O
Vuppuluri	O
,	O
2015	O
)	O
,	O
authors	O
showed	O
that	O
their	O
method	O
outperforms	O
AMALGr	O
(	O
Schneider	O
et	O
al	O
,	O
2014	O
)	O
.	O
Their	O
best	O
method	O
achieved	O
an	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
about	O
95	O
%	O
on	O
the	O
VNC	O
tokens	O
dataset	O
.	O
Thus	O
,	O
ICE	O
includes	O
extraction	O
methods	O
for	O
idioms	O
and	O
collocations	O
that	O
are	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
.	O
Other	O
tools	O
exist	O
for	O
collocation	O
extraction	O
,	O
e.g.	O
,	O
see	O
(	O
Anagnostou	O
and	O
Weir	O
,	O
2006	O
)	O
,	O
in	O
which	O
four	O
methods	O
including	O
Text	O
-	O
NSP	O
are	O
compared	O
.	O

As	O
ICE	O
's	O
algorithms	O
are	O
based	O
on	O
Bing	O
search	O
,	O
users	O
must	O
provide	O
a	O
valid	O
user	O
i	O
d	O
for	O
the	O
Bing	O
API	O
.	O
ICE	O
receives	O
a	O
list	O
of	O
sentences	O
as	O
an	O
input	O
and	O
outputs	O
a	O
list	O
of	O
all	O
collocations	O
and	O
idioms	O
.	O
It	O
first	O
splits	O
the	O
input	O
sentences	O
using	O
NLTK	O
sentence	O
tokenizer	O
,	O
then	O
generates	O
n	O
-	O
grams	O
and	O
part	O
of	O
speech	O
tags	O
.	O
ICE	O
's	O
n	O
-	O
gram	O
generator	O
takes	O
care	O
of	O
punctuation	O
marks	O
and	O
has	O
been	O
shown	O
to	O
be	O
better	O
than	O
NSP	O
's	O
n	O
-	O
gram	O
generator	O
.	O
Finally	O
,	O
the	O
output	O
n	O
-	O
grams	O
are	O
given	O
to	O
the	O
collocation	O
and	O
idiom	O
detection	O
algorithms	O
.	O
Collocation	O
and	O
idiom	O
extraction	O
has	O
been	O
done	O
by	O
the	O
algorithm	O
given	O
by	O
(	O
Verma	O
et	O
al	O
,	O
2016	O
)	O
1	O
and	O
(	O
Verma	O
and	O
Vuppuluri	O
,	O
2015	O
)	O
.	O
For	O
part	O
of	O
speech	O
tagging	O
we	O
combined	O
NLTK	O
's	O
regex	O
tagger	O
with	O
NLTK	O
's	O
N	O
-	O
Gram	O
Tagger	O
to	O
have	O
a	O
better	O
performance	O
on	O
POS	O
tagging	O
.	O
We	O
compared	O
our	O
tagger	O
with	O
Stanford	O
POS	O
tagger	O
(	O
Manning	O
et	O
al	O
,	O
2014	O
)	O
on	O
the	O
CoNLL	O
dataset	O
.	O
2	O
The	O
accuracy	B-MetricName
of	O
our	O
tagger	O
is	O
92.11	O
%	O
,	O
which	O
is	O
Collocation	O
/	O
Idiom	O
Extractor	O
.	O
The	O
collocation	O
extraction	O
technique	O
combines	O
different	O
methods	O
in	O
a	O
pipeline	O
in	O
order	O
to	O
increase	O
precision	O
.	O
Figures	O
1	O
and	O
2	O
show	O
the	O
idiom	O
and	O
collocation	O
extraction	O
system	O
architectures	O
separately	O
.	O
As	O
shown	O
in	O
the	O
diagrams	O
,	O
there	O
are	O
two	O
methods	O
for	O
identifying	O
idioms	O
(	O
called	O
And	O
and	O
Or	O
)	O
and	O
four	O
different	O
methods	O
for	O
identifying	O
collocations	O
including	O
:	O
offline	O
dictionary	O
search	O
,	O
online	O
dictionary	O
search	O
,	O
web	O
search	O
and	O
substitution	O
,	O
and	O
web	O
search	O
and	O
independence	O
.	O
For	O
collocations	O
,	O
ICE	O
pipelines	O
the	O
first	O
and	O
second	O
methods	O
,	O
then	O
pipelines	O
them	O
with	O
the	O
third	O
or	O
the	O
fourth	O
method	O
(	O
both	O
options	O
are	O
available	O
in	O
the	O
code	O
)	O
.	O
These	O
methods	O
are	O
connected	O
sequentially	O
.	O
This	O
means	O
that	O
if	O
something	O
is	O
considered	O
as	O
a	O
collocation	O
in	O
one	O
component	O
,	O
it	O
will	O
be	O
added	O
to	O
the	O
list	O
of	O
collocations	O
and	O
will	O
not	O
be	O
given	O
to	O
the	O
next	O
component	O
(	O
yes	O
/	O
no	O
arrows	O
in	O
the	O
diagram	O
)	O
.	O
Table	O
1	O
shows	O
the	O
description	O
of	O
each	O
component	O
in	O
collocation	O
extractor	O
diagram	O
.	O
The	O
Ngram	O
Extractor	O
receives	O
all	O
sentences	O
and	O
generates	O
n	O
-	O
grams	O
ranging	O
from	O
bigrams	O
up	O
to	O
8grams	O
.	O
It	O
uses	O
NLTK	O
sentence	O
and	O
word	O
tokenizers	O
for	O
generating	O
tokens	O
.	O
Then	O
,	O
it	O
combines	O
the	O
generated	O
tokens	O
together	O
taking	O
care	O
of	O
punctuation	O
to	O
generate	O
the	O
n	O
-	O
grams	O
.	O
Dictionary	O
Check	O
uses	O
WordNet	O
(	O
Miller	O
,	O
1995	O
)	O
as	O
a	O
dictionary	O
and	O
looks	O
up	O
each	O
n	O
-	O
gram	O
to	O
see	O
if	O
it	O
exists	O
in	O
WordNet	O
or	O
not	O
(	O
a	O
collocation	O
should	O
exist	O
in	O
the	O
dictionary	O
)	O
.	O
All	O
n	O
-	O
grams	O
that	O
are	O
considered	O
as	O
non	O
-	O
collocations	O
are	O
given	O
to	O
the	O
next	O
component	O
as	O
input	O
.	O
The	O
next	O
component	O
is	O
Online	O
Dictionary	O
.	O
It	O
searches	O
online	O
dictionaries	O
to	O
see	O
if	O
the	O
n	O
-	O
gram	O
exists	O
in	O
any	O
of	O
them	O
or	O
not	O
.	O
It	O
uses	O
Bing	O
Search	O
API	O
3	O
to	O
search	O
for	O
n	O
-	O
grams	O
in	O
the	O
web	O
.	O
Web	O
Search	O
and	O
Substitution	O
is	O
the	O
next	O
component	O
in	O
the	O
pipeline	O
.	O
This	O
method	O
uses	O
Bing	O
Search	O
API	O
to	O
obtain	O
hit	O
counts	O
for	O
a	O
phrase	O
query	O
.	O
Then	O
each	O
word	O
in	O
the	O
n	O
-	O
gram	O
will	O
be	O
replaced	O
by	O
5	O
random	O
words	O
(	O
one	O
at	O
the	O
time	O
)	O
,	O
and	O
the	O
hit	O
counts	O
are	O
obtained	O
.	O
At	O
the	O
end	O
,	O
we	O
will	O
have	O
a	O
list	O
of	O
hit	O
counts	O
.	O
These	O
values	O
will	O
be	O
used	O
to	O
differentiate	O
between	O
collocations	O
and	O
non	O
-	O
collocations	O
.	O
The	O
last	O
component	O
in	O
the	O
pipeline	O
of	O
collocation	O
extraction	O
is	O
Web	O
Search	O
and	O
Independence	O
.	O
The	O
idea	O
of	O
this	O
method	O
is	O
to	O
check	O
whether	O
the	O
probability	O
of	O
a	O
phrase	O
exceeds	O
the	O
probability	O
that	O
we	O
would	O
expect	O
if	O
the	O
words	O
are	O
independent	O
.	O
It	O
uses	O
hit	O
counts	O
in	O
order	O
to	O
estimate	O
the	O
probabilities	O
.	O
These	O
probabilities	O
are	O
used	O
to	O
differentiate	O
between	O
collocations	O
and	O
non	O
-	O
collocations	O
.	O
When	O
running	O
the	O
collocation	O
extraction	O
function	O
,	O
one	O
of	O
the	O
components	O
should	O
be	O
selected	O
out	O
of	O
the	O
third	O
and	O
fourth	O
ones	O
.	O
The	O
Idiom	O
Extractor	O
diagram	O
is	O
relatively	O
simpler	O
.	O
Given	O
the	O
input	O
n	O
-	O
gram	O
,	O
it	O
creates	O
n	O
+	O
1	O
sets	O
.	O
The	O
first	O
contains	O
(	O
stemmed	O
)	O
words	O
in	O
the	O
meaning	O
of	O
the	O
phrase	O
.	O
The	O
next	O
n	O
sets	O
contain	O
stemmed	O
word	O
in	O
the	O
meaning	O
of	O
each	O
word	O
in	O
the	O
n	O
-	O
gram	O
.	O
Then	O
it	O
applies	O
the	O
set	O
difference	O
operator	O
to	O
n	O
pairs	O
containing	O
the	O
first	O
set	O
and	O
each	O
of	O
the	O
n	O
sets	O
.	O
The	O
Or	O
subsystem	O
considers	O
a	O
phrase	O
as	O
an	O
idiom	O
if	O
at	O
least	O
one	O
word	O
survives	O
one	O
of	O
the	O
subtractions	O
(	O
union	O
of	O
difference	O
sets	O
should	O
be	O
non	O
-	O
empty	O
)	O
.	O
For	O
the	O
And	O
,	O
at	O
least	O
one	O
word	O
has	O
to	O
exist	O
that	O
survived	O
every	O
subtraction	O
(	O
intersection	O
of	O
difference	O
sets	O
should	O
be	O
non	O
-	O
empty	O
)	O
Performance	O
.	O
ICE	O
outperforms	O
both	O
Text	O
-	O
NSP	O
and	O
MWEToolkit	O
.	O
On	O
the	O
gold	O
-	O
standard	O
dataset	O
,	O
ICE	O
's	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
was	O
40.40	O
%	O
,	O
MWE	O
-	O
Toolkit	O
's	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
was	O
18.31	O
%	O
,	O
and	O
Text	O
-	O
NSP	O
had	O
18	O
%	O
.	O
We	O
also	O
compared	O
our	O
idiom	O
extraction	O
with	O
AMALGr	O
method	O
(	O
Schneider	O
et	O
al	O
,	O
2014	O
)	O
on	O
their	O
dataset	O
and	O
the	O
highest	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
achieved	O
by	O
ICE	O
was	O
95	O
%	O
compared	O
to	O
67.42	O
%	O
for	O
AMALGr	O
.	O
For	O
detailed	O
comparison	O
of	O
ICE	O
's	O
collocation	O
and	O
idiom	O
extraction	O
algorithm	O
with	O
existing	O
tools	O
,	O
please	O
refer	O
to	O
(	O
Verma	O
et	O
al	O
,	O
2016	O
)	O
and	O
(	O
Verma	O
and	O
Vuppuluri	O
,	O
2015	O
)	O
.	O
Sample	O
Code	O
.	O
Below	O
is	O
the	O
sample	O
code	O
for	O
using	O
ICE	O
's	O
collocation	O
extraction	O
as	O
part	O
of	O
a	O
bigger	O
system	O
.	O
For	O
idiom	O
extraction	O
you	O
can	O
use	O
IdiomExtractor	O
class	O
instead	O
of	O
collocationExtractor	O
.	O
>	O
>	O
input=	O
[	O
"	O
he	O
and	O
Chazz	O
duel	O
with	O
all	O
keys	O
on	O
the	O
line	O
.	O
"	O
]	O
>	O
>	O
from	O
ICE	O
import	O
CollocationExtractor	O
>	O
>	O
extractor	O
=	O
CollocationExtractor	O
.	O
with_collocation_pipeline	O
(	O
"	O
T1	O
"	O
,	O
bing_key	O
=	O
"	O
Temp	O
"	O
,	O
pos_check	O
=	O
False	O
)	O
>	O
>	O
print	O
(	O
extractor	O
.	O
get_collocations_of_length	O
(	O
input	O
,	O
length	O
=	O
3	O
)	O
)	O
>	O
>	O
[	O
"	O
on	O
the	O
line	O
"	O
]	O
Educational	O
Uses	O
.	O
ICE	O
also	O
has	O
a	O
web	O
-	O
based	O
interface	O
for	O
demonstration	O
and	O
educational	O
purposes	O
.	O
A	O
user	O
can	O
type	O
in	O
a	O
sentence	O
into	O
an	O
input	O
field	O
and	O
get	O
a	O
list	O
of	O
the	O
idioms	O
or	O
collocations	O
in	O
the	O
sentence	O
.	O
A	O
screen	O
-	O
shot	O
of	O
the	O
web	O
-	O
based	O
interface	O
is	O
shown	O
in	O
Figure	O
3	O
.	O
4	O

We	O
train	O
our	O
models	O
on	O
a	O
single	O
NVIDIA	O
K80	O
GPU	O
.	O
We	O
tune	O
hyperparameter	O
values	O
for	O
our	O
model	O
using	O
the	O
validation	O
sets	O
provided	O
by	O
our	O
evaluation	O
datasets	O
;	O
we	O
achieve	O
the	O
best	O
validation	O
performance	O
using	O
8	O
attention	O
blocks	O
per	O
Transformer	B-MethodName
,	O
each	O
with	O
5	O
attention	O
heads	O
,	O
and	O
a	O
hidden	O
size	O
was	O
set	O
to	O
40	O
.	O
The	O
dropout	O
rate	O
was	O
set	O
to	O
0.15	O
;	O
the	O
best	O
learning	B-HyperparameterName
rate	I-HyperparameterName
for	O
IEMOCAP	B-DatasetName
was	O
0.02	O
,	O
while	O
for	O
CMU	O
-	O
MOSI	B-DatasetName
and	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
it	O
was	O
0.01	O
,	O
with	O
batch	O
sizes	O
of	O
32	O
,	O
128	O
,	O
and	O
40	O
,	O
respectively	O
.	O

IEMOCAP	B-DatasetName
[	O
Busso	O
et	O
al	O
,	O
2008	O
]	O
consists	O
of	O
video	O
recordings	O
of	O
151	O
conversation	O
sessions	O
(	O
dialogues	O
)	O
,	O
totaling	O
around	O
6k	O
verbal	O
interactions	O
.	O
This	O
dataset	O
is	O
intended	O
for	O
multilabel	O
emotion	B-TaskName
classification	I-TaskName
;	O
we	O
evaluate	O
on	O
the	O
four	O
labeled	O
emotions	O
(	O
Happy	O
,	O
Sad	O
,	O
Angry	O
,	O
and	O
Neutral	O
)	O
used	O
in	O
previous	O
work	O
[	O
Wang	O
et	O
al	O
,	O
2019	O
]	O
;	O
also	O
following	O
previous	O
work	O
,	O
we	O
report	O
binary	O
accuracy	B-MetricName
and	O
F1	B-MetricName
score	I-MetricName
as	O
the	O
evaluation	O
metrics	O
on	O
this	O
dataset	O
.	O
CMU	O
-	O
MOSI	B-DatasetName
[	O
Zadeh	O
et	O
al	O
,	O
2016	O
]	O
is	O
a	O
sentiment	B-TaskName
analysis	I-TaskName
dataset	O
of	O
2199	O
short	O
monologues	O
labeled	O
in	O
the	O
range	O
[	O
−3	O
,	O
3	O
]	O
,	O
with	O
−3	O
being	O
strongly	O
negative	O
and	O
+3	O
being	O
strongly	O
positive	O
.	O
Following	O
previous	O
work	O
,	O
we	O
report	O
seven	O
-	O
class	O
and	O
binary	O
accuracy	B-MetricName
,	O
F1	B-MetricName
score	I-MetricName
,	O
mean	O
absolute	O
error	O
,	O
and	O
correlation	O
with	O
human	O
judgments	O
.	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
[	O
Zadeh	O
et	O
al	O
,	O
2018b	O
]	O
is	O
a	O
sentiment	O
and	O
emotion	B-DatasetName
analysis	O
dataset	O
of	O
23	O
K	O
movie	O
reviews	O
from	O
YouTube	O
.	O
As	O
with	O
CMU	O
-	O
MOSI	B-DatasetName
,	O
it	O
is	O
labeled	O
in	O
the	O
range	O
of	O
[	O
−3	O
,	O
3	O
]	O
,	O
and	O
its	O
evaluation	O
metrics	O
are	O
the	O
same	O
as	O
in	O
CMU	O
-	O
MOSI	B-DatasetName
.	O

We	O
present	O
the	O
results	O
of	O
our	O
model	O
compared	O
to	O
the	O
reported	O
results	O
of	O
our	O
baseline	O
models	O
in	O
Tables	O
1	O
,	O
2	O
,	O
and	O
3	O
.	O
The	O
best	O
-	O
performing	O
MuLT	O
and	O
FMT	O
models	O
are	O
extremely	O
dense	O
,	O
with	O
around	O
15	O
and	O
77	O
million	O
parameters	O
,	O
respectively	O
.	O
In	O
contrast	O
,	O
our	O
models	O
have	O
between	O
7	O
-	O
9	O
million	O
trainable	O
parameters	O
,	O
depending	O
on	O
the	O
architecture	O
;	O
despite	O
using	O
about	O
half	O
as	O
many	O
parameters	O
as	O
MuLT	O
,	O
we	O
see	O
that	O
our	O
models	O
produce	O
comparable	O
results	O
.	O
We	O
perform	O
fairly	O
well	O
on	O
IEMOCAP	B-DatasetName
,	O
which	O
has	O
around	O
2717	O
training	O
samples	O
;	O
we	O
achieve	O
scores	O
around	O
1	O
-	O
2	O
%	O
below	O
the	O
best	O
-	O
performing	O
model	O
,	O
FMT	O
.	O
Late	O
Fusion	O
models	O
give	O
state	O
of	O
the	O
art	O
results	O
on	O
seven	O
-	O
way	O
and	O
binary	O
accuracy	B-MetricName
,	O
respectively	O
.	O
The	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
dataset	O
is	O
much	O
larger	O
than	O
IEMOCAP	B-DatasetName
and	O
CMU	O
-	O
MOSI	B-DatasetName
,	O
with	O
close	O
to	O
16265	O
training	O
samples	O
.	O
Our	O
models	O
perform	O
the	O
weakest	O
on	O
this	O
dataset	O
,	O
falling	O
short	O
of	O
the	O
state	O
of	O
the	O
art	O
models	O
by	O
around	O
2	O
-	O
3	O
%	O
,	O
suggesting	O
that	O
our	O
models	O
may	O
be	O
too	O
small	O
to	O
learn	O
the	O
entire	O
distribution	O
.	O
Neither	O
MARN	O
[	O
Zadeh	O
et	O
al	O
,	O
2018c	O
]	O
nor	O
FMT	O
reports	O
results	O
on	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
,	O
so	O
they	O
are	O
omitted	O
from	O
Table	O
3	O
.	O
We	O
also	O
experiment	O
with	O
the	O
open	O
source	O
code	O
available	O
for	O
MuLT	O
and	O
FMT	O
(	O
denoted	O
by	O
*	O
)	O
.	O
Using	O
the	O
hyperparameter	O
settings	O
provided	O
2	O
,	O
we	O
were	O
nevertheless	O
unable	O
to	O
match	O
those	O
systems	O
'	O
reported	O
performance	O
,	O
possibly	O
due	O
to	O
differences	O
2	O
Batch	B-HyperparameterName
size	I-HyperparameterName
for	O
FMT	O
*	O
is	O
not	O
given	O
;	O
we	O
use	O
20	O
,	O
the	O
default	O
.	O
resulting	O
from	O
random	O
initialization	O
.	O
In	O
training	O
MuLT	O
*	O
and	O
FMT	O
*	O
,	O
we	O
observe	O
that	O
the	O
models	O
are	O
overfitting	O
,	O
with	O
a	O
mean	O
difference	O
of	O
15	O
-	O
20	O
%	O
between	O
the	O
train	O
and	O
test	O
accuracy	B-MetricName
;	O
in	O
contrast	O
,	O
the	O
largest	O
train	O
-	O
test	O
accuracy	B-MetricName
difference	O
among	O
our	O
three	O
models	O
is	O
only	O
about	O
10	O
%	O
.	O
The	O
smaller	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
in	O
our	O
model	O
reduces	O
the	O
risk	O
of	O
overfitting	O
on	O
smaller	O
datasets	O
,	O
while	O
still	O
achieving	O
good	O
performance	O
on	O
larger	O
datasets	O
.	O

We	O
compare	O
the	O
training	O
time	O
and	O
memory	O
footprint	O
of	O
our	O
models	O
with	O
MuLT	O
*	O
and	O
FMT	O
*	O
in	O
Table	O
4	O
on	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
(	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
needed	O
for	O
MuLT	O
to	O
converge	O
,	O
as	O
reported	O
by	O
Tsai	O
et	O
al	O
[	O
2019	O
]	O
)	O
.	O
On	O
the	O
smallest	O
dataset	O
,	O
CMU	O
-	O
MOSI	B-DatasetName
,	O
training	O
MuLT	O
*	O
took	O
just	O
over	O
seven	O
minutes	O
,	O
while	O
FMT	O
*	O
took	O
2.5	O
hours	O
.	O
Our	O
models	O
train	O
in	O
under	O
three	O
minutes	O
and	O
outperform	O
both	O
MuLT	O
*	O
and	O
FMT	O
*	O
,	O
and	O
this	O
difference	O
in	O
training	O
speed	O
holds	O
for	O
CMU	O
-	O
MOSI	B-DatasetName
and	O
CMU	B-DatasetName
-	I-DatasetName
MOSEI	I-DatasetName
as	O
well	O
.	O
Thus	O
our	O
model	O
,	O
available	O
in	O
the	O
supplementary	O
materials	O
4	O
,	O
is	O
the	O
fastest	O
and	O
best	O
-	O
performing	O
multimodal	O
sentiment	O
system	O
currently	O
available	O
for	O
public	O
use	O
.	O
We	O
also	O
conduct	O
experiments	O
on	O
a	O
substantially	O
reduced	O
IEMOCAP	B-DatasetName
training	O
subset	O
of	O
1284	O
samples	O
,	O
matching	O
the	O
size	O
of	O
CMU	O
-	O
MOSI	B-DatasetName
,	O
which	O
we	O
create	O
by	O
randomly	O
sampling	O
from	O
the	O
full	O
IEMO	O
-	O
CAP	B-DatasetName
training	O
set	O
.	O
Table	O
5	O
shows	O
the	O
results	O
of	O
our	O
models	O
,	O
as	O
well	O
as	O
MuLT	O
*	O
and	O
FMT	O
*	O
,	O
retrained	O
on	O
this	O
smaller	O
IEMOCAP	B-DatasetName
training	O
set	O
,	O
and	O
evaluated	O
on	O
the	O
full	O
IEMOCAP	B-DatasetName
test	O
set	O
.	O
We	O
see	O
that	O
our	O
models	O
,	O
with	O
their	O
smaller	O
numbers	O
of	O
parameters	O
,	O
are	O
better	O
able	O
to	O
learn	O
from	O
limited	O
training	O
data	O
than	O
are	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
with	O
double	O
or	O
more	O
the	O
number	O
of	O
trainable	O
parameters	O
.	O

The	O
first	O
component	O
of	O
our	O
framework	O
MIN	O
is	O
composed	O
of	O
A	O
-	O
LSTM	B-MethodName
and	O
O	O
-	O
LSTM	B-MethodName
.	O
Both	O
LSTMs	O
have	O
extended	O
memories	O
for	O
task	O
-	O
level	O
memory	O
interactions	O
.	O
A	O
-	O
LSTM	B-MethodName
involves	O
a	O
large	O
aspect	O
memory	O
H	O
A	O
t	O
R	O
nm×dim	O
A	O
h	O
and	O
an	O
opinion	O
summary	O
vector	O
m	O
O	O
t	O
R	O
dim	O
O	O
h	O
where	O
H	O
A	O
t	O
contains	O
n	O
m	O
pieces	O
of	O
aspect	O
hidden	O
states	O
of	O
dimension	O
dim	O
A	O
h	O
and	O
m	O
O	O
t	O
is	O
distilled	O
from	O
H	O
O	O
t	O
.	O
As	O
for	O
O	O
-	O
LSTM	B-MethodName
,	O
similarly	O
,	O
an	O
opinion	O
memory	O
H	O
O	O
t	O
R	O
nm×dim	O
O	O
h	O
and	O
an	O
aspect	O
-	O
specific	O
summary	O
vector	O
m	O
A	O
t	O
R	O
dim	O
A	O
h	O
are	O
included	O
.	O
We	O
use	O
the	O
aspect	O
term	O
annotations	O
in	O
the	O
training	O
data	O
for	O
training	O
A	O
-	O
LSTM	B-MethodName
.	O
As	O
there	O
is	O
no	O
ground	O
truth	O
available	O
for	O
opinion	O
words	O
in	O
the	O
training	O
data	O
,	O
sentiment	O
lexicon	O
and	O
highprecision	O
dependency	O
rules	O
are	O
introduced	O
to	O
find	O
potential	O
opinion	O
words	O
.	O
Commonly	O
used	O
opinion	O
words	O
can	O
be	O
found	O
in	O
some	O
general	O
sentiment	O
lexicons	O
.	O
To	O
find	O
opinion	O
words	O
,	O
not	O
in	O
sentiment	O
lexicons	O
,	O
in	O
a	O
sentence	O
,	O
we	O
build	O
a	O
small	O
rule	O
set	O
R	O
composed	O
of	O
dependency	O
relations	O
with	O
high	O
confidence	O
,	O
e.g.	O
,	O
amod	O
,	O
nsubj	O
,	O
and	O
determine	O
if	O
w	O
t	O
directly	O
depends	O
on	O
the	O
gold	O
aspect	O
word	O
through	O
the	O
dependencies	O
in	O
R.	O
If	O
so	O
,	O
w	O
t	O
will	O
be	O
treated	O
as	O
a	O
potential	O
opinion	O
word	O
.	O
Then	O
such	O
opinion	O
words	O
are	O
used	O
as	O
training	O
data	O
for	O
O	O
-	O
LSTM	B-MethodName
.	O
In	O
the	O
memory	O
-	O
enhanced	O
A	O
-	O
LSTM	B-MethodName
and	O
O	O
-	O
LSTM	B-MethodName
,	O
we	O
manually	O
design	O
three	O
kinds	O
of	O
operations	O
:	O
(	O
1	O
)	O
READ	O
to	O
select	O
n	O
m	O
pieces	O
of	O
aspect	O
(	O
opinion	O
)	O
hidden	O
states	O
from	O
the	O
past	O
memories	O
and	O
build	O
H	O
A	O
t	O
(	O
H	O
O	O
t	O
)	O
;	O
(	O
2	O
)	O
DIGEST	O
to	O
distill	O
an	O
aspect	O
(	O
opinion	O
)	O
-	O
specific	O
summary	O
m	O
A	O
t	O
(	O
m	O
O	O
t	O
)	O
from	O
H	O
A	O
t	O
(	O
H	O
O	O
t	O
)	O
where	O
influences	O
of	O
opinion	O
terms	O
and	O
relative	O
positions	O
of	O
inputs	O
are	O
considered	O
;	O
(	O
3	O
)	O
INTERACT	O
to	O
perform	O
interaction	O
between	O
A	O
-	O
LSTM	B-MethodName
and	O
O	O
-	O
LSTM	B-MethodName
using	O
the	O
task	O
specific	O
summaries	O
(	O
i.e.	O
,	O
m	O
A	O
t	O
and	O
m	O
O	O
t	O
)	O
.	O
Consider	O
the	O
work	O
flow	O
of	O
A	O
-	O
LSTM	B-MethodName
for	O
aspect	O
term	B-TaskName
extraction	I-TaskName
.	O
Since	O
opinion	O
words	O
and	O
aspect	O
terms	O
should	O
co	O
-	O
occur	O
,	O
the	O
goal	O
of	O
A	O
-	O
LSTM	B-MethodName
participating	O
in	O
memory	O
interactions	O
is	O
to	O
acquire	O
opinion	O
summaries	O
from	O
O	O
-	O
LSTM	B-MethodName
(	O
i.e.	O
,	O
m	O
O	O
t	O
)	O
for	O
better	O
aspect	O
prediction	O
.	O
First	O
of	O
all	O
,	O
MIN	O
will	O
READ	O
n	O
m	O
pieces	O
of	O
opinion	O
memories	O
which	O
are	O
most	O
related	O
to	O
w	O
t	O
from	O
O	O
-	O
LSTM	B-MethodName
.	O
Syntax	O
structure	O
could	O
be	O
used	O
but	O
syntactic	O
parsers	O
are	O
not	O
effective	O
for	O
processing	O
short	O
informal	O
review	O
sentences	O
.	O
Therefore	O
,	O
MIN	O
selects	O
memory	O
segments	O
temporally	O
related	O
to	O
w	O
t	O
.	O
Precisely	O
,	O
the	O
opinion	O
memory	O
at	O
the	O
time	O
step	O
t	O
is	O
H	O
O	O
t	O
=	O
[	O
h	O
O	O
t−1	O
;	O
...	O
;	O
h	O
O	O
t−nm	O
]	O
where	O
h	O
O	O
t−i	O
is	O
the	O
(	O
t	O
−	O
i	O
)	O
-	O
th	O
hidden	O
state	O
from	O
O	O
-	O
LSTM	B-MethodName
.	O
Since	O
the	O
linear	O
context	O
contains	O
most	O
of	O
the	O
parent	O
nodes	O
and	O
the	O
child	O
nodes	O
of	O
w	O
t	O
on	O
the	O
dependency	O
parse	O
tree	O
,	O
treating	O
the	O
corresponding	O
memory	O
segments	O
as	O
relevant	O
segments	O
to	O
w	O
t	O
is	O
reasonable	O
.	O
Then	O
MIN	O
will	O
DIGEST	O
the	O
collected	O
opinion	O
memories	O
H	O
O	O
t	O
in	O
the	O
A	O
-	O
LSTM	B-MethodName
.	O
As	O
different	O
memory	O
segments	O
are	O
not	O
of	O
equal	O
importance	O
for	O
the	O
current	O
decision	O
and	O
the	O
same	O
segment	O
in	O
different	O
memories	O
(	O
i.e.	O
,	O
different	O
H	O
O	O
t	O
)	O
also	O
makes	O
a	O
difference	O
,	O
MIN	O
leverages	O
two	O
kind	O
of	O
weights	O
to	O
summarize	O
the	O
collected	O
content	O
.	O
The	O
first	O
weight	O
is	O
the	O
indicator	O
score	O
of	O
being	O
opinion	O
terms	O
denoted	O
as	O
v	O
I	O
R	O
nm	O
,	O
which	O
is	O
used	O
to	O
measure	O
how	O
much	O
opinion	O
information	O
the	O
word	O
w	O
t−i	O
(	O
i	O
=	O
1	O
,	O
..	O
,	O
n	O
m	O
)	O
holds	O
.	O
We	O
adopt	O
Euclidean	O
distance	O
between	O
distributed	O
representations	O
of	O
w	O
t−i	O
and	O
opinion	O
words	O
.	O
It	O
is	O
obvious	O
that	O
computing	O
the	O
distance	O
between	O
x	O
t−i	O
and	O
each	O
opinion	O
word	O
is	O
expensive	O
.	O
Thus	O
,	O
we	O
run	O
an	O
off	O
-	O
the	O
-	O
shelf	O
clustering	O
algorithm	O
over	O
opinion	O
words	O
in	O
the	O
training	O
set	O
and	O
then	O
use	O
the	O
produced	O
n	O
c	O
centroids	O
to	O
estimate	O
the	O
indicator	O
score	O
v	O
I	O
i	O
of	O
w	O
t−i	O
being	O
an	O
opinion	O
word	O
:	O
v	O
I	O
i	O
=	O
nc	O
j=1	O
1	O
|	O
|	O
x	O
t−i	O
−	O
c	O
j	O
|	O
|	O
2	O
(	O
1	O
)	O
where	O
x	O
t−i	O
is	O
the	O
distributed	O
representation	O
of	O
w	O
t−i	O
and	O
c	O
j	O
is	O
the	O
centroid	O
vector	O
representation	O
of	O
j	O
-	O
th	O
cluster	O
.	O
This	O
weighting	O
scheme	O
ensures	O
that	O
w	O
t−i	O
is	O
assigned	O
a	O
high	O
score	O
as	O
long	O
as	O
x	O
t−i	O
is	O
close	O
to	O
a	O
particular	O
centroid	O
.	O
The	O
aspect	O
decision	O
of	O
w	O
t	O
is	O
also	O
affected	O
by	O
relative	O
position	O
between	O
w	O
t−i	O
and	O
w	O
t	O
.	O
Thus	O
,	O
MIN	O
employs	O
the	O
second	O
weight	O
v	O
P	O
to	O
explicitly	O
model	O
their	O
positional	O
relevance	O
and	O
the	O
initial	O
weight	O
for	O
the	O
i	O
-	O
th	O
segment	O
v	O
P	O
i	O
is	O
calculated	O
as	O
below	O
:	O
v	O
P	O
i	O
=	O
n	O
m	O
−	O
i	O
+	O
1	O
nm	O
k=1	O
k	O
(	O
2	O
)	O
where	O
n	O
m	O
is	O
the	O
number	O
of	O
hidden	O
state	O
in	O
H	O
O	O
t	O
.	O
This	O
position	O
-	O
aware	O
weight	O
enables	O
that	O
the	O
closer	O
the	O
word	O
w	O
t−i	O
is	O
to	O
the	O
current	O
input	O
,	O
the	O
more	O
the	O
corresponding	O
memory	O
segment	O
will	O
contribute	O
to	O
the	O
current	O
decision	O
.	O
To	O
better	O
capture	O
the	O
local	O
positional	O
relevance	O
,	O
we	O
make	O
the	O
initialized	O
v	O
P	O
as	O
learnable	O
parameters	O
.	O
Combining	O
the	O
above	O
two	O
weights	O
helps	O
to	O
utilize	O
each	O
active	O
memory	O
segment	O
according	O
to	O
the	O
importance	O
for	O
prediction	O
and	O
m	O
O	O
t	O
,	O
the	O
summary	O
of	O
H	O
O	O
t	O
is	O
generated	O
:	O
m	O
O	O
t	O
=	O
(	O
H	O
O	O
t	O
)	O
(	O
v	O
I	O
v	O
P	O
|	O
|	O
v	O
I	O
|	O
|	O
2	O
)	O
(	O
3	O
)	O
where	O
denotes	O
element	O
-	O
wise	O
multiplication	O
and	O
|	O
|	O
*	O
|	O
|	O
2	O
is	O
Euclidean	O
norm	O
of	O
vectors	O
.	O
From	O
Equation	O
3	O
,	O
m	O
O	O
t	O
is	O
dominated	O
by	O
the	O
associated	O
memory	O
segment	O
of	O
w	O
t−i	O
that	O
obtains	O
the	O
high	O
combined	O
weights	O
.	O
In	O
the	O
last	O
operation	O
INTERACT	O
,	O
A	O
-	O
LSTM	B-MethodName
communicates	O
with	O
O	O
-	O
LSTM	B-MethodName
by	O
acquiring	O
m	O
O	O
t	O
from	O
O	O
-	O
LSTM	B-MethodName
and	O
incorporating	O
the	O
summary	O
into	O
the	O
memory	O
update	O
.	O
The	O
update	O
process	O
is	O
as	O
follows	O
:	O
i	O
A	O
t	O
=	O
σ	O
(	O
W	O
A	O
i	O
x	O
t	O
+	O
U	O
A	O
i	O
[	O
H	O
A	O
t	O
[	O
1	O
]	O
:	O
m	O
O	O
t	O
]	O
)	O
+	O
b	O
A	O
i	O
)	O
f	O
A	O
t	O
=	O
σ	O
(	O
W	O
A	O
f	O
x	O
t	O
+	O
U	O
A	O
f	O
[	O
H	O
A	O
t	O
[	O
1	O
]	O
:	O
m	O
O	O
t	O
]	O
)	O
+	O
b	O
A	O
f	O
)	O
c	O
A	O
t	O
=	O
tanh	O
(	O
W	O
A	O
c	O
x	O
t	O
+	O
U	O
A	O
c	O
[	O
H	O
A	O
t	O
[	O
1	O
]	O
:	O
m	O
O	O
t	O
]	O
)	O
+	O
b	O
A	O
c	O
)	O
o	O
A	O
t	O
=	O
σ	O
(	O
W	O
A	O
o	O
x	O
t	O
+	O
U	O
A	O
o	O
[	O
H	O
A	O
t	O
[	O
1	O
]	O
:	O
m	O
O	O
t	O
]	O
)	O
+	O
b	O
A	O
o	O
)	O
c	O
A	O
t	O
=	O
i	O
A	O
t	O
ĉ	O
A	O
t	O
+	O
c	O
A	O
t−1	O
f	O
A	O
t	O
h	O
A	O
t	O
=	O
tanh	O
(	O
c	O
A	O
t	O
)	O
o	O
A	O
t	O
(	O
4	O
)	O
where	O
W	O
A	O
*	O
,	O
U	O
A	O
*	O
and	O
b	O
A	O
*	O
are	O
weight	O
parameters	O
of	O
the	O
A	O
-	O
LSTM	B-MethodName
and	O
σ	O
is	O
the	O
sigmoid	B-MethodName
activation	I-MethodName
function	O
.	O
[	O
:	O
]	O
denotes	O
vector	O
concatenation	O
operation	O
.	O
m	O
O	O
t	O
can	O
be	O
seen	O
as	O
the	O
summary	O
of	O
the	O
opinion	O
indicator	O
in	O
the	O
left	O
context	O
of	O
w	O
t	O
and	O
H	O
A	O
t	O
[	O
1	O
]	O
is	O
the	O
most	O
immediate	O
hidden	O
memory	O
of	O
A	O
-	O
LSTM	B-MethodName
.	O
MIN	O
blends	O
the	O
opinion	O
summary	O
from	O
O	O
-	O
LSTM	B-MethodName
with	O
the	O
memory	O
from	O
A	O
-	O
LSTM	B-MethodName
.	O
The	O
co	O
-	O
occurrence	O
relation	O
between	O
aspects	O
and	O
opinion	O
words	O
is	O
modeled	O
by	O
such	O
"	O
memory	O
fusion	O
"	O
strategy	O
.	O
Since	O
opinion	O
words	O
can	O
appear	O
on	O
both	O
sides	O
of	O
w	O
t	O
,	O
memory	O
segments	O
corresponding	O
to	O
the	O
right	O
context	O
(	O
i.e.	O
,	O
"	O
future	O
"	O
memory	O
)	O
should	O
be	O
included	O
.	O
Hence	O
,	O
we	O
conduct	O
bi	O
-	O
directional	O
training	O
for	O
A	O
-	O
LSTM	B-MethodName
.	O
The	O
work	O
flow	O
of	O
memory	O
interaction	O
and	O
the	O
update	O
process	O
of	O
the	O
internal	O
memories	O
in	O
O	O
-	O
LSTM	B-MethodName
are	O
kept	O
same	O
with	O
those	O
in	O
A	O
-	O
LSTM	B-MethodName
except	O
the	O
DIGEST	O
operation	O
.	O
Specifically	O
,	O
we	O
set	O
m	O
A	O
t	O
,	O
the	O
task	O
-	O
specific	O
summary	O
of	O
A	O
-	O
LSTM	B-MethodName
,	O
as	O
h	O
A	O
t	O
.	O
The	O
second	O
component	O
of	O
MIN	O
is	O
a	O
generic	O
LSTM	B-MethodName
called	O
S	O
-	O
LSTM	B-MethodName
for	O
discriminating	O
sentimental	O
sentences	O
and	O
non	O
-	O
sentimental	O
sentences	O
.	O
The	O
design	O
and	O
the	O
process	O
of	O
the	O
memory	O
update	O
in	O
this	O
component	O
are	O
similar	O
to	O
that	O
in	O
Jozefowicz	O
et	O
al	O
(	O
2015	O
)	O
.	O
In	O
sentences	O
not	O
conveying	O
any	O
sentimental	O
meanings	O
,	O
some	O
words	O
like	O
food	O
,	O
service	O
tend	O
to	O
be	O
misclassified	O
as	O
aspect	O
terms	O
since	O
they	O
are	O
commonly	O
used	O
in	O
user	O
reviews	O
.	O
To	O
avoid	O
this	O
kind	O
of	O
error	O
,	O
we	O
add	O
a	O
constraint	O
that	O
an	O
aspect	O
term	O
should	O
come	O
from	O
sentimental	O
sentence	O
.	O
Specifically	O
,	O
S	O
-	O
LSTM	B-MethodName
learns	O
the	O
sentimental	O
representation	O
h	O
S	O
T	O
of	O
the	O
sentence	O
and	O
then	O
feeds	O
it	O
in	O
aspect	O
prediction	O
as	O
a	O
soft	O
constraint	O
:	O
On	O
the	O
whole	O
,	O
our	O
proposed	O
MIN	O
framework	O
has	O
three	O
LSTMs	O
and	O
each	O
of	O
them	O
is	O
differentiable	O
.	O
Thus	O
,	O
our	O
MIN	O
framework	O
can	O
be	O
efficiently	O
trained	O
with	O
gradient	O
descent	O
.	O
For	O
A	O
-	O
LSTM	B-MethodName
and	O
O	O
-	O
LSTM	B-MethodName
,	O
we	O
use	O
the	O
token	O
-	O
level	O
cross	O
-	O
entropy	O
error	O
between	O
the	O
predicted	O
distribution	O
P	O
(	O
y	O
T	O
t	O
|	O
x	O
t	O
)	O
and	O
the	O
gold	O
standard	O
distribution	O
P	O
(	O
y	O
T	O
,	O
g	O
t	O
|	O
x	O
t	O
)	O
as	O
the	O
loss	B-MetricName
function	O
(	O
T	O
{	O
A	O
,	O
O	O
}	O
)	O
:	O
Loss	O
(	O
T	O
)	O
=	O
−	O
1	O
N	O
*	O
T	O
N	O
i=1	O
T	O
t=1	O
P	O
(	O
Y	O
T	O
,	O
g	O
i	O
,	O
t	O
|	O
X	O
i	O
,	O
t	O
)	O
log	O
[	O
P	O
(	O
Y	O
T	O
i	O
,	O
t	O
|	O
X	O
i	O
,	O
t	O
)	O
]	O
(	O
6	O
)	O
For	O
S	O
-	O
LSTM	B-MethodName
,	O
sentence	O
-	O
level	O
cross	O
entropy	O
error	O
are	O
employed	O
to	O
calculate	O
the	O
corresponding	O
loss	B-MetricName
:	O
3	O
Experiment	O
Loss	O
(	O
S	O
)	O
=	O
−	O
1	O
N	O
N	O
i=1	O
P	O
(	O
Y	O
S	O
,	O
g	O
i	O
|	O
X	O
i	O
)	O
log	O
[	O
P	O
(	O
Y	O
S	O
i	O
|	O
X	O
i	O
)	O
]	O
(	O
7	O

To	O
evaluate	O
the	O
proposed	O
MIN	O
framework	O
,	O
we	O
perform	O
comparison	O
with	O
the	O
following	O
two	O
groups	O
of	O
methods	O
:	O
(	O
1	O
)	O
CRF	B-MethodName
based	O
methods	O
:	O
CRF	B-MethodName
:	O
Conditional	O
Random	O
Fields	O
with	O
basic	O
feature	O
templates	O
2	O
and	O
word	B-TaskName
embeddings	I-TaskName
.	O
Semi	O
-	O
CRF	B-MethodName
:	O
First	O
-	O
order	O
semi	O
-	O
Markov	O
conditional	O
random	O
fields	O
(	O
Sarawagi	O
et	O
al	O
,	O
2004	O
)	O
and	O
the	O
feature	O
template	O
in	O
Cuong	O
et	O
al	O
(	O
2014	O
)	O
is	O
adopted	O
.	O
IHS	O
RD	O
(	O
Chernyshevich	O
,	O
2014	O
)	O
,	O
NLANGP	O
(	O
Toh	O
and	O
Su	O
,	O
2016	O
)	O
:	O
Best	O
systems	O
in	O
ATE	O
subtask	O
in	O
SemEval	O
ABSA	O
challenges	O
(	O
Pontiki	O
et	O
al	O
,	O
2014	O
(	O
Pontiki	O
et	O
al	O
,	O
,	O
2016	O
.	O
DLIREC	O
(	O
Toh	O
and	O
Wang	O
,	O
2014	O
)	O
,	O
AUEB	O
(	O
Xenos	O
et	O
al	O
,	O
2016	O
)	O
:	O
Top	O
-	O
ranked	O
CRF	B-MethodName
-	O
based	O
systems	O
in	O
ATE	O
subtask	O
in	O
SemEval	O
ABSA	O
challenges	O
(	O
Pontiki	O
et	O
al	O
,	O
2014	O
(	O
Pontiki	O
et	O
al	O
,	O
,	O
2016	O
.	O
WDEmb	O
(	O
Yin	O
et	O
al	O
,	O
2016	O
)	O
:	O
Enhanced	O
CRF	B-MethodName
with	O
word	B-TaskName
embeddings	I-TaskName
,	O
linear	O
context	O
embeddings	O
and	O
dependency	O
path	O
embeddings	O
.	O
(	O
2	O
)	O
Neural	O
Network	O
based	O
methods	O
LSTM	B-MethodName
:	O
Vanilla	O
bi	O
-	O
directional	O
LSTM	B-MethodName
with	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
3	O
.	O
RNCRF	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
)	O
:	O
Dependency	O
Tree	O
based	O
Recursive	O
Neural	O
Network	O
with	O
CRF	B-MethodName
extractor	O
4	O
.	O
For	O
datasets	O
in	O
the	O
restaurant	O
domain	O
,	O
we	O
train	O
word	B-TaskName
embeddings	I-TaskName
of	O
dimension	O
200	O
with	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
on	O
Yelp	O
reviews	O
5	O
.	O
For	O
those	O
in	O
laptop	O
domain	O
,	O
we	O
use	O
pre	O
-	O
trained	O
glove.840B.300d	O
6	O
.	O
2	O
http://sklearn	O
-	O
crfsuite.readthedocs.io/en/latest/	O
3	O
As	O
we	O
use	O
our	O
own	O
implementation	O
of	O
LSTM	B-MethodName
,	O
the	O
reported	O
results	O
are	O
different	O
from	O
those	O
in	O
(	O
Liu	O
et	O
al	O
,	O
2015	O
)	O
4	O
Specifically	O
,	O
we	O
list	O
the	O
result	O
of	O
RNCRF	O
over	O
D1	O
without	O
opinion	O
annotations	O
for	O
fair	O
comparison	O
.	O
As	O
no	O
result	O
is	O
provided	O
for	O
RNCRF	O
-	O
no	O
-	O
opinion	O
over	O
D2	O
,	O
we	O
report	O
the	O
corresponding	O
performance	O
of	O
the	O
full	O
model	O
.	O
See	O
their	O
following	O
works	O
(	O
Wang	O
et	O
al	O
,	O
2017a	O
,	O
b	O
)	O
.	O
Also	O
,	O
CMLA	O
(	O
Wang	O
et	O
al	O
,	O
2017a	O
)	O
The	O
hyper	O
-	O
parameters	O
are	O
selected	O
via	O
ten	O
-	O
fold	O
cross	O
validation	O
.	O
The	O
dimension	O
of	O
hidden	O
representations	O
are	O
100	O
,	O
20	O
,	O
40	O
for	O
A	O
-	O
LSTM	B-MethodName
,	O
O	O
-	O
LSTM	B-MethodName
and	O
S	O
-	O
LSTM	B-MethodName
respectively	O
.	O
The	O
dropout	O
rate	O
for	O
O	O
-	O
LSTM	B-MethodName
and	O
S	O
-	O
LSTM	B-MethodName
is	O
0.4	O
.	O
The	O
size	O
of	O
the	O
aspect	O
(	O
opinion	O
)	O
memory	O
n	O
m	O
is	O
4	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
32	O
.	O
As	O
for	O
initialization	O
of	O
network	O
parameters	O
,	O
we	O
adopt	O
the	O
strategy	O
that	O
the	O
initial	O
weights	O
are	O
sampled	O
from	O
the	O
uniform	O
distribution	O
(	O
Glorot	O
and	O
Bengio	O
,	O
2010	O
)	O
.	O
We	O
employ	O
ADAM	B-DatasetName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
as	O
optimizer	B-HyperparameterName
and	O
the	O
default	O
settings	O
of	O
ADAM	B-DatasetName
are	O
used	O
.	O
To	O
better	O
reveal	O
the	O
capability	O
of	O
the	O
proposed	O
MIN	O
,	O
we	O
train	O
5	O
models	O
with	O
the	O
same	O
group	O
of	O
hyper	O
-	O
parameters	O
and	O
report	O
the	O
average	O
F	O
1	O
score	O
over	O
the	O
testing	O
set	O
.	O

The	O
first	O
set	O
of	O
experiments	O
consists	O
in	O
evaluating	O
which	O
is	O
the	O
best	O
method	O
for	O
combining	O
word	O
-	O
level	O
embeddings	O
into	O
sentence	O
representations	O
in	O
order	O
to	O
understand	O
what	O
kind	O
of	O
implicit	O
linguistic	O
properties	O
are	O
encoded	O
within	O
both	O
contextual	O
and	O
noncontextual	O
representations	O
using	O
different	O
combining	O
methods	O
.	O
To	O
do	O
so	O
,	O
we	O
firstly	O
extracted	O
from	O
each	O
sentence	O
in	O
the	O
UD	B-DatasetName
dataset	O
the	O
corresponding	O
word	B-TaskName
embeddings	I-TaskName
using	O
the	O
output	O
of	O
the	O
internal	O
representations	O
of	O
Word2vec	O
and	O
BERT	B-MethodName
layers	O
1	O
As	O
suggested	O
in	O
Jawahar	O
et	O
al	O
(	O
2019	O
)	O
(	O
from	O
input	O
layer	O
-	O
12	O
to	O
output	O
layer	O
-	O
1	O
)	O
.	O
Secondly	O
,	O
we	O
computed	O
the	O
sentence	O
-	O
representations	O
according	O
to	O
the	O
different	O
combining	O
strategies	O
defined	O
in	O
3.3	O
.	O
We	O
then	O
performed	O
our	O
set	O
of	O
68	O
probing	O
tasks	O
using	O
the	O
LinearSVR	O
model	O
for	O
each	O
sentence	O
representation	O
.	O
Since	O
the	O
majority	O
of	O
our	O
probing	O
features	O
is	O
correlated	O
to	O
sentence	O
length	O
,	O
we	O
compared	O
probing	O
results	O
with	O
the	O
ones	O
obtained	O
with	O
a	O
baseline	O
computed	O
by	O
measuring	O
the	O
ρ	O
coefficient	O
between	O
the	O
length	O
of	O
the	O
UD	B-DatasetName
sentences	O
and	O
each	O
of	O
the	O
68	O
probing	O
features	O
.	O
Evaluation	O
was	O
performed	O
with	O
a	O
5	O
-	O
cross	O
fold	O
validation	O
and	O
using	O
Spearman	B-MetricName
correlation	I-MetricName
score	O
(	O
ρ	O
)	O
between	O
predicted	O
and	O
gold	O
labels	O
as	O
evaluation	O
metric	O
.	O
Table	O
2	O
report	O
average	O
ρ	O
scores	O
aggregating	O
all	O
probing	O
results	O
(	O
All	O
features	O
)	O
and	O
according	O
to	O
raw	O
text	O
(	O
Raw	O
text	O
)	O
,	O
morphosyntactic	O
(	O
Morphosyntax	O
)	O
and	O
syntactic	O
(	O
Syntax	O
)	O
levels	O
of	O
annotations	O
.	O
Scores	O
are	O
computed	O
by	O
averaging	O
Max	O
-	O
,	O
Min	O
-	O
pooling	O
,	O
Mean	O
and	O
Sum	O
results	O
.	O
As	O
a	O
general	O
remark	O
,	O
we	O
notice	O
that	O
the	O
scores	O
obtained	O
by	O
Word2vec	O
and	O
BERT	B-MethodName
's	O
internal	O
representations	O
outperforms	O
the	O
ones	O
obtained	O
with	O
the	O
correlation	O
baseline	O
,	O
thus	O
showing	O
that	O
both	O
models	O
are	O
capable	O
of	O
implicitly	O
encoding	O
a	O
wide	O
spectrum	O
of	O
linguistic	O
phenomena	O
.	O
Interestingly	O
,	O
we	O
can	O
notice	O
that	O
Word2vec	O
sentence	O
representations	O
outperform	O
BERT	B-MethodName
ones	O
when	O
considering	O
all	O
the	O
probing	O
features	O
in	O
average	O
.	O
We	O
report	O
in	O
Table	O
3	O
and	O
Figure	O
1	O
the	O
probing	O
scores	O
obtained	O
by	O
the	O
two	O
models	O
.	O
For	O
what	O
concerns	O
Word2vec	O
representations	O
,	O
we	O
notice	O
that	O
the	O
Sum	O
method	O
prove	O
to	O
be	O
the	O
best	O
one	O
for	O
encoding	O
raw	O
text	O
and	O
syntactic	O
features	O
,	O
while	O
mo	O
-	O
rophosyntactic	O
properties	O
are	O
better	O
represented	O
averaging	O
all	O
the	O
word	B-TaskName
embeddings	I-TaskName
(	O
Mean	O
)	O
.	O
In	O
general	O
,	O
best	O
results	O
are	O
obtained	O
with	O
probing	O
tasks	O
related	O
to	O
morphosyntactic	O
and	O
syntactic	O
features	O
,	O
like	O
the	O
distribution	O
of	O
POS	O
(	O
e.g.	O
upos	O
dist	O
PRON	O
,	O
upos	O
dist	O
VERB	O
)	O
or	O
the	O
maximum	B-HyperparameterName
depth	I-HyperparameterName
of	O
the	O
syntactic	O
tree	O
(	O
parse	O
depth	O
)	O
.	O
If	O
we	O
look	O
instead	O
at	O
the	O
average	O
ρ	O
scores	O
obtained	O
with	O
BERT	B-MethodName
layerwise	O
representations	O
(	O
Figure	O
1	O
)	O
,	O
we	O
observe	O
that	O
,	O
differently	O
from	O
Word2vec	O
,	O
best	O
results	O
are	O
the	O
ones	O
related	O
to	O
raw	O
-	O
text	O
features	O
,	O
such	O
as	O
sentence	O
length	O
or	O
Type	O
/	O
Token	O
Ratio	O
.	O
The	O
Mean	O
method	O
prove	O
to	O
be	O
the	O
best	O
one	O
for	O
almost	O
all	O
the	O
probing	O
tasks	O
,	O
achieving	O
highest	O
scores	O
in	O
the	O
first	O
five	O
layers	O
.	O
The	O
only	O
exceptions	O
mainly	O
concern	O
some	O
of	O
the	O
linguistic	O
features	O
related	O
to	O
syntactic	O
properties	O
,	O
e.g.	O
the	O
average	O
length	O
of	O
dependency	O
links	O
(	O
avg	O
links	O
len	O
)	O
or	O
the	O
maximum	B-HyperparameterName
depth	I-HyperparameterName
of	O
the	O
syntactic	O
tree	O
(	O
parse	O
depth	O
)	O
,	O
for	O
which	O
best	O
scores	O
across	O
layers	O
are	O
obtained	O
with	O
the	O
Sum	O
strategy	O
.	O
The	O
Maxand	O
Min	O
-	O
pooling	O
methods	O
,	O
instead	O
,	O
show	O
a	O
similar	O
trend	O
for	O
almost	O
all	O
the	O
probing	O
features	O
.	O
Interestingly	O
,	O
the	O
representations	O
corresponding	O
to	O
the	O
[	O
CLS	O
]	O
token	O
,	O
although	O
considered	O
as	O
a	O
summarization	B-TaskName
of	O
the	O
entire	O
input	O
sequence	O
,	O
achieve	O
results	O
comparable	O
to	O
those	O
obtained	O
with	O
Maxand	O
Minpooling	O
methods	O
.	O
Moreover	O
,	O
it	O
can	O
be	O
noticed	O
that	O
,	O
unlike	O
Maxand	O
Min	O
-	O
pooling	O
,	O
the	O
representations	O
computed	O
with	O
Mean	O
and	O
Sum	O
methods	O
tend	O
to	O
lose	O
their	O
average	B-MetricName
precision	I-MetricName
in	O
encoding	O
our	O
set	O
of	O
linguistic	O
properties	O
across	O
the	O
12	O
layers	O
.	O
In	O
order	O
to	O
investigate	O
more	O
in	O
depth	O
how	O
the	O
linguistic	O
knowledge	O
encoded	O
by	O
BERT	B-MethodName
across	O
its	O
layers	O
differs	O
from	O
that	O
learned	O
by	O
Word2vec	O
,	O
we	O
report	O
in	O
Table	O
4	O
average	O
ρ	O
differences	O
between	O
the	O
two	O
models	O
according	O
to	O
the	O
four	O
combining	O
strategies	O
.	O
As	O
a	O
general	O
remark	O
,	O
we	O
can	O
notice	O
that	O
,	O
regardless	O
of	O
the	O
aggregation	O
strategy	O
taken	O
into	O
account	O
,	O
BERT	B-MethodName
and	O
Word2vec	O
sentence	O
representations	O
achieve	O
quite	O
similar	O
results	O
on	O
average	O
.	O
Hence	O
,	O
although	O
BERT	B-MethodName
is	O
capable	O
of	O
understanding	O
the	O
full	O
context	O
of	O
each	O
word	O
in	O
an	O
input	O
sequence	O
,	O
the	O
amount	O
of	O
linguistic	O
knowledge	O
implicitly	O
encoded	O
in	O
its	O
aggregated	O
sentence	O
representations	O
is	O
still	O
comparable	O
to	O
that	O
which	O
can	O
be	O
achieved	O
with	O
a	O
non	O
-	O
contextual	O
language	O
model	O
.	O
In	O
Figure	O
2	O
we	O
report	O
instead	O
the	O
differences	O
between	O
BERT	B-MethodName
and	O
Word2vec	O
scores	O
for	O
all	O
the	O
68	O
probing	O
features	O
(	O
ordered	O
by	O
correlation	O
with	O
sentence	O
length	O
)	O
.	O
For	O
the	O
comparison	O
,	O
we	O
used	O
the	O
representations	O
obtained	O
with	O
the	O
Mean	O
combining	O
method	O
.	O
As	O
a	O
first	O
remark	O
,	O
we	O
notice	O
that	O
there	O
is	O
a	O
clear	O
distinction	O
in	O
terms	O
of	O
ρ	O
scores	O
between	O
features	O
better	O
predicted	O
by	O
BERT	B-MethodName
and	O
Word2vec	O
.	O
In	O
fact	O
,	O
features	O
most	O
related	O
to	O
syntactic	O
properties	O
(	O
left	O
heatmap	B-MethodName
)	O
are	O
those	O
for	O
which	O
BERT	B-MethodName
results	O
are	O
generally	O
higher	O
with	O
respect	O
to	O
those	O
obtained	O
with	O
Word2vec	O
.	O
This	O
result	O
demonstrates	O
that	O
BERT	B-MethodName
,	O
unlike	O
a	O
non	O
-	O
contextual	O
language	O
model	O
as	O
Word2vec	O
,	O
is	O
able	O
to	O
encode	O
information	O
within	O
its	O
representa	O
-	O
tions	O
that	O
involves	O
the	O
entire	O
input	O
sequence	O
,	O
thus	O
making	O
more	O
simple	O
to	O
solve	O
probing	O
tasks	O
that	O
refer	O
to	O
syntatic	O
characteristics	O
.	O
Focusing	O
instead	O
on	O
the	O
right	O
heatmap	B-MethodName
,	O
we	O
observe	O
that	O
Word2vec	O
non	O
-	O
contextual	O
representations	O
are	O
still	O
capable	O
of	O
encoding	O
a	O
wide	O
spectrum	O
of	O
linguistic	O
properties	O
with	O
higher	O
ρ	O
values	O
compared	O
to	O
BERT	B-MethodName
ones	O
,	O
especially	O
if	O
we	O
consider	O
scores	O
closer	O
to	O
BERT	B-MethodName
's	O
output	O
layers	O
(	O
from	O
-	O
4	O
to	O
-	O
1	O
)	O
.	O
This	O
is	O
particularly	O
evident	O
for	O
morphosyntactic	O
features	O
related	O
to	O
the	O
distribution	O
of	O
POS	O
categories	O
(	O
xpos	O
dist	O
*	O
,	O
upos	O
dist	O
*	O
)	O
,	O
most	O
likely	O
because	O
non	O
-	O
contextual	O
representations	O
tend	O
to	O
encode	O
properties	O
related	O
to	O
single	O
tokens	O
rather	O
than	O
syntactic	O
relations	O
between	O
them	O
.	O

With	O
the	O
growth	O
of	O
the	O
internet	O
,	O
social	O
media	O
becomes	O
a	O
crucial	O
part	O
of	O
everyone	O
's	O
life	O
.	O
As	O
every	O
coin	O
has	O
two	O
side	O
positive	O
and	O
negative	O
,	O
social	O
media	O
also	O
comes	O
with	O
a	O
number	O
of	O
problems	O
.	O
The	O
challenge	O
of	O
identifying	O
misogyny	O
(	O
Srivastava	O
et	O
al	O
,	O
2017	O
)	O
in	O
different	O
social	O
media	O
specially	O
in	O
forms	O
of	O
meme	O
which	O
contains	O
both	O
image	O
and	O
text	O
is	O
very	O
complicated	O
.	O
Misogyny	O
meme	O
highly	O
affected	O
the	O
life	O
of	O
women	O
's	O
as	O
its	O
spread	O
hate	O
and	O
prejudice	O
behaviour	O
against	O
women	O
's	O
.	O
Social	O
media	O
like	O
twitter	O
,	O
Instagram	O
,	O
etc	O
have	O
handled	O
by	O
their	O
own	O
ways	O
.	O
However	O
,	O
detecting	O
such	O
memes	O
is	O
highly	O
challenging	O
.	O
Due	O
to	O
this	O
challenge	O
,	O
it	O
attracts	O
the	O
researcher	O
's	O
attention	O
.	O
According	O
to	O
one	O
social	O
media	O
Instagram	O
,	O
more	O
than	O
1	O
million	O
users	O
shared	O
memes	O
daily	O
.	O
So	O
,	O
with	O
this	O
huge	O
amount	O
of	O
data	O
in	O
social	O
medias	O
and	O
internet	O
it	O
is	O
impossible	O
to	O
detect	O
every	O
misogyny	O
meme	O
by	O
man	O
power	O
.	O
So	O
,	O
we	O
need	O
machine	O
learning	O
,	O
deep	O
learning	O
and	O
artificial	O
intelligence	O
techniques	O
to	O
detect	O
automatically	O
misogyny	O
memes	O
in	O
social	O
media	O
.	O
In	O
this	O
paper	O
,	O
we	O
have	O
explored	O
various	O
Machine	O
Learning	O
(	O
ML	O
)	O
and	O
Deep	O
Learning	O
(	O
DL	O
)	O
algorithms	O
for	O
misogyny	O
identification	O
in	O
shared	O
task	O
MAMI	O
(	O
Fersini	O
et	O
al	O
,	O
2022	O
)	O
challenge	O
and	O
my	O
team	O
's	O
name	O
is	O
IIT	O
DHANBAD	O
CODECHAMPS	O
.	O
As	O
per	O
requirement	O
of	O
MAMI	O
,	O
I	O
have	O
submitted	O
4	O
runs	O
for	O
Subtask	O
-	O
A.	O
My	O
best	O
run	O
in	O
Subtask	O
-	O
A	O
has	O
achieved	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
score	O
of	O
0.656	O
.	O

For	O
Subtask	O
-	O
A	O
,	O
we	O
have	O
submitted	O
4	O
runs	O
based	O
on	O
four	O
different	O
algorithms	O
,	O
namely	O
-	O
Logistic	B-MethodName
Regression	I-MethodName
(	O
Sammut	O
and	O
Webb	O
,	O
2010	O
)	O
,	O
SVM	B-MethodName
(	O
Noble	O
,	O
2006	O
)	O
,	O
LSTM	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
,	O
Bert	O
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
with	O
different	O
parameters	O
like	O
batch	B-HyperparameterName
size	I-HyperparameterName
,	O
epochs	O
,	O
number	O
of	O
perceptron	O
etc	O
.	O
We	O
have	O
used	O
the	O
scikit	O
-	O
learn	O
library	O
for	O
logistic	B-MethodName
regression	I-MethodName
based	O
models	O
and	O
SVM	B-MethodName
(	O
support	O
vector	O
machines	O
)	O
models	O
.	O
Keras	O
is	O
used	O
for	O
LSTM	B-MethodName
and	O
BERT	B-MethodName
.	O
We	O
scored	O
maximum	O
F1	B-MetricName
score	I-MetricName
0.656	O
using	O
BERT	B-MethodName
.	O
We	O
have	O
used	O
the	O
following	O
value	O
of	O
parameters	O
:	O
-	O
1.For	O
TfidfVectorizer	O
,	O
we	O
have	O
used	O
mindf=20	O
,	O
maxfeatures=2000	O
and	O
maxdf=0.6	O
.	O
2	O
.	O
For	O
LSTM	B-MethodName
and	O
BERT	B-MethodName
,	O
we	O
have	O
used	O
batch	B-HyperparameterName
size	I-HyperparameterName
=	O
2	O
,	O
epochs	O
=	O
3	O
and	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
=	O
2	O
.	O

The	O
results	O
of	O
Subtask	O
-	O
A	O
are	O
represented	O
in	O
terms	O
of	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
(	O
shown	O
in	O
Table	O
2	O
)	O
.	O
The	O
best	O
score	O
as	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
for	O
Subtask	O
-	O
A	O
we	O
get	O
is	O
0.656	O
.	O
Table	O
2	O
shows	O
the	O
score	O
of	O
our	O
submissions	O
based	O
on	O
different	O
algorithms	O
on	O
MAMI	O
challenge	O
official	O
ranking	O
.	O
For	O
Subtask	O
-	O
A	O
BERT	B-MethodName
performs	O
better	O
than	O
all	O
other	O
models	O
with	O
the	O
parameters	O
batch	B-HyperparameterName
size	I-HyperparameterName
=	O
2	O
,	O
epochs	O
=	O
3	O
,	O
number	O
of	O
hidden	O
layers	O
=	O
2	O
and	O
number	O
of	O
perceptron	O
's	O
is	O
128	O
in	O
first	O
layer	O
and	O
64	O
in	O
second	O
layer	O
.	O

In	O
previous	O
entailment	O
graph	O
research	O
(	O
Hosseini	O
et	O
al	O
,	O
2018	O
)	O
a	O
representation	O
vector	O
is	O
computed	O
for	O
each	O
typed	O
predicate	O
in	O
the	O
graph	O
.	O
These	O
are	O
compared	O
via	O
the	O
DIH	O
to	O
establish	O
entailment	O
edges	O
between	O
predicates	O
.	O
The	O
features	O
of	O
each	O
vector	O
are	O
typically	O
based	O
on	O
the	O
argument	O
pairs	O
seen	O
with	O
that	O
predicate	O
.	O
Specifically	O
,	O
for	O
a	O
typed	O
predicate	O
p	O
with	O
corresponding	O
vector	O
v	O
,	O
v	O
consists	O
of	O
features	O
f	O
i	O
which	O
are	O
the	O
pointwise	O
mutual	O
information	O
(	O
PMI	O
)	O
of	O
p	O
and	O
the	O
argument	O
pair	O
a	O
i	O
{	O
(	O
e	O
m	O
,	O
e	O
n	O
)	O
|	O
e	O
m	O
E	O
t	O
1	O
,	O
e	O
n	O
E	O
t	O
2	O
}	O
.	O
Here	O
t	O
1	O
,	O
t	O
2	O
T	O
,	O
and	O
E	O
t	O
is	O
the	O
subset	O
of	O
entities	O
of	O
type	O
t.	O
For	O
example	O
,	O
the	O
predicate	O
BUILD	O
(	O
:	O
company	O
,	O
:	O
thing	O
)	O
might	O
have	O
some	O
feature	O
f	O
37	O
,	O
the	O
PMI	O
of	O
"	O
build	O
"	O
with	O
argument	O
pair	O
(	O
Apple	O
,	O
iPhone	O
)	O
.	O
A	O
Balanced	O
Inclusion	O
(	O
BInc	O
)	O
score	O
is	O
calculated	O
for	O
the	O
directed	O
entailment	O
from	O
one	O
predicate	O
to	O
another	O
(	O
Szpektor	O
and	O
Dagan	O
,	O
2008	O
)	O
.	O
BInc	O
is	O
the	O
geometric	O
mean	O
of	O
two	O
subscores	O
:	O
a	O
directional	O
score	O
,	O
Weeds	O
Precision	B-MetricName
(	O
Weeds	O
and	O
Weir	O
,	O
2003	O
)	O
,	O
measuring	O
how	O
much	O
one	O
vector	O
's	O
features	O
"	O
cover	O
"	O
the	O
other	O
's	O
;	O
and	O
a	O
symmetrical	O
score	O
,	O
Lin	O
Similarity	O
(	O
Lin	O
,	O
1998	O
)	O
,	O
which	O
downweights	O
infrequent	O
predicates	O
that	O
cause	O
spurious	O
false	O
positives	O
.	O
In	O
this	O
work	O
we	O
compute	O
local	O
binary	O
graphs	O
following	O
Hosseini	O
et	O
al	O
(	O
2018	O
)	O
and	O
leverage	O
the	O
new	O
MDIH	O
to	O
compute	O
additional	O
entailments	O
for	O
unaries	O
and	O
between	O
valencies	O
.	O
To	O
do	O
this	O
we	O
compute	O
a	O
vector	O
for	O
each	O
argument	O
slot	O
respecting	O
its	O
position	O
in	O
the	O
predicate	O
.	O
For	O
a	O
predicate	O
p	O
,	O
a	O
slot	O
vector	O
v	O
(	O
s	O
)	O
for	O
s	O
{	O
1	O
,	O
2	O
}	O
consists	O
of	O
features	O
f	O
i	O
E	O
t	O
.	O
Slot	O
vectors	O
are	O
computed	O
for	O
the	O
slot	O
in	O
unary	O
relations	O
and	O
both	O
slots	O
in	O
binaries	O
.	O
Each	O
slot	O
vector	O
for	O
p	O
has	O
size	O
|	O
v	O
(	O
s	O
)	O
|	O
=	O
|	O
E	O
t	O
|	O
,	O
the	O
number	O
of	O
entities	O
in	O
the	O
data	O
with	O
the	O
same	O
type	O
t.	O
Continuing	O
the	O
example	O
,	O
we	O
calculate	O
two	O
vectors	O
for	O
BUILD	O
(	O
:	O
company	O
,	O
:	O
thing	O
)	O
:	O
v	O
(	O
1	O
)	O
R	O
|	O
E	O
:	O
company	O
|	O
which	O
contains	O
a	O
feature	O
for	O
Apple	O
,	O
and	O
v	O
(	O
2	O
)	O
R	O
|	O
E	O
:	O
thing	O
|	O
which	O
contains	O
a	O
feature	O
for	O
iPhone	O
.	O
Slot	O
vectors	O
are	O
comparable	O
if	O
they	O
represent	O
the	O
same	O
entity	O
type	O
.	O
Edges	O
are	O
learned	O
by	O
comparing	O
corresponding	O
slot	O
vectors	O
between	O
predicates	O
.	O
For	O
instance	O
,	O
DEFEAT	O
(	O
:	O
person1	O
,	O
:	O
person2	O
)	O
BE.WINNER	O
(	O
:	O
person1	O
)	O
4	O
is	O
learned	O
by	O
comparing	O
the	O
slot	O
1	O
vector	O
of	O
DEFEAT	O
with	O
the	O
slot	O
1	O
vector	O
of	O
BE.WINNER	O
.	O
If	O
the	O
entities	O
who	O
have	O
defeated	O
someone	O
are	O
usually	O
found	O
amongst	O
the	O
entities	O
who	O
are	O
winners	O
then	O
we	O
get	O
a	O
high	O
BInc	O
score	O
,	O
indicating	O
defeat	O
entails	O
that	O
its	O
subject	O
is	O
a	O
winner	O
.	O
Figure	O
2	O
Because	O
a	O
unary	O
has	O
only	O
one	O
type	O
t	O
i	O
it	O
may	O
be	O
entailed	O
by	O
binaries	O
in	O
up	O
to	O
2	O
*	O
|	O
T	O
|	O
−	O
1	O
subgraphs	O
with	O
types	O
{	O
(	O
t	O
i	O
,	O
t	O
j	O
)	O
|	O
j	O
T	O
}	O
,	O
i.e.	O
all	O
bivalent	O
graphs	O
containing	O
type	O
t	O
i	O
.	O
We	O
learn	O
entailments	O
from	O
unaries	O
(	O
UU	O
)	O
in	O
separate	O
1	O
-	O
type	O
univalent	O
graphs	O
.	O
This	O
efficiently	O
learns	O
one	O
set	O
of	O
entailments	O
for	O
each	O
unary	O
,	O
but	O
allows	O
them	O
to	O
be	O
freely	O
entailed	O
by	O
higher	O
-	O
valency	O
predicates	O
,	O
e.g.	O
binaries	O
.	O
Bivalent	O
graphs	O
point	O
transitively	O
into	O
univalent	O
graphs	O
.	O
In	O
Figure	O
2	O
,	O
DEFEAT	O
(	O
:	O
person1	O
,	O
:	O
person2	O
)	O
BE.WINNER	O
(	O
:	O
person1	O
)	O
in	O
the	O
person	O
-	O
person	O
graph	O
.	O
E.g.	O
further	O
entailments	O
of	O
BE.WINNER	O
(	O
:	O
person	O
)	O
are	O
in	O
the	O
person	O
univalent	O
graph	O
.	O
Figure	O
2	O
:	O
Bivalent	O
graphs	O
model	O
entailments	O
from	O
binary	O
predicates	O
to	O
equal	O
-	O
and	O
lower	O
-	O
valency	O
predicates	O
(	O
binary	O
and	O
unary	O
)	O
.	O
Univalent	O
graphs	O
model	O
entailments	O
from	O
unaries	O
to	O
equal	O
-	O
valency	O
unary	O
predicates	O
.	O

The	O
models	O
produce	O
a	O
gradation	O
of	O
judgement	O
scores	O
between	O
0	B-DatasetName
(	O
false	O
)	O
and	O
1	O
(	O
true	O
)	O
.	O
As	O
in	O
earlier	O
work	O
,	O
we	O
slide	O
a	O
classification	B-HyperparameterName
threshold	I-HyperparameterName
over	O
the	O
score	O
range	O
to	O
produce	O
a	O
precision	O
-	O
recall	O
curve	O
for	O
each	O
model	O
.	O
Results	O
are	O
in	O
Figure	O
3	O
(	O
left	O
)	O
.	O
Multivalent	O
graph	O
performance	O
is	O
shown	O
incrementally	O
.	O
The	O
BB	O
model	O
can	O
answer	O
a	O
portion	O
of	O
binary	O
questions	O
;	O
the	O
UU	O
model	O
can	O
answer	O
more	O
unary	O
questions	O
;	O
adding	O
the	O
BU	O
model	O
can	O
answer	O
still	O
more	O
unary	O
questions	O
using	O
binary	O
evidence	O
.	O
We	O
observe	O
successful	O
inference	O
of	O
our	O
kill	O
/	O
die	O
example	O
and	O
others	O
.	O
"	O
Obama	O
was	O
elected	O
to	O
office	O
"	O
affirms	O
the	O
question	O
"	O
Was	O
Obama	O
a	O
candidate	O
?	O
"	O
and	O
"	O
Zach	O
Randolph	O
returned	O
"	O
affirms	O
"	O
Did	O
Zach	O
Randolph	O
arrive	O
?	O
"	O
Our	O
test	O
set	O
is	O
from	O
multiple	O
sources	O
over	O
the	O
same	O
time	O
period	O
.	O
The	O
exact	O
-	O
match	O
baseline	O
shows	O
the	O
limitations	O
of	O
answering	O
questions	O
simply	O
by	O
collecting	O
more	O
data	O
;	O
most	O
questions	O
require	O
inference	O
to	O
answer	O
.	O
The	O
complete	O
MGraph	O
achieves	O
3x	O
this	O
recall	O
by	O
drawing	O
inferences	O
.	O
Our	O
model	O
achieves	O
higher	O
precision	O
than	O
BERT	B-MethodName
and	O
RoBERTa	B-MethodName
similarity	O
models	O
in	O
the	O
low	O
recall	O
range	O
.	O
The	O
similarity	O
models	O
perform	O
well	O
,	O
achieving	O
full	O
recall	O
by	O
generalizing	O
for	O
rarer	O
predicates	O
.	O
We	O
note	O
that	O
RoBERTa	B-MethodName
bests	O
BERT	B-MethodName
due	O
to	O
extensive	O
in	O
-	O
domain	O
pretraining	O
.	O
The	O
BB	O
model	O
appears	O
to	O
struggle	O
.	O
In	O
fact	O
90.5	O
%	O
of	O
unary	O
questions	O
have	O
a	O
vertex	O
in	O
the	O
graph	O
,	O
but	O
only	O
64.1	O
%	O
of	O
binaries	O
do	O
.	O
The	O
BB	O
model	O
frequently	O
can	O
not	O
answer	O
questions	O
because	O
the	O
question	O
predicate	O
was	O
n't	O
seen	O
in	O
training	O
.	O
This	O
difference	O
is	O
because	O
binary	O
predicates	O
are	O
more	O
diverse	O
so	O
suffer	O
more	O
from	O
sparsity	O
:	O
they	O
are	O
often	O
multiword	O
expressions	O
and	O
have	O
a	O
second	O
,	O
typed	O
argument	O
.	O
Indeed	O
,	O
most	O
binary	O
predicate	O
research	O
(	O
in	O
symbolic	O
methods	O
)	O
focuses	O
on	O
only	O
the	O
top	O
50	O
%	O
of	O
recall	O
in	O
several	O
datasets	O
(	O
Berant	O
et	O
al	O
,	O
2010	O
(	O
Berant	O
et	O
al	O
,	O
,	O
2015Levy	O
and	O
Dagan	O
,	O
2016	O
;	O
Hosseini	O
et	O
al	O
,	O
2018	O
)	O
.	O
For	O
an	O
even	O
comparison	O
we	O
create	O
a	O
filtered	O
question	O
set	O
.	O
From	O
all	O
questions	O
we	O
remove	O
those	O
without	O
a	O
vertex	O
in	O
the	O
MGraph	O
,	O
then	O
balance	O
them	O
as	O
in	O
5	O
,	O
resulting	O
in	O
20	O
,	O
519	O
questions	O
(	O
10	O
,	O
273	O
unary	O
and	O
10	O
,	O
246	O
binary	O
)	O
.	O
This	O
filtered	O
test	O
directly	O
compares	O
the	O
models	O
,	O
since	O
both	O
the	O
entailment	O
graphs	O
and	O
the	O
similarity	O
models	O
have	O
a	O
chance	O
to	O
answer	O
all	O
the	O
questions	O
.	O
Results	O
are	O
shown	O
in	O
Figure	O
3	O
(	O
right	O
)	O
,	O
with	O
a	O
very	O
different	O
outcome	O
.	O
Head	O
-	O
to	O
-	O
head	O
,	O
the	O
MGraph	O
offers	O
substantially	O
better	O
precision	O
across	O
all	O
recall	O
levels	O
.	O
At	O
50	O
%	O
recall	O
,	O
the	O
MGraph	O
has	O
76	O
%	O
precision	O
with	O
RoBERTa	B-MethodName
at	O
65	O
%	O
.	O
Notably	O
,	O
on	O
both	O
tests	O
,	O
more	O
unary	O
questions	O
are	O
answered	O
using	O
both	O
unary	O
and	O
binary	O
predicate	O
evidence	O
than	O
just	O
using	O
unary	O
evidence	O
alone	O
.	O
On	O
the	O
filtered	O
test	O
,	O
the	O
BU	O
model	O
increases	O
max	O
recall	O
from	O
54	O
%	O
to	O
70	O
%	O
.	O
Finally	O
,	O
we	O
note	O
PPDB	O
's	O
poor	O
performance	O
(	O
highest	O
recall	O
shown	O
)	O
,	O
only	O
1	O
%	O
higher	O
recall	O
than	O
the	O
exact	O
-	O
match	O
baseline	O
despite	O
having	O
entries	O
for	O
88	O
%	O
of	O
questions	O
.	O
Though	O
PPDB	O
features	O
many	O
directional	O
entailments	O
,	O
this	O
sparsity	O
of	O
edges	O
useful	O
for	O
the	O
task	O
is	O
likely	O
because	O
bilingual	O
pivoting	O
excels	O
at	O
detecting	O
near	O
-	O
paraphrases	O
,	O
not	O
relations	O
between	O
distinct	O
eventualities	O
,	O
e.g.	O
it	O
ca	O
n't	O
learn	O
"	O
getting	O
elected	O
"	O
entails	O
"	O
being	O
a	O
candidate	O
.	O
"	O
Advantageously	O
,	O
our	O
method	O
learns	O
this	O
open	O
-	O
domain	O
knowledge	O
by	O
tracking	O
entities	O
across	O
all	O
the	O
events	O
they	O
participate	O
in	O
.	O
We	O
show	O
a	O
breakdown	O
of	O
the	O
filtered	O
test	O
results	O
in	O
Table	O
3	O
.	O
Models	O
do	O
n't	O
answer	O
all	O
the	O
questions	O
,	O
so	O
following	O
Lewis	O
and	O
Steedman	O
(	O
2013	O
)	O
who	O
design	O
a	O
similar	O
QA	O
task	O
,	O
we	O
evaluate	O
models	O
on	O
the	O
accuracy	B-MetricName
of	O
their	O
K	O
most	O
confident	O
predictions	O
.	O

In	O
general	O
,	O
each	O
option	O
should	O
have	O
the	O
same	O
correct	O
rate	O
for	O
multi	O
-	O
choice	O
questions	O
,	O
but	O
in	O
fact	O
,	O
the	O
order	O
in	O
which	O
the	O
correct	O
options	O
appear	O
is	O
not	O
completely	O
random	O
,	O
and	O
the	O
more	O
the	O
number	O
of	O
options	O
,	O
the	O
lower	O
the	O
degree	O
of	O
randomization	O
(	O
Poundstone	O
,	O
2014	O
)	O
.	O
Given	O
the	O
complex	O
nature	O
of	O
multi	O
-	O
choice	O
tasks	O
,	O
we	O
employ	O
three	O
control	O
methods	O
to	O
ensure	O
a	O
fair	O
comparison	O
among	O
various	O
open	O
-	O
domain	O
QA	O
models	O
.	O
Random	O
A	O
′	O
=	O
Random	O
(	O
O	O
)	O
.	O
For	O
each	O
question	O
,	O
an	O
option	O
is	O
randomly	O
chosen	O
as	O
the	O
answer	O
from	O
five	O
candidate	O
options	O
.	O
We	O
perform	O
this	O
experiment	O
five	O
times	O
and	O
average	O
the	O
results	O
as	O
the	O
baseline	O
of	O
the	O
Random	O
method	O
.	O
Constant	O
A	O
′	O
=	O
Constant	O
j	O
(	O
O	O
)	O
,	O
where	O
j	O
{	O
A	O
,	O
B	O
,	O
C	O
,	O
D	O
,	O
E	O
}	O
.	O
For	O
each	O
question	O
,	O
the	O
j	O
th	O
option	O
is	O
always	O
chosen	O
as	O
the	O
answer	O
to	O
obtain	O
the	O
accuracy	B-MetricName
distribution	O
of	O
five	O
candidate	O
options	O
.	O
Mixed	O
A	O
′	O
=	O
M	O
ixed	O
(	O
O	O
)	O
.	O
Incorporating	O
the	O
previous	O
experiences	O
of	O
NMLEC	O
and	O
multi	O
-	O
choice	O
task	O
work	O
(	O
Vilares	O
and	O
Gómez	O
-	O
Rodríguez	O
,	O
2019	O
)	O
,	O
the	O
Mixed	O
method	O
simulates	O
how	O
humans	O
solving	O
uncertain	O
questions	O
,	O
and	O
consists	O
of	O
the	O
following	O
three	O
strategies	O
:	O
(	O
1	O
)	O
the	O
correct	O
rate	O
of	O
choosing	O
"	O
All	O
of	O
the	O
options	O
above	O
is	O
correct	O
/	O
incorrect	O
"	O
is	O
much	O
higher	O
than	O
the	O
other	O
options	O
.	O
(	O
2	O
)	O
Supposing	O
the	O
length	O
of	O
options	O
is	O
roughly	O
equal	O
,	O
only	O
one	O
option	O
is	O
obviously	O
longer	O
with	O
more	O
detailed	O
and	O
specific	O
descriptions	O
,	O
or	O
is	O
obviously	O
shorter	O
than	O
the	O
other	O
options	O
,	O
then	O
choose	O
this	O
option	O
.	O
(	O
3	O
)	O
The	O
correct	O
option	O
tends	O
to	O
appear	O
in	O
the	O
middle	O
of	O
candidate	O
options	O
.	O
The	O
three	O
strategies	O
are	O
applied	O
in	O
turn	O
.	O
If	O
any	O
strategy	O
matches	O
,	O
then	O
the	O
option	O
that	O
matches	O
the	O
strategy	O
is	O
chosen	O
as	O
the	O
answer	O
.	O

We	O
conduct	O
detailed	O
experiments	O
and	O
analyses	O
to	O
investigate	O
the	O
performance	O
of	O
control	O
methods	O
and	O
open	O
-	O
domain	O
QA	O
methods	O
on	O
MLEC	O
-	O
QA	O
.	O
As	O
shown	O
in	O
Figure	O
2	O
,	O
we	O
implement	O
a	O
two	O
-	O
stage	O
retriever	O
-	O
reader	O
framework	O
:	O
(	O
1	O
)	O
a	O
retriever	O
first	O
retrieves	O
question	O
relevant	O
documents	O
from	O
Chinese	O
Wikipedia	O
using	O
ElasticSearch	O
,	O
(	O
2	O
)	O
and	O
then	O
a	O
reader	O
employs	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
models	O
to	O
generate	O
answers	O
in	O
given	O
documents	O
retrieved	O
by	O
the	O
retriever	O
.	O
For	O
the	O
reader	O
,	O
all	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
models	O
are	O
trained	O
with	O
12	O
epochs	O
,	O
an	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e	O
-	O
6	O
,	O
a	O
maximum	O
sequence	O
length	O
of	O
512	O
,	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
5	O
.	O
The	O
parameters	O
are	O
selected	O
based	O
on	O
the	O
best	O
performance	O
on	O
the	O
development	O
set	O
,	O
and	O
we	O
keep	O
the	O
default	O
values	O
for	O
the	O
other	O
hyper	O
-	O
parameters	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
use	O
accuracy	B-MetricName
as	O
the	O
metric	O
to	O
evaluate	O
different	O
methods	O
,	O
and	O
provide	O
baseline	O
results	O
,	O
as	O
well	O
as	O
human	O
pass	O
mark	O
(	O
60	O
%	O
)	O
instead	O
of	O
human	O
performance	O
due	O
to	O
the	O
wide	O
variations	O
exist	O
in	O
human	O
performance	O
,	O
from	O
almost	O
full	O
marks	O
to	O
can	O
not	O
even	O
pass	O
the	O
exam	O
.	O

The	O
main	O
drawbacks	O
of	O
the	O
Chinese	O
Wikipedia	O
database	O
in	O
biomedicine	O
are	O
that	O
it	O
is	O
not	O
comprehensive	O
and	O
thorough	O
,	O
that	O
is	O
,	O
it	O
may	O
not	O
provide	O
complete	O
coverage	O
of	O
all	O
subjects	O
.	O
To	O
evaluate	O
whether	O
retrieved	O
documents	O
can	O
cover	O
enough	O
evidence	O
to	O
answer	O
questions	O
,	O
we	O
sampled	O
5	O
%	O
(	O
681	O
)	O
questions	O
from	O
the	O
development	O
sets	O
of	O
five	O
categories	O
using	O
stratified	O
random	O
sampling	O
,	O
and	O
manually	O
annotate	O
each	O
question	O
by	O
five	O
medical	O
experts	O
with	O
3	O
labels	O
:	O
(	O
1	O
)	O
Exactly	O
Match	O
(	O
EM	B-MetricName
)	O
:	O
the	O
retrieved	O
documents	O
exactly	O
match	O
the	O
question	O
.	O
(	O
2	O
)	O
Partial	O
Match	O
(	O
PM	O
)	O
:	O
the	O
retrieved	O
documents	O
partially	O
match	O
the	O
question	O
,	O
can	O
be	O
confused	O
with	O
the	O
correct	O
options	O
or	O
are	O
incomplete	O
.	O
(	O
3	O
)	O
Mismatch	O
(	O
MM	O
)	O
:	O
the	O
retrieved	O
documents	O
do	O
not	O
match	O
the	O
question	O
at	O
all	O
.	O
Table	O
7	O
lists	O
the	O
performance	O
of	O
the	O
retrieval	O
strategy	O
as	O
well	O
as	O
the	O
results	O
of	O
the	O
annotation	O
for	O
KQ	O
and	O
CQ	O
questions	O
on	O
five	O
subsets	O
.	O
From	O
the	O
table	O
,	O
we	O
make	O
the	O
following	O
observations	O
.	O
First	O
,	O
most	O
retrieved	O
documents	O
indicate	O
PM	O
with	O
the	O
questions	O
,	O
while	O
the	O
matching	O
rates	O
of	O
EM	B-MetricName
and	O
MM	O
achieve	O
maximums	O
of	O
20.83	O
%	O
(	O
CWM	O
)	O
and	O
50	O
%	O
(	O
PH	O
)	O
,	O
respectively	O
.	O
Second	O
,	O
the	O
matching	O
rate	O
of	O
CQ	O
is	O
higher	O
than	O
KQ	O
in	O
most	O
subsets	O
as	O
CQ	O
are	O
usually	O
related	O
to	O
simpler	O
concepts	O
,	O
and	O
use	O
more	O
words	O
to	O
describe	O
questions	O
,	O
which	O
leads	O
to	O
easier	O
retrieval	O
.	O
By	O
contrast	O
,	O
KQ	O
usually	O
involve	O
more	O
complex	O
concepts	O
that	O
may	O
not	O
be	O
included	O
in	O
the	O
Chinese	O
Wikipedia	O
database	O
.	O
Therefore	O
,	O
the	O
mismatching	O
rate	O
of	O
KQ	O
is	O
significantly	O
higher	O
than	O
that	O
of	O
CQ	O
.	O
Third	O
,	O
among	O
different	O
subsets	O
,	O
the	O
performance	O
in	O
the	O
subset	O
Cli	O
achieves	O
the	O
best	O
as	O
clinical	O
medicine	O
is	O
more	O
"	O
general	O
"	O
to	O
retrieve	O
compare	O
with	O
other	O
specialties	O
.	O
Whereas	O
the	O
performance	O
in	O
the	O
subset	O
PH	O
achieves	O
the	O
worst	O
as	O
the	O
Public	O
Health	O
is	O
usually	O
related	O
to	O
"	O
confusing	O
concepts	O
"	O
,	O
which	O
leads	O
to	O
poor	O
retrieval	O
performance	O
.	O

Sources	O
Both	O
books	O
and	O
Wikipedia	O
have	O
been	O
used	O
as	O
the	O
information	O
sources	O
in	O
previous	O
research	O
.	O
One	O
of	O
our	O
subsets	O
,	O
Clinic	O
,	O
has	O
been	O
studied	O
by	O
MEDQA	O
(	O
Jin	O
et	O
al	O
,	O
2020	O
)	O
as	O
a	O
subset	O
(	O
MCMLE	O
)	O
for	O
cross	O
-	O
lingual	O
research	O
.	O
MEDQA	O
uses	O
33	O
medical	O
textbooks	O
as	O
their	O
information	O
sources	O
and	O
the	O
evaluation	O
result	O
shows	O
that	O
their	O
collected	O
text	O
materials	O
can	O
provide	O
enough	O
information	O
to	O
answer	O
all	O
the	O
questions	O
in	O
MCMLE	O
.	O
We	O
compare	O
the	O
best	O
model	O
(	O
RoBERTa	B-MethodName
-	O
wwm	O
-	O
ext	O
-	O
large	O
)	O
performance	O
on	O
both	O
datasets	O
as	O
shown	O
in	O
Table	O
9	O
.	O
Notably	O
,	O
questions	O
in	O
MCMLE	O
have	O
four	O
candidate	O
options	O
due	O
to	O
one	O
of	O
the	O
wrong	O
options	O
being	O
deleted	O
.	O
Therefore	O
,	O
the	O
random	O
accuracy	B-MetricName
on	O
MCMLE	O
is	O
higher	O
than	O
ours	O
.	O
From	O
the	O
results	O
we	O
can	O
see	O
that	O
even	O
with	O
100	O
%	O
covered	O
materials	O
,	O
the	O
best	O
model	O
can	O
only	O
achieve	O
16.88	O
%	O
higher	O
accuracy	B-MetricName
on	O
the	O
test	O
set	O
than	O
ours	O
,	O
which	O
indicates	O
that	O
using	O
Wikipedia	O
as	O
information	O
sources	O
is	O
not	O
that	O
terrible	O
compared	O
with	O
medical	O
books	O
,	O
and	O
the	O
main	O
reason	O
for	O
baseline	O
performance	O
may	O
come	O
from	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
models	O
that	O
lack	O
sophisticated	O
reasoning	O
ability	O
.	O
Retriever	O
-	O
Reader	O
We	O
also	O
perform	O
an	O
experiment	O
that	O
sampled	O
5	O
%	O
(	O
92	O
)	O
questions	O
from	O
the	O
development	O
set	O
of	O
Public	O
Health	O
,	O
and	O
manually	O
annotate	O
each	O
question	O
by	O
a	O
medical	O
expert	O
to	O
determine	O
whether	O
that	O
can	O
exactly	O
or	O
partially	O
match	O
with	O
the	O
top	O
K	O
retrieved	O
documents	O
,	O
as	O
shown	O
in	O
Table	O
10	O
.	O
Notably	O
,	O
the	O
actual	O
number	O
of	O
retrieved	O
documents	O
is	O
5×K	O
as	O
we	O
define	O
Q	O
i	O
O	O
ij	O
=	O
Q	O
i	O
+	O
O	O
ij	O
as	O
a	O
search	O
query	O
and	O
is	O
repeated	O
for	O
all	O
options	O
.	O
From	O
the	O
results	O
,	O
we	O
can	O
see	O
that	O
more	O
documents	O
even	O
bring	O
more	O
noise	O
instead	O
,	O
as	O
the	O
best	O
match	O
documents	O
have	O
already	O
been	O
fetched	O
in	O
the	O
top	O
1	O
documents	O
.	O
It	O
indicates	O
that	O
the	O
poor	O
performance	O
of	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
models	O
is	O
coming	O
from	O
the	O
insufficiency	O
of	O
reasoning	O
ability	O
rather	O
than	O
the	O
number	O
of	O
retrieved	O
documents	O
.	O
In	O
order	O
to	O
benefit	O
researchers	O
on	O
improving	O
the	O
open	O
-	O
domain	O
QA	O
models	O
,	O
and	O
also	O
make	O
advances	O
for	O
Biomedical	O
Question	B-TaskName
Answering	I-TaskName
(	O
BQA	O
)	O
systems	O
,	O
we	O
present	O
MLEC	O
-	O
QA	O
,	O
the	O
largest	O
-	O
scale	O
Chinese	O
multi	O
-	O
choice	O
BQA	O
dataset	O
to	O
date	O
.	O

We	O
use	O
the	O
following	O
notation	O
.	O
If	O
n	O
is	O
an	O
integer	O
,	O
[	O
n	O
]	O
denotes	O
the	O
set	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
Let	O
Γ	B-HyperparameterName
be	O
an	O
alphabet	O
,	O
i.e.	O
,	O
a	O
finite	O
set	O
.	O
Then	O
s	O
Γ	B-HyperparameterName
*	O
denotes	O
that	O
s	O
is	O
a	O
sequence	O
of	O
arbitrary	O
length	O
,	O
each	O
element	O
of	O
which	O
is	O
in	O
Γ.	O
We	O
denote	O
by	O
|	O
s	O
|	O
the	O
length	O
of	O
s.	O
A	O
ranked	O
alphabet	O
is	O
an	O
alphabet	O
Γ	B-HyperparameterName
paired	O
with	O
an	O
arity	O
mapping	O
(	O
i.e.	O
,	O
a	O
total	O
function	O
)	O
rank	O
:	O
Γ	B-HyperparameterName
N.	O
Definition	O
1	O
.	O
A	O
hypergraph	O
(	O
or	O
simply	O
graph	O
)	O
over	O
a	O
ranked	O
alphabet	O
Γ	B-HyperparameterName
is	O
a	O
tuple	O
G	O
=	O
(	O
V	O
G	O
,	O
E	O
G	O
,	O
att	O
G	O
,	O
lab	O
G	O
,	O
ext	O
G	O
)	O
where	O
V	O
G	O
is	O
a	O
finite	O
set	O
of	O
nodes	O
;	O
E	O
G	O
is	O
a	O
finite	O
set	O
of	O
edges	O
(	O
distinct	O
from	O
V	O
G	O
)	O
;	O
att	O
G	O
:	O
E	O
G	O
V	O
*	O
G	O
maps	O
each	O
edge	O
to	O
a	O
sequence	O
of	O
nodes	O
;	O
lab	O
G	O
:	O
E	O
G	O
Γ	B-HyperparameterName
maps	O
each	O
edge	O
to	O
a	O
label	O
such	O
that	O
|	O
att	O
G	O
(	O
e	O
)	O
|	O
=	O
rank	O
(	O
lab	O
G	O
(	O
e	O
)	O
)	O
;	O
and	O
ext	O
G	O
is	O
an	O
ordered	O
subset	O
of	O
V	O
G	O
called	O
the	O
external	O
nodes	O
of	O
G.	O
We	O
assume	O
that	O
the	O
elements	O
of	O
ext	O
G	O
are	O
pairwise	O
distinct	O
,	O
and	O
the	O
elements	O
of	O
att	O
G	O
(	O
e	O
)	O
for	O
each	O
edge	O
e	O
are	O
also	O
pairwise	O
distinct	O
.	O
An	O
edge	O
e	O
is	O
attached	O
to	O
its	O
nodes	O
by	O
tentacles	O
,	O
each	O
labeled	O
by	O
an	O
integer	O
indicating	O
the	O
node	O
's	O
position	O
in	O
att	O
G	O
(	O
e	O
)	O
=	O
(	O
v	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
k	O
)	O
.	O
The	O
tentacle	O
from	O
e	O
to	O
v	O
i	O
will	O
have	O
label	O
i	O
,	O
so	O
the	O
tentacle	O
labels	O
lie	O
in	O
the	O
set	O
[	O
k	O
]	O
where	O
k	B-HyperparameterName
=	I-HyperparameterName
rank	O
(	O
e	O
)	O
.	O
To	O
express	O
that	O
a	O
node	O
v	O
is	O
attached	O
to	O
the	O
ith	O
tentacle	O
of	O
an	O
edge	O
e	O
,	O
we	O
say	O
vert	O
(	O
e	O
,	O
i	O
)	O
=	O
v.	O
Likewise	O
,	O
the	O
nodes	O
in	O
ext	O
G	O
are	O
labeled	O
by	O
their	O
position	O
in	O
ext	O
G	O
.	O
We	O
refer	O
to	O
the	O
ith	O
external	O
node	O
of	O
G	O
by	O
ext	O
G	O
(	O
i	O
)	O
and	O
in	O
figures	O
this	O
will	O
be	O
labeled	O
(	O
i	O
)	O
.	O
The	O
rank	O
of	O
an	O
edge	O
e	O
is	O
k	O
if	O
att	O
(	O
e	O
)	O
=	O
(	O
v	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
k	O
)	O
(	O
or	O
equivalently	O
,	O
rank	O
(	O
lab	O
(	O
e	O
)	O
)	O
=	O
k	O
)	O
.	O
The	O
rank	O
of	O
a	O
hypergraph	O
G	O
,	O
denoted	O
by	O
rank	O
(	O
G	O
)	O
is	O
the	O
size	O
of	O
ext	O
G	O
.	O
Example	O
1	O
.	O
Hypergraph	O
G	O
in	O
Figure	O
2	O
has	O
four	O
nodes	O
(	O
shown	O
as	O
black	O
dots	O
)	O
and	O
three	O
hyperedges	O
labeled	O
a	O
,	O
b	O
,	O
and	O
X	O
(	O
shown	O
boxed	O
)	O
.	O
The	O
bracketed	O
numbers	O
(	O
1	O
)	O
and	O
(	O
2	O
)	O
denote	O
its	O
external	O
nodes	O
and	O
the	O
numbers	O
between	O
edges	O
and	O
the	O
nodes	O
are	O
tentacle	O
labels	O
.	O
Call	O
the	O
top	O
node	O
v	O
1	O
and	O
,	O
proceeding	O
clockwise	O
,	O
call	O
the	O
other	O
nodes	O
v	O
2	O
,	O
v	O
3	O
,	O
and	O
v	O
4	O
.	O
Call	O
its	O
edges	O
e	O
1	O
,	O
e	O
2	O
and	O
e	O
3	O
.	O
Its	O
definition	O
would	O
state	O
att	O
G	O
(	O
e	O
1	O
)	O
=	O
(	O
v	O
1	O
,	O
v	O
2	O
)	O
,	O
att	O
G	O
(	O
e	O
2	O
)	O
=	O
(	O
v	O
2	O
,	O
v	O
3	O
)	O
,	O
att	O
G	O
(	O
e	O
3	O
)	O
=	O
(	O
v	O
1	O
,	O
v	O
4	O
,	O
v	O
3	O
)	O
,	O
lab	O
G	O
(	O
e	O
1	O
)	O
=	O
a	O
,	O
lab	O
G	O
(	O
e	O
2	O
)	O
=	O
b	O
,	O
lab	O
G	O
(	O
e	O
3	O
)	O
=	O
X	O
,	O
and	O
ext	O
G	O
=	O
(	O
v	O
4	O
,	O
v	O
2	O
)	O
.	O
Definition	O
2	O
.	O
Let	O
G	O
be	O
a	O
hypergraph	O
containing	O
an	O
edge	O
e	O
with	O
att	O
G	O
(	O
e	O
)	O
=	O
(	O
v	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
k	O
)	O
and	O
let	O
H	O
be	O
a	O
hypergraph	O
of	O
rank	O
k	O
with	O
node	O
and	O
edge	O
sets	O
disjoint	O
from	O
those	O
of	O
G.	O
The	O
replacement	O
of	O
e	O
by	O
H	O
is	O
the	O
graph	O
G	O
=	O
G	O
[	O
e	O
/	O
H	O
]	O
.	O
Its	O
node	O
set	O
V	O
G	O
is	O
V	O
∪	O
V	O
H	O
where	O
V	O
=	O
V	O
G	O
−	O
{	O
v	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
k	O
}	O
.	O
Its	O
edge	O
set	O
is	O
E	O
G	O
=	O
(	O
E	O
G	O
−	O
{	O
e	O
}	O
)	O
∪	O
E	O
H	O
.	O
We	O
define	O
att	O
G	O
=	O
att	O
∪	O
att	O
H	O
where	O
for	O
every	O
e	O
(	O
E	O
G	O
−	O
{	O
e	O
}	O
)	O
,	O
att	O
(	O
e	O
)	O
is	O
obtained	O
from	O
att	O
G	O
(	O
e	O
)	O
by	O
replacing	O
v	O
i	O
by	O
the	O
ith	O
external	O
node	O
of	O
H.	O
Let	O
lab	O
G	O
=	O
lab	O
∪	O
lab	O
H	O
where	O
lab	O
is	O
the	O
restriction	O
of	O
lab	O
G	O
to	O
edges	O
in	O
E	O
G	O
−	O
{	O
e	O
}	O
.	O
Finally	O
,	O
let	O
ext	O
G	O
=	O
ext	O
G	O
.	O
Example	O
2	O
.	O
A	O
replacement	O
is	O
shown	O
in	O
Figure	O
2	O
.	O

A	O
regular	O
graph	O
grammar	O
(	O
RGG	O
;	O
Courcelle	O
1991	O
)	O
is	O
a	O
restricted	O
form	O
of	O
HRG	O
.	O
To	O
explain	O
the	O
restrictions	O
,	O
we	O
first	O
require	O
some	O
definitions	O
.	O
Definition	O
4	O
.	O
Given	O
a	O
graph	O
G	O
,	O
a	O
path	O
in	O
G	O
from	O
a	O
node	O
v	O
to	O
a	O
node	O
v	O
is	O
a	O
sequence	O
(	O
v	O
0	B-DatasetName
,	O
i	O
1	O
,	O
e	O
1	O
,	O
j	O
1	O
,	O
v	O
1	O
)	O
(	O
v	O
1	O
,	O
i	O
2	O
,	O
e	O
2	O
,	O
j	O
2	O
,	O
v	O
2	O
)	O
.	O
.	O
.	O
(	O
v	O
k−1	O
,	O
i	O
k	O
,	O
e	O
k	O
,	O
j	O
k	O
,	O
v	O
k	O
)	O
(	O
1	O
)	O
such	O
that	O
v	O
0	B-DatasetName
=	O
v	O
,	O
v	O
k	B-HyperparameterName
=	I-HyperparameterName
v	O
,	O
and	O
for	O
each	O
r	O
[	O
k	O
]	O
,	O
vert	O
(	O
e	O
r	O
,	O
i	O
r	O
)	O
=	O
v	O
r−1	O
and	O
vert	O
(	O
e	O
r	O
,	O
j	O
r	O
)	O
=	O
v	O
r	O
.	O
The	O
length	O
of	O
this	O
path	O
is	O
k.	O
A	O
path	O
is	O
terminal	O
if	O
every	O
edge	O
in	O
the	O
path	O
has	O
a	O
terminal	O
label	O
.	O
A	O
path	O
is	O
internal	O
if	O
each	O
v	O
i	O
is	O
internal	O
for	O
1	O
≤	O
i	O
≤	O
k	O
−	O
1	O
.	O
Note	O
that	O
the	O
endpoints	O
v	O
0	B-DatasetName
and	O
v	O
k	O
of	O
an	O
internal	O
path	O
can	O
be	O
external	O
.	O
Definition	O
5	O
.	O
A	O
HRG	O
G	O
is	O
a	O
Regular	O
Graph	O
Grammar	O
(	O
or	O
simply	O
RGG	O
)	O
if	O
each	O
nonterminal	O
in	O
N	O
G	O
has	O
rank	O
at	O
least	O
one	O
and	O
for	O
each	O
p	O
P	O
G	O
the	O
following	O
hold	O
:	O
(	O
C1	O
)	O
R	O
(	O
p	O
)	O
has	O
at	O
least	O
one	O
edge	O
.	O
Either	O
it	O
is	O
a	O
single	O
terminal	O
edge	O
,	O
all	O
nodes	O
of	O
which	O
are	O
external	O
,	O
or	O
each	O
of	O
its	O
edges	O
has	O
at	O
least	O
one	O
internal	O
node	O
.	O
(	O
C2	O
)	O
Every	O
pair	O
of	O
nodes	O
in	O
R	O
(	O
p	O
)	O
is	O
connected	O
by	O
a	O
terminal	O
and	O
internal	O
path	O
.	O
Example	O
4	O
.	O
The	O
grammar	O
in	O
Table	O
1	O
is	O
an	O
RGG	O
.	O
Although	O
HRGs	O
can	O
produce	O
context	O
-	O
free	O
languages	O
(	O
and	O
beyond	O
)	O
as	O
shown	O
in	O
Figure	O
4	O
,	O
the	O
only	O
string	O
languages	O
RGGs	O
can	O
produce	O
are	O
the	O
regular	O
string	O
languages	O
.	O
See	O
Figure	O
5	O
for	O
an	O
example	O
of	O
a	O
string	O
generating	O
RGG	O
.	O
Similarly	O
,	O
RGGs	O
can	O
produce	O
regular	O
tree	O
languages	O
,	O
but	O
not	O
context	O
-	O
free	O
tree	O
languages	O
.	O
Figure	O
6	O
shows	O
a	O
tree	O
generating	O
RGG	O
that	O
generates	O
binary	O
trees	O
the	O
internal	O
nodes	O
of	O
which	O
are	O
represented	O
by	O
a	O
-	O
labeled	O
edges	O
,	O
and	O
the	O
leaves	O
of	O
which	O
are	O
represented	O
by	O
b	O
-	O
labeled	O
edges	O
.	O
Note	O
that	O
these	O
two	O
results	O
of	O
regularity	O
of	O
the	O
string	O
-	O
and	O
tree	O
-	O
languages	O
generated	O
by	O
RGG	O
follow	O
from	O
the	O
fact	O
that	O
graph	O
languages	O
produced	O
by	O
RGG	O
are	O
MSO	O
-	O
definable	O
(	O
Courcelle	O
,	O
1991	O
)	O
,	O
and	O
the	O
well	O
-	O
known	O
facts	O
that	O
the	O
regular	O
string	O
and	O
graph	O
languages	O
are	O
MSO	O
-	O
definable	O
.	O
X	O
(	O
1	O
)	O
a	O
Y	O
(	O
1	O
)	O
b	O
1	O
1	O
Figure	O
5	O
:	O
A	O
RGG	O
for	O
a	O
regular	O
string	O
language	O
.	O

Just	O
as	O
the	O
algorithm	O
of	O
Chiang	O
et	O
al	O
(	O
2013	O
)	O
generalizes	O
CKY	O
to	O
HRG	O
,	O
our	O
algorithm	O
generalizes	O
Earley	O
's	O
algorithm	O
(	O
Earley	O
,	O
1970	O
)	O
.	O
Both	O
algorithms	O
operate	O
by	O
recognizing	O
incrementally	O
larger	O
subgraphs	O
of	O
the	O
input	O
graph	O
,	O
using	O
a	O
succinct	O
representation	O
for	O
subgraphs	O
that	O
depends	O
on	O
an	O
arbitrarily	O
chosen	O
marker	O
node	O
m	O
of	O
the	O
input	O
graph	O
.	O
For	O
each	O
production	O
p	O
of	O
the	O
grammar	O
,	O
we	O
impose	O
a	O
fixed	O
order	O
on	O
the	O
edges	O
of	O
R	O
(	O
p	O
)	O
,	O
as	O
in	O
Drewes	O
et	O
al	O
(	O
2015	O
)	O
.	O
We	O
discuss	O
this	O
order	O
in	O
detail	O
in	O
3.2	O
.	O
As	O
in	O
Earley	O
's	O
algorithm	O
,	O
we	O
use	O
dotted	O
rules	O
to	O
represent	O
partial	O
recognition	O
of	O
productions	O
:	O
X	O
ē	O
1	O
.	O
.	O
.ē	O
i−1	O
ē	O
i	O
.	O
.	O
.ē	O
n	O
means	O
that	O
we	O
have	O
identified	O
the	O
edgesē	O
1	O
toē	O
i−1	O
and	O
that	O
we	O
must	O
next	O
recognize	O
edgeē	O
i	O
.	O
We	O
writeē	O
and	O
v	O
for	O
edges	O
and	O
nodes	O
in	O
productions	O
and	O
e	O
and	O
v	O
for	O
edges	O
and	O
nodes	O
in	O
a	O
derived	O
graph	O
.	O
When	O
the	O
identity	O
of	O
the	O
sequence	O
is	O
immaterial	O
we	O
abbreviate	O
it	O
as	O
α	B-HyperparameterName
,	O
for	O
example	O
writing	O
X	O
α	B-HyperparameterName
.	O
We	O
present	O
our	O
recognizer	O
as	O
a	O
deductive	O
proof	O
system	O
(	O
Shieber	O
et	O
al	O
,	O
1995	O
)	O
.	O
The	O
items	O
of	O
the	O
recognizer	O
are	O
of	O
the	O
form	O
Name	O
Rule	O
Conditions	O
PREDICT	O
[	O
b	O
(	O
I	O
)	O
,	O
p	O
:	O
X	O
ē1	O
.	O
.	O
.	O
ē	O
i	O
.	O
.	O
.ēn	O
,	O
φp	O
]	O
[	O
q	O
:	O
Y	O
α	B-HyperparameterName
]	O
[	O
φp	O
(	O
ēi	O
)	O
,	O
q	O
:	O
Y	O
α	B-HyperparameterName
,	O
φ	O
0	B-DatasetName
q	O
[	O
ext	O
R	O
(	O
q	O
)	O
=	O
φp	O
(	O
ēi	O
)	O
]	O
]	O
lab	O
(	O
ēi	O
)	O
=	O
Y	O
SCAN	B-DatasetName
[	O
b	O
(	O
I	O
)	O
,	O
X	O
ē1	O
.	O
.	O
.	O
ē	O
i	O
.	O
.	O
.ēn	O
,	O
φp	O
]	O
[	O
e	O
=	O
edg	O
lab	O
(	O
ē	O
i	O
)	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vm	O
)	O
]	O
[	O
b	O
(	O
I	O
∪	O
{	O
e	O
}	O
)	O
,	O
X	O
ē1	O
.	O
.	O
.	O
ē	O
i+1	O
.	O
.	O
.ēn	O
,	O
φp	O
[	O
att	O
(	O
ēi	O
)	O
=	O
(	O
v1	O
,	O
.	O
.	O
.	O
,	O
vm	O
)	O
]	O
]	O
φp	O
(	O
ēi	O
)	O
(	O
j	O
)	O
VG	O
⇒	O
φp	O
(	O
ēi	O
)	O
(	O
j	O
)	O
=	O
vert	O
(	O
e	O
,	O
j	O
)	O
COMPLETE	O
[	O
b	O
(	O
I	O
)	O
,	O
p	O
:	O
X	O
ē1	O
.	O
.	O
.	O
ē	O
i	O
.	O
.	O
.ēn	O
,	O
φp	O
]	O
[	O
b	O
(	O
J	O
)	O
,	O
q	O
:	O
Y	O
α	B-HyperparameterName
,	O
φq	O
]	O
[	O
b	O
(	O
I	O
∪	O
J	O
)	O
,	O
X	O
ē1	O
.	O
.	O
.	O
ē	O
i+1	O
.	O
.	O
.ēn	O
,	O
φp	O
[	O
att	O
(	O
ēi	O
)	O
=	O
φp	O
(	O
ext	O
R	O
(	O
q	O
)	O
)	O
]	O
]	O
φp	O
(	O
ēi	O
)	O
(	O
j	O
)	O
VG	O
⇒	O
φp	O
(	O
ēi	O
)	O
(	O
j	O
)	O
=	O
φq	O
(	O
ext	O
R	O
(	O
q	O
)	O
)	O
(	O
j	O
)	O
,	O
lab	O
(	O
ēi	O
)	O
=	O
Y	O
,	O
EI	O
∩	O
EJ	O
=	O
[	O
b	O
(	O
I	O
)	O
,	O
p	O
:	O
X	O
ē	O
1	O
.	O
.	O
.	O
ē	O
i	O
.	O
.	O
.ē	O
n	O
,	O
φ	O
p	O
]	O
where	O
I	O
is	O
a	O
subgraph	O
that	O
has	O
been	O
recognized	O
as	O
matchingē	O
1	O
,	O
.	O
.	O
.	O
,	O
ē	O
i−1	O
;	O
p	O
:	O
X	O
ē	O
1	O
,	O
.	O
.	O
.	O
,	O
ē	O
n	O
is	O
a	O
production	O
in	O
the	O
grammar	O
with	O
the	O
edges	O
in	O
order	O
;	O
and	O
φ	O
p	O
:	O
E	O
R	O
(	O
p	O
)	O
V	O
*	O
G	O
maps	O
the	O
endpoints	O
of	O
edges	O
in	O
R	O
(	O
p	O
)	O
to	O
nodes	O
in	O
G.	O
For	O
each	O
production	O
p	O
,	O
we	O
number	O
the	O
nodes	O
in	O
some	O
arbitrary	O
but	O
fixed	O
order	O
.	O
Using	O
this	O
,	O
we	O
construct	O
the	O
function	O
φ	O
0	B-DatasetName
p	O
:	O
E	O
R	O
(	O
p	O
)	O
V	O
*	O
R	O
(	O
p	O
)	O
such	O
that	O
forē	O
E	O
R	O
(	O
p	O
)	O
if	O
att	O
(	O
ē	O
)	O
=	O
(	O
v	O
1	O
,	O
v	O
2	O
)	O
then	O
φ	O
0	B-DatasetName
p	O
(	O
ē	O
)	O
=	O
(	O
v	O
1	O
,	O
v	O
2	O
)	O
.	O
As	O
we	O
match	O
edges	O
in	O
the	O
graph	O
with	O
edges	O
in	O
p	O
,	O
we	O
assign	O
the	O
nodesv	O
to	O
nodes	O
in	O
the	O
graph	O
.	O
For	O
example	O
,	O
if	O
we	O
have	O
an	O
edgeē	O
in	O
a	O
production	O
p	O
such	O
that	O
att	O
(	O
ē	O
)	O
=	O
(	O
v	O
1	O
,	O
v	O
2	O
)	O
and	O
we	O
find	O
an	O
edge	O
e	O
which	O
matchesē	O
,	O
then	O
we	O
update	O
φ	O
p	O
to	O
record	O
this	O
fact	O
,	O
written	O
φ	O
p	O
[	O
att	O
(	O
ē	O
)	O
=	O
att	O
(	O
e	O
)	O
]	O
.	O
We	O
also	O
use	O
φ	O
p	O
to	O
record	O
assignments	O
of	O
external	O
nodes	O
.	O
If	O
we	O
assign	O
the	O
ith	O
external	O
node	O
to	O
v	O
,	O
we	O
write	O
φ	O
p	O
[	O
ext	O
p	O
(	O
i	O
)	O
=	O
v	O
]	O
.	O
We	O
write	O
φ	O
0	B-DatasetName
p	O
to	O
represent	O
a	O
mapping	O
with	O
no	O
grounded	O
nodes	O
.	O
Since	O
our	O
algorithm	O
makes	O
top	O
-	O
down	O
predictions	O
based	O
on	O
known	O
external	O
nodes	O
,	O
our	O
boundary	O
representation	O
must	O
cover	O
the	O
case	O
where	O
a	O
subgraph	O
is	O
empty	O
except	O
for	O
these	O
nodes	O
.	O
If	O
at	O
some	O
point	O
we	O
know	O
that	O
our	O
subgraph	O
has	O
external	O
nodes	O
φ	O
(	O
ē	O
)	O
,	O
then	O
we	O
use	O
the	O
shorthand	O
φ	O
(	O
ē	O
)	O
rather	O
than	O
the	O
full	O
boundary	O
representation	O
φ	O
(	O
ē	O
)	O
,	O
,	O
m	O
φ	O
(	O
ē	O
)	O
.	O
To	O
keep	O
notation	O
uniform	O
,	O
we	O
use	O
dummy	O
nonterminal	O
S	O
*	O
N	O
G	O
that	O
derives	O
S	O
G	O
via	O
the	O
production	O
p	O
0	B-DatasetName
.	O
For	O
graph	O
G	O
,	O
our	O
system	O
includes	O
the	O
axiom	O
:	O
[	O
ext	O
G	O
,	O
p	O
0	B-DatasetName
:	O
S	O
*	O
S	O
G	O
,	O
φ	O
0	B-DatasetName
p	O
0	B-DatasetName
[	O
ext	O
R	O
(	O
p	O
0	B-DatasetName
)	O
=	O
ext	O
G	O
]	O
]	O
.	O
Our	O
goal	O
is	O
to	O
prove	O
:	O
[	O
b	O
(	O
G	O
)	O
,	O
p	O
S	O
:	O
S	O
*	O
S	O
G	O
,	O
φ	O
p	O
S	O
]	O
where	O
φ	O
p	O
S	O
has	O
a	O
single	O
edgeē	O
in	O
its	O
domain	O
which	O
has	O
label	O
S	O
G	O
in	O
R	O
(	O
p	O
S	O
)	O
and	O
φ	O
p	O
S	O
(	O
ē	O
)	O
=	O
ext	O
G	O
.	O
As	O
in	O
Earley	O
's	O
algorithm	O
,	O
we	O
have	O
three	O
inference	O
rules	O
:	O
PREDICT	O
,	O
SCAN	B-DatasetName
and	O
COMPLETE	O
(	O
Table	O
2	O
)	O
.	O
PREDICT	O
is	O
applied	O
when	O
the	O
edge	O
after	O
the	O
dot	O
is	O
nonterminal	O
,	O
assigning	O
any	O
external	O
nodes	O
that	O
have	O
been	O
identified	O
.	O
SCAN	B-DatasetName
is	O
applied	O
when	O
the	O
edge	O
after	O
the	O
dot	O
is	O
terminal	O
.	O
Using	O
φ	O
p	O
,	O
we	O
may	O
already	O
know	O
where	O
some	O
of	O
the	O
endpoints	O
of	O
the	O
edge	O
should	O
be	O
,	O
so	O
it	O
requires	O
the	O
endpoints	O
of	O
the	O
scanned	O
edge	O
to	O
match	O
.	O
COMPLETE	O
requires	O
that	O
each	O
of	O
the	O
nodes	O
ofē	O
i	O
in	O
R	O
(	O
p	O
)	O
have	O
been	O
identified	O
,	O
these	O
nodes	O
match	O
up	O
with	O
the	O
corresponding	O
external	O
nodes	O
of	O
the	O
subgraph	O
J	O
,	O
and	O
that	O
the	O
subgraphs	O
I	O
and	O
J	O
are	O
edge	O
-	O
disjoint	O
.	O
We	O
provide	O
a	O
high	O
-	O
level	O
proof	O
that	O
the	O
recognizer	O
is	O
sound	O
and	O
complete	O
.	O
Proposition	O
1	O
.	O
Let	O
G	O
be	O
a	O
HRG	O
and	O
G	O
a	O
graph	O
.	O

The	O
Transformer	B-MethodName
architecture	O
has	O
become	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
Machine	B-TaskName
Translation	I-TaskName
.	O
This	O
model	O
,	O
which	O
relies	O
on	O
attention	O
-	O
based	O
mechanisms	O
,	O
has	O
outperformed	O
previous	O
neural	O
machine	B-TaskName
translation	I-TaskName
architectures	O
in	O
several	O
tasks	O
.	O
In	O
this	O
system	O
description	O
paper	O
,	O
we	O
report	O
details	O
of	O
training	O
neural	O
machine	B-TaskName
translation	I-TaskName
with	O
multi	O
-	O
source	O
Romance	O
languages	O
with	O
the	O
Transformer	B-MethodName
model	O
and	O
in	O
the	O
evaluation	O
frame	O
of	O
the	O
biomedical	O
WMT	B-DatasetName
2018	I-DatasetName
task	O
.	O
Using	O
multi	O
-	O
source	O
languages	O
from	O
the	O
same	O
family	O
allows	O
improvements	O
of	O
over	O
6	O
BLEU	B-MetricName
points	O
.	O

Neural	O
Machine	B-TaskName
Translation	I-TaskName
(	O
NMT	O
)	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
proved	O
to	O
be	O
competitive	O
with	O
the	O
encoder	O
-	O
decoder	O
architecture	O
based	O
on	O
recurrent	O
neural	O
networks	O
and	O
attention	O
.	O
After	O
this	O
architecture	O
,	O
new	O
proposals	O
based	O
on	O
convolutional	O
neural	O
networks	O
(	O
Gehring	O
et	O
al	O
,	O
2017	O
)	O
or	O
only	O
attention	O
-	O
based	O
mechanisms	O
appeared	O
.	O
The	O
latter	O
architecture	O
has	O
achieved	O
great	O
success	O
in	O
Machine	B-TaskName
Translation	I-TaskName
(	O
MT	O
)	O
and	O
it	O
has	O
already	O
been	O
extended	O
to	O
other	O
tasks	O
such	O
as	O
Parsing	O
,	O
Speech	B-TaskName
Recognition	I-TaskName
1	O
,	O
Speech	O
Translation	B-TaskName
(	O
Cros	O
et	O
al	O
,	O
2018	O
)	O
,	O
Chatbots	O
among	O
others	O
.	O
However	O
,	O
training	O
with	O
low	O
resources	O
is	O
still	O
a	O
big	O
drawback	O
for	O
neural	O
architectures	O
and	O
NMT	O
is	O
not	O
an	O
exception	O
(	O
Koehn	O
and	O
Knowles	O
,	O
2017	O
)	O
.	O
To	O
face	O
low	O
resource	O
scenarios	O
,	O
several	O
techniques	O
have	O
been	O
proposed	O
,	O
like	O
using	O
multi	O
-	O
source	O
(	O
Zoph	O
and	O
Knight	O
,	O
2016	O
)	O
,	O
multiple	O
languages	O
(	O
Johnson	O
et	O
al	O
,	O
2017	O
)	O
or	O
unsupervised	O
techniques	O
(	O
Lample	O
et	O
al	O
,	O
2018	O
;	O
Artetxe	O
et	O
al	O
,	O
2018	O
)	O
,	O
among	O
many	O
others	O
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
the	O
Transformer	B-MethodName
enhanced	O
with	O
the	O
multi	O
-	O
source	O
technique	O
to	O
participate	O
in	O
the	O
Biomedical	O
WMT	B-DatasetName
2018	I-DatasetName
task	O
,	O
which	O
can	O
be	O
somehow	O
considered	O
a	O
low	O
-	O
resourced	O
task	O
,	O
given	O
the	O
large	O
quantity	O
of	O
data	O
that	O
it	O
is	O
required	O
for	O
NMT	O
.	O
Our	O
multi	O
-	O
source	O
enhancement	O
is	O
done	O
only	O
with	O
Romance	O
languages	O
.	O
The	O
fact	O
of	O
using	O
similar	O
languages	O
in	O
a	O
multi	O
-	O
source	O
system	O
may	O
be	O
a	O
factor	O
towards	O
improving	O
the	O
final	O
system	O
which	O
ends	O
up	O
with	O
over	O
6	O
BLEU	B-MetricName
points	O
of	O
improvement	O
over	O
the	O
single	O
source	O
system	O
.	O

The	O
results	O
of	O
the	O
regression	O
models	O
showed	O
an	O
association	O
between	O
the	O
estimated	O
correction	O
factors	O
and	O
the	O
proposed	O
adequacy	O
indicators	O
.	O
We	O
trained	O
three	O
single	O
-	O
language	O
systems	O
,	O
one	O
for	O
each	O
language	O
pair	O
.	O
We	O
required	O
14	O
epochs	O
for	O
the	O
Spanish	O
-	O
to	O
-	O
English	O
system	O
(	O
7	O
hours	O
of	O
training	O
)	O
,	O
16	O
epochs	O
for	O
the	O
French	O
-	O
to	O
-	O
English	O
system	O
(	O
9	O
hours	O
of	O
training	O
)	O
,	O
and	O
17	O
epochs	O
for	O
the	O
Portuguese	O
-	O
to	O
-	O
English	O
system	O
(	O
7	O
hours	O
of	O
training	O
)	O
.	O
For	O
the	O
multi	O
-	O
source	O
system	O
,	O
which	O
concatenated	O
the	O
three	O
parallel	O
corpus	O
together	O
,	O
we	O
required	O
11	O
epochs	O
(	O
23	O
hours	O
of	O
training	O
)	O
.	O
We	O
stopped	O
training	O
when	O
the	O
validation	O
accuracy	B-MetricName
did	O
not	O
increase	O
in	O
two	O
consecutive	O
epochs	O
.	O

Best	O
ranking	O
systems	O
from	O
WMT17	O
and	O
WMT18	O
are	O
shown	O
in	O
Table	O
1	O
,	O
except	O
for	O
French	O
-	O
to	O
-	O
English	O
WMT17	O
since	O
the	O
references	O
for	O
this	O
set	O
are	O
not	O
available	O
.	O
For	O
this	O
pair	O
,	O
we	O
used	O
1000	O
sentences	O
from	O
the	O
Khresmoi	O
development	O
data	O
.	O
Table	O
1	O
shows	O
BLEU	B-MetricName
results	O
for	O
the	O
baseline	O
systems	O
,	O
the	O
single	O
-	O
language	O
and	O
multi	O
-	O
source	O
approaches	O
.	O
The	O
Transformer	B-MethodName
architecture	O
outperforms	O
WMT17	O
best	O
system	O
.	O
Results	O
become	O
even	O
better	O
with	O
the	O
system	O
is	O
trained	O
with	O
the	O
common	O
corpus	O
of	O
Romance	O
languages	O
,	O
what	O
we	O
call	O
the	O
multi	O
-	O
source	O
approach	O
.	O
The	O
latter	O
is	O
consistent	O
with	O
the	O
universal	O
truth	O
that	O
more	O
data	O
equals	O
better	O
results	O
,	O
even	O
if	O
the	O
source	O
language	O
is	O
not	O
the	O
same	O
.	O
Finally	O
,	O
Table	O
2	O
shows	O
some	O
examples	O
of	O
the	O
output	O
translations	O
.	O

The	O
main	O
conclusions	O
of	O
our	O
experiments	O
are	O
that	O
the	O
multi	O
-	O
source	O
inputs	O
of	O
the	O
same	O
family	O
applied	O
to	O
the	O
Transformer	B-MethodName
architecture	O
can	O
improve	O
the	O
single	O
input	O
.	O
Best	O
improvements	O
achieve	O
an	O
increase	O
of	O
6	O
BLEU	B-MetricName
points	O
in	O
translation	O
quality	O
.	O
part	O
by	O
the	O
Spanish	O
Ministerio	O
de	O
Economía	O
y	O
Competitividad	O
,	O
the	O
European	O
Regional	O
Development	O
Fund	O
and	O
the	O
Agencia	O
Estatal	O
de	O
Investigación	O
,	O
through	O
the	O
postdoctoral	O
senior	O
grant	O
Ramón	O
y	O
Cajal	O
,	O
the	O
contract	O
TEC2015	O
-	O
69266	O
-	O
P	O
(	O
MINECO	O
/	O
FEDER	O
,	O
EU	O
)	O
and	O
the	O
contract	O
PCIN	O
-	O
2017	O
-	O
079	O
(	O
AEI	O
/	O
MINECO	O
)	O
.	O

We	O
study	O
relation	B-TaskName
extraction	I-TaskName
for	O
knowledge	O
base	O
(	O
KB	O
)	O
enrichment	O
.	O
Specifically	O
,	O
we	O
aim	O
to	O
extract	O
entities	O
and	O
their	O
relationships	O
from	O
sentences	O
in	O
the	O
form	O
of	O
triples	O
and	O
map	O
the	O
elements	O
of	O
the	O
extracted	O
triples	O
to	O
an	O
existing	O
KB	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
.	O
Previous	O
studies	O
focus	O
on	O
the	O
extraction	O
itself	O
and	O
rely	O
on	O
Named	O
Entity	B-TaskName
Disambiguation	I-TaskName
(	O
NED	O
)	O
to	O
map	O
triples	O
into	O
the	O
KB	O
space	O
.	O
This	O
way	O
,	O
NED	O
errors	O
may	O
cause	O
extraction	O
errors	O
that	O
affect	O
the	O
overall	O
precision	O
and	O
recall	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
relation	B-TaskName
extraction	I-TaskName
model	O
for	O
KB	O
enrichment	O
based	O
on	O
a	O
neural	O
encoder	O
-	O
decoder	O
model	O
.	O
We	O
collect	O
high	O
-	O
quality	O
training	O
data	O
by	O
distant	O
supervision	O
with	O
co	O
-	O
reference	O
resolution	O
and	O
paraphrase	O
detection	O
.	O
We	O
propose	O
an	O
n	O
-	O
gram	O
based	O
attention	O
model	O
that	O
captures	O
multi	O
-	O
word	O
entity	O
names	O
in	O
a	O
sentence	O
.	O
Our	O
model	O
employs	O
jointly	O
learned	O
word	O
and	O
entity	B-TaskName
embeddings	I-TaskName
to	O
support	O
named	O
entity	B-TaskName
disambiguation	I-TaskName
.	O
Finally	O
,	O
our	O
model	O
uses	O
a	O
modified	O
beam	O
search	O
and	O
a	O
triple	O
classifier	O
to	O
help	O
generate	O
high	O
-	O
quality	O
triples	O
.	O
Our	O
model	O
outperforms	O
state	O
-	O
of	O
-	O
theart	O
baselines	O
by	O
15.51	O
%	O
and	O
8.38	O
%	O
in	O
terms	O
of	O
F1	B-MetricName
score	I-MetricName
on	O
two	O
real	O
-	O
world	O
datasets	O
.	O

Knowledge	O
bases	O
(	O
KBs	O
)	O
,	O
often	O
in	O
the	O
form	O
of	O
knowledge	B-TaskName
graphs	I-TaskName
(	O
KGs	O
)	O
,	O
have	O
become	O
essential	O
resources	O
in	O
many	O
tasks	O
including	O
Q&A	O
systems	O
,	O
recommender	O
system	O
,	O
and	O
natural	O
language	O
generation	O
.	O
Large	O
KBs	O
such	O
as	O
DBpedia	B-DatasetName
(	O
Auer	O
et	O
al	O
,	O
2007	O
)	O
,	O
Wikidata	O
(	O
Vrandecic	O
and	O
Krötzsch	O
,	O
2014	O
)	O
and	O
Yago	B-DatasetName
(	O
Suchanek	O
et	O
al	O
,	O
2007	O
)	O
contain	O
millions	O
of	O
facts	O
about	O
entities	O
,	O
which	O
are	O
represented	O
in	O
the	O
form	O
of	O
subject	O
-	O
predicate	O
-	O
object	O
triples	O
.	O
However	O
,	O
these	O
KBs	O
are	O
far	O
from	O
complete	O
and	O
mandate	O
continuous	O
enrichment	O
and	O
curation	O
.	O
Previous	O
studies	O
work	O
on	O
embedding	O
-	O
based	O
model	O
(	O
Nguyen	O
et	O
al	O
,	O
2018	O
;	O
and	O
entity	B-TaskName
alignment	I-TaskName
model	O
(	O
Chen	O
et	O
al	O
,	O
2017	O
;	O
Trisedya	O
et	O
al	O
,	O
2019	O
)	O
to	O
enrich	O
a	O
knowledge	O
base	O
.	O
Following	O
the	O
success	O
of	O
the	O
sequence	O
-	O
to	O
-	O
sequence	O
architecture	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
for	O
generating	O
sentences	O
from	O
structured	O
data	O
(	O
Marcheggiani	O
and	O
Perez	O
-	O
Beltrachini	O
,	O
2018	O
;	O
Trisedya	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
employ	O
this	O
architecture	O
to	O
do	O
the	O
opposite	O
,	O
which	O
is	O
extracting	O
triples	O
from	O
a	O
sentence	O
.	O
In	O
this	O
paper	O
,	O
we	O
study	O
how	O
to	O
enrich	O
a	O
KB	O
by	O
relation	O
exaction	O
from	O
textual	O
sources	O
.	O
Specifically	O
,	O
we	O
aim	O
to	O
extract	O
triples	O
in	O
the	O
form	O
of	O
h	O
,	O
r	O
,	O
t	O
,	O
where	O
h	O
is	O
a	O
head	O
entity	O
,	O
t	O
is	O
a	O
tail	O
entity	O
,	O
and	O
r	O
is	O
a	O
relationship	O
between	O
the	O
entities	O
.	O
Importantly	O
,	O
as	O
KBs	O
typically	O
have	O
much	O
better	O
coverage	O
on	O
entities	O
than	O
on	O
relationships	O
,	O
we	O
assume	O
that	O
h	O
and	O
t	O
are	O
existing	O
entities	O
in	O
a	O
KB	O
,	O
r	O
is	O
a	O
predicate	O
that	O
falls	O
in	O
a	O
predefined	O
set	O
of	O
predicates	O
we	O
are	O
interested	O
in	O
,	O
but	O
the	O
relationship	O
h	O
,	O
r	O
,	O
t	O
does	O
not	O
exist	O
in	O
the	O
KB	O
yet	O
.	O
We	O
aim	O
to	O
find	O
more	O
relationships	O
between	O
h	O
and	O
t	O
and	O
add	O
them	O
to	O
the	O
KB	O
.	O
For	O
example	O
,	O
from	O
the	O
first	O
extracted	O
triples	O
in	O
Table	O
1	O
we	O
may	O
recognize	O
two	O
entities	O
"	O
NYU	O
"	O
(	O
abbreviation	O
of	O
New	O
York	O
University	O
)	O
and	O
"	O
Private	O
University	O
"	O
,	O
which	O
already	O
exist	O
in	O
the	O
KB	O
;	O
also	O
the	O
predicate	O
"	O
instance	O
of	O
"	O
is	O
in	O
the	O
set	O
of	O
predefined	O
predicates	O
we	O
are	O
interested	O
in	O
,	O
but	O
the	O
relationship	O
of	O
NYU	O
,	O
instance	O
of	O
,	O
Private	O
University	O
does	O
not	O
exist	O
in	O
the	O
KB	O
.	O
We	O
aim	O
to	O
add	O
this	O
relationship	O
to	O
our	O
KB	O
.	O
This	O
is	O
the	O
typical	O
situation	O
for	O
KB	O
enrichment	O
(	O
as	O
opposed	O
to	O
constructing	O
a	O
KB	O
from	O
scratch	O
or	O
performing	O
relation	B-TaskName
extraction	I-TaskName
for	O
other	O
purposes	O
,	O
such	O
as	O
Q&A	O
or	O
summarization	B-TaskName
)	O
.	O
KB	O
enrichment	O
mandates	O
that	O
the	O
entities	O
and	O
relationships	O
of	O
the	O
extracted	O
triples	O
are	O
canonicalized	O
by	O
mapping	O
them	O
to	O
their	O
proper	O
entity	O
and	O
predicate	O
IDs	O
in	O
a	O
KB	O
.	O
Table	O
1	O
illustrates	O
an	O
example	O
of	O
triples	O
extracted	O
from	O
a	O
sentence	O
.	O
The	O
entities	O
and	O
predicate	O
of	O
the	O
first	O
extracted	O
triple	O
,	O
including	O
NYU	O
,	O
instance	O
of	O
,	O
and	O
Private	O
University	O
,	O
are	O
mapped	O
to	O
their	O
unique	O
IDs	O
Q49210	O
,	O
P31	O
,	O
and	O
Q902104	O
,	O
respectively	O
,	O
to	O
comply	O
with	O
the	O
semantic	O
space	O
of	O
the	O
KB	O
.	O
Previous	O
studies	O
on	O
relation	B-TaskName
extraction	I-TaskName
have	O
employed	O
both	O
unsupervised	O
and	O
supervised	O
approaches	O
.	O
Unsupervised	O
approaches	O
typically	O
start	O
with	O
a	O
small	O
set	O
of	O
manually	O
defined	O
extraction	O
patterns	O
to	O
detect	O
entity	O
names	O
and	O
phrases	O
about	O
relationships	O
in	O
an	O
input	O
text	O
.	O
This	O
paradigm	O
is	O
known	O
as	O
Open	B-TaskName
Information	I-TaskName
Extraction	I-TaskName
(	O
Open	O
IE	O
)	O
(	O
Banko	O
et	O
al	O
,	O
2007	O
;	O
Corro	O
and	O
Gemulla	O
,	O
2013	O
;	O
Gashteovski	O
et	O
al	O
,	O
2017	O
)	O
.	O
In	O
this	O
line	O
of	O
approaches	O
,	O
both	O
entities	O
and	O
predicates	O
are	O
captured	O
in	O
their	O
surface	O
forms	O
without	O
canonicalization	O
.	O
Supervised	O
approaches	O
train	O
statistical	O
and	O
neural	O
models	O
for	O
inferring	O
the	O
relationship	O
between	O
two	O
known	O
entities	O
in	O
a	O
sentence	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
;	O
Riedel	O
et	O
al	O
,	O
2010Riedel	O
et	O
al	O
,	O
,	O
2013Zeng	O
et	O
al	O
,	O
2015	O
;	O
Lin	O
et	O
al	O
,	O
2016	O
)	O
.	O
Most	O
of	O
these	O
studies	O
employ	O
a	O
preprocessing	O
step	O
to	O
recognize	O
the	O
entities	O
.	O
Only	O
few	O
studies	O
have	O
fully	O
integrated	O
the	O
mapping	O
of	O
extracted	O
triples	O
onto	O
uniquely	O
identified	O
KB	O
entities	O
by	O
using	O
logical	O
reasoning	O
on	O
the	O
existing	O
KB	O
to	O
disambiguate	O
the	O
extracted	O
entities	O
(	O
e.g.	O
,	O
(	O
Suchanek	O
et	O
al	O
,	O
2009	O
;	O
Sa	O
et	O
al	O
,	O
2017	O
)	O
)	O
.	O
Most	O
existing	O
methods	O
thus	O
entail	O
the	O
need	O
for	O
Named	O
Entity	B-TaskName
Disambiguation	I-TaskName
(	O
NED	O
)	O
(	O
cf	O
.	O
the	O
survey	O
by	O
Shen	O
et	O
al	O
(	O
2015	O
)	O
)	O
as	O
a	O
separate	O
processing	O
step	O
.	O
In	O
addition	O
,	O
the	O
mapping	O
of	O
relationship	O
phrases	O
onto	O
KB	O
predicates	O
necessitates	O
another	O
mapping	O
step	O
,	O
typically	O
aided	O
by	O
paraphrase	O
dictionaries	O
.	O
This	O
two	O
-	O
stage	O
architecture	O
is	O
inherently	O
prone	O
to	O
error	O
propagation	O
across	O
its	O
two	O
stages	O
:	O
NED	O
errors	O
may	O
cause	O
extraction	O
errors	O
(	O
and	O
vice	O
versa	O
)	O
that	O
lead	O
to	O
inaccurate	O
relationships	O
being	O
added	O
to	O
the	O
KB	O
.	O
We	O
aim	O
to	O
integrate	O
the	O
extraction	O
and	O
the	O
canonicalization	O
tasks	O
by	O
proposing	O
an	O
endto	O
-	O
end	O
neural	O
learning	O
model	O
to	O
jointly	O
extract	O
triples	O
from	O
sentences	O
and	O
map	O
them	O
into	O
an	O
existing	O
KB	O
.	O
Our	O
method	O
is	O
based	O
on	O
the	O
encoder	O
-	O
decoder	O
framework	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
)	O
by	O
treating	O
the	O
task	O
as	O
a	O
translation	O
of	O
a	O
sentence	O
into	O
a	O
sequence	O
of	O
elements	O
of	O
triples	O
.	O
For	O
the	O
example	O
in	O
Table	O
1	O
,	O
our	O
model	O
aims	O
to	O
translate	O
"	O
New	O
York	O
University	O
is	O
a	O
private	O
university	O
in	O
Manhattan	O
"	O
into	O
a	O
sequence	O
of	O
IDs	O
"	O
Q49210	O
P31	O
Q902104	O
Q49210	O
P131	O
Q11299	O
"	O
,	O
from	O
which	O
we	O
can	O
derive	O
two	O
triples	O
to	O
be	O
added	O
to	O
the	O
KB	O
.	O
A	O
standard	O
encoder	O
-	O
decoder	O
model	O
with	O
attention	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
is	O
,	O
however	O
,	O
unable	O
to	O
capture	O
the	O
multi	O
-	O
word	O
entity	O
names	O
and	O
verbal	O
or	O
noun	O
phrases	O
that	O
denote	O
predicates	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
propose	O
a	O
novel	O
form	O
of	O
n	O
-	O
gram	O
based	O
attention	O
that	O
computes	O
the	O
ngram	O
combination	O
of	O
attention	O
weight	O
to	O
capture	O
the	O
verbal	O
or	O
noun	O
phrase	O
context	O
that	O
complements	O
the	O
word	O
level	O
attention	O
of	O
the	O
standard	O
attention	O
model	O
.	O
Our	O
model	O
thus	O
can	O
better	O
capture	O
the	O
multi	O
-	O
word	O
context	O
of	O
entities	O
and	O
relationships	O
.	O
Our	O
model	O
harnesses	O
pre	O
-	O
trained	O
word	O
and	O
entity	B-TaskName
embeddings	I-TaskName
that	O
are	O
jointly	O
learned	O
with	O
skip	O
gram	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
andTransE	O
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
advantages	O
of	O
our	O
jointly	O
learned	O
embeddings	O
are	O
twofold	O
.	O
First	O
,	O
the	O
embeddings	O
capture	O
the	O
relationship	O
between	O
words	O
and	O
entities	O
,	O
which	O
is	O
essential	O
for	O
named	O
entity	B-TaskName
disambiguation	I-TaskName
.	O
Second	O
,	O
the	O
entity	B-TaskName
embeddings	I-TaskName
preserve	O
the	O
relationships	O
between	O
entities	O
,	O
which	O
help	O
to	O
build	O
a	O
highly	O
accurate	O
classifier	O
to	O
filter	O
the	O
invalid	O
extracted	O
triples	O
.	O
To	O
cope	O
with	O
the	O
lack	O
of	O
fully	O
labeled	O
training	O
data	O
,	O
we	O
adapt	O
distant	O
supervision	O
to	O
generate	O
aligned	O
pairs	O
of	O
sentence	O
and	O
triple	O
as	O
the	O
training	O
data	O
.	O
We	O
augment	O
the	O
process	O
with	O
co	O
-	O
reference	O
resolution	O
(	O
Clark	O
and	O
Manning	O
,	O
2016	O
)	O
and	O
dictionary	O
-	O
based	O
paraphrase	O
detection	O
(	O
Ganitkevitch	O
et	O
al	O
,	O
2013	O
;	O
Grycner	O
and	O
Weikum	O
,	O
2016	O
)	O
.	O
The	O
co	O
-	O
reference	O
resolution	O
helps	O
extract	O
sentences	O
with	O
implicit	O
entity	O
names	O
,	O
which	O
enlarges	O
the	O
set	O
of	O
candidate	O
sentences	O
to	O
be	O
aligned	O
with	O
existing	O
triples	O
in	O
a	O
KB	O
.	O
The	O
paraphrase	O
detection	O
helps	O
filter	O
sentences	O
that	O
do	O
not	O
express	O
any	O
relationships	O
between	O
entities	O
.	O
The	O
main	O
contributions	O
of	O
this	O
paper	O
are	O
:	O
We	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
model	O
for	O
extract	O
-	O
(	O
Lin	O
et	O
al	O
,	O
2016	O
)	O
coupled	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
NED	O
models	O
(	O
Hoffart	O
et	O
al	O
,	O
2011	O
;	O
Kolitsas	O
et	O
al	O
,	O
2018	O
)	O
.	O
2	O
Related	O
Work	O
2.1	O
Open	B-TaskName
Information	I-TaskName
Extraction	I-TaskName
Banko	O
et	O
al	O
(	O
2007	O
)	O
introduced	O
the	O
paradigm	O
of	O
Open	B-TaskName
Information	I-TaskName
Extraction	I-TaskName
(	O
Open	O
IE	O
)	O
and	O
proposed	O
a	O
pipeline	O
that	O
consists	O
of	O
three	O
stages	O
:	O
learner	O
,	O
extractor	O
,	O
and	O
assessor	O
.	O
The	O
learner	O
uses	O
dependency	O
-	O
parsing	O
information	O
to	O
learn	O
patterns	O
for	O
extraction	O
,	O
in	O
an	O
unsupervised	O
way	O
.	O
The	O
extractor	O
generates	O
candidate	O
triples	O
by	O
identifying	O
noun	O
phrases	O
as	O
arguments	O
and	O
connecting	O
phrases	O
as	O
predicates	O
.	O
The	O
assessor	O
assigns	O
a	O
probability	O
to	O
each	O
candidate	O
triple	O
based	O
on	O
statistical	O
evidence	O
.	O
This	O
approach	O
was	O
prone	O
to	O
extracting	O
incorrect	O
,	O
verbose	O
and	O
uninformative	O
triples	O
.	O
Various	O
followup	B-DatasetName
studies	O
(	O
Fader	O
et	O
al	O
,	O
2011	O
;	O
Mausam	O
et	O
al	O
,	O
2012	O
;	O
Angeli	O
et	O
al	O
,	O
2015	O
;	O
Mausam	O
,	O
2016	O
)	O
improved	O
the	O
accuracy	B-MetricName
of	O
Open	O
IE	O
,	O
by	O
adding	O
handcrafted	O
patterns	O
or	O
by	O
using	O
distant	O
supervision	O
.	O
Corro	O
and	O
Gemulla	O
(	O
2013	O
)	O
developed	O
ClausIE	O
,	O
a	O
method	O
that	O
analyzes	O
the	O
clauses	O
in	O
a	O
sentence	O
and	O
derives	O
triples	O
from	O
this	O
structure	O
.	O
Gashteovski	O
et	O
al	O
(	O
2017	O
)	O
developed	O
MinIE	O
to	O
advance	O
ClausIE	O
by	O
making	O
the	O
resulting	O
triples	O
more	O
concise	O
.	O
Stanovsky	O
et	O
al	O
(	O
2018	O
)	O
proposed	O
a	O
supervised	O
learner	O
for	O
Open	O
IE	O
by	O
casting	O
relation	B-TaskName
extraction	I-TaskName
into	O
sequence	O
tagging	O
.	O
A	O
bi	O
-	O
LSTM	B-MethodName
model	O
is	O
trained	O
to	O
predict	O
the	O
label	O
(	O
entity	O
,	O
predicate	O
,	O
or	O
other	O
)	O
of	O
each	O
token	O
of	O
the	O
input	O
.	O
The	O
work	O
most	O
related	O
to	O
ours	O
is	O
Neural	O
Open	O
IE	O
(	O
Cui	O
et	O
al	O
,	O
2018	O
)	O
,	O
which	O
proposed	O
an	O
encoder	O
-	O
decoder	O
with	O
attention	O
model	O
to	O
extract	O
triples	O
.	O
However	O
,	O
this	O
work	O
is	O
not	O
geared	O
for	O
extracting	O
relations	O
of	O
canonicalized	O
entities	O
.	O
Another	O
line	O
of	O
studies	O
use	O
neural	O
learning	O
for	O
semantic	B-TaskName
role	I-TaskName
labeling	I-TaskName
(	O
He	O
et	O
al	O
,	O
2018	O
)	O
,	O
but	O
the	O
goal	O
here	O
is	O
to	O
recognize	O
the	O
predicate	O
-	O
argument	O
structure	O
of	O
a	O
single	O
input	O
sentence	O
-	O
as	O
opposed	O
to	O
extracting	O
relations	O
from	O
a	O
corpus	O
.	O
All	O
of	O
these	O
methods	O
generate	O
triples	O
where	O
the	O
head	O
and	O
tail	O
entities	O
and	O
the	O
predicate	O
stay	O
in	O
their	O
surface	O
forms	O
.	O
Therefore	O
,	O
different	O
names	O
and	O
phrases	O
for	O
the	O
same	O
entities	O
result	O
in	O
multiple	O
triples	O
,	O
which	O
would	O
pollute	O
the	O
KG	O
if	O
added	O
this	O
way	O
.	O
The	O
only	O
means	O
to	O
map	O
triples	O
to	O
uniquely	O
identified	O
entities	O
in	O
a	O
KG	O
is	O
by	O
post	O
-	O
processing	O
via	O
entity	B-TaskName
linking	I-TaskName
(	O
NED	O
)	O
methods	O
(	O
Shen	O
et	O
al	O
,	O
2015	O
)	O
or	O
by	O
clustering	O
with	O
subsequent	O
mapping	O
(	O
Galárraga	O
et	O
al	O
,	O
2014	O
)	O
.	O

Our	O
relation	B-TaskName
extraction	I-TaskName
model	O
is	O
based	O
on	O
the	O
encoder	O
-	O
decoder	O
framework	O
which	O
has	O
been	O
widely	O
used	O
in	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName
to	O
translate	O
text	O
from	O
one	O
language	O
to	O
another	O
.	O
In	O
our	O
setup	O
,	O
we	O
aim	O
to	O
translate	O
a	O
sentence	O
into	O
triples	O
,	O
and	O
hence	O
the	O
vocabulary	O
of	O
the	O
source	O
input	O
is	O
a	O
set	O
of	O
English	O
words	O
while	O
the	O
vocabulary	O
of	O
the	O
target	O
output	O
is	O
a	O
set	O
of	O
entity	O
and	O
predicate	O
IDs	O
in	O
an	O
existing	O
KG	O
.	O
To	O
compute	O
the	O
embeddings	O
of	O
the	O
source	O
and	O
target	O
vocabularies	O
,	O
we	O
propose	O
a	O
joint	O
learning	O
of	O
word	O
and	O
entity	B-TaskName
embeddings	I-TaskName
that	O
is	O
effective	O
to	O
capture	O
the	O
similarity	O
between	O
words	O
and	O
entities	O
for	O
named	O
entity	B-TaskName
disambiguation	I-TaskName
(	O
Yamada	O
et	O
al	O
,	O
2016	O
)	O
.	O
Note	O
that	O
our	O
method	O
differs	O
from	O
that	O
of	O
Yamada	O
et	O
al	O
(	O
2016	O
)	O
.	O
We	O
use	O
joint	O
learning	O
by	O
combining	O
skip	O
-	O
gram	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
to	O
compute	O
the	O
word	B-TaskName
embeddings	I-TaskName
and	O
TransE	B-MethodName
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
to	O
compute	O
the	O
entity	B-TaskName
embeddings	I-TaskName
(	O
including	O
the	O
relationship	O
embeddings	O
)	O
,	O
while	O
Yamada	O
et	O
al	O
(	O
2016	O
)	O
use	O
Wikipedia	O
Link	O
-	O
based	O
Measure	O
(	O
WLM	O
)	O
(	O
Milne	O
and	O
Witten	O
,	O
2008	O
)	O
that	O
does	O
not	O
consider	O
the	O
relationship	O
embeddings	O
.	O
Our	O
model	O
learns	O
the	O
entity	B-TaskName
embeddings	I-TaskName
by	O
minimizing	O
a	O
margin	O
-	O
based	O
objective	O
function	O
J	O
E	O
:	O
JE	O
=	O
tr	O
Tr	O
t	O
r	O
T	O
r	O
max	O
0	B-DatasetName
,	O
γ	B-HyperparameterName
+	O
f	O
(	O
tr	O
)	O
−	O
f	O
(	O
t	O
r	O
)	O
(	O
1	O
)	O
Tr	O
=	O
{	O
h	O
,	O
r	O
,	O
t	O
|	O
h	O
,	O
r	O
,	O
t	O
G	O
}	O
(	O
2	O
)	O
Tr	O
=	O
h	O
,	O
r	O
,	O
t	O
|	O
h	O
E	O
∪	O
h	O
,	O
r	O
,	O
t	O
|	O
t	O
E	O
(	O
3	O
)	O
f	O
(	O
tr	O
)	O
=	O
h	O
+	O
r	O
−	O
t	O
(	O
4	O
)	O
Here	O
,	O
x	O
is	O
the	O
L1	O
-	O
Norm	O
of	O
vector	O
x	O
,	O
γ	B-HyperparameterName
is	O
a	O
margin	O
hyperparameter	O
,	O
T	O
r	O
is	O
the	O
set	O
of	O
valid	O
relationship	O
triples	O
from	O
a	O
KG	O
G	O
,	O
and	O
T	O
r	O
is	O
the	O
set	O
of	O
corrupted	O
relationship	O
triples	O
(	O
recall	O
that	O
E	O
is	O
the	O
set	O
of	O
entities	O
in	O
G	O
)	O
.	O
The	O
corrupted	O
triples	O
are	O
used	O
as	O
negative	O
samples	O
,	O
which	O
are	O
created	O
by	O
replacing	O
the	O
head	O
or	O
tail	O
entity	O
of	O
a	O
valid	O
triple	O
in	O
T	O
r	O
with	O
a	O
random	O
entity	O
.	O
We	O
use	O
all	O
triples	O
in	O
Wikidata	O
except	O
those	O
which	O
belong	O
to	O
the	O
testing	O
data	O
to	O
compute	O
the	O
entity	B-TaskName
embeddings	I-TaskName
.	O
To	O
establish	O
the	O
interaction	O
between	O
the	O
entity	O
and	O
word	B-TaskName
embeddings	I-TaskName
,	O
we	O
follow	O
the	O
Anchor	O
Context	O
Model	O
proposed	O
by	O
Yamada	O
et	O
al	O
(	O
2016	O
)	O
.	O
First	O
,	O
we	O
generate	O
a	O
text	O
corpus	O
by	O
combining	O
the	O
original	O
text	O
and	O
the	O
modified	O
anchor	O
text	O
of	O
Wikipedia	O
.	O
This	O
is	O
done	O
by	O
replacing	O
the	O
entity	O
names	O
in	O
a	O
sentence	O
with	O
the	O
related	O
entity	O
or	O
predicate	O
IDs	O
.	O
For	O
example	O
,	O
the	O
sentence	O
"	O
New	O
York	O
University	O
is	O
a	O
private	O
university	O
in	O
Manhattan	O
"	O
is	O
modified	O
into	O
"	O
Q49210	O
is	O
a	O
Q902104	O
in	O
Q11299	O
"	O
.	O
Then	O
,	O
we	O
use	O
the	O
skip	O
-	O
gram	O
method	O
to	O
compute	O
the	O
word	B-TaskName
embeddings	I-TaskName
from	O
the	O
generated	O
corpus	O
(	O
the	O
entity	O
IDs	O
in	O
the	O
modified	O
anchor	O
text	O
are	O
treated	O
as	O
words	O
in	O
the	O
skip	O
-	O
gram	O
model	O
)	O
.	O
Given	O
a	O
sequence	O
of	O
n	O
words	O
[	O
w	O
1	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
n	O
]	O
,	O
The	O
model	O
learns	O
the	O
word	B-TaskName
embeddings	I-TaskName
,	O
by	O
minimizing	O
the	O
following	O
objective	O
function	O
J	O
W	O
:	O
J	O
W	O
=	O
1	O
T	O
n	O
t=1	O
−c≤j≤c	O
,	O
j	O
=	O
0	B-DatasetName
log	O
P	O
(	O
w	O
t+j	O
|	O
w	O
t	O
)	O
(	O
5	O
)	O
P	O
(	O
w	O
t+j	O
|	O
w	O
t	O
)	O
=	O
exp	O
(	O
v	O
w	O
t+j	O
v	O
wt	O
)	O
W	O
i=1	O
(	O
v	O
i	O
v	O
wt	O
)	O
(	O
6	O
)	O
where	O
c	O
is	O
the	O
size	O
of	O
the	O
context	O
window	O
,	O
w	O
t	O
denotes	O
the	O
target	O
word	O
,	O
and	O
w	O
t+j	O
is	O
the	O
context	O
word	O
;	O
v	O
w	O
and	O
v	O
w	O
are	O
the	O
input	O
and	O
output	O
vector	O
representations	O
of	O
word	O
w	O
,	O
and	O
W	O
is	O
the	O
vocabulary	O
size	O
.	O
The	O
overall	O
objective	O
function	O
of	O
the	O
joint	O
learning	O
of	O
word	O
and	O
entity	B-TaskName
embeddings	I-TaskName
is	O
:	O
J	O
=	O
J	O
E	O
+	O
J	O
W	O
(	O
7	O
)	O

Our	O
proposed	O
relation	B-TaskName
extraction	I-TaskName
model	O
integrates	O
the	O
extraction	O
and	O
canonicalization	O
tasks	O
for	O
KB	O
enrichment	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
.	O
To	O
build	O
such	O
a	O
model	O
,	O
we	O
employ	O
an	O
encoder	O
-	O
decoder	O
model	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
)	O
to	O
translate	O
a	O
sentence	O
into	O
a	O
sequence	O
of	O
triples	O
.	O
The	O
encoder	O
encodes	O
a	O
sentence	O
into	O
a	O
vector	O
that	O
is	O
used	O
by	O
the	O
decoder	O
as	O
a	O
context	O
to	O
generate	O
a	O
sequence	O
of	O
triples	O
.	O
Because	O
we	O
treat	O
the	O
input	O
and	O
output	O
as	O
a	O
sequence	O
,	O
We	O
use	O
the	O
LSTM	B-MethodName
networks	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
in	O
the	O
encoder	O
and	O
the	O
decoder	O
.	O
The	O
encoder	O
-	O
decoder	O
with	O
attention	O
model	O
(	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
has	O
been	O
used	O
in	O
machine	B-TaskName
translation	I-TaskName
.	O
However	O
,	O
in	O
the	O
relation	B-TaskName
extraction	I-TaskName
task	O
,	O
the	O
attention	O
model	O
can	O
not	O
capture	O
the	O
multiword	O
entity	O
names	O
.	O
In	O
our	O
preliminary	O
investigation	O
,	O
we	O
found	O
that	O
the	O
attention	O
model	O
yields	O
misalignment	O
between	O
the	O
word	O
and	O
the	O
entity	O
.	O
The	O
above	O
problem	O
is	O
due	O
to	O
the	O
same	O
words	O
in	O
the	O
names	O
of	O
different	O
entities	O
(	O
e.g.	O
,	O
the	O
word	O
University	O
in	O
different	O
university	O
names	O
such	O
as	O
New	O
York	O
University	O
,	O
Washington	O
University	O
,	O
etc	O
.	O
)	O
.	O
During	O
training	O
,	O
the	O
model	O
pays	O
more	O
attention	O
to	O
the	O
word	O
University	O
to	O
differentiate	O
different	O
types	O
of	O
entities	O
of	O
a	O
similar	O
name	O
,	O
e.g.	O
,	O
New	O
York	O
University	O
,	O
New	O
York	O
Times	O
Building	O
,	O
or	O
New	O
York	O
Life	O
Building	O
,	O
but	O
not	O
the	O
same	O
types	O
of	O
entities	O
of	O
different	O
names	O
(	O
e.g.	O
,	O
New	O
York	O
University	O
and	O
Washington	O
University	O
)	O
.	O
This	O
may	O
cause	O
errors	O
in	O
entity	B-TaskName
alignment	I-TaskName
,	O
especially	O
when	O
predicting	O
the	O
ID	O
of	O
an	O
entity	O
that	O
is	O
not	O
in	O
the	O
training	O
data	O
.	O
Even	O
though	O
we	O
add	O
Entity	O
-	O
name	O
,	O
Entity	O
-	O
ID	O
pairs	O
as	O
training	O
data	O
(	O
see	O
the	O
Training	O
section	O
)	O
,	O
the	O
misalignments	O
still	O
take	O
place	O
.	O
We	O
address	O
the	O
above	O
problem	O
by	O
proposing	O
an	O
n	O
-	O
gram	O
based	O
attention	O
model	O
.	O
This	O
model	O
computes	O
the	O
attention	O
of	O
all	O
possible	O
n	O
-	O
grams	O
of	O
the	O
sentence	O
input	O
.	O
The	O
attention	O
weights	O
are	O
computed	O
over	O
the	O
n	O
-	O
gram	O
combinations	O
of	O
the	O
word	B-TaskName
embeddings	I-TaskName
,	O
and	O
hence	O
the	O
context	O
vector	O
for	O
the	O
decoder	O
is	O
computed	O
as	O
follows	O
.	O
c	O
d	O
t	O
=	O
	O
	O
h	O
e	O
;	O
|	O
N	O
|	O
n=1	O
W	O
n	O
	O
	O
|	O
X	O
n	O
|	O
i=1	O
α	B-HyperparameterName
n	O
i	O
x	O
n	O
i	O
	O
	O
	O
	O
(	O
8	O
)	O
α	B-HyperparameterName
n	O
i	O
=	O
exp	O
(	O
h	O
e	O
V	O
n	O
x	O
n	O
i	O
)	O
|	O
X	O
n	O
|	O
j=1	O
exp	O
(	O
h	O
e	O
V	O
n	O
x	O
n	O
j	O
)	O
(	O
9	O
)	O
Here	O
,	O
c	O
d	O
t	O
is	O
the	O
context	O
vector	O
of	O
the	O
decoder	O
at	O
timestep	O
t	O
,	O
h	O
e	O
is	O
the	O
last	O
hidden	O
state	O
of	O
the	O
encoder	O
,	O
the	O
superscript	O
n	O
indicates	O
the	O
n	O
-	O
gram	O
combination	O
,	O
x	O
is	O
the	O
word	B-TaskName
embeddings	I-TaskName
of	O
input	O
sentence	O
,	O
|	O
X	O
n	O
|	O
is	O
the	O
total	O
number	O
of	O
n	O
-	O
gram	O
token	O
combination	O
,	O
N	O
indicates	O
the	O
maximum	O
value	O
of	O
n	O
used	O
in	O
the	O
n	O
-	O
gram	O
combinations	O
(	O
N	O
=	O
3	O
in	O
our	O
experiments	O
)	O
,	O
W	O
and	O
V	O
are	O
learned	O
parameter	O
matrices	O
,	O
and	O
α	B-HyperparameterName
is	O
the	O
attention	O
weight	O
.	O

The	O
output	O
of	O
the	O
encoder	O
-	O
decoder	O
model	O
is	O
a	O
sequence	O
of	O
the	O
entity	O
and	O
predicate	O
IDs	O
where	O
every	O
three	O
tokens	O
indicate	O
a	O
triple	O
.	O
Therefore	O
,	O
to	O
extract	O
a	O
triple	O
,	O
we	O
simply	O
group	O
every	O
three	O
tokens	O
of	O
the	O
generated	O
output	O
.	O
However	O
,	O
the	O
greedy	O
approach	O
(	O
i.e.	O
,	O
picking	O
the	O
entity	O
with	O
the	O
highest	O
probability	O
of	O
the	O
last	O
softmax	B-MethodName
layer	O
of	O
the	O
decoder	O
)	O
may	O
lead	O
the	O
model	O
to	O
extract	O
incorrect	O
entities	O
due	O
to	O
the	O
similarity	O
between	O
entity	B-TaskName
embeddings	I-TaskName
(	O
e.g.	O
,	O
the	O
embeddings	O
of	O
New	O
York	O
City	O
and	O
Chicago	O
may	O
be	O
similar	O
because	O
both	O
are	O
cities	O
in	O
USA	O
)	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
propose	O
two	O
strategies	O
:	O
re	O
-	O
ranking	O
the	O
predicted	O
entities	O
using	O
a	O
modified	O
beam	O
search	O
and	O
filtering	O
invalid	O
triples	O
using	O
a	O
triple	O
classifier	O
.	O
The	O
modified	O
beam	O
search	O
re	O
-	O
ranks	O
top	O
-	O
k	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
10	O
in	O
our	O
experiments	O
)	O
entity	O
IDs	O
that	O
are	O
predicted	O
by	O
the	O
decoder	O
by	O
computing	O
the	O
edit	O
distance	O
between	O
the	O
entity	O
names	O
(	O
obtained	O
from	O
the	O
KB	O
)	O
and	O
every	O
n	O
-	O
gram	O
token	O
of	O
the	O
input	O
sentence	O
.	O
The	O
intuition	O
is	O
that	O
the	O
entity	O
name	O
should	O
be	O
mentioned	O
in	O
the	O
sentence	O
so	O
that	O
the	O
entity	O
with	O
the	O
highest	O
similarity	O
will	O
be	O
chosen	O
as	O
the	O
output	O
.	O
Our	O
triple	O
classifier	O
is	O
trained	O
with	O
entity	B-TaskName
embeddings	I-TaskName
from	O
the	O
joint	O
learning	O
(	O
see	O
Section	O
3.3	O
)	O
.	O
Triple	B-TaskName
classification	I-TaskName
is	O
one	O
of	O
the	O
metrics	O
to	O
evaluate	O
the	O
quality	O
of	O
entity	B-TaskName
embeddings	I-TaskName
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
.	O
We	O
build	O
a	O
classifier	O
to	O
determine	O
the	O
validity	O
of	O
a	O
triple	O
h	O
,	O
r	O
,	O
t	O
.	O
We	O
train	O
a	O
binary	O
classifier	O
based	O
on	O
the	O
plausibility	O
score	O
(	O
h	O
+	O
r	O
−	O
t	O
)	O
(	O
the	O
score	O
to	O
compute	O
the	O
entity	B-TaskName
embeddings	I-TaskName
)	O
.	O
We	O
create	O
negative	O
samples	O
by	O
corrupting	O
the	O
valid	O
triples	O
(	O
i.e.	O
,	O
replacing	O
the	O
head	O
or	O
tail	O
entity	O
by	O
a	O
random	O
entity	O
)	O
.	O
The	O
triple	O
classifier	O
is	O
effective	O
to	O
filter	O
invalid	O
triple	O
such	O
as	O
New	O
York	O
University	O
,	O
capital	O
of	O
,	O
Manhattan	O
.	O

We	O
evaluate	O
our	O
model	O
on	O
two	O
real	O
datasets	O
including	O
WIKI	O
and	O
GEO	O
test	O
datasets	O
(	O
see	O
Section	O
3.2	O
)	O
.	O
We	O
use	O
precision	O
,	O
recall	O
,	O
and	O
F1	B-MetricName
score	I-MetricName
as	O
the	O
evaluation	O
metrics	O
.	O

We	O
use	O
grid	O
search	O
to	O
find	O
the	O
best	O
hyperparameters	O
for	O
the	O
networks	O
.	O
We	O
use	O
512	O
hidden	O
units	O
for	O
both	O
the	O
encoder	O
and	O
the	O
decoder	O
.	O
We	O
use	O
64	O
dimensions	O
of	O
pre	O
-	O
trained	O
word	O
and	O
entity	B-TaskName
embeddings	I-TaskName
(	O
see	O
Section	O
3.3	O
)	O
.	O
We	O
use	O
a	O
0.5	O
dropout	O
rate	O
for	O
regularization	O
on	O
both	O
the	O
encoder	O
and	O
the	O
decoder	O
.	O
We	O
use	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.0002	O
.	O

Table	O
3	O
shows	O
that	O
the	O
end	O
-	O
to	O
-	O
end	O
models	O
outperform	O
the	O
existing	O
model	O
.	O
In	O
particular	O
,	O
our	O
proposed	O
n	O
-	O
gram	O
attention	O
model	O
achieves	O
the	O
best	O
results	O
in	O
terms	O
of	O
precision	O
,	O
recall	O
,	O
and	O
F1	B-MetricName
score	I-MetricName
.	O
Our	O
proposed	O
model	O
outperforms	O
the	O
best	O
existing	O
model	O
(	O
MinIE	O
)	O
by	O
33.39	O
%	O
and	O
34.78	O
%	O
in	O
terms	O
of	O
F1	B-MetricName
score	I-MetricName
on	O
the	O
WIKI	O
and	O
GEO	O
test	O
dataset	O
respectively	O
.	O
These	O
results	O
are	O
expected	O
since	O
the	O
existing	O
models	O
are	O
affected	O
by	O
the	O
error	O
propagation	O
of	O
the	O
NED	O
.	O
As	O
expected	O
,	O
the	O
combination	O
of	O
the	O
existing	O
models	O
with	O
AIDA	O
achieves	O
higher	O
F1	B-MetricName
scores	O
than	O
the	O
combination	O
with	O
NeuralEL	O
as	O
AIDA	O
achieves	O
a	O
higher	O
precision	O
than	O
NeuralEL	O
.	O
To	O
further	O
show	O
the	O
effect	O
of	O
error	O
propagation	O
,	O
we	O
set	O
up	O
an	O
experiment	O
without	O
the	O
canonicalization	O
task	O
(	O
i.e.	O
,	O
the	O
objective	O
is	O
predicting	O
a	O
relationship	O
between	O
known	O
entities	O
)	O
.	O
We	O
remove	O
the	O
NED	O
pre	O
-	O
processing	O
step	O
by	O
allowing	O
the	O
CNN	O
model	O
to	O
access	O
the	O
correct	O
entities	O
.	O
Meanwhile	O
,	O
we	O
provide	O
the	O
correct	O
entities	O
to	O
the	O
decoder	O
of	O
our	O
proposed	O
model	O
.	O
In	O
this	O
setup	O
,	O
our	O
proposed	O
model	O
achieves	O
86.34	O
%	O
and	O
79.11	O
%	O
,	O
while	O
CNN	O
achieves	O
81.92	O
%	O
and	O
75.82	O
%	O
in	O
precision	O
over	O
the	O
WIKI	O
and	O
GEO	O
test	O
datasets	O
,	O
respectively	O
.	O
Our	O
proposed	O
n	O
-	O
gram	O
attention	O
model	O
outperforms	O
the	O
end	O
-	O
to	O
-	O
end	O
models	O
by	O
15.51	O
%	O
and	O
8.38	O
%	O
in	O
terms	O
of	O
F1	B-MetricName
score	I-MetricName
on	O
the	O
WIKI	O
and	O
GEO	O
test	O
datasets	O
,	O
respectively	O
.	O
The	O
Transformer	B-MethodName
model	O
also	O
only	O
yields	O
similar	O
performance	O
to	O
that	O
of	O
the	O
Single	O
Attention	O
model	O
,	O
which	O
is	O
worse	O
than	O
ours	O
.	O
These	O
results	O
indicate	O
that	O
our	O
model	O
captures	O
multi	O
-	O
word	O
entity	O
name	O
(	O
in	O
both	O
datasets	O
,	O
82.9	O
%	O
of	O
the	O
entities	O
have	O
multi	O
-	O
word	O
entity	O
name	O
)	O
in	O
the	O
input	O
sentence	O
better	O
than	O
the	O
other	O
models	O
.	O
Table	O
3	O
also	O
shows	O
that	O
the	O
pre	O
-	O
trained	O
embeddings	O
improve	O
the	O
performance	O
of	O
the	O
model	O
in	O
all	O
measures	O
.	O
Moreover	O
,	O
the	O
pre	O
-	O
trained	O
embeddings	O
help	O
the	O
model	O
to	O
converge	O
faster	O
.	O
In	O
our	O
experiments	O
,	O
the	O
models	O
that	O
use	O
the	O
pre	O
-	O
trained	O
embeddings	O
converge	O
in	O
20	O
epochs	O
on	O
average	O
,	O
while	O
the	O
models	O
that	O
do	O
not	O
use	O
the	O
pre	O
-	O
trained	O
embeddings	O
converge	O
in	O
30	O
−	O
40	O
epochs	O
.	O
Our	O
triple	O
classifier	O
combined	O
with	O
the	O
modified	O
beam	O
search	O
boost	O
the	O
performance	O
of	O
the	O
model	O
.	O
The	O
modified	O
beam	O
search	O
provides	O
a	O
high	O
recall	O
by	O
extracting	O
the	O
correct	O
entities	O
based	O
on	O
the	O
surface	O
form	O
in	O
the	O
input	O
sentence	O
while	O
the	O
triple	O
classifier	O
provides	O
a	O
high	O
precision	O
by	O
filtering	O
the	O
invalid	O
triples	O
.	O

We	O
proposed	O
an	O
end	O
-	O
to	O
-	O
end	O
relation	B-TaskName
extraction	I-TaskName
model	O
for	O
KB	O
enrichment	O
that	O
integrates	O
the	O
extraction	O
and	O
canonicalization	O
tasks	O
.	O
Our	O
model	O
thus	O
reduces	O
the	O
error	O
propagation	O
between	O
relation	B-TaskName
extraction	I-TaskName
and	O
NED	O
that	O
existing	O
approaches	O
are	O
prone	O
to	O
.	O
To	O
obtain	O
high	O
-	O
quality	O
training	O
data	O
,	O
we	O
adapt	O
distant	O
supervision	O
and	O
augment	O
it	O
with	O
co	O
-	O
reference	O
resolution	O
and	O
paraphrase	O
detection	O
.	O
We	O
propose	O
an	O
n	O
-	O
gram	O
based	O
attention	O
model	O
that	O
better	O
captures	O
the	O
multi	O
-	O
word	O
entity	O
names	O
in	O
a	O
sentence	O
.	O
Moreover	O
,	O
we	O
propose	O
a	O
modified	O
beam	O
search	O
and	O
a	O
triple	B-TaskName
classification	I-TaskName
that	O
helps	O
the	O
model	O
to	O
generate	O
high	O
-	O
quality	O
triples	O
.	O
Experimental	O
results	O
show	O
that	O
our	O
proposed	O
model	O
outperforms	O
the	O
existing	O
models	O
by	O
33.39	O
%	O
and	O
34.78	O
%	O
in	O
terms	O
of	O
F1	B-MetricName
score	I-MetricName
on	O
the	O
WIKI	O
and	O
GEO	O
test	O
dataset	O
respectively	O
.	O
These	O
results	O
confirm	O
that	O
our	O
model	O
reduces	O
the	O
error	O
propagation	O
between	O
NED	O
and	O
relation	B-TaskName
extraction	I-TaskName
.	O
Our	O
proposed	O
n	O
-	O
gram	O
attention	O
model	O
outperforms	O
the	O
other	O
encoder	O
-	O
decoder	O
models	O
by	O
15.51	O
%	O
and	O
8.38	O
%	O
in	O
terms	O
of	O
F1	B-MetricName
score	I-MetricName
on	O
the	O
two	O
real	O
-	O
world	O
datasets	O
.	O
These	O
results	O
confirm	O
that	O
our	O
model	O
better	O
captures	O
the	O
multi	O
-	O
word	O
entity	O
names	O
in	O
a	O
sentence	O
.	O
In	O
the	O
future	O
,	O
we	O
plan	O
to	O
explore	O
contextbased	O
similarity	O
to	O
complement	O
the	O
lexical	O
similarity	O
to	O
improve	O
the	O
overall	O
performance	O
.	O

NELL	B-DatasetName
-	O
995	O
(	O
Xiong	O
et	O
al	O
,	O
2017	O
)	O
was	O
taken	O
from	O
the	O
Never	O
Ending	O
Language	O
Learner	O
(	O
NELL	B-DatasetName
)	O
sys	O
-	O
tem	O
(	O
Mitchell	O
et	O
al	O
,	O
2018	O
)	O
,	O
which	O
continuously	O
reads	O
the	O
web	O
to	O
obtain	O
and	O
update	O
its	O
knowledge	O
.	O
NELL	B-DatasetName
-	O
995	O
,	O
a	O
subset	O
of	O
the	O
995th	O
iteration	O
of	O
NELL	B-DatasetName
,	O
contains	O
75	O
,	O
492	O
entities	O
,	O
200	O
relations	O
,	O
and	O
154	O
,	O
213	O
triples	O
.	O
While	O
NELL	B-DatasetName
-	O
995	O
is	O
general	O
and	O
covers	O
many	O
domains	O
,	O
its	O
mean	O
average	B-MetricName
precision	I-MetricName
was	O
less	O
than	O
50	O
%	O
around	O
its	O
1000th	O
iteration	O
(	O
Mitchell	O
et	O
al	O
,	O
2018	O
)	O
.	O
A	O
cursory	O
inspection	O
reveals	O
that	O
many	O
of	O
the	O
triples	O
in	O
NELL	B-DatasetName
-	O
995	O
are	O
nonsensical	O
or	O
overly	O
generic	O
,	O
suggesting	O
that	O
NELL	B-DatasetName
-	O
995	O
is	O
not	O
a	O
meaningful	O
dataset	O
for	O
KGC	O
evaluation	O
.	O
1	O
YAGO3	B-DatasetName
-	I-DatasetName
10	I-DatasetName
(	O
Dettmers	O
et	O
al	O
,	O
2018	O
)	O
is	O
a	O
subset	O
of	O
YAGO3	O
(	O
Mahdisoltani	O
et	O
al	O
,	O
2014	O
)	O
,	O
which	O
covers	O
portions	O
of	O
Wikipedia	O
,	O
Wikidata	O
,	O
and	O
Word	O
-	O
Net	O
.	O
YAGO3	B-DatasetName
-	I-DatasetName
10	I-DatasetName
has	O
123	O
,	O
182	O
entities	O
,	O
37	O
relations	O
,	O
and	O
1	O
,	O
089	O
,	O
040	O
triples	O
mostly	O
limited	O
to	O
facts	O
about	O
people	O
and	O
locations	O
.	O
While	O
YAGO3	B-DatasetName
-	I-DatasetName
10	I-DatasetName
is	O
a	O
highprecision	O
dataset	O
,	O
it	O
was	O
recently	O
shown	O
to	O
be	O
too	O
easy	O
for	O
link	B-TaskName
prediction	I-TaskName
because	O
it	O
contains	O
a	O
large	O
proportion	O
of	O
duplicate	O
relations	O
(	O
Akrami	O
et	O
al	O
,	O
2020	O
;	O
Pezeshkpour	O
et	O
al	O
,	O
2020	O
)	O
.	O

To	O
create	O
smaller	O
data	O
snapshots	O
,	O
we	O
filtered	O
the	O
initial	O
1.15	O
million	O
triples	O
to	O
k	O
-	O
cores	O
,	O
which	O
are	O
maximal	O
subgraphs	O
G	O
of	O
a	O
given	O
graph	O
G	O
such	O
that	O
every	O
node	O
in	O
G	O
has	O
a	O
degree	O
of	O
at	O
least	O
k	O
(	O
Batagelj	O
and	O
Zaveršnik	O
,	O
2011	O
)	O
.	O
2	O
We	O
constructed	O
three	O
CODEX	O
datasets	O
(	O
Table	O
2	O
)	O
:	O
CODEX	O
-	O
S	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
15	O
)	O
,	O
which	O
has	O
36k	O
triples	O
.	O
Because	O
of	O
its	O
smaller	O
size	O
,	O
we	O
recommend	O
that	O
CODEX	O
-	O
S	O
be	O
used	O
for	O
model	O
testing	O
and	O
debugging	O
,	O
as	O
well	O
as	O
evaluation	O
of	O
methods	O
that	O
are	O
less	O
computationally	O
efficient	O
(	O
e.g.	O
,	O
symbolic	O
search	O
-	O
based	O
approaches	O
)	O
.	O
CODEX	O
-	O
M	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
10	O
)	O
,	O
which	O
has	O
206k	O
triples	O
.	O
CODEX	O
-	O
M	O
is	O
all	O
-	O
purpose	O
,	O
being	O
comparable	O
in	O
size	O
to	O
FB15	O
K	O
-	O
237	O
(	O
2.1	O
)	O
,	O
one	O
of	O
the	O
most	O
popular	O
benchmarks	O
for	O
KGC	O
evaluation	O
.	O
CODEX	O
-	O
L	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
5	O
)	O
,	O
which	O
has	O
612k	O
triples	O
.	O
CODEX	O
-	O
L	O
is	O
comparable	O
in	O
size	O
to	O
FB15	O
K	O
(	O
2.1	O
)	O
,	O
and	O
can	O
be	O
used	O
for	O
both	O
general	O
evaluation	O
and	O
"	O
few	O
-	O
shot	O
"	O
evaluation	O
.	O
We	O
also	O
release	O
the	O
raw	O
dump	O
that	O
we	O
collected	O
via	O
snowball	O
sampling	O
,	O
but	O
focus	O
on	O
CODEX	O
-	O
S	O
through	O
L	O
for	O
the	O
remainder	O
of	O
this	O
paper	O
.	O
To	O
minimize	O
train	O
/	O
test	O
leakage	O
,	O
we	O
removed	O
inverse	O
relations	O
from	O
each	O
dataset	O
.	O
We	O
computed	O
(	O
head	O
,	O
tail	O
)	O
and	O
(	O
tail	O
,	O
head	O
)	O
overlap	O
between	O
all	O
pairs	O
of	O
relations	O
,	O
and	O
removed	O
each	O
relation	O
whose	O
entity	O
pair	O
set	O
overlapped	O
with	O
that	O
of	O
another	O
relation	O
more	O
than	O
50	O
%	O
of	O
the	O
time	O
.	O
Finally	O
,	O
we	O
split	O
each	O
dataset	O
into	O
90/5/5	O
train	O
/	O
validation	O
/	O
test	O
triples	O
such	O
that	O
the	O
validation	O
and	O
test	O
sets	O
contained	O
only	O
entities	O
and	O
relations	O
seen	O
in	O
the	O
respective	O
training	O
sets	O
.	O

Link	B-TaskName
prediction	I-TaskName
The	O
link	B-TaskName
prediction	I-TaskName
task	O
is	O
conducted	O
as	O
follows	O
:	O
Given	O
a	O
test	O
triple	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
,	O
we	O
construct	O
queries	O
(	O
?	O
,	O
r	O
,	O
t	O
)	O
and	O
(	O
h	O
,	O
r	O
,	O
?	O
)	O
.	O
For	O
each	O
query	O
,	O
a	O
model	O
scores	O
candidate	O
head	O
(	O
tail	O
)	O
entitieŝ	O
h	O
(	O
t	O
)	O
according	O
to	O
its	O
belief	O
thatĥ	O
(	O
t	O
)	O
completes	O
the	O
triple	O
(	O
i.e.	O
,	O
answers	O
the	O
query	O
)	O
.	O
The	O
goal	O
is	O
of	O
link	B-TaskName
prediction	I-TaskName
is	O
to	O
rank	O
true	O
triples	O
(	O
ĥ	O
,	O
r	O
,	O
t	O
)	O
or	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
higher	O
than	O
false	O
and	O
unseen	O
triples	O
.	O
Link	B-TaskName
prediction	I-TaskName
performance	O
is	O
evaluated	O
with	O
mean	O
reciprocal	O
rank	O
(	O
MRR	B-MetricName
)	O
and	O
hits@k	O
.	O
MRR	B-MetricName
is	O
the	O
average	O
reciprocal	O
of	O
each	O
ground	O
-	O
truth	O
entity	O
's	O
rank	O
over	O
all	O
(	O
?	O
,	O
r	O
,	O
t	O
)	O
and	O
(	O
h	O
,	O
r	O
,	O
?	O
)	O
test	O
triples	O
.	O
Hits@k	O
measures	O
the	O
proportion	O
of	O
test	O
triples	O
for	O
which	O
the	O
ground	O
-	O
truth	O
entity	O
is	O
ranked	O
in	O
the	O
top	O
-	O
k	O
predicted	O
entities	O
.	O
In	O
computing	O
these	O
metrics	O
,	O
we	O
exclude	O
the	O
predicted	O
entities	O
for	O
which	O
(	O
ĥ	O
,	O
r	O
,	O
t	O
)	O
G	O
or	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
G	O
so	O
that	O
known	O
positive	O
triples	O
do	O
not	O
artificially	O
lower	O
ranking	O
scores	O
.	O
This	O
is	O
called	O
"	O
filtering	O
"	O
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
.	O
Triple	B-TaskName
classification	I-TaskName
Given	O
a	O
triple	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
,	O
the	O
goal	O
of	O
triple	B-TaskName
classification	I-TaskName
is	O
to	O
predict	O
a	O
corresponding	O
label	O
y	O
{	O
−1	O
,	O
1	O
}	O
.	O
Since	O
knowledge	B-TaskName
graph	I-TaskName
embedding	I-TaskName
models	O
output	O
real	O
-	O
valued	O
scores	O
for	O
triples	O
,	O
we	O
convert	O
these	O
scores	O
into	O
labels	O
by	O
selecting	O
a	O
decision	O
threshold	O
per	O
relation	O
on	O
the	O
validation	O
set	O
such	O
that	O
validation	O
accuracy	B-MetricName
is	O
maximized	O
for	O
the	O
model	O
in	O
question	O
.	O
A	O
similar	O
approach	O
was	O
used	O
by	O
Socher	O
et	O
al	O
(	O
2013	O
)	O
.	O
We	O
compare	O
results	O
on	O
three	O
sets	O
of	O
evaluation	O
negatives	O
:	O
(	O
1	O
)	O
We	O
generate	O
one	O
negative	O
per	O
positive	O
by	O
replacing	O
the	O
positive	O
triple	O
's	O
tail	O
entity	O
by	O
a	O
tail	O
entity	O
t	O
sampled	O
uniformly	O
at	O
random	O
;	O
(	O
2	O
)	O
We	O
generate	O
negatives	O
by	O
sampling	O
tail	O
entities	O
according	O
to	O
their	O
relative	O
frequency	O
in	O
the	O
tail	O
slot	O
of	O
all	O
triples	O
;	O
and	O
(	O
3	O
)	O
We	O
use	O
the	O
CODEX	O
hard	O
negatives	O
.	O
We	O
measure	O
accuracy	B-MetricName
and	O
F1	B-MetricName
score	I-MetricName
.	O

As	O
recent	O
studies	O
have	O
observed	O
that	O
training	O
strategies	O
are	O
equally	O
,	O
if	O
not	O
more	O
,	O
important	O
than	O
architecture	O
for	O
link	B-TaskName
prediction	I-TaskName
(	O
Kadlec	O
et	O
al	O
,	O
2017	O
;	O
Lacroix	O
et	O
al	O
,	O
2018	O
;	O
Ruffinelli	O
et	O
al	O
,	O
2020	O
)	O
,	O
we	O
search	O
across	O
a	O
large	O
range	O
of	O
hyperparameters	O
to	O
ensure	O
a	O
truly	O
fair	O
comparison	O
.	O
To	O
this	O
end	O
we	O
use	O
the	O
PyTorch	O
-	O
based	O
LibKGE	O
framework	O
for	O
training	O
and	O
selecting	O
knowledge	B-TaskName
graph	I-TaskName
embeddings	I-TaskName
.	O
4	O
In	O
the	O
remainder	O
of	O
this	O
section	O
we	O
outline	O
the	O
most	O
important	O
parameters	O
of	O
our	O
model	B-TaskName
selection	I-TaskName
process	O
.	O
Training	O
negatives	O
Given	O
a	O
set	O
of	O
positive	O
training	O
triples	O
{	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
}	O
,	O
we	O
compare	O
three	O
types	O
of	O
negative	O
sampling	O
strategy	O
implemented	O
by	O
LibKGE	O
:	O
(	O
a	O
)	O
NegSamp	O
,	O
or	O
randomly	O
corrupting	O
head	O
entities	O
h	O
or	O
tail	O
entities	O
t	O
to	O
create	O
negatives	O
;	O
(	O
b	O
)	O
1vsAll	O
,	O
or	O
treating	O
all	O
possible	O
head	O
/	O
tail	O
corruptions	O
of	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
as	O
negatives	O
,	O
including	O
the	O
corruptions	O
that	O
are	O
actually	O
positives	O
;	O
and	O
(	O
c	O
)	O
KvsAll	O
,	O
or	O
treating	O
batches	O
of	O
head	O
/	O
tail	O
corruptions	O
not	O
seen	O
in	O
the	O
knowledge	O
graph	O
as	O
negatives	O
.	O
Loss	O
functions	O
We	O
consider	O
the	O
following	O
loss	B-MetricName
functions	O
:	O
(	O
i	O
)	O
MR	B-DatasetName
or	O
margin	O
ranking	O
,	O
which	O
aims	O
to	O
maximize	O
a	O
margin	O
between	O
positive	O
and	O
negative	O
triples	O
;	O
(	O
ii	O
)	O
BCE	O
or	O
binary	O
cross	O
-	O
entropy	O
,	O
which	O
is	O
computed	O
by	O
applying	O
the	O
logistic	O
sigmoid	O
to	O
triple	O
scores	O
;	O
and	O
(	O
iii	O
)	O
CE	O
or	O
cross	O
-	O
entropy	O
between	O
the	O
softmax	B-MethodName
over	O
the	O
entire	O
distribution	O
of	O
triple	O
scores	O
and	O
the	O
label	O
distribution	O
over	O
all	O
triples	O
,	O
normalized	O
to	O
sum	O
to	O
one	O
.	O
Search	O
strategies	O
We	O
select	O
models	O
using	O
the	O
Ax	O
platform	O
,	O
which	O
supports	O
hyperparameter	O
search	O
using	O
both	O
quasi	O
-	O
random	O
sequences	O
of	O
generated	O
configurations	O
and	O
Bayesian	O
optimization	O
(	O
BO	O
)	O
with	O
Gaussian	B-TaskName
processes	I-TaskName
.	O
5	O
At	O
a	O
high	O
level	O
,	O
for	O
each	O
dataset	O
and	O
model	O
,	O
we	O
generate	O
both	O
quasirandom	O
and	O
BO	O
trials	O
per	O
negative	O
sampling	O
and	O
loss	B-MetricName
function	O
combination	O
,	O
ensuring	O
that	O
we	O
search	O
over	O
a	O
wide	O
range	O
of	O
hyperparameters	O
for	O
different	O
types	O
of	O
training	O
strategy	O
.	O
Appendix	O
F	O
provides	O
specific	O
details	O
on	O
the	O
search	O
strategy	O
for	O
each	O
dataset	O
,	O
which	O
was	O
determined	O
according	O
to	O
resource	O
constraints	O
and	O
observed	O
performance	O
patterns	O
.	O

Table	O
5	O
gives	O
link	B-TaskName
prediction	I-TaskName
results	O
.	O
We	O
find	O
that	O
ComplEx	O
is	O
the	O
best	O
at	O
modeling	O
symmetry	O
and	O
antisymmetry	O
,	O
and	O
indeed	O
it	O
was	O
designed	O
specifically	O
to	O
improve	O
upon	O
bilinear	O
models	O
that	O
do	O
not	O
capture	O
symmetry	O
,	O
like	O
DistMult	O
(	O
Trouillon	O
et	O
al	O
,	O
2016	O
)	O
.	O
As	O
such	O
,	O
it	O
performs	O
the	O
best	O
on	O
CODEX	O
-	O
S	O
,	O
which	O
has	O
the	O
highest	O
proportion	O
of	O
symmetric	O
relations	O
.	O
For	O
example	O
,	O
on	O
the	O
most	O
frequent	O
symmetric	O
relation	O
(	O
diplomatic	O
relation	O
)	O
,	O
ComplEx	O
achieves	O
0.859	O
MRR	B-MetricName
,	O
compared	O
to	O
0.793	O
for	O
ConvE	O
,	O
0.490	O
for	O
RESCAL	B-MethodName
,	O
and	O
0.281	O
for	O
TransE.	O
By	O
contrast	O
,	O
TuckER	B-MethodName
is	O
strongest	O
at	O
modeling	O
compositional	O
relations	O
,	O
so	O
it	O
performs	O
best	O
on	O
CODEX	O
-	O
L	O
,	O
which	O
has	O
a	O
high	O
degree	O
of	O
compositionality	O
.	O
For	O
example	O
,	O
on	O
the	O
most	O
frequent	O
compositional	O
relation	O
in	O
CODEX	O
-	O
L	O
(	O
languages	O
spoken	O
,	O
written	O
,	O
or	O
signed	O
)	O
,	O
TuckER	B-MethodName
achieves	O
0.465	O
MRR	B-MetricName
,	O
compared	O
to	O
0.464	O
for	O
RESCAL	B-MethodName
,	O
0.463	O
for	O
ConvE	O
,	O
0.456	O
for	O
ComplEx	O
,	O
and	O
0.385	O
for	O
TransE.	O
By	O
contrast	O
,	O
since	O
CODEX	O
-	O
M	O
is	O
mostly	O
asymmetric	O
and	O
non	O
-	O
compositional	O
,	O
ComplEx	O
performs	O
best	O
because	O
of	O
its	O
ability	O
to	O
model	O
asymmetry	O
.	O
Effect	O
of	O
hyperparameters	O
As	O
shown	O
by	O
Figure	O
1	O
,	O
hyperparameters	O
have	O
a	O
strong	O
impact	O
on	O
link	B-TaskName
prediction	I-TaskName
performance	O
:	O
Validation	O
MRR	B-MetricName
for	O
all	O
models	O
varies	O
by	O
over	O
30	O
percentage	O
points	O
depending	O
on	O
the	O
training	O
strategy	O
and	O
input	O
configuration	O
.	O
This	O
finding	O
is	O
consistent	O
with	O
previous	O
observations	O
in	O
the	O
literature	O
(	O
Kadlec	O
et	O
al	O
,	O
2017	O
;	O
Ruffinelli	O
et	O
al	O
,	O
2020	O
)	O
.	O
Appendix	O
F	O
provides	O
the	O
best	O
configurations	O
for	O
each	O
model	O
.	O
Overall	O
,	O
we	O
find	O
that	O
the	O
choice	O
of	O
loss	B-MetricName
function	O
in	O
particular	O
significantly	O
impacts	O
model	O
performance	O
.	O
Each	O
model	O
consistently	O
achieved	O
its	O
respective	O
peak	O
performance	O
with	O
cross	O
-	O
entropy	O
(	O
CE	O
)	O
loss	B-MetricName
,	O
a	O
finding	O
which	O
is	O
corroborated	O
by	O
several	O
other	O
KGC	O
comparison	O
papers	O
(	O
Kadlec	O
et	O
al	O
,	O
2017	O
;	O
Ruffinelli	O
et	O
al	O
,	O
2020	O
;	O
Jain	O
et	O
al	O
,	O
2020	O
)	O
.	O
As	O
far	O
as	O
negative	O
sampling	O
techniques	O
,	O
we	O
do	O
not	O
find	O
that	O
a	O
single	O
strategy	O
is	O
dominant	O
,	O
suggesting	O
that	O
the	O
choice	O
of	O
loss	B-MetricName
function	O
is	O
more	O
important	O
.	O

Table	O
6	O
gives	O
triple	B-TaskName
classification	I-TaskName
results	O
.	O
Evidently	O
,	O
triple	B-TaskName
classification	I-TaskName
on	O
randomly	O
generated	O
negatives	O
is	O
a	O
nearly	O
-	O
solved	O
task	O
.	O
On	O
negatives	O
generated	O
uniformly	O
at	O
random	O
,	O
performance	O
scores	O
are	O
nearly	O
identical	O
at	O
almost	O
100	O
%	O
accuracy	B-MetricName
.	O
Even	O
with	O
a	O
negative	O
sampling	O
strategy	O
"	O
smarter	O
"	O
than	O
uniform	O
random	O
,	O
all	O
models	O
perform	O
well	O
.	O
Hard	O
negatives	O
Classification	B-TaskName
performance	O
degenerates	O
considerably	O
on	O
our	O
hard	O
negatives	O
,	O
around	O
8	O
to	O
11	O
percentage	O
points	O
from	O
relative	O
frequency	O
-	O
based	O
sampling	O
and	O
13	O
to	O
19	O
percentage	O
points	O
from	O
uniformly	O
random	O
sampling	O
.	O
Relative	O
performance	O
also	O
varies	O
:	O
In	O
contrast	O
to	O
our	O
link	B-TaskName
prediction	I-TaskName
task	O
in	O
which	O
ComplEx	O
and	O
TuckER	B-MethodName
were	O
by	O
far	O
the	O
strongest	O
models	O
,	O
RESCAL	B-MethodName
is	O
slightly	O
stronger	O
on	O
the	O
CODEX	O
-	O
S	O
hard	O
negatives	O
,	O
whereas	O
ConvE	O
performs	O
best	O
on	O
the	O
CODEX	O
-	O
M	O
hard	O
negatives	O
.	O
These	O
results	O
indicate	O
that	O
triple	B-TaskName
classification	I-TaskName
is	O
indeed	O
a	O
distinct	O
task	O
that	O
requires	O
different	O
architectures	O
and	O
,	O
in	O
many	O
cases	O
,	O
different	O
training	O
strategies	O
(	O
Appendix	O
F	O
)	O
.	O
We	O
believe	O
that	O
few	O
recent	O
works	O
use	O
triple	B-TaskName
classification	I-TaskName
as	O
an	O
evaluation	O
task	O
because	O
of	O
the	O
lack	O
of	O
true	O
hard	O
negatives	O
in	O
existing	O
benchmarks	O
.	O
Early	O
works	O
reported	O
high	O
triple	B-TaskName
classification	I-TaskName
accuracy	B-MetricName
on	O
sampled	O
negatives	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
;	O
Wang	O
et	O
al	O
,	O
2014	O
)	O
,	O
perhaps	O
leading	O
the	O
community	O
to	O
believe	O
that	O
the	O
task	O
was	O
nearly	O
solved	O
.	O
However	O
,	O
our	O
results	O
demonstrate	O
that	O
the	O
task	O
is	O
far	O
from	O
solved	O
when	O
the	O
negatives	O
are	O
plausible	O
but	O
truly	O
false	O
.	O

Next	O
,	O
we	O
compare	O
the	O
datasets	O
in	O
a	O
link	B-TaskName
prediction	I-TaskName
task	O
to	O
show	O
that	O
CODEX	O
-	O
M	O
is	O
more	O
difficult	O
.	O
Baseline	O
We	O
devise	O
a	O
"	O
non	O
-	O
learning	O
"	O
link	B-TaskName
prediction	I-TaskName
baseline	O
.	O
Let	O
(	O
h	O
,	O
r	O
,	O
?	O
)	O
be	O
a	O
test	O
query	O
.	O
Our	O
baseline	O
scores	O
candidate	O
tail	O
entities	O
by	O
their	O
relative	O
frequency	O
in	O
the	O
tail	O
slot	O
of	O
all	O
training	O
triples	O
mentioning	O
r	O
,	O
filtering	O
out	O
tail	O
entities	O
t	O
for	O
which	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
is	O
already	O
observed	O
in	O
the	O
training	O
set	O
.	O
If	O
all	O
tail	O
entities	O
t	O
are	O
filtered	O
out	O
,	O
we	O
score	O
entities	O
by	O
frequency	O
before	O
filtering	O
.	O
The	O
logic	O
of	O
our	O
approach	O
works	O
in	O
reverse	O
for	O
(	O
?	O
,	O
r	O
,	O
t	O
)	O
queries	O
.	O
In	O
evaluating	O
our	O
baseline	O
,	O
we	O
follow	O
LibKGE	O
's	O
protocol	O
for	O
breaking	O
ties	O
in	O
ranking	O
(	O
i.e.	O
,	O
for	O
entities	O
that	O
appear	O
with	O
equal	O
frequency	O
)	O
by	O
taking	O
the	O
mean	O
rank	O
of	O
all	O
entities	O
with	O
the	O
same	O
score	O
.	O
Setup	O
We	O
compare	O
our	O
baseline	O
to	O
the	O
best	O
pretrained	O
embedding	O
model	O
per	O
dataset	O
:	O
RESCAL	B-MethodName
for	O
FB15	O
K	O
-	O
237	O
,	O
which	O
was	O
released	O
by	O
Ruffinelli	O
et	O
al	O
(	O
2020	O
)	O
,	O
and	O
ComplEx	O
for	O
CODEX	O
-	O
M.	O
We	O
evaluate	O
performance	O
with	O
MRR	B-MetricName
and	O
Hits@10	B-MetricName
.	O
Beyond	O
overall	O
performance	O
,	O
we	O
also	O
compute	O
per	O
-	O
relation	O
improvement	O
of	O
the	O
respective	O
embedding	O
over	O
our	O
baseline	O
in	O
terms	O
of	O
percentage	O
points	O
MRR	B-MetricName
.	O
This	O
measures	O
the	O
amount	O
of	O
learning	O
beyond	O
frequency	O
statistics	O
necessary	O
for	O
each	O
relation	O
.	O
Results	O
and	O
discussion	O
Table	O
7	O
compares	O
the	O
overall	O
performance	O
of	O
our	O
baseline	O
versus	O
the	O
best	O
embedding	O
per	O
dataset	O
,	O
and	O
Figure	O
3	O
shows	O
the	O
improvement	O
of	O
the	O
respective	O
embedding	O
over	O
our	O
baseline	O
per	O
relation	O
type	O
on	O
each	O
dataset	O
.	O
The	O
improvement	O
of	O
the	O
embedding	O
is	O
much	O
smaller	O
on	O
FB15	O
K	O
-	O
237	O
than	O
CODEX	O
-	O
M	O
,	O
and	O
in	O
fact	O
our	O
baseline	O
performs	O
on	O
par	O
with	O
or	O
even	O
outperforms	O
the	O
embedding	O
on	O
FB15	O
K	O
-	O
237	O
for	O
some	O
relation	O
types	O
.	O
To	O
further	O
explore	O
these	O
cases	O
,	O
Figure	O
4	O
gives	O
the	O
empirical	O
cumulative	O
distribution	O
function	O
of	O
improvement	O
,	O
which	O
shows	O
the	O
percentage	O
of	O
test	O
triples	O
for	O
which	O
the	O
level	O
of	O
improvement	O
is	O
less	O
than	O
or	O
equal	O
to	O
a	O
given	O
value	O
on	O
each	O
dataset	O
.	O
Surprisingly	O
,	O
the	O
improvement	O
for	O
both	O
MRR	B-MetricName
and	O
Hits@10	B-MetricName
is	O
less	O
than	O
five	O
percentage	O
points	O
for	O
nearly	O
40	O
%	O
of	O
FB15	O
K	O
-	O
237	O
's	O
test	O
set	O
,	O
and	O
is	O
zero	O
or	O
negative	O
15	O
%	O
of	O
the	O
time	O
.	O
By	O
contrast	O
,	O
our	O
baseline	O
is	O
significantly	O
weaker	O
than	O
the	O
strongest	O
embedding	O
method	O
on	O
CODEX	O
-	O
M.	O
The	O
disparity	O
in	O
improvement	O
is	O
due	O
to	O
two	O
relation	O
patterns	O
prevalent	O
in	O
FB15	O
K	O
-	O
237	O
:	O
Skewed	O
relations	O
FB15	O
K	O
-	O
237	O
contains	O
many	O
relations	O
that	O
are	O
skewed	O
toward	O
a	O
single	O
head	O
or	O
tail	O
entity	O
.	O
For	O
example	O
,	O
our	O
baseline	O
achieves	O
perfect	O
performance	O
over	O
all	O
(	O
h	O
,	O
r	O
,	O
?	O
)	O
We	O
conclude	O
that	O
while	O
FB15	O
K	O
-	O
237	O
is	O
a	O
valuable	O
dataset	O
,	O
CODEX	O
is	O
more	O
appropriately	O
difficult	O
for	O
link	B-TaskName
prediction	I-TaskName
.	O
Additionally	O
,	O
we	O
note	O
that	O
in	O
FB15	O
K	O
-	O
237	O
,	O
all	O
validation	O
and	O
test	O
triples	O
containing	O
entity	O
pairs	O
directly	O
linked	O
in	O
the	O
training	O
set	O
were	O
deleted	O
,	O
meaning	O
that	O
symmetry	O
can	O
not	O
be	O
tested	O
for	O
in	O
FB15	O
K	O
-	O
237	O
.	O
Given	O
that	O
CODEX	O
datasets	O
contain	O
both	O
symmetry	O
and	O
compositionality	O
,	O
CODEX	O
is	O
more	O
suitable	O
for	O
assessing	O
how	O
well	O
models	O
can	O
learn	O
relation	O
patterns	O
that	O
go	O
beyond	O
frequency	O
.	O
queries	O

Table	O
10	O
gives	O
our	O
hyperparameter	O
search	O
space	O
.	O
Tables	O
11	O
,	O
12	O
,	O
and	O
13	O
report	O
the	O
best	O
hyperparameter	O
configurations	O
for	O
link	B-TaskName
prediction	I-TaskName
on	O
CODEX	O
-	O
S	O
,	O
CODEX	O
-	O
M	O
,	O
and	O
CODEX	O
-	O
L	O
,	O
respectively	O
.	O
Tables	O
14	O
and	O
15	O
report	O
the	O
best	O
hyperparameter	O
configurations	O
for	O
triple	B-TaskName
classification	I-TaskName
on	O
the	O
hard	O
negatives	O
in	O
CODEX	O
-	O
S	O
and	O
CODEX	O
-	O
M	O
,	O
respectively	O
.	O
Terminology	O
For	O
embedding	O
initialization	O
,	O
Xv	O
refers	O
to	O
Xavier	B-MethodName
initialization	I-MethodName
(	O
Glorot	O
and	O
Bengio	O
,	O
2010	O
)	O
.	O
The	O
reciprocal	O
relations	O
model	O
refers	O
to	O
learning	O
separate	O
relation	O
embeddings	O
for	O
queries	O
in	O
the	O
direction	O
of	O
(	O
h	O
,	O
r	O
,	O
?	O
)	O
versus	O
(	O
?	O
,	O
r	O
,	O
t	O
)	O
(	O
Kazemi	O
and	O
Poole	O
,	O
2018	O
)	O
.	O
The	O
frequency	O
weighting	O
regularization	O
technique	O
refers	O
to	O
regularizing	O
embeddings	O
by	O
the	O
relative	O
frequency	O
of	O
the	O
corresponding	O
entity	O
or	O
relation	O
in	O
the	O
training	O
data	O
.	O
Search	O
strategies	O
Recall	B-MetricName
that	O
we	O
select	O
models	O
using	O
Ax	O
,	O
which	O
supports	O
hyperparameter	O
search	O
using	O
both	O
quasi	O
-	O
random	O
sequences	O
of	O
generated	O
configurations	O
and	O
Bayesian	O
optimization	O
(	O
BO	O
)	O
.	O
The	O
search	O
strategy	O
for	O
each	O
CODEX	O
dataset	O
is	O
as	O
follows	O
:	O
CODEX	O
-	O
S	O
:	O
Per	O
negative	O
sampling	O
type	O
/	O
loss	B-MetricName
combination	O
,	O
we	O
generate	O
30	O
quasi	O
-	O
random	O
trials	O
followed	O
by	O
10	O
BO	O
trials	O
.	O
We	O
select	O
the	O
best	O
-	O
performing	O
model	O
by	O
validation	O
MRR	B-MetricName
over	O
all	O
such	O
combinations	O
.	O
In	O
each	O
trial	O
,	O
the	O
model	O
is	O
trained	O
for	O
a	O
maximum	O
of	O
400	O
epochs	O
with	O
an	O
early	B-MethodName
stopping	I-MethodName
patience	O
of	O
5	O
.	O
We	O
also	O
terminate	O
a	O
trial	O
after	O
50	O
epochs	O
if	O
the	O
model	O
does	O
not	O
reach	O
≥	O
0.05	O
MRR	B-MetricName
.	O
CODEX	O
-	O
M	O
:	O
Per	O
negative	O
sampling	O
type	O
/	O
loss	B-MetricName
combination	O
,	O
we	O
generate	O
20	O
quasi	O
-	O
random	O
trials	O
.	O
The	O
maximum	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
and	O
early	B-MethodName
stopping	I-MethodName
criteria	O
are	O
the	O
same	O
as	O
for	O
CODEX	O
-	O
S.	O
CODEX	O
-	O
L	O
:	O
Per	O
negative	O
sampling	O
type	O
/	O
loss	B-MetricName
combination	O
,	O
we	O
generate	O
10	O
quasi	O
-	O
random	O
trials	O
of	O
20	O
training	O
epochs	O
instead	O
of	O
400	O
.	O
We	O
reduce	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
to	O
limit	O
resource	O
usage	O
.	O
In	O
most	O
cases	O
,	O
MRR	B-MetricName
plateaus	O
after	O
20	O
-	O
30	O
epochs	O
,	O
an	O
observation	O
which	O
is	O
consistent	O
with	O
(	O
Ruffinelli	O
et	O
al	O
,	O
2020	O
)	O
.	O
Then	O
,	O
we	O
take	O
the	O
best	O
-	O
performing	O
model	O
by	O
validation	O
MRR	B-MetricName
over	O
all	O
such	O
combinations	O
,	O
and	O
retrain	O
that	O
model	O
for	O
a	O
maximum	O
of	O
400	O
epochs	O
.	O
Note	O
that	O
we	O
search	O
using	O
MRR	B-MetricName
as	O
our	O
metric	O
,	O
but	O
the	O
triple	B-TaskName
classification	I-TaskName
task	O
measures	O
0/1	O
accuracy	B-MetricName
,	O
not	O
ranking	O
performance	O
.	O
For	O
triple	B-TaskName
classification	I-TaskName
,	O
we	O
choose	O
the	O
model	O
with	O
the	O
highest	O
validation	O
accuracy	B-MetricName
among	O
the	O
pre	O
-	O
trained	O
models	O
across	O
all	O
negative	O
sampling	O
type	O
/	O
loss	B-MetricName
function	O
combinations	O
.	O
We	O
release	O
all	O
pretrained	O
LibKGE	O
models	O
and	O
accompanying	O
configuration	O
files	O
in	O
the	O
centralized	O
CODEX	O
repository	O
.	O

