The	O
1	O
:	O
BLEU	B-MetricName
scores	O
of	O
all	O
systems	O
.	O
Bold	O
figures	O
mean	O
GBMT	O
ctx	O
is	O
significantly	O
better	O
than	O
GBMT	O
at	O
p	O
≤	O
0.01	O
.	O
*	O
means	O
a	O
system	O
is	O
significantly	O
better	O
than	O
PBMT	O
at	O
p	O
≤	O
0.01	O
.	O
+	O
means	O
a	O
system	O
is	O
significantly	O
better	O
than	O
TBMT	O
at	O
p	O
≤	O
0.01	O
.	O
Following	O
Li	O
et	O
al	O
(	O
2016	O
)	O
,	O
Chinese	O
and	O
German	O
sentences	O
are	O
parsed	O
into	O
projective	O
dependency	O
trees	O
which	O
are	O
then	O
converted	O
to	O
graphs	O
by	O
adding	O
bigram	O
edges	O
.	O
Word	B-TaskName
alignment	I-TaskName
is	O
performed	O
by	O
GIZA++	O
(	O
Och	O
and	O
Ney	O
,	O
2003	O
)	O
with	O
the	O
heuristic	O
function	O
grow	O
-	O
diag	O
-	O
final	O
-	O
and	O
.	O
We	O
use	O
SRILM	O
(	O
Stolcke	O
,	O
2002	O
)	O
to	O
train	O
a	O
5	O
-	O
gram	O
language	O
model	O
on	O
the	O
Xinhua	O
portion	O
of	O
the	O
English	O
Gigaword	O
corpus	O
5th	O
edition	O
with	O
modified	O
Kneser	O
-	O
Ney	O
discounting	O
(	O
Chen	O
and	O
Goodman	O
,	O
1996	O
)	O
.	O
Batch	O
MIRA	O
(	O
Cherry	O
and	O
Foster	O
,	O
2012	O
)	O
is	O
used	O
to	O
tune	O
feature	O
weights	O
.	O
We	O
report	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
scores	O
averaged	O
on	O
three	O
runs	O
of	O
MIRA	O
(	O
Clark	O
et	O
al	O
,	O
2011	O
)	O
.	O
We	O
compare	O
our	O
system	O
GBMT	O
ctx	O
with	O
several	O
other	O
systems	O
.	O
A	O
system	O
PBMT	O
is	O
built	O
using	O
the	O
phrase	O
-	O
based	O
model	O
in	O
Moses	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
.	O
GBMT	O
is	O
the	O
graph	O
-	O
based	O
translation	O
system	O
described	O
in	O
Li	O
et	O
al	O
(	O
2016	O
)	O
.	O
To	O
examine	O
the	O
influence	O
of	O
bigram	O
links	O
,	O
GBMT	O
is	O
also	O
used	O
to	O
translate	O
dependency	O
trees	O
where	O
treelets	O
Xiong	O
et	O
al	O
,	O
2007	O
)	O
are	O
the	O
basic	O
translation	O
units	O
.	O
Accordingly	O
,	O
we	O
name	O
the	O
system	O
TBMT	O
.	O
All	O
systems	O
are	O
implemented	O
in	O
Moses	O
.	O

Table	O
1	O
shows	O
BLEU	B-MetricName
scores	O
of	O
all	O
systems	O
.	O
We	O
found	O
that	O
GBMT	O
ctx	O
is	O
better	O
than	O
PBMT	O
across	O
all	O
test	O
sets	O
.	O
Specifically	O
,	O
the	O
improvements	O
are	O
+2.0/+0.7	O
BLEU	B-MetricName
on	O
average	O
on	O
ZH	O
-	O
EN	O
and	O
DE	O
-	O
EN	O
,	O
respectively	O
.	O
This	O
improvement	O
is	O
reasonable	O
as	O
our	O
system	O
allows	O
discontinuous	O
phrases	O
which	O
can	O
reduce	O
data	O
sparsity	O
and	O
handle	O
longdistance	O
relations	O
(	O
Galley	O
and	O
Manning	O
,	O
2010	O
)	O
.	O
In	O
addition	O
,	O
the	O
system	O
TBMT	O
does	O
not	O
show	O
consistent	O
improvements	O
over	O
PBMT	O
while	O
both	O
GBMT	O
and	O
GBMT	O
ctx	O
achieve	O
better	O
BLEU	B-MetricName
scores	O
than	O
TBMT	O
on	O
both	O
ZH	O
-	O
EN	O
(	O
+1.8	O
BLEU	B-MetricName
,	O
in	O
terms	O
of	O
The	O
number	O
of	O
rules	O
in	O
GBMT	O
ctx	O
according	O
to	O
their	O
type	O
GBMT	O
ctx	O
)	O
and	O
DE	O
-	O
EN	O
(	O
+0.6	O
BLEU	B-MetricName
,	O
in	O
terms	O
of	O
GBMT	O
ctx	O
)	O
.	O
This	O
suggests	O
that	O
continuous	O
phrases	O
connected	O
by	O
bigram	O
links	O
are	O
essential	O
to	O
system	O
performance	O
since	O
they	O
help	O
to	O
improve	O
phrase	O
coverage	O
(	O
Hanneman	O
and	O
Lavie	O
,	O
2009	O
)	O
.	O
We	O
also	O
found	O
that	O
GBMT	O
ctx	O
is	O
significantly	O
better	O
than	O
GBMT	O
on	O
both	O
ZH	O
-	O
EN	O
(	O
+1.0	O
BLEU	B-MetricName
)	O
and	O
,	O
which	O
indicates	O
that	O
explicitly	O
modeling	O
a	O
segmentation	O
using	O
context	O
is	O
helpful	O
.	O
The	O
main	O
reason	O
for	O
the	O
improvement	O
is	O
that	O
context	O
helps	O
to	O
select	O
proper	O
subgraphs	O
and	O
target	O
phrases	O
.	O
Figure	O
3	O
shows	O
example	O
translations	O
.	O
We	O
found	O
that	O
in	O
Figure	O
3a	O
,	O
after	O
translating	O
a	O
parenthesis	O
,	O
GBMT	O
ctx	O
correctly	O
selects	O
a	O
subgraph	O
Gang	O
Ao	O
Tai	O
and	O
generates	O
a	O
target	O
phrase	O
hong	O
kong	O
,	O
macao	O
and	O
taiwan	O
.	O
In	O
Figure	O
3b	O
,	O
both	O
GBMT	O
and	O
GBMT	O
ctx	O
choose	O
to	O
translate	O
the	O
subgraph	O
WoMen	O
Ye	O
ZhiLi	O
.	O
However	O
,	O
given	O
the	O
context	O
of	O
the	O
subgraph	O
,	O
GBMT	O
ctx	O
selects	O
a	O
correct	O
target	O
phrase	O
we	O
are	O
also	O
committed	O
to	O
for	O
it	O
.	O

Recall	B-MetricName
that	O
,	O
compared	O
with	O
GBMT	O
,	O
GBMT	O
ctx	O
contains	O
three	O
types	O
of	O
rules	O
:	O
basic	O
rules	O
,	O
segmenting	O
rules	O
,	O
and	O
selecting	O
rules	O
.	O
While	O
basic	O
rules	O
exist	O
in	O
both	O
systems	O
,	O
segmenting	O
and	O
selecting	O
rules	O
make	O
GBMT	O
ctx	O
context	O
-	O
aware	O
.	O
Table	O
2	O
shows	O
the	O
number	O
of	O
rules	O
in	O
GBMT	O
ctx	O
according	O
to	O
their	O
types	O
.	O
We	O
found	O
that	O
on	O
both	O
language	O
pairs	O
35	O
%	O
-	O
36	O
%	O
of	O
rules	O
are	O
basic	O
rules	O
.	O
While	O
the	O
proportion	O
of	O
segmenting	O
rules	O
is	O
∼53	O
%	O
,	O
selecting	O
rules	O
only	O
account	O
for	O
11	O
%	O
-	O
12	O
%	O
.	O
This	O
is	O
because	O
segmenting	O
rules	O
contain	O
richer	O
contextual	O
information	O
than	O
selecting	O
rules	O
.	O
Table	O
3	O
shows	O
BLEU	B-MetricName
scores	O
of	O
GBMT	O
ctx	O
when	O
different	O
types	O
of	O
rules	O
are	O
used	O
.	O
Note	O
that	O
when	O
only	O
basic	O
rules	O
are	O
allowed	O
,	O
our	O
system	O
degrades	O
to	O
the	O
conventional	O
GBMT	O
system	O
.	O
The	O
results	O
in	O
Table	O
3	O
suggest	O
that	O
both	O
segmenting	O
and	O
selecting	O
rules	O
consistently	O
improve	O
GBMT	O
on	O
both	O
language	O
pairs	O
.	O
However	O
,	O
segmenting	O
rules	O
are	O
more	O
useful	O
than	O
selecting	O
rules	O
.	O
This	O
is	O
reasonable	O
since	O
(	O
hong	O
kong	O
macao	O
taiwan	O
)	O
hong	O
kong	O
spring	O
festival	O
retail	O
business	O
rise	O
10	O
%	O
(	O
Gang	O
Ao	O
Tai	O
)	O
XiangGang	O
XinChun	O
LingShou	O
ShengYi	O
ShangSheng	O
YiCheng	O
Ref	O
:	O
GBMT	O
:	O
GBMTctx	O
:	O
(	O
hong	O
kong	O
,	O
macao	O
and	O
taiwan	O
)	O
hong	O
kong	O
's	O
retail	O
sales	O
up	O
10	O
%	O
during	O
spring	O
festival	O
(	O
the	O
spring	O
festival	O
)	O
hong	O
kong	O
retail	O
business	O
in	O
hong	O
kong	O
,	O
macao	O
and	O
taiwan	O
rose	O
by	O
10	O
%	O
(	O
hong	O
kong	O
,	O
macao	O
and	O
taiwan	O
)	O
hong	O
kong	O
spring	O
retail	O
business	O
will	O
increase	O
by	O
10	O
%	O
(	O
a	O
)	O
subgraph	O
selection	O
we	O
also	O
dedicate	O
protect	O
and	O
improve	O
living	O
emvironment	O
.	O

A	O
more	O
structured	O
approach	O
to	O
building	O
joint	O
models	O
over	O
sentences	O
can	O
instead	O
be	O
observed	O
in	O
Fact	B-TaskName
Verification	I-TaskName
Systems	O
,	O
e.g.	O
,	O
the	O
methods	O
developed	O
in	O
the	O
FEVER	B-DatasetName
challenge	O
(	O
Thorne	O
et	O
al	O
,	O
2018a	O
)	O
.	O
Such	O
systems	O
take	O
a	O
claim	O
,	O
e.g.	O
,	O
Joe	O
Walsh	O
was	O
inducted	O
in	O
2001	O
,	O
as	O
input	O
(	O
see	O
Tab	O
.	O
1	O
)	O
,	O
and	O
verify	O
if	O
it	O
is	O
valid	O
,	O
using	O
related	O
sentences	O
called	O
evidences	O
(	O
typically	O
retrieved	O
by	O
a	O
search	O
engine	O
)	O
.	O
For	O
example	O
,	O
Ev	O
1	O
,	O
As	O
a	O
member	O
of	O
the	O
Eagles	O
,	O
Walsh	O
was	O
inducted	O
into	O
the	O
Rock	O
and	O
Roll	O
Hall	O
of	O
Fame	O
in	O
1998	O
,	O
and	O
into	O
the	O
Vocal	O
Group	O
Hall	O
of	O
Fame	O
in	O
2001	O
,	O
and	O
Ev	O
3	O
,	O
Walsh	O
was	O
awarded	O
with	O
the	O
Vocal	O
Group	O
Hall	O
of	O
Fame	O
in	O
2001	O
,	O
support	O
the	O
veracity	O
of	O
the	O
claim	O
.	O
In	O
contrast	O
,	O
Ev	O
2	O
is	O
neutral	O
as	O
it	O
describes	O
who	O
Joe	O
Walsh	O
is	O
but	O
does	O
not	O
contribute	O
to	O
establish	O
the	O
induction	O
.	O
We	O
conjecture	O
that	O
supporting	O
evidence	O
for	O
answer	O
correctness	O
in	O
AS2	O
task	O
can	O
be	O
modeled	O
with	O
a	O
similar	O
rationale	O
.	O
In	O
this	O
paper	O
,	O
we	O
design	O
joint	O
models	O
for	O
AS2	O
based	O
on	O
the	O
assumption	O
that	O
,	O
given	O
q	O
and	O
a	O
target	O
answer	O
candidate	O
t	O
,	O
the	O
other	O
answer	O
candidates	O
,	O
(	O
c	O
1	O
,	O
..	O
c	O
k	O
)	O
can	O
provide	O
positive	O
,	O
negative	O
,	O
or	O
neutral	O
support	O
to	O
decide	O
the	O
correctness	O
of	O
t.	O
Our	O
first	O
approach	O
exploits	O
Fact	B-TaskName
Checking	I-TaskName
research	O
:	O
we	O
adapted	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
FEVER	B-DatasetName
system	O
,	O
KGAT	O
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
,	O
for	O
AS2	O
.	O
We	O
defined	O
a	O
claim	O
as	O
a	O
pair	O
constituted	O
of	O
the	O
question	O
and	O
one	O
target	O
answer	O
,	O
while	O
considering	O
all	O
the	O
other	O
answers	O
as	O
evidences	O
.	O
We	O
re	O
-	O
trained	O
and	O
rebuilt	O
all	O
its	O
embeddings	O
for	O
the	O
AS2	O
task	O
.	O
Our	O
second	O
method	O
,	O
Answer	O
Support	O
-	O
based	O
Reranker	O
(	O
ASR	O
)	O
,	O
is	O
completely	O
new	O
,	O
it	O
is	O
based	O
on	O
the	O
representation	O
of	O
the	O
pair	O
,	O
(	O
q	O
,	O
t	O
)	O
,	O
generated	O
by	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
AS2	O
models	O
,	O
concatenated	O
with	O
the	O
representation	O
of	O
all	O
the	O
pairs	O
(	O
t	O
,	O
c	O
i	O
)	O
.	O
The	O
latter	O
summarizes	O
the	O
contribution	O
of	O
each	O
c	O
i	O
to	O
t	O
using	O
a	O
maxpooling	O
operation	O
.	O
c	O
i	O
can	O
be	O
unrelated	O
to	O
(	O
q	O
,	O
t	O
)	O
since	O
the	O
candidates	O
are	O
automatically	O
retrieved	O
,	O
thus	O
it	O
may	O
introduce	O
just	O
noise	O
.	O
To	O
mitigate	O
this	O
problem	O
,	O
we	O
use	O
an	O
Answer	O
Support	O
Classifier	O
(	O
ASC	O
)	O
to	O
learn	O
the	O
relatedness	O
between	O
t	O
and	O
c	O
i	O
by	O
classifying	O
their	O
embedding	O
,	O
which	O
we	O
obtain	O
by	O
applying	O
a	O
transformer	O
network	O
to	O
their	O
concatenated	O
text	O
.	O
ASC	O
tunes	O
the	O
(	O
t	O
,	O
c	O
i	O
)	O
embedding	O
parameters	O
according	O
to	O
the	O
evidence	O
that	O
c	O
i	O
provides	O
to	O
t.	O
Our	O
Answer	O
Support	O
-	O
based	O
Reranker	O
(	O
ASR	O
)	O
significantly	O
improves	O
the	O
state	O
of	O
the	O
art	O
,	O
and	O
is	O
also	O
simpler	O
than	O
our	O
approach	O
based	O
on	O
KGAT	O
.	O
Our	O
third	O
method	O
is	O
an	O
extension	O
of	O
ASR	O
.	O
It	O
should	O
be	O
noted	O
that	O
,	O
although	O
ASR	O
exploits	O
the	O
information	O
from	O
the	O
k	O
candidates	O
,	O
it	O
still	O
produces	O
a	O
score	O
for	O
a	O
target	O
t	O
without	O
knowing	O
the	O
scores	O
produced	O
for	O
the	O
other	O
target	O
answers	O
.	O
Thus	O
,	O
we	O
jointly	O
model	O
the	O
representation	O
obtained	O
for	O
each	O
target	O
in	O
a	O
multi	O
-	O
ASR	O
(	O
MASR	O
)	O
architecture	O
,	O
which	O
can	O
then	O
carry	O
out	O
a	O
complete	O
global	O
reasoning	O
over	O
all	O
target	O
answers	O
.	O
We	O
experimented	O
with	O
our	O
models	O
over	O
three	O
datasets	O
,	O
WikiQA	B-DatasetName
,	O
TREC	B-DatasetName
-	O
QA	O
and	O
WQA	O
,	O
where	O
the	O
latter	O
is	O
an	O
internal	O
dataset	O
built	O
on	O
anonymized	O
customer	O
questions	O
.	O
The	O
results	O
show	O
that	O
:	O
ASR	O
improves	O
the	O
best	O
current	O
model	O
for	O
AS2	O
,	O
i.e.	O
,	O
TANDA	O
by	O
∼3	O
%	O
,	O
corresponding	O
to	O
an	O
error	O
reduction	O
of	O
10	O
%	O
in	O
Accuracy	B-MetricName
,	O
on	O
both	O
Wik	O
-	O
iQA	O
and	O
TREC	B-DatasetName
-	O
QA	O
.	O
We	O
also	O
obtain	O
a	O
relative	O
improvement	O
of	O
∼3	O
%	O
over	O
TANDA	O
on	O
WQA	O
,	O
confirming	O
that	O
ASR	O
is	O
a	O
general	O
solution	O
to	O
design	O
accurate	O
QA	O
systems	O
.	O
Most	O
interestingly	O
,	O
MASR	O
improves	O
ASR	O
by	O
additional	O
2	O
%	O
,	O
confirming	O
the	O
benefit	O
of	O
joint	O
modeling	O
.	O
Finally	O
,	O
it	O
is	O
interesting	O
to	O
mention	O
that	O
MASR	O
improvement	O
is	O
also	O
due	O
to	O
the	O
use	O
of	O
FEVER	B-DatasetName
data	O
for	O
pre	O
-	O
fine	O
-	O
tuning	O
ASC	O
,	O
suggesting	O
that	O
the	O
fact	B-TaskName
verification	I-TaskName
inference	O
and	O
the	O
answer	O
support	O
inference	O
are	O
similar	O
.	O

The	O
task	O
of	O
reranking	O
answer	O
-	O
sentence	O
candidates	O
provided	O
by	O
a	O
retrieval	O
engine	O
can	O
be	O
modeled	O
with	O
a	O
classifier	O
scoring	O
the	O
candidates	O
.	O
Let	O
q	O
be	O
an	O
element	O
of	O
the	O
question	O
set	O
,	O
Q	O
,	O
and	O
A	O
=	O
{	O
c	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
n	O
}	O
be	O
a	O
set	O
of	O
candidates	O
for	O
q	O
,	O
a	O
reranker	O
can	O
be	O
defined	O
as	O
R	O
:	O
Q	O
×	O
Π	O
(	O
A	O
)	O
Π	O
(	O
A	O
)	O
,	O
where	O
Π	O
(	O
A	O
)	O
is	O
the	O
set	O
of	O
all	O
permutations	O
of	O
A.	O
Previous	O
work	O
targeting	O
ranking	O
problems	O
in	O
the	O
text	O
domain	O
has	O
classified	O
reranking	O
functions	O
into	O
three	O
buckets	O
:	O
pointwise	O
,	O
pairwise	O
,	O
and	O
listwise	O
methods	O
.	O
Pointwise	O
reranking	O
:	O
This	O
approach	O
learns	O
p	O
(	O
q	O
,	O
c	O
i	O
)	O
,	O
which	O
is	O
the	O
probability	O
of	O
c	O
i	O
correctly	O
answering	O
q	O
,	O
using	O
a	O
standard	O
binary	O
classification	O
setting	O
.	O
The	O
final	O
rank	O
is	O
simply	O
obtained	O
sorting	O
c	O
i	O
,	O
based	O
on	O
p	O
(	O
q	O
,	O
c	O
i	O
)	O
.	O
Previous	O
work	O
estimates	O
p	O
(	O
q	O
,	O
c	O
i	O
)	O
with	O
neural	O
models	O
(	O
Severyn	O
and	O
Moschitti	O
,	O
2015	O
)	O
,	O
also	O
using	O
attention	O
mechanisms	O
,	O
e.g.	O
,	O
Compare	O
-	O
Aggregate	O
(	O
Yoon	O
et	O
al	O
,	O
2019	O
)	O
,	O
inter	O
-	O
weighted	O
alignment	O
networks	O
(	O
Shen	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
pre	O
-	O
trained	O
Transformer	B-MethodName
models	O
,	O
which	O
are	O
the	O
state	O
of	O
the	O
art	O
.	O
Garg	O
et	O
al	O
(	O
2020	O
)	O
proposed	O
TANDA	O
,	O
which	O
is	O
the	O
current	O
most	O
accurate	O
model	O
on	O
WikiQA	B-DatasetName
and	O
TREC	B-DatasetName
-	O
QA	O
.	O
Pairwise	O
reranking	O
:	O
The	O
method	O
considers	O
binary	O
classifiers	O
of	O
the	O
form	O
χ	O
(	O
q	O
,	O
c	O
i	O
,	O
c	O
j	O
)	O
for	O
determining	O
the	O
partial	O
rank	O
between	O
c	O
i	O
and	O
c	O
j	O
,	O
then	O
the	O
scoring	O
function	O
p	O
(	O
q	O
,	O
c	O
i	O
)	O
is	O
obtained	O
by	O
summing	O
up	O
all	O
the	O
contributions	O
with	O
respect	O
to	O
the	O
target	O
candidate	O
t	O
=	O
c	O
i	O
,	O
e.g.	O
,	O
p	O
(	O
q	O
,	O
c	O
i	O
)	O
=	O
j	O
χ	O
(	O
q	O
,	O
c	O
i	O
,	O
c	O
j	O
)	O
.	O
There	O
has	O
been	O
a	O
large	O
body	O
of	O
work	O
preceding	O
Transformer	B-MethodName
models	O
,	O
e.g.	O
,	O
(	O
Laskar	O
et	O
al	O
,	O
2020	O
;	O
Tayyar	O
Madabushi	O
et	O
al	O
,	O
2018	O
;	O
Rao	O
et	O
al	O
,	O
2016	O
)	O
.	O
However	O
,	O
these	O
methods	O
are	O
largely	O
outperformed	O
by	O
the	O
pointwise	O
TANDA	O
model	O
.	O
Listwise	O
reranking	O
:	O
This	O
approach	O
,	O
e.g.	O
,	O
(	O
Bian	O
et	O
al	O
,	O
2017	O
;	O
Cao	O
et	O
al	O
,	O
2007	O
;	O
Ai	O
et	O
al	O
,	O
2018	O
)	O
,	O
aims	O
at	O
learning	O
p	O
(	O
q	O
,	O
π	O
)	O
,	O
π	O
Π	O
(	O
A	O
)	O
,	O
using	O
the	O
information	O
on	O
the	O
entire	O
set	O
of	O
candidates	O
.	O
The	O
loss	B-MetricName
function	O
for	O
training	O
such	O
networks	O
is	O
constituted	O
by	O
the	O
contribution	O
of	O
all	O
elements	O
of	O
its	O
ranked	O
items	O
.	O
The	O
closest	O
work	O
to	O
our	O
research	O
is	O
by	O
Bonadiman	O
and	O
Moschitti	O
(	O
2020	O
)	O
,	O
who	O
designed	O
several	O
joint	O
models	O
.	O
These	O
improved	O
early	O
neural	O
networks	O
based	O
on	O
CNN	O
and	O
LSTM	B-MethodName
for	O
AS2	O
,	O
but	O
failed	O
to	O
improve	O
the	O
state	O
of	O
the	O
art	O
using	O
pre	O
-	O
trained	O
Transformer	B-MethodName
models	O
.	O

One	O
simple	O
and	O
effective	O
method	O
to	O
build	O
an	O
answer	O
selector	O
is	O
to	O
use	O
a	O
pre	O
-	O
trained	O
Transformer	B-MethodName
model	O
,	O
adding	O
a	O
simple	O
classification	O
layer	O
to	O
it	O
,	O
and	O
fine	O
-	O
tuning	O
the	O
model	O
on	O
the	O
AS2	O
task	O
.	O
Specifically	O
,	O
q	O
=	O
Tok	O
q	O
1	O
,	O
...	O
,	O
Tok	O
,	O
inserted	O
at	O
the	O
beginning	O
,	O
as	O
separator	O
,	O
and	O
at	O
the	O
end	O
,	O
respectively	O
.	O
This	O
input	O
is	O
encoded	O
as	O
three	O
embeddings	O
based	O
on	O
tokens	O
,	O
segments	O
and	O
their	O
positions	O
,	O
which	O
are	O
fed	O
as	O
input	O
to	O
several	O
layers	O
(	O
up	O
to	O
24	O
)	O
.	O
Each	O
of	O
them	O
contains	O
sublayers	O
for	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
,	O
normalization	O
and	O
feed	O
forward	O
processing	O
.	O
The	O
result	O
of	O
this	O
transformation	O
is	O
an	O
embedding	O
,	O
E	O
,	O
representing	O
(	O
q	O
,	O
c	O
)	O
,	O
which	O
models	O
the	O
dependencies	O
between	O
words	O
and	O
segments	O
of	O
the	O
two	O
sentences	O
.	O
For	O
the	O
downstream	O
task	O
,	O
E	O
is	O
fed	O
(	O
after	O
applying	O
a	O
non	O
-	O
linearity	O
function	O
)	O
to	O
a	O
fully	O
connected	O
layer	O
having	O
weights	O
:	O
W	O
and	O
B.	O
The	O
output	O
layer	O
can	O
be	O
used	O
to	O
implement	O
the	O
task	O
function	O
.	O
For	O
example	O
,	O
a	O
softmax	B-MethodName
can	O
be	O
used	O
to	O
model	O
the	O
probability	O
of	O
the	O
question	O
/	O
candidate	O
pair	O
classification	O
,	O
as	O
:	O
p	O
(	O
q	O
,	O
c	O
)	O
=	O
sof	B-DatasetName
tmax	O
(	O
W	O
×	O
tanh	O
(	O
E	O
(	O
q	O
,	O
c	O
)	O
)	O
+	O
B	O
)	O
.	O
We	O
can	O
train	O
this	O
model	O
with	O
log	O
cross	O
-	O
entropy	O
loss	B-MetricName
:	O
L	O
=	O
−	O
l	O
{	O
0	B-DatasetName
,	O
1	O
}	O
y	O
l	O
×	O
log	O
(	O
ŷ	O
l	O
)	O
on	O
pairs	O
of	O
texts	O
,	O
where	O
y	O
l	O
is	O
the	O
correct	O
and	O
incorrect	O
answer	O
label	O
,	O
ŷ	O
1	O
=	O
p	O
(	O
q	O
,	O
c	O
)	O
,	O
andŷ	O
0	B-DatasetName
=	O
1	O
−	O
p	O
(	O
q	O
,	O
c	O
)	O
.	O
Training	O
the	O
Transformer	B-MethodName
from	O
scratch	O
requires	O
a	O
large	O
amount	O
of	O
labeled	O
data	O
,	O
but	O
it	O
can	O
be	O
pre	O
-	O
trained	O
using	O
a	O
masked	O
language	O
model	O
,	O
and	O
the	O
next	O
sentence	O
prediction	O
tasks	O
,	O
for	O
which	O
labels	O
can	O
be	O
automatically	O
generated	O
.	O
Several	O
methods	O
for	O
pretraining	O
Transformer	B-MethodName
-	O
based	O
language	O
models	O
have	O
been	O
proposed	O
,	O
e.g.	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
,	O
XLNet	B-MethodName
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
,	O
AlBERT	O
(	O
Lan	O
et	O
al	O
,	O
2020	O
)	O
.	O

The	O
first	O
baseline	O
is	O
also	O
a	O
Transformer	B-MethodName
-	O
based	O
architecture	O
:	O
we	O
concatenate	O
the	O
question	O
with	O
the	O
top	O
k	O
+	O
1	O
answer	O
can	O
-	O
didates	O
,	O
i.e.	O
,	O
(	O
q	O
[	O
SEP	O
]	O
c	O
1	O
[	O
SEP	O
]	O
c	O
2	O
.	O
.	O
.	O
[	O
SEP	O
]	O
c	O
k+1	O
)	O
,	O
and	O
provide	O
this	O
input	O
to	O
the	O
same	O
Transformer	B-MethodName
model	O
used	O
for	O
pointwise	O
reranking	O
.	O
We	O
use	O
the	O
final	O
hidden	O
vector	O
E	O
corresponding	O
to	O
the	O
first	O
input	O
token	O
[	O
CLS	O
]	O
generated	O
by	O
the	O
Transformer	B-MethodName
,	O
and	O
a	O
classification	O
layer	O
with	O
weights	O
W	O
R	O
(	O
k+1	O
)	O
×	O
|	O
E	O
|	O
,	O
and	O
train	O
the	O
model	O
using	O
a	O
standard	O
cross	O
-	O
entropy	O
classification	O
loss	B-MetricName
:	O
y	O
×	O
log	O
(	O
sof	B-DatasetName
tmax	O
(	O
EW	O
T	O
)	O
)	O
,	O
where	O
y	O
is	O
a	O
one	O
-	O
hot	O
vector	O
representing	O
labels	O
for	O
the	O
k	O
+	O
1	O
candidates	O
,	O
i.e.	O
,	O
|	O
y	O
|	O
=	O
k	O
+	O
1	O
.	O
We	O
use	O
a	O
transformer	O
model	O
fine	O
-	O
tuned	O
with	O
the	O
TANDA	O
-	O
RoBERTa	B-MethodName
-	O
base	O
or	O
large	O
models	O
,	O
i.e.	O
,	O
RoBERTa	B-MethodName
models	O
fine	O
-	O
tuned	O
on	O
ASNQ	B-DatasetName
(	O
Garg	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
scores	O
for	O
the	O
candidate	O
answers	O
are	O
calculated	O
as	O
p	O
(	O
c	O
1	O
)	O
,	O
..	O
,	O
p	O
(	O
c	O
k+1	O
)	O
=	O
sof	B-DatasetName
tmax	O
(	O
EW	O
T	O
)	O
.	O
Then	O
,	O
we	O
rerank	O
c	O
i	O
according	O
their	O
probability	O
.	O
Joint	O
Model	O
Pairwise	O
Our	O
second	O
baseline	O
is	O
similar	O
to	O
the	O
first	O
.	O
We	O
concatenate	O
the	O
question	O
with	O
each	O
c	O
i	O
to	O
constitute	O
the	O
(	O
q	O
,	O
c	O
i	O
)	O
pairs	O
,	O
which	O
are	O
input	O
to	O
the	O
Transformer	B-MethodName
,	O
and	O
we	O
use	O
the	O
first	O
input	O
token	O
[	O
CLS	O
]	O
as	O
the	O
representation	O
of	O
each	O
(	O
q	O
,	O
c	O
i	O
)	O
pair	O
.	O
Then	O
,	O
we	O
concatenate	O
the	O
embedding	O
of	O
the	O
pair	O
containing	O
the	O
target	O
candidate	O
,	O
(	O
q	O
,	O
t	O
)	O
with	O
the	O
embedding	O
of	O
all	O
the	O
other	O
candidates	O
'	O
[	O
CLS	O
]	O
.	O
(	O
q	O
,	O
t	O
)	O
is	O
always	O
in	O
the	O
first	O
position	O
.	O
We	O
train	O
the	O
model	O
using	O
a	O
standard	O
classification	O
loss	B-MetricName
.	O
At	O
classification	O
time	O
,	O
we	O
select	O
one	O
target	O
candidate	O
at	O
a	O
time	O
,	O
and	O
set	O
it	O
in	O
the	O
first	O
position	O
,	O
followed	O
by	O
all	O
the	O
others	O
.	O
We	O
classify	O
all	O
k	O
+	O
1	O
candidates	O
and	O
use	O
their	O
score	O
for	O
reranking	O
them	O
.	O
It	O
should	O
be	O
noted	O
that	O
to	O
qualify	O
for	O
a	O
pairwise	O
approach	O
,	O
Joint	O
Model	O
Pairwise	O
should	O
use	O
a	O
ranking	O
loss	B-MetricName
.	O
However	O
,	O
we	O
always	O
use	O
standard	O
cross	O
-	O
entropy	O
loss	B-MetricName
as	O
it	O
is	O
more	O
efficient	O
and	O
the	O
different	O
is	O
performance	O
is	O
negligible	O
.	O
Joint	O
Model	O
with	O
KGAT	O
Liu	O
et	O
al	O
(	O
2020	O
)	O
presented	O
an	O
interesting	O
model	O
,	O
Kernel	O
Graph	B-MethodName
Attention	I-MethodName
Network	I-MethodName
(	O
KGAT	O
)	O
,	O
for	O
fact	B-TaskName
verification	I-TaskName
:	O
given	O
a	O
claimed	O
fact	O
f	O
,	O
and	O
a	O
set	O
of	O
evidences	O
Ev	O
=	O
{	O
ev	O
1	O
,	O
ev	O
2	O
,	O
.	O
.	O
.	O
,	O
ev	O
m	O
}	O
,	O
their	O
model	O
carries	O
out	O
joint	O
reasoning	O
over	O
Ev	O
,	O
e.g.	O
,	O
aggregating	O
information	O
to	O
estimate	O
the	O
probability	O
of	O
f	O
to	O
be	O
true	O
or	O
false	O
,	O
p	O
(	O
y	O
|	O
f	O
,	O
Ev	O
)	O
,	O
where	O
y	O
{	O
true	O
,	O
false	O
}	O
.	O
The	O
approach	O
is	O
based	O
on	O
a	O
fully	O
connected	O
graph	O
,	O
G	O
,	O
whose	O
nodes	O
are	O
the	O
n	O
i	O
=	O
(	O
f	O
,	O
ev	O
i	O
)	O
pairs	O
,	O
and	O
p	O
(	O
y	O
|	O
f	O
,	O
Ev	O
)	O
=	O
p	O
(	O
y	O
|	O
f	O
,	O
ev	O
i	O
,	O
Ev	O
)	O
p	O
(	O
ev	O
i	O
|	O
f	O
,	O
Ev	O
)	O
,	O
where	O
p	O
(	O
y	O
|	O
f	O
,	O
ev	O
i	O
,	O
Ev	O
)	O
=	O
p	O
(	O
y	O
|	O
n	O
i	O
,	O
G	O
)	O
is	O
the	O
label	O
probability	O
in	O
each	O
node	O
i	O
conditioned	O
on	O
the	O
whole	O
graph	O
,	O
and	O
p	O
(	O
ev	O
i	O
|	O
f	O
,	O
Ev	O
)	O
=	O
p	O
(	O
n	O
i	O
|	O
G	O
)	O
is	O
the	O
probability	O
of	O
selecting	O
the	O
most	O
informative	O
evidence	O
.	O
KGAT	O
uses	O
an	O
edge	O
kernel	O
to	O
perform	O
a	O
hierarchi	O
-	O
cal	O
attention	O
mechanism	O
,	O
which	O
propagates	O
information	O
between	O
nodes	O
and	O
aggregate	O
evidences	O
.	O
We	O
built	O
a	O
KGAT	O
model	O
for	O
AS2	O
as	O
follows	O
:	O
we	O
replace	O
(	O
i	O
)	O
ev	O
i	O
with	O
the	O
set	O
of	O
candidate	O
answers	O
c	O
i	O
,	O
and	O
(	O
ii	O
)	O
the	O
claim	O
f	O
with	O
the	O
question	O
and	O
a	O
target	O
answer	O
pair	O
,	O
(	O
q	O
,	O
t	O
)	O
.	O
KGAT	O
constructs	O
the	O
evidence	O
graph	O
G	O
by	O
using	O
each	O
claim	O
-	O
evidence	O
pair	O
as	O
a	O
node	O
,	O
which	O
,	O
in	O
our	O
case	O
,	O
is	O
(	O
(	O
q	O
,	O
t	O
)	O
,	O
c	O
i	O
)	O
,	O
and	O
connects	O
all	O
node	O
pairs	O
with	O
edges	O
,	O
making	O
it	O
a	O
fully	O
-	O
connected	O
evidence	O
graph	O
.	O
This	O
way	O
,	O
sentence	O
and	O
token	O
attention	O
operate	O
over	O
the	O
triplets	O
,	O
(	O
q	O
,	O
t	O
,	O
c	O
i	O
)	O
,	O
establishing	O
semantic	O
links	O
,	O
which	O
can	O
help	O
to	O
support	O
or	O
undermine	O
the	O
correctness	O
of	O
t.	O
The	O
original	O
KGAT	O
aggregates	O
all	O
the	O
pieces	O
of	O
information	O
we	O
built	O
,	O
based	O
on	O
their	O
relevance	O
,	O
to	O
determine	O
the	O
probability	O
of	O
t.	O
As	O
we	O
use	O
AS2	O
data	O
,	O
the	O
probability	O
will	O
be	O
about	O
the	O
correctness	O
of	O
t.	O
More	O
in	O
detail	O
,	O
we	O
initialize	O
the	O
node	O
representation	O
using	O
the	O
contextual	O
embeddings	O
obtained	O
with	O
two	O
TANDA	O
-	O
RoBERTa	B-MethodName
-	O
base	O
models	O
1	O
:	O
the	O
first	O
produces	O
the	O
embedding	O
of	O
(	O
q	O
,	O
t	O
)	O
,	O
while	O
the	O
second	O
outputs	O
the	O
embedding	O
of	O
(	O
q	O
,	O
c	O
i	O
)	O
.	O
Then	O
,	O
we	O
apply	O
a	O
max	O
-	O
pooling	O
operation	O
on	O
these	O
two	O
to	O
get	O
the	O
final	O
node	O
representation	O
.	O
The	O
rest	O
of	O
the	O
architecture	O
is	O
identical	O
to	O
the	O
original	O
KGAT	O
.	O
Finally	O
,	O
at	O
test	O
time	O
,	O
we	O
select	O
one	O
c	O
i	O
at	O
a	O
time	O
,	O
as	O
the	O
target	O
t	O
,	O
and	O
compute	O
its	O
probability	O
,	O
which	O
ranks	O
c	O
i	O
.	O

We	O
developed	O
ASR	O
architecture	O
described	O
in	O
Figure	O
2	O
.	O
To	O
reduce	O
the	O
noise	O
that	O
may	O
be	O
introduced	O
by	O
irrelevant	O
c	O
i	O
,	O
we	O
use	O
the	O
Answer	O
Support	O
Classifier	O
(	O
ASC	O
)	O
,	O
which	O
classifies	O
each	O
(	O
t	O
,	O
c	O
i	O
)	O
in	O
one	O
of	O
the	O
following	O
four	O
classes	O
:	O
0	B-DatasetName
:	O
t	O
and	O
c	O
i	O
are	O
both	O
correct	O
,	O
1	O
:	O
t	O
is	O
correct	O
while	O
c	O
i	O
is	O
not	O
,	O
2	O
:	O
vice	O
versa	O
,	O
and	O
3	O
:	O
both	O
incorrect	O
.	O
This	O
multi	O
-	O
classifier	O
,	O
described	O
in	O
Figure	O
1b	O
,	O
is	O
built	O
on	O
top	O
a	O
RoBERTa	B-MethodName
Transformer	B-MethodName
,	O
which	O
produced	O
a	O
PairWise	O
Representation	O
(	O
PWR	O
)	O
.	O
ASC	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
with	O
the	O
rest	O
of	O
the	O
network	O
in	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
fashion	O
,	O
using	O
its	O
specific	O
cross	O
-	O
entropy	O
loss	B-MetricName
,	O
computed	O
with	O
the	O
labels	O
above	O
.	O
3	O
.	O
The	O
ASR	O
(	O
see	O
Figure	O
1c	O
)	O
uses	O
the	O
joint	O
representation	O
of	O
(	O
q	O
,	O
t	O
)	O
with	O
(	O
t	O
,	O
c	O
i	O
)	O
,	O
i	O
=	O
1	O
,	O
..	O
,	O
k	O
,	O
where	O
t	O
and	O
c	O
i	O
are	O
the	O
top	O
-	O
candidates	O
reranked	O
by	O
PR	O
.	O
The	O
k	O
representations	O
are	O
summarized	O
by	O
applying	O
a	O
max	O
-	O
pooling	O
operation	O
,	O
which	O
will	O
aggregate	O
all	O
the	O
supporting	O
or	O
not	O
supporting	O
properties	O
of	O
the	O
candidates	O
with	O
respect	O
to	O
the	O
target	O
answer	O
.	O
The	O
concatenation	O
of	O
the	O
PR	O
embedding	O
with	O
the	O
max	O
-	O
pooling	O
embedding	O
is	O
given	O
as	O
input	O
to	O
the	O
final	O
classification	O
layer	O
,	O
which	O
scores	O
t	O
with	O
respect	O
to	O
q	O
,	O
also	O
using	O
the	O
information	O
from	O
the	O
other	O
candidates	O
.	O
For	O
training	O
and	O
testing	O
,	O
we	O
select	O
a	O
t	O
from	O
the	O
k	O
+	O
1	O
candidates	O
of	O
q	O
at	O
a	O
time	O
,	O
and	O
compute	O
its	O
score	O
.	O
This	O
way	O
,	O
we	O
can	O
rerank	O
all	O
the	O
k	O
+	O
1	O
candidates	O
with	O
their	O
scores	O
.	O
Implementation	O
details	O
:	O
ASR	O
is	O
a	O
PR	O
that	O
also	O
exploits	O
the	O
relation	O
between	O
t	O
and	O
A	O
\	O
{	O
t	O
}	O
.	O
We	O
use	O
RoBERTa	B-MethodName
to	O
generate	O
the	O
[	O
CLS	O
]	O
R	O
d	O
embedding	O
of	O
(	O
q	O
,	O
t	O
)	O
=	O
E	O
t	O
.	O
We	O
denote	O
withÊ	O
j	O
the	O
[	O
CLS	O
]	O
output	O
by	O
another	O
RoBERTa	B-MethodName
Transformer	B-MethodName
applied	O
to	O
answer	O
pairs	O
,	O
i.e.	O
,	O
(	O
t	O
,	O
c	O
j	O
)	O
.	O
Then	O
,	O
we	O
concatenate	O
E	O
t	O
to	O
the	O
max	O
-	O
pooling	O
tensor	O
fromÊ	O
1	O
,	O
..	O
,	O
Ê	O
k	O
:	O
V	O
=	O
[	O
E	O
t	O
:	O
Maxpool	O
(	O
[	O
Ê	O
1	O
,	O
..	O
,	O
Ê	O
k	O
]	O
)	O
]	O
,	O
(	O
1	O
)	O
where	O
V	O
R	O
2d	O
is	O
the	O
final	O
representation	O
of	O
the	O
target	O
answer	O
t.	O
Then	O
,	O
we	O
use	O
a	O
standard	O
feedforward	B-MethodName
network	I-MethodName
to	O
implement	O
a	O
binary	O
classification	O
layer	O
:	O
p	O
(	O
y	O
i	O
|	O
q	O
,	O
t	O
,	O
C	O
k	O
)	O
=	O
sof	B-DatasetName
tmax	O
(	O
V	O
W	O
T	O
+	O
B	O
)	O
,	O
where	O
W	O
R	O
2×2d	O
and	O
B	O
are	O
parameters	O
to	O
transform	O
the	O
representation	O
of	O
the	O
target	O
answer	O
t	O
from	O
dimension	O
2d	O
to	O
dimension	O
2	O
,	O
which	O
represents	O
correct	O
or	O
incorrect	O
labels	O
.	O
ASC	O
labels	O
There	O
can	O
be	O
different	O
interpretations	O
when	O
attempting	O
to	O
define	O
labels	O
for	O
answer	O
pairs	O
.	O
An	O
alternative	O
to	O
the	O
definition	O
illustrated	O
above	O
is	O
to	O
use	O
the	O
following	O
FEVER	B-DatasetName
compatible	O
encoding	O
:	O
0	B-DatasetName
:	O
t	O
is	O
correct	O
,	O
while	O
c	O
i	O
can	O
be	O
any	O
value	O
,	O
as	O
also	O
an	O
incorrect	O
c	O
i	O
may	O
provide	O
important	O
context	O
(	O
corresponding	O
to	O
FEVER	B-DatasetName
Support	O
label	O
)	O
;	O
1	O
:	O
t	O
is	O
incorrect	O
,	O
c	O
i	O
correct	O
,	O
since	O
c	O
i	O
can	O
provide	O
evidence	O
that	O
t	O
is	O
not	O
similar	O
to	O
a	O
correct	O
answer	O
(	O
corresponding	O
to	O
FEVER	B-DatasetName
Refutal	O
label	O
)	O
;	O
and	O
2	O
:	O
both	O
are	O
incorrect	O
,	O
in	O
this	O
case	O
,	O
nothing	O
can	O
be	O
told	O
(	O
corresponding	O
to	O
FEVER	B-DatasetName
Neutral	O
label	O
)	O
.	O

The	O
goal	O
of	O
MASR	O
is	O
to	O
measure	O
the	O
relation	O
between	O
k	O
+	O
1	O
target	O
answers	O
,	O
t	O
0	B-DatasetName
,	O
..	O
,	O
t	O
k	O
.	O
The	O
representation	O
of	O
each	O
target	O
answer	O
is	O
the	O
embedding	O
V	O
R	O
2d	O
from	O
Equation	O
1	O
in	O
ASR	O
.	O
Then	O
,	O
we	O
concatenate	O
the	O
hidden	O
vectors	O
of	O
k	O
+	O
1	O
target	O
answers	O
to	O
form	O
a	O
matrix	O
V	O
(	O
q	O
,	O
k+1	O
)	O
R	O
(	O
k+1	O
)	O
×2d	O
.	O
We	O
use	O
this	O
matrix	O
and	O
a	O
classification	O
layer	O
weights	O
W	O
R	O
2d	O
,	O
and	O
compute	O
a	O
standard	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
loss	B-MetricName
:	O
L	O
M	O
ASR	O
=	O
y	O
*	O
log	O
(	O
sof	B-DatasetName
tmax	O
(	O
V	O
(	O
q	O
,	O
k+1	O
)	O
W	O
T	O
)	O
,	O
(	O
2	O
)	O
where	O
y	O
is	O
a	O
one	O
-	O
hot	O
-	O
vector	O
,	O
and	O
|	O
y	O
|	O
=	O
|	O
k	O
+	O
1	O
|	O
.	O

Metrics	O
The	O
performance	O
of	O
QA	O
systems	O
is	O
typically	O
measured	O
with	O
Accuracy	B-MetricName
in	O
providing	O
correct	O
answers	O
,	O
i.e.	O
,	O
the	O
percentage	O
of	O
correct	O
responses	O
.	O
This	O
is	O
also	O
referred	O
to	O
Precision	B-MetricName
-	O
at	O
-	O
1	O
(	O
P@1	B-MetricName
)	O
in	O
the	O
context	O
of	O
reranking	O
,	O
while	O
standard	O
Precision	B-MetricName
and	O
Recall	B-MetricName
are	O
not	O
essential	O
in	O
our	O
case	O
as	O
we	O
assume	O
the	O
system	O
does	O
not	O
abstain	O
from	O
providing	O
answers	O
.	O
We	O
also	O
use	O
Mean	O
Average	B-MetricName
Precision	I-MetricName
(	O
MAP	B-DatasetName
)	O
and	O
Mean	O
Reciprocal	O
Recall	B-MetricName
(	O
MRR	B-MetricName
)	O
evaluated	O
on	O
the	O
test	O
set	O
,	O
using	O
the	O
entire	O
set	O
of	O
candidates	O
for	O
each	O
Table	O
4	O
:	O
Results	O
on	O
WikiQA	B-DatasetName
,	O
TREC	B-DatasetName
-	O
QA	O
and	O
WQA	O
,	O
using	O
RoBERTa	B-MethodName
base	O
Transformer	B-MethodName
.	O
†	O
is	O
used	O
to	O
indicate	O
that	O
the	O
difference	O
in	O
P@1	B-MetricName
between	O
ASR	O
and	O
the	O
other	O
marked	O
systems	O
is	O
statistically	O
significant	O
at	O
95	O
%	O
.	O
JOINT	O
-	O
MULTICLASSIFER	O
JOINT	O
-	O
PAIR	O
KGAT	O
ASR	O
1	O
2	O
3	O
4	O
5	O
−3	O
−2	O
−1	O
0	B-DatasetName
1	O
2	O
3	O
4	O
5	O
k	O
Improvement	O
(	O
%	O
)	O
Figure	O
2	O
:	O
Impact	O
of	O
k	O
on	O
the	O
WQA	O
dev	O
.	O
set	O
question	O
(	O
this	O
varies	O
according	O
to	O
the	O
dataset	O
)	O
,	O
to	O
have	O
a	O
direct	O
comparison	O
with	O
the	O
state	O
of	O
the	O
art	O
.	O
Models	O
We	O
use	O
the	O
pre	O
-	O
trained	O
RoBERTa	B-MethodName
-	O
Base	O
(	O
12	O
layer	O
)	O
and	O
RoBERTa	B-MethodName
-	O
Large	O
-	O
MNLI	B-DatasetName
(	O
24	O
layer	O
)	O
models	O
,	O
which	O
were	O
released	O
as	O
checkpoints	O
for	O
use	O
in	O
downstream	O
tasks	O
4	O
.	O
Reranker	O
training	O
We	O
adopt	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e	O
-	O
5	O
for	O
the	O
transfer	O
step	O
on	O
the	O
ASNQ	B-DatasetName
dataset	O
(	O
Garg	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	O
-	O
6	O
for	O
the	O
adapt	O
step	O
on	O
the	O
target	O
dataset	O
.	O
We	O
apply	O
early	B-MethodName
stopping	I-MethodName
on	O
the	O
development	O
set	O
of	O
the	O
target	O
corpus	O
for	O
both	O
fine	O
-	O
tuning	O
steps	O
based	O
on	O
the	O
highest	O
MAP	B-DatasetName
score	O
.	O
We	O
set	O
the	O
max	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
equal	O
to	O
3	O
and	O
9	O
for	O
the	O
adapt	O
and	O
transfer	O
steps	O
,	O
respectively	O
.	O
We	O
set	O
the	O
maximum	O
sequence	O
length	O
for	O
RoBERTa	B-MethodName
to	O
128	O
tokens	O
.	O
KGAT	O
and	O
ASR	O
training	O
Again	O
,	O
we	O
use	O
the	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e	O
-	O
6	O
for	O
training	O
the	O
ASR	O
model	O
on	O
the	O
target	O
dataset	O
.	O
We	O
utilize	O
1	O
Tesla	O
V100	O
GPU	O
with	O
32	O
GB	O
memory	O
and	O
a	O
train	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
eight	O
.	O
We	O
set	O
the	O
maximum	O
sequence	O
length	O
for	O
RoBERTa	B-MethodName
Base	O
/	O
Large	O
to	O
130	O
tokens	O
and	O
the	O
number	O
of	O
training	O
epochs	O
to	O
20	O
.	O
The	O
other	O
training	O
configurations	O
are	O
the	O
same	O
of	O
the	O
original	O
KGAT	O
model	O
from	O
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
use	O
two	O
transformer	O
models	O
for	O
ASR	O
:	O
a	O
RoBERTa	B-MethodName
4	O
https://github.com/pytorch/fairseq	O
Base	O
/	O
Large	O
for	O
PR	O
,	O
and	O
one	O
for	O
ASC	O
.	O
We	O
set	O
the	O
maximum	O
sequence	O
length	O
for	O
RoBERTa	B-MethodName
to	O
128	O
tokens	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
to	O
20	O
.	O

Table	O
4	O
reports	O
the	O
P@1	B-MetricName
,	O
MAP	B-DatasetName
and	O
MRR	B-MetricName
of	O
the	O
rerankers	O
,	O
and	O
different	O
answer	O
supporting	O
models	O
on	O
WikiQA	B-DatasetName
,	O
TREC	B-DatasetName
-	O
QA	O
and	O
WQA	O
datasets	O
.	O
As	O
WQA	O
is	O
an	O
internal	O
dataset	O
,	O
we	O
only	O
report	O
the	O
improvement	O
over	O
PR	O
in	O
the	O
tables	O
.	O
All	O
models	O
use	O
RoBERTa	B-MethodName
-	O
Base	O
pre	O
-	O
trained	O
checkpoint	O
and	O
start	O
from	O
the	O
same	O
set	O
of	O
k	O
candidates	O
reranked	O
by	O
PR	O
(	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
)	O
.	O
The	O
table	O
shows	O
that	O
:	O
PR	O
replicates	O
the	O
MAP	B-DatasetName
and	O
MRR	B-MetricName
of	O
the	O
stateof	O
-	O
the	O
-	O
art	O
reranker	O
by	O
Garg	O
et	O
al	O
(	O
2020	O
)	O
on	O
WikiQA	B-DatasetName
.	O
Joint	O
Model	O
Multi	O
-	O
classifier	O
performs	O
lower	O
than	O
PR	O
for	O
all	O
measures	O
and	O
all	O
datasets	O
.	O
This	O
is	O
in	O
line	O
with	O
the	O
findings	O
of	O
Bonadiman	O
and	O
Moschitti	O
(	O
2020	O
)	O
,	O
who	O
also	O
did	O
not	O
obtain	O
improvement	O
when	O
jointly	O
used	O
all	O
the	O
candidates	O
altogether	O
in	O
a	O
representation	O
.	O
Joint	O
Model	O
Pairwise	O
differs	O
from	O
ASR	O
as	O
it	O
concatenates	O
the	O
embeddings	O
of	O
the	O
(	O
q	O
,	O
c	O
i	O
)	O
,	O
instead	O
of	O
using	O
max	O
-	O
pooling	O
,	O
and	O
does	O
not	O
use	O
any	O
Answer	O
Support	O
Classifier	O
(	O
ASC	O
)	O
.	O
Still	O
,	O
it	O
exploits	O
the	O
idea	O
of	O
aggregating	O
the	O
information	O
of	O
all	O
pairs	O
(	O
q	O
,	O
c	O
i	O
)	O
with	O
respect	O
to	O
a	O
target	O
answer	O
t	O
,	O
which	O
proves	O
to	O
be	O
effective	O
,	O
as	O
the	O
model	O
improves	O
on	O
PR	O
over	O
all	O
measures	O
and	O
datasets	O
.	O
Our	O
KGAT	O
version	O
for	O
AS2	O
also	O
improves	O
PR	O
over	O
all	O
datasets	O
and	O
almost	O
all	O
measures	O
,	O
confirming	O
that	O
the	O
idea	O
of	O
using	O
candidates	O
as	O
support	O
of	O
the	O
target	O
answer	O
is	O
generally	O
valid	O
.	O
However	O
,	O
it	O
is	O
not	O
superior	O
to	O
Joint	O
Model	O
Pairwise	O
.	O
ASR	O
achieves	O
the	O
highest	O
performance	O
among	O
all	O
models	O
(	O
but	O
MASR	O
-	O
FP	O
on	O
WQA	O
)	O
,	O
all	O
datasets	O
,	O
and	O
all	O
measures	O
.	O
For	O
example	O
,	O
it	O
outperforms	O
PR	O
by	O
almost	O
3	O
absolute	O
percent	O
points	O
in	O
P@1	B-MetricName
on	O
WikiQA	B-DatasetName
,	O
and	O
by	O
almost	O
6	O
points	O
on	O
TREC	B-DatasetName
from	O
91.18	O
%	O
to	O
97.06	O
%	O
,	O
which	O
corresponds	O
to	O
an	O
error	O
reduction	O
of	O
60	O
%	O
.	O
We	O
perform	O
randomization	O
test	O
(	O
Yeh	O
,	O
2000	O
)	O
to	O
verify	O
if	O
the	O
models	O
significantly	O
differ	O
in	O
terms	O
of	O
prediction	O
outcome	O
.	O
We	O
use	O
100	O
,	O
000	O
trials	O
for	O
each	O
calculation	O
.	O
The	O
results	O
confirm	O
the	O
statistically	O
significant	O
difference	O
between	O
ASR	O
and	O
all	O
the	O
baselines	O
,	O
with	O
p	O
<	O
0.05	O
for	O
WikiQA	B-DatasetName
,	O
and	O
between	O
ASR	O
and	O
all	O
models	O
(	O
i.e.	O
,	O
including	O
also	O
KGAT	O
)	O
on	O
WQA	O
.	O

As	O
the	O
state	O
of	O
the	O
art	O
for	O
AS2	O
is	O
obtained	O
using	O
RoBERTa	B-MethodName
Large	O
,	O
we	O
trained	O
KGAT	O
and	O
ASR	O
using	O
this	O
pre	O
-	O
trained	O
language	O
model	O
.	O
Table	O
5	O
also	O
reports	O
the	O
comparison	O
with	O
PR	O
,	O
which	O
is	O
the	O
official	O
state	O
of	O
the	O
art	O
.	O
Again	O
,	O
our	O
PR	O
replicates	O
the	O
results	O
of	O
Garg	O
et	O
al	O
(	O
2020	O
)	O
,	O
obtaining	O
slightly	O
lower	O
performance	O
on	O
WikiQA	B-DatasetName
but	O
higher	O
on	O
TREC	B-DatasetName
-	O
QA	O
.	O
KGAT	O
performs	O
lower	O
than	O
PR	O
on	O
both	O
datasets	O
.	O
ASR	O
establishes	O
the	O
new	O
state	O
of	O
the	O
art	O
on	O
WikiQA	B-DatasetName
with	O
an	O
MAP	B-DatasetName
of	O
92.80	O
vs.	O
92.00	O
.	O
The	O
P@1	B-MetricName
also	O
significantly	O
improves	O
by	O
2	O
%	O
,	O
i.e.	O
,	O
achieving	O
89.71	O
,	O
which	O
is	O
impressively	O
high	O
.	O
Also	O
,	O
on	O
TREC	B-DatasetName
-	O
QA	O
,	O
ASR	O
outperforms	O
all	O
models	O
,	O
being	O
on	O
par	O
with	O
PR	O
regarding	O
P@1	B-MetricName
.	O
The	O
latter	O
is	O
97.06	O
,	O
which	O
corresponds	O
to	O
mistaking	O
the	O
answers	O
of	O
only	O
two	O
questions	O
.	O
We	O
manually	O
checked	O
these	O
and	O
found	O
out	O
that	O
these	O
were	O
two	O
annotation	O
errors	O
:	O
ASR	O
achieves	O
perfect	O
accuracy	B-MetricName
while	O
PR	O
only	O
mistakes	O
one	O
answer	O
.	O
Of	O
course	O
,	O
this	O
just	O
provides	O
evidence	O
that	O
PR	O
based	O
on	O
RoBERTa	B-MethodName
-	O
Large	O
solves	O
the	O
task	O
of	O
selecting	O
the	O
best	O
answers	O
(	O
i.e.	O
,	O
measuring	O
P@1	B-MetricName
on	O
this	O
dataset	O
is	O
not	O
meaningful	O
anymore	O
)	O
.	O
Sec	O
.	O
4.1	O
)	O
.	O
ACC	B-MetricName
is	O
the	O
overall	B-MetricName
accuracy	I-MetricName
while	O
F1	B-MetricName
refers	O
to	O
the	O
category	O
0	B-DatasetName
.	O
We	O
note	O
that	O
ASC	O
in	O
MASR	O
-	O
FP	O
achieves	O
the	O
highest	O
accuracy	B-MetricName
with	O
respect	O
to	O
the	O
average	O
over	O
all	O
datasets	O
.	O
This	O
happens	O
since	O
we	O
pre	O
-	O
fine	O
-	O
tuned	O
it	O
with	O
the	O
FEVER	B-DatasetName
data	O
.	O

We	O
analyzed	O
examples	O
for	O
which	O
ASR	O
is	O
correct	O
and	O
PR	O
is	O
not	O
.	O
Tab	O
.	O
7	O
shows	O
that	O
,	O
given	O
q	O
and	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
candidates	O
,	O
PR	O
chooses	O
c	O
1	O
,	O
a	O
suitable	O
but	O
wrong	O
answer	O
.	O
This	O
probably	O
happens	O
since	O
the	O
answer	O
best	O
matches	O
the	O
syntactic	O
/	O
semantic	O
pattern	O
of	O
the	O
question	O
,	O
which	O
asks	O
for	O
a	O
type	O
of	O
color	O
,	O
indeed	O
,	O
the	O
answer	O
offers	O
such	O
type	O
,	O
primary	O
colors	O
.	O
PR	O
does	O
not	O
rely	O
on	O
any	O
background	O
information	O
that	O
can	O
support	O
the	O
set	O
of	O
colors	O
in	O
the	O
answer	O
.	O
In	O
contrast	O
,	O
ASR	O
selects	O
c	O
2	O
as	O
it	O
can	O
rely	O
on	O
the	O
support	O
of	O
other	O
answers	O
.	O
Its	O
ASC	O
provides	O
an	O
average	O
score	O
for	O
the	O
category	O
0	B-DatasetName
(	O
both	O
members	O
are	O
correct	O
)	O
of	O
c	O
2	O
,	O
i.e.	O
,	O
1	O
k	O
i	O
=	O
2	O
ASC	O
(	O
c	O
2	O
,	O
c	O
i	O
)	O
=	O
0.653	O
,	O
while	O
for	O
c	O
1	O
the	O
average	O
score	O
is	O
significant	O
lower	O
,	O
i.e.	O
,	O
0.522	O
.	O
This	O
provides	O
higher	O
support	O
for	O
c	O
2	O
,	O
which	O
is	O
used	O
by	O
ASR	O
to	O
rerank	O
the	O
output	O
of	O
PR	O
.	O
Tab	O
.	O
8	O
shows	O
an	O
interesting	O
case	O
where	O
all	O
the	O
sentences	O
contain	O
the	O
required	O
information	O
,	O
i.e.	O
,	O
February	O
.	O
However	O
,	O
PR	O
and	O
ASR	O
both	O
choose	O
answer	O
c	O
0	B-DatasetName
,	O
which	O
is	O
correct	O
but	O
not	O
natural	O
,	O
as	O
it	O
provides	O
the	O
requested	O
information	O
indirectly	O
.	O
Also	O
,	O
it	O
contains	O
a	O
lot	O
of	O
ancillary	O
information	O
.	O
In	O
contrast	O
,	O
MASR	O
is	O
able	O
to	O
rerank	O
the	O
best	O
answer	O
,	O
c	O
1	O
,	O
in	O
the	O
top	O
position	O
.	O

Why	O
-	O
question	B-TaskName
answering	I-TaskName
(	O
why	O
-	O
QA	O
)	O
tasks	O
retrieve	O
from	O
a	O
text	O
archive	O
answers	O
to	O
such	O
why	O
-	O
questions	O
as	O
"	O
Why	O
does	O
honey	O
last	O
such	O
a	O
long	O
time	O
?	O
"	O
Previous	O
why	O
-	O
QA	O
methods	O
retrieve	O
from	O
a	O
text	O
archive	O
answer	O
passages	O
,	O
each	O
of	O
which	O
consists	O
of	O
several	O
sentences	O
,	O
like	O
A	O
in	O
Table	O
1	O
(	O
Girju	O
,	O
2003	O
;	O
Higashinaka	O
and	O
Isozaki	O
,	O
2008	O
;	O
Oh	O
et	O
al	O
,	O
,	O
2013Sharp	O
et	O
al	O
,	O
2016	O
;	O
Verberne	O
et	O
al	O
,	O
2011	O
)	O
,	O
and	O
then	O
determine	O
whether	O
the	O
passages	O
answer	O
the	O
question	O
.	O
A	O
proper	O
answer	O
passage	O
must	O
contain	O
(	O
1	O
)	O
a	O
paraphrase	O
of	O
the	O
why	O
-	O
question	O
(	O
e.g.	O
,	O
the	O
underlined	O
texts	O
in	O
Table	O
1	O
)	O
and	O
(	O
2	O
)	O
the	O
reasons	O
or	O
the	O
causes	O
(	O
e.g.	O
,	O
the	O
bold	O
texts	O
in	O
Table	O
1	O
)	O
of	O
Q	O
Why	O
does	O
honey	O
last	O
a	O
long	O
time	O
?	O
A	O
While	O
excavating	O
Egypt	O
's	O
pyramids	O
,	O
archaeologists	O
have	O
found	O
pots	O
of	O
honey	O
in	O
an	O
ancient	O
tomb	O
:	O
thousands	O
of	O
years	O
old	O
and	O
still	O
preserved	O
.	O
Honey	O
can	O
last	O
a	O
long	O
time	O
due	O
to	O
three	O
special	O
properties	O
.	O
Its	O
average	O
pH	O
is	O
3.9	O
,	O
which	O
is	O
quite	O
acidic	O
.	O
Such	O
high	O
level	O
of	O
acidity	O
is	O
certainly	O
hostile	O
and	O
hinders	O
the	O
growth	O
of	O
many	O
microbes	O
.	O
Though	O
honey	O
contains	O
around	O
17	O
-	O
18	O
%	O
water	O
,	O
its	O
water	O
activity	O
is	O
too	O
low	O
to	O
support	O
the	O
growth	O
of	O
microbes	O
.	O
Moreover	O
honey	O
contains	O
hydrogen	O
peroxide	O
,	O
which	O
is	O
thought	O
to	O
help	O
prevent	O
the	O
growth	O
of	O
microbes	O
in	O
honey	O
.	O
Despite	O
these	O
properties	O
,	O
honey	O
can	O
be	O
contaminated	O
under	O
certain	O
circumstances	O
.	O
C	O
Because	O
its	O
acidity	O
,	O
low	O
water	O
activity	O
,	O
and	O
hydrogen	O
peroxide	O
together	O
hinder	O
the	O
growth	O
of	O
microbes	O
.	O
Table	O
1	O
:	O
Answer	O
passage	O
A	O
to	O
why	O
-	O
question	O
Q	O
and	O
its	O
compact	O
answer	O
C	O
the	O
events	O
described	O
in	O
the	O
why	O
-	O
question	O
,	O
both	O
of	O
which	O
are	O
often	O
written	O
in	O
multiple	O
non	O
-	O
adjacent	O
sentences	O
.	O
This	O
multi	O
-	O
sentenceness	O
implies	O
that	O
the	O
answer	O
passages	O
often	O
contain	O
redundant	O
parts	O
that	O
are	O
not	O
directly	O
related	O
to	O
a	O
why	O
-	O
question	O
or	O
its	O
reason	O
/	O
cause	O
and	O
whose	O
presence	O
complicates	O
the	O
why	O
-	O
QA	O
task	O
.	O
Highly	O
accurate	O
why	O
-	O
QA	O
methods	O
should	O
be	O
able	O
to	O
find	O
the	O
exact	O
reason	O
sought	O
by	O
a	O
why	O
-	O
question	O
in	O
an	O
answer	O
passage	O
without	O
being	O
distracted	O
by	O
redundancy	O
.	O
In	O
this	O
paper	O
,	O
we	O
train	O
a	O
neural	O
network	O
(	O
NN	O
)	O
to	O
generate	O
,	O
from	O
an	O
answer	O
passage	O
,	O
a	O
vector	O
representation	O
of	O
the	O
non	O
-	O
redundant	O
reason	O
asked	O
by	O
a	O
why	O
-	O
question	O
,	O
and	O
exploit	O
the	O
generated	O
vector	O
representation	O
as	O
evidence	O
for	O
judging	O
whether	O
the	O
passage	O
answers	O
the	O
why	O
-	O
question	O
.	O
This	O
idea	O
was	O
inspired	O
by	O
Ishida	O
et	O
al	O
(	O
2018	O
)	O
,	O
who	O
used	O
a	O
seq2seq	B-MethodName
model	O
to	O
automatically	O
generate	O
such	O
compact	O
answers	O
as	O
C	O
in	O
Table	O
1	O
from	O
the	O
answer	O
passages	O
retrieved	O
by	O
a	O
why	O
-	O
QA	O
method	O
.	O
Compact	O
answers	O
are	O
sentences	O
or	O
phrases	O
that	O
express	O
the	O
reasons	O
for	O
a	O
given	O
why	O
-	O
question	O
without	O
redundancy	O
.	O
If	O
we	O
can	O
use	O
such	O
automatically	O
generated	O
compact	O
-	O
answers	O
to	O
support	O
a	O
why	O
-	O
QA	O
method	O
in	O
finding	O
the	O
exact	O
reason	O
of	O
a	O
whyquestion	O
in	O
these	O
passages	O
,	O
why	O
-	O
QA	O
accuracy	B-MetricName
may	O
be	O
improved	O
.	O
We	O
actually	O
tried	O
this	O
idea	O
in	O
a	O
preliminary	O
study	O
in	O
which	O
we	O
generated	O
a	O
compact	O
answer	O
from	O
a	O
given	O
question	O
-	O
passage	O
pair	O
by	O
using	O
the	O
compact	O
-	O
answer	B-TaskName
generation	I-TaskName
method	O
of	O
Iida	O
et	O
al	O
(	O
2019	O
)	O
and	O
used	O
the	O
generated	O
compactanswer	O
along	O
with	O
the	O
given	O
question	O
-	O
passage	O
pair	O
to	O
find	O
proper	O
answer	O
passages	O
.	O
However	O
,	O
we	O
were	O
disappointed	O
by	O
the	O
small	O
performance	O
improvement	O
,	O
as	O
shown	O
in	O
our	O
experimental	O
results	O
.	O
We	O
chose	O
an	O
alternative	O
approach	O
.	O
Instead	O
of	O
generating	O
a	O
compact	O
answer	O
of	O
an	O
answer	O
passage	O
as	O
word	O
sequences	O
,	O
we	O
devised	O
a	O
model	O
to	O
generate	O
a	O
compact	O
-	O
answer	O
representation	O
,	O
which	O
is	O
a	O
vector	O
representation	O
for	O
a	O
compact	O
answer	O
,	O
from	O
an	O
answer	O
passage	O
.	O
Inspired	B-DatasetName
by	O
the	O
generative	B-MethodName
adversarial	I-MethodName
network	I-MethodName
(	O
GAN	B-MethodName
)	O
approach	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
,	O
we	O
developed	O
an	O
adversarial	O
network	O
called	O
the	O
Adversarial	O
networks	O
for	O
Generating	O
compact	O
-	O
answer	O
Representation	O
(	O
AGR	O
)	O
.	O
Like	O
the	O
original	O
GAN	B-MethodName
,	O
an	O
AGR	O
is	O
composed	O
of	O
a	O
generator	O
and	O
a	O
discriminator	O
:	O
the	O
generator	O
network	O
is	O
trained	O
for	O
generating	O
(	O
from	O
answer	O
passages	O
)	O
fake	O
representations	O
to	O
make	O
it	O
hard	O
for	O
the	O
discriminator	O
network	O
to	O
distinguish	O
these	O
fake	O
representations	O
from	O
the	O
true	O
representations	O
derived	O
from	O
manually	O
created	O
compact	O
-	O
answers	O
.	O
We	O
combined	O
the	O
generator	O
network	O
in	O
the	O
AGR	O
with	O
an	O
extension	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
why	O
-	O
QA	O
method	O
.	O
Our	O
evaluation	O
against	O
a	O
Japanese	O
open	O
-	O
domain	O
why	O
-	O
QA	O
dataset	O
,	O
which	O
was	O
created	O
using	O
general	O
web	O
texts	O
as	O
a	O
source	O
of	O
answer	O
passages	O
,	O
revealed	O
that	O
the	O
generator	O
network	O
significantly	O
improved	O
the	O
accuracy	B-MetricName
of	O
the	O
top	O
-	O
ranked	O
answer	O
passages	O
and	O
that	O
the	O
combination	O
significantly	O
outperformed	O
several	O
strong	O
baselines	O
,	O
including	O
a	O
combination	O
of	O
a	O
generator	O
network	O
and	O
a	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
This	O
combination	O
also	O
outperformed	O
a	O
vanilla	O
BERT	B-MethodName
model	O
,	O
suggesting	O
that	O
the	O
generator	O
network	O
in	O
our	O
AGR	O
may	O
be	O
effective	O
even	O
if	O
it	O
is	O
combined	O
with	O
many	O
types	O
of	O
NN	O
architectures	O
.	O
Another	O
interesting	O
point	O
is	O
that	O
the	O
performance	O
improved	O
even	O
when	O
we	O
replaced	O
,	O
as	O
the	O
inputs	O
to	O
AGR	O
,	O
the	O
word	O
embedding	O
vectors	O
that	O
represent	O
an	O
answer	O
passage	O
,	O
with	O
a	O
random	O
vector	O
.	O
This	O
observation	O
warrants	O
further	O
exploration	O
in	O
our	O
future	O
work	O
.	O
Finally	O
,	O
we	O
applied	O
our	O
AGR	O
to	O
a	O
distantly	O
su	O
-	O
(	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
,	O
which	O
is	O
an	O
extension	O
of	O
a	O
machinereading	O
task	O
,	O
to	O
check	O
whether	O
it	O
is	O
applicable	O
to	O
other	O
datasets	O
.	O
We	O
combined	O
our	O
generator	O
network	O
with	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
DS	O
-	O
QA	O
method	O
,	O
OpenQA	O
(	O
Lin	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
used	O
a	O
generated	O
compact	O
-	O
answer	O
representation	O
from	O
a	O
given	O
passage	O
as	O
evidence	O
to	O
1	O
)	O
select	O
relevant	O
passages	O
from	O
the	O
retrieved	O
ones	O
and	O
2	O
)	O
find	O
an	O
answer	O
from	O
the	O
selected	O
passages	O
.	O
Although	O
the	O
task	O
was	O
not	O
our	O
initial	O
target	O
(	O
why	O
-	O
QA	O
)	O
and	O
the	O
answers	O
in	O
the	O
DS	O
-	O
QA	O
task	O
were	O
considerably	O
shorter	O
than	O
those	O
in	O
the	O
why	O
-	O
QA	O
,	O
experiments	O
using	O
three	O
publicly	O
available	O
datasets	O
(	O
Quasar	B-DatasetName
-	I-DatasetName
T	I-DatasetName
(	O
Dhingra	O
et	O
al	O
,	O
2017	O
)	O
,	O
SearchQA	B-DatasetName
(	O
Dunn	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
Triv	O
-	O
iaQA	O
(	O
Joshi	O
et	O
al	O
,	O
2017	O
)	O
)	O
revealed	O
that	O
the	O
generator	O
network	O
improved	O
the	O
performance	O
in	O
most	O
cases	O
.	O
This	O
suggests	O
that	O
AGR	O
may	O
be	O
applicable	O
to	O
many	O
QA	O
-	O
like	O
tasks	O
.	O
2	O
Why	O
-	O
QA	O
Model	O
Figure	O
1	O
illustrates	O
the	O
architecture	O
of	O
our	O
why	O
-	O
QA	O
model	O
and	O
the	O
AGR	O
.	O
Our	O
why	O
-	O
QA	O
model	O
computes	O
the	O
probability	O
that	O
a	O
given	O
answer	O
passage	O
describes	O
a	O
proper	O
answer	O
to	O
a	O
given	O
why	O
-	O
question	O
using	O
the	O
representations	O
of	O
a	O
question	O
,	O
an	O
answer	O
passage	O
,	O
and	O
a	O
compact	O
answer	O
.	O
The	O
probability	O
(	O
the	O
why	O
-	O
QA	O
model	O
's	O
final	O
output	O
)	O
is	O
computed	O
from	O
these	O
representations	O
by	O
our	O
answer	B-TaskName
selection	I-TaskName
module	O
,	O
which	O
is	O
a	O
logistic	B-MethodName
regression	I-MethodName
layer	O
with	O
dropout	O
and	O
softmax	B-MethodName
output	O
.	O
The	O
representations	O
of	O
why	O
-	O
questions	O
and	O
answer	O
passages	O
are	O
generated	O
by	O
Convolutional	O
Neural	O
Networks	O
(	O
CNNs	O
)	O
(	O
Collobert	O
et	O
al	O
,	O
2011	O
;	O
LeCun	O
et	O
al	O
,	O
1998	O
)	O
that	O
(	O
1	O
)	O
are	O
augmented	O
by	O
two	O
types	O
of	O
attention	O
mechanisms	O
,	O
similarityattention	O
and	O
causality	O
-	O
attention	O
,	O
and	O
(	O
2	O
)	O
are	O
given	O
two	O
types	O
of	O
word	B-TaskName
embeddings	I-TaskName
,	O
general	O
word	B-TaskName
embeddings	I-TaskName
computed	O
by	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
using	O
Wikipedia	O
and	O
causal	O
word	B-TaskName
embeddings	I-TaskName
(	O
Sharp	O
et	O
al	O
,	O
2016	O
)	O
.	O
Note	O
that	O
in	O
computing	O
a	O
question	O
's	O
representation	O
,	O
the	O
answer	O
passage	O
is	O
given	O
to	O
the	O
question	O
encoder	O
to	O
guide	O
the	O
computation	O
.	O
Likewise	O
the	O
passage	O
encoder	O
is	O
given	O
the	O
question	O
and	O
the	O
representation	O
of	O
the	O
compact	O
answer	O
.	O
We	O
represent	O
these	O
information	O
flows	O
with	O
dotted	O
arrows	O
in	O
Fig	O
.	O
1	O
(	O
a	O
)	O
.	O
The	O
representations	O
of	O
compact	O
answers	O
are	O
created	O
by	O
a	O
generator	O
network	O
called	O
a	O
fakerepresentation	O
generator	O
(	O
F	O
in	O
Fig	O
.	O
1	O
(	O
a	O
)	O
)	O
,	O
which	O
is	O
pre	O
-	O
trained	O
in	O
an	O
adversarial	O
learning	O
manner	O
(	O
Fig	O
.	O
1	O
(	O
b	O
)	O
)	O
.	O
During	O
the	O
training	O
of	O
the	O
whole	O
why	O
-	O
QA	O
model	O
,	O
the	O
generator	O
's	O
parameters	O
are	O
fixed	O
and	O
no	O
further	O
fine	O
-	O
tuning	O
is	O
conducted	O
.	O
In	O
the	O
next	O
section	O
,	O
we	O
describe	O
our	O
main	O
contribution	O
:	O
the	O
AGR	O
and	O
the	O
fake	O
-	O
representation	O
generator	O
.	O
The	O
entire	O
why	O
-	O
QA	O
model	O
can	O
be	O
seen	O
as	O
an	O
extension	O
of	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
why	O
-	O
QA	O
method	O
.	O
Its	O
details	O
are	O
described	O
in	O
Section	O
A	O
of	O
the	O
supplementary	O
materials	O
.	O

In	O
our	O
implementation	O
,	O
both	O
F	O
and	O
R	O
are	O
networks	O
with	O
identical	O
structure	O
called	O
Encoder	O
.	O
They	O
are	O
defined	O
as	O
follows	O
,	O
where	O
p	O
,	O
c	O
,	O
and	O
q	O
are	O
respectively	O
an	O
answer	O
passage	O
,	O
a	O
manually	O
created	O
compact	O
-	O
answer	O
,	O
and	O
a	O
why	O
-	O
question	O
:	O
F	O
(	O
p	O
|	O
q	O
)	O
=	O
Encoder	O
(	O
p	O
;	O
θ	B-HyperparameterName
F	O
,	O
q	O
)	O
R	O
(	O
c	O
|	O
q	O
)	O
=	O
Encoder	O
(	O
c	O
;	O
θ	B-HyperparameterName
R	O
,	O
q	O
)	O
Here	O
θ	B-HyperparameterName
F	O
and	O
θ	B-HyperparameterName
R	O
represent	O
the	O
parameters	O
of	O
networks	O
F	O
and	O
R.	O
The	O
details	O
of	O
Encoder	O
are	O
described	O
below	O
.	O
Discriminator	O
D	O
(	O
r	O
)	O
takes	O
as	O
input	O
r	O
,	O
either	O
the	O
output	O
of	O
F	O
(	O
p	O
|	O
q	O
)	O
or	O
that	O
of	O
R	O
(	O
c	O
|	O
q	O
)	O
,	O
and	O
computes	O
the	O
probability	O
that	O
given	O
representation	O
r	O
comes	O
from	O
a	O
real	O
compact	O
-	O
answer	O
using	O
a	O
feedforward	B-MethodName
network	I-MethodName
with	O
two	O
hidden	O
layers	O
(	O
100	O
nodes	O
in	O
the	O
first	O
layer	O
and	O
50	O
in	O
the	O
second	O
layer	O
)	O
and	O
a	O
logistic	B-MethodName
regression	I-MethodName
layer	O
on	O
top	O
of	O
the	O
hidden	O
layers	O
.	O
We	O
used	O
sigmoid	O
outputs	O
by	O
the	O
logistic	B-MethodName
regression	I-MethodName
layer	O
as	O
the	O
output	O
probability	O
.	O

Figure	O
2	O
illustrates	O
the	O
architecture	O
shared	O
by	O
our	O
fake	O
-	O
representation	O
generator	O
F	O
and	O
real	O
-	O
representation	O
generator	O
R	O
,	O
namely	O
,	O
Encoder	O
(	O
t	O
;	O
θ	B-HyperparameterName
,	O
q	O
)	O
,	O
where	O
θ	B-HyperparameterName
is	O
a	O
set	O
of	O
parameters	O
,	O
q	O
is	O
a	O
why	O
-	O
question	O
,	O
and	O
t	O
is	O
either	O
an	O
answer	O
passage	O
or	O
a	O
manually	O
created	O
compact	O
-	O
answer	O
.	O
Encoder	O
(	O
t	O
;	O
θ	B-HyperparameterName
,	O
q	O
)	O
first	O
represents	O
question	O
q	O
and	O
passage	O
/	O
compact	O
-	O
answer	O
t	O
with	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
,	O
which	O
are	O
supplemented	O
with	O
attention	O
mechanisms	O
.	O
The	O
resulting	O
attention	O
-	O
weighted	O
word	B-TaskName
embeddings	I-TaskName
are	O
given	O
to	O
convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
that	O
generate	O
a	O
single	O
feature	O
vector	O
,	O
which	O
is	O
an	O
output	O
/	O
value	O
of	O
Encoder	O
(	O
t	O
;	O
θ	B-HyperparameterName
,	O
q	O
)	O
.	O
In	O
the	O
following	O
,	O
we	O
give	O
an	O
overview	O
of	O
the	O
word	B-TaskName
embeddings	I-TaskName
,	O
the	O
attention	O
mechanisms	O
,	O
and	O
the	O
CNNs	O
used	O
in	O
Encoder	O
(	O
t	O
;	O
θ	B-HyperparameterName
,	O
q	O
)	O
.	O
All	O
of	O
these	O
techniques	O
were	O
proposed	O
by	O
previous	O
works	O
.	O
Further	O
details	O
are	O
given	O
in	O
Section	O
B	O
of	O
the	O
supplementary	O
materials	O
.	O

The	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
used	O
in	O
Encoder	O
(	O
t	O
;	O
θ	B-HyperparameterName
,	O
q	O
)	O
were	O
obtained	O
by	O
concatenating	O
two	O
types	O
of	O
d	O
-	O
dimensional	O
word	B-TaskName
embeddings	I-TaskName
(	O
d	O
=	O
300	O
in	O
this	O
work	O
)	O
:	O
general	O
word	B-TaskName
embeddings	I-TaskName
and	O
causal	O
word	B-TaskName
embeddings	I-TaskName
.	O
General	B-DatasetName
word	B-TaskName
embeddings	I-TaskName
are	O
widely	O
used	O
embedding	O
vectors	O
(	O
300	O
dimensions	O
)	O
that	O
were	O
pretrained	O
for	O
about	O
1.65	O
million	O
words	O
by	O
applying	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
to	O
about	O
35	O
million	O
sentences	O
from	O
Japanese	O
Wikipedia	O
(	O
January	O
2015	O
version	O
)	O
.	O
Causal	O
word	B-TaskName
embeddings	I-TaskName
(	O
Sharp	O
et	O
al	O
,	O
2016	O
)	O
were	O
proposed	O
for	O
representing	O
the	O
causal	O
associations	O
between	O
words	O
.	O
Sharp	O
et	O
al	O
(	O
2016	O
)	O
cre	O
-	O
ated	O
a	O
set	O
of	O
cause	O
-	O
effect	O
word	O
pairs	O
by	O
paring	O
each	O
content	O
word	O
in	O
a	O
cause	O
part	O
with	O
each	O
content	O
word	O
in	O
an	O
effect	O
part	O
of	O
the	O
same	O
causality	O
expression	O
,	O
such	O
as	O
"	O
Volcanoes	O
erupt	O
because	O
magma	O
pushes	O
through	O
vents	O
and	O
fissures	O
.	O
"	O
In	O
this	O
work	O
,	O
we	O
extracted	O
100	O
million	O
causality	O
expressions	O
from	O
4	O
-	O
billion	O
Japanese	O
web	O
pages	O
using	O
the	O
causality	O
recognizer	O
of	O
Oh	O
et	O
al	O
(	O
2013	O
)	O
.	O
Then	O
,	O
following	O
Sharp	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
trained	O
300dimensional	O
causal	O
word	B-TaskName
embeddings	I-TaskName
for	O
about	O
1.85	O
million	O
words	O
by	O
applying	O
the	O
generalized	O
skip	O
-	O
gram	O
embedding	O
model	O
of	O
Levy	O
and	O
Goldberg	O
(	O
2014	O
)	O
to	O
the	O
causality	O
expressions	O
.	O

att	O
is	O
given	O
to	O
CNNs	O
to	O
generate	O
final	O
representation	O
r	O
t	O
of	O
a	O
given	O
passage	O
/	O
compact	O
-	O
answer	O
t.	O
The	O
CNNs	O
resembles	O
those	O
in	O
Kim	O
(	O
2014	O
)	O
.	O
Convolutions	O
are	O
performed	O
over	O
the	O
word	B-TaskName
embeddings	I-TaskName
using	O
both	O
multiple	O
filters	O
and	O
multiple	O
filter	O
windows	O
(	O
e.g.	O
,	O
sliding	O
over	O
1	O
,	O
2	O
,	O
or	O
3	O
word	O
windows	O
at	O
a	O
time	O
and	O
100	O
filters	O
for	O
each	O
window	O
)	O
.	O
An	O
average	B-MethodName
pooling	I-MethodName
operation	O
is	O
applied	O
to	O
the	O
convolution	B-MethodName
results	O
to	O
generate	O
representation	O
r	O
t	O
,	O
which	O
is	O
the	O
output	O
/	O
value	O
of	O
Encoder	O
(	O
t	O
;	O
θ	B-HyperparameterName
,	O
q	O
)	O
;	O
r	O
t	O
=	O
Encoder	O
(	O
t	O
;	O
θ	B-HyperparameterName
,	O
q	O
)	O
.	O
In	O
our	O
experiments	O
,	O
we	O
set	O
the	O
dimension	O
of	O
representation	O
r	O
t	O
to	O
300	O
.	O
4	O
Why	O
-	O
QA	O
Experiments	O

In	O
our	O
proposed	O
methods	O
and	O
their	O
variants	O
,	O
all	O
the	O
weights	O
in	O
the	O
CNNs	O
were	O
initialized	O
using	O
He	O
's	O
method	O
(	O
He	O
et	O
al	O
,	O
2015	O
)	O
,	O
and	O
the	O
other	O
weights	O
in	O
our	O
why	O
-	O
QA	O
model	O
were	O
initialized	O
randomly	O
with	O
a	O
uniform	O
distribution	O
in	O
the	O
range	O
of	O
(	O
-	O
0.01	O
,	O
0.01	O
)	O
.	O
For	O
the	O
CNN	O
-	O
based	O
components	O
,	O
we	O
set	O
the	O
window	O
size	O
of	O
the	O
filters	O
to	O
"	O
1	O
,	O
2	O
,	O
3	O
"	O
with	O
100	O
filters	O
each	O
2	O
.	O
We	O
used	O
dropout	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
with	O
probability	O
0.5	O
on	O
the	O
final	O
logistic	B-MethodName
regression	I-MethodName
layer	O
.	O
All	O
of	O
these	O
hyper	O
-	O
parameters	O
were	O
chosen	O
with	O
our	O
development	O
data	O
.	O
We	O
optimized	O
the	O
learned	O
parameters	O
with	O
the	O
Adam	B-MethodName
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
was	O
set	O
to	O
0.001	O
,	O
and	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
for	O
each	O
iteration	O
was	O
set	O
to	O
20	O
.	O

Same	O
as	O
BERT+FOP	O
except	O
that	O
it	O
used	O
FRV	O
instead	O
of	O
FOP	O
for	O
producing	O
compact	O
-	O
answer	O
representation	O
.	O
To	O
pre	O
-	O
train	O
the	O
BERT	B-MethodName
-	O
based	O
models	O
,	O
we	O
used	O
a	O
combination	O
of	O
sentences	O
extracted	O
from	O
Japanese	O
Wikipedia	O
articles	O
(	O
August	O
2018	O
version	O
)	O
and	O
causality	O
expressions	O
automatically	O
recognized	O
from	O
a	O
causality	O
recognizer	O
(	O
Oh	O
et	O
al	O
,	O
2013	O
)	O
.	O
This	O
data	O
mix	O
consists	O
of	O
75	O
%	O
of	O
sentences	O
extracted	O
from	O
Wikipedia	O
(	O
14	O
,	O
675	O
,	O
535	O
sentences	O
taken	O
out	O
of	O
784	O
,	O
869	O
articles	O
randomly	O
sampled	O
)	O
and	O
25	O
%	O
of	O
cause	O
and	O
effect	O
phrases	O
taken	O
from	O
causality	O
expressions	O
(	O
4	O
,	O
891	O
,	O
846	O
phrases	O
from	O
2	O
,	O
445	O
,	O
923	O
causal	O
relations	O
)	O
.	O
This	O
ratio	O
was	O
determined	O
through	O
preliminary	O
experiments	O
using	O
the	O
development	O
data	O
.	O
For	O
the	O
pre	O
-	O
training	O
parameters	O
,	O
we	O
followed	O
the	O
settings	O
of	O
BERT	B-MethodName
BASE	B-MethodName
in	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
3	O
except	O
for	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
50	O
.	O
We	O
ran	O
3	O
epochs	O
with	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	O
-	O
5	O
for	O
finetuning	O
the	O
BERT	B-MethodName
-	O
based	O
models	O
4	O
.	O
+	O
+	O
+	O
+	O
+	O
+	O
E	O
[	O
CLS	O
]	O
E	O
'	O
E	O
′	O
)	O
E	O
Q	O
E	O
Q	O
E	O
P	O
E	O
0	B-DatasetName
E	O
1	O
E	O
N+M+1	O
E	O
[	O
A	O
BERT	B-MethodName
-	O
based	O
model	O
,	O
BERT	B-MethodName
,	O
takes	O
a	O
questionpassage	O
pair	O
as	O
input	O
and	O
computes	O
the	O
input	O
representation	O
using	O
token	O
,	O
segment	O
,	O
position	O
,	O
and	O
attention	O
feature	O
embeddings	O
(	O
Fig	O
.	O
3	O
)	O
.	O
For	O
the	O
input	O
representation	O
computation	O
,	O
the	O
original	O
BERT	B-MethodName
only	O
used	O
the	O
token	O
,	O
segment	O
,	O
and	O
position	O
embeddings	O
,	O
while	O
BERT	B-MethodName
additionally	O
used	O
the	O
attention	O
feature	O
embeddings	O
5	O
to	O
exploit	O
the	O
same	O
similarity	O
-	O
attention	O
and	O
causalityattention	O
features	O
used	O
in	O
our	O
proposed	O
method	O
.	O
We	O
used	O
the	O
attention	O
feature	O
embeddings	O
during	O
the	O
fine	O
-	O
tuning	O
and	O
testing	O
,	O
but	O
not	O
during	O
the	O
pretraining	O
of	O
the	O
BERT	B-MethodName
-	O
based	O
model	O
.	O
The	O
attention	O
feature	O
embeddings	O
for	O
answer	O
passages	O
(	O
i.e.	O
,	O
E	O
sim	O
w	O
1	O
,	O
,	O
E	O
sim	O
w	O
M	O
,	O
and	O
E	O
caus	O
w	O
1	O
,	O
,	O
E	O
caus	O
w	O
M	O
)	O
were	O
computed	O
from	O
the	O
same	O
attention	O
feature	O
vectors	O
,	O
a	O
s	O
and	O
a	O
c	O
,	O
as	O
those	O
in	O
our	O
proposed	O
methods	O
;	O
those	O
for	O
the	O
other	O
parts	O
(	O
i.e.	O
,	O
questions	O
,	O
[	O
CLS	O
]	O
,	O
and	O
[	O
SEP	O
]	O
)	O
were	O
computed	O
from	O
a	O
zero	O
vector	O
(	O
indicating	O
no	O
attention	O
feature	O
)	O
.	O
The	O
transformer	O
encoder	O
processed	O
the	O
input	O
representation	O
to	O
gen	O
-	O
3	O
12	O
-	O
layers	O
,	O
768	O
hidden	O
states	O
,	O
12	O
heads	O
and	O
training	O
for	O
1	O
-	O
million	O
steps	O
with	O
the	O
warmup	O
rate	O
of	O
1	O
%	O
using	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	O
-	O
4	O
.	O
4	O
We	O
tested	O
all	O
the	O
combinations	O
of	O
epochs	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
}	O
and	O
learning	O
rates	O
of	O
{	O
1e	O
-	O
5	O
,	O
2e	O
-	O
5	O
,	O
3e	O
-	O
5	O
}	O
and	O
chose	O
the	O
one	O
that	O
maximized	O
the	O
performance	O
on	O
the	O
development	O
data	O
in	O
W	O
hySet	O
.	O
5	O
We	O
also	O
evaluated	O
a	O
BERT	B-MethodName
-	O
based	O
model	O
that	O
did	O
not	O
use	O
the	O
attention	O
feature	O
embeddings	O
,	O
but	O
its	O
P@1	B-MetricName
(	O
41.4	O
)	O
was	O
much	O
lower	O
than	O
that	O
of	O
BERT	B-MethodName
(	O
51.2	O
)	O
.	O
P@1	B-MetricName
MAP	B-DatasetName
Oh	O
et	O
al	O
(	O
2013	O
)	O
41.8	O
41.0	O
Sharp	O
et	O
al	O
(	O
2016	O
)	O

Table	O
3	O
shows	O
the	O
performances	O
of	O
all	O
the	O
methods	O
in	O
the	O
Precision	B-MetricName
of	O
the	O
top	O
answer	O
(	O
P@1	B-MetricName
)	O
and	O
the	O
Mean	O
Average	B-MetricName
Precision	I-MetricName
(	O
MAP	B-DatasetName
)	O
(	O
Oh	O
et	O
al	O
,	O
2013	O
)	O
.	O
Note	O
that	O
the	O
Oracle	O
method	O
indicates	O
the	O
performance	O
of	O
a	O
fictional	O
method	O
that	O
ranks	O
the	O
answer	O
passages	O
perfectly	O
,	O
i.e.	O
,	O
it	O
locates	O
all	O
the	O
m	O
correct	O
answers	O
to	O
a	O
question	O
in	O
the	O
top	O
-	O
m	O
ranks	O
,	O
based	O
on	O
the	O
gold	O
-	O
standard	O
labels	O
.	O
This	O
performance	O
is	O
the	O
upper	O
bound	O
of	O
those	O
of	O
all	O
the	O
implementable	O
methods	O
.	O
Our	O
proposed	O
method	O
,	O
Ours	O
(	O
OP	O
)	O
,	O
outperformed	O
all	O
the	O
other	O
methods	O
.	O
Our	O
starting	O
point	O
,	O
i.e.	O
,	O
BASE	B-MethodName
,	O
was	O
already	O
superior	O
to	O
the	O
methods	O
in	O
the	O
previous	O
works	O
.	O
Compared	O
with	O
BASE	B-MethodName
and	O
BASE+AddTr	O
,	O
neither	O
of	O
which	O
used	O
compactanswer	O
representations	O
or	O
fake	O
-	O
representation	O
generator	O
F	O
,	O
Ours	O
(	O
OP	O
)	O
gave	O
3.4	O
%	O
and	O
2.8	O
%	O
improvement	O
in	O
P@1	B-MetricName
,	O
respectively	O
.	O
It	O
also	O
outperformed	O
BASE+CAns	O
and	O
BASE+CEnc	O
,	O
which	O
generated	O
compact	O
-	O
answer	O
representations	O
in	O
a	O
way	O
different	O
from	O
the	O
proposed	O
method	O
,	O
and	O
BASE+Enc	O
,	O
which	O
trained	O
the	O
fake	O
-	O
representation	O
generator	O
without	O
adversarial	O
learning	O
.	O
These	O
performance	O
differences	O
were	O
statistically	O
significant	O
(	O
p	O
<	O
0.01	O
by	O
the	O
McNemar	O
's	O
test	O
)	O
.	O
Ours	O
(	O
OP	O
)	O
also	O
outperformed	O
all	O
the	O
BERTbased	O
models	O
but	O
an	O
interesting	O
point	O
is	O
that	O
fakerepresentation	O
generator	O
F	O
boosted	O
the	O
performance	O
of	O
the	O
BERT	B-MethodName
-	O
based	O
models	O
(	O
statistically	O
significant	O
with	O
p	O
<	O
0.01	O
by	O
the	O
McNemar	O
's	O
test	O
)	O
.	O
These	O
results	O
suggest	O
that	O
AGR	O
is	O
effective	O
in	O
both	O
our	O
why	O
-	O
QA	O
model	O
and	O
our	O
BERT	B-MethodName
-	O
based	O
model	O
.	O

Another	O
interesting	O
point	O
is	O
that	O
Ours	O
(	O
RV	O
)	O
,	O
in	O
which	O
fake	O
-	O
representation	O
generator	O
F	O
RV	O
was	O
trained	O
using	O
random	O
vectors	O
,	O
achieved	O
almost	O
the	O
same	O
performance	O
as	O
that	O
of	O
Ours	O
(	O
OP	O
)	O
.	O
This	O
result	O
was	O
puzzling	O
,	O
so	O
we	O
first	O
checked	O
whether	O
F	O
RV	O
's	O
output	O
was	O
not	O
just	O
random	O
noise	O
(	O
which	O
could	O
prevent	O
the	O
why	O
-	O
QA	O
model	O
from	O
overfitting	O
)	O
by	O
replacing	O
in	O
Ours	O
(	O
RV	O
)	O
the	O
output	O
of	O
F	O
RV	O
by	O
random	O
vectors	O
.	O
Although	O
we	O
sampled	O
the	O
random	O
vectors	O
from	O
different	O
distribution	O
types	O
with	O
various	O
ranges	O
,	O
we	O
obtained	O
at	O
best	O
similar	O
performance	O
to	O
that	O
of	O
BASE	B-MethodName
:	O
51.6	O
in	O
P@1	B-MetricName
.	O
This	O
result	O
confirms	O
that	O
it	O
is	O
not	O
trivial	O
to	O
mimic	O
F	O
RV	O
using	O
random	O
vectors	O
at	O
least	O
.	O
We	O
investigated	O
the	O
F	O
RV	O
's	O
output	O
to	O
check	O
whether	O
it	O
actually	O
focused	O
on	O
the	O
compact	O
answer	O
in	O
a	O
given	O
passage	O
.	O
We	O
computed	O
the	O
following	O
three	O
representation	O
sets	O
from	O
a	O
gold	O
set	O
of	O
3	O
,	O
608	O
triples	O
of	O
why	O
-	O
questions	O
,	O
answer	O
passages	O
and	O
manually	O
created	O
compact	O
-	O
answers	O
that	O
do	O
not	O
overlap	O
with	O
CmpAns	O
:	O
{	O
r	O
org	O
i	O
}	O
:	O
F	O
RV	O
's	O
output	O
with	O
the	O
pairs	O
of	O
a	O
whyquestion	O
and	O
an	O
answer	O
passage	O
in	O
the	O
gold	O
set	O
as	O
its	O
input	O
;	O
{	O
r	O
in	O
i	O
}	O
:	O
F	O
RV	O
's	O
output	O
for	O
the	O
same	O
input	O
as	O
{	O
r	O
org	O
i	O
}	O
,	O
where	O
we	O
replaced	O
the	O
word	B-TaskName
embeddings	I-TaskName
of	O
all	O
the	O
content	O
words	O
in	O
the	O
answer	O
passages	O
that	O
also	O
appeared	O
in	O
the	O
associated	O
gold	O
compact	O
-	O
answers	O
with	O
random	O
vectors	O
;	O
{	O
r	O
out	O
i	O
}	O
:	O
F	O
RV	O
's	O
output	O
for	O
the	O
same	O
input	O
as	O
{	O
r	O
org	O
i	O
}	O
,	O
where	O
we	O
replaced	O
the	O
word	B-TaskName
embeddings	I-TaskName
of	O
all	O
the	O
content	O
words	O
in	O
the	O
answer	O
passages	O
that	O
did	O
not	O
appear	O
in	O
the	O
associated	O
gold	O
compact	O
-	O
answers	O
with	O
random	O
vectors	O
6	O
.	O
If	O
F	O
RV	O
perfectly	O
focuses	O
on	O
the	O
gold	O
standard	O
compact	O
-	O
answers	O
,	O
for	O
each	O
question	O
-	O
passage	O
pair	O
,	O
6	O
For	O
both	O
r	O
in	O
i	O
and	O
r	O
out	O
i	O
,	O
we	O
never	O
replaced	O
the	O
word	B-TaskName
embeddings	I-TaskName
for	O
the	O
words	O
that	O
also	O
appeared	O
in	O
the	O
question	O
.	O
r	O
out	O
i	O
should	O
be	O
the	O
same	O
as	O
r	O
org	O
i	O
and	O
r	O
in	O
i	O
should	O
significantly	O
differ	O
from	O
r	O
org	O
i	O
.	O
Next	O
we	O
computed	O
the	O
average	O
Euclidian	O
distance	O
among	O
{	O
r	O
org	O
i	O
}	O
,	O
{	O
r	O
in	O
i	O
}	O
and	O
{	O
r	O
out	O
i	O
}	O
.	O
The	O
average	O
distance	O
(	O
2.67	O
)	O
between	O
{	O
r	O
org	O
i	O
}	O
and	O
{	O
r	O
out	O
i	O
}	O
was	O
much	O
smaller	O
than	O
the	O
average	O
distance	O
(	O
13.3	O
)	O
between	O
{	O
r	O
org	O
i	O
}	O
and	O
{	O
r	O
in	O
i	O
}	O
.	O
Note	O
that	O
we	O
replaced	O
the	O
word	B-TaskName
embeddings	I-TaskName
for	O
much	O
more	O
words	O
with	O
random	O
vectors	O
in	O
the	O
computation	O
of	O
{	O
r	O
out	O
i	O
}	O
than	O
those	O
in	O
the	O
computation	O
of	O
{	O
r	O
in	O
i	O
}	O
(	O
38.1	O
words	O
vs.	O
5.6	O
words	O
)	O
.	O
This	O
implies	O
that	O
the	O
distance	O
between	O
{	O
r	O
org	O
i	O
}	O
and	O
{	O
r	O
out	O
i	O
}	O
might	O
be	O
much	O
larger	O
than	O
that	O
between	O
{	O
r	O
org	O
i	O
}	O
and	O
{	O
r	O
in	O
i	O
}	O
if	O
F	O
RV	O
focused	O
equally	O
on	O
every	O
answer	O
passage	O
word	O
.	O
However	O
,	O
the	O
actual	O
results	O
suggest	O
that	O
this	O
is	O
not	O
the	O
case	O
.	O
Although	O
we	O
can	O
not	O
draw	O
decisive	O
conclusions	O
due	O
to	O
the	O
complex	O
nature	O
of	O
neural	O
networks	O
,	O
we	O
believe	O
from	O
the	O
results	O
that	O
F	O
RV	O
does	O
actually	O
focus	O
more	O
on	O
words	O
that	O
are	O
a	O
part	O
of	O
a	O
compact	O
answer	O
than	O
on	O
other	O
words	O
.	O
We	O
also	O
computed	O
{	O
r	O
org	O
i	O
}	O
,	O
{	O
r	O
in	O
i	O
}	O
,	O
and	O
{	O
r	O
out	O
i	O
}	O
with	O
fakerepresentation	O
generator	O
F	O
OP	O
in	O
the	O
same	O
way	O
and	O
observed	O
the	O
same	O
tendency	O
.	O

We	O
tested	O
our	O
framework	O
on	O
another	O
task	O
,	O
the	O
distantly	O
supervised	O
open	B-TaskName
-	I-TaskName
domain	I-TaskName
question	I-TaskName
answering	I-TaskName
(	O
DS	O
-	O
QA	O
)	O
task	O
(	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
,	O
to	O
check	O
its	O
generalizability	O
.	O
Table	O
4	O
shows	O
the	O
statistics	O
for	O
the	O
datasets	O
used	O
in	O
this	O
experiment	O
.	O
The	O
first	O
three	O
,	O
Quasar	B-DatasetName
-	I-DatasetName
T	I-DatasetName
,	O
SearchQA	B-DatasetName
,	O
and	O
TriviaQA	B-DatasetName
provided	O
by	O
Lin	O
et	O
al	O
(	O
2018	O
)	O
,	O
were	O
used	O
for	O
training	O
and	O
evaluating	O
DS	O
-	O
QA	O
methods	O
.	O
The	O
training	O
data	O
of	O
SQuAD	B-DatasetName
v1.1	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
was	O
used	O
for	O
training	O
our	O
AGR	O
.	O
The	O
SQuAD	B-DatasetName
dataset	O
consisted	O
of	O
the	O
triples	O
of	O
a	O
question	O
,	O
an	O
answer	O
,	O
and	O
a	O
paragraph	O
that	O
includes	O
the	O
answer	O
.	O
We	O
assume	O
that	O
the	O
answers	O
are	O
our	O
compact	O
answers	O
,	O
although	O
the	O
answers	O
in	O
the	O
dataset	O
are	O
consecutive	O
short	O
word	O
sequences	O
(	O
2.8	O
words	O
on	O
average	O
)	O
,	O
whose	O
majority	O
are	O
noun	O
phrases	O
,	O
unlike	O
the	O
compact	O
answers	O
for	O
our	O
why	O
-	O
QA	O
experiment	O
,	O
i.e.	O
,	O
sentences	O
or	O
phrases	O
(	O
8.3	O
words	O
on	O
average	O
)	O
.	O
We	O
trained	O
our	O
AGR	O
with	O
all	O
the	O
triples	O
of	O
a	O
question	O
,	O
an	O
answer	O
,	O
and	O
a	O
paragraph	O
in	O
the	O
training	O
data	O
of	O
SQuAD	B-DatasetName
-	O
v1.1	O
under	O
the	O
same	O
settings	O
for	O
the	O
AGR	O
's	O
hyperparameters	O
as	O
in	O
our	O
why	O
-	O
QA	O
experiment	O
except	O
that	O
we	O
use	O
neither	O
causal	O
word	B-TaskName
embeddings	I-TaskName
nor	O
causality	O
-	O
attention	O
.	O
In	O
this	O
experiment	O
,	O
we	O
used	O
the	O
AGR	O
training	O
schemes	O
for	O
Ours	O
(	O
OP	O
)	O
and	O
Ours	O
(	O
RV	O
)	O
.	O
We	O
used	O
the	O
300	O
-	O
dimensional	O
GloVe	B-MethodName
word	B-TaskName
embeddings	I-TaskName
learned	O
from	O
840	O
billion	O
tokens	O
in	O
the	O
web	O
crawl	O
data	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
as	O
general	O
word	B-TaskName
embeddings	I-TaskName
.	O
Then	O
we	O
combined	O
the	O
resulting	O
fake	O
-	O
representation	O
generator	O
F	O
in	O
the	O
AGR	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
DS	O
-	O
QA	O
method	O
,	O
OpenQA	O
(	O
Lin	O
et	O
al	O
,	O
2018	O
)	O
7	O
.	O
We	O
also	O
used	O
the	O
hyperparameters	O
presented	O
in	O
Lin	O
et	O
al	O
(	O
2018	O
)	O
.	O
OpenQA	O
is	O
composed	O
of	O
two	O
components	O
:	O
a	O
paragraph	O
selector	O
to	O
choose	O
relevant	O
paragraphs	O
(	O
or	O
answer	O
passages	O
in	O
our	O
terms	O
)	O
from	O
a	O
set	O
of	O
paragraphs	O
and	O
a	O
paragraph	O
reader	O
to	O
extract	O
answers	O
from	O
the	O
selected	O
paragraphs	O
.	O
For	O
identifying	O
answer	O
a	O
to	O
given	O
question	O
q	O
from	O
set	O
of	O
paragraphs	O
P	O
=	O
{	O
p	O
i	O
}	O
,	O
the	O
paragraph	O
selector	O
and	O
the	O
paragraph	O
reader	O
respectively	O
compute	O
probabilities	O
P	O
r	O
(	O
p	O
i	O
|	O
q	O
,	O
P	O
)	O
and	O
P	O
r	O
(	O
a	O
|	O
q	O
,	O
p	O
i	O
)	O
,	O
and	O
final	O
output	O
P	O
r	O
(	O
a	O
|	O
q	O
,	O
P	O
)	O
is	O
obtained	O
by	O
combining	O
the	O
probabilities	O
.	O
We	O
introduced	O
c	O
i	O
,	O
which	O
is	O
a	O
compact	O
-	O
answer	O
representation	O
generated	O
by	O
fakerepresentation	O
generator	O
F	O
with	O
question	O
q	O
and	O
paragraph	O
p	O
i	O
as	O
its	O
input	O
,	O
to	O
the	O
computation	O
of	O
the	O
probabilities	O
as	O
follows	O
:	O
P	O
r	O
(	O
a	O
|	O
q	O
,	O
P	O
,	O
C	O
)	O
=	O
i	O
P	O
r	O
(	O
a	O
|	O
q	O
,	O
p	O
i	O
,	O
c	O
i	O
)	O
P	O
r	O
(	O
p	O
i	O
|	O
q	O
,	O
P	O
,	O
c	O
i	O
)	O
In	O
the	O
original	O
OpenQA	O
,	O
the	O
paragraph	O
selector	O
and	O
the	O
reader	O
use	O
bidirectional	O
stacked	O
RNNs	O
for	O
encoding	O
paragraphs	O
,	O
where	O
word	B-TaskName
embeddings	I-TaskName
p	O
i	O
of	O
a	O
paragraph	O
is	O
used	O
as	O
the	O
input	O
.	O
In	O
our	O
implementation	O
,	O
we	O
computed	O
attention	O
-	O
weighted	O
embeddingp	O
i	O
of	O
a	O
paragraph	O
by	O
using	O
compactanswer	O
representation	O
c	O
i	O
.	O
Given	O
word	O
embedding	O
p	O
j	O
i	O
for	O
the	O
j	O
-	O
th	O
word	O
in	O
paragraph	O
p	O
i	O
,	O
its	O
attentionweighted	O
embeddingp	O
j	O
i	O
was	O
computed	O
by	O
using	O
a	O
bilinear	O
function	O
(	O
Sutskever	O
et	O
al	O
,	O
2009	O
)	O
:	O
p	O
j	O
i	O
=	O
softmax	B-MethodName
j	O
(	O
p	O
T	O
i	O
Mc	O
i	O
)	O
p	O
j	O
i	O
,	O
where	O
M	O
R	O
d×d	O
is	O
a	O
trainable	O
matrix	O
,	O
softmax	B-MethodName
j	O
(	O
x	O
)	O
denotes	O
the	O
j	O
-	O
th	O
element	O
of	O
the	O
softmaxed	O
vector	O
of	O
x	O
,	O
and	O
d	O
=	O
300	O
.	O
We	O
gave	O
[	O
p	O
j	O
i	O
;	O
p	O
j	O
i	O
]	O
,	O
a	O
concatenation	O
of	O
p	O
j	O
i	O
andp	O
j	O
i	O
,	O
as	O
the	O
word	O
embedding	O
of	O
the	O
j	O
-	O
th	O
word	O
in	O
paragraph	O
p	O
i	O
to	O
the	O
bidirectional	O
stacked	O
RNNs	O
.	O
Table	O
5	O
shows	O
the	O
performances	O
of	O
the	O
four	O
DS	O
-	O
QA	O
methods	O
:	O
R	O
3	O
(	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
,	O
OpenQA	O
(	O
Lin	O
et	O
al	O
,	O
2018	O
)	O
,	O
Ours	O
(	O
OP	O
)	O
,	O
and	O
Ours	O
(	O
RV	O
)	O
evaluated	O
against	O
the	O
Quasar	B-DatasetName
-	I-DatasetName
T	I-DatasetName
,	O
SearchQA	B-DatasetName
and	O
TriviaQA	B-DatasetName
datasets	O
.	O
All	O
the	O
methods	O
were	O
evaluated	O
with	O
EM	B-MetricName
and	O
F1	B-MetricName
scores	O
,	O
following	O
Lin	O
et	O
al	O
(	O
2018	O
)	O
.	O
EM	B-MetricName
measures	O
the	O
percentage	O
of	O
predictions	O
that	O
exactly	O
match	O
one	O
of	O
the	O
ground	O
-	O
truth	O
answers	O
and	O
F1	B-MetricName
is	O
a	O
metric	O
that	O
loosely	O
measures	O
the	O
average	O
overlap	O
between	O
the	O
prediction	O
and	O
ground	O
-	O
truth	O
answer	O
.	O
Note	O
that	O
both	O
Ours	O
(	O
OP	O
)	O
and	O
Ours	O
(	O
RV	O
)	O
outperformed	O
both	O
previous	O
methods	O
,	O
R	O
3	O
and	O
OpenQA	O
,	O
except	O
for	O
the	O
F1	B-MetricName
score	I-MetricName
for	O
the	O
TriviaQA	B-DatasetName
dataset	O
.	O
Some	O
of	O
the	O
improvements	O
over	O
the	O
previous	O
state	O
-	O
ofthe	O
-	O
art	O
method	O
,	O
OpenQA	O
,	O
were	O
statistically	O
significant	O
.	O
These	O
findings	O
suggest	O
that	O
our	O
framework	O
can	O
be	O
effective	O
for	O
tasks	O
other	O
than	O
the	O
original	O
why	O
-	O
QA	O
and	O
the	O
other	O
datasets	O
.	O

Along	O
with	O
the	O
flourishing	O
development	O
of	O
neural	O
networks	O
,	O
the	O
sequence	O
-	O
to	O
-	O
sequence	O
framework	O
has	O
been	O
widely	O
used	O
for	O
conversation	O
response	B-TaskName
generation	I-TaskName
(	O
Shang	O
et	O
al	O
,	O
2015	O
;	O
Sordoni	O
et	O
al	O
,	O
2015	O
)	O
where	O
the	O
mapping	O
from	O
a	O
query	O
x	O
to	O
a	O
reply	O
y	O
is	O
learned	O
with	O
the	O
negative	O
log	O
likelihood	O
.	O
However	O
,	O
these	O
models	O
suffer	O
from	O
the	O
"	O
safe	O
"	O
response	O
problem	O
.	O
To	O
address	O
this	O
problem	O
,	O
various	O
methods	O
have	O
been	O
proposed	O
.	O
Li	O
et	O
al	O
(	O
2016a	O
)	O
propose	O
a	O
diversity	O
-	O
promoting	O
objective	O
function	O
to	O
encourage	O
diverse	O
responses	O
during	O
decoding	O
.	O
Zhou	O
et	O
al	O
(	O
,	O
2018a	O
introduce	O
a	O
responding	O
mechanism	O
between	O
the	O
encoder	O
and	O
decoder	O
to	O
generate	O
various	O
responses	O
.	O
incorporate	O
topic	O
information	O
to	O
generate	O
informative	O
responses	O
.	O
However	O
,	O
these	O
models	O
suffer	O
from	O
the	O
deterministic	O
structure	O
when	O
generating	O
multiple	O
diverse	O
responses	O
.	O
Besides	O
,	O
during	O
the	O
training	O
of	O
these	O
models	O
,	O
response	O
utterances	O
are	O
only	O
used	O
in	O
the	O
loss	B-MetricName
function	O
and	O
ignored	O
when	O
forward	O
computing	O
,	O
which	O
can	O
confuse	O
the	O
model	O
for	O
pursuing	O
multiple	O
objectives	O
simultaneously	O
.	O
A	O
few	O
works	O
explore	O
to	O
change	O
the	O
deterministic	O
structure	O
of	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
by	O
introducing	O
stochastic	O
latent	O
variables	O
.	O
VAE	B-MethodName
is	O
one	O
of	O
the	O
most	O
popular	O
methods	O
(	O
Bowman	O
et	O
al	O
,	O
2016	O
;	O
Serban	O
et	O
al	O
,	O
2017	O
;	O
Cao	O
and	O
Clark	O
,	O
2017	O
)	O
,	O
where	O
the	O
discourse	O
-	O
level	O
diversity	O
is	O
modeled	O
by	O
a	O
Gaussian	O
distribution	O
.	O
However	O
,	O
it	O
is	O
observed	O
that	O
in	O
the	O
CVAE	B-MethodName
with	O
a	O
fixed	O
Gaussian	O
prior	O
,	O
the	O
learned	O
conditional	O
posteriors	O
tend	O
to	O
collapse	O
to	O
a	O
single	O
mode	O
,	O
resulting	O
in	O
a	O
relatively	O
simple	O
scope	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
.	O
To	O
tackle	O
this	O
,	O
WAE	O
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
which	O
adopts	O
a	O
Gaussian	O
mixture	O
prior	O
network	O
with	O
Wasserstein	O
distance	O
and	O
VAD	O
(	O
Du	O
et	O
al	O
,	O
2018	O
)	O
which	O
sequentially	O
introduces	O
a	O
series	O
of	O
latent	O
variables	O
to	O
condition	O
each	O
word	O
in	O
the	O
response	O
sequence	O
are	O
proposed	O
.	O
Although	O
these	O
models	O
overcome	O
the	O
deterministic	O
structure	O
of	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
,	O
they	O
still	O
ignore	O
the	O
correlation	O
of	O
multiple	O
valid	O
responses	O
and	O
each	O
case	O
is	O
trained	O
separately	O
.	O
To	O
consider	O
the	O
multiple	O
responses	O
jointly	O
,	O
the	O
maximum	O
likelihood	O
strategy	O
is	O
explored	O
.	O
Zhang	O
et	O
al	O
(	O
2018a	O
)	O
propose	O
the	O
maximum	O
generated	O
likelihood	O
criteria	O
which	O
model	O
a	O
query	O
with	O
its	O
multiple	O
responses	O
as	O
a	O
bag	O
of	O
instances	O
and	O
proposes	O
to	O
optimize	O
the	O
model	O
towards	O
the	O
most	O
likely	O
answer	O
rather	O
than	O
all	O
possible	O
responses	O
.	O
Similarly	O
,	O
Rajendran	O
et	O
al	O
(	O
2018	O
)	O
propose	O
to	O
reward	O
the	O
dialogue	O
system	O
if	O
any	O
valid	O
answer	O
is	O
produced	O
in	O
the	O
reinforcement	O
learning	O
phase	O
.	O
Though	O
considering	O
multiple	O
responses	O
jointly	O
,	O
the	O
maximum	O
likelihood	O
strategy	O
fails	O
to	O
utilize	O
all	O
the	O
references	O
during	O
training	O
with	O
some	O
cases	O
ig	O
-	O
Figure	O
2	O
:	O
The	O
overall	O
architecture	O
of	O
our	O
proposed	O
dialogue	O
system	O
where	O
the	O
two	O
generation	O
steps	O
and	O
testing	O
process	O
are	O
illustrated	O
.	O
Given	O
an	O
input	O
query	O
x	O
,	O
the	O
model	O
aims	O
to	O
approximate	O
the	O
multiple	O
responses	O
in	O
a	O
bag	O
{	O
y	O
}	O
simultaneously	O
with	O
the	O
continuous	O
common	O
and	O
distinctive	O
features	O
,	O
i.e.	O
,	O
the	O
latent	O
variables	O
c	O
and	O
z	O
obtained	O
from	O
the	O
two	O
generation	O
phases	O
respectively	O
.	O
nored	O
.	O
In	O
our	O
approach	O
,	O
we	O
consider	O
multiple	O
responses	O
jointly	O
and	O
model	O
each	O
specific	O
response	O
separately	O
by	O
a	O
two	O
-	O
step	O
generation	O
architecture	O
.	O

In	O
the	O
first	O
generation	O
step	O
,	O
we	O
aim	O
to	O
map	O
from	O
the	O
input	O
query	O
x	O
to	O
the	O
common	O
feature	O
c	O
of	O
the	O
response	O
bag	O
{	O
y	O
}	O
.	O
Inspired	B-DatasetName
by	O
multi	O
-	O
instance	O
learning	O
(	O
Zhou	O
,	O
2004	O
)	O
,	O
we	O
start	O
from	O
the	O
simple	O
intuition	O
that	O
it	O
is	O
much	O
easier	O
for	O
the	O
model	O
to	O
fit	O
multiple	O
instances	O
from	O
their	O
mid	O
-	O
point	O
than	O
a	O
random	O
start	O
-	O
point	O
,	O
as	O
illustrated	O
in	O
Figure	O
1	O
.	O
To	O
obtain	O
this	O
,	O
we	O
model	O
the	O
common	O
feature	O
of	O
the	O
response	O
bag	O
as	O
the	O
mid	O
-	O
point	O
of	O
embeddings	O
of	O
multiple	O
responses	O
.	O
In	O
practice	O
,	O
we	O
first	O
encode	O
the	O
input	O
x	O
with	O
a	O
bidirectional	O
gated	O
recurrent	O
units	O
(	O
GRU	B-MethodName
)	O
to	O
obtain	O
an	O
input	O
representation	O
h	O
x	O
.	O
Then	O
,	O
the	O
common	O
feature	O
c	O
is	O
computed	O
by	O
a	O
mapping	O
network	O
which	O
is	O
implemented	O
by	O
a	O
feed	O
-	O
forward	O
neural	O
network	O
whose	O
trainable	O
parameter	O
is	O
denoted	O
as	O
θ	B-HyperparameterName
.	O
The	O
feature	O
c	O
is	O
then	O
fed	O
into	O
the	O
response	O
decoder	O
to	O
obtain	O
the	O
intermediate	O
response	O
y	O
c	O
which	O
is	O
considered	O
to	O
approximate	O
all	O
valid	O
responses	O
.	O
Mathematically	O
,	O
the	O
objective	O
function	O
is	O
defined	O
as	O
:	O
L	O
avg	O
=	O
1	O
|	O
{	O
y	O
}	O
|	O
y	O
{	O
y	O
}	O
log	O
p	O
ψ	O
(	O
y	O
|	O
c	O
)	O
(	O
1	O
)	O
where	O
|	O
{	O
y	O
}	O
|	O
is	O
the	O
cardinality	O
of	O
the	O
response	O
bag	O
{	O
y	O
}	O
and	O
p	O
ψ	O
represents	O
the	O
response	O
decoder	O
.	O
Besides	O
,	O
to	O
measure	O
how	O
well	O
the	O
intermediate	O
response	O
y	O
c	O
approximates	O
the	O
mid	O
-	O
point	O
response	O
,	O
we	O
set	O
up	O
an	O
individual	O
discriminator	O
and	O
derive	O
the	O
mapping	O
function	O
to	O
produce	O
better	O
results	O
.	O
As	O
to	O
the	O
discriminator	O
,	O
we	O
first	O
project	O
each	O
utterance	O
to	O
an	O
embedding	O
space	O
with	O
fixed	O
dimensionality	O
via	O
convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
with	O
different	O
kernels	O
as	O
the	O
process	O
shown	O
in	O
Figure	O
3	O
.	O
Then	O
,	O
the	O
cosine	O
similarity	O
of	O
the	O
query	O
and	O
response	O
embeddings	O
is	O
computed	O
,	O
denoted	O
as	O
D	O
θ	B-HyperparameterName
(	O
x	O
,	O
y	O
)	O
,	O
where	O
θ	B-HyperparameterName
represents	O
trainable	O
parameter	O
in	O
the	O
discriminator	O
.	O
For	O
the	O
response	O
bag	O
{	O
y	O
}	O
,	O
the	O
average	O
response	O
embedding	O
is	O
used	O
to	O
compute	O
the	O
matching	O
score	O
.	O
The	O
objective	O
of	O
intermediate	O
response	O
y	O
c	O
is	O
then	O
to	O
minimize	O
the	O
difference	O
between	O
D	O
θ	B-HyperparameterName
(	O
x	O
,	O
y	O
c	O
)	O
and	O
D	O
θ	B-HyperparameterName
(	O
x	O
,	O
{	O
y	O
}	O
)	O
:	O
L	O
disc	O
=	O
E	O
x	O
,	O
{	O
y	O
}	O
,	O
y	O
c	O
[	O
D	O
θ	B-HyperparameterName
(	O
x	O
,	O
y	O
c	O
)	O
−	O
D	O
θ	B-HyperparameterName
(	O
x	O
,	O
{	O
y	O
}	O
)	O
]	O
(	O
2	O
)	O
where	O
y	O
c	O
denotes	O
the	O
utterance	O
produced	O
by	O
the	O
decoder	O
conditioned	O
on	O
the	O
variable	O
c.	O
To	O
overcome	O
the	O
discrete	O
and	O
non	O
-	O
differentiable	O
problem	O
,	O
which	O
breaks	O
down	O
gradient	O
propagation	O
from	O
the	O
discriminator	O
,	O
we	O
adopt	O
a	O
"	O
soft	O
"	O
continuous	O
approximation	O
(	O
Hu	O
et	O
al	O
,	O
2017	O
)	O
:	O
y	O
ct	O
∼	O
softmax	B-MethodName
(	O
o	O
t	O
/τ	O
)	O
(	O
3	O
)	O
where	O
o	O
t	O
is	O
the	O
logit	O
vector	O
as	O
the	O
inputs	O
to	O
the	O
softmax	B-MethodName
function	O
at	O
time	O
-	O
step	O
t	O
and	O
the	O
temperature	O
τ	O
is	O
set	O
to	O
τ	O
0	B-DatasetName
as	O
training	O
proceeds	O
for	O
increasingly	O
peaked	O
distributions	O
.	O
The	O
whole	O
loss	B-MetricName
for	O
the	O
step	O
-	O
one	O
generation	O
is	O
then	O
L	O
f	O
irst	O
=	O
L	O
avg	O
+	O
L	O
disc	O
(	O
4	O
)	O
which	O
is	O
optimized	O
by	O
a	O
minimax	O
game	O
with	O
adversarial	O
training	O
(	O
Goodfellow	O
et	O
al	O
,	O
2014	O
)	O
.	O

The	O
second	O
generation	O
phase	O
aims	O
to	O
model	O
each	O
specific	O
response	O
in	O
a	O
response	O
bag	O
respectively	O
.	O
In	O
practice	O
,	O
we	O
adopt	O
the	O
CVAE	B-MethodName
architecture	O
,	O
while	O
two	O
prominent	O
modifications	O
remain	O
.	O
Firstly	O
,	O
rather	O
than	O
modeling	O
each	O
response	O
with	O
the	O
latent	O
variable	O
z	O
from	O
scratch	O
,	O
our	O
model	O
approximates	O
each	O
response	O
based	O
on	O
the	O
bag	O
representation	O
c	O
with	O
only	O
the	O
distinctive	O
feature	O
of	O
each	O
specific	O
response	O
remaining	O
to	O
be	O
captured	O
.	O
Secondly	O
,	O
the	O
prior	O
common	O
feature	O
c	O
can	O
provide	O
extra	O
information	O
for	O
the	O
sampling	O
network	O
which	O
is	O
supposed	O
to	O
decrease	O
the	O
latent	O
searching	O
space	O
.	O
Specifically	O
,	O
similar	O
to	O
the	O
CVAE	B-MethodName
architecture	O
,	O
the	O
overall	O
objective	O
for	O
our	O
model	O
in	O
the	O
second	O
generation	O
phase	O
is	O
as	O
below	O
:	O
L	O
cvae	B-MethodName
=	O
E	O
q	O
φ	O
(	O
z	O
|	O
x	O
,	O
y	O
,	O
c	O
)	O
p	O
θ	B-HyperparameterName
(	O
c	O
|	O
x	O
)	O
[	O
log	O
p	O
ψ	O
(	O
y	O
|	O
c	O
,	O
z	O
)	O
]	O
−	O
D	O
[	O
q	O
φ	O
(	O
z	O
|	O
x	O
,	O
y	O
,	O
c	O
)	O
|	O
|	O
p	O
ϕ	O
(	O
z	O
|	O
x	O
,	O
c	O
)	O
]	O
(	O
5	O
)	O
where	O
q	O
φ	O
represents	O
the	O
recognition	O
network	O
and	O
p	O
ϕ	O
is	O
the	O
prior	O
network	O
with	O
φ	O
and	O
ϕ	O
as	O
the	O
trainable	O
parameters	O
;	O
D	O
(	O
|	O
|	O
)	O
is	O
the	O
regularization	O
term	O
which	O
measures	O
the	O
distance	O
between	O
the	O
two	O
distributions	O
.	O
In	O
practice	O
,	O
the	O
recognition	O
networks	O
are	O
implemented	O
with	O
a	O
feed	O
-	O
forward	O
network	O
that	O
µ	O
log	O
σ	O
2	O
=	O
W	O
q	O
	O
	O
h	O
x	O
h	O
y	O
c	O
	O
	O
+	O
b	O
q	O
(	O
6	O
)	O
where	O
h	O
x	O
and	O
h	O
y	O
are	O
the	O
utterance	O
representations	O
of	O
query	O
and	O
response	O
got	O
by	O
GRU	B-MethodName
respectively	O
,	O
and	O
the	O
latent	O
variable	O
z	O
∼	O
N	O
(	O
µ	O
,	O
σ	O
2	O
I	O
)	O
.	O
For	O
the	O
prior	O
networks	O
,	O
we	O
consider	O
two	O
kinds	O
of	O
implements	O
.	O
One	O
is	O
the	O
vanilla	O
CVAE	B-MethodName
model	O
where	O
the	O
prior	O
p	O
ϕ	O
(	O
z	O
|	O
x	O
,	O
c	O
)	O
is	O
modeled	O
by	O
a	O
another	O
feed	O
-	O
forward	O
network	O
conditioned	O
on	O
the	O
representations	O
h	O
x	O
and	O
c	O
as	O
follows	O
,	O
µ	O
log	O
σ	O
2	O
=	O
W	O
p	O
h	O
x	O
c	O
+	O
b	O
p	O
(	O
7	O
)	O
and	O
the	O
distance	O
D	O
(	O
|	O
|	O
)	O
here	O
is	O
measured	O
by	O
the	O
KL	O
divergence	O
.	O
For	O
the	O
other	O
,	O
we	O
adopt	O
the	O
WAE	O
model	O
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
in	O
which	O
the	O
prior	O
p	O
ϕ	O
(	O
z	O
|	O
x	O
,	O
c	O
)	O
is	O
modeled	O
by	O
a	O
mixture	O
of	O
Gaussian	O
distributions	O
GMM	O
(	O
π	O
k	O
,	O
µ	O
k	O
,	O
σ	O
k	O
2	O
I	O
)	O
K	O
k=1	O
,	O
where	O
K	O
is	O
the	O
number	O
of	O
Gaussian	O
distributions	O
and	O
π	O
k	O
is	O
the	O
mixture	O
coefficient	O
of	O
the	O
k	O
-	O
th	O
component	O
of	O
the	O
GMM	O
module	O
as	O
computed	O
:	O
π	O
k	B-HyperparameterName
=	I-HyperparameterName
exp	O
(	O
e	O
k	O
)	O
K	O
i=1	O
exp	O
(	O
e	O
i	O
)	O
(	O
8	O
)	O
and	O
	O
	O
e	O
k	O
µ	O
k	O
log	O
σ	O
2	O
k	O
	O
	O
=	O
W	O
p	O
,	O
k	O
h	O
x	O
c	O
+	O
b	O
p	O
,	O
k	O
(	O
9	O
)	O
To	O
sample	O
an	O
instance	O
,	O
Gumble	O
-	O
Softmax	B-MethodName
reparametrization	O
trick	O
(	O
Kusner	O
and	O
Hernández	O
-	O
Lobato	O
,	O
2016	O
)	O
is	O
utilized	O
to	O
normalize	O
the	O
coefficients	O
.	O
The	O
distance	O
here	O
is	O
measured	O
by	O
the	O
Wasserstein	O
distance	O
which	O
is	O
implemented	O
with	O
an	O
adversarial	O
discriminator	O
.	O
Recap	O
that	O
in	O
the	O
second	O
generation	O
phase	O
the	O
latent	O
variable	O
z	O
is	O
considered	O
to	O
only	O
capture	O
the	O
distinctive	O
feature	O
of	O
each	O
specific	O
response	O
.	O
Hence	O
to	O
distinguish	O
the	O
latent	O
variable	O
z	O
for	O
each	O
separate	O
response	O
,	O
we	O
further	O
introduce	O
a	O
multireference	O
bag	O
-	O
of	O
-	O
word	O
loss	B-MetricName
(	O
MBOW	O
)	O
which	O
requires	O
the	O
network	O
to	O
predict	O
the	O
current	O
response	O
y	O
against	O
the	O
response	O
bag	O
:	O
L	O
mbow	O
=	O
E	O
q	O
φ	O
(	O
z	O
|	O
x	O
,	O
y	O
,	O
c	O
)	O
[	O
log	O
p	O
(	O
y	O
bow	O
|	O
x	O
,	O
z	O
)	O
+	O
λ	O
log	O
(	O
1	O
−	O
p	O
(	O
{	O
ȳ	O
}	O
bow	O
|	O
x	O
,	O
z	O
)	O
)	O
]	O
(	O
10	O
)	O
where	O
the	O
probability	O
is	O
computed	O
by	O
a	O
feedforward	B-MethodName
network	I-MethodName
f	O
as	O
the	O
vanilla	O
bag	O
-	O
of	O
-	O
word	O
loss	B-MetricName
does	O
;	O
{	O
ȳ	O
}	O
is	O
the	O
complementary	O
response	O
bag	O
of	O
y	O
and	O
its	O
probability	O
is	O
computed	O
as	O
the	O
average	O
probability	O
of	O
responses	O
in	O
the	O
bag	O
;	O
and	O
λ	O
is	O
a	O
scaling	O
factor	O
accounting	O
for	O
the	O
difference	O
in	O
magnitude	O
.	O
As	O
it	O
shows	O
,	O
the	O
MBOW	O
loss	B-MetricName
penalizes	O
the	O
recognition	O
networks	O
if	O
other	O
complementary	O
responses	O
can	O
be	O
predicted	O
from	O
the	O
distinctive	O
variable	O
z.	O
Besides	O
,	O
since	O
the	O
probability	O
of	O
the	O
complementary	O
term	O
may	O
approach	O
zero	O
which	O
makes	O
it	O
difficult	O
to	O
optimize	O
,	O
we	O
actually	O
adopt	O
its	O
lower	O
bound	O
in	O
practice	O
:	O
log	O
(	O
1	O
−	O
p	O
(	O
y	O
bow	O
|	O
x	O
,	O
z	O
)	O
)	O
=	O
log	O
(	O
1	O
−	O
|	O
y	O
|	O
t=1	O
e	O
fy	O
t	O
|	O
V	O
|	O
j	O
e	O
f	O
j	O
)	O
≥	O
log	O
(	O
|	O
y	O
|	O
t=1	O
(	O
1	O
−	O
e	O
fy	O
t	O
|	O
V	O
|	O
j	O
e	O
f	O
j	O
)	O
)	O
(	O
11	O
)	O
where	O
|	O
V	O
|	O
is	O
vocabulary	O
size	O
.	O
Totally	O
,	O
the	O
whole	O
loss	B-MetricName
for	O
the	O
step	O
-	O
two	O
generation	O
is	O
then	O
:	O
L	O
second	O
=	O
L	O
cvae	B-MethodName
+	O
L	O
mbow	O
(	O
12	O
)	O
which	O
can	O
be	O
optimized	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
way	O
.	O

Our	O
whole	O
model	O
can	O
be	O
trained	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
fashion	O
.	O
To	O
train	O
the	O
model	O
,	O
we	O
first	O
pre	O
-	O
train	O
the	O
word	O
embedding	O
using	O
Glove	O
(	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
)	O
1	O
.	O
Then	O
modules	O
of	O
the	O
model	O
are	O
jointly	O
trained	O
by	O
optimizing	O
the	O
losses	O
L	O
f	O
irst	O
and	O
L	O
second	O
of	O
the	O
two	O
generation	O
phases	O
respectively	O
.	O
To	O
overcome	O
the	O
vanishing	O
latent	O
variable	O
problem	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
of	O
CVAE	B-MethodName
,	O
we	O
adopt	O
the	O
KL	O
annealing	O
strategy	O
(	O
Bowman	O
et	O
al	O
,	O
2016	O
)	O
,	O
where	O
the	O
weight	O
of	O
the	O
KL	O
term	O
is	O
gradually	O
increased	O
during	O
training	O
.	O
The	O
other	O
technique	O
employed	O
is	O
the	O
MBOW	O
loss	B-MetricName
which	O
is	O
able	O
to	O
sharpen	O
the	O
distribution	O
of	O
latent	O
variable	O
z	O
for	O
each	O
specific	O
response	O
and	O
alleviate	O
the	O
vanishing	O
problem	O
at	O
the	O
same	O
time	O
.	O
During	O
testing	O
,	O
diverse	O
responses	O
can	O
be	O
obtained	O
by	O
the	O
two	O
generation	O
phases	O
described	O
above	O
,	O
where	O
the	O
distinctive	O
latent	O
variable	O
z	O
corresponding	O
to	O
each	O
specific	O
response	O
is	O
sampled	O
from	O
the	O
prior	O
probability	O
network	O
.	O
This	O
process	O
is	O
illustrated	O
in	O
Figure	O
2	O
.	O
Capable	O
of	O
capturing	O
the	O
common	O
feature	O
of	O
the	O
response	O
bag	O
,	O
the	O
variable	O
c	O
is	O
obtained	O
from	O
the	O
mapping	O
network	O
and	O
no	O
intermediate	O
utterance	O
is	O
required	O
,	O
which	O
facilitates	O
reducing	O
the	O
complexity	O
of	O
decoding	O
.	O

We	O
compare	O
our	O
model	O
with	O
representative	O
dialogue	B-TaskName
generation	I-TaskName
approaches	O
as	O
listed	O
below	O
:	O
S2S	O
:	O
the	O
vanilla	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
with	O
attention	O
mechanism	O
where	O
standard	O
beam	O
search	O
is	O
applied	O
in	O
testing	O
to	O
generate	O
multiple	O
different	O
responses	O
.	O
Method	O
Multi	O
-	O
BLEU	B-MetricName
EMBEDDING	O
Intra	O
-	O
Dist	O
Inter	O
-	O
Dist	O
BLEU	B-MetricName
-	O
1	O
BLEU	B-MetricName
-	O
2	O
G	O
A	O
E	O
Dist	O
-	O
1	O
Dist	O
-	O
2	O
S2S+DB	O
:	O
the	O
vanilla	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
with	O
the	O
modified	O
diversity	O
-	O
promoting	O
beam	O
search	O
method	O
(	O
Li	O
et	O
al	O
,	O
2016b	O
)	O
where	O
a	O
fixed	O
diversity	O
rate	O
0.5	O
is	O
used	O
.	O
MMS	O
:	O
the	O
modified	O
multiple	O
responding	O
mechanisms	O
enhanced	O
dialogue	O
model	O
proposed	O
by	O
Zhou	O
et	O
al	O
(	O
2018a	O
)	O
which	O
introduces	O
responding	O
mechanism	O
embeddings	O
for	O
diverse	O
response	B-TaskName
generation	I-TaskName
.	O
CVAE	B-MethodName
:	O
the	O
vanilla	O
CVAE	B-MethodName
model	O
with	O
and	O
without	O
BOW	O
(	O
bag	O
-	O
of	O
-	O
word	O
)	O
loss	B-MetricName
(	O
CVAE+BOW	O
and	O
CVAE	B-MethodName
)	O
.	O
WAE	O
:	O
the	O
conditional	O
Wasserstein	O
autoencoder	B-MethodName
model	O
for	O
dialogue	B-TaskName
generation	I-TaskName
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
which	O
models	O
the	O
distribution	O
of	O
data	O
by	O
training	O
a	O
GAN	B-MethodName
within	O
the	O
latent	O
variable	O
space	O
.	O
Ours	O
:	O
we	O
explore	O
our	O
model	O
Ours	O
and	O
conduct	O
various	O
ablation	O
studies	O
:	O
the	O
model	O
with	O
only	O
the	O
second	O
stage	O
generation	O
(	O
Ours	O
-	O
First	O
)	O
,	O
the	O
model	O
without	O
the	O
discriminator	O
(	O
Ours	O
-	O
Disc	O
)	O
and	O
multireference	O
BOW	O
loss	B-MetricName
(	O
Ours	O
-	O
MBOW	O
)	O
,	O
and	O
the	O
model	O
with	O
GMM	O
prior	O
networks	O
(	O
Ours+GMP	O
)	O
.	O

To	O
comprehensively	O
evaluate	O
the	O
quality	O
of	O
generated	O
response	O
utterances	O
,	O
we	O
adopt	O
both	O
automatic	O
and	O
human	O
evaluation	O
metrics	O
:	O
BLEU	B-MetricName
:	O
In	O
dialogue	B-TaskName
generation	I-TaskName
,	O
BLEU	B-MetricName
is	O
widely	O
used	O
in	O
previous	O
studies	O
(	O
Yao	O
et	O
al	O
,	O
2017	O
;	O
Shang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Since	O
multiple	O
valid	O
responses	O
exist	O
in	O
this	O
paper	O
,	O
we	O
adopt	O
multi	O
-	O
reference	O
BLEU	B-MetricName
where	O
the	O
evaluated	O
utterance	O
is	O
compared	O
to	O
provided	O
multiple	O
references	O
simultaneously	O
.	O
Distinctness	O
:	O
To	O
distinguish	O
safe	O
and	O
commonplace	O
responses	O
,	O
the	O
distinctness	O
score	O
(	O
Li	O
et	O
al	O
,	O
2016a	O
)	O
is	O
designed	O
to	O
measure	O
word	O
-	O
level	O
diversity	O
by	O
counting	O
the	O
ratio	O
of	O
distinctive	O
[	O
1	O
,	O
2	O
]	O
-	O
grams	O
.	O
In	O
our	O
experiments	O
,	O
we	O
adopt	O
both	O
Intra	O
-	O
Dist	O
:	O
the	O
distinctness	O
scores	O
of	O
multiple	O
responses	O
for	O
a	O
given	O
query	O
and	O
Inter	O
-	O
Dist	O
:	O
the	O
distinctness	O
scores	O
of	O
generated	O
responses	O
of	O
the	O
whole	O
testing	O
set	O
.	O
Embedding	O
Similarity	O
:	O
Embedding	O
-	O
based	O
metrics	O
compute	O
the	O
cosine	O
similarity	O
between	O
the	O
sentence	B-TaskName
embedding	I-TaskName
of	O
a	O
ground	O
-	O
truth	O
response	O
and	O
that	O
of	O
the	O
generated	O
one	O
.	O
There	O
are	O
various	O
ways	O
to	O
obtain	O
the	O
sentence	O
-	O
level	O
embedding	O
from	O
the	O
constituent	O
word	B-TaskName
embeddings	I-TaskName
.	O
In	O
our	O
experiments	O
,	O
we	O
apply	O
three	O
most	O
commonly	O
used	O
strategies	O
:	O
Greedy	O
matches	O
each	O
word	O
of	O
the	O
reference	O
with	O
the	O
most	O
similar	O
word	O
in	O
the	O
evaluated	O
sentence	O
;	O
Average	O
uses	O
the	O
average	O
of	O
word	O
embed	O
-	O
Input	O
火山喷发瞬间的一些壮观景象	O
。	O
再过十分钟就进入win8时代，我是系统升级控	O
。	O
Query	O
These	O
are	O
some	O
magnificent	O
sights	O
at	O
the	O
moment	O
of	O
the	O
volcanic	O
eruption	O
.	O
There	O
remain	O
ten	O
minutes	O
before	O
we	O
entering	O
the	O
era	O
of	O
win8	O
.	O
I	O
am	O
a	O
geek	O
of	O
system	O
updating	O
.	O
What	O
application	O
is	O
this	O
.	O
如此这般这般淼小	O
。	O
我觉得这样的界面更像windows8	O
。	O
It	O
is	O
so	O
so	O
imperceptible	O
.	O
I	O
think	O
interface	O
like	O
this	O
looks	O
more	O
like	O
windows8	O
.	O
Table	O
3	O
:	O
Case	O
study	O
for	O
the	O
generated	O
responses	O
from	O
the	O
testing	O
set	O
of	O
Weibo	B-DatasetName
,	O
where	O
the	O
Chinese	O
utterances	O
are	O
translated	O
into	O
English	O
for	O
the	O
sake	O
of	O
readability	O
.	O
For	O
each	O
input	O
query	O
,	O
we	O
show	O
four	O
responses	O
generated	O
by	O
each	O
method	O
and	O
an	O
additional	O
intermediate	O
utterance	O
(	O
marked	O
with	O
underline	O
)	O
for	O
our	O
model	O
.	O
dings	O
;	O
and	O
Extreme	O
takes	O
the	O
most	O
extreme	O
value	O
among	O
all	O
words	O
for	O
each	O
dimension	O
of	O
word	B-TaskName
embeddings	I-TaskName
in	O
a	O
sentence	O
.	O
Since	O
multiple	O
references	O
exist	O
,	O
for	O
each	O
utterance	O
to	O
be	O
evaluated	O
,	O
we	O
compute	O
its	O
score	O
with	O
the	O
most	O
similar	O
reference	O
.	O
Human	O
Evaluation	O
with	O
Case	O
Analysis	O
:	O
As	O
automatic	O
evaluation	O
metrics	O
lose	O
sight	O
of	O
the	O
overall	O
quality	O
of	O
a	O
response	O
(	O
Tao	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
also	O
adopt	O
human	O
evaluation	O
on	O
100	O
random	O
samples	O
to	O
assess	O
the	O
generation	O
quality	O
with	O
three	O
independent	O
aspects	O
considered	O
:	O
relevance	O
(	O
whether	O
the	O
reply	O
is	O
relevant	O
to	O
the	O
query	O
)	O
,	O
diversity	O
(	O
whether	O
the	O
reply	O
narrates	O
with	O
diverse	O
words	O
)	O
and	O
readability	O
(	O
whether	O
the	O
utterance	O
is	O
grammatically	O
formed	O
)	O
.	O
Each	O
property	O
is	O
assessed	O
with	O
a	O
score	O
from	O
1	O
(	O
worst	O
)	O
to	O
5	O
(	O
best	O
)	O
by	O
three	O
annotators	O
.	O
The	O
evaluation	O
is	O
conducted	O
in	O
a	O
blind	O
process	O
with	O
the	O
utterance	O
belonging	O
unknown	O
to	O
the	O
reviewers	O
.	O

All	O
models	O
are	O
trained	O
with	O
the	O
following	O
hyperparameters	O
:	O
both	O
encoder	O
and	O
decoder	O
are	O
set	O
to	O
one	O
layer	O
with	O
GRU	B-MethodName
cells	O
,	O
where	O
the	O
hidden	O
state	O
size	O
of	O
GRU	B-MethodName
is	O
256	O
;	O
the	O
utterance	O
length	O
is	O
limited	O
to	O
50	O
;	O
the	O
vocabulary	O
size	O
is	O
50	O
,	O
000	O
and	O
the	O
word	B-HyperparameterName
embedding	I-HyperparameterName
dimension	I-HyperparameterName
is	O
256	O
;	O
the	O
word	B-TaskName
embeddings	I-TaskName
are	O
shared	O
by	O
the	O
encoder	O
and	O
decoder	O
;	O
all	O
trainable	O
parameters	O
are	O
initialized	O
from	O
a	O
uniform	O
distribution	O
[	O
-	O
0.08	O
,	O
0.08	O
]	O
;	O
we	O
employ	O
the	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
for	O
optimization	O
with	O
a	O
mini	B-HyperparameterName
-	I-HyperparameterName
batch	I-HyperparameterName
size	I-HyperparameterName
128	O
and	O
initialized	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.001	O
;	O
the	O
gradient	B-MethodName
clipping	I-MethodName
strategy	O
is	O
utilized	O
to	O
avoid	O
gradient	O
explosion	O
,	O
where	O
the	O
gradient	B-MethodName
clipping	I-MethodName
value	O
is	O
set	O
to	O
be	O
5	O
.	O
For	O
the	O
latent	O
variable	O
,	O
we	O
adopt	O
dimensional	O
size	O
256	O
and	O
the	O
component	O
number	O
of	O
the	O
mixture	O
Gaussian	O
for	O
prior	O
networks	O
in	O
WAE	O
is	O
set	O
to	O
5	O
.	O
As	O
to	O
the	O
discriminator	O
,	O
we	O
set	O
the	O
initialized	O
learning	B-HyperparameterName
rate	I-HyperparameterName
as	O
0.0002	O
and	O
use	O
128	O
different	O
kernels	O
for	O
each	O
kernel	B-HyperparameterName
size	I-HyperparameterName
in	O
{	O
2	O
,	O
3	O
,	O
4	O
}	O
.	O
The	O
size	O
of	O
the	O
response	O
bag	O
is	O
limited	O
to	O
10	O
where	O
the	O
instances	O
inside	O
are	O
randomly	O
sampled	O
for	O
each	O
mini	O
-	O
batch	O
.	O
All	O
the	O
models	O
are	O
implemented	O
with	O
Pytorch	O
0.4.1	O
4	O
.	O

Table	O
1	O
shows	O
our	O
main	O
experimental	O
results	O
,	O
with	O
baselines	O
shown	O
in	O
the	O
top	O
and	O
our	O
models	O
at	O
the	O
bottom	O
.	O
The	O
results	O
show	O
that	O
our	O
model	O
(	O
Ours	O
)	O
outperforms	O
competitive	O
baselines	O
on	O
various	O
evaluation	O
metrics	O
.	O
The	O
Seq2seq	B-MethodName
based	O
models	O
(	O
S2S	O
,	O
S2S	O
-	O
DB	O
and	O
MMS	O
)	O
tend	O
to	O
generate	O
fluent	O
utterances	O
and	O
can	O
share	O
some	O
overlapped	O
words	O
with	O
the	O
references	O
,	O
as	O
the	O
high	O
BLEU	B-MetricName
-	O
2	O
scores	O
show	O
.	O
However	O
,	O
the	O
distinctness	O
scores	O
illustrate	O
that	O
these	O
models	O
fail	O
to	O
generate	O
multiple	O
diverse	O
responses	O
in	O
spite	O
of	O
the	O
diversitypromoting	O
objective	O
and	O
responding	O
mechanisms	O
used	O
.	O
We	O
attribute	O
this	O
to	O
that	O
these	O
models	O
fail	O
to	O
consider	O
multiple	O
references	O
for	O
the	O
same	O
query	O
,	O
which	O
may	O
confuse	O
the	O
models	O
and	O
lead	O
to	O
a	O
commonplace	O
utterance	O
.	O
As	O
to	O
the	O
CVAE	B-MethodName
and	O
WAE	O
models	O
,	O
with	O
the	O
latent	O
variable	O
to	O
control	O
the	O
discourse	O
-	O
level	O
diversity	O
,	O
diverse	O
responses	O
can	O
be	O
obtained	O
.	O
Compared	O
against	O
these	O
previous	O
methods	O
,	O
our	O
model	O
can	O
achieve	O
the	O
best	O
or	O
second	O
best	O
performances	O
on	O
different	O
automatic	O
evaluation	O
metrics	O
where	O
the	O
improvements	O
are	O
most	O
consistent	O
on	O
BLEU	B-MetricName
-	O
1	O
and	O
embedding	O
-	O
based	O
metrics	O
,	O
which	O
demonstrates	O
the	O
overall	O
effectiveness	O
of	O
our	O
proposed	O
architecture	O
.	O
In	O
order	O
to	O
better	O
study	O
the	O
quality	O
of	O
generated	O
responses	O
,	O
we	O
also	O
report	O
the	O
human	O
evaluation	O
results	O
in	O
Table	O
2	O
.	O
As	O
results	O
show	O
,	O
although	O
there	O
remains	O
a	O
huge	O
gap	O
between	O
existing	O
methods	O
and	O
human	O
performance	O
(	O
the	O
Gold	O
)	O
,	O
our	O
model	O
gains	O
promising	O
promotions	O
over	O
previous	O
methods	O
on	O
generating	O
appropriate	O
responses	O
with	O
diverse	O
expressions	O
.	O
With	O
both	O
obvious	O
superiority	O
(	O
readability	O
for	O
S2S	O
and	O
diversity	O
for	O
CVAE	B-MethodName
)	O
and	O
inferiority	O
(	O
diversity	O
for	O
S2S	O
and	O
relevance	O
for	O
CVAE	B-MethodName
)	O
,	O
the	O
baselines	O
show	O
limited	O
overall	O
performances	O
,	O
in	O
contrast	O
to	O
which	O
our	O
method	O
can	O
output	O
more	O
diverse	O
utterances	O
while	O
maintaining	O
the	O
relevance	O
to	O
the	O
input	O
query	O
and	O
achieve	O
a	O
high	O
overall	O
score	O
.	O

To	O
better	O
understand	O
the	O
effectiveness	O
of	O
each	O
component	O
in	O
our	O
model	O
,	O
we	O
further	O
conduct	O
the	O
ablation	O
studies	O
with	O
results	O
shown	O
at	O
the	O
bottom	O
of	O
Table	O
1	O
.	O
Above	O
all	O
,	O
to	O
validate	O
the	O
effectiveness	O
of	O
the	O
common	O
feature	O
,	O
we	O
remove	O
the	O
first	O
generation	O
stage	O
and	O
get	O
the	O
Ours	O
-	O
First	O
model	O
.	O
As	O
the	O
results	O
of	O
BLEU	B-MetricName
and	O
embedding	O
-	O
based	O
metrics	O
show	O
,	O
the	O
system	O
can	O
benefit	O
from	O
the	O
common	O
feature	O
for	O
better	O
relevance	O
to	O
the	O
query	O
.	O
Moreover	O
,	O
pairwise	O
comparisons	O
Ours	O
-	O
Disc	O
vs.	O
Ours	O
and	O
Ours	O
-	O
MBOW	O
vs.	O
Ours	O
validate	O
the	O
effects	O
of	O
the	O
discriminator	O
and	O
modified	O
multireference	O
bag	O
-	O
of	O
-	O
word	O
loss	B-MetricName
(	O
MBOW	O
)	O
.	O
As	O
results	O
show	O
,	O
the	O
discriminator	O
facilitates	O
extracting	O
the	O
common	O
feature	O
and	O
yields	O
more	O
relevant	O
responses	O
to	O
the	O
input	O
query	O
afterward	O
.	O
The	O
MBOW	O
loss	B-MetricName
,	O
similar	O
to	O
the	O
effects	O
of	O
BOW	O
loss	B-MetricName
in	O
the	O
CVAE	B-MethodName
,	O
can	O
lead	O
to	O
a	O
more	O
unique	O
latent	O
variable	O
for	O
each	O
response	O
and	O
improve	O
the	O
final	O
distinctness	O
scores	O
of	O
generated	O
utterances	O
.	O
In	O
the	O
experiments	O
,	O
we	O
also	O
observed	O
the	O
KL	O
vanishing	O
problem	O
when	O
training	O
our	O
model	O
and	O
we	O
overcame	O
it	O
with	O
the	O
KL	O
weight	O
annealing	O
strategy	O
and	O
the	O
MBOW	O
loss	B-MetricName
described	O
above	O
.	O

Decoders	O
are	O
algorithms	O
to	O
search	O
for	O
the	O
highest	O
scoring	O
hypothesis	O
.	O
The	O
list	O
of	O
predictors	O
determines	O
how	O
(	O
partial	O
)	O
hypotheses	O
are	O
scored	O
by	O
implementing	O
the	O
methods	O
initialize	O
(	O
)	O
,	O
get	O
state	O
(	O
)	O
,	O
set	O
state	O
(	O
)	O
,	O
predict	O
next	O
(	O
)	O
,	O
and	O
consume	O
(	O
)	O
.	O
The	O
Decoder	O
class	O
implements	O
versions	O
of	O
these	O
methods	O
which	O
apply	O
to	O
all	O
predictors	O
in	O
the	O
list	O
.	O
initialize	O
(	O
)	O
is	O
always	O
called	O
prior	O
to	O
decoding	O
a	O
new	O
sentence	O
.	O
Many	O
popular	O
search	O
strategies	O
can	O
be	O
described	O
via	O
the	O
remaining	O
methods	O
get	O
state	O
(	O
)	O
,	O
set	O
state	O
(	O
)	O
,	O
predict	O
next	O
(	O
)	O
,	O
and	O
consume	O
(	O
)	O
.	O
Algs	O
.	O
1	O
and	O
2	O
show	O
how	O
to	O
define	O
greedy	O
and	O
beam	O
decoding	O
in	O
this	O
way	O
.	O
45	O
Tab	O
.	O
3	O
contains	O
a	O
list	O
of	O
currently	O
implemented	O
decoders	O
.	O
The	O
UML	O
diagram	O
in	O
Fig	O
.	O
1	O
illustrates	O
the	O
relation	O
between	O
decoders	O
and	O
predictors	O
.	O
Algorithm	O
1	O
Greedy	O
(	O
src	O
sen	O
)	O
1	O
:	O
initialize	O
(	O
src	O
sen	O
)	O
2	O
:	O
h	O
<	O
s	O
>	O
3	O
:	O
repeat	O
4	O
:	O
P	O
predict	O
next	O
(	O
)	O
5	O
:	O
(	O
t	O
,	O
c	O
)	O
arg	O
max	O
(	O
t	O
,	O
c	O
)	O
P	O
c	O
6	O
:	O
h	O
h	O
t	O
7	O
:	O
consume	O
(	O
t	O
)	O
8	O
:	O
until	O
t	O
=	O
<	O
/s	O
>	O
9	O
:	O
return	O
h	O
NMT	O
batch	O
decoding	O
The	O
flexibility	O
of	O
the	O
predictor	O
framework	O
comes	O
with	O
degradation	O
in	O
decoding	O
time	O
.	O
SGNMT	O
provides	O
two	O
ways	O
of	O
speeding	O
up	O
pure	O
NMT	O
decoding	O
,	O
especially	O
on	O
the	O
GPU	O
.	O
The	O
vanilla	O
decoding	O
strategy	O
exposes	O
the	O
beam	O
search	O
implementation	O
in	O
Blocks	O
(	O
van	O
Merriënboer	O
et	O
al	O
,	O
2015	O
)	O
which	O
processes	O
all	O
active	O
hypotheses	O
in	O
the	O
beam	O
in	O
parallel	O
.	O
We	O
also	O
implemented	O
a	O
beam	O
decoder	O
version	O
which	O
decodes	O
multiple	O
sentences	O
at	O
once	O
(	O
batch	O
decoding	O
)	O
rather	O
than	O
in	O
a	O
sequential	O
order	O
.	O
Batch	O
decoding	O
is	O
potentially	O
more	O
efficient	O
since	O
larger	O
batches	O
can	O
make	O
better	O
use	O
of	O
GPU	O
parallelism	O
.	O
The	O
key	O
concepts	O
of	O
our	O
batch	O
decoder	O
implementation	O
are	O
:	O
We	O
use	O
a	O
scheduler	O
running	O
on	O
a	O
separate	O
CPU	O
thread	O
to	O
construct	O
large	O
batches	O
of	O
computation	O
(	O
GPU	O
jobs	O
)	O
from	O
multiple	O
sentences	O
and	O
feeding	O
them	O
to	O
the	O
jobs	O
queue	O
.	O
The	O
GPU	O
is	O
operated	O
by	O
a	O
single	O
thread	O
which	O
communicates	O
with	O
the	O
CPU	O
scheduler	O
thread	O
via	O
queues	O
containing	O
jobs	O
.	O
This	O
thread	O
is	O
only	O
responsible	O
for	O
retrieving	O
jobs	O
in	O
the	O
jobs	O
queue	O
,	O
computing	O
them	O
,	O
and	O
putting	O
them	O
in	O
the	O
jobs	O
results	O
queue	O
,	O
minimizing	O
the	O
down	O
-	O
time	O
of	O
GPU	O
computation	O
.	O
Yet	O
another	O
CPU	O
thread	O
is	O
responsible	O
for	O
processing	O
the	O
results	O
computed	O
on	O
the	O
GPU	O
H	O
next	O
H	O
next	O
∪	O
(	O
t	O
,	O
c	O
)	O
P	O
(	O
h	O
t	O
,	O
c	O
+	O
c	O
,	O
s	O
)	O
9	O
:	O
end	O
for	O
10	O
:	O
H	O
11	O
:	O
for	O
all	O
(	O
h	O
,	O
c	O
,	O
s	O
)	O
n	O
-	O
best	O
(	O
H	O
next	O
)	O
do	O
end	O
for	O
16	O
:	O
until	O
Best	O
hypothesis	O
in	O
H	O
ends	O
with	O
<	O
/s	O
>	O
17	O
:	O
return	O
Best	O
hypothesis	O
in	O
H	O
in	O
the	O
job	O
results	O
queue	O
,	O
e.g.	O
by	O
getting	O
the	O
n	O
-	O
best	O
words	O
from	O
the	O
posteriors	O
.	O
Processed	O
jobs	O
are	O
sent	O
back	O
to	O
the	O
CPU	O
scheduler	O
where	O
they	O
are	O
reassembled	O
into	O
new	O
jobs	O
.	O
This	O
decoder	O
is	O
able	O
to	O
translate	O
the	O
WMT	O
English	O
-	O
French	O
test	O
sets	O
news	O
-	O
test2012	O
to	O
news	O
-	O
test2014	O
on	O
a	O
Titan	B-DatasetName
X	O
GPU	O
with	O
911.6	O
words	O
per	O
second	O
with	O
the	O
word	O
-	O
based	O
NMT	O
model	O
described	O
in	O
Stahlberg	O
et	O
al	O
(	O
2016	O
)	O
.	O
6	O
This	O
decoding	O
speed	O
seems	O
to	O
be	O
slightly	O
faster	O
than	O
sequential	O
decoding	O
with	O
high	O
-	O
performance	O
NMT	O
decoders	O
like	O
Marian	O
-	O
NMT	O
(	O
Junczys	O
-	O
Dowmunt	O
et	O
al	O
,	O
2016	O
)	O
with	O
reported	O
decoding	O
speeds	O
of	O
865	O
words	O
per	O
second	O
.	O
7	O
However	O
,	O
batch	O
decoding	O
with	O
Marian	O
-	O
NMT	O
is	O
much	O
faster	O
reaching	O
over	O
4	O
,	O
500	O
words	O
per	O
second	O
.	O
8	O
We	O
think	O
that	O
these	O
differences	O
are	O
mainly	O
due	O
to	O
the	O
limited	O
multithreading	O
support	O
and	O
performance	O
in	O
Python	O
especially	O
when	O
using	O
external	O
libraries	O
as	O
opposed	O
to	O
the	O
highly	O
optimized	O
C++	O
code	O
in	O
Marian	O
-	O
NMT	O
.	O
We	O
did	O
not	O
push	O
for	O
even	O
faster	O
decoding	O
as	O
speed	O
is	O
not	O
a	O
major	O
design	O
goal	O
of	O
SGNMT	O
.	O
Note	O
that	O
batch	O
decoding	O
bypasses	O
the	O
predictor	O
framework	O
and	O
can	O
only	O
be	O
used	O
for	O
pure	O
NMT	O
decoding	O
.	O
Ensembling	O
with	O
models	O
at	O
multiple	O
tokenization	O
levels	O
SGNMT	O
allows	O
masking	O
predictors	O
with	O
alternative	O
sets	O
of	O
modelling	O
units	O
.	O
The	O
conversion	O
between	O
the	O
tokenization	O
schemes	O
of	O
different	O
predictors	O
is	O
defined	O
with	O
FSTs	O
.	O
This	O
makes	O
it	O
possible	O
to	O
decode	O
by	O
combining	O
scores	O
from	O
both	O
a	O
subword	O
-	O
unit	O
(	O
BPE	B-MethodName
)	O
based	O
NMT	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
and	O
a	O
word	O
-	O
based	O
NMT	O
model	O
with	O
character	O
-	O
based	O
NMT	O
,	O
masking	O
the	O
BPE	B-MethodName
-	O
based	O
and	O
word	O
-	O
based	O
NMT	O
predictors	O
with	O
FSTs	O
which	O
transduce	O
character	O
sequences	O
to	O
BPE	B-MethodName
or	O
word	O
sequences	O
.	O
Masking	O
is	O
transparent	O
to	O
the	O
decoding	O
strategy	O
as	O
predictors	O
are	O
replaced	O
by	O
a	O
special	O
wrapper	O
(	O
fsttok	O
)	O
that	O
uses	O
the	O
masking	O
FST	O
to	O
translate	O
predict	O
next	O
(	O
)	O
and	O
consume	O
(	O
)	O
calls	O
to	O
(	O
a	O
series	O
of	O
)	O
predictor	O
calls	O
with	O
alternative	O
tokens	O
.	O
The	O
syncbeam	O
variation	O
of	O
beam	O
search	O
compares	O
competing	O
hypotheses	O
only	O
after	O
consuming	O
a	O
special	O
word	O
boundary	O
symbol	O
rather	O
than	O
after	O
each	O
token	O
.	O
This	O
allows	O
combining	O
scores	O
at	O
the	O
word	O
level	O
even	O
when	O
using	O
models	O
with	O
multiple	O
levels	O
of	O
tokenization	O
.	O
Joint	O
decoding	O
with	O
different	O
tokenization	O
schemes	O
has	O
the	O
potential	O
of	O
combining	O
the	O
benefits	O
of	O
the	O
different	O
schemes	O
:	O
character	O
-	O
and	O
BPE	B-MethodName
-	O
based	O
models	O
are	O
able	O
to	O
address	O
rare	O
words	O
,	O
but	O
word	O
-	O
based	O
NMT	O
can	O
model	O
long	O
-	O
range	O
dependencies	O
more	O
efficiently	O
.	O
System	O
-	O
level	O
combination	O
We	O
showed	O
in	O
Sec	O
.	O
2.1	O
how	O
to	O
formulate	O
NMT	O
ensembling	O
as	O
a	O
set	O
of	O
NMT	O
predictors	O
.	O
Ensembling	O
averages	O
the	O
individual	O
model	O
scores	O
in	O
each	O
decoding	O
step	O
.	O
Alternatively	O
,	O
system	O
-	O
level	O
combination	O
decodes	O
the	O
entire	O
sentence	O
with	O
each	O
model	O
separately	O
,	O
and	O
selects	O
the	O
best	O
scoring	O
complete	O
hypothesis	O
over	O
all	O
models	O
.	O
In	O
our	O
experiments	O
,	O
system	O
-	O
level	O
combination	O
is	O
not	O
as	O
effective	O
as	O
en	O
-	O
1080	O
)	O
,	O
(	O
b	O
)	O
a	O
different	O
training	O
and	O
test	O
set	O
,	O
(	O
c	O
)	O
a	O
slightly	O
different	O
network	O
architecture	O
,	O
and	O
(	O
d	O
)	O
words	O
rather	O
than	O
subword	O
units	O
.	O
8	O
https://marian	O
-	O
nmt.github.io/	O
features/	O
sembling	O
but	O
still	O
leads	O
to	O
moderate	O
gains	O
for	O
pure	O
NMT	O
.	O
However	O
,	O
a	O
trivial	O
implementation	O
which	O
selects	O
the	O
best	O
translation	O
in	O
a	O
postprocessing	O
step	O
after	O
separate	O
decoding	O
runs	O
is	O
slow	O
.	O
The	O
sepbeam	O
decoding	O
strategy	O
reduces	O
the	O
runtime	O
of	O
system	O
-	O
level	O
combination	O
to	O
the	O
single	O
system	O
level	O
.	O
The	O
strategy	O
applies	O
only	O
one	O
predictor	O
rather	O
than	O
a	O
linear	O
combination	O
of	O
all	O
predictors	O
to	O
expand	O
a	O
hypothesis	O
.	O
The	O
single	O
predictor	O
is	O
linked	O
by	O
the	O
parent	O
hypothesis	O
.	O
The	O
initial	O
stack	O
in	O
sepbeam	O
contains	O
hypotheses	O
for	O
each	O
predictor	O
(	O
i.e.	O
system	O
)	O
rather	O
than	O
only	O
one	O
as	O
in	O
normal	O
beam	O
search	O
.	O
We	O
report	O
a	O
moderate	O
gain	O
of	O
0.5	O
BLEU	B-MetricName
over	O
a	O
single	O
system	O
on	O
the	O
Japanese	O
-	O
English	O
ASPEC	B-DatasetName
test	O
set	O
(	O
Nakazawa	O
et	O
al	O
,	O
2016	O
)	O
by	O
combining	O
three	O
BPE	B-MethodName
-	O
based	O
NMT	O
models	O
from	O
using	O
the	O
sepbeam	O
decoder	O
.	O
Iterative	O
beam	O
search	O
Normal	O
beam	O
search	O
is	O
difficult	O
to	O
use	O
in	O
a	O
time	O
-	O
constrained	O
setting	O
since	O
the	O
runtime	O
depends	O
on	O
the	O
target	O
sentence	O
length	O
which	O
is	O
a	O
priori	O
not	O
known	O
,	O
and	O
it	O
is	O
therefore	O
hard	O
to	O
choose	O
the	O
right	O
beam	O
size	O
beforehand	O
.	O
The	O
bucket	O
search	O
algorithm	O
sidesteps	O
the	O
problem	O
of	O
setting	O
the	O
beam	O
size	O
by	O
repeatedly	O
performing	O
small	O
beam	O
search	O
passes	O
until	O
a	O
fixed	O
computational	O
budget	O
is	O
exhausted	O
.	O
Bucket	O
search	O
produces	O
an	O
initial	O
hypothesis	O
very	O
quickly	O
,	O
and	O
keeps	O
the	O
partial	O
hypotheses	O
for	O
each	O
length	O
in	O
buckets	O
.	O
Subsequent	O
beam	O
search	O
passes	O
refine	O
the	O
initial	O
hypothesis	O
by	O
iteratively	O
updating	O
these	O
buckets	O
.	O
Our	O
initial	O
experiments	O
suggest	O
that	O
bucket	O
search	O
often	O
performs	O
on	O
a	O
similar	O
level	O
as	O
standard	O
beam	O
search	O
with	O
the	O
benefit	O
of	O
being	O
able	O
to	O
support	O
hard	O
time	O
constraints	O
.	O
Unlike	O
beam	O
search	O
,	O
bucket	O
search	O
lends	O
itself	O
to	O
risk	O
-	O
free	O
(	O
i.e.	O
admissible	O
)	O
pruning	O
since	O
all	O
partial	O
hypotheses	O
worse	O
than	O
the	O
current	O
best	O
complete	O
hypothesis	O
can	O
be	O
discarded	O
.	O

Instead	O
of	O
applying	O
the	O
aforementioned	O
word	O
substitutions	O
directly	O
to	O
the	O
original	O
test	O
dataset	O
,	O
our	O
framework	O
perturbs	O
the	O
test	O
dataset	O
within	O
a	O
small	O
neighborhood	O
to	O
construct	O
similar	O
natural	O
sentences	O
.	O
This	O
is	O
to	O
identify	O
vulnerable	O
examples	O
with	O
respect	O
to	O
the	O
model	O
.	O
Note	O
that	O
examples	O
in	O
the	O
neighborhood	O
are	O
not	O
required	O
to	O
have	O
the	O
same	O
meaning	O
as	O
the	O
original	O
example	O
,	O
since	O
we	O
only	O
study	O
the	O
prediction	O
difference	O
caused	O
by	O
applying	O
synonym	O
substitution	O
p	O
(	O
2.1	O
)	O
.	O
Constraints	O
on	O
the	O
neighborhood	O
.	O
We	O
limit	O
the	O
neighborhood	O
sentences	O
within	O
a	O
small	O
0	B-DatasetName
norm	O
ball	O
(	O
regarding	O
the	O
test	O
instance	O
)	O
to	O
ensure	O
syntactic	O
similarity	O
,	O
and	O
empirically	O
ensure	O
the	O
naturalness	O
through	O
a	O
language	O
model	O
.	O
The	O
neighborhood	O
of	O
an	O
input	O
sentence	O
x	O
0	B-DatasetName
X	O
is	O
:	O
Neighbor	O
k	O
(	O
x	O
0	B-DatasetName
)	O
⊆	O
Ball	O
k	O
(	O
x	O
0	B-DatasetName
)	O
∩	O
X	O
natural	O
,	O
(	O
1	O
)	O
where	O
Ball	O
k	O
(	O
x	O
0	B-DatasetName
)	O
=	O
{	O
x	O
|	O
x	O
−	O
x	O
0	B-DatasetName
0	B-DatasetName
≤	O
k	O
,	O
x	O
X	O
}	O
is	O
the	O
0	B-DatasetName
norm	O
ball	O
around	O
x	O
0	B-DatasetName
(	O
i.e.	O
,	O
at	O
most	O
k	O
different	O
tokens	O
)	O
,	O
and	O
X	O
natural	O
denotes	O
natural	O
sentences	O
that	O
satisfy	O
a	O
certain	O
language	O
model	O
score	O
which	O
will	O
be	O
discussed	O
next	O
.	O
Construction	O
with	O
masked	O
language	O
model	O
.	O
We	O
construct	O
neighborhood	O
sentences	O
from	O
x	O
0	B-DatasetName
by	O
substituting	O
at	O
most	O
k	O
tokens	O
.	O
As	O
shown	O
in	O
Algorithm	O
1	O
,	O
the	O
construction	O
employs	O
a	O
recursive	O
approach	O
and	O
replaces	O
one	O
token	O
at	O
a	O
time	O
.	O
For	O
each	O
recursion	O
,	O
the	O
algorithm	O
first	O
masks	O
each	O
token	O
of	O
the	O
input	O
sentence	O
(	O
may	O
be	O
the	O
original	O
x	O
0	B-DatasetName
or	O
thex	O
from	O
last	O
recursion	O
)	O
separately	O
and	O
predicts	O
likely	O
replacements	O
with	O
a	O
masked	O
language	O
model	O
(	O
e.g.	O
,	O
DistilBERT	B-MethodName
,	O
Sanh	O
et	O
al	O
2019	O
)	O
.	O
To	O
ensure	O
the	O
naturalness	O
,	O
we	O
keep	O
the	O
top	O
20	O
tokens	O
for	O
each	O
mask	O
with	O
the	O
largest	O
logit	O
(	O
subject	O
to	O
a	O
threshold	O
,	O
Line	O
9	O
)	O
.	O
Then	O
,	O
the	O
algorithm	O
constructs	O
neighborhood	O
sentences	O
by	O
replacing	O
the	O
mask	O
with	O
found	O
tokens	O
.	O
We	O
use	O
the	O
notationx	O
in	O
the	O
following	O
sections	O
to	O
denote	O
the	O
constructed	O
sentences	O
within	O
the	O
neighborhood	O
.	O
lmin	O
max	O
{	O
L	O
(	O
κ	O
)	O
,	O
L	O
(	O
0	B-DatasetName
)	O
−	O
δ	B-HyperparameterName
}	O
;	O
(	O
i	O
)	O
denotes	O
the	O
ith	O
element	O
.	O
We	O
empirically	O
set	O
κ	O
20	O
and	O
δ	B-HyperparameterName
3	O
.	O
L	O
10	O
Tnew	O
{	O
t	O
|	O
l	O
>	O
lmin	O
,	O
(	O
t	O
,	O
l	O
)	O
T	O
×	O
L	O
}	O
;	O
11	O
Xnew	O
{	O
x0	O
|	O
x	O
(	O
i	O
)	O
0	B-DatasetName
t	O
,	O
t	O
Tnew	O
}	O
;	O
Construct	O
new	O
sentences	O
by	O
replacing	O
the	O
ith	O
token	O
.	O
12	O
Xneighbor	O
Xneighbor	O
∪	O
Xnew	O
;	O
13	O
return	O
Xneighbor	O
;	O

Adversarial	O
attacks	O
search	O
for	O
small	O
and	O
invariant	O
perturbations	O
on	O
the	O
model	O
input	O
that	O
can	O
alter	O
the	O
prediction	O
.	O
To	O
simplify	O
the	O
discussion	O
,	O
in	O
the	O
following	O
,	O
we	O
take	O
a	O
binary	O
classifier	O
f	O
(	O
x	O
)	O
:	O
X	O
{	O
0	B-DatasetName
,	O
1	O
}	O
as	O
an	O
example	O
to	O
describe	O
our	O
framework	O
.	O
Let	O
x	O
0	B-DatasetName
be	O
the	O
sentence	O
from	O
the	O
test	O
set	O
with	O
label	O
y	O
0	B-DatasetName
,	O
then	O
the	O
smallest	O
perturbation	O
δ	B-HyperparameterName
*	O
under	O
0	B-DatasetName
norm	O
distance	O
is	O
:	O
2	O
δ	B-HyperparameterName
*	O
:	O
=	O
argmin	O
δ	B-HyperparameterName
δ	B-HyperparameterName
0	B-DatasetName
s.t	O
.	O
f	O
(	O
x	O
0	B-DatasetName
δ	B-HyperparameterName
)	O
=	O
y	O
0	B-DatasetName
.	O
Here	O
δ	B-HyperparameterName
=	O
p	O
1	O
p	O
l	O
denotes	O
a	O
series	O
of	O
substitutions	O
.	O
In	O
contrast	O
,	O
our	O
second	O
-	O
order	O
attacks	O
fix	O
δ	B-HyperparameterName
=	O
p	O
and	O
search	O
for	O
the	O
vulnerable	O
x	O
0	B-DatasetName
.	O

A	O
naive	O
approach	O
for	O
solving	O
Eq	O
.	O
(	O
3	O
)	O
is	O
to	O
enumerate	O
through	O
Neighbor	O
k	O
(	O
x	O
0	B-DatasetName
)	O
.	O
The	O
enumeration	O
finds	O
the	O
smallest	O
perturbation	O
,	O
but	O
is	O
only	O
applicable	O
for	O
small	O
k	O
(	O
e.g.	O
,	O
k	O
≤	O
2	O
)	O
given	O
the	O
exponential	O
complexity	O
.	O
Beam	O
-	O
search	O
attack	O
(	O
SO	O
-	O
Beam	O
)	O
.	O
The	O
efficiency	O
can	O
be	O
improved	O
by	O
utilizing	O
the	O
probability	O
output	O
,	O
where	O
we	O
solve	O
Eq	O
.	O
(	O
3	O
)	O
by	O
minimizing	O
the	O
crossentropy	O
loss	B-MetricName
with	O
regard	O
to	O
x	O
Neighbor	O
k	O
(	O
x	O
0	B-DatasetName
)	O
:	O
L	O
(	O
x	O
;	O
p	O
)	O
:	O
=	O
−	O
log	O
(	O
1	O
−	O
f	O
min	O
)	O
−	O
log	O
(	O
f	O
max	O
)	O
,	O
(	O
4	O
)	O
where	O
f	O
min	O
and	O
f	O
max	O
are	O
the	O
smaller	O
and	O
the	O
larger	O
output	O
probability	O
between	O
f	O
soft	O
(	O
x	O
)	O
and	O
f	O
soft	O
(	O
x	O
3	O
We	O
assume	O
a	O
binary	O
classification	O
task	O
,	O
but	O
our	O
framework	O
is	O
general	O
and	O
can	O
be	O
extended	O
to	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
.	O
p	O
)	O
,	O
respectively	O
.	O
Minimizing	O
Eq	O
.	O
(	O
4	O
)	O
effectively	O
leads	O
to	O
f	O
min	O
0	B-DatasetName
and	O
f	O
max	O
1	O
,	O
and	O
we	O
use	O
a	O
beam	O
search	O
to	O
find	O
the	O
best	O
x.	O
At	O
each	O
iteration	O
,	O
we	O
construct	O
sentences	O
through	O
Neighbor	O
1	O
(	O
x	O
)	O
and	O
only	O
keep	O
the	O
top	O
20	O
sentences	O
with	O
the	O
smallest	O
L	O
(	O
x	O
;	O
p	O
)	O
.	O
We	O
run	O
at	O
most	O
k	O
iterations	O
,	O
and	O
stop	O
earlier	O
if	O
we	O
find	O
a	O
vulnerable	O
example	O
.	O
We	O
provide	O
the	O
detailed	O
implementation	O
in	O
Algorithm	O
2	O
and	O
a	O
flowchart	O
in	O
Fig	O
.	O
3	O
.	O
4	O
for	O
i	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
do	O
5	O
Xnew	O
x	O
X	O
beam	O
Neighbor	O
1	O
(	O
x	O
)	O
;	O
6x0	O
argmax	O
x	O
Xnew	O
|	O
F	O
(	O
x	O
;	O
p	O
)	O
|	O
;	O
7	O
if	O
F	O
(	O
x0	O
;	O
p	O
)	O
=	O
0	B-DatasetName
then	O
returnx0	O
;	O
8	O
Xnew	O
SortIncreasing	O
(	O
Xnew	O
,	O
L	O
)	O
;	O
9	O
Xbeam	O
{	O
X	O
(	O
0	B-DatasetName
)	O
new	O
,	O
.	O
.	O
.	O
,	O
X	O
(	O
β−1	O
)	O
new	O
}	O
;	O
Keep	O
the	O
best	O
beam	O
.	O
We	O
set	O
β	B-HyperparameterName
20	O
.	O
10	O
return	O
None	O
;	O

Base	O
models	O
.	O
For	O
BoW	O
,	O
CNN	O
,	O
and	O
LSTM	B-MethodName
,	O
all	O
models	O
use	O
pre	O
-	O
trained	O
GloVe	B-MethodName
embeddings	I-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
and	O
have	O
one	O
hidden	O
layer	O
of	O
the	O
corresponding	O
type	O
with	O
100	O
hidden	O
size	O
.	O
Similar	O
to	O
the	O
baseline	O
performance	O
reported	O
in	O
GLUE	B-DatasetName
,	O
our	O
trained	O
models	O
have	O
an	O
evaluation	O
accuracy	B-MetricName
of	O
81.4	O
%	O
,	O
82.5	O
%	O
,	O
and	O
81.7	O
%	O
,	O
respectively	O
.	O
For	O
attention	O
-	O
based	O
models	O
,	O
we	O
train	O
a	O
3	O
-	O
layer	O
Transformer	B-MethodName
(	O
the	O
largest	O
size	O
in	O
)	O
and	O
fine	O
-	O
tune	O
a	O
pre	O
-	O
trained	O
bertbase	O
-	O
uncased	O
from	O
HuggingFace	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
(	O
Morris	O
et	O
al	O
,	O
2020	O
)	O
.	O
Attack	O
success	O
rate	O
(	O
second	O
-	O
order	O
)	O
.	O
We	O
also	O
quantify	O
second	O
-	O
order	O
robustness	O
through	O
attack	O
success	O
rate	O
,	O
which	O
measures	O
the	O
ratio	O
of	O
test	O
examples	O
that	O
a	O
vulnerable	O
example	O
can	O
be	O
found	O
.	O
To	O
evaluate	O
the	O
impact	O
of	O
neighborhood	O
size	O
,	O
we	O
experiment	O
with	O
two	O
configurations	O
:	O
(	O
1	O
)	O
For	O
the	O
small	O
neighborhood	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
2	O
)	O
,	O
we	O
use	O
SO	O
-	O
Enum	O
that	O
finds	O
the	O
most	O
similar	O
vulnerable	O
example	O
.	O
(	O
2	O
)	O
For	O
the	O
large	O
neighborhood	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
6	O
)	O
,	O
SO	O
-	O
Enum	O
is	O
not	O
applicable	O
and	O
we	O
use	O
SO	O
-	O
Beam	O
to	O
find	O
vulnerable	O
examples	O
.	O
We	O
consider	O
the	O
most	O
challenging	O
setup	O
and	O
use	O
patch	O
words	O
p	O
from	O
the	O
same	O
set	O
of	O
counter	O
-	O
fitted	O
synonyms	O
as	O
robust	O
models	O
(	O
they	O
are	O
provably	O
robust	O
to	O
these	O
synonyms	O
on	O
the	O
test	O
set	O
)	O
.	O
We	O
also	O
provide	O
a	O
random	O
baseline	O
to	O
validate	O
the	O
effectiveness	O
of	O
minimizing	O
Eq	O
.	O
(	O
4	O
)	O
(	O
Appendix	O
A.1	O
)	O
.	O

We	O
experiment	O
with	O
the	O
validation	O
split	O
(	O
872	O
examples	O
)	O
on	O
a	O
single	O
RTX	O
3090	O
.	O
The	O
average	O
running	O
time	O
per	O
example	O
(	O
in	O
seconds	O
)	O
on	O
base	O
LSTM	B-MethodName
is	O
31.9	O
for	O
Genetic	O
,	O
1.1	O
for	O
BAE	O
,	O
7.0	O
for	O
SO	O
-	O
Enum	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
2	O
)	O
,	O
and	O
1.9	O
for	O
SO	O
-	O
Beam	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
6	O
)	O
.	O
We	O
provide	O
additional	O
running	O
time	O
results	O
in	O
Appendix	O
A.3	O
.	O
Table	O
1	O
provides	O
an	O
example	O
of	O
the	O
attack	O
result	O
where	O
all	O
attacks	O
are	O
successful	O
(	O
additional	O
examples	O
in	O
Appendix	O
A.5	O
)	O
.	O
As	O
shown	O
,	O
our	O
secondorder	O
attacks	O
find	O
a	O
vulnerable	O
example	O
by	O
replacing	O
grease	O
musicals	O
,	O
and	O
the	O
vulnerable	O
example	O
has	O
different	O
predictions	O
for	O
bad	O
and	O
unhealthy	O
.	O
Note	O
that	O
,	O
Genetic	O
and	O
BAE	O
have	O
different	O
objectives	O
from	O
second	O
-	O
order	O
attacks	O
and	O
focus	O
on	O
finding	O
the	O
adversarial	O
example	O
.	O
Next	O
we	O
discuss	O
the	O
results	O
from	O
two	O
perspectives	O
.	O
Second	O
-	O
order	O
robustness	O
.	O
We	O
observe	O
that	O
existing	O
robustly	O
trained	O
models	O
are	O
not	O
second	O
-	O
order	O
robust	O
.	O
As	O
shown	O
in	O
Furthermore	O
,	O
applying	O
existing	O
attacks	O
on	O
the	O
vulnerable	O
examples	O
constructed	O
by	O
our	O
method	O
will	O
lead	O
to	O
much	O
smaller	O
perturbations	O
.	O
As	O
a	O
reference	O
,	O
on	O
the	O
robustly	O
trained	O
CNN	O
,	O
Genetic	O
attack	O
constructs	O
adversarial	O
examples	O
by	O
perturbing	O
2.7	O
words	O
on	O
average	O
(	O
starting	O
from	O
the	O
input	O
examples	O
)	O
.	O
However	O
,	O
if	O
Genetic	O
starts	O
from	O
our	O
vulnerable	O
examples	O
,	O
it	O
would	O
only	O
need	O
to	O
perturb	O
a	O
single	O
word	O
(	O
i.e.	O
,	O
the	O
patch	O
words	O
p	O
)	O
to	O
alter	O
the	O
prediction	O
.	O
These	O
results	O
demonstrate	O
the	O
weakness	O
of	O
the	O
models	O
(	O
even	O
robustly	O
trained	O
)	O
for	O
those	O
inputs	O
beyond	O
the	O
test	O
set	O
.	O

In	O
contrast	O
to	O
second	O
-	O
order	O
robustness	O
,	O
where	O
we	O
consider	O
the	O
model	O
vulnerable	O
as	O
long	O
as	O
there	O
exists	O
one	O
vulnerable	O
example	O
,	O
counterfactual	O
bias	O
focuses	O
on	O
the	O
expected	O
prediction	O
,	O
which	O
is	O
the	O
average	O
prediction	O
among	O
all	O
examples	O
within	O
the	O
neighborhood	O
.	O
We	O
consider	O
a	O
model	O
biased	O
if	O
the	O
expected	O
predictions	O
for	O
protected	O
groups	O
are	O
different	O
(	O
assuming	O
the	O
model	O
is	O
not	O
intended	O
to	O
discriminate	O
between	O
these	O
groups	O
)	O
.	O
For	O
instance	O
,	O
a	O
sentiment	O
classifier	O
is	O
biased	O
if	O
the	O
expected	O
prediction	O
for	O
inputs	O
containing	O
woman	O
is	O
more	O
positive	O
(	O
or	O
negative	O
)	O
than	O
inputs	O
containing	O
man	O
.	O
Such	O
bias	O
is	O
harmful	O
as	O
they	O
may	O
make	O
unfair	O
decisions	O
based	O
on	O
protected	O
attributes	O
,	O
for	O
example	O
in	O
situations	O
such	O
as	O
hiring	O
and	O
college	O
admission	O
.	O
Counterfactual	O
token	O
bias	O
.	O
We	O
study	O
a	O
narrow	O
case	O
of	O
counterfactual	O
bias	O
,	O
where	O
counterfactual	O
examples	O
are	O
constructed	O
by	O
substituting	O
protected	O
tokens	O
in	O
the	O
input	O
.	O
A	O
naive	O
approach	O
of	O
measuring	O
this	O
bias	O
is	O
to	O
construct	O
counterfactual	O
examples	O
directly	O
from	O
the	O
test	O
set	O
,	O
however	O
such	O
evaluation	O
may	O
not	O
be	O
robust	O
since	O
test	O
examples	O
are	O
only	O
a	O
small	O
subset	O
of	O
natural	O
sentences	O
.	O
Formally	O
,	O
let	O
p	O
be	O
a	O
pair	O
of	O
protected	O
tokens	O
such	O
as	O
(	O
he	O
,	O
she	O
)	O
or	O
(	O
Asian	O
,	O
American	O
)	O
,	O
X	O
test	O
⊆	O
X	O
p	O
be	O
a	O
test	O
set	O
(	O
as	O
in	O
2.1	O
)	O
,	O
we	O
define	O
counterfactual	O
token	O
bias	O
by	O
:	O
B	O
p	O
,	O
k	O
:	O
=	O
E	O
x	O
Neighbor	O
k	O
(	O
Xtest	O
)	O
F	O
soft	O
(	O
x	O
;	O
p	O
)	O
.	O
(	O
5	O
)	O
We	O
calculate	O
Eq	O
.	O
(	O
5	O
)	O
through	O
an	O
enumeration	O
across	O
all	O
natural	O
sentences	O
within	O
the	O
neighborhood	O
.	O
7	O
Here	O
Neighbor	O
k	O
(	O
X	O
test	O
)	O
=	O
x	O
Xtest	O
Neighbor	O
k	O
(	O
x	O
)	O
denotes	O
the	O
union	O
of	O
neighborhood	O
examples	O
(	O
of	O
distance	O
k	O
)	O
around	O
the	O
test	O
set	O
,	O
and	O
F	O
soft	O
(	O
x	O
;	O
p	O
)	O
:	O
X	O
×	O
V	O
2	O
[	O
−1	O
,	O
1	O
]	O
denotes	O
the	O
difference	O
between	O
probability	O
outputs	O
f	O
soft	O
(	O
similar	O
to	O
Eq	O
.	O
(	O
2	O
)	O
)	O
:	O
F	O
soft	O
(	O
x	O
;	O
p	O
)	O
:	O
=	O
f	O
soft	O
(	O
x	O
p	O
)	O
−	O
f	O
soft	O
(	O
x	O
)	O
.	O
(	O
6	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
)	O
in	O
X	O
filter	O
.	O
The	O
model	O
is	O
unbiased	O
on	O
p	O
if	O
B	O
p	O
,	O
k	O
≈	O
0	B-DatasetName
,	O
whereas	O
a	O
positive	O
or	O
negative	O
B	O
p	O
,	O
k	O
indicates	O
that	O
the	O
model	O
shows	O
preference	O
or	O
against	O
to	O
p	O
(	O
2	O
)	O
,	O
respectively	O
.	O
Fig	O
.	O
4	O
illustrates	O
the	O
distribution	O
of	O
(	O
x	O
,	O
x	O
p	O
)	O
for	O
both	O
an	O
unbiased	O
model	O
and	O
a	O
biased	O
model	O
.	O
The	O
aforementioned	O
neighborhood	O
construction	O
does	O
not	O
introduce	O
additional	O
bias	O
.	O
For	O
instance	O
,	O
let	O
x	O
0	B-DatasetName
be	O
a	O
sentence	O
containing	O
he	O
,	O
even	O
though	O
it	O
is	O
possible	O
for	O
Neighbor	O
1	O
(	O
x	O
0	B-DatasetName
)	O
to	O
contain	O
many	O
stereotyping	O
sentences	O
(	O
e.g.	O
,	O
contains	O
tokens	O
such	O
as	O
doctor	O
and	O
driving	O
)	O
that	O
affect	O
the	O
distribution	O
of	O
f	O
soft	O
(	O
x	O
)	O
,	O
but	O
it	O
does	O
not	O
bias	O
Eq	O
.	O
(	O
6	O
)	O
as	O
we	O
only	O
care	O
about	O
the	O
prediction	O
difference	O
of	O
replacing	O
he	O
she	O
.	O
The	O
construction	O
has	O
no	O
information	O
about	O
the	O
model	O
objective	O
,	O
thus	O
it	O
would	O
be	O
difficult	O
to	O
bias	O
f	O
soft	O
(	O
x	O
)	O
and	O
f	O
soft	O
(	O
x	O
p	O
)	O
differently	O
.	O

We	O
evaluate	O
counterfactual	O
token	O
bias	O
on	O
the	O
SST	B-DatasetName
-	O
2	O
dataset	O
with	O
both	O
the	O
base	O
and	O
debiased	O
models	O
.	O
We	O
focus	O
on	O
binary	O
gender	O
bias	O
and	O
set	O
p	O
to	O
pairs	O
of	O
gendered	O
pronouns	O
from	O
Zhao	O
et	O
al	O
(	O
2018a	O
)	O
.	O
Base	O
Model	O
.	O
We	O
train	O
a	O
single	O
layer	O
LSTM	B-MethodName
with	O
pre	O
-	O
trained	O
GloVe	B-MethodName
embeddings	I-MethodName
and	O
75	O
hidden	O
size	O
(	O
from	O
TextAttack	O
,	O
Morris	O
et	O
al	O
2020	O
)	O
.	O
The	O
model	O
has	O
82.9	O
%	O
accuracy	B-MetricName
similar	O
to	O
the	O
baseline	O
performance	O
reported	O
in	O
GLUE	B-DatasetName
.	O
Debiased	O
Model	O
.	O
Data	O
-	O
augmentation	O
with	O
gender	O
swapping	O
has	O
been	O
shown	O
effective	O
in	O
mitigating	O
gender	O
bias	O
(	O
Zhao	O
et	O
al	O
,	O
2018a	O
.	O
We	O
augment	O
the	O
training	O
split	O
by	O
swapping	O
all	O
male	O
entities	O
with	O
the	O
corresponding	O
female	O
entities	O
and	O
vice	O
-	O
versa	O
.	O
We	O
use	O
the	O
same	O
setup	O
as	O
the	O
base	O
LSTM	B-MethodName
and	O
attain	O
82.45	O
%	O
accuracy	B-MetricName
.	O
Here	O
"	O
original	O
"	O
is	O
equivalent	O
to	O
k	B-HyperparameterName
=	I-HyperparameterName
0	B-DatasetName
,	O
"	O
perturbed	O
"	O
is	O
equivalent	O
to	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
,	O
p	O
is	O
in	O
the	O
form	O
of	O
(	O
male	O
,	O
female	O
)	O
.	O
Metrics	O
.	O
We	O
evaluate	O
model	O
bias	O
through	O
the	O
proposed	O
B	O
p	O
,	O
k	O
for	O
k	B-HyperparameterName
=	I-HyperparameterName
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
3	O
.	O
Here	O
the	O
bias	O
for	O
k	B-HyperparameterName
=	I-HyperparameterName
0	B-DatasetName
is	O
effectively	O
measured	O
on	O
the	O
original	O
test	O
set	O
,	O
and	O
the	O
bias	O
for	O
k	O
≥	O
1	O
is	O
measured	O
on	O
our	O
constructed	O
neighborhood	O
.	O
We	O
randomly	O
sample	O
a	O
subset	O
of	O
constructed	O
examples	O
when	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
due	O
to	O
the	O
exponential	O
complexity	O
.	O
Filtered	O
test	O
set	O
.	O
To	O
investigate	O
whether	O
our	O
method	O
is	O
able	O
to	O
reveal	O
model	O
bias	O
that	O
was	O
hidden	O
in	O
the	O
test	O
set	O
,	O
we	O
construct	O
a	O
filtered	O
test	O
set	O
on	O
which	O
the	O
bias	O
can	O
not	O
be	O
observed	O
directly	O
.	O
Let	O
X	O
test	O
be	O
the	O
original	O
validation	O
split	O
,	O
we	O
construct	O
X	O
filter	O
by	O
the	O
equation	O
below	O
and	O
empirically	O
set	O
=	O
0.005	O
.	O
We	O
provide	O
statistics	O
in	O
Table	O
5	O
.	O
X	O
filter	O
:	O
=	O
{	O
x	O
|	O
|	O
F	O
soft	O
(	O
x	O
;	O
p	O
)	O
|	O
<	O
,	O
x	O
X	O
test	O
}	O
.	O

Our	O
method	O
is	O
able	O
to	O
reveal	O
the	O
hidden	O
model	O
bias	O
on	O
X	O
filter	O
,	O
which	O
is	O
not	O
visible	O
with	O
naive	O
measurements	O
.	O
In	O
Fig	O
.	O
5	O
,	O
the	O
naive	O
approach	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
0	B-DatasetName
)	O
observes	O
very	O
small	O
biases	O
on	O
most	O
tokens	O
(	O
as	O
constructed	O
)	O
.	O
In	O
contrast	O
,	O
when	O
evaluated	O
by	O
our	O
double	O
perturbation	O
framework	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
)	O
,	O
we	O
are	O
able	O
to	O
observe	O
noticeable	O
bias	O
,	O
where	O
most	O
p	O
has	O
a	O
positive	O
bias	O
on	O
the	O
base	O
model	O
.	O
This	O
observed	O
bias	O
is	O
in	O
line	O
with	O
the	O
measurements	O
on	O
the	O
original	O
X	O
test	O
(	O
Appendix	O
A.4	O
)	O
,	O
indicating	O
that	O
we	O
reveal	O
the	O
correct	O
model	O
bias	O
.	O
Furthermore	O
,	O
we	O
observe	O
mitigated	O
biases	O
in	O
the	O
debiased	O
model	O
,	O
which	O
demonstrates	O
the	O
effectiveness	O
of	O
data	B-TaskName
augmentation	I-TaskName
.	O
To	O
demonstrate	O
how	O
our	O
method	O
reveals	O
hidden	O
bias	O
,	O
we	O
conduct	O
a	O
case	O
study	O
with	O
p	O
=	O
(	O
actor	O
,	O
actress	O
)	O
and	O
show	O
the	O
relationship	O
between	O
the	O
bias	O
B	O
p	O
,	O
k	O
and	O
the	O
neighborhood	O
distance	O
k.	O
We	O
present	O
the	O
histograms	O
for	O
F	O
soft	O
(	O
x	O
;	O
p	O
)	O
in	O
Fig	O
.	O
6	O
and	O
plot	O
the	O
corresponding	O
B	O
p	O
,	O
k	O
vs.	O
k	O
in	O
the	O
right	O
-	O
most	O
panel	O
.	O
Surprisingly	O
,	O
for	O
the	O
base	O
model	O
,	O
the	O
bias	O
is	O
Figure	O
6	O
:	O
Left	O
and	O
Middle	O
:	O
Histograms	O
for	O
F	O
soft	O
(	O
x	O
;	O
p	O
)	O
(	O
x	O
-	O
axis	O
)	O
with	O
p	O
=	O
(	O
actor	O
,	O
actress	O
)	O
.	O
Right	O
:	O
The	O
plot	O
for	O
the	O
average	O
F	O
soft	O
(	O
x	O
;	O
p	O
)	O
(	O
i.e.	O
,	O
counterfactual	O
token	O
bias	O
)	O
vs.	O
neighborhood	O
distance	O
k.	O
Results	O
show	O
that	O
the	O
counterfactual	O
bias	O
on	O
p	O
can	O
be	O
revealed	O
when	O
increasing	O
k.	O
negative	O
when	O
k	B-HyperparameterName
=	I-HyperparameterName
0	B-DatasetName
,	O
but	O
becomes	O
positive	O
when	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
.	O
This	O
is	O
because	O
the	O
naive	O
approach	O
only	O
has	O
two	O
test	O
examples	O
(	O
Table	O
5	O
)	O
thus	O
the	O
measurement	O
is	O
not	O
robust	O
.	O
In	O
contrast	O
,	O
our	O
method	O
is	O
able	O
to	O
construct	O
141	O
,	O
780	O
similar	O
natural	O
sentences	O
when	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
and	O
shifts	O
the	O
distribution	O
to	O
the	O
right	O
(	O
positive	O
)	O
.	O
As	O
shown	O
in	O
the	O
right	O
-	O
most	O
panel	O
,	O
the	O
bias	O
is	O
small	O
when	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
,	O
and	O
becomes	O
more	O
significant	O
as	O
k	O
increases	O
(	O
larger	O
neighborhood	O
)	O
.	O
As	O
discussed	O
in	O
4.1	O
,	O
the	O
neighborhood	O
construction	O
does	O
not	O
introduce	O
additional	O
bias	O
,	O
and	O
these	O
results	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
method	O
in	O
revealing	O
hidden	O
model	O
bias	O
.	O

We	O
experiment	O
with	O
the	O
validation	O
split	O
on	O
a	O
single	O
RTX	O
3090	O
,	O
and	O
measure	O
the	O
average	O
running	O
time	O
per	O
example	O
.	O
As	O
shown	O
in	O
A.4	O
Additional	O
Results	O
on	O
Protected	O
Tokens	O
Fig	O
.	O
7	O
presents	O
the	O
experimental	O
results	O
with	O
additional	O
protected	O
tokens	O
such	O
as	O
nationality	O
,	O
religion	O
,	O
and	O
sexual	O
orientation	O
(	O
from	O
Ribeiro	O
et	O
al	O
(	O
2020	O
)	O
)	O
.	O
We	O
use	O
the	O
same	O
base	O
LSTM	B-MethodName
as	O
described	O
in	O
4.2	O
.	O
One	O
interesting	O
observation	O
is	O
when	O
p	O
=	O
(	O
gay	O
,	O
straight	O
)	O
where	O
the	O
bias	O
is	O
negative	O
,	O
indicating	O
that	O
the	O
sentiment	O
classifier	O
tends	O
to	O
give	O
more	O
negative	O
prediction	O
when	O
substituting	O
gay	O
straight	O
in	O
the	O
input	O
.	O
This	O
phenomenon	O
is	O
opposite	O
to	O
the	O
behavior	O
of	O
toxicity	O
classifiers	O
(	O
Dixon	O
et	O
al	O
,	O
2018	O
)	O
In	O
Fig	O
.	O
8	O
,	O
we	O
measure	O
the	O
bias	O
on	O
X	O
test	O
and	O
observe	O
positive	O
bias	O
on	O
most	O
tokens	O
for	O
both	O
k	B-HyperparameterName
=	I-HyperparameterName
0	B-DatasetName
and	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
,	O
which	O
indicates	O
that	O
the	O
model	O
"	O
tends	O
"	O
to	O
make	O
more	O
positive	O
predictions	O
for	O
examples	O
containing	O
certain	O
female	O
pronouns	O
than	O
male	O
pro	O
-	O
nouns	O
.	O
Notice	O
that	O
even	O
though	O
gender	O
swap	O
mitigates	O
the	O
bias	O
to	O
some	O
extent	O
,	O
it	O
is	O
still	O
difficult	O
to	O
fully	O
eliminate	O
the	O
bias	O
.	O
This	O
is	O
probably	O
caused	O
by	O
tuples	O
like	O
(	O
him	O
,	O
his	O
,	O
her	O
)	O
which	O
can	O
not	O
be	O
swapped	O
perfectly	O
,	O
and	O
requires	O
additional	O
processing	O
such	O
as	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
resolving	O
(	O
Zhao	O
et	O
al	O
,	O
2018a	O
)	O
.	O
To	O
help	O
evaluate	O
the	O
naturalness	O
of	O
our	O
constructed	O
examples	O
used	O
in	O
4	O
,	O
we	O
provide	O
sample	O
sentences	O
in	O
Table	O
9	O
and	O
Table	O
10	O
.	O
Bold	O
words	O
are	O
the	O
corresponding	O
patch	O
words	O
p	O
,	O
taken	O
from	O
the	O
predefined	O
list	O
of	O
gendered	O
pronouns	O
.	O
Distance	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
97	O
%	O
Negative	O
(	O
97	O
%	O
Negative	O
)	O
it	O
's	O
hampered	O
by	O
a	O
lifetime	O
-	O
channel	O
kind	O
of	O
plot	O
and	O
lone	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
is	O
out	O
of	O
their	O
depth	O
.	O
56	O
%	O
Negative	O
(	O
55	O
%	O
Positive	O
)	O
it	O
's	O
hampered	O
by	O
a	O
lifetime	O
-	O
channel	O
kind	O
of	O
plot	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
is	O
out	O
of	O
creative	O
depth	O
.	O
89	O
%	O
Negative	O
(	O
84	O
%	O
Negative	O
)	O
it	O
's	O
hampered	O
by	O
a	O
lifetime	O
-	O
channel	O
kind	O
of	O
plot	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
talks	O
out	O
of	O
their	O
depth	O
.	O
98	O
%	O
Negative	O
(	O
98	O
%	O
Negative	O
)	O
it	O
's	O
hampered	O
by	O
a	O
lifetime	O
-	O
channel	O
kind	O
of	O
plot	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
is	O
out	O
of	O
production	O
depth	O
.	O
96	O
%	O
Negative	O
(	O
96	O
%	O
Negative	O
)	O
it	O
's	O
hampered	O
by	O
a	O
lifetime	O
-	O
channel	O
kind	O
of	O
plot	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
that	O
is	O
out	O
of	O
their	O
depth	O
.	O

Distance	O
k	B-HyperparameterName
=	I-HyperparameterName
2	O
88	O
%	O
Negative	O
(	O
87	O
%	O
Negative	O
)	O
it	O
's	O
hampered	O
by	O
a	O
lifetime	O
-	O
channel	O
cast	O
of	O
stars	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
is	O
out	O
of	O
their	O
depth	O
.	O
96	O
%	O
Negative	O
(	O
95	O
%	O
Negative	O
)	O
it	O
's	O
hampered	O
by	O
a	O
simple	O
set	O
of	O
plot	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
is	O
out	O
of	O
their	O
depth	O
.	O
54	O
%	O
Negative	O
(	O
54	O
%	O
Negative	O
)	O
it	O
's	O
framed	O
about	O
a	O
lifetime	O
-	O
channel	O
kind	O
of	O
plot	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
is	O
out	O
of	O
their	O
depth	O
.	O
90	O
%	O
Negative	O
(	O
88	O
%	O
Negative	O
)	O
it	O
's	O
hampered	O
by	O
a	O
lifetime	O
-	O
channel	O
mix	O
between	O
plot	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
is	O
out	O
of	O
their	O
depth	O
.	O
78	O
%	O
Negative	O
(	O
68	O
%	O
Negative	O
)	O
it	O
's	O
hampered	O
by	O
a	O
lifetime	O
-	O
channel	O
kind	O
of	O
plot	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
storms	O
out	O
of	O
their	O
mind	O
.	O
Distance	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
52	O
%	O
Positive	O
(	O
64	O
%	O
Positive	O
)	O
it	O
's	O
characterized	O
by	O
a	O
lifetime	O
-	O
channel	O
combination	O
comedy	O
plot	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
is	O
out	O
of	O
their	O
depth	O
.	O
93	O
%	O
Negative	O
(	O
93	O
%	O
Negative	O
)	O
it	O
's	O
hampered	O
by	O
a	O
lifetime	O
-	O
channel	O
kind	O
of	O
star	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
falls	O
out	O
of	O
their	O
depth	O
.	O
58	O
%	O
Negative	O
(	O
57	O
%	O
Negative	O
)	O
it	O
's	O
hampered	O
by	O
a	O
tough	O
kind	O
of	O
singer	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
is	O
out	O
of	O
their	O
teens	O
.	O
70	O
%	O
Negative	O
(	O
52	O
%	O
Negative	O
)	O
it	O
's	O
hampered	O
with	O
a	O
lifetime	O
-	O
channel	O
kind	O
of	O
plot	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
operates	O
regardless	O
of	O
their	O
depth	O
.	O
58	O
%	O
Negative	O
(	O
53	O
%	O
Positive	O
)	O
it	O
's	O
hampered	O
with	O
a	O
lifetime	O
-	O
channel	O
cast	O
of	O
plot	O
and	O
a	O
lead	O
actor	O
(	O
actress	O
)	O
who	O
is	O
out	O
of	O
creative	O
depth	O
.	O

Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
is	O
one	O
of	O
the	O
most	O
fundamental	O
and	O
important	O
tasks	O
in	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
.	O
While	O
the	O
literature	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Akbik	O
et	O
al	O
,	O
2018	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
largely	O
focuses	O
on	O
training	O
deep	O
language	O
models	O
to	O
improve	O
the	O
contextualized	O
word	O
representations	O
,	O
previous	O
studies	O
show	O
that	O
the	O
structured	O
information	O
such	O
as	O
interactions	O
between	O
non	O
-	O
adjacent	O
words	O
can	O
also	O
be	O
important	O
for	O
NER	B-TaskName
(	O
Finkel	O
et	O
al	O
,	O
2005	O
;	O
Jie	O
et	O
al	O
,	O
2017	O
;	O
Aguilar	O
and	O
Solorio	O
,	O
2019	O
)	O
.	O
However	O
,	O
sequence	O
models	O
such	O
as	O
bidirectional	B-MethodName
LSTM	I-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
are	O
not	O
able	O
to	O
fully	O
capture	O
the	O
long	O
-	O
range	O
dependencies	O
(	O
Bengio	O
,	O
2009	O
)	O
.	O
For	O
instance	O
,	O
Figure	O
1	O
(	O
top	O
)	O
shows	O
one	O
type	O
of	O
structured	O
information	O
in	O
NER	B-TaskName
.	O
The	O
words	O
"	O
Precision	B-MetricName
Castparts	O
Corp.	O
"	O
can	O
be	O
easily	O
inferred	O
as	O
ORGANIZATION	O
by	O
its	O
context	O
(	O
i.e.	O
,	O
Corp.	O
)	O
.	O
However	O
,	O
the	O
second	O
entity	O
"	O
PCP	O
"	O
could	O
be	O
misclassified	O
as	O
a	O
PRODUCT	O
entity	O
if	O
a	O
model	O
relies	O
more	O
on	O
the	O
context	O
"	O
begin	O
trading	O
with	O
"	O
but	O
ignores	O
the	O
hidden	O
information	O
that	O
"	O
PCP	O
"	O
is	O
the	O
symbol	O
of	O
"	O
Precision	B-MetricName
Castparts	O
Corp.	O
"	O
.	O
Previous	O
research	O
works	O
(	O
Li	O
et	O
al	O
,	O
2017	O
;	O
Jie	O
and	O
Lu	O
,	O
2019	O
;	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
have	O
been	O
using	O
the	O
parse	O
trees	O
(	O
Chomsky	O
,	O
1956	O
(	O
Chomsky	O
,	O
,	O
1969Sandra	O
and	O
Taft	O
,	O
2014	O
)	O
to	O
incorporate	O
such	O
structured	O
information	O
.	O
Figure	O
1	O
(	O
Dependency	O
Path	O
)	O
shows	O
that	O
the	O
first	O
entity	O
can	O
be	O
connected	O
to	O
the	O
second	O
entity	O
following	O
the	O
dependency	O
tree	O
with	O
5	O
hops	O
.	O
Incorporating	O
the	O
dependency	O
information	O
can	O
be	O
done	O
with	O
graph	O
neural	O
networks	O
(	O
GNNs	O
)	O
such	O
as	O
graph	O
convolutional	O
networks	O
(	O
GCNs	O
)	O
(	O
Kipf	O
and	O
Welling	O
,	O
2017	O
)	O
.	O
However	O
,	O
simply	O
stacking	O
the	O
LSTM	B-MethodName
and	O
GCN	B-MethodName
architectures	O
for	O
NER	B-TaskName
can	O
only	O
provide	O
us	O
with	O
modest	O
improvements	O
;	O
sometimes	O
,	O
it	O
decreases	O
performance	O
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
.	O
Based	O
on	O
the	O
depen	O
-	O
dency	O
path	O
in	O
Figure	O
1	O
,	O
it	O
requires	O
a	O
5	O
-	O
layer	O
GCN	B-MethodName
to	O
capture	O
the	O
connections	O
between	O
these	O
two	O
entities	O
.	O
However	O
,	O
deep	O
GCN	B-MethodName
architectures	O
often	O
face	O
training	O
difficulties	O
,	O
which	O
cause	O
a	O
performance	O
drop	O
(	O
Hamilton	O
et	O
al	O
,	O
2017b	O
;	O
Kipf	O
and	O
Welling	O
,	O
2017	O
)	O
.	O
Directly	O
stacking	O
GCN	B-MethodName
and	O
LSTM	B-MethodName
has	O
difficulties	O
in	O
modeling	O
the	O
interaction	O
between	O
dependency	O
trees	O
and	O
contextual	O
information	O
.	O
To	O
address	O
the	O
above	O
limitations	O
,	O
we	O
propose	O
the	O
Synergized	O
-	O
LSTM	B-MethodName
(	O
Syn	O
-	O
LSTM	B-MethodName
)	O
,	O
a	O
new	O
recurrent	O
neural	O
network	O
architecture	O
that	O
considers	O
an	O
additional	O
graph	O
-	O
encoded	O
representation	O
to	O
update	O
the	O
memory	O
and	O
hidden	O
states	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O
More	O
specifically	O
,	O
the	O
graph	O
-	O
encoded	O
representation	O
for	O
each	O
word	O
can	O
be	O
obtained	O
with	O
GCNs	O
.	O
Our	O
proposed	O
Syn	O
-	O
LSTM	B-MethodName
allows	O
the	O
cell	O
to	O
receive	O
the	O
structured	O
information	O
from	O
the	O
graph	O
-	O
encoded	O
representation	O
.	O
With	O
the	O
newly	O
designed	O
gating	O
mechanism	O
,	O
our	O
model	O
is	O
able	O
to	O
make	O
independent	O
assessments	O
on	O
the	O
amounts	O
of	O
information	O
to	O
be	O
retrieved	O
from	O
the	O
word	O
representation	O
and	O
the	O
graph	O
-	O
encoded	O
representation	O
respectively	O
.	O
Such	O
a	O
mechanism	O
allows	O
for	O
better	O
integration	O
of	O
both	O
contextual	O
and	O
structured	O
information	O
.	O
Our	O
contributions	O
can	O
be	O
summarized	O
as	O
:	O
We	O
propose	O
a	O
simple	O
and	O
robust	O
Syn	O
-	O
LSTM	B-MethodName
model	O
to	O
better	O
incorporate	O
the	O
structured	O
information	O
conveyed	O
by	O
dependency	O
trees	O
.	O
The	O
output	O
of	O
the	O
Syn	O
-	O
LSTM	B-MethodName
cell	O
is	O
jointly	O
determined	O
by	O
both	O
contextual	O
and	O
structured	O
information	O
.	O
We	O
adopt	O
the	O
classic	O
conditional	O
random	O
fields	O
(	O
CRF	B-MethodName
)	O
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
on	O
top	O
of	O
the	O
Syn	O
-	O
LSTM	B-MethodName
for	O
NER	B-TaskName
.	O
We	O
conduct	O
extensive	O
experiments	O
on	O
several	O
standard	O
datasets	O
across	O
four	O
languages	O
.	O
The	O
proposed	O
model	O
significantly	O
outperforms	O
previous	O
approaches	O
on	O
these	O
datasets	O
.	O
We	O
show	O
that	O
the	O
proposed	O
model	O
can	O
capture	O
long	O
-	O
distance	O
interactions	O
between	O
entities	O
.	O
Our	O
further	O
analysis	O
statistically	O
demonstrates	O
the	O
proposed	O
gating	O
mechanism	O
is	O
able	O
to	O
aggregate	O
the	O
structured	O
information	O
selectively	O
.	O
2	O
Synergized	O
-	O
LSTM	B-MethodName

We	O
propose	O
the	O
Synergized	O
-	O
LSTM	B-MethodName
(	O
Syn	O
-	O
LSTM	B-MethodName
)	O
to	O
better	O
integrate	O
the	O
contextual	O
and	O
structured	O
information	O
to	O
address	O
the	O
above	O
limitations	O
.	O
The	O
inputs	O
of	O
the	O
Syn	O
-	O
LSTM	B-MethodName
cell	O
include	O
previous	O
cell	O
state	O
c	O
t−1	O
,	O
previous	O
hidden	O
state	O
h	O
t−1	O
,	O
current	O
cell	O
input	O
x	O
t	O
,	O
and	O
an	O
additional	O
graph	O
-	O
encoded	O
representation	O
g	O
t	O
.	O
The	O
outputs	O
of	O
the	O
Syn	O
-	O
LSTM	B-MethodName
cell	O
include	O
current	O
cell	O
state	O
c	O
t	O
and	O
current	O
hidden	O
state	O
h	O
t	O
.	O
Within	O
the	O
cell	O
,	O
there	O
are	O
four	O
gates	O
:	O
input	O
gate	O
i	O
t	O
,	O
forget	O
gate	O
f	O
t	O
,	O
output	O
gate	O
o	O
t	O
,	O
and	O
an	O
additional	O
new	O
gate	O
m	O
t	O
to	O
control	O
the	O
flow	O
of	O
information	O
.	O
Note	O
that	O
the	O
forget	O
gate	O
f	O
t	O
and	O
output	O
gate	O
o	O
t	O
are	O
not	O
just	O
looking	O
at	O
h	O
t−1	O
and	O
x	O
t	O
;	O
they	O
are	O
also	O
affected	O
by	O
the	O
graph	O
-	O
encoded	O
representation	O
g	O
t	O
.	O
The	O
cell	O
state	O
c	O
t	O
and	O
hidden	O
state	O
h	O
t	O
are	O
computed	O
as	O
follows	O
:	O
f	O
t	O
=	O
σ	O
(	O
W	O
(	O
f	O
)	O
x	O
t	O
+	O
U	O
(	O
f	O
)	O
h	O
t−1	O
+	O
Q	O
(	O
f	O
)	O
g	O
t	O
+	O
b	O
(	O
f	O
)	O
)	O
(	O
1	O
)	O
o	O
t	O
=	O
σ	O
(	O
W	O
(	O
o	O
)	O
x	O
t	O
+	O
U	O
(	O
o	O
)	O
h	O
t−1	O
+	O
Q	O
(	O
o	O
)	O
g	O
t	O
+	O
b	O
(	O
o	O
)	O
)	O
(	O
2	O
)	O
i	O
t	O
=	O
σ	O
(	O
W	O
(	O
i	O
)	O
x	O
t	O
+	O
U	O
(	O
i	O
)	O
h	O
t−1	O
+	O
b	O
(	O
i	O
)	O
)	O
(	O
3	O
)	O
m	O
t	O
=	O
σ	O
(	O
W	O
(	O
m	O
)	O
g	O
t	O
+	O
U	O
(	O
m	O
)	O
h	O
t−1	O
+	O
b	O
(	O
m	O
)	O
)	O
(	O
4	O
)	O
c	O
t	O
=	O
tanh	O
(	O
W	O
(	O
u	O
)	O
x	O
t	O
+	O
U	O
(	O
u	O
)	O
h	O
t−1	O
+	O
b	O
(	O
u	O
)	O
)	O
(	O
5	O
)	O
s	O
t	O
=	O
tanh	O
(	O
W	O
(	O
n	O
)	O
g	O
t	O
+	O
U	O
(	O
n	O
)	O
h	O
t−1	O
+	O
b	O
(	O
n	O
)	O
)	O
(	O
6	O
)	O
c	O
t	O
=	O
f	O
t	O
c	O
t−1	O
+	O
i	O
t	O
c	O
t	O
+	O
m	O
t	O
s	O
t	O
(	O
7	O
)	O
h	O
t	O
=	O
o	O
t	O
tanh	O
(	O
c	O
t	O
)	O
(	O
8	O
)	O
where	O
σ	O
is	O
the	O
sigmoid	O
function	O
,	O
W	O
(	O
)	O
,	O
U	O
(	O
)	O
,	O
Q	O
(	O
)	O
and	O
b	O
(	O
)	O
are	O
learnable	O
parameters	O
.	O
The	O
additional	O
new	O
gate	O
m	O
t	O
is	O
used	O
to	O
control	O
the	O
information	O
from	O
the	O
graph	O
-	O
encoded	O
representation	O
directly	O
.	O
Such	O
a	O
design	O
allows	O
the	O
original	O
input	O
gates	O
i	O
t	O
and	O
our	O
new	O
gate	O
m	O
t	O
to	O
make	O
independent	O
assessments	O
on	O
the	O
amounts	O
of	O
information	O
to	O
be	O
retrieved	O
from	O
the	O
word	O
representation	O
x	O
t	O
and	O
the	O
graph	O
-	O
encoded	O
representation	O
g	O
t	O
respectively	O
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
also	O
have	O
a	O
different	O
candidate	O
states	O
t	O
to	O
represent	O
the	O
cell	O
state	O
that	O
corresponds	O
to	O
the	O
graph	O
-	O
encoded	O
representation	O
separately	O
.	O
With	O
the	O
proposed	O
Syn	O
-	O
LSTM	B-MethodName
,	O
the	O
structured	O
information	O
captured	O
by	O
the	O
dependency	O
trees	O
can	O
be	O
passed	O
to	O
each	O
cell	O
,	O
and	O
the	O
additional	O
gate	O
m	O
t	O
is	O
able	O
to	O
control	O
how	O
much	O
structured	O
information	O
can	O
be	O
incorporated	O
.	O
The	O
additional	O
gate	O
enables	O
the	O
model	O
to	O
feed	O
the	O
contextual	O
and	O
structured	O
information	O
into	O
the	O
LSTM	B-MethodName
cell	O
separately	O
.	O
Such	O
a	O
mechanism	O
allows	O
our	O
model	O
to	O
aggregate	O
the	O
information	O
from	O
linear	O
sequence	O
and	O
dependency	O
trees	O
selectively	O
.	O
Similar	O
to	O
the	O
previous	O
work	O
(	O
Levy	O
et	O
al	O
,	O
2018	O
)	O
,	O
it	O
is	O
also	O
possible	O
to	O
show	O
that	O
the	O
cell	O
state	O
c	O
t	O
implicitly	O
computes	O
the	O
element	O
-	O
wise	O
weighted	O
sum	O
of	O
the	O
previous	O
states	O
by	O
expanding	O
Equation	O
7	O
:	O
xt	O
-	O
1	O
xt	O
xt+1	O
xt+2	O
g	O
L	O
t	O
-	O
1	O
g	O
L	O
t	O
g	O
L	O
t+1	O
g	O
L	O
t+2	O
Syn	O
-	O
LSTM	B-MethodName
Syn	O
-	O
LSTM	B-MethodName
Syn	O
-	O
LSTM	B-MethodName
Syn	O
-	O
LSTM	B-MethodName
y	O
t−1	O
y	O
t	O
y	O
t+1	O
y	O
t+2	O
g	O
0	B-DatasetName
t	O
-	O
1	O
g	O
0	B-DatasetName
t	O
g	O
0	B-DatasetName
t+1	O
g	O
0	B-DatasetName
t+2	O
Graph	B-MethodName
Convolutional	I-MethodName
Network	I-MethodName
c	O
t	O
=	O
f	O
t	O
c	O
t−1	O
+	O
i	O
t	O
c	O
t	O
+	O
m	O
t	O
s	O
t	O
(	O
9	O
)	O
=	O
t	O
j=0	O
(	O
i	O
j	O
t	O
k	B-HyperparameterName
=	I-HyperparameterName
j+1	O
f	O
k	O
)	O
c	O
j	O
+	O
t	O
j=0	O
(	O
m	O
j	O
t	O
k	B-HyperparameterName
=	I-HyperparameterName
j+1	O
f	O
k	O
)	O
s	O
j	O
(	O
10	O
)	O
=	O
t	O
j=0	O
a	O
t	O
j	O
c	O
j	O
+	O
t	O
j=0	O
q	O
t	O
j	O
s	O
j	O
(	O
11	O
)	O
Note	O
that	O
the	O
two	O
terms	O
,	O
a	O
t	O
j	O
and	O
q	O
t	O
j	O
,	O
are	O
the	O
product	O
of	O
gates	O
.	O
The	O
value	O
of	O
the	O
two	O
terms	O
are	O
in	O
the	O
range	O
from	O
0	B-DatasetName
to	O
1	O
.	O
Since	O
thec	O
t	O
ands	O
t	O
represent	O
contextual	O
and	O
structured	O
features	O
,	O
the	O
corresponding	O
weights	O
control	O
the	O
flow	O
of	O
information	O
.	O

The	O
goal	O
of	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
is	O
to	O
predict	O
the	O
label	O
sequence	O
y	O
=	O
{	O
y	O
1	O
,	O
y	O
2	O
,	O
...	O
,	O
y	O
n	O
}	O
given	O
the	O
input	O
sequence	O
w	O
=	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
n	O
}	O
,	O
where	O
w	O
t	O
represents	O
the	O
t	O
-	O
th	O
word	O
and	O
n	O
is	O
the	O
number	O
of	O
words	O
.	O
Our	O
model	O
is	O
mainly	O
constructed	O
with	O
three	O
layers	O
:	O
input	O
representation	O
layer	O
,	O
bi	O
-	O
directional	O
Syn	O
-	O
LSTM	B-MethodName
layer	O
,	O
and	O
CRF	B-MethodName
layer	O
.	O
The	O
architecture	O
of	O
our	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
is	O
shown	O
in	O
Figure	O
3	O
.	O
Input	O
Representation	O
Layer	O
Similar	O
to	O
the	O
work	O
by	O
Lample	O
et	O
al	O
(	O
2016	O
)	O
,	O
our	O
input	O
representation	O
also	O
includes	O
the	O
character	O
embeddings	O
,	O
which	O
are	O
the	O
hidden	O
states	O
of	O
character	O
-	O
based	O
BiLSTM	B-MethodName
.	O
Jie	O
and	O
Lu	O
(	O
2019	O
)	O
highlight	O
that	O
the	O
dependency	O
relation	O
helps	O
to	O
enhance	O
the	O
input	O
representation	O
.	O
Furthermore	O
,	O
previous	O
methods	O
)	O
use	O
embeddings	O
of	O
part	O
-	O
ofspeech	O
(	O
POS	O
)	O
tags	O
as	O
additional	O
input	O
representation	O
.	O
The	O
input	O
representation	O
x	O
t	O
of	O
our	O
model	O
is	O
the	O
concatenation	O
of	O
the	O
word	O
embedding	O
v	O
t	O
,	O
the	O
character	O
representation	O
e	O
t	O
,	O
the	O
dependency	O
relation	O
embedding	O
r	O
t	O
,	O
and	O
the	O
POS	O
embedding	O
p	O
t	O
:	O
x	O
t	O
=	O
[	O
v	O
t	O
;	O
e	O
t	O
;	O
r	O
t	O
;	O
p	O
t	O
]	O
(	O
12	O
)	O
where	O
both	O
r	O
t	O
and	O
p	O
t	O
embeddings	O
are	O
randomly	O
initialized	O
and	O
are	O
fine	O
-	O
tuned	O
during	O
training	O
.	O
For	O
experiments	O
with	O
the	O
contextualized	O
representations	O
(	O
e.g.	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
)	O
,	O
we	O
further	O
concatenate	O
the	O
contextual	O
word	O
representation	O
to	O
x	O
t	O
.	O
For	O
our	O
task	O
,	O
we	O
employ	O
the	O
graph	B-MethodName
convolutional	I-MethodName
network	I-MethodName
(	O
Kipf	O
and	O
Welling	O
,	O
2017	O
;	O
Zhang	O
et	O
al	O
,	O
2018b	O
)	O
to	O
get	O
the	O
graph	O
-	O
encoded	O
representation	O
g	O
t	O
.	O
Given	O
a	O
graph	O
,	O
an	O
adjacency	O
matrix	O
A	O
of	O
size	O
n	O
×	O
n	O
is	O
able	O
to	O
represent	O
the	O
graph	O
structure	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
nodes	O
;	O
A	O
i	O
,	O
j	O
=	O
1	O
indicates	O
that	O
node	O
i	O
and	O
node	O
j	O
are	O
connected	O
.	O
We	O
transform	O
dependency	O
tree	O
into	O
its	O
corresponding	O
adjacency	O
matrix	O
3	O
A	O
,	O
and	O
A	O
i	O
,	O
j	O
=	O
1	O
denotes	O
that	O
node	O
i	O
and	O
node	O
j	O
have	O
dependency	O
relation	O
.	O
Note	O
that	O
the	O
purpose	O
of	O
graph	O
-	O
encoded	O
representation	O
g	O
t	O
is	O
to	O
incorporate	O
the	O
dependency	O
information	O
from	O
neighbor	O
nodes	O
.	O
The	O
input	O
and	O
output	O
representations	O
of	O
the	O
l	O
-	O
th	O
layer	O
GCN	B-MethodName
at	O
t	O
-	O
th	O
position	O
are	O
denoted	O
as	O
g	O
l−1	O
t	O
and	O
g	O
l	O
t	O
respectively	O
.	O
Similar	O
to	O
the	O
work	O
by	O
Zhang	O
et	O
al	O
(	O
2018b	O
)	O
,	O
we	O
use	O
d	O
t	O
=	O
n	O
j=1	O
A	O
t	O
,	O
j	O
,	O
which	O
is	O
the	O
total	O
number	O
of	O
neighbors	O
of	O
node	O
t	O
,	O
to	O
normalize	O
the	O
representation	O
before	O
going	O
through	O
the	O
nonlinear	O
function	O
.	O
The	O
GCN	B-MethodName
operation	O
is	O
defined	O
as	O
:	O
g	O
l	O
t	O
=	O
ReLU	B-MethodName
(	O
n	O
j=1	O
A	O
t	O
,	O
j	O
W	O
l	O
g	O
l−1	O
t	O
/d	O
t	O
+	O
b	O
l	O
)	O
(	O
13	O
)	O
where	O
W	O
l	O
is	O
a	O
linear	O
transformation	O
and	O
b	O
l	O
is	O
a	O
bias	O
.	O
The	O
initial	O
g	O
0	B-DatasetName
t	O
is	O
the	O
concatenation	O
of	O
word	O
embedding	O
v	O
t	O
,	O
character	O
embedding	O
e	O
t	O
,	O
and	O
dependency	O
relation	O
embedding	O
r	O
t	O
:	O
−	O
h	O
t	O
from	O
backward	O
Syn	O
-	O
LSTM	B-MethodName
to	O
form	O
the	O
contextual	O
representation	O
of	O
t	O
-	O
th	O
token	O
:	O
g	O
0	B-DatasetName
t	O
=	O
[	O
v	O
t	O
;	O
e	O
t	O
;	O
r	O
t	O
]	O
.	O
Bi	O
-	O
directional	O
h	O
t	O
=	O
[	O
−	O
h	O
t	O
;	O
−	O
h	O
t	O
]	O
.	O
CRF	B-MethodName
Layer	O
The	O
CRF	B-MethodName
(	O
Lafferty	O
et	O
al	O
,	O
2001	O
)	O
is	O
widely	O
used	O
in	O
NER	B-TaskName
tasks	O
as	O
it	O
is	O
capable	O
of	O
capturing	O
the	O
structured	O
correlations	O
between	O
adjacent	O
output	O
labels	O
.	O
Given	O
the	O
sentence	O
w	O
and	O
dependency	O
tree	O
τ	O
,	O
the	O
probability	O
of	O
the	O
label	O
sequence	O
y	O
is	O
defined	O
as	O
:	O
P	O
(	O
y	O
|	O
w	O
,	O
τ	O
)	O
=	O
exp	O
(	O
score	O
(	O
w	O
,	O
τ	O
,	O
y	O
)	O
)	O
y	O
exp	O
(	O
score	O
(	O
w	O
,	O
τ	O
,	O
y	O
)	O
)	O
(	O
14	O
)	O
The	O
score	O
function	O
is	O
defined	O
as	O
:	O
score	O
(	O
w	O
,	O
τ	O
,	O
y	O
)	O
=	O
n	O
t=0	O
T	O
yt	O
,	O
y	O
t+1	O
+	O
n	O
t=1	O
E	O
yt	O
(	O
15	O
)	O
where	O
T	O
yt	O
,	O
y	O
t+1	O
denotes	O
the	O
transition	O
score	O
from	O
label	O
y	O
t	O
to	O
y	O
t+1	O
,	O
E	O
yt	O
denotes	O
the	O
score	O
of	O
label	O
y	O
t	O
at	O
the	O
t	O
-	O
th	O
position	O
and	O
the	O
scores	O
are	O
computed	O
using	O
the	O
hidden	O
state	O
h	O
t	O
.	O
We	O
learn	O
the	O
model	O
parameters	O
by	O
minimizing	O
the	O
negative	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
and	O
employ	O
the	O
Viterbi	O
algorithm	O
to	O
obtain	O
the	O
best	O
label	O
sequence	O
during	O
evaluation	O
.	O

Datasets	O
The	O
proposed	O
model	O
is	O
evaluated	O
on	O
four	O
benchmark	O
datasets	O
:	O
SemEval	O
2010	O
Task	O
1	O
(	O
Recasens	O
et	O
al	O
,	O
2010	O
)	O
Catalan	O
and	O
Spanish	O
datasets	O
,	O
and	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
(	O
Weischedel	O
et	O
al	O
,	O
2013	O
)	O
English	O
and	O
Chinese	O
datasets	O
.	O
We	O
choose	O
these	O
four	O
datasets	O
as	O
they	O
have	O
explicit	O
dependency	O
annotations	O
which	O
allow	O
us	O
to	O
evaluate	O
the	O
effectiveness	O
of	O
our	O
approach	O
when	O
dependency	O
trees	O
of	O
different	O
qualities	O
are	O
used	O
.	O
For	O
SemEval	O
2010	O
Task	O
1	O
datasets	O
,	O
there	O
are	O
4	O
entity	O
types	O
:	O
PER	O
,	O
LOC	O
and	O
ORG	O
and	O
MISC	O
.	O
For	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
datasets	O
,	O
there	O
are	O
18	O
entity	O
types	O
in	O
total	O
.	O
Following	O
the	O
work	O
by	O
Jie	O
and	O
Lu	O
(	O
2019	O
)	O
,	O
we	O
transform	O
the	O
parse	O
trees	O
into	O
the	O
Stanford	O
dependency	O
trees	O
(	O
De	O
Marneffe	O
and	O
Manning	O
,	O
2008	O
)	O
by	O
using	O
Stanford	O
CoreNLP	O
.	O
Detailed	O
statistics	O
of	O
each	O
dataset	O
can	O
be	O
found	O
in	O
Table	O
1	O
.	O
Intuitively	O
,	O
longer	O
sentences	O
would	O
require	O
the	O
model	O
to	O
capture	O
more	O
long	O
-	O
distance	O
interactions	O
in	O
the	O
sentences	O
.	O
We	O
present	O
the	O
number	O
of	O
entities	O
in	O
terms	O
of	O
different	O
sentence	O
lengths	O
to	O
show	O
that	O
these	O
datasets	O
have	O
a	O
modest	O
amount	O
of	O
entities	O
in	O
long	O
sentences	O
.	O
Experimental	O
Setup	O
For	O
Catalan	O
,	O
Spanish	O
,	O
and	O
Chinese	O
,	O
we	O
use	O
the	O
FastText	B-MethodName
(	O
Grave	O
et	O
al	O
,	O
2018	O
)	O
300	O
dimensional	O
embeddings	O
to	O
initialize	O
the	O
word	B-TaskName
embeddings	I-TaskName
.	O
For	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	O
,	O
we	O
adopt	O
the	O
publicly	O
available	O
GloVE	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
100	O
dimensional	O
embeddings	O
to	O
initialize	O
the	O
word	B-TaskName
embeddings	I-TaskName
.	O
For	O
experiments	O
with	O
the	O
contextualized	O
representation	O
,	O
we	O
adopt	O
the	O
pre	O
-	O
trained	O
language	O
model	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
for	O
the	O
four	O
datasets	O
.	O
Specifically	O
,	O
we	O
use	O
bert	O
-	O
as	O
-	O
service	O
(	O
Xiao	O
,	O
2018	O
)	O
to	O
generate	O
the	O
contextualized	O
word	O
representation	O
without	O
fine	O
-	O
tuning	O
.	O
Following	O
Luo	O
et	O
al	O
(	O
2020	O
)	O
,	O
we	O
use	O
the	O
cased	O
version	O
of	O
BERT	B-MethodName
large	O
model	O
for	O
the	O
experiments	O
on	O
the	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	O
data	O
.	O
We	O
use	O
the	O
cased	O
version	O
of	O
BERT	B-MethodName
base	O
model	O
for	O
the	O
experiments	O
on	O
the	O
other	O
three	O
datasets	O
.	O
For	O
the	O
character	O
embedding	O
,	O
we	O
randomly	O
initialize	O
the	O
character	O
embeddings	O
and	O
set	O
the	O
dimension	O
as	O
30	O
,	O
and	O
set	O
the	O
hidden	O
size	O
of	O
character	O
-	O
level	O
BiLSTM	B-MethodName
as	O
50	O
.	O
The	O
hidden	O
size	O
of	O
GCN	B-MethodName
and	O
Syn	O
-	O
LSTM	B-MethodName
is	O
set	O
as	O
200	O
,	O
the	O
number	O
of	O
GCN	B-MethodName
layer	O
is	O
2	O
.	O
We	O
adopt	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
(	O
SGD	B-MethodName
)	O
to	O
optimize	O
our	O
model	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
100	O
,	O
L2	B-HyperparameterName
regularization	I-HyperparameterName
10	O
−8	O
,	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
lr	O
0.2	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
decayed	O
4	O
with	O
respect	O
to	O
the	O
number	O
of	O
epoch	O
.	O
We	O
select	O
the	O
best	O
model	O
based	O
on	O
the	O
performance	O
on	O
the	O
dev	O
set	O
5	O
and	O
apply	O
it	O
to	O
the	O
test	O
set	O
.	O
We	O
use	O
the	O
bootstrapping	O
t	O
-	O
test	O
to	O
compare	O
the	O
results	O
.	O
Baselines	O
We	O
compare	O
our	O
model	O
with	O
several	O
baselines	O
with	O
or	O
without	O
dependency	O
tree	O
information	O
.	O
The	O
first	O
one	O
is	O
BERT	B-MethodName
-	O
CRF	B-MethodName
,	O
where	O
we	O
apply	O
a	O
CRF	B-MethodName
layer	O
on	O
top	O
of	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
Secondly	O
,	O
we	O
compare	O
with	O
the	O
BERT	B-MethodName
implementation	O
by	O
HuggingFace	O
(	O
Wolf	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
models	O
with	O
dependency	O
trees	O
,	O
we	O
take	O
the	O
models	O
BiLSTM	B-MethodName
-	O
GCN	B-MethodName
-	O
CRF	B-MethodName
and	O
dependency	O
-	O
4	O
We	O
set	O
the	O
decay	O
as	O
0.1	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
for	O
each	O
epoch	O
equals	O
to	O
lr/	O
(	O
1	O
+	O
decay	O
*	O
(	O
epoch	O
−	O
1	O
)	O
)	O
.	O
5	O
The	O
experimental	O
results	O
on	O
the	O
dev	O
set	O
and	O
other	O
experimental	O
details	O
can	O
be	O
found	O
in	O
the	O
Appendix	O
.	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
,	O
but	O
we	O
also	O
implement	O
it	O
with	O
BERT	B-MethodName
.	O
Besides	O
,	O
we	O
compare	O
our	O
model	O
with	O
previous	O
works	O
that	O
have	O
results	O
on	O
these	O
datasets	O
.	O

In	O
language	O
model	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
for	O
the	O
four	O
datasets	O
.	O
Specifically	O
,	O
we	O
use	O
bert	O
-	O
as	O
-	O
service	O
(	O
Xiao	O
,	O
2018	O
)	O
to	O
generate	O
the	O
contextualized	O
word	O
representation	O
without	O
fine	O
-	O
tuning	O
.	O
Following	O
Luo	O
et	O
al	O
(	O
2020	O
)	O
,	O
we	O
select	O
the	O
18	O
th	O
layer	O
of	O
the	O
cased	O
version	O
of	O
BERT	B-MethodName
large	O
model	O
for	O
the	O
experiments	O
on	O
the	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	O
data	O
.	O
We	O
use	O
the	O
the	O
9	O
th	O
layer	O
of	O
cased	O
version	O
of	O
BERT	B-MethodName
base	O
model	O
for	O
the	O
experiments	O
on	O
the	O
rest	O
three	O
datasets	O
.	O
For	O
the	O
character	O
embedding	O
,	O
we	O
randomly	O
initialize	O
the	O
character	O
embeddings	O
and	O
set	O
the	O
dimension	O
as	O
30	O
,	O
and	O
set	O
the	O
hidden	O
size	O
of	O
character	O
-	O
level	O
BiLSTM	B-MethodName
as	O
50	O
.	O
The	O
hidden	O
size	O
of	O
GCN	B-MethodName
and	O
Syn	O
-	O
LSTM	B-MethodName
is	O
set	O
as	O
200	O
.	O
Note	O
that	O
we	O
only	O
use	O
one	O
layer	O
of	O
bi	O
-	O
directional	O
Syn	O
-	O
LSTM	B-MethodName
for	O
our	O
experiments	O
.	O
Dropout	B-MethodName
is	O
set	O
to	O
0.5	O
for	O
input	O
embeddings	O
and	O
hidden	O
states	O
.	O
We	O
adopt	O
stochastic	B-MethodName
gradient	I-MethodName
descent	I-MethodName
(	O
SGD	B-MethodName
)	O
to	O
optimize	O
our	O
model	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
100	O
,	O
L2	B-HyperparameterName
regularization	I-HyperparameterName
10	O
−8	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.2	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
decayed	O
with	O
respect	O
to	O
the	O
number	O
of	O
epoch	O
9	O
.	O

Table	O
8	O
presents	O
the	O
performance	O
of	O
dependency	O
parser	O
.	O
9	O
We	O
set	O
the	O
decay	O
as	O
0.1	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
for	O
each	O
epoch	O
equals	O
to	O
learning_rate/	O
(	O
1	O
+	O
decay	O
*	O
(	O
epoch	O
−	O
1	O
)	O
)	O
.	O

We	O
test	O
our	O
model	O
on	O
RTX	O
2080	O
Ti	O
GPU	O
and	O
Nvidia	O
Tesla	O
V100	O
GPU	O
,	O
with	O
CUDA	O
version	O
10.1	O
,	O
PyTorch	O
version	O
1.40	O
.	O
The	O
average	O
run	O
time	O
for	O
Syn	O
-	O
LSTM	B-MethodName
is	O
52	O
sec	O
/	O
epoch	O
,	O
55	O
sec	O
/	O
epoch	O
,	O
290	O
sec	O
/	O
epoch	O
,	O
350	O
sec	O
/	O
epoch	O
for	O
Catalan	O
,	O
Spanish	O
,	O
Chinese	O
and	O
English	O
datasets	O
respectively	O
.	O
The	O
total	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
is	O
11M.	O
Table	O
10	O
shows	O
the	O
performance	O
of	O
our	O
model	O
on	O
the	O
dev	O
sets	O
of	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	O
and	O
Chinese	O
,	O
SemEval	O
2010	O
Task	O
1	O
Catalan	O
and	O
Spanish	O
.	O
For	O
hyper	O
-	O
parameter	O
,	O
we	O
use	O
the	O
FastText	B-MethodName
(	O
Grave	O
et	O
al	O
,	O
2018	O
)	O
300	O
dimensional	O
embeddings	O
to	O
initialize	O
the	O
word	B-TaskName
embeddings	I-TaskName
for	O
Catalan	O
,	O
Spanish	O
,	O
and	O
Chinese	O
.	O
For	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	O
,	O
we	O
adopt	O
the	O
publicly	O
available	O
GloVE	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
100	O
dimensional	O
embeddings	O
to	O
initialize	O
the	O
word	B-TaskName
embeddings	I-TaskName
.	O
For	O
experiments	O
with	O
the	O
contextualized	O
representation	O
,	O
we	O
adopt	O
the	O
pre	O
-	O
trained	O

We	O
first	O
train	O
100	O
and	O
300	O
dimensions	O
for	O
both	O
GloVe	B-MethodName
embeddings	I-MethodName
and	O
skip	O
-	O
thought	O
embeddings	O
using	O
the	O
same	O
mechanism	O
as	O
in	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
;	O
Kiros	O
et	O
al	O
,	O
2015	O
)	O
.	O
In	O
some	O
posts	O
the	O
length	O
of	O
sentences	O
is	O
very	O
large	O
,	O
so	O
we	O
bound	O
the	O
length	O
at	O
50	O
words	O
.	O
We	O
do	O
not	O
treat	O
the	O
problem	O
separately	O
from	O
the	O
negative	O
take	O
as	O
the	O
GRU	B-MethodName
will	O
anyway	O
put	O
more	O
importance	O
on	O
the	O
information	O
that	O
comes	O
last	O
.	O
We	O
split	O
the	O
labelled	O
data	O
in	O
a	O
8	O
:	O
1	O
:	O
1	O
ratio	O
for	O
training	O
,	O
validation	O
and	O
testing	O
in	O
a	O
10	O
-	O
fold	O
cross	O
validation	O
for	O
both	O
GRU	B-MethodName
and	O
CNN	O
training	O
.	O
A	O
distinct	O
network	O
is	O
trained	O
for	O
each	O
concept	O
,	O
i.	O
e.	O
one	O
for	O
thinking	O
errors	O
,	O
one	O
for	O
emotions	O
and	O
one	O
for	O
situations	O
.	O
The	O
hidden	O
size	O
of	O
the	O
FNN	O
is	O
150	O
.	O
To	O
tackle	O
the	O
data	O
bias	O
problem	O
,	O
we	O
utilise	O
oversampling	O
.	O
Different	O
ratios	O
(	O
1:1	O
,	O
1:3	O
,	O
1:5	O
,	O
1:7	O
)	O
of	O
positive	O
and	O
negative	O
samples	O
are	O
explored	O
.	O
We	O
used	O
filter	O
windows	O
of	O
2	O
,	O
3	O
,	O
and	O
4	O
with	O
50	O
feature	O
maps	O
for	O
the	O
CNN	O
model	O
.	O
For	O
the	O
GRU	B-MethodName
model	O
,	O
the	O
hidden	O
size	O
is	O
set	O
at	O
150	O
,	O
so	O
that	O
both	O
models	O
have	O
comparable	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
.	O
Mini	O
-	O
batches	O
of	O
size	O
24	O
are	O
used	O
and	O
gradients	O
are	O
clipped	O
with	O
maximum	O
norm	O
5	O
.	O
We	O
initialise	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
as	O
0.001	O
with	O
a	O
decay	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.986	O
every	O
10	O
steps	O
.	O
The	O
non	O
-	O
recurrent	O
weights	O
with	O
a	O
truncated	O
normal	O
distribution	O
(	O
0	B-DatasetName
,	O
0.01	O
)	O
,	O
and	O
the	O
recurrent	O
weights	O
with	O
orthogonal	O
initialisation	O
(	O
Saxe	O
et	O
al	O
,	O
2013	O
)	O
.	O
To	O
overcome	O
over	O
-	O
fitting	O
,	O
we	O
employ	O
dropout	O
with	O
rate	O
0.8	O
and	O
l2	O
-	O
normalisation	O
.	O
Both	O
models	O
were	O
trained	O
with	O
Adam	B-MethodName
algorithm	O
and	O
implemented	O
in	O
Tensorflow	O
(	O
Girija	O
,	O
2016	O
)	O
.	O

Table	O
5	O
gives	O
the	O
average	B-MetricName
F1	I-MetricName
scores	O
and	O
the	O
average	B-MetricName
F1	I-MetricName
scores	O
weighted	O
with	O
the	O
frequency	O
of	O
CBT	B-DatasetName
labels	O
for	O
all	O
models	O
under	O
the	O
oversampling	O
ratio	O
1:1	O
.	O
It	O
shows	O
that	O
GloVe	B-MethodName
word	O
vectors	O
with	O
CNN	O
achieves	O
the	O
best	O
performance	O
both	O
in	O
100	O
and	O
300	O
dimensions	O
.	O
Table	O
6	O
shows	O
the	O
F1	B-MetricName
-	O
measure	O
of	O
the	O
compared	O
models	O
that	O
detect	O
thinking	O
errors	O
,	O
emotions	O
and	O
situations	O
under	O
the	O
1	O
:	O
1	O
oversampling	O
ratio	O
.	O
We	O
only	O
include	O
the	O
results	O
of	O
the	O
best	O
performing	O
models	O
,	O
SVMs	O
,	O
CNNs	O
and	O
GRUs	O
,	O
due	O
to	O
limited	O
space	O
.	O
The	O
results	O
show	O
that	O
both	O
models	O
outperform	O
SVM	B-MethodName
-	O
BOW	O
in	O
larger	O
embedding	O
dimensions	O
.	O
Although	O
SVM	B-MethodName
-	O
BOW	O
is	O
comparable	O
to	O
100	O
dimensional	O
GRU	B-MethodName
-	O
Skip	O
-	O
thought	O
in	O
terms	O
on	O
average	B-MetricName
F1	I-MetricName
,	O
in	O
all	O
other	O
cases	O
CNN	O
-	O
GloVe	B-MethodName
and	O
GRU	B-MethodName
-	O
Skipthought	O
overshadow	O
SVM	B-MethodName
-	O
BOW	O
.	O
We	O
also	O
find	O
that	O
CNN	O
-	O
GloVe	B-MethodName
on	O
average	O
works	O
better	O
than	O
GRU	B-MethodName
-	O
Skip	O
-	O
thought	O
,	O
which	O
is	O
expected	O
as	O
the	O
space	O
of	O
words	O
is	O
smaller	O
in	O
comparison	O
to	O
the	O
space	O
of	O
sentences	O
so	O
the	O
word	O
vectors	O
can	O
be	O
more	O
accurately	O
trained	O
.	O
While	O
the	O
CNN	O
operating	O
on	O
100	O
dimensional	O
word	O
vectors	O
is	O
comparable	O
to	O
the	O
CNN	O
operating	O
on	O
300	O
dimensional	O
word	O
vectors	O
,	O
the	O
GRU	B-MethodName
-	O
Skip	O
-	O
thought	O
tends	O
to	O
be	O
worse	O
on	O
100	O
dimensional	O
skip	O
-	O
thoughts	O
,	O
suggesting	O
that	O
sentence	O
vectors	O
generally	O
need	O
to	O
be	O
of	O
a	O
higher	O
dimension	O
to	O
represent	O
the	O
meaning	O
more	O
accurately	O
than	O
word	O
vectors	O
.	O
Table	O
7	O
shows	O
a	O
more	O
detailed	O
analysis	O
of	O
the	O
300	O
dimensional	O
CNN	O
-	O
GloVe	B-MethodName
performance	O
,	O
where	O
both	O
precision	O
and	O
recall	O
are	O
presented	O
,	O
indicating	O
that	O
oversampling	O
mechanism	O
can	O
help	O
overcome	O
the	O
data	O
bias	O
problem	O
.	O
To	O
illustrate	O
the	O
capabilities	O
of	O
this	O
model	O
,	O
we	O
give	O
samples	O
of	O
two	O
posts	O
and	O
their	O
predicted	O
and	O
true	O
labels	O
in	O
Figure	O
6	O
,	O
which	O
shows	O
that	O
our	O
model	O
discerns	O
the	O
classes	O
reasonably	O
well	O
even	O
in	O
some	O
difficult	O
cases	O
.	O
While	O
oversampling	O
is	O
essential	O
for	O
both	O
models	O
,	O
GRU	B-MethodName
-	O
Skip	O
-	O
thought	O
is	O
less	O
sensitive	O
to	O
lower	O
oversampling	O
ratios	O
,	O
suggesting	O
that	O
skip	O
-	O
thoughts	O
can	O
already	O
capture	O
sentiment	O
on	O
the	O
sentence	O
level	O
.	O
Therefore	O
,	O
including	O
only	O
a	O
limited	O
ratio	O
of	O
positive	O
samples	O
is	O
sufficient	O
to	O
train	O
the	O
classifier	O
.	O
Instead	O
,	O
models	O
using	O
word	O
vectors	O
need	O
more	O
positive	O
data	O
to	O
learn	O
sentence	O
sentiment	O
features	O
.	O

We	O
now	O
present	O
our	O
metric	O
for	O
unintended	O
demographic	O
bias	O
,	O
RNSB	O
.	O
For	O
gold	O
standard	O
labeled	O
positive	O
/	O
negative	O
sentiment	O
words	O
,	O
(	O
x	O
i	O
,	O
y	O
i	O
)	O
,	O
in	O
training	O
set	O
,	O
S	O
,	O
where	O
x	O
i	O
is	O
a	O
word	O
vector	O
from	O
a	O
possibly	O
biased	O
word	O
embedding	O
model	O
,	O
we	O
find	O
the	O
minimizer	O
,	O
f	O
*	O
(	O
x	O
i	O
)	O
=	O
σ	O
(	O
w	O
T	O
x	O
i	O
)	O
,	O
for	O
the	O
logistic	O
loss	B-MetricName
,	O
l	O
,	O
and	O
learned	O
weights	O
,	O
w.	O
min	O
w	O
R	O
d	O
n	O
i=0	O
l	O
(	O
y	O
i	O
,	O
w	O
T	O
x	O
i	O
)	O
+	O
λ	O
w	O
2	O
,	O
λ	O
>	O
0	B-DatasetName
Then	O
for	O
a	O
set	O
,	O
K	B-HyperparameterName
=	I-HyperparameterName
{	O
k	O
1	O
,	O
...	O
,	O
k	O
t	O
}	O
,	O
of	O
t	O
demographic	O
identity	O
word	O
vectors	O
from	O
a	O
particular	O
protected	O
group	O
(	O
i.e.	O
national	O
origin	O
,	O
religion	O
,	O
etc	O
.	O
)	O
,	O
we	O
define	O
a	O
set	O
,	O
P	O
,	O
containing	O
the	O
predicted	O
negative	O
sentiment	O
probability	O
via	O
minimizer	O
,	O
f	O
*	O
,	O
normalized	O
to	O
be	O
one	O
probability	O
mass	O
.	O
P	O
=	O
f	O
*	O
(	O
k	O
1	O
)	O
t	O
i=1	O
f	O
*	O
(	O
k	O
i	O
)	O
,	O
...	O
,	O
f	O
*	O
(	O
k	O
t	O
)	O
t	O
i=1	O
f	O
*	O
(	O
k	O
i	O
)	O
Thus	O
,	O
our	O
metric	O
,	O
RN	O
SB	O
(	O
P	O
)	O
,	O
is	O
defined	O
as	O
the	O
KL	O
divergence	O
of	O
P	O
from	O
U	O
,	O
where	O
U	O
is	O
the	O
uniform	O
distribution	O
for	O
t	O
elements	O
.	O
RN	O
SB	O
(	O
P	O
)	O
=	O
D	O
KL	O
(	O
P	O
U	O
)	O
We	O
choose	O
our	O
set	O
of	O
neutral	O
identity	O
terms	O
based	O
on	O
the	O
most	O
populous	O
demographics	O
for	O
each	O
protected	O
group	O
.	O
However	O
,	O
due	O
to	O
the	O
simplicity	O
of	O
this	O
method	O
,	O
one	O
can	O
easily	O
adapt	O
it	O
to	O
include	O
identity	O
terms	O
that	O
suit	O
the	O
application	O
in	O
need	O
of	O
analysis	O
.	O
Since	O
neutral	O
identity	O
terms	O
are	O
inherently	O
not	O
associated	O
with	O
sentiment	O
,	O
it	O
is	O
unfair	O
to	O
have	O
identity	O
term	O
with	O
differing	O
levels	O
of	O
negative	O
sentiment	O
.	O
This	O
type	O
of	O
discrimination	O
can	O
show	O
up	O
in	O
many	O
downstream	O
sentiment	B-TaskName
analysis	I-TaskName
applications	O
.	O
Thus	O
,	O
we	O
want	O
no	O
differences	O
between	O
negative	O
sentiment	O
predictions	O
of	O
various	O
identity	O
terms	O
.	O
Mathematically	O
,	O
this	O
can	O
be	O
represented	O
as	O
a	O
uniform	O
distribution	O
of	O
negative	O
sentiment	O
probability	O
for	O
identity	O
terms	O
from	O
a	O
protected	O
group	O
.	O
Our	O
RNSB	O
metric	O
captures	O
the	O
distance	O
,	O
via	O
KL	O
divergence	O
,	O
between	O
the	O
current	O
distribution	O
of	O
negative	O
sentiment	O
and	O
the	O
fair	O
uniform	O
distribution	O
.	O
So	O
the	O
more	O
fair	O
a	O
word	O
embedding	O
model	O
with	O
respect	O
to	O
sentiment	O
bias	O
,	O
the	O
lower	O
the	O
RNSB	O
metric	O
.	O

I	O
like	O
going	O
outside	O
.	O
I	O
love	O
Disneyland	O
!	O
I	O
go	O
there	O
every	O
week	O
.	O
'	O
Will	O
I	O
sound	O
like	O
me	O
?	O
'	O
Figure	O
1	O
:	O
Illustration	O
of	O
the	O
consistency	O
issue	O
in	O
dialogue	O
.	O
While	O
a	O
literal	O
dialogue	O
agent	B-DatasetName
(	O
S	O
0	B-DatasetName
)	O
fails	O
to	O
deliver	O
a	O
consistent	O
persona	O
,	O
our	O
self	O
-	O
conscious	O
agent	B-DatasetName
(	O
S	O
1	O
)	O
does	O
so	O
,	O
by	O
modeling	O
an	O
imaginary	O
listener	O
.	O
Icons	O
are	O
designed	O
by	O
Nhor	O
Phai	O
and	O
Vincent	O
Le	O
Moign	O
.	O
are	O
highly	O
insensitive	O
to	O
contradictory	O
words	O
,	O
and	O
thus	O
fail	O
to	O
deliver	O
consistent	O
persona	O
to	O
the	O
interlocutor	O
(	O
Figure	O
1	O
)	O
.	O
Also	O
,	O
extra	O
modules	O
other	O
than	O
the	O
generative	O
model	O
is	O
often	O
required	O
for	O
improving	O
consistency	O
.	O
Recent	O
works	O
on	O
consistency	O
in	O
persona	O
-	O
based	O
dialogue	O
actively	O
adopt	O
the	O
NLIbased	O
approach	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
;	O
Song	O
et	O
al	O
,	O
2019	O
;	O
Li	O
et	O
al	O
,	O
2020	O
;	O
Song	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
have	O
the	O
following	O
prerequisites	O
.	O
First	O
,	O
they	O
require	O
labeled	O
pairs	O
of	O
persona	O
sentences	O
and	O
dialogue	O
utterances	O
with	O
three	O
categories	O
:	O
entailment	O
,	O
neutral	O
,	O
and	O
contradiction	O
.	O
Next	O
,	O
methods	O
with	O
NLI	O
models	O
for	O
rating	O
the	O
agent	B-DatasetName
's	O
consistency	O
also	O
need	O
to	O
train	O
them	O
separately	O
with	O
those	O
labels	O
.	O
In	O
this	O
work	O
,	O
we	O
step	O
back	O
from	O
this	O
NLI	O
-	O
based	O
supervised	O
approach	O
and	O
ponder	O
:	O
how	O
do	O
humans	O
maintain	O
consistency	O
?	O
We	O
humans	O
never	O
learn	O
how	O
to	O
be	O
consistent	O
.	O
Instead	O
,	O
we	O
have	O
an	O
innate	O
drive	O
for	O
consistency	O
to	O
hold	O
our	O
beliefs	O
and	O
behavior	O
in	O
harmony	O
(	O
Festinger	O
,	O
1962	O
)	O
.	O
If	O
so	O
,	O
how	O
do	O
we	O
know	O
we	O
are	O
consistent	O
or	O
not	O
?	O
We	O
do	O
not	O
ask	O
others	O
.	O
We	O
ask	O
ourselves	O
by	O
predicting	O
how	O
we	O
are	O
perceived	O
by	O
others	O
.	O
Public	O
self	O
-	O
consciousness	O
is	O
this	O
awareness	O
of	O
the	O
self	O
as	O
a	O
social	O
object	O
that	O
can	O
be	O
observed	O
and	O
evaluated	O
by	O
others	O
(	O
Fenigstein	O
et	O
al	O
,	O
1975	O
)	O
.	O
We	O
particularly	O
emphasize	O
that	O
public	O
self	O
-	O
consciousness	O
is	O
not	O
equivalent	O
to	O
the	O
philosophical	O
self	O
-	O
consciousness	O
(	O
or	O
self	O
-	O
awareness	O
)	O
1	O
.	O
Simply	O
put	O
,	O
public	O
self	O
-	O
consciousness	O
is	O
the	O
concern	O
about	O
how	O
oneself	O
will	O
be	O
perceived	O
by	O
others	O
,	O
as	O
opposed	O
to	O
the	O
philosophical	O
state	O
of	O
being	O
conscious	O
of	O
self	O
-	O
existence	O
.	O
According	O
to	O
Doherty	O
and	O
Schlenker	O
(	O
1991	O
)	O
,	O
people	O
with	O
high	O
public	O
self	O
-	O
consciousness	O
tend	O
to	O
act	O
more	O
consistent	O
with	O
known	O
information	O
about	O
themselves	O
.	O
They	O
care	O
deeply	O
about	O
how	O
others	O
will	O
evaluate	O
them	O
and	O
have	O
a	O
strong	O
tendency	O
to	O
avoid	O
negative	O
evaluations	O
(	O
Fenigstein	O
et	O
al	O
,	O
1975	O
)	O
.	O
Since	O
inconsistency	O
is	O
condemned	O
by	O
others	O
,	O
one	O
who	O
has	O
high	O
public	O
self	O
-	O
consciousness	O
will	O
try	O
more	O
to	O
maintain	O
consistency	O
.	O
In	O
order	O
to	O
predict	O
how	O
we	O
are	O
perceived	O
,	O
we	O
rely	O
on	O
abstract	O
models	O
of	O
others	O
(	O
Gopnik	O
and	O
Wellman	O
,	O
1992	O
)	O
and	O
simulate	O
others	O
'	O
reactions	O
based	O
on	O
imagination	O
(	O
Hassabis	O
et	O
al	O
,	O
2013	O
)	O
.	O
Inspired	B-DatasetName
by	O
this	O
,	O
our	O
intuition	O
is	O
that	O
self	O
-	O
consciousness	O
through	O
an	O
imaginary	O
listener	O
will	O
let	O
dialogue	O
agents	O
better	O
maintain	O
consistency	O
.	O
Modeling	O
a	O
listener	O
has	O
been	O
one	O
of	O
the	O
main	O
topics	O
in	O
computational	O
pragmatics	O
.	O
Our	O
work	O
extends	O
this	O
long	O
line	O
of	O
work	O
in	O
cognitive	O
science	O
by	O
making	O
use	O
of	O
the	O
Bayesian	O
Rational	O
Speech	O
Acts	O
framework	O
(	O
Frank	O
and	O
Goodman	O
,	O
2012	O
)	O
,	O
which	O
has	O
been	O
originally	O
applied	O
to	O
improving	O
informativeness	O
of	O
referring	O
expressions	O
.	O
Since	O
personas	O
ought	O
to	O
express	O
who	O
we	O
are	O
,	O
we	O
adopt	O
this	O
framework	O
for	O
dialogue	O
agents	O
by	O
regarding	O
personas	O
as	O
targets	O
that	O
should	O
be	O
conveyed	O
to	O
the	O
interlocutor	O
.	O
As	O
the	O
agent	B-DatasetName
tries	O
to	O
generate	O
tokens	O
that	O
help	O
the	O
imaginary	O
listener	O
identify	O
the	O
agent	B-DatasetName
's	O
persona	O
,	O
it	O
can	O
lastly	O
generate	O
more	O
consistent	O
utterances	O
.	O
In	O
summary	O
,	O
we	O
take	O
inspiration	O
from	O
social	O
cognition	O
and	O
pragmatics	O
to	O
endow	O
generative	O
agents	O
with	O
self	O
-	O
consciousness	O
,	O
which	O
makes	O
them	O
imagine	O
the	O
listener	O
's	O
reaction	O
and	O
incorporate	O
it	O
to	O
the	O
generation	O
process	O
for	O
improving	O
consistency	O
.	O
Our	O
major	O
contributions	O
can	O
be	O
outlined	O
as	O
follows	O
:	O
(	O
1	O
)	O
We	O
propose	O
an	O
orthogonally	O
applicable	O
approach	O
for	O
any	O
persona	O
-	O
based	O
generative	O
agents	O
to	O
improve	O
consistency	O
without	O
the	O
use	O
of	O
additional	O
consistency	O
labels	O
and	O
training	O
.	O
Moreover	O
,	O
it	O
is	O
even	O
generalizable	O
to	O
improve	O
context	O
-	O
consistency	O
beyond	O
persona	O
in	O
dialogue	O
.	O
(	O
2	O
)	O
We	O
extend	O
the	O
Rational	O
Speech	O
Acts	O
framework	O
(	O
Frank	O
and	O
Goodman	O
,	O
2012	O
)	O
with	O
two	O
new	O
technical	O
features	O
:	O
(	O
i	O
)	O
a	O
learning	O
method	O
for	O
distractor	O
selection	O
(	O
e.g.	O
other	O
samples	O
different	O
from	O
the	O
given	O
target	O
(	O
Andreas	O
and	O
Klein	O
,	O
2016	O
)	O
)	O
,	O
which	O
has	O
been	O
usually	O
done	O
manually	O
or	O
randomly	O
,	O
and	O
(	O
ii	O
)	O
a	O
different	O
update	O
for	O
the	O
listener	O
's	O
world	O
prior	O
that	O
better	O
preserves	O
information	O
of	O
previous	O
states	O
.	O
(	O
3	O
)	O
Our	O
approach	O
improves	O
consistency	O
of	O
three	O
recent	O
generative	O
agents	O
(	O
See	O
et	O
al	O
,	O
2019	O
;	O
Wolf	O
et	O
al	O
,	O
2019b	O
;	O
over	O
Dialogue	O
NLI	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
and	O
PersonaChat	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Along	O
with	O
large	O
reduction	O
in	O
contradiction	O
,	O
the	O
utterance	O
accuracy	B-MetricName
significantly	O
increases	O
too	O
.	O

Persona	O
&	O
Consistency	O
in	O
Dialogue	O
.	O
Li	O
et	O
al	O
(	O
2016	O
)	O
learn	O
personas	O
in	O
embeddings	O
.	O
Zhang	O
et	O
al	O
(	O
2018	O
)	O
release	O
the	O
PersonaChat	O
dataset	O
,	O
a	O
chitchat	O
dialogue	O
set	O
involving	O
two	O
interlocutors	O
each	O
playing	O
their	O
given	O
persona	O
.	O
Madotto	O
et	O
al	O
(	O
2019	O
)	O
use	O
meta	B-TaskName
-	I-TaskName
learning	I-TaskName
to	O
adapt	O
to	O
new	O
personas	O
with	O
few	O
dialogue	O
samples	O
.	O
use	O
reinforcement	O
learning	O
to	O
enhance	O
mutual	O
persona	O
perception	O
.	O
Recent	O
works	O
use	O
extra	O
modules	O
or	O
NLI	O
labels	O
to	O
improve	O
consistency	O
.	O
Shum	O
et	O
al	O
(	O
2019	O
)	O
fill	O
generated	O
templates	O
,	O
and	O
rank	O
with	O
a	O
language	O
model	O
.	O
use	O
self	O
-	O
supervised	O
feature	O
extractors	O
for	O
generation	O
.	O
Welleck	O
et	O
al	O
(	O
2019	O
)	O
annotate	O
NLI	O
labels	O
to	O
the	O
PersonaChat	O
dataset	O
.	O
They	O
train	O
an	O
NLI	O
model	O
and	O
run	O
pairwise	O
comparison	O
between	O
candidates	O
and	O
persona	O
to	O
compute	O
contradiction	O
scores	O
.	O
The	O
NLI	O
approach	O
is	O
applied	O
for	O
coherence	B-TaskName
evaluation	I-TaskName
(	O
Dziri	O
et	O
al	O
,	O
2019	O
)	O
,	O
rewards	O
to	O
reinforcement	O
learning	O
agents	O
(	O
Song	O
et	O
al	O
,	O
2019	O
)	O
,	O
finding	O
inconsistent	O
words	O
(	O
Song	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
unlikelihood	O
training	O
(	O
Li	O
et	O
al	O
,	O
2020	O
)	O
.	O
They	O
require	O
NLI	O
labels	O
on	O
the	O
target	O
dialogue	O
dataset	O
;	O
otherwise	O
,	O
sharp	O
decrease	O
in	O
performance	O
is	O
observed	O
,	O
due	O
to	O
mismatch	O
of	O
data	O
distribution	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
.	O
Such	O
dataset	O
-	O
specific	O
NLI	O
annotations	O
and	O
training	O
NLI	O
models	O
can	O
be	O
costly	O
and	O
time	O
-	O
consuming	O
.	O
Compared	O
to	O
previous	O
methods	O
,	O
the	O
novelty	O
of	O
our	O
approach	O
is	O
to	O
improve	O
consistency	O
without	O
NLI	O
labels	O
and	O
extra	O
modules	O
.	O
Pragmatics	O
.	O
Our	O
approach	O
belongs	O
to	O
the	O
general	O
family	O
of	O
Bayesian	O
Rational	O
Speech	O
Acts	O
(	O
Andreas	O
and	O
Klein	O
,	O
2016	O
)	O
,	O
image	B-TaskName
captioning	I-TaskName
(	O
Mao	O
et	O
al	O
,	O
2016	O
;	O
Vedantam	O
et	O
al	O
,	O
2017	O
;	O
Cohn	O
-	O
Gordon	O
et	O
al	O
,	O
2018	O
)	O
,	O
instruction	O
following	O
(	O
Fried	O
et	O
al	O
,	O
2017	O
)	O
,	O
navigating	O
(	O
Fried	O
et	O
al	O
,	O
2018	O
)	O
,	O
translation	O
(	O
Cohn	O
-	O
Gordon	O
and	O
Goodman	O
,	O
2019	O
)	O
,	O
summarization	B-TaskName
(	O
Shen	O
et	O
al	O
,	O
2019	O
)	O
and	O
referring	B-TaskName
expression	I-TaskName
generation	O
(	O
Zarrieß	O
and	O
Schlangen	O
,	O
2019	O
)	O
.	O
However	O
,	O
its	O
application	O
to	O
the	O
dialogue	O
domain	O
remains	O
understudied	O
.	O
In	O
this	O
work	O
,	O
we	O
explore	O
how	O
the	O
RSA	O
framework	O
can	O
be	O
adopted	O
in	O
dialogue	O
agents	O
to	O
alleviate	O
the	O
inconsistency	O
problem	O
.	O
Also	O
,	O
we	O
further	O
extend	O
the	O
framework	O
by	O
making	O
the	O
distractor	O
selection	O
as	O
a	O
learnable	O
process	O
.	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
They	O
collect	O
entailing	O
and	O
contradictory	O
utterances	O
to	O
the	O
given	O
persona	O
,	O
and	O
release	O
an	O
evaluation	O
set	O
comprised	O
of	O
dialogues	O
each	O
with	O
31	O
utterance	O
candidates	O
:	O
10	O
entailing	O
,	O
10	O
neutral	O
,	O
and	O
10	O
contradictory	O
utterances	O
with	O
1	O
ground	O
-	O
truth	O
(	O
GT	O
)	O
utterance	O
.	O
On	O
this	O
evaluation	O
set	O
,	O
we	O
run	O
three	O
recent	O
models	O
(	O
See	O
et	O
al	O
,	O
2019	O
;	O
Wolf	O
et	O
al	O
,	O
2019b	O
et	O
al	O
,	O
2020	O
)	O
that	O
achieve	O
the	O
best	O
performance	O
on	O
PersonaChat	O
.	O
We	O
report	O
four	O
ranking	O
metrics	O
following	O
Welleck	O
et	O
al	O
(	O
2019	O
)	O
:	O
Hits@1	B-MetricName
,	O
Entail@1	O
,	O
Neutral@1	O
and	O
Contradict@1	O
.	O
Each	O
metric	O
is	O
the	O
proportion	O
of	O
GT	O
,	O
entailing	O
,	O
neutral	O
and	O
contradictory	O
utterances	O
in	O
the	O
top	O
-	O
1	O
candidates	O
returned	O
by	O
the	O
model	O
,	O
respectively	O
.	O
The	O
models	O
rank	O
the	O
candidates	O
by	O
perplexity	B-MetricName
scores	O
.	O
Figure	O
2	O
shows	O
that	O
all	O
three	O
models	O
select	O
contradictory	O
candidates	O
much	O
more	O
often	O
than	O
the	O
GT	O
utterances	O
(	O
see	O
further	O
results	O
in	O
Table	O
3	O
)	O
.	O
Though	O
models	O
are	O
conditioned	O
on	O
a	O
given	O
persona	O
,	O
they	O
are	O
highly	O
insensitive	O
to	O
contradictions	O
.	O

To	O
investigate	O
why	O
insensitivity	O
to	O
contradiction	O
prevails	O
in	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
,	O
we	O
further	O
analyze	O
the	O
contradictory	O
utterances	O
returned	O
by	O
the	O
models	O
(	O
Contradict@1	O
-	O
Utt	O
)	O
,	O
comparing	O
with	O
the	O
GT	O
utterances	O
and	O
the	O
top	O
-	O
ranked	O
entailing	O
candidates	O
(	O
Top	O
Entail	O
-	O
Utt	O
)	O
.	O
Table	O
1	O
reports	O
language	O
metrics	O
between	O
the	O
selected	O
candidates	O
and	O
the	O
given	O
persona	O
sentences	O
using	O
SPICE	B-DatasetName
(	O
Anderson	O
et	O
al	O
,	O
2016	O
)	O
and	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
.	O
SPICE	B-DatasetName
metric	O
measures	O
semantic	B-TaskName
similarity	I-TaskName
and	O
ROUGE	O
metric	O
measures	O
n	O
-	O
gram	O
overlaps	O
between	O
two	O
sentences	O
.	O
Contradict@1	O
-	O
Utt	O
shows	O
lower	O
SPICE	B-DatasetName
scores	O
and	O
higher	O
ROUGE	O
scores	O
than	O
other	O
utterances	O
,	O
implying	O
that	O
it	O
may	O
be	O
different	O
in	O
semantics	O
but	O
similar	O
in	O
syntax	O
to	O
the	O
given	O
persona	O
.	O
To	O
take	O
a	O
closer	O
look	O
,	O
we	O
extract	O
the	O
contradicting	O
words	O
from	O
Contradict@1	O
-	O
Utt	O
and	O
their	O
counterparts	O
from	O
GT	O
utterances	O
to	O
compare	O
their	O
average	O
perplexity	B-MetricName
scores	O
.	O
In	O
the	O
Dialogue	O
NLI	O
dataset	O
,	O
every	O
utterance	O
is	O
labeled	O
with	O
a	O
triple	O
(	O
entity	O
1	O
,	O
relation	O
,	O
entity	O
2	O
)	O
,	O
such	O
as	O
"	O
I	O
just	O
like	O
to	O
listen	O
to	O
rock	O
music	O
"	O
with	O
(	O
i	O
,	O
like	O
music	O
,	O
rock	O
)	O
.	O
By	O
construction	O
,	O
Contradict@1	O
-	O
Utt	O
must	O
contain	O
words	O
that	O
are	O
contradictory	O
to	O
the	O
GT	O
utterance	O
and	O
the	O
given	O
persona	O
.	O
The	O
perplexity	B-MetricName
scores	O
of	O
contradictory	O
words	O
(	O
106.7	O
)	O
were	O
considerably	O
lower	O
than	O
those	O
of	O
the	O
counterparts	O
in	O
GT	O
utterances	O
(	O
280.1	O
)	O
.	O
Table	O
2	O
shows	O
an	O
example	O
of	O
such	O
dialogue	O
instance	O
with	O
perplexity	B-MetricName
per	O
word	O
.	O
If	O
properly	O
conditioned	O
with	O
the	O
given	O
persona	O
,	O
models	O
should	O
show	O
lower	O
perplexity	B-MetricName
for	O
the	O
words	O
in	O
the	O
persona	O
.	O
However	O
,	O
their	O
perplexity	B-MetricName
scores	O
are	O
significantly	O
higher	O
than	O
those	O
of	O
contradictory	O
words	O
.	O
It	O
reveals	O
that	O
models	O
behave	O
more	O
as	O
a	O
plain	O
language	O
model	O
rather	O
than	O
as	O
a	O
persona	O
-	O
conditioned	O
model	O
.	O
Thus	O
,	O
guarantee	O
of	O
consistency	O
for	O
each	O
word	O
generation	O
step	O
is	O
required	O
for	O
persona	O
-	O
based	O
dialogue	O
agents	O
to	O
resolve	O
such	O
issue	O
.	O

We	O
seek	O
to	O
build	O
a	O
dialogue	O
agent	B-DatasetName
who	O
is	O
selfconscious	O
about	O
its	O
consistency	O
without	O
the	O
need	O
for	O
training	O
on	O
NLI	O
labels	O
or	O
rating	O
consistency	O
with	O
NLI	O
models	O
.	O
Given	O
that	O
modeling	O
the	O
interactions	O
between	O
listener	O
and	O
speaker	O
is	O
a	O
main	O
topic	O
in	O
pragmatics	O
,	O
we	O
take	O
advantage	O
of	O
the	O
RSA	O
framework	O
(	O
Frank	O
and	O
Goodman	O
,	O
2012	O
)	O
.	O
It	O
treats	O
language	O
use	O
as	O
a	O
recursive	O
process	O
where	O
probabilistic	O
speaker	O
and	O
listener	O
reason	O
about	O
each	O
other	O
's	O
intentions	O
in	O
a	O
Bayesian	O
fashion	O
.	O
To	O
apply	O
the	O
framework	O
to	O
sequence	O
generation	O
for	O
dialogues	O
,	O
we	O
extend	O
the	O
incremental	O
approach	O
proposed	O
for	O
image	B-TaskName
captioning	I-TaskName
(	O
Cohn	O
-	O
Gordon	O
et	O
al	O
,	O
2018	O
)	O
.	O
To	O
generate	O
an	O
utterance	O
,	O
the	O
agent	B-DatasetName
computes	O
the	O
distribution	O
of	O
every	O
next	O
token	O
u	O
t	O
at	O
timestep	O
t	O
in	O
Bayesian	O
fashion	O
as	O
follows	O
.	O
Base	O
Speaker	O
S	O
0	B-DatasetName
.	O
We	O
first	O
assume	O
persona	O
i	O
is	O
given	O
to	O
the	O
base	O
speaker	O
,	O
along	O
with	O
the	O
dialogue	O
∝	O
#	O
"	O
ℎ	O
,	O
$	O
"	O
,	O
"	O
%	O
×	O
#	O
"	O
"	O
,	O
ℎ	O
,	O
&	O
"	O
)	O
"	O
'	O
!	O
(	O
)	O
Imaginary	O
Listener	O
:	O
#	O
"	O
(	O
|	O
$	O
"	O
,	O
ℎ	O
,	O
"	O
)	O
Base	O
Speaker	O
:	O
#	O
"	O
"	O
,	O
ℎ	O
,	O
&	O
"	O
)	O
Figure	O
3	O
:	O
The	O
proposed	O
self	O
-	O
conscious	O
agent	B-DatasetName
S	O
1	O
consists	O
of	O
base	O
speaker	O
S	O
0	B-DatasetName
and	O
imaginary	O
listener	O
L	O
0	B-DatasetName
.	O
It	O
recursively	O
generates	O
the	O
next	O
token	O
u	O
t	O
at	O
every	O
time	O
t.	O
history	O
h	O
and	O
partial	O
utterance	O
u	O
<	O
t	O
,	O
as	O
shown	O
in	O
Figure	O
3	O
.	O
The	O
base	O
speaker	O
S	O
t	O
0	B-DatasetName
returns	O
a	O
distribution	O
over	O
the	O
next	O
token	O
at	O
timestep	O
t	O
:	O
S	O
t	O
0	B-DatasetName
(	O
u	O
t	O
|	O
i	O
,	O
h	O
,	O
u	O
<	O
t	O
)	O
.	O
Any	O
conditional	O
dialogue	O
agent	B-DatasetName
can	O
be	O
used	O
as	O
a	O
base	O
speaker	O
.	O
See	O
the	O
details	O
in	O
Section	O
5.2	O
.	O
Imaginary	O
Listener	O
L	O
0	B-DatasetName
.	O
While	O
the	O
base	O
speaker	O
generates	O
each	O
token	O
one	O
at	O
a	O
time	O
,	O
the	O
imaginary	O
listener	O
reasons	O
about	O
the	O
speaker	O
's	O
persona	O
.	O
The	O
imaginary	O
listener	O
L	O
t	O
0	B-DatasetName
is	O
the	O
posterior	O
distribution	O
of	O
the	O
speaker	O
's	O
persona	O
in	O
terms	O
of	O
the	O
base	O
speaker	O
and	O
the	O
world	O
prior	O
p	O
t	O
(	O
i	O
)	O
over	O
personas	O
as	O
follows	O
,	O
L	O
t	O
0	B-DatasetName
(	O
i	O
|	O
h	O
,	O
u	O
≤t	O
,	O
p	O
t	O
)	O
∝	O
S	O
t	O
0	B-DatasetName
(	O
u	O
t	O
|	O
i	O
,	O
h	O
,	O
u	O
<	O
t	O
)	O
β	B-HyperparameterName
×	O
p	O
t	O
(	O
i	O
)	O
i	O
I	O
S	O
t	O
0	B-DatasetName
(	O
u	O
t	O
|	O
i	O
,	O
h	O
,	O
u	O
<	O
t	O
)	O
β	B-HyperparameterName
×	O
p	O
t	O
(	O
i	O
)	O
.	O
(	O
1	O
)	O
where	O
β	B-HyperparameterName
on	O
S	O
t	O
0	B-DatasetName
is	O
the	O
listener	O
rationality	O
coefficient	O
that	O
controls	O
the	O
amount	O
of	O
information	O
from	O
the	O
current	O
timestep	O
compared	O
to	O
the	O
cumulative	O
prior	O
p	O
t	O
(	O
i	O
)	O
.	O
L	O
0	B-DatasetName
returns	O
a	O
probability	O
distribution	O
over	O
the	O
personas	O
in	O
world	O
I	O
,	O
which	O
is	O
a	O
finite	O
set	O
(	O
|	O
I	O
|	O
=	O
3	O
)	O
comprising	O
the	O
given	O
persona	O
i	O
and	O
distractor	O
personas	O
.	O
The	O
distractors	O
are	O
different	O
personas	O
from	O
other	O
dialogue	O
instances	O
in	O
the	O
dataset	O
.	O
We	O
decide	O
world	O
I	O
per	O
dialogue	O
instance	O
through	O
learning	O
,	O
which	O
will	O
be	O
elaborated	O
in	O
Section	O
4.2	O
.	O
Self	O
-	O
Conscious	O
Speaker	O
S	O
1	O
.	O
With	O
S	O
t	O
0	B-DatasetName
and	O
L	O
t	O
0	B-DatasetName
,	O
the	O
self	O
-	O
conscious	O
speaker	O
S	O
t	O
1	O
is	O
defined	O
as	O
S	O
t	O
1	O
(	O
u	O
t	O
|	O
i	O
,	O
h	O
,	O
u	O
<	O
t	O
)	O
∝	O
L	O
t	O
0	B-DatasetName
(	O
i	O
|	O
h	O
,	O
u	O
≤t	O
,	O
p	O
t	O
)	O
α	B-HyperparameterName
×	O
S	O
t	O
0	B-DatasetName
(	O
u	O
t	O
|	O
i	O
,	O
h	O
,	O
u	O
<	O
t	O
)	O
,	O
(	O
2	O
)	O
where	O
α	B-HyperparameterName
is	O
the	O
speaker	O
rationality	O
coefficient	O
that	O
determines	O
how	O
much	O
the	O
likelihood	O
is	O
considered	O
.	O
By	O
taking	O
the	O
listener	O
's	O
distribution	O
into	O
account	O
,	O
the	O
speaker	O
is	O
now	O
self	O
-	O
conscious	O
about	O
what	O
persona	O
it	O
sounds	O
like	O
.	O
Especially	O
,	O
the	O
agent	B-DatasetName
seeks	O
to	O
be	O
perceived	O
as	O
the	O
given	O
persona	O
i	O
rather	O
than	O
some	O
other	O
persona	O
i	O
.	O
The	O
likelihood	O
of	O
each	O
token	O
being	O
identified	O
as	O
the	O
persona	O
i	O
acts	O
as	O
a	O
bonus	O
added	O
to	O
the	O
base	O
speaker	O
's	O
token	O
scores	O
.	O
Hence	O
,	O
tokens	O
that	O
are	O
consistent	O
to	O
the	O
given	O
persona	O
are	O
preferred	O
to	O
others	O
.	O
The	O
token	O
with	O
the	O
highest	O
probability	O
is	O
added	O
to	O
the	O
partial	O
utterance	O
,	O
becoming	O
the	O
next	O
input	O
u	O
<	O
t+1	O
for	O
the	O
speaker	O
.	O
Updating	O
the	O
world	O
prior	O
with	O
L	O
0	B-DatasetName
.	O
Starting	O
from	O
a	O
uniform	O
distribution	O
as	O
the	O
initial	O
prior	O
p	O
0	B-DatasetName
(	O
i	O
)	O
,	O
we	O
update	O
the	O
world	O
prior	O
p	O
t+1	O
(	O
i	O
)	O
according	O
to	O
S	O
1	O
's	O
output	O
u	O
t	O
at	O
every	O
time	O
step	O
:	O
p	O
t+1	O
(	O
i	O
)	O
=	O
L	O
t	O
0	B-DatasetName
(	O
i	O
|	O
h	O
,	O
u	O
≤t	O
,	O
p	O
t	O
)	O
.	O
(	O
3	O
)	O
Hence	O
,	O
p	O
t	O
(	O
i	O
)	O
represents	O
the	O
cumulative	O
state	O
of	O
the	O
partial	O
utterance	O
up	O
to	O
t.	O
Cohn	O
-	O
Gordon	O
et	O
al	O
(	O
2018	O
)	O
report	O
the	O
prior	O
update	O
with	O
L	O
1	O
∝	O
S	O
t	O
0	B-DatasetName
(	O
u	O
t	O
|	O
i	O
,	O
h	O
,	O
u	O
<	O
t	O
)	O
×	O
L	O
t	O
0	B-DatasetName
(	O
i	O
|	O
h	O
,	O
u	O
≤t	O
,	O
p	O
t	O
)	O
makes	O
little	O
practical	O
effect	O
compared	O
to	O
a	O
uniform	O
prior	O
.	O
We	O
find	O
that	O
updating	O
the	O
prior	O
with	O
Eq	O
.	O
(	O
3	O
)	O
instead	O
is	O
effective	O
.	O
See	O
the	O
results	O
in	O
Section	O
5.6	O
.	O

Distractors	O
(	O
Andreas	O
and	O
Klein	O
,	O
2016	O
)	O
are	O
samples	O
(	O
e.g.	O
other	O
personas	O
in	O
the	O
dataset	O
)	O
which	O
are	O
different	O
from	O
the	O
given	O
target	O
.	O
In	O
previous	O
works	O
of	O
RSA	O
,	O
the	O
distractors	O
to	O
be	O
included	O
in	O
world	O
I	O
are	O
selected	O
manually	O
or	O
randomly	O
from	O
the	O
dataset	O
.	O
However	O
,	O
we	O
find	O
that	O
performance	O
variance	O
is	O
large	O
according	O
to	O
the	O
selected	O
distractors	O
.	O
We	O
thus	O
propose	O
to	O
learn	O
distractor	O
selection	O
,	O
especially	O
based	O
on	O
the	O
life	O
-	O
long	O
memory	B-MethodName
network	I-MethodName
(	O
Kaiser	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
life	O
-	O
long	O
memory	B-MethodName
network	I-MethodName
is	O
capable	O
of	O
implicitly	O
clustering	O
similar	O
dialogue	O
contexts	O
into	O
a	O
few	O
slots	O
with	O
associated	O
persona	O
.	O
Therefore	O
,	O
it	O
can	O
efficiently	O
memorize	O
and	O
retrieve	O
distractor	O
personas	O
for	O
each	O
context	O
.	O
In	O
Appendix	O
,	O
we	O
experiment	O
that	O
our	O
approach	O
outperforms	O
other	O
models	O
including	O
BERT	B-MethodName
-	O
based	O
algorithms	O
.	O
To	O
better	O
select	O
useful	O
distractor	O
personas	O
,	O
supervised	O
learning	O
is	O
desirable	O
.	O
However	O
,	O
there	O
is	O
no	O
explicit	O
label	O
indicating	O
which	O
distractors	O
are	O
helpful	O
for	O
each	O
dialogue	O
.	O
We	O
select	O
the	O
persona	O
that	O
have	O
the	O
best	O
Hits@1	B-MetricName
as	O
the	O
distractor	O
label	O
per	O
training	O
dialogue	O
.	O
The	O
Hits@1	B-MetricName
is	O
the	O
score	O
for	O
favoring	O
the	O
ground	O
-	O
truth	O
next	O
utterance	O
(	O
consistent	O
and	O
context	O
-	O
relevant	O
)	O
over	O
other	O
candidate	O
utterances	O
which	O
are	O
just	O
being	O
consistent	O
(	O
i.e.	O
entailing	O
)	O
or	O
contradictory	O
to	O
the	O
given	O
persona	O
.	O
In	O
other	O
words	O
,	O
the	O
score	O
represents	O
consistency	O
and	O
also	O
appropriateness	O
at	O
the	O
same	O
time	O
.	O
Thus	O
,	O
such	O
distractors	O
can	O
help	O
the	O
self	O
-	O
conscious	O
agent	B-DatasetName
to	O
generate	O
responses	O
which	O
are	O
context	O
-	O
relevant	O
and	O
allow	O
the	O
imaginary	O
listener	O
to	O
identify	O
the	O
speaker	O
's	O
persona	O
.	O
Each	O
training	O
datapoint	O
comprises	O
a	O
given	O
persona	O
,	O
a	O
distractor	O
persona	O
and	O
dialogue	O
context	O
.	O
Memory	O
Structure	O
.	O
The	O
memory	O
consists	O
of	O
three	O
types	O
of	O
information	O
:	O
M	O
=	O
(	O
K	O
,	O
v	O
,	O
a	O
)	O
.	O
K	O
R	O
m×d	O
is	O
a	O
key	O
matrix	O
,	O
where	O
m	O
is	O
the	O
number	O
of	O
memory	O
slots	O
and	O
d	O
is	O
the	O
dimension	O
of	O
the	O
key	O
vectors	O
,	O
which	O
are	O
the	O
embedding	O
of	O
datapoints	O
.	O
The	O
value	O
vector	O
v	O
R	O
m	O
stores	O
the	O
index	O
of	O
a	O
persona	O
.	O
a	O
R	O
m	O
is	O
an	O
age	O
vector	O
,	O
which	O
is	O
used	O
for	O
memory	O
update	O
.	O
We	O
set	O
m	O
=	O
16	O
,	O
000	O
and	O
d	O
=	O
768	O
.	O
Memory	O
Addressing	O
.	O
We	O
construct	O
the	O
query	O
vector	O
q	O
for	O
each	O
datapoint	O
with	O
the	O
BERT	B-MethodName
-	O
Uncased	O
-	O
Base	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
model	O
.	O
We	O
use	O
the	O
output	O
embedding	O
of	O
BERT	B-MethodName
's	O
[	O
CLS	O
]	O
token	O
,	O
and	O
normalize	O
it	O
to	O
a	O
unit	O
length	O
to	O
build	O
q	O
R	O
d	O
.	O
Using	O
the	O
cosine	O
similarity	O
between	O
q	O
and	O
each	O
memory	O
key	O
,	O
we	O
can	O
find	O
the	O
k	O
nearest	O
neighbors	O
:	O
(	O
n	O
1	O
,	O
n	O
2	O
,	O
...	O
,	O
n	O
k	O
)	O
=	O
N	O
N	O
k	O
(	O
q	O
,	O
K	O
)	O
.	O
(	O
4	O
)	O
Memory	O
Loss	O
.	O
Suppose	O
that	O
the	O
query	O
datapoint	O
has	O
a	O
distractor	O
label	O
l.	O
Among	O
(	O
n	O
1	O
,	O
...	O
,	O
n	O
k	O
)	O
,	O
we	O
denote	O
the	O
positive	O
neighbor	O
n	O
p	O
as	O
the	O
one	O
with	O
v	O
[	O
n	O
p	O
]	O
=	O
l	O
and	O
the	O
negative	O
neighbor	O
n	O
b	O
with	O
v	O
[	O
n	O
b	O
]	O
=	O
l.	O
If	O
there	O
are	O
multiple	O
positive	O
neighbors	O
,	O
we	O
pick	O
the	O
one	O
with	O
the	O
smallest	O
memory	O
index	O
.	O
If	O
no	O
positive	O
neighbor	O
is	O
found	O
,	O
we	O
select	O
a	O
random	O
key	O
whose	O
value	O
is	O
l.	O
For	O
the	O
negative	O
neighbor	O
,	O
we	O
select	O
one	O
randomly	O
from	O
(	O
n	O
1	O
,	O
...	O
,	O
n	O
k	O
)	O
.	O
We	O
set	O
k	B-HyperparameterName
=	I-HyperparameterName
2048	B-DatasetName
.	O
Then	O
,	O
the	O
loss	B-MetricName
is	O
computed	O
as	O
L	O
=	O
max	O
(	O
q	O
K	O
[	O
n	O
b	O
]	O
−	O
q	O
K	O
[	O
n	O
p	O
]	O
+	O
α	B-HyperparameterName
,	O
0	B-DatasetName
)	O
,	O
(	O
5	O
)	O
where	O
α	B-HyperparameterName
is	O
a	O
positive	O
margin	O
,	O
which	O
we	O
set	O
as	O
0.2	O
.	O
This	O
loss	B-MetricName
maximizes	O
the	O
cosine	O
similarity	O
between	O
the	O
query	O
q	O
and	O
the	O
positive	O
key	O
K	O
[	O
n	O
p	O
]	O
,	O
while	O
minimizing	O
the	O
similarity	O
to	O
the	O
negative	O
key	O
K	O
[	O
n	O
b	O
]	O
.	O
We	O
finetune	O
the	O
query	O
network	O
BERT	B-MethodName
with	O
this	O
loss	B-MetricName
.	O
Memory	O
Update	O
.	O
After	O
computing	O
the	O
loss	B-MetricName
,	O
memory	O
M	O
is	O
updated	O
differently	O
for	O
two	O
cases	O
.	O
(	O
1	O
)	O
If	O
the	O
top	O
-	O
1	O
neighbor	O
's	O
value	O
(	O
i.e.	O
persona	O
)	O
is	O
correct	O
(	O
v	O
[	O
n	O
1	O
]	O
=	O
l	O
)	O
,	O
the	O
key	O
vector	O
is	O
updated	O
as	O
:	O
K	O
[	O
n	O
1	O
]	O
q	O
+	O
K	O
[	O
n	O
1	O
]	O
q	O
+	O
K	O
[	O
n	O
1	O
]	O
.	O
(	O
6	O
)	O
(	O
2	O
)	O
Otherwise	O
(	O
v	O
[	O
n	O
1	O
]	O
=	O
l	O
)	O
,	O
we	O
make	O
a	O
slot	O
for	O
the	O
query	O
;	O
we	O
find	O
the	O
oldest	O
memory	O
slot	O
n	O
according	O
to	O
the	O
age	O
vector	O
a	O
and	O
write	O
K	O
[	O
n	O
]	O
q	O
,	O
v	O
[	O
n	O
]	O
l	O
,	O
a	O
[	O
n	O
]	O
0	B-DatasetName
.	O
(	O
7	O
)	O
Training	O
&	O
Inference	O
.	O
In	O
our	O
Distractor	O
Memory	B-MethodName
network	I-MethodName
,	O
training	O
corresponds	O
to	O
updating	O
the	O
memory	O
and	O
the	O
parameters	O
of	O
the	O
query	O
network	O
.	O
At	O
inference	O
,	O
given	O
a	O
test	O
example	O
,	O
we	O
obtain	O
the	O
query	O
by	O
encoding	O
the	O
dialogue	O
context	O
and	O
the	O
persona	O
using	O
BERT	B-MethodName
.	O
We	O
find	O
n	O
nearest	O
keys	O
from	O
the	O
memory	O
,	O
and	O
use	O
their	O
values	O
(	O
i.e.	O
persona	O
indices	O
)	O
as	O
the	O
distractor	O
personas	O
.	O
We	O
set	O
n	O
=	O
2	O
.	O

Base	O
Speakers	O
.	O
We	O
experiment	O
on	O
three	O
pretrained	O
models	O
including	O
ControlSeq2Seq	O
(	O
See	O
et	O
al	O
,	O
2019	O
)	O
,	O
TransferTransfo	O
(	O
Wolf	O
et	O
al	O
,	O
2019b	O
)	O
,	O
and	O
Blender	B-MethodName
as	O
base	O
speakers	O
(	O
S	O
0	B-DatasetName
)	O
for	O
our	O
self	O
-	O
conscious	O
agents	O
(	O
S	O
1	O
)	O
.	O
The	O
ControlSeq2Seq	O
is	O
a	O
Seq2Seq	B-MethodName
model	O
with	O
attention	O
trained	O
on	O
Twitter	O
dataset	O
(	O
Miller	O
et	O
al	O
,	O
2017	O
)	O
and	O
finetuned	O
on	O
PersonaChat	O
.	O
TranferTransfo	O
based	O
on	O
GPT	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
is	O
the	O
winner	O
of	O
the	O
ConvAI2	B-DatasetName
competition	O
in	O
automatic	O
evaluation	O
.	O
Blender	B-MethodName
,	O
a	O
recently	O
released	O
generative	O
dialogue	O
model	O
,	O
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
open	O
-	O
domain	O
chatbot	B-TaskName
.	O
Our	O
approach	O
improves	O
these	O
base	O
speakers	O
by	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
.	O
+	O
DM	O
is	O
the	O
Distractor	O
Memory	O
.	O
High	O
scores	O
in	O
Hits@1	B-MetricName
,	O
Entail@1	O
and	O
low	O
scores	O
in	O
Contradict@1	O
imply	O
better	O
consistency	O
.	O
granting	O
them	O
the	O
sense	O
of	O
self	O
-	O
consciousness	O
.	O
We	O
defer	O
implementation	O
details	O
to	O
Appendix	O
.	O
Evaluation	O
Metrics	O
.	O
For	O
Dialogue	O
NLI	O
,	O
we	O
report	O
three	O
ranking	O
metrics	O
introduced	O
in	O
the	O
original	O
paper	O
:	O
Hits@1	B-MetricName
,	O
Entail@1	O
,	O
and	O
Contradict@1	O
.	O
Each	O
metric	O
is	O
the	O
proportion	O
of	O
GT	O
,	O
entailing	O
,	O
and	O
contradictory	O
utterances	O
in	O
the	O
top	O
-	O
1	O
candidates	O
returned	O
by	O
the	O
model	O
,	O
respectively	O
.	O
High	O
scores	O
in	O
Entail@1	O
and	O
low	O
scores	O
in	O
Contradict@1	O
indicate	O
better	O
consistency	O
with	O
the	O
persona	O
.	O
For	O
PersonaChat	O
,	O
we	O
report	O
Hits@1	B-MetricName
,	O
standard	O
F1	B-MetricName
score	I-MetricName
,	O
perplexity	B-MetricName
and	O
C	O
score	O
,	O
following	O
the	O
Con	O
-	O
vAI2	O
protocol	O
.	O
Hits@1	B-MetricName
is	O
the	O
accuracy	B-MetricName
of	O
choosing	O
the	O
ground	O
-	O
truth	O
next	O
-	O
utterance	O
among	O
20	O
candidates	O
as	O
the	O
models	O
rank	O
the	O
candidates	O
by	O
perplexity	B-MetricName
.	O
The	O
C	O
score	O
is	O
a	O
metric	O
for	O
dialogue	O
consistency	O
,	O
introduced	O
in	O
Madotto	O
et	O
al	O
(	O
2019	O
)	O
.	O
It	O
computes	O
pairwise	O
comparison	O
between	O
utterance	O
u	O
and	O
persona	O
sentence	O
p	O
j	O
with	O
a	O
pretrained	O
NLI	O
model	O
.	O
The	O
NLI	O
model	O
returns	O
1	O
,	O
0	B-DatasetName
,	O
-	O
1	O
for	O
entailment	O
,	O
neutrality	O
,	O
and	O
contradiction	O
,	O
respectively	O
.	O
We	O
sum	O
the	O
NLI	O
scores	O
across	O
persona	O
sentences	O
per	O
dialogue	O
instance	O
:	O
C	O
(	O
u	O
)	O
=	O
j	O
NLI	O
(	O
u	O
,	O
p	O
j	O
)	O
.	O

Results	O
on	O
Dialogue	O
NLI	O
.	O
Table	O
3	O
compares	O
the	O
performance	O
of	O
dialogue	O
agents	O
on	O
the	O
Dialogue	O
NLI	O
evaluation	O
set	O
.	O
Our	O
self	O
-	O
conscious	O
agent	B-DatasetName
S	O
1	O
significantly	O
reduces	O
Contradict@1	O
scores	O
and	O
increases	O
the	O
Entail@1	O
along	O
with	O
the	O
Hits@1	B-MetricName
accuracy	B-MetricName
of	O
the	O
literal	O
agents	O
S	O
0	B-DatasetName
.	O
We	O
remind	O
that	O
each	O
entailing	O
candidate	O
shares	O
the	O
same	O
annotated	O
triple	O
as	O
the	O
GT	O
utterance	O
.	O
In	O
other	O
words	O
,	O
they	O
have	O
similar	O
semantics	O
to	O
the	O
GT	O
utterance	O
and	O
follow	O
the	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
C	O
is	O
the	O
consistency	O
score	O
evaluated	O
by	O
a	O
pretrained	O
NLI	O
model	O
(	O
Madotto	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
TransferTransfo	O
,	O
we	O
use	O
the	O
generative	O
version	O
to	O
calculate	O
Hits@1	B-MetricName
.	O
given	O
persona	O
.	O
Thus	O
,	O
Entail@1	O
is	O
a	O
lenient	O
version	O
of	O
Hits@1	B-MetricName
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
Distractor	O
Memory	O
(	O
DM	O
)	O
is	O
better	O
than	O
random	O
distractor	O
selection	O
for	O
S	O
1	O
across	O
all	O
metrics	O
.	O
It	O
concludes	O
that	O
learned	O
distractors	O
are	O
more	O
effective	O
than	O
random	O
distractors	O
for	O
pragmatic	O
agents	O
.	O
Results	O
on	O
PersonaChat	O
.	O
Table	O
4	O
compares	O
the	O
performance	O
of	O
different	O
dialogue	O
agents	O
on	O
the	O
PersonaChat	O
dataset	O
.	O
Our	O
model	O
S	O
1	O
outperforms	O
all	O
other	O
generative	O
dialogue	O
agents	O
in	O
terms	O
of	O
consistency	O
related	O
metrics	O
,	O
i.e.	O
Hits@1	B-MetricName
and	O
C	O
score	O
.	O
Since	O
the	O
posterior	O
update	O
of	O
our	O
self	O
-	O
conscious	O
agent	B-DatasetName
revises	O
the	O
distribution	O
learned	O
by	O
the	O
base	O
speaker	O
,	O
the	O
increase	O
in	O
perplexity	B-MetricName
is	O
natural	O
due	O
to	O
the	O
effect	O
of	O
regularization	O
.	O
Nevertheless	O
,	O
our	O
approach	O
improves	O
the	O
F1	B-MetricName
score	I-MetricName
for	O
TransferTransfo	O
and	O
Blender	B-MethodName
.	O
Thus	O
,	O
being	O
consistent	O
to	O
the	O
given	O
persona	O
can	O
also	O
help	O
improve	O
the	O
generation	O
performance	O
of	O
dialogue	O
agents	O
.	O
Comparison	O
with	O
agents	O
that	O
use	O
NLI	O
model	O
.	O
We	O
also	O
test	O
agents	O
with	O
pretrained	O
NLI	O
models	O
attached	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
,	O
denoted	O
by	O
+	O
NLI	O
in	O
Table	O
5	O
.	O
The	O
NLI	O
model	O
computes	O
contradiction	O
scores	O
of	O
each	O
candidate	O
utterances	O
,	O
and	O
penalize	O
its	O
rank	O
accordingly	O
.	O
Compared	O
to	O
base	O
agents	O
with	O
no	O
self	O
-	O
consciousness	O
,	O
our	O
agents	O
improve	O
consistency	O
in	O
all	O
three	O
metrics	O
even	O
further	O
when	O
using	O
additional	O
NLI	O
models	O
.	O
Another	O
notable	O
result	O
is	O
that	O
our	O
agents	O
without	O
NLI	O
(	O
S	O
1	O
+	O
DM	O
in	O
Table	O
3	O
)	O
for	O
ControlSeq2Seq	O
and	O
TransferTransfo	O
even	O
outperform	O
the	O
base	O
agents	O
with	O
NLI	O
(	O
S	O
0	B-DatasetName
+	O
NLI	O
)	O
on	O
Hits@1	B-MetricName
.	O
That	O
is	O
,	O
our	O
self	O
-	O
conscious	O
agents	O
achieve	O
better	O
GT	O
accuracy	B-MetricName
even	O
without	O
the	O
help	O
of	O
an	O
NLI	O
model	O
trained	O
on	O
consistency	O
labels	O
.	O

To	O
further	O
analyze	O
our	O
self	O
-	O
conscious	O
agent	B-DatasetName
,	O
we	O
conduct	O
experiments	O
by	O
controlling	O
three	O
features	O
of	O
our	O
agent	B-DatasetName
:	O
world	O
prior	O
updates	O
p	O
t	O
(	O
i	O
)	O
,	O
listener	O
rationality	O
β	B-HyperparameterName
and	O
speaker	O
rationality	O
α	B-HyperparameterName
.	O
World	O
Prior	O
Update	O
.	O
In	O
the	O
self	O
-	O
conscious	O
agent	B-DatasetName
,	O
the	O
world	O
prior	O
acts	O
as	O
a	O
cumulative	O
state	O
over	O
personas	O
.	O
We	O
remind	O
that	O
we	O
propose	O
to	O
update	O
the	O
world	O
prior	O
with	O
L	O
t	O
0	B-DatasetName
instead	O
of	O
L	O
t	O
1	O
in	O
Eq	O
.	O
(	O
3	O
)	O
.	O
As	O
reported	O
in	O
Cohn	O
-	O
Gordon	O
et	O
al	O
(	O
2018	O
)	O
,	O
our	O
experiments	O
on	O
the	O
Dialogue	O
NLI	O
dataset	O
confirm	O
the	O
prior	O
update	O
with	O
L	O
t	O
1	O
makes	O
little	O
difference	O
in	O
performance	O
compared	O
with	O
using	O
a	O
uniform	O
distribution	O
.	O
However	O
,	O
our	O
approach	O
with	O
L	O
t	O
0	B-DatasetName
makes	O
significant	O
difference	O
,	O
as	O
shown	O
in	O
Figure	O
5	O
.	O
The	O
reason	O
is	O
that	O
the	O
pragmatic	O
listener	O
L	O
t	O
1	O
∝	O
S	O
t	O
0	B-DatasetName
(	O
u	O
t	O
|	O
i	O
,	O
h	O
,	O
u	O
<	O
t	O
)	O
×	O
L	O
t	O
0	B-DatasetName
(	O
i	O
|	O
h	O
,	O
u	O
≤t	O
,	O
p	O
t	O
)	O
reflects	O
the	O
current	O
S	O
t	O
0	B-DatasetName
twice	O
(	O
i.e.	O
in	O
L	O
t	O
0	B-DatasetName
and	O
in	O
itself	O
)	O
per	O
time	O
step	O
.	O
Hence	O
,	O
the	O
update	O
with	O
L	O
t	O
1	O
becomes	O
more	O
of	O
an	O
instantaneous	O
prior	O
rather	O
than	O
a	O
cumulative	O
one	O
.	O
On	O
the	O
other	O
hand	O
,	O
L	O
t	O
0	B-DatasetName
moderately	O
combines	O
the	O
information	O
from	O
both	O
S	O
t	O
0	B-DatasetName
and	O
p	O
t	O
(	O
i	O
)	O
,	O
preserving	O
better	O
cumulative	O
information	O
.	O
Listener	O
Rationality	O
β	B-HyperparameterName
.	O
We	O
add	O
β	B-HyperparameterName
in	O
L	O
t	O
0	B-DatasetName
to	O
control	O
the	O
amount	O
of	O
information	O
incorporated	O
to	O
the	O
world	O
prior	O
p	O
t	O
(	O
i	O
)	O
.	O
Figure	O
5	O
depicts	O
that	O
when	O
β	B-HyperparameterName
is	O
large	O
,	O
the	O
Hits@1	B-MetricName
scores	O
(	O
i.e.	O
the	O
GT	O
accuracy	B-MetricName
)	O
drop	O
.	O
With	O
a	O
big	O
β	B-HyperparameterName
,	O
the	O
information	O
S	O
t	O
0	B-DatasetName
at	O
current	O
time	O
step	O
overrides	O
the	O
cumulative	O
prior	O
p	O
t	O
(	O
i	O
)	O
.	O
That	O
is	O
,	O
the	O
utterance	O
state	O
evolves	O
shortsightedly	O
,	O
ignoring	O
the	O
context	O
information	O
from	O
the	O
previous	O
steps	O
.	O
Therefore	O
,	O
setting	O
of	O
β	B-HyperparameterName
≤	O
1	O
is	O
advantageous	O
for	O
the	O
self	O
-	O
conscious	O
agent	B-DatasetName
to	O
incrementally	O
decode	O
.	O
Speaker	O
Rationality	O
α	B-HyperparameterName
.	O
Figure	O
6	O
shows	O
an	O
example	O
of	O
how	O
generated	O
responses	O
vary	O
according	O
to	O
the	O
intensity	O
of	O
speaker	O
rationality	O
α	B-HyperparameterName
.	O
As	O
α	B-HyperparameterName
increases	O
,	O
the	O
self	O
-	O
conscious	O
agent	B-DatasetName
reflects	O
the	O
listener	O
's	O
distribution	O
(	O
i.e.	O
the	O
likelihood	O
)	O
more	O
into	O
the	O
posterior	O
.	O
When	O
α	B-HyperparameterName
is	O
too	O
large	O
,	O
the	O
posterior	O
distribution	O
is	O
overwhelmed	O
by	O
the	O
likelihood	O
of	O
the	O
persona	O
.	O
Then	O
,	O
the	O
language	O
model	O
degenerates	O
to	O
favor	O
uttering	O
fragments	O
of	O
the	O
given	O
persona	O
while	O
even	O
ignoring	O
the	O
syntax	O
.	O
Hence	O
,	O
α	B-HyperparameterName
can	O
control	O
the	O
degree	O
of	O
copying	O
the	O
given	O
condition	O
text	O
.	O
An	O
appropriate	O
α	B-HyperparameterName
value	O
allows	O
the	O
given	O
persona	O
condition	O
to	O
blend	O
smoothly	O
in	O
the	O
utterance	O
.	O

This	O
work	O
investigated	O
how	O
modeling	O
public	O
selfconsciousness	O
can	O
help	O
dialogue	O
agents	O
improve	O
persona	O
-	O
consistency	O
.	O
We	O
showed	O
existing	O
dialogue	O
agents	O
are	O
highly	O
insensitive	O
to	O
contradiction	O
,	O
and	O
introduced	O
an	O
orthogonally	O
applicable	O
method	O
using	O
the	O
RSA	O
framework	O
(	O
Frank	O
and	O
Goodman	O
,	O
2012	O
)	O
to	O
alleviate	O
the	O
issue	O
.	O
We	O
also	O
designed	O
a	O
0.0	O
0.5	O
1.0	O
1.5	O
2.0	O
2.5	O
3.0	O
3	O
.	O
learning	O
method	O
for	O
distractor	O
selection	O
,	O
named	O
Distractor	O
Memory	O
and	O
proposed	O
a	O
better	O
update	O
for	O
the	O
listener	O
's	O
world	O
prior	O
.	O
Furthermore	O
,	O
we	O
demonstrated	O
how	O
our	O
approach	O
can	O
be	O
generalized	O
to	O
improve	O
dialogue	O
context	O
-	O
consistency	O
.	O
Our	O
self	O
-	O
conscious	O
agents	O
improved	O
the	O
base	O
agents	O
on	O
the	O
Dialogue	O
NLI	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
and	O
PersonaChat	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
dataset	O
,	O
without	O
consistency	O
labels	O
and	O
NLI	O
models	O
.	O
An	O
important	O
future	O
direction	O
will	O
be	O
generating	O
the	O
distractors	O
and	O
learning	O
the	O
rationality	O
coefficients	O
.	O
A	O
Results	O
on	O
Variants	O
of	O
Distractor	O
Selection	O
(	O
Section	O
4.2	O
)	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
compare	O
our	O
proposed	O
Distractor	O
Memory	O
(	O
DM	O
)	O
with	O
three	O
heuristic	O
methods	O
,	O
and	O
two	O
variants	O
of	O
the	O
pretrained	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
a	O
straightforward	O
baseline	O
,	O
we	O
randomly	O
select	O
k	O
personas	O
from	O
training	O
set	O
and	O
directly	O
use	O
it	O
as	O
distractors	O
.	O
Second	O
,	O
we	O
test	O
the	O
k	O
-	O
nearest	O
search	O
by	O
speaker	O
's	O
persona	O
,	O
denoted	O
by	O
Nearest	O
;	O
for	O
a	O
given	O
persona	O
descriptions	O
,	O
we	O
find	O
its	O
closest	O
training	O
persona	O
embedding	O
using	O
cosine	O
similarity	O
on	O
average	O
pooled	O
BERT	B-MethodName
features	O
.	O
The	O
third	O
baseline	O
denoted	O
by	O
Farthest	O
is	O
to	O
find	O
the	O
k	O
-	O
farthest	O
persona	O
among	O
the	O
training	O
personas	O
.	O
We	O
also	O
compare	O
with	O
two	O
variants	O
of	O
the	O
BERT	B-MethodName
model	O
.	O
The	O
first	O
variant	O
is	O
BERT	B-MethodName
-	O
Classifier	O
,	O
which	O
takes	O
dialogue	O
context	O
as	O
input	O
and	O
returns	O
the	O
index	O
of	O
persona	O
from	O
training	O
set	O
as	O
output	O
.	O
The	O
second	O
variant	O
is	O
bi	O
-	O
encoder	O
ranking	O
model	O
of	O
Miller	O
et	O
al	O
(	O
2017	O
)	O
,	O
denoted	O
by	O
BERT	B-MethodName
-	O
Ranker	O
.	O
It	O
encodes	O
dialogue	O
context	O
and	O
candidate	O
persona	O
with	O
separate	O
BERT	B-MethodName
encoders	O
measuring	O
its	O
ranking	O
with	O
cosine	O
similarity	O
.	O
For	O
both	O
methods	O
,	O
we	O
use	O
top	O
-	O
k	O
ranked	O
personas	O
as	O
distractors	O
and	O
set	O
k	B-HyperparameterName
=	I-HyperparameterName
4	O
for	O
all	O
the	O
methods	O
.	O
We	O
use	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
2e	O
-	O
5	O
and	O
finetune	O
BERT	B-MethodName
-	O
Uncased	O
-	O
Base	O
up	O
to	O
3	O
epochs	O
.	O
Table	O
8	O
compares	O
the	O
performance	O
of	O
different	O
distractor	O
selecting	O
methods	O
on	O
the	O
Dialogue	O
NLI	O
evaluation	O
set	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
set	O
α	B-HyperparameterName
=	O
8	O
,	O
β	B-HyperparameterName
=	O
0.5	O
,	O
and	O
|	O
I	O
|	O
=	O
5	O
.	O
The	O
DM	O
model	O
outperforms	O
all	O
the	O
baselines	O
across	O
all	O
metrics	O
.	O
The	O
Farthest	O
shows	O
better	O
performance	O
than	O
the	O
Nearest	O
.	O
It	O
can	O
be	O
understood	O
that	O
dissimilar	O
distractors	O
are	O
more	O
effective	O
in	O
the	O
Rational	O
Speech	O
Acts	O
framework	O
(	O
Frank	O
and	O
Goodman	O
,	O
2012	O
)	O
.	O
The	O
BERT	B-MethodName
-	O
Ranker	O
performs	O
the	O
best	O
among	O
baselines	O
,	O
but	O
not	O
as	O
good	O
as	O
ours	O
,	O
which	O
validates	O
that	O
memorization	O
capability	O
is	O
effective	O
for	O
selecting	O
useful	O
distractors	O
.	O

Base	O
Codes	O
and	O
Datasets	O
.	O
We	O
use	O
the	O
ParlAI	O
framework	O
2	O
(	O
Miller	O
et	O
al	O
,	O
2017	O
)	O
and	O
Hugging	O
-	O
Face	O
's	O
Transformers	O
3	O
(	O
Wolf	O
et	O
al	O
,	O
2019a	O
)	O
to	O
implement	O
our	O
models	O
and	O
baselines	O
.	O
We	O
use	O
Dialogue	O
NLI	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
and	O
PersonaChat	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
datasets	O
from	O
the	O
ParlAI	O
framework	O
as	O
is	O
.	O
We	O
use	O
the	O
default	O
preprocessing	O
in	O
ParlAI	O
.	O
Training	O
.	O
Our	O
self	O
-	O
consciousness	O
approach	O
improves	O
consistency	O
for	O
any	O
pretrained	O
dialogueagents	O
without	O
additional	O
consistency	O
labels	O
and	O
pretrained	O
NLI	O
models	O
.	O
Since	O
it	O
post	O
-	O
processes	O
the	O
output	O
probability	O
of	O
pretrained	O
dialogue	O
-	O
agents	O
in	O
a	O
Bayesian	O
fashion	O
,	O
no	O
additional	O
model	O
parameters	O
are	O
added	O
to	O
the	O
dialogue	O
agents	O
.	O
Thus	O
,	O
it	O
does	O
not	O
require	O
any	O
training	O
.	O
In	O
the	O
case	O
of	O
using	O
the	O
Distractor	O
Memory	O
(	O
DM	O
)	O
,	O
first	O
we	O
initialize	O
BERT	B-MethodName
-	O
Uncased	O
-	O
Base	O
with	O
pretrained	O
weights	O
and	O
finetune	O
it	O
up	O
to	O
3	O
epochs	O
with	O
Adam	B-MethodName
optimizer	B-HyperparameterName
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
2e	O
-	O
5	O
.	O
Then	O
we	O
find	O
the	O
best	O
distractor	O
persona	O
for	O
each	O
model	O
and	O
use	O
those	O
labels	O
to	O
train	O
our	O
DM	O
.	O
We	O
train	O
our	O
DM	O
on	O
one	O
NVIDIA	O
TITAN	B-DatasetName
Xp	O
GPU	O
up	O
to	O
7	O
epochs	O
.	O
Hyperparameters	O
.	O
For	O
Dialogue	O
NLI	O
evaluation	O
,	O
we	O
set	O
the	O
speaker	O
rationality	O
α	B-HyperparameterName
=	O
8.0	O
,	O
the	O
listener	O
rationality	O
β	B-HyperparameterName
=	O
1.0	O
,	O
and	O
the	O
cardinality	O
of	O
the	O
world	O
I	O
to	O
3	O
.	O
In	O
PersonaChat	O
evaluation	O
,	O
we	O
set	O
α	B-HyperparameterName
=	O
2.0	O
,	O
β	B-HyperparameterName
=	O
0.3	O
for	O
ControlSeq2Seq	O
(	O
See	O
et	O
al	O
,	O
2019	O
)	O
,	O
α	B-HyperparameterName
=	O
2	O
,	O
β	B-HyperparameterName
=	O
0.9	O
for	O
TransferTransfo	O
(	O
Wolf	O
et	O
al	O
,	O
2019b	O
)	O
,	O
and	O
α	B-HyperparameterName
=	O
2.0	O
,	O
β	B-HyperparameterName
=	O
0.5	O
for	O
Blender	B-MethodName
90	O
M	O
.	O
We	O
also	O
set	O
|	O
I	O
|	O
=	O
3	O
.	O
We	O
experiment	O
α	B-HyperparameterName
=	O
{	O
1.0	O
,	O
2.0	O
,	O
4.0	O
,	O
8.0	O
,	O
16.0	O
}	O
,	O
β	B-HyperparameterName
=	O
{	O
0.3	O
,	O
0.5	O
,	O
0.9	O
,	O
1.0	O
,	O
2.0	O
,	O
4.0	O
}	O
,	O
and	O
|	O
I	O
|	O
=	O
{	O
2	O
,	O
3	O
,	O
5	O
}	O
.	O
We	O
choose	O
the	O
hyper	O
-	O
parameter	O
configuration	O
showing	O
the	O
best	O
performance	O
in	O
Hits@1	B-MetricName
for	O
Dialogue	O
NLI	O
and	O
F1	B-MetricName
score	I-MetricName
for	O
PersonaChat	O
.	O
The	O
posterior	O
distribution	O
of	O
our	O
self	O
-	O
conscious	O
agents	O
are	O
computed	O
deterministically	O
.	O
For	O
our	O
Distractor	O
Memory	O
,	O
we	O
set	O
the	O
memory	O
key	O
matrix	O
as	O
K	O
R	O
m×d	O
,	O
where	O
m	O
=	O
16000	O
and	O
d	O
=	O
768	O
.	O
We	O
set	O
the	O
number	O
of	O
nearest	O
neighbor	O
k	B-HyperparameterName
=	I-HyperparameterName
2048	B-DatasetName
.	O
Inference	O
.	O
We	O
use	O
greedy	O
decoding	O
for	O
all	O
methods	O
.	O
The	O
average	O
runtime	O
for	O
our	O
self	O
-	O
conscious	O
approach	O
is	O
dependent	O
on	O
the	O
base	O
dialogue	O
agents	O
and	O
the	O
cardinality	O
of	O
world	O
I	O
which	O
can	O
be	O
run	O
in	O
parallel	O
like	O
beam	O
search	O
.	O
Evaluation	O
.	O
We	O
follow	O
the	O
evaluation	O
of	O
the	O
Par	O
-	O
lAI	O
framework	O
.	O
Following	O
Madotto	O
et	O
al	O
(	O
2019	O
)	O
,	O
2	O
https://parl.ai/	O
3	O
https://huggingface.co/transformers/	O
we	O
use	O
the	O
finetuned	O
BERT	B-MethodName
-	O
based	O
NLI	O
model	O
4	O
to	O
compute	O
the	O
C	O
score	O
.	O

To	O
investigate	O
the	O
influence	O
of	O
context	O
features	O
on	O
analogical	O
reasoning	O
,	O
we	O
consider	O
not	O
only	O
word	O
features	O
,	O
but	O
also	O
ngram	O
features	O
inspired	O
by	O
statistical	O
language	O
models	O
,	O
and	O
character	O
(	O
Hanzi	O
)	O
features	O
based	O
on	O
the	O
close	O
relationship	O
between	O
Chinese	O
words	O
and	O
their	O
composing	O
characters	O
5	O
.	O
Specifically	O
,	O
we	O
use	O
word	O
bigrams	O
for	O
ngram	O
features	O
,	O
character	O
unigrams	O
and	O
bigrams	O
for	O
character	O
features	O
.	O
Ngrams	O
and	O
Chinese	O
characters	O
are	O
effective	O
features	O
in	O
training	O
word	O
representations	O
(	O
Zhao	O
et	O
al	O
,	O
2017	O
;	O
Chen	O
et	O
al	O
,	O
2015	O
;	O
Bojanowski	O
et	O
al	O
,	O
2016	O
)	O
.	O
However	O
,	O
Table	O
4	O
shows	O
that	O
there	O
is	O
only	O
a	O
slight	O
increase	O
on	O
CA_translated	O
dataset	O
with	O
ngram	O
features	O
,	O
and	O
the	O
accuracies	O
in	O
most	O
cases	O
decrease	O
after	O
integrating	O
character	O
features	O
.	O
In	O
contrast	O
,	O
on	O
CA8	O
dataset	O
,	O
the	O
introduction	O
of	O
ngram	O
and	O
character	O
features	O
brings	O
significant	O
and	O
consistent	O
improvements	O
on	O
almost	O
all	O
the	O
categories	O
.	O
Furthermore	O
,	O
character	O
features	O
are	O
especially	O
advantageous	O
for	O
reasoning	O
of	O
morphological	O
relations	O
.	O
SGNS	O
model	O
integrating	O
with	O
character	O
features	O
even	O
doubles	O
the	O
accuracy	B-MetricName
in	O
morphological	O
questions	O
.	O
Besides	O
,	O
the	O
representations	O
achieve	O
surprisingly	O
high	O
accuracies	O
in	O
some	O
categories	O
of	O
CA_translated	O
,	O
which	O
means	O
that	O
there	O
is	O
little	O
room	O
for	O
further	O
improvement	O
.	O
However	O
it	O
is	O
much	O
harder	O
for	O
representation	O
methods	O
to	O
achieve	O
high	O
accuracies	O
on	O
CA8	O
.	O
The	O
best	O
configuration	O
only	O
achieves	O
68.0	O
%	O
.	O

We	O
used	O
the	O
scoring	O
metric	O
described	O
in	O
Thorne	O
et	O
al	O
(	O
2018	O
)	O
to	O
evaluate	O
the	O
submissions	O
.	O
The	O
FEVER	B-DatasetName
shared	O
task	O
requires	O
submission	O
of	O
evidence	O
to	O
justify	O
the	O
labeling	O
of	O
a	O
claim	O
.	O
The	O
training	O
,	O
development	O
and	O
test	O
data	O
splits	O
contain	O
multiple	O
sets	O
of	O
evidence	O
for	O
each	O
claim	O
,	O
each	O
set	O
being	O
a	O
minimal	O
set	O
of	O
sentences	O
that	O
fully	O
support	O
or	O
refute	O
it	O
.	O
The	O
primary	O
scoring	O
metric	O
for	O
the	O
task	O
is	O
the	O
label	O
accuracy	B-MetricName
conditioned	O
on	O
providing	O
at	O
least	O
one	O
complete	O
set	O
of	O
evidence	O
,	O
referred	O
to	O
as	O
the	O
FEVER	B-DatasetName
score	O
.	O
Sentences	O
labeled	O
(	O
correctly	O
)	O
as	O
NOTENOUGHINFO	O
do	O
not	O
require	O
evidence	O
.	O
Correctly	O
labeled	O
claims	O
with	O
no	O
or	O
only	O
partial	O
evidence	O
received	O
no	O
points	O
for	O
the	O
FEVER	B-DatasetName
score	O
.	O
Where	O
multiple	O
sets	O
of	O
evidence	O
was	O
annotated	O
in	O
the	O
data	O
,	O
only	O
one	O
set	O
was	O
required	O
for	O
the	O
claim	O
to	O
be	O
considered	O
correct	O
for	O
the	O
FEVER	B-DatasetName
score	O
.	O
Since	O
the	O
development	O
and	O
evaluation	O
data	O
splits	O
are	O
balanced	O
,	O
random	O
baseline	O
label	O
accuracy	B-MetricName
ignoring	O
the	O
requirement	O
for	O
evidence	O
is	O
33.33	O
%	O
.	O
This	O
performance	O
level	O
can	O
also	O
be	O
achieved	O
for	O
the	O
FEVER	B-DatasetName
score	O
by	O
predicting	O
NOTENOUGHINFO	O
for	O
every	O
claim	O
.	O
However	O
,	O
as	O
the	O
FEVER	B-DatasetName
score	O
requires	O
evidence	O
for	O
SUP	O
-	O
PORTED	O
and	O
REFUTED	O
claims	O
,	O
a	O
random	O
baseline	O
is	O
expected	O
to	O
score	O
lower	O
on	O
this	O
metric	O
.	O
We	O
provide	O
an	O
open	O
-	O
source	O
release	O
of	O
the	O
scoring	O
software	O
.	O
2	O
Beyond	O
the	O
FEVER	B-DatasetName
score	O
,	O
it	O
computes	O
precision	O
,	O
recall	O
,	O
F	O
1	O
,	O
and	O
label	O
accuracy	B-MetricName
to	O
provide	O
diagnostic	O
information	O
.	O
The	O
recall	O
point	O
is	O
awarded	O
,	O
as	O
is	O
the	O
case	O
for	O
the	O
FEVER	B-DatasetName
score	O
,	O
only	O
by	O
providing	O
a	O
complete	O
set	O
of	O
evidence	O
for	O
the	O
claim	O
.	O

A	O
large	O
number	O
of	O
teams	O
report	O
a	O
multi	O
-	O
step	O
approach	O
to	O
document	O
selection	O
.	O
The	O
majority	O
of	O
submissions	O
report	O
extracting	O
some	O
combination	O
of	O
Named	O
Entities	O
,	O
Noun	O
Phrases	O
and	O
Capitalized	O
Expressions	O
from	O
the	O
claim	O
.	O
These	O
were	O
used	O
either	O
as	O
inputs	O
to	O
a	O
search	O
API	O
(	O
i.e.	O
Wikipedia	O
Search	O
or	O
Google	B-DatasetName
Search	O
)	O
,	O
search	O
server	O
(	O
e.g.	O
Lucene	O
5	O
or	O
Solr	O
6	O
)	O
or	O
as	O
keywords	O
for	O
matching	O
against	O
Wikipedia	O
page	O
titles	O
or	O
article	O
bodies	O
.	O
BUPT	O
-	O
NLPer	O
report	O
using	O
S	O
-	O
MART	O
for	O
entity	B-TaskName
linking	I-TaskName
(	O
Yang	O
and	O
Chang	O
,	O
2015	O
)	O
and	O
the	O
highest	O
scor	O
-	O
ing	O
team	O
,	O
UNC	O
-	O
NLP	O
,	O
report	O
using	O
page	O
viewership	O
statistics	O
to	O
rank	O
the	O
candidate	O
pages	O
.	O
GESIS	O
Cologne	O
report	O
directly	O
selecting	O
sentences	O
using	O
the	O
Solr	O
search	O
,	O
bypassing	O
the	O
need	O
to	O
perform	O
document	O
retrieval	O
as	O
a	O
separate	O
step	O
.	O
The	O
team	O
which	O
scored	O
highest	O
on	O
evidence	O
precision	O
and	O
evidence	O
F1	B-MetricName
was	O
Papelo	O
(	O
precision	O
=	O
92.18	O
%	O
and	O
F	O
1	O
=	O
64.85	O
%	O
)	O
who	O
report	O
using	O
a	O
combination	O
of	O
TF	O
-	O
IDF	O
for	O
document	O
retrieval	O
and	O
string	O
matching	O
using	O
named	O
entities	O
and	O
capitalized	O
expressions	O
.	O
The	O
teams	O
which	O
scored	O
highest	O
on	O
evidence	O
recall	O
were	O
Athene	O
UKP	B-DatasetName
TU	O
Darmstadt	O
(	O
recall	O
=	O
85.19	O
%	O
)	O
and	O
UCL	O
Machine	O
Reading	O
Group	O
(	O
recall	O
=	O
82.84	O
%	O
)	O
7	O
8	O
Athene	O
report	O
extracting	O
nounphrases	O
from	O
the	O
claim	O
and	O
using	O
these	O
to	O
query	O
the	O
Wikipedia	O
search	O
API	O
.	O
A	O
similar	O
approach	O
was	O
used	O
by	O
Columbia	O
NLP	O
who	O
query	O
the	O
Wikipedia	O
search	O
API	O
using	O
named	O
entities	O
extracted	O
from	O
the	O
claim	O
as	O
a	O
query	O
string	O
,	O
all	O
the	O
text	O
before	O
the	O
first	O
lowercase	O
verb	O
phrase	O
as	O
a	O
query	O
string	O
and	O
also	O
combine	O
this	O
result	O
with	O
Wikipedia	O
pages	O
identified	O
with	O
Google	B-DatasetName
search	O
using	O
the	O
entire	O
claim	O
.	O
UCL	O
Machine	O
Reading	O
Group	O
report	O
a	O
document	O
retrieval	O
approach	O
that	O
identifies	O
Wikipedia	O
article	O
titles	O
within	O
the	O
claim	O
and	O
ranks	O
the	O
results	O
using	O
features	O
such	O
as	O
capitalization	O
,	O
sentence	O
position	O
and	O
token	O
match	O
.	O

Document	O
retrieval	O
We	O
applied	O
the	O
constituency	O
parser	O
from	O
AllenNLP	O
to	O
extract	O
noun	O
phrases	O
in	O
the	O
claim	O
and	O
made	O
use	O
of	O
Wikipedia	O
API	O
to	O
search	O
corresponding	O
pages	O
for	O
each	O
noun	O
phrase	O
.	O
So	O
as	O
to	O
remove	O
noisy	O
pages	O
from	O
the	O
results	O
,	O
we	O
have	O
stemmed	O
the	O
words	O
of	O
their	O
titles	O
and	O
the	O
claim	O
,	O
and	O
then	O
discarded	O
pages	O
whose	O
stemmed	O
words	O
of	O
the	O
title	O
are	O
not	O
completely	O
included	O
in	O
the	O
set	O
of	O
stemmed	O
words	O
in	O
the	O
claim	O
.	O
Sentence	O
selection	O
The	O
hinge	O
loss	B-MetricName
with	O
negative	O
sampling	O
is	O
applied	O
to	O
train	O
the	O
enhanced	O
LSTM	B-MethodName
.	O
For	O
a	O
given	O
positive	O
claim	O
-	O
evidence	O
pair	O
,	O
negative	O
samples	O
are	O
generated	O
by	O
randomly	O
sampling	O
sentences	O
from	O
the	O
retrieved	O
documents	O
.	O
RTE	B-DatasetName
We	O
combine	O
the	O
5	O
sentences	O
from	O
sentence	O
selection	O
and	O
the	O
claim	O
to	O
form	O
5	O
pairs	O
and	O
then	O
apply	O
enhanced	O
LSTM	B-MethodName
for	O
each	O
pair	O
.	O
We	O
combine	O
the	O
resulting	O
representations	O
using	O
average	O
and	O
max	B-MethodName
pooling	I-MethodName
and	O
feed	O
the	O
resulting	O
vector	O
through	O
an	O
MLP	B-DatasetName
for	O
classification	O
.	O

We	O
develop	O
a	O
system	O
for	O
the	O
FEVER	B-DatasetName
fact	O
extraction	O
and	O
verification	O
challenge	O
that	O
uses	O
a	O
high	O
precision	O
entailment	O
classifier	O
based	O
on	O
transformer	O
networks	O
pretrained	O
with	O
language	O
modeling	O
(	O
Radford	O
and	O
Salimans	O
,	O
2018	O
)	O
,	O
to	O
classify	O
a	O
broad	O
set	O
of	O
potential	O
evidence	O
.	O
The	O
precision	O
of	O
the	O
entailment	O
classifier	O
allows	O
us	O
to	O
enhance	O
recall	O
by	O
considering	O
every	O
statement	O
from	O
several	O
articles	O
to	O
decide	O
upon	O
each	O
claim	O
.	O
We	O
include	O
not	O
only	O
the	O
articles	O
best	O
matching	O
the	O
claim	O
text	O
by	O
TFIDF	O
score	O
,	O
but	O
read	O
additional	O
articles	O
whose	O
titles	O
match	O
named	O
entities	O
and	O
capitalized	O
expressions	O
occurring	O
in	O
the	O
claim	O
text	O
.	O
The	O
entailment	O
module	O
evaluates	O
potential	O
evidence	O
one	O
statement	O
at	O
a	O
time	O
,	O
together	O
with	O
the	O
title	O
of	O
the	O
page	O
the	O
evidence	O
came	O
from	O
(	O
providing	O
a	O
hint	O
about	O
possible	O
pronoun	O
antecedents	O
)	O
.	O
In	O
preliminary	O
evaluation	O
,	O
the	O
system	O
achieved	O
57.36	O
%	O
FEVER	B-DatasetName
score	O
,	O
61.08	O
%	O
label	O
accuracy	B-MetricName
,	O
and	O
64.85	O
%	O
evidence	O
F1	B-MetricName
on	O
the	O
FEVER	B-DatasetName
shared	O
task	O
test	O
set	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
the	O
system	O
we	O
designed	O
for	O
the	O
FEVER	B-DatasetName
2018	O
Shared	O
Task	O
.	O
The	O
aim	O
of	O
this	O
task	O
was	O
to	O
conceive	O
a	O
system	O
that	O
can	O
not	O
only	O
automatically	O
assess	O
the	O
veracity	O
of	O
a	O
claim	O
but	O
also	O
retrieve	O
evidence	O
supporting	O
this	O
assessment	O
from	O
Wikipedia	O
.	O
In	O
our	O
approach	O
,	O
the	O
Wikipedia	O
documents	O
whose	O
Term	O
Frequency	O
-	O
Inverse	O
Document	O
Frequency	O
(	O
TFIDF	O
)	O
vectors	O
are	O
most	O
similar	O
to	O
the	O
vector	O
of	O
the	O
claim	O
and	O
those	O
documents	O
whose	O
names	O
are	O
similar	O
to	O
the	O
named	O
entities	O
(	O
NEs	O
)	O
mentioned	O
in	O
the	O
claim	O
are	O
identified	O
as	O
the	O
documents	O
which	O
might	O
contain	O
evidence	O
.	O
The	O
sentences	O
in	O
these	O
documents	O
are	O
then	O
supplied	O
to	O
a	O
decomposable	O
attention	O
-	O
based	O
textual	O
entailment	O
recognition	O
module	O
.	O
This	O
module	O
calculates	O
the	O
probability	O
of	O
each	O
sentence	O
supporting	O
the	O
claim	O
,	O
contradicting	O
the	O
claim	O
or	O
not	O
providing	O
any	O
relevant	O
information	O
.	O
Various	O
features	O
computed	O
using	O
these	O
probabilities	O
are	O
finally	O
used	O
by	O
a	O
Random	O
Forest	O
classifier	O
to	O
determine	O
the	O
overall	O
truthfulness	O
of	O
the	O
claim	O
.	O
The	O
sentences	O
which	O
support	O
this	O
classification	O
are	O
returned	O
as	O
evidence	O
.	O
Our	O
approach	O
achieved	O
a	O
42.77	O
%	O
evidence	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
,	O
a	O
51.36	O
%	O
label	O
accuracy	B-MetricName
and	O
a	O
38.33	O
%	O
FEVER	B-DatasetName
score	O
.	O

We	O
describe	O
the	O
UMBC	O
-	O
FEVER	B-DatasetName
system	O
that	O
we	O
used	O
in	O
the	O
2018	O
FEVER	B-DatasetName
shared	O
task	O
.	O
The	O
system	O
employed	O
a	O
frame	O
-	O
based	O
information	B-TaskName
retrieval	I-TaskName
approach	O
to	O
select	O
Wikipedia	O
sentences	O
providing	O
evidence	O
and	O
used	O
a	O
two	O
-	O
layer	O
multilayer	O
perceptron	O
(	O
MLP	B-DatasetName
)	O
for	O
classification	O
.	O
Our	O
submission	O
achieved	O
a	O
score	O
of	O
0.3695	O
on	O
the	O
Evidence	O
F1	B-MetricName
metric	O
for	O
retrieving	O
relevant	O
evidential	O
sentences	O
(	O
10	O
th	O
out	O
of	O
24	O
)	O
and	O
a	O
score	O
of	O
0.2376	O
on	O
the	O
FEVER	B-DatasetName
metric	O
(	O
just	O
below	O
the	O
baseline	O
system	O
)	O
.	O

Social	O
chatbots	O
,	O
also	O
known	O
as	O
chit	O
-	O
chat	O
chatbots	O
,	O
evolve	O
rapidly	O
with	O
large	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
.	O
Despite	O
the	O
huge	O
progress	O
,	O
privacy	O
concerns	O
have	O
arisen	O
recently	O
:	O
training	O
data	O
of	O
large	O
language	O
models	O
can	O
be	O
extracted	O
via	O
model	O
inversion	O
attacks	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
datasets	O
used	O
for	O
training	O
chatbots	O
contain	O
many	O
private	O
conversations	O
between	O
two	O
individuals	O
.	O
In	O
this	O
work	O
,	O
we	O
further	O
investigate	O
the	O
privacy	O
leakage	O
of	O
the	O
hidden	O
states	O
of	O
chatbots	O
trained	O
by	O
language	O
modeling	O
which	O
has	O
not	O
been	O
well	O
studied	O
yet	O
.	O
We	O
show	O
that	O
speakers	O
'	O
personas	O
can	O
be	O
inferred	O
through	O
a	O
simple	O
neural	O
network	O
with	O
high	O
accuracy	B-MetricName
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
effective	O
defense	O
objectives	O
to	O
protect	O
persona	O
leakage	O
from	O
hidden	O
states	O
.	O
We	O
conduct	O
extensive	O
experiments	O
to	O
demonstrate	O
that	O
our	O
proposed	O
defense	O
objectives	O
can	O
greatly	O
reduce	O
the	O
attack	O
accuracy	B-MetricName
from	O
37.6	O
%	O
to	O
0.5	O
%	O
.	O
Meanwhile	O
,	O
the	O
proposed	O
objectives	O
preserve	O
language	O
models	O
'	O
powerful	O
generation	O
ability	O
.	O

Social	O
chatbots	O
have	O
been	O
widely	O
used	O
to	O
benefit	O
many	O
applications	O
from	O
answering	O
factual	O
questions	O
to	O
showing	O
emotional	O
companionship	O
.	O
With	O
recent	O
progress	O
in	O
large	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
(	O
Radford	O
et	O
al	O
,	O
2019	O
;	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
,	O
some	O
attempts	O
(	O
Wolf	O
et	O
al	O
,	O
2019	O
;	O
Ham	O
et	O
al	O
,	O
2020	O
;	O
Shen	O
et	O
al	O
,	O
2021	O
;	O
Sevegnani	O
et	O
al	O
,	O
2021	O
;	O
Gu	O
et	O
al	O
,	O
2021b	O
)	O
are	O
made	O
to	O
build	O
chatbots	O
based	O
on	O
large	O
generative	O
language	O
models	O
(	O
LMs	O
)	O
.	O
To	O
train	O
such	O
LM	O
-	O
based	O
chatbots	O
,	O
private	O
conversations	O
are	O
collected	O
.	O
Unfortunately	O
,	O
large	O
language	O
models	O
tend	O
to	O
memorize	O
training	O
data	O
and	O
some	O
private	O
data	O
can	O
be	O
recovered	O
from	O
models	O
(	O
Pan	O
et	O
al	O
,	O
2020	O
;	O
Carlini	O
et	O
al	O
,	O
2021	O
)	O
.	O
Besides	O
such	O
memorization	O
problems	O
,	O
"	O
overlearning	O
"	O
on	O
simple	O
training	O
objectives	O
can	O
reveal	O
sensitive	O
attributes	O
indirectly	O
related	O
to	O
the	O
learning	O
task	O
(	O
Song	O
and	O
Shmatikov	O
,	O
2020	O
)	O
.	O
LM	O
-	O
based	O
social	O
chatbots	O
essentially	O
inherit	O
the	O
privacy	O
issues	O
of	O
general	O
LMs	O
and	O
the	O
overlearning	O
problem	O
.	O
For	O
example	O
,	O
as	O
Figure	O
1	O
shows	O
,	O
when	O
using	O
a	O
fine	O
-	O
tuned	O
GPT	B-MethodName
-	O
2	O
as	O
the	O
encoder	O
and	O
decoder	O
of	O
an	O
LM	O
-	O
based	O
social	O
chatbot	B-TaskName
,	O
if	O
the	O
learned	O
representation	O
of	O
each	O
utterance	O
can	O
be	O
obtained	O
by	O
an	O
adversary	O
,	O
then	O
the	O
adversary	O
can	O
build	O
a	O
classifier	O
to	O
predict	O
the	O
persona	O
information	O
based	O
on	O
the	O
representation	O
.	O
As	O
shown	O
by	O
the	O
example	O
,	O
for	O
five	O
out	O
of	O
14	O
utterances	O
,	O
the	O
attacker	O
can	O
successfully	O
predict	O
the	O
persona	O
,	O
which	O
can	O
be	O
harmful	O
if	O
the	O
users	O
(	O
speakers	O
of	O
the	O
utterances	O
)	O
do	O
not	O
prefer	O
to	O
reveal	O
the	O
persona	O
information	O
.	O
Thus	O
,	O
in	O
practice	O
,	O
when	O
deploying	O
such	O
kinds	O
of	O
chatbots	O
in	O
real	O
applications	O
,	O
we	O
should	O
first	O
make	O
sure	O
that	O
no	O
private	O
information	O
can	O
be	O
leaked	O
by	O
the	O
models	O
.	O
To	O
systematically	O
study	O
the	O
privacy	O
issues	O
in	O
LMbased	O
social	O
chatbots	O
,	O
there	O
are	O
several	O
challenges	O
.	O
First	O
,	O
there	O
is	O
no	O
existing	O
data	O
that	O
can	O
be	O
used	O
to	O
quantify	O
how	O
much	O
private	O
information	O
is	O
revealed	O
by	O
an	O
LM	O
.	O
Second	O
,	O
there	O
has	O
been	O
no	O
existing	O
work	O
showing	O
how	O
to	O
attack	O
utterance	O
-	O
level	O
representations	O
to	O
obtain	O
sensitive	O
information	O
.	O
Third	O
,	O
there	O
has	O
been	O
no	O
existing	O
LM	O
-	O
based	O
chatbot	B-TaskName
that	O
can	O
defend	O
against	O
persona	O
inference	O
attacks	O
,	O
and	O
no	O
study	O
shows	O
how	O
to	O
protect	O
both	O
known	O
and	O
unknown	O
persona	O
attributes	O
.	O
In	O
this	O
paper	O
,	O
to	O
address	O
the	O
above	O
challenges	O
,	O
we	O
use	O
the	O
fine	O
-	O
tuned	O
GPT	B-MethodName
-	O
2	O
as	O
our	O
chatbot	B-TaskName
.	O
We	O
first	O
collect	O
a	O
dataset	O
by	O
aligning	O
personas	O
with	O
corresponding	O
utterances	O
in	O
PersonaChat	O
dataset	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Then	O
we	O
show	O
that	O
"	O
overlearning	O
"	O
can	O
happen	O
for	O
LM	O
-	O
based	O
chatbots	O
to	O
reveal	O
personas	O
of	O
speakers	O
.	O
We	O
build	O
a	O
single	O
external	O
multi	O
-	O
layer	O
perception	O
(	O
MLP	B-DatasetName
)	O
attacker	O
model	O
to	O
perform	O
black	O
-	O
box	O
persona	O
inference	O
attacks	O
on	O
the	O
utterance	O
-	O
level	O
embeddings	O
.	O
With	O
no	O
access	O
to	O
parameters	O
of	O
the	O
chatbot	B-TaskName
,	O
the	O
attacker	O
model	O
can	O
infer	O
speakers	O
'	O
personas	O
with	O
37.59	O
%	O
accuracy	B-MetricName
over	O
4	O
,	O
332	O
personas	O
.	O
The	O
high	O
accuracy	B-MetricName
of	O
the	O
attacker	O
model	O
implies	O
that	O
the	O
utterance	O
-	O
level	O
embeddings	O
have	O
potential	O
vulnerabilities	O
to	O
reveal	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
Figure	O
1	O
:	O
Black	O
-	O
box	O
persona	O
inference	O
attacks	O
(	O
over	O
4	O
,	O
332	O
personas	O
)	O
on	O
a	O
dialog	O
.	O
Every	O
representation	O
of	O
the	O
utterance	O
,	O
which	O
is	O
based	O
on	O
the	O
last	O
hidden	O
state	O
of	O
GPT	B-MethodName
-	O
2	O
,	O
is	O
attacked	O
without	O
defense	O
(	O
column	O
of	O
"	O
Attacks	O
on	O
LM	O
"	O
)	O
and	O
with	O
defense	O
(	O
column	O
of	O
"	O
Attacks	O
on	O
the	O
defensed	O
LM	O
"	O
)	O
.	O
If	O
the	O
model	O
can	O
predict	O
the	O
persona	O
of	O
the	O
speaker	O
based	O
on	O
the	O
observed	O
representation	O
,	O
then	O
we	O
regard	O
it	O
as	O
a	O
successful	O
attack	O
;	O
otherwise	O
,	O
unsuccessful	O
.	O
In	O
practice	O
,	O
when	O
deploying	O
a	O
model	O
,	O
a	O
robust	O
model	O
which	O
will	O
reveal	O
nothing	O
of	O
the	O
encoded	O
utterances	O
is	O
expected	O
.	O
speakers	O
'	O
private	O
persona	O
attributes	O
.	O
Thus	O
,	O
it	O
is	O
necessary	O
to	O
improve	O
training	O
algorithms	O
to	O
address	O
such	O
overlearning	O
issues	O
.	O
Finally	O
,	O
we	O
apply	O
defense	O
learning	O
strategies	O
on	O
the	O
GPT	B-MethodName
-	O
2	O
to	O
prevent	O
such	O
black	O
-	O
box	O
attacks	O
.	O
We	O
combine	O
proposed	O
KL	O
divergence	O
loss	B-MetricName
(	O
KL	O
loss	B-MetricName
)	O
with	O
mutual	O
information	O
loss	B-MetricName
(	O
MI	O
loss	B-MetricName
)	O
(	O
Song	O
et	O
al	O
,	O
2019	O
)	O
as	O
additional	O
defense	O
objectives	O
to	O
train	O
the	O
GPT	B-MethodName
-	O
2	O
and	O
decrease	O
the	O
attacker	O
's	O
persona	O
inference	O
accuracy	B-MetricName
to	O
0.53	O
%	O
.	O
Our	O
contributions	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
1	O
1	O
)	O
:	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
disclose	O
and	O
analyze	O
the	O
persona	O
inference	B-TaskName
attack	I-TaskName
for	O
LM	O
-	O
based	O
chatbots	O
and	O
treat	O
it	O
as	O
a	O
privacy	O
risk	O
.	O
2	O
)	O
:	O
We	O
propose	O
an	O
effective	O
defensive	O
training	O
algorithm	O
to	O
prevent	O
dialog	O
representations	O
from	O
leaking	O
personas	O
of	O
the	O
corresponding	O
speakers	O
by	O
uniform	O
distribution	O
approximation	O
and	O
mutual	O
information	O
minimization	O
.	O
3	O
)	O
:	O
We	O
conduct	O
extensive	O
experiments	O
to	O
quantify	O
both	O
privacy	O
and	O
utility	O
of	O
proposed	O
defense	O
mechanisms	O
.	O
Besides	O
solving	O
the	O
persona	O
leakage	O
issue	O
,	O
the	O
proposed	O
training	O
algorithm	O
has	O
nearly	O
no	O
negative	O
influence	O
on	O
utility	O
.	O

We	O
assume	O
that	O
there	O
is	O
a	O
GPT	B-MethodName
-	O
2	O
based	O
chatbot	B-TaskName
f	O
pretrained	O
on	O
private	O
conversations	O
D.	O
Only	O
language	O
modeling	O
is	O
used	O
to	O
train	O
the	O
chatbot	B-TaskName
:	O
L	O
f	O
(	O
u	O
;	O
θ	B-HyperparameterName
f	O
)	O
=	O
−	O
|	O
u	O
|	O
i=1	O
log	O
(	O
Pr	O
(	O
wi	O
|	O
c	O
,	O
w0	O
,	O
w1	O
,	O
...	O
,	O
wi−1	O
)	O
)	O
,	O
(	O
1	O
)	O
where	O
f	O
refers	O
to	O
the	O
LM	O
-	O
based	O
chatbot	B-TaskName
with	O
given	O
utterance	O
u	O
=	O
{	O
w	O
0	B-DatasetName
,	O
w	O
1	O
,	O
...	O
,	O
w	O
|	O
u	O
|	O
−1	O
}	O
and	O
previous	O
context	O
c.	O
An	O
adversary	O
owns	O
one	O
external	O
annotated	O
dialog	O
dataset	O
D	O
a	O
=	O
{	O
(	O
U	O
1	O
,	O
s	O
1	O
)	O
,	O
(	O
U	O
2	O
,	O
s	O
2	O
)	O
,	O
...	O
,	O
(	O
U	O
n	O
,	O
s	O
n	O
)	O
}	O
with	O
n	O
conversations	O
where	O
U	O
i	O
indicates	O
a	O
list	O
of	O
utterances	O
{	O
u	O
i1	O
,	O
u	O
i2	O
,	O
...	O
,	O
u	O
in	O
i	O
}	O
of	O
i	O
-	O
th	O
conversation	O
and	O
s	O
i	O
corresponds	O
to	O
a	O
list	O
of	O
sensitive	O
personas	O
{	O
s	O
i1	O
,	O
s	O
i2	O
,	O
...	O
,	O
s	O
in	O
i	O
}	O
for	O
corresponding	O
utterance	O
.	O
Each	O
persona	O
s	O
kj	O
is	O
an	O
integer	O
that	O
can	O
be	O
mapped	O
to	O
its	O
persona	O
according	O
to	O
a	O
predefined	O
dictionary	O
and	O
0	B-DatasetName
≤	O
s	O
kj	O
≤	O
C	O
−	O
1	O
where	O
C	O
is	O
the	O
total	O
number	O
of	O
predefined	O
persona	O
attributes	O
.	O
The	O
goal	O
of	O
the	O
adversary	O
is	O
to	O
infer	O
speakers	O
'	O
personas	O
s	O
from	O
utterances	O
'	O
embeddings	O
f	O
(	O
u	O
)	O
where	O
u	O
and	O
s	O
refer	O
to	O
any	O
utterance	O
and	O
its	O
persona	O
label	O
.	O

The	O
persona	O
inference	B-TaskName
attack	I-TaskName
can	O
be	O
viewed	O
as	O
a	O
supervised	O
classification	O
task	O
.	O
For	O
the	O
black	O
-	O
box	O
attack	O
setup	O
,	O
the	O
adversary	O
can	O
only	O
query	O
the	O
target	O
dialog	O
model	O
f	O
with	O
access	O
to	O
embeddings	O
of	O
adversary	O
's	O
inputs	O
and	O
can	O
not	O
access	O
or	O
modify	O
model	O
parameters	O
θ	B-HyperparameterName
f	O
.	O
As	O
shown	O
in	O
the	O
left	O
part	O
of	O
Figure	O
2	O
,	O
the	O
adversary	O
tries	O
to	O
build	O
its	O
attacker	O
model	O
A	O
with	O
its	O
external	O
data	O
D	O
a	O
and	O
dialog	O
model	O
f	O
.	O
The	O
persona	O
predictor	O
's	O
output	O
A	O
(	O
f	O
(	O
u	O
)	O
)	O
is	O
the	O
estimated	O
probability	O
distribution	O
over	O
C	O
persona	O
attributes	O
.	O
Its	O
loss	B-MetricName
function	O
L	O
A	O
exploits	O
cross	O
-	O
entropy	O
between	O
the	O
predicted	O
distribution	O
and	O
ground	O
truth	O
distribution	O
that	O
can	O
be	O
formulated	O
as	O
:	O
LA	O
(	O
u	O
kj	O
,	O
s	O
kj	O
;	O
θA	O
)	O
=	O
CE	O
(	O
A	O
(	O
f	O
(	O
u	O
kj	O
)	O
)	O
,	O
s	O
kj	O
)	O
,	O
(	O
2	O
)	O
where	O
CE	O
refers	O
to	O
cross	O
-	O
entropy	O
loss	B-MetricName
between	O
persona	O
label	O
s	O
kj	O
and	O
A	O
(	O
f	O
(	O
u	O
kj	O
)	O
)	O
.	O
A	O
well	O
-	O
performed	O
persona	O
predictor	O
A	O
can	O
cause	O
great	O
privacy	O
threats	O
.	O
For	O
machine	O
learning	O
as	O
a	O
service	O
(	O
MLaaS	O
)	O
,	O
A	O
can	O
be	O
applied	O
to	O
perform	O
a	O
man	O
-	O
in	O
-	O
the	O
-	O
middle	O
attack	O
on	O
the	O
application	O
programming	O
interfaces	O
.	O
Moreover	O
,	O
even	O
if	O
the	O
raw	O
data	O
are	O
protected	O
and	O
the	O
transmission	O
channel	O
is	O
secure	O
,	O
a	O
curious	O
service	O
provider	O
can	O
train	O
its	O
attacker	O
A	O
to	O
collect	O
personas	O
of	O
service	O
users	O
.	O

The	O
LM	O
training	O
objective	O
in	O
Equation	O
1	O
only	O
considers	O
the	O
utility	O
of	O
chatbots	O
.	O
In	O
later	O
experiment	O
sections	O
,	O
we	O
show	O
that	O
LM	O
brings	O
severe	O
overlearning	O
issues	O
.	O
Ideally	O
,	O
to	O
achieve	O
an	O
optimal	O
privacypreserving	O
chatbot	B-TaskName
against	O
persona	O
inference	O
attacks	O
,	O
the	O
probability	O
distribution	O
of	O
the	O
attacker	O
model	O
A	O
should	O
be	O
close	O
to	O
the	O
uniform	O
distribution	O
.	O
That	O
is	O
,	O
the	O
adversary	O
can	O
not	O
improve	O
its	O
inference	O
accuracy	B-MetricName
from	O
posterior	O
estimation	O
A	O
(	O
f	O
(	O
u	O
)	O
)	O
and	O
the	O
accuracy	B-MetricName
is	O
no	O
better	O
than	O
making	O
random	O
guesses	O
on	O
the	O
persona	O
attributes	O
.	O
Moreover	O
,	O
the	O
constraints	O
on	O
privacy	O
should	O
have	O
minor	O
degradation	O
on	O
the	O
utility	O
to	O
maintain	O
the	O
strong	O
generation	O
ability	O
of	O
chatbots	O
.	O
Following	O
the	O
intuition	O
that	O
the	O
adversary	O
can	O
not	O
obtain	O
better	O
results	O
than	O
a	O
random	O
guess	O
,	O
in	O
Section	O
4.1	O
,	O
we	O
propose	O
KL	O
loss	B-MetricName
that	O
aims	O
to	O
flatten	O
the	O
persona	O
predictor	O
's	O
estimated	O
distribution	O
.	O
Based	O
on	O
minimizing	O
the	O
mutual	O
information	O
between	O
hidden	O
states	O
f	O
(	O
u	O
)	O
of	O
chatbots	O
and	O
private	O
persona	O
attributes	O
s	O
,	O
we	O
propose	O
MI	O
loss	B-MetricName
in	O
Section	O
4.2	O
.	O
Lastly	O
,	O
we	O
show	O
the	O
overall	O
training	O
objective	O
in	O
Section	O
4.3	O
.	O

KL	O
loss	B-MetricName
aims	O
to	O
minimize	O
the	O
Kullback	O
-	O
Leibler	O
divergence	O
between	O
A	O
(	O
f	O
(	O
u	O
)	O
)	O
and	O
the	O
uniform	O
distribution	O
.	O
It	O
flattens	O
the	O
distribution	O
of	O
A	O
(	O
f	O
(	O
u	O
)	O
)	O
so	O
that	O
the	O
adversary	O
can	O
not	O
gain	O
any	O
useful	O
knowledge	O
after	O
training	O
attacker	O
model	O
A.	O
The	O
KL	O
divergence	O
between	O
the	O
uniform	O
distribution	O
and	O
A	O
(	O
f	O
(	O
u	O
)	O
)	O
can	O
be	O
formulated	O
as	O
:	O
DKL	O
(	O
UNI	O
|	O
|	O
A	O
(	O
f	O
(	O
u	O
)	O
)	O
)	O
=	O
−	O
1	O
C	O
C−1	O
k=0	O
log	O
(	O
CPr	O
(	O
k	O
|	O
f	O
(	O
u	O
)	O
,	O
θA	O
)	O
)	O
,	O
(	O
3	O
)	O
where	O
UNI	O
indicates	O
the	O
uniform	O
distribution	O
and	O
k	O
indicates	O
the	O
k	O
-	O
th	O
persona	O
label	O
of	O
C	O
labels	O
.	O
For	O
optimization	O
,	O
we	O
can	O
leave	O
out	O
constant	O
terms	O
and	O
the	O
logarithm	O
(	O
Mireshghallah	O
et	O
al	O
,	O
2021	O
)	O
⃝	O
and	O
the	O
attacking	O
stage	O
is	O
marked	O
by	O
2	O
⃝.	O
Both	O
language	O
modeling	O
and	O
defender	O
objectives	O
are	O
jointly	O
trained	O
for	O
the	O
defense	O
to	O
optimize	O
the	O
GPT	B-MethodName
-	O
2	O
model	O
.	O
After	O
GPT	B-MethodName
-	O
2	O
's	O
training	O
stage	O
1	O
⃝	O
is	O
finished	O
,	O
parameters	O
of	O
GPT	B-MethodName
-	O
2	O
are	O
all	O
frozen	O
and	O
then	O
the	O
attacking	O
stage	O
2	O
⃝	O
starts	O
.	O
The	O
defender	O
shares	O
the	O
same	O
architecture	O
as	O
the	O
attacker	O
and	O
uses	O
L	O
kl	O
with	O
L	O
mi	O
as	O
defense	O
objectives	O
.	O
the	O
following	O
loss	B-MetricName
function	O
:	O
LD	O
(	O
u	O
;	O
θA	O
)	O
=	O
−	O
1	O
C	O
C−1	O
k=0	O
Pr	O
(	O
k	O
|	O
f	O
(	O
u	O
)	O
,	O
θA	O
)	O
.	O
However	O
,	O
from	O
the	O
perspective	O
of	O
defenders	O
,	O
they	O
have	O
no	O
access	O
to	O
attacker	O
model	O
A	O
and	O
its	O
parameters	O
.	O
Instead	O
,	O
they	O
can	O
build	O
their	O
own	O
persona	O
predictor	O
as	O
a	O
fake	O
attacker	O
.	O
More	O
specifically	O
,	O
they	O
may	O
mimic	O
the	O
adversary	O
to	O
annotate	O
a	O
dataset	O
D	O
′	O
a	O
and	O
a	O
persona	O
predictor	O
A	O
p	O
.	O
Then	O
the	O
KL	O
loss	B-MetricName
becomes	O
:	O
L	O
kl	O
(	O
u	O
;	O
θA	O
p	O
,	O
θ	B-HyperparameterName
f	O
)	O
=	O
−	O
1	O
C	O
C−1	O
k=0	O
Pr	O
(	O
k	O
|	O
f	O
(	O
u	O
)	O
,	O
θA	O
p	O
)	O
,	O
(	O
5	O
)	O
where	O
parameters	O
of	O
the	O
chatbot	B-TaskName
θ	B-HyperparameterName
f	O
and	O
the	O
fake	O
attacker	O
θ	B-HyperparameterName
Ap	O
are	O
updated	O
via	O
KL	O
loss	B-MetricName
.	O
The	O
intuition	O
is	O
to	O
train	O
the	O
chatbot	B-TaskName
together	O
with	O
a	O
fake	O
attacker	O
to	O
prevent	O
model	O
overlearning	O
by	O
flattening	O
the	O
attacker	O
model	O
's	O
distribution	O
.	O

The	O
privacy	O
constraint	O
requires	O
that	O
hidden	O
representations	O
should	O
not	O
reveal	O
the	O
persona	O
attributes	O
.	O
In	O
other	O
words	O
,	O
given	O
any	O
utterance	O
u	O
and	O
persona	O
s	O
behind	O
the	O
utterance	O
u	O
,	O
we	O
want	O
to	O
minimize	O
the	O
mutual	O
information	O
between	O
f	O
(	O
u	O
)	O
and	O
s	O
:	O
min	O
θ	B-HyperparameterName
f	O
I	O
(	O
f	O
(	O
u	O
)	O
;	O
s	O
)	O
.	O
(	O
6	O
)	O
Following	O
the	O
derivation	O
in	O
Song	O
et	O
al	O
(	O
2019	O
)	O
and	O
,	O
the	O
upper	O
bound	O
can	O
be	O
formulated	O
as	O
:	O
I	O
(	O
f	O
(	O
u	O
)	O
;	O
s	O
)	O
≤	O
E	O
q	O
(	O
f	O
(	O
u	O
)	O
)	O
DKL	O
(	O
q	O
(	O
s	O
|	O
f	O
(	O
u	O
)	O
)	O
|	O
|	O
p	O
(	O
s	O
)	O
)	O
,	O
(	O
7	O
)	O
where	O
p	O
(	O
s	O
)	O
can	O
be	O
any	O
distribution	O
for	O
s	O
,	O
q	O
(	O
x	O
)	O
refers	O
to	O
probability	O
distribution	O
of	O
model	O
f	O
parameterized	O
by	O
θ	B-HyperparameterName
f	O
and	O
f	O
(	O
u	O
)	O
is	O
assumed	O
to	O
be	O
sampled	O
from	O
the	O
conditional	O
distribution	O
q	O
(	O
f	O
(	O
u	O
)	O
|	O
x	O
,	O
s	O
)	O
.	O
However	O
,	O
q	O
(	O
s	O
|	O
f	O
(	O
u	O
)	O
)	O
is	O
hard	O
to	O
estimate	O
.	O
Instead	O
,	O
we	O
use	O
p	O
Ψ	O
(	O
s	O
|	O
f	O
(	O
u	O
)	O
)	O
to	O
approximate	O
q	O
(	O
s	O
|	O
f	O
(	O
u	O
)	O
)	O
via	O
minimizing	O
their	O
KL	O
divergence	O
and	O
then	O
we	O
can	O
obtain	O
the	O
following	O
lower	O
bound	O
(	O
Song	O
et	O
al	O
,	O
2019	O
)	O
:	O
E	O
q	O
(	O
f	O
(	O
u	O
)	O
)	O
DKL	O
(	O
q	O
(	O
s	O
|	O
f	O
(	O
u	O
)	O
)	O
|	O
|	O
p	O
(	O
s	O
)	O
)	O
≥	O
E	O
q	O
(	O
f	O
(	O
u	O
)	O
)	O
[	O
log	O
pΨ	O
(	O
s	O
|	O
f	O
(	O
u	O
)	O
)	O
−	O
log	O
p	O
(	O
s	O
)	O
]	O
.	O
(	O
8	O
)	O
Therefore	O
,	O
our	O
objective	O
in	O
Equation	O
6	O
can	O
be	O
formulated	O
as	O
an	O
adversarial	O
training	O
objective	O
:	O
min	O
θ	B-HyperparameterName
f	O
max	O
Ψ	O
E	O
q	O
(	O
f	O
(	O
u	O
)	O
)	O
[	O
log	O
pΨ	O
(	O
s	O
|	O
f	O
(	O
u	O
)	O
)	O
−	O
log	O
p	O
(	O
s	O
)	O
]	O
.	O
(	O
9	O
)	O
log	O
p	O
(	O
s	O
)	O
is	O
independent	O
of	O
f	O
(	O
u	O
)	O
,	O
and	O
we	O
may	O
leave	O
this	O
term	O
out	O
in	O
Equation	O
9	O
:	O
min	O
θ	B-HyperparameterName
f	O
max	O
Ψ	O
E	O
q	O
(	O
f	O
(	O
u	O
)	O
)	O
[	O
log	O
pΨ	O
(	O
s	O
|	O
f	O
(	O
u	O
)	O
)	O
]	O
.	O
(	O
10	O
)	O
Then	O
,	O
Equation	O
10	O
illustrates	O
an	O
adversarial	O
game	O
between	O
an	O
adversary	O
p	O
Ψ	O
who	O
manages	O
to	O
infer	O
s	O
from	O
f	O
(	O
u	O
)	O
and	O
a	O
defender	O
who	O
modifies	O
θ	B-HyperparameterName
f	O
to	O
protect	O
s	O
from	O
persona	O
inference	B-TaskName
attack	I-TaskName
.	O
Adversarial	O
training	O
is	O
widely	O
used	O
to	O
protect	O
sensitive	O
features	O
in	O
natural	O
language	O
processing	O
(	O
Elazar	O
and	O
Goldberg	O
,	O
2018	O
;	O
Coavoux	O
et	O
al	O
,	O
2018	O
;	O
Li	O
et	O
al	O
,	O
2018	O
)	O
.	O
Using	O
the	O
persona	O
predictor	O
model	O
A	O
p	O
with	O
softmax	B-MethodName
activation	O
to	O
learn	O
p	O
Ψ	O
,	O
we	O
obtain	O
the	O
final	O
objective	O
for	O
the	O
defender	O
:	O
min	O
θ	B-HyperparameterName
Ap	O
max	O
θ	B-HyperparameterName
f	O
CE	O
(	O
Ap	O
(	O
f	O
(	O
u	O
)	O
)	O
,	O
s	O
)	O
.	O
(	O
11	O
)	O
We	O
can	O
rewrite	O
Equation	O
11	O
into	O
two	O
losses	O
:	O
L	O
mi1	O
(	O
u	O
kj	O
,	O
s	O
kj	O
;	O
θ	B-HyperparameterName
Ap	O
)	O
=	O
CE	O
(	O
A	O
p	O
(	O
f	O
(	O
u	O
kj	O
)	O
)	O
,	O
s	O
kj	O
)	O
and	O
L	O
mi2	O
(	O
u	O
kj	O
,	O
s	O
kj	O
;	O
θ	B-HyperparameterName
f	O
)	O
=	O
−CE	O
(	O
A	O
p	O
(	O
f	O
(	O
u	O
kj	O
)	O
)	O
,	O
s	O
kj	O
)	O
for	O
the	O
fake	O
adversary	O
and	O
the	O
chatbot	B-TaskName
respectively	O
.	O
Then	O
our	O
MI	O
loss	B-MetricName
can	O
be	O
formulated	O
as	O
:	O
Lmi	O
=	O
λ0Lmi1	O
+	O
Lmi2	O
,	O
(	O
12	O
)	O
where	O
λ	O
0	B-DatasetName
controls	O
the	O
ratio	O
between	O
two	O
the	O
fake	O
attacker	O
A	O
p	O
and	O
the	O
defensed	O
chatbot	B-TaskName
f	O
.	O

The	O
right	O
part	O
of	O
Figure	O
2	O
illustrates	O
how	O
the	O
chatbot	B-TaskName
is	O
trained	O
to	O
address	O
the	O
black	O
-	O
box	O
attack	O
.	O
The	O
loss	B-MetricName
function	O
for	O
the	O
defender	O
combines	O
KL	O
loss	B-MetricName
,	O
MI	O
loss	B-MetricName
and	O
LM	O
loss	B-MetricName
.	O
Notice	O
that	O
the	O
fake	O
adversary	O
objective	O
in	O
MI	O
loss	B-MetricName
violates	O
KL	O
loss	B-MetricName
which	O
tries	O
to	O
make	O
the	O
distribution	O
of	O
A	O
p	O
flatten	O
.	O
Our	O
proposed	O
loss	B-MetricName
assigns	O
more	O
weights	O
to	O
the	O
KL	O
loss	B-MetricName
:	O
L	O
=	O
L	O
f	O
+	O
λ1L	O
kl	O
+	O
λ2Lmi	O
,	O
(	O
13	O
)	O
where	O
λ	O
1	O
and	O
λ	O
2	O
are	O
hyper	O
-	O
parameters	O
and	O
λ	O
1	O
≥	O
10λ	O
2	O
to	O
flatten	O
the	O
distribution	O
of	O
A	O
p	O
.	O
Though	O
the	O
chatbot	B-TaskName
trained	O
with	O
overall	O
loss	B-MetricName
L	O
still	O
can	O
not	O
interfere	O
training	O
process	O
of	O
A	O
during	O
black	O
-	O
box	O
attacks	O
,	O
L	O
aims	O
to	O
mitigate	O
persona	O
overlearning	O
issues	O
of	O
f	O
to	O
address	O
such	O
persona	O
inference	O
attacks	O
.	O

Dataset	O
.	O
To	O
train	O
the	O
GPT	B-MethodName
-	O
2	O
as	O
our	O
chatbot	B-TaskName
,	O
we	O
use	O
the	O
DialoGPT	O
pretrained	O
on	O
Reddit	B-DatasetName
comment	O
chains	O
.	O
Then	O
we	O
use	O
PersonaChat	O
dataset	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
to	O
fine	O
-	O
tune	O
the	O
GPT	B-MethodName
-	O
2	O
.	O
To	O
obtain	O
annotated	O
dataset	O
D	O
a	O
for	O
the	O
adversary	O
,	O
we	O
align	O
personas	O
to	O
corresponding	O
utterances	O
through	O
positive	O
(	O
utterance	O
,	O
persona	O
)	O
pairs	O
provided	O
in	O
Dialogue	O
NLI	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
dataset	O
.	O
For	O
those	O
utterances	O
with	O
no	O
annotations	O
,	O
we	O
assign	O
label	O
−1	O
to	O
them	O
.	O
We	O
reshuffle	O
the	O
dataset	O
to	O
balance	O
the	O
label	O
distribution	O
among	O
train	O
/	O
val	O
/	O
test	O
datasets	O
with	O
the	O
ratio	O
of	O
8	O
:	O
1	O
:	O
1	O
.	O
We	O
first	O
let	O
the	O
attacker	O
and	O
defender	O
share	O
the	O
same	O
training	O
data	O
.	O
In	O
later	O
sections	O
,	O
we	O
will	O
separate	O
the	O
annotated	O
data	O
for	O
the	O
adversary	O
and	O
defender	O
with	O
no	O
overlap	O
.	O
A	O
summary	O
statistics	O
of	O
D	O
a	O
is	O
shown	O
in	O
Table	O
1	O
.	O
Attacker	O
model	O
.	O
In	O
our	O
experiment	O
,	O
we	O
use	O
a	O
2	O
-	O
layer	O
neural	O
network	O
with	O
cross	O
-	O
entropy	O
loss	B-MetricName
as	O
the	O
attacker	O
model	O
.	O
The	O
attacker	O
model	O
exploits	O
the	O
final	O
layer	O
embedding	O
of	O
the	O
last	O
token	O
"	O
<	O
|	O
endof	O
-	O
text	O
|	O
>	O
"	O
from	O
the	O
GPT	B-MethodName
-	O
2	O
as	O
model	O
input	O
.	O
We	O
also	O
try	O
other	O
attacker	O
model	O
architectures	O
(	O
transformer	O
block	O
based	O
attackers	O
)	O
and	O
input	O
embeddings	O
(	O
average	O
of	O
all	O
embeddings	O
in	O
the	O
final	O
layer	O
of	O
GPT	B-MethodName
-	O
2	O
)	O
,	O
but	O
the	O
attacking	O
performance	O
is	O
worse	O
than	O
the	O
2	O
-	O
layer	O
model	O
mentioned	O
above	O
.	O
Evaluation	O
Metrics	O
.	O
The	O
evaluation	O
metrics	O
are	O
based	O
on	O
privacy	O
and	O
utility	O
.	O
For	O
privacy	O
,	O
we	O
use	O
persona	O
inference	O
accuracy	B-MetricName
and	O
weighted	O
F1score	O
to	O
evaluate	O
the	O
attacker	O
's	O
performance	O
.	O
We	O
also	O
use	O
Bayesian	O
Privacy	O
(	O
BP	O
)	O
(	O
Gu	O
et	O
al	O
,	O
2021a	O
)	O
to	O
quantify	O
the	O
attacker	O
's	O
privacy	O
loss	B-MetricName
for	O
the	O
estimated	O
persona	O
distribution	O
.	O
Top	O
-	O
k	O
accuracy	B-MetricName
is	O
reported	O
in	O
the	O
Appendix	O
.	O
For	O
utility	O
,	O
we	O
apply	O
BERTScore	O
,	O
Distinct	O
(	O
Li	O
et	O
al	O
,	O
2016	O
)	O
,	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
and	O
perplexity	B-MetricName
(	O
PPL	O
)	O
as	O
evaluation	O
metrics	O
.	O
BERTScore	O
and	O
BLEU	B-MetricName
measure	O
similarity	O
between	O
generated	O
outputs	O
and	O
ground	O
truth	O
while	O
Distinct	O
(	O
Dist	O
)	O
focuses	O
on	O
diversity	O
.	O
Perplexity	B-MetricName
shows	O
the	O
uncertainty	O
when	O
the	O
LM	O
model	O
fits	O
the	O
data	O
.	O

Attacks	O
without	O
Defense	O
.	O
We	O
list	O
the	O
attacking	O
performance	O
of	O
A	O
in	O
multiple	O
scenarios	O
shown	O
in	O
Acc	B-MetricName
refers	O
to	O
test	O
persona	O
inference	O
accuracy	B-MetricName
.	O
F1	B-MetricName
uses	O
weighted	O
average	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
.	O
Max	O
-	O
Ratio	O
indicates	O
the	O
ratio	O
that	O
the	O
most	O
frequent	O
prediction	O
shares	O
among	O
all	O
predictions	O
.	O
The	O
worse	O
the	O
attack	O
model	O
performs	O
,	O
the	O
better	O
privacy	O
protection	O
can	O
be	O
achieved	O
.	O
distribution	O
,	O
then	O
it	O
can	O
randomly	O
guess	O
over	O
4	O
,	O
332	O
labels	O
(	O
Random	O
Pred	O
)	O
.	O
Otherwise	O
the	O
adversary	O
can	O
perform	O
Best	O
Guess	O
that	O
only	O
guesses	O
the	O
most	O
frequent	O
persona	O
in	O
the	O
dataset	O
.	O
LM	O
indicates	O
the	O
attacker	O
performance	O
that	O
only	O
language	O
modeling	O
objective	O
is	O
applied	O
to	O
train	O
the	O
chatbot	B-TaskName
without	O
any	O
defense	O
mechanism	O
.	O
From	O
the	O
table	O
,	O
the	O
test	O
persona	O
inference	O
accuracy	B-MetricName
on	O
the	O
LM	O
achieves	O
37.59	O
%	O
while	O
guessing	O
on	O
the	O
label	O
with	O
most	O
occurrences	O
merely	O
has	O
0.72	O
%	O
accuracy	B-MetricName
.	O
That	O
is	O
,	O
the	O
black	O
-	O
box	O
persona	O
inference	B-TaskName
attack	I-TaskName
has	O
52×	O
the	O
accuracy	B-MetricName
of	O
guessing	O
.	O
The	O
huge	O
performance	O
gap	O
between	O
the	O
attacker	O
model	O
and	O
the	O
baseline	O
guess	O
method	O
indicates	O
that	O
simple	O
language	O
modeling	O
objective	O
has	O
serious	O
overlearning	O
issues	O
that	O
unintentionally	O
capture	O
private	O
personas	O
of	O
speakers	O
.	O
Attacks	O
on	O
the	O
Defensed	O
LM	O
.	O
To	O
avoid	O
the	O
persona	O
overlearning	O
issue	O
,	O
we	O
use	O
additional	O
defense	O
objectives	O
illustrated	O
in	O
Section	O
4	O
.	O
LM+KL+MI	O
utilizes	O
language	O
modeling	O
,	O
KL	O
loss	B-MetricName
and	O
MI	O
loss	B-MetricName
in	O
Equation	O
13	O
to	O
train	O
the	O
GPT	B-MethodName
-	O
2	O
.	O
As	O
demonstrated	O
in	O
Table	O
2	O
,	O
the	O
attacker	O
performance	O
on	O
LM+KL+MI	O
significantly	O
reduces	O
the	O
attacking	O
accuracy	B-MetricName
from	O
37.59	O
%	O
to	O
0.53	O
%	O
and	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
drops	O
from	O
0.37	O
to	O
nearly	O
0	B-DatasetName
.	O
This	O
defense	O
mechanism	O
can	O
even	O
outperform	O
Best	O
Guess	O
in	O
terms	O
of	O
privacy	O
protection	O
.	O
That	O
is	O
,	O
even	O
if	O
the	O
adversary	O
annotates	O
its	O
own	O
dataset	O
to	O
train	O
an	O
attacker	O
model	O
,	O
the	O
attacking	O
performance	O
is	O
still	O
worse	O
than	O
simply	O
guessing	O
the	O
most	O
frequent	O
label	O
.	O
As	O
a	O
result	O
,	O
the	O
black	O
-	O
box	O
persona	O
prediction	O
attack	O
becomes	O
useless	O
after	O
applying	O
the	O
defenses	O
for	O
the	O
chatbot	B-TaskName
.	O
The	O
adversary	O
can	O
not	O
obtain	O
any	O
speaker	O
's	O
persona	O
from	O
the	O
embedding	O
f	O
(	O
u	O
)	O
by	O
training	O
A.	O
To	O
learn	O
why	O
the	O
proposed	O
defenses	O
work	O
so	O
well	O
,	O
we	O
further	O
examine	O
the	O
ratio	O
of	O
the	O
most	O
frequent	O
predicted	O
label	O
(	O
Max	O
-	O
Ratio	O
)	O
among	O
all	O
pre	O
-	O
dictions	O
.	O
The	O
accuracy	B-MetricName
of	O
Best	O
Guess	O
reveals	O
that	O
the	O
most	O
frequent	O
label	O
in	O
the	O
test	O
set	O
has	O
a	O
ratio	O
of	O
0.72	O
%	O
.	O
After	O
applying	O
KL	O
loss	B-MetricName
and	O
MI	O
loss	B-MetricName
,	O
the	O
attacker	O
model	O
tends	O
to	O
make	O
predictions	O
on	O
a	O
single	O
label	O
.	O
For	O
LM+KL+MI	O
,	O
the	O
Max	O
-	O
Ratio	O
even	O
occupies	O
81.87	O
%	O
predictions	O
.	O
This	O
implies	O
that	O
the	O
proposed	O
defense	O
strategies	O
may	O
have	O
the	O
potential	O
to	O
fool	O
the	O
attacker	O
model	O
to	O
make	O
wrong	O
predictions	O
on	O
a	O
single	O
slot	O
.	O
We	O
will	O
further	O
investigate	O
this	O
implication	O
in	O
later	O
sections	O
.	O
Overall	O
,	O
the	O
above	O
experiment	O
demonstrates	O
that	O
our	O
proposed	O
defense	O
learning	O
strategies	O
can	O
effectively	O
mitigate	O
the	O
persona	O
overlearning	O
issue	O
and	O
avoid	O
black	O
-	O
box	O
persona	O
inference	O
attacks	O
.	O

To	O
show	O
the	O
effectiveness	O
of	O
proposed	O
KL	O
loss	B-MetricName
and	O
MI	O
loss	B-MetricName
and	O
how	O
they	O
affect	O
the	O
performance	O
of	O
black	O
-	O
box	O
persona	O
inference	O
attacks	O
,	O
we	O
consider	O
the	O
inclusion	O
and	O
exclusion	O
of	O
proposed	O
defense	O
objectives	O
.	O
The	O
result	O
is	O
shown	O
in	O
Table	O
2	O
.	O
LM+KL	O
indicates	O
the	O
GPT	B-MethodName
-	O
2	O
is	O
trained	O
with	O
language	O
modeling	O
and	O
KL	O
loss	B-MetricName
.	O
LM+MI	O
applies	O
language	O
modeling	O
and	O
MI	O
loss	B-MetricName
.	O
From	O
the	O
table	O
,	O
it	O
can	O
be	O
seen	O
that	O
LM+KL	O
,	O
LM+MI	O
and	O
LM+KL+MI	O
are	O
all	O
able	O
to	O
reduce	O
the	O
test	O
accuracy	B-MetricName
of	O
the	O
attacks	O
.	O
The	O
KL	O
loss	B-MetricName
is	O
weaker	O
from	O
the	O
perspective	O
of	O
defense	O
,	O
but	O
it	O
tends	O
to	O
flatten	O
the	O
estimated	O
persona	O
distribution	O
with	O
much	O
smaller	O
Max	O
-	O
Ratio	O
.	O
The	O
LM+MI	O
shares	O
similar	O
test	O
accuracy	B-MetricName
and	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
with	O
LM+KL+MI	O
,	O
but	O
nearly	O
all	O
predictions	O
are	O
made	O
on	O
a	O
single	O
persona	O
label	O
with	O
a	O
ratio	O
of	O
99.84	O
%	O
.	O
This	O
suggests	O
that	O
MI	O
loss	B-MetricName
causes	O
the	O
attacker	O
model	O
to	O
predict	O
all	O
labels	O
on	O
a	O
single	O
persona	O
attribute	O
.	O
After	O
KL	O
loss	B-MetricName
is	O
applied	O
on	O
LM+KL+MI	O
,	O
the	O
Max	O
-	O
Ratio	O
drops	O
to	O
81.87	O
%	O
.	O
As	O
discussed	O
earlier	O
,	O
high	O
Max	O
-	O
Ratio	O
may	O
also	O
cause	O
privacy	O
leakage	O
.	O
Suppose	O
the	O
adversary	O
knows	O
the	O
persona	O
with	O
Max	O
-	O
Ratio	O
,	O
then	O
it	O
can	O
improve	O
its	O
guess	O
by	O
not	O
predicting	O
this	O
persona	O
,	O
which	O
is	O
a	O
threat	O
for	O
fewer	O
persona	O
labels	O
(	O
for	O
example	O
,	O
binary	O
classification	O
)	O
.	O
These	O
results	O
verify	O
that	O
KL	O
loss	B-MetricName
introduces	O
flatter	O
estimation	O
and	O
MI	O
loss	B-MetricName
is	O
more	O
effective	O
against	O
persona	O
overlearning	O
,	O
which	O
conforms	O
to	O
our	O
intuition	O
of	O
their	O
objectives	O
in	O
Section	O
4	O
.	O

Besides	O
privacy	O
,	O
utility	O
is	O
another	O
key	O
objective	O
to	O
train	O
a	O
chatbot	B-TaskName
.	O
Several	O
automatic	O
metrics	O
are	O
considered	O
to	O
evaluate	O
the	O
generation	O
performance	O
.	O
For	O
generation	O
,	O
we	O
use	O
Table	O
4	O
:	O
Evaluation	O
on	O
the	O
privacy	O
for	O
8	O
clusters	O
.	O
Unseen	O
shows	O
the	O
results	O
only	O
for	O
the	O
first	O
3	O
persona	O
labels	O
that	O
defender	O
has	O
never	O
seen	O
.	O
Overall	O
refers	O
to	O
the	O
results	O
on	O
all	O
8	O
labels	O
.	O
Acc	B-MetricName
and	O
Max	O
-	O
Ratio	O
are	O
measured	O
in	O
%	O
.	O
BP	O
u	O
corresponds	O
to	O
Bayesian	O
Privacy	O
loss	B-MetricName
on	O
the	O
uniform	O
distribution	O
.	O
Still	O
,	O
the	O
worse	O
the	O
attack	O
model	O
performs	O
,	O
the	O
better	O
privacy	O
protection	O
can	O
be	O
achieved	O
.	O
the	O
second	O
speaker	O
(	O
Human	O
B	O
in	O
Figure	O
1	O
)	O
with	O
all	O
previous	O
turns	O
as	O
context	O
.	O
Then	O
we	O
compared	O
the	O
generated	O
model	O
outputs	O
with	O
ground	O
truth	O
replies	O
.	O
We	O
use	O
Dist	O
-	O
1	O
and	O
Dist	O
-	O
2	O
to	O
count	O
ratios	O
of	O
distinct	O
unigrams	O
and	O
bigrams	O
.	O
BLEU	B-MetricName
-	O
1	O
,	O
BLEU	B-MetricName
-	O
2	O
and	O
BLEU	B-MetricName
-	O
4	O
are	O
applied	O
to	O
evaluate	O
generation	O
similarity	O
with	O
ground	O
truth	O
.	O
Due	O
to	O
the	O
one	O
-	O
to	O
-	O
many	O
nature	O
of	O
chit	O
-	O
chats	O
,	O
the	O
BLEU	B-MetricName
is	O
not	O
adequate	O
to	O
compare	O
generated	O
responses	O
with	O
ground	O
truth	O
.	O
Hence	O
,	O
we	O
adapt	O
Precision	B-MetricName
,	O
Recall	B-MetricName
and	O
Precision	B-MetricName
of	O
BERTScore	O
to	O
measure	O
the	O
similarity	O
in	O
the	O
embedding	O
space	O
.	O
The	O
evaluation	O
result	O
is	O
shown	O
in	O
Table	O
3	O
,	O
where	O
same	O
models	O
from	O
Table	O
2	O
are	O
evaluated	O
.	O
The	O
result	O
indicates	O
that	O
adding	O
KL	O
loss	B-MetricName
will	O
increase	O
the	O
perplexity	B-MetricName
greatly	O
from	O
14.8	O
to	O
28.9	O
.	O
After	O
combining	O
KL	O
loss	B-MetricName
with	O
MI	O
loss	B-MetricName
,	O
its	O
perplexity	B-MetricName
decreases	O
to	O
19.674	O
.	O
A	O
plausible	O
explanation	O
is	O
that	O
KL	O
loss	B-MetricName
confuses	O
the	O
persona	O
predictor	O
and	O
indirectly	O
increases	O
the	O
uncertainty	O
of	O
the	O
GPT	B-MethodName
-	O
2	O
.	O
All	O
GPT	B-MethodName
-	O
2	O
models	O
have	O
relatively	O
low	O
BLEU	B-MetricName
scores	O
due	O
to	O
the	O
one	O
-	O
to	O
-	O
many	O
mapping	O
between	O
contexts	O
and	O
responses	O
.	O
For	O
Distinct	O
and	O
BERTScore	O
,	O
there	O
are	O
only	O
minor	O
differences	O
between	O
LM	O
and	O
defensed	O
LMs	O
.	O
Though	O
the	O
uncertainty	O
increases	O
after	O
applying	O
KL	O
loss	B-MetricName
and	O
MI	O
loss	B-MetricName
,	O
it	O
does	O
no	O
harm	O
to	O
the	O
quality	O
of	O
generation	O
.	O
In	O
summary	O
,	O
there	O
is	O
almost	O
no	O
negative	O
influence	O
on	O
the	O
utility	O
after	O
applying	O
the	O
proposed	O
defense	O
strategies	O
.	O

Attacks	O
on	O
Imbalanced	O
Data	O
Distribution	O
.	O
Previous	O
black	O
-	O
box	O
attacks	O
usually	O
assume	O
that	O
the	O
annotated	O
dataset	O
D	O
a	O
must	O
share	O
similar	O
data	O
distri	O
-	O
bution	O
with	O
the	O
defender	O
's	O
training	O
data	O
.	O
To	O
examine	O
the	O
performance	O
of	O
defense	O
strategies	O
on	O
unseen	O
personas	O
,	O
we	O
assign	O
the	O
adversary	O
's	O
dataset	O
D	O
a	O
with	O
labels	O
that	O
the	O
defender	O
can	O
not	O
acquire	O
.	O
We	O
split	O
data	O
with	O
500	O
persona	O
labels	O
that	O
are	O
uniquely	O
held	O
by	O
the	O
adversary	O
.	O
The	O
defender	O
owns	O
8	O
,	O
031	O
conversations	O
with	O
persona	O
labels	O
ranging	O
from	O
500	O
to	O
4	O
,	O
331	O
while	O
the	O
adversary	O
holds	O
2	O
,	O
376	O
dialogues	O
with	O
persona	O
labels	O
ranging	O
from	O
0	B-DatasetName
to	O
4	O
,	O
331	O
.	O
For	O
testing	O
,	O
500	O
conversations	O
with	O
persona	O
labels	O
ranging	O
from	O
0	B-DatasetName
to	O
4	O
,	O
331	O
are	O
used	O
.	O
Under	O
imbalanced	O
data	O
distribution	O
,	O
the	O
attack	O
on	O
the	O
defensed	O
LM	O
has	O
Acc	B-MetricName
0.47	O
%	O
,	O
F1	B-MetricName
1.90e	O
-	O
3	O
and	O
Max	O
-	O
Ratio	O
94.06	O
%	O
.	O
The	O
persona	O
inference	O
accuracy	B-MetricName
is	O
still	O
very	O
low	O
and	O
the	O
attacker	O
model	O
tends	O
to	O
predict	O
more	O
on	O
a	O
single	O
persona	O
label	O
than	O
the	O
balanced	O
data	O
distribution	O
setup	O
.	O
This	O
result	O
shows	O
that	O
the	O
proposed	O
overall	O
loss	B-MetricName
can	O
also	O
prevent	O
black	O
-	O
box	O
persona	O
inference	O
attacks	O
on	O
unseen	O
personas	O
.	O
It	O
also	O
verifies	O
previous	O
suggestions	O
that	O
combining	O
LM	O
loss	B-MetricName
with	O
MI	O
loss	B-MetricName
may	O
fool	O
the	O
attacker	O
model	O
to	O
make	O
wrong	O
predictions	O
.	O
Attacks	O
on	O
Fewer	O
Persona	O
Labels	O
.	O
The	O
above	O
experiments	O
are	O
based	O
on	O
4	O
,	O
332	O
persona	O
labels	O
.	O
In	O
fact	O
,	O
many	O
personas	O
share	O
similar	O
meanings	O
and	O
can	O
be	O
further	O
clustered	O
.	O
Besides	O
,	O
to	O
better	O
evaluate	O
privacy	O
loss	B-MetricName
for	O
the	O
estimated	O
distribution	O
,	O
a	O
smaller	O
label	O
space	O
is	O
preferred	O
.	O
Therefore	O
,	O
it	O
is	O
necessary	O
to	O
consider	O
defense	O
performance	O
on	O
a	O
smaller	O
label	O
space	O
.	O
We	O
use	O
Sentence	O
-	O
BERT	B-MethodName
(	O
Reimers	O
and	O
Gurevych	O
,	O
2020	O
)	O
to	O
embed	O
all	O
persona	O
sentences	O
and	O
perform	O
k	B-MethodName
-	I-MethodName
means	I-MethodName
clustering	I-MethodName
on	O
the	O
embeddings	O
to	O
obtain	O
8	O
clusters	O
.	O
We	O
manually	O
checked	O
these	O
clusters	O
and	O
classified	O
them	O
as	O
cars	O
,	O
food	O
,	O
animals	O
For	O
both	O
conversations	O
,	O
the	O
"	O
context	O
"	O
is	O
fixed	O
and	O
used	O
as	O
the	O
first	O
four	O
utterances	O
.	O
Then	O
the	O
bot	O
and	O
the	O
user	O
start	O
interactive	O
conversations	O
with	O
the	O
"	O
context	O
"	O
.	O
Since	O
there	O
is	O
no	O
gold	O
standard	O
,	O
the	O
results	O
are	O
annotated	O
by	O
the	O
authors	O
.	O
(	O
pets	O
)	O
,	O
family	O
information	O
,	O
hobbies	O
,	O
jobs	O
,	O
personal	O
information	O
and	O
music	O
tastes	O
respectively	O
.	O
To	O
evaluate	O
how	O
the	O
clustering	O
performs	O
,	O
we	O
randomly	O
sample	O
100	O
utterances	O
with	O
clustered	O
labels	O
and	O
invite	O
two	O
volunteers	O
to	O
inspect	O
those	O
samples	O
.	O
Both	O
of	O
them	O
agree	O
on	O
90	O
%	O
of	O
the	O
clustered	O
annotations	O
.	O
After	O
manual	O
inspection	O
of	O
the	O
remaining	O
10	O
%	O
annotations	O
,	O
the	O
clustering	O
error	O
rate	O
is	O
8	O
%	O
.	O
Following	O
previous	O
imbalanced	O
data	O
split	O
,	O
we	O
assign	O
data	O
in	O
the	O
first	O
3	O
clusters	O
only	O
to	O
the	O
adversary	O
to	O
make	O
the	O
data	O
distribution	O
imbalanced	O
.	O
Here	O
,	O
the	O
defender	O
owns	O
6	O
,	O
654	O
conversations	O
with	O
persona	O
labels	O
ranging	O
from	O
3	O
to	O
7	O
while	O
the	O
adversary	O
holds	O
3	O
,	O
753	O
dialogues	O
with	O
persona	O
labels	O
ranging	O
from	O
0	B-DatasetName
to	O
7	O
.	O
For	O
testing	O
,	O
500	O
conversations	O
with	O
persona	O
labels	O
ranging	O
from	O
0	B-DatasetName
to	O
7	O
are	O
used	O
.	O
The	O
attacking	O
performance	O
for	O
both	O
unseen	O
labels	O
and	O
all	O
labels	O
is	O
displayed	O
in	O
Table	O
4	O
.	O
BP	O
u	O
measures	O
the	O
KL	O
divergence	O
D	O
KL	O
(	O
F	O
0	B-DatasetName
|	O
|	O
A	O
(	O
f	O
(	O
u	O
)	O
)	O
)	O
where	O
F	O
0	B-DatasetName
refers	O
to	O
uniform	O
distribution	O
.	O
For	O
imbalanced	O
data	O
distribution	O
with	O
a	O
small	O
label	O
space	O
,	O
our	O
proposed	O
defenses	O
can	O
still	O
achieve	O
much	O
lower	O
attack	O
accuracy	B-MetricName
than	O
LM	O
on	O
both	O
Unseen	O
and	O
Overall	O
.	O
However	O
,	O
for	O
Overall	O
,	O
LM+KL+MI	O
has	O
higher	O
accuracy	B-MetricName
with	O
a	O
lower	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
compared	O
with	O
two	O
baselines	O
.	O
This	O
indicates	O
that	O
proposed	O
defenses	O
fail	O
to	O
protect	O
privacy	O
as	O
we	O
desired	O
in	O
the	O
baselines	O
.	O
For	O
BP	O
u	O
,	O
LM+KL+MI	O
are	O
around	O
10	O
times	O
smaller	O
than	O
LM	O
.	O
It	O
means	O
that	O
after	O
applying	O
defense	O
objectives	O
,	O
the	O
attacker	O
's	O
estimated	O
distribution	O
is	O
much	O
closer	O
to	O
the	O
uniform	O
distribution	O
.	O
Thus	O
the	O
effectiveness	O
of	O
the	O
KL	O
loss	B-MetricName
is	O
verified	O
.	O
In	O
addition	O
,	O
Max	O
-	O
Ratio	O
with	O
8	O
clusters	O
on	O
Unseen	O
is	O
smaller	O
than	O
4	O
,	O
332	O
labels	O
even	O
though	O
the	O
distribution	O
of	O
8	O
clusters	O
is	O
obviously	O
tighter	O
.	O
Still	O
,	O
the	O
Max	O
-	O
Ratio	O
of	O
58.15	O
%	O
accounts	O
for	O
a	O
much	O
larger	O
fraction	O
than	O
other	O
predictions	O
.	O
In	O
summary	O
,	O
the	O
above	O
results	O
imply	O
that	O
for	O
the	O
smaller	O
label	O
space	O
,	O
our	O
proposed	O
defense	O
objectives	O
are	O
still	O
effective	O
even	O
on	O
unseen	O
persona	O
labels	O
.	O

Previous	O
experiments	O
mainly	O
consider	O
accuracy	B-MetricName
as	O
the	O
evaluation	O
metric	O
.	O
In	O
this	O
section	O
,	O
we	O
use	O
top	O
-	O
k	O
accuracy	B-MetricName
for	O
the	O
black	O
-	O
box	O
persona	O
inference	O
attacks	O
to	O
measure	O
privacy	O
protection	O
.	O
As	O
shown	O
in	O
Table	O
5	O
,	O
our	O
defense	O
is	O
much	O
more	O
robust	O
than	O
LM	O
when	O
k	O
≤	O
50	O
.	O
When	O
k	O
is	O
larger	O
than	O
500	O
,	O
the	O
defense	O
degrades	O
rapidly	O
as	O
k	O
increases	O
.	O
This	O
result	O
implies	O
that	O
the	O
ground	O
truth	O
personas	O
mostly	O
lie	O
in	O
the	O
top	O
2	O
,	O
000	O
predictions	O
even	O
if	O
the	O
defense	O
is	O
applied	O
.	O
For	O
a	O
smaller	O
k	O
,	O
our	O
proposed	O
defense	O
learning	O
strategies	O
are	O
still	O
effective	O
.	O

For	O
each	O
conversation	O
,	O
the	O
utterances	O
are	O
concatenated	O
by	O
the	O
special	O
token	O
"	O
<	O
|	O
endoftext	O
|	O
>	O
"	O
to	O
train	O
the	O
GPT	B-MethodName
-	O
2	O
.	O
To	O
decode	O
outputs	O
from	O
GPT	B-MethodName
-	O
2	O
,	O
we	O
apply	O
the	O
Nucleus	O
Sampling	O
(	O
Holtzman	O
et	O
al	O
,	O
2020	O
)	O
method	O
.	O
We	O
set	O
top	O
-	O
p	O
=	O
0.9	O
with	O
a	O
temperature	O
coefficient	O
0.9	O
to	O
sample	O
words	O
from	O
the	O
GPT	B-MethodName
-	O
2	O
.	O
For	O
optimization	O
,	O
we	O
set	O
2	O
AdamW	B-MethodName
optimizers	O
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2019	O
)	O
for	O
the	O
chatbot	B-TaskName
and	O
the	O
persona	O
predictor	O
respectively	O
.	O
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
3e	O
-	O
5	O
with	O
linear	O
warm	O
-	O
up	O
and	O
decay	O
.	O
For	O
hyper	O
-	O
parameters	O
,	O
we	O
set	O
λ	O
0	B-DatasetName
=	O
1	O
,	O
λ	O
1	O
=	O
10	O
and	O
λ	O
2	O
=	O
1	O
.	O

To	O
make	O
predictions	O
on	O
personas	O
,	O
the	O
arg	O
max	O
function	O
is	O
used	O
for	O
the	O
estimated	O
distribution	O
of	O
persona	O
predictors	O
.	O
However	O
,	O
the	O
internal	O
distribution	O
conveys	O
crucial	O
information	O
about	O
how	O
the	O
persona	O
predictors	O
estimate	O
f	O
(	O
u	O
)	O
.	O
We	O
follow	O
the	O
setup	O
of	O
imbalanced	O
data	O
split	O
of	O
8	O
clusters	O
in	O
Section	O
5.5	O
to	O
examine	O
persona	O
predictors	O
of	O
attacker	O
A	O
and	O
fake	O
attacker	O
A	O
p	O
.	O
Figure	O
4	O
shows	O
the	O
data	O
distribution	O
of	O
the	O
test	O
set	O
and	O
average	O
distribution	O
after	O
softmax	B-MethodName
activation	O
over	O
the	O
8	O
labels	O
for	O
attacker	O
A	O
and	O
defender	O
A	O
p	O
.	O
For	O
attacker	O
A	O
,	O
we	O
consider	O
the	O
attack	O
on	O
LM	O
and	O
LM+KL+MI	O
.	O
The	O
defender	O
A	O
p	O
tends	O
to	O
have	O
a	O
large	O
difference	O
with	O
Data	O
and	O
tries	O
to	O
flatten	O
its	O
distribution	O
among	O
its	O
own	O
training	O
set	O
(	O
the	O
last	O
5	O
labels	O
)	O
.	O
This	O
behavior	O
conforms	O
to	O
the	O
KL	O
loss	B-MetricName
's	O
objective	O
that	O
aims	O
to	O
flatten	O
the	O
distribution	O
and	O
deviate	O
from	O
the	O
ground	O
truth	O
distribution	O
.	O
For	O
attacker	O
A	O
,	O
distributions	O
of	O
both	O
LM	O
and	O
LM+KL+MI	O
seem	O
close	O
to	O
the	O
ground	O
truth	O
distribution	O
.	O
This	O
indicates	O
that	O
the	O
attacker	O
model	O
A	O
can	O
still	O
learn	O
statistical	O
in	O
-	O

Since	O
NMT	O
systems	O
have	O
achieved	O
the	O
highest	O
translation	O
quality	O
in	O
recent	O
evaluation	O
contests	O
,	O
the	O
Marian	O
-	O
NMT	O
package	O
(	O
Junczys	O
-	O
Dowmunt	O
et	O
al	O
,	O
2018	O
)	O
is	O
adopted	O
for	O
experimentation	O
here	O
.	O
Marian	O
-	O
NMT	O
development	O
was	O
funded	O
by	O
the	O
European	O
Commission	O
to	O
consolidate	O
NMT	O
research	O
and	O
incorporates	O
the	O
most	O
recent	O
advances	O
in	O
NMT	O
.	O
Its	O
code	O
is	O
optimized	O
to	O
reduce	O
the	O
CPU	O
/	O
GPU	O
time	O
required	O
to	O
complete	O
the	O
simulations	O
of	O
NMT	O
systems	O
.	O
For	O
creating	O
NMT	O
systems	O
,	O
three	O
of	O
the	O
models	O
provided	O
by	O
Marian	O
-	O
NMT	O
were	O
chosen	O
,	O
termed	O
as	O
the	O
"	O
transformer	O
"	O
,	O
"	O
amun	O
"	O
and	O
"	O
s2s	O
"	O
models	O
.	O
The	O
"	O
transformer	O
"	O
model	O
has	O
been	O
based	O
on	O
the	O
work	O
of	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
and	O
uses	O
a	O
simple	O
structure	O
incorporating	O
attention	O
mechanisms	O
and	O
dispensing	O
with	O
recurrence	O
to	O
implement	O
a	O
fast	O
NMT	O
system	O
.	O
The	O
other	O
two	O
models	O
are	O
more	O
conventional	O
,	O
using	O
a	O
recurrent	O
neural	O
network	O
to	O
implement	O
the	O
translation	O
.	O
The	O
"	O
amun	O
"	O
model	O
follows	O
the	O
approach	O
of	O
Bahdanau	O
et	O
al	O
(	O
2016	O
)	O
,	O
employing	O
a	O
recurrent	O
neural	O
network	O
but	O
allowing	O
the	O
model	O
to	O
automatically	O
search	O
for	O
wider	O
ranges	O
of	O
the	O
source	O
language	O
(	O
SL	O
)	O
to	O
connect	O
with	O
the	O
target	O
language	O
side	O
(	O
TL	O
)	O
words	O
.	O
Finally	O
,	O
"	O
s2s	O
"	O
implements	O
a	O
recurrent	O
neural	O
network	O
-	O
based	O
encoder	O
-	O
decoder	O
model	O
with	O
attention	O
mechanism	O
,	O
using	O
the	O
architecture	O
proposed	O
in	O
(	O
Sennrich	O
et	O
al	O
,	O
2017	O
)	O
.	O
Hereafter	O
,	O
the	O
three	O
models	O
are	O
identified	O
via	O
the	O
names	O
used	O
within	O
Marian	O
-	O
NMT	O
,	O
which	O
are	O
also	O
used	O
in	O
evaluations	O
(	O
cf	O
.	O
Bojar	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
main	O
configuration	O
parameters	O
used	O
for	O
each	O
model	O
are	O
depicted	O
in	O
Table	O
1	O
,	O
to	O
enable	O
replication	O
of	O
experiments	O
.	O
For	O
each	O
model	O
,	O
different	O
optimization	O
options	O
from	O
Marian	O
-	O
NMT	O
during	O
the	O
validation	O
phase	O
are	O
used	O
to	O
create	O
three	O
NMT	O
variants	O
of	O
each	O
model	O
,	O
namely	O
optimizing	O
with	O
(	O
i	O
)	O
BLEU	B-MetricName
,	O
(	O
ii	O
)	O
entropy	O
and	O
(	O
iii	O
)	O
word	O
-	O
wise	O
normalized	O
crossentropy	O
(	O
denoted	O
as	O
"	O
ce	O
-	O
mean	O
"	O
and	O
representing	O
the	O
default	O
optimization	O
for	O
Marian	O
-	O
NMT	O
)	O
.	O
Regarding	O
the	O
main	O
NMT	O
parameters	O
,	O
all	O
recurrent	O
networks	O
comprise	O
1	O
,	O
024	O
units	O
in	O
the	O
hidden	O
layer	O
,	O
an	O
encoder	O
depth	O
of	O
6	O
layers	O
and	O
an	O
embedding	O
size	O
of	O
512	O
.	O
All	O
cells	O
used	O
both	O
in	O
the	O
encoder	O
and	O
decoder	O
side	O
are	O
gated	O
recurrent	O
units	O
(	O
GRU	B-MethodName
)	O
.	O
The	O
transformer	O
dimension	O
is	O
set	O
to	O
2	O
,	O
048	O
.	O
To	O
reduce	O
the	O
lexicon	O
size	O
,	O
a	O
total	O
of	O
85	O
,	O
000	O
merge	O
operations	O
are	O
allowed	O
using	O
the	O
BPE	B-MethodName
(	O
Byte	B-MethodName
Pair	I-MethodName
Encoding	I-MethodName
)	O
method	O
proposed	O
in	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
,	O
this	O
being	O
the	O
default	O
setting	O
for	O
marian	O
-	O
nmt	O
applications	O
.	O
Initially	O
,	O
the	O
three	O
Marian	O
-	O
NMT	O
models	O
are	O
trained	O
to	O
provide	O
the	O
base	O
NMT	O
systems	O
.	O
Typically	O
,	O
for	O
a	O
single	O
-	O
GPU	O
system	O
(	O
equipped	O
with	O
an	O
NVIDIA	O
Titan	B-DatasetName
XP	O
GTX1080	O
GPU	O
card	O
driven	O
by	O
an	O
Intel	O
i	O
-	O
9700	O
K	O
CPU	O
)	O
,	O
24	O
hours	O
are	O
required	O
for	O
training	O
the	O
transformer	O
,	O
130	O
hours	O
for	O
amun	O
and	O
308	O
hours	O
for	O
s2s	O
.	O
This	O
is	O
equivalent	O
to	O
a	O
ratio	O
of	O
1:5:12	O
to	O
train	O
the	O
respective	O
systems	O
.	O

The	O
experiments	O
aim	O
to	O
improve	O
the	O
translation	O
accuracy	B-MetricName
of	O
an	O
NMT	O
system	O
,	O
taking	O
into	O
account	O
limited	O
training	O
data	O
and	O
constrained	O
computing	O
resources	O
.	O
In	O
order	O
to	O
investigate	O
translation	O
into	O
a	O
lesser	O
-	O
used	O
and	O
highly	O
inflectional	O
language	O
,	O
we	O
Common	O
to	O
all	O
3	O
models	O
layer	O
-	O
normalization	O
yes	O
exponential	O
smoothing	O
yes	O
beam	O
-	O
size	O
6	O
normalize	O
0.6	O
early	O
-	O
stopping	O
5	O
Transformer	B-MethodName
-	O
specific	O
transformer	O
-	O
dropout	O
0.1	O
transformer	O
-	O
dropout	O
-	O
attention	O
0.1	O
transformer	O
-	O
dropout	O
-	O
ffn	O
0.1	O
Amun	O
and	O
s2s	O
-	O
specific	O
dropout	O
-	O
rnn	O
0.2	O
dropout	O
-	O
src	O
0.1	O
dropout	O
-	O
trg	O
0.1	O
have	O
chosen	O
the	O
English	O
-	O
to	O
-	O
Greek	O
language	O
pair	O
.	O
When	O
selecting	O
the	O
training	O
corpora	O
,	O
it	O
has	O
been	O
decided	O
to	O
refrain	O
from	O
using	O
expensive	O
language	O
resources	O
such	O
as	O
specialized	O
or	O
hand	O
-	O
built	O
parallel	O
corpora	O
.	O
Instead	O
,	O
only	O
standard	O
publicly	O
available	O
parallel	O
corpora	O
have	O
been	O
adopted	O
,	O
namely	O
the	O
Europarl	O
and	O
DGT	O
-	O
Acquis	O
corpora	O
2	O
,	O
as	O
listed	O
in	O
Table	O
2	O
.	O
The	O
largest	O
part	O
of	O
the	O
Europarl	O
corpus	O
and	O
the	O
entire	O
DGT	O
-	O
Acquis	O
corpus	O
are	O
used	O
to	O
train	O
the	O
NMT	O
system	O
.	O
Three	O
small	O
portions	O
of	O
the	O
Europarl	O
corpus	O
have	O
been	O
reserved	O
for	O
test	O
and	O
validation	O
purposes	O
.	O
More	O
specifically	O
,	O
two	O
independent	O
sets	O
of	O
approx	O
.	O
3	O
,	O
000	O
Europarl	O
sentences	O
each	O
are	O
excluded	O
,	O
to	O
ensure	O
that	O
the	O
NMT	O
evaluation	O
is	O
unbiased	O
.	O
In	O
the	O
present	O
experiment	O
,	O
one	O
of	O
these	O
sets	O
is	O
used	O
for	O
in	O
-	O
training	O
validation	O
.	O
The	O
other	O
set	O
is	O
reserved	O
to	O
allow	O
additional	O
cross	O
-	O
evaluation	O
of	O
experiments	O
in	O
the	O
future	O
,	O
without	O
invalidating	O
the	O
previously	O
trained	O
models	O
.	O
Finally	O
,	O
a	O
sample	O
2	O
The	O
Europarl	O
corpus	O
(	O
ver.7	O
)	O
was	O
retrieved	O
from	O
https://www.statmt.org/europarl	O
.	O
The	O
DGT	O
-	O
Acquis	O
corpus	O
was	O
retrieved	O
from	O
https://ec.europa.eu/jrc/en/languagetechnologies/dgt	O
-	O
translation	O
-	O
memory	O
of	O
1	O
,	O
000	O
sentences	O
from	O
Europarl	O
(	O
Testset2	O
)	O
is	O
retained	O
to	O
provide	O
an	O
unseen	O
in	O
-	O
domain	O
test	O
set	O
.	O
Another	O
independent	O
test	O
set	O
was	O
drawn	O
from	O
the	O
PRESEMT	O
project	O
resources	O
,	O
comprising	O
200	O
sentences	O
which	O
have	O
not	O
been	O
used	O
to	O
either	O
train	O
an	O
MT	O
model	O
or	O
create	O
any	O
resources	O
used	O
herewith	O
(	O
denoted	O
as	O
Testset1	O
)	O
.	O
A	O
preliminary	O
analysis	O
of	O
the	O
NMT	O
outputs	O
has	O
shown	O
that	O
translations	O
are	O
commendably	O
fluent	O
,	O
though	O
errors	O
are	O
evident	O
.	O
A	O
sample	O
of	O
amun	O
translations	O
is	O
shown	O
in	O
Figure	O
1	O
.	O
In	O
sentence	O
#	O
1	O
,	O
the	O
term	O
"	O
Αμερικανοί	O
"	O
(	O
Transl	O
.	O
"	O
Americans	O
"	O
)	O
is	O
erroneously	O
used	O
as	O
a	O
translation	O
of	O
the	O
terms	O
"	O
American	O
"	O
,	O
"	O
European	O
"	O
,	O
and	O
"	O
Japanese	O
monopolies	O
"	O
.	O
Similarly	O
,	O
in	O
sentence	O
#	O
2	O
,	O
the	O
phrase	O
"	O
η	O
καταπολέμηση	O
της	O
φτώχειας	O
"	O
(	O
meaning	O
"	O
the	O
reduction	O
of	O
poverty	O
"	O
)	O
is	O
used	O
to	O
translate	O
semantically	O
diverse	O
phrases	O
,	O
including	O
"	O
genetically	O
modified	O
organisms	O
"	O
,	O
and	O
"	O
the	O
negative	O
social	O
effects	O
of	O
unbridled	O
,	O
unregulated	O
globalization	O
"	O
.	O
Repetition	O
is	O
a	O
widely	O
reported	O
weakness	O
of	O
NMT	O
systems	O
,	O
most	O
frequently	O
attributed	O
to	O
insufficient	O
training	O
data	O
.	O
An	O
additional	O
problem	O
concerns	O
the	O
translation	O
of	O
rare	O
words	O
(	O
i.e.	O
words	O
with	O
low	O
frequency	O
in	O
the	O
corpus	O
)	O
,	O
due	O
to	O
the	O
limited	O
vocabulary	O
that	O
NMT	O
systems	O
can	O
directly	O
handle	O
.	O
This	O
is	O
especially	O
severe	O
when	O
translating	O
towards	O
languages	O
with	O
complex	O
morphology	O
,	O
which	O
increases	O
the	O
effective	O
vocabulary	O
size	O
.	O
For	O
example	O
the	O
word	O
"	O
ostensibly	O
"	O
is	O
translated	O
into	O
Greek	O
as	O
"	O
ostenfigher	O
"	O
(	O
ungrammatical	O
)	O
.	O
Similarly	O
the	O
word	O
"	O
room	O
"	O
is	O
translated	O
as	O
"	O
δωματείο	O
"	O
instead	O
of	O
the	O
correct	O
"	O
δωμάτιο	O
"	O
(	O
meaning	O
room	O
)	O
,	O
whilst	O
the	O
word	O
"	O
indistinct	O
"	O
is	O
translated	O
as	O
"	O
άχωρος	O
"	O
which	O
is	O
not	O
a	O
valid	O
Greek	O
word	O
.	O
Αnother	O
issue	O
is	O
that	O
entire	O
phrases	O
present	O
in	O
the	O
source	O
text	O
may	O
be	O
omitted	O
in	O
the	O
translation	O
.	O
For	O
instance	O
the	O
sentence	O
"	O
Businesses	O
have	O
undertaken	O
the	O
education	O
"	O
is	O
translated	O
by	O
a	O
transformer	O
NMT	O
as	O
"	O
H	O
εκπαίδευση	O
έχει	O
αναλάβει	O
,	O
[	O
meaning	O
"	O
education	O
has	O
undertaken	O
"	O
]	O
.	O
Hence	O
,	O
the	O
subject	O
"	O
business	O
"	O
has	O
been	O
deleted	O
.	O

To	O
improve	O
translation	O
accuracy	B-MetricName
,	O
the	O
main	O
errors	O
need	O
to	O
be	O
identified	O
in	O
an	O
automated	O
manner	O
.	O
The	O
idea	O
is	O
that	O
a	O
poor	O
alignment	O
between	O
source	O
text	O
and	O
translation	O
indicates	O
substantial	O
loss	B-MetricName
of	O
meaning	O
during	O
translation	O
.	O
On	O
the	O
contrary	O
a	O
high	O
alignment	O
score	O
is	O
indicative	O
of	O
a	O
high	O
likelihood	O
that	O
Figure	O
1	O
:	O
Example	O
translations	O
generated	O
by	O
amun	O
,	O
with	O
repetitions	O
of	O
texts	O
highlighted	O
in	O
grey	O
the	O
NMT	O
output	O
is	O
an	O
accurate	O
translation	O
.	O
To	O
this	O
end	O
a	O
module	O
will	O
be	O
added	O
to	O
implement	O
alignment	O
verification	O
(	O
AVM	O
)	O
,	O
by	O
determining	O
the	O
match	O
between	O
the	O
input	O
sentence	O
and	O
its	O
translation	O
.	O
The	O
establishment	O
of	O
representative	O
alignment	O
scores	O
allows	O
in	O
turn	O
the	O
combination	O
of	O
multiple	O
NMT	O
models	O
,	O
using	O
AVM	O
to	O
evaluate	O
the	O
accuracy	B-MetricName
of	O
each	O
candidate	O
translation	O
and	O
thus	O
select	O
the	O
best	O
translation	O
on	O
a	O
sentence	O
-	O
by	O
-	O
sentence	O
basis	O
.	O
For	O
this	O
research	O
,	O
MT	O
software	O
and	O
tools	O
released	O
via	O
open	O
-	O
source	O
code	O
have	O
been	O
surveyed	O
and	O
the	O
Phrase	O
Aligner	O
Module	O
(	O
PAM	O
,	O
cf	O
.	O
Troullinos	O
,	O
2013	O
)	O
has	O
been	O
selected	O
.	O
The	O
architecture	O
of	O
the	O
proposal	O
hybrid	O
NMT	O
is	O
depicted	O
in	O
Figure	O
2	O
.	O

PAM	O
was	O
developed	O
as	O
part	O
of	O
the	O
PRESEMT	O
hybrid	O
MT	O
methodology	O
(	O
Tambouratzis	O
et	O
al	O
,	O
2017	O
(	O
Tambouratzis	O
et	O
al	O
,	O
2017	O
)	O
.	O
PRESEMT	O
was	O
designed	O
to	O
create	O
MT	O
systems	O
requiring	O
only	O
very	O
limited	O
amounts	O
of	O
specialized	O
,	O
expensive	O
linguistic	O
resources	O
.	O
Frequently	O
,	O
the	O
most	O
expensive	O
resource	O
is	O
the	O
parallel	O
corpus	O
of	O
SL	O
-	O
TL	O
sentences	O
.	O
PRESEMT	O
uses	O
parallel	O
corpora	O
of	O
only	O
a	O
few	O
hundred	O
sentences	O
,	O
augmented	O
by	O
very	O
extensive	O
but	O
comparatively	O
inexpensive	O
monolingual	O
corpora	O
.	O
Within	O
the	O
PRESEMT	O
methodology	O
,	O
the	O
small	O
parallel	O
corpus	O
serves	O
to	O
establish	O
the	O
transformation	O
from	O
the	O
SL	O
structure	O
to	O
the	O
TL	O
one	O
,	O
using	O
the	O
Phrase	O
Aligner	O
module	O
.	O
This	O
module	O
,	O
handling	O
sentence	O
pairs	O
from	O
this	O
parallel	O
corpus	O
,	O
identifies	O
the	O
correspondence	O
of	O
words	O
and	O
phrases	O
from	O
SL	O
to	O
TL	O
,	O
to	O
determine	O
the	O
translation	O
accuracy	B-MetricName
.	O

In	O
the	O
current	O
application	O
,	O
PAM	O
determines	O
the	O
suitability	O
of	O
each	O
candidate	O
translation	O
,	O
based	O
on	O
its	O
match	O
with	O
the	O
source	O
sentence	O
.	O
Thus	O
,	O
the	O
assumption	O
made	O
is	O
that	O
the	O
input	O
sentence	O
and	O
the	O
candidate	O
translation	O
represent	O
the	O
corresponding	O
SL	O
and	O
TL	O
entries	O
of	O
a	O
parallel	O
corpus	O
and	O
PAM	O
determines	O
their	O
level	O
of	O
parallelism	O
.	O
As	O
the	O
requirement	O
is	O
to	O
grade	O
various	O
translations	O
,	O
the	O
PAM	O
operation	O
is	O
reversed	O
,	O
to	O
identify	O
the	O
quality	O
of	O
match	O
between	O
the	O
input	O
sentence	O
and	O
the	O
generated	O
translations	O
.	O
When	O
PAM	O
was	O
used	O
in	O
PRESEMT	O
,	O
sentence	O
pairs	O
from	O
the	O
parallel	O
corpus	O
with	O
a	O
very	O
low	O
percentage	O
of	O
successful	O
alignments	O
were	O
discarded	O
without	O
measuring	O
their	O
degree	O
of	O
parallelism	O
,	O
as	O
poor	O
exemplars	O
of	O
the	O
structural	O
transformations	O
from	O
SL	O
to	O
TL	O
.	O
Here	O
,	O
PAM	O
is	O
modified	O
so	O
that	O
for	O
all	O
pairs	O
of	O
input	O
sentence	O
and	O
NMT	O
-	O
translation	O
the	O
word	O
alignments	O
and	O
assignments	O
of	O
words	O
to	O
phrases	O
are	O
calculated	O
.	O
This	O
allows	O
the	O
re	O
-	O
roled	O
PAM	O
to	O
grade	O
any	O
source	O
/	O
translation	O
pair	O
,	O
no	O
matter	O
how	O
poor	O
the	O
match	O
of	O
the	O
two	O
sentences	O
is	O
.	O
Two	O
metrics	O
have	O
been	O
established	O
to	O
calculate	O
divergence	O
between	O
the	O
SL	O
sentence	O
and	O
its	O
NMTderived	O
translations	O
.	O
The	O
first	O
metric	O
(	O
Uscore	O
)	O
calculates	O
the	O
number	O
of	O
unaligned	O
words	O
of	O
the	O
source	O
sentence	O
,	O
after	O
PAM	O
is	O
applied	O
.	O
The	O
aim	O
is	O
to	O
have	O
as	O
few	O
unaligned	O
words	O
as	O
possible	O
,	O
so	O
the	O
lower	O
Uscore	O
is	O
,	O
the	O
better	O
the	O
translation	O
is	O
.	O
U	O
score	O
=	O
#	O
unaligned	O
words	O
(	O
1	O
)	O
The	O
second	O
metric	O
(	O
Wscore	O
)	O
is	O
a	O
weighted	O
combination	O
of	O
several	O
indicators	O
of	O
alignment	O
between	O
source	O
sentence	O
and	O
candidate	O
translation	O
.	O
This	O
summarizes	O
in	O
one	O
measurement	O
the	O
type	O
of	O
alignments	O
and	O
the	O
stage	O
at	O
which	O
they	O
were	O
achieved	O
.	O
Hence	O
,	O
for	O
a	O
sentence	O
with	O
K	O
words	O
,	O
Wscore	O
is	O
defined	O
as	O
:	O
W	O
score	O
=	O
K	O
i=1	O
(	O
w	O
i	O
*	O
align	O
stage	O
i	O
)	O
(	O
2	O
)	O
In	O
equation	O
(	O
2	O
)	O
,	O
align	O
-	O
stage	O
i	O
denotes	O
the	O
stage	O
(	O
cf	O
.	O
section	O
4.3	O
for	O
the	O
different	O
stages	O
)	O
at	O
which	O
the	O
i	O
-	O
th	O
SL	O
word	O
is	O
aligned	O
successfully	O
to	O
a	O
TL	O
word	O
,	O
and	O
w	O
i	O
denotes	O
the	O
relevant	O
weight	O
for	O
this	O
stage	O
.	O
In	O
the	O
case	O
of	O
the	O
weighted	O
metric	O
Wscore	O
,	O
the	O
higher	O
the	O
score	O
,	O
the	O
more	O
accurate	O
the	O
corresponding	O
translation	O
is	O
.	O
The	O
actual	O
weight	O
values	O
must	O
reward	O
the	O
establishment	O
of	O
alignments	O
at	O
an	O
earlier	O
rather	O
than	O
a	O
later	O
stage	O
.	O
Thus	O
,	O
w	O
i	O
should	O
be	O
larger	O
than	O
w	O
j	O
,	O
for	O
i	O
smaller	O
than	O
j.	O
For	O
the	O
purposes	O
of	O
the	O
present	O
article	O
,	O
w	O
i	O
is	O
set	O
to	O
integer	O
values	O
of	O
5	O
,	O
2	O
and	O
1	O
for	O
the	O
first	O
,	O
second	O
and	O
third	O
stage	O
respectively	O
(	O
other	O
sets	O
of	O
weight	O
values	O
that	O
follow	O
this	O
reasoning	O
produce	O
similar	O
results	O
to	O
those	O
reported	O
here	O
)	O
.	O
The	O
code	O
of	O
PAM	O
has	O
been	O
modified	O
to	O
integrate	O
Wscore	O
and	O
Uscore	O
calculation	O
,	O
though	O
the	O
actual	O
alignment	O
(	O
Schmid	O
,	O
1994	O
)	O
is	O
used	O
,	O
with	O
a	O
reported	O
tagging	O
accuracy	B-MetricName
exceeding	O
96	O
%	O
.	O

To	O
determine	O
the	O
quality	O
of	O
the	O
NMT	O
-	O
based	O
translations	O
(	O
amun	O
-	O
,	O
s2s	O
-	O
and	O
transformer	O
-	O
based	O
models	O
)	O
,	O
two	O
widely	O
used	O
MT	O
evaluation	O
metrics	O
are	O
utilised	O
,	O
namely	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
and	O
NIST	O
(	O
Doddington	O
,	O
2002	O
)	O
.	O
To	O
calculate	O
both	O
these	O
metrics	O
,	O
the	O
mt	O
-	O
eval	O
package	O
(	O
version	O
13a	O
)	O
is	O
used	O
.	O
For	O
PAM	O
,	O
the	O
PRESEMT	O
bilingual	O
lexicon	O
from	O
Greek	O
to	O
English	O
is	O
used	O
,	O
which	O
contains	O
approx	O
.	O
8	O
,	O
000	O
lemmas	O
and	O
40	O
,	O
000	O
Greek	O
-	O
English	O
token	O
pairs	O
.	O
This	O
lexicon	O
is	O
from	O
the	O
same	O
domain	O
as	O
testset1	O
and	O
is	O
thus	O
out	O
-	O
of	O
-	O
domain	O
for	O
testset2	O
,	O
providing	O
a	O
more	O
limited	O
coverage	O
for	O
this	O
testset	O
.	O
Two	O
different	O
types	O
of	O
experiments	O
are	O
possible	O
,	O
depending	O
on	O
whether	O
the	O
ensemble	O
comprises	O
multiple	O
NMT	O
architectures	O
,	O
or	O
only	O
one	O
type	O
of	O
architecture	O
.	O
The	O
first	O
experiment	O
reported	O
here	O
involves	O
NMT	O
ensembles	O
that	O
all	O
share	O
the	O
same	O
architecture	O
,	O
but	O
are	O
optimized	O
with	O
different	O
criteria	O
.	O
The	O
second	O
type	O
of	O
experiment	O
studies	O
ensembles	O
which	O
consist	O
of	O
systems	O
with	O
different	O
architectures	O
,	O
to	O
investigate	O
if	O
their	O
combination	O
results	O
in	O
a	O
better	O
translation	O
quality	O
.	O

The	O
results	O
obtained	O
for	O
testset1	O
of	O
the	O
Englishto	O
-	O
Greek	O
translation	O
pair	O
are	O
depicted	O
in	O
Table	O
3	O
,	O
when	O
running	O
the	O
single	O
transformer	O
,	O
amun	O
and	O
s2s	O
models	O
respectively	O
,	O
as	O
well	O
as	O
their	O
ensembles	O
.	O
The	O
corresponding	O
results	O
for	O
testset2	O
are	O
depicted	O
in	O
Table	O
4	O
.	O
In	O
Tables	O
3	O
and	O
4	O
,	O
the	O
first	O
3	O
rows	O
correspond	O
to	O
single	O
NMT	O
models	O
generated	O
when	O
Marian	O
-	O
NMT	O
is	O
trained	O
to	O
optimise	O
(	O
i	O
)	O
BLEU	B-MetricName
,	O
(	O
ii	O
)	O
entropy	O
and	O
(	O
iii	O
)	O
the	O
word	O
-	O
wise	O
normalised	O
cross	O
-	O
entropy	O
(	O
this	O
is	O
denoted	O
as	O
"	O
ce	O
-	O
mean	O
"	O
)	O
.	O
The	O
final	O
three	O
rows	O
of	O
Tables	O
3	O
and	O
4	O
report	O
the	O
accuracy	B-MetricName
of	O
translations	O
obtained	O
by	O
NMT	O
ensembles	O
.	O
Marian	O
-	O
NMT	O
implements	O
a	O
standard	O
ensemble	O
method	O
,	O
which	O
allows	O
the	O
user	O
to	O
combine	O
different	O
models	O
provided	O
they	O
use	O
the	O
same	O
lexicon	O
.	O
The	O
user	O
may	O
specify	O
weighting	O
factors	O
to	O
boost	O
selection	O
of	O
the	O
models	O
deemed	O
to	O
be	O
better	O
.	O
For	O
this	O
article	O
,	O
this	O
ensemble	O
combines	O
the	O
three	O
aforementioned	O
NMT	O
models	O
(	O
i	O
)	O
,	O
(	O
ii	O
)	O
and	O
(	O
iii	O
)	O
,	O
with	O
equal	O
weights	O
for	O
all	O
NMTs	O
.	O
The	O
last	O
two	O
rows	O
report	O
the	O
accuracy	B-MetricName
of	O
ensembles	O
using	O
PAM	O
with	O
(	O
i	O
)	O
Uscore	O
and	O
(	O
ii	O
)	O
Wscore	O
,	O
respectively	O
.	O
A	O
key	O
difference	O
of	O
the	O
Marian	O
-	O
NMT	O
ensemble	O
is	O
that	O
it	O
is	O
able	O
to	O
recombine	O
partial	O
results	O
of	O
the	O
translation	O
process	O
from	O
each	O
NMT	O
model	O
and	O
thus	O
may	O
generate	O
a	O
new	O
translation	O
that	O
is	O
different	O
from	O
all	O
the	O
translations	O
of	O
single	O
-	O
NMT	O
systems	O
.	O
On	O
the	O
contrary	O
,	O
Uscore	O
and	O
Wscore	O
grade	O
the	O
translations	O
generated	O
by	O
single	O
-	O
NMT	O
models	O
in	O
the	O
ensemble	O
,	O
and	O
then	O
select	O
the	O
highest	O
-	O
scoring	O
translation	O
to	O
be	O
the	O
translation	O
produced	O
by	O
the	O
PAM	O
-	O
based	O
ensemble	O
.	O
To	O
evaluate	O
the	O
quality	O
of	O
translations	O
produced	O
by	O
the	O
PAM	O
-	O
based	O
ensembles	O
,	O
two	O
baselines	O
are	O
selected	O
.	O
The	O
first	O
baseline	O
is	O
the	O
"	O
ce	O
-	O
mean	O
"	O
option	O
of	O
the	O
Marian	O
-	O
NMT	O
translation	O
system	O
.	O
The	O
second	O
,	O
and	O
stronger	O
,	O
baseline	O
is	O
the	O
Marian	O
-	O
NMT	O
ensemble	O
(	O
referred	O
to	O
as	O
"	O
Marian	O
-	O
ensemble	O
"	O
hereafter	O
)	O
.	O
Entries	O
that	O
exceed	O
the	O
first	O
baseline	O
are	O
depicted	O
in	O
bold	O
.	O
Entries	O
with	O
scores	O
that	O
exceed	O
the	O
stronger	O
Marian	O
-	O
ensemble	O
baseline	O
are	O
annotated	O
with	O
an	O
asterisk	O
.	O
Based	O
on	O
Table	O
3	O
,	O
for	O
testset1	O
the	O
best	O
BLEU	B-MetricName
scores	O
are	O
achieved	O
by	O
the	O
Marian	O
-	O
NMT	O
ensemble	O
in	O
comparison	O
to	O
single	O
-	O
NMT	O
models	O
.	O
The	O
PAM	O
-	O
Wscore	O
ensemble	O
gives	O
a	O
higher	O
accuracy	B-MetricName
than	O
the	O
Marian	O
-	O
NMT	O
ensemble	O
,	O
whilst	O
the	O
accuracy	B-MetricName
of	O
PAM	O
-	O
Uscore	O
is	O
lower	O
than	O
PAM	O
-	O
Wscore	O
.	O
On	O
the	O
whole	O
,	O
it	O
is	O
Marian	O
-	O
ensemble	O
and	O
PAM	O
-	O
Wscore	O
that	O
generate	O
the	O
best	O
NIST	O
and	O
BLEU	B-MetricName
scores	O
.	O
A	O
broadly	O
similar	O
situation	O
is	O
found	O
when	O
using	O
testset2	O
(	O
Table	O
4	O
)	O
.	O
Here	O
,	O
the	O
improvement	O
conferred	O
by	O
the	O
ensemble	O
methods	O
over	O
the	O
three	O
base	O
models	O
is	O
much	O
more	O
marked	O
.	O
For	O
instance	O
,	O
for	O
BLEU	B-MetricName
,	O
the	O
score	O
is	O
only	O
19.0	O
to	O
20.0	O
for	O
single	O
NMT	O
models	O
,	O
but	O
rises	O
to	O
more	O
than	O
28.0	O
for	O
the	O
ensembles	O
,	O
which	O
equates	O
to	O
more	O
than	O
eight	O
BLEU	B-MetricName
percentage	O
points	O
of	O
improvement	O
.	O

One	O
question	O
is	O
whether	O
the	O
improvements	O
conferred	O
by	O
the	O
ensembles	O
are	O
statistically	O
significant	O
.	O
To	O
that	O
end	O
,	O
the	O
BLEU	B-MetricName
and	O
NIST	O
scores	O
of	O
all	O
the	O
independent	O
sentences	O
are	O
assembled	O
,	O
forming	O
two	O
populations	O
of	O
scores	O
(	O
one	O
for	O
BLEU	B-MetricName
and	O
one	O
for	O
NIST	O
)	O
for	O
each	O
experimental	O
run	O
.	O
Then	O
the	O
Wilcoxon	O
and	O
sign	O
tests	O
are	O
used	O
to	O
determine	O
if	O
these	O
populations	O
have	O
significant	O
differences	O
.	O
For	O
testset1	O
,	O
the	O
scores	O
of	O
the	O
single	O
NMT	O
systems	O
and	O
the	O
NMT	O
-	O
ensembles	O
are	O
relatively	O
close	O
,	O
differing	O
by	O
less	O
than	O
2	O
BLEU	B-MetricName
points	O
.	O
Applying	O
the	O
sign	O
and	O
Wilcoxon	O
tests	O
,	O
Marian	O
-	O
ensemble	O
produces	O
statistically	O
better	O
NIST	O
scores	O
(	O
at	O
a	O
0.05	O
level	O
)	O
than	O
the	O
default	O
Marian	O
-	O
NMT	O
output	O
for	O
amun	O
and	O
s2s	O
models	O
,	O
but	O
not	O
for	O
the	O
transformer	O
model	O
.	O
For	O
the	O
transformer	O
and	O
s2s	O
models	O
,	O
the	O
scores	O
generated	O
by	O
PAM	O
-	O
Wscore	O
are	O
significantly	O
better	O
that	O
those	O
of	O
single	O
-	O
model	O
Marian	O
-	O
NMT	O
,	O
according	O
to	O
both	O
the	O
Wilcoxon	O
and	O
sign	O
tests	O
(	O
at	O
a	O
0.05	O
level	O
)	O
.	O
Similarly	O
,	O
PAM	O
-	O
Uscore	O
gives	O
statistically	O
superior	O
results	O
to	O
Marian	O
-	O
NMT	O
(	O
ce	O
-	O
mean	O
optimization	O
)	O
for	O
the	O
s2s	O
model	O
(	O
at	O
a	O
significance	O
level	O
of	O
0.05	O
)	O
.	O
Comparing	O
the	O
ensembles	O
to	O
each	O
other	O
,	O
Wscore	O
consistently	O
produces	O
higher	O
scores	O
than	O
Uscore	O
.	O
This	O
superiority	O
is	O
statistically	O
significant	O
at	O
a	O
0.05	O
level	O
according	O
to	O
both	O
Wilcoxon	O
and	O
sign	O
tests	O
,	O
for	O
the	O
transformer	O
and	O
the	O
amun	O
models	O
.	O
PAM	O
-	O
Wscore	O
achieves	O
consistently	O
higher	O
translation	O
scores	O
than	O
Marian	O
-	O
ensemble	O
for	O
both	O
BLEU	B-MetricName
and	O
NIST	O
.	O
According	O
to	O
the	O
Wilcoxon	O
test	O
,	O
these	O
differences	O
are	O
statistically	O
significant	O
,	O
at	O
a	O
0.05	O
level	O
,	O
only	O
for	O
the	O
s2s	O
(	O
BLEU	B-MetricName
score	I-MetricName
)	O
and	O
the	O
transformer	O
model	O
(	O
both	O
BLEU	B-MetricName
and	O
NIST	O
scores	O
)	O
.	O
Turning	O
to	O
testset2	O
,	O
the	O
results	O
are	O
more	O
clearly	O
separated	O
.	O
All	O
three	O
ensembles	O
(	O
i.e.	O
PAM	O
-	O
Wscore	O
,	O
PAM	O
-	O
Uscore	O
and	O
Marian	O
-	O
ensemble	O
)	O
have	O
statistically	O
superior	O
scores	O
to	O
Marian	O
(	O
optimised	O
with	O
ce	O
-	O
mean	O
)	O
for	O
both	O
BLEU	B-MetricName
and	O
NIST	O
,	O
at	O
a	O
significance	O
level	O
of	O
0.01	O
.	O
This	O
extends	O
to	O
all	O
three	O
NMT	O
models	O
(	O
amun	O
,	O
transformer	O
and	O
s2s	O
)	O
,	O
and	O
indicates	O
that	O
both	O
Marian	O
-	O
ensemble	O
and	O
the	O
two	O
PAM	O
-	O
based	O
ensembles	O
give	O
substantially	O
higher	O
scores	O
than	O
single	O
Marian	O
-	O
NMT	O
models	O
.	O
On	O
the	O
other	O
hand	O
,	O
when	O
comparing	O
PAM	O
-	O
Wscore	O
to	O
PAM	O
-	O
Uscore	O
for	O
testset2	O
,	O
no	O
statistically	O
significant	O
difference	O
(	O
at	O
a	O
0.05	O
level	O
of	O
signifi	O
-	O
cance	O
)	O
between	O
the	O
two	O
systems	O
is	O
discerned	O
by	O
either	O
the	O
Wilcoxon	O
or	O
sign	O
test	O
.	O
Similarly	O
,	O
no	O
statistically	O
significant	O
differences	O
at	O
a	O
0.05	O
level	O
are	O
found	O
between	O
the	O
PAM	O
-	O
based	O
ensembles	O
and	O
the	O
Marian	O
-	O
ensemble	O
and	O
only	O
small	O
differences	O
at	O
a	O
0.10	O
level	O
.	O
Thus	O
,	O
even	O
though	O
PAM	O
-	O
based	O
ensembles	O
achieve	O
scores	O
higher	O
than	O
Marian	O
-	O
ensemble	O
,	O
differences	O
are	O
not	O
significant	O
.	O

To	O
quantize	O
the	O
improvements	O
achieved	O
by	O
the	O
proposed	O
PAM	O
-	O
Wscore	O
approach	O
,	O
in	O
this	O
section	O
the	O
computational	O
requirements	O
posed	O
by	O
each	O
NMT	O
system	O
are	O
also	O
considered	O
.	O
To	O
this	O
end	O
,	O
the	O
most	O
accurate	O
NMT	O
system	O
is	O
defined	O
for	O
each	O
dataset	O
and	O
metric	O
combination	O
.	O
Two	O
baselines	O
are	O
chosen	O
,	O
namely	O
the	O
most	O
accurate	O
NMT	O
model	O
and	O
the	O
most	O
accurate	O
Marian	O
-	O
ensemble	O
.	O
We	O
focus	O
on	O
the	O
transformer	O
model	O
,	O
which	O
is	O
the	O
least	O
expensive	O
model	O
to	O
train	O
.	O
For	O
each	O
ensemble	O
using	O
transformers	O
,	O
the	O
aim	O
is	O
to	O
determine	O
how	O
close	O
to	O
the	O
Marian	O
-	O
ensemble	O
baseline	O
this	O
is	O
.	O
Results	O
are	O
shown	O
in	O
Table	O
5	O
,	O
where	O
the	O
accuracy	B-MetricName
of	O
each	O
transformer	O
NMT	O
is	O
expressed	O
as	O
a	O
fraction	O
of	O
the	O
Marian	O
-	O
ensemble	O
score	O
.	O
The	O
best	O
single	O
transformer	O
model	O
achieves	O
for	O
testset1	O
88.7	O
%	O
of	O
the	O
baseline	O
BLEU	B-MetricName
score	I-MetricName
and	O
93.1	O
%	O
of	O
the	O
NIST	O
score	O
.	O
Using	O
the	O
Wscore	O
ensembling	O
method	O
,	O
this	O
rises	O
to	O
90.7	O
%	O
for	O
BLEU	B-MetricName
and	O
95.2	O
%	O
for	O
NIST	O
,	O
showing	O
a	O
gain	O
of	O
2	O
%	O
.	O
Turning	O
to	O
dataset2	O
,	O
the	O
single	O
transformer	O
scores	O
just	O
70.5	O
%	O
in	O
comparison	O
to	O
the	O
baseline	O
BLEU	B-MetricName
score	I-MetricName
and	O
73.4	O
%	O
of	O
the	O
NIST	O
score	O
(	O
therefore	O
it	O
is	O
27	O
%	O
to	O
30	O
%	O
lower	O
)	O
.	O
The	O
Wscore	O
ensemble	O
improves	O
relative	O
scores	O
,	O
reaching	O
92.7	O
%	O
and	O
94.5	O
%	O
of	O
the	O
baseline	O
scores	O
for	O
BLEU	B-MetricName
and	O
NIST	O
respectively	O
.	O
This	O
equates	O
to	O
an	O
increase	O
of	O
ca	O
.	O
22	O
%	O
in	O
both	O
scores	O
,	O
making	O
the	O
final	O
result	O
directly	O
comparable	O
to	O
s2s	O
,	O
though	O
GPU	O
training	O
requirements	O
are	O
reduced	O
by	O
a	O
factor	O
of	O
12	O
.	O

A	O
second	O
type	O
of	O
evaluation	O
moves	O
away	O
from	O
metrics	O
to	O
focus	O
on	O
analysing	O
the	O
translation	O
errors	O
by	O
different	O
models	O
,	O
with	O
subjective	O
methods	O
.	O
For	O
instance	O
,	O
when	O
transformer	O
NMT	O
models	O
are	O
tasked	O
to	O
translate	O
testset1	O
,	O
the	O
BLEU	B-MetricName
-	O
optimised	O
NMT	O
generates	O
26	O
ungrammatical	O
words	O
,	O
the	O
entropyoptimised	O
NMT	O
generates	O
24	O
ungrammatical	O
words	O
and	O
the	O
cross	O
-	O
entropy	O
optimised	O
model	O
produces	O
23	O
ungrammatical	O
words	O
.	O
The	O
Wscore	O
-	O
ensemble	O
reduces	O
the	O
ungrammatical	O
words	O
to	O
21	O
,	O
improving	O
translation	O
.	O
The	O
ungrammatical	O
words	O
were	O
determined	O
in	O
all	O
cases	O
by	O
visual	O
inspection	O
of	O
the	O
body	O
of	O
translations	O
complemented	O
by	O
spell	O
-	O
checking	O
tools	O
to	O
aid	O
detection	O
.	O
Further	O
inspection	O
of	O
translation	O
quality	O
has	O
involved	O
comparing	O
the	O
Marian	O
-	O
ensemble	O
and	O
Wscore	O
-	O
ensemble	O
outputs	O
.	O
The	O
length	O
(	O
in	O
words	O
)	O
of	O
translations	O
per	O
test	O
sentence	O
is	O
found	O
to	O
differ	O
substantially	O
between	O
the	O
two	O
ensembles	O
,	O
with	O
the	O
difference	O
being	O
more	O
than	O
1/10	O
for	O
9	O
%	O
of	O
sentences	O
,	O
more	O
than	O
1/4	O
for	O
2.5	O
%	O
of	O
sentences	O
and	O
more	O
than	O
1/2	O
for	O
1	O
%	O
of	O
sentences	O
(	O
close	O
to	O
identical	O
results	O
are	O
obtained	O
for	O
testset1	O
and	O
testset2	O
)	O
.	O
As	O
such	O
deviations	O
are	O
unexpectedly	O
large	O
,	O
an	O
analysis	O
was	O
performed	O
,	O
with	O
typical	O
examples	O
being	O
shown	O
in	O
Figure	O
3	O
.	O
As	O
can	O
be	O
seen	O
,	O
PAM	O
assists	O
the	O
Wscore	O
-	O
ensemble	O
in	O
retaining	O
all	O
phrases	O
of	O
the	O
sentence	O
.	O
On	O
the	O
contrary	O
,	O
Marian	O
-	O
ensemble	O
fails	O
to	O
ensure	O
this	O
,	O
and	O
frequently	O
discards	O
portions	O
of	O
the	O
input	O
sentence	O
.	O
In	O
one	O
case	O
(	O
sentence	O
#	O
774	O
)	O
Marianensemble	O
results	O
in	O
a	O
null	O
-	O
length	O
translation	O
,	O
and	O
in	O
another	O
(	O
sentence	O
#	O
648	O
)	O
the	O
final	O
translation	O
covers	O
less	O
than	O
10	O
%	O
of	O
the	O
input	O
text	O
,	O
radically	O
distorting	O
meaning	O
.	O
Both	O
PAM	O
-	O
ensembles	O
are	O
unaffected	O
by	O
such	O
phenomena	O
.	O

This	O
article	O
has	O
studied	O
the	O
creation	O
of	O
translation	O
systems	O
towards	O
highly	O
inflectional	O
languages	O
,	O
when	O
the	O
amount	O
of	O
in	O
-	O
domain	O
training	O
data	O
is	O
limited	O
.	O
Emphasis	O
has	O
been	O
placed	O
on	O
improving	O
the	O
translation	O
accuracy	B-MetricName
of	O
NMT	O
models	O
that	O
can	O
be	O
trained	O
more	O
rapidly	O
and	O
cost	O
-	O
effectively	O
(	O
in	O
terms	O
of	O
CPU	O
processing	O
power	O
)	O
and	O
rendering	O
this	O
performance	O
comparable	O
to	O
that	O
of	O
more	O
complex	O
models	O
.	O
The	O
Marian	O
-	O
NMT	O
package	O
has	O
been	O
chosen	O
as	O
the	O
starting	O
point	O
to	O
create	O
NMT	O
models	O
for	O
the	O
English	O
to	O
Greek	O
language	O
pair	O
.	O
Using	O
only	O
publicly	O
available	O
text	O
corpora	O
,	O
the	O
NMT	O
models	O
produce	O
commendably	O
fluent	O
translations	O
.	O
Identified	O
errors	O
in	O
the	O
NMT	O
translations	O
are	O
typical	O
of	O
a	O
lack	O
of	O
training	O
data	O
.	O
A	O
hybrid	O
methodology	O
has	O
been	O
proposed	O
that	O
samples	O
an	O
ensemble	O
of	O
NMT	O
models	O
to	O
select	O
the	O
final	O
translation	O
,	O
chosen	O
by	O
a	O
module	O
calculating	O
the	O
alignment	O
level	O
between	O
the	O
input	O
sentence	O
and	O
each	O
translation	O
.	O
This	O
module	O
was	O
developed	O
for	O
resource	O
-	O
poor	O
MT	O
systems	O
.	O
The	O
proposed	O
hybrid	O
approach	O
has	O
resulted	O
in	O
higher	O
BLEU	B-MetricName
and	O
NIST	O
scores	O
,	O
compared	O
to	O
those	O
of	O
single	O
NMT	O
models	O
.	O
Improvements	O
are	O
in	O
many	O
cases	O
statistically	O
significant	O
even	O
over	O
the	O
ensemble	O
system	O
provided	O
within	O
the	O
Marian	O
-	O
NMT	O
package	O
,	O
indicating	O
the	O
promising	O
nature	O
of	O
the	O
hybrid	O
approach	O
.	O
Also	O
,	O
the	O
translation	O
process	O
is	O
found	O
to	O
be	O
more	O
robust	O
,	O
giving	O
more	O
consistent	O
translations	O
in	O
comparison	O
to	O
the	O
Marian	O
-	O
NMT	O
ensemble	O
system	O
,	O
which	O
occasionally	O
omits	O
large	O
portions	O
of	O
the	O
input	O
text	O
from	O
the	O
translation	O
.	O
One	O
of	O
the	O
advantages	O
of	O
the	O
proposed	O
method	O
is	O
that	O
it	O
is	O
general	O
-	O
purpose	O
and	O
does	O
not	O
rely	O
on	O
the	O
use	O
of	O
ensembles	O
of	O
Neural	O
MT	O
systems	O
with	O
a	O
specific	O
architecture	O
.	O
Instead	O
,	O
it	O
can	O
be	O
used	O
to	O
combine	O
the	O
results	O
of	O
different	O
types	O
of	O
Neural	O
MT	O
systems	O
,	O
or	O
MT	O
systems	O
that	O
belong	O
to	O
different	O
paradigms	O
,	O
or	O
even	O
to	O
combine	O
human	O
translations	O
.	O
In	O
addition	O
the	O
proposed	O
method	O
can	O
be	O
used	O
to	O
clean	O
up	O
a	O
corpus	O
of	O
parallel	O
sentences	O
or	O
several	O
such	O
corpora	O
,	O
by	O
removing	O
sentence	O
pairs	O
for	O
which	O
the	O
source	O
and	O
target	O
-	O
language	O
texts	O
do	O
not	O
have	O
a	O
high	O
degree	O
of	O
parallelism	O
.	O
Similarly	O
,	O
the	O
proposed	O
method	O
may	O
be	O
used	O
to	O
filter	O
a	O
corpus	O
consisting	O
of	O
original	O
text	O
and	O
its	O
MT	O
-	O
derived	O
translation	O
,	O
to	O
produce	O
a	O
parallel	O
corpus	O
for	O
training	O
of	O
other	O
MT	O
systems	O
,	O
fulfilling	O
a	O
role	O
similar	O
to	O
that	O
proposed	O
by	O
(	O
Rikters	O
and	O
Fishel	O
,	O
2017	O
)	O
.	O
One	O
point	O
for	O
future	O
research	O
is	O
how	O
effective	O
a	O
filtering	O
system	O
based	O
on	O
PAM	O
would	O
be	O
,	O
in	O
comparison	O
to	O
already	O
proposed	O
systems	O
.	O
Future	O
work	O
involves	O
some	O
relatively	O
simple	O
activities	O
that	O
can	O
be	O
imminently	O
implemented	O
,	O
such	O
as	O
releasing	O
the	O
modified	O
version	O
of	O
PAM	O
for	O
experimentation	O
by	O
interested	O
parties	O
.	O
Another	O
short	O
term	O
activity	O
involves	O
using	O
the	O
proposed	O
method	O
with	O
sacreBLEU	B-MetricName
instead	O
of	O
the	O
BLEU	B-MetricName
and	O
NIST	O
metrics	O
provided	O
by	O
mt	O
-	O
eval	O
.	O
Future	O
experiments	O
will	O
investigate	O
the	O
effectiveness	O
of	O
this	O
hybrid	O
approach	O
for	O
other	O
language	O
pairs	O
.	O
One	O
area	O
of	O
interest	O
would	O
be	O
to	O
determine	O
the	O
effectiveness	O
of	O
the	O
PAM	O
-	O
based	O
method	O
when	O
very	O
limited	O
dictionaries	O
are	O
available	O
as	O
well	O
as	O
the	O
limitations	O
when	O
the	O
accuracy	B-MetricName
of	O
the	O
parser	O
used	O
is	O
relatively	O
low	O
.	O
All	O
these	O
represent	O
issues	O
for	O
the	O
future	O
.	O
It	O
is	O
also	O
planned	O
to	O
study	O
the	O
approach	O
using	O
systematic	O
optimisation	O
of	O
the	O
PAM	O
parameters	O
,	O
to	O
identify	O
in	O
more	O
detail	O
configurations	O
that	O
produce	O
more	O
accurate	O
translations	O
.	O
Another	O
possibility	O
is	O
to	O
use	O
PAM	O
to	O
detect	O
sub	O
-	O
sentential	O
parts	O
of	O
the	O
translated	O
sentences	O
with	O
particularly	O
poor	O
alignments	O
between	O
input	O
and	O
translation	O
and	O
seek	O
better	O
translations	O
of	O
only	O
these	O
specific	O
parts	O
.	O
Another	O
direction	O
is	O
to	O
investigate	O
more	O
extensively	O
cases	O
where	O
the	O
translation	O
is	O
not	O
sufficiently	O
close	O
to	O
the	O
input	O
sentence	O
.	O
Then	O
,	O
comparisons	O
to	O
other	O
low	O
-	O
scored	O
translations	O
are	O
more	O
difficult	O
and	O
result	O
in	O
a	O
reduced	O
level	O
of	O
confidence	O
of	O
the	O
chosen	O
translation	O
.	O
Such	O
a	O
line	O
of	O
study	O
will	O
evaluate	O
more	O
thoroughly	O
the	O
robustness	O
of	O
the	O
proposed	O
method	O
.	O

To	O
ensure	O
translation	O
quality	O
,	O
we	O
hired	O
two	O
professional	O
translators	O
with	O
at	O
least	O
seven	O
years	O
of	O
experience	O
who	O
specialize	O
in	O
academic	O
papers	O
/	O
books	O
as	O
well	O
as	O
business	O
contracts	O
.	O
The	O
two	O
translators	O
each	O
post	O
-	O
edited	O
half	O
of	O
the	O
dataset	O
and	O
cross	O
-	O
checked	O
each	O
other	O
's	O
translation	O
afterward	O
.	O
This	O
was	O
further	O
examined	O
by	O
one	O
of	O
the	O
authors	O
,	O
who	O
is	O
fluent	O
in	O
both	O
English	O
and	O
Korean	O
.	O
We	O
also	O
note	O
that	O
the	O
professional	O
translators	O
did	O
not	O
have	O
to	O
edit	O
much	O
during	O
post	O
-	O
editing	O
,	O
suggesting	O
that	O
the	O
machine	O
-	O
translated	O
sentences	O
were	O
often	O
good	O
enough	O
to	O
begin	O
with	O
.	O
We	O
found	O
that	O
the	O
BLEU	B-MetricName
scores	O
between	O
the	O
machine	O
-	O
translated	O
and	O
post	O
-	O
edited	O
sentences	O
were	O
63.30	O
for	O
KorNLI	B-DatasetName
and	O
73.26	O
for	O
KorSTS	B-DatasetName
,	O
and	O
for	O
approximately	O
half	O
the	O
time	O
(	O
47	O
%	O
for	O
KorNLI	B-DatasetName
and	O
53	O
%	O
for	O
KorSTS	B-DatasetName
)	O
,	O
the	O
translators	O
did	O
not	O
have	O
to	O
change	O
the	O
machinetranslated	O
sentence	O
at	O
all	O
.	O
Finally	O
,	O
we	O
note	O
that	O
translators	O
did	O
not	O
see	O
the	O
English	O
gold	O
labels	O
during	O
post	O
-	O
editing	O
,	O
in	O
order	O
to	O
expedite	O
the	O
post	O
-	O
editing	O
process	O
.	O
See	O
Section	O
5	O
for	O
a	O
discussion	O
on	O
the	O
effect	O
of	O
translation	O
on	O
data	O
quality	O
.	O

As	O
illustrated	O
with	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
many	O
of	O
its	O
variants	O
,	O
the	O
de	O
facto	O
standard	O
approach	O
for	O
NLU	O
tasks	O
is	O
to	O
pre	O
-	O
train	O
a	O
large	O
language	O
model	O
and	O
fine	O
-	O
tune	O
it	O
on	O
each	O
task	O
.	O
In	O
the	O
cross	O
-	O
encoding	O
Examples	O
Score	B-MetricName
A	O
:	O
ᄒ	O
ᅡ	O
ᆫ	O
ᄂ	O
ᅡ	O
ᆷᄌ	O
ᅡᄀ	O
ᅡ	O
ᄋ	O
ᅳ	O
ᆷᄉ	O
ᅵ	O
ᆨᄋ	O
ᅳ	O
ᆯ	O
ᄆ	O
ᅥ	O
ᆨᄀ	O
ᅩ	O
ᄋ	O
ᅵ	O
ᆻᄃ	O
ᅡ	O
.	O
4.2	O
"	O
A	O
man	O
is	O
eating	O
food	O
.	O
"	O
B	O
:	O
ᄒ	O
ᅡ	O
ᆫ	O
ᄂ	O
ᅡ	O
ᆷᄌ	O
ᅡᄀ	O
ᅡ	O
ᄆ	O
ᅯ	O
ᆫᄀ	O
ᅡᄅ	O
ᅳ	O
ᆯ	O
ᄆ	O
ᅥ	O
ᆨᄀ	O
ᅩ	O
ᄋ	O
ᅵ	O
ᆻᄃ	O
ᅡ	O
.	O
"	O
A	O
man	O
is	O
eating	O
something	O
.	O
"	O
A	O
:	O
ᄒ	O
ᅡ	O
ᆫ	O
ᄋ	O
ᅧᄉ	O
ᅥ	O
ᆼᄋ	O
ᅵ	O
ᄀ	O
ᅩᄀ	O
ᅵᄅ	O
ᅳ	O
ᆯ	O
ᄋ	O
ᅭᄅ	O
ᅵᄒ	O
ᅡᄀ	O
ᅩ	O
ᄋ	O
ᅵ	O
ᆻᄃ	O
ᅡ	O
.	O
0.0	O
"	O
A	O
woman	O
is	O
cooking	O
meat	O
.	O
"	O
B	O
:	O
ᄒ	O
ᅡ	O
ᆫ	O
ᄂ	O
ᅡ	O
ᆷᄌ	O
ᅡᄀ	O
ᅡ	O
ᄆ	O
ᅡ	O
ᆯᄒ	O
ᅡᄀ	O
ᅩ	O
ᄋ	O
ᅵ	O
ᆻᄃ	O
ᅡ	O
.	O
"	O
A	O
man	O
is	O
speaking	O
.	O
"	O
approach	O
,	O
the	O
pre	O
-	O
trained	O
language	O
model	O
takes	O
each	O
sentence	O
pair	O
as	O
a	O
single	O
input	O
for	O
fine	O
-	O
tuning	O
.	O
These	O
cross	O
-	O
encoding	O
models	O
typically	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
over	O
bi	O
-	O
encoding	O
models	O
,	O
which	O
encode	O
each	O
input	O
sentence	O
separately	O
.	O
For	O
both	O
KorNLI	B-DatasetName
and	O
KorSTS	B-DatasetName
,	O
we	O
consider	O
two	O
pre	O
-	O
trained	O
language	O
models	O
.	O
We	O
first	O
pre	O
-	O
train	O
a	O
Korean	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
,	O
both	O
base	O
and	O
large	O
versions	O
,	O
on	O
a	O
collection	O
of	O
internally	O
collected	O
Korean	O
corpora	O
(	O
65	O
GB	O
)	O
.	O
We	O
construct	O
a	O
byte	B-MethodName
pair	I-MethodName
encoding	I-MethodName
(	O
BPE	B-MethodName
)	O
(	O
Gage	O
,	O
1994	O
;	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
dictionary	O
of	O
32	O
K	O
tokens	O
using	O
Sen	O
-	O
tencePiece	O
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
.	O
We	O
train	O
our	O
models	O
using	O
fairseq	O
with	O
32	O
V100	O
GPUs	O
for	O
the	O
base	O
model	O
(	O
25	O
days	O
)	O
and	O
64	O
for	O
the	O
large	O
model	O
(	O
20	O
days	O
)	O
.	O
We	O
also	O
use	O
XLM	B-MethodName
-	O
R	O
(	O
Conneau	O
and	O
Lample	O
,	O
2019	O
)	O
,	O
a	O
publicly	O
available	O
cross	O
-	O
lingual	O
language	O
model	O
that	O
was	O
pre	O
-	O
trained	O
on	O
2.5	O
TB	O
of	O
Common	B-DatasetName
Crawl	I-DatasetName
corpora	O
in	O
100	O
languages	O
including	O
Korean	O
(	O
54	O
GB	O
)	O
.	O
Note	O
that	O
the	O
base	O
and	O
large	O
architectures	O
of	O
XLM	B-MethodName
-	O
R	O
are	O
identical	O
to	O
those	O
of	O
RoBERTa	B-MethodName
,	O
except	O
that	O
the	O
vocabulary	O
size	O
is	O
significantly	O
larger	O
(	O
250	O
K	O
)	O
,	O
making	O
the	O
embedding	O
and	O
output	O
layers	O
that	O
much	O
larger	O
.	O
In	O
Table	O
5	O
,	O
we	O
report	O
the	O
test	O
set	O
scores	O
for	O
crossencoding	O
models	O
fine	O
-	O
tuned	O
on	O
KorNLI	B-DatasetName
(	O
accuracy	B-MetricName
)	O
and	O
KorSTS	B-DatasetName
(	O
Spearman	B-MetricName
correlation	I-MetricName
)	O
.	O
For	O
KorNLI	B-DatasetName
,	O
we	O
additionally	O
include	O
results	O
for	O
XLM	B-MethodName
-	O
R	O
models	O
fine	O
-	O
tuned	O
on	O
the	O
original	O
MNLI	B-DatasetName
training	O
set	O
(	O
also	O
known	O
as	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
in	O
XNLI	B-DatasetName
)	O
.	O
To	O
ensure	O
comparability	O
across	O
settings	O
,	O
we	O
only	O
train	O
on	O
the	O
MNLI	B-DatasetName
portion	O
when	O
fine	O
-	O
tuning	O
on	O
KorNLI	B-DatasetName
.	O
Overall	O
,	O
the	O
Korean	O
RoBERTa	B-MethodName
models	O
outperform	O
the	O
XLM	B-MethodName
-	O
R	O
models	O
,	O
regardless	O
of	O
whether	O
they	O
are	O
fine	O
-	O
tuned	O
on	O
Korean	O
or	O
English	O
training	O
sets	O
.	O
For	O
each	O
model	O
,	O
the	O
larger	O
variant	O
outperforms	O
the	O
base	O
one	O
,	O
consistent	O
with	O
previous	O
findings	O
.	O
The	O
large	O
version	O
of	O
Korean	O
RoBERTa	B-MethodName
performs	O
the	O
best	O
for	O
both	O
KorNLI	B-DatasetName
(	O
83.67	O
%	O
)	O
and	O
KorSTS	B-DatasetName
(	O
85.27	O
%	O
)	O
among	O
all	O
models	O
tested	O
.	O
Among	O
the	O
XLM	B-MethodName
-	O
R	O
models	O
for	O
KorNLI	B-DatasetName
,	O
those	O
fine	O
-	O
tuned	O
on	O
the	O
Korean	O
training	O
set	O
consistently	O
outperform	O
the	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
variants	O
.	O

We	O
also	O
report	O
the	O
KorSTS	B-DatasetName
scores	O
of	O
bi	O
-	O
encoding	O
models	O
.	O
The	O
bi	O
-	O
encoding	O
approach	O
bears	O
practical	O
importance	O
in	O
applications	O
such	O
as	O
semantic	O
search	O
,	O
where	O
computing	O
pairwise	O
similarity	O
among	O
a	O
large	O
set	O
of	O
sentences	O
is	O
computationally	O
expensive	O
with	O
cross	O
-	O
encoding	O
.	O
Here	O
,	O
we	O
first	O
provide	O
two	O
baselines	O
that	O
do	O
not	O
use	O
pre	O
-	O
trained	O
language	O
models	O
:	O
Korean	O
fastText	B-MethodName
and	O
the	O
multilingual	B-MethodName
universal	I-MethodName
sentence	I-MethodName
encoder	I-MethodName
(	O
M	O
-	O
USE	B-MethodName
)	O
.	O
Korean	O
fastText	B-MethodName
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
is	O
a	O
pre	O
-	O
trained	O
word	O
embedding	O
model	O
6	O
trained	O
on	O
Korean	O
text	O
from	O
Common	B-DatasetName
Crawl	I-DatasetName
.	O
To	O
produce	O
sentence	B-TaskName
embeddings	I-TaskName
,	O
we	O
take	O
the	O
average	O
of	O
fastText	B-MethodName
word	B-TaskName
embeddings	I-TaskName
for	O
each	O
sentence	O
.	O
M	O
-	O
USE	B-MethodName
7	O
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
,	O
is	O
a	O
CNN	O
-	O
based	O
sentence	O
encoder	O
model	O
trained	O
for	O
NLI	O
,	O
questionanswering	O
,	O
and	O
translation	O
ranking	O
across	O
16	O
languages	O
including	O
Korean	O
.	O
For	O
both	O
Korean	O
fastText	B-MethodName
and	O
M	O
-	O
USE	B-MethodName
,	O
we	O
compute	O
the	O
cosine	O
similarity	O
between	O
two	O
input	O
sentence	B-TaskName
embeddings	I-TaskName
to	O
make	O
an	O
unsupervised	O
STS	B-TaskName
prediction	O
.	O
Pre	O
-	O
trained	O
language	O
models	O
can	O
also	O
be	O
used	O
as	O
bi	O
-	O
encoding	O
models	O
following	O
the	O
approach	O
of	O
Sen	O
-	O
tenceBERT	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
,	O
which	O
involves	O
fine	O
-	O
tuning	O
a	O
BERT	B-MethodName
-	O
like	O
model	O
with	O
a	O
Siamese	B-MethodName
network	I-MethodName
structure	O
on	O
NLI	O
and/or	O
STS	B-TaskName
.	O
We	O
use	O
the	O
SentenceBERT	O
approach	O
for	O
both	O
Korean	O
RoBERTa	B-MethodName
(	O
"	O
Korean	O
SRoBERTa	O
"	O
)	O
and	O
XLM	B-MethodName
-	O
R	O
(	O
"	O
SXLM	O
-	O
R	O
"	O
)	O
.	O
We	O
adopt	O
the	O
MEAN	O
pooling	O
strategy	O
,	O
i.e.	O
,	O
computing	O
the	O
sentence	O
vector	O
as	O
the	O
mean	O
of	O
all	O
contextualized	O
word	O
vectors	O
.	O
In	O
Table	O
6	O
,	O
we	O
present	O
the	O
KorSTS	B-DatasetName
test	O
set	O
scores	O
(	O
100	O
×	O
Spearman	B-MetricName
correlation	I-MetricName
)	O
for	O
the	O
biencoding	O
models	O
.	O
We	O
categorize	O
each	O
result	O
based	O
on	O
whether	O
the	O
model	O
was	O
additionally	O
trained	O
on	O
KorNLI	B-DatasetName
and/or	O
KorSTS	B-DatasetName
.	O
Note	O
that	O
models	O
that	O
are	O
not	O
fine	O
-	O
tuned	O
at	O
all	O
or	O
only	O
fine	O
-	O
tuned	O
to	O
KorNLI	B-DatasetName
can	O
be	O
considered	O
as	O
unsupervised	O
w.r.t	O
.	O
KorSTS	B-DatasetName
.	O
Also	O
note	O
that	O
M	O
-	O
USE	B-MethodName
is	O
trained	O
on	O
a	O
machinetranslated	O
version	O
of	O
SNLI	B-DatasetName
,	O
which	O
is	O
a	O
subset	O
of	O
KorNLI	B-DatasetName
,	O
as	O
part	O
of	O
its	O
pre	O
-	O
training	O
step	O
.	O
6	O
https://dl.fbaipublicfiles.com/	O
fasttext	B-MethodName
/	O
vectors	O
-	O
crawl	O
/	O
cc.ko.300.bin.gz	O
7	O
https://tfhub.dev/google/	O
universal	O
-	O
sentence	O
-	O
encoder	O
-	O
multilingual/	O
3	O
First	O
,	O
given	O
each	O
model	O
,	O
we	O
find	O
that	O
supplementary	O
training	O
on	O
KorNLI	B-DatasetName
consistently	O
improves	O
the	O
KorSTS	B-DatasetName
scores	O
for	O
both	O
unsupervised	O
and	O
supervised	O
settings	O
,	O
as	O
was	O
the	O
case	O
with	O
English	O
models	O
(	O
Conneau	O
et	O
al	O
,	O
2017	O
;	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
.	O
This	O
shows	O
that	O
the	O
KorNLI	B-DatasetName
dataset	O
can	O
be	O
an	O
effective	O
intermediate	O
training	O
source	O
for	O
biencoding	O
approaches	O
.	O
When	O
comparing	O
the	O
baseline	O
models	O
in	O
each	O
setting	O
,	O
we	O
find	O
that	O
both	O
M	O
-	O
USE	B-MethodName
and	O
the	O
SentenceBERT	O
-	O
based	O
models	O
trained	O
on	O
KorNLI	B-DatasetName
achieve	O
competitive	O
unsupervised	O
Ko	O
-	O
rSTS	O
scores	O
.	O
Both	O
models	O
significantly	O
outperform	O
the	O
average	O
of	O
fastText	B-MethodName
embeddings	O
model	O
and	O
the	O
Korean	O
SRoBERTa	O
and	O
SXLM	O
-	O
R	O
models	O
without	O
fine	O
-	O
tuning	O
.	O
Among	O
our	O
baselines	O
,	O
large	O
SXLM	O
-	O
R	O
trained	O
on	O
KorNLI	B-DatasetName
followed	O
by	O
KorSTS	B-DatasetName
achieves	O
the	O
best	O
score	O
(	O
81.84	O
)	O
.	O

As	O
noted	O
in	O
(	O
Conneau	O
et	O
al	O
,	O
2018	O
)	O
,	O
translation	O
quality	O
does	O
not	O
necessarily	O
guarantee	O
that	O
the	O
semantic	O
relationships	O
between	O
sentences	O
are	O
preserved	O
.	O
We	O
also	O
translated	O
each	O
sentence	O
independently	O
and	O
took	O
the	O
gold	O
labels	O
from	O
the	O
original	O
English	O
pair	O
,	O
so	O
the	O
resulting	O
label	O
might	O
no	O
longer	O
be	O
"	O
gold	O
,	O
"	O
due	O
to	O
both	O
incorrect	O
translations	O
and	O
(	O
in	O
rarer	O
cases	O
)	O
linguistic	O
differences	O
that	O
make	O
it	O
difficult	O
to	O
translate	O
specific	O
concepts	O
.	O
Fortunately	O
,	O
it	O
was	O
also	O
pointed	O
out	O
in	O
(	O
Conneau	O
et	O
al	O
,	O
2018	O
)	O
that	O
annotators	O
could	O
recover	O
the	O
NLI	O
labels	O
at	O
a	O
similar	O
accuracy	B-MetricName
in	O
translated	O
pairs	O
(	O
83	O
%	O
in	O
French	O
)	O
as	O
in	O
original	O
pairs	O
(	O
85	O
%	O
in	O
English	O
)	O
.	O
In	O
addition	O
,	O
our	O
baseline	O
experiments	O
in	O
Section	O
4.1	O
show	O
that	O
supplementary	O
training	O
on	O
KorNLI	B-DatasetName
improves	O
KorSTS	B-DatasetName
performance	O
(	O
+1	O
%	O
for	O
RoBERTa	B-MethodName
and	O
+4	O
-	O
11	O
%	O
for	O
XLM	B-MethodName
-	O
R	O
)	O
,	O
suggesting	O
that	O
the	O
labels	O
of	O
KorNLI	B-DatasetName
are	O
still	O
meaningful	O
.	O
Another	O
quantitative	O
evidence	O
is	O
that	O
the	O
performance	O
of	O
XLM	B-MethodName
-	O
R	O
fine	O
-	O
tuned	O
on	O
KorNLI	B-DatasetName
(	O
80.3	O
%	O
with	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
)	O
is	O
within	O
a	O
comparable	O
range	O
of	O
the	O
model	O
's	O
performance	O
on	O
other	O
XNLI	B-DatasetName
languages	O
(	O
80.1	O
%	O
on	O
average	O
)	O
.	O
Nevertheless	O
,	O
we	O
could	O
also	O
find	O
some	O
(	O
not	O
many	O
)	O
examples	O
the	O
gold	O
label	O
becomes	O
incorrect	O
after	O
translating	O
input	O
sentences	O
to	O
Korean	O
.	O
For	O
example	O
,	O
there	O
were	O
cases	O
in	O
which	O
the	O
two	O
input	O
sentences	O
for	O
KorSTS	B-DatasetName
were	O
so	O
similar	O
(	O
with	O
4	O
+	O
similarity	O
scores	O
)	O
that	O
upon	O
translation	O
,	O
the	O
two	O
inputs	O
simply	O
became	O
identical	O
.	O
In	O
another	O
case	O
,	O
the	O
English	O
word	O
sir	O
appeared	O
in	O
the	O
premise	O
of	O
an	O
NLI	O
example	O
and	O
was	O
translated	O
to	O
ᄉ	O
ᅥ	O
ᆫᄉ	O
ᅢ	O
ᆼᄂ	O
ᅵ	O
ᆷ	O
,	O
which	O
is	O
a	O
correct	O
word	B-TaskName
translation	I-TaskName
but	O
is	O
a	O
gender	O
-	O
neutral	O
noun	O
,	O
because	O
there	O
is	O
no	O
gender	O
-	O
specific	O
counterpart	O
to	O
the	O
word	O
in	O
Korean	O
.	O
As	O
a	O
result	O
,	O
when	O
the	O
hypothesis	O
referencing	O
the	O
entity	O
as	O
the	O
man	O
got	O
translated	O
into	O
ᄂ	O
ᅡ	O
ᆷᄌ	O
ᅡ	O
(	O
gender	O
-	O
specific	O
)	O
,	O
the	O
English	O
gold	O
label	O
(	O
entailment	O
)	O
was	O
no	O
longer	O
correct	O
in	O
the	O
translated	O
example	O
.	O
More	O
systematically	O
analyzing	O
these	O
errors	O
is	O
an	O
interesting	O
future	O
work	O
,	O
although	O
the	O
amount	O
of	O
human	O
efforts	O
involved	O
in	O
this	O
analysis	O
would	O
match	O
that	O
of	O
labeling	O
a	O
new	O
dataset	O
.	O

To	O
fine	O
-	O
tune	O
Korean	O
RoBERTa	B-MethodName
and	O
XLM	B-MethodName
-	O
R	O
models	O
using	O
the	O
bi	O
-	O
encoding	O
approach	O
(	O
4.2	O
)	O
,	O
we	O
train	O
Korean	O
Sentence	O
RoBERTa	B-MethodName
(	O
"	O
Korean	O
SRoBERTa	O
"	O
)	O
and	O
Sentence	O
XLM	B-MethodName
-	O
R	O
(	O
"	O
SXLM	O
-	O
R	O
"	O
)	O
,	O
following	O
the	O
fine	O
-	O
tuning	O
procedure	O
of	O
SentenceBERT	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
.	O
Unless	O
described	O
otherwise	O
,	O
we	O
follow	O
the	O
experimental	O
settings	O
,	O
including	O
all	O
hyperparameters	O
,	O
of	O
SentenceBERT	O
10	O
.	O
For	O
each	O
model	O
size	O
,	O
we	O
manually	O
search	O
among	O
learning	O
rates	O
{	O
2e	O
-	O
5	O
,	O
1e	O
-	O
5	O
}	O
for	O
training	O
on	O
KorNLI	B-DatasetName
,	O
{	O
1e	O
-	O
5	O
,	O
2e	O
-	O
6	O
}	O
for	O
training	O
on	O
KorSTS	B-DatasetName
,	O
and	O
{	O
1e	O
-	O
5	O
,	O
2e	O
-	O
6	O
}	O
for	O
training	O
on	O
KorSTS	B-DatasetName
after	O
KorNLI	B-DatasetName
.	O
After	O
training	O
until	O
convergence	O
,	O
we	O
choose	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
that	O
lead	O
to	O
the	O
highest	O
KorSTS	B-DatasetName
score	O
on	O
the	O
development	O
set	O
.	O
These	O
hyperparameters	O
are	O
shown	O
in	O
Table	O
10	O
.	O
We	O
report	O
the	O
development	O
set	O
scores	O
in	O
Table	O
11	O
.	O
Korean	O
SRoBERTa	O
(	O
large	O
)	O
achieves	O
the	O
best	O
development	O
set	O
performance	O
on	O
both	O
supervised	O
settings	O
,	O
but	O
SXLM	O
-	O
R	O
(	O
large	O
)	O
achieves	O
the	O
best	O
performance	O
for	O
the	O
KorNLI	B-DatasetName
KorSTS	B-DatasetName
setting	O
on	O
test	O
set	O
.	O

We	O
propose	O
a	O
novel	O
transition	O
-	O
based	O
algorithm	O
that	O
straightforwardly	O
parses	O
sentences	O
from	O
left	O
to	O
right	O
by	O
building	O
n	O
attachments	O
,	O
with	O
n	O
being	O
the	O
length	O
of	O
the	O
input	O
sentence	O
.	O
Similarly	O
to	O
the	O
recent	O
stack	O
-	O
pointer	O
parser	O
by	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
use	O
the	O
pointer	B-MethodName
network	I-MethodName
framework	O
that	O
,	O
given	O
a	O
word	O
,	O
can	O
directly	O
point	O
to	O
a	O
position	O
from	O
the	O
sentence	O
.	O
However	O
,	O
our	O
left	O
-	O
to	O
-	O
right	O
approach	O
is	O
simpler	O
than	O
the	O
original	O
top	O
-	O
down	O
stack	O
-	O
pointer	O
parser	O
(	O
not	O
requiring	O
a	O
stack	O
)	O
and	O
reduces	O
transition	O
sequence	O
length	O
in	O
half	O
,	O
from	O
2n	O
−	O
1	O
actions	O
to	O
n.	O
This	O
results	O
in	O
a	O
quadratic	O
non	O
-	O
projective	O
parser	O
that	O
runs	O
twice	O
as	O
fast	O
as	O
the	O
original	O
while	O
achieving	O
the	O
best	O
accuracy	B-MetricName
to	O
date	O
on	O
the	O
English	O
PTB	B-DatasetName
dataset	O
(	O
96.04	O
%	O
UAS	O
,	O
94.43	O
%	O
LAS	O
)	O
among	O
fully	O
-	O
supervised	O
singlemodel	O
dependency	O
parsers	O
,	O
and	O
improves	O
over	O
the	O
former	O
top	O
-	O
down	O
transition	O
system	O
in	O
the	O
majority	O
of	O
languages	O
tested	O
.	O

Dependency	B-TaskName
parsing	I-TaskName
,	O
the	O
task	O
of	O
automatically	O
obtaining	O
the	O
grammatical	O
structure	O
of	O
a	O
sentence	O
expressed	O
as	O
a	O
dependency	O
tree	O
,	O
has	O
been	O
widely	O
studied	O
by	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
researchers	O
in	O
the	O
last	O
decades	O
.	O
Most	O
of	O
the	O
models	O
providing	O
competitive	O
accuracies	O
fall	O
into	O
two	O
broad	O
families	O
of	O
approaches	O
:	O
graph	O
-	O
based	O
(	O
Mc	O
-	O
Donald	O
et	O
al	O
,	O
2005a	O
,	O
b	O
)	O
and	O
transition	O
-	O
based	O
(	O
Yamada	O
and	O
Matsumoto	O
,	O
2003	O
;	O
Nivre	O
,	O
2003	O
)	O
dependency	O
parsers	O
.	O
Given	O
an	O
input	O
sentence	O
,	O
a	O
graph	O
-	O
based	O
parser	O
scores	O
trees	O
by	O
decomposing	O
them	O
into	O
factors	O
,	O
and	O
performs	O
a	O
search	O
for	O
the	O
highest	O
-	O
scoring	O
tree	O
.	O
In	O
the	O
past	O
two	O
years	O
,	O
this	O
kind	O
of	O
dependency	O
parsers	O
have	O
been	O
ahead	O
in	O
terms	O
of	O
accuracy	B-MetricName
thanks	O
to	O
the	O
graph	O
-	O
based	O
neural	O
architecture	O
developed	O
by	O
Dozat	O
and	O
Manning	O
(	O
2016	O
)	O
,	O
which	O
not	O
only	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracies	O
on	O
the	O
Stanford	O
Dependencies	O
conversion	O
of	O
the	O
English	O
Penn	B-DatasetName
Treebank	I-DatasetName
(	O
hereinafter	O
,	O
PTB	B-DatasetName
-	O
SD	O
)	O
,	O
but	O
also	O
obtained	O
the	O
best	O
results	O
in	O
the	O
majority	O
of	O
languages	O
in	O
the	O
CoNLL	O
2017	O
Shared	O
Task	O
(	O
Dozat	O
et	O
al	O
,	O
2017	O
)	O
.	O
This	O
tendency	O
recently	O
changed	O
,	O
since	O
a	O
transition	O
-	O
based	O
parser	O
developed	O
by	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
managed	O
to	O
outperform	O
the	O
best	O
graphbased	O
model	O
in	O
the	O
majority	O
of	O
datasets	O
tested	O
.	O
Transition	O
-	O
based	O
parsers	O
incrementally	O
build	O
a	O
dependency	O
graph	O
for	O
an	O
input	O
sentence	O
by	O
applying	O
a	O
sequence	O
of	O
transitions	O
.	O
This	O
results	O
in	O
more	O
efficient	O
parsers	O
with	O
linear	O
time	O
complexity	O
for	O
parsing	O
projective	O
sentences	O
,	O
or	O
quadratic	O
for	O
handling	O
non	O
-	O
projective	O
structures	O
,	O
when	O
implemented	O
with	O
greedy	O
or	O
beam	O
search	O
.	O
However	O
,	O
their	O
main	O
weakness	O
is	O
the	O
lack	O
of	O
access	O
to	O
global	O
context	O
information	O
when	O
transitions	O
are	O
greedily	O
chosen	O
.	O
This	O
favours	O
error	O
propagation	O
,	O
mainly	O
affecting	O
long	O
dependencies	O
that	O
require	O
a	O
larger	O
number	O
of	O
transitions	O
to	O
be	O
built	O
(	O
McDonald	O
and	O
Nivre	O
,	O
2011	O
)	O
.	O
Many	O
attempts	O
have	O
been	O
made	O
to	O
alleviate	O
the	O
impact	O
of	O
error	O
propagation	O
in	O
transition	B-TaskName
-	I-TaskName
based	I-TaskName
dependency	I-TaskName
parsing	I-TaskName
,	O
but	O
the	O
latest	O
and	O
most	O
successful	O
approach	O
was	O
developed	O
by	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
.	O
In	O
particular	O
,	O
they	O
make	O
use	O
of	O
pointer	O
networks	O
(	O
Vinyals	O
et	O
al	O
,	O
2015	O
)	O
to	O
implement	O
a	O
new	O
neural	O
network	O
architecture	O
called	O
stack	O
-	O
pointer	B-MethodName
network	I-MethodName
.	O
The	O
proposed	O
framework	O
provides	O
a	O
global	O
view	O
of	O
the	O
input	O
sentence	O
by	O
capturing	O
information	O
from	O
the	O
whole	O
sentence	O
and	O
all	O
the	O
arcs	O
previously	O
built	O
,	O
crucial	O
for	O
reducing	O
the	O
effect	O
of	O
error	O
propagation	O
;	O
and	O
,	O
thanks	O
to	O
an	O
attention	O
mechanism	O
(	O
Bahdanau	O
et	O
al	O
,	O
2014	O
;	O
Luong	O
et	O
al	O
,	O
2015	O
)	O
,	O
is	O
able	O
to	O
return	O
a	O
position	O
in	O
that	O
sentence	O
that	O
corresponds	O
to	O
a	O
word	O
related	O
to	O
the	O
word	O
currently	O
on	O
top	O
of	O
the	O
stack	O
.	O
They	O
take	O
advantage	O
of	O
this	O
and	O
propose	O
a	O
novel	O
transition	O
system	O
that	O
follows	O
a	O
top	O
-	O
down	O
depth	O
-	O
first	O
strategy	O
to	O
perform	O
the	O
syntactic	O
analysis	O
.	O
Concretely	O
,	O
it	O
considers	O
the	O
word	O
pointed	O
by	O
the	O
neural	O
network	O
as	O
the	O
child	O
of	O
the	O
word	O
on	O
top	O
of	O
the	O
stack	O
,	O
and	O
builds	O
the	O
corresponding	O
dependency	O
relation	O
between	O
them	O
.	O
This	O
results	O
in	O
a	O
transition	O
-	O
based	O
algorithm	O
that	O
can	O
process	O
unrestricted	O
non	O
-	O
projective	O
sentences	O
in	O
O	O
(	O
n	O
2	O
)	O
time	O
complexity	O
and	O
requires	O
2n	O
-	O
1	O
actions	O
to	O
successfully	O
parse	O
a	O
sentence	O
with	O
n	O
words	O
.	O
We	O
also	O
take	O
advantage	O
of	O
pointer	B-MethodName
network	I-MethodName
capabilities	O
and	O
use	O
the	O
neural	O
network	O
architecture	O
introduced	O
by	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
to	O
design	O
a	O
nonprojective	O
left	O
-	O
to	O
-	O
right	O
transition	O
-	O
based	O
algorithm	O
,	O
where	O
the	O
position	O
value	O
pointed	O
by	O
the	O
network	O
has	O
the	O
opposite	O
meaning	O
:	O
it	O
denotes	O
the	O
index	O
that	O
corresponds	O
to	O
the	O
head	O
node	O
of	O
the	O
current	O
focus	O
word	O
.	O
This	O
results	O
in	O
a	O
straightforward	O
transition	O
system	O
that	O
can	O
parse	O
a	O
sentence	O
in	O
just	O
n	O
actions	O
,	O
without	O
the	O
need	O
of	O
any	O
additional	O
data	O
structure	O
and	O
by	O
just	O
attaching	O
each	O
word	O
from	O
the	O
sentence	O
to	O
another	O
word	O
(	O
including	O
the	O
root	O
node	O
)	O
.	O
Apart	O
from	O
increasing	O
the	O
parsing	O
speed	O
twofold	O
(	O
while	O
keeping	O
the	O
same	O
quadratic	O
time	O
complexity	O
)	O
,	O
it	O
achieves	O
the	O
best	O
accuracy	B-MetricName
to	O
date	O
among	O
fully	O
-	O
supervised	O
single	O
-	O
model	O
dependency	O
parsers	O
on	O
the	O
PTB	B-DatasetName
-	O
SD	O
,	O
and	O
obtains	O
competitive	O
accuracies	O
on	O
twelve	O
different	O
languages	O
in	O
comparison	O
to	O
the	O
original	O
top	O
-	O
down	O
version	O
.	O
2	O
Preliminaries	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
propose	O
a	O
novel	O
neural	O
network	O
architecture	O
whose	O
main	O
backbone	O
is	O
a	O
pointer	B-MethodName
network	I-MethodName
(	O
Vinyals	O
et	O
al	O
,	O
2015	O
)	O
.	O
This	O
kind	O
of	O
neural	O
networks	O
are	O
able	O
to	O
learn	O
the	O
conditional	O
probability	O
of	O
a	O
sequence	O
of	O
discrete	O
numbers	O
that	O
correspond	O
to	O
positions	O
in	O
an	O
input	O
sequence	O
(	O
in	O
this	O
case	O
,	O
indexes	O
of	O
words	O
in	O
a	O
sentence	O
)	O
and	O
,	O
by	O
means	O
of	O
attention	O
(	O
Bahdanau	O
et	O
al	O
,	O
2014	O
;	O
Luong	O
et	O
al	O
,	O
2015	O
)	O
,	O
implement	O
a	O
pointer	O
that	O
selects	O
a	O
position	O
from	O
the	O
input	O
at	O
decoding	O
time	O
.	O
Their	O
approach	O
initially	O
reads	O
the	O
whole	O
sentence	O
,	O
composed	O
of	O
the	O
n	O
words	O
w	O
1	O
,	O
.	O
.	O
.	O
,	O
w	O
n	O
,	O
and	O
encodes	O
each	O
w	O
i	O
one	O
by	O
one	O
into	O
an	O
encoder	O
hidden	O
state	O
e	O
i	O
.	O
As	O
encoder	O
,	O
they	O
employ	O
a	O
combination	O
of	O
CNNs	O
and	O
bi	O
-	O
directional	O
LSTMs	O
(	O
Chiu	O
and	O
Nichols	O
,	O
2016	O
;	O
Ma	O
and	O
Hovy	O
,	O
2016	O
)	O
.	O
For	O
each	O
word	O
,	O
CNNs	O
are	O
used	O
to	O
obtain	O
its	O
character	O
-	O
level	O
representation	O
that	O
is	O
concatenated	O
to	O
the	O
word	O
and	O
PoS	O
embeddings	O
to	O
finally	O
be	O
fed	O
into	O
BiLSTMs	O
that	O
encode	O
word	O
context	O
information	O
.	O
As	O
decoder	O
they	O
present	O
a	O
top	O
-	O
down	O
transition	O
system	O
,	O
where	O
parsing	O
configurations	O
use	O
the	O
classic	O
data	O
structures	O
(	O
Nivre	O
,	O
2008	O
)	O
:	O
a	O
buffer	O
(	O
that	O
contains	O
unattached	O
words	O
)	O
and	O
a	O
stack	O
(	O
that	O
holds	O
partially	O
processed	O
words	O
)	O
.	O
The	O
available	O
parser	O
actions	O
are	O
two	O
transitions	O
that	O
we	O
call	O
Shift	O
-	O
Attach	O
-	O
p	O
and	O
Reduce	O
.	O
Given	O
a	O
configuration	O
with	O
word	O
w	O
i	O
on	O
top	O
of	O
the	O
stack	O
,	O
as	O
the	O
pointer	B-MethodName
network	I-MethodName
just	O
returns	O
a	O
position	O
p	O
from	O
a	O
given	O
sentence	O
,	O
they	O
proceed	O
as	O
follows	O
to	O
determine	O
which	O
transition	O
should	O
be	O
applied	O
:	O
If	O
p	O
=	O
i	O
,	O
then	O
the	O
pointed	O
word	O
w	O
p	O
is	O
considered	O
as	O
a	O
child	O
of	O
w	O
i	O
;	O
so	O
the	O
parser	O
chooses	O
a	O
Shift	O
-	O
Attach	O
-	O
p	O
transition	O
to	O
move	O
w	O
p	O
from	O
the	O
buffer	O
to	O
the	O
stack	O
and	O
build	O
an	O
arc	O
w	O
i	O
w	O
p	O
.	O
On	O
the	O
other	O
hand	O
,	O
if	O
p	O
=	O
i	O
,	O
then	O
w	O
i	O
is	O
considered	O
to	O
have	O
found	O
all	O
its	O
children	O
,	O
and	O
a	O
Reduce	O
transition	O
is	O
applied	O
to	O
pop	O
the	O
stack	O
.	O
The	O
parsing	O
process	O
starts	O
with	O
a	O
dummy	O
root	O
$	O
on	O
the	O
stack	O
and	O
,	O
by	O
applying	O
2n	O
-	O
1	O
transitions	O
,	O
a	O
dependency	O
tree	O
is	O
built	O
for	O
the	O
input	O
in	O
a	O
top	O
-	O
down	O
depth	O
-	O
first	O
fashion	O
,	O
where	O
multiple	O
children	O
of	O
a	O
same	O
word	O
are	O
forced	O
during	O
training	O
to	O
be	O
created	O
in	O
an	O
inside	O
-	O
out	O
manner	O
.	O
More	O
in	O
detail	O
,	O
for	O
each	O
parsing	O
configuration	O
c	O
t	O
,	O
the	O
decoder	O
(	O
implemented	O
as	O
a	O
uni	O
-	O
directional	O
LSTM	B-MethodName
)	O
receives	O
the	O
encoder	O
hidden	O
state	O
e	O
i	O
of	O
the	O
word	O
w	O
i	O
on	O
top	O
of	O
the	O
stack	O
to	O
generate	O
a	O
decoder	O
hidden	O
state	O
d	O
t	O
.	O
After	O
that	O
,	O
d	O
t	O
,	O
together	O
with	O
the	O
sequence	O
s	O
i	O
of	O
encoder	O
hidden	O
states	O
from	O
words	O
still	O
in	O
the	O
buffer	O
plus	O
e	O
i	O
,	O
are	O
used	O
to	O
compute	O
the	O
attention	O
vector	O
a	O
t	O
as	O
follows	O
:	O
v	O
t	O
i	O
=	O
score	O
(	O
d	O
t	O
,	O
s	O
i	O
)	O
(	O
1	O
)	O
a	O
t	O
=	O
sof	B-DatasetName
tmax	O
(	O
v	O
t	O
)	O
(	O
2	O
)	O
As	O
attention	O
scoring	O
function	O
(	O
score	O
(	O
)	O
)	O
,	O
they	O
adopt	O
the	O
biaffine	O
attention	O
mechanism	O
described	O
in	O
(	O
Luong	O
et	O
al	O
,	O
2015	O
;	O
Dozat	O
and	O
Manning	O
,	O
2016	O
)	O
.	O
Finally	O
,	O
the	O
attention	O
vector	O
a	O
t	O
will	O
be	O
used	O
to	O
return	O
the	O
highest	O
-	O
scoring	O
position	O
p	O
and	O
choose	O
the	O
next	O
transition	O
.	O
The	O
parsing	O
process	O
ends	O
when	O
only	O
the	O
root	O
remains	O
on	O
the	O
stack	O
.	O
As	O
extra	O
high	O
-	O
order	O
features	O
,	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
add	O
grandparent	O
and	O
sibling	O
information	O
,	O
whose	O
encoder	O
hidden	O
states	O
are	O
added	O
to	O
that	O
of	O
the	O
word	O
on	O
top	O
of	O
the	O
stack	O
to	O
generate	O
the	O
corresponding	O
decoder	O
hidden	O
state	O
d	O
t	O
.	O
They	O
prove	O
that	O
these	O
additions	O
improve	O
final	O
accuracy	B-MetricName
,	O
especially	O
when	O
children	O
are	O
attached	O
in	O
an	O
inside	O
-	O
out	O
fashion	O
.	O
According	O
to	O
the	O
authors	O
,	O
the	O
original	O
stackpointer	O
network	O
is	O
trained	O
to	O
maximize	O
the	O
likelihood	O
of	O
choosing	O
the	O
correct	O
word	O
for	O
each	O
possible	O
top	O
-	O
down	O
path	O
from	O
the	O
root	O
to	O
a	O
leaf	O
.	O
More	O
in	O
detail	O
,	O
a	O
dependency	O
tree	O
can	O
be	O
represented	O
as	O
a	O
sequence	O
of	O
top	O
-	O
down	O
paths	O
p	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
k	O
,	O
where	O
each	O
path	O
p	O
i	O
corresponds	O
to	O
a	O
sequence	O
of	O
words	O
$	O
,	O
w	O
i	O
,	O
1	O
,	O
w	O
i	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
w	O
i	O
,	O
l	O
i	O
from	O
the	O
root	O
to	O
a	O
leaf	O
.	O
Thus	O
,	O
the	O
conditional	O
probability	O
P	O
θ	B-HyperparameterName
(	O
y	O
|	O
x	O
)	O
of	O
the	O
dependency	O
tree	O
y	O
for	O
an	O
input	O
sentence	O
x	O
can	O
be	O
factorized	O
according	O
to	O
this	O
top	O
-	O
down	O
structure	O
as	O
:	O
P	O
θ	B-HyperparameterName
(	O
y	O
|	O
x	O
)	O
=	O
k	O
i=1	O
P	O
θ	B-HyperparameterName
(	O
p	O
i	O
|	O
p	O
<	O
i	O
,	O
x	O
)	O
=	O
k	O
i=1	O
l	O
i	O
j=1	O
P	O
θ	B-HyperparameterName
(	O
w	O
i	O
,	O
j	O
|	O
w	O
i	O
,	O
<	O
j	O
,	O
p	O
<	O
i	O
,	O
x	O
)	O
where	O
θ	B-HyperparameterName
represents	O
model	O
parameters	O
,	O
p	O
<	O
i	O
stands	O
for	O
previous	O
paths	O
already	O
explored	O
,	O
w	O
i	O
,	O
j	O
denotes	O
the	O
jth	O
word	O
in	O
path	O
p	O
i	O
and	O
w	O
i	O
,	O
<	O
j	O
represents	O
all	O
the	O
previous	O
words	O
on	O
p	O
i	O
.	O
For	O
more	O
thorough	O
details	O
of	O
the	O
stack	O
-	O
pointer	B-MethodName
network	I-MethodName
architecture	O
and	O
the	O
top	O
-	O
down	O
transition	O
system	O
,	O
please	O
read	O
the	O
original	O
work	O
by	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
.	O

We	O
take	O
advantage	O
of	O
the	O
neural	O
network	O
architecture	O
designed	O
by	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
and	O
introduce	O
a	O
simpler	O
left	O
-	O
to	O
-	O
right	O
transition	O
system	O
that	O
requires	O
neither	O
a	O
stack	O
nor	O
a	O
buffer	O
to	O
process	O
the	O
input	O
sentence	O
and	O
where	O
,	O
instead	O
of	O
selecting	O
a	O
child	O
of	O
the	O
word	O
on	O
top	O
of	O
the	O
stack	O
,	O
the	O
network	O
points	O
to	O
the	O
parent	O
of	O
the	O
current	O
focus	O
word	O
.	O
In	O
particular	O
,	O
in	O
our	O
proposed	O
approach	O
,	O
the	O
parsing	O
configuration	O
just	O
corresponds	O
to	O
a	O
focus	O
word	O
pointer	O
i	O
,	O
that	O
is	O
used	O
to	O
point	O
to	O
the	O
word	O
currently	O
being	O
processed	O
.	O
The	O
decoding	O
process	O
starts	O
with	O
i	O
pointing	O
at	O
the	O
first	O
word	O
of	O
the	O
sentence	O
and	O
,	O
at	O
each	O
parsing	O
configuration	O
,	O
only	O
one	O
action	O
is	O
available	O
:	O
the	O
parameterized	O
Attach	O
-	O
p	O
transition	O
,	O
that	O
links	O
the	O
focus	O
word	O
w	O
i	O
to	O
the	O
head	O
word	O
w	O
p	O
in	O
position	O
p	O
of	O
the	O
sentence	O
(	O
producing	O
the	O
dependency	O
arc	O
w	O
p	O
w	O
i	O
)	O
and	O
moves	O
i	O
one	O
position	O
to	O
the	O
right	O
.	O
Note	O
that	O
,	O
in	O
our	O
algorithm	O
,	O
p	O
can	O
equal	O
0	B-DatasetName
,	O
attaching	O
,	O
in	O
that	O
case	O
,	O
w	O
i	O
to	O
the	O
dummy	O
root	O
node	O
.	O
The	O
parsing	O
process	O
ends	O
when	O
the	O
last	O
word	O
from	O
the	O
sentence	O
is	O
attached	O
.	O
This	O
can	O
be	O
easily	O
represented	O
as	O
a	O
loop	O
that	O
traverses	O
the	O
input	O
sentence	O
from	O
left	O
to	O
right	O
,	O
linking	O
each	O
word	O
to	O
another	O
from	O
the	O
same	O
sentence	O
or	O
to	O
the	O
dummy	O
root	O
.	O
Therefore	O
,	O
we	O
just	O
need	O
n	O
steps	O
to	O
process	O
the	O
n	O
words	O
of	O
a	O
given	O
sentence	O
and	O
build	O
a	O
dependency	O
tree	O
.	O
While	O
our	O
novel	O
transition	O
system	O
intrinsically	O
holds	O
the	O
single	O
-	O
head	O
constraint	O
(	O
since	O
,	O
after	O
attaching	O
the	O
word	O
w	O
i	O
,	O
i	O
points	O
to	O
the	O
next	O
word	O
w	O
i+1	O
in	O
the	O
sentence	O
)	O
,	O
it	O
can	O
produce	O
an	O
output	O
with	O
cycles	O
.	O
1	O
Therefore	O
,	O
in	O
order	O
to	O
build	O
a	O
wellformed	O
dependency	O
tree	O
during	O
decoding	O
,	O
attachments	O
that	O
generate	O
cycles	O
in	O
the	O
already	O
-	O
built	O
dependency	O
graph	O
must	O
be	O
forbidden	O
.	O
Please	O
note	O
that	O
the	O
need	O
of	O
a	O
cycle	O
-	O
checking	O
extension	O
does	O
not	O
increase	O
the	O
overall	O
quadratic	O
runtime	O
complexity	O
of	O
the	O
original	O
implementation	O
by	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
since	O
,	O
as	O
in	O
other	O
transition	O
-	O
based	O
parsers	O
such	O
as	O
(	O
Covington	O
,	O
2001	O
;	O
Gómez	O
-	O
Rodríguez	O
and	O
Nivre	O
,	O
2010	O
)	O
,	O
cycles	O
can	O
be	O
incrementally	O
identified	O
in	O
amortized	O
constant	O
time	O
by	O
keeping	O
track	O
of	O
connected	O
components	O
using	O
path	O
compression	O
and	O
union	O
by	O
rank	O
.	O
Therefore	O
,	O
the	O
left	O
-	O
to	O
-	O
right	O
algorithm	O
requires	O
n	O
steps	O
to	O
produce	O
a	O
parse	O
.	O
In	O
addition	O
,	O
at	O
each	O
step	O
,	O
the	O
attention	O
vector	O
a	O
t	O
needs	O
to	O
be	O
computed	O
and	O
cycles	O
must	O
be	O
checked	O
,	O
both	O
in	O
O	O
(	O
n	O
)	O
+	O
O	O
(	O
n	O
)	O
=	O
O	O
(	O
n	O
)	O
runtime	O
.	O
This	O
results	O
in	O
a	O
O	O
(	O
n	O
2	O
)	O
time	O
complexity	O
for	O
decoding	O
.	O
2	O
On	O
the	O
other	O
hand	O
,	O
while	O
in	O
the	O
top	O
-	O
down	O
decoding	O
only	O
available	O
words	O
in	O
the	O
buffer	O
(	O
plus	O
the	O
word	O
on	O
top	O
of	O
the	O
stack	O
)	O
can	O
be	O
pointed	O
to	O
by	O
the	O
network	O
and	O
they	O
are	O
reduced	O
as	O
arcs	O
are	O
created	O
(	O
basically	O
to	O
keep	O
the	O
single	O
-	O
head	O
constraint	O
)	O
;	O
our	O
proposed	O
approach	O
is	O
less	O
rigid	O
:	O
all	O
words	O
from	O
the	O
sentence	O
(	O
including	O
the	O
root	O
node	O
and	O
excluding	O
w	O
i	O
)	O
can	O
be	O
pointed	O
to	O
,	O
as	O
long	O
as	O
they	O
satisfy	O
the	O
acyclicity	O
constraint	O
.	O
This	O
is	O
necessary	O
because	O
two	O
different	O
words	O
might	O
be	O
attached	O
to	O
the	O
same	O
head	O
node	O
and	O
the	O
latter	O
can	O
be	O
located	O
in	O
the	O
sentence	O
either	O
before	O
or	O
after	O
w	O
i	O
.	O
Therefore	O
,	O
the	O
sequence	O
s	O
i	O
,	O
required	O
by	O
the	O
attention	O
score	O
function	O
(	O
Eq	O
.	O
(	O
1	O
)	O
)	O
,	O
is	O
composed	O
of	O
the	O
encoder	O
hidden	O
states	O
of	O
all	O
words	O
from	O
the	O
input	O
,	O
excluding	O
e	O
i	O
,	O
and	O
prepending	O
a	O
special	O
vector	O
representation	O
denoting	O
the	O
root	O
node	O
.	O
We	O
also	O
add	O
extra	O
features	O
to	O
represent	O
the	O
current	O
focus	O
word	O
.	O
Instead	O
of	O
using	O
grandparent	O
and	O
sibling	O
information	O
(	O
more	O
beneficial	O
for	O
a	O
topdown	O
approach	O
)	O
,	O
we	O
just	O
add	O
the	O
encoder	O
hidden	O
states	O
of	O
the	O
previous	O
and	O
next	O
words	O
in	O
the	O
sentence	O
to	O
generate	O
d	O
t	O
,	O
which	O
seems	O
to	O
be	O
more	O
suitable	O
for	O
a	O
left	O
-	O
to	O
-	O
right	O
decoding	O
.	O
In	O
dependency	B-TaskName
parsing	I-TaskName
,	O
a	O
tree	O
for	O
an	O
input	O
sentence	O
of	O
length	O
n	O
can	O
be	O
represented	O
as	O
a	O
set	O
of	O
n	O
directed	O
and	O
binary	O
links	O
l	O
1	O
,	O
.	O
.	O
.	O
,	O
l	O
n	O
.	O
Each	O
link	O
l	O
i	O
is	O
characterized	O
by	O
the	O
word	O
w	O
i	O
in	O
position	O
i	O
in	O
the	O
sentence	O
and	O
its	O
head	O
word	O
w	O
h	O
,	O
resulting	O
in	O
a	O
pair	O
(	O
w	O
i	O
,	O
w	O
h	O
)	O
.	O
Therefore	O
,	O
to	O
train	O
this	O
novel	O
variant	O
,	O
we	O
factorize	O
the	O
conditional	O
probability	O
P	O
θ	B-HyperparameterName
(	O
y	O
|	O
x	O
)	O
to	O
a	O
set	O
of	O
head	O
-	O
dependent	O
pairs	O
as	O
follows	O
:	O
P	O
θ	B-HyperparameterName
(	O
y	O
|	O
x	O
)	O
=	O
n	O
i=1	O
P	O
θ	B-HyperparameterName
(	O
l	O
i	O
|	O
l	O
<	O
i	O
,	O
x	O
)	O
=	O
n	O
i=1	O
P	O
θ	B-HyperparameterName
(	O
w	O
h	O
|	O
w	O
i	O
,	O
l	O
<	O
i	O
,	O
x	O
)	O
Therefore	O
,	O
the	O
left	O
-	O
to	O
-	O
right	O
parser	O
is	O
trained	O
by	O
maximizing	O
the	O
likelihood	O
of	O
choosing	O
the	O
correct	O
head	O
word	O
w	O
h	O
for	O
the	O
word	O
w	O
i	O
in	O
position	O
i	O
,	O
given	O
the	O
previous	O
predicted	O
links	O
l	O
<	O
i	O
.	O
Finally	O
,	O
following	O
a	O
widely	O
-	O
used	O
approach	O
(	O
also	O
implemented	O
in	O
(	O
Ma	O
et	O
al	O
,	O
2018	O
)	O
)	O
,	O
dependency	O
labels	O
are	O
predicted	O
by	O
a	O
multiclass	O
classifier	O
,	O
which	O
is	O
trained	O
in	O
parallel	O
with	O
the	O
parser	O
by	O
optimizing	O
the	O
sum	O
of	O
their	O
objectives	O
.	O

UAS	O
LAS	O
Chen	O
and	O
Manning	O
(	O
2014	O
)	O
91.8	O
89.6	O
Dyer	O
et	O
al	O
(	O
2015	O
)	O
93.1	O
90.9	O
93.99	O
92.05	O
93.56	O
91.42	O
Kiperwasser	O
and	O
Goldberg	O
(	O
2016	O
)	O
93.9	O
91.9	O
94.23	O
92.36	O
94.3	O
92.2	O
Fernández	O
-	O
G	O
and	O
Gómez	O
-	O
R	O
(	O
2018	O
)	O
Systems	O
marked	O
with	O
*	O
,	O
including	O
the	O
improved	O
variant	O
described	O
in	O
(	O
Ma	O
et	O
al	O
,	O
2018	O
)	O
of	O
the	O
graph	O
-	O
based	O
parser	O
by	O
(	O
Dozat	O
and	O
Manning	O
,	O
2016	O
)	O
,	O
are	O
implemented	O
under	O
the	O
same	O
framework	O
as	O
our	O
approach	O
and	O
use	O
the	O
same	O
training	O
settings	O
.	O
Like	O
(	O
Ma	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
report	O
the	O
average	B-MetricName
accuracy	I-MetricName
over	O
5	O
repetitions	O
.	O
Finally	O
,	O
we	O
use	O
the	O
same	O
hyper	O
-	O
parameter	O
values	O
,	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
and	O
beam	O
size	O
(	O
10	O
for	O
PTB	B-DatasetName
-	O
SD	O
and	O
5	O
for	O
UD	B-DatasetName
)	O
as	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
.	O

There	O
is	O
previous	O
work	O
that	O
proposes	O
to	O
implement	O
dependency	B-TaskName
parsing	I-TaskName
by	O
independently	O
selecting	O
the	O
head	O
of	O
each	O
word	O
in	O
a	O
sentence	O
,	O
using	O
neural	O
networks	O
.	O
In	O
particular	O
,	O
Zhang	O
et	O
al	O
(	O
2017	O
)	O
make	O
use	O
of	O
a	O
BiLSTM	B-MethodName
-	O
based	O
neural	O
architecture	O
to	O
compute	O
the	O
probability	O
of	O
attaching	O
each	O
word	O
to	O
one	O
of	O
the	O
other	O
input	O
words	O
,	O
in	O
a	O
similar	O
way	O
as	O
pointer	O
networks	O
do	O
.	O
During	O
decoding	O
,	O
a	O
postprocessing	O
step	O
is	O
needed	O
to	O
produce	O
well	O
-	O
formed	O
trees	O
by	O
means	O
of	O
a	O
maximum	O
spanning	O
tree	O
algorithm	O
.	O
Our	O
approach	O
does	O
not	O
need	O
this	O
postprocessing	O
,	O
as	O
cycles	O
are	O
forbidden	O
during	O
parsing	O
instead	O
,	O
and	O
achieves	O
a	O
higher	O
accuracy	B-MetricName
thanks	O
to	O
the	O
pointer	B-MethodName
network	I-MethodName
architecture	O
and	O
the	O
use	O
of	O
information	O
about	O
previous	O
dependencies	O
.	O
Before	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
presented	O
their	O
topdown	O
parser	O
,	O
Chorowski	O
et	O
al	O
(	O
2017	O
)	O
had	O
already	O
employed	O
pointer	O
networks	O
(	O
Vinyals	O
et	O
al	O
,	O
2015	O
)	O
for	O
dependency	B-TaskName
parsing	I-TaskName
.	O
Concretely	O
,	O
they	O
developed	O
a	O
pointer	O
-	O
network	O
-	O
based	O
neural	O
architecture	O
with	O
multitask	O
learning	O
able	O
to	O
perform	O
preprocessing	O
,	O
tagging	O
and	O
dependency	B-TaskName
parsing	I-TaskName
exclusively	O
by	O
reading	O
tokens	O
from	O
an	O
input	O
sen	O
-	O
tence	O
,	O
without	O
needing	O
POS	O
tags	O
or	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
.	O
Like	O
our	O
approach	O
,	O
they	O
also	O
use	O
the	O
capabilities	O
provided	O
by	O
pointer	O
networks	O
to	O
undertake	O
the	O
parsing	O
task	O
as	O
a	O
simple	O
process	O
of	O
attaching	O
each	O
word	O
as	O
dependent	O
of	O
another	O
.	O
They	O
also	O
try	O
to	O
improve	O
the	O
network	O
performance	O
with	O
POS	O
tag	O
prediction	O
as	O
auxiliary	O
task	O
and	O
with	O
different	O
approaches	O
to	O
perform	O
label	O
prediction	O
.	O
They	O
do	O
not	O
exclude	O
cycles	O
,	O
neither	O
by	O
forbidding	O
them	O
at	O
parsing	O
time	O
or	O
by	O
removing	O
them	O
by	O
post	O
-	O
processing	O
,	O
as	O
they	O
report	O
that	O
their	O
system	O
produces	O
parses	O
with	O
a	O
negligible	O
amount	O
of	O
cycles	O
,	O
even	O
with	O
greedy	O
decoding	O
(	O
matching	O
our	O
observation	O
for	O
our	O
own	O
system	O
,	O
in	O
our	O
case	O
with	O
beam	O
-	O
search	O
decoding	O
)	O
.	O
Finally	O
,	O
the	O
system	O
developed	O
by	O
Chorowski	O
et	O
al	O
(	O
2017	O
)	O
is	O
constrained	O
to	O
projective	O
dependencies	O
,	O
while	O
our	O
approach	O
can	O
handle	O
unrestricted	O
non	O
-	O
projective	O
structures	O
.	O

We	O
present	O
a	O
novel	O
left	O
-	O
to	O
-	O
right	O
dependency	O
parser	O
based	O
on	O
pointer	O
networks	O
.	O
We	O
follow	O
the	O
same	O
neural	O
network	O
architecture	O
as	O
the	O
stack	O
-	O
pointerbased	O
approach	O
developed	O
by	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
,	O
but	O
just	O
using	O
a	O
focus	O
word	O
index	O
instead	O
of	O
a	O
buffer	O
and	O
a	O
stack	O
.	O
Apart	O
from	O
doubling	O
their	O
system	O
's	O
speed	O
,	O
our	O
approach	O
proves	O
to	O
be	O
a	O
competitive	O
alternative	O
on	O
a	O
variety	O
of	O
languages	O
and	O
achieves	O
the	O
best	O
accuracy	B-MetricName
to	O
date	O
on	O
the	O
PTB	B-DatasetName
-	O
SD	O
.	O
The	O
good	O
performance	O
of	O
our	O
algorithm	O
can	O
be	O
explained	O
by	O
the	O
shortening	O
of	O
the	O
transition	O
sequence	O
length	O
.	O
In	O
fact	O
,	O
it	O
has	O
been	O
proved	O
by	O
several	O
studies	O
(	O
Fernández	O
-	O
González	O
and	O
Gómez	O
-	O
Rodríguez	O
,	O
2012	O
;	O
Fernández	O
-	O
González	O
and	O
Gómez	O
-	O
Rodríguez	O
,	O
2018	O
)	O
that	O
by	O
reducing	O
the	O
number	O
of	O
applied	O
transitions	O
,	O
the	O
impact	O
of	O
error	O
propagation	O
is	O
alleviated	O
,	O
yielding	O
more	O
accurate	O
parsers	O
.	O
Our	O
system	O
's	O
source	O
code	O
is	O
freely	O
available	O
at	O
https://github.com/danifg/	O
Left2Right	O
-	O
Pointer	O
-	O
Parser	O
.	O

Recent	O
events	O
such	O
as	O
the	O
last	O
two	O
U.S.	O
presidential	O
elections	O
have	O
been	O
greatly	O
affected	O
by	O
fake	O
news	O
,	O
defined	O
as	O
"	O
fabricated	O
information	O
that	O
disseminates	O
deceptive	O
content	O
,	O
or	O
grossly	O
distort	O
actual	O
news	O
reports	O
,	O
shared	O
on	O
social	O
media	O
platforms	O
"	O
(	O
Allcott	O
and	O
Gentzkow	O
,	O
2017	O
)	O
.	O
In	O
fact	O
,	O
the	O
World	O
Economic	O
Forum	O
2013	O
report	O
designates	O
massive	O
digital	O
misinformation	O
as	O
a	O
major	O
technological	O
and	O
geopolitical	O
risk	O
(	O
Bovet	O
and	O
Makse	O
,	O
2019	O
)	O
.	O
As	O
daily	O
social	O
media	O
usage	O
increases	O
(	O
Statista	O
Research	O
Department	O
,	O
2021	O
)	O
,	O
manual	O
fact	O
-	O
checking	O
can	O
not	O
keep	O
up	O
with	O
this	O
deluge	O
of	O
information	O
.	O
Automatic	O
fact	O
-	O
checking	O
models	O
are	O
therefore	O
a	O
necessity	O
,	O
and	O
most	O
of	O
them	O
function	O
using	O
a	O
system	O
of	O
claims	O
and	O
evidence	O
(	O
Hassan	O
et	O
al	O
,	O
2017	O
)	O
.	O
Given	O
a	O
specific	O
claim	O
,	O
the	O
models	O
use	O
external	O
knowledge	O
as	O
evidence	O
.	O
Typically	O
,	O
a	O
web	O
search	O
query	O
is	O
treated	O
as	O
the	O
claim	O
,	O
and	O
a	O
subset	O
of	O
the	O
top	O
search	O
results	O
is	O
treated	O
as	O
the	O
evidence	O
.	O
There	O
is	O
an	O
implicit	O
assumption	O
that	O
the	O
fact	O
-	O
checking	O
models	O
are	O
reasoning	O
in	O
some	O
way	O
,	O
using	O
the	O
evidence	O
to	O
confirm	O
or	O
refute	O
the	O
claim	O
.	O
Recent	O
research	O
(	O
Hansen	O
et	O
al	O
,	O
2021	O
)	O
found	O
this	O
conclusion	O
may	O
be	O
premature	O
;	O
current	O
models	O
can	O
show	O
improved	O
performance	O
when	O
considering	O
evidence	O
alone	O
,	O
essentially	O
fact	O
-	O
checking	O
an	O
unasked	O
question	O
.	O
While	O
this	O
might	O
seem	O
reasonable	O
given	O
that	O
the	O
evidence	O
is	O
conditioned	O
on	O
the	O
claims	O
by	O
the	O
search	O
engine	O
,	O
this	O
can	O
be	O
exploited	O
as	O
illustrated	O
in	O
Figure	O
1	O
,	O
which	O
shows	O
that	O
evidence	O
returned	O
using	O
a	O
ridiculous	O
claim	O
can	O
still	O
appear	O
reasonable	O
if	O
we	O
view	O
the	O
evidence	O
alone	O
without	O
the	O
claim	O
.	O
Furthermore	O
,	O
textual	O
entailment	O
requires	O
both	O
a	O
text	O
and	O
a	O
hypothesis	O
;	O
if	O
we	O
have	O
a	O
result	O
without	O
a	O
hypothesis	O
,	O
we	O
are	O
performing	O
a	O
different	O
,	O
unknown	O
task	O
.	O
This	O
finding	O
indicates	O
a	O
problem	O
with	O
current	O
automatic	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
,	O
signaling	O
that	O
the	O
models	O
rely	O
on	O
features	O
in	O
the	O
evidence	O
typical	O
to	O
fake	O
news	O
,	O
rather	O
than	O
using	O
entailment	O
.	O
Since	O
most	O
automated	O
fact	O
-	O
checking	O
research	O
is	O
primarily	O
concerned	O
with	O
the	O
accuracy	B-MetricName
of	O
the	O
results	O
,	O
rather	O
than	O
addressing	O
how	O
the	O
results	O
are	O
achieved	O
,	O
we	O
propose	O
a	O
novel	O
investigation	O
into	O
these	O
models	O
and	O
their	O
evidence	O
.	O
We	O
use	O
a	O
variety	O
of	O
pre	O
-	O
processing	O
steps	O
,	O
including	O
neural	O
and	O
non	O
-	O
neural	O
ones	O
,	O
to	O
attempt	O
to	O
reduce	O
the	O
affectations	O
common	O
in	O
evidence	O
:	O
Stemming	O
,	O
stopword	O
removal	O
,	O
negation	O
,	O
and	O
POS	O
-	O
filtering	O
(	O
Babanejad	O
et	O
al	O
,	O
2020	O
)	O
.	O
Style	B-TaskName
transfer	I-TaskName
neural	O
models	O
using	O
the	O
Styleformer	O
model	O
to	O
perform	O
informal	O
-	O
to	O
-	O
formal	O
and	O
formal	O
-	O
to	O
-	O
informal	O
paraphrasing	O
methods	O
(	O
Li	O
et	O
al	O
,	O
2018	O
;	O
Schmidt	O
,	O
2020	O
)	O
.	O
We	O
also	O
develop	O
our	O
own	O
BERT	B-MethodName
-	O
based	O
model	O
as	O
an	O
extension	O
of	O
the	O
EmoCred	O
system	O
(	O
Giachanou	O
Figure	O
1	O
:	O
An	O
example	O
of	O
why	O
evidence	O
alone	O
does	O
not	O
suffice	O
in	O
identifying	O
fake	O
news	O
,	O
despite	O
the	O
evidence	O
being	O
conditioned	O
on	O
the	O
claim	O
as	O
a	O
search	O
-	O
engine	O
query	O
.	O
Although	O
the	O
returned	O
evidence	O
appearing	O
reputable	O
,	O
it	O
is	O
clear	O
that	O
it	O
has	O
little	O
relevance	O
to	O
deciding	O
the	O
veracity	O
of	O
the	O
claim	O
that	O
"	O
all	O
Canadians	O
have	O
eaten	O
at	O
least	O
one	O
bear	O
.	O
"	O
et	O
al	O
,	O
2019	O
)	O
,	O
adding	O
an	O
"	O
emotional	O
attention	O
"	O
layer	O
to	O
weight	O
the	O
most	O
relevant	O
emotional	O
signals	O
in	O
a	O
given	O
evidence	O
snippet	O
.	O
We	O
make	O
our	O
code	O
publicly	O
available	O
.	O
1	O
With	O
each	O
of	O
these	O
methods	O
,	O
we	O
focus	O
on	O
scores	O
where	O
the	O
models	O
perform	O
better	O
using	O
both	O
the	O
claims	O
and	O
the	O
evidence	O
combined	O
,	O
S	O
C&E	O
,	O
rather	O
than	O
with	O
the	O
evidence	O
alone	O
,	O
S	O
E	O
.	O
Going	O
forward	O
,	O
we	O
will	O
refer	O
to	O
the	O
difference	O
between	O
these	O
dataset	O
combinations	O
as	O
the	O
delta	O
of	O
the	O
pre	O
-	O
processing	O
step	O
,	O
where	O
delta	O
=	O
S	O
C&E	O
−	O
S	O
E	O
.	O
A	O
positive	O
delta	O
score	O
indicates	O
that	O
the	O
claim	O
was	O
useful	O
and	O
helped	O
yield	O
an	O
increase	O
in	O
performance	O
.	O
Since	O
we	O
are	O
removing	O
indicators	O
that	O
the	O
current	O
models	O
rely	O
on	O
,	O
some	O
of	O
the	O
models	O
perform	O
worse	O
at	O
the	O
task	O
than	O
they	O
did	O
previously	O
.	O
However	O
,	O
a	O
surprising	O
result	O
is	O
that	O
many	O
improved	O
,	O
and	O
the	O
need	O
to	O
consider	O
the	O
claim	O
and	O
the	O
evidence	O
together	O
is	O
a	O
sign	O
of	O
using	O
reasoning	O
rather	O
than	O
manipulable	O
indicators	O
.	O
Under	O
current	O
fact	O
-	O
checking	O
models	O
,	O
adversarial	O
data	O
can	O
subvert	O
these	O
detectors	O
.	O
Paraphrasing	O
can	O
be	O
performed	O
by	O
inserting	O
fictitious	O
statements	O
into	O
otherwise	O
truthful	O
evidence	O
with	O
little	O
effect	O
on	O
the	O
model	O
's	O
output	O
.	O
For	O
example	O
,	O
an	O
article	O
titled	O
"	O
Is	O
the	O
GOP	O
losing	O
Walmart	O
?	O
"	O
,	O
could	O
have	O
"	O
Walmart	O
"	O
substituted	O
with	O
"	O
Apple	O
,	O
"	O
and	O
the	O
predictions	O
are	O
nearly	O
identical	O
despite	O
the	O
news	O
now	O
being	O
fictitious	O
(	O
Zhou	O
et	O
al	O
,	O
2019	O
)	O
.	O
1	O

The	O
EmoCred	O
systems	O
of	O
EmoLexi	O
and	O
EmoInt	O
use	O
a	O
lexicon	O
to	O
determine	O
emotional	O
word	O
counts	O
and	O
intensities	O
,	O
respectively	O
(	O
Giachanou	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
use	O
the	O
NRC	O
Affect	O
Intensity	O
Lexicon	O
,	O
a	O
"	O
highcoverage	O
lexicons	O
that	O
captures	O
word	O
-	O
affect	O
intensities	O
"	O
for	O
eight	O
basic	O
emotions	O
,	O
which	O
were	O
created	O
using	O
a	O
technique	O
called	O
best	O
-	O
worst	O
scaling	O
(	O
Mohammad	O
,	O
2017	O
)	O
.	O
These	O
eight	O
emotions	O
can	O
be	O
used	O
to	O
create	O
an	O
emotion	B-DatasetName
vector	O
for	O
a	O
sentence	O
,	O
where	O
each	O
index	O
corresponds	O
to	O
a	O
score	O
:	O
[	O
anger	O
,	O
anticipation	O
,	O
disgust	O
,	O
fear	O
,	O
joy	O
,	O
sadness	O
,	O
surprise	O
,	O
trust	O
]	O
.	O
As	O
an	O
example	O
,	O
a	O
sentence	O
that	O
contains	O
the	O
word	O
"	O
suffering	O
"	O
conveys	O
sadness	O
with	O
an	O
NRC	O
Affect	O
Intensity	O
Lexicon	O
intensity	O
of	O
0.844	O
,	O
whereas	O
the	O
word	O
"	O
affection	O
"	O
indicates	O
joy	O
with	O
an	O
intensity	O
of	O
0.647	O
.	O
We	O
create	O
the	O
vector	O
of	O
length	O
eight	O
,	O
and	O
for	O
each	O
word	O
associated	O
with	O
an	O
emotion	B-DatasetName
,	O
the	O
emotion	B-DatasetName
's	O
indexed	O
value	O
is	O
either	O
:	O
(	O
1	O
)	O
incremented	O
by	O
one	O
for	O
EmoLexi	O
;	O
or	O
,	O
(	O
2	O
)	O
incremented	O
by	O
its	O
intensity	O
for	O
EmoInt	O
.	O
Thus	O
,	O
the	O
sentence	O
"	O
He	O
had	O
an	O
affection	O
for	O
suffering	O
"	O
would	O
have	O
an	O
EmoLexi	O
emotion	B-DatasetName
vector	O
of	O
[	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
1	O
,	O
1	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
and	O
an	O
EmoInt	O
emotion	B-DatasetName
vector	O
of	O
[	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0	B-DatasetName
,	O
0.647	O
,	O
0.844	O
,	O
0	B-DatasetName
,	O
0	B-DatasetName
]	O
We	O
build	O
on	O
this	O
EmoCred	O
framework	O
,	O
adding	O
an	O
attention	O
system	O
for	O
emotion	B-DatasetName
that	O
gives	O
a	O
weight	O
to	O
each	O
emotion	B-DatasetName
vector	O
,	O
just	O
as	O
the	O
attention	O
layer	O
for	O
each	O
snippet	O
gives	O
a	O
weight	O
to	O
each	O
snippet	O
.	O
The	O
end	O
result	O
is	O
that	O
two	O
independent	O
attention	B-HyperparameterName
layers	I-HyperparameterName
attend	O
to	O
the	O
ten	O
snippets	O
and	O
ten	O
emotional	O
representations	O
independently	O
,	O
and	O
we	O
call	O
the	O
resulting	O
system	O
Emotional	O
Attention	O
(	O
see	O
Figure	O
3	O
)	O
.	O

Surprisingly	O
,	O
the	O
four	O
top	O
-	O
performing	O
models	O
with	O
the	O
Snopes	B-DatasetName
dataset	O
include	O
two	O
non	O
-	O
neural	O
models	O
and	O
two	O
neural	O
models	O
.	O
All	O
four	O
achieve	O
greater	O
F1	B-MetricName
Macro	I-MetricName
scores	O
than	O
the	O
baseline	O
BERT	B-MethodName
model	O
without	O
pre	O
-	O
processing	O
(	O
see	O
Figure	O
2	O
)	O
.	O
POS	O
and	O
STOP	O
yield	O
the	O
biggest	O
delta	O
between	O
S	O
C&E	O
vs.	O
S	O
E	O
,	O
followed	O
by	O
EmoInt	O
and	O
Informal	O
Style	B-TaskName
Transfer	I-TaskName
.	O
However	O
,	O
EmoInt	O
yields	O
the	O
highest	O
F1	B-MetricName
Macro	I-MetricName
,	O
followed	O
by	O
POS	O
,	O
Informal	O
,	O
and	O
STOP	O
.	O
In	O
PolitiFact	B-DatasetName
,	O
none	O
of	O
the	O
pre	O
-	O
processing	O
steps	O
achieve	O
a	O
delta	O
greater	O
than	O
zero	O
for	O
S	O
C&E	O
versus	O
S	O
E	O
.	O
The	O
combination	O
of	O
POS+STOP	O
steps	O
come	O
closest	O
to	O
parity	O
,	O
followed	O
by	O
EmoInt	O
,	O
then	O
POS	O
and	O
STOP	O
.	O
For	O
the	O
best	O
F1	B-MetricName
Macro	I-MetricName
scores	O
overall	O
,	O
EmoAttention	O
's	O
two	O
forms	O
(	O
i.e.	O
,	O
EmoInt	O
and	O
EmoLexi	O
)	O
were	O
the	O
two	O
best	O
,	O
followed	O
by	O
STOP	O
Macro	O
scores	O
and	O
deltas	O
are	O
in	O
red	O
.	O
With	O
the	O
exception	O
of	O
EmoLexi	O
tying	O
for	O
the	O
lowest	O
delta	O
,	O
the	O
best	O
pre	O
-	O
processing	O
steps	O
outperform	O
the	O
baseline	O
BERT	B-MethodName
model	O
from	O
Hansen	O
et	O
al	O
(	O
2021	O
)	O
.	O
and	O
POS	O
.	O
All	O
of	O
these	O
pre	O
-	O
processing	O
steps	O
achieve	O
higher	O
F1	B-MetricName
Macro	I-MetricName
scores	O
than	O
the	O
baseline	O
BERT	B-MethodName
model	O
.	O
Further	O
,	O
they	O
yield	O
better	O
deltas	O
for	O
S	O
C&E	O
versus	O
S	O
E	O
,	O
implying	O
that	O
the	O
model	O
now	O
requires	O
the	O
claims	O
to	O
reason	O
.	O

Many	O
pre	O
-	O
processing	O
steps	O
increase	O
both	O
the	O
model	O
's	O
F1	B-MetricName
scores	O
and	O
its	O
need	O
for	O
claims	O
and	O
evidence	O
,	O
validating	O
our	O
hypothesis	O
that	O
signals	O
in	O
style	O
and	O
tone	O
have	O
become	O
a	O
crutch	O
for	O
factchecking	O
models	O
.	O
Rather	O
than	O
doing	O
entailment	O
,	O
they	O
are	O
leveraging	O
other	O
signals	O
-	O
perhaps	O
similar	O
to	O
sentiment	B-TaskName
analysis	I-TaskName
-	O
and	O
relying	O
on	O
a	O
"	O
gut	O
feeling	O
"	O
.	O
EmoAttention	O
generates	O
our	O
best	O
predictions	O
and	O
deltas	O
,	O
confirming	O
our	O
suspicion	O
that	O
the	O
models	O
rely	O
on	O
emotionally	O
charged	O
style	O
as	O
a	O
predictive	O
feature	O
.	O
This	O
is	O
further	O
narrowed	O
to	O
emotional	O
intensity	O
:	O
the	O
EmoInt	O
intensity	O
score	O
-	O
based	O
model	O
performs	O
much	O
better	O
than	O
its	O
count	O
-	O
based	O
counterpart	O
EmoLexi	O
.	O
Thus	O
,	O
evidence	O
containing	O
emotions	O
associated	O
with	O
fake	O
news	O
will	O
be	O
considered	O
more	O
when	O
scoring	O
the	O
claim	O
.	O
One	O
surprising	O
result	O
is	O
the	O
effectiveness	O
of	O
the	O
simple	O
POS	O
and	O
STOP	O
pre	O
-	O
processing	O
steps	O
.	O
POS	O
only	O
included	O
nouns	O
,	O
verbs	O
,	O
and	O
adjectives	O
(	O
i.e.	O
,	O
a	O
superset	O
of	O
STOP	O
)	O
.	O
This	O
could	O
explain	O
why	O
it	O
has	O
the	O
best	O
delta	O
between	O
S	O
C&E	O
vs.	O
S	O
E	O
.	O
Future	O
research	O
could	O
investigate	O
if	O
stopwords	O
,	O
which	O
are	O
often	O
discarded	O
,	O
actually	O
contain	O
signals	O
such	O
as	O
anaphora	O
:	O
a	O
repetitive	O
rhetoric	O
style	O
which	O
can	O
affect	O
NLP	O
analyses	O
(	O
Liddy	O
,	O
1990	O
)	O
.	O
As	O
an	O
example	O
,	O
Donald	O
Trump	O
makes	O
heavy	O
use	O
of	O
anaphora	O
in	O
his	O
2017	O
inauguration	O
speech	O
:	O
"	O
Together	O
,	O
we	O
will	O
make	O
America	O
strong	O
again	O
.	O
We	O
will	O
make	O
America	O
wealthy	O
again	O
.	O
We	O
will	O
make	O
America	O
proud	O
again	O
.	O
We	O
will	O
make	O
America	O
safe	O
again	O
.	O
And	O
,	O
yes	O
,	O
together	O
,	O
we	O
will	O
make	O
america	O
great	O
again	O
.	O
"	O
(	O
Trump	O
Inauguration	O
Address	O
,	O
2017	O
)	O
By	O
removing	O
stopwords	O
"	O
we	O
"	O
,	O
"	O
will	O
"	O
and	O
"	O
again	O
"	O
,	O
the	O
model	O
relies	O
less	O
on	O
the	O
text	O
's	O
rhetoric	O
style	O
and	O
more	O
on	O
the	O
entailment	O
we	O
are	O
seeking	O
.	O
We	O
propose	O
further	O
study	O
on	O
the	O
effects	O
of	O
STOP	O
and	O
POS	O
,	O
as	O
well	O
as	O
experimenting	O
with	O
different	O
emotional	O
vectors	O
and	O
EmoAttention	O
to	O
make	O
factchecking	O
models	O
more	O
robust	O
.	O
Automatic	O
Fake	B-TaskName
News	I-TaskName
detection	I-TaskName
remains	O
a	O
challenging	O
problem	O
,	O
and	O
unfortunately	O
,	O
current	O
fact	O
-	O
checking	O
models	O
can	O
be	O
subverted	O
by	O
adversarial	O
techniques	O
that	O
exploit	O
emotionally	O
charged	O
writing	O
.	O

We	O
trained	O
the	O
models	O
on	O
transcriptions	O
of	O
childdirected	O
speech	O
,	O
i.e.	O
samples	O
of	O
naturalistic	O
productions	O
in	O
the	O
linguistic	O
environment	O
of	O
a	O
child	O
.	O
We	O
extracted	O
the	O
child	O
-	O
directed	O
speech	O
data	O
from	O
the	O
CHILDES	O
database	O
(	O
MacWhinney	O
,	O
2000	O
)	O
,	O
for	O
all	O
the	O
varieties	O
of	O
English	O
,	O
for	O
ages	O
ranging	O
from	O
0	B-DatasetName
to	O
60	O
months	O
.	O
We	O
used	O
the	O
childesr	O
library	O
to	O
extract	O
the	O
child	O
-	O
directed	O
utterances	O
(	O
Sanchez	O
et	O
al	O
,	O
2019	O
)	O
2	O
.	O
Word	O
tokens	O
were	O
coded	O
at	O
the	O
lemma	B-DatasetName
level	O
.	O
The	O
resulting	O
dataset	O
contains	O
a	O
total	O
number	O
of	O
3	O
,	O
135	O
,	O
822	O
sentences	O
,	O
34	O
,	O
961	O
word	O
types	O
,	O
and	O
12	O
,	O
975	O
,	O
520	O
word	O
tokens	O
.	O
To	O
evaluate	O
the	O
models	O
,	O
we	O
used	O
data	O
collected	O
with	O
the	O
MacArthur	O
-	O
Bates	O
Communicative	O
Development	O
Inventory	O
forms	O
(	O
CDI	O
)	O
.	O
These	O
are	O
forms	O
,	O
given	O
to	O
parents	O
of	O
young	O
children	O
,	O
that	O
contain	O
checklists	O
of	O
common	O
early	O
acquired	O
words	O
.	O
Parents	O
complete	O
the	O
forms	O
according	O
to	O
whether	O
their	O
child	O
understands	O
or	O
produces	O
each	O
of	O
those	O
words	O
.	O
These	O
forms	O
are	O
collected	O
at	O
different	O
ages	O
,	O
and	O
thus	O
can	O
be	O
used	O
to	O
estimate	O
the	O
Age	O
of	O
Acquisition	O
(	O
AoA	O
)	O
of	O
words	O
.	O
We	O
used	O
all	O
the	O
variants	O
of	O
English	O
'	O
Words	O
&	O
Sentences	O
'	O
CDIs	O
from	O
the	O
Wordbank	O
database	O
(	O
Frank	O
et	O
al	O
,	O
2017	O
)	O
,	O
with	O
the	O
exception	O
of	O
those	O
involving	O
twins	O
(	O
as	O
significant	O
differences	O
have	O
been	O
observed	O
in	O
the	O
language	O
development	O
of	O
twins	O
and	O
singletons	O
,	O
Tomasello	O
et	O
al	O
,	O
1986	O
)	O
.	O
We	O
estimated	O
the	O
AoA	O
of	O
a	O
word	O
by	O
considering	O
that	O
a	O
word	O
is	O
acquired	O
at	O
the	O
age	O
at	O
which	O
at	O
least	O
50	O
%	O
of	O
the	O
children	O
in	O
the	O
sample	O
produced	O
a	O
given	O
word	O
.	O
2	O
http://childes	O
-	O
db.stanford.edu/about	O
.	O
html	O
4	O
Method	O
1	O
:	O
Neighbourhood	O
Density	O
Our	O
first	O
evaluation	O
method	O
is	O
inspired	O
by	O
prior	O
work	O
on	O
human	O
word	O
learning	O
,	O
presented	O
in	O
Hills	O
et	O
al	O
(	O
2010	O
)	O
.	O
In	O
their	O
work	O
,	O
the	O
authors	O
modeled	O
the	O
emerging	O
network	O
of	O
semantic	O
associations	O
that	O
children	O
build	O
during	O
language	B-TaskName
acquisition	I-TaskName
.	O
Their	O
model	O
consists	O
of	O
a	O
simple	O
word	O
co	O
-	O
occurrence	O
matrix	O
,	O
where	O
all	O
the	O
counts	O
greater	O
than	O
zero	O
are	O
flattened	O
into	O
a	O
count	O
of	O
one	O
,	O
resulting	O
in	O
a	O
binary	O
matrix	O
.	O
The	O
authors	O
view	O
the	O
resulting	O
matrix	O
as	O
a	O
network	O
of	O
associations	O
,	O
where	O
words	O
are	O
connected	O
only	O
if	O
they	O
have	O
co	O
-	O
occurred	O
.	O
The	O
number	O
of	O
connections	O
of	O
each	O
word	O
is	O
then	O
used	O
as	O
an	O
index	O
,	O
which	O
the	O
authors	O
call	O
Contextual	O
Diversity	O
(	O
CD	O
)	O
.	O
This	O
index	O
has	O
been	O
repeatedly	O
shown	O
to	O
predict	O
language	B-TaskName
acquisition	I-TaskName
phenomena	O
,	O
such	O
as	O
the	O
age	O
of	O
acquisition	O
of	O
words	O
in	O
different	O
syntactic	O
categories	O
(	O
Hills	O
et	O
al	O
,	O
2010	O
;	O
Stella	O
et	O
al	O
,	O
2017	O
)	O
and	O
individual	O
differences	O
between	O
typically	O
developing	O
children	O
and	O
late	O
talkers	O
(	O
Beckage	O
et	O
al	O
,	O
2011	O
)	O
.	O
We	O
propose	O
a	O
variant	O
evaluation	O
method	O
that	O
takes	O
token	O
co	O
-	O
occurrences	O
into	O
account	O
.	O
Because	O
of	O
the	O
binarization	B-TaskName
of	O
the	O
co	O
-	O
occurrence	O
matrix	O
,	O
the	O
CD	O
index	O
is	O
an	O
indicator	O
of	O
type	O
co	O
-	O
occurrences	O
,	O
and	O
is	O
therefore	O
agnostic	O
to	O
co	O
-	O
occurrence	O
frequency	O
.	O
The	O
models	O
we	O
work	O
with	O
,	O
on	O
the	O
contrary	O
,	O
are	O
sensitive	O
to	O
co	O
-	O
occurrence	O
frequencies	O
,	O
providing	O
a	O
more	O
fine	O
-	O
grained	O
characterization	O
of	O
the	O
semantic	O
space	O
.	O
Our	O
method	O
works	O
as	O
follows	O
.	O
First	O
,	O
we	O
derived	O
the	O
semantic	O
networks	O
based	O
on	O
the	O
cosine	O
distance	O
between	O
representations	O
.	O
This	O
required	O
us	O
to	O
set	O
a	O
minimum	O
cosine	O
similarity	O
threshold	O
θ	B-HyperparameterName
to	O
determine	O
if	O
two	O
words	O
are	O
connected	O
,	O
which	O
we	O
treat	O
as	O
a	O
hyperparameter	O
(	O
with	O
values	O
[	O
.6	O
,	O
.7	O
,	O
.8	O
,	O
.9	O
]	O
)	O
.	O
Second	O
,	O
given	O
this	O
network	O
,	O
we	O
counted	O
the	O
number	O
of	O
neighbours	O
of	O
each	O
word	O
as	O
the	O
number	O
of	O
other	O
words	O
connected	O
to	O
it	O
.	O
We	O
refer	O
to	O
this	O
index	O
as	O
neighbourhood	O
density	O
(	O
ND	O
)	O
.	O
Third	O
,	O
we	O
computed	O
the	O
Pearson	O
's	O
r	O
correlation	O
between	O
this	O
index	O
and	O
the	O
AoA	O
norms	O
.	O
Figure	O
1	O
shows	O
the	O
distribution	O
of	O
the	O
computed	O
metric	O
.	O
Note	O
that	O
these	O
correlations	O
can	O
not	O
be	O
expected	O
to	O
be	O
of	O
the	O
same	O
order	O
as	O
those	O
found	O
when	O
evaluating	O
against	O
adult	O
ratings	O
,	O
since	O
age	O
of	O
acquisition	O
is	O
predicted	O
by	O
a	O
variety	O
of	O
factors	O
,	O
of	O
which	O
distributional	O
information	O
is	O
only	O
one	O
,	O
and	O
it	O
is	O
subject	O
to	O
greater	O
individual	O
differences	O
than	O
adult	O
semantic	O
knowledge	O
.	O
Therefore	O
,	O
moderate	O
but	O
significant	O
correlations	O
are	O
generally	O
consid	O
-	O
ered	O
meaningful	O
.	O
As	O
a	O
reference	O
,	O
the	O
CD	O
index	O
,	O
has	O
a	O
correlation	O
of	O
r	O
=	O
0.32	O
in	O
our	O
dataset	O
3	O
.	O
As	O
can	O
be	O
seen	O
,	O
the	O
SGNS	O
model	O
is	O
more	O
likely	O
to	O
provide	O
a	O
semantic	O
space	O
that	O
correlates	O
with	O
AoA	O
,	O
and	O
some	O
configurations	O
yield	O
an	O
effect	O
size	O
comparable	O
(	O
even	O
larger	O
)	O
than	O
the	O
CD	O
metric	O
.	O
This	O
indicates	O
that	O
the	O
SGNS	O
model	O
builds	O
word	O
representations	O
in	O
a	O
way	O
that	O
reflects	O
the	O
relative	O
difficulty	O
of	O
each	O
word	O
,	O
and	O
thus	O
offers	O
a	O
good	O
starting	O
point	O
for	O
understanding	O
how	O
children	O
use	O
distributional	O
context	O
for	O
vocabulary	O
acquisition	O
.	O
The	O
fact	O
that	O
the	O
correlation	O
is	O
positive	O
prompts	O
the	O
prediction	O
that	O
,	O
when	O
co	O
-	O
occurrence	O
frequency	O
is	O
incorporated	O
in	O
the	O
model	O
,	O
words	O
inhabiting	O
less	O
dense	O
neighbourhoods	O
are	O
acquired	O
earlier	O
.	O
This	O
finding	O
suggests	O
that	O
semantic	O
neighbours	O
may	O
act	O
as	O
competitors	O
in	O
the	O
process	O
of	O
word	O
learning	O
.	O
Among	O
the	O
hyperparameters	O
of	O
these	O
models	O
,	O
one	O
that	O
is	O
particularly	O
relevant	O
to	O
language	B-TaskName
acquisition	I-TaskName
is	O
the	O
window	O
size	O
,	O
as	O
this	O
reveals	O
the	O
amount	O
of	O
context	O
that	O
children	O
most	O
likely	O
attend	O
to	O
in	O
the	O
analyzed	O
ages	O
.	O
To	O
investigate	O
this	O
,	O
we	O
took	O
the	O
best	O
model	O
of	O
our	O
previous	O
analyses	O
(	O
SGNS	O
with	O
window	O
size	O
1	O
,	O
negative	O
sampling	O
15	O
,	O
frequency	O
threshold	O
10	O
)	O
,	O
and	O
varied	O
only	O
the	O
window	O
size	O
.	O
Results	O
are	O
in	O
figure	O
2	O
.	O
As	O
can	O
be	O
seen	O
,	O
smaller	O
window	O
sizes	O
have	O
better	O
correlation	O
with	O
the	O
data	O
,	O
indicating	O
that	O
the	O
exploited	O
context	O
at	O
this	O
age	O
is	O
very	O
local	O
.	O
Such	O
a	O
result	O
makes	O
intuitive	O
sense	O
in	O
the	O
context	O
of	O
children	O
's	O
immature	O
verbal	O
memory	O
spans	O
,	O
which	O
only	O
improve	O
as	O
they	O
acquire	O
more	O
language	O
.	O

We	O
implemented	O
a	O
neural	O
machine	B-TaskName
translation	I-TaskName
system	O
that	O
uses	O
automatic	O
sequence	O
tagging	O
to	O
improve	O
the	O
quality	O
of	O
translation	O
.	O
Instead	O
of	O
operating	O
on	O
unannotated	O
sentence	O
pairs	O
,	O
our	O
system	O
uses	O
pre	O
-	O
trained	O
tagging	O
systems	O
to	O
add	O
linguistic	O
features	O
to	O
source	O
and	O
target	O
sentences	O
.	O
Our	O
proposed	O
neural	O
architecture	O
learns	O
a	O
combined	O
embedding	O
of	O
tokens	O
and	O
tags	O
in	O
the	O
encoder	O
,	O
and	O
simultaneous	O
token	O
and	O
tag	O
prediction	O
in	O
the	O
decoder	O
.	O
Compared	O
to	O
a	O
baseline	O
with	O
unannotated	O
training	O
,	O
this	O
architecture	O
increased	O
the	O
BLEU	B-MetricName
score	I-MetricName
of	O
German	O
to	O
English	O
film	O
subtitle	O
translation	O
outputs	O
by	O
1.61	O
points	O
using	O
named	O
entity	O
tags	O
;	O
however	O
,	O
the	O
BLEU	B-MetricName
score	I-MetricName
decreased	O
by	O
0.38	O
points	O
using	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tags	O
.	O
This	O
demonstrates	O
that	O
certain	O
token	O
-	O
level	O
tag	O
outputs	O
from	O
off	O
-	O
theshelf	O
tagging	O
systems	O
can	O
improve	O
the	O
output	O
of	O
neural	O
translation	O
systems	O
using	O
our	O
combined	O
embedding	O
and	O
simultaneous	O
decoding	O
extensions	O
.	O

Neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
uses	O
neural	O
networks	O
to	O
translate	O
unannotated	O
text	O
between	O
a	O
source	O
and	O
target	O
language	O
,	O
but	O
without	O
additional	O
linguistic	O
information	O
certain	O
ambiguous	O
inputs	O
may	O
be	O
translated	O
incorrectly	O
.	O
Consider	O
the	O
following	O
examples	O
:	O
1	O
)	O
Titanic	O
struggles	O
between	O
good	O
and	O
evil	O
.	O
선과	O
악	O
사이의	O
엄청난	O
투쟁	O
.	O
big	O
fight	O
between	O
good	O
and	O
evil	O
타이타닉은	O
선과	O
악	O
사이에서	O
투쟁	O
중이다	O
.	O
The	O
Titanic	O
is	O
fighting	O
between	O
good	O
and	O
evil	O
2	O
)	O
Titanic	O
struggles	O
to	O
stay	O
afloat	O
.	O
타이타닉은	O
침몰하지	O
않도록	O
고군분투	O
중이다	O
.	O
The	O
Titanic	O
is	O
struggling	O
not	O
to	O
sink	O
침몰하지	O
않기	O
위한	O
엄청난	O
투쟁	O
.	O
big	O
fight	O
not	O
to	O
sink	O
In	O
(	O
1	O
)	O
,	O
"	O
Titanic	O
"	O
is	O
best	O
translated	O
as	O
a	O
common	O
adjective	O
;	O
in	O
(	O
2	O
)	O
,	O
it	O
most	O
likely	O
refers	O
to	O
a	O
named	O
entity	O
,	O
the	O
famous	O
ship	O
.	O
In	O
addition	O
to	O
the	O
bare	O
token	O
sequences	O
,	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
or	O
named	O
entity	O
annotation	O
of	O
each	O
token	O
,	O
provided	O
manually	O
or	O
automatically	O
,	O
could	O
provide	O
additional	O
information	O
to	O
improve	O
the	O
quality	O
of	O
translation	O
.	O
Natural	O
language	O
processing	O
(	O
NLP	O
)	O
tools	O
have	O
benefited	O
from	O
the	O
same	O
explosion	O
in	O
deep	O
learning	O
and	O
neural	O
network	O
developments	O
that	O
has	O
spurred	O
NMT	O
.	O
NLP	O
tools	O
include	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
(	O
POS	O
)	O
taggers	O
,	O
identifying	O
the	O
syntactic	O
function	O
of	O
each	O
input	O
token	O
,	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
systems	O
.	O
Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
identifies	O
which	O
tokens	O
refer	O
to	O
named	O
entities	O
,	O
including	O
proper	O
nouns	O
such	O
as	O
people	O
,	O
place	O
names	O
,	O
organizations	O
,	O
or	O
dates	O
.	O
Recently	O
,	O
automatic	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
systems	O
have	O
seen	O
much	O
development	O
and	O
refinement	O
with	O
the	O
same	O
deep	O
learning	O
tools	O
used	O
for	O
NMT	O
(	O
Li	O
et	O
al	O
,	O
2020	O
)	O
.	O
Automatic	O
neural	O
NER	B-TaskName
systems	O
have	O
achieved	O
accuracy	B-MetricName
exceeding	O
92	O
%	O
F	O
1	O
scores	O
in	O
many	O
languages	O
and	O
domains	O
(	O
Wang	O
et	O
al	O
,	O
2019	O
;	O
Akbik	O
et	O
al	O
,	O
2018	O
)	O
.	O
NER	B-TaskName
tags	O
produced	O
by	O
these	O
systems	O
are	O
useful	O
in	O
many	O
other	O
natural	O
language	O
processing	O
contexts	O
,	O
such	O
as	O
coreference	B-TaskName
resolution	I-TaskName
,	O
entity	B-TaskName
linking	I-TaskName
,	O
or	O
entity	O
extraction	O
(	O
Ferreira	O
Cruz	O
et	O
al	O
,	O
2020	O
)	O
.	O
POS	O
taggers	O
have	O
also	O
achieved	O
very	O
high	O
accuracy	B-MetricName
exceeding	O
98	O
%	O
on	O
public	O
treebank	O
datasets	O
(	O
Akbik	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
aim	O
to	O
use	O
tags	O
from	O
publicly	O
available	O
pre	O
-	O
trained	O
tagging	O
systems	O
as	O
additional	O
features	O
to	O
improve	O
NMT	O
training	O
and	O
output	O
.	O
Tag	O
assisted	O
NMT	O
requires	O
modifications	O
to	O
the	O
neural	O
architecture	O
to	O
accommodate	O
a	O
tag	O
at	O
each	O
token	O
position	O
.	O
The	O
encoder	O
must	O
learn	O
an	O
embedding	O
that	O
combines	O
information	O
from	O
each	O
token	O
and	O
its	O
tag	O
,	O
then	O
compute	O
a	O
hidden	O
state	O
from	O
these	O
embeddings	O
.	O
The	O
decoder	O
must	O
learn	O
to	O
predict	O
tokens	O
and	O
their	O
tags	O
simultaneously	O
from	O
the	O
decoder	O
state	O
.	O
Adding	O
tag	O
information	O
to	O
the	O
predic	O
-	O
tion	O
and	O
corresponding	O
training	O
loss	B-MetricName
encourages	O
the	O
model	O
to	O
incorporate	O
this	O
information	O
into	O
its	O
latent	O
representations	O
to	O
improve	O
outputs	O
.	O
Compared	O
to	O
an	O
untagged	O
baseline	O
system	O
on	O
word	O
-	O
tokenized	O
data	O
,	O
our	O
tagged	O
translation	O
system	O
improved	O
the	O
BLEU	B-MetricName
score	I-MetricName
by	O
1.61	O
points	O
on	O
German	O
to	O
English	O
parallel	O
film	O
subtitles	O
data	O
tagged	O
with	O
publicly	O
available	O
pre	O
-	O
trained	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
systems	O
,	O
while	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
decreased	O
the	O
score	O
by	O
0.38	O
BLEU	B-MetricName
points	O
.	O
Subword	O
tokenization	O
reduced	O
these	O
effects	O
to	O
+0.22	O
points	O
and	O
-	O
0.22	O
points	O
respectively	O
.	O
Nonetheless	O
,	O
this	O
demonstrates	O
the	O
feasibility	O
of	O
using	O
certain	O
pre	O
-	O
trained	O
tagging	O
outputs	O
to	O
improve	O
translation	O
quality	O
.	O

The	O
decoder	O
state	O
d	O
i	O
at	O
each	O
step	O
is	O
conditioned	O
on	O
the	O
target	O
prefix	O
and	O
the	O
encoded	O
source	O
sentence	O
(	O
3	O
)	O
.	O
d	O
i	O
=	O
Decoder	O
(	O
prefix	O
,	O
src	O
)	O
(	O
3	O
)	O
This	O
shared	O
decoder	O
state	O
is	O
used	O
to	O
predict	O
both	O
the	O
next	O
token	O
and	O
the	O
next	O
tag	O
,	O
with	O
token	O
and	O
tag	O
feature	O
projections	O
T	O
and	O
τ	O
(	O
4	O
and	O
5	O
)	O
.	O
P	O
(	O
token	O
k	O
|	O
prefix	O
;	O
src	O
)	O
=	O
softmax	B-MethodName
k	O
(	O
T	O
d	O
i	O
)	O
(	O
4	O
)	O
P	O
(	O
tag	O
k	O
|	O
prefix	O
;	O
src	O
)	O
=	O
softmax	B-MethodName
k	O
(	O
τ	O
d	O
i	O
)	O
(	O
5	O
)	O
We	O
model	O
these	O
probabilities	O
independently	O
(	O
6	O
)	O
for	O
the	O
same	O
data	O
sparsity	O
and	O
model	O
size	O
reasons	O
as	O
the	O
embeddings	O
,	O
and	O
we	O
can	O
compute	O
each	O
pair	O
probability	O
and	O
loss	B-MetricName
accordingly	O
(	O
7	O
)	O
.	O
P	O
(	O
token	O
,	O
tag	O
|	O
prefix	O
;	O
src	O
)	O
=	O
P	O
(	O
token	O
|	O
pre	O
.	O
;	O
src	O
)	O
P	O
(	O
tag	O
|	O
pre	O
.	O
;	O
src	O
)	O
(	O
6	O
)	O
L	O
=	O
−	O
log	O
P	O
(	O
token	O
|	O
prefix	O
;	O
src	O
)	O
−	O
log	O
P	O
(	O
tag	O
|	O
prefix	O
;	O
src	O
)	O
(	O
7	O
)	O
This	O
combined	O
loss	B-MetricName
encourages	O
the	O
shared	O
decoder	O
state	O
d	O
i	O
to	O
model	O
the	O
correct	O
tag	O
identity	O
so	O
that	O
it	O
can	O
be	O
used	O
by	O
the	O
token	O
prediction	O
layer	O
to	O
improve	O
translation	O
.	O
4	O
Data	O
Preparation	O

We	O
used	O
a	O
Transformer	B-MethodName
encoder	O
and	O
decoder	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
for	O
the	O
base	O
seq2seq	B-MethodName
system	O
,	O
each	O
with	O
6	O
layers	O
and	O
8	O
attention	O
heads	O
,	O
and	O
layer	O
and	O
embedding	O
dimensions	O
512	O
.	O
Training	O
was	O
done	O
for	O
40	O
epochs	O
at	O
half	O
precision	O
with	O
the	O
optimizer	B-HyperparameterName
known	O
as	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
β	B-HyperparameterName
=	O
(	O
0.9	O
,	O
0.98	O
)	O
and	O
an	O
inverse	O
square	O
root	O
learning	O
schedule	O
with	O
maximum	O
learning	B-HyperparameterName
rate	I-HyperparameterName
5	O
×	O
10	O
−4	O
after	O
500	O
updates	O
and	O
decay	O
1	O
×	O
10	O
−4	O
.	O
Parameter	O
updates	O
occurred	O
after	O
every	O
8	O
,	O
192	O
token	O
-	O
tag	O
pairs	O
at	O
most	O
(	O
rounding	O
off	O
to	O
complete	O
sentences	O
)	O
,	O
with	O
30	O
%	O
dropout	O
and	O
label	B-MethodName
smoothing	I-MethodName
of	O
0.1	O
on	O
the	O
training	O
loss	B-MetricName
.	O
At	O
inference	O
time	O
,	O
a	O
beam	O
of	O
5	O
candidates	O
was	O
maintained	O
,	O
and	O
the	O
models	O
were	O
evaluated	O
with	O
their	O
BLEU	B-MetricName
score	I-MetricName
on	O
the	O
token	O
sequence	O
only	O
(	O
tagging	O
accuracy	B-MetricName
was	O
not	O
evaluated	O
due	O
to	O
the	O
difficulty	O
of	O
establishing	O
alignment	O
)	O
.	O

BLEU	B-MetricName
scores	O
from	O
untagged	O
and	O
tagged	O
translation	O
experiments	O
show	O
an	O
improvement	O
from	O
the	O
use	O
of	O
NER	B-TaskName
tags	O
(	O
Table	O
1	O
)	O
.	O
Adding	O
NER	B-TaskName
tags	O
,	O
the	O
3	O
baseline	O
4	O
enhanced	O
baseline	O
/	O
ablation	O
study	O
5	O
ablation	O
study	O
BLEU	B-MetricName
score	I-MetricName
on	O
sentences	O
containing	O
some	O
named	O
entities	O
improved	O
by	O
a	O
larger	O
margin	O
,	O
3.07	O
points	O
,	O
presumably	O
due	O
to	O
the	O
tags	O
'	O
assistance	O
with	O
translating	O
those	O
named	O
entities	O
.	O
We	O
also	O
note	O
an	O
improvement	O
in	O
the	O
BLEU	B-MetricName
score	I-MetricName
on	O
sentences	O
containing	O
no	O
named	O
entities	O
,	O
which	O
increased	O
by	O
1.14	O
points	O
.	O
This	O
suggests	O
that	O
given	O
O	O
tag	O
information	O
the	O
model	O
can	O
also	O
treat	O
common	O
words	O
with	O
confidence	O
that	O
they	O
are	O
not	O
named	O
entities	O
and	O
should	O
not	O
be	O
translated	O
as	O
such	O
.	O
These	O
improvements	O
averaged	O
out	O
to	O
a	O
net	O
gain	O
of	O
1.61	O
BLEU	B-MetricName
points	O
on	O
the	O
entire	O
test	O
split	O
.	O
We	O
also	O
evaluated	O
a	O
model	O
trained	O
with	O
POS	O
tags	O
,	O
but	O
found	O
a	O
decrease	O
in	O
BLEU	B-MetricName
score	I-MetricName
(	O
Table	O
2	O
)	O
.	O
Translation	B-TaskName
scores	O
with	O
POS	O
tags	O
decreased	O
by	O
0.38	O
BLEU	B-MetricName
points	O
.	O
There	O
are	O
two	O
ways	O
to	O
understand	O
this	O
in	O
comparison	O
with	O
NER	B-TaskName
tags	O
.	O
First	O
,	O
POS	O
tags	O
carry	O
a	O
significant	O
amount	O
of	O
information	O
about	O
the	O
sentence	O
,	O
not	O
only	O
helping	O
to	O
disambiguate	O
between	O
different	O
word	O
senses	O
by	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
,	O
but	O
also	O
assisting	O
the	O
model	O
with	O
encoding	O
the	O
sentence	O
's	O
syntactic	O
structure	O
.	O
Compared	O
to	O
NER	B-TaskName
tags	O
,	O
this	O
amount	O
of	O
structural	O
information	O
might	O
be	O
difficult	O
to	O
model	O
with	O
the	O
same	O
decoder	O
architecture	O
used	O
for	O
token	O
prediction	O
.	O
Second	O
,	O
POS	O
tags	O
tend	O
to	O
carry	O
the	O
same	O
amount	O
of	O
information	O
for	O
each	O
tag	O
at	O
each	O
position	O
,	O
compared	O
to	O
NER	B-TaskName
tags	O
only	O
conveying	O
most	O
of	O
their	O
information	O
at	O
the	O
named	O
entity	O
spans	O
which	O
are	O
few	O
and	O
far	O
between	O
.	O
This	O
also	O
lends	O
itself	O
to	O
the	O
idea	O
that	O
POS	O
tags	O
have	O
a	O
higher	O
information	O
content	O
that	O
is	O
less	O
easily	O
modeled	O
by	O
the	O
decoder	O
,	O
leading	O
to	O
worse	O
results	O
than	O
NER	B-TaskName
tagging	O
.	O

For	O
both	O
NER	B-TaskName
and	O
POS	O
tagged	O
results	O
,	O
the	O
baseline	O
was	O
the	O
same	O
Transformer	B-MethodName
architecture	O
trained	O
only	O
on	O
untagged	O
data	O
(	O
without	O
adding	O
tag	O
embeddings	O
or	O
predicting	O
tags	O
from	O
the	O
decoder	O
)	O
.	O
Adding	O
in	O
only	O
source	O
-	O
side	O
tag	O
embeddings	O
could	O
be	O
considered	O
an	O
enhanced	O
baseline	O
,	O
since	O
this	O
kind	O
of	O
(	O
Sennrich	O
and	O
Haddow	O
,	O
2016	O
;	O
Hoang	O
et	O
al	O
,	O
2016b	O
)	O
.	O
Our	O
results	O
show	O
that	O
this	O
source	O
-	O
only	O
tagging	O
does	O
not	O
provide	O
significant	O
benefits	O
compared	O
to	O
training	O
on	O
untagged	O
data	O
(	O
Table	O
1	O
)	O
,	O
although	O
for	O
POS	O
tagging	O
this	O
remains	O
the	O
best	O
result	O
.	O
On	O
the	O
other	O
hand	O
,	O
adding	O
in	O
target	O
-	O
side	O
tags	O
while	O
also	O
predicting	O
them	O
from	O
the	O
decoder	O
,	O
without	O
adding	O
in	O
source	O
-	O
side	O
tag	O
embeddings	O
could	O
be	O
considered	O
an	O
ablation	O
test	O
to	O
isolate	O
the	O
effects	O
of	O
our	O
main	O
contribution	O
:	O
target	O
-	O
side	O
tag	O
decoding	O
.	O
Our	O
results	O
show	O
that	O
this	O
target	O
tagging	O
provides	O
the	O
same	O
benefit	O
as	O
the	O
fully	O
tagged	O
training	O
regime	O
,	O
demonstrating	O
that	O
it	O
is	O
the	O
simultaneous	O
tag	O
decoding	O
that	O
accounts	O
for	O
the	O
entire	O
effect	O
observed	O
.	O
For	O
NER	B-TaskName
tagging	O
this	O
was	O
an	O
improvement	O
in	O
BLEU	B-MetricName
scores	O
,	O
but	O
for	O
POS	O
tagging	O
scores	O
decreased	O
when	O
adding	O
target	O
tagging	O
.	O
Whereas	O
source	O
-	O
side	O
tag	O
information	O
is	O
added	O
into	O
the	O
embeddings	O
without	O
any	O
modification	O
to	O
the	O
training	O
objective	O
,	O
target	O
-	O
side	O
tag	O
predictions	O
are	O
a	O
part	O
of	O
the	O
modified	O
training	O
loss	B-MetricName
,	O
so	O
that	O
it	O
is	O
the	O
target	O
-	O
side	O
tag	O
prediction	O
that	O
pushes	O
the	O
model	O
to	O
incorporate	O
accurate	O
knowledge	O
of	O
the	O
tags	O
into	O
its	O
learning	O
representations	O
.	O
That	O
NER	B-TaskName
tag	O
modeling	O
improved	O
results	O
while	O
POS	O
tag	O
modeling	O
did	O
not	O
is	O
consistent	O
with	O
our	O
earlier	O
observation	O
that	O
POS	O
tag	O
modeling	O
seems	O
to	O
be	O
more	O
difficult	O
than	O
NER	B-TaskName
tag	O
modeling	O
,	O
and	O
is	O
not	O
done	O
effectively	O
by	O
the	O
current	O
architecture	O
.	O

Experiments	O
with	O
subword	O
tokenized	O
data	O
showed	O
similar	O
effects	O
,	O
but	O
of	O
a	O
significantly	O
reduced	O
size	O
.	O
Adding	O
NER	B-TaskName
tags	O
improved	O
the	O
results	O
,	O
adding	O
0.22	O
points	O
to	O
the	O
BLEU	B-MetricName
score	I-MetricName
,	O
with	O
the	O
improvement	O
again	O
coming	O
largely	O
from	O
the	O
target	O
side	O
tagging	O
,	O
and	O
again	O
showing	O
a	O
larger	O
improvement	O
on	O
sentences	O
with	O
named	O
entities	O
than	O
on	O
those	O
without	O
(	O
Table	O
3	O
)	O
.	O
Adding	O
POS	O
tags	O
hurt	O
results	O
,	O
decreasing	O
the	O
score	O
by	O
0.22	O
,	O
and	O
again	O
we	O
see	O
that	O
source	O
-	O
only	O
tagging	O
is	O
best	O
case	O
for	O
POS	O
tagging	O
(	O
Table	O
4	O
)	O
.	O
However	O
,	O
the	O
reduced	O
magnitude	O
of	O
these	O
deltas	O
to	O
the	O
range	O
of	O
0.1	O
-	O
0.4	O
BLEU	B-MetricName
points	O
suggests	O
these	O
are	O
not	O
significant	O
changes	O
to	O
the	O
translation	O
performance	O
,	O
in	O
the	O
subword	O
tokenization	O
case	O
.	O
It	O
would	O
appear	O
that	O
subword	O
tokenization	O
interferes	O
with	O
the	O
benefits	O
of	O
tagging	O
the	O
data	O
.	O
Since	O
tags	O
are	O
aligned	O
one	O
-	O
to	O
-	O
one	O
with	O
the	O
input	O
words	O
,	O
subword	O
tokenization	O
destroys	O
this	O
alignment	O
,	O
and	O
copying	O
tags	O
across	O
a	O
word	O
's	O
constituent	O
subwords	O
may	O
interfere	O
with	O
the	O
model	O
's	O
ability	O
to	O
make	O
sense	O
the	O
of	O
tag	O
information	O
.	O
In	O
particular	O
for	O
named	O
entities	O
,	O
rare	O
words	O
are	O
likely	O
to	O
tokenized	O
into	O
a	O
larger	O
number	O
of	O
subword	O
tokens	O
,	O
exacerbating	O
this	O
effect	O
.	O
The	O
set	O
of	O
embeddings	O
for	O
the	O
subwords	O
in	O
a	O
word	O
may	O
not	O
be	O
as	O
useful	O
to	O
the	O
model	O
for	O
translating	O
a	O
named	O
entity	O
or	O
other	O
rare	O
category	O
as	O
the	O
single	O
embedding	O
learned	O
specifically	O
for	O
the	O
full	O
word	O
in	O
a	O
word	O
tokenization	O
setting	O
,	O
and	O
further	O
these	O
subword	O
embeddings	O
may	O
be	O
affected	O
by	O
other	O
contexts	O
unrelated	O
to	O
the	O
larger	O
word	O
.	O
Specifically	O
for	O
the	O
named	O
entity	O
case	O
,	O
subword	O
tokenization	O
algorithms	O
might	O
prioritize	O
the	O
atomicity	O
of	O
certain	O
rare	O
words	O
tagged	O
as	O
named	O
entities	O
in	O
order	O
to	O
counteract	O
this	O
.	O

Due	O
to	O
the	O
conditional	O
independence	O
assumption	O
,	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
(	O
7	O
)	O
conveniently	O
decomposes	O
into	O
separate	O
terms	O
for	O
tokens	O
and	O
tags	O
(	O
8	O
)	O
,	O
allowing	O
us	O
to	O
measure	O
the	O
relative	O
information	O
content	O
of	O
each	O
channel	O
(	O
Table	O
5	O
)	O
.	O
While	O
adding	O
tag	O
information	O
naturally	O
increases	O
the	O
overall	O
cross	O
-	O
entropy	O
,	O
as	O
there	O
are	O
more	O
possibilities	O
to	O
account	O
for	O
and	O
to	O
be	O
predicted	O
,	O
restricting	O
our	O
attention	O
only	O
to	O
the	O
token	O
loss	B-MetricName
shows	O
that	O
the	O
token	O
-	O
level	O
cross	O
-	O
entropy	O
is	O
consistently	O
reduced	O
from	O
2.000	O
(	O
base	O
-	O
2	O
)	O
to	O
1.985	O
with	O
NER	B-TaskName
tags	O
or	O
1.972	O
for	O
POS	O
tags	O
.	O
This	O
shows	O
how	O
both	O
tag	O
types	O
can	O
add	O
disambiguating	O
information	O
to	O
the	O
token	O
prediction	O
process	O
,	O
with	O
POS	O
tags	O
naturally	O
add	O
more	O
of	O
such	O
information	O
,	O
since	O
they	O
carry	O
syntactic	O
information	O
.	O
Looking	O
only	O
at	O
tag	O
-	O
level	O
cross	O
-	O
entropy	O
,	O
it	O
's	O
interesting	O
to	O
notice	O
that	O
the	O
POS	O
tagging	O
loss	B-MetricName
is	O
significantly	O
higher	O
than	O
the	O
NER	B-TaskName
tagging	O
loss	B-MetricName
.	O
While	O
this	O
could	O
be	O
simply	O
because	O
the	O
lower	O
-	O
bound	O
inherent	O
entropy	O
is	O
higher	O
(	O
POS	O
tags	O
naturally	O
contain	O
more	O
information	O
,	O
being	O
more	O
uniformly	O
distributed	O
than	O
NER	B-TaskName
tags	O
)	O
,	O
this	O
could	O
also	O
be	O
consistent	O
with	O
the	O
idea	O
that	O
POS	O
tag	O
modeling	O
is	O
more	O
difficult	O
,	O
explaining	O
the	O
decreased	O
translation	O
scores	O
observed	O
with	O
POS	O
tag	O
prediction	O
.	O

We	O
implemented	O
extensions	O
to	O
existing	O
neural	O
machine	B-TaskName
translation	I-TaskName
models	O
that	O
allow	O
the	O
use	O
of	O
offthe	O
-	O
shelf	O
token	O
-	O
level	O
tagging	O
systems	O
to	O
improve	O
translation	O
accuracy	B-MetricName
.	O
Translation	B-TaskName
inputs	O
and	O
training	O
outputs	O
were	O
tagged	O
with	O
pre	O
-	O
trained	O
sequence	O
labeling	O
systems	O
.	O
A	O
standard	O
encoder	O
-	O
decoder	O
architecture	O
was	O
extended	O
to	O
include	O
tag	O
embeddings	O
and	O
tag	O
prediction	O
at	O
each	O
token	O
position	O
.	O
At	O
model	O
input	O
,	O
token	O
and	O
tag	O
embedding	O
vectors	O
were	O
added	O
to	O
produce	O
a	O
combined	O
embedding	O
.	O
At	O
model	O
output	O
,	O
the	O
final	O
decoder	O
layer	O
used	O
separate	O
softmax	B-MethodName
layers	O
to	O
predict	O
tokens	O
and	O
tags	O
.	O
During	O
training	O
,	O
a	O
combined	O
loss	B-MetricName
function	O
encouraged	O
the	O
model	O
to	O
learn	O
token	O
and	O
tag	O
information	O
jointly	O
.	O
This	O
tag	O
assisted	O
translation	O
system	O
was	O
tested	O
against	O
baseline	O
token	O
-	O
only	O
systems	O
on	O
a	O
German	O
to	O
English	O
film	O
subtitle	O
corpus	O
with	O
both	O
word	O
and	O
subword	O
tokenization	O
.	O
Subword	O
tokenization	O
reduced	O
the	O
size	O
of	O
the	O
effect	O
,	O
suggesting	O
the	O
need	O
for	O
specialized	O
subword	O
tokenization	O
to	O
prioritize	O
the	O
integrity	O
of	O
important	O
word	O
categories	O
.	O
However	O
,	O
on	O
word	O
tokenized	O
data	O
,	O
the	O
1.61	O
point	O
increase	O
in	O
BLEU	B-MetricName
score	I-MetricName
using	O
named	O
entity	O
tags	O
demonstrates	O
that	O
the	O
proposed	O
architecture	O
is	O
useful	O
for	O
improving	O
translation	O
outputs	O
with	O
automatic	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
,	O
while	O
the	O
0.38	O
point	O
decrease	O
using	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tags	O
indicates	O
more	O
difficulty	O
in	O
utilizing	O
that	O
tag	O
information	O
.	O
Further	O
examination	O
of	O
the	O
cross	O
-	O
entropy	O
showed	O
that	O
adding	O
tags	O
reduced	O
the	O
token	O
cross	O
-	O
entropy	O
thereby	O
improving	O
token	O
modeling	O
.	O
Future	O
experiments	O
can	O
explore	O
the	O
use	O
of	O
other	O
types	O
of	O
tag	O
data	O
as	O
well	O
as	O
other	O
decoding	O
paradigms	O
.	O

The	O
next	O
step	O
was	O
to	O
train	O
a	O
classifier	O
using	O
the	O
Labelled	O
Corpus	O
as	O
training	O
data	O
,	O
so	O
that	O
the	O
resulting	O
model	O
could	O
be	O
deployed	O
on	O
the	O
Raw	O
Corpus	O
.	O
Our	O
goal	O
is	O
to	O
obtain	O
automatic	O
predictions	O
for	O
the	O
relevance	O
of	O
each	O
tweet	O
in	O
this	O
corpus	O
,	O
according	O
to	O
probabilities	O
given	O
by	O
our	O
model	O
.	O
We	O
created	O
(	O
stratified	O
)	O
test	O
and	O
training	O
sets	O
that	O
maintain	O
the	O
same	O
proportion	O
of	O
relevant	O
and	O
irrelevant	O
tweets	O
associated	O
with	O
each	O
query	O
word	O
in	O
the	O
Labelled	O
Corpus	O
.	O
We	O
chose	O
to	O
include	O
80	O
%	O
of	O
these	O
tweets	O
in	O
the	O
training	O
set	O
and	O
20	O
%	O
in	O
the	O
test	O
set	O
(	O
see	O
Table	O
1	O
Using	O
the	O
AffectiveTweets	O
package	O
(	O
Bravo	O
-	O
Marquez	O
et	O
al	O
,	O
2019	O
)	O
,	O
our	O
labelled	O
tweets	O
were	O
transformed	O
into	O
feature	O
vectors	O
based	O
on	O
the	O
word	O
n	O
-	O
grams	O
they	O
contain	O
.	O
We	O
then	O
trained	O
various	O
classification	O
models	O
on	O
this	O
transformed	O
data	O
in	O
Weka	O
(	O
Hall	O
et	O
al	O
,	O
2009	O
)	O
.	O
The	O
models	O
we	O
tested	O
were	O
1	O
)	O
Multinomial	O
Naive	O
Bayes	O
(	O
McCallum	O
et	O
al	O
,	O
1998	O
)	O
with	O
unigram	O
attributes	O
and	O
2	O
)	O
L2regularised	O
logistic	B-MethodName
regression	I-MethodName
models	O
with	O
different	O
word	O
n	O
-	O
gram	O
features	O
,	O
as	O
implemented	O
in	O
LIB	O
-	O
LINEAR	O
2	O
.	O
We	O
selected	O
Multinomial	O
Naive	O
Bayes	O
as	O
the	O
best	O
model	O
because	O
it	O
produced	O
the	O
highest	O
AUC	B-MetricName
,	O
Kappa	O
and	O
weighted	O
average	O
F	O
-	O
Score	B-MetricName
(	O
see	O
Table	O
2	O
for	O
a	O
summary	O
of	O
results	O
)	O
.	O
Overall	O
,	O
logistic	B-MethodName
regression	I-MethodName
with	O
unigrams	O
performed	O
the	O
worst	O
,	O
yielding	O
(	O
slightly	O
)	O
lower	O
values	O
for	O
all	O
three	O
measures	O
.	O
After	O
deploying	O
the	O
Multinomial	O
Naive	O
Bayes	O
model	O
on	O
the	O
Raw	O
Corpus	O
,	O
we	O
found	O
that	O
1	O
,	O
179	O
,	O
390	O
tweets	O
were	O
classified	O
as	O
relevant	O
and	O
448	O
,	O
652	O
as	O
irrelevant	O
(	O
with	O
probability	O
threshold	O
=	O
0.5	O
)	O
.	O
Table	O
3	O
shows	O
examples	O
from	O
our	O
corpus	O
of	O
each	O
type	O
of	O
classification	O
.	O
Some	O
tweets	O
were	O
falsely	O
classified	O
as	O
"	O
irrelevant	O
"	O
and	O
some	O
were	O
falsely	O
classified	O
as	O
"	O
relevant	O
"	O
.	O
A	O
short	O
explanation	O
why	O
the	O
irrelevant	O
tweets	O
were	O
coded	O
as	O
such	O
is	O
given	O
in	O
brackets	O
at	O
the	O
end	O
of	O
each	O
tweet	O
.	O
We	O
removed	O
all	O
tweets	O
classified	O
as	O
irrelevant	O
,	O
thereby	O
producing	O
the	O
Processed	O
Corpus	O
.	O
A	O
summary	O
of	O
all	O
three	O
corpora	O
is	O
given	O
in	O
Table	O
4	O
.	O

Semantic	O
Dependency	B-TaskName
Parsing	I-TaskName
(	O
SDP	O
)	O
is	O
defined	O
as	O
the	O
task	O
of	O
recovering	O
sentence	O
-	O
internal	O
bilexical	O
semantic	O
dependency	O
structures	O
,	O
which	O
encode	O
predicate	O
-	O
argument	O
relationships	O
for	O
all	O
content	O
words	O
.	O
Such	O
sentence	O
-	O
level	O
semantic	O
analysis	O
of	O
text	O
is	O
concerned	O
with	O
the	O
characterization	O
of	O
events	O
and	O
is	O
therefore	O
important	O
to	O
understand	O
the	O
essential	O
meaning	O
of	O
a	O
natural	O
language	O
sentence	O
.	O
With	O
the	O
advent	O
of	O
many	O
supporting	O
resources	O
,	O
SDP	O
has	O
become	O
a	O
well	O
-	O
defined	O
task	O
with	O
a	O
substantial	O
body	O
of	O
work	O
and	O
comparative	O
evaluation	O
.	O
(	O
Almeida	O
and	O
Martins	O
,	O
2015	O
;	O
Du	O
et	O
al	O
,	O
2015a	O
;	O
Zhang	O
et	O
al	O
,	O
2016	O
;	O
Peng	O
et	O
al	O
,	O
2017	O
;	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Two	O
SDP	O
shared	O
tasks	O
have	O
been	O
run	O
as	O
part	O
of	O
the	O
2014	O
and	O
2015	O
International	O
Workshops	O
on	O
Semantic	O
Evaluation	O
(	O
SemEval	O
)	O
(	O
Oepen	O
et	O
al	O
,	O
2014	O
(	O
Oepen	O
et	O
al	O
,	O
,	O
2015	O
.	O
There	O
are	O
two	O
key	O
dimensions	O
of	O
the	O
data	O
-	O
driven	O
dependency	B-TaskName
parsing	I-TaskName
approach	O
:	O
decoding	O
and	O
disambiguation	O
.	O
Existing	O
decoding	O
approaches	O
to	O
syntactic	O
or	O
semantic	O
analysis	O
into	O
bilexical	O
dependencies	O
can	O
be	O
categorized	O
into	O
two	O
dominant	O
types	O
:	O
transition	O
-	O
based	O
(	O
Zhang	O
et	O
al	O
,	O
2016	O
;	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
and	O
graph	O
-	O
based	O
,	O
i.e.	O
,	O
Maximum	O
Subgraph	O
(	O
Kuhlmann	O
and	O
Jonsson	O
,	O
2015	O
;	O
Cao	O
et	O
al	O
,	O
2017a	O
)	O
approaches	O
.	O
For	O
disambiguation	O
,	O
while	O
early	O
work	O
on	O
dependency	B-TaskName
parsing	I-TaskName
focused	O
on	O
global	O
linear	O
models	O
,	O
e.g.	O
,	O
structured	O
perceptron	O
(	O
Collins	O
,	O
2002	O
)	O
,	O
recent	O
work	O
shows	O
that	O
deep	O
learning	O
techniques	O
,	O
e.g.	O
,	O
LSTM	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
,	O
is	O
able	O
to	O
significantly	O
advance	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
of	O
the	O
parsing	O
accuracy	B-MetricName
.	O
From	O
the	O
above	O
two	O
perspectives	O
,	O
i.e.	O
,	O
the	O
decoding	O
and	O
disambiguation	O
frameworks	O
,	O
we	O
find	O
that	O
what	O
is	O
still	O
underexploited	O
is	O
neural	O
Maximum	O
Subgraph	O
parsing	O
for	O
highly	O
constrained	O
graph	O
classes	O
,	O
e.g.	O
,	O
noncrossing	O
graphs	O
.	O
In	O
this	O
paper	O
,	O
we	O
fill	O
this	O
gap	O
in	O
the	O
literature	O
by	O
developing	O
a	O
neural	O
Maximum	O
Subgraph	O
parser	O
.	O
Previous	O
work	O
showed	O
that	O
the	O
1	O
-	O
endpointcrossing	O
,	O
pagenumber	O
-	O
2	O
(	O
1EC	O
/	O
P2	O
)	O
graphs	O
are	O
an	O
appropriate	O
graph	O
class	O
for	O
modeling	O
semantic	O
dependency	O
structures	O
(	O
Cao	O
et	O
al	O
,	O
2017a	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
build	O
a	O
parser	O
that	O
targets	O
1EC	O
/	O
P2	O
graphs	O
.	O
Based	O
on	O
an	O
efficient	O
first	O
-	O
order	O
Maximum	O
Subgraph	O
decoder	O
,	O
we	O
implement	O
a	O
data	O
-	O
driven	O
parser	O
that	O
scores	O
arcs	O
based	O
on	O
stacked	O
bidirectional	O
-	O
LSTM	B-MethodName
(	O
BiLSTM	B-MethodName
)	O
together	O
with	O
a	O
multi	O
-	O
layer	O
perceptron	O
.	O
Using	O
the	B-DatasetName
benchmark	I-DatasetName
data	O
sets	O
from	O
the	O
SemEval	O
2015	O
Task	O
18	O
(	O
Oepen	O
et	O
al	O
,	O
2015	O
)	O
,	O
our	O
parser	O
gives	O
very	O
competitive	O
results	O
for	O
English	O
semantic	B-TaskName
parsing	I-TaskName
.	O
To	O
test	O
the	O
ability	O
for	O
crosslingual	O
parsing	O
,	O
we	O
also	O
conduct	O
experiments	O
on	O
the	O
Chinese	O
CCGBank	B-DatasetName
(	O
Tse	O
and	O
Curran	O
,	O
2010	O
)	O
and	O
Enju	O
HPSGBank	O
(	O
Yu	O
et	O
al	O
,	O
2010	O
)	O
data	O
.	O
Our	O
parser	O
plays	O
equally	O
well	O
for	O
Chinese	O
,	O
resulting	O
in	O
an	O
error	O
reduction	O
of	O
23.5	O
%	O
and	O
9.4	O
%	O
over	O
the	O
best	O
published	O
result	O
reported	O
in	O
Zhang	O
et	O
al	O
(	O
2016	O
)	O
and	O
Du	O
et	O
al	O
(	O
2015b	O
)	O
.	O
Most	O
studies	O
on	O
semantic	B-TaskName
parsing	I-TaskName
focused	O
on	O
the	O
in	O
-	O
domain	O
setting	O
,	O
meaning	O
that	O
both	O
training	O
and	O
testing	O
data	O
are	O
drawn	O
from	O
the	O
same	O
domain	O
.	O
Even	O
a	O
data	O
-	O
driven	O
parsing	O
system	O
achieves	O
a	O
high	O
in	O
-	O
domain	O
accuracy	B-MetricName
,	O
it	O
usually	O
performs	O
rather	O
poorly	O
on	O
the	O
out	O
-	O
of	O
-	O
domain	O
data	O
(	O
Oepen	O
et	O
al	O
,	O
2015	O
)	O
.	O
How	O
to	O
build	O
robust	O
semantic	O
dependency	O
parsers	O
that	O
can	O
learn	O
across	O
domains	O
remains	O
an	O
under	O
-	O
addressed	O
problem	O
.	O
To	O
improve	O
the	O
cross	O
-	O
domain	O
parsing	O
performance	O
,	O
we	O
propose	O
a	O
data	O
-	O
oriented	O
model	O
to	O
explore	O
the	O
linguistic	O
generality	O
encoded	O
in	O
a	O
hand	O
-	O
crafted	O
,	O
domainindependent	O
,	O
linguistically	O
-	O
precise	O
English	O
grammar	O
,	O
namely	O
English	O
Resource	O
Grammar	O
(	O
ERG	O
;	O
Flickinger	O
,	O
2000	O
)	O
.	O
In	O
particular	O
,	O
we	O
introduce	O
a	O
cost	O
-	O
sensitive	O
training	O
model	O
to	O
learn	O
crossdomain	O
semantic	O
information	O
implicitly	O
encoded	O
in	O
WikiWoods	O
(	O
Flickinger	O
et	O
al	O
,	O
2010	O
)	O
,	O
i.e.	O
,	O
a	O
corpus	O
that	O
collects	O
the	O
wikipedia	O
1	O
texts	O
as	O
well	O
as	O
their	O
automatic	O
syntactico	O
-	O
semantic	O
annotations	O
produced	O
by	O
ERG	O
.	O
Evaluation	O
demonstrates	O
the	O
usefulness	O
of	O
the	O
imperfect	O
annotations	O
automatically	O
created	O
by	O
ERG	O
.	O
Our	O
parser	O
is	O
available	O
at	O
https://github	O
.	O
com	O
/	O
draplater	O
/	O
msg	O
-	O
parser	O
.	O

In	O
order	O
to	O
update	O
graphs	O
which	O
achieve	O
high	O
model	O
scores	O
but	O
are	O
actually	O
wrong	O
,	O
we	O
use	O
a	O
margin	O
-	O
based	O
approach	O
to	O
compute	O
loss	B-MetricName
from	O
the	O
gold	O
graph	O
G	O
*	O
and	O
the	O
best	O
predictionĜ	O
under	O
current	O
model	O
.	O
We	O
define	O
the	O
loss	B-MetricName
term	O
as	O
:	O
max	O
(	O
0	B-DatasetName
,	O
∆	O
(	O
G	O
*	O
,	O
Ĝ	O
)	O
−	O
SCORE	O
(	O
G	O
*	O
)	O
+	O
SCORE	O
(	O
Ĝ	O
)	O
)	O
The	O
margin	O
objective	O
∆	O
measures	O
the	O
similarity	O
between	O
the	O
gold	O
graph	O
G	O
*	O
and	O
the	O
prediction	O
G.	O
Follow	O
Peng	O
et	O
al	O
(	O
2017	O
)	O
's	O
approach	O
,	O
we	O
define	O
∆	O
as	O
weighted	O
Hamming	O
to	O
trade	O
off	O
between	O
precision	O
and	O
recall	O
.	O
Banarescu	O
et	O
al	O
,	O
2013	O
)	O
.	O
Different	O
from	O
data	O
-	O
driven	O
syntactic	O
parsing	O
,	O
semantic	B-TaskName
parsing	I-TaskName
for	O
the	O
first	O
type	O
of	O
annotation	O
can	O
leverage	O
a	O
precision	O
grammar	O
-	O
guided	O
model	O
.	O
Such	O
a	O
model	O
applies	O
a	O
rich	O
set	O
of	O
precise	O
linguistic	O
rules	O
to	O
constrain	O
their	O
search	O
for	O
a	O
preferable	O
syntactic	O
or	O
semantic	O
analysis	O
.	O
In	O
recent	O
years	O
,	O
several	O
of	O
these	O
linguistically	O
motivated	O
parsing	O
systems	O
achieved	O
high	O
performances	O
that	O
are	O
comparable	O
or	O
even	O
superior	O
to	O
the	O
treebank	O
-	O
based	O
purely	O
data	O
-	O
driven	O
parsers	O
.	O
For	O
example	O
,	O
using	O
ERG	O
(	O
Flickinger	O
,	O
2000	O
)	O
,	O
which	O
provides	O
precise	O
linguistic	O
analyses	O
for	O
a	O
broad	O
range	O
of	O
phenomena	O
,	O
as	O
the	O
the	O
core	O
engine	O
,	O
PET	B-DatasetName
2	O
(	O
Callmeier	O
,	O
2000	O
)	O
and	O
ACE	O
3	O
produce	O
better	O
results	O
than	O
all	O
existing	O
datadriven	O
semantic	O
parsers	O
for	O
sentences	O
that	O
can	O
be	O
parsed	O
by	O
ERG	O
.	O
The	O
main	O
weakness	O
of	O
the	O
precision	O
grammarguided	O
parsers	O
is	O
their	O
robustness	O
with	O
respect	O
to	O
both	O
coverage	O
and	O
efficiency	O
.	O
Even	O
for	O
treebanking	O
on	O
the	O
newswire	O
data	O
,	O
i.e.	O
,	O
the	O
Wall	O
Street	O
Journal	O
data	O
from	O
Penn	B-DatasetName
TreeBank	I-DatasetName
,	O
ERG	O
lacks	O
analyses	O
for	O
c.a	O
.	O
11	O
%	O
sentences	O
(	O
Oepen	O
et	O
al	O
,	O
2015	O
)	O
.	O
For	O
the	O
texts	O
from	O
the	O
web	O
,	O
e.g.	O
,	O
tweets	O
,	O
this	O
problem	O
is	O
much	O
more	O
serious	O
.	O
Moreover	O
,	O
checking	O
all	O
linguistic	O
constraints	O
makes	O
a	O
grammar	O
-	O
guided	O
parser	O
too	O
slow	O
for	O
many	O
realistic	O
NLP	O
applications	O
.	O
On	O
the	O
contrary	O
,	O
light	O
-	O
weight	O
,	O
data	O
-	O
driven	O
parsers	O
usually	O
have	O
complementary	O
strengthes	O
in	O
terms	O
of	O
both	O
coverage	O
and	O
efficiency	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
new	O
data	O
-	O
oriented	O
strategy	O
to	O
consume	O
a	O
precision	O
grammar	O
.	O
The	O
key	O
idea	O
is	O
to	O
take	O
a	O
grammar	O
as	O
an	O
imperfect	O
annotator	O
:	O
We	O
let	O
a	O
precision	O
grammar	O
-	O
guided	O
parser	O
parse	O
large	O
-	O
scale	O
raw	O
texts	O
in	O
an	O
offline	O
way	O
,	O
and	O
then	O
utilize	O
the	O
automatically	O
generated	O
analysis	O
as	O
imperfect	O
training	O
data	O
.	O
Because	O
we	O
only	O
need	O
raw	O
texts	O
to	O
be	O
parsed	O
once	O
,	O
even	O
if	O
this	O
process	O
takes	O
much	O
time	O
,	O
it	O
is	O
still	O
reasonable	O
.	O
A	O
grammarguided	O
parser	O
can	O
not	O
parse	O
a	O
considerable	O
portion	O
of	O
data	O
,	O
but	O
this	O
will	O
not	O
cause	O
serious	O
problems	O
because	O
we	O
can	O
take	O
an	O
enormous	O
amount	O
of	O
sentences	O
as	O
annotation	O
candidates	O
.	O
Just	O
considering	O
the	O
wikipedia	O
,	O
we	O
can	O
collect	O
at	O
least	O
dozens	O
of	O
millions	O
of	O
comparatively	O
high	O
-	O
quality	O
sentences	O
.	O
An	O
essential	O
problem	O
of	O
this	O
method	O
is	O
that	O
such	O
imperfect	O
annotations	O
bring	O
in	O
annotation	O
errors	O
which	O
may	O
hurt	O
parser	O
training	O
.	O
To	O
deal	O
with	O
this	O
problem	O
,	O
we	O
adopted	O
a	O
cost	O
-	O
sensitive	O
training	O
method	O
to	O
train	O
our	O
model	O
on	O
the	O
extended	O
training	O
data	O
.	O
In	O
each	O
epoch	O
,	O
we	O
trained	O
on	O
imperfect	O
corpus	O
first	O
and	O
then	O
on	O
gold	O
-	O
standard	O
corpus	O
.	O
When	O
processing	O
an	O
imperfect	O
sentence	O
,	O
we	O
do	O
not	O
take	O
a	O
loss	B-MetricName
into	O
consideration	O
if	O
the	O
loss	B-MetricName
of	O
this	O
sentence	O
is	O
too	O
small	O
.	O
In	O
particular	O
,	O
if	O
a	O
loss	B-MetricName
of	O
a	O
bilexical	O
relation	O
between	O
two	O
tokens	O
is	O
less	O
than	O
0.05	O
,	O
we	O
would	O
exclude	O
the	O
loss	B-MetricName
.	O
As	O
for	O
label	O
assigning	O
,	O
we	O
exclude	O
losses	O
less	O
than	O
0.5	O
.	O
These	O
threshold	O
numbers	O
are	O
tuned	O
on	O
the	O
development	O
data	O
.	O

To	O
evaluate	O
neural	O
Maximum	O
Subgraph	O
parsing	O
in	O
practice	O
,	O
we	O
first	O
conduct	O
experiments	O
on	O
the	O
three	O
English	O
data	O
sets	O
,	O
namely	O
DM	O
,	O
PAS	O
and	O
PSD	O
4	O
,	O
which	O
are	O
from	O
the	O
SemEval	O
2015	O
Task18	O
(	O
Oepen	O
et	O
al	O
,	O
2015	O
)	O
.	O
We	O
use	O
the	O
"	O
standard	O
"	O
training	O
,	O
validation	O
,	O
and	O
test	O
splits	O
to	O
facilitate	O
comparisons	O
.	O
In	O
other	O
words	O
,	O
the	O
data	O
splitting	O
policy	O
follows	O
the	O
shared	O
task	O
.	O
In	O
addition	O
to	O
English	O
parsing	O
,	O
we	O
consider	O
Chinese	O
SDP	O
and	O
use	O
two	O
data	O
sets	O
:	O
(	O
1	O
)	O
Chinese	O
PAS	O
data	O
provided	O
by	O
SemEval	O
2015	O
,	O
and	O
(	O
2	O
)	O
Chinese	O
CCGBank	B-DatasetName
(	O
Tse	O
and	O
Curran	O
,	O
2010	O
)	O
to	O
evaluate	O
the	O
cross	O
-	O
lingual	O
ability	O
of	O
our	O
model	O
.	O
All	O
the	O
SemEval	O
data	O
sets	O
are	O
publicly	O
available	O
from	O
LDC	O
(	O
Oepen	O
et	O
al	O
,	O
2016	O
)	O
.	O
We	O
use	O
DyNet	O
5	O
to	O
implement	O
our	O
neural	O
models	O
.	O
We	O
use	O
the	O
automatic	O
batch	O
technique	O
(	O
Neubig	O
et	O
al	O
,	O
2017	O
)	O
in	O
DyNet	O
to	O
perform	O
mini	O
-	O
batch	O
gradient	O
descent	O
training	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
32	O
.	O
The	O
detailed	O
network	O
hyper	O
-	O
parameters	O
are	O
summarized	O
in	O
Table	O
2	O
.	O
We	O
use	O
the	O
same	O
pre	O
-	O
trained	O
word	O
embedding	O
as	O
Kiperwasser	O
and	O
Goldberg	O
(	O
2016	O
)	O
.	O

Table	O
1	O
lists	O
the	O
parsing	O
accuracy	B-MetricName
of	O
our	O
system	O
as	O
well	O
as	O
the	O
best	O
published	O
results	O
in	O
the	O
literature	O
for	O
comparison	O
.	O
Results	O
from	O
other	O
papers	O
are	O
of	O
different	O
yet	O
representative	O
decoding	O
or	O
disambiguation	O
frameworks	O
.	O
Du	O
et	O
al	O
(	O
2015a	O
)	O
sion	O
of	O
the	O
two	O
linear	O
model	O
-	O
based	O
parsers	O
is	O
comparable	O
or	O
even	O
superior	O
to	O
our	O
neural	O
parser	O
,	O
but	O
the	O
recall	O
is	O
far	O
behind	O
.	O

Ensemble	O
methods	O
have	O
been	O
shown	O
very	O
helpful	O
to	O
boost	O
the	O
accuracy	B-MetricName
of	O
neural	O
network	O
based	O
parsing	O
.	O
We	O
evaluate	O
two	O
ensemble	O
methods	O
,	O
voting	O
and	O
score	O
averaging	O
.	O
In	O
the	O
voting	O
method	O
,	O
each	O
model	O
parses	O
the	O
sentence	O
to	O
graph	O
respectively	O
.	O
An	O
edge	O
will	O
exist	O
on	O
the	O
combined	O
graph	O
only	O
if	O
more	O
than	O
half	O
output	O
graphs	O
of	O
these	O
models	O
contain	O
this	O
edge	O
.	O
The	O
label	O
of	O
this	O
edge	O
will	O
be	O
the	O
most	O
common	O
label	O
.	O
In	O
the	O
score	O
averaging	O
method	O
,	O
we	O
use	O
averaged	O
score	O
parts	O
to	O
get	O
a	O
maximum	O
graph	O
and	O
classify	O
labels	O
.	O
We	O
choose	O
3/10	O
kind	O
of	O
different	O
initial	O
parameters	O
to	O
train	O
models	O
for	O
ensemble	O
.	O
Figure	O
5	O
shows	O
the	O
result	O
of	O
the	O
two	O
ensemble	O
methods	O
.	O
The	O
averaging	O
method	O
has	O
slightly	O
better	O
performance	O
on	O
the	O
3	O
datasets	O
.	O
The	O
performance	O
of	O
this	O
method	O
on	O
test	O
data	O
is	O
shown	O
on	O
Table	O
1	O
.	O

To	O
test	O
the	O
ability	O
for	O
cross	O
-	O
lingual	O
parsing	O
,	O
we	O
conduct	O
experiments	O
on	O
HPSG	O
and	O
CCG	O
grounded	O
semantic	O
analyses	O
respectively	O
.	O
The	O
HPSG	O
grounded	O
analysis	O
is	O
provided	O
by	O
SemEval	O
2015	O
and	O
the	O
underlying	O
framework	O
is	O
the	O
same	O
to	O
the	O
English	O
PAS	O
data	O
.	O
The	O
CCG	O
grounded	O
analysis	O
is	O
from	O
Chinese	O
CCGBank	B-DatasetName
.	O
We	O
use	O
the	O
same	O
set	O
-	O
up	O
as	O
Zhang	O
et	O
al	O
(	O
2016	O
)	O
.	O
Both	O
data	O
sets	O
are	O
transformed	O
from	O
Chinese	B-DatasetName
TreeBank	I-DatasetName
with	O
two	O
rich	O
sets	O
of	O
heuristic	O
rules	O
(	O
Yu	O
et	O
al	O
,	O
2010	O
;	O
Tse	O
and	O
Curran	O
,	O
2010	O
)	O
.	O
Table	O
4	O
and	O
5	O
Chinese	O
POS	O
tagging	O
has	O
a	O
great	O
impact	O
on	O
parsing	O
.	O
In	O
this	O
paper	O
,	O
we	O
consider	O
two	O
POS	O
taggers	O
:	O
a	O
symbol	O
-	O
refined	O
generative	O
HMM	O
tagger	O
(	O
SR	O
-	O
HMM	O
)	O
(	O
Huang	O
et	O
al	O
,	O
2009	O
)	O
and	O
a	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
when	O
assisting	O
Chinese	O
SDG	O
.	O
For	O
the	O
neural	O
tagging	O
model	O
,	O
in	O
addition	O
to	O
a	O
BiL	O
-	O
STM	O
layer	O
for	O
encoding	O
words	O
,	O
we	O
set	O
a	O
BiLSTM	B-MethodName
layer	O
for	O
encoding	O
characters	O
,	O
which	O
supports	O
us	O
to	O
derive	O
character	O
-	O
level	O
representations	O
for	O
all	O
words	O
.	O
In	O
particular	O
,	O
vectors	O
from	O
the	O
characterlevel	O
LSTM	B-MethodName
is	O
concatenated	O
with	O
the	O
pre	O
-	O
trained	O
word	O
embedding	O
before	O
feeding	O
into	O
the	O
other	O
word	O
-	O
level	O
BiLSTM	B-MethodName
network	O
to	O
capture	O
contextual	O
information	O
.	O
The	O
final	O
module	O
of	O
our	O
CRF	B-MethodName
tagger	O
is	O
a	O
linear	O
chain	O
CRF	B-MethodName
which	O
scores	O
the	O
output	O
sequence	O
by	O
factoring	O
it	O
in	O
local	O
tag	O
bi	O
-	O
grams	O
.	O
From	O
Table	O
5	O
,	O
we	O
can	O
see	O
that	O
POS	O
information	O
is	O
very	O
important	O
to	O
Chinese	O
SDP	O
.	O
This	O
phenomenon	O
is	O
consist	O
with	O
Chinese	O
syntactic	O
parsing	O
,	O
including	O
both	O
constituency	O
and	O
dependency	B-TaskName
parsing	I-TaskName
.	O
Mandarin	O
Chinese	O
is	O
recognized	O
as	O
a	O
morphology	O
-	O
poor	O
language	O
:	O
POS	O
tags	O
are	O
defined	O
mainly	O
according	O
to	O
words	O
'	O
distributional	O
rather	O
than	O
morphological	O
properties	O
.	O
"	O
ZDSW	O
"	O
is	O
the	O
system	O
that	O
obtained	O
the	O
best	O
parsing	O
accuracy	B-MetricName
on	O
the	O
Chinese	O
CCGBank	B-DatasetName
data	O
in	O
the	O
literature	O
.	O
the	O
power	O
of	O
the	O
RNN	O
architecture	O
to	O
learn	O
nonlocal	O
dependencies	O
and	O
thus	O
benefit	O
our	O
semantic	O
dependency	O
parser	O
a	O
lot	O
.	O

Parsing	O
sentences	O
to	O
linguistically	O
-	O
rich	O
semantic	O
representations	O
is	O
a	O
key	O
goal	O
of	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
.	O
We	O
introduce	O
a	O
new	O
parser	O
for	O
semantic	O
dependency	O
analysis	O
,	O
which	O
combines	O
two	O
promising	O
parsing	O
techniques	O
,	O
i.e.	O
,	O
decoding	O
based	O
on	O
Maximum	O
Subgraph	O
algorithms	O
and	O
disambiguation	O
based	O
on	O
BiLSTMs	O
.	O
To	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
neural	O
Maximum	O
Subgraph	O
parser	O
.	O
Our	O
parser	O
significantly	O
improves	O
state	O
-	O
ofthe	O
-	O
art	O
accuracy	B-MetricName
on	O
three	O
out	O
of	O
total	O
four	O
data	O
sets	O
from	O
SemEval	O
2015	O
for	O
English	O
/	O
Chinese	O
parsing	O
and	O
the	O
CCGBank	B-DatasetName
data	O
for	O
Chinese	O
parsing	O
.	O
We	O
also	O
propose	O
a	O
new	O
data	O
-	O
oriented	O
method	O
to	O
leverage	O
ERG	O
,	O
a	O
linguistically	O
-	O
motivated	O
,	O
hand	O
-	O
crafted	O
grammar	O
,	O
to	O
improve	O
cross	O
-	O
domain	O
performance	O
.	O
Experiments	O
demonstrate	O
the	O
effectiveness	O
of	O
taking	O
ERG	O
as	O
an	O
imperfect	O
annotator	O
.	O
We	O
think	O
this	O
method	O
can	O
be	O
re	O
-	O
used	O
for	O
other	O
types	O
of	O
datadriven	O
semantic	B-TaskName
parsing	I-TaskName
models	O
.	O

Relation	B-TaskName
extraction	I-TaskName
(	O
RE	O
)	O
aims	O
to	O
extract	O
relational	O
facts	O
between	O
two	O
entities	O
from	O
plain	O
texts	O
.	O
For	O
example	O
,	O
with	O
the	O
sentence	O
"	O
Hayao	O
Miyazaki	O
is	O
the	O
director	O
of	O
the	O
film	O
'	O
The	O
Wind	O
Rises	O
'	O
"	O
,	O
we	O
can	O
extract	O
a	O
relation	O
"	O
director_of	O
"	O
between	O
two	O
entities	O
"	O
Hayao	O
Miyazaki	O
"	O
and	O
"	O
The	O
Wind	O
Rises	O
"	O
.	O
Recent	O
progress	O
in	O
supervised	O
methods	O
to	O
RE	O
has	O
achieved	O
great	O
successes	O
.	O
Supervised	O
methods	O
can	O
effectively	O
learn	O
significant	O
relation	O
semantic	O
patterns	O
based	O
on	O
existing	O
labeled	O
data	O
,	O
but	O
the	O
data	O
constructions	O
are	O
time	O
-	O
consuming	O
and	O
human	O
-	O
intensive	O
.	O
To	O
lower	O
the	O
level	O
of	O
supervision	O
,	O
several	O
semi	O
-	O
supervised	O
approaches	O
have	O
been	O
developed	O
,	O
including	O
bootstrapping	O
,	O
active	B-TaskName
learning	I-TaskName
,	O
label	O
propagation	O
(	O
Pawar	O
et	O
al	O
,	O
2017	O
)	O
.	O
Mintz	O
(	O
2009	O
)	O
also	O
proposes	O
distant	O
supervision	O
to	O
generate	O
training	O
data	O
automatically	O
.	O
It	O
assumes	O
that	O
if	O
two	O
entities	O
have	O
a	O
relation	O
in	O
KBs	O
,	O
all	O
sentences	O
that	O
contain	O
these	O
two	O
entities	O
will	O
express	O
this	O
relation	O
.	O
Still	O
,	O
all	O
these	O
approaches	O
can	O
only	O
extract	O
pre	O
-	O
defined	O
relations	O
that	O
have	O
already	O
appeared	O
either	O
in	O
human	O
-	O
annotated	O
datasets	O
or	O
KBs	O
.	O
It	O
is	O
hard	O
for	O
them	O
to	O
cover	O
the	O
great	O
variety	O
of	O
novel	O
relational	O
facts	O
in	O
the	O
open	O
-	O
domain	O
corpora	O
.	O
Open	O
relation	B-TaskName
extraction	I-TaskName
(	O
OpenRE	O
)	O
aims	O
to	O
extract	O
relational	O
facts	O
on	O
the	O
open	O
-	O
domain	O
corpus	O
,	O
where	O
the	O
relation	O
types	O
may	O
not	O
be	O
predefined	O
.	O
There	O
are	O
some	O
efforts	O
concentrating	O
on	O
extracting	O
triples	O
with	O
new	O
relation	O
types	O
.	O
Banko	O
(	O
2008	O
)	O
directly	O
extracts	O
words	O
or	O
phrases	O
in	O
sentences	O
to	O
represent	O
new	O
relation	O
types	O
.	O
However	O
,	O
some	O
relations	O
can	O
not	O
be	O
explicitly	O
represented	O
with	O
tokens	O
in	O
sentences	O
,	O
and	O
it	O
is	O
hard	O
to	O
align	O
different	O
relational	O
tokens	O
that	O
exactly	O
have	O
the	O
same	O
meanings	O
.	O
Yao	O
(	O
2011	O
)	O
consid	O
-	O
ers	O
OpenRE	O
as	O
a	O
clustering	O
task	O
for	O
extracting	O
triples	O
with	O
new	O
relation	O
types	O
.	O
However	O
,	O
previous	O
clustering	O
-	O
based	O
OpenRE	O
methods	O
(	O
Yao	O
et	O
al	O
,	O
2011	O
(	O
Yao	O
et	O
al	O
,	O
,	O
2012Marcheggiani	O
and	O
Titov	O
,	O
2016	O
;	O
Elsahar	O
et	O
al	O
,	O
2017	O
)	O
are	O
mostly	O
unsupervised	O
,	O
and	O
can	O
not	O
effectively	O
select	O
meaningful	O
relation	O
patterns	O
and	O
discard	O
irrelevant	O
information	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
to	O
take	O
advantage	O
of	O
high	O
-	O
quality	O
supervised	O
data	O
of	O
pre	O
-	O
defined	O
relations	O
for	O
OpenRE	O
.	O
The	O
approach	O
is	O
non	O
-	O
trivial	O
,	O
however	O
,	O
due	O
to	O
the	O
considerable	O
gap	O
between	O
the	O
pre	O
-	O
defined	O
relations	O
and	O
novel	O
relations	O
of	O
interest	O
in	O
open	O
domain	O
.	O
To	O
bridge	O
the	O
gap	O
,	O
we	O
propose	O
Relational	O
Siamese	O
Networks	O
(	O
RSNs	O
)	O
to	O
learn	O
transferable	O
relational	O
knowledge	O
from	O
supervised	O
data	O
for	O
OpenRE	O
.	O
Specifically	O
,	O
RSNs	O
learn	O
relational	O
similarity	O
metrics	O
from	O
labeled	O
data	O
of	O
pre	O
-	O
defined	O
relations	O
,	O
and	O
then	O
transfer	O
the	O
metrics	O
to	O
measure	O
the	O
similarity	O
of	O
unlabeled	O
sentences	O
for	O
open	O
relation	O
clustering	O
.	O
We	O
describe	O
the	O
flowchart	O
of	O
our	O
framework	O
in	O
Figure	O
1	O
.	O
Moreover	O
,	O
we	O
show	O
that	O
RSNs	O
can	O
also	O
be	O
generalized	O
to	O
various	O
weakly	O
-	O
supervised	O
scenarios	O
.	O
We	O
propose	O
Semi	O
-	O
supervised	O
RSN	O
to	O
learn	O
from	O
both	O
supervised	O
data	O
of	O
pre	O
-	O
defined	O
relations	O
and	O
unsupervised	O
data	O
with	O
novel	O
relations	O
,	O
and	O
Distantly	O
-	O
supervised	O
RSN	O
to	O
learn	O
from	O
distantly	O
-	O
supervised	O
data	O
and	O
unsupervised	O
data	O
.	O
We	O
conduct	O
experiments	O
on	O
real	O
-	O
world	O
RE	O
datasets	O
,	O
FewRel	B-DatasetName
and	O
FewRel	B-DatasetName
-	O
distant	O
,	O
by	O
splitting	O
relations	O
into	O
seen	O
and	O
unseen	O
set	O
,	O
and	O
evaluate	O
our	O
models	O
in	O
supervised	O
,	O
semi	O
-	O
supervised	O
,	O
and	O
distantly	O
-	O
supervised	O
scenarios	O
.	O
The	O
results	O
demonstrate	O
that	O
our	O
models	O
significantly	O
outperform	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baseline	O
methods	O
in	O
all	O
scenarios	O
without	O
using	O
external	O
linguistic	O
tools	O
.	O
To	O
summarize	O
,	O
the	O
main	O
contributions	O
of	O
this	O
work	O
are	O
as	O
follows	O
:	O
(	O
1	O
)	O
We	O
develop	O
a	O
novel	O
relational	O
knowledge	O
transfer	O
framework	O
RSN	O
for	O
OpenRE	O
,	O
which	O
can	O
effectively	O
transfer	O
existing	O
relational	O
knowledge	O
to	O
novel	O
-	O
relation	O
data	O
and	O
accurately	O
identify	O
novel	O
relations	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
RSN	O
is	O
the	O
first	O
model	O
to	O
consider	O
knowledge	O
transfer	O
in	O
clustering	O
-	O
based	O
OpenRE	O
task	O
.	O
(	O
2	O
)	O
We	O
further	O
propose	O
Semi	O
-	O
supervised	O
RSNs	O
and	O
Distantly	O
-	O
supervised	O
RSNs	O
that	O
can	O
learn	O
from	O
various	O
weakly	O
supervised	O
scenarios	O
.	O
The	O
experimental	O
results	O
show	O
that	O
all	O
these	O
RSN	O
models	O
achieve	O
significant	O
improvements	O
in	O
F	B-MetricName
-	I-MetricName
measure	I-MetricName
compared	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
.	O

The	O
architecture	O
of	O
our	O
Relational	O
Siamese	O
Networks	O
is	O
shown	O
in	O
Figure	O
2	O
.	O
CNN	O
modules	O
encode	O
a	O
pair	O
of	O
relational	O
instances	O
into	O
vectors	O
,	O
and	O
several	O
shared	O
layers	O
compute	O
their	O
similarity	O
.	O
Sentence	O
Encoder	O
.	O
We	O
use	O
a	O
CNN	O
module	O
as	O
the	O
sentence	O
encoder	O
.	O
The	O
CNN	O
module	O
includes	O
an	O
embedding	O
layer	O
,	O
a	O
convolutional	O
layer	O
,	O
a	O
max	O
-	O
pooling	O
layer	O
,	O
and	O
a	O
fully	O
-	O
connected	O
(	O
FC	O
)	O
layer	O
.	O
The	O
embedding	O
layer	O
transforms	O
the	O
words	O
in	O
a	O
sentence	O
x	O
and	O
the	O
positions	O
of	O
entities	O
e	O
head	O
and	O
e	O
tail	O
into	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
and	O
random	O
-	O
initialized	O
position	O
embeddings	O
.	O
Following	O
(	O
Zeng	O
et	O
al	O
,	O
2014	O
)	O
,	O
we	O
concatenate	O
these	O
embeddings	O
to	O
form	O
a	O
vector	O
sequence	O
.	O
Next	O
,	O
a	O
one	O
-	O
dimensional	O
convolutional	O
layer	O
and	O
a	O
maxpooling	O
layer	O
transform	O
the	O
vector	O
sequence	O
into	O
features	O
.	O
Finally	O
,	O
an	O
FC	O
layer	O
with	O
sigmoid	B-MethodName
activation	I-MethodName
maps	O
features	O
into	O
a	O
relational	O
vector	O
v.	O
To	O
summarize	O
,	O
we	O
obtain	O
a	O
vector	O
representation	O
v	O
for	O
a	O
relational	O
sentence	O
with	O
our	O
CNN	O
module	O
:	O
v	O
=	O
CNN	O
(	O
s	O
)	O
,	O
(	O
1	O
)	O
in	O
which	O
we	O
denote	O
the	O
joint	O
information	O
of	O
a	O
sentence	O
x	O
and	O
two	O
entities	O
in	O
it	O
e	O
head	O
and	O
e	O
tail	O
as	O
a	O
data	O
sample	O
s.	O
And	O
with	O
paired	O
input	O
relational	O
instances	O
,	O
we	O
have	O
:	O
vl	O
=	O
CNN	O
(	O
s	O
l	O
)	O
,	O
vr	O
=	O
CNN	O
(	O
sr	O
)	O
,	O
(	O
2	O
)	O
in	O
which	O
two	O
CNN	O
modules	O
are	O
identical	O
and	O
share	O
all	O
the	O
parameters	O
.	O
Similarity	O
Computation	O
.	O
Next	O
,	O
to	O
measure	O
the	O
similarity	O
of	O
two	O
relational	O
vectors	O
,	O
we	O
calculate	O
their	O
absolute	O
distance	O
and	O
transform	O
it	O
into	O
a	O
realnumber	O
similarity	O
p	O
[	O
0	B-DatasetName
,	O
1	O
]	O
.	O
First	O
,	O
a	O
distance	O
layer	O
computes	O
the	O
element	O
-	O
wise	O
absolute	O
distance	O
of	O
two	O
vectors	O
:	O
vd	O
=	O
|	O
vl	O
−	O
vr	O
|	O
.	O
(	O
3	O
)	O
Then	O
,	O
a	O
classifier	O
layer	O
calculates	O
a	O
metric	O
p	O
for	O
relation	O
similarity	O
.	O
The	O
layer	O
is	O
a	O
one	O
-	O
dimensionaloutput	O
FC	O
layer	O
with	O
sigmoid	B-MethodName
activation	I-MethodName
:	O
p	O
=	O
σ	O
(	O
kvd	O
+	O
b	O
)	O
,	O
(	O
4	O
)	O
in	O
which	O
σ	O
denotes	O
the	O
sigmoid	O
function	O
,	O
k	O
and	O
b	O
denote	O
the	O
weights	O
and	O
bias	O
.	O
To	O
summarize	O
,	O
we	O
obtain	O
a	O
good	O
similarity	O
metric	O
p	O
of	O
relational	O
instances	O
.	O
Cross	O
Entropy	O
Loss	O
.	O
The	O
output	O
of	O
RSN	O
p	O
can	O
also	O
be	O
explained	O
as	O
the	O
probability	O
of	O
two	O
sentences	O
mentioning	O
two	O
different	O
relations	O
.	O
Thus	O
,	O
we	O
can	O
use	O
binary	O
labels	O
q	O
and	O
binary	O
cross	O
entropy	O
loss	B-MetricName
to	O
train	O
our	O
RSN	O
:	O
L	O
l	O
=	O
E	O
d	O
l	O
∼D	O
l	O
[	O
q	O
ln	O
(	O
p	O
θ	B-HyperparameterName
(	O
d	O
l	O
)	O
)	O
+	O
(	O
1	O
−	O
q	O
)	O
ln	O
(	O
1	O
−	O
p	O
θ	B-HyperparameterName
(	O
d	O
l	O
)	O
)	O
]	O
,	O
(	O
5	O
)	O
in	O
which	O
θ	B-HyperparameterName
indicates	O
all	O
the	O
parameters	O
in	O
the	O
RSN	O
.	O

To	O
discover	O
relation	O
clusters	O
in	O
the	O
open	O
-	O
domain	O
corpus	O
,	O
it	O
is	O
beneficial	O
to	O
not	O
only	O
learn	O
from	O
labeled	O
data	O
,	O
but	O
also	O
capture	O
the	O
manifold	O
of	O
unlabeled	O
data	O
in	O
the	O
semantic	O
space	O
.	O
To	O
this	O
end	O
,	O
we	O
need	O
to	O
push	O
the	O
decision	O
boundaries	O
away	O
from	O
high	O
-	O
density	O
areas	O
,	O
which	O
is	O
known	O
as	O
the	O
cluster	O
assumption	O
(	O
Chapelle	O
and	O
Zien	O
,	O
2005	O
)	O
.	O
We	O
try	O
to	O
achieve	O
this	O
goal	O
with	O
several	O
additional	O
loss	B-MetricName
functions	O
.	O
In	O
the	O
following	O
paragraphs	O
,	O
we	O
denote	O
the	O
labeled	O
training	O
dataset	O
as	O
D	O
l	O
and	O
a	O
couple	O
of	O
labeled	O
relational	O
instances	O
as	O
d	O
l	O
.	O
Similarly	O
,	O
we	O
denote	O
the	O
unlabeled	O
training	O
dataset	O
as	O
D	O
u	O
and	O
a	O
couple	O
of	O
unlabeled	O
instances	O
as	O
d	O
u	O
.	O
Conditional	O
Entropy	O
Loss	O
.	O
In	O
classification	O
problems	O
,	O
a	O
well	O
-	O
classified	O
embedding	O
space	O
usually	O
reserves	O
large	O
margins	O
between	O
different	O
classified	O
clusters	O
,	O
and	O
optimizing	O
margin	O
can	O
be	O
a	O
promising	O
way	O
to	O
facilitate	O
training	O
.	O
However	O
,	O
in	O
clustering	O
problems	O
,	O
type	O
labels	O
are	O
not	O
available	O
during	O
training	O
.	O
To	O
optimize	O
margin	O
without	O
explicit	O
supervision	O
,	O
we	O
can	O
push	O
the	O
data	O
points	O
away	O
from	O
the	O
decision	O
boundaries	O
.	O
Intuitively	O
,	O
when	O
the	O
distance	O
similarity	O
p	O
between	O
two	O
relational	O
instances	O
equals	O
0.5	O
,	O
there	O
is	O
a	O
high	O
prob	O
-	O
ability	O
that	O
at	O
least	O
one	O
of	O
two	O
instances	O
is	O
near	O
the	O
decision	O
boundary	O
between	O
relation	O
clusters	O
.	O
Thus	O
,	O
we	O
use	O
the	O
conditional	O
entropy	O
loss	B-MetricName
(	O
Grandvalet	O
and	O
Bengio	O
,	O
2005	O
)	O
,	O
which	O
reaches	O
the	O
maximum	O
when	O
p	O
=	O
0.5	O
,	O
to	O
penalize	O
close	O
-	O
boundary	O
distribution	O
of	O
data	O
points	O
:	O
Lu	O
=	O
E	O
du∼Du	O
[	O
p	O
θ	B-HyperparameterName
(	O
du	O
)	O
ln	O
(	O
p	O
θ	B-HyperparameterName
(	O
du	O
)	O
)	O
+	O
(	O
1	O
−	O
p	O
θ	B-HyperparameterName
(	O
du	O
)	O
)	O
ln	O
(	O
1	O
−	O
p	O
θ	B-HyperparameterName
(	O
du	O
)	O
)	O
]	O
.	O
(	O
6	O
)	O
Virtual	O
Adversarial	O
Loss	O
.	O
Despite	O
its	O
theoretical	O
promise	O
,	O
conditional	O
entropy	O
minimization	O
suffers	O
from	O
shortcomings	O
in	O
practice	O
.	O
Due	O
to	O
neural	O
networks	O
'	O
strong	O
fitting	O
ability	O
,	O
a	O
very	O
complex	O
decision	O
hyperplane	O
might	O
be	O
learned	O
so	O
as	O
to	O
keep	O
away	O
from	O
all	O
the	O
training	O
samples	O
,	O
which	O
lacks	O
generalizability	O
.	O
As	O
a	O
solution	O
,	O
we	O
can	O
smooth	O
the	O
relational	O
representation	O
space	O
with	O
locally	O
-	O
Lipschitz	O
constraint	O
.	O
To	O
satisfy	O
this	O
constraint	O
,	O
we	O
introduce	O
virtual	O
adversarial	O
training	O
(	O
Miyato	O
et	O
al	O
,	O
2016	O
)	O
on	O
both	O
branches	O
of	O
RSN	O
.	O
Virtual	O
adversarial	O
training	O
can	O
search	O
through	O
data	O
point	O
neighborhoods	O
,	O
and	O
penalize	O
most	O
sharp	O
changes	O
in	O
distance	O
prediction	O
.	O
For	O
labeled	O
data	O
,	O
we	O
have	O
L	O
vl	O
=	O
E	O
d	O
l	O
∼D	O
l	O
[	O
DKL	O
(	O
p	O
θ	B-HyperparameterName
(	O
d	O
l	O
)	O
|	O
|	O
p	O
θ	B-HyperparameterName
(	O
d	O
l	O
,	O
t1	O
,	O
t2	O
)	O
)	O
]	O
,	O
(	O
7	O
)	O
in	O
which	O
D	O
KL	O
indicates	O
the	O
Kullback	O
-	O
Leibler	O
divergence	O
,	O
p	O
θ	B-HyperparameterName
(	O
d	O
l	O
,	O
t	O
1	O
,	O
t	O
2	O
)	O
indicates	O
a	O
new	O
distance	O
estimation	O
with	O
perturbations	O
t	O
1	O
and	O
t	O
2	O
on	O
both	O
input	O
instances	O
respectively	O
.	O
Specifically	O
,	O
t	O
1	O
and	O
t	O
2	O
are	O
worst	O
-	O
case	O
perturbations	O
that	O
maximize	O
the	O
KL	O
divergence	O
between	O
p	O
θ	B-HyperparameterName
(	O
d	O
l	O
)	O
and	O
p	O
θ	B-HyperparameterName
(	O
d	O
l	O
,	O
t	O
1	O
,	O
t	O
2	O
)	O
with	O
a	O
limited	O
length	O
.	O
Empirically	O
,	O
we	O
approximate	O
the	O
perturbations	O
the	O
same	O
as	O
the	O
original	O
paper	O
(	O
Miyato	O
et	O
al	O
,	O
2016	O
)	O
.	O
Specifically	O
,	O
we	O
first	O
add	O
a	O
random	O
noise	O
to	O
the	O
input	O
,	O
and	O
calculate	O
the	O
gradient	O
of	O
the	O
KL	O
-	O
divergence	O
between	O
the	O
outputs	O
of	O
the	O
original	O
input	O
and	O
the	O
noisy	O
input	O
.	O
We	O
then	O
add	O
the	O
normalized	O
gradient	O
to	O
the	O
original	O
input	O
and	O
get	O
the	O
perturbed	O
input	O
.	O
And	O
for	O
unlabeled	O
data	O
,	O
we	O
have	O
Lvu	O
=	O
E	O
du∼Du	O
[	O
DKL	O
(	O
p	O
θ	B-HyperparameterName
(	O
du	O
)	O
|	O
|	O
p	O
θ	B-HyperparameterName
(	O
du	O
,	O
t1	O
,	O
t2	O
)	O
)	O
]	O
,	O
(	O
8	O
)	O
in	O
which	O
the	O
perturbations	O
t	O
1	O
and	O
t	O
2	O
are	O
added	O
to	O
word	B-TaskName
embeddings	I-TaskName
rather	O
than	O
the	O
words	O
themselves	O
.	O
To	O
summarize	O
,	O
we	O
use	O
the	O
following	O
loss	B-MetricName
function	O
to	O
train	O
Semi	O
-	O
supervised	O
RSN	O
,	O
which	O
learns	O
from	O
both	O
labeled	O
and	O
unlabeled	O
data	O
:	O
L	O
all	O
=	O
L	O
l	O
+	O
λvL	O
vl	O
+	O
λu	O
(	O
Lu	O
+	O
λvLvu	O
)	O
,	O
(	O
9	O
)	O
in	O
which	O
λ	O
v	O
and	O
λ	O
u	O
are	O
two	O
hyperparameters	O
.	O

To	O
alleviate	O
the	O
intensive	O
human	O
labor	O
for	O
annotation	O
,	O
the	O
topic	O
of	O
distantly	O
-	O
supervised	O
learning	O
has	O
attracted	O
much	O
attention	O
in	O
RE	O
.	O
Here	O
,	O
we	O
propose	O
Distantly	O
-	O
supervised	O
RSN	O
,	O
which	O
can	O
learn	O
from	O
both	O
distantly	O
-	O
supervised	O
data	O
and	O
unsupervised	O
data	O
for	O
relational	O
knowledge	O
transfer	O
.	O
Specifically	O
,	O
we	O
use	O
the	O
following	O
loss	B-MetricName
function	O
:	O
L	O
all	O
=	O
L	O
l	O
+	O
λu	O
(	O
Lu	O
+	O
λvLvu	O
)	O
,	O
(	O
10	O
)	O
which	O
treats	O
auto	O
-	O
labeled	O
data	O
as	O
labeled	O
data	O
but	O
removes	O
the	O
virtual	O
adversarial	O
loss	B-MetricName
on	O
the	O
autolabeled	O
data	O
.	O
The	O
reason	O
to	O
remove	O
the	O
loss	B-MetricName
is	O
simple	O
:	O
virtual	O
adversarial	O
training	O
on	O
auto	O
-	O
labeled	O
data	O
can	O
amplify	O
the	O
noise	O
from	O
false	O
labels	O
.	O
Indeed	O
,	O
we	O
do	O
find	O
that	O
the	O
virtual	O
adversarial	O
loss	B-MetricName
on	O
autolabeled	O
data	O
can	O
harm	O
our	O
model	O
's	O
performance	O
in	O
experiments	O
.	O
We	O
do	O
not	O
use	O
more	O
denoising	B-TaskName
methods	O
,	O
since	O
we	O
think	O
RSN	O
has	O
some	O
inherent	O
advantages	O
of	O
tolerating	O
such	O
noise	O
.	O
Firstly	O
,	O
the	O
noise	O
will	O
be	O
overwhelmed	O
by	O
the	O
large	O
proportion	O
of	O
negative	O
sampling	O
during	O
training	O
.	O
Secondly	O
,	O
during	O
clustering	O
,	O
the	O
prediction	O
of	O
a	O
new	O
relation	O
cluster	O
is	O
based	O
on	O
areas	O
where	O
the	O
density	O
of	O
relational	O
instances	O
is	O
high	O
.	O
Outliers	O
from	O
noise	O
,	O
as	O
a	O
result	O
,	O
will	O
not	O
influence	O
the	O
prediction	O
process	O
so	O
much	O
.	O

After	O
RSN	O
is	O
learned	O
,	O
we	O
can	O
use	O
RSN	O
to	O
calculate	O
the	O
similarity	O
matrix	O
of	O
testing	O
instances	O
.	O
With	O
this	O
matrix	O
,	O
several	O
clustering	O
methods	O
can	O
be	O
applied	O
to	O
extract	O
new	O
relation	O
clusters	O
.	O
Hierarchical	O
Agglomerative	O
Clustering	O
.	O
The	O
first	O
clustering	O
method	O
we	O
adopt	O
is	O
hierarchical	O
agglomerative	O
clustering	O
(	O
HAC	O
)	O
.	O
HAC	O
is	O
a	O
bottomup	O
clustering	O
algorithm	O
.	O
At	O
the	O
start	O
,	O
every	O
testing	O
instance	O
is	O
regarded	O
as	O
a	O
cluster	O
.	O
For	O
every	O
step	O
,	O
it	O
agglomerates	O
two	O
closest	O
instances	O
.	O
There	O
are	O
several	O
criteria	O
to	O
evaluate	O
the	O
distance	O
between	O
two	O
clusters	O
.	O
Here	O
,	O
we	O
adopt	O
the	O
complete	O
-	O
linkage	O
criterion	O
,	O
which	O
is	O
more	O
robust	O
to	O
extreme	O
instances	O
.	O
However	O
,	O
there	O
is	O
a	O
significant	O
shortcoming	O
of	O
HAC	O
:	O
it	O
needs	O
the	O
exact	O
number	O
of	O
clusters	O
in	O
advance	O
.	O
A	O
potential	O
solution	O
is	O
to	O
stop	O
agglomerating	O
according	O
to	O
an	O
empirical	O
distance	O
threshold	O
,	O
but	O
it	O
is	O
hard	O
to	O
determine	O
such	O
a	O
threshold	O
.	O
This	O
problem	O
leads	O
us	O
to	O
consider	O
another	O
clustering	O
algorithm	O
Louvain	O
(	O
Blondel	O
et	O
al	O
,	O
2008	O
)	O
.	O
Louvain	O
.	O
Louvain	O
is	O
a	O
graph	O
-	O
based	O
clustering	O
algorithm	O
traditionally	O
used	O
for	O
detecting	O
communities	O
.	O
To	O
construct	O
the	O
graph	O
,	O
we	O
use	O
the	O
binary	O
approximation	O
of	O
RSN	O
's	O
output	O
,	O
with	O
0	B-DatasetName
indicating	O
an	O
edge	O
between	O
two	O
nodes	O
.	O
The	O
advantage	O
of	O
Louvain	O
is	O
that	O
it	O
does	O
not	O
need	O
the	O
number	O
of	O
potential	O
clusters	O
beforehand	O
.	O
It	O
will	O
automatically	O
find	O
proper	O
sizes	O
of	O
clusters	O
by	O
optimizing	O
community	O
modularity	O
.	O
According	O
to	O
the	O
experiments	O
we	O
conduct	O
,	O
Louvain	O
performs	O
better	O
than	O
HAC	O
.	O
After	O
running	O
,	O
Louvain	O
might	O
produce	O
a	O
number	O
of	O
singleton	O
clusters	O
with	O
few	O
instances	O
.	O
It	O
is	O
not	O
proper	O
to	O
call	O
these	O
clusters	O
new	O
relation	O
types	O
,	O
so	O
we	O
label	O
these	O
instances	O
the	O
same	O
as	O
their	O
closest	O
labeled	O
neighbors	O
.	O
Finally	O
,	O
we	O
want	O
to	O
explain	O
the	O
reason	O
why	O
we	O
do	O
not	O
use	O
some	O
other	O
common	O
clustering	O
methods	O
like	O
K	O
-	O
Means	O
,	O
Mean	O
-	O
Shift	O
and	O
Ward	O
's	O
(	O
Ward	O
Jr	O
,	O
1963	O
)	O
method	O
of	O
HAC	O
:	O
these	O
methods	O
calculate	O
the	O
centroid	O
of	O
several	O
points	O
during	O
clustering	O
by	O
merely	O
averaging	O
them	O
.	O
However	O
,	O
the	O
relation	O
vectors	O
in	O
our	O
model	O
are	O
high	O
-	O
dimensional	O
,	O
and	O
the	O
distance	B-HyperparameterName
metric	I-HyperparameterName
described	O
by	O
RSN	O
is	O
non	O
-	O
linear	O
.	O
Consequently	O
,	O
it	O
is	O
not	O
proper	O
to	O
calculate	O
the	O
centroid	O
by	O
simply	O
averaging	O
the	O
vectors	O
.	O

Data	O
Sampling	O
.	O
The	O
input	O
of	O
RSN	O
should	O
be	O
a	O
pair	O
of	O
sampled	O
instances	O
.	O
For	O
the	O
unlabeled	O
set	O
,	O
the	O
only	O
possible	O
sampling	O
method	O
is	O
to	O
select	O
two	O
instances	O
randomly	O
.	O
For	O
the	O
labeled	O
set	O
,	O
however	O
,	O
random	O
selection	O
would	O
result	O
in	O
too	O
many	O
different	O
-	O
relation	O
pairs	O
,	O
and	O
cause	O
severe	O
biases	O
for	O
RSN	O
.	O
To	O
solve	O
this	O
problem	O
,	O
we	O
use	O
downsampling	O
.	O
In	O
our	O
experiments	O
,	O
we	O
fix	O
the	O
percentage	O
of	O
same	O
-	O
relation	O
pairs	O
in	O
every	O
labeled	O
data	O
batch	O
as	O
6	O
%	O
.	O
Let	O
us	O
denote	O
this	O
percentage	O
number	O
as	O
the	O
sample	O
ratio	O
for	O
convenience	O
.	O
Experimental	O
results	O
show	O
that	O
the	O
sample	O
ratio	O
decides	O
RSN	O
's	O
tendency	O
to	O
predict	O
larger	O
or	O
smaller	O
clusters	O
.	O
In	O
other	O
words	O
,	O
it	O
controls	O
the	O
granularity	O
of	O
the	O
predicted	O
relation	O
types	O
.	O
This	O
phenomenon	O
suggests	O
a	O
potential	O
application	O
of	O
our	O
model	O
in	O
hierarchical	O
relation	B-TaskName
extraction	I-TaskName
.	O
However	O
,	O
we	O
leave	O
any	O
serious	O
discussion	O
to	O
future	O
work	O
.	O
Hyperparameter	O
Settings	O
.	O
Following	O
(	O
Lin	O
et	O
al	O
,	O
2016	O
)	O
and	O
(	O
Zeng	O
et	O
al	O
,	O
2014	O
)	O
,	O
we	O
fix	O
the	O
less	O
influencing	O
hyperparameters	O
for	O
sentence	O
encoding	O
as	O
their	O
reported	O
optimal	O
values	O
.	O
For	O
word	B-TaskName
embeddings	I-TaskName
,	O
we	O
use	O
pre	O
-	O
trained	O
50	O
-	O
dimensional	O
Glove	O
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
word	B-TaskName
embeddings	I-TaskName
.	O
For	O
position	O
embeddings	O
,	O
we	O
use	O
randominitialized	O
5	O
-	O
dimensional	O
position	O
embeddings	O
.	O
During	O
training	O
,	O
all	O
the	O
embeddings	O
are	O
trainable	O
.	O
For	O
the	O
neural	O
network	O
,	O
the	O
number	O
of	O
feature	O
maps	O
in	O
the	O
convolutional	O
layer	O
is	O
230	O
.	O
The	O
filter	O
length	O
is	O
3	O
.	O
The	O
activation	B-HyperparameterName
function	I-HyperparameterName
after	O
the	O
max	O
-	O
pooling	O
layer	O
is	O
ReLU	B-MethodName
,	O
and	O
the	O
activation	O
functions	O
after	O
FC	O
layers	O
are	O
sigmoid	O
.	O
Besides	O
,	O
we	O
adopt	O
two	O
regularization	O
methods	O
in	O
the	O
CNN	O
module	O
.	O
We	O
put	O
a	O
dropout	O
layer	O
right	O
after	O
the	O
embedding	O
layer	O
as	O
(	O
Miyato	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
dropout	O
rate	O
is	O
0.2	O
.	O
We	O
also	O
impose	O
L2	B-HyperparameterName
regularization	I-HyperparameterName
on	O
the	O
convolutional	O
layer	O
and	O
the	O
FC	O
layer	O
,	O
with	O
parameters	O
of	O
0.0002	O
and	O
0.001	O
respectively	O
.	O
Hyperparameters	O
for	O
virtual	O
adversarial	O
training	O
are	O
just	O
the	O
same	O
as	O
(	O
Miyato	O
et	O
al	O
,	O
2016	O
)	O
proposed	O
.	O
At	O
the	O
same	O
time	O
,	O
major	O
hyperparameters	O
are	O
selected	O
with	O
grid	O
search	O
according	O
to	O
the	O
model	O
performance	O
on	O
a	O
validation	O
set	O
.	O
Specifically	O
,	O
the	O
validation	O
set	O
contains	O
10	O
,	O
000	O
randomly	O
chosen	O
sentence	O
pairs	O
from	O
the	O
unlabeled	O
set	O
(	O
i.e.	O
16	O
novel	O
relations	O
)	O
and	O
does	O
not	O
overlap	O
with	O
the	O
test	O
set	O
.	O
The	O
model	O
is	O
evaluated	O
according	O
to	O
the	O
precision	O
of	O
binary	O
classification	O
of	O
sentence	O
pairs	O
on	O
the	O
validation	O
set	O
,	O
which	O
is	O
an	O
estimation	O
for	O
models	O
'	O
clustering	O
ability	O
.	O
We	O
do	O
not	O
use	O
F1	B-MetricName
during	O
model	O
validation	O
because	O
the	O
clustering	O
steps	O
are	O
timeconsuming	O
.	O
For	O
optimization	O
,	O
we	O
use	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.0001	O
,	O
which	O
is	O
selected	O
from	O
{	O
0.1	O
,	O
0.01	O
,	O
0.001	O
,	O
0.0001	O
,	O
0.00001	O
}	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
100	O
selected	O
from	O
{	O
25	O
,	O
50	O
,	O
100	O
}	O
.	O
For	O
hyperparameters	O
in	O
Equation	O
9	O
and	O
Equation	O
10	O
,	O
λ	O
v	O
is	O
1.0	O
selected	O
from	O
{	O
0.1	O
,	O
0.5	O
,	O
1.0	O
,	O
2.0	O
}	O
and	O
λ	O
u	O
is	O
0.03	O
selected	O
from	O
{	O
0.01	O
,	O
0.02	O
,	O
0.03	O
,	O
0.04	O
,	O
0.05	O
}	O
.	O
For	O
baseline	O
models	O
,	O
original	O
papers	O
do	O
grid	O
search	O
for	O
all	O
possible	O
hyperparameters	O
and	O
report	O
the	O
best	O
result	O
during	O
testing	O
.	O
We	O
follow	O
their	O
settings	O
and	O
do	O
grid	O
search	O
directly	O
on	O
the	O
test	O
set	O
.	O

In	O
this	O
section	O
,	O
we	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
RSN	O
models	O
by	O
comparing	O
our	O
models	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
clustering	O
-	O
based	O
OpenRE	O
methods	O
.	O
We	O
also	O
conduct	O
ablation	O
experiments	O
to	O
detailedly	O
investigate	O
the	O
contributions	O
of	O
different	O
mechanisms	O
of	O
Semi	O
-	O
supervised	O
RSN	O
and	O
Distantly	O
-	O
supervised	O
RSN	O
.	O
Baselines	O
.	O
Conventional	O
clustering	O
-	O
based	O
OpenRE	O
models	O
usually	O
cluster	O
instances	O
by	O
either	O
clustering	O
their	O
linguistic	O
features	O
(	O
Lin	O
and	O
Pantel	O
,	O
2001	O
;	O
Yao	O
et	O
al	O
,	O
2012	O
;	O
Elsahar	O
et	O
al	O
,	O
2017	O
)	O
or	O
imposing	O
reconstruction	O
constraints	O
(	O
Yao	O
et	O
al	O
,	O
2011	O
;	O
Marcheggiani	O
and	O
Titov	O
,	O
2016	O
)	O
.	O
To	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
RSN	O
models	O
,	O
we	O
compare	O
our	O
models	O
with	O
two	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
:	O
(	O
1	O
)	O
HAC	O
with	O
re	O
-	O
weighted	O
word	B-TaskName
embeddings	I-TaskName
(	O
RW	O
-	O
HAC	O
)	O
(	O
Elsahar	O
et	O
al	O
,	O
2017	O
)	O
:	O
RW	O
-	O
HAC	O
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
feature	O
clustering	O
model	O
for	O
OpenRE	O
.	O
The	O
model	O
first	O
extracts	O
KB	O
types	O
and	O
NER	B-TaskName
tags	O
of	O
entities	O
as	O
well	O
as	O
re	O
-	O
weighted	O
word	B-TaskName
embeddings	I-TaskName
from	O
sentences	O
,	O
then	O
adopts	O
principal	O
component	O
analysis	O
(	O
PCA	B-MethodName
)	O
to	O
reduce	O
feature	O
dimensionality	O
,	O
and	O
finally	O
uses	O
HAC	O
to	O
cluster	O
the	O
concatenation	O
of	O
reduced	O
feature	O
representations	O
.	O
(	O
2	O
)	O
Discrete	O
-	O
state	O
variational	B-MethodName
autoencoder	I-MethodName
(	O
VAE	B-MethodName
)	O
(	O
Marcheggiani	O
and	O
Titov	O
,	O
2016	O
)	O
:	O
VAE	B-MethodName
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
reconstruction	O
-	O
based	O
model	O
for	O
OpenRE	O
via	O
unlabeled	O
instances	O
.	O
It	O
optimizes	O
a	O
relation	O
classifier	O
by	O
reconstructing	O
entities	O
from	O
pairing	O
entities	O
and	O
predicted	O
relation	O
types	O
.	O
Rich	O
features	O
including	O
entity	O
words	O
,	O
context	O
words	O
,	O
trigger	O
words	O
,	O
dependency	O
paths	O
,	O
and	O
context	O
POS	O
tags	O
are	O
used	O
to	O
predict	O
the	O
relation	O
type	O
.	O
RW	O
-	O
HAC	O
and	O
VAE	B-MethodName
both	O
rely	O
on	O
external	O
linguistic	O
tools	O
to	O
extract	O
rich	O
features	O
from	O
plain	O
texts	O
.	O
Specifically	O
,	O
we	O
first	O
align	O
entities	O
to	O
Wikidata	O
and	O
get	O
their	O
KB	O
types	O
.	O
Next	O
,	O
we	O
preprocess	O
the	O
instances	O
with	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
(	O
POS	O
)	O
tagging	O
,	O
named	O
-	O
entity	O
recognition	O
(	O
NER	B-TaskName
)	O
,	O
and	O
dependency	B-TaskName
parsing	I-TaskName
with	O
Stanford	O
CoreNLP	O
.	O
It	O
is	O
worth	O
noting	O
that	O
these	O
features	O
are	O
only	O
used	O
by	O
baseline	O
models	O
.	O
Our	O
models	O
,	O
in	O
contrast	O
,	O
only	O
use	O
sentences	O
and	O
entity	O
pairs	O
as	O
inputs	O
.	O
Evaluation	O
Protocol	O
.	O
In	O
evaluation	O
,	O
we	O
use	O
B	O
3	O
metric	O
(	O
Bagga	O
and	O
Baldwin	O
,	O
1998	O
)	O
as	O
the	O
scoring	O
function	O
.	O
B	O
3	O
metric	O
is	O
a	O
standard	O
measure	O
to	O
balance	O
the	O
precision	O
and	O
recall	O
of	O
clustering	O
tasks	O
,	O
and	O
is	O
commonly	O
used	O
in	O
previous	O
OpenRE	O
works	O
(	O
Marcheggiani	O
and	O
Titov	O
,	O
2016	O
;	O
Elsahar	O
et	O
al	O
,	O
2017	O
)	O
.	O
To	O
be	O
specific	O
,	O
we	O
use	O
F	O
1	O
measure	O
,	O
the	O
harmonic	O
mean	O
of	O
precision	O
and	O
recall	O
.	O
First	O
,	O
we	O
report	O
the	O
result	O
of	O
supervised	O
RSN	O
with	O
different	O
clustering	O
methods	O
.	O
Specifically	O
,	O
SN	O
represents	O
the	O
original	O
RSN	O
structure	O
,	O
HAC	O
and	O
L	O
indicate	O
HAC	O
and	O
Louvain	O
clustering	O
introduced	O
in	O
Sec	O
.	O
3.3	O
.	O
The	O
result	O
shows	O
that	O
Louvain	O
performs	O
better	O
than	O
HAC	O
,	O
so	O
in	O
the	O
following	O
experiments	O
we	O
focus	O
on	O
using	O
Louvain	O
clustering	O
.	O
Next	O
,	O
for	O
Semi	O
-	O
supervised	O
and	O
Distantlysupervised	O
RSN	O
,	O
we	O
conduct	O
various	O
combinations	O
of	O
different	O
mechanisms	O
to	O
verify	O
the	O
contribution	O
of	O
each	O
part	O
.	O
(	O
+	O
C	O
)	O
indicates	O
that	O
the	O
model	O
is	O
powered	O
up	O
with	O
conditional	O
entropy	O
minimization	O
,	O
while	O
(	O
+	O
V	O
)	O
indicates	O
that	O
the	O
model	O
is	O
pow	O
-	O
Experimental	O
Result	O
Analysis	O
.	O
Table	O
1	O
shows	O
the	O
experimental	O
results	O
,	O
from	O
which	O
we	O
can	O
observe	O
that	O
:	O
(	O
1	O
)	O
RSN	O
models	O
outperform	O
all	O
baseline	O
models	O
on	O
precision	O
,	O
recall	O
,	O
and	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
,	O
among	O
which	O
Weakly	O
-	O
supervised	O
RSN	O
(	O
SN	O
-	O
L+CV	O
)	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
.	O
This	O
indicates	O
that	O
RSN	O
is	O
capable	O
of	O
understanding	O
new	O
relations	O
'	O
semantic	O
meanings	O
within	O
sentences	O
.	O
(	O
2	O
)	O
Supervised	O
and	O
distantly	O
-	O
supervised	O
relational	O
representations	O
improve	O
clustering	O
performances	O
.	O
Compared	O
with	O
RW	O
-	O
HAC	O
,	O
SN	O
-	O
HAC	O
achieves	O
better	O
clustering	O
results	O
because	O
of	O
its	O
supervised	O
relational	O
representation	O
and	O
similarity	O
metric	O
.	O
Specifically	O
,	O
unsupervised	O
baselines	O
mainly	O
use	O
sparse	O
one	O
-	O
hot	O
features	O
.	O
RW	O
-	O
HAC	O
uses	O
word	B-TaskName
embeddings	I-TaskName
,	O
but	O
integrates	O
them	O
in	O
a	O
rulebased	O
way	O
.	O
In	O
contrast	O
,	O
RSN	O
uses	O
distributed	O
feature	O
representations	O
,	O
and	O
can	O
optimize	O
information	O
integration	O
process	O
according	O
to	O
supervision	O
.	O
(	O
3	O
)	O
Louvain	O
outperforms	O
HAC	O
for	O
clustering	O
with	O
RSN	O
,	O
comparing	O
SN	O
-	O
HAC	O
with	O
SN	O
-	O
L.	O
One	O
explanation	O
is	O
that	O
our	O
model	O
does	O
not	O
put	O
additional	O
constraints	O
on	O
the	O
prior	O
distribution	O
of	O
relational	O
vectors	O
,	O
and	O
therefore	O
the	O
relation	O
clusters	O
might	O
have	O
odd	O
shapes	O
in	O
violation	O
of	O
HAC	O
's	O
assumption	O
.	O
Moreover	O
,	O
when	O
representations	O
are	O
not	O
distinguishable	O
enough	O
,	O
forcing	O
HAC	O
to	O
find	O
finegrained	O
clusters	O
may	O
harm	O
recall	O
while	O
contributing	O
minimally	O
to	O
precision	O
.	O
In	O
practice	O
,	O
we	O
do	O
observe	O
that	O
the	O
number	O
of	O
relations	O
SN	O
-	O
L	O
extracts	O
is	O
constantly	O
less	O
than	O
the	O
true	O
number	O
16	O
.	O
(	O
4	O
)	O
Both	O
SN	O
-	O
L+V	O
and	O
SN	O
-	O
L+C	O
improve	O
the	O
performance	O
of	O
supervised	O
or	O
distantly	O
-	O
supervised	O
RSN	O
by	O
further	O
utilizing	O
unsupervised	O
corpora	O
.	O
Both	O
semi	O
-	O
supervised	O
approaches	O
bring	O
significant	O
improvements	O
for	O
F	O
1	O
scores	O
by	O
increasing	O
the	O
precision	O
and	O
recall	O
,	O
and	O
combining	O
both	O
can	O
further	O
increase	O
the	O
F	O
1	O
score	O
.	O
(	O
5	O
)	O
One	O
interesting	O
observation	O
is	O
that	O
SN	O
-	O
L+V	O
does	O
not	O
outperform	O
SN	O
-	O
L	O
so	O
much	O
on	O
FewReldistant	O
.	O
This	O
is	O
probably	O
because	O
VAT	O
on	O
the	O
noisy	O
data	O
might	O
amplify	O
the	O
noise	O
.	O
In	O
further	O
experiments	O
,	O
we	O
perform	O
VAT	O
only	O
on	O
unlabeled	O
set	O
and	O
observe	O
improvements	O
on	O
F	O
1	O
,	O
with	O
SN	O
-	O
L+V	O
from	O
45.8	O
%	O
to	O
49.2	O
%	O
and	O
SN	O
-	O
L+CV	O
from	O
52.0	O
%	O
to	O
52.6	O
%	O
,	O
which	O
proves	O
this	O
conjecture	O
.	O

The	O
goal	O
of	O
an	O
NLU	O
task	O
is	O
to	O
predict	O
the	O
label	O
y	O
of	O
the	O
given	O
input	O
instance	O
x	O
,	O
where	O
the	O
input	O
x	O
contains	O
the	O
sequence	O
of	O
tokens	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
:	O
x	O
=	O
[	O
w	O
1	O
,	O
w	O
2	O
,	O
.	O
.	O
.	O
,	O
w	O
|	O
x	O
|	O
]	O
.	O
Then	O
,	O
given	O
a	O
training	O
dataset	O
D	O
=	O
{	O
(	O
x	O
(	O
i	O
)	O
,	O
y	O
(	O
i	O
)	O
)	O
}	O
N	O
i=1	O
,	O
the	O
objective	O
is	O
to	O
maximize	O
the	O
log	B-MetricName
-	I-MetricName
likelihood	I-MetricName
as	O
follows	O
:	O
max	O
θ	B-HyperparameterName
L	O
(	O
θ	B-HyperparameterName
)	O
:	O
=	O
max	O
θ	B-HyperparameterName
(	O
x	O
,	O
y	O
)	O
∼D	O
log	O
p	O
(	O
y	O
|	O
x	O
;	O
θ	B-HyperparameterName
)	O
,	O
p	O
(	O
y	O
|	O
x	O
;	O
θ	B-HyperparameterName
)	O
=	O
g	O
(	O
H	O
;	O
θ	B-HyperparameterName
g	O
)	O
,	O
H	O
=	O
f	O
(	O
x	O
;	O
θ	B-HyperparameterName
f	O
)	O
,	O
where	O
f	O
is	O
an	O
encoder	O
of	O
the	O
PLM	O
which	O
outputs	O
contextualized	O
representation	O
H	O
from	O
x	O
,	O
and	O
g	O
is	O
a	O
decoder	O
which	O
models	O
the	O
probability	O
distribution	O
p	O
of	O
the	O
label	O
y	O
,	O
with	O
trainable	O
parameters	O
θ	B-HyperparameterName
=	O
(	O
θ	B-HyperparameterName
f	O
,	O
θ	B-HyperparameterName
g	O
)	O
.	O
If	O
the	O
LM	O
is	O
composed	O
of	O
L	O
-	O
layers	O
of	O
transformer	O
blocks	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
the	O
function	O
f	O
is	O
decomposed	O
to	O
multiple	O
functions	O
f	O
=	O
[	O
f	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
f	O
L	O
]	O
,	O
where	O
each	O
block	O
gets	O
the	O
output	O
of	O
the	O
previous	O
block	O
as	O
the	O
input	O
:	O
H	O
l	O
=	O
f	O
l	O
(	O
H	O
l−1	O
)	O
.	O
1	O
Knowledge	O
-	O
Augmented	O
Language	O
Model	O
The	O
conventional	O
learning	O
objective	O
defined	O
above	O
might	O
be	O
sufficient	O
for	O
understanding	O
the	O
texts	O
if	O
the	O
tasks	O
require	O
only	O
the	O
general	B-TaskName
knowledge	I-TaskName
stored	O
in	O
PLMs	O
.	O
However	O
,	O
it	O
is	O
suboptimal	O
for	O
tackling	O
domain	O
-	O
specific	O
tasks	O
since	O
the	O
general	B-TaskName
knowledge	I-TaskName
captured	O
by	O
the	O
parameters	O
θ	B-HyperparameterName
f	O
may	O
not	O
include	O
the	O
knowledge	O
required	O
for	O
solving	O
the	O
domain	O
-	O
specific	O
tasks	O
.	O
Thus	O
,	O
contextualizing	O
the	O
texts	O
by	O
the	O
domain	O
knowledge	O
,	O
captured	O
by	O
the	O
domain	O
-	O
specific	O
entities	O
and	O
their	O
relations	O
,	O
is	O
more	O
appropriate	O
for	O
handling	O
such	O
domain	O
-	O
specific	O
problems	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
a	O
function	O
h	O
(	O
;	O
φ	O
)	O
which	O
augments	O
PLMs	O
conditioned	O
on	O
the	O
domain	O
knowledge	O
.	O
Formally	O
,	O
the	O
objective	O
for	O
a	O
NLU	O
task	O
with	O
our	O
knowledge	O
-	O
augmented	O
LM	O
is	O
given	O
as	O
follows	O
:	O
max	O
θ	B-HyperparameterName
,	O
φ	O
L	O
(	O
θ	B-HyperparameterName
,	O
φ	O
)	O
:	O
=	O
max	O
θ	B-HyperparameterName
,	O
φ	O
(	O
x	O
,	O
y	O
)	O
∼D	O
log	O
p	O
(	O
y	O
|	O
x	O
;	O
θ	B-HyperparameterName
,	O
φ	O
)	O
,	O
p	O
(	O
y	O
|	O
x	O
;	O
θ	B-HyperparameterName
,	O
φ	O
)	O
=	O
g	O
(	O
H	O
;	O
θ	B-HyperparameterName
g	O
)	O
,	O
H	O
l	O
=	O
f	O
l	O
(	O
H	O
l−1	O
,	O
h	O
l	O
(	O
H	O
l−1	O
,	O
E	O
,	O
M	O
,	O
G	O
;	O
φ	O
)	O
;	O
θ	B-HyperparameterName
f	O
l	O
)	O
,	O
where	O
φ	O
is	O
parameters	O
for	O
the	O
function	O
h	O
,	O
E	O
is	O
the	O
set	O
of	O
entities	O
,	O
M	O
is	O
the	O
set	O
of	O
corresponding	O
mentions	O
,	O
and	O
G	O
is	O
a	O
knowledge	O
graph	O
.	O
In	O
the	O
following	O
,	O
we	O
will	O
describe	O
the	O
definition	O
of	O
the	O
knowledgerelated	O
inputs	O
E	O
,	O
M	O
,	O
G	O
,	O
and	O
the	O
details	O
of	O
h	O
(	O
,	O
φ	O
)	O
.	O
Definition	O
1	O
(	O
Entity	O
and	O
Mention	O
)	O
.	O
Given	O
a	O
sequence	O
of	O
tokens	O
x	O
=	O
[	O
w	O
1	O
,	O
.	O
.	O
.	O
,	O
w	O
|	O
x	O
|	O
]	O
,	O
let	O
E	O
be	O
a	O
set	O
of	O
entities	O
in	O
x.	O
Then	O
an	O
entity	O
e	O
E	O
is	O
composed	O
of	O
one	O
or	O
multiple	O
adjacent	O
tokens	O
within	O
the	O
input	O
text	O
:	O
[	O
w	O
m	O
α	B-HyperparameterName
,	O
.	O
.	O
.	O
,	O
w	O
m	O
ω	O
]	O
x	O
2	O
.	O
Here	O
,	O
m	O
=	O
(	O
m	O
α	B-HyperparameterName
,	O
m	O
ω	O
)	O
is	O
a	O
mention	O
that	O
denotes	O
the	O
start	O
and	O
end	O
locations	O
for	O
the	O
entity	O
within	O
the	O
input	O
tokens	O
x	O
,	O
which	O
term	O
is	O
commonly	O
used	O
for	O
defining	O
entities	O
(	O
Févry	O
et	O
al	O
,	O
2020	O
)	O
.	O
Consequently	O
,	O
for	O
each	O
given	O
input	O
x	O
(	O
i	O
)	O
,	O
there	O
are	O
a	O
set	O
of	O
entities	O
E	O
(	O
i	O
)	O
=	O
{	O
e	O
1	O
,	O
.	O
.	O
.	O
,	O
e	O
K	O
}	O
and	O
their	O
corresponding	O
mentions	O
M	O
(	O
i	O
)	O
=	O
{	O
m	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
K	O
}	O
.	O
For	O
example	O
,	O
given	O
an	O
input	O
x	O
=	O
[	O
New	O
,	O
York	O
,	O
is	O
,	O
a	O
,	O
city	O
]	O
,	O
we	O
have	O
two	O
entities	O
E	O
=	O
{	O
New_York	O
,	O
city	O
}	O
and	O
their	O
associated	O
mentions	O
M	O
=	O
{	O
(	O
1	O
,	O
2	O
)	O
,	O
(	O
4	O
,	O
4	O
)	O
}	O
.	O
We	O
further	O
construct	O
the	O
entity	O
vocabulary	O
E	O
train	O
=	O
N	O
i=1	O
E	O
(	O
i	O
)	O
,	O
which	O
consists	O
of	O
all	O
entities	O
appearing	O
in	O
the	O
training	O
dataset	O
.	O
However	O
,	O
at	O
test	O
time	O
,	O
we	O
may	O
encounter	O
unseen	O
entities	O
that	O
are	O
not	O
in	O
E	O
train	O
.	O
To	O
tackle	O
this	O
,	O
we	O
regard	O
unknown	O
entities	O
as	O
the	O
null	O
entity	O
e	O
,	O
so	O
that	O
∀e	O
E	O
train	O
∪	O
{	O
e	O
}	O
.	O
Definition	O
2	O
(	O
Entity	O
Memory	O
)	O
.	O
Given	O
a	O
set	O
of	O
all	O
entities	O
E	O
train	O
∪	O
{	O
e	O
}	O
,	O
we	O
represent	O
them	O
in	O
the	O
continuous	O
vector	O
(	O
feature	O
)	O
space	O
to	O
learn	O
meaningful	O
entity	B-TaskName
embeddings	I-TaskName
.	O
In	O
order	O
to	O
implement	O
this	O
,	O
we	O
define	O
the	O
entity	O
memory	O
E	O
R	O
(	O
|	O
E	O
train	O
|	O
+1	O
)	O
×d	O
that	O
comprises	O
of	O
an	O
entity	O
e	O
R	O
as	O
a	O
key	O
and	O
its	O
embedding	O
e	O
R	O
d	O
as	O
its	O
value	O
.	O
Also	O
,	O
to	O
access	O
the	O
value	O
in	O
the	O
entity	O
memory	O
,	O
we	O
define	O
the	O
point	O
-	O
wise	O
memory	O
access	O
function	O
EntEmbed	O
which	O
takes	O
an	O
entity	O
as	O
an	O
input	O
.	O
For	O
instance	O
,	O
e	O
=	O
EntEmbed	O
(	O
New_York	O
)	O
returns	O
the	O
embedding	O
of	O
the	O
New_York	O
entity	O
,	O
and	O
e	O
=	O
EntEmbed	O
(	O
e	O
)	O
returns	O
the	O
zero	O
embedding	O
.	O
This	O
entity	O
memory	O
E	O
is	O
the	O
part	O
of	O
the	O
parameter	O
φ	O
used	O
in	O
function	O
h.	O
Definition	O
3	O
(	O
Knowledge	O
Graph	O
)	O
.	O
Since	O
the	O
entity	O
memory	O
alone	O
can	O
not	O
represent	O
relational	O
information	O
between	O
entities	O
,	O
we	O
further	O
define	O
a	O
Knowledge	O
Graph	O
(	O
KG	O
)	O
G	O
that	O
consists	O
of	O
a	O
set	O
of	O
factual	O
triplets	O
{	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
}	O
,	O
where	O
the	O
head	O
and	O
the	O
tail	O
entities	O
,	O
h	O
and	O
t	O
,	O
are	O
the	O
elements	O
of	O
E	O
,	O
and	O
a	O
relation	O
r	O
is	O
an	O
element	O
of	O
a	O
set	O
of	O
relations	O
R	O
:	O
h	O
,	O
t	O
E	O
and	O
r	O
R.	O
We	O
assume	O
that	O
a	O
preconstructed	O
KG	O
G	O
(	O
i	O
)	O
is	O
given	O
for	O
each	O
input	O
x	O
(	O
i	O
)	O
,	O
and	O
provide	O
the	O
details	O
of	O
the	O
KGs	O
and	O
how	O
to	O
construct	O
them	O
in	O
Appendix	O
A.	O

The	O
remaining	O
problem	O
is	O
how	O
to	O
augment	O
a	O
PLM	O
by	O
conditioning	O
it	O
on	O
the	O
domain	O
-	O
specific	O
knowledge	O
,	O
through	O
the	O
function	O
h.	O
An	O
effective	O
approach	O
to	O
do	O
so	O
without	O
stacking	O
additional	O
layers	O
on	O
top	O
of	O
the	O
LM	O
is	O
to	O
interleave	O
the	O
knowledge	O
from	O
h	O
with	O
the	O
pre	O
-	O
trained	O
parameters	O
of	O
the	O
language	O
model	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
consisting	O
of	O
transformer	O
layers	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Before	O
describing	O
our	O
interleaving	O
method	O
in	O
detail	O
,	O
we	O
first	O
describe	O
the	O
Transformer	B-MethodName
architecture	O
.	O
Transformer	B-MethodName
Given	O
|	O
x	O
|	O
token	O
representations	O
H	O
l−1	O
=	O
[	O
h	O
l−1	O
1	O
,	O
.	O
.	O
.	O
,	O
h	O
l−1	O
|	O
x	O
|	O
]	O
R	O
|	O
x	O
|	O
×d	O
from	O
the	O
layer	O
l	O
−	O
1	O
where	O
d	O
is	O
the	O
embedding	O
size	O
,	O
each	O
transformer	O
block	O
outputs	O
the	O
contextualized	O
representations	O
for	O
all	O
tokens	O
.	O
In	O
detail	O
,	O
the	O
l	O
-	O
th	O
block	O
consists	O
of	O
the	O
multi	O
-	O
head	O
self	O
-	O
attention	O
(	O
Attn	O
)	O
layer	O
and	O
the	O
residual	O
feed	O
-	O
forward	O
(	O
FF	O
)	O
layer	O
as	O
follows	O
:	O
H	O
l	O
=	O
LN	O
(	O
H	O
l−1	O
+	O
Attn	O
(	O
H	O
l−1	O
)	O
)	O
F	O
F	O
(	O
Ĥ	O
l	O
)	O
=	O
σ	O
(	O
Ĥ	O
l	O
W	O
1	O
)	O
W	O
2	O
,	O
H	O
l	O
=	O
LN	O
(	O
Ĥ	O
l	O
+	O
F	O
F	O
(	O
Ĥ	O
l	O
)	O
)	O
,	O
where	O
LN	O
is	O
a	O
layer	B-MethodName
normalization	I-MethodName
(	O
Ba	O
et	O
al	O
,	O
2016	O
)	O
,	O
σ	O
is	O
an	O
activation	B-HyperparameterName
function	I-HyperparameterName
(	O
Hendrycks	O
and	O
Gimpel	O
,	O
2016	O
)	O
,	O
W	O
2	O
R	O
d	O
×d	O
and	O
W	O
1	O
R	O
d×d	O
are	O
weight	O
matrices	O
,	O
and	O
d	O
is	O
an	O
intermediate	O
hidden	O
size	O
.	O
We	O
omit	O
the	O
bias	O
term	O
for	O
brevity	O
.	O
Linear	O
Modulation	O
on	O
Transformer	B-MethodName
An	O
effective	O
yet	O
efficient	O
way	O
to	O
fuse	O
knowledge	O
from	O
different	O
sources	O
without	O
modifying	O
the	O
original	O
model	O
architecture	O
is	O
to	O
scale	O
and	O
shift	O
the	O
features	O
of	O
one	O
source	O
with	O
respect	O
to	O
the	O
data	O
from	O
another	O
source	O
.	O
This	O
scheme	O
of	O
feature	O
-	O
wise	O
affine	O
transformation	O
is	O
effective	O
on	O
various	O
tasks	O
,	O
such	O
as	O
language	O
-	O
conditioned	O
image	O
reasoning	O
or	O
style	O
-	O
transfer	O
in	O
image	B-TaskName
generation	I-TaskName
(	O
Huang	O
and	O
Belongie	O
,	O
2017	O
Motivated	O
by	O
them	O
,	O
we	O
propose	O
to	O
linearly	O
transform	O
the	O
intermediate	O
features	O
after	O
the	O
layer	B-MethodName
normalization	I-MethodName
of	O
the	O
transformer	O
-	O
based	O
PLM	O
,	O
conditioned	O
on	O
the	O
knowledge	O
sources	O
E	O
,	O
M	O
,	O
G.	O
We	O
term	O
this	O
method	O
as	O
the	O
Knowledge	O
-	O
conditioned	O
Feature	O
Modulation	O
(	O
KFM	O
)	O
,	O
described	O
as	O
follows	O
:	O
Γ	B-HyperparameterName
,	O
B	O
,	O
Γ	B-HyperparameterName
,	O
B	O
=	O
h	O
l	O
(	O
H	O
l−1	O
,	O
E	O
,	O
M	O
,	O
G	O
;	O
φ	O
)	O
,	O
H	O
l	O
=	O
Γ	B-HyperparameterName
LN	O
(	O
H	O
l−1	O
+	O
Attn	O
(	O
H	O
l−1	O
)	O
)	O
+	O
B	O
,	O
F	O
F	O
(	O
Ĥ	O
l	O
)	O
=	O
σ	O
(	O
Ĥ	O
l	O
W	O
1	O
)	O
W	O
2	O
,	O
H	O
l	O
=	O
Γ	B-HyperparameterName
LN	O
(	O
Ĥ	O
l	O
+	O
F	O
F	O
(	O
Ĥ	O
l	O
)	O
)	O
+	O
B	O
,	O
(	O
1	O
)	O
where	O
H	O
l−1	O
R	O
|	O
x	O
|	O
×d	O
is	O
the	O
matrix	O
of	O
hidden	O
representations	O
from	O
the	O
previous	O
layer	O
,	O
denotes	O
the	O
hadamard	O
(	O
element	O
-	O
wise	O
)	O
product	O
,	O
and	O
Γ	B-HyperparameterName
=	O
[	O
γ	B-HyperparameterName
1	O
,	O
.	O
.	O
.	O
,	O
γ	B-HyperparameterName
|	O
x	O
|	O
]	O
R	O
|	O
x	O
|	O
×d	O
,	O
B	O
=	O
[	O
β	B-HyperparameterName
1	O
,	O
.	O
.	O
.	O
,	O
β	B-HyperparameterName
|	O
x	O
|	O
]	O
R	O
|	O
x	O
|	O
×d	O
.	O
Γ	B-HyperparameterName
and	O
B	O
are	O
learnable	O
modulation	O
parameters	O
from	O
the	O
function	O
h	O
,	O
which	O
are	O
conditioned	O
by	O
the	O
entity	O
representation	O
.	O
For	O
instance	O
,	O
in	O
Figure	O
3	O
,	O
γ	B-HyperparameterName
and	O
β	B-HyperparameterName
for	O
token	O
'	O
New	O
'	O
are	O
conditioned	O
on	O
the	O
corresponding	O
entity	O
New_York	O
.	O
However	O
,	O
if	O
tokens	O
are	O
not	O
part	O
of	O
any	O
entity	O
(	O
e.g.	O
,	O
'	O
is	O
'	O
)	O
,	O
γ	B-HyperparameterName
and	O
β	B-HyperparameterName
for	O
such	O
tokens	O
are	O
fixed	O
to	O
1	O
and	O
0	B-DatasetName
,	O
respectively	O
.	O
One	O
notable	O
advantage	O
of	O
our	O
KFM	O
is	O
that	O
multiple	O
tokens	O
associated	O
to	O
the	O
identical	O
entity	O
are	O
affected	O
by	O
the	O
same	O
modulation	O
(	O
e.g.	O
,	O
'	O
New	O
'	O
and	O
'	O
York	O
'	O
in	O
Figure	O
3	O
)	O
,	O
which	O
allows	O
the	O
PLM	O
to	O
know	O
which	O
adjacent	O
tokens	O
are	O
in	O
the	O
same	O
entity	O
.	O
This	O
is	O
important	O
for	O
representing	O
the	O
tokens	O
of	O
the	O
domain	O
entity	O
(	O
e.g.	O
,	O
'	O
cod	O
'	O
and	O
'	O
on	O
'	O
)	O
,	O
since	O
the	O
original	O
PLM	O
might	O
regard	O
them	O
as	O
separate	O
,	O
unrelated	O
tokens	O
(	O
See	O
analysis	O
in	O
5.5	O
with	O
Figure	O
5	O
)	O
.	O
However	O
,	O
with	O
our	O
KFM	O
,	O
the	O
PLM	O
can	O
identify	O
associated	O
tokens	O
and	O
embed	O
them	O
to	O
be	O
close	O
to	O
each	O
other	O
.	O
Then	O
,	O
how	O
can	O
we	O
design	O
such	O
functional	O
operations	O
in	O
h	O
?	O
The	O
easiest	O
way	O
is	O
to	O
retrieve	O
the	O
entity	O
embedding	O
of	O
e	O
,	O
associated	O
to	O
the	O
typical	O
to	O
-	O
ken	O
,	O
from	O
the	O
entity	O
memory	O
E	O
,	O
and	O
then	O
use	O
the	O
retrieved	O
entity	O
embedding	O
as	O
the	O
input	O
to	O
obtain	O
γ	B-HyperparameterName
and	O
β	B-HyperparameterName
for	O
every	O
entity	O
(	O
See	O
Figure	O
3	O
)	O
.	O
Formally	O
,	O
for	O
each	O
entity	O
e	O
E	O
and	O
its	O
mention	O
(	O
m	O
α	B-HyperparameterName
,	O
m	O
ω	O
)	O
M	O
,	O
v	O
=	O
EntEmbed	O
(	O
e	O
)	O
(	O
2	O
)	O
γ	B-HyperparameterName
j	O
=	O
1	O
+	O
h	O
1	O
(	O
v	O
)	O
,	O
β	B-HyperparameterName
j	O
=	O
h	O
2	O
(	O
v	O
)	O
,	O
γ	B-HyperparameterName
j	O
=	O
1	O
+	O
h	O
3	O
(	O
v	O
)	O
,	O
β	B-HyperparameterName
j	O
=	O
h	O
4	O
(	O
v	O
)	O
,	O
m	O
α	B-HyperparameterName
≤	O
j	O
≤	O
m	O
ω	O
,	O
where	O
v	O
is	O
the	O
retrieved	O
entity	O
embedding	O
from	O
the	O
entity	O
memory	O
,	O
h	O
1	O
,	O
h	O
2	O
,	O
h	O
3	O
,	O
and	O
h	O
4	O
are	O
mutually	O
independent	O
Multi	O
-	O
Layer	O
Perceptrons	O
(	O
MLPs	O
)	O
which	O
return	O
a	O
zero	O
vector	O
0	B-DatasetName
if	O
e	O
=	O
e	O
.	O

Although	O
the	O
simple	O
access	O
to	O
the	O
entity	O
memory	O
can	O
retrieve	O
the	O
necessary	O
entity	B-TaskName
embeddings	I-TaskName
for	O
the	O
modulation	O
,	O
this	O
approach	O
has	O
obvious	O
drawbacks	O
as	O
it	O
not	O
only	O
fails	O
to	O
reflect	O
the	O
relations	O
with	O
other	O
entities	O
,	O
but	O
also	O
regards	O
unseen	O
entities	O
as	O
the	O
same	O
null	O
entity	O
e	O
.	O
If	O
so	O
,	O
all	O
unseen	O
entities	O
are	O
inevitably	O
modulated	O
by	O
the	O
same	O
parameters	O
even	O
if	O
they	O
have	O
essentially	O
different	O
meaning	O
.	O
To	O
tackle	O
these	O
limitations	O
,	O
we	O
further	O
consider	O
the	O
relational	O
information	O
between	O
two	O
entities	O
that	O
are	O
linked	O
with	O
a	O
particular	O
relation	O
.	O
For	O
example	O
,	O
the	O
entity	O
New_York	O
alone	O
will	O
not	O
give	O
meaningful	O
information	O
.	O
However	O
,	O
with	O
two	O
associated	O
facts	O
(	O
New_York	O
,	O
instance	O
of	O
,	O
city	O
)	O
and	O
(	O
New_York	O
,	O
country	O
,	O
USA	O
)	O
,	O
it	O
is	O
clear	O
that	O
New_York	O
is	O
a	O
city	O
in	O
the	O
USA	O
.	O
Motivated	O
by	O
this	O
observation	O
,	O
we	O
propose	O
Relational	O
Retrieval	O
which	O
leverages	O
a	O
KG	O
G	O
to	O
retrieve	O
entity	B-TaskName
embeddings	I-TaskName
from	O
the	O
memory	O
,	O
according	O
to	O
the	O
relations	O
defined	O
in	O
the	O
given	O
KG	O
(	O
See	O
Figure	O
3	O
,	O
right	O
)	O
.	O
More	O
specifically	O
,	O
our	O
goal	O
is	O
to	O
effectively	O
utilize	O
the	O
relations	O
among	O
entities	O
in	O
G	O
,	O
to	O
improve	O
the	O
EntEmbed	O
function	O
in	O
equation	O
2	O
.	O
We	O
tackle	O
this	O
objective	O
by	O
utilizing	O
a	O
Graph	O
Neural	O
Network	O
(	O
GNN	O
)	O
which	O
learns	O
feature	O
representations	O
of	O
each	O
node	O
using	O
a	O
neighborhood	O
aggregation	O
scheme	O
(	O
Hamilton	O
et	O
al	O
,	O
2017	O
)	O
,	O
as	O
follows	O
:	O
v	O
=	O
UPDATE	O
(	O
EntEmbed	O
(	O
e	O
)	O
,	O
AGG	O
(	O
{	O
EntEmbed	O
(	O
ê	O
)	O
:	O
∀ê	O
N	O
(	O
e	O
;	O
G	O
)	O
}	O
)	O
)	O
,	O
where	O
N	O
(	O
e	O
;	O
G	O
)	O
is	O
a	O
set	O
of	O
neighboring	O
entities	O
of	O
the	O
entity	O
e	O
,	O
AGG	O
is	O
the	O
function	O
that	O
aggregates	O
embeddings	O
of	O
neighboring	O
entities	O
of	O
e	O
,	O
and	O
UPDATE	O
is	O
the	O
function	O
that	O
updates	O
the	O
representation	O
of	O
e	O
with	O
the	O
aggregated	O
messages	O
from	O
AGG	O
.	O
However	O
,	O
simple	O
aggregation	O
(	O
e.g.	O
,	O
mean	O
)	O
can	O
not	O
reflect	O
the	O
relative	O
importance	O
on	O
neighboring	O
nodes	O
,	O
thus	O
we	O
consider	O
the	O
attentive	O
scheme	O
(	O
Velickovic	O
et	O
al	O
,	O
2018	O
;	O
Brody	O
et	O
al	O
,	O
2021	O
)	O
for	O
neighborhood	O
aggregation	O
,	O
to	O
allocate	O
weights	O
to	O
the	O
target	O
entity	O
's	O
neighbors	O
by	O
their	O
importance	O
.	O
This	O
scheme	O
is	O
helpful	O
in	O
filtering	O
out	O
less	O
useful	O
relations	O
.	O
Formally	O
,	O
we	O
first	O
define	O
a	O
scoring	O
function	O
ψ	O
that	O
calculates	O
a	O
score	O
for	O
every	O
triplet	O
(	O
e	O
i	O
,	O
r	O
ij	O
,	O
e	O
j	O
)	O
,	O
which	O
is	O
then	O
used	O
to	O
weigh	O
each	O
node	O
during	O
aggregation	O
:	O
e	O
i	O
=	O
EntEmbed	O
(	O
e	O
i	O
)	O
,	O
e	O
j	O
=	O
EntEmbed	O
(	O
e	O
j	O
)	O
,	O
e	O
*	O
=	O
[	O
e	O
i	O
r	O
ij	O
e	O
j	O
h	O
e	O
i	O
]	O
,	O
ψ	O
(	O
e	O
i	O
,	O
r	O
ij	O
,	O
e	O
j	O
,	O
h	O
e	O
i	O
)	O
=	O
a	O
σ	O
(	O
W	O
e	O
*	O
)	O
,	O
where	O
σ	O
is	O
a	O
nonlinear	O
activation	O
,	O
e	O
*	O
R	O
4d	O
is	O
concatenated	O
vector	O
where	O
denotes	O
the	O
concatenation	O
,	O
a	O
R	O
d	O
and	O
W	O
R	O
d×4d	O
are	O
learnable	O
parameters	O
,	O
r	O
ij	O
R	O
d	O
is	O
a	O
embedding	O
of	O
the	O
relation	O
,	O
and	O
h	O
e	O
i	O
R	O
d	O
is	O
a	O
context	O
representation	O
of	O
the	O
entity	O
e	O
i	O
obtained	O
from	O
the	O
intermediate	O
hidden	O
states	O
of	O
the	O
LM	O
3	O
.	O
The	O
scores	O
obtained	O
from	O
ψ	O
are	O
normalized	O
across	O
all	O
neighbors	O
e	O
j	O
N	O
(	O
e	O
i	O
;	O
G	O
)	O
with	O
softmax	B-MethodName
:	O
α	B-HyperparameterName
ij	O
=	O
softmax	B-MethodName
(	O
ψ	O
(	O
e	O
i	O
,	O
r	O
ij	O
,	O
e	O
j	O
)	O
)	O
=	O
exp	O
(	O
ψ	O
(	O
e	O
i	O
,	O
r	O
ij	O
,	O
e	O
j	O
)	O
)	O
e	O
j	O
N	O
(	O
e	O
i	O
;	O
G	O
)	O
exp	O
(	O
ψ	O
(	O
e	O
i	O
,	O
r	O
ij	O
,	O
e	O
j	O
)	O
)	O
.	O
Then	O
,	O
we	O
update	O
the	O
entity	O
embedding	O
with	O
a	O
weighted	O
average	O
of	O
the	O
neighboring	O
nodes	O
with	O
α	B-HyperparameterName
as	O
an	O
attention	O
coefficient	O
,	O
denoted	O
as	O
follows	O
:	O
v	O
=	O
UPDATE	O
e	O
j	O
N	O
(	O
e	O
i	O
;	O
G	O
)	O
α	B-HyperparameterName
ij	O
e	O
j	O
.	O
(	O
3	O
)	O
1	O
m	O
ω	O
−m	O
α	B-HyperparameterName
+1	O
m	O
ω	O
i	O
=	O
m	O
α	B-HyperparameterName
h	O
l−1	O
i	O
By	O
replacing	O
the	O
EntEmbed	O
function	O
in	O
equation	O
2	O
with	O
the	O
above	O
GNN	O
in	O
equation	O
3	O
,	O
we	O
now	O
represent	O
each	O
entity	O
with	O
its	O
relational	O
information	O
in	O
KG	O
.	O
This	O
relational	O
retrieval	O
has	O
several	O
advantages	O
over	O
simple	O
retrieval	O
of	O
a	O
single	O
entity	O
from	O
the	O
entity	O
memory	O
.	O
First	O
,	O
the	O
relational	O
retrieval	O
with	O
KG	O
can	O
consider	O
richer	O
interactions	O
among	O
entities	O
,	O
as	O
described	O
in	O
Figure	O
3	O
.	O
In	O
addition	O
,	O
we	O
can	O
naturally	O
represent	O
an	O
unseen	O
entity	O
-	O
which	O
is	O
not	O
seen	O
during	O
training	O
but	O
appears	O
at	O
test	O
time	O
-	O
through	O
neighboring	O
aggregation	O
,	O
which	O
is	O
impossible	O
only	O
with	O
the	O
entity	O
memory	O
.	O
In	O
Figure	O
2	O
,	O
we	O
provide	O
an	O
illustrative	O
example	O
of	O
the	O
unseen	O
entity	O
representation	O
,	O
where	O
the	O
unseen	O
entity	O
restenosis	O
is	O
represented	O
with	O
a	O
weighted	O
sum	O
of	O
representations	O
of	O
its	O
neighboring	O
entities	O
myocardial_infarction	O
,	O
asthma	O
,	O
and	O
pethidine	O
,	O
which	O
is	O
beneficial	O
when	O
the	O
set	O
of	O
entities	O
for	O
training	O
and	O
test	O
datasets	O
have	O
small	O
overlaps	O
.	O

We	O
evaluate	O
our	O
model	O
on	O
two	O
NLU	O
tasks	O
:	O
Question	B-TaskName
Answering	I-TaskName
(	O
QA	O
)	O
and	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
.	O
For	O
QA	O
,	O
we	O
use	O
three	O
domain	O
-	O
specific	O
datasets	O
:	O
NewsQA	B-DatasetName
(	O
News	O
,	O
Trischler	O
et	O
al	O
,	O
2017	O
)	O
and	O
two	O
subsets	O
(	O
Relation	O
,	O
Medication	O
)	O
of	O
EMRQA	B-DatasetName
(	O
Clinical	O
,	O
Pampari	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
use	O
the	O
Exact	O
-	O
Match	O
(	O
EM	B-MetricName
)	O
and	O
the	O
F1	B-MetricName
score	I-MetricName
as	O
evaluation	O
metrics	O
.	O
For	O
NER	B-TaskName
,	O
we	O
use	O
three	O
datasets	O
from	O
different	O
domains	O
,	O
namely	O
CoNLL	O
-	O
2003	O
(	O
News	O
,	O
Sang	O
andMeulder	O
,	O
2003	O
)	O
,	O
WNUT	O
-	O
17	O
(	O
Social	O
Media	O
,	O
Derczynski	B-DatasetName
et	O
al	O
,	O
2017	O
)	O
and	O
NCBI	B-DatasetName
-	I-DatasetName
Disease	I-DatasetName
(	O
Biomedical	O
,	O
Dogan	O
et	O
al	O
,	O
2014	O
)	O
.	O
We	O
use	O
the	O
F1	B-MetricName
score	I-MetricName
as	O
the	O
evaluation	O
metric	O
.	O
We	O
report	O
statistics	O
and	O
detailed	O
descriptions	O
of	O
each	O
dataset	O
in	O
Appendix	O
B.2	O
.	O

A	O
direct	O
baseline	O
of	O
our	O
KALA	O
is	O
the	O
adaptive	O
pre	O
-	O
training	O
,	O
which	O
is	O
commonly	O
used	O
to	O
adapt	O
the	O
PLM	O
independent	O
to	O
the	O
choice	O
of	O
a	O
domain	O
and	O
task	O
.	O
Also	O
,	O
to	O
compare	O
ours	O
against	O
a	O
more	O
powerful	O
baseline	O
,	O
we	O
modify	O
a	O
recent	O
method	O
that	O
alleviates	O
forgetting	O
of	O
PLM	O
during	O
fine	O
-	O
tuning	O
.	O
Details	O
for	O
each	O
baseline	O
we	O
use	O
are	O
described	O
as	O
follows	O
:	O
1	O
.	O
Vanilla	O
Fine	O
-	O
Tuning	O
(	O
FT	O
)	O
:	O
A	O
baseline	O
that	O
directly	O
fine	O
-	O
tunes	O
the	O
LM	O
on	O
downstream	O
tasks	O
.	O
2	O
.	O
Fine	O
-	O
Tuning	O
+	O
more	O
params	B-MetricName
:	O
A	O
baseline	O
with	O
one	O
more	O
transformer	O
layer	O
at	O
the	O
end	O
of	O
the	O
means	O
and	O
standard	O
deviations	O
of	O
performances	O
over	O
five	O
different	O
runs	O
with	O
Exact	B-MetricName
Match	I-MetricName
/	O
F1	B-MetricName
score	I-MetricName
as	O
a	O
metric	O
.	O
The	O
numbers	O
in	O
bold	O
fonts	O
denote	O
the	O
best	O
score	O
.	O
†	O
indicates	O
the	O
method	O
under	O
an	O
extremely	O
high	O
computational	O
resource	O
setting	O
(	O
See	O
Figure	O
1	O
)	O
.	O
LM	O
.	O
We	O
use	O
this	O
baseline	O
to	O
show	O
that	O
the	O
performance	O
gain	O
of	O
our	O
model	O
does	O
not	O
come	O
from	O
the	O
use	O
of	O
additional	O
parameters	O
.	O
6	O
.	O
KALA	O
(	O
pointwise	O
)	O
:	O
A	O
variant	O
of	O
KALA	O
that	O
only	O
uses	O
the	O
entity	O
memory	O
and	O
does	O
not	O
use	O
the	O
knowledge	B-TaskName
graphs	I-TaskName
.	O
7	O
.	O
KALA	O
(	O
relational	O
)	O
:	O
Our	O
full	O
model	O
that	O
uses	O
KGs	O
to	O
perform	O
relational	O
retrieval	O
from	O
the	O
entity	O
memory	O
.	O

Performance	O
on	O
QA	O
and	O
NER	B-TaskName
tasks	O
On	O
both	O
extractive	O
QA	O
and	O
NER	B-TaskName
tasks	O
,	O
our	O
KALA	O
outperforms	O
all	O
baselines	O
,	O
including	O
TAPT	O
and	O
TAPT+RedcAdam	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
;	O
,	O
as	O
shown	O
in	O
Table	O
1	O
and	O
2	O
.	O
These	O
results	O
show	O
that	O
our	O
KALA	O
is	O
highly	O
effective	O
for	O
the	O
language	O
model	O
adaptation	O
task	O
.	O
KALA	O
also	O
largely	O
outperforms	O
DAPT	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
which	O
is	O
trained	O
with	O
extra	O
data	O
and	O
requires	O
a	O
significantly	O
higher	O
computational	O
cost	O
compare	O
to	O
KALA	O
(	O
See	O
Figure	O
1	O
for	O
the	O
plot	O
of	O
efficiency	O
,	O
discussed	O
in	O
Section	O
5.3	O
)	O
.	O
Effect	O
of	O
Using	O
more	O
Parameters	O
One	O
may	O
suspect	O
whether	O
the	O
performance	O
of	O
our	O
KALA	O
comes	O
from	O
the	O
increment	O
of	O
parameters	O
.	O
However	O
,	O
the	O
experimental	O
results	O
in	O
Table	O
1	O
and	O
2	O
show	O
that	O
increasing	O
the	O
parameters	O
for	O
PLM	O
during	O
fine	O
-	O
tuning	O
(	O
+	O
more	O
params	B-MetricName
)	O
yields	O
marginal	O
performance	O
improvements	O
over	O
naive	O
fine	O
-	O
tuning	O
.	O
This	O
result	O
confirms	O
that	O
the	O
performance	O
improvement	O
of	O
KALA	O
is	O
not	O
due	O
to	O
the	O
increased	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
.	O

We	O
first	O
analyze	O
the	O
effect	O
of	O
feature	O
modulation	O
parameters	O
(	O
i.e.	O
,	O
gamma	B-HyperparameterName
and	O
beta	B-HyperparameterName
)	O
in	O
transformers	O
by	O
ablating	O
a	O
subset	O
of	O
them	O
in	O
Table	O
3	O
,	O
in	O
which	O
we	O
observe	O
that	O
using	O
both	O
gamma	B-HyperparameterName
and	O
beta	B-HyperparameterName
after	O
both	O
layer	B-MethodName
normalization	I-MethodName
on	O
a	O
transformer	O
layer	O
obtains	O
the	O
best	O
performance	O
.	O
Architectural	O
Variants	O
We	O
now	O
examine	O
the	O
effectiveness	O
of	O
the	O
proposed	O
knowledge	O
conditioning	O
scheme	O
in	O
our	O
KALA	O
framework	O
.	O
To	O
this	O
end	O
,	O
we	O
use	O
or	O
adapt	O
the	O
knowledge	O
integration	O
methods	O
from	O
previous	O
literature	O
,	O
to	O
compare	O
their	O
effectiveness	O
.	O
Specifically	O
,	O
we	O
couple	O
the	O
following	O
five	O
components	O
with	O
KALA	O
:	O
Entity	O
-	O
as	O
-	O
Experts	O
(	O
Févry	O
et	O
al	O
,	O
2020	O
)	O
,	O
Adapter	B-MethodName
(	O
Houlsby	O
et	O
al	O
,	O
2019	O
)	O
,	O
KT	O
-	O
Net	O
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
,	O
ERNIE	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
ERICA	O
(	O
Qin	O
et	O
al	O
,	O
2021	O
)	O
.	O
Note	O
that	O
,	O
most	O
of	O
them	O
were	O
proposed	O
for	O
improving	O
pre	O
-	O
training	O
from	O
scratch	O
,	O
while	O
we	O
adapt	O
them	O
for	O
fine	O
-	O
tuning	O
under	O
our	O
KALA	O
framework	O
(	O
The	O
details	O
are	O
given	O
in	O
Appendix	O
B.4	O
)	O
.	O
As	O
shown	O
in	O
Table	O
4	O
,	O
our	O
KFM	O
used	O
in	O
KALA	O
outperforms	O
all	O
variants	O
,	O
demonstrating	O
the	O
effectiveness	O
of	O
feature	O
modulation	O
in	O
the	O
middle	O
of	O
transformer	O
layers	O
for	O
fine	O
-	O
tuning	O
.	O

Figure	O
1	O
illustrates	O
the	O
performance	O
and	O
training	O
FLOPs	O
of	O
KALA	O
against	O
baselines	O
on	O
the	O
NewsQA	B-DatasetName
dataset	O
.	O
We	O
observe	O
that	O
the	O
performance	O
of	O
TAPT	O
decreases	O
with	O
the	O
increased	O
number	B-HyperparameterName
of	I-HyperparameterName
iterations	I-HyperparameterName
,	O
which	O
could	O
be	O
due	O
to	O
forgetting	O
of	O
the	O
knowledge	O
from	O
the	O
PLM	O
.	O
On	O
the	O
other	O
hand	O
,	O
DAPT	O
,	O
while	O
not	O
suffering	O
from	O
performance	O
loss	B-MetricName
,	O
requires	O
huge	O
computational	O
costs	O
as	O
it	O
trains	O
on	O
112	O
times	O
larger	O
data	O
for	O
further	O
pre	O
-	O
training	O
(	O
See	O
Appendix	O
B.3	O
for	O
detailed	O
explanations	O
on	O
training	O
data	O
)	O
.	O
On	O
the	O
other	O
hand	O
,	O
our	O
KALA	O
outperforms	O
DAPT	O
without	O
using	O
external	O
data	O
,	O
while	O
requiring	O
17	O
times	O
fewer	O
computational	O
costs	O
,	O
which	O
shows	O
that	O
KALA	O
is	O
not	O
only	O
effective	O
but	O
also	O
highly	O
efficient	O
.	O
To	O
further	O
compare	O
the	O
efficiency	O
in	O
various	O
aspects	O
,	O
we	O
report	O
GPU	O
memory	O
,	O
training	O
wall	O
time	O
,	O
and	O
training	O
FLOPs	O
for	O
baselines	O
and	O
ours	O
in	O
Table	O
6	O
.	O
Through	O
this	O
,	O
we	O
verify	O
that	O
our	O
KALA	O
is	O
more	O
efficient	O
to	O
train	O
for	O
language	O
model	O
adaptation	O
settings	O
than	O
baselines	O
.	O
Note	O
that	O
the	O
resource	O
requirement	O
of	O
KALA	O
could	O
be	O
further	O
reduced	O
by	O
adjusting	O
the	O
size	O
of	O
the	O
entity	O
memory	O
(	O
e.g.	O
,	O
removing	O
less	O
frequent	O
entities	O
)	O
.	O
Therefore	O
,	O
to	O
show	O
the	O
flexibility	O
of	O
our	O
KALA	O
on	O
the	O
typical	O
resource	O
constraint	O
,	O
we	O
provide	O
the	O
experimental	O
results	O
on	O
two	O
different	O
settings	O
(	O
i.e.	O
,	O
tuning	O
the	O
number	O
of	O
entities	O
in	O
the	O
entity	O
memory	O
)	O
-	O
KALA	O
with	O
memory	O
size	O
of	O
200	O
and	O
62.8k	O
(	O
full	O
memory	O
)	O
in	O
Appendix	O
C.6	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
to	O
use	O
the	O
Knowledge	O
Graph	O
(	O
KG	O
)	O
that	O
can	O
define	O
the	O
relational	O
information	O
among	O
entities	O
that	O
only	O
appear	O
in	O
each	O
dataset	O
.	O
However	O
,	O
unfortunately	O
,	O
most	O
of	O
the	O
task	O
datasets	O
do	O
not	O
contain	O
such	O
relational	O
facts	O
on	O
its	O
context	O
,	O
thus	O
we	O
need	O
to	O
construct	O
them	O
manually	O
to	O
obtain	O
the	O
knowledge	O
graph	O
.	O
In	O
this	O
section	O
,	O
we	O
explain	O
the	O
way	O
of	O
constructing	O
the	O
knowledge	O
graph	O
that	O
we	O
used	O
,	O
consisting	O
of	O
facts	O
of	O
entities	O
for	O
each	O
context	O
in	O
the	O
task	O
dataset	O
.	O
Relation	B-TaskName
extraction	I-TaskName
is	O
the	O
way	O
how	O
we	O
obtain	O
the	O
factual	O
knowledge	O
from	O
the	O
text	O
of	O
the	O
target	O
dataset	O
.	O
To	O
do	O
so	O
,	O
we	O
first	O
need	O
to	O
extract	O
entities	O
and	O
their	O
corresponding	O
mentions	O
from	O
the	O
text	O
,	O
and	O
then	O
link	O
it	O
to	O
the	O
existing	O
entities	O
in	O
wikidata	O
(	O
Vrandecic	O
and	O
Krötzsch	O
,	O
2014	O
)	O
.	O
In	O
order	O
to	O
do	O
this	O
,	O
we	O
use	O
the	O
existing	O
library	O
named	O
as	O
spaCy	O
5	O
,	O
and	O
opensourced	O
implementation	O
of	O
Entity	O
Linker	O
6	O
.	O
To	O
sum	O
up	O
,	O
in	O
our	O
work	O
,	O
a	O
set	O
of	O
entities	O
E	O
(	O
i	O
)	O
and	O
corresponding	O
mentions	O
M	O
(	O
i	O
)	O
for	O
the	O
given	O
input	O
x	O
(	O
i	O
)	O
are	O
obtained	O
through	O
this	O
step	O
.	O
Regarding	O
a	O
concrete	O
example	O
,	O
please	O
see	O
format	O
(	O
a	O
)	O
in	O
Figure	O
6	O
.	O
In	O
the	O
example	O
,	O
"	O
Text	O
"	O
indicates	O
the	O
entity	O
mention	O
within	O
the	O
input	O
x	O
,	O
the	O
"	O
start	O
"	O
and	O
"	O
end	O
"	O
indicates	O
its	O
mention	O
position	O
denoted	O
as	O
(	O
m	O
α	B-HyperparameterName
,	O
m	O
ω	O
)	O
,	O
and	O
"	O
i	O
d	O
"	O
indicates	O
the	O
wikidata	O
i	O
d	O
for	O
the	O
entity	O
identification	O
used	O
in	O
the	O
next	O
step	O
.	O
To	O
extract	O
the	O
relation	O
among	O
entities	O
that	O
we	O
obtained	O
above	O
,	O
we	O
use	O
the	O
scheme	O
of	O
Relation	B-TaskName
Extraction	I-TaskName
(	O
RE	O
)	O
.	O
In	O
other	O
words	O
,	O
we	O
use	O
the	O
trained	O
5	O
https://spacy.io/	O
6	O
https://github.com/egerber/spaCy	O
-	O
entity	O
-	O
linker	O
RE	O
model	O
to	O
build	O
our	O
own	O
knowledge	O
base	O
(	O
KB	O
)	O
instead	O
of	O
using	O
the	O
existing	O
KG	O
directly	O
from	O
the	O
existing	O
general	O
-	O
domain	O
KB	O
7	O
.	O
Specifically	O
,	O
we	O
first	O
fine	O
-	O
tune	O
the	O
BERT	B-MethodName
-	O
base	O
model	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
for	O
2	O
epochs	O
with	O
600k	O
distantly	O
supervised	O
data	O
used	O
in	O
Qin	O
et	O
al	O
(	O
2021	O
)	O
,	O
where	O
the	O
Wikipedia	O
document	O
and	O
the	O
Wikidata	O
triplets	O
are	O
aligned	O
.	O
Then	O
,	O
we	O
use	O
the	O
fine	O
-	O
tuned	O
BERT	B-MethodName
model	O
to	O
extract	O
the	O
relations	O
between	O
entity	O
pairs	O
in	O
the	O
text	O
.	O
We	O
use	O
the	O
model	O
with	O
a	O
simple	O
bilinear	O
layer	O
on	O
top	O
of	O
it	O
,	O
which	O
is	O
widely	O
used	O
scheme	O
in	O
the	O
relation	B-TaskName
extraction	I-TaskName
literature	O
(	O
Yao	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
an	O
example	O
of	O
the	O
extracted	O
fact	O
,	O
please	O
see	O
format	O
(	O
b	O
)	O
in	O
Figure	O
6	O
.	O
In	O
the	O
example	O
,	O
"	O
h	O
"	O
denotes	O
the	O
wikidata	O
i	O
d	O
of	O
the	O
head	O
entity	O
,	O
"	O
r	O
"	O
denotes	O
the	O
wikidata	O
i	O
d	O
of	O
the	O
extracted	O
relation	O
,	O
and	O
"	O
t	O
"	O
denotes	O
the	O
wikidata	O
i	O
d	O
of	O
the	O
tail	O
entity	O
.	O
In	O
the	O
relation	B-TaskName
extraction	I-TaskName
,	O
the	O
model	O
returns	O
the	O
categorical	O
distribution	O
over	O
the	O
top	O
100	O
frequent	O
relations	O
.	O
In	O
general	O
,	O
the	O
relation	O
of	O
top	O
-	O
1	O
probability	O
is	O
used	O
as	O
the	O
relation	O
for	O
the	O
corresponding	O
entity	O
pair	O
.	O
However	O
,	O
this	O
approach	O
sometimes	O
results	O
in	O
predicting	O
no_relation	O
on	O
most	O
entity	O
pairs	O
.	O
Thus	O
,	O
to	O
obtain	O
more	O
relations	O
,	O
we	O
further	O
use	O
the	O
relation	O
of	O
top	O
-	O
2	O
probability	O
in	O
the	O
case	O
where	O
no_relation	O
has	O
a	O
top	O
-	O
1	O
probability	O
but	O
the	O
top	O
-	O
2	O
probability	O
is	O
larger	O
than	O
a	O
certain	O
threshold	O
(	O
e.g.	O
,	O
>	O
0.1	O
)	O
.	O
In	O
Figure	O
6	O
,	O
we	O
summarize	O
our	O
KG	O
construction	O
pipeline	O
.	O
In	O
Table	O
8	O
,	O
we	O
report	O
the	O
hyperparameters	O
related	O
to	O
our	O
KG	O
construction	O
.	O

We	O
use	O
the	O
Pytorch	O
(	O
Paszke	O
et	O
al	O
,	O
2019	O
)	O
for	O
the	O
implementation	O
of	O
all	O
models	O
.	O
Also	O
,	O
to	O
easily	O
implement	O
the	O
language	O
model	O
,	O
we	O
use	O
the	O
huggingface	O
library	O
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
containing	O
various	O
transformer	O
-	O
based	O
pre	O
-	O
trained	O
language	O
models	O
(	O
PLMs	O
)	O
and	O
their	O
checkpoints	O
.	O
Details	O
for	O
KALA	O
In	O
this	O
paragraph	O
,	O
we	O
describe	O
the	O
implementation	O
details	O
of	O
the	O
components	O
,	O
such	O
as	O
four	O
linear	O
layers	O
in	O
the	O
proposed	O
KFM	O
,	O
architectural	O
specifications	O
in	O
the	O
attentionbased	O
GNN	O
,	O
and	O
initialization	O
of	O
both	O
the	O
entity	O
memory	O
and	O
relational	O
embeddings	O
,	O
in	O
the	O
following	O
.	O
In	O
terms	O
of	O
the	O
functions	O
h	O
1	O
,	O
h	O
2	O
,	O
h	O
3	O
,	O
and	O
h	O
4	O
in	O
the	O
KFM	O
of	O
Equation	O
2	O
,	O
we	O
use	O
two	O
linear	O
layers	O
with	O
the	O
ReLU	B-MethodName
(	O
Nair	O
and	O
Hinton	O
,	O
2010	O
)	O
activation	B-HyperparameterName
function	I-HyperparameterName
,	O
where	O
the	O
dimension	O
is	O
set	O
to	O
768	O
.	O
For	O
relational	O
retrieval	O
,	O
we	O
implement	O
the	O
novel	O
GNN	O
model	O
based	O
on	O
GATv2	B-MethodName
(	O
Brody	O
et	O
al	O
,	O
2021	O
)	O
provided	O
by	O
the	O
torch	O
-	O
geometric	O
package	O
(	O
Fey	O
and	O
Lenssen	O
,	O
2019	O
)	O
.	O
Specifically	O
,	O
we	O
stack	O
two	O
GNN	O
layers	O
with	O
the	O
RELU	B-MethodName
activation	B-HyperparameterName
function	I-HyperparameterName
and	O
also	O
use	O
the	O
dropout	O
with	O
a	O
probability	O
of	O
0.1	O
.	O
For	O
attention	O
in	O
our	O
GNN	O
,	O
we	O
mask	O
the	O
nodes	O
of	O
the	O
null	O
entity	O
,	O
so	O
that	O
the	O
attention	O
score	O
becomes	O
zero	O
for	O
them	O
.	O
Moreover	O
,	O
to	O
obtain	O
the	O
context	O
representation	O
of	O
the	O
entity	O
(	O
See	O
Footnote	O
3	O
in	O
the	O
main	O
paper	O
)	O
used	O
in	O
the	O
GNN	O
attention	O
,	O
we	O
use	O
the	O
scatter	O
operation	O
8	O
for	O
reduced	O
computational	O
cost	O
.	O
For	O
Entity	O
Memory	O
,	O
we	O
experimentally	O
found	O
that	O
initializing	O
the	O
embeddings	O
of	O
the	O
entity	O
memory	O
with	O
the	O
contextualized	O
features	O
obtained	O
from	O
8	O
https://github.com/rusty1s/pytorch_scatter	O
the	O
pre	O
-	O
trained	O
language	O
model	O
could	O
be	O
helpful	O
.	O
Therefore	O
,	O
the	O
dimension	O
of	O
the	O
entity	O
embedding	O
is	O
set	O
to	O
the	O
same	O
as	O
the	O
language	O
model	O
d	O
=	O
768	O
.	O
For	O
relation	O
embeddings	O
,	O
we	O
randomly	O
initialize	O
them	O
,	O
where	O
the	O
dimension	O
size	O
is	O
set	O
to	O
128	O
.	O
Location	O
of	O
KLM	O
in	O
the	O
PLM	O
Note	O
that	O
,	O
the	O
number	O
and	O
location	O
of	O
the	O
KFM	O
layers	O
inside	O
the	O
PLM	O
are	O
hyperparameters	O
.	O
However	O
,	O
we	O
empirically	O
found	O
that	O
inserting	O
one	O
to	O
three	O
KFM	O
layers	O
at	O
the	O
end	O
of	O
the	O
PLM	O
(	O
i.e.	O
,	O
after	O
the	O
9th	O
-	O
11th	O
layers	O
of	O
the	O
BERT	B-MethodName
-	O
base	O
language	O
model	O
)	O
is	O
beneficial	O
to	O
the	O
performance	O
(	O
See	O
Appendix	O
C.4	O
for	O
experiments	O
on	O
diverse	O
layer	O
locations	O
)	O
.	O

All	O
experiments	O
are	O
constrained	O
to	O
be	O
done	O
with	O
a	O
single	O
12	O
GB	O
Geforce	O
RTX	O
2080	O
Ti	O
GPU	O
for	O
fairness	O
in	O
terms	O
of	O
memory	O
and	O
the	O
availability	O
on	O
the	O
academic	O
budget	O
,	O
except	O
for	O
the	O
DAPT	O
and	O
generative	O
QA	O
which	O
use	O
a	O
single	O
48	O
GB	O
Quadro	O
8000	O
GPU	O
.	O
KALA	O
training	O
needs	O
3	O
hours	O
in	O
wall	O
time	O
with	O
a	O
single	O
GPU	O
.	O
For	O
all	O
experiments	O
,	O
we	O
select	O
the	O
best	O
checkpoint	O
on	O
the	O
validation	O
set	O
.	O
For	O
the	O
summary	O
of	O
training	O
setups	O
,	O
please	O
see	O
Table	O
10	O
and	O
12	O
.	O
Fine	O
-	O
tuning	O
Setup	O
In	O
the	O
following	O
three	O
paragraphs	O
,	O
we	O
explain	O
the	O
setting	O
of	O
fine	O
-	O
tuning	O
for	O
QA	O
,	O
NER	B-TaskName
,	O
and	O
generative	O
QA	O
tasks	O
.	O
For	O
all	O
experiments	O
on	O
extractive	O
QA	O
tasks	O
,	O
we	O
fine	O
-	O
tune	O
the	O
Pre	O
-	O
trained	O
Language	O
Model	O
(	O
PLM	O
)	O
for	O
2	O
epochs	O
with	O
the	O
weight	B-MethodName
decay	I-MethodName
of	O
0.01	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e	O
-	O
5	O
,	O
maximum	O
sequence	O
length	O
of	O
384	O
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
12	O
,	O
linear	O
learning	B-HyperparameterName
rate	I-HyperparameterName
decay	O
of	O
0.06	O
warmup	O
rate	O
,	O
and	O
half	O
precision	O
(	O
Micikevicius	O
et	O
al	O
,	O
2018	O
)	O
.	O
For	O
all	O
experiments	O
on	O
NER	B-TaskName
tasks	O
,	O
we	O
finetune	O
the	O
PLM	O
for	O
20	O
epochs	O
,	O
where	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
5e	O
-	O
5	O
,	O
maximum	O
sequence	O
length	O
is	O
set	O
to	O
128	O
,	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
32	O
.	O
We	O
use	O
AdamW	B-MethodName
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2019	O
)	O
as	O
an	O
optimizer	B-HyperparameterName
using	O
BERT	B-MethodName
-	O
base	O
as	O
the	O
PLM	O
.	O
For	O
the	O
generative	O
QA	O
task	O
in	O
Table	O
7	O
,	O
we	O
finetune	O
the	O
T5	B-MethodName
-	O
small	O
(	O
Raffel	O
et	O
al	O
,	O
2020	O
)	O
for	O
4	O
epochs	O
with	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	O
-	O
4	O
,	O
maximum	O
sequence	O
length	O
of	O
512	O
,	O
and	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	O
.	O
We	O
also	O
use	O
the	O
Adafactor	B-MethodName
(	O
Shazeer	O
and	O
Stern	O
,	O
2018	O
)	O
optimizer	B-HyperparameterName
.	O
Instead	O
of	O
training	O
with	O
the	O
same	O
optimizer	B-HyperparameterName
as	O
in	O
BERT	B-MethodName
for	O
QA	O
and	O
NER	B-TaskName
,	O
we	O
instead	O
use	O
the	O
independent	O
AdamW	B-MethodName
optimizer	B-HyperparameterName
with	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e	O
-	O
4	O
and	O
weight	B-MethodName
decay	I-MethodName
of	O
0.01	O
to	O
train	O
the	O
KALA	O
module	O
with	O
T5	B-MethodName
.	O
Adaptive	O
Pre	O
-	O
training	O
Setup	O
In	O
this	O
paragraph	O
,	O
we	O
describe	O
the	O
experimental	O
settings	O
of	O
adaptive	O
pre	O
-	O
training	O
baselines	O
,	O
namely	O
TAPT	O
,	O
TAPT	O
(	O
+	O
RecAdam	O
)	O
,	O
and	O
DAPT	O
.	O
For	O
QA	O
tasks	O
,	O
we	O
further	O
pre	O
-	O
train	O
the	O
PLM	O
for	O
{	O
1	O
,	O
3	O
,	O
5	O
,	O
10	O
}	O
epochs	O
and	O
then	O
report	O
the	O
best	O
performance	O
among	O
them	O
.	O
Specifically	O
,	O
reported	O
TAPT	O
result	O
on	O
NewsQA	B-DatasetName
,	O
Relation	O
,	O
and	O
Medication	O
are	O
obtained	O
by	O
1	O
epoch	O
of	O
further	O
pre	O
-	O
training	O
.	O
We	O
use	O
the	O
weight	B-MethodName
decay	I-MethodName
of	O
0.01	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e	O
-	O
5	O
,	O
maximum	O
sequence	O
length	O
of	O
384	O
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
12	O
,	O
and	O
linear	O
learning	B-HyperparameterName
rate	I-HyperparameterName
decay	O
of	O
0.06	O
warmup	O
rate	O
,	O
with	O
a	O
half	O
-	O
precision	O
.	O
Also	O
,	O
the	O
masking	O
ratio	O
for	O
the	O
pre	O
-	O
training	O
objective	O
is	O
set	O
to	O
0.15	O
,	O
following	O
the	O
existing	O
strategy	O
introduced	O
in	O
the	O
original	O
BERT	B-MethodName
paper	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
For	O
NER	B-TaskName
tasks	O
,	O
we	O
further	O
pre	O
-	O
train	O
the	O
PLM	O
for	O
3	O
epochs	O
across	O
all	O
datasets	O
.	O
In	O
particular	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
5e	O
-	O
5	O
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
32	O
,	O
and	O
the	O
maximum	O
sequence	O
length	O
is	O
set	O
to	O
128	O
.	O
We	O
also	O
use	O
AdamW	B-MethodName
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2019	O
)	O
as	O
the	O
optimizer	B-HyperparameterName
for	O
all	O
experiments	O
.	O
In	O
the	O
case	O
of	O
T5	B-MethodName
-	O
small	O
for	O
generative	O
QA	O
in	O
Table	O
7	O
,	O
we	O
further	O
pre	O
-	O
train	O
the	O
PLM	O
for	O
4	O
epochs	O
with	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.001	O
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	O
,	O
maximum	O
sequence	O
length	O
of	O
384	O
,	O
and	O
Adafac	O
-	O
tor	O
(	O
Shazeer	O
and	O
Stern	O
,	O
2018	O
)	O
optimizer	B-HyperparameterName
.	O
Regarding	O
the	O
setting	O
of	O
TAPT	O
(	O
+	O
RecAdam	O
)	O
on	O
all	O
tasks	O
,	O
we	O
follow	O
the	O
best	O
setting	O
in	O
the	O
original	O
paper	O
-	O
sigmoid	O
as	O
an	O
annealing	O
function	O
with	O
annealing	O
parameters	O
:	O
k	B-HyperparameterName
=	I-HyperparameterName
0.5	O
,	O
t	O
0	B-DatasetName
=	O
250	O
,	O
and	O
the	O
pretraining	O
coefficient	O
of	O
5000	O
.	O
For	O
training	O
with	O
DAPT	O
,	O
we	O
need	O
an	O
external	O
corpus	O
having	O
a	O
large	O
amount	O
of	O
data	O
for	O
adaptive	O
pre	O
-	O
training	O
.	O
Thus	O
,	O
we	O
first	O
choose	O
the	O
datasets	O
of	O
two	O
domains	O
-	O
News	O
and	O
Medical	O
.	O
Specifically	O
,	O
as	O
the	O
source	O
of	O
corpus	O
for	O
the	O
News	O
domain	O
,	O
we	O
use	O
the	O
sampled	O
set	O
of	O
10	O
million	O
News	O
from	O
the	O
RealNews	B-DatasetName
dataset	O
used	O
in	O
Gururangan	O
et	O
al	O
(	O
2021	O
)	O
.	O
As	O
the	O
source	O
of	O
corpus	O
for	O
the	O
Medical	B-DatasetName
domain	I-DatasetName
,	O
we	O
use	O
the	O
set	O
of	O
approximately	O
100k	O
passages	O
from	O
the	O
Medical	O
textbook	O
provided	O
in	O
Jin	O
et	O
al	O
(	O
2020	O
)	O
.	O
The	O
size	O
of	O
pre	O
-	O
training	O
data	O
used	O
in	O
DAPT	O
is	O
much	O
larger	O
than	O
TAPT	O
.	O
In	O
other	O
words	O
,	O
for	O
experiments	O
on	O
NewsQA	B-DatasetName
,	O
TAPT	O
only	O
uses	O
fine	O
-	O
tuning	O
contexts	O
containing	O
5.8	O
million	O
words	O
from	O
the	O
NewsQA	B-DatasetName
training	O
dataset	O
,	O
while	O
DAPT	O
uses	O
more	O
than	O
a	O
hundred	O
times	O
larger	O
data	O
-	O
enormous	O
contexts	O
containing	O
about	O
618	O
million	O
words	O
from	O
the	O
RealNews	B-DatasetName
database	O
.	O
For	O
both	O
News	O
and	O
Medical	O
domains	O
,	O
we	O
further	O
pre	O
-	O
train	O
the	O
BERT	B-MethodName
-	O
base	O
model	O
for	O
50	O
epochs	O
with	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	O
,	O
to	O
match	O
the	O
similar	O
computational	O
cost	O
used	O
in	O
Gururangan	O
et	O
al	O
(	O
2020	O
)	O
.	O
Other	O
experimental	O
details	O
are	O
the	O
same	O
as	O
TAPT	O
described	O
above	O
.	O

In	O
this	O
subsection	O
,	O
we	O
describe	O
the	O
details	O
of	O
architectural	O
variants	O
reported	O
in	O
Section	O
5.1	O
.	O
For	O
all	O
variants	O
,	O
we	O
use	O
the	O
same	O
KGs	O
used	O
in	O
KALA	O
.	O
Entity	O
-	O
as	O
-	O
Experts	O
(	O
Févry	O
et	O
al	O
(	O
2020	O
)	O
;	O
EaE	O
)	O
utilizes	O
the	O
entity	O
memory	O
similar	O
to	O
our	O
work	O
,	O
but	O
they	O
use	O
the	O
parametric	O
dense	O
retrieval	O
more	O
like	O
the	O
memory	O
neural	O
network	O
(	O
Sukhbaatar	O
et	O
al	O
,	O
2015	O
)	O
.	O
Similar	O
to	O
Févry	O
et	O
al	O
(	O
2020	O
)	O
;	O
Verga	O
et	O
al	O
(	O
2021	O
)	O
,	O
we	O
change	O
the	O
formulation	O
of	O
query	O
and	O
memory	O
retrieval	O
by	O
using	O
the	O
mention	O
representation	O
of	O
the	O
entity	O
from	O
the	O
intermediate	O
hidden	O
states	O
of	O
PLMs	O
,	O
which	O
is	O
formally	O
defined	O
as	O
follows	O
:	O
h	O
e	O
=	O
1	O
m	O
ω	O
−	O
m	O
α	B-HyperparameterName
+	O
1	O
m	O
ω	O
i	O
=	O
m	O
α	B-HyperparameterName
h	O
l−1	O
i	O
,	O
(	O
4	O
)	O
v	O
=	O
softmax	B-MethodName
(	O
h	O
e	O
E	O
)	O
E	O
,	O
where	O
h	O
e	O
represents	O
the	O
average	O
of	O
token	O
representations	O
of	O
the	O
entity	O
mention	O
m	O
=	O
(	O
m	O
α	B-HyperparameterName
,	O
m	O
ω	O
)	O
.	O
We	O
also	O
give	O
the	O
supervised	O
retrieval	O
loss	B-MetricName
(	O
ELLoss	O
in	O
Févry	O
et	O
al	O
(	O
2020	O
)	O
)	O
,	O
when	O
training	O
the	O
EaE	O
model	O
.	O
With	O
this	O
retrieval	O
,	O
EaE	O
also	O
can	O
represent	O
the	O
unseen	O
entity	O
e	O
/	O
E	O
train	O
if	O
we	O
know	O
the	O
mention	O
boundary	O
of	O
the	O
given	O
entity	O
on	O
the	O
context	O
.	O
We	O
believe	O
it	O
is	O
expected	O
to	O
work	O
well	O
,	O
if	O
the	O
entity	O
memory	O
is	O
pre	O
-	O
trained	O
on	O
the	O
enormous	O
text	O
along	O
with	O
the	O
pre	O
-	O
training	O
of	O
the	O
language	O
model	O
from	O
the	O
scratch	O
.	O
However	O
,	O
it	O
might	O
underperform	O
for	O
the	O
language	O
model	O
adaptation	O
scenario	O
,	O
since	O
it	O
can	O
fall	O
into	O
the	O
problem	O
of	O
circular	O
reasoning	O
-	O
the	O
PLM	O
does	O
not	O
properly	O
represent	O
the	O
unseen	O
entity	O
,	O
but	O
it	O
should	O
predict	O
which	O
entity	O
it	O
is	O
similar	O
from	O
the	O
representation	O
.	O
Regarding	O
the	O
integration	O
of	O
the	O
knowledge	O
from	O
the	O
entity	O
memory	O
into	O
the	O
PLM	O
,	O
the	O
retrieved	O
entity	O
representation	O
v	O
is	O
simply	O
added	O
(	O
Peters	O
et	O
al	O
,	O
2019	O
)	O
to	O
the	O
hidden	O
representations	O
H	O
after	O
the	O
transformer	O
block	O
as	O
follows	O
:	O
H	O
l	O
=	O
H	O
l	O
+	O
h	O
(	O
v	O
)	O
(	O
5	O
)	O
where	O
h	O
is	O
Multi	O
-	O
Layer	O
Perceptrons	O
(	O
MLPs	O
)	O
.	O
Adapter	B-MethodName
(	O
Houlsby	O
et	O
al	O
,	O
2019	O
)	O
is	O
introduced	O
to	O
fine	O
-	O
tune	O
the	O
PLM	O
only	O
with	O
a	O
few	O
trainable	O
parameters	O
,	O
instead	O
of	O
fine	O
-	O
tuning	O
the	O
whole	O
parameters	O
of	O
the	O
PLM	O
.	O
To	O
adapt	O
this	O
original	O
implementation	O
into	O
our	O
KALA	O
framework	O
,	O
we	O
replace	O
our	O
Knowledge	O
-	O
conditioned	O
Feature	O
Modulation	O
with	O
it	O
,	O
where	O
the	O
Adapter	B-MethodName
is	O
used	O
as	O
the	O
knowledge	O
integration	O
module	O
.	O
We	O
interleave	O
the	O
layer	O
of	O
Adapter	B-MethodName
after	O
the	O
feed	O
-	O
forward	O
layer	O
(	O
F	O
F	O
)	O
and	O
before	O
the	O
residual	B-MethodName
connection	I-MethodName
of	O
the	O
transformer	O
block	O
.	O
Also	O
,	O
instead	O
of	O
only	O
providing	O
the	O
LM	O
hidden	O
states	O
as	O
an	O
input	O
,	O
we	O
concatenate	O
the	O
knowledge	O
representation	O
in	O
Equation	O
3	O
to	O
the	O
LM	O
hidden	O
states	O
.	O
Note	O
that	O
we	O
fine	O
-	O
tune	O
the	O
whole	O
parameters	O
following	O
our	O
KALA	O
setting	O
,	O
unlike	O
fine	O
-	O
tuning	O
the	O
parameters	O
of	O
only	O
Adapter	B-MethodName
layers	O
in	O
Houlsby	O
et	O
al	O
(	O
2019	O
)	O
.	O
ERNIE	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
is	O
a	O
notable	O
PLM	O
model	O
that	O
utilizes	O
the	O
external	O
KB	O
as	O
an	O
input	O
for	O
the	O
language	O
model	O
.	O
The	O
key	O
feature	O
of	O
ERNIE	O
can	O
be	O
summarized	O
into	O
two	O
folds	O
.	O
First	O
,	O
they	O
use	O
the	O
multi	O
-	O
head	O
self	O
-	O
attention	O
scheme	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
to	O
contextualize	O
the	O
input	O
entities	O
.	O
Second	O
,	O
ERNIE	O
fuses	O
the	O
entity	O
representation	O
at	O
the	O
end	O
of	O
the	O
PLM	O
by	O
adding	O
it	O
to	O
the	O
corresponding	O
language	O
representation	O
.	O
We	O
assume	O
that	O
those	O
two	O
features	O
are	O
important	O
points	O
of	O
ERNIE	O
.	O
Therefore	O
,	O
instead	O
of	O
using	O
a	O
Graph	O
Neural	O
Network	O
(	O
GNN	O
)	O
layer	O
,	O
we	O
use	O
a	O
multi	O
-	O
head	O
self	O
-	O
attention	O
layer	O
to	O
contextualize	O
the	O
entity	B-TaskName
embeddings	I-TaskName
.	O
Then	O
,	O
we	O
add	O
it	O
to	O
a	O
representation	O
of	O
the	O
entity	O
from	O
the	O
PLM	O
,	O
which	O
is	O
the	O
same	O
as	O
the	O
design	O
in	O
equation	O
5	O
.	O
KT	O
-	O
Net	O
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
uses	O
knowledge	O
as	O
an	O
external	O
input	O
in	O
the	O
fine	O
-	O
tuning	O
stage	O
for	O
extractive	O
QA	O
.	O
Since	O
they	O
have	O
a	O
typical	O
layer	O
for	O
integrating	O
existing	O
KB	O
(	O
Miller	O
,	O
1995	O
;	O
Carlson	O
et	O
al	O
,	O
2010	O
)	O
with	O
the	O
PLM	O
,	O
we	O
only	O
adopt	O
the	O
self	O
-	O
matching	O
layer	O
as	O
the	O
architecture	O
variant	O
of	O
the	O
KFM	O
layer	O
used	O
in	O
our	O
KALA	O
framework	O
.	O
The	O
computation	O
of	O
the	O
self	O
-	O
matching	O
matrix	O
in	O
KT	O
-	O
Net	O
is	O
costly	O
,	O
i.e.	O
,	O
it	O
requires	O
a	O
large	O
computational	O
cost	O
that	O
is	O
approximately	O
12	O
times	O
larger	O
than	O
KALA	O
.	O
ERICA	O
(	O
Qin	O
et	O
al	O
,	O
2021	O
)	O
uses	O
contrastive	B-MethodName
learning	I-MethodName
in	O
LM	O
pre	O
-	O
training	O
to	O
reflect	O
the	O
relational	O
knowledge	O
into	O
the	O
language	O
model	O
.	O
We	O
use	O
the	O
Entity	O
Discrimination	O
task	O
from	O
ERICA	O
on	O
the	O
primary	O
task	O
of	O
fine	O
-	O
tuning	O
.	O
We	O
would	O
like	O
to	O
note	O
that	O
,	O
as	O
reported	O
in	O
Section	O
5	O
of	O
the	O
original	O
paper	O
(	O
Qin	O
et	O
al	O
,	O
2021	O
)	O
,	O
the	O
use	O
of	O
ERICA	O
on	O
fine	O
-	O
tuning	O
has	O
no	O
effect	O
,	O
since	O
the	O
size	O
and	O
diversity	O
of	O
entities	O
and	O
relations	O
in	O
downstream	O
training	O
data	O
are	O
limited	O
.	O
Such	O
limited	O
information	O
rather	O
harms	O
the	O
performance	O
,	O
as	O
it	O
can	O
hinder	O
the	O
generalization	O
.	O
In	O
other	O
words	O
,	O
contrastive	B-MethodName
learning	I-MethodName
can	O
not	O
reflect	O
the	O
entity	O
and	O
relation	O
in	O
the	O
test	O
dataset	O
.	O

To	O
see	O
how	O
much	O
amount	O
of	O
value	O
on	O
gamma	B-HyperparameterName
and	O
beta	B-HyperparameterName
is	O
used	O
to	O
shift	O
and	O
scale	O
the	O
intermediate	O
hidden	O
representations	O
in	O
transformer	O
layers	O
,	O
we	O
visualize	O
the	O
modulation	O
values	O
,	O
namely	O
gamma	B-HyperparameterName
and	O
beta	B-HyperparameterName
,	O
in	O
Figure	O
11	O
.	O
We	O
first	O
observe	O
that	O
,	O
as	O
shown	O
in	O
Figure	O
11	O
,	O
the	O
distribution	O
of	O
values	O
of	O
gamma	B-HyperparameterName
and	O
beta	B-HyperparameterName
approximately	O
follow	O
the	O
Gaussian	O
dis	O
-	O
tribution	O
,	O
with	O
zero	O
mean	O
for	O
beta	B-HyperparameterName
and	O
one	O
mean	O
for	O
gamma	B-HyperparameterName
.	O
Also	O
,	O
we	O
notice	O
that	O
the	O
scale	O
of	O
values	O
remain	O
nearly	O
around	O
the	O
mean	O
point	O
,	O
which	O
suggests	O
that	O
the	O
small	O
amount	O
of	O
shifting	O
to	O
intermediate	O
hidden	O
representations	O
on	O
transformer	O
layers	O
is	O
enough	O
to	O
contribute	O
to	O
the	O
performance	O
gain	O
,	O
as	O
we	O
can	O
see	O
in	O
the	O
main	O
results	O
of	O
Table	O
1	O
,	O
2	O
.	O

While	O
we	O
provide	O
the	O
efficiency	O
on	O
FLOPs	O
in	O
Figure	O
1	O
,	O
we	O
further	O
provide	O
the	O
efficiency	O
on	O
GPU	O
memory	O
,	O
wall	O
time	O
,	O
and	O
FLOPs	O
for	O
training	O
each	O
method	O
in	O
Table	O
6	O
.	O
Specifically	O
,	O
we	O
measure	O
the	O
computational	O
cost	O
on	O
the	O
NewsQA	B-DatasetName
dataset	O
with	O
BERT	B-MethodName
-	O
base	O
,	O
where	O
we	O
use	O
the	O
single	O
Geforce	O
RTX	O
2080	O
Ti	O
GPU	O
on	O
the	O
same	O
machine	O
.	O
For	O
our	O
KALA	O
,	O
as	O
we	O
can	O
flexibly	O
manage	O
the	O
cost	O
of	O
GPU	O
memory	O
by	O
reducing	O
the	O
number	O
of	O
entities	O
in	O
entity	O
memory	O
(	O
See	O
Figure	O
8	O
with	O
Appendix	O
C.2	O
for	O
more	O
analysis	O
on	O
the	O
effects	O
of	O
the	O
size	O
of	O
entity	O
memory	O
)	O
,	O
we	O
provide	O
the	O
experimental	O
results	O
on	O
two	O
settings	O
-	O
KALA	O
with	O
memory	O
size	O
0.2k	O
and	O
62.8k	O
(	O
full	O
memory	O
)	O
.	O
As	O
shown	O
in	O
Table	O
6	O
,	O
we	O
confirm	O
that	O
the	O
computational	O
cost	O
of	O
our	O
KALA	O
with	O
the	O
full	O
memory	O
is	O
similar	O
to	O
the	O
cost	O
of	O
the	O
more	O
params	B-MetricName
baseline	O
that	O
uses	O
one	O
additional	O
transformer	O
layer	O
on	O
top	O
of	O
BERT	B-MethodName
-	O
base	O
.	O
However	O
,	O
by	O
reducing	O
the	O
number	O
of	O
entities	O
in	O
the	O
memory	O
,	O
we	O
can	O
achieve	O
better	O
efficiency	O
than	O
more	O
params	B-MetricName
in	O
terms	O
of	O
GPU	O
memory	O
and	O
FLOPs	O
.	O
Also	O
,	O
we	O
observe	O
that	O
the	O
training	O
cost	O
(	O
i.e.	O
,	O
Wall	O
Time	O
and	O
FLOPs	O
)	O
of	O
TAPT	O
and	O
DAPT	O
is	O
high	O
,	O
especially	O
on	O
DAPT	O
,	O
thus	O
we	O
verify	O
that	O
our	O
KALA	O
is	O
more	O
efficient	O
to	O
train	O
for	O
domain	B-TaskName
adaptation	I-TaskName
settings	O
.	O

The	O
development	O
of	O
online	O
media	O
platforms	O
has	O
given	O
users	O
more	O
opportunities	O
to	O
post	O
and	O
comment	O
freely	O
,	O
but	O
the	O
negative	O
impact	O
of	O
offensive	O
language	O
has	O
become	O
increasingly	O
apparent	O
.	O
It	O
is	O
very	O
necessary	O
for	O
the	O
automatic	O
identification	O
system	O
of	O
offensive	O
language	O
.	O
This	O
paper	O
describes	O
our	O
work	O
on	O
the	O
task	O
of	O
Offensive	O
Language	B-TaskName
Identification	I-TaskName
in	O
Dravidian	O
language	O
-	O
EACL	O
2021	O
.	O
To	O
complete	O
this	O
task	O
,	O
we	O
propose	O
a	O
system	O
based	O
on	O
the	O
multilingual	O
model	O
XLM	B-MethodName
-	O
Roberta	O
and	O
DPCNN	O
.	O
The	O
test	O
results	O
on	O
the	O
official	O
test	O
data	O
set	O
confirm	O
the	O
effectiveness	O
of	O
our	O
system	O
.	O
The	O
weighted	O
average	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
of	O
Kannada	O
,	O
Malayalam	O
,	O
and	O
Tamil	O
language	O
are	O
0.69	O
,	O
0.92	O
,	O
and	O
0.76	O
respectively	O
,	O
ranked	O
6th	O
,	O
6th	O
,	O
and	O
3rd	O
.	O

Due	O
to	O
the	O
harm	O
of	O
offensive	O
language	O
to	O
the	O
network	O
environment	O
,	O
the	O
identification	O
of	O
offensive	O
language	O
has	O
been	O
carried	O
out	O
for	O
a	O
long	O
time	O
.	O
Research	O
so	O
far	O
has	O
focused	O
on	O
automating	O
the	O
decision	O
-	O
making	O
process	O
in	O
the	O
form	O
of	O
supervised	O
machine	O
learning	O
for	O
classification	O
tasks	O
(	O
Sun	O
et	O
al	O
,	O
2019	O
)	O
.	O
As	O
far	O
back	O
as	O
2012	O
,	O
Chen	O
et	O
al	O
(	O
2012	O
)	O
proposed	O
a	O
lexical	O
syntactic	O
feature	O
(	O
LSF	O
)	O
framework	O
to	O
detect	O
offensive	O
content	O
in	O
social	O
media	O
,	O
distinguished	O
the	O
roles	O
of	O
derogatory	O
/	O
profane	O
and	O
obscenity	O
in	O
identifying	O
offensive	O
content	O
,	O
and	O
introduced	O
handwritten	O
syntax	O
rules	O
to	O
identify	O
abusive	O
harassment	O
.	O
In	O
contrast	O
to	O
the	O
start	O
-	O
to	O
-	O
end	O
training	O
model	O
,	O
Howard	O
and	O
Ruder	O
(	O
2018	O
)	O
proposed	O
an	O
effective	O
transfer	B-TaskName
learning	I-TaskName
method	O
,	O
Universal	O
Language	O
Model	O
Tuning	O
(	O
ULMFIT	B-MethodName
)	O
,	O
which	O
can	O
be	O
applied	O
to	O
any	O
task	O
in	O
natural	O
language	O
processing	O
,	O
and	O
has	O
shown	O
significant	O
results	O
on	O
six	O
text	B-TaskName
classification	I-TaskName
tasks	O
.	O
Subsequently	O
,	O
Abdellatif	O
and	O
Elgammal	O
(	O
2020	O
)	O
used	O
the	O
ULMFiT	B-MethodName
transfer	B-TaskName
learning	I-TaskName
method	O
to	O
train	O
forward	O
and	O
backward	O
models	O
on	O
Arabic	O
datasets	O
and	O
ensemble	O
the	O
results	O
to	O
perform	O
an	O
offensive	O
language	O
detection	O
task	O
.	O
Although	O
English	O
is	O
currently	O
one	O
of	O
the	O
most	O
commonly	O
spoken	O
languages	O
in	O
the	O
world	O
,	O
work	O
is	O
ongoing	O
to	O
identify	O
the	O
offensive	O
language	O
in	O
other	O
languages	O
that	O
are	O
less	O
widely	O
spoken	O
.	O
Pitenis	O
et	O
al	O
(	O
2020	O
)	O
tested	O
the	O
performance	O
of	O
several	O
traditional	O
machine	O
learning	O
models	O
and	O
deep	O
learning	O
models	O
on	O
an	O
offensive	O
language	O
dataset	O
of	O
Greek	O
,	O
and	O
the	O
best	O
results	O
were	O
achieved	O
with	O
the	O
attention	O
model	O
of	O
LSTM	B-MethodName
and	O
GRU	B-MethodName
.	O
Ozdemir	O
and	O
Yeniterzi	O
(	O
2020	O
)	O
ensembled	O
CNN	O
-	O
LSTM	B-MethodName
,	O
BILSTM	B-MethodName
-	O
Attention	O
,	O
and	O
BERT	B-MethodName
three	O
models	O
,	O
combined	O
with	O
pre	O
-	O
trained	O
word	O
embedding	O
on	O
Twitter	O
to	O
complete	O
the	O
identification	O
task	O
of	O
offensive	O
Turkish	O
language	O
,	O
and	O
achieved	O
a	O
good	O
result	O
.	O
A	O
key	O
challenge	O
in	O
automatically	O
detecting	O
hate	B-DatasetName
speech	I-DatasetName
on	O
social	O
media	O
is	O
to	O
separate	O
hate	B-DatasetName
speech	I-DatasetName
from	O
other	O
offensive	O
languages	O
.	O
Davidson	O
et	O
al	O
(	O
2017	O
)	O
used	O
the	O
crowd	O
-	O
sourced	O
hate	B-DatasetName
speech	I-DatasetName
lexicon	O
to	O
collect	O
tweets	O
containing	O
hate	B-DatasetName
speech	I-DatasetName
keywords	O
.	O
They	O
trained	O
a	O
multi	O
-	O
class	O
classifier	O
to	O
reliably	O
distinguish	O
hate	B-DatasetName
speech	I-DatasetName
from	O
other	O
offensive	O
languages	O
,	O
and	O
found	O
that	O
racist	O
and	O
homophobic	O
tweets	O
were	O
more	O
likely	O
to	O
be	O
classified	O
as	O
hate	B-DatasetName
speech	I-DatasetName
,	O
but	O
sexist	O
tweets	O
were	O
generally	O
classified	O
as	O
offensive	O
.	O
Razavi	O
et	O
al	O
(	O
2010	O
)	O
proposed	O
to	O
extract	O
features	O
at	O
different	O
conceptual	O
levels	O
and	O
apply	O
multilevel	O
classification	O
for	O
offensive	O
language	O
detection	O
.	O
The	O
system	O
leverages	O
a	O
variety	O
of	O
statistical	O
models	O
and	O
rule	O
-	O
based	O
patterns	O
,	O
combined	O
with	O
an	O
auxiliary	O
weighted	O
pattern	O
library	O
,	O
to	O
improve	O
accuracy	B-MetricName
by	O
matching	O
text	O
with	O
its	O
graded	O
entries	O
.	O
Pitsilis	O
et	O
al	O
(	O
2018	O
)	O
proposed	O
the	O
ensemble	O
of	O
a	O
recursive	O
neural	O
network	O
(	O
RNN	O
)	O
classifier	O
,	O
which	O
combines	O
various	O
characteristics	O
related	O
to	O
user	O
-	O
related	O
information	O
,	O
such	O
as	O
the	O
user	O
's	O
sexist	O
or	O
racist	O
tendencies	O
,	O
and	O
was	O
then	O
fed	O
to	O
the	O
classifier	O
as	O
input	O
along	O
with	O
a	O
word	O
frequency	O
vector	O
derived	O
from	O
the	O
text	O
content	O
.	O
When	O
there	O
is	O
a	O
large	O
amount	O
of	O
labeled	O
data	O
,	O
increasing	O
the	O
size	O
and	O
parameters	O
of	O
the	O
model	O
will	O
definitely	O
improve	O
the	O
performance	O
of	O
the	O
model	O
.	O
However	O
,	O
when	O
the	O
amount	O
of	O
training	O
is	O
relatively	O
small	O
,	O
the	O
large	O
-	O
scale	O
model	O
may	O
not	O
be	O
able	O
to	O
achieve	O
good	O
results	O
,	O
so	O
solving	O
the	O
problem	O
of	O
model	O
training	O
under	O
the	O
condition	O
of	O
a	O
small	O
amount	O
of	O
target	O
data	O
has	O
become	O
a	O
research	O
hotspot	O
.	O
Sun	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
a	O
Hierarchical	O
Attention	O
Prototype	O
Network	O
(	O
HAPN	O
)	O
for	O
fewshot	O
text	B-TaskName
classification	I-TaskName
,	O
which	O
designed	O
multiple	O
cross	O
-	O
concerns	O
of	O
a	O
feature	O
layer	O
,	O
word	O
layer	O
,	O
and	O
instance	O
layer	O
for	O
the	O
model	O
to	O
enhance	O
the	O
expressive	O
power	O
of	O
semantic	O
space	O
.	O
The	O
model	O
was	O
validated	O
on	O
two	O
standard	O
reference	O
text	B-TaskName
classification	I-TaskName
datasets	O
,	O
Fewrel	B-DatasetName
and	O
CSID	O
.	O
Prettenhofer	O
and	O
Stein	O
(	O
2010	O
)	O
built	O
on	O
structural	O
correspondence	O
learning	O
,	O
using	O
untagged	O
documents	O
and	O
simple	O
word	B-TaskName
translation	I-TaskName
to	O
induce	O
task	O
-	O
specific	O
,	O
cross	O
-	O
language	O
word	O
correspondence	O
.	O
English	O
was	O
used	O
as	O
the	O
source	O
language	O
and	O
German	O
,	O
French	O
,	O
and	O
Japanese	O
were	O
used	O
as	O
the	O
target	O
language	O
to	O
conduct	O
the	O
experiment	O
in	O
the	O
field	O
of	O
cross	O
-	O
language	O
sentiment	O
classification	O
.	O
Using	O
English	O
data	O
,	O
Ranasinghe	O
and	O
Zampieri	O
(	O
2020	O
)	O
trained	O
the	O
model	O
by	O
applying	O
cross	O
-	O
language	O
contextual	O
word	O
embedding	O
and	O
transfer	B-TaskName
learning	I-TaskName
methods	O
,	O
and	O
then	O
predicted	O
the	O
effect	O
of	O
cross	O
-	O
language	O
contextual	O
embedding	O
and	O
transfer	B-TaskName
learning	I-TaskName
on	O
this	O
task	O
in	O
less	O
resourceintensive	O
languages	O
such	O
as	O
Bengali	O
,	O
Hindi	O
,	O
and	O
Spanish	O
.	O

In	O
this	O
experiment	O
,	O
the	O
pre	O
-	O
training	O
model	O
I	O
used	O
was	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
-	O
base	O
.	O
After	O
adding	O
the	O
DPCNN	O
module	O
,	O
we	O
began	O
to	O
set	O
the	O
experimental	O
parameters	O
.	O
We	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
as	O
2e	O
-	O
5	O
,	O
the	O
maximum	O
sequence	O
length	O
is	O
256	O
,	O
and	O
the	O
gradient	O
steps	O
are	O
set	O
to	O
4	O
.	O
The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
32	O
,	O
as	O
shown	O
in	O
table	O
2	O
.	O
In	O
the	O
training	O
process	O
,	O
we	O
used	O
five	O
-	O
fold	O
stratified	O
cross	O
-	O
validation	O
to	O
make	O
the	O
proportion	O
of	O
data	O
of	O
each	O
category	O
in	O
each	O
subsample	O
the	O
same	O
as	O
that	O
in	O
the	O
original	O
data	O
and	O
finally	O
obtained	O
the	O
optimal	O
result	O
through	O
the	O
voting	O
(	O
Onan	O
et	O
al	O
,	O
2016	O
)	O
system	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O

After	O
the	O
evaluation	O
by	O
the	O
organizer	O
,	O
we	O
obtained	O
the	O
weighted	O
average	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
in	O
the	O
three	O
languages	O
,	O
as	O
shown	O
in	O
table	O
3	O
.	O
Our	O
team	O
's	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
is	O
0.69	O
,	O
ranked	O
6th	O
place	O
for	O
the	O
Kannada	O
language	O
.	O
For	O
the	O
Malayalam	O
language	O
,	O
our	O
team	O
's	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
is	O
0.92	O
ranked	O
6th	O
place	O
,	O
and	O
for	O
the	O
Tamil	O
language	O
,	O
our	O
team	O
's	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
is	O
0.76	O
ranked	O
3rd	O
place	O
.	O

We	O
begin	O
by	O
introducing	O
the	O
task	O
and	O
the	O
datasets	O
we	O
use	O
.	O
Tabular	O
Inference	O
is	O
a	O
reasoning	O
task	O
that	O
,	O
like	O
conventional	O
NLI	O
(	O
Dagan	O
et	O
al	O
,	O
2013	O
;	O
Bowman	O
et	O
al	O
,	O
2015	O
;	O
Williams	O
et	O
al	O
,	O
2018	O
)	O
,	O
asks	O
whether	O
a	O
natural	O
language	O
hypothesis	O
can	O
be	O
inferred	O
from	O
a	O
tabular	O
premise	O
.	O
Concretely	O
,	O
given	O
a	O
premise	O
table	O
T	O
with	O
m	O
rows	O
{	O
r	O
1	O
,	O
r	O
2	O
,	O
.	O
.	O
.	O
,	O
r	O
m	O
}	O
,	O
and	O
a	O
hypothesis	O
sentence	O
H	O
,	O
the	O
task	O
maps	O
them	O
to	O
ENTAIL	O
(	O
E	O
)	O
,	O
CONTRADICT	O
(	O
C	O
)	O
or	O
NEUTRAL	O
(	O
N	O
)	O
.	O
We	O
can	O
denote	O
the	O
mapping	O
as	O
f	O
(	O
T	O
,	O
H	O
)	O
y	O
(	O
1	O
)	O
where	O
,	O
y	O
{	O
E	O
,	O
N	O
,	O
C	O
}	O
.	O
For	O
example	O
,	O
for	O
the	O
tabular	O
premise	O
in	O
Figure	O
1	O
,	O
the	O
model	O
should	O
predict	O
E	O
,	O
C	O
,	O
and	O
N	O
for	O
the	O
hypotheses	O
H1	O
,	O
H2	O
,	O
and	O
H3	O
,	O
respectively	O
.	O
Trustworthy	O
Tabular	O
Inference	O
is	O
a	O
table	O
reasoning	O
problem	O
that	O
seeks	O
not	O
just	O
the	O
NLI	O
label	O
,	O
but	O
also	O
relevant	O
evidence	O
from	O
the	O
input	O
table	O
that	O
supports	O
the	O
label	O
prediction	O
.	O
We	O
use	O
T	O
R	O
,	O
a	O
subset	O
of	O
T	O
,	O
to	O
denote	O
the	O
relevant	O
rows	O
or	O
evidence	O
.	O
Then	O
,	O
the	O
task	O
is	O
defined	O
as	O
follows	O
.	O
f	O
(	O
T	O
,	O
H	O
)	O
{	O
T	O
R	O
,	O
y	O
}	O
(	O
2	O
)	O
In	O
our	O
example	O
table	O
,	O
this	O
task	O
will	O
also	O
indicate	O
the	O
evidence	O
rows	O
T	O
R	O
of	O
Producer	O
and	O
Length	O
for	O
hypothesis	O
H1	O
,	O
Recorded	O
for	O
hypothesis	O
H2	O
,	O
and	O
Released	O
and	O
Recorded	O
for	O
hypothesis	O
H3	O
.	O
While	O
the	O
notion	O
of	O
evidence	O
is	O
well	O
-	O
defined	O
for	O
the	O
ENTAIL	O
and	O
CONTRADICT	O
labels	O
,	O
the	O
NEU	O
-	O
TRAL	O
label	O
requires	O
explanation	O
.	O
To	O
decide	O
on	O
the	O
NEUTRAL	O
label	O
,	O
one	O
must	O
first	O
search	O
for	O
relevant	O
rows	O
(	O
if	O
any	O
)	O
,	O
i.e.	O
,	O
identify	O
evidence	O
in	O
the	O
premise	O
tables	O
.	O
In	O
fact	O
,	O
this	O
is	O
a	O
causally	O
correct	O
sequential	O
approach	O
.	O
Indeed	O
,	O
INFOTABS	B-DatasetName
has	O
multiple	O
neutral	O
hypotheses	O
that	O
are	O
partly	O
entailed	O
by	O
the	O
table	O
;	O
if	O
any	O
part	O
of	O
a	O
hypothesis	O
contradicts	O
the	O
table	O
,	O
then	O
the	O
inference	O
label	O
should	O
be	O
CONTRADICT	O
.	O
For	O
example	O
,	O
in	O
our	O
example	O
table	O
,	O
the	O
premise	O
table	O
indicates	O
that	O
the	O
album	O
was	O
recorded	O
in	O
1978	O
,	O
emphasizing	O
the	O
importance	O
of	O
the	O
Recorded	O
row	O
for	O
the	O
hypothesis	O
H2	O
.	O
For	O
NEUTRAL	O
examples	O
,	O
we	O
refer	O
to	O
any	O
such	O
pertinent	O
rows	O
as	O
evidence	O
.	O
Dataset	O
Details	O
.	O
There	O
are	O
several	O
datasets	O
for	O
tabular	O
NLI	O
:	O
TabFact	B-DatasetName
,	O
INFOTABS	B-DatasetName
,	O
and	O
the	O
Se	O
-	O
mEval'21	O
Task	O
9	O
(	O
Wang	O
et	O
al	O
,	O
2021b	O
)	O
and	O
the	O
FEVEROUS'21	O
shared	O
task	O
(	O
Aly	O
et	O
al	O
,	O
2021	O
)	O
datasets	O
.	O
We	O
use	O
the	O
INFOTABS	B-DatasetName
data	O
in	O
this	O
work	O
.	O
It	O
contains	O
finer	O
-	O
grained	O
annotation	O
(	O
e.g.	O
,	O
TabFact	B-DatasetName
lacks	O
NEUTRAL	O
hypotheses	O
)	O
and	O
more	O
complex	O
reasoning	O
than	O
the	O
others	O
3	O
.	O
The	O
dataset	O
consists	O
of	O
23	O
,	O
738	O
premisehypothesis	O
pairs	O
collected	O
via	O
crowdsourcing	O
on	O
Amazon	O
MTurk	O
.	O
The	O
tabular	O
premises	O
are	O
based	O
on	O
2	O
,	O
540	O
Wikipedia	O
Infoboxes	O
representing	O
twelve	O
diverse	O
domains	O
,	O
and	O
the	O
hypotheses	O
are	O
short	O
statements	O
paired	O
with	O
NLI	O
labels	O
.	O
All	O
tables	O
contain	O
a	O
title	O
followed	O
by	O
two	O
columns	O
(	O
cf	O
.	O
Figure	O
1	O
)	O
;	O
the	O
left	O
columns	O
are	O
keys	O
and	O
the	O
right	O
ones	O
are	O
values	O
)	O
.	O
In	O
addition	O
to	O
the	O
train	O
and	O
development	O
sets	O
,	O
the	O
data	O
includes	O
multiple	O
test	O
sets	O
,	O
some	O
of	O
which	O
are	O
adversarial	O
:	O
α	B-HyperparameterName
1	O
represents	O
a	O
standard	O
test	O
set	O
that	O
is	O
both	O
topically	O
and	O
lexically	O
similar	O
to	O
the	O
training	O
data	O
;	O
α	B-HyperparameterName
2	O
hypotheses	O
are	O
designed	O
to	O
be	O
lexically	O
adversarial	O
4	O
;	O
and	O
α	B-HyperparameterName
3	O
tables	O
are	O
drawn	O
from	O
topics	O
unavailable	O
in	O
the	O
training	O
set	O
.	O
The	O
dev	O
and	O
test	O
set	O
,	O
comprising	O
of	O
7200	O
table	O
-	O
hypothesis	O
pairs	O
,	O
were	O
recently	O
extended	O
with	O
crowdsourced	O
evidence	O
rows	O
.	O
As	O
one	O
of	O
our	O
contributions	O
,	O
we	O
describe	O
the	O
evidence	O
rows	O
annotation	O
for	O
the	O
training	O
set	O
in	O
the	O
next	O
Section	O
3	O
.	O

For	O
the	O
downstream	O
NLI	O
task	O
,	O
the	O
function	O
h	O
is	O
a	O
two	O
-	O
sentence	O
classifier	O
whose	O
inputs	O
are	O
T	O
R	O
(	O
the	O
rows	O
selected	O
by	O
g	O
)	O
and	O
the	O
hypothesis	O
H.	O
We	O
use	O
BPR	O
to	O
represent	O
T	O
R	O
as	O
we	O
did	O
for	O
the	O
full	O
table	O
T.	O
Since	O
|	O
T	O
R	O
|	O
|	O
T	O
|	O
,	O
the	O
extraction	O
benefits	O
larger	O
tables	O
(	O
especially	O
in	O
α	B-HyperparameterName
3	O
set	O
)	O
which	O
exceed	O
the	O
model	O
's	O
token	O
limit	O
.	O

First	O
,	O
we	O
briefly	O
summarize	O
the	O
models	O
used	O
in	O
our	O
experiments	O
.	O
We	O
investigate	O
both	O
unsupervised	O
(	O
$	O
4.1	O
)	O
and	O
supervised	O
(	O
$	O
4.2	O
)	O
evidence	O
extraction	O
methods	O
.	O
We	O
use	O
only	O
the	O
extracted	O
evidence	O
as	O
the	O
premise	O
for	O
the	O
tabular	O
inference	O
task	O
(	O
$	O
4.3	O
)	O
.	O
We	O
compare	O
both	O
tasks	O
against	O
human	O
performance	O
.	O
As	O
baselines	O
,	O
we	O
use	O
the	O
Word	O
Mover	O
Distance	O
(	O
WMD	O
)	O
of	O
and	O
the	O
original	O
DRR	O
(	O
Neeraja	O
et	O
al	O
,	O
2021	O
)	O
with	O
Top	O
-	O
4	O
extracted	O
evidence	O
rows	O
.	O
For	O
DRR	O
(	O
Re	O
-	O
Rank	O
+	O
Top	O
-	O
S	O
τ	O
)	O
,	O
which	O
uses	O
static	O
embeddings	O
,	O
we	O
set	O
the	O
sparsity	O
parameter	O
S	O
=	O
2	O
,	O
and	O
the	O
dynamic	O
row	O
selection	O
parameter	O
τ	O
=	O
1.0	O
.	O
Our	O
choice	O
of	O
S	O
is	O
based	O
on	O
the	O
observation	O
that	O
in	O
INFOTABS	B-DatasetName
most	O
(	O
92	O
%	O
)	O
instances	O
have	O
only	O
one	O
(	O
54	O
%	O
)	O
or	O
two	O
(	O
38	O
%	O
)	O
relevant	O
rows	O
.	O
We	O
set	O
δ	B-HyperparameterName
to	O
0.5	O
for	O
all	O
experiments	O
.	O
For	O
the	O
Sentence	O
Transformer	B-MethodName
,	O
we	O
used	O
the	O
paraphrase	O
-	O
mpnet	B-MethodName
-	O
base	O
v2	O
model	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
which	O
is	O
a	O
pre	O
-	O
trained	O
with	O
the	O
mpnet	B-MethodName
-	O
base	O
architecture	O
using	O
several	O
existing	O
paraphrase	O
datasets	O
.	O
This	O
choice	O
is	O
based	O
on	O
performance	O
on	O
the	O
development	O
set	O
.	O
Both	O
the	O
supervised	O
and	O
unsupervised	O
SimCSE	B-MethodName
models	O
use	O
the	O
same	O
parameters	O
as	O
DRR	O
(	O
Re	O
-	O
Rank	O
+	O
Top	O
-	O
K	O
τ	O
)	O
.	O
We	O
refer	O
to	O
the	O
supervised	O
and	O
unsupervised	O
variants	O
as	O
SimCSE	B-MethodName
-	O
Supervised	O
and	O
SimCSE	B-MethodName
-	O
Unsupervised	O
respectively	O
.	O
For	O
the	O
NLI	O
task	O
,	O
we	O
use	O
the	O
BPR	O
representation	O
over	O
extracted	O
evidence	O
T	O
R	O
with	O
the	O
RoBERTa	B-MethodName
LARGE	O
two	O
sentence	B-TaskName
classification	I-TaskName
model	O
.	O
We	O
compare	O
the	O
following	O
settings	O
:	O
(	O
a	O
)	O
WMD	O
Top	O
-	O
3	O
from	O
,	O
(	O
b	O
)	O
No	O
extraction	O
i.e.	O
using	O
the	O
full	O
premise	O
table	O
with	O
the	O
"	O
para	O
"	O
representation	O
from	O
,	O
(	O
c	O
)	O
DRR	O
Top	O
-	O
4	O
,	O
(	O
d	O
)	O
DRR	O
(	O
Re	O
-	O
Rank	O
+	O
Top	O
-	O
2	O
(	O
τ	O
=	O
1	O
)	O
)	O
for	O
training	O
,	O
de	O
-	O
velopment	O
and	O
test	O
sets	O
,	O
(	O
e	O
)	O
training	O
a	O
supervised	O
classifier	O
with	O
a	O
human	O
oracle	O
i.e.	O
annotated	O
evidence	O
extraction	O
as	O
discussed	O
in	O
$	O
3	O
,	O
and	O
using	O
the	O
best	O
extraction	O
model	O
,	O
i.e.	O
supervised	O
evidence	O
extraction	O
with	O
Hard	O
Negative	O
(	O
3×	O
)	O
for	O
the	O
test	O
sets	O
,	O
and	O
(	O
f	O
)	O
the	O
human	O
oracle	O
across	O
the	O
training	O
,	O
development	O
,	O
and	O
test	O
sets	O
.	O

Unsupervised	O
evidence	O
extraction	O
.	O
For	O
RQ1	O
,	O
Table	O
2	O
shows	O
the	O
performance	O
of	O
unsupervised	O
methods	O
.	O
We	O
see	O
that	O
the	O
contextual	O
embedding	O
method	O
,	O
SimCSE	B-MethodName
-	O
Supervised	O
(	O
Hypo	O
-	O
Title	O
-	O
Swap	O
+	O
Re	O
-	O
Rank	O
+	O
Top	O
-	O
2	O
(	O
τ	O
=	O
1	O
)	O
)	O
,	O
performs	O
the	O
best	O
.	O
Among	O
the	O
static	O
embedding	O
cases	O
,	O
DRR	O
(	O
Re	O
-	O
Rank	O
+	O
Top	O
-	O
2	O
(	O
τ	O
=	O
1	O
)	O
)	O
sees	O
substantial	O
performance	O
improvement	O
over	O
the	O
original	O
DRR	O
baseline	O
.	O
The	O
alignment	O
based	O
approach	O
using	O
SimAlign	O
underperforms	O
,	O
especially	O
on	O
the	O
α	B-HyperparameterName
1	O
and	O
α	B-HyperparameterName
2	O
test	O
sets	O
.	O
However	O
,	O
its	O
performance	O
on	O
the	O
α	B-HyperparameterName
3	O
data	O
,	O
with	O
out	O
of	O
domain	O
and	O
longer	O
tables	O
,	O
is	O
competitive	O
to	O
other	O
methods	O
.	O
Overall	O
,	O
the	O
idea	O
of	O
using	O
Top	O
-	O
S	O
τ	O
,	O
i.e.	O
,	O
using	O
the	O
dynamic	O
number	O
of	O
rows	O
prediction	O
and	O
Re	O
-	O
Rank	O
(	O
exact	O
-	O
match	O
based	O
re	O
-	O
ranking	O
)	O
is	O
beneficial	O
.	O
Previously	O
used	O
approaches	O
such	O
as	O
DRR	O
and	O
WMD	O
have	O
low	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
,	O
because	O
of	O
poor	O
precision	O
.	O
Using	O
Re	O
-	O
Rank	O
based	O
on	O
exact	B-MetricName
match	I-MetricName
improves	O
the	O
evidence	O
extraction	O
recall	O
.	O
Furthermore	O
,	O
introducing	O
sparsity	O
with	O
Top	O
-	O
S	O
τ	O
,	O
i.e.	O
considering	O
only	O
the	O
Top	O
-	O
2	O
rows	O
(	O
S=2	O
)	O
and	O
dynamic	O
row	O
selection	O
(	O
τ	O
=	O
1	O
)	O
substantially	O
enhances	O
evidence	O
extraction	O
precision	O
.	O
Furthermore	O
,	O
the	O
zero	O
weighting	O
of	O
title	O
matches	O
using	O
the	O
Hypo	O
-	O
Title	O
-	O
Swap	O
heuristic	O
,	O
benefits	O
contextualized	O
embedding	O
models	O
such	O
as	O
SimCSE	B-MethodName
12	O
.	O
SimCSE	B-MethodName
-	O
supervised	O
(	O
Hypo	O
-	O
Title	O
-	O
Swap	O
+	O
Re	O
-	O
Rank	O
+	O
Top	O
-	O
2	O
(	O
τ	O
=	O
1	O
)	O
)	O
outperforms	O
DRR	O
(	O
Re	O
-	O
Rank	O
+	O
Top	O
-	O
2	O
(	O
τ	O
=	O
1	O
)	O
)	O
by	O
4.3	O
%	O
(	O
α	B-HyperparameterName
1	O
)	O
,	O
2.5	O
%	O
(	O
α	B-HyperparameterName
2	O
)	O
and	O
5.4	O
%	O
(	O
α	B-HyperparameterName
3	O
)	O
absolute	O
score	O
.	O
Since	O
the	O
table	O
domains	O
and	O
the	O
NLI	O
reasoning	O
involved	O
for	O
α	B-HyperparameterName
1	O
and	O
α	B-HyperparameterName
2	O
are	O
sim	O
-	O
ilar	O
,	O
so	O
is	O
their	O
evidence	O
extraction	O
performance	O
.	O
However	O
,	O
the	O
performance	O
of	O
α	B-HyperparameterName
3	O
,	O
which	O
contains	O
out	O
-	O
of	O
-	O
domain	O
and	O
longer	O
tables	O
(	O
an	O
average	O
of	O
thirteen	O
rows	O
,	O
versus	O
nine	O
rows	O
in	O
α	B-HyperparameterName
1	O
and	O
α	B-HyperparameterName
2	O
)	O
is	O
relatively	O
worse	O
.	O
The	O
unsupervised	O
approaches	O
are	O
still	O
12.69	O
%	O
(	O
α	B-HyperparameterName
1	O
)	O
,	O
13.49	O
%	O
(	O
α	B-HyperparameterName
2	O
)	O
,	O
and	O
19.81	O
%	O
(	O
α	B-HyperparameterName
3	O
)	O
behind	O
the	O
human	O
performance	O
,	O
highlighting	O
the	O
challenges	O
of	O
the	O
task	O
.	O
Supervised	O
evidence	O
extraction	O
.	O
For	O
RQ2	O
,	O
Table	O
4	O
shows	O
the	O
performance	O
of	O
the	O
supervised	O
relevant	O
row	O
extraction	O
approaches	O
that	O
use	O
binary	O
classifiers	O
trained	O
with	O
several	O
sampling	O
techniques	O
for	O
irrelevant	O
rows	O
.	O
Overall	O
,	O
adding	O
supervision	O
is	O
advantageous	O
13	O
.	O
Furthermore	O
,	O
we	O
observe	O
that	O
using	O
the	O
unsupervised	O
DRR	O
technique	O
to	O
extract	O
challenging	O
irrelevant	O
rows	O
,	O
i.e.	O
,	O
Hard	O
Negative	O
,	O
is	O
more	O
effective	O
than	O
random	O
sampling	O
.	O
Indeed	O
,	O
using	O
random	O
negative	O
examples	O
as	O
the	O
irrelevant	O
rows	O
performs	O
the	O
worst	O
.	O
Not	O
sampling	O
(	O
6×	O
)	O
or	O
using	O
only	O
one	O
irrelevant	O
row	O
,	O
namely	O
Hard	O
Negative	O
(	O
1×	O
)	O
,	O
also	O
underperforms	O
.	O
We	O
see	O
that	O
employing	O
moderate	O
sampling	O
,	O
i.e.	O
,	O
Hard	O
Negative	O
(	O
3×	O
)	O
,	O
performs	O
best	O
across	O
all	O
test	O
sets	O
.	O
The	O
best	O
supervised	O
model	O
with	O
Hard	O
Negative	O
(	O
3×	O
)	O
sampling	O
improves	O
evidence	O
extraction	O
performance	O
by	O
8.7	O
%	O
(	O
α	B-HyperparameterName
1	O
)	O
,	O
10.8	O
%	O
(	O
α	B-HyperparameterName
2	O
)	O
,	O
and	O
4.2	O
%	O
(	O
α	B-HyperparameterName
3	O
)	O
absolute	O
score	O
over	O
the	O
best	O
unsupervised	O
model	O
,	O
namely	O
SimCSE	B-MethodName
-	O
Supervised	O
(	O
Hypo	O
-	O
Title	O
-	O
Swap	O
+	O
Re	O
-	O
Rank	O
+	O
Top	O
-	O
2	O
(	O
τ	O
=	O
1	O
)	O
)	O
.	O
14	O
The	O
human	O
oracle	O
outperforms	O
the	O
best	O
supervised	O
model	O
by	O
4.13	O
%	O
(	O
α	B-HyperparameterName
1	O
)	O
and	O
2.65	O
%	O
(	O
α	B-HyperparameterName
2	O
)	O
absolute	O
scores	O
-	O
a	O
smaller	O
gap	O
than	O
the	O
best	O
unsupervised	O
approach	O
.	O
We	O
also	O
observe	O
that	O
the	O
supervision	O
does	O
not	O
benefit	O
the	O
α	B-HyperparameterName
3	O
set	O
much	O
,	O
where	O
the	O
performance	O
gap	O
to	O
humans	O
is	O
still	O
about	O
15.95	O
%	O
(	O
only	O
3.80	O
%	O
improvement	O
over	O
unsupervised	O
approach	O
)	O
.	O
We	O
suspect	O
this	O
is	O
because	O
of	O
the	O
distributional	O
changes	O
in	O
α	B-HyperparameterName
3	O
set	O
noted	O
earlier	O
.	O
13	O
We	O
investigate	O
"	O
How	O
much	O
supervision	O
is	O
adequate	O
?	O
"	O
in	O
Appendix	O
A.	O
14	O
Although	O
α2	O
is	O
adversarial	O
owing	O
to	O
label	O
flipping	O
,	O
rendering	O
the	O
NLI	O
task	O
more	O
difficult	O
,	O
both	O
α1	O
and	O
α2	O
have	O
instances	O
with	O
the	O
same	O
domain	O
tables	O
and	O
hypotheses	O
with	O
similar	O
reasoning	O
types	O
,	O
making	O
the	O
relevant	O
row	O
extraction	O
task	O
equally	O
challenging	O
.	O
This	O
highlights	O
directions	O
for	O
future	O
improvement	O
via	O
domain	B-TaskName
adaptation	I-TaskName
.	O

For	O
RQ3	O
,	O
we	O
investigate	O
how	O
using	O
only	O
extracted	O
evidence	O
as	O
a	O
premise	O
impacts	O
the	O
performance	O
of	O
the	O
tabular	O
NLI	O
task	O
.	O
Table	O
3	O
shows	O
the	O
results	O
.	O
Compared	O
to	O
the	O
baseline	O
DRR	O
,	O
our	O
unsupervised	O
DRR	O
(	O
Re	O
-	O
Rank	O
+	O
Top	O
-	O
2	O
(	O
τ	O
=	O
1	O
)	O
)	O
performs	O
similarly	O
for	O
α	B-HyperparameterName
2	O
,	O
worse	O
by	O
1.12	O
%	O
on	O
α	B-HyperparameterName
1	O
,	O
and	O
outperforms	O
by	O
0.95	O
%	O
on	O
α	B-HyperparameterName
3	O
.	O
Using	O
evidence	O
extraction	O
with	O
the	O
best	O
supervised	O
model	O
,	O
Hard	O
Negative	O
(	O
3×	O
)	O
,	O
trained	O
on	O
human	O
-	O
extracted	O
(	O
Oracle	O
)	O
rows	O
results	O
in	O
2.68	O
%	O
(	O
α	B-HyperparameterName
1	O
)	O
,	O
3.93	O
%	O
(	O
α	B-HyperparameterName
2	O
)	O
,	O
and	O
4.04	O
%	O
(	O
α	B-HyperparameterName
3	O
)	O
improvements	O
against	O
DRR	O
.	O
Furthermore	O
,	O
using	O
human	O
extracted	O
(	O
Oracle	O
)	O
rows	O
for	O
both	O
training	O
and	O
testing	O
sets	O
outperforms	O
all	O
models	O
-	O
based	O
extraction	O
methods	O
.	O
The	O
human	O
oracle	O
based	O
evidence	O
extraction	O
leads	O
to	O
largest	O
performance	O
improvements	O
of	O
3.05	O
%	O
(	O
α	B-HyperparameterName
1	O
)	O
,	O
4.39	O
%	O
(	O
α	B-HyperparameterName
2	O
)	O
,	O
and	O
6.67	O
%	O
(	O
α	B-HyperparameterName
3	O
)	O
over	O
DRR	O
.	O
Overall	O
,	O
these	O
findings	O
indicate	O
that	O
extracting	O
evidence	O
is	O
beneficial	O
for	O
reasoning	O
in	O
tabular	O
inference	O
task	O
.	O
Despite	O
using	O
human	O
extracted	O
(	O
Oracle	O
)	O
rows	O
for	O
both	O
training	O
and	O
testing	O
,	O
the	O
NLI	O
model	O
still	O
falls	O
far	O
behind	O
human	O
reasoning	O
(	O
Human	O
NLI	O
)	O
.	O
This	O
gap	O
exists	O
because	O
,	O
in	O
addition	O
to	O
extracting	O
evidence	O
,	O
the	O
INFOTABS	B-DatasetName
hypotheses	O
require	O
inference	O
with	O
the	O
evidence	O
involving	O
common	O
-	O
sense	O
and	O
knowledge	O
,	O
which	O
the	O
NLI	O
component	O
does	O
not	O
adequately	O
perform	O
.	O

We	O
perform	O
an	O
error	O
analysis	O
of	O
how	O
well	O
our	O
proposed	O
supervised	O
extraction	O
model	O
(	O
Hard	O
Negative	O
(	O
3x	O
)	O
)	O
performs	O
compared	O
to	O
the	O
human	O
annotators	O
.	O
The	O
model	O
makes	O
two	O
types	O
of	O
errors	O
:	O
a	O
Type	O
I	O
error	O
occurs	O
when	O
an	O
evidence	O
row	O
is	O
marked	O
as	O
irrelevant	O
,	O
whereas	O
Type	O
II	O
error	O
occurs	O
when	O
an	O
irrelevant	O
row	O
is	O
marked	O
as	O
evidence	O
.	O
A	O
Type	O
I	O
error	O
will	O
reduce	O
the	O
model	O
's	O
precision	O
for	O
the	O
extraction	O
model	O
,	O
whereas	O
a	O
Type	O
II	O
error	O
will	O
decrease	O
the	O
model	O
's	O
recall	O
.	O
Type	O
I	O
errors	O
are	O
especially	O
concerning	O
for	O
the	O
downstream	O
NLI	O
task	O
.	O
Since	O
mislabeled	O
evidence	O
rows	O
will	O
be	O
absent	O
from	O
the	O
extracted	O
premise	O
,	O
necessary	O
evidence	O
will	O
be	O
omitted	O
,	O
leading	O
to	O
inaccurate	O
entailment	O
labels	O
.	O
On	O
the	O
other	O
hand	O
,	O
with	O
Type	O
II	O
errors	O
,	O
when	O
an	O
irrelevant	O
row	O
is	O
labeled	O
as	O
evidence	O
,	O
the	O
model	O
has	O
to	O
deal	O
with	O
from	O
extra	O
noise	O
in	O
the	O
premise	O
.	O
However	O
,	O
all	O
the	O
required	O
evidence	O
remains	O
.	O
Table	O
5	O
shows	O
a	O
comparison	O
of	O
the	O
supervised	O
extraction	O
(	O
Hard	O
Negative	O
(	O
3x	O
)	O
)	O
approach	O
with	O
the	O
ground	O
truth	O
human	O
labels	O
on	O
all	O
the	O
three	O
test	O
sets	O
for	O
both	O
error	O
types	O
.	O
On	O
the	O
α	B-HyperparameterName
3	O
set	O
,	O
Type	O
-	O
I	O
and	O
Type	O
-	O
II	O
errors	O
are	O
substantially	O
higher	O
than	O
α	B-HyperparameterName
1	O
and	O
α	B-HyperparameterName
2	O
.	O
This	O
highlights	O
the	O
fact	O
that	O
on	O
the	O
α	B-HyperparameterName
3	O
set	O
,	O
the	O
model	O
disagrees	O
with	O
with	O
humans	O
the	O
most	O
.	O
Furthermore	O
,	O
the	O
ratio	O
of	O
Type	O
-	O
II	O
over	O
Type	O
-	O
I	O
errors	O
is	O
much	O
higher	O
for	O
α	B-HyperparameterName
3	O
.	O
This	O
indicates	O
that	O
the	O
super	O
-	O
vised	O
extraction	O
model	O
marks	O
many	O
irrelevant	O
rows	O
as	O
evidence	O
(	O
Type	O
-	O
II	O
error	O
)	O
for	O
α	B-HyperparameterName
3	O
set	O
.	O
The	O
out	O
-	O
ofdomain	O
origin	O
of	O
α	B-HyperparameterName
3	O
tables	O
,	O
as	O
well	O
as	O
their	O
larger	O
size	O
,	O
might	O
be	O
one	O
explanation	O
for	O
this	O
poor	O
performance	O
.	O
Appendix	O
C	O
provides	O
several	O
examples	O
of	O
both	O
types	O
of	O
errors	O
.	O

Why	O
Sequential	O
Prediction	O
?	O
Our	O
choice	O
of	O
the	O
sequential	O
paradigm	O
is	O
motivated	O
by	O
the	O
observation	O
that	O
it	O
enforces	O
a	O
causal	O
structure	O
.	O
Of	O
course	O
,	O
a	O
joint	O
or	O
a	O
multi	O
-	O
task	O
model	O
may	O
make	O
better	O
predictions	O
.	O
However	O
,	O
these	O
models	O
ignore	O
the	O
causal	O
relationship	O
between	O
evidence	O
selection	O
and	O
label	O
prediction	O
(	O
Herzig	O
et	O
al	O
,	O
2021	O
;	O
Paranjape	O
et	O
al	O
,	O
2020	O
)	O
.	O
Ideally	O
,	O
each	O
row	O
is	O
independent	O
and	O
,	O
its	O
relevance	O
to	O
the	O
hypothesis	O
can	O
be	O
determined	O
on	O
its	O
own	O
.	O
In	O
a	O
joint	O
or	O
a	O
multi	O
-	O
task	O
model	O
that	O
exploits	O
correlations	O
across	O
rows	O
and	O
the	O
final	O
label	O
,	O
irrelevant	O
rows	O
and	O
the	O
NLI	O
label	O
,	O
can	O
erroneously	O
influence	O
row	O
selection	O
.	O
Future	O
Directions	O
.	O
Based	O
on	O
the	O
observations	O
and	O
discussions	O
,	O
we	O
identify	O
the	O
future	O
directions	O
as	O
follows	O
.	O
(	O
1	O
)	O
Joint	O
Causal	O
Model	O
.	O
To	O
build	O
a	O
joint	O
or	O
a	O
multi	O
-	O
task	O
model	O
that	O
follows	O
the	O
causal	O
reasoning	O
structure	O
,	O
significant	O
changes	O
in	O
model	O
architecture	O
are	O
required	O
.	O
Such	O
a	O
model	O
would	O
first	O
identify	O
important	O
rows	O
and	O
then	O
use	O
them	O
for	O
NLI	O
predictions	O
,	O
but	O
without	O
risking	O
spurious	O
correlations	O
.	O
(	O
2	O
)	O
How	O
much	O
Supervision	O
is	O
Needed	O
?	O
As	O
evident	O
from	O
our	O
experiments	O
,	O
relevant	O
row	O
supervision	O
improves	O
the	O
evidence	O
extraction	O
,	O
especially	O
on	O
α	B-HyperparameterName
1	O
and	O
α	B-HyperparameterName
2	O
sets	O
compared	O
to	O
unsupervised	O
extraction	O
.	O
But	O
do	O
we	O
need	O
full	O
supervision	O
for	O
all	O
examples	O
?	O
Is	O
there	O
any	O
lower	O
limit	O
to	O
supervision	O
?	O
We	O
partially	O
answered	O
this	O
question	O
in	O
the	O
affirmative	O
by	O
training	O
the	O
evidence	O
extraction	O
model	O
with	O
limited	O
supervision	O
(	O
semi	O
-	O
supervised	O
setting	O
)	O
,	O
but	O
a	O
deeper	O
analysis	O
is	O
needed	O
to	O
understand	O
the	O
limits	O
.	O
See	O
Appendix	O
A	O
for	O
details	O
.	O
(	O
3	O
)	O
Improving	O
Zero	O
-	O
shot	O
Domain	O
Performance	O
.	O
As	O
evident	O
from	O
5.2	O
,	O
the	O
evidence	O
extraction	O
performance	O
of	O
outof	O
-	O
domain	O
tables	O
in	O
α	B-HyperparameterName
3	O
needs	O
further	O
improvements	O
,	O
setting	O
up	O
a	O
domain	B-TaskName
adaptation	I-TaskName
research	O
question	O
as	O
future	O
work	O
.	O
(	O
4	O
)	O
Finally	O
,	O
inspired	O
by	O
Neeraja	O
et	O
al	O
(	O
2021	O
)	O
,	O
we	O
may	O
be	O
able	O
to	O
add	O
explicit	O
knowledge	O
to	O
improve	O
evidence	O
extraction	O
.	O

Tabular	O
Reasoning	O
Many	O
recent	O
studies	O
investigate	O
various	O
NLP	O
tasks	O
on	O
semi	O
-	O
structured	O
tabular	O
data	O
,	O
including	O
tabular	O
NLI	O
and	O
fact	B-TaskName
verification	I-TaskName
(	O
Chen	O
et	O
al	O
,	O
2020b	O
;	O
,	O
various	O
question	B-TaskName
answering	I-TaskName
and	O
semantic	B-TaskName
parsing	I-TaskName
tasks	O
(	O
Zhang	O
and	O
Balog	O
,	O
2020	O
;	O
Pasupat	O
and	O
Liang	O
,	O
2015	O
;	O
Krishnamurthy	O
et	O
al	O
,	O
2017	O
;	O
Abbas	O
et	O
al	O
,	O
2016	O
;	O
Sun	O
et	O
al	O
,	O
2016	O
;	O
Chen	O
et	O
al	O
,	O
2020c	O
;	O
Lin	O
et	O
al	O
,	O
2020	O
;	O
Zayats	O
et	O
al	O
,	O
2021	O
;	O
Oguz	O
et	O
al	O
,	O
2020	O
;	O
Chen	O
et	O
al	O
,	O
2021	O
,	O
inter	O
alia	O
)	O
,	O
andtable	O
-	O
to	O
-	O
text	B-TaskName
generation	I-TaskName
(	O
e.g.	O
,	O
Parikh	O
et	O
al	O
,	O
2020	O
;	O
Nan	O
et	O
al	O
,	O
2021	O
;	O
Yoran	O
et	O
al	O
,	O
2021	O
;	O
Chen	O
et	O
al	O
,	O
2020a	O
)	O
.	O
Several	O
strategies	O
for	O
representing	O
Wikipedia	O
relational	O
tables	O
are	O
proposed	O
,	O
such	O
as	O
Ta	O
-	O
ble2vec	O
(	O
Deng	B-DatasetName
et	I-DatasetName
al	I-DatasetName
,	O
2019	O
)	O
,	O
TAPAS	B-MethodName
(	O
Herzig	O
et	O
al	O
,	O
2020	O
)	O
,	O
TaBERT	B-MethodName
(	O
Yin	O
et	O
al	O
,	O
2020	O
)	O
,	O
TabStruc	O
(	O
Zhang	O
et	O
al	O
,	O
2020a	O
)	O
,	O
TABBIE	B-MethodName
(	O
Iida	O
et	O
al	O
,	O
2021	O
)	O
,	O
TabGCN	O
(	O
Pramanick	O
and	O
Bhattacharya	O
,	O
2021	O
)	O
and	O
RCI	O
(	O
Glass	O
et	O
al	O
,	O
2021	O
)	O
.	O
Yu	O
et	O
al	O
(	O
2018	O
;	O
and	O
Neeraja	O
et	O
al	O
(	O
2021	O
)	O
study	O
pre	O
-	O
training	O
for	O
improving	O
tabular	O
inference	O
.	O
Interpretability	O
and	O
Explainability	O
Model	O
interpretability	O
can	O
either	O
be	O
through	O
explanations	O
or	O
by	O
identifying	O
the	O
evidence	O
for	O
the	O
predictions	O
(	O
Feng	O
et	O
al	O
,	O
2018	O
;	O
Serrano	O
and	O
Smith	O
,	O
2019	O
;	O
Jain	O
and	O
Wallace	O
,	O
2019	O
;	O
Wiegreffe	O
and	O
Pinter	O
,	O
2019	O
;	O
DeYoung	O
et	O
al	O
,	O
2020	O
;	O
Paranjape	O
et	O
al	O
,	O
2020	O
)	O
.	O
Additionally	O
,	O
NLI	O
models	O
(	O
e.g.	O
Ribeiro	O
et	O
al	O
,	O
2016Ribeiro	O
et	O
al	O
,	O
,	O
2018aZhao	O
et	O
al	O
,	O
2018	O
;	O
Glockner	O
et	O
al	O
,	O
2018	O
;	O
Naik	O
et	O
al	O
,	O
2018	O
;	O
McCoy	O
et	O
al	O
,	O
2019	O
;	O
Nie	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2019a	O
)	O
must	O
be	O
subjected	O
to	O
numerous	O
test	O
sets	O
with	O
adversarial	O
settings	O
.	O
These	O
settings	O
can	O
focus	O
on	O
various	O
aspects	O
of	O
reasoning	O
,	O
such	O
as	O
perturbed	O
premises	O
for	O
evidence	O
selection	O
,	O
zeroshot	O
transferability	O
(	O
α	B-HyperparameterName
3	O
)	O
,	O
counterfactual	O
premises	O
(	O
Jain	O
et	O
al	O
,	O
2021	O
)	O
,	O
and	O
contrasting	O
hypotheses	O
α	B-HyperparameterName
2	O
.	O
Recently	O
,	O
Kumar	B-DatasetName
and	O
Talukdar	O
(	O
2020	O
)	O
introduced	O
Natural	O
-	O
language	O
Inference	O
over	O
Label	O
-	O
specific	O
Explanations	O
(	O
NILE	O
)	O
,	O
an	O
NLI	O
approach	O
for	O
generating	O
labels	O
and	O
accompanying	O
faithful	O
explanations	O
using	O
auto	O
-	O
generated	O
label	O
-	O
specific	O
natural	O
language	O
explanations	O
.	O
Our	O
work	O
focuses	O
on	O
the	O
extraction	O
of	O
label	O
-	O
independent	O
evidence	O
for	O
correct	O
inference	O
,	O
rather	O
than	O
on	O
the	O
generation	O
of	O
abstractive	O
explanations	O
for	O
a	O
given	O
label	O
.	O
Comparison	O
with	O
Shared	O
Tasks	O
The	O
Se	O
-	O
mEval'21	O
Task	O
9	O
(	O
Wang	O
et	O
al	O
,	O
2021b	O
)	O
and	O
FEVEROUS'21	O
shared	O
task	O
(	O
Aly	O
et	O
al	O
,	O
2021	O
)	O
are	O
conceptually	O
close	O
to	O
this	O
work	O
.	O
The	O
SemEval	O
task	O
focuses	O
on	O
statement	O
verification	O
and	O
evidence	O
extraction	O
using	O
relational	O
tables	O
from	O
scientific	O
articles	O
.	O
In	O
this	O
work	O
,	O
we	O
focus	O
on	O
item	O
evidence	O
extraction	O
for	O
non	O
-	O
scientific	O
Wikipedia	O
Infobox	O
entity	O
tables	O
,	O
proposed	O
a	O
twostage	O
sequential	O
approach	O
,	O
and	O
used	O
the	O
INFOTABS	B-DatasetName
dataset	O
which	O
has	O
complex	O
reasoning	O
and	O
multiple	O
adversarial	O
tests	O
for	O
robust	O
evaluation	O
.	O
The	O
FEVEROUS'21	O
shared	O
task	O
focuses	O
on	O
verifying	O
information	O
using	O
unstructured	O
and	O
structured	O
evidence	O
from	O
open	O
-	O
domain	O
Wikipedia	O
.	O
Our	O
approach	O
concerns	O
evidence	O
extraction	O
from	O
a	O
single	O
table	O
rather	O
than	O
open	O
-	O
domain	O
document	O
,	O
table	O
or	O
paragraph	O
retrieval	O
.	O
Furthermore	O
,	O
we	O
are	O
only	O
concerned	O
with	O
entity	O
tables	O
rather	O
than	O
relational	O
tables	O
or	O
unstructured	O
text	O
,	O
while	O
the	O
FEVEROUS	B-DatasetName
data	O
has	O
relational	O
tables	O
,	O
unstructured	O
text	O
,	O
and	O
fewer	O
entity	O
tables	O
.	O

To	O
investigate	O
this	O
,	O
we	O
use	O
Hard	O
Negative	O
(	O
3x	O
)	O
with	O
RoBERTa	B-MethodName
LARGE	O
model	O
as	O
our	O
evidence	O
extraction	O
classifier	O
,	O
which	O
is	O
similar	O
to	O
the	O
full	O
supervision	O
method	O
.	O
To	O
simulate	O
semi	O
-	O
supervision	O
settings	O
,	O
we	O
randomly	O
sample	O
10	O
%	O
,	O
20	O
%	O
,	O
30	O
%	O
,	O
40	O
%	O
,	O
and	O
50	O
%	O
example	O
instances	O
of	O
the	O
train	O
set	O
in	O
an	O
incremental	O
fashion	O
for	O
model	O
training	O
,	O
where	O
we	O
repeat	O
the	O
random	O
samplings	O
three	O
times	O
.	O
Figure	O
3	O
,	O
4	O
,	O
and	O
5	O
compare	O
the	O
average	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
over	O
three	O
runs	O
on	O
the	O
three	O
test	O
sets	O
α	B-HyperparameterName
1	O
,	O
α	B-HyperparameterName
2	O
and	O
α	B-HyperparameterName
3	O
respectively	O
.	O
We	O
discovered	O
that	O
adding	O
some	O
supervision	O
had	O
advantages	O
over	O
not	O
having	O
any	O
supervision	O
.	O
However	O
,	O
we	O
also	O
find	O
that	O
20	O
%	O
supervision	O
is	O
adequate	O
for	O
reasonably	O
good	O
evidence	O
extraction	O
with	O
only	O
<	O
5	O
%	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
gap	O
with	O
full	O
supervision	O
.	O
One	O
key	O
issue	O
we	O
observe	O
is	O
the	O
lack	O
of	O
a	O
visible	O
trend	O
due	O
to	O
significant	O
variation	O
produced	O
by	O
random	O
data	O
sub	O
-	O
sampling	O
.	O
It	O
would	O
be	O
worthwhile	O
to	O
explore	O
if	O
this	O
volatility	O
could	O
be	O
reduced	O
by	O
strategic	O
sampling	O
using	O
an	O
unsupervised	O
extraction	O
model	O
,	O
an	O
active	B-TaskName
learning	I-TaskName
framework	O
,	O
and	O
strategic	O
diversity	O
maximizing	O
sampling	O
,	O
which	O
is	O
left	O
as	O
future	O
work	O
.	O

Towards	O
automated	O
evaluation	O
Some	O
aspects	O
of	O
the	O
evaluation	O
dimensions	O
are	O
built	O
into	O
the	O
way	O
the	O
poems	O
are	O
constructed	O
.	O
Form	O
:	O
the	O
haiku	O
-	O
generating	O
subsystem	O
guarantees	O
that	O
the	O
requirements	O
of	O
a	O
grammatical	O
skeleton	O
are	O
met	O
,	O
and	O
the	O
5/7/5	O
syllable	O
pattern	O
is	O
guaranteed	O
(	O
up	O
to	O
the	O
accuracy	B-MetricName
of	O
the	O
CMU	O
Pronouncing	O
Dictionary	O
)	O
.	O
Surface	O
form	O
scales	O
up	O
well	O
for	O
rengas	O
.	O
Sense	O
:	O
the	O
haiku	O
generating	O
subsystem	O
uses	O
an	O
ngram	O
model	O
of	O
text	O
likelihood	O
,	O
which	O
will	O
yield	O
a	O
higher	O
score	O
for	O
constructions	O
that	O
match	O
frequently	O
observed	O
phrases	O
.	O
In	O
our	O
first	O
round	O
of	O
experiments	O
with	O
rengas	O
,	O
sense	O
tended	O
to	O
degrade	O
quickly	O
.	O
Our	O
subsequent	O
adaptations	O
to	O
the	O
renga	O
generation	O
algorithm	O
prioritise	O
greater	O
continuitity	O
between	O
links	O
.	O
Topic	O
:	O
we	O
used	O
a	O
vector	O
model	O
of	O
the	O
topic	O
word	O
(	O
s	O
)	O
,	O
and	O
can	O
measure	O
the	O
distance	O
to	O
the	O
vector	O
given	O
by	O
the	O
sum	O
of	O
the	O
words	O
in	O
the	O
poem	O
.	O
Emotion	O
:	O
In	O
our	O
experiment	O
with	O
FloWr	O
,	O
we	O
used	O
a	O
quite	O
simple	O
method	O
,	O
filtering	O
a	O
list	O
for	O
the	O
"	O
most	O
positive	O
"	O
haikus	O
.	O
Mohammad	O
(	O
2016	O
)	O
surveys	O
more	O
recent	O
work	O
in	O
NLP	O
on	O
modelling	O
emotion	B-DatasetName
,	O
which	O
could	O
be	O
exploited	O
in	O
future	O
work	O
.	O
Beauty	O
:	O
Waugh	O
(	O
1980	O
)	O
points	O
out	O
that	O
language	O
is	O
based	O
on	O
a	O
"	O
hierarchy	O
of	O
signs	O
.	O
.	O
.	O
of	O
ascending	O
complexity	O
,	O
but	O
also	O
one	O
of	O
ascending	O
freedom	O
or	O
creativity	O
,	O
"	O
and	O
also	O
remarks	O
that	O
a	O
"	O
poem	O
provides	O
its	O
own	O
'	O
universe	O
of	O
discourse	O
.	O
'	O
"	O
To	O
some	O
extent	O
these	O
criteria	O
pull	O
in	O
opposite	O
directions	O
:	O
towards	O
complexity	O
,	O
and	O
towards	O
coherence	O
,	O
respectively	O
.	O
Our	O
first	O
rengas	O
could	O
not	O
be	O
reasonably	O
described	O
as	O
a	O
'	O
universe	O
of	O
discourse	O
'	O
but	O
rather	O
,	O
a	O
'	O
universe	O
of	O
random	O
nonsense	O
'	O
.	O
This	O
is	O
improved	O
in	O
the	O
subsequent	O
experiment	O
.	O
Traditional	O
rengas	O
forbid	O
repetition	O
,	O
and	O
discourage	O
overt	O
reflection	O
on	O
themes	O
like	O
death	O
,	O
war	O
,	O
illness	O
,	O
impermanence	O
,	O
religion	O
and	O
sex	O
(	O
Carley	O
,	O
2015	O
)	O
.	O
Thus	O
,	O
despite	O
being	O
coherent	O
,	O
the	O
repetitive	O
"	O
military	O
"	O
theme	O
in	O
the	O
final	O
example	O
above	O
is	O
not	O
appropriate	O
to	O
classical	O
constraints	O
.	O
A	O
reader	O
may	O
identify	O
some	O
fortuitous	O
resonances	O
,	O
e.g.	O
,	O
"	O
the	O
flower	O
war	O
"	O
is	O
interesting	O
within	O
the	O
"	O
afghan	O
"	O
context	O
established	O
in	O
earlier	O
links	O
-	O
but	O
the	O
system	O
itself	O
does	O
not	O
yet	O
recognise	O
these	O
features	O
.	O
Some	O
paths	O
forward	O
Wiggins	O
and	O
Forth	O
(	O
2015	O
)	O
use	O
hierarchical	O
models	O
in	O
a	O
system	O
that	O
builds	O
a	O
formative	O
evaluation	O
as	O
it	O
composes	O
or	O
reads	O
sentences	O
,	O
judging	O
how	O
well	O
they	O
match	O
learned	O
patterns	O
.	O
While	O
this	O
seems	O
to	O
have	O
more	O
to	O
do	O
with	O
constraints	O
around	O
typicality	O
,	O
per	O
Waugh	O
,	O
there	O
is	O
room	O
for	O
creativity	O
within	O
hierarchies	O
.	O
Hoey	O
(	O
2005	O
)	O
makes	O
a	O
convincing	O
argument	O
that	O
satisfying	O
lexical	O
constraints	O
while	O
violating	O
some	O
familiar	O
patterns	O
may	O
come	O
across	O
as	O
interesting	O
and	O
creative	O
.	O
Word	O
similarities	O
can	O
be	O
found	O
using	O
GloVe	B-MethodName
:	O
this	O
would	O
presumably	O
produce	O
links	O
with	O
more	O
coherent	O
meanings	O
,	O
compared	O
to	O
the	O
edit	O
distance	O
-	O
based	O
measure	O
we	O
used	O
.	O
Ali	O
Javaheri	O
Javid	O
et	O
al	O
(	O
2016	O
)	O
use	O
information	O
gain	O
to	O
model	O
the	O
aesthetics	O
of	O
cellular	O
automata	O
.	O
Can	O
these	O
ideas	O
be	O
combined	O
to	O
model	O
evolving	O
topic	O
salience	O
,	O
complexity	O
,	O
and	O
coherence	O
?	O
If	O
the	O
system	O
provided	O
a	O
razo	O
(	O
the	O
troubadours	O
'	O
jargon	O
for	O
"	O
rationale	O
"	O
;	O
see	O
Agamben	O
(	O
1999	O
,	O
p.	O
79	O
)	O
)	O
,	O
we	O
could	O
debug	O
that	O
,	O
and	O
perhaps	O
involve	O
additional	O
AI	O
systems	O
in	O
the	O
process	O
(	O
Corneli	O
et	O
al	O
,	O
2015	O
)	O
.	O

We	O
explore	O
whether	O
contemporary	O
vector	O
-	O
space	O
sentence	O
representation	O
techniques	O
also	O
provide	O
a	O
structured	O
representation	O
of	O
the	O
different	O
messages	O
in	O
"	O
dogwhistle	B-DatasetName
"	O
political	O
communication	O
.	O
A	O
dogwhistle	B-DatasetName
refers	O
to	O
a	O
word	O
or	O
phrase	O
used	O
in	O
manipulative	O
communication	O
,	O
usually	O
in	O
a	O
political	O
context	O
.	O
Dogwhistles	O
carry	O
at	O
least	O
two	O
messages	O
:	O
one	O
message	O
intended	O
for	O
the	O
broader	O
community	O
,	O
and	O
another	O
"	O
payload	O
"	O
message	O
intended	O
to	O
communicate	O
a	O
specific	O
,	O
less	O
acceptable	O
message	O
to	O
a	O
receptive	O
"	O
in	O
-	O
group	O
"	O
.	O
Dogwhistles	O
depend	O
on	O
the	O
"	O
out	O
-	O
group	O
"	O
members	O
not	O
picking	O
up	O
on	O
the	O
payload	O
message	O
(	O
Albertson	O
,	O
2014	O
;	O
Bhat	O
and	O
Klein	O
,	O
2020	O
)	O
.	O
We	O
take	O
several	O
Swedish	O
-	O
language	O
dogwhistles	O
and	O
survey	O
data	O
from	O
the	O
Swedish	O
population	O
about	O
the	O
interpretation	O
of	O
these	O
dogwhistles	O
,	O
and	O
we	O
apply	O
clustering	O
techniques	O
based	O
on	O
the	O
transformerderived	O
representation	O
of	O
the	O
responses	O
.	O
We	O
ask	O
the	O
question	O
:	O
are	O
the	O
responses	O
clearly	O
partitioned	O
in	O
the	O
semantic	O
space	O
,	O
and	O
does	O
the	O
"	O
sharpness	O
"	O
of	O
this	O
partitioning	O
reflect	O
the	O
ease	O
of	O
dogwhistle	B-DatasetName
identification	O
by	O
expert	O
annotators	O
?	O
While	O
there	O
has	O
been	O
work	O
exploring	O
dogwhistles	O
through	O
the	O
lens	O
of	O
linguistics	O
(	O
Henderson	O
and	O
McCready	O
,	O
2019	O
;	O
Bhat	O
and	O
Klein	O
,	O
2020	O
;	O
Saul	O
,	O
2018	O
)	O
,	O
automated	O
approaches	O
to	O
exploring	O
dogwhistles	O
using	O
NLP	O
techniques	O
are	O
generally	O
lacking	O
(	O
Xu	O
et	O
al	O
,	O
2021	O
)	O
.	O
Considering	O
the	O
volume	O
of	O
social	O
media	O
data	O
and	O
the	O
extent	O
to	O
which	O
dogwhistles	O
have	O
been	O
employed	O
on	O
these	O
channels	O
,	O
it	O
is	O
important	O
to	O
create	O
new	O
computational	O
techniques	O
to	O
detect	O
and	O
analyze	O
dogwhistles	O
that	O
might	O
succeed	O
at	O
higher	O
data	O
volumes	O
.	O
The	O
first	O
step	O
in	O
accomplishing	O
this	O
is	O
to	O
show	O
that	O
automatic	O
techniques	O
can	O
be	O
used	O
to	O
reliably	O
extend	O
and	O
enhance	O
manual	O
analysis	O
.	O
Dogwhistles	O
can	O
be	O
strategically	O
used	O
,	O
e.g.	O
politically	O
to	O
send	O
a	O
veiled	O
message	O
to	O
one	O
group	O
of	O
voters	O
while	O
avoiding	O
alienating	O
another	O
group	O
(	O
Bhat	O
and	O
Klein	O
,	O
2020	O
)	O
.	O
This	O
could	O
pose	O
a	O
problem	O
in	O
a	O
representative	O
democracy	O
since	O
the	O
out	O
-	O
group	O
portion	O
of	O
the	O
voter	O
-	O
base	O
are	O
deceived	O
into	O
voting	O
for	O
a	O
certain	O
candidate	O
that	O
might	O
not	O
represent	O
their	O
political	O
views	O
(	O
Goodin	O
and	O
Saward	O
,	O
2005	O
)	O
.	O
Therefore	O
,	O
we	O
contribute	O
the	O
following	O
:	O
We	O
present	O
a	O
preliminary	O
dataset	O
of	O
a	O
word	O
replacement	O
task	O
by	O
members	O
of	O
the	O
Swedish	O
population	O
as	O
part	O
of	O
a	O
survey	O
of	O
political	O
attitudes	O
,	O
including	O
a	O
manual	O
annotation	O
for	O
dogwhistle	B-DatasetName
identification	O
with	O
inter	O
-	O
annotator	O
agreement	O
(	O
IAA	O
;	O
Krippendorff	O
's	O
α	B-HyperparameterName
)	O
scores	O
.	O
We	O
use	O
a	O
transformer	O
-	O
based	O
model	O
to	O
represent	O
the	O
responses	O
in	O
a	O
semantic	O
space	O
and	O
apply	O
classification	O
(	O
SVM	B-MethodName
)	O
and	O
clustering	O
techniques	O
(	O
K	O
-	O
means	O
)	O
to	O
the	O
vectors	O
.	O
We	O
evaluate	O
the	O
clusterings	O
in	O
terms	O
of	O
cluster	O
purity	O
metrics	O
,	O
and	O
we	O
show	O
that	O
the	O
lower	O
the	O
IAA	O
,	O
the	O
lower	O
the	O
linear	O
separability	O
of	O
the	O
responses	O
in	O
the	O
vector	O
space	O
.	O
We	O
then	O
conclude	O
that	O
a	O
Swedish	O
BERT	B-MethodName
variant	O
already	O
represents	O
important	O
aspects	O
of	O
the	O
underlying	O
semantics	O
of	O
dogwhistles	O
.	O

The	O
results	O
in	O
Table	O
3	O
show	O
that	O
a	O
high	O
separability	O
among	O
clusters	O
does	O
indeed	O
correspond	O
with	O
the	O
IAA	O
agreement	O
,	O
which	O
indicates	O
the	O
annotators	O
ease	O
of	O
categorizing	O
a	O
response	O
as	O
"	O
in	O
-	O
group	O
"	O
or	O
"	O
out	O
-	O
group	O
"	O
.	O
For	O
example	O
,	O
the	O
dogwhistle	B-DatasetName
"	O
remigration	O
"	O
had	O
the	O
lowest	O
F1	B-MetricName
score	I-MetricName
for	O
both	O
the	O
dimensionality	O
reduced	O
sentence	O
representations	O
(	O
0.72	O
)	O
and	O
the	O
original	O
sentence	O
representations	O
(	O
0.85	O
)	O
,	O
as	O
well	O
as	O
the	O
lowest	O
IAA	O
overall	O
(	O
0.74/0.55	O
)	O
,	O
as	O
can	O
be	O
seen	O
in	O
table	O
2	O
.	O
Similarly	O
,	O
"	O
suburban	O
gang	O
"	O
had	O
the	O
highest	O
IAA	O
(	O
1/1	O
)	O
and	O
very	O
high	O
F1	B-MetricName
scores	O
as	O
well	O
(	O
0.98/0.97	O
)	O
.	O
However	O
,	O
the	O
evaluation	O
of	O
the	O
K	O
-	O
means	O
labeled	O
clusters	O
did	O
not	O
correspond	O
well	O
to	O
the	O
IAA	O
.	O
The	O
evaluation	O
metrics	O
for	O
"	O
refugee	O
policy	O
"	O
is	O
higher	O
than	O
"	O
help	O
on	O
location	O
"	O
(	O
1/0.82	O
)	O
despite	O
having	O
a	O
much	O
lower	O
IAA	O
score	O
(	O
0.74/0.55	O
)	O
.	O
An	O
explanation	O
for	O
this	O
might	O
be	O
that	O
some	O
dogwhistle	B-DatasetName
clusterings	O
are	O
spread	O
over	O
a	O
wider	O
semantic	O
space	O
,	O
while	O
still	O
being	O
linearly	O
separatable	O
(	O
with	O
an	O
SVM	B-MethodName
)	O
from	O
other	O
clusterings	O
.	O
This	O
type	O
of	O
data	O
distribution	O
will	O
still	O
obtain	O
good	O
clustering	O
results	O
.	O
For	O
example	O
,	O
"	O
enrich	O
"	O
in	O
table	O
4	O
reports	O
the	O
best	O
defined	O
clusters	O
overall	O
(	O
measured	O
by	O
a	O
low	O
DB	O
score	O
and	O
high	O
CH	O
score	O
)	O
,	O
while	O
only	O
having	O
a	O
marginally	O
greater	O
F1	B-MetricName
score	I-MetricName
(	O
0.98/0.98	O
)	O
on	O
the	O
SVM	B-MethodName
task	O
than	O
"	O
suburban	O
gang	O
"	O
(	O
0.98/0.97	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
to	O
use	O
dice	B-MethodName
loss	I-MethodName
in	O
replacement	O
of	O
the	O
standard	O
cross	O
-	O
entropy	O
objective	O
for	O
data	O
-	O
imbalanced	O
NLP	O
tasks	O
.	O
Dice	B-MethodName
loss	I-MethodName
is	O
based	O
on	O
the	O
Sørensen	O
-	O
Dice	B-MetricName
coefficient	O
(	O
Sorensen	O
,	O
1948	O
)	O
or	O
Tversky	O
index	O
(	O
Tversky	O
,	O
1977	O
)	O
,	O
which	O
attaches	O
similar	O
importance	O
to	O
false	O
positives	O
and	O
false	O
negatives	O
,	O
and	O
is	O
more	O
immune	O
to	O
the	O
data	O
-	O
imbalance	O
issue	O
.	O
To	O
further	O
alleviate	O
the	O
dominating	O
influence	O
from	O
easy	O
-	O
negative	O
examples	O
in	O
training	O
,	O
we	O
propose	O
to	O
associate	O
training	O
examples	O
with	O
dynamically	O
adjusted	O
weights	O
to	O
deemphasize	O
easy	O
-	O
negative	O
examples	O
.	O
Experimental	O
results	O
show	O
that	O
this	O
strategy	O
narrows	O
down	O
the	O
gap	O
between	O
the	O
F1	B-MetricName
score	I-MetricName
in	O
evaluation	O
and	O
the	O
dice	B-MethodName
loss	I-MethodName
in	O
training	O
.	O
With	O
the	O
proposed	O
training	O
objective	O
,	O
we	O
observe	O
significant	O
performance	O
boosts	O
over	O
a	O
wide	O
range	O
of	O
data	O
imbalanced	O
NLP	O
tasks	O
.	O
Notably	O
,	O
we	O
are	O
able	O
to	O
achieve	O
SOTA	O
results	O
on	O
CTB5	O
,	O
CTB6	O
and	O
UD1.4	O
for	O
the	O
part	O
of	O
speech	O
tagging	O
task	O
,	O
and	O
competitive	O
or	O
even	O
better	O
results	O
on	O
CoNLL03	B-DatasetName
,	O
OntoNotes5.0	O
,	O
MSRA	O
and	O
OntoNotes4.0	O
for	O
the	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
task	O
along	O
with	O
the	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
and	O
paraphrase	B-TaskName
identification	I-TaskName
tasks	O
.	O
The	O
code	O
can	O
be	O
found	O
at	O
https://github.com/ShannonAI/	O
dice_loss_for_NLP	O
.	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
10.3	O
M	O
175	O
K	O
55.9	O
SQuAD	B-DatasetName
2.0	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2018	O
)	O
15.4	O
M	O
188	O
K	O
82.0	O
QUOREF	B-DatasetName
(	O
Dasigi	O
et	O
al	O
,	O
2019	O
)	O
6.52	O
M	O
38.6	O
K	O
169	O

Data	O
imbalance	O
is	O
a	O
common	O
issue	O
in	O
a	O
variety	O
of	O
NLP	O
tasks	O
such	O
as	O
tagging	O
and	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
.	O
Table	O
1	O
gives	O
concrete	O
examples	O
:	O
for	O
the	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
task	O
Nadeau	O
and	O
Sekine	O
,	O
2007	O
)	O
,	O
most	O
tokens	O
are	O
backgrounds	O
with	O
tagging	O
class	O
O.	O
Specifically	O
,	O
the	O
number	O
of	O
tokens	O
with	O
tagging	O
class	O
O	O
is	O
5	O
times	O
as	O
many	O
as	O
those	O
with	O
entity	O
labels	O
for	O
the	O
CoNLL03	B-DatasetName
dataset	O
and	O
8	O
times	O
for	O
the	O
OntoNotes5.0	O
dataset	O
;	O
Dataimbalanced	O
issue	O
is	O
more	O
severe	O
for	O
MRC	O
tasks	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
;	O
Nguyen	O
et	O
al	O
,	O
2016	O
;	O
Rajpurkar	O
et	O
al	O
,	O
2018	O
;	O
Kočiskỳ	O
et	O
al	O
,	O
2018	O
;	O
Dasigi	O
et	O
al	O
,	O
2019	O
)	O
with	O
the	O
value	O
of	O
negative	O
-	O
positive	O
ratio	O
being	O
50	O
-	O
200	O
,	O
which	O
is	O
due	O
to	O
the	O
reason	O
that	O
the	O
task	O
of	O
MRC	O
is	O
usually	O
formalized	O
as	O
predicting	O
the	O
starting	O
and	O
ending	O
indexes	O
conditioned	O
on	O
the	O
query	O
and	O
the	O
context	O
,	O
and	O
given	O
a	O
chunk	O
of	O
text	O
of	O
an	O
arbitrary	O
length	O
,	O
only	O
two	O
tokens	O
are	O
positive	O
(	O
or	O
of	O
interest	O
)	O
with	O
all	O
the	O
rest	O
being	O
background	O
.	O
Data	O
imbalance	O
results	O
in	O
the	O
following	O
two	O
issues	O
:	O
(	O
1	O
)	O
the	O
training	O
-	O
test	O
discrepancy	O
:	O
Without	O
balancing	O
the	O
labels	O
,	O
the	O
learning	O
process	O
tends	O
to	O
converge	O
to	O
a	O
point	O
that	O
strongly	O
biases	O
towards	O
class	O
with	O
the	O
majority	O
label	O
.	O
(	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Devlin	O
et	O
al	O
,	O
2018	O
;	O
Yu	O
et	O
al	O
,	O
2018a	O
;	O
McCann	O
et	O
al	O
,	O
2018	O
;	O
Ma	O
and	O
Hovy	O
,	O
2016	O
;	O
,	O
handles	O
neither	O
of	O
the	O
issues	O
.	O
To	O
handle	O
the	O
first	O
issue	O
,	O
we	O
propose	O
to	O
replace	O
CE	O
or	O
MLE	O
with	O
losses	O
based	O
on	O
the	O
Sørensen	O
-	O
Dice	B-MetricName
coefficient	O
(	O
Sorensen	O
,	O
1948	O
)	O
or	O
Tversky	O
index	O
(	O
Tversky	O
,	O
1977	O
)	O
.	O
The	O
Sørensen	O
-	O
Dice	B-MetricName
coefficient	O
,	O
dice	B-MethodName
loss	I-MethodName
for	O
short	O
,	O
is	O
the	O
harmonic	O
mean	O
of	O
precision	O
and	O
recall	O
.	O
It	O
attaches	O
equal	O
importance	O
to	O
false	O
positives	O
(	O
FPs	O
)	O
and	O
false	O
negatives	O
(	O
FNs	O
)	O
and	O
is	O
thus	O
more	O
immune	O
to	O
data	O
-	O
imbalanced	O
datasets	O
.	O
Tversky	O
index	O
extends	O
dice	B-MethodName
loss	I-MethodName
by	O
using	O
a	O
weight	O
that	O
trades	O
precision	O
and	O
recall	O
,	O
which	O
can	O
be	O
thought	O
as	O
the	O
approximation	O
of	O
the	O
F	O
β	B-HyperparameterName
score	O
,	O
and	O
thus	O
comes	O
with	O
more	O
flexibility	O
.	O
Therefore	O
,	O
we	O
use	O
dice	B-MethodName
loss	I-MethodName
or	O
Tversky	O
index	O
to	O
replace	O
CE	O
loss	B-MetricName
to	O
address	O
the	O
first	O
issue	O
.	O
Only	O
using	O
dice	B-MethodName
loss	I-MethodName
or	O
Tversky	O
index	O
is	O
not	O
enough	O
since	O
they	O
are	O
unable	O
to	O
address	O
the	O
dominating	O
influence	O
of	O
easy	O
-	O
negative	O
examples	O
.	O
This	O
is	O
intrinsically	O
because	O
dice	B-MethodName
loss	I-MethodName
is	O
actually	O
a	O
soft	O
version	O
of	O
the	O
F1	B-MetricName
score	I-MetricName
.	O
Taking	O
the	O
binary	O
classification	O
task	O
as	O
an	O
example	O
,	O
at	O
test	O
time	O
,	O
an	O
example	O
will	O
be	O
classified	O
as	O
negative	O
as	O
long	O
as	O
its	O
probability	O
is	O
smaller	O
than	O
0.5	O
,	O
but	O
training	O
will	O
push	O
the	O
value	O
to	O
0	B-DatasetName
as	O
much	O
as	O
possible	O
.	O
The	O
rest	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
:	O
related	O
work	O
is	O
presented	O
in	O
Section	O
2	O
.	O
We	O
describe	O
different	O
proposed	O
losses	O
in	O
Section	O
3	O
.	O
Experimental	O
results	O
are	O
presented	O
in	O
Section	O
4	O
.	O
We	O
perform	O
ablation	O
studies	O
in	O
Section	O
5	O
,	O
followed	O
by	O
a	O
brief	O
conclusion	O
in	O
Section	O
6	O
.	O
2	O
Related	O
Work	O

The	O
background	O
-	O
object	O
label	O
imbalance	O
issue	O
is	O
severe	O
and	O
thus	O
well	O
studied	O
in	O
the	O
field	O
of	O
object	B-TaskName
detection	I-TaskName
(	O
Li	O
et	O
al	O
,	O
2015	O
;	O
Girshick	O
,	O
2015	O
;	O
Girshick	O
et	O
al	O
,	O
2013	O
;	O
.	O
The	O
idea	O
of	O
hard	O
negative	O
mining	O
(	O
HNM	O
)	O
(	O
Girshick	O
et	O
al	O
,	O
2013	O
)	O
has	O
gained	O
much	O
attention	O
recently	O
.	O
Pang	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
a	O
novel	O
method	O
called	O
IoU	B-MethodName
-	I-MethodName
balanced	I-MethodName
sampling	I-MethodName
and	O
designed	O
a	O
ranking	O
model	O
to	O
replace	O
the	O
conventional	O
classification	O
task	O
with	O
an	O
average	O
-	O
precision	O
loss	B-MetricName
to	O
alleviate	O
the	O
class	O
imbalance	O
issue	O
.	O
The	O
efforts	O
made	O
on	O
object	B-TaskName
detection	I-TaskName
have	O
greatly	O
inspired	O
us	O
to	O
solve	O
the	O
data	O
imbalance	O
issue	O
in	O
NLP	O
.	O
Sudre	O
et	O
al	O
(	O
2017	O
)	O
addressed	O
the	O
severe	O
class	O
imbalance	O
issue	O
for	O
the	O
image	O
segmentation	O
task	O
.	O
They	O
proposed	O
to	O
use	O
the	O
class	O
re	O
-	O
balancing	O
property	O
of	O
the	O
Generalized	O
Dice	B-MethodName
Loss	I-MethodName
as	O
the	O
training	O
objective	O
for	O
unbalanced	O
tasks	O
.	O
Shen	O
et	O
al	O
(	O
2018	O
)	O
investigated	O
the	O
influence	O
of	O
Dice	B-MetricName
-	O
based	O
loss	B-MetricName
for	O
multi	O
-	O
class	O
organ	O
segmentation	O
using	O
a	O
dataset	O
of	O
abdominal	O
CT	O
volumes	O
.	O
Kodym	O
et	O
al	O
(	O
2018	O
)	O
3	O
Losses	O

The	O
vanilla	O
cross	O
entropy	O
(	O
CE	O
)	O
loss	B-MetricName
is	O
given	O
by	O
:	O
CE	O
=	O
−	O
1	O
N	O
i	O
j	O
{	O
0	B-DatasetName
,	O
1	O
}	O
y	O
ij	O
log	O
p	O
ij	O
(	O
1	O
)	O
As	O
can	O
be	O
seen	O
from	O
Eq.1	O
,	O
each	O
x	O
i	O
contributes	O
equally	O
to	O
the	O
final	O
objective	O
.	O
Two	O
strategies	O
are	O
normally	O
used	O
to	O
address	O
the	O
the	O
case	O
where	O
we	O
wish	O
that	O
not	O
all	O
x	O
i	O
are	O
treated	O
equally	O
:	O
associating	O
different	O
classes	O
with	O
different	O
weighting	O
factor	O
α	B-HyperparameterName
or	O
resampling	O
the	O
datasets	O
.	O
For	O
the	O
former	O
,	O
Eq.1	O
is	O
adjusted	O
as	O
follows	O
:	O
Weighted	O
CE	O
=	O
−	O
1	O
N	O
i	O
α	B-HyperparameterName
i	O
j	O
{	O
0	B-DatasetName
,	O
1	O
}	O
y	O
ij	O
log	O
p	O
ij	O
(	O
2	O
)	O
where	O
α	B-HyperparameterName
i	O
[	O
0	B-DatasetName
,	O
1	O
]	O
may	O
be	O
set	O
by	O
the	O
inverse	O
class	O
frequency	O
or	O
treated	O
as	O
a	O
hyperparameter	O
to	O
set	O
by	O
cross	O
validation	O
.	O
In	O
this	O
work	O
,	O
we	O
use	O
lg	O
(	O
n−nt	O
nt	O
+	O
K	O
)	O
to	O
calculate	O
the	O
coefficient	O
α	B-HyperparameterName
,	O
where	O
n	O
t	O
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
with	O
class	O
t	O
and	O
n	O
is	O
the	O
total	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
in	O
the	O
training	O
set	O
.	O
K	O
is	O
a	O
hyperparameter	O
to	O
tune	O
.	O
Intuitively	O
,	O
this	O
equation	O
assigns	O
less	O
weight	O
to	O
the	O
majority	O
class	O
and	O
more	O
weight	O
to	O
the	O
minority	O
class	O
.	O
The	O
data	O
resampling	O
strategy	O
constructs	O
a	O
new	O
dataset	O
by	O
sampling	O
training	O
examples	O
from	O
the	O
original	O
dataset	O
based	O
on	O
human	O
-	O
designed	O
criteria	O
,	O
e.g.	O
extracting	O
equal	O
training	O
samples	O
from	O
each	O
class	O
.	O
Both	O
strategies	O
are	O
equivalent	O
to	O
changing	O
the	O
data	O
distribution	O
during	O
training	O
and	O
thus	O
are	O
of	O
the	O
same	O
nature	O
.	O
Empirically	O
,	O
these	O
two	O
methods	O
are	O
not	O
widely	O
used	O
due	O
to	O
the	O
trickiness	O
of	O
selecting	O
α	B-HyperparameterName
especially	O
for	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
tasks	O
and	O
that	O
inappropriate	O
selection	O
can	O
easily	O
bias	O
towards	O
rare	O
classes	O
(	O
Valverde	O
et	O
al	O
,	O
2017	O
)	O
.	O

Sørensen	O
-	O
Dice	B-MetricName
coefficient	O
(	O
Sorensen	O
,	O
1948	O
;	O
Dice	B-MetricName
,	O
1945	O
)	O
,	O
dice	O
coefficient	O
(	O
DSC	O
)	O
for	O
short	O
,	O
is	O
an	O
F1oriented	O
statistic	O
used	O
to	O
gauge	O
the	O
similarity	O
of	O
two	O
sets	O
.	O
Given	O
two	O
sets	O
A	O
and	O
B	O
,	O
the	O
vanilla	O
dice	O
coefficient	O
between	O
them	O
is	O
given	O
as	O
follows	O
:	O
DSC	O
(	O
A	O
,	O
B	O
)	O
=	O
2	O
|	O
A	O
∩	O
B	O
|	O
|	O
A	O
|	O
+	O
|	O
B	O
|	O
(	O
3	O
)	O
In	O
our	O
case	O
,	O
A	O
is	O
the	O
set	O
that	O
contains	O
all	O
positive	O
examples	O
predicted	O
by	O
a	O
specific	O
model	O
,	O
and	O
B	O
is	O
the	O
set	O
of	O
all	O
golden	O
positive	O
examples	O
in	O
the	O
dataset	O
.	O
When	O
applied	O
to	O
boolean	O
data	O
with	O
the	O
definition	O
of	O
true	O
positive	O
(	O
TP	O
)	O
,	O
false	O
positive	O
(	O
FP	O
)	O
,	O
and	O
false	O
negative	O
(	O
FN	O
)	O
,	O
it	O
can	O
be	O
then	O
written	O
as	O
follows	O
:	O
DSC	O
=	O
2TP	O
2TP	O
+	O
FN	O
+	O
FP	O
=	O
2	O
TP	O
TP+FN	O
TP	O
TP+FP	O
TP	O
TP+FN	O
+	O
TP	O
TP+FP	O
=	O
2Pre	O
×	O
Rec	O
Pre+Rec	O
=	O
F	O
1	O
(	O
4	O
)	O
For	O
an	O
individual	O
example	O
x	O
i	O
,	O
its	O
corresponding	O
dice	O
coefficient	O
is	O
given	O
as	O
follows	O
:	O
DSC	O
(	O
x	O
i	O
)	O
=	O
2p	O
i1	O
y	O
i1	O
p	O
i1	O
+	O
y	O
i1	O
(	O
5	O
)	O
As	O
can	O
be	O
seen	O
,	O
a	O
negative	O
example	O
(	O
y	O
i1	O
=	O
0	B-DatasetName
)	O
does	O
not	O
contribute	O
to	O
the	O
objective	O
.	O
For	O
smoothing	O
purposes	O
,	O
it	O
is	O
common	O
to	O
add	O
a	O
γ	B-HyperparameterName
factor	O
to	O
both	O
the	O
nominator	O
and	O
the	O
denominator	O
,	O
making	O
the	O
form	O
to	O
be	O
as	O
follows	O
(	O
we	O
simply	O
set	O
γ	B-HyperparameterName
=	O
1	O
in	O
the	O
rest	O
of	O
Loss	O
Formula	O
(	O
one	O
sample	O
x	O
i	O
)	O
CE	O
−	O
j	O
{	O
0	B-DatasetName
,	O
1	O
}	O
y	O
ij	O
log	O
p	O
ij	O
WCE	O
−α	O
i	O
j	O
{	O
0	B-DatasetName
,	O
1	O
}	O
y	O
ij	O
log	O
p	O
ij	O
DL	O
1	O
−	O
2p	O
i1	O
y	O
i1	O
+	O
γ	B-HyperparameterName
p	O
2	O
i1	O
+	O
y	O
2	O
i1	O
+	O
γ	B-HyperparameterName
TL	O
1	O
−	O
p	O
i1	O
y	O
i1	O
+	O
γ	B-HyperparameterName
p	O
i1	O
y	O
i1	O
+	O
α	B-HyperparameterName
p	O
i1	O
y	O
i0	O
+	O
β	B-HyperparameterName
p	O
i0	O
y	O
i1	O
+	O
γ	B-HyperparameterName
DSC	O
1	O
−	O
2	O
(	O
1−p	O
i1	O
)	O
p	O
i1	O
y	O
i1	O
+	O
γ	B-HyperparameterName
(	O
1−p	O
i1	O
)	O
p	O
i1	O
+	O
y	O
i1	O
+	O
γ	B-HyperparameterName
FL	O
−α	O
i	O
j	O
{	O
0	B-DatasetName
,	O
1	O
}	O
(	O
1	O
−	O
p	O
ij	O
)	O
γ	B-HyperparameterName
log	O
p	O
ij	O
DSC	O
(	O
x	O
i	O
)	O
=	O
2p	O
i1	O
y	O
i1	O
+	O
γ	B-HyperparameterName
p	O
i1	O
+	O
y	O
i1	O
+	O
γ	B-HyperparameterName
(	O
6	O
)	O
As	O
can	O
be	O
seen	O
,	O
negative	O
examples	O
whose	O
DSC	O
is	O
γ	B-HyperparameterName
p	O
i1	O
+	O
γ	B-HyperparameterName
,	O
also	O
contribute	O
to	O
the	O
training	O
.	O
Additionally	O
,	O
Milletari	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
to	O
change	O
the	O
denominator	O
to	O
the	O
square	O
form	O
for	O
faster	O
convergence	O
,	O
which	O
leads	O
to	O
the	O
following	O
dice	B-MethodName
loss	I-MethodName
(	O
DL	O
)	O
:	O
DL	O
=	O
1	O
N	O
i	O
1	O
−	O
2p	O
i1	O
y	O
i1	O
+	O
γ	B-HyperparameterName
p	O
2	O
i1	O
+	O
y	O
2	O
i1	O
+	O
γ	B-HyperparameterName
(	O
7	O
)	O
Another	O
version	O
of	O
DL	O
is	O
to	O
directly	O
compute	O
setlevel	O
dice	O
coefficient	O
instead	O
of	O
the	O
sum	O
of	O
individual	O
dice	O
coefficient	O
,	O
which	O
is	O
easier	O
for	O
optimization	O
:	O
DL	O
=	O
1	O
−	O
2	O
i	O
p	O
i1	O
y	O
i1	O
+	O
γ	B-HyperparameterName
i	O
p	O
2	O
i1	O
+	O
i	O
y	O
2	O
i1	O
+	O
γ	B-HyperparameterName
(	O
8	O
)	O
Tversky	O
index	O
(	O
TI	O
)	O
,	O
which	O
can	O
be	O
thought	O
as	O
the	O
approximation	O
of	O
the	O
F	O
β	B-HyperparameterName
score	O
,	O
extends	O
dice	O
coefficient	O
to	O
a	O
more	O
general	O
case	O
.	O
Given	O
two	O
sets	O
A	O
and	O
B	O
,	O
tversky	O
index	O
is	O
computed	O
as	O
follows	O
:	O
TI	O
=	O
|	O
A	O
∩	O
B	O
|	O
|	O
A	O
∩	O
B	O
|	O
+	O
α	B-HyperparameterName
|	O
A\B	O
|	O
+	O
β	B-HyperparameterName
|	O
B\A	O
|	O
(	O
9	O
)	O
Tversky	O
index	O
offers	O
the	O
flexibility	O
in	O
controlling	O
the	O
tradeoff	O
between	O
false	O
-	O
negatives	O
and	O
falsepositives	O
.	O
It	O
degenerates	O
to	O
DSC	O
if	O
α	B-HyperparameterName
=	O
β	B-HyperparameterName
=	O
0.5	O
.	O
The	O
Tversky	O
loss	B-MetricName
(	O
TL	O
)	O
is	O
thus	O
given	O
as	O
follows	O
:	O
TL	O
=	O
1	O
N	O
i	O
1	O
−	O
pi1yi1	O
+	O
γ	B-HyperparameterName
pi1yi1	O
+	O
α	B-HyperparameterName
pi1yi0	O
+	O
β	B-HyperparameterName
pi0yi1	O
+	O
γ	B-HyperparameterName
(	O
10	O
)	O

Consider	O
a	O
simple	O
case	O
where	O
the	O
dataset	O
consists	O
of	O
only	O
one	O
example	O
x	O
i	O
,	O
which	O
is	O
classified	O
as	O
positive	O
as	O
long	O
as	O
p	O
i1	O
is	O
larger	O
than	O
0.5	O
.	O
The	O
computation	O
of	O
F	O
1	O
score	O
is	O
actually	O
as	O
follows	O
:	O
The	O
derivative	O
of	O
DSC	O
approaches	O
zero	O
right	O
after	O
p	O
exceeds	O
0.5	O
,	O
and	O
for	O
the	O
other	O
losses	O
,	O
the	O
derivatives	O
reach	O
0	B-DatasetName
only	O
if	O
the	O
probability	O
is	O
exactly	O
1	O
,	O
which	O
means	O
they	O
will	O
push	O
p	O
to	O
1	O
as	O
much	O
as	O
possible	O
.	O
F1	B-MetricName
(	O
x	O
i	O
)	O
=	O
2	O
I	O
(	O
p	O
i1	O
>	O
0.5	O
)	O
y	O
i1	O
I	O
(	O
p	O
i1	O
>	O
0.5	O
)	O
+	O
y	O
i1	O
(	O
11	O
)	O
Comparing	O
Eq.5	O
with	O
Eq.11	O
,	O
we	O
can	O
see	O
that	O
Eq.5	O
is	O
actually	O
a	O
soft	O
form	O
of	O
F	O
1	O
,	O
using	O
a	O
continuous	O
p	O
rather	O
than	O
the	O
binary	O
I	O
(	O
p	O
i1	O
>	O
0	B-DatasetName
.	O
To	O
address	O
this	O
issue	O
,	O
we	O
propose	O
to	O
multiply	O
the	O
soft	O
probability	O
p	O
with	O
a	O
decaying	O
factor	O
(	O
1	O
−	O
p	O
)	O
,	O
changing	O
Eq.11	O
to	O
the	O
following	O
adaptive	O
variant	O
of	O
DSC	O
:	O
DSC	O
(	O
x	O
i	O
)	O
=	O
2	O
(	O
1	O
−	O
p	O
i1	O
)	O
p	O
i1	O
y	O
i1	O
+	O
γ	B-HyperparameterName
(	O
1	O
−	O
p	O
i1	O
)	O
p	O
i1	O
+	O
y	O
i1	O
+	O
γ	B-HyperparameterName
(	O
12	O
)	O
One	O
can	O
think	O
(	O
1	O
−	O
p	O
i1	O
)	O
as	O
a	O
weight	O
associated	O
with	O
each	O
example	O
,	O
which	O
changes	O
as	O
training	O
proceeds	O
.	O
The	O
intuition	O
of	O
changing	O
p	O
i1	O
to	O
(	O
1	O
−	O
p	O
i1	O
)	O
p	O
i1	O
is	O
to	O
push	O
down	O
the	O
weight	O
of	O
easy	O
examples	O
.	O
For	O
easy	O
examples	O
whose	O
probability	O
are	O
approaching	O
0	B-DatasetName
or	O
1	O
,	O
(	O
1	O
−	O
p	O
i1	O
)	O
p	O
i1	O
makes	O
the	O
model	O
attach	O
significantly	O
less	O
focus	O
to	O
them	O
.	O
A	O
close	O
look	O
at	O
Eq.12	O
reveals	O
that	O
it	O
actually	O
mimics	O
the	O
idea	O
of	O
focal	B-MethodName
loss	I-MethodName
(	O
FL	O
for	O
short	O
)	O
(	O
Lin	O
et	O
al	O
,	O
2017	O
)	O
In	O
Table	O
2	O
,	O
we	O
summarize	O
all	O
the	O
aforementioned	O
losses	O
.	O
Figure	O
1	O
gives	O
an	O
explanation	O
from	O
the	O
perspective	O
in	O
derivative	O
:	O
The	O
derivative	O
of	O
DSC	O
approaches	O
zero	O
right	O
after	O
p	O
exceeds	O
0.5	O
,	O
which	O
suggests	O
the	O
model	O
attends	O
less	O
to	O
examples	O
once	O
they	O
are	O
correctly	O
classified	O
.	O
But	O
for	O
the	O
other	O
losses	O
,	O
the	O
derivatives	O
reach	O
0	B-DatasetName
only	O
if	O
the	O
probability	O
is	O
exactly	O
1	O
,	O
which	O
means	O
they	O
will	O
push	O
p	O
to	O
1	O
as	O
much	O
as	O
possible	O
.	O

Settings	O
Part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
(	O
POS	O
)	O
is	O
the	O
task	O
of	O
assigning	O
a	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
label	O
(	O
e.g.	O
,	O
noun	O
,	O
verb	O
,	O
adjective	O
)	O
to	O
each	O
word	O
in	O
a	O
given	O
text	O
.	O
In	O
this	O
paper	O
,	O
we	O
choose	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
as	O
the	O
backbone	O
and	O
conduct	O
experiments	O
on	O
three	O
widely	O
used	O
Chinese	O
POS	O
datasets	O
including	O
Chinese	B-DatasetName
Treebank	I-DatasetName
(	O
Xue	O
et	O
al	O
,	O
2005	O
)	O
5.0/6.0	O
and	O
UD1.4	O
and	O
English	O
datasets	O
including	O
Wall	O
Street	O
Journal	O
(	O
WSJ	O
)	O
and	O
the	O
dataset	O
proposed	O
by	O
Ritter	O
et	O
al	O
(	O
2011	O
)	O
.	O
We	O
report	O
the	O
span	O
-	O
level	O
micro	O
-	O
averaged	O
precision	O
,	O
recall	O
and	O
F1	B-MetricName
for	O
evaluation	O
.	O
Baselines	O
We	O
used	O
the	O
following	O
baselines	O
:	O
Results	O

Settings	O
Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
is	O
the	O
task	O
of	O
detecting	O
the	O
span	O
and	O
semantic	O
category	O
of	O
entities	O
within	O
a	O
chunk	O
of	O
text	O
.	O
Our	O
implementation	O
uses	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
proposed	O
by	O
as	O
the	O
backbone	O
,	O
and	O
changes	O
the	O
MLE	O
loss	B-MetricName
to	O
DSC	O
loss	B-MetricName
.	O
Datasets	O
that	O
we	O
use	O
include	O
OntoNotes4.0	O
(	O
Pradhan	O
et	O
al	O
,	O
2011	O
)	O
,	O
MSRA	O
(	O
Levow	O
,	O
2006	O
)	O
,	O
CoNLL2003	B-DatasetName
(	O
Sang	O
and	O
Meulder	O
,	O
2003	O
and	O
OntoNotes5.0	O
(	O
Pradhan	O
et	O
al	O
,	O
2013	O
)	O
.	O
We	O
report	O
span	O
-	O
level	O
micro	O
-	O
averaged	O
precision	O
,	O
recall	O
and	O
F1	B-MetricName
.	O
Baselines	O
We	O
use	O
the	O
following	O
baselines	O
:	O
ELMo	B-MethodName
:	O
a	O
tagging	O
model	O
with	O
pretraining	O
from	O
Peters	O
et	O
al	O
(	O
2018	O
)	O
.	O
Lattice	O
-	O
LSTM	B-MethodName
:	O
Zhang	O
and	O
Yang	O
(	O
2018	O
)	O
Results	O

Settings	O
The	O
task	O
of	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
(	O
MRC	O
)	O
(	O
Seo	O
et	O
al	O
,	O
2016	O
;	O
Wang	O
and	O
Jiang	O
,	O
2016	O
;	O
Shen	O
et	O
al	O
,	O
2017	O
;	O
predicts	O
the	O
answer	O
span	O
in	O
the	O
passage	O
given	O
a	O
question	O
and	O
the	O
passage	O
.	O
We	O
followed	O
the	O
standard	O
protocols	O
in	O
Seo	O
et	O
al	O
(	O
2016	O
)	O
,	O
in	O
which	O
the	O
start	O
and	O
end	O
indexes	O
of	O
answer	O
are	O
predicted	O
.	O
We	O
report	O
Extract	O
Match	O
(	O
EM	B-MetricName
)	O
as	O
well	O
as	O
F1	B-MetricName
score	I-MetricName
on	O
validation	O
set	O
.	O
We	O
use	O
three	O
datasets	O
on	O
this	O
task	O
:	O
SQuAD	B-DatasetName
v1.1	O
,	O
SQuAD	B-DatasetName
v2.0	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
(	O
Rajpurkar	O
et	O
al	O
,	O
,	O
2018	O
and	O
Quoref	B-DatasetName
(	O
Dasigi	O
et	O
al	O
,	O
2019	O
)	O
.	O
Baselines	O
We	O
used	O
the	O
following	O
baselines	O
:	O
enables	O
learning	O
bidirectional	O
contexts	O
.	O
Results	O
Table	O
6	O
shows	O
the	O
experimental	O
results	O
for	O
MRC	O
task	O
.	O
With	O
either	O
BERT	B-MethodName
or	O
XLNet	B-MethodName
,	O
our	O
proposed	O
DSC	O
loss	B-MetricName
obtains	O
significant	O
performance	O
boost	O
on	O
both	O
EM	B-MetricName
and	O
F1	B-MetricName
.	O
For	O
SQuADv1.1	O
,	O
our	O
proposed	O
method	O
outperforms	O
XLNet	B-MethodName
by	O
+1.25	O
in	O
terms	O
of	O
F1	B-MetricName
score	I-MetricName
and	O
+0.84	O
in	O
terms	O
of	O
EM	B-MetricName
.	O
For	O
SQuAD	B-DatasetName
v2.0	O
,	O
the	O
proposed	O
method	O
achieves	O
87.65	O
on	O
EM	B-MetricName
and	O
89.51	O
on	O
F1	B-MetricName
.	O
On	O
QuoRef	B-DatasetName
,	O
the	O
proposed	O
method	O
surpasses	O
XLNet	B-MethodName
by	O
+1.46	O
on	O
EM	B-MetricName
and	O
+1.41	O
on	O
F1	B-MetricName
.	O

Settings	O
Paraphrase	B-TaskName
identification	I-TaskName
(	O
PI	O
)	O
is	O
the	O
task	O
of	O
identifying	O
whether	O
two	O
sentences	O
have	O
the	O
same	O
meaning	O
or	O
not	O
.	O
We	O
conduct	O
experiments	O
on	O
the	O
two	O
widely	O
-	O
used	O
datasets	O
:	O
MRPC	B-DatasetName
(	O
Dolan	O
and	O
Brockett	O
,	O
2005	O
)	O
and	O
QQP	B-DatasetName
.	O
F1	B-MetricName
score	I-MetricName
is	O
reported	O
for	O
comparison	O
.	O
We	O
use	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
and	O
XLNet	B-MethodName
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
as	O
baselines	O
.	O
Results	O
Table	O
7	O
shows	O
the	O
results	O
.	O
We	O
find	O
that	O
replacing	O
the	O
training	O
objective	O
with	O
DSC	O
introduces	O
performance	O
boost	O
for	O
both	O
settings	O
,	O
+0.58	O
for	O
MRPC	B-DatasetName
and	O
+0.73	O
for	O
QQP	B-DatasetName
.	O

It	O
is	O
interesting	O
to	O
see	O
how	O
differently	O
the	O
proposed	O
objectives	O
affect	O
datasets	O
imbalanced	O
to	O
different	O
extents	O
.	O
We	O
use	O
the	O
paraphrase	B-TaskName
identification	I-TaskName
dataset	O
QQP	B-DatasetName
(	O
37	O
%	O
positive	O
and	O
63	O
%	O
negative	O
)	O
for	O
studies	O
.	O
To	O
construct	O
datasets	O
with	O
different	O
imbalance	O
degrees	O
,	O
we	O
used	O
the	O
original	O
QQP	B-DatasetName
dataset	O
to	O
construct	O
synthetic	O
training	O
sets	O
with	O
different	O
positive	O
-	O
negative	O
ratios	O
.	O
Models	O
are	O
trained	O
on	O
these	O
different	O
synthetic	O
sets	O
and	O
then	O
test	O
on	O
the	O
same	O
original	O
test	O
set	O
.	O
Results	O
are	O
shown	O
in	O
Table	O
8	O
.	O
We	O
first	O
look	O
at	O
the	O
first	O
line	O
,	O
with	O
all	O
results	O
obtained	O
using	O
the	O
MLE	O
objective	O
.	O
We	O
can	O
see	O
that	O
+	O
positive	O
outperforms	O
original	O
,	O
and	O
+	O
negative	O
underperforms	O
original	O
.	O
This	O
is	O
in	O
line	O
with	O
our	O
expectation	O
since	O
+	O
positive	O
creates	O
a	O
balanced	O
dataset	O
while	O
+	O
negative	O
creates	O
a	O
more	O
imbalanced	O
dataset	O
.	O
Despite	O
the	O
fact	O
that	O
-	O
negative	O
creates	O
a	O
balanced	O
dataset	O
,	O
the	O
number	O
of	O
training	O
data	O
decreases	O
,	O
resulting	O
in	O
inferior	O
performances	O
.	O
DSC	O
achieves	O
the	O
highest	O
F1	B-MetricName
score	I-MetricName
across	O
all	O
datasets	O
.	O
Specially	O
,	O
for	O
+	O
positive	O
,	O
DSC	O
achieves	O
minor	O
improvements	O
(	O
+0.05	O
F1	B-MetricName
)	O
over	O
DL	O
.	O
In	O
contrast	O
,	O
it	O
significantly	O
outperforms	O
DL	O
for	O
+	O
negative	O
dataset	O
.	O
This	O
is	O
in	O
line	O
with	O
our	O
expectation	O
since	O
DSC	O
helps	O
more	O
on	O
more	O
imbalanced	O
datasets	O
.	O
The	O
performance	O
of	O
FL	O
and	O
DL	O
are	O
not	O
consistent	O
across	O
different	O
datasets	O
,	O
while	O
DSC	O
consistently	O
performs	O
the	O
best	O
on	O
all	O
datasets	O
.	O

As	O
mentioned	O
in	O
Section	O
3.3	O
,	O
Tversky	O
index	O
(	O
TI	O
)	O
offers	O
the	O
flexibility	O
in	O
controlling	O
the	O
tradeoff	O
between	O
false	O
-	O
negatives	O
and	O
false	O
-	O
positives	O
.	O
In	O
this	O
subsection	O
,	O
we	O
explore	O
the	O
effect	O
of	O
hyperparameters	O
(	O
i.e.	O
,	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
)	O
in	O
TI	O
to	O
test	O
how	O
they	O
manipulate	O
the	O
tradeoff	O
.	O
We	O
conduct	O
experiments	O
on	O
the	O
Chinese	O
OntoNotes4.0	O
NER	B-TaskName
dataset	O
and	O
English	O
QuoRef	B-DatasetName
MRC	O
dataset	O
.	O
Experimental	O
results	O
are	O
shown	O
in	O
Table	O
10	O
.	O
The	O
highest	O
F1	B-MetricName
on	O
Chinese	O
OntoNotes4.0	O
is	O
84.67	O
when	O
α	B-HyperparameterName
is	O
set	O
to	O
0.6	O
while	O
for	O
QuoRef	B-DatasetName
,	O
the	O
highest	O
F1	B-MetricName
is	O
68.44	O
when	O
α	B-HyperparameterName
is	O
set	O
to	O
0.4	O
.	O
In	O
addition	O
,	O
we	O
can	O
observe	O
that	O
the	O
performance	O
varies	O
a	O
lot	O
as	O
α	B-HyperparameterName
changes	O
in	O
distinct	O
datasets	O
,	O
which	O
shows	O
that	O
the	O
hyperparameters	O
α	B-HyperparameterName
,	O
β	B-HyperparameterName
acturally	O
play	O
an	O
important	O
role	O
in	O
TI	O
.	O

In	O
Table	O
10	O
:	O
The	O
effect	O
of	O
hyperparameters	O
in	O
Tversky	O
Index	O
.	O
We	O
set	O
β	B-HyperparameterName
=	O
1	O
−	O
α	B-HyperparameterName
and	O
thus	O
we	O
only	O
list	O
α	B-HyperparameterName
here	O
.	O
to	O
achieve	O
significant	O
performance	O
boost	O
without	O
changing	O
model	O
architectures	O
.	O
annotation	O
of	O
grammar	O
(	O
parts	O
of	O
speech	O
,	O
morphological	O
features	O
,	O
and	O
syntactic	O
dependencies	O
)	O
across	O
different	O
human	O
languages	O
.	O
In	O
this	O
work	O
,	O
we	O
use	O
UD1.4	O
for	O
Chinese	O
POS	O
tagging	O
.	O

Table	O
3	O
presents	O
the	O
classification	O
results	O
for	O
the	O
baselines	O
,	O
the	O
full	O
feature	O
set	O
,	O
and	O
the	O
selected	O
features	O
for	O
both	O
logistic	B-MethodName
regression	I-MethodName
and	O
random	O
forests	O
.	O
Results	O
are	O
reported	O
using	O
a	O
weighted	O
F1	B-MetricName
score	I-MetricName
,	O
which	O
is	O
a	O
classification	O
accuracy	B-MetricName
measure	O
based	O
on	O
the	O
mean	O
between	O
the	O
precision	O
and	O
recall	O
after	O
adjusting	O
for	O
class	O
imbalance	O
.	O
The	O
linguistic	O
and	O
clinical	O
content	O
features	O
improve	O
predictive	O
accuracy	B-MetricName
above	O
the	O
baselines	O
,	O
yielding	O
a	O
higher	O
F1	B-MetricName
score	I-MetricName
than	O
the	O
strongest	O
baseline	O
(	O
.67	O
compared	O
to	O
.59	O
)	O
.	O
The	O
reduced	O
feature	O
set	O
does	O
not	O
lead	O
to	O
a	O
meaningful	O
performance	O
drop	O
compared	O
to	O
the	O
full	O
feature	O
set	O
,	O
suggesting	O
that	O
no	O
signal	O
was	O
lost	O
due	O
to	O
feature	O
elimination	O
.	O
Figure	O
2	O
reports	O
the	O
eight	O
best	O
-	O
performing	O
features	O
:	O
UMLS	B-DatasetName
phrases	O
count	O
,	O
Unique	O
word	O
count	O
,	O
Polysemic	O
word	O
count	O
,	O
Average	O
noun	O
phrase	O
length	O
,	O
Automated	O
readability	O
index	O
,	O
Prepositional	O
phrases	O
,	O
UMLS	B-DatasetName
distinct	O
terms	O
count	O
,	O
and	O
Concreteness	O
ratio	O
.	O

The	O
results	O
presented	O
in	O
the	O
previous	O
section	O
lead	O
to	O
three	O
main	O
findings	O
:	O
i	O
)	O
the	O
linguistic	O
characteristics	O
of	O
the	O
items	O
carry	O
signal	O
relevant	O
to	O
response	O
4	O
Classes	O
were	O
balanced	O
using	O
the	O
balanced	O
subample	O
setting	O
of	O
the	O
class	O
weight	O
parameter	O
in	O
Scikit	O
-	O
learn	O
's	O
RandomForrestClassifier	O
process	O
complexity	O
;	O
ii	O
)	O
no	O
individual	O
features	O
stand	O
out	O
as	O
strong	O
predictors	O
,	O
and	O
iii	O
)	O
the	O
most	O
important	O
features	O
were	O
those	O
related	O
to	O
syntax	O
and	O
semantics	O
.	O
The	O
first	O
of	O
these	O
findings	O
relates	O
to	O
the	O
fact	O
that	O
the	O
linguistic	O
characteristics	O
of	O
the	O
items	O
carry	O
signal	O
that	O
is	O
predictive	O
of	O
response	O
process	O
complexity	O
,	O
revealing	O
that	O
the	O
problems	O
posed	O
by	O
lowcomplexity	O
and	O
high	O
-	O
complexity	O
items	O
are	O
described	O
using	O
slightly	O
different	O
language	O
.	O
While	O
this	O
signal	O
outperformed	O
several	O
baselines	O
,	O
the	O
overall	O
low	O
predictive	O
utility	O
of	O
the	O
models	O
suggests	O
that	O
there	O
are	O
other	O
factors	O
,	O
yet	O
to	O
be	O
captured	O
,	O
that	O
have	O
a	O
significant	O
effect	O
on	O
response	O
process	O
complexity	O
.	O
The	O
retention	O
of	O
56	O
features	O
indicates	O
that	O
individual	O
linguistic	O
predictors	O
provide	O
a	O
weak	O
classification	O
signal	O
but	O
,	O
taken	O
together	O
,	O
they	O
complement	O
each	O
other	O
in	O
a	O
way	O
that	O
ultimately	O
provides	O
a	O
higher	O
accuracy	B-MetricName
.	O
The	O
fact	O
that	O
there	O
are	O
many	O
predictive	O
features	O
with	O
none	O
standing	O
out	O
is	O
also	O
a	O
positive	O
evaluation	O
outcome	O
for	O
item	O
writing	O
quality	O
,	O
as	O
it	O
shows	O
that	O
the	O
response	O
process	O
complexity	O
associated	O
with	O
an	O
item	O
is	O
not	O
distributed	O
along	O
a	O
small	O
number	O
of	O
linguistic	O
parameters	O
.	O
The	O
most	O
important	O
features	O
that	O
helped	O
with	O
classification	O
were	O
those	O
related	O
to	O
syntax	O
and	O
semantics	O
(	O
Figure	O
2	O
)	O
.	O
The	O
poor	O
performance	O
of	O
the	O
Word	O
Count	O
baseline	O
suggests	O
that	O
differences	O
in	O
response	O
process	O
complexity	O
can	O
not	O
be	O
explained	O
solely	O
by	O
item	O
length	O
and	O
that	O
more	O
complex	O
linguistic	O
features	O
capture	O
some	O
of	O
the	O
nuance	O
in	O
the	O
response	O
process	O
.	O
As	O
can	O
be	O
seen	O
in	O
Figure	O
2	O
,	O
high	O
-	O
complexity	O
items	O
contain	O
a	O
slightly	O
higher	O
number	O
of	O
UMLS	B-DatasetName
phrases	O
and	O
(	O
distinct	O
)	O
medical	O
terms	O
,	O
as	O
well	O
as	O
a	O
higher	O
number	O
of	O
unique	O
words	O
.	O
These	O
features	O
suggest	O
high	O
-	O
complexity	O
items	O
re	O
-	O
peat	O
words	O
less	O
frequently	O
and	O
may	O
contain	O
a	O
higher	O
concentration	O
of	O
new	O
information	O
and	O
specialized	O
terminology	O
than	O
low	O
-	O
complexity	O
items	O
.	O
The	O
individual	O
phrases	O
in	O
high	O
-	O
complexity	O
items	O
are	O
also	O
slightly	O
longer	O
,	O
which	O
naturally	O
influences	O
readability	O
metrics	O
that	O
are	O
based	O
on	O
word	O
and	O
sentence	O
length	O
,	O
such	O
as	O
the	O
Automated	O
Readability	O
Index	O
(	O
higher	O
values	O
are	O
indicative	O
of	O
a	O
more	O
complex	O
text	O
)	O
.	O
Prepositional	O
phrases	O
were	O
also	O
identified	O
as	O
more	O
important	O
than	O
other	O
phrase	O
types	O
in	O
distinguishing	O
between	O
response	O
process	O
complexity	O
.	O
Prepositional	O
phrases	O
often	O
serve	O
as	O
modifiers	O
of	O
the	O
primary	O
noun	O
phrase	O
and	O
the	O
higher	O
number	O
of	O
prepositional	O
phrases	O
in	O
the	O
high	O
-	O
complexity	O
items	O
suggests	O
the	O
use	O
of	O
more	O
specific	O
descriptions	O
(	O
e.g.	O
,	O
"	O
small	O
cell	O
carcinoma	O
of	O
the	O
ovary	O
"	O
instead	O
of	O
just	O
"	O
small	O
cell	O
carcinoma	O
"	O
)	O
.	O
The	O
words	O
contained	O
in	O
the	O
high	O
-	O
complexity	O
items	O
also	O
have	O
slightly	O
higher	O
concreteness	O
levels	O
,	O
providing	O
another	O
indication	O
that	O
they	O
may	O
contain	O
more	O
terms	O
,	O
as	O
terms	O
tend	O
to	O
be	O
more	O
concrete	O
than	O
common	O
words	O
.	O
Finally	O
,	O
the	O
words	O
contained	O
in	O
the	O
high	O
-	O
complexity	O
items	O
also	O
tend	O
to	O
have	O
more	O
possible	O
meanings	O
,	O
as	O
indicated	O
by	O
the	O
polysemous	O
word	O
count	O
variable	O
,	O
which	O
results	O
in	O
higher	O
complexity	O
owing	O
to	O
disambiguation	O
efforts	O
.	O
Overall	O
,	O
these	O
features	O
indicate	O
that	O
the	O
language	O
used	O
in	O
the	O
low	O
-	O
complexity	O
items	O
is	O
less	O
ambiguous	O
and	O
descriptive	O
,	O
and	O
potentially	O
contains	O
fewer	O
medical	O
terms	O
.	O
One	O
limitation	O
of	O
the	O
study	O
is	O
the	O
fact	O
that	O
it	O
treats	O
item	O
difficulty	O
and	O
time	O
intensiveness	O
as	O
independent	O
variables	O
.	O
This	O
may	O
not	O
always	O
be	O
the	O
case	O
,	O
as	O
examinees	O
do	O
employ	O
strategies	O
to	O
optimize	O
their	O
time	O
.	O
Given	O
finite	O
time	O
limits	O
,	O
examinees	O
may	O
ig	O
-	O
nore	O
time	O
intensive	O
items	O
if	O
they	O
believe	O
the	O
time	O
needed	O
for	O
such	O
items	O
can	O
be	O
better	O
utilized	O
attempting	O
other	O
,	O
less	O
time	O
intensive	O
items	O
.	O
Therefore	O
,	O
the	O
relationship	O
between	O
difficulty	O
and	O
response	O
time	O
and	O
their	O
association	O
with	O
item	O
text	O
would	O
differ	O
for	O
exams	O
that	O
do	O
not	O
impose	O
strict	O
time	O
limits	O
.	O
When	O
using	O
data	O
-	O
driven	O
approaches	O
to	O
defining	O
item	O
classes	O
,	O
our	O
data	O
did	O
not	O
lend	O
itself	O
to	O
a	O
categorization	O
that	O
would	O
allow	O
investigating	O
high	O
difficulty	O
/	O
low	O
response	O
time	O
items	O
and	O
vice	O
-	O
versa	O
.	O
While	O
the	O
approach	O
taken	O
in	O
this	O
paper	O
has	O
a	O
higher	O
ecological	O
validity	O
,	O
studying	O
such	O
cases	O
in	O
the	O
future	O
may	O
lead	O
to	O
a	O
greater	O
understanding	O
of	O
various	O
aspects	O
of	O
response	O
process	O
complexity	O
and	O
their	O
relationship	O
to	O
item	O
text	O
.	O
Other	O
future	O
work	O
includes	O
exploration	O
of	O
potential	O
item	O
position	O
effects	O
.	O

Measuring	O
semantic	B-TaskName
textual	I-TaskName
similarity	I-TaskName
(	O
STS	B-TaskName
)	O
is	O
the	O
task	O
of	O
determining	O
the	O
similarity	O
between	O
two	O
different	O
text	O
passages	O
.	O
The	O
task	O
is	O
important	O
for	O
various	O
natural	O
language	O
processing	O
tasks	O
like	O
topic	O
detection	O
or	O
automated	O
text	B-TaskName
summarization	I-TaskName
because	O
languages	O
are	O
versatile	O
and	O
authors	O
can	O
express	O
similar	O
content	O
or	O
even	O
the	O
same	O
content	O
with	O
different	O
words	O
.	O
Predicting	O
semantic	B-TaskName
textual	I-TaskName
similarity	I-TaskName
has	O
been	O
a	O
recurring	O
task	O
in	O
SemEval	O
challenges	O
(	O
Agirre	O
et	O
al	O
,	O
2015	O
;	O
Agirre	O
et	O
al	O
,	O
2014	O
;	O
Agirre	O
et	O
al	O
,	O
2013	O
;	O
Agirre	O
et	O
al	O
,	O
2012	O
)	O
.	O
As	O
in	O
previous	O
years	O
,	O
the	O
purpose	O
of	O
the	O
STS	B-TaskName
task	O
is	O
the	O
development	O
of	O
systems	O
that	O
automatically	O
predict	O
the	O
semantic	B-TaskName
similarity	I-TaskName
of	O
two	O
sentences	O
in	O
the	O
continuous	O
interval	O
[	O
0	B-DatasetName
,	O
5	O
]	O
where	O
0	B-DatasetName
represents	O
a	O
complete	O
dissimilarity	O
and	O
5	O
denotes	O
a	O
complete	O
semantic	O
equivalence	O
between	O
the	O
sentences	O
(	O
Agirre	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
organizers	O
provide	O
sentence	O
pairs	O
whose	O
semantic	O
similarities	O
have	O
to	O
be	O
predicted	O
by	O
the	O
contestants	O
.	O
The	O
quality	O
of	O
a	O
system	O
is	O
determined	O
by	O
calculating	O
the	O
Pearson	B-MetricName
correlation	I-MetricName
between	O
the	O
predicted	O
values	O
and	O
a	O
human	O
gold	O
standard	O
that	O
has	O
been	O
created	O
by	O
crowdsourcing	O
.	O
The	O
data	O
from	O
previous	O
STS	B-TaskName
tasks	O
can	O
be	O
used	O
for	O
training	O
supervised	O
methods	O
.	O
The	O
test	O
data	O
consists	O
of	O
text	O
content	O
from	O
different	O
sources	O
.	O
In	O
this	O
year	O
's	O
shared	O
task	O
,	O
the	O
systems	O
are	O
tested	O
on	O
five	O
different	O
categories	O
with	O
different	O
topics	O
and	O
varying	O
textual	O
characteristics	O
like	O
text	O
length	O
or	O
spelling	O
errors	O
:	O
answer	O
-	O
answer	O
,	O
plagiarism	O
,	O
postediting	O
,	O
headlines	O
,	O
and	O
question	O
-	O
question	O
The	O
remainder	O
of	O
the	O
paper	O
is	O
structured	O
as	O
follows	O
:	O
Section	O
2	O
discusses	O
related	O
approaches	O
to	O
automatically	O
determining	O
semantic	B-TaskName
textual	I-TaskName
similarity	I-TaskName
.	O
Section	O
3	O
describes	O
our	O
three	O
methods	O
in	O
detail	O
.	O
We	O
discuss	O
their	O
results	O
in	O
section	O
4	O
.	O
Finally	O
,	O
we	O
conclude	O
in	O
chapter	O
5	O
and	O
outline	O
future	O
work	O
.	O

The	O
Overlap	O
method	O
measures	O
the	O
token	O
-	O
based	O
overlap	O
between	O
two	O
sentences	O
.	O
Therefore	O
,	O
we	O
need	O
to	O
define	O
a	O
similarity	O
function	O
for	O
tokens	O
:	O
We	O
first	O
try	O
to	O
identify	O
a	O
textual	O
similarity	O
of	O
1	O
by	O
comparing	O
the	O
lower	O
case	O
lemmas	O
of	O
both	O
tokens	O
or	O
by	O
checking	O
if	O
their	O
most	O
common	O
WordNet	O
synsets	O
are	O
the	O
same	O
.	O
We	O
assess	O
their	O
similarity	O
as	O
0.5	O
if	O
they	O
share	O
any	O
synset	O
.	O
If	O
this	O
is	O
not	O
the	O
case	O
,	O
we	O
use	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
with	O
the	O
300	O
-	O
dimensional	O
GoogleNews	O
-	O
vectors	O
-	O
negative300	O
model	O
.	O
We	O
look	O
up	O
both	O
words	O
(	O
or	O
their	O
lemmas	O
if	O
the	O
words	O
are	O
not	O
present	O
in	O
the	O
model	O
)	O
and	O
calculate	O
the	O
cosine	O
similarity	O
of	O
their	O
word	B-TaskName
embeddings	I-TaskName
.	O
Otherwise	O
,	O
we	O
return	O
a	O
default	O
value	O
.	O
This	O
yields	O
the	O
following	O
similarity	O
function	O
for	O
two	O
tokens	O
:	O
sim	O
(	O
t	O
1	O
,	O
t	O
2	O
)	O
:	O
=	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
1	O
if	O
t	O
1	O
.lemma	O
=	O
=	O
t	O
2	O
.lemma	O
1	O
if	O
t	O
1	O
and	O
t	O
2	O
have	O
the	O
same	O
most	O
common	O
synset	O
0.5	O
if	O
t	O
1	O
and	O
t	O
2	O
share	O
any	O
synset	O
d	O
(	O
t	O
1	O
,	O
t	O
2	O
)	O
if	O
t	O
1	O
and	O
t	O
2	O
have	O
word	B-TaskName
embeddings	I-TaskName
default	O
otherwise	O
where	O
d	O
(	O
t	O
1	O
,	O
t	O
2	O
)	O
denotes	O
the	O
cosine	O
similarity	O
between	O
the	O
two	O
word	B-TaskName
embeddings	I-TaskName
of	O
the	O
tokens	O
.	O
Given	O
a	O
token	O
t	O
from	O
one	O
sentence	O
,	O
we	O
calculate	O
its	O
similarity	O
to	O
another	O
sentence	O
S	O
by	O
taking	O
the	O
maximum	O
similarity	O
between	O
t	O
and	O
all	O
tokens	O
of	O
S	O
:	O
msim	O
(	O
t	O
,	O
S	O
)	O
:	O
=	O
max	O
t	O
2	O
S	O
sim	O
(	O
t	O
,	O
t	O
2	O
)	O
We	O
define	O
the	O
similarity	O
score	O
between	O
two	O
sentences	O
in	O
[	O
0	B-DatasetName
,	O
1	O
]	O
as	O
follows	O
:	O
ssim	B-TaskName
(	O
s	O
1	O
,	O
s	O
2	O
)	O
:	O
=	O
t	O
s	O
1	O
msim	O
(	O
t	O
,	O
s	O
2	O
)	O
2	O
|	O
s	O
1	O
|	O
+	O
t	O
s	O
2	O
msim	O
(	O
t	O
,	O
s	O
1	O
)	O
2	O
|	O
s	O
2	O
|	O
To	O
predict	O
the	O
semantic	B-TaskName
similarity	I-TaskName
score	O
in	O
[	O
0	B-DatasetName
,	O
5	O
]	O
,	O
we	O
multiply	O
ssim	B-TaskName
by	O
5	O
,	O
however	O
,	O
this	O
does	O
not	O
change	O
our	O
evaluation	O
results	O
because	O
the	O
Pearson	B-MetricName
correlation	I-MetricName
is	O
scale	O
invariant	O
:	O
STS	B-TaskName
(	O
s	O
1	O
,	O
s	O
2	O
)	O
:	O
=	O
5	O
ssim	B-TaskName
(	O
s	O
1	O
,	O
s	O
2	O
)	O
We	O
observed	O
that	O
some	O
samples	O
in	O
the	O
STS	B-TaskName
2016	O
test	O
data	O
consist	O
almost	O
entirely	O
of	O
stopwords	O
.	O
For	O
example	O
,	O
the	O
STS	B-TaskName
2016	O
evaluation	O
data	O
contained	O
a	O
sample	O
with	O
the	O
sentences	O
"	O
I	O
think	O
you	O
should	O
do	O
both	O
.	O
"	O
and	O
"	O
You	O
should	O
do	O
both	O
.	O
"	O
before	O
the	O
final	O
filtering	O
.	O
After	O
filtering	O
stop	O
words	O
,	O
the	O
first	O
sentence	O
would	O
only	O
contain	O
the	O
word	O
"	O
think	O
"	O
and	O
the	O
second	O
sentence	O
would	O
be	O
empty	O
,	O
which	O
would	O
result	O
in	O
a	O
predicted	O
score	O
of	O
zero	O
.	O
To	O
avoid	O
these	O
extreme	O
cases	O
,	O
we	O
do	O
not	O
filter	O
stop	O
words	O
if	O
this	O
would	O
result	O
in	O
a	O
sentence	O
length	O
of	O
less	O
than	O
two	O
tokens	O
in	O
both	O
sentences	O
.	O

Hidden	O
layer	O
Output	O
layer	O
All	O
samples	O
are	O
preprocessed	O
as	O
described	O
in	O
section	O
3.1.1	O
.	O
For	O
each	O
sample	O
(	O
s	O
1	O
,	O
s	O
2	O
,	O
gs	O
)	O
in	O
the	O
training	O
set	O
,	O
we	O
create	O
a	O
vocabulary	O
list	O
of	O
the	O
lowercase	O
lemmas	O
from	O
both	O
sentences	O
.	O
Lemmas	O
that	O
share	O
a	O
most	O
common	O
synset	O
in	O
WordNet	O
are	O
grouped	O
together	O
.	O
Let	O
n	O
be	O
the	O
size	O
of	O
the	O
vocabulary	O
.	O
We	O
create	O
two	O
bag	O
-	O
of	O
-	O
words	O
vectors	O
bow	O
s	O
1	O
and	O
bow	O
s	O
2	O
.	O
For	O
each	O
lemma	B-DatasetName
l	O
,	O
we	O
calculate	O
the	O
minimum	O
number	O
of	O
times	O
l	O
occurs	O
in	O
each	O
sentence	O
and	O
the	O
delta	O
between	O
the	O
minimum	O
and	O
the	O
maximum	O
:	O
min	O
i	O
:	O
=	O
min	O
(	O
bow	O
s	O
1	O
[	O
i	O
]	O
,	O
bow	O
s	O
2	O
[	O
i	O
]	O
)	O
|	O
∆	O
i	O
|	O
:	O
=	O
|	O
bow	O
s	O
1	O
[	O
i	O
]	O
−	O
bow	O
s	O
2	O
[	O
i	O
]	O
|	O
As	O
input	O
vectors	O
for	O
the	O
neural	O
net	O
,	O
we	O
build	O
two	O
sums	O
per	O
sample	O
and	O
use	O
them	O
as	O
the	O
two	O
dimensional	O
feature	O
vector	O
(	O
sameWords	O
,	O
notSameWords	O
)	O
for	O
the	O
expected	O
output	O
gs	O
:	O
1	O
shows	O
an	O
example	O
of	O
the	O
same	O
word	O
neural	O
network	O
method	O
for	O
the	O
two	O
input	O
sentences	O
"	O
Tim	O
plays	O
the	O
guitar	O
"	O
and	O
"	O
Tim	O
likes	O
guitar	O
songs	O
"	O
,	O
which	O
have	O
the	O
input	O
vector	O
(	O
2	O
,	O
3	O
)	O
.	O
We	O
trained	O
the	O
neural	O
net	O
until	O
the	O
error	O
rate	O
between	O
two	O
iterations	O
did	O
not	O
change	O
more	O
than	O
ε	B-HyperparameterName
=	O
10	O
−5	O
.	O
sameWords	O
:	O
=	O
n	O
i=1	O
min	O
i	O
notSameWords	O
:	O
=	O
n	O
i=1	O
|	O
∆	O
i	O
|	O
Table	O
i	O
Lemma	B-DatasetName
bow	O
s	O
1	O
bow	O
s	O
2	O
min	O
i	O
|	O
∆	O
i	O
|	O
1	O
tim	O
1	O
1	O
1	O
0	B-DatasetName
2	O
play	O
1	O
0	B-DatasetName
0	B-DatasetName
1	O
3	O
guitar	O
1	O
1	O
1	O
0	B-DatasetName
4	O
like	O
0	B-DatasetName
1	O
0	B-DatasetName
1	O
5	O
song	O
0	B-DatasetName
1	O
0	B-DatasetName
1	O
2	O
3	O

The	O
surface	O
-	O
level	O
similarity	O
can	O
to	O
some	O
extent	O
(	O
although	O
not	O
entirely	O
)	O
capture	O
the	O
semantic	B-TaskName
similarity	I-TaskName
between	O
documents	O
.	O
Let	O
s	O
1	O
and	O
s	O
2	O
be	O
the	O
reference	O
and	O
the	O
candidate	O
documents	O
respectively	O
.	O
We	O
compute	O
the	O
components	O
f	O
1	O
,	O
f	O
2	O
R	O
as	O
follows	O
:	O
f	O
1	O
(	O
s	O
1	O
,	O
s	O
2	O
,	O
N	O
)	O
=	O
m	O
N	O
l	O
s	O
1	O
N	O
f	O
2	O
(	O
s	O
1	O
,	O
s	O
2	O
,	O
N	O
)	O
=	O
N	O
n=1	O
m	O
N	O
l	O
s	O
2	O
n	O
1	O
N	O
where	O
m	O
N	O
is	O
the	O
number	O
of	O
matched	O
N	O
-	O
grams	O
between	O
s	O
1	O
and	O
s	O
2	O
,	O
l	O
s	O
1	O
N	O
denotes	O
the	O
total	O
number	O
of	O
Ngrams	O
in	O
s	O
1	O
and	O
l	O
s	O
2	O
n	O
is	O
the	O
total	O
number	O
of	O
n	O
-	O
grams	O
in	O
s	O
2	O
.	O
f	O
1	O
is	O
the	O
common	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
metric	O
used	O
in	O
automatic	O
text	B-TaskName
summarization	I-TaskName
and	O
f	O
2	O
is	O
a	O
modified	O
version	O
of	O
the	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
metric	O
(	O
standard	O
machine	B-TaskName
translation	I-TaskName
metric	O
)	O
where	O
the	O
brevity	O
penalty	O
is	O
eliminated	O
.	O
Note	O
that	O
f	O
1	O
can	O
be	O
interpreted	O
as	O
the	O
recall	O
-	O
oriented	O
surface	O
similarity	O
and	O
f	O
2	O
as	O
the	O
precision	O
-	O
oriented	O
one	O
.	O

To	O
model	O
the	O
topical	O
similarity	O
between	O
two	O
documents	O
,	O
we	O
use	O
Latent	O
Dirichlet	O
Allocation	O
(	O
LDA	B-MethodName
)	O
(	O
Blei	O
et	O
al	O
,	O
2003	O
)	O
to	O
train	O
models	O
on	O
the	O
English	O
Wikipedia	O
.	O
For	O
both	O
documents	O
s	O
1	O
and	O
s	O
2	O
,	O
we	O
compute	O
the	O
topic	O
distributions	O
θ	B-HyperparameterName
1	O
and	O
θ	B-HyperparameterName
2	O
and	O
use	O
the	O
Hellinger	O
distance	O
to	O
measure	O
the	O
similarity	O
between	O
the	O
documents	O
.	O
This	O
can	O
be	O
formally	O
written	O
as	O
f	O
4	O
(	O
s	O
1	O
,	O
s	O
2	O
)	O
=	O
1	O
−	O
1	O
√	O
2	O
k	O
i=1	O
θ	B-HyperparameterName
1	O
i	O
−	O
θ	B-HyperparameterName
2	O
i	O
2	O
where	O
k	O
represents	O
the	O
number	O
of	O
learned	O
LDA	B-MethodName
topics	O
.	O

In	O
this	O
years	O
shared	O
task	O
,	O
117	O
runs	O
were	O
submitted	O
.	O
We	O
achieved	O
weighted	O
mean	O
Pearson	O
correlations	O
of	O
0.71134	O
,	O
0.67502	O
and	O
0.62078	O
.	O
In	O
this	O
year	O
's	O
run	O
,	O
our	O
best	O
result	O
was	O
the	O
Overlap	O
method	O
,	O
followed	O
by	O
the	O
Same	O
Word	O
Neural	O
Network	O
method	O
and	O
the	O
Deep	O
LDA	B-MethodName
approach	O
.	O
Table	O
2	O
shows	O
examples	O
of	O
good	O
and	O
bad	O
results	O
of	O
our	O
Overlap	O
method	O
on	O
the	O
2016	O
data	O
.	O
Detailed	O
results	O
of	O
our	O
runs	O
are	O
given	O
in	O
Table	O
3	O
per	O
test	O
set	O
.	O
Our	O
three	O
approaches	O
achieved	O
different	O
results	O
.	O
From	O
a	O
semantic	O
point	O
of	O
view	O
,	O
the	O
most	O
obvious	O
value	O
for	O
the	O
default	O
value	O
in	O
our	O
Overlap	O
method	O
is	O
0	B-DatasetName
.	O
However	O
,	O
we	O
have	O
discovered	O
that	O
a	O
default	O
value	O
0.15	O
returned	O
better	O
results	O
on	O
the	O
STS	B-TaskName
Core	O
test	O
data	O
from	O
2015	O
and	O
also	O
chose	O
this	O
default	O
value	O
for	O
our	O
submission	O
.	O
In	O
the	O
Deep	O
LDA	B-MethodName
approach	O
,	O
we	O
set	O
the	O
parameter	O
N	O
=	O
2	O
,	O
although	O
the	O
use	O
of	O
unigrams	O
did	O
not	O
show	O
any	O
significant	O
statistical	O
difference	O
in	O
the	O
results	O
.	O
We	O
choose	O
the	O
number	O
of	O
topics	O
in	O
the	O
LDA	B-MethodName
model	O
to	O
be	O
300	O
.	O
In	O
the	O
prediction	O
phase	O
of	O
the	O
al	O
-	O
gorithm	O
,	O
we	O
select	O
k	B-HyperparameterName
=	I-HyperparameterName
100	O
nearest	O
neighbors	O
from	O
the	O
data	O
sets	O
provided	O
from	O
2012	O
to	O
2015	O
.	O

The	O
Statistics	O
for	O
the	O
6	O
largest	O
categories	O
used	O
in	O
this	O
paper	O
are	O
shown	O
in	O
Table	O
1	O
,	O
containing	O
a	O
snapshot	O
of	O
product	O
details	O
up	O
to	O
January	O
2019	O
.	O
Except	O
for	O
mobiles	O
,	O
for	O
other	O
domains	O
,	O
300	O
products	O
were	O
sampled	O
.	O
As	O
the	O
number	O
of	O
question	O
-	O
specification	O
pairs	O
is	O
huge	O
,	O
manually	O
labelling	O
a	O
sufficiently	O
large	O
dataset	O
is	O
a	O
tedious	O
task	O
.	O
So	O
,	O
we	O
propose	O
a	O
semisupervised	O
method	O
to	O
create	O
a	O
large	O
training	O
dataset	O
using	O
Dual	O
Embedding	O
Space	O
model	O
(	O
DESM	O
)	O
(	O
Mitra	O
et	O
al	O
,	O
2016	O
)	O
.	O
Suppose	O
a	O
product	O
P	O
has	O
S	O
specifications	O
and	O
Q	O
questions	O
.	O
For	O
a	O
question	O
q	O
i	O
Q	O
and	O
a	O
specification	O
s	O
j	O
S	O
,	O
we	O
find	O
dual	O
embedding	O
score	O
DU	O
AL	O
(	O
q	O
i	O
,	O
s	O
j	O
)	O
using	O
Equation	O
1	O
,	O
where	O
t	O
q	O
and	O
t	O
s	O
denote	O
the	O
vectors	O
for	O
the	O
question	O
and	O
specification	O
terms	O
,	O
respectively	O
.	O
We	O
consider	O
(	O
q	O
i	O
,	O
s	O
j	O
)	O
pair	O
positive	O
if	O
DU	O
AL	O
(	O
q	O
i	O
,	O
s	O
j	O
)	O
≥	O
θ	B-HyperparameterName
and	O
negative	O
if	O
DU	O
AL	O
(	O
q	O
i	O
,	O
s	O
j	O
)	O
<	O
θ	B-HyperparameterName
.	O
DU	O
AL	O
(	O
q	O
i	O
,	O
s	O
j	O
)	O
=	O
1	O
|	O
q	O
i	O
|	O
tq	O
q	O
i	O
t	O
q	O
T	O
s	O
j	O
t	O
q	O
s	O
j	O
(	O
1	O
)	O
where	O
s	O
j	O
=	O
1	O
|	O
s	O
j	O
|	O
ts	B-MethodName
s	O
j	O
t	O
s	O
t	O
s	O
(	O
2	O
)	O
We	O
take	O
M	O
obile	O
dataset	O
to	O
create	O
labelled	O
training	O
data	O
since	O
most	O
of	O
the	O
questions	O
come	O
from	O
this	O
vertical	O
.	O
We	O
choose	O
the	O
threshold	O
value	O
(	O
θ	B-HyperparameterName
)	O
which	O
gives	O
the	O
best	O
accuracy	B-MetricName
on	O
manually	O
labelled	O
balanced	O
validation	B-DatasetName
dataset	I-DatasetName
consisting	O
of	O
380	O
question	O
and	O
specification	O
pairs	O
.	O
We	O
train	O
a	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
model	O
on	O
our	O
training	O
dataset	O
to	O
get	O
the	O
embeddings	O
of	O
the	O
words	O
.	O
The	O
word2vec	O
model	O
learns	O
two	O
weight	O
matrices	O
during	O
training	O
.	O
The	O
matrix	O
corresponding	O
to	O
the	O
input	O
space	O
and	O
the	O
output	O
space	O
is	O
denoted	O
as	O
IN	O
and	O
OUT	O
word	O
embedding	O
space	O
respectively	O
.	O
2	O
.	O
We	O
analyze	O
the	O
questions	O
in	O
the	O
test	O
datasets	O
and	O
find	O
that	O
the	O
questions	O
can	O
be	O
roughly	O
categorized	O
into	O
three	O
classes	O
-	O
numerical	O
,	O
yes	O
/	O
no	O
and	O
others	O
based	O
upon	O
the	O
answer	O
type	O
of	O
the	O
questions	O
.	O
For	O
a	O
question	O
,	O
we	O
have	O
a	O
number	O
of	O
specifications	O
and	O
only	O
one	O
of	O
them	O
is	O
correct	O
.	O

We	O
ing	O
rate	O
0.01	O
.	O
The	O
fine	O
-	O
tuning	O
of	O
BERT	B-MethodName
and	O
XL	O
-	O
Net	O
is	O
done	O
with	O
the	O
same	O
experimental	O
settings	O
as	O
given	O
in	O
the	O
original	O
papers	O
.	O
In	O
all	O
the	O
models	O
,	O
we	O
minimize	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
while	O
training	O
.	O
BERT	B-MethodName
-	O
380	O
and	O
XLNet	B-MethodName
-	O
380	O
models	O
are	O
fine	O
-	O
tuned	O
on	O
the	O
380	O
labeled	O
validation	B-DatasetName
dataset	I-DatasetName
that	O
was	O
used	O
for	O
creating	O
the	O
training	O
dataset	O
in	O
Section	O
5.1	O
.	O
During	O
evaluation	O
,	O
we	O
sort	O
the	O
question	O
specification	O
pairs	O
according	O
to	O
their	O
relevance	O
score	O
.	O
From	O
this	O
ranked	O
list	O
,	O
we	O
compute	O
whether	O
the	O
correct	O
specification	O
appears	O
within	O
top	O
k	O
,	O
k	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
positions	O
.	O
The	O
ratio	O
of	O
correctly	O
identified	O
specifications	O
in	O
top	O
1	O
,	O
2	O
,	O
and	O
3	O
positions	O
to	O
the	O
total	O
number	O
of	O
questions	O
is	O
denoted	O
as	O
HIT@1	O
,	O
HIT@2	O
and	O
HIT@3	O
respectively	O
.	O

Table	O
3	O
shows	O
the	O
performance	O
of	O
the	O
models	O
on	O
different	O
datasets	O
3	O
.	O
BERT	B-MethodName
-	O
380	O
and	O
XLNet	B-MethodName
-	O
380	O
perform	O
very	O
poorly	O
,	O
but	O
when	O
we	O
use	O
the	O
train	O
dataset	O
created	O
with	O
DESM	O
,	O
there	O
is	O
a	O
large	O
boost	O
in	O
the	O
models	O
'	O
performance	O
and	O
it	O
shows	O
the	O
effectiveness	O
of	O
our	O
semi	O
-	O
supervised	O
method	O
in	O
generating	O
labeled	O
dataset	O
.	O
Both	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
outperform	O
the	O
baseline	O
Siamese	O
model	O
(	O
Lai	O
et	O
al	O
,	O
2018	O
)	O
by	O
a	O
large	O
margin	O
,	O
and	O
retrieve	O
the	O
correct	O
specification	O
within	O
top	O
3	O
results	O
for	O
most	O
of	O
the	O
queries	O
.	O
For	O
Backpack	O
and	O
AC	O
,	O
both	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
are	O
very	O
competitive	O
.	O
XLNet	B-MethodName
outperforms	O
BERT	B-MethodName
in	O
Computer	O
,	O
Shoes	O
,	O
and	O
Watches	O
.	O
Only	O
in	O
HIT@1	O
of	O
AC	O
,	O
BERT	B-MethodName
has	O
surpassed	O
XLNet	B-MethodName
with	O
0.07	O
points	O
.	O
We	O
see	O
that	O
all	O
the	O
models	O
have	O
performed	O
better	O
in	O
Computer	O
compared	O
to	O
the	O
other	O
datasets	O
.	O
Computer	O
has	O
the	O
highest	O
percentage	O
of	O
yes	O
/	O
no	O
questions	O
and	O
this	O
might	O
be	O
one	O
of	O
the	O
reasons	O
,	O
as	O
some	O
questions	O
might	O
have	O
word	O
overlap	O
with	O
correct	O
specification	O
.	O
Table	O
4	O
shows	O
the	O
top	O
three	O
specifications	O
returned	O
by	O
different	O
models	O
for	O
some	O
questions	O
.	O
We	O
see	O
that	O
Siamese	O
architecture	O
returns	O
results	O
which	O
look	O
similar	O
to	O
naïve	O
word	O
match	O
,	O
and	O
retrieve	O
wrong	O
specifications	O
.	O
On	O
the	O
other	O
hand	O
,	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
are	O
able	O
to	O
retrieve	O
the	O
correct	O
specifications	O
.	O
Error	B-MetricName
Analysis	O
:	O
We	O
assume	O
that	O
for	O
each	O
question	O
,	O
there	O
is	O
only	O
one	O
correct	O
specification	O
,	O
but	O
the	O
correct	O
answer	O
may	O
span	O
multiple	O
specifications	O
and	O
our	O
models	O
can	O
not	O
provide	O
a	O
full	O
answer	O
.	O
For	O
example	O
,	O
in	O
Backpack	O
dataset	O
,	O
the	O
dimension	O
of	O
the	O
backpack	O
,	O
i.e.	O
,	O
its	O
height	O
,	O
weight	O
,	O
depth	O
is	O
defined	O
separately	O
.	O
So	O
,	O
when	O
user	O
queries	O
about	O
the	O
dimension	O
,	O
only	O
one	O
specification	O
is	O
provided	O
.	O
Some	O
specifications	O
are	O
given	O
in	O
one	O
unit	O
,	O
but	O
users	O
want	O
the	O
answer	O
in	O
another	O
unit	O
,	O
e.g.	O
,	O
"	O
what	O
is	O
the	O
width	O
of	O
this	O
bag	O
in	O
cms	O
?	O
"	O
.	O
Since	O
the	O
specification	O
is	O
given	O
in	O
inches	O
,	O
the	O
models	O
show	O
the	O
answer	O
in	O
inches	O
.	O
So	O
,	O
the	O
answer	O
is	O
related	O
,	O
but	O
not	O
exactly	O
correct	O
.	O
Users	O
sometimes	O
want	O
to	O
know	O
the	O
difference	O
between	O
certain	O
specification	O
types	O
,	O
what	O
is	O
meant	O
by	O
some	O
specifications	O
.	O
For	O
example	O
,	O
consider	O
the	O
questions	O
"	O
what	O
is	O
the	O
difference	O
between	O
inverter	O
and	O
non	O
-	O
inverter	O
AC	O
?	O
"	O
,	O
"	O
what	O
is	O
meant	O
by	O
water	O
resistant	O
depth	O
?	O
"	O
.	O
While	O
we	O
can	O
find	O
the	O
type	O
of	O
inverter	O
,	O
the	O
water	O
resistant	O
depth	O
of	O
a	O
watch	O
etc	O
.	O
from	O
specifications	O
,	O
the	O
definition	O
of	O
the	O
specification	O
is	O
not	O
given	O
.	O
As	O
we	O
have	O
generated	O
train	O
data	O
labels	O
in	O
semi	O
-	O
supervised	O
fashion	O
,	O
it	O
also	O
contributes	O
to	O
inaccurate	O
classification	O
in	O
some	O
cases	O
.	O

An	O
important	O
desideratum	O
of	O
natural	O
language	O
generation	O
(	O
NLG	O
)	O
is	O
to	O
produce	O
outputs	O
that	O
are	O
not	O
only	O
correct	O
but	O
also	O
diverse	O
(	O
Tevet	O
and	O
Berant	O
,	O
2021	O
)	O
.	O
The	O
term	O
"	O
diversity	O
"	O
in	O
NLG	O
is	O
defined	O
as	O
the	O
ability	O
of	O
a	O
generative	O
model	O
to	O
create	O
a	O
set	O
of	O
possible	O
outputs	O
that	O
are	O
each	O
valid	O
given	O
the	O
input	O
and	O
vary	O
as	O
widely	O
as	O
possible	O
in	O
terms	O
of	O
content	O
,	O
language	O
style	O
,	O
and	O
word	O
variability	O
(	O
Gupta	O
et	O
al	O
,	O
2018	O
)	O
.	O
This	O
research	O
problem	O
is	O
also	O
referred	O
as	O
one	O
-	O
to	O
-	O
many	O
generation	O
(	O
Shen	O
et	O
al	O
,	O
2019	O
;	O
Cho	O
et	O
al	O
,	O
2019	O
;	O
Shen	O
et	O
al	O
,	O
2022	O
)	O
.	O
Diversity	O
in	O
NLG	O
has	O
been	O
extensively	O
studied	O
for	O
various	O
tasks	O
in	O
the	O
past	O
few	O
years	O
,	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
(	O
Shen	O
et	O
al	O
,	O
2019	O
)	O
and	O
paraphrase	O
Codes	O
of	O
our	O
model	O
and	O
baselines	O
are	O
available	O
at	O
https://github.com/DM2	O
-	O
ND	O
/	O
MoKGE	O
.	O
[	O
1	O
]	O
[	O
4	O
]	O
[	O
3	O
]	O
[	O
1	O
]	O
[	O
3	O
]	O
[	O
4	O
]	O
[	O
4	O
]	O
[	O
1	O
]	O
[	O
3	O
]	O
[	O
4	O
]	O
[	O
2	O
]	O
[	O
4	O
]	O
[	O
1	O
]	O
(	O
1	O
)	O
You	O
can	O
produce	O
music	O
when	O
pressing	O
keys	O
on	O
the	O
piano	O
,	O
so	O
it	O
is	O
an	O
instrument	O
.	O
generation	O
(	O
Gupta	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
these	O
tasks	O
,	O
output	O
spaces	O
are	O
constrained	O
by	O
input	O
context	O
,	O
i.e.	O
,	O
the	O
contents	O
of	O
multiple	O
outputs	O
should	O
be	O
similar	O
,	O
and	O
globally	O
,	O
under	O
the	O
same	O
topic	O
.	O
However	O
,	O
many	O
NLG	O
tasks	O
,	O
e.g.	O
,	O
generative	O
commonsense	O
reasoning	O
,	O
pose	O
unique	O
challenges	O
for	O
generating	O
multiple	O
reasonable	O
outputs	O
that	O
are	O
semantically	O
different	O
.	O
Figure	O
1	O
shows	O
an	O
example	O
in	O
the	O
commonsense	O
explanation	B-TaskName
generation	I-TaskName
(	O
ComVE	O
)	O
task	O
.	O
The	O
dataset	O
has	O
collected	O
explanations	O
to	O
counterfactual	O
statements	O
for	O
sense	O
-	O
making	O
from	O
three	O
annotators	O
(	O
Wang	O
et	O
al	O
,	O
2020	O
)	O
.	O
From	O
the	O
annotations	O
,	O
we	O
observed	O
that	O
different	O
annotators	O
gave	O
explanations	O
to	O
the	O
unreasonable	O
statement	O
from	O
different	O
perspectives	O
to	O
make	O
them	O
diverse	O
in	O
terms	O
of	O
content	O
,	O
e.g.	O
,	O
wrong	O
effect	O
and	O
inappropriate	O
usage	O
.	O
In	O
order	O
to	O
create	O
diversity	O
,	O
existing	O
methods	O
attempted	O
to	O
produce	O
uncertainty	O
by	O
introducing	O
random	O
noise	O
into	O
a	O
latent	O
variable	O
(	O
Gupta	O
et	O
al	O
,	O
2018	O
)	O
or	O
sampling	O
next	O
token	O
widely	O
from	O
the	O
vo	O
-	O
cabulary	O
.	O
However	O
,	O
these	O
methods	O
were	O
not	O
able	O
to	O
explicitly	O
control	O
varying	O
semantics	O
units	O
and	O
produce	O
outputs	O
of	O
diverse	O
content	O
.	O
Meanwhile	O
,	O
the	O
input	O
text	O
alone	O
contains	O
too	O
limited	O
knowledge	O
to	O
support	O
diverse	O
reasoning	O
and	O
produce	O
multiple	O
reasonable	O
outputs	O
(	O
Yu	O
et	O
al	O
,	O
2022c	O
)	O
.	O
As	O
an	O
example	O
,	O
Table	O
1	O
shows	O
the	O
human	O
evaluation	O
results	O
on	O
two	O
GCR	O
tasks	O
.	O
While	O
human	O
annotators	O
were	O
able	O
to	O
produce	O
2.60	O
different	O
yet	O
reasonable	O
explanations	O
on	O
the	O
ComVE	O
dataset	O
,	O
one	O
SoTA	O
diversity	O
-	O
promoting	O
method	O
(	O
i.e.	O
,	O
nucleus	O
sampling	O
)	O
could	O
produce	O
only	O
2.15	O
reasonable	O
explanations	O
.	O
To	O
improve	O
the	O
diversity	O
in	O
outputs	O
for	O
GCR	O
tasks	O
,	O
we	O
investigated	O
the	O
ComVE	O
task	O
and	O
found	O
that	O
75	O
%	O
of	O
the	O
concepts	O
(	O
nouns	O
and	O
verbs	O
)	O
in	O
human	O
annotations	O
were	O
among	O
2	O
-	O
hop	O
neighbors	O
of	O
the	O
concepts	O
contained	O
in	O
the	O
input	O
sequence	O
on	O
the	O
commonsense	O
KG	O
ConceptNet	B-DatasetName
1	O
.	O
Therefore	O
,	O
to	O
produce	O
diverse	O
GCR	O
,	O
our	O
idea	O
is	O
enabling	O
NLG	O
models	O
to	O
reason	O
from	O
different	O
perspectives	O
of	O
knowledge	O
on	O
commonsense	O
KG	O
and	O
use	O
them	O
to	O
generate	O
diverse	O
outputs	O
like	O
the	O
human	O
annotators	O
.	O
Thus	O
,	O
we	O
present	O
a	O
novel	O
Mixture	O
of	O
Knowledge	O
Graph	O
Expert	O
(	O
MoKGE	O
)	O
method	O
for	O
diverse	O
generative	O
commonsense	O
reasoning	O
on	O
KG	O
.	O
MoKGE	O
contains	O
two	O
major	O
components	O
:	O
(	O
i	O
)	O
a	O
knowledge	O
graph	O
(	O
KG	O
)	O
enhanced	O
generative	O
reasoning	O
module	O
to	O
reasonably	O
associate	O
relevant	O
concepts	O
into	O
the	O
generation	O
process	O
,	O
and	O
(	O
ii	O
)	O
a	O
mixture	O
of	O
expert	O
(	O
MoE	O
)	O
module	O
to	O
produce	O
diverse	O
reasonable	O
outputs	O
.	O
Specifically	O
,	O
the	O
generative	O
reasoning	O
module	O
performs	O
compositional	O
operations	O
on	O
KG	O
to	O
obtain	O
structure	O
-	O
aware	O
representations	O
of	O
concepts	O
and	O
relations	O
.	O
Then	O
,	O
each	O
expert	O
uses	O
these	O
representations	O
to	O
seek	O
different	O
yet	O
relevant	O
sets	O
of	O
concepts	O
and	O
sends	O
them	O
into	O
a	O
standard	O
Transformer	B-MethodName
model	O
to	O
generate	O
the	O
corresponding	O
output	O
.	O
To	O
encourage	O
different	O
experts	O
to	O
specialize	O
in	O
different	O
reasoning	O
abilities	O
,	O
we	O
employ	O
the	O
stochastic	O
hard	O
-	O
EM	B-MetricName
algorithm	O
by	O
assigning	O
full	O
responsibility	O
of	O
the	O
largest	O
joint	O
probability	O
to	O
each	O
expert	O
.	O
We	O
conducted	O
experiments	O
on	O
two	O
GCR	O
benchmarks	O
,	O
i.e.	O
,	O
commonsense	O
explanation	B-TaskName
generation	I-TaskName
and	O
abductive	O
commonsense	O
reasoning	O
.	O
Empirical	O
experiments	O
demonstrated	O
that	O
our	O
proposed	O
MoKGE	O
can	O
outperform	O
existing	O
diversitypromoting	O
generation	O
methods	O
in	O
diversity	O
,	O
while	O
achieving	O
on	O
par	O
performance	O
in	O
quality	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
to	O
boost	O
diversity	O
in	O
NLG	O
by	O
diversifying	O
knowledge	O
reasoning	O
on	O
commonsense	O
KG	O
.	O

Generating	O
multiple	O
valid	O
outputs	O
given	O
a	O
source	O
sequence	O
has	O
a	O
wide	O
range	O
of	O
applications	O
,	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
(	O
Shen	O
et	O
al	O
,	O
2019	O
)	O
,	O
paraphrase	B-TaskName
generation	I-TaskName
(	O
Gupta	O
et	O
al	O
,	O
2018	O
)	O
,	O
question	B-TaskName
generation	I-TaskName
(	O
Cho	O
et	O
al	O
,	O
2019	O
)	O
,	O
dialogue	O
system	O
(	O
Dou	O
et	O
al	O
,	O
2021	O
)	O
,	O
and	O
story	B-TaskName
generation	I-TaskName
.	O
For	O
example	O
,	O
in	O
machine	B-TaskName
translation	I-TaskName
,	O
there	O
are	O
often	O
many	O
plausible	O
and	O
semantically	O
equivalent	O
translations	O
due	O
to	O
information	O
asymmetry	O
between	O
different	O
languages	O
(	O
Lachaux	O
et	O
al	O
,	O
2020	O
)	O
.	O
Methods	O
of	O
improving	O
diversity	O
in	O
NLG	O
have	O
been	O
explored	O
from	O
various	O
perspectives	O
.	O
Sampling	O
-	O
based	O
decoding	O
is	O
one	O
of	O
the	O
most	O
effective	O
solutions	O
to	O
improve	O
diversity	O
.	O
For	O
example	O
,	O
nucleus	O
sampling	O
samples	O
next	O
tokens	O
from	O
the	O
dynamic	O
nucleus	O
of	O
tokens	O
containing	O
the	O
vast	O
majority	O
of	O
the	O
probability	O
mass	O
,	O
instead	O
of	O
decoding	O
text	O
by	O
maximizing	O
the	O
likelihood	O
.	O
Another	O
line	O
of	O
work	O
focused	O
on	O
introducing	O
random	O
noise	O
(	O
Gupta	O
et	O
al	O
,	O
2018	O
)	O
or	O
changing	O
latent	O
variables	O
(	O
Lachaux	O
et	O
al	O
,	O
2020	O
)	O
to	O
produce	O
uncertainty	O
.	O
In	O
addition	O
,	O
Shen	O
et	O
al	O
(	O
2019	O
)	O
adopted	O
a	O
mixture	O
of	O
experts	O
to	O
diversify	O
machine	B-TaskName
translation	I-TaskName
,	O
where	O
a	O
minimum	O
-	O
loss	B-MetricName
predictor	O
is	O
assigned	O
to	O
each	O
source	O
input	O
.	O
Shi	O
et	O
al	O
(	O
2018	O
)	O
employed	O
an	O
inverse	O
reinforcement	O
learning	O
approach	O
for	O
unconditional	O
diverse	O
text	B-TaskName
generation	I-TaskName
.	O
However	O
,	O
no	O
existing	O
work	O
considered	O
performing	O
diverse	O
knowledge	O
reasoning	O
to	O
generate	O
multiple	O
reasonable	O
outputs	O
of	O
different	O
contents	O
.	O

Not	O
all	O
concepts	O
in	O
G	O
appear	O
in	O
the	O
outputs	O
.	O
Thus	O
,	O
we	O
design	O
a	O
concept	O
selection	O
module	O
to	O
choose	O
salient	O
concepts	O
that	O
should	O
be	O
considered	O
during	O
generation	O
.	O
For	O
each	O
concept	O
v	O
V	O
x	O
,	O
we	O
calculate	O
its	O
probability	O
of	O
being	O
selected	O
by	O
taking	O
a	O
multilayer	O
perception	O
(	O
MLP	B-DatasetName
)	O
on	O
the	O
top	O
of	O
graph	O
encoder	O
:	O
p	O
v	O
=	O
P	O
r	O
[	O
v	O
is	O
selected	O
|	O
x	O
]	O
=	O
MLP	B-DatasetName
(	O
h	O
L	O
v	O
)	O
.	O
To	O
supervise	O
the	O
concept	O
selection	O
process	O
,	O
we	O
use	O
the	O
overlapping	O
concepts	O
between	O
concepts	O
appearing	O
in	O
the	O
output	O
sequence	O
C	O
y	O
and	O
concepts	O
in	O
input	O
sequence	O
associated	O
subgraph	O
G	O
x	O
,	O
i.e.	O
,	O
V	O
x	O
∩	O
C	O
y	O
,	O
as	O
a	O
simple	O
proxy	O
for	O
the	O
ground	O
-	O
truth	O
supervision	O
.	O
So	O
,	O
the	O
concept	O
selection	O
loss	B-MetricName
(	O
here	O
only	O
for	O
one	O
expert	O
,	O
see	O
MoE	O
loss	B-MetricName
in	O
Eq	O
.	O
(	O
8	O
)	O
)	O
is	O
:	O
L	O
concept	O
=	O
−	O
v	O
Vx∩Cy	O
v	O
log	O
p	O
v	O
(	O
4	O
)	O
+	O
v	O
Vx−Cy	O
(	O
1	O
−	O
v	O
)	O
log	O
(	O
1	O
−	O
p	O
v	O
)	O
.	O
Finally	O
,	O
the	O
top	O
-	O
N	O
ranked	O
concepts	O
on	O
the	O
subgraph	O
G	O
x	O
(	O
denoted	O
as	O
v	O
1	O
,	O
...	O
,	O
v	O
N	O
)	O
are	O
selected	O
as	O
the	O
additional	O
input	O
to	O
the	O
generation	O
process	O
.	O

We	O
utilize	O
a	O
standard	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
as	O
our	O
generation	O
model	O
.	O
It	O
takes	O
the	O
concatenation	O
of	O
the	O
sequence	O
x	O
and	O
all	O
the	O
selected	O
concepts	O
v	O
1	O
,	O
...	O
,	O
v	O
N	O
as	O
input	O
and	O
auto	O
-	O
regressively	O
generates	O
the	O
outputs	O
y.	O
We	O
adopt	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
,	O
which	O
can	O
be	O
written	O
as	O
:	O
L	O
generation	O
=	O
−	O
log	O
p	O
(	O
y	O
|	O
x	O
,	O
v	O
1	O
,	O
,	O
v	O
N	O
)	O
(	O
5	O
)	O
=	O
−	O
|	O
y	O
|	O
t=1	O
log	O
p	O
(	O
y	O
t	O
|	O
x	O
,	O
v	O
1	O
,	O
,	O
v	O
N	O
,	O
y	O
<	O
t	O
)	O
.	O
Note	O
that	O
since	O
the	O
selected	O
concepts	O
do	O
not	O
have	O
a	O
rigorous	O
order	O
,	O
we	O
only	O
apply	O
positional	O
encodings	O
(	O
used	O
in	O
Transformer	B-MethodName
)	O
to	O
the	O
input	O
sequence	O
x.	O

We	O
jointly	O
optimizes	O
the	O
following	O
loss	B-MetricName
:	O
L	O
=	O
L	O
generation	O
+	O
λ	O
L	O
concept	O
.	O
(	O
6	O
)	O
where	O
λ	O
is	O
a	O
hyperparameter	O
to	O
control	O
the	O
importance	O
of	O
different	O
tasks	O
2	O
.	O

To	O
empower	O
the	O
generation	O
model	O
to	O
produce	O
multiple	O
reasonable	O
outputs	O
,	O
we	O
employ	O
a	O
mixture	O
of	O
expert	O
(	O
MoE	O
)	O
module	O
to	O
model	O
uncertainty	O
and	O
generate	O
diverse	O
outputs	O
.	O
While	O
the	O
MoE	O
models	O
have	O
primarily	O
been	O
explored	O
as	O
a	O
means	O
of	O
increasing	O
model	O
capacity	O
,	O
they	O
are	O
also	O
being	O
used	O
to	O
boost	O
diverse	O
generation	O
process	O
(	O
Shen	O
et	O
al	O
,	O
2019	O
;	O
Cho	O
et	O
al	O
,	O
2019	O
)	O
.	O
Formally	O
,	O
the	O
MoE	O
module	O
introduces	O
a	O
multinomial	O
latent	O
variable	O
z	O
{	O
1	O
,	O
,	O
K	O
}	O
,	O
and	O
decomposes	O
the	O
marginal	O
likelihood	O
as	O
follows	O
:	O
p	O
(	O
y	O
|	O
x	O
,	O
G	O
x	O
)	O
=	O
K	O
z=1	O
p	O
(	O
z	O
|	O
x	O
,	O
G	O
x	O
)	O
p	O
(	O
y	O
|	O
z	O
,	O
x	O
,	O
G	O
x	O
)	O
.	O
(	O
7	O
)	O
Training	O
.	O
We	O
minimize	O
the	O
loss	B-MetricName
function	O
(	O
in	O
Eq	O
.	O
(	O
6	O
)	O
)	O
using	O
the	O
MoE	O
decomposition	O
,	O
∇	O
log	O
p	O
(	O
y	O
|	O
x	O
,	O
G	O
x	O
)	O
(	O
8	O
)	O
=	O
K	O
z=1	O
p	O
(	O
z	O
|	O
x	O
,	O
y	O
,	O
G	O
x	O
)	O
∇	O
log	O
p	O
(	O
y	O
,	O
z	O
|	O
x	O
,	O
G	O
x	O
)	O
,	O
and	O
train	O
the	O
model	O
with	O
the	O
EM	B-MetricName
algorithm	O
(	O
Dempster	O
et	O
al	O
,	O
1977	O
)	O
.	O
Ideally	O
,	O
we	O
would	O
like	O
different	O
experts	O
to	O
specialize	O
in	O
different	O
reasoning	O
abilities	O
so	O
that	O
they	O
can	O
generate	O
diverse	O
outputs	O
.	O
The	O
specialization	O
of	O
experts	O
means	O
that	O
given	O
the	O
input	O
,	O
only	O
one	O
element	O
in	O
{	O
p	O
(	O
y	O
,	O
z	O
|	O
x	O
,	O
G	O
x	O
)	O
}	O
K	O
z=1	O
should	O
dominate	O
in	O
value	O
(	O
Shen	O
et	O
al	O
,	O
2019	O
)	O
.	O
To	O
encourage	O
this	O
,	O
we	O
employ	O
a	O
hard	O
mixture	O
model	O
to	O
maximize	O
max	O
z	O
p	O
(	O
y	O
,	O
z	O
|	O
x	O
,	O
G	O
x	O
)	O
by	O
assigning	O
full	O
responsibility	O
to	O
the	O
expert	O
with	O
the	O
largest	O
joint	O
probability	O
.	O
Training	O
proceeds	O
via	O
hard	O
-	O
EM	B-MetricName
can	O
be	O
written	O
as	O
:	O
E	O
-	O
step	O
:	O
estimate	O
the	O
responsibilities	O
of	O
each	O
expert	O
r	O
z	O
1	O
[	O
z	O
=	O
arg	O
max	O
z	O
p	O
(	O
y	O
,	O
z	O
|	O
x	O
,	O
G	O
x	O
)	O
]	O
using	O
the	O
current	O
parameters	O
θ	B-HyperparameterName
;	O
M	O
-	O
step	O
:	O
update	O
the	O
parameters	O
with	O
gradients	O
of	O
the	O
chosen	O
expert	O
(	O
r	O
z	O
=	O
1	O
)	O
from	O
E	O
-	O
step	O
.	O
Expert	O
parameterization	O
.	O
Independently	O
parameterizing	O
each	O
expert	O
may	O
exacerbate	O
overfitting	O
since	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
increases	O
linearly	O
with	O
the	O
number	O
of	O
experts	O
(	O
Shen	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
follow	O
the	O
parameter	O
sharing	O
schema	O
in	O
Cho	O
et	O
al	O
(	O
2019	O
)	O
;	O
Shen	O
et	O
al	O
(	O
2019	O
)	O
to	O
avoid	O
this	O
issue	O
.	O
This	O
only	O
requires	O
a	O
negligible	O
increase	O
in	O
parameters	O
over	O
the	O
baseline	O
model	O
that	O
does	O
not	O
uses	O
MoE.	O
In	O
our	O
experiments	O
,	O
we	O
compared	O
adding	O
a	O
unique	O
expert	O
embedding	O
to	O
each	O
input	O
token	O
with	O
adding	O
an	O
expert	O
prefix	O
token	O
before	O
the	O
input	O
text	O
sequence	O
,	O
where	O
they	O
achieved	O
very	O
similar	O
performance	O
.	O
Producing	O
K	O
outputs	O
during	O
inference	O
.	O
In	O
order	O
to	O
generate	O
K	O
different	O
outputs	O
on	O
test	O
set	O
,	O
we	O
follow	O
Shen	O
et	O
al	O
(	O
2019	O
)	O
to	O
enumerate	O
all	O
latent	O
variables	O
z	O
and	O
then	O
greedily	O
decoding	O
each	O
token	O
byŷ	O
t	O
=	O
arg	O
max	O
p	O
(	O
y	O
|	O
ŷ	O
1	O
:	O
t−1	O
,	O
z	O
,	O
x	O
)	O
.	O
In	O
other	O
words	O
,	O
we	O
ask	O
each	O
expert	O
to	O
seek	O
different	O
sets	O
of	O
concepts	O
on	O
the	O
knowledge	O
graph	O
,	O
and	O
use	O
the	O
selected	O
concepts	O
to	O
generate	O
K	O
different	O
outputs	O
.	O
Notably	O
,	O
this	O
decoding	O
procedure	O
is	O
efficient	O
and	O
easily	O
parallelizable	O
.	O
Furthermore	O
,	O
to	O
make	O
fair	O
comparisons	O
with	O
sampling	O
-	O
based	O
methods	O
,	O
we	O
use	O
greedy	O
decoding	O
without	O
any	O
sampling	O
strategy	O
.	O

All	O
baseline	O
methods	O
were	O
built	O
on	O
the	O
Transformer	B-MethodName
architecture	O
with	O
6	O
-	O
layer	O
encoder	O
and	O
decoder	O
,	O
and	O
initialized	O
with	O
pre	O
-	O
trained	O
parameters	O
from	O
BARTbase	O
(	O
Lewis	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
is	O
one	O
of	O
the	O
stateof	O
-	O
the	O
-	O
art	O
pre	O
-	O
trained	O
Transformer	B-MethodName
models	O
for	O
natural	O
language	O
generation	O
(	O
Gehrmann	O
et	O
al	O
,	O
2021	O
)	O
.	O
In	O
our	O
MoKGE	O
,	O
the	O
Transformer	B-MethodName
parameters	O
were	O
also	O
initialized	O
by	O
BART	B-MethodName
-	O
base	O
,	O
in	O
order	O
to	O
make	O
fair	O
comparison	O
with	O
all	O
baseline	O
methods	O
.	O
The	O
R	O
-	O
GCN	B-MethodName
parameters	O
were	O
random	O
initialized	O
.	O
For	O
model	O
training	O
,	O
we	O
used	O
Adam	B-MethodName
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
60	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e	O
-	O
5	O
,	O
L2	O
weight	B-MethodName
decay	I-MethodName
of	O
0.01	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
warm	O
up	O
over	O
the	O
first	O
10	O
,	O
000	O
steps	O
,	O
and	O
linear	O
decay	O
of	O
learning	B-HyperparameterName
rate	I-HyperparameterName
.	O
Our	O
models	O
were	O
trained	O
by	O
one	O
Tesla	O
V100	O
GPU	O
card	O
with	O
32	O
GB	O
memory	O
,	O
and	O
implemented	O
on	O
PyTorch	O
with	O
the	O
Huggingface	O
's	O
Transformer	B-MethodName
(	O
Wolf	O
et	O
al	O
,	O
2020	O
)	O
.	O
All	O
Transformer	B-MethodName
-	O
based	O
methods	O
were	O
trained	O
with	O
30	O
epochs	O
,	O
taken	O
about	O
4	O
-	O
5	O
hours	O
on	O
the	O
ComVE	O
dataset	O
and	O
7	O
-	O
9	O
hours	O
on	O
the	O
α	B-HyperparameterName
-	O
NLG	O
dataset	O
.	O
In	O
addition	O
to	O
our	O
MoKGE	O
implementation	O
,	O
we	O
also	O
provide	O
the	O
baseline	O
implementation	O
code	O
on	O
GitHub	O
https://github.com/DM2	O
-	O
ND	O
/	O
MoKGE	O
.	O

We	O
evaluated	O
the	O
performance	O
of	O
different	O
generation	O
models	O
from	O
two	O
aspects	O
:	O
quality	O
(	O
or	O
say	O
accuracy	B-MetricName
)	O
and	O
diversity	O
.	O
Quality	O
tests	O
the	O
appropriateness	O
of	O
the	O
generated	O
response	O
with	O
respect	O
to	O
the	O
context	O
,	O
and	O
diversity	O
tests	O
the	O
lexical	O
and	O
semantic	O
diversity	O
of	O
the	O
appropriate	O
sequences	O
generated	O
by	O
the	O
model	O
.	O
These	O
evaluation	O
metrics	O
have	O
been	O
widely	O
used	O
in	O
existing	O
work	O
(	O
Ott	O
et	O
al	O
,	O
2018	O
;	O
Vijayakumar	O
et	O
al	O
,	O
2018	O
;	O
Cho	O
et	O
al	O
,	O
2019	O
;	O
.	O

The	O
quality	O
is	O
measured	O
by	O
standard	O
N	O
-	O
gram	O
based	O
metrics	O
,	O
including	O
the	O
BLEU	B-MetricName
score	I-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
and	O
the	O
ROUGE	O
score	O
(	O
Lin	O
,	O
2004	O
)	O
.	O
This	O
measures	O
the	O
highest	O
accuracy	B-MetricName
comparing	O
the	O
best	O
hypothesis	O
among	O
the	O
top	O
-	O
K	O
with	O
the	O
target	O
(	O
Vijayakumar	O
et	O
al	O
,	O
2018	O
)	O
.	O
Concretely	O
,	O
we	O
generate	O
hypotheses	O
{	O
Ŷ	O
(	O
1	O
)	O
,	O
Ŷ	O
(	O
K	O
)	O
}	O
from	O
each	O
source	O
X	O
and	O
keep	O
the	O
hypothesisŶ	O
best	O
that	O
achieves	O
the	O
best	O
sentencelevel	O
metric	O
with	O
the	O
target	O
Y	O
.	O
Then	O
we	O
calculate	O
a	O
corpus	O
-	O
level	O
metric	O
with	O
the	O
greedily	O
-	O
selected	O
hypotheses	O
{	O
Y	O
(	O
i	O
)	O
,	O
best	O
}	O
N	O
i=1	O
and	O
references	O
{	O
Y	O
(	O
i	O
)	O
}	O
N	O
i=1	O
.	O
The	O
diversity	O
of	O
evaluated	O
by	O
three	O
aspects	O
:	O
concept	O
,	O
pairwise	O
and	O
corpus	O
diversity	O
.	O
Concept	O
diversity	O
.	O
The	O
number	O
of	O
unique	O
concepts	O
(	O
short	O
as	O
Uni	O
.	O
C	O
)	O
measures	O
how	O
many	O
unique	O
concepts	O
on	O
the	O
commonsense	O
KG	O
are	O
covered	O
in	O
the	O
generated	O
outputs	O
.	O
A	O
higher	O
value	O
indicates	O
the	O
higher	O
concept	O
diversity	O
.	O
Besides	O
,	O
we	O
also	O
measure	O
the	O
pairwise	O
concept	O
diversity	O
by	O
using	O
Jaccard	O
similarity	O
.	O
It	O
is	O
defined	O
as	O
the	O
size	O
of	O
the	O
intersection	O
divided	O
by	O
the	O
size	O
of	O
the	O
union	O
of	O
two	O
sets	O
.	O
Lower	O
value	O
indicates	O
the	O
higher	O
concept	O
diversity	O
.	O
Pairwise	O
diversity	O
(	O
⇓	O
)	O
.	O
Referred	O
as	O
"	O
self	O
-	O
"	O
(	O
e.g.	O
,	O
self	O
-	O
BLEU	B-MetricName
)	O
,	O
it	O
measures	O
the	O
within	O
-	O
distribution	O
similarity	O
.	O
This	O
metric	O
computes	O
the	O
average	O
of	O
sentence	O
-	O
level	O
metrics	O
between	O
all	O
pairwise	O
combinations	O
of	O
hypotheses	O
{	O
Y	O
(	O
1	O
)	O
,	O
,	O
Y	O
(	O
K	O
)	O
}	O
generated	O
from	O
each	O
source	O
sequence	O
X.	O
Lower	O
pairwise	O
metric	O
indicates	O
high	O
diversity	O
between	O
generated	O
hypotheses	O
.	O

Comparison	O
with	O
baseline	O
methods	O
.	O
We	O
evaluated	O
our	O
proposed	O
MoKGE	O
and	O
baseline	O
methods	O
based	O
on	O
both	O
quality	O
and	O
diversity	O
.	O
As	O
shown	O
in	O
Table	O
2	O
,	O
MoE	O
-	O
based	O
methods	O
achieved	O
the	O
best	O
performance	O
among	O
all	O
baseline	O
methods	O
.	O
MoKGE	O
can	O
further	O
boost	O
diversity	O
by	O
at	O
least	O
1.57	O
%	O
and	O
1.83	O
%	O
on	O
Self	O
-	O
BLEU	B-MetricName
-	O
3	O
and	O
Self	O
-	O
BLEU	B-MetricName
-	O
4	O
,	O
compared	O
with	O
the	O
vanilla	O
MoE	O
methods	O
.	O
At	O
the	O
same	O
time	O
,	O
MoKGE	O
achieved	O
on	O
par	O
performance	O
with	O
other	O
baseline	O
methods	O
based	O
on	O
the	O
quality	O
evaluation	O
.	O
Specifically	O
,	O
on	O
the	O
ComVE	O
dataset	O
,	O
MoKGE	O
achieved	O
the	O
best	O
performance	O
on	O
BLEU	B-MetricName
-	O
4	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
,	O
and	O
on	O
the	O
α	B-HyperparameterName
-	O
NLG	O
dataset	O
,	O
the	O
perfor	O
-	O
mance	O
gap	O
between	O
MoKGE	O
and	O
the	O
best	O
baseline	O
method	O
was	O
always	O
less	O
than	O
0.5	O
%	O
on	O
BLEU	B-MetricName
-	O
4	O
.	O
Ablation	O
study	O
.	O
We	O
conducted	O
an	O
ablation	O
study	O
to	O
analyze	O
the	O
two	O
major	O
components	O
in	O
the	O
MoKGE	O
.	O
The	O
experimental	O
results	O
are	O
shown	O
in	O
Table	O
3	O
.	O
First	O
,	O
we	O
note	O
that	O
when	O
not	O
using	O
MoE	O
(	O
line	O
-	O
w/o	O
MoE	O
)	O
,	O
we	O
used	O
the	O
most	O
basic	O
decoding	O
strategy	O
-	O
beam	O
search	O
-	O
to	O
generate	O
multiple	O
outputs	O
.	O
We	O
observed	O
that	O
the	O
outputs	O
generated	O
by	O
beam	O
search	O
differed	O
only	O
on	O
punctuation	O
and	O
minor	O
morphological	O
variations	O
,	O
and	O
typically	O
only	O
the	O
last	O
few	O
words	O
were	O
different	O
from	O
others	O
.	O
Besides	O
,	O
integrating	O
commonsense	O
knowledge	O
graph	O
into	O
the	O
MoEbased	O
generation	O
model	O
brought	O
both	O
quality	O
and	O
diversity	O
improvement	O
on	O
the	O
ComVE	O
,	O
but	O
might	O
sacrifice	O
a	O
little	O
quality	O
(	O
less	O
than	O
0.5	O
%	O
on	O
BLEU	B-MetricName
-	O
4	O
)	O
on	O
the	O
α	B-HyperparameterName
-	O
NLG	O
dataset	O
.	O
Overall	O
,	O
our	O
MoKGE	O
benefited	O
from	O
KG	O
and	O
MoE	O
modules	O
,	O
and	O
achieved	O
great	O
performance	O
on	O
both	O
diversity	O
and	O
quality	O
.	O

Automatic	O
diversity	O
evaluation	O
(	O
e.g.	O
,	O
Self	O
-	O
BLEU	B-MetricName
,	O
Distinct	O
-	O
k	O
)	O
can	O
not	O
reflect	O
the	O
content	O
-	O
level	O
diversity	O
.	O
Therefore	O
,	O
we	O
conducted	O
extensive	O
human	O
evaluations	O
to	O
assess	O
both	O
the	O
quality	O
and	O
diversity	O
of	O
outputs	O
generated	O
from	O
different	O
models	O
.	O
The	O
human	O
evaluation	O
was	O
divided	O
into	O
two	O
parts	O
:	O
independent	O
scoring	O
and	O
pairwise	O
comparisons	O
.	O
All	O
evaluations	O
were	O
conducted	O
on	O
Amazon	O
Mechanical	O
Turk	O
(	O
AMT	O
)	O
,	O
and	O
each	O
evaluation	O
form	O
was	O
answered	O
by	O
at	O
least	O
three	O
AMT	O
workers	O
.	O
Independent	O
scoring	O
.	O
In	O
this	O
part	O
,	O
human	O
annotators	O
were	O
asked	O
to	O
evaluate	O
the	O
generated	O
outputs	O
from	O
a	O
single	O
model	O
.	O
We	O
first	O
presented	O
top	O
-	O
3	O
generated	O
outputs	O
from	O
a	O
certain	O
model	O
to	O
human	O
annotators	O
.	O
The	O
annotators	O
would	O
first	O
evaluate	O
the	O
diversity	O
by	O
answering	O
"	O
How	O
many	O
different	O
meanings	O
do	O
three	O
outputs	O
express	O
?	O
"	O
Then	O
we	O
presented	O
human	O
-	O
written	O
outputs	O
to	O
the	O
annotators	O
.	O
The	O
annotator	O
would	O
evaluate	O
the	O
quality	O
by	O
comparing	O
machine	O
generated	O
outputs	O
and	O
human	O
-	O
written	O
outputs	O
,	O
and	O
answering	O
"	O
How	O
many	O
machine	O
generated	O
out	O
-	O
puts	O
are	O
correct	O
?	O
"	O
The	O
diversity	O
and	O
quality	O
scores	O
are	O
normalized	O
to	O
the	O
range	O
from	O
0	B-DatasetName
to	O
3	O
.	O
Besides	O
,	O
the	O
annotators	O
need	O
to	O
give	O
a	O
fluency	O
and	O
grammar	O
score	O
from	O
1	O
to	O
4	O
for	O
each	O
generated	O
output	O
.	O
Pairwise	O
comparisons	O
.	O
In	O
this	O
part	O
,	O
the	O
annotators	O
were	O
given	O
two	O
sets	O
of	O
top	O
-	O
3	O
generated	O
explanations	O
from	O
two	O
different	O
methods	O
each	O
time	O
and	O
instructed	O
to	O
pick	O
the	O
more	O
diverse	O
set	O
.	O
The	O
choices	O
are	O
"	O
win	O
,	O
"	O
"	O
lose	O
,	O
"	O
or	O
"	O
tie	O
.	O
"	O
As	O
shown	O
in	O
Table	O
4	O
-	O
5	O
,	O
our	O
MoKGE	O
can	O
significantly	O
outperform	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
samplingbased	O
methods	O
in	O
diversity	O
evaluation	O
(	O
p	O
-	O
value	O
<	O
0.05	O
under	O
paired	O
t	O
-	O
test	O
)	O
,	O
even	O
slightly	O
better	O
than	O
human	O
performance	O
on	O
the	O
ComVE	O
task	O
.	O
At	O
the	O
same	O
time	O
,	O
we	O
can	O
observe	O
MoKGE	O
is	O
able	O
to	O
obtain	O
on	O
par	O
performance	O
with	O
other	O
methods	O
based	O
on	O
quality	O
evaluation	O
.	O
The	O
p	O
-	O
value	O
is	O
not	O
smaller	O
than	O
0.05	O
(	O
i.e.	O
,	O
not	O
significant	O
difference	O
)	O
under	O
paired	O
t	O
-	O
test	O
between	O
MoKGE	O
and	O
baseline	O
methods	O
based	O
on	O
the	O
quality	O
evaluation	O
.	O
(	O
1	O
)	O
Billy	O
's	O
parents	O
took	O
him	O
to	O
the	O
zoo	O
as	O
a	O
reward	O
.	O

[	O
4	O
]	O
[	O
4	O
]	O
[	O
1	O
]	O
[	O
1	O
]	O
[	O
4	O
]	O
[	O
3	O
]	O
[	O
4	O
]	O
[	O
4	O
]	O
[	O
1	O
]	O
[	O
1	O
]	O
[	O
1	O
]	O
[	O
2	O
]	O
[	O
1	O
]	O
ComVE	O
-	O
-	O
Input	O
:	O
Cars	O
are	O
made	O
of	O
fuel	O
.	O
Goal	O
(	O
explanation	O
for	O
sense	O
-	O
making	O
)	O
:	O
[	O
]	O
.	O
(	O
1	O
)	O
Cars	O
are	O
not	O
made	O
of	O
fuel	O
.	O
(	O
2	O
)	O
Cars	O
burn	O
fuel	O
to	O
produce	O
energy	O
and	O
work	O
.	O
(	O
3	O
)	O
Fuel	O
is	O
a	O
liquid	O
which	O
can	O
not	O
make	O
cars	O
.	O
meanings	O
,	O
e.g.	O
,	O
"	O
go	O
to	O
the	O
zoo	O
and	O
see	O
elephants	O
"	O
and	O
"	O
took	O
him	O
to	O
the	O
zoo	O
and	O
see	O
elephants	O
"	O
in	O
the	O
α	B-HyperparameterName
-	O
NLG	O
case	O
.	O
On	O
the	O
contrary	O
,	O
MoKGE	O
can	O
generate	O
semantically	O
richer	O
and	O
more	O
diverse	O
contents	O
than	O
the	O
other	O
two	O
methods	O
by	O
incorporating	O
more	O
commonsense	O
concepts	O
on	O
the	O
knowledge	O
graph	O
.	O

Improving	O
content	O
diversity	O
in	O
NLG	O
.	O
Most	O
of	O
the	O
existing	O
diversity	O
-	O
promoting	O
work	O
has	O
focused	O
on	O
improving	O
syntactic	O
and	O
lexical	O
diversity	O
,	O
such	O
as	O
different	O
language	O
style	O
in	O
machine	B-TaskName
translation	I-TaskName
(	O
Shen	O
et	O
al	O
,	O
2019	O
)	O
and	O
word	O
variability	O
in	O
paraphrase	B-TaskName
generation	I-TaskName
(	O
Gupta	O
et	O
al	O
,	O
2018	O
)	O
.	O
Nevertheless	O
,	O
methods	O
for	O
improving	O
content	O
diversity	O
in	O
NLG	O
systems	O
have	O
been	O
rarely	O
studied	O
in	O
the	O
existing	O
literature	O
.	O
We	O
believe	O
that	O
generating	O
diverse	O
content	O
is	O
one	O
of	O
the	O
most	O
promising	O
aspects	O
of	O
machine	O
intelligence	O
,	O
which	O
can	O
be	O
applied	O
to	O
a	O
wide	O
range	O
of	O
real	O
-	O
world	O
applications	O
,	O
not	O
only	O
limited	O
to	O
commonsense	O
reasoning	O
.	O
Besides	O
,	O
leveraging	O
knowledge	O
graph	O
is	O
not	O
the	O
only	O
way	O
to	O
promote	O
content	O
diversity	O
as	O
it	O
is	O
a	O
highly	O
knowledge	O
-	O
intensive	O
task	O
.	O
Many	O
existing	O
knowledge	O
-	O
enhanced	O
methods	O
(	O
Yu	O
et	O
al	O
,	O
2022c	O
)	O
can	O
be	O
used	O
to	O
acquire	O
different	O
external	O
knowledge	O
for	O
producing	O
diverse	O
outputs	O
,	O
e.g.	O
,	O
taking	O
different	O
retrieved	O
documents	O
as	O
conditions	O
for	O
generator	O
.	O
Designing	O
neural	O
diversity	O
metrics	O
.	O
In	O
spite	O
of	O
growing	O
interest	O
in	O
NLG	O
models	O
that	O
produce	O
diverse	O
outputs	O
,	O
there	O
is	O
currently	O
no	O
principled	O
neu	O
-	O
ral	O
method	O
for	O
evaluating	O
the	O
diversity	O
of	O
an	O
NLG	O
system	O
.	O
As	O
described	O
in	O
Tevet	O
and	O
Berant	O
(	O
2021	O
)	O
,	O
existing	O
automatic	O
diversity	O
metrics	O
(	O
e.g.	O
Self	O
-	O
BLEU	B-MetricName
)	O
perform	O
worse	O
than	O
humans	O
on	O
the	O
task	O
of	O
estimating	O
content	O
diversity	O
,	O
indicating	O
a	O
low	O
correlation	O
between	O
metrics	O
and	O
human	O
judgments	O
.	O
Therefore	O
,	O
neural	O
-	O
based	O
diversity	O
metrics	O
are	O
highly	O
demanded	O
.	O
Intuitively	O
,	O
the	O
metrics	O
should	O
include	O
computational	O
comparisons	O
of	O
multiple	O
references	O
and	O
hypotheses	O
by	O
projecting	O
them	O
into	O
the	O
same	O
semantic	O
space	O
,	O
unlike	O
metrics	O
for	O
evaluating	O
the	O
generation	O
quality	O
,	O
e.g.	O
,	O
BERTScore	O
(	O
Zhang	O
et	O
al	O
,	O
2020b	O
)	O
and	O
BLEURT	O
(	O
Sellam	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
only	O
measures	O
the	O
correlation	O
between	O
a	O
pair	O
of	O
reference	O
and	O
hypothesis	O
.	O

Multiple	O
-	O
choice	O
questions	O
(	O
MCQs	O
)	O
are	O
widely	O
used	O
for	O
student	O
assessments	O
,	O
from	O
high	O
-	O
stakes	O
graduation	O
tests	O
to	O
lower	O
-	O
stakes	O
reading	B-TaskName
comprehension	I-TaskName
tests	O
.	O
An	O
MCQ	O
consists	O
of	O
a	O
question	O
(	O
stem	O
)	O
,	O
the	O
correct	O
answer	O
(	O
key	O
)	O
and	O
a	O
number	O
of	O
wrong	O
,	O
but	O
plausible	O
options	O
(	O
distractors	O
)	O
.	O
The	O
problem	O
of	O
automatically	O
generating	O
stems	O
with	O
a	O
key	O
has	O
received	O
a	O
great	O
deal	O
of	O
attention	O
,	O
e.g.	O
,	O
see	O
the	O
survey	O
by	O
Amidei	O
et	O
al	O
(	O
2018	O
)	O
.	O
By	O
comparison	O
,	O
automatically	O
generating	O
distractors	O
is	O
substantially	O
less	O
researched	O
,	O
although	O
Welbl	O
et	O
al	O
(	O
2017	O
)	O
report	O
that	O
manually	O
finding	O
reasonable	O
distractors	O
was	O
the	O
most	O
time	O
-	O
consuming	O
part	O
in	O
writing	O
science	O
MCQs	O
.	O
Indeed	O
,	O
reasonable	O
distractors	O
should	O
be	O
grammatically	O
consistent	O
and	O
similar	O
in	O
length	O
compared	O
to	O
the	O
key	O
and	O
within	O
themselves	O
.	O
Given	O
the	O
challenges	O
above	O
,	O
we	O
attempt	O
using	O
machine	O
learning	O
(	O
ML	O
)	O
to	O
aid	O
teachers	O
in	O
creating	O
distractors	O
for	O
reading	B-TaskName
comprehension	I-TaskName
MCQs	O
.	O
The	O
problem	O
is	O
not	O
new	O
,	O
however	O
most	O
of	O
the	O
prior	O
work	O
has	O
been	O
done	O
for	O
English	O
.	O
In	O
this	O
paper	O
we	O
propose	O
the	O
first	O
such	O
solution	O
for	O
Swedish	O
(	O
although	O
the	O
proposed	O
method	O
is	O
novel	O
even	O
for	O
English	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
)	O
.	O
The	O
key	O
contributions	O
of	O
this	O
work	O
are	O
:	O
proposing	O
a	O
BERT	B-MethodName
-	O
based	O
method	O
for	O
generating	O
distractors	O
using	O
only	O
a	O
small	O
-	O
scale	O
dataset	O
,	O
releasing	O
SweQUAD	O
-	O
MC	O
1	O
,	O
a	O
dataset	O
of	O
Swedish	O
MCQs	O
,	O
and	O
proposing	O
a	O
methodology	O
for	O
conducting	O
human	O
evaluation	O
aimed	O
at	O
assessing	O
the	O
plausibility	O
of	O
distractors	O
.	O
2	O
Background	O
2.1	O
BERT	B-MethodName
for	O
NLG	O
Devlin	O
et	O
al	O
(	O
2019	O
)	O
introduced	O
BERT	B-MethodName
as	O
the	O
first	O
application	O
of	O
the	O
Transformer	B-MethodName
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
to	O
language	B-TaskName
modelling	I-TaskName
.	O
BERT	B-MethodName
uses	O
only	O
Transformer	B-MethodName
's	O
encoder	O
stacks	O
(	O
with	O
multihead	O
self	O
-	O
attention	O
,	O
MHSA	O
)	O
,	O
while	O
the	O
NLG	O
community	O
relies	O
more	O
on	O
Transformer	B-MethodName
's	O
decoder	O
stacks	O
(	O
with	O
masked	O
MHSA	O
)	O
for	O
text	B-TaskName
generation	I-TaskName
,	O
e.g.	O
,	O
GPT	B-MethodName
(	O
Radford	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
Wang	O
and	O
Cho	O
(	O
2019	O
)	O
showed	O
that	O
BERT	B-MethodName
is	O
a	O
Markov	O
random	O
field	O
,	O
meaning	O
that	O
BERT	B-MethodName
learns	O
a	O
joint	O
probability	O
distribution	O
over	O
all	O
sentences	O
of	O
a	O
fixed	O
length	O
,	O
and	O
one	O
could	O
use	O
Gibbs	O
sampling	O
to	O
generate	O
a	O
new	O
sentence	O
.	O
The	O
authors	O
compared	O
samples	O
generated	O
autoregressively	O
left	O
-	O
to	O
-	O
right	O
by	O
BERT	B-MethodName
and	O
GPT	B-MethodName
,	O
and	O
found	O
the	O
perplexity	B-MetricName
of	O
BERT	B-MethodName
samples	O
to	O
be	O
higher	O
than	O
GPT	B-MethodName
's	O
(	O
BERT	B-MethodName
samples	O
are	O
of	O
worse	O
quality	O
)	O
,	O
but	O
the	O
n	O
-	O
gram	O
overlap	O
between	O
the	O
generated	O
texts	O
and	O
texts	O
from	O
the	O
dataset	O
to	O
be	O
lower	O
(	O
BERT	B-MethodName
samples	O
are	O
more	O
diverse	O
)	O
.	O
Liao	O
et	O
al	O
(	O
2020	O
)	O
show	O
a	O
way	O
to	O
improve	O
BERT	B-MethodName
's	O
generation	O
capabilities	O
via	O
changing	O
the	O
masking	O
scheme	O
to	O
a	O
probabilistic	O
one	O
at	O
training	O
time	O
.	O
Probabilistically	O
masked	O
language	O
models	O
(	O
PMLMs	O
)	O
assume	O
that	O
the	O
masking	O
ratio	O
r	O
for	O
each	O
sentence	O
is	O
drawn	O
from	O
a	O
prior	O
distribution	O
p	O
(	O
r	O
)	O
.	O
The	O
au	O
-	O
2.1	O
±	O
0.5	O
2.1	O
±	O
0.4	O
2.0	O
±	O
0.2	O
Len	O
(	O
Text	O
)	O
384.9	O
±	O
330.1	O
355.1	O
±	O
233.1	O
357.9	O
±	O
254.3	O
Len	O
(	O
A	O
)	O
4.2	O
±	O
3.4	O
4.4	O
±	O
3.5	O
4.6	O
±	O
4.5	O
Len	O
(	O
D	O
)	O
4.5	O
±	O
3.9	O
4.3	O
±	O
4.0	O
4.0	O
±	O
3.7	O
|	O
Len	O
(	O
A	O
)	O
-	O
Len	O
(	O
D	O
)	O
|	O
1.9	O
±	O
2.4	O
1.9	O
±	O
2.3	O
1.9	O
±	O
2.9	O
Table	O
1	O
:	O
Descriptive	O
statistics	O
of	O
SweQUAD	O
-	O
MC	O
dataset	O
splits	O
.	O
A	O
denotes	O
the	O
key	O
,	O
D	O
denotes	O
a	O
distractor	O
,	O
Len	O
(	O
X	O
)	O
denotes	O
a	O
length	O
of	O
X	O
in	O
words	O
.	O
x	O
±	O
y	O
shows	O
mean	O
x	O
and	O
a	O
standard	O
deviation	O
y	O
thors	O
proposed	O
to	O
train	O
a	O
PMLM	B-MethodName
with	O
a	O
uniform	O
prior	O
(	O
referred	O
to	O
as	O
u	O
-	O
PMLM	B-MethodName
)	O
.	O
The	O
absence	O
of	O
the	O
left	O
-	O
to	O
-	O
right	O
restriction	O
allows	O
the	O
model	O
to	O
generate	O
sequences	O
in	O
an	O
word	O
arbitrary	O
order	O
.	O
In	O
fact	O
,	O
Liao	O
et	O
al	O
(	O
2020	O
)	O
propose	O
to	O
generate	O
sentences	O
by	O
randomly	O
selecting	O
the	O
masked	O
position	O
,	O
predicting	O
a	O
token	O
for	O
it	O
,	O
replacing	O
the	O
masked	O
token	O
with	O
the	O
predicted	O
one	O
and	O
repeating	O
the	O
process	O
until	O
no	O
masked	O
tokens	O
are	O
left	O
.	O
The	O
authors	O
showed	O
that	O
the	O
perplexity	B-MetricName
of	O
the	O
texts	O
generated	O
by	O
u	O
-	O
PMLM	B-MethodName
is	O
comparable	O
to	O
the	O
ones	O
by	O
GPT	B-MethodName
.	O

Given	O
the	O
small	O
scale	O
of	O
SweQUAD	O
-	O
MC	O
we	O
have	O
decided	O
to	O
fine	O
-	O
tune	O
a	O
pretrained	O
BERT	B-MethodName
2	O
for	O
Swedish	O
(	O
Malmsten	O
et	O
al	O
,	O
2020	O
)	O
on	O
the	O
task	O
of	O
distractor	B-TaskName
generation	I-TaskName
(	O
DG	O
)	O
.	O
For	O
achieving	O
this	O
,	O
we	O
have	O
added	O
on	O
top	O
of	O
BERT	B-MethodName
two	O
linear	O
layers	O
with	O
layer	B-MethodName
normalization	I-MethodName
(	O
Ba	O
et	O
al	O
,	O
2016	O
)	O
in	O
the	O
middle	O
to	O
be	O
trained	O
from	O
scratch	O
(	O
see	O
architecture	O
in	O
Figure	O
1	O
)	O
.	O
The	O
last	O
linear	B-MethodName
layer	I-MethodName
is	O
followed	O
by	O
a	O
softmax	B-MethodName
activation	O
giving	O
probabilities	O
over	O
the	O
tokens	O
in	O
the	O
vocabulary	O
for	O
each	O
position	O
in	O
the	O
text	O
.	O
We	O
trained	O
the	O
model	O
using	O
cross	O
-	O
entropy	O
loss	B-MetricName
only	O
for	O
tokens	O
in	O
masked	O
positions	O
.	O
Recall	B-MetricName
that	O
each	O
MCQ	O
consists	O
of	O
a	O
base	O
text	O
T	O
,	O
the	O
stem	O
Q	O
based	O
on	O
T	O
,	O
the	O
key	O
A	O
and	O
(	O
on	O
average	O
)	O
two	O
distractors	O
D1	O
and	O
D2	O
.	O
The	O
DG	O
problem	O
is	O
then	O
to	O
generate	O
distractors	O
conditioned	O
on	O
the	O
context	O
,	O
consisting	O
of	O
T	O
,	O
Q	O
and	O
A.	O
We	O
provide	O
all	O
context	O
components	O
as	O
input	O
to	O
the	O
BERT	B-MethodName
model	O
,	O
separated	O
from	O
each	O
other	O
by	O
the	O
special	O
separator	O
token	O
[	O
SEP	O
]	O
.	O
Given	O
that	O
BERT	B-MethodName
's	O
maximum	O
input	O
length	O
is	O
512	O
tokens	O
,	O
we	O
trim	O
T	O
to	O
the	O
first	O
384	O
tokens	O
(	O
later	O
referred	O
to	O
as	O
T	O
384	O
)	O
,	O
since	O
that	O
is	O
the	O
average	O
text	O
length	O
of	O
the	O
training	O
set	O
.	O
We	O
have	O
explored	O
two	O
different	O
solution	O
variants	O
of	O
DG	O
.	O
The	O
first	O
variant	O
aims	O
at	O
generating	O
distractors	O
autoregressively	O
,	O
left	O
to	O
right	O
.	O
At	O
generation	O
time	O
,	O
the	O
input	O
to	O
BERT	B-MethodName
consists	O
of	O
a	O
context	O
CTX	O
(	O
T	O
384	O
,	O
Q	O
and	O
A	O
separated	O
by	O
[	O
SEP	O
]	O
token	O
)	O
,	O
a	O
[	O
SEP	O
]	O
token	O
,	O
and	O
a	O
[	O
MASK	O
]	O
token	O
at	O
the	O
end	O
.	O
After	O
a	O
forward	O
pass	O
through	O
BERT	B-MethodName
,	O
the	O
[	O
MASK	O
]	O
token	O
gets	O
replaced	O
by	O
the	O
word	O
with	O
the	O
highest	O
softmax	B-MethodName
score	O
,	O
which	O
becomes	O
the	O
first	O
word	O
of	O
the	O
first	O
distractor	O
(	O
dubbed	O
D11	O
)	O
.	O
The	O
generation	O
of	O
the	O
first	O
distractor	O
continues	O
by	O
appending	O
a	O
[	O
MASK	O
]	O
token	O
after	O
each	O
forward	O
pass	O
until	O
the	O
network	O
generates	O
a	O
separator	O
token	O
[	O
SEP	O
]	O
,	O
which	O
concludes	O
the	O
generation	O
of	O
the	O
first	O
distractor	O
D1	O
.	O
The	O
next	O
distractor	O
D2	O
is	O
generated	O
in	O
the	O
same	O
way	O
,	O
except	O
that	O
the	O
CTX	O
is	O
extended	O
by	O
D1	O
.	O
At	O
training	O
time	O
,	O
we	O
use	O
the	O
same	O
procedure	O
,	O
but	O
with	O
teacher	O
forcing	O
,	O
allowing	O
us	O
to	O
use	O
the	O
correct	O
distractor	O
tokens	O
as	O
targets	O
for	O
the	O
cross	O
-	O
entropy	O
loss	B-MetricName
(	O
see	O
example	O
training	O
datapoints	O
for	O
one	O
MCQ	O
in	O
Table	O
2	O
)	O
.	O
The	O
second	O
variant	O
is	O
inspired	O
by	O
u	O
-	O
PMLM	B-MethodName
,	O
and	O
aims	O
at	O
generating	O
distractors	O
autoregressively	O
,	O
but	O
in	O
an	O
arbitrary	O
word	O
order	O
.	O
At	O
generation	O
time	O
,	O
the	O
input	O
to	O
BERT	B-MethodName
consists	O
of	O
a	O
context	O
CTX	O
,	O
a	O
[	O
SEP	O
]	O
token	O
,	O
and	O
a	O
predefined	O
number	O
of	O
[	O
MASK	O
]	O
tokens	O
(	O
see	O
Section	O
6.1	O
)	O
.	O
The	O
generation	O
proceeds	O
by	O
unmasking	O
the	O
token	O
at	O
the	O
position	O
where	O
the	O
model	O
is	O
most	O
confident	O
.	O
This	O
differs	O
from	O
unmasking	O
a	O
random	O
position	O
,	O
proposed	O
by	O
Liao	O
et	O
al	O
(	O
2020	O
)	O
.	O
The	O
training	O
procedure	O
largely	O
follows	O
a	O
masking	O
scheme	O
employed	O
by	O
u	O
-	O
PMLM	B-MethodName
by	O
drawing	O
the	O
masking	O
ratio	O
from	O
the	O
uniform	O
distribution	O
(	O
see	O
example	O
training	O
datapoints	O
for	O
one	O
MCQ	O
in	O
Table	O
2	O
)	O
.	O
Note	O
that	O
we	O
do	O
not	O
include	O
the	O
[	O
SEP	O
]	O
token	O
when	O
training	O
,	O
since	O
we	O
found	O
that	O
the	O
trained	O
model	O
would	O
constantly	O
generate	O
[	O
SEP	O
]	O
tokens	O
.	O
Each	O
sampled	O
masking	O
ratio	O
r	O
for	O
the	O
u	O
-	O
PMLM	B-MethodName
variant	O
means	O
that	O
each	O
token	O
in	O
the	O
distractors	O
from	O
the	O
dataset	O
has	O
a	O
probability	O
r	O
to	O
be	O
masked	O
.	O
Hence	O
,	O
different	O
r	O
will	O
potentially	O
result	O
in	O
different	O
number	O
of	O
masked	O
tokens	O
and	O
at	O
different	O
positions	O
.	O
The	O
number	O
of	O
times	O
we	O
draw	O
r	O
per	O
distractor	O
DX	O
is	O
proposed	O
to	O
be	O
min	O
(	O
Len	O
(	O
DX	O
)	O
,	O
MAX	O
MASKINGS	O
)	O
.	O

Automatic	O
evaluation	O
metrics	O
,	O
such	O
as	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
,	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
,	O
METEOR	B-DatasetName
(	O
Denkowski	O
and	O
Lavie	O
,	O
2014	O
)	O
,	O
CIDEr	B-MetricName
(	O
Vedantam	O
et	O
al	O
,	O
2015	O
)	O
,	O
became	O
popular	O
in	O
NLG	O
in	O
recent	O
years	O
.	O
Essentially	O
,	O
these	O
metrics	O
rely	O
on	O
comparing	O
word	O
overlap	O
between	O
a	O
generated	O
distractor	O
and	O
a	O
reference	O
one	O
.	O
Such	O
metrics	O
can	O
yield	O
a	O
low	O
score	O
even	O
if	O
the	O
generated	O
distractor	O
is	O
valid	O
but	O
just	O
happens	O
to	O
be	O
different	O
from	O
the	O
reference	O
one	O
,	O
or	O
a	O
high	O
score	O
even	O
though	O
the	O
distractor	O
is	O
ungrammatical	O
but	O
happens	O
to	O
have	O
a	O
high	O
word	O
overlap	O
with	O
the	O
reference	O
one	O
(	O
see	O
the	O
article	O
by	O
Callison	O
-	O
Burch	O
et	O
al	O
(	O
2006	O
)	O
for	O
a	O
further	O
discussion	O
)	O
.	O
Furthermore	O
,	O
they	O
do	O
not	O
take	O
into	O
account	O
how	O
well	O
a	O
generated	O
distractor	O
is	O
aligned	O
with	O
the	O
key	O
grammatically	O
or	O
how	O
challenging	O
the	O
whole	O
group	O
of	O
generated	O
distractors	O
would	O
be	O
.	O
To	O
account	O
for	O
the	O
properties	O
mentioned	O
above	O
,	O
we	O
have	O
experimented	O
with	O
a	O
number	O
of	O
quantitative	O
metrics	O
and	O
propose	O
the	O
following	O
set	O
to	O
be	O
used	O
(	O
the	O
whole	O
list	O
is	O
available	O
in	O
Appendix	O
B	O
)	O
.	O
In	O
the	O
following	O
list	O
MCQ%	O
means	O
"	O
Percentage	O
of	O
MCQ	O
"	O
and	O
DIS	O
means	O
"	O
generated	O
distractor	O
(	O
s	O
)	O
"	O
.	O
The	O
first	O
group	O
consists	O
of	O
metrics	O
1	O
-	O
3	O
.	O
The	O
first	O
two	O
metrics	O
count	O
exact	O
matches	O
between	O
generated	O
and	O
reference	O
distractors	O
.	O
The	O
rationale	O
behind	O
metric	O
3	O
is	O
our	O
assumption	O
that	O
distractors	O
coming	O
from	O
the	O
same	O
text	O
are	O
more	O
challenging	O
.	O
The	O
higher	O
the	O
values	O
of	O
all	O
these	O
metrics	O
are	O
,	O
the	O
better	O
.	O
The	O
second	O
group	O
contains	O
metrics	O
4	O
-	O
8	O
,	O
which	O
give	O
an	O
idea	O
of	O
how	O
challenging	O
the	O
whole	O
group	O
of	O
distractors	O
would	O
be	O
.	O
For	O
instance	O
,	O
duplicate	O
distractors	O
or	O
ones	O
with	O
word	O
repetitions	O
could	O
be	O
excluded	O
by	O
students	O
using	O
common	O
sense	O
.	O
The	O
lower	O
the	O
metrics	O
in	O
this	O
group	O
are	O
,	O
the	O
better	O
.	O
The	O
third	O
group	O
consists	O
only	O
of	O
metric	O
9	O
,	O
serving	O
as	O
an	O
overfitting	O
indicator	O
.	O
The	O
metric	O
accounts	O
for	O
the	O
distractors	O
appearing	O
as	O
distractors	O
in	O
training	O
data	O
and	O
high	O
percentage	O
indicates	O
an	O
overfitting	O
possibility	O
.	O
The	O
lower	O
the	O
values	O
,	O
the	O
better	O
.	O
The	O
final	O
group	O
(	O
item	O
10	O
)	O
measures	O
how	O
syntactically	O
aligned	O
generated	O
distractors	O
and	O
the	O
respective	O
keys	O
are	O
.	O
We	O
employ	O
NCPTK	O
to	O
measure	O
the	O
similarity	O
of	O
syntactic	O
structures	O
between	O
each	O
distractor	O
and	O
the	O
respective	O
key	O
.	O
Then	O
we	O
take	O
mean	O
,	O
median	O
and	O
mode	O
of	O
the	O
sequence	O
of	O
NCPTKs	O
obtained	O
in	O
the	O
previous	O
step	O
.	O
The	O
higher	O
the	O
values	O
of	O
these	O
metrics	O
are	O
,	O
the	O
better	O
.	O
Based	O
on	O
these	O
metrics	O
,	O
we	O
performed	O
a	O
model	B-TaskName
selection	I-TaskName
on	O
the	O
development	O
set	O
and	O
chose	O
the	O
models	O
performing	O
best	O
on	O
the	O
most	O
of	O
these	O
metrics	O
.	O
Left	O
-	O
to	O
-	O
right	O
model	O
generated	O
distractors	O
token	O
by	O
token	O
until	O
either	O
a	O
[	O
SEP	O
]	O
token	O
was	O
generated	O
or	O
the	O
length	O
of	O
the	O
distractor	O
was	O
20	O
tokens	O
.	O

MCQs	O
is	O
that	O
the	O
students	O
should	O
be	O
unable	O
to	O
answer	O
them	O
correctly	O
without	O
reading	O
the	O
actual	O
text	O
.	O
To	O
put	O
more	O
formally	O
,	O
the	O
average	O
number	O
of	O
correctly	O
answered	O
MCQs	O
without	O
reading	O
the	O
actual	O
text	O
(	O
denoted	O
N	O
s	O
)	O
should	O
not	O
differ	O
significantly	O
from	O
the	O
average	O
number	O
of	O
correctly	O
answered	O
MCQs	O
when	O
choosing	O
the	O
answer	O
uniformly	O
at	O
random	O
(	O
denoted	O
N	O
r	O
)	O
.	O
To	O
test	O
for	O
this	O
property	O
,	O
we	O
have	O
formulated	O
the	O
following	O
two	O
hypotheses	O
.	O
5	O
H	O
0	B-DatasetName
:	O
N	O
s	O
=	O
N	O
r	O
.	O
H	O
1	O
:	O
N	O
s	O
=	O
N	O
r	O
.	O
For	O
N	O
MCQs	O
with	O
4	O
options	O
,	O
N	O
r	O
=	O
0.25N	O
,	O
which	O
for	O
our	O
test	O
set	O
would	O
be	O
equal	O
to	O
N	O
r	O
=	O
0.25	O
102	O
=	O
25.5	O
.	O
The	O
appropriate	O
statistical	O
test	O
in	O
this	O
case	O
is	O
one	O
-	O
sample	O
two	O
-	O
tailed	O
t	O
-	O
test	O
with	O
the	O
aim	O
of	O
not	O
being	O
able	O
to	O
reject	O
H	O
0	B-DatasetName
.	O
Given	O
that	O
the	O
purpose	O
is	O
to	O
show	O
that	O
the	O
data	O
supports	O
H	O
0	B-DatasetName
,	O
we	O
have	O
set	O
both	O
the	O
probability	O
α	B-HyperparameterName
of	O
type	O
I	O
errors	O
and	O
the	O
probability	O
β	B-HyperparameterName
of	O
type	O
II	O
errors	O
to	O
be	O
0.05	O
.	O
Then	O
we	O
have	O
used	O
G*Power	O
(	O
Faul	O
et	O
al	O
,	O
2009	O
)	O
to	O
calculate	O
the	O
required	O
sample	O
size	O
for	O
finding	O
a	O
medium	O
effect	O
size	O
(	O
0.5	O
)	O
and	O
the	O
given	O
α	B-HyperparameterName
and	O
β	B-HyperparameterName
,	O
which	O
turned	O
out	O
to	O
be	O
54	O
subjects	O
.	O
Following	O
the	O
calculations	O
above	O
,	O
we	O
have	O
recruited	O
54	O
subjects	O
on	O
the	O
Prolific	O
platform	O
6	O
,	O
and	O
instructed	O
them	O
to	O
choose	O
the	O
most	O
plausible	O
answer	O
to	O
a	O
number	O
of	O
reading	B-TaskName
comprehension	I-TaskName
MCQs	O
without	O
providing	O
the	O
original	O
texts	O
.	O
The	O
collected	O
data	O
did	O
not	O
violate	O
any	O
assumptions	O
for	O
a	O
one	O
-	O
sample	O
t	O
-	O
test	O
(	O
see	O
Appendix	O
D.1	O
for	O
more	O
details	O
)	O
.	O
On	O
average	O
,	O
the	O
subjects	O
correctly	O
answered	O
a	O
significantly	O
larger	O
number	O
of	O
questions	O
than	O
N	O
r	O
(	O
N	O
s	O
=	O
62.26	O
,	O
SE	O
=	O
1.09	O
,	O
t	O
(	O
53	O
)	O
=	O
33.51	O
,	O
p	O
<	O
0.05	O
,	O
r	O
=	O
0.98	O
)	O
.	O
To	O
summarize	O
,	O
the	O
chances	O
of	O
this	O
sample	O
to	O
be	O
collected	O
are	O
very	O
low	O
if	O
H	O
0	B-DatasetName
were	O
true	O
.	O
However	O
,	O
evidently	O
some	O
of	O
the	O
generated	O
distractors	O
were	O
actually	O
plausible	O
,	O
given	O
that	O
N	O
s	O
=	O
N	O
.	O
To	O
investigate	O
the	O
matter	O
we	O
have	O
plotted	O
the	O
histogram	O
of	O
the	O
frequency	O
of	O
choice	O
of	O
distractors	O
by	O
the	O
subjects	O
in	O
Figure	O
2	O
.	O
As	O
suggested	O
by	O
Haladyna	O
and	O
Downing	O
(	O
1993	O
)	O
,	O
distractors	O
that	O
are	O
chosen	O
by	O
less	O
than	O
5	O
%	O
of	O
students	O
should	O
not	O
be	O
used	O
,	O
which	O
in	O
our	O
case	O
amounts	O
to	O
39	O
%	O
of	O
the	O
dis	O
-	O
5	O
Preregistration	O
is	O
available	O
here	O
6	O
https://www.prolific.co/	O
tractors	O
(	O
the	O
leftmost	O
bar	O
in	O
Figure	O
2	O
)	O
.	O
If	O
we	O
eliminate	O
these	O
low	O
-	O
frequency	O
distractors	O
(	O
LF	O
-	O
DIS	O
)	O
,	O
68	O
MCQs	O
(	O
66.67	O
%	O
)	O
will	O
lose	O
at	O
least	O
one	O
distractor	O
,	O
10	O
MCQs	O
(	O
9.8	O
%	O
)	O
will	O
lose	O
all	O
distractors	O
and	O
thus	O
34	O
MCQs	O
(	O
33.33	O
%	O
)	O
will	O
keep	O
all	O
3	O
distractors	O
.	O
A	O
more	O
relaxed	O
question	O
is	O
how	O
many	O
MCQs	O
had	O
at	O
least	O
one	O
plausible	O
distractor	O
,	O
which	O
can	O
be	O
estimated	O
by	O
calculating	O
the	O
entropy	O
for	O
each	O
question	O
as	O
shown	O
in	O
Equation	O
(	O
2	O
)	O
,	O
where	O
A	O
is	O
the	O
key	O
,	O
D	O
is	O
a	O
distractor	O
,	O
Q	O
is	O
the	O
stem	O
,	O
P	O
Q	O
(	O
A	O
)	O
(	O
P	O
Q	O
(	O
D	O
)	O
)	O
is	O
the	O
probability	O
that	O
the	O
key	O
(	O
any	O
distractor	O
)	O
is	O
chosen	O
for	O
Q	O
by	O
a	O
subject	O
.	O
H	O
(	O
Q	O
)	O
=	O
−	O
O	O
{	O
A	O
,	O
D	O
}	O
p	O
Q	O
(	O
O	O
)	O
log	O
(	O
p	O
Q	O
(	O
O	O
)	O
)	O
(	O
2	O
)	O
The	O
distribution	O
of	O
entropies	O
per	O
question	O
is	O
shown	O
in	O
Figure	O
3	O
.	O
Assuming	O
the	O
natural	O
logarithm	O
,	O
the	O
highest	O
theoretically	O
possible	O
value	O
for	O
H	O
(	O
Q	O
)	O
is	O
0.69	O
,	O
if	O
p	O
Q	O
(	O
A	O
)	O
=	O
p	O
Q	O
(	O
D	O
)	O
=	O
0.5	O
.	O
32	O
%	O
of	O
MCQs	O
had	O
an	O
entropy	O
larger	O
than	O
0.65	O
,	O
whereas	O
51	O
%	O
had	O
an	O
entropy	O
larger	O
than	O
0.6	O
,	O
which	O
means	O
that	O
half	O
of	O
MCQs	O
had	O
at	O
least	O
one	O
plausible	O
distractor	O
.	O

Bearing	O
in	O
mind	O
the	O
findings	O
of	O
Section	O
6.2.1	O
,	O
it	O
is	O
interesting	O
to	O
see	O
which	O
of	O
the	O
proposed	O
distractors	O
(	O
especially	O
,	O
among	O
LF	O
-	O
DIS	O
)	O
teachers	O
would	O
mark	O
as	O
acceptable	O
.	O
Given	O
the	O
complexity	O
of	O
such	O
evaluation	O
,	O
using	O
the	O
whole	O
test	O
set	O
was	O
infeasible	O
.	O
To	O
get	O
a	O
representative	O
sample	O
,	O
we	O
used	O
entropy	O
per	O
question	O
(	O
shown	O
in	O
Figure	O
3	O
)	O
.	O
All	O
MCQs	O
were	O
divided	O
into	O
5	O
equally	O
sized	O
buckets	O
by	O
entropy	O
and	O
9	O
MCQs	O
were	O
sampled	O
uniformly	O
at	O
random	O
from	O
each	O
bucket	O
,	O
resulting	O
in	O
45	O
MCQs	O
in	O
total	O
.	O
We	O
asked	O
5	O
teachers	O
to	O
evaluate	O
each	O
MCQ	O
(	O
presented	O
in	O
a	O
random	O
order	O
for	O
each	O
of	O
them	O
)	O
.	O
Each	O
MCQ	O
contained	O
the	O
base	O
text	O
,	O
the	O
stem	O
,	O
the	O
key	O
and	O
the	O
generated	O
distractors	O
.	O
The	O
teachers	O
were	O
instructed	O
to	O
select	O
those	O
of	O
generated	O
distractors	O
(	O
if	O
any	O
)	O
deemed	O
suitable	O
for	O
testing	O
reading	B-TaskName
comprehension	I-TaskName
.	O
Additionally	O
,	O
we	O
asked	O
to	O
provide	O
their	O
reasons	O
for	O
each	O
rejected	O
distractor	O
in	O
a	O
free	O
-	O
text	O
input	O
.	O
The	O
inter	O
-	O
annotator	O
agreement	O
(	O
IAA	O
)	O
was	O
estimated	O
using	O
Goodman	O
-	O
Kruskal	O
's	O
γ	B-HyperparameterName
(	O
Goodman	O
and	O
Kruskal	O
,	O
1979	O
)	O
,	O
specifically	O
its	O
multirater	O
version	O
γ	B-HyperparameterName
N	O
proposed	O
by	O
Kalpakchi	O
and	O
Boye	O
(	O
2021	O
)	O
.	O
On	O
the	O
scale	O
proposed	O
by	O
Rosenthal	O
(	O
1996	O
)	O
,	O
we	O
have	O
found	O
a	O
very	O
large	O
agreement	O
(	O
γ	B-HyperparameterName
N	O
=	O
0.85	O
,	O
see	O
Appendix	O
D.2.2	O
for	O
more	O
details	O
on	O
IAA	O
calculations	O
)	O
.	O
On	O
average	O
,	O
1.47	O
distractors	O
per	O
MCQ	O
were	O
accepted	O
by	O
a	O
teacher	O
.	O
Their	O
reasons	O
for	O
rejections	O
are	O
distributed	O
as	O
shown	O
in	O
Figure	O
4	O
.	O
All	O
teachers	O
accepted	O
at	O
least	O
one	O
generated	O
distractor	O
for	O
39	O
MCQs	O
(	O
86.7	O
%	O
)	O
,	O
whereas	O
the	O
majority	O
of	O
teachers	O
did	O
so	O
for	O
27	O
MCQs	O
(	O
60	O
%	O
)	O
.	O
Interestingly	O
,	O
there	O
are	O
no	O
MCQs	O
in	O
which	O
all	O
5	O
teachers	O
have	O
either	O
accepted	O
or	O
rejected	O
all	O
generated	O
distractors	O
.	O
However	O
,	O
the	O
majority	O
of	O
teachers	O
has	O
accepted	O
or	O
rejected	O
all	O
distractors	O
for	O
4	O
MCQs	O
(	O
8.9	O
%	O
)	O
and	O
6	O
MCQs	O
(	O
13.3	O
%	O
)	O
respectively	O
.	O
Out	O
of	O
45	O
MCQs	O
,	O
31	O
(	O
68.9	O
%	O
)	O
had	O
at	O
least	O
one	O
LF	O
-	O
DIS	O
,	O
as	O
defined	O
in	O
Section	O
6.2.1	O
.	O
For	O
these	O
31	O
MCQs	O
we	O
report	O
a	O
distribution	O
of	O
accepted	O
/	O
rejected	O
LF	O
-	O
DIS	O
by	O
the	O
majority	O
of	O
teachers	O
in	O
Figure	O
5	O
.	O
Let	O
us	O
call	O
the	O
15	O
MCQs	O
with	O
all	O
LF	O
-	O
DIS	O
accepted	O
by	O
the	O
majority	O
of	O
teachers	O
as	O
mismatch	O
MCQs	O
(	O
lowest	O
row	O
in	O
Figure	O
5	O
)	O
.	O
Interestingly	O
,	O
12	O
of	O
the	O
15	O
mismatch	O
MCQs	O
had	O
at	O
least	O
one	O
more	O
distractor	O
in	O
addition	O
to	O
LF	O
-	O
DIS	O
being	O
accepted	O
by	O
the	O
majority	O
of	O
teachers	O
.	O
Furthermore	O
,	O
all	O
mismatch	O
MCQs	O
had	O
entropy	O
higher	O
than	O
0.3	O
.	O
This	O
entails	O
that	O
almost	O
a	O
half	O
of	O
LF	O
-	O
DIS	O
should	O
not	O
necessarily	O
be	O
thrown	O
away	O
,	O
since	O
they	O
were	O
accepted	O
by	O
teachers	O
,	O
but	O
the	O
MCQs	O
either	O
happened	O
to	O
have	O
more	O
plausible	O
distractors	O
or	O
subjects	O
might	O
have	O
had	O
relevant	O
background	O
knowledge	O
to	O
answer	O
the	O
questions	O
.	O

We	O
employed	O
a	O
systematic	O
process	O
to	O
get	O
a	O
comprehensive	O
overview	O
of	O
DG	O
methods	O
(	O
see	O
Appendix	O
E	O
for	O
more	O
details	O
)	O
.	O
Out	O
of	O
the	O
resulting	O
28	O
articles	O
(	O
see	O
an	O
overview	O
in	O
Table	O
4	O
)	O
,	O
only	O
2	O
worked	O
with	O
a	O
language	O
other	O
than	O
English	O
(	O
Chinese	O
and	O
Basque	O
)	O
.	O
In	O
this	O
paper	O
we	O
work	O
on	O
reading	B-TaskName
comprehension	I-TaskName
MCQs	O
,	O
which	O
makes	O
only	O
12	O
papers	O
,	O
dealing	O
with	O
factual	O
questions	O
,	O
relevant	O
.	O
Two	O
of	O
these	O
used	O
rule	O
-	O
based	O
approaches	O
.	O
Majumder	O
and	O
Saha	O
(	O
2015	O
)	O
generated	O
MCQs	O
for	O
cricket	O
domain	O
and	O
used	O
a	O
number	O
of	O
hand	O
-	O
crafted	O
rules	O
based	O
on	O
gazeteers	O
and	O
Wikipedia	O
entries	O
to	O
generate	O
distractors	O
.	O
Mitkov	O
and	O
Ha	O
(	O
2003	O
)	O
proposed	O
to	O
generate	O
distractors	O
for	O
MCQs	O
on	O
electronic	O
instructional	O
documents	O
using	O
WordNet	O
.	O
Six	O
of	O
these	O
relied	O
on	O
extractive	O
approaches	O
.	O
Liang	O
et	O
al	O
(	O
2018	O
)	O
,	O
Welbl	O
et	O
al	O
(	O
2017	O
)	O
,	O
and	O
Ha	O
and	O
Yaneva	O
(	O
2018	O
)	O
formulated	O
choosing	O
a	O
distractor	O
as	O
a	O
ranking	O
problem	O
from	O
the	O
given	O
candidate	O
set	O
.	O
In	O
the	O
first	O
two	O
articles	O
the	O
candidate	O
set	O
constituted	O
all	O
distractors	O
from	O
the	O
available	O
MCQ	O
dataset	O
.	O
The	O
authors	O
then	O
trained	O
ML	O
-	O
based	O
ranker	O
(	O
s	O
)	O
for	O
choosing	O
the	O
best	O
distractors	O
.	O
In	O
the	O
last	O
one	O
,	O
the	O
candidate	O
set	O
was	O
created	O
using	O
content	O
engineers	O
.	O
Distractors	O
with	O
a	O
high	O
similarity	O
of	O
their	O
concept	O
embeddings	O
(	O
summed	O
for	O
multiple	O
words	O
)	O
and	O
appearing	O
in	O
the	O
same	O
document	O
as	O
the	O
key	O
are	O
ranked	O
higher	O
.	O
Stasaski	O
and	O
Hearst	O
(	O
2017	O
)	O
and	O
Araki	O
et	O
al	O
(	O
2016	O
)	O
worked	O
in	O
the	O
domain	O
of	O
biology	O
.	O
The	O
former	O
used	O
an	O
ontology	B-MethodName
and	O
the	O
latter	O
employed	O
event	O
graphs	O
containing	O
information	O
about	O
coreferences	O
to	O
generate	O
distractors	O
.	O
Karamanis	O
et	O
al	O
(	O
2006	O
)	O
used	O
thesaurus	O
and	O
tf	O
-	O
idf	O
to	O
identify	O
key	O
concepts	O
in	O
the	O
given	O
text	O
and	O
then	O
select	O
as	O
distractors	O
those	O
having	O
the	O
same	O
semantic	O
type	O
as	O
the	O
key	O
.	O
The	O
remaining	O
four	O
employed	O
neural	O
methods	O
and	O
are	O
most	O
relevant	O
among	O
the	O
surveyed	O
.	O
Qiu	O
et	O
al	O
(	O
2020	O
)	O
trained	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
(	O
seq2seq	B-MethodName
)	O
model	O
with	O
a	O
number	O
of	O
attention	B-HyperparameterName
layers	I-HyperparameterName
.	O
Zhou	O
et	O
al	O
(	O
2020	O
)	O
also	O
employed	O
a	O
seq2seq	B-MethodName
model	O
,	O
but	O
with	O
a	O
hierarchical	O
attention	O
to	O
capture	O
the	O
interaction	O
between	O
a	O
text	O
and	O
a	O
question	O
,	O
as	O
well	O
as	O
semantic	B-TaskName
similarity	I-TaskName
loss	B-MetricName
.	O
Both	O
articles	O
used	O
a	O
beam	O
search	O
combined	O
with	O
filtering	O
based	O
on	O
Jaccard	O
coefficient	O
at	O
generation	O
time	O
.	O
Offerijns	O
et	O
al	O
(	O
2020	O
)	O
trained	O
a	O
GPT	B-MethodName
-	O
2	O
model	O
to	O
generate	O
3	O
distractors	O
for	O
a	O
given	O
MCQ	O
,	O
and	O
used	O
BERT	B-MethodName
-	O
based	O
question	B-TaskName
answering	I-TaskName
model	O
for	O
quantitative	O
evaluation	O
(	O
along	O
with	O
human	O
evaluation	O
)	O
.	O
Finally	O
,	O
Chung	O
et	O
al	O
(	O
2020	O
)	O
proposed	O
a	O
BERTbased	O
method	O
for	O
English	O
with	O
answer	O
-	O
negative	O
regularization	O
,	O
penalizing	O
distractors	O
for	O
containing	O
Then	O
they	O
rank	O
every	O
triple	O
of	O
distractors	O
based	O
on	O
the	O
entropy	O
of	O
a	O
separately	O
trained	O
QA	O
model	O
.	O
Our	O
method	O
also	O
relies	O
on	O
BERT	B-MethodName
,	O
but	O
has	O
a	O
number	O
of	O
differences	O
beyond	O
being	O
applied	O
to	O
Swedish	O
.	O
Firstly	O
,	O
we	O
did	O
not	O
include	O
answer	O
-	O
negative	O
regularization	O
,	O
since	O
it	O
is	O
not	O
always	O
a	O
good	O
strategy	O
.	O
For	O
instance	O
,	O
given	O
the	O
stem	O
"	O
When	O
should	O
you	O
pay	O
a	O
fee	O
if	O
you	O
apply	O
for	O
a	O
visa	O
?	O
"	O
and	O
a	O
key	O
"	O
before	O
you	O
have	O
submitted	O
the	O
application	O
"	O
,	O
the	O
best	O
distractor	O
would	O
be	O
"	O
after	O
you	O
have	O
submitted	O
the	O
application	O
"	O
,	O
which	O
shares	O
most	O
of	O
the	O
words	O
with	O
the	O
key	O
.	O
Secondly	O
,	O
we	O
generate	O
distractors	O
in	O
arbitrary	O
word	O
order	O
compared	O
to	O
left	O
-	O
to	O
-	O
right	O
generation	O
in	O
(	O
Chung	O
et	O
al	O
,	O
2020	O
)	O
.	O
Thirdly	O
,	O
at	O
generation	O
time	O
,	O
we	O
use	O
previously	O
generated	O
distractors	O
as	O
input	O
for	O
generating	O
next	O
ones	O
,	O
and	O
always	O
take	O
tokens	O
with	O
a	O
maximum	O
probability	O
.	O
This	O
lowers	O
the	O
risk	O
of	O
generating	O
ungrammatical	O
distractors	O
.	O
Finally	O
,	O
our	O
training	O
set	O
is	O
100	O
times	O
smaller	O
compared	O
to	O
the	O
training	O
set	O
used	O
by	O
Chung	O
et	O
al	O
(	O
2020	O
)	O
.	O

To	O
evaluate	O
the	O
inter	O
-	O
annotator	O
agreement	O
(	O
IAA	O
)	O
between	O
the	O
teachers	O
,	O
we	O
have	O
reformulated	O
the	O
problem	O
into	O
a	O
ranking	O
problem	O
,	O
where	O
all	O
accepted	O
distractors	O
were	O
given	O
the	O
rank	O
of	O
1	O
and	O
those	O
rejected	O
-	O
the	O
rank	O
of	O
2	O
.	O
IAA	O
was	O
then	O
estimated	O
using	O
Goodman	O
-	O
Kruskal	O
's	O
γ	B-HyperparameterName
(	O
Goodman	O
and	O
Kruskal	O
,	O
1979	O
)	O
,	O
specifically	O
its	O
multirater	O
version	O
γ	B-HyperparameterName
N	O
proposed	O
by	O
Kalpakchi	O
and	O
Boye	O
(	O
2021	O
)	O
.	O
The	O
total	O
number	O
of	O
concordant	O
and	O
discordant	O
pairs	O
were	O
summed	O
for	O
each	O
pair	O
of	O
teachers	O
for	O
each	O
MCQ	O
.	O
The	O
resulting	O
γ	B-HyperparameterName
N	O
equals	O
to	O
0.85	O
,	O
indicating	O
a	O
very	O
large	O
agreement	O
on	O
the	O
scale	O
proposed	O
by	O
Rosenthal	O
(	O
1996	O
)	O
.	O

A	O
number	O
of	O
generated	O
distractors	O
along	O
with	O
the	O
respective	O
stems	O
and	O
keys	O
from	O
the	O
dataset	O
are	O
presented	O
in	O
Figures	O
11	O
,	O
12	O
,	O
13	O
,	O
14	O
,	O
15	O
.	O
The	O
questions	O
are	O
sampled	O
based	O
on	O
the	O
entropy	O
of	O
student	O
's	O
an	O
-	O
Thank	O
you	O
for	O
participating	O
in	O
our	O
study	O
!	O
You	O
will	O
be	O
presented	O
with	O
a	O
number	O
of	O
tests	O
.	O
Each	O
test	O
contains	O
a	O
text	O
,	O
a	O
reading	B-TaskName
comprehension	I-TaskName
question	O
based	O
on	O
the	O
text	O
,	O
the	O
explicitly	O
marked	O
correct	O
answer	O
to	O
this	O
question	O
and	O
a	O
number	O
of	O
suggestions	O
for	O
wrong	O
,	O
but	O
plausible	O
alternatives	O
(	O
distractors	O
)	O
.	O
Suppose	O
you	O
would	O
like	O
to	O
use	O
the	O
given	O
question	O
for	O
testing	O
reading	B-TaskName
comprehension	I-TaskName
of	O
the	O
given	O
text	O
.	O
Your	O
task	O
is	O
to	O
judge	O
which	O
of	O
the	O
suggested	O
distractors	O
(	O
if	O
any	O
)	O
you	O
would	O
fit	O
the	O
purpose	O
.	O
Select	O
suitable	O
distractors	O
by	O
simply	O
ticking	O
the	O
respective	O
checkboxes	O
.	O
For	O
the	O
other	O
distractors	O
(	O
that	O
you	O
did	O
n't	O
select	O
)	O
,	O
please	O
briefly	O
state	O
your	O
reasons	O
why	O
these	O
distractors	O
were	O
inappropriate	O
in	O
the	O
respective	O
text	O
fields	O
(	O
max	O
1	O
sentence	O
)	O
.	O
Figure	O
10	O
:	O
An	O
English	O
translation	O
of	O
the	O
original	O
instructions	O
given	O
to	O
teachers	O
(	O
the	O
original	O
instructions	O
in	O
Swedish	O
can	O
be	O
found	O
in	O
the	O
GitHub	O
repository	O
)	O
swers	O
using	O
the	O
same	O
5	O
buckets	O
as	O
in	O
sampling	O
for	O
teachers	O
'	O
evaluation	O
.	O
Recall	B-MetricName
that	O
distractors	O
are	O
said	O
to	O
be	O
low	O
frequency	O
(	O
LF	O
-	O
DIS	O
)	O
if	O
they	O
were	O
chosen	O
by	O
less	O
than	O
5	O
%	O
of	O
students	O
.	O
Hence	O
,	O
a	O
red	O
cross	O
in	O
the	O
column	O
"	O
F	O
-	O
DIS	O
>	O
5	O
%	O
"	O
entails	O
that	O
a	O
given	O
distractor	O
is	O
in	O
fact	O
an	O
LF	O
-	O
DIS	O
.	O
The	O
MCQ	O
in	O
sample	O
1	O
has	O
an	O
entropy	O
of	O
0	B-DatasetName
,	O
meaning	O
all	O
students	O
have	O
selected	O
the	O
same	O
option	O
,	O
which	O
in	O
this	O
case	O
was	O
the	O
key	O
.	O
In	O
this	O
case	O
,	O
two	O
of	O
three	O
distractors	O
were	O
accepted	O
by	O
the	O
majority	O
of	O
teachers	O
,	O
although	O
all	O
of	O
them	O
were	O
LF	O
-	O
DIS	O
.	O
This	O
is	O
a	O
good	O
example	O
of	O
an	O
MCQ	O
with	O
plausible	O
distractors	O
,	O
but	O
where	O
the	O
stem	O
is	O
too	O
easy	O
.	O
The	O
MCQ	O
in	O
sample	O
2	O
presents	O
an	O
interesting	O
case	O
,	O
when	O
the	O
distractor	O
contains	O
an	O
obvious	O
grammatical	O
error	O
(	O
comma	O
before	O
the	O
first	O
word	O
in	O
the	O
distractor	O
3	O
)	O
.	O
While	O
the	O
distractor	O
was	O
rightfully	O
rejected	O
by	O
the	O
majority	O
of	O
teachers	O
,	O
it	O
was	O
still	O
selected	O
by	O
more	O
than	O
5	O
%	O
of	O
students	O
.	O
The	O
MCQ	O
in	O
sample	O
3	O
is	O
a	O
good	O
example	O
of	O
longer	O
distractors	O
.	O
In	O
this	O
case	O
,	O
two	O
distractors	O
were	O
accepted	O
by	O
teachers	O
and	O
two	O
were	O
selected	O
by	O
more	O
than	O
5	O
%	O
of	O
students	O
.	O
However	O
,	O
interestingly	O
these	O
sets	O
are	O
disjoint	O
,	O
meaning	O
that	O
all	O
three	O
distractors	O
could	O
potentially	O
be	O
useful	O
.	O
Another	O
more	O
general	O
observation	O
,	O
requiring	O
future	O
research	O
,	O
is	O
that	O
our	O
model	O
seems	O
to	O
struggle	O
more	O
when	O
generating	O
longer	O
distractors	O
in	O
general	O
,	O
resulting	O
in	O
non	O
-	O
finished	O
sentences	O
or	O
repetitions	O
of	O
words	O
.	O
The	O
MCQ	O
in	O
sample	O
4	O
is	O
somewhat	O
opposite	O
to	O
sample	O
3	O
,	O
since	O
one	O
distractor	O
that	O
was	O
accepted	O
by	O
the	O
teachers	O
turned	O
out	O
to	O
be	O
an	O
LF	O
-	O
DIS	O
.	O
This	O
either	O
means	O
that	O
the	O
stem	O
was	O
too	O
easy	O
or	O
that	O
none	O
of	O
the	O
distractors	O
were	O
potentially	O
useful	O
.	O
The	O
MCQ	O
in	O
sample	O
5	O
is	O
the	O
one	O
with	O
a	O
highest	O
theoretically	O
possible	O
entropy	O
between	O
selecting	O
the	O
correct	O
or	O
a	O
wrong	O
option	O
.	O
Note	O
that	O
it	O
might	O
still	O
happen	O
that	O
some	O
of	O
the	O
distractors	O
is	O
LF	O
-	O
DIS	O
,	O
since	O
the	O
entropy	O
is	O
calculated	O
not	O
between	O
all	O
four	O
options	O
,	O
but	O
only	O
between	O
the	O
key	O
and	O
the	O
distractors	O
as	O
a	O
group	O
.	O

Because	O
of	O
globalization	O
,	O
it	O
is	O
becoming	O
more	O
and	O
more	O
common	O
to	O
use	O
multiple	O
languages	O
in	O
a	O
single	O
utterance	O
,	O
also	O
called	O
codeswitching	O
.	O
This	O
results	O
in	O
special	O
linguistic	O
structures	O
and	O
,	O
therefore	O
,	O
poses	O
many	O
challenges	O
for	O
Natural	O
Language	O
Processing	O
.	O
Existing	O
models	O
for	O
language	B-TaskName
identification	I-TaskName
in	O
code	O
-	O
switched	O
data	O
are	O
all	O
supervised	O
,	O
requiring	O
annotated	O
training	O
data	O
which	O
is	O
only	O
available	O
for	O
a	O
limited	O
number	O
of	O
language	O
pairs	O
.	O
In	O
this	O
paper	O
,	O
we	O
explore	O
semi	O
-	O
supervised	O
approaches	O
,	O
that	O
exploit	O
out	O
-	O
of	O
-	O
domain	O
monolingual	O
training	O
data	O
.	O
We	O
experiment	O
with	O
word	O
uni	O
-	O
grams	O
,	O
word	O
n	O
-	O
grams	O
,	O
character	O
ngrams	O
,	O
Viterbi	O
Decoding	O
,	O
Latent	O
Dirichlet	O
Allocation	O
,	O
Support	B-MethodName
Vector	I-MethodName
Machine	I-MethodName
and	O
Logistic	B-MethodName
Regression	I-MethodName
.	O
The	O
Viterbi	O
model	O
was	O
the	O
best	O
semi	O
-	O
supervised	O
model	O
,	O
scoring	O
a	O
weighted	O
F1	B-MetricName
score	I-MetricName
of	O
92.23	O
%	O
,	O
whereas	O
a	O
fully	O
supervised	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
BERT	B-MethodName
-	O
based	O
model	O
scored	O
98.43	O
%	O
.	O
1	O

Social	O
platforms	O
have	O
been	O
the	O
cradle	O
of	O
the	O
internet	O
,	O
driving	O
vast	O
amounts	O
of	O
communication	O
among	O
people	O
from	O
all	O
over	O
the	O
world	O
.	O
As	O
a	O
consequence	O
,	O
the	O
way	O
people	O
communicate	O
in	O
written	O
text	O
has	O
changed	O
,	O
as	O
now	O
it	O
is	O
common	O
to	O
use	O
,	O
for	O
example	O
,	O
abbreviations	O
of	O
words	O
,	O
emoticons	O
,	O
references	O
to	O
other	O
users	O
and	O
use	O
multiple	O
languages	O
within	O
the	O
same	O
utterance	O
.	O
An	O
annotated	O
example	O
sentence	O
of	O
this	O
is	O
the	O
following	O
tweet	O
:	O
Word	O
El	O
online	O
exercise	O
de	O
hoy	O
:	O
Label	O
es	O
en	O
en	O
es	O
es	O
other	O
This	O
phenomenon	O
has	O
caught	O
particular	O
interest	O
in	O
both	O
sociolinguistics	O
and	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
(	O
Aguilar	O
et	O
al	O
,	O
2020	O
;	O
Khanuja	O
et	O
al	O
,	O
2020	O
)	O
.	O
Classifying	O
the	O
language	O
labels	O
on	O
the	O
word	O
level	O
(	O
i.e.	O
code	O
-	O
switch	O
detection	O
)	O
has	O
shown	O
to	O
be	O
beneficial	O
to	O
improve	O
performance	O
on	O
downstream	O
NLP	O
tasks	O
,	O
like	O
dependency	B-TaskName
parsing	I-TaskName
(	O
Bhat	O
et	O
al	O
,	O
2018	O
)	O
or	O
lexical	B-TaskName
normalization	I-TaskName
(	O
Barik	O
et	O
al	O
,	O
2019	O
)	O
.	O
Previous	O
work	O
has	O
shown	O
that	O
high	O
performances	O
can	O
be	O
achieved	O
for	O
this	O
task	O
for	O
many	O
language	O
pairs	O
(	O
Molina	O
et	O
al	O
,	O
2016	O
;	O
Banerjee	O
et	O
al	O
,	O
2016	O
)	O
.	O
However	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
most	O
previous	O
work	O
focused	O
on	O
supervised	O
settings	O
,	O
restraining	O
their	O
usefulness	O
to	O
language	O
pairs	O
for	O
which	O
annotated	O
datasets	O
exist	O
.	O
Recent	O
efforts	O
to	O
unify	O
existing	O
datasets	O
have	O
collected	O
annotation	O
for	O
4	O
(	O
Aguilar	O
et	O
al	O
,	O
2020	O
)	O
and	O
2	O
(	O
Khanuja	O
et	O
al	O
,	O
2020	O
)	O
language	O
pairs	O
,	O
which	O
confirms	O
that	O
annotated	O
data	O
is	O
not	O
available	O
for	O
most	O
language	O
pairs	O
.	O
In	O
supervised	O
settings	O
,	O
recent	O
transformer	O
models	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
have	O
reached	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
Aguilar	O
et	O
al	O
,	O
2020	O
;	O
Khanuja	O
et	O
al	O
,	O
2020	O
)	O
,	O
outperforming	O
Bi	O
-	O
LSTMS	O
and	O
traditional	O
machine	O
learning	O
methods	O
used	O
earlier	O
(	O
Molina	O
et	O
al	O
,	O
2016	O
;	O
Banerjee	O
et	O
al	O
,	O
2016	O
)	O
.	O
Yirmibeşoglu	O
and	O
Eryigit	O
(	O
2018	O
)	O
tackled	O
this	O
task	O
in	O
a	O
semi	O
-	O
supervised	O
setup	O
as	O
well	O
,	O
where	O
they	O
used	O
character	O
n	O
-	O
gram	O
language	O
models	O
trained	O
on	O
monolingual	O
data	O
to	O
predict	O
perplexity	B-MetricName
on	O
the	O
target	O
word	O
for	O
classification	O
.	O
They	O
show	O
that	O
this	O
obtains	O
a	O
micro	O
-	O
average	B-MetricName
F1	I-MetricName
score	O
of	O
92.9	O
%	O
,	O
compared	O
to	O
95.6	O
%	O
with	O
a	O
supervised	O
CRF	B-MethodName
-	O
model	O
.	O
To	O
overcome	O
this	O
limitation	O
,	O
we	O
focus	O
on	O
exploiting	O
only	O
mono	O
-	O
lingual	O
datasets	O
for	O
performing	O
word	O
-	O
level	O
language	B-TaskName
identification	I-TaskName
in	O
code	O
-	O
switched	O
data	O
.	O
We	O
refer	O
to	O
this	O
setup	O
as	O
semi	O
-	O
supervised	O
,	O
since	O
we	O
have	O
no	O
data	O
annotated	O
for	O
the	O
task	O
at	O
hand	O
(	O
code	O
-	O
switch	O
detection	O
)	O
.	O
This	O
enables	O
the	O
possibility	O
to	O
easily	O
train	O
models	O
for	O
new	O
language	O
pairs	O
,	O
and	O
leads	O
to	O
the	O
research	O
question	O
:	O
How	O
do	O
semi	O
-	O
supervised	O
models	O
compare	O
and	O
perform	O
in	O
the	O
task	O
of	O
language	B-TaskName
identification	I-TaskName
in	O
English	O
-	O
Spanish	O
code	O
-	O
switched	O
data	O
?	O
(	O
RQ1	O
)	O
.	O
Since	O
supervised	O
methods	O
have	O
the	O
advantage	O
of	O
learning	O
from	O
annotated	O
data	O
,	O
the	O
second	O
research	O
question	O
is	O
:	O
How	O
much	O
can	O
we	O
reduce	O
the	O
gap	O
in	O
performance	O
between	O
the	O
aforementioned	O
semisupervised	O
models	O
and	O
a	O
supervised	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
?	O
(	O
RQ2	O
)	O
.	O
Previous	O
work	O
in	O
similar	O
setups	O
have	O
automatically	O
generated	O
code	O
-	O
switched	O
data	O
from	O
monolingual	O
datasets	O
(	O
Santy	O
et	O
al	O
,	O
2021	O
)	O
.	O
We	O
consider	O
this	O
approach	O
to	O
be	O
orthogonal	O
to	O
ours	O
,	O
and	O
Santy	O
et	O
al	O
(	O
2021	O
)	O
exploit	O
mono	O
-	O
lingual	O
in	O
-	O
domain	O
data	O
,	O
syntactic	O
parsers	O
and	O
parallel	O
sentences	O
.	O

The	O
problem	O
of	O
code	O
-	O
switching	O
can	O
be	O
represented	O
as	O
a	O
Hidden	O
Markov	O
Model	O
(	O
HMM	O
)	O
problem	O
,	O
since	O
a	O
sentence	O
can	O
be	O
seen	O
as	O
a	O
Markov	O
chain	O
with	O
hidden	O
states	O
that	O
are	O
the	O
two	O
different	O
languages	O
.	O
We	O
use	O
the	O
Viterbi	O
decoding	O
algorithm	O
(	O
Forney	O
,	O
1973	O
)	O
to	O
find	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
given	O
the	O
observations	O
-	O
namely	O
,	O
to	O
assign	O
a	O
language	O
label	O
(	O
state	O
)	O
to	O
each	O
word	O
(	O
observation	O
)	O
.	O
We	O
used	O
eginhard	O
's	O
implementation	O
5	O
of	O
the	O
Viterbi	O
algorithm	O
and	O
modified	O
the	O
starting	O
and	O
transition	O
probabilities	O
to	O
the	O
values	O
specified	O
below	O
,	O
which	O
were	O
found	O
to	O
be	O
optimal	O
using	O
grid	O
search	O
on	O
the	O
development	O
set	O
using	O
the	O
range	O
of	O
initial	O
probabilities	O
for	O
English	O
from	O
0.1	O
to	O
0.9	O
with	O
step	B-HyperparameterName
size	I-HyperparameterName
0.1	O
,	O
transition	O
probabilities	O
for	O
English	O
from	O
0.05	O
to	O
0.95	O
with	O
step	B-HyperparameterName
size	I-HyperparameterName
0.05	O
.	O
The	O
final	O
hyperparameters	O
are	O
as	O
follows	O
:	O
states	O
:	O
lang1	O
and	O
lang2	O
,	O
other	O
tokens	O
are	O
identified	O
based	O
on	O
heuristics	O
(	O
see	O
Section	O
2.3	O
)	O
;	O
initial	O
probabilities	O
:	O
0.6	O
for	O
English	O
and	O
0.4	O
for	O
Spanish	O
;	O
transition	O
probabilities	O
:	O
0.15	O
for	O
transitioning	O
to	O
a	O
different	O
language	O
and	O
0.85	O
for	O
transitioning	O
to	O
the	O
same	O
language	O
;	O
emission	O
probabilities	O
:	O
these	O
are	O
estimated	O
through	O
a	O
relative	O
probability	O
model	O
,	O
the	O
probability	O
of	O
the	O
word	O
being	O
emitted	O
from	O
English	O
,	O
for	O
example	O
,	O
is	O
:	O
P	O
(	O
w	O
)	O
=	O
P	O
(	O
w	O
|	O
EN	O
)	O
P	O
(	O
w	O
|	O
EN	O
)	O
+	O
P	O
(	O
w	O
|	O
SP	O
A	O
)	O
,	O
where	O
P	O
(	O
w	O
|	O
EN	O
)	O
and	O
P	O
(	O
w	O
|	O
SP	O
A	O
)	O
are	O
probabilities	O
given	O
by	O
the	O
dictionaries	O
described	O
in	O
section	O
3.1	O
.	O
In	O
case	O
this	O
is	O
0	B-DatasetName
(	O
i.e.	O
the	O
word	O
does	O
not	O
occur	O
in	O
our	O
monolingual	O
data	O
)	O
,	O
the	O
emission	O
probability	O
is	O
calculated	O
by	O
a	O
relative	O
character	O
bi	O
-	O
gram	O
probability	O
.	O

To	O
evaluate	O
the	O
performance	O
of	O
our	O
models	O
,	O
we	O
use	O
weighted	O
F1	B-MetricName
score	I-MetricName
7	O
.	O
As	O
found	O
in	O
We	O
use	O
multilingual	O
BERT	B-MethodName
and	O
all	O
default	O
settings	O
.	O
Results	O
in	O
Table	O
1	O
show	O
that	O
there	O
is	O
still	O
a	O
performance	O
gap	O
between	O
the	O
semi	O
-	O
supervised	O
approaches	O
and	O
this	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
supervised	O
model	O
.	O
When	O
comparing	O
common	O
confusions	O
of	O
our	O
best	O
semi	O
-	O
supervised	O
model	O
(	O
Viterbi	O
)	O
to	O
the	O
output	O
of	O
MaChAmp	O
,	O
we	O
found	O
that	O
there	O
was	O
more	O
confusion	O
in	O
the	O
Viterbi	O
model	O
about	O
other	O
,	O
where	O
213	O
words	O
were	O
classified	O
as	O
lang1	O
and	O
60	O
as	O
lang2	O
instead	O
,	O
compared	O
to	O
just	O
3	O
and	O
1	O
in	O
MaChAmp	O
.	O
Full	O
confusion	O
matrices	O
can	O
be	O
found	O
in	O
the	O
appendix	O
.	O
The	O
majority	O
voting	O
ensembling	O
models	O
do	O
not	O
lead	O
to	O
improved	O
performance	O
.	O
However	O
,	O
the	O
oracle	O
ensemble	O
,	O
which	O
always	O
picks	O
the	O
correct	O
label	O
if	O
it	O
is	O
available	O
from	O
one	O
of	O
the	O
models	O
,	O
shows	O
that	O
there	O
is	O
potential	O
in	O
improving	O
the	O
selection	O
method	O
for	O
ensembling	O
.	O

When	O
inspecting	O
the	O
performances	O
of	O
the	O
models	O
per	O
class	O
(	O
see	O
also	O
Table	O
2	O
in	O
the	O
appendix	O
)	O
,	O
we	O
found	O
that	O
,	O
for	O
the	O
development	O
dataset	O
,	O
all	O
models	O
have	O
a	O
better	O
F1	B-MetricName
score	I-MetricName
for	O
English	O
than	O
for	O
Spanish	O
and	O
,	O
for	O
the	O
test	O
dataset	O
,	O
the	O
other	O
way	O
around	O
.	O
This	O
might	O
be	O
due	O
to	O
a	O
discrepancy	O
between	O
the	O
label	O
distribution	O
of	O
the	O
two	O
datasets	O
and	O
is	O
a	O
significant	O
aspect	O
to	O
be	O
investigated	O
in	O
future	O
work	O
.	O
Regarding	O
the	O
LDA	B-MethodName
model	O
,	O
its	O
low	O
performance	O
can	O
be	O
explained	O
by	O
the	O
results	O
of	O
(	O
Zhang	O
et	O
al	O
,	O
2014	O
)	O
,	O
which	O
show	O
that	O
for	O
the	O
task	O
of	O
language	O
filtering	O
,	O
the	O
performance	O
of	O
LDA	B-MethodName
decreases	O
when	O
the	O
dominating	O
language	O
decreases	O
under	O
70	O
%	O
of	O
the	O
whole	O
text	O
.	O
This	O
is	O
also	O
the	O
case	O
in	O
our	O
experiments	O
,	O
where	O
the	O
test	O
data	O
had	O
a	O
54	O
%	O
English	O
and	O
46	O
%	O
Spanish	O
ratio	O
.	O
Furthermore	O
,	O
the	O
amount	O
of	O
evidence	O
per	O
sample	O
is	O
rather	O
low	O
compared	O
to	O
the	O
normal	O
use	O
of	O
LDA	B-MethodName
(	O
it	O
is	O
commonly	O
used	O
on	O
the	O
document	O
level	O
)	O
.	O
For	O
character	O
n	O
-	O
grams	O
,	O
we	O
observed	O
that	O
the	O
more	O
we	O
increased	O
the	O
value	O
of	O
n	O
,	O
the	O
better	O
results	O
we	O
got	O
,	O
up	O
until	O
n	O
=	O
6	O
.	O
The	O
higher	O
order	O
n	O
-	O
grams	O
performed	O
better	O
with	O
around	O
12	O
%	O
difference	O
in	O
validation	O
weighted	O
F1	B-MetricName
score	I-MetricName
,	O
as	O
we	O
can	O
capture	O
groups	O
of	O
letters	O
that	O
are	O
representative	O
for	O
a	O
language	O
,	O
e.g.	O
'	O
tion	O
'	O
in	O
English	O
and	O
'	O
cion	O
'	O
in	O
Spanish	O
.	O
This	O
model	O
achieves	O
good	O
results	O
also	O
because	O
it	O
addresses	O
the	O
problem	O
of	O
misspelled	O
words	O
.	O
For	O
word	O
n	O
-	O
grams	O
,	O
using	O
tri	O
-	O
grams	O
resulted	O
in	O
worse	O
predictions	O
than	O
using	O
bi	O
-	O
grams	O
with	O
around	O
11	O
%	O
difference	O
in	O
validation	O
weighted	O
F1	B-MetricName
score	I-MetricName
.	O
For	O
LDA	B-MethodName
,	O
SVM	B-MethodName
and	O
Logistic	B-MethodName
Regression	I-MethodName
models	O
we	O
tried	O
to	O
vectorize	O
data	O
with	O
CountVectorizer	O
from	O
Scikit	O
Learn	O
,	O
which	O
gives	O
the	O
termfrequency	O
for	O
each	O
n	O
-	O
gram	O
in	O
a	O
word	O
.	O
However	O
,	O
TfidfVectorizer	O
performed	O
approximately	O
1	O
%	O
better	O
in	O
LDA	B-MethodName
and	O
Logistic	B-MethodName
Regression	I-MethodName
and	O
4	O
%	O
for	O
SVM	B-MethodName
in	O
validation	O
data	O
.	O
This	O
was	O
then	O
the	O
preferred	O
vectorizer	O
in	O
all	O
models	O
,	O
as	O
it	O
helps	O
decreasing	O
the	O
impact	O
of	O
very	O
frequent	O
character	O
n	O
-	O
grams	O
that	O
are	O
not	O
expressing	O
much	O
value	O
and	O
gives	O
more	O
importance	O
to	O
less	O
frequent	O
character	O
n	O
-	O
grams	O
.	O
The	O
fact	O
that	O
the	O
oracle	O
model	O
has	O
a	O
3	O
%	O
higher	O
weighted	O
F1	B-MetricName
score	I-MetricName
than	O
the	O
best	O
model	O
(	O
in	O
validation	O
data	O
)	O
,	O
suggests	O
that	O
there	O
is	O
room	O
for	O
improvement	O
for	O
the	O
ensemble	O
model	O
with	O
other	O
methods	O
than	O
majority	O
voting	O
.	O
Improvements	O
on	O
the	O
single	O
models	O
could	O
be	O
achieved	O
by	O
using	O
bigger	O
monolingual	O
datasets	O
of	O
the	O
same	O
size	O
or	O
selecting	O
a	O
corpus	O
that	O
is	O
more	O
similar	O
to	O
the	O
test	O
set	O
(	O
social	O
media	O
-	O
like	O
posts	O
)	O
,	O
which	O
is	O
not	O
as	O
easy	O
to	O
query	O
as	O
Wikipedia	O
articles	O
.	O
The	O
overall	O
performance	O
of	O
the	O
models	O
can	O
also	O
be	O
slightly	O
improved	O
by	O
a	O
more	O
complex	O
method	O
for	O
the	O
other	O
class	O
(	O
the	O
existing	O
rule	O
-	O
based	O
method	O
scored	O
an	O
F1	B-MetricName
of	O
96.76	O
,	O
see	O
Table	O
2	O
in	O
the	O
appendix	O
)	O
.	O
The	O
training	O
efficiency	O
of	O
the	O
Viterbi	O
model	O
and	O
the	O
supervised	O
model	O
were	O
measured	O
in	O
a	O
Windows	O
Sub	O
-	O
system	O
for	O
Linux	B-DatasetName
environment	O
on	O
an	O
i7	O
-	O
7700	O
K	O
processor	O
with	O
16	O
GB	O
ram	O
.	O
We	O
ran	O
the	O
MaChAmp	O
model	O
in	O
this	O
environment	O
and	O
it	O
completed	O
in	O
53	O
,	O
990	O
seconds	O
.	O
In	O
comparison	O
,	O
the	O
Viterbi	O
training	O
completed	O
in	O
1	O
,	O
805	O
seconds	O
,	O
which	O
is	O
an	O
improvement	O
of	O
almost	O
30	O
times	O
faster	O
than	O
the	O
MaChAmp	O
model	O
.	O

In	O
this	O
study	O
we	O
evaluated	O
different	O
types	O
of	O
models	O
,	O
namely	O
word	O
uni	O
-	O
grams	O
,	O
word	O
n	O
-	O
grams	O
,	O
character	O
n	O
-	O
grams	O
,	O
Viterbi	O
Decoding	O
,	O
Latent	O
Dirichlet	O
Allocation	O
,	O
Support	B-MethodName
Vector	I-MethodName
Machine	I-MethodName
and	O
Logistic	B-MethodName
Regression	I-MethodName
,	O
for	O
the	O
task	O
of	O
semi	O
-	O
supervised	O
language	B-TaskName
identification	I-TaskName
in	O
English	O
-	O
Spanish	O
codeswitched	O
data	O
.	O
We	O
found	O
that	O
most	O
of	O
the	O
models	O
achieved	O
promising	O
results	O
,	O
however	O
,	O
the	O
Viterbi	O
model	O
performed	O
the	O
best	O
with	O
a	O
weighted	O
F1	B-MetricName
score	I-MetricName
of	O
95.76	O
%	O
on	O
validation	O
data	O
and	O
92.23	O
%	O
on	O
test	O
data	O
(	O
RQ1	O
)	O
.	O
Using	O
this	O
model	O
,	O
one	O
can	O
potentially	O
train	O
CS	B-DatasetName
-	O
detection	O
for	O
many	O
more	O
language	O
pairs	O
as	O
previously	O
possible	O
.	O
Furthermore	O
,	O
since	O
the	O
majority	O
voting	O
did	O
not	O
lead	O
to	O
improvements	O
,	O
we	O
experimented	O
with	O
an	O
Oracle	O
model	O
,	O
which	O
showed	O
that	O
by	O
combining	O
results	O
form	O
our	O
models	O
,	O
the	O
best	O
score	O
we	O
could	O
achieve	O
is	O
98.47	O
%	O
on	O
validation	O
data	O
.	O
Even	O
though	O
the	O
results	O
were	O
good	O
,	O
our	O
models	O
still	O
underperformed	O
compared	O
to	O
the	O
supervised	O
MaChAmp	O
model	O
,	O
that	O
scored	O
99.24	O
%	O
weighted	O
F1	B-MetricName
score	I-MetricName
on	O
validation	O
data	O
and	O
98.43	O
%	O
on	O
test	O
data	O
(	O
RQ2	O
)	O
.	O
There	O
is	O
also	O
a	O
clear	O
take	O
away	O
that	O
,	O
by	O
using	O
simpler	O
,	O
faster	O
approaches	O
like	O
ours	O
and	O
when	O
top	O
performance	O
is	O
not	O
crucial	O
,	O
one	O
can	O
avoid	O
the	O
extensive	O
process	O
of	O
human	O
-	O
annotation	O
and	O
long	O
training	O
time	O
that	O
are	O
needed	O
by	O
finetuning	O
these	O
large	O
transformer	O
models	O
on	O
supervised	O
data	O
.	O
It	O
can	O
be	O
noted	O
that	O
the	O
confusion	O
matrix	O
for	O
MaChAmp	O
model	O
has	O
more	O
than	O
the	O
three	O
labels	O
we	O
used	O
,	O
because	O
it	O
was	O
trained	O
on	O
part	O
of	O
the	O
original	O
training	O
set	O
presented	O
in	O
Section	O
2.1	O
.	O
This	O
set	O
contained	O
8	O
classes	O
,	O
and	O
,	O
thus	O
,	O
occasionally	O
,	O
the	O
model	O
mistakenly	O
predicted	O
some	O
of	O
these	O
classes	O
.	O
It	O
can	O
be	O
seen	O
that	O
there	O
was	O
more	O
confusion	O
in	O
Viterbi	O
model	O
about	O
other	O
,	O
where	O
213	O
words	O
were	O
classified	O
as	O
lang1	O
and	O
60	O
as	O
lang2	O
instead	O
,	O
compared	O
to	O
just	O
3	O
and	O
1	O
in	O
MaChAmp	O
,	O
which	O
also	O
had	O
7	O
other	O
misclassifications	O
.	O

Relation	B-TaskName
extraction	I-TaskName
is	O
a	O
core	O
natural	O
language	O
processing	O
task	O
which	O
is	O
concerned	O
with	O
the	O
extraction	O
of	O
relations	O
between	O
entities	O
from	O
text	O
.	O
It	O
has	O
numerous	O
applications	O
ranging	O
from	O
question	B-TaskName
answering	I-TaskName
(	O
Xu	O
et	O
al	O
,	O
2016	O
)	O
to	O
automated	O
knowledge	O
base	O
construction	O
(	O
Dong	O
et	O
al	O
,	O
2014	O
)	O
.	O
While	O
the	O
vast	O
majority	O
of	O
existing	O
research	O
focuses	O
on	O
extracting	O
binary	O
relations	O
,	O
there	O
exists	O
only	O
few	O
recent	O
approaches	O
to	O
extract	O
n	O
-	O
ary	O
relations	O
,	O
that	O
is	O
,	O
relations	O
among	O
n	O
≥	O
2	O
entities	O
(	O
Li	O
et	O
al	O
,	O
2015	O
;	O
Ernst	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
,	O
relation	O
mentions	O
tend	O
to	O
span	O
multiple	O
sentences	O
more	O
frequently	O
as	O
n	O
increases	O
.	O
Thus	O
,	O
Peng	O
et	O
al	O
(	O
2017	O
)	O
recently	O
extended	O
the	O
problem	O
to	O
cross	O
-	O
sentence	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
in	O
which	O
n	O
-	O
ary	O
relations	O
are	O
extracted	O
from	O
multiple	O
sentences	O
.	O
As	O
a	O
motivating	O
example	O
,	O
consider	O
the	O
following	O
text	O
from	O
Wikipedia	O
:	O
"	O
Revis	O
started	O
off	O
the	O
2009	O
season	O
matched	O
up	O
against	O
some	O
of	O
football	O
's	O
best	O
wide	O
receivers	O
.	O
In	O
Week	O
1	O
,	O
he	O
helped	O
limit	O
Houston	B-DatasetName
Texans	O
Pro	O
-	O
bowler	O
Andre	O
Johnson	O
to	O
four	O
receptions	O
for	O
35	O
yards	O
.	O
"	O
In	O
this	O
example	O
,	O
two	O
sentences	O
collectively	O
describes	O
that	O
Andre	O
Johnson	O
is	O
a	O
player	O
of	O
the	O
football	O
team	O
the	O
Texans	O
during	O
2009	O
season	O
,	O
and	O
thus	O
we	O
need	O
cross	O
-	O
sentence	O
information	O
to	O
correctly	O
extract	O
this	O
ternary	O
interaction	O
among	O
the	O
three	O
entities	O
,	O
i.e.	O
Player	O
(	O
Andre	O
Johnson	O
,	O
Texans	O
,	O
2009	O
season	O
)	O
.	O
Previous	O
methods	O
(	O
Peng	O
et	O
al	O
,	O
2017	O
;	O
Song	O
et	O
al	O
,	O
2018	O
)	O
capture	O
cross	O
-	O
sentence	O
n	O
-	O
ary	O
relation	O
mentions	O
by	O
representing	O
texts	O
with	O
a	O
document	O
graph	O
which	O
consists	O
of	O
both	O
intra	O
-	O
and	O
cross	O
-	O
sentence	O
links	O
between	O
words	O
.	O
With	O
this	O
graphical	O
representation	O
,	O
they	O
applied	O
graph	O
neural	O
networks	O
to	O
predict	O
ternary	O
relations	O
in	O
the	O
medical	B-DatasetName
domain	I-DatasetName
.	O
However	O
,	O
these	O
methods	O
train	O
the	O
neural	O
networks	O
in	O
a	O
supervised	O
manner	O
using	O
distant	O
supervision	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
)	O
and	O
,	O
therefore	O
,	O
may	O
suffer	O
from	O
the	O
lack	O
of	O
sufficient	O
positive	O
labels	O
when	O
a	O
well	O
-	O
populated	O
knowledge	O
base	O
is	O
not	O
available	O
.	O
On	O
the	O
other	O
hand	O
,	O
for	O
binary	B-TaskName
relation	I-TaskName
extraction	I-TaskName
,	O
the	O
problem	O
of	O
insufficient	O
positive	O
labels	O
can	O
be	O
mitigated	O
with	O
universal	O
schemas	O
.	O
In	O
a	O
universal	O
schema	O
approach	O
,	O
textual	O
representations	O
(	O
surface	O
patterns	O
)	O
of	O
entities	O
and	O
their	O
relations	O
are	O
encoded	O
into	O
the	O
same	O
vector	O
space	O
as	O
the	O
canonical	O
knowledge	O
base	O
relations	O
.	O
Thus	O
,	O
semantically	O
similar	O
surface	O
patterns	O
can	O
share	O
information	O
of	O
relation	O
labels	O
in	O
a	O
semisupervised	O
manner	O
.	O
This	O
reduces	O
the	O
amount	O
of	O
required	O
labeled	O
training	O
data	O
.	O
Applying	O
the	O
universal	O
schema	O
approach	O
to	O
n	O
-	O
ary	O
(	O
n	O
>	O
2	O
)	O
relation	B-TaskName
extraction	I-TaskName
is	O
,	O
however	O
,	O
not	O
straight	O
-	O
forward	O
due	O
to	O
the	O
sparsity	O
of	O
higher	O
-	O
order	O
relation	O
mentions	O
among	O
a	O
specific	O
set	O
of	O
n	O
>	O
2	O
entities	O
.	O
1	O
This	O
is	O
be	O
-	O
cause	O
the	O
universal	O
schema	O
approach	O
and	O
its	O
extensions	O
(	O
Toutanova	O
et	O
al	O
,	O
2015	O
;	O
Verga	O
et	O
al	O
,	O
2016Verga	O
et	O
al	O
,	O
,	O
2017	O
utilize	O
co	O
-	O
occurring	O
patterns	O
of	O
relation	O
types	O
between	O
specific	O
pair	O
of	O
entities	O
.	O
Also	O
,	O
prior	O
work	O
has	O
only	O
addressed	O
binary	O
relations	O
,	O
and	O
it	O
is	O
not	O
trivial	O
to	O
define	O
surface	O
patterns	O
among	O
n	O
>	O
2	O
entities	O
and	O
to	O
encode	O
these	O
patterns	O
into	O
a	O
vector	O
representation	O
.	O
To	O
mitigate	O
the	O
aforementioned	O
sparsity	O
problem	O
and	O
utilize	O
existing	O
encoders	O
for	O
binary	O
and	O
unary	O
surface	O
patterns	O
,	O
we	O
propose	O
to	O
train	O
universal	O
schema	O
models	O
on	O
more	O
dense	O
lower	O
-	O
arity	O
(	O
unary	O
and	O
binary	O
)	O
facts	O
instead	O
of	O
original	O
sparse	O
n	O
-	O
ary	O
facts	O
.	O
Since	O
most	O
n	O
-	O
ary	O
relations	O
can	O
be	O
decomposed	O
into	O
a	O
set	O
of	O
k	O
-	O
ary	O
relations	O
(	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
,	O
2	O
)	O
which	O
are	O
implied	O
by	O
the	O
n	O
-	O
ary	O
relation	O
,	O
2	O
we	O
can	O
easily	O
acquire	O
lower	O
-	O
arity	O
facts	O
by	O
decomposing	O
n	O
-	O
ary	O
facts	O
.	O
Our	O
model	O
learns	O
representations	O
of	O
these	O
lower	O
-	O
arity	O
relations	O
using	O
the	O
universal	O
schema	O
framework	O
,	O
and	O
predicts	O
new	O
n	O
-	O
ary	O
facts	O
by	O
aggregating	O
scores	O
of	O
lower	O
-	O
arity	O
facts	O
.	O
To	O
evaluate	O
the	O
proposed	O
method	O
,	O
we	O
create	O
new	O
cross	O
-	O
sentence	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
datasets	O
with	O
multiple	O
ternary	O
relations	O
.	O
3	O
The	O
new	O
datasets	O
contain	O
more	O
entity	O
tuples	O
with	O
known	O
relational	O
facts	O
appeared	O
in	O
a	O
knowledge	O
base	O
than	O
the	O
existing	O
dataset	O
(	O
Peng	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
,	O
therefore	O
,	O
these	O
datasets	O
can	O
be	O
used	O
to	O
more	O
effectively	O
evaluate	O
methods	O
which	O
predict	O
relation	O
labels	O
for	O
each	O
individual	O
entity	O
tuple	O
.	O
We	O
show	O
empirically	O
that	O
by	O
jointly	O
training	O
lower	O
-	O
arity	O
models	O
and	O
an	O
nary	O
score	O
aggregation	O
model	O
,	O
the	O
proposed	O
method	O
improves	O
the	O
performance	O
of	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
attempt	O
to	O
apply	O
universal	O
schemas	O
to	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
,	O
taking	O
advantage	O
of	O
the	O
compositionality	O
of	O
higher	O
-	O
arity	O
facts	O
.	O

To	O
alleviate	O
the	O
sparsity	O
problem	O
of	O
facts	O
among	O
n	O
entities	O
(	O
n	O
>	O
2	O
)	O
and	O
to	O
utilize	O
well	O
-	O
studied	O
encoders	O
for	O
binary	O
and	O
unary	O
surface	O
patterns	O
,	O
we	O
decompose	O
a	O
set	O
of	O
original	O
n	O
-	O
ary	O
facts	O
,	O
O	O
,	O
into	O
a	O
set	O
of	O
unary	O
facts	O
O	O
1	O
and	O
a	O
set	O
of	O
binary	O
facts	O
O	O
2	O
(	O
Figure	O
1	O
)	O
.	O
Unary	O
Facts	O
:	O
Given	O
an	O
n	O
-	O
ary	O
fact	O
r	O
,	O
(	O
e	O
1	O
,	O
...	O
,	O
e	O
k	O
)	O
O	O
,	O
we	O
decompose	O
it	O
into	O
a	O
set	O
of	O
n	O
unary	O
facts	O
{	O
r	O
(	O
k	O
)	O
,	O
e	O
k	O
:	O
k	B-HyperparameterName
=	I-HyperparameterName
1	O
,	O
...	O
,	O
n	O
}	O
,	O
where	O
r	O
(	O
k	O
)	O
is	O
a	O
tentative	O
unary	O
relation	O
w.r.t	O
.	O
the	O
k	O
-	O
th	O
argument	O
of	O
the	O
original	O
relation	O
r.	O
If	O
r	O
is	O
a	O
KB	O
relation	O
,	O
we	O
define	O
unary	O
relation	O
r	O
(	O
k	O
)	O
as	O
a	O
new	O
canonicalized	O
relation	O
.	O
If	O
r	O
is	O
section	O
T	O
,	O
we	O
define	O
unary	O
relation	O
r	O
(	O
k	O
)	O
as	O
a	O
tuple	O
r	O
(	O
k	O
)	O
=	O
(	O
T	O
,	O
pos	O
(	O
e	O
k	O
)	O
)	O
,	O
where	O
pos	O
(	O
e	O
k	O
)	O
is	O
a	O
set	O
of	O
word	O
position	O
indices	O
of	O
entity	O
e	O
k	O
in	O
section	O
T	O
(	O
Figure	O
2	O
)	O
.	O
We	O
denote	O
a	O
set	O
of	O
all	O
decomposed	O
unary	O
facts	O
by	O
O	O
1	O
.	O
Intuitively	O
,	O
these	O
unary	O
relations	O
represent	O
semantic	O
roles	O
or	O
types	O
of	O
corresponding	O
arguments	O
of	O
the	O
original	O
relation	O
r	O
.	O
Binary	O
Facts	O
:	O
Given	O
an	O
n	O
-	O
ary	O
fact	O
r	O
,	O
(	O
e	O
1	O
,	O
...	O
,	O
e	O
k	O
)	O
O	O
,	O
we	O
decompose	O
it	O
into	O
a	O
set	O
of	O
n	O
(	O
n	O
−	O
1	O
)	O
binary	O
facts	O
{	O
r	O
(	O
k	O
,	O
l	O
)	O
,	O
(	O
e	O
k	O
,	O
e	O
l	O
)	O
:	O
k	O
,	O
l	O
=	O
1	O
,	O
...	O
,	O
n	O
,	O
k	B-HyperparameterName
=	I-HyperparameterName
l	O
}	O
,	O
where	O
r	O
(	O
k	O
,	O
l	O
)	O
is	O
a	O
tentative	O
binary	O
relation	O
between	O
the	O
k	O
-	O
th	O
and	O
l	O
-	O
th	O
argument	O
of	O
the	O
original	O
relation	O
r.	O
If	O
r	O
is	O
a	O
KB	O
relation	O
,	O
we	O
define	O
binary	O
relation	O
r	O
(	O
k	O
,	O
l	O
)	O
as	O
a	O
new	O
canonicalized	O
relation	O
.	O
If	O
r	O
is	O
a	O
section	O
T	O
,	O
we	O
represent	O
it	O
by	O
the	O
shortest	O
path	O
between	O
e	O
k	O
and	O
e	O
l	O
on	O
the	O
document	O
graph	O
(	O
Quirk	O
and	O
Poon	O
,	O
2017	O
)	O
of	O
T	O
(	O
Figure	O
2	O
)	O
,	O
and	O
denote	O
it	O
by	O
path	O
(	O
T	O
;	O
e	O
k	O
,	O
e	O
l	O
)	O
.	O
We	O
denote	O
the	O
set	O
of	O
all	O
decomposed	O
binary	O
facts	O
by	O
O	O
2	O
.	O

We	O
follow	O
Verga	O
et	O
al	O
(	O
2017	O
)	O
to	O
train	O
relation	O
representations	O
(	O
3.2	O
)	O
.	O
We	O
define	O
a	O
score	O
θ	B-HyperparameterName
r	O
,	O
p	O
for	O
each	O
lower	O
-	O
arity	O
fact	O
r	O
,	O
p	O
O	O
1	O
∪	O
O	O
2	O
,	O
and	O
minimize	O
the	O
following	O
loss	B-MetricName
(	O
3	O
)	O
for	O
each	O
arity	O
i	O
=	O
1	O
,	O
2	O
.	O
Here	O
,	O
placeholder	O
p	O
refers	O
to	O
either	O
an	O
entity	O
(	O
if	O
r	O
,	O
p	O
O	O
1	O
)	O
or	O
an	O
entity	O
tuple	O
(	O
if	O
r	O
,	O
p	O
O	O
2	O
)	O
,	O
and	O
we	O
simply	O
refer	O
to	O
both	O
as	O
entity	O
tuple	O
.	O
The	O
loss	B-MetricName
functions	O
contrast	O
a	O
score	O
of	O
an	O
original	O
fact	O
r	O
,	O
p	O
+	O
O	O
i	O
and	O
those	O
of	O
K	O
sampled	O
negative	O
facts	O
r	O
,	O
p	O
−	O
k	O
/	O
O	O
i	O
.	O
We	O
sample	O
negative	O
facts	O
by	O
randomly	O
replacing	O
entity	O
tuple	O
p	O
+	O
in	O
the	O
original	O
fact	O
by	O
different	O
entity	O
tuples	O
p	O
−	O
k	O
.	O
L	O
i	O
=	O
E	O
r	O
,	O
p	O
+	O
O	O
i	O
r	O
,	O
p	O
−	O
k	O
/	O
O	O
i	O
[	O
−	O
log	O
(	O
exp	O
(	O
θ	B-HyperparameterName
r	O
,	O
p	O
+	O
)	O
exp	O
(	O
θ	B-HyperparameterName
r	O
,	O
p	O
+	O
)	O
+	O
k	O
exp	O
(	O
θ	B-HyperparameterName
r	O
,	O
p	O
−	O
k	O
)	O
)	O
]	O
.	O
(	O
3	O
)	O
The	O
score	O
of	O
fact	O
r	O
,	O
p	O
is	O
defined	O
as	O
θ	B-HyperparameterName
r	O
,	O
p	O
=	O
v	O
(	O
r	O
)	O
T	O
v	O
(	O
p	O
;	O
r	O
)	O
.	O
Entity	O
tuple	O
representations	O
v	O
(	O
p	O
;	O
r	O
)	O
are	O
computed	O
with	O
a	O
weighted	O
average	O
of	O
the	O
representations	O
{	O
v	O
(	O
r	O
)	O
:	O
r	O
V	O
(	O
p	O
)	O
}	O
as	O
shown	O
in	O
(	O
4	O
)	O
and	O
(	O
5	O
)	O
where	O
a	O
(	O
r	O
,	O
r	O
;	O
V	O
(	O
p	O
)	O
)	O
is	O
the	O
attention	O
weight	O
for	O
each	O
relation	O
r	O
V	O
(	O
p	O
)	O
.	O
5	O
v	O
(	O
p	O
;	O
r	O
)	O
=	O
r	O
V	O
(	O
p	O
)	O
a	O
(	O
r	O
,	O
r	O
;	O
V	O
(	O
p	O
)	O
)	O
v	O
(	O
r	O
)	O
,	O
a	O
(	O
r	O
,	O
r	O
;	O
V	O
(	O
p	O
)	O
)	O
=	O
exp	O
(	O
v	O
(	O
r	O
)	O
T	O
v	O
(	O
r	O
)	O
)	O
r	O
V	O
(	O
p	O
)	O
exp	O
(	O
v	O
(	O
r	O
)	O
T	O
v	O
(	O
r	O
)	O
)	O
.	O
(	O
4	O
)	O
V	O
(	O
p	O
)	O
=	O
{	O
r	O
:	O
r	O
,	O
e	O
k	O
O	O
1	O
}	O
(	O
if	O
p	O
=	O
e	O
k	O
)	O
{	O
r	O
:	O
r	O
,	O
(	O
e	O
k	O
,	O
e	O
l	O
)	O
O	O
2	O
}	O
(	O
if	O
p	O
=	O
(	O
e	O
k	O
,	O
e	O
l	O
)	O
)	O
.	O
(	O
5	O
)	O

To	O
predict	O
n	O
-	O
ary	O
facts	O
of	O
KB	O
relation	O
r	O
R	O
KB	O
,	O
we	O
compute	O
its	O
score	O
θ	B-HyperparameterName
r	O
,	O
(	O
e	O
1	O
,	O
...	O
,	O
en	O
)	O
by	O
aggregating	O
lower	O
-	O
arity	O
scores	O
as	O
in	O
(	O
6	O
)	O
,	O
where	O
w	O
(	O
)	O
r	O
is	O
a	O
positive	O
scalar	O
weight	O
defined	O
for	O
each	O
KB	O
relation	O
which	O
sum	O
to	O
one	O
:	O
k	O
w	O
(	O
k	O
)	O
r	O
+	O
k	B-HyperparameterName
=	I-HyperparameterName
l	O
w	O
(	O
k	O
,	O
l	O
)	O
r	O
=	O
1	O
.	O
We	O
can	O
set	O
al	O
weights	O
w	O
(	O
k	O
)	O
r	O
and	O
w	O
(	O
k	O
,	O
l	O
)	O
r	O
to	O
1	O
/	O
n	O
2	O
,	O
or	O
train	O
these	O
weights	O
to	O
give	O
higher	O
scores	O
to	O
positive	O
n	O
-	O
ary	O
facts	O
by	O
minimizing	O
additional	O
loss	B-MetricName
function	O
L	O
n	O
.	O
Note	O
that	O
L	O
n	O
directly	O
contrasts	O
n	O
-	O
ary	O
scores	O
associated	O
with	O
KB	O
relations	O
r	O
R	O
KB	O
in	O
a	O
more	O
supervised	O
manner	O
than	O
both	O
L	O
1	O
and	O
L	O
2	O
.	O
6	O
(	O
6	O
)	O
L	O
n	O
=	O
E	O
r	O
,	O
p	O
+	O
O	O
KB	O
r	O
,	O
p	O
−	O
/	O
O	O
KB	O
[	O
max	O
(	O
0	B-DatasetName
,	O
1	O
−	O
θ	B-HyperparameterName
r	O
,	O
p	O
+	O
+	O
θ	B-HyperparameterName
r	O
,	O
p	O
−	O
)	O
]	O
(	O
7	O
)	O
The	O
overall	O
loss	B-MetricName
function	O
is	O
now	O
L	O
=	O
L	O
1	O
+	O
L	O
2	O
+	O
αL	O
n	O
.	O
By	O
changing	O
α	B-HyperparameterName
,	O
we	O
can	O
balance	O
the	O
semisupervised	O
effect	O
of	O
lower	O
-	O
arity	O
universal	O
schemas	O
(	O
L	O
1	O
,	O
L	O
2	O
)	O
and	O
that	O
of	O
the	O
supervision	O
with	O
n	O
-	O
ary	O
relation	O
labels	O
(	O
L	O
n	O
)	O
.	O

We	O
compared	O
the	O
methods	O
in	O
the	O
held	O
-	O
out	O
evaluation	O
as	O
in	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
)	O
and	O
report	O
(	O
weighted	O
)	O
mean	O
average	B-MetricName
precision	I-MetricName
(	O
MAP	B-DatasetName
)	O
.	O
Unless	O
otherwise	O
noted	O
,	O
reported	O
values	O
are	O
average	O
values	O
over	O
six	O
experiments	O
,	O
in	O
which	O
network	O
parameters	O
are	O
randomly	O
initialized	O
.	O
All	O
reported	O
p	O
-	O
values	O
are	O
calculated	O
based	O
on	O
Wilcoxon	O
rank	O
sum	O
test	O
(	O
Wilcoxon	O
,	O
1945	O
)	O
with	O

Table	O
1	O
illustrates	O
the	O
performance	O
of	O
each	O
method	O
.	O
12	O
Compared	O
to	O
the	O
baseline	O
methods	O
,	O
our	O
proposed	O
method	O
achieves	O
higher	O
weighted	O
MAP	B-DatasetName
for	O
both	O
datasets	O
.	O
Interestingly	O
,	O
Model	O
F	O
performs	O
well	O
in	O
Verga	O
et	O
al	O
(	O
2017	O
)	O
baseline	O
,	O
while	O
it	O
shows	O
low	O
performance	O
in	O
Toutanova	O
et	O
al	O
(	O
2015	O
)	O
baseline	O
.	O
Ablation	O
Study	O
:	O
Table	O
2	O
illustrates	O
the	O
performance	O
of	O
various	O
settings	O
of	O
our	O
proposed	O
method	O
.	O
U	O
,	O
B	O
,	O
and	O
N	O
stand	O
for	O
using	O
the	O
loss	B-MetricName
functions	O
L	O
1	O
,	O
L	O
2	O
,	O
and	O
αL	O
n	O
respectively	O
.	O
In	O
the	O
result	O
,	O
U+B	O
performs	O
significantly	O
better	O
(	O
p	O
<	O
0.005	O
)	O
than	O
U	O
and	O
B	O
,	O
and	O
this	O
shows	O
effectiveness	O
of	O
combining	O
scores	O
of	O
both	O
binary	O
facts	O
and	O
unary	O
facts	O
.	O
On	O
the	O
other	O
hand	O
,	O
there	O
was	O
no	O
significant	O
difference	O
between	O
U+B+N	O
and	O
N	O
(	O
p	O
>	O
0.9	O
)	O
.	O
Note	O
that	O
we	O
used	O
all	O
positive	O
labels	O
in	O
this	O
experiment	O
,	O
that	O
is	O
,	O
sufficient	O
amount	O
of	O
positive	O
labels	O
are	O
used	O
for	O
calculating	O
the	O
loss	B-MetricName
N.	O
Data	O
efficiency	O
:	O
Furthermore	O
,	O
we	O
also	O
investigated	O
the	O
influence	O
of	O
the	O
training	O
data	O
size	O
(	O
the	O
number	O
of	O
positive	O
labels	O
)	O
of	O
our	O
proposed	O
method	O
and	O
baseline	O
methods	O
.	O
13	O
Here	O
,	O
α	B-HyperparameterName
=	O
stands	O
for	O
optimizing	O
L	O
n	O
instead	O
of	O
L	O
1	O
+	O
L	O
2	O
+	O
αL	O
n	O
.	O
As	O
shown	O
in	O
Figure	O
3	O
,	O
α	B-HyperparameterName
=	O
1	O
achieved	O
higher	O
performance	O
than	O
α	B-HyperparameterName
=	O
,	O
showing	O
that	O
introducing	O
lower	O
-	O
arity	O
semi	O
-	O
supervised	O
loss	B-MetricName
(	O
L	O
1	O
+	O
L	O
2	O
)	O
improves	O
the	O
performance	O
for	O
dataset	O
with	O
few	O
positive	O
labels	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
lower	O
performance	O
of	O
α	B-HyperparameterName
=	O
0	B-DatasetName
compared	O
to	O
α	B-HyperparameterName
=	O
0.1	O
,	O
1	O
suggests	O
that	O
information	O
of	O
higher	O
-	O
arity	O
facts	O
introduced	O
from	O
L	O
n	O
is	O
benefitial	O
for	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
.	O

We	O
proposed	O
a	O
new	O
method	O
for	O
cross	O
-	O
sentence	O
nary	O
relation	B-TaskName
extraction	I-TaskName
that	O
decomposes	O
sparse	O
n	O
-	O
12	O
For	O
the	O
proposed	O
method	O
,	O
we	O
set	O
α	B-HyperparameterName
=	O
10	O
.	O
13	O
In	O
this	O
experiment	O
,	O
we	O
conducted	O
four	O
experiments	O
per	O
each	O
setting	O
and	O
set	O
K	B-HyperparameterName
=	I-HyperparameterName
10	O
.	O
ary	O
facts	O
into	O
dense	O
unary	O
and	O
binary	O
facts	O
.	O
Experiments	O
on	O
two	O
datasets	O
with	O
multiple	O
ternary	O
relations	O
show	O
that	O
our	O
proposed	O
method	O
can	O
statistically	O
significantly	O
improve	O
over	O
previous	O
works	O
,	O
which	O
suggests	O
the	O
effectiveness	O
of	O
using	O
unary	O
and	O
binary	O
interaction	O
among	O
entities	O
in	O
surface	O
patterns	O
.	O
However	O
,	O
as	O
Fatemi	O
et	O
al	O
(	O
2019	O
)	O
suggests	O
,	O
there	O
exists	O
cases	O
in	O
which	O
reconstructing	O
n	O
-	O
ary	O
facts	O
from	O
decomposed	O
binary	O
facts	O
induces	O
false	O
positives	O
.	O
Tackling	O
this	O
issue	O
is	O
one	O
important	O
future	O
research	O
direction	O
.	O

ZAR	O
as	O
argument	O
selection	O
As	O
illustrated	O
in	O
Figure	O
3	O
,	O
the	O
basic	O
idea	O
behind	O
BERT	B-MethodName
-	O
based	O
ZAR	O
is	O
that	O
given	O
the	O
powerful	O
neural	O
encoder	O
,	O
the	O
joint	O
task	O
of	O
omission	O
detection	O
and	O
antecedent	O
identification	O
can	O
be	O
formalized	O
as	O
argument	O
selection	O
(	O
Shibata	O
and	O
Kurohashi	O
,	O
2018	O
;	O
Kurita	O
et	O
al	O
,	O
2018	O
;	O
Ueda	O
et	O
al	O
,	O
2020	O
)	O
.	O
Omission	O
detection	O
concerns	O
whether	O
a	O
given	O
predicate	O
has	O
an	O
argument	O
for	O
a	O
given	O
case	O
(	O
relation	O
)	O
.	O
If	O
not	O
,	O
the	O
model	O
must	O
point	O
to	O
the	O
special	O
token	O
[	O
NULL	O
]	O
.	O
Otherwise	O
the	O
model	O
must	O
identify	O
the	O
antecedent	O
of	O
the	O
zero	O
pronoun	O
by	O
pointing	O
either	O
to	O
a	O
token	O
in	O
the	O
given	O
text	O
or	O
to	O
a	O
special	O
token	O
reserved	O
for	O
exophora	O
.	O
Note	O
that	O
by	O
getting	O
the	O
entire	O
document	O
as	O
the	O
input	O
,	O
the	O
model	O
can	O
handle	O
inter	O
-	O
sentential	O
anaphora	O
as	O
well	O
as	O
intra	O
-	O
sentential	O
anaphora	O
.	O
In	O
practice	O
,	O
the	O
input	O
length	O
limitation	O
of	O
BERT	B-MethodName
forces	O
us	O
to	O
implement	O
a	O
sliding	O
window	O
approach	O
.	O
Also	O
note	O
that	O
in	O
this	O
formulation	O
,	O
ZAR	O
is	O
naturally	O
subsumed	O
into	O
verbal	O
predicate	O
analysis	O
(	O
VPA	O
)	O
,	O
which	O
also	O
covers	O
instances	O
where	O
the	O
predicate	O
and	O
the	O
argument	O
have	O
a	O
dependency	O
relation	O
and	O
only	O
the	O
case	O
marker	O
is	O
absent	O
.	O
Formally	O
,	O
the	O
probability	O
of	O
the	O
token	O
t	O
j	O
being	O
the	O
argument	O
of	O
the	O
predicate	O
t	O
i	O
for	O
case	O
c	O
is	O
:	O
P	O
(	O
t	O
j	O
|	O
t	O
i	O
,	O
c	O
)	O
=	O
exp	O
(	O
s	O
c	O
(	O
t	O
j	O
,	O
t	O
i	O
)	O
)	O
j	O
exp	O
(	O
s	O
c	O
(	O
t	O
j	O
,	O
t	O
i	O
)	O
)	O
(	O
1	O
)	O
s	O
c	O
(	O
t	O
j	O
,	O
t	O
i	O
)	O
=	O
v	O
tanh	O
(	O
W	O
c	O
t	O
j	O
+	O
U	O
c	O
t	O
i	O
)	O
(	O
2	O
)	O
where	O
t	O
i	O
is	O
the	O
context	O
-	O
aware	O
embedding	O
of	O
t	O
i	O
provided	O
by	O
BERT	B-MethodName
,	O
W	O
c	O
and	O
U	O
c	O
are	O
case	O
-	O
specific	O
weight	O
matrices	O
,	O
and	O
v	O
is	O
a	O
weight	O
vector	O
shared	O
among	O
cases	O
.	O
We	O
output	O
t	O
j	O
with	O
the	O
highest	O
probability	O
.	O
For	O
each	O
predicate	O
,	O
we	O
repeat	O
this	O
for	O
the	O
nominative	O
(	O
NOM	O
)	O
,	O
accusative	O
(	O
ACC	B-MetricName
)	O
,	O
and	O
dative	O
(	O
DAT	O
)	O
cases	O
,	O
and	O
another	O
nominative	O
case	O
for	O
the	O
double	O
nominative	O
construction	O
(	O
NOM2	O
)	O
.	O

As	O
discussed	O
in	O
Section	O
2.2	O
,	O
MT	O
as	O
an	O
intermediate	O
task	O
reportedly	O
harms	O
target	O
-	O
task	O
performance	O
,	O
probably	O
because	O
MT	O
forces	O
the	O
model	O
to	O
forget	O
what	O
it	O
has	O
learned	O
from	O
MLM	B-DatasetName
pretraining	O
(	O
catastrophic	O
forgetting	O
)	O
.	O
To	O
overcome	O
this	O
problem	O
,	O
we	O
incorporate	O
the	O
MLM	B-DatasetName
training	O
objective	O
into	O
MT	O
,	O
as	O
suggested	O
by	O
Pruksachatkun	O
et	O
al	O
(	O
2020	O
)	O
.	O
Specifically	O
,	O
we	O
mask	O
some	O
input	O
tokens	O
on	O
the	O
encoder	O
Web	O
News	O
#	O
of	O
sentences	O
16	O
,	O
038	O
11	O
,	O
276	O
#	O
of	O
zeros	O
30	O
,	O
852	O
27	O
,	O
062	O
side	O
and	O
force	O
the	O
model	O
to	O
recover	O
the	O
original	O
tokens	O
,	O
as	O
depicted	O
in	O
the	O
center	O
of	O
Figure	O
2	O
.	O
Our	O
masking	O
strategy	O
is	O
the	O
same	O
as	O
BERT	B-MethodName
's	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
:	O
We	O
choose	O
15	O
%	O
of	O
the	O
tokens	O
at	O
random	O
and	O
80	O
%	O
of	O
them	O
are	O
replaced	O
with	O
[	O
MASK	O
]	O
,	O
10	O
%	O
of	O
them	O
with	O
a	O
random	O
token	O
,	O
and	O
the	O
rest	O
are	O
unchanged	O
.	O
The	O
corresponding	O
losses	O
are	O
added	O
to	O
the	O
MT	O
loss	B-MetricName
function	O
.	O

BERT	B-MethodName
We	O
employed	O
a	O
Japanese	O
BERT	B-MethodName
model	O
with	O
BPE	B-MethodName
segmentation	O
distributed	O
by	O
NICT	O
.	O
6	O
It	O
had	O
the	O
same	O
architecture	O
as	O
Google	B-DatasetName
's	O
BERT	B-MethodName
-	O
Base	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
:	O
12	O
layers	O
,	O
768	O
hidden	O
units	O
,	O
and	O
12	O
attention	O
heads	O
.	O
It	O
was	O
trained	O
on	O
the	O
full	O
text	O
of	O
Japanese	O
Wikipedia	O
for	O
approximately	O
1	O
million	O
steps	O
.	O
MT	O
We	O
used	O
the	O
Transformer	B-MethodName
encoder	O
-	O
decoder	O
architecture	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
encoder	O
was	O
initialized	O
with	O
BERT	B-MethodName
while	O
the	O
decoder	O
was	O
a	O
randomly	O
initialized	O
six	O
-	O
layer	O
Transformer	B-MethodName
.	O
The	O
numbers	O
of	O
hidden	O
units	O
and	O
heads	O
were	O
set	O
to	O
be	O
the	O
same	O
as	O
BERT	B-MethodName
's	O
(	O
i.e.	O
,	O
768	O
units	O
and	O
12	O
attention	O
heads	O
)	O
.	O
We	O
adopted	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2017	O
)	O
as	O
the	O
optimizer	B-HyperparameterName
.	O
We	O
set	O
the	O
total	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
to	O
50	O
.	O
In	O
two	O
-	O
stage	O
optimization	O
,	O
the	O
encoder	O
was	O
frozen	O
during	O
the	O
first	O
15	O
epochs	O
,	O
then	O
the	O
entire	O
model	O
was	O
updated	O
for	O
the	O
remaining	O
35	O
epochs	O
.	O
We	O
set	O
a	O
mini	B-HyperparameterName
-	I-HyperparameterName
batch	I-HyperparameterName
size	I-HyperparameterName
to	O
about	O
500	O
.	O
The	O
details	O
of	O
hyper	O
-	O
parameters	O
are	O
given	O
in	O
Appendix	O
A.	O
ZAR	O
For	O
a	O
fair	O
comparison	O
with	O
Ueda	O
et	O
al	O
(	O
2020	O
)	O
,	O
we	O
used	O
almost	O
the	O
same	O
configuration	O
as	O
theirs	O
.	O
We	O
dealt	O
with	O
all	O
subtypes	O
of	O
ZAR	O
:	O
intra	O
-	O
sentential	O
anaphora	O
,	O
inter	O
-	O
sentential	O
anaphora	O
,	O
and	O
exophora	O
.	O
For	O
exophora	O
,	O
we	O
targeted	O
[	O
author	O
]	O
,	O
[	O
reader	O
]	O
,	O
and	O
[	O
unspecified	O
person	O
]	O
.	O
We	O
set	O
the	O
maximum	O
sequence	O
length	O
to	O
128	O
.	O
7	O
All	O
documents	O
from	O
the	O
Web	O
met	O
this	O
limitation	O
.	O
In	O
the	O
News	O
corpus	O
,	O
however	O
,	O
many	O
documents	O
exceeded	O
the	O
sequence	O
length	O
of	O
128	O
.	O
For	O
such	O
documents	O
,	O
we	O
divided	O
the	O
document	O
into	O
multiple	O
parts	O
such	O
that	O
it	O
had	O
the	O
longest	O
preceding	O
contexts	O
.	O
The	O
evaluation	O
of	O
ZAR	O
was	O
relaxed	O
using	O
a	O
gold	O
coreference	O
chain	O
.	O
The	O
model	O
was	O
trained	O
on	O
the	O
mixture	O
of	O
both	O
corpora	O
and	O
evaluated	O
on	O
each	O
corpus	O
.	O
We	O
used	O
almost	O
the	O
same	O

Web	O
News	O
hyper	O
-	O
parameters	O
as	O
Ueda	O
et	O
al	O
(	O
2020	O
)	O
,	O
which	O
are	O
included	O
in	O
Appendix	O
B.	O
We	O
decided	O
to	O
tune	O
the	O
training	O
epochs	O
for	O
MT	O
since	O
we	O
found	O
that	O
it	O
slightly	O
affected	O
ZAR	O
performance	O
.	O
We	O
collected	O
checkpoints	O
at	O
the	O
interval	O
of	O
5	O
epochs	O
out	O
of	O
45	O
epochs	O
,	O
in	O
addition	O
to	O
the	O
one	O
with	O
the	O
lowest	O
validation	O
loss	B-MetricName
.	O
They	O
were	O
all	O
trained	O
on	O
ZAR	O
,	O
and	O
we	O
chose	O
the	O
one	O
with	O
the	O
highest	O
score	O
on	O
the	O
validation	O
set	O
.	O
We	O
ran	O
the	O
model	O
with	O
3	O
seeds	B-DatasetName
on	O
MT	O
and	O
with	O
3	O
seeds	B-DatasetName
on	O
ZAR	O
,	O
which	O
resulted	O
in	O
9	O
seed	O
combinations	O
.	O
We	O
report	O
the	O
mean	O
and	O
the	O
standard	O
deviation	O
of	O
the	O
9	O
runs	O
.	O

The	O
experimental	O
results	O
demonstrate	O
that	O
MT	O
helps	O
ZAR	O
,	O
but	O
why	O
does	O
it	O
work	O
?	O
Unfortunately	O
,	O
conventional	O
evaluation	O
metrics	O
for	O
MT	O
(	O
e.g.	O
,	O
BLEU	B-MetricName
)	O
reveal	O
little	O
about	O
the	O
model	O
's	O
ability	O
to	O
handle	O
zero	O
anaphora	O
.	O
To	O
address	O
this	O
problem	O
,	O
Shimazu	O
et	O
al	O
(	O
2020	O
)	O
and	O
Nagata	O
and	O
Morishita	O
(	O
2020	O
)	O
constructed	O
Japanese	O
-	O
English	O
parallel	O
datasets	O
that	O
were	O
designed	O
to	O
automatically	O
evaluate	O
MT	O
models	O
with	O
regard	O
to	O
the	O
translation	O
of	O
Japanese	O
zero	O
pronouns	O
(	O
ZPT	O
)	O
.	O
We	O
used	O
Shimazu	O
et	O
al	O
's	O
dataset	O
for	O
its	O
larger	O
data	O
size	O
.	O
10	O
To	O
facilitate	O
automatic	O
evaluation	O
of	O
ZPT	O
,	O
this	O
dataset	O
paired	O
a	O
correct	O
English	O
sentence	O
with	O
an	O
incorrect	O
one	O
.	O
All	O
we	O
had	O
to	O
do	O
was	O
to	O
calculate	O
the	O
ratio	O
of	O
instances	O
for	O
which	O
the	O
model	O
assigned	O
higher	O
translation	O
scores	O
to	O
the	O
correct	O
candidates	O
.	O
The	O
only	O
difference	O
between	O
the	O
two	O
sentences	O
involved	O
the	O
translation	O
of	O
a	O
Japanese	O
zero	O
pronoun	O
.	O
To	O
choose	O
the	O
correct	O
one	O
,	O
the	O
MT	O
model	O
must	O
sometimes	O
refer	O
to	O
preceding	O
sentences	O
.	O
As	O
in	O
intermediate	O
training	O
,	O
multiple	O
source	O
sentences	O
were	O
fed	O
to	O
the	O
model	O
to	O
generate	O
multiple	O
target	O
sentences	O
.	O
We	O
prepended	O
as	O
many	O
preceding	O
sentences	O
as	O
possible	O
given	O
the	O
limit	O
of	O
128	O
tokens	O
.	O
In	O
addition	O
,	O
this	O
dataset	O
recorded	O
d	O
,	O
the	O
sentencelevel	O
distance	O
between	O
the	O
zero	O
pronoun	O
in	O
question	O
and	O
its	O
antecedent	O
.	O
The	O
number	O
of	O
instances	O
with	O
d	O
=	O
0	B-DatasetName
was	O
218	O
while	O
the	O
number	O
of	O
remaining	O
instances	O
was	O
506	O
.	O
We	O
regarded	O
the	O
former	O
as	O
the	O
instances	O
of	O
intra	O
-	O
sentential	O
anaphora	O
and	O
the	O
latter	O
as	O
the	O
instances	O
of	O
inter	O
-	O
sentential	O
anaphora	O
.	O
We	O
chose	O
the	O
model	O
with	O
the	O
best	O
performance	O
(	O
i.e.	O
,	O
one	O
-	O
stage	O
optimization	O
with	O
MLM	B-DatasetName
)	O
.	O
For	O
each	O
checkpoint	O
we	O
collected	O
during	O
intermediate	O
training	O
,	O
we	O
(	O
1	O
)	O
measured	O
the	O
ZPT	O
accuracy	B-MetricName
and	O
(	O
2	O
)	O
finetuned	O
it	O
to	O
obtain	O
the	O
F1	B-MetricName
score	I-MetricName
for	O
ZAR	O
.	O
As	O
before	O
,	O
Web	O
News	O
intra	O
-	O
sentential	O
anaphora	O
0.758	O
0.763	O
inter	O
-	O
sentential	O
anaphora	O
0.871	O
0.879	O
5	O
shows	O
the	O
strong	O
positive	O
correlations	O
between	O
the	O
two	O
performance	O
measures	O
,	O
especially	O
the	O
very	O
strong	O
correlation	O
for	O
inter	O
-	O
sentential	O
anaphora	O
.	O
These	O
results	O
were	O
in	O
line	O
with	O
our	O
speculation	O
that	O
the	O
performance	O
gains	O
in	O
ZAR	O
stemmed	O
from	O
the	O
model	O
's	O
increased	O
ability	O
to	O
translate	O
zero	O
pronouns	O
.	O

Web	O
News	O
F1	B-MetricName
F1	B-MetricName
+	O
MT	O
70.5	O
-	O
57.7	O
-	O
+	O
MT	O
w/	O
masking	O
71.1	O
0.6	O
57.8	O
0.1	O
+	O
MT	O
w/	O
MLM	B-DatasetName
71.9	O
1.4	O
58.3	O
0.6	O
To	O
dig	O
into	O
this	O
question	O
,	O
we	O
conducted	O
an	O
ablation	O
study	O
by	O
introducing	O
a	O
model	O
with	O
token	O
masking	O
but	O
without	O
the	O
corresponding	O
loss	B-MetricName
function	O
(	O
denoted	O
as	O
+	O
MT	O
w/	O
masking	O
)	O
.	O
We	O
assume	O
that	O
this	O
model	O
was	O
largely	O
deprived	O
of	O
the	O
power	O
to	O
mitigate	O
CF	O
while	O
token	O
masking	O
still	O
acted	O
as	O
a	O
data	O
augmenter	O
.	O
Table	O
6	O
shows	O
the	O
results	O
.	O
Not	O
surprisingly	O
,	O
+	O
MT	O
w/	O
masking	O
was	O
beaten	O
by	O
+	O
MT	O
w/	O
MLM	B-DatasetName
with	O
large	O
margins	O
.	O
However	O
,	O
it	O
did	O
outperform	O
+	O
MT	O
,	O
and	O
the	O
gain	O
was	O
particularly	O
large	O
for	O
the	O
Web	O
.	O
The	O
fact	O
that	O
the	O
contribution	O
of	O
the	O
loss	B-MetricName
function	O
was	O
larger	O
than	O
that	O
of	O
token	O
masking	O
indicates	O
that	O
the	O
improvements	O
were	O
mainly	O
attributed	O
to	O
CF	O
mitigation	O
,	O
but	O
the	O
contribution	O
of	O
token	O
masking	O
alone	O
should	O
not	O
be	O
overlooked	O
.	O

Various	O
studies	O
have	O
explored	O
paraphrase	B-TaskName
generation	I-TaskName
for	O
dialog	O
systems	O
.	O
Bowman	O
et	O
al	O
(	O
2016	O
)	O
showed	O
that	O
generating	O
sentences	O
from	O
a	O
continuous	O
latent	O
space	O
is	O
possible	O
using	O
a	O
variational	B-MethodName
autoencoder	I-MethodName
model	O
and	O
provided	O
guidelines	O
on	O
how	O
to	O
train	O
such	O
a	O
generation	O
model	O
.	O
However	O
,	O
our	O
model	O
uses	O
an	O
encoder	O
-	O
decoder	O
approach	O
which	O
can	O
handle	O
the	O
intent	O
and	O
language	O
as	O
categorical	O
inputs	O
in	O
addition	O
to	O
the	O
sequence	O
input	O
.	O
Malandrakis	O
et	O
al	O
(	O
2019	O
)	O
explored	O
a	O
variety	O
of	O
controlled	O
paraphrase	B-TaskName
generation	I-TaskName
approaches	O
for	O
data	B-TaskName
augmentation	I-TaskName
and	O
proposed	O
to	O
use	O
conditional	O
variational	O
autoencoders	B-MethodName
which	O
they	O
showed	O
obtained	O
the	O
best	O
results	O
.	O
Our	O
method	O
is	O
different	O
as	O
it	O
uses	O
a	O
conditional	O
seq2seq	B-MethodName
model	O
that	O
can	O
generate	O
text	O
from	O
any	O
sequence	O
of	O
slots	O
and	O
does	O
not	O
require	O
an	O
utterance	O
as	O
an	O
input	O
.	O
Xia	O
et	O
al	O
(	O
2020	O
)	O
propose	O
a	O
transformer	O
-	O
based	O
conditional	O
variational	B-MethodName
autoencoder	I-MethodName
for	O
few	O
shot	O
utterance	O
generation	O
where	O
the	O
latent	O
space	O
represents	O
the	O
intent	O
as	O
two	O
independent	O
parts	O
(	O
domain	O
and	O
action	O
)	O
.	O
Our	O
approach	O
is	O
different	O
since	O
it	O
models	O
the	O
language	O
and	O
intent	O
of	O
the	O
generation	O
that	O
can	O
be	O
controlled	O
explicitly	O
.	O
Also	O
,	O
our	O
model	O
is	O
the	O
first	O
to	O
enable	O
zero	O
-	O
shot	O
utterance	O
generation	O
.	O
Cho	O
et	O
al	O
(	O
2019	O
)	O
generate	O
paraphrases	O
for	O
seed	O
examples	O
with	O
a	O
transformer	O
seq2seq	B-MethodName
model	O
and	O
self	O
-	O
label	O
them	O
with	O
a	O
baseline	O
intent	O
and	O
slot	O
model	O
.	O
We	O
follow	O
a	O
similar	O
approach	O
but	O
our	O
model	O
generates	O
utterances	O
from	O
a	O
sequence	O
of	O
slots	O
rather	O
than	O
an	O
utterance	O
,	O
which	O
enables	O
an	O
explicitly	O
controlled	O
generation	O
.	O
Also	O
the	O
number	O
of	O
seed	O
utterances	O
we	O
use	O
is	O
merely	O
20	O
for	O
the	O
few	O
shot	O
setup	O
unlike	O
around	O
1	O
M	O
seed	O
para	O
-	O
carrier	O
phrase	O
pairs	O
in	O
Cho	O
et	O
al	O
(	O
2019	O
)	O
.	O
Several	O
other	O
studies	O
follow	O
a	O
text	O
-	O
to	O
-	O
text	O
ap	O
-	O
proach	O
and	O
assume	O
training	O
data	O
in	O
the	O
form	O
of	O
paraphrase	O
pairs	O
for	O
training	O
paraphrase	B-TaskName
generation	I-TaskName
models	O
in	O
a	O
single	O
language	O
Li	O
et	O
al	O
,	O
2018Li	O
et	O
al	O
,	O
,	O
2019	O
.	O
Our	O
approach	O
is	O
focused	O
towards	O
generating	O
utterances	O
in	O
the	O
dialog	O
domain	O
that	O
can	O
generate	O
utterances	O
from	O
a	O
sequence	O
of	O
slots	O
conditioned	O
on	O
both	O
intent	O
and	O
language	O
.	O
Jolly	O
et	O
al	O
(	O
2020	O
)	O
showed	O
that	O
an	O
interpretationto	O
-	O
text	O
model	O
can	O
be	O
used	O
with	O
shuffling	O
-	O
based	O
sampling	O
techniques	O
to	O
generate	O
diverse	O
and	O
novel	O
paraphrases	O
from	O
small	O
amounts	O
of	O
seed	O
data	O
,	O
that	O
improve	O
accuracy	B-MetricName
when	O
augmenting	O
to	O
the	O
existing	O
training	O
data	O
.	O
Our	O
approach	O
is	O
different	O
as	O
our	O
model	O
can	O
generate	O
the	O
slot	O
annotations	O
along	O
with	O
the	O
the	O
utterance	O
,	O
which	O
are	O
necessary	O
for	O
the	O
slot	O
labeling	O
task	O
.	O
Our	O
model	O
can	O
be	O
seen	O
as	O
an	O
extension	O
of	O
the	O
model	O
by	O
Jolly	O
et	O
al	O
(	O
2020	O
)	O
to	O
a	O
transformer	O
based	O
model	O
,	O
with	O
the	O
added	O
functionality	O
of	O
controlling	O
the	O
language	O
in	O
which	O
the	O
utterance	O
generation	O
is	O
needed	O
,	O
which	O
in	O
turn	O
enables	O
zero	O
shot	O
generation	O
.	O
Using	O
large	O
pre	O
-	O
trained	O
models	O
has	O
also	O
been	O
shown	O
to	O
be	O
effective	O
for	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
Chen	O
et	O
al	O
(	O
2020	O
)	O
for	O
instance	O
show	O
the	O
effectiveness	O
of	O
using	O
GPT	B-MethodName
-	O
2	O
(	O
Radford	O
et	O
al	O
,	O
2019	O
)	O
for	O
generating	O
text	O
from	O
tabular	O
data	O
(	O
a	O
set	O
of	O
attributevalue	O
pairs	O
)	O
.	O
Our	O
model	O
,	O
however	O
,	O
does	O
not	O
rely	O
on	O
pre	O
-	O
trained	O
weights	O
from	O
another	O
model	O
such	O
as	O
GPT	B-MethodName
-	O
2	O
,	O
is	O
scalable	O
,	O
and	O
can	O
be	O
applied	O
to	O
training	O
data	O
from	O
any	O
domain	O
,	O
for	O
instance	O
,	O
dialog	O
domain	O
.	O
Beyond	O
paraphrase	B-TaskName
generation	I-TaskName
,	O
several	O
other	O
techniques	O
have	O
been	O
proposed	O
for	O
feature	O
bootstrapping	O
.	O
Machine	B-TaskName
translation	I-TaskName
can	O
be	O
used	O
from	O
data	O
-	O
rich	O
to	O
data	O
-	O
scarce	O
languages	O
(	O
Gaspers	O
et	O
al	O
,	O
2018	O
;	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
.	O
Cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
learning	O
can	O
also	O
leverage	O
use	O
existing	O
data	O
in	O
other	O
languages	O
(	O
Do	O
and	O
Gaspers	O
,	O
2019	O
)	O
.	O
If	O
a	O
feature	O
is	O
already	O
being	O
actively	O
used	O
,	O
feedback	O
signals	O
from	O
users	O
,	O
such	O
as	O
paraphrases	O
or	O
interruptions	O
,	O
can	O
be	O
used	O
to	O
obtain	O
additional	O
training	O
data	O
(	O
Muralidharan	O
et	O
al	O
,	O
2019	O
;	O
.	O

Generating	O
the	O
output	O
sequence	O
token	O
-	O
by	O
-	O
token	O
can	O
be	O
done	O
by	O
using	O
greedy	O
decoding	O
where	O
given	O
learned	O
model	O
parameters	O
θ	B-HyperparameterName
,	O
the	O
most	O
likely	O
token	O
is	O
picked	O
at	O
each	O
decoding	O
step	O
as	O
x	O
t	O
=	O
argmax	O
p	O
θ	B-HyperparameterName
(	O
x	O
t	O
|	O
x	O
<	O
t	O
)	O
.	O
Such	O
a	O
generation	O
process	O
is	O
deterministic	O
.	O
For	O
our	O
task	O
of	O
generating	O
paraphrases	O
,	O
we	O
are	O
interested	O
in	O
generating	O
diverse	O
and	O
novel	O
utterances	O
.	O
Non	O
-	O
deterministic	O
sampling	O
methods	O
such	O
as	O
top	O
-	O
k	O
sampling	O
has	O
been	O
used	O
in	O
related	O
work	O
(	O
Fan	O
et	O
al	O
,	O
2018	O
;	O
Welleck	O
et	O
al	O
,	O
2020	O
;	O
Jolly	O
et	O
al	O
,	O
2020	O
)	O
to	O
achieve	O
this	O
.	O
In	O
top	O
-	O
k	O
random	O
sampling	O
,	O
we	O
first	O
scale	O
the	O
logits	O
z	O
w	O
by	O
using	O
a	O
temperature	O
parameter	O
τ	O
before	O
applying	O
softmax	B-MethodName
.	O
p	O
(	O
x	O
t	O
=	O
w	O
|	O
x	O
<	O
t	O
)	O
=	O
exp	O
(	O
z	O
w	O
/τ	O
)	O
w	O
V	O
exp	O
(	O
z	O
w	O
/τ	O
)	O
,	O
(	O
1	O
)	O
where	O
V	O
is	O
the	O
decoder	O
's	O
vocabulary	O
.	O
Setting	O
τ	O
>	O
1	O
encourages	O
the	O
resulting	O
probability	O
distribution	O
to	O
be	O
less	O
spiky	O
,	O
thereby	O
encouraging	O
diverse	O
choices	O
during	O
sampling	O
.	O
The	O
top	O
-	O
k	O
sampling	O
restricts	O
the	O
size	O
of	O
the	O
most	O
likely	O
candidate	O
pool	O
to	O
k	O
≤	O
|	O
V	O
|	O
.	O

Paraphrase	B-TaskName
generation	I-TaskName
training	O
Since	O
the	O
training	O
data	O
is	O
imbalanced	O
,	O
we	O
balanced	O
the	O
training	O
data	O
by	O
oversampling	O
the	O
intents	O
to	O
match	O
the	O
frequency	O
of	O
the	O
most	O
frequent	O
intent	O
.	O
3	O
For	O
both	O
the	O
encoder	O
and	O
the	O
decoder	O
,	O
the	O
multi	B-MethodName
-	I-MethodName
head	I-MethodName
attention	I-MethodName
layers	O
'	O
hidden	O
dimension	O
was	O
set	O
to	O
128	O
and	O
the	O
position	O
-	O
wise	O
feed	O
forward	O
layers	O
'	O
hidden	O
dimension	O
was	O
set	O
to	O
256	O
.	O
The	O
number	O
of	O
encoder	O
and	O
decoder	O
layers	O
was	O
set	O
to	O
3	O
each	O
.	O
The	O
number	O
of	O
heads	O
was	O
set	O
to	O
8	O
.	O
Dropout	B-MethodName
of	O
0.1	O
was	O
used	O
in	O
both	O
the	O
encoder	O
and	O
the	O
decoder	O
.	O
The	O
model	O
parameters	O
were	O
initialized	O
with	O
Xavier	B-MethodName
initialization	I-MethodName
(	O
Glorot	O
and	O
Bengio	O
,	O
2010	O
)	O
.	O
The	O
model	O
was	O
trained	O
using	O
Adam	B-MethodName
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e	O
-	O
4	O
and	O
a	O
gradient	B-MethodName
clipping	I-MethodName
of	O
1	O
.	O
The	O
training	O
was	O
stopped	O
when	O
the	O
development	O
loss	B-MetricName
did	O
not	O
improve	O
for	O
5	O
epochs	O
.	O
Generating	O
paraphrases	O
For	O
generating	O
paraphrases	O
in	O
the	O
target	O
intent	O
in	O
the	O
target	O
language	O
,	O
we	O
used	O
the	O
slots	O
appearing	O
in	O
the	O
existing	O
training	O
data	O
in	O
the	O
target	O
intent	O
.	O
We	O
used	O
greedy	O
decoding	O
and	O
top	O
-	O
k	O
sampling	O
with	O
k	B-HyperparameterName
=	I-HyperparameterName
3	O
,	O
5	O
,	O
10	O
and	O
τ	O
=	O
1.0	O
,	O
2.0	O
.	O
For	O
a	O
given	O
input	O
,	O
we	O
generated	O
using	O
the	O
top	O
-	O
k	O
random	O
sampling	O
three	O
times	O
with	O
different	O
random	O
seeds	B-DatasetName
.	O
We	O
finally	O
combined	O
all	O
generations	O
and	O
ranked	O
the	O
candidates	O
using	O
the	O
baseline	O
downstream	O
system	O
's	O
prediction	O
probability	O
.	O
The	O
number	O
of	O
paraphrases	O
that	O
are	O
selected	O
is	O
determined	O
as	O
in	O
3.3	O
,	O
with	O
20	O
as	O
the	O
minimum	O
.	O
Methods	O
for	O
comparison	O
We	O
compare	O
our	O
method	O
against	O
four	O
alternatives	O
:	O
(	O
a	O
)	O
Baseline	O
:	O
No	O
data	B-TaskName
augmentation	I-TaskName
at	O
all	O
.	O
The	O
downstream	O
model	O
is	O
trained	O
using	O
just	O
the	O
available	O
seed	O
examples	O
for	O
the	O
target	O
intent	O
.	O
(	O
b	O
)	O
Oversampling	O
:	O
We	O
oversample	O
the	O
samples	O
per	O
intent	O
uniformly	O
at	O
random	O
to	O
match	O
the	O
size	O
of	O
the	O
augmented	O
training	O
data	O
using	O
the	O
proposed	O
method	O
.	O
This	O
is	O
only	O
applicable	O
to	O
the	O
few	O
shot	O
setup	O
since	O
for	O
the	O
zero	O
shot	O
setup	O
,	O
there	O
are	O
no	O
existing	O
samples	O
in	O
the	O
target	O
intent	O
in	O
the	O
target	O
language	O
to	O
sample	O
from	O
.	O
(	O
c	O
)	O
CVAE	B-MethodName
seq2seq	B-MethodName
model	O
:	O
We	O
generate	O
paraphrases	O
using	O
the	O
CVAE	B-MethodName
seq2seq	B-MethodName
model	O
by	O
Malandrakis	O
et	O
al	O
(	O
2019	O
)	O
.	O
The	O
original	O
CVAE	B-MethodName
seq2seq	B-MethodName
model	O
as	O
proposed	O
by	O
Malandrakis	O
et	O
al	O
(	O
2019	O
)	O
defines	O
the	O
set	O
{	O
domain	O
,	O
intent	O
,	O
slots	O
}	O
as	O
the	O
signature	O
of	O
an	O
utterance	O
and	O
denotes	O
the	O
carrier	O
phrases	O
for	O
a	O
given	O
signature	O
to	O
be	O
paraphrases	O
.	O
These	O
carrier	O
phrases	O
are	O
then	O
used	O
to	O
create	O
input	O
-	O
output	O
pairs	O
for	O
the	O
CVAE	B-MethodName
seq2seq	B-MethodName
model	O
training	O
.	O
Since	O
the	O
original	O
formulation	O
does	O
not	O
take	O
into	O
account	O
the	O
language	O
of	O
generation	O
,	O
we	O
adapt	O
the	O
method	O
for	O
our	O
case	O
by	O
defining	O
the	O
signature	O
as	O
the	O
set	O
{	O
language	O
,	O
intent	O
,	O
slots	O
}	O
.	O
We	O
set	O
the	O
model	O
's	O
hidden	O
dimension	O
to	O
128	O
,	O
used	O
the	O
100	O
-	O
dimensional	O
GloVe	B-MethodName
embeddings	I-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
pretrained	O
on	O
Wikipedia	O
,	O
and	O
trained	O
the	O
model	O
without	O
freezing	O
embeddings	O
using	O
early	B-MethodName
stopping	I-MethodName
with	O
a	O
patience	O
of	O
5	O
epochs	O
by	O
monitoring	O
the	O
development	O
loss	B-MetricName
.	O
Finally	O
we	O
generated	O
100	O
carrier	O
phrases	O
for	O
each	O
carrier	O
phrase	O
input	O
in	O
the	O
target	O
intent	O
in	O
the	O
target	O
language	O
.	O
Paraphrases	O
were	O
obtained	O
by	O
injecting	O
the	O
slot	O
values	O
to	O
the	O
generated	O
carrier	O
phrases	O
.	O
The	O
pool	O
of	O
all	O
paraphrases	O
was	O
sorted	O
using	O
the	O
baseline	O
downstream	O
system	O
's	O
prediction	O
probabilities	O
.	O
The	O
CVAE	B-MethodName
seq2seq	B-MethodName
model	O
was	O
only	O
applicable	O
to	O
the	O
few	O
shot	O
setup	O
since	O
in	O
the	O
zero	O
shot	O
setup	O
there	O
are	O
no	O
existing	O
carrier	O
phrases	O
in	O
the	O
target	O
language	O
in	O
the	O
target	O
intent	O
that	O
can	O
be	O
used	O
to	O
sample	O
from	O
.	O
(	O
d	O
)	O
Machine	B-TaskName
translation	I-TaskName
:	O
We	O
augmented	O
the	O
translations	O
generated	O
from	O
English	O
using	O
the	O
MT+fastalign	O
approach	O
from	O
the	O
MultiATIS++	O
paper	O
(	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
.	O
For	O
the	O
few	O
shot	O
setup	O
,	O
we	O
added	O
all	O
the	O
translated	O
utterances	O
except	O
the	O
ones	O
that	O
correspond	O
to	O
those	O
utterances	O
we	O
already	O
picked	O
as	O
the	O
few	O
shot	O
samples	O
.	O
For	O
the	O
zero	O
shot	O
setup	O
,	O
we	O
added	O
all	O
the	O
translated	O
utterances	O
.	O
(	O
Su	O
et	O
al	O
,	O
2018	O
)	O
did	O
not	O
improve	O
for	O
3	O
epochs	O
.	O

We	O
evaluate	O
the	O
quality	O
of	O
the	O
generated	O
paraphrases	O
using	O
the	O
following	O
metrics	O
.	O
Let	O
S	O
be	O
the	O
set	O
of	O
input	O
slot	O
types	O
and	O
G	O
be	O
the	O
set	O
of	O
generated	O
slot	O
types	O
.	O
All	O
retrieval	O
score	O
The	O
all	O
retrieval	O
score	O
r	O
measures	O
if	O
all	O
the	O
input	O
slots	O
were	O
retrieved	O
in	O
the	O
generation	O
.	O
r	O
=	O
1	O
if	O
|	O
S	O
∩	O
G	O
|	O
=	O
|	O
S	O
|	O
0	B-DatasetName
otherwise	O
(	O
2	O
)	O
Exact	B-MetricName
match	I-MetricName
The	O
exact	B-MetricName
match	I-MetricName
score	O
r	O
measures	O
if	O
all	O
the	O
input	O
slots	O
and	O
output	O
slots	O
exactly	O
match	O
(	O
Malandrakis	O
et	O
al	O
,	O
2019	O
)	O
.	O
r	O
=	O
1	O
if	O
S	O
=	O
G	O
0	B-DatasetName
otherwise	O
(	O
3	O
)	O
Partial	O
match	O
The	O
partial	O
match	O
score	O
r	O
measures	O
if	O
at	O
least	O
one	O
output	O
slot	O
matches	O
an	O
input	O
slot	O
.	O
r	O
=	O
1	O
if	O
|	O
S	O
∩	O
G	O
|	O
>	O
0	B-DatasetName
0	B-DatasetName
otherwise	O
(	O
4	O
)	O
F1	B-MetricName
slot	O
score	O
The	O
F1	B-MetricName
slot	O
score	O
F	O
1	O
measures	O
the	O
set	O
similarity	O
between	O
S	O
and	O
G	O
using	O
precision	O
and	O
recall	O
which	O
are	O
defined	O
for	O
sets	O
as	O
follows	O
.	O
precision	O
=	O
|	O
S	O
∩	O
G	O
|	O
|	O
G	O
|	O
,	O
recall	O
=	O
|	O
S	O
∩	O
G	O
|	O
|	O
S	O
|	O
(	O
5	O
)	O
Jaccard	O
index	O
Jaccard	O
index	O
measures	O
the	O
set	O
similarity	O
between	O
S	O
and	O
G	O
as	O
their	O
intersection	O
size	O
divided	O
by	O
the	O
union	O
size	O
.	O
Novelty	O
Let	O
P	O
be	O
the	O
set	O
of	O
paraphrases	O
generated	O
from	O
a	O
base	O
utterance	O
u.	O
novelty	O
=	O
1	O
|	O
P	O
|	O
u	O
P	O
1	O
−	O
BLEU4	O
(	O
u	O
,	O
u	O
)	O
(	O
6	O
)	O
Diversity	O
The	O
diversity	O
is	O
computed	O
using	O
the	O
generated	O
paraphrases	O
P	O
.	O
diversity	O
=	O
u	O
P	O
,	O
u	O
P	O
,	O
u	O
=	O
u	O
1	O
−	O
BLEU4	O
(	O
u	O
,	O
u	O
)	O
|	O
P	O
|	O
×	O
(	O
|	O
P	O
|	O
−	O
1	O
)	O
(	O
7	O
)	O
Language	O
detection	O
score	O
We	O
are	O
interested	O
in	O
quantifying	O
if	O
a	O
generated	O
paraphrase	O
is	O
in	O
the	O
target	O
language	O
.	O
We	O
use	O
langdetect	O
5	O
to	O
compute	O
p	O
(	O
lang	O
=	O
target	O
lang	O
)	O
.	O
Higher	O
scores	O
denote	O
better	O
language	O
generation	O
.	O
Table	O
5	O
:	O
Downstream	O
slot	O
labeling	O
F1	B-MetricName
scores	O
(	O
%	O
)	O
.	O
Each	O
score	O
shown	O
is	O
the	O
average	O
score	O
of	O
10	O
runs	O
.	O
5	O
Experimental	O
results	O

For	O
both	O
the	O
few	O
shot	O
and	O
zero	O
shot	O
setups	O
,	O
the	O
paraphrases	O
used	O
for	O
intrinsic	O
evaluation	O
are	O
generated	O
in	O
the	O
target	O
intent	O
and	O
the	O
target	O
language	O
only	O
.	O
For	O
the	O
top	O
-	O
k	O
sampling	O
based	O
generation	O
,	O
we	O
generate	O
for	O
each	O
input	O
three	O
times	O
with	O
different	O
random	O
seeds	B-DatasetName
and	O
compute	O
novelty	O
and	O
diversity	O
scores	O
.	O
Table	O
2	O
shows	O
intrinsic	O
evaluation	O
results	O
for	O
different	O
generation	O
methods	O
.	O
For	O
the	O
few	O
shot	O
setup	O
,	O
the	O
all	O
retrieval	O
,	O
exact	B-MetricName
match	I-MetricName
,	O
partial	O
match	O
,	O
F1	B-MetricName
slot	O
and	O
Jaccard	O
index	O
scores	O
decrease	O
upon	O
increasing	O
top	O
-	O
k	O
and	O
temperature	O
.	O
The	O
highest	O
scores	O
for	O
the	O
above	O
metrics	O
are	O
obtained	O
for	O
the	O
greedy	O
generation	O
,	O
which	O
indicates	O
that	O
the	O
generated	O
slot	O
types	O
are	O
most	O
similar	O
to	O
the	O
input	O
slot	O
types	O
in	O
that	O
case	O
.	O
However	O
,	O
it	O
is	O
the	O
opposite	O
for	O
the	O
novelty	O
and	O
diversity	O
metrics	O
where	O
the	O
scores	O
are	O
higher	O
with	O
larger	O
top	O
-	O
k	O
and	O
temperatures	O
.	O
For	O
the	O
zero	O
shot	O
setup	O
,	O
the	O
overall	O
trend	O
is	O
similar	O
to	O
the	O
few	O
shot	O
setup	O
.	O
The	O
slot	O
similarity	O
based	O
metrics	O
are	O
lower	O
in	O
general	O
,	O
which	O
indicates	O
that	O
even	O
as	O
little	O
as	O
20	O
samples	O
in	O
the	O
few	O
shot	O
setup	O
improve	O
the	O
generation	O
of	O
desired	O
slots	O
.	O
The	O
novelty	O
scores	O
for	O
the	O
zero	O
shot	O
setup	O
are	O
1	O
as	O
we	O
would	O
expect	O
.	O
In	O
Table	O
3	O
,	O
we	O
show	O
that	O
the	O
intrinsic	O
evaluation	O
results	O
using	O
the	O
proposed	O
approach	O
are	O
consistently	O
better	O
than	O
the	O
CVAE	B-MethodName
seq2seq	B-MethodName
paraphrase	B-TaskName
generation	I-TaskName
model	O
(	O
Malandrakis	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
language	O
detection	O
score	O
varies	O
across	O
languages	O
,	O
which	O
may	O
be	O
due	O
to	O
the	O
vocabulary	O
overlap	O
between	O
languages	O
,	O
e.g.	O
,	O
San	O
Francisco	O
appears	O
in	O
both	O
English	O
and	O
German	O
utterances	O
.	O
Interestingly	O
we	O
also	O
observe	O
code	O
switching	O
,	O
i.e.	O
mixedlanguage	O
generations	O
,	O
while	O
using	O
our	O
approach	O
.	O

We	O
evaluate	O
the	O
downstream	O
intent	B-TaskName
classification	I-TaskName
using	O
accuracy	B-MetricName
and	O
the	O
slot	O
labeling	O
using	O
F1	B-MetricName
score	I-MetricName
.	O
Since	O
we	O
are	O
interested	O
in	O
measuring	O
the	O
variation	O
in	O
scores	O
for	O
the	O
target	O
intents	O
,	O
we	O
only	O
report	O
the	O
scores	O
for	O
the	O
test	O
samples	O
in	O
the	O
target	O
intents	O
in	O
Tables	O
4	O
and	O
5	O
.	O
We	O
run	O
each	O
downstream	O
training	O
experiment	O
10	O
times	O
and	O
report	O
the	O
mean	O
scores	O
for	O
each	O
language	O
and	O
also	O
the	O
average	O
across	O
languages	O
in	O
the	O
AVG	O
column	O
in	O
Tables	O
4	O
and	O
5	O
.	O
We	O
are	O
also	O
interested	O
in	O
tracking	O
the	O
scores	O
for	O
the	O
test	O
samples	O
having	O
intents	O
other	O
than	O
the	O
target	O
intents	O
since	O
we	O
need	O
to	O
ensure	O
that	O
the	O
scores	O
on	O
the	O
other	O
intents	O
does	O
not	O
go	O
down	O
.	O
We	O
found	O
that	O
the	O
effect	O
on	O
the	O
scores	O
(	O
both	O
intent	B-TaskName
classification	I-TaskName
and	O
slot	O
labeling	O
)	O
for	O
the	O
other	O
intents	O
is	O
negligible	O
using	O
paraphrasing	O
and	O
other	O
methods	O
.	O
6	O
In	O
Tables	O
4	O
and	O
5	O
,	O
our	O
paraphrasing	O
results	O
outperform	O
the	O
baseline	O
scores	O
on	O
average	O
.	O
In	O
the	O
few	O
shot	O
setup	O
,	O
our	O
paraphrasing	O
approach	O
outperforms	O
the	O
CVAE	B-MethodName
seq2seq	B-MethodName
approach	O
in	O
6	O
(	O
DE	O
,	O
ES	O
,	O
FR	O
,	O
HI	O
,	O
JA	O
,	O
ZH	O
)	O
out	O
of	O
8	O
languages	O
in	O
intent	B-TaskName
classification	I-TaskName
and	O
overall	O
obtains	O
an	O
improvement	O
of	O
1.9	O
%	O
intent	B-TaskName
classification	I-TaskName
accuracy	B-MetricName
across	O
all	O
target	O
languages	O
.	O
Both	O
oversampling	O
and	O
MT	O
approaches	O
are	O
competitive	O
.	O
Oversampling	O
performs	O
the	O
best	O
for	O
JA	O
whereas	O
MT	O
performs	O
the	O
best	O
for	O
ES	O
and	O
HI	O
.	O
Our	O
paraphrasing	O
approach	O
results	O
in	O
the	O
best	O
intent	B-TaskName
classification	I-TaskName
scores	O
overall	O
(	O
78	O
%	O
)	O
.	O
In	O
terms	O
of	O
slot	O
F1	B-MetricName
scores	O
,	O
we	O
see	O
mixed	O
results	O
with	O
no	O
clear	O
best	O
method	O
(	O
baseline	O
,	O
oversampling	O
and	O
CVAE	B-MethodName
all	O
result	O
in	O
87.6	O
%	O
F1	B-MetricName
score	I-MetricName
)	O
.	O
Notably	O
,	O
the	O
MT	O
approach	O
results	O
in	O
the	O
lowest	O
overall	O
slot	O
F1	B-MetricName
score	I-MetricName
of	O
just	O
84.8	O
%	O
on	O
average	O
.	O
In	O
the	O
zero	O
shot	O
setup	O
,	O
the	O
MT	O
approach	O
outperforms	O
our	O
paraphrasing	O
approach	O
by	O
a	O
large	O
margin	O
in	O
intent	B-TaskName
classification	I-TaskName
(	O
62.5	O
%	O
)	O
.	O
However	O
we	O
note	O
that	O
the	O
paraphrasing	O
approach	O
requires	O
no	O
dependencies	O
on	O
other	O
models	O
or	O
other	O
data	O
,	O
unlike	O
the	O
MT	O
approach	O
which	O
requires	O
a	O
parallel	O
corpus	O
to	O
train	O
the	O
MT	O
model	O
.	O
In	O
terms	O
of	O
slot	O
F1	B-MetricName
scores	O
,	O
our	O
paraphrasing	O
approach	O
and	O
the	O
baseline	O
approach	O
both	O
result	O
in	O
almost	O
similar	O
overall	O
scores	O
(	O
85.5	O
%	O
and	O
85.4	O
%	O
)	O
,	O
both	O
higher	O
than	O
the	O
MT	O
approach	O
.	O
The	O
lower	O
slot	O
F1	B-MetricName
scores	O
using	O
the	O
MT	O
approach	O
in	O
few	O
and	O
zero	O
shot	O
setups	O
indicate	O
that	O
the	O
fast	O
align	O
method	O
to	O
align	O
slots	O
in	O
source	O
and	O
translation	O
might	O
result	O
in	O
noisy	O
training	O
data	O
affecting	O
the	O
SL	O
model	O
.	O

Word	B-TaskName
embeddings	I-TaskName
pre	O
-	O
trained	O
over	O
large	O
texts	O
have	O
demonstrated	O
benefits	O
for	O
many	O
NLP	O
tasks	O
,	O
especially	O
when	O
the	O
task	O
is	O
label	O
-	O
deprived	O
.	O
However	O
,	O
many	O
popular	O
pre	O
-	O
trained	O
sets	O
of	O
word	B-TaskName
embeddings	I-TaskName
assume	O
fixed	O
finite	O
-	O
size	O
vocabularies	O
1	O
,	O
2	O
,	O
which	O
hinders	O
their	O
ability	O
to	O
provide	O
useful	O
word	O
representations	O
for	O
out	O
-	O
of	O
-	O
vocabulary	O
(	O
OOV	O
)	O
words	O
.	O
We	O
look	O
into	O
the	O
task	O
of	O
generalizing	O
word	B-TaskName
embeddings	I-TaskName
:	O
extrapolating	O
a	O
set	O
of	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
to	O
words	O
out	O
of	O
its	O
fixed	O
vocabulary	O
,	O
without	O
extra	O
access	O
to	O
contextual	O
information	O
(	O
e.g.	O
example	O
sentences	O
or	O
text	O
corpus	O
)	O
.	O
In	O
contrast	O
,	O
the	O
more	O
common	O
task	O
of	O
learning	O
word	B-TaskName
embeddings	I-TaskName
,	O
or	O
often	O
just	O
word	O
embedding	O
,	O
is	O
to	O
obtain	O
distributed	O
representations	O
of	O
words	O
directly	O
from	O
large	O
unlabeled	O
text	O
.	O
The	O
motivation	O
here	O
is	O
to	O
extend	O
the	O
usefulness	O
of	O
pre	O
-	O
trained	O
embeddings	O
without	O
expensive	O
retraining	O
over	O
large	O
text	O
.	O
There	O
have	O
been	O
works	O
showing	O
that	O
contextual	O
information	O
can	O
also	O
help	O
generalize	O
word	B-TaskName
embeddings	I-TaskName
(	O
for	O
example	O
,	O
Khodak	O
et	O
al	O
,	O
2018	O
;	O
Schick	O
and	O
Schütze	O
,	O
2019a	O
,	O
b	O
)	O
.	O
We	O
here	O
,	O
however	O
,	O
focus	O
more	O
on	O
the	O
research	O
question	O
of	O
how	O
much	O
one	O
can	O
achieve	O
from	O
just	O
word	O
compositions	O
.	O
In	O
addition	O
,	O
our	O
proposed	O
way	O
of	O
utilizing	O
word	O
composition	O
information	O
can	O
be	O
combined	O
with	O
the	O
contextual	O
embedding	O
algorithms	O
to	O
further	O
improve	O
the	O
performance	O
of	O
generalized	O
embeddings	O
.	O
The	O
hidden	O
assumption	O
here	O
is	O
that	O
words	O
are	O
made	O
of	O
meaningful	O
parts	O
(	O
cf	O
.	O
morphemes	O
)	O
and	O
that	O
the	O
meaning	O
of	O
a	O
word	O
is	O
related	O
to	O
the	O
meaning	O
of	O
their	O
parts	O
.	O
This	O
way	O
,	O
humans	O
are	O
often	O
able	O
to	O
guess	O
the	O
meaning	O
of	O
a	O
word	O
or	O
term	O
they	O
have	O
never	O
seen	O
before	O
.	O
For	O
example	O
,	O
"	O
postEMNLP	O
"	O
probably	O
means	O
"	O
after	O
EMNLP	O
"	O
.	O
Different	O
models	O
have	O
been	O
proposed	O
for	O
that	O
task	O
of	O
generalizing	O
word	B-TaskName
embeddings	I-TaskName
using	O
word	O
compositions	O
,	O
usually	O
under	O
the	O
name	O
of	O
subword	O
(	O
level	O
)	O
models	O
.	O
Stratos	O
(	O
2017	O
)	O
;	O
Pinter	O
et	O
al	O
(	O
2017	O
)	O
;	O
Kim	O
et	O
al	O
(	O
2018b	O
)	O
model	O
words	O
at	O
the	O
character	O
level	O
.	O
However	O
,	O
they	O
have	O
been	O
surpassed	O
by	O
later	O
subword	O
-	O
level	O
models	O
,	O
probably	O
because	O
of	O
putting	O
too	O
much	O
burden	O
on	O
the	O
models	O
to	O
form	O
and	O
discover	O
meaningful	O
subwords	O
from	O
characters	O
.	O
Bag	O
-	O
of	O
-	O
subwords	O
(	O
BoS	O
)	O
is	O
a	O
simple	O
yet	O
effective	O
model	O
for	O
learning	O
and	O
generalizing	O
(	O
Zhao	O
et	O
al	O
,	O
2018	O
)	O
word	B-TaskName
embeddings	I-TaskName
.	O
BoS	O
composes	O
a	O
word	O
embedding	O
vector	O
by	O
taking	O
the	O
sum	O
or	O
average	O
of	O
the	O
vectors	O
of	O
the	O
subwords	O
(	O
character	O
n	O
-	O
grams	O
)	O
that	O
appear	O
in	O
the	O
given	O
word	O
.	O
However	O
,	O
it	O
ignores	O
the	O
importance	O
of	O
different	O
subwords	O
since	O
all	O
of	O
them	O
are	O
given	O
the	O
same	O
weight	O
.	O
Intuitively	O
,	O
"	O
farm	O
"	O
and	O
"	O
land	O
"	O
should	O
be	O
more	O
relevant	O
in	O
composing	O
representation	O
for	O
word	O
"	O
farmland	O
"	O
than	O
some	O
random	O
subwords	O
like	O
"	O
armla	O
"	O
.	O
Even	O
more	O
favorable	O
would	O
be	O
a	O
model	O
's	O
ability	O
to	O
discover	O
meaningful	O
subword	O
segmentations	O
on	O
its	O
own	O
.	O
Cotterell	O
et	O
al	O
(	O
2016	O
)	O
bases	O
their	O
model	O
over	O
morphemes	O
but	O
needs	O
help	O
from	O
an	O
external	O
morphological	O
analyzer	O
such	O
as	O
Morfessor	O
(	O
Virpioja	O
et	O
al	O
,	O
2013	O
)	O
.	O
Sasaki	O
et	O
al	O
(	O
2019	O
)	O
use	O
trainable	O
self	O
-	O
attention	O
to	O
combine	O
subword	O
vectors	O
.	O
While	O
the	O
attention	O
implicitly	O
facilitates	O
interactions	O
among	O
subwords	O
,	O
there	O
has	O
been	O
no	O
explicit	O
enforcement	O
of	O
mutual	O
exclusiveness	O
from	O
subword	O
segmentation	O
,	O
making	O
it	O
sometimes	O
difficult	O
to	O
rule	O
out	O
less	O
relevant	O
subwords	O
.	O
For	O
example	O
,	O
"	O
her	O
"	O
is	O
itself	O
a	O
likely	O
subword	O
,	O
but	O
is	O
unlikely	O
to	O
be	O
relevant	O
for	O
"	O
higher	O
"	O
as	O
the	O
remaining	O
"	O
hig	O
"	O
is	O
unlikely	O
.	O
We	O
propose	O
the	O
probabilistic	O
bag	O
-	O
of	O
-	O
subwords	O
(	O
PBoS	O
)	O
model	O
for	O
generalizing	O
word	O
embedding	O
.	O
PBoS	O
simultaneously	O
models	O
subword	O
segmentation	O
and	O
composition	O
of	O
word	O
representations	O
out	O
of	O
subword	O
representations	O
.	O
The	O
subword	O
segmentation	O
part	O
is	O
a	O
probabilistic	O
model	O
capable	O
of	O
handling	O
ambiguity	O
of	O
subword	O
boundaries	O
and	O
ranking	O
possible	O
segmentations	O
based	O
on	O
their	O
overall	O
likelihood	O
.	O
For	O
each	O
segmentation	O
,	O
we	O
compose	O
a	O
word	O
vector	O
as	O
the	O
sum	O
of	O
all	O
subwords	O
that	O
appear	O
in	O
the	O
segmentation	O
.	O
The	O
final	O
embedding	O
vector	O
is	O
the	O
expectation	O
of	O
the	O
word	O
vectors	O
from	O
all	O
possible	O
segmentations	O
.	O
An	O
alternative	O
view	O
is	O
that	O
the	O
model	O
assigns	O
word	O
-	O
specific	O
weights	O
to	O
subwords	O
based	O
on	O
how	O
likely	O
they	O
appear	O
as	O
meaningful	O
segments	O
for	O
the	O
given	O
word	O
.	O
Coupled	O
with	O
an	O
efficient	O
algorithm	O
,	O
our	O
model	O
is	O
able	O
to	O
compose	O
better	O
word	O
embedding	O
vectors	O
with	O
little	O
computational	O
overhead	O
compared	O
to	O
BoS.	O
Manual	O
inspections	O
show	O
that	O
PBoS	O
is	O
able	O
to	O
produce	O
subword	O
segmentations	O
and	O
subword	O
weights	O
that	O
align	O
with	O
human	O
intuition	O
.	O
Affix	O
prediction	O
experiment	O
quantitatively	O
shows	O
that	O
the	O
subword	O
weights	O
given	O
by	O
PBoS	O
are	O
able	O
to	O
recover	O
most	O
eminent	O
affixes	O
of	O
words	O
with	O
good	O
accuracy	B-MetricName
.	O
To	O
assess	O
the	O
quality	O
of	O
generated	O
word	B-TaskName
embeddings	I-TaskName
,	O
we	O
evaluate	O
with	O
the	O
intrinsic	O
task	O
of	O
word	B-TaskName
similarity	I-TaskName
which	O
relates	O
to	O
the	O
semantics	O
;	O
as	O
well	O
as	O
the	O
extrinsic	O
task	O
of	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
(	O
POS	O
)	O
tagging	O
which	O
requires	O
rich	O
information	O
to	O
determine	O
each	O
word	O
's	O
role	O
in	O
a	O
sentence	O
.	O
English	O
word	B-TaskName
similarity	I-TaskName
experiment	O
shows	O
that	O
PBoS	O
improves	O
the	O
correlation	O
scores	O
over	O
previous	O
best	O
models	O
under	O
vari	O
-	O
ous	O
settings	O
and	O
is	O
the	O
only	O
model	O
that	O
consistently	O
improves	O
over	O
the	O
target	O
pre	O
-	O
trained	O
embeddings	O
.	O
POS	O
tagging	O
experiment	O
over	O
23	O
languages	O
shows	O
that	O
PBoS	O
improves	O
accuracy	B-MetricName
compared	O
in	O
all	O
but	O
one	O
language	O
to	O
the	O
previous	O
best	O
models	O
,	O
often	O
by	O
a	O
big	O
margin	O
.	O
We	O
summarize	O
our	O
contributions	O
as	O
follows	O
:	O
We	O
propose	O
PBoS	O
,	O
a	O
subword	O
-	O
level	O
word	O
embedding	O
model	O
that	O
is	O
based	O
on	O
probabilistic	O
segmentation	O
of	O
words	O
into	O
subwords	O
,	O
the	O
first	O
of	O
its	O
kind	O
(	O
Section	O
2	O
)	O
.	O
We	O
propose	O
an	O
efficient	O
algorithm	O
that	O
leads	O
to	O
an	O
efficient	O
implementation	O
3	O
of	O
PBoS	O
with	O
little	O
overhead	O
over	O
previous	O
much	O
simpler	O
BoS.	O
(	O
Section	O
3	O
)	O
.	O
Manual	O
inspection	O
and	O
affix	O
prediction	O
experiment	O
show	O
that	O
PBoS	O
is	O
able	O
to	O
give	O
reasonable	O
subword	O
segmentations	O
and	O
subword	O
weights	O
(	O
Section	O
4.1	O
and	O
4.2	O
)	O
.	O

For	O
a	O
given	O
language	O
,	O
let	O
Γ	B-HyperparameterName
be	O
its	O
alphabet	O
.	O
A	O
word	O
w	O
of	O
length	O
l	O
=	O
|	O
w	O
|	O
is	O
a	O
string	O
made	O
of	O
l	O
letters	O
in	O
Γ	B-HyperparameterName
,	O
i.e.	O
w	O
=	O
c	O
1	O
c	O
2	O
.	O
.	O
.	O
c	O
l	O
Γ	B-HyperparameterName
l	O
where	O
w	O
[	O
i	O
]	O
=	O
c	O
i	O
is	O
the	O
i	O
-	O
th	O
letter	O
.	O
Let	O
p	O
w	O
[	O
0	B-DatasetName
,	O
1	O
]	O
be	O
the	O
probability	O
that	O
w	O
appears	O
in	O
the	O
language	O
.	O
Empirically	O
,	O
this	O
is	O
proportional	O
to	O
the	O
unigram	O
frequency	O
of	O
word	O
w	O
observed	O
in	O
large	O
text	O
in	O
that	O
language	O
.	O
Note	O
that	O
we	O
do	O
not	O
assume	O
a	O
vocabulary	O
.	O
That	O
is	O
,	O
we	O
do	O
not	O
distinguish	O
words	O
from	O
arbitrary	O
strings	O
made	O
out	O
of	O
the	O
alphabet	O
.	O
The	O
implicit	O
assumption	O
here	O
is	O
that	O
a	O
"	O
word	O
"	O
in	O
common	O
sense	O
is	O
just	O
a	O
string	O
associated	O
with	O
high	O
probability	O
.	O
In	O
this	O
sense	O
,	O
p	O
w	O
can	O
also	O
be	O
seen	O
as	O
the	O
likelihood	O
of	O
string	O
w	O
being	O
a	O
"	O
legit	O
word	O
"	O
.	O
This	O
blurs	O
the	O
boundary	O
between	O
words	O
and	O
non	O
-	O
words	O
,	O
and	O
automatically	O
enables	O
us	O
to	O
handle	O
unseen	O
words	O
,	O
alternative	O
spellings	O
,	O
typos	O
,	O
and	O
nonce	O
words	O
as	O
normal	O
cases	O
.	O
We	O
say	O
a	O
string	O
s	O
Γ	B-HyperparameterName
+	O
is	O
a	O
subword	O
of	O
word	O
w	O
,	O
denoted	O
as	O
s	O
⊆	O
w	O
,	O
if	O
s	O
=	O
w	O
[	O
i	O
:	O
j	O
]	O
=	O
c	O
i	O
.	O
.	O
.	O
c	O
j	O
for	O
some	O
1	O
≤	O
i	O
≤	O
j	O
≤	O
|	O
w	O
|	O
,	O
i.e.	O
s	O
is	O
a	O
substring	O
of	O
w.	O
The	O
probability	O
that	O
subword	O
s	O
appears	O
in	O
the	O
language	O
can	O
then	O
be	O
defined	O
as	O
p	O
s	O
∝	O
w	O
Γ	B-HyperparameterName
+	O
p	O
w	O
1≤i≤j≤	O
|	O
w	O
|	O
1	O
(	O
s	O
=	O
w	O
[	O
i	O
:	O
j	O
]	O
)	O
(	O
1	O
)	O
where	O
1	O
(	O
pred	O
)	O
gives	O
1	O
and	O
otherwise	O
0	B-DatasetName
only	O
if	O
pred	O
holds	O
.	O
Note	O
that	O
a	O
subword	O
s	O
may	O
occur	O
more	O
than	O
once	O
in	O
the	O
same	O
word	O
w.	O
For	O
example	O
,	O
subword	O
"	O
ana	O
"	O
occurs	O
twice	O
in	O
the	O
word	O
"	O
banana	O
"	O
.	O
A	O
subword	O
segmentation	O
g	O
of	O
word	O
w	O
of	O
length	O
k	B-HyperparameterName
=	I-HyperparameterName
|	O
g	O
|	O
is	O
a	O
tuple	O
(	O
s	O
1	O
,	O
s	O
2	O
,	O
.	O
.	O
.	O
,	O
s	O
k	O
)	O
of	O
subwords	O
of	O
w	O
,	O
so	O
that	O
w	O
is	O
the	O
concatenation	O
of	O
s	O
1	O
,	O
.	O
.	O
.	O
,	O
s	O
k	O
.	O

Based	O
on	O
the	O
above	O
modeling	O
of	O
subword	O
segmentations	O
,	O
we	O
propose	O
the	O
Probabilistic	O
Bag	O
-	O
of	O
-	O
Subword	O
(	O
PBoS	O
)	O
model	O
for	O
composing	O
word	B-TaskName
embeddings	I-TaskName
.	O
The	O
embedding	O
vector	O
w	O
for	O
word	O
w	O
is	O
the	O
expectation	O
of	O
all	O
its	O
segmentation	O
-	O
based	O
word	O
embedding	O
:	O
w	O
=	O
g	O
Segw	O
p	O
g	O
|	O
w	O
g	O
(	O
3	O
)	O
where	O
g	O
is	O
the	O
embedding	O
for	O
segmentation	O
g.	O
Given	O
a	O
subword	O
segmentation	O
g	O
,	O
we	O
adopt	O
the	O
Bag	O
-	O
of	O
-	O
Subwords	O
(	O
BoS	O
)	O
model	O
Zhao	O
et	O
al	O
,	O
2018	O
)	O
for	O
composing	O
word	O
embedding	O
from	O
subwords	O
.	O
Specifically	O
,	O
we	O
apply	O
BoS	O
4	O
over	O
the	O
subword	O
segments	O
in	O
g	O
:	O
g	O
=	O
s	O
g	O
s	O
,	O
(	O
4	O
)	O
where	O
s	O
is	O
the	O
vector	O
representation	O
for	O
subword	O
s	O
,	O
as	O
if	O
the	O
current	O
segmentation	O
g	O
is	O
the	O
"	O
golden	O
"	O
segmentation	O
of	O
the	O
word	O
.	O
In	O
such	O
case	O
,	O
we	O
assume	O
the	O
meaning	O
of	O
the	O
word	O
is	O
the	O
combination	O
of	O
the	O
meaning	O
of	O
all	O
its	O
subword	O
segments	O
.	O
We	O
maintain	O
a	O
look	O
-	O
up	O
table	O
S	O
:	O
Γ	B-HyperparameterName
+	O
R	O
d	O
for	O
all	O
subword	O
vectors	O
(	O
i.e.	O
s	O
=	O
S	O
(	O
s	O
)	O
)	O
as	O
trainable	O
parameters	O
of	O
the	O
model	O
,	O
where	O
d	O
is	O
the	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
.	O
Combining	O
Eq	O
.	O
(	O
3	O
)	O
and	O
(	O
4	O
)	O
,	O
we	O
can	O
compose	O
vector	O
representation	O
for	O
any	O
word	O
w	O
Γ	B-HyperparameterName
+	O
as	O
w	O
=	O
g	O
Segw	O
p	O
g	O
|	O
w	O
s	O
g	O
s.	O
(	O
5	O
)	O
Given	O
a	O
set	O
of	O
target	O
pre	O
-	O
trained	O
word	O
vectors	O
w	O
*	O
defined	O
for	O
words	O
within	O
a	O
finite	O
vocabulary	O
W	O
,	O
our	O
model	O
can	O
be	O
trained	O
by	O
minimizing	O
the	O
mean	O
square	O
loss	B-MetricName
:	O
minimize	O
S	O
1	O
|	O
W	O
|	O
w	O
W	O
w	O
−	O
w	O
*	O
2	O
2	O
.	O
(	O
6	O
)	O
3	O
Efficient	O
Algorithm	O
PBoS	O
simultaneously	O
considers	O
all	O
possible	O
subword	O
segmentations	O
and	O
their	O
contributions	O
in	O
composing	O
word	O
representations	O
.	O
However	O
,	O
summing	O
over	O
embeddings	O
of	O
all	O
possible	O
segmentations	O
can	O
be	O
awfully	O
inefficient	O
,	O
as	O
simply	O
enumerating	O
all	O
possible	O
segmentations	O
of	O
w	O
takes	O
number	O
of	O
steps	O
exponential	O
to	O
the	O
length	O
of	O
w	O
(	O
Proposition	O
2	O
)	O
.	O
We	O
therefore	O
need	O
an	O
efficient	O
way	O
to	O
compute	O
Eq	O
.	O
(	O
5	O
)	O
.	O

Now	O
we	O
can	O
efficiently	O
compute	O
Eq	O
.	O
(	O
7	O
)	O
if	O
we	O
can	O
efficiently	O
compute	O
a	O
s	O
|	O
w	O
.	O
Here	O
we	O
present	O
an	O
algorithm	O
that	O
computes	O
a	O
s	O
|	O
w	O
for	O
all	O
s	O
⊆	O
w	O
in	O
O	O
(	O
|	O
w	O
|	O
2	O
)	O
time	O
.	O
The	O
specific	O
structure	O
of	O
the	O
subword	O
transition	O
graph	O
means	O
that	O
edges	O
only	O
go	O
from	O
left	O
to	O
right	O
.	O
Thus	O
,	O
we	O
can	O
split	O
every	O
path	O
going	O
through	O
e	O
into	O
three	O
parts	O
:	O
edges	O
left	O
to	O
e	O
,	O
e	O
itself	O
and	O
edges	O
right	O
to	O
e.	O
In	O
terms	O
of	O
subwords	O
,	O
that	O
is	O
,	O
for	O
s	O
=	O
w	O
[	O
i	O
:	O
j	O
]	O
,	O
l	O
=	O
|	O
w	O
|	O
,	O
each	O
segmentation	O
g	O
that	O
contains	O
s	O
can	O
be	O
divided	O
into	O
three	O
parts	O
:	O
segmentation	O
g	O
w	O
[	O
1	O
:	O
i−1	O
]	O
over	O
w	O
[	O
1	O
:	O
i	O
−	O
1	O
]	O
,	O
=	O
p	O
s	O
b	O
1	O
,	O
i−1	O
b	O
j+1	O
,	O
l	O
,	O
(	O
10	O
)	O
where	O
b	O
i	O
,	O
j	O
=	O
g	O
Seg	O
w	O
[	O
i	O
:	O
j	O
]	O
s	O
g	O
p	O
s	O
.	O
Now	O
we	O
can	O
efficiently	O
compute	O
a	O
s	O
|	O
w	O
if	O
we	O
can	O
efficiently	O
compute	O
b	O
1	O
,	O
i−1	O
and	O
b	O
j+1	O
,	O
l	O
for	O
all	O
1	O
≤	O
i	O
,	O
j	O
≤	O
l.	O
Fortunately	O
,	O
we	O
can	O
do	O
so	O
for	O
b	O
1	O
,	O
i	O
using	O
the	O
following	O
recursive	O
relation	O
b	O
1	O
,	O
i	O
=	O
i−1	O
k=0	O
b	O
1	O
,	O
k	O
p	O
w	O
[	O
k+1	O
:	O
i	O
]	O
(	O
11	O
)	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
l	O
with	O
b	O
1	O
,	O
0	B-DatasetName
=	O
1	O
.	O
Similar	O
formulas	O
hold	O
for	O
b	O
j	O
,	O
l	O
,	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
l	O
with	O
b	O
l+1	O
,	O
l	O
=	O
1	O
.	O
Based	O
on	O
this	O
,	O
we	O
devise	O
Algorithm	O
1	O
for	O
computing	O
a	O
s	O
|	O
w	O
for	O
all	O
s	O
⊆	O
w.	O
Here	O
we	O
take	O
the	O
alternative	O
view	O
of	O
our	O
model	O
as	O
a	O
weighted	O
average	O
of	O
all	O
possible	O
subwords	O
(	O
thus	O
the	O
normalization	O
in	O
Line	O
12	O
)	O
,	O
and	O
an	O
extension	O
to	O
the	O
unweighted	O
averaging	O
of	O
subwords	O
as	O
used	O
in	O
Zhao	O
et	O
al	O
(	O
2018	O
)	O
.	O
Algorithm	O
1	O
Computing	O
a	O
s	O
|	O
w	O
.	O
1	O
:	O
Input	O
:	O
Word	O
w	O
,	O
p	O
s	O
for	O
all	O
s	O
⊆	O
w.	O
l	O
=	O
|	O
w	O
|	O
.	O
2	O
:	O
b	O
1	O
,	O
0	B-DatasetName
1	O
;	O
b	O
l+1	O
,	O
l	O
1	O
;	O
3	O
:	O
for	O
i	O
1	O
.	O
.	O
.	O
l	O
do	O
4	O
:	O
b	O
1	O
,	O
i	O
i−1	O
k=0	O
p	O
w	O
[	O
k+1	O
:	O
i	O
]	O
b	O
1	O
,	O
k	O
5	O
:	O
b	O
l−i+1	O
,	O
l	O
l	O
k	B-HyperparameterName
=	I-HyperparameterName
l−i+1	O
p	O
w	O
[	O
l−i+1	O
:	O
k	O
]	O
b	O
k+1	O
,	O
l6	O
:	O
end	O
for	O
7	O
:	O
ã	O
s	O
|	O
w	O
0	B-DatasetName
for	O
all	O
s	O
⊆	O
w	O
8	O
:	O
for	O
i	O
1	O
.	O
.	O
.	O
l	O
,	O
j	O
i	O
.	O
.	O
.	O
l	O
do	O
9	O
:	O
ã	O
p	O
w	O
[	O
i	O
:	O
j	O
]	O
b	O
1	O
,	O
i−1	O
b	O
j+1	O
,	O
l	O
10	O
:	O
ã	O
w	O
[	O
i	O
:	O
j	O
]	O
|	O
w	O
ã	O
w	O
[	O
i	O
:	O
j	O
]	O
|	O
w	O
+	O
ã	O
11	O
:	O
end	O
for	O
12	O
:	O
a	O
s	O
|	O
w	O
ã	O
s	O
|	O
w	O
/	O
s	O
⊆wã	O
s	O
|	O
w	O
for	O
all	O
s	O
⊆	O
w	O
13	O
:	O
return	O
a	O
|	O
w	O
Time	O
complexity	O
As	O
we	O
only	O
access	O
each	O
subword	O
once	O
in	O
each	O
for	O
-	O
statement	O
,	O
the	O
number	O
of	O
multiplications	O
and	O
additions	O
involved	O
is	O
bounded	O
by	O
the	O
number	O
of	O
subword	O
locations	O
of	O
w.	O
Each	O
of	O
Line	O
4	O
and	O
Line	O
5	O
take	O
i	O
multiplications	O
and	O
i	O
−	O
1	O
additions	O
respectively	O
.	O
So	O
Line	O
3	O
to	O
Line	O
6	O
in	O
total	O
takes	O
2l	O
2	O
computations	O
.	O
Line	O
8	O
to	O
Line	O
11	O
takes	O
3l	O
(	O
l+1	O
)	O
2	O
computations	O
.	O
Thus	O
,	O
the	O
time	O
complexity	O
of	O
Algorithm	O
1	O
is	O
O	O
(	O
l	O
2	O
)	O
.	O
Given	O
a	O
word	O
of	O
length	O
20	O
,	O
O	O
(	O
l	O
2	O
)	O
(	O
20	O
2	O
=	O
400	O
)	O
is	O
much	O
better	O
than	O
enumerating	O
all	O
O	O
(	O
2	O
l	O
)	O
(	O
2	O
20	O
=	O
1	O
,	O
048	O
,	O
576	O
)	O
segmentations	O
.	O
Using	O
the	O
setting	O
in	O
Section	O
4.3	O
,	O
PBoS	O
only	O
takes	O
30	O
%	O
more	O
time	O
(	O
590	O
µs	O
vs	O
454	O
µs	O
)	O
in	O
average	O
than	O
BoS	O
(	O
by	O
disabling	O
a	O
s	O
|	O
w	O
computation	O
)	O
to	O
compose	O
a	O
300	O
-	O
dimensional	O
word	O
embedding	O
vector	O
.	O

We	O
quantitatively	O
evaluate	O
the	O
quality	O
of	O
subword	O
segmentations	O
and	O
subsequent	O
subword	O
weights	O
by	O
testing	O
if	O
our	O
PBoS	O
model	O
is	O
able	O
to	O
discover	O
the	O
most	O
eminent	O
word	O
affixes	O
.	O
Note	O
this	O
has	O
nothing	O
to	O
do	O
with	O
embeddings	O
,	O
so	O
no	O
training	O
is	O
involved	O
in	O
this	O
experiment	O
.	O
The	O
affix	O
prediction	O
task	O
is	O
to	O
predict	O
the	O
most	O
eminent	O
affix	O
for	O
a	O
given	O
word	O
.	O
For	O
example	O
,	O
"	O
-	O
able	O
"	O
for	O
"	O
replaceable	O
"	O
and	O
"	O
re	O
-	O
"	O
for	O
"	O
rename	O
"	O
.	O
Models	O
We	O
get	O
affix	O
prediction	O
from	O
our	O
PBoS	O
by	O
taking	O
the	O
top	O
-	O
ranked	O
subword	O
that	O
is	O
one	O
of	O
the	O
possible	O
affixes	O
.	O
To	O
show	O
our	O
advantage	O
,	O
we	O
Word	O
w	O
Top	O
segmentation	O
g	O
(	O
and	O
their	O
p	O
g	O
|	O
w	O
)	O
Top	O
subword	O
s	O
(	O
and	O
their	O
a	O
s	O
|	O
w	O
)	O
higher	O
higher	O
(	O
0.924	O
)	O
,	O
high	O
/	O
er	O
(	O
0.030	O
)	O
,	O
highe	O
/	O
r	O
(	O
0.027	O
)	O
,	O
h	O
/	O
igher	O
(	O
0.007	O
)	O
,	O
hig	O
/	O
her	O
(	O
0.004	O
)	O
.	O
higher	O
(	O
0.852	O
)	O
,	O
high	O
(	O
0.031	O
)	O
,	O
er	O
(	O
0.029	O
)	O
,	O
r	O
(	O
0.029	O
)	O
,	O
highe	O
(	O
0.025	O
)	O
.	O
farmland	O
farmland	O
(	O
0.971	O
)	O
,	O
farmlan	O
/	O
d	O
(	O
0.010	O
)	O
,	O
farm	O
/	O
land	O
(	O
0.006	O
)	O
,	O
f	O
/	O
armland	O
(	O
0.005	O
)	O
.	O
farmland	O
(	O
0.941	O
)	O
,	O
d	O
(	O
0.010	O
)	O
,	O
farmlan	O
(	O
0.009	O
)	O
,	O
farm	O
(	O
0.008	O
)	O
,	O
land	O
(	O
0.007	O
)	O
.	O
penpineapplepie	O
pen	O
/	O
pineapple	O
/	O
pie	O
(	O
0.359	O
)	O
,	O
pen	O
/	O
pineapple	O
/	O
pi	O
/	O
e	O
(	O
0.157	O
)	O
,	O
pen	O
/	O
pineapple	O
/	O
p	O
/	O
ie	O
(	O
0.101	O
)	O
.	O
pineapple	O
(	O
0.238	O
)	O
,	O
pen	O
(	O
0.186	O
)	O
,	O
pie	O
(	O
0.131	O
)	O
,	O
p	O
(	O
0.101	O
)	O
,	O
e	O
(	O
0.099	O
)	O
.	O
paradichlorobenzene	O
para	O
/	O
dichlorobenzene	O
(	O
0.611	O
)	O
,	O
par	O
/	O
a	O
/	O
dichlorobenzene	O
(	O
0.110	O
)	O
,	O
paradi	O
/	O
chlorobenzene	O
(	O
0.083	O
)	O
.	O
dichlorobenzene	O
(	O
0.344	O
)	O
,	O
para	O
(	O
0.283	O
)	O
,	O
a	O
(	O
0.061	O
)	O
,	O
par	O
(	O
0.054	O
)	O
,	O
ichlorobenzene	O
(	O
0.042	O
)	O
.	O
compare	O
it	O
with	O
a	O
BoS	O
-	O
style	O
baseline	O
affix	O
predictor	O
.	O
Because	O
BoS	O
gives	O
same	O
weight	O
to	O
all	O
subwords	O
in	O
a	O
given	O
word	O
,	O
we	O
randomly	O
choose	O
one	O
of	O
the	O
possible	O
affixes	O
that	O
appear	O
as	O
subword	O
of	O
the	O
word	O
.	O
Benchmark	O
We	O
use	O
the	O
derivational	O
morphology	O
dataset	O
8	O
from	O
Lazaridou	O
et	O
al	O
(	O
2013	O
)	O
.	O
The	O
dataset	O
contains	O
7449	O
English	O
words	O
in	O
total	O
along	O
with	O
their	O
most	O
eminent	O
affixes	O
.	O
Because	O
no	O
training	O
is	O
needed	O
in	O
this	O
experiment	O
,	O
we	O
use	O
all	O
the	O
words	O
for	O
evaluation	O
.	O
To	O
make	O
the	O
task	O
more	O
challenging	O
,	O
we	O
drop	O
trivial	O
instances	O
where	O
there	O
is	O
only	O
one	O
possible	O
affix	O
appears	O
as	O
a	O
subword	O
in	O
the	O
given	O
word	O
.	O
For	O
example	O
,	O
"	O
rename	O
"	O
is	O
dropped	O
because	O
only	O
prefix	O
"	O
re	O
-	O
"	O
is	O
present	O
;	O
on	O
the	O
other	O
hand	O
,	O
"	O
replaceable	O
"	O
is	O
kept	O
because	O
both	O
"	O
re	O
-	O
"	O
and	O
"	O
-	O
able	O
"	O
are	O
present	O
.	O
Besides	O
excluding	O
the	O
trivial	O
cases	O
described	O
above	O
,	O
we	O
also	O
exclude	O
instances	O
labeled	O
with	O
suffix	O
"	O
-	O
y	O
"	O
,	O
because	O
it	O
is	O
always	O
included	O
by	O
"	O
-	O
ly	O
"	O
and	O
"	O
-	O
ity	O
"	O
.	O
Altogether	O
,	O
we	O
acquire	O
3546	O
words	O
with	O
17	O
possible	O
affixes	O
for	O
this	O
evaluation	O
.	O
Results	O
Affix	O
prediction	O
results	O
in	O
terms	O
of	O
macro	O
precision	O
,	O
recall	O
,	O
and	O
F1	B-MetricName
score	I-MetricName
are	O
shown	O
in	O
Table	O
2	O
.	O
We	O
can	O
see	O
a	O
definite	O
advantage	O
of	O
PBoS	O
at	O
predicting	O
most	O
word	O
affixes	O
,	O
where	O
all	O
the	O
metrics	O
boost	O
about	O
0.4	O
and	O
F1	B-MetricName
almost	O
doubles	O
compared	O
to	O
BoS	O
,	O
providing	O
evidence	O
that	O
PBoS	O
is	O
able	O
to	O
assign	O
meaningful	O
subword	O
weights	O
.	O

Given	O
that	O
PBoS	O
is	O
able	O
to	O
produce	O
sensible	O
segmentation	O
likelihood	O
and	O
subword	O
weights	O
,	O
we	O
now	O
turn	O
our	O
focus	O
onto	O
the	O
quality	O
of	O
the	O
generated	O
8	O
http://marcobaroni.org/PublicData/	O
affix_complete_set.txt.gz	O
word	B-TaskName
embeddings	I-TaskName
.	O
In	O
this	O
section	O
,	O
we	O
evaluate	O
the	O
word	O
vectors	O
'	O
ability	O
to	O
capture	O
word	O
senses	O
using	O
the	O
intrinsic	O
task	O
of	O
word	B-TaskName
similarity	I-TaskName
.	O
Word	B-TaskName
similarity	I-TaskName
aims	O
to	O
test	O
how	O
well	O
word	B-TaskName
embeddings	I-TaskName
capture	O
words	O
'	O
semantic	B-TaskName
similarity	I-TaskName
.	O
The	O
task	O
is	O
given	O
as	O
pairs	O
of	O
words	O
,	O
along	O
with	O
their	O
similarity	O
scores	O
labeled	O
by	O
language	O
speakers	O
.	O
Given	O
a	O
set	O
of	O
word	B-TaskName
embeddings	I-TaskName
,	O
we	O
compute	O
the	O
similarity	O
scores	O
induced	O
by	O
the	O
cosine	O
distance	O
between	O
the	O
embedding	O
vectors	O
of	O
each	O
pair	O
of	O
words	O
.	O
The	O
performance	O
is	O
then	O
measured	O
in	O
Spearman	O
's	O
correlation	O
ρ	O
for	O
all	O
pairs	O
.	O
Benchmarks	O
We	O
use	O
WordSim353	O
(	O
WS	O
)	O
from	O
Finkelstein	O
et	O
al	O
(	O
2001	O
)	O
which	O
mainly	O
consists	O
of	O
common	O
words	O
.	O
To	O
better	O
access	O
models	O
'	O
ability	O
to	O
generalize	O
word	B-TaskName
embeddings	I-TaskName
towards	O
OOV	O
words	O
,	O
we	O
include	O
the	O
rare	O
word	O
datasets	O
RareWord	O
(	O
RW	O
)	O
from	O
Luong	O
et	O
al	O
(	O
2013	O
)	O
and	O
the	O
newer	O
Card	O
-	O
660	O
(	O
Card	O
)	O
from	O
Pilehvar	O
et	O
al	O
(	O
2018	O
)	O
.	O
Model	O
Setup	O
PBoS	O
composes	O
word	B-TaskName
embeddings	I-TaskName
out	O
of	O
subword	O
vectors	O
exactly	O
as	O
described	O
in	O
Section	O
3	O
.	O
Unlike	O
some	O
of	O
previous	O
models	O
,	O
we	O
do	O
not	O
add	O
special	O
characters	O
to	O
indicate	O
word	O
boundaries	O
and	O
do	O
not	O
set	O
any	O
constraint	O
on	O
subword	O
lengths	O
.	O
PBoS	O
is	O
trained	O
50	O
epochs	O
using	O
vanilla	O
SGD	B-MethodName
with	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
1	O
and	O
inverse	O
square	O
root	O
decay	O
.	O
For	O
baselines	O
,	O
we	O
compare	O
against	O
the	O
bag	O
-	O
ofsubword	O
model	O
(	O
BoS	O
)	O
from	O
Zhao	O
et	O
al	O
(	O
2018	O
)	O
,	O
and	O
the	O
best	O
attention	O
-	O
based	O
model	O
(	O
KVQ	O
-	O
FH	O
)	O
from	O
Sasaki	O
et	O
al	O
(	O
2019	O
)	O
.	O
For	O
BoS	O
,	O
we	O
use	O
our	O
implementation	O
by	O
disabling	O
subword	O
weight	O
computation	O
.	O
For	O
KVQ	O
-	O
FH	O
,	O
we	O
use	O
the	O
implementation	O
given	O
in	O
the	O
paper	O
.	O
All	O
the	O
hyperparameters	O
are	O
set	O
the	O
same	O
as	O
described	O
in	O
the	O
original	O
papers	O
.	O
We	O
choose	O
to	O
not	O
include	O
the	O
character	O
-	O
RNN	O
model	O
(	O
MIMICK	O
)	O
from	O
Pinter	O
et	O
al	O
(	O
2017	O
)	O
,	O
as	O
it	O
has	O
been	O
shown	O
clearly	O
outperformed	O
by	O
the	O
two	O
.	O
KVQ	O
-	O
FH	O
,	O
PBoS	O
can	O
often	O
match	O
and	O
sometimes	O
surpass	O
it	O
even	O
though	O
PBoS	O
is	O
a	O
much	O
simpler	O
model	O
with	O
better	O
explainability	O
.	O
Compared	O
to	O
the	O
scores	O
by	O
using	O
just	O
the	O
target	O
embeddings	O
(	O
Table	O
3	O
,	O
All	O
pairs	O
)	O
,	O
PBoS	O
is	O
the	O
only	O
model	O
that	O
demonstrates	O
improvement	O
across	O
all	O
cases	O
.	O
The	O
only	O
case	O
where	O
PBoS	O
is	O
not	O
doing	O
well	O
is	O
with	O
Polyglot	O
vectors	O
and	O
RW	O
benchmark	O
.	O
After	O
many	O
manual	O
inspections	O
,	O
we	O
conjecture	O
that	O
it	O
may	O
be	O
related	O
to	O
the	O
vector	O
norm	O
.	O
Sometimes	O
the	O
vector	O
of	O
a	O
relevant	O
subword	O
can	O
be	O
of	O
a	O
small	O
norm	O
,	O
prone	O
to	O
be	O
overwhelmed	O
by	O
less	O
relevant	O
subword	O
vectors	O
.	O
To	O
counter	O
this	O
,	O
we	O
tried	O
to	O
normalize	O
subword	O
vectors	O
before	O
summing	O
them	O
up	O
into	O
a	O
word	O
vector	O
(	O
PBoS	O
-	O
n	O
)	O
.	O
PBoS	O
-	O
n	O
showed	O
good	O
improvement	O
for	O
the	O
Polyglot	O
RW	O
case	O
(	O
25	O
to	O
32	O
)	O
,	O
matching	O
the	O
performance	O
of	O
the	O
other	O
two	O
.	O
One	O
may	O
argue	O
that	O
PBoS	O
has	O
an	O
advantage	O
for	O
using	O
the	O
most	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
.	O
However	O
,	O
this	O
is	O
largely	O
because	O
we	O
do	O
not	O
constrain	O
the	O
length	O
of	O
subwords	O
as	O
in	O
BoS	O
or	O
use	O
hashing	O
as	O
in	O
KVQ	O
-	O
FH	O
.	O
In	O
fact	O
,	O
restricting	O
subword	O
length	O
and	O
using	O
hashing	O
helped	O
them	O
for	O
the	O
word	B-TaskName
similarity	I-TaskName
task	O
.	O
We	O
found	O
that	O
PBoS	O
is	O
insensitive	O
to	O
subword	O
length	O
constraints	O
and	O
decide	O
to	O
keep	O
the	O
setting	O
simple	O
.	O
Despite	O
being	O
an	O
interesting	O
direction	O
,	O
we	O
decide	O
to	O
not	O
involve	O
hashing	O
in	O
this	O
work	O
to	O
focus	O
on	O
the	O
effect	O
of	O
our	O
unique	O
weighting	O
scheme	O
.	O
FaxtText	O
Comparison	O
Albeit	O
targeted	O
for	O
a	O
different	O
task	O
(	O
training	O
word	O
embedding	O
)	O
which	O
have	O
access	O
to	O
contextual	O
information	O
,	O
the	O
popular	O
fast	O
-	O
Text	O
)	O
also	O
uses	O
a	O
subwordlevel	O
model	O
.	O
We	O
train	O
fastText	B-MethodName
12	O
over	O
the	O
same	O
English	O
corpus	O
on	O
which	O
the	O
Polyglot	O
target	O
vectors	O
are	O
trained	O
,	O
in	O
order	O
to	O
understand	O
the	O
quantitative	O
impact	O
of	O
contextual	O
information	O
.	O
To	O
ensure	O
a	O
fair	O
comparison	O
,	O
we	O
restrict	O
the	O
vocabulary	O
sizes	O
and	O
embedding	O
dimensions	O
to	O
match	O
those	O
of	O
Polyglot	O
vectors	O
.	O
The	O
word	B-TaskName
similarity	I-TaskName
scores	O
we	O
get	O
for	O
the	O
trained	O
fastText	B-MethodName
model	O
are	O
65/40/14	O
for	O
WS	O
/	O
RW	O
/	O
Card	O
.	O
We	O
note	O
the	O
great	O
gain	O
for	O
WS	O
and	O
RW	O
,	O
suggesting	O
the	O
helpfulness	O
of	O
contextual	O
information	O
in	O
learning	O
and	O
generalizing	O
word	B-TaskName
embeddings	I-TaskName
in	O
the	O
setting	O
of	O
small	O
to	O
moderate	O
OOV	O
rates	O
.	O
Surprisingly	O
,	O
we	O
find	O
that	O
for	O
the	O
case	O
of	O
extremely	O
high	O
OOV	O
rate	O
(	O
Card	O
)	O
,	O
PBoS	O
slightly	O
surpasses	O
fastText	B-MethodName
,	O
suggesting	O
PBoS	O
'	O
effectiveness	O
in	O
generalizing	O
embeddings	O
to	O
OOV	O
words	O
even	O
without	O
any	O
help	O
from	O
contexts	O
.	O
Multilingual	O
Results	O
To	O
evaluate	O
and	O
compare	O
the	O
effectiveness	O
of	O
PBoS	O
across	O
languages	O
,	O
we	O
further	O
train	O
the	O
models	O
targeting	O
multilingual	O
Wikipedia2Vec	O
vectors	O
(	O
Yamada	O
et	O
al	O
,	O
2020	O
)	O
and	O
evaluate	O
them	O
on	O
multilingual	O
WordSim353	O
and	O
SemLex999	O
from	O
Leviant	O
and	O
Reichart	O
(	O
2015	O
)	O
which	O
are	O
available	O
in	O
English	O
,	O
German	O
,	O
Italian	O
and	O
Russian	O
.	O
To	O
better	O
access	O
the	O
models	O
'	O
ability	O
to	O
generalize	O
,	O
we	O
only	O
take	O
the	O
top	O
10k	O
words	O
from	O
the	O
target	O
vectors	O
for	O
training	O
,	O
which	O
yields	O
decent	O
OOV	O
rates	O
,	O
ranging	O
from	O
23	O
%	O
to	O
84	O
%	O
.	O
Detailed	O
results	O
can	O
be	O
found	O
in	O
Appendix	O
Section	O
A.3	O
.	O
In	O
summary	O
,	O
we	O
find	O
1	O
)	O
that	O
PBoS	O
surpasses	O
KVQ	O
-	O
FH	O
for	O
English	O
and	O
German	O
and	O
is	O
comparable	O
to	O
KVQ	O
-	O
FH	O
for	O
Italian	O
;	O
2	O
)	O
that	O
PBoS	O
and	O
KVQ	O
-	O
FH	O
surpasses	O
BoS	O
for	O
English	O
,	O
German	O
and	O
Italian	O
;	O
and	O
3	O
)	O
no	O
definitive	O
trend	O
among	O
the	O
three	O
models	O
for	O
Russian	O
.	O

We	O
follow	O
the	O
evaluation	O
protocol	O
for	O
sequential	O
labeling	O
used	O
by	O
Kiros	O
et	O
al	O
(	O
2015	O
)	O
and	O
Li	O
et	O
al	O
(	O
2017	O
)	O
,	O
and	O
use	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
13	O
as	O
the	O
model	O
for	O
POS	O
tagging	O
.	O
When	O
predicting	O
the	O
tag	O
for	O
the	O
i	O
-	O
th	O
word	O
w	O
i	O
in	O
a	O
sentence	O
,	O
the	O
input	O
to	O
the	O
classifier	O
is	O
the	O
concatenation	O
of	O
the	O
vectors	O
w	O
i−2	O
,	O
w	O
i−1	O
,	O
w	O
i	O
,	O
w	O
i+1	O
,	O
w	O
i+2	O
for	O
the	O
word	O
itself	O
and	O
the	O
words	O
in	O
its	O
context	O
.	O
This	O
setup	O
allows	O
a	O
more	O
direct	O
evaluation	O
of	O
the	O
quality	O
of	O
word	O
vectors	O
themselves	O
,	O
and	O
thus	O
gives	O
better	O
discriminative	O
power	O
.	O
14	O
Dataset	O
We	O
train	O
and	O
evaluate	O
the	O
performance	O
of	O
generated	O
word	B-TaskName
embeddings	I-TaskName
over	O
23	O
languages	O
at	O
the	O
intersection	O
of	O
the	O
Polyglot	O
(	O
Al	O
-	O
Rfou	O
'	O
et	O
al	O
,	O
2013	O
)	O
pre	O
-	O
trained	O
embedding	O
vectors	O
15	O
and	O
the	O
Universal	O
Dependency	O
(	O
UD	B-DatasetName
,	O
v1.4	O
16	O
)	O
dataset	O
.	O
Polyglot	O
vectors	O
contain	O
64	O
-	O
dimensional	O
vectors	O
over	O
13	O
https://scikit	O
-	O
learn.org/0.19/	O
modules	O
/	O
generated	O
/	O
sklearn.linear_model	O
.	O
LogisticRegression.html	O
14	O
As	O
a	O
side	O
note	O
,	O
in	O
our	O
early	O
trials	O
,	O
we	O
tried	O
to	O
evaluate	O
using	O
an	O
LSTM	B-MethodName
model	O
following	O
Pinter	O
et	O
al	O
(	O
2017	O
)	O
and	O
Zhao	O
et	O
al	O
(	O
2018	O
)	O
,	O
but	O
found	O
the	O
numbers	O
rather	O
similar	O
across	O
embedding	O
models	O
.	O
One	O
possible	O
explanation	O
is	O
that	O
LSTMs	O
are	O
so	O
good	O
at	O
picking	O
up	O
contextual	O
features	O
that	O
the	O
impact	O
of	O
mild	O
deviations	O
of	O
a	O
single	O
word	O
vector	O
is	O
marginal	O
.	O
an	O
100k	O
vocabulary	O
for	O
each	O
language	O
and	O
are	O
used	O
as	O
target	O
vectors	O
for	O
each	O
of	O
the	O
subword	O
-	O
level	O
embedding	O
models	O
in	O
this	O
experiment	O
.	O
For	O
PBoS	O
,	O
we	O
use	O
the	O
Polyglot	O
word	O
counts	O
for	O
each	O
language	O
as	O
the	O
base	O
for	O
subword	O
segmentation	O
and	O
subword	O
weights	O
calculation	O
.	O
UD	B-DatasetName
is	O
used	O
as	O
the	O
POS	O
tagging	O
dataset	O
to	O
train	O
and	O
test	O
the	O
POS	O
tagging	O
model	O
.	O
We	O
use	O
the	O
default	O
partition	O
of	O
training	O
and	O
testing	O
set	O
.	O
Statistics	O
vary	O
from	O
language	O
to	O
language	O
.	O
See	O
Appendix	O
A.4	O
for	O
more	O
details	O
.	O
Results	O
Table	O
5	O
shows	O
the	O
POS	O
tagging	O
accuracy	B-MetricName
over	O
the	O
23	O
languages	O
that	O
appear	O
in	O
both	O
Polyglot	O
and	O
UD	B-DatasetName
.	O
All	O
the	O
subword	O
-	O
level	O
embedding	O
models	O
follow	O
the	O
same	O
hyperparameters	O
as	O
in	O
Section	O
4.3	O
.	O
Following	O
Sasaki	O
et	O
al	O
(	O
2019	O
)	O
,	O
we	O
tune	O
the	O
regularization	O
term	O
of	O
the	O
logistic	B-MethodName
regression	I-MethodName
model	O
when	O
evaluating	O
KVQ	O
-	O
FH	O
.	O
Even	O
with	O
that	O
,	O
PBoS	O
is	O
able	O
to	O
achieve	O
the	O
best	O
POS	O
tagging	O
accuracy	B-MetricName
in	O
all	O
but	O
one	O
language	O
regardless	O
of	O
morphological	O
types	O
,	O
OOV	O
rates	O
,	O
and	O
the	O
number	O
of	O
training	O
instances	O
(	O
Appendix	O
Table	O
12	O
)	O
.	O
Particularly	O
,	O
PBoS	O
improvement	O
accuracy	B-MetricName
by	O
greater	O
than	O
0.1	O
for	O
9	O
languages	O
.	O
For	O
the	O
one	O
language	O
(	O
Tamil	O
)	O
where	O
PBoS	O
is	O
not	O
the	O
most	O
accurate	O
,	O
the	O
difference	O
to	O
the	O
best	O
is	O
small	O
(	O
0.003	O
)	O
.	O
KVQ	O
-	O
FH	O
gives	O
no	O
significantly	O
more	O
accurate	O
predictions	O
than	O
BoS	O
despite	O
it	O
is	O
more	O
complex	O
and	O
is	O
the	O
only	O
one	O
tuned	O
with	O
hyperparameters	O
.	O
Overall	O
,	O
Table	O
5	O
shows	O
that	O
the	O
word	B-TaskName
embeddings	I-TaskName
composed	O
by	O
our	O
PBoS	O
is	O
effective	O
at	O
predicting	O
POS	O
tags	O
for	O
a	O
wide	O
range	O
of	O
languages	O
.	O

Table	O
7	O
and	O
Table	O
8	O
show	O
the	O
hyperparameter	O
values	O
used	O
in	O
the	O
POS	O
tagging	O
experiment	O
(	O
Section	O
4.4	O
)	O
.	O
For	O
the	O
prediction	O
model	O
,	O
we	O
use	O
the	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
from	O
scikit	O
-	O
learn	O
0.19.1	O
with	O
the	O
default	O
settings	O
.	O
Following	O
the	O
observation	O
in	O
Sasaki	O
et	O
al	O
(	O
2019	O
)	O
,	O
we	O
tune	O
the	O
regularization	O
parameter	O
C	O
for	O
KVQ	O
-	O
FH	O
for	O
all	O
values	O
a	O
×	O
10	O
b	O
where	O
a	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
9	O
and	O
b	O
=	O
−1	O
,	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
4	O
.	O
We	O
use	O
the	O
POS	O
tagging	O
accuracy	B-MetricName
for	O
English	O
as	O
criterion	O
,	O
and	O
choose	O
C	O
=	O
70	O
.	O
Table	O
12	O
lists	O
some	O
statistics	O
of	O
the	O
datasets	O
used	O
in	O
the	O
POS	O
tagging	O
experiment	O
.	O
PBoS	O
is	O
able	O
to	O
achieve	O
better	O
accuracy	B-MetricName
over	O
BoS	O
and	O
KVQ	O
-	O
FH	O
in	O
all	O
languages	O
regardless	O
of	O
their	O
morphological	O
type	O
,	O
OOV	O
rate	O
and	O
number	O
of	O
training	O
instances	O
for	O
POS	O
tagging	O
.	O

To	O
encode	O
a	O
semantic	O
component	O
p	O
,	O
we	O
take	O
the	O
sequence	O
of	O
both	O
predicate	O
ids	O
and	O
predicate	O
names	O
into	O
consideration	O
.	O
As	O
the	O
example	O
shown	O
in	O
Figure	O
3	O
,	O
the	O
i	O
d	O
sequence	O
of	O
the	O
first	O
semantic	O
component	O
is	O
{	O
contained	O
by	O
}	O
,	O
and	O
the	O
predicate	O
word	O
sequence	O
is	O
the	O
concatenation	O
of	O
canonical	O
names	O
for	O
each	O
predicate	O
,	O
that	O
is	O
{	O
"	O
contained	O
"	O
,	O
"	O
by	O
"	O
}	O
.	O
Given	O
the	O
word	O
sequence	O
{	O
p	O
(	O
w	O
)	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
n	O
}	O
,	O
we	O
first	O
use	O
a	O
word	O
embedding	O
matrix	O
E	O
w	O
R	O
|	O
Vw	O
|	O
×d	O
to	O
convert	O
the	O
original	O
sequence	O
into	O
word	B-TaskName
embeddings	I-TaskName
{	O
p	O
(	O
w	O
)	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
(	O
w	O
)	O
n	O
}	O
,	O
where	O
|	O
V	O
w	O
|	O
denotes	O
the	O
vocabulary	O
size	O
of	O
natural	O
language	O
words	O
,	O
and	O
d	O
denotes	O
the	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
.	O
Then	O
we	O
represent	O
the	O
word	O
sequence	O
using	O
word	O
averaging	O
:	O
p	O
(	O
w	O
)	O
=	O
1	O
n	O
i	O
p	O
(	O
w	O
)	O
i	O
.	O
For	O
the	O
i	O
d	O
sequence	O
{	O
p	O
(	O
i	O
d	O
)	O
1	O
,	O
.	O
.	O
.	O
,	O
p	O
(	O
i	O
d	O
)	O
m	O
}	O
,	O
we	O
simply	O
take	O
it	O
as	O
a	O
whole	O
unit	O
,	O
and	O
directly	O
translate	O
it	O
into	O
vector	O
representation	O
using	O
the	O
embedding	O
matrix	O
E	O
p	O
R	O
|	O
Vp×d	O
|	O
at	O
path	O
level	O
,	O
where	O
|	O
V	O
p	O
|	O
is	O
the	O
vocabulary	O
size	O
of	O
predicate	O
sequences	O
.	O
There	O
are	O
two	O
reasons	O
for	O
using	O
such	O
path	O
embedding	O
:	O
1	O
)	O
the	O
length	O
of	O
i	O
d	O
sequence	O
is	O
not	O
larger	O
than	O
two	O
,	O
based	O
on	O
our	O
generation	O
method	O
;	O
2	O
)	O
the	O
number	O
of	O
distinct	O
predicate	O
sequences	O
is	O
roughly	O
the	O
same	O
as	O
the	O
number	O
of	O
distinct	O
predicates	O
.	O
We	O
get	O
the	O
fi	O
-	O
nal	O
vector	O
of	O
the	O
semantic	O
component	O
by	O
elementwise	O
addition	O
:	O
p	O
=	O
p	O
(	O
w	O
)	O
+	O
p	O
(	O
i	O
d	O
)	O
.	O

To	O
predict	O
the	O
best	O
query	O
graph	O
from	O
candidates	O
,	O
we	O
calculate	O
the	O
overall	O
association	O
score	O
S	O
(	O
q	O
,	O
G	O
)	O
between	O
the	O
question	O
q	O
and	O
each	O
candidate	O
G	O
,	O
which	O
is	O
the	O
weighted	O
sum	O
of	O
features	O
over	O
entity	B-TaskName
linking	I-TaskName
,	O
semantic	O
matching	O
and	O
structural	O
level	O
.	O
Table	O
1	O
lists	O
the	O
detail	O
features	O
.	O
During	O
training	O
step	O
,	O
we	O
adopt	O
hinge	O
loss	B-MetricName
to	O
maximize	O
the	O
margin	O
between	O
positive	O
graphs	O
G	O
+	O
and	O
negative	O
graphs	O
G	O
−	O
:	O
loss	B-MetricName
=	O
max	O
{	O
0	B-DatasetName
,	O
λ	O
−	O
S	O
(	O
q	O
,	O
G	O
+	O
)	O
+	O
S	O
(	O
q	O
,	O
G	O
−	O
)	O
}	O
.	O
(	O
2	O
)	O
For	O
each	O
question	O
,	O
we	O
pick	O
a	O
candidate	O
graph	O
as	O
positive	O
data	O
,	O
if	O
the	O
F	O
1	O
score	O
of	O
its	O
answer	O
is	O
larger	O
than	O
a	O
threshold	O
(	O
set	O
to	O
0.1	O
in	O
our	O
work	O
)	O
.	O
We	O
randomly	O
sample	O
20	O
negative	O
graphs	O
G	O
−	O
from	O
the	O
candidate	O
set	O
whose	O
F	O
1	O
is	O
lower	O
than	O
the	O
corresponding	O
G	O
+	O
.	O

QA	O
datasets	O
:	O
We	O
conduct	O
our	O
experiments	O
on	O
ComplexQuestions	O
(	O
Bao	O
et	O
al	O
,	O
2016	O
)	O
,	O
We	O
-	O
bQuestions	O
(	O
Berant	O
et	O
al	O
,	O
2013	O
)	O
and	O
SimpleQuestions	B-DatasetName
(	O
Bordes	O
et	O
al	O
,	O
2015	O
)	O
.	O
We	O
use	O
CompQ	O
,	O
WebQ	O
and	O
SimpQ	O
as	O
abbreviations	O
of	O
the	O
above	O
datasets	O
,	O
respectively	O
.	O
CompQ	O
contains	O
2	O
,	O
100	O
complex	O
questions	O
collected	O
from	O
Bing	O
search	O
query	O
log	O
,	O
and	O
the	O
dataset	O
is	O
split	O
into	O
1	O
,	O
300	O
training	O
and	O
800	O
testing	O
questions	O
.	O
WebQ	O
contains	O
5	O
,	O
810	O
questions	O
collected	O
from	O
Google	B-DatasetName
Suggest	O
API	O
,	O
and	O
is	O
split	O
into	O
3	O
,	O
778	O
training	O
and	O
2	O
,	O
032	O
testing	O
QA	O
pairs	O
.	O
Each	O
question	O
is	O
manually	O
labeled	O
with	O
at	O
least	O
one	O
answer	O
entity	O
in	O
both	O
datasets	O
.	O
SimpQ	O
consists	O
of	O
more	O
than	O
100	O
K	O
questions	O
,	O
and	O
the	O
gold	O
answer	O
of	O
each	O
question	O
is	O
a	O
gold	O
focus	O
entity	O
paired	O
with	O
a	O
single	O
predicate	O
.	O
This	O
dataset	O
is	O
designed	O
mainly	O
for	O
answering	O
simple	O
questions	O
,	O
and	O
we	O
use	O
it	O
for	O
complementary	O
evaluation	O
.	O
Knowledge	O
bases	O
:	O
For	O
experiments	O
on	O
both	O
CompQ	O
and	O
WebQ	O
,	O
we	O
follow	O
the	O
settings	O
of	O
Berant	O
et	O
al	O
(	O
2013	O
)	O
and	O
Xu	O
et	O
al	O
(	O
2016	O
)	O
to	O
use	O
the	O
full	O
Freebase	O
dump	O
5	O
as	O
the	O
knowledge	O
base	O
,	O
which	O
contains	O
46	O
M	O
entities	O
and	O
5	O
,	O
323	O
predicates	O
.	O
We	O
host	O
the	O
knowledge	O
base	O
with	O
Virtuoso	O
engine	O
6	O
.	O
For	O
the	O
experiments	O
on	O
SimpQ	O
,	O
the	O
knowledge	O
base	O
we	O
use	O
is	O
FB2	O
M	O
,	O
which	O
is	O
a	O
subset	O
of	O
Freebase	O
provided	O
with	O
the	O
dataset	O
,	O
consisting	O
2	O
M	O
entities	O
and	O
10	O
M	O
triple	O
facts	O
.	O
Implementation	O
detail	O
:	O
For	O
all	O
experiments	O
in	O
this	O
section	O
,	O
we	O
initialize	O
word	B-TaskName
embeddings	I-TaskName
using	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
word	O
vectors	O
with	O
dimensions	O
set	O
to	O
300	O
,	O
and	O
the	O
size	O
of	O
Bi	O
-	O
GRU	B-MethodName
hidden	O
layer	O
is	O
also	O
set	O
to	O
300	O
.	O
We	O
tune	O
the	O
margin	O
λ	O
in	O
{	O
0.1	O
,	O
0.2	O
,	O
0.5	O
}	O
,	O
the	O
ensemble	O
threshold	O
K	O
in	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
5	O
,	O
10	O
,	O
+	O
INF	O
}	O
,	O
and	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
B	O
in	O
{	O
16	O
,	O
32	O
,	O
64	O
}	O
.	O
All	O
the	O
source	O
codes	O
,	O
QA	O
datasets	O
,	O
and	O
detail	O
results	O
can	O
be	O
downloaded	O
from	O
http://202.120.38.146/CompQA/.	O

Now	O
we	O
perform	O
KBQA	O
experiments	O
on	O
WebQ	O
and	O
CompQ.	O
We	O
use	O
the	O
average	O
F	O
1	O
score	O
over	O
all	O
questions	O
as	O
our	O
evaluation	O
metric	O
.	O
The	O
official	O
evaluation	O
script	O
7	O
measures	O
the	O
correctness	O
of	O
output	O
entities	O
at	O
string	O
level	O
.	O
While	O
in	O
CompQ	O
,	O
the	O
annotated	O
names	O
of	O
gold	O
answer	O
entities	O
do	O
n't	O
match	O
the	O
case	O
of	O
their	O
names	O
in	O
Freebase	O
,	O
thus	O
we	O
follow	O
Bao	O
et	O
al	O
(	O
2016	O
)	O
to	O
lowercase	O
both	O
annotated	O
names	O
and	O
the	O
output	O
answer	O
names	O
before	O
calculating	O
the	O
F	O
1	O
score	O
.	O
We	O
set	O
λ	O
=	O
0.5	O
,	O
B	O
=	O
32	O
,	O
K	B-HyperparameterName
=	I-HyperparameterName
3	O
for	O
WebQ	O
and	O
K	B-HyperparameterName
=	I-HyperparameterName
5	O
for	O
CompQ	O
,	O
as	O
reaching	O
the	O
highest	O
average	O
F	O
1	O
on	O
the	O
validation	O
set	O
of	O
each	O
dataset	O
.	O
We	O
report	O
the	O
experimental	O
results	O
in	O
Table	O
2	O
.	O
The	O
result	O
of	O
Yih	O
et	O
al	O
(	O
2015	O
)	O
on	O
CompQ	O
is	O
reported	O
by	O
Bao	O
et	O
al	O
(	O
2016	O
)	O
as	O
their	O
implemented	O
result	O
.	O
Our	O
approach	O
outperforms	O
existing	O
approaches	O
on	O
CompQ	O
dataset	O
,	O
and	O
ranks	O
2nd	O
on	O
WebQ	O
among	O
a	O
long	O
list	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
works	O
.	O
Jain	O
(	O
2016	O
)	O
achieves	O
highest	O
F	O
1	O
score	O
on	O
WebQ	O
using	O
memory	O
networks	O
,	O
which	O
is	O
not	O
semantic	B-TaskName
parsing	I-TaskName
based	O
,	O
and	O
thus	O
less	O
interpretable	O
.	O
We	O
point	O
out	O
that	O
Xu	O
et	O
al	O
(	O
2016	O
)	O
uses	O
Wikipedia	O
texts	O
as	O
the	O
external	O
community	O
knowledge	O
for	O
verifying	O
candidate	O
answers	O
,	O
and	O
achieves	O
a	O
slightly	O
higher	O
F	O
1	O
score	O
(	O
53.3	O
)	O
than	O
our	O
model	O
,	O
but	O
the	O
performance	O
decreases	O
to	O
47.0	O
if	O
this	O
step	O
is	O
removed	O
.	O
Besides	O
,	O
Yih	O
et	O
al	O
(	O
2015	O
)	O
and	O
Bao	O
et	O
al	O
(	O
2016	O
)	O
used	O
ClueWeb	O
dataset	O
for	O
learning	O
more	O
accurate	O
semantics	O
,	O
while	O
based	O
on	O
the	O
ablation	O
test	O
of	O
Yih	O
,	O
the	O
F	O
1	O
score	O
of	O
WebQ	O
drops	O
by	O
0.9	O
if	O
ClueWeb	O
information	O
is	O
removed	O
.	O
Our	O
results	O
show	O
that	O
entity	O
enrichment	O
method	O
improves	O
the	O
results	O
on	O
both	O
datasets	O
by	O
a	O
large	O
margin	O
(	O
0.8	O
)	O
,	O
which	O
is	O
a	O
good	O
help	O
to	O
our	O
approach	O
.	O
We	O
argue	O
that	O
the	O
enriched	O
results	O
are	O
directly	O
comparable	O
with	O
other	O
approaches	O
,	O
as	O
S	O
-	O
MART	O
itself	O
is	O
learned	O
from	O
semi	O
-	O
structured	O
information	O
in	O
Wikipedia	O
,	O
such	O
as	O
anchor	O
texts	O
,	O
redirect	O
links	O
and	O
disambiguation	O
pages	O
,	O
the	O
enrichment	O
step	O
does	O
not	O
bring	O
extra	O
knowledge	O
into	O
our	O
system	O
.	O
In	O
addition	O
,	O
the	O
improvements	O
of	O
the	O
candidate	O
generation	O
step	O
also	O
show	O
a	O
positive	O
effect	O
.	O
If	O
we	O
remove	O
our	O
implicit	O
type	O
filtering	O
in	O
Step	O
4	O
and	O
time	O
interval	O
constraints	O
in	O
Step	O
5	O
,	O
the	O
F	O
1	O
of	O
CompQ	O
slightly	O
drops	O
from	O
42.84	O
to	O
42.37	O
.	O
Al	O
-	O
though	O
these	O
improvements	O
mainly	O
concern	O
timerelated	O
questions	O
(	O
around	O
25	O
%	O
in	O
CompQ	O
)	O
,	O
we	O
believe	O
these	O
strategies	O
can	O
be	O
useful	O
tricks	O
in	O
the	O
further	O
researches	O
.	O
As	O
a	O
complementary	O
evaluation	O
,	O
we	O
perform	O
semantic	O
matching	O
experiments	O
on	O
SimpQ.	O
Given	O
the	O
gold	O
entity	O
of	O
each	O
question	O
,	O
we	O
recognize	O
the	O
entity	O
mention	O
in	O
the	O
question	O
,	O
replace	O
it	O
with	O
E	O
,	O
then	O
predict	O
the	O
correct	O
predicate	O
.	O
Table	O
3	O
shows	O
the	O
experimental	O
results	O
.	O
The	O
best	O
result	O
is	O
from	O
Qu	O
et	O
al	O
(	O
2018	O
)	O
,	O
which	O
learns	O
the	O
semantic	B-TaskName
similarity	I-TaskName
through	O
both	O
attentive	O
RNN	O
and	O
similarity	O
matrix	O
based	O
CNN	O
.	O
Yu	O
et	O
al	O
(	O
2017	O
)	O
proposed	O
another	O
approach	O
using	O
multi	O
-	O
layer	O
BiL	O
-	O
STM	O
with	O
residual	O
connections	O
.	O
Our	O
semantic	O
matching	O
model	O
performs	O
slightly	O
below	O
these	O
two	O
systems	O
,	O
since	O
answering	O
simple	O
questions	O
is	O
not	O
the	O
main	O
goal	O
of	O
this	O
paper	O
.	O
Comparing	O
with	O
these	O
approaches	O
,	O
our	O
semantic	O
matching	O
model	O
is	O
lightweighted	O
,	O
with	O
a	O
simpler	O
structure	O
and	O
fewer	O
parameters	O
,	O
thus	O
is	O
easier	O
to	O
tune	O
and	O
remains	O
effective	O
.	O

For	O
our	O
work	O
in	O
our	O
lab	O
(	O
Orange	O
-	O
Deskiñ	O
)	O
we	O
needed	O
a	O
robust	O
dependency	O
analysis	O
for	O
written	O
French	O
with	O
the	O
highest	O
Labeled	O
Attachment	O
Score	B-MetricName
(	O
LAS	O
)	O
1	O
possible	O
,	O
using	O
a	O
wide	O
range	O
of	O
dependency	O
relations	O
.	O
Having	O
worked	O
in	O
the	O
past	O
on	O
rule	O
based	O
dependency	O
analysis	O
,	O
it	O
became	O
obvious	O
that	O
we	O
need	O
to	O
adopt	O
a	O
more	O
modern	O
approach	O
to	O
dependency	O
analysis	O
.	O
Thus	O
during	O
the	O
last	O
year	O
we	O
tried	O
several	O
freely	O
available	O
open	O
source	O
tools	O
available	O
(	O
e.g.	O
MaltParser	O
2	O
,	O
Google	B-DatasetName
's	O
SyntaxNet	O
3	O
,	O
Standford	O
Dependency	O
Tools	O
4	O
,	O
Bist	O
-	O
1	O
Since	O
we	O
are	O
interested	O
in	O
semantic	O
relations	O
a	O
good	O
CLAS	O
score	O
(	O
Nivre	O
and	O
Fang	O
,	O
2017	O
)	O
is	O
even	O
more	O
relevant	O
.	O
2	O
http://www.maltparser.org/	O
3	O
https://www.tensorflow.org/versions/	O
r0.11	O
/	O
tutorials	O
/	O
syntaxnet/	O
4	O
https://nlp.stanford.edu/software/	O
stanford	O
-	O
dependencies.shtml	O
Parser	O
5	O
and	O
HTParser	O
6	O
)	O
,	O
trained	O
on	O
different	O
Treebanks	O
(	O
notably	O
French	O
Sequoia	O
(	O
Candito	O
et	O
al	O
,	O
2014	O
)	O
and	O
Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
McDonald	O
et	O
al	O
,	O
2013	O
)	O
)	O
.	O
All	O
combinations	O
of	O
tools	O
and	O
treebanks	O
had	O
some	O
advantages	O
and	O
some	O
inconveniences	O
.	O
For	O
instance	O
,	O
the	O
underlying	O
linguistic	O
models	O
of	O
the	O
treebanks	O
are	O
not	O
the	O
same	O
or	O
some	O
tools	O
would	O
not	O
accept	O
CONLLU	O
input	O
but	O
only	O
raw	O
text	O
and	O
apply	O
their	O
own	O
segmentation	O
and	O
POS	O
tagging	O
.	O
In	O
a	O
next	O
step	O
we	O
enriched	O
the	O
French	O
treebanks	O
with	O
additional	O
information	O
like	O
lemmas	O
,	O
morphological	O
features	O
and	O
more	O
fine	O
-	O
graded	O
XPOS	O
in	O
addition	O
to	O
the	O
about	O
20	O
UPOS	O
categories	O
of	O
the	O
treebanks	O
(	O
UD	B-DatasetName
-	O
French	O
v1.2	O
does	O
not	O
contain	O
neither	O
lemmas	O
nor	O
morphological	O
features	O
)	O
and	O
conducted	O
a	O
new	O
training	O
/	O
test	O
/	O
evaluation	O
cycle	O
.	O
Since	O
the	O
initial	O
results	O
for	O
French	O
were	O
encouraging	O
we	O
tried	O
the	O
same	O
approaches	O
with	O
other	O
languages	O
,	O
such	O
as	O
the	O
languages	O
proposed	O
for	O
CoNLL	O
2017	O
UD	B-DatasetName
Shared	O
Task	O
(	O
Zeman	O
et	O
al	O
,	O
2017	O
)	O
.	O
However	O
,	O
for	O
participation	O
at	O
the	O
shared	O
task	O
,	O
we	O
relied	O
exclusively	O
on	O
the	O
data	O
provided	O
by	O
Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
Nivre	O
et	O
al	O
,	O
2016	O
(	O
Nivre	O
et	O
al	O
,	O
,	O
2017b	O
,	O
also	O
for	O
French	O
in	O
spite	O
of	O
our	O
previous	O
work	O
.	O
For	O
the	O
shared	O
task	O
we	O
have	O
trained	O
models	O
separately	O
for	O
each	O
language	O
.	O
So	O
strictly	O
speaking	O
,	O
this	O
is	O
not	O
a	O
multilingual	O
but	O
a	O
monolingual	O
multimodel	O
approach	O
.	O

The	O
biggest	O
challenge	O
were	O
the	O
4	O
surprise	O
languages	O
.	O
Having	O
only	O
between	O
20	O
and	O
109	O
sentences	O
to	O
train	O
on	O
(	O
even	O
less	O
if	O
we	O
wanted	O
to	O
split	O
it	O
into	O
a	O
train	O
and	O
development	O
corpus	O
)	O
did	O
not	O
help	O
(	O
see	O
table	O
2	O
for	O
some	O
details	O
)	O
.	O
Since	O
the	O
word	O
embedding	O
files	O
where	O
also	O
rather	O
small	O
we	O
chose	O
not	O
to	O
train	O
on	O
the	O
languages	O
themselves	O
,	O
but	O
to	O
keep	O
all	O
of	O
the	O
provided	O
sentences	O
for	O
the	O
development	O
corpus	O
.	O
So	O
we	O
first	O
tried	O
three	O
similar	O
approaches	O
in	O
order	O
to	O
be	O
able	O
to	O
predict	O
dependency	O
relations	O
for	O
these	O
languages	O
:	O
In	O
all	O
three	O
cases	O
we	O
replaced	O
the	O
forms	O
of	O
all	O
closed	O
word	O
classes	O
(	O
i.e.	O
all	O
but	O
nouns	O
,	O
adjectives	O
and	O
verbs	O
)	O
with	O
the	O
corresponding	O
UPOS	O
in	O
the	O
training	O
and	O
in	O
the	O
test	O
corpus	O
(	O
for	O
the	O
CoNLL	O
2017	O
UD	B-DatasetName
Shared	O
Task	O
we	O
inserted	O
the	O
original	O
forms	O
again	O
after	O
predicting	O
the	O
dependency	O
relations	O
.	O
The	O
"	O
mix	O
"	O
is	O
then	O
trained	O
with	O
a	O
hidden	B-HyperparameterName
layer	I-HyperparameterName
size	I-HyperparameterName
of	O
either	O
100	O
or	O
50	O
,	O
but	O
without	O
word	B-TaskName
embeddings	I-TaskName
.	O
We	O
initially	O
tested	O
these	O
models	O
using	O
the	O
test	O
corpus	O
for	O
the	O
Tamil	O
treebank	O
(	O
UD	B-DatasetName
v2.0	O
)	O
.	O
Using	O
the	O
"	O
mix	O
"	O
with	O
23	O
languages	O
(	O
3	O
)	O
resulted	O
in	O
the	O
best	O
weighted	O
LAS	O
,	O
35.2	O
%	O
(	O
35.3	O
%	O
if	O
using	O
a	O
hidden	B-HyperparameterName
layer	I-HyperparameterName
size	I-HyperparameterName
of	O
50	O
)	O
.	O
The	O
weighted	O
LAS	O
for	O
the	O
surprise	O
languages	O
is	O
shown	O
in	O
table	O
3	O
Upper	O
Sorbian	O
is	O
a	O
slavonic	O
language	O
very	O
close	O
to	O
Czech	O
(	O
and	O
slightly	O
less	O
close	O
to	O
Polish	O
)	O
.	O
Northern	O
Sami	O
shares	O
quite	O
a	O
lot	O
of	O
typological	O
features	O
with	O
the	O
Finnic	O
branch	O
of	O
the	O
Fenno	O
-	O
Ugric	O
languages	O
(	O
here	O
Finnish	O
and	O
Estonian	O
)	O
,	O
and	O
Kurmanji	O
shares	O
at	O
least	O
some	O
typolological	O
feature	O
with	O
Persian	O
(	O
both	O
are	O
from	O
the	O
Iranian	O
subgroup	O
of	O
the	O
Indo	O
-	O
European	O
language	O
family	O
.	O
However	O
Buryat	O
,	O
a	O
Mongolian	O
language	O
,	O
is	O
not	O
typologically	O
close	O
to	O
any	O
of	O
the	O
shared	O
task	O
's	O
languages	O
.	O
Even	O
though	O
Turkish	O
seems	O
close	O
enough	O
,	O
to	O
our	O
surprise	O
Hindi	O
was	O
finally	O
the	O
best	O
guess	O
.	O
With	O
Urdu	B-DatasetName
,	O
which	O
is	O
very	O
similar	O
to	O
Hindi	O
apart	O
from	O
the	O
fact	O
that	O
it	O
uses	O
the	O
Arabic	O
alphabet	O
instead	O
of	O
Devanagari	O
,	O
the	O
LAS	O
was	O
less	O
good	O
.	O
As	O
for	O
the	O
language	O
mix	O
,	O
we	O
replaced	O
the	O
forms	O
of	O
the	O
closed	O
word	O
classes	O
in	O
the	O
training	O
corpora	O
by	O
the	O
corresponding	O
UPOS	O
(	O
except	O
nouns	O
,	O
verbs	O
and	O
adjectives	O
)	O
and	O
trained	O
the	O
modified	O
treebanks	O
(	O
cf	O
.	O
tables	O
4	O
and	O
5	O
,	O
best	O
configuration	O
in	O
bold	O
)	O
.	O

Our	O
final	O
macro	O
-	O
averaged	O
LAS	O
F1	B-MetricName
score	I-MetricName
on	O
the	O
CoNLL	O
2017	O
UD	B-DatasetName
Shared	O
Task	O
test	O
data	O
(	O
Nivre	O
et	O
al	O
,	O
2017a	O
)	O
was	O
68.61	O
%	O
,	O
(	O
10th	O
out	O
of	O
33	O
)	O
17	O
.	O
The	O
details	O
show	O
that	O
our	O
approach	O
worked	O
well	O
for	O
the	O
bigger	O
treebanks	O
and	O
the	O
surprise	O
languages	O
(	O
where	O
we	O
ended	O
up	O
as	O
8th	O
)	O
.	O
In	O
general	O
,	O
the	O
results	O
per	O
language	O
are	O
slightly	O
lower	O
than	O
those	O
we	O
had	O
during	O
training	O
on	O
the	O
development	O
corpora	O
(	O
cf	O
.	O
table	O
1	O
)	O
.	O
This	O
is	O
due	O
to	O
the	O
fact	O
we	O
did	O
our	O
training	O
on	O
forms	O
,	O
lemmas	O
,	O
UPOS	O
and	O
XPOS	O
of	O
the	O
training	O
corpus	O
,	O
which	O
are	O
gold	O
.	O
In	O
the	O
test	O
data	O
,	O
lemmas	O
,	O
UPOS	O
and	O
XPOS	O
(	O
if	O
present	O
)	O
,	O
however	O
,	O
are	O
predicted	O
by	O
UDpipe	O
,	O
and	O
do	O
contain	O
some	O
errors	O
with	O
respect	O
to	O
the	O
gold	O
standard	O
.	O
After	O
the	O
end	O
of	O
the	O
test	O
phase	O
,	O
we	O
discovered	O
a	O
bug	O
in	O
our	O
chain	O
,	O
which	O
concerned	O
languages	O
,	O
which	O
have	O
only	O
UPOS	O
data	O
.	O
In	O
this	O
case	O
the	O
UPOS	O
information	O
was	O
totally	O
discarded	O
by	O
error	O
.	O
Thus	O
all	O
training	O
and	O
testing	O
are	O
done	O
only	O
on	O
the	O
17	O
http://universaldependencies.org/	O
conll17	O
/	O
results.html	O
forms	O
18	O
.	O
Further	O
we	O
made	O
en	O
error	O
uploading	O
the	O
models	O
for	O
the	O
gl	O
TreeGal	O
,	O
fr	O
parTut	O
and	O
sl	O
sst	B-DatasetName
treebanks	O
.	O
During	O
the	O
tests	O
the	O
models	O
trained	O
on	O
the	O
basic	O
gl	O
,	O
fr	O
and	O
sl	O
treebanks	O
were	O
used	O
instead	O
.	O
After	O
the	O
test	O
phase	O
we	O
corrected	O
these	O
errors	O
.	O
Fortunately	O
,	O
their	O
impact	O
was	O
not	O
that	O
hard	O
.	O
Apart	O
from	O
the	O
result	O
for	O
gl	O
TreeGal	O
and	O
sl	O
sst	B-DatasetName
,	O
which	O
went	O
up	O
to	O
66.13	O
%	O
(	O
from	O
22.46	O
%	O
)	O
and	O
to	O
47.68	O
(	O
from	O
40.25	O
)	O
respectively	O
once	O
the	O
correct	O
model	O
was	O
used	O
,	O
the	O
results	O
for	O
the	O
other	O
corpora	O
changed	O
only	O
slightly	O
,	O
the	O
global	O
results	O
could	O
have	O
been	O
69.38	O
%	O
.	O
All	O
results	O
are	O
shown	O
in	O
table	O
6	O
.	O
The	O
column	O
on	O
the	O
right	O
shows	O
the	O
difference	O
between	O
the	O
results	O
of	O
the	O
development	O
corpora	O
and	O
test	O
data	O
.	O
For	O
some	O
languages	O
,	O
the	O
test	O
results	O
are	O
unexpectedly	O
lower	O
than	O
the	O
results	O
on	O
the	O
development	O
corpora	O
.	O
For	O
gl	O
TreeGal	O
,	O
fr	O
parTut	O
and	O
sl	O
sst	B-DatasetName
,	O
this	O
is	O
due	O
to	O
errors	O
when	O
installing	O
our	O
system	O
on	O
the	O
Tira	O
-	O
platform	O
.	O
The	O
lower	O
performance	O
on	O
languages	O
like	O
Chinese	O
,	O
Ukrainian	O
,	O
Vietnamese	O
or	O
Latin	O
(	O
both	O
ITTB	O
and	O
PROIEL	O
)	O
seems	O
to	O
be	O
caused	O
by	O
the	O
nature	O
of	O
the	O
test	O
corpora	O
themselves	O
.	O
Systems	O
of	O
other	O
participants	O
seem	O
to	O
drop	O
in	O
performance	O
as	O
well	O
;	O
for	O
all	O
these	O
languages	O
our	O
system	O
is	O
still	O
around	O
the	O
10th	O
position	O
of	O
the	O
global	O
ranking	O
.	O
Perhaps	O
a	O
cause	O
may	O
be	O
the	O
fact	O
that	O
the	O
XPOS	O
we	O
use	O
(	O
predicted	O
by	O
UDpipe	O
)	O
contain	O
more	O
errors	O
than	O
average	O
for	O
the	O
Chinese	O
,	O
Ukrainian	O
or	O
Vietnamese	O
treebanks	O
than	O
for	O
languages	O
where	O
our	O
test	O
score	O
is	O
closer	O
to	O
the	O
development	O
score	O
.	O

The	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
dataset	O
has	O
greatly	O
boosted	O
the	O
research	O
on	O
dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
(	O
DST	O
)	O
.	O
However	O
,	O
substantial	O
noise	O
has	O
been	O
discovered	O
in	O
its	O
state	O
annotations	O
.	O
Such	O
noise	O
brings	O
about	O
huge	O
challenges	O
for	O
training	O
DST	O
models	O
robustly	O
.	O
Although	O
several	O
refined	O
versions	O
,	O
including	O
MultiWOZ	B-DatasetName
2.1	I-DatasetName
-	O
2.4	O
,	O
have	O
been	O
published	O
recently	O
,	O
there	O
are	O
still	O
lots	O
of	O
noisy	O
labels	O
,	O
especially	O
in	O
the	O
training	O
set	O
.	O
Besides	O
,	O
it	O
is	O
costly	O
to	O
rectify	O
all	O
the	O
problematic	O
annotations	O
.	O
In	O
this	O
paper	O
,	O
instead	O
of	O
improving	O
the	O
annotation	O
quality	O
further	O
,	O
we	O
propose	O
a	O
general	O
framework	O
,	O
named	O
ASSIST	O
(	O
lAbel	O
noiSe	O
-	O
robuSt	O
dIalogue	B-TaskName
State	I-TaskName
Tracking	I-TaskName
)	O
,	O
to	O
train	O
DST	O
models	O
robustly	O
from	O
noisy	O
labels	O
.	O
ASSIST	O
first	O
generates	O
pseudo	O
labels	O
for	O
each	O
sample	O
in	O
the	O
training	O
set	O
by	O
using	O
an	O
auxiliary	O
model	O
trained	O
on	O
a	O
small	O
clean	O
dataset	O
,	O
then	O
puts	O
the	O
generated	O
pseudo	O
labels	O
and	O
vanilla	O
noisy	O
labels	O
together	O
to	O
train	O
the	O
primary	O
model	O
.	O
We	O
show	O
the	O
validity	O
of	O
ASSIST	O
theoretically	O
.	O
Experimental	O
results	O
also	O
demonstrate	O
that	O
AS	O
-	O
SIST	O
improves	O
the	O
joint	O
goal	O
accuracy	B-MetricName
of	O
DST	O
by	O
up	O
to	O
28.16	O
%	O
on	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
and	O
8.41	O
%	O
on	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
,	O
compared	O
to	O
using	O
only	O
the	O
vanilla	O
noisy	O
labels	O
.	O

Conventionally	O
,	O
all	O
the	O
state	O
labels	O
are	O
assumed	O
to	O
be	O
correct	O
.	O
However	O
,	O
this	O
assumption	O
may	O
not	O
hold	O
.	O
In	O
practice	O
,	O
dialogue	O
state	O
annotations	O
are	O
errorprone	O
(	O
Han	O
et	O
al	O
,	O
2020b	O
)	O
.	O
There	O
are	O
a	O
couple	O
of	O
reasons	O
.	O
First	O
,	O
the	O
states	O
are	O
usually	O
annotated	O
by	O
crowdworkers	O
to	O
improve	O
the	O
labelling	O
efficiency	O
.	O
Due	O
to	O
limited	O
knowledge	O
,	O
crowdworkers	O
can	O
not	O
annotate	O
all	O
the	O
states	O
with	O
100	O
%	O
accuracy	B-MetricName
,	O
which	O
naturally	O
incurs	O
noisy	O
labels	O
(	O
Han	O
et	O
al	O
,	O
2020a	O
)	O
.	O
Second	O
,	O
the	O
dialogue	O
may	O
span	O
multiple	O
domains	O
,	O
which	O
also	O
increases	O
the	O
labelling	O
difficulty	O
.	O
Apparently	O
,	O
the	O
noisy	O
labels	O
are	O
harmful	O
and	O
likely	O
to	O
lead	O
to	O
sub	O
-	O
optimal	O
performance	O
.	O
Therefore	O
,	O
it	O
is	O
crucial	O
to	O
take	O
them	O
into	O
consideration	O
so	O
as	O
to	O
train	O
DST	O
models	O
more	O
robustly	O
.	O
LetB	O
t	O
=	O
{	O
(	O
s	O
,	O
ṽ	O
t	O
)	O
|	O
s	O
S	O
}	O
denote	O
the	O
noisy	O
state	O
annotations	O
,	O
whereṽ	O
t	O
is	O
the	O
noisy	O
label	O
of	O
slot	O
s	O
at	O
turn	O
t.	O
We	O
use	O
B	O
t	O
=	O
{	O
(	O
s	O
,	O
v	O
t	O
)	O
|	O
s	O
S	O
}	O
to	O
denote	O
the	O
noise	O
-	O
free	O
state	O
annotations	O
.	O
Here	O
,	O
v	O
t	O
represents	O
the	O
true	O
label	O
of	O
slot	O
s	O
at	O
turn	O
t	O
,	O
which	O
is	O
unknown	O
.	O
In	O
fact	O
,	O
existing	O
DST	O
approaches	O
are	O
only	O
able	O
to	O
learn	O
a	O
sub	O
-	O
optimal	O
dialogue	O
state	O
trackerF	O
:	O
X	O
t	O
B	O
t	O
rather	O
than	O
the	O
optimal	O
state	O
tracker	O
F	O
:	O
X	O
t	O
B	O
t	O
,	O
as	O
none	O
of	O
them	O
have	O
considered	O
the	O
influence	O
of	O
noisy	O
labels	O
.	O
In	O
this	O
work	O
,	O
we	O
aim	O
to	O
learn	O
a	O
robust	O
state	O
tracker	O
F	O
*	O
that	O
can	O
better	O
approximate	O
F	O
from	O
the	O
noisy	O
state	O
annotationsB	O
t	O
.	O

To	O
reduce	O
the	O
influence	O
of	O
noisy	O
labels	O
,	O
we	O
combine	O
the	O
generated	O
pseudo	O
labels	O
and	O
vanilla	O
noisy	O
labels	O
to	O
train	O
the	O
primary	O
model	O
.	O
Letv	O
t	O
andṽ	O
t	O
be	O
the	O
one	O
-	O
hot	O
representation	O
of	O
the	O
pseudo	O
labelv	O
t	O
and	O
vanilla	O
noisy	O
labelṽ	O
t	O
,	O
respectively	O
.	O
Then	O
,	O
we	O
can	O
define	O
the	O
combined	O
label	O
as	O
:	O
v	O
c	O
t	O
=	O
αv	O
t	O
+	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
ṽ	O
t	O
,	O
where	O
α	B-HyperparameterName
(	O
0	B-DatasetName
≤	O
α	B-HyperparameterName
≤	O
1	O
)	O
L	O
pri	O
=	O
(	O
s	O
,	O
v	O
c	O
t	O
)	O
C	O
(	O
Bt	O
,	O
Bt	O
)	O
−	O
log	O
p	O
(	O
v	O
c	O
t	O
|	O
X	O
t	O
,	O
s	O
)	O
=	O
α	B-HyperparameterName
(	O
s	O
,	O
vt	O
)	O
Bt	O
−	O
log	O
p	O
(	O
v	O
t	O
|	O
X	O
t	O
,	O
s	O
)	O
+	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
(	O
s	O
,	O
ṽt	O
)	O
Bt	O
−	O
log	O
p	O
(	O
ṽ	O
t	O
|	O
X	O
t	O
,	O
s	O
)	O
=	O
αL	O
pseudo	O
+	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
L	O
vanilla	O
,	O
where	O
L	O
pseudo	O
and	O
L	O
vanilla	O
correspond	O
to	O
the	O
training	O
objective	O
of	O
using	O
only	O
the	O
pseudo	O
labels	O
and	O
using	O
only	O
the	O
vanilla	O
noisy	O
labels	O
,	O
respectively	O
.	O
By	O
minimizing	O
L	O
pri	O
,	O
the	O
primary	O
model	O
is	O
trained	O
to	O
learn	O
from	O
the	O
vanilla	O
noisy	O
labels	O
and	O
at	O
the	O
same	O
time	O
imitate	O
the	O
predictions	O
of	O
the	O
auxiliary	O
model	O
.	O

Since	O
the	O
pseudo	O
labels	O
are	O
generated	O
by	O
the	O
auxiliary	O
model	O
that	O
has	O
been	O
trained	O
on	O
a	O
small	O
clean	O
dataset	O
,	O
it	O
can	O
be	O
expected	O
that	O
the	O
combined	O
labels	O
are	O
able	O
to	O
serve	O
as	O
a	O
better	O
approximation	O
to	O
the	O
unknown	O
true	O
labels	O
.	O
Let	O
v	O
t	O
denote	O
the	O
one	O
-	O
hot	O
representation	O
of	O
the	O
unknown	O
true	O
value	O
v	O
t	O
of	O
slot	O
s	O
at	O
turn	O
t.	O
We	O
adopt	O
the	O
mean	O
squared	O
loss	B-MetricName
to	O
define	O
the	O
approximation	O
error	O
of	O
any	O
corrupted	O
labelsv	O
t	O
associated	O
with	O
the	O
noisy	O
training	O
set	O
D	O
n	O
as	O
:	O
Yv	O
=	O
1	O
|	O
D	O
n	O
|	O
|	O
S	O
|	O
Xt	O
Dn	O
s	O
S	O
E	O
Dc	O
[	O
v	O
t	O
−	O
v	O
t	O
2	O
2	O
]	O
,	O
where	O
the	O
expectation	O
ranges	O
over	O
different	O
choices	O
of	O
the	O
clean	O
dataset	O
D	O
c	O
,	O
and	O
|	O
|	O
returns	O
the	O
cardinality	O
of	O
a	O
set	O
.	O
Next	O
,	O
we	O
show	O
that	O
the	O
approximation	O
error	O
of	O
the	O
combined	O
labels	O
can	O
be	O
smaller	O
than	O
that	O
of	O
both	O
the	O
vanilla	O
noisy	O
labels	O
and	O
the	O
generated	O
pseudo	O
labels	O
.	O
The	O
details	O
are	O
presented	O
in	O
Theorem	O
1	O
.	O
Theorem	O
1	O
.	O
The	O
optimal	O
approximation	O
error	O
with	O
respect	O
to	O
the	O
combined	O
labels	O
v	O
c	O
t	O
is	O
smaller	O
than	O
that	O
of	O
the	O
vanilla	O
labelsṽ	O
t	O
and	O
pseudo	O
labelsv	O
t	O
,	O
i.e.	O
,	O
min	O
α	B-HyperparameterName
Y	O
v	O
c	O
<	O
min	O
{	O
Yṽ	O
,	O
Yv	O
}	O
.	O
By	O
setting	O
α	B-HyperparameterName
=	O
Yṽ	O
Yṽ+Yv	O
,	O
Y	O
v	O
c	O
reaches	O
its	O
minimum	O
:	O
min	O
α	B-HyperparameterName
Y	O
v	O
c	O
=	O
YṽYv	O
Yṽ	O
+	O
Yv	O
.	O
Proof	O
.	O
The	O
proof	O
is	O
presented	O
in	O
Appendix	O
A.	O
Theorem	O
1	O
indicates	O
that	O
if	O
α	B-HyperparameterName
is	O
set	O
properly	O
,	O
the	O
combined	O
labels	O
can	O
approximate	O
the	O
unknown	O
true	O
labels	O
more	O
accurately	O
.	O
Hence	O
,	O
we	O
can	O
potentially	O
train	O
the	O
primary	O
model	O
more	O
robustly	O
.	O
Note	O
that	O
we	O
can	O
not	O
calculate	O
the	O
optimal	O
value	O
of	O
α	B-HyperparameterName
directly	O
.	O

We	O
exploit	O
joint	O
goal	O
accuracy	B-MetricName
and	O
slot	O
accuracy	B-MetricName
as	O
the	O
evaluation	O
metrics	O
.	O
The	O
joint	O
goal	O
accuracy	B-MetricName
is	O
2	O
Despite	O
this	O
change	O
,	O
we	O
still	O
call	O
the	O
dataset	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
in	O
this	O
paper	O
for	O
ease	O
of	O
exposition	O
.	O
defined	O
as	O
the	O
proportion	O
of	O
dialogue	O
turns	O
in	O
which	O
the	O
values	O
of	O
all	O
slots	O
are	O
correctly	O
predicted	O
.	O
It	O
is	O
the	O
most	O
important	O
metric	O
in	O
the	O
DST	O
task	O
.	O
The	O
slot	O
accuracy	B-MetricName
is	O
defined	O
as	O
the	O
average	O
of	O
all	O
individual	O
slot	O
accuracies	O
.	O
The	O
accuracy	B-MetricName
of	O
an	O
individual	O
slot	O
is	O
calculated	O
as	O
the	O
ratio	O
of	O
dialogue	O
turns	O
in	O
which	O
its	O
value	O
is	O
correctly	O
predicted	O
.	O
We	O
also	O
propose	O
a	O
new	O
evaluation	O
metric	O
,	O
termed	O
as	O
joint	O
turn	O
accuracy	B-MetricName
.	O
We	O
define	O
joint	O
turn	O
accuracy	B-MetricName
as	O
the	O
proportion	O
of	O
dialogue	O
turns	O
in	O
which	O
the	O
values	O
of	O
all	O
active	O
slots	O
are	O
correctly	O
predicted	O
.	O
A	O
slot	O
becomes	O
active	O
if	O
its	O
value	O
is	O
mentioned	O
in	O
current	O
turn	O
and	O
is	O
not	O
inherited	O
from	O
previous	O
turns	O
.	O
The	O
advantage	O
of	O
joint	O
turn	O
accuracy	B-MetricName
is	O
that	O
it	O
can	O
tell	O
us	O
in	O
how	O
many	O
turns	O
the	O
turn	O
-	O
level	O
information	O
is	O
fully	O
captured	O
by	O
the	O
model	O
.	O

For	O
the	O
auxiliary	O
model	O
,	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
-	O
baseuncased	O
model	O
is	O
utilized	O
as	O
the	O
dialogue	O
context	O
encoder	O
.	O
Another	O
pre	O
-	O
trained	O
BERT	B-MethodName
-	O
base	O
-	O
uncased	O
model	O
with	O
fixed	O
weights	O
is	O
employed	O
to	O
encode	O
the	O
slots	O
and	O
their	O
candidate	O
values	O
.	O
The	O
maximum	O
input	O
length	O
of	O
the	O
BERT	B-MethodName
model	O
is	O
set	O
to	O
512	O
.	O
The	O
number	O
of	O
heads	O
in	O
the	O
slot	B-MethodName
attention	I-MethodName
module	O
is	O
set	O
to	O
4	O
.	O
The	O
output	O
dimension	O
of	O
the	O
linear	O
transformation	O
layer	O
is	O
set	O
to	O
768	O
,	O
which	O
is	O
the	O
same	O
as	O
the	O
dimension	O
of	O
the	O
BERT	B-MethodName
outputs	O
.	O
Recall	B-MetricName
that	O
the	O
previous	O
turn	O
dialogue	O
state	O
is	O
treated	O
as	O
part	O
of	O
the	O
input	O
.	O
The	O
ground	O
-	O
truth	O
one	O
is	O
used	O
during	O
training	O
,	O
and	O
the	O
predicted	O
one	O
is	O
used	O
during	O
testing	O
3	O
.	O
We	O
train	O
the	O
auxiliary	O
model	O
on	O
the	O
clean	O
validation	O
set	O
and	O
the	O
primary	O
model	O
on	O
the	O
noisy	O
training	O
set	O
.	O
When	O
training	O
the	O
auxiliary	O
model	O
,	O
the	O
noisy	O
training	O
set	O
is	O
leveraged	O
to	O
choose	O
the	O
best	O
model	O
.	O
For	O
all	O
primary	O
models	O
,	O
the	O
parameter	O
α	B-HyperparameterName
is	O
set	O
to	O
0.6	O
on	O
MutliWOZ	O
2.0	O
and	O
0.4	O
on	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
.	O
More	O
training	O
details	O
can	O
be	O
found	O
in	O
Appendix	O
B.	O

Table	O
1	O
presents	O
the	O
performance	O
scores	O
of	O
the	O
three	O
different	O
primary	O
DST	O
models	O
on	O
the	O
test	O
sets	O
of	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
&	O
2.4	O
when	O
they	O
are	O
trained	O
using	O
our	O
proposed	O
framework	O
ASSIST	O
.	O
For	O
comparison	O
,	O
we	O
also	O
include	O
the	O
results	O
when	O
only	O
the	O
vanilla	O
labels	O
or	O
only	O
the	O
pseudo	O
labels	O
are	O
used	O
to	O
train	O
the	O
primary	O
models	O
.	O
As	O
can	O
be	O
seen	O
,	O
ASSIST	O
consistently	O
improves	O
the	O
performance	O
of	O
the	O
three	O
primary	O
models	O
on	O
both	O
datasets	O
.	O
More	O
concretely	O
,	O
compared	O
to	O
the	O
results	O
obtained	O
using	O
only	O
the	O
vanilla	O
labels	O
,	O
AS	O
-	O
SIST	O
improves	O
the	O
joint	O
goal	O
accuracy	B-MetricName
of	O
SOM	B-MethodName
-	O
DST	O
,	O
STAR	B-DatasetName
,	O
and	O
AUX	O
-	O
DST	O
on	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
by	O
25.69	O
%	O
,	O
25.82	O
%	O
,	O
and	O
28.16	O
%	O
absolute	O
gains	O
,	O
respectively	O
.	O
On	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
,	O
ASSIST	O
also	O
leads	O
to	O
8.41	O
%	O
,	O
5.79	O
%	O
,	O
and	O
7.77	O
%	O
absolute	O
joint	O
goal	O
accuracy	B-MetricName
gains	O
.	O
From	O
Table	O
1	O
,	O
we	O
further	O
observe	O
that	O
the	O
performance	O
improvements	O
on	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
are	O
lower	O
than	O
on	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
.	O
This	O
is	O
because	O
the	O
training	O
set	O
of	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
is	O
the	O
same	O
as	O
that	O
of	O
MultiWOZ	B-DatasetName
2.1	I-DatasetName
(	O
Eric	O
et	O
al	O
,	O
2020	O
)	O
,	O
in	O
which	O
lots	O
of	O
annotation	O
errors	O
have	O
been	O
fixed	O
.	O
We	O
also	O
observe	O
that	O
all	O
the	O
primary	O
models	O
demonstrate	O
relatively	O
good	O
performance	O
when	O
only	O
the	O
pseudo	O
labels	O
are	O
used	O
.	O
From	O
these	O
results	O
,	O
it	O
can	O
be	O
con	O
-	O
cluded	O
that	O
the	O
pseudo	O
labels	O
are	O
beneficial	O
and	O
they	O
can	O
help	O
us	O
train	O
DST	O
models	O
more	O
robustly	O
.	O
Another	O
observation	O
from	O
Table	O
1	O
is	O
that	O
SOM	B-MethodName
-	O
DST	O
tends	O
to	O
show	O
comparable	O
or	O
even	O
higher	O
joint	O
turn	O
accuracy	B-MetricName
compared	O
to	O
STAR	B-DatasetName
and	O
AUX	O
-	O
DST	O
,	O
although	O
its	O
performance	O
is	O
worse	O
in	O
terms	O
of	O
joint	O
goal	O
accuracy	B-MetricName
and	O
slot	O
accuracy	B-MetricName
.	O
This	O
is	O
because	O
SOM	B-MethodName
-	O
DST	O
focuses	O
on	O
turn	O
-	O
active	O
slots	O
and	O
copies	O
the	O
values	O
for	O
other	O
slots	O
from	O
previous	O
turns	O
,	O
while	O
both	O
STAR	B-DatasetName
and	O
AUX	O
-	O
DST	O
predict	O
the	O
values	O
of	O
all	O
slots	O
from	O
scratch	O
at	O
each	O
turn	O
.	O
These	O
results	O
show	O
that	O
the	O
joint	O
turn	O
accuracy	B-MetricName
can	O
help	O
us	O
understand	O
in	O
more	O
depth	O
how	O
different	O
models	O
behave	O
.	O

The	O
parameter	O
α	B-HyperparameterName
adjusts	O
the	O
weights	O
of	O
the	O
pseudo	O
labels	O
and	O
vanilla	O
labels	O
in	O
the	O
training	O
phase	O
.	O
Here	O
,	O
we	O
study	O
the	O
effects	O
of	O
α	B-HyperparameterName
by	O
varying	O
its	O
value	O
in	O
the	O
range	O
of	O
0	B-DatasetName
to	O
1	O
with	O
a	O
step	B-HyperparameterName
size	I-HyperparameterName
of	O
0.1	O
.	O
Figure	O
4	O
shows	O
the	O
results	O
of	O
AUX	O
-	O
DST	O
.	O
As	O
can	O
be	O
seen	O
,	O
α	B-HyperparameterName
plays	O
an	O
important	O
role	O
in	O
balancing	O
the	O
pseudo	O
labels	O
and	O
vanilla	O
labels	O
.	O
The	O
best	O
performance	O
is	O
achieved	O
when	O
α	B-HyperparameterName
is	O
set	O
to	O
0.6	O
on	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
and	O
0.4	O
on	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
.	O
Since	O
the	O
training	O
set	O
of	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
has	O
more	O
noisy	O
labels	O
than	O
that	O
of	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
,	O
more	O
emphasis	O
should	O
be	O
put	O
on	O
its	O
pseudo	O
labels	O
to	O
obtain	O
the	O
best	O
performance	O
.	O
It	O
is	O
also	O
noted	O
that	O
the	O
performance	O
difference	O
between	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
and	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
dwindles	O
away	O
as	O
α	B-HyperparameterName
increases	O
.	O
This	O
is	O
because	O
the	O
vanilla	O
labels	O
will	O
contribute	O
less	O
to	O
the	O
training	O
of	O
the	O
primary	O
model	O
when	O
α	B-HyperparameterName
is	O
set	O
to	O
be	O
larger	O
.	O

We	O
first	O
investigate	O
whether	O
the	O
pseudo	O
labels	O
are	O
consistent	O
with	O
the	O
true	O
labels	O
.	O
To	O
achieve	O
this	O
goal	O
,	O
we	O
can	O
compute	O
the	O
joint	O
goal	O
accuracy	B-MetricName
and	O
joint	O
turn	O
accuracy	B-MetricName
of	O
the	O
auxiliary	O
model	O
on	O
the	O
training	O
set	O
.	O
However	O
,	O
the	O
true	O
labels	O
of	O
the	O
training	O
set	O
are	O
unavailable	O
.	O
As	O
an	O
alternative	O
,	O
we	O
treat	O
the	O
vanilla	O
noisy	O
labels	O
as	O
true	O
labels	O
(	O
note	O
that	O
only	O
a	O
portion	O
of	O
the	O
vanilla	O
labels	O
are	O
noisy	O
)	O
.	O
In	O
this	O
experiment	O
,	O
we	O
also	O
vary	O
the	O
number	O
of	O
clean	O
dialogues	O
to	O
train	O
the	O
auxiliary	O
model	O
.	O
Figure	O
6	O
presents	O
the	O
results	O
.	O
As	O
shown	O
in	O
Figure	O
6	O
,	O
the	O
auxiliary	O
model	O
achieves	O
higher	O
performance	O
when	O
more	O
clean	O
dialogues	O
are	O
utilized	O
to	O
train	O
it	O
.	O
If	O
the	O
entire	O
validation	O
set	O
is	O
used	O
,	O
it	O
achieves	O
around	O
50	O
%	O
joint	O
goal	O
accuracy	B-MetricName
and	O
around	O
75	O
%	O
joint	O
turn	O
accuracy	B-MetricName
.	O
Given	O
that	O
the	O
vanilla	O
noisy	O
labels	O
are	O
regarded	O
as	O
the	O
true	O
labels	O
,	O
we	O
can	O
conjecture	O
that	O
the	O
true	O
performance	O
is	O
actually	O
higher	O
.	O
This	O
experiment	O
shows	O
that	O
the	O
pseudo	O
labels	O
are	O
consistent	O
with	O
the	O
unknown	O
true	O
labels	O
to	O
some	O
extent	O
and	O
can	O
serve	O
as	O
a	O
good	O
complement	O
to	O
the	O
vanilla	O
noisy	O
labels	O
.	O

We	O
further	O
investigate	O
the	O
error	O
rate	O
with	O
respect	O
to	O
each	O
slot	O
.	O
We	O
adopt	O
AUX	O
-	O
DST	O
as	O
the	O
primary	O
model	O
and	O
use	O
AUX	O
-	O
DST	O
(	O
w/o	O
p	O
)	O
to	O
denote	O
the	O
case	O
when	O
only	O
the	O
vanilla	O
labels	O
are	O
employed	O
to	O
train	O
the	O
model	O
.	O
The	O
results	O
on	O
the	O
test	O
set	O
of	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
are	O
illustrated	O
in	O
Figure	O
7	O
,	O
from	O
which	O
we	O
can	O
observe	O
that	O
the	O
slot	O
"	O
hotel	O
-	O
type	O
"	O
has	O
the	O
highest	O
error	O
rate	O
.	O
Even	O
though	O
the	O
error	O
rate	O
is	O
reduced	O
with	O
the	O
aid	O
of	O
the	O
pseudo	O
labels	O
,	O
it	O
is	O
still	O
the	O
highest	O
one	O
among	O
all	O
the	O
slots	O
.	O
This	O
is	O
because	O
the	O
labels	O
of	O
this	O
slot	O
are	O
confusing	O
.	O
It	O
is	O
also	O
observed	O
that	O
the	O
"	O
name	O
"	O
-	O
related	O
slots	O
have	O
relatively	O
high	O
error	O
rates	O
.	O
However	O
,	O
when	O
the	O
pseudo	O
labels	O
are	O
used	O
,	O
their	O
error	O
rates	O
reduce	O
remarkably	O
.	O
Besides	O
,	O
we	O
observe	O
that	O
the	O
error	O
rates	O
of	O
some	O
slots	O
are	O
higher	O
when	O
the	O
pseudo	O
labels	O
are	O
leveraged	O
.	O
This	O
is	O
probably	O
due	O
to	O
the	O
fact	O
that	O
we	O
have	O
used	O
the	O
same	O
parameter	O
α	B-HyperparameterName
to	O
combine	O
the	O
pseudo	O
labels	O
and	O
vanilla	O
labels	O
of	O
all	O
slots	O
.	O
In	O
practice	O
,	O
the	O
noise	O
rate	O
with	O
respect	O
to	O
each	O
slot	O
in	O
the	O
vanilla	O
labels	O
may	O
not	O
be	O
exactly	O
the	O
same	O
.	O
This	O
observation	O
in	O
-	O
spires	O
us	O
that	O
more	O
advanced	O
techniques	O
should	O
be	O
developed	O
to	O
combine	O
the	O
pseudo	O
labels	O
and	O
vanilla	O
labels	O
,	O
which	O
we	O
leave	O
as	O
our	O
future	O
work	O
.	O

In	O
this	O
work	O
,	O
we	O
have	O
presented	O
a	O
general	O
framework	O
ASSIST	O
,	O
aiming	O
to	O
train	O
DST	O
models	O
robustly	O
from	O
noisy	O
labels	O
.	O
ASSIST	O
leverages	O
an	O
auxiliary	O
model	O
that	O
is	O
trained	O
on	O
a	O
small	O
clean	O
dataset	O
to	O
generate	O
pseudo	O
labels	O
for	O
the	O
large	O
noisy	O
training	O
set	O
.	O
The	O
pseudo	O
labels	O
are	O
combined	O
with	O
the	O
vanilla	O
labels	O
to	O
train	O
the	O
primary	O
model	O
.	O
Both	O
theoretical	O
analysis	O
and	O
empirical	O
study	O
have	O
verified	O
the	O
validity	O
of	O
our	O
proposed	O
framework	O
.	O
In	O
the	O
future	O
,	O
we	O
intend	O
to	O
explore	O
more	O
advanced	O
techniques	O
to	O
combine	O
the	O
pseudo	O
labels	O
and	O
vanilla	O
noisy	O
labels	O
in	O
a	O
better	O
way	O
.	O
Considering	O
that	O
the	O
pseudo	O
labels	O
are	O
generated	O
by	O
the	O
auxiliary	O
model	O
that	O
is	O
trained	O
on	O
an	O
extra	O
small	O
clean	O
dataset	O
and	O
this	O
clean	O
dataset	O
is	O
independent	O
of	O
the	O
noisy	O
training	O
set	O
,	O
we	O
can	O
regard	O
the	O
pseudo	O
labels	O
and	O
vanilla	O
labels	O
as	O
independent	O
of	O
each	O
other	O
.	O
Consequently	O
,	O
we	O
obtain	O
:	O
E	O
Dc	O
[	O
(	O
ṽ	O
t	O
−	O
v	O
t	O
)	O
T	O
(	O
v	O
t	O
−	O
v	O
t	O
)	O
]	O
=	O
[	O
E	O
Dc	O
[	O
ṽ	O
t	O
−	O
v	O
t	O
]	O
]	O
T	O
E	O
Dc	O
[	O
v	O
t	O
−	O
v	O
t	O
]	O
=	O
[	O
E	O
Dc	O
[	O
ṽ	O
t	O
−	O
v	O
t	O
]	O
]	O
T	O
E	O
Dc	O
[	O
v	O
t	O
−	O
E	O
Dc	O
[	O
v	O
t	O
]	O
]	O
=	O
[	O
E	O
Dc	O
[	O
ṽ	O
t	O
−	O
v	O
t	O
]	O
]	O
T	O
0	B-DatasetName
=	O
0	B-DatasetName
.	O
Based	O
on	O
the	O
formula	O
above	O
,	O
we	O
can	O
now	O
calculate	O
the	O
approximation	O
error	O
with	O
respect	O
to	O
the	O
combined	O
label	O
v	O
c	O
t	O
of	O
slot	O
s	O
as	O
below	O
:	O
E	O
Dc	O
[	O
v	O
c	O
t	O
−	O
v	O
t	O
2	O
2	O
]	O
=	O
E	O
Dc	O
[	O
αv	O
t	O
+	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
ṽ	O
t	O
−	O
v	O
t	O
2	O
2	O
]	O
=	O
E	O
Dc	O
[	O
α	B-HyperparameterName
(	O
v	O
t	O
−	O
v	O
t	O
)	O
+	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
(	O
ṽ	O
t	O
−	O
v	O
t	O
)	O
2	O
2	O
]	O
=	O
α	B-HyperparameterName
2	O
E	O
Dc	O
[	O
v	O
t	O
−	O
v	O
t	O
2	O
2	O
]	O
+	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
2	O
E	O
Dc	O
[	O
ṽ	O
t	O
−	O
v	O
t	O
2	O
2	O
]	O
,	O
where	O
the	O
last	O
equality	O
holds	O
because	O
of	O
E	O
Dc	O
[	O
(	O
ṽ	O
t	O
−	O
v	O
t	O
)	O
T	O
(	O
v	O
t	O
−	O
v	O
t	O
)	O
]	O
=	O
0	B-DatasetName
.	O
Then	O
,	O
we	O
have	O
:	O
Y	O
v	O
c	O
=	O
1	O
|	O
D	O
n	O
|	O
|	O
S	O
|	O
Xt	O
Dn	O
s	O
S	O
E	O
Dc	O
[	O
v	O
c	O
t	O
−	O
v	O
t	O
2	O
2	O
]	O
=	O
α	B-HyperparameterName
2	O
|	O
D	O
n	O
|	O
|	O
S	O
|	O
Xt	O
Dn	O
s	O
S	O
E	O
Dc	O
[	O
v	O
t	O
−	O
v	O
t	O
2	O
2	O
]	O
+	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
2	O
|	O
D	O
n	O
|	O
|	O
S	O
|	O
Xt	O
Dn	O
s	O
S	O
E	O
Dc	O
[	O
ṽ	O
t	O
−	O
v	O
t	O
2	O
2	O
]	O
=	O
α	B-HyperparameterName
2	O
Yv	O
+	O
(	O
1	O
−	O
α	B-HyperparameterName
)	O
2	O
Yṽ	O
.	O
Y	O
v	O
c	O
reaches	O
its	O
minimum	O
when	O
α	B-HyperparameterName
=	O
Yṽ	O
Yṽ+Yv	O
,	O
and	O
min	O
α	B-HyperparameterName
Y	O
v	O
c	O
=	O
YṽYv	O
Yṽ	O
+	O
Yv	O
,	O
which	O
concludes	O
the	O
proof	O
.	O

Note	O
that	O
the	O
proposed	O
auxiliary	O
model	O
is	O
also	O
applied	O
as	O
one	O
primary	O
model	O
in	O
our	O
experiments	O
.	O
In	O
both	O
cases	O
,	O
AdamW	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
is	O
adopted	O
as	O
the	O
optimizer	B-HyperparameterName
,	O
and	O
a	O
linear	O
schedule	O
with	O
warmup	O
is	O
created	O
to	O
adjust	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
dynamically	O
.	O
The	O
peak	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
2.5e	O
-	O
5	O
.	O
The	O
warmup	O
proportion	O
is	O
fixed	O
at	O
0.1	O
.	O
The	O
dropout	O
(	O
Srivastava	O
et	O
al	O
,	O
2014	O
)	O
probability	O
and	O
word	O
dropout	O
(	O
Bowman	O
et	O
al	O
,	O
2016	O
)	O
probability	O
are	O
also	O
fixed	O
at	O
0.1	O
.	O
When	O
taken	O
as	O
the	O
auxiliary	O
model	O
,	O
the	O
model	O
is	O
trained	O
for	O
at	O
most	O
30	O
epochs	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8	O
.	O
When	O
taken	O
as	O
the	O
primary	O
model	O
,	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
and	O
training	O
epochs	O
are	O
set	O
to	O
8	O
and	O
12	O
,	O
respectively	O
.	O
The	O
best	O
model	O
is	O
chosen	O
according	O
to	O
the	O
performance	O
on	O
the	O
validation	O
set	O
.	O
We	O
apply	O
left	O
truncation	O
when	O
the	O
input	O
exceeds	O
the	O
maximum	O
input	O
length	O
of	O
BERT	B-MethodName
.	O
For	O
SOM	B-MethodName
-	O
DST	O
and	O
STAR	B-DatasetName
,	O
the	O
default	O
hyperparameters	O
are	O
adopted	O
when	O
they	O
are	O
applied	O
as	O
the	O
primary	O
model	O
(	O
except	O
setting	O
num_workers	O
=	O
0	B-DatasetName
)	O
.	O

We	O
use	O
the	O
large	O
Transformer	B-MethodName
from	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
with	O
8	O
encoder	O
and	O
decoder	O
layers	O
and	O
replicate	O
all	O
the	O
parameters	O
from	O
.	O
The	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
in	O
the	O
model	O
are	O
approximately	O
248	O
Million	O
and	O
it	O
takes	O
∼26	O
hours	O
on	O
4	O
Nvidia	O
V100	O
(	O
32	O
GB	O
)	O
GPUs	O
.	O
NMT	O
(	O
mBART	B-MethodName
)	O
For	O
this	O
,	O
we	O
use	O
12	O
Transformer	B-MethodName
encoder	O
and	O
decoder	O
layers	O
,	O
with	O
total	O
number	O
of	O
model	O
parameters	O
∼611	O
Million	O
.	O
We	O
use	O
the	O
pretrained	O
mBART	B-MethodName
for	O
initializing	O
the	O
model	O
weights	O
.	O
We	O
follow	O
the	O
recommendations	O
of	O
Liu	O
et	O
al	O
(	O
2020	O
)	O
for	O
the	O
hyperparameter	O
settings	O
.	O
We	O
stop	O
the	O
training	O
after	O
25	O
K	O
gradient	O
updates	O
for	O
the	O
model	O
.	O
These	O
updates	O
take	O
∼35	O
hours	O
on	O
4	O
Nvidia	O
V100	O
(	O
32	O
GB	O
)	O
GPUs	O
.	O

We	O
use	O
case	O
-	O
insensitive	O
BLEU	B-MetricName
scores	O
(	O
Papineni	O
et	O
al	O
,	O
2002	O
)	O
calculated	O
using	O
sacreBLEU	B-MetricName
2	O
(	O
Post	O
,	O
2018	O
)	O
.	O
These	O
scores	O
are	O
calculated	O
on	O
the	O
validation	O
set	O
to	O
decide	O
our	O
primary	O
and	O
contrastive	O
submissions	O
.	O
For	O
evaluating	O
performance	O
on	O
the	O
test	O
set	O
,	O
the	O
organizers	O
use	O
BLEU	B-MetricName
,	O
TER	O
(	O
Snover	O
et	O
al	O
,	O
2006	O
)	O
,	O
and	O
RIBES	O
(	O
Isozaki	O
et	O
al	O
,	O
2010	O
)	O
.	O

Results	O
Table	O
3	O
shows	O
our	O
results	O
on	O
the	O
test	O
set	O
for	O
our	O
primary	O
and	O
contrastive	O
submissions	O
.	O
We	O
observed	O
the	O
performance	O
of	O
our	O
three	O
model	O
settings	O
on	O
the	O
validation	O
set	O
,	O
and	O
we	O
selected	O
the	O
mBART	B-MethodName
model	O
as	O
our	O
primary	O
submission	O
and	O
SMT	O
model	O
as	O
the	O
contrastive	O
submission	O
for	O
hi	O
↔	O
mr	O
.	O
Similarly	O
,	O
the	O
mBART	B-MethodName
model	O
forms	O
our	O
primary	O
submission	O
for	O
es	O
↔	O
pt	O
.	O
Table	O
4	O
lists	O
our	O
final	O
results	O
on	O
this	O
shared	O
task	O
.	O
We	O
also	O
list	O
the	O
BLEU	B-MetricName
scores	O
for	O
the	O
submission	O
that	O
got	O
first	O
rank	O
in	O
each	O
of	O
the	O
language	O
directions	O
.	O
Since	O
the	O
test	O
sets	O
were	O
hidden	O
at	O
the	O
time	O
of	O
submission	O
,	O
we	O
do	O
not	O
report	O
our	O
numbers	O
on	O
the	O
standard	O
Transformer	B-MethodName
architecture	O
.	O
Analysis	O
Even	O
though	O
Marathi	O
and	O
Portuguese	O
are	O
not	O
present	O
during	O
the	O
pre	O
-	O
training	O
phase	O
of	O
mBART	B-MethodName
,	O
fine	O
-	O
tuning	O
on	O
these	O
languages	O
provides	O
significant	O
boosts	O
over	O
SMT	O
and	O
standard	O
Transformer	B-MethodName
.	O
This	O
shows	O
that	O
some	O
level	O
of	O
language	O
independent	O
multilingual	O
embeddings	O
are	O
present	O
in	O
the	O
pre	O
-	O
trained	O
model	O
weights	O
which	O
can	O
be	O
exploited	O
for	O
the	O
transfer	O
task	O
.	O

We	O
present	O
a	O
brief	O
overview	O
of	O
the	O
models	O
which	O
we	O
considered	O
for	O
our	O
analysis	O
in	O
this	O
section	O
.	O
Bi	O
-	O
Directional	O
Attention	O
Flow	O
(	O
BiDAF	O
)	O
:	O
This	O
model	O
,	O
proposed	O
by	O
Seo	O
et	O
al	O
(	O
2016	O
)	O
,	O
is	O
a	O
hierarchical	O
multi	O
-	O
stage	O
end	O
-	O
to	O
-	O
end	O
neural	O
network	O
which	O
takes	O
inputs	O
of	O
different	O
granularity	O
(	O
character	O
,	O
word	O
and	O
phrase	O
)	O
to	O
obtain	O
a	O
query	O
-	O
aware	O
context	O
representation	O
using	O
memory	O
-	O
less	O
contextto	O
-	O
query	O
(	O
C2Q	O
)	O
and	O
query	O
-	O
to	O
-	O
context	O
(	O
Q2C	O
)	O
attention	O
.	O
This	O
representation	O
can	O
then	O
be	O
used	O
for	O
different	O
final	O
tasks	O
.	O
Many	O
versions	O
of	O
this	O
model	O
(	O
with	O
different	O
types	O
of	O
input	O
features	O
)	O
exist	O
on	O
the	O
SQuAD	B-DatasetName
leaderboard	O
,	O
but	O
the	O
basic	O
architecture	O
2	O
(	O
which	O
we	O
use	O
for	O
our	O
experiments	O
in	O
this	O
paper	O
)	O
contains	O
character	O
,	O
word	O
and	O
phrase	O
embedding	O
layers	O
,	O
followed	O
by	O
an	O
attention	O
flow	O
layer	O
,	O
a	O
modeling	O
layer	O
and	O
an	O
output	O
layer	O
.	O
Gated	O
Self	O
-	O
Matching	O
Networks	O
(	O
R	O
-	O
Net	O
)	O
:	O
This	O
model	O
,	O
proposed	O
by	O
Wang	O
et	O
al	O
(	O
2017	O
)	O
,	O
is	O
a	O
multilayer	O
end	O
-	O
to	O
-	O
end	O
neural	O
network	O
whose	O
novelty	O
lies	O
in	O
the	O
use	O
of	O
a	O
gated	O
attention	O
mechanism	O
so	O
as	O
to	O
give	O
different	O
levels	O
of	O
importance	O
to	O
different	O
passage	O
parts	O
.	O
It	O
also	O
uses	O
self	O
-	O
matching	O
attention	O
for	O
the	O
context	O
to	O
aggregate	O
evidence	O
from	O
the	O
entire	O
passage	O
to	O
refine	O
the	O
query	O
-	O
aware	O
context	O
representation	O
obtained	O
.	O
The	O
architecture	O
contains	O
character	O
and	O
word	O
embedding	O
layers	O
,	O
followed	O
by	O
question	O
-	O
passage	O
encoding	O
and	O
matching	O
layers	O
,	O
a	O
passage	O
self	O
-	O
matching	O
layer	O
and	O
an	O
output	O
layer	O
.	O
The	O
implementation	O
we	O
used	O
3	O
had	O
some	O
minor	O
changes	O
for	O
increased	O
efficiency	O
.	O
Chen	O
et	O
al	O
(	O
2017	O
)	O
,	O
focuses	O
on	O
answering	O
open	O
-	O
domain	O
factoid	O
questions	O
using	O
Wikipedia	O
,	O
but	O
also	O
performs	O
well	O
on	O
SQuAD	B-DatasetName
(	O
skipping	O
the	O
document	O
retrieval	O
stage	O
)	O
.	O
Its	O
implementation	O
4	O
has	O
paragraph	O
and	O
question	O
encoding	O
layers	O
,	O
and	O
an	O
output	O
layer	O
.	O
The	O
paragraph	O
encoding	O
is	O
computed	O
by	O
representing	O
each	O
context	O
as	O
a	O
sequence	O
of	O
feature	O
vectors	O
derived	O
from	O
tokens	O
:	O
word	O
embedding	O
,	O
exact	B-MetricName
match	I-MetricName
with	O
question	O
word	O
,	O
POS	O
/	O
NER	B-TaskName
/	O
TF	O
and	O
aligned	O
question	O
embedding	O
,	O
and	O
passing	O
these	O
as	O
inputs	O
to	O
a	O
recurrent	O
neural	O
network	O
.	O
The	O
question	O
encoding	O
is	O
obtained	O
by	O
using	O
word	B-TaskName
embeddings	I-TaskName
as	O
inputs	O
to	O
a	O
recurrent	O
neural	O
network	O
.	O
Multi	O
-	O
Paragraph	O
Reading	B-TaskName
Comprehension	I-TaskName
(	O
DocQA	O
)	O
:	O
This	O
model	O
,	O
proposed	O
by	O
Clark	O
and	O
Gardner	O
(	O
2017	O
)	O
,	O
aims	O
to	O
answer	O
questions	O
based	O
on	O
entire	O
documents	O
(	O
multiple	O
paras	O
)	O
rather	O
than	O
specific	O
paragraphs	O
,	O
but	O
also	O
gives	O
good	O
results	O
for	O
SQuAD	B-DatasetName
(	O
considering	O
the	O
given	O
paragraph	O
as	O
the	O
document	O
)	O
.	O
The	O
implementation	O
5	O
contains	O
input	O
,	O
embedding	O
(	O
character	O
and	O
word	O
-	O
level	O
)	O
,	O
pre	O
-	O
processing	O
(	O
shared	O
bidirectional	B-MethodName
GRU	I-MethodName
between	O
question	O
and	O
passage	O
)	O
,	O
attention	O
(	O
similar	O
to	O
BiDAF	O
)	O
,	O
self	O
-	O
attention	O
(	O
residual	O
)	O
and	O
output	O
(	O
bidirectional	B-MethodName
GRU	I-MethodName
and	O
linear	O
scoring	O
)	O
layers	O
.	O

The	O
span	O
-	O
level	O
performance	O
is	O
measured	O
typically	O
by	O
Exact	B-MetricName
Match	I-MetricName
(	O
EM	B-MetricName
)	O
and	O
F1	B-MetricName
metrics	O
which	O
are	O
reported	O
with	O
respect	O
to	O
the	O
ground	O
truth	O
answer	O
spans	O
.	O
These	O
results	O
are	O
summarized	O
in	O
Table	O
1	O
.	O
The	O
DocQA	O
model	O
gives	O
the	O
best	O
overall	O
performance	O
which	O
aligns	O
well	O
with	O
our	O
expectation	O
,	O
owing	O
to	O
the	O
usage	O
of	O
and	O
improvements	O
in	O
the	O
prior	O
mechanisms	O
introduced	O
in	O
BiDAF	O
and	O
R	O
-	O
Net	O
.	O

To	O
investigate	O
trends	O
at	O
different	O
granularities	O
,	O
we	O
also	O
measure	O
sentence	O
retrieval	O
performance	O
.	O
The	O
context	O
given	O
for	O
each	O
question	O
-	O
answer	O
pair	O
is	O
split	O
into	O
sentences	O
using	O
the	O
NLTK	O
sentence	O
tokenizer	O
7	O
,	O
and	O
the	O
sentence	O
-	O
level	O
accuracy	B-MetricName
of	O
each	O
of	O
the	O
models	O
is	O
computed	O
(	O
Table	O
1	O
)	O
.	O
Since	O
the	O
default	O
sentence	O
tokenizer	O
for	O
English	O
in	O
NLTK	O
is	O
pre	O
-	O
trained	O
on	O
Penn	B-DatasetName
Treebank	I-DatasetName
data	O
which	O
contains	O
formal	O
language	O
(	O
news	O
articles	O
)	O
,	O
we	O
expect	O
it	O
to	O
perform	O
reasonably	O
well	O
on	O
Wikipedia	O
articles	O
too	O
.	O
We	O
observe	O
that	O
all	O
the	O
models	O
have	O
high	O
sentencelevel	O
accuracy	B-MetricName
,	O
with	O
DocQA	O
outperforming	O
the	O
other	O
models	O
with	O
respect	O
to	O
this	O
metric	O
as	O
well	O
.	O
Interestingly	O
,	O
DrQA	O
performs	O
better	O
on	O
sentence	O
retrieval	O
accuracy	B-MetricName
than	O
both	O
BiDAF	O
and	O
R	O
-	O
Net	O
,	O
but	O
has	O
a	O
worse	O
span	O
-	O
level	O
exact	B-MetricName
match	I-MetricName
score	O
,	O
which	O
is	O
probably	O
because	O
of	O
the	O
rich	O
feature	O
vector	O
representation	O
of	O
the	O
passage	O
due	O
to	O
the	O
model	O
's	O
focus	O
on	O
open	O
domain	O
QA	O
(	O
and	O
hence	O
retrieval	O
)	O
.	O
But	O
,	O
none	O
of	O
these	O
neural	O
models	O
have	O
near	O
-	O
perfect	O
ability	O
to	O
identify	O
the	O
correct	O
sentence	O
,	O
and	O
∼90	O
%	O
accuracy	B-MetricName
indicates	O
that	O
even	O
if	O
we	O
have	O
a	O
perfect	O
answer	B-TaskName
selection	I-TaskName
method	O
,	O
this	O
is	O
the	O
best	O
EM	B-MetricName
score	O
we	O
can	O
achieve	O
.	O
However	O
,	O
incorrect	O
span	O
identification	O
contributes	O
more	O
to	O
errors	O
in	O
prediction	O
for	O
all	O
the	O
models	O
,	O
as	O
seen	O
from	O
the	O
disparity	O
between	O
the	O
sentence	O
-	O
level	O
accuracies	O
and	O
the	O
final	O
spanlevel	O
exact	B-MetricName
match	I-MetricName
score	O
values	O
.	O

In	O
Table	O
2	O
,	O
we	O
analyze	O
the	O
number	O
of	O
erroneous	O
predictions	O
which	O
overlap	O
for	O
different	O
pairs	O
of	O
models	O
,	O
i.e.	O
,	O
which	O
belong	O
to	O
the	O
intersection	O
of	O
the	O
sets	O
of	O
incorrect	O
answers	O
generated	O
by	O
models	O
in	O
each	O
(	O
row	O
,	O
column	O
)	O
pair	O
.	O
Thus	O
,	O
the	O
values	O
in	O
the	O
table	O
represent	O
a	O
symmetric	O
matrix	O
with	O
diagonal	O
elements	O
indicating	O
the	O
number	O
of	O
errors	O
which	O
each	O
model	O
commits	O
.	O
This	O
analysis	O
can	O
be	O
useful	O
while	O
determining	O
suitable	O
models	O
for	O
creating	O
meta	O
ensembles	O
since	O
a	O
low	O
incorrect	O
answer	O
overlap	O
indicates	O
that	O
the	O
combined	O
predictive	O
power	O
One	O
way	O
in	O
which	O
this	O
analysis	O
can	O
help	O
in	O
exploring	O
ensemble	O
-	O
based	O
methods	O
is	O
that	O
instead	O
of	O
trying	O
all	O
possible	O
combinations	O
of	O
models	O
,	O
we	O
can	O
adopt	O
a	O
greedy	O
approach	O
based	O
on	O
the	O
incorrect	O
answer	O
overlap	O
metric	O
to	O
decide	O
which	O
model	O
to	O
add	O
to	O
the	O
ensemble	O
(	O
and	O
only	O
if	O
it	O
leads	O
to	O
a	O
statistically	O
significant	O
difference	O
in	O
this	O
overlap	O
)	O
.	O
After	O
determining	O
an	O
approximately	O
optimal	O
set	O
of	O
models	O
which	O
such	O
an	O
ensemble	O
should	O
be	O
composed	O
of	O
,	O
each	O
of	O
these	O
models	O
can	O
be	O
trained	O
independently	O
followed	O
by	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
(	O
to	O
select	O
one	O
of	O
the	O
generated	O
answers	O
)	O
using	O
techniques	O
like	O
logistic	B-MethodName
regression	I-MethodName
,	O
a	O
feed	O
-	O
forward	O
neural	O
network	O
or	O
a	O
recurrent	O
or	O
convolutional	O
neural	O
network	O
with	O
input	O
features	O
based	O
on	O
the	O
question	O
,	O
the	O
passage	O
and	O
their	O
token	O
overlap	O
.	O
The	O
entire	O
network	O
can	O
also	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
.	O
Also	O
,	O
all	O
5	O
models	O
combined	O
have	O
an	O
error	O
overlap	O
of	O
13.68	O
%	O
,	O
i.e.	O
,	O
if	O
we	O
had	O
a	O
mechanism	O
to	O
perfectly	O
choose	O
between	O
these	O
models	O
,	O
we	O
would	O
get	O
an	O
Exact	B-MetricName
Match	I-MetricName
score	O
of	O
86.32	O
%	O
.	O
This	O
indicates	O
that	O
future	O
work	O
based	O
on	O
ensembling	O
different	O
neural	O
models	O
can	O
give	O
promising	O
results	O
and	O
is	O
worth	O
exploring	O
.	O
An	O
example	O
of	O
a	O
passage	O
-	O
question	O
-	O
answer	O
that	O
all	O
of	O
the	O
models	O
get	O
wrong	O
is	O
:	O
Passage	O
:	O
The	O
University	O
of	O
Warsaw	O
was	O
established	O
in	O
1816	O
,	O
when	O
the	O
partitions	O
of	O
Poland	O
separated	O
Warsaw	O
from	O
the	O
oldest	O
and	O
most	O
influential	O
Polish	O
academic	O
center	O
,	O
in	O
Krakow	O
.	O
Warsaw	O
University	O
of	O
Technology	O
is	O
the	O
second	O
academic	O
school	O
of	O
technology	O
in	O
the	O
country	O
,	O
and	O
one	O
of	O
the	O
largest	O
in	O
East	O
-	O
Central	O
Europe	O
,	O
employing	O
2	O
,	O
000	O
professors	O
.	O
Other	O
institutions	O
for	O
higher	O
education	O
include	O
the	O
Medical	O
University	O
of	O
Warsaw	O
,	O
the	O
largest	O
medical	O
school	O
in	O
Poland	O
and	O
one	O
of	O
the	O
most	O
prestigious	O
,	O
the	O
National	O
Defence	O
University	O
,	O
highest	O
military	O
academic	O
institution	O
in	O
Poland	O
,	O
the	O
Fryderyk	O
Chopin	O
University	O
of	O
Music	O
the	O
oldest	O
and	O
largest	O
music	O
school	O
in	O
Poland	O
,	O
and	O
one	O
of	O
the	O
largest	O
in	O
Europe	O
,	O
the	O
Warsaw	O
School	O
of	O
Economics	O
,	O
the	O
oldest	O
and	O
most	O
renowned	O
economic	O
university	O
in	O
the	O
country	O
,	O
and	O
the	O
Warsaw	O
University	O
of	O
Life	O
Sciences	O
the	O
largest	O
agricultural	O
university	O
founded	O
in	O
1818	O
.	O
Question	O
:	O
What	O
is	O
one	O
of	O
the	O
largest	O
music	O
schools	O
in	O
Europe	O
?	O
Answer	O
:	O
Fryderyk	O
Chopin	O
University	O
of	O
Music	O
This	O
passage	O
-	O
question	O
-	O
answer	O
is	O
difficult	O
for	O
automatic	O
processing	O
because	O
there	O
several	O
entities	O
of	O
the	O
same	O
type	O
(	O
school	O
/	O
university	O
)	O
in	O
the	O
passage	O
,	O
and	O
the	O
question	O
is	O
a	O
paraphrase	O
of	O
one	O
segment	O
of	O
a	O
very	O
long	O
,	O
syntactically	O
complicated	O
sentence	O
which	O
contains	O
the	O
information	O
required	O
to	O
be	O
able	O
to	O
infer	O
the	O
correct	O
answer	O
.	O
This	O
presents	O
an	O
interesting	O
challenge	O
,	O
and	O
such	O
qualitative	O
observations	O
can	O
be	O
used	O
to	O
formulate	O
a	O
general	O
technique	O
for	O
effectively	O
testing	O
machine	O
reading	O
systems	O
.	O

For	O
qualitative	O
error	O
analysis	O
,	O
we	O
sample	O
100	O
incorrect	O
predictions	O
(	O
based	O
on	O
EM	B-MetricName
)	O
from	O
each	O
model	O
and	O
try	O
to	O
find	O
common	O
error	O
categories	O
.	O
Broadly	O
,	O
the	O
errors	O
observed	O
were	O
either	O
because	O
of	O
incorrect	O
answer	O
span	O
boundaries	O
or	O
inability	O
to	O
infer	O
the	O
meaning	O
of	O
the	O
question	O
/	O
passage	O
.	O
Examples	O
of	O
each	O
error	O
type	O
are	O
shown	O
in	O
Table	O
3	O
,	O
and	O
these	O
are	O
further	O
described	O
below	O
.	O

In	O
this	O
section	O
,	O
we	O
record	O
the	O
main	O
observations	O
from	O
our	O
qualitative	O
error	O
analysis	O
and	O
analyze	O
potential	O
reasons	O
for	O
the	O
error	O
trends	O
observed	O
.	O
Figure	O
4	O
shows	O
the	O
different	O
types	O
of	O
errors	O
in	O
predictions	O
by	O
various	O
models	O
.	O
We	O
observe	O
that	O
BiDAF	O
makes	O
many	O
boundarybased	O
errors	O
which	O
indicates	O
that	O
a	O
better	O
output	O
layer	O
(	O
since	O
this	O
is	O
responsible	O
for	O
span	O
identification	O
-	O
although	O
errors	O
might	O
have	O
percolated	O
from	O
previous	O
layers	O
,	O
most	O
of	O
these	O
are	O
cases	O
where	O
the	O
model	O
almost	O
got	O
the	O
correct	O
answer	O
but	O
not	O
exactly	O
)	O
or	O
some	O
post	O
-	O
processing	O
of	O
the	O
answer	O
might	O
help	O
improve	O
performance	O
.	O
Paraphrases	O
also	O
contribute	O
to	O
almost	O
15	O
%	O
of	O
errors	O
observed	O
which	O
indicates	O
that	O
the	O
question	O
and	O
the	O
relevant	O
parts	O
of	O
the	O
context	O
are	O
not	O
effectively	O
matched	O
in	O
these	O
cases	O
.	O
We	O
observe	O
that	O
R	O
-	O
Net	O
makes	O
fewer	O
boundary	O
errors	O
,	O
perhaps	O
because	O
self	O
-	O
attention	O
enables	O
it	O
to	O
accumulate	O
evidence	O
and	O
return	O
better	O
answer	O
spans	O
,	O
although	O
this	O
leads	O
to	O
more	O
errors	O
of	O
the	O
'	O
shorter	O
'	O
answer	O
type	O
than	O
'	O
longer	O
'	O
.	O
Also	O
,	O
missing	O
inference	O
contributes	O
to	O
almost	O
20	O
%	O
of	O
the	O
observed	O
errors	O
(	O
not	O
including	O
multiple	O
sentences	O
or	O
paraphrases	O
)	O
.	O
Paraphrasing	O
is	O
the	O
most	O
frequent	O
error	O
category	O
observed	O
for	O
DrQA	O
,	O
which	O
makes	O
sense	O
if	O
we	O
con	O
-	O
sider	O
the	O
features	O
used	O
to	O
represent	O
each	O
passage	O
,	O
such	O
as	O
exact	B-MetricName
match	I-MetricName
with	O
a	O
question	O
word	O
,	O
which	O
depend	O
on	O
lexical	O
overlap	O
between	O
the	O
question	O
and	O
passage	O
.	O
We	O
observe	O
that	O
DocQA	O
makes	O
many	O
boundary	O
errors	O
too	O
,	O
again	O
making	O
more	O
mistakes	O
by	O
pre	O
-	O
dicting	O
shorter	O
answers	O
than	O
expected	O
in	O
most	O
of	O
the	O
observed	O
cases	O
.	O
A	O
better	O
root	O
cause	O
analysis	O
can	O
be	O
performed	O
by	O
visualizing	O
outputs	O
from	O
different	O
layers	O
and	O
evaluating	O
these	O
,	O
and	O
we	O
leave	O
this	O
in	O
-	O
depth	O
investigation	O
to	O
future	O
work	O
.	O
Also	O
,	O
the	O
high	O
number	O
of	O
Soft	O
Correct	O
outputs	O
across	O
all	O
models	O
points	O
to	O
some	O
deficiencies	O
in	O
the	O
SQuAD	B-DatasetName
annotations	O
,	O
which	O
might	O
limit	O
the	O
reliability	O
of	O
the	O
performance	O
evaluation	O
metrics	O
.	O
Although	O
these	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
deep	O
learning	O
models	O
for	O
machine	O
reading	O
are	O
supposed	O
to	O
have	O
inference	O
capabilities	O
,	O
our	O
error	O
analysis	O
above	O
points	O
to	O
their	O
limitations	O
.	O
These	O
insights	O
can	O
be	O
useful	O
for	O
developing	O
benchmarks	O
and	O
datasets	O
which	O
enable	O
realistic	O
evaluation	O
of	O
systems	O
which	O
aim	O
to	O
'	O
solve	O
'	O
the	O
RC	O
task	O
.	O
In	O
Wadhwa	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
take	O
a	O
first	O
step	O
in	O
this	O
direction	O
by	O
proposing	O
a	O
method	O
focused	O
on	O
questions	O
involving	O
referential	O
inference	O
,	O
a	O
setting	O
to	O
which	O
these	O
models	O
fail	O
to	O
generalize	O
well	O
.	O

Data	O
and	O
evaluation	O
Our	O
approach	O
requires	O
a	O
large	O
amount	O
of	O
monolingual	O
data	O
that	O
is	O
used	O
for	O
generating	O
synthetic	O
training	O
pairs	O
.	O
We	O
use	O
the	O
publicly	O
available	O
News	O
crawl	O
data	O
5	O
released	O
for	O
the	O
WMT	O
shared	O
tasks	O
(	O
Bojar	O
et	O
al	O
,	O
2018	O
)	O
.	O
For	O
English	O
and	O
German	O
,	O
we	O
limit	O
the	O
size	O
of	O
the	O
data	O
to	O
100	O
million	O
sentences	O
;	O
for	O
Russian	O
,	O
we	O
use	O
all	O
the	O
available	O
80.5	O
million	O
sentences	O
.	O
As	O
primary	O
development	O
and	O
test	O
data	O
,	O
we	O
use	O
the	O
following	O
learner	O
corpora	O
(	O
Table	O
2	O
)	O
:	O
English	O
:	O
the	O
new	O
W&I+LOCNESS	O
corpus	O
Granger	O
,	O
1998	O
)	O
released	O
for	O
the	O
BEA	O
2019	O
shared	O
task	O
and	O
representing	O
a	O
diverse	O
cross	O
-	O
section	O
of	O
English	O
language	O
;	O
German	O
:	O
the	O
Falko	O
-	O
MERLIN	O
GEC	O
corpus	O
(	O
Boyd	O
,	O
2018	O
)	O
Russian	O
:	O
the	O
recently	O
introduced	O
RULEC	O
-	O
GEC	O
dataset	O
(	O
Alsufieva	O
et	O
al	O
,	O
2012	O
;	O
Rozovskaya	O
and	O
Roth	O
,	O
2019	O
)	O
containing	O
Russian	O
texts	O
from	O
foreign	O
and	O
heritage	O
speakers	O
.	O
Unless	O
explicitly	O
stated	O
,	O
we	O
do	O
not	O
use	O
the	O
training	O
parts	O
of	O
those	O
datasets	O
.	O
For	O
each	O
language	O
we	O
follow	O
the	O
originally	O
proposed	O
preprocessing	O
and	O
evaluation	O
settings	O
.	O
English	O
and	O
German	O
data	O
are	O
tokenized	O
with	O
Spacy	O
6	O
,	O
while	O
Russian	O
is	O
preprocessed	O
with	O
Mystem	O
(	O
Segalovich	O
,	O
2003	O
)	O
.	O
We	O
additionally	O
normalise	O
punctuation	O
in	O
monolingual	O
data	O
using	O
Moses	O
scripts	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
.	O
During	O
training	O
,	O
we	O
limit	O
the	O
vocabulary	O
size	O
to	O
32	O
,	O
000	O
subwords	O
computed	O
with	O
SentencePiece	B-MethodName
using	O
the	O
unigram	O
method	O
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
.	O
English	O
models	O
are	O
evaluated	O
with	O
ERRANT	O
(	O
Bryant	O
et	O
al	O
,	O
2017	O
)	O
using	O
F	O
0.5	O
;	O
for	O
German	O
and	O
Russian	O
,	O
the	O
M2Scorer	O
with	O
the	O
MaxMatch	O
metric	O
(	O
Dahlmeier	O
and	O
Ng	O
,	O
2012	O
)	O
is	O
used	O
.	O
Synthetic	O
data	O
Confusion	O
sets	O
are	O
created	O
for	O
each	O
language	O
for	O
V	O
=	O
96	O
,	O
000	O
most	O
frequent	O
lexical	O
word	O
forms	O
from	O
monolingual	O
data	O
.	O
We	O
use	O
the	O
Levenshtein	O
distance	O
to	O
generate	O
edit	O
-	O
distance	O
based	O
confusion	O
sets	O
.	O
The	O
maximum	O
considered	O
distance	O
is	O
2	O
.	O
Word	B-TaskName
embeddings	I-TaskName
are	O
computed	O
with	O
word2vec	O
7	O
from	O
monolingual	O
data	O
.	O
To	O
generate	O
spell	O
-	O
broken	O
confusion	O
sets	O
we	O
use	O
Enchant	O
8	O
with	O
Aspell	O
dictionaries	O
.	O
9	O
The	O
size	O
of	O
confusion	O
sets	O
is	O
limited	O
to	O
top	O
20	O
words	O
.	O
Synthetic	O
errors	O
are	O
introduced	O
into	O
monolingual	O
texts	O
to	O
mimic	O
word	B-MetricName
error	I-MetricName
rate	I-MetricName
(	I-MetricName
WER	I-MetricName
)	I-MetricName
of	O
about	O
15	O
%	O
,	O
i.e.	O
p	O
WER	O
=	O
0.15	O
,	O
which	O
resembles	O
error	O
frequency	O
in	O
common	O
ESL	O
error	O
corpora	O
.	O
When	O
confusing	O
a	O
word	O
,	O
the	O
probability	O
p	O
sub	O
is	O
set	O
to	O
0.7	O
,	O
other	O
probabilities	O
are	O
set	O
to	O
0.1	O
.	O
Training	O
settings	O
We	O
adapt	O
the	O
recent	O
state	O
-	O
ofthe	O
-	O
art	O
GEC	O
system	O
by	O
Junczys	O
-	O
Dowmunt	O
et	O
al	O
(	O
2018b	O
)	O
,	O
an	O
ensemble	O
of	O
sequence	O
-	O
to	O
-	O
sequence	O
Transformer	B-MethodName
models	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
and	O
a	O
neural	O
language	O
model	O
.	O
10	O
We	O
use	O
the	O
training	O
setting	O
proposed	O
by	O
the	O
authors	O
11	O
,	O
but	O
introduce	O
stronger	O
regularization	O
:	O
we	O
increase	O
dropout	O
probabilities	O
of	O
source	O
words	O
to	O
0.3	O
,	O
add	O
dropout	O
on	O
transformer	O
self	O
-	O
attention	O
and	O
filter	O
layers	O
of	O
0.1	O
,	O
and	O
use	O
larger	O
mini	O
-	O
batches	O
with	O
2	O
,	O
500	O
sentences	O
.	O
We	O
do	O
not	O
pre	O
-	O
train	O
the	O
decoder	O
parameters	O
with	O
a	O
language	O
model	O
and	O
train	O
directly	O
on	O
the	O
synthetic	O
data	O
.	O
We	O
increase	O
the	O
size	O
of	O
language	O
model	O
used	O
for	O
ensembling	O
to	O
match	O
the	O
Transformer	B-MethodName
-	O
big	O
configuration	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
with	O
16	O
-	O
head	O
self	O
-	O
attention	O
,	O
embeddings	O
size	O
of	O
1024	O
and	O
feed	O
-	O
forward	O
filter	O
size	O
size	O
of	O
4096	O
.	O
In	O
experiments	O
with	O
fine	O
-	O
tuning	O
,	O
the	O
training	O
hyperparameters	O
remain	O
unchanged	O
.	O
All	O
models	O
are	O
trained	O
with	O
Marian	O
(	O
Junczys	O
-	O
Dowmunt	O
et	O
al	O
,	O
2018a	O
)	O
.	O
The	O
training	O
is	O
continued	O
for	O
at	O
most	O
5	O
epochs	O
or	O
until	O
early	O
-	O
stopping	O
is	O
triggered	O
after	O
5	O
stalled	O
validation	O
steps	O
.	O
We	O
found	O
that	O
using	O
10	O
,	O
000	O
synthetic	O
sentences	O
as	O
validation	O
sets	O
,	O
i.e.	O
a	O
fully	O
unsupervised	O
approach	O
,	O
is	O
as	O
effective	O
as	O
using	O
the	O
development	O
parts	O
of	O
error	O
corpora	O
and	O
does	O
not	O
decrease	O
the	O
final	O
performance	O
.	O

We	O
first	O
compare	O
the	O
GEC	O
systems	O
with	O
simple	O
baselines	O
using	O
a	O
greedy	O
and	O
context	O
spell	O
-	O
checking	O
(	O
Table	O
4	O
)	O
;	O
the	O
latter	O
selects	O
the	O
best	O
correction	O
suggestion	O
based	O
on	O
the	O
sentence	O
perplexity	B-MetricName
from	O
a	O
Transformer	B-MethodName
language	O
model	O
.	O
All	O
systems	O
outperform	O
the	O
spell	O
-	O
checker	O
baselines	O
.	O
On	O
German	O
and	O
Russian	O
test	O
sets	O
,	O
single	O
MAGEC	O
models	O
without	O
ensembling	O
with	O
a	O
language	O
model	O
already	O
achieve	O
better	O
performance	O
than	O
reported	O
by	O
Boyd	O
(	O
2018	O
)	O
Roth	O
(	O
2019	O
)	O
for	O
their	O
systems	O
that	O
use	O
authentic	O
error	O
-	O
annotated	O
data	O
for	O
training	O
(	O
Table	O
4b	O
and	O
4c	O
)	O
.	O
Our	O
best	O
unsupervised	O
ensemble	O
systems	O
that	O
combine	O
three	O
Transformer	B-MethodName
models	O
and	O
a	O
LM	O
12	O
outperform	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
for	O
these	O
languages	O
by	O
+7.0	O
and	O
+11.4	O
F	O
0.5	O
.	O
Our	O
English	O
models	O
do	O
not	O
compete	O
with	O
the	O
top	O
systems	O
(	O
Grundkiewicz	O
et	O
al	O
,	O
2019	O
)	O
from	O
the	O
BEA	O
shared	O
task	O
trained	O
on	O
publicly	O
available	O
errorannotated	O
corpora	O
(	O
Table	O
4a	O
)	O
.	O
It	O
is	O
difficult	O
to	O
compare	O
with	O
the	O
top	O
low	O
-	O
resource	O
system	O
from	O
the	O
shared	O
task	O
,	O
because	O
it	O
uses	O
additional	O
parallel	O
data	O
from	O
Wikipedia	O
(	O
Grundkiewicz	O
and	O
Junczys	O
-	O
Dowmunt	O
,	O
2014	O
)	O
,	O
larger	O
ensemble	O
,	O
and	O
n	O
-	O
best	O
list	O
re	O
-	O
ranking	O
with	O
right	O
-	O
to	O
-	O
left	O
models	O
,	O
which	O
can	O
be	O
also	O
implemented	O
in	O
this	O
work	O
.	O
MAGEC	O
systems	O
are	O
generally	O
on	O
par	O
with	O
the	O
results	O
achieved	O
by	O
a	O
recent	O
unsupervised	O
contribution	O
based	O
on	O
finite	O
state	O
transducers	O
by	O
Stahlberg	O
et	O
al	O
(	O
2019	O
)	O
on	O
the	O
CoNLL	O
-	O
2014	O
(	O
Dahlmeier	O
et	O
al	O
,	O
2013	O
)	O
and	O
JFLEG	B-DatasetName
test	O
sets	O
(	O
Napoles	O
et	O
al	O
,	O
2017	O
All	O
unsupervised	O
systems	O
benefit	O
from	O
domainadaptation	O
via	O
fine	O
-	O
tuning	O
on	O
authentic	O
labelled	O
data	O
(	O
Miceli	O
Barone	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
more	O
authentic	O
high	O
-	O
quality	O
and	O
in	O
-	O
domain	O
training	O
data	O
is	O
used	O
,	O
the	O
greater	O
the	O
improvement	O
,	O
but	O
even	O
as	O
few	O
as~2	O
,	O
000	O
sentences	O
are	O
helpful	O
(	O
Fig	O
.	O
1	O
)	O
.	O
We	O
found	O
that	O
fine	O
-	O
tuning	O
on	O
a	O
2:1	O
mixture	O
of	O
synthetic	O
and	O
oversampled	O
authentic	O
data	O
prevents	O
the	O
model	O
from	O
over	O
-	O
fitting	O
.	O
This	O
is	O
particularly	O
visible	O
for	O
English	O
which	O
has	O
the	O
largest	O
fine	O
-	O
tuning	O
set	O
(	O
34	O
K	O
sentences	O
)	O
,	O
and	O
the	O
difference	O
of	O
5.2	O
F	O
0.5	O
between	O
finetuning	O
with	O
and	O
without	O
synthetic	O
data	O
is	O
largest	O
.	O

Feature	O
attribution	O
methods	O
,	O
proposed	O
recently	O
,	O
help	O
users	O
interpret	O
the	O
predictions	O
of	O
complex	O
models	O
.	O
Our	O
approach	O
integrates	O
feature	O
attributions	O
into	O
the	O
objective	O
function	O
to	O
allow	O
machine	O
learning	O
practitioners	O
to	O
incorporate	O
priors	O
in	O
model	O
building	O
.	O
To	O
demonstrate	O
the	O
effectiveness	O
our	O
technique	O
,	O
we	O
apply	O
it	O
to	O
two	O
tasks	O
:	O
(	O
1	O
)	O
mitigating	O
unintended	O
bias	O
in	O
text	O
classifiers	O
by	O
neutralizing	O
identity	O
terms	O
;	O
(	O
2	O
)	O
improving	O
classifier	O
performance	O
in	O
a	O
scarce	O
data	O
setting	O
by	O
forcing	O
the	O
model	O
to	O
focus	O
on	O
toxic	O
terms	O
.	O
Our	O
approach	O
adds	O
an	O
L	O
2	O
distance	O
loss	B-MetricName
between	O
feature	O
attributions	O
and	O
task	O
-	O
specific	O
prior	O
values	O
to	O
the	O
objective	O
.	O
Our	O
experiments	O
show	O
that	O
i	O
)	O
a	O
classifier	O
trained	O
with	O
our	O
technique	O
reduces	O
undesired	O
model	O
biases	O
without	O
a	O
tradeoff	O
on	O
the	O
original	O
task	O
;	O
ii	O
)	O
incorporating	O
priors	O
helps	O
model	O
performance	O
in	O
scarce	O
data	O
settings	O
.	O

Baseline	O
I	O
am	O
gay	O
0.915	O
I	O
am	O
straight	O
0.085	O
Our	O
Method	O
I	O
am	O
gay	O
0.141	O
I	O
am	O
straight	O
0.144	O
On	O
the	O
other	O
hand	O
,	O
the	O
amount	O
of	O
research	O
focusing	O
on	O
explainable	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
models	O
(	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Murdoch	O
et	O
al	O
,	O
2018	O
;	O
Lei	O
et	O
al	O
,	O
2016	O
)	O
is	O
modest	O
as	O
opposed	O
to	O
image	O
explanation	O
techniques	O
.	O
Inherent	O
problems	O
in	O
data	O
emerge	O
in	O
a	O
trained	O
model	O
in	O
several	O
ways	O
.	O
Model	O
explanations	O
can	O
show	O
that	O
the	O
model	O
is	O
not	O
inline	O
with	O
human	O
judgment	O
or	O
domain	O
expertise	O
.	O
A	O
canonical	O
example	O
is	O
model	O
unfairness	O
,	O
which	O
stems	O
from	O
biases	O
in	O
the	O
training	O
data	O
.	O
Fairness	B-TaskName
in	O
ML	O
models	O
rightfully	O
came	O
under	O
heavy	O
scrutiny	O
in	O
recent	O
years	O
(	O
Zhang	O
et	O
al	O
,	O
2018a	O
;	O
Dixon	O
et	O
al	O
,	O
2018	O
;	O
Angwin	O
et	O
al	O
,	O
2016	O
)	O
.	O
Some	O
examples	O
include	O
sentiment	B-TaskName
analysis	I-TaskName
models	O
weighing	O
negatively	O
for	O
inputs	O
containing	O
identity	O
terms	O
such	O
as	O
"	O
jew	O
"	O
and	O
"	O
black	O
"	O
,	O
and	O
hate	B-DatasetName
speech	I-DatasetName
classifiers	O
leaning	O
to	O
predict	O
any	O
sentence	O
containing	O
"	O
islam	O
"	O
as	O
toxic	O
(	O
Waseem	O
and	O
Hovy	O
,	O
2016	O
)	O
.	O
If	O
employed	O
,	O
explanation	O
techniques	O
help	O
divulge	O
these	O
issues	O
,	O
but	O
fail	O
to	O
offer	O
a	O
remedy	O
.	O
For	O
instance	O
,	O
the	O
sentence	O
"	O
I	O
am	O
gay	O
"	O
receives	O
a	O
high	O
score	O
on	O
a	O
toxicity	O
model	O
as	O
seen	O
in	O
Table	O
1	O
.	O
The	O
Integrated	O
Gradients	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
)	O
explanation	O
method	O
attributes	O
the	O
majority	O
of	O
this	O
decision	O
to	O
the	O
word	O
"	O
gay	O
.	O
"	O
However	O
,	O
none	O
of	O
the	O
explanations	O
methods	O
suggest	O
next	O
steps	O
to	O
fix	O
the	O
issue	O
.	O
Instead	O
,	O
researchers	O
try	O
to	O
reduce	O
biases	O
indirectly	O
by	O
mostly	O
adding	O
more	O
data	O
(	O
Dixon	O
et	O
al	O
,	O
2018	O
;	O
,	O
using	O
unbiased	O
word	O
vectors	O
(	O
Park	O
et	O
al	O
,	O
2018	O
)	O
,	O
or	O
directly	O
optimizing	O
for	O
a	O
fairness	O
proxy	O
with	O
adversarial	O
training	O
(	O
Madras	O
et	O
al	O
,	O
2018	O
;	O
Zhang	O
et	O
al	O
,	O
2018a	O
)	O
.	O
These	O
methods	O
either	O
offer	O
to	O
collect	O
more	O
data	O
,	O
which	O
is	O
costly	O
in	O
many	O
cases	O
,	O
or	O
make	O
a	O
tradeoff	O
between	O
original	O
task	O
performance	O
and	O
fairness	O
.	O
In	O
this	O
paper	O
,	O
we	O
attempt	O
to	O
enable	O
injecting	O
priors	O
through	O
model	O
explanations	O
to	O
rectify	O
issues	O
in	O
trained	O
models	O
.	O
We	O
demonstrate	O
our	O
approach	O
on	O
two	O
problems	O
in	O
text	B-TaskName
classification	I-TaskName
settings	O
:	O
(	O
1	O
)	O
model	O
biases	O
towards	O
protected	O
identity	O
groups	O
;	O
(	O
2	O
)	O
low	O
classification	O
performance	O
due	O
to	O
lack	O
of	O
data	O
.	O
The	O
core	O
idea	O
is	O
to	O
add	O
L	O
2	O
distance	O
between	O
Path	O
Integrated	O
Gradients	O
attributions	O
for	O
pre	O
-	O
selected	O
tokens	O
and	O
a	O
target	O
attribution	O
value	O
in	O
the	O
objective	O
function	O
as	O
a	O
loss	B-MetricName
term	O
.	O
For	O
model	O
fairness	O
,	O
we	O
impose	O
the	O
loss	B-MetricName
on	O
keywords	O
identifying	O
protected	O
groups	O
with	O
target	O
attribution	O
of	O
0	B-DatasetName
,	O
so	O
the	O
trained	O
model	O
is	O
penalized	O
for	O
attributing	O
model	O
decisions	O
to	O
those	O
keywords	O
.	O
Our	O
main	O
intuition	O
is	O
that	O
undesirable	O
correlations	O
between	O
toxicity	O
labels	O
and	O
instances	O
of	O
identity	O
terms	O
cause	O
the	O
model	O
to	O
learn	O
unfair	O
biases	O
which	O
can	O
be	O
corrected	O
by	O
incorporating	O
priors	O
on	O
these	O
identity	O
terms	O
.	O
Moreover	O
,	O
our	O
approach	O
allows	O
practitioners	O
to	O
impose	O
priors	O
in	O
the	O
other	O
direction	O
to	O
tackle	O
the	O
problem	O
of	O
training	O
a	O
classifier	O
when	O
there	O
is	O
only	O
a	O
small	O
amount	O
of	O
data	O
.	O
As	O
shown	O
in	O
our	O
experiments	O
,	O
by	O
setting	O
a	O
positive	O
target	O
attribution	O
for	O
known	O
toxic	O
words	O
1	O
,	O
one	O
can	O
improve	O
the	O
performance	O
of	O
a	O
toxicity	O
classifier	O
in	O
a	O
scarce	O
data	O
regime	O
.	O
We	O
validate	O
our	O
approach	O
on	O
the	O
Wikipedia	O
toxic	O
comments	O
dataset	O
(	O
Wulczyn	O
et	O
al	O
,	O
2017	O
)	O
.	O
Our	O
fairness	O
experiments	O
show	O
that	O
the	O
classifiers	O
trained	O
with	O
our	O
method	O
achieve	O
the	O
same	O
performance	O
,	O
if	O
not	O
better	O
,	O
on	O
the	O
original	O
task	O
,	O
while	O
improving	O
AUC	B-MetricName
and	O
fairness	O
metrics	O
on	O
a	O
synthetic	O
,	O
unbiased	O
dataset	O
.	O
Models	O
trained	O
with	O
our	O
technique	O
also	O
show	O
lower	O
attributions	O
to	O
identity	O
terms	O
on	O
average	O
.	O
Our	O
technique	O
produces	O
much	O
better	O
word	O
vectors	O
as	O
a	O
by	O
-	O
product	O
when	O
compared	O
to	O
the	O
baseline	O
.	O
Lastly	O
,	O
by	O
setting	O
an	O
attribution	O
target	O
of	O
1	O
on	O
toxic	O
words	O
,	O
a	O
classifier	O
trained	O
with	O
our	O
objective	O
function	O
achieves	O
better	O
performance	O
when	O
only	O
a	O
subset	O
of	O
the	O
data	O
is	O
present	O
.	O
1	O
Full	O
list	O
of	O
identity	O
terms	O
and	O
toxic	O
terms	O
used	O
as	O
priors	O
can	O
be	O
found	O
in	O
supplemental	O
material	O
.	O
Please	O
note	O
the	O
toxic	O
terms	O
are	O
not	O
censored	O
.	O

Problems	O
in	O
data	O
manifest	O
themselves	O
in	O
a	O
trained	O
model	O
's	O
performance	O
on	O
classification	O
or	O
fairness	O
metrics	O
.	O
Traditionally	O
,	O
model	O
deficiencies	O
were	O
addressed	O
by	O
providing	O
priors	O
through	O
extensive	O
feature	B-TaskName
engineering	I-TaskName
and	O
collecting	O
more	O
data	O
.	O
Recently	O
,	O
attributions	O
help	O
uncover	O
deficiencies	O
causing	O
models	O
to	O
perform	O
poorly	O
,	O
but	O
do	O
not	O
offer	O
actionability	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
to	O
add	O
an	O
extra	O
term	O
to	O
the	O
objective	O
function	O
to	O
penalize	O
the	O
L	O
2	O
distance	O
between	O
model	O
attributions	O
on	O
certain	O
features	O
and	O
target	O
attribution	O
values	O
.	O
This	O
modification	O
allows	O
model	O
practitioners	O
to	O
inject	O
priors	O
.	O
For	O
example	O
,	O
consider	O
a	O
model	O
that	O
tends	O
to	O
predict	O
every	O
sentence	O
containing	O
"	O
gay	O
"	O
as	O
toxic	O
in	O
a	O
comment	O
moderation	O
system	O
.	O
Penalizing	O
non	O
-	O
zero	O
attributions	O
on	O
the	O
tokens	O
identifying	O
protected	O
groups	O
would	O
force	O
the	O
model	O
to	O
focus	O
more	O
on	O
the	O
context	O
words	O
rather	O
than	O
mere	O
existence	O
of	O
certain	O
tokens	O
.	O
We	O
give	O
the	O
formal	O
definition	O
of	O
the	O
new	O
objective	O
function	O
that	O
incorporates	O
priors	O
as	O
the	O
follows	O
:	O
Definition	O
3.1	O
.	O
Given	O
a	O
vector	O
t	O
of	O
size	O
n	O
,	O
where	O
n	O
is	O
the	O
length	O
of	O
the	O
input	O
sequence	O
and	O
t	O
i	O
is	O
the	O
attribution	O
target	O
value	O
for	O
the	O
ith	O
token	O
in	O
the	O
input	O
sequence	O
.	O
The	O
prior	O
loss	B-MetricName
for	O
a	O
scalar	O
output	O
is	O
defined	O
as	O
:	O
L	O
prior	O
(	O
a	O
,	O
t	O
)	O
=	O
n	O
i	O
(	O
a	O
i	O
−	O
t	O
i	O
)	O
2	O
(	O
2	O
)	O
where	O
a	O
i	O
refers	O
to	O
attribution	O
of	O
the	O
ith	O
token	O
as	O
in	O
Definition	O
2.1	O
.	O
For	O
a	O
multi	O
-	O
class	O
problem	O
,	O
we	O
train	O
our	O
model	O
with	O
the	O
following	O
joint	O
objective	O
,	O
L	O
joint	O
=	O
L	O
(	O
y	O
,	O
p	O
)	O
+	O
λ	O
C	O
c	O
L	O
prior	O
(	O
a	O
c	O
,	O
t	O
c	O
)	O
(	O
3	O
)	O
where	O
a	O
c	O
and	O
t	O
c	O
are	O
the	O
attribution	O
and	O
attribution	O
target	O
for	O
class	O
c	O
,	O
λ	O
is	O
the	O
hyperparameter	O
that	O
controls	O
the	O
stength	O
of	O
the	O
prior	O
loss	B-MetricName
and	O
L	O
is	O
the	O
crossentropy	O
loss	B-MetricName
defined	O
as	O
follows	O
:	O
L	O
(	O
y	O
,	O
p	O
)	O
=	O
C	O
c	O
−y	O
c	O
log	O
(	O
p	O
c	O
)	O
(	O
4	O
)	O
where	O
y	O
is	O
an	O
indicator	O
vector	O
of	O
the	O
ground	O
truth	O
label	O
and	O
p	O
c	O
is	O
the	O
posterior	O
probability	O
of	O
class	O
c.	O
The	O
joint	O
objective	O
function	O
is	O
differentiable	O
w.r.t	O
.	O
model	O
parameters	O
when	O
attribution	O
is	O
calculated	O
through	O
Equation	O
1	O
and	O
can	O
be	O
trained	O
with	O
most	O
off	O
-	O
the	O
-	O
shelf	O
optimizers	O
.	O
The	O
proposed	O
objective	O
is	O
not	O
dataset	O
-	O
dependent	O
and	O
is	O
applicable	O
to	O
different	O
problem	O
settings	O
such	O
as	O
sentiment	O
classification	O
,	O
abuse	B-TaskName
detection	I-TaskName
,	O
etc	O
.	O
It	O
only	O
requires	O
users	O
to	O
specify	O
the	O
target	O
attribution	O
value	O
for	O
tokens	O
of	O
interest	O
in	O
the	O
corpus	O
.	O
We	O
illustrate	O
the	O
effectiveness	O
of	O
our	O
method	O
by	O
applying	O
it	O
to	O
a	O
toxic	B-TaskName
comment	I-TaskName
classification	I-TaskName
problem	O
.	O
In	O
the	O
next	O
section	O
,	O
we	O
first	O
show	O
how	O
we	O
set	O
the	O
target	O
attribution	O
value	O
for	O
identity	O
terms	O
to	O
remove	O
unintended	O
biases	O
while	O
retaining	O
the	O
same	O
performance	O
on	O
the	O
original	O
task	O
.	O
Then	O
,	O
using	O
the	O
same	O
technique	O
,	O
we	O
show	O
how	O
to	O
set	O
target	O
attribution	O
for	O
toxic	O
words	O
to	O
improve	O
classifier	O
performance	O
in	O
a	O
scarce	O
data	O
setting	O
.	O

We	O
incorporate	O
human	O
prior	O
in	O
model	O
building	O
on	O
two	O
applications	O
.	O
First	O
,	O
we	O
tackle	O
the	O
problem	O
of	O
unintended	O
bias	O
in	O
toxic	B-TaskName
comment	I-TaskName
classification	I-TaskName
(	O
Dixon	O
et	O
al	O
,	O
2018	O
)	O
with	O
our	O
proposed	O
method	O
.	O
For	O
our	O
experiments	O
,	O
we	O
aim	O
to	O
mitigate	O
the	O
issue	O
of	O
neutral	O
sentences	O
with	O
identity	O
terms	O
being	O
classified	O
as	O
toxic	O
for	O
a	O
given	O
a	O
set	O
of	O
identity	O
terms	O
.	O
A	O
subset	O
of	O
the	O
identity	O
terms	O
are	O
listed	O
in	O
the	O
first	O
column	O
of	O
Table	O
2	O
.	O
Second	O
,	O
we	O
force	O
the	O
model	O
to	O
focus	O
on	O
a	O
list	O
of	O
human	O
-	O
selected	O
toxic	O
terms	O
under	O
scarce	O
data	O
scenario	O
to	O
increase	O
model	O
performance	O
.	O
In	O
the	O
following	O
section	O
,	O
we	O
introduce	O
the	O
dataset	O
we	O
train	O
and	O
evaluate	O
on	O
along	O
with	O
a	O
synthetic	O
dataset	O
to	O
further	O
validate	O
our	O
fairness	O
improvements	O
.	O
After	O
that	O
,	O
we	O
describe	O
our	O
experimental	O
setup	O
.	O
Then	O
,	O
we	O
compare	O
our	O
method	O
to	O
a	O
classifier	O
trained	O
without	O
the	O
prior	O
loss	B-MetricName
and	O
2	O
other	O
baselines	O
.	O
Lastly	O
,	O
we	O
show	O
the	O
results	O
demonstrating	O
usefulness	O
of	O
our	O
approach	O
with	O
data	O
scarcity	O
.	O

In	O
this	O
work	O
,	O
we	O
use	O
a	O
dataset	O
containing	O
comments	O
from	O
Wikipedia	O
Talk	O
Pages	O
(	O
Dixon	O
et	O
al	O
,	O
2018	O
)	O
.	O
Number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
are	O
95	O
,	O
692	O
/	O
32	O
,	O
128	O
/	O
31	O
,	O
866	O
in	O
the	O
train	O
/	O
dev	O
/	O
test	O
sets	O
respectively	O
.	O
The	O
ratio	O
of	O
positive	O
(	O
toxic	O
)	O
labels	O
in	O
the	O
training	O
set	O
is	O
9.7	O
%	O
.	O
The	O
dataset	O
was	O
annotated	O
by	O
human	O
raters	O
,	O
where	O
toxicity	O
was	O
defined	O
as	O
a	O
"	O
rude	O
,	O
disrespectful	O
,	O
or	O
unreasonable	O
comment	O
that	O
is	O
likely	O
to	O
make	O
you	O
leave	O
a	O
discussion	O
"	O
per	O
Dixon	O
et	O
al	O
(	O
2018	O
)	O
.	O
Please	O
refer	O
to	O
the	O
corresponding	O
paper	O
for	O
more	O
details	O
about	O
collection	O
methodology	O
,	O
biases	O
present	O
in	O
the	O
data	O
,	O
and	O
toxicity	O
distribution	O
per	O
comment	O
length	O
.	O
We	O
also	O
use	O
a	O
synthetically	O
generated	O
dataset	O
to	O
validate	O
our	O
approach	O
on	O
fairness	O
as	O
in	O
Park	O
et	O
al	O

For	O
the	O
text	O
classifier	O
,	O
we	O
built	O
a	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
classifier	O
as	O
in	O
Kim	O
(	O
2014	O
)	O
.	O
The	O
network	O
contains	O
a	O
convolution	B-MethodName
layer	O
with	O
128	O
2	O
-	O
,	O
3	O
-	O
,	O
4	O
-	O
gram	O
filters	O
for	O
a	O
sequence	O
length	O
of	O
100	O
followed	O
by	O
a	O
max	O
-	O
pooling	O
layer	O
and	O
softmax	B-MethodName
function	O
.	O
Embeddings	O
were	O
randomly	O
initialized	O
and	O
their	O
size	O
was	O
set	O
to	O
128	O
.	O
Shorter	O
sequences	O
are	O
padded	O
with	O
<	O
pad	O
>	O
token	O
and	O
longer	O
sequences	O
are	O
truncated	O
.	O
Tokens	O
occurring	O
5	O
times	O
or	O
more	O
are	O
retained	O
in	O
the	O
vocabulary	O
.	O
We	O
set	O
dropout	O
as	O
0.2	O
and	O
used	O
Adam	B-MethodName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
as	O
our	O
optimizer	B-HyperparameterName
with	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
set	O
to	O
0.001	O
.	O
We	O
did	O
n't	O
perform	O
extensive	O
network	O
architecture	O
search	O
to	O
improve	O
the	O
performance	O
as	O
it	O
is	O
a	O
reasonably	O
strong	O
classifier	O
with	O
the	O
initial	O
performance	O
of	O
95.5	O
%	O
accuracy	B-MetricName
.	O
The	O
number	O
of	O
interpolating	O
steps	O
for	O
IG	O
is	O
set	O
to	O
50	O
(	O
as	O
in	O
the	O
original	O
paper	O
)	O
for	O
calculating	O
Riemann	O
approximation	O
of	O
the	O
integral	O
.	O
Since	O
the	O
output	O
of	O
the	O
binary	O
classification	O
can	O
be	O
reduced	O
to	O
a	O
single	O
scalar	O
output	O
by	O
taking	O
the	O
posterior	O
of	O
the	O
positive	O
(	O
toxic	O
)	O
class	O
,	O
the	O
prior	O
is	O
only	O
added	O
to	O
the	O
positive	O
class	O
in	O
equation	O
3	O
.	O
We	O
set	O
t	O
i	O
=	O
k	O
,	O
if	O
x	O
i	O
I	O
a	O
i	O
,	O
otherwise	O
,	O
(	O
5	O
)	O
where	O
I	O
is	O
the	O
set	O
of	O
selected	O
terms	O
and	O
x	O
i	O
being	O
the	O
i	O
th	O
token	O
in	O
the	O
sequence	O
.	O
For	O
fairness	O
experiments	O
,	O
we	O
set	O
k	O
to	O
be	O
0	B-DatasetName
and	O
I	O
to	O
the	O
set	O
of	O
identity	O
terms	O
with	O
the	O
hope	O
that	O
these	O
terms	O
should	O
be	O
as	O
neutral	O
as	O
possible	O
when	O
making	O
predictions	O
.	O
Hyperparamter	O
λ	O
is	O
searched	O
in	O
the	O
range	O
of	O
(	O
1	O
,	O
10	O
8	O
)	O
and	O
increased	O
from	O
1	O
by	O
a	O
scale	O
of	O
10	O
on	O
the	O
dev	O
set	O
and	O
we	O
pick	O
the	O
one	O
with	O
best	O
F	O
-	O
1	O
score	O
.	O
λ	O
is	O
set	O
to	O
10	O
6	O
for	O
the	O
final	O
model	O
.	O
For	O
data	O
scarcity	O
experiments	O
,	O
we	O
set	O
k	O
to	O
1	O
and	O
I	O
to	O
the	O
set	O
of	O
toxic	O
terms	O
to	O
force	O
the	O
model	O
to	O
make	O
high	O
attributions	O
to	O
these	O
terms	O
.	O
Hyperparameter	O
λ	O
is	O
set	O
to	O
10	O
5	O
across	O
all	O
data	O
size	O
experiments	O
by	O
tuning	O
on	O
the	O
dev	O
set	O
with	O
model	O
given	O
1	O
%	O
of	O
training	O
data	O
.	O
Each	O
experiment	O
was	O
repeated	O
for	O
5	O
runs	O
with	O
10	O
epochs	O
and	O
the	O
best	O
model	O
is	O
selected	O
according	O
to	O
the	O
dev	O
set	O
.	O
Training	O
takes	O
1	O
minute	O
for	O
a	O
model	O
with	O
cross	O
-	O
entropy	O
loss	B-MetricName
and	O
30	O
minutes	O
for	O
a	O
model	O
with	O
joint	O
loss	B-MetricName
on	O
an	O
NVidia	O
V100	O
GPU	O
.	O
However	O
,	O
reducing	O
the	O
step	B-HyperparameterName
size	I-HyperparameterName
in	O
IG	O
for	O
calculating	O
Riemann	O
approximation	O
of	O
the	O
integral	O
to	O
10	O
steps	O
reduces	O
the	O
training	O
time	O
to	O
6	O
minutes	O
.	O
Lastly	O
,	O
training	O
with	O
joint	O
loss	B-MetricName
reaches	O
its	O
best	O
performance	O
in	O
later	O
epochs	O
than	O
training	O
with	O
crossentropy	O
loss	B-MetricName
.	O

We	O
compare	O
our	O
work	O
to	O
3	O
models	O
with	O
the	O
same	O
CNN	O
architecture	O
,	O
but	O
different	O
training	O
settings	O
:	O
Baseline	O
:	O
A	O
baseline	O
classifier	O
trained	O
with	O
cross	O
-	O
entropy	O
loss	B-MetricName
.	O
Importance	O
:	O
Classifier	O
trained	O
with	O
crossentropy	O
loss	B-MetricName
,	O
but	O
the	O
loss	B-MetricName
for	O
samples	O
containing	O
identity	O
words	O
are	O
weighted	O
in	O
the	O
range	O
(	O
1	O
,	O
10	O
8	O
)	O
,	O
where	O
the	O
actual	O
coefficient	O
is	O
determined	O
to	O
be	O
10	O
on	O
the	O
dev	O
set	O
based	O
on	O
F	O
-	O
1	O
score	O
.	O
TOK	O
Replace	O
:	O
Common	O
technique	O
for	O
making	O
models	O
blind	O
to	O
identity	O
terms	O
(	O
Garg	O
et	O
al	O
,	O
2018	O
)	O
.	O
All	O
identity	O
terms	O
are	O
replaced	O
with	O
a	O
special	O
<	O
i	O
d	O
>	O
token	O
.	O
We	O
also	O
explore	O
a	O
different	O
training	O
schedule	O
for	O
cases	O
where	O
a	O
model	O
has	O
been	O
trained	O
to	O
optimize	O
for	O
a	O
classification	O
loss	B-MetricName
:	O
Finetuned	O
:	O
An	O
already	O
-	O
trained	O
classifier	O
is	O
finetuned	O
with	O
joint	O
loss	B-MetricName
for	O
several	O
epochs	O
.	O
The	O
aim	O
of	O
this	O
experiment	O
is	O
to	O
show	O
that	O
our	O
method	O
is	O
also	O
applicable	O
for	O
tweaking	O
trained	O
models	O
,	O
which	O
could	O
be	O
useful	O
if	O
the	O
original	O
had	O
been	O
trained	O
for	O
a	O
long	O
time	O
.	O

We	O
first	O
verify	O
that	O
the	O
prior	O
loss	B-MetricName
term	O
does	O
not	O
adversely	O
affect	O
overall	O
classifier	O
performance	O
on	O
the	O
main	O
task	O
using	O
general	O
performance	O
metrics	O
such	O
as	O
accuracy	B-MetricName
and	O
F	O
-	O
1	O
.	O
Results	O
are	O
shown	O
in	O
Table	O
4	O
.	O
Unlike	O
previous	O
approaches	O
(	O
Park	O
et	O
al	O
,	O
2018	O
;	O
Dixon	O
et	O
al	O
,	O
2018	O
;	O
Madras	O
et	O
al	O
,	O
2018	O
)	O
,	O
our	O
method	O
does	O
not	O
degrade	O
classifier	O
performance	O
(	O
it	O
even	O
improves	O
)	O
in	O
terms	O
of	O
all	O
reported	O
metrics	O
.	O
We	O
also	O
look	O
at	O
samples	O
containing	O
identity	O
terms	O
.	O
Table	O
5	O
shows	O
classifier	O
performance	O
metrics	O
for	O
such	O
samples	O
.	O
The	O
importance	O
weighting	O
approach	O
slightly	O
outperforms	O
the	O
baseline	O
classifier	O
.	O
Replacing	O
identity	O
words	O
with	O
a	O
special	O
tokens	O
,	O
on	O
the	O
other	O
hand	O
,	O
hurts	O
the	O
performance	O
on	O
the	O
main	O
task	O
.	O
One	O
of	O
the	O
reasons	O
might	O
be	O
that	O
replacing	O
all	O
identity	O
terms	O
with	O
a	O
token	O
potentially	O
removes	O
other	O
useful	O
information	O
model	O
can	O
rely	O
on	O
.	O
If	O
we	O
were	O
to	O
make	O
an	O
analogy	O
between	O
the	O
token	O
replacement	O
method	O
and	O
hard	O
ablation	O
,	O
then	O
the	O
same	O
analogy	O
can	O
be	O
made	O
between	O
our	O
method	O
and	O
soft	O
ablation	O
.	O
Hence	O
,	O
the	O
information	O
pertaining	O
to	O
identity	O
terms	O
is	O
not	O
completely	O
lost	O
for	O
our	O
method	O
,	O
but	O
come	O
at	O
a	O
cost	O
.	O
Results	O
for	O
fine	O
-	O
tuning	O
experiments	O
show	O
the	O
performance	O
after	O
2	O
epochs	O
.	O
It	O
is	O
seen	O
that	O
the	O
model	O
converges	O
to	O
similar	O
performance	O
with	O
joint	O
training	O
after	O
only	O
2	O
epochs	O
,	O
albeit	O
being	O
slightly	O
poorer	O
.	O

Now	O
we	O
run	O
our	O
experiments	O
on	O
the	O
templatebased	O
synthetic	O
data	O
.	O
As	O
stated	O
,	O
this	O
dataset	O
is	O
used	O
to	O
measure	O
biases	O
in	O
the	O
model	O
since	O
it	O
is	O
unbiased	O
towards	O
identities	O
.	O
We	O
use	O
AUC	B-MetricName
along	O
with	O
False	O
Positive	O
Equality	O
Difference	O
(	O
FPED	O
)	O
and	O
False	O
Negative	O
Equality	O
Difference	O
(	O
FNED	O
)	O
,	O
which	O
measure	O
a	O
proxy	O
of	O
Equality	O
of	O
Odds	O
(	O
Hardt	O
et	O
al	O
,	O
2016	O
)	O
,	O
as	O
in	O
Dixon	O
et	O
al	O
(	O
2018	O
)	O
;	O
Park	O
et	O
al	O
(	O
2018	O
)	O
.	O
FPED	O
sums	O
absolute	O
differences	O
between	O
overall	O
false	O
positive	O
rate	O
and	O
false	O
positive	O
rates	O
for	O
each	O
identity	O
term	O
.	O
FNED	O
calculates	O
the	O
same	O
for	O
false	O
negatives	O
.	O
Results	O
on	O
this	O
dataset	O
are	O
shown	O
in	O
Table	O
7	O
.	O
Our	O
method	O
provides	O
substantial	O
improvement	O
on	O
AUC	B-MetricName
and	O
almost	O
completely	O
eliminates	O
false	O
positive	O
and	O
false	O
negative	O
inequality	O
across	O
identities	O
.	O
The	O
fine	O
-	O
tuned	O
model	O
also	O
outperforms	O
the	O
baseline	O
for	O
mitigating	O
the	O
bias	O
.	O
The	O
token	O
replacement	O
method	O
comes	O
out	O
as	O
a	O
good	O
baseline	O
for	O
mitigating	O
the	O
bias	O
since	O
it	O
treats	O
all	O
identities	O
the	O
same	O
.	O
The	O
importance	O
weighting	O
approach	O
fails	O
to	O
produce	O
an	O
unbiased	O
model	O
.	O

We	O
now	O
demonstrate	O
our	O
approach	O
on	O
encouraging	O
higher	O
attributions	O
on	O
toxic	O
words	O
to	O
increase	O
model	O
performance	O
in	O
scarce	O
data	O
regime	O
.	O
We	O
down	O
-	O
sample	O
the	O
dataset	O
with	O
different	O
ratios	O
to	O
simulate	O
a	O
data	O
scarcity	O
scenario	O
.	O
To	O
directly	O
validate	O
the	O
effectiveness	O
of	O
prior	O
loss	B-MetricName
on	O
attributions	O
,	O
we	O
first	O
show	O
that	O
the	O
attribution	O
of	O
the	O
toxic	O
words	O
have	O
higher	O
values	O
for	O
our	O
method	O
across	O
different	O
data	O
ratios	O
compared	O
to	O
the	O
baseline	O
in	O
Table	O
8	O
.	O
We	O
also	O
show	O
that	O
the	O
attribution	O
for	O
these	O
terms	O
increases	O
as	O
training	O
data	O
increases	O
for	O
the	O
baseline	O
method	O
.	O
We	O
then	O
show	O
model	O
performance	O
on	O
testing	O
data	O
for	O
different	O
data	O
size	O
ratios	O
for	O
the	O
baseline	O
and	O
our	O
method	O
in	O
Figure	O
1	O
.	O
Our	O
method	O
outperforms	O
the	O
baseline	O
by	O
a	O
big	O
margin	O
in	O
1	O
%	O
and	O
5	O
%	O
ratio	O
.	O
However	O
,	O
the	O
impact	O
of	O
our	O
approach	O
diminishes	O
after	O
adding	O
more	O
data	O
,	O
since	O
the	O
model	O
starts	O
to	O
learn	O
to	O
focus	O
on	O
toxic	O
words	O
itself	O
for	O
predicting	O
toxicity	O
without	O
the	O
need	O
for	O
prior	O
injection	O
.	O
We	O
can	O
also	O
see	O
that	O
both	O
the	O
baseline	O
and	O
our	O
method	O
start	O
to	O
catch	O
up	O
with	O
the	O
rule	O
based	O
approach	O
,	O
where	O
we	O
give	O
positive	O
prediction	O
if	O
the	O
toxic	O
word	O
is	O
in	O
the	O
sentence	O
,	O
and	O
eventually	O
outperform	O
it	O
.	O

For	O
explaining	O
ML	O
models	O
,	O
recent	O
research	O
attempts	O
offer	O
techniques	O
ranging	O
from	O
building	O
inherently	O
interpretable	O
models	O
to	O
building	O
a	O
proxy	O
model	O
for	O
explaining	O
a	O
more	O
complex	O
model	O
(	O
Ribeiro	O
et	O
al	O
,	O
2016	O
;	O
Frosst	O
and	O
Hinton	O
,	O
2017	O
)	O
to	O
explaining	O
inner	O
mechanics	O
of	O
mostly	O
uninterpretable	O
neural	O
networks	O
(	O
Sundararajan	O
et	O
al	O
,	O
2017	O
;	O
Bach	O
et	O
al	O
,	O
2015	O
)	O
.	O
One	O
family	O
of	O
interpretability	O
methods	O
uses	O
sensitivity	O
of	O
the	O
network	O
with	O
respect	O
to	O
data	O
points	O
(	O
Koh	O
and	O
Liang	O
,	O
2017	O
)	O
or	O
features	O
(	O
Ribeiro	O
et	O
al	O
,	O
2016	O
)	O
as	O
a	O
form	O
of	O
explanation	O
.	O
These	O
methods	O
rely	O
on	O
small	O
,	O
local	O
perturbations	O
and	O
check	O
how	O
a	O
network	O
's	O
response	O
changes	O
.	O
Explaining	O
text	O
models	O
has	O
another	O
layer	O
of	O
complexity	O
due	O
to	O
a	O
lock	O
of	O
proper	O
technique	O
to	O
generate	O
counterfactuals	O
in	O
the	O
form	O
of	O
small	O
perturbations	O
.	O
Hence	O
,	O
interpretability	O
methods	O
tailored	O
for	O
text	O
are	O
quite	O
sparse	O
(	O
Mudrakarta	O
et	O
al	O
,	O
2018	O
;	O
Jia	O
and	O
Liang	O
,	O
2017	O
;	O
Murdoch	O
et	O
al	O
,	O
2018	O
)	O
.	O
On	O
the	O
other	O
hand	O
,	O
there	O
are	O
many	O
papers	O
criticizing	O
the	O
aforementioned	O
methods	O
by	O
questioning	O
their	O
faithfulness	O
,	O
correctness	O
(	O
Adebayo	O
et	O
al	O
,	O
2018	O
;	O
Kindermans	O
et	O
al	O
,	O
2017	O
)	O
and	O
usefulness	O
.	O
Smilkov	O
et	O
al	O
(	O
2017	O
)	O
show	O
that	O
gradient	O
based	O
methods	O
are	O
susceptible	O
to	O
saturation	O
and	O
can	O
be	O
fooled	O
by	O
adversarial	O
techniques	O
.	O
Other	O
sets	O
of	O
papers	O
(	O
Miller	O
,	O
2019	O
;	O
Gilpin	O
et	O
al	O
,	O
2018	O
)	O
attack	O
model	O
explanation	O
papers	O
from	O
a	O
philosophical	O
perspective	O
.	O
However	O
,	O
the	O
lack	O
of	O
actionability	O
angle	O
is	O
often	O
overlooked	O
.	O
Lipton	O
(	O
2018	O
)	O
briefly	O
questions	O
the	O
practical	O
benefit	O
of	O
having	O
model	O
explanations	O
from	O
a	O
practitioners	O
perspective	O
.	O
There	O
are	O
several	O
works	O
taking	O
advantage	O
of	O
model	O
explanations	O
.	O
Namely	O
,	O
using	O
model	O
explanations	O
to	O
aid	O
doctors	O
in	O
diagnosing	O
retinopathy	O
patients	O
,	O
and	O
removing	O
minimal	O
features	O
,	O
called	O
pathologies	O
,	O
from	O
neural	O
networks	O
by	O
tuning	O
the	O
model	O
to	O
have	O
high	O
entropy	O
on	O
pathologies	O
(	O
Feng	O
et	O
al	O
,	O
2018	O
)	O
.	O
The	O
authors	O
of	O
Ross	O
et	O
al	O
(	O
2017	O
)	O
propose	O
a	O
similar	O
idea	O
to	O
our	O
approach	O
in	O
that	O
they	O
regularize	O
input	O
gradients	O
to	O
alter	O
the	O
decision	O
boundary	O
of	O
the	O
model	O
to	O
make	O
it	O
more	O
consistent	O
with	O
domain	O
knowledge	O
.	O
However	O
,	O
the	O
input	O
gradients	O
technique	O
has	O
been	O
shown	O
to	O
be	O
an	O
inaccurate	O
explanation	O
technique	O
(	O
Adebayo	O
et	O
al	O
,	O
2018	O
)	O
.	O
Addressing	O
and	O
mitigating	O
bias	O
in	O
NLP	O
models	O
are	O
paramount	O
tasks	O
as	O
the	O
effects	O
on	O
these	O
models	O
adversely	O
affect	O
protected	O
subpopulations	O
(	O
Schmidt	O
and	O
Wiegand	O
,	O
2017	O
)	O
.	O
One	O
of	O
the	O
earliest	O
works	O
is	O
Calders	O
and	O
Verwer	O
(	O
2010	O
)	O
.	O
Later	O
,	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
proposed	O
to	O
unbias	O
word	O
vectors	O
from	O
gender	O
stereotypes	O
.	O
Park	O
et	O
al	O
(	O
2018	O
)	O
also	O
try	O
to	O
address	O
gender	O
bias	O
for	O
abusive	B-TaskName
language	I-TaskName
detection	O
models	O
by	O
debiasing	O
word	O
vectors	O
,	O
augmenting	O
more	O
data	O
and	O
changing	O
model	O
architecture	O
.	O
While	O
their	O
results	O
seem	O
to	O
show	O
promise	O
for	O
removing	O
gender	O
bias	O
,	O
their	O
method	O
does	O
n't	O
scale	O
for	O
other	O
identity	O
dimensions	O
such	O
as	O
race	O
and	O
religion	O
.	O
The	O
authors	O
of	O
Dixon	O
et	O
al	O
(	O
2018	O
)	O
highlight	O
the	O
bias	O
in	O
toxic	O
comment	O
classifier	O
models	O
originating	O
from	O
the	O
dataset	O
.	O
They	O
also	O
supplement	O
the	O
training	O
dataset	O
from	O
Wikipedia	O
articles	O
to	O
shift	O
positive	O
class	O
imbalance	O
for	O
sentences	O
containing	O
identity	O
terms	O
to	O
dataset	O
average	O
.	O
Similarly	O
,	O
their	O
approach	O
alleviates	O
the	O
issue	O
to	O
a	O
certain	O
extent	O
,	O
but	O
does	O
not	O
scale	O
to	O
similar	O
problems	O
as	O
their	O
augmentation	O
technique	O
is	O
too	O
data	O
-	O
specific	O
.	O
Also	O
,	O
both	O
methods	O
trade	O
original	O
task	O
accuracy	B-MetricName
for	O
fairness	O
,	O
while	O
our	O
method	O
does	O
not	O
.	O
Lastly	O
,	O
there	O
are	O
several	O
works	O
(	O
Davidson	O
et	O
al	O
,	O
2017	O
;	O
Zhang	O
et	O
al	O
,	O
2018b	O
)	O
offering	O
methodologies	O
or	O
datasets	O
to	O
evaluate	O
models	O
for	O
unintended	O
bias	O
,	O
but	O
they	O
fail	O
to	O
offer	O
a	O
general	O
framework	O
.	O
One	O
of	O
the	O
main	O
reasons	O
our	O
approach	O
improves	O
the	O
model	O
in	O
the	O
original	O
task	O
is	O
that	O
the	O
model	O
is	O
now	O
more	O
robust	O
thanks	O
to	O
the	O
reinforcement	O
provided	O
to	O
the	O
model	O
builder	O
through	O
attributions	O
.	O
From	O
a	O
fairness	O
angle	O
,	O
our	O
technique	O
shares	O
similarities	O
with	O
adversarial	O
training	O
(	O
Zhang	O
et	O
al	O
,	O
2018a	O
;	O
Madras	O
et	O
al	O
,	O
2018	O
)	O
in	O
asking	O
the	O
model	O
to	O
optimize	O
for	O
an	O
additional	O
objective	O
that	O
transitively	O
unbiases	O
the	O
classifier	O
.	O
However	O
,	O
those	O
approaches	O
work	O
to	O
remove	O
protected	O
attributes	O
from	O
the	O
representation	O
layer	O
,	O
which	O
is	O
unstable	O
.	O
Our	O
approach	O
,	O
on	O
the	O
other	O
hand	O
,	O
works	O
with	O
basic	O
human	O
-	O
interpretable	O
units	O
of	O
information	O
-	O
tokens	O
.	O
Also	O
,	O
those	O
approaches	O
propose	O
to	O
sacrifice	O
main	O
task	O
performance	O
for	O
fairness	O
as	O
well	O
.	O
While	O
our	O
method	O
enables	O
model	O
builders	O
to	O
inject	O
priors	O
to	O
aid	O
a	O
model	O
,	O
it	O
has	O
several	O
limitations	O
.	O
In	O
solving	O
the	O
fairness	O
problem	O
in	O
question	O
,	O
it	O
causes	O
the	O
classifier	O
to	O
not	O
focus	O
on	O
the	O
identity	O
terms	O
even	O
for	O
the	O
cases	O
where	O
an	O
identity	O
term	O
itself	O
is	O
being	O
used	O
as	O
an	O
insult	O
.	O
Moreover	O
,	O
our	O
approach	O
requires	O
prior	O
terms	O
to	O
be	O
manually	O
provided	O
,	O
which	O
bears	O
resemblance	O
to	O
blacklist	O
approaches	O
and	O
suffers	O
from	O
the	O
same	O
drawbacks	O
.	O
Lastly	O
,	O
the	O
evaluation	O
methodology	O
that	O
we	O
and	O
previous	O
papers	O
(	O
Dixon	O
et	O
al	O
,	O
2018	O
;	O
Park	O
et	O
al	O
,	O
2018	O
)	O
rely	O
on	O
are	O
based	O
on	O
a	O
syntheticallygenerated	O
dataset	O
,	O
which	O
may	O
contain	O
biases	O
of	O
the	O
individuals	O
creating	O
it	O
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
actionability	O
on	O
model	O
explanations	O
that	O
enable	O
ML	O
practitioners	O
to	O
enforce	O
priors	O
on	O
their	O
model	O
.	O
We	O
apply	O
this	O
technique	O
to	O
model	O
fairness	O
in	O
toxic	B-TaskName
comment	I-TaskName
classification	I-TaskName
.	O
Our	O
method	O
incorporates	O
Path	O
Integrated	O
Gradients	O
attributions	O
into	O
the	O
objective	O
function	O
with	O
the	O
aim	O
of	O
stopping	O
the	O
classifier	O
from	O
carrying	O
along	O
false	O
positive	O
bias	O
from	O
the	O
data	O
by	O
punishing	O
it	O
when	O
it	O
focuses	O
on	O
identity	O
words	O
.	O
Our	O
experiments	O
indicate	O
that	O
the	O
models	O
trained	O
jointly	O
with	O
cross	O
-	O
entropy	O
and	O
prior	O
loss	B-MetricName
do	O
not	O
suffer	O
a	O
performance	O
drop	O
on	O
the	O
original	O
task	O
,	O
while	O
achieving	O
a	O
better	O
performance	O
in	O
fairness	O
metrics	O
on	O
the	O
template	O
-	O
based	O
dataset	O
.	O
Applying	O
model	O
attribution	O
as	O
a	O
fine	O
-	O
tuning	O
step	O
on	O
a	O
trained	O
classifier	O
makes	O
it	O
converge	O
to	O
a	O
more	O
debiased	O
classifier	O
in	O
just	O
a	O
few	O
epochs	O
.	O
Additionally	O
,	O
we	O
show	O
that	O
model	O
can	O
be	O
also	O
forced	O
to	O
focus	O
on	O
pre	O
-	O
determined	O
tokens	O
.	O
There	O
are	O
several	O
avenues	O
we	O
can	O
explore	O
as	O
future	O
research	O
.	O
Our	O
technique	O
can	O
be	O
applied	O
to	O
implement	O
a	O
more	O
robust	O
model	O
by	O
penalizing	O
the	O
attributions	O
falling	O
outside	O
of	O
tokens	O
annotated	O
to	O
be	O
relevant	O
to	O
the	O
predicted	O
class	O
.	O
Another	O
avenue	O
is	O
to	O
incorporate	O
different	O
model	O
attribution	O
strategies	O
such	O
as	O
DeepLRP	O
(	O
Bach	O
et	O
al	O
,	O
2015	O
)	O
into	O
the	O
objective	O
function	O
.	O
Finally	O
,	O
it	O
would	O
be	O
worthwhile	O
to	O
invest	O
in	O
a	O
technique	O
to	O
extract	O
problematic	O
terms	O
from	O
the	O
model	O
automatically	O
rather	O
than	O
providing	O
prescribed	O
identity	O
or	O
toxic	O
terms	O
.	O

