Context	O
-	O
Aware	O
Graph	O
Segmentation	O
for	O
Graph	O
-	O
Based	O
Translation	B-TaskName

During	O
training	O
,	O
given	O
a	O
word	O
-	O
aligned	O
graphstring	O
pair	O
g	O
,	O
t	O
,	O
a	O
,	O
we	O
extract	O
translation	O
rules	O
g	O
a	O
i	O
,	O
c	O
a	O
i	O
,	O
t	O
i	O
,	O
each	O
of	O
which	O
consists	O
of	O
a	O
continuous	O
target	O
phrase	O
t	O
i	O
,	O
a	O
source	O
subgraph	O
g	O
a	O
i	O
aligned	O
to	O
t	O
i	O
,	O
and	O
a	O
source	O
context	O
c	O
a	O
i	O
.	O
We	O
first	O
find	O
initial	O
pairs	O
.	O
s	O
a	O
i	O
,	O
t	O
i	O
is	O
an	O
initial	O
pair	O
,	O
iff	O
it	O
is	O
consistent	O
with	O
the	O
word	B-TaskName
alignment	I-TaskName
a	O
(	O
Och	O
and	O
Ney	O
,	O
2004	O
)	O
.	O
s	O
a	O
j	O
is	O
a	O
set	O
of	O
source	O
words	O
which	O
are	O
aligned	O
to	O
t	O
i	O
.	O
Then	O
,	O
the	O
set	O
of	O
rules	O
satisfies	O
the	O
following	O
:	O
1	O
.	O
If	O
s	O
a	O
i	O
,	O
t	O
i	O
is	O
an	O
initial	O
pair	O
ands	O
a	O
i	O
is	O
covered	O
by	O
a	O
subgraph	O
g	O
a	O
i	O
which	O
is	O
connected	O
,	O
then	O
g	O
a	O
i	O
,	O
*	O
,	O
t	O
i	O
is	O
a	O
basic	O
rule	O
.	O
c	O
a	O
i	O
=	O
*	O
means	O
that	O
a	O
basic	O
rule	O
is	O
applied	O
without	O
considering	O
context	O
to	O
make	O
sure	O
that	O
at	O
least	O
one	O
translation	O
is	O
produced	O
for	O
any	O
inputs	O
during	O
decoding	O
.	O
Therefore	O
,	O
basic	O
rules	O
are	O
the	O
same	O
as	O
rules	O
in	O
the	O
conventional	O
graph	O
-	O
based	O
model	O
.	O
Rule	O
(	O
3	O
)	O
shows	O
an	O
example	O
of	O
a	O
basic	O
rule	O
:	O
2010Nian	O
FIFA	O
Shijiebei	O
2010	O
FIFA	O
World	O
Cup	O
(	O
3	O
)	O
2	O
.	O
Assume	O
g	O
a	O
i	O
,	O
*	O
,	O
t	O
i	O
is	O
a	O
basic	O
rule	O
and	O
s	O
a	O
i+1	O
,	O
t	O
i+1	O
is	O
an	O
initial	O
pair	O
where	O
t	O
i+1	O
is	O
on	O
the	O
right	O
of	O
and	O
adjacent	O
to	O
t	O
i	O
.	O
If	O
there	O
are	O
edges	O
between	O
g	O
a	O
i	O
ands	O
a	O
i+1	O
,	O
then	O
g	O
a	O
i	O
,	O
c	O
a	O
i	O
,	O
t	O
i	O
is	O
a	O
segmenting	O
rule	O
,	O
where	O
c	O
a	O
i	O
is	O
the	O
set	O
of	O
edges	O
between	O
g	O
a	O
i	O
ands	O
a	O
i+1	O
by	O
treatings	O
a	O
i+1	O
as	O
a	O
single	O
node	O
x.	O
Rule	O
(	O
4	O
)	O
is	O
an	O
example	O
of	O
a	O
segmenting	O
rule	O
:	O
2010Nian	O
FIFA	O
x	O
2010	O
FIFA	O
(	O
4	O
)	O
where	O
dashed	O
links	O
are	O
contextual	O
connections	O
.	O
During	O
decoding	O
,	O
when	O
the	O
context	O
matches	O
,	O
rule	O
(	O
4	O
)	O
translates	O
a	O
subgraph	O
over	O
2010Nian	O
FIFA	O
into	O
a	O
target	O
phrase	O
2010	O
FIFA	O
.	O
For	O
example	O
,	O
it	O
can	O
be	O
applied	O
to	O
graph	O
(	O
5	O
)	O
where	O
Shijiebei	O
Zai	O
Nanfei	O
(	O
in	O
the	O
dashed	O
rectangle	O
)	O
is	O
treated	O
as	O
x	O
:	O
2010Nian	O
FIFA	O
Shijiebei	O
Zai	O
Nanfei	O
(	O
5	O
)	O
3	O
.	O
If	O
there	O
are	O
no	O
edges	O
between	O
g	O
a	O
i	O
ands	O
a	O
i+1	O
,	O
then	O
c	O
a	O
i	O
is	O
equal	O
to	O
and	O
g	O
a	O
i	O
,	O
,	O
t	O
i	O
is	O
a	O
translation	O
rule	O
,	O
called	O
a	O
selecting	O
rule	O
in	O
this	O
paper	O
.	O
During	O
decoding	O
,	O
the	O
untranslated	O
input	O
could	O
be	O
a	O
set	O
of	O
subgraphs	O
which	O
are	O
disjoint	O
with	O
each	O
other	O
.	O
A	O
selecting	O
rule	O
is	O
used	O
to	O
select	O
one	O
of	O
them	O
.	O
For	O
example	O
,	O
rule	O
(	O
6	O
)	O
can	O
be	O
applied	O
to	O
(	O
7	O
)	O
to	O
translate	O
2010Nian	O
FIFA	O
to	O
2010	O
FIFA	O
.	O
In	O
this	O
example	O
,	O
the	O
x	O
in	O
rule	O
(	O
6	O
)	O
matches	O
with	O
Chenggong	O
Juxing	O
(	O
in	O
the	O
dashed	O
rectangle	O
)	O
in	O
(	O
7	O
)	O
.	O
By	O
comparing	O
these	O
three	O
types	O
of	O
rules	O
,	O
we	O
observe	O
that	O
both	O
segmenting	O
rules	O
and	O
selecting	O
rules	O
are	O
based	O
on	O
basic	O
rules	O
.	O
They	O
extend	O
basic	O
rules	O
by	O
adding	O
contextual	O
information	O
to	O
their	O
source	O
subgraphs	O
so	O
that	O
basic	O
rules	O
are	O
split	O
into	O
different	O
groups	O
according	O
to	O
the	O
context	O
.	O
During	O
decoding	O
,	O
the	O
context	O
will	O
help	O
to	O
select	O
target	O
phrases	O
as	O
well	O
.	O
Algorithm	O
1	O
illustrates	O
a	O
simple	O
process	O
for	O
rule	O
extraction	O
.	O
Given	O
a	O
word	O
-	O
aligned	O
graph	O
-	O
string	O
pair	O
,	O
we	O
first	O
extract	O
all	O
initial	O
pairs	O
(	O
Line	O
1	O
)	O
.	O
Then	O
,	O
we	O
find	O
basic	O
rules	O
from	O
these	O
pairs	O
(	O
Lines	O
3	O
-	O
4	O
)	O
.	O
Basic	O
Algorithm	O
1	O
:	O
An	O
algorithm	O
for	O
extracting	O
translation	O
rules	O
from	O
a	O
graph	O
-	O
string	O
pair	O
.	O
Data	O
:	O
Word	O
-	O
aligned	O
graph	O
-	O
string	O
pair	O
g	O
,	O
t	O
,	O
a	O
Result	O
:	O
A	O
set	O
of	O
translation	O
rules	O
R	O
1	O
find	O
a	O
set	O
of	O
initial	O
pairs	O
P	O
;	O
2	O
for	O
each	O
p	O
=	O
s	O
a	O
i	O
,	O
t	O
i	O
in	O
P	O
do	O
3	O
if	O
s	O
j	O
i	O
is	O
connected	O
then	O
//	O
basic	O
rules	O

Joint	O
Models	O
for	O
Answer	O
Verification	O
in	O
Question	B-TaskName
Answering	I-TaskName
Systems	O

This	O
paper	O
studies	O
joint	O
models	O
for	O
selecting	O
correct	O
answer	O
sentences	O
among	O
the	O
top	O
k	O
provided	O
by	O
answer	O
sentence	O
selection	O
(	O
AS2	O
)	O
modules	O
,	O
which	O
are	O
core	O
components	O
of	O
retrievalbased	O
Question	B-TaskName
Answering	I-TaskName
(	O
QA	O
)	O
systems	O
.	O
Our	O
work	O
shows	O
that	O
a	O
critical	O
step	O
to	O
effectively	O
exploiting	O
an	O
answer	O
set	O
regards	O
modeling	O
the	O
interrelated	O
information	O
between	O
pair	O
of	O
answers	O
.	O
For	O
this	O
purpose	O
,	O
we	O
build	O
a	O
three	O
-	O
way	O
multiclassifier	O
,	O
which	O
decides	O
if	O
an	O
answer	O
supports	O
,	O
refutes	O
,	O
or	O
is	O
neutral	O
with	O
respect	O
to	O
another	O
one	O
.	O
More	O
specifically	O
,	O
our	O
neural	O
architecture	O
integrates	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
AS2	O
module	O
with	O
the	O
multi	O
-	O
classifier	O
,	O
and	O
a	O
joint	O
layer	O
connecting	O
all	O
components	O
.	O
We	O
tested	O
our	O
models	O
on	O
Wik	O
-	O
iQA	O
,	O
TREC	B-DatasetName
-	O
QA	O
,	O
and	O
a	O
real	O
-	O
world	O
dataset	O
.	O
The	O
results	O
show	O
that	O
our	O
models	O
obtain	O
the	O
new	O
state	O
of	O
the	O
art	O
in	O
AS2	O
.	O
*	O
Work	O
done	O
while	O
the	O
author	O
was	O
an	O
intern	O
at	O
Amazon	O
Alexa	O
Claim	O
:	O
Joe	O
Walsh	O
was	O
inducted	O
in	O
2001	O
.	O
Ev1	O
:	O
As	O
a	O
member	O
of	O
the	O
Eagles	O
,	O
Walsh	O
was	O
inducted	O
into	O
the	O
Rock	O
and	O
Roll	O
Hall	O
of	O
Fame	O
in	O
1998	O
,	O
and	O
into	O
the	O
Vocal	O
Group	O
Hall	O
of	O
Fame	O
in	O
2001	O
.	O
Ev2	O
:	O
Joseph	O
Fidler	O
Walsh	O
(	O
born	O
November	O
20	O
,	O
1947	O
)	O
is	O
an	O
American	O
singer	O
songwriter	O
,	O
composer	O
,	O
multiinstrumentalist	O
and	O
record	O
producer	O
.	O
Walsh	O
was	O
awarded	O
with	O
the	O
Vocal	O
Group	O
Hall	O
of	O
Fame	O
in	O
2001	O
.	O

Automated	O
Question	B-TaskName
Answering	I-TaskName
(	O
QA	O
)	O
research	O
has	O
received	O
a	O
renewed	O
attention	O
thanks	O
to	O
the	O
diffusion	O
of	O
Virtual	O
Assistants	O
.	O
Among	O
the	O
different	O
types	O
of	O
methods	O
to	O
implement	O
QA	O
systems	O
,	O
we	O
focus	O
on	O
Answer	O
Sentence	O
Selection	O
(	O
AS2	O
)	O
research	O
,	O
originated	O
from	O
TREC	B-DatasetName
-	O
QA	O
track	O
(	O
Voorhees	O
and	O
Tice	O
,	O
1999	O
)	O
,	O
as	O
it	O
proposes	O
efficient	O
models	O
that	O
are	O
more	O
suitable	O
for	O
a	O
production	O
setting	O
,	O
e.g.	O
,	O
they	O
are	O
more	O
efficient	O
than	O
those	O
developed	O
in	O
machine	O
reading	O
(	O
MR	B-DatasetName
)	O
work	O
.	O
Garg	O
et	O
al	O
(	O
2020	O
)	O
proposed	O
the	O
TANDA	O
approach	O
based	O
on	O
pre	O
-	O
trained	O
Transformer	B-MethodName
models	O
,	O
obtaining	O
impressive	O
improvement	O
over	O
the	O
state	O
of	O
the	O
art	O
for	O
AS2	O
,	O
measured	O
on	O
the	O
two	O
most	O
used	O
datasets	O
,	O
WikiQA	B-DatasetName
(	O
Yang	O
et	O
al	O
,	O
2015	O
)	O
and	O
TREC	B-DatasetName
-	O
QA	O
(	O
Wang	O
et	O
al	O
,	O
2007	O
)	O
.	O
However	O
,	O
TANDA	O
was	O
applied	O
only	O
to	O
pointwise	O
rerankers	O
(	O
PR	O
)	O
,	O
e.g.	O
,	O
simple	O
binary	O
classifiers	O
.	O
Bonadiman	O
(	O
2020	O
)	O
tried	O
to	O
improve	O
this	O
model	O
by	O
jointly	O
modeling	O
all	O
answer	O
candidates	O
with	O
listwise	O
methods	O
,	O
e.g.	O
,	O
(	O
Bian	O
et	O
al	O
,	O
2017	O
)	O
.	O
Unfortunately	O
,	O
merging	O
the	O
embeddings	O
from	O
all	O
candidates	O
with	O
standard	O
approaches	O
,	O
e.g.	O
,	O
CNN	O
or	O
LSTM	B-MethodName
,	O
did	O
not	O
improve	O
over	O
TANDA	O
.	O

MR	B-DatasetName
is	O
a	O
popular	O
QA	O
task	O
that	O
identifies	O
an	O
answer	O
string	O
in	O
a	O
paragraph	O
or	O
a	O
text	O
of	O
limited	O
size	O
for	O
a	O
question	O
.	O
Its	O
application	O
to	O
retrieval	O
scenario	O
has	O
also	O
been	O
studied	O
Hu	O
et	O
al	O
,	O
2019	O
;	O
Kratzwald	O
and	O
Feuerriegel	O
,	O
2018	O
)	O
.	O
However	O
,	O
the	O
large	O
volume	O
of	O
retrieved	O
content	O
makes	O
their	O
use	O
not	O
practical	O
yet	O
.	O
Moreover	O
,	O
the	O
joint	O
modeling	O
aspect	O
of	O
MR	B-DatasetName
regards	O
sentences	O
from	O
the	O
same	O
paragraphs	O
.	O
Jin	O
et	O
al	O
(	O
2020	O
)	O
use	O
the	O
relation	O
between	O
candidates	O
in	O
Multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
approach	O
for	O
AS2	O
.	O
However	O
,	O
they	O
do	O
not	O
exploit	O
transformer	O
models	O
,	O
thus	O
their	O
results	O
are	O
rather	O
below	O
the	O
state	O
of	O
the	O
art	O
.	O
In	O
contrast	O
with	O
the	O
work	O
above	O
,	O
our	O
modeling	O
is	O
driven	O
by	O
an	O
answer	O
support	O
strategy	O
,	O
where	O
the	O
pieces	O
of	O
information	O
are	O
taken	O
from	O
different	O
documents	O
.	O
This	O
makes	O
our	O
model	O
even	O
more	O
unique	O
;	O
it	O
allows	O
us	O
to	O
design	O
innovative	O
joint	O
models	O
,	O
which	O
are	O
still	O
not	O
designed	O
in	O
any	O
MR	B-DatasetName
systems	O
.	O

Fact	B-TaskName
verification	I-TaskName
has	O
become	O
a	O
social	O
need	O
given	O
the	O
massive	O
amount	O
of	O
information	O
generated	O
daily	O
.	O
The	O
problem	O
is	O
,	O
therefore	O
,	O
becoming	O
increasingly	O
important	O
in	O
NLP	O
context	O
(	O
Mihaylova	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
QA	O
,	O
answer	O
verification	O
is	O
directly	O
relevant	O
due	O
to	O
its	O
nature	O
of	O
content	O
delivery	O
(	O
Mihaylova	O
et	O
al	O
,	O
2019	O
)	O
.	O
The	O
problem	O
has	O
been	O
explored	O
in	O
MR	B-DatasetName
setting	O
(	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
.	O
Zhang	O
et	O
al	O
(	O
2020a	O
)	O
also	O
proposed	O
to	O
fact	O
check	O
for	O
product	O
questions	O
using	O
additional	O
associated	O
evidence	O
sentences	O
.	O
The	O
latter	O
are	O
retrieved	O
based	O
on	O
similarity	O
scores	O
computed	O
with	O
both	O
TF	O
-	O
IDF	O
and	O
sentence	O
-	O
embeddings	O
from	O
pre	O
-	O
trained	O
BERT	B-MethodName
models	O
.	O
While	O
the	O
process	O
is	O
technically	O
sound	O
,	O
the	O
retrieval	O
of	O
evidence	O
is	O
an	O
expensive	O
process	O
,	O
which	O
is	O
prohibitive	O
to	O
scale	O
in	O
production	O
.	O
We	O
instead	O
address	O
this	O
problem	O
by	O
leveraging	O
the	O
top	O
answer	O
candidates	O
.	O

ASR	O
still	O
selects	O
answers	O
with	O
a	O
pointwise	O
approach	O
2	O
.	O
This	O
means	O
that	O
we	O
can	O
improve	O
it	O
by	O
building	O
a	O
listwise	O
model	O
,	O
to	O
select	O
the	O
best	O
answer	O
for	O
each	O
question	O
,	O
by	O
utilizing	O
the	O
information	O
from	O
all	O
target	O
answers	O
.	O
In	O
particular	O
,	O
the	O
architecture	O
of	O
MASR	O
shown	O
in	O
Figure	O
1d	O
is	O
made	O
up	O
of	O
two	O
parts	O
:	O
(	O
i	O
)	O
a	O
list	O
of	O
ASR	O
containing	O
k	O
+	O
1	O
ASR	O
blocks	O
,	O
in	O
which	O
each	O
ASR	O
block	O
provides	O
the	O
representation	O
of	O
a	O
target	O
answer	O
t.	O
(	O
ii	O
)	O
A	O
final	O
multiclassifier	O
and	O
a	O
softmax	B-MethodName
function	O
,	O
which	O
scores	O
each	O
t	O
from	O
k	O
+	O
1	O
embedding	O
concatenation	O
and	O
selects	O
the	O
one	O
with	O
highest	O
score	O
.	O
For	O
training	O
and	O
testing	O
,	O
we	O
select	O
the	O
t	O
from	O
the	O
k	O
+	O
1	O
candidates	O
of	O
q	O
based	O
on	O
a	O
softmax	B-MethodName
output	O
at	O
a	O
time	O
.	O

We	O
used	O
two	O
most	O
popular	O
AS2	O
datasets	O
,	O
and	O
one	O
real	O
world	O
application	O
dataset	O
we	O
built	O
to	O
test	O
the	O
generality	O
of	O
our	O
approach	O
.	O
WikiQA	B-DatasetName
is	O
a	O
QA	O
dataset	O
(	O
Yang	O
et	O
al	O
,	O
2015	O
)	O
containing	O
a	O
sample	O
of	O
questions	O
and	O
answer	O
-	O
sentence	O
candidates	O
from	O
Bing	O
query	O
logs	O
over	O
Wikipedia	O
.	O
The	O
answers	O
are	O
manually	O
labeled	O
.	O
We	O
follow	O
the	O
most	O
used	O
setting	O
:	O
training	O
with	O
all	O
the	O
questions	O
that	O
have	O
at	O
least	O
one	O
correct	O
answer	O
,	O
and	O
validating	O
and	O
testing	O
with	O
all	O
the	O
questions	O
having	O
at	O
least	O
one	O
correct	O
and	O
one	O
incorrect	O
answer	O
.	O
Wang	O
et	O
al	O
(	O
2007	O
)	O
.	O
We	O
use	O
the	O
same	O
splits	O
of	O
the	O
original	O
data	O
,	O
following	O
the	O
common	O
setting	O
of	O
previous	O
work	O
,	O
e.g.	O
,	O
(	O
Garg	O
et	O
al	O
,	O
2020	O
)	O
.	O
WQA	O
The	O
Web	O
-	O
based	O
Question	B-TaskName
Answering	I-TaskName
is	O
a	O
dataset	O
built	O
by	O
Alexa	O
AI	O
as	O
part	O
of	O
the	O
effort	O
to	O
improve	O
understanding	O
and	O
benchmarking	O
in	O
QA	O
systems	O
.	O
The	O
creation	O
process	O
includes	O
the	O
following	O
steps	O
:	O
(	O
i	O
)	O
given	O
a	O
set	O
of	O
questions	O
we	O
collected	O
from	O
the	O
web	O
,	O
a	O
search	O
engine	O
is	O
used	O
to	O
retrieve	O
up	O
to	O
1	O
,	O
000	O
web	O
pages	O
from	O
an	O
index	O
containing	O
hundreds	O
of	O
millions	O
pages	O
.	O
(	O
ii	O
)	O
From	O
the	O
set	O
of	O
retrieved	O
documents	O
,	O
all	O
candidate	O
sentences	O
are	O
extracted	O
and	O
ranked	O
using	O
AS2	O
models	O
from	O
(	O
Garg	O
et	O
al	O
,	O
2020	O
)	O
.	O
Finally	O
,	O
(	O
iii	O
)	O
top	O
candidates	O
for	O
each	O
question	O
are	O
manually	O
assessed	O
as	O
correct	O
or	O
incorrect	O
by	O
human	O
judges	O
.	O
This	O
allowed	O
us	O
to	O
obtain	O
a	O
richer	O
variety	O
of	O
answers	O
from	O
multiple	O
sources	O
with	O
a	O
higher	O
average	O
number	O
of	O
answers	O
.	O
FEVER	B-DatasetName
is	O
a	O
large	O
-	O
scale	O
public	O
corpus	O
,	O
proposed	O
by	O
Thorne	O
et	O
al	O
(	O
2018a	O
)	O
for	O
fact	B-TaskName
verification	I-TaskName
task	O
,	O
consisting	O
of	O
185	O
,	O
455	O
annotated	O
claims	O
from	O
5	O
,	O
416	O
,	O
537	O
documents	O
from	O
the	O
Wikipedia	O
dump	O
in	O
June	O
2017	O
.	O
All	O
claims	O
are	O
labelled	O
as	O
Supported	O
,	O
Refuted	O
or	O
Not	O
Enough	O
Info	O
by	O
annotators	O
.	O
Table	O
3	O
shows	O
the	O
statistics	O
of	O
the	O
dataset	O
,	O
which	O
remains	O
the	O
same	O
as	O
in	O
(	O
Thorne	O
et	O
al	O
,	O
2018b	O
)	O
.	O

We	O
use	O
the	O
same	O
configuration	O
of	O
the	O
ASR	O
training	O
,	O
including	O
the	O
optimizer	B-HyperparameterName
type	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
epochs	I-HyperparameterName
,	O
GPU	O
type	O
,	O
maximum	O
sequence	O
length	O
,	O
etc	O
.	O
Additionally	O
,	O
we	O
design	O
two	O
different	O
models	O
MASR	O
-	O
F	O
,	O
using	O
an	O
ASC	O
classifier	O
targeting	O
the	O
FEVER	B-DatasetName
labels	O
,	O
and	O
MASR	O
-	O
FP	O
,	O
which	O
initializes	O
ASC	O
with	O
the	O
data	O
from	O
FEVER	B-DatasetName
.	O
This	O
is	O
possible	O
as	O
the	O
labels	O
are	O
compatible	O
.	O

The	O
selection	O
of	O
the	O
hyper	O
-	O
parameter	O
k	O
,	O
i.e.	O
,	O
the	O
number	O
of	O
candidates	O
to	O
consider	O
for	O
supporting	O
a	O
target	O
answer	O
is	O
rather	O
tricky	O
.	O
Indeed	O
,	O
the	O
standard	O
validation	O
set	O
is	O
typically	O
used	O
for	O
tuning	O
PR	O
.	O
This	O
means	O
that	O
the	O
candidates	O
PR	O
moves	O
to	O
the	O
top	O
k	O
+1	O
positions	O
are	O
optimistically	O
accurate	O
.	O
Thus	O
,	O
when	O
selecting	O
also	O
the	O
optimal	O
k	O
on	O
the	O
same	O
validation	O
set	O
,	O
there	O
is	O
high	O
risk	O
to	O
overfit	O
the	O
model	O
.	O
We	O
solved	O
this	O
problem	O
by	O
running	O
a	O
PR	O
version	O
not	O
heavily	O
optimized	O
on	O
the	O
dev	O
.	O
set	O
,	O
i.e.	O
,	O
we	O
randomly	O
choose	O
a	O
checkpoint	O
after	O
the	O
standard	O
three	O
epochs	O
of	O
fine	O
-	O
tuning	O
of	O
RoBERTa	B-MethodName
transformer	O
.	O
Additionally	O
,	O
we	O
tuned	O
k	O
only	O
using	O
the	O
WQA	O
dev	O
.	O
set	O
,	O
which	O
contains	O
∼	O
36	O
,	O
000	O
Q	O
/	O
A	O
pairs	O
.	O
WikiQA	B-DatasetName
and	O
TREC	B-DatasetName
-	O
QA	O
dev	O
.	O
sets	O
are	O
too	O
small	O
to	O
be	O
used	O
(	O
121	O
and	O
65	O
questions	O
,	O
respectively	O
)	O
.	O
Fig	O
.	O
2	O
plots	O
the	O
improvement	O
of	O
four	O
different	O
models	O
,	O
Joint	O
Model	O
Multi	O
-	O
classifier	O
,	O
Joint	O
Model	O
Pairwise	O
,	O
KGAT	O
,	O
and	O
ASR	O
,	O
when	O
using	O
different	O
k	O
values	O
.	O
Their	O
best	O
results	O
are	O
reached	O
for	O
5	O
,	O
3	O
,	O
2	O
,	O
and	O
3	O
,	O
respectively	O
.	O
We	O
note	O
that	O
the	O
most	O
reliable	O
curve	O
shape	O
(	O
convex	O
)	O
is	O
the	O
one	O
of	O
ASR	O
and	O
Joint	O
Model	O
Pairwise	O
.	O

We	O
have	O
proposed	O
new	O
joint	O
models	O
for	O
AS2	O
.	O
ASR	O
encodes	O
the	O
relation	O
between	O
the	O
target	O
answer	O
and	O
all	O
the	O
other	O
candidates	O
,	O
using	O
an	O
additional	O
Transformer	B-MethodName
model	O
,	O
and	O
an	O
Answer	O
Support	O
Classifier	O
,	O
while	O
MASR	O
jointly	O
models	O
the	O
ASR	O
representations	O
for	O
all	O
target	O
answers	O
.	O
We	O
extensively	O
tested	O
KGAT	O
,	O
ASR	O
,	O
MASR	O
,	O
and	O
other	O
joint	O
model	O
baselines	O
we	O
designed	O
.	O
The	O
results	O
show	O
that	O
our	O
models	O
can	O
outperform	O
the	O
state	O
of	O
the	O
art	O
.	O
Most	O
interestingly	O
,	O
ASR	O
constantly	O
outperforms	O
all	O
the	O
models	O
(	O
but	O
MASR	O
-	O
FP	O
)	O
,	O
on	O
all	O
datasets	O
,	O
through	O
all	O
measures	O
,	O
and	O
for	O
both	O
base	O
and	O
large	O
transformers	O
.	O
For	O
example	O
,	O
ASR	O
q	O
:	O
What	O
kind	O
of	O
colors	O
are	O
in	O
the	O
rainbow	O
?	O
c1	O
:	O
Red	O
,	O
yellow	O
,	O
and	O
blue	O
are	O
called	O
the	O
primary	O
colors	O
.	O
c2	O
:	O
The	O
order	O
of	O
the	O
colors	O
in	O
the	O
rainbow	O
goes	O
:	O
red	O
,	O
orange	O
,	O
yellow	O
,	O
green	O
,	O
blue	O
,	O
indigo	O
and	O
violet	O
.	O
c3	B-DatasetName
:	O
The	O
colors	O
in	O
all	O
rainbows	O
are	O
present	O
in	O
the	O
same	O
order	O
:	O
red	O
,	O
orange	O
,	O
yellow	O
,	O
green	O
,	O
blue	O
,	O
indigo	O
,	O
and	O
violet	O
.	O
c4	B-DatasetName
:	O
A	O
rainbow	O
occurs	O
when	O
white	O
light	O
bends	O
and	O
separates	O
into	O
red	O
,	O
orange	O
,	O
yellow	O
,	O
green	O
blue	O
,	O
indigo	O
and	O
violet	O
.	O

Open	O
-	O
Domain	O
Why	O
-	O
Question	B-TaskName
Answering	I-TaskName
with	O
Adversarial	O
Learning	O
to	O
Encode	O
Answer	O
Texts	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
method	O
for	O
whyquestion	O
answering	O
(	O
why	O
-	O
QA	O
)	O
that	O
uses	O
an	O
adversarial	O
learning	O
framework	O
.	O
Existing	O
why	O
-	O
QA	O
methods	O
retrieve	O
answer	O
passages	O
that	O
usually	O
consist	O
of	O
several	O
sentences	O
.	O
These	O
multi	O
-	O
sentence	O
passages	O
contain	O
not	O
only	O
the	O
reason	O
sought	O
by	O
a	O
why	O
-	O
question	O
and	O
its	O
connection	O
to	O
the	O
why	O
-	O
question	O
,	O
but	O
also	O
redundant	O
and/or	O
unrelated	O
parts	O
.	O
We	O
use	O
our	O
proposed	O
Adversarial	O
networks	O
for	O
Generating	O
compact	O
-	O
answer	O
Representation	O
(	O
AGR	O
)	O
to	O
generate	O
from	O
a	O
passage	O
a	O
vector	O
representation	O
of	O
the	O
non	O
-	O
redundant	O
reason	O
sought	O
by	O
a	O
why	O
-	O
question	O
and	O
exploit	O
the	O
representation	O
for	O
judging	O
whether	O
the	O
passage	O
actually	O
answers	O
the	O
why	O
-	O
question	O
.	O
Through	O
a	O
series	O
of	O
experiments	O
using	O
Japanese	O
why	O
-	O
QA	O
datasets	O
,	O
we	O
show	O
that	O
these	O
representations	O
improve	O
the	O
performance	O
of	O
our	O
why	O
-	O
QA	O
neural	O
model	O
as	O
well	O
as	O
that	O
of	O
a	O
BERT	B-MethodName
-	O
based	O
why	O
-	O
QA	O
model	O
.	O
We	O
show	O
that	O
they	O
also	O
improve	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
distantly	O
supervised	O
open	O
-	O
domain	O
QA	O
(	O
DS	O
-	O
QA	O
)	O
method	O
on	O
publicly	O
available	O
English	O
datasets	O
,	O
even	O
though	O
the	O
target	O
task	O
is	O
not	O
a	O
why	O
-	O
QA	O
.	O

We	O
also	O
applied	O
two	O
types	O
of	O
attention	O
mechanisms	O
to	O
the	O
above	O
word	B-TaskName
embeddings	I-TaskName
.	O
The	O
first	O
type	O
of	O
attention	O
,	O
similarity	O
-	O
attention	O
,	O
was	O
used	O
for	O
estimating	O
the	O
similarities	O
between	O
words	O
in	O
question	O
q	O
and	O
those	O
in	O
passage	O
/	O
compact	O
-	O
answers	O
t	O
and	O
focusing	O
on	O
the	O
attended	O
words	O
as	O
those	O
that	O
directly	O
indicate	O
the	O
connection	O
between	O
the	O
question	O
and	O
passage	O
/	O
compact	O
-	O
answers	O
.	O
Basically	O
,	O
the	O
mechanism	O
computes	O
the	O
cosine	O
similarity	O
between	O
the	O
embeddings	O
of	O
the	O
words	O
in	O
q	O
and	O
t	O
,	O
and	O
uses	O
it	O
for	O
producing	O
attention	O
feature	O
vector	O
a	O
s	O
j	O
R	O
for	O
word	O
t	O
j	O
in	O
passage	O
/	O
compact	O
-	O
answers	O
.	O
Another	O
attention	O
mechanism	O
,	O
causalityattention	O
,	O
was	O
proposed	O
for	O
focusing	O
on	O
passage	O
words	O
causally	O
associated	O
with	O
question	O
words	O
.	O
They	O
used	O
normalized	O
point	O
-	O
wise	O
mutual	O
information	O
to	O
measure	O
the	O
strength	O
of	O
the	O
causal	O
associations	O
with	O
the	O
causality	O
expressions	O
used	O
for	O
creating	O
the	O
causal	O
embeddings	O
.	O
The	O
scores	O
are	O
used	O
for	O
producing	O
causality	O
-	O
attention	O
feature	O
vector	O
a	O
c	O
j	O
for	O
word	O
t	O
j	O
.	O
Finally	O
,	O
we	O
form	O
two	O
attention	O
feature	O
vectors	O
,	O
a	O
s	O
=	O
[	O
a	O
s	O
1	O
,	O
,	O
a	O
s	O
|	O
t	O
|	O
]	O
and	O
a	O
c	O
=	O
[	O
a	O
c	O
1	O
,	O
,	O
a	O
c	O
|	O
t	O
|	O
]	O
,	O
con	O
-	O
catenate	O
them	O
into	O
a	O
=	O
[	O
a	O
s	O
;	O
a	O
c	O
]	O
R	O
2×	O
|	O
t	O
|	O
,	O
and	O
produce	O
attention	O
-	O
weighted	O
word	O
embedding	O
t	O
att	O
of	O
given	O
text	O
t	O
,	O
which	O
is	O
either	O
an	O
answer	O
passage	O
or	O
a	O
compact	O
answer	O
:	O
t	O
att	O
=	O
ReLU	B-MethodName
(	O
W	O
t	O
t	O
+	O
W	O
a	O
a	O
)	O
where	O
W	O
t	O
R	O
2d×2d	O
and	O
W	O
a	O
R	O
2d×2	O
are	O
trainable	O
parameters	O
,	O
t	O
is	O
the	O
representation	O
of	O
text	O
t	O
,	O
and	O
ReLU	B-MethodName
represents	O
the	O
rectified	B-MethodName
linear	I-MethodName
units	I-MethodName
.	O

We	O
used	O
three	O
datasets	O
,	O
W	O
hySet	O
,	O
CmpAns	O
,	O
and	O
AddT	O
r	O
,	O
for	O
our	O
why	O
-	O
QA	O
experiments	O
.	O
W	O
hySet	O
and	O
AddT	O
r	O
were	O
used	O
for	O
training	O
and	O
evaluating	O
the	O
why	O
-	O
QA	O
models	O
,	O
while	O
CmpAns	O
was	O
used	O
for	O
training	O
AGR	O
.	O
The	O
W	O
hySet	O
dataset	O
,	O
which	O
was	O
used	O
in	O
previous	O
works	O
for	O
why	O
-	O
QA	O
(	O
Oh	O
et	O
al	O
,	O
,	O
2013	O
,	O
is	O
composed	O
of	O
850	O
Japanese	O
why	O
-	O
questions	O
and	O
their	O
top	O
-	O
20	O
answer	O
passages	O
(	O
17	O
,	O
000	O
question	O
-	O
passage	O
pairs	O
)	O
obtained	O
from	O
600	O
million	O
Japanese	O
web	O
pages	O
using	O
the	O
answerretrieval	O
method	O
of	O
Murata	O
et	O
al	O
(	O
2007	O
)	O
,	O
where	O
a	O
question	O
-	O
passage	O
pair	O
is	O
composed	O
of	O
a	O
singlesentence	O
question	O
and	O
a	O
five	O
-	O
sentence	O
passage	O
.	O
The	O
label	O
of	O
each	O
question	O
-	O
answer	O
pair	O
(	O
i.e.	O
,	O
correct	O
answer	O
and	O
incorrect	O
answer	O
)	O
was	O
manually	O
annotated	O
(	O
See	O
Oh	O
et	O
al	O
(	O
2013	O
)	O
for	O
more	O
details	O
)	O
.	O
Oh	O
et	O
al	O
(	O
2013	O
)	O
selected	O
10	O
,	O
000	O
questionpassage	O
pairs	O
as	O
training	O
and	O
test	O
data	O
in	O
10	O
-	O
fold	O
cross	O
-	O
validation	O
(	O
9	O
,	O
000	O
for	O
training	O
and	O
1	O
,	O
000	O
for	O
testing	O
)	O
and	O
used	O
the	O
remainder	O
(	O
7	O
,	O
000	O
questionpassage	O
pairs	O
)	O
as	O
additional	O
training	O
data	O
during	O
the	O
10	O
-	O
fold	O
cross	O
-	O
validation	O
.	O
We	O
followed	O
the	O
settings	O
and	O
,	O
in	O
each	O
fold	O
,	O
we	O
selected	O
1	O
,	O
000	O
pairs	O
from	O
the	O
9	O
,	O
000	O
pairs	O
for	O
training	O
to	O
use	O
as	O
development	O
data	O
for	O
tuning	O
hyperparameters	O
.	O
Note	O
that	O
there	O
are	O
no	O
shared	O
questions	O
in	O
the	O
training	O
,	O
development	O
,	O
or	O
test	O
data	O
.	O
For	O
training	O
the	O
AGR	O
,	O
we	O
used	O
CmpAns	O
,	O
the	O
training	O
data	O
set	O
created	O
in	O
Ishida	O
et	O
al	O
(	O
2018	O
)	O
for	O
compact	O
-	O
answer	B-TaskName
generation	I-TaskName
;	O
CmpAns	O
consists	O
of	O
15	O
,	O
130	O
triples	O
of	O
a	O
why	O
-	O
question	O
,	O
an	O
answer	O
passage	O
,	O
and	O
a	O
manually	O
-	O
created	O
compact	O
answer	O
.	O
These	O
cover	O
2	O
,	O
060	O
unique	O
why	O
-	O
questions	O
.	O
Note	O
that	O
there	O
was	O
no	O
overlap	O
between	O
the	O
questions	O
in	O
CmpAns	O
and	O
those	O
in	O
W	O
hySet	O
.	O
CmpAns	O
was	O
created	O
in	O
the	O
following	O
manner	O
:	O
1	O
)	O
human	O
annotators	O
manually	O
came	O
up	O
with	O
open	O
-	O
domain	O
why	O
-	O
questions	O
,	O
2	O
)	O
retrieved	O
the	O
top	O
-	O
20	O
passages	O
for	O
each	O
why	O
-	O
question	O
using	O
the	O
open	O
-	O
domain	O
why	O
-	O
QA	O
module	O
of	O
a	O
publicly	O
available	O
web	O
-	O
based	O
QA	O
system	O
WISDOM	B-DatasetName
X	O
(	O
Mizuno	O
et	O
al	O
,	O
2016	O
;	O
,	O
and	O
3	O
)	O
three	O
annotators	O
created	O
(	O
when	O
possible	O
)	O
a	O
compact	O
answer	O
for	O
each	O
of	O
the	O
retrieved	O
passages	O
.	O
The	O
passages	O
for	O
which	O
no	O
annotator	O
could	O
create	O
a	O
compact	O
answer	O
were	O
discarded	O
,	O
and	O
were	O
not	O
included	O
in	O
the	O
15	O
,	O
130	O
triples	O
mentioned	O
previously	O
.	O
The	O
average	O
lengths	O
of	O
questions	O
,	O
passages	O
,	O
and	O
compact	O
answers	O
in	O
CmpAns	O
were	O
10.5	O
words	O
,	O
184.4	O
words	O
,	O
and	O
8.3	O
words	O
,	O
respectively	O
.	O
Finally	O
,	O
we	O
created	O
additional	O
training	O
data	O
AddT	O
r	O
for	O
training	O
the	O
why	O
-	O
QA	O
models	O
.	O
If	O
an	O
annotator	O
could	O
write	O
a	O
compact	O
answer	O
for	O
a	O
question	O
and	O
an	O
answer	O
passage	O
,	O
she	O
/	O
he	O
probably	O
recognized	O
the	O
passage	O
as	O
a	O
proper	O
answer	O
passage	O
to	O
the	O
question	O
.	O
Based	O
on	O
this	O
observation	O
,	O
we	O
built	O
AddT	O
r	O
from	O
CmpAns	O
by	O
applying	O
a	O
majority	O
vote	O
.	O
We	O
only	O
gave	O
a	O
correct	O
answer	O
label	O
to	O
a	O
question	O
and	O
a	O
passage	O
if	O
at	O
least	O
two	O
of	O
the	O
three	O
annotators	O
wrote	O
compact	O
answers	O
,	O
and	O
it	O
received	O
an	O
incorrect	O
answer	O
label	O
otherwise	O
.	O
AddT	O
r	O
has	O
10	O
,	O
401	O
pairs	O
in	O
total	O
.	O
We	O
used	O
AddT	O
r	O
as	O
additional	O
training	O
data	O
for	O
baselines	O
that	O
lack	O
a	O
mechanism	O
for	O
generating	O
compact	O
-	O
answer	O
representations	O
,	O
for	O
a	O
fair	O
comparison	O
with	O
other	O
methods	O
that	O
use	O
CmpAns	O
for	O
such	O
mechanisms	O
.	O
We	O
processed	O
all	O
the	O
data	O
with	O
MeCab	O
1	O
,	O
a	O
morphological	O
analyzer	O
,	O
to	O
segment	O
the	O
words	O
.	O

We	O
tried	O
three	O
schemes	O
for	O
training	O
our	O
AGR	O
in	O
our	O
proposed	O
method	O
.	O
In	O
the	O
first	O
scheme	O
,	O
pairs	O
of	O
passages	O
and	O
compact	O
answers	O
in	O
CmpAns	O
were	O
given	O
to	O
fake	O
-	O
representation	O
generator	O
F	O
and	O
realrepresentation	O
generator	O
R	O
as	O
their	O
inputs	O
.	O
We	O
called	O
the	O
fake	O
-	O
representation	O
generator	O
trained	O
in	O
this	O
way	O
F	O
OP	O
and	O
referred	O
to	O
our	O
proposed	O
method	O
using	O
F	O
OP	O
as	O
Ours	O
(	O
OP	O
)	O
.	O
In	O
the	O
second	O
scheme	O
,	O
we	O
randomly	O
sampled	O
five	O
-	O
sentence	O
passages	O
that	O
contain	O
some	O
clue	O
words	O
indicating	O
the	O
existence	O
of	O
causal	O
relations	O
,	O
such	O
as	O
"	O
because	O
,	O
"	O
from	O
4	O
-	O
billion	O
web	O
pages	O
and	O
fed	O
them	O
to	O
fakerepresentation	O
generator	O
F	O
.	O
We	O
fed	O
the	O
same	O
number	O
of	O
the	O
sampled	O
passages	O
as	O
in	O
CmpAns	O
for	O
fair	O
comparison	O
.	O
We	O
refer	O
to	O
the	O
method	O
trained	O
by	O
this	O
scheme	O
as	O
Ours	O
(	O
RP	O
)	O
.	O
In	O
the	O
final	O
scheme	O
,	O
we	O
replaced	O
the	O
word	B-TaskName
embeddings	I-TaskName
for	O
the	O
passages	O
given	O
to	O
fake	O
-	O
representation	O
generator	O
F	O
with	O
random	O
vectors	O
and	O
used	O
similarity	O
-	O
attention	O
but	O
not	O
causality	O
-	O
attention	O
.	O
The	O
fake	O
-	O
representation	O
generator	O
trained	O
in	O
this	O
way	O
is	O
called	O
F	O
RV	O
,	O
and	O
our	O
proposed	O
method	O
using	O
F	O
RV	O
is	O
called	O
Ours	O
(	O
RV	O
)	O
.	O
This	O
scheme	O
is	O
more	O
similar	O
to	O
the	O
original	O
GAN	B-MethodName
than	O
the	O
others	O
because	O
the	O
fake	O
-	O
representation	O
generator	O
is	O
given	O
random	O
noises	O
.	O
We	O
implemented	O
and	O
evaluated	O
the	O
following	O
four	O
why	O
-	O
QA	O
models	O
in	O
previous	O
works	O
as	O
baselines	O
,	O
using	O
the	O
same	O
dataset	O
as	O
ours	O
:	O
We	O
also	O
evaluated	O
nine	O
baseline	O
neural	O
models	O
,	O
four	O
of	O
which	O
are	O
BERT	B-MethodName
-	O
based	O
models	O
(	O
BERT	B-MethodName
,	O
BERT+AddTr	O
,	O
BERT+F	O
OP	O
,	O
and	O
BERT+F	O
RV	O
)	O
,	O
to	O
show	O
the	O
effectiveness	O
of	O
our	O
why	O
-	O
QA	O
model	O
and	O
AGR	O
.	O
They	O
are	O
listed	O
in	O
Table	O
2	O
.	O

Proposed	O
method	O
from	O
which	O
we	O
removed	O
fake	O
-	O
representation	O
generator	O
F	O
.	O
BASE+AddTr	O
BASE	B-MethodName
that	O
used	O
both	O
W	O
hySet	O
and	O
AddT	O
r	O
as	O
its	O
training	O
data	O
.	O

On	O
top	O
of	O
BASE	B-MethodName
,	O
it	O
additionally	O
used	O
real	O
-	O
representation	O
generator	O
R	O
to	O
encode	O
compact	O
answers	O
,	O
which	O
were	O
generated	O
by	O
the	O
compactanswer	O
generator	O
of	O
Iida	O
et	O
al	O
(	O
2019	O
)	O
.	O
R	O
was	O
trained	O
alongside	O
the	O
why	O
-	O
QA	O
model	O
using	O
W	O
hySet	O
and	O
the	O
compact	O
-	O
answer	O
generator	O
was	O
pre	O
-	O
trained	O
with	O
CmpAns	O
.	O

On	O
top	O
of	O
BASE	B-MethodName
,	O
it	O
additionally	O
used	O
the	O
encoder	O
in	O
the	O
compact	O
-	O
answer	O
generator	O
of	O
Iida	O
et	O
al	O
(	O
2019	O
)	O
to	O
create	O
compact	O
-	O
answer	O
representation	O
.	O
The	O
encoder	O
was	O
pre	O
-	O
trained	O
with	O
CmpAns	O
.	O

Same	O
as	O
Ours	O
(	O
OP	O
)	O
except	O
that	O
the	O
fake	O
-	O
representation	O
generator	O
was	O
trained	O
in	O
a	O
supervised	O
manner	O
alongside	O
the	O
why	O
-	O
QA	O
model	O
using	O
W	O
hySet	O
and	O
AddT	O
r	O
as	O
the	O
training	O
data	O
.	O
BERT	B-MethodName
Same	O
as	O
BASE	B-MethodName
except	O
that	O
the	O
CNNbased	O
encoders	O
for	O
questions	O
and	O
passages	O
were	O
replaced	O
with	O
the	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
BERT+AddTr	O
BERT	B-MethodName
,	O
which	O
used	O
both	O
W	O
hySet	O
and	O
AddT	O
r	O
as	O
its	O
training	O
data	O
.	O

On	O
top	O
of	O
BERT	B-MethodName
,	O
it	O
additionally	O
used	O
compact	O
-	O
answer	O
representation	O
produced	O
by	O
FOP	O
for	O
answer	B-TaskName
selection	I-TaskName
.	O

We	O
proposed	O
a	O
method	O
for	O
why	O
-	O
question	B-TaskName
answering	I-TaskName
(	O
why	O
-	O
QA	O
)	O
that	O
used	O
an	O
adversarial	O
learning	O
framework	O
.	O
It	O
employed	O
adversarial	O
learning	O
to	O
generate	O
vector	O
representations	O
of	O
reasons	O
or	O
true	O
answers	O
from	O
answer	O
passages	O
and	O
exploited	O
the	O
representations	O
for	O
judging	O
whether	O
the	O
passages	O
are	O
proper	O
answer	O
passages	O
to	O
the	O
given	O
whyquestions	O
.	O
Through	O
experiments	O
using	O
Japanese	O
why	O
-	O
QA	O
datasets	O
,	O
we	O
showed	O
that	O
this	O
idea	O
improved	O
why	O
-	O
QA	O
performance	O
.	O
We	O
also	O
showed	O
that	O
our	O
method	O
improved	O
the	O
performance	O
in	O
a	O
distantly	O
supervised	O
open	O
-	O
domain	O
QA	O
task	O
.	O
In	O
our	O
why	O
-	O
QA	O
method	O
,	O
causality	O
expressions	O
extracted	O
from	O
the	O
web	O
were	O
used	O
as	O
background	O
knowledge	O
for	O
computing	O
causality	O
-	O
attention	O
/	O
embeddings	O
.	O
As	O
a	O
future	O
work	O
,	O
we	O
plan	O
to	O
introduce	O
a	O
wider	O
range	O
of	O
background	O
knowledge	O
including	O
another	O
type	O
of	O
event	O
causality	O
(	O
Hashimoto	O
et	O
al	O
,	O
,	O
2014	O
(	O
Hashimoto	O
et	O
al	O
,	O
,	O
2015	O
.	O

Due	O
to	O
its	O
potential	O
applications	O
,	O
open	O
-	O
domain	O
dialogue	B-TaskName
generation	I-TaskName
has	O
become	O
popular	O
and	O
achieved	O
remarkable	O
progress	O
in	O
recent	O
years	O
,	O
but	O
sometimes	O
suffers	O
from	O
generic	O
responses	O
.	O
Previous	O
models	O
are	O
generally	O
trained	O
based	O
on	O
1	O
-	O
to	O
-	O
1	O
mapping	O
from	O
an	O
input	O
query	O
to	O
its	O
response	O
,	O
which	O
actually	O
ignores	O
the	O
nature	O
of	O
1	O
-	O
to	O
-	O
n	O
mapping	O
in	O
dialogue	O
that	O
there	O
may	O
exist	O
multiple	O
valid	O
responses	O
corresponding	O
to	O
the	O
same	O
query	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
to	O
utilize	O
the	O
multiple	O
references	O
by	O
considering	O
the	O
correlation	O
of	O
different	O
valid	O
responses	O
and	O
modeling	O
the	O
1	O
-	O
to	O
-	O
n	O
mapping	O
with	O
a	O
novel	O
two	O
-	O
step	O
generation	O
architecture	O
.	O
The	O
first	O
generation	O
phase	O
extracts	O
the	O
common	O
features	O
of	O
different	O
responses	O
which	O
,	O
combined	O
with	O
distinctive	O
features	O
obtained	O
in	O
the	O
second	O
phase	O
,	O
can	O
generate	O
multiple	O
diverse	O
and	O
appropriate	O
responses	O
.	O
Experimental	O
results	O
show	O
that	O
our	O
proposed	O
model	O
can	O
effectively	O
improve	O
the	O
quality	O
of	O
response	O
and	O
outperform	O
existing	O
neural	O
dialogue	O
models	O
on	O
both	O
automatic	O
and	O
human	O
evaluations	O
.	O

In	O
recent	O
years	O
,	O
open	O
-	O
domain	O
dialogue	B-TaskName
generation	I-TaskName
has	O
become	O
a	O
research	O
hotspot	O
in	O
Natural	O
Language	O
Processing	O
due	O
to	O
its	O
broad	O
application	O
prospect	O
,	O
including	O
chatbots	O
,	O
virtual	O
personal	O
assistants	O
,	O
etc	O
.	O
Though	O
plenty	O
of	O
systems	O
have	O
been	O
proposed	O
to	O
improve	O
the	O
quality	O
of	O
generated	O
responses	O
from	O
various	O
aspects	O
such	O
as	O
topic	O
,	O
persona	O
modeling	O
and	O
emotion	B-DatasetName
controlling	O
(	O
Zhou	O
et	O
al	O
,	O
2018b	O
)	O
,	O
most	O
of	O
these	O
recent	O
approaches	O
are	O
primarily	O
built	O
upon	O
the	O
sequence	O
-	O
to	O
-	O
sequence	O
architecture	O
Shang	O
et	O
al	O
,	O
2015	O
)	O
which	O
suffers	O
from	O
the	O
"	O
safe	O
"	O
response	O
problem	O
(	O
Li	O
et	O
al	O
,	O
2016a	O
;	O
Sato	O
et	O
al	O
,	O
2017	O
)	O
.	O
This	O
can	O
be	O
ascribed	O
to	O
modeling	O
the	O
response	B-TaskName
generation	I-TaskName
process	O
as	O
1to	O
-	O
1	O
mapping	O
,	O
which	O
ignores	O
the	O
nature	O
of	O
1	O
-	O
to	O
-	O
n	O
mapping	O
of	O
dialogue	O
that	O
multiple	O
possible	O
responses	O
can	O
correspond	O
to	O
the	O
same	O
query	O
.	O
To	O
deal	O
with	O
the	O
generic	O
response	O
problem	O
,	O
various	O
methods	O
have	O
been	O
proposed	O
,	O
including	O
diversity	O
-	O
promoting	O
objective	O
function	O
(	O
Li	O
et	O
al	O
,	O
2016a	O
)	O
,	O
enhanced	O
beam	O
search	O
(	O
Shao	O
et	O
al	O
,	O
2016	O
)	O
,	O
latent	O
dialogue	O
mechanism	O
(	O
Zhou	O
et	O
al	O
,	O
,	O
2018a	O
,	O
Variational	O
Autoencoders	B-MethodName
(	O
VAEs	O
)	O
based	O
models	O
Serban	O
et	O
al	O
,	O
2017	O
)	O
,	O
etc	O
.	O
However	O
,	O
these	O
methods	O
still	O
view	O
multiple	O
responses	O
as	O
independent	O
ones	O
and	O
fail	O
to	O
model	O
multiple	O
responses	O
jointly	O
.	O
Recently	O
,	O
Zhang	O
et	O
al	O
(	O
2018a	O
)	O
introduce	O
a	O
maximum	O
likelihood	O
strategy	O
that	O
given	O
an	O
input	O
query	O
,	O
the	O
most	O
likely	O
response	O
is	O
approximated	O
rather	O
than	O
all	O
possible	O
responses	O
,	O
which	O
is	O
further	O
implemented	O
by	O
Rajendran	O
et	O
al	O
(	O
2018	O
)	O
with	O
reinforcement	O
learning	O
for	O
task	O
-	O
oriented	O
dialogue	O
.	O
Although	O
capable	O
of	O
generating	O
the	O
most	O
likely	O
response	O
,	O
these	O
methods	O
fail	O
to	O
model	O
other	O
possible	O
responses	O
and	O
ignore	O
the	O
correlation	O
of	O
different	O
responses	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
response	B-TaskName
generation	I-TaskName
model	O
for	O
open	O
-	O
domain	O
conversation	O
,	O
which	O
learns	O
to	O
generate	O
multiple	O
diverse	O
responses	O
with	O
multiple	O
references	O
by	O
considering	O
the	O
correlation	O
of	O
different	O
responses	O
.	O
Our	O
motivation	O
lies	O
in	O
two	O
aspects	O
:	O
1	O
)	O
multiple	O
responses	O
for	O
a	O
query	O
are	O
likely	O
correlated	O
,	O
which	O
can	O
facilitate	O
building	O
the	O
dialogue	O
system	O
.	O
2	O
)	O
it	O
is	O
easier	O
to	O
model	O
each	O
response	O
based	O
on	O
other	O
responses	O
than	O
from	O
scratch	O
every	O
time	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
given	O
an	O
input	O
query	O
,	O
different	O
responses	O
may	O
share	O
some	O
common	O
features	O
e.g.	O
positive	O
attitudes	O
or	O
something	O
else	O
,	O
but	O
vary	O
in	O
discourses	O
or	O
expressions	O
which	O
we	O
refer	O
to	O
as	O
distinct	O
features	O
.	O
Accordingly	O
,	O
the	O
system	O
can	O
benefit	O
from	O
modeling	O
these	O
features	O
respectively	O
rather	O
than	O
learning	O
each	O
query	O
-	O
response	O
mapping	O
from	O
scratch	O
.	O
Inspired	B-DatasetName
by	O
this	O
idea	O
,	O
we	O
propose	O
a	O
two	O
-	O
step	O
dialogue	B-TaskName
generation	I-TaskName
architecture	O
as	O
follows	O
.	O
We	O
jointly	O
view	O
the	O
multiple	O
possible	O
responses	O
to	O
the	O
same	O
query	O
as	O
a	O
response	O
bag	O
.	O
In	O
the	O
first	O
generation	O
phase	O
,	O
the	O
common	O
feature	O
of	O
different	O
valid	O
responses	O
is	O
extracted	O
,	O
serving	O
as	O
a	O
base	O
from	O
which	O
each	O
specific	O
response	O
in	O
the	O
bag	O
is	O
further	O
approximated	O
.	O
The	O
system	O
then	O
,	O
in	O
the	O
second	O
generation	O
phase	O
,	O
learns	O
to	O
model	O
the	O
distinctive	O
feature	O
of	O
each	O
individual	O
response	O
which	O
,	O
combined	O
with	O
the	O
common	O
feature	O
,	O
can	O
generate	O
multiple	O
diverse	O
responses	O
simultaneously	O
.	O
Experimental	O
results	O
show	O
that	O
our	O
method	O
can	O
outperform	O
existing	O
competitive	O
neural	O
models	O
under	O
both	O
automatic	O
and	O
human	O
evaluation	O
metrics	O
,	O
which	O
demonstrates	O
the	O
effectiveness	O
of	O
the	O
overall	O
approach	O
.	O
We	O
also	O
provide	O
ablation	O
analyses	O
to	O
validate	O
each	O
component	O
of	O
our	O
model	O
.	O
To	O
summarize	O
,	O
our	O
contributions	O
are	O
threefold	O
:	O
We	O
propose	O
to	O
model	O
multiple	O
responses	O
to	O
a	O
query	O
jointly	O
by	O
considering	O
the	O
correlations	O
of	O
responses	O
with	O
multi	O
-	O
reference	O
learning	O
.	O
We	O
consider	O
the	O
common	O
and	O
distinctive	O
features	O
of	O
the	O
response	O
bag	O
and	O
propose	O
a	O
novel	O
two	O
-	O
step	O
dialogue	B-TaskName
generation	I-TaskName
architecture	O
.	O
Experiments	O
show	O
that	O
the	O
proposed	O
method	O
can	O
generate	O
multiple	O
diverse	O
responses	O
and	O
outperform	O
existing	O
competitive	O
models	O
on	O
both	O
automatic	O
and	O
human	O
evaluations	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
response	B-TaskName
generation	I-TaskName
model	O
for	O
short	B-TaskName
-	I-TaskName
text	I-TaskName
conversation	I-TaskName
,	O
which	O
models	O
multiple	O
valid	O
responses	O
for	O
a	O
given	O
query	O
jointly	O
.	O
We	O
posit	O
that	O
a	O
dialogue	O
system	O
can	O
benefit	O
from	O
multi	O
-	O
reference	O
learning	O
by	O
considering	O
the	O
correlation	O
of	O
multiple	O
responses	O
.	O
Figure	O
2	O
demonstrates	O
the	O
whole	O
architecture	O
of	O
our	O
model	O
.	O
We	O
now	O
describe	O
the	O
details	O
as	O
follows	O
.	O

Training	O
samples	O
{	O
(	O
x	O
,	O
{	O
y	O
}	O
)	O
i	O
}	O
i	O
=	O
N	O
i=1	O
consist	O
of	O
each	O
query	O
x	O
and	O
the	O
set	O
of	O
its	O
valid	O
responses	O
{	O
y	O
}	O
,	O
where	O
N	O
denotes	O
the	O
number	O
of	O
training	O
samples	O
.	O
For	O
a	O
dialogue	B-TaskName
generation	I-TaskName
model	O
,	O
it	O
aims	O
to	O
map	O
from	O
the	O
input	O
query	O
x	O
to	O
the	O
output	O
response	O
y	O
{	O
y	O
}	O
.	O
To	O
achieve	O
this	O
,	O
different	O
from	O
conventional	O
methods	O
which	O
view	O
the	O
multiple	O
responses	O
as	O
independent	O
ones	O
,	O
we	O
propose	O
to	O
consider	O
the	O
correlation	O
of	O
multiple	O
responses	O
with	O
a	O
novel	O
twostep	O
generation	O
architecture	O
,	O
where	O
the	O
response	O
bag	O
{	O
y	O
}	O
and	O
each	O
response	O
y	O
{	O
y	O
}	O
are	O
modeled	O
by	O
two	O
separate	O
features	O
which	O
are	O
obtained	O
in	O
each	O
generation	O
phase	O
respectively	O
.	O
Specifically	O
,	O
we	O
assume	O
a	O
variable	O
c	O
R	O
n	O
representing	O
the	O
common	O
feature	O
of	O
different	O
responses	O
and	O
an	O
unobserved	O
latent	O
variable	O
z	O
Z	O
corresponding	O
to	O
the	O
distinct	O
feature	O
for	O
each	O
y	O
in	O
the	O
bag	O
.	O
The	O
com	O
-	O
mon	O
feature	O
c	O
is	O
generated	O
in	O
the	O
first	O
stage	O
given	O
x	O
and	O
the	O
distinctive	O
feature	O
z	O
is	O
sampled	O
from	O
the	O
latent	O
space	O
Z	O
in	O
the	O
second	O
stage	O
given	O
the	O
query	O
x	O
and	O
common	O
feature	O
c.	O
The	O
final	O
responses	O
are	O
then	O
generated	O
conditioned	O
on	O
both	O
the	O
common	O
feature	O
c	O
and	O
distinct	O
feature	O
z	O
simultaneously	O
.	O

Focusing	O
on	O
open	O
-	O
domain	O
dialogue	O
,	O
we	O
perform	O
experiments	O
on	O
a	O
large	O
-	O
scale	O
single	O
-	O
turn	O
conversation	O
dataset	O
Weibo	B-DatasetName
(	O
Shang	O
et	O
al	O
,	O
2015	O
)	O
,	O
where	O
each	O
input	O
post	O
is	O
generally	O
associated	O
with	O
multiple	O
response	O
utterances	O
2	O
.	O
Concretely	O
,	O
the	O
Weibo	B-DatasetName
dataset	O
consists	O
of	O
short	O
-	O
text	O
online	O
chit	O
-	O
chat	O
dialogues	O
in	O
Chinese	O
,	O
which	O
is	O
crawled	O
from	O
Sina	O
Weibo	B-DatasetName
3	O
.	O
Totally	O
,	O
there	O
are	O
4	O
,	O
423	O
,	O
160	O
queryresponse	O
pairs	O
for	O
training	O
set	O
and	O
10000	O
pairs	O
for	O
the	O
validation	O
and	O
testing	O
,	O
where	O
there	O
are	O
around	O
200k	O
unique	O
query	O
in	O
the	O
training	O
set	O
and	O
each	O
query	O
used	O
in	O
testing	O
correlates	O
with	O
four	O
responses	O
respectively	O
.	O
For	O
preprocessing	O
,	O
we	O
follow	O
the	O
conventional	O
settings	O
(	O
Shang	O
et	O
al	O
,	O
2015	O
)	O
.	O

Table	O
3	O
illustrates	O
two	O
examples	O
of	O
generated	O
replies	O
to	O
the	O
input	O
query	O
got	O
from	O
the	O
testing	O
set	O
.	O
Comparing	O
the	O
CVAE	B-MethodName
and	O
Ours	O
,	O
we	O
can	O
find	O
that	O
although	O
the	O
CVAE	B-MethodName
model	O
can	O
generate	O
diverse	O
utterances	O
,	O
its	O
responses	O
tend	O
to	O
be	O
irrelevant	O
to	O
the	O
query	O
and	O
sometimes	O
not	O
grammatically	O
formed	O
,	O
e.g.	O
the	O
words	O
"	O
glowworm	O
"	O
and	O
"	O
robot	O
"	O
in	O
the	O
sentences	O
.	O
In	O
contrast	O
,	O
responses	O
generated	O
by	O
our	O
model	O
show	O
better	O
quality	O
,	O
achieving	O
both	O
high	O
relevance	O
and	O
diversity	O
.	O
This	O
demonstrates	O
the	O
ability	O
of	O
the	O
two	O
-	O
step	O
generation	O
architecture	O
.	O
For	O
better	O
insight	O
into	O
the	O
procedure	O
,	O
we	O
present	O
the	O
intermediately	O
generated	O
utterances	O
which	O
show	O
that	O
the	O
feature	O
extracted	O
in	O
the	O
first	O
stage	O
can	O
focus	O
on	O
some	O
common	O
and	O
key	O
aspects	O
of	O
the	O
query	O
and	O
its	O
possible	O
responses	O
,	O
such	O
as	O
the	O
"	O
amazing	O
"	O
and	O
"	O
software	O
"	O
.	O
With	O
the	O
distinctive	O
features	O
sampled	O
in	O
the	O
second	O
generation	O
phase	O
,	O
the	O
model	O
further	O
revises	O
the	O
response	O
and	O
outputs	O
multiple	O
responses	O
with	O
diverse	O
contents	O
and	O
expressions	O
.	O
Recap	O
that	O
the	O
common	O
feature	O
is	O
expected	O
to	O
capture	O
the	O
correlations	O
of	O
different	O
responses	O
and	O
serve	O
as	O
the	O
base	O
of	O
a	O
response	O
bag	O
from	O
which	O
different	O
responses	O
are	O
further	O
generated	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
.	O
To	O
investigate	O
the	O
actual	O
performances	O
achieved	O
by	O
our	O
model	O
,	O
we	O
compute	O
the	O
distance	O
between	O
the	O
input	O
query	O
/	O
intermediate	O
utterance	O
and	O
gold	O
references	O
/	O
generated	O
responses	O
and	O
present	O
the	O
results	O
in	O
Figure	O
4	O
.	O
As	O
shown	O
,	O
intermediate	O
utterances	O
obtained	O
in	O
the	O
first	O
generation	O
phase	O
tend	O
to	O
approximate	O
multiple	O
responses	O
with	O
similar	O
distances	O
at	O
the	O
same	O
time	O
.	O
Comparing	O
the	O
generated	O
responses	O
and	O
the	O
references	O
,	O
we	O
find	O
that	O
generated	O
responses	O
show	O
both	O
high	O
relevant	O
and	O
irrelevant	O
ratios	O
,	O
as	O
the	O
values	O
near	O
0.00	O
and	O
1.00	O
show	O
.	O
This	O
actually	O
agrees	O
well	O
with	O
our	O
observation	O
that	O
the	O
model	O
may	O
sometimes	O
rely	O
heavily	O
on	O
or	O
ignore	O
the	O
prior	O
common	O
feature	O
information	O
.	O
From	O
a	O
further	O
comparison	O
between	O
the	O
input	O
query	O
and	O
the	O
mid	O
,	O
we	O
also	O
observe	O
that	O
the	O
intermediate	O
utterance	O
is	O
more	O
similar	O
to	O
final	O
responses	O
than	O
the	O
input	O
query	O
,	O
which	O
correlates	O
well	O
with	O
our	O
original	O
intention	O
shown	O
in	O
Figure	O
1	O
.	O

This	O
paper	O
introduces	O
SGNMT	O
,	O
our	O
experimental	O
platform	O
for	O
machine	B-TaskName
translation	I-TaskName
research	O
.	O
SGNMT	O
provides	O
a	O
generic	O
interface	O
to	O
neural	O
and	O
symbolic	O
scoring	O
modules	O
(	O
predictors	O
)	O
with	O
left	O
-	O
to	O
-	O
right	O
semantic	O
such	O
as	O
translation	O
models	O
like	O
NMT	O
,	O
language	O
models	O
,	O
translation	O
lattices	O
,	O
n	O
-	O
best	O
lists	O
or	O
other	O
kinds	O
of	O
scores	O
and	O
constraints	O
.	O
Predictors	O
can	O
be	O
combined	O
with	O
other	O
predictors	O
to	O
form	O
complex	O
decoding	O
tasks	O
.	O
SGNMT	O
implements	O
a	O
number	O
of	O
search	O
strategies	O
for	O
traversing	O
the	O
space	O
spanned	O
by	O
the	O
predictors	O
which	O
are	O
appropriate	O
for	O
different	O
predictor	O
constellations	O
.	O
Adding	O
new	O
predictors	O
or	O
decoding	O
strategies	O
is	O
particularly	O
easy	O
,	O
making	O
it	O
a	O
very	O
efficient	O
tool	O
for	O
prototyping	O
new	O
research	O
ideas	O
.	O
SGNMT	O
is	O
actively	O
being	O
used	O
by	O
students	O
in	O
the	O
MPhil	O
program	O
in	O
Machine	O
Learning	O
,	O
Speech	O
and	O
Language	O
Technology	O
at	O
the	O
University	O
of	O
Cambridge	B-DatasetName
for	O
course	O
work	O
and	O
theses	O
,	O
as	O
well	O
as	O
for	O
most	O
of	O
the	O
research	O
work	O
in	O
our	O
group	O
.	O

We	O
are	O
developing	O
an	O
open	O
source	O
decoding	O
framework	O
called	O
SGNMT	O
,	O
short	O
for	O
Syntactically	O
Guided	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName
.	O
1	O
The	O
software	O
package	O
supports	O
a	O
number	O
of	O
well	O
-	O
known	O
frameworks	O
,	O
including	O
TensorFlow	O
2	O
(	O
Abadi	O
et	O
al	O
,	O
2016	O
)	O
,	O
OpenFST	O
(	O
Allauzen	O
et	O
al	O
,	O
2007	O
)	O
,	O
Blocks	O
/	O
Theano	O
(	O
Bastien	O
et	O
al	O
,	O
2012	O
;	O
van	O
Merriënboer	O
et	O
al	O
,	O
2015	O
)	O
,	O
and	O
NPLM	O
(	O
Vaswani	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
two	O
central	O
concepts	O
in	O
the	O
SGNMT	O
tool	O
are	O
predictors	O
and	O
decoders	O
.	O
Predictors	O
are	O
scoring	O
modules	O
which	O
define	O
scores	O
over	O
the	O
target	O
language	O
vocabulary	O
given	O
the	O
current	O
internal	O
predictor	O
state	O
,	O
the	O
history	O
,	O
the	O
source	O
sentence	O
,	O
and	O
external	O
side	O
information	O
.	O
Scores	O
from	O
multiple	O
,	O
diverse	O
predictors	O
can	O
be	O
combined	O
for	O
use	O
in	O
decoding	O
.	O
Decoders	O
are	O
search	O
strategies	O
which	O
traverse	O
the	O
space	O
spanned	O
by	O
the	O
predictors	O
.	O
SGNMT	O
provides	O
implementations	O
of	O
common	O
search	O
tree	O
traversal	O
algorithms	O
like	O
beam	O
search	O
.	O
Since	O
decoders	O
differ	O
in	O
runtime	O
complexity	O
and	O
the	O
kind	O
of	O
search	O
errors	O
they	O
make	O
,	O
different	O
decoders	O
are	O
appropriate	O
for	O
different	O
predictor	O
constellations	O
.	O
The	O
strict	O
separation	O
of	O
scoring	O
module	O
and	O
search	O
strategy	O
and	O
the	O
decoupling	O
of	O
scoring	O
modules	O
from	O
each	O
other	O
makes	O
SGNMT	O
a	O
very	O
flexible	O
decoding	O
tool	O
for	O
neural	O
and	O
symbolic	O
models	O
which	O
is	O
applicable	O
not	O
only	O
to	O
machine	B-TaskName
translation	I-TaskName
.	O
SGNMT	O
is	O
based	O
on	O
the	O
OpenFSTbased	O
Cambridge	B-DatasetName
SMT	O
system	O
(	O
Allauzen	O
et	O
al	O
,	O
2014	O
)	O
.	O
Although	O
the	O
system	O
is	O
less	O
than	O
a	O
year	O
old	O
,	O
we	O
have	O
found	O
it	O
to	O
be	O
very	O
flexible	O
and	O
easy	O
for	O
new	O
researchers	O
to	O
adopt	O
.	O
Our	O
group	O
has	O
already	O
integrated	O
SGNMT	O
into	O
most	O
of	O
its	O
research	O
work	O
.	O
We	O
also	O
find	O
that	O
SGNMT	O
is	O
very	O
well	O
-	O
suited	O
for	O
teaching	O
and	O
student	O
research	O
projects	O
.	O
In	O
the	O
2015	O
-	O
16	O
academic	O
year	O
,	O
two	O
students	O
on	O
the	O
Cambridge	B-DatasetName
MPhil	O
in	O
Machine	O
Learning	O
,	O
Speech	O
and	O
Language	O
Technology	O
used	O
SGNMT	O
for	O
their	O
dissertation	O
projects	O
.	O
3	O
The	O
first	O
project	O
involved	O
using	O
SGNMT	O
with	O
OpenFST	O
for	O
applying	O
subword	O
models	O
in	O
SMT	O
(	O
Gao	O
,	O
2016	O
)	O
.	O
The	O
second	O
project	O
developed	O
automatic	O
music	O
composition	O
by	O
LSTMs	O
where	O
WFSAs	O
were	O
used	O
to	O
define	O
the	O
space	O
of	O
allowable	O
chord	O
progressions	O
in	O
'	O
Bach	O
'	O
chorales	O
(	O
Tomczak	O
,	O
2016	O
that	O
the	O
chorales	O
must	O
obey	O
.	O
This	O
second	O
project	O
in	O
particular	O
demonstrates	O
the	O
versatility	O
of	O
the	O
approach	O
.	O
For	O
the	O
current	O
,	O
2016	O
-	O
17	O
academic	O
year	O
,	O
SGNMT	O
is	O
being	O
used	O
heavily	O
in	O
two	O
courses	O
.	O

SGNMT	O
consequently	O
emphasizes	O
flexibility	O
and	O
extensibility	O
by	O
providing	O
a	O
common	O
interface	O
to	O
a	O
wide	O
range	O
of	O
constraints	O
or	O
models	O
used	O
in	O
MT	O
research	O
.	O
The	O
concept	O
facilitates	O
quick	O
prototyping	O
of	O
new	O
research	O
ideas	O
.	O
Our	O
platform	O
aims	O
to	O
minimize	O
the	O
effort	O
required	O
for	O
implementation	O
;	O
decoding	O
speed	O
is	O
secondary	O
as	O
optimized	O
code	O
for	O
production	O
systems	O
can	O
be	O
produced	O
once	O
an	O
idea	O
has	O
been	O
proven	O
successful	O
in	O
the	O
SGNMT	O
framework	O
.	O
In	O
SGNMT	O
,	O
scores	O
are	O
assigned	O
to	O
partial	O
hypotheses	O
via	O
one	O
or	O
many	O
predictors	O
.	O
One	O
predictor	O
usually	O
has	O
a	O
single	O
responsibility	O
as	O
it	O
represents	O
a	O
single	O
model	O
or	O
type	O
of	O
constraint	O
.	O
Predictors	O
need	O
to	O
implement	O
the	O
following	O
methods	O
:	O
initialize	O
(	O
src	O
sentence	O
)	O
Initialize	O
the	O
predictor	O
state	O
using	O
the	O
source	O
sentence	O
.	O
get	O
state	O
(	O
)	O
Get	O
the	O
internal	O
predictor	O
state	O
.	O
set	O
state	O
(	O
state	O
)	O
Set	O
the	O
internal	O
predictor	O
state	O
.	O
predict	O
next	O
(	O
)	O
Given	O
the	O
internal	O
predictor	O
state	O
,	O
produce	O
the	O
posterior	O
over	O
target	O
tokens	O
for	O
the	O
next	O
position	O
.	O
Predictor	O
Description	O
nmt	O
Attention	O
-	O
based	O
neural	O
machine	B-TaskName
translation	I-TaskName
following	O
.	O
Supports	O
Blocks	O
/	O
Theano	O
(	O
Bastien	O
et	O
al	O
,	O
2012	O
;	O
van	O
Merriënboer	O
et	O
al	O
,	O
2015	O
)	O
and	O
TensorFlow	O
(	O
Abadi	O
et	O
al	O
,	O
2016	O
)	O
.	O
fst	O
Predictor	O
for	O
rescoring	O
deterministic	O
lattices	O
(	O
Heafield	O
et	O
al	O
,	O
2013	O
;	O
Stolcke	O
et	O
al	O
,	O
2002	O
)	O
toolkit	O
.	O
nplm	O
Neural	O
n	O
-	O
gram	O
language	O
models	O
based	O
on	O
NPLM	O
(	O
Vaswani	O
et	O
al	O
,	O
2013	O
)	O
.	O
rnnlm	O
Integrates	O
RNN	O
language	O
models	O
with	O
TensorFlow	O
as	O
described	O
by	O
Zaremba	O
et	O
al	O
(	O
2014	O
)	O
.	O
forced	O
Forced	O
decoding	O
with	O
a	O
single	O
reference	O
.	O
forcedlst	O
n	O
-	O
best	O
list	O
rescoring	O
.	O
bow	O
Restricts	O
the	O
search	O
space	O
to	O
a	O
bag	O
of	O
words	O
with	O
or	O
without	O
repetition	O
consume	O
(	O
token	O
)	O
Update	O
the	O
internal	O
predictor	O
state	O
by	O
adding	O
token	O
to	O
the	O
current	O
history	O
.	O
The	O
structure	O
of	O
the	O
predictor	O
state	O
and	O
the	O
implementations	O
of	O
these	O
methods	O
differ	O
substantially	O
between	O
predictors	O
.	O
Tab	O
.	O
2	O
lists	O
all	O
predictors	O
which	O
are	O
currently	O
implemented	O
.	O
Tab	O
.	O
1	O
summarizes	O
the	O
semantics	O
of	O
this	O
interface	O
for	O
three	O
very	O
common	O
predictors	O
:	O
the	O
neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
predictor	O
,	O
the	O
(	O
deterministic	O
)	O
finite	O
state	O
transducer	O
(	O
FST	O
)	O
predictor	O
for	O
lattice	O
rescoring	O
,	O
and	O
the	O
n	O
-	O
gram	O
predictor	O
for	O
applying	O
n	O
-	O
gram	O
language	O
models	O
.	O
We	O
also	O
included	O
two	O
examples	O
(	O
word	O
count	O
and	O
UNK	O
count	O
)	O
which	O
do	O
not	O
have	O
a	O
natural	O
left	O
-	O
to	O
-	O
right	O
semantic	O
but	O
can	O
still	O
be	O
represented	O
as	O
predictors	O
.	O

Recent	O
studies	O
show	O
that	O
NLP	O
models	O
are	O
vulnerable	O
to	O
adversarial	O
perturbations	O
.	O
A	O
seemingly	O
"	O
invariance	O
transformation	O
"	O
(	O
a.k.a	O
.	O
adversarial	O
perturbation	O
)	O
such	O
as	O
synonym	O
substitutions	O
(	O
Alzantot	O
et	O
al	O
,	O
2018	O
;	O
Zang	O
et	O
al	O
,	O
2020	O
)	O
or	O
syntax	O
-	O
guided	O
paraphrasing	O
(	O
Iyyer	O
et	O
al	O
,	O
2018	O
;	O
Huang	O
and	O
Chang	O
,	O
2021	O
)	O
can	O
alter	O
the	O
prediction	O
.	O
To	O
mitigate	O
the	O
model	O
vulnerability	O
,	O
robust	O
training	O
methods	O
have	O
been	O
proposed	O
and	O
shown	O
effective	O
(	O
Miyato	O
et	O
al	O
,	O
2017	O
;	O
Jia	O
et	O
al	O
,	O
2019	O
;	O
Huang	O
et	O
al	O
,	O
2019	O
;	O
Zhou	O
et	O
al	O
,	O
2020	O
)	O
.	O
x	O
0	B-DatasetName
=	O
"	O
a	O
deep	O
and	O
meaningful	O
film	O
(	O
movie	O
)	O
.	O
"	O

x	O
0	B-DatasetName
=	O
"	O
a	O
short	O
and	O
moving	O
film	O
(	O
movie	O
)	O
.	O
"	O
73	O
%	O
positive	O
(	O
70	O
%	O
negative	O
)	O
(	O
99	O
%	O
positive	O
)	O
99	O
%	O
positive	O
perturb	O
Figure	O
1	O
:	O
A	O
vulnerable	O
example	O
beyond	O
the	O
test	O
dataset	O
.	O
Numbers	O
on	O
the	O
bottom	O
right	O
are	O
the	O
sentiment	O
predictions	O
for	O
film	O
and	O
movie	O
.	O
Blue	O
x	O
0	B-DatasetName
comes	O
from	O
the	O
test	O
dataset	O
and	O
its	O
prediction	O
can	O
not	O
be	O
altered	O
by	O
the	O
substitution	O
film	O
movie	O
(	O
robust	O
)	O
.	O
Yellow	O
examplex	O
0	B-DatasetName
is	O
slightly	O
perturbed	O
but	O
remains	O
natural	O
.	O
Its	O
prediction	O
can	O
be	O
altered	O
by	O
the	O
substitution	O
(	O
vulnerable	O
)	O
.	O
In	O
most	O
studies	O
,	O
model	O
robustness	O
is	O
evaluated	O
based	O
on	O
a	O
given	O
test	O
dataset	O
or	O
synthetic	O
sentences	O
constructed	O
from	O
templates	O
(	O
Ribeiro	O
et	O
al	O
,	O
2020	O
)	O
.	O
Specifically	O
,	O
the	O
robustness	O
of	O
a	O
model	O
is	O
often	O
evaluated	O
by	O
the	O
ratio	O
of	O
test	O
examples	O
where	O
the	O
model	O
prediction	O
can	O
not	O
be	O
altered	O
by	O
semantic	O
-	O
invariant	O
perturbation	O
.	O
We	O
refer	O
to	O
this	O
type	O
of	O
evaluations	O
as	O
the	O
first	O
-	O
order	O
robustness	O
evaluation	O
.	O
However	O
,	O
even	O
if	O
a	O
model	O
is	O
first	O
-	O
order	O
robust	O
on	O
an	O
input	O
sentence	O
x	O
0	B-DatasetName
,	O
it	O
is	O
possible	O
that	O
the	O
model	O
is	O
not	O
robust	O
on	O
a	O
natural	O
sentencex	O
0	B-DatasetName
that	O
is	O
slightly	O
modified	O
from	O
x	O
0	B-DatasetName
.	O
In	O
that	O
case	O
,	O
adversarial	O
examples	O
still	O
exist	O
even	O
if	O
first	O
-	O
order	O
attacks	O
can	O
not	O
find	O
any	O
of	O
them	O
from	O
the	O
given	O
test	O
dataset	O
.	O
Throughout	O
this	O
paper	O
,	O
we	O
callx	O
0	B-DatasetName
a	O
vulnerable	O
example	O
.	O
The	O
existence	O
of	O
such	O
examples	O
exposes	O
weaknesses	O
in	O
models	O
'	O
understanding	O
and	O
presents	O
challenges	O
for	O
model	O
deployment	O
.	O
Fig	O
.	O
1	O
illustrates	O
an	O
example	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
the	O
double	O
perturbation	O
framework	O
for	O
evaluating	O
a	O
stronger	O
notion	O
of	O
second	O
-	O
order	O
robustness	O
.	O
Given	O
a	O
test	O
dataset	O
,	O
we	O
consider	O
a	O
model	O
to	O
be	O
second	O
-	O
order	O
robust	O
if	O
there	O
is	O
no	O
vulnerable	O
example	O
that	O
can	O
be	O
identified	O
in	O
the	O
neighborhood	O
of	O
given	O
test	O
instances	O
(	O
2.2	O
)	O
.	O
In	O
particular	O
,	O
our	O
framework	O
first	O
perturbs	O
the	O
test	O
set	O
to	O
construct	O
the	O
neighborhood	O
,	O
and	O
then	O
diagnoses	O
the	O
robustness	O
regarding	O
a	O
single	O
-	O
word	O
synonym	O
substitution	O
.	O
Taking	O
Fig	O
.	O
2	O
as	O
an	O
example	O
,	O
the	O
model	O
is	O
first	O
-	O
order	O
robust	O
on	O
the	O
input	O
sentence	O
x	O
0	B-DatasetName
(	O
the	O
prediction	O
can	O
not	O
be	O
altered	O
)	O
,	O
but	O
it	O
is	O
not	O
second	O
-	O
order	O
robust	O
due	O
to	O
the	O
existence	O
of	O
the	O
vulnerable	O
examplex	O
0	B-DatasetName
.	O
Our	O
framework	O
is	O
designed	O
to	O
identifyx	O
0	B-DatasetName
.	O
We	O
apply	O
the	O
proposed	O
framework	O
and	O
quantify	O
second	O
-	O
order	O
robustness	O
through	O
two	O
second	O
-	O
order	O
attacks	O
(	O
3	O
)	O
.	O
We	O
experiment	O
with	O
English	O
sentiment	O
classification	O
on	O
the	O
SST	B-DatasetName
-	O
2	O
dataset	O
(	O
Socher	O
et	O
al	O
,	O
2013	O
)	O
across	O
various	O
model	O
architectures	O
.	O
Surprisingly	O
,	O
although	O
robustly	O
trained	O
CNN	O
(	O
Jia	O
et	O
al	O
,	O
2019	O
)	O
and	O
Transformer	B-MethodName
(	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
can	O
achieve	O
high	O
robustness	O
under	O
strong	O
attacks	O
(	O
Alzantot	O
et	O
al	O
,	O
2018	O
;	O
Garg	O
and	O
Ramakrishnan	O
,	O
2020	O
)	O
(	O
23.0	O
%	O
-	O
71.6	O
%	O
success	O
rates	O
)	O
,	O
for	O
around	O
96.0	O
%	O
of	O
the	O
test	O
examples	O
our	O
attacks	O
can	O
find	O
a	O
vulnerable	O
example	O
by	O
perturbing	O
1.3	O
words	O
on	O
average	O
.	O
This	O
finding	O
indicates	O
that	O
these	O
robustly	O
trained	O
models	O
,	O
despite	O
being	O
first	O
-	O
order	O
robust	O
,	O
are	O
not	O
second	O
-	O
order	O
robust	O
.	O
Furthermore	O
,	O
we	O
extend	O
the	O
double	O
perturbation	O
framework	O
to	O
evaluate	O
counterfactual	O
biases	O
(	O
Kusner	O
et	O
al	O
,	O
2017	O
)	O
(	O
4	O
)	O
in	O
English	O
.	O
When	O
the	O
test	O
dataset	O
is	O
small	O
,	O
our	O
framework	O
can	O
help	O
improve	O
the	O
evaluation	O
robustness	O
by	O
revealing	O
the	O
hidden	O
biases	O
not	O
directly	O
shown	O
in	O
the	O
test	O
dataset	O
.	O
Intuitively	O
,	O
a	O
fair	O
model	O
should	O
make	O
the	O
same	O
prediction	O
for	O
nearly	O
identical	O
examples	O
referencing	O
different	O
groups	O
(	O
Garg	O
et	O
al	O
,	O
2019	O
)	O
with	O
different	O
protected	O
attributes	O
(	O
e.g.	O
,	O
gender	O
,	O
race	O
)	O
.	O
In	O
our	O
evaluation	O
,	O
we	O
consider	O
a	O
model	O
biased	O
if	O
substituting	O
tokens	O
associated	O
with	O
protected	O
attributes	O
changes	O
the	O
expected	O
prediction	O
,	O
which	O
is	O
the	O
average	O
prediction	O
among	O
all	O
examples	O
within	O
the	O
neighborhood	O
.	O
For	O
instance	O
,	O
a	O
toxicity	O
classifier	O
is	O
biased	O
if	O
it	O
tends	O
to	O
increase	O
the	O
toxicity	O
if	O
we	O
substitute	O
straight	O
gay	O
in	O
an	O
input	O
sentence	O
(	O
Dixon	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
the	O
experiments	O
,	O
we	O
evaluate	O
the	O
expected	O
sentiment	O
predictions	O
on	O
pairs	O
of	O
protected	O
tokens	O
(	O
e.g.	O
,	O
(	O
he	O
,	O
she	O
)	O
,	O
(	O
gay	O
,	O
straight	O
)	O
)	O
,	O
and	O
demonstrate	O
that	O
our	O
method	O
is	O
able	O
to	O
reveal	O
the	O
hidden	O
model	O
biases	O
.	O
Our	O
main	O
contributions	O
are	O
:	O
(	O
1	O
)	O
We	O
propose	O
the	O
double	O
perturbation	O
framework	O
to	O
diagnose	O
the	O
robustness	O
of	O
existing	O
robustness	O
and	O
fairness	O
evaluation	O
methods	O
.	O
(	O
2	O
)	O
We	O
propose	O
two	O
second	O
-	O
order	O
attacks	O
to	O
quantify	O
the	O
stronger	O
notion	O
of	O
second	O
-	O
x	O
0x	O
0	B-DatasetName
x	O
0	B-DatasetName
x	O
1	O
negative	O
positive	O
Figure	O
2	O
:	O
An	O
illustration	O
of	O
the	O
decision	O
boundary	O
.	O
Diamond	O
area	O
denotes	O
invariance	O
transformations	O
.	O
Blue	O
x	O
0	B-DatasetName
is	O
a	O
robust	O
input	O
example	O
(	O
the	O
entire	O
diamond	O
is	O
green	O
)	O
.	O
Yellowx	O
0	B-DatasetName
is	O
a	O
vulnerable	O
example	O
in	O
the	O
neighborhood	O
of	O
x	O
0	B-DatasetName
.	O
Redx	O
0	B-DatasetName
is	O
an	O
adversarial	O
example	O
tox	O
0	B-DatasetName
.	O
Note	O
:	O
x	O
0	B-DatasetName
is	O
not	O
an	O
adversarial	O
example	O
to	O
x	O
0	B-DatasetName
since	O
they	O
have	O
different	O
meanings	O
to	O
human	O
(	O
outside	O
the	O
diamond	O
)	O
.	O
order	O
robustness	O
and	O
reveal	O
the	O
models	O
'	O
vulnerabilities	O
that	O
can	O
not	O
be	O
identified	O
by	O
previous	O
attacks	O
.	O
(	O
3	O
)	O
We	O
propose	O
a	O
counterfactual	O
bias	O
evaluation	O
method	O
to	O
reveal	O
the	O
hidden	O
model	O
bias	O
based	O
on	O
our	O
double	O
perturbation	O
framework	O
.	O

We	O
focus	O
our	O
study	O
on	O
word	O
-	O
level	O
substitution	O
,	O
where	O
existing	O
works	O
evaluate	O
robustness	O
and	O
counterfactual	O
bias	O
by	O
directly	O
perturbing	O
the	O
test	O
dataset	O
.	O
For	O
instance	O
,	O
adversarial	O
attacks	O
alter	O
the	O
prediction	O
by	O
making	O
synonym	O
substitutions	O
,	O
and	O
the	O
fairness	O
literature	O
evaluates	O
counterfactual	O
fairness	O
by	O
substituting	O
protected	O
tokens	O
.	O
We	O
integrate	O
the	O
word	O
substitution	O
strategy	O
into	O
our	O
framework	O
as	O
the	O
component	O
for	O
evaluating	O
robustness	O
and	O
fairness	O
.	O
For	O
simplicity	O
,	O
we	O
consider	O
a	O
single	O
-	O
word	O
substitution	O
and	O
denote	O
it	O
with	O
the	O
operator	O
.	O
Let	O
X	O
⊆	O
V	O
l	O
be	O
the	O
input	O
space	O
where	O
V	O
is	O
the	O
vocabulary	O
and	O
l	O
is	O
the	O
sentence	O
length	O
,	O
p	O
=	O
(	O
p	O
(	O
1	O
)	O
,	O
p	O
(	O
2	O
)	O
)	O
V	O
2	O
be	O
a	O
pair	O
of	O
synonyms	O
(	O
called	O
patch	O
words	O
)	O
,	O
X	O
p	O
⊆	O
X	O
denotes	O
sentences	O
with	O
a	O
single	O
occurrence	O
of	O
p	O
(	O
1	O
)	O
(	O
for	O
simplicity	O
we	O
skip	O
other	O
sentences	O
)	O
,	O
x	O
0	B-DatasetName
X	O
p	O
be	O
an	O
input	O
sentence	O
,	O
then	O
x	O
0	B-DatasetName
p	O
means	O
"	O
substitute	O
p	O
(	O
1	O
)	O
p	O
(	O
2	O
)	O
in	O
x	O
0	B-DatasetName
"	O
.	O
The	O
result	O
after	O
substitution	O
is	O
:	O
x	O
0	B-DatasetName
=	O
x	O
0	B-DatasetName
p.	O
Taking	O
Fig	O
.	O
1	O
as	O
an	O
example	O
,	O
where	O
p	O
=	O
(	O
film	O
,	O
movie	O
)	O
and	O
x	O
0	B-DatasetName
=	O
a	O
deep	O
and	O
meaningful	O
film	O
,	O
the	O
perturbed	O
sentence	O
is	O
x	O
0	B-DatasetName
=	O
a	O
deep	O
and	O
meaningful	O
movie	O
.	O
Now	O
we	O
introduce	O
other	O
components	O
in	O
our	O
framework	O
.	O

Second	O
-	O
order	O
attacks	O
study	O
the	O
prediction	O
difference	O
caused	O
by	O
applying	O
p.	O
For	O
notation	O
convenience	O
we	O
define	O
the	O
prediction	O
difference	O
F	O
(	O
x	O
;	O
p	O
)	O
:	O
x0	O
=	O
a	O
deep	O
and	O
meaningful	O
film	O
.	O
p	O
=	O
film	O
,	O
movie	O
x	O
(	O
i	O
=	O
2	O
)	O
a	O
short	O
and	O
moving	O
film	O
(	O
movie	O
)	O
.	O
a	O
slow	O
and	O
moving	O
film	O
(	O
movie	O
)	O
.	O
a	O
dramatic	O
or	O
meaningful	O
film	O
(	O
movie	O
)	O
.	O
p	O
alters	O
the	O
prediction	O
.	O
x0	O
=	O
"	O
a	O
short	O
and	O
moving	O
film	O
(	O
movie	O
)	O
.	O
"	O
(	O
70	O
%	O
negative	O
)	O
73	O
%	O
positive	O
Figure	O
3	O
:	O
The	O
attack	O
flow	O
for	O
SO	O
-	O
Beam	O
(	O
Algorithm	O
2	O
)	O
.	O
Blue	O
x	O
0	B-DatasetName
is	O
the	O
input	O
sentence	O
and	O
yellowx	O
0	B-DatasetName
is	O
our	O
constructed	O
vulnerable	O
example	O
(	O
the	O
prediction	O
can	O
be	O
altered	O
by	O
substituting	O
film	O
movie	O
)	O
.	O
Green	O
boxes	O
in	O
the	O
middle	O
show	O
intermediate	O
sentences	O
,	O
and	O
f	O
soft	O
(	O
x	O
)	O
denotes	O
the	O
probability	O
outputs	O
for	O
film	O
and	O
movie	O
.	O
X	O
×	O
V	O
2	O
{	O
−1	O
,	O
0	B-DatasetName
,	O
1	O
}	O
by	O
:	O
3	O
F	O
(	O
x	O
;	O
p	O
)	O
:	O
=	O
f	O
(	O
x	O
p	O
)	O
−	O
f	O
(	O
x	O
)	O
.	O
(	O
2	O
)	O
Taking	O
Fig	O
.	O
1	O
as	O
an	O
example	O
,	O
the	O
prediction	O
difference	O
forx	O
0	B-DatasetName
on	O
p	O
is	O
F	O
(	O
x	O
0	B-DatasetName
;	O
p	O
)	O
=	O
f	O
(	O
...	O
moving	O
movie	O
.	O
)	O
−	O
f	O
(	O
...	O
moving	O
film	O
.	O
)	O
=	O
−1	O
.	O
Given	O
an	O
input	O
sentence	O
x	O
0	B-DatasetName
,	O
we	O
want	O
to	O
find	O
patch	O
words	O
p	O
and	O
a	O
vulnerable	O
examplex	O
0	B-DatasetName
such	O
that	O
f	O
(	O
x	O
0	B-DatasetName
p	O
)	O
=	O
f	O
(	O
x	O
0	B-DatasetName
)	O
.	O
Follow	O
Alzantot	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
choose	O
p	O
from	O
a	O
predefined	O
list	O
of	O
counter	O
-	O
fitted	O
synonyms	O
(	O
Mrkšić	O
et	O
al	O
,	O
2016	O
)	O
that	O
maximizes	O
|	O
f	O
soft	O
(	O
p	O
(	O
2	O
)	O
)	O
−	O
f	O
soft	O
(	O
p	O
(	O
1	O
)	O
)	O
|	O
.	O
Here	O
f	O
soft	O
(	O
x	O
)	O
:	O
X	O
[	O
0	B-DatasetName
,	O
1	O
]	O
denotes	O
probability	O
output	O
(	O
e.g.	O
,	O
after	O
the	O
softmax	B-MethodName
layer	O
but	O
before	O
the	O
final	O
argmax	O
)	O
,	O
f	O
soft	O
(	O
p	O
(	O
1	O
)	O
)	O
and	O
f	O
soft	O
(	O
p	O
(	O
2	O
)	O
)	O
denote	O
the	O
predictions	O
for	O
the	O
single	O
word	O
,	O
and	O
we	O
enumerate	O
through	O
all	O
possible	O
p	O
for	O
x	O
0	B-DatasetName
.	O
Let	O
k	O
be	O
the	O
neighborhood	O
distance	O
,	O
then	O
the	O
attack	O
is	O
equivalent	O
to	O
solving	O
:	O
x	O
0	B-DatasetName
=	O
argmax	O
x	O
Neighbor	O
k	O
(	O
x	O
0	B-DatasetName
)	O
|	O
F	O
(	O
x	O
;	O
p	O
)	O
|	O
.	O
(	O
3	O
)	O

We	O
follow	O
the	O
setup	O
from	O
the	O
robust	O
training	O
literature	O
(	O
Jia	O
et	O
al	O
,	O
2019	O
;	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
and	O
experiment	O
with	O
both	O
the	O
base	O
(	O
non	O
-	O
robust	O
)	O
and	O
robustly	O
trained	O
models	O
.	O
We	O
train	O
the	O
binary	O
sentiment	O
classifiers	O
on	O
the	O
SST	B-DatasetName
-	O
2	O
dataset	O
with	O
bag	O
-	O
ofwords	O
(	O
BoW	O
)	O
,	O
CNN	O
,	O
LSTM	B-MethodName
,	O
and	O
attention	O
-	O
based	O
Original	O
:	O
70	O
%	O
Negative	O
Input	O
Example	O
:	O
in	O
its	O
best	O
moments	O
,	O
resembles	O
a	O
bad	O
high	O
school	O
production	O
of	O
grease	O
,	O
without	O
benefit	O
of	O
song	O
.	O
Genetic	O
:	O
56	O
%	O
Positive	O
Adversarial	O
Example	O
:	O
in	O
its	O
best	O
moment	O
,	O
recalling	O
a	O
naughty	O
high	O
school	O
production	O
of	O
lubrication	O
,	O
unless	O
benefit	O
of	O
song	O
.	O
BAE	O
:	O
56	O
%	O
Positive	O
Adversarial	O
Example	O
:	O
in	O
its	O
best	O
moments	O
,	O
resembles	O
a	O
great	O
high	O
school	O
production	O
of	O
grease	O
,	O
without	O
benefit	O
of	O
song	O
.	O
SO	O
-	O
Enum	O
and	O
SO	O
-	O
Beam	O
(	O
ours	O
)	O
:	O
60	O
%	O
Negative	O
(	O
67	O
%	O
Positive	O
)	O
Vulnerable	O
Example	O
:	O
in	O
its	O
best	O
moments	O
,	O
resembles	O
a	O
bad	O
(	O
unhealthy	O
)	O
high	O
school	O
production	O
of	O
musicals	O
,	O
without	O
benefit	O
of	O
song	O
.	O
Table	O
1	O
:	O
Sampled	O
attack	O
results	O
on	O
the	O
robust	O
BoW.	O
For	O
Genetic	O
and	O
BAE	O
the	O
goal	O
is	O
to	O
find	O
an	O
adversarial	O
example	O
that	O
alters	O
the	O
original	O
prediction	O
,	O
whereas	O
for	O
SO	O
-	O
Enum	O
and	O
SO	O
-	O
Beam	O
the	O
goal	O
is	O
to	O
find	O
a	O
vulnerable	O
example	O
beyond	O
the	O
test	O
set	O
such	O
that	O
the	O
prediction	O
can	O
be	O
altered	O
by	O
substituting	O
bad	O
unhealthy	O
.	O

Intended	O
use	O
.	O
One	O
primary	O
goal	O
of	O
NLP	O
models	O
is	O
the	O
generalization	O
to	O
real	O
-	O
world	O
inputs	O
.	O
However	O
,	O
existing	O
test	O
datasets	O
and	O
templates	O
are	O
often	O
not	O
comprehensive	O
,	O
and	O
thus	O
it	O
is	O
difficult	O
to	O
evaluate	O
real	O
-	O
world	O
performance	O
(	O
Recht	O
et	O
al	O
,	O
2019	O
;	O
Ribeiro	O
et	O
al	O
,	O
2020	O
)	O
.	O
Our	O
work	O
sheds	O
a	O
light	O
on	O
quantifying	O
performance	O
for	O
inputs	O
beyond	O
the	O
test	O
dataset	O
and	O
help	O
uncover	O
model	O
weaknesses	O
prior	O
to	O
the	O
realworld	O
deployment	O
.	O
Misuse	O
potential	O
.	O
Similar	O
to	O
other	O
existing	O
adversarial	B-TaskName
attack	I-TaskName
methods	O
(	O
Ebrahimi	O
et	O
al	O
,	O
2018	O
;	O
Jin	O
et	O
al	O
,	O
2019	O
;	O
Zhao	O
et	O
al	O
,	O
2018b	O
)	O
,	O
our	O
second	O
-	O
order	O
attacks	O
can	O
be	O
used	O
for	O
finding	O
vulnerable	O
examples	O
to	O
a	O
NLP	O
system	O
.	O
Therefore	O
,	O
it	O
is	O
essential	O
to	O
study	O
how	O
to	O
improve	O
the	O
robustness	O
of	O
NLP	O
models	O
against	O
second	O
-	O
order	O
attacks	O
.	O
Limitations	O
.	O
While	O
the	O
core	O
idea	O
about	O
the	O
double	O
perturbation	O
framework	O
is	O
general	O
,	O
in	O
4	O
,	O
we	O
consider	O
only	O
binary	O
gender	O
in	O
the	O
analysis	O
of	O
counterfactual	O
fairness	O
due	O
to	O
the	O
restriction	O
of	O
the	O
English	O
corpus	O
we	O
used	O
,	O
which	O
only	O
have	O
words	O
associated	O
with	O
binary	O
gender	O
such	O
as	O
he	O
/	O
she	O
,	O
waiter	O
/	O
waitress	O
,	O
etc	O
.	O

To	O
validate	O
the	O
effectiveness	O
of	O
minimizing	O
Eq	O
.	O
(	O
4	O
)	O
,	O
we	O
also	O
experiment	O
on	O
a	O
second	O
-	O
order	O
baseline	O
that	O
constructs	O
vulnerable	O
examples	O
by	O
randomly	O
replacing	O
up	O
to	O
6	O
words	O
.	O
We	O
use	O
the	O
same	O
masked	O
language	O
model	O
and	O
threshold	O
as	O
SO	O
-	O
Beam	O
such	O
that	O
they	O
share	O
a	O
similar	O
neighborhood	O
.	O
We	O
perform	O
the	O
attack	O
on	O
the	O
same	O
models	O
as	O
Table	O
2	O
,	O
and	O
the	O
attack	O
success	O
rates	O
on	O
robustly	O
trained	O
BoW	O
,	O
CNN	O
,	O
LSTM	B-MethodName
,	O
and	O
Transformers	O
are	O
18.8	O
%	O
,	O
22.3	O
%	O
,	O
15.2	O
%	O
,	O
and	O
25.1	O
%	O
,	O
respectively	O
.	O
Despite	O
being	O
a	O
second	O
-	O
order	O
attack	O
,	O
the	O
random	O
baseline	O
has	O
low	O
attack	O
success	O
rates	O
thus	O
demonstrates	O
the	O
effectiveness	O
of	O
SO	O
-	O
Beam	O
.	O

We	O
randomly	O
select	O
100	O
successful	O
attacks	O
from	O
SO	O
-	O
Beam	O
and	O
consider	O
four	O
types	O
of	O
examples	O
(	O
for	O
a	O
total	O
of	O
400	O
examples	O
)	O
:	O
The	O
original	O
examples	O
with	O
and	O
without	O
synonym	O
substitution	O
p	O
,	O
and	O
the	O
vulnerable	O
examples	O
with	O
and	O
without	O
synonym	O
substitution	O
p.	O
For	O
each	O
example	O
,	O
we	O
annotate	O
the	O
naturalness	O
and	O
sentiment	O
separately	O
as	O
described	O
below	O
.	O
Naturalness	O
of	O
vulnerable	O
examples	O
.	O
We	O
ask	O
the	O
annotators	O
to	O
score	O
the	O
likelihood	O
of	O
being	O
an	O
original	O
example	O
(	O
i.e.	O
,	O
not	O
altered	O
by	O
computer	O
)	O
based	O
on	O
grammar	O
correctness	O
and	O
naturalness	O
,	O
with	O
a	O
Likert	O
scale	O
of	O
1	O
-	O
5	O
:	O
(	O
1	O
)	O
Sure	O
adversarial	O
example	O
.	O
(	O
2	O
)	O
Likely	O
an	O
adversarial	O
example	O
.	O
(	O
3	O
)	O
Neutral	O
.	O
(	O
4	O
)	O
Likely	O
an	O
original	O
example	O
.	O
(	O
5	O
)	O
Sure	O
original	O
example	O
.	O
Semantic	B-TaskName
similarity	I-TaskName
after	O
the	O
synonym	O
substitution	O
.	O
We	O
first	O
ask	O
the	O
annotators	O
to	O
predict	O
the	O
sentiment	O
on	O
a	O
Likert	O
scale	O
of	O
1	O
-	O
5	O
,	O
and	O
then	O
map	O
the	O
prediction	O
to	O
three	O
categories	O
:	O
negative	O
,	O
neutral	O
,	O
and	O
positive	O
.	O
We	O
consider	O
two	O
examples	O
to	O
have	O
the	O
same	O
semantic	O
meaning	O
if	O
and	O
only	O
if	O
they	O
are	O
both	O
positive	O
or	O
negative	O
.	O

Better	O
Feature	O
Integration	O
for	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName

It	O
has	O
been	O
shown	O
that	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
could	O
benefit	O
from	O
incorporating	O
the	O
long	O
-	O
distance	O
structured	O
information	O
captured	O
by	O
dependency	O
trees	O
.	O
We	O
believe	O
this	O
is	O
because	O
both	O
types	O
of	O
features	O
-	O
the	O
contextual	O
information	O
captured	O
by	O
the	O
linear	O
sequences	O
and	O
the	O
structured	O
information	O
captured	O
by	O
the	O
dependency	O
trees	O
may	O
complement	O
each	O
other	O
.	O
However	O
,	O
existing	O
approaches	O
largely	O
focused	O
on	O
stacking	O
the	O
LSTM	B-MethodName
and	O
graph	O
neural	O
networks	O
such	O
as	O
graph	O
convolutional	O
networks	O
(	O
GCNs	O
)	O
for	O
building	O
improved	O
NER	B-TaskName
models	O
,	O
where	O
the	O
exact	O
interaction	O
mechanism	O
between	O
the	O
two	O
different	O
types	O
of	O
features	O
is	O
not	O
very	O
clear	O
,	O
and	O
the	O
performance	O
gain	O
does	O
not	O
appear	O
to	O
be	O
significant	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
simple	O
and	O
robust	O
solution	O
to	O
incorporate	O
both	O
types	O
of	O
features	O
with	O
our	O
Synergized	O
-	O
LSTM	B-MethodName
(	O
Syn	O
-	O
LSTM	B-MethodName
)	O
,	O
which	O
clearly	O
captures	O
how	O
the	O
two	O
types	O
of	O
features	O
interact	O
.	O
We	O
conduct	O
extensive	O
experiments	O
on	O
several	O
standard	O
datasets	O
across	O
four	O
languages	O
.	O
The	O
results	O
demonstrate	O
that	O
the	O
proposed	O
model	O
achieves	O
better	O
performance	O
than	O
previous	O
approaches	O
while	O
requiring	O
fewer	O
parameters	O
.	O
Our	O
further	O
analysis	O
demonstrates	O
that	O
our	O
model	O
can	O
capture	O
longer	O
dependencies	O
compared	O
with	O
strong	O
baselines	O
.	O
1	O

To	O
incorporate	O
the	O
long	O
-	O
range	O
dependencies	O
,	O
we	O
consider	O
an	O
additional	O
graph	O
-	O
encoded	O
representation	O
g	O
t	O
(	O
Figure	O
2	O
)	O
as	O
the	O
model	O
input	O
to	O
integrate	O
σ	O
σ	O
σ	O
tanh	O
tanh	O
σ	O
×	O
+	O
×	O
+	O
×	O
×	O
tanh	O
c	O
t	O
-	O
1	O
Previous	O
Cell	B-DatasetName
contextual	O
and	O
structured	O
information	O
.	O
The	O
graphencoded	O
representation	O
g	O
t	O
can	O
be	O
derived	O
from	O
Graph	O
Neural	O
Networks	O
(	O
GNNs	O
)	O
such	O
as	O
GCN	B-MethodName
(	O
Kipf	O
and	O
Welling	O
,	O
2017	O
)	O
,	O
which	O
are	O
capable	O
of	O
bringing	O
in	O
structured	O
information	O
through	O
graph	O
structure	O
(	O
Hamilton	O
et	O
al	O
,	O
2017a	O
)	O
.	O
However	O
,	O
structured	O
information	O
sometimes	O
is	O
hard	O
to	O
encode	O
,	O
as	O
we	O
can	O
see	O
from	O
the	O
example	O
in	O
Figure	O
1	O
.	O
One	O
naive	O
approach	O
is	O
to	O
use	O
a	O
deep	O
GNN	O
to	O
capture	O
such	O
information	O
along	O
multiple	O
dependency	O
arcs	O
between	O
two	O
words	O
,	O
which	O
could	O
mess	O
up	O
information	O
and	O
lead	O
to	O
training	O
difficulties	O
.	O
A	O
straightforward	O
solution	O
is	O
to	O
integrate	O
both	O
structured	O
and	O
contextual	O
information	O
via	O
LSTM	B-MethodName
.	O
As	O
shown	O
in	O
Figure	O
1	O
(	O
Hybrid	O
Paths	O
)	O
,	O
the	O
structured	O
information	O
can	O
be	O
passed	O
to	O
neighbors	O
or	O
context	O
,	O
which	O
allows	O
a	O
model	O
to	O
use	O
less	O
number	O
of	O
GNN	O
layers	O
and	O
alleviate	O
such	O
issues	O
for	O
long	O
-	O
range	O
dependencies	O
.	O
The	O
input	O
to	O
the	O
LSTM	B-MethodName
can	O
simply	O
be	O
the	O
concatenation	O
of	O
word	O
representation	O
x	O
t	O
and	O
g	O
t	O
at	O
each	O
position	O
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
2	O
.	O
However	O
,	O
because	O
such	O
an	O
approach	O
requires	O
both	O
x	O
t	O
and	O
g	O
t	O
to	O
decide	O
the	O
value	O
of	O
the	O
input	O
gate	O
jointly	O
,	O
it	O
could	O
be	O
a	O
potential	O
victim	O
of	O
two	O
sources	O
of	O
uncertainties	O
:	O
1	O
)	O
the	O
uncertainty	O
of	O
the	O
quality	O
of	O
graph	O
-	O
encoded	O
representation	O
g	O
t	O
,	O
and	O
2	O
)	O
the	O
uncertainty	O
of	O
the	O
exact	O
interaction	O
mechanism	O
between	O
the	O
two	O
types	O
of	O
features	O
.	O
These	O
may	O
lead	O
to	O
sub	O
-	O
optimal	O
performance	O
,	O
especially	O
if	O
the	O
graph	O
-	O
encoded	O
representation	O
g	O
t	O
is	O
unsatisfactory	O
.	O
Thus	O
,	O
we	O
need	O
to	O
design	O
a	O
new	O
approach	O
to	O
incorporate	O
both	O
types	O
of	O
information	O
from	O
x	O
t	O
and	O
g	O
t	O
with	O
a	O
more	O
explicit	O
interaction	O
mechanism	O
,	O
with	O
which	O
we	O
hope	O
to	O
alleviate	O
the	O
above	O
issues	O
.	O

SemEval	O
2010	O
Task	O
1	O
Table	O
2	O
shows	O
comparisons	O
of	O
our	O
model	O
with	O
baseline	O
models	O
on	O
the	O
SemEval	O
2010	O
Task	O
1	O
Catalan	O
and	O
Spanish	O
datasets	O
.	O
Our	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
outperforms	O
all	O
existing	O
models	O
with	O
F	O
1	O
82.76	O
and	O
85.09	O
(	O
p	O
<	O
10	O
−5	O
)	O
compared	O
to	O
DGLSTM	O
-	O
CRF	B-MethodName
on	O
Catalan	O
and	O
Spanish	O
datasets	O
when	O
FastText	B-MethodName
word	B-TaskName
embeddings	I-TaskName
are	O
used	O
.	O
Our	O
model	O
outperforms	O
the	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
by	O
13.25	O
and	O
11.22	O
F	O
1	O
points	O
,	O
and	O
outperforms	O
BiLSTM	B-MethodName
-	O
GCN	B-MethodName
-	O
CRF	B-MethodName
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
model	O
by	O
4.64	O
and	O
3.16	O
on	O
Catalan	O
and	O
Spanish	O
.	O
The	O
large	O
performance	O
gap	O
between	O
BiLSTM	B-MethodName
-	O
GCN	B-MethodName
-	O
CRF	B-MethodName
and	O
our	O
model	O
indicates	O
that	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
shows	O
better	O
compatibility	O
with	O
GCN	B-MethodName
,	O
and	O
this	O
confirms	O
that	O
simply	O
stacking	O
GCN	B-MethodName
on	O
top	O
of	O
the	O
BiLSTM	B-MethodName
does	O
not	O
perform	O
well	O
.	O
Our	O
method	O
outperforms	O
GCN	B-MethodName
-	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
by	O
5.33	O
and	O
3.24	O
F	O
1	O
points	O
on	O
Catalan	O
and	O
Spanish	O
.	O
This	O
shows	O
that	O
our	O
proposed	O
model	O
demonstrates	O
a	O
better	O
integration	O
of	O
contextual	O
information	O
and	O
structured	O
information	O
.	O
Furthermore	O
,	O
our	O
proposed	O
method	O
brings	O
1.12	O
and	O
1.62	O
F	O
1	O
points	O
improvement	O
on	O
Catalan	O
and	O
Spanish	O
datasets	O
compare	O
to	O
the	O
DGLSTM	O
-	O
CRF	B-MethodName
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
.	O
The	O
DGLSTM	O
-	O
CRF	B-MethodName
employs	O
2	O
-	O
layer	O
dependency	O
guided	O
BiLSTM	B-MethodName
to	O
capture	O
grandchild	O
dependencies	O
,	O
which	O
leads	O
to	O
longer	O
training	O
time	O
and	O
more	O
model	O
parameters	O
.	O
However	O
,	O
our	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
is	O
able	O
to	O
get	O
better	O
performance	O
with	O
fewer	O
model	O
parameters	O
and	O
shorter	O
training	O
time	O
because	O
of	O
the	O
fewer	O
LSTM	B-MethodName
layers	O
.	O
Such	O
results	O
demonstrate	O
that	O
our	O
proposed	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
manages	O
to	O
capture	O
structured	O
information	O
effectively	O
.	O
Furthermore	O
,	O
with	O
the	O
contextualized	O
word	O
representation	O
,	O
the	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
achieves	O
much	O
higher	O
performance	O
improvement	O
than	O
any	O
other	O
method	O
.	O
Our	O
model	O
outperforms	O
the	O
strong	O
baseline	O
model	O
DGLSTM	O
-	O
CRF	B-MethodName
+	O
ELMO	B-MethodName
by	O
4.83	O
and	O
2.54	O
in	O
terms	O
of	O
F	O
1	O
(	O
p	O
<	O
10	O
−5	O
)	O
on	O
Catalan	O
and	O
Spanish	O
,	O
respectively	O
.	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	O
To	O
understand	O
the	O
generalizability	O
of	O
our	O
model	O
,	O
we	O
evaluate	O
the	O
proposed	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
on	O
large	O
scale	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
datasets	O
.	O
Table	O
3	O
shows	O
comparisons	O
of	O
our	O
model	O
with	O
baseline	O
models	O
on	O
English	O
.	O
Our	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
outperforms	O
all	O
existing	O
methods	O
with	O
89.04	O
in	O
terms	O
of	O
F	O
1	O
score	O
(	O
p	O
<	O
0.01	O
)	O
compared	O
to	O
DGLSTM	O
-	O
CRF	B-MethodName
,	O
when	O
GloVE	O
word	B-TaskName
embeddings	I-TaskName
are	O
used	O
.	O
Our	O
model	O
outperforms	O
the	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
by	O
1.97	O
in	O
F	O
1	O
,	O
BiLSTM	B-MethodName
-	O
GCN	B-MethodName
-	O
CRF	B-MethodName
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
model	O
by	O
0.86	O
.	O
Note	O
that	O
our	O
implemented	O
GCN	B-MethodName
-	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
outperforms	O
the	O
previous	O
DGLSTM	O
-	O
CRF	B-MethodName
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
by	O
0.14	O
in	O
F	O
1	O
.	O
Our	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
further	O
brings	O
the	O
improvement	O
to	O
0.52	O
.	O
Moreover	O
,	O
with	O
the	O
contextualized	O
word	O
representation	O
BERT	B-MethodName
,	O
our	O
method	O
achieves	O
an	O
F	O
1	O
score	O
of	O
90.85	O
(	O
p	O
<	O
10	O
−5	O
)	O
compared	O
to	O
DGLSTM	O
-	O
CRF	B-MethodName
+	O
ELMO	B-MethodName
.	O
Our	O
method	O
outperforms	O
the	O
previous	O
model	O
(	O
Luo	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
relies	O
on	O
document	O
-	O
level	O
information	O
,	O
by	O
0.55	O
in	O
F	O
1	O
.	O
Furthermore	O
,	O
the	O
performance	O
improvement	O
on	O
recall	O
is	O
more	O
prominent	O
as	O
compared	O
to	O
precision	O
.	O
This	O
shows	O
that	O
the	O
proposed	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
is	O
able	O
to	O
extract	O
more	O
entities	O
.	O
Jie	O
and	O
Lu	O
(	O
2019	O
)	O
.	O
There	O
are	O
also	O
other	O
methods	O
(	O
Li	O
et	O
al	O
,	O
2020a	O
,	O
b	O
)	O
that	O
use	O
external	O
information	O
,	O
(	O
Yu	O
et	O
al	O
,	O
2020	O
)	O
use	O
document	O
-	O
level	O
information	O
to	O
encode	O
the	O
sentence	O
,	O
which	O
are	O
not	O
direct	O
comparisons	O
to	O
ours	O
.	O
forms	O
the	O
baseline	O
models	O
,	O
specifically	O
by	O
2.04	O
in	O
F	O
1	O
compared	O
to	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
,	O
by	O
2.39	O
compared	O
to	O
BiLSTM	B-MethodName
-	O
GCN	B-MethodName
-	O
CRF	B-MethodName
,	O
by	O
1.86	O
compared	O
to	O
GCN	B-MethodName
-	O
BILSTM	B-MethodName
-	O
CRF	B-MethodName
and	O
by	O
1.11	O
(	O
p	O
<	O
10	O
−5	O
)	O
compared	O
to	O
DGLSTM	O
-	O
CRF	B-MethodName
when	O
FastText	B-MethodName
is	O
used	O
.	O
Note	O
that	O
the	O
baseline	O
BiLSTM	B-MethodName
-	O
GCN	B-MethodName
-	O
CRF	B-MethodName
model	O
is	O
0.35	O
points	O
worse	O
than	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
.	O
Such	O
results	O
further	O
confirm	O
the	O
effectiveness	O
of	O
our	O
proposed	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
for	O
incorporating	O
structured	O
information	O
.	O
We	O
find	O
a	O
similar	O
behavior	O
when	O
the	O
contextualized	O
word	O
representation	O
BERT	B-MethodName
is	O
used	O
.	O
With	O
the	O
contextualized	O
word	O
representation	O
,	O
we	O
achieve	O
a	O
higher	O
F	O
1	O
score	O
of	O
80.20	O
.	O

Robustness	O
Analysis	O
To	O
study	O
the	O
robustness	O
of	O
our	O
model	O
and	O
check	O
whether	O
our	O
model	O
can	O
regulate	O
the	O
flow	O
of	O
information	O
from	O
the	O
graphencoded	O
representation	O
,	O
we	O
analyze	O
the	O
influence	O
of	O
the	O
quality	O
of	O
dependency	O
trees	O
.	O
We	O
train	O
and	O
evaluate	O
an	O
additional	O
dependency	O
parser	O
(	O
Dozat	O
and	O
Manning	O
,	O
2017	O
)	O
.	O
Specifically	O
,	O
we	O
train	O
the	O
Jie	O
and	O
Lu	O
(	O
2019	O
)	O
.	O
There	O
are	O
also	O
other	O
methods	O
(	O
Li	O
et	O
al	O
,	O
2020a	O
,	O
b	O
)	O
that	O
use	O
external	O
information	O
,	O
which	O
are	O
not	O
direct	O
comparisons	O
to	O
ours	O
.	O
dependency	O
parser	O
6	O
on	O
the	O
given	O
training	O
datasets	O
and	O
select	O
the	O
best	O
model	O
based	O
on	O
the	O
dev	O
sets	O
.	O
Then	O
we	O
apply	O
the	O
best	O
model	O
to	O
the	O
test	O
sets	O
to	O
obtain	O
dependency	O
trees	O
.	O
We	O
also	O
train	O
and	O
evaluate	O
our	O
model	O
with	O
random	O
dependency	O
trees	O
.	O
Table	O
8	O
presents	O
the	O
comparisons	O
between	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
and	O
DGLSTM	O
-	O
CRF	B-MethodName
+	O
ELMO	B-MethodName
with	O
given	O
,	O
predicted	O
and	O
random	O
dependency	O
trees	O
.	O
We	O
observe	O
that	O
both	O
models	O
encounter	O
a	O
performance	O
drop	O
when	O
we	O
use	O
the	O
predicted	O
parse	O
trees	O
and	O
random	O
trees	O
.	O
Our	O
performance	O
differences	O
with	O
the	O
given	O
parse	O
trees	O
are	O
relatively	O
smaller	O
than	O
the	O
corresponding	O
differences	O
in	O
DGLSTM	O
-	O
CRF	B-MethodName
+	O
ELMO	B-MethodName
.	O
Such	O
an	O
observation	O
demonstrates	O
the	O
robustness	O
of	O
our	O
proposed	O
model	O
against	O
structured	O
information	O
from	O
the	O
trees	O
of	O
different	O
quality	O
.	O
It	O
is	O
worthwhile	O
to	O
note	O
that	O
,	O
with	O
the	O
predicted	O
dependencies	O
,	O
our	O
proposed	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
is	O
still	O
able	O
to	O
outperform	O
the	O
strong	O
baseline	O
DGLSTM	O
-	O
CRF	B-MethodName
+	O
ELMO	B-MethodName
even	O
with	O
the	O
given	O
parse	O
trees	O
on	O
Catalan	O
,	O
English	O
,	O
and	O
Chinese	O
datasets	O
.	O
To	O
further	O
study	O
the	O
robustness	O
,	O
we	O
conduct	O
an	O
analysis	O
to	O
investigate	O
if	O
the	O
gate	O
m	O
t	O
(	O
Figure	O
2	O
)	O
has	O
the	O
ability	O
to	O
regulate	O
the	O
flow	O
of	O
information	O
from	O
the	O
graph	O
-	O
encoded	O
representation	O
.	O
Intuitively	O
,	O
the	O
gate	O
m	O
t	O
should	O
tend	O
to	O
have	O
a	O
small	O
value	O
when	O
0	B-DatasetName
-	O
0	B-DatasetName
.4	O
0	B-DatasetName
.4	O
-	O
0	B-DatasetName
.5	O
0	B-DatasetName
.5	O
-	O
0	B-DatasetName
.6	O
0	B-DatasetName
.6	O
-	O
0	B-DatasetName
.7	O
0	B-DatasetName
.7	O
-	O
0	B-DatasetName
.8	O
0	B-DatasetName
.8	O
-	O
0	B-DatasetName
.9	O
0	B-DatasetName
.9	O
-	O
1	O
the	O
quality	O
of	O
the	O
parse	O
tree	O
is	O
not	O
good	O
(	O
e.g.	O
,	O
with	O
random	O
trees	O
)	O
.	O
We	O
statistically	O
plot	O
the	O
number	O
of	O
words	O
with	O
respect	O
to	O
different	O
gate	O
value	O
ranges	O
(	O
m	O
t	O
)	O
.	O
Figure	O
4	O
shows	O
the	O
comparison	O
between	O
the	O
models	O
of	O
using	O
random	O
trees	O
and	O
given	O
trees	O
on	O
Catalan	O
and	O
Spanish	O
7	O
.	O
We	O
observe	O
that	O
the	O
gate	O
m	O
t	O
is	O
more	O
likely	O
to	O
open	O
(	O
the	O
value	O
is	O
higher	O
)	O
when	O
we	O
use	O
the	O
given	O
parse	O
trees	O
compared	O
with	O
random	O
parse	O
trees	O
.	O
Such	O
behavior	O
demonstrates	O
that	O
our	O
proposed	O
model	O
can	O
selectively	O
aggregate	O
the	O
information	O
from	O
the	O
graph	O
-	O
encoded	O
representation	O
.	O

We	O
compare	O
the	O
performance	O
of	O
our	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
with	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
and	O
DGLSTM	O
-	O
CRF	B-MethodName
+	O
ELMO	B-MethodName
models	O
with	O
respect	O
to	O
sentence	O
length	O
,	O
and	O
the	O
results	O
are	O
shown	O
in	O
Figure	O
5	O
.	O
We	O
observe	O
that	O
the	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
model	O
consistently	O
outperforms	O
the	O
two	O
baseline	O
models	O
on	O
the	O
four	O
languages	O
8	O
.	O
In	O
particular	O
,	O
although	O
the	O
performance	O
tends	O
to	O
drop	O
as	O
the	O
sentence	O
length	O
increases	O
,	O
our	O
proposed	O
model	O
shows	O
relatively	O
better	O
performance	O
when	O
the	O
sentence	O
length	O
is	O
≥	O
60	O
.	O
This	O
confirms	O
that	O
the	O
proposed	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
is	O
able	O
to	O
effectively	O
incorporate	O
structured	O
information	O
.	O
Note	O
that	O
our	O
2	O
-	O
layer	O
GCN	B-MethodName
is	O
computed	O
based	O
on	O
the	O
dependency	O
trees	O
,	O
which	O
include	O
both	O
short	O
-	O
range	O
dependencies	O
and	O
long	O
-	O
range	O
dependencies	O
.	O
With	O
the	O
graph	O
-	O
encoded	O
representation	O
and	O
the	O
proposed	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
,	O
the	O
individual	O
word	O
representation	O
is	O
enhanced	O
by	O
both	O
contextual	O
and	O
structured	O
information	O
.	O
Therefore	O
,	O
for	O
the	O
sentences	O
with	O
length	O
of	O
≤	O
14	O
,	O
we	O
can	O
still	O
observe	O
obvious	O
improvements	O
.	O
The	O
significant	O
performance	O
improvements	O
on	O
the	O
four	O
datasets	O
show	O
the	O
capability	O
of	O
our	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
to	O
capture	O
the	O
structured	O
information	O
despite	O
the	O
sentence	O
length	O
.	O

We	O
conduct	O
another	O
evaluation	O
on	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
,	O
DGLSTM	O
-	O
CRF	B-MethodName
+	O
ELMO	B-MethodName
,	O
and	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
models	O
with	O
respect	O
to	O
entity	O
length	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
≥	O
6	O
}	O
on	O
the	O
four	O
languages	O
.	O
Table	O
6	O
shows	O
the	O
performance	O
comparison	O
of	O
two	O
models	O
with	O
respect	O
to	O
entity	O
length	O
.	O
With	O
the	O
structured	O
information	O
,	O
both	O
DGLSTM	O
-	O
CRF	B-MethodName
+	O
ELMO	B-MethodName
and	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
achieve	O
better	O
performance	O
compared	O
to	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
.	O
When	O
the	O
length	O
of	O
entity	O
is	O
≤	O
3	O
,	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
achieves	O
better	O
results	O
compared	O
to	O
DGLSTM	O
-	O
CRF	B-MethodName
+	O
ELMO	B-MethodName
.	O
This	O
confirms	O
that	O
our	O
proposed	O
method	O
can	O
effectively	O
incorporate	O
the	O
structured	O
information	O
.	O
Our	O
model	O
consistently	O
outperforms	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
,	O
and	O
the	O
performance	O
tends	O
to	O
have	O
more	O
improvements	O
when	O
entities	O
are	O
getting	O
longer	O
except	O
on	O
the	O
Chinese	O
dataset	O
.	O
We	O
note	O
there	O
are	O
some	O
special	O
characteristics	O
of	O
the	O
Chinese	O
language	O
.	O
As	O
mentioned	O
by	O
Jie	O
and	O
Lu	O
(	O
2019	O
)	O
,	O
the	O
percentage	O
of	O
entities	O
that	O
are	O
able	O
to	O
perfectly	O
form	O
a	O
sub	O
-	O
tree	O
is	O
only	O
92.9	O
%	O
for	O
OntoNotes	B-DatasetName
Chinese	O
,	O
as	O
compared	O
to	O
98.5	O
%	O
,	O
100	O
%	O
,	O
100	O
%	O
for	O
OntoNotes	B-DatasetName
English	O
,	O
SemEval	O
Catalan	O
and	O
Spanish	O
.	O
Furthermore	O
,	O
the	O
ratio	O
of	O
long	O
entities	O
is	O
much	O
higher	O
for	O
Catalan	O
and	O
Spanish	O
compared	O
to	O
English	O
and	O
Chinese	O
.	O
The	O
experimental	O
results	O
on	O
Catalan	O
and	O
Spanish	O
datasets	O
show	O
significant	O
improvements	O
for	O
long	O
entities	O
.	O
Such	O
results	O
show	O
that	O
the	O
structured	O
information	O
conveyed	O
by	O
the	O
dependency	O
trees	O
can	O
be	O
more	O
crucial	O
when	O
entity	O
length	O
becomes	O
longer	O
.	O

To	O
fully	O
explore	O
the	O
impact	O
of	O
the	O
number	O
of	O
GCN	B-MethodName
layers	O
,	O
we	O
conduct	O
another	O
experiment	O
on	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
model	O
with	O
the	O
number	O
of	O
GCN	B-MethodName
layers	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
,	O
and	O
Figure	O
6	O
shows	O
the	O
performance	O
on	O
the	O
dev	O
set	O
of	O
the	O
four	O
languages	O
.	O
The	O
last	O
bar	O
,	O
indicated	O
as	O
AVG	O
,	O
is	O
obtained	O
by	O
averaging	O
the	O
dev	O
results	O
on	O
the	O
four	O
datasets	O
.	O
We	O
observe	O
that	O
the	O
overall	O
performance	O
is	O
better	O
when	O
the	O
number	O
of	O
GCN	B-MethodName
layers	O
equals	O
2	O
.	O
Note	O
that	O
similar	O
behavior	O
can	O
also	O
be	O
found	O
in	O
the	O
work	O
by	O
Kipf	O
and	O
Welling	O
(	O
2017	O
)	O
for	O
document	B-TaskName
classification	I-TaskName
and	O
node	B-TaskName
classification	I-TaskName
.	O
Therefore	O
,	O
we	O
evaluate	O
our	O
proposed	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
model	O
with	O
2	O
-	O
layer	O
GCN	B-MethodName
.	O
Ablation	O
Study	O
To	O
understand	O
the	O
contribution	O
of	O
each	O
component	O
,	O
we	O
conduct	O
an	O
ablation	O
study	O
on	O
the	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	O
dataset	O
,	O
and	O
Table	O
7	O
presents	O
the	O
detailed	O
results	O
of	O
our	O
model	O
with	O
contextualized	O
representation	O
.	O
We	O
find	O
that	O
the	O
performance	O
drops	O
by	O
0.24	O
F	O
1	O
score	O
when	O
we	O
only	O
use	O
1	O
-	O
layer	O
GCN	B-MethodName
.	O
Without	O
GCN	B-MethodName
at	O
all	O
,	O
the	O
score	O
drops	O
by	O
1.13	O
F	O
1	O
.	O
The	O
original	O
dependency	O
contributes	O
0.27	O
F	O
1	O
score	O
.	O
Removing	O
the	O
dependency	O
relation	O
embedding	O
also	O
decreases	O
the	O
performance	O
by	O
0.27	O
F	O
1	O
.	O
When	O
we	O
remove	O
the	O
POS	O
tags	O
embedding	O
,	O
the	O
result	O
drops	O
by	O
0.39	O
F	O
1	O
.	O

LSTM	B-MethodName
LSTM	B-MethodName
has	O
demonstrated	O
its	O
great	O
effectiveness	O
in	O
many	O
NLP	O
tasks	O
and	O
becomes	O
a	O
standard	O
module	O
for	O
many	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
(	O
Wen	O
et	O
al	O
,	O
2015	O
;	O
Ma	O
and	O
Hovy	O
,	O
2016	O
;	O
Dozat	O
and	O
Manning	O
,	O
2017	O
)	O
.	O
However	O
,	O
the	O
sequential	O
nature	O
of	O
the	O
LSTM	B-MethodName
makes	O
it	O
challenging	O
to	O
capture	O
long	O
-	O
range	O
dependencies	O
.	O
Zhang	O
et	O
al	O
(	O
2018a	O
)	O
propose	O
the	O
S	O
-	O
LSTM	B-MethodName
model	O
to	O
include	O
a	O
sentence	O
state	O
to	O
allow	O
both	O
local	O
and	O
global	O
information	O
exchange	O
simultaneously	O
.	O
Mogrifier	B-MethodName
LSTM	I-MethodName
(	O
Melis	O
et	O
al	O
,	O
2020	O
)	O
mutually	O
gates	O
the	O
current	O
input	O
and	O
the	O
previous	O
output	O
to	O
enhance	O
the	O
interaction	O
between	O
the	O
input	O
and	O
the	O
context	O
.	O
These	O
two	O
works	O
do	O
not	O
consider	O
structured	O
information	O
for	O
the	O
LSTM	B-MethodName
design	O
.	O
Since	O
natural	O
language	O
is	O
usually	O
structured	O
,	O
Shen	O
et	O
al	O
(	O
2018	O
)	O
propose	O
ON	O
-	O
LSTM	B-MethodName
to	O
add	O
a	O
hierarchical	O
bias	O
to	O
allow	O
the	O
neurons	O
to	O
be	O
updated	O
by	O
following	O
certain	O
order	O
.	O
While	O
the	O
ON	O
-	O
LSTM	B-MethodName
is	O
learning	O
the	O
latent	O
constituency	O
parse	O
trees	O
,	O
we	O
focus	O
on	O
incorporating	O
the	O
explicit	O
structured	O
information	O
conveyed	O
by	O
the	O
dependency	O
parse	O
trees	O
.	O
NER	B-TaskName
Early	O
work	O
(	O
Sasano	O
and	O
Kurohashi	O
,	O
2008	O
)	O
uses	O
syntactic	O
dependency	O
features	O
to	O
improve	O
the	O
SVM	B-MethodName
performance	O
on	O
Japanese	O
NER	B-TaskName
task	O
.	O
Liu	O
et	O
al	O
(	O
2010	O
)	O
propose	O
to	O
construct	O
skip	O
-	O
edges	O
to	O
link	O
similar	O
words	O
or	O
words	O
having	O
typed	O
dependencies	O
to	O
capture	O
long	O
-	O
range	O
dependencies	O
.	O
The	O
later	O
works	O
(	O
Collobert	O
et	O
al	O
,	O
2010	O
;	O
Lample	O
et	O
al	O
,	O
2016	O
;	O
Chiu	O
and	O
Nichols	O
,	O
2016b	O
)	O
focus	O
on	O
using	O
neural	O
networks	O
to	O
extract	O
features	O
and	O
achieved	O
the	O
stateof	O
-	O
the	O
-	O
art	O
performance	O
.	O
Jie	O
et	O
al	O
(	O
2017	O
)	O
find	O
that	O
some	O
relations	O
between	O
the	O
dependency	O
edges	O
and	O
the	O
entities	O
can	O
be	O
used	O
to	O
reduce	O
the	O
search	O
space	O
of	O
their	O
model	O
,	O
which	O
significantly	O
reduces	O
the	O
time	O
complexity	O
.	O
Yu	O
et	O
al	O
(	O
2020	O
)	O
employ	O
pre	O
-	O
trained	O
language	O
model	O
to	O
encode	O
document	O
-	O
level	O
information	O
to	O
explore	O
all	O
spans	O
with	O
the	O
graph	O
-	O
based	O
dependency	O
graph	O
based	O
ideas	O
.	O
The	O
pre	O
-	O
trained	O
language	O
models	O
(	O
e.g.	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
ELMO	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
)	O
further	O
improve	O
neuralbased	O
approaches	O
with	O
a	O
good	O
contextualized	O
representation	O
.	O
However	O
,	O
previous	O
works	O
did	O
not	O
focus	O
on	O
investigating	O
how	O
to	O
effectively	O
integrate	O
structured	O
and	O
contextual	O
information	O
well	O
.	O

Table	O
9	O
shows	O
the	O
statistics	O
of	O
the	O
number	O
of	O
entities	O
with	O
respect	O
to	O
entity	O
length	O
for	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	O
and	O
Chinese	O
,	O
SemEval	O
2010	O
Task	O
1	O
Catalan	O
and	O
Spanish	O
datasets	O
.	O

Figure	O
7	O
shows	O
the	O
comparisons	O
of	O
the	O
models	O
of	O
using	O
random	O
trees	O
and	O
given	O
trees	O
on	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	O
and	O
Chinese	O
datasets	O
.	O

We	O
compare	O
the	O
performance	O
of	O
our	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
with	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
+	O
BERT	B-MethodName
and	O
DGLSTM	O
-	O
CRF	B-MethodName
+	O
ELMO	B-MethodName
models	O
with	O
respect	O
to	O
sentence	O
length	O
,	O
and	O
the	O
results	O
are	O
shown	O
in	O
Figure	O
8	O
.	O

We	O
further	O
show	O
an	O
example	O
to	O
visualize	O
the	O
propagation	O
of	O
non	O
-	O
local	O
information	O
(	O
Figure	O
9	O
)	O
.	O
The	O
example	O
is	O
selected	O
from	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	O
dataset	O
.	O
Even	O
though	O
the	O
DGLSTM	O
-	O
CRF	B-MethodName
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
model	O
is	O
able	O
to	O
recognize	O
"	O
Tianshui	O
"	O
as	O
a	O
named	O
entity	O
,	O
it	O
predicts	O
a	O
wrong	O
entity	O
type	O
as	O
PERSON	O
while	O
the	O
true	O
type	O
is	O
GPE	O
.	O
If	O
only	O
looking	O
at	O
the	O
first	O
half	O
of	O
the	O
sentence	O
,	O
it	O
is	O
possible	O
to	O
predict	O
"	O
Tianshui	O
"	O
as	O
PERSON	O
because	O
of	O
the	O
local	O
information	O
"	O
age	O
"	O
.	O
However	O
,	O
the	O
second	O
half	O
of	O
the	O
sentence	O
confirms	O
that	O
the	O
entity	O
type	O
of	O

English	O
Chinese	O
Catalan	O
Spanish	O
P.	O
R.	O
F	O
1	O
P.	O
During	O
Tanshui	O
's	O
golden	O
age	O
,	O
large	O
and	O
small	O
boats	O
were	O
constantly	O
coming	O
and	O
going	O
in	O
the	O
harbor	O
,	O
and	O
it	O
was	O
not	O
usual	O
to	O
see	O
enormous	O
steamships	O
.	O
ROOT	O
Figure	O
9	O
:	O
An	O
example	O
of	O
dependency	O
tree	O
.	O
The	O
mentioned	O
entity	O
is	O
highlighted	O
in	O
orange	O
,	O
and	O
the	O
entity	O
type	O
is	O
GPE	O
.	O
"	O
Tianshui	O
"	O
is	O
GPE	O
.	O
With	O
the	O
non	O
-	O
local	O
information	O
from	O
the	O
graph	O
-	O
encoded	O
representation	O
,	O
our	O
Syn	O
-	O
LSTM	B-MethodName
-	O
CRF	B-MethodName
successfully	O
predicts	O
the	O
right	O
entity	O
type	O
.	O

In	O
recent	O
years	O
,	O
we	O
have	O
seen	O
deep	O
learning	O
and	O
distributed	O
representations	O
of	O
words	O
and	O
sentences	O
make	O
impact	O
on	O
a	O
number	O
of	O
natural	O
language	O
processing	O
tasks	O
,	O
such	O
as	O
similarity	O
,	O
entailment	O
and	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
Here	O
we	O
introduce	O
a	O
new	O
task	O
:	O
understanding	O
of	O
mental	O
health	O
concepts	O
derived	O
from	O
Cognitive	O
Behavioural	O
Therapy	O
(	O
CBT	B-DatasetName
)	O
.	O
We	O
define	O
a	O
mental	O
health	O
ontology	B-MethodName
based	O
on	O
the	O
CBT	B-DatasetName
principles	O
,	O
annotate	O
a	O
large	O
corpus	O
where	O
this	O
phenomena	O
is	O
exhibited	O
and	O
perform	O
understanding	O
using	O
deep	O
learning	O
and	O
distributed	O
representations	O
.	O
Our	O
results	O
show	O
that	O
the	O
performance	O
of	O
deep	O
learning	O
models	O
combined	O
with	O
word	B-TaskName
embeddings	I-TaskName
or	O
sentence	B-TaskName
embeddings	I-TaskName
significantly	O
outperform	O
non	O
-	O
deep	O
-	O
learning	O
models	O
in	O
this	O
difficult	O
task	O
.	O
This	O
understanding	O
module	O
will	O
be	O
an	O
essential	O
component	O
of	O
a	O
statistical	O
dialogue	O
system	O
delivering	O
therapy	O
.	O

Promotion	O
of	O
mental	O
well	O
-	O
being	O
is	O
at	O
the	O
core	O
of	O
the	O
action	O
plan	O
on	O
mental	O
health	O
2013	O
-	O
2020	O
of	O
the	O
World	O
Health	O
Organisation	O
(	O
WHO	O
)	O
(	O
World	O
Health	O
Organization	O
,	O
2013	O
)	O
and	O
of	O
the	O
European	O
Pact	O
on	O
Mental	O
Health	O
and	O
Well	O
-	O
being	O
of	O
the	O
European	O
Union	O
(	O
EU	O
high	O
-	O
level	O
conference	O
:	O
Together	O
for	O
Mental	O
Health	O
and	O
Well	O
-	O
being	O
,	O
2008	O
)	O
.	O
The	O
biggest	O
potential	O
breakthrough	O
in	O
fighting	O
mental	O
illness	O
would	O
lie	O
in	O
finding	O
tools	O
for	O
early	O
detection	O
and	O
preventive	O
intervention	O
(	O
Insel	O
and	O
Scholnick	O
,	O
2006	O
)	O
.	O
The	O
WHO	O
action	O
plan	O
stresses	O
the	O
importance	O
of	O
health	O
policies	O
and	O
programmes	O
that	O
not	O
only	O
meet	O
the	O
need	O
of	O
people	O
affected	O
by	O
mental	O
disorders	O
but	O
also	O
protect	O
mental	O
well	O
-	O
being	O
.	O
The	O
emphasis	O
is	O
on	O
early	O
evidence	O
-	O
based	O
non	O
-	O
pharmacological	O
intervention	O
,	O
avoiding	O
institutionalisation	O
and	O
medicalisation	O
.	O
What	O
is	O
particularly	O
important	O
for	O
successful	O
intervention	O
is	O
the	O
frequency	O
with	O
which	O
the	O
therapy	O
can	O
be	O
accessed	O
(	O
Hansen	O
et	O
al	O
,	O
2002	O
)	O
.	O
This	O
gives	O
automated	O
systems	O
a	O
huge	O
advantage	O
over	O
conventional	O
therapies	O
,	O
as	O
they	O
can	O
be	O
used	O
continuously	O
with	O
marginal	O
extra	O
cost	O
.	O
Health	O
assistants	O
that	O
can	O
deliver	O
therapy	O
,	O
have	O
gained	O
great	O
interest	O
in	O
recent	O
years	O
(	O
Bickmore	O
et	O
al	O
,	O
2005	O
;	O
Fitzpatrick	O
et	O
al	O
,	O
2017	O
)	O
.	O
These	O
systems	O
however	O
are	O
largely	O
based	O
on	O
hand	O
-	O
crafted	O
rules	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
main	O
research	O
effort	O
in	O
statistical	O
approaches	O
to	O
conversational	O
systems	O
has	O
focused	O
on	O
limited	O
-	O
domain	O
information	O
seeking	O
dialogues	O
(	O
Schatzmann	O
et	O
al	O
,	O
2006	O
;	O
Geist	O
and	O
Pietquin	O
,	O
2011	O
;	O
Gasic	O
and	O
Young	O
,	O
2014	O
;	O
Fatemi	O
et	O
al	O
,	O
2016	O
;	O
Li	O
et	O
al	O
,	O
2016	O
;	O
Williams	O
et	O
al	O
,	O
2017	O
)	O
.	O
In	O
this	O
paper	O
we	O
introduce	O
a	O
new	O
task	O
:	O
understanding	O
of	O
mental	O
health	O
concepts	O
derived	O
from	O
Cognitive	O
Behavioural	O
Therapy	O
(	O
CBT	B-DatasetName
)	O
.	O
We	O
present	O
an	O
ontology	B-MethodName
that	O
is	O
formulated	O
according	O
to	O
Cognitive	O
Behavioural	O
Therapy	O
principles	O
.	O
We	O
label	O
a	O
high	O
quality	O
mental	O
health	O
corpus	O
,	O
which	O
exhibits	O
targeted	O
psychological	O
phenomena	O
.	O
We	O
use	O
the	O
whole	O
unlabelled	O
dataset	O
to	O
train	O
distributed	O
representations	O
of	O
words	O
and	O
sentences	O
.	O
We	O
then	O
investigate	O
two	O
approaches	O
for	O
classifying	O
the	O
user	O
input	O
according	O
to	O
the	O
defined	O
ontology	B-MethodName
.	O
The	O
first	O
model	O
involves	O
a	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
operating	O
over	O
distributed	O
words	O
representations	O
.	O
The	O
second	O
involves	O
a	O
gated	O
recurrent	O
network	O
(	O
GRU	B-MethodName
)	O
operating	O
over	O
distributed	O
representation	O
of	O
sentences	O
.	O
Our	O
models	O
perform	O
significantly	O
better	O
than	O
chance	O
and	O
for	O
instances	O
with	O
a	O
large	O
number	O
of	O
data	O
they	O
reach	O
the	O
inter	O
-	O
annotator	O
agreement	O
.	O
This	O
understanding	O
module	O
will	O
be	O
an	O
essential	O
component	O
of	O
a	O
statistical	O
dialogue	O
system	O
delivering	O
therapy	O
.	O
The	O
paper	O
is	O
organised	O
as	O
follows	O
.	O
In	O
Section	O
2	O
we	O
give	O
a	O
brief	O
background	O
of	O
the	O
statistical	O
approach	O
to	O
dialogue	O
modelling	O
,	O
focusing	O
on	O
dialogue	O
ontology	B-MethodName
and	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
.	O
In	O
Section	O
3	O
we	O
review	O
related	O
work	O
in	O
the	O
area	O
of	O
automated	O
mental	O
-	O
health	O
assistants	O
.	O
The	O
sections	O
that	O
follow	O
represent	O
the	O
main	O
contribution	O
of	O
this	O
work	O
:	O
a	O
CBT	B-DatasetName
ontology	B-MethodName
in	O
Section	O
4	O
,	O
a	O
labelled	O
dataset	O
in	O
Section	O
5	O
,	O
and	O
models	O
for	O
language	O
understanding	O
in	O
Section	O
6	O
.	O
We	O
present	O
the	O
results	O
in	O
Section	O
7	O
and	O
our	O
conclusion	O
in	O
Section	O
8	O
.	O

A	O
dialogue	O
system	O
can	O
be	O
treated	O
as	O
a	O
trainable	O
statistical	O
model	O
suitable	O
for	O
goal	O
-	O
oriented	O
information	O
seeking	O
dialogues	O
(	O
Young	O
,	O
2002	O
)	O
.	O
In	O
these	O
dialogues	O
,	O
the	O
user	O
has	O
a	O
clear	O
goal	O
that	O
he	O
or	O
she	O
is	O
trying	O
to	O
achieve	O
and	O
this	O
involves	O
extracting	O
particular	O
information	O
from	O
a	O
back	O
-	O
end	O
database	O
.	O
A	O
structured	O
representation	O
of	O
the	O
database	O
,	O
the	O
ontology	B-MethodName
is	O
a	O
central	O
element	O
of	O
a	O
dialogue	O
system	O
.	O
It	O
defines	O
the	O
concepts	O
that	O
the	O
dialogue	O
system	O
can	O
understand	O
and	O
talk	O
about	O
.	O
Another	O
critical	O
component	O
is	O
the	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
unit	O
,	O
which	O
takes	O
textual	O
user	O
input	O
and	O
detects	O
presence	O
of	O
the	O
ontology	B-MethodName
concepts	O
in	O
the	O
text	O
.	O

Statistical	O
approaches	O
to	O
dialogue	O
modelling	O
have	O
been	O
applied	O
to	O
relatively	O
simple	O
domains	O
.	O
These	O
systems	O
interface	O
databases	O
of	O
up	O
to	O
1000	O
entities	O
where	O
each	O
entity	O
has	O
up	O
to	O
20	O
properties	O
,	O
i.e.	O
slots	O
(	O
Cuayáhuitl	O
,	O
2009	O
)	O
.	O
There	O
has	O
been	O
a	O
significant	O
amount	O
of	O
work	O
in	O
spoken	B-TaskName
language	I-TaskName
understanding	I-TaskName
focused	O
on	O
exploiting	O
large	O
knowledge	B-TaskName
graphs	I-TaskName
in	O
order	O
to	O
improve	O
coverage	O
(	O
Tür	O
et	O
al	O
,	O
2012	O
;	O
Heck	O
et	O
al	O
,	O
2013	O
)	O
.	O
Despite	O
these	O
efforts	O
,	O
little	O
work	O
has	O
been	O
done	O
on	O
mental	O
health	O
ontologies	O
for	O
supporting	O
cognitive	O
behavioural	O
therapy	O
on	O
dialogue	O
systems	O
.	O
Available	O
medical	O
ontologies	O
follow	O
a	O
symptom	O
-	O
treatment	O
categorisation	O
and	O
are	O
not	O
suitable	O
for	O
dialogue	O
or	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
(	O
Bluhm	O
,	O
2017	O
;	O
Hofmann	O
,	O
2014	O
;	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
.	O

Within	O
a	O
dialogue	O
system	O
,	O
a	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
unit	O
extracts	O
meaning	O
from	O
user	O
sentences	O
.	O
Both	O
classification	O
(	O
Mairesse	O
et	O
al	O
,	O
2009	O
)	O
and	O
sequence	O
-	O
to	O
-	O
sequence	O
(	O
Yao	O
et	O
al	O
,	O
2014	O
;	O
Mesnil	O
et	O
al	O
,	O
2015	O
)	O
models	O
have	O
been	O
applied	O
to	O
address	O
this	O
task	O
.	O
Deep	O
learning	O
architectures	O
that	O
exploit	O
distributed	O
word	O
-	O
vector	O
representations	O
have	O
been	O
successfully	O
applied	O
to	O
different	O
tasks	O
in	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
,	O
such	O
as	O
semantic	O
role	O
labelling	O
,	O
semantic	B-TaskName
parsing	I-TaskName
,	O
spoken	O
language	O
un	O
-	O
derstanding	O
,	O
sentiment	B-TaskName
analysis	I-TaskName
or	O
dialogue	O
belief	O
tracking	O
(	O
Collobert	O
et	O
al	O
,	O
2011	O
;	O
Kim	O
,	O
2014	O
;	O
Kalchbrenner	O
et	O
al	O
,	O
2014	O
;	O
Le	O
and	O
Mikolov	O
,	O
2014a	O
;	O
Rojas	O
Barahona	O
et	O
al	O
,	O
2016	O
;	O
Mrkšić	O
et	O
al	O
,	O
2017	O
)	O
.	O
In	O
this	O
work	O
we	O
consider	O
understanding	O
of	O
mental	O
health	O
concepts	O
of	O
as	O
a	O
classification	O
task	O
.	O
To	O
facilitate	O
this	O
process	O
,	O
we	O
use	O
distributed	O
representations	O
.	O

The	O
aim	O
of	O
building	O
an	O
automated	O
therapist	O
has	O
been	O
around	O
since	O
the	O
first	O
time	O
researchers	O
attempted	O
to	O
build	O
a	O
dialogue	O
system	O
(	O
Weizenbaum	O
,	O
1966	O
)	O
.	O
Automated	O
health	O
advice	O
systems	O
built	O
to	O
date	O
typically	O
rely	O
on	O
expert	O
coded	O
rules	O
and	O
have	O
limited	O
conversational	O
capabilities	O
(	O
Rojas	O
-	O
Barahona	O
and	O
Giorgino	O
,	O
2009	O
;	O
Vardoulakis	O
et	O
al	O
,	O
2012	O
;	O
Ring	O
et	O
al	O
,	O
2013	O
;	O
Riccardi	O
,	O
2014	O
;	O
DeVault	O
et	O
al	O
,	O
2014	O
;	O
Ring	O
et	O
al	O
,	O
2016	O
)	O
.	O
One	O
particular	O
system	O
that	O
we	O
would	O
like	O
to	O
highlight	O
is	O
an	O
affectively	O
aware	O
virtual	O
therapist	O
(	O
Ring	O
et	O
al	O
,	O
2016	O
)	O
.	O
This	O
system	O
is	O
based	O
on	O
Cognitive	O
Behavioural	O
Therapy	O
and	O
the	O
system	O
behaviour	O
is	O
scripted	O
using	O
VoiceXML	O
.	O
There	O
is	O
no	O
language	O
understanding	O
:	O
the	O
agent	B-DatasetName
simply	O
asks	O
questions	O
and	O
the	O
user	O
selects	O
answers	O
from	O
a	O
given	O
list	O
.	O
The	O
agent	B-DatasetName
is	O
however	O
able	O
to	O
interpret	O
hand	O
gestures	O
,	O
posture	O
shifts	O
,	O
and	O
facial	O
expressions	O
.	O
Another	O
notable	O
system	O
(	O
De	O
-	O
Vault	O
et	O
al	O
,	O
2014	O
)	O
has	O
a	O
multi	O
-	O
modal	O
perception	O
unit	O
which	O
captures	O
and	O
analyses	O
user	O
behaviour	O
for	O
both	O
behavioural	O
understanding	O
and	O
interaction	O
.	O
The	O
measurements	O
contribute	O
to	O
the	O
indicator	O
analysis	O
of	O
affect	O
,	O
gesture	O
,	O
emotion	B-DatasetName
and	O
engagement	O
.	O
Again	O
,	O
no	O
statistical	O
language	O
understanding	O
takes	O
place	O
and	O
the	O
behaviour	O
of	O
the	O
system	O
is	O
scripted	O
.	O
The	O
system	O
does	O
not	O
provide	O
therapy	O
to	O
the	O
user	O
but	O
is	O
rather	O
a	O
tool	O
that	O
can	O
support	O
healthcare	O
decisions	O
(	O
by	O
human	O
healthcare	O
professionals	O
)	O
.	O
The	O
Stanford	O
Woebot	O
chat	O
-	O
bot	O
proposed	O
by	O
(	O
Fitzpatrick	O
et	O
al	O
,	O
2017	O
)	O
is	O
designed	O
for	O
delivering	O
CBT	B-DatasetName
to	O
young	O
adults	O
with	O
depression	O
and	O
anxiety	O
.	O
It	O
has	O
been	O
shown	O
that	O
the	O
interaction	O
with	O
this	O
chat	O
-	O
bot	O
can	O
significantly	O
reduce	O
the	O
symptoms	O
of	O
depression	O
when	O
compared	O
to	O
a	O
group	O
of	O
people	O
directed	O
to	O
a	O
read	O
a	O
CBT	B-DatasetName
manual	O
.	O
The	O
conversational	O
agent	B-DatasetName
appears	O
to	O
be	O
effective	O
in	O
engaging	O
the	O
users	O
.	O
However	O
,	O
the	O
understanding	O
component	O
of	O
Woebot	O
has	O
not	O
been	O
fully	O
described	O
.	O
The	O
dialogue	O
decisions	O
are	O
based	O
on	O
decision	O
trees	O
.	O
At	O
each	O
node	O
,	O
the	O
user	O
is	O
expected	O
to	O
choose	O
one	O
of	O
several	O
predefined	O
responses	O
.	O
Limited	O
language	O
understanding	O
was	O
in	O
-	O
troduced	O
at	O
specific	O
points	O
in	O
the	O
tree	O
to	O
determine	O
routing	O
to	O
subsequent	O
conversational	O
nodes	O
.	O
Still	O
,	O
one	O
of	O
the	O
main	O
deficiencies	O
reported	O
by	O
the	O
trial	O
participants	O
in	O
(	O
Fitzpatrick	O
et	O
al	O
,	O
2017	O
)	O
was	O
the	O
inability	O
to	O
converse	B-DatasetName
naturally	O
.	O
Here	O
we	O
address	O
this	O
problem	O
by	O
performing	O
statistical	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
.	O

To	O
define	O
the	O
ontology	B-MethodName
we	O
draw	O
from	O
principles	O
of	O
Cognitive	O
Behavioural	O
Therapy	O
(	O
CBT	B-DatasetName
)	O
.	O
This	O
is	O
one	O
of	O
the	O
best	O
studied	O
psychotherapeutic	O
interventions	O
,	O
and	O
the	O
most	O
widely	O
used	O
psychological	O
treatment	O
for	O
mental	O
disorders	O
in	O
Britain	O
(	O
Bhasi	O
et	O
al	O
,	O
2013	O
)	O
.	O
There	O
is	O
evidence	O
that	O
CBT	B-DatasetName
is	O
more	O
effective	O
than	O
other	O
forms	O
of	O
psychotherapy	O
(	O
Tolin	O
,	O
2010	O
)	O
.	O
Unlike	O
other	O
,	O
longer	O
-	O
term	O
,	O
forms	O
of	O
therapy	O
such	O
as	O
psychoanalysis	O
,	O
CBT	B-DatasetName
can	O
have	O
a	O
positive	O
effect	O
on	O
the	O
client	O
within	O
a	O
few	O
sessions	O
.	O
Also	O
,	O
due	O
to	O
it	O
being	O
highly	O
structured	O
,	O
it	O
is	O
more	O
easily	O
amenable	O
by	O
computer	O
interpretation	O
.	O
This	O
is	O
why	O
we	O
adopted	O
CBT	B-DatasetName
as	O
the	O
basis	O
of	O
our	O
work	O
.	O
Cognitive	O
Behavioural	O
Therapy	O
is	O
derived	O
from	O
Cognitive	O
Therapy	O
model	O
theory	O
(	O
Beck	O
,	O
1976	O
;	O
Beck	O
et	O
al	O
,	O
1979	O
)	O
,	O
which	O
postulates	O
that	O
our	O
emotions	O
and	O
behaviour	O
are	O
influenced	O
by	O
the	O
way	O
we	O
think	O
and	O
by	O
how	O
we	O
make	O
sense	O
of	O
the	O
world	O
.	O
The	O
idea	O
is	O
that	O
,	O
if	O
the	O
client	O
changes	O
the	O
way	O
he	O
or	O
she	O
thinks	O
about	O
their	O
problem	O
,	O
this	O
will	O
in	O
turn	O
change	O
the	O
way	O
he	O
or	O
she	O
feels	O
,	O
and	O
behaves	O
.	O
A	O
major	O
underlying	O
principle	O
of	O
CBT	B-DatasetName
is	O
the	O
idea	O
of	O
cognitive	O
distortions	O
,	O
and	O
the	O
value	O
in	O
challenging	O
them	O
.	O
In	O
CBT	B-DatasetName
,	O
clients	O
are	O
helped	O
to	O
test	O
their	O
assumptions	O
and	O
views	O
of	O
the	O
world	O
in	O
order	O
to	O
check	O
if	O
they	O
fit	O
with	O
reality	O
.	O
When	O
clients	O
learn	O
that	O
their	O
perceptions	O
and	O
interpretations	O
are	O
distorted	O
or	O
unhelpful	O
they	O
then	O
work	O
on	O
correcting	O
them	O
.	O
Within	O
the	O
realm	O
of	O
cognitive	O
distortion	O
,	O
CBT	B-DatasetName
identifies	O
a	O
number	O
of	O
specific	O
self	O
-	O
defeating	O
thought	O
processes	O
,	O
or	O
thinking	O
errors	O
.	O
There	O
is	O
a	O
core	O
of	O
around	O
10	O
to	O
15	O
thinking	O
errors	O
,	O
with	O
their	O
exact	O
titles	O
having	O
some	O
fluidity	O
.	O
A	O
strong	O
component	O
of	O
CBT	B-DatasetName
is	O
teaching	O
clients	O
to	O
be	O
able	O
to	O
recognize	O
and	O
identify	O
the	O
thinking	O
errors	O
themselves	O
,	O
and	O
ultimately	O
discard	O
the	O
negative	O
thought	O
processes	O
and	O
're	O
-	O
think	O
'	O
their	O
problems	O
.	O
We	O
consider	O
the	O
main	O
analytical	O
step	O
in	O
this	O
therapy	O
:	O
an	O
adequate	O
decoding	O
of	O
these	O
'	O
thinking	O
error	O
'	O
concepts	O
,	O
and	O
the	O
identification	O
of	O
the	O
key	O
emotion	B-DatasetName
(	O
s	O
)	O
and	O
the	O
situational	O
context	O
of	O
a	O
particular	O
problem	O
.	O
Therefore	O
,	O
our	O
ontology	B-MethodName
consists	O
of	O
think	O
-	O
ing	O
errors	O
,	O
emotions	O
,	O
and	O
situations	O
.	O

Notwithstanding	O
slight	O
variations	O
in	O
number	O
and	O
terminology	O
,	O
the	O
list	O
of	O
thinking	O
errors	O
is	O
fairly	O
well	O
standardised	O
in	O
the	O
CBT	B-DatasetName
literature	O
.	O
We	O
present	O
one	O
such	O
list	O
in	O
Table	O
1	O
.	O
However	O
,	O
it	O
is	O
important	O
to	O
note	O
that	O
there	O
is	O
a	O
fair	O
degree	O
of	O
overlap	O
between	O
different	O
thinking	O
errors	O
,	O
for	O
example	O
,	O
between	O
Jumping	O
to	O
Negative	O
Conclusions	O
and	O
Fortune	O
Telling	O
,	O
or	O
between	O
Disqualifying	O
the	O
Positives	O
and	O
Mental	O
Filtering	O
.	O
In	O
addition	O
,	O
within	O
the	O
data	O
used	O
-	O
and	O
as	O
is	O
likely	O
to	O
be	O
the	O
case	O
in	O
any	O
data	O
of	O
spontaneous	O
expressions	O
of	O
psychological	O
upset	O
-	O
a	O
single	O
problem	O
can	O
exhibit	O
several	O
thinking	O
errors	O
simultaneously	O
.	O
Thus	O
,	O
the	O
situation	O
is	O
much	O
more	O
challenging	O
than	O
in	O
simple	O
information	O
-	O
seeking	O
dialogues	O
,	O
where	O
ontologies	O
are	O
typically	O
clearly	O
defined	O
and	O
there	O
is	O
no	O
or	O
very	O
little	O
overlap	O
between	O
concepts	O
.	O

In	O
addition	O
to	O
thinking	O
errors	O
,	O
we	O
define	O
a	O
set	O
of	O
emotions	O
.	O
We	O
mainly	O
focus	O
on	O
negative	O
emotions	O
,	O
relevant	O
to	O
people	O
in	O
psychological	O
distress	O
.	O
In	O
CBT	B-DatasetName
,	O
emotions	O
tend	O
to	O
be	O
divided	O
into	O
positive	O
and	O
negative	O
,	O
or	O
helpful	O
/	O
healthy	O
and	O
unhelpful/	O
unhealthy	O
emotions	O
(	O
Branch	O
and	O
Willson	O
,	O
2010	O
)	O
.	O
The	O
set	O
of	O
emotions	O
for	O
this	O
work	O
evolved	O
over	O
time	O
in	O
the	O
early	O
days	O
of	O
annotation	O
.	O
Although	O
we	O
initally	O
agreed	O
to	O
focus	O
on	O
'	O
unhealthy	O
'	O
emotions	O
,	O
as	O
defined	O
by	O
CBT	B-DatasetName
,	O
there	O
seemed	O
also	O
to	O
be	O
a	O
place	O
for	O
the	O
'	O
healthy	O
'	O
emotion	B-DatasetName
Grief	O
/	O
sadness	O
.	O
Overall	O
,	O
the	O
list	O
of	O
emotions	O
used	O
was	O
drawn	O
from	O
a	O
number	O
of	O
sources	O
,	O
including	O
CBT	B-DatasetName
literature	O
,	O
the	O
annotators	O
'	O
own	O
knowledge	O
of	O
what	O
they	O
work	O
with	O
in	O
psychological	O
therapy	O
,	O
and	O
the	O
common	O
emotions	O
that	O
were	O
seen	O
emerging	O
from	O
the	O
data	O
early	O
on	O
in	O
the	O
process	O
.	O
Note	O
that	O
more	O
than	O
one	O
emotion	B-DatasetName
might	O
be	O
expressed	O
within	O
an	O
individual	O
problem	O
-	O
for	O
example	O
Depression	O
and	O
Loneliness	O
.	O
The	O
list	O
of	O
emotions	O
is	O
given	O
in	O
Table	O
2	O
.	O

The	O
corpus	O
consists	O
of	O
500	O
K	O
written	O
posts	O
that	O
users	O
anonymously	O
posted	O
on	O
the	O
Koko	O
platform	O
1	O
.	O
This	O
platform	O
is	O
based	O
on	O
the	O
peer	O
-	O
to	O
-	O
peer	O
therapy	O
proposed	O
by	O
(	O
Morris	O
et	O
al	O
,	O
2015	O
)	O
.	O
In	O
this	O
set	O
-	O
up	O
,	O
a	O
user	O
anonymously	O
posts	O
their	O
problem	O
(	O
referred	O
to	O
1	O
https://itskoko.com/	O
as	O
the	O
problem	O
)	O
and	O
is	O
prompted	O
to	O
consider	O
their	O
most	O
negative	O
take	O
on	O
the	O
problem	O
(	O
referred	O
to	O
as	O
the	O
negative	O
take	O
)	O
.	O
Subsequently	O
,	O
peers	O
post	O
responses	O
that	O
attempt	O
to	O
offer	O
a	O
re	O
-	O
think	O
and	O
give	O
a	O
more	O
positive	O
angle	O
on	O
the	O
problem	O
.	O
When	O
first	O
developed	O
,	O
this	O
peer	O
-	O
to	O
-	O
peer	O
framework	O
was	O
shown	O
to	O
be	O
more	O
efficacious	O
than	O
expressive	O
writing	O
,	O
an	O
intervention	O
that	O
is	O
known	O
to	O
improve	O
physical	O
and	O
emotional	O
well	O
-	O
being	O
(	O
Morris	O
et	O
al	O
,	O
2015	O
)	O
.	O
Since	O
then	O
,	O
the	O
app	O
developed	O
by	O
Koko	O
has	O
collected	O
a	O
very	O
large	O
number	O
of	O
posts	O
and	O
associated	O
responses	O
.	O
Initially	O
,	O
any	O
first	O
-	O
time	O
Koko	O
user	O
would	O
be	O
given	O
a	O
short	O
introductory	O
tutorial	O
in	O
the	O
art	O
of	O
're	O
-	O
thinking'/'re	O
-	O
framing	O
'	O
problems	O
(	O
based	O
on	O
CBT	B-DatasetName
principles	O
)	O
,	O
before	O
being	O
able	O
to	O
use	O
the	O
platform	O
.	O
This	O
however	O
changed	O
over	O
time	O
,	O
as	O
the	O
age	O
of	O
the	O
users	O
decreased	O
,	O
and	O
a	O
different	O
tutorial	O
,	O
emphasizing	O
empathy	O
and	O
optimism	O
,	O
was	O
used	O
(	O
less	O
CBT	B-DatasetName
-	O
based	O
than	O
the	O
're	O
-	O
thinking	O
'	O
)	O
.	O
Most	O
of	O
the	O
data	O
annotated	O
in	O
this	O
study	O
was	O
drawn	O
from	O
the	O
earlier	O
phase	O
.	O
Figure	O
1	O
gives	O
an	O
annotated	O
post	O
example	O
.	O

A	O
subset	O
of	O
posts	O
was	O
annotated	O
by	O
two	O
psychological	O
therapists	O
using	O
a	O
web	O
annotation	O
tool	O
that	O
we	O
developed	O
.	O
The	O
annotation	O
tool	O
allowed	O
annotators	O
to	O
have	O
a	O
quick	O
view	O
of	O
the	O
posts	O
,	O
showing	O
up	O
to	O
50	O
posts	O
per	O
page	O
,	O
to	O
navigate	O
through	O
posts	O
,	O
to	O
check	O
pending	O
posts	O
and	O
to	O
annotate	O
them	O
by	O
adding	O
or	O
removing	O
thinking	O
errors	O
,	O
emotions	O
and	O
situations	O
.	O
All	O
annotations	O
were	O
stored	O
in	O
a	O
MySQL	O
database	O
.	O
Initially	O
1000	O
posts	O
were	O
analysed	O
.	O
These	O
were	O
used	O
to	O
define	O
the	O
ontology	B-MethodName
.	O
Then	O
4035	O
posts	O
were	O
labelled	O
with	O
thinking	O
errors	O
,	O
emotions	O
and	O
situations	O
.	O
It	O
takes	O
an	O
experienced	O
psychological	O
therapist	O
about	O
one	O
minute	O
to	O
annotate	O
one	O
post	O
.	O
Note	O
that	O
the	O
same	O
post	O
can	O
exhibit	O
multiple	O
thinking	O
errors	O
,	O
emotions	O
and	O
situations	O
,	O
which	O
makes	O
the	O
whole	O
process	O
more	O
complex	O
.	O
We	O
randomly	O
selected	O
50	O
posts	O
and	O
calculated	O
the	O
inter	O
-	O
annotator	O
agreement	O
.	O
The	O
inter	O
-	O
annotator	O
agreement	O
was	O
calculated	O
using	O
a	O
contingency	O
table	O
for	O
thinking	O
error	O
,	O
emotion	B-DatasetName
and	O
situation	O
,	O
showing	O
agreement	O
and	O
disagreement	O
between	O
the	O
two	O
annotators	O
.	O
Then	O
,	O
Cohen	O
's	O
kappa	O
was	O
calculated	O
discounting	O
the	O
possibility	O
that	O
the	O
agreement	O
may	O
happen	O
by	O
chance	O
.	O
The	O
result	O
is	O
shown	O
in	O
due	O
to	O
the	O
unbounded	O
number	O
of	O
thinking	O
errors	O
per	O
post	O
.	O
In	O
other	O
words	O
,	O
the	O
annotators	O
typically	O
have	O
three	O
or	O
four	O
thinking	O
errors	O
in	O
common	O
but	O
one	O
of	O
them	O
might	O
have	O
detected	O
one	O
or	O
two	O
more	O
.	O
Still	O
,	O
the	O
agreement	O
is	O
much	O
higher	O
than	O
chance	O
,	O
so	O
we	O
think	O
that	O
while	O
challenging	O
,	O
it	O
is	O
possible	O
to	O
build	O
a	O
classifier	O
for	O
this	O
task	O
.	O
The	O
distributions	O
of	O
labelled	O
posts	O
with	O
multiple	O
sub	O
-	O
categories	O
for	O
three	O
super	O
-	O
categories	O
are	O
shown	O
in	O
Figure	O
2	O
6	O
Deep	O
learning	O
model	O

The	O
task	O
of	O
decoding	O
thinking	O
errors	O
and	O
emotions	O
is	O
closely	O
related	O
to	O
the	O
task	O
of	O
sentiment	B-TaskName
analysis	I-TaskName
.	O
In	O
sentiment	B-TaskName
analysis	I-TaskName
we	O
are	O
concerned	O
with	O
positive	O
or	O
negative	O
sentiment	O
expressed	O
in	O
a	O
sentence	O
.	O
Detecting	O
thinking	O
errors	O
or	O
emotions	O
could	O
be	O
perceived	O
as	O
detecting	O
different	O
kinds	O
of	O
negative	O
sentiment	O
.	O
Distributed	O
representations	O
of	O
words	O
,	O
sentences	O
and	O
documents	O
have	O
gained	O
success	O
in	O
sentiment	O
detection	O
and	O
similarity	O
tasks	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014a	O
;	O
Maas	O
et	O
al	O
,	O
2011	O
;	O
Kiros	O
et	O
al	O
,	O
2015	O
)	O
.	O
A	O
key	O
advantage	O
of	O
these	O
representations	O
is	O
that	O
they	O
can	O
be	O
obtained	O
in	O
an	O
unsupervised	O
manner	O
,	O
thus	O
allowing	O
exploitation	O
of	O
large	O
amounts	O
of	O
unlabelled	O
data	O
.	O
This	O
is	O
precisely	O
what	O
we	O
have	O
in	O
our	O
set	O
-	O
up	O
,	O
where	O
only	O
a	O
small	O
portion	O
of	O
our	O
posts	O
is	O
labelled	O
.	O
We	O
utilise	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
word	O
vectors	O
,	O
which	O
have	O
previously	O
achieved	O
competitive	O
results	O
in	O
a	O
similarity	O
task	O
.	O
We	O
train	O
the	O
word	O
vectors	O
on	O
the	O
whole	O
dataset	O
and	O
then	O
use	O
a	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
to	O
extract	O
features	O
from	O
posts	O
where	O
words	O
are	O
represented	O
as	O
vectors	O
.	O
We	O
also	O
consider	O
distributed	O
representation	O
of	O
sentences	O
.	O
A	O
particularly	O
competitive	O
model	O
is	O
the	O
skip	O
-	O
thought	O
model	O
,	O
which	O
is	O
obtained	O
from	O
an	O
encoder	O
-	O
decoder	O
model	O
that	O
tries	O
to	O
reconstruct	O
the	O
surrounding	O
sentences	O
of	O
an	O
encoded	O
passage	O
(	O
Kiros	O
et	O
al	O
,	O
2015	O
)	O
.	O
On	O
similarity	O
tasks	O
it	O
outperfoms	O
the	O
simpler	O
doc2vec	O
model	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014a	O
)	O
.	O
An	O
approach	O
that	O
represents	O
vectors	O
by	O
weighted	O
averages	O
of	O
word	O
vectors	O
and	O
then	O
modifies	O
them	O
using	O
PCA	B-MethodName
and	O
SVD	B-DatasetName
outperforms	O
skipthought	O
vectors	O
(	O
Arora	O
et	O
al	O
,	O
2017	O
)	O
.	O
This	O
method	O
however	O
does	O
not	O
do	O
well	O
on	O
a	O
sentiment	B-TaskName
analysis	I-TaskName
task	O
due	O
to	O
down	O
-	O
weighting	O
of	O
words	O
like	O
"	O
not	O
"	O
.	O
As	O
these	O
often	O
appear	O
in	O
our	O
corpus	O
,	O
we	O
chose	O
skipthought	O
vectors	O
for	O
investigation	O
here	O
.	O
The	O
skip	O
-	O
thought	O
model	O
allows	O
a	O
dense	O
representation	O
of	O
the	O
utterance	O
.	O
We	O
train	O
skip	O
-	O
thought	O
vectors	O
using	O
the	O
method	O
described	O
in	O
(	O
Kiros	O
et	O
al	O
,	O
2015	O
)	O
.	O
The	O
automatically	O
generated	O
post	O
shown	O
in	O
Fig	O
3	O
demonstrates	O
that	O
skip	O
-	O
thought	O
vectors	O
can	O
convey	O
the	O
sentiment	O
well	O
in	O
accordance	O
to	O
context	O
.	O
We	O
then	O
train	O
a	O
gated	B-MethodName
recurrent	I-MethodName
unit	I-MethodName
(	O
GRU	B-MethodName
)	O
network	O
using	O
the	O
skip	O
-	O
thoughts	O
as	O
input	O
.	O
i	O
'	O
m	O
so	O
depressed	O
.	O
i	O
'	O
m	O
worthless	O
.	O
No	O
one	O
likes	O
me	O
i	O
'	O
m	O
try	O
being	O
nice	O
but	O
.	O
No	O
light	O
at	O
every	O
point	O
i	O
'	O
m	O
unpopular	O
and	O
i	O
'	O
m	O
a	O
<	O
NUM	O
>	O
year	O
old	O
potato	O
.	O
my	O
most	O
negative	O
take	O
is	O
that	O
i	O
'll	O
never	O
know	O
how	O
to	O
be	O
as	O
socially	O
as	O
a	O
quiet	O
girl	O
.	O
i	O
will	O
stop	O
talking	O
to	O
how	O
fragile	O
is	O
and	O
be	O
any	O
ways	O
of	O
normal	O
people	O
.	O
Figure	O
3	O
:	O
An	O
example	O
of	O
a	O
generated	O
post	O
using	O
skipthought	O
vectors	O
initialised	O
with	O
"	O
I	O
'm	O
so	O
depressed	O
"	O
.	O

The	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
model	O
is	O
preferred	O
over	O
a	O
recurrent	O
neural	O
network	O
(	O
RNN	O
)	O
model	O
,	O
because	O
the	O
posts	O
are	O
generally	O
too	O
long	O
for	O
an	O
RNN	O
to	O
maintain	O
memory	O
over	O
words	O
.	O
The	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
used	O
in	O
this	O
work	O
is	O
inspired	O
by	O
(	O
Kim	O
,	O
2014	O
)	O
and	O
operates	O
over	O
pre	O
-	O
trained	O
GloVe	B-MethodName
embeddings	I-MethodName
of	O
dimensionality	O
d.	O
As	O
shown	O
in	O
Fig	O
4	O
,	O
the	O
network	O
has	O
two	O
inputs	O
,	O
one	O
for	O
the	O
problem	O
and	O
the	O
other	O
for	O
the	O
negative	O
take	O
.	O
These	O
are	O
represented	O
as	O
two	O
tensors	O
.	O
A	O
convolutional	O
operation	O
involves	O
a	O
filter	O
w	O
R	O
ld	O
which	O
is	O
applied	O
to	O
l	O
words	O
to	O
produce	O
the	O
feature	O
map	O
.	O
Then	O
,	O
a	O
max	O
-	O
pooling	O
operation	O
is	O
applied	O
to	O
produce	O
two	O
vectors	O
:	O
p	O
for	O
problem	O
and	O
n	O
for	O
negative	O
take	O
.	O
The	O
reason	O
for	O
this	O
is	O
that	O
the	O
negative	O
take	O
is	O
usually	O
a	O
summary	O
of	O
the	O
post	O
,	O
carrying	O
stronger	O
sentiment	O
(	O
see	O
Figure	O
1	O
)	O
.	O
We	O
use	O
a	O
gating	O
mechanism	O
to	O
combine	O
p	O
and	O
n	O
as	O
follows	O
:	O
g	O
=	O
σ	O
(	O
W	O
p	O
p	O
+	O
W	O
n	O
n	O
+	O
b	O
)	O
(	O
1	O
)	O
h	O
=	O
g	O
p	O
+	O
(	O
1	O
−	O
g	O
)	O
n	O
(	O
2	O
)	O
Here	O
,	O
σ	O
is	O
the	O
sigmoid	O
function	O
,	O
W	O
p	O
,	O
W	O
n	O
and	O
W	O
are	O
weight	O
matrices	O
,	O
b	O
is	O
a	O
bias	O
term	O
,	O
1	O
is	O
a	O
vector	O
of	O
ones	O
,	O
is	O
the	O
element	O
-	O
wise	O
product	O
,	O
and	O
g	O
is	O
the	O
output	O
of	O
the	O
gating	O
mechanism	O
.	O
The	O
extracted	O
feature	O
h	O
is	O
then	O
processed	O
with	O
a	O
one	O
-	O
layer	O
fullyconnected	O
neural	O
network	O
(	O
FNN	O
)	O
to	O
perform	O
binary	O
classification	O
.	O
The	O
model	O
is	O
illustrated	O
in	O
Fig	O
4	O
.	O

We	O
use	O
the	O
gated	B-MethodName
recurrent	I-MethodName
unit	I-MethodName
(	O
GRU	B-MethodName
)	O
model	O
to	O
process	O
skip	O
-	O
thought	O
sentence	O
vectors	O
,	O
for	O
two	O
reasons	O
.	O
First	O
,	O
most	O
posts	O
contain	O
less	O
than	O
5	O
sentences	O
,	O
so	O
a	O
recurrent	O
neural	O
network	O
is	O
more	O
suitable	O
than	O
a	O
convolutional	O
neural	O
network	O
.	O
Second	O
,	O
since	O
our	O
corpus	O
only	O
comprises	O
very	O
limited	O
labelled	O
data	O
,	O
a	O
GRU	B-MethodName
should	O
perform	O
better	O
than	O
a	O
long	B-MethodName
short	I-MethodName
-	I-MethodName
term	I-MethodName
memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
network	O
as	O
it	O
has	O
less	O
parameters	O
.	O
Denote	O
each	O
post	O
as	O
P	O
=	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
...	O
,	O
s	O
t	O
,	O
...	O
}	O
,	O
where	O
s	O
t	O
is	O
the	O
t	O
th	O
sentence	O
in	O
post	O
P	O
.	O
First	O
,	O
we	O
use	O
an	O
already	O
trained	O
GRU	B-MethodName
to	O
extract	O
skip	O
-	O
thought	O
embeddings	O
e	O
t	O
from	O
the	O
sentences	O
s	O
t	O
.	O
Then	O
,	O
taking	O
the	O
sequence	O
of	O
sentence	O
vectors	O
{	O
e	O
1	O
,	O
e	O
2	O
,	O
...	O
,	O
e	O
t	O
,	O
...	O
}	O
as	O
input	O
,	O
another	O
GRU	B-MethodName
is	O
used	O
as	O
follows	O
:	O
z	O
t	O
=	O
σ	O
(	O
W	O
z	O
h	O
t−1	O
+	O
U	O
z	O
e	O
t	O
+	O
b	O
z	O
)	O
(	O
3	O
)	O
r	O
t	O
=	O
σ	O
(	O
W	O
r	O
h	O
t−1	O
+	O
U	O
r	O
e	O
t	O
+	O
b	O
r	O
)	O
(	O
4	O
)	O
h	O
t	O
=	O
tanh	O
(	O
W	O
(	O
r	O
t	O
h	O
t−1	O
)	O
+	O
Ue	O
t	O
+	O
b	O
h	O
)	O
(	O
5	O
)	O
h	O
t	O
=	O
z	O
t	O
h	O
t−1	O
+	O
(	O
1	O
−	O
z	O
t	O
)	O
h	O
t	O
(	O
6	O
)	O
W	O
z	O
,	O
U	O
z	O
,	O
W	O
r	O
,	O
U	O
r	O
,	O
W	O
,	O
U	O
are	O
recurrent	O
weight	O
ma	O
-	O
trices	O
,	O
b	O
z	O
,	O
b	O
r	O
,	O
b	O
h	O
are	O
bias	O
terms	O
,	O
is	O
the	O
elementwise	O
dot	O
product	O
,	O
and	O
σ	O
is	O
the	O
sigmoid	O
function	O
.	O
Finally	O
,	O
the	O
last	O
hidden	O
state	O
h	O
T	O
is	O
fed	O
into	O
a	O
FNN	O
with	O
one	O
hidden	O
layer	O
of	O
the	O
same	O
size	O
as	O
input	O
.	O
The	O
model	O
is	O
illustrated	O
in	O
Fig	O
5	O
.	O

For	O
rule	O
-	O
based	O
models	O
,	O
we	O
chose	O
a	O
chance	O
classifier	O
and	O
a	O
majority	O
classifier	O
,	O
where	O
all	O
the	O
posts	O
are	O
treated	O
as	O
positive	O
examples	O
for	O
each	O
class	O
.	O
In	O
addition	O
,	O
we	O
trained	O
two	O
non	O
-	O
deep	O
-	O
learning	O
models	O
,	O
the	O
logistic	B-MethodName
regression	I-MethodName
(	O
LR	O
)	O
model	O
and	O
the	O
Support	B-MethodName
Vector	I-MethodName
Machine	I-MethodName
(	O
SVM	B-MethodName
)	O
.	O
Both	O
of	O
them	O
take	O
the	O
bag	O
-	O
of	O
-	O
words	O
feature	O
as	O
input	O
and	O
implemented	O
in	O
sklearn	O
(	O
Pedregosa	O
et	O
al	O
,	O
2011	O
)	O
.	O
For	O
completeness	O
,	O
we	O
also	O
trained	O
100	O
and	O
300	O
dimensions	O
PV	O
-	O
DM	O
document	O
embeddings	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014b	O
)	O
as	O
the	O
distributed	O
representations	O
of	O
the	O
posts	O
using	O
the	O
gensim	O
toolkit	O
(	O
Řehůřek	O
and	O
Sojka	O
,	O
2010	O
)	O
,	O
and	O
employ	O
FNNs	O
to	O
do	O
the	O
classification	O
,	O
the	O
hidden	O
size	O
is	O
set	O
as	O
800	O
to	O
ensure	O
parameters	O
of	O
all	O
deep	O
learning	O
models	O
comparable	O
.	O
All	O
the	O
baseline	O
models	O
are	O
trained	O
with	O
the	O
same	O
set	O
-	O
up	O
as	O
described	O
in	O
section	O
6.4	O
.	O

We	O
presented	O
an	O
ontology	B-MethodName
based	O
on	O
the	O
principles	O
of	O
Cognitive	O
Behavioural	O
Therapy	O
.	O
We	O
then	O
annotated	O
data	O
that	O
exhibits	O
psychological	O
problems	O
and	O
computed	O
the	O
inter	O
-	O
annotator	O
agreement	O
.	O
We	O
found	O
that	O
classifying	O
thinking	O
errors	O
is	O
a	O
difficult	O
task	O
as	O
suggested	O
by	O
the	O
low	O
inter	O
-	O
annotator	O
agreement	O
.	O
We	O
trained	O
GloVe	B-MethodName
word	B-TaskName
embeddings	I-TaskName
and	O
skip	O
-	O
thought	O
embeddings	O
on	O
500	O
K	O
posts	O
in	O
an	O
unsupervised	O
fashion	O
and	O
generated	O
distributed	O
representations	O
both	O
of	O
words	O
and	O
of	O
sentences	O
.	O
We	O
then	O
used	O
the	O
GloVe	B-MethodName
word	O
vectors	O
as	O
input	O
to	O
a	O
CNN	O
and	O
the	O
skip	O
-	O
thought	O
sentence	O
vectors	O
as	O
input	O
to	O
a	O
GRU	B-MethodName
.	O
The	O
results	O
suggest	O
that	O
both	O
models	O
significantly	O
outperform	O
a	O
chance	O
classifier	O
for	O
all	O
thinking	O
errors	O
,	O
emotions	O
and	O
situations	O
with	O
CNN	O
-	O
GloVe	B-MethodName
on	O
average	O
achieving	O
better	O
results	O
.	O
Areas	O
of	O
future	O
investigation	O
include	O
richer	O
dis	O
-	O
tributed	O
representations	O
,	O
or	O
a	O
fusion	O
of	O
distributed	O
representations	O
from	O
word	O
-	O
level	O
,	O
sentence	O
-	O
level	O
and	O
document	O
-	O
level	O
,	O
to	O
acquire	O
more	O
powerful	O
semantic	O
features	O
.	O
We	O
also	O
plan	O
to	O
extend	O
the	O
current	O
ontology	B-MethodName
with	O
its	O
focus	O
on	O
thinking	O
errors	O
,	O
emotions	O
and	O
situations	O
to	O
include	O
a	O
much	O
lager	O
number	O
of	O
concepts	O
.	O
The	O
development	O
of	O
a	O
statistical	O
system	O
delivering	O
therapy	O
will	O
moreover	O
require	O
further	O
research	O
on	O
other	O
modules	O
of	O
a	O
dialogue	O
system	O
.	O

A	O
Transparent	O
Framework	O
for	O
Evaluating	O
Unintended	O
Demographic	O
Bias	O
in	O
Word	B-TaskName
Embeddings	I-TaskName

Word	O
embedding	O
models	O
have	O
gained	O
a	O
lot	O
of	O
traction	O
in	O
the	O
Natural	O
Language	O
Processing	O
community	O
,	O
however	O
,	O
they	O
suffer	O
from	O
unintended	O
demographic	O
biases	O
.	O
Most	O
approaches	O
to	O
evaluate	O
these	O
biases	O
rely	O
on	O
vector	O
space	O
based	O
metrics	O
like	O
the	O
Word	O
Embedding	O
Association	O
Test	O
(	O
WEAT	O
)	O
.	O
While	O
these	O
approaches	O
offer	O
great	O
geometric	O
insights	O
into	O
unintended	O
biases	O
in	O
the	O
embedding	O
vector	O
space	O
,	O
they	O
fail	O
to	O
offer	O
an	O
interpretable	O
meaning	O
for	O
how	O
the	O
embeddings	O
could	O
cause	O
discrimination	O
in	O
downstream	O
NLP	O
applications	O
.	O
In	O
this	O
work	O
,	O
we	O
present	O
a	O
transparent	O
framework	O
and	O
metric	O
for	O
evaluating	O
discrimination	O
across	O
protected	O
groups	O
with	O
respect	O
to	O
their	O
word	O
embedding	O
bias	O
.	O
Our	O
metric	O
(	O
Relative	O
Negative	O
Sentiment	O
Bias	O
,	O
RNSB	O
)	O
measures	O
fairness	O
in	O
word	B-TaskName
embeddings	I-TaskName
via	O
the	O
relative	O
negative	O
sentiment	O
associated	O
with	O
demographic	O
identity	O
terms	O
from	O
various	O
protected	O
groups	O
.	O
We	O
show	O
that	O
our	O
framework	O
and	O
metric	O
enable	O
useful	O
analysis	O
into	O
the	O
bias	O
in	O
word	B-TaskName
embeddings	I-TaskName
.	O

Word	B-TaskName
embeddings	I-TaskName
have	O
established	O
themselves	O
as	O
an	O
integral	O
part	O
of	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
applications	O
.	O
Unfortunately	O
word	B-TaskName
embeddings	I-TaskName
have	O
also	O
introduced	O
unintended	O
biases	O
that	O
could	O
cause	O
downstream	O
NLP	O
systems	O
to	O
be	O
unfair	O
.	O
Recent	O
studies	O
have	O
shown	O
that	O
word	B-TaskName
embeddings	I-TaskName
exhibit	O
unintended	O
gender	O
and	O
stereotype	O
biases	O
inherent	O
in	O
the	O
training	O
corpus	O
.	O
Bias	O
can	O
be	O
defined	O
as	O
an	O
unfair	O
expression	O
of	O
prejudice	O
for	O
or	O
against	O
a	O
person	O
,	O
a	O
group	O
,	O
or	O
an	O
idea	O
.	O
Bias	O
is	O
a	O
broad	O
term	O
,	O
which	O
covers	O
a	O
range	O
of	O
problems	O
particularly	O
relevant	O
in	O
natural	O
language	O
systems	O
such	O
as	O
,	O
discriminatory	O
gender	O
bias	O
(	O
Bolukbasi	O
et	O
al	O
,	O
2016a	O
;	O
Zhao	O
et	O
al	O
,	O
2017	O
)	O
,	O
bias	O
against	O
regionally	O
accented	O
speech	O
(	O
Najafian	O
et	O
al	O
,	O
2016	O
(	O
Najafian	O
et	O
al	O
,	O
,	O
2017	O
,	O
personal	O
or	O
political	O
view	O
bias	O
(	O
Iyyer	O
et	O
al	O
,	O
2014	O
;	O
Recasens	O
et	O
al	O
,	O
2013	O
)	O
,	O
and	O
many	O
other	O
examples	O
.	O
In	O
Figure	O
1	O
:	O
2	O
-	O
D	O
PCA	B-MethodName
embeddings	O
for	O
positive	O
/	O
negative	O
sentiment	O
words	O
and	O
a	O
set	O
of	O
national	O
origin	O
identity	O
terms	O
.	O
Geometrically	O
,	O
it	O
is	O
difficult	O
to	O
parse	O
how	O
these	O
embeddings	O
can	O
lead	O
to	O
discrimination	O
.	O
our	O
work	O
,	O
we	O
restrict	O
our	O
definition	O
of	O
bias	O
to	O
unequal	O
distributions	O
of	O
negative	O
sentiment	O
among	O
demographic	O
identity	O
terms	O
in	O
word	B-TaskName
embeddings	I-TaskName
.	O
One	O
could	O
also	O
look	O
at	O
unequal	O
distributions	O
of	O
positive	O
sentiment	O
,	O
but	O
for	O
this	O
work	O
we	O
restrict	O
ourselves	O
to	O
the	O
negative	O
case	O
.	O
Sentiment	B-TaskName
analysis	I-TaskName
makes	O
up	O
a	O
large	O
portion	O
of	O
current	O
NLP	O
systems	O
.	O
Therefore	O
,	O
preventing	O
negative	O
sentiment	O
from	O
mixing	O
with	O
sensitive	O
attributes	O
(	O
i.e.	O
race	O
,	O
gender	O
,	O
religion	O
)	O
in	O
word	B-TaskName
embeddings	I-TaskName
is	O
needed	O
to	O
prevent	O
discrimination	O
in	O
ML	O
models	O
using	O
the	O
embeddings	O
.	O
As	O
studied	O
in	O
(	O
Packer	O
et	O
al	O
,	O
2018	O
)	O
,	O
unintentionally	O
biased	O
word	B-TaskName
embeddings	I-TaskName
can	O
have	O
adverse	O
consequences	O
when	O
deployed	O
in	O
applications	O
,	O
such	O
as	O
movie	O
sentiment	O
analyzers	O
or	O
messaging	O
apps	O
.	O
Negative	O
sentiment	O
can	O
be	O
unfairly	O
entangled	O
in	O
the	O
word	B-TaskName
embeddings	I-TaskName
,	O
and	O
detecting	O
this	O
unintended	O
bias	O
is	O
a	O
difficult	O
problem	O
.	O
We	O
need	O
clear	O
signals	O
to	O
evaluate	O
which	O
groups	O
are	O
discriminated	O
against	O
due	O
to	O
the	O
bias	O
in	O
an	O
embedding	O
model	O
.	O
That	O
way	O
we	O
can	O
pinpoint	O
where	O
to	O
mitigate	O
those	O
biases	O
.	O
To	O
demonstrate	O
this	O
need	O
for	O
clear	O
signals	O
of	O
bias	O
in	O
word	B-TaskName
embeddings	I-TaskName
,	O
we	O
look	O
at	O
Figure	O
1	O
.	O
Figure	O
1	O
shows	O
a	O
2D	O
word	O
embedding	O
projection	O
of	O
positive	O
sentiment	O
(	O
green	O
)	O
and	O
negative	O
sentiment	O
(	O
red	O
)	O
words	O
.	O
It	O
would	O
be	O
unfair	O
for	O
any	O
given	O
demographic	O
identity	O
word	O
vector	O
(	O
blue	O
)	O
to	O
be	O
more	O
semantically	O
related	O
to	O
negative	O
terms	O
than	O
the	O
other	O
identities	O
.	O
However	O
,	O
many	O
identity	O
terms	O
exist	O
closer	O
to	O
negative	O
words	O
than	O
other	O
identity	O
terms	O
in	O
the	O
vector	O
space	O
.	O
This	O
bias	O
may	O
affect	O
a	O
downstream	O
ML	O
model	O
,	O
but	O
the	O
vector	O
space	O
has	O
no	O
absolute	O
interpretable	O
meaning	O
,	O
especially	O
when	O
it	O
comes	O
to	O
whether	O
this	O
embedding	O
model	O
will	O
lead	O
to	O
a	O
unfairly	O
discriminative	O
algorithm	O
.	O
Our	O
framework	O
enables	O
transparent	O
insights	O
into	O
word	O
embedding	O
bias	O
by	O
instead	O
viewing	O
the	O
output	O
of	O
a	O
simple	O
logistic	B-MethodName
regression	I-MethodName
algorithm	O
trained	O
on	O
an	O
unbiased	O
positive	O
/	O
negative	O
word	O
sentiment	O
dataset	O
initialized	O
with	O
biased	O
word	O
vectors	O
.	O
We	O
use	O
this	O
framework	O
to	O
create	O
a	O
clear	O
metric	O
for	O
unintended	O
demographic	O
bias	O
in	O
word	B-TaskName
embeddings	I-TaskName
.	O

Researchers	O
have	O
found	O
a	O
variety	O
of	O
ways	O
in	O
which	O
dangerous	O
unintended	O
bias	O
can	O
show	O
up	O
in	O
NLP	O
applications	O
(	O
Blodgett	O
and	O
O'Connor	O
,	O
2017	O
;	O
Hovy	O
and	O
Spruit	O
,	O
2016	O
;	O
Tatman	O
,	O
2017	O
)	O
.	O
Mitigating	O
such	O
biases	O
is	O
a	O
difficult	O
problem	O
,	O
and	O
researchers	O
have	O
created	O
many	O
ways	O
to	O
make	O
fairer	O
NLP	O
applications	O
.	O
Much	O
of	O
the	O
focus	O
for	O
mitigating	O
unintended	O
bias	O
in	O
NLP	O
is	O
either	O
targeted	O
at	O
reducing	O
gender	O
stereotypes	O
in	O
text	O
(	O
Bolukbasi	O
et	O
al	O
,	O
2016b	O
,	O
a	O
;	O
Zhao	O
et	O
al	O
,	O
2017	O
;	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
,	O
or	O
inequality	O
of	O
sentiment	O
or	O
toxicity	O
for	O
various	O
protected	O
groups	O
(	O
Caliskan	O
-	O
Islam	O
et	O
al	O
,	O
2016	O
;	O
Bakarov	O
,	O
2018	O
;	O
Dixon	O
et	O
al	O
;	O
Garg	O
et	O
al	O
,	O
2018	O
;	O
Kiritchenko	O
and	O
Mohammad	O
,	O
2018	O
)	O
.	O
More	O
specifically	O
,	O
word	B-TaskName
embeddings	I-TaskName
has	O
been	O
an	O
area	O
of	O
focus	O
for	O
evaluating	O
unintended	O
bias	O
.	O
(	O
Bolukbasi	O
et	O
al	O
,	O
2016b	O
)	O
defines	O
a	O
useful	O
metric	O
for	O
identifying	O
gender	O
bias	O
and	O
(	O
Caliskan	O
-	O
Islam	O
et	O
al	O
,	O
2016	O
)	O
defines	O
a	O
metric	O
called	O
the	O
WEAT	O
score	O
for	O
evaluating	O
unfair	O
correlations	O
with	O
sentiment	O
for	O
various	O
demographics	O
in	O
text	O
.	O
Unfortunately	O
metrics	O
like	O
these	O
leverage	O
vector	O
space	O
arguments	O
between	O
only	O
two	O
identities	O
at	O
a	O
time	O
like	O
man	O
vs	O
woman	O
(	O
Bolukbasi	O
et	O
al	O
,	O
2016a	O
)	O
,	O
or	O
European	O
American	O
names	O
vs.	O
African	O
American	O
names	O
(	O
Caliskan	O
-	O
Islam	O
et	O
al	O
,	O
2016	O
)	O
.	O
Though	O
geometrically	O
intuitive	O
,	O
these	O
tests	O
do	O
not	O
have	O
a	O
direct	O
relation	O
to	O
discrimination	O
in	O
general	O
.	O
Our	O
framework	O
and	O
RNSB	O
metric	O
enable	O
a	O
clear	O
evaluation	O
of	O
discrimination	O
with	O
respect	O
to	O
word	O
embedding	O
bias	O
for	O
a	O
whole	O
class	O
of	O
demographics	O
.	O

We	O
present	O
our	O
framework	O
for	O
understanding	O
and	O
evaluating	O
unintentional	O
demographic	O
bias	O
in	O
word	B-TaskName
embeddings	I-TaskName
.	O
We	O
first	O
describe	O
the	O
flow	O
of	O
our	O
framework	O
.	O
Then	O
,	O
we	O
address	O
which	O
datasets	O
/	O
models	O
were	O
chosen	O
for	O
our	O
approach	O
.	O
Finally	O
,	O
we	O
show	O
how	O
our	O
framework	O
can	O
enable	O
analysis	O
and	O
new	O
metrics	O
like	O
RNSB	O
.	O

Figure	O
2	O
:	O
We	O
isolate	O
unintended	O
bias	O
to	O
the	O
word	B-TaskName
embeddings	I-TaskName
by	O
training	O
a	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
on	O
a	O
unbiased	O
positive	O
/	O
negative	O
word	O
sentiment	O
dataset	O
(	O
initialized	O
with	O
the	O
biased	O
word	B-TaskName
embeddings	I-TaskName
)	O
.	O
We	O
measure	O
word	O
embedding	O
bias	O
by	O
analyzing	O
the	O
predicted	O
probability	O
of	O
negative	O
sentiment	O
for	O
identity	O
terms	O
.	O
Our	O
framework	O
enables	O
the	O
evaluation	O
of	O
unintended	O
bias	O
in	O
word	B-TaskName
embeddings	I-TaskName
through	O
the	O
results	O
of	O
negative	O
sentiment	O
predictions	O
.	O
Our	O
framework	O
has	O
a	O
simple	O
layout	O
.	O
Figure	O
2	O
shows	O
the	O
flow	O
of	O
our	O
system	O
.	O
We	O
first	O
use	O
the	O
embedding	O
model	O
we	O
are	O
trying	O
to	O
evaluate	O
to	O
initialize	O
vectors	O
for	O
an	O
unbiased	O
positive	O
/	O
negative	O
word	O
sentiment	O
dataset	O
.	O
Using	O
this	O
dataset	O
,	O
we	O
train	O
a	O
logistic	O
classification	O
algorithm	O
to	O
predict	O
the	O
probability	O
of	O
any	O
word	O
being	O
a	O
negative	O
sentiment	O
word	O
.	O
After	O
training	O
,	O
we	O
take	O
a	O
set	O
of	O
neutral	O
identity	O
terms	O
from	O
a	O
protected	O
group	O
(	O
i.e.	O
national	O
origin	O
)	O
and	O
predict	O
the	O
probability	O
of	O
negative	O
sentiment	O
for	O
each	O
word	O
in	O
the	O
set	O
.	O
Neutral	O
identity	O
terms	O
that	O
are	O
unfairly	O
entangled	O
with	O
negative	O
sentiment	O
in	O
the	O
word	B-TaskName
embeddings	I-TaskName
will	O
be	O
classified	O
like	O
their	O
neighboring	O
sentiment	O
words	O
from	O
the	O
sentiment	O
dataset	O
.	O
We	O
leverage	O
this	O
set	O
of	O
negative	O
sentiment	O
probabilities	O
to	O
summarize	O
unintended	O
demographic	O
bias	O
using	O
RNSB	O
.	O

We	O
evaluate	O
three	O
pretrained	O
embedding	O
models	O
:	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
Word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
(	O
trained	O
on	O
the	O
large	O
Google	B-DatasetName
News	O
corpus	O
)	O
,	O
and	O
ConceptNet	B-DatasetName
.	O
GloVe	B-MethodName
and	O
Word2vec	O
embeddings	O
have	O
been	O
shown	O
to	O
contain	O
unintended	O
bias	O
in	O
(	O
Bolukbasi	O
et	O
al	O
,	O
2016a	O
;	O
Caliskan	O
-	O
Islam	O
et	O
al	O
,	O
2016	O
)	O
.	O
ConceptNet	B-DatasetName
has	O
been	O
shown	O
to	O
be	O
less	O
biased	O
than	O
these	O
models	O
(	O
Speer	O
,	O
2017	O
)	O
due	O
to	O
the	O
mixture	O
of	O
curated	O
corpora	O
used	O
for	O
training	O
.	O
As	O
part	O
of	O
our	O
pipeline	O
,	O
we	O
also	O
use	O
a	O
labeled	O
positive	O
/	O
negative	O
sentiment	O
training	O
set	O
(	O
Hu	O
and	O
Liu	O
,	O
2004	O
)	O
.	O
This	O
dataset	O
has	O
been	O
shown	O
to	O
be	O
a	O
trustworthy	O
lexicon	O
for	O
negative	O
and	O
positive	O
sentiment	O
words	O
(	O
Pang	O
et	O
al	O
,	O
2008	O
;	O
Liu	O
,	O
2012	O
;	O
Wilson	O
et	O
al	O
,	O
2005	O
)	O
.	O
We	O
trust	O
these	O
labels	O
to	O
be	O
unbiased	O
so	O
that	O
we	O
may	O
isolate	O
the	O
unintended	O
biases	O
entering	O
our	O
system	O
to	O
the	O
word	B-TaskName
embeddings	I-TaskName
.	O
Finally	O
,	O
we	O
use	O
a	O
simple	O
logistic	B-MethodName
regression	I-MethodName
algorithm	O
to	O
predict	O
negative	O
sentiment	O
.	O
Although	O
the	O
choice	O
of	O
ML	O
model	O
can	O
have	O
an	O
impact	O
on	O
fairness	O
for	O
sentiment	O
applications	O
as	O
shown	O
in	O
(	O
Kiritchenko	O
and	O
Mohammad	O
,	O
2018	O
)	O
,	O
we	O
choose	O
a	O
simple	O
ML	O
model	O
to	O
limit	O
the	O
possible	O
unintended	O
biases	O
introduced	O
downstream	O
from	O
our	O
word	B-TaskName
embeddings	I-TaskName
.	O

We	O
evaluate	O
our	O
framework	O
and	O
metric	O
on	O
two	O
cases	O
studies	O
:	O
National	O
Origin	O
Discrimination	O
and	O
Religious	O
Discrimination	O
.	O
For	O
each	O
case	O
study	O
,	O
we	O
create	O
a	O
set	O
of	O
the	O
most	O
frequent	O
identity	O
terms	O
from	O
the	O
protected	O
groups	O
in	O
the	O
Wikipedia	O
word	O
corpus	O
and	O
analyze	O
bias	O
with	O
respect	O
to	O
these	O
terms	O
via	O
our	O
framework	O
.	O
First	O
,	O
we	O
compare	O
the	O
RNSB	O
metric	O
for	O
3	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
,	O
showing	O
that	O
our	O
metric	O
is	O
consistent	O
with	O
other	O
word	O
embedding	O
analysis	O
like	O
WEAT	O
(	O
Caliskan	O
-	O
Islam	O
et	O
al	O
,	O
2016	O
)	O
.	O
We	O
then	O
show	O
that	O
our	O
framework	O
enables	O
an	O
insightful	O
view	O
into	O
word	O
embedding	O
bias	O
.	O

We	O
vary	O
the	O
word	B-TaskName
embeddings	I-TaskName
used	O
in	O
our	O
framework	O
and	O
calculate	O
the	O
RNSB	O
metric	O
for	O
each	O
embedding	O
.	O
The	O
results	O
are	O
displayed	O
in	O
Table	O
1	O
.	O
For	O
both	O
case	O
studies	O
,	O
the	O
bias	O
is	O
largest	O
in	O
GloVe	B-MethodName
,	O
as	O
shown	O
by	O
the	O
largest	O
RNSB	O
metric	O
.	O
As	O
mentioned	O
earlier	O
,	O
ConceptNet	B-DatasetName
is	O
a	O
state	O
of	O
the	O
art	O
model	O
that	O
mixes	O
models	O
like	O
GloVe	B-MethodName
and	O
Word2vec	O
,	O
creating	O
fairer	O
word	B-TaskName
embeddings	I-TaskName
.	O
Through	O
the	O
RNSB	O
metric	O
,	O
one	O
can	O
see	O
that	O
the	O
unintended	O
demographic	O
bias	O
of	O
these	O
word	B-TaskName
embeddings	I-TaskName
is	O
an	O
order	O
of	O
magnitude	O
lower	O
than	O
GloVe	B-MethodName
or	O
Word2vec	O
.	O
Although	O
the	O
RNSB	O
metric	O
is	O
not	O
directly	O
comparable	O
to	O
WEAT	O
scores	O
,	O
these	O
results	O
are	O
still	O
consistent	O
with	O
some	O
of	O
the	O
bias	O
predicted	O
by	O
(	O
Caliskan	O
-	O
Islam	O
et	O
al	O
,	O
2016	O
)	O
.	O
The	O
WEAT	O
score	O
shows	O
that	O
word	B-TaskName
embeddings	I-TaskName
like	O
Word2vec	O
and	O
GloVe	B-MethodName
are	O
biased	O
with	O
respect	O
to	O
national	O
origin	O
because	O
European	O
-	O
American	O
names	O
are	O
more	O
correlated	O
with	O
positive	O
sentiment	O
than	O
African	O
-	O
American	O
names	O
.	O
RNSB	O
captures	O
the	O
same	O
types	O
of	O
biases	O
,	O
but	O
has	O
a	O
clear	O
and	O
larger	O
scope	O
,	O
measuring	O
discrimination	O
with	O
respect	O
to	O
more	O
than	O
two	O
demographics	O
within	O
a	O
protected	O
group	O
.	O

Using	O
the	O
probability	O
distribution	O
of	O
negative	O
sentiment	O
for	O
the	O
identity	O
terms	O
in	O
a	O
protected	O
group	O
,	O
we	O
can	O
gain	O
insights	O
into	O
the	O
relative	O
risks	O
for	O
discrimination	O
between	O
various	O
demographics	O
.	O
Figure	O
3	O
shows	O
three	O
histograms	O
.	O
The	O
bottom	O
histogram	O
is	O
the	O
uniform	O
distribution	O
.	O
As	O
described	O
earlier	O
,	O
zero	O
unintended	O
demographic	O
bias	O
with	O
respect	O
to	O
our	O
definition	O
is	O
achieved	O
when	O
all	O
the	O
identity	O
terms	O
within	O
a	O
protected	O
group	O
have	O
equal	O
negative	O
sentiment	O
.	O
The	O
top	O
two	O
histograms	O
show	O
the	O
negative	O
sentiment	O
probability	O
for	O
each	O
identity	O
normalized	O
across	O
all	O
terms	O
to	O
be	O
a	O
probability	O
distribution	O
.	O
The	O
left	O
histogram	O
is	O
computed	O
using	O
the	O
GloVe	B-MethodName
word	B-TaskName
embeddings	I-TaskName
,	O
and	O
the	O
right	O
histogram	O
is	O
computed	O
using	O
the	O
fairer	O
Concept	O
-	O
Net	O
embeddings	O
.	O
One	O
can	O
see	O
that	O
certain	O
demographics	O
have	O
very	O
high	O
negative	O
sentiment	O
predictions	O
,	O
while	O
others	O
have	O
very	O
low	O
predictions	O
.	O
The	O
ConceptNet	B-DatasetName
distribution	O
seems	O
to	O
equalize	O
much	O
of	O
this	O
disparity	O
.	O
This	O
type	O
of	O
analysis	O
is	O
very	O
insightful	O
as	O
it	O
enables	O
one	O
to	O
see	O
which	O
identities	O
are	O
more	O
at	O
risk	O
for	O
discrimination	O
.	O
A	O
more	O
direct	O
way	O
to	O
measure	O
how	O
certain	O
groups	O
receive	O
similar	O
unfair	O
treatment	O
is	O
to	O
compute	O
a	O
correlation	O
matrix	O
between	O
the	O
vectors	O
containing	O
negative	O
sentiment	O
predictions	O
for	O
each	O
identity	O
term	O
.	O
We	O
compute	O
this	O
matrix	O
for	O
the	O
same	O
two	O
cases	O
:	O
GloVe	B-MethodName
word	B-TaskName
embeddings	I-TaskName
(	O
top	O
)	O
and	O
ConceptNet	B-DatasetName
word	B-TaskName
embeddings	I-TaskName
(	O
bottom	O
)	O
shown	O
in	O
Figure	O
4	O
.	O
The	O
GloVe	B-MethodName
word	O
embedding	O
correlation	O
matrix	O
contains	O
a	O
lot	O
of	O
dark	O
low	O
correlations	O
between	O
identities	O
,	O
as	O
a	O
lot	O
of	O
identities	O
contain	O
small	O
amounts	O
of	O
negative	O
sentiment	O
.	O
But	O
this	O
visual	O
brings	O
out	O
that	O
certain	O
groups	O
like	O
Indian	O
,	O
Mexican	O
,	O
and	O
Russian	O
have	O
a	O
high	O
correlation	O
,	O
indicating	O
that	O
they	O
could	O
be	O
treated	O
similarly	O
unfairly	O
in	O
a	O
downstream	O
ML	O
algorithm	O
.	O
This	O
is	O
a	O
useful	O
insight	O
that	O
could	O
allow	O
a	O
practitioner	O
to	O
change	O
to	O
embedding	O
training	O
corpora	O
to	O
create	O
fairer	O
models	O
.	O
For	O
the	O
ConceptNet	B-DatasetName
word	B-TaskName
embeddings	I-TaskName
,	O
we	O
see	O
a	O
much	O
more	O
colorful	O
heat	O
map	O
,	O
indicating	O
there	O
are	O
higher	O
correlations	O
between	O
more	O
identity	O
terms	O
.	O
This	O
hints	O
that	O
ConceptNet	B-DatasetName
contains	O
less	O
targeted	O
discrimination	O
via	O
negative	O
sentiment	O
.	O
This	O
visual	O
also	O
brings	O
out	O
slight	O
differences	O
in	O
negative	O
sentiment	O
prediction	O
.	O
Identity	O
terms	O
like	O
Scottish	O
have	O
lower	O
correlations	O
across	O
the	O
board	O
,	O
manifesting	O
that	O
this	O
identity	O
has	O
slightly	O
less	O
negative	O
sentiment	O
than	O
the	O
rest	O
of	O
the	O
identities	O
.	O
This	O
is	O
important	O
to	O
analyze	O
to	O
get	O
a	O
broader	O
context	O
for	O
how	O
various	O
identities	O
could	O
receive	O
different	O
amounts	O
of	O
discrimination	O
stemming	O
from	O
the	O
word	O
embedding	O
bias	O
.	O
We	O
can	O
use	O
these	O
figures	O
to	O
analyze	O
how	O
certain	O
groups	O
could	O
be	O
similarly	O
discriminated	O
against	O
via	O
their	O
negative	O
sentiment	O
correlation	O
.	O

We	O
showed	O
how	O
our	O
framework	O
can	O
be	O
used	O
in	O
the	O
religious	O
and	O
national	O
origin	O
case	O
studies	O
.	O
In	O
practice	O
,	O
our	O
framework	O
should	O
be	O
used	O
to	O
measure	O
bias	O
among	O
demographics	O
of	O
interest	O
for	O
the	O
NLP	O
application	O
in	O
question	O
.	O
Our	O
RNSB	O
metric	O
is	O
a	O
useful	O
signal	O
a	O
practitioner	O
can	O
use	O
to	O
choose	O
the	O
embedding	O
model	O
with	O
the	O
least	O
amount	O
of	O
risk	O
for	O
discrimination	O
in	O
their	O
application	O
,	O
or	O
even	O
to	O
evaluate	O
what	O
types	O
of	O
unintended	O
biases	O
exists	O
in	O
their	O
training	O
corpora	O
.	O
We	O
used	O
our	O
framework	O
to	O
evaluate	O
unintended	O
bias	O
with	O
respect	O
to	O
sentiment	O
,	O
but	O
there	O
exists	O
many	O
other	O
types	O
of	O
unintended	O
demographic	O
bias	O
to	O
create	O
clear	O
signals	O
for	O
in	O
word	B-TaskName
embeddings	I-TaskName
.	O

We	O
presented	O
a	O
transparent	O
framework	O
for	O
evaluating	O
unintended	O
demographic	O
bias	O
in	O
word	B-TaskName
embeddings	I-TaskName
.	O
For	O
this	O
work	O
our	O
scope	O
was	O
limited	O
to	O
unfair	O
biases	O
with	O
respect	O
to	O
negative	O
sentiment	O
.	O
In	O
our	O
framework	O
,	O
we	O
train	O
a	O
classifier	O
on	O
an	O
unbiased	O
positive	O
/	O
negative	O
word	O
sentiment	O
dataset	O
initialized	O
with	O
biased	O
word	B-TaskName
embeddings	I-TaskName
.	O
This	O
way	O
,	O
we	O
can	O
observe	O
the	O
unfairness	O
in	O
the	O
word	B-TaskName
embeddings	I-TaskName
at	O
the	O
ML	O
prediction	O
level	O
.	O
This	O
allows	O
us	O
to	O
observe	O
clearer	O
signals	O
of	O
bias	O
in	O
our	O
metric	O
,	O
Relative	O
Negative	O
Sentiment	O
Bias	O
(	O
RNSB	O
)	O
.	O
Previous	O
metrics	O
and	O
analysis	O
into	O
unintended	O
bias	O
in	O
word	B-TaskName
embeddings	I-TaskName
rely	O
on	O
vector	O
space	O
arguments	O
for	O
only	O
two	O
demographics	O
at	O
a	O
time	O
,	O
which	O
does	O
not	O
lend	O
itself	O
well	O
to	O
evaluating	O
real	O
world	O
discrimination	O
.	O
Our	O
metric	O
has	O
a	O
direct	O
connection	O
to	O
discrimination	O
and	O
can	O
evaluate	O
any	O
number	O
of	O
demographics	O
in	O
a	O
protected	O
group	O
.	O
Finally	O
,	O
our	O
framework	O
and	O
metric	O
reveal	O
transparent	O
analysis	O
of	O
the	O
unintended	O
bias	O
hidden	O
in	O
word	B-TaskName
embeddings	I-TaskName
.	O

We	O
explore	O
the	O
task	O
of	O
improving	O
persona	O
consistency	O
of	O
dialogue	O
agents	O
.	O
Recent	O
models	O
tackling	O
consistency	O
often	O
train	O
with	O
additional	O
Natural	B-TaskName
Language	I-TaskName
Inference	I-TaskName
(	O
NLI	O
)	O
labels	O
or	O
attach	O
trained	O
extra	O
modules	O
to	O
the	O
generative	O
agent	B-DatasetName
for	O
maintaining	O
consistency	O
.	O
However	O
,	O
such	O
additional	O
labels	O
and	O
training	O
can	O
be	O
demanding	O
.	O
Also	O
,	O
we	O
find	O
even	O
the	O
bestperforming	O
persona	O
-	O
based	O
agents	O
are	O
insensitive	O
to	O
contradictory	O
words	O
.	O
Inspired	B-DatasetName
by	O
social	O
cognition	O
and	O
pragmatics	O
,	O
we	O
endow	O
existing	O
dialogue	O
agents	O
with	O
public	O
self	O
-	O
consciousness	O
on	O
the	O
fly	O
through	O
an	O
imaginary	O
listener	O
.	O
Our	O
approach	O
,	O
based	O
on	O
the	O
Rational	O
Speech	O
Acts	O
framework	O
(	O
Frank	O
and	O
Goodman	O
,	O
2012	O
)	O
,	O
can	O
enforce	O
dialogue	O
agents	O
to	O
refrain	O
from	O
uttering	O
contradiction	O
.	O
We	O
further	O
extend	O
the	O
framework	O
by	O
learning	O
the	O
distractor	O
selection	O
,	O
which	O
has	O
been	O
usually	O
done	O
manually	O
or	O
randomly	O
.	O
Results	O
on	O
Dialogue	O
NLI	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
and	O
PersonaChat	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
dataset	O
show	O
that	O
our	O
approach	O
reduces	O
contradiction	O
and	O
improves	O
consistency	O
of	O
existing	O
dialogue	O
models	O
.	O
Moreover	O
,	O
we	O
show	O
that	O
it	O
can	O
be	O
generalized	O
to	O
improve	O
contextconsistency	O
beyond	O
persona	O
in	O
dialogues	O
.	O

Literal	O
Agent	B-DatasetName
:	O
!	O

I	O
like	O
going	O
outside	O
.	O
Interlocutor	O
Self	O
-	O
Conscious	O
Agent	B-DatasetName
:	O
"	O

We	O
introduce	O
how	O
to	O
endow	O
dialogue	O
agents	O
with	O
public	O
self	O
-	O
consciousness	O
,	O
which	O
helps	O
them	O
keep	O
consistency	O
in	O
mind	O
at	O
each	O
generation	O
step	O
by	O
reflecting	O
an	O
imaginary	O
listener	O
's	O
distribution	O
over	O
personas	O
.	O
Since	O
the	O
imaginary	O
listener	O
arises	O
from	O
the	O
plain	O
dialogue	O
-	O
agent	B-DatasetName
,	O
separate	O
training	O
is	O
not	O
needed	O
.	O
Figure	O
3	O
illustrates	O
its	O
overall	O
structure	O
.	O
We	O
present	O
how	O
to	O
model	O
public	O
selfconsciousness	O
using	O
the	O
Rational	O
Speech	O
Acts	O
(	O
RSA	O
)	O
framework	O
(	O
Frank	O
and	O
Goodman	O
,	O
2012	O
)	O
in	O
Section	O
4.1	O
.	O
We	O
then	O
discuss	O
learning	O
of	O
distractor	O
selection	O
as	O
our	O
major	O
novelty	O
for	O
the	O
RSA	O
in	O
Section	O
4.2	O
.	O

We	O
show	O
that	O
our	O
self	O
-	O
conscious	O
framework	O
can	O
significantly	O
improve	O
consistency	O
and	O
accuracy	B-MetricName
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
persona	O
-	O
based	O
agents	O
on	O
two	O
benchmark	O
datasets	O
.	O
We	O
prove	O
its	O
effectiveness	O
using	O
both	O
automatic	O
and	O
human	O
evaluations	O
.	O
We	O
also	O
show	O
our	O
framework	O
can	O
be	O
generalized	O
to	O
improve	O
consistency	O
of	O
dialogue	O
context	O
beyond	O
persona	O
.	O

Dialogue	O
NLI	O
Evaluation	O
Set	O
(	O
Welleck	O
et	O
al	O
,	O
2019	O
)	O
.	O
This	O
dataset	O
is	O
based	O
on	O
PersonaChat	O
with	O
additional	O
NLI	O
annotations	O
.	O
Its	O
main	O
task	O
is	O
to	O
rank	O
next	O
-	O
utterance	O
candidates	O
given	O
previous	O
context	O
.	O
For	O
each	O
dialogue	O
,	O
they	O
collect	O
31	O
next	O
-	O
utterance	O
candidates	O
in	O
respect	O
to	O
the	O
given	O
persona	O
:	O
10	O
entailing	O
,	O
10	O
neutral	O
and	O
10	O
contradicting	O
candidates	O
with	O
1	O
ground	O
-	O
truth	O
utterance	O
.	O
In	O
total	O
,	O
the	O
evaluation	O
set	O
includes	O
542	O
instances	O
.	O
PersonaChat	O
dialogue	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
This	O
dataset	O
involves	O
two	O
interlocutors	O
who	O
are	O
each	O
given	O
a	O
persona	O
and	O
asked	O
to	O
get	O
to	O
know	O
each	O
other	O
while	O
playing	O
their	O
roles	O
.	O
This	O
task	O
was	O
the	O
subject	O
of	O
the	O
ConvAI2	B-DatasetName
competition	O
(	O
Dinan	O
et	O
al	O
,	O
2019	O
)	O
at	O
NeurIPS	O
2018	O
.	O
The	O
competition	O
version	O
contains	O
17	O
,	O
878	O
chitchat	O
conversations	O
conditioned	O
on	O
1	O
,	O
155	O
personas	O
for	O
training	O
and	O
1	O
,	O
000	O
conversations	O
conditioned	O
on	O
100	O
personas	O
for	O
validation	O
.	O

We	O
perform	O
human	O
evaluation	O
via	O
Amazon	O
Mechanical	O
Turk	O
.	O
We	O
random	O
sample	O
250	O
test	O
examples	O
,	O
each	O
is	O
rated	O
by	O
three	O
unique	O
human	O
judges	O
in	O
terms	O
of	O
(	O
i	O
)	O
Consistency	O
and	O
(	O
ii	O
)	O
Engagingness	O
.	O
Turkers	O
are	O
shown	O
a	O
given	O
persona	O
,	O
a	O
dialogue	O
context	O
,	O
and	O
the	O
model	O
's	O
generated	O
utterance	O
.	O
For	O
consistency	O
,	O
we	O
follow	O
Madotto	O
et	O
al	O
(	O
2019	O
)	O
and	O
ask	O
judges	O
to	O
assign	O
1	O
,	O
0	B-DatasetName
,	O
−1	O
to	O
the	O
utterance	O
for	O
consistency	O
,	O
neutrality	O
,	O
and	O
contradiction	O
,	O
respectively	O
.	O
Following	O
See	O
et	O
al	O
(	O
2019	O
)	O
,	O
we	O
evaluate	O
the	O
engagingness	O
of	O
the	O
utterance	O
in	O
a	O
4	O
-	O
point	O
scale	O
,	O
where	O
higher	O
scores	O
are	O
better	O
.	O
To	O
alleviate	O
annotator	O
bias	O
and	O
inter	O
-	O
annotator	O
variability	O
,	O
we	O
apply	O
Bayesian	O
calibration	O
(	O
Kulikov	O
et	O
al	O
,	O
2019	O
)	O
to	O
the	O
scores	O
.	O
Table	O
6	O
summarizes	O
the	O
human	O
evaluation	O
results	O
.	O
The	O
agent	B-DatasetName
with	O
our	O
self	O
-	O
consciousness	O
method	O
S	O
1	O
is	O
rated	O
as	O
more	O
consistent	O
than	O
the	O
base	O
agent	B-DatasetName
S	O
0	B-DatasetName
while	O
maintaining	O
a	O
similar	O
level	O
of	O
engagingness	O
.	O
While	O
it	O
can	O
be	O
trivial	O
to	O
increase	O
consistency	O
at	O
the	O
cost	O
of	O
engagingness	O
(	O
e.g.	O
perfect	O
consistency	O
can	O
by	O
generating	O
boring	O
utterances	O
with	O
very	O
little	O
variance	O
)	O
,	O
it	O
is	O
not	O
the	O
case	O
for	O
our	O
agent	B-DatasetName
.	O
Since	O
our	O
agent	B-DatasetName
seeks	O
to	O
be	O
heard	O
as	O
the	O
given	O
persona	O
to	O
the	O
listener	O
,	O
self	O
-	O
distinctive	O
words	O
tend	O
to	O
meld	O
into	O
generated	O
responses	O
(	O
see	O
Figure	O
6	O
)	O
.	O
Thus	O
,	O
the	O
responses	O
from	O
self	O
-	O
conscious	O
agents	O
have	O
their	O
own	O
color	O
,	O
which	O
can	O
help	O
improving	O
engagingness	O
.	O
Figure	O
4	O
displays	O
selected	O
examples	O
of	O
utterance	O
generation	O
.	O
Each	O
example	O
is	O
comprised	O
of	O
dialogue	O
history	O
,	O
human	O
response	O
,	O
and	O
utterances	O
generated	O
by	O
our	O
method	O
and	O
baselines	O
.	O

We	O
demonstrate	O
that	O
our	O
self	O
-	O
conscious	O
agent	B-DatasetName
can	O
be	O
generalized	O
to	O
generate	O
context	O
-	O
consistent	O
utterances	O
beyond	O
persona	O
.	O
We	O
condition	O
the	O
agent	B-DatasetName
with	O
its	O
previous	O
responses	O
in	O
the	O
dialogue	O
history	O
;	O
that	O
is	O
,	O
i	O
in	O
Eq	O
.	O
(	O
2	O
)	O
is	O
the	O
agent	B-DatasetName
's	O
past	O
responses	O
instead	O
of	O
persona	O
sentences	O
.	O
Hence	O
,	O
tokens	O
that	O
are	O
inconsistent	O
to	O
the	O
agent	B-DatasetName
's	O
past	O
response	O
would	O
be	O
less	O
favored	O
by	O
the	O
model	O
.	O
Table	O
7	O
reports	O
the	O
results	O
of	O
context	O
conditioned	O
self	O
-	O
conscious	O
agents	O
.	O
The	O
EmpatheticDialogue	O
(	O
Rashkin	O
et	O
al	O
,	O
2019	O
)	O
is	O
an	O
open	O
-	O
domain	O
dialogue	O
dataset	O
where	O
a	O
speaker	O
describes	O
a	O
past	O
emotional	O
experience	O
and	O
the	O
listener	O
responds	O
accordingly	O
.	O
Since	O
the	O
speaker	O
's	O
descriptions	O
should	O
be	O
consistent	O
to	O
the	O
experience	O
and	O
previous	O
utterances	O
,	O
it	O
is	O
a	O
suitable	O
benchmark	O
for	O
consistency	O
.	O
We	O
model	O
the	O
speaker	O
's	O
utterances	O
and	O
measure	O
its	O
consistency	O
.	O
Our	O
S	O
1	O
agent	B-DatasetName
outperforms	O
other	O
literal	O
agents	O
on	O
all	O
three	O
datasets	O
in	O
terms	O
of	O
consistency	O
.	O
Thus	O
,	O
our	O
approach	O
can	O
also	O
be	O
applied	O
to	O
help	O
agents	O
stay	O
more	O
consistent	O
to	O
its	O
context	O
.	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
compare	O
it	O
with	O
the	O
base	O
speaker	O
(	O
S	O
0	B-DatasetName
)	O
of	O
TransferTransfo	O
(	O
Wolf	O
et	O
al	O
,	O
2019b	O
)	O
and	O
the	O
human	O
response	O
(	O
Human	O
)	O
.	O

Figure	O
7	O
shows	O
selected	O
examples	O
of	O
generated	O
responses	O
.	O
In	O
each	O
set	O
,	O
we	O
show	O
given	O
persona	O
,	O
dialogue	O
context	O
,	O
human	O
responses	O
,	O
and	O
generated	O
responses	O
by	O
our	O
self	O
-	O
conscious	O
agent	B-DatasetName
and	O
the	O
base	O
speaker	O
.	O
We	O
use	O
TransferTransfo	O
(	O
Wolf	O
et	O
al	O
,	O
2019b	O
)	O
as	O
a	O
base	O
speaker	O
.	O

[	O
P1	O
]	O
I	O
really	O
enjoy	O
shopping	O
and	O
my	O
dream	O
is	O
to	O
one	O
day	O
own	O
a	O
Rolls	O
Royce	O
ghost	O
.	O
[	O
P2	O
]	O
Wow	O
.	O
I	O
enjoy	O
running	O
over	O
driving	O
.	O
[	O
P1	O
]	O
Running	O
is	O
also	O
quite	O
lovely	O
.	O
Breathing	O
in	O
the	O
lovely	O
outside	O
air	O
.	O
[	O
P2	O
]	O
Yes	O
it	O
is	O
.	O
It	O
clears	O
my	O
head	O
when	O
I	O
need	O
to	O
as	O
well	O
.	O
(	O
S	O
1	O
+	O
DM	O
)	O
shopping	O
is	O
a	O
great	O
way	O
to	O
clear	O
my	O
head	O
.	O
(	O
S	O
0	B-DatasetName
)	O
i	O
love	O
to	O
shop	O
and	O
watch	O
movies	O
.	O
(	O
Human	O
)	O
yes	O
,	O
and	O
it	O
also	O
helps	O
with	O
depression	O
i	O
have	O
found	O
.	O
Figure	O
7	O
:	O
Examples	O
of	O
generated	O
responses	O
by	O
our	O
self	O
-	O
conscious	O
agent	B-DatasetName
with	O
Distractor	O
Memory	O
(	O
S	O
1	O
+	O
DM	O
)	O
on	O
the	O
PersonaChat	O
dataset	O
(	O
Zhang	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
compare	O
it	O
with	O
the	O
base	O
speaker	O
(	O
S	O
0	B-DatasetName
)	O
of	O
TransferTransfo	O
(	O
Wolf	O
et	O
al	O
,	O
2019b	O
)	O
and	O
the	O
human	O
response	O
(	O
Human	O
)	O
.	O

Analogical	O
reasoning	O
is	O
effective	O
in	O
capturing	O
linguistic	O
regularities	O
.	O
This	O
paper	O
proposes	O
an	O
analogical	O
reasoning	O
task	O
on	O
Chinese	O
.	O
After	O
delving	O
into	O
Chinese	O
lexical	O
knowledge	O
,	O
we	O
sketch	O
68	O
implicit	O
morphological	O
relations	O
and	O
28	O
explicit	O
semantic	O
relations	O
.	O
A	O
big	O
and	O
balanced	O
dataset	O
CA8	O
is	O
then	O
built	O
for	O
this	O
task	O
,	O
including	O
17813	O
questions	O
.	O
Furthermore	O
,	O
we	O
systematically	O
explore	O
the	O
influences	O
of	O
vector	O
representations	O
,	O
context	O
features	O
,	O
and	O
corpora	O
on	O
analogical	O
reasoning	O
.	O
With	O
the	O
experiments	O
,	O
CA8	O
is	O
proved	O
to	O
be	O
a	O
reliable	O
benchmark	O
for	O
evaluating	O
Chinese	O
word	B-TaskName
embeddings	I-TaskName
.	O

Recently	O
,	O
the	O
boom	O
of	O
word	O
embedding	O
draws	O
our	O
attention	O
to	O
analogical	O
reasoning	O
on	O
linguistic	O
regularities	O
.	O
Given	O
the	O
word	O
representations	O
,	O
analogy	O
questions	O
can	O
be	O
automatically	O
solved	O
via	O
vector	O
computation	O
,	O
e.g.	O
"	O
apples	O
-	O
apple	O
+	O
car	O
≈	O
cars	O
"	O
for	O
morphological	O
regularities	O
and	O
"	O
kingman	O
+	O
woman	O
≈	O
queen	O
"	O
for	O
semantic	O
regularities	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
Analogical	O
reasoning	O
has	O
become	O
a	O
reliable	O
evaluation	O
method	O
for	O
word	B-TaskName
embeddings	I-TaskName
.	O
In	O
addition	O
,	O
It	O
can	O
be	O
used	O
in	O
inducing	O
morphological	O
transformations	O
(	O
Soricut	O
and	O
Och	O
,	O
2015	O
)	O
,	O
detecting	O
semantic	O
relations	O
(	O
Herdagdelen	O
and	O
Baroni	O
,	O
2009	O
)	O
,	O
and	O
translating	O
unknown	O
words	O
(	O
Langlais	O
and	O
Patry	O
,	O
2007	O
)	O
.	O
It	O
is	O
well	O
known	O
that	O
linguistic	O
regularities	O
vary	O
a	O
lot	O
among	O
different	O
languages	O
.	O
For	O
example	O
,	O
Chinese	O
is	O
a	O
typical	O
analytic	O
language	O
which	O
lacks	O
inflection	O
.	O
Figure	O
1	O
shows	O
that	O
function	O
words	O
and	O
reduplication	O
are	O
used	O
to	O
denote	O
grammatical	O
and	O
semantic	O
information	O
.	O
In	O
addition	O
,	O
many	O
semantic	O
†	O
Corresponding	O
author	O
.	O
relations	O
are	O
closely	O
related	O
with	O
social	O
and	O
cultural	O
factors	O
,	O
e.g.	O
in	O
Chinese	O
"	O
shī	O
-	O
xiān	O
"	O
(	O
god	O
of	O
poetry	O
)	O
refers	O
to	O
the	O
poet	O
Li	O
-	O
bai	O
and	O
"	O
shī	O
-	O
shèng	O
"	O
(	O
saint	O
of	O
poetry	O
)	O
refers	O
to	O
the	O
poet	O
Du	O
-	O
fu	O
.	O
However	O
,	O
few	O
attempts	O
have	O
been	O
made	O
in	O
Chinese	O
analogical	O
reasoning	O
.	O
The	O
only	O
Chinese	O
analogy	O
dataset	O
is	O
translated	O
from	O
part	O
of	O
an	O
English	O
dataset	O
(	O
Chen	O
et	O
al	O
,	O
2015	O
)	O
(	O
denote	O
as	O
CA_translated	O
)	O
.	O
Although	O
it	O
has	O
been	O
widely	O
used	O
in	O
evaluation	O
of	O
word	B-TaskName
embeddings	I-TaskName
(	O
Yang	O
and	O
Sun	O
,	O
2015	O
;	O
Yin	O
et	O
al	O
,	O
2016	O
;	O
Su	O
and	O
Lee	O
,	O
2017	O
)	O
,	O
it	O
could	O
not	O
serve	O
as	O
a	O
reliable	O
benchmark	O
since	O
it	O
includes	O
only	O
134	O
unique	O
Chinese	O
words	O
in	O
three	O
semantic	O
relations	O
(	O
capital	O
,	O
state	O
,	O
and	O
family	O
)	O
,	O
and	O
morphological	O
knowledge	O
is	O
not	O
even	O
considered	O
.	O
Therefore	O
,	O
we	O
would	O
like	O
to	O
investigate	O
linguistic	O
regularities	O
beneath	O
Chinese	O
.	O
By	O
modeling	O
them	O
as	O
an	O
analogical	O
reasoning	O
task	O
,	O
we	O
could	O
further	O
examine	O
the	O
effects	O
of	O
vector	O
offset	O
methods	O
in	O
detecting	O
Chinese	O
morphological	O
and	O
semantic	O
relations	O
.	O
As	O
far	O
as	O
we	O
know	O
,	O
this	O
is	O
the	O
first	O
study	O
focusing	O
on	O
Chinese	O
analogical	O
reasoning	O
.	O
Moreover	O
,	O
we	O
release	O
a	O
standard	O
benchmark	O
for	O
evaluation	O
of	O
Chinese	O
word	O
embedding	O
,	O
together	O
with	O
36	O
open	O
-	O
source	O
pre	O
-	O
trained	O
embeddings	O
at	O
GitHub	O
1	O
,	O
which	O
could	O
serve	O
as	O
a	O
solid	O
basis	O
for	O
Chinese	O
NLP	O
tasks	O
.	O

Analogical	O
reasoning	O
task	O
is	O
to	O
retrieve	O
the	O
answer	O
of	O
the	O
question	O
"	O
a	O
is	O
to	O
b	O
as	O
c	O
is	O
to	O
?	O
"	O
.	O
Based	O
on	O
the	O
relations	O
discussed	O
above	O
,	O
we	O
firstly	O
collect	O
word	O
pairs	O
for	O
each	O
relation	O
.	O
Since	O
there	O
are	O
no	O
explicit	O
word	O
boundaries	O
in	O
Chinese	O
,	O
we	O
take	O
dictionaries	O
and	O
word	O
segmentation	O
specifications	O
as	O
references	O
to	O
confirm	O
the	O
inclusion	O
of	O
each	O
word	O
Levy	O
and	O
Goldberg	O
(	O
2014b	O
)	O
unifies	O
SGNS	O
and	O
PPMI	B-DatasetName
in	O
a	O
framework	O
,	O
which	O
share	O
the	O
same	O
hyper	O
-	O
parameter	O
settings	O
.	O
We	O
exploit	O
3COSMUL	O
to	O
solve	O
the	O
analogical	O
questions	O
suggested	O
by	O
Levy	O
and	O
Goldberg	O
(	O
2014a	O
)	O
.	O
pair	O
.	O
To	O
avoid	O
the	O
imbalance	O
problem	O
addressed	O
in	O
English	O
benchmarks	O
(	O
Gladkova	O
et	O
al	O
,	O
2016	O
)	O
,	O
we	O
set	O
a	O
limit	O
of	O
50	O
word	O
pairs	O
at	O
most	O
for	O
each	O
relation	O
.	O
In	O
this	O
step	O
,	O
1852	O
unique	O
Chinese	O
word	O
pairs	O
are	O
retrieved	O
.	O
We	O
then	O
build	O
CA8	O
,	O
a	O
big	O
,	O
balanced	O
dataset	O
for	O
Chinese	O
analogical	O
reasoning	O
including	O
17813	O
questions	O
.	O
Compared	O
with	O
CA_translated	O
(	O
Chen	O
et	O
al	O
,	O
2015	O
)	O
,	O
CA8	O
incorporates	O
both	O
morphological	O
and	O
semantic	O
questions	O
,	O
and	O
it	O
brings	O
in	O
much	O
more	O
words	O
,	O
relation	O
types	O
and	O
questions	O
.	O
Table	O
1	O
shows	O
details	O
of	O
the	O
two	O
datasets	O
.	O
They	O
are	O
both	O
used	O
for	O
evaluation	O
in	O
Experiments	O
section	O
.	O

In	O
Chinese	O
analogical	O
reasoning	O
task	O
,	O
we	O
aim	O
at	O
investigating	O
to	O
what	O
extent	O
word	O
vectors	O
capture	O
the	O
linguistic	O
relations	O
,	O
and	O
how	O
it	O
is	O
affected	O
by	O
three	O
important	O
factors	O
:	O
vector	O
representations	O
(	O
sparse	O
and	O
dense	O
)	O
,	O
context	O
features	O
(	O
character	O
,	O
word	O
,	O
and	O
ngram	O
)	O
,	O
and	O
training	O
corpora	O
(	O
size	O
and	O
domain	O
)	O
.	O
Table	O
2	O
shows	O
the	O
hyper	O
-	O
parameters	O
used	O
in	O
this	O
work	O
.	O
All	O
the	O
text	O
data	O
used	O
in	O
our	O
experiments	O
(	O
as	O
shown	O
in	O
Table	O
3	O
)	O
are	O
preprocessed	O
via	O
the	O
following	O
steps	O
:	O
Remove	O
the	O
html	O
and	O
xml	O
tags	O
from	O
the	O
texts	O
and	O
set	O
the	O
encoding	O
as	O
utf	O
-	O
8	O
.	O
Digits	B-DatasetName
and	O
punctuations	O
are	O
remained	O
.	O
Convert	O
traditional	O
Chinese	O
characters	O
into	O
simplified	O
characters	O
with	O
Open	O
Chinese	O
Convert	O
(	O
OpenCC	O
)	O
2	O
.	O
Conduct	O
Chinese	B-TaskName
word	I-TaskName
segmentation	I-TaskName
with	O
HanLP	O
(	O
v_1.5.3	O
)	O
3	O
.	O

Existing	O
vector	O
representations	O
fall	O
into	O
two	O
types	O
,	O
dense	O
vectors	O
and	O
sparse	O
vectors	O
.	O
SGNS	O
(	O
skipgram	O
model	O
with	O
negative	O
sampling	O
)	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
and	O
PPMI	B-DatasetName
(	O
Positive	O
Pointwise	O
Mutual	O
Information	O
)	O
(	O
Levy	O
and	O
Goldberg	O
,	O
2014a	O
)	O
are	O
respectively	O
typical	O
methods	O
for	O
learning	O
dense	O
and	O
sparse	O
word	O
vectors	O
.	O
Table	O
4	O
lists	O
the	O
performance	O
of	O
them	O
on	O
CA_translated	O
and	O
CA8	O
datasets	O
under	O
different	O
configurations	O
.	O
We	O
can	O
observe	O
that	O
on	O
CA8	O
dataset	O
,	O
SGNS	O
representations	O
perform	O
better	O
in	O
analogical	O
reasoning	O
of	O
morphological	O
relations	O
and	O
PPMI	B-DatasetName
representations	O
show	O
great	O
advantages	O
in	O
semantic	O
relations	O
.	O
This	O
result	O
is	O
consistent	O
with	O
performance	O
of	O
English	O
dense	O
and	O
sparse	O
vectors	O
on	O
MSR	O
(	O
morphology	O
-	O
only	O
)	O
,	O
SemEval	O
(	O
semanticonly	O
)	O
,	O
and	O
Google	B-DatasetName
(	O
mixed	O
)	O
analogy	O
datasets	O
(	O
Levy	O
and	O
Goldberg	O
,	O
2014b	O
;	O
Levy	O
et	O
al	O
,	O
2015	O
966	O
.603	O
.117	O
.162	O
.181	O
.389	O
.222	O
.414	O
.345	O
.236	O
.223	O
.327	O
word+ngram	O
.715	O
.977	O
.640	O
.143	O
.184	O
.197	O
.429	O
.250	O
.449	O
.308	O
.276	O
.310	O
.368	O
word+char	O
.676	O
.966	O
.548	O
.358	O
.540	O
.326	O
.612	O
.455	O
.468	O
.226	O
.296	O
.305	O
.368	O
PPMI	B-DatasetName
word	O
.925	O
.920	O
.548	O
.103	O
.139	O
.138	O
.464	O
.226	O
.627	O
.501	O
.300	O
.515	O
.522	O
word+ngram	O
.943	O
.960	O
.658	O
.102	O
.129	O
.168	O
.456	O
.230	O
.680	O
.535	O
.371	O
.626	O
.586	O
word+char	O
.913	O
.886	O
.614	O
.106	O
.190	O
.173	O
.505	O
.260	O
.638	O
.502	O
.288	O
.515	O
.524	O
probably	O
because	O
the	O
reasoning	O
on	O
morphological	O
relations	O
relies	O
more	O
on	O
common	O
words	B-DatasetName
in	I-DatasetName
context	I-DatasetName
,	O
and	O
the	O
training	O
procedure	O
of	O
SGNS	O
favors	O
frequent	O
word	O
pairs	O
.	O
Meanwhile	O
,	O
PPMI	B-DatasetName
model	O
is	O
more	O
sensitive	O
to	O
infrequent	O
and	O
specific	O
word	O
pairs	O
,	O
which	O
are	O
beneficial	O
to	O
semantic	O
relations	O
.	O
The	O
above	O
observation	O
shows	O
that	O
CA8	O
is	O
a	O
reliable	O
benchmark	O
for	O
studying	O
the	O
effects	O
of	O
dense	O
and	O
sparse	O
vectors	O
.	O
Compared	O
with	O
CA_translated	O
and	O
existing	O
English	O
analogy	O
datasets	O
,	O
it	O
offers	O
both	O
morphological	O
and	O
semantic	O
questions	O
which	O
are	O
also	O
balanced	O
across	O
different	O
types	O
4	O
.	O

We	O
compare	O
word	O
representations	O
learned	O
upon	O
corpora	O
of	O
different	O
sizes	O
and	O
domains	O
.	O
As	O
shown	O
in	O
Table	O
3	O
,	O
six	O
corpora	O
are	O
used	O
in	O
the	O
experiments	O
:	O
Chinese	O
Wikipedia	O
,	O
Baidubaike	O
,	O
People	O
's	O
Daily	O
News	O
,	O
Sogou	O
News	O
,	O
Zhihu	O
QA	O
,	O
and	O
"	O
Com	O
-	O
bination	O
"	O
which	O
is	O
built	O
by	O
combining	O
the	O
first	O
five	O
corpora	O
together	O
.	O
Table	O
5	O
shows	O
that	O
accuracies	O
increase	O
with	O
the	O
growth	O
in	O
corpus	O
size	O
,	O
e.g.	O
Baidubaike	O
(	O
an	O
online	O
Chinese	O
encyclopedia	O
)	O
has	O
a	O
clear	O
advantage	O
over	O
Wikipedia	O
.	O
Also	O
,	O
the	O
domain	O
of	O
a	O
corpus	O
plays	O
an	O
important	O
role	O
in	O
the	O
experiments	O
.	O
We	O
can	O
observe	O
that	O
vectors	O
trained	O
on	O
news	O
data	O
are	O
beneficial	O
to	O
geography	O
relations	O
,	O
especially	O
on	O
People	O
's	O
Daily	O
which	O
has	O
a	O
focus	O
on	O
political	O
news	O
.	O
Another	O
example	O
is	O
Zhihu	O
QA	O
,	O
an	O
online	O
questionanswering	O
corpus	O
which	O
contains	O
more	O
informal	O
data	O
than	O
others	O
.	O
It	O
is	O
helpful	O
to	O
reduplication	O
relations	O
since	O
many	O
reduplication	O
words	O
appear	O
frequently	O
in	O
spoken	O
language	O
.	O
With	O
the	O
largest	O
size	O
and	O
varied	O
domains	O
,	O
"	O
Combination	O
"	O
corpus	O
performs	O
much	O
better	O
than	O
others	O
in	O
both	O
morphological	O
and	O
semantic	O
relations	O
.	O
Based	O
on	O
the	O
above	O
experiments	O
,	O
we	O
find	O
that	O
vector	O
representations	O
,	O
context	O
features	O
,	O
and	O
corpora	O
all	O
have	O
important	O
influences	O
on	O
Chinese	O
analogical	O
reasoning	O
.	O
Also	O
,	O
CA8	O
is	O
proved	O
to	O
be	O
a	O
reliable	O
benchmark	O
for	O
evaluation	O
of	O
Chinese	O
word	B-TaskName
embeddings	I-TaskName
.	O

In	O
this	O
paper	O
,	O
we	O
investigate	O
the	O
linguistic	O
regularities	O
beneath	O
Chinese	O
,	O
and	O
propose	O
a	O
Chinese	O
analogical	O
reasoning	O
task	O
based	O
on	O
68	O
morphological	O
relations	O
and	O
28	O
semantic	O
relations	O
.	O
In	O
the	O
experiments	O
,	O
we	O
apply	O
vector	O
offset	O
method	O
to	O
this	O
task	O
,	O
and	O
examine	O
the	O
effects	O
of	O
vector	O
representations	O
,	O
context	O
features	O
,	O
and	O
corpora	O
.	O
This	O
study	O
offers	O
an	O
interesting	O
perspective	O
combining	O
linguistic	O
analysis	O
and	O
representation	O
models	O
.	O
The	B-DatasetName
benchmark	I-DatasetName
and	O
embedding	O
sets	O
we	O
release	O
could	O
also	O
serve	O
as	O
a	O
solid	O
basis	O
for	O
Chinese	O
NLP	O
tasks	O
.	O

The	O
Fact	O
Extraction	O
and	O
VERification	O
(	O
FEVER	B-DatasetName
)	O
Shared	O
Task	O

We	O
present	O
the	O
results	O
of	O
the	O
first	O
Fact	O
Extraction	O
and	O
VERification	O
(	O
FEVER	B-DatasetName
)	O
Shared	O
Task	O
.	O
The	O
task	O
challenged	O
participants	O
to	O
classify	O
whether	O
human	O
-	O
written	O
factoid	O
claims	O
could	O
be	O
SUPPORTED	O
or	O
REFUTED	O
using	O
evidence	O
retrieved	O
from	O
Wikipedia	O
.	O
We	O
received	O
entries	O
from	O
23	O
competing	O
teams	O
,	O
19	O
of	O
which	O
scored	O
higher	O
than	O
the	O
previously	O
published	O
baseline	O
.	O
The	O
best	O
performing	O
system	O
achieved	O
a	O
FEVER	B-DatasetName
score	O
of	O
64.21	O
%	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
the	O
results	O
of	O
the	O
shared	O
task	O
and	O
a	O
summary	O
of	O
the	O
systems	O
,	O
highlighting	O
commonalities	O
and	O
innovations	O
among	O
participating	O
systems	O
.	O

Information	O
extraction	O
is	O
a	O
well	O
studied	O
domain	O
and	O
the	O
outputs	O
of	O
such	O
systems	O
enable	O
many	O
natural	O
language	O
technologies	O
such	O
as	O
question	B-TaskName
answering	I-TaskName
and	O
text	B-TaskName
summarization	I-TaskName
.	O
However	O
,	O
since	O
information	O
sources	O
can	O
contain	O
errors	O
,	O
there	O
exists	O
an	O
additional	O
need	O
to	O
verify	O
whether	O
the	O
information	O
is	O
correct	O
.	O
For	O
this	O
purpose	O
,	O
we	O
hosted	O
the	O
first	O
Fact	O
Extraction	O
and	O
VERification	O
(	O
FEVER	B-DatasetName
)	O
shared	O
task	O
to	O
raise	O
interest	O
in	O
and	O
awareness	O
of	O
the	O
task	O
of	O
automatic	O
information	O
verificationa	O
research	O
domain	O
that	O
is	O
orthogonal	O
to	O
information	O
extraction	O
.	O
This	O
shared	O
task	O
required	O
participants	O
to	O
develop	O
systems	O
to	O
predict	O
the	O
veracity	O
of	O
human	O
-	O
generated	O
textual	O
claims	O
against	O
textual	O
evidence	O
to	O
be	O
retrieved	O
from	O
Wikipedia	O
.	O
We	O
constructed	O
a	O
purpose	O
-	O
built	O
dataset	O
for	O
this	O
task	O
(	O
Thorne	O
et	O
al	O
,	O
2018	O
)	O
that	O
contains	O
185	O
,	O
445	O
human	O
-	O
generated	O
claims	O
,	O
manually	O
verified	O
against	O
the	O
introductory	O
sections	O
of	O
Wikipedia	O
pages	O
and	O
labeled	O
as	O
SUPPORTED	O
,	O
REFUTED	O
or	O
NOTENOUGHINFO	O
.	O
The	O
claims	O
were	O
generated	O
by	O
paraphrasing	O
facts	O
from	O
Wikipedia	O
and	O
mutating	O
them	O
in	O
a	O
variety	O
of	O
ways	O
,	O
some	O
of	O
which	O
were	O
meaning	O
-	O
altering	O
.	O
For	O
each	O
claim	O
,	O
and	O
without	O
the	O
knowledge	O
of	O
where	O
the	O
claim	O
was	O
generated	O
from	O
,	O
annotators	O
selected	O
evidence	O
in	O
the	O
form	O
of	O
sentences	O
from	O
Wikipedia	O
to	O
justify	O
the	O
labeling	O
of	O
the	O
claim	O
.	O
The	O
systems	O
participating	O
in	O
the	O
FEVER	B-DatasetName
shared	O
task	O
were	O
required	O
to	O
label	O
claims	O
with	O
the	O
correct	O
class	O
and	O
also	O
return	O
the	O
sentence	O
(	O
s	O
)	O
forming	O
the	O
necessary	O
evidence	O
for	O
the	O
assigned	O
label	O
.	O
Performing	O
well	O
at	O
this	O
task	O
requires	O
both	O
identifying	O
relevant	O
evidence	O
and	O
reasoning	O
correctly	O
with	O
respect	O
to	O
the	O
claim	O
.	O
A	O
key	O
difference	O
between	O
this	O
task	O
and	O
other	O
textual	O
entailment	O
and	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
tasks	O
(	O
Dagan	O
et	O
al	O
,	O
2009	O
;	O
Bowman	O
et	O
al	O
,	O
2015	O
)	O
is	O
the	O
need	O
to	O
identify	O
the	O
evidence	O
from	O
a	O
large	O
textual	O
corpus	O
.	O
Furthermore	O
,	O
in	O
comparison	O
to	O
large	O
-	O
scale	O
question	B-TaskName
answering	I-TaskName
tasks	O
(	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
,	O
systems	O
must	O
reason	O
about	O
information	O
that	O
is	O
not	O
present	O
in	O
the	O
claim	O
.	O
We	O
hope	O
that	O
research	O
in	O
these	O
fields	O
will	O
be	O
stimulated	O
by	O
the	O
challenges	O
present	O
in	O
FEVER	B-DatasetName
.	O
One	O
of	O
the	O
limitations	O
of	O
using	O
human	O
annotators	O
to	O
identify	O
correct	O
evidence	O
when	O
constructing	O
the	O
dataset	O
was	O
the	O
trade	O
-	O
off	O
between	O
annotation	O
velocity	O
and	O
evidence	O
recall	O
(	O
Thorne	O
et	O
al	O
,	O
2018	O
)	O
.	O
Evidence	O
selected	O
by	O
annotators	O
was	O
often	O
incomplete	O
.	O
As	O
part	O
of	O
the	O
FEVER	B-DatasetName
shared	O
task	O
,	O
any	O
evidence	O
retrieved	O
by	O
participating	O
systems	O
that	O
was	O
not	O
contained	O
in	O
the	O
original	O
dataset	O
was	O
annotated	O
and	O
used	O
to	O
augment	O
the	O
evidence	O
in	O
the	O
test	O
set	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
short	O
description	O
of	O
the	O
task	O
and	B-DatasetName
dataset	I-DatasetName
,	O
present	O
a	O
summary	O
of	O
the	O
submissions	O
and	O
the	O
leader	O
board	O
,	O
and	O
highlight	O
future	O
research	O
directions	O
.	O

Candidate	O
systems	O
for	O
the	O
FEVER	B-DatasetName
shared	O
task	O
were	O
given	O
a	O
sentence	O
of	O
unknown	O
veracity	O
called	O
a	O
claim	O
.	O
The	O
systems	O
must	O
identify	O
suitable	O
evidence	O
from	O
Wikipedia	O
at	O
the	O
sentence	O
level	O
and	O
Claim	O
:	O
The	O
Rodney	O
King	O
riots	O
took	O
place	O
in	O
the	O
most	O
populous	O
county	O
in	O
the	O
USA	O
.	O

Training	O
and	O
development	O
data	O
was	O
released	O
through	O
the	O
FEVER	B-DatasetName
website	O
.	O
1	O
We	O
used	O
the	O
reserved	O
portion	O
of	O
the	O
data	O
presented	O
in	O
Thorne	O
et	O
al	O
(	O
2018	O
)	O
as	O
a	O
blind	O
test	O
set	O
.	O
Disjoint	O
training	O
,	O
development	O
and	O
test	O
splits	O
of	O
the	O
dataset	O
were	O
generated	O
by	O
splitting	O
the	O
dataset	O
by	O
the	O
page	O
used	O
to	O
generate	O
the	O
claim	O
.	O
The	O
development	O
and	O
test	O
datasets	O
were	O
balanced	O
by	O
randomly	O
discarding	O
claims	O
from	O
the	O
more	O
populous	O
classes	O
.	O

The	O
FEVER	B-DatasetName
shared	O
task	O
was	O
hosted	O
as	O
a	O
competition	O
on	O
Codalab	O
3	O
which	O
allowed	O
submissions	O
to	O
be	O
scored	O
against	O
the	O
blind	O
test	O
set	O
without	O
the	O
need	O
to	O
publish	O
the	O
correct	O
labels	O
.	O

86	O
submissions	O
(	O
excluding	O
the	O
baseline	O
)	O
were	O
made	O
to	O
Codalab	O
for	O
scoring	O
on	O
the	O
blind	O
test	O
set	O
.	O
There	O
were	O
23	O
different	O
teams	O
which	O
participated	O
in	O
the	O
task	O
(	O
presented	O
in	O
Table	O
2	O
)	O
.	O
19	O
of	O
these	O
teams	O
scored	O
higher	O
than	O
the	O
baseline	O
presented	O
in	O
Thorne	O
et	O
al	O
(	O
2018	O
)	O
.	O
All	O
participating	O
teams	O
were	O
invited	O
to	O
submit	O
a	O
description	O
of	O
their	O
systems	O
.	O
We	O
received	O
15	O
descriptions	O
at	O
the	O
time	O
of	O
writing	O
and	O
the	O
remaining	O
are	O
considered	O
as	O
withdrawn	O
.	O
The	O
system	O
with	O
the	O
highest	O
score	O
was	O
submitted	O
by	O
UNC	O
-	O
NLP	O
(	O
FEVER	B-DatasetName
score	O
:	O
64.21	O
%	O
)	O
.	O
Most	O
participants	O
followed	O
a	O
similar	O
pipeline	O
structure	O
to	O
the	O
baseline	O
model	O
.	O
This	O
consisted	O
of	O
three	O
stages	O
:	O
document	O
selection	O
,	O
sentence	O
selection	O
and	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
.	O
However	O
,	O
some	O
teams	O
constructed	O
models	O
to	O
jointly	O
select	O
sentences	O
and	O
perform	O
inference	O
in	O
a	O
single	O
pipeline	O
step	O
,	O
while	O
others	O
added	O
an	O
additional	O
step	O
,	O
discarding	O
inconsistent	O
evidence	O
after	O
performing	O
inference	O
.	O
Based	O
on	O
the	O
team	O
-	O
submitted	O
system	O
description	O
summaries	O
(	O
Appendix	O
A	O
)	O
,	O
in	O
the	O
following	O
section	O
we	O
present	O
an	O
overview	O
of	O
which	O
models	O
and	O
techniques	O
were	O
applied	O
to	O
the	O
task	O
and	O
their	O
relative	O
performance	O
.	O

There	O
were	O
three	O
common	O
approaches	O
to	O
sentence	O
selection	O
:	O
keyword	O
matching	O
,	O
supervised	O
classification	O
and	O
sentence	O
similarity	O
scoring	O
.	O
Ohio	O
State	O
and	O
UCL	O
Machine	O
Reading	O
Group	O
report	O
using	O
keyword	O
matching	O
techniques	O
:	O
matching	O
either	O
named	O
entities	O
or	O
tokens	O
appearing	O
in	O
both	O
the	O
claim	O
and	O
article	O
body	O
.	O
UNC	O
-	O
NLP	O
,	O
Athene	O
UKP	B-DatasetName
TU	O
Darmstadt	O
and	O
Columbia	O
NLP	O
modeled	O
the	O
task	O
as	O
supervised	O
binary	O
classification	O
,	O
using	O
architectures	O
such	O
as	O
Enhanced	O
LSTM	B-MethodName
(	O
Chen	O
et	O
al	O
,	O
2016	O
)	O
,	O
Decomposable	O
Attention	O
(	O
Parikh	O
et	O
al	O
,	O
2016	O
)	O
or	O
similar	O
to	O
them	O
.	O
SWEEPer	O
and	O
BUPT	O
-	O
NLPer	O
present	O
jointly	O
learned	O
models	O
for	O
sentence	O
selection	O
and	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
.	O
Other	O
teams	O
report	O
scoring	O
based	O
on	O
sentence	O
similarity	O
using	O
Word	O
Mover	O
's	O
Distance	O
(	O
Kusner	O
et	O
al	O
,	O
2015	O
)	O
or	O
cosine	O
similarity	O
over	O
smooth	O
inverse	O
frequency	O
weightings	O
(	O
Arora	O
et	O
al	O
,	O
2017	O
)	O
,	O
ELMo	B-MethodName
embeddings	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
and	O
TF	O
-	O
IDF	O
(	O
Salton	O
et	O
al	O
,	O
1983	O
)	O
.	O
UCL	O
Machine	O
Reading	O
Group	O
and	O
Directed	O
Acyclic	O
Graph	O
report	O
an	O
additional	O
aggregation	O
stage	O
after	O
the	O
classification	O
stage	O
in	O
the	O
pipeline	O
where	O
evidence	O
that	O
is	O
inconsistent	O
is	O
discarded	O
.	O

NLI	O
was	O
modeled	O
as	O
supervised	O
classification	O
in	O
all	O
reported	O
submissions	O
.	O
We	O
compare	O
and	O
discuss	O
the	O
approaches	O
for	O
combining	O
the	O
evidence	O
sentences	O
together	O
with	O
the	O
claim	O
,	O
sentence	O
representations	O
and	O
training	O
schemes	O
.	O
While	O
many	O
different	O
approaches	O
were	O
used	O
for	O
sentence	O
pair	O
classification	O
,	O
e.g.	O
Enhanced	O
LSTM	B-MethodName
(	O
Chen	O
et	O
al	O
,	O
2016	O
)	O
,	O
Decomposable	O
Attention	O
(	O
Parikh	O
et	O
al	O
,	O
2016	O
)	O
,	O
Transformer	B-MethodName
Model	O
(	O
Radford	O
and	O
Salimans	O
,	O
2018	O
)	O
,	O
Random	O
Forests	O
(	O
Svetnik	O
et	O
al	O
,	O
2003	O
)	O
and	O
ensembles	O
thereof	O
,	O
these	O
are	O
not	O
specific	O
to	O
the	O
task	O
and	O
it	O
is	O
difficult	O
to	O
assess	O
their	O
impact	O
due	O
to	O
the	O
differences	O
in	O
the	O
processing	O
preceding	O
this	O
stage	O
.	O
Evidence	O
Combination	O
:	O
UNC	O
-	O
NLP	O
(	O
the	O
highest	O
scoring	O
team	O
)	O
concatenate	O
the	O
evidence	O
sentences	O
into	O
a	O
single	O
string	O
for	O
classification	O
;	O
UCL	O
Machine	O
Reading	O
Group	O
classify	O
each	O
evidenceclaim	O
pair	O
individually	O
and	O
aggregate	O
the	O
results	O
using	O
a	O
simple	O
multilayer	O
perceptron	O
(	O
MLP	B-DatasetName
)	O
;	O
Columbia	O
NLP	O
perform	O
majority	O
voting	O
;	O
and	O
finally	O
,	O
Athene	O
-	O
UKP	B-DatasetName
TU	O
Darmstadt	O
encode	O
each	O
evidence	O
-	O
claim	O
pair	O
individually	O
using	O
an	O
Enhanced	O
LSTM	B-MethodName
,	O
pool	O
the	O
resulting	O
vectors	O
and	O
use	O
an	O
MLP	B-DatasetName
for	O
classification	O
.	O
Sentence	O
Representation	O
:	O
University	O
of	O
Arizona	O
explore	O
using	O
non	O
-	O
lexical	O
features	O
for	O
predicting	O
entailment	O
,	O
considering	O
the	O
proportion	O
of	O
negated	O
verbs	O
,	O
presence	O
of	O
antonyms	O
and	O
noun	O
overlap	O
.	O
Columbia	O
NLP	O
learn	O
universal	O
sentence	O
representations	O
(	O
Conneau	O
et	O
al	O
,	O
2017	O
)	O
.	O
UNC	O
-	O
NLP	O
include	O
an	O
additional	O
token	O
-	O
level	O
feature	O
the	O
sentence	O
similarity	O
score	O
from	O
the	O
sentence	O
selection	O
module	O
.	O
Both	O
Ohio	O
State	O
and	O
UNC	O
-	O
NLP	O
report	O
alternative	O
token	O
encodings	O
:	O
UNC	O
-	O
NLP	O
report	O
using	O
ELMo	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
and	O
WordNet	O
(	O
Miller	O
,	O
1995	O
)	O
and	O
Ohio	O
State	O
report	O
using	O
vector	O
representations	O
of	O
named	O
entities	O
.	O
FujiXerox	O
report	O
representing	O
sentences	O
using	O
DEISTE	O
(	O
Yin	O
et	O
al	O
,	O
2018	O
)	O
.	O
Training	O
:	O
BUPT	O
-	O
NLPer	O
and	O
SWEEPer	O
model	O
the	O
evidence	O
selection	O
and	O
claim	O
verification	O
using	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
model	O
under	O
the	O
hypothesis	O
that	O
information	O
from	O
each	O
task	O
supplements	O
the	O
other	O
.	O
SWEEPer	O
also	O
report	O
parameter	O
tuning	O
using	O
reinforcement	O
learning	O
.	O

As	O
mentioned	O
in	O
the	O
introduction	O
,	O
to	O
increase	O
the	O
evidence	O
coverage	O
in	O
the	O
test	O
set	O
,	O
the	O
evidence	O
submitted	O
by	O
participating	O
systems	O
was	O
annotated	O
by	O
shared	O
task	O
volunteers	O
after	O
the	O
competition	O
ended	O
.	O
There	O
were	O
18	O
,	O
846	O
claims	O
where	O
at	O
least	O
one	O
system	O
returned	O
an	O
incorrect	O
label	O
,	O
according	O
to	O
the	O
FEVER	B-DatasetName
score	O
,	O
i.e.	O
taking	O
evidence	O
into	O
account	O
.	O
These	O
claims	O
were	O
sampled	O
for	O
annotation	O
with	O
a	O
probability	O
proportional	O
to	O
the	O
number	O
of	O
systems	O
which	O
labeled	O
each	O
of	O
them	O
incorrectly	O
.	O
The	O
evidence	O
sentences	O
returned	O
by	O
each	O
system	O
for	O
each	O
claim	O
was	O
sampled	O
further	O
with	O
a	O
probability	O
proportional	O
to	O
the	O
system	O
's	O
FEVER	B-DatasetName
score	O
in	O
an	O
attempt	O
to	O
focus	O
annotation	O
efforts	O
towards	O
higher	O
quality	O
candidate	O
evidence	O
.	O
These	O
extra	O
annotations	O
were	O
performed	O
by	O
volunteers	O
from	O
the	O
teams	O
participating	O
in	O
the	O
shared	O
task	O
and	O
three	O
of	O
the	O
organizers	O
.	O
Annotators	O
were	O
asked	O
to	O
label	O
whether	O
the	O
retrieved	O
evidence	O
sentences	O
supported	O
or	O
refuted	O
the	O
claim	O
at	O
question	O
,	O
and	O
to	O
highlight	O
which	O
sentences	O
(	O
if	O
any	O
)	O
,	O
either	O
individually	O
or	O
as	O
a	O
group	O
,	O
can	O
be	O
used	O
as	O
evidence	O
.	O
We	O
retained	O
the	O
annotation	O
guidelines	O
from	O
Thorne	O
et	O
al	O
(	O
2018	O
)	O
(	O
see	O
Sections	O
A.7.1	O
,	O
A.7.3	O
and	O
A.8	O
from	O
that	O
paper	O
for	O
more	O
details	O
)	O
.	O
At	O
the	O
time	O
of	O
writing	O
,	O
1	O
,	O
003	O
annotations	O
were	O
collected	O
for	O
618	O
claims	O
.	O
This	O
identified	O
3	O
claims	O
that	O
were	O
incorrectly	O
labeled	O
as	O
SUPPORTED	O
or	O
REFUTED	O
and	O
87	O
claims	O
that	O
were	O
originally	O
labeled	O
as	O
NOTENOUGHINFO	O
that	O
should	O
be	O
relabeled	O
as	O
SUPPORTED	O
or	O
REFUTED	O
through	O
the	O
introduction	O
of	O
new	O
evidence	O
(	O
44	O
and	O
43	O
claims	O
respectively	O
)	O
.	O
308	O
new	O
evidence	O
sets	O
were	O
identified	O
for	O
claims	O
originally	O
labeled	O
as	O
SUPPORTED	O
or	O
REFUTED	O
,	O
consisting	O
of	O
280	O
single	O
sentences	O
and	O
28	O
sets	O
of	O
2	O
or	O
more	O
sentences	O
.	O
Further	O
annotation	O
is	O
in	O
progress	O
and	O
the	O
data	O
collected	O
as	O
well	O
as	O
the	O
final	O
results	O
will	O
be	O
made	O
public	O
at	O
the	O
workshop	O
.	O

A.1	O
UNC	O
-	O
NLP	O
Our	O
system	O
is	O
composed	O
of	O
three	O
connected	O
components	O
namely	O
,	O
a	O
document	O
retriever	O
,	O
a	O
sentence	O
selector	O
,	O
and	O
a	O
claim	O
verifier	O
.	O
The	O
document	O
retriever	O
chooses	O
candidate	O
wiki	O
-	O
documents	O
via	O
matching	O
of	O
keywords	O
between	O
the	O
claims	O
and	O
the	O
wiki	O
-	O
document	O
titles	O
,	O
also	O
using	O
external	O
pageview	O
frequency	O
statistics	O
for	O
wiki	O
-	O
page	O
ranking	O
.	O
The	O
sentence	O
selector	O
is	O
a	O
sequence	O
-	O
matching	O
neural	O
network	O
that	O
conducts	O
further	O
fine	O
-	O
grained	O
selection	O
of	O
evidential	O
sentences	O
by	O
comparing	O
the	O
given	O
claim	O
with	O
all	O
the	O
sentences	O
in	O
the	O
candidate	O
documents	O
.	O
This	O
module	O
is	O
trained	O
as	O
a	O
binary	O
classifier	O
given	O
the	O
ground	O
truth	O
evidence	O
as	O
positive	O
examples	O
and	O
all	O
the	O
other	O
sentences	O
as	O
negative	O
examples	O
with	O
an	O
annealing	O
sampling	O
strategy	O
.	O
Finally	O
,	O
the	O
claim	O
verifier	O
is	O
a	O
state	O
-	O
of	O
-	O
theart	O
3	O
-	O
way	O
neural	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
NLI	O
)	O
classifier	O
(	O
with	O
WordNet	O
and	O
ELMo	B-MethodName
features	O
)	O
that	O
takes	O
the	O
concatenation	O
of	O
all	O
selected	O
evidence	O
as	O
the	O
premise	O
and	O
the	O
claim	O
as	O
the	O
hypothesis	O
,	O
and	O
labels	O
each	O
such	O
evidences	O
-	O
claim	O
pair	O
as	O
one	O
of	O
'	O
support	O
'	O
,	O
'	O
refute	O
'	O
,	O
or	O
'	O
not	O
enough	O
info	O
'	O
.	O
To	O
improve	O
the	O
claim	O
verifier	O
via	O
better	O
awareness	O
of	O
the	O
selected	O
evidence	O
,	O
we	O
further	O
combine	O
the	O
last	O
two	O
modules	O
by	O
feeding	O
the	O
sentence	O
similarity	O
score	O
(	O
produced	O
by	O
the	O
sentence	O
selector	O
)	O
as	O
an	O
additional	O
token	O
-	O
level	O
feature	O
to	O
the	O
claim	O
verifier	O
.	O

The	O
UCLMR	O
system	O
is	O
a	O
four	O
stage	O
model	O
consisting	O
of	O
document	O
retrieval	O
,	O
sentence	O
retrieval	O
,	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
and	O
aggregation	O
.	O
Document	O
retrieval	O
attempts	O
to	O
find	O
the	O
name	O
of	O
a	O
Wikipedia	O
article	O
in	O
the	O
claim	O
,	O
and	O
then	O
ranks	O
each	O
article	O
based	O
on	O
capitalization	O
,	O
sentence	O
position	O
and	O
token	O
match	O
features	O
.	O
A	O
set	O
of	O
sentences	O
are	O
then	O
retrieved	O
from	O
the	O
top	O
ranked	O
articles	O
,	O
based	O
on	O
token	O
matches	O
with	O
the	O
claim	O
and	O
position	O
in	O
the	O
article	O
.	O
A	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
model	O
is	O
then	O
applied	O
to	O
each	O
of	O
these	O
sentences	O
paired	O
with	O
the	O
claim	O
,	O
giving	O
a	O
prediction	O
for	O
each	O
potential	O
evidence	O
.	O
These	O
predictions	O
are	O
then	O
aggregated	O
using	O
a	O
simple	O
MLP	B-DatasetName
,	O
and	O
the	O
sentences	O
are	O
reranked	O
to	O
keep	O
only	O
the	O
evidence	O
consistent	O
with	O
the	O
final	O
prediction	O
.	O

Our	O
model	O
for	O
fact	B-TaskName
checking	I-TaskName
and	O
verification	O
consists	O
of	O
two	O
stages	O
:	O
1	O
)	O
identifying	O
relevant	O
documents	O
using	O
lexical	O
and	O
syntactic	O
features	O
from	O
the	O
claim	O
and	O
first	O
two	O
sentences	O
in	O
the	O
Wikipedia	O
article	O
and	O
2	O
)	O
jointly	O
modeling	O
sentence	O
extraction	O
and	O
verification	O
.	O
As	O
the	O
tasks	O
of	O
fact	B-TaskName
checking	I-TaskName
and	O
finding	O
evidence	O
are	O
dependent	O
on	O
each	O
other	O
,	O
an	O
ideal	O
model	O
would	O
consider	O
the	O
veracity	O
of	O
the	O
claim	O
when	O
finding	O
evidence	O
and	O
also	O
find	O
only	O
the	O
evidence	O
that	O
supports	O
/	O
refutes	O
the	O
position	O
of	O
the	O
claim	O
.	O
We	O
thus	O
jointly	O
model	O
the	O
second	O
stage	O
by	O
using	O
a	O
pointer	B-MethodName
network	I-MethodName
with	O
the	O
claim	O
and	O
evidence	O
sentence	O
represented	O
using	O
the	O
ESIM	B-MethodName
module	O
.	O
For	O
stage	O
2	O
,	O
we	O
first	O
train	O
both	O
components	O
using	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
over	O
a	O
larger	O
memory	O
of	O
extracted	O
sentences	O
,	O
then	O
tune	O
parameters	O
using	O
re	O
-	O
inforcement	O
learning	O
to	O
first	O
extract	O
sentences	O
and	O
predict	O
the	O
relation	O
over	O
only	O
the	O
extracted	O
sentences	O
.	O

For	O
document	O
retrieval	O
we	O
use	O
three	O
components	O
:	O
1	O
)	O
use	O
google	O
custom	O
search	O
API	O
with	O
the	O
claim	O
as	O
a	O
query	O
and	O
return	O
the	O
top	O
2	O
Wikipedia	O
pages	O
;	O
2	O
)	O
extract	O
all	O
name	O
entities	O
from	O
the	O
claims	O
and	O
use	O
Wikipedia	O
python	O
API	O
to	O
return	O
a	O
page	O
for	O
each	O
name	O
entity	O
and	O
3	O
)	O
;	O
use	O
the	O
prefix	O
of	O
the	O
claim	O
until	O
the	O
first	O
lowercase	O
verb	O
phrase	O
,	O
and	O
use	O
Wikipedia	O
API	O
to	O
return	O
the	O
top	O
page	O
.	O
For	O
Sentence	O
Selection	O
we	O
used	O
the	O
modified	O
document	O
retrieval	O
component	O
of	O
DrQA	O
to	O
get	O
the	O
top	O
5	O
sentences	O
and	O
then	O
further	O
extracted	O
the	O
top	O
3	O
sentences	O
using	O
cosine	O
similarity	O
between	O
vectors	O
obtained	O
from	O
Elmo	B-MethodName
(	O
Peters	O
et	O
al	O
,	O
2018	O
)	O
sentence	B-TaskName
embeddings	I-TaskName
of	O
the	O
claim	O
and	O
the	O
evidence	O
.	O
For	O
RTE	B-DatasetName
we	O
used	O
the	O
same	O
model	O
as	O
outlined	O
by	O
(	O
Conneau	O
et	O
al	O
,	O
2017	O
)	O
in	O
their	O
work	O
for	O
recognizing	O
textual	O
entailment	O
and	O
learning	O
universal	O
sentence	O
representations	O
.	O
If	O
at	O
least	O
one	O
out	O
of	O
the	O
three	O
evidences	O
SUPPORTS	O
/	O
REFUTES	O
the	O
claim	O
and	O
the	O
rest	O
are	O
NOT	O
ENOUGH	O
INFO	O
,	O
then	O
we	O
treat	O
the	O
label	O
as	O
SUPPORTS	O
/	O
REFUTES	O
,	O
else	O
we	O
return	O
the	O
majority	O
among	O
three	O
classes	O
as	O
the	O
predicted	O
label	O
.	O

We	O
prepared	O
a	O
pipeline	O
system	O
which	O
composes	O
of	O
document	O
selection	O
,	O
a	O
sentence	O
retrieval	O
,	O
and	O
a	O
recognizing	O
textual	O
entailment	O
(	O
RTE	B-DatasetName
)	O
components	O
.	O
A	O
simple	O
entity	B-TaskName
linking	I-TaskName
approach	O
with	O
text	O
match	O
is	O
used	O
as	O
the	O
document	O
selection	O
component	O
,	O
this	O
component	O
identifies	O
relevant	O
documents	O
for	O
a	O
given	O
claim	O
by	O
using	O
mentioned	O
entities	O
as	O
clues	O
.	O
The	O
sentence	O
retrieval	O
component	O
selects	O
relevant	O
sentences	O
as	O
candidate	O
evidence	O
from	O
the	O
documents	O
based	O
on	O
TF	O
-	O
IDF	O
.	O
Finally	O
,	O
the	O
RTE	B-DatasetName
component	O
selects	O
evidence	O
sentences	O
by	O
ranking	O
the	O
sentences	O
and	O
classifies	O
the	O
claim	O
as	O
SUPPORTED	O
,	O
REFUTED	O
,	O
or	O
NOTENOUGH	O
-	O
INFO	O
simultaneously	O
.	O
As	O
the	O
RTE	B-DatasetName
component	O
,	O
we	O
adopted	O
DEISTE	O
(	O
Deep	O
Explorations	O
of	O
Inter	O
-	O
Sentence	O
interactions	O
for	O
Textual	O
Entailment	O
)	O
(	O
Yin	O
et	O
al	O
,	O
2018	O
)	O
model	O
that	O
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
RTE	B-DatasetName
task	O
.	O

We	O
generate	O
a	O
Lucene	O
index	O
from	O
the	O
provided	O
Wikipedia	O
dump	O
.	O
Then	O
we	O
use	O
two	O
neural	O
networks	O
,	O
one	O
for	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
and	O
the	O
other	O
for	O
constituency	B-TaskName
parsing	I-TaskName
,	O
and	O
also	O
the	O
Stanford	O
dependency	O
parser	O
to	O
create	O
the	O
keywords	O
used	O
inside	O
the	O
Lucene	O
queries	O
.	O
Depending	O
on	O
the	O
amount	O
of	O
keywords	O
found	O
for	O
each	O
claim	O
,	O
we	O
run	O
multiple	O
Lucene	O
searches	O
on	O
the	O
generated	O
index	O
to	O
create	O
a	O
list	O
of	O
candidate	O
sentences	O
for	O
each	O
claim	O
.	O
The	O
resulting	O
list	O
of	O
claim	O
-	O
candidate	O
pairs	O
is	O
processed	O
in	O
three	O
ways	O
:	O
1	O
.	O
We	O
use	O
the	O
Standford	O
POS	O
-	O
Tagger	O
to	O
generate	O
POS	O
-	O
Tags	O
for	O
the	O
claim	O
and	O
candidate	O
sentences	O
which	O
are	O
then	O
used	O
in	O
a	O
handcrafted	O
scoring	O
script	O
to	O
assign	O
a	O
score	O
on	O
a	O
0	B-DatasetName
to	O
15	O
scale	O
.	O
2	O
.	O
We	O
run	O
each	O
pair	O
through	O
a	O
modified	O
version	O
of	O
the	O
Decomposable	O
Attention	O
network	O
.	O
3	O
.	O
We	O
merge	O
all	O
candidate	O
sentences	O
per	O
claim	O
into	O
one	O
long	O
piece	O
of	O
text	O
and	O
run	O
the	O
result	O
paired	O
with	O
the	O
claim	O
through	O
the	O
same	O
modified	O
Decomposable	O
Attention	O
network	O
as	O
in	O
(	O
2	O
.	O
)	O
.	O
We	O
then	O
make	O
the	O
final	O
prediction	O
in	O
a	O
handcrafted	O
script	O
combining	O
the	O
results	O
of	O
the	O
three	O
previous	O
steps	O
.	O

We	O
NER	B-TaskName
tagged	O
the	O
claim	O
using	O
SpaCy	O
and	O
used	O
the	O
Named	O
Entities	O
as	O
candidate	O
page	O
IDs	O
.	O
We	O
resolved	O
redirects	O
by	O
following	O
the	O
Wikipedia	O
URL	O
if	O
an	O
item	O
was	O
not	O
in	O
the	O
preprocessed	O
dump	O
.	O
If	O
a	O
page	O
could	O
not	O
be	O
found	O
,	O
we	O
fell	O
back	O
to	O
the	O
baseline	O
document	O
selection	O
method	O
.	O
The	O
rest	O
of	O
the	O
system	O
was	O
identical	O
to	O
the	O
baseline	O
system	O
,	O
al	O
-	O
though	O
we	O
used	O
our	O
document	O
retrieval	O
system	O
to	O
generate	O
alternative	O
training	O
data	O
.	O

This	O
article	O
presents	O
the	O
SIRIUS	O
-	O
LTG	O
system	O
for	O
the	O
Fact	O
Extraction	O
and	O
VERification	O
(	O
FEVER	B-DatasetName
)	O
Shared	O
Task	O
.	O
Our	O
system	O
consists	O
of	O
three	O
components	O
:	O
1	O
.	O
Wikipedia	O
Page	O
Retrieval	O
:	O
First	O
we	O
extract	O
the	O
entities	O
in	O
the	O
claim	O
,	O
then	O
we	O
find	O
potential	O
Wikipedia	O
URI	O
candidates	O
for	O
each	O
of	O
the	O
entities	O
using	O
the	O
SPARQL	O
query	O
over	O
DBpedia	B-DatasetName
(	O
Khot	O
et	O
al	O
,	O
2018	O
)	O
and	O
a	O
Gradient	O
-	O
Boosted	O
Decision	O
Trees	O
(	O
TalosTree	O
)	O
model	O
(	O
Baird	O
et	O
al	O
,	O
2017	O
)	O
for	O
this	O
task	O
.	O
The	O
experiments	O
show	O
that	O
the	O
pipeline	O
with	O
simple	O
Cosine	O
Similarity	O
using	O
TFIDF	O
in	O
sentence	O
selection	O
along	O
with	O
DA	O
as	O
labeling	O
model	O
achieves	O
better	O
results	O
in	O
development	O
and	O
test	O
dataset	O
.	O

We	O
introduce	O
an	O
end	O
-	O
to	O
-	O
end	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
model	O
for	O
fact	O
extraction	O
and	O
verification	O
with	O
bidirection	O
attention	O
.	O
We	O
propose	O
a	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
framework	O
for	O
the	O
evidence	O
extraction	O
and	O
claim	O
verification	O
because	O
these	O
two	O
tasks	O
can	O
be	O
accomplished	O
at	O
the	O
same	O
time	O
.	O
Each	O
task	O
provides	O
supplementary	O
information	O
for	O
the	O
other	O
and	O
improves	O
the	O
results	O
of	O
another	O
task	O
.	O
For	O
each	O
claim	O
,	O
our	O
system	O
firstly	O
uses	O
the	O
entity	B-TaskName
linking	I-TaskName
tool	O
S	O
-	O
MART	O
to	O
retrieve	O
relative	O
pages	O
from	O
Wikipedia	O
.	O
Then	O
,	O
we	O
use	O
attention	O
mechanisms	O
in	O
both	O
directions	O
,	O
claim	O
-	O
to	O
-	O
page	O
and	O
pageto	O
-	O
claim	O
,	O
which	O
provide	O
complementary	O
information	O
to	O
each	O
other	O
.	O
Aimed	O
at	O
the	O
different	O
task	O
,	O
our	O
system	O
obtains	O
claim	O
-	O
aware	O
sentence	O
representation	O
for	O
evidence	O
extraction	O
and	O
page	O
-	O
aware	O
claim	O
representation	O
for	O
claim	O
verification	O
.	O

Many	O
approaches	O
to	O
automatically	O
recognizing	O
entailment	O
relations	O
have	O
employed	O
classifiers	O
over	O
hand	O
engineered	O
lexicalized	O
features	O
,	O
or	O
deep	O
learning	O
models	O
that	O
implicitly	O
capture	O
lexicalization	O
through	O
word	B-TaskName
embeddings	I-TaskName
.	O
This	O
reliance	O
on	O
lexicalization	O
may	O
complicate	O
the	O
adaptation	O
of	O
these	O
tools	O
between	O
domains	O
.	O
For	O
example	O
,	O
such	O
a	O
system	O
trained	O
in	O
the	O
news	O
domain	O
may	O
learn	O
that	O
a	O
sentence	O
like	O
"	O
Palestinians	O
recognize	O
Texas	B-DatasetName
as	O
part	O
of	O
Mexico	O
"	O
tends	O
to	O
be	O
unsupported	O
,	O
a	O
fact	O
which	O
has	O
no	O
value	O
in	O
say	O
a	O
scientific	O
domain	O
.	O
To	O
mitigate	O
this	O
dependence	O
on	O
lexicalized	O
information	O
,	O
in	O
this	O
paper	O
we	O
propose	O
a	O
model	O
that	O
reads	O
two	O
sentences	O
,	O
from	O
any	O
given	O
domain	O
,	O
to	O
determine	O
entailment	O
without	O
using	O
any	O
lexicalized	O
features	O
.	O
Instead	O
our	O
model	O
relies	O
on	O
features	O
like	O
proportion	O
of	O
negated	O
verbs	O
,	O
antonyms	O
,	O
noun	O
overlap	O
etc	O
.	O
In	O
its	O
current	O
implementation	O
,	O
this	O
model	O
does	O
not	O
perform	O
well	O
on	O
the	O
FEVER	B-DatasetName
dataset	O
,	O
due	O
to	O
two	O
reasons	O
.	O
First	O
,	O
for	O
the	O
information	B-TaskName
retrieval	I-TaskName
part	O
of	O
the	O
task	O
we	O
used	O
the	O
baseline	O
system	O
provided	O
,	O
since	O
this	O
was	O
not	O
the	O
aim	O
of	O
our	O
project	O
.	O
Second	O
,	O
this	O
is	O
work	O
in	O
progress	O
and	O
we	O
still	O
are	O
in	O
the	O
process	O
of	O
identifying	O
more	O
features	O
and	O
gradually	O
increasing	O
the	O
accuracy	B-MetricName
of	O
our	O
model	O
.	O
In	O
the	O
end	O
,	O
we	O
hope	O
to	O
build	O
a	O
generic	O
end	O
-	O
to	O
-	O
end	O
classifier	O
,	O
which	O
can	O
be	O
used	O
in	O
a	O
domain	O
outside	O
the	O
one	O
in	O
which	O
it	O
was	O
trained	O
,	O
with	O
no	O
or	O
minimal	O
re	O
-	O
training	O
.	O

The	O
work	O
reported	O
was	O
partly	O
conducted	O
while	O
James	O
Thorne	O
was	O
at	O
Amazon	O
Research	O
Cambridge	B-DatasetName
.	O
Andreas	O
Vlachos	O
is	O
supported	O
by	O
the	O
EU	O
H2020	O
SUMMA	O
project	O
(	O
grant	O
agreement	O
number	O
688139	O
)	O
.	O

Language	O
models	O
trained	O
on	O
private	O
data	O
suffer	O
privacy	O
risks	O
of	O
revealing	O
sensitive	O
information	O
.	O
Previous	O
researches	O
mainly	O
considered	O
black	O
-	O
box	O
attacks	O
that	O
assumed	O
attackers	O
only	O
had	O
access	O
to	O
1	O
Code	O
is	O
publicly	O
available	O
at	O
https://github	O
.	O
com	O
/	O
HKUST	O
-	O
KnowComp	O
/	O
Persona_leakage_and	O
_	O
defense_in_GPT	O
-	O
2	O
.	O
inputs	O
and	O
outputs	O
of	O
language	O
models	O
.	O
Carlini	O
et	O
al	O
(	O
2021	O
)	O
performed	O
black	O
-	O
box	O
model	O
inversion	O
attack	O
on	O
GPT	B-MethodName
-	O
2	O
through	O
descriptive	O
prompts	O
with	O
beam	O
search	O
.	O
Lehman	O
et	O
al	O
(	O
2021	O
)	O
examined	O
BERT	B-MethodName
pretrained	O
on	O
Electronic	O
Health	O
Records	O
via	O
blank	O
filling	O
and	O
model	O
probing	O
to	O
recover	O
Personal	O
Health	O
Information	O
.	O
Furthermore	O
,	O
given	O
black	O
-	O
box	O
access	O
to	O
a	O
language	O
model	O
's	O
pre	O
-	O
train	O
and	O
finetune	O
stages	O
,	O
Zanella	O
-	O
Béguelin	O
et	O
al	O
(	O
2020	O
)	O
showed	O
that	O
sensitive	O
sequences	O
of	O
the	O
fine	O
-	O
tuning	O
dataset	O
can	O
be	O
extracted	O
.	O
For	O
the	O
distributed	O
client	O
-	O
server	O
setup	O
,	O
Malekzadeh	O
et	O
al	O
(	O
2021	O
)	O
considered	O
the	O
sensitive	O
attribute	O
leakage	O
from	O
the	O
server	O
side	O
with	O
honest	O
-	O
but	O
-	O
curious	O
(	O
HBC	O
)	O
classifiers	O
.	O
What	O
is	O
worse	O
,	O
for	O
an	O
LM	O
-	O
based	O
chatbot	B-TaskName
,	O
its	O
training	O
conversations	O
are	O
prone	O
to	O
include	O
more	O
private	O
attributes	O
than	O
other	O
commonly	O
-	O
used	O
corpora	O
for	O
language	O
modeling	O
like	O
BooksCorpus	O
(	O
Zhu	O
et	O
al	O
,	O
2015	O
)	O
and	O
Wikipedia	O
.	O
Tigunova	O
et	O
al	O
(	O
2019	O
)	O
proposed	O
Hidden	O
Attribute	O
Model	O
(	O
HAM	B-DatasetName
)	O
to	O
extract	O
professions	O
and	O
genders	O
of	O
speakers	O
from	O
various	O
dialog	O
datasets	O
.	O
further	O
applied	O
Attribute	O
Extractor	O
to	O
generate	O
speakers	O
'	O
attribute	O
triplets	O
flexibly	O
and	O
suggested	O
downstream	O
tasks	O
based	O
on	O
the	O
triplets	O
.	O
Pan	O
et	O
al	O
(	O
2020	O
)	O
exploited	O
embeddings	O
of	O
language	O
models	O
to	O
recover	O
inputs	O
'	O
digits	O
and	O
keywords	O
.	O
Though	O
the	O
setup	O
of	O
this	O
work	O
is	O
similar	O
to	O
ours	O
,	O
they	O
merely	O
consider	O
simple	O
cases	O
of	O
data	O
recovery	O
with	O
given	O
rules	O
and	O
suffer	O
great	O
utility	O
degradation	O
to	O
obtain	O
optimal	O
defense	O
performance	O
.	O
For	O
our	O
work	O
,	O
there	O
is	O
no	O
fixed	O
pattern	O
or	O
rule	O
for	O
the	O
model	O
input	O
.	O
Instead	O
of	O
finding	O
key	O
-	O
words	O
or	O
recovering	O
digits	O
,	O
we	O
aim	O
to	O
infer	O
more	O
complicated	O
private	O
attributes	O
from	O
such	O
embeddings	O
.	O
Moreover	O
,	O
our	O
proposed	O
defenses	O
have	O
almost	O
no	O
influence	O
on	O
the	O
utility	O
.	O

In	O
this	O
section	O
,	O
we	O
illustrate	O
black	O
-	O
box	O
persona	O
inference	O
attacks	O
on	O
GPT	B-MethodName
-	O
2	O
and	O
our	O
defense	O
strategies	O
.	O
In	O
Section	O
3.1	O
,	O
we	O
first	O
give	O
the	O
problem	O
formulation	O
.	O
Then	O
we	O
describe	O
the	O
attack	O
in	O
Section	O
3.2	O
.	O

In	O
Figure	O
3	O
,	O
we	O
give	O
an	O
example	O
of	O
the	O
persona	O
inference	B-TaskName
attack	I-TaskName
,	O
where	O
conversations	O
are	O
generated	O
between	O
the	O
chatbot	B-TaskName
and	O
the	O
user	O
with	O
the	O
given	O
context	O
.	O
We	O
manually	O
mark	O
True	O
/	O
False	O
on	O
the	O
predicted	O
results	O
.	O
As	O
shown	O
in	O
the	O
figure	O
,	O
there	O
are	O
several	O
successful	O
attacks	O
on	O
LM	O
and	O
no	O
correct	O
prediction	O
on	O
the	O
defensed	O
LM	O
.	O
For	O
attacks	O
on	O
LM	O
,	O
speakers	O
'	O
hobbies	O
and	O
jobs	O
can	O
be	O
inferred	O
.	O
For	O
incorrect	O
predictions	O
,	O
the	O
attacker	O
model	O
can	O
still	O
predict	O
context	O
-	O
aware	O
personas	O
.	O
After	O
applying	O
proposed	O
defense	O
learning	O
strategies	O
,	O
the	O
predicted	O
personas	O
become	O
irrelevant	O
with	O
context	O
and	O
mostly	O
predict	O
"	O
My	O
favorite	O
color	O
is	O
blue	O
.	O
"	O
In	O
fact	O
,	O
it	O
is	O
the	O
most	O
frequent	O
prediction	O
for	O
LM+KL+MI	O
over	O
4	O
,	O
332	O
persona	O
labels	O
.	O
This	O
attack	O
example	O
illustrates	O
that	O
our	O
defense	O
objectives	O
can	O
prevent	O
the	O
black	O
-	O
box	O
persona	O
inference	B-TaskName
attack	I-TaskName
from	O
inferring	O
relevant	O
personas	O
.	O

In	O
this	O
paper	O
,	O
we	O
show	O
that	O
LM	O
-	O
based	O
chatbots	O
tend	O
to	O
reveal	O
personas	O
of	O
speakers	O
and	O
propose	O
effective	O
defense	O
objectives	O
to	O
prevent	O
GPT	B-MethodName
-	O
2	O
from	O
overlearning	O
.	O
Unlike	O
other	O
works	O
that	O
suffer	O
from	O
utility	O
degradation	O
,	O
our	O
defense	O
learning	O
strategies	O
do	O
no	O
harm	O
to	O
the	O
powerful	O
generation	O
ability	O
of	O
LM	O
-	O
based	O
chatbots	O
.	O
We	O
conduct	O
extensive	O
experiments	O
to	O
evaluate	O
both	O
privacy	O
and	O
utility	O
.	O
We	O
perform	O
black	O
-	O
box	O
persona	O
inference	O
attacks	O
under	O
various	O
setups	O
to	O
demonstrate	O
the	O
robustness	O
of	O
proposed	O
defense	O
learning	O
strategies	O
.	O
In	O
addition	O
,	O
we	O
use	O
automatic	O
metrics	O
to	O
show	O
that	O
proposed	O
defense	O
learning	O
strategies	O
maintain	O
the	O
utility	O
.	O
For	O
future	O
work	O
,	O
we	O
suggest	O
working	O
on	O
flattening	O
the	O
distributions	O
of	O
attacker	O
models	O
.	O

We	O
declare	O
that	O
all	O
authors	O
of	O
this	O
paper	O
acknowledge	O
the	O
ACM	B-DatasetName
Code	O
of	O
Ethics	B-DatasetName
and	O
honor	O
the	O
code	O
of	O
conduct	O
.	O
This	O
work	O
essentially	O
considers	O
blackbox	O
attacks	O
on	O
the	O
private	O
persona	O
attributes	O
and	O
proposes	O
effective	O
learning	O
strategies	O
to	O
prevent	O
chatbots	O
from	O
overlearning	O
private	O
personas	O
.	O
Dataset	O
.	O
During	O
our	O
dataset	O
collection	O
,	O
all	O
the	O
conversations	O
and	O
personas	O
are	O
collected	O
from	O
publicly	O
available	O
datasets	O
including	O
PersonaChat	O
and	O
DNLI	O
.	O
All	O
the	O
speakers	O
are	O
anonymized	O
and	O
no	O
identifiable	O
personal	O
information	O
is	O
included	O
.	O
Model	O
.	O
For	O
training	O
our	O
LM	O
-	O
based	O
chatbots	O
,	O
we	O
follow	O
standard	O
methods	O
.	O
We	O
are	O
well	O
aware	O
of	O
the	O
bias	O
issue	O
inside	O
current	O
language	O
models	O
.	O
In	O
the	O
future	O
,	O
if	O
there	O
are	O
other	O
fairer	O
language	O
models	O
,	O
we	O
will	O
extend	O
our	O
defenses	O
on	O
them	O
.	O
formation	O
about	O
personas	O
.	O
However	O
,	O
its	O
attacking	O
performance	O
is	O
poor	O
.	O
The	O
poor	O
performance	O
implies	O
our	O
proposed	O
defense	O
learning	O
strategies	O
may	O
obfuscate	O
Attacker	O
for	O
estimating	O
single	O
sample	O
f	O
(	O
u	O
)	O
and	O
finally	O
make	O
the	O
wrong	O
prediction	O
.	O

The	O
state	O
of	O
the	O
art	O
in	O
MT	O
involves	O
corpus	O
-	O
based	O
systems	O
developed	O
with	O
machine	O
-	O
learning	O
methods	O
.	O
These	O
methods	O
learn	O
from	O
corpora	O
the	O
models	O
needed	O
for	O
translation	O
.	O
A	O
key	O
strength	O
of	O
this	O
approach	O
is	O
that	O
the	O
system	O
is	O
adapted	O
specifically	O
towards	O
the	O
data	O
it	O
is	O
trained	O
with	O
.	O
For	O
many	O
years	O
,	O
the	O
most	O
successful	O
data	O
-	O
driven	O
approaches	O
were	O
phrase	O
-	O
based	O
and	O
syntax	O
-	O
based	O
Statistical	O
MT	O
(	O
SMT	O
;	O
Koehn	O
,	O
2009	O
)	O
.	O
However	O
,	O
lately	O
Neural	O
MT	O
(	O
NMT	O
)	O
based	O
on	O
the	O
encoderdecoder	O
architecture	O
and	O
the	O
concept	O
of	O
attention	O
(	O
Sutskever	O
et	O
al	O
,	O
2014	O
;	O
Bahdanau	O
et	O
al	O
,	O
2016	O
)	O
has	O
become	O
very	O
popular	O
.	O
Indeed	O
,	O
since	O
2015	O
,	O
in	O
MT	O
shared	O
tasks	O
(	O
Cettolo	O
et	O
al	O
,	O
2015	O
;	O
Bojar	O
et	O
al	O
,	O
2015	O
;	O
Bojar	O
et	O
al	O
,	O
2016	O
)	O
most	O
top	O
-	O
performing	O
systems	O
have	O
been	O
NMT	O
systems	O
.	O
This	O
trend	O
is	O
confirmed	O
in	O
the	O
most	O
recent	O
MT	O
shared	O
task	O
(	O
Barrault	O
et	O
al	O
,	O
2019	O
)	O
,	O
where	O
80	O
%	O
of	O
participating	O
systems	O
are	O
of	O
NMT	O
type	O
.	O
Though	O
NMT	O
represents	O
the	O
state	O
of	O
the	O
art	O
for	O
MT	O
,	O
specific	O
weaknesses	O
have	O
been	O
reported	O
:	O
NMT	O
performance	O
suffers	O
from	O
the	O
lack	O
of	O
data	O
resources	O
(	O
Koehn	O
and	O
Knowles	O
,	O
2017	O
)	O
,	O
giving	O
lower	O
translation	O
performance	O
,	O
especially	O
when	O
training	O
with	O
out	O
-	O
of	O
-	O
domain	O
rather	O
than	O
in	O
-	O
domain	O
data	O
.	O
Recent	O
advances	O
in	O
NMT	O
models	O
have	O
been	O
shown	O
(	O
Sennrich	O
and	O
Zhang	O
,	O
2019	O
)	O
to	O
allow	O
good	O
translations	O
to	O
be	O
achieved	O
with	O
smaller	O
parallel	O
corpora	O
of	O
typically	O
10	O
5	O
sentences	O
,	O
though	O
substantial	O
improvements	O
are	O
achieved	O
when	O
the	O
corpus	O
size	O
reaches	O
10	O
6	O
sentences	O
.	O
However	O
,	O
training	O
sets	O
of	O
such	O
sizes	O
are	O
not	O
available	O
for	O
all	O
languages	O
.	O
Translation	B-TaskName
performance	O
is	O
affected	O
by	O
nonparallel	O
texts	O
and	O
non	O
-	O
literal	O
translations	O
(	O
Carpuat	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
integration	O
of	O
multiple	O
algorithms	O
into	O
an	O
NMT	O
system	O
does	O
not	O
necessarily	O
improve	O
translation	O
(	O
Denkowski	O
and	O
Neubig	O
,	O
2017	O
)	O
.	O
The	O
time	O
complexity	O
of	O
training	O
a	O
new	O
NMT	O
system	O
can	O
be	O
very	O
high	O
,	O
with	O
training	O
sessions	O
of	O
the	O
order	O
of	O
weeks	O
.	O
NMT	O
requires	O
very	O
large	O
amounts	O
of	O
parallel	O
data	O
,	O
measured	O
in	O
millions	O
of	O
parallel	O
sentences	O
.	O
This	O
is	O
reflected	O
by	O
the	O
separate	O
studies	O
carried	O
out	O
for	O
MT	O
with	O
limited	O
resources	O
,	O
which	O
includes	O
initiatives	O
such	O
as	O
Lorelei	O
1	O
.	O
In	O
the	O
case	O
of	O
morphologically	O
-	O
rich	O
languages	O
,	O
the	O
requirements	O
for	O
parallel	O
corpora	O
are	O
further	O
exacerbated	O
.	O
Proposed	O
approaches	O
for	O
translating	O
towards	O
lowresource	O
and	O
morphologically	O
-	O
rich	O
languages	O
have	O
included	O
transfer	B-TaskName
learning	I-TaskName
(	O
Zoph	O
et	O
al	O
,	O
2016	O
)	O
as	O
well	O
as	O
multilingual	O
and	O
multi	O
-	O
way	O
NMT	O
(	O
Rikters	O
et	O
al	O
,	O
2018	O
)	O
.	O
In	O
this	O
paper	O
,	O
an	O
effort	O
to	O
improve	O
the	O
translation	O
quality	O
is	O
presented	O
,	O
when	O
translating	O
towards	O
a	O
morphologically	O
-	O
rich	O
language	O
,	O
while	O
reducing	O
the	O
training	O
time	O
.	O
This	O
approach	O
combines	O
the	O
output	O
of	O
multiple	O
NMT	O
systems	O
with	O
an	O
NLP	O
module	O
developed	O
for	O
an	O
example	O
-	O
based	O
MT	O
paradigm	O
,	O
resulting	O
in	O
a	O
hybridized	O
solution	O
.	O
The	O
latter	O
module	O
is	O
fast	O
and	O
runs	O
independently	O
of	O
its	O
original	O
MT	O
system	O
and	O
thus	O
the	O
computational	O
complexity	O
of	O
the	O
proposed	O
hybrid	O
solution	O
is	O
not	O
substantially	O
increased	O
over	O
the	O
base	O
NMT	O
system	O
.	O
The	O
idea	O
of	O
combining	O
multiple	O
MT	O
models	O
to	O
produce	O
a	O
higher	O
performing	O
MT	O
system	O
has	O
been	O
studied	O
extensively	O
in	O
the	O
area	O
of	O
MT	O
.	O
For	O
instance	O
in	O
the	O
recent	O
shared	O
task	O
(	O
Barrault	O
et	O
al	O
,	O
2019	O
)	O
more	O
than	O
20	O
entries	O
consist	O
of	O
ensembles	O
of	O
multiple	O
NMT	O
systems	O
.	O
Ensembles	O
of	O
weaker	O
NMT	O
systems	O
of	O
the	O
same	O
general	O
architecture	O
have	O
been	O
proposed	O
by	O
Freitag	O
et	O
al	O
(	O
2017	O
)	O
to	O
train	O
a	O
higher	O
performing	O
NMT	O
system	O
.	O
In	O
addition	O
ensembles	O
of	O
factored	O
NMT	O
models	O
have	O
been	O
proposed	O
for	O
automatic	B-TaskName
post	I-TaskName
-	I-TaskName
editing	I-TaskName
and	O
quality	O
estimation	O
(	O
for	O
example	O
Hokamp	O
,	O
2017	O
)	O
.	O
This	O
base	O
NMT	O
system	O
is	O
described	O
in	O
section	O
2	O
.	O
The	O
training	O
data	O
used	O
is	O
reported	O
in	O
section	O
3	O
.	O
The	O
proposed	O
hybridization	O
is	O
presented	O
in	O
section	O
4	O
,	O
whilst	O
the	O
improvements	O
attained	O
are	O
presented	O
in	O
section	O
5	O
.	O
Future	O
developments	O
are	O
discussed	O
in	O
section	O
6	O
.	O

The	O
authors	O
acknowledge	O
support	O
of	O
this	O
work	O
by	O
the	O
project	O
"	O
DRASSI	O
"	O
(	O
MIS5002437	O
)	O
which	O
is	O
implemented	O
under	O
the	O
Action	O
"	O
Reinforcement	O
of	O
the	O
Research	O
and	O
Innovation	O
Infrastructure	O
"	O
,	O
funded	O
by	O
the	O
Operational	O
Programme	O
"	O
Competitiveness	O
,	O
Entrepreneurship	O
and	O
Innovation	O
"	O
(	O
NSRF2014	O
-	O
2020	O
)	O
and	O
co	O
-	O
financed	O
by	O
Greece	O
and	O
the	O
European	O
Commission	O
(	O
European	O
Regional	O
Development	O
Fund	O
)	O
.	O
The	O
authors	O
wish	O
to	O
acknowledge	O
the	O
contribution	O
of	O
NVIDIA	O
who	O
donated	O
for	O
research	O
purposes	O
in	O
the	O
area	O
of	O
Machine	B-TaskName
Translation	I-TaskName
a	O
Titan	B-DatasetName
XP	O
GPU	O
card	O
under	O
the	O
NVIDIA	O
Academic	O
Support	O
Programme	O
to	O
the	O
MT	O
group	O
of	O
ILSP	O
/	O
Athena	O
R.C.	O

KorNLI	B-DatasetName
and	O
KorSTS	B-DatasetName
:	O
New	O
Benchmark	O
Datasets	O
for	O
Korean	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName

Natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
NLI	O
)	O
and	O
semantic	B-TaskName
textual	I-TaskName
similarity	I-TaskName
(	O
STS	B-TaskName
)	O
are	O
key	O
tasks	O
in	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
(	O
NLU	O
)	O
.	O
Although	O
several	O
benchmark	O
datasets	O
for	O
those	O
tasks	O
have	O
been	O
released	O
in	O
English	O
and	O
a	O
few	O
other	O
languages	O
,	O
there	O
are	O
no	O
publicly	O
available	O
NLI	O
or	O
STS	B-TaskName
datasets	O
in	O
the	O
Korean	O
language	O
.	O
Motivated	O
by	O
this	O
,	O
we	O
construct	O
and	O
release	O
new	O
datasets	O
for	O
Korean	O
NLI	O
and	O
STS	B-TaskName
,	O
dubbed	O
KorNLI	B-DatasetName
and	O
KorSTS	B-DatasetName
,	O
respectively	O
.	O
Following	O
previous	O
approaches	O
,	O
we	O
machine	O
-	O
translate	O
existing	O
English	O
training	O
sets	O
and	O
manually	O
translate	O
development	O
and	O
test	O
sets	O
into	O
Korean	O
.	O
To	O
accelerate	O
research	O
on	O
Korean	O
NLU	O
,	O
we	O
also	O
establish	O
baselines	O
on	O
KorNLI	B-DatasetName
and	O
KorSTS	B-DatasetName
.	O
Our	O
datasets	O
are	O
publicly	O
available	O
at	O
https://github.com/	O
kakaobrain	O
/	O
KorNLUDatasets	O
.	O

Natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
NLI	O
)	O
and	O
semantic	B-TaskName
textual	I-TaskName
similarity	I-TaskName
(	O
STS	B-TaskName
)	O
are	O
considered	O
as	O
two	O
of	O
the	O
central	O
tasks	O
in	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
(	O
NLU	O
)	O
.	O
They	O
are	O
not	O
only	O
featured	O
in	O
GLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
,	O
2018	O
)	O
and	O
SuperGLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
,	O
which	O
are	O
two	O
popular	O
benchmarks	O
for	O
NLU	O
,	O
but	O
also	O
known	O
to	O
be	O
useful	O
for	O
supplementary	O
training	O
of	O
pre	O
-	O
trained	O
language	O
models	O
(	O
Phang	O
et	O
al	O
,	O
2018	O
)	O
as	O
well	O
as	O
for	O
building	O
and	O
evaluating	O
fixedsize	O
sentence	B-TaskName
embeddings	I-TaskName
(	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
.	O
Accordingly	O
,	O
several	O
benchmark	O
datasets	O
have	O
been	O
released	O
for	O
both	O
NLI	O
(	O
Bowman	O
et	O
al	O
,	O
2015	O
;	O
and	O
STS	B-TaskName
(	O
Cer	O
et	O
al	O
,	O
2017	O
)	O
in	O
the	O
English	O
language	O
.	O
When	O
it	O
comes	O
to	O
the	O
Korean	O
language	O
,	O
however	O
,	O
benchmark	O
datasets	O
for	O
NLI	O
and	O
STS	B-TaskName
do	O
not	O
exist	O
.	O
Popular	O
benchmark	O
datasets	O
for	O
Korean	O
NLU	O
typically	O
involve	O
question	B-TaskName
answering	I-TaskName
12	O
and	O
sentiment	B-TaskName
analysis	I-TaskName
3	O
,	O
but	O
not	O
NLI	O
or	O
STS	B-TaskName
.	O
We	O
believe	O
that	O
the	O
lack	O
of	O
publicly	O
available	O
benchmark	O
datasets	O
for	O
Korean	O
NLI	O
and	O
STS	B-TaskName
has	O
led	O
to	O
the	O
lack	O
of	O
interest	O
for	O
building	O
Korean	O
NLU	O
models	O
suited	O
for	O
these	O
key	O
understanding	O
tasks	O
.	O
Motivated	O
by	O
this	O
,	O
we	O
construct	O
and	O
release	O
Ko	O
-	O
rNLI	O
and	O
KorSTS	B-DatasetName
,	O
two	O
new	O
benchmark	O
datasets	O
for	O
NLI	O
and	O
STS	B-TaskName
in	O
the	O
Korean	O
language	O
.	O
Following	O
previous	O
work	O
(	O
Conneau	O
et	O
al	O
,	O
2018	O
)	O
,	O
we	O
construct	O
our	O
datasets	O
by	O
machine	O
-	O
translating	O
existing	O
English	O
training	O
sets	O
and	O
by	O
translating	O
English	O
development	O
and	O
test	O
sets	O
via	O
human	O
translators	O
.	O
We	O
then	O
establish	O
baselines	O
for	O
both	O
KorNLI	B-DatasetName
and	O
KorSTS	B-DatasetName
to	O
facilitate	O
research	O
on	O
Korean	O
NLU	O
.	O

In	O
an	O
NLI	O
task	O
,	O
a	O
system	O
receives	O
a	O
pair	O
of	O
sentences	O
,	O
a	O
premise	O
and	O
a	O
hypothesis	O
,	O
and	O
classifies	O
their	O
relationship	O
into	O
one	O
out	O
of	O
three	O
categories	O
:	O
entailment	O
,	O
contradiction	O
,	O
and	O
neutral	O
.	O
There	O
are	O
several	O
publicly	O
available	O
NLI	O
datasets	O
in	O
English	O
.	O
Bowman	O
et	O
al	O
(	O
2015	O
)	O
introduced	O
the	O
Stanford	O
NLI	O
(	O
SNLI	B-DatasetName
)	O
dataset	O
,	O
which	O
consists	O
of	O
570	O
K	O
English	O
sentence	O
pairs	O
based	O
on	O
image	O
captions	O
.	O
introduced	O
the	O
Multi	O
-	O
Genre	O
NLI	O
(	O
MNLI	B-DatasetName
)	O
dataset	O
,	O
which	O
consists	O
of	O
455	O
K	O
English	O
sentence	O
pairs	O
from	O
ten	O
genres	O
.	O
Conneau	O
et	O
al	O
(	O
2018	O
)	O
released	O
the	O
Cross	O
-	O
lingual	O
NLI	O
(	O
XNLI	B-DatasetName
)	O
dataset	O
by	O
extending	O
the	O
development	O
and	O
test	O
data	O
of	O
the	O
MNLI	B-DatasetName
corpus	O
to	O
15	O
languages	O
.	O
Note	O
that	O
Korean	O
is	O
not	O
one	O
of	O
the	O
15	O
languages	O
in	O
XNLI	B-DatasetName
.	O
There	O
are	O
also	O
publicly	O
available	O
NLI	O
datasets	O
in	O
a	O
few	O
other	O
non	O
-	O
English	O
languages	O
(	O
Fonseca	O
et	O
al	O
,	O
2016	O
;	O
Real	O
et	O
al	O
,	O
2019	O
;	O
Hayashibe	O
,	O
2020	O
)	O
,	O
but	O
none	O
exists	O
for	O
Korean	O
at	O
the	O
time	O
of	O
publication	O
.	O
Figure	O
1	O
:	O
Data	O
construction	O
process	O
.	O
MT	O
and	O
PE	O
indicate	O
machine	B-TaskName
translation	I-TaskName
and	O
post	O
-	O
editing	O
,	O
respectively	O
.	O
We	O
translate	O
original	O
English	O
data	O
into	O
Korean	O
using	O
an	O
internal	O
translation	O
engine	O
.	O
For	O
development	O
and	O
test	O
data	O
,	O
the	O
machine	B-TaskName
translation	I-TaskName
outputs	O
are	O
further	O
post	O
-	O
edited	O
by	O
human	O
experts	O
.	O

STS	B-TaskName
is	O
a	O
task	O
that	O
assesses	O
the	O
gradations	O
of	O
semantic	B-TaskName
similarity	I-TaskName
between	O
two	O
sentences	O
.	O
The	O
similarity	O
score	O
ranges	O
from	O
0	B-DatasetName
(	O
completely	O
dissimilar	O
)	O
to	O
5	O
(	O
completely	O
equivalent	O
)	O
.	O
It	O
is	O
commonly	O
used	O
to	O
evaluate	O
either	O
how	O
well	O
a	O
model	O
grasps	O
the	O
closeness	O
of	O
two	O
sentences	O
in	O
meaning	O
,	O
or	O
how	O
well	O
a	O
sentence	B-TaskName
embedding	I-TaskName
embodies	O
the	O
semantic	O
representation	O
of	O
the	O
sentence	O
.	O
The	O
STS	B-DatasetName
-	I-DatasetName
B	I-DatasetName
dataset	O
consists	O
of	O
8	O
,	O
628	O
English	O
sentence	O
pairs	O
selected	O
from	O
the	O
STS	B-TaskName
tasks	O
organized	O
in	O
the	O
context	O
of	O
SemEval	O
between	O
2012	O
and	O
2017	O
(	O
Agirre	O
et	O
al	O
,	O
2012	O
(	O
Agirre	O
et	O
al	O
,	O
,	O
2013	O
(	O
Agirre	O
et	O
al	O
,	O
,	O
2014	O
(	O
Agirre	O
et	O
al	O
,	O
,	O
2015	O
(	O
Agirre	O
et	O
al	O
,	O
,	O
2016	O
.	O
The	O
domain	O
of	O
input	O
sentences	O
covers	O
image	O
captions	O
,	O
news	O
headlines	O
,	O
and	O
user	O
forums	O
.	O
For	O
details	O
,	O
we	O
refer	O
readers	O
to	O
Cer	O
et	O
al	O
(	O
2017	O
)	O
.	O

We	O
explain	O
how	O
we	O
develop	O
two	O
new	O
Korean	O
language	O
understanding	O
datasets	O
:	O
KorNLI	B-DatasetName
and	O
Ko	O
-	O
rSTS	O
.	O
The	O
KorNLI	B-DatasetName
dataset	O
is	O
derived	O
from	O
three	O
different	O
sources	O
:	O
SNLI	B-DatasetName
,	O
MNLI	B-DatasetName
,	O
and	O
XNLI	B-DatasetName
,	O
while	O
the	O
KorSTS	B-DatasetName
dataset	O
stems	O
from	O
the	O
STS	B-DatasetName
-	I-DatasetName
B	I-DatasetName
dataset	O
.	O
The	O
overall	O
construction	O
process	O
,	O
which	O
is	O
applied	O
identically	O
to	O
the	O
two	O
new	O
datasets	O
,	O
is	O
illustrated	O
in	O
Figure	O
1	O
.	O
First	O
,	O
we	O
translate	O
the	O
training	O
sets	O
of	O
the	O
SNLI	B-DatasetName
,	O
MNLI	B-DatasetName
,	O
and	O
STS	B-DatasetName
-	I-DatasetName
B	I-DatasetName
datasets	O
,	O
as	O
well	O
as	O
the	O
development	O
and	O
test	O
sets	O
of	O
the	O
XNLI	B-DatasetName
4	O
and	O
STS	B-DatasetName
-	I-DatasetName
B	I-DatasetName
datasets	O
,	O
into	O
Korean	O
using	O
an	O
internal	O
neural	O
machine	B-TaskName
translation	I-TaskName
engine	O
.	O
Then	O
,	O
the	O
translation	O
results	O
of	O
the	O
development	O
and	O
test	O
sets	O
are	O
post	O
-	O
edited	O
by	O
professional	O
translators	O
in	O
order	O
to	O
guarantee	O
the	O
quality	O
of	O
evaluation	O
.	O
This	O
multi	O
-	O
stage	O
translation	O
strategy	O
aims	O
not	O
only	O
to	O
expedite	O
the	O
translators	O
'	O
work	O
,	O
but	O
also	O
to	O
help	O
maintain	O
the	O
translation	O
consistency	O
between	O
the	O
training	O
and	O
evaluation	O
datasets	O
.	O
It	O
is	O
worth	O
noting	O
that	O
the	O
post	O
-	O
editing	O
procedure	O
does	O
not	O
simply	O
mean	O
proofreading	O
.	O
Rather	O
,	O
it	O
refers	O
to	O
human	O
translation	O
based	O
on	O
the	O
prior	O
machine	B-TaskName
translation	I-TaskName
results	O
,	O
which	O
serve	O
as	O
first	O
drafts	O
.	O

In	O
this	O
section	O
,	O
we	O
provide	O
baselines	O
for	O
the	O
Korean	O
NLI	O
and	O
STS	B-TaskName
tasks	O
using	O
our	O
newly	O
created	O
benchmark	O
datasets	O
.	O
Because	O
both	O
tasks	O
receive	O
a	O
pair	O
of	O
sentences	O
as	O
an	O
input	O
,	O
there	O
are	O
two	O
different	O
approaches	O
depending	O
on	O
whether	O
the	O
model	O
encodes	O
the	O
sentences	O
jointly	O
(	O
"	O
cross	O
-	O
encoding	O
"	O
)	O
or	O
separately	O
(	O
"	O
bi	O
-	O
encoding	O
"	O
)	O
.	O
5	O

We	O
introduced	O
KorNLI	B-DatasetName
and	O
KorSTS	B-DatasetName
-	O
new	O
datasets	O
for	O
Korean	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
.	O
Using	O
these	O
datasets	O
,	O
we	O
also	O
established	O
baselines	O
for	O
Korean	O
NLI	O
and	O
STS	B-TaskName
with	O
both	O
cross	O
-	O
encoding	O
and	O
bi	O
-	O
encoding	O
approaches	O
.	O
Looking	O
forward	O
,	O
we	O
hope	O
that	O
our	O
datasets	O
and	O
baselines	O
will	O
facilitate	O
future	O
research	O
on	O
not	O
only	O
improving	O
Korean	O
NLU	O
systems	O
but	O
also	O
increasing	O
language	O
diversity	O
in	O
NLU	O
research	O
.	O

For	O
the	O
Korean	O
RoBERTa	B-MethodName
baselines	O
used	O
in	O
4	O
,	O
we	O
pre	O
-	O
train	O
a	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
model	O
on	O
an	O
internal	O
Korean	O
corpora	O
of	O
size	O
65	O
GB	O
,	O
consisting	O
of	O
online	O
news	O
articles	O
(	O
56	O
GB	O
)	O
,	O
encyclopedia	O
(	O
7	O
GB	O
)	O
,	O
movie	O
subtitles	O
(	O
∼1	O
GB	O
)	O
,	O
and	O
the	O
Sejong	O
corpus	O
8	O
(	O
∼0.5	O
GB	O
)	O
.	O
We	O
use	O
fairseq	O
,	O
which	O
includes	O
the	O
official	O
implementation	O
for	O
RoBERTa	B-MethodName
.	O
In	O
compared	O
to	O
the	O
original	O
RoBERTa	B-MethodName
(	O
English	O
)	O
,	O
the	O
model	O
architectures	O
are	O
identical	O
except	O
for	O
the	O
token	O
embedding	O
layer	O
,	O
as	O
we	O
use	O
different	O
vocabularies	O
(	O
32	O
K	O
sentencepiece	B-MethodName
vocab	O
instead	O
of	O
50	O
K	O
byte	O
-	O
level	O
BPE	B-MethodName
)	O
.	O
After	O
training	O
,	O
the	O
base	O
and	O
large	O
models	O
achieve	O
validation	O
perplexities	O
of	O
2.55	O
and	O
2.39	O
respectively	O
,	O
where	O
the	O
validation	O
set	O
is	O
a	O
random	O
5	O
%	O
subset	O
of	O
the	O
entire	O
corpora	O
.	O

To	O
fine	O
-	O
tune	O
Korean	O
RoBERTa	B-MethodName
and	O
XLM	B-MethodName
-	O
R	O
models	O
using	O
the	O
cross	O
-	O
encoding	O
approach	O
(	O
4.1	O
)	O
,	O
we	O
follow	O
the	O
fine	O
-	O
tuning	O
procedures	O
of	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2019	O
)	O
The	O
fine	O
-	O
tuning	O
hyperparameters	O
are	O
summarized	O
in	O
Table	O
8	O
.	O
For	O
each	O
dataset	O
and	O
model	O
size	O
,	O
we	O
choose	O
the	O
hyperparameter	O
configurations	O
that	O
are	O
used	O
in	O
the	O
corresponding	O
English	O
version	O
of	O
the	O
dataset	O
and	O
model	O
size	O
(	O
except	O
for	O
the	O
XLM	B-MethodName
-	O
R	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
using	O
MNLI	B-DatasetName
,	O
where	O
we	O
also	O
use	O
the	O
same	O
hyperparameters	O
as	O
RoBERTa	B-MethodName
and	O
XLM	B-MethodName
-	O
R	O
on	O
KorNLI	B-DatasetName
)	O
.	O
We	O
find	O
that	O
the	O
hyperparameters	O
used	O
for	O
English	O
models	O
and	O
datasets	O
give	O
sufficiently	O
good	O
performances	O
on	O
the	O
development	O
set	O
,	O
so	O
we	O
do	O
not	O
perform	O
an	O
additional	O
hyperparameter	O
search	O
.	O
After	O
training	O
each	O
model	O
for	O
10	O
epochs	O
,	O
we	O
choose	O
the	O
model	O
checkpoint	O
that	O
achieve	O
the	O
highest	O
score	O
on	O
the	O
development	O
set	O
and	O
evaluate	O
it	O
on	O
the	O
test	O
set	O
to	O
obtain	O
our	O
final	O
results	O
in	O
4.1	O
.	O
We	O
also	O
report	O
the	O
development	O
set	O
scores	O
for	O
the	O
best	O
checkpoint	O
in	O
Table	O
9	O
.	O
We	O
observe	O
that	O
the	O
XLM	B-MethodName
-	O
R	O
models	O
fine	O
-	O
tuned	O
on	O
KorNLI	B-DatasetName
and	O
KorSTS	B-DatasetName
achieve	O
the	O
highest	O
scores	O
on	O
the	O
development	O
set	O
,	O
although	O
the	O
Korean	O
RoBERTa	B-MethodName
models	O
perform	O
better	O
on	O
the	O
test	O
set	O
(	O
Table	O
5	O

The	O
goal	O
of	O
the	O
DialPort	O
spoken	O
dialog	O
portal	O
is	O
to	O
gather	O
large	O
amounts	O
of	O
real	O
user	O
data	O
for	O
spoken	O
dialog	O
systems	O
(	O
SDS	O
)	O
.	O
Sophisticated	O
statistical	O
representations	O
in	O
state	O
of	O
the	O
art	O
SDS	O
,	O
require	O
large	O
amounts	O
of	O
data	O
.	O
While	O
industry	O
has	O
this	O
,	O
they	O
can	O
not	O
share	O
this	O
treasure	O
.	O
Academia	O
has	O
difficulty	O
getting	O
even	O
small	O
amounts	O
of	O
similar	O
data	O
.	O
With	O
one	O
central	O
portal	O
,	O
connected	O
to	O
many	O
different	O
systems	O
,	O
the	O
task	O
of	O
advertising	O
and	O
affording	O
user	O
access	O
can	O
be	O
done	O
in	O
one	O
centralized	O
place	O
that	O
all	O
systems	O
can	O
connect	O
to	O
.	O
DialPort	O
provides	O
a	O
steady	O
stream	O
of	O
data	O
,	O
allowing	O
system	O
creators	O
to	O
focus	O
on	O
developing	O
their	O
systems	O
.	O
The	O
portal	O
decides	O
what	O
service	O
the	O
user	O
wants	O
and	O
connects	O
them	O
to	O
the	O
appropriate	O
system	O
which	O
carries	O
on	O
a	O
dialog	O
with	O
the	O
user	O
,	O
returning	O
control	O
to	O
the	O
portal	O
at	O
the	O
end	O
.	O
DialPort	O
(	O
Zhao	O
et	O
al	O
,	O
2016	O
)	O
began	O
with	O
a	O
central	O
agent	B-DatasetName
and	O
the	O
Let'sForecast	O
weather	O
information	O
system	O
.	O
The	O
Cambridge	B-DatasetName
restaurant	O
system	O
(	O
Gasic	O
et	O
al	O
,	O
2015	O
)	O
and	O
a	O
general	O
restaurant	O
system	O
(	O
Let	O
's	O
Eat	O
,	O
that	O
handles	O
cities	O
that	O
Cambridge	B-DatasetName
does	O
not	O
cover	O
)	O
joined	O
the	O
portal	O
.	O
A	O
chatbot	B-TaskName
,	O
Qubot	O
,	O
was	O
developed	O
to	O
deal	O
with	O
out	O
-	O
of	O
-	O
domain	O
requests	O
.	O
Later	O
,	O
more	O
systems	O
connected	O
to	O
the	O
portal	O
.	O
A	O
flow	O
of	O
users	O
has	O
begun	O
interacting	O
with	O
the	O
portal	O
.	O
Originally	O
envisioned	O
as	O
a	O
website	O
with	O
a	O
list	O
of	O
the	O
urls	O
of	O
systems	O
a	O
user	O
could	O
try	O
,	O
the	O
portal	O
has	O
become	O
easier	O
to	O
use	O
,	O
more	O
closely	O
resembling	O
what	O
users	O
might	O
expect	O
,	O
given	O
their	O
exposure	O
to	O
the	O
Amazon	O
ECHO	O
1	O
and	O
Google	B-DatasetName
HOME	O
2	O
,	O
etc	O
.	O
In	O
order	O
to	O
get	O
a	O
flow	O
of	O
users	O
started	O
,	O
DialPort	O
developers	O
expanded	O
the	O
number	O
of	O
connected	O
systems	O
to	O
make	O
the	O
portal	O
offerings	O
more	O
attractive	O
and	O
relevant	O
.	O
They	O
also	O
made	O
the	O
interface	O
easier	O
to	O
use	O
.	O
By	O
the	O
end	O
of	O
March	O
2017	O
,	O
in	O
addition	O
to	O
the	O
above	O
systems	O
,	O
the	O
portal	O
also	O
included	O
Mr.	O
Clue	O
,	O
a	O
word	O
game	O
from	O
USC	B-DatasetName
(	O
Pincus	O
and	O
Traum	O
,	O
2016	O
)	O
,	O
a	O
restaurant	O
opinion	O
bot	O
(	O
Let	O
's	O
Discuss	O
,	O
CMU	O
)	O
,	O
and	O
a	O
bus	O
information	O
system	O
derived	O
from	O
Let	O
's	O
Go	O
(	O
Raux	O
et	O
al	O
,	O
2005	O
)	O
.	O
The	O
portal	O
offers	O
users	O
the	O
option	O
of	O
typing	O
or	O
talking	O
and	O
of	O
seeing	O
an	O
agent	B-DatasetName
or	O
just	O
hearing	O
it	O
.	O
With	O
few	O
connected	O
systems	O
in	O
previous	O
versions	O
it	O
was	O
difficult	O
to	O
assess	O
the	O
portal	O
's	O
switching	O
mechanisms	O
.	O
The	O
increased	O
number	O
of	O
systems	O
challenges	O
the	O
portal	O
to	O
make	O
better	O
decisions	O
and	O
have	O
better	O
a	O
switching	O
strategy	O
.	O
It	O
also	O
demands	O
changes	O
in	O
the	O
frequency	O
of	O
recommendations	O
to	O
connected	O
systems	O
.	O
And	O
it	O
challenged	O
the	O
nature	O
of	O
the	O
agent	B-DatasetName
:	O
some	O
users	O
prefer	O
no	O
visual	O
agent	B-DatasetName
;	O
others	O
could	O
n't	O
use	O
speech	O
with	O
the	O
system	O
.	O
A	O
short	O
history	O
of	O
DialPort	O
DialPort	O
started	O
with	O
a	O
call	O
for	O
research	O
groups	O
to	O
link	O
their	O
SDS	O
to	O
the	O
portal	O
and	O
a	O
website	O
listing	O
SDS	O
urls	O
for	O
users	O
to	O
try	O
out	O
.	O
It	O
quickly	O
evolved	O
into	O
one	O
userfriendly	O
portal	O
where	O
,	O
all	O
of	O
the	O
SDS	O
are	O
accessed	O
through	O
one	O
central	O
agent	B-DatasetName
,	O
users	O
being	O
seamlessly	O
transferred	O
from	O
one	O
system	O
to	O
another	O
.	O
System	O
connections	O
go	O
through	O
an	O
API	O
that	O
sends	O
them	O
the	O
ASR	O
result	O
(	O
Chrome	O
at	O
present	O
)	O
.	O
The	O
system	O
was	O
tried	O
out	O
informally	O
(	O
Lee	O
et	O
al	O
,	O
2017	O
)	O
to	O
determine	O
whether	O
the	O
portal	O
fulfilled	O
criteria	O
such	O
as	O
:	O
timely	O
response	O
,	O
correct	O
transfer	O
(	O
to	O
what	O
the	O
user	O
wanted	O
)	O
,	O
and	O
correct	O
recommendation	O
of	O
systems	O
(	O
not	O
saying	O
for	O
example	O
,	O
you	O
can	O
ask	O
me	O
about	O
restaurants	O
in	O
Cambridge	B-DatasetName
just	O
after	O
the	O
user	O
has	O
finished	O
talking	O
to	O
that	O
system	O
)	O
.	O

The	O
first	O
assessment	O
of	O
the	O
interface	O
(	O
Lee	O
et	O
al	O
,	O
2017	O
)	O
included	O
five	O
External	O
Systems	O
(	O
ESes	O
,	O
that	O
is	O
,	O
systems	O
that	O
are	O
joined	O
to	O
the	O
portal	O
and	O
are	O
thus	O
not	O
part	O
of	O
the	O
central	O
portal	O
-	O
they	O
can	O
be	O
from	O
CMU	O
as	O
well	O
as	O
from	O
other	O
sites	O
)	O
:	O
Let'sForecast	O
,	O
Cambridge	B-DatasetName
SDS	O
on	O
restaurants	O
,	O
Lets	O
Eat	O
;	O
Mr	O
Clue	O
word	O
game	O
;	O
and	O
Qubot	O
chatbot	B-TaskName
handling	O
out	O
of	O
domain	O
requests	O
.	O
Since	O
then	O
,	O
Let	O
's	O
Go	O
and	O
Let'sDiscuss	O
,	O
a	O
chatbot	B-TaskName
that	O
gives	O
restaurant	O
reviews	O
,	O
have	O
joined	O
.	O
The	O
latter	O
systems	O
,	O
by	O
the	O
CMU	O
portal	O
group	O
,	O
offer	O
new	O
services	O
hoping	O
to	O
attract	O
more	O
diverse	O
users	O
and	O
encourage	O
them	O
to	O
become	O
return	O
users	O
.	O
Cambridge	B-DatasetName
The	O
Cambridge	B-DatasetName
restaurant	O
information	O
system	O
helps	O
users	O
find	O
a	O
restaurant	O
in	O
Cambridge	B-DatasetName
,	O
UK	O
based	O
on	O
the	O
area	O
,	O
the	O
price	O
range	O
or	O
the	O
food	O
type	O
.	O
The	O
current	O
database	O
has	O
just	O
over	O
100	O
restaurants	O
and	O
is	O
implemented	O
using	O
the	O
multi	O
-	O
domain	O
statistical	O
dialogue	O
system	O
toolkit	O
PyDial	O
.	O
To	O
connect	O
PyDial	O
to	O
Dialport	O
,	O
PyDial	O
's	O
dialogue	O
server	O
interface	O
is	O
used	O
.	O
It	O
is	O
implemented	O
as	O
an	O
HTTP	O
server	O
expecting	O
JSON	O
messages	O
from	O
the	O
Dialport	O
client	O
.	O
The	O
system	O
runs	O
a	O
trained	O
dialogue	O
policy	O
based	O
on	O
the	O
GP	O
-	O
SARSA	B-MethodName
algorithm	O
(	O
Gašić	O
et	O
al	O
,	O
2010	O
)	O
.	O
Mr.	O
Clue	O
Mr.	O
Clue	O
plays	O
a	O
simple	O
wordguessing	O
game	O
(	O
Pincus	O
and	O
Traum	O
,	O
2016	O
)	O
.	O
Mr.	O
Clue	O
is	O
the	O
clue	O
-	O
giver	O
and	O
the	O
user	O
plays	O
the	O
role	O
of	O
guesser	O
.	O
Mr.	O
Clue	O
mines	O
his	O
clues	O
from	O
pre	O
-	O
existing	O
web	O
and	O
database	O
resources	O
such	O
as	O
dictionary.com	O
and	O
WordNet	O
.	O
Clue	O
lists	O
used	O
are	O
only	O
clues	O
that	O
pass	O
an	O
automatic	O
filter	O
described	O
in	O
(	O
Pincus	O
and	O
Traum	O
,	O
2016	O
)	O
.	O
The	O
original	O
Mr.	O
Clue	O
was	O
updated	O
to	O
enable	O
successful	O
communication	O
with	O
Dialport	O
.	O
First	O
,	O
since	O
the	O
original	O
Mr.	O
Clue	O
listens	O
for	O
VH	O
messages	O
(	O
a	O
variant	O
of	O
ActiveMQ	O
messaging	O
used	O
by	O
the	O
Virtual	O
Human	O
Toolkit	O
(	O
Hartholt	O
et	O
al	O
,	O
2013	O
)	O
,	O
we	O
built	O
an	O
HTTP	O
server	O
that	O
converts	O
HTTP	O
messages	O
(	O
expected	O
in	O
JSON	O
format	O
)	O
to	O
VH	O
messages	O
.	O
Second	O
,	O
since	O
Di	O
-	O
alPort	O
has	O
multiple	O
users	O
in	O
parallel	O
,	O
Mr.	O
Clue	O
was	O
updated	O
to	O
launch	O
a	O
new	O
agent	B-DatasetName
instance	O
for	O
each	O
new	O
HTTP	O
session	O
(	O
user	O
)	O
that	O
is	O
directed	O
to	O
the	O
game	O
from	O
the	O
main	O
DialPort	O
system	O
.	O
Mr.	O
Clue	O
is	O
always	O
in	O
one	O
of	O
2	O
states	O
(	O
in	O
-	O
game	O
or	O
out	O
-	O
game	O
)	O
.	O
The	O
out	O
-	O
game	O
state	O
dialogue	O
is	O
limited	O
to	O
asking	O
if	O
the	O
user	O
wants	O
to	O
play	O
another	O
round	O
(	O
and	O
offering	O
to	O
give	O
instructions	O
in	O
the	O
beginning	O
of	O
a	O
session	O
)	O
.	O
The	O
user	O
can	O
use	O
goodbye	O
keyword	O
to	O
exit	O
the	O
system	O
at	O
any	O
time	O
.	O
This	O
sends	O
an	O
exit	O
message	O
to	O
Di	O
-	O
alPort	O
and	O
allowing	O
it	O
to	O
take	O
back	O
control	O
.	O
For	O
its	O
150	O
second	O
rounds	O
,	O
timing	O
information	O
is	O
kept	O
on	O
the	O
back	O
-	O
end	O
and	O
sent	O
to	O
the	O
front	O
-	O
end	O
(	O
DialPort	O
)	O
in	O
every	O
message	O
.	O
For	O
each	O
new	O
session	O
,	O
the	O
agent	B-DatasetName
chooses	O
1	O
of	O
77	O
different	O
pre	O
-	O
compiled	O
clue	O
lists	O
(	O
each	O
with	O
10	O
unique	O
target	O
-	O
words	O
)	O
at	O
random	O
.	O
It	O
keeps	O
track	O
of	O
which	O
lists	O
have	O
been	O
used	O
for	O
a	O
session	O
so	O
a	O
user	O
will	O
never	O
play	O
the	O
same	O
round	O
twice	O
(	O
for	O
a	O
given	O
session	O
)	O
.	O
Let'sDiscuss	O
LetsDiscuss	O
responds	O
to	O
queries	O
about	O
a	O
specific	O
restaurant	O
by	O
finding	O
relevant	O
segments	O
of	O
user	O
reviews	O
.	O
It	O
searches	O
a	O
database	O
of	O
restaurant	O
reviews	O
obtained	O
from	O
Zomato	O
and	O
Yelp	O
.	O
We	O
formed	O
a	O
list	O
of	O
general	O
discussion	O
points	O
for	O
restaurants	O
(	O
service	O
,	O
atmosphere	O
,	O
etc	O
)	O
.	O
For	O
each	O
discussion	O
point	O
,	O
a	O
list	O
of	O
relevant	O
keywords	O
was	O
compiled	O
using	O
WordNet	O
,	O
thesaurus	O
,	O
and	O
by	O
categorizing	O
the	O
most	O
frequently	O
words	O
found	O
in	O
reviews	O
.	O
Other	O
Systems	O
QuBot	O
,	O
a	O
chatbot	B-TaskName
from	O
Pohang	O
University	O
and	O
CMU	O
,	O
is	O
used	O
for	O
out	O
-	O
of	O
-	O
domain	O
handling	O
.	O
Let'sForecast	O
,	O
from	O
CMU	O
,	O
uses	O
the	O
NOAA	O
website	O
.	O
Let	O
's	O
Eat	O
from	O
CMU	O
is	O
based	O
on	O
Yelp	O
,	O
finding	O
restaurants	O
for	O
cities	O
that	O
Cambridge	B-DatasetName
does	O
not	O
cover	O
and	O
for	O
Cambridge	B-DatasetName
if	O
that	O
system	O
is	O
down	O
.	O
Let	O
's	O
Go	O
,	O
derived	O
from	O
the	O
Let	O
's	O
Go	O
system	O
(	O
Raux	O
et	O
al	O
,	O
2005	O
)	O
,	O
is	O
based	O
on	O
an	O
end	O
-	O
to	O
-	O
end	O
recurrent	O
neural	O
network	O
structure	O
and	O
a	O
backend	O
that	O
covers	O
cities	O
other	O
than	O
Pittsburgh	O
.	O

In	O
informal	O
trials	O
,	O
some	O
aspects	O
of	O
the	O
portal	O
's	O
interaction	O
were	O
not	O
effective	O
for	O
some	O
users	O
.	O
This	O
included	O
the	O
use	O
of	O
speech	O
(	O
as	O
opposed	O
to	O
typing	O
)	O
,	O
the	O
use	O
of	O
a	O
visual	O
agent	B-DatasetName
,	O
the	O
absence	O
of	O
both	O
graphical	O
and	O
speech	O
response	O
,	O
feedback	O
and	O
portal	O
behavior	O
.	O
Some	O
ES	O
need	O
graphics	O
to	O
supplement	O
their	O
verbal	O
information	O
.	O
Since	O
Mr	O
Clue	O
keeps	O
score	O
and	O
timing	O
of	O
users	O
'	O
answers	O
,	O
its	O
instructions	O
and	O
scores	O
are	O
shown	O
on	O
a	O
blackboard	O
.	O
Let	O
's	O
Go	O
shows	O
a	O
map	O
with	O
the	O
bus	O
trajectory	O
from	O
departure	O
to	O
arrival	O
.	O

As	O
more	O
ES	O
join	O
the	O
portal	O
,	O
policies	O
and	O
strategies	O
have	O
become	O
more	O
flexible	O
.	O
There	O
are	O
two	O
major	O
changes	O
to	O
the	O
portal	O
's	O
behavior	O
:	O
ES	O
selection	O
policy	O
and	O
ES	O
recommendation	O
policy	O
.	O
Starting	O
with	O
few	O
ESes	O
,	O
each	O
on	O
very	O
different	O
topics	O
,	O
the	O
agent	B-DatasetName
selection	O
policy	O
simply	O
tried	O
to	O
detect	O
the	O
topic	O
in	O
the	O
users	O
'	O
request	O
and	O
select	O
the	O
corresponding	O
ES	O
.	O
As	O
more	O
ESes	O
connect	O
to	O
the	O
portal	O
,	O
non	O
-	O
trivial	O
relationships	O
among	O
ESes	O
emerge	O
:	O
1	O
)	O
Dialog	O
context	O
sensitive	O
agent	B-DatasetName
selection	O
:	O
The	O
optimal	O
choice	O
of	O
ES	O
may	O
depend	O
on	O
discourse	O
history	O
.	O
For	O
example	O
,	O
Let'sForecast	O
,	O
Cambridge	B-DatasetName
restaurant	O
and	O
Let	O
's	O
Eat	O
:	O
after	O
the	O
user	O
has	O
weather	O
information	O
for	O
city	O
X	O
,	O
they	O
say	O
,	O
recommend	O
a	O
place	O
to	O
have	O
lunch	O
.	O
Choosing	O
between	O
Let	O
's	O
Eat	O
and	O
Cambridge	B-DatasetName
restaurant	O
depends	O
on	O
the	O
value	O
of	O
city	O
X	O
,	O
because	O
Cambridge	B-DatasetName
restaurant	O
covers	O
places	O
to	O
eat	O
in	O
Cambridge	B-DatasetName
UK	O
and	O
Let	O
's	O
Eat	O
covers	O
other	O
places	O
.	O
2	O
)	O
Discourse	O
Obligation	O
for	O
Agent	B-DatasetName
selection	O
:	O
Users	O
have	O
various	O
ways	O
to	O
make	O
requests	O
:	O
request	O
(	O
tell	O
me	O
xxx	O
)	O
,	O
WH	O
-	O
question	O
(	O
what	O
's	O
the	O
weather	O
in	O
xx	O
)	O
or	O
Yes	O
/	O
No	O
-	O
question	O
(	O
Is	O
it	O
going	O
to	O
rain	O
?	O
)	O
.	O
A	O
natural	O
dialog	O
should	O
answer	O
a	O
user	O
according	O
to	O
the	O
way	O
in	O
which	O
they	O
made	O
their	O
earlier	O
requests	O
(	O
Traum	O
and	O
Allen	O
,	O
1994	O
)	O
.	O
For	O
example	O
,	O
the	O
weather	O
system	O
should	O
produce	O
the	O
natural	O
Yes	O
it	O
's	O
going	O
to	O
rain	O
instead	O
of	O
a	O
full	O
weather	O
report	O
,	O
for	O
the	O
third	O
question	O
above	O
.	O
We	O
thus	O
keep	O
the	O
user	O
's	O
initial	O
request	O
intent	O
in	O
the	O
global	O
dialog	O
context	O
and	O
share	O
it	O
with	O
the	O
relevant	O
ESes	O
.	O
The	O
recommendation	O
policy	O
has	O
been	O
improved	O
in	O
two	O
ways	O
:	O
1	O
)	O
All	O
participating	O
system	O
developers	O
agreed	O
that	O
Skylar	O
should	O
give	O
ES	O
recommendations	O
on	O
a	O
rotating	O
basis	O
so	O
that	O
all	O
systems	O
are	O
recommended	O
equally	O
.	O
Skylar	O
no	O
longer	O
makes	O
a	O
recommendation	O
at	O
the	O
end	O
of	O
each	O
system	O
turn	O
.	O
Recommendations	O
are	O
made	O
about	O
every	O
four	O
turns	O
and	O
,	O
as	O
mentioned	O
above	O
,	O
are	O
not	O
for	O
a	O
system	O
that	O
the	O
user	O
recently	O
interacted	O
with	O
.	O
2	O
)	O
Fine	O
grained	O
recommendation	O
:	O
As	O
more	O
ESes	O
joined	O
the	O
portal	O
,	O
we	O
began	O
to	O
exploit	O
the	O
relatedness	O
among	O
ESs	O
in	O
order	O
to	O
generate	O
more	O
targeted	O
recommendations	O
.	O
For	O
instance	O
,	O
we	O
tuned	O
the	O
policy	O
to	O
have	O
a	O
higher	O
probability	O
of	O
recommending	O
the	O
Let'sDiscuss	O
restaurant	O
review	O
function	O
when	O
users	O
obtain	O
restaurant	O
information	O
by	O
prompting	O
,	O
do	O
you	O
want	O
to	O
hear	O
a	O
review	O
about	O
this	O
place	O
?	O
Finally	O
,	O
the	O
NLU	O
has	O
been	O
extended	O
to	O
support	O
multi	O
-	O
intent	O
multi	O
-	O
domain	O
identification	O
by	O
reducing	O
the	O
problem	O
to	O
a	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
task	O
using	O
a	O
one	O
-	O
vs	O
-	O
all	O
strategy	O
.	O
The	O
weighted	O
average	O
F	O
-	O
1	O
score	O
for	O
multi	O
-	O
intent	O
and	O
multi	O
-	O
domain	O
classification	O
is	O
0.93	O
.	O
There	O
are	O
several	O
types	O
of	O
portal	O
users	O
.	O
First	O
,	O
the	O
developers	O
themselves	O
try	O
out	O
the	O
system	O
.	O
Then	O
they	O
ask	O
friends	O
and	O
family	O
to	O
try	O
it	O
.	O
Users	O
can	O
be	O
paid	O
.	O
Finally	O
we	O
have	O
users	O
who	O
really	O
need	O
the	O
information	O
or	O
gaming	O
pleasure	O
.	O
We	O
define	O
two	O
potential	O
types	O
of	O
users	O
(	O
using	O
IP	O
addresses	O
)	O
:	O
explorers	O
and	O
real	O
users	O
.	O
Explorers	O
are	O
trying	O
the	O
system	O
for	O
the	O
first	O
time	O
.	O
They	O
explore	O
several	O
of	O
the	O
ESes	O
,	O
but	O
they	O
do	O
not	O
have	O
any	O
real	O
gaming	O
or	O
information	O
need	O
.	O
Real	O
users	O
have	O
returned	O
to	O
use	O
the	O
por	O
-	O
tal	O
,	O
asking	O
for	O
something	O
they	O
need	O
or	O
enjoy	O
.	O
They	O
may	O
speak	O
to	O
less	O
of	O
the	O
ESes	O
during	O
their	O
visit	O
,	O
but	O
have	O
some	O
real	O
.	O
The	O
first	O
advertising	O
attempt	O
using	O
Google	B-DatasetName
AdWords	O
3	O
attracted	O
few	O
explorers	O
and	O
no	O
real	O
users	O
.	O
The	O
following	O
factors	O
may	O
explain	O
why	O
users	O
did	O
not	O
have	O
a	O
dialog	O
with	O
the	O
system	O
:	O
presence	O
of	O
human	O
study	O
consent	O
form	O
;	O
not	O
using	O
Chrome	O
browser	O
(	O
solved	O
by	O
making	O
a	O
typing	O
-	O
only	O
version	O
)	O
;	O
user	O
did	O
n't	O
want	O
any	O
portal	O
services	O
;	O
user	O
did	O
n't	O
have	O
a	O
microphone	O
;	O
user	O
did	O
n't	O
understand	O
the	O
purpose	O
of	O
the	O
portal	O
(	O
we	O
gave	O
Skylar	O
an	O
opening	O
monologue	O
explaining	O
what	O
the	O
data	O
is	O
for	O
)	O
.	O

Left	O
-	O
to	O
-	O
Right	O
Dependency	B-TaskName
Parsing	I-TaskName
with	O
Pointer	O
Networks	O

We	O
use	O
the	O
same	O
implementation	O
as	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
and	O
conduct	O
experiments	O
on	O
the	O
Stanford	O
Dependencies	O
(	O
de	O
Marneffe	O
and	O
Manning	O
,	O
2008	O
)	O
conversion	O
(	O
using	O
the	O
Stanford	O
parser	O
v3.3.0	O
)	O
3	O
of	O
the	O
English	O
Penn	B-DatasetName
Treebank	I-DatasetName
(	O
Marcus	O
et	O
al	O
,	O
1993	O
)	O
,	O
with	O
standard	O
splits	O
and	O
predicted	O
PoS	O
tags	O
.	O
In	O
addition	O
,	O
we	O
compare	O
our	O
approach	O
to	O
the	O
original	O
top	O
-	O
down	O
parser	O
on	O
the	O
same	O
twelve	O
languages	O
from	O
the	O
Universal	O
Dependency	O
Treebanks	O
4	O
(	O
UD	B-DatasetName
)	O
that	O
were	O
used	O
by	O
Ma	O
et	O
al	O
(	O
2018	O
)	O
.	O
5	O
Following	O
standard	O
practice	O
,	O
we	O
just	O
exclude	O
punctuation	O
for	O
evaluating	O
on	O
PTB	B-DatasetName
-	O
SD	O
and	O
,	O
for	O
each	O
experiment	O
,	O
we	O
report	O
the	O
average	O
Labelled	O
and	O
Unlabelled	O
Attachment	O
Scores	O
(	O
LAS	O
and	O
UAS	O
)	O
over	O
3	O
and	O
5	O
repetitions	O
for	O
UD	B-DatasetName
and	O
PTB	B-DatasetName
-	O
SD	O
,	O
respectively	O
.	O
3	O
https://nlp.stanford.edu/software/	O
lex	O
-	O
parser.shtml	O
4	O
http://universaldependencies.org	O
5	O
Please	O
note	O
that	O
,	O
since	O
they	O
used	O
a	O
former	O
version	O
of	O
UD	B-DatasetName
datasets	O
,	O
we	O
reran	O
also	O
the	O
top	O
-	O
down	O
algorithm	O
on	O
the	O
latest	O
treebank	O
version	O
(	O
2.2	O
)	O
in	O
order	O
to	O
perform	O
a	O
fair	O
comparison	O
.	O

By	O
outperforming	O
the	O
two	O
current	O
state	O
-	O
of	O
-	O
theart	O
graph	O
-	O
based	O
(	O
Dozat	O
and	O
Manning	O
,	O
2016	O
)	O
and	O
transition	O
-	O
based	O
(	O
Ma	O
et	O
al	O
,	O
2018	O
)	O
models	O
on	O
the	O
PTB	B-DatasetName
-	O
SD	O
,	O
our	O
approach	O
becomes	O
the	O
most	O
accurate	O
fully	O
-	O
supervised	O
dependency	O
parser	O
developed	O
so	O
far	O
,	O
as	O
shown	O
in	O
Table	O
1	O
.	O
6	O
In	O
addition	O
,	O
in	O
Table	O
2	O
we	O
can	O
see	O
how	O
,	O
under	O
the	O
exactly	O
same	O
conditions	O
,	O
the	O
left	O
-	O
to	O
-	O
right	O
algorithm	O
improves	O
over	O
the	O
original	O
top	O
-	O
down	O
variant	O
in	O
nine	O
out	O
of	O
twelve	O
languages	O
in	O
terms	O
of	O
LAS	O
,	O
obtaining	O
competitive	O
results	O
in	O
the	O
remaining	O
three	O
datasets	O
.	O
Finally	O
,	O
in	O
spite	O
of	O
requiring	O
a	O
cycle	O
-	O
checking	O
procedure	O
,	O
our	O
approach	O
proves	O
to	O
be	O
twice	O
as	O
fast	O
as	O
the	O
top	O
-	O
down	O
alternative	O
in	O
decoding	O
time	O
,	O
achieving	O
,	O
under	O
the	O
exact	O
same	O
conditions	O
,	O
a	O
23.08	O
-	O
sentences	O
-	O
per	O
-	O
second	O
speed	O
on	O
the	O
PTB	B-DatasetName
-	O
SD	O
compared	O
to	O
10.24	O
of	O
the	O
original	O
system	O
.	O
7	O

Automatic	O
Fake	B-TaskName
News	I-TaskName
Detection	I-TaskName
:	O
Are	O
current	O
models	O
"	O
fact	O
-	O
checking	O
"	O
or	O
"	O
gut	O
-	O
checking	O
"	O
?	O

Automatic	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
models	O
are	O
ostensibly	O
based	O
on	O
logic	O
,	O
where	O
the	O
truth	O
of	O
a	O
claim	O
made	O
in	O
a	O
headline	O
can	O
be	O
determined	O
by	O
supporting	O
or	O
refuting	O
evidence	O
found	O
in	O
a	O
resulting	O
web	O
query	O
.	O
These	O
models	O
are	O
believed	O
to	O
be	O
reasoning	O
in	O
some	O
way	O
;	O
however	O
,	O
it	O
has	O
been	O
shown	O
that	O
these	O
same	O
results	O
,	O
or	O
better	O
,	O
can	O
be	O
achieved	O
without	O
considering	O
the	O
claim	O
at	O
all	O
-	O
only	O
the	O
evidence	O
.	O
This	O
implies	O
that	O
other	O
signals	O
are	O
contained	O
within	O
the	O
examined	O
evidence	O
,	O
and	O
could	O
be	O
based	O
on	O
manipulable	O
factors	O
such	O
as	O
emotion	B-DatasetName
,	O
sentiment	O
,	O
or	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
(	O
POS	O
)	O
frequencies	O
,	O
which	O
are	O
vulnerable	O
to	O
adversarial	O
inputs	O
.	O
We	O
neutralize	O
some	O
of	O
these	O
signals	O
through	O
multiple	O
forms	O
of	O
both	O
neural	O
and	O
non	O
-	O
neural	O
pre	O
-	O
processing	O
and	O
style	B-TaskName
transfer	I-TaskName
,	O
and	O
find	O
that	O
this	O
flattening	O
of	O
extraneous	O
indicators	O
can	O
induce	O
the	O
models	O
to	O
actually	O
require	O
both	O
claims	O
and	O
evidence	O
to	O
perform	O
well	O
.	O
We	O
conclude	O
with	O
the	O
construction	O
of	O
a	O
model	O
using	O
emotion	B-DatasetName
vectors	O
built	O
off	O
a	O
lexicon	O
and	O
passed	O
through	O
an	O
"	O
emotional	O
attention	O
"	O
mechanism	O
to	O
appropriately	O
weight	O
certain	O
emotions	O
.	O
We	O
provide	O
quantifiable	O
results	O
that	O
prove	O
our	O
hypothesis	O
that	O
manipulable	O
features	O
are	O
being	O
used	O
for	O
fact	O
-	O
checking	O
.	O

There	O
has	O
been	O
significant	O
work	O
with	O
automatic	O
fact	O
-	O
checking	O
models	O
using	O
RNNs	O
and	O
Transformers	O
(	O
Shaar	O
et	O
al	O
,	O
2020a	O
;	O
Alam	O
et	O
al	O
,	O
2020	O
;	O
Shaar	O
et	O
al	O
,	O
2020b	O
)	O
as	O
well	O
as	O
non	O
-	O
neural	O
machine	O
learning	O
using	O
TF	O
-	O
IDF	O
vectors	O
(	O
Reddy	O
et	O
al	O
,	O
2018	O
)	O
.	O
Current	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
models	O
that	O
use	O
a	O
claim	O
's	O
search	O
engine	O
results	O
as	O
evidence	O
may	O
unintentionally	O
use	O
hidden	O
signals	O
that	O
are	O
not	O
attributed	O
to	O
the	O
claim	O
(	O
Hansen	O
et	O
al	O
,	O
2021	O
)	O
.	O
Additionally	O
,	O
models	O
may	O
in	O
fact	O
simply	O
memorize	O
biases	O
within	O
data	O
(	O
Gururangan	O
et	O
al	O
,	O
2018	O
)	O
.	O
Improvements	O
can	O
be	O
made	O
when	O
using	O
human	O
-	O
identified	O
justifications	O
for	O
fact	O
-	O
checking	O
(	O
Alhindi	O
et	O
al	O
,	O
2018	O
;	O
Vo	O
and	O
Lee	O
,	O
2020	O
)	O
,	O
and	O
making	O
use	O
of	O
textual	O
entailment	O
can	O
offer	O
improvements	O
(	O
Saikh	O
et	O
al	O
,	O
2019	O
)	O
.	O
Emotional	O
text	O
can	O
signal	O
low	O
credibility	O
(	O
Rashkin	O
et	O
al	O
,	O
2017	O
)	O
,	O
characterizing	O
fake	O
news	O
as	O
a	O
task	O
where	O
pre	O
-	O
processing	O
can	O
be	O
used	O
effectively	O
to	O
diminish	O
bias	O
(	O
Giachanou	O
et	O
al	O
,	O
2019	O
;	O
Babanejad	O
et	O
al	O
,	O
2020	O
)	O
.	O
A	O
framework	O
to	O
both	O
categorize	O
fake	O
news	O
and	O
to	O
identify	O
features	O
that	O
differentiate	O
fake	O
news	O
from	O
real	O
news	O
has	O
been	O
described	O
by	O
Molina	O
et	O
al	O
(	O
2021	O
)	O
,	O
and	O
debiasing	O
inappropriate	O
subjectivity	O
in	O
text	O
can	O
be	O
accomplished	O
by	O
replacing	O
a	O
single	O
biased	O
word	O
in	O
each	O
sentence	O
(	O
Pryzant	O
et	O
al	O
,	O
2020	O
)	O
.	O
Figure	O
2	O
:	O
Ablation	O
studies	O
where	O
evidence	O
was	O
sequentially	O
removed	O
for	O
training	O
and	O
evaluation	O
of	O
models	O
.	O
On	O
the	O
far	O
left	O
,	O
we	O
show	O
the	O
most	O
effective	O
non	O
-	O
neural	O
pre	O
-	O
processing	O
compared	O
to	O
the	O
baseline	O
of	O
none	O
.	O
Performance	O
generally	O
worsens	O
as	O
the	O
ablation	O
increases	O
.	O
Using	O
the	O
claim	O
as	O
a	O
query	O
,	O
the	O
top	O
ten	O
results	O
from	O
Google	B-DatasetName
News	O
(	O
"	O
snippets	O
"	O
)	O
constitute	O
the	O
evidence	O
(	O
Hansen	O
et	O
al	O
,	O
2021	O
)	O
.	O
PolitiFact	B-DatasetName
and	O
Snopes	B-DatasetName
use	O
five	O
labels	O
(	O
False	O
,	O
Mostly	O
False	O
,	O
Mixture	O
,	O
Mostly	O
True	O
,	O
True	O
)	O
,	O
which	O
we	O
collapse	O
to	O
True	O
,	O
Mixture	O
,	O
and	O
False	O
.	O
To	O
construct	O
the	O
emotion	B-DatasetName
vectors	O
for	O
our	O
EmoAttention	O
system	O
,	O
we	O
use	O
the	O
NRC	O
Affect	O
Intensity	O
Lexicon	O
,	O
which	O
maps	O
approximately	O
6	O
,	O
000	O
terms	O
to	O
values	O
between	O
0	B-DatasetName
and	O
1	O
,	O
representing	O
the	O
term	O
's	O
intensity	O
along	O
8	O
different	O
emotions	O
(	O
Mohammad	O
,	O
2017	O
)	O
.	O
For	O
example	O
,	O
"	O
interrupt	O
"	O
and	O
"	O
rage	O
"	O
are	O
both	O
categorized	O
as	O
anger	O
words	O
,	O
but	O
with	O
the	O
respective	O
intensity	O
values	O
of	O
0.333	O
and	O
0.911	O
.	O

The	O
most	O
common	O
automatic	O
fact	O
-	O
checking	O
NLP	O
models	O
are	O
based	O
on	O
term	O
frequency	O
,	O
word	B-TaskName
embeddings	I-TaskName
,	O
and	O
contextualized	O
word	B-TaskName
embeddings	I-TaskName
,	O
using	O
Random	O
Forests	O
,	O
LSTMs	O
,	O
and	O
BERT	B-MethodName
(	O
Hassan	O
et	O
al	O
,	O
2017	O
)	O
.	O
We	O
limit	O
our	O
experimentation	O
to	O
the	O
BERT	B-MethodName
model	O
,	O
as	O
it	O
is	O
the	O
highest	O
performing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
and	O
was	O
thoroughly	O
tested	O
in	O
(	O
Hansen	O
et	O
al	O
,	O
2021	O
)	O
.	O
This	O
BERT	B-MethodName
model	O
with	O
no	O
pre	O
-	O
processing	O
is	O
our	O
baseline	O
model	O
.	O
For	O
the	O
style	B-TaskName
transfer	I-TaskName
model	O
we	O
use	O
the	O
Styleformer	O
model	O
(	O
Li	O
et	O
al	O
,	O
2018	O
;	O
Schmidt	O
,	O
2020	O
)	O
,	O
a	O
Transformer	B-MethodName
-	O
based	O
seq2seq	B-MethodName
model	O
.	O
We	O
also	O
develop	O
our	O
own	O
BERT	B-MethodName
-	O
based	O
model	O
using	O
the	O
EmoLexi	O
and	O
EmoInt	O
implementation	O
of	O
the	O
EmoCred	O
system	O
by	O
adding	O
an	O
emotional	O
attention	O
layer	O
to	O
emphasize	O
certain	O
emotion	B-DatasetName
representations	O
for	O
a	O
given	O
claim	O
and	O
its	O
evidence	O
(	O
Giachanou	O
et	O
al	O
,	O
2019	O
)	O
.	O
There	O
is	O
also	O
a	O
snippet	O
attention	O
layer	O
at	O
-	O
tending	O
to	O
which	O
evidence	O
itself	O
should	O
be	O
weighted	O
most	O
heavily	O
for	O
the	O
given	O
claim	O
.	O

Our	O
goal	O
is	O
to	O
separate	O
affect	O
-	O
based	O
properties	O
from	O
factual	O
content	O
of	O
the	O
text	O
.	O
Toward	O
this	O
,	O
we	O
run	O
a	O
large	O
number	O
of	O
permutations	O
of	O
the	O
following	O
four	O
simple	O
pre	O
-	O
processing	O
steps	O
(	O
see	O
Figure	O
4	O
in	O
Appendix	O
B	O
for	O
results	O
)	O
.	O
These	O
steps	O
were	O
chosen	O
as	O
they	O
have	O
been	O
shown	O
to	O
facilitate	O
affective	O
tasks	O
such	O
as	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
emotion	B-TaskName
classification	I-TaskName
,	O
and	O
sarcasm	B-TaskName
detection	I-TaskName
(	O
Babanejad	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
some	O
cases	O
we	O
used	O
a	O
modified	O
form	O
-	O
such	O
as	O
removing	O
adverbs	O
for	O
POS	O
pre	O
-	O
processing	O
.	O
Negation	O
(	O
NEG	O
)	O
:	O
A	O
mechanism	O
that	O
transforms	O
a	O
negated	O
statement	O
into	O
its	O
inverse	O
(	O
Benamara	O
et	O
al	O
,	O
2012	O
)	O
.	O
An	O
example	O
,	O
"	O
I	O
am	O
not	O
happy	O
"	O
would	O
have	O
"	O
not	O
"	O
removed	O
and	O
"	O
happy	O
"	O
replaced	O
by	O
its	O
antonym	O
,	O
forming	O
the	O
sentence	O
"	O
I	O
am	O
sad	O
.	O
"	O
Parts	O
-	O
of	O
-	O
Speech	O
(	O
POS	O
)	O
:	O
We	O
keep	O
only	O
three	O
parts	O
of	O
speech	O
:	O
nouns	O
,	O
verbs	O
,	O
and	O
adjectives	O
.	O
We	O
initially	O
included	O
adverbs	O
but	O
found	O
removing	O
them	O
improved	O
results	O
.	O
This	O
could	O
be	O
due	O
to	O
some	O
adverbs	O
being	O
emotionally	O
charged	O
.	O
Stopwords	O
(	O
STOP	O
)	O
:	O
These	O
are	O
generally	O
the	O
most	O
common	O
words	O
in	O
a	O
language	O
,	O
such	O
as	O
function	O
words	O
and	O
prepositions	O
.	O
We	O
use	O
the	O
NLTK	O
library	O
.	O
Stemming	O
(	O
STEM	O
)	O
:	O
Reducing	O
a	O
word	O
to	O
its	O
root	O
form	O
.	O
We	O
use	O
the	O
NLTK	O
Snowball	O
Stemmer	O
.	O

We	O
use	O
the	O
adversarial	O
technique	O
of	O
generating	O
paraphrases	O
for	O
all	O
the	O
claims	O
and	O
evidence	O
through	O
style	B-TaskName
transfer	I-TaskName
.	O
As	O
well	O
,	O
it	O
removes	O
punctuation	O
and	O
alters	O
phrasing	O
that	O
might	O
be	O
understood	O
as	O
sarcasm	O
,	O
such	O
as	O
"	O
Melania	O
Trump	O
said	O
that	O
Native	O
Americans	O
upset	O
about	O
the	O
Dakota	O
Access	O
Pipeline	O
should	O
'	O
go	O
back	O
to	O
India	O
"	O
'	O
to	O
"	O
Melania	O
Trump	O
told	O
Native	O
Americans	O
that	O
was	O
upset	O
by	O
the	O
Dakota	O
Access	O
Pipeline	O
,	O
that	O
they	O
should	O
travel	O
to	O
India	O
.	O
"	O
The	O
informalto	O
-	O
formal	O
model	O
lowercases	O
everything	O
and	O
also	O
changes	O
the	O
text	O
significantly	O
.	O
We	O
chose	O
this	O
paraphrasing	O
model	O
based	O
on	O
the	O
idea	O
that	O
fake	O
news	O
-	O
especially	O
that	O
which	O
is	O
frequently	O
posted	O
on	O
social	O
media	O
-	O
has	O
a	O
certain	O
polarizing	O
style	O
that	O
might	O
be	O
neutralized	O
by	O
altering	O
the	O
formality	O
of	O
the	O
text	O
.	O
Rather	O
surprisingly	O
,	O
we	O
received	O
better	O
results	O
transforming	O
the	O
style	O
from	O
formal	O
-	O
to	O
-	O
informal	O
than	O
we	O
did	O
with	O
informal	O
-	O
toformal	O
.	O

In	O
Figure	O
4	O
,	O
we	O
report	O
all	O
results	O
for	O
each	O
preprocessing	O
step	O
.	O
Figure	O
4	O
:	O
The	O
full	O
table	O
of	O
results	O
for	O
all	O
pre	O
-	O
processing	O
steps	O
for	O
the	O
Snopes	B-DatasetName
(	O
SNES	O
)	O
and	O
PolitiFact	B-DatasetName
(	O
POMT	O
)	O
datasets	O
.	O
Due	O
to	O
the	O
high	O
compute	O
requirements	O
of	O
the	O
formal	O
and	O
informal	O
style	B-TaskName
transfer	I-TaskName
models	O
,	O
these	O
datasets	O
were	O
only	O
prepared	O
for	O
the	O
Snopes	B-DatasetName
dataset	O
.	O
The	O
darkest	O
green	O
colors	O
indicate	O
the	O
best	O
results	O
,	O
while	O
the	O
red	O
indicates	O
the	O
worst	O
.	O
Multiple	O
pre	O
-	O
processing	O
steps	O
such	O
as	O
(	O
pos	O
,	O
stop	O
)	O
were	O
performed	O
in	O
the	O
order	O
written	O
.	O

The	O
text	O
evaluation	O
tool	O
helps	O
to	O
assess	O
Estonian	O
texts	O
for	O
their	O
degrees	O
of	O
complexity	O
according	O
to	O
the	O
CERF	O
(	O
see	O
Figure	O
4	O
)	O
.	O
Currently	O
,	O
the	O
tool	O
takes	O
into	O
account	O
only	O
lexical	O
information	O
and	O
defines	O
the	O
CEFR	O
level	O
of	O
each	O
particular	O
lemma	B-DatasetName
based	O
on	O
CEFR	O
vocabulary	O
lists	O
(	O
see	O
Chapter	O
3.1	O
)	O
.	O
Similar	O
tools	O
have	O
also	O
been	O
developed	O
for	O
many	O
other	O
languages	O
(	O
see	O
e.g.	O
Alfter	O
,	O
2021	O
)	O
.	O
The	O
program	O
runs	O
on	O
EstNLTK	O
v	O
1.6	O
,	O
which	O
offers	O
functionality	O
to	O
lemmatise	O
and	O
perform	O
morphological	B-TaskName
analysis	I-TaskName
.	O
The	O
tool	O
needs	O
to	O
be	O
developed	O
further	O
.	O
First	O
,	O
there	O
is	O
a	O
need	O
to	O
implement	O
methods	O
for	O
the	O
improvement	O
of	O
the	O
analysis	O
of	O
homonyms	O
(	O
for	O
example	O
tamm	O
can	O
mean	O
either	O
an	O
oak	O
or	O
a	O
dam	O
,	O
which	O
is	O
assigned	O
different	O
levels	O
in	O
word	O
lists	O
)	O
and	O
multiword	O
expressions	O
.	O
So	O
far	O
,	O
the	O
analysis	O
is	O
based	O
only	O
on	O
single	O
word	O
lists	O
,	O
which	O
is	O
not	O
sufficient	O
.	O
4	O
Summary	O
"	O
Teacher	O
's	O
Tools	O
"	O
is	O
the	O
first	O
attempt	O
to	O
provide	O
a	O
systematic	O
overview	O
of	O
the	O
development	O
of	O
lexical	O
and	O
grammatical	O
competence	O
in	O
Estonian	O
as	O
a	O
Second	O
Language	O
.	O
The	O
project	O
is	O
a	O
work	O
in	O
progress	O
and	O
further	O
development	O
of	O
the	O
toolkit	O
is	O
foreseen	O
.	O
We	O
plan	O
to	O
add	O
descriptions	O
of	O
the	O
development	O
of	O
grammar	O
competence	O
and	O
communicative	O
language	O
activities	O
for	O
adult	O
learners	O
.	O
The	O
enrichment	O
of	O
the	O
text	O
evaluation	O
module	O
with	O
the	O
possibility	O
of	O
measuring	O
grammatical	O
difficulty	O
and	O
readability	O
(	O
by	O
for	O
example	O
adding	O
Lix	O
-	O
index	O
value	O
)	O
is	O
under	O
development	O
.	O
"	O
Teacher	O
's	O
Tools	O
"	O
as	O
a	O
resource	O
can	O
be	O
used	O
for	O
different	O
CALL	O
tasks	O
,	O
including	O
auto	O
-	O
matic	O
CEFR	O
-	O
related	O
vocabulary	O
and	O
grammar	O
exercise	O
generation	O
or	O
lexical	B-TaskName
simplification	I-TaskName
tasks	O
.	O
The	O
use	O
of	O
NLP	O
for	O
the	O
development	O
of	O
such	O
computerassisted	O
tools	O
has	O
enormous	O
potential	O
for	O
enhancing	O
the	O
teaching	O
and	O
learning	O
of	O
Estonian	O
as	O
an	O
L2	O
.	O

Arguably	O
,	O
spoken	B-TaskName
dialogue	I-TaskName
systems	I-TaskName
are	O
most	O
often	O
used	O
not	O
in	O
hands	O
/	O
eyes	O
-	O
busy	O
situations	O
,	O
but	O
rather	O
in	O
settings	O
where	O
a	O
graphical	O
display	O
is	O
also	O
available	O
,	O
such	O
as	O
a	O
mobile	O
phone	O
.	O
We	O
explore	O
the	O
use	O
of	O
a	O
graphical	O
output	O
modality	O
for	O
signalling	O
incremental	O
understanding	O
and	O
prediction	O
state	O
of	O
the	O
dialogue	O
system	O
.	O
By	O
visualising	O
the	O
current	O
dialogue	O
state	O
and	O
possible	O
continuations	O
of	O
it	O
as	O
a	O
simple	O
tree	O
,	O
and	O
allowing	O
interaction	O
with	O
that	O
visualisation	O
(	O
e.g.	O
,	O
for	O
confirmations	O
or	O
corrections	O
)	O
,	O
the	O
system	O
provides	O
both	O
feedback	O
on	O
past	O
user	O
actions	O
and	O
guidance	O
on	O
possible	O
future	O
ones	O
,	O
and	O
it	O
can	O
span	O
the	O
continuum	O
from	O
slot	B-TaskName
filling	I-TaskName
to	O
full	O
prediction	O
of	O
user	O
intent	O
(	O
such	O
as	O
GoogleNow	O
)	O
.	O
We	O
evaluate	O
our	O
system	O
with	O
real	O
users	O
and	O
report	O
that	O
they	O
found	O
the	O
system	O
intuitive	O
and	O
easy	O
to	O
use	O
,	O
and	O
that	O
incremental	O
and	O
adaptive	O
settings	O
enable	O
users	O
to	O
accomplish	O
more	O
tasks	O
.	O

This	O
section	O
introduces	O
and	O
describes	O
our	O
SDS	O
,	O
which	O
is	O
modularised	O
into	O
four	O
main	O
components	O
:	O
ASR	O
,	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
(	O
NLU	O
)	O
,	O
dialogue	B-TaskName
management	I-TaskName
(	O
DM	O
)	O
,	O
and	O
the	O
graphical	O
user	O
interface	O
(	O
GUI	O
)	O
which	O
,	O
as	O
explained	O
below	O
,	O
is	O
visualised	O
as	O
a	O
right	O
-	O
branching	O
tree	O
.	O
The	O
overall	O
system	O
is	O
represented	O
in	O
Figure	O
1	O
.	O
For	O
the	O
remainder	O
of	O
this	O
section	O
,	O
each	O
module	O
is	O
explained	O
in	O
turn	O
.	O
As	O
each	O
module	O
processes	O
input	O
incrementally	O
(	O
i.e.	O
,	O
word	O
for	O
word	O
)	O
,	O
we	O
first	O
explain	O
our	O
framework	O
for	O
incremental	O
processing	O
.	O

An	O
aspect	O
of	O
our	O
SDS	O
that	O
sets	O
it	O
apart	O
from	O
others	O
is	O
the	O
requirement	O
that	O
it	O
process	O
incrementally	O
.	O
One	O
potential	O
concern	O
with	O
incremental	O
processing	O
is	O
regarding	O
informativeness	O
:	O
why	O
act	O
early	O
when	O
waiting	O
might	O
provide	O
additional	O
information	O
,	O
resulting	O
in	O
better	O
-	O
informed	O
decisions	O
?	O
The	O
trade	O
off	O
is	O
naturalness	O
as	O
perceived	O
by	O
the	O
user	O
who	O
is	O
interacting	O
with	O
the	O
SDS	O
.	O
Indeed	O
,	O
it	O
has	O
been	O
shown	O
that	O
human	O
users	O
perceive	O
incremental	O
systems	O
as	O
being	O
more	O
natural	O
than	O
traditional	O
,	O
turn	O
-	O
based	O
systems	O
(	O
Aist	O
et	O
al	O
,	O
2006	O
;	O
Skantze	O
and	O
Schlangen	O
,	O
2009	O
;	O
Skantze	O
and	O
Hjalmarsson	O
,	O
1991	O
;	O
Asri	O
et	O
al	O
,	O
2014	O
)	O
,	O
offer	O
a	O
more	O
human	O
-	O
like	O
experience	O
(	O
Edlund	O
et	O
al	O
,	O
2008	O
)	O
and	O
are	O
more	O
satisfying	O
to	O
interact	O
with	O
than	O
non	O
-	O
incremental	O
systems	O
(	O
Aist	O
et	O
al	O
,	O
2007	O
)	O
.	O
Psycholinguistic	O
research	O
has	O
also	O
shown	O
that	O
humans	O
comprehend	O
utterances	O
as	O
they	O
unfold	O
and	O
do	O
not	O
wait	O
until	O
the	O
end	O
of	O
an	O
utterance	O
to	O
begin	O
the	O
comprehension	O
process	O
(	O
Tanenhaus	O
et	O
al	O
,	O
1995	O
;	O
Spivey	O
et	O
al	O
,	O
2002	O
)	O
.	O
The	O
trade	O
-	O
off	O
between	O
informativeness	O
and	O
naturalness	O
can	O
be	O
reconciled	O
when	O
mechanisms	O
are	O
in	O
place	O
that	O
allow	O
earlier	O
decisions	O
to	O
be	O
repaired	O
.	O
Such	O
mechanisms	O
are	O
offered	O
by	O
the	O
incremental	O
unit	O
(	O
IU	O
)	O
framework	O
for	O
SDS	O
(	O
Schlangen	O
and	O
Skantze	O
,	O
2011	O
)	O
,	O
which	O
we	O
apply	O
here	O
.	O
Following	O
Kennington	O
et	O
al	O
(	O
2014	O
)	O
,	O
the	O
IU	O
framework	O
consists	O
of	O
a	O
network	O
of	O
processing	O
modules	O
.	O
A	O
typical	O
module	O
takes	O
input	O
,	O
performs	O
some	O
kind	O
of	O
processing	O
on	O
that	O
data	O
,	O
and	O
produces	O
output	O
.	O
The	O
data	O
are	O
packaged	O
as	O
the	O
payload	O
of	O
incremental	O
units	O
(	O
IUs	O
)	O
which	O
are	O
passed	O
between	O
modules	O
.	O
The	O
IUs	O
themselves	O
are	O
interconnected	O
via	O
so	O
-	O
called	O
same	O
level	O
links	O
(	O
SLL	O
)	O
and	O
groundedin	O
links	O
(	O
GRIN	B-MethodName
)	O
,	O
the	O
former	O
allowing	O
the	O
linking	O
of	O
IUs	O
as	O
a	O
growing	O
sequence	O
,	O
the	O
latter	O
allowing	O
that	O
sequence	O
to	O
convey	O
what	O
IUs	O
directly	O
affect	O
it	O
(	O
see	O
Figure	O
2	O
for	O
an	O
example	O
of	O
incremental	O
ASR	O
)	O
.	O
Thus	O
IUs	O
can	O
be	O
added	O
,	O
but	O
can	O
be	O
later	O
revoked	O
and	O
replaced	O
in	O
light	O
of	O
new	O
information	O
.	O
The	O
IU	O
framework	O
can	O
take	O
advantage	O
of	O
up	O
-	O
to	O
-	O
date	O
information	O
,	O
but	O
have	O
the	O
potential	O
to	O
function	O
in	O
such	O
a	O
way	O
that	O
users	O
perceive	O
as	O
more	O
natural	O
.	O
The	O
modules	O
explained	O
in	O
the	O
remainder	O
of	O
this	O
section	O
are	O
implemented	O
as	O
IU	O
-	O
modules	O
and	O
process	O
incrementally	O
.	O
Each	O
will	O
now	O
be	O
explained	O
.	O

The	O
module	O
that	O
takes	O
speech	O
input	O
from	O
the	O
user	O
in	O
our	O
SDS	O
is	O
the	O
ASR	O
component	O
.	O
Incremental	O
ASR	O
must	O
transcribe	O
uttered	O
speech	O
into	O
words	O
which	O
must	O
be	O
forthcoming	O
from	O
the	O
ASR	O
as	O
early	O
as	O
possible	O
(	O
i.e.	O
,	O
the	O
ASR	O
must	O
not	O
wait	O
for	O
endpointing	O
to	O
produce	O
output	O
)	O
.	O
Each	O
module	O
that	O
follows	O
must	O
also	O
process	O
incrementally	O
,	O
acting	O
in	O
lock	O
-	O
step	O
upon	O
input	O
as	O
it	O
is	O
received	O
.	O
Incremental	O
ASR	O
is	O
not	O
new	O
(	O
Baumann	O
et	O
al	O
,	O
2009	O
)	O
and	O
many	O
of	O
the	O
current	O
freely	O
-	O
accessible	O
ASR	O
systems	O
can	O
produce	O
output	O
(	O
semi	O
-	O
)	O
incrementally	O
.	O
We	O
opt	O
for	O
Google	B-DatasetName
ASR	O
for	O
its	O
vocabulary	O
coverage	O
of	O
our	O
evaluation	O
language	O
(	O
German	O
)	O
.	O
Following	O
,	O
Baumann	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
package	O
output	O
from	O
the	O
Google	B-DatasetName
service	O
into	O
IUs	O
which	O
are	O
passed	O
to	O
the	O
NLU	O
module	O
,	O
which	O
we	O
now	O
explain	O
.	O

We	O
approach	O
the	O
task	O
of	O
NLU	O
as	O
a	O
slot	O
-	O
filling	O
task	O
(	O
a	O
very	O
common	O
approach	O
;	O
see	O
Tur	O
et	O
al	O
(	O
2012	O
)	O
)	O
where	O
an	O
intent	O
is	O
complete	O
when	O
all	O
slots	O
of	O
a	O
frame	O
are	O
filled	O
.	O
The	O
main	O
driver	O
of	O
the	O
NLU	O
in	O
our	O
SDS	O
is	O
the	O
SIUM	O
model	O
of	O
NLU	O
introduced	O
in	O
Kennington	O
et	O
al	O
(	O
2013	O
)	O
.	O
SIUM	O
has	O
been	O
used	O
in	O
several	O
systems	O
which	O
have	O
reported	O
substantial	O
results	O
in	O
various	O
domains	O
,	O
languages	O
,	O
and	O
tasks	O
(	O
Han	O
et	O
al	O
,	O
2015	O
;	O
Kennington	O
and	O
Schlangen	O
,	O
2017	O
)	O
Though	O
originally	O
a	O
model	O
of	O
reference	O
resolution	O
,	O
it	O
was	O
always	O
intended	O
to	O
be	O
used	O
for	O
general	O
NLU	O
,	O
which	O
we	O
do	O
here	O
.	O
The	O
model	O
is	O
formalised	O
as	O
follows	O
:	O
P	O
(	O
I	O
|	O
U	O
)	O
=	O
1	O
P	O
(	O
U	O
)	O
P	O
(	O
I	O
)	O
r	O
R	O
P	O
(	O
U	O
|	O
R	O
=	O
r	O
)	O
P	O
(	O
R	O
=	O
r	O
|	O
I	O
)	O
(	O
1	O
)	O
That	O
is	O
,	O
P	O
(	O
I	O
|	O
U	O
)	O
is	O
the	O
probability	O
of	O
the	O
intent	O
I	O
(	O
i.e.	O
,	O
a	O
frame	O
slot	O
)	O
behind	O
the	O
speaker	O
's	O
(	O
ongoing	O
)	O
utterance	O
U	O
.	O
This	O
is	O
recovered	O
using	O
the	O
mediating	O
variable	O
R	O
,	O
a	O
set	O
of	O
properties	O
which	O
map	O
between	O
aspects	O
of	O
U	O
and	O
aspects	O
of	O
I.	O
We	O
opt	O
for	O
abstract	O
properties	O
here	O
(	O
e.g.	O
,	O
the	O
frame	O
for	O
restaurant	O
might	O
be	O
filled	O
by	O
a	O
certain	O
type	O
of	O
cuisine	O
intent	O
such	O
as	O
italian	O
which	O
has	O
properties	O
like	O
pasta	O
,	O
mediterranean	O
,	O
vegetarian	O
,	O
etc	O
.	O
)	O
.	O
Properties	O
are	O
pre	O
-	O
defined	O
by	O
a	O
system	O
designer	O
and	O
can	O
match	O
words	O
that	O
might	O
be	O
uttered	O
to	O
describe	O
the	O
intent	O
in	O
question	O
.	O
For	O
P	O
(	O
R	O
|	O
I	O
)	O
,	O
probability	O
is	O
distributed	O
uniformly	O
over	O
all	O
properties	O
that	O
a	O
given	O
intent	O
is	O
specified	O
to	O
have	O
.	O
(	O
If	O
other	O
information	O
is	O
available	O
,	O
more	O
informative	O
priors	O
could	O
be	O
used	O
as	O
well	O
.	O
)	O
The	O
mapping	O
between	O
properties	O
and	O
aspects	O
of	O
U	O
can	O
be	O
learned	O
from	O
data	O
.	O
During	O
application	O
,	O
R	O
is	O
marginalised	O
over	O
,	O
resulting	O
in	O
a	O
distribution	O
over	O
possible	O
intents	O
.	O
1	O
This	O
occurs	O
at	O
each	O
word	O
increment	O
,	O
where	O
the	O
distribution	O
from	O
the	O
previous	O
increment	O
is	O
combined	O
via	O
P	O
(	O
I	O
)	O
,	O
keeping	O
track	O
of	O
the	O
distribution	O
over	O
time	O
.	O
We	O
further	O
apply	O
a	O
simple	O
rule	O
to	O
add	O
in	O
apriori	O
knowledge	O
:	O
if	O
some	O
r	O
R	O
and	O
w	O
U	O
are	O
such	O
that	O
r	O
.	O
=	O
w	O
(	O
where	O
.	O
=	O
is	O
string	O
equality	O
;	O
e.g.	O
,	O
an	O
intent	O
has	O
the	O
property	O
of	O
pasta	O
and	O
the	O
word	O
pasta	O
is	O
uttered	O
)	O
,	O
then	O
we	O
set	O
C	O
(	O
U	O
=	O
w	O
|	O
R	O
=	O
r	O
)	O
=	O
1	O
.	O
To	O
allow	O
for	O
possible	O
ASR	O
confusions	O
,	O
we	O
also	O
apply	O
C	O
(	O
U	O
=	O
w	O
|	O
R	O
=	O
r	O
)	O
=	O
1	O
−	O
ld	O
(	O
w	O
,	O
r	O
)	O
/max	O
(	O
len	O
(	O
w	O
)	O
,	O
len	O
(	O
r	O
)	O
)	O
,	O
where	O
ld	O
is	O
the	O
Levenshtein	O
distance	O
(	O
but	O
we	O
only	O
apply	O
this	O
if	O
the	O
calculated	O
value	O
is	O
above	O
a	O
threshold	O
of	O
0.6	O
;	O
i.e.	O
,	O
the	O
two	O
strings	O
are	O
mostly	O
similar	O
)	O
.	O
For	O
all	O
other	O
w	O
,	O
C	O
(	O
w	O
|	O
r	O
)	O
=	O
0	B-DatasetName
.	O
This	O
results	O
in	O
a	O
distribution	O
C	O
,	O
which	O
we	O
renormalise	O
and	O
blend	O
with	O
learned	O
distribution	O
to	O
yield	O
P	O
(	O
U	O
|	O
R	O
)	O
.	O
We	O
apply	O
an	O
instantiation	O
of	O
SIUM	O
for	O
each	O
slot	O
.	O
The	O
candidate	O
slots	O
which	O
are	O
processed	O
depends	O
on	O
the	O
state	O
of	O
the	O
dialogue	O
;	O
only	O
slots	O
represented	O
by	O
visible	O
nodes	O
are	O
considered	O
,	O
thereby	O
reducing	O
the	O
possible	O
frames	O
that	O
could	O
be	O
predicted	O
.	O
At	O
each	O
word	O
increment	O
,	O
the	O
updated	O
slots	O
(	O
and	O
their	O
corresponding	O
)	O
distributions	O
are	O
given	O
to	O
the	O
DM	O
,	O
which	O
will	O
now	O
be	O
explained	O
.	O

The	O
goal	O
of	O
the	O
GUI	O
is	O
to	O
intuitively	O
inform	O
the	O
user	O
about	O
the	O
internal	O
state	O
of	O
the	O
ongoing	O
understanding	O
.	O
One	O
motivation	O
for	O
this	O
is	O
that	O
the	O
user	O
can	O
determine	O
if	O
the	O
system	O
understood	O
the	O
user	O
's	O
intent	O
before	O
providing	O
the	O
user	O
with	O
a	O
response	O
(	O
e.g.	O
,	O
a	O
list	O
of	O
restaurants	O
of	O
a	O
certain	O
type	O
)	O
;	O
i.e.	O
,	O
if	O
any	O
misunderstanding	O
takes	O
place	O
,	O
it	O
happens	O
before	O
the	O
system	O
commits	O
to	O
an	O
action	O
and	O
is	O
potentially	O
more	O
easily	O
repaired	O
.	O
The	O
display	O
is	O
a	O
rightbranching	O
tree	O
,	O
where	O
the	O
branches	O
directly	O
off	O
the	O
root	O
node	O
display	O
the	O
affordances	O
of	O
the	O
system	O
(	O
i.e.	O
,	O
what	O
domains	O
of	O
things	O
it	O
can	O
understand	O
and	O
do	O
something	O
about	O
)	O
.	O
When	O
the	O
first	O
tree	O
is	O
displayed	O
,	O
it	O
represents	O
a	O
state	O
of	O
the	O
NLU	O
where	O
none	O
of	O
the	O
slots	O
are	O
filled	O
,	O
as	O
in	O
Figure	O
3	O
.	O
When	O
a	O
user	O
verbally	O
selects	O
a	O
domain	O
to	O
ask	O
about	O
,	O
the	O
tree	O
is	O
adjusted	O
to	O
make	O
that	O
domain	O
the	O
only	O
one	O
displayed	O
and	O
the	O
slots	O
that	O
are	O
required	O
for	O
that	O
domain	O
are	O
shown	O
as	O
branches	O
.	O
The	O
user	O
can	O
then	O
fill	O
those	O
slots	O
(	O
i.e.	O
,	O
branches	O
)	O
by	O
uttering	O
the	O
displayed	O
name	O
,	O
or	O
,	O
alternatively	O
,	O
by	O
uttering	O
the	O
item	O
to	O
fill	O
the	O
slot	O
directly	O
.	O
For	O
example	O
,	O
at	O
a	O
minimum	O
,	O
the	O
user	O
could	O
utter	O
the	O
name	O
of	O
the	O
domain	O
then	O
an	O
item	O
for	O
each	O
slot	O
(	O
e.g.	O
,	O
food	O
Thai	O
downtown	O
)	O
or	O
the	O
speech	O
could	O
be	O
more	O
natural	O
(	O
e.g.	O
,	O
I	O
'm	O
quite	O
hungry	O
,	O
I	O
am	O
looking	O
for	O
some	O
Thai	O
food	O
maybe	O
in	O
the	O
downtown	O
area	O
)	O
.	O
Crucially	O
,	O
the	O
user	O
can	O
also	O
hesitate	O
within	O
and	O
between	O
chunks	O
,	O
as	O
advancement	O
is	O
not	O
triggered	O
by	O
silence	O
thresholding	O
,	O
but	O
rather	O
semantically	O
.	O
When	O
something	O
is	O
uttered	O
that	O
falls	O
into	O
the	O
request	O
state	O
of	O
the	O
DM	O
as	O
explained	O
above	O
,	O
the	O
display	O
expands	O
the	O
subtree	O
under	O
question	O
and	O
marks	O
the	O
item	O
with	O
a	O
question	O
mark	O
(	O
see	O
Figure	O
4	O
)	O
.	O
At	O
this	O
point	O
,	O
the	O
user	O
can	O
utter	O
any	O
kind	O
of	O
confirmation	O
.	O
A	O
positive	O
confirmation	O
fills	O
the	O
slot	O
with	O
the	O
item	O
in	O
question	O
.	O
A	O
negative	O
confirmation	O
retracts	O
the	O
question	O
,	O
but	O
leaves	O
the	O
branch	O
expanded	O
.	O
The	O
expanded	O
branches	O
are	O
displayed	O
according	O
to	O
their	O
rank	O
as	O
given	O
by	O
the	O
NLU	O
's	O
probability	O
distribution	O
.	O
Though	O
a	O
branch	O
in	O
the	O
display	O
can	O
theoretically	O
display	O
an	O
unlimited	O
number	O
of	O
children	O
,	O
we	O
opted	O
to	O
only	O
show	O
7	O
children	O
;	O
if	O
a	O
branch	O
had	O
more	O
,	O
the	O
final	O
child	O
displayed	O
as	O
an	O
ellipsis	O
.	O
A	O
completed	O
branch	O
is	O
collapsed	O
,	O
visually	O
marking	O
its	O
corresponding	O
slot	O
as	O
filled	O
.	O
At	O
any	O
time	O
,	O
a	O
user	O
can	O
backtrack	O
by	O
saying	O
no	O
(	O
or	O
equivalent	O
)	O
or	O
start	O
the	O
entire	O
interaction	O
over	O
from	O
the	O
beginning	O
with	O
a	O
keyword	O
,	O
e.g.	O
,	O
restart	O
.	O
To	O
aid	O
the	O
user	O
's	O
attention	O
,	O
the	O
node	O
under	O
question	O
is	O
marked	O
in	O
red	O
,	O
where	O
completed	O
slots	O
are	O
represented	O
by	O
outlined	O
nodes	O
,	O
and	O
filled	O
nodes	O
represent	O
candidates	O
for	O
the	O
current	O
slot	O
in	O
question	O
(	O
see	O
examples	O
of	O
all	O
three	O
in	O
Figure	O
4	O
)	O
.	O
For	O
cases	O
where	O
the	O
system	O
is	O
in	O
the	O
wait	O
state	O
for	O
several	O
words	O
(	O
during	O
which	O
there	O
is	O
no	O
change	O
in	O
the	O
tree	O
)	O
,	O
the	O
system	O
signals	O
activity	O
at	O
each	O
word	O
by	O
causing	O
the	O
red	O
node	O
in	O
question	O
to	O
temporarily	O
change	O
to	O
white	O
,	O
then	O
back	O
to	O
red	O
(	O
i.e.	O
,	O
appearing	O
as	O
a	O
blinking	O
node	O
to	O
the	O
user	O
)	O
.	O
Figure	O
5	O
shows	O
a	O
filled	O
frame	O
,	O
represented	O
as	O
tree	O
with	O
one	O
branch	O
for	O
each	O
filled	O
slot	O
.	O
Such	O
an	O
interface	O
clearly	O
shows	O
the	O
internal	O
state	O
of	O
the	O
SDS	O
and	O
whether	O
or	O
not	O
it	O
has	O
understood	O
the	O
request	O
so	O
far	O
.	O
It	O
is	O
designed	O
to	O
aid	O
the	O
user	O
's	O
attention	O
to	O
the	O
slot	O
in	O
question	O
,	O
and	O
clearly	O
indicates	O
the	O
affordances	O
that	O
the	O
system	O
has	O
.	O
The	O
interface	O
is	O
currently	O
a	O
read	O
-	O
only	O
display	O
that	O
is	O
purely	O
speech	O
-	O
driven	O
,	O
but	O
it	O
could	O
be	O
augmented	O
with	O
additional	O
functionalities	O
,	O
such	O
as	O
tapping	O
a	O
node	O
for	O
expansion	O
or	O
typing	O
input	O
that	O
the	O
system	O
might	O
not	O
yet	O
display	O
.	O
It	O
is	O
currently	O
implemented	O
as	O
a	O
web	O
-	O
based	O
interface	O
(	O
using	O
the	O
JavaScript	O
D3	O
library	B-DatasetName
)	O
,	O
allowing	O
it	O
to	O
be	O
usable	O
as	O
a	O
web	O
application	O
on	O
any	O
machine	O
or	O
mobile	O
device	O
.	O
Adaptive	O
Branching	O
The	O
GUI	O
as	O
explained	O
affords	O
an	O
additional	O
straight	O
-	O
forward	O
extension	O
:	O
in	O
order	O
to	O
move	O
our	O
system	O
towards	O
adaptivity	O
on	O
the	O
above	O
-	O
mentioned	O
continuum	O
,	O
the	O
GUI	O
can	O
be	O
used	O
to	O
signal	O
what	O
the	O
system	O
thinks	O
the	O
user	O
might	O
say	O
next	O
.	O
This	O
is	O
done	O
by	O
expanding	O
a	O
branch	O
and	O
displaying	O
a	O
confirmation	O
on	O
that	O
branch	O
,	O
signalling	O
that	O
the	O
system	O
predicts	O
that	O
the	O
user	O
will	O
choose	O
that	O
particular	O
branch	O
.	O
Alternatively	O
,	O
if	O
the	O
system	O
is	O
confident	O
that	O
a	O
user	O
will	O
fill	O
a	O
slot	O
with	O
a	O
particular	O
value	O
,	O
that	O
particular	O
slot	O
can	O
be	O
filled	O
without	O
confirmation	O
.	O
This	O
is	O
displayed	O
as	O
a	O
collapsed	O
tree	O
branch	O
.	O
A	O
system	O
that	O
perfectly	O
predicts	O
a	O
user	O
's	O
intent	O
would	O
fill	O
an	O
entire	O
tree	O
(	O
i.e.	O
,	O
all	O
slots	O
)	O
only	O
requiring	O
the	O
user	O
to	O
confirm	O
once	O
.	O
A	O
more	O
careful	O
system	O
would	O
confirm	O
at	O
each	O
step	O
(	O
such	O
an	O
interaction	O
would	O
only	O
require	O
the	O
user	O
to	O
utter	O
confirmations	O
and	O
nothing	O
else	O
)	O
.	O
We	O
applied	O
this	O
adaptive	O
variant	O
of	O
the	O
tree	O
in	O
one	O
of	O
our	O
experiments	O
explained	O
below	O
.	O

The	O
participants	O
were	O
seated	O
at	O
a	O
desk	O
and	O
given	O
written	O
instructions	O
indicating	O
that	O
they	O
were	O
to	O
use	O
the	O
system	O
to	O
perform	O
as	O
many	O
tasks	O
as	O
possible	O
in	O
the	O
allotted	O
time	O
.	O
Figure	O
6	O
shows	O
some	O
example	O
tasks	O
as	O
they	O
would	O
be	O
displayed	O
(	O
one	O
at	O
a	O
time	O
)	O
to	O
the	O
user	O
.	O
A	O
screen	O
,	O
tablet	O
,	O
and	O
keyboard	O
were	O
on	O
the	O
desk	O
in	O
front	O
of	O
the	O
user	O
(	O
see	O
Figure	O
7	O
)	O
.	O
2	O
The	O
user	O
was	O
instructed	O
to	O
convey	O
the	O
task	O
presented	O
on	O
the	O
screen	O
to	O
the	O
system	O
such	O
that	O
the	O
GUI	O
on	O
the	O
tablet	O
would	O
have	O
a	O
completed	O
tree	O
(	O
e.g.	O
,	O
as	O
in	O
Figure	O
5	O
)	O
.	O
When	O
the	O
participant	O
was	O
satisfied	O
that	O
the	O
system	O
understood	O
her	O
intent	O
,	O
she	O
was	O
to	O
press	O
space	O
bar	O
on	O
the	O
keyboard	O
which	O
triggered	O
a	O
new	O
task	O
to	O
be	O
displayed	O
on	O
the	O
screen	O
and	O
reset	O
the	O
tree	O
to	O
its	O
start	O
state	O
on	O
the	O
tablet	O
(	O
as	O
in	O
Figure	O
3	O
)	O
.	O
The	O
possible	O
task	O
domains	O
were	O
call	O
,	O
which	O
had	O
a	O
single	O
slot	O
for	O
name	O
to	O
be	O
filled	O
(	O
i.e.	O
,	O
one	O
out	O
of	O
the	O
22	O
most	O
common	O
German	O
given	O
names	O
)	O
;	O
message	O
which	O
had	O
a	O
slot	O
for	O
name	O
and	O
a	O
slot	O
for	O
the	O
message	O
(	O
which	O
,	O
when	O
invoked	O
,	O
would	O
simply	O
fill	O
in	O
directly	O
from	O
the	O
ASR	O
until	O
1	O
second	O
of	O
silence	O
was	O
detected	O
)	O
;	O
eat	O
which	O
had	O
slots	O
for	O
type	O
(	O
in	O
this	O
case	O
,	O
6	O
possible	O
types	O
)	O
and	O
location	O
(	O
in	O
this	O
case	O
,	O
6	O
locations	O
based	O
around	O
the	O
city	O
of	O
Bielefeld	O
)	O
;	O
route	O
which	O
had	O
slots	O
for	O
source	O
city	O
and	O
the	O
destination	O
city	O
(	O
which	O
shared	O
the	O
same	O
list	O
of	O
the	O
top	O
100	O
most	O
populous	O
German	O
cities	O
)	O
;	O
and	O
reminder	O
which	O
had	O
a	O
slot	O
for	O
message	O
.	O
For	O
each	O
task	O
,	O
the	O
domain	O
was	O
first	O
randomly	O
chosen	O
from	O
the	O
5	O
possible	O
domains	O
,	O
and	O
then	O
each	O
slot	O
value	O
to	O
be	O
filled	O
was	O
randomly	O
chosen	O
(	O
the	O
message	O
slot	O
for	O
the	O
name	O
and	O
message	O
domains	O
was	O
randomly	O
selected	O
from	O
a	O
list	O
of	O
6	O
possible	O
"	O
messages	O
"	O
,	O
each	O
with	O
2	O
-	O
3	O
words	O
;	O
e.g.	O
,	O
feed	O
the	O
cat	O
,	O
visit	O
grandma	O
,	O
etc	O
.	O
)	O
.	O
The	O
system	O
kept	O
track	O
of	O
which	O
tasks	O
were	O
already	O
presented	O
to	O
the	O
participant	O
.	O
At	O
any	O
time	O
after	O
the	O
first	O
task	O
,	O
the	O
system	O
could	O
choose	O
a	O
task	O
that	O
was	O
previously	O
presented	O
and	O
present	O
it	O
again	O
to	O
the	O
participant	O
(	O
with	O
a	O
50	O
%	O
chance	O
)	O
so	O
the	O
user	O
would	O
often	O
see	O
tasks	O
that	O
she	O
had	O
seen	O
before	O
(	O
with	O
the	O
assumption	O
that	O
humans	O
who	O
use	O
PAs	O
often	O
do	O
perform	O
similar	O
,	O
if	O
not	O
the	O
same	O
,	O
tasks	O
more	O
than	O
once	O
)	O
.	O
The	O
participant	O
was	O
told	O
that	O
she	O
would	O
interact	O
with	O
the	O
system	O
in	O
three	O
different	O
phases	O
,	O
each	O
for	O
4	O
minutes	O
,	O
and	O
to	O
accomplish	O
as	O
many	O
tasks	O
as	O
possible	O
in	O
that	O
time	O
allotment	O
.	O
The	O
participant	O
was	O
not	O
told	O
what	O
the	O
different	O
phases	O
were	O
.	O
The	O
experiments	O
described	O
in	O
Sections	O
4.2	O
and	O
screen	O
tablet	O
keyboard	O
participant	O
4.3	O
respectively	O
describe	O
and	O
report	O
a	O
comparison	O
first	O
between	O
the	O
Phase	O
1	O
and	O
2	O
(	O
denoted	O
as	O
the	O
endpointed	O
and	O
incremental	O
variants	O
of	O
the	O
system	O
)	O
in	O
order	O
to	O
establish	O
whether	O
or	O
not	O
the	O
incremental	O
variant	O
produced	O
better	O
results	O
than	O
the	O
endpointed	O
variant	O
.	O
We	O
also	O
report	O
a	O
comparison	O
between	O
Phase	O
2	O
and	O
3	O
(	O
incremental	O
and	O
incremental	O
-	O
adaptive	O
phases	O
)	O
.	O
Phase	O
1	O
and	O
Phase	O
3	O
are	O
not	O
directly	O
comparable	O
to	O
each	O
other	O
as	O
Phase	O
3	O
is	O
really	O
a	O
variant	O
of	O
Phase	O
2	O
.	O
Because	O
of	O
this	O
,	O
we	O
fixed	O
the	O
order	O
of	O
the	O
phase	O
presentation	O
for	O
all	O
participants	O
.	O
Each	O
of	O
these	O
phases	O
are	O
described	O
below	O
.	O
Before	O
the	O
participant	O
began	O
Phase	O
1	O
,	O
they	O
were	O
able	O
to	O
try	O
it	O
out	O
for	O
up	O
to	O
4	O
minutes	O
(	O
in	O
Phase	O
1	O
settings	O
)	O
and	O
ask	O
for	O
help	O
from	O
the	O
experimenter	O
,	O
allowing	O
them	O
to	O
get	O
used	O
to	O
the	O
Phase	O
1	O
interface	O
before	O
the	O
actual	O
experiment	O
began	O
.	O
After	O
this	O
trial	O
phase	O
,	O
the	O
experiment	O
began	O
with	O
Phase	O
1	O
.	O
Phase	O
1	O
:	O
Non	O
-	O
incremental	O
In	O
this	O
phase	O
,	O
the	O
system	O
did	O
not	O
appear	O
to	O
work	O
incrementally	O
;	O
i.e.	O
,	O
the	O
system	O
displayed	O
tree	O
updates	O
after	O
ASR	O
endpointing	O
(	O
of	O
1.2	O
seconds	O
-	O
a	O
reasonable	O
amount	O
of	O
time	O
to	O
expect	O
a	O
response	O
from	O
a	O
commercial	O
spoken	O
PA	O
)	O
.	O
The	O
system	O
displayed	O
the	O
ongoing	O
ASR	O
on	O
the	O
tablet	O
as	O
it	O
was	O
recognised	O
(	O
as	O
is	O
often	O
done	O
in	O
commercial	O
PAs	O
)	O
.	O
At	O
the	O
end	O
of	O
Phase	O
1	O
,	O
a	O
pop	O
up	O
window	O
notified	O
the	O
user	O
that	O
the	O
phase	O
was	O
complete	O
.	O
They	O
then	O
moved	O
onto	O
Phase	O
2	O
.	O
Phase	O
2	O
:	O
Incremental	O
In	O
this	O
phase	O
,	O
the	O
system	O
displayed	O
the	O
tree	O
information	O
incrementally	O
without	O
endpointing	O
.	O
The	O
ASR	O
was	O
no	O
longer	O
displayed	O
;	O
only	O
the	O
tree	O
provided	O
feedback	O
in	O
understanding	O
,	O
as	O
explained	O
in	O
Section	O
3.5	O
.	O
After	O
Phase	O
2	O
,	O
a	O
10	O
-	O
question	O
questionnaire	O
was	O
displayed	O
on	O
the	O
screen	O
for	O
the	O
participant	O
to	O
fill	O
out	O
comparing	O
Phase	O
1	O
and	O
Phase	O
2	O
.	O
For	O
each	O
question	O
,	O
they	O
had	O
the	O
choice	O
of	O
Phase	O
1	O
,	O
Phase	O
2	O
,	O
Both	O
,	O
and	O
Neither	O
.	O
(	O
See	O
Appendix	O
for	O
full	O
list	O
of	O
questions	O
.	O
)	O
After	O
completing	O
the	O
questionnaire	O
,	O
they	O
moved	O
onto	O
Phase	O
3	O
.	O
Phase	O
3	O
:	O
Incremental	O
-	O
adaptive	O
In	O
this	O
phase	O
,	O
the	O
incremental	O
system	O
was	O
again	O
presented	O
to	O
the	O
participant	O
with	O
an	O
added	O
user	O
model	O
that	O
"	O
learned	O
"	O
about	O
the	O
user	O
.	O
If	O
the	O
user	O
saw	O
a	O
task	O
more	O
than	O
once	O
,	O
the	O
user	O
model	O
would	O
predict	O
that	O
,	O
if	O
the	O
user	O
chose	O
that	O
task	O
domain	O
again	O
(	O
e.g.	O
,	O
route	O
)	O
then	O
the	O
system	O
would	O
automatically	O
ask	O
a	O
clarification	O
using	O
the	O
previously	O
filled	O
values	O
(	O
except	O
for	O
the	O
message	O
slot	O
,	O
which	O
the	O
user	O
always	O
had	O
to	O
fill	O
)	O
.	O
If	O
the	O
user	O
saw	O
a	O
task	O
more	O
than	O
3	O
times	O
,	O
the	O
system	O
skipped	O
asking	O
for	O
clarifications	O
and	O
filled	O
in	O
the	O
domain	O
slots	O
completely	O
,	O
requiring	O
the	O
user	O
only	O
to	O
press	O
the	O
space	O
bar	O
to	O
confirm	O
it	O
was	O
the	O
correct	O
one	O
(	O
i.e.	O
,	O
to	O
complete	O
the	O
task	O
)	O
.	O
An	O
example	O
progression	O
might	O
be	O
as	O
follows	O
:	O
a	O
participant	O
is	O
presented	O
with	O
the	O
task	O
route	O
from	O
Bielefeld	O
to	O
Berlin	O
,	O
then	O
the	O
user	O
would	O
attempt	O
to	O
get	O
the	O
system	O
to	O
fill	O
in	O
the	O
tree	O
(	O
i.e.	O
,	O
slots	O
)	O
with	O
those	O
values	O
.	O
After	O
some	O
interaction	O
in	O
other	O
domains	O
,	O
the	O
user	O
sees	O
the	O
same	O
task	O
again	O
,	O
and	O
now	O
after	O
indicating	O
the	O
intent	O
type	O
route	O
,	O
the	O
user	O
must	O
only	O
say	O
"	O
yes	O
"	O
for	O
each	O
slot	O
to	O
confirm	O
the	O
system	O
's	O
prediction	O
.	O
Later	O
,	O
if	O
the	O
task	O
is	O
presented	O
a	O
third	O
time	O
,	O
when	O
entering	O
that	O
domain	O
(	O
i.e	O
,	O
route	O
)	O
,	O
the	O
two	O
slots	O
would	O
already	O
be	O
filled	O
.	O
If	O
later	O
a	O
different	O
route	O
task	O
was	O
presented	O
,	O
e.g.	O
,	O
route	O
from	O
Bielefeld	O
to	O
Hamburg	O
,	O
the	O
system	O
would	O
already	O
have	O
the	O
two	O
slots	O
filled	O
,	O
but	O
the	O
user	O
could	O
backtrack	O
by	O
saying	O
"	O
no	O
,	O
to	O
Hamburg	O
"	O
which	O
would	O
trigger	O
the	O
system	O
to	O
fill	O
the	O
appropriate	O
slot	O
with	O
the	O
corrected	O
value	O
.	O
Later	O
interactions	O
within	O
the	O
route	O
domain	O
would	O
ask	O
for	O
a	O
clarification	O
on	O
the	O
destination	O
slot	O
since	O
it	O
has	O
had	O
several	O
possible	O
values	O
given	O
by	O
the	O
participant	O
,	O
but	O
continue	O
to	O
fill	O
the	O
from	O
slot	O
with	O
Bielefeld	O
.	O
After	O
Phase	O
3	O
,	O
the	O
participants	O
were	O
presented	O
with	O
another	O
questionnaire	O
on	O
the	O
screen	O
to	O
fill	O
out	O
with	O
the	O
same	O
questions	O
(	O
plus	O
two	O
additional	O
questions	O
)	O
,	O
this	O
time	O
comparing	O
Phase	O
2	O
and	O
Phase	O
3	O
.	O
For	O
each	O
item	O
,	O
they	O
had	O
the	O
choice	O
of	O
Phase	O
2	O
,	O
Phase	O
3	O
,	O
Both	O
,	O
and	O
Neither	O
.	O
At	O
the	O
end	O
of	O
the	O
three	O
phases	O
and	O
questionnaires	O
,	O
the	O
participants	O
were	O
given	O
a	O
final	O
questionnaire	O
to	O
fill	O
out	O
by	O
hand	O
on	O
their	O
general	O
impressions	O
of	O
the	O
systems	O
.	O
We	O
recruited	O
14	O
participants	O
for	O
the	O
evaluation	O
.	O
We	O
used	O
the	O
Mint	B-DatasetName
tools	O
data	O
collection	O
framework	O
(	O
Kousidis	O
et	O
al	O
,	O
2012	O
)	O
to	O
log	O
the	O
interactions	O
.	O
Due	O
to	O
some	O
technical	O
issues	O
,	O
one	O
of	O
the	O
participants	O
did	O
not	O
log	O
interactions	O
.	O
We	O
collected	O
data	O
from	O
13	O
participants	O
,	O
post	O
-	O
Phase	O
2	O
questionnaires	O
from	O
12	O
participants	O
,	O
post	O
-	O
Phase	O
3	O
questionnaires	O
from	O
all	O
14	O
participants	O
,	O
and	O
general	O
questionnaires	O
from	O
all	O
14	O
participants	O
.	O
In	O
the	O
experiments	O
that	O
follow	O
,	O
we	O
report	O
objective	O
and	O
subjective	O
measures	O
to	O
determine	O
the	O
settings	O
that	O
produced	O
superior	O
results	O
.	O
Metrics	O
We	O
report	O
the	O
subjective	O
results	O
of	O
the	O
participant	O
questionnaires	O
.	O
We	O
only	O
report	O
those	O
items	O
that	O
were	O
statistically	O
significant	O
(	O
see	O
Appendix	O
for	O
a	O
full	O
list	O
of	O
the	O
questions	O
)	O
.	O
We	O
further	O
report	O
objective	O
measures	O
for	O
each	O
system	O
variant	O
:	O
total	O
number	O
of	O
completed	O
tasks	O
,	O
fully	O
correct	O
frames	O
,	O
average	O
frame	O
f	O
-	O
score	O
,	O
and	O
average	O
time	O
elapsed	O
(	O
averages	O
are	O
taken	O
over	O
all	O
participants	O
for	O
each	O
variant	O
;	O
we	O
only	O
used	O
the	O
10	O
participants	O
who	O
fully	O
interacted	O
with	O
all	O
three	O
phases	O
)	O
.	O
Discussion	O
is	O
left	O
to	O
the	O
end	O
of	O
this	O
section	O
.	O

Given	O
the	O
results	O
and	O
analysis	O
,	O
we	O
conclude	O
that	O
an	O
intuitive	O
presentation	O
that	O
signals	O
a	O
system	O
's	O
ongoing	O
understanding	O
benefits	O
end	O
users	O
who	O
perform	O
simple	O
tasks	O
which	O
might	O
be	O
performed	O
by	O
a	O
PA	O
.	O
The	O
GUI	O
that	O
we	O
provided	O
,	O
using	O
a	O
right	O
-	O
branching	O
tree	O
,	O
worked	O
well	O
;	O
indeed	O
,	O
the	O
participants	O
who	O
used	O
it	O
found	O
it	O
intuitive	O
and	O
easy	O
to	O
understand	O
.	O
There	O
are	O
gains	O
to	O
be	O
made	O
when	O
the	O
system	O
signals	O
understanding	O
at	O
finer	O
-	O
grained	O
levels	O
than	O
just	O
at	O
the	O
end	O
of	O
a	O
pre	O
-	O
formulated	O
utterance	O
.	O
There	O
are	O
further	O
gains	O
to	O
be	O
made	O
when	O
a	O
PA	O
attempts	O
to	O
learn	O
(	O
even	O
a	O
rudimentary	O
)	O
user	O
model	O
to	O
predict	O
what	O
the	O
user	O
might	O
want	O
to	O
do	O
next	O
.	O
The	O
adaptivity	O
moves	O
our	O
system	O
from	O
one	O
extreme	O
of	O
the	O
continuum	O
-	O
simple	O
slot	B-TaskName
filling	I-TaskName
-	O
closer	O
towards	O
the	O
extreme	O
that	O
is	O
fully	O
predictive	O
,	O
with	O
the	O
additional	O
benefit	O
of	O
being	O
able	O
to	O
easily	O
correct	O
mistakes	O
in	O
the	O
predictions	O
.	O
For	O
future	O
work	O
,	O
we	O
intend	O
to	O
provide	O
simple	O
authoring	O
tools	O
for	O
the	O
system	O
to	O
make	O
building	O
simple	O
PAs	O
using	O
our	O
GUI	O
easy	O
.	O
We	O
want	O
to	O
improve	O
the	O
NLU	O
and	O
scale	O
to	O
larger	O
domains	O
.	O
3	O
We	O
also	O
plan	O
on	O
implementing	O
this	O
as	O
a	O
standalone	O
application	O
that	O
could	O
be	O
run	O
on	O
a	O
mobile	O
device	O
,	O
which	O
could	O
actually	O
perform	O
the	O
tasks	O
.	O
It	O
would	O
further	O
be	O
beneficial	O
to	O
compare	O
the	O
GUI	O
with	O
a	O
system	O
that	O
responds	O
with	O
speech	O
(	O
i.e.	O
,	O
without	O
a	O
GUI	O
)	O
.	O
Lastly	O
,	O
we	O
will	O
investigate	O
using	O
touch	O
as	O
an	O
additional	O
input	O
modality	O
to	O
select	O
between	O
possible	O
alternatives	O
that	O
are	O
offered	O
by	O
the	O
system	O
.	O
It	O
was	O
sometimes	O
unclear	O
to	O
me	O
if	O
the	O
assistant	O
understood	O
me	O
.	O
The	O
assistant	O
responded	O
while	O
I	O
spoke	O
.	O
The	O
assistant	O
sometimes	O
did	O
things	O
that	O
I	O
did	O
not	O
expect	O
.	O
When	O
the	O
assistant	O
made	O
mistakes	O
,	O
it	O
was	O
easy	O
for	O
me	O
to	O
correct	O
them	O
.	O
In	O
addition	O
to	O
the	O
above	O
10	O
questions	O
,	O
the	O
following	O
were	O
also	O
asked	O
on	O
the	O
questionnaire	O
following	O
Phase	O
3	O
:	O
I	O
had	O
the	O
feeling	O
that	O
the	O
assistant	O
attempted	O
to	O
learn	O
about	O
me	O
.	O
I	O
had	O
the	O
feeling	O
that	O
the	O
assistant	O
made	O
incorrect	O
guesses	O
.	O
The	O
following	O
questions	O
were	O
used	O
on	O
the	O
general	O
questionnaire	O
:	O
I	O
regularly	O
use	O
personal	O
assistants	O
such	O
as	O
Siri	O
,	O
Cortana	O
,	O
Google	B-DatasetName
now	O
or	O
Amazon	O
Echo	O
:	O
Yes	O
/	O
No	O
I	O
have	O
never	O
used	O
a	O
speech	O
-	O
based	O
personal	O
assistant	O
:	O
Yes	O
/	O
No	O
What	O
was	O
your	O
general	O
impression	O
of	O
our	O
personal	O
assistants	O
?	O
Would	O
you	O
use	O
one	O
of	O
these	O
assistants	O
on	O
a	O
smart	O
phone	O
or	O
tablet	O
if	O
it	O
were	O
available	O
?	O
If	O
yes	O
,	O
which	O
one	O
?	O
Do	O
you	O
have	O
suggestions	O
that	O
you	O
think	O
would	O
help	O
us	O
improve	O
our	O
assistants	O
?	O
If	O
you	O
have	O
used	O
other	O
speech	O
-	O
based	O
interfaces	O
before	O
,	O
do	O
you	O
prefer	O
this	O
interface	O
?	O

Evaluating	O
Word	B-TaskName
Embeddings	I-TaskName
for	O
Language	B-TaskName
Acquisition	I-TaskName

Continuous	O
vector	O
word	O
representations	O
(	O
or	O
word	B-TaskName
embeddings	I-TaskName
)	O
have	O
shown	O
success	O
in	O
capturing	O
semantic	O
relations	O
between	O
words	O
,	O
as	O
evidenced	O
by	O
evaluation	O
against	O
behavioral	O
data	O
of	O
adult	O
performance	O
on	O
semantic	O
tasks	O
(	O
Pereira	O
et	O
al	O
,	O
2016	O
)	O
.	O
Adult	O
semantic	O
knowledge	O
is	O
the	O
endpoint	O
of	O
a	O
language	B-TaskName
acquisition	I-TaskName
process	O
;	O
thus	O
,	O
a	O
relevant	O
question	O
is	O
whether	O
these	O
models	O
can	O
also	O
capture	O
emerging	O
word	O
representations	O
of	O
young	O
language	O
learners	O
.	O
However	O
,	O
the	O
data	O
for	O
children	O
's	O
semantic	O
knowledge	O
across	O
development	O
is	O
scarce	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
to	O
bridge	O
this	O
gap	O
by	O
using	O
Age	O
of	O
Acquisition	O
norms	O
to	O
evaluate	O
word	B-TaskName
embeddings	I-TaskName
learnt	O
from	O
child	O
-	O
directed	O
input	O
.	O
We	O
present	O
two	O
methods	O
that	O
evaluate	O
word	B-TaskName
embeddings	I-TaskName
in	O
terms	O
of	O
(	O
a	O
)	O
the	O
semantic	O
neighbourhood	O
density	O
of	O
learnt	O
words	O
,	O
and	O
(	O
b	O
)	O
convergence	O
to	O
adult	O
word	O
associations	O
.	O
We	O
apply	O
our	O
methods	O
to	O
bag	O
-	O
of	O
-	O
words	O
models	O
,	O
and	O
find	O
that	O
(	O
1	O
)	O
children	O
acquire	O
words	O
with	O
fewer	O
semantic	O
neighbours	O
earlier	O
,	O
and	O
(	O
2	O
)	O
young	O
learners	O
only	O
attend	O
to	O
very	O
local	O
context	O
.	O
These	O
findings	O
provide	O
converging	O
evidence	O
for	O
validity	O
of	O
our	O
methods	O
in	O
understanding	O
the	O
prerequisite	O
features	O
for	O
a	O
distributional	O
model	O
of	O
word	O
learning	O
.	O

Word	B-TaskName
embeddings	I-TaskName
have	O
a	O
long	O
tradition	O
in	O
Computational	O
Linguistics	O
.	O
There	O
exist	O
a	O
range	O
of	O
methods	O
to	O
derive	O
word	B-TaskName
embeddings	I-TaskName
based	O
on	O
the	O
distributional	O
paradigm	O
,	O
such	O
that	O
words	O
with	O
similar	O
embeddings	O
are	O
semantically	O
related	O
.	O
These	O
embeddings	O
are	O
often	O
evaluated	O
either	O
extrinsically	O
,	O
on	O
how	O
well	O
they	O
boost	O
performance	O
on	O
a	O
certain	O
task	O
,	O
or	O
intrinsically	O
,	O
by	O
comparing	O
representations	O
against	O
behavioral	O
data	O
from	O
tests	O
of	O
semantic	O
sim	O
-	O
ilarity	O
,	O
synonymity	O
,	O
analogy	O
or	O
word	O
association	O
(	O
Pereira	O
et	O
al	O
,	O
2016	O
)	O
.	O
Adult	O
semantic	O
knowledge	O
is	O
the	O
culmination	O
of	O
a	O
language	B-TaskName
acquisition	I-TaskName
process	O
;	O
therefore	O
,	O
a	O
relevant	O
question	O
is	O
whether	O
these	O
models	O
can	O
also	O
capture	O
emerging	O
word	O
representations	O
of	O
language	O
learners	O
.	O
A	O
capacity	O
for	O
distributional	O
analysis	O
is	O
a	O
basic	O
assumption	O
of	O
all	O
theories	O
of	O
language	B-TaskName
acquisition	I-TaskName
:	O
children	O
are	O
capable	O
of	O
performing	O
distributional	O
analyses	O
over	O
their	O
input	O
from	O
a	O
young	O
age	O
(	O
Saffran	O
et	O
al	O
,	O
1996	O
)	O
,	O
motivating	O
the	O
use	O
of	O
word	B-TaskName
embeddings	I-TaskName
for	O
modelling	O
language	B-TaskName
acquisition	I-TaskName
.	O
However	O
,	O
the	O
evaluation	O
of	O
emergent	O
word	O
representations	O
is	O
far	O
from	O
straightforward	O
,	O
as	O
there	O
is	O
no	O
availability	O
of	O
the	O
kind	O
of	O
semantic	O
judgements	O
that	O
we	O
have	O
for	O
adults	O
.	O
This	O
paper	O
presents	O
two	O
methods	O
for	O
evaluating	O
word	B-TaskName
embeddings	I-TaskName
for	O
language	B-TaskName
acquisition	I-TaskName
.	O
We	O
apply	O
our	O
methods	O
to	O
two	O
bag	O
-	O
of	O
-	O
words	O
models	O
,	O
and	O
evaluate	O
them	O
on	O
the	O
acquisition	O
of	O
nouns	O
in	O
English	O
-	O
speaking	O
children	O
1	O
.	O

Bag	O
-	O
of	O
-	O
words	O
models	O
offer	O
a	O
good	O
starting	O
point	O
to	O
evaluate	O
word	O
representations	O
in	O
the	O
context	O
of	O
language	B-TaskName
acquisition	I-TaskName
,	O
given	O
their	O
minimal	O
assumptions	O
on	O
knowledge	O
of	O
word	O
order	O
:	O
once	O
the	O
context	O
of	O
a	O
word	O
is	O
determined	O
,	O
the	O
order	O
in	O
which	O
words	O
appear	O
in	O
this	O
context	O
is	O
ignored	O
by	O
these	O
type	O
of	O
models	O
.	O
We	O
explore	O
a	O
range	O
of	O
hyperparameter	O
configurations	O
of	O
two	O
models	O
:	O
a	O
'	O
contextcounting	O
'	O
model	O
involving	O
a	O
PPMI	B-DatasetName
matrix	O
compressed	O
with	O
Singular	O
Value	O
Decomposition	O
(	O
SVD	B-DatasetName
)	O
,	O
and	O
the	O
Skipgram	O
with	O
Negative	O
Sampling	O
(	O
SGNS	O
)	O
version	O
of	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
Note	O
that	O
,	O
although	O
these	O
models	O
have	O
been	O
found	O
to	O
implicitly	O
optimize	O
the	O
same	O
shifted	O
-	O
PPMI	B-DatasetName
matrix	O
(	O
Levy	O
and	O
Goldberg	O
,	O
2014	O
)	O
,	O
they	O
are	O
unlikely	O
to	O
obtain	O
the	O
same	O
results	O
without	O
careful	O
parameter	O
alignment	O
.	O
Our	O
goal	O
by	O
selecting	O
these	O
two	O
approaches	O
is	O
to	O
increase	O
the	O
variability	O
of	O
model	O
performance	O
within	O
the	O
bag	O
-	O
of	O
-	O
words	O
paradigm	O
.	O
The	O
hyperparameters	O
we	O
explore	O
include	O
:	O
window	O
size	O
[	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
7	O
,	O
10	O
]	O
,	O
minimum	O
frequency	O
threshold	O
[	O
10	O
,	O
50	O
,	O
100	O
]	O
,	O
dynamic	O
window	O
(	O
for	O
SGNS	O
)	O
,	O
negative	O
sampling	O
in	O
SGNS	O
[	O
0	B-DatasetName
,	O
15	O
]	O
(	O
and	O
its	O
equivalents	O
as	O
shifted	O
-	O
PPMI	B-DatasetName
)	O
,	O
eigenvalue	O
in	O
SVD	B-DatasetName
[	O
0	B-DatasetName
,	O
0.5	O
,	O
1	O
]	O
.	O
We	O
restrict	O
our	O
analyses	O
to	O
vectors	O
of	O
size	O
100	O
.	O
We	O
use	O
the	O
Hyperwords	O
package	O
from	O
Levy	O
et	O
al	O
(	O
2015	O
)	O
.	O

Our	O
first	O
evaluation	O
method	O
above	O
focused	O
on	O
the	O
structure	O
of	O
the	O
semantic	O
spaces	O
provided	O
by	O
the	O
learnt	O
word	B-TaskName
embeddings	I-TaskName
.	O
Now	O
we	O
turn	O
our	O
attention	O
to	O
the	O
specific	O
lexical	O
items	O
and	O
their	O
position	O
in	O
the	O
semantic	O
space	O
.	O
Children	O
tend	O
to	O
under	O
-	O
and	O
overextend	O
word	O
meaning	O
in	O
the	O
first	O
stages	O
of	O
acquisition	O
,	O
and	O
over	O
time	O
they	O
become	O
more	O
precise	O
on	O
capturing	O
the	O
semantics	O
of	O
words	O
.	O
A	O
logical	O
assumption	O
then	O
,	O
is	O
that	O
words	O
learnt	O
earlier	O
also	O
converge	O
earlier	O
to	O
adult	O
-	O
like	O
semantic	O
representations	O
(	O
assuming	O
that	O
early	O
and	O
late	O
words	O
take	O
,	O
on	O
average	O
,	O
approximately	O
the	O
same	O
amount	O
of	O
time	O
to	O
converge	O
)	O
.	O
We	O
incorporated	O
this	O
idea	O
in	O
our	O
second	O
method	O
by	O
relating	O
the	O
AoA	O
of	O
words	O
with	O
adult	O
free	O
word	O
association	O
norms	O
.	O
Note	O
that	O
this	O
method	O
can	O
be	O
applied	O
to	O
other	O
semantic	O
tasks	O
,	O
but	O
we	O
focus	O
on	O
word	O
association	O
because	O
it	O
does	O
not	O
impose	O
the	O
specific	O
type	O
of	O
semantic	O
relation	O
that	O
words	O
need	O
to	O
have	O
(	O
i.e.	O
there	O
is	O
no	O
distinction	O
between	O
similarity	O
,	O
analogy	O
or	O
others	O
)	O
.	O
The	O
dataset	O
of	O
free	O
word	O
association	O
that	O
we	O
used	O
is	O
known	O
as	O
Small	O
World	O
of	O
Worlds	O
(	O
SWOW	O
,	O
De	O
Deyne	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
it	O
is	O
the	O
largest	O
dataset	O
of	O
word	O
associations	O
in	O
English	O
,	O
containing	O
responses	O
to	O
over	O
12	O
,	O
000	O
cue	O
words	O
.	O
We	O
filtered	O
the	O
preprocessed	O
version	O
of	O
the	O
dataset	O
to	O
include	O
only	O
words	O
that	O
have	O
been	O
acquired	O
before	O
60	O
months	O
old	O
.	O
This	O
results	O
in	O
613	O
cue	O
words	O
,	O
and	O
1839	O
responses	O
(	O
word	O
associates	O
)	O
to	O
these	O
cues	O
.	O
We	O
then	O
performed	O
a	O
similar	O
cue	O
-	O
response	O
experiment	O
,	O
with	O
the	O
best	O
model	O
from	O
the	O
previous	O
section	O
:	O
for	O
each	O
cue	O
,	O
we	O
retrieved	O
the	O
closest	O
n	O
neighbours	O
.	O
As	O
in	O
Pereira	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
used	O
n	O
=	O
50	O
,	O
and	O
then	O
computed	O
how	O
many	O
of	O
these	O
neighbours	O
overlap	O
with	O
the	O
word	O
associates	O
(	O
responses	O
)	O
provided	O
by	O
human	O
adults	O
.	O
However	O
,	O
unlike	O
that	O
work	O
,	O
our	O
evaluation	O
is	O
not	O
based	O
directly	O
on	O
the	O
number	O
of	O
overlaps	O
.	O
Instead	O
,	O
we	O
computed	O
the	O
Spearman	O
rank	O
correlation	O
between	O
the	O
number	O
of	O
overlaps	O
and	O
the	O
AoA	O
norms	O
,	O
in	O
order	O
to	O
quantify	O
whether	O
word	B-TaskName
embeddings	I-TaskName
corresponding	O
to	O
words	O
learned	O
earlier	O
by	O
children	O
are	O
also	O
those	O
that	O
are	O
converging	O
faster	O
to	O
adult	O
semantic	O
knowledge	O
.	O
Figure	O
3	O
shows	O
the	O
result	O
of	O
this	O
procedure	O
.	O
As	O
can	O
be	O
seen	O
,	O
there	O
is	O
a	O
statistically	O
significant	O
rank	O
correlation	O
(	O
ρ	O
=	O
−0.378	O
,	O
p	O
<	O
0.001	O
)	O
.	O
The	O
negative	O
direction	O
confirms	O
that	O
words	O
acquired	O
earlier	O
have	O
a	O
network	O
of	O
word	O
associates	O
that	O
is	O
more	O
similar	O
to	O
those	O
of	O
adults	O
,	O
suggesting	O
that	O
convergence	O
to	O
adult	O
semantic	O
knowledge	O
is	O
at	O
a	O
more	O
advanced	O
state	O
.	O
One	O
limitation	O
of	O
this	O
procedure	O
is	O
that	O
it	O
requires	O
a	O
choice	O
on	O
the	O
number	O
of	O
neighbours	O
to	O
be	O
retrieved	O
.	O
In	O
order	O
to	O
see	O
how	O
much	O
the	O
metric	O
is	O
affected	O
by	O
this	O
parameter	O
,	O
we	O
report	O
the	O
rank	O
correlations	O
of	O
the	O
previous	O
model	O
for	O
several	O
values	O
of	O
n.	O
As	O
can	O
be	O
seen	O
in	O
Figure	O
4	O
,	O
this	O
number	O
stabilizes	O
after	O
n	O
=	O
25	O
.	O
The	O
figure	O
also	O
shows	O
whether	O
this	O
metric	O
favours	O
a	O
model	O
that	O
did	O
not	O
perform	O
well	O
in	O
our	O
previous	O
evaluation	O
metric	O
(	O
SVD	B-DatasetName
with	O
window	O
size	O
4	O
,	O
shift	O
15	O
,	O
frequency	O
threshold	O
10	O
)	O
.	O
The	O
graph	O
shows	O
that	O
this	O
model	O
is	O
consistently	O
worse	O
on	O
our	O
second	O
evaluation	O
method	O
as	O
well	O
.	O

We	O
proposed	O
two	O
methods	O
to	O
evaluate	O
word	B-TaskName
embeddings	I-TaskName
for	O
language	B-TaskName
acquisition	I-TaskName
.	O
The	O
main	O
feature	O
of	O
these	O
methods	O
is	O
the	O
use	O
of	O
AoA	O
norms	O
for	O
assessing	O
whether	O
the	O
semantic	O
organization	O
of	O
the	O
word	B-TaskName
embeddings	I-TaskName
support	O
the	O
developmental	O
trajectory	O
of	O
word	O
learning	O
.	O
The	O
use	O
of	O
these	O
metrics	O
already	O
prompted	O
the	O
discovery	O
that	O
(	O
1	O
)	O
words	O
with	O
fewer	O
neighbours	O
are	O
easier	O
to	O
acquire	O
,	O
suggesting	O
competition	O
of	O
neighbouring	O
words	O
,	O
and	O
(	O
2	O
)	O
at	O
young	O
age	O
,	O
infants	O
only	O
attend	O
to	O
very	O
local	O
context	O
.	O
The	O
application	O
of	O
these	O
methods	O
to	O
distributional	O
models	O
that	O
incorporate	O
additional	O
assumptions	O
(	O
e.g.	O
knowledge	O
of	O
word	O
order	O
)	O
holds	O
promise	O
for	O
further	O
understanding	O
of	O
the	O
role	O
of	O
distributional	O
information	O
in	O
word	O
learning	O
.	O

Tag	O
Assisted	O
Neural	O
Machine	B-TaskName
Translation	I-TaskName
of	O
Film	O
Subtitles	O

Very	O
early	O
work	O
addressed	O
named	O
entity	O
translation	O
by	O
treating	O
automatically	O
identified	O
named	O
entities	O
with	O
a	O
special	O
translation	O
system	O
,	O
usually	O
a	O
transliterator	O
(	O
Babych	O
and	O
Hartley	O
,	O
2003	O
)	O
.	O
This	O
work	O
did	O
not	O
attempt	O
to	O
integrate	O
the	O
translation	O
models	O
for	O
one	O
to	O
benefit	O
from	O
information	O
learned	O
by	O
the	O
other	O
.	O
Later	O
,	O
especially	O
with	O
neural	O
machine	B-TaskName
translation	I-TaskName
(	O
NMT	O
)	O
systems	O
,	O
source	O
-	O
side	O
feature	O
augmentation	O
research	O
studied	O
the	O
inclusion	O
of	O
linguistic	O
feature	O
information	O
into	O
the	O
source	O
-	O
side	O
token	O
embeddings	O
,	O
usually	O
by	O
adding	O
in	O
or	O
concatenating	O
additional	O
learned	O
feature	O
vectors	O
to	O
the	O
token	O
embedding	O
vectors	O
,	O
as	O
we	O
do	O
in	O
this	O
work	O
(	O
Sennrich	O
and	O
Haddow	O
,	O
2016	O
;	O
Hoang	O
et	O
al	O
,	O
2016b	O
;	O
Ugawa	O
et	O
al	O
,	O
2018	O
;	O
Modrzejewski	O
,	O
2020	O
;	O
Armengol	O
-	O
Estapé	O
et	O
al	O
,	O
2020	O
)	O
.	O
This	O
approach	O
can	O
also	O
be	O
adopted	O
on	O
the	O
target	O
-	O
side	O
,	O
as	O
presented	O
here	O
or	O
in	O
(	O
Hoang	O
et	O
al	O
,	O
2016a	O
(	O
Hoang	O
et	O
al	O
,	O
,	O
2018Nguyen	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
these	O
methods	O
only	O
add	O
linguistic	O
feature	O
information	O
to	O
the	O
input	O
,	O
without	O
encouraging	O
the	O
system	O
to	O
model	O
that	O
information	O
in	O
any	O
particular	O
way	O
.	O
Factored	O
translation	O
systems	O
,	O
under	O
both	O
statistical	O
and	O
neural	O
machine	B-TaskName
translation	I-TaskName
,	O
instead	O
explore	O
the	O
addition	O
of	O
externally	O
supplied	O
linguistic	O
features	O
to	O
the	O
raw	O
text	O
at	O
both	O
input	O
and	O
output	O
.	O
These	O
features	O
include	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
(	O
POS	O
)	O
tags	O
,	O
word	O
lemmatizations	O
,	O
morphological	B-TaskName
analysis	I-TaskName
,	O
and	O
semantic	O
analysis	O
(	O
Koehn	O
and	O
Hoang	O
,	O
2007	O
;	O
Garcia	O
-	O
Martinez	O
et	O
al	O
,	O
2016	O
,	O
2017Tan	O
et	O
al	O
,	O
2020	O
)	O
.	O
Factored	O
translation	O
models	O
map	O
feature	O
-	O
augmented	O
input	O
into	O
feature	O
-	O
augmented	O
output	O
,	O
however	O
outputs	O
include	O
only	O
an	O
underlying	O
lemma	B-DatasetName
together	O
with	O
the	O
predicted	O
features	O
.	O
These	O
systems	O
also	O
use	O
a	O
rule	O
-	O
based	O
morphology	O
toolkit	O
in	O
post	O
-	O
processing	O
to	O
generate	O
the	O
output	O
surface	O
forms	O
from	O
predicted	O
output	O
features	O
,	O
requiring	O
knowledge	O
of	O
appropriate	O
rule	O
systems	O
for	O
the	O
output	O
language	O
.	O
An	O
additional	O
tagged	O
architecture	O
(	O
Nȃdejde	O
et	O
al	O
,	O
2017	O
)	O
predicted	O
syntax	O
-	O
tagged	O
surface	O
forms	O
,	O
but	O
did	O
so	O
by	O
appending	O
the	O
tags	O
to	O
the	O
surface	O
form	O
tokens	O
directly	O
,	O
rather	O
than	O
predicting	O
separate	O
factors	O
.	O
In	O
general	O
,	O
the	O
focus	O
of	O
factored	O
models	O
has	O
been	O
to	O
increase	O
vocabulary	O
coverage	O
,	O
for	O
example	O
of	O
highly	O
agglutitanative	O
languages	O
with	O
rich	O
morphologies	O
,	O
rather	O
than	O
our	O
goal	O
of	O
disambiguating	O
polysemous	O
of	O
polysyntactic	O
words	O
or	O
otherwise	O
handling	O
named	O
entities	O
in	O
a	O
more	O
nuanced	O
way	O
.	O
Finally	O
,	O
one	O
previous	O
work	O
does	O
consider	O
a	O
fully	O
tagged	O
(	O
both	O
source	O
and	O
target	O
)	O
factored	O
neural	O
model	O
predicting	O
tags	O
with	O
surface	O
forms	O
with	O
independent	O
layers	O
in	O
much	O
the	O
same	O
way	O
as	O
presented	O
here	O
(	O
Wagner	O
,	O
2017	O
)	O
.	O
This	O
work	O
showed	O
negative	O
results	O
for	O
various	O
syntactic	O
tag	O
types	O
on	O
IWSLT'14	O
shared	O
task	O
data	O
(	O
Cettolo	O
et	O
al	O
,	O
2014	O
)	O
,	O
whereas	O
this	O
work	O
presents	O
NER	B-TaskName
and	O
POS	O
tags	O
on	O
film	O
subtitles	O
data	O
.	O

We	O
implemented	O
two	O
extensions	O
to	O
the	O
standard	O
seq2seq	B-MethodName
encoder	O
-	O
decoder	O
architecture	O
for	O
neural	O
machine	B-TaskName
translation	I-TaskName
to	O
use	O
token	O
-	O
level	O
tags	O
to	O
improve	O
translation	O
results	O
.	O
1	O
By	O
combining	O
token	O
and	O
tag	O
embeddings	O
in	O
the	O
input	O
and	O
simultaneously	O
predicting	O
tokens	O
and	O
tags	O
in	O
the	O
output	O
,	O
the	O
NMT	O
system	O
learned	O
to	O
translate	O
tagged	O
source	O
sentences	O
to	O
tagged	O
target	O
sentences	O
(	O
Figure	O
1	O
)	O
.	O
We	O
used	O
a	O
Transformer	B-MethodName
encoder	O
and	O
decoder	O
for	O
the	O
base	O
seq2seq	B-MethodName
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Tags	O
are	O
added	O
to	O
the	O
data	O
as	O
a	O
preprocessing	O
step	O
.	O

Our	O
experiments	O
focused	O
on	O
film	O
subtitles	O
in	O
German	O
and	O
English	O
.	O
The	O
Opus	O
project	O
provided	O
a	O
parallel	O
German	O
to	O
English	O
subtitles	O
corpus	O
from	O
OpenSubtitles	B-DatasetName
(	O
Tiedemann	O
,	O
2012	O
;	O
Aulamo	O
et	O
al	O
,	O
2020	O
)	O
.	O
This	O
data	O
was	O
cleaned	O
with	O
some	O
rudimentary	O
sentence	O
length	O
filtering	O
,	O
and	O
randomly	O
divided	O
into	O
a	O
3	O
million	O
sentence	O
-	O
pair	O
training	O
split	O
(	O
about	O
49	O
million	O
tokens	O
)	O
,	O
along	O
with	O
100	O
,	O
000	O
pair	O
validation	O
and	O
test	O
splits	O
(	O
about	O
1.6	O
million	O
tokens	O
each	O
)	O
.	O
Around	O
3	O
%	O
of	O
words	O
in	O
the	O
OpenSubtitles	B-DatasetName
corpus	O
were	O
tagged	O
as	O
named	O
entities	O
(	O
non	O
O	O
)	O
.	O
We	O
further	O
divided	O
the	O
test	O
split	O
based	O
on	O
whether	O
any	O
named	O
entities	O
were	O
found	O
in	O
either	O
the	O
source	O
or	O
the	O
target	O
sentence	O
.	O
Out	O
of	O
100	O
,	O
000	O
test	O
pairs	O
,	O
79	O
,	O
201	O
had	O
no	O
named	O
entities	O
,	O
and	O
20	O
,	O
799	O
had	O
some	O
.	O

It	O
should	O
not	O
go	O
unnoticed	O
that	O
the	O
typical	O
inference	O
algorithms	O
for	O
sequence	O
labeling	O
,	O
particularly	O
the	O
BiLSTM	B-MethodName
-	O
CRF	B-MethodName
inference	O
employed	O
by	O
most	O
NER	B-TaskName
systems	O
,	O
are	O
incompatible	O
with	O
the	O
autoregressive	O
sequence	O
decoding	O
algorithms	O
(	O
greedy	O
decoding	O
and	O
beam	O
search	O
)	O
used	O
for	O
inference	O
by	O
seq2seq	B-MethodName
models	O
.	O
That	O
the	O
beam	O
decoding	O
algorithm	O
(	O
and	O
autoregressive	O
likelihood	O
model	O
)	O
used	O
here	O
for	O
tags	O
was	O
unable	O
to	O
account	O
for	O
(	O
be	O
conditioned	O
on	O
)	O
the	O
as	O
-	O
yet	O
uncomputed	O
right	O
context	O
was	O
cause	O
for	O
much	O
apprehension	O
before	O
experimental	O
results	O
became	O
available	O
.	O
These	O
positive	O
results	O
notwithstanding	O
,	O
future	O
work	O
could	O
explore	O
how	O
to	O
better	O
incorporate	O
the	O
full	O
tagging	O
context	O
in	O
tag	O
de	O
-	O
coding	O
,	O
perhaps	O
,	O
for	O
example	O
,	O
by	O
predicting	O
the	O
sequence	O
more	O
wholistically	O
with	O
non	O
-	O
autoregressive	O
decoding	O
(	O
Gu	O
et	O
al	O
,	O
2018	O
)	O
.	O
We	O
also	O
imagine	O
that	O
the	O
design	O
of	O
the	O
underlying	O
seq2seq	B-MethodName
architecture	O
may	O
lend	O
itself	O
to	O
certain	O
types	O
of	O
sequence	O
labeling	O
.	O
For	O
example	O
,	O
the	O
bidirectional	O
context	O
modeled	O
by	O
a	O
BiLSTM	B-MethodName
-	O
based	O
translation	O
model	O
may	O
be	O
more	O
suitable	O
for	O
certain	O
types	O
of	O
sequence	O
labeling	O
tasks	O
than	O
the	O
Transformer	B-MethodName
's	O
attentional	O
activations	O
.	O
Because	O
our	O
contributions	O
are	O
agnostic	O
to	O
the	O
type	O
of	O
sequence	O
labeling	O
(	O
NER	B-TaskName
or	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
or	O
any	O
other	O
kind	O
)	O
as	O
well	O
as	O
to	O
the	O
design	O
of	O
the	O
encoder	O
and	O
decoder	O
,	O
future	O
experiments	O
should	O
also	O
explore	O
these	O
possibilities	O
.	O

This	O
paper	O
introduced	O
the	O
first	O
purpose	O
-	O
built	O
corpus	O
of	O
Māori	O
loanwords	O
on	O
Twitter	O
,	O
as	O
well	O
as	O
a	O
methodology	O
for	O
automatically	O
filtering	O
out	O
irrelevant	O
data	O
via	O
machine	O
learning	O
.	O
The	O
MLT	O
Corpus	O
opens	O
up	O
a	O
myriad	O
of	O
opportunities	O
for	O
future	O
work	O
.	O
Since	O
our	O
corpus	O
is	O
a	O
diachronic	O
one	O
(	O
i.e.	O
,	O
all	O
tweets	O
are	O
time	O
-	O
stamped	O
)	O
,	O
we	O
are	O
planning	O
to	O
use	O
it	O
for	O
testing	O
hypotheses	O
about	O
language	O
change	O
.	O
This	O
is	O
especially	O
desirable	O
in	O
the	O
context	O
of	O
New	O
Zealand	O
English	O
,	O
which	O
has	O
recently	O
undergone	O
considerable	O
change	O
as	O
it	O
comes	O
into	O
the	O
final	O
stage	O
of	O
dialect	O
formation	O
(	O
Schneider	O
,	O
2003	O
)	O
.	O
Another	O
avenue	O
of	O
future	O
research	O
is	O
to	O
automatically	O
identify	O
other	O
Māori	O
loanwords	O
that	O
are	O
not	O
part	O
of	O
our	O
initial	O
list	O
of	O
query	O
words	O
.	O
This	O
could	O
be	O
achieved	O
by	O
deploying	O
a	O
language	O
detector	O
tool	O
on	O
every	O
unique	O
word	O
in	O
the	O
corpus	O
(	O
Martins	O
and	O
Silva	O
,	O
2005	O
)	O
.	O
The	O
"	O
discovered	O
"	O
words	O
could	O
be	O
used	O
as	O
new	O
query	O
words	O
to	O
further	O
expand	O
our	O
corpus	O
.	O
In	O
addition	O
,	O
we	O
intend	O
to	O
explore	O
the	O
meaning	O
of	O
our	O
Māori	O
loanwords	O
using	O
distributional	O
semantic	O
models	O
.	O
We	O
will	O
train	O
popular	O
word	B-TaskName
embeddings	I-TaskName
algorithms	O
on	O
the	O
MLT	O
Corpus	O
,	O
such	O
as	O
Word2Vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
and	O
FastText	B-MethodName
(	O
Bojanowski	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
identify	O
words	O
that	O
are	O
close	O
to	O
our	O
loanwords	O
in	O
the	O
semantic	O
space	O
.	O
We	O
predict	O
that	O
these	O
neighbouring	O
words	O
will	O
enable	O
us	O
to	O
understand	O
the	O
semantic	O
make	O
-	O
up	O
of	O
our	O
loanwords	O
according	O
to	O
their	O
usage	O
.	O
Finally	O
,	O
we	O
hope	O
to	O
extrapolate	O
these	O
findings	O
by	O
deploying	O
our	O
trained	O
classifier	O
on	O
other	O
online	O
discourse	O
sources	O
,	O
such	O
as	O
Reddit	B-DatasetName
posts	O
.	O
This	O
has	O
great	O
potential	O
for	O
enriching	O
our	O
understanding	O
of	O
how	O
Māori	O
loanwords	O
are	O
used	O
in	O
social	O
media	O
.	O

We	O
present	O
experiments	O
for	O
cross	O
-	O
domain	O
semantic	O
dependency	O
analysis	O
with	O
a	O
neural	O
Maximum	O
Subgraph	O
parser	O
.	O
Our	O
parser	O
targets	O
1	O
-	O
endpoint	O
-	O
crossing	O
,	O
pagenumber	O
-	O
2	O
graphs	O
which	O
are	O
a	O
good	O
fit	O
to	O
semantic	O
dependency	O
graphs	O
,	O
and	O
utilizes	O
an	O
efficient	O
dynamic	O
programming	O
algorithm	O
for	O
decoding	O
.	O
For	O
disambiguation	O
,	O
the	O
parser	O
associates	O
words	O
with	O
BiLSTM	B-MethodName
vectors	O
and	O
utilizes	O
these	O
vectors	O
to	O
assign	O
scores	O
to	O
candidate	O
dependencies	O
.	O
We	O
conduct	O
experiments	O
on	O
the	O
data	O
sets	O
from	O
Se	O
-	O
mEval	O
2015	O
as	O
well	O
as	O
Chinese	O
CCGBank	B-DatasetName
.	O
Our	O
parser	O
achieves	O
very	O
competitive	O
results	O
for	O
both	O
English	O
and	O
Chinese	O
.	O
To	O
improve	O
the	O
parsing	O
performance	O
on	O
cross	O
-	O
domain	O
texts	O
,	O
we	O
propose	O
a	O
data	O
-	O
oriented	O
method	O
to	O
explore	O
the	O
linguistic	O
generality	O
encoded	O
in	O
English	O
Resource	O
Grammar	O
,	O
which	O
is	O
a	O
precisionoriented	O
,	O
hand	O
-	O
crafted	O
HPSG	O
grammar	O
,	O
in	O
an	O
implicit	O
way	O
.	O
Experiments	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
data	O
-	O
oriented	O
method	O
across	O
a	O
wide	O
range	O
of	O
conditions	O
.	O

SDP	O
is	O
the	O
task	O
of	O
mapping	O
a	O
natural	O
language	O
sentence	O
into	O
a	O
formal	O
meaning	O
representation	O
in	O
the	O
form	O
of	O
a	O
dependency	O
graph	O
.	O
Figure	O
1	O
shows	O
an	O
Minimal	O
Recursion	O
Semantics	O
(	O
MRS	B-DatasetName
;	O
Copestake	O
et	O
al	O
,	O
2005	O
)	O
reduced	O
semantic	O
dependency	O
analysis	O
(	O
Ivanova	O
et	O
al	O
,	O
2012	O
)	O
.	O
In	O
this	O
example	O
,	O
the	O
semantic	O
analysis	O
is	O
represented	O
as	O
a	O
labeled	O
directed	O
graph	O
in	O
which	O
the	O
vertices	O
are	O
tokens	O
in	O
the	O
sentence	O
.	O
The	O
graph	O
abstracts	O
away	O
from	O
syntactic	O
analysis	O
(	O
e.g.	O
,	O
the	O
complementizer	O
-	O
thatand	O
passive	O
construction	O
are	O
excluded	O
)	O
and	O
includes	O
most	O
semantically	O
relevant	O
non	O
-	O
anaphoric	O
local	O
(	O
e.g.	O
,	O
from	O
"	O
wants	O
"	O
to	O
"	O
Mark	O
"	O
)	O
and	O
longdistance	O
(	O
e.g.	O
,	O
from	O
"	O
buy	O
"	O
to	O
"	O
company	O
"	O
)	O
dependencies	O
.	O
The	O
arc	O
labels	O
encode	O
linguisticallymotivated	O
,	O
broadly	O
-	O
applicable	O
semantic	O
relations	O
that	O
are	O
grounded	O
under	O
the	O
type	O
-	O
driven	O
semantics	O
.	O
It	O
is	O
worth	O
noting	O
that	O
semantic	O
dependency	O
graphs	O
are	O
not	O
necessarily	O
trees	O
:	O
(	O
1	O
)	O
a	O
token	O
may	O
be	O
multiply	O
headed	O
because	O
a	O
word	O
can	O
be	O
the	O
arguments	O
of	O
more	O
than	O
one	O
predicate	O
;	O
(	O
2	O
)	O
cycles	O
are	O
allowed	O
if	O
the	O
direction	O
of	O
arcs	O
are	O
not	O
taken	O
into	O
account	O
.	O

Usually	O
,	O
syntactic	O
dependency	O
analysis	O
employs	O
the	O
tree	O
-	O
shaped	O
representation	O
.	O
Dependency	B-TaskName
parsing	I-TaskName
,	O
thus	O
,	O
can	O
be	O
formulated	O
as	O
the	O
search	O
for	O
a	O
maximum	O
spanning	O
tree	O
(	O
MST	O
)	O
from	O
an	O
arcweighted	O
(	O
complete	O
)	O
graph	O
.	O
For	O
SDP	O
where	O
the	O
target	O
representation	O
are	O
no	O
longer	O
trees	O
,	O
Kuhlmann	O
and	O
Jonsson	O
(	O
2015	O
)	O
proposed	O
to	O
generalize	O
the	O
MST	O
model	O
to	O
other	O
types	O
of	O
subgraphs	O
.	O
In	O
general	O
,	O
dependency	B-TaskName
parsing	I-TaskName
is	O
formulated	O
as	O
the	O
search	O
for	O
Maximum	O
Subgraph	O
regarding	O
to	O
a	O
particular	O
graph	O
class	O
,	O
viz	O
.	O
G	O
:	O
Given	O
a	O
graph	O
G	O
=	O
(	O
V	O
,	O
A	O
)	O
,	O
find	O
a	O
subset	O
A	O
⊆	O
A	O
with	O
maximum	O
total	O
weight	O
such	O
that	O
the	O
induced	O
subgraph	O
G	O
=	O
(	O
V	O
,	O
A	O
)	O
belongs	O
to	O
G.	O
Formally	O
,	O
we	O
have	O
the	O
following	O
optimization	O
problem	O
:	O
G	O
(	O
s	O
)	O
=	O
arg	O
max	O
H	O
G	O
(	O
s	O
,	O
G	O
)	O
SCORE	O
(	O
H	O
)	O
=	O
arg	O
max	O
H	O
G	O
(	O
s	O
,	O
G	O
)	O
p	O
in	O
H	O
SCOREPART	O
(	O
s	O
,	O
p	O
)	O
(	O
1	O
)	O
Here	O
,	O
G	O
(	O
s	O
,	O
G	O
)	O
is	O
the	O
set	O
of	O
all	O
graphs	O
that	O
belong	O
to	O
G	O
and	O
are	O
compatible	O
with	O
s	O
and	O
G.	O
For	O
parsing	O
,	O
G	O
is	O
usually	O
a	O
complete	O
graph	O
.	O
SCOREPART	O
(	O
s	O
,	O
p	O
)	O
evaluates	O
whether	O
a	O
small	O
subgraph	O
p	O
of	O
a	O
candidate	O
graph	O
H	O
is	O
a	O
good	O
partial	O
analysis	O
for	O
sentence	O
s.	O
For	O
some	O
graph	O
classes	O
and	O
some	O
types	O
of	O
score	O
functions	O
,	O
there	O
exists	O
efficient	O
algorithms	O
for	O
solving	O
(	O
1	O
)	O
.	O
For	O
example	O
,	O
when	O
G	O
is	O
the	O
set	O
of	O
noncrossing	O
graphs	O
and	O
SCOREPART	O
is	O
limited	O
to	O
handle	O
individual	O
dependencies	O
,	O
(	O
1	O
)	O
can	O
be	O
solved	O
in	O
cubic	O
-	O
time	O
(	O
Kuhlmann	O
and	O
Jonsson	O
,	O
2015	O
)	O
.	O

We	O
use	O
words	O
as	O
well	O
as	O
POS	O
tags	O
as	O
clues	O
for	O
scoring	O
an	O
individual	O
arc	O
.	O
In	O
particular	O
,	O
we	O
transform	O
all	O
of	O
them	O
into	O
continuous	O
and	O
dense	O
vectors	O
.	O
Inspired	B-DatasetName
by	O
Costa	O
-	O
jussà	O
and	O
Fonollosa	O
(	O
2016	O
)	O
's	O
work	O
,	O
we	O
utilize	O
character	O
-	O
based	O
embedding	O
for	O
low	O
-	O
frequency	O
words	O
,	O
i.e.	O
,	O
words	O
that	O
appear	O
more	O
than	O
k	O
times	O
in	O
the	O
training	O
data	O
,	O
and	O
word	O
-	O
based	O
embeddings	O
for	O
other	O
words	O
.	O
The	O
word	O
-	O
based	O
embedding	O
module	O
applies	O
the	O
common	O
lookup	O
-	O
table	O
mechanism	O
,	O
while	O
the	O
character	O
-	O
based	O
word	O
embedding	O
w	O
i	O
is	O
implemented	O
by	O
extracting	O
the	O
features	O
(	O
denoted	O
as	O
c	O
1	O
,	O
c	O
2	O
,	O
.	O
.	O
.	O
,	O
c	O
n	O
)	O
within	O
a	O
character	O
-	O
based	O
BiLSTM	B-MethodName
:	O
x	O
1	O
:	O
x	O
n	O
=	O
BiLSTM	B-MethodName
(	O
c	O
1	O
:	O
c	O
n	O
)	O
w	O
i	O
=	O
x	O
1	O
+	O
x	O
n	O

The	O
concatenation	O
of	O
word	O
embedding	O
w	O
i	O
and	O
POS	O
-	O
tag	O
embedding	O
p	O
i	O
of	O
each	O
word	O
in	O
specific	O
sentence	O
is	O
used	O
as	O
the	O
input	O
of	O
BiLSTMs	O
to	O
extract	O
context	O
-	O
related	O
feature	O
vectors	O
r	O
i	O
for	O
each	O
position	O
i.	O
a	O
i	O
=	O
w	O
i	O
p	O
i	O
r	O
1	O
:	O
r	O
n	O
=	O
BiLSTM	B-MethodName
(	O
a	O
1	O
:	O
a	O
n	O
)	O

In	O
our	O
first	O
order	O
model	O
,	O
the	O
SCORE	O
function	O
evaluates	O
the	O
preference	O
of	O
a	O
semantic	O
dependency	O
graph	O
by	O
considering	O
every	O
bilexical	O
relation	O
in	O
this	O
graph	O
one	O
by	O
one	O
.	O
In	O
particular	O
,	O
the	O
corresponding	O
SCOREPART	O
function	O
assigns	O
a	O
score	O
to	O
a	O
candidate	O
arc	O
between	O
word	O
i	O
and	O
word	O
j	O
using	O
a	O
non	O
-	O
linear	O
transform	O
from	O
the	O
two	O
feature	O
vectors	O
,	O
viz	O
.	O
r	O
i	O
and	O
r	O
j	O
,	O
associated	O
to	O
the	O
two	O
words	O
:	O
SCOREPART	O
(	O
i	O
,	O
j	O
)	O
=	O
W	O
2	O
ReLU	B-MethodName
(	O
W	O
1	O
,	O
1	O
r	O
i	O
+	O
W	O
1	O
,	O
2	O
r	O
j	O
+	O
b	O
)	O
The	O
assignment	O
task	O
for	O
dependency	O
labels	O
can	O
be	O
regarded	O
as	O
a	O
classification	O
task	O
.	O
Our	O
label	O
scoring	O
process	O
is	O
similar	O
to	O
the	O
prediction	O
of	O
dependencies	O
:	O
LABEL	O
(	O
i	O
,	O
j	O
)	O
=	O
arg	O
max	O
W	O
2	O
ReLU	B-MethodName
(	O
W	O
1	O
,	O
1	O
r	O
i	O
+	O
W	O
1	O
,	O
2	O
r	O
j	O
+	O
b	O
)	O
+	O
b	O
2	O
We	O
can	O
see	O
here	O
the	O
two	O
local	O
score	O
functions	O
explicitly	O
utilize	O
the	O
positions	O
of	O
a	O
semantic	O
head	O
and	O
a	O
semantic	O
dependent	O
.	O
It	O
is	O
similar	O
to	O
the	O
firstorder	O
factorization	O
as	O
defined	O
in	O
a	O
number	O
of	O
linear	O
parsing	O
models	O
,	O
e.g.	O
,	O
the	O
models	O
defined	O
by	O
Martins	O
and	O
Almeida	O
(	O
2014	O
)	O
and	O
Cao	O
et	O
al	O
(	O
2017a	O
)	O
.	O

Intuitively	O
,	O
a	O
hand	O
-	O
crafted	O
precision	O
grammar	O
,	O
e.g.	O
,	O
ERG	O
,	O
reflects	O
highly	O
generalized	O
properties	O
of	O
a	O
particular	O
language	O
and	O
is	O
thus	O
highly	O
resilient	O
to	O
domain	O
shifts	O
.	O
Accordingly	O
,	O
one	O
should	O
expect	O
that	O
a	O
precision	O
grammar	O
-	O
guided	O
parser	O
which	O
guarantees	O
the	O
a	O
rich	O
set	O
of	O
domain	O
-	O
independent	O
linguistic	O
constraints	O
to	O
be	O
met	O
can	O
be	O
more	O
robust	O
to	O
domain	O
shifts	O
than	O
a	O
purely	O
data	O
-	O
driven	O
parser	O
.	O
In	O
related	O
work	O
for	O
syntactic	O
parsing	O
,	O
Ivanova	O
et	O
al	O
(	O
2013	O
)	O
showed	O
that	O
the	O
ERG	O
-	O
based	O
parser	O
was	O
more	O
robust	O
to	O
domain	O
variation	O
than	O
several	O
representative	O
data	O
-	O
driven	O
parsers	O
.	O
Zhang	O
and	O
Wang	O
(	O
2009	O
)	O
proposed	O
to	O
derive	O
features	O
from	O
syntactic	O
parses	O
generated	O
by	O
PET	B-DatasetName
to	O
assist	O
a	O
data	O
-	O
driven	O
dependency	O
tree	O
parser	O
and	O
observed	O
some	O
encouraging	O
results	O
for	O
cross	O
-	O
domain	O
evaluation	O
.	O
However	O
,	O
there	O
are	O
at	O
least	O
two	O
drawbacks	O
of	O
their	O
ERG	O
-	O
guided	O
parser	O
based	O
method	O
:	O
1	O
.	O
A	O
considerable	O
number	O
of	O
sentences	O
can	O
not	O
benefit	O
from	O
ERG	O
since	O
PET	B-DatasetName
may	O
produce	O
no	O
analysis	O
.	O
2	O
.	O
This	O
method	O
fails	O
to	O
take	O
parsing	O
efficiency	O
into	O
account	O
.	O

Since	O
around	O
2001	O
,	O
the	O
ERG	O
has	O
been	O
accompanied	O
by	O
syntactico	O
-	O
semantic	O
annotations	O
,	O
where	O
for	O
each	O
sentence	O
an	O
annotator	O
has	O
selected	O
the	O
intended	O
analysis	O
among	O
all	O
alternatives	O
licensed	O
by	O
the	O
grammar	O
.	O
This	O
derived	O
resource	O
,	O
namly	O
Redwoods	O
6	O
(	O
Oepen	O
et	O
al	O
,	O
2002	O
;	O
Flickinger	O
et	O
al	O
,	O
2017	O
)	O
,	O
is	O
a	O
collection	O
of	O
hand	O
-	O
annotated	O
corpora	O
and	O
consists	O
of	O
data	O
sets	O
from	O
several	O
distinct	O
domains	O
.	O
Redwoods	O
also	O
includes	O
(	O
re	O
)	O
treebanking	O
results	O
of	O
the	O
first	O
22	O
sections	O
of	O
the	O
venerable	O
Wall	O
Street	O
Journal	O
(	O
WSJ	O
)	O
text	O
and	O
the	O
section	O
of	O
Brown	O
Corpus	O
in	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
(	O
Marcus	O
et	O
al	O
,	O
1993	O
)	O
.	O
The	O
WSJ	O
part	O
is	O
also	O
known	O
as	O
Deep	O
-	O
Bank	O
.	O
The	O
Brown	O
corpus	O
part	O
is	O
used	O
as	O
the	O
out	O
-	O
of	O
-	O
domain	O
test	O
data	O
by	O
Se	O
-	O
mEval	O
2015	O
.	O
The	O
DM	O
data	O
sets	O
for	O
both	O
SemEval	O
2014	O
and	O
2015	O
SDP	O
shared	O
tasks	O
are	O
based	O
on	O
the	O
RedWoods	O
corpus	O
.	O
Besides	O
gold	O
standard	O
annoations	O
,	O
Flickinger	O
et	O
al	O
(	O
2010	O
)	O
built	O
the	O
WikiWoods	O
corpus	O
7	O
,	O
which	O
provides	O
automatically	O
created	O
annotations	O
for	O
the	O
texts	O
from	O
wikipedia	O
.	O
The	O
annotations	O
are	O
disambiguated	O
using	O
the	O
MaxEnt	O
model	O
trained	O
using	O
redwoods	O
without	O
DeepBank	O
.	O
We	O
use	O
a	O
small	O
portion	O
of	O
Wikiwoods	O
,	O
which	O
contains	O
857	O
,	O
329	O
sentences	O
in	O
total	O
.	O
To	O
evaluate	O
the	O
(	O
positive	O
)	O
impact	O
of	O
ERG	O
on	O
out	O
-	O
of	O
-	O
domain	O
parsing	O
,	O
we	O
conduct	O
experiments	O
on	O
the	O
DM	O
data	O
.	O
The	O
first	O
group	O
of	O
experiments	O
are	O
designed	O
to	O
be	O
comparable	O
with	O
the	O
results	O
obtained	O
by	O
various	O
participant	O
systems	O
of	O
SemEval	O
2015	O
.	O
The	O
detailed	O
data	O
set	O
-	O
up	O
is	O
as	O
follows	O
:	O
Test	O
Data	O
.	O
We	O
use	O
the	O
Brown	O
corpus	O
section	O
which	O
is	O
provided	O
by	O
SemEval	O
2015	O
.	O
Training	O
Data	O
.	O
We	O
use	O
three	O
data	O
sets	O
for	O
training	O
:	O
(	O
1	O
)	O
DeepBank	O
,	O
(	O
2	O
)	O
RedWoods	O
and	O
(	O
3	O
)	O
a	O
small	O
portion	O
of	O
WikiWoods	O
reparsed	O
using	O
the	O
MaxEnt	O
model	O
trained	O
on	O
Deep	O
-	O
Bank	O
.	O
We	O
denote	O
this	O
reparsed	O
WikiWoods	O
as	O
WikiWoods	O
-	O
ACE	O
,	O
since	O
the	O
HPSG	O
analysis	O
is	O
provided	O
by	O
the	O
ACE	O
parser	O
.	O
To	O
extract	O
the	O
semantic	O
dependency	O
graph	O
,	O
we	O
use	O
the	O
pydelphin	O
tool	O
8	O
.	O
For	O
the	O
second	O
group	O
of	O
experiments	O
,	O
we	O
use	O
the	O
section	O
wsj21	O
from	O
the	O
DeepBank	O
as	O
test	O
data	O
,	O
which	O
is	O
the	O
official	O
in	O
-	O
domain	O
test	O
of	O
the	O
SemEval	O
2015	O
.	O
The	O
training	O
data	O
includes	O
the	O
"	O
RedWoods	O
minus	O
DeepBank	O
"	O
annotations	O
(	O
RedwoodsWOD	O
for	O
short	O
)	O
as	O
well	O
as	O
the	O
official	O
WikiWoods	O
annotations	O
.	O
Note	O
that	O
the	O
MaxEnt	O
model	O
used	O
to	O
obtain	O
the	O
official	O
WikiWoods	O
annotations	O
are	O
compatible	O
with	O
RedwoodswWOD	O
.	O
Due	O
to	O
the	O
diversity	O
of	O
the	O
RedwoodsWOD	O
and	O
DeepBank	O
sentences	O
,	O
this	O
set	O
-	O
up	O
can	O
also	O
be	O
viewed	O
as	O
an	O
outof	O
-	O
domain	O
evaluation	O
.	O
WikiWoods	O
to	O
train	O
another	O
model	O
,	O
and	O
leave	O
out	O
other	O
parts	O
of	O
Redwoods	O
.	O
The	O
performance	O
improvement	O
is	O
more	O
remarkable	O
when	O
providing	O
more	O
data	O
,	O
even	O
though	O
such	O
data	O
contains	O
annotation	O
errors	O
.	O
For	O
the	O
second	O
group	O
of	O
experiments	O
,	O
we	O
use	O
the	O
RedwoodsWOD	O
sentences	O
for	O
training	O
and	O
the	O
DeepBank	O
WSJ	O
sentences	O
for	O
evaluation	O
.	O
For	O
this	O
set	O
-	O
up	O
,	O
consistent	O
improvements	O
of	O
the	O
parser	O
quality	O
are	O
observed	O
.	O

Open	O
Relation	B-TaskName
Extraction	I-TaskName
:	O
Relational	O
Knowledge	O
Transfer	O
from	O
Supervised	O
Data	O
to	O
Unsupervised	O
Data	O

Open	O
relation	B-TaskName
extraction	I-TaskName
(	O
OpenRE	O
)	O
aims	O
to	O
extract	O
relational	O
facts	O
from	O
the	O
open	O
-	O
domain	O
corpus	O
.	O
To	O
this	O
end	O
,	O
it	O
discovers	O
relation	O
patterns	O
between	O
named	O
entities	O
and	O
then	O
clusters	O
those	O
semantically	O
equivalent	O
patterns	O
into	O
a	O
united	O
relation	O
cluster	O
.	O
Most	O
OpenRE	O
methods	O
typically	O
confine	O
themselves	O
to	O
unsupervised	O
paradigms	O
,	O
without	O
taking	O
advantage	O
of	O
existing	O
relational	O
facts	O
in	O
knowledge	O
bases	O
(	O
KBs	O
)	O
and	O
their	O
high	O
-	O
quality	O
labeled	O
instances	O
.	O
To	O
address	O
this	O
issue	O
,	O
we	O
propose	O
Relational	O
Siamese	O
Networks	O
(	O
RSNs	O
)	O
to	O
learn	O
similarity	O
metrics	O
of	O
relations	O
from	O
labeled	O
data	O
of	O
pre	O
-	O
defined	O
relations	O
,	O
and	O
then	O
transfer	O
the	O
relational	O
knowledge	O
to	O
identify	O
novel	O
relations	O
in	O
unlabeled	O
data	O
.	O
Experiment	O
results	O
on	O
two	O
real	O
-	O
world	O
datasets	O
show	O
that	O
our	O
framework	O
can	O
achieve	O
significant	O
improvements	O
as	O
compared	O
with	O
other	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O
Our	O
code	O
is	O
available	O
at	O
https://github	O
.	O
com	O
/	O
thunlp	O
/	O
RSN	O
.	O

Open	O
Relation	B-TaskName
Extraction	I-TaskName
.	O
Relation	B-TaskName
extraction	I-TaskName
(	O
RE	O
)	O
is	O
an	O
important	O
task	O
in	O
NLP	O
.	O
Traditional	O
RE	O
methods	O
mainly	O
concentrate	O
on	O
classifying	O
relational	O
facts	O
into	O
pre	O
-	O
defined	O
relation	O
types	O
(	O
Mintz	O
et	O
al	O
,	O
2009	O
;	O
Yu	O
et	O
al	O
,	O
2017	O
)	O
.	O
Zeng	O
(	O
2014	O
)	O
utilizes	O
CNN	O
encoders	O
to	O
build	O
sentence	O
representations	O
with	O
the	O
help	O
of	O
position	O
embeddings	O
.	O
Lin	O
(	O
2016	O
)	O
further	O
improves	O
RE	O
performance	O
on	O
distantlysupervised	O
data	O
via	O
instance	O
-	O
level	O
attention	O
.	O
These	O
methods	O
take	O
advantage	O
of	O
supervised	O
or	O
distantlysupervised	O
data	O
to	O
learn	O
neural	O
sentence	O
encoders	O
for	O
distributed	O
representations	O
,	O
and	O
have	O
achieved	O
promising	O
results	O
.	O
However	O
,	O
these	O
methods	O
can	O
not	O
handle	O
the	O
open	O
-	O
ended	O
growth	O
of	O
new	O
relation	O
types	O
in	O
the	O
open	O
-	O
domain	O
corpora	O
.	O
To	O
solve	O
this	O
problem	O
,	O
recently	O
many	O
efforts	O
have	O
been	O
invested	O
in	O
exploring	O
methods	O
for	O
open	O
relation	B-TaskName
extraction	I-TaskName
(	O
OpenRE	O
)	O
,	O
which	O
aims	O
to	O
discover	O
new	O
relation	O
types	O
from	O
unsupervised	O
open	O
-	O
domain	O
corpora	O
.	O
OpenRE	O
methods	O
can	O
be	O
roughly	O
divided	O
into	O
two	O
categories	O
:	O
taggingbased	O
and	O
clustering	O
-	O
based	O
.	O
Tagging	O
-	O
based	O
methods	O
cast	O
OpenRE	O
as	O
a	O
sequence	O
labeling	O
problem	O
,	O
and	O
extract	O
relational	O
phrases	O
consisting	O
of	O
words	O
from	O
sentences	O
in	O
unsupervised	O
(	O
Banko	O
et	O
al	O
,	O
2007	O
;	O
Banko	O
and	O
Etzioni	O
,	O
2008	O
)	O
or	O
supervised	O
paradigms	O
(	O
Jia	O
et	O
al	O
,	O
2018	O
;	O
Cui	O
et	O
al	O
,	O
2018	O
;	O
Stanovsky	O
et	O
al	O
,	O
2018	O
)	O
.	O
However	O
,	O
tagging	O
-	O
based	O
methods	O
often	O
extract	O
multiple	O
overly	O
-	O
specific	O
relational	O
phrases	O
for	O
the	O
same	O
relation	O
type	O
,	O
and	O
can	O
not	O
be	O
readily	O
utilized	O
for	O
downstream	O
tasks	O
.	O
In	O
comparison	O
,	O
conventional	O
clustering	O
-	O
based	O
OpenRE	O
methods	O
extract	O
rich	O
features	O
for	O
relation	O
instances	O
via	O
external	O
linguistic	O
tools	O
,	O
and	O
cluster	O
semantic	O
patterns	O
into	O
several	O
relation	O
types	O
(	O
Lin	O
and	O
Pantel	O
,	O
2001	O
;	O
Yao	O
et	O
al	O
,	O
2011Yao	O
et	O
al	O
,	O
,	O
2012	O
.	O
Marcheggiani	O
(	O
2016	O
)	O
proposes	O
a	O
reconstructionbased	O
model	O
discrete	O
-	O
state	O
variational	B-MethodName
autoencoder	I-MethodName
for	O
OpenRE	O
via	O
unlabeled	O
instances	O
.	O
Elsahar	O
(	O
2017	O
)	O
utilizes	O
a	O
clustering	O
algorithm	O
over	O
linguistic	O
features	O
.	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
the	O
clustering	O
-	O
based	O
OpenRE	O
methods	O
,	O
which	O
have	O
the	O
advantage	O
of	O
discovering	O
highly	O
distinguishable	O
relation	O
types	O
.	O
Few	B-TaskName
-	I-TaskName
shot	I-TaskName
Learning	I-TaskName
.	O
Few	B-TaskName
-	I-TaskName
shot	I-TaskName
learning	I-TaskName
aims	O
to	O
classify	O
instances	O
with	O
a	O
handful	O
of	O
labeled	O
samples	O
.	O
Many	O
efforts	O
are	O
devoted	O
to	O
few	B-TaskName
-	I-TaskName
shot	I-TaskName
image	I-TaskName
classification	I-TaskName
(	O
Koch	O
et	O
al	O
,	O
2015	O
)	O
and	O
relation	B-TaskName
classification	I-TaskName
(	O
Yuan	O
et	O
al	O
,	O
2017	O
;	O
Han	O
et	O
al	O
,	O
2018	O
)	O
.	O
Notably	O
,	O
(	O
Koch	O
et	O
al	O
,	O
2015	O
)	O
introduces	O
Convolu	O
-	O
tional	O
Siamese	O
Neural	O
Network	O
for	O
image	O
metric	B-TaskName
learning	I-TaskName
,	O
which	O
inspires	O
us	O
to	O
learn	O
relational	O
similarity	O
metrics	O
for	O
OpenRE	O
.	O
Semi	O
-	O
supervised	O
Clustering	O
.	O
Semi	O
-	O
supervised	O
clustering	O
aims	O
to	O
cluster	O
semantic	O
patterns	O
given	O
instance	O
seeds	B-DatasetName
of	O
target	O
categories	O
(	O
Bair	O
,	O
2013	O
;	O
Hongtao	O
Lin	O
,	O
2019	O
)	O
.	O
Differently	O
,	O
our	O
proposed	O
Semi	O
-	O
supervised	O
RSN	O
only	O
leverages	O
labeled	O
instances	O
of	O
pre	O
-	O
defined	O
relations	O
,	O
and	O
does	O
not	O
need	O
any	O
seed	O
of	O
new	O
relations	O
.	O

In	O
experiments	O
,	O
we	O
use	O
FewRel	B-DatasetName
(	O
Han	O
et	O
al	O
,	O
2018	O
)	O
as	O
our	O
first	O
dataset	O
.	O
FewRel	B-DatasetName
is	O
a	O
human	O
-	O
annotated	O
dataset	O
containing	O
80	O
types	O
of	O
relations	O
,	O
each	O
with	O
700	O
instances	O
.	O
An	O
advantage	O
of	O
FewRel	B-DatasetName
is	O
that	O
every	O
instance	O
contains	O
a	O
unique	O
entity	O
pair	O
,	O
so	O
RE	O
models	O
can	O
not	O
choose	O
the	O
easy	O
way	O
to	O
memorize	O
the	O
entities	O
.	O
We	O
use	O
the	O
original	O
train	O
set	O
of	O
FewRel	B-DatasetName
,	O
which	O
contains	O
64	O
relations	O
,	O
as	O
labeled	O
set	O
with	O
predefined	O
relations	O
,	O
and	O
the	O
original	O
validation	O
set	O
of	O
FewRel	B-DatasetName
,	O
which	O
contains	O
16	O
new	O
relations	O
,	O
as	O
the	O
unlabeled	O
set	O
with	O
novel	O
relations	O
to	O
extract	O
.	O
We	O
then	O
randomly	O
choose	O
1	O
,	O
600	O
instances	O
from	O
the	O
unlabeled	O
set	O
as	O
the	O
test	O
set	O
,	O
with	O
the	O
rest	O
labeled	O
and	O
unlabeled	O
instances	O
considered	O
as	O
the	O
train	O
set	O
.	O
The	O
second	O
dataset	O
we	O
use	O
is	O
FewRel	B-DatasetName
-	O
distant	O
,	O
which	O
contains	O
the	O
distantly	O
-	O
supervised	O
data	O
obtained	O
by	O
the	O
authors	O
of	O
FewRel	B-DatasetName
before	O
human	O
an	O
-	O
notation	O
.	O
We	O
follow	O
the	O
split	O
of	O
FewRel	B-DatasetName
to	O
obtain	O
the	O
auto	O
-	O
labeled	O
train	O
set	O
and	O
unlabeled	O
train	O
set	O
.	O
For	O
evaluation	O
,	O
we	O
use	O
the	O
human	O
-	O
annotated	O
test	O
set	O
of	O
FewRel	B-DatasetName
with	O
1	O
,	O
600	O
instances	O
.	O
Unlabeled	O
instances	O
already	O
existing	O
in	O
this	O
test	O
set	O
are	O
removed	O
from	O
the	O
unlabeled	O
train	O
set	O
of	O
FewRel	B-DatasetName
-	O
distant	O
.	O
Finally	O
,	O
the	O
auto	O
-	O
labeled	O
train	O
set	O
contains	O
323	O
,	O
549	O
relational	O
instances	O
,	O
and	O
the	O
unlabeled	O
train	O
set	O
contains	O
60	O
,	O
581	O
instances	O
.	O
A	O
previous	O
OpenRE	O
work	O
reports	O
performance	O
on	O
an	O
unpublic	O
dataset	O
called	O
NYT	O
-	O
FB	O
(	O
Marcheggiani	O
and	O
Titov	O
,	O
2016	O
)	O
.	O
However	O
,	O
it	O
has	O
several	O
shortcomings	O
compared	O
with	O
FewRel	B-DatasetName
-	O
distant	O
.	O
First	O
,	O
NTY	O
-	O
FB	O
's	O
test	O
set	O
is	O
distantly	O
-	O
supervised	O
and	O
is	O
noisy	O
for	O
instance	O
-	O
level	O
RE	O
.	O
Moreover	O
,	O
instances	O
in	O
NYT	O
-	O
FB	O
often	O
share	O
entity	O
pairs	O
or	O
relational	O
phrases	O
,	O
which	O
makes	O
it	O
much	O
easier	O
for	O
relation	O
clustering	O
.	O
Therefore	O
,	O
we	O
think	O
the	O
results	O
on	O
FewRel	B-DatasetName
-	O
distant	O
are	O
convincing	O
enough	O
for	O
Distantly	O
-	O
supervised	O
OpenRE	O
.	O

In	O
this	O
subsection	O
,	O
we	O
mainly	O
focus	O
on	O
analyzing	O
the	O
influence	O
of	O
pre	O
-	O
defined	O
relation	O
diversity	O
,	O
i.e.	O
,	O
the	O
number	O
of	O
relations	O
in	O
the	O
labeled	O
train	O
set	O
.	O
To	O
study	O
this	O
influence	O
,	O
we	O
use	O
FewRel	B-DatasetName
for	O
evaluation	O
and	O
change	O
the	O
number	O
of	O
relations	O
in	O
the	O
labeled	O
train	O
set	O
from	O
40	O
to	O
64	O
while	O
fixing	O
the	O
total	O
num	O
-	O
ber	O
of	O
labeled	O
instances	O
to	O
25	O
,	O
000	O
,	O
and	O
report	O
the	O
clustering	O
results	O
in	O
Figure	O
5	O
.	O
Several	O
conclusions	O
can	O
be	O
drawn	O
according	O
to	O
Figure	O
5	O
.	O
Firstly	O
,	O
a	O
rich	O
variety	O
of	O
labeled	O
relations	O
do	O
improve	O
the	O
performance	O
of	O
our	O
models	O
,	O
especially	O
RSN	O
.	O
The	O
models	O
trained	O
on	O
64	O
relations	O
perform	O
better	O
than	O
those	O
trained	O
on	O
40	O
relations	O
constantly	O
.	O
Secondly	O
,	O
while	O
the	O
performance	O
of	O
supervised	O
RSN	O
is	O
very	O
sensitive	O
to	O
pre	O
-	O
defined	O
relation	O
diversity	O
,	O
its	O
semi	O
-	O
supervised	O
counterparts	O
suffer	O
much	O
less	O
from	O
the	O
relation	O
number	O
limit	O
.	O
This	O
phenomenon	O
suggests	O
that	O
Semi	O
-	O
supervised	O
RSNs	O
succeed	O
in	O
learning	O
from	O
unlabeled	O
novelrelation	O
data	O
and	O
are	O
more	O
generalizable	O
to	O
novel	O
relations	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
model	O
Relational	O
Siamese	B-MethodName
Network	I-MethodName
(	O
RSN	O
)	O
for	O
OpenRE	O
.	O
Different	O
from	O
conventional	O
unsupervised	O
models	O
,	O
our	O
model	O
learns	O
to	O
measure	O
relational	O
similarity	O
from	O
supervised	O
/	O
distantly	O
-	O
supervised	O
data	O
of	O
predefined	O
relations	O
,	O
as	O
well	O
as	O
unsupervised	O
data	O
of	O
novel	O
relations	O
.	O
There	O
are	O
mainly	O
two	O
innovative	O
points	O
in	O
our	O
model	O
.	O
First	O
,	O
we	O
propose	O
to	O
transfer	O
relational	O
similarity	O
knowledge	O
with	O
RSN	O
structure	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
propose	O
knowledge	O
transfer	O
for	O
OpenRE	O
.	O
Second	O
,	O
we	O
propose	O
Semi	O
/	O
Distantly	O
-	O
supervised	O
RSN	O
,	O
to	O
further	O
perform	O
semi	O
-	O
supervised	O
and	O
distantlysupervised	O
transfer	B-TaskName
learning	I-TaskName
.	O
Experiments	O
show	O
that	O
our	O
models	O
significantly	O
surpass	O
conventional	O
OpenRE	O
models	O
and	O
achieve	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O
For	O
future	O
research	O
,	O
we	O
plan	O
to	O
explore	O
the	O
following	O
directions	O
:	O
(	O
1	O
)	O
Besides	O
CNN	O
,	O
there	O
are	O
some	O
other	O
popular	O
sentence	O
encoder	O
structures	O
like	O
piecewise	O
convolutional	O
neural	O
network	O
(	O
PCNN	O
)	O
and	O
Long	B-MethodName
Short	I-MethodName
-	I-MethodName
Term	I-MethodName
Memory	I-MethodName
(	O
LSTM	B-MethodName
)	O
for	O
RE	O
.	O
In	O
the	O
future	O
,	O
we	O
can	O
try	O
different	O
sentence	O
encoders	O
in	O
our	O
model	O
.	O
(	O
2	O
)	O
As	O
mentioned	O
above	O
,	O
our	O
model	O
has	O
the	O
potential	O
ability	O
to	O
discover	O
the	O
hierarchical	O
structure	O
of	O
relations	O
.	O
In	O
the	O
future	O
,	O
we	O
will	O
try	O
to	O
explore	O
this	O
application	O
with	O
additional	O
experiments	O
.	O

Pre	O
-	O
trained	O
language	O
models	O
(	O
PLMs	O
)	O
have	O
achieved	O
remarkable	O
success	O
on	O
various	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
tasks	O
.	O
Simple	O
fine	O
-	O
tuning	O
of	O
PLMs	O
,	O
on	O
the	O
other	O
hand	O
,	O
might	O
be	O
suboptimal	O
for	O
domain	O
-	O
specific	O
tasks	O
because	O
they	O
can	O
not	O
possibly	O
cover	O
knowledge	O
from	O
all	O
domains	O
.	O
While	O
adaptive	O
pre	O
-	O
training	O
of	O
PLMs	O
can	O
help	O
them	O
obtain	O
domain	O
-	O
specific	O
knowledge	O
,	O
it	O
requires	O
a	O
large	O
training	O
cost	O
.	O
Moreover	O
,	O
adaptive	O
pre	O
-	O
training	O
can	O
harm	O
the	O
PLM	O
's	O
performance	O
on	O
the	O
downstream	O
task	O
by	O
causing	O
catastrophic	O
forgetting	O
of	O
its	O
general	B-TaskName
knowledge	I-TaskName
.	O
To	O
overcome	O
such	O
limitations	O
of	O
adaptive	O
pre	O
-	O
training	O
for	O
PLM	O
adaption	O
,	O
we	O
propose	O
a	O
novel	O
domain	O
adaption	O
framework	O
for	O
PLMs	O
coined	O
as	O
Knowledge	O
-	O
Augmented	O
Language	O
model	O
Adaptation	O
(	O
KALA	O
)	O
,	O
which	O
modulates	O
the	O
intermediate	O
hidden	O
representations	O
of	O
PLMs	O
with	O
domain	O
knowledge	O
,	O
consisting	O
of	O
entities	O
and	O
their	O
relational	O
facts	O
.	O
We	O
validate	O
the	O
performance	O
of	O
our	O
KALA	O
on	O
question	B-TaskName
answering	I-TaskName
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
tasks	O
on	O
multiple	O
datasets	O
across	O
various	O
domains	O
.	O
The	O
results	O
show	O
that	O
,	O
despite	O
being	O
computationally	O
efficient	O
,	O
our	O
KALA	O
largely	O
outperforms	O
adaptive	O
pre	O
-	O
training	O
.	O
Code	O
is	O
available	O
at	O
:	O
https://github.com/Nardien/KALA	O
.	O

Pre	O
-	O
trained	O
Language	O
Models	O
(	O
PLMs	O
)	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Brown	O
et	O
al	O
,	O
2020	O
)	O
have	O
shown	O
to	O
be	O
effective	O
on	O
various	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
(	O
NLU	O
)	O
tasks	O
.	O
Although	O
PLMs	O
aim	O
to	O
address	O
diverse	O
downstream	O
tasks	O
from	O
various	O
data	O
sources	O
,	O
there	O
have	O
been	O
considerable	O
efforts	O
to	O
adapt	O
the	O
PLMs	O
to	O
specific	O
domains	O
-	O
distributions	O
over	O
the	O
language	O
characterizing	O
a	O
given	O
topic	O
or	O
genre	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
-	O
for	O
which	O
the	O
acquisition	O
of	O
domain	O
knowledge	O
is	O
required	O
to	O
accurately	O
solve	O
the	O
downstream	O
tasks	O
(	O
e.g.	O
,	O
Biomedical	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
Dogan	O
et	O
al	O
,	O
2014	O
)	O
)	O
.	O
This	O
problem	O
,	O
known	O
as	O
Language	O
Model	O
Adaptation	O
,	O
can	O
be	O
viewed	O
as	O
a	O
transfer	B-TaskName
learning	I-TaskName
problem	O
(	O
Yosinski	O
et	O
al	O
,	O
2014	O
;	O
Ruder	O
,	O
2019	O
)	O
under	O
domain	O
shift	O
,	O
where	O
the	O
model	O
is	O
pre	O
-	O
trained	O
on	O
the	O
general	O
domain	O
and	O
the	O
labeled	O
distribution	O
is	O
available	O
for	O
the	O
target	O
domain	O
-	O
specific	O
task	O
.	O
The	O
most	O
prevalent	O
approach	O
to	O
this	O
problem	O
is	O
adaptive	O
pre	O
-	O
training	O
(	O
Figure	O
2a	O
)	O
which	O
further	O
updates	O
all	O
parameters	O
of	O
the	O
PLM	O
on	O
a	O
large	O
domain	O
-	O
specific	O
or	O
curated	O
task	O
-	O
specific	O
corpus	O
,	O
with	O
the	O
same	O
pretraining	O
strategy	O
(	O
e.g.	O
,	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
)	O
before	O
fine	O
-	O
tuning	O
it	O
on	O
the	O
downstream	O
task	O
(	O
Beltagy	O
et	O
al	O
,	O
2019	O
;	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
.	O
This	O
continual	O
pre	O
-	O
training	O
of	O
a	O
PLM	O
on	O
the	O
target	O
domain	O
corpus	O
allows	O
it	O
to	O
learn	O
the	O
distribution	O
of	O
the	O
target	O
domain	O
,	O
resulting	O
in	O
improved	O
performance	O
on	O
domain	O
-	O
specific	O
tasks	O
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
;	O
Han	O
and	O
Eisenstein	O
,	O
2019	O
)	O
.	O
While	O
it	O
has	O
shown	O
to	O
be	O
effective	O
,	O
adaptive	O
pretraining	O
has	O
obvious	O
drawbacks	O
.	O
First	O
,	O
it	O
is	O
computationally	O
inefficient	O
.	O
Although	O
a	O
PLM	O
becomes	O
more	O
powerful	O
with	O
the	O
increasing	O
amount	O
of	O
pretraining	O
data	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
,	O
further	O
pre	O
-	O
training	O
on	O
the	O
additional	O
data	O
requires	O
larger	O
memory	O
and	O
computational	O
cost	O
as	O
the	O
dataset	O
size	O
grows	O
(	O
Bai	O
et	O
al	O
,	O
2021	O
)	O
.	O
Besides	O
,	O
it	O
is	O
difficult	O
to	O
adapt	O
the	O
PLM	O
to	O
a	O
new	O
domain	O
without	O
forgetting	O
the	O
general	B-TaskName
knowledge	I-TaskName
it	O
obtained	O
from	O
the	O
initial	O
pretraining	O
step	O
,	O
since	O
all	O
pre	O
-	O
trained	O
parameters	O
are	O
continually	O
updated	O
to	O
fit	O
the	O
domain	O
-	O
specific	O
corpus	O
during	O
adaptive	O
pre	O
-	O
training	O
.	O
This	O
catastrophic	O
forgetting	O
of	O
the	O
task	O
-	O
Our	O
KALA	O
framework	O
embeds	O
the	O
unseen	O
entities	O
on	O
the	O
embedding	O
space	O
of	O
seen	O
entities	O
by	O
representing	O
them	O
with	O
their	O
relational	O
knowledge	O
over	O
the	O
graph	O
,	O
while	O
the	O
strong	O
DAPT	O
baseline	O
(	O
Gururangan	O
et	O
al	O
,	O
2020	O
)	O
can	O
not	O
appropriately	O
handle	O
unseen	O
entities	O
that	O
are	O
not	O
given	O
for	O
task	O
fine	O
-	O
tuning	O
.	O
general	B-TaskName
knowledge	I-TaskName
may	O
lead	O
to	O
the	O
performance	O
degradation	O
on	O
the	O
downstream	O
tasks	O
.	O
In	O
Figure	O
1	O
,	O
we	O
show	O
that	O
adaptive	O
pre	O
-	O
training	O
with	O
more	O
training	O
steps	O
could	O
lead	O
to	O
performance	O
degeneration	O
.	O
Thus	O
,	O
it	O
would	O
be	O
preferable	O
if	O
we	O
could	O
adapt	O
the	O
PLM	O
to	O
the	O
domain	O
-	O
specific	O
task	O
without	O
costly	O
adaptive	O
pre	O
-	O
training	O
.	O
To	O
this	O
end	O
,	O
we	O
aim	O
to	O
integrate	O
the	O
domain	O
-	O
specific	O
knowledge	O
into	O
the	O
PLM	O
directly	O
during	O
the	O
task	O
-	O
specific	O
fine	O
-	O
tuning	O
step	O
,	O
as	O
shown	O
in	O
Figure	O
2b	O
,	O
eliminating	O
the	O
adaptive	O
pre	O
-	O
training	O
stage	O
.	O
Specifically	O
,	O
we	O
first	O
note	O
that	O
entities	O
and	O
relations	O
are	O
core	O
building	O
blocks	O
of	O
the	O
domain	O
-	O
specific	O
knowledge	O
that	O
are	O
required	O
to	O
solve	O
for	O
the	O
domain	O
-	O
specific	O
downstream	O
tasks	O
.	O
Clinical	O
domain	O
experts	O
,	O
for	O
example	O
,	O
are	O
familiar	O
with	O
medical	O
terminologies	O
and	O
their	O
complex	O
relations	O
.	O
Then	O
,	O
to	O
represent	O
the	O
domain	O
knowledge	O
consisting	O
of	O
entities	O
and	O
relations	O
,	O
we	O
introduce	O
the	O
Entity	O
Memory	O
,	O
which	O
is	O
the	O
source	O
of	O
entity	B-TaskName
embeddings	I-TaskName
but	O
independent	O
of	O
the	O
PLM	O
parameters	O
(	O
See	O
Entity	O
Memory	O
in	O
Figure	O
2b	O
)	O
.	O
Then	O
,	O
we	O
further	O
exploit	O
the	O
relational	O
structures	O
of	O
the	O
entities	O
by	O
utilizing	O
a	O
Knowledge	O
Graph	O
(	O
KG	O
)	O
,	O
which	O
denotes	O
the	O
factual	O
relationships	O
between	O
entities	O
,	O
as	O
shown	O
in	O
Knowledge	O
Graph	O
of	O
Figure	O
2b	O
.	O
The	O
remaining	O
step	O
is	O
how	O
to	O
integrate	O
the	O
knowledge	O
into	O
the	O
PLM	O
during	O
fine	O
-	O
tuning	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
a	O
novel	O
layer	O
named	O
Knowledgeconditioned	O
Feature	O
Modulation	O
(	O
KFM	O
,	O
3.2	O
)	O
,	O
which	O
scales	O
and	O
shifts	O
the	O
intermediate	O
hidden	O
representations	O
of	O
PLMs	O
by	O
conditioning	O
them	O
with	O
retrieved	O
knowledge	O
representations	O
.	O
This	O
knowledge	O
integration	O
scheme	O
has	O
several	O
advantages	O
.	O
First	O
,	O
it	O
does	O
not	O
modify	O
the	O
original	O
PLM	O
architecture	O
,	O
and	O
thus	O
could	O
be	O
integrated	O
into	O
any	O
PLMs	O
regardless	O
of	O
their	O
architectures	O
.	O
Also	O
,	O
it	O
only	O
re	O
-	O
quires	O
marginal	O
computational	O
and	O
memory	O
overhead	O
,	O
while	O
eliminating	O
the	O
need	O
of	O
excessive	O
further	O
pre	O
-	O
training	O
(	O
Figure	O
1	O
)	O
.	O
Finally	O
,	O
it	O
can	O
effectively	O
handle	O
unseen	O
entities	O
with	O
relational	O
knowledge	O
from	O
the	O
KG	O
,	O
which	O
are	O
suboptimally	O
embedded	O
by	O
adaptive	O
pre	O
-	O
training	O
.	O
For	O
example	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
,	O
an	O
entity	O
restenosis	O
does	O
not	O
appear	O
in	O
the	O
training	O
dataset	O
for	O
fine	O
-	O
tuning	O
,	O
thus	O
adaptive	O
pre	O
-	O
training	O
only	O
implicitly	O
infers	O
them	O
within	O
the	O
context	O
from	O
the	O
broad	O
domain	O
corpus	O
.	O
However	O
,	O
we	O
can	O
explicitly	O
represent	O
the	O
unknown	O
entity	O
by	O
aggregating	O
the	O
representations	O
of	O
known	O
entities	O
in	O
the	O
entity	O
memory	O
(	O
i.e.	O
,	O
in	O
Figure	O
2	O
,	O
neighboring	O
entities	O
,	O
such	O
as	O
asthma	O
and	O
pethidine	O
,	O
are	O
used	O
to	O
represent	O
the	O
unseen	O
entity	O
restenosis	O
)	O
.	O
We	O
combine	O
all	O
the	O
previously	O
described	O
components	O
into	O
a	O
novel	O
language	O
model	O
adaptation	O
framework	O
,	O
coined	O
as	O
Knowledge	O
-	O
Augmented	O
Language	O
model	O
Adaptation	O
(	O
KALA	O
)	O
(	O
Figure	O
3	O
)	O
.	O
We	O
empirically	O
verify	O
that	O
KALA	O
improves	O
the	O
performance	O
of	O
the	O
PLM	O
over	O
adaptive	O
pre	O
-	O
training	O
on	O
various	O
domains	O
with	O
two	O
knowledge	O
-	O
intensive	O
tasks	O
:	O
Question	B-TaskName
Answering	I-TaskName
(	O
QA	O
)	O
and	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(	O
NER	B-TaskName
)	O
.	O
Our	O
contribution	O
is	O
threefold	O
:	O
We	O
propose	O
a	O
novel	O
LM	O
adaptation	O
framework	O
,	O
which	O
augments	O
PLMs	O
with	O
entities	O
and	O
their	O
relations	O
from	O
the	O
target	O
domain	O
,	O
during	O
fine	O
-	O
tuning	O
without	O
any	O
further	O
pre	O
-	O
training	O
.	O
To	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
that	O
utilizes	O
the	O
structured	O
knowledge	O
for	O
language	O
model	O
adaptation	O
.	O
To	O
reflect	O
structural	O
knowledge	O
into	O
the	O
PLM	O
,	O
we	O
introduce	O
a	O
novel	O
layer	O
which	O
scales	O
and	O
shifts	O
the	O
intermediate	O
PLM	O
representations	O
with	O
the	O
entity	O
representations	O
contextualized	O
by	O
their	O
related	O
entities	O
according	O
to	O
the	O
KG	O
.	O
We	O
show	O
that	O
our	O
KALA	O
significantly	O
enhances	O
the	O
model	O
's	O
performance	O
on	O
domain	O
-	O
specific	O
QA	O
and	O
NER	B-TaskName
tasks	O
,	O
while	O
being	O
significantly	O
more	O
efficient	O
over	O
existing	O
LM	O
adaptation	O
methods	O
.	O

Language	O
Model	O
Adaptation	O
Nowadays	O
,	O
transfer	B-TaskName
learning	I-TaskName
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
is	O
a	O
dominant	O
approach	O
for	O
solving	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
(	O
NLU	O
)	O
tasks	O
.	O
This	O
strategy	O
first	O
pretrains	O
a	O
Language	O
Model	O
(	O
LM	O
)	O
on	O
a	O
large	O
and	O
unlabeled	O
corpus	O
,	O
then	O
fine	O
-	O
tunes	O
it	O
on	O
downstream	O
tasks	O
with	O
labeled	O
data	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
While	O
this	O
scheme	O
alone	O
achieves	O
impressive	O
performance	O
on	O
various	O
NLU	O
tasks	O
,	O
adaptive	O
pre	O
-	O
training	O
of	O
the	O
PLM	O
on	O
a	O
domain	O
-	O
specific	O
corpus	O
helps	O
the	O
PLM	O
achieve	O
better	O
performance	O
on	O
the	O
domain	O
-	O
specific	O
tasks	O
.	O
For	O
example	O
,	O
demonstrated	O
that	O
a	O
further	O
pre	O
-	O
trained	O
LM	O
on	O
biomedical	O
documents	O
outperforms	O
the	O
original	O
LM	O
on	O
biomedical	O
NLU	O
tasks	O
.	O
Also	O
,	O
Gururangan	O
et	O
al	O
(	O
2020	O
)	O
showed	O
that	O
adaptive	O
pre	O
-	O
training	O
of	O
the	O
PLM	O
on	O
the	O
corpus	O
of	O
a	O
target	O
domain	O
(	O
Domain	O
-	O
adaptive	O
Pre	O
-	O
training	O
;	O
DAPT	O
)	O
or	O
a	O
target	O
task	O
(	O
Task	O
-	O
adaptive	O
Pre	O
-	O
training	O
;	O
TAPT	O
)	O
improves	O
its	O
performance	O
on	O
domain	O
-	O
specific	O
tasks	O
.	O
However	O
,	O
above	O
approaches	O
generally	O
require	O
a	O
large	O
amount	O
of	O
computational	O
costs	O
for	O
pre	O
-	O
training	O
.	O
Knowledge	O
-	O
aware	O
LM	O
Accompanied	O
with	O
increasing	O
sources	O
of	O
knowledge	O
(	O
Vrandecic	O
and	O
Krötzsch	O
,	O
2014	O
)	O
,	O
some	O
prior	O
works	O
have	O
proposed	O
to	O
integrate	O
external	O
knowledge	O
into	O
PLMs	O
,	O
to	O
enhance	O
their	O
performance	O
on	O
tasks	O
that	O
require	O
structured	O
knowledge	O
.	O
For	O
instance	O
,	O
ERNIE	O
(	O
Zhang	O
et	O
al	O
,	O
2019	O
)	O
and	O
KnowBERT	O
(	O
Peters	O
et	O
al	O
,	O
2019	O
)	O
incorporate	O
entities	O
as	O
additional	O
inputs	O
in	O
the	O
pretraining	O
stage	O
to	O
obtain	O
a	O
knowledge	O
-	O
aware	O
LM	O
,	O
wherein	O
a	O
pre	O
-	O
trained	O
knowledge	B-TaskName
graph	I-TaskName
embedding	I-TaskName
from	O
Wikidata	O
(	O
Vrandecic	O
and	O
Krötzsch	O
,	O
2014	O
)	O
is	O
used	O
to	O
represent	O
entities	O
.	O
Entity	O
-	O
as	O
-	O
Experts	O
(	O
Févry	O
et	O
al	O
,	O
2020	O
)	O
and	O
LUKE	O
(	O
Yamada	O
et	O
al	O
,	O
2020	O
)	O
use	O
the	O
entity	O
memory	O
that	O
is	O
pre	O
-	O
trained	O
along	O
with	O
the	O
LMs	O
from	O
scratch	O
.	O
ERICA	O
(	O
Qin	O
et	O
al	O
,	O
2021	O
)	O
further	O
uses	O
the	O
fact	O
consisting	O
of	O
entities	O
and	O
their	O
relations	O
in	O
the	O
pre	O
-	O
training	O
stage	O
of	O
LMs	O
from	O
scratch	O
.	O
Previous	O
works	O
aim	O
to	O
integrate	O
external	O
knowledge	O
into	O
the	O
LMs	O
during	O
the	O
pre	O
-	O
training	O
step	O
to	O
obtain	O
a	O
universal	O
knowledge	O
-	O
aware	O
LM	O
that	O
requires	O
additional	O
parameters	O
for	O
millions	O
of	O
entities	O
.	O
In	O
contrast	O
to	O
this	O
,	O
our	O
framework	O
aims	O
to	O
efficiently	O
modify	O
a	O
general	O
PLM	O
for	O
the	O
domain	O
-	O
specific	O
task	O
with	O
a	O
linear	O
modulation	O
layer	O
scheme	O
discussed	O
in	O
Section	O
3.2	O
,	O
during	O
fine	O
-	O
tuning	O
.	O

Our	O
goal	O
is	O
to	O
solve	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
(	O
NLU	O
)	O
tasks	O
for	O
a	O
specific	O
domain	O
,	O
with	O
a	O
knowledge	O
-	O
augmented	O
Language	O
Model	O
(	O
LM	O
)	O
.	O
We	O
first	O
introduce	O
the	O
NLU	O
tasks	O
we	O
target	O
,	O
followed	O
by	O
the	O
descriptions	O
of	O
the	O
proposed	O
knowledgeaugmented	O
LM	O
.	O
After	O
that	O
,	O
we	O
formally	O
define	O
the	O
ingredients	O
for	O
structured	O
knowledge	O
integration	O
.	O

We	O
use	O
the	O
uncased	O
BERT	B-MethodName
-	O
base	O
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
as	O
the	O
base	O
PLM	O
for	O
all	O
our	O
experiments	O
on	O
QA	O
and	O
NER	B-TaskName
tasks	O
.	O
For	O
more	O
details	O
on	O
training	O
and	O
implementation	O
,	O
please	O
see	O
the	O
Appendix	O
B.	O

Although	O
we	O
believe	O
our	O
experimental	O
results	O
on	O
more	O
params	B-MetricName
on	O
NewsQA	B-DatasetName
)	O
.	O
Thus	O
,	O
we	O
believe	O
that	O
our	O
KALA	O
would	O
be	O
useful	O
to	O
any	O
PLMs	O
,	O
not	O
depending	O
on	O
specific	O
PLMs	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduced	O
KALA	O
,	O
a	O
novel	O
framework	O
for	O
language	O
model	O
adaptation	O
,	O
which	O
modulates	O
the	O
intermediate	O
representations	O
of	O
a	O
PLM	O
by	O
conditioning	O
it	O
with	O
the	O
entity	O
memory	O
and	O
the	O
relational	O
facts	O
from	O
KGs	O
.	O
We	O
validated	O
KALA	O
on	O
various	O
domains	O
of	O
QA	O
and	O
NER	B-TaskName
tasks	O
,	O
on	O
which	O
KALA	O
significantly	O
outperforms	O
relevant	O
baselines	O
while	O
being	O
computationally	O
efficient	O
.	O
We	O
demonstrate	O
that	O
the	O
success	O
of	O
KALA	O
comes	O
from	O
both	O
KFM	O
and	O
relational	O
retrieval	O
,	O
allowing	O
the	O
PLM	O
to	O
recognize	O
entities	O
but	O
also	O
handle	O
unseen	O
ones	O
that	O
might	O
frequently	O
appear	O
in	O
domain	O
-	O
specific	O
tasks	O
.	O
There	O
are	O
many	O
other	O
avenues	O
for	O
future	O
work	O
,	O
including	O
the	O
application	O
of	O
KALA	O
on	O
pre	O
-	O
training	O
of	O
knowledge	O
-	O
augmented	O
PLMs	O
from	O
scratch	O
.	O

Enhancing	O
the	O
domain	O
converge	O
of	O
pre	O
-	O
traind	O
language	O
models	O
(	O
PLMs	O
)	O
with	O
external	O
knowledge	O
is	O
increasingly	O
important	O
,	O
since	O
the	O
PLMs	O
can	O
not	O
observe	O
all	O
the	O
data	O
during	O
training	O
and	O
can	O
not	O
memorize	O
all	O
the	O
necessary	O
knowledge	O
for	O
solving	O
down	O
-	O
stream	O
tasks	O
.	O
Our	O
KALA	O
contributes	O
to	O
this	O
problem	O
by	O
augmenting	O
domain	O
knowledge	B-TaskName
graphs	I-TaskName
for	O
PLMs	O
.	O
However	O
,	O
we	O
have	O
to	O
still	O
consider	O
the	O
accurateness	O
of	O
knowledge	O
,	O
i.e.	O
,	O
the	O
fact	O
in	O
the	O
knowledge	O
graph	O
may	O
not	O
be	O
correct	O
,	O
which	O
affects	O
the	O
model	O
to	O
generate	O
incorrect	O
answers	O
.	O
Also	O
,	O
the	O
model	O
's	O
prediction	O
performance	O
is	O
still	O
far	O
from	O
optimal	O
.	O
Thus	O
,	O
we	O
should	O
be	O
aware	O
of	O
model	O
's	O
failure	O
from	O
errors	O
in	O
knowledge	O
and	O
prediction	O
,	O
especially	O
on	O
high	O
-	O
risk	O
domains	O
(	O
e.g.	O
,	O
biomedicine	O
)	O
.	O
Fine	O
-	O
tuned	O
model	O
"	O
text	O
"	O
:	O
"	O
Arvane	O
Rezai	O
"	O
,	O
"	O
start	O
"	O
:	O
30	O
,	O
"	O
end	O
"	O
:	O
43	O
,	O
"	O
i	O
d	O
"	O
:	O
228998	O
"	O
h	O
"	O
:	O
11578	O
,	O
"	O
r	O
"	O
:	O
"	O
P3373	O
"	O
,	O
"	O
t	O
"	O
:	O
228998	O

Here	O
we	O
describe	O
the	O
dataset	O
details	O
with	O
its	O
statistics	O
for	O
two	O
different	O
tasks	O
:	O
extractive	O
question	B-TaskName
answering	I-TaskName
(	O
QA	O
)	O
and	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
.	O
Question	B-TaskName
Answering	I-TaskName
We	O
evaluate	O
models	O
on	O
three	O
domain	O
-	O
specific	O
datasets	O
:	O
NewsQA	B-DatasetName
,	O
Relation	O
,	O
and	O
Medication	O
.	O
Notably	O
,	O
NewsQA	B-DatasetName
(	O
Trischler	O
et	O
al	O
,	O
2017	O
)	O
is	O
curated	O
from	O
CNN	O
news	O
articles	O
.	O
Relation	O
and	O
Medication	O
are	O
originally	O
part	O
of	O
the	O
emrQA	B-DatasetName
(	O
Pampari	O
et	O
al	O
,	O
2018	O
)	O
,	O
which	O
is	O
an	O
automatically	O
constructed	O
question	B-TaskName
answering	I-TaskName
dataset	O
based	O
on	O
the	O
electrical	O
medical	O
record	O
from	O
n2c2	O
challenges	O
9	O
.	O
However	O
,	O
Yue	O
et	O
al	O
(	O
2020	O
)	O
extract	O
two	O
major	O
subsets	O
by	O
dividing	O
the	O
entire	O
dataset	O
into	O
Relation	O
and	O
Medication	O
and	O
suggest	O
the	O
usage	O
of	O
sampled	O
questions	O
from	O
the	O
original	O
em	O
-	O
rQA	O
dataset	O
.	O
Following	O
the	O
suggestion	O
of	O
Yue	O
et	O
al	O
(	O
2020	O
)	O
,	O
we	O
use	O
only	O
1	O
%	O
of	O
generated	O
questions	O
of	O
Relation	O
for	O
training	O
,	O
validation	O
,	O
and	O
testing	O
.	O
Also	O
,	O
we	O
only	O
use	O
1	O
%	O
of	O
generated	O
questions	O
of	O
Medication	O
for	O
training	O
and	O
use	O
5	O
%	O
of	O
generated	O
questions	O
of	O
Medication	O
for	O
validation	O
and	O
testing	O
.	O
Since	O
the	O
original	O
emrQA	B-DatasetName
is	O
automatically	O
generated	O
based	O
on	O
templates	O
,	O
the	O
quality	O
is	O
poor	O
-	O
it	O
means	O
that	O
the	O
original	O
emrQA	B-DatasetName
dataset	O
was	O
inappropriate	O
to	O
evaluate	O
the	O
ability	O
of	O
the	O
model	O
to	O
reason	O
over	O
the	O
clinical	O
text	O
since	O
the	O
most	O
of	O
questions	O
can	O
be	O
answered	O
by	O
the	O
simple	O
text	B-TaskName
matching	I-TaskName
.	O
To	O
overcome	O
this	O
limitation	O
,	O
Yue	O
et	O
al	O
(	O
2020	O
)	O
suggests	O
two	O
ways	O
to	O
make	O
the	O
task	O
more	O
difficult	O
.	O
First	O
,	O
they	O
divide	O
the	O
question	O
templates	O
into	O
easy	O
and	O
hard	O
versions	O
and	O
then	O
use	O
the	O
hard	O
question	O
only	O
.	O
Second	O
,	O
they	O
suggest	O
replacing	O
medical	O
terminologies	O
in	O
the	O
question	O
of	O
the	O
test	O
set	O
into	O
synonyms	O
to	O
avoid	O
the	O
trivial	O
question	O
which	O
can	O
be	O
solvable	O
with	O
a	O
simple	O
text	B-TaskName
matching	I-TaskName
.	O
We	O
use	O
both	O
methods	O
to	O
Relation	O
and	O
Medication	O
datasets	O
to	O
report	O
the	O
performance	O
of	O
every	O
model	O
.	O
For	O
more	O
details	O
on	O
Relation	O
and	O
Medication	O
datasets	O
,	O
please	O
refer	O
to	O
the	O
original	O
paper	O
(	O
Yue	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
statistics	O
of	O
training	O
,	O
validation	O
,	O
and	O
test	O
sets	O
on	O
all	O
QA	O
datasets	O
are	O
provided	O
in	O
Table	O
9	O
.	O
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
We	O
use	O
three	O
different	O
domain	O
-	O
specific	O
datasets	O
for	O
evaluating	O
our	O
KALA	O
on	O
NER	B-TaskName
tasks	O
:	O
CoNLL	O
-	O
2003	O
(	O
Sang	O
andMeulder	O
,	O
2003	O
)	O
(	O
News	O
)	O
,	O
WNUT	O
-	O
17	O
(	O
Derczynski	B-DatasetName
et	O
al	O
,	O
2017	O
)	O

In	O
this	O
subsection	O
,	O
we	O
give	O
detailed	O
descriptions	O
of	O
how	O
the	O
FLOPs	O
in	O
Figure	O
1	O
are	O
measured	O
.	O
We	O
majorly	O
follow	O
the	O
script	O
from	O
the	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
,	O
2020	O
)	O
repository	O
to	O
compute	O
the	O
approximated	O
FLOPs	O
for	O
all	O
models	O
including	O
ours	O
.	O
For	O
FLOPs	O
computation	O
of	O
our	O
KALA	O
,	O
we	O
additionally	O
include	O
the	O
FLOPs	O
of	O
the	O
entity	O
embedding	O
layer	O
,	O
linear	O
layers	O
for	O
h	O
1	O
,	O
h	O
2	O
,	O
h	O
3	O
,	O
h	O
4	O
,	O
and	O
GNN	O
layer	O
.	O
Since	O
the	O
GNN	O
layer	O
is	O
implemented	O
based	O
on	O
the	O
sparse	O
implementation	O
,	O
we	O
first	O
calculate	O
the	O
FLOPs	O
of	O
the	O
message	O
propagation	O
over	O
one	O
edge	O
,	O
and	O
then	O
multiply	O
it	O
to	O
the	O
average	O
number	O
of	O
edges	O
per	O
node	O
.	O
Also	O
,	O
in	O
terms	O
of	O
the	O
computation	O
on	O
mentions	O
,	O
we	O
consider	O
the	O
maximum	O
sequence	O
length	O
of	O
the	O
context	O
rather	O
than	O
the	O
average	O
number	O
of	O
mentions	O
,	O
to	O
set	O
the	O
upper	O
bound	O
of	O
FLOPs	O
for	O
our	O
KALA	O
.	O
Note	O
that	O
,	O
in	O
NewsQA	B-DatasetName
training	O
data	O
,	O
the	O
average	O
number	O
of	O
nodes	O
is	O
57	O
,	O
the	O
average	O
number	O
of	O
edges	O
for	O
each	O
node	O
is	O
0.64	O
,	O
and	O
the	O
average	O
number	O
of	O
mentions	O
in	O
the	O
context	O
is	O
92.68	O
.	O

In	O
this	O
subsection	O
,	O
we	O
analyze	O
how	O
the	O
size	O
of	O
entity	O
memory	O
affects	O
the	O
performance	O
of	O
our	O
KALA	O
.	O
In	O
Figure	O
8	O
,	O
we	O
plot	O
the	O
performance	O
of	O
KALA	O
on	O
the	O
NewsQA	B-DatasetName
dataset	O
by	O
varying	O
the	O
number	O
of	O
entity	O
elements	O
in	O
the	O
memory	O
.	O
Note	O
that	O
,	O
we	O
reduce	O
the	O
size	O
of	O
the	O
entity	O
memory	O
by	O
eliminating	O
the	O
entity	O
appearing	O
fewer	O
times	O
.	O
Thus	O
,	O
the	O
results	O
are	O
obtained	O
by	O
only	O
considering	O
the	O
entities	O
that	O
appear	O
more	O
than	O
[	O
1000	O
,	O
100	O
,	O
10	O
,	O
5	O
,	O
0	B-DatasetName
]	O
times	O
,	O
e.g.	O
,	O
0	B-DatasetName
means	O
the	O
model	O
with	O
full	O
entity	O
memory	O
.	O
As	O
shown	O
in	O
Figure	O
8	O
,	O
we	O
observe	O
that	O
the	O
size	O
of	O
the	O
entity	O
memory	O
is	O
larger	O
,	O
the	O
performance	O
of	O
our	O
KALA	O
is	O
better	O
in	O
general	O
.	O
However	O
,	O
interestingly	O
,	O
we	O
also	O
observe	O
that	O
the	O
smallest	O
size	O
of	O
the	O
entity	O
memory	O
shows	O
decent	O
performance	O
,	O
which	O
might	O
be	O
due	O
to	O
the	O
fact	O
that	O
some	O
parameters	O
in	O
the	O
entity	O
memory	O
are	O
stale	O
.	O
For	O
more	O
discussions	O
on	O
it	O
including	O
visualization	O
,	O
please	O
refer	O
to	O
Appendix	O
D.2	O
.	O
Finally	O
,	O
we	O
would	O
like	O
to	O
note	O
that	O
,	O
in	O
Figure	O
1	O
,	O
we	O
report	O
the	O
performance	O
of	O
our	O
KALA	O
in	O
the	O
case	O
of	O
[	O
1000	O
,	O
5	O
,	O
0	B-DatasetName
]	O
(	O
i.e.	O
,	O
considering	O
entities	O
appearing	O
more	O
than	O
[	O
1000	O
,	O
5	O
,	O
0	B-DatasetName
]	O
times	O
)	O
.	O

In	O
this	O
subsection	O
,	O
we	O
aim	O
to	O
analyze	O
which	O
numbers	O
of	O
entities	O
and	O
facts	O
per	O
context	O
are	O
appropriate	O
to	O
achieve	O
good	O
performance	O
in	O
NER	B-TaskName
tasks	O
.	O
Specifically	O
,	O
we	O
first	O
collect	O
the	O
contexts	O
having	O
more	O
than	O
or	O
equal	O
to	O
the	O
k	O
number	O
of	O
entities	O
(	O
or	O
facts	O
)	O
,	O
and	O
then	O
calculate	O
the	O
performance	O
difference	O
from	O
our	O
KALA	O
to	O
the	O
fine	O
-	O
tuning	O
baseline	O
.	O
As	O
shown	O
in	O
Figure	O
9	O
,	O
while	O
there	O
are	O
no	O
obvious	O
patterns	O
,	O
performance	O
improvements	O
from	O
the	O
baseline	O
are	O
consistent	O
across	O
a	O
varying	O
number	O
of	O
entities	O
and	O
facts	O
.	O
This	O
result	O
suggests	O
that	O
our	O
KALA	O
is	O
indeed	O
beneficial	O
when	O
entities	O
and	O
facts	O
are	O
given	O
to	O
the	O
model	O
,	O
whereas	O
the	O
appropriate	O
number	O
of	O
entities	O
and	O
facts	O
to	O
obtain	O
the	O
best	O
performance	O
against	O
the	O
baseline	O
is	O
different	O
across	O
datasets	O
.	O

In	O
the	O
main	O
paper	O
and	O
Appendix	O
B.1	O
,	O
we	O
describe	O
that	O
the	O
location	O
of	O
the	O
KFM	O
layer	O
inside	O
the	O
PLM	O
architecture	O
is	O
the	O
hyperparameter	O
.	O
However	O
,	O
someone	O
might	O
wonder	O
which	O
location	O
of	O
KFM	O
yields	O
the	O
best	O
performance	O
,	O
and	O
what	O
is	O
the	O
reason	O
for	O
this	O
.	O
Therefore	O
,	O
in	O
this	O
section	O
,	O
we	O
analyze	O
where	O
we	O
obtain	O
the	O
best	O
performance	O
in	O
various	O
locations	O
of	O
the	O
KFM	O
layer	O
on	O
the	O
NewsQA	B-DatasetName
dataset	O
.	O
Specifically	O
,	O
in	O
Figure	O
10	O
,	O
we	O
show	O
the	O
performance	O
of	O
our	O
KALA	O
with	O
varying	O
the	O
location	O
of	O
the	O
KFM	O
layer	O
insider	O
the	O
BERT	B-MethodName
-	O
base	O
model	O
.	O
The	O
results	O
demonstrate	O
that	O
the	O
model	O
with	O
the	O
KFM	O
on	O
the	O
last	O
layer	O
of	O
the	O
BERT	B-MethodName
-	O
base	O
outperforms	O
all	O
the	O
other	O
choices	O
.	O
This	O
might	O
be	O
because	O
,	O
as	O
the	O
final	O
layer	O
of	O
the	O
PLM	O
is	O
generally	O
considered	O
as	O
the	O
most	O
task	O
-	O
specific	O
layer	O
,	O
our	O
KFM	O
interleaved	O
in	O
the	O
latest	O
layer	O
of	O
BERT	B-MethodName
expressively	O
injects	O
the	O
task	O
-	O
specific	O
information	O
from	O
the	O
entity	O
memory	O
and	O
KGs	O
,	O
to	O
such	O
a	O
task	O
-	O
specific	O
layer	O
.	O

While	O
we	O
already	O
show	O
the	O
contextualized	O
representations	O
of	O
seen	O
and	O
unseen	O
entities	O
in	O
the	O
latent	O
5163	O
space	O
in	O
Figure	O
2	O
right	O
,	O
we	O
further	O
visualize	O
them	O
including	O
the	O
missing	O
baselines	O
of	O
Figure	O
2	O
,	O
such	O
as	O
Fine	O
-	O
tuning	O
or	O
TAPT	O
,	O
in	O
Figure	O
12	O
on	O
the	O
NCBI	B-DatasetName
-	I-DatasetName
Disease	I-DatasetName
dataset	O
.	O
Similar	O
to	O
Figure	O
2	O
,	O
we	O
observe	O
that	O
all	O
baselines	O
fail	O
to	O
closely	O
embed	O
the	O
unseen	O
entities	O
in	O
the	O
representation	O
space	O
of	O
seen	O
entities	O
.	O
While	O
this	O
visualization	O
result	O
does	O
not	O
give	O
a	O
strong	O
evidence	O
of	O
why	O
our	O
KALA	O
outperforms	O
other	O
baselines	O
,	O
we	O
clearly	O
observe	O
that	O
KALA	O
is	O
beneficial	O
to	O
represent	O
unseen	O
entities	O
in	O
the	O
feature	O
space	O
of	O
seen	O
entities	O
,	O
which	O
suggests	O
that	O
such	O
an	O
advantage	O
of	O
our	O
KALA	O
helps	O
the	O
PLM	O
to	O
generalize	O
over	O
the	O
test	O
dataset	O
,	O
where	O
the	O
context	O
contains	O
unseen	O
entities	O
.	O

We	O
visualize	O
the	O
frequency	O
of	O
entities	O
in	O
Figure	O
13	O
and	O
14	O
.	O
The	O
entity	O
frequency	O
denotes	O
the	O
number	O
of	O
mentions	O
of	O
their	O
associated	O
entities	O
within	O
the	O
entire	O
text	O
corpus	O
of	O
the	O
training	O
dataset	O
.	O
As	O
shown	O
in	O
Figure	O
13	O
and	O
14	O
of	O
QA	O
and	O
NER	B-TaskName
datasets	O
,	O
the	O
entity	O
frequency	O
follows	O
the	O
long	O
-	O
tail	O
distribution	O
,	O
where	O
most	O
entities	O
appear	O
a	O
few	O
times	O
.	O
For	O
instance	O
,	O
in	O
the	O
NewsQA	B-DatasetName
dataset	O
,	O
more	O
than	O
20k	O
entities	O
among	O
entire	O
60k	O
entities	O
appear	O
only	O
once	O
in	O
the	O
training	O
dataset	O
,	O
whereas	O
one	O
entity	O
(	O
CNN	O
10	O
)	O
appears	O
approximately	O
20k	O
times	O
.	O
This	O
observation	O
suggests	O
that	O
most	O
of	O
the	O
elements	O
in	O
the	O
entity	O
memory	O
are	O
not	O
utilized	O
frequently	O
.	O
In	O
other	O
words	O
,	O
only	O
few	O
entities	O
are	O
accurately	O
trained	O
with	O
many	O
training	O
instances	O
,	O
whereas	O
there	O
exists	O
the	O
stale	O
embeddings	O
which	O
are	O
rarely	O
updated	O
.	O
This	O
observation	O
raises	O
an	O
interesting	O
research	O
question	O
on	O
the	O
efficient	O
usage	O
of	O
the	O
entity	O
memory	O
,	O
as	O
we	O
can	O
see	O
in	O
Figure	O
8	O
that	O
the	O
small	O
size	O
of	O
entity	O
memory	O
could	O
result	O
in	O
the	O
better	O
performance	O
(	O
See	O
Appendix	O
C.2	O
)	O
.	O
We	O
leave	O
the	O
more	O
in	O
-	O
depth	O
analysis	O
on	O
the	O
entity	O
memory	O
as	O
the	O
future	O
work	O
.	O
where	O
almost	O
all	O
entities	O
appear	O
less	O
than	O
10	O
times	O
,	O
while	O
an	O
extremely	O
few	O
numbers	O
of	O
entities	O
appear	O
very	O
frequently	O
.	O

In	O
addition	O
to	O
the	O
case	O
study	O
in	O
Figure	O
5	O
,	O
we	O
further	O
show	O
the	O
case	O
on	O
the	O
question	B-TaskName
answering	I-TaskName
task	O
in	O
Figure	O
15	O
,	O
like	O
in	O
Section	O
5.5	O
,	O
With	O
this	O
example	O
,	O
we	O
explain	O
how	O
the	O
factual	O
knowledge	O
in	O
KGs	O
could	O
be	O
utilized	O
to	O
solve	O
the	O
task	O
via	O
our	O
KALA	O
.	O
The	O
question	O
in	O
the	O
example	O
is	O
"	O
who	O
was	O
kidnapped	O
because	O
of	O
her	O
neighbor	O
"	O
.	O
We	O
observe	O
that	O
DAPT	O
answers	O
this	O
question	O
as	O
Araceli	O
Valencia	O
.	O
This	O
prediction	O
may	O
come	O
from	O
matching	O
the	O
word	O
'	O
her	O
'	O
in	O
the	O
question	O
to	O
the	O
feminine	O
name	O
'	O
Araceli	O
Valencia	O
'	O
in	O
the	O
context	O
.	O
In	O
contrast	O
,	O
our	O
KALA	O
predicts	O
the	O
Jaime	O
Andrade	O
as	O
an	O
answer	O
,	O
which	O
is	O
the	O
ground	O
truth	O
.	O
We	O
suspect	O
that	O
this	O
might	O
be	O
because	O
of	O
the	O
fact	O
"	O
(	O
Jaime	O
Andrade	O
,	O
spouse	O
,	O
Valencia	O
)	O
"	O
in	O
the	O
knowledge	O
graph	O
,	O
which	O
relates	O
the	O
'	O
Valencia	O
'	O
to	O
the	O
'	O
Jaime	O
Andrade	O
'	O
.	O
Although	O
it	O
is	O
not	O
clear	O
how	O
it	O
directly	O
affects	O
the	O
model	O
's	O
performance	O
,	O
we	O
can	O
reason	O
that	O
KALA	O
can	O
successfully	O
answer	O
the	O
question	O
by	O
utilizing	O
the	O
existing	O
facts	O
.	O

ZYJ123@DravidianLangTech	O
-	O
EACL2021	O
:	O
Offensive	O
Language	B-TaskName
Identification	I-TaskName
based	O
on	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
with	O
DPCNN	O

With	O
the	O
development	O
of	O
the	O
information	O
society	O
,	O
people	O
have	O
become	O
accustomed	O
to	O
uploading	O
content	O
on	O
social	O
media	O
platforms	O
in	O
the	O
form	O
of	O
text	O
,	O
pictures	O
,	O
or	O
videos	O
.	O
At	O
the	O
same	O
time	O
,	O
they	O
also	O
comment	O
on	O
the	O
content	O
uploaded	O
by	O
other	O
users	O
and	O
interact	O
with	O
each	O
other	O
,	O
thus	O
increasing	O
the	O
activity	O
of	O
social	O
media	O
platforms	O
Mahesan	O
,	O
2019	O
,	O
2020a	O
,	O
b	O
)	O
.	O
Inevitably	O
,	O
however	O
,	O
some	O
users	O
will	O
post	O
offensive	O
posts	O
or	O
comments	O
.	O
The	O
use	O
of	O
offensive	O
discourse	O
is	O
a	O
kind	O
of	O
impolite	O
phenomenon	O
which	O
has	O
negative	O
effects	O
on	O
the	O
civilization	O
of	O
the	O
network	O
community	O
(	O
Chakravarthi	O
,	O
2020	O
)	O
.	O
It	O
usually	O
has	O
the	O
characteristics	O
of	O
causing	O
conflicts	O
and	O
the	O
purpose	O
of	O
publishing	O
intentionally	O
.	O
The	O
publisher	O
of	O
offensive	O
language	O
may	O
use	O
reproach	O
,	O
sarcasm	O
,	O
swear	O
and	O
other	O
language	O
means	O
to	O
achieve	O
intentional	O
offense	O
,	O
and	O
express	O
a	O
variety	O
of	O
intentions	O
,	O
such	O
as	O
disturbing	O
,	O
provoking	O
,	O
and	O
expressing	O
negative	O
emotions	O
(	O
Chakravarthi	O
and	O
Muralidaran	O
,	O
2021	O
;	O
Suryawanshi	O
and	O
Chakravarthi	O
,	O
2021	O
)	O
.	O
Most	O
people	O
will	O
take	O
measures	O
to	O
respond	O
to	O
offensive	O
words	O
.	O
The	O
way	O
to	O
respond	O
to	O
the	O
direct	O
conflict	O
of	O
offensive	O
words	O
is	O
mainly	O
rhetorical	O
questions	O
,	O
swear	O
,	O
sarcasm	O
and	O
threat	O
,	O
so	O
as	O
to	O
express	O
dissatisfaction	O
,	O
deny	O
and	O
satirize	O
the	O
other	O
party	O
and	O
provoke	O
the	O
other	O
party	O
.	O
This	O
will	O
further	O
cause	O
conflicts	O
and	O
destroy	O
the	O
harmony	O
of	O
the	O
network	O
environment	O
.	O
Many	O
social	O
media	O
platforms	O
use	O
a	O
content	O
review	O
process	O
,	O
in	O
which	O
human	O
reviewers	O
check	O
users	O
'	O
comments	O
for	O
offensive	O
language	O
and	O
other	O
infractions	O
,	O
and	O
which	O
comments	O
have	O
been	O
removed	O
from	O
the	O
platform	O
because	O
of	O
the	O
violation	O
(	O
Mandl	O
et	O
al	O
,	O
2020	O
)	O
.	O
It	O
is	O
up	O
to	O
the	O
moderator	O
to	O
decide	O
which	O
comments	O
will	O
be	O
removed	O
from	O
the	O
platform	O
due	O
to	O
violations	O
and	O
which	O
ones	O
will	O
be	O
kept	O
.	O
As	O
the	O
number	O
of	O
network	O
users	O
increases	O
and	O
user	O
activity	O
increases	O
,	O
the	O
manual	O
approach	O
is	O
undoubtedly	O
inefficient	O
.	O
Therefore	O
,	O
the	O
automatic	O
detection	O
and	O
identification	O
of	O
offensive	O
content	O
are	O
very	O
necessary	O
.	O
However	O
,	O
offensive	O
words	O
often	O
depend	O
on	O
the	O
emotions	O
and	O
psychology	O
of	O
the	O
listener	O
,	O
and	O
some	O
seemingly	O
innocuous	O
words	O
can	O
be	O
potentially	O
offensive	O
,	O
and	O
words	O
that	O
often	O
seem	O
offensive	O
are	O
watered	O
down	O
by	O
the	O
emotions	O
of	O
the	O
listener	O
.	O
This	O
kind	O
of	O
language	O
phenomenon	O
is	O
not	O
uncommon	O
in	O
real	O
life	O
,	O
either	O
unintentionally	O
or	O
deliberately	O
used	O
to	O
achieve	O
the	O
speaker	O
's	O
expected	O
purpose	O
,	O
which	O
is	O
a	O
challenging	O
work	O
for	O
the	O
current	O
detection	O
system	O
.	O
Our	O
team	O
takes	O
part	O
in	O
the	O
shared	O
task	O
of	O
Offensive	O
Language	B-TaskName
Identification	I-TaskName
in	O
Dravidian	O
Languages	O
-	O
EACL	O
2021	O
(	O
Chakravarthi	O
et	O
al	O
,	O
,	O
2020aHande	O
et	O
al	O
,	O
2020	O
)	O
.	O
This	O
is	O
a	O
classification	O
task	O
at	O
the	O
comment	O
/	O
post	O
level	O
.	O
The	O
goal	O
of	O
this	O
task	O
is	O
to	O
identify	O
offensive	O
language	O
content	O
of	O
the	O
code	O
-	O
mixed	O
dataset	O
of	O
comments	O
/	O
posts	O
in	O
Dravidian	O
Languages	O
(	O
(	O
Tamil	O
-	O
English	O
,	O
Malayalam	O
-	O
English	O
,	O
and	O
Kannada	O
-	O
English	O
)	O
)	O
collected	O
from	O
social	O
media	O
.	O
Tamil	O
language	O
is	O
the	O
oldest	O
language	O
in	O
Indian	O
languages	O
,	O
Malayalam	O
and	O
Kannada	O
evolved	O
from	O
Tamil	O
language	O
.	O
For	O
a	O
comment	O
on	O
Youtube	O
,	O
the	O
system	O
must	O
classify	O
it	O
into	O
not	O
-	O
offensive	O
,	O
offensive	O
-	O
untargeted	O
,	O
offensive	O
-	O
targeted	O
-	O
individual	O
,	O
offensive	O
-	O
targeted	O
-	O
group	O
,	O
offensive	O
-	O
targeted	O
-	O
other	O
,	O
or	O
not	O
-	O
in	O
-	O
indented	O
-	O
language	O
.	O
In	O
our	O
approach	O
,	O
the	O
multilingual	O
model	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
and	O
DPCNN	O
are	O
combined	O
to	O
carry	O
out	O
the	O
classification	O
task	O
.	O
This	O
method	O
can	O
combine	O
the	O
advantages	O
of	O
the	O
two	O
models	O
to	O
achieve	O
a	O
better	O
classification	O
effect	O
.	O
The	O
rest	O
of	O
the	O
paper	O
is	O
divided	O
into	O
the	O
following	O
parts	O
.	O
In	O
the	O
second	O
part	O
,	O
we	O
introduce	O
the	O
relevant	O
work	O
in	O
this	O
field	O
,	O
which	O
involves	O
offensive	O
language	O
detection	O
and	O
text	B-TaskName
classification	I-TaskName
methods	O
.	O
In	O
the	O
third	O
part	O
,	O
we	O
introduce	O
the	O
model	O
structure	O
and	O
the	O
composition	O
of	O
our	O
training	O
data	O
.	O
The	O
fourth	O
part	O
introduces	O
our	O
experimental	O
setup	O
and	O
results	O
.	O
The	O
fifth	O
part	O
is	O
the	O
conclusion	O
.	O

Compared	O
with	O
the	O
original	O
BERT	B-MethodName
model	O
,	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
increases	O
the	O
number	O
of	O
languages	O
and	O
the	O
number	O
of	O
training	O
data	O
sets	O
.	O
Specifically	O
,	O
a	O
preprocessed	O
CommonCrawl	O
dataset	O
of	O
more	O
than	O
2	O
TB	O
based	O
on	O
100	O
languages	O
is	O
used	O
to	O
train	O
crosslanguage	O
representations	O
in	O
a	O
self	O
-	O
supervised	O
manner	O
.	O
This	O
includes	O
generating	O
new	O
unlabeled	O
corpora	O
for	O
low	O
-	O
resource	O
languages	O
and	O
expanding	O
the	O
amount	O
of	O
training	O
data	O
available	O
for	O
these	O
languages	O
by	O
two	O
orders	O
of	O
magnitude	O
.	O
In	O
the	O
finetuning	O
period	O
,	O
the	O
multi	O
-	O
language	O
tagging	O
data	O
is	O
used	O
based	O
on	O
the	O
ability	O
of	O
the	O
multi	O
-	O
language	O
model	O
to	O
improve	O
the	O
performance	O
of	O
the	O
downstream	O
tasks	O
.	O
This	O
enables	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
to	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
cross	O
-	O
language	O
benchmarks	O
while	O
exceeding	O
the	O
performance	O
of	O
the	O
single	O
-	O
language	O
BERT	B-MethodName
model	O
for	O
each	O
language	O
.	O
Tune	O
the	O
parameters	O
of	O
the	O
model	O
to	O
address	O
cases	O
where	O
extending	O
the	O
model	O
to	O
more	O
languages	O
using	O
cross	O
-	O
language	O
migration	O
limits	O
the	O
ability	O
of	O
the	O
model	O
to	O
understand	O
each	O
language	O
.	O
The	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
parameter	O
changes	O
include	O
up	O
-	O
sampling	O
of	O
low	O
-	O
resource	O
languages	O
during	O
training	O
and	O
vocabulary	O
building	O
,	O
generating	O
a	O
larger	O
shared	O
vocabulary	O
,	O
and	O
increasing	O
the	O
overall	O
model	O
to	O
550	O
million	O
parameters	O
.	O

In	O
this	O
task	O
,	O
we	O
combined	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
with	O
DPCNN	O
(	O
Johnson	O
and	O
Zhang	O
,	O
2017	O
)	O
to	O
make	O
the	O
whole	O
model	O
more	O
suitable	O
for	O
the	O
downstream	O
classification	O
task	O
.	O
DPCNN	O
(	O
Deep	O
Pyramid	O
Convolutional	O
Neural	O
Networks	O
)	O
is	O
a	O
kind	O
of	O
deep	O
word	O
level	O
CNN	O
structure	O
,	O
the	O
calculation	O
amount	O
of	O
each	O
layer	O
of	O
the	O
structure	O
decreases	O
exponentially	O
.	O
DPCNN	O
simply	O
stacks	O
the	O
convolution	B-MethodName
module	O
and	O
negative	O
sampling	O
layer	O
.	O
The	O
computation	O
volume	O
of	O
the	O
whole	O
model	O
is	O
limited	O
to	O
less	O
than	O
two	O
times	O
the	O
number	O
of	O
convolution	B-MethodName
blocks	O
.	O
At	O
the	O
same	O
time	O
,	O
the	O
pyramid	O
structure	O
also	O
enables	O
the	O
model	O
to	O
discover	O
long	O
-	O
term	O
dependencies	O
in	O
the	O
text	O
.	O
In	O
a	O
common	O
classification	O
task	O
,	O
the	O
last	O
hidden	O
state	O
of	O
the	O
first	O
token	O
of	O
the	O
sequence	O
(	O
CLS	O
token	O
)	O
,	O
namely	O
the	O
original	O
output	O
of	O
XLM	B-MethodName
-	O
Roberta	O
(	O
Pooler	O
output	O
)	O
,	O
is	O
further	O
processed	O
through	O
the	O
linear	B-MethodName
layer	I-MethodName
and	O
the	O
tanh	B-MethodName
activation	I-MethodName
function	O
for	O
classification	O
purposes	O
.	O
To	O
obtain	O
richer	O
semantic	O
information	O
features	O
of	O
the	O
model	O
and	O
improve	O
the	O
performance	O
of	O
the	O
model	O
,	O
we	O
first	O
processed	O
the	O
output	O
of	O
the	O
last	O
three	O
layers	O
of	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
through	O
DPCNN	O
,	O
and	O
then	O
concatenate	O
it	O
with	O
the	O
original	O
output	O
of	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
(	O
Pooler	O
output	O
)	O
to	O
get	O
a	O
new	O
and	O
more	O
effective	O
feature	O
vector	O
,	O
and	O
then	O
input	O
this	O
feature	O
vector	O
into	O
the	O
classifier	O
for	O
classification	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
our	O
system	O
in	O
the	O
task	O
of	O
offensive	O
language	B-TaskName
identification	I-TaskName
for	O
Tamil	O
,	O
Malayalam	O
,	O
and	O
Kannada	O
language	O
.	O
In	O
this	O
model	O
,	O
the	O
XLM	B-MethodName
-	O
RoBERTa	B-MethodName
pre	O
-	O
training	O
model	O
is	O
used	O
to	O
extract	O
semantic	O
information	O
features	O
of	O
the	O
text	O
,	O
and	O
DPCNN	O
is	O
used	O
to	O
further	O
process	O
the	O
output	O
features	O
.	O
At	O
the	O
same	O
time	O
,	O
the	O
hierarchical	O
crossvalidation	O
method	O
is	O
used	O
to	O
improve	O
the	O
training	O
effect	O
.	O
The	O
final	O
results	O
show	O
that	O
our	O
model	O
achieves	O
satisfactory	O
performance	O
.	O
In	O
future	O
work	O
,	O
we	O
will	O
try	O
to	O
adjust	O
the	O
structure	O
of	O
the	O
new	O
model	O
,	O
so	O
as	O
to	O
improve	O
its	O
effect	O
more	O
significantly	O
.	O

When	O
pre	O
-	O
trained	O
contextualized	O
embeddingbased	O
models	O
developed	O
for	O
unstructured	O
data	O
are	O
adapted	O
for	O
structured	O
tabular	O
data	O
,	O
they	O
perform	O
admirably	O
.	O
However	O
,	O
recent	O
probing	O
studies	O
show	O
that	O
these	O
models	O
use	O
spurious	O
correlations	O
,	O
and	O
often	O
predict	O
inference	O
labels	O
by	O
focusing	O
on	O
false	O
evidence	O
or	O
ignoring	O
it	O
altogether	O
.	O
To	O
study	O
this	O
issue	O
,	O
we	O
introduce	O
the	O
task	O
of	O
Trustworthy	O
Tabular	O
Reasoning	O
,	O
where	O
a	O
model	O
needs	O
to	O
extract	O
evidence	O
to	O
be	O
used	O
for	O
reasoning	O
,	O
in	O
addition	O
to	O
predicting	O
the	O
label	O
.	O
As	O
a	O
case	O
study	O
,	O
we	O
propose	O
a	O
twostage	O
sequential	O
prediction	O
approach	O
,	O
which	O
includes	O
an	O
evidence	O
extraction	O
and	O
an	O
inference	O
stage	O
.	O
First	O
,	O
we	O
crowdsource	O
evidence	O
row	O
labels	O
and	O
develop	O
several	O
unsupervised	O
and	O
supervised	O
evidence	O
extraction	O
strategies	O
for	O
INFOTABS	B-DatasetName
,	O
a	O
tabular	O
NLI	O
benchmark	O
.	O
Our	O
evidence	O
extraction	O
strategy	O
outperforms	O
earlier	O
baselines	O
.	O
On	O
the	O
downstream	O
tabular	O
inference	O
task	O
,	O
using	O
only	O
the	O
automatically	O
extracted	O
evidence	O
as	O
the	O
premise	O
,	O
our	O
approach	O
outperforms	O
prior	O
benchmarks	O
.	O

Reasoning	O
on	O
tabular	O
or	O
semi	O
-	O
structured	O
knowledge	O
is	O
a	O
fundamental	O
challenge	O
for	O
today	O
's	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
systems	O
.	O
Two	O
recently	O
created	O
tabular	O
Natural	B-TaskName
language	I-TaskName
Inference	I-TaskName
(	O
NLI	O
)	O
datasets	O
,	O
TabFact	B-DatasetName
(	O
Chen	O
et	O
al	O
,	O
2020b	O
)	O
on	O
Wikipedia	O
relational	O
tables	O
and	O
INFOTABS	B-DatasetName
on	O
Wikipedia	O
Infoboxes	O
help	O
study	O
the	O
question	O
of	O
inferential	O
reasoning	O
over	O
semi	O
-	O
structured	O
tables	O
.	O
Today	O
's	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
NLI	O
over	O
unstructured	O
text	O
uses	O
contextualized	O
embeddings	O
(	O
e.g.	O
,	O
Devlin	O
et	O
al	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
,	O
2019b	O
)	O
.	O
When	O
adapted	O
for	O
tabular	O
NLI	O
by	O
flattening	O
tables	O
into	O
synthetic	O
sentences	O
using	O
heuristics	O
,	O
these	O
models	O
achieve	O
remarkable	O
performance	O
on	O
the	O
datasets	O
.	O
However	O
,	O
a	O
recent	O
study	O
demonstrates	O
that	O
these	O
models	O
fail	O
to	O
reason	O
prop	O
-	O
*	O
Work	O
done	O
during	O
an	O
internship	O
at	O
Bloomberg	O
Peter	O
Henderson	O
,	O
Supertramp	O
1	O
H1	O
H1	O
:	O
Supertramp	O
produced	O
1	O
an	O
album	O
that	O
was	O
less	O
than	O
an	O
hour	O
long	O
2	O
.	O
H2	O
:	O
Most	O
of	O
Breakfast	B-DatasetName
in	O
America	O
was	O
recorded	O
3	O
in	O
the	O
last	O
month	O
of	O
1978	O
3	O
.	O
H3	O
:	O
Breakfast	B-DatasetName
in	O
America	O
was	O
released	O
4	O
the	O
same	O
month	O
recording	O
ended	O
4	O
.	O
Figure	O
1	O
:	O
A	O
semi	O
-	O
structured	O
premise	O
(	O
the	O
table	O
'	O
Breakfast	B-DatasetName
in	O
America	O
'	O
)	O
example	O
from	O
.	O
Hypotheses	O
H1	O
are	O
entailed	O
by	O
it	O
,	O
H2	O
is	O
neither	O
entailed	O
nor	O
contradictory	O
,	O
and	O
H3	O
is	O
a	O
contradiction	O
.	O
The	O
Relevant	O
column	O
shows	O
the	O
hypotheses	O
that	O
use	O
the	O
corresponding	O
row	O
.	O
The	O
colored	O
text	O
(	O
and	O
superscripts	O
)	O
in	O
the	O
table	O
and	O
hypothesis	O
highlights	O
relevance	O
token	O
level	O
alignment	O
.	O
erly	O
on	O
the	O
semi	O
-	O
structured	O
inputs	O
in	O
many	O
cases	O
.	O
For	O
example	O
,	O
they	O
can	O
ignore	O
relevant	O
rows	O
,	O
and	O
(	O
a	O
)	O
focus	O
on	O
the	O
irrelevant	O
rows	O
(	O
Neeraja	O
et	O
al	O
,	O
2021	O
)	O
,	O
(	O
b	O
)	O
use	O
only	O
the	O
hypothesis	O
sentence	O
(	O
Poliak	O
et	O
al	O
,	O
2018	O
;	O
Gururangan	O
et	O
al	O
,	O
2018	O
)	O
,	O
or	O
(	O
c	O
)	O
knowledge	O
acquired	O
during	O
pre	O
-	O
training	O
(	O
Jain	O
et	O
al	O
,	O
2021	O
;	O
.	O
In	O
essence	O
,	O
they	O
use	O
spurious	O
correlations	O
between	O
irrelevant	O
rows	O
,	O
the	O
hypothesis	O
,	O
and	O
the	O
inference	O
label	O
to	O
predict	O
labels	O
.	O
This	O
paper	O
argues	O
that	O
existing	O
NLI	O
systems	O
optimized	O
solely	O
for	O
label	O
prediction	O
can	O
not	O
be	O
trusted	O
.	O
It	O
is	O
not	O
sufficient	O
for	O
a	O
model	O
to	O
be	O
merely	O
Right	O
but	O
also	O
Right	O
for	O
the	O
Right	O
Reasons	O
.	O
In	O
particular	O
,	O
at	O
least	O
identifying	O
the	O
relevant	O
elements	O
of	O
inputs	O
as	O
the	O
'	O
Right	O
Reasons	O
'	O
is	O
essential	O
for	O
trustworthy	O
reasoning	O
1	O
.	O
We	O
address	O
this	O
issue	O
by	O
introducing	O
the	O
task	O
of	O
Trustworthy	O
Tabular	O
Inference	O
,	O
where	O
the	O
goal	O
is	O
to	O
extract	O
relevant	O
rows	O
as	O
evidence	O
and	O
predict	O
inference	O
labels	O
.	O
To	O
illustrate	O
this	O
task	O
,	O
consider	O
an	O
example	O
from	O
the	O
INFOTABS	B-DatasetName
dataset	O
in	O
Figure	O
1	O
,	O
which	O
shows	O
a	O
premise	O
table	O
and	O
three	O
hypotheses	O
.	O
The	O
figure	O
also	O
marks	O
the	O
rows	O
needed	O
to	O
make	O
decisions	O
about	O
each	O
hypothesis	O
,	O
and	O
also	O
indicates	O
the	O
relevant	O
tokens	O
for	O
each	O
hypothesis	O
.	O
For	O
trustworthy	O
tabular	O
reasoning	O
,	O
in	O
addition	O
to	O
predicting	O
the	O
label	O
ENTAIL	O
for	O
H1	O
,	O
CONTRADICT	O
for	O
H2	O
and	O
NEU	O
-	O
TRAL	O
for	O
H3	O
,	O
the	O
model	O
should	O
also	O
identify	O
the	O
evidence	O
rows	O
-	O
namely	O
,	O
the	O
rows	O
Producer	O
and	O
Length	O
for	O
hypothesis	O
H1	O
,	O
Recorded	O
for	O
hypothesis	O
H2	O
,	O
Released	O
and	O
Recorded	O
for	O
hypothesis	O
H3	O
.	O
As	O
a	O
first	O
step	O
,	O
we	O
propose	O
a	O
two	O
-	O
stage	O
sequential	O
prediction	O
approach	O
for	O
the	O
task	O
,	O
comprising	O
of	O
an	O
evidence	O
extraction	O
stage	O
,	O
followed	O
by	O
an	O
inference	O
stage	O
.	O
In	O
the	O
evidence	O
extraction	O
stage	O
,	O
the	O
model	O
extracts	O
the	O
necessary	O
information	O
needed	O
for	O
the	O
second	O
stage	O
.	O
In	O
the	O
inference	O
stage	O
,	O
the	O
NLI	O
model	O
uses	O
only	O
the	O
extracted	O
evidence	O
as	O
the	O
premise	O
for	O
the	O
label	O
prediction	O
task	O
.	O
We	O
explore	O
several	O
unsupervised	O
evidence	O
extraction	O
approaches	O
for	O
INFOTABS	B-DatasetName
.	O
Our	O
best	O
unsupervised	O
evidence	O
extraction	O
method	O
outperforms	O
a	O
previously	O
developed	O
baseline	O
by	O
4.3	O
%	O
,	O
2.5	O
%	O
and	O
5.4	O
%	O
absolute	O
score	O
on	O
the	O
three	O
test	O
sets	O
.	O
For	O
supervised	O
evidence	O
extraction	O
,	O
we	O
annotate	O
the	O
IN	O
-	O
FOTABS	O
training	O
set	O
(	O
17	O
K	O
table	O
-	O
hypothesis	O
pairs	O
with	O
1740	O
unique	O
tables	O
)	O
with	O
relevant	O
rows	O
following	O
the	O
methodology	O
of	O
,	O
and	O
then	O
train	O
a	O
RoBERTa	B-MethodName
LARGE	O
classifier	O
.	O
The	O
supervised	O
model	O
improves	O
the	O
evidence	O
extraction	O
performance	O
by	O
8.7	O
%	O
,	O
10.8	O
%	O
,	O
and	O
4.2	O
%	O
absolute	O
scores	O
on	O
the	O
three	O
test	O
sets	O
over	O
the	O
unsupervised	O
approach	O
.	O
Finally	O
,	O
for	O
the	O
full	O
inference	O
task	O
,	O
we	O
demonstrate	O
that	O
our	O
two	O
-	O
stage	O
approach	O
with	O
best	O
extraction	O
,	O
outperforms	O
the	O
earlier	O
baseline	O
by	O
1.6	O
%	O
,	O
3.8	O
%	O
,	O
and	O
4.2	O
%	O
on	O
the	O
three	O
test	O
sets	O
.	O
In	O
summary	O
,	O
our	O
contributions	O
are	O
as	O
follows	O
2	O
:	O
We	O
introduce	O
the	O
problem	O
of	O
trustworthy	O
tabular	O
reasoning	O
and	O
study	O
a	O
two	O
-	O
stage	O
prediction	O
approach	O
that	O
first	O
extracts	O
evidence	O
and	O
then	O
predicts	O
the	O
NLI	O
label	O
.	O
We	O
investigate	O
a	O
variety	O
of	O
unsupervised	O
evidence	O
extraction	O
techniques	O
.	O
Our	O
unsupervised	O
approach	O
for	O
evidence	O
extraction	O
outperforms	O
the	O
previous	O
methods	O
.	O
We	O
enrich	O
the	O
INFOTABS	B-DatasetName
training	O
set	O
with	O
evidence	O
rows	O
,	O
and	O
develop	O
a	O
supervised	O
extractor	O
that	O
has	O
near	O
-	O
human	O
performance	O
.	O
We	O
demonstrate	O
that	O
our	O
two	O
-	O
stage	O
technique	O
with	O
best	O
extraction	O
outperforms	O
all	O
the	O
prior	O
benchmarks	O
on	O
the	O
downstream	O
NLI	O
task	O
.	O

This	O
section	O
describes	O
the	O
process	O
of	O
using	O
Amazon	O
MTurk	O
to	O
annotate	O
evidence	O
rows	O
for	O
the	O
16	O
,	O
538	O
premise	O
-	O
hypothesis	O
pairs	O
that	O
make	O
the	O
training	O
set	O
of	O
INFOTABS	B-DatasetName
.	O
We	O
followed	O
the	O
protocol	O
of	O
:	O
one	O
table	O
and	O
three	O
distinct	O
hypotheses	O
formed	O
a	O
HIT	O
.	O
For	O
each	O
of	O
the	O
hypotheses	O
,	O
five	O
annotators	O
would	O
select	O
the	O
evidence	O
rows	O
.	O
We	O
divide	O
the	O
tasks	O
equally	O
into	O
110	O
batches	O
,	O
each	O
batch	O
having	O
51	O
HITs	O
each	O
having	O
three	O
examples	O
.	O
To	O
reduce	O
bias	O
induced	O
by	O
a	O
link	O
between	O
the	O
NLI	O
label	O
and	O
row	O
selection	O
,	O
we	O
do	O
not	O
reveal	O
the	O
labels	O
to	O
the	O
annotators	O
.	O
The	O
quality	O
control	O
details	O
are	O
provided	O
in	O
the	O
Appendix	O
B.	O
In	O
total	O
,	O
we	O
collected	O
81	O
,	O
282	O
annotations	O
from	O
3	O
As	O
per	O
,	O
33	O
%	O
of	O
examples	O
in	O
INFOTABS	B-DatasetName
involve	O
multiple	O
rows	O
.	O
The	O
dataset	O
covers	O
all	O
the	O
reasoning	O
types	O
present	O
in	O
the	O
Glue	O
and	O
SuperGlue	B-DatasetName
Choice	O
of	O
Semi	O
-	O
structured	O
Data	O
.	O
The	O
rows	O
of	O
an	O
Infobox	O
table	O
are	O
semantically	O
distinct	O
,	O
though	O
all	O
connected	O
to	O
the	O
title	O
entity	O
.	O
Each	O
row	O
can	O
be	O
considered	O
a	O
separate	O
and	O
uniquely	O
distinct	O
source	O
of	O
information	O
about	O
the	O
title	O
entity	O
.	O
Because	O
of	O
this	O
property	O
,	O
the	O
problem	O
of	O
evidence	O
extraction	O
is	O
well	O
-	O
formed	O
as	O
relevant	O
row	O
selection	O
.	O
The	O
same	O
is	O
not	O
valid	O
for	O
unstructured	O
text	O
,	O
whose	O
units	O
of	O
information	O
may	O
be	O
tokens	O
,	O
phrases	O
,	O
sentences	O
or	O
entire	O
paragraphs	O
,	O
and	O
is	O
typically	O
unavailable	O
(	O
Ribeiro	O
et	O
al	O
,	O
2020	O
;	O
Goel	O
et	O
al	O
,	O
2021	O
;	O
Mishra	O
et	O
al	O
,	O
2021	O
;	O
Yin	O
et	O
al	O
,	O
2021	O
)	O
.	O

Inspired	B-DatasetName
by	O
the	O
Distracting	O
Row	O
Removal	O
(	O
DRR	O
)	O
heuristic	O
of	O
Neeraja	O
et	O
al	O
(	O
2021	O
)	O
,	O
we	O
propose	O
DRR	O
(	O
Re	O
-	O
Rank	O
+	O
Top	O
-	O
S	O
τ	O
)	O
,	O
which	O
uses	O
fastText	B-MethodName
(	O
Joulin	O
et	O
al	O
,	O
2016	O
;	O
Mikolov	O
et	O
al	O
,	O
2018	O
)	O
based	O
static	O
embeddings	O
to	O
measure	O
sentence	O
similarity	O
.	O
We	O
employ	O
three	O
modifications	O
to	O
improve	O
DRR	O
.	O

We	O
observed	O
that	O
the	O
raw	O
similarity	O
scores	O
(	O
i.e.	O
,	O
using	O
only	O
fastText	B-MethodName
)	O
for	O
some	O
valid	O
evidence	O
rows	O
could	O
be	O
low	O
,	O
despite	O
exact	O
wordlevel	O
lexical	O
matching	O
with	O
the	O
row	O
's	O
key	O
and	O
values	O
.	O
We	O
augmented	O
the	O
scores	O
by	O
δ	B-HyperparameterName
for	O
each	O
exact	B-MetricName
match	I-MetricName
to	O
incentivize	O
precise	O
matches	O
.	O

This	O
approach	O
consists	O
of	O
two	O
parts	O
(	O
a	O
)	O
aligning	O
rows	O
and	O
hypothesis	O
words	O
,	O
and	O
(	O
b	O
)	O
then	O
computing	O
cosine	O
similarity	O
between	O
the	O
aligned	O
words	O
.	O
Specifically	O
,	O
we	O
use	O
the	O
SimAlign	O
(	O
Jalili	O
Sabet	O
et	O
al	O
,	O
2020	O
)	O
method	O
for	O
word	O
-	O
level	O
alignment	O
.	O
SimAlign	O
uses	O
static	O
and	O
contextualized	O
embeddings	O
without	O
parallel	O
training	O
data	O
to	O
get	O
word	O
alignments	O
.	O
Among	O
the	O
approaches	O
explored	O
by	O
SimAlign	O
,	O
we	O
use	O
the	O
Match	O
(	O
mwmf	O
)	O
method	O
,	O
which	O
uses	O
maximum	O
-	O
weight	O
maximal	O
matching	O
in	O
the	O
bipartite	O
weighted	O
network	O
formed	O
by	O
the	O
word	O
level	O
similarity	O
matrix	O
.	O
Our	O
choice	O
of	O
this	O
approach	O
over	O
the	O
other	O
greedy	O
methods	O
(	O
Itermax	O
and	O
Argmax	O
)	O
is	O
motivated	O
by	O
the	O
fact	O
that	O
it	O
finds	O
the	O
global	O
optimum	O
matching	O
,	O
while	O
the	O
other	O
two	O
do	O
not	O
.	O
After	O
alignment	O
,	O
we	O
normalize	O
the	O
sum	O
of	O
cosine	O
similarities	O
of	O
RoBERTa	B-MethodName
LARGE	O
token	O
embeddings	O
8	O
to	O
derive	O
the	O
relevance	O
score	O
.	O
Furthermore	O
,	O
because	O
all	O
rows	O
use	O
the	O
same	O
title	O
,	O
we	O
assign	O
title	O
matching	O
terms	O
zero	O
weight	O
.	O
This	O
paper	O
refers	O
to	O
this	O
method	O
as	O
SimAlign	O
(	O
Match	O
(	O
mwmf	O
)	O
)	O
.	O

The	O
approach	O
we	O
saw	O
in	O
$	O
4.1.2	O
defines	O
rowhypothesis	O
similarity	O
using	O
word	O
alignments	O
.	O
As	O
an	O
alternative	O
,	O
we	O
can	O
directly	O
compute	O
similarities	O
between	O
the	O
contextualised	O
sentence	B-TaskName
embeddings	I-TaskName
of	O
rows	O
and	O
the	O
hypothesis	O
.	O
We	O
explore	O
two	O
options	O
here	O
.	O
Sentence	O
Transformer	B-MethodName
:	O
We	O
use	O
Sentence	O
-	O
BERT	B-MethodName
(	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
and	O
its	O
variants	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2020	O
;	O
Thakur	O
et	O
al	O
,	O
2021	O
;	O
Wang	O
et	O
al	O
,	O
2021a	O
)	O
,	O
which	O
use	O
Siamese	O
neural	O
networks	O
(	O
Koch	O
et	O
al	O
,	O
2015	O
;	O
Chicco	O
,	O
2021	O
)	O
.	O
We	O
explore	O
several	O
pre	O
-	O
trained	O
sentence	O
transformers	O
models	O
9	O
for	O
sentence	O
representation	O
.	O
These	O
models	O
differ	O
in	O
(	O
a	O
)	O
the	O
data	O
used	O
for	O
pre	O
-	O
training	O
,	O
(	O
b	O
)	O
the	O
main	O
model	O
type	O
and	O
it	O
size	O
,	O
and	O
(	O
c	O
)	O
the	O
maximum	O
sequence	O
length	O
.	O
SimCSE	B-MethodName
:	O
SimCSE	B-MethodName
(	O
Gao	O
et	O
al	O
,	O
2021	O
)	O
uses	O
a	O
contrastive	B-MethodName
learning	I-MethodName
to	O
train	O
sentence	B-TaskName
embeddings	I-TaskName
in	O
both	O
unsupervised	O
and	O
supervised	O
settings	O
.	O
The	O
former	O
is	O
trained	O
to	O
take	O
an	O
input	O
sentence	O
and	O
reconstruct	O
it	O
using	O
standard	O
dropout	O
as	O
noise	O
.	O
The	O
latter	O
uses	O
example	O
pairs	O
from	O
the	O
MNLI	B-DatasetName
dataset	O
(	O
Williams	O
et	O
al	O
,	O
2018	O
)	O
with	O
entailments	O
serving	O
as	O
positive	O
examples	O
and	O
contradiction	O
serving	O
as	O
hard	O
negatives	O
for	O
contrastive	B-MethodName
learning	I-MethodName
.	O
We	O
give	O
the	O
row	O
sentences	O
directly	O
to	O
SimCSE	B-MethodName
to	O
get	O
their	O
embeddings	O
.	O
To	O
avoid	O
misleading	O
matches	O
between	O
the	O
hypothesis	O
tokens	O
and	O
those	O
in	O
the	O
premise	O
title	O
,	O
we	O
swap	O
the	O
hypothesis	O
title	O
tokens	O
with	O
a	O
single	O
token	O
title	O
from	O
another	O
randomly	O
selected	O
table	O
of	O
the	O
same	O
category	O
.	O
We	O
then	O
use	O
the	O
cosine	O
similarity	O
between	O
SimCSE	B-MethodName
sentence	B-TaskName
embeddings	I-TaskName
to	O
compute	O
the	O
final	O
relevance	O
score	O
.	O
We	O
again	O
use	O
the	O
sparsity	O
and	O
dynamic	O
selection	O
as	O
earlier	O
.	O
In	O
the	O
study	O
,	O
we	O
refer	O
to	O
this	O
method	O
as	O
SimCSE	B-MethodName
(	O
Hypo	O
-	O
Title	O
-	O
Swap	O
+	O
Re	O
-	O
rank	O
+	O
Top	O
-	O
K	O
τ	O
)	O
.	O

The	O
supervised	O
evidence	O
extraction	O
procedure	O
consists	O
of	O
three	O
aspects	O
:	O
Dataset	O
Construction	O
.	O
We	O
use	O
the	O
annotated	O
relevant	O
row	O
data	O
(	O
$	O
3	O
)	O
to	O
construct	O
a	O
supervised	O
extraction	O
training	O
dataset	O
.	O
Every	O
row	O
in	O
the	O
table	O
,	O
paired	O
with	O
the	O
hypothesis	O
,	O
is	O
associated	O
with	O
a	O
binary	O
label	O
signifying	O
whether	O
the	O
row	O
is	O
relevant	O
or	O
not	O
.	O
As	O
before	O
,	O
we	O
use	O
the	O
sentences	O
from	O
Better	O
Paragraph	O
Representation	O
(	O
BPR	O
)	O
(	O
Neeraja	O
et	O
al	O
,	O
2021	O
)	O
to	O
represent	O
each	O
row	O
.	O
Label	O
Balancing	O
.	O
Our	O
annotation	O
,	O
and	O
the	O
perturbation	O
probing	O
analysis	O
of	O
10	O
,	O
show	O
that	O
the	O
number	O
of	O
irrelevant	O
rows	O
can	O
be	O
much	O
larger	O
than	O
the	O
relevant	O
ones	O
for	O
a	O
tablehypothesis	O
pair	O
.	O
Therefore	O
,	O
if	O
we	O
use	O
all	O
irrelevant	O
rows	O
from	O
tables	O
as	O
negative	O
examples	O
,	O
the	O
resulting	O
training	O
set	O
would	O
be	O
imbalanced	O
,	O
with	O
about	O
6×	O
more	O
irrelevant	O
rows	O
than	O
relevant	O
rows	O
.	O
We	O
investigate	O
several	O
label	O
balancing	O
strategies	O
by	O
sub	O
-	O
sampling	O
irrelevant	O
rows	O
for	O
training	O
.	O
We	O
explore	O
the	O
following	O
schemes	O
:	O
(	O
a	O
)	O
taking	O
all	O
irrelevant	O
rows	O
from	O
the	O
table	O
without	O
sub	O
-	O
sampling	O
(	O
on	O
average	O
6×	O
more	O
irrelevant	O
rows	O
)	O
referred	O
to	O
as	O
Without	O
Sample	O
(	O
6×	O
)	O
,	O
(	O
b	O
)	O
randomly	O
sampling	O
unrelated	O
rowsin	O
the	O
same	O
proportion	O
as	O
relevant	O
rows	O
,	O
referred	O
to	O
as	O
Random	O
Negative	O
(	O
1×	O
)	O
,	O
(	O
c	O
)	O
using	O
the	O
unsupervised	O
DRR	O
(	O
Re	O
-	O
Rank	O
+	O
Top	O
-	O
S	O
τ	O
)	O
method	O
to	O
pick	O
the	O
irrelevant	O
rows	O
that	O
are	O
most	O
similar	O
to	O
the	O
hypothesis	O
,	O
in	O
equal	O
proportion	O
as	O
the	O
relevant	O
rows	O
,	O
referred	O
to	O
as	O
Hard	O
Negative	O
(	O
1×	O
)	O
,	O
and	O
(	O
d	O
)	O
same	O
as	O
(	O
c	O
)	O
,	O
except	O
picking	O
three	O
times	O
as	O
many	O
irrelevant	O
rows	O
,	O
referred	O
to	O
as	O
Hard	O
Negative	O
(	O
3×	O
)	O
11	O
.	O
Classifier	O
Training	O
.	O
We	O
train	O
a	O
relevant	O
-	O
vsirrelevant	O
row	O
classifier	O
using	O
RoBERTa	B-MethodName
LARGE	O
's	O
two	O
sentence	O
classifier	O
.	O
We	O
use	O
RoBERTa	B-MethodName
LARGE	O
because	O
of	O
its	O
superior	O
performance	O
over	O
other	O
models	O
in	O
preliminary	O
experiments	O
,	O
and	O
also	O
the	O
fact	O
that	O
it	O
is	O
also	O
used	O
for	O
the	O
NLI	O
classifier	O
.	O

Working	O
with	O
a	O
small	O
haiku	O
corpus	O
,	O
we	O
used	O
a	O
POS	O
tagger	O
to	O
reveal	O
the	O
grammatical	O
structure	O
typical	O
to	O
haikus	O
.	O
The	O
CMU	O
Pronouncing	O
Dictionary	O
is	O
used	O
to	O
count	O
syllables	O
of	O
words	O
that	O
fill	O
in	O
this	O
structure	O
.	O
3	O
The	O
Brown	O
corpus	O
was	O
used	O
to	O
generate	O
n	O
-	O
grams	O
,	O
and	O
the	O
generation	O
process	O
prefers	O
more	O
common	O
constructions	O
in	O
haikus	O
.	O
4	O
Wikipedia	O
data	O
was	O
processed	O
with	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
to	O
create	O
a	O
semantic	O
vector	O
space	O
model	O
of	O
topics	O
,	O
based	O
on	O
word	O
co	O
-	O
occurrences	O
.	O
5	O
Adding	O
a	O
web	O
API	O
turned	O
the	O
haiku	O
generating	O
system	O
into	O
a	O
haiku	O
server	O
,	O
and	O
facilitated	O
subsequent	O
work	O
with	O
FloWr	O
.	O
In	O
short	O
:	O
(	O
1	O
)	O
whether	O
a	O
given	O
haiku	O
makes	O
sense	O
and	O
how	O
well	O
it	O
fits	O
the	O
topic	O
,	O
(	O
2	O
)	O
whether	O
it	O
fits	O
the	O
form	O
,	O
i.e.	O
,	O
is	O
it	O
a	O
valid	O
haiku	O
?	O
,	O
and	O
(	O
3	O
)	O
,	O
the	O
beauty	O
of	O
the	O
writing	O
,	O
the	O
emotion	B-DatasetName
it	O
evokes	O
.	O
Details	O
of	O
a	O
surveybased	O
blind	O
comparison	O
of	O
human	O
and	O
computerwritten	O
haikus	O
were	O
written	O
up	O
by	O
Aji	O
(	O
2015	O
)	O
.	O
The	O
system	O
was	O
then	O
extended	O
with	O
multiple	O
inputs	O
,	O
in	O
some	O
cases	O
producing	O
interesting	O
blends	O
:	O
e.g.	O
,	O
the	O
following	O
in	O
response	O
to	O
"	O
frog	O
pond	O
"	O
and	O
"	O
moon	O
"	O
:	O
that	O
gull	O
in	O
the	O
dressvivacious	O
in	O
statue	O
from	O
so	O
many	O
ebbs	O
II	O
.	O
Generation	O
of	O
rengas	O
Here	O
are	O
two	O
rengas	O
generated	O
by	O
wrapping	O
the	O
haiku	O
API	O
inside	O
the	O
FloWr	O
flowchart	O
system	O
(	O
Charnley	O
et	O
al	O
,	O
2016	O
In	O
each	O
case	O
,	O
the	O
prompt	O
for	O
the	O
first	O
link	O
is	O
"	O
flower	O
blossom	O
"	O
and	O
each	O
link	O
is	O
passed	O
on	O
to	O
the	O
next	O
link	O
along	O
with	O
a	O
secondary	O
prompt	O
.	O
The	O
secondary	O
links	O
are	O
"	O
moon	O
,	O
"	O
"	O
autumn	O
,	O
"	O
and	O
"	O
love	O
,	O
"	O
respectively	O
.	O
For	O
the	O
first	O
renga	O
,	O
we	O
designed	O
a	O
flowchart	O
that	O
selects	O
the	O
"	O
most	O
positive	O
"	O
haiku	O
from	O
the	O
ten	O
that	O
the	O
haiku	O
API	O
returns	O
,	O
using	O
the	O
AFINN	O
word	O
list	O
.	O
6	O
In	O
the	O
second	O
renga	O
,	O
we	O
designed	O
a	O
flowchart	O
to	O
select	O
the	O
haiku	O
with	O
the	O
lowest	O
word	O
variety	O
(	O
computed	O
in	O
terms	O
of	O
Levenshtein	O
distance	O
)	O
.	O
We	O
made	O
improvements	O
to	O
the	O
use	O
of	O
the	O
Brown	O
corpus	O
to	O
utilise	O
n	O
-	O
grams	O
for	O
word	O
-	O
flow	O
and	O
sense	O
,	O
as	O
well	O
as	O
tuning	O
the	O
weightings	O
given	O
to	O
sense	O
and	O
topic	O
.	O
We	O
implemented	O
the	O
injection	O
of	O
topics	O
via	O
by	O
blending	O
,	O
as	O
per	O
classical	O
constraints	O
(	O
e.g.	O
,	O
required	O
seasonal	O
themes	O
like	O
"	O
winter	O
,	O
"	O
or	O
"	O
flowers	O
"	O
in	O
the	O
penultimate	O
link	O
)	O
.	O
At	O
left	O
,	O
we	O
quote	O
the	O
closing	O
links	O
of	O
the	O
first	O
Nijiun	O
renga	O
generated	O
by	O
our	O
software	O
.	O

Distributional	O
properties	O
of	O
political	O
dogwhistle	B-DatasetName
representations	O
in	O
Swedish	O
BERT	B-MethodName

Dogwhistles	O
"	O
are	O
expressions	O
intended	O
by	O
the	O
speaker	O
to	O
have	O
two	O
messages	O
:	O
a	O
sociallyunacceptable	O
"	O
in	O
-	O
group	O
"	O
message	O
understood	O
by	O
a	O
subset	O
of	O
listeners	O
and	O
a	O
benign	O
message	O
intended	O
for	O
the	O
out	O
-	O
group	O
.	O
We	O
take	O
the	O
result	O
of	O
a	O
word	O
-	O
replacement	O
survey	O
of	O
the	O
Swedish	O
population	O
intended	O
to	O
reveal	O
how	O
dogwhistles	O
are	O
understood	O
,	O
and	O
we	O
show	O
that	O
the	O
difficulty	O
of	O
annotating	O
dogwhistles	O
is	O
reflected	O
in	O
the	O
separability	O
of	O
the	O
space	O
of	O
a	O
sentence	O
-	O
transformer	O
Swedish	O
BERT	B-MethodName
trained	O
on	O
general	O
data	O
.	O
1	O

Dogwhistle	B-DatasetName
politics	O
has	O
become	O
increasingly	O
salient	O
in	O
the	O
current	O
mass	O
and	O
social	O
media	O
environment	O
.	O
This	O
is	O
also	O
the	O
case	O
in	O
Swedish	O
society	O
.	O
Recent	O
studies	O
have	O
shown	O
that	O
certain	O
issues	O
,	O
in	O
particular	O
immigration	O
,	O
have	O
produced	O
examples	O
of	O
emergent	O
dogwhistles	O
gaining	O
in	O
public	O
use	O
(	O
Åkerlund	O
,	O
2021	O
;	O
Filimon	O
et	O
al	O
,	O
2020	O
)	O
.	O
Using	O
a	O
professional	O
polling	O
firm	O
,	O
we	O
anonymously	O
sampled	O
1000	O
members	O
of	O
the	O
Swedish	O
public	O
using	O
a	O
word	O
replacement	O
task	O
.	O
We	O
constructed	O
5	O
sentences	O
containing	O
words	O
or	O
phrases	O
we	O
suspected	O
were	O
being	O
used	O
as	O
dogwhistles	O
and	O
asked	O
survey	O
participants	O
to	O
replace	O
the	O
words	O
with	O
what	O
they	O
thought	O
it	O
"	O
really	O
"	O
meant	O
.	O
Then	O
we	O
manually	O
annotated	O
these	O
responses	O
for	O
whether	O
they	O
identified	O
a	O
dogwhistle	B-DatasetName
use	O
or	O
not	O
.	O
The	O
survey	O
was	O
conducted	O
under	O
institutional	O
ethical	O
review	O
in	O
a	O
process	O
that	O
involved	O
survey	O
administration	O
and	O
anonymized	O
data	O
compilation	O
at	O
a	O
remove	O
from	O
the	O
authors	O
.	O
Each	O
item	O
therefore	O
contains	O
the	O
substitution	O
of	O
participant	O
-	O
provided	O
words	O
or	O
phrases	O
for	O
the	O
original	O
dogwhistle	B-DatasetName
in	O
the	O
full	O
context	O
of	O
the	O
corresponding	O
stimulus	O
sentence	O
.	O
An	O
illustrative	O
stimulus	O
example	O
would	O
be	O
the	O
following	O
:	O
"	O
The	O
Swedish	O
unions	O
are	O
controlled	O
by	O
globalists	O
"	O
.	O
Each	O
person	O
taking	O
the	O
survey	O
would	O
replace	O
"	O
globalists	O
"	O
with	O
a	O
word	O
or	O
phrase	O
they	O
believe	O
to	O
convey	O
the	O
same	O
information	O
.	O
The	O
replacements	O
can	O
vary	O
widely	O
:	O
someone	O
might	O
replace	O
"	O
globalists	O
"	O
with	O
"	O
communists	O
"	O
or	O
an	O
anti	O
-	O
Semitic	O
slur	O
,	O
which	O
might	O
be	O
considered	O
an	O
"	O
in	O
-	O
group	O
"	O
response	O
.	O
Others	O
would	O
replace	O
"	O
globalists	O
"	O
with	O
,	O
e.g.	O
,	O
"	O
people	O
concerned	O
with	O
international	O
affairs	O
"	O
thus	O
not	O
showing	O
an	O
understanding	O
of	O
the	O
dogwhistle	B-DatasetName
as	O
having	O
any	O
associations	O
with	O
the	O
aforementioned	O
groups	O
.	O
The	O
actual	O
Swedish	O
dogwhistles	O
we	O
use	O
and	O
their	O
English	O
translations	O
are	O
listed	O
in	O
table	O
1	O
.	O
Each	O
replacement	O
thus	O
gave	O
rise	O
to	O
a	O
slightly	O
altered	O
sentence	O
that	O
,	O
according	O
to	O
the	O
person	O
taking	O
the	O
survey	O
,	O
would	O
convey	O
the	O
same	O
information	O
as	O
the	O
original	O
sentence	O
.	O
The	O
replacements	O
for	O
each	O
dogwhistle	B-DatasetName
was	O
manually	O
labeled	O
depending	O
on	O
a	O
person	O
picking	O
up	O
on	O
the	O
dogwhistle	B-DatasetName
meaning	O
or	O
not	O
.	O
An	O
inter	O
-	O
annotator	O
score	O
was	O
then	O
calculated	O
for	O
the	O
labeling	O
of	O
each	O
dogwhistle	B-DatasetName
.	O
IAA	O
was	O
calculated	O
in	O
two	O
rounds	O
,	O
an	O
initial	O
round	O
and	O
a	O
confirmatory	O
round	O
partway	O
through	O
the	O
annotation	O
.	O
We	O
report	O
both	O
scores	O
in	O
table	O
2	O
.	O

The	O
goal	O
of	O
the	O
annotation	O
and	O
the	O
computation	O
of	O
IAA	O
is	O
to	O
determine	O
whether	O
or	O
not	O
the	O
annotation	O
task	O
can	O
be	O
designed	O
with	O
the	O
following	O
criterion	O
in	O
mind	O
:	O
that	O
a	O
panel	O
of	O
trained	O
annotators	O
with	O
access	O
to	O
the	O
guidelines	O
can	O
reliably	O
distinguish	O
between	O
participant	O
responses	O
that	O
did	O
pick	O
up	O
on	O
the	O
"	O
ingroup	O
"	O
dogwhistle	B-DatasetName
meaning	O
from	O
those	O
that	O
did	O
not	O
.	O
The	O
identification	O
and	O
interpretation	O
of	O
a	O
dogwhistle	B-DatasetName
is	O
an	O
inherently	O
subjective	O
task	O
which	O
stems	O
directly	O
from	O
one	O
of	O
the	O
reasons	O
to	O
use	O
a	O
dogwhistle	B-DatasetName
in	O
the	O
first	O
place	O
:	O
to	O
take	O
advantage	O
of	O
the	O
ambiguity	O
of	O
interpretation	O
based	O
on	O
the	O
standpoint	O
of	O
the	O
individual	O
recipients	O
of	O
the	O
message	O
.	O
There	O
are	O
good	O
reasons	O
to	O
critique	O
the	O
widespread	O
use	O
of	O
IAA	O
statistics	O
to	O
represent	O
reader	O
or	O
listener	O
reaction	O
in	O
subjective	O
tasks	O
like	O
these	O
(	O
Sayeed	O
,	O
2013	O
)	O
.	O
However	O
,	O
in	O
this	O
case	O
,	O
the	O
annotation	O
guidelines	O
were	O
developed	O
in	O
an	O
iterative	O
process	O
to	O
be	O
presented	O
in	O
future	O
publications	O
that	O
ensured	O
that	O
Swedish	O
-	O
speaking	O
annotators	O
informed	O
about	O
Swedish	O
politics	O
could	O
consistently	O
identify	O
the	O
dogwhistle	B-DatasetName
interpretations	O
of	O
survey	O
participants	O
.	O
The	O
focus	O
of	O
this	O
work	O
is	O
to	O
explore	O
the	O
extent	O
to	O
which	O
the	O
intuitions	O
behind	O
the	O
annotation	O
guidelines	O
are	O
reflected	O
in	O
a	O
Swedish	O
BERT	B-MethodName
model	O
trained	O
on	O
a	O
multi	O
-	O
genre	O
corpus	O
.	O

Sentence	O
transformers	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2019	O
)	O
are	O
based	O
on	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2018	O
)	O
and	O
produce	O
state	O
of	O
the	O
art	O
semantic	O
representations	O
of	O
entire	O
sentences	O
and	O
paragraphs	O
.	O
A	O
high	O
performing	O
sentence	O
model	O
returns	O
semantic	O
representations	O
of	O
sentences	O
,	O
with	O
a	O
cosine	O
distance	O
that	O
correlates	O
with	O
their	O
semantic	B-TaskName
similarity	I-TaskName
.	O
Different	O
sentences	O
can	O
thus	O
be	O
compared	O
computationally	O
.	O
The	O
specific	O
sentence	O
model	O
we	O
used	O
was	O
Swedish	O
sentence	O
-	O
Bert	O
(	O
Rekathati	O
,	O
2021	O
)	O
.	O
Resources	O
for	O
training	O
machine	O
learning	O
models	O
on	O
Swedish	O
text	O
are	O
somewhat	O
limited	O
.	O
The	O
lack	O
of	O
resources	O
prevents	O
training	O
a	O
sentence	O
transformer	O
in	O
Swedish	O
using	O
the	O
same	O
procedure	O
as	O
training	O
sentence	O
transformers	O
in	O
English	O
.	O
However	O
,	O
the	O
training	O
of	O
a	O
sentence	O
transformer	O
in	O
the	O
target	O
language	O
can	O
be	O
obtained	O
by	O
fine	O
-	O
turning	O
a	O
Swedish	O
model	O
(	O
Malmsten	O
et	O
al	O
,	O
2020	O
)	O
3	O
on	O
the	O
output	O
of	O
an	O
already	O
trained	O
English	O
sentence	O
transformer	O
and	O
a	O
parallel	O
corpora	O
of	O
the	O
source	O
and	O
target	O
language	O
.	O
(	O
Reimers	O
and	O
Gurevych	O
,	O
2020	O
)	O
.	O
This	O
procedure	O
is	O
an	O
accessible	O
way	O
to	O
train	O
sentence	O
transformers	O
in	O
a	O
variety	O
of	O
languages	O
faced	O
with	O
the	O
same	O
data	O
limitations	O
as	O
Swedish	O
.	O

As	O
we	O
were	O
interested	O
in	O
the	O
semantic	O
representations	O
given	O
by	O
the	O
sentence	O
replacements	O
for	O
each	O
dogwhistle	B-DatasetName
response	O
,	O
we	O
did	O
the	O
following	O
:	O
we	O
input	O
each	O
of	O
the	O
sentences	O
containing	O
the	O
replaced	O
dogwhistle	B-DatasetName
from	O
the	O
dataset	O
into	O
a	O
sentence	O
transformer	O
in	O
order	O
to	O
get	O
dense	O
768	O
-	O
dimensional	O
vector	O
representations	O
.	O
Then	O
in	O
order	O
to	O
visualize	O
the	O
semantic	O
clustering	O
of	O
these	O
sentence	O
representations	O
we	O
used	O
Principal	O
Component	O
Analysis	O
(	O
PCA	B-MethodName
;	O
Abdi	O
and	O
Williams	O
,	O
2010	O
)	O
to	O
reduce	O
the	O
vectors	O
to	O
3	O
dimensions	O
.	O

The	O
general	O
purpose	O
of	O
the	O
clustering	O
validations	O
is	O
to	O
measure	O
the	O
compactness	O
,	O
i.e.	O
,	O
how	O
similar	O
objects	O
within	O
a	O
cluster	O
are	O
,	O
and	O
separation	O
,	O
which	O
measures	O
how	O
far	O
apart	O
the	O
clusters	O
are	O
.	O
We	O
evaluated	O
the	O
clustering	O
created	O
in	O
the	O
semantic	O
space	O
using	O
two	O
different	O
evaluation	O
metrics	O
:	O
The	O
overwhelming	O
bulk	O
of	O
the	O
training	O
data	O
is	O
news	O
media	O
.	O
Davies	O
-	O
Bouldin	O
(	O
DB	O
;	O
Davies	O
and	O
Bouldin	O
,	O
1979	O
)	O
score	O
measures	O
the	O
average	O
of	O
the	O
intra	O
-	O
cluster	O
dispersion	O
within	O
each	O
individual	O
cluster	O
divided	O
by	O
the	O
distance	O
between	O
the	O
centroid	O
of	O
one	O
cluster	O
to	O
the	O
centroid	O
of	O
the	O
other	O
cluster	O
.	O
A	O
more	O
compact	O
cluster	O
further	O
apart	O
from	O
the	O
other	O
cluster	O
will	O
result	O
in	O
a	O
lower	O
score	O
,	O
with	O
0	B-DatasetName
indicating	O
two	O
very	O
distinct	O
clusters	O
.	O
Calinski	O
-	O
Harabasz	O
(	O
CH	O
;	O
Caliński	O
and	O
Harabasz	O
,	O
1974	O
)	O
,	O
measures	O
intra	O
-	O
cluster	O
dispersion	O
and	O
each	O
cluster	O
center	O
's	O
distance	O
from	O
the	O
global	O
centroid	O
.	O

We	O
evaluated	O
the	O
same	O
sentence	O
representations	O
using	O
the	O
previous	O
metrics	O
,	O
but	O
with	O
the	O
annotated	O
labels	O
rather	O
than	O
the	O
K	O
-	O
means	O
labels	O
.	O
In	O
addition	O
,	O
we	O
trained	O
a	O
linear	O
-	O
kernel	O
support	B-MethodName
vector	I-MethodName
machine	I-MethodName
(	O
SVM	B-MethodName
)	O
.	O
When	O
training	O
the	O
SVM	B-MethodName
,	O
we	O
randomly	O
sampled	O
the	O
sentence	O
representations	O
and	O
labels	O
,	O
and	O
split	O
the	O
data	O
into	O
training	O
and	O
testing	O
(	O
70	O
%	O
-	O
30	O
%	O
)	O
.	O
A	O
higher	O
F	O
1	O
score	O
corresponds	O
to	O
a	O
better	O
division	O
of	O
the	O
clusters	O
.	O

Our	O
main	O
question	O
:	O
is	O
there	O
an	O
easily	O
detected	O
separation	O
between	O
the	O
in	O
-	O
group	O
responses	O
and	O
the	O
out	O
-	O
group	O
responses	O
in	O
the	O
representation	O
space	O
?	O
If	O
this	O
was	O
the	O
case	O
,	O
it	O
would	O
mean	O
that	O
the	O
model	O
has	O
picked	O
up	O
on	O
some	O
distinction	O
between	O
the	O
responses	O
that	O
corresponds	O
to	O
the	O
distinction	O
made	O
by	O
the	O
annotators	O
.	O
Given	O
the	O
distance	O
in	O
the	O
semantic	O
space	O
between	O
the	O
two	O
groups	O
,	O
it	O
should	O
be	O
possible	O
to	O
separate	O
the	O
space	O
with	O
a	O
linear	O
SVM	B-MethodName
trained	O
on	O
a	O
subset	O
of	O
the	O
data	O
.	O
A	O
further	O
question	O
is	O
whether	O
there	O
is	O
a	O
correlation	O
between	O
the	O
clusterings	O
and	O
the	O
IAA	O
scores	O
?	O
Being	O
able	O
to	O
linearly	O
separate	O
the	O
two	O
groups	O
is	O
a	O
necessary	O
but	O
not	O
sufficient	O
condition	O
for	O
good	O
clustering	O
scores	O
.	O
The	O
dogwhistle	B-DatasetName
replacements	O
might	O
vary	O
widely	O
enough	O
to	O
not	O
cluster	O
well	O
while	O
still	O
being	O
separatable	O
using	O
a	O
hyperplane	O
to	O
a	O
high	O
de	O
-	O
gree	O
of	O
accuracy	B-MetricName
.	O
Ideally	O
,	O
two	O
differentiable	O
dense	O
clusters	O
would	O
correspond	O
to	O
the	O
IAA	O
.	O

The	O
SVM	B-MethodName
was	O
generally	O
able	O
to	O
separate	O
the	O
two	O
clusters	O
well	O
,	O
even	O
given	O
fairly	O
small	O
amounts	O
of	O
training	O
data	O
.	O
The	O
general	O
correlation	O
with	O
IAA	O
scores	O
were	O
higher	O
with	O
PCA	B-MethodName
dimensionalityreduced	O
vector	O
representations	O
.	O
Possible	O
reasons	O
for	O
the	O
performance	O
of	O
the	O
SVM	B-MethodName
might	O
be	O
that	O
the	O
SVM	B-MethodName
does	O
not	O
take	O
into	O
account	O
the	O
separation	O
of	O
the	O
data	O
from	O
its	O
cluster	O
centroid	O
in	O
the	O
opposite	O
di	O
-	O
rection	O
of	O
the	O
other	O
cluster	O
or	O
the	O
dispersion	O
of	O
the	O
datapoints	O
along	O
an	O
axis	O
orthogonal	O
to	O
the	O
separating	O
plane	O
.	O
The	O
SVM	B-MethodName
measurement	O
only	O
takes	O
into	O
account	O
the	O
overlapping	O
of	O
the	O
semantic	O
meanings	O
of	O
the	O
sentences	O
,	O
represented	O
in	O
the	O
space	O
.	O

Our	O
work	O
contributes	O
a	O
computationally	O
straightforward	O
method	O
to	O
extend	O
the	O
manual	O
analysis	O
of	O
dogwhistles	O
that	O
is	O
available	O
for	O
many	O
languages	O
at	O
a	O
resource	O
level	O
similar	O
to	O
Swedish	O
.	O
Our	O
evaluations	O
show	O
that	O
easily	O
identified	O
dogwhistle	B-DatasetName
interpretations	O
are	O
partitioned	O
well	O
enough	O
in	O
the	O
vector	O
space	O
given	O
by	O
SOTA	O
sentence	O
models	O
that	O
they	O
are	O
linearly	O
separable	O
using	O
a	O
simple	O
SVM	B-MethodName
.	O
The	O
representation	O
of	O
sentences	O
given	O
by	O
the	O
model	O
is	O
largely	O
derived	O
from	O
the	O
corpora	O
that	O
the	O
model	O
is	O
trained	O
on	O
.	O
The	O
corpora	O
thus	O
has	O
a	O
large	O
impact	O
on	O
the	O
semantic	O
space	O
.	O
Given	O
this	O
,	O
models	O
trained	O
on	O
different	O
corpora	O
would	O
give	O
rise	O
to	O
different	O
semantic	O
spaces	O
where	O
the	O
clustering	O
of	O
the	O
sentences	O
would	O
be	O
different	O
.	O
Since	O
K	O
-	O
means	O
does	O
not	O
seem	O
to	O
be	O
able	O
to	O
differentiate	O
between	O
in	O
-	O
group	O
sentence	O
replacements	O
and	O
out	O
-	O
group	O
sentence	O
replacements	O
,	O
future	O
work	O
might	O
include	O
an	O
investigation	O
into	O
modeling	O
the	O
semantic	O
space	O
by	O
training	O
a	O
sentence	O
transformer	O
on	O
different	O
sources	O
of	O
text	O
.	O
This	O
would	O
also	O
allow	O
us	O
to	O
investigate	O
the	O
role	O
of	O
specific	O
lexical	O
choices	O
in	O
the	O
detection	O
and	O
representation	O
of	O
dogwhistles	O
.	O
In	O
theory	O
,	O
it	O
should	O
be	O
possible	O
to	O
train	O
a	O
model	O
that	O
creates	O
a	O
semantic	O
space	O
that	O
clusters	O
the	O
points	O
in	O
a	O
way	O
that	O
that	O
the	O
labels	O
can	O
be	O
retrieved	O
by	O
an	O
algorithm	O
like	O
K	O
-	O
means	O
using	O
only	O
the	O
data	O
itself	O
.	O

Dice	B-MethodName
Loss	I-MethodName
for	O
Data	O
-	O
imbalanced	O
NLP	O
Tasks	O

Many	O
NLP	O
tasks	O
such	O
as	O
tagging	O
and	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
(	O
MRC	O
)	O
are	O
faced	O
with	O
the	O
severe	O
data	O
imbalance	O
issue	O
:	O
negative	O
examples	O
significantly	O
outnumber	O
positive	O
ones	O
,	O
and	O
the	O
huge	O
number	O
of	O
easy	O
-	O
negative	O
examples	O
overwhelms	O
training	O
.	O
The	O
most	O
commonly	O
used	O
cross	O
entropy	O
criteria	O
is	O
actually	O
accuracy	B-MetricName
-	O
oriented	O
,	O
which	O
creates	O
a	O
discrepancy	O
between	O
training	O
and	O
test	O
.	O
At	O
training	O
time	O
,	O
each	O
training	O
instance	O
contributes	O
equally	O
to	O
the	O
objective	O
function	O
,	O
while	O
at	O
test	O
time	O
F1	B-MetricName
score	I-MetricName
concerns	O
more	O
about	O
positive	O
examples	O
.	O

For	O
illustration	O
purposes	O
,	O
we	O
use	O
the	O
binary	O
classification	O
task	O
to	O
demonstrate	O
how	O
different	O
losses	O
work	O
.	O
The	O
mechanism	O
can	O
be	O
easily	O
extended	O
to	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
.	O
Let	O
X	O
denote	O
a	O
set	O
of	O
training	O
instances	O
and	O
each	O
instance	O
x	O
i	O
X	O
is	O
associated	O
with	O
a	O
golden	O
binary	O
label	O
y	O
i	O
=	O
[	O
y	O
i0	O
,	O
y	O
i1	O
]	O
denoting	O
the	O
ground	O
-	O
truth	O
class	O
x	O
i	O
belongs	O
to	O
,	O
and	O
p	O
i	O
=	O
[	O
p	O
i0	O
,	O
p	O
i1	O
]	O
is	O
the	O
predicted	O
probabilities	O
of	O
the	O
two	O
classes	O
respectively	O
,	O
where	O
y	O
i0	O
,	O
y	O
i1	O
{	O
0	B-DatasetName
,	O
1	O
}	O
,	O
p	O
i0	O
,	O
p	O
i1	O
[	O
0	B-DatasetName
,	O
1	O
]	O
and	O
p	O
i1	O
+	O
p	O
i0	O
=	O
1	O
.	O

We	O
evaluated	O
the	O
proposed	O
method	O
on	O
four	O
NLP	O
tasks	O
,	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
,	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
,	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
and	O
paraphrase	B-TaskName
identification	I-TaskName
.	O
Hyperparameters	O
are	O
tuned	O
on	O
the	O
corresponding	O
development	O
set	O
of	O
each	O
dataset	O
.	O
More	O
experiment	O
details	O
including	O
datasets	O
and	O
hyperparameters	O
are	O
shown	O
in	O
supplementary	B-DatasetName
material	I-DatasetName
.	O

We	O
argue	O
that	O
the	O
cross	O
-	O
entropy	O
objective	O
is	O
actually	O
accuracy	B-MetricName
-	O
oriented	O
,	O
whereas	O
the	O
proposed	O
losses	O
perform	O
as	O
a	O
soft	O
version	O
of	O
F1	B-MetricName
score	I-MetricName
.	O
To	O
These	O
results	O
verify	O
that	O
the	O
proposed	O
dice	B-MethodName
loss	I-MethodName
is	O
not	O
accuracy	B-MetricName
-	O
oriented	O
,	O
and	O
should	O
not	O
be	O
used	O
for	O
accuracy	B-MetricName
-	O
oriented	O
tasks	O
.	O

Datasets	O
For	O
the	O
NER	B-TaskName
task	O
,	O
we	O
consider	O
both	O
Chinese	O
datasets	O
,	O
i.e.	O
,	O
OntoNotes4.0	O
5	O
and	O
MSRA	O
6	O
,	O
and	O
English	O
datasets	O
,	O
i.e.	O
,	O
CoNLL2003	B-DatasetName
7	O
and	O
OntoNotes5.0	O
8	O
.	O
CoNLL2003	B-DatasetName
is	O
an	O
English	O
dataset	O
with	O
4	O
entity	O
types	O
:	O
Location	O
,	O
Organization	O
,	O
Person	O
and	O
Miscellaneous	B-TaskName
.	O
We	O
followed	O
data	O
processing	O
protocols	O
in	O
(	O
Ma	O
and	O
Hovy	O
,	O
2016	O
)	O
.	O
English	O
OntoNotes5.0	O
consists	O
of	O
texts	O
from	O
a	O
wide	O
variety	O
of	O
sources	O
and	O
contains	O
18	O
entity	O
types	O
.	O
We	O
use	O
the	O
standard	O
train	O
/	O
dev	O
/	O
test	O
split	O
of	O
CoNLL2012	O
shared	O
task	O
.	O
Chinese	O
MSRA	O
performs	O
as	O
a	O
Chinese	O
benchmark	O
dataset	O
containing	O
3	O
entity	O
types	O
.	O
Data	O
in	O
MSRA	O
is	O
collected	O
from	O
news	O
domain	O
.	O
Since	O
the	O
development	O
set	O
is	O
not	O
provided	O
in	O
the	O
original	O
MSRA	O
dataset	O
,	O
we	O
randomly	O
split	O
the	O
training	O
set	O
into	O
training	O
and	O
development	O
splits	O
by	O
9:1	O
.	O
We	O
use	O
the	O
official	O
test	O
set	O
for	O
evaluation	O
.	O
Chinese	O
OntoNotes4.0	O
is	O
a	O
Chinese	O
dataset	O
and	O
consists	O
of	O
texts	O
from	O
news	O
domain	O
,	O
which	O
has	O
18	O
entity	O
types	O
.	O
In	O
this	O
paper	O
,	O
we	O
take	O
the	O
same	O
data	O
split	O
as	O
did	O
.	O

Datasets	O
For	O
MRC	O
task	O
,	O
we	O
use	O
three	O
datasets	O
:	O
SQuADv1.1	O
/	O
v2.0	O
9	O
and	O
Queref	O
10	O
datasets	O
.	O
SQuAD	B-DatasetName
v1.1	O
and	O
SQuAD	B-DatasetName
v2.0	O
are	O
the	O
most	O
widely	O
used	O
QA	O
benchmarks	O
.	O
SQuAD1.1	B-DatasetName
is	O
a	O
collection	O
of	O
100	O
K	O
crowdsourced	O
question	O
-	O
answer	O
pairs	O
,	O
and	O
SQuAD2.0	B-DatasetName
extends	O
SQuAD1.1	B-DatasetName
allowing	O
no	O
short	O
answer	O
exists	O
in	O
the	O
provided	O
passage	O
.	O
MRPC	B-DatasetName
is	O
a	O
corpus	O
of	O
sentence	O
pairs	O
automatically	O
extracted	O
from	O
online	O
news	O
sources	O
,	O
with	O
human	O
annotations	O
of	O
whether	O
the	O
sentence	O
pairs	O
are	O
semantically	O
equivalent	O
.	O
The	O
MRPC	B-DatasetName
dataset	O
has	O
imbalanced	O
classes	O
(	O
6800	O
pairs	O
in	O
total	O
,	O
and	O
68	O
%	O
for	O
positive	O
,	O
32	O
%	O
for	O
negative	O
)	O
.	O
QQP	B-DatasetName
is	O
a	O
collection	O
of	O
question	O
pairs	O
from	O
the	O
community	O
question	O
-	O
answering	O
website	O
Quora	O
.	O
The	O
class	O
distribution	O
in	O
QQP	B-DatasetName
is	O
also	O
unbalanced	O
(	O
over	O
400	O
,	O
000	O
question	O
pairs	O
in	O
total	O
,	O
and	O
37	O
%	O
for	O
positive	O
,	O
63	O
%	O
for	O
negative	O
)	O
.	O

The	O
success	O
of	O
high	O
-	O
stakes	O
exams	O
,	O
such	O
as	O
those	O
used	O
in	O
licensing	O
,	O
certification	O
,	O
and	O
college	O
admission	O
,	O
depends	O
on	O
the	O
use	O
of	O
items	O
(	O
test	O
questions	O
)	O
that	O
meet	O
stringent	O
quality	O
criteria	O
.	O
To	O
provide	O
useful	O
information	O
about	O
examinee	O
ability	O
,	O
good	O
items	O
must	O
be	O
neither	O
too	O
difficult	O
,	O
nor	O
too	O
easy	O
for	O
the	O
intended	O
test	O
-	O
takers	O
.	O
Furthermore	O
,	O
the	O
timing	O
demands	O
of	O
items	O
should	O
be	O
such	O
that	O
different	O
exam	O
forms	O
seen	O
by	O
different	O
test	O
-	O
takers	O
should	O
entail	O
similar	O
times	O
to	O
complete	O
.	O
Nevertheless	O
,	O
while	O
an	O
extreme	O
difficulty	O
or	O
mean	O
response	O
time	O
can	O
indicate	O
that	O
an	O
item	O
is	O
not	O
functioning	O
correctly	O
,	O
within	O
these	O
extremes	O
variability	O
in	O
difficulty	O
and	O
item	O
response	O
time	O
is	O
expected	O
.	O
For	O
good	O
items	O
,	O
it	O
is	O
hoped	O
that	O
this	O
variability	O
simply	O
reflects	O
the	O
breadth	O
and	O
depth	O
of	O
the	O
relevant	O
exam	O
content	O
.	O
The	O
interaction	O
between	O
item	O
difficulty	O
(	O
as	O
measured	O
by	O
the	O
proportion	O
of	O
examinees	O
who	O
respond	O
correctly	O
)	O
and	O
time	O
intensiveness	O
(	O
as	O
measured	O
by	O
the	O
average	O
time	O
examinees	O
spend	O
answering	O
)	O
can	O
help	O
quantify	O
the	O
complexity	O
of	O
the	O
response	O
process	O
associated	O
with	O
an	O
item	O
.	O
This	O
is	O
valuable	O
,	O
since	O
the	O
more	O
we	O
know	O
about	O
the	O
way	O
examinees	O
think	O
about	O
the	O
problem	O
presented	O
in	O
an	O
item	O
,	O
the	O
better	O
we	O
can	O
evaluate	O
exam	O
validity	O
.	O
Although	O
easier	O
items	O
usually	O
require	O
less	O
time	O
than	O
difficult	O
items	O
,	O
the	O
interaction	O
between	O
these	O
two	O
item	O
properties	O
is	O
not	O
strictly	O
linear	O
-	O
examinees	O
may	O
spend	O
very	O
little	O
time	O
responding	O
to	O
certain	O
difficult	O
items	O
and	O
,	O
likewise	O
,	O
examinees	O
may	O
spend	O
a	O
great	O
deal	O
of	O
time	O
on	O
items	O
that	O
are	O
relatively	O
easy	O
.	O
The	O
idea	O
of	O
response	O
process	O
complexity	O
is	O
best	O
illustrated	O
with	O
items	O
that	O
have	O
similar	O
difficulty	O
but	O
different	O
mean	O
response	O
times	O
.	O
In	O
such	O
cases	O
,	O
one	O
item	O
may	O
require	O
the	O
formation	O
of	O
a	O
complex	O
cognitive	O
model	O
of	O
the	O
problem	O
and	O
thus	O
take	O
a	O
long	O
time	O
,	O
while	O
another	O
item	O
with	O
a	O
similar	O
level	O
of	O
difficulty	O
may	O
require	O
factual	O
knowledge	O
that	O
few	O
examinees	O
recall	O
(	O
or	O
that	O
many	O
recall	O
incorrectly	O
)	O
and	O
thus	O
take	O
a	O
short	O
time	O
on	O
average	O
.	O
The	O
interaction	O
between	O
item	O
difficulty	O
and	O
time	O
intensity	O
can	O
therefore	O
provide	O
valuable	O
information	O
about	O
the	O
complexity	O
of	O
the	O
response	O
process	O
demanded	O
by	O
an	O
item	O
,	O
which	O
,	O
we	O
argue	O
,	O
can	O
be	O
further	O
explained	O
by	O
examining	O
the	O
linguistic	O
properties	O
of	O
the	O
item	O
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
a	O
data	O
-	O
driven	O
approach	O
to	O
capture	O
the	O
interaction	O
between	O
item	O
difficulty	O
and	O
response	O
time	O
within	O
a	O
pool	O
of	O
18	O
,	O
961	O
multiplechoice	O
items	O
from	O
a	O
high	O
-	O
stakes	O
medical	O
exam	O
,	O
where	O
each	O
item	O
was	O
answered	O
by	O
335	O
examinees	O
on	O
average	O
.	O
For	O
our	O
data	O
,	O
this	O
resulted	O
in	O
the	O
definition	O
of	O
two	O
clusters	O
,	O
one	O
of	O
which	O
consisted	O
of	O
items	O
that	O
are	O
relatively	O
easy	O
and	O
less	O
time	O
-	O
intensive	O
,	O
and	O
another	O
one	O
which	O
consisted	O
of	O
items	O
that	O
are	O
relatively	O
difficult	O
and/or	O
time	O
-	O
intensive	O
.	O
For	O
the	O
purposes	O
of	O
this	O
study	O
,	O
we	O
name	O
these	O
two	O
clusters	O
low	O
-	O
complexity	O
class	O
and	O
high	O
-	O
complexity	O
class	O
,	O
respectively	O
.	O
The	O
use	O
of	O
the	O
term	O
response	O
process	O
A	O
16	O
-	O
year	O
-	O
old	O
boy	O
is	O
brought	O
to	O
the	O
emergency	O
department	O
because	O
of	O
a	O
2	O
-	O
day	O
history	O
of	O
fever	O
,	O
nausea	O
,	O
vomiting	O
,	O
headache	O
,	O
chills	O
,	O
and	O
fatigue	O
.	O
He	O
has	O
not	O
had	O
any	O
sick	O
contacts	O
.	O
He	O
underwent	O
splenectomy	O
for	O
traumatic	O
injury	O
at	O
the	O
age	O
of	O
13	O
years	O
.	O
He	O
has	O
no	O
other	O
history	O
of	O
serious	O
illness	O
and	O
takes	O
no	O
medications	O
.	O
He	O
appears	O
ill	O
.	O
His	O
temperature	O
is	O
39.2	O
°	O
C	O
(	O
102.5	O
°	O
F	O
)	O
,	O
pulse	O
is	O
130	O
/	O
min	O
,	O
respirations	O
are	O
14	O
/	O
min	O
,	O
and	O
blood	O
pressure	O
is	O
110/60	O
mm	O
Hg	O
.	O
On	O
pulmonary	O
examination	O
,	O
scattered	O
crackles	O
are	O
heard	O
bilaterally	O
.	O
Abdominal	O
shows	O
a	O
well	O
-	O
healed	O
midline	O
scar	O
and	O
mild	O
,	O
diffuse	O
tenderness	O
to	O
palpation	O
.	O
Which	O
of	O
the	O
following	O
is	O
the	O
most	O
appropriate	O
next	O
step	O
in	O
management	O
?	O
(	O
A	O
)	O
Antibiotic	O
therapy	O
(	O
B	O
)	O
Antiemetic	O
therapy	O
(	O
C	O
)	O
CT	O
scan	O
of	O
the	O
chest	O
(	O
D	O
)	O
X	O
-	O
ray	O
of	O
the	O
abdomen	O
(	O
E	O
)	O
Reassurance	O
Table	O
1	O
:	O
An	O
example	O
of	O
a	O
practice	O
item	O
complexity	O
here	O
is	O
not	O
based	O
on	O
an	O
operational	O
definition	O
of	O
this	O
construct	O
,	O
which	O
would	O
require	O
extensive	O
research	O
on	O
its	O
own	O
,	O
but	O
rather	O
,	O
as	O
a	O
succinct	O
label	O
that	O
summarises	O
the	O
differences	O
between	O
the	O
two	O
classes	O
along	O
the	O
interaction	O
of	O
empirical	O
item	O
difficulty	O
and	O
item	O
time	O
intensiveness	O
.	O
Studying	O
the	O
linguistic	O
characteristics	O
of	O
these	O
two	O
categories	O
may	O
help	O
test	O
developers	O
gain	O
a	O
more	O
nuanced	O
understanding	O
of	O
how	O
cognitively	O
complex	O
items	O
differ	O
from	O
those	O
with	O
a	O
straightforward	O
solution	O
.	O
Provided	O
that	O
strong	O
relationships	O
are	O
found	O
,	O
such	O
insight	O
can	O
also	O
be	O
used	O
to	O
guide	O
item	O
writers	O
or	O
inform	O
innovative	O
automated	O
item	O
generation	O
algorithms	O
when	O
seeking	O
to	O
create	O
high	O
-	O
or	O
low	O
-	O
complexity	O
items	O
.	O
For	O
this	O
reason	O
,	O
our	O
goal	O
is	O
not	O
to	O
train	O
a	O
black	O
-	O
box	O
model	O
to	O
predict	O
item	O
complexity	O
;	O
instead	O
,	O
our	O
goal	O
is	O
to	O
isolate	O
interpretable	O
relationships	O
between	O
item	O
text	O
and	O
item	O
complexity	O
that	O
can	O
inform	O
our	O
understanding	O
of	O
the	O
response	O
process	O
and	O
provide	O
better	O
itemwriting	O
strategies	O
.	O
In	O
addition	O
to	O
its	O
utility	O
for	O
improving	O
highstakes	O
exams	O
,	O
the	O
problem	O
of	O
modeling	O
response	O
process	O
complexity	O
is	O
interesting	O
from	O
an	O
NLP	O
perspective	O
because	O
it	O
requires	O
the	O
modeling	O
of	O
cognitive	O
processes	O
beyond	O
reading	B-TaskName
comprehension	I-TaskName
.	O
This	O
is	O
especially	O
relevant	O
for	O
the	O
data	O
used	O
here	O
because	O
,	O
as	O
we	O
explain	O
in	O
Section	O
3	O
below	O
,	O
the	O
items	O
in	O
our	O
bank	O
assess	O
expert	O
-	O
level	O
clinical	B-TaskName
knowledge	I-TaskName
and	O
are	O
written	O
to	O
a	O
common	O
reading	O
level	O
using	O
standardized	O
language	O
.	O
Contributions	O
:	O
i	O
)	O
We	O
use	O
unsupervised	O
clustering	O
to	O
define	O
classes	O
of	O
high	O
and	O
low	O
responseprocess	O
complexity	O
from	O
a	O
large	O
sample	O
of	O
items	O
and	O
test	O
-	O
takers	O
in	O
a	O
high	O
-	O
stakes	O
medical	O
exam	O
;	O
ii	O
)	O
the	O
study	O
provides	O
empirical	O
evidence	O
that	O
linguistic	O
characteristics	O
carry	O
signal	O
relevant	O
to	O
an	O
item	O
's	O
response	O
process	O
complexity	O
;	O
iii	O
)	O
the	O
most	O
predictive	O
features	O
are	O
identified	O
through	O
several	O
feature	B-MethodName
selection	I-MethodName
methods	O
and	O
their	O
potential	O
relationship	O
to	O
response	O
process	O
complexity	O
is	O
discussed	O
;	O
iv	O
)	O
the	O
errors	O
made	O
by	O
the	O
model	O
and	O
their	O
implications	O
for	O
predicting	O
response	O
process	O
complexity	O
are	O
analysed	O
.	O

This	O
section	O
discusses	O
related	O
work	O
on	O
the	O
topics	O
of	O
modeling	O
item	O
difficulty	O
and	O
response	O
time	O
.	O
Most	O
NLP	O
studies	O
modeling	O
the	O
difficulty	O
of	O
test	O
questions	O
for	O
humans	O
have	O
been	O
conducted	O
in	O
the	O
domain	O
of	O
reading	B-TaskName
comprehension	I-TaskName
,	O
where	O
the	O
readability	O
of	O
reading	O
passages	O
is	O
associated	O
with	O
the	O
difficulty	O
of	O
their	O
corresponding	O
comprehension	O
questions	O
(	O
Huang	O
et	O
al	O
,	O
2017	O
;	O
Beinborn	O
et	O
al	O
,	O
2015	O
;	O
Loukina	O
et	O
al	O
,	O
2016	O
)	O
.	O
For	O
other	O
exams	O
,	O
taxonomies	O
representing	O
knowledge	O
dimensions	O
and	O
cognitive	O
processes	O
involved	O
in	O
the	O
completion	O
of	O
a	O
test	O
task	O
have	O
been	O
used	O
to	O
predict	O
the	O
difficulty	O
of	O
short	O
-	O
answer	O
questions	O
(	O
Padó	O
,	O
2017	O
)	O
and	O
identify	O
skills	O
required	O
to	O
answer	O
school	O
science	O
questions	O
(	O
Nadeem	O
and	O
Ostendorf	O
,	O
2017	O
)	O
.	O
Difficulty	O
prediction	O
has	O
also	O
been	O
explored	O
in	O
the	O
context	O
of	O
evaluating	O
automatically	O
generated	O
questions	O
(	O
Alsubait	O
et	O
al	O
,	O
2013	O
;	O
Ha	O
and	O
Yaneva	O
,	O
2018	O
;	O
Kurdi	O
,	O
2020	O
;	O
through	O
measures	O
such	O
as	O
question	O
-	O
answer	O
similarity	O
.	O
Response	O
time	O
prediction	O
has	O
mainly	O
been	O
explored	O
in	O
the	O
field	O
of	O
educational	O
testing	O
using	O
predictors	O
such	O
as	O
item	O
presentation	O
position	O
(	O
Parshall	O
et	O
al	O
,	O
1994	O
)	O
,	O
item	O
content	O
category	O
(	O
Parshall	O
et	O
al	O
,	O
1994	O
;	O
Smith	O
,	O
2000	O
)	O
,	O
the	O
presence	O
of	O
a	O
figure	O
(	O
Smith	O
,	O
2000	O
;	O
Swanson	O
et	O
al	O
,	O
2001	O
)	O
,	O
and	O
item	O
difficulty	O
and	O
discrimination	O
(	O
Halkitis	O
et	O
al	O
,	O
1996	O
;	O
Smith	O
,	O
2000	O
)	O
.	O
The	O
only	O
text	O
-	O
related	O
feature	O
explored	O
in	O
these	O
studies	O
was	O
word	O
count	O
,	O
and	O
it	O
was	O
shown	O
to	O
have	O
a	O
very	O
limited	O
predictive	O
power	O
in	O
most	O
domains	O
.	O
Several	O
studies	O
have	O
explored	O
the	O
prediction	O
of	O
item	O
difficulty	O
and	O
response	O
time	O
in	O
the	O
context	O
of	O
clinical	O
multiple	O
choice	O
questions	O
(	O
MCQs	O
)	O
.	O
Ha	O
et	O
al	O
(	O
2019	O
)	O
propose	O
a	O
large	O
number	O
of	O
linguis	O
-	O
tic	O
features	O
and	O
embeddings	O
for	O
modeling	O
item	O
difficulty	O
.	O
The	O
results	O
show	O
that	O
the	O
full	O
model	O
outperforms	O
several	O
baselines	O
with	O
a	O
statistically	O
significant	O
improvement	O
,	O
however	O
,	O
its	O
practical	O
significance	O
for	O
successfully	O
predicting	O
item	O
difficulty	O
remains	O
limited	O
,	O
confirming	O
the	O
challenging	O
nature	O
of	O
the	O
problem	O
.	O
Continuations	O
of	O
this	O
study	O
include	O
the	O
use	O
of	O
transfer	B-TaskName
learning	I-TaskName
to	O
predict	O
difficulty	O
and	O
response	O
time	O
(	O
Xue	O
et	O
al	O
,	O
2020	O
)	O
,	O
as	O
well	O
as	O
using	O
predicted	O
difficulty	O
for	O
filtering	O
out	O
items	O
that	O
are	O
too	O
easy	O
or	O
too	O
difficult	O
for	O
the	O
intended	O
examinee	O
population	O
.	O
used	O
a	O
broad	O
range	O
of	O
linguistic	O
features	O
and	O
embeddings	O
(	O
similar	O
to	O
those	O
in	O
Ha	O
et	O
al	O
(	O
2019	O
)	O
)	O
to	O
predict	O
item	O
response	O
time	O
,	O
showing	O
that	O
a	O
wide	O
range	O
of	O
linguistic	O
predictors	O
at	O
various	O
levels	O
of	O
linguistic	O
processing	O
were	O
all	O
relevant	O
to	O
responsetime	O
prediction	O
.	O
The	O
predicted	O
response	O
times	O
were	O
then	O
used	O
in	O
a	O
subsequent	O
experiment	O
to	O
improve	O
fairness	O
by	O
reducing	O
the	O
time	O
intensity	O
variance	O
of	O
exam	O
forms	O
.	O

The	O
data	O
1	O
used	O
in	O
this	O
study	O
comprises	O
18	O
,	O
961	O
Step	O
2	O
Clinical	B-TaskName
Knowledge	I-TaskName
items	O
from	O
the	O
United	O
States	O
Medical	O
Licensing	O
Examination	O
(	O
USMLE	O
®	O
)	O
,	O
a	O
large	O
-	O
scale	O
high	O
-	O
stakes	O
medical	O
assessment	O
.	O
All	O
items	O
were	O
MCQs	O
.	O
An	O
example	O
practice	O
item	O
2	O
is	O
given	O
in	O
Table	O
1	O
.	O
The	O
exam	O
comprises	O
several	O
one	O
-	O
hour	O
testing	O
blocks	O
with	O
40	O
items	O
per	O
block	O
.	O
All	O
items	O
test	O
medical	O
knowledge	O
and	O
are	O
written	O
by	O
experienced	O
item	O
-	O
writers	O
following	O
guidelines	O
intended	O
to	O
produce	O
items	O
that	O
vary	O
in	O
their	O
difficulty	O
and	O
response	O
times	O
only	O
due	O
to	O
differences	O
in	O
the	O
medical	O
content	O
they	O
assess	O
.	O
These	O
guidelines	O
stipulate	O
that	O
item	O
writers	O
adhere	O
to	O
a	O
standard	O
structure	O
and	O
avoid	O
excessive	O
verbosity	O
,	O
extraneous	O
material	O
not	O
needed	O
to	O
answer	O
the	O
item	O
,	O
information	O
designed	O
to	O
mislead	O
the	O
test	O
-	O
taker	O
,	O
and	O
grammatical	O
cues	O
(	O
e.g.	O
,	O
correct	O
answers	O
that	O
are	O
more	O
specific	O
than	O
the	O
other	O
options	O
)	O
.	O
All	O
items	O
were	O
administered	O
between	O
2010	O
and	O
2015	O
as	O
pretest	O
items	O
and	O
presented	O
alongside	O
scored	O
items	O
on	O
operational	O
exams	O
.	O
Examinees	O
were	O
medical	O
students	O
from	O
accredited	O
US	O
and	O
Canadian	O
medical	O
schools	O
taking	O
the	O
exam	O
for	O
the	O
first	O
time	O
and	O
had	O
no	O
way	O
of	O
knowing	O
which	O
items	O
were	O
pretest	O
items	O
and	O
which	O
were	O
1	O
The	O
data	O
can	O
not	O
be	O
made	O
available	O
due	O
to	O
exam	O
security	O
considerations	O
.	O
2	O
Source	O
:	O
https://www.usmle.org/pdfs/	O
step	O
-	O
2	O
-	O
ck/2020_Step2CK_SampleItems.pdf	O
scored	O
.	O
On	O
average	O
,	O
each	O
item	O
was	O
attempted	O
by	O
335	O
examinees	O
(	O
SD	O
=	O
156.8	O
)	O
.	O

We	O
base	O
our	O
definition	O
of	O
the	O
two	O
classes	O
of	O
items	O
on	O
empirical	O
item	O
difficulty	O
and	O
time	O
intensity	O
.	O
Item	O
difficulty	O
is	O
measured	O
by	O
the	O
proportion	O
of	O
examinees	O
who	O
answered	O
the	O
item	O
correctly	O
,	O
a	O
metric	O
commonly	O
referred	O
to	O
by	O
the	O
educational	O
testing	O
community	O
as	O
p	O
-	O
value	O
and	O
calculated	O
as	O
follows	O
:	O
P	O
i	O
=	O
N	O
n=1	O
U	O
n	O
N	O
,	O
where	O
P	O
i	O
is	O
the	O
p	O
-	O
value	O
for	O
item	O
i	O
,	O
U	O
n	O
is	O
the	O
0	B-DatasetName
-	O
1	O
score	O
(	O
incorrect	O
-	O
correct	O
)	O
on	O
item	O
i	O
earned	O
by	O
examinee	O
n	O
,	O
and	O
N	O
is	O
the	O
total	O
number	O
of	O
examinees	O
in	O
the	O
sample	O
.	O
Thus	O
,	O
difficulty	O
measured	O
in	O
this	O
way	O
ranges	O
from	O
0	B-DatasetName
to	O
1	O
and	O
higher	O
values	O
correspond	O
to	O
easier	O
items	O
.	O
Time	O
intensity	O
is	O
found	O
by	O
taking	O
the	O
arithmetic	O
mean	O
response	O
time	O
,	O
measured	O
in	O
seconds	O
,	O
across	O
all	O
examinees	O
who	O
attempted	O
a	O
given	O
item	O
.	O
This	O
includes	O
all	O
time	O
spent	O
on	O
the	O
item	O
from	O
the	O
moment	O
it	O
is	O
presented	O
on	O
the	O
screen	O
until	O
the	O
examinee	O
moves	O
to	O
the	O
next	O
item	O
,	O
as	O
well	O
as	O
any	O
revisits	O
.	O
To	O
assign	O
items	O
to	O
classes	O
,	O
p	O
-	O
value	O
and	O
mean	O
response	O
time	O
are	O
rescaled	O
such	O
that	O
each	O
variable	O
has	O
a	O
mean	O
of	O
0	B-DatasetName
and	O
a	O
standard	O
deviation	O
of	O
1	O
.	O
Moreover	O
,	O
we	O
use	O
two	O
quantitative	O
methods	O
to	O
categorize	O
items	O
and	O
retain	O
only	O
those	O
items	O
where	O
there	O
was	O
agreement	O
between	O
the	O
two	O
methods	O
.	O
Method	O
1	O
:	O
Items	O
were	O
classified	O
by	O
applying	O
a	O
K	B-MethodName
-	I-MethodName
means	I-MethodName
clustering	I-MethodName
algorithm	O
via	O
the	O
kmeans	O
function	O
in	O
Python	O
's	O
Scikit	O
-	O
learn	O
(	O
Pedregosa	O
et	O
al	O
,	O
2011	O
)	O
.	O
K	O
-	O
means	O
is	O
an	O
unsupervised	O
data	O
classification	O
technique	O
that	O
discovers	O
patterns	O
in	O
the	O
data	O
by	O
assigning	O
instances	O
to	O
a	O
pre	O
-	O
defined	O
number	O
of	O
classes	O
(	O
Wagstaff	O
et	O
al	O
,	O
2001	O
)	O
.	O
This	O
approach	O
also	O
allows	O
us	O
to	O
evaluate	O
the	O
plausibility	O
of	O
categorizing	O
items	O
into	O
more	O
than	O
two	O
complexity	O
classes	O
,	O
or	O
whether	O
the	O
items	O
fail	O
to	O
show	O
any	O
meaningful	O
separation	O
along	O
the	O
interaction	O
of	O
p	O
-	O
value	O
and	O
duration	O
(	O
one	O
class	O
)	O
.	O
Results	O
suggest	O
that	O
two	O
classes	O
best	O
fit	O
these	O
data	O
and	O
identified	O
11	O
,	O
067	O
items	O
as	O
low	O
complexity	O
and	O
7	O
,	O
894	O
items	O
as	O
high	O
complexity	O
3	O
.	O
Method	O
2	O
:	O
Any	O
item	O
with	O
a	O
rescaled	O
p	O
-	O
value	O
greater	O
than	O
its	O
rescaled	O
mean	O
response	O
time	O
-	O
indicating	O
that	O
the	O
item	O
is	O
relatively	O
easier	O
than	O
it	O
is	O
time	O
-	O
consuming	O
-	O
is	O
classified	O
as	O
low	O
-	O
complexity	O
(	O
11	O
,	O
682	O
items	O
)	O
.	O
Likewise	O
,	O
the	O
remaining	O
items	O
,	O
which	O
had	O
rescaled	O
p	O
-	O
values	O
less	O
than	O
their	O
rescaled	O
mean	O
response	O
times	O
,	O
were	O
assigned	O
to	O
the	O
highcomplexity	O
class	O
(	O
7	O
,	O
279	O
items	O
)	O
.	O
Put	O
another	O
way	O
,	O
if	O
an	O
item	O
takes	O
less	O
time	O
than	O
we	O
would	O
expect	O
given	O
its	O
difficulty	O
,	O
the	O
item	O
is	O
classified	O
as	O
low	O
response	O
process	O
complexity	O
and	O
if	O
it	O
takes	O
more	O
time	O
than	O
we	O
would	O
expect	O
,	O
it	O
is	O
classified	O
as	O
high	O
response	O
process	O
complexity	O
.	O
The	O
two	O
methods	O
achieved	O
strong	O
agreement	O
,	O
with	O
only	O
673	O
(	O
3.5	O
%	O
)	O
items	O
being	O
assigned	O
to	O
different	O
classes	O
across	O
methods	O
.	O
These	O
discrepant	O
items	O
are	O
excluded	O
,	O
leaving	O
a	O
total	O
of	O
18	O
,	O
288	O
items	O
for	O
further	O
analysis	O
:	O
11	O
,	O
038	O
low	O
-	O
complexity	O
items	O
and	O
7	O
,	O
250	O
high	O
-	O
complexity	O
ones	O
.	O
Figure	O
1	O
shows	O
the	O
class	O
assignment	O
,	O
p	O
-	O
value	O
,	O
and	O
mean	O
response	O
time	O
for	O
each	O
item	O
.	O
As	O
can	O
be	O
seen	O
from	O
the	O
figure	O
,	O
the	O
class	O
of	O
lowcomplexity	O
items	O
was	O
dense	O
and	O
homogenous	O
compared	O
to	O
the	O
high	O
-	O
complexity	O
class	O
,	O
meaning	O
that	O
it	O
contained	O
a	O
large	O
number	O
of	O
easy	O
items	O
whose	O
response	O
times	O
were	O
always	O
below	O
125	O
seconds	O
.	O
The	O
high	O
-	O
complexity	O
class	O
on	O
the	O
other	O
hand	O
was	O
highly	O
heterogeneous	O
,	O
with	O
items	O
whose	O
response	O
times	O
and	O
p	O
-	O
values	O
spanned	O
almost	O
the	O
entire	O
scale	O
.	O

This	O
group	O
of	O
features	O
relates	O
to	O
the	O
medical	O
content	O
of	O
the	O
items	O
by	O
mapping	O
terms	O
and	O
phrases	O
in	O
the	O
text	O
to	O
medical	O
concepts	O
contained	O
in	O
the	O
Unified	O
Medical	O
Language	O
System	O
(	O
UMLS	B-DatasetName
)	O
Metathesaurus	O
(	O
Schuyler	O
et	O
al	O
,	O
1993	O
)	O
the	O
item	O
contains	O
(	O
note	O
that	O
a	O
given	O
term	O
found	O
in	O
the	O
items	O
can	O
refer	O
to	O
multiple	O
UMLS	B-DatasetName
concepts	O
)	O
.	O
First	O
,	O
we	O
ask	O
:	O
how	O
many	O
of	O
the	O
words	O
and	O
phrases	O
in	O
the	O
items	O
are	O
medical	O
terms	O
?	O
This	O
information	O
is	O
captured	O
by	O
UMLS	B-DatasetName
Terms	O
Count	O
,	O
indicating	O
the	O
number	O
of	O
terms	O
in	O
an	O
item	O
that	O
appear	O
in	O
the	O
UMLS	B-DatasetName
wherein	O
each	O
instance	O
of	O
a	O
given	O
term	O
contributes	O
to	O
the	O
total	O
count	O
,	O
as	O
well	O
as	O
UMLS	B-DatasetName
Distinct	O
Terms	O
Count	O
:	O
the	O
number	O
of	O
terms	O
in	O
an	O
item	O
that	O
appear	O
in	O
the	O
UMLS	B-DatasetName
wherein	O
multiple	O
instances	O
of	O
a	O
given	O
term	O
contribute	O
only	O
once	O
to	O
the	O
total	O
count	O
.	O
The	O
same	O
kinds	O
of	O
counts	O
are	O
done	O
for	O
medical	O
phrases	O
-	O
UMLS	B-DatasetName
Phrases	O
Count	O
refers	O
to	O
the	O
number	O
of	O
phrases	O
in	O
an	O
item	O
.	O
For	O
example	O
,	O
Metamap	O
maps	O
'	O
ocular	O
complications	O
of	O
myasthenia	O
gravis	O
'	O
to	O
two	O
phrases	O
:	O
the	O
noun	O
phrase	O
'	O
ocular	O
complications	O
'	O
and	O
the	O
prepositional	O
phrase	O
'	O
of	O
myasthenia	O
gravis	O
'	O
(	O
Aronson	O
,	O
2001	O
)	O
.	O
Next	O
,	O
we	O
introduce	O
features	O
that	O
measure	O
the	O
ambiguity	O
of	O
medical	O
terms	O
within	O
the	O
items	O
.	O
These	O
include	O
Average	O
Number	O
of	O
Competing	O
UMLS	B-DatasetName
Concepts	O
Per	O
Term	O
Count	O
,	O
which	O
captures	O
the	O
average	O
number	O
of	O
UMLS	B-DatasetName
concepts	O
that	O
a	O
term	O
could	O
be	O
referring	O
to	O
,	O
averaged	O
for	O
all	O
terms	O
in	O
an	O
item	O
,	O
and	O
weighted	O
by	O
the	O
number	O
of	O
times	O
Metamap	O
returns	O
the	O
term	O
.	O
A	O
similar	O
version	O
of	O
this	O
feature	O
but	O
without	O
weighting	O
by	O
the	O
number	O
of	O
times	O
Metamap	O
returns	O
the	O
term	O
is	O
Average	O
Number	O
of	O
UMLS	B-DatasetName
Concepts	O
Per	O
Term	O
Count	O
.	O
This	O
metric	O
is	O
then	O
computed	O
at	O
the	O
level	O
of	O
sentences	O
and	O
items	O
,	O
resulting	O
in	O
:	O
Average	O
Number	O
of	O
UMLS	B-DatasetName
Concepts	O
per	O
Sentence	O
,	O
which	O
measures	O
the	O
medical	O
ambigu	O
-	O
ity	O
of	O
sentences	O
and	O
UMLS	B-DatasetName
Concept	O
Count	O
,	O
which	O
measures	O
item	O
medical	O
ambiguity	O
through	O
the	O
total	O
number	O
of	O
UMLS	B-DatasetName
concepts	O
all	O
terms	O
in	O
an	O
item	O
could	O
refer	O
to	O
.	O
Finally	O
,	O
UMLS	B-DatasetName
concept	O
incidence	O
refers	O
to	O
the	O
number	O
of	O
UMLS	B-DatasetName
concepts	O
per	O
1000	O
words	O
.	O

This	O
section	O
describes	O
three	O
baseline	O
models	O
(	O
Section	O
4.5	O
)	O
,	O
the	O
training	O
of	O
classifiers	O
using	O
the	O
full	O
feature	O
set	O
(	O
Section	O
4.6	O
)	O
,	O
and	O
the	O
feature	B-MethodName
selection	I-MethodName
procedures	O
(	O
Section	O
4.7	O
)	O
.	O

After	O
scaling	O
the	O
features	O
,	O
two	O
models	O
were	O
fit	O
using	O
Python	O
's	O
scikit	O
-	O
learn	O
library	O
and	O
the	O
full	O
set	O
of	O
features	O
:	O
a	O
logistic	B-MethodName
regression	I-MethodName
model	O
and	O
a	O
random	O
forests	O
one	O
(	O
400	O
trees	O
)	O
.	O
Twenty	O
percent	O
of	O
the	O
data	O
(	O
3	O
,	O
658	O
items	O
)	O
were	O
used	O
as	O
a	O
test	O
set	O
.	O

Feature	B-MethodName
selection	I-MethodName
was	O
undertaken	O
to	O
better	O
understand	O
which	O
features	O
were	O
most	O
strongly	O
associated	O
with	O
class	O
differences	O
.	O
The	O
selection	O
process	O
utilized	O
three	O
distinct	O
strategies	O
,	O
where	O
the	O
final	O
set	O
of	O
selected	O
features	O
comprises	O
only	O
those	O
features	O
retained	O
by	O
all	O
three	O
methods	O
.	O
After	O
applying	O
feature	B-MethodName
selection	I-MethodName
to	O
the	O
training	O
set	O
,	O
the	O
predictive	O
performance	O
of	O
the	O
selected	O
features	O
is	O
evaluated	O
on	O
the	O
test	O
set	O
and	O
compared	O
to	O
the	O
performance	O
of	O
the	O
full	O
feature	O
set	O
and	O
the	O
baseline	O
models	O
outlined	O
above	O
.	O
Embedded	O
methods	O
:	O
The	O
first	O
method	O
is	O
LASSO	O
regularized	O
regression	O
wherein	O
the	O
coefficients	O
of	O
variables	O
that	O
have	O
low	O
contributions	O
towards	O
the	O
classification	O
performance	O
are	O
shrunk	O
to	O
zero	O
by	O
forcing	O
the	O
sum	O
of	O
the	O
absolute	O
value	O
of	O
the	O
regression	O
coefficients	O
to	O
be	O
less	O
than	O
a	O
fixed	O
value	O
.	O
We	O
use	O
the	O
LassoCV	O
algorithm	O
with	O
100	O
-	O
fold	O
cross	O
validation	O
and	O
maximum	O
iterations	O
set	O
to	O
5	O
,	O
000	O
.	O
Wrapper	O
methods	O
:	O
We	O
next	O
apply	O
recursive	O
feature	O
elimination	O
,	O
performed	O
using	O
two	O
different	O
classification	O
algorithms	O
:	O
random	O
forests	O
classifier	O
(	O
400	O
trees	O
,	O
step	O
=	O
5	O
)	O
and	O
gradient	O
boosting	O
classifier	O
(	O
Friedman	O
,	O
2002	O
)	O
(	O
default	O
parameters	O
,	O
step	O
=	O
5	O
)	O
.	O
The	O
final	O
set	O
of	O
selected	O
linguistic	O
features	O
comprised	O
57	O
features	O
that	O
were	O
retained	O
by	O
all	O
three	O
strategies	O
.	O
These	O
features	O
and	O
their	O
evaluation	O
are	O
discussed	O
in	O
sections	O
5	O
and	O
7	O
.	O

HHU	O
at	O
SemEval	O
-	O
2016	O
Task	O
1	O
:	O
Multiple	O
Approaches	O
to	O
Measuring	O
Semantic	B-TaskName
Textual	I-TaskName
Similarity	I-TaskName

This	O
paper	O
describes	O
our	O
participation	O
in	O
the	O
SemEval	O
-	O
2016	O
Task	O
1	O
:	O
Semantic	B-TaskName
Textual	I-TaskName
Similarity	I-TaskName
(	O
STS	B-TaskName
)	O
.	O
We	O
developed	O
three	O
methods	O
for	O
the	O
English	O
subtask	O
(	O
STS	B-TaskName
Core	O
)	O
.	O
The	O
first	O
method	O
is	O
unsupervised	O
and	O
uses	O
WordNet	O
and	O
word2vec	O
to	O
measure	O
a	O
token	O
-	O
based	O
overlap	O
.	O
In	O
our	O
second	O
approach	O
,	O
we	O
train	O
a	O
neural	O
network	O
on	O
two	O
features	O
.	O
The	O
third	O
method	O
uses	O
word2vec	O
and	O
LDA	B-MethodName
with	O
regression	O
splines	O
.	O

In	O
the	O
last	O
shared	O
tasks	O
,	O
most	O
of	O
the	O
teams	O
used	O
natural	O
languages	O
processing	O
techniques	O
like	O
tokenization	O
,	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagging	I-TaskName
,	O
lemmatization	B-TaskName
,	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
and	O
word	B-TaskName
embeddings	I-TaskName
.	O
External	O
resources	O
like	O
WordNet	O
(	O
Miller	O
,	O
1995	O
)	O
and	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
are	O
commonly	O
used	O
.	O
In	O
(	O
Agirre	O
et	O
al	O
,	O
2012	O
)	O
and	O
(	O
Agirre	O
et	O
al	O
,	O
2013	O
)	O
,	O
the	O
organizers	O
provide	O
a	O
list	O
and	O
a	O
comparison	O
of	O
the	O
tools	O
and	O
resources	O
used	O
by	O
the	O
participants	O
in	O
the	O
first	O
two	O
years	O
,	O
respectively	O
.	O
In	O
each	O
year	O
,	O
the	O
organizers	O
provide	O
a	O
baseline	O
value	O
by	O
calculating	O
the	O
cosine	O
similarity	O
of	O
the	O
binary	O
bag	O
-	O
of	O
-	O
words	O
vectors	O
from	O
both	O
sentences	O
in	O
each	O
sample	O
.	O
Since	O
2013	O
,	O
TakeLab	O
(	O
Šarić	O
et	O
al	O
,	O
2012	O
,	O
the	O
best	O
ranked	O
system	O
in	O
2012	O
,	O
has	O
also	O
been	O
used	O
as	O
another	O
baseline	O
value	O
.	O
Most	O
of	O
the	O
teams	O
used	O
machine	O
learning	O
in	O
2015	O
(	O
Agirre	O
et	O
al	O
,	O
2015	O
)	O
.	O
In	O
2014	O
,	O
the	O
best	O
two	O
submitted	O
runs	O
were	O
from	O
unsupervised	O
systems	O
.	O
The	O
work	O
most	O
closely	O
related	O
to	O
our	O
Overlap	O
method	O
is	O
(	O
Han	O
et	O
al	O
,	O
2015	O
)	O
,	O
which	O
uses	O
a	O
twophased	O
approach	O
called	O
Align	O
-	O
and	O
-	O
Differentiate	O
.	O
In	O
the	O
first	O
phase	O
,	O
they	O
compute	O
an	O
alignment	O
score	O
.	O
Afterwards	O
,	O
they	O
modify	O
the	O
alignment	O
score	O
in	O
a	O
differentiate	O
phase	O
by	O
subtracting	O
a	O
penalty	O
score	O
for	O
terms	O
that	O
can	O
not	O
be	O
aligned	O
.	O
The	O
idea	O
behind	O
the	O
computation	O
of	O
our	O
alignment	O
scores	O
is	O
the	O
same	O
:	O
For	O
each	O
sample	O
,	O
we	O
average	O
over	O
the	O
crosswise	O
similarities	O
between	O
the	O
sentences	O
by	O
aligning	O
them	O
,	O
accumulating	O
similarities	O
between	O
tokens	O
and	O
dividing	O
by	O
sentence	O
lengths	O
.	O
The	O
results	O
of	O
the	O
alignment	O
score	O
in	O
our	O
Overlap	O
method	O
differ	O
because	O
(	O
i	O
)	O
our	O
alignment	O
is	O
different	O
,	O
(	O
ii	O
)	O
we	O
use	O
another	O
similarity	O
function	O
for	O
tokens	O
,	O
and	O
(	O
iii	O
)	O
our	O
preprocessing	O
is	O
different	O
.	O
In	O
(	O
Vu	O
et	O
al	O
,	O
2015	O
)	O
,	O
the	O
similarity	O
between	O
LDA	B-MethodName
vectors	O
calculated	O
from	O
documents	O
is	O
used	O
together	O
with	O
syntactic	O
and	O
lexical	O
similarity	O
measures	O
to	O
compute	O
the	O
similarity	O
between	O
text	O
fragments	O
.	O
This	O
idea	O
is	O
also	O
incorporated	O
in	O
our	O
Deep	O
LDA	B-MethodName
method	O
.	O
Moreover	O
,	O
both	O
approaches	O
use	O
different	O
flavors	O
of	O
regression	O
analysis	O
for	O
the	O
final	O
model	O
prediction	O
.	O
Regression	O
analysis	O
was	O
also	O
used	O
in	O
(	O
Sultan	O
et	O
al	O
,	O
2015	O
)	O
,	O
where	O
the	O
authors	O
combine	O
an	O
unsupervised	O
method	O
with	O
ridge	O
regression	O
analysis	O
.	O
Our	O
approach	O
differs	O
in	O
the	O
sense	O
that	O
it	O
introduces	O
knearest	O
neighbors	O
as	O
a	O
lazy	O
training	O
layer	O
before	O
the	O
regression	O
analysis	O
phase	O
to	O
decrease	O
the	O
effect	O
of	O
noisy	O
data	O
points	O
.	O

For	O
preprocessing	O
the	O
input	O
text	O
,	O
we	O
first	O
process	O
each	O
sentence	O
with	O
Stanford	O
CoreNLP	O
(	O
Manning	O
et	O
al	O
,	O
2014	O
)	O
.	O
Afterwards	O
,	O
we	O
use	O
Hunspell	O
1	O
with	O
the	O
latest	O
OpenOffice	O
English	O
dictionaries	O
to	O
suggest	O
spelling	O
corrections	O
for	O
tokens	O
with	O
at	O
least	O
two	O
characters	O
in	O
length	O
.	O
For	O
each	O
token	O
,	O
we	O
calculate	O
the	O
Levenshtein	O
distance	O
for	O
all	O
suggestions	O
.	O
If	O
suggestions	O
have	O
the	O
same	O
lowest	O
distance	O
,	O
we	O
choose	O
the	O
longest	O
word	O
and	O
replace	O
the	O
former	O
misspelt	O
word	O
.	O
Abbreviations	O
are	O
also	O
replaced	O
by	O
their	O
full	O
forms	O
.	O
Afterwards	O
,	O
we	O
process	O
the	O
corrected	O
sentence	O
with	O
Stanford	O
CoreNLP	O
again	O
.	O
We	O
use	O
the	O
WordnetStemmer	O
from	O
the	O
Java	O
Wordnet	O
Interface	O
(	O
Finlayson	O
,	O
2014	O
)	O
to	O
look	O
up	O
lemmas	O
with	O
the	O
help	O
of	O
WordNet	O
(	O
Miller	O
,	O
1995	O
)	O
.	O
If	O
the	O
WordnetStemmer	O
can	O
not	O
provide	O
a	O
lemma	B-DatasetName
for	O
a	O
token	O
,	O
we	O
use	O
the	O
predicted	O
lemma	B-DatasetName
from	O
the	O
Stanford	O
CoreNLP	O
.	O
Instead	O
of	O
accessing	O
all	O
tokens	O
in	O
a	O
sentence	O
,	O
we	O
start	O
from	O
the	O
root	O
token	O
and	O
recursively	O
follow	O
outgoing	O
dependency	O
edges	O
and	O
add	O
all	O
visited	O
tokens	O
to	O
a	O
list	O
.	O
This	O
approach	O
improves	O
our	O
results	O
slightly	O
because	O
some	O
tokens	O
will	O
be	O
ignored	O
.	O
Furthermore	O
,	O
the	O
tokens	O
are	O
filtered	O
for	O
stopwords	O
2	O
.	O

We	O
train	O
a	O
neural	O
network	O
with	O
3	O
layers	O
and	O
a	O
sigmoid	B-MethodName
activation	I-MethodName
function	O
in	O
Accord	O
.	O
NET	O
(	O
de	O
Souza	O
,	O
2014	O
)	O
.	O
Our	O
network	O
consists	O
of	O
2	O
neurons	O
in	O
the	O
input	O
layer	O
,	O
3	O
neurons	O
in	O
the	O
hidden	O
layer	O
and	O
1	O
neuron	O
in	O
the	O
output	O
layer	O
,	O
as	O
illustrated	O
in	O
Figure	O
1	O
.	O
The	O
layer	O
weights	O
are	O
initialized	O
by	O
the	O
Nguyen	O
-	O
Widrow	O
function	O
(	O
Nguyen	O
and	O
Widrow	O
,	O
1990	O
)	O
.	O
We	O
use	O
the	O
Levenberg	O
-	O
Marquardt	O
algorithm	O
(	O
Levenberg	O
,	O
1944	O
;	O
Marquardt	O
,	O
1963	O
)	O
to	O
train	O
our	O
network	O
on	O
the	O
STS	B-TaskName
Core	O
test	O
data	O
from	O
2015	O
and	O
2014	O
.	O

We	O
represent	O
the	O
semantic	B-TaskName
similarity	I-TaskName
between	O
two	O
documents	O
s	O
1	O
and	O
s	O
2	O
by	O
means	O
of	O
a	O
vector	O
F	O
=	O
[	O
f	O
1	O
,	O
f	O
2	O
,	O
f	O
3	O
,	O
f	O
4	O
]	O
R	O
4	O
,	O
where	O
each	O
component	O
of	O
F	O
is	O
responsible	O
for	O
modelling	O
a	O
different	O
aspect	O
of	O
the	O
semantic	B-TaskName
similarity	I-TaskName
,	O
namely	O
the	O
surface	O
-	O
level	O
similarity	O
,	O
context	O
similarity	O
,	O
and	O
the	O
topical	O
similarity	O
.	O

In	O
order	O
to	O
model	O
the	O
context	O
similarity	O
between	O
documents	O
,	O
we	O
use	O
word	B-TaskName
embeddings	I-TaskName
that	O
learn	O
semantically	O
meaningful	O
representations	O
for	O
words	O
from	O
local	O
co	O
-	O
occurrences	O
in	O
sentences	O
.	O
More	O
specifically	O
we	O
use	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
which	O
seems	O
to	O
be	O
a	O
reasonable	O
choice	O
to	O
model	O
context	O
similarity	O
as	O
the	O
word	O
vectors	O
are	O
trained	O
to	O
maximize	O
the	O
log	O
probability	O
of	O
context	O
words	O
.	O
We	O
denote	O
the	O
context	O
similarity	O
of	O
two	O
documents	O
s	O
1	O
and	O
s	O
2	O
by	O
f	O
3	O
R	O
and	O
compute	O
it	O
as	O
follows	O
:	O
f	O
3	O
(	O
s	O
1	O
,	O
s	O
2	O
)	O
=	O
cos	O
(	O
ṽ	O
s	O
1	O
,	O
ṽ	O
s	O
2	O
)	O
=	O
cos	O
v	O
s	O
1	O
v	O
|	O
s	O
1	O
|	O
,	O
v	O
s	O
2	O
v	O
|	O
s	O
2	O
|	O
where	O
v	O
is	O
the	O
dense	O
vector	O
representation	O
of	O
a	O
token	O
andṽ	O
represents	O
the	O
centroid	O
of	O
the	O
word	O
vectors	O
in	O
a	O
document	O
.	O

In	O
order	O
to	O
predict	O
the	O
semantic	B-TaskName
similarity	I-TaskName
between	O
two	O
documents	O
,	O
we	O
use	O
a	O
combination	O
of	O
k	B-MethodName
-	I-MethodName
NN	I-MethodName
and	O
Multivariate	O
Adaptive	O
Regression	O
Splines	O
(	O
MARS	B-DatasetName
)	O
(	O
Friedman	O
,	O
1991	O
)	O
.	O
Let	O
T	O
=	O
{	O
(	O
s	O
1	O
,	O
s	O
1	O
,	O
gs	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
s	O
m	O
,	O
s	O
m	O
,	O
gs	O
m	O
)	O
}	O
be	O
the	O
training	O
set	O
consisting	O
of	O
m	O
document	O
pairs	O
together	O
with	O
their	O
corresponding	O
gold	O
standard	O
semantic	B-TaskName
similarity	I-TaskName
and	O
(	O
s	O
i	O
,	O
s	O
i	O
)	O
/	O
T	O
be	O
a	O
document	O
pair	O
for	O
which	O
the	O
semantic	B-TaskName
similarity	I-TaskName
has	O
to	O
be	O
computed	O
.	O
We	O
construct	O
a	O
set	O
F	O
=	O
{	O
(	O
F	O
1	O
,	O
gs	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
F	O
m	O
,	O
gs	O
m	O
)	O
}	O
where	O
each	O
F	O
j	O
is	O
the	O
four	O
-	O
dimensional	O
vector	O
representation	O
of	O
the	O
semantic	B-TaskName
similarity	I-TaskName
between	O
s	O
j	O
and	O
s	O
j	O
.	O
Moreover	O
,	O
we	O
Sentence	O
1	O
Sentence	O
2	O
gs	O
STS	B-TaskName
Unfortunately	O
the	O
answer	O
to	O
your	O
question	O
is	O
we	O
simply	O
do	O
not	O
know	O
.	O
Sorry	O
,	O
I	O
do	O
n't	O
know	O
the	O
answer	O
to	O
your	O
question	O
.	O

Table	O
2	O
:	O
Examples	O
for	O
the	O
results	O
of	O
the	O
Overlap	O
method	O
with	O
the	O
corresponding	O
gold	O
standards	O
compute	O
the	O
vector	O
F	O
i	O
.	O
Next	O
,	O
we	O
construct	O
a	O
set	O
F	O
k	O
containing	O
the	O
k	B-MethodName
-	I-MethodName
nearest	I-MethodName
neighbors	I-MethodName
to	O
the	O
vector	O
F	O
i	O
.	O
In	O
order	O
to	O
calculate	O
the	O
distances	O
between	O
the	O
vectors	O
,	O
we	O
use	O
the	O
Euclidean	O
distance	O
.	O
Finally	O
,	O
we	O
construct	O
a	O
vector	O
gs	O
k	O
containing	O
the	O
gold	O
standard	O
similarity	O
values	O
of	O
the	O
k	B-MethodName
-	I-MethodName
nearest	I-MethodName
neighbors	I-MethodName
and	O
feed	O
it	O
into	O
a	O
MARS	B-DatasetName
model	O
to	O
predict	O
the	O
semantic	B-TaskName
similarity	I-TaskName
of	O
the	O
pair	O
(	O
s	O
i	O
,	O
s	O
i	O
)	O
.	O
The	O
choice	O
of	O
MARS	B-DatasetName
is	O
due	O
to	O
its	O
capability	O
to	O
automatically	O
model	O
non	O
-	O
linearities	O
between	O
variables	O
.	O

We	O
report	O
the	O
results	O
of	O
our	O
three	O
approaches	O
for	O
the	O
STS	B-TaskName
Core	O
test	O
from	O
2016	O
and	O
2015	O
.	O

We	O
list	O
the	O
results	O
of	O
our	O
methods	O
for	O
the	O
2015	O
test	O
data	O
in	O
Table	O
3	O
to	O
discuss	O
the	O
effect	O
of	O
different	O
evaluation	O
sets	O
.	O
It	O
is	O
interesting	O
to	O
see	O
that	O
the	O
Deep	O
LDA	B-MethodName
method	O
performed	O
best	O
out	O
of	O
our	O
three	O
systems	O
on	O
2015	O
.	O
Its	O
results	O
on	O
2016	O
were	O
surprisingly	O
lower	O
.	O
We	O
attribute	O
this	O
difference	O
to	O
the	O
lack	O
of	O
domain	O
specific	O
training	O
data	O
for	O
2016	O
.	O
As	O
an	O
unsupervised	O
approach	O
,	O
the	O
Overlap	O
method	O
has	O
fewer	O
problems	O
with	O
the	O
domain	O
change	O
.	O
It	O
should	O
be	O
noted	O
that	O
the	O
gold	O
standard	O
of	O
the	O
2015	O
test	O
data	O
was	O
available	O
during	O
the	O
development	O
of	O
our	O
methods	O
.	O
For	O
the	O
training	O
phase	O
,	O
the	O
Same	O
Word	O
Neural	O
Network	O
method	O
used	O
the	O
STS	B-TaskName
Core	O
test	O
from	O
2014	O
.	O
The	O
Deep	O
LDA	B-MethodName
method	O
was	O
trained	O
on	O
the	O
data	O
from	O
2012	O
to	O
2014	O
.	O

We	O
have	O
presented	O
three	O
approaches	O
to	O
measure	O
textual	O
semantic	B-TaskName
similarity	I-TaskName
.	O
This	O
year	O
,	O
our	O
unsupervised	O
method	O
achieved	O
the	O
best	O
result	O
.	O
By	O
comparing	O
our	O
result	O
for	O
2016	O
and	O
2015	O
,	O
we	O
showed	O
that	O
the	O
approaches	O
yielded	O
different	O
results	O
in	O
a	O
different	O
order	O
.	O
In	O
our	O
future	O
work	O
,	O
we	O
will	O
try	O
to	O
modify	O
the	O
Overlap	O
method	O
,	O
by	O
also	O
using	O
a	O
penalty	O
score	O
and	O
by	O
applying	O
certain	O
similarity	O
score	O
shifters	O
,	O
for	O
instance	O
modifying	O
the	O
score	O
by	O
applying	O
a	O
date	O
extraction	O
with	O
a	O
specific	O
distance	O
function	O
for	O
dates	O
.	O
We	O
tried	O
to	O
group	O
words	O
into	O
phrases	O
by	O
using	O
a	O
sliding	O
window	O
approach	O
with	O
a	O
shrinking	O
window	O
size	O
and	O
matching	O
phrases	O
in	O
word2vec	O
.	O
In	O
our	O
initial	O
attempt	O
,	O
this	O
worsened	O
the	O
results	O
for	O
the	O
Overlap	O
method	O
.	O
We	O
will	O
adjust	O
the	O
similarity	O
function	O
to	O
increase	O
the	O
weight	O
of	O
phrases	O
in	O
comparison	O
to	O
unigrams	O
.	O
We	O
aim	O
to	O
adapt	O
the	O
techniques	O
for	O
German	O
and	O
Spanish	O
.	O

Using	O
Large	O
Pretrained	B-TaskName
Language	I-TaskName
Models	I-TaskName
for	O
Answering	O
User	O
Queries	O
from	O
Product	O
Specifications	O

While	O
buying	O
a	O
product	O
from	O
the	O
e	O
-	O
commerce	O
websites	O
,	O
customers	O
generally	O
have	O
a	O
plethora	O
of	O
questions	O
.	O
From	O
the	O
perspective	O
of	O
both	O
the	O
e	O
-	O
commerce	O
service	O
provider	O
as	O
well	O
as	O
the	O
customers	O
,	O
there	O
must	O
be	O
an	O
effective	O
question	B-TaskName
answering	I-TaskName
system	O
to	O
provide	O
immediate	O
answers	O
to	O
the	O
user	O
queries	O
.	O
While	O
certain	O
questions	O
can	O
only	O
be	O
answered	O
after	O
using	O
the	O
product	O
,	O
there	O
are	O
many	O
questions	O
which	O
can	O
be	O
answered	O
from	O
the	O
product	O
specification	O
itself	O
.	O
Our	O
work	O
takes	O
a	O
first	O
step	O
in	O
this	O
direction	O
by	O
finding	O
out	O
the	O
relevant	O
product	O
specifications	O
,	O
that	O
can	O
help	O
answering	O
the	O
user	O
questions	O
.	O
We	O
propose	O
an	O
approach	O
to	O
automatically	O
create	O
a	O
training	O
dataset	O
for	O
this	O
problem	O
.	O
We	O
utilize	O
recently	O
proposed	O
XLNet	B-MethodName
and	O
BERT	B-MethodName
architectures	O
for	O
this	O
problem	O
and	O
find	O
that	O
they	O
provide	O
much	O
better	O
performance	O
than	O
the	O
Siamese	O
model	O
,	O
previously	O
applied	O
for	O
this	O
problem	O
(	O
Lai	O
et	O
al	O
,	O
2018	O
)	O
.	O
Our	O
model	O
gives	O
a	O
good	O
performance	O
even	O
when	O
trained	O
on	O
one	O
vertical	O
and	O
tested	O
across	O
different	O
verticals	O
.	O

Product	O
specifications	O
are	O
the	O
attributes	O
of	O
a	O
product	O
.	O
These	O
specifications	O
help	O
a	O
user	O
to	O
easily	O
identify	O
and	O
differentiate	O
products	O
and	O
choose	O
the	O
one	O
that	O
matches	O
certain	O
specifications	O
.	O
There	O
are	O
more	O
than	O
80	O
million	O
products	O
across	O
80	O
+	O
product	O
categories	O
on	O
Flipkart	O
1	O
.	O
The	O
6	O
largest	O
categories	O
are	O
-	O
Mobile	O
,	O
AC	O
,	O
Backpack	O
,	O
Computer	O
,	O
Shoes	O
,	O
and	O
Watches	O
.	O
A	O
large	O
fraction	O
of	O
user	O
queries	O
(	O
∼	O
20	O
%	O
)	O
2	O
can	O
be	O
answered	O
with	O
the	O
specifications	O
.	O
Product	O
specifications	O
would	O
be	O
helpful	O
in	O
providing	O
instant	O
responses	O
to	O
questions	O
newly	O
posed	O
by	O
users	O
about	O
*	O
Work	O
done	O
while	O
author	O
was	O
at	O
IIT	O
Kharagpur	O
.	O
1	O
Flipkart	O
Pvt	B-MethodName
Ltd.	O
is	O
an	O
e	O
-	O
commerce	O
company	O
based	O
in	O
Bangalore	O
,	O
India	O
.	O
2	O
We	O
randomly	O
sampled	O
1500	O
questions	O
from	O
all	O
these	O
verticals	O
except	O
Mobile	O
and	O
manually	O
annotated	O
them	O
as	O
to	O
whether	O
these	O
can	O
be	O
answered	O
through	O
product	O
specifications	O
.	O
the	O
corresponding	O
product	O
.	O
Consider	O
a	O
question	O
"	O
What	O
is	O
the	O
fabric	O
of	O
this	O
bag	O
?	O
"	O
This	O
new	O
question	O
can	O
be	O
easily	O
answered	O
by	O
retrieving	O
the	O
specification	O
"	O
Material	O
"	O
as	O
the	O
response	O
.	O
Fig	O
.	O
1	O
depicts	O
this	O
scenario	O
.	O
Most	O
of	O
the	O
recent	O
works	O
on	O
product	O
related	O
queries	O
on	O
e	O
-	O
commerce	O
leverage	O
the	O
product	O
reviews	O
to	O
answer	O
the	O
questions	O
(	O
Gao	O
et	O
al	O
,	O
2019	O
;	O
McAuley	O
and	O
Yang	O
,	O
2016	O
)	O
.	O
Although	O
reviews	O
are	O
a	O
rich	O
source	O
of	O
data	O
,	O
they	O
are	O
also	O
subject	O
to	O
personal	O
experiences	O
.	O
People	O
tend	O
to	O
give	O
many	O
reviews	O
on	O
some	O
products	O
and	O
since	O
it	O
is	O
based	O
upon	O
their	O
personal	O
experience	O
,	O
the	O
opinion	O
is	O
also	O
diverse	O
.	O
This	O
creates	O
a	O
massive	O
volume	O
and	O
range	O
of	O
opinions	O
and	O
thus	O
makes	O
review	O
systems	O
difficult	O
to	O
navigate	O
.	O
Sometimes	O
products	O
do	O
not	O
even	O
have	O
any	O
reviews	O
that	O
can	O
be	O
used	O
to	O
find	O
an	O
answer	O
,	O
also	O
the	O
reviews	O
do	O
not	O
mention	O
the	O
specifications	O
a	O
lot	O
,	O
but	O
mainly	O
deal	O
with	O
the	O
experience	O
.	O
So	O
,	O
there	O
are	O
several	O
reasons	O
why	O
product	O
specifications	O
might	O
be	O
a	O
useful	O
source	O
of	O
information	O
to	O
answer	O
product	O
-	O
related	O
queries	O
which	O
does	O
not	O
involve	O
user	O
experience	O
to	O
find	O
an	O
answer	O
.	O
As	O
the	O
specifications	O
are	O
readily	O
available	O
,	O
users	O
can	O
get	O
the	O
response	O
instantly	O
.	O
This	O
paper	O
attempts	O
to	O
retrieve	O
the	O
product	O
specifications	O
that	O
would	O
answer	O
the	O
user	O
queries	O
.	O
While	O
solving	O
this	O
problem	O
,	O
our	O
key	O
contributions	O
are	O
as	O
follows	O
-	O
(	O
i	O
)	O
We	O
demonstrate	O
the	O
success	O
of	O
XL	O
-	O
Net	O
on	O
finding	O
product	O
specifications	O
that	O
can	O
help	O
answering	O
product	O
related	O
queries	O
.	O
It	O
beats	O
the	O
baseline	O
Siamese	O
method	O
by	O
0.14	O
−	O
0.31	O
points	O
in	O
HIT@1	O
.	O
(	O
ii	O
)	O
We	O
utilize	O
a	O
method	O
to	O
automatically	O
create	O
a	O
large	O
training	O
dataset	O
using	O
a	O
semisupervised	O
approach	O
,	O
that	O
was	O
used	O
to	O
fine	O
-	O
tune	O
XLNet	B-MethodName
and	O
other	O
models	O
.	O
(	O
iii	O
)	O
While	O
we	O
trained	O
on	O
Mobile	O
vertical	O
,	O
we	O
tested	O
on	O
different	O
verticals	O
,	O
namely	O
,	O
AC	O
,	O
Backpack	O
,	O
Computer	O
,	O
Shoes	O
,	O
Watches	O
,	O
which	O
show	O
promising	O
results	O
.	O

In	O
recent	O
years	O
,	O
e	O
-	O
commerce	O
product	O
question	B-TaskName
answering	I-TaskName
(	O
PQA	O
)	O
has	O
received	O
a	O
lot	O
of	O
attention	O
.	O
Yu	O
et	O
al	O
(	O
2018	O
)	O
present	O
a	O
framework	O
to	O
answer	O
product	O
related	O
questions	O
by	O
retrieving	O
a	O
ranked	O
list	O
of	O
reviews	O
and	O
they	O
employ	O
the	O
Positional	O
Language	O
Model	O
(	O
PLM	O
)	O
to	O
create	O
the	O
training	O
data	O
.	O
Chen	O
et	O
al	O
(	O
2019	O
)	O
apply	O
a	O
multi	O
-	O
task	O
attentive	O
model	O
to	O
identify	O
plausible	O
answers	O
.	O
Lai	O
et	O
al	O
(	O
2018	O
)	O
propose	O
a	O
Siamese	O
deep	O
learning	O
model	O
for	O
answering	O
questions	O
regarding	O
product	O
specifications	O
.	O
The	O
model	O
returns	O
a	O
score	O
for	O
a	O
question	O
and	O
specification	O
pair	O
.	O
McAuley	O
and	O
Yang	O
(	O
2016	O
)	O
exploit	O
product	O
reviews	O
for	O
answer	O
prediction	O
via	O
a	O
Mixture	O
of	O
Expert	O
(	O
MoE	O
)	O
model	O
.	O
This	O
MoE	O
model	O
makes	O
use	O
of	O
a	O
review	O
relevance	O
function	O
and	O
an	O
answer	O
prediction	O
function	O
.	O
It	O
assumes	O
that	O
a	O
candidate	O
answer	O
set	O
containing	O
the	O
correct	O
answers	O
is	O
available	O
for	O
answer	B-TaskName
selection	I-TaskName
.	O
Cui	O
et	O
al	O
(	O
2017	O
)	O
develop	O
a	O
chatbot	B-TaskName
for	O
e	O
-	O
commerce	O
sites	O
known	O
as	O
SuperAgent	O
.	O
SuperAgent	O
considers	O
question	O
answer	O
collections	O
,	O
reviews	O
and	O
specifications	O
when	O
answering	O
questions	O
.	O
It	O
selects	O
the	O
best	O
answer	O
from	O
multiple	O
data	O
sources	O
.	O
Language	O
representation	O
models	O
like	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
and	O
XLNet	B-MethodName
(	O
Yang	O
et	O
al	O
,	O
2019	O
)	O
are	O
pre	O
-	O
trained	O
on	O
vast	O
amounts	O
of	O
text	O
and	O
then	O
fine	O
-	O
tuned	O
on	O
task	O
-	O
specific	O
labelled	O
data	O
.	O
The	O
resulting	O
models	O
have	O
achieved	O
state	O
of	O
the	O
art	O
in	O
many	O
natural	O
language	O
processing	O
tasks	O
including	O
question	B-TaskName
answering	I-TaskName
.	O
Dzendzik	O
et	O
al	O
(	O
2019	O
)	O
employ	O
BERT	B-MethodName
to	O
answer	O
binary	O
questions	O
by	O
utilizing	O
customer	O
reviews	O
.	O
In	O
this	O
paper	O
,	O
unlike	O
some	O
of	O
the	O
previous	O
works	O
(	O
Lai	O
et	O
al	O
,	O
2018	O
;	O
Chen	O
et	O
al	O
,	O
2019	O
)	O
on	O
PQA	O
that	O
solely	O
rely	O
on	O
human	O
annotators	O
to	O
annotate	O
the	O
training	O
instances	O
,	O
we	O
propose	O
a	O
semi	O
-	O
supervised	O
method	O
to	O
label	O
training	O
data	O
.	O
We	O
leverage	O
the	O
product	O
specifications	O
to	O
answer	O
user	O
queries	O
by	O
using	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
.	O

Our	O
goal	O
is	O
to	O
train	O
a	O
classifier	O
that	O
takes	O
a	O
question	O
and	O
a	O
specification	O
as	O
input	O
(	O
e.g.	O
,	O
"	O
Color	O
Code	O
Black	O
"	O
)	O
and	O
predicts	O
whether	O
the	O
specification	O
is	O
relevant	O
to	O
the	O
question	O
.	O
We	O
take	O
Siamese	O
architecture	O
(	O
Lai	O
et	O
al	O
,	O
2018	O
)	O
as	O
our	O
baseline	O
method	O
.	O
We	O
fine	O
-	O
tune	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
for	O
this	O
classification	O
task	O
.	O
Siamese	O
:	O
We	O
train	O
a	O
100	O
-	O
dimensional	O
word2vec	O
embedding	O
on	O
the	O
whole	O
corpus	O
(	O
all	O
questions	O
and	O
specifications	O
as	O
shown	O
in	O
Table	O
1	O
.	O
)	O
to	O
get	O
the	O
input	O
word	O
representation	O
.	O
In	O
the	O
Siamese	O
model	O
,	O
the	O
question	O
and	O
specification	O
is	O
passed	O
through	O
a	O
Siamese	O
Bi	O
-	O
LSTM	B-MethodName
layer	O
.	O
Then	O
we	O
use	O
max	O
-	O
pooling	O
on	O
the	O
contextual	O
representations	O
to	O
get	O
the	O
feature	O
vectors	O
of	O
the	O
question	O
and	O
specification	O
.	O
We	O
concatenate	O
the	O
absolute	O
difference	O
and	O
hadamard	O
product	O
of	O
these	O
two	O
feature	O
vectors	O
and	O
feed	O
it	O
to	O
two	O
fully	O
connected	O
layers	O
of	O
dimension	O
50	O
and	O
25	O
,	O
subsequently	O
.	O
Finally	O
,	O
the	O
softmax	B-MethodName
layer	O
gives	O
the	O
relevance	O
score	O
.	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
:	O
The	O
architecture	O
we	O
use	O
for	O
fine	O
-	O
tuning	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
is	O
the	O
same	O
.	O
We	O
begin	O
with	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
Base	O
and	O
XLNet	B-MethodName
Base	O
model	O
.	O
To	O
adapt	O
the	O
models	O
for	O
our	O
task	O
,	O
we	O
introduce	O
a	O
fully	O
-	O
connected	O
layer	O
over	O
the	O
final	O
hidden	O
state	O
corresponding	O
to	O
the	O
[	O
CLS	O
]	O
input	O
token	O
.	O
During	O
fine	O
-	O
tuning	O
,	O
we	O
optimize	O
the	O
entire	O
model	O
end	O
-	O
to	O
-	O
end	O
,	O
with	O
the	O
additional	O
softmax	B-MethodName
classifier	O
parameters	O
W	O
R	O
K×H	O
,	O
where	O
H	O
is	O
the	O
dimen	O
-	O
sion	O
of	O
the	O
hidden	O
state	O
vectors	O
and	O
K	O
is	O
the	O
number	O
of	O
classes	O
.	O
5	O
Experimental	O
Setup	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
a	O
method	O
to	O
label	O
training	O
data	O
with	O
little	O
supervision	O
.	O
We	O
demonstrated	O
that	O
large	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
such	O
as	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
can	O
be	O
fine	O
-	O
tuned	O
successfully	O
to	O
obtain	O
product	O
specifications	O
that	O
can	O
help	O
answer	O
user	O
queries	O
.	O
We	O
also	O
achieve	O
reasonably	O
good	O
results	O
even	O
while	O
testing	O
on	O
different	O
verticals	O
.	O
We	O
would	O
like	O
to	O
extend	O
our	O
method	O
to	O
take	O
into	O
account	O
multiple	O
specifications	O
as	O
an	O
answer	O
.	O
We	O
also	O
plan	O
to	O
develop	O
a	O
classifier	O
to	O
identify	O
which	O
questions	O
can	O
not	O
be	O
answered	O
from	O
the	O
specifications	O
.	O

Generative	O
commonsense	O
reasoning	O
(	O
GCR	O
)	O
in	O
natural	O
language	O
is	O
to	O
reason	O
about	O
the	O
commonsense	O
while	O
generating	O
coherent	O
text	O
.	O
Recent	O
years	O
have	O
seen	O
a	O
surge	O
of	O
interest	O
in	O
improving	O
the	O
generation	O
quality	O
of	O
commonsense	O
reasoning	O
tasks	O
.	O
Nevertheless	O
,	O
these	O
approaches	O
have	O
seldom	O
investigated	O
diversity	O
in	O
the	O
GCR	O
tasks	O
,	O
which	O
aims	O
to	O
generate	O
alternative	O
explanations	O
for	O
a	O
real	O
-	O
world	O
situation	O
or	O
predict	O
all	O
possible	O
outcomes	O
.	O
Diversifying	O
GCR	O
is	O
challenging	O
as	O
it	O
expects	O
to	O
generate	O
multiple	O
outputs	O
that	O
are	O
not	O
only	O
semantically	O
different	O
but	O
also	O
grounded	O
in	O
commonsense	O
knowledge	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
MoKGE	O
,	O
a	O
novel	O
method	O
that	O
diversifies	O
the	O
generative	O
reasoning	O
by	O
a	O
mixture	O
of	O
expert	O
(	O
MoE	O
)	O
strategy	O
on	O
commonsense	O
knowledge	B-TaskName
graphs	I-TaskName
(	O
KG	O
)	O
.	O
A	O
set	O
of	O
knowledge	O
experts	O
seek	O
diverse	O
reasoning	O
on	O
KG	O
to	O
encourage	O
various	O
generation	O
outputs	O
.	O
Empirical	O
experiments	O
demonstrated	O
that	O
MoKGE	O
can	O
significantly	O
improve	O
the	O
diversity	O
while	O
achieving	O
on	O
par	O
performance	O
on	O
accuracy	B-MetricName
on	O
two	O
GCR	O
benchmarks	O
,	O
based	O
on	O
both	O
automatic	O
and	O
human	O
evaluations	O
.	O

Incorporating	O
external	O
knowledge	O
is	O
essential	O
for	O
many	O
NLG	O
tasks	O
to	O
augment	O
the	O
limited	O
textual	O
information	O
(	O
Yu	O
et	O
al	O
,	O
2022c	O
;	O
Dong	O
et	O
al	O
,	O
2021	O
;	O
Yu	O
et	O
al	O
,	O
2022b	O
)	O
.	O
Some	O
recent	O
work	O
explored	O
using	O
graph	O
neural	O
networks	O
(	O
GNN	O
)	O
to	O
reason	O
over	O
multihop	O
relational	O
knowledge	O
graph	O
(	O
KG	O
)	O
paths	O
(	O
Zhou	O
et	O
al	O
,	O
2018	O
;	O
Jiang	O
et	O
al	O
,	O
2019	O
;	O
Zhang	O
et	O
al	O
,	O
2020a	O
;	O
Wu	O
et	O
al	O
,	O
2020	O
;	O
Yu	O
et	O
al	O
,	O
2022a	O
;	O
Zeng	O
et	O
al	O
,	O
2021	O
)	O
.	O
For	O
example	O
,	O
Zhou	O
et	O
al	O
(	O
2018	O
)	O
enriched	O
the	O
context	O
representations	O
of	O
the	O
input	O
sequence	O
with	O
neighbouring	O
concepts	O
on	O
ConceptNet	B-DatasetName
using	O
graph	O
attention	O
.	O
Ji	O
et	O
al	O
(	O
2020	O
)	O
performed	O
dynamic	O
multi	O
-	O
hop	O
reasoning	O
on	O
multi	O
-	O
relational	O
paths	O
extracted	O
from	O
the	O
external	O
commonsense	O
KG	O
.	O
Recently	O
,	O
some	O
work	O
attempted	O
to	O
integrate	O
external	O
commonsense	O
knowledge	O
into	O
generative	O
pretrained	B-TaskName
language	I-TaskName
models	I-TaskName
(	O
Guan	O
et	O
al	O
,	O
2020	O
;	O
Bhagavatula	O
et	O
al	O
,	O
2020	O
;	O
Liu	O
et	O
al	O
,	O
2021	O
)	O
.	O
For	O
example	O
,	O
Guan	O
et	O
al	O
(	O
2020	O
)	O
conducted	O
post	O
-	O
training	O
on	O
sythetic	O
data	O
constructed	O
from	O
commonsense	O
KG	O
by	O
translating	O
triplets	O
into	O
natural	O
language	O
texts	O
using	O
templates	O
.	O
Yu	O
et	O
al	O
(	O
2022c	O
)	O
wrote	O
a	O
comprehensive	O
survey	O
for	O
more	O
detailed	O
comparisons	O
of	O
different	O
knowledge	O
graph	O
enhanced	O
NLG	O
methods	O
.	O

Problem	O
formulation	O
.	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
diversifying	O
the	O
outputs	O
of	O
generative	O
commonsense	O
reasoning	O
(	O
GCR	O
)	O
tasks	O
,	O
e.g.	O
commonsense	O
explanation	B-TaskName
generation	I-TaskName
and	O
abductive	O
commonsense	O
reasoning	O
.	O
These	O
tasks	O
require	O
one	O
-	O
to	O
-	O
many	O
generation	O
,	O
i.e.	O
,	O
creating	O
a	O
set	O
of	O
reasonable	O
outputs	O
that	O
vary	O
as	O
widely	O
as	O
possible	O
in	O
terms	O
of	O
con	O
-	O
tents	O
,	O
language	O
style	O
and	O
word	O
variability	O
.	O
Formally	O
,	O
given	O
a	O
source	O
input	O
x	O
,	O
our	O
goal	O
is	O
to	O
model	O
a	O
conditional	O
distribution	O
for	O
the	O
target	O
outputs	O
p	O
(	O
y	O
|	O
x	O
)	O
that	O
assigns	O
high	O
values	O
to	O
{	O
p	O
(	O
y	O
1	O
|	O
x	O
)	O
,	O
,	O
p	O
(	O
y	O
K	O
|	O
x	O
)	O
}	O
for	O
K	O
mappings	O
,	O
i.e.	O
,	O
{	O
x	O
y	O
1	O
,	O
,	O
x	O
y	O
K	O
}	O
.	O
Meanwhile	O
,	O
the	O
outputs	O
{	O
y	O
1	O
,	O
,	O
y	O
K	O
}	O
are	O
expected	O
to	O
be	O
diverse	O
with	O
each	O
other	O
in	O
terms	O
of	O
contents	O
.	O
Existing	O
diversity	O
-	O
promoting	O
methods	O
only	O
varied	O
the	O
language	O
styles	O
and	O
failed	O
to	O
perform	O
different	O
knowledge	O
reasoning	O
to	O
generate	O
diverse	O
contents	O
(	O
Cho	O
et	O
al	O
,	O
2019	O
;	O
Shen	O
et	O
al	O
,	O
2019	O
;	O
.	O
Here	O
,	O
incorporating	O
commonsense	O
KG	O
is	O
essential	O
for	O
the	O
generative	O
reasoning	O
(	O
GR	O
)	O
tasks	O
because	O
the	O
KG	O
can	O
not	O
only	O
augment	O
the	O
limited	O
information	O
in	O
the	O
input	O
text	O
,	O
but	O
also	O
provide	O
a	O
rich	O
searching	O
space	O
for	O
knowledge	O
reasoning	O
.	O
Therefore	O
,	O
we	O
propose	O
to	O
employ	O
commonsense	O
KG	O
to	O
play	O
the	O
central	O
role	O
of	O
performing	O
diverse	O
knowledge	O
reasoning	O
,	O
then	O
use	O
different	O
sets	O
of	O
selected	O
concepts	O
to	O
produce	O
diverse	O
outputs	O
.	O
Model	O
Outline	O
.	O
Our	O
model	O
has	O
two	O
major	O
components	O
:	O
(	O
i	O
)	O
a	O
knowledge	O
graph	O
(	O
KG	O
)	O
enhanced	O
generative	O
reasoning	O
module	O
to	O
reasonably	O
associate	O
relevant	O
concepts	O
and	O
background	O
into	O
the	O
generation	O
process	O
,	O
and	O
(	O
ii	O
)	O
a	O
mixture	O
of	O
expert	O
(	O
MoE	O
)	O
module	O
to	O
diversify	O
the	O
generation	O
process	O
and	O
produce	O
multiple	O
reasonable	O
outputs	O
.	O

To	O
model	O
the	O
relational	O
information	O
in	O
the	O
commonsen	O
KG	O
,	O
we	O
employ	O
the	O
relational	O
graph	B-MethodName
convolutional	I-MethodName
network	I-MethodName
(	O
R	O
-	O
GCN	B-MethodName
)	O
(	O
Schlichtkrull	O
et	O
al	O
,	O
2018	O
)	O
which	O
generalizes	O
GCN	B-MethodName
with	O
relation	O
specific	O
weight	O
matrices	O
.	O
We	O
follow	O
Vashishth	O
et	O
al	O
(	O
2020	O
)	O
and	O
Ji	O
et	O
al	O
(	O
2020	O
)	O
to	O
use	O
a	O
non	O
-	O
parametric	O
compositional	O
operation	O
ϕ	O
(	O
)	O
to	O
combine	O
the	O
concept	O
node	O
embedding	O
and	O
the	O
relation	O
embedding	O
.	O
Specifically	O
,	O
given	O
the	O
input	O
subgraph	O
G	O
x	O
=	O
{	O
V	O
x	O
,	O
E	O
x	O
}	O
and	O
an	O
R	O
-	O
GCN	B-MethodName
with	O
L	O
layers	O
,	O
we	O
update	O
the	O
embedding	O
of	O
each	O
node	O
v	O
V	O
x	O
at	O
the	O
(	O
l+1	O
)	O
-	O
th	O
layer	O
by	O
aggregating	O
information	O
from	O
the	O
embeddings	O
of	O
its	O
neighbours	O
in	O
N	O
(	O
v	O
)	O
at	O
the	O
l	O
-	O
th	O
layer	O
:	O
o	O
l	O
v	O
=	O
1	O
|	O
N	O
(	O
v	O
)	O
|	O
(	O
u	O
,	O
v	O
,	O
r	O
)	O
E	O
W	O
l	O
N	O
ϕ	O
(	O
h	O
l	O
u	O
,	O
h	O
l	O
r	O
)	O
,	O
(	O
1	O
)	O
h	O
l+1	O
v	O
=	O
ReLU	B-MethodName
(	O
o	O
l	O
v	O
+	O
W	O
l	O
S	O
h	O
l	O
v	O
)	O
,	O
(	O
2	O
)	O
where	O
h	O
v	O
and	O
h	O
r	O
are	O
node	O
embedding	O
and	O
relation	O
embedding	O
.	O
We	O
define	O
the	O
compositional	O
operation	O
as	O
ϕ	O
(	O
h	O
u	O
,	O
h	O
r	O
)	O
=	O
h	O
u	O
−h	O
r	O
inspired	O
by	O
the	O
TransE	B-MethodName
(	O
Bordes	O
et	O
al	O
,	O
2013	O
)	O
.	O
The	O
relation	O
embedding	O
is	O
also	O
updated	O
via	O
another	O
linear	O
transformation	O
:	O
h	O
l+1	O
r	O
=	O
W	O
l	O
R	O
h	O
l	O
r	O
.	O
(	O
3	O
)	O
Finally	O
,	O
we	O
obtain	O
concept	O
embedding	O
h	O
L	O
v	O
that	O
encodes	O
the	O
sequence	O
-	O
associated	O
subgraph	O
context	O
.	O

Commonsense	O
explanation	B-TaskName
generation	I-TaskName
.	O
It	O
aims	O
to	O
generate	O
an	O
explanation	O
given	O
a	O
counterfactual	O
statement	O
for	O
sense	O
-	O
making	O
(	O
Wang	O
et	O
al	O
,	O
2019	O
)	O
.	O
We	O
use	O
the	B-DatasetName
benchmark	I-DatasetName
dataset	O
ComVE	O
from	O
SemEval	O
-	O
2020	O
Task	O
4	O
(	O
Wang	O
et	O
al	O
,	O
2020	O
)	O
.	O
The	O
dataset	O
contains	O
10	O
,	O
000	O
/	O
997	O
/	O
1	O
,	O
000	O
examples	O
for	O
training	O
/	O
development	O
/	O
test	O
sets	O
,	O
respectively	O
.	O
The	O
average	O
input	O
/	O
output	O
length	O
is	O
7	O
.	O

We	O
note	O
that	O
as	O
we	O
targeted	O
at	O
the	O
one	O
-	O
to	O
-	O
many	O
generation	O
problem	O
,	O
we	O
excluded	O
those	O
baseline	O
methods	O
mentioned	O
in	O
the	O
related	O
work	O
that	O
can	O
not	O
produce	O
multiple	O
outputs	O
,	O
e.g.	O
,	O
Zhang	O
et	O
al	O
(	O
2020a	O
)	O
;	O
Ji	O
et	O
al	O
(	O
2020	O
)	O
;	O
Liu	O
et	O
al	O
(	O
2021	O
)	O
.	O
Different	O
from	O
aforementioned	O
methods	O
,	O
our	O
MoKGE	O
can	O
seek	O
diverse	O
reasoning	O
on	O
KG	O
to	O
encourage	O
various	O
generation	O
outputs	O
without	O
any	O
additional	O
conditions	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
work	O
to	O
explore	O
diverse	O
knowledge	O
reasoning	O
on	O
commonsense	O
KG	O
to	O
generate	O
multiple	O
diverse	O
output	O
sequences	O
.	O
Therefore	O
,	O
we	O
only	O
compared	O
our	O
MoKGE	O
with	O
existing	O
diversity	O
-	O
promoting	O
baselines	O
without	O
using	O
knowledge	O
graph	O
.	O
VAE	B-MethodName
-	O
based	O
method	O
.	O
The	O
variational	O
auto	O
-	O
encoder	O
(	O
VAE	B-MethodName
)	O
(	O
Kingma	O
and	O
Welling	O
,	O
2014	O
)	O
is	O
a	O
deep	O
generative	O
latent	O
variable	O
model	O
.	O
VAE	B-MethodName
-	O
based	O
methods	O
produce	O
diverse	O
outputs	O
by	O
sampling	O
different	O
latent	O
variables	O
from	O
an	O
approximate	O
posterior	O
distribution	O
.	O
CVAE	B-MethodName
-	O
SVG	O
(	O
SVG	O
is	O
short	O
for	O
sentence	O
variant	O
generation	O
)	O
(	O
Gupta	O
et	O
al	O
,	O
2018	O
)	O
is	O
a	O
conditional	O
VAE	B-MethodName
model	O
that	O
can	O
produce	O
multiple	O
outputs	O
based	O
an	O
original	O
sentence	O
as	O
input	O
.	O
MoE	O
-	O
based	O
method	O
.	O
Mixture	O
models	O
provide	O
an	O
alternative	O
approach	O
to	O
generate	O
diverse	O
outputs	O
by	O
sampling	O
different	O
mixture	O
components	O
.	O
We	O
compare	O
against	O
two	O
mixture	O
of	O
experts	O
(	O
MoE	O
)	O
implementations	O
by	O
Shen	O
et	O
al	O
(	O
2019	O
)	O
and	O
Cho	O
et	O
al	O
(	O
2019	O
)	O
.	O
We	O
refer	O
them	O
as	O
MoE	O
-	O
prompt	O
(	O
Shen	O
et	O
al	O
,	O
2019	O
)	O
and	O
MoE	O
-	O
embed	O
(	O
Cho	O
et	O
al	O
,	O
2019	O
)	O
.	O
Sampling	O
-	O
based	O
method	O
.	O
Sampling	O
methods	O
create	O
diverse	O
outputs	O
by	O
sampling	O
next	O
token	O
widely	O
from	O
the	O
vocabulary	O
.	O
We	O
compare	O
against	O
two	O
sampling	O
algorithms	O
for	O
decoding	O
,	O
including	O
truncated	O
sampling	O
(	O
Fan	O
et	O
al	O
,	O
2018	O
)	O
and	O
nucleus	O
sampling	O
.	O
Truncated	O
sampling	O
(	O
Fan	O
et	O
al	O
,	O
2018	O
)	O
randomly	O
samples	O
words	O
from	O
top	O
-	O
k	O
probability	O
candidates	O
of	O
the	O
predicted	O
distribution	O
at	O
each	O
decoding	O
step	O
.	O
Nucleus	O
sampling	O
avoids	O
text	O
degeneration	O
by	O
truncating	O
the	O
unreliable	O
tails	O
and	O
sampling	O
from	O
the	O
dynamic	O
nucleus	O
of	O
tokens	O
containing	O
the	O
vast	O
majority	O
of	O
the	O
probability	O
mass	O
.	O

BERT	B-MethodName
-	O
based	O
distractor	B-TaskName
generation	I-TaskName
for	O
Swedish	O
reading	B-TaskName
comprehension	I-TaskName
questions	O
using	O
a	O
small	O
-	O
scale	O
dataset	O

An	O
important	O
part	O
when	O
constructing	O
multiplechoice	O
questions	O
(	O
MCQs	O
)	O
for	O
reading	B-TaskName
comprehension	I-TaskName
assessment	O
are	O
the	O
distractors	O
,	O
the	O
incorrect	O
but	O
preferably	O
plausible	O
answer	O
options	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
new	O
BERTbased	O
method	O
for	O
automatically	O
generating	O
distractors	O
using	O
only	O
a	O
small	O
-	O
scale	O
dataset	O
.	O
We	O
also	O
release	O
a	O
new	O
such	O
dataset	O
of	O
Swedish	O
MCQs	O
(	O
used	O
for	O
training	O
the	O
model	O
)	O
,	O
and	O
propose	O
a	O
methodology	O
for	O
assessing	O
the	O
generated	O
distractors	O
.	O
Evaluation	O
shows	O
that	O
from	O
a	O
student	O
's	O
perspective	O
,	O
our	O
method	O
generated	O
one	O
or	O
more	O
plausible	O
distractors	O
for	O
more	O
than	O
50	O
%	O
of	O
the	O
MCQs	O
in	O
our	O
test	O
set	O
.	O
From	O
a	O
teacher	O
's	O
perspective	O
,	O
about	O
50	O
%	O
of	O
the	O
generated	O
distractors	O
were	O
deemed	O
appropriate	O
.	O
We	O
also	O
do	O
a	O
thorough	O
analysis	O
of	O
the	O
results	O
.	O

As	O
mentioned	O
previously	O
,	O
plausible	O
distractors	O
should	O
be	O
grammatically	O
consistent	O
with	O
the	O
key	O
.	O
Hence	O
,	O
a	O
metric	O
measuring	O
grammatical	O
consistency	O
would	O
be	O
useful	O
both	O
for	O
quantitative	O
evaluation	O
and	O
as	O
a	O
basis	O
for	O
a	O
baseline	O
method	O
.	O
We	O
propose	O
to	O
use	O
convolution	B-MethodName
partial	O
tree	O
kernels	O
(	O
CPTK	O
)	O
for	O
these	O
purposes	O
.	O
CPTK	O
were	O
proposed	O
by	O
Moschitti	O
(	O
2006	O
)	O
for	O
dependency	O
trees	O
and	O
essentially	O
calculate	O
the	O
number	O
of	O
common	O
tree	O
structures	O
(	O
not	O
only	O
full	O
subtrees	O
)	O
between	O
two	O
given	O
trees	O
.	O
However	O
,	O
CPTKs	O
can	O
not	O
handle	O
labeled	O
edges	O
and	O
were	O
applied	O
to	O
dependency	O
trees	O
containing	O
only	O
lexicals	O
.	O
Another	O
solution	O
,	O
proposed	O
by	O
Croce	O
et	O
al	O
(	O
2011	O
)	O
and	O
used	O
in	O
this	O
article	O
,	O
is	O
to	O
include	O
edge	O
labels	O
,	O
i.e.	O
,	O
grammatical	O
relations	O
(	O
GR	O
)	O
,	O
as	O
separate	O
nodes	O
.	O
A	O
resulting	O
computational	O
structure	O
is	O
Grammatical	O
Relation	O
Centered	O
Tree	O
(	O
GRCT	O
)	O
,	O
which	O
transforms	O
the	O
original	O
dependency	O
tree	O
by	O
making	O
each	O
PoS	O
-	O
tag	O
a	O
child	O
of	O
a	O
GR	O
node	O
and	O
a	O
father	O
of	O
a	O
lexical	O
node	O
.	O
CPTKs	O
can	O
take	O
any	O
non	O
-	O
negative	O
values	O
and	O
are	O
thus	O
hard	O
to	O
interpret	O
.	O
Hence	O
,	O
we	O
use	O
normalized	O
CPTK	O
(	O
NCPTK	O
)	O
shown	O
in	O
Equation	O
(	O
1	O
)	O
,	O
where	O
K	O
(	O
T	O
1	O
,	O
T	O
2	O
)	O
is	O
the	O
CPTK	O
applied	O
to	O
the	O
dependency	O
trees	O
T	O
1	O
and	O
T	O
2	O
.	O
K	O
(	O
T	O
1	O
,	O
T	O
2	O
)	O
=	O
K	O
(	O
T	O
1	O
,	O
T	O
2	O
)	O
K	O
(	O
T	O
1	O
,	O
T	O
1	O
)	O
K	O
(	O
T	O
2	O
,	O
T	O
2	O
)	O
,	O
(	O
1	O
)	O
Evidently	O
,	O
when	O
T	O
1	O
and	O
T	O
2	O
are	O
the	O
same	O
,	O
K	O
(	O
T	O
1	O
,	O
T	O
2	O
)	O
equals	O
to	O
1	O
,	O
which	O
is	O
the	O
highest	O
value	O
it	O
can	O
take	O
.	O

As	O
mentioned	O
in	O
Section	O
2.2	O
,	O
NCPTK	O
measures	O
grammatical	O
consistency	O
between	O
the	O
key	O
and	O
a	O
distractor	O
.	O
Our	O
baseline	O
uses	O
NCPTK	O
on	O
Universal	B-DatasetName
Dependencies	I-DatasetName
(	O
UD	B-DatasetName
)	O
trees	O
(	O
Nivre	O
et	O
al	O
,	O
2020	O
)	O
in	O
the	O
following	O
way	O
.	O
For	O
each	O
given	O
MCQ	O
,	O
we	O
exclude	O
the	O
sentence	O
containing	O
the	O
key	O
from	O
the	O
base	O
text	O
and	O
then	O
parse	O
each	O
remaining	O
sentence	O
s	O
i	O
of	O
the	O
text	O
,	O
and	O
the	O
key	O
using	O
the	O
UD	B-DatasetName
parser	O
for	O
Swedish	O
.	O
Let	O
T	O
s	O
i	O
and	O
T	O
k	O
denote	O
a	O
dependency	O
tree	O
corresponding	O
to	O
s	O
i	O
and	O
the	O
key	O
respectively	O
.	O
For	O
each	O
T	O
s	O
i	O
,	O
we	O
find	O
all	O
subtrees	O
with	O
the	O
root	O
having	O
the	O
same	O
universal	O
PoS	O
-	O
tag	O
and	O
the	O
same	O
universal	O
features	O
(	O
representing	O
morphological	O
properties	O
of	O
the	O
token	O
)	O
as	O
the	O
root	O
of	O
T	O
k	O
.	O
If	O
no	O
subtrees	O
are	O
found	O
,	O
no	O
distractors	O
can	O
be	O
suggested	O
for	O
this	O
MCQ	O
.	O
Otherwise	O
,	O
we	O
calculate	O
NCPTK	O
between	O
each	O
found	O
subtree	O
and	O
T	O
k	O
(	O
both	O
as	O
GRCT	O
,	O
but	O
without	O
lexicals	O
)	O
.	O
Then	O
we	O
take	O
the	O
textual	O
representation	O
of	O
the	O
K	O
subtrees	O
with	O
the	O
highest	O
NCPTK	O
as	O
the	O
distractor	O
suggestions	O
.	O
With	O
these	O
settings	O
,	O
training	O
took	O
about	O
3.67h	O
for	O
the	O
left	O
-	O
to	O
-	O
right	O
and	O
3h	O
for	O
the	O
u	O
-	O
PMLM	B-MethodName
variant	O
.	O
UD	B-DatasetName
trees	O
for	O
the	O
baseline	O
were	O
obtained	O
using	O
Stanza	O
package	O
(	O
Qi	O
et	O
al	O
,	O
2020	O
)	O
and	O
convolution	B-MethodName
partial	O
tree	O
kernels	O
on	O
the	O
UD	B-DatasetName
trees	O
were	O
calculated	O
using	O
UDon2	O
library	O
(	O
Kalpakchi	O
and	O
Boye	O
,	O
2020	O
)	O
.	O
Baseline	O
requires	O
no	O
training	O
and	O
running	O
our	O
implementation	O
of	O
the	O
baseline	O
takes	O
about	O
a	O
minute	O
on	O
the	O
development	O
or	O
test	O
set	O
.	O

Following	O
the	O
analysis	O
of	O
Rodriguez	O
(	O
2005	O
)	O
,	O
we	O
generate	O
three	O
distractors	O
per	O
MCQ	O
for	O
each	O
model	O
.	O
Due	O
to	O
prohibitively	O
high	O
costs	O
of	O
human	O
evaluation	O
,	O
we	O
have	O
divided	O
the	O
evaluation	O
process	O
into	O
two	O
stages	O
.	O
The	O
first	O
stage	O
is	O
quantitative	O
evaluation	O
,	O
which	O
gives	O
limited	O
information	O
about	O
the	O
model	O
's	O
quality	O
,	O
but	O
is	O
sufficient	O
for	O
model	B-TaskName
selection	I-TaskName
.	O
The	O
second	O
stage	O
is	O
human	O
evaluation	O
of	O
the	O
best	O
model	O
,	O
selected	O
during	O
the	O
first	O
stage	O
.	O

Baseline	O
When	O
using	O
u	O
-	O
PMLM	B-MethodName
,	O
shortest	O
distractors	O
were	O
generated	O
first	O
.	O
↑	O
(	O
↓	O
)	O
means	O
"	O
the	O
higher	O
(	O
lower	O
)	O
,	O
the	O
better	O
"	O
.	O
In	O
contrast	O
,	O
u	O
-	O
PMLM	B-MethodName
needs	O
the	O
lengths	O
of	O
the	O
distractors	O
to	O
be	O
decided	O
in	O
beforehand	O
,	O
which	O
we	O
set	O
to	O
be	O
the	O
lengths	O
of	O
the	O
two	O
reference	O
distractors	O
and	O
the	O
length	O
of	O
the	O
key	O
4	O
.	O
Surprisingly	O
,	O
the	O
order	O
of	O
distractors	O
in	O
terms	O
of	O
their	O
length	O
also	O
matters	O
for	O
generation	O
with	O
u	O
-	O
PMLM	B-MethodName
,	O
so	O
we	O
have	O
tested	O
three	O
options	O
:	O
shortest	O
first	O
,	O
longest	O
first	O
and	O
random	O
order	O
.	O
According	O
to	O
the	O
results	O
of	O
model	B-TaskName
selection	I-TaskName
on	O
the	O
development	O
set	O
(	O
presented	O
in	O
detail	O
in	O
Appendix	O
C	O
)	O
,	O
u	O
-	O
PMLM	B-MethodName
models	O
outperformed	O
left	O
-	O
to	O
-	O
right	O
models	O
by	O
a	O
substantial	O
margin	O
.	O
The	O
best	O
u	O
-	O
PMLM	B-MethodName
model	O
(	O
generating	O
shortest	O
distractors	O
first	O
)	O
and	O
the	O
baseline	O
have	O
been	O
evaluated	O
on	O
the	O
test	O
set	O
(	O
see	O
Table	O
3	O
)	O
.	O
Interestingly	O
,	O
the	O
similarity	O
of	O
syntactic	O
structures	O
between	O
the	O
key	O
and	O
distractors	O
(	O
assessed	O
by	O
NCPTK	O
)	O
is	O
the	O
same	O
for	O
both	O
baseline	O
(	O
that	O
actually	O
relies	O
on	O
NCPTK	O
)	O
and	O
u	O
-	O
PMLM	B-MethodName
.	O
At	O
the	O
same	O
time	O
,	O
u	O
-	O
PMLM	B-MethodName
generates	O
more	O
distractors	O
matching	O
the	O
reference	O
ones	O
compared	O
to	O
the	O
baseline	O
(	O
as	O
seen	O
from	O
DisRecall	O
and	O
AnyDisRefMatch	O
)	O
.	O
The	O
baseline	O
generates	O
at	O
least	O
one	O
empty	O
string	O
as	O
a	O
distractor	O
11.76	O
%	O
of	O
the	O
time	O
(	O
compared	O
to	O
no	O
such	O
cases	O
for	O
u	O
-	O
PMLM	B-MethodName
)	O
limiting	O
possibilities	O
of	O
using	O
the	O
baseline	O
in	O
the	O
real	O
-	O
life	O
applications	O
.	O

We	O
have	O
used	O
distractors	O
generated	O
on	O
the	O
test	O
set	O
by	O
the	O
best	O
u	O
-	O
PMLM	B-MethodName
model	O
(	O
selected	O
after	O
quantitative	O
evaluation	O
in	O
Section	O
6.1	O
)	O
to	O
conduct	O
human	O
evaluation	O
in	O
2	O
stages	O
:	O
from	O
a	O
perspective	O
of	O
a	O
student	O
and	O
a	O
teacher	O
.	O

We	O
have	O
collected	O
SweQUAD	O
-	O
MC	O
,	O
the	O
first	O
dataset	O
of	O
Swedish	O
MCQs	O
,	O
and	O
showed	O
the	O
possibility	O
of	O
training	O
usable	O
BERT	B-MethodName
-	O
based	O
DG	O
models	O
,	O
despite	O
the	O
small	O
scale	O
of	O
the	O
dataset	O
.	O
We	O
have	O
showed	O
that	O
a	O
u	O
-	O
PMLM	B-MethodName
variant	O
of	O
the	O
BERT	B-MethodName
-	O
based	O
DG	O
model	O
performs	O
best	O
on	O
the	O
dataset	O
,	O
and	O
proposed	O
a	O
novel	O
methodology	O
of	O
evaluating	O
the	O
plausibility	O
of	O
generated	O
distractors	O
.	O
Around	O
half	O
of	O
the	O
generated	O
distractors	O
were	O
found	O
acceptable	O
by	O
the	O
majority	O
of	O
teachers	O
,	O
and	O
more	O
than	O
50	O
%	O
of	O
MCQs	O
had	O
at	O
least	O
one	O
plausible	O
generated	O
distractor	O
,	O
judging	O
by	O
the	O
entropy	O
of	O
students	O
'	O
responses	O
.	O
Bearing	O
in	O
mind	O
that	O
the	O
aim	O
of	O
the	O
proposed	O
method	O
is	O
to	O
support	O
(	O
not	O
replace	O
)	O
teachers	O
,	O
we	O
deem	O
that	O
our	O
method	O
works	O
well	O
for	O
MCQs	O
in	O
Swedish	O
(	O
and	O
potentially	O
in	O
other	O
languages	O
with	O
a	O
pretrained	O
BERT	B-MethodName
and	O
a	O
dataset	O
of	O
a	O
similar	O
scale	O
)	O
.	O
Furthermore	O
,	O
we	O
have	O
presented	O
a	O
baseline	O
applicable	O
to	O
any	O
language	O
with	O
a	O
UD	B-DatasetName
treebank	O
(	O
currently	O
about	O
100	O
languages	O
)	O
.	O
Although	O
its	O
performance	O
is	O
nowhere	O
near	O
the	O
u	O
-	O
PMLM	B-MethodName
variant	O
,	O
we	O
believe	O
that	O
it	O
can	O
serve	O
as	O
a	O
good	O
point	O
of	O
comparison	O
to	O
emerging	O
neural	O
methods	O
for	O
other	O
languages	O
.	O
System	O
Demonstrations	O
,	O
pages	O
38	O
-	O
45	O
,	O
Online	O
.	O
Association	O
for	O
Computational	O
Linguistics	O
.	O

We	O
have	O
trained	O
both	O
left	O
-	O
to	O
-	O
right	O
and	O
u	O
-	O
PMLM	B-MethodName
variants	O
for	O
6	O
epochs	O
(	O
fixing	O
a	O
random	O
seed	O
for	O
u	O
-	O
PMLM	B-MethodName
masking	O
procedure	O
to	O
42	O
)	O
.	O
The	O
quantitative	O
performance	O
metrics	O
on	O
the	O
development	O
set	O
for	O
the	O
top	O
-	O
3	O
models	O
for	O
each	O
variant	O
are	O
presented	O
in	O
Table	O
5	O
.	O
The	O
best	O
u	O
-	O
PMLM	B-MethodName
model	O
(	O
i	O
-	O
14000	O
)	O
outperformed	O
the	O
best	O
left	O
-	O
to	O
-	O
right	O
model	O
(	O
i	O
-	O
18000	O
)	O
on	O
most	O
of	O
the	O
quantitative	O
metrics	O
.	O
The	O
next	O
experiment	O
concerned	O
the	O
order	O
in	O
which	O
distractors	O
are	O
generated	O
,	O
which	O
we	O
tested	O
only	O
for	O
the	O
best	O
u	O
-	O
PMLM	B-MethodName
model	O
.	O
We	O
tried	O
generating	O
shortest	O
distractors	O
first	O
(	O
SF	O
)	O
,	O
longest	O
first	O
(	O
LF	O
)	O
or	O
in	O
a	O
random	O
order	O
with	O
a	O
fixed	O
seed	O
of	O
42	O
(	O
RND	O
)	O
.	O
The	O
results	O
of	O
the	O
experiment	O
are	O
presented	O
in	O
Table	O
6	O
.	O
Evidently	O
,	O
models	O
with	O
SF	O
-	O
generation	O
consistently	O
outperform	O
ones	O
with	O
LF	O
-	O
generation	O
.	O
SF	O
-	O
generation	O
also	O
performs	O
on	O
-	O
par	O
or	O
better	O
than	O
RND	O
-	O
generation	O
.	O
However	O
,	O
fixing	O
a	O
seed	O
is	O
not	O
a	O
generalizable	O
solution	O
,	O
which	O
is	O
why	O
we	O
opted	O
for	O
SF	O
-	O
generation	O
.	O

Evaluation	O
from	O
the	O
student	O
's	O
perspective	O
has	O
been	O
conducted	O
on	O
the	O
Prolific	O
platform	O
7	O
.	O
We	O
used	O
Prolific	O
's	O
pre	O
-	O
screening	O
feature	O
and	O
required	O
each	O
subject	O
to	O
have	O
Swedish	O
as	O
the	O
first	O
language	O
and	O
hold	O
at	O
least	O
a	O
high	O
school	O
diploma	O
(	O
A	O
-	O
levels	O
)	O
.	O
Descriptive	O
statistics	O
about	O
the	O
recruited	O
sample	O
of	O
subjects	O
Imagine	O
that	O
you	O
are	O
a	O
teacher	O
checking	O
reading	B-TaskName
comprehension	I-TaskName
skills	O
of	O
your	O
students	O
.	O
Given	O
a	O
text	O
,	O
your	O
task	O
is	O
to	O
create	O
one	O
or	O
more	O
multiple	O
choice	O
questions	O
based	O
on	O
the	O
text	O
,	O
i.e.	O
:	O
1	O
.	O
formulate	O
a	O
question	O
with	O
the	O
correct	O
answer	O
in	O
the	O
text	O
;	O
2	O
.	O
mark	O
the	O
correct	O
answer	O
in	O
the	O
text	O
;	O
3	O
.	O
mark	O
some	O
wrong	O
,	O
but	O
plausible	O
options	O
in	O
the	O
text	O
.	O
When	O
you	O
have	O
written	O
your	O
questions	O
,	O
marked	O
the	O
correct	O
answer	O
(	O
CA	O
)	O
and	O
the	O
wrong	O
alternatives	O
in	O
the	O
text	O
,	O
click	O
on	O
"	O
Submit	O
"	O
.	O
When	O
you	O
formulate	O
the	O
question	O
,	O
think	O
about	O
the	O
following	O
aspects	O
.	O
The	O
question	O
must	O
be	O
independent	O
,	O
i.e.	O
,	O
one	O
should	O
not	O
require	O
additional	O
information	O
(	O
on	O
top	O
of	O
the	O
given	O
text	O
)	O
to	O
be	O
able	O
to	O
answer	O
the	O
question	O
.	O
The	O
question	O
should	O
be	O
unambiguous	O
and	O
have	O
only	O
one	O
possible	O
interpretation	O
.	O
One	O
should	O
not	O
be	O
able	O
to	O
answer	O
your	O
question	O
without	O
reading	O
the	O
text	O
,	O
which	O
is	O
why	O
even	O
wrong	O
alternatives	O
should	O
be	O
plausible	O
.	O
Wrong	O
options	O
must	O
be	O
in	O
the	O
same	O
grammatical	O
form	O
as	O
the	O
CA	O
.	O
For	O
instance	O
,	O
if	O
the	O
CA	O
begins	O
with	O
a	O
verb	O
in	O
Past	O
Simple	O
,	O
all	O
wrong	O
options	O
must	O
begin	O
with	O
a	O
verb	O
in	O
Past	O
Simple	O
.	O
Find	O
as	O
many	O
questions	O
as	O
you	O
can	O
(	O
+	O
the	O
correct	O
answer	O
and	O
wrong	O
alternatives	O
)	O
on	O
each	O
text	O
and	O
then	O
get	O
a	O
new	O
text	O
when	O
you	O
ca	O
n't	O
find	O
more	O
.	O
Figure	O
6	O
:	O
An	O
English	O
translation	O
of	O
the	O
original	O
instructions	O
for	O
SweQUAD	O
-	O
MC	O
data	O
collection	O
(	O
the	O
original	O
instructions	O
in	O
Swedish	O
can	O
be	O
found	O
in	O
the	O
GitHub	O
repository	O
)	O
Metric	O
left	O
-	O
to	O
-	O
right	O
u	O
-	O
PMLM	B-MethodName
i	O
-	O
10000	O
i	O
-	O
14000	O
i	O
-	O
18000	O
i	O
-	O
10000	O
i	O
-	O
14000	O
i	O
-	O
16000	O
e	O
-	O
3	O
.	O
Thank	O
you	O
for	O
participating	O
in	O
our	O
study	O
!	O
You	O
will	O
be	O
presented	O
with	O
a	O
number	O
of	O
multiple	O
choice	O
questions	O
.	O
Your	O
task	O
is	O
to	O
answer	O
as	O
many	O
of	O
these	O
questions	O
correctly	O
as	O
possible	O
.	O
If	O
you	O
do	O
n't	O
know	O
which	O
alternative	O
is	O
correct	O
,	O
choose	O
the	O
one	O
that	O
seems	O
the	O
most	O
plausible	O
.	O
You	O
are	O
allowed	O
to	O
use	O
ONLY	O
your	O
own	O
prior	O
knowledge	O
and	O
common	O
sense	O
.	O
Please	O
,	O
do	O
NOT	O
consult	O
any	O
other	O
external	O
sources	O
of	O
information	O
.	O
Figure	O
8	O
:	O
An	O
English	O
translation	O
of	O
the	O
original	O
instructions	O
given	O
to	O
subjects	O
on	O
the	O
Prolific	O
platform	O
(	O
the	O
original	O
instructions	O
in	O
Swedish	O
can	O
be	O
found	O
in	O
the	O
GitHub	O
repository	O
)	O
is	O
presented	O
in	O
Figure	O
7	O
.	O
The	O
exact	O
guidelines	O
given	O
to	O
the	O
subjects	O
(	O
and	O
their	O
translation	O
to	O
English	O
)	O
are	O
presented	O
in	O
Figure	O
8	O
.	O
MCQs	O
were	O
presented	O
in	O
a	O
random	O
order	O
,	O
but	O
the	O
order	O
of	O
options	O
for	O
each	O
MCQs	O
was	O
the	O
same	O
for	O
each	O
subject	O
.	O

The	O
observations	O
in	O
the	O
sample	O
should	O
be	O
independent	O
.	O
Subjects	O
have	O
performed	O
the	O
task	O
independently	O
of	O
each	O
other	O
through	O
a	O
Prolific	O
platform	O
,	O
hence	O
the	O
observations	O
are	O
independent	O
.	O
3	O
.	O
The	O
variable	O
under	O
study	O
should	O
be	O
approximately	O
normally	O
distributed	O
.	O
The	O
distribution	O
of	O
the	O
number	O
of	O
correctly	O
answered	O
MCQs	O
is	O
presented	O
in	O
Figure	O
7	O
(	O
the	O
plot	O
in	O
the	O
last	O
row	O
and	O
the	O
last	O
column	O
with	O
the	O
title	O
"	O
num	O
correct	O
"	O
)	O
.	O
The	O
distribution	O
is	O
indeed	O
approximately	O
normal	O
.	O
4	O
.	O
The	O
variable	O
under	O
study	O
should	O
have	O
no	O
extreme	O
outliers	O
.	O
Outliers	O
are	O
typically	O
defined	O
in	O
terms	O
of	O
the	O
interquartile	O
range	O
(	O
IQR	B-DatasetName
)	O
,	O
which	O
equals	O
to	O
Q3	O
-	O
Q1	O
.	O
The	O
datapoints	O
outside	O
1.5IQR	O
are	O
deemed	O
mild	O
outliers	O
,	O
whereas	O
those	O
outside	O
3IQR	O
are	O
considered	O
extreme	O
outliers	O
.	O
Boxplots	O
for	O
our	O
data	O
with	O
whiskers	O
within	O
both	O
1.5IQR	O
and	O
3IQR	O
are	O
presented	O
in	O
Figure	O
9	O
.	O
Two	O
datapoints	O
can	O
be	O
considered	O
mild	O
outliers	O
,	O
but	O
no	O
extreme	O
outliers	O
are	O
present	O
,	O
which	O
means	O
this	O
assumption	O
for	O
the	O
one	O
sample	O
t	O
-	O
test	O
is	O
not	O
violated	O
.	O

To	O
get	O
a	O
comprehensive	O
overview	O
of	O
methods	O
for	O
generating	O
distractors	O
for	O
MCQs	O
,	O
we	O
employed	O
a	O
two	O
-	O
step	O
process	O
.	O
The	O
first	O
step	O
was	O
to	O
issue	O
queries	O
"	O
distractor	B-TaskName
generation	I-TaskName
"	O
and	O
"	O
multiple	O
choice	O
question	B-TaskName
generation	I-TaskName
"	O
to	O
ACL	O
Anthology	O
and	O
Google	B-DatasetName
Scholar	O
.	O
The	O
result	O
was	O
20	O
articles	O
from	O
ACL	O
Anthology	O
and	O
4	O
additional	O
ones	O
from	O
Google	B-DatasetName
Scholar	O
.	O
The	O
second	O
step	O
was	O
to	O
select	O
relevant	O
references	O
from	O
the	O
"	O
Related	O
work	O
"	O
sections	O
of	O
these	O
articles	O
.	O
This	O
resulted	O
into	O
15	O
additional	O
articles	O
.	O
Out	O
of	O
found	O
39	O
articles	O
,	O
11	O
were	O
filtered	O
out	O
(	O
8	O
focused	O
only	O
on	O
generating	O
questions	O
,	O
1	O
relied	O
mostly	O
on	O
expert	O
knowledge	O
,	O
1	O
on	O
the	O
auxiliary	O
relation	B-TaskName
extraction	I-TaskName
task	O
and	O
1	O
was	O
a	O
demo	O
paper	O
)	O
,	O
leaving	O
28	O
articles	O
in	O
total	O
.	O
Only	O
2	O
of	O
these	O
28	O
papers	O
worked	O
with	O
a	O
language	O
other	O
than	O
English	O
(	O
Chinese	O
and	O
Basque	O
)	O
.	O

To	O
evaluate	O
and	O
compare	O
our	O
models	O
,	O
we	O
use	O
the	O
Spanish	O
-	O
English	O
(	O
SPA	O
-	O
EN	O
)	O
part	O
of	O
the	O
LinCE	B-DatasetName
benchmark	O
(	O
a	O
total	O
of	O
32	O
,	O
651	O
posts	O
equivalent	O
to	O
390	O
,	O
953	O
tokens	O
)	O
(	O
Aguilar	O
et	O
al	O
,	O
2020	O
)	O
.	O
We	O
chose	O
this	O
language	O
pair	O
because	O
it	O
has	O
the	O
challenge	O
of	O
increased	O
similarity	O
between	O
the	O
languages	O
(	O
Tristram	O
,	O
1999	O
)	O
.	O
In	O
the	O
original	O
data	O
,	O
8	O
labels	O
are	O
used	O
,	O
from	O
which	O
we	O
only	O
focus	O
on	O
the	O
3	O
labels	O
necessary	O
for	O
the	O
language	B-TaskName
identification	I-TaskName
task	O
:	O
lang1	O
,	O
lang2	O
and	O
other	O
,	O
for	O
English	O
,	O
Spanish	O
and	O
punctuation	O
,	O
numbers	O
,	O
symbols	O
and	O
emoticons	O
,	O
respectively	O
.	O
We	O
use	O
the	O
default	O
development	O
and	O
test	O
splits	O
for	O
our	O
experiments	O
.	O

Generally	O
,	O
Latent	O
Dirichlet	O
Allocation	O
(	O
LDA	B-MethodName
)	O
aims	O
to	O
find	O
the	O
topics	O
a	O
document	O
belongs	O
to	O
using	O
the	O
words	O
in	O
the	O
document	O
as	O
features	O
.	O
In	O
our	O
case	O
,	O
the	O
documents	O
are	O
the	O
words	O
,	O
the	O
features	O
are	O
character	O
n	O
-	O
grams	O
(	O
with	O
n	O
1	O
to	O
5	O
)	O
and	O
the	O
topics	O
are	O
English	O
and	O
Spanish	O
.	O
The	O
LDA	B-MethodName
algorithm	O
does	O
not	O
output	O
labels	O
for	O
the	O
resulting	O
clusters	O
,	O
so	O
we	O
select	O
the	O
top	O
10	O
words	O
based	O
on	O
weight	O
that	O
represent	O
best	O
each	O
cluster	O
,	O
and	O
assign	O
them	O
a	O
language	O
using	O
the	O
word	O
uni	O
-	O
gram	O
method	O
(	O
Section	O
3.1	O
)	O
.	O
We	O
use	O
the	O
Scikit	O
Learn	O
6	O
implementation	O
of	O
LDA	B-MethodName
with	O
the	O
TfidfVectorizer	O
and	O
use	O
only	O
the	O
first	O
100	O
,	O
000	O
words	O
from	O
each	O
monolingual	O
dataset	O
,	O
in	O
order	O
to	O
reduce	O
training	O
time	O
.	O

For	O
our	O
Support	B-MethodName
Vector	I-MethodName
Machine	I-MethodName
(	O
SVM	B-MethodName
)	O
model	O
,	O
we	O
consider	O
the	O
monolingual	O
data	O
(	O
Section	O
2.2	O
)	O
to	O
be	O
the	O
gold	O
training	O
data	O
,	O
without	O
tokens	O
from	O
the	O
other	O
class	O
.	O
Using	O
TfidfVectorizer	O
,	O
we	O
extract	O
character	O
n	O
-	O
gram	O
features	O
from	O
each	O
word	O
,	O
with	O
n	O
1	O
to	O
5	O
.	O
We	O
use	O
the	O
Scikit	O
Learn	O
implementation	O
with	O
all	O
default	O
parameters	O
and	O
select	O
the	O
first	O
100	O
,	O
000	O
words	O
from	O
each	O
dataset	O
.	O

We	O
use	O
Logistic	B-MethodName
Regression	I-MethodName
in	O
a	O
weakly	O
-	O
supervised	O
manner	O
,	O
the	O
same	O
as	O
with	O
SVM	B-MethodName
,	O
where	O
we	O
consider	O
the	O
first	O
100	O
,	O
000	O
words	O
from	O
each	O
Wikipedia	O
dataset	O
to	O
be	O
the	O
gold	O
training	O
data	O
.	O
Again	O
,	O
we	O
use	O
TfidfVectorizer	O
to	O
extract	O
character	O
n	O
-	O
gram	O
features	O
,	O
with	O
n	O
1	O
to	O
5	O
,	O
and	O
rely	O
on	O
the	O
default	O
Scikit	O
Learn	O
implementation	O
.	O

Cross	O
-	O
Sentence	O
N	O
-	O
ary	O
Relation	B-TaskName
Extraction	I-TaskName
using	O
Lower	O
-	O
Arity	O
Universal	O
Schemas	O

Most	O
existing	O
relation	B-TaskName
extraction	I-TaskName
approaches	O
exclusively	O
target	O
binary	O
relations	O
,	O
and	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
is	O
relatively	O
unexplored	O
.	O
Current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
method	O
is	O
based	O
on	O
a	O
supervised	O
learning	O
approach	O
and	O
,	O
therefore	O
,	O
may	O
suffer	O
from	O
the	O
lack	O
of	O
sufficient	O
relation	O
labels	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
approach	O
to	O
cross	O
-	O
sentence	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
based	O
on	O
universal	O
schemas	O
.	O
To	O
alleviate	O
the	O
sparsity	O
problem	O
and	O
to	O
leverage	O
inherent	O
decomposability	O
of	O
n	O
-	O
ary	O
relations	O
,	O
we	O
propose	O
to	O
learn	O
relation	O
representations	O
of	O
lower	O
-	O
arity	O
facts	O
that	O
result	O
from	O
decomposing	O
higher	O
-	O
arity	O
facts	O
.	O
The	O
proposed	O
method	O
computes	O
a	O
score	O
of	O
a	O
new	O
nary	O
fact	O
by	O
aggregating	O
scores	O
of	O
its	O
decomposed	O
lower	O
-	O
arity	O
facts	O
.	O
We	O
conduct	O
experiments	O
with	O
datasets	O
for	O
ternary	O
relation	B-TaskName
extraction	I-TaskName
and	O
empirically	O
show	O
that	O
our	O
method	O
improves	O
the	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
performance	O
compared	O
to	O
previous	O
methods	O
.	O

The	O
cross	O
-	O
sentence	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
task	O
(	O
Peng	O
et	O
al	O
,	O
2017	O
)	O
is	O
defined	O
as	O
follows	O
.	O
Let	O
E	O
be	O
a	O
set	O
of	O
entities	O
,	O
R	O
KB	O
be	O
a	O
set	O
of	O
relation	O
types	O
of	O
an	O
external	O
knowledge	O
base	O
KB	O
,	O
and	O
O	O
KB	O
=	O
{	O
r	O
,	O
(	O
e	O
1	O
,	O
...	O
,	O
e	O
n	O
)	O
:	O
r	O
(	O
e	O
1	O
,	O
...	O
,	O
e	O
n	O
)	O
KB	O
,	O
r	O
2	O
For	O
example	O
,	O
the	O
ternary	O
relation	O
AwardedFor	O
(	O
director	O
,	O
movie	O
,	O
award	O
)	O
can	O
be	O
decomposed	O
into	O
the	O
binary	O
relations	O
DirectorOf	O
(	O
director	O
,	O
movie	O
)	O
and	O
WonAward	O
(	O
director	O
,	O
award	O
)	O
.	O
Note	O
that	O
a	O
similar	O
idea	O
is	O
introduced	O
in	O
(	O
Ernst	O
et	O
al	O
,	O
2018	O
)	O
as	O
partial	O
facts	O
or	O
partial	O
patterns	O
.	O
3	O
Our	O
codes	O
and	O
datasets	O
are	O
available	O
at	O
https://github.com/aurtg/	O
nary	O
-	O
relation	O
-	O
extraction	O
-	O
decomposed	O
.	O
R	O
KB	O
,	O
e	O
i	O
E	O
}	O
be	O
the	O
set	O
of	O
facts	O
in	O
KB	O
.	O
We	O
collect	O
a	O
set	O
of	O
candidate	O
entity	O
tuples	O
among	O
which	O
KB	O
relation	O
r	O
R	O
KB	O
possibly	O
holds	O
.	O
4	O
Here	O
,	O
all	O
entities	O
in	O
each	O
candidate	O
tuple	O
(	O
e	O
1	O
,	O
...	O
,	O
e	O
n	O
)	O
are	O
mentioned	O
in	O
the	O
same	O
text	O
section	O
T	O
in	O
a	O
given	O
set	O
of	O
documents	O
.	O
We	O
define	O
a	O
set	O
of	O
these	O
entity	O
mentions	O
as	O
O	O
text	O
=	O
{	O
T	O
,	O
(	O
e	O
1	O
,	O
...	O
,	O
e	O
n	O
)	O
:	O
e	O
i	O
E	O
is	O
mentioned	O
in	O
T	O
}	O
.	O
Here	O
,	O
text	O
section	O
T	O
is	O
a	O
(	O
short	O
)	O
span	O
in	O
a	O
document	O
which	O
can	O
describes	O
relational	O
facts	O
among	O
entities	O
.	O
In	O
the	O
cross	O
-	O
sentence	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
task	O
,	O
text	O
section	O
T	O
can	O
contain	O
multiple	O
sentences	O
.	O
In	O
this	O
paper	O
,	O
following	O
(	O
Peng	O
et	O
al	O
,	O
2017	O
)	O
,	O
we	O
define	O
M	O
consecutive	O
sentences	O
(	O
M	O
≥	O
1	O
)	O
which	O
contain	O
n	O
target	O
entities	O
as	O
a	O
text	O
section	O
in	O
the	O
crosssentence	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
task	O
.	O
We	O
use	O
the	O
term	O
"	O
relation	O
"	O
to	O
refer	O
to	O
both	O
relations	O
r	O
R	O
KB	O
and	O
sections	O
T	O
.	O
The	O
goal	O
of	O
the	O
cross	O
-	O
sentence	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
task	O
is	O
to	O
predict	O
new	O
facts	O
r	O
,	O
(	O
e	O
1	O
,	O
...	O
,	O
e	O
n	O
)	O
/	O
O	O
KB	O
for	O
relation	O
r	O
R	O
KB	O
given	O
O	O
=	O
O	O
KB	O
∪	O
O	O
text	O
,	O
where	O
n	O
≥	O
2	O
.	O
3	O
Proposed	O
Method	O

We	O
learn	O
a	O
vector	O
representation	O
v	O
(	O
r	O
)	O
R	O
dr	O
for	O
each	O
unary	O
or	O
binary	O
relation	O
in	O
O	O
1	O
or	O
O	O
2	O
.	O
For	O
r	O
(	O
k	O
)	O
or	O
r	O
(	O
k	O
,	O
l	O
)	O
derived	O
from	O
a	O
KB	O
relation	O
,	O
we	O
represent	O
it	O
by	O
a	O
trainable	O
parameter	O
vector	O
.	O
On	O
the	O
other	O
hand	O
,	O
for	O
the	O
one	O
derived	O
from	O
a	O
textual	O
relation	O
,	O
we	O
use	O
the	O
following	O
encoders	O
to	O
compute	O
its	O
representations	O
.	O
Unary	O
encoder	O
:	O
For	O
an	O
unary	O
textual	O
relation	O
r	O
(	O
k	O
)	O
=	O
(	O
T	O
,	O
pos	O
(	O
e	O
k	O
)	O
)	O
,	O
we	O
represent	O
each	O
section	O
T	O
by	O
a	O
sequence	O
of	O
word	O
vectors	O
and	O
use	O
a	O
bidirectional	B-MethodName
LSTM	I-MethodName
(	O
Bi	O
-	O
LSTM	B-MethodName
)	O
(	O
Schuster	O
and	O
Paliwal	O
,	O
1997	O
)	O
to	O
compute	O
a	O
hidden	O
representation	O
h	O
l	O
R	O
dr	O
at	O
each	O
word	O
position	O
l.	O
Following	O
recent	O
works	O
He	O
et	O
al	O
,	O
2018	O
;	O
Lee	O
et	O
al	O
,	O
2017	O
)	O
,	O
we	O
aggregate	O
h	O
l	O
within	O
a	O
phrase	O
of	O
entity	O
e	O
k	O
to	O
compute	O
v	O
(	O
T	O
(	O
k	O
)	O
)	O
.	O
We	O
use	O
elementwise	O
mean	O
as	O
aggregation	O
function	O
:	O
v	O
(	O
r	O
(	O
k	O
)	O
)	O
=	O
mean	O
(	O
{	O
h	O
l	O
:	O
l	O
pos	O
(	O
e	O
k	O
)	O
}	O
)	O
.	O
(	O
1	O
)	O
Binary	O
encoder	O
:	O
For	O
a	O
binary	O
textual	O
relation	O
r	O
(	O
k	O
,	O
l	O
)	O
=	O
path	O
(	O
T	O
;	O
e	O
k	O
,	O
e	O
l	O
)	O
,	O
we	O
represent	O
each	O
token	O
(	O
word	O
or	O
edge	O
label	O
)	O
in	O
path	O
(	O
T	O
;	O
e	O
k	O
,	O
e	O
l	O
)	O
by	O
an	O
embedding	O
vector	O
(	O
Toutanova	O
et	O
al	O
,	O
2015	O
;	O
Verga	O
et	O
al	O
,	O
2016	O
)	O
.	O
We	O
use	O
a	O
Bi	O
-	O
LSTM	B-MethodName
to	O
compute	O
a	O
hidden	O
representation	O
h	O
l	O
R	O
dr	O
at	O
each	O
token	O
position	O
l	O
,	O
and	O
max	O
-	O
pool	O
along	O
the	O
path	O
to	O
compute	O
the	O
relation	O
representation	O
:	O
v	O
(	O
T	O
(	O
k	O
,	O
l	O
)	O
)	O
=	O
max	O
(	O
{	O
h	O
l	O
:	O
l	O
=	O
1	O
,	O
...	O
,	O
L	O
}	O
)	O
.	O
(	O
2	O
)	O

The	O
cross	O
-	O
sentence	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
dataset	O
from	O
Peng	O
et	O
al	O
(	O
2017	O
)	O
contains	O
only	O
59	O
distinct	O
ternary	O
KB	O
facts	O
including	O
the	O
train	O
and	O
test	O
set	O
.	O
Since	O
our	O
proposed	O
method	O
and	O
universal	O
schemas	O
baselines	O
predict	O
KB	O
relations	O
for	O
each	O
entity	O
tuple	O
instead	O
of	O
each	O
surface	O
pattern	O
,	O
the	O
number	O
of	O
known	O
facts	O
of	O
KB	O
relations	O
is	O
crucial	O
to	O
reliably	O
evaluate	O
and	O
compare	O
these	O
methods	O
.	O
Thus	O
,	O
we	O
created	O
two	O
new	O
n	O
-	O
ary	O
cross	O
-	O
sentence	O
relation	B-TaskName
extraction	I-TaskName
datasets	O
(	O
dubbed	O
with	O
Wiki	O
-	O
90k	O
and	O
WF	O
-	O
20k	O
)	O
that	O
contain	O
more	O
known	O
facts	O
retrieved	O
from	O
public	O
knowledge	O
bases	O
.	O
To	O
create	O
the	O
Wiki	O
-	O
90k	O
and	O
WF	O
-	O
20k	O
datasets	O
,	O
we	O
used	O
Wikidata	O
and	O
Freebase	O
respectively	O
as	O
external	O
knowledge	O
bases	O
.	O
Since	O
these	O
knowledge	O
bases	O
store	O
only	O
binary	O
relational	O
facts	O
,	O
we	O
defined	O
multiple	O
ternary	O
relations	O
by	O
combining	O
a	O
few	O
binary	O
relations	O
.	O
7	O
,	O
8	O
For	O
both	O
datasets	O
,	O
we	O
collected	O
paragraphs	O
from	O
the	O
English	O
Wikipedia	O
,	O
and	O
used	O
Stanford	O
CoreNLP	O
(	O
Manning	O
et	O
al	O
,	O
2014	O
)	O
to	O
extract	O
dependency	O
and	O
co	O
-	O
reference	O
links	O
.	O
Entity	O
mentions	O
are	O
detected	O
using	O
DBpedia	B-DatasetName
Spotlight	O
(	O
Daiber	O
et	O
al	O
,	O
2013	O
)	O
.	O
We	O
followed	O
(	O
Peng	O
et	O
al	O
,	O
2017	O
)	O
to	O
extract	O
co	O
-	O
occurring	O
entity	O
tuples	O
and	O
their	O
surface	O
patterns	O
,	O
that	O
is	O
,	O
we	O
selected	O
tuples	O
which	O
occurred	O
in	O
a	O
minimal	O
span	O
within	O
at	O
most	O
M	O
≤	O
3	O
consecutive	O
sentences	O
.	O
Entity	O
tuples	O
without	O
a	O
known	O
KB	O
relation	O
are	O
subsampled	O
,	O
since	O
the	O
number	O
of	O
such	O
tuples	O
are	O
too	O
large	O
.	O
We	O
randomly	O
partitioned	O
all	O
entity	O
tuples	O
into	O
train	O
,	O
development	O
(	O
dev	O
)	O
,	O
and	O
test	O
sets	O
.	O
(	O
Song	O
et	O
al	O
,	O
2018	O
)	O
:	O
The	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
crosssentence	O
n	O
-	O
ary	O
relation	B-TaskName
extraction	I-TaskName
method	O
proposed	O
by	O
Song	O
et	O
al	O
(	O
2018	O
)	O
represents	O
each	O
surface	O
pattern	O
by	O
the	O
concatenation	O
of	O
entity	O
vectors	O
from	O
the	O
last	O
layer	O
of	O
a	O
Graph	O
State	O
LSTM	B-MethodName
,	O
a	O
variant	O
of	O
a	O
graph	O
neural	O
network	O
.	O
The	O
concatenated	O
vector	O
is	O
then	O
fed	O
into	O
a	O
classifier	O
to	O
predict	O
the	O
relation	O
label	O
.	O
Since	O
their	O
method	O
directly	O
predicts	O
a	O
relation	O
label	O
for	O
each	O
surface	O
pattern	O
,	O
it	O
is	O
more	O
robust	O
to	O
the	O
sparsity	O
of	O
surface	O
patterns	O
among	O
a	O
specific	O
higher	O
arity	O
entity	O
tuple	O
.	O
However	O
,	O
due	O
to	O
their	O
purely	O
supervised	O
training	O
objective	O
,	O
its	O
performance	O
may	O
degrade	O
if	O
the	O
number	O
of	O
available	O
training	O
labels	O
is	O
small	O
.	O

Wiki	O
-	O
90k	O
WF	O
-	O
20k	O
average	O
weighted	O
average	O
weighted	O
Proposed	O
0.584	O
0.634	O
0.821	O
0.842	O
(	O
Song	O
et	O
al	O
,	O
2018	O
)	O
0.471	O
0.536	O
0.639	O
0.680	O
(	O
Toutanova	O
et	O
al	O
,	O
2015	O
)	O
with	O
Graph	O
State	O
LSTM	B-MethodName
(	O
Song	O
et	O
al	O
,	O
2018	O
)	O
(	O
Verga	O
et	O
al	O
,	O
2017	O
)	O
with	O
Graph	O
State	O
LSTM	B-MethodName
(	O
Song	O
et	O
al	O
,	O
2018	O
)	O
multiple	O
-	O
test	O
adjustment	O
using	O
Holm	O
's	O
method	O
(	O
Holm	O
,	O
1979	O
)	O
.	O

Japanese	O
Zero	O
Anaphora	O
Resolution	O
Can	O
Benefit	O
from	O
Parallel	O
Texts	O
Through	O
Neural	O
Transfer	B-TaskName
Learning	I-TaskName

Parallel	O
texts	O
of	O
Japanese	O
and	O
a	O
non	O
-	O
pro	O
-	O
drop	O
language	O
have	O
the	O
potential	O
of	O
improving	O
the	O
performance	O
of	O
Japanese	O
zero	O
anaphora	O
resolution	O
(	O
ZAR	O
)	O
because	O
pronouns	O
dropped	O
in	O
the	O
former	O
are	O
usually	O
mentioned	O
explicitly	O
in	O
the	O
latter	O
.	O
However	O
,	O
rule	O
-	O
based	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
is	O
hampered	O
by	O
error	O
propagation	O
in	O
an	O
NLP	O
pipeline	O
and	O
the	O
frequent	O
lack	O
of	O
transparency	O
in	O
translation	O
correspondences	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
implicit	O
transfer	O
by	O
injecting	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	O
)	O
as	O
an	O
intermediate	O
task	O
between	O
pretraining	O
and	O
ZAR	O
.	O
We	O
employ	O
a	O
pretrained	O
BERT	B-MethodName
model	O
to	O
initialize	O
the	O
encoder	O
part	O
of	O
the	O
encoder	O
-	O
decoder	O
model	O
for	O
MT	O
,	O
and	O
eject	O
the	O
encoder	O
part	O
for	O
finetuning	O
on	O
ZAR	O
.	O
The	O
proposed	O
framework	O
empirically	O
demonstrates	O
that	O
ZAR	O
performance	O
can	O
be	O
improved	O
by	O
transfer	B-TaskName
learning	I-TaskName
from	O
MT	O
.	O
In	O
addition	O
,	O
we	O
find	O
that	O
the	O
incorporation	O
of	O
the	O
masked	O
language	O
model	O
training	O
into	O
MT	O
leads	O
to	O
further	O
gains	O
.	O

Figuring	O
out	O
who	O
did	O
what	O
to	O
whom	O
is	O
an	O
essential	O
part	O
of	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
.	O
This	O
is	O
,	O
however	O
,	O
especially	O
challenging	O
for	O
so	O
-	O
called	O
prodrop	O
languages	O
like	O
Japanese	O
and	O
Chinese	O
because	O
they	O
usually	O
omit	O
pronouns	O
that	O
are	O
inferable	O
from	O
context	O
.	O
The	O
task	O
of	O
identifying	O
the	O
referent	O
of	O
such	O
a	O
dropped	O
element	O
,	O
as	O
illustrated	O
in	O
Figure	O
1	O
(	O
a	O
)	O
,	O
is	O
referred	O
to	O
as	O
zero	O
anaphora	O
resolution	O
(	O
ZAR	O
)	O
.	O
Although	O
Japanese	O
ZAR	O
saw	O
a	O
performance	O
boost	O
with	O
the	O
introduction	O
of	O
BERT	B-MethodName
(	O
Ueda	O
et	O
al	O
,	O
2020	O
;	O
Konno	O
et	O
al	O
,	O
2020	O
)	O
,	O
there	O
is	O
still	O
a	O
good	O
amount	O
of	O
room	O
for	O
improvement	O
.	O
A	O
major	O
barrier	O
to	O
improvement	O
is	O
the	O
scarcity	O
of	O
training	O
data	O
.	O
The	O
number	O
of	O
annotated	O
sentences	O
is	O
the	O
order	O
of	O
tens	O
of	O
thousands	O
or	O
less	O
(	O
Kawahara	O
et	O
al	O
,	O
2002	O
;	O
Hangyo	O
et	O
al	O
,	O
2012	O
;	O
Iida	O
et	O
al	O
,	O
2017	O
)	O
,	O
and	O
the	O
considerable	O
linguistic	O
expertise	O
required	O
for	O
annotation	O
makes	O
drastic	O
corpus	O
expansion	O
impractical	O
.	O
Previous	O
attempts	O
to	O
overcome	O
this	O
limitation	O
exploit	O
orders	O
-	O
of	O
-	O
magnitude	O
larger	O
parallel	O
texts	O
of	O
Japanese	O
and	O
English	O
,	O
a	O
non	O
-	O
pro	O
-	O
drop	O
language	O
(	O
Nakaiwa	O
,	O
1999	O
;	O
Furukawa	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
key	O
idea	O
is	O
that	O
Japanese	O
zero	O
pronouns	O
can	O
be	O
recovered	O
from	O
parallel	O
texts	O
because	O
they	O
are	O
usually	O
mentioned	O
explicitly	O
in	O
English	O
,	O
as	O
in	O
Figure	O
1	O
(	O
b	O
)	O
.	O
If	O
translation	O
correspondences	O
are	O
identified	O
and	O
the	O
anaphoric	O
relation	O
in	O
English	O
is	O
identified	O
,	O
then	O
we	O
can	O
identify	O
the	O
antecedent	O
of	O
the	O
omitted	O
argument	O
in	O
Japanese	O
.	O
Their	O
rule	O
-	O
based	O
transfer	O
from	O
English	O
to	O
Japanese	O
had	O
met	O
with	O
limited	O
success	O
,	O
however	O
.	O
It	O
is	O
prone	O
to	O
error	O
propagation	O
due	O
to	O
its	O
dependence	O
on	O
word	B-TaskName
alignment	I-TaskName
,	O
parsing	O
,	O
and	O
English	O
coreference	B-TaskName
resolution	I-TaskName
.	O
More	O
importantly	O
,	O
the	O
great	O
linguistic	O
differences	O
between	O
the	O
two	O
language	O
often	O
lead	O
to	O
parallel	O
sentences	O
without	O
transparent	O
syntactic	O
correspondences	O
(	O
Figure	O
1	O
(	O
c	O
)	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
neural	O
transfer	B-TaskName
learning	I-TaskName
from	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	O
)	O
.	O
By	O
generating	O
English	O
translations	O
,	O
a	O
neural	O
MT	O
model	O
should	O
be	O
able	O
to	O
implicitly	O
recover	O
omitted	O
Japanese	O
pronouns	O
,	O
thanks	O
to	O
its	O
expressiveness	O
and	O
large	O
training	O
data	O
.	O
We	O
expect	O
the	O
knowledge	O
gained	O
during	O
MT	O
training	O
to	O
be	O
transferred	O
to	O
ZAR	O
.	O
Given	O
that	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
ZAR	O
models	O
are	O
based	O
on	O
BERT	B-MethodName
(	O
Ueda	O
et	O
al	O
,	O
2020	O
;	O
Konno	O
et	O
al	O
,	O
2020Konno	O
et	O
al	O
,	O
,	O
2021	O
,	O
it	O
is	O
a	O
natural	O
choice	O
to	O
explore	O
intermediate	O
task	O
transfer	B-TaskName
learning	I-TaskName
(	O
Phang	O
et	O
al	O
,	O
2018	O
;	O
Wang	O
et	O
al	O
,	O
2019a	O
;	O
Pruksachatkun	O
et	O
al	O
,	O
2020	O
;	O
Vu	O
et	O
al	O
,	O
2020	O
)	O
:	O
A	O
pretrained	O
BERT	B-MethodName
model	O
is	O
first	O
trained	O
on	O
MT	O
and	O
the	O
resultant	O
model	O
is	O
then	O
fine	O
-	O
tuned	O
on	O
ZAR	O
.	O
1	O
A	O
key	O
challenge	O
to	O
this	O
approach	O
is	O
a	O
mismatch	O
in	O
model	O
architectures	O
.	O
While	O
BERT	B-MethodName
is	O
an	O
encoder	O
,	O
the	O
dominant	O
paradigm	O
of	O
neural	O
MT	O
is	O
the	O
encoder	O
-	O
decoder	O
.	O
Although	O
both	O
share	O
Transformer	B-MethodName
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
The	O
nominative	O
argument	O
of	O
the	O
underlined	O
predicate	O
is	O
omitted	O
.	O
The	O
goal	O
of	O
the	O
task	O
is	O
to	O
detect	O
the	O
omission	O
and	O
to	O
identify	O
its	O
antecedent	O
"	O
son	O
"	O
.	O
(	O
b	O
)	O
The	O
corresponding	O
English	O
text	O
.	O
The	O
omitted	O
argument	O
in	O
Japanese	O
is	O
present	O
as	O
a	O
pronoun	O
in	O
English	O
.	O
(	O
c	O
)	O
A	O
Japanese	O
-	O
English	O
pair	O
(	O
Nabeshima	O
and	O
Brooks	O
,	O
2020	O
,	O
p.	O
74	O
)	O
whose	O
correspondences	O
are	O
too	O
obscure	O
for	O
rule	O
-	O
based	O
transfer	O
.	O
Because	O
Japanese	O
generally	O
avoids	O
having	O
inanimate	O
agents	O
with	O
animate	O
patients	O
,	O
the	O
English	O
inanimate	O
-	O
subject	O
sentence	O
corresponds	O
to	O
two	O
animate	O
-	O
subject	O
clauses	O
in	O
Japanese	O
,	O
with	O
two	O
exophoric	O
references	O
to	O
the	O
reader	O
(	O
i.e.	O
,	O
you	O
)	O
.	O
it	O
is	O
non	O
-	O
trivial	O
to	O
combine	O
the	O
two	O
distinct	O
architectures	O
,	O
with	O
the	O
goal	O
to	O
help	O
the	O
former	O
.	O
We	O
use	O
a	O
pretrained	O
BERT	B-MethodName
model	O
to	O
initialize	O
the	O
encoder	O
part	O
of	O
the	O
encoder	O
-	O
decoder	O
model	O
for	O
MT	O
.	O
While	O
this	O
technique	O
was	O
previously	O
used	O
by	O
Imamura	O
and	O
Sumita	O
(	O
2019	O
)	O
and	O
Clinchant	O
et	O
al	O
(	O
2019	O
)	O
,	O
they	O
both	O
aimed	O
at	O
improving	O
MT	O
performance	O
.	O
We	O
show	O
that	O
by	O
ejecting	O
the	O
encoder	O
part	O
for	O
use	O
in	O
fine	O
-	O
tuning	O
(	O
Figure	O
2	O
)	O
,	O
we	O
can	O
achieve	O
performance	O
improvements	O
in	O
ZAR	O
.	O
We	O
also	O
demonstrate	O
further	O
improvements	O
can	O
be	O
brought	O
by	O
incorporating	O
encoder	O
-	O
side	O
masked	O
language	O
model	O
(	O
MLM	B-DatasetName
)	O
training	O
into	O
the	O
intermediate	O
training	O
on	O
MT	O
.	O

ZAR	O
has	O
been	O
extensively	O
studied	O
in	O
major	O
East	O
Asian	O
languages	O
,	O
Chinese	O
and	O
Korean	O
as	O
well	O
as	O
Japanese	O
,	O
which	O
not	O
only	O
omit	O
contextually	O
inferable	O
pronouns	O
but	O
also	O
show	O
no	O
verbal	O
agreement	O
for	O
person	O
,	O
number	O
,	O
or	O
gender	O
(	O
Park	O
et	O
al	O
,	O
2015	O
;	O
Song	O
et	O
al	O
,	O
2020	O
;	O
Kim	O
et	O
al	O
,	O
2021	O
)	O
.	O
While	O
supervised	O
learning	O
is	O
the	O
standard	O
approach	O
to	O
ZAR	O
(	O
Iida	O
et	O
al	O
,	O
2016	O
;	O
Ouchi	O
et	O
al	O
,	O
2017	O
;	O
Shibata	O
and	O
Kurohashi	O
,	O
2018	O
)	O
,	O
training	O
data	O
are	O
so	O
small	O
that	O
additional	O
resources	O
are	O
clearly	O
needed	O
.	O
Early	O
studies	O
work	O
on	O
case	O
frame	O
construction	O
from	O
a	O
large	O
raw	O
corpus	O
(	O
Sasano	O
et	O
al	O
,	O
2008	O
;	O
Sasano	O
and	O
Kurohashi	O
,	O
2011	O
;	O
Yamashiro	O
et	O
al	O
,	O
2018	O
)	O
,	O
pseudo	O
training	O
data	O
generation	O
,	O
and	O
adversarial	O
training	O
(	O
Kurita	O
et	O
al	O
,	O
2018	O
)	O
.	O
These	O
efforts	O
are	O
,	O
however	O
,	O
overshadowed	O
by	O
the	O
surprising	O
effectiveness	O
of	O
BERT	B-MethodName
's	O
pretraining	O
(	O
Ueda	O
et	O
al	O
,	O
2020	O
;	O
Konno	O
et	O
al	O
,	O
2020	O
)	O
.	O
Adopting	O
BERT	B-MethodName
,	O
recent	O
studies	O
seek	O
gains	O
through	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
(	O
Ueda	O
et	O
al	O
,	O
2020	O
)	O
,	O
data	B-TaskName
augmentation	I-TaskName
(	O
Konno	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
an	O
intermediate	O
task	O
tailored	O
to	O
ZAR	O
(	O
Konno	O
et	O
al	O
,	O
2021	O
)	O
.	O
The	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
approach	O
of	O
Ueda	O
et	O
al	O
(	O
2020	O
)	O
covers	O
verbal	O
predicate	O
analysis	O
(	O
which	O
subsumes	O
ZAR	O
)	O
,	O
and	O
nominal	O
predicate	O
analysis	O
,	O
coreference	B-TaskName
resolution	I-TaskName
,	O
and	O
bridging	B-TaskName
anaphora	I-TaskName
resolution	I-TaskName
.	O
Their	O
method	O
is	O
used	O
as	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baseline	O
in	O
our	O
experiments	O
.	O
Konno	O
et	O
al	O
(	O
2020	O
)	O
perform	O
data	B-TaskName
augmentation	I-TaskName
by	O
simply	O
masking	O
some	O
tokens	O
.	O
They	O
found	O
that	O
performance	O
gains	O
were	O
achieved	O
by	O
selecting	O
target	O
tokens	O
by	O
part	O
of	O
speech	O
.	O
Konno	O
et	O
al	O
(	O
2021	O
)	O
introduce	O
a	O
more	O
elaborate	O
masking	O
strategy	O
as	O
a	O
ZAR	O
-	O
specific	O
intermediate	O
task	O
They	O
spot	O
multiple	O
occurrences	O
of	O
the	O
same	O
noun	O
phrase	O
,	O
mask	O
one	O
of	O
them	O
,	O
and	O
force	O
the	O
model	O
to	O
identify	O
the	O
pseudo	O
-	O
antecedent	O
.	O
Our	O
use	O
of	O
parallel	O
texts	O
in	O
ZAR	O
is	O
inspired	O
by	O
Nakaiwa	O
(	O
1999	O
)	O
and	O
Furukawa	O
et	O
al	O
(	O
2017	O
)	O
,	O
who	O
identify	O
a	O
multi	O
-	O
hop	O
link	O
from	O
a	O
Japanese	O
zero	O
pronoun	O
to	O
its	O
Japanese	O
antecedent	O
via	O
English	O
counterparts	O
.	O
Their	O
rule	O
-	O
based	O
methods	O
suffer	O
from	O
accumulated	O
errors	O
and	O
syntactically	O
non	O
-	O
transparent	O
correspondences	O
.	O
In	O
addition	O
,	O
they	O
do	O
not	O
handle	O
inter	O
-	O
sentential	O
anaphora	O
,	O
a	O
non	O
-	O
negligible	O
subtype	O
of	O
anaphora	O
we	O
cover	O
in	O
this	O
paper	O
.	O
While	O
we	O
exploit	O
MT	O
to	O
improve	O
the	O
performance	O
of	O
ZAR	O
,	O
the	O
exploitation	O
in	O
the	O
reverse	O
direction	O
has	O
been	O
studied	O
.	O
A	O
line	O
of	O
research	O
has	O
been	O
done	O
on	O
Chinese	O
zero	O
pronoun	O
prediction	O
(	O
ZPP	O
)	O
with	O
a	O
primary	O
aim	O
of	O
improving	O
Chinese	O
-	O
English	O
translation	O
(	O
Wang	O
et	O
al	O
,	O
2016	O
(	O
Wang	O
et	O
al	O
,	O
,	O
2018	O
(	O
Wang	O
et	O
al	O
,	O
,	O
2019b	O
.	O
ZPP	O
is	O
different	O
from	O
ZAR	O
in	O
that	O
it	O
does	O
not	O
identify	O
antecedents	O
.	O
This	O
is	O
understandable	O
given	O
that	O
classification	O
of	O
zero	O
pronouns	O
into	O
overt	O
ones	O
suffices	O
for	O
MT	O
.	O
Although	O
Wang	O
et	O
al	O
(	O
2019b	O
)	O
open	O
question	O
whether	O
MT	O
helps	O
ZAR	O
as	O
well	O
.	O

Inspired	B-DatasetName
by	O
the	O
great	O
success	O
of	O
the	O
pretraining	O
/	O
finetuning	O
paradigm	O
on	O
a	O
broad	O
range	O
of	O
tasks	O
(	O
Peters	O
et	O
al	O
,	O
2018	O
;	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
a	O
line	O
of	O
research	O
inserts	O
an	O
intermediate	O
task	O
between	O
pretraining	O
and	O
fine	O
-	O
tuning	O
on	O
a	O
target	O
task	O
(	O
Phang	O
et	O
al	O
,	O
2018	O
;	O
Wang	O
et	O
al	O
,	O
2019a	O
;	O
Pruksachatkun	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
Wang	O
et	O
al	O
(	O
2019a	O
)	O
found	O
that	O
MT	O
used	O
as	O
an	O
intermediate	O
task	O
led	O
to	O
performance	O
degeneration	O
in	O
various	O
target	O
tasks	O
,	O
such	O
as	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
and	O
sentiment	O
classification	O
.	O
2	O
They	O
argue	O
that	O
the	O
considerable	O
difference	O
between	O
MLM	B-DatasetName
pretraining	O
and	O
MT	O
causes	O
catastrophic	O
forgetting	O
(	O
CF	O
)	O
.	O
Pruksachatkun	O
et	O
al	O
(	O
2020	O
)	O
suggest	O
injecting	O
the	O
MLM	B-DatasetName
objective	O
during	O
intermediate	O
training	O
as	O
a	O
possible	O
way	O
to	O
mitigate	O
CF	O
,	O
which	O
we	O
empirically	O
test	O
in	O
this	O
paper	O
.	O

Motivated	O
by	O
BERT	B-MethodName
's	O
success	O
in	O
a	O
wide	O
range	O
of	O
applications	O
,	O
some	O
studies	O
incorporate	O
BERT	B-MethodName
into	O
MT	O
models	O
.	O
A	O
straightforward	O
way	O
to	O
do	O
this	O
is	O
to	O
initialize	O
the	O
encoder	O
part	O
of	O
the	O
encoder	O
-	O
decoder	O
with	O
pretrained	O
BERT	B-MethodName
,	O
but	O
it	O
has	O
had	O
mixed	O
success	O
at	O
best	O
(	O
Clinchant	O
et	O
al	O
,	O
2019	O
;	O
Zhu	O
et	O
al	O
,	O
2020	O
)	O
.	O
Abandoning	O
this	O
approach	O
,	O
Zhang	O
et	O
al	O
(	O
2020	O
)	O
simply	O
use	O
BERT	B-MethodName
as	O
a	O
supplier	O
of	O
context	O
-	O
aware	O
embeddings	O
to	O
their	O
own	O
encoder	O
-	O
decoder	O
model	O
.	O
Similarly	O
,	O
Guo	O
et	O
al	O
(	O
2020	O
)	O
stack	O
adapter	O
layers	O
on	O
top	O
of	O
two	O
frozen	O
BERT	B-MethodName
models	O
to	O
use	O
them	O
as	O
the	O
encoder	O
and	O
decoder	O
of	O
a	O
non	O
-	O
autoregressive	O
MT	O
2	O
We	O
suspect	O
that	O
the	O
poor	O
performance	O
resulted	O
in	O
part	O
from	O
their	O
excessively	O
simple	O
decoder	O
,	O
a	O
single	O
-	O
layer	O
LSTM	B-MethodName
.	O

[	O
author	O
]	O
[	O
NA	O
]	O
!	O
[	O
CLS	O
]	O
[	O
author	O
]	O
[	O
NA	O
]	O
!	O
Figure	O
3	O
:	O
ZAR	O
as	O
argument	O
selection	O
.	O
model	O
.	O
However	O
,	O
these	O
methods	O
can	O
not	O
be	O
adopted	O
for	O
our	O
purpose	O
because	O
we	O
want	O
BERT	B-MethodName
itself	O
to	O
learn	O
from	O
MT	O
.	O
Imamura	O
and	O
Sumita	O
(	O
2019	O
)	O
manage	O
to	O
maintain	O
the	O
straightforward	O
approach	O
by	O
adopting	O
a	O
twostage	O
training	O
procedure	O
:	O
In	O
the	O
first	O
stage	O
,	O
only	O
the	O
decoder	O
is	O
updated	O
with	O
the	O
encoder	O
frozen	O
,	O
while	O
in	O
the	O
second	O
stage	O
,	O
the	O
entire	O
model	O
is	O
updated	O
.	O
Although	O
they	O
offer	O
some	O
insights	O
,	O
it	O
remains	O
unclear	O
how	O
best	O
to	O
exploit	O
BERT	B-MethodName
when	O
MT	O
is	O
an	O
intermediate	O
task	O
,	O
not	O
the	O
target	O
task	O
.	O

We	O
adopt	O
a	O
ZAR	O
model	O
of	O
Ueda	O
et	O
al	O
(	O
2020	O
)	O
,	O
which	O
adds	O
a	O
thin	O
layer	O
on	O
top	O
of	O
BERT	B-MethodName
during	O
fine	O
-	O
tuning	O
to	O
solve	O
ZAR	O
and	O
related	O
tasks	O
(	O
Section	O
3.1	O
)	O
.	O
Instead	O
of	O
directly	O
moving	O
from	O
MLM	B-DatasetName
pretraining	O
to	O
fine	O
-	O
tuning	O
on	O
ZAR	O
,	O
we	O
inject	O
MT	O
as	O
an	O
intermediate	O
task	O
(	O
Section	O
3.2	O
)	O
.	O
In	O
addition	O
,	O
we	O
introduce	O
the	O
MLM	B-DatasetName
training	O
objective	O
during	O
the	O
intermediate	O
training	O
(	O
Section	O
3.3	O
)	O
.	O

Our	O
main	O
proposal	O
is	O
to	O
use	O
MT	O
as	O
an	O
intermediate	O
task	O
prior	O
to	O
fine	O
-	O
tuning	O
on	O
ZAR	O
.	O
Following	O
Imamura	O
and	O
Sumita	O
(	O
2019	O
)	O
and	O
Clinchant	O
et	O
al	O
(	O
2019	O
)	O
,	O
we	O
use	O
a	O
pretrained	O
BERT	B-MethodName
to	O
initialize	O
the	O
encoder	O
part	O
of	O
the	O
Transformer	B-MethodName
-	O
based	O
encoderdecoder	O
model	O
while	O
the	O
decoder	O
is	O
randomly	O
initialized	O
.	O
After	O
the	O
intermediate	O
training	O
on	O
MT	O
,	O
we	O
extract	O
the	O
encoder	O
and	O
move	O
on	O
to	O
fine	O
-	O
tuning	O
on	O
ZAR	O
and	O
related	O
tasks	O
(	O
Figure	O
2	O
)	O
.	O
Specifically	O
,	O
we	O
test	O
the	O
following	O
two	O
procedures	O
for	O
intermediate	O
training	O
:	O
One	O
-	O
stage	O
optimization	O
The	O
entire	O
model	O
is	O
updated	O
throughout	O
the	O
training	O
.	O
Two	O
-	O
stage	O
optimization	O
In	O
the	O
first	O
stage	O
,	O
the	O
encoder	O
is	O
frozen	O
and	O
only	O
the	O
decoder	O
is	O
updated	O
.	O
In	O
the	O
second	O
stage	O
,	O
the	O
entire	O
model	O
is	O
updated	O
(	O
Imamura	O
and	O
Sumita	O
,	O
2019	O
)	O
.	O

ZAR	O
We	O
used	O
two	O
corpora	O
in	O
our	O
experiments	O
:	O
the	O
Kyoto	O
University	O
Web	O
Document	O
Lead	O
Corpus	O
(	O
Hangyo	O
et	O
al	O
,	O
2012	O
)	O
and	O
the	O
Kyoto	O
University	O
Text	O
Corpus	O
(	O
Kawahara	O
et	O
al	O
,	O
2002	O
)	O
.	O
Based	O
on	O
their	O
genres	O
,	O
we	O
refer	O
to	O
them	O
as	O
the	O
Web	O
and	O
News	O
,	O
respectively	O
.	O
These	O
corpora	O
have	O
been	O
widely	O
used	O
in	O
previous	O
studies	O
(	O
Shibata	O
and	O
Kurohashi	O
,	O
2018	O
;	O
Kurita	O
et	O
al	O
,	O
2018	O
;	O
Ueda	O
et	O
al	O
,	O
2020	O
)	O
.	O
They	O
contained	O
manual	O
annotation	O
for	O
predicate	O
-	O
argument	O
structures	O
(	O
including	O
zero	O
anaphora	O
)	O
as	O
well	O
as	O
word	O
segmentation	O
,	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tags	O
,	O
dependency	O
relations	O
,	O
and	O
coreference	O
chains	O
.	O
We	O
split	O
the	O
datasets	O
into	O
training	O
,	O
validation	O
,	O
and	O
test	O
sets	O
following	O
the	O
published	O
setting	O
,	O
where	O
the	O
ratio	O
was	O
around	O
0.75:0.1:0.15	O
.	O
Key	O
statistics	O
are	O
shown	O
in	O
Table	O
1	O
.	O
MT	O
We	O
used	O
a	O
Japanese	O
-	O
English	O
parallel	O
corpus	O
of	O
newspaper	O
articles	O
distributed	O
by	O
the	O
Yomiuri	O
Shimbun	O
.	O
3	O
It	O
consisted	O
of	O
about	O
1.3	O
million	O
sentence	O
pairs	O
4	O
with	O
sentence	O
alignment	O
scores	O
.	O
We	O
discarded	O
pairs	O
with	O
scores	O
of	O
0	B-DatasetName
.	O
Because	O
the	O
task	O
of	O
interest	O
,	O
ZAR	O
,	O
required	O
inter	O
-	O
sentential	O
reasoning	O
,	O
consecutive	O
sentences	O
were	O
concatenated	O
into	O
chunks	O
,	O
with	O
the	O
maximum	O
number	O
of	O
tokens	O
equal	O
to	O
that	O
of	O
ZAR	O
.	O
As	O
a	O
result	O
,	O
we	O
obtained	O
around	O
373	O
,	O
000	O
,	O
21	O
,	O
000	O
,	O
and	O
21	O
,	O
000	O
chunks	O
for	O
the	O
training	O
,	O
validation	O
,	O
and	O
test	O
data	O
,	O
respectively	O
.	O
Japanese	O
sentences	O
were	O
split	O
into	O
words	O
using	O
the	O
morphological	O
analyzer	O
MeCab	O
with	O
the	O
Juman	O
dictionary	O
(	O
Kudo	O
et	O
al	O
,	O
2004	O
)	O
.	O
5	O
Both	O
Japanese	O
and	O
English	O
texts	O
underwent	O
subword	O
tokenization	O
.	O
We	O
used	O
Subword	O
-	O
NMT	O
(	O
Sennrich	O
et	O
al	O
,	O
2016	O
)	O
for	O
Japanese	O
and	O
SentencePiece	B-MethodName
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
for	O
English	O
.	O
We	O
used	O
separate	O
vocabularies	O
for	O
Japanese	O
and	O
English	O
,	O
with	O
the	O
vocabulary	O
sizes	O
of	O
around	O
32	O
,	O
000	O
and	O
16	O
,	O
000	O
,	O
respectively	O
.	O

Table	O
2	O
summarizes	O
the	O
experimental	O
results	O
.	O
Our	O
baseline	O
is	O
Ueda	O
et	O
al	O
(	O
2020	O
)	O
,	O
who	O
drastically	O
outperformed	O
previous	O
models	O
,	O
thanks	O
to	O
BERT	B-MethodName
.	O
+	O
MT	O
refers	O
to	O
the	O
model	O
with	O
intermediate	O
training	O
on	O
MT	O
while	O
+	O
MT	O
w/	O
MLM	B-DatasetName
corresponds	O
to	O
the	O
model	O
that	O
incorporated	O
the	O
MLM	B-DatasetName
objective	O
into	O
MT	O
.	O
We	O
can	O
see	O
that	O
MT	O
combined	O
with	O
MLM	B-DatasetName
performed	O
the	O
best	O
and	O
that	O
the	O
gains	O
reached	O
1.6	O
points	O
for	O
both	O
the	O
Web	O
and	O
News	O
.	O
Tables	O
3	O
and	O
4	O
provide	O
more	O
detailed	O
results	O
.	O
For	O
comparison	O
,	O
we	O
performed	O
additional	O
pretraining	O
with	O
ordinary	O
MLM	B-DatasetName
on	O
the	O
Japanese	O
part	O
of	O
the	O
parallel	O
corpus	O
(	O
denoted	O
as	O
+	O
MLM	B-DatasetName
)	O
,	O
because	O
the	O
possibility	O
remained	O
that	O
the	O
model	O
simply	O
took	O
advantage	O
of	O
additional	O
data	O
.	O
The	O
subsequent	O
two	O
blocks	O
compare	O
one	O
-	O
stage	O
(	O
unmarked	O
)	O
optimization	O
with	O
two	O
-	O
stage	O
optimization	O
.	O
MT	O
yielded	O
gains	O
on	O
all	O
settings	O
.	O
The	O
gains	O
were	O
consistent	O
across	O
anaphora	O
categories	O
.	O
Although	O
+	O
MLM	B-DatasetName
somehow	O
beat	O
the	O
baseline	O
,	O
it	O
was	O
outperformed	O
by	O
most	O
models	O
trained	O
on	O
MT	O
,	O
ruling	O
out	O
the	O
possibility	O
that	O
the	O
gains	O
were	O
solely	O
attributed	O
to	O
extra	O
data	O
.	O
We	O
can	O
conclude	O
that	O
Japanese	O
ZAR	O
benefits	O
from	O
parallel	O
texts	O
through	O
neural	O
transfer	B-TaskName
learning	I-TaskName
.	O
Two	O
-	O
stage	O
optimization	O
showed	O
mixed	O
results	O
.	O
It	O
worked	O
for	O
the	O
Web	O
but	O
did	O
not	O
for	O
the	O
News	O
.	O
What	O
is	O
worse	O
,	O
its	O
combination	O
with	O
MLM	B-DatasetName
led	O
to	O
performance	O
degeneration	O
on	O
both	O
datasets	O
.	O
MLM	B-DatasetName
achieved	O
superior	O
performance	O
as	O
it	O
worked	O
well	O
in	O
all	O
settings	O
.	O
The	O
gains	O
were	O
larger	O
with	O
one	O
-	O
stage	O
optimization	O
than	O
with	O
two	O
-	O
stage	O
optimization	O
(	O
1.4	O
vs.	O
0.3	O
on	O
the	O
Web	O
)	O
.	O

The	O
MLM	B-DatasetName
objective	O
during	O
intermediate	O
training	O
on	O
MT	O
is	O
shown	O
to	O
be	O
very	O
effective	O
,	O
but	O
why	O
?	O
Pruksachatkun	O
et	O
al	O
(	O
2020	O
)	O
conjecture	O
that	O
it	O
would	O
mitigate	O
catastrophic	O
forgetting	O
(	O
CF	O
)	O
,	O
but	O
this	O
is	O
not	O
the	O
sole	O
explanation	O
.	O
In	O
fact	O
,	O
Konno	O
et	O
al	O
(	O
2020	O
)	O
see	O
token	O
masking	O
as	O
a	O
way	O
to	O
augment	O
data	O
.	O

Due	O
to	O
space	O
limitation	O
,	O
we	O
have	O
limited	O
our	O
focus	O
to	O
BERT	B-MethodName
,	O
but	O
for	O
the	O
sake	O
of	O
future	O
practitioners	O
,	O
we	O
would	O
like	O
to	O
briefly	O
note	O
that	O
we	O
extensively	O
tested	O
BART	B-MethodName
and	O
its	O
variants	O
before	O
switching	O
to	O
BERT	B-MethodName
.	O
Unlike	O
BERT	B-MethodName
,	O
BART	B-MethodName
is	O
an	O
encoder	O
-	O
decoder	O
model	O
pretrained	O
on	O
a	O
monolingual	O
corpus	O
(	O
original	O
)	O
or	O
a	O
non	O
-	O
parallel	O
multilingual	O
corpus	O
(	O
mBART	B-MethodName
)	O
.	O
Because	O
MT	O
requires	O
the	O
encoder	O
-	O
decoder	O
architecture	O
,	O
maintaining	O
the	O
model	O
architecture	O
between	O
pretraining	O
and	O
intermediate	O
training	O
looked	O
promising	O
to	O
us	O
.	O
We	O
specifically	O
tested	O
(	O
1	O
)	O
the	O
officially	O
distributed	O
mBART	B-MethodName
model	O
,	O
(	O
2	O
)	O
a	O
BART	B-MethodName
model	O
we	O
pretrained	O
on	O
Japanese	O
Wikipedia	O
,	O
and	O
(	O
3	O
)	O
an	O
mBART	B-MethodName
model	O
we	O
pretrained	O
on	O
Japanese	O
and	O
English	O
texts	O
.	O
During	O
fine	O
-	O
tuning	O
,	O
we	O
added	O
the	O
ZAR	O
argument	O
selection	O
layer	O
on	O
top	O
of	O
either	O
the	O
encoder	O
or	O
the	O
decoder	O
.	O
Unfortunately	O
,	O
gains	O
from	O
MT	O
intermediate	O
training	O
were	O
marginal	O
for	O
these	O
models	O
.	O
A	O
more	O
serious	O
problem	O
was	O
that	O
they	O
came	O
close	O
to	O
but	O
rarely	O
outperformed	O
the	O
strong	O
BERT	B-MethodName
baseline	O
.	O
We	O
gave	O
up	O
identifying	O
the	O
cause	O
of	O
poorer	O
performance	O
because	O
it	O
was	O
extremely	O
hard	O
to	O
apply	O
comparable	O
experimental	O
conditions	O
to	O
large	O
pretrained	O
models	O
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
to	O
exploit	O
parallel	O
texts	O
for	O
Japanese	O
zero	O
anaphora	O
resolution	O
(	O
ZAR	O
)	O
by	O
inserting	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	O
)	O
as	O
an	O
intermediate	O
task	O
between	O
masked	O
language	O
model	O
(	O
MLM	B-DatasetName
)	O
pretraining	O
and	O
fine	O
-	O
tuning	O
on	O
ZAR	O
.	O
Although	O
previous	O
studies	O
reported	O
negative	O
results	O
on	O
the	O
use	O
of	O
MT	O
as	O
an	O
intermediate	O
task	O
,	O
we	O
demonstrated	O
that	O
it	O
did	O
work	O
for	O
Japanese	O
ZAR	O
.	O
Our	O
analysis	O
suggests	O
that	O
the	O
intermediate	O
training	O
on	O
MT	O
simultaneously	O
improved	O
the	O
model	O
's	O
ability	O
to	O
translate	O
Japanese	O
zero	O
pronouns	O
and	O
the	O
ZAR	O
performance	O
.	O
We	O
bridged	O
the	O
gap	O
between	O
BERT	B-MethodName
-	O
based	O
ZAR	O
and	O
the	O
encoder	O
-	O
decoder	O
architecture	O
for	O
MT	O
by	O
initializing	O
the	O
encoder	O
part	O
of	O
the	O
MT	O
model	O
with	O
a	O
pretrained	O
BERT	B-MethodName
.	O
Previous	O
studies	O
focusing	O
on	O
MT	O
reported	O
mixed	O
results	O
on	O
this	O
approach	O
,	O
but	O
again	O
,	O
we	O
demonstrated	O
its	O
considerable	O
positive	O
impact	O
on	O
ZAR	O
.	O
We	O
found	O
that	O
incorporating	O
the	O
MLM	B-DatasetName
objective	O
into	O
the	O
intermediate	O
training	O
was	O
particularly	O
effective	O
.	O
Our	O
experimental	O
results	O
were	O
consistent	O
with	O
the	O
speculation	O
that	O
MLM	B-DatasetName
mitigated	O
catastrophic	O
forgetting	O
during	O
intermediate	O
training	O
.	O
With	O
neural	O
transfer	B-TaskName
learning	I-TaskName
,	O
we	O
successfully	O
revived	O
the	O
old	O
idea	O
that	O
Japanese	O
ZAR	O
can	O
benefit	O
from	O
parallel	O
texts	O
(	O
Nakaiwa	O
,	O
1999	O
)	O
.	O
Thanks	O
to	O
the	O
astonishing	O
flexibility	O
of	O
neural	O
networks	O
,	O
we	O
would	O
probably	O
be	O
able	O
to	O
connect	O
ZAR	O
to	O
other	O
tasks	O
through	O
transfer	B-TaskName
learning	I-TaskName
.	O
The	O
example	O
in	O
which	O
MT	O
apparently	O
helped	O
ZAR	O
.	O
The	O
nominative	O
zero	O
pronoun	O
of	O
"	O
あり	O
"	O
(	O
is	O
)	O
was	O
correctly	O
translated	O
as	O
"	O
the	O
school	O
"	O
.	O
The	O
model	O
also	O
succeeded	O
in	O
identifying	O
its	O
antecedent	O
"	O
学	O
校	O
"	O
(	O
school	O
)	O
.	O
(	O
b	O
)	O
The	O
example	O
in	O
which	O
MT	O
was	O
not	O
helpful	O
.	O
The	O
model	O
successfully	O
translated	O
the	O
nominative	O
zero	O
pronoun	O
of	O
the	O
underlined	O
predicate	O
,	O
"	O
で	O
"	O
(	O
be	O
)	O
,	O
as	O
"	O
He	O
"	O
.	O
It	O
misidentified	O
its	O
antecedent	O
,	O
however	O
.	O

Exploratory	O
Analysis	O
for	O
Ontology	B-MethodName
Learning	O
from	O
Social	O
Events	O
on	O
Social	O
Media	O
Streaming	O
in	O
Spanish	O

The	O
problem	O
of	O
event	O
analysis	O
in	O
Spanish	O
social	O
media	O
streaming	O
is	O
that	O
of	O
difficulty	O
on	O
automatically	O
processing	O
the	O
data	O
as	O
well	O
as	O
obtaining	O
the	O
most	O
relevant	O
information	O
,	O
such	O
as	O
mentioned	O
by	O
Derczynski	B-DatasetName
et	O
al	O
(	O
2015	O
)	O
.	O
An	O
event	O
is	O
defined	O
as	O
a	O
real	O
world	O
occurrence	O
that	O
takes	O
place	O
in	O
a	O
specific	O
time	O
and	O
space	O
;	O
Atefeh	O
and	O
Khreich	O
(	O
2013	O
)	O
identifies	O
these	O
occurrences	O
by	O
the	O
entities	O
that	O
took	O
part	O
on	O
it	O
as	O
well	O
as	O
the	O
activities	O
done	O
in	O
it	O
.	O
This	O
project	O
focuses	O
on	O
researching	O
about	O
the	O
viability	O
of	O
modeling	O
these	O
events	O
as	O
ontologies	O
using	O
an	O
automatic	O
approach	O
for	O
entities	O
and	O
relationships	O
extraction	O
in	O
order	O
to	O
obtain	O
relevant	O
information	O
about	O
the	O
event	O
in	O
case	O
.	O
Spanish	O
data	O
from	O
Twitter	O
was	O
used	O
as	O
a	O
study	O
case	O
and	O
tested	O
with	O
the	O
developed	O
application	O
.	O

According	O
to	O
Lobzhanidze	O
et	O
al	O
(	O
2013	O
)	O
,	O
globalization	O
and	O
the	O
increased	O
use	O
of	O
social	O
networks	O
has	O
made	O
it	O
possible	O
for	O
news	O
and	O
events	O
related	O
information	O
to	O
be	O
propagated	O
in	O
a	O
much	O
faster	O
manner	O
to	O
every	O
part	O
of	O
the	O
world	O
.	O
It	O
is	O
in	O
this	O
context	O
that	O
event	O
analysis	O
is	O
the	O
most	O
relevant	O
since	O
,	O
as	O
Valkanas	O
and	O
Gunopulos	O
(	O
2013	O
)	O
mention	O
,	O
now	O
there	O
is	O
more	O
data	O
available	O
to	O
study	O
and	O
analyze	O
than	O
ever	O
before	O
.	O
An	O
event	O
is	O
defined	O
as	O
a	O
real	O
world	O
occurrence	O
that	O
takes	O
place	O
in	O
a	O
specific	O
time	O
and	O
space	O
;	O
Atefeh	O
and	O
Khreich	O
(	O
2013	O
)	O
identifies	O
these	O
occurrences	O
by	O
the	O
entities	O
that	O
took	O
part	O
on	O
it	O
as	O
well	O
as	O
the	O
activities	O
done	O
in	O
it	O
.	O
Events	O
will	O
be	O
the	O
main	O
study	O
object	O
in	O
this	O
paper	O
and	O
,	O
more	O
specifically	O
,	O
event	O
data	O
in	O
Spanish	O
obtained	O
from	O
Twitter	O
will	O
be	O
used	O
to	O
test	O
the	O
different	O
methods	O
and	O
techniques	O
exposed	O
on	O
each	O
Section	O
.	O
In	O
order	O
to	O
effectively	O
analyze	O
events	O
there	O
are	O
two	O
steps	O
that	O
need	O
to	O
be	O
taken	O
into	O
consideration	O
as	O
mentioned	O
in	O
Kumbla	O
(	O
2016	O
)	O
:	O
(	O
1	O
)	O
event	O
data	O
acquisition	O
,	O
and	O
(	O
2	O
)	O
event	O
data	O
processing	O
.	O
The	O
first	O
step	O
is	O
the	O
one	O
that	O
benefits	O
the	O
most	O
by	O
social	O
media	O
streaming	O
since	O
more	O
data	O
is	O
available	O
,	O
though	O
one	O
of	O
the	O
downsides	O
to	O
this	O
is	O
that	O
the	O
data	O
is	O
usually	O
not	O
ready	O
to	O
be	O
used	O
right	O
away	O
and	O
most	O
of	O
the	O
times	O
a	O
preprocessing	O
step	O
needs	O
to	O
happen	O
.	O
This	O
step	O
is	O
further	O
explained	O
on	O
section	O
Section	O
3	O
.	O
The	O
second	O
step	O
will	O
be	O
the	O
main	O
focus	O
on	O
this	O
paper	O
since	O
the	O
biggest	O
problem	O
on	O
event	O
data	O
analysis	O
in	O
Spanish	O
is	O
this	O
one	O
.	O
In	O
particular	O
,	O
automatic	O
approaches	O
for	O
entities	O
and	O
relationships	O
extraction	O
will	O
be	O
presented	O
on	O
Section	O
4	O
.	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows	O
.	O
In	O
Section	O
2	O
some	O
relevant	O
related	O
work	O
is	O
exposed	O
.	O
Later	O
,	O
in	O
Section	O
3	O
the	O
event	O
acquisition	O
process	O
is	O
further	O
expanded	O
upon	O
.	O
The	O
ontology	B-MethodName
structure	O
used	O
for	O
the	O
events	O
representation	O
as	O
well	O
as	O
the	O
algorithms	O
employed	O
in	O
order	O
to	O
obtain	O
entities	O
and	O
relationships	O
between	O
these	O
are	O
further	O
explained	O
on	O
Section	O
4	O
.	O
Section	O
5	O
introduces	O
a	O
simple	O
application	O
developed	O
in	O
order	O
to	O
make	O
use	O
of	O
the	O
algorithms	O
and	O
techniques	O
mentioned	O
on	O
the	O
previous	O
sections	O
.	O
On	O
section	O
6	O
we	O
compare	O
the	O
results	O
obtained	O
with	O
manually	O
created	O
ontologies	O
and	O
obtain	O
precision	O
and	O
recall	O
values	O
for	O
each	O
case	O
.	O
Finally	O
,	O
concluding	O
remarks	O
are	O
provided	O
in	O
Section	O
7	O
.	O
In	O
Al	O
-	O
Smadi	O
and	O
Qawasmeh	O
(	O
2016	O
)	O
an	O
unsupervised	O
approach	O
for	O
event	B-TaskName
extraction	I-TaskName
from	O
Arabic	O
tweets	O
is	O
discussed	O
.	O
Entities	O
appearing	O
in	O
the	O
data	O
are	O
linked	O
to	O
corresponding	O
entities	O
found	O
on	O
Wikipedia	O
and	O
DBpedia	B-DatasetName
through	O
an	O
ontology	B-MethodName
based	O
knowledge	O
base	O
.	O
The	O
entities	O
from	O
the	O
data	O
are	O
extracted	O
based	O
on	O
rules	O
related	O
to	O
the	O
Arabic	O
language	O
.	O
In	O
Derczynski	B-DatasetName
et	O
al	O
(	O
2015	O
)	O
a	O
comparative	O
evaluation	O
of	O
different	O
NER	B-TaskName
is	O
done	O
based	O
on	O
three	O
different	O
datasets	O
.	O
Also	O
,	O
some	O
common	O
challenges	O
or	O
errors	O
when	O
handling	O
data	O
from	O
Twitter	O
are	O
presented	O
as	O
well	O
as	O
methods	O
for	O
reducing	O
microblog	O
noise	O
through	O
pre	O
-	O
processing	O
such	O
as	O
language	B-TaskName
identification	I-TaskName
,	O
POStagging	O
and	O
normalization	O
.	O
In	O
Ilknur	O
et	O
al	O
(	O
2011	O
)	O
a	O
framework	O
for	O
learning	O
relations	O
between	O
entities	O
in	O
Twitter	O
is	O
presented	O
.	O
This	O
framework	O
allows	O
for	O
entities	O
as	O
well	O
as	O
entity	O
types	O
or	O
topics	O
to	O
be	O
detected	O
,	O
which	O
results	O
in	O
a	O
graph	O
connecting	O
semantically	O
enriched	O
resources	O
to	O
their	O
respective	O
entities	O
.	O
Then	O
relation	O
discovery	O
strategies	O
are	O
employed	O
to	O
detect	O
pair	O
of	O
entities	O
that	O
have	O
a	O
certain	O
type	O
of	O
relationship	O
in	O
a	O
specific	O
period	O
of	O
time	O
.	O
In	O
Raimond	O
and	O
Abdallah	O
(	O
2007	O
)	O
an	O
event	O
ontology	B-MethodName
is	O
described	O
.	O
This	O
model	O
also	O
contains	O
some	O
key	O
characteristics	O
such	O
as	O
place	O
,	O
location	O
,	O
agents	O
and	O
products	O
.	O
On	O
the	O
other	O
hand	O
,	O
event	O
-	O
subevent	O
relationships	O
are	O
used	O
to	O
build	O
the	O
related	O
ontologies	O
.	O
This	O
model	O
was	O
developed	O
for	O
the	O
Center	O
for	O
Digital	O
Music	O
and	O
tested	O
by	O
structuring	O
proceedings	O
and	O
concert	O
descriptions	O
.	O
Finally	O
,	O
an	O
ontology	B-MethodName
model	O
for	O
events	O
is	O
proposed	O
in	O
which	O
entities	O
are	O
extracted	O
using	O
the	O
CMU	O
tweet	O
analyzer	O
and	O
relationships	O
are	O
inferred	O
from	O
Wikipedia	O
,	O
DBpedia	B-DatasetName
and	O
Web	O
data	O
.	O
This	O
approach	O
also	O
uses	O
a	O
POS	O
-	O
tagging	O
step	O
in	O
order	O
to	O
obtain	O
the	O
initial	O
set	O
of	O
entities	O
to	O
process	O
.	O

Ontology	B-MethodName
learning	O
is	O
defined	O
by	O
Cimiano	O
(	O
2006	O
)	O
as	O
the	O
automatic	O
acquisition	O
of	O
a	O
domain	O
model	O
from	O
some	O
dataset	O
.	O
In	O
this	O
paper	O
we	O
focus	O
on	O
applying	O
ontology	B-MethodName
learning	O
techniques	O
for	O
data	O
represented	O
as	O
text	O
.	O
Cimiano	O
points	O
towards	O
two	O
main	O
approaches	O
for	O
ontology	B-MethodName
learning	O
:	O
1	O
.	O
Machine	O
learning	O
2	O
.	O
Statistical	O
approach	O
Statistical	O
based	O
algorithms	O
are	O
further	O
discussed	O
on	O
Sections	O
4.3	O
and	O
4.4	O
.	O

Before	O
we	O
start	O
using	O
different	O
techniques	O
in	O
order	O
to	O
populate	O
an	O
ontology	B-MethodName
or	O
to	O
learn	O
entities	O
and	O
relationships	O
from	O
the	O
data	O
that	O
was	O
retrieved	O
previously	O
,	O
an	O
ontology	B-MethodName
structure	O
had	O
to	O
be	O
defined	O
.	O
The	O
ontology	B-MethodName
structure	O
that	O
we	O
define	O
will	O
point	O
us	O
towards	O
different	O
techniques	O
depending	O
on	O
the	O
information	O
that	O
must	O
be	O
retrieved	O
to	O
populate	O
this	O
particular	O
structure	O
.	O
Therefore	O
,	O
the	O
proposed	O
ontology	B-MethodName
structure	O
in	O
this	O
paper	O
is	O
defined	O
on	O
Figure	O
1	O
.	O
The	O
ontology	B-MethodName
will	O
be	O
populated	O
by	O
such	O
triples	O
composed	O
of	O
(	O
Entity	O
,	O
Temporal	O
entity	O
,	O
object	O
)	O
.	O
Where	O
Entity	O
denotes	O
a	O
subject	O
that	O
interacts	O
in	O
the	O
event	O
,	O
Temporal	O
entity	O
refers	O
to	O
the	O
date	O
when	O
the	O
particular	O
activity	O
takes	O
place	O
and	O
object	O
is	O
the	O
recipient	O
of	O
the	O
activity	O
.	O

This	O
was	O
one	O
of	O
the	O
main	O
points	O
of	O
interest	O
and	O
research	O
on	O
this	O
paper	O
,	O
how	O
to	O
select	O
the	O
most	O
representative	O
entities	O
for	O
the	O
event	O
in	O
order	O
to	O
not	O
overwhelm	O
people	O
analyzing	O
the	O
results	O
but	O
also	O
to	O
not	O
present	O
too	O
little	O
or	O
irrelevant	O
information	O
.	O
In	O
order	O
to	O
achieve	O
this	O
,	O
two	O
initial	O
tools	O
for	O
entity	B-TaskName
retrieval	I-TaskName
were	O
tested	O
:	O
1	O
.	O
Stanford	O
NER	B-TaskName
:	O
The	O
Stanford	O
NER	B-TaskName
used	O
with	O
a	O
trained	O
Spanish	O
model	O
from	O
late	O
2016	O
was	O
used	O
in	O
order	O
to	O
retrieve	O
persons	O
,	O
entities	O
and	O
organizations	O
and	O
group	O
them	O
all	O
together	O
as	O
entities	O
.	O
2	O
.	O
UDPipe	O
:	O
UDPipe	O
allows	O
to	O
parse	O
text	O
in	O
order	O
to	O
obtain	O
the	O
grammatical	O
categories	O
of	O
the	O
words	O
in	O
each	O
sentence	O
,	O
as	O
well	O
as	O
the	O
syntactic	O
dependencies	O
or	O
syntactic	O
tree	O
that	O
envelops	O
the	O
whole	O
sentence	O
.	O
The	O
entities	O
are	O
obtained	O
from	O
the	O
grammatical	O
category	O
PROPN	O
.	O
These	O
two	O
approaches	O
were	O
then	O
implemented	O
and	O
tested	O
with	O
each	O
dataset	O
and	O
a	O
manual	O
comparison	O
was	O
made	O
between	O
the	O
entities	O
that	O
each	O
approach	O
captured	O
.	O
The	O
results	O
showed	O
that	O
,	O
while	O
the	O
Stanford	O
NER	B-TaskName
worked	O
really	O
well	O
in	O
the	O
case	O
where	O
the	O
tweets	O
were	O
news	O
related	O
or	O
had	O
a	O
more	O
formal	O
undertone	O
,	O
such	O
as	O
in	O
the	O
case	O
of	O
the	O
Australian	O
Open	O
,	O
it	O
failed	O
to	O
find	O
a	O
lot	O
of	O
basic	O
entities	O
in	O
the	O
other	O
two	O
datasets	O
where	O
the	O
data	O
was	O
more	O
unstructured	O
as	O
one	O
would	O
very	O
likely	O
find	O
when	O
working	O
on	O
social	O
streaming	O
.	O
Also	O
,	O
the	O
Stanford	O
NER	B-TaskName
has	O
heavily	O
influenced	O
by	O
correct	O
capitalization	O
and	O
punctuation	O
,	O
whereas	O
UDPipe	O
was	O
n't	O
influenced	O
by	O
these	O
factors	O
as	O
much	O
.	O
Because	O
of	O
this	O
,	O
UDPipe	O
was	O
chosen	O
as	O
the	O
main	O
initial	O
entity	O
extraction	O
tool	O
moving	O
forward	O
.	O
After	O
having	O
a	O
set	O
of	O
initial	O
entities	O
,	O
further	O
processing	O
steps	O
were	O
taken	O
to	O
ensure	O
a	O
better	O
result	O
.	O

Entity	O
clustering	O
was	O
done	O
on	O
two	O
stages	O
.	O
First	O
,	O
an	O
algorithm	O
for	O
entity	O
clustering	O
was	O
devised	O
based	O
on	O
two	O
metrics	O
:	O
1	O
.	O
Normalized	O
frequency	O
of	O
two	O
entities	O
appearing	O
in	O
a	O
single	O
tweet	O
:	O
The	O
frequency	O
of	O
appearance	O
between	O
two	O
specific	O
entities	O
in	O
tweets	O
.	O
2	O
.	O
Average	O
Entity	O
to	O
entity	O
distance	O
in	O
a	O
tweet	O
(	O
i.e.	O
in	O
the	O
sentence	O
"	O
Nadal	O
venció	O
a	O
Federer	O
"	O
,	O
if	O
both	O
Nadal	O
and	O
Federer	O
are	O
identified	O
as	O
entities	O
,	O
they	O
would	O
have	O
a	O
distance	O
of	O
3	O
for	O
this	O
tweet	O
)	O
A	O
threshold	O
of	O
0.125	O
was	O
set	O
as	O
the	O
minimum	O
normalized	O
frequency	O
for	O
a	O
pair	O
of	O
entities	O
and	O
a	O
minimum	O
average	O
Entity	O
to	O
Entity	O
distance	O
of	O
1.65	O
.	O
These	O
two	O
values	O
were	O
set	O
based	O
on	O
experimentation	O
with	O
the	O
resulting	O
clustered	O
entities	O
from	O
each	O
dataset	O
.	O
After	O
that	O
,	O
an	O
approach	O
based	O
on	O
Levenshtein	O
distance	O
(	O
minimum	O
amount	O
of	O
additions	O
,	O
replacements	O
or	O
deletions	O
needed	O
to	O
turn	O
a	O
word	O
into	O
another	O
)	O
was	O
employed	O
,	O
where	O
two	O
entities	O
were	O
clustered	O
together	O
if	O
their	O
distance	O
was	O
more	O
than	O
0.9	O
times	O
the	O
length	O
of	O
the	O
longest	O
entity	O
from	O
the	O
two	O
.	O
An	O
example	O
of	O
this	O
distance	O
can	O
be	O
seen	O
on	O
Figure	O
2	O
.	O
FCA	O
is	O
one	O
of	O
the	O
approaches	O
for	O
entity	O
extraction	O
detailed	O
on	O
Cimiano	O
(	O
2006	O
)	O
.	O
It	O
is	O
the	O
one	O
that	O
garners	O
the	O
most	O
focus	O
on	O
this	O
book	O
as	O
the	O
main	O
set	O
-	O
theoretical	O
approach	O
based	O
on	O
verb	O
-	O
subject	O
components	O
.	O
This	O
approach	O
is	O
based	O
on	O
obtaining	O
the	O
formal	O
context	O
for	O
a	O
specific	O
domain	O
or	O
dataset	O
and	O
then	O
proceed	O
to	O
use	O
it	O
to	O
create	O
a	O
hierarchy	O
ontology	B-MethodName
.	O
An	O
example	O
of	O
how	O
a	O
formal	O
context	O
would	O
look	O
for	O
a	O
tourism	O
domain	O
knowledge	O
can	O
be	O
seen	O
on	O
Table	O
1	O
.	O
Cimiano	O
(	O
2006	O
)	O
bookable	O
rentable	O
rideable	O
hotel	O
X	O
apartment	O
X	O
X	O
bike	O
X	O
X	O
X	O
excursion	O
X	O
trip	O
X	O
In	O
this	O
paper	O
we	O
use	O
the	O
created	O
formal	O
contexts	O
to	O
discriminate	O
between	O
entities	O
based	O
on	O
three	O
metrics	O
:	O
Conditional	O
(	O
n	O
,	O
v	O
)	O
=	O
P	O
(	O
n	O
,	O
v	O
)	O
=	O
f	O
(	O
n	O
,	O
v	O
)	O
f	O
(	O
v	O
)	O
(	O
1	O
)	O
P	O
M	O
I	O
(	O
n	O
,	O
v	O
)	O
=	O
log	O
2	O
P	O
(	O
n	O
|	O
v	O
)	O
P	O
(	O
n	O
)	O
(	O
2	O
)	O
Resnik	O
(	O
n	O
,	O
v	O
)	O
=	O
SR	O
(	O
v	O
)	O
*	O
P	O
(	O
n	O
|	O
v	O
)	O
(	O
3	O
)	O
Where	O
:	O
1	O
.	O
f	O
(	O
n	O
,	O
v	O
)	O
=	O
>	O
Frequency	O
of	O
apparition	O
of	O
entity	O
n	O
with	O
verb	O
v	O
2	O
.	O
f	O
(	O
v	O
)	O
=	O
>	O
Frequency	O
of	O
apparition	O
of	O
verb	O
v	O
with	O
any	O
entity	O
And	O
:	O
SR	O
(	O
v	O
)	O
=	O
n	O
P	O
(	O
n	O
|	O
v	O
)	O
*	O
log	O
2	O
P	O
(	O
n	O
|	O
v	O
)	O
P	O
(	O
n	O
)	O
(	O
4	O
)	O
A	O
threshold	O
of	O
0.1	O
as	O
a	O
minimum	O
value	O
is	O
set	O
for	O
all	O
of	O
the	O
three	O
aforementioned	O
metrics	O
(	O
Conditional	O
,	O
PMI	O
and	O
Resnik	O
weights	O
)	O
,	O
meaning	O
that	O
the	O
(	O
entity	O
,	O
verb	O
)	O
pairs	O
that	O
not	O
surpass	O
this	O
threshold	O
for	O
any	O
of	O
the	O
three	O
metrics	O
are	O
pruned	O
.	O

A	O
desktop	O
application	O
was	O
developed	O
in	O
order	O
to	O
allow	O
for	O
easier	O
visualization	O
of	O
both	O
the	O
ontology	B-MethodName
and	O
the	O
resulting	O
activities	O
that	O
each	O
entity	O
participated	O
in	O
,	O
as	O
well	O
as	O
the	O
activities	O
that	O
create	O
a	O
relationship	O
between	O
two	O
particular	O
entities	O
.	O

In	O
order	O
to	O
verify	O
the	O
approach	O
applied	O
for	O
ontology	B-MethodName
extraction	O
,	O
we	O
manually	O
created	O
ontologies	O
for	O
each	O
test	O
case	O
where	O
the	O
most	O
relevant	O
entities	O
and	O
relationships	O
are	O
specified	O
based	O
on	O
investigation	O
related	O
to	O
these	O
cases	O
,	O
these	O
ontologies	O
can	O
be	O
seen	O
on	O
Figures	O
5	O
,	O
6	O
and	O
7	O
.	O
These	O
ontologies	O
were	O
then	O
presented	O
to	O
colleagues	O
with	O
more	O
profound	O
knowledge	O
on	O
each	O
of	O
the	O
events	O
for	O
validation	O
and	O
were	O
redone	O
based	O
on	O
their	O
feedback	O
until	O
they	O
were	O
accepted	O
by	O
them	O
.	O
From	O
these	O
ontologies	O
we	O
obtained	O
precision	O
and	O
recall	O
values	O
for	O
both	O
entities	O
and	O
relationships	O
for	O
each	O
case	O
.	O
These	O
can	O
be	O
seen	O
on	O
Tables	O
2	O
,	O
3	O
and	O
4	O
:	O
The	O
main	O
point	O
of	O
interest	O
in	O
these	O
metrics	O
lies	O
on	O
the	O
precision	O
,	O
where	O
the	O
precision	O
on	O
the	O
Australian	O
Open	O
case	O
in	O
quite	O
higher	O
than	O
on	O
the	O
other	O
two	O
cases	O
.	O
From	O
further	O
inspection	O
on	O
the	O
corresponding	O
data	O
we	O
could	O
infer	O
that	O
this	O
was	O
the	O
case	O
because	O
a	O
big	O
part	O
of	O
the	O
tweets	O
for	O
the	O
Australian	O
Open	O
where	O
either	O
formal	O
tweets	O
made	O
by	O
users	O
representing	O
news	O
outlets	O
or	O
by	O
the	O
players	O
themselves	O
.	O
As	O
for	O
the	O
other	O
two	O
cases	O
,	O
most	O
of	O
the	O
tweets	O
where	O
a	O
mix	O
of	O
news	O
and	O
discussion	O
from	O
common	O
people	O
about	O
these	O
events	O
.	O

We	O
conclude	O
that	O
,	O
while	O
the	O
methods	O
exposed	O
on	O
this	O
paper	O
work	O
good	O
enough	O
on	O
cases	O
such	O
as	O
the	O
Australian	O
Open	O
one	O
,	O
there	O
is	O
still	O
work	O
to	O
be	O
done	O
when	O
the	O
general	O
public	O
is	O
more	O
engaged	O
on	O
the	O
event	O
such	O
as	O
the	O
cases	O
of	O
the	O
Puente	O
Piedra	O
toll	O
and	O
the	O
March	O
against	O
the	O
corruption	O
.	O
This	O
paper	O
's	O
aim	O
was	O
to	O
give	O
a	O
foundation	O
and	O
a	O
initial	O
stage	O
of	O
exploratory	O
analysis	O
on	O
social	O
media	O
streaming	O
in	O
Spanish	O
by	O
using	O
ontologies	O
,	O
after	O
which	O
future	O
work	O
could	O
be	O
based	O
upon	O
in	O
order	O
to	O
expand	O
the	O
knowledge	O
in	O
the	O
ontologies	O
or	O
use	O
this	O
analysis	O
together	O
with	O
an	O
event	B-TaskName
detection	I-TaskName
system	O
in	O
order	O
to	O
be	O
able	O
to	O
both	O
detect	O
and	O
analyze	O
events	O
in	O
real	O
time	O
.	O

Multilingual	O
Paraphrase	B-TaskName
Generation	I-TaskName
For	O
Bootstrapping	O
New	O
Features	O
in	O
Task	O
-	O
Oriented	O
Dialog	O
Systems	O

The	O
lack	O
of	O
labeled	O
training	O
data	O
for	O
new	O
features	O
is	O
a	O
common	O
problem	O
in	O
rapidly	O
changing	O
real	O
-	O
world	O
dialog	O
systems	O
.	O
As	O
a	O
solution	O
,	O
we	O
propose	O
a	O
multilingual	O
paraphrase	B-TaskName
generation	I-TaskName
model	O
that	O
can	O
be	O
used	O
to	O
generate	O
novel	O
utterances	O
for	O
a	O
target	O
feature	O
and	O
target	O
language	O
.	O
The	O
generated	O
utterances	O
can	O
be	O
used	O
to	O
augment	O
existing	O
training	O
data	O
to	O
improve	O
intent	B-TaskName
classification	I-TaskName
and	O
slot	O
labeling	O
models	O
.	O
We	O
evaluate	O
the	O
quality	O
of	O
generated	O
utterances	O
using	O
intrinsic	O
evaluation	O
metrics	O
and	O
by	O
conducting	O
downstream	O
evaluation	O
experiments	O
with	O
English	O
as	O
the	O
source	O
language	O
and	O
nine	O
different	O
target	O
languages	O
.	O
Our	O
method	O
shows	O
promise	O
across	O
languages	O
,	O
even	O
in	O
a	O
zero	O
-	O
shot	O
setting	O
where	O
no	O
seed	O
data	O
is	O
available	O
.	O

Spoken	B-TaskName
language	I-TaskName
understanding	I-TaskName
is	O
a	O
core	O
problem	O
in	O
task	O
oriented	O
dialog	O
systems	O
with	O
the	O
goal	O
of	O
understanding	O
and	O
formalizing	O
the	O
intent	O
expressed	O
by	O
an	O
utterance	O
(	O
Tur	O
and	O
De	O
Mori	O
,	O
2011	O
)	O
.	O
It	O
is	O
often	O
modeled	O
as	O
intent	B-TaskName
classification	I-TaskName
(	O
IC	O
)	O
,	O
an	O
utterance	O
-	O
level	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
problem	O
,	O
and	O
slot	O
labeling	O
(	O
SL	O
)	O
,	O
a	O
sequence	O
labeling	O
problem	O
over	O
the	O
utterance	O
's	O
tokens	O
.	O
In	O
recent	O
years	O
,	O
approaches	O
that	O
train	O
joint	O
models	O
for	O
both	O
tasks	O
and	O
that	O
leverage	O
powerful	O
pre	O
-	O
trained	O
neural	O
models	O
greatly	O
improved	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
available	O
benchmarks	O
for	O
IC	O
and	O
SL	O
(	O
Louvan	O
and	O
Magnini	O
,	O
2020	O
;	O
Weld	O
et	O
al	O
,	O
2021	O
)	O
.	O
A	O
common	O
challenge	O
in	O
real	O
-	O
world	O
systems	O
is	O
the	O
problem	O
of	O
feature	O
bootstrapping	O
:	O
If	O
a	O
new	O
feature	O
should	O
be	O
supported	O
,	O
the	O
label	O
space	O
needs	O
to	O
be	O
extended	O
with	O
new	O
intent	O
or	O
slot	O
labels	O
,	O
and	O
the	O
model	O
needs	O
to	O
be	O
retrained	O
to	O
learn	O
to	O
classify	O
corresponding	O
utterances	O
.	O
However	O
,	O
labeled	O
examples	O
for	O
the	O
new	O
feature	O
are	O
typically	O
limited	O
to	O
a	O
small	O
set	O
of	O
seed	O
examples	O
,	O
as	O
the	O
collection	O
of	O
more	O
annotations	O
would	O
make	O
feature	O
expansion	O
costly	O
and	O
slow	O
.	O
As	O
a	O
possible	O
solution	O
,	O
previous	O
work	O
explored	O
the	O
automatic	O
generation	O
of	O
paraphrases	O
to	O
augment	O
the	O
seed	O
data	O
(	O
Malandrakis	O
et	O
al	O
,	O
2019	O
;	O
Cho	O
et	O
al	O
,	O
2019	O
;	O
Jolly	O
et	O
al	O
,	O
2020	O
)	O
.	O
In	O
this	O
work	O
,	O
we	O
study	O
feature	O
bootstrapping	O
in	O
the	O
case	O
of	O
a	O
multilingual	O
dialog	O
system	O
.	O
Many	O
large	O
-	O
scale	O
real	O
-	O
world	O
dialog	O
systems	O
,	O
e.g.	O
Apple	O
's	O
Siri	O
,	O
Amazon	O
's	O
Alexa	O
and	O
Google	B-DatasetName
's	O
Assistant	O
,	O
support	O
interactions	O
in	O
multiple	O
languages	O
.	O
In	O
such	O
systems	O
,	O
the	O
coverage	O
of	O
languages	O
and	O
the	O
range	O
of	O
features	O
is	O
continuously	O
expanded	O
.	O
That	O
can	O
lead	O
to	O
differences	O
in	O
the	O
supported	O
intent	O
and	O
slot	O
labels	O
across	O
languages	O
,	O
in	O
particular	O
if	O
a	O
new	O
language	O
is	O
added	O
later	O
or	O
if	O
new	O
features	O
are	O
not	O
rolled	O
out	O
to	O
all	O
languages	O
simultaneously	O
.	O
As	O
a	O
consequence	O
,	O
labeled	O
data	O
for	O
a	O
feature	O
can	O
be	O
available	O
in	O
one	O
language	O
,	O
but	O
limited	O
or	O
completely	O
absent	O
in	O
another	O
.	O
With	O
multilingual	O
paraphrase	B-TaskName
generation	I-TaskName
,	O
we	O
can	O
benefit	O
from	O
this	O
setup	O
and	O
improve	O
data	B-TaskName
augmentation	I-TaskName
for	O
data	O
-	O
scarce	O
languages	O
via	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
transfer	I-TaskName
from	O
data	O
-	O
rich	O
languages	O
.	O
As	O
a	O
result	O
,	O
the	O
data	B-TaskName
augmentation	I-TaskName
can	O
not	O
only	O
be	O
applied	O
with	O
seed	O
data	O
,	O
i.e.	O
in	O
a	O
few	O
-	O
shot	O
setting	O
,	O
but	O
even	O
under	O
zero	O
-	O
shot	O
conditions	O
with	O
no	O
seeds	B-DatasetName
at	O
all	O
for	O
the	O
target	O
language	O
.	O
To	O
address	O
this	O
setup	O
,	O
we	O
follow	O
the	O
recent	O
work	O
of	O
Jolly	O
et	O
al	O
(	O
2020	O
)	O
,	O
which	O
proposes	O
to	O
use	O
an	O
encoder	O
-	O
decoder	O
model	O
that	O
maps	O
from	O
structured	O
meaning	O
representations	O
to	O
corresponding	O
utterances	O
.	O
Because	O
such	O
an	O
input	O
is	O
language	O
-	O
agnostic	O
,	O
it	O
is	O
particularly	O
well	O
-	O
suited	O
for	O
the	O
multilingual	O
setup	O
.	O
We	O
make	O
the	O
following	O
extensions	O
:	O
First	O
,	O
we	O
port	O
their	O
model	O
to	O
a	O
transformer	O
-	O
based	O
architecture	O
and	O
allow	O
multilingual	O
training	O
by	O
adding	O
the	O
desired	O
target	O
language	O
as	O
a	O
new	O
input	O
to	O
the	O
conditional	O
generation	O
.	O
Second	O
,	O
we	O
let	O
the	O
model	O
generate	O
slot	O
labels	O
along	O
with	O
tokens	O
to	O
alleviate	O
the	O
need	O
for	O
additional	O
slot	O
projection	O
techniques	O
.	O
And	O
third	O
,	O
we	O
introduce	O
improved	O
paraphrase	O
decoding	O
methods	O
that	O
leverage	O
a	O
model	O
-	O
based	O
selec	O
-	O
tion	O
strategy	O
.	O
With	O
that	O
,	O
we	O
are	O
able	O
to	O
generate	O
labeled	O
data	O
for	O
a	O
new	O
feature	O
even	O
in	O
the	O
zero	O
-	O
shot	O
setting	O
where	O
no	O
seeds	B-DatasetName
are	O
available	O
at	O
all	O
.	O
We	O
evaluate	O
our	O
approach	O
by	O
simulating	O
a	O
crosslingual	O
feature	O
bootstrapping	O
setting	O
,	O
either	O
fewshot	O
or	O
zero	O
-	O
shot	O
,	O
on	O
MultiATIS	O
,	O
a	O
common	O
IC	O
/	O
SL	O
benchmark	O
spanning	O
nine	O
languages	O
.	O
The	O
experiments	O
compare	O
against	O
several	O
alternative	O
methods	O
,	O
including	O
previous	O
work	O
for	O
mono	O
-	O
lingual	O
paraphrase	B-TaskName
generation	I-TaskName
and	O
machine	B-TaskName
translation	I-TaskName
.	O
We	O
find	O
that	O
our	O
method	O
produces	O
paraphrases	O
of	O
high	O
novelty	O
and	O
diversity	O
and	O
using	O
it	O
for	O
IC	O
/	O
SL	O
training	O
shows	O
promising	O
downstream	O
classification	O
performance	O
.	O

In	O
order	O
to	O
generate	O
paraphrases	O
,	O
we	O
train	O
a	O
multilingual	O
paraphrase	B-TaskName
generation	I-TaskName
model	O
that	O
generates	O
a	O
paraphrase	O
given	O
a	O
language	O
,	O
an	O
intent	O
and	O
a	O
set	O
of	O
slot	O
types	O
.	O
The	O
model	O
architecture	O
is	O
outlined	O
in	O
Figure	O
1	O
.	O
The	O
model	O
uses	O
self	O
-	O
attention	O
based	O
encoder	O
and	O
decoder	O
similar	O
to	O
the	O
transformer	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
encoder	O
of	O
the	O
model	O
receives	O
as	O
input	O
the	O
language	O
embedding	O
and	O
the	O
intent	O
embedding	O
,	O
which	O
are	O
added	O
to	O
the	O
slot	O
embedding	O
.	O
Unlike	O
the	O
transformer	O
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
,	O
we	O
do	O
not	O
use	O
the	O
positional	O
embedding	O
in	O
the	O
encoder	O
.	O
This	O
is	O
because	O
the	O
order	O
of	O
the	O
slot	O
types	O
in	O
the	O
input	O
sequence	O
does	O
not	O
matter	O
and	O
is	O
thus	O
made	O
indistinguishable	O
for	O
the	O
encoder	O
.	O
In	O
order	O
to	O
generate	O
paraphrases	O
which	O
can	O
be	O
used	O
for	O
data	B-TaskName
augmentation	I-TaskName
,	O
we	O
would	O
need	O
the	O
slot	O
annotations	O
and	O
the	O
intents	O
of	O
the	O
generations	O
.	O
Note	O
that	O
we	O
already	O
know	O
the	O
intent	O
of	O
the	O
generated	O
paraphrase	O
since	O
it	O
is	O
the	O
same	O
intent	O
as	O
specified	O
while	O
generating	O
it	O
.	O
The	O
slot	O
annotations	O
,	O
however	O
,	O
are	O
not	O
readily	O
obtained	O
from	O
the	O
input	O
slot	O
types	O
.	O
We	O
can	O
make	O
the	O
slot	O
annotations	O
part	O
of	O
the	O
output	O
sequence	O
by	O
generating	O
the	O
slot	O
label	O
in	O
BIO	O
format	O
in	O
every	O
alternate	O
time	O
step	O
,	O
which	O
would	O
be	O
the	O
slot	O
label	O
for	O
the	O
token	O
generated	O
in	O
the	O
previous	O
time	O
step	O
.	O
This	O
enables	O
the	O
model	O
to	O
generate	O
the	O
slot	O
annotations	O
along	O
with	O
the	O
paraphrase	O
.	O
An	O
illustrative	O
example	O
is	O
shown	O
in	O
Figure	O
1	O
.	O

The	O
generated	O
paraphrases	O
can	O
be	O
used	O
to	O
augment	O
the	O
existing	O
training	O
data	O
.	O
Since	O
the	O
training	O
data	O
we	O
use	O
is	O
highly	O
imbalanced	O
,	O
data	B-TaskName
augmentation	I-TaskName
might	O
lead	O
to	O
disturbance	O
in	O
the	O
original	O
intent	O
distribution	O
.	O
To	O
ensure	O
that	O
the	O
data	B-TaskName
augmentation	I-TaskName
process	O
does	O
not	O
disturb	O
the	O
original	O
intent	O
distribution	O
,	O
we	O
compute	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
to	O
augment	O
using	O
the	O
following	O
constraint	O
:	O
the	O
ratio	O
of	O
target	O
intent	O
to	O
other	O
intents	O
for	O
the	O
target	O
language	O
should	O
be	O
the	O
same	O
as	O
the	O
ratio	O
of	O
target	O
intent	O
to	O
other	O
intents	O
in	O
the	O
source	O
language	O
.	O
Sometimes	O
,	O
using	O
the	O
above	O
constraint	O
results	O
in	O
a	O
negligible	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
for	O
augmentation	O
,	O
in	O
which	O
cases	O
we	O
use	O
a	O
minimal	O
number	B-HyperparameterName
of	I-HyperparameterName
samples	I-HyperparameterName
(	O
see	O
experiments	O
)	O
.	O

In	O
addition	O
to	O
deciding	O
how	O
many	O
paraphrases	O
to	O
augment	O
,	O
it	O
is	O
also	O
crucial	O
to	O
decide	O
which	O
paraphrases	O
to	O
use	O
.	O
Preliminary	O
experimental	O
results	O
showed	O
that	O
samping	O
uniformly	O
from	O
all	O
generated	O
paraphrases	O
does	O
not	O
lead	O
to	O
improvement	O
over	O
the	O
baseline	O
.	O
Upon	O
manual	O
examination	O
we	O
found	O
that	O
not	O
all	O
the	O
paraphrases	O
belong	O
to	O
the	O
desired	O
target	O
intent	O
.	O
To	O
cope	O
with	O
that	O
problem	O
,	O
we	O
use	O
the	O
baseline	O
downstream	O
intent	B-TaskName
classification	I-TaskName
and	O
slot	O
labeling	O
model	O
,	O
which	O
is	O
trained	O
only	O
on	O
the	O
existing	O
data	O
,	O
to	O
compute	O
the	O
likelihood	O
of	O
the	O
generated	O
paraphrases	O
to	O
belong	O
to	O
the	O
target	O
intent	O
.	O
We	O
rank	O
all	O
the	O
generated	O
paraphrases	O
based	O
on	O
these	O
probabilities	O
and	O
select	O
from	O
the	O
top	O
of	O
the	O
pool	O
for	O
augmentation	O
of	O
the	O
seed	O
data	O
.	O

We	O
use	O
the	O
MultiATIS++	O
data	O
(	O
Xu	O
et	O
al	O
,	O
2020	O
)	O
,	O
a	O
parallel	O
IC	O
/	O
SL	O
corpus	O
that	O
was	O
created	O
by	O
translating	O
the	O
original	O
English	O
dataset	O
.	O
It	O
covers	O
a	O
total	O
of	O
9	O
languages	O
:	O
English	O
,	O
Hindi	O
,	O
Turkish	O
,	O
German	O
,	O
French	O
,	O
Portuguese	O
,	O
Spanish	O
,	O
Japanese	O
and	O
Chinese	O
.	O
The	O
languages	O
encompass	O
a	O
diverse	O
set	O
of	O
language	O
families	O
:	O
Indo	O
-	O
European	O
,	O
Sino	O
-	O
Tibetan	O
,	O
Japonic	O
and	O
Altaic	O
.	O
Choosing	O
target	O
intents	O
To	O
reduce	O
the	O
number	O
of	O
experiments	O
,	O
we	O
only	O
choose	O
three	O
different	O
intents	O
for	O
simulating	O
the	O
feature	O
bootstrapping	O
scenario	O
.	O
The	O
MultiATIS++	O
dataset	O
is	O
highly	O
imbalanced	O
in	O
terms	O
of	O
intent	O
frequencies	O
.	O
For	O
instance	O
,	O
74	O
%	O
of	O
the	O
English	O
training	O
data	O
has	O
the	O
intent	O
atis_flight	O
and	O
as	O
many	O
as	O
9	O
intents	O
have	O
less	O
than	O
20	O
training	O
samples	O
.	O
The	O
trend	O
is	O
similar	O
for	O
the	O
non	O
-	O
English	O
languages	O
.	O
For	O
choosing	O
target	O
intents	O
for	O
simulating	O
the	O
zero	O
shot	O
and	O
few	O
shot	O
training	O
data	O
,	O
we	O
therefore	O
consider	O
the	O
following	O
three	O
target	O
intents	O
:	O
(	O
a	O
)	O
atis_airfare	O
,	O
which	O
is	O
highly	O
frequent	O
,	O
(	O
b	O
)	O
atis_airline	O
,	O
which	O
has	O
medium	O
frequency	O
,	O
and	O
(	O
c	O
)	O
atis_city	O
which	O
is	O
scarce	O
.	O
Preprocessing	O
We	O
remove	O
the	O
samples	O
in	O
the	O
MultiATIS++	O
data	O
for	O
which	O
the	O
number	O
of	O
tokens	O
and	O
the	O
number	O
of	O
slot	O
values	O
do	O
not	O
match	O
.	O
1	O
We	O
also	O
only	O
consider	O
the	O
first	O
intent	O
for	O
the	O
samples	O
that	O
have	O
multiple	O
intent	O
annotations	O
.	O
We	O
show	O
the	O
data	O
sizes	O
after	O
preprocessing	O
in	O
Table	O
1	O
.	O
Training	O
setup	O
To	O
simulate	O
the	O
feature	O
bootstrapping	O
scenario	O
,	O
we	O
consider	O
only	O
20	O
samples	O
(	O
few	O
shot	O
setup	O
)	O
or	O
no	O
samples	O
at	O
all	O
(	O
zero	O
shot	O
setup	O
)	O
from	O
the	O
MultiATIS++	O
data	O
for	O
a	O
specific	O
target	O
intent	O
in	O
a	O
target	O
language	O
.	O
2	O
Language	O
setup	O
We	O
use	O
English	O
as	O
the	O
source	O
language	O
and	O
consider	O
8	O
target	O
languages	O
(	O
Hindi	O
,	O
Turkish	O
,	O
German	O
,	O
French	O
,	O
Portuguese	O
,	O
Spanish	O
,	O
Japanese	O
,	O
Chinese	O
)	O
simultaneously	O
.	O
This	O
encourages	O
the	O
model	O
parameters	O
to	O
be	O
shared	O
across	O
all	O
the	O
9	O
languages	O
including	O
the	O
source	O
language	O
English	O
.	O
The	O
purpose	O
of	O
this	O
setup	O
is	O
to	O
enable	O
us	O
to	O
study	O
the	O
knowledge	O
transfer	O
across	O
multiple	O
target	O
languages	O
in	O
addition	O
to	O
that	O
from	O
the	O
source	O
language	O
.	O
We	O
train	O
a	O
single	O
model	O
for	O
paraphrase	B-TaskName
generation	I-TaskName
on	O
all	O
the	O
languages	O
as	O
well	O
as	O
a	O
single	O
multi	O
-	O
lingual	O
downstream	O
IC	O
/	O
SL	O
model	O
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
a	O
multilingual	O
paraphrase	B-TaskName
generation	I-TaskName
model	O
that	O
can	O
be	O
used	O
for	O
feature	O
bootstrapping	O
with	O
or	O
without	O
seed	O
data	O
in	O
the	O
target	O
language	O
.	O
In	O
addition	O
to	O
generating	O
a	O
paraphrase	O
,	O
the	O
model	O
also	O
generates	O
the	O
associated	O
slot	O
labels	O
,	O
enabling	O
the	O
generation	O
to	O
be	O
used	O
directly	O
for	O
data	B-TaskName
augmentation	I-TaskName
to	O
existing	O
training	O
data	O
.	O
Our	O
method	O
is	O
language	O
agnostic	O
and	O
scalable	O
,	O
with	O
no	O
dependencies	O
on	O
pre	O
-	O
trained	O
models	O
or	O
additional	O
data	O
.	O
We	O
validate	O
our	O
method	O
using	O
experiments	O
on	O
the	O
MultiATIS++	O
dataset	O
containing	O
utterances	O
spanning	O
9	O
languages	O
.	O
Intrinsic	O
evaluation	O
shows	O
that	O
paraphrases	O
generated	O
using	O
our	O
approach	O
have	O
higher	O
novelty	O
and	O
diversity	O
in	O
comparison	O
to	O
CVAE	B-MethodName
seq2seq	B-MethodName
based	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
Additionally	O
,	O
downstream	O
evaluation	O
shows	O
that	O
using	O
the	O
generated	O
paraphrases	O
for	O
data	B-TaskName
augmentation	I-TaskName
results	O
in	O
improvements	O
over	O
baseline	O
and	O
related	O
techniques	O
in	O
a	O
wide	O
range	O
of	O
languages	O
and	O
setups	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
successful	O
exploration	O
of	O
generating	O
paraphrases	O
for	O
SLU	O
in	O
a	O
cross	O
-	O
lingual	O
setup	O
.	O
In	O
the	O
future	O
,	O
we	O
would	O
like	O
to	O
explore	O
strategies	O
to	O
exploit	O
monolingual	O
data	O
in	O
the	O
target	O
languages	O
to	O
further	O
refine	O
the	O
paraphrase	B-TaskName
generation	I-TaskName
.	O
We	O
would	O
also	O
like	O
to	O
leverage	O
pre	O
-	O
trained	O
multilingual	O
text	O
-	O
to	O
-	O
text	O
models	O
such	O
as	O
mT5	B-MethodName
(	O
Xue	O
et	O
al	O
,	O
2020	O
)	O
for	O
multilingual	O
paraphrase	B-TaskName
generation	I-TaskName
in	O
the	O
dialog	O
system	O
domain	O
.	O

We	O
look	O
into	O
the	O
task	O
of	O
generalizing	O
word	B-TaskName
embeddings	I-TaskName
:	O
given	O
a	O
set	O
of	O
pre	O
-	O
trained	O
word	O
vectors	O
over	O
a	O
finite	O
vocabulary	O
,	O
the	O
goal	O
is	O
to	O
predict	O
embedding	O
vectors	O
for	O
out	O
-	O
of	O
-	O
vocabulary	O
words	O
,	O
without	O
extra	O
contextual	O
information	O
.	O
We	O
rely	O
solely	O
on	O
the	O
spellings	O
of	O
words	O
and	O
propose	O
a	O
model	O
,	O
along	O
with	O
an	O
efficient	O
algorithm	O
,	O
that	O
simultaneously	O
models	O
subword	O
segmentation	O
and	O
computes	O
subword	O
-	O
based	O
compositional	O
word	O
embedding	O
.	O
We	O
call	O
the	O
model	O
probabilistic	O
bag	O
-	O
of	O
-	O
subwords	O
(	O
PBoS	O
)	O
,	O
as	O
it	O
applies	O
bag	O
-	O
of	O
-	O
subwords	O
for	O
all	O
possible	O
segmentations	O
based	O
on	O
their	O
likelihood	O
.	O
Inspections	O
and	O
affix	O
prediction	O
experiment	O
show	O
that	O
PBoS	O
is	O
able	O
to	O
produce	O
meaningful	O
subword	O
segmentations	O
and	O
subword	O
rankings	O
without	O
any	O
source	O
of	O
explicit	O
morphological	O
knowledge	O
.	O
Word	B-TaskName
similarity	I-TaskName
and	O
POS	O
tagging	O
experiments	O
show	O
clear	O
advantages	O
of	O
PBoS	O
over	O
previous	O
subword	O
-	O
level	O
models	O
in	O
the	O
quality	O
of	O
generated	O
word	B-TaskName
embeddings	I-TaskName
across	O
languages	O
.	O

Following	O
the	O
above	O
intuition	O
,	O
in	O
this	O
section	O
we	O
describe	O
the	O
PBoS	O
model	O
in	O
detail	O
.	O
We	O
first	O
develop	O
a	O
model	O
that	O
segments	O
a	O
word	O
into	O
subword	O
and	O
associates	O
each	O
subword	O
segmentation	O
with	O
a	O
likelihood	O
based	O
on	O
the	O
meaningfulness	O
of	O
each	O
subword	O
segment	O
.	O
We	O
then	O
apply	O
BoS	O
over	O
each	O
segmentation	O
to	O
compose	O
a	O
"	O
segmentation	O
vector	O
"	O
.	O
The	O
final	O
word	O
embedding	O
vector	O
is	O
then	O
the	O
probabilistic	O
expectation	O
of	O
all	O
the	O
segmentation	O
vectors	O
.	O
The	O
subword	O
segmentation	O
and	O
likelihood	O
association	O
part	O
require	O
no	O
explicit	O
source	O
of	O
morphological	O
knowledge	O
and	O
are	O
tightly	O
integrated	O
with	O
the	O
word	O
vector	O
composition	O
part	O
,	O
which	O
in	O
turn	O
gives	O
rise	O
to	O
an	O
efficient	O
algorithm	O
that	O
considers	O
all	O
possible	O
segmentations	O
simultaneously	O
(	O
Section	O
3	O
)	O
.	O
The	O
model	O
can	O
be	O
trained	O
by	O
fitting	O
a	O
set	O
of	O
pre	O
-	O
trained	O
word	B-TaskName
embeddings	I-TaskName
.	O

A	O
subword	O
transition	O
graph	O
for	O
word	O
w	O
is	O
a	O
directed	O
acyclic	O
graph	O
G	O
w	O
=	O
(	O
N	O
w	O
,	O
E	O
w	O
)	O
.	O
Let	O
l	O
=	O
|	O
w	O
|	O
.	O
The	O
vertices	O
N	O
w	O
=	O
{	O
0	B-DatasetName
,	O
.	O
.	O
.	O
,	O
l	O
}	O
correspond	O
to	O
the	O
positions	O
between	O
w	O
[	O
i	O
]	O
and	O
w	O
[	O
i	O
+	O
1	O
]	O
for	O
all	O
i	O
[	O
l	O
−	O
1	O
]	O
,	O
as	O
well	O
as	O
to	O
the	O
beginning	O
(	O
vertiex	O
0	B-DatasetName
)	O
and	O
the	O
end	O
(	O
vertex	O
l	O
)	O
of	O
w.	O
Each	O
edge	O
(	O
i	O
,	O
j	O
)	O
E	O
w	O
=	O
{	O
(	O
i	O
,	O
j	O
)	O
:	O
0	B-DatasetName
≤	O
i	O
<	O
j	O
≤	O
l	O
}	O
corresponds	O
to	O
subword	O
w	O
[	O
i	O
:	O
j	O
]	O
.	O
We	O
use	O
G	O
w	O
as	O
a	O
useful	O
image	O
for	O
developing	O
our	O
model	O
.	O
Proposition	O
1	O
.	O
Paths	O
from	O
0	B-DatasetName
to	O
|	O
w	O
|	O
in	O
G	O
w	O
are	O
in	O
one	O
-	O
to	O
-	O
one	O
correspondence	O
to	O
segmentations	O
of	O
w.	O
Proposition	O
2	O
.	O
There	O
are	O
2	O
|	O
w	O
|	O
−1	O
different	O
possible	O
segmentations	O
for	O
word	O
w.	O
Each	O
edge	O
(	O
i	O
,	O
j	O
)	O
is	O
associated	O
with	O
a	O
weight	O
p	O
w	O
[	O
i	O
:	O
j	O
]	O
-	O
how	O
likely	O
w	O
[	O
i	O
:	O
j	O
]	O
itself	O
is	O
a	O
meaningful	O
subword	O
.	O
We	O
model	O
the	O
likelihood	O
of	O
segmentation	O
g	O
being	O
a	O
segmentation	O
of	O
w	O
as	O
being	O
proportional	O
to	O
the	O
product	O
of	O
all	O
its	O
subword	O
likelihood	O
-	O
the	O
0	B-DatasetName
1	O
2	O
3	O
4	O
5	O
6	O
h	O
p	O
"	O
h	O
"	O
i	O
p	O
"	O
i	O
"	O
g	O
p	O
"	O
g	O
"	O
h	O
p	O
"	O
h	O
"	O
e	O
p	O
"	O
e	O
"	O
r	O
p	O
"	O
r	O
"	O
hi	O
p	O
"	O
hi	O
"	O
gher	O
p	O
"	O
gher	O
"	O
gh	O
p	O
"	O
gh	O
"	O
her	O
p	O
"	O
her	O
"	O
high	O
p	O
"	O
high	O
"	O
er	O
p	O
"	O
er	O
"	O
Figure	O
1	O
:	O
Diagram	O
of	O
probabilistic	O
subwords	O
transitions	O
for	O
word	O
"	O
higher	O
"	O
.	O
Some	O
edges	O
are	O
omitted	O
to	O
reduce	O
clutter	O
.	O
Each	O
edge	O
is	O
labeled	O
by	O
a	O
subword	O
s	O
of	O
the	O
word	O
,	O
associated	O
with	O
ps	O
.	O
Bold	O
edges	O
constituent	O
a	O
path	O
from	O
node	O
0	B-DatasetName
to	O
6	O
,	O
corresponding	O
to	O
the	O
segmentation	O
of	O
the	O
word	O
into	O
"	O
high	O
"	O
and	O
"	O
er	O
"	O
.	O
transition	O
along	O
a	O
path	O
from	O
0	B-DatasetName
to	O
|	O
w	O
|	O
in	O
G	O
w	O
:	O
p	O
g	O
|	O
w	O
∝	O
s	O
g	O
p	O
s	O
.	O
(	O
2	O
)	O
Example	O
.	O
Figure	O
1	O
illustrates	O
G	O
w	O
for	O
word	O
w	O
=	O
"	O
higher	O
"	O
of	O
length	O
6	O
.	O
Bold	O
edges	O
(	O
0	B-DatasetName
,	O
4	O
)	O
and	O
(	O
4	O
,	O
6	O
)	O
form	O
a	O
path	O
from	O
0	B-DatasetName
to	O
6	O
,	O
which	O
corresponds	O
to	O
the	O
segmentation	O
(	O
"	O
high	O
"	O
,	O
"	O
er	O
"	O
)	O
.	O
The	O
likelihood	O
p	O
(	O
"	O
high	O
"	O
,	O
"	O
er	O
"	O
)	O
|	O
w	O
of	O
this	O
particular	O
segmentation	O
is	O
proportional	O
to	O
p	O
"	O
high	O
"	O
p	O
"	O
er	O
"	O
-	O
the	O
product	O
of	O
weights	O
along	O
the	O
path	O
.	O

We	O
design	O
experiments	O
to	O
answer	O
two	O
questions	O
:	O
Do	O
the	O
segmentation	O
likelihood	O
and	O
subword	O
weights	O
computed	O
by	O
PBoS	O
align	O
with	O
their	O
meaningfulness	O
?	O
Are	O
the	O
word	O
embedding	O
vectors	O
generated	O
by	O
PBoS	O
of	O
good	O
quality	O
?	O
For	O
the	O
former	O
,	O
we	O
inspect	O
segmentation	O
results	O
and	O
subword	O
weights	O
(	O
Section	O
4.1	O
)	O
,	O
and	O
see	O
how	O
good	O
they	O
are	O
at	O
predicting	O
word	O
affixes	O
(	O
Section	O
4.2	O
)	O
.	O
For	O
the	O
latter	O
,	O
we	O
evaluate	O
the	O
word	B-TaskName
embeddings	I-TaskName
composed	O
by	O
PBoS	O
at	O
word	B-TaskName
similarity	I-TaskName
task	O
(	O
Section	O
4.3	O
)	O
and	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
(	O
POS	O
)	O
tagging	O
task	O
(	O
Section	O
4.4	O
)	O
.	O
Due	O
to	O
the	O
page	O
limit	O
,	O
we	O
only	O
report	O
the	O
most	O
relevant	O
settings	O
and	O
results	O
in	O
this	O
section	O
.	O
Other	O
details	O
,	O
including	O
hardware	O
,	O
running	O
time	O
and	O
detailed	O
list	O
of	O
hyperparameters	O
,	O
can	O
be	O
found	O
in	O
Appendix	O
A.	O

In	O
this	O
subsection	O
,	O
we	O
provide	O
anecdotal	O
evidence	O
that	O
PBoS	O
is	O
able	O
to	O
assign	O
meaningful	O
segmentation	O
likelihood	O
and	O
subword	O
weights	O
.	O
Table	O
1	O
shows	O
top	O
subword	O
segmentations	O
and	O
subsequent	O
top	O
subwords	O
calculated	O
by	O
PBoS	O
for	O
some	O
example	O
word	O
,	O
ranked	O
by	O
their	O
likelihood	O
and	O
weights	O
respectively	O
.	O
The	O
calculation	O
is	O
based	O
on	O
the	O
word	O
frequency	O
derived	O
from	O
the	O
Google	B-DatasetName
Web	O
Trillion	O
Word	O
Corpus	O
6	O
.	O
We	O
use	O
the	O
same	O
list	O
for	O
word	O
probability	O
p	O
w	O
throughout	O
our	O
experiments	O
if	O
not	O
otherwise	O
mentioned	O
.	O
All	O
other	O
settings	O
are	O
the	O
same	O
as	O
described	O
for	O
PBoS	O
in	O
Section	O
4.3	O
.	O
We	O
can	O
see	O
the	O
segmentation	O
likelihood	O
and	O
subword	O
weight	O
favors	O
the	O
whole	O
words	O
as	O
subword	O
segments	O
if	O
the	O
word	O
appears	O
in	O
the	O
word	O
list	O
,	O
e.g.	O
"	O
higher	O
"	O
,	O
"	O
farmland	O
"	O
.	O
This	O
allows	O
the	O
model	O
to	O
closely	O
mimic	O
the	O
word	B-TaskName
embeddings	I-TaskName
for	O
frequent	O
words	O
that	O
are	O
probably	O
part	O
of	O
the	O
target	O
vectors	O
.	O
Second	O
to	O
the	O
whole	O
-	O
word	O
segmentation	O
,	O
or	O
when	O
the	O
word	O
is	O
rare	O
,	O
e.g.	O
"	O
penpineanpplepie	O
"	O
,	O
"	O
paradichlorobenzene	O
"	O
,	O
we	O
see	O
that	O
PBoS	O
gives	O
higher	O
likelihood	O
to	O
meaningful	O
segmentations	O
such	O
as	O
"	O
high	O
/	O
er	O
"	O
,	O
"	O
farm	O
/	O
land	O
"	O
,	O
"	O
pen	O
/	O
pineapple	O
/	O
pie	O
"	O
and	O
"	O
para	O
/	O
dichlorobenzene"against	O
other	O
possible	O
segmentations	O
.	O
7	O
Subsequently	O
,	O
respective	O
subword	O
segments	O
get	O
higher	O
weights	O
among	O
all	O
possible	O
subwords	O
for	O
the	O
word	O
,	O
often	O
by	O
a	O
good	O
amount	O
.	O
This	O
behavior	O
would	O
help	O
PBoS	O
to	O
focus	O
on	O
meaningful	O
subwords	O
when	O
composing	O
word	O
embedding	O
.	O
The	O
fact	O
that	O
this	O
can	O
be	O
achieved	O
without	O
any	O
explicit	O
source	O
of	O
morphological	O
knowledge	O
is	O
itself	O
interesting	O
.	O

Popular	O
word	O
embedding	O
methods	O
,	O
such	O
as	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
,	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
,	O
2014	O
)	O
,	O
often	O
assume	O
finite	O
-	O
size	O
vocabularies	O
,	O
giving	O
rise	O
to	O
the	O
problem	O
of	O
OOV	O
words	O
.	O
FastText	B-MethodName
attempted	O
to	O
alleviate	O
the	O
problem	O
using	O
subword	O
-	O
level	O
model	O
,	O
and	O
was	O
followed	O
by	O
interests	O
of	O
using	O
subword	O
information	O
to	O
improve	O
word	O
embedding	O
(	O
Wieting	O
et	O
al	O
,	O
2016	O
;	O
Cao	O
and	O
Lu	O
,	O
2017	O
;	O
Li	O
et	O
al	O
,	O
2017	O
;	O
Athiwaratkun	O
et	O
al	O
,	O
2018	O
;	O
Li	O
et	O
al	O
,	O
2018	O
;	O
Salle	O
and	O
Villavicencio	O
,	O
2018	O
;	O
Xu	O
et	O
al	O
,	O
2019	O
;	O
Zhu	O
et	O
al	O
,	O
2019	O
)	O
.	O
Among	O
them	O
are	O
Charagram	O
by	O
Wieting	O
et	O
al	O
(	O
2016	O
)	O
which	O
,	O
albeit	O
trained	O
on	O
specific	O
downstream	O
tasks	O
,	O
is	O
similar	O
to	O
BoS	O
followed	O
by	O
a	O
non	O
-	O
linear	O
activation	O
,	O
and	O
the	O
systematic	O
evaluation	O
by	O
Zhu	O
et	O
al	O
(	O
2019	O
)	O
over	O
various	O
choices	O
of	O
word	O
composition	O
functions	O
and	O
subword	O
segmentation	O
methods	O
.	O
However	O
,	O
all	O
works	O
above	O
either	O
pay	O
little	O
attention	O
to	O
the	O
interaction	O
among	O
subwords	O
inside	O
a	O
given	O
word	O
,	O
or	O
treat	O
subword	O
segmentation	O
and	O
composing	O
word	O
representation	O
as	O
separate	O
problems	O
.	O
Another	O
interesting	O
thread	O
of	O
works	O
(	O
Oshikiri	O
,	O
2017	O
;	O
Kim	O
et	O
al	O
,	O
2018aKim	O
et	O
al	O
,	O
,	O
2019	O
attempted	O
to	O
model	O
language	O
solely	O
at	O
the	O
subword	O
level	O
and	O
learn	O
subword	O
embeddings	O
directly	O
from	O
text	O
,	O
providing	O
evidence	O
to	O
the	O
power	O
of	O
subword	O
-	O
level	O
models	O
,	O
especially	O
as	O
the	O
notion	O
of	O
word	O
is	O
thought	O
doubtful	O
by	O
some	O
linguistics	O
(	O
Haspelmath	O
,	O
2011	O
)	O
.	O
Besides	O
the	O
recent	O
interest	O
in	O
subwords	O
,	O
there	O
have	O
been	O
long	O
efforts	O
of	O
using	O
morphology	O
to	O
improve	O
word	O
embedding	O
(	O
Luong	O
et	O
al	O
,	O
2013	O
;	O
Cotterell	O
and	O
Schütze	O
,	O
2015	O
;	O
Cui	O
et	O
al	O
,	O
2015	O
;	O
Soricut	O
and	O
Och	O
,	O
2015	O
;	O
Bhatia	O
et	O
al	O
,	O
2016	O
;	O
Cao	O
and	O
Rei	O
,	O
2016	O
;	O
Xu	O
et	O
al	O
,	O
2018	O
;	O
Üstün	O
et	O
al	O
,	O
2018	O
;	O
Edmiston	O
and	O
Stratos	O
,	O
2018	O
;	O
Chaudhary	O
et	O
al	O
,	O
2018	O
;	O
Park	O
and	O
Shin	O
,	O
2018	O
)	O
.	O
However	O
,	O
most	O
of	O
them	O
require	O
an	O
external	O
oracle	O
,	O
such	O
as	O
Morfessor	O
(	O
Creutz	O
and	O
Lagus	O
,	O
2002	O
;	O
Virpioja	O
et	O
al	O
,	O
2013	O
)	O
,	O
for	O
the	O
morphological	O
segmentations	O
of	O
input	O
words	O
,	O
limiting	O
their	O
power	O
to	O
the	O
quality	O
and	O
availability	O
of	O
such	O
segmenters	O
.	O
The	O
only	O
exception	O
is	O
the	O
character	O
LSTM	B-MethodName
model	O
by	O
Cao	O
and	O
Rei	O
(	O
2016	O
)	O
,	O
which	O
has	O
shown	O
some	O
ability	O
to	O
recover	O
the	O
morphological	O
boundary	O
as	O
a	O
byproduct	O
of	O
learning	O
word	O
embedding	O
.	O
The	O
most	O
related	O
works	O
in	O
generalizing	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
have	O
been	O
discussed	O
in	O
Section	O
1	O
and	O
compared	O
throughout	O
the	O
paper	O
.	O

We	O
propose	O
PBoS	O
model	O
for	O
generalizing	O
pretrained	O
word	B-TaskName
embeddings	I-TaskName
without	O
contextual	O
information	O
.	O
PBoS	O
simultaneously	O
considers	O
all	O
possible	O
subword	O
segmentations	O
of	O
a	O
word	O
and	O
derives	O
meaningful	O
subword	O
weights	O
that	O
lead	O
to	O
better	O
composed	O
word	B-TaskName
embeddings	I-TaskName
.	O
Experiments	O
on	O
segmentation	O
results	O
,	O
affix	O
prediction	O
,	O
word	B-TaskName
similarity	I-TaskName
,	O
and	O
POS	O
tagging	O
over	O
23	O
languages	O
support	O
the	O
claim	O
.	O
In	O
the	O
future	O
,	O
it	O
would	O
be	O
interesting	O
to	O
see	O
if	O
PBoS	O
can	O
also	O
help	O
with	O
the	O
task	O
of	O
learning	O
word	O
embedding	O
,	O
and	O
how	O
hashing	O
would	O
impact	O
the	O
quality	O
of	O
composed	O
embedding	O
while	O
facilitating	O
a	O
more	O
compact	O
model	O
.	O

epochs	O
:	O
The	O
number	O
of	O
training	O
epochs	O
.	O
lr	O
:	O
Learning	B-HyperparameterName
rate	I-HyperparameterName
.	O
lr	O
decay	O
:	O
Whether	O
to	O
set	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
be	O
inversely	O
proportional	O
to	O
the	O
square	O
root	O
of	O
the	O
epoch	B-HyperparameterName
number	I-HyperparameterName
.	O
normalize	O
semb	O
:	O
Whether	O
to	O
normalize	O
subword	O
embeddings	O
before	O
composing	O
word	B-TaskName
embeddings	I-TaskName
.	O
prob	O
eps	B-HyperparameterName
:	O
Default	O
likelihood	O
for	O
unknown	O
characters	O
.	O

C	O
:	O
The	O
inverse	O
regularization	O
term	O
used	O
by	O
the	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
.	O

Table	O
6	O
and	O
Table	O
8	O
show	O
the	O
hyperparameter	O
values	O
used	O
in	O
the	O
word	B-TaskName
similarity	I-TaskName
experiment	O
(	O
Section	O
4.3	O
)	O
.	O
We	O
transform	O
all	O
words	O
in	O
the	O
benchmarks	O
into	O
lowercase	O
,	O
following	O
the	O
convention	O
in	O
FastText	B-MethodName
,	O
BoS	O
(	O
Zhao	O
et	O
al	O
,	O
2018	O
)	O
,	O
and	O
KVQ	O
-	O
FH	O
(	O
Sasaki	O
et	O
al	O
,	O
2019	O
)	O
.	O
During	O
the	O
evaluation	O
,	O
we	O
use	O
0	B-DatasetName
as	O
the	O
similarity	O
score	O
for	O
a	O
pair	O
of	O
words	O
if	O
we	O
can	O
not	O
get	O
word	O
vector	O
for	O
one	O
of	O
the	O
words	O
,	O
or	O
the	O
magnitude	O
of	O
the	O
word	O
vector	O
is	O
too	O
small	O
.	O
This	O
is	O
especially	O
the	O
case	O
when	O
we	O
evaluate	O
the	O
target	O
vectors	O
,	O
where	O
OOV	O
rates	O
can	O
be	O
significant	O
.	O
Table	O
9	O
lists	O
experimental	O
result	O
for	O
word	B-TaskName
similarity	I-TaskName
in	O
greater	O
detail	O
.	O
Regarding	O
the	O
training	O
epoch	O
time	O
,	O
note	O
that	O
KVQ	O
-	O
FH	O
uses	O
GPU	O
and	O
is	O
implemented	O
using	O
a	O
deep	O
learning	O
library	O
17	O
with	O
underlying	O
optimized	O
C	O
code	O
,	O
whereas	O
our	O
PBoS	O
is	O
implemented	O
using	O
pure	O
Python	O
and	O
uses	O
only	O
single	O
thread	O
CPU	O
.	O
We	O
omit	O
the	O
prediction	O
time	O
for	O
KVQ	O
-	O
FH	O
,	O
as	O
we	O
found	O
it	O
hard	O
to	O
separate	O
the	O
actual	O
inference	O
time	O
from	O
time	O
used	O
for	O
other	O
processes	O
such	O
as	O
batching	O
and	O
data	O
transfer	O
between	O
CPU	O
and	O
GPU	O
.	O
However	O
,	O
we	O
believe	O
the	O
overall	O
trend	O
should	O
be	O
similar	O
as	O
for	O
the	O
training	O
time	O
.	O
One	O
may	O
notice	O
that	O
the	O
prediction	O
time	O
for	O
BoS	O
in	O
Table	O
9	O
is	O
different	O
from	O
what	O
was	O
reported	O
at	O
the	O
end	O
of	O
Section	O
3	O
.	O
This	O
is	O
largely	O
because	O
the	O
BoS	O
in	O
Table	O
9	O
has	O
a	O
different	O
(	O
smaller	O
)	O
set	O
of	O
possible	O
subwords	O
to	O
consider	O
due	O
to	O
the	O
subword	O
length	O
limits	O
.	O
In	O
Section	O
3	O
,	O
to	O
fairly	O
access	O
the	O
impact	O
of	O
subword	O
weights	O
computation	O
,	O
we	O
ensure	O
that	O
BoS	O
and	O
PBoS	O
work	O
with	O
the	O
same	O
set	O
of	O
possible	O
subwords	O
(	O
that	O
used	O
by	O
PBoS	O
in	O
Section	O
4.3	O
)	O
,	O
and	O
thus	O
observe	O
a	O
slight	O
longer	O
prediction	O
time	O
for	O
BoS.	O

We	O
use	O
Wikipedia2Vec	O
(	O
Yamada	O
et	O
al	O
,	O
2020	O
)	O
as	O
target	O
vectors	O
,	O
and	O
keep	O
the	O
most	O
frequent	O
10k	O
words	O
to	O
get	O
decent	O
OOV	O
rates	O
.	O
The	O
OOV	O
rates	O
and	O
word	B-TaskName
similarity	I-TaskName
scores	O
can	O
be	O
found	O
in	O
Table	O
10	O
.	O
We	O
do	O
not	O
clean	O
or	O
filter	O
words	O
as	O
we	O
did	O
for	O
the	O
English	O
word	B-TaskName
similarity	I-TaskName
,	O
because	O
we	O
found	O
it	O
difficult	O
to	O
have	O
a	O
consistent	O
way	O
of	O
pre	O
-	O
processing	O
words	O
across	O
languages	O
.	O
For	O
PBoS	O
,	O
we	O
use	O
the	O
word	O
frequencies	O
from	O
Polyglot	O
for	O
subword	O
segmentation	O
and	O
subword	O
weight	O
calculation	O
as	O
the	O
same	O
for	O
the	O
multilingual	O
POS	O
tagging	O
experiment	O
(	O
Section	O
4.4	O
)	O
.	O
We	O
evaluate	O
all	O
the	O
models	O
on	O
multilingual	O
Word	O
-	O
Sim353	O
(	O
mWS	O
)	O
and	O
SemLex999	O
(	O
mSL	O
)	O
from	O
Leviant	O
and	O
Reichart	O
(	O
2015	O
)	O
,	O
which	O
is	O
available	O
for	O
English	O
,	O
German	O
,	O
Italian	O
and	O
Russian	O
.	O
The	O
dataset	O
al	O
o	O
contains	O
the	O
relatedness	O
(	O
rel	O
)	O
and	O
similarity	O
(	O
sim	O
)	O
benchmarks	O
derived	O
from	O
mWS	O
.	O
We	O
list	O
the	O
results	O
for	O
multilingual	O
word	B-TaskName
similarity	I-TaskName
in	O
Table	O
11	O
.	O

Knowledge	B-TaskName
Base	I-TaskName
Question	I-TaskName
Answering	I-TaskName
via	O
Encoding	O
of	O
Complex	O
Query	O
Graphs	O

The	O
knowledge	O
-	O
based	O
question	B-TaskName
answering	I-TaskName
(	O
KBQA	O
)	O
is	O
a	O
task	O
which	O
takes	O
a	O
natural	O
language	O
question	O
as	O
input	O
and	O
returns	O
a	O
factual	O
answer	O
using	O
structured	O
knowledge	O
bases	O
such	O
as	O
Freebase	O
(	O
Bollacker	O
et	O
al	O
,	O
2008	O
)	O
,	O
YAGO	B-DatasetName
(	O
Suchanek	O
et	O
al	O
,	O
2007	O
)	O
and	O
DBpedia	B-DatasetName
(	O
Auer	O
et	O
al	O
,	O
2007	O
)	O
.	O
One	O
simple	O
example	O
is	O
a	O
question	O
like	O
this	O
:	O
"	O
What	O
's	O
the	O
capital	O
of	O
the	O
United	O
States	O
?	O
"	O
A	O
common	O
answer	O
to	O
such	O
question	O
is	O
to	O
identify	O
the	O
focus	O
entity	O
and	O
the	O
main	O
relation	O
predicate	O
(	O
or	O
a	O
sequence	O
)	O
in	O
the	O
question	O
,	O
and	O
map	O
the	O
question	O
to	O
a	O
triple	O
fact	O
query	O
(	O
U	O
S	O
,	O
capital	O
,	O
?	O
)	O
over	O
KB	O
.	O
The	O
object	O
answers	O
are	O
returned	O
by	O
executing	O
the	O
query	O
.	O
The	O
mapping	O
above	O
is	O
typically	O
learned	O
from	O
question	O
-	O
answer	O
pairs	O
through	O
distant	O
supervision	O
.	O
While	O
the	O
above	O
question	O
can	O
be	O
answered	O
by	O
querying	O
a	O
single	O
predicate	O
or	O
predicate	O
sequence	O
in	O
the	O
KB	O
,	O
many	O
other	O
more	O
complex	O
questions	O
can	O
not	O
,	O
e.g.	O
the	O
question	O
in	O
Figure	O
1	O
.	O
To	O
answer	O
the	O
question	O
"	O
What	O
is	O
the	O
second	O
longest	O
river	O
in	O
United	O
States	O
"	O
,	O
we	O
need	O
to	O
infer	O
several	O
semantic	O
clues	O
:	O
1	O
)	O
the	O
answer	O
is	O
contained	O
by	O
United	O
States	O
;	O
2	O
)	O
the	O
answer	O
is	O
a	O
river	O
;	O
3	O
)	O
the	O
answer	O
ranks	O
second	O
by	O
its	O
length	O
in	O
descending	O
order	O
.	O
Thus	O
,	O
multiple	O
predicates	O
are	O
required	O
to	O
constrain	O
the	O
answer	O
set	O
,	O
and	O
we	O
call	O
such	O
questions	O
"	O
complex	O
questions	O
"	O
throughout	O
this	O
paper	O
.	O
For	O
answering	O
complex	O
questions	O
,	O
it	O
's	O
more	O
important	O
to	O
understand	O
the	O
compositional	O
semantic	O
meanings	O
of	O
the	O
question	O
.	O
As	O
a	O
classic	O
branch	O
of	O
KBQA	O
solutions	O
,	O
semantic	B-TaskName
parsing	I-TaskName
(	O
SP	O
)	O
technique	O
(	O
Berant	O
et	O
al	O
,	O
2013	O
;	O
Yih	O
et	O
al	O
,	O
2015	O
;	O
Hu	O
et	O
al	O
,	O
2018	O
)	O
aims	O
at	O
learning	O
semantic	O
parse	O
trees	O
or	O
equivalent	O
query	O
graphs	O
1	O
for	O
representing	O
semantic	O
structures	O
of	O
the	O
questions	O
.	O
For	O
example	O
in	O
Figure	O
1	O
,	O
the	O
query	O
graph	O
forms	O
a	O
tree	O
shape	O
.	O
The	O
answer	O
node	O
A	O
,	O
serving	O
as	O
the	O
root	O
of	O
the	O
tree	O
,	O
is	O
the	O
variable	O
vertex	O
that	O
represents	O
the	O
real	O
answer	O
entities	O
.	O
The	O
focus	O
nodes	O
(	O
US	O
,	O
river	O
,	O
2nd	O
)	O
are	O
extracted	O
from	O
the	O
mentions	O
of	O
the	O
question	O
,	O
and	O
they	O
constrain	O
the	O
answer	O
node	O
via	O
predicate	O
sequences	O
in	O
the	O
knowledge	O
base	O
.	O
Recently	O
,	O
neural	O
network	O
(	O
NN	O
)	O
models	O
have	O
shown	O
great	O
promise	O
in	O
improving	O
the	O
performance	O
of	O
KBQA	O
systems	O
,	O
and	O
SP+NN	O
techniques	O
become	O
the	O
stateof	O
-	O
the	O
-	O
art	O
on	O
several	O
KBQA	O
datasets	O
(	O
Qu	O
et	O
al	O
,	O
2018	O
;	O
Bao	O
et	O
al	O
,	O
2016	O
)	O
.	O
According	O
to	O
the	O
discussion	O
above	O
,	O
our	O
work	O
extends	O
the	O
current	O
research	O
in	O
the	O
SP+NN	O
direction	O
.	O
The	O
common	O
step	O
of	O
SP	O
-	O
based	O
approaches	O
is	O
to	O
first	O
collect	O
candidate	O
query	O
graphs	O
using	O
bottom	O
up	O
parsing	O
(	O
Berant	O
et	O
al	O
,	O
2013	O
;	O
Cai	O
and	O
Yates	O
,	O
2013	O
)	O
or	O
staged	O
query	O
generation	O
methods	O
(	O
Yih	O
et	O
al	O
,	O
2015	O
;	O
Bao	O
et	O
al	O
,	O
2016	O
)	O
,	O
then	O
predict	O
the	O
best	O
graph	O
mainly	O
based	O
on	O
the	O
semantic	B-TaskName
similarity	I-TaskName
with	O
the	O
given	O
question	O
.	O
Existing	O
NN	O
-	O
based	O
methods	O
follow	O
an	O
encode	O
-	O
andcompare	O
framework	O
for	O
answering	O
simple	O
questions	O
,	O
where	O
both	O
the	O
question	O
and	O
the	O
predicate	O
sequence	O
are	O
encoded	O
as	O
semantic	O
vectors	O
in	O
a	O
common	O
embedding	O
space	O
,	O
and	O
the	O
semantic	B-TaskName
similarity	I-TaskName
is	O
calculated	O
by	O
the	O
cosine	O
score	O
between	O
vectors	O
.	O
In	O
order	O
to	O
define	O
the	O
similarity	O
function	O
between	O
one	O
question	O
and	O
a	O
complex	O
query	O
graph	O
,	O
an	O
intuitive	O
solution	O
is	O
to	O
split	O
the	O
query	O
graph	O
into	O
multiple	O
semantic	O
components	O
,	O
as	O
the	O
predicate	O
sequences	O
separated	O
by	O
dashed	O
boxes	O
in	O
Figure	O
1	O
.	O
Then	O
previous	O
methods	O
can	O
be	O
applied	O
for	O
modeling	O
the	O
similarity	O
between	O
the	O
question	O
and	O
each	O
part	O
of	O
the	O
graph	O
.	O
However	O
,	O
such	O
approach	O
faces	O
two	O
limitations	O
.	O
First	O
,	O
each	O
semantic	O
component	O
is	O
not	O
directly	O
comparable	O
with	O
the	O
whole	O
question	O
,	O
since	O
it	O
conveys	O
only	O
partial	O
information	O
of	O
the	O
question	O
.	O
Second	O
,	O
and	O
more	O
importantly	O
,	O
the	O
model	O
encodes	O
different	O
components	O
separately	O
,	O
without	O
learning	O
the	O
representation	O
of	O
the	O
whole	O
graph	O
,	O
hence	O
it	O
's	O
not	O
able	O
to	O
capture	O
the	O
compositional	O
semantics	O
in	O
a	O
global	O
perspective	O
.	O
In	O
order	O
to	O
attack	O
the	O
above	O
limitations	O
,	O
we	O
propose	O
a	O
neural	O
network	O
based	O
approach	O
to	O
improve	O
the	O
performance	O
of	O
semantic	B-TaskName
similarity	I-TaskName
measurement	O
in	O
complex	O
question	B-TaskName
answering	I-TaskName
.	O
Given	O
candidate	O
query	O
graphs	O
generated	O
from	O
one	O
question	O
,	O
our	O
model	O
embeds	O
the	O
question	O
surface	O
and	O
predicate	O
sequences	O
into	O
a	O
uniform	O
vector	O
space	O
.	O
The	O
main	O
difference	O
between	O
our	O
approach	O
and	O
previous	O
methods	O
is	O
that	O
we	O
integrate	O
hidden	O
vectors	O
of	O
various	O
semantic	O
components	O
and	O
encode	O
their	O
interaction	O
as	O
the	O
hidden	O
semantics	O
of	O
the	O
entire	O
query	O
graph	O
.	O
In	O
addition	O
,	O
to	O
cope	O
with	O
different	O
semantic	O
components	O
of	O
a	O
query	O
graph	O
,	O
we	O
leverage	O
dependency	B-TaskName
parsing	I-TaskName
information	O
as	O
a	O
complementary	O
of	O
sentential	O
information	O
for	O
question	O
encoding	O
,	O
which	O
makes	O
the	O
model	O
better	O
align	O
each	O
component	O
to	O
the	O
question	O
.	O
The	O
contribution	O
of	O
this	O
paper	O
is	O
summarized	O
below	O
.	O
We	O
propose	O
a	O
light	O
-	O
weighted	O
and	O
effective	O
neural	O
network	O
model	O
to	O
solve	O
complex	O
KBQA	O
task	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
attempt	O
to	O
explicitly	O
encode	O
the	O
complete	O
semantics	O
of	O
a	O
complex	O
query	O
graph	O
(	O
Section	O
2.2	O
)	O
;	O
We	O
leverage	O
dependency	B-TaskName
parsing	I-TaskName
to	O
enrich	O
question	O
representation	O
in	O
the	O
NN	O
model	O
,	O
and	O
conduct	O
thorough	O
investigations	O
to	O
verify	O
its	O
effectiveness	O
(	O
Section	O
2.2.2	O
)	O
;	O
We	O
propose	O
an	O
ensemble	O
method	O
to	O
enrich	O
entity	B-TaskName
linking	I-TaskName
from	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
linking	O
tool	O
,	O
which	O
further	O
improves	O
the	O
performance	O
of	O
the	O
overall	O
task	O
(	O
Section	O
2.3	O
)	O
;	O
We	O
perform	O
comprehensive	O
experiments	O
on	O
multiple	O
QA	O
datasets	O
,	O
and	O
our	O
proposed	O
method	O
consistently	O
outperforms	O
previous	O
approaches	O
on	O
complex	O
questions	O
,	O
and	O
produces	O
competitive	O
results	O
on	O
datasets	O
made	O
up	O
of	O
simple	O
questions	O
(	O
Section	O
3	O
)	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
our	O
approach	O
for	O
solving	O
complex	O
KBQA	O
.	O
First	O
,	O
we	O
generate	O
candidate	O
query	O
graphs	O
by	O
staged	O
generation	O
method	O
(	O
Section	O
2.1	O
)	O
.	O
Second	O
,	O
we	O
measure	O
the	O
semantic	O
similarities	O
between	O
the	O
question	O
and	O
each	O
query	O
graph	O
using	O
deep	O
neural	O
networks	O
(	O
Section	O
2.2	O
)	O
.	O
Then	O
we	O
introduce	O
an	O
ensemble	O
approach	O
for	O
entity	B-TaskName
linking	I-TaskName
enrichment	O
(	O
Section	O
2.3	O
)	O
,	O
Finally	O
,	O
we	O
discuss	O
the	O
prediction	O
and	O
parameter	O
learning	O
step	O
of	O
this	O
task	O
(	O
Section	O
2.4	O
)	O
.	O

We	O
illustrate	O
our	O
staged	O
candidate	O
generation	O
method	O
in	O
this	O
section	O
.	O
Compared	O
to	O
previous	O
methods	O
,	O
such	O
as	O
Bao	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
employ	O
a	O
more	O
effective	O
candidate	O
generation	O
strategy	O
,	O
which	O
takes	O
advantage	O
of	O
implicit	O
type	O
information	O
in	O
query	O
graphs	O
and	O
time	O
interval	O
information	O
in	O
the	O
KB	O
.	O
In	O
our	O
work	O
,	O
we	O
take	O
4	O
kinds	O
of	O
semantic	O
constraints	O
into	O
account	O
:	O
entity	O
,	O
type	O
,	O
time	O
and	O
ordinal	O
constraints	O
.	O
Figure	O
2	O
shows	O
a	O
concrete	O
example	O
of	O
our	O
candidate	O
generation	O
.	O
For	O
simplicity	O
of	O
discussion	O
,	O
we	O
assume	O
Freebase	O
as	O
the	O
KB	O
in	O
this	O
section	O
.	O
Step	O
1	O
:	O
Focus	O
linking	O
.	O
We	O
extract	O
possible	O
(	O
mention	O
,	O
focus	O
node	O
)	O
pairs	O
from	O
the	O
question	O
.	O
Focus	O
nodes	O
are	O
the	O
starting	O
points	O
of	O
various	O
semantic	O
constraints	O
,	O
refer	O
to	O
Figure	O
2	O
(	O
a	O
)	O
.	O
For	O
entity	B-TaskName
linking	I-TaskName
,	O
we	O
generate	O
(	O
mention	O
,	O
entity	O
)	O
pairs	O
using	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
entity	B-TaskName
linking	I-TaskName
tool	O
S	O
-	O
MART	O
(	O
Yang	O
and	O
Chang	O
,	O
2015	O
)	O
.	O
For	O
type	O
linking	O
,	O
we	O
brutally	O
combine	O
each	O
type	O
with	O
all	O
uni	O
-	O
,	O
bi	O
-	O
and	O
tri	O
-	O
gram	O
mentions	O
in	O
the	O
question	O
,	O
and	O
pick	O
top	O
-	O
10	O
(	O
mention	O
,	O
type	O
)	O
pairs	O
with	O
the	O
highest	O
word	O
embedding	O
similarities	O
of	O
each	O
pair	O
.	O
For	O
time	O
linking	O
,	O
we	O
extract	O
time	O
mentions	O
by	O
simply	O
matching	O
year	O
regex	O
.	O
For	O
ordinal	O
linking	O
,	O
we	O
leverage	O
a	O
predefined	O
superlative	O
word	O
list	O
2	O
and	O
recognize	O
mentions	O
by	O
matching	O
superlative	O
words	O
,	O
or	O
the	O
"	O
ordinal	O
number	O
+	O
superlative	O
"	O
pattern	O
.	O
The	O
ordinal	O
node	O
is	O
an	O
integer	O
representing	O
the	O
ordinal	O
number	O
in	O
the	O
mention	O
.	O
Step	O
2	O
:	O
Main	O
path	O
generation	O
.	O
We	O
build	O
different	O
main	O
paths	O
by	O
connecting	O
the	O
answer	O
node	O
to	O
different	O
focus	O
entities	O
using	O
1	O
-	O
hop	O
or	O
2	O
-	O
hopwith	O
-	O
mediator	O
3	O
predicate	O
sequence	O
.	O
Figure	O
2	O
(	O
b	O
)	O
shows	O
one	O
of	O
the	O
main	O
paths	O
.	O
Further	O
constraints	O
are	O
attached	O
by	O
connecting	O
an	O
anchor	O
node	O
x	O
to	O
an	O
unused	O
focus	O
node	O
through	O
predicate	O
sequences	O
,	O
where	O
the	O
anchor	O
node	O
x	O
is	O
a	O
non	O
-	O
focus	O
node	O
in	O
the	O
main	O
path	O
(	O
A	O
or	O
v	O
1	O
in	O
the	O
example	O
)	O
.	O
Step	O
3	O
:	O
Attaching	O
entity	O
constraints	O
.	O
We	O
apply	O
a	O
depth	O
-	O
first	O
search	O
to	O
search	O
for	O
combinations	O
of	O
multiple	O
entity	O
constraints	O
to	O
the	O
main	O
path	O
through	O
1	O
-	O
hop	O
predicate	O
.	O
Figure	O
2	O
(	O
c	O
)	O
shows	O
a	O
valid	O
entity	O
constraint	O
,	O
(	O
v	O
1	O
,	O
basic	O
title	O
,	O
president	O
)	O
.	O
The	O
advantage	O
of	O
depth	O
-	O
first	O
search	O
is	O
that	O
we	O
can	O
involve	O
unlimited	O
number	O
of	O
entities	O
in	O
a	O
query	O
graph	O
,	O
which	O
has	O
a	O
better	O
coverage	O
than	O
template	O
-	O
based	O
methods	O
.	O
Step	O
4	O
:	O
Type	O
constraint	O
generation	O
.	O
Type	O
constraints	O
can	O
only	O
be	O
applied	O
at	O
the	O
answer	O
node	O
using	O
IsA	O
predicate	O
.	O
Our	O
improvement	O
in	O
this	O
step	O
is	O
to	O
filter	O
type	O
constraints	O
using	O
implicit	O
types	O
2~2	O
0	B-DatasetName
superlative	O
words	O
,	O
such	O
as	O
largest	O
,	O
highest	O
,	O
latest	O
.	O
3	O
Mediator	O
is	O
a	O
kind	O
of	O
auxiliary	O
nodes	O
in	O
Freebase	O
maintaining	O
N	O
-	O
ary	O
facts	O
.	O
of	O
the	O
answer	O
,	O
derived	O
from	O
the	O
outgoing	O
predicates	O
of	O
the	O
answer	O
node	O
.	O
For	O
example	O
in	O
Figure	O
2	O
(	O
c	O
)	O
,	O
the	O
domain	O
type	O
of	O
the	O
predicate	O
government	O
position	O
is	O
politician	O
,	O
which	O
becomes	O
the	O
implicit	O
type	O
of	O
the	O
answer	O
.	O
Thus	O
we	O
can	O
filter	O
type	O
constraints	O
which	O
are	O
irrelevant	O
to	O
the	O
implicit	O
types	O
,	O
preventing	O
semantic	O
drift	O
and	O
speeding	O
up	O
the	O
generation	O
process	O
.	O
To	O
judge	O
whether	O
two	O
types	O
in	O
Freebase	O
are	O
relevant	O
or	O
not	O
,	O
we	O
adopt	O
the	O
method	O
in	O
Luo	O
et	O
al	O
(	O
2015	O
)	O
to	O
build	O
a	O
rich	O
type	O
hierarchy	O
of	O
Freebase	O
.	O
Focus	O
types	O
are	O
discarded	O
,	O
if	O
they	O
are	O
not	O
the	O
super	O
-	O
or	O
sub	O
-	O
types	O
of	O
any	O
implicit	O
types	O
of	O
the	O
answer	O
.	O
Step	O
5	O
:	O
Time	O
and	O
ordinal	O
constraint	O
generation	O
.	O
As	O
shown	O
in	O
Figure	O
2	O
(	O
d	O
)	O
,	O
the	O
time	O
constraint	O
is	O
represented	O
as	O
a	O
2	O
-	O
hop	O
predicate	O
sequence	O
,	O
where	O
the	O
second	O
is	O
a	O
virtual	O
predicate	O
determined	O
by	O
the	O
preposition	O
before	O
the	O
focus	O
time	O
,	O
indicating	O
the	O
time	O
comparing	O
operation	O
,	O
like	O
"	O
before	O
"	O
,	O
"	O
after	O
"	O
and	O
"	O
in	O
"	O
.	O
Similarly	O
,	O
the	O
ordinal	O
constraint	O
also	O
forms	O
a	O
2	O
-	O
hop	O
predicate	O
sequence	O
,	O
where	O
the	O
second	O
predicate	O
represents	O
descending	O
(	O
MaxAtN	O
)	O
or	O
ascending	O
order	O
(	O
MinAtN	O
)	O
.	O
For	O
the	O
detail	O
of	O
time	O
constraint	O
,	O
while	O
existing	O
approaches	O
(	O
Yih	O
et	O
al	O
,	O
2015	O
;	O
Bao	O
et	O
al	O
,	O
2016	O
)	O
link	O
the	O
focus	O
time	O
with	O
only	O
single	O
time	O
predicate	O
,	O
our	O
improvement	O
is	O
to	O
leverage	O
paired	O
time	O
predicates	O
for	O
representing	O
a	O
more	O
accurate	O
time	O
constraint	O
.	O
In	O
Freebase	O
,	O
paired	O
time	O
predicates	O
are	O
used	O
to	O
represent	O
facts	O
within	O
certain	O
time	O
intervals	O
,	O
like	O
f	O
rom	O
and	O
to	O
4	O
in	O
Figure	O
2	O
(	O
d	O
)	O
.	O
For	O
time	O
comparing	O
operation	O
"	O
in	O
"	O
,	O
we	O
link	O
the	O
time	O
focus	O
to	O
the	O
starting	O
time	O
predicate	O
,	O
but	O
use	O
both	O
predi	O
-	O
cates	O
in	O
SPARQL	O
query	O
,	O
restricting	O
that	O
the	O
focus	O
time	O
lies	O
in	O
the	O
time	O
interval	O
of	O
the	O
paired	O
predicates	O
.	O
After	O
finishing	O
all	O
these	O
querying	O
stages	O
,	O
we	O
translate	O
candidate	O
graphs	O
into	O
SPARQL	O
query	O
,	O
and	O
produce	O
their	O
final	O
output	O
answers	O
.	O
Finally	O
,	O
we	O
discard	O
query	O
graphs	O
with	O
zero	O
outputs	O
,	O
or	O
using	O
overlapped	O
mentions	O
.	O

The	O
architecture	O
of	O
the	O
proposed	O
model	O
is	O
shown	O
in	O
Figure	O
3	O
.	O
We	O
first	O
replace	O
all	O
entity	O
(	O
or	O
time	O
)	O
mentions	O
used	O
in	O
the	O
query	O
graph	O
by	O
dummy	O
tokens	O
E	O
(	O
or	O
T	O
m	O
)	O
.	O
To	O
encode	O
the	O
complex	O
query	O
structure	O
,	O
we	O
split	O
it	O
into	O
predicate	O
sequences	O
starting	O
from	O
answer	O
to	O
focus	O
nodes	O
,	O
which	O
we	O
call	O
semantic	O
components	O
.	O
The	O
predicate	O
sequence	O
does	O
n't	O
include	O
the	O
information	O
of	O
focus	O
nodes	O
,	O
except	O
for	O
type	O
constraints	O
,	O
where	O
we	O
append	O
the	O
focus	O
type	O
to	O
the	O
IsA	O
predicate	O
,	O
resulting	O
in	O
the	O
predicate	O
sequence	O
like	O
{	O
IsA	O
,	O
river	O
}	O
.	O
We	O
introduce	O
in	O
detail	O
the	O
encoding	O
methods	O
for	O
questions	O
and	O
predicate	O
sequences	O
,	O
and	O
how	O
to	O
calculate	O
the	O
semantic	B-TaskName
similarity	I-TaskName
score	O
.	O

We	O
encode	O
the	O
question	O
in	O
both	O
global	O
and	O
local	O
level	O
,	O
which	O
captures	O
the	O
semantic	O
information	O
with	O
respect	O
to	O
each	O
component	O
p.	O
The	O
global	O
information	O
takes	O
the	O
token	O
sequence	O
as	O
the	O
input	O
.	O
We	O
use	O
the	O
same	O
word	O
embedding	O
matrix	O
E	O
w	O
to	O
convert	O
the	O
token	O
sequence	O
into	O
vectors	O
{	O
q	O
(	O
w	O
)	O
1	O
,	O
.	O
.	O
.	O
,	O
q	O
(	O
w	O
)	O
n	O
}	O
.	O
Then	O
we	O
encode	O
the	O
token	O
sequence	O
by	O
applying	O
bidirectional	B-MethodName
GRU	I-MethodName
network	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
)	O
.	O
The	O
representation	O
of	O
the	O
token	O
sequence	O
is	O
the	O
concatenation	O
of	O
the	O
last	O
forward	O
and	O
backward	O
hidden	O
states	O
through	O
the	O
Bi	O
-	O
GRU	B-MethodName
layer	O
,	O
q	O
(	O
tok	O
)	O
=	O
[	O
−	O
h	O
(	O
w	O
)	O
1	O
;	O
−	O
h	O
(	O
w	O
)	O
n	O
]	O
.	O
To	O
encode	O
the	O
question	O
at	O
local	O
level	O
,	O
we	O
leverage	O
dependency	B-TaskName
parsing	I-TaskName
to	O
represent	O
long	O
-	O
range	O
dependencies	O
between	O
the	O
answer	O
and	O
the	O
focus	O
node	O
in	O
p.	O
Since	O
the	O
answer	O
is	O
denoted	O
by	O
the	O
whword	O
in	O
the	O
question	O
,	O
we	O
extract	O
the	O
dependency	O
path	O
from	O
the	O
answer	O
node	O
to	O
the	O
focus	O
mention	O
in	O
the	O
question	O
.	O
Similar	O
with	O
Xu	O
et	O
al	O
(	O
2016	O
)	O
,	O
we	O
treat	O
the	O
path	O
as	O
the	O
concatenation	O
of	O
words	O
and	O
dependency	O
labels	O
with	O
directions	O
.	O
For	O
example	O
,	O
the	O
dependency	O
path	O
between	O
"	O
what	O
"	O
and	O
"	O
United	O
States	O
"	O
is	O
{	O
what	O
,	O
−−−	O
nsubj	O
,	O
is	O
,	O
−	O
−	O
prep	O
,	O
in	O
,	O
−	O
−	O
pobj	O
,	O
E	O
}	O
.	O
We	O
apply	O
another	O
bidirectional	B-MethodName
GRU	I-MethodName
layer	O
to	O
produce	O
the	O
vector	O
representation	O
at	O
dependency	O
level	O
q	O
(	O
dep	O
)	O
p	O
,	O
capturing	O
both	O
syntactic	O
features	O
and	O
local	O
semantic	O
features	O
.	O
Finally	O
we	O
combine	O
global	O
and	O
local	O
representation	O
by	O
element	O
-	O
wise	O
addition	O
,	O
returning	O
the	O
representation	O
of	O
the	O
question	O
with	O
respect	O
to	O
the	O
semantic	O
component	O
,	O
q	O
p	O
=	O
q	O
(	O
tok	O
)	O
+	O
q	O
(	O
dep	O
)	O
p	O
.	O

Given	O
the	O
query	O
graph	O
with	O
multiple	O
semantic	O
components	O
,	O
G	O
=	O
{	O
p	O
(	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
p	O
(	O
N	O
)	O
}	O
,	O
now	O
all	O
its	O
semantic	O
components	O
have	O
been	O
projected	O
into	O
a	O
common	O
vector	O
space	O
,	O
representing	O
hidden	O
features	O
in	O
different	O
aspects	O
.	O
We	O
apply	O
max	B-MethodName
pooling	I-MethodName
over	O
the	O
hidden	O
vectors	O
of	O
semantic	O
components	O
,	O
and	O
get	O
the	O
compositional	O
semantic	O
representation	O
of	O
the	O
entire	O
query	O
graph	O
.	O
Similarly	O
,	O
we	O
perform	O
max	B-MethodName
pooling	I-MethodName
for	O
the	O
question	O
vectors	O
with	O
respect	O
to	O
each	O
semantic	O
component	O
.	O
Finally	O
,	O
we	O
compute	O
the	O
semantic	B-TaskName
similarity	I-TaskName
score	O
between	O
the	O
graph	O
and	O
question	O
:	O
Based	O
on	O
this	O
framework	O
,	O
our	O
proposed	O
method	O
ensures	O
the	O
vector	O
spaces	O
of	O
the	O
question	O
and	O
the	O
entire	O
query	O
graph	O
are	O
comparable	O
,	O
and	O
captures	O
complementary	O
semantic	O
features	O
from	O
different	O
parts	O
of	O
the	O
query	O
graph	O
.	O
It	O
's	O
worth	O
mentioning	O
that	O
the	O
semantic	O
matching	O
model	O
is	O
agnostic	O
to	O
the	O
candidate	O
generation	O
method	O
of	O
the	O
query	O
graphs	O
,	O
hence	O
it	O
can	O
be	O
applied	O
to	O
the	O
other	O
existing	O
semantic	B-TaskName
parsing	I-TaskName
frameworks	O
.	O
S	O
sem	O
(	O
q	O
,	O
G	O
)	O
=	O
cos	O
(	O
max	O
i	O
p	O
(	O
i	O
)	O
,	O
max	O
i	O
q	O
(	O
i	O
)	O
p	O
)	O
.	O
(	O
1	O
)	O

The	O
S	O
-	O
MART	O
linker	O
is	O
a	O
black	O
box	O
for	O
our	O
system	O
,	O
which	O
is	O
not	O
extendable	O
and	O
tend	O
to	O
produce	O
high	O
precision	O
but	O
low	O
recall	O
linking	O
results	O
.	O
To	O
seek	O
a	O
better	O
balance	O
at	O
entity	B-TaskName
linking	I-TaskName
,	O
we	O
propose	O
an	O
ensemble	O
approach	O
to	O
enrich	O
linking	O
results	O
.	O
We	O
first	O
build	O
a	O
large	O
lexicon	O
by	O
collecting	O
all	O
(	O
mention	O
,	O
entity	O
)	O
pairs	O
from	O
article	O
titles	O
,	O
anchor	O
texts	O
,	O
redirects	O
and	O
disambiguation	O
pages	O
of	O
Wikipedia	O
.	O
Each	O
pair	O
is	O
associated	O
with	O
statistical	O
features	O
,	O
such	O
as	O
linking	O
probability	O
,	O
letter	O
-	O
tri	O
-	O
gram	O
jaccard	O
similarity	O
and	O
popularity	O
of	O
the	O
entity	O
in	O
Wikipedia	O
.	O
For	O
the	O
pairs	O
found	O
in	O
S	O
-	O
MART	O
results	O
,	O
we	O
take	O
the	O
above	O
features	O
as	O
the	O
input	O
to	O
a	O
2	O
-	O
layer	O
linear	B-MethodName
regression	I-MethodName
model	O
fitting	O
their	O
linking	O
scores	O
.	O
Thus	O
we	O
learn	O
a	O
pseudo	O
linking	O
score	O
for	O
every	O
pair	O
in	O
the	O
lexicon	O
,	O
and	O
for	O
each	O
question	O
,	O
we	O
pick	O
top	O
-	O
K	O
highest	O
pairs	O
to	O
enrich	O
S	O
-	O
MART	O
linking	O
results	O
,	O
where	O
K	O
is	O
a	O
hyperparameter	O
.	O

In	O
this	O
section	O
,	O
we	O
explore	O
the	O
contributions	O
of	O
various	O
components	O
in	O
our	O
system	O
.	O
Semantic	O
component	O
representation	O
:	O
We	O
first	O
evaluate	O
the	O
results	O
on	O
CompQ	O
and	O
WebQ	O
under	O
different	O
path	O
encoding	O
methods	O
.	O
Recap	O
that	O
the	O
encoding	O
result	O
of	O
a	O
semantic	O
component	O
is	O
the	O
summation	O
of	O
its	O
word	O
and	O
i	O
d	O
path	O
representations	O
(	O
Section	O
2.2.1	O
)	O
,	O
thus	O
we	O
compare	O
encoding	O
methods	O
by	O
multiple	O
combinations	O
.	O
For	O
encoding	O
predicate	O
word	O
sequence	O
,	O
we	O
use	O
BiGRU	B-MethodName
(	O
the	O
same	O
setting	O
as	O
encoding	O
question	O
word	O
sequence	O
)	O
as	O
the	O
alternative	O
of	O
average	O
word	O
embedding	O
.	O
For	O
encoding	O
predicate	O
i	O
d	O
sequence	O
,	O
we	O
use	O
average	O
predicate	O
embedding	O
as	O
the	O
alternative	O
of	O
the	O
current	O
path	O
-	O
level	O
embedding	O
(	O
P	O
athEmb	O
)	O
.	O
The	O
experimental	O
results	O
are	O
shown	O
in	O
Table	O
4	O
.	O
The	O
encoding	O
method	O
N	O
one	O
means	O
that	O
we	O
do	O
n't	O
encode	O
the	O
i	O
d	O
or	O
word	O
sequence	O
,	O
and	O
simply	O
take	O
the	O
result	O
of	O
the	O
other	O
sequence	O
as	O
the	O
representation	O
of	O
the	O
whole	O
component	O
.	O
we	O
observe	O
that	O
the	O
top	O
three	O
combination	O
settings	O
,	O
ignoring	O
either	O
word	O
or	O
i	O
d	O
sequence	O
,	O
perform	O
worse	O
than	O
the	O
bottom	O
three	O
settings	O
.	O
The	O
comparison	O
demonstrates	O
that	O
predicate	O
word	O
and	O
i	O
d	O
representation	O
can	O
be	O
complementary	O
to	O
each	O
other	O
.	O
The	O
performance	O
gain	O
is	O
not	O
that	O
large	O
,	O
mainly	O
because	O
predicate	O
i	O
d	O
features	O
are	O
largely	O
covered	O
by	O
their	O
word	O
name	O
features	O
.	O
For	O
the	O
encoding	O
of	O
i	O
d	O
sequences	O
,	O
P	O
athEmb	O
works	O
better	O
than	O
average	O
embedding	O
,	O
consistently	O
boosting	O
F	O
1	O
by	O
0.65	O
on	O
both	O
datasets	O
.	O
The	O
former	O
method	O
treats	O
the	O
whole	O
sequence	O
as	O
a	O
single	O
unit	O
,	O
which	O
is	O
more	O
flexible	O
and	O
can	O
potentially	O
learn	O
diverse	O
representations	O
of	O
i	O
d	O
sequences	O
that	O
share	O
the	O
same	O
predicates	O
.	O
For	O
the	O
encoding	O
of	O
word	O
sequences	O
,	O
the	O
average	O
word	O
embedding	O
method	O
outperforms	O
BiGRU	B-MethodName
on	O
CompQ	O
,	O
and	O
the	O
gap	O
becomes	O
smaller	O
when	O
running	O
on	O
WebQ.	O
This	O
is	O
mainly	O
because	O
the	O
training	O
set	O
of	O
WebQ	O
is	O
about	O
3	O
times	O
larger	O
than	O
that	O
of	O
CompQ	O
,	O
making	O
it	O
easier	O
for	O
training	O
a	O
more	O
complex	O
model	O
.	O
Semantic	B-TaskName
composition	I-TaskName
and	O
question	O
representation	O
:	O
To	O
demonstrate	O
the	O
effectiveness	O
of	O
semantic	B-TaskName
composition	I-TaskName
,	O
we	O
construct	O
a	O
straightforward	O
baseline	O
,	O
where	O
we	O
remove	O
the	O
max	B-MethodName
pooling	I-MethodName
operation	O
in	O
Eq	O
.	O
(	O
2	O
)	O
,	O
and	O
instead	O
calculate	O
the	O
semantic	B-TaskName
similarity	I-TaskName
score	O
as	O
the	O
summation	O
of	O
individual	O
cosine	O
similarities	O
:	O
S	O
sem	O
(	O
q	O
,	O
G	O
)	O
=	O
i	O
cos	O
(	O
p	O
(	O
i	O
)	O
,	O
q	O
(	O
i	O
)	O
p	O
)	O
.	O
For	O
methods	O
of	O
question	O
encoding	O
,	O
we	O
setup	O
ablations	O
by	O
turning	O
off	O
either	O
sentential	O
encoding	O
or	O
dependency	O
encoding	O
.	O
Table	O
5	O
shows	O
the	O
ablation	O
results	O
on	O
CompQ	O
and	O
WebQ.	O
When	O
dependency	O
path	O
information	O
is	O
augmented	O
with	O
sentential	O
information	O
,	O
the	O
performance	O
boosts	O
by	O
0.42	O
on	O
average	O
.	O
Dependency	O
paths	O
focus	O
on	O
hidden	O
features	O
at	O
syntactic	O
and	O
functional	O
perspective	O
,	O
which	O
is	O
a	O
good	O
complementary	O
to	O
sentential	O
encoding	O
results	O
.	O
However	O
,	O
performances	O
drop	O
by	O
2.17	O
if	O
only	O
dependency	O
information	O
is	O
used	O
,	O
we	O
find	O
that	O
under	O
certain	O
dependency	O
structures	O
,	O
crucial	O
words	O
(	O
bolded	O
)	O
are	O
not	O
in	O
the	O
path	O
between	O
the	O
answer	O
and	O
the	O
focus	O
mention	O
(	O
underlined	O
)	O
,	O
for	O
example	O
,	O
"	O
who	O
did	O
draco	O
malloy	O
end	O
up	O
marrying	O
"	O
and	O
"	O
who	O
did	O
the	O
philippines	O
gain	O
independence	O
from	O
"	O
.	O
While	O
we	O
observe	O
about	O
5	O
%	O
of	O
such	O
questions	O
in	O
WebQ	O
,	O
it	O
's	O
hard	O
to	O
predict	O
the	O
correct	O
query	O
graph	O
without	O
crucial	O
words	O
.	O
In	O
terms	O
of	O
semantic	B-TaskName
composition	I-TaskName
,	O
Our	O
max	B-MethodName
pooling	I-MethodName
based	O
method	O
consistently	O
outperforms	O
the	O
baseline	O
method	O
.	O
The	O
improvement	O
on	O
WebQ	O
is	O
smaller	O
than	O
on	O
CompQ	O
,	O
largely	O
due	O
to	O
the	O
fact	O
that	O
85	O
%	O
questions	O
in	O
WebQ	O
are	O
simple	O
questions	O
(	O
Bao	O
et	O
al	O
,	O
2016	O
)	O
.	O
As	O
a	O
result	O
of	O
combination	O
,	O
our	O
approach	O
significantly	O
outperforms	O
the	O
vanilla	O
SP+NN	O
approach	O
on	O
CompQ	O
by	O
1.28	O
,	O
demonstrating	O
the	O
effectiveness	O
of	O
our	O
approach	O
.	O
Theoretically	O
,	O
the	O
pooling	O
outcome	O
may	O
lead	O
to	O
worse	O
end	O
-	O
to	O
-	O
end	O
result	O
when	O
there	O
are	O
too	O
many	O
semantic	O
components	O
in	O
one	O
graph	O
,	O
because	O
the	O
pooling	O
layer	O
takes	O
too	O
many	O
vectors	O
as	O
input	O
,	O
different	O
semantic	O
features	O
between	O
similar	O
query	O
graphs	O
become	O
indistinguishable	O
.	O
In	O
our	O
task	O
,	O
only	O
0.5	O
%	O
of	O
candidate	O
graphs	O
have	O
more	O
than	O
3	O
semantic	O
components	O
,	O
so	O
pooling	O
is	O
a	O
reasonable	O
way	O
to	O
aggregate	O
semantic	O
components	O
in	O
this	O
scenario	O
.	O
To	O
further	O
explain	O
the	O
advantage	O
of	O
semantic	B-TaskName
composition	I-TaskName
,	O
we	O
take	O
the	O
following	O
question	O
as	O
an	O
example	O
:	O
"	O
who	O
is	O
gimli	O
's	O
father	O
in	O
the	O
hobbit	O
"	O
.	O
Two	O
query	O
graphs	O
are	O
likely	O
to	O
be	O
the	O
final	O
answer	O
:	O
1	O
)	O
(	O
?	O
,	O
children	O
,	O
gimli	O
person	O
)	O
;	O
2	O
)	O
(	O
?	O
,	O
f	O
ictional	O
children	O
,	O
gimli	O
character	O
)	O
(	O
?	O
,	O
appear	O
in	O
,	O
hobbit	O
)	O
.	O
If	O
observing	O
semantic	O
components	O
individually	O
,	O
the	O
predicate	O
children	O
is	O
most	O
likely	O
to	O
be	O
the	O
correct	O
one	O
since	O
"	O
's	O
father	O
"	O
is	O
highly	O
related	O
and	O
with	O
plenty	O
of	O
positive	O
training	O
data	O
.	O
Both	O
f	O
ictional	O
children	O
and	O
appear	O
in	O
get	O
a	O
much	O
lower	O
similarity	O
compared	O
with	O
children	O
,	O
hence	O
the	O
baseline	O
method	O
prefer	O
the	O
first	O
query	O
graph	O
.	O
In	O
the	O
meantime	O
,	O
our	O
proposed	O
method	O
learns	O
the	O
hidden	O
semantics	O
of	O
the	O
second	O
candidate	O
by	O
absorbing	O
salient	O
features	O
from	O
both	O
predicates	O
,	O
and	O
such	O
compositional	O
representation	O
is	O
closer	O
to	O
the	O
semantics	O
of	O
the	O
entire	O
question	O
than	O
a	O
simple	O
"	O
children	O
"	O
predicate	O
.	O
That	O
's	O
why	O
our	O
method	O
manages	O
to	O
answer	O
it	O
correctly	O
.	O

We	O
randomly	O
analyzed	O
100	O
questions	O
from	O
CompQ	O
where	O
no	O
correct	O
answers	O
are	O
returned	O
.	O
We	O
list	O
the	O
major	O
causes	O
of	O
errors	O
as	O
follows	O
:	O
Main	O
path	O
error	O
(	O
10	O
%	O
)	O
:	O
This	O
type	O
of	O
error	O
occurred	O
when	O
the	O
model	O
failed	O
to	O
understand	O
the	O
main	O
semantics	O
when	O
facing	O
some	O
difficult	O
questions	O
(	O
e.g.	O
"	O
What	O
native	O
american	O
sports	O
heroes	O
earning	O
two	O
gold	O
medals	O
in	O
the	O
1912	O
Olympics	O
"	O
)	O
;	O
Constraint	O
missing	O
(	O
42	O
%	O
)	O
:	O
These	O
types	O
of	O
questions	O
involve	O
implicit	O
constraints	O
,	O
for	O
example	O
,	O
the	O
question	O
"	O
Who	O
was	O
US	O
president	O
when	O
Traicho	O
Kostov	O
was	O
teenager	O
"	O
is	O
difficult	O
to	O
answer	O
because	O
it	O
implies	O
an	O
implicit	O
time	O
constraint	O
"	O
when	O
Traicho	O
Kostov	O
was	O
teenager	O
"	O
;	O
Entity	B-TaskName
linking	I-TaskName
error	O
(	O
16	O
%	O
)	O
:	O
This	O
error	O
occurs	O
due	O
to	O
the	O
highly	O
ambiguity	O
of	O
mentions	O
.	O
For	O
example	O
,	O
the	O
question	O
"	O
What	O
character	O
did	O
Robert	O
Pattinson	O
play	O
in	O
Harry	O
Potter	O
"	O
expects	O
the	O
film	O
"	O
Harry	O
Potter	O
and	O
the	O
Goblet	O
of	O
Fire	O
"	O
as	O
the	O
focus	O
,	O
while	O
there	O
are	O
7	O
movies	O
in	O
Harry	O
Potter	O
series	O
;	O
Miscellaneous	B-TaskName
(	O
32	O
%	O
)	O
:	O
This	O
error	O
class	O
contains	O
questions	O
with	O
semantic	O
ambiguity	O
or	O
not	O
reasonable	O
.	O
For	O
example	O
,	O
the	O
question	O
"	O
Where	O
is	O
Byron	O
Nelson	O
2012	O
"	O
is	O
hard	O
to	O
understand	O
,	O
because	O
"	O
Byron	O
Nelson	O
"	O
died	O
in	O
2006	O
and	O
maybe	O
this	O
question	O
wants	O
to	O
ask	O
where	O
did	O
he	O
die	O
.	O

Knowledge	B-TaskName
Base	I-TaskName
Question	I-TaskName
Answering	I-TaskName
(	O
KBQA	O
)	O
has	O
been	O
a	O
hot	O
research	O
top	O
in	O
recent	O
years	O
.	O
Generally	O
speaking	O
,	O
the	O
most	O
popular	O
methods	O
for	O
KBQA	O
can	O
be	O
mainly	O
divided	O
into	O
two	O
classes	O
:	O
information	B-TaskName
retrieval	I-TaskName
and	O
semantic	B-TaskName
parsing	I-TaskName
.	O
Information	B-TaskName
retrieval	I-TaskName
based	O
system	O
tries	O
to	O
obtain	O
target	O
answer	O
directly	O
from	O
question	O
information	O
and	O
KB	O
knowledge	O
without	O
explicit	O
considering	O
interior	O
query	O
structure	O
.	O
There	O
are	O
various	O
methods	O
(	O
Yao	O
and	O
Van	O
Durme	O
,	O
2014	O
;	O
Bordes	O
et	O
al	O
,	O
2015	O
;	O
Dong	O
et	O
al	O
,	O
2015	O
;	O
Xu	O
et	O
al	O
,	O
2016	O
)	O
to	O
select	O
candidate	O
answers	O
and	O
to	O
rank	O
results	O
.	O
Semantic	B-TaskName
parsing	I-TaskName
based	O
approach	O
focuses	O
on	O
constructing	O
a	O
semantic	B-TaskName
parsing	I-TaskName
tree	O
or	O
equivalent	O
query	O
structure	O
that	O
represents	O
the	O
semantic	O
meaning	O
of	O
the	O
question	O
.	O
In	O
terms	O
of	O
logical	O
representation	O
of	O
natural	O
language	O
questions	O
,	O
many	O
methods	O
have	O
been	O
tried	O
,	O
such	O
as	O
query	O
graph	O
(	O
Yih	O
et	O
al	O
,	O
2014	O
(	O
Yih	O
et	O
al	O
,	O
,	O
2015	O
or	O
RDF	O
query	O
language	O
(	O
Unger	O
et	O
al	O
,	O
2012	O
;	O
Cui	O
et	O
al	O
,	O
2017	O
;	O
Hu	O
et	O
al	O
,	O
2018	O
)	O
.	O
Recently	O
,	O
as	O
the	O
development	O
of	O
deep	O
learning	O
,	O
NN	O
-	O
based	O
approaches	O
have	O
been	O
combined	O
into	O
the	O
KBQA	O
task	O
(	O
Bordes	O
et	O
al	O
,	O
2014	O
)	O
,	O
showing	O
promising	O
result	O
.	O
These	O
approaches	O
tries	O
to	O
use	O
neural	O
network	O
models	O
to	O
encode	O
both	O
questions	O
and	O
answers	O
(	O
or	O
query	O
structures	O
)	O
into	O
the	O
vector	O
space	O
.	O
Subsequently	O
,	O
similarity	O
functions	O
are	O
used	O
to	O
select	O
the	O
most	O
appropriate	O
query	O
structure	O
to	O
generate	O
the	O
final	O
answer	O
.	O
For	O
example	O
,	O
Bordes	O
et	O
al	O
(	O
2014	O
)	O
focuses	O
on	O
embedding	O
the	O
subgraph	O
of	O
the	O
candidate	O
answer	O
;	O
Yin	O
et	O
al	O
(	O
2016	O
)	O
uses	O
character	O
-	O
level	O
CNN	O
and	O
word	O
-	O
level	O
CNN	O
to	O
match	O
different	O
information	O
;	O
Yu	O
et	O
al	O
(	O
2017	O
)	O
introduces	O
the	O
method	O
of	O
hierarchical	O
residual	O
RNN	O
to	O
compare	O
questions	O
and	O
relation	O
names	O
;	O
Qu	O
et	O
al	O
(	O
2018	O
)	O
proposes	O
the	O
AR	O
-	O
SMCNN	O
model	O
,	O
which	O
uses	O
RNN	O
to	O
capture	O
semantic	O
-	O
level	O
correlation	O
and	O
employs	O
CNN	O
to	O
extract	O
literallevel	O
words	O
interaction	O
.	O
Belonging	O
to	O
NN	O
-	O
based	O
semantic	B-TaskName
parsing	I-TaskName
category	O
,	O
our	O
approach	O
employs	O
a	O
novel	O
encoding	O
structure	O
method	O
to	O
solve	O
complex	O
questions	O
.	O
Previous	O
works	O
such	O
as	O
Yih	O
et	O
al	O
(	O
2015	O
)	O
and	O
Bao	O
et	O
al	O
(	O
2016	O
)	O
require	O
a	O
recognition	O
of	O
a	O
main	O
relation	O
and	O
regard	O
other	O
constraints	O
as	O
variables	O
added	O
to	O
this	O
main	O
relation	O
.	O
Unlike	O
their	O
approaches	O
,	O
our	O
method	O
encodes	O
multiple	O
relations	O
(	O
paths	O
)	O
into	O
a	O
uniform	O
query	O
structure	O
representation	O
(	O
semantic	B-TaskName
composition	I-TaskName
)	O
,	O
which	O
allows	O
more	O
flexible	O
query	O
structures	O
.	O
There	O
are	O
also	O
some	O
works	O
ca	O
n't	O
be	O
simply	O
classified	O
in	O
to	O
IR	O
based	O
methods	O
or	O
SP	O
based	O
methods	O
.	O
Jain	O
(	O
2016	O
)	O
introduces	O
Factual	O
Memory	B-MethodName
Network	I-MethodName
,	O
which	O
tries	O
to	O
encode	O
KB	O
and	O
questions	O
in	O
same	O
word	O
vector	O
space	O
,	O
extract	O
a	O
subset	O
of	O
initial	O
candidate	O
facts	O
,	O
then	O
try	O
to	O
employ	O
multi	O
-	O
hop	O
reasoning	O
and	O
refinement	O
to	O
find	O
a	O
path	O
to	O
answer	O
entity	O
.	O
,	O
Abujabal	O
et	O
al	O
(	O
2017	O
)	O
,	O
andCui	O
et	O
al	O
(	O
2017	O
)	O
try	O
to	O
interpret	O
question	O
intention	O
by	O
templates	O
,	O
which	O
learned	O
from	O
KB	O
or	O
QA	O
corpora	O
.	O
Talmor	O
and	O
Berant	O
(	O
2018	O
)	O
attempts	O
to	O
answering	O
complex	O
questions	O
by	O
decomposing	O
them	O
into	O
a	O
sequence	O
of	O
simple	O
questions	O
.	O

This	O
paper	O
describes	O
the	O
system	O
of	O
the	O
team	O
Orange	O
-	O
Deskiñ	O
,	O
used	O
for	O
the	O
CoNLL	O
2017	O
UD	B-DatasetName
Shared	O
Task	O
.	O
We	O
based	O
our	O
approach	O
on	O
an	O
existing	O
open	O
source	O
tool	O
(	O
BistParser	O
)	O
,	O
which	O
we	O
modified	O
in	O
order	O
to	O
produce	O
the	O
required	O
output	O
.	O
Additionally	O
we	O
added	O
a	O
kind	O
of	O
pseudoprojectivisation	O
.	O
This	O
was	O
needed	O
since	O
some	O
of	O
the	O
task	O
's	O
languages	O
have	O
a	O
high	O
percentage	O
of	O
non	O
-	O
projective	O
dependency	O
trees	O
.	O
In	O
most	O
cases	O
we	O
also	O
employed	O
word	B-TaskName
embeddings	I-TaskName
.	O
For	O
the	O
4	O
surprise	O
languages	O
,	O
the	O
data	O
provided	O
seemed	O
too	O
little	O
to	O
train	O
on	O
.	O
Thus	O
we	O
decided	O
to	O
use	O
the	O
training	O
data	O
of	O
typologically	O
close	O
languages	O
instead	O
.	O
Our	O
system	O
achieved	O
a	O
macro	O
-	O
averaged	O
LAS	O
of	O
68.61	O
%	O
(	O
10th	O
in	O
the	O
overall	O
ranking	O
)	O
which	O
improved	O
to	O
69.38	O
%	O
after	O
bug	O
fixes	O
.	O

For	O
the	O
shared	O
task	O
,	O
we	O
used	O
an	O
(	O
older	O
)	O
version	O
of	O
BistParser	O
for	O
all	O
treebanks	O
(	O
ud	B-DatasetName
-	O
treebanks	O
-	O
conll2017	O
)	O
.	O
BistParser	O
(	O
Kiperwasser	O
and	O
Goldberg	O
,	O
2016	O
)	O
is	O
a	O
transition	O
based	O
parser	O
(	O
Nivre	O
(	O
2008	O
)	O
,	O
and	O
which	O
uses	O
the	O
arc	O
-	O
hybrid	O
transition	O
system	O
(	O
Kuhlmann	O
et	O
al	O
,	O
2011	O
)	O
)	O
with	O
the	O
three	O
"	O
basic	O
"	O
transitions	O
LEFT	O
ARC	B-DatasetName
,	O
RIGHT	O
ARC	B-DatasetName
and	O
SHIFT	O
.	O
Since	O
the	O
shared	O
task	O
requires	O
that	O
output	O
dependency	O
trees	O
have	O
exactly	O
one	O
root	O
,	O
we	O
modified	O
BistParser	O
accordingly	O
by	O
deleting	O
the	O
additional	O
ROOT	O
node	O
added	O
to	O
each	O
sentence	O
in	O
the	O
original	O
version	O
of	O
this	O
parser	O
.	O
BistParser	O
uses	O
a	O
bidirectional	B-MethodName
LSTM	I-MethodName
neural	O
network	O
.	O
Currently	O
BistParser	O
uses	O
forms	O
and	O
XPOS	O
for	O
both	O
learning	O
and	O
predicting	O
.	O
We	O
have	O
started	O
implementing	O
the	O
use	O
of	O
feature	O
column	O
as	O
well	O
,	O
but	O
this	O
has	O
not	O
been	O
used	O
for	O
the	O
CoNLL	O
2017	O
UD	B-DatasetName
Shared	O
Task	O
.	O
Some	O
of	O
the	O
languages	O
in	O
the	O
shared	O
task	O
have	O
a	O
large	O
percentage	O
of	O
non	O
-	O
projective	O
sentences	O
.	O
We	O
thus	O
decided	O
to	O
implement	O
a	O
pseudoprojectivisation	O
(	O
Kübler	O
et	O
al	O
,	O
2009	O
,	O
p.	O
37	O
)	O
of	O
the	O
input	O
sentences	O
before	O
training	O
or	O
predicting	O
.	O
The	O
output	O
sentences	O
are	O
than	O
de	O
-	O
projectivised	O
.	O
Sometimes	O
of	O
course	O
,	O
the	O
de	O
-	O
projectivisation	O
can	O
fail	O
,	O
especially	O
if	O
there	O
are	O
other	O
dependency	O
relation	O
errors	O
.	O
Our	O
tests	O
showed	O
,	O
however	O
,	O
that	O
the	O
overall	O
result	O
for	O
most	O
languages	O
is	O
still	O
better	O
than	O
without	O
any	O
pseudo	O
-	O
projectivisation	O
.	O
Finally	O
we	O
implemented	O
filters	O
which	O
ignore	O
the	O
special	O
CONLLU	O
lines	O
for	O
multi	O
-	O
word	O
tokens	O
(	O
2	O
-	O
3	O
...	O
)	O
and	O
elliptic	O
insertions	O
(	O
4.1	O
...	O
)	O
and	O
reinsert	O
those	O
lines	O
after	O
predicting	O
.	O
In	O
order	O
to	O
reduce	O
memory	O
usage	O
during	O
training	O
and	O
prediction	O
,	O
we	O
modified	O
BistParser	O
and	O
the	O
underlying	O
CNN	O
library	O
7	O
to	O
load	O
word	B-TaskName
embeddings	I-TaskName
only	O
for	O
the	O
words	O
present	O
in	O
the	O
training	O
or	O
test	O
data	O
.	O
For	O
the	O
same	O
reason	O
we	O
modified	O
Bist	O
-	O
Parser	O
to	O
read	O
sentences	O
one	O
by	O
one	O
,	O
to	O
predict	O
,	O
and	O
to	O
output	O
the	O
result	O
,	O
instead	O
of	O
reading	O
the	O
entire	O
test	O
file	O
at	O
once	O
8	O
.	O

We	O
trained	O
our	O
models	O
using	O
all	O
treebanks	O
provided	O
by	O
the	O
CoNLL	O
2017	O
UD	B-DatasetName
Shared	O
Task	O
.	O
Since	O
for	O
some	O
of	O
the	O
languages	O
there	O
were	O
no	O
development	O
treebanks	O
available	O
,	O
we	O
split	O
the	O
training	O
treebank	O
in	O
order	O
to	O
get	O
a	O
small	O
development	O
corpus	O
(	O
10	O
%	O
of	O
the	O
training	O
corpus	O
is	O
split	O
to	O
test	O
during	O
development	O
)	O
.	O
This	O
posed	O
a	O
certain	O
problem	O
for	O
treebanks	O
like	O
Kazakh	O
and	O
Uyghur	O
,	O
which	O
are	O
hopelessly	O
small	O
(	O
31	O
and	O
100	O
sentences	O
respectively	O
)	O
.	O
Eventhough	O
both	O
languages	O
are	O
geneti	O
-	O
cally	O
and	O
typologically	O
very	O
close	O
to	O
Turkish	O
(	O
3685	O
sentences	O
)	O
,	O
we	O
finally	O
trained	O
on	O
those	O
small	O
treebanks	O
for	O
time	O
constraints	O
(	O
with	O
more	O
time	O
available	O
we	O
would	O
have	O
experimented	O
with	O
various	O
other	O
parameters	O
and	O
a	O
cross	O
-	O
lingual	O
approach	O
)	O
.	O
In	O
most	O
cases	O
,	O
adding	O
word	B-TaskName
embeddings	I-TaskName
improved	O
the	O
LAS	O
considerably	O
.	O
We	O
downloaded	O
the	O
language	O
specific	O
corpora	O
provided	O
9	O
by	O
the	O
task	O
organisers	O
and	O
calculated	O
our	O
own	O
word	B-TaskName
embeddings	I-TaskName
with	O
Mikolov	O
's	O
word2vec	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
10	O
,	O
which	O
gave	O
better	O
results	O
than	O
the	O
100dimensional	O
word	B-TaskName
embeddings	I-TaskName
provided	O
.	O
In	O
order	O
to	O
get	O
the	O
best	O
results	O
,	O
we	O
cleaned	O
the	O
text	O
corpora	O
(	O
e.g.	O
deleting	O
letter	O
-	O
digit	O
combinations	O
and	O
separating	O
punctuation	O
symbols	O
such	O
as	O
commas	O
,	O
question	O
marks	O
etc	O
.	O
by	O
a	O
white	O
space	O
from	O
the	O
preceding	O
token	O
)	O
.	O
For	O
those	O
languages	O
which	O
use	O
an	O
alphabet	O
which	O
has	O
case	O
distinction	O
(	O
Latin	O
,	O
Cyrillic	O
and	O
Greek	O
)	O
we	O
put	O
everything	O
in	O
lowercase	O
.	O
Finally	O
we	O
trained	O
word	B-TaskName
embeddings	I-TaskName
with	O
300	O
and	O
500	O
dimensional	O
vectors	O
respectively	O
.	O
For	O
all	O
other	O
parameters	O
of	O
word2vec	O
we	O
used	O
the	O
default	O
setting	O
,	O
apart	O
from	O
the	O
lower	O
frequency	O
limit	O
,	O
which	O
we	O
increased	O
to	O
15	O
words	O
.	O
The	O
word	B-TaskName
embeddings	I-TaskName
were	O
calculated	O
on	O
a	O
server	O
with	O
a	O
32	O
core	O
CPU	O
running	O
Ubuntu	O
14.04	O
11	O
.	O
For	O
the	O
biggest	O
text	O
corpora	O
like	O
English	O
(	O
9	O
billion	O
words	O
)	O
,	O
German	O
(	O
5	O
,	O
9	O
billion	O
words	O
)	O
,	O
Indonesian	O
(	O
5	O
billion	O
words	O
)	O
French	O
(	O
4	O
,	O
8	O
billion	O
words	O
)	O
training	O
for	O
500	O
dimensional	O
word	O
vectors	O
took	O
up	O
to	O
6	O
hours	O
(	O
English	O
)	O
.	O
A	O
similar	O
approach	O
to	O
word2vec	O
is	O
fastText	B-MethodName
The	O
fundamental	O
difference	O
is	O
the	O
adoption	O
of	O
the	O
"	O
subword	O
model	O
"	O
described	O
in	O
.	O
A	O
subword	O
model	O
is	O
described	O
as	O
a	O
open	O
model	O
allowing	O
each	O
word	O
to	O
be	O
represented	O
not	O
only	O
by	O
the	O
word	O
itself	O
but	O
also	O
the	O
subword	O
components	O
of	O
the	O
word	O
in	O
combination	O
.	O
Subword	O
components	O
can	O
be	O
n	O
-	O
grams	O
with	O
varying	O
values	O
for	O
n	O
,	O
stems	O
,	O
root	O
words	O
,	O
prefixes	O
,	O
and	O
suffixes	O
or	O
any	O
other	O
possible	O
formalism	O
.	O
As	O
a	O
matter	O
of	O
fact	O
,	O
word2vec	O
can	O
been	O
seen	O
as	O
the	O
minimum	O
configuration	O
of	O
fastText	B-MethodName
where	O
only	O
the	O
words	O
are	O
considered	O
.	O
FastText	B-MethodName
has	O
been	O
demonstrated	O
to	O
perform	O
rather	O
well	O
in	O
two	O
different	O
tasks	O
i.e.	O
sentiment	B-TaskName
analysis	I-TaskName
and	O
tag	O
prediction	O
.	O
For	O
the	O
CoNLL	O
2017	O
UD	B-DatasetName
Shared	O
Task	O
we	O
finally	O
used	O
word2vec	O
,	O
since	O
the	O
results	O
were	O
similar	O
,	O
but	O
fastText	B-MethodName
was	O
taking	O
significantly	O
more	O
time	O
to	O
train	O
.	O

We	O
trained	O
all	O
treebanks	O
without	O
any	O
word	B-TaskName
embeddings	I-TaskName
,	O
with	O
300	O
and	O
with	O
500	O
dimensional	O
word	B-TaskName
embeddings	I-TaskName
.	O
For	O
BistParser	O
,	O
the	O
only	O
other	O
parameter	O
we	O
changed	O
was	O
the	O
size	O
of	O
the	O
first	O
hidden	O
layer	O
(	O
default	O
100	O
)	O
which	O
we	O
set	O
to	O
50	O
(	O
or	O
lower	O
,	O
especially	O
for	O
languages	O
whose	O
treebanks	O
are	O
very	O
small	O
)	O
.	O
Every	O
sentence	O
of	O
the	O
training	O
treebanks	O
was	O
pseudo	O
-	O
projectivised	O
before	O
training	O
.	O
Using	O
the	O
weighted	O
LAS	O
,	O
we	O
then	O
chose	O
the	O
best	O
combination	O
of	O
parameters	O
for	O
each	O
language	O
.	O
Since	O
the	O
python	O
version	O
of	O
CNN	O
(	O
used	O
by	O
our	O
adaptation	O
of	O
BistParser	O
)	O
does	O
not	O
support	O
GPU	O
,	O
training	O
was	O
slow	O
12	O
.	O
Thus	O
we	O
stopped	O
training	O
usually	O
after	O
15	O
epochs	O
unless	O
the	O
intermediary	O
results	O
were	O
promising	O
enough	O
to	O
continue	O
.	O
Figure	O
1	O
shows	O
the	O
system	O
architecture	O
.	O
The	O
upper	O
part	O
represents	O
the	O
data	O
flow	O
for	O
the	O
training	O
,	O
the	O
lower	O
part	O
represents	O
the	O
predicting	O
phase	O
.	O
We	O
did	O
all	O
training	O
on	O
two	O
Ubuntu	O
16.04	O
servers	O
13	O
with	O
64	O
GB	O
RAM	B-MethodName
.	O
As	O
said	O
above	O
,	O
the	O
version	O
of	O
the	O
CNN	O
library	O
we	O
used	O
,	O
does	O
not	O
run	O
on	O
GPU	O
,	O
so	O
all	O
training	O
was	O
single	O
threaded	O
.	O
The	O
training	O
processes	O
used	O
up	O
to	O
15	O
GB	O
RAM	B-MethodName
,	O
and	O
took	O
between	O
1	O
minute	O
(	O
Kazakh	O
)	O
and	O
53	O
hours	O
(	O
Czech	O
)	O
.	O
depending	O
on	O
the	O
size	O
of	O
the	O
treebank	O
.	O
This	O
corresponds	O
to	O
0.5	O
to	O
3	O
seconds	O
per	O
sentence	O
during	O
training	O
.	O
Training	O
for	O
the	O
surprise	O
languages	O
(	O
using	O
treebanks	O
of	O
typologically	O
close	O
languages	O
,	O
cf	O
.	O
section	O
,	O
4	O
)	O
,	O
took	O
significantly	O
longer	O
(	O
up	O
to	O
90	O
hours	O
for	O
Czech	O
)	O
.	O
Training	O
was	O
on	O
the	O
gold	O
values	O
(	O
form	O
,	O
lemma	B-DatasetName
,	O
XPOS	O
,	O
UPOS	O
,	O
deprel	O
,	O
head	O
)	O
of	O
the	O
training	O
treebanks	O
14	O
,	O
however	O
,	O
both	O
,	O
the	O
development	O
set	O
(	O
on	O
the	O
Tira	O
-	O
platform	O
)	O
and	O
the	O
final	O
test	O
set	O
use	O
the	O
UD	B-DatasetName
-	O
Pipe	O
output	O
e.g.	O
lemma	B-DatasetName
,	O
XPOS	O
or	O
UPOS	O
(	O
Straka	O
et	O
al	O
,	O
2016	O
)	O
which	O
may	O
be	O
erroneous	O
.	O
So	O
we	O
expected	O
a	O
certain	O
drop	O
of	O
LAS	O
for	O
the	O
tests	O
.	O
In	O
order	O
to	O
be	O
prepared	O
,	O
we	O
tried	O
to	O
add	O
erroneous	O
lemmas	O
and	O
UPOS	O
in	O
the	O
training	O
data	O
.	O
This	O
,	O
however	O
,	O
did	O
not	O
produce	O
better	O
results	O
,	O
so	O
we	O
abandoned	O
the	O
12	O
The	O
successor	O
of	O
CNN	O
,	O
Dynet	O
,	O
supports	O
GPU	O
,	O
but	O
since	O
BistParser	O
learns	O
on	O
a	O
phrase	O
by	O
phrase	O
base	O
,	O
no	O
gain	O
in	O
time	O
can	O
be	O
observed	O
.	O
13	O
Intel	O
Xeon	O
CPU	O
E5	O
-	O
1620	O
v4	O
at	O
3.50GHz	O
and	O
Intel	O
Core	O
i7	O
-	O
6900	O
K	O
CPU	O
at	O
3.20GHz	O
respectively	O
.	O
14	O
Apart	O
from	O
numerous	O
punctuation	O
symbols	O
with	O
wrong	O
heads	O
,	O
we	O
found	O
several	O
bad	O
annotations	O
for	O
words	O
as	O
well	O
in	O
different	O
languages	O
.	O
UDpipe	O
to	O
have	O
similar	O
"	O
noise	O
"	O
than	O
the	O
test	O
treebanks	O
.	O
The	O
final	O
results	O
obtained	O
with	O
the	O
development	O
corpora	O
(	O
or	O
split	O
from	O
train	O
corpora	O
when	O
there	O
were	O
no	O
development	O
corpora	O
)	O
are	O
shown	O
in	O
table	O
1	O
.	O
We	O
did	O
not	O
(	O
yet	O
)	O
use	O
the	O
morphological	O
features	O
(	O
column	O
6	O
)	O
.	O
First	O
tests	O
on	O
French	O
showed	O
that	O
a	O
slight	O
increase	O
in	O
LAS	O
is	O
possible	O
,	O
so	O
we	O
will	O
work	O
on	O
this	O
in	O
the	O
future	O
.	O
With	O
16	O
GB	O
RAM	B-MethodName
on	O
the	O
virtual	O
machine	O
provided	O
by	O
Tira	O
(	O
Potthast	O
et	O
al	O
,	O
2014	O
)	O
15	O
the	O
56	O
development	O
corpora	O
(	O
on	O
the	O
Tira	O
platform	O
)	O
were	O
processed	O
in	O
about	O
130	O
minutes	O
.	O

Buryat	O
fa	O
(	O
100	O
As	O
expected	O
,	O
Upper	O
Sorbian	O
and	O
Norther	O
Sami	O
give	O
quite	O
acceptable	O
results	O
using	O
models	O
trained	O
on	O
Czech	O
and	O
Finnish	O
respectively	O
.	O
Due	O
to	O
the	O
fact	O
that	O
the	O
provided	O
treebanks	O
for	O
Kazakh	O
and	O
Uyghur	O
are	O
both	O
very	O
small	O
we	O
tried	O
to	O
apply	O
the	O
same	O
approach	O
of	O
using	O
the	O
training	O
corpus	O
of	O
a	O
typologically	O
close	O
language	O
(	O
here	O
Turkish	O
)	O
.	O
However	O
,	O
the	O
results	O
were	O
disappointing	O
.	O
Thus	O
,	O
we	O
continue	O
to	O
use	O
the	O
models	O
trained	O
on	O
very	O
small	O
corpora	O
for	O
these	O
two	O
languages	O
in	O
the	O
shared	O
task	O
.	O
Possibly	O
the	O
fact	O
that	O
the	O
raw	O
text	O
corpus	O
used	O
to	O
calculate	O
word	B-TaskName
embeddings	I-TaskName
for	O
Kazakh	O
and	O
Uyghur	O
are	O
much	O
bigger	O
than	O
those	O
of	O
the	O
surprise	O
languages	O
allowed	O
to	O
produce	O
usable	O
word	B-TaskName
embeddings	I-TaskName
.	O
If	O
so	O
,	O
this	O
would	O
mean	O
that	O
word	B-TaskName
embeddings	I-TaskName
play	O
a	O
very	O
prominent	O
role	O
in	O
data	O
driven	O
dependency	B-TaskName
parsing	I-TaskName
.	O

ASSIST	O
:	O
Towards	O
Label	O
Noise	O
-	O
Robust	O
Dialogue	B-TaskName
State	I-TaskName
Tracking	I-TaskName

Task	B-TaskName
-	I-TaskName
oriented	I-TaskName
dialogue	I-TaskName
systems	I-TaskName
play	O
an	O
important	O
role	O
in	O
helping	O
users	O
accomplish	O
a	O
variety	O
of	O
tasks	O
through	O
verbal	O
interactions	O
(	O
Young	O
et	O
al	O
,	O
2013	O
;	O
Gao	O
et	O
al	O
,	O
2019	O
)	O
.	O
Dialogue	B-TaskName
state	I-TaskName
tracking	I-TaskName
(	O
DST	O
)	O
is	O
an	O
essential	O
component	O
of	O
the	O
dialogue	O
manager	O
in	O
pipeline	O
-	O
based	O
task	B-TaskName
-	I-TaskName
oriented	I-TaskName
dialogue	I-TaskName
systems	I-TaskName
.	O
It	O
aims	O
to	O
keep	O
track	O
of	O
users	O
'	O
intentions	O
at	O
each	O
turn	O
of	O
the	O
conversation	O
(	O
Mrkšić	O
et	O
al	O
,	O
2017	O
)	O
.	O
The	O
state	O
information	O
indicates	O
the	O
progress	O
of	O
the	O
conversation	O
and	O
is	O
leveraged	O
to	O
determine	O
the	O
next	O
system	O
action	O
and	O
generate	O
the	O
next	O
system	O
response	O
(	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
.	O
As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
dialogue	O
state	O
is	O
typically	O
represented	O
as	O
a	O
set	O
of	O
(	O
slot	O
,	O
value	O
)	O
pairs	O
.	O
Hi	O
,	O
how	O
may	O
I	O
help	O
you	O
?	O
I	O
need	O
to	O
book	O
a	O
room	O
at	O
autumn	O
house	O
.	O
Definitely	O
,	O
for	O
how	O
many	O
people	O
and	O
how	O
many	O
nights	O
?	O
Just	O
me	O
,	O
3	O
nights	O
.	O
Can	O
you	O
also	O
give	O
me	O
information	O
on	O
the	O
vue	O
cinema	O
?	O
Sure	O
.	O
It	O
is	O
in	O
the	O
city	O
centre	O
,	O
and	O
the	O
phone	O
number	O
is	O
08451962320	O
.	O
Thanks	O
for	O
your	O
help	O
.	O
That	O
's	O
all	O
I	O
need	O
.	O
(	O
hotel	O
-	O
name	O
,	O
autumn	O
house	O
)	O
(	O
hotel	O
-	O
name	O
,	O
autumn	O
house	O
)	O
(	O
hotel	O
-	O
book	O
people	O
,	O
1	O
)	O
(	O
hotel	O
-	O
book	O
stay	O
,	O
3	O
)	O
(	O
attraction	O
-	O
name	O
,	O
vue	O
cinema	O
)	O
(	O
hotel	O
-	O
name	O
,	O
autumn	O
house	O
)	O
(	O
hotel	O
-	O
book	O
people	O
,	O
1	O
)	O
(	O
hotel	O
-	O
book	O
stay	O
,	O
3	O
)	O
(	O
attraction	O
-	O
name	O
,	O
vue	O
cinema	O
)	O

Dialogue	O
State	O
Figure	O
1	O
:	O
An	O
example	O
dialogue	O
spanning	O
two	O
domains	O
.	O
On	O
the	O
left	O
is	O
the	O
dialogue	O
context	O
with	O
system	O
responses	O
shown	O
in	O
orange	O
and	O
user	O
utterances	O
in	O
green	O
.	O
The	O
dialogue	O
state	O
at	O
each	O
turn	O
is	O
presented	O
on	O
the	O
right	O
.	O
Therefore	O
,	O
the	O
problem	O
of	O
DST	O
is	O
defined	O
as	O
extracting	O
the	O
values	O
for	O
all	O
slots	O
from	O
the	O
dialogue	O
context	O
at	O
each	O
turn	O
of	O
the	O
conversation	O
.	O
Over	O
the	O
past	O
few	O
years	O
,	O
DST	O
has	O
made	O
significant	O
progress	O
,	O
attributed	O
to	O
a	O
number	O
of	O
publicly	O
available	O
dialogue	O
datasets	O
,	O
such	O
as	O
DSTC2	O
,	O
FRAMES	O
(	O
El	O
Asri	O
et	O
al	O
,	O
2017	O
)	O
,	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
(	O
Budzianowski	O
et	O
al	O
,	O
2018	O
)	O
,	O
Cross	O
-	O
WOZ	O
,	O
and	O
SGD	B-MethodName
.	O
Among	O
these	O
datasets	O
,	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
is	O
the	O
most	O
popular	O
one	O
.	O
So	O
far	O
,	O
lots	O
of	O
DST	O
models	O
have	O
been	O
built	O
on	O
top	O
of	O
it	O
Wu	O
et	O
al	O
,	O
2019	O
;	O
Ouyang	O
et	O
al	O
,	O
2020	O
;	O
Hu	O
et	O
al	O
,	O
2020	O
;	O
Ye	O
et	O
al	O
,	O
2021b	O
;	O
Lin	O
et	O
al	O
,	O
2021	O
)	O
.	O
However	O
,	O
it	O
has	O
been	O
found	O
out	O
that	O
there	O
is	O
substantial	O
noise	O
in	O
the	O
state	O
annotations	O
of	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
(	O
Eric	O
et	O
al	O
,	O
2020	O
)	O
.	O
These	O
noisy	O
labels	O
may	O
impede	O
the	O
training	O
of	O
robust	O
DST	O
models	O
and	O
lead	O
to	O
noticeable	O
performance	O
decrease	O
(	O
Zhang	O
et	O
al	O
,	O
2016	O
)	O
.	O
To	O
remedy	O
this	O
issue	O
,	O
massive	O
efforts	O
have	O
been	O
devoted	O
to	O
rectifying	O
the	O
annotations	O
,	O
and	O
four	O
refined	O
versions	O
,	O
including	O
MultiWOZ	B-DatasetName
2.1	I-DatasetName
(	O
Eric	O
et	O
al	O
,	O
2020	O
)	O
,	O
MultiWOZ	B-DatasetName
2.2	I-DatasetName
,	O
MultiWOZ	B-DatasetName
2.3	I-DatasetName
(	O
Han	O
et	O
al	O
,	O
2020b	O
)	O
,	O
and	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
(	O
Ye	O
et	O
al	O
,	O
2021a	O
)	O
,	O
have	O
been	O
released	O
.	O
Even	O
so	O
,	O
there	O
are	O
still	O
plenty	O
of	O
noisy	O
and	O
inconsistent	O
la	O
-	O
bels	O
.	O
For	O
example	O
,	O
in	O
the	O
latest	O
version	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
,	O
the	O
validation	O
set	O
and	O
test	O
set	O
have	O
been	O
manually	O
re	O
-	O
annotated	O
and	O
tend	O
to	O
be	O
noise	O
-	O
free	O
.	O
While	O
the	O
training	O
set	O
is	O
still	O
noisy	O
,	O
as	O
it	O
remains	O
intact	O
.	O
In	O
reality	O
,	O
it	O
is	O
costly	O
and	O
laborious	O
to	O
refine	O
existing	O
large	O
-	O
scale	O
noisy	O
datasets	O
or	O
collect	O
new	O
ones	O
with	O
fully	O
precise	O
annotations	O
(	O
Wei	O
et	O
al	O
,	O
2020	O
)	O
,	O
let	O
al	O
ne	O
dialogue	O
datasets	O
with	O
multiple	O
domains	O
and	O
multiple	O
turns	O
.	O
In	O
view	O
of	O
this	O
,	O
we	O
argue	O
that	O
it	O
is	O
essential	O
to	O
devise	O
particular	O
learning	O
algorithms	O
to	O
train	O
DST	O
models	O
robustly	O
from	O
noisy	O
labels	O
.	O
Although	O
loads	O
of	O
noisy	O
label	O
learning	O
algorithms	O
(	O
Natarajan	O
et	O
al	O
,	O
2013	O
;	O
Han	O
et	O
al	O
,	O
2020a	O
)	O
have	O
been	O
proposed	O
in	O
the	O
machine	O
learning	O
community	O
,	O
most	O
of	O
them	O
target	O
only	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
(	O
Song	O
et	O
al	O
,	O
2020	O
)	O
.	O
However	O
,	O
as	O
illustrated	O
in	O
Figure	O
1	O
,	O
the	O
dialogue	O
state	O
may	O
contain	O
multiple	O
labels	O
,	O
which	O
makes	O
it	O
unstraightforward	O
to	O
apply	O
existing	O
noisy	O
label	O
learning	O
algorithms	O
to	O
the	O
DST	O
task	O
.	O
In	O
this	O
paper	O
we	O
propose	O
a	O
general	O
framework	O
,	O
named	O
ASSIST	O
(	O
lAbel	O
noiSe	O
-	O
robuSt	O
dIalogue	B-TaskName
State	I-TaskName
Tracking	I-TaskName
)	O
,	O
to	O
train	O
DST	O
models	O
robustly	O
from	O
noisy	O
labels	O
.	O
ASSIST	O
first	O
trains	O
an	O
auxiliary	O
model	O
on	O
a	O
small	O
clean	O
dataset	O
to	O
generate	O
pseudo	O
labels	O
for	O
each	O
sample	O
in	O
the	O
noisy	O
training	O
set	O
.	O
Then	O
,	O
it	O
leverages	O
both	O
the	O
generated	O
pseudo	O
labels	O
and	O
vanilla	O
noisy	O
labels	O
to	O
train	O
the	O
primary	O
model	O
.	O
Since	O
the	O
auxiliary	O
model	O
is	O
trained	O
on	O
the	O
clean	O
dataset	O
,	O
it	O
can	O
be	O
expected	O
that	O
the	O
pseudo	O
labels	O
will	O
help	O
us	O
train	O
the	O
primary	O
model	O
more	O
robustly	O
.	O
Note	O
that	O
ASSIST	O
is	O
based	O
on	O
the	O
assumption	O
that	O
we	O
have	O
access	O
to	O
a	O
small	O
clean	O
dataset	O
.	O
This	O
assumption	O
is	O
reasonable	O
,	O
as	O
it	O
is	O
feasible	O
to	O
manually	O
collect	O
a	O
small	O
noise	O
-	O
free	O
dataset	O
or	O
re	O
-	O
annotate	O
a	O
portion	O
of	O
a	O
large	O
noisy	O
dataset	O
.	O
In	O
summary	O
,	O
our	O
main	O
contributions	O
include	O
:	O
We	O
propose	O
a	O
general	O
framework	O
ASSIST	O
to	O
train	O
robust	O
DST	O
models	O
from	O
noisy	O
labels	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
tackle	O
the	O
DST	O
problem	O
by	O
taking	O
into	O
consideration	O
the	O
label	O
noise	O
.	O
We	O
theoretically	O
analyze	O
why	O
the	O
pseudo	O
labels	O
are	O
beneficial	O
and	O
show	O
that	O
a	O
proper	O
combination	O
of	O
the	O
pseudo	O
labels	O
and	O
vanilla	O
noisy	O
labels	O
can	O
approximate	O
the	O
unknown	O
true	O
labels	O
more	O
accurately	O
.	O
We	O
conduct	O
extensive	O
experiments	O
on	O
Multi	O
-	O
WOZ	O
2.0	O
&	O
2.4	O
.	O
The	O
results	O
demonstrate	O
that	O
ASSIST	O
can	O
improve	O
the	O
DST	O
performance	O
on	O
both	O
datasets	O
by	O
a	O
large	O
margin	O
.	O

Figure	O
2	O
shows	O
the	O
architecture	O
,	O
which	O
consists	O
of	O
a	O
dialogue	O
context	O
semantic	O
encoder	O
,	O
a	O
slot	B-MethodName
attention	I-MethodName
module	O
,	O
and	O
a	O
slot	O
-	O
value	O
matching	O
module	O
.	O

Similar	O
to	O
Ye	O
et	O
al	O
,	O
2021b	O
)	O
,	O
we	O
utilize	O
the	O
pre	O
-	O
trained	O
language	O
model	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
to	O
encode	O
the	O
dialogue	O
context	O
X	O
t	O
into	O
contextual	O
semantic	O
representations	O
.	O
Let	O
Z	O
t	O
=	O
R	O
t	O
U	O
t	O
be	O
the	O
concatenation	O
of	O
the	O
system	O
response	O
and	O
user	O
utterance	O
at	O
turn	O
t	O
,	O
where	O
denotes	O
the	O
operator	O
of	O
sequence	O
concatenation	O
.	O
Then	O
,	O
the	O
dialogue	O
context	O
X	O
t	O
can	O
be	O
represented	O
as	O
X	O
t	O
=	O
Z	O
1	O
Z	O
2	O
Z	O
t	O
.	O
We	O
also	O
concatenate	O
each	O
slot	O
-	O
value	O
pair	O
and	O
denote	O
the	O
representation	O
of	O
the	O
dialogue	O
state	O
at	O
turn	O
t	O
as	O
B	O
t	O
=	O
(	O
s	O
,	O
vt	O
)	O
Bt	O
,	O
vt	O
=	O
none	O
s	O
v	O
t	O
,	O
in	O
which	O
only	O
non	O
-	O
none	O
slots	O
are	O
included	O
.	O
B	O
t	O
can	O
serve	O
as	O
a	O
compact	O
representation	O
of	O
the	O
dialogue	O
history	O
.	O
In	O
view	O
of	O
this	O
,	O
we	O
treat	O
the	O
previous	O
turn	O
dialogue	O
state	O
B	O
t−1	O
as	O
part	O
of	O
the	O
input	O
as	O
well	O
,	O
which	O
can	O
be	O
beneficial	O
when	O
X	O
t	O
exceeds	O
the	O
maximum	O
input	O
length	O
of	O
BERT	B-MethodName
.	O
The	O
complete	O
input	O
sequence	O
to	O
the	O
encoder	O
module	O
is	O
then	O
denoted	O
as	O
:	O
I	O
t	O
=	O
[	O
CLS	O
]	O
X	O
t−1	O
B	O
t−1	O
[	O
SEP	O
]	O
Z	O
t	O
[	O
SEP	O
]	O
,	O
1	O
We	O
adopt	O
existing	O
DST	O
models	O
as	O
the	O
primary	O
model	O
.	O

[	O
CLS	O
]	O
⋯	O
⋯	O
[	O
SEP	O
]	O
[	O
SEP	O
]	O
[	O
CLS	O
]	O
⋯	O
[	O
SEP	O
]	O
Slot	B-MethodName
Attention	I-MethodName
Let	O
H	O
t	O
R	O
|	O
It	O
|	O
×d	O
be	O
the	O
semantic	O
matrix	O
representation	O
of	O
I	O
t	O
.	O
Here	O
,	O
|	O
I	O
t	O
|	O
and	O
d	O
denote	O
the	O
sequence	O
length	O
of	O
I	O
t	O
and	O
the	O
BERT	B-MethodName
output	O
dimension	O
,	O
respectively	O
.	O
Then	O
,	O
we	O
have	O
:	O
H	O
t	O
=	O
BERT	B-MethodName
f	O
inetune	O
(	O
I	O
t	O
)	O
,	O
where	O
BERT	B-MethodName
f	O
inetune	O
means	O
that	O
the	O
BERT	B-MethodName
model	O
will	O
be	O
fine	O
-	O
tuned	O
during	O
the	O
training	O
process	O
.	O
For	O
each	O
slot	O
s	O
and	O
its	O
candidate	O
value	O
v	O
V	O
s	O
,	O
we	O
employ	O
another	O
BERT	B-MethodName
to	O
encode	O
them	O
into	O
semantic	O
vectors	O
h	O
s	O
R	O
d	O
and	O
h	O
v	O
R	O
d	O
.	O
Here	O
,	O
V	O
s	O
denotes	O
the	O
candidate	O
value	O
set	O
of	O
slot	O
s.	O
Unlike	O
the	O
dialogue	O
context	O
,	O
we	O
leverage	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
without	O
fine	O
-	O
tuning	O
to	O
embed	O
s	O
and	O
v	O
.	O
Besides	O
,	O
we	O
adopt	O
the	O
output	O
vector	O
corresponding	O
to	O
the	O
special	O
token	O
[	O
CLS	O
]	O
as	O
an	O
aggregated	O
representation	O
of	O
slot	O
s	O
and	O
value	O
v	O
,	O
i.e.	O
,	O
h	O
s	O
=	O
BERT	B-MethodName
[	O
CLS	O
]	O
f	O
ixed	O
(	O
[	O
CLS	O
]	O
s	O
[	O
SEP	O
]	O
)	O
,	O
h	O
v	O
=	O
BERT	B-MethodName
[	O
CLS	O
]	O
f	O
ixed	O
(	O
[	O
CLS	O
]	O
v	O
[	O
SEP	O
]	O
)	O
.	O

The	O
slot	B-MethodName
attention	I-MethodName
module	O
is	O
exploited	O
to	O
retrieve	O
slot	O
-	O
relevant	O
information	O
for	O
all	O
the	O
slots	O
from	O
the	O
same	O
dialogue	O
context	O
.	O
The	O
slot	B-MethodName
attention	I-MethodName
is	O
a	O
multihead	O
attention	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
Specifically	O
,	O
the	O
slot	O
representation	O
h	O
s	O
is	O
regarded	O
as	O
the	O
query	O
vector	O
,	O
and	O
the	O
dialogue	O
context	O
representation	O
H	O
t	O
is	O
taken	O
as	O
both	O
the	O
key	O
matrix	O
and	O
value	O
matrix	O
.	O
The	O
slot	B-MethodName
attention	I-MethodName
matches	O
h	O
s	O
to	O
the	O
semantic	O
vector	O
of	O
each	O
word	O
in	O
the	O
dialogue	O
context	O
and	O
calculates	O
the	O
attention	O
score	O
,	O
based	O
on	O
which	O
the	O
slot	O
-	O
specific	O
information	O
can	O
be	O
extracted	O
.	O
Let	O
a	O
s	O
t	O
R	O
d	O
denote	O
a	O
d	O
-	O
dimensional	O
vector	O
representation	O
of	O
the	O
related	O
information	O
of	O
slot	O
s	O
at	O
turn	O
t	O
,	O
we	O
obtain	O
:	O
a	O
s	O
t	O
=	O
MultiHead	O
(	O
h	O
s	O
,	O
H	O
t	O
,	O
H	O
t	O
)	O
.	O
a	O
s	O
t	O
is	O
expected	O
to	O
be	O
close	O
to	O
the	O
semantic	O
vector	O
representation	O
of	O
the	O
true	O
value	O
of	O
slot	O
s.	O
Considering	O
that	O
the	O
output	O
of	O
BERT	B-MethodName
is	O
normalized	O
by	O
layer	B-MethodName
normalization	I-MethodName
(	O
Ba	O
et	O
al	O
,	O
2016	O
)	O
,	O
we	O
also	O
feed	O
a	O
s	O
t	O
to	O
a	O
layer	B-MethodName
normalization	I-MethodName
layer	O
,	O
which	O
is	O
preceded	O
by	O
a	O
linear	O
transformation	O
layer	O
.	O
The	O
final	O
slot	O
-	O
specific	O
vector	O
g	O
s	O
t	O
R	O
d	O
is	O
calculated	O
as	O
:	O
g	O
s	O
t	O
=	O
LayerNorm	O
(	O
Linear	O
(	O
a	O
s	O
t	O
)	O
)	O
.	O

Our	O
approach	O
depends	O
on	O
the	O
auxiliary	O
model	O
A	O
to	O
generate	O
pseudo	O
labelsB	O
t	O
=	O
{	O
(	O
s	O
,	O
v	O
t	O
)	O
|	O
s	O
S	O
}	O
for	O
each	O
sample	O
in	O
the	O
noisy	O
training	O
set	O
.	O
In	O
this	O
work	O
,	O
we	O
treat	O
each	O
dialogue	O
context	O
X	O
t	O
rather	O
than	O
the	O
entire	O
dialogue	O
as	O
a	O
training	O
sample	O
.	O
Without	O
loss	B-MetricName
of	O
generality	O
,	O
the	O
pseudo	O
label	O
generation	O
process	O
is	O
denoted	O
as	O
follows	O
:	O
B	O
t	O
=	O
A	O
(	O
X	O
t	O
,	O
S	O
)	O
,	O
where	O
X	O
t	O
belongs	O
to	O
the	O
noisy	O
training	O
set	O
.	O

We	O
adopt	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
(	O
Budzianowski	O
et	O
al	O
,	O
2018	O
)	O
and	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
(	O
Ye	O
et	O
al	O
,	O
2021a	O
)	O
as	O
the	O
datasets	O
in	O
our	O
experiments	O
.	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
is	O
one	O
of	O
the	O
largest	O
publicly	O
available	O
multi	O
-	O
domain	O
taskoriented	O
dialogue	O
datasets	O
,	O
including	O
about	O
10	O
,	O
000	O
dialogues	O
spanning	O
seven	O
domains	O
.	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
is	O
the	O
latest	O
refined	O
version	O
of	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
.	O
The	O
annotations	O
of	O
its	O
validation	O
set	O
and	O
test	O
set	O
have	O
been	O
manually	O
rectified	O
.	O
While	O
its	O
training	O
set	O
remains	O
intact	O
and	O
is	O
the	O
same	O
as	O
that	O
of	O
MultiWOZ	B-DatasetName
2.1	I-DatasetName
(	O
Eric	O
et	O
al	O
,	O
2020	O
)	O
,	O
in	O
which	O
41.34	O
%	O
of	O
the	O
state	O
values	O
are	O
changed	O
,	O
compared	O
to	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
.	O
Since	O
the	O
hospital	O
domain	O
and	O
police	O
domain	O
never	O
occur	O
in	O
the	O
test	O
set	O
,	O
we	O
use	O
only	O
the	O
remaining	O
five	O
domains	O
{	O
attraction	O
,	O
hotel	O
,	O
restaurant	O
,	O
taxi	O
,	O
train	O
}	O
in	O
our	O
experiments	O
.	O
These	O
domains	O
have	O
30	O
slots	O
in	O
total	O
.	O
Considering	O
that	O
the	O
validation	O
set	O
and	O
test	O
set	O
of	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
are	O
noisy	O
,	O
we	O
replace	O
them	O
with	O
the	O
counterparts	O
of	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
2	O
.	O
We	O
preprocess	O
the	O
datasets	O
following	O
(	O
Ye	O
et	O
al	O
,	O
2021b	O
)	O
.	O
We	O
use	O
the	O
validation	O
set	O
as	O
the	O
small	O
clean	O
dataset	O
.	O

To	O
verify	O
the	O
effectiveness	O
of	O
the	O
proposed	O
framework	O
,	O
we	O
apply	O
the	O
generated	O
pseudo	O
labels	O
to	O
three	O
different	O
primary	O
models	O
.	O
SOM	B-MethodName
-	O
DST	O
:	O
SOM	B-MethodName
-	O
DST	O
)	O
is	O
an	O
open	O
vocabulary	O
-	O
based	O
method	O
.	O
It	O
treats	O
the	O
dialogue	O
state	O
as	O
an	O
explicit	O
fixed	O
-	O
sized	O
memory	O
and	O
selectively	O
overwrites	O
this	O
memory	O
at	O
each	O
turn	O
.	O
STAR	B-DatasetName
:	O
STAR	B-DatasetName
(	O
Ye	O
et	O
al	O
,	O
2021b	O
)	O
is	O
a	O
predefined	O
ontology	B-MethodName
-	O
based	O
method	O
.	O
It	O
leverages	O
a	O
stacked	O
slot	O
self	O
-	O
attention	O
mechanism	O
to	O
capture	O
the	O
slot	O
dependencies	O
automatically	O
.	O

Although	O
any	O
existing	O
DST	O
models	O
can	O
be	O
adopted	O
as	O
the	O
auxiliary	O
model	O
,	O
we	O
chose	O
to	O
propose	O
a	O
new	O
simple	O
one	O
to	O
reduce	O
overfitting	O
.	O
In	O
order	O
to	O
verify	O
the	O
superiority	O
of	O
the	O
proposed	O
model	O
,	O
we	O
also	O
apply	O
STAR	B-DatasetName
as	O
the	O
auxiliary	O
model	O
and	O
compare	O
their	O
performance	O
in	O
Figure	O
3	O
.	O
We	O
chose	O
STAR	B-DatasetName
due	O
to	O
its	O
good	O
performance	O
,	O
as	O
shown	O
in	O
Table	O
1	O
.	O
From	O
Figure	O
3	O
,	O
we	O
observe	O
that	O
all	O
three	O
primary	O
0	B-DatasetName
0.1	O
0	B-DatasetName
.	O
2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
models	O
demonstrate	O
higher	O
performance	O
on	O
both	O
datasets	O
when	O
using	O
the	O
proposed	O
auxiliary	O
model	O
than	O
using	O
STAR	B-DatasetName
as	O
the	O
auxiliary	O
model	O
.	O
The	O
results	O
indicate	O
that	O
the	O
proposed	O
auxiliary	O
model	O
is	O
able	O
to	O
generate	O
pseudo	O
labels	O
with	O
higher	O
quality	O
.	O

Pseudo	O
Labels	O
[	O
sys	O
]	O
:	O
Sure	O
,	O
da	O
vinci	O
pizzeria	O
is	O
a	O
cheap	O
Italian	O
restaurant	O
in	O
the	O
area	O
.	O
[	O
usr	O
]	O
:	O
Would	O
you	O
mind	O
making	O
a	O
reservation	O
for	O
Thursday	O
at	O
17:15	O
?	O
(	O
restaurant	O
-	O
name	O
,	O
da	O
vinci	O
pizzeria	O
)	O
(	O
restaurant	O
-	O
book	O
day	O
,	O
thursday	O
)	O
(	O
restaurant	O
-	O
book	O
time	O
,	O
17:15	O
)	O
(	O
restaurant	O
-	O
name	O
,	O
da	O
vinci	O
pizzeria	O
)	O
[	O
sys	O
]	O
:	O
Do	O
you	O
have	O
a	O
preferred	O
section	O
of	O
town	O
?	O
[	O
usr	O
]	O
:	O
Not	O
really	O
,	O
but	O
I	O
want	O
free	O
wifi	O
and	O
it	O
should	O
be	O
4	O
star	O
.	O
(	O
hotel	O
-	O
internet	O
,	O
free	O
)	O
(	O
hotel	O
-	O
stars	O
,	O
4	O
)	O
(	O
hotel	O
-	O
area	O
,	O
dontcare	O
)	O
(	O
hotel	O
-	O
internet	O
,	O
free	O
)	O
(	O
hotel	O
-	O
stars	O
,	O
4	O
)	O
[	O
usr	O
]	O
:	O
I	O
need	O
to	O
find	O
out	O
if	O
there	O
is	O
a	O
train	O
going	O
to	O
stansted	O
airport	O
that	O
leaves	O
after	O
12:30	O
.	O
(	O
train	O
-	O
arriveby	O
,	O
13:03	O
)	O
(	O
train	O
-	O
destination	O
,	O
stansted	O
airport	O
)	O
(	O
train	O
-	O
leaveat	O
,	O
12:30	O
)	O
(	O
train	O
-	O
destination	O
,	O
stansted	O
airport	O
)	O
(	O
train	O
-	O
leaveat	O
,	O
12:30	O
)	O
[	O
usr	O
]	O
:	O
I	O
am	O
staying	O
in	O
the	O
west	O
part	O
of	O
Cambridge	B-DatasetName
and	O
would	O
like	O
to	O
know	O
about	O
some	O
places	O
to	O
go	O
.	O
(	O
attraction	O
-	O
area	O
,	O
west	O
)	O
(	O
attraction	O
-	O
area	O
,	O
west	O
)	O
(	O
hotel	O
-	O
area	O
,	O
west	O
)	O
Table	O
2	O
:	O
Four	O
dialogue	O
snippets	O
with	O
their	O
vanilla	O
labels	O
and	O
the	O
generated	O
pseudo	O
labels	O
.	O
These	O
dialogue	O
snippets	O
are	O
chosen	O
from	O
the	O
training	O
set	O
of	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
.	O
To	O
save	O
space	O
,	O
we	O
only	O
present	O
turn	O
-	O
active	O
slots	O
and	O
their	O
values	O
.	O

Aiming	O
to	O
better	O
validate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
framework	O
,	O
we	O
also	O
report	O
the	O
results	O
when	O
the	O
small	O
clean	O
dataset	O
is	O
directly	O
combined	O
with	O
the	O
large	O
noisy	O
training	O
set	O
to	O
train	O
the	O
primary	O
model	O
.	O
We	O
adopt	O
AUX	O
-	O
DST	O
as	O
the	O
primary	O
model	O
and	O
show	O
the	O
results	O
in	O
Table	O
3	O
.	O
Since	O
the	O
clean	O
dataset	O
(	O
i.e.	O
,	O
the	O
validation	O
set	O
in	O
our	O
experiments	O
)	O
is	O
combined	O
with	O
the	O
training	O
set	O
,	O
all	O
the	O
results	O
in	O
Table	O
3	O
are	O
the	O
best	O
ones	O
on	O
the	O
test	O
set	O
.	O
As	O
can	O
be	O
observed	O
,	O
a	O
simple	O
combination	O
of	O
the	O
noisy	O
training	O
set	O
and	O
clean	O
dataset	O
can	O
lead	O
to	O
better	O
results	O
.	O
However	O
,	O
the	O
performance	O
improvements	O
are	O
lower	O
,	O
compared	O
to	O
using	O
pseudo	O
labels	O
(	O
especially	O
on	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
due	O
to	O
its	O
noisier	O
training	O
set	O
)	O
.	O
It	O
is	O
also	O
observed	O
that	O
when	O
both	O
the	O
clean	O
dataset	O
and	O
the	O
pseudo	O
labels	O
are	O
utilized	O
to	O
train	O
the	O
model	O
,	O
even	O
higher	O
performance	O
can	O
be	O
achieved	O
.	O
These	O
results	O
indicate	O
that	O
our	O
proposed	O
framework	O
can	O
make	O
better	O
use	O
of	O
the	O
small	O
clean	O
dataset	O
to	O
train	O
the	O
primary	O
model	O
.	O

Recently	O
,	O
DST	O
has	O
got	O
an	O
enormous	O
amount	O
of	O
attention	O
,	O
thanks	O
to	O
the	O
availability	O
of	O
multiple	O
largescale	O
multi	O
-	O
domain	O
dialogue	O
datasets	O
such	O
as	O
Multi	O
-	O
WOZ	O
2.0	O
(	O
Budzianowski	O
et	O
al	O
,	O
2018	O
)	O
,	O
MultiWOZ	B-DatasetName
2.1	I-DatasetName
(	O
Eric	O
et	O
al	O
,	O
2020	O
)	O
,	O
RiSAWOZ	B-DatasetName
(	O
Quan	O
et	O
al	O
,	O
2020	O
)	O
,	O
and	O
SGD	B-MethodName
.	O
The	O
most	O
popular	O
datasets	O
are	O
MultiWOZ	B-DatasetName
2.0	I-DatasetName
and	O
MultiWOZ	B-DatasetName
2.1	I-DatasetName
,	O
and	O
lots	O
of	O
DST	O
models	O
have	O
been	O
built	O
on	O
top	O
of	O
them	O
Wu	O
et	O
al	O
,	O
2019	O
;	O
Ouyang	O
et	O
al	O
,	O
2020	O
;	O
Hosseini	O
-	O
Asl	O
et	O
al	O
,	O
2020	O
;	O
Hu	O
et	O
al	O
,	O
2020	O
;	O
Ye	O
et	O
al	O
,	O
2021b	O
;	O
Lin	O
et	O
al	O
,	O
2021	O
;	O
Liang	O
et	O
al	O
,	O
2021	O
)	O
.	O
These	O
recent	O
DST	O
models	O
can	O
be	O
grouped	O
into	O
two	O
categories	O
:	O
predefined	O
ontology	B-MethodName
-	O
based	O
models	O
and	O
open	O
vocabulary	O
-	O
based	O
models	O
.	O
The	O
predefined	O
ontology	B-MethodName
-	O
based	O
models	O
treat	O
DST	O
as	O
a	O
multi	B-TaskName
-	I-TaskName
label	I-TaskName
classification	I-TaskName
problem	O
and	O
tend	O
to	O
demonstrate	O
better	O
performance	O
Shan	O
et	O
al	O
,	O
2020	O
;	O
Ye	O
et	O
al	O
,	O
2021b	O
)	O
.	O
The	O
open	O
vocabulary	O
-	O
based	O
models	O
leverage	O
either	O
span	O
prediction	O
(	O
Heck	O
et	O
al	O
,	O
2020	O
;	O
or	O
sequence	O
generation	O
(	O
Wu	O
et	O
al	O
,	O
2019	O
;	O
Hosseini	O
-	O
Asl	O
et	O
al	O
,	O
2020	O
)	O
to	O
extract	O
slot	O
values	O
from	O
the	O
dialogue	O
context	O
directly	O
.	O
Although	O
these	O
DST	O
models	O
have	O
made	O
a	O
huge	O
success	O
,	O
they	O
can	O
only	O
achieve	O
sub	O
-	O
optimal	O
performance	O
,	O
due	O
to	O
the	O
lack	O
of	O
handling	O
noisy	O
labels	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
take	O
the	O
noisy	O
labels	O
into	O
consideration	O
when	O
tackling	O
the	O
DST	O
problem	O
.	O

Addressing	O
noisy	O
labels	O
in	O
supervised	O
learning	O
is	O
a	O
long	O
-	O
term	O
studied	O
problem	O
(	O
Frénay	O
and	O
Verleysen	O
,	O
2013	O
;	O
Song	O
et	O
al	O
,	O
2020	O
;	O
Han	O
et	O
al	O
,	O
2020a	O
)	O
.	O
This	O
issue	O
becomes	O
more	O
prominent	O
in	O
the	O
era	O
of	O
deep	O
learning	O
,	O
as	O
training	O
deep	O
models	O
generally	O
requires	O
a	O
lot	O
of	O
well	O
-	O
labelled	O
data	O
,	O
but	O
it	O
is	O
expensive	O
and	O
time	O
-	O
consuming	O
to	O
collect	O
large	O
-	O
scale	O
datasets	O
with	O
completely	O
clean	O
annotations	O
.	O
This	O
dilemma	O
has	O
sparked	O
a	O
surge	O
of	O
noisy	O
label	O
learning	O
methods	O
(	O
Hendrycks	O
et	O
al	O
,	O
2018	O
;	O
Zhang	O
and	O
Sabuncu	O
,	O
2018	O
;	O
Song	O
et	O
al	O
,	O
2019	O
;	O
Wei	O
et	O
al	O
,	O
2020	O
)	O
.	O
Even	O
so	O
,	O
these	O
methods	O
mainly	O
focus	O
on	O
multi	B-TaskName
-	I-TaskName
class	I-TaskName
classification	I-TaskName
(	O
Song	O
et	O
al	O
,	O
2020	O
)	O
,	O
which	O
makes	O
it	O
not	O
straightforward	O
to	O
apply	O
them	O
to	O
the	O
DST	O
task	O
.	O

Except	O
for	O
the	O
size	O
of	O
the	O
clean	O
dataset	O
,	O
the	O
distribution	O
of	O
the	O
clean	O
dataset	O
may	O
also	O
affect	O
the	O
performance	O
of	O
the	O
primary	O
model	O
,	O
especially	O
when	O
the	O
clean	O
dataset	O
has	O
a	O
significantly	O
different	O
distribution	O
from	O
the	O
training	O
set	O
.	O
Thus	O
,	O
it	O
is	O
important	O
to	O
study	O
the	O
effects	O
of	O
the	O
distribution	O
of	O
the	O
clean	O
dataset	O
.	O
However	O
,	O
we	O
are	O
short	O
of	O
clean	O
datasets	O
with	O
different	O
distributions	O
.	O
It	O
is	O
also	O
challenging	O
to	O
model	O
the	O
distribution	O
explicitly	O
since	O
the	O
dialogue	O
state	O
may	O
contain	O
multiple	O
labels	O
.	O
To	O
address	O
this	O
issue	O
,	O
we	O
propose	O
to	O
remove	O
all	O
the	O
dialogues	O
that	O
are	O
related	O
to	O
a	O
specific	O
domain	O
and	O
use	O
only	O
the	O
remaining	O
ones	O
as	O
the	O
clean	O
dataset	O
.	O
As	O
thus	O
,	O
we	O
can	O
create	O
multiple	O
clean	O
datasets	O
with	O
different	O
distributions	O
.	O
The	O
results	O
of	O
AUX	O
-	O
DST	O
on	O
MultiWOZ	B-DatasetName
2.4	I-DatasetName
are	O
shown	O
in	O
Figure	O
8	O
.	O
As	O
can	O
be	O
observed	O
,	O
although	O
different	O
clean	O
datasets	O
indeed	O
lead	O
to	O
different	O
performance	O
,	O
compared	O
to	O
the	O
situation	O
where	O
no	O
clean	O
data	O
is	O
used	O
(	O
i.e.	O
,	O
only	O
the	O
vanilla	O
labels	O
are	O
used	O
to	O
train	O
the	O
model	O
)	O
,	O
all	O
these	O
clean	O
datasets	O
still	O
bring	O
huge	O
performance	O
improvements	O
.	O

This	O
project	O
was	O
funded	O
by	O
the	O
EPSRC	O
Fellowship	O
titled	O
"	O
Task	O
Based	O
Information	B-TaskName
Retrieval	I-TaskName
"	O
and	O
grant	O
reference	O
number	O
EP	O
/	O
P024289/1	O
.	O

Transfer	B-TaskName
Learning	I-TaskName
for	O
Related	O
Languages	O
:	O
Submissions	O
to	O
the	O
WMT20	O
Similar	O
Language	O
Translation	B-TaskName
Task	O

In	O
this	O
paper	O
,	O
we	O
describe	O
IIT	O
Delhi	O
's	O
submissions	O
to	O
the	O
WMT	B-DatasetName
2020	I-DatasetName
task	O
on	O
Similar	O
Language	O
Translation	B-TaskName
for	O
four	O
language	O
directions	O
:	O
Hindi	O
↔	O
Marathi	O
and	O
Spanish	O
↔	O
Portuguese	O
.	O
We	O
try	O
out	O
three	O
different	O
model	O
settings	O
for	O
the	O
translation	O
task	O
and	O
select	O
our	O
primary	O
and	O
contrastive	O
submissions	O
on	O
the	O
basis	O
of	O
performance	O
of	O
these	O
three	O
models	O
.	O
For	O
our	O
best	O
submissions	O
,	O
we	O
fine	O
-	O
tune	O
the	O
mBART	B-MethodName
model	O
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
on	O
the	O
parallel	O
data	O
provided	O
for	O
the	O
task	O
.	O
The	O
pre	O
-	O
training	O
is	O
done	O
using	O
self	O
-	O
supervised	O
objectives	O
on	O
a	O
large	O
amount	O
of	O
monolingual	O
data	O
for	O
many	O
languages	O
.	O
Overall	O
,	O
our	O
models	O
are	O
ranked	O
in	O
the	O
top	O
four	O
of	O
all	O
systems	O
for	O
the	O
submitted	O
language	O
pairs	O
,	O
with	O
first	O
rank	O
in	O
Spanish	O
Portuguese	O
.	O

Machine	B-TaskName
Translation	I-TaskName
(	O
MT	O
)	O
is	O
currently	O
tackled	O
using	O
rule	O
-	O
based	O
methods	O
(	O
RBMT	O
)	O
(	O
Charoenpornsawat	O
et	O
al	O
,	O
2002	O
)	O
,	O
phrase	O
-	O
based	O
statistical	O
methods	O
(	O
SMT	O
)	O
(	O
Koehn	O
et	O
al	O
,	O
2003	O
)	O
and	O
neural	O
methods	O
(	O
NMT	O
)	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
;	O
Sutskever	O
et	O
al	O
,	O
2014	O
;	O
Bahdanau	O
et	O
al	O
,	O
2015	O
;	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
.	O
NMT	O
has	O
achieved	O
high	O
translation	O
quality	O
for	O
several	O
language	O
pairs	O
(	O
Bojar	O
et	O
al	O
,	O
2018	O
;	O
Barrault	O
et	O
al	O
,	O
2019	O
)	O
,	O
but	O
this	O
level	O
of	O
performance	O
usually	O
requires	O
large	O
amounts	O
of	O
aligned	O
data	O
in	O
the	O
order	O
of	O
millions	O
of	O
sentence	O
pairs	O
.	O
For	O
low	O
and	O
medium	O
resource	O
languages	O
,	O
SMT	O
performs	O
better	O
than	O
NMT	O
(	O
Koehn	O
and	O
Knowles	O
,	O
2017	O
;	O
Sennrich	O
and	O
Zhang	O
,	O
2019	O
)	O
.	O
SMT	O
also	O
shows	O
better	O
performance	O
when	O
there	O
is	O
a	O
domain	O
mismatch	O
between	O
the	O
train	O
and	O
test	O
datasets	O
,	O
which	O
is	O
typical	O
of	O
low	O
and	O
medium	O
resource	O
language	O
pairs	O
.	O
In	O
these	O
settings	O
,	O
NMT	O
performance	O
can	O
be	O
boosted	O
by	O
leveraging	O
additional	O
monolingual	O
data	O
to	O
enforce	O
various	O
types	O
of	O
constraints	O
or	O
increasing	O
the	O
training	O
data	O
using	O
back	O
-	O
translation	O
.	O
These	O
methods	O
can	O
be	O
particularly	O
helpful	O
if	O
the	O
source	O
and	O
target	O
languages	O
in	O
MT	O
are	O
closely	O
related	O
and	O
share	O
language	O
structure	O
and	O
alphabet	O
.	O
Recently	O
,	O
pre	O
-	O
training	O
methods	O
for	O
sequence	O
-	O
to	O
-	O
sequence	O
(	O
seq2seq	B-MethodName
)	O
models	O
have	O
been	O
introduced	O
like	O
MASS	O
(	O
Song	O
et	O
al	O
,	O
2019a	O
)	O
,	O
XLM	B-MethodName
(	O
Conneau	O
and	O
Lample	O
,	O
2019	O
)	O
,	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2019	O
)	O
,	O
and	O
mBART	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
.	O
These	O
methods	O
show	O
significant	O
gains	O
in	O
downstream	O
tasks	O
like	O
NMT	O
,	O
summarization	B-TaskName
,	O
natural	B-TaskName
language	I-TaskName
inference	I-TaskName
(	O
NLI	O
)	O
,	O
etc	O
.	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
the	O
transfer	B-TaskName
learning	I-TaskName
capabilities	O
in	O
NMT	O
for	O
the	O
task	O
of	O
translation	O
between	O
related	O
languages	O
where	O
parallel	O
data	O
is	O
scarce	O
.	O
IIT	O
Delhi	O
participated	O
in	O
the	O
WMT	B-DatasetName
2020	I-DatasetName
Shared	O
task	O
on	O
Similar	O
Language	O
Translation	B-TaskName
for	O
four	O
language	O
directions	O
:	O
Hindi	O
(	O
hi	O
)	O
↔	O
Marathi	O
(	O
mr	O
)	O
and	O
Spanish	O
(	O
es	O
)	O
↔	O
Portuguese	O
(	O
pt	O
)	O
.	O
The	O
first	O
language	O
pair	O
is	O
low	O
resource	O
and	O
second	O
is	O
medium	O
resource	O
in	O
terms	O
of	O
the	O
parallel	O
data	O
available	O
for	O
the	O
task	O
.	O
Refer	O
to	O
Table	O
2	O
for	O
the	O
classification	O
.	O
We	O
fine	O
-	O
tuned	O
the	O
pre	O
-	O
trained	O
mBART	B-MethodName
model	O
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
on	O
the	O
parallel	O
data	O
provided	O
for	O
the	O
task	O
.	O
mBART	B-MethodName
gives	O
better	O
performance	O
than	O
SMT	O
models	O
even	O
when	O
the	O
parallel	O
data	O
is	O
very	O
limited	O
.	O
mBART	B-MethodName
is	O
pre	O
-	O
trained	O
on	O
25	O
languages	O
,	O
which	O
contain	O
Hindi	O
and	O
Spanish	O
,	O
but	O
not	O
Marathi	O
and	O
Portuguese	O
.	O
mBART	B-MethodName
is	O
able	O
to	O
leverage	O
transfer	B-TaskName
learning	I-TaskName
capabilities	O
even	O
for	O
those	O
languages	O
that	O
are	O
originally	O
not	O
present	O
during	O
the	O
pre	O
-	O
training	O
phase	O
.	O
The	O
fine	O
-	O
tuned	O
mBART	B-MethodName
architecture	O
forms	O
our	O
best	O
submissions	O
for	O
both	O
language	O
pairs	O
:	O
hi	O
↔	O
mr	O
and	O
es	O
↔	O
pt	O
.	O
The	O
rankings	O
obtained	O
by	O
us	O
in	O
each	O
of	O
the	O
language	O
directions	O
are	O
listed	O
in	O
Table	O
1	O
The	O
results	O
and	O
analysis	O
are	O
detailed	O
in	O
Section	O
5	O
.	O
We	O
finally	O
conclude	O
in	O
Section	O
6	O
.	O

SMT	O
is	O
tackled	O
by	O
building	O
a	O
phrase	O
table	O
from	O
the	O
aligned	O
parallel	O
data	O
.	O
The	O
target	O
side	O
translation	O
is	O
then	O
generated	O
by	O
matching	O
the	O
most	O
appropriate	O
phrases	O
in	O
the	O
source	O
sentence	O
conditioned	O
on	O
the	O
target	O
side	O
language	O
model	O
along	O
with	O
a	O
reordering	O
model	O
(	O
Koehn	O
et	O
al	O
,	O
2003	O
)	O
.	O
NMT	O
is	O
modeled	O
using	O
Encoder	O
-	O
Decoder	O
models	O
(	O
Cho	O
et	O
al	O
,	O
2014	O
;	O
Sutskever	O
et	O
al	O
,	O
2014	O
;	O
Bahdanau	O
et	O
al	O
,	O
2015	O
)	O
,	O
with	O
the	O
Transformer	B-MethodName
model	O
(	O
Vaswani	O
et	O
al	O
,	O
2017	O
)	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
many	O
MT	O
problems	O
.	O
But	O
these	O
models	O
'	O
reliance	O
on	O
large	O
aligned	O
parallel	O
data	O
for	O
the	O
source	O
and	O
target	O
languages	O
makes	O
them	O
unsuitable	O
for	O
low	O
/	O
medium	O
resource	O
language	O
pairs	O
(	O
Koehn	O
and	O
Knowles	O
,	O
2017	O
)	O
.	O
Some	O
of	O
the	O
previous	O
works	O
in	O
these	O
settings	O
to	O
improve	O
NMT	O
performance	O
are	O
described	O
below	O
:	O

Back	O
-	O
Translation	B-TaskName
Hoang	O
et	O
al	O
,	O
2018	O
)	O
increases	O
the	O
amount	O
of	O
training	O
data	O
by	O
using	O
monolingual	O
corpus	O
along	O
with	O
partially	O
-	O
trained	O
NMT	O
models	O
on	O
the	O
limited	O
parallel	O
data	O
.	O
Pseudo	O
-	O
parallel	O
corpus	O
for	O
each	O
direction	O
is	O
first	O
obtained	O
by	O
generating	O
the	O
translations	O
of	O
the	O
monolingual	O
data	O
for	O
each	O
language	O
using	O
the	O
partially	O
-	O
trained	O
MT	O
models	O
on	O
the	O
limited	O
parallel	O
data	O
.	O
Using	O
these	O
pseudoparallel	O
corpora	O
,	O
the	O
partially	O
-	O
trained	O
NMT	O
models	O
are	O
then	O
trained	O
further	O
for	O
some	O
number	O
of	O
steps	O
.	O
In	O
this	O
way	O
,	O
millions	O
of	O
pseudo	O
-	O
parallel	O
sentence	O
pairs	O
can	O
be	O
generated	O
to	O
improve	O
NMT	O
models	O
because	O
of	O
the	O
abundance	O
of	O
monolingual	O
data	O
.	O
Another	O
version	O
of	O
using	O
back	O
-	O
translation	O
is	O
the	O
copying	O
mechanism	O
.	O
Currey	O
et	O
al	O
(	O
2017	O
)	O
proposes	O
to	O
copy	O
the	O
target	O
side	O
monolingual	O
data	O
on	O
the	O
source	O
side	O
to	O
create	O
additional	O
data	O
without	O
modifying	O
the	O
training	O
regimen	O
for	O
NMT	O
.	O
This	O
helps	O
the	O
model	O
to	O
generate	O
fluent	O
translations	O
.	O

For	O
NMT	O
,	O
the	O
first	O
step	O
is	O
the	O
random	O
initialization	O
of	O
model	O
weights	O
in	O
both	O
the	O
encoder	O
and	O
decoder	O
.	O
Instead	O
of	O
random	O
initialization	O
,	O
NMT	O
models	O
can	O
be	O
initialized	O
by	O
pre	O
-	O
training	O
parts	O
of	O
the	O
model	O
(	O
Conneau	O
and	O
Lample	O
,	O
2019	O
;	O
Edunov	O
et	O
al	O
,	O
2019	O
)	O
,	O
or	O
pre	O
-	O
training	O
the	O
complete	O
seq2seq	B-MethodName
model	O
(	O
Ramachandran	O
et	O
al	O
,	O
2017	O
;	O
Song	O
et	O
al	O
,	O
2019b	O
;	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
.	O
These	O
pre	O
-	O
training	O
methods	O
leverage	O
different	O
kinds	O
of	O
masking	O
techniques	O
and	O
the	O
pretraining	O
objective	O
is	O
to	O
predict	O
these	O
masked	O
tokens	O
,	O
similar	O
to	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
.	O
Denoising	B-TaskName
auto	O
-	O
encoding	O
can	O
also	O
be	O
used	O
where	O
a	O
sentence	O
is	O
corrupted	O
by	O
various	O
noising	O
techniques	O
and	O
the	O
pre	O
-	O
training	O
objective	O
is	O
to	O
generate	O
the	O
original	O
uncorrupted	O
sentence	O
as	O
in	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2019	O
)	O
and	O
mBART	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
.	O

There	O
also	O
have	O
been	O
works	O
to	O
improve	O
low	O
/	O
medium	O
resource	O
NMT	O
by	O
adding	O
linguistic	O
information	O
either	O
using	O
data	B-TaskName
augmentation	I-TaskName
(	O
Currey	O
and	O
Heafield	O
,	O
2019	O
)	O
,	O
subword	O
embedding	O
augmentation	O
,	O
or	O
architectural	O
changes	O
(	O
Eriguchi	O
et	O
al	O
,	O
2017	O
)	O
.	O
This	O
helps	O
the	O
model	O
to	O
not	O
only	O
learn	O
the	O
alignment	O
between	O
source	O
and	O
target	O
language	O
spaces	O
,	O
but	O
also	O
syntax	O
structure	O
like	O
dependency	O
parse	O
,	O
part	O
of	O
speech	O
,	O
etc	O
.	O
This	O
helps	O
in	O
making	O
the	O
target	O
side	O
translations	O
more	O
fluent	O
and	O
conforming	O
to	O
the	O
structure	O
of	O
the	O
language	O
.	O
We	O
do	O
not	O
explore	O
this	O
direction	O
in	O
this	O
paper	O
.	O

We	O
experimented	O
with	O
three	O
different	O
settings	O
for	O
hi	O
↔	O
mr	O
as	O
listed	O
below	O
.	O
SMT	O
This	O
phrase	O
-	O
based	O
system	O
leverages	O
both	O
monolingual	O
and	O
parallel	O
data	O
provided	O
for	O
the	O
task	O
.	O
We	O
use	O
Moses	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
for	O
training	O
the	O
SMT	O
systems	O
.	O
NMT	O
(	O
Transformer	B-MethodName
)	O
For	O
this	O
,	O
we	O
used	O
the	O
standard	O
Transformer	B-MethodName
large	O
architecture	O
from	O
Vaswani	O
et	O
al	O
(	O
2017	O
)	O
for	O
training	O
on	O
the	O
parallel	O
data	O
provided	O
for	O
the	O
task	O
.	O
NMT	O
(	O
mBART	B-MethodName
)	O
mBART	B-MethodName
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
is	O
a	O
large	O
Transformer	B-MethodName
pre	O
-	O
trained	O
on	O
monolingual	O
data	O
for	O
25	O
languages	O
.	O
The	O
pre	O
-	O
training	O
objective	O
for	O
mBART	B-MethodName
is	O
seq2seq	B-MethodName
de	O
-	O
noising	O
for	O
natural	O
text	O
as	O
in	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
,	O
2019	O
)	O
.	O
mBART	B-MethodName
provides	O
a	O
general	O
-	O
purpose	O
pre	O
-	O
trained	O
Transformer	B-MethodName
for	O
any	O
downstream	O
task	O
.	O
It	O
has	O
been	O
shown	O
to	O
give	O
significant	O
improvements	O
over	O
the	O
random	O
initialization	O
for	O
NMT	O
and	O
is	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
many	O
low	O
resource	O
language	O
pairs	O
.	O
Implementation	O
Details	O
mBART	B-MethodName
uses	O
a	O
shared	O
subword	O
vocabulary	O
of	O
250	O
K	O
tokens	O
for	O
all	O
the	O
25	O
languages	O
present	O
in	O
the	O
pre	O
-	O
training	O
.	O
We	O
use	O
the	O
same	O
vocabulary	O
for	O
Marathi	O
and	O
Portuguese	O
also	O
,	O
even	O
though	O
they	O
were	O
not	O
used	O
during	O
the	O
pre	O
-	O
training	O
phase	O
.	O
Marathi	O
shares	O
its	O
subword	O
vocabulary	O
with	O
languages	O
like	O
Hindi	O
and	O
Nepali	O
in	O
mBART	B-MethodName
,	O
and	O
Portuguese	O
shares	O
with	O
Spanish	O
,	O
Italian	O
and	O
other	O
European	O
languages	O
present	O
in	O
mBART	B-MethodName
.	O
The	O
percentage	O
of	O
unknown	O
tokens	O
[	O
UNK	O
]	O
in	O
Marathi	O
and	O
Portuguese	O
parallel	O
datasets	O
is	O
less	O
than	O
0.003	O
%	O
when	O
using	O
the	O
shared	O
mBART	B-MethodName
vocabulary	O
.	O
Additionally	O
,	O
the	O
mBART	B-MethodName
architecture	O
requires	O
language	O
specific	O
token	O
at	O
the	O
end	O
of	O
each	O
input	O
sequence	O
to	O
provide	O
the	O
language	O
specific	O
context	O
for	O
the	O
decoder	O
.	O
Since	O
Marathi	O
and	O
Portuguese	O
were	O
not	O
present	O
during	O
the	O
pre	O
-	O
training	O
phase	O
,	O
we	O
use	O
the	O
token	O
corresponding	O
to	O
the	O
second	O
most	O
related	O
language	O
present	O
in	O
mBART	B-MethodName
pre	O
-	O
training	O
for	O
specifying	O
the	O
context	O
at	O
the	O
time	O
of	O
decoding	O
in	O
each	O
case	O
.	O
For	O
Marathi	O
,	O
we	O
used	O
the	O
Nepali	O
language	O
token	O
and	O
for	O
Portuguese	O
,	O
we	O
used	O
the	O
Italian	O
language	O
token	O
.	O
We	O
could	O
not	O
use	O
Spanish	O
language	O
token	O
for	O
Portuguese	O
because	O
we	O
are	O
doing	O
translations	O
to	O
and	O
from	O
Spanish	O
.	O

Because	O
of	O
the	O
constrained	O
nature	O
of	O
the	O
shared	O
task	O
,	O
we	O
only	O
use	O
the	O
parallel	O
data	O
provided	O
for	O
this	O
task	O
.	O
We	O
removed	O
the	O
empty	O
instances	O
for	O
both	O
language	O
pairs	O
(	O
<	O
2000	O
instances	O
)	O
.	O
For	O
es	O
↔	O
pt	O
,	O
we	O
do	O
not	O
use	O
'	O
WikiTitles	O
v2	O
'	O
part	O
of	O
the	O
parallel	O
data	O
for	O
training	O
because	O
of	O
very	O
short	O
sentences	O
in	O
the	O
dataset	O
.	O
The	O
cleaned	O
parallel	O
dataset	O
statistics	O
are	O
provided	O
in	O
Table	O
2	O
.	O
Preprocessing	O
We	O
use	O
sentence	O
piece	O
tokenization	O
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
for	O
generating	O
the	O
source	O
and	O
target	O
sequences	O
for	O
the	O
NMT	O
architectures	O
.	O
For	O
the	O
standard	O
Transformer	B-MethodName
,	O
we	O
train	O
a	O
sentence	O
piece	O
model	O
using	O
40	O
K	O
subword	O
tokens	O
for	O
hi	O
↔	O
mr	O
.	O
For	O
mBART	B-MethodName
,	O
we	O
use	O
Liu	O
et	O
al	O
(	O
2020	O
)	O
's	O
pre	O
-	O
trained	O
1	O
sentence	O
piece	O
model	O
comprising	O
of	O
250	O
K	O
subword	O
tokens	O
as	O
the	O
vocabulary	O
.	O
For	O
the	O
SMT	O
model	O
on	O
hi	O
↔	O
mr	O
,	O
we	O
also	O
use	O
the	O
monolingual	O
data	O
provided	O
for	O
this	O
task	O
.	O
We	O
extract	O
5	O
Million	O
monolingual	O
sentences	O
each	O
for	O
Hindi	O
and	O
Marathi	O
after	O
deduplication	O
and	O
use	O
this	O
set	O
for	O
training	O
the	O
language	O
models	O
.	O
We	O
use	O
Moses	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
for	O
all	O
tokenization	O
/	O
detokenization	O
scripts	O
.	O
Lample	O
et	O
al	O
(	O
2018	O
)	O
.	O
We	O
used	O
Moses	O
(	O
Koehn	O
et	O
al	O
,	O
2007	O
)	O
and	O
Giza++	O
with	O
standard	O
settings	O
to	O
train	O
the	O
SMT	O
model	O
in	O
both	O
directions	O
.	O

We	O
have	O
participated	O
in	O
the	O
Similar	O
Language	O
Translation	B-TaskName
task	O
on	O
four	O
language	O
directions	O
.	O
We	O
have	O
shown	O
that	O
pre	O
-	O
trained	O
models	O
can	O
help	O
in	O
low	O
and	O
medium	O
resource	O
NMT	O
.	O
Our	O
best	O
system	O
uses	O
the	O
pre	O
-	O
trained	O
mBART	B-MethodName
model	O
(	O
Liu	O
et	O
al	O
,	O
2020	O
)	O
and	O
fine	O
-	O
tunes	O
on	O
the	O
parallel	O
data	O
provided	O
for	O
the	O
specific	O
translation	O
task	O
.	O
Our	O
results	O
demonstrate	O
that	O
pre	O
-	O
training	O
can	O
help	O
even	O
when	O
the	O
language	O
used	O
for	O
fine	O
-	O
tuning	O
is	O
not	O
present	O
during	O
pre	O
-	O
training	O
.	O
One	O
direction	O
of	O
future	O
work	O
is	O
to	O
add	O
linguistic	O
information	O
during	O
the	O
pre	O
-	O
training	O
phase	O
to	O
get	O
more	O
fluent	O
translations	O
.	O
When	O
this	O
information	O
is	O
not	O
available	O
directly	O
(	O
especially	O
for	O
low	O
resource	O
languages	O
)	O
,	O
pre	O
-	O
training	O
on	O
a	O
related	O
high	O
resource	O
language	O
with	O
syntax	O
information	O
can	O
help	O
low	O
resource	O
languages	O
also	O
.	O
by	O
the	O
DARPA	B-DatasetName
Explainable	B-TaskName
Artificial	I-TaskName
Intelligence	I-TaskName
(	O
XAI	O
)	O
Program	O
with	O
number	O
N66001	O
-	O
17	O
-	O
2	O
-	O
4032	O
,	O
Visvesvaraya	O
Young	O
Faculty	O
Fellowships	O
by	O
Govt	O
.	O
of	O
India	O
and	O
IBM	O
SUR	O
awards	O
.	O
Any	O
opinions	O
,	O
findings	O
,	O
conclusions	O
or	O
recommendations	O
expressed	O
in	O
this	O
paper	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
necessarily	O
reflect	O
the	O
views	O
or	O
official	O
policies	O
,	O
either	O
expressed	O
or	O
implied	O
,	O
of	O
the	O
funding	O
agencies	O
.	O

Comparative	O
Analysis	O
of	O
Neural	O
QA	O
models	O
on	O
SQuAD	B-DatasetName

The	O
task	O
of	O
Question	B-TaskName
Answering	I-TaskName
has	O
gained	O
prominence	O
in	O
the	O
past	O
few	O
decades	O
for	O
testing	O
the	O
ability	O
of	O
machines	O
to	O
understand	O
natural	O
language	O
.	O
Large	O
datasets	O
for	O
Machine	O
Reading	O
have	O
led	O
to	O
the	O
development	O
of	O
neural	O
models	O
that	O
cater	O
to	O
deeper	O
language	O
understanding	O
compared	O
to	O
information	B-TaskName
retrieval	I-TaskName
tasks	O
.	O
Different	O
components	O
in	O
these	O
neural	O
architectures	O
are	O
intended	O
to	O
tackle	O
different	O
challenges	O
.	O
As	O
a	O
first	O
step	O
towards	O
achieving	O
generalization	O
across	O
multiple	O
domains	O
,	O
we	O
attempt	O
to	O
understand	O
and	O
compare	O
the	O
peculiarities	O
of	O
existing	O
end	O
-	O
to	O
-	O
end	O
neural	O
models	O
on	O
the	B-DatasetName
Stanford	I-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
SQuAD	B-DatasetName
)	O
by	O
performing	O
quantitative	O
as	O
well	O
as	O
qualitative	O
analysis	O
of	O
the	O
results	O
attained	O
by	O
each	O
of	O
them	O
.	O
We	O
observed	O
that	O
prediction	O
errors	O
reflect	O
certain	O
model	O
-	O
specific	O
biases	O
,	O
which	O
we	O
further	O
discuss	O
in	O
this	O
paper	O
.	O

Machine	O
Reading	O
is	O
a	O
task	O
in	O
which	O
a	O
model	O
reads	O
a	O
piece	O
of	O
text	O
and	O
attempts	O
to	O
formally	O
represent	O
it	O
or	O
performs	O
a	O
downstream	O
task	O
like	O
Question	B-TaskName
Answering	I-TaskName
(	O
QA	O
)	O
.	O
Neural	O
approaches	O
to	O
the	O
latter	O
have	O
gained	O
a	O
lot	O
of	O
prominence	O
especially	O
owing	O
to	O
the	O
recent	O
spur	O
in	O
developing	O
and	O
publicly	O
releasing	O
large	O
datasets	O
on	O
Machine	O
Reading	O
and	O
Comprehension	O
(	O
MRC	O
)	O
.	O
These	O
datasets	O
are	O
created	O
from	O
different	O
underlying	O
sources	O
such	O
as	O
web	O
resources	O
in	O
MS	B-DatasetName
MARCO	I-DatasetName
(	O
Nguyen	O
et	O
al	O
,	O
2016	O
)	O
;	O
trivia	O
and	O
web	O
in	O
QUASAR	B-DatasetName
-	I-DatasetName
S	I-DatasetName
and	O
QUASAR	B-DatasetName
-	I-DatasetName
T	I-DatasetName
(	O
Dhingra	O
et	O
al	O
,	O
2017	O
)	O
,	O
SearchQA	B-DatasetName
(	O
Dunn	O
et	O
al	O
,	O
2017	O
)	O
,	O
TriviaQA	B-DatasetName
(	O
Joshi	O
et	O
al	O
,	O
2017	O
)	O
;	O
news	O
articles	O
in	O
CNN	B-DatasetName
/	I-DatasetName
Daily	I-DatasetName
Mail	I-DatasetName
(	O
Chen	O
et	O
al	O
)	O
,	O
NewsQA	B-DatasetName
(	O
Trischler	O
et	O
al	O
,	O
2016	O
)	O
and	O
stories	O
in	O
NarrativeQA	B-DatasetName
(	O
Kočiskỳ	O
et	O
al	O
,	O
2017	O
)	O
.	O
Another	O
common	O
source	O
is	O
large	O
unstructured	O
text	O
documents	O
from	O
Wikipedia	O
such	O
as	O
in	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
,	O
WikiReading	B-DatasetName
(	O
Hewlett	O
et	O
al	O
,	O
2016	O
)	O
and	O
WikiHop	B-DatasetName
(	O
Welbl	O
et	O
al	O
,	O
2017	O
)	O
.	O
These	O
different	O
sources	O
implicitly	O
affect	O
the	O
nature	O
and	O
properties	O
of	O
questions	O
and	O
answers	O
in	O
these	O
datasets	O
.	O
Based	O
on	O
the	O
dataset	O
,	O
certain	O
neural	O
models	O
capitalize	O
on	O
these	O
biases	O
while	O
others	O
are	O
unable	O
to	O
.	O
The	O
ability	O
to	O
generalize	O
across	O
different	O
sources	O
and	O
domains	O
is	O
a	O
desirable	O
characteristic	O
for	O
any	O
machine	O
reading	O
system	O
.	O
Evaluating	O
and	O
analyzing	O
systems	O
on	O
QA	O
tasks	O
can	O
lead	O
to	O
insights	O
for	O
advancements	O
in	O
machine	O
reading	O
and	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
,	O
and	O
Peñas	O
et	O
al	O
(	O
2011	O
)	O
have	O
also	O
previously	O
worked	O
on	O
this	O
.	O
One	O
of	O
the	O
first	O
large	O
MRC	O
datasets	O
(	O
over	O
100k	O
QA	O
pairs	O
)	O
is	O
the	B-DatasetName
Stanford	I-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
SQuAD	B-DatasetName
)	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
.	O
For	O
its	O
collection	O
,	O
different	O
sets	O
of	O
crowd	O
-	O
workers	O
formulated	O
questions	O
and	O
answers	O
using	O
passages	O
obtained	O
from	O
∼500	O
Wikipedia	O
articles	O
.	O
The	O
answer	O
to	O
each	O
question	O
is	O
a	O
span	O
in	O
the	O
given	O
passage	O
,	O
and	O
many	O
effective	O
neural	O
QA	O
models	O
have	O
been	O
developed	O
for	O
this	O
dataset	O
.	O
Our	O
main	O
focus	O
in	O
this	O
work	O
is	O
to	O
perform	O
comparative	O
subjective	O
and	O
empirical	O
analysis	O
of	O
errors	O
in	O
answer	O
predictions	O
by	O
four	O
top	O
performing	O
models	O
on	O
the	O
SQuAD	B-DatasetName
leaderboard	O
1	O
.	O
We	O
focused	O
on	O
Bi	O
-	O
Directional	O
Attention	O
Flow	O
(	O
BiDAF	O
)	O
(	O
Seo	O
et	O
al	O
,	O
2016	O
)	O
,	O
Gated	O
Self	O
-	O
Matching	O
Networks	O
(	O
R	O
-	O
Net	O
)	O
(	O
Wang	O
et	O
al	O
,	O
2017	O
)	O
,	O
Document	O
Reader	O
(	O
DrQA	O
)	O
(	O
Chen	O
et	O
al	O
,	O
2017	O
)	O
,	O
Multi	O
-	O
Paragraph	O
Reading	B-TaskName
Comprehension	I-TaskName
(	O
DocQA	O
)	O
(	O
Clark	O
and	O
Gardner	O
,	O
2017	O
)	O
,	O
and	O
the	O
Logistic	B-MethodName
Regression	I-MethodName
baseline	O
model	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
We	O
mainly	O
choose	O
these	O
models	O
since	O
they	O
have	O
comparable	O
high	O
performance	O
on	O
the	O
evaluation	O
metrics	O
and	O
it	O
is	O
easy	O
to	O
replicate	O
their	O
results	O
due	O
to	O
availability	O
of	O
open	O
source	O
implementations	O
.	O
While	O
we	O
limit	O
ourselves	O
to	O
in	O
-	O
domain	O
analysis	O
of	O
the	O
performance	O
of	O
these	O
models	O
on	O
SQuAD	B-DatasetName
in	O
this	O
paper	O
,	O
similar	O
principles	O
can	O
be	O
used	O
to	O
extend	O
this	O
work	O
to	O
study	O
biases	O
of	O
combinations	O
of	O
different	O
models	O
on	O
different	O
datasets	O
and	O
thereby	O
understand	O
the	O
generalization	O
capabilities	O
of	O
these	O
neural	O
architectures	O
.	O
The	O
organization	O
of	O
the	O
paper	O
is	O
as	O
follows	O
.	O
Section	O
2	O
gives	O
a	O
comprehensive	O
overview	O
of	O
the	O
models	O
that	O
are	O
compared	O
in	O
further	O
sections	O
.	O
Section	O
3	O
describes	O
the	O
different	O
experiments	O
we	O
conducted	O
,	O
and	O
discusses	O
our	O
observations	O
.	O
In	O
Section	O
4	O
,	O
we	O
summarize	O
our	O
main	O
conclusions	O
from	O
this	O
work	O
and	O
describe	O
our	O
vision	O
for	O
the	O
future	O
.	O

This	O
model	O
was	O
proposed	O
as	O
a	O
baseline	O
in	O
the	O
SQuAD	B-DatasetName
dataset	O
paper	O
(	O
Rajpurkar	O
et	O
al	O
,	O
2016	O
)	O
and	O
uses	O
features	O
based	O
on	O
n	O
-	O
gram	O
frequencies	O
,	O
lengths	O
,	O
part	B-DatasetName
-	I-DatasetName
of	I-DatasetName
-	O
speech	O
tags	O
,	O
constituency	O
and	O
dependency	O
parse	O
trees	O
of	O
questions	O
and	O
passages	O
as	O
inputs	O
to	O
a	O
logistic	B-MethodName
regression	I-MethodName
classifier	O
6	O
to	O
predict	O
whether	O
each	O
constituent	O
span	O
is	O
an	O
answer	O
or	O
not	O
.	O

We	O
trained	O
the	O
aforementioned	O
end	O
-	O
to	O
-	O
end	O
neural	O
models	O
and	O
compare	O
their	O
performance	O
on	O
the	O
SQuAD	B-DatasetName
development	O
set	O
which	O
contains	O
10	O
,	O
570	O
question	O
-	O
answer	O
pairs	O
based	O
on	O
Wikipedia	O
articles	O
.	O

In	O
this	O
work	O
,	O
we	O
analyze	O
-	O
both	O
quantitatively	O
and	O
qualitatively	O
-	O
results	O
generated	O
by	O
4	O
end	O
-	O
to	O
-	O
end	O
neural	O
models	O
on	O
the	B-DatasetName
Stanford	I-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
.	O
We	O
observe	O
interesting	O
trends	O
in	O
the	O
analysis	O
,	O
with	O
some	O
error	O
patterns	O
which	O
are	O
consistent	O
across	O
different	O
models	O
and	O
some	O
others	O
which	O
are	O
specific	O
to	O
each	O
model	O
due	O
to	O
their	O
different	O
input	O
features	O
and	O
architectures	O
.	O
This	O
is	O
important	O
to	O
be	O
able	O
to	O
interpret	O
and	O
gain	O
an	O
intuition	O
for	O
the	O
effective	O
functions	O
that	O
different	O
components	O
in	O
a	O
neural	O
model	O
architecture	O
perform	O
versus	O
their	O
intended	O
functions	O
,	O
and	O
also	O
to	O
understand	O
model	O
-	O
specific	O
biases	O
.	O
Eventually	O
,	O
this	O
can	O
enable	O
us	O
to	O
come	O
up	O
with	O
new	O
models	O
including	O
specific	O
components	O
which	O
tackle	O
these	O
errors	O
.	O
Alternatively	O
,	O
the	O
overlap	O
analysis	O
demonstrates	O
that	O
learning	O
ensembles	O
of	O
different	O
neural	O
models	O
to	O
combine	O
their	O
individual	O
strengths	O
and	O
quirks	O
might	O
be	O
an	O
interesting	O
direction	O
to	O
explore	O
to	O
achieve	O
better	O
performance	O
.	O
Even	O
though	O
the	O
scope	O
of	O
this	O
paper	O
is	O
restricted	O
to	O
SQuAD	B-DatasetName
,	O
similar	O
analysis	O
can	O
be	O
done	O
for	O
any	O
datasets	O
/	O
models	O
/	O
features	O
,	O
to	O
gain	O
a	O
better	O
understanding	O
and	O
enable	O
a	O
better	O
assessment	O
of	O
stateof	O
-	O
the	O
-	O
art	O
in	O
neural	O
machine	O
reading	O
.	O
To	O
this	O
end	O
,	O
we	O
also	O
performed	O
some	O
preliminary	O
experiments	O
on	O
TriviaQA	B-DatasetName
so	O
as	O
to	O
analyze	O
the	O
difference	O
between	O
the	O
properties	O
of	O
the	O
two	O
datasets	O
,	O
but	O
were	O
unable	O
to	O
replicate	O
the	O
published	O
results	O
owing	O
to	O
pre	O
-	O
processing	O
/	O
hyperparameters	O
.	O
We	O
will	O
continue	O
to	O
work	O
on	O
this	O
since	O
the	O
ability	O
of	O
a	O
model	O
to	O
generalize	O
and	O
to	O
be	O
able	O
to	O
learn	O
from	O
a	O
particular	O
domain	O
and	O
transfer	O
some	O
knowledge	O
to	O
a	O
different	O
domain	O
is	O
a	O
very	O
exciting	O
research	O
area	O
.	O
We	O
also	O
believe	O
that	O
such	O
analysis	O
can	O
help	O
curate	O
datasets	O
which	O
are	O
better	O
indicators	O
of	O
the	O
actual	O
natural	O
language	O
'	O
reading	O
'	O
and	O
'	O
comprehending	O
'	O
capabilities	O
of	O
models	O
rather	O
than	O
falling	O
prey	O
to	O
shallow	O
pattern	O
matching	O
.	O
One	O
way	O
to	O
achieve	O
this	O
is	O
by	O
building	O
new	O
challenges	O
that	O
are	O
specifically	O
designed	O
to	O
put	O
pressure	O
on	O
the	O
identified	O
weaknesses	O
of	O
neural	O
models	O
.	O
Thus	O
,	O
we	O
can	O
move	O
towards	O
the	O
development	O
of	O
datasets	O
and	O
models	O
which	O
truly	O
push	O
the	O
envelope	O
of	O
the	O
challenging	O
machine	O
reading	O
task	O
.	O

Minimally	O
-	O
Augmented	O
Grammatical	B-TaskName
Error	I-TaskName
Correction	I-TaskName

There	O
has	O
been	O
an	O
increased	O
interest	O
in	O
lowresource	O
approaches	O
to	O
automatic	O
grammatical	B-TaskName
error	I-TaskName
correction	I-TaskName
.	O
We	O
introduce	O
Minimally	O
-	O
Augmented	O
Grammatical	B-TaskName
Error	I-TaskName
Correction	I-TaskName
(	O
MAGEC	O
)	O
that	O
does	O
not	O
require	O
any	O
errorlabelled	O
data	O
.	O
Our	O
unsupervised	O
approach	O
is	O
based	O
on	O
a	O
simple	O
but	O
effective	O
synthetic	O
error	O
generation	O
method	O
based	O
on	O
confusion	O
sets	O
from	O
inverted	O
spell	O
-	O
checkers	O
.	O
In	O
low	O
-	O
resource	O
settings	O
,	O
we	O
outperform	O
the	O
current	O
state	O
-	O
ofthe	O
-	O
art	O
results	O
for	O
German	O
and	O
Russian	O
GEC	O
tasks	O
by	O
a	O
large	O
margin	O
without	O
using	O
any	O
real	O
error	O
-	O
annotated	O
training	O
data	O
.	O
When	O
combined	O
with	O
labelled	O
data	O
,	O
our	O
method	O
can	O
serve	O
as	O
an	O
efficient	O
pre	O
-	O
training	O
technique	O
.	O

Most	O
neural	O
approaches	O
to	O
automatic	O
grammatical	B-TaskName
error	I-TaskName
correction	I-TaskName
(	O
GEC	O
)	O
require	O
error	O
-	O
labelled	O
training	O
data	O
to	O
achieve	O
their	O
best	O
performance	O
.	O
Unfortunately	O
,	O
such	O
resources	O
are	O
not	O
easily	O
available	O
,	O
particularly	O
for	O
languages	O
other	O
than	O
English	O
.	O
This	O
has	O
lead	O
to	O
an	O
increased	O
interest	O
in	O
unsupervised	O
and	O
low	O
-	O
resource	O
GEC	O
(	O
Rozovskaya	O
et	O
al	O
,	O
2017	O
;	O
Bryant	O
and	O
Briscoe	O
,	O
2018	O
;	O
Boyd	O
,	O
2018	O
;	O
Rozovskaya	O
and	O
Roth	O
,	O
2019	O
)	O
,	O
which	O
recently	O
culminated	O
in	O
the	O
low	O
-	O
resource	O
track	O
of	O
the	O
Building	O
Educational	O
Application	O
(	O
BEA	O
)	O
shared	O
task	O
.	O
1	O
We	O
present	O
Minimally	O
-	O
Augmented	O
Grammatical	B-TaskName
Error	I-TaskName
Correction	I-TaskName
(	O
MAGEC	O
)	O
,	O
a	O
simple	O
but	O
effective	O
approach	O
to	O
unsupervised	O
and	O
low	O
-	O
resource	O
GEC	O
which	O
does	O
not	O
require	O
any	O
authentic	O
error	O
-	O
labelled	O
training	O
data	O
.	O
A	O
neural	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
is	O
trained	O
on	O
clean	O
and	O
synthetically	O
noised	O
sentences	O
alone	O
.	O
The	O
noise	O
is	O
automatically	O
created	O
from	O
confusion	O
sets	O
.	O
Additionally	O
,	O
if	O
labelled	O
data	O
1	O
https://www.cl.cam.ac.uk/research/nl/	O
bea2019st	O
is	O
available	O
for	O
fine	O
-	O
tuning	O
(	O
Hinton	O
and	O
Salakhutdinov	O
,	O
2006	O
)	O
,	O
MAGEC	O
can	O
also	O
serve	O
as	O
an	O
efficient	O
pre	O
-	O
training	O
technique	O
.	O
The	O
proposed	O
unsupervised	O
synthetic	O
error	O
generation	O
method	O
does	O
not	O
require	O
a	O
seed	O
corpus	O
with	O
example	O
errors	O
as	O
most	O
other	O
methods	O
based	O
on	O
statistical	O
error	O
injection	O
(	O
Felice	O
and	O
Yuan	O
,	O
2014	O
)	O
or	O
back	O
-	O
translation	O
models	O
(	O
Rei	O
et	O
al	O
,	O
2017	O
;	O
Kasewa	O
et	O
al	O
,	O
2018	O
;	O
Htut	O
and	O
Tetreault	O
,	O
2019	O
)	O
.	O
It	O
also	O
outperforms	O
noising	O
techniques	O
that	O
rely	O
on	O
random	O
word	O
replacements	O
(	O
Xie	O
et	O
al	O
,	O
2018	O
;	O
Zhao	O
et	O
al	O
,	O
2019	O
)	O
.	O
Contrary	O
to	O
Ge	O
et	O
al	O
(	O
2018	O
)	O
or	O
Lichtarge	O
et	O
al	O
(	O
2018	O
)	O
,	O
our	O
approach	O
can	O
be	O
easily	O
used	O
for	O
effective	O
pre	O
-	O
training	O
of	O
full	O
encoder	O
-	O
decoder	O
models	O
as	O
it	O
is	O
model	O
-	O
independent	O
and	O
only	O
requires	O
clean	O
monolingual	O
data	O
and	O
potentially	O
an	O
available	O
spell	O
-	O
checker	O
dictionary	O
.	O
2	O
In	O
comparison	O
to	O
pretraining	O
with	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
,	O
2019	O
)	O
,	O
synthetic	O
errors	O
provide	O
more	O
task	O
-	O
specific	O
training	O
examples	O
than	O
masking	O
.	O
As	O
an	O
unsupervised	O
approach	O
,	O
MAGEC	O
is	O
an	O
alternative	O
to	O
recently	O
proposed	O
language	O
model	O
(	O
LM	O
)	O
based	O
approaches	O
(	O
Bryant	O
and	O
Briscoe	O
,	O
2018	O
;	O
Stahlberg	O
et	O
al	O
,	O
2019	O
)	O
,	O
but	O
it	O
does	O
not	O
require	O
any	O
amount	O
of	O
annotated	O
sentences	O
for	O
tuning	O
.	O

Our	O
minimally	O
-	O
augmented	O
GEC	O
approach	O
uses	O
synthetic	O
noise	O
as	O
its	O
primary	O
source	O
of	O
training	O
data	O
.	O
We	O
generate	O
erroneous	O
sentences	O
from	O
monolingual	O
texts	O
via	O
random	O
word	O
perturbations	O
selected	O
from	O
automatically	O
created	O
confusion	O
sets	O
.	O
These	O
are	O
traditionally	O
defined	O
as	O
sets	O
of	O
frequently	O
confused	O
words	O
(	O
Rozovskaya	O
and	O
Roth	O
,	O
2010	O
)	O
.	O
We	O
experiment	O
with	O
three	O
unsupervised	O
methods	O
for	O
generating	O
confusion	O
sets	O
:	O
Edit	O
distance	O
Confusion	O
sets	O
consist	O
of	O
words	O
with	O
the	O
shortest	O
Levenshtein	O
distance	O
(	O
Levenshtein	O
,	O
1966	O
)	O
to	O
the	O
selected	O
confused	O
word	O
.	O
Word	B-TaskName
embeddings	I-TaskName
Confusion	O
sets	O
contain	O
the	O
most	O
similar	O
words	O
to	O
the	O
confused	O
word	O
based	O
on	O
the	O
cosine	O
similarity	O
of	O
their	O
word	O
embedding	O
vectors	O
(	O
Mikolov	O
et	O
al	O
,	O
2013	O
)	O
.	O
Spell	O
-	O
breaking	O
Confusion	O
sets	O
are	O
composed	O
of	O
suggestions	O
from	O
a	O
spell	O
-	O
checker	O
;	O
a	O
suggestion	O
list	O
is	O
extracted	O
for	O
the	O
confused	O
word	O
regardless	O
of	O
its	O
actual	O
correctness	O
.	O
These	O
methods	O
can	O
be	O
used	O
to	O
build	O
confusion	O
sets	O
for	O
any	O
alphabetic	O
language	O
.	O
3	O
We	O
find	O
that	O
confusion	O
sets	O
constructed	O
via	O
spell	O
-	O
breaking	O
perform	O
best	O
(	O
Section	O
4	O
)	O
.	O
Most	O
context	O
-	O
free	O
spell	O
-	O
checkers	O
combine	O
a	O
weighted	O
edit	O
distance	O
and	O
phonetic	O
algorithms	O
to	O
order	O
suggestions	O
,	O
which	O
produces	O
reliable	O
confusion	O
sets	O
(	O
Table	O
1	O
)	O
.	O
We	O
synthesize	O
erroneous	O
sentences	O
as	O
follows	O
:	O
given	O
a	O
confusion	O
set	O
C	O
i	O
=	O
{	O
c	O
i	O
1	O
,	O
c	O
i	O
2	O
,	O
c	O
i	O
3	O
,	O
...	O
}	O
,	O
and	O
the	O
vocabulary	O
V	O
,	O
we	O
sample	O
word	O
w	O
j	O
V	O
from	O
the	O
input	O
sentence	O
with	O
a	O
probability	O
approximated	O
with	O
a	O
normal	O
distribution	O
N	O
(	O
p	O
WER	O
,	O
0.2	O
)	O
,	O
and	O
perform	O
one	O
of	O
the	O
following	O
operations	O
:	O
(	O
1	O
)	O
substitution	O
of	O
w	O
j	O
with	O
a	O
random	O
word	O
c	O
j	O
k	O
from	O
its	O
confusion	O
set	O
with	O
probability	O
p	O
sub	O
,	O
(	O
2	O
)	O
deletion	O
of	O
w	O
j	O
with	O
p	O
del	O
,	O
(	O
3	O
)	O
insertion	O
of	O
a	O
random	O
word	O
w	O
k	O
V	O
at	O
j	O
+	O
1	O
with	O
p	O
ins	O
,	O
and	O
(	O
4	O
)	O
swapping	O
w	O
j	O
and	O
w	O
j+1	O
with	O
p	O
swp	O
.	O
When	O
making	O
a	O
substitution	O
,	O
words	O
within	O
confusion	O
sets	O
are	O
sampled	O
uniformly	O
.	O
To	O
improve	O
the	O
model	O
's	O
capability	O
of	O
correcting	O
spelling	O
errors	O
,	O
inspired	O
by	O
Lichtarge	O
et	O
al	O
(	O
2018	O
)	O
;	O
Xie	O
et	O
al	O
(	O
2018	O
)	O
,	O
we	O
randomly	O
perturb	O
10	O
%	O
of	O
characters	O
using	O
the	O
same	O
edit	O
operations	O
as	O
above	O
.	O
Character	O
-	O
level	O
noise	O
is	O
introduced	O
on	O
top	O
of	O
the	O
synthetic	O
errors	O
generated	O
via	O
confusion	O
sets	O
.	O
A	O
MAGEC	O
model	O
is	O
trained	O
solely	O
on	O
the	O
synthetically	O
noised	O
data	O
and	O
then	O
ensembled	O
with	O
a	O
language	O
model	O
.	O
Being	O
limited	O
only	O
by	O
the	O
amount	O
of	O
clean	O
monolingual	O
data	O
,	O
this	O
large	O
-	O
scale	O
unsupervised	O
approach	O
can	O
perform	O
better	O
than	O
training	O
on	O
small	O
authentic	O
error	O
corpora	O
.	O
A	O
large	O
amount	O
of	O
training	O
examples	O
increases	O
the	O
chance	O
that	O
synthetic	O
errors	O
resemble	O
real	O
error	O
patterns	O
and	O
results	O
in	O
better	O
language	B-TaskName
modelling	I-TaskName
properties	O
.	O
If	O
any	O
small	O
amount	O
of	O
error	O
-	O
annotated	O
learner	O
data	O
is	O
available	O
,	O
it	O
can	O
be	O
used	O
to	O
fine	O
-	O
tune	O
the	O
pre	O
-	O
trained	O
model	O
and	O
further	O
boost	O
its	O
performance	O
.	O
Pre	O
-	O
training	O
of	O
decoders	O
of	O
GEC	O
models	O
from	O
language	O
models	O
has	O
been	O
introduced	O
by	O
Junczys	O
-	O
Dowmunt	O
et	O
al	O
(	O
2018b	O
)	O
,	O
we	O
pretrain	O
the	O
full	O
encoder	O
-	O
decoder	O
models	O
instead	O
,	O
as	O
proposed	O
by	O
Grundkiewicz	O
et	O
al	O
(	O
2019	O
)	O
.	O

Confusion	O
sets	O
On	O
English	O
data	O
,	O
all	O
proposed	O
confusion	O
set	O
generation	O
methods	O
perform	O
better	O
than	O
random	O
word	O
substitution	O
(	O
Table	O
3	O
)	O
.Confusion	O
sets	O
based	O
on	O
word	B-TaskName
embeddings	I-TaskName
are	O
the	O
least	O
effective	O
,	O
while	O
spell	O
-	O
broken	O
sets	O
perform	O
best	O
at	O
26.66	O
F	O
0.5	O
.	O
We	O
observe	O
further	O
gains	O
of	O
+1.04	O
from	O
keeping	O
out	O
-	O
of	O
-	O
vocabulary	O
spell	O
-	O
checker	O
suggestions	O
(	O
OOV	O
)	O
and	O
preserving	O
consistent	O
letter	O
casing	O
within	O
confusion	O
sets	O
(	O
Case	O
)	O
.	O
The	O
word	O
error	O
rate	O
of	O
error	O
corpora	O
is	O
an	O
useful	O
statistic	O
that	O
can	O
be	O
used	O
to	O
balance	O
precision	O
/	O
recall	O
ratios	O
(	O
Rozovskaya	O
and	O
Roth	O
,	O
2010	O
;	O
Junczys	O
-	O
Dowmunt	O
et	O
al	O
,	O
2018b	O
;	O
Hotate	O
et	O
al	O
,	O
2019	O
)	O
.	O
Increasing	O
WER	O
in	O
the	O
synthetic	O
data	O
from	O
15	O
%	O
to	O
25	O
%	O
increases	O
recall	O
at	O
the	O
expense	O
of	O
precision	O
,	O
but	O
no	O
overall	O
improvement	O
is	O
observed	O
.	O
A	O
noticeable	O
recall	O
gain	O
that	O
transfers	O
to	O
a	O
higher	O
F	O
-	O
score	O
of	O
28.99	O
is	O
achieved	O
by	O
increasing	O
the	O
importance	O
of	O
edited	O
fragments	O
with	O
the	O
edit	O
-	O
weighted	O
MLE	O
objective	O
from	O
Junczys	O
-	O
Dowmunt	O
et	O
al	O
(	O
2018b	O
)	O
with	O
Λ	O
=	O
2	O
.	O
We	O
use	O
this	O
setting	O
for	O
the	O
rest	O
of	O
our	O
experiments	O
.	O

The	O
GEC	O
task	O
involves	O
detection	O
and	O
correction	O
of	O
all	O
types	O
of	O
error	O
in	O
written	O
texts	O
,	O
including	O
grammatical	O
,	O
lexical	O
and	O
orthographical	O
errors	O
.	O
Spelling	O
and	O
punctuation	O
errors	O
are	O
among	O
the	O
most	O
frequent	O
error	O
types	O
and	O
also	O
the	O
easiest	O
to	O
synthesize	O
.	O
To	O
counter	O
the	O
argument	O
that	O
-	O
mostly	O
due	O
to	O
the	O
introduced	O
character	O
-	O
level	O
noise	O
and	O
strong	O
language	B-TaskName
modelling	I-TaskName
-	O
MAGEC	O
can	O
only	O
correct	O
these	O
"	O
simple	O
"	O
errors	O
,	O
we	O
evaluate	O
it	O
against	O
test	O
sets	O
that	O
contain	O
either	O
spelling	O
and	O
punctuation	O
errors	O
or	O
all	O
other	O
error	O
types	O
;	O
with	O
the	O
complement	O
errors	O
corrected	O
(	O
Table	O
6	O
)	O
.	O
Our	O
systems	O
indeed	O
perform	O
best	O
on	O
misspellings	O
and	O
punctuation	O
errors	O
,	O
but	O
are	O
capable	O
of	O
correcting	O
various	O
error	O
types	O
.	O
The	O
disparity	O
for	O
Russian	O
can	O
be	O
explained	O
by	O
the	O
fact	O
that	O
it	O
is	O
a	O
morphologically	O
-	O
rich	O
language	O
and	O
we	O
suffer	O
from	O
generally	O
lower	O
performance	O
.	O

We	O
have	O
presented	O
Minimally	O
-	O
Augmented	O
Grammatical	B-TaskName
Error	I-TaskName
Correction	I-TaskName
(	O
MAGEC	O
)	O
,	O
which	O
can	O
be	O
effectively	O
used	O
in	O
both	O
unsupervised	O
and	O
lowresource	O
scenarios	O
.	O
The	O
method	O
is	O
model	O
independent	O
,	O
requires	O
easily	O
available	O
resources	O
,	O
and	O
can	O
be	O
used	O
for	O
creating	O
reliable	O
baselines	O
for	O
supervised	O
techniques	O
or	O
as	O
an	O
efficient	O
pre	O
-	O
training	O
method	O
for	O
neural	O
GEC	O
models	O
with	O
labelled	O
data	O
.	O
We	O
have	O
demonstrated	O
the	O
effectiveness	O
of	O
our	O
method	O
and	O
outperformed	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
for	O
German	O
and	O
Russian	O
benchmarks	O
,	O
trained	O
with	O
labelled	O
data	O
,	O
by	O
a	O
large	O
margin	O
.	O
For	O
future	O
work	O
,	O
we	O
plan	O
to	O
evaluate	O
MAGEC	O
on	O
more	O
languages	O
and	O
experiment	O
with	O
more	O
diversified	O
confusion	O
sets	O
created	O
with	O
additional	O
unsupervised	O
generation	O
methods	O
.	O

Incorporating	O
Priors	O
with	O
Feature	O
Attribution	O
on	O
Text	B-TaskName
Classification	I-TaskName

In	O
this	O
section	O
,	O
we	O
give	O
formal	O
definitions	O
of	O
feature	O
attribution	O
and	O
a	O
primer	O
on	O
[	O
Path	O
]	O
Integrated	O
Gradients	O
(	O
IG	O
)	O
,	O
which	O
is	O
the	O
basis	O
for	O
our	O
method	O
.	O
Definition	O
2.1	O
.	O
Given	O
a	O
function	O
f	O
:	O
R	O
n	O
[	O
0	B-DatasetName
,	O
1	O
]	O
that	O
represents	O
a	O
model	O
,	O
and	O
an	O
input	O
x	O
=	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
n	O
)	O
R	O
n	O
.	O
An	O
attribution	O
of	O
the	O
prediction	O
at	O
input	O
x	O
is	O
a	O
vector	O
a	O
=	O
(	O
a	O
1	O
,	O
...	O
,	O
a	O
n	O
)	O
and	O
a	O
i	O
is	O
defined	O
as	O
the	O
attribution	O
of	O
x	O
i	O
.	O
Feature	O
attribution	O
methods	O
have	O
been	O
studied	O
to	O
understand	O
the	O
contribution	O
of	O
each	O
input	O
feature	O
to	O
the	O
output	O
prediction	O
score	O
.	O
This	O
contribution	O
,	O
then	O
,	O
can	O
further	O
be	O
used	O
to	O
interpret	O
model	O
decisions	O
.	O
Linear	O
models	O
are	O
considered	O
to	O
be	O
more	O
desirable	O
because	O
of	O
their	O
implicit	O
interpretability	O
,	O
where	O
feature	O
attribution	O
is	O
the	O
product	O
of	O
the	O
feature	O
value	O
and	O
the	O
coefficient	O
.	O
To	O
some	O
,	O
non	O
-	O
linear	O
models	O
such	O
as	O
gradient	O
boosting	O
trees	O
and	O
neural	O
networks	O
are	O
less	O
favorable	O
due	O
to	O
the	O
fact	O
that	O
they	O
do	O
not	O
enjoy	O
such	O
transparent	O
contribution	O
of	O
each	O
feature	O
and	O
are	O
harder	O
to	O
interpret	O
(	O
Lou	O
et	O
al	O
,	O
2012	O
)	O
.	O
Despite	O
the	O
complexity	O
of	O
these	O
models	O
,	O
prior	O
work	O
has	O
been	O
able	O
to	O
extract	O
attributions	O
with	O
gradient	O
based	O
methods	O
(	O
Smilkov	O
et	O
al	O
,	O
2017	O
)	O
,	O
Shapley	O
values	O
from	O
game	O
theory	O
(	O
SHAP	B-MethodName
)	O
(	O
Lundberg	O
and	O
Lee	O
,	O
2017	O
)	O
,	O
or	O
other	O
similar	O
methods	O
(	O
Bach	O
et	O
al	O
,	O
2015	O
;	O
Shrikumar	O
et	O
al	O
,	O
2017	O
)	O
.	O
Some	O
of	O
these	O
attributions	O
methods	O
,	O
for	O
example	O
Path	O
Intergrated	O
Gradients	O
and	O
SHAP	B-MethodName
,	O
not	O
only	O
follow	O
Definition	O
2.1	O
,	O
but	O
also	O
satisfy	O
axioms	O
or	O
properties	O
that	O
resemble	O
linear	O
models	O
.	O
One	O
of	O
these	O
axioms	O
is	O
completeness	O
,	O
which	O
postulates	O
that	O
the	O
sum	O
of	O
attributions	O
should	O
be	O
equal	O
to	O
the	O
difference	O
between	O
uncertainty	O
and	O
model	O
output	O
.	O

When	O
taking	O
the	O
derivative	O
with	O
respect	O
to	O
the	O
loss	B-MetricName
,	O
we	O
treat	O
the	O
interpolated	O
embeddings	O
as	O
constants	O
.	O
Thus	O
,	O
the	O
prior	O
loss	B-MetricName
does	O
not	O
back	O
-	O
propagate	O
to	O
the	O
embedding	O
parameters	O
.	O
There	O
are	O
two	O
reasons	O
that	O
lead	O
to	O
this	O
decision	O
:	O
(	O
i	O
)	O
taking	O
the	O
gradient	O
of	O
the	O
interpolate	O
operation	O
would	O
break	O
the	O
axioms	O
that	O
IG	O
guarantees	O
;	O
(	O
ii	O
)	O
the	O
Hessian	O
of	O
the	O
embedding	O
matrix	O
is	O
slow	O
to	O
compute	O
.	O
The	O
implementation	O
decision	O
does	O
not	O
imply	O
that	O
prior	O
loss	B-MetricName
has	O
no	O
effect	O
on	O
the	O
word	B-TaskName
embeddings	I-TaskName
,	O
though	O
.	O
During	O
training	O
,	O
the	O
model	O
parameters	O
are	O
updated	O
with	O
respect	O
to	O
both	O
losses	O
.	O
Therefore	O
,	O
the	O
word	B-TaskName
embeddings	I-TaskName
had	O
to	O
adjust	O
accordingly	O
to	O
the	O
new	O
model	O
parameters	O
by	O
updating	O
the	O
embedding	O
parameters	O
with	O
cross	O
-	O
entropy	O
loss	B-MetricName
.	O

Models	O
convert	O
input	O
tokens	O
to	O
embeddings	O
before	O
providing	O
them	O
to	O
convolutional	O
layers	O
.	O
As	O
embeddings	O
make	O
up	O
the	O
majority	O
of	O
the	O
parameters	O
of	O
the	O
network	O
and	O
can	O
be	O
exported	O
for	O
use	O
in	O
other	O
tasks	O
,	O
we	O
're	O
interested	O
in	O
how	O
they	O
change	O
for	O
the	O
identity	O
terms	O
.	O
We	O
show	O
10	O
nearest	O
neighbors	O
of	O
the	O
terms	O
<	O
i	O
d	O
>	O
(	O
for	O
the	O
token	O
replacement	O
method	O
)	O
,	O
"	O
gay	O
"	O
,	O
and	O
"	O
homosexual	O
"	O
-	O
top	O
two	O
identity	O
terms	O
with	O
the	O
most	O
mean	O
attribution	O
difference	O
(	O
our	O
method	O
vs.	O
baseline	O
)	O
,	O
in	O
Table	O
6	O
.	O
The	O
word	O
embedding	O
of	O
the	O
term	O
"	O
gay	O
"	O
shifts	O
from	O
having	O
swear	O
words	O
as	O
its	O
neighbors	O
to	O
having	O
the	O
<	O
pad	O
>	O
token	O
as	O
the	O
closest	O
neighbor	O
.	O
Although	O
the	O
term	O
"	O
homosexual	O
"	O
has	O
lower	O
mean	O
attribution	O
,	O
its	O
neighboring	O
words	O
are	O
still	O
mostly	O
swear	O
words	O
in	O
the	O
baseline	O
embedding	O
space	O
.	O
"	O
homosexual	O
"	O
also	O
moved	O
to	O
more	O
neutral	O
terms	O
that	O
should	O
n't	O
play	O
a	O
role	O
in	O
deciding	O
if	O
the	O
comment	O
is	O
toxic	O
or	O
not	O
.	O
Although	O
they	O
are	O
not	O
as	O
high	O
quality	O
as	O
one	O
would	O
expect	O
general	O
-	O
purpose	O
word	B-TaskName
embeddings	I-TaskName
to	O
be	O
possibly	O
due	O
to	O
data	O
size	O
and	O
the	O
model	O
having	O
a	O
different	O
objective	O
,	O
the	O
results	O
show	O
that	O
our	O
method	O
yields	O
inherently	O
unbiased	O
embeddings	O
.	O
It	O
removes	O
the	O
necessity	O
to	O
initialize	O
word	B-TaskName
embeddings	I-TaskName
with	O
pre	O
-	O
debiased	O
embeddings	O
as	O
proposed	O
in	O
Bolukbasi	O
et	O
al	O
(	O
2016	O
)	O
.	O
The	O
importance	O
weighting	O
technique	O
penalizes	O
the	O
model	O
on	O
the	O
sentence	O
level	O
instead	O
of	O
focusing	O
on	O
the	O
token	O
level	O
.	O
Therefore	O
,	O
the	O
word	O
embedding	O
of	O
"	O
gay	O
"	O
does	O
n't	O
seem	O
to	O
shift	O
to	O
neutral	O
words	O
.	O
The	O
token	O
replacement	O
method	O
,	O
on	O
the	O
other	O
hand	O
,	O
replaces	O
the	O
identity	O
terms	O
with	O
a	O
token	O
that	O
is	O
surrounded	O
with	O
neutral	O
words	O
in	O
the	O
embedding	O
space	O
,	O
so	O
it	O
results	O
in	O
greater	O
improvement	O
on	O
the	O
synthetic	O
dataset	O
.	O
However	O
,	O
since	O
all	O
identity	O
terms	O
are	O
collapsed	O
into	O
one	O
,	O
it	O
's	O
harder	O
for	O
the	O
model	O
to	O
capture	O
the	O
context	O
and	O
as	O
a	O
result	O
,	O
classification	O
performance	O
on	O
the	O
original	O
dataset	O
drops	O
.	O

