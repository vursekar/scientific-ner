Right for the Right Reason : Evidence Extraction for Trustworthy Tabular Reasoning
When pre - trained contextualized embeddingbased models developed for unstructured data are adapted for structured tabular data , they perform admirably . However , recent probing studies show that these models use spurious correlations , and often predict inference labels by focusing on false evidence or ignoring it altogether . To study this issue , we introduce the task of Trustworthy Tabular Reasoning , where a model needs to extract evidence to be used for reasoning , in addition to predicting the label . As a case study , we propose a twostage sequential prediction approach , which includes an evidence extraction and an inference stage . First , we crowdsource evidence row labels and develop several unsupervised and supervised evidence extraction strategies for INFOTABS , a tabular NLI benchmark . Our evidence extraction strategy outperforms earlier baselines . On the downstream tabular inference task , using only the automatically extracted evidence as the premise , our approach outperforms prior benchmarks .
Reasoning on tabular or semi - structured knowledge is a fundamental challenge for today 's natural language processing ( NLP ) systems . Two recently created tabular Natural language Inference ( NLI ) datasets , TabFact ( Chen et al , 2020b ) on Wikipedia relational tables and INFOTABS on Wikipedia Infoboxes help study the question of inferential reasoning over semi - structured tables . Today 's state - of - the - art for NLI over unstructured text uses contextualized embeddings ( e.g. , Devlin et al , 2019 ; Liu et al , 2019b ) . When adapted for tabular NLI by flattening tables into synthetic sentences using heuristics , these models achieve remarkable performance on the datasets . However , a recent study demonstrates that these models fail to reason prop - * Work done during an internship at Bloomberg Peter Henderson , Supertramp 1 H1 H1 : Supertramp produced 1 an album that was less than an hour long 2 . H2 : Most of Breakfast in America was recorded 3 in the last month of 1978 3 . H3 : Breakfast in America was released 4 the same month recording ended 4 . Figure 1 : A semi - structured premise ( the table ' Breakfast in America ' ) example from . Hypotheses H1 are entailed by it , H2 is neither entailed nor contradictory , and H3 is a contradiction . The Relevant column shows the hypotheses that use the corresponding row . The colored text ( and superscripts ) in the table and hypothesis highlights relevance token level alignment . erly on the semi - structured inputs in many cases . For example , they can ignore relevant rows , and ( a ) focus on the irrelevant rows ( Neeraja et al , 2021 ) , ( b ) use only the hypothesis sentence ( Poliak et al , 2018 ; Gururangan et al , 2018 ) , or ( c ) knowledge acquired during pre - training ( Jain et al , 2021 ; . In essence , they use spurious correlations between irrelevant rows , the hypothesis , and the inference label to predict labels . This paper argues that existing NLI systems optimized solely for label prediction can not be trusted . It is not sufficient for a model to be merely Right but also Right for the Right Reasons . In particular , at least identifying the relevant elements of inputs as the ' Right Reasons ' is essential for trustworthy reasoning 1 . We address this issue by introducing the task of Trustworthy Tabular Inference , where the goal is to extract relevant rows as evidence and predict inference labels . To illustrate this task , consider an example from the INFOTABS dataset in Figure 1 , which shows a premise table and three hypotheses . The figure also marks the rows needed to make decisions about each hypothesis , and also indicates the relevant tokens for each hypothesis . For trustworthy tabular reasoning , in addition to predicting the label ENTAIL for H1 , CONTRADICT for H2 and NEU - TRAL for H3 , the model should also identify the evidence rows - namely , the rows Producer and Length for hypothesis H1 , Recorded for hypothesis H2 , Released and Recorded for hypothesis H3 . As a first step , we propose a two - stage sequential prediction approach for the task , comprising of an evidence extraction stage , followed by an inference stage . In the evidence extraction stage , the model extracts the necessary information needed for the second stage . In the inference stage , the NLI model uses only the extracted evidence as the premise for the label prediction task . We explore several unsupervised evidence extraction approaches for INFOTABS . Our best unsupervised evidence extraction method outperforms a previously developed baseline by 4.3 % , 2.5 % and 5.4 % absolute score on the three test sets . For supervised evidence extraction , we annotate the IN - FOTABS training set ( 17 K table - hypothesis pairs with 1740 unique tables ) with relevant rows following the methodology of , and then train a RoBERTa LARGE classifier . The supervised model improves the evidence extraction performance by 8.7 % , 10.8 % , and 4.2 % absolute scores on the three test sets over the unsupervised approach . Finally , for the full inference task , we demonstrate that our two - stage approach with best extraction , outperforms the earlier baseline by 1.6 % , 3.8 % , and 4.2 % on the three test sets . In summary , our contributions are as follows 2 : We introduce the problem of trustworthy tabular reasoning and study a two - stage prediction approach that first extracts evidence and then predicts the NLI label . We investigate a variety of unsupervised evidence extraction techniques . Our unsupervised approach for evidence extraction outperforms the previous methods . We enrich the INFOTABS training set with evidence rows , and develop a supervised extractor that has near - human performance . We demonstrate that our two - stage technique with best extraction outperforms all the prior benchmarks on the downstream NLI task .
We begin by introducing the task and the datasets we use . Tabular Inference is a reasoning task that , like conventional NLI ( Dagan et al , 2013 ; Bowman et al , 2015 ; Williams et al , 2018 ) , asks whether a natural language hypothesis can be inferred from a tabular premise . Concretely , given a premise table T with m rows { r 1 , r 2 , . . . , r m } , and a hypothesis sentence H , the task maps them to ENTAIL ( E ) , CONTRADICT ( C ) or NEUTRAL ( N ) . We can denote the mapping as f ( T , H ) y ( 1 ) where , y { E , N , C } . For example , for the tabular premise in Figure 1 , the model should predict E , C , and N for the hypotheses H1 , H2 , and H3 , respectively . Trustworthy Tabular Inference is a table reasoning problem that seeks not just the NLI label , but also relevant evidence from the input table that supports the label prediction . We use T R , a subset of T , to denote the relevant rows or evidence . Then , the task is defined as follows . f ( T , H ) { T R , y } ( 2 ) In our example table , this task will also indicate the evidence rows T R of Producer and Length for hypothesis H1 , Recorded for hypothesis H2 , and Released and Recorded for hypothesis H3 . While the notion of evidence is well - defined for the ENTAIL and CONTRADICT labels , the NEU - TRAL label requires explanation . To decide on the NEUTRAL label , one must first search for relevant rows ( if any ) , i.e. , identify evidence in the premise tables . In fact , this is a causally correct sequential approach . Indeed , INFOTABS has multiple neutral hypotheses that are partly entailed by the table ; if any part of a hypothesis contradicts the table , then the inference label should be CONTRADICT . For example , in our example table , the premise table indicates that the album was recorded in 1978 , emphasizing the importance of the Recorded row for the hypothesis H2 . For NEUTRAL examples , we refer to any such pertinent rows as evidence . Dataset Details . There are several datasets for tabular NLI : TabFact , INFOTABS , and the Se - mEval'21 Task 9 ( Wang et al , 2021b ) and the FEVEROUS'21 shared task ( Aly et al , 2021 ) datasets . We use the INFOTABS data in this work . It contains finer - grained annotation ( e.g. , TabFact lacks NEUTRAL hypotheses ) and more complex reasoning than the others 3 . The dataset consists of 23 , 738 premisehypothesis pairs collected via crowdsourcing on Amazon MTurk . The tabular premises are based on 2 , 540 Wikipedia Infoboxes representing twelve diverse domains , and the hypotheses are short statements paired with NLI labels . All tables contain a title followed by two columns ( cf . Figure 1 ) ; the left columns are keys and the right ones are values ) . In addition to the train and development sets , the data includes multiple test sets , some of which are adversarial : α 1 represents a standard test set that is both topically and lexically similar to the training data ; α 2 hypotheses are designed to be lexically adversarial 4 ; and α 3 tables are drawn from topics unavailable in the training set . The dev and test set , comprising of 7200 table - hypothesis pairs , were recently extended with crowdsourced evidence rows . As one of our contributions , we describe the evidence rows annotation for the training set in the next Section 3 .
This section describes the process of using Amazon MTurk to annotate evidence rows for the 16 , 538 premise - hypothesis pairs that make the training set of INFOTABS . We followed the protocol of : one table and three distinct hypotheses formed a HIT . For each of the hypotheses , five annotators would select the evidence rows . We divide the tasks equally into 110 batches , each batch having 51 HITs each having three examples . To reduce bias induced by a link between the NLI label and row selection , we do not reveal the labels to the annotators . The quality control details are provided in the Appendix B. In total , we collected 81 , 282 annotations from 3 As per , 33 % of examples in INFOTABS involve multiple rows . The dataset covers all the reasoning types present in the Glue and SuperGlue Choice of Semi - structured Data . The rows of an Infobox table are semantically distinct , though all connected to the title entity . Each row can be considered a separate and uniquely distinct source of information about the title entity . Because of this property , the problem of evidence extraction is well - formed as relevant row selection . The same is not valid for unstructured text , whose units of information may be tokens , phrases , sentences or entire paragraphs , and is typically unavailable ( Ribeiro et al , 2020 ; Goel et al , 2021 ; Mishra et al , 2021 ; Yin et al , 2021 ) .
Trustworthy inference has an intrinsic sequential causal structure : extract evidence first , then predict the inference label using the extracted evidence data , knowledge / common sense , and perhaps formal reasoning ( Herzig et al , 2021 ; Paranjape et al , 2020 ) 7 . To operationalize this intuition , we chose a two - stage sequential approach which consists of an evidence extraction followed by the NLI classi - fication , as shown in Figure 2 . Notation . The function f in Eq . 2 can be rewritten with functions g and h , f ( . ) = g ( . ) , h g ( . ) , as f ( T , H ) = { g ( T , H ) , h ( g ( T , H ) , H ) } ( 3 ) Here , g extracts the evidence rows T R subset of T , and h uses the extracted evidence T R and the hypothesis H to predict the inference label y , as g ( T , H ) T R h ( T R , H ) y ( 4 ) To obtain f , we need to define the functions g and h , and a flexible representation of a semi - structured . We explore unsupervised ( $ 4.1 ) and supervised ( $ 4.2 ) methods for the evidence row extractor g.
The unsupervised approaches extract Top - K rows are based on relevance scores , where K is a hyperparameter . We use the cosine similarity between the row and the hypothesis sentence representations to score rows . We study three ways to define relevance described next .
Inspired by the Distracting Row Removal ( DRR ) heuristic of Neeraja et al ( 2021 ) , we propose DRR ( Re - Rank + Top - S τ ) , which uses fastText ( Joulin et al , 2016 ; Mikolov et al , 2018 ) based static embeddings to measure sentence similarity . We employ three modifications to improve DRR .
We observed that the raw similarity scores ( i.e. , using only fastText ) for some valid evidence rows could be low , despite exact wordlevel lexical matching with the row 's key and values . We augmented the scores by δ for each exact match to incentivize precise matches .
For most instances , the number of relevant rows ( K ) is much lower than the total number of rows ( m ) ; most examples have only one or two relevant rows . We constrained the sparsity in the extraction by capping the value of K to S m.
We use a threshold τ to select rows dynamically Top - K τ based on the hypothesis , rather than always selecting fixed K rows . We only select rows whose similarity ( after Re - Ranking ) to the hypothesis sentence representations is greater than a threshold τ . We adopt this strategy because ( a ) the number of rows in the premise table can vary across examples , and ( b ) different hypotheses may require a differing number of evidence rows .
This approach consists of two parts ( a ) aligning rows and hypothesis words , and ( b ) then computing cosine similarity between the aligned words . Specifically , we use the SimAlign ( Jalili Sabet et al , 2020 ) method for word - level alignment . SimAlign uses static and contextualized embeddings without parallel training data to get word alignments . Among the approaches explored by SimAlign , we use the Match ( mwmf ) method , which uses maximum - weight maximal matching in the bipartite weighted network formed by the word level similarity matrix . Our choice of this approach over the other greedy methods ( Itermax and Argmax ) is motivated by the fact that it finds the global optimum matching , while the other two do not . After alignment , we normalize the sum of cosine similarities of RoBERTa LARGE token embeddings 8 to derive the relevance score . Furthermore , because all rows use the same title , we assign title matching terms zero weight . This paper refers to this method as SimAlign ( Match ( mwmf ) ) .
The approach we saw in $ 4.1.2 defines rowhypothesis similarity using word alignments . As an alternative , we can directly compute similarities between the contextualised sentence embeddings of rows and the hypothesis . We explore two options here . Sentence Transformer : We use Sentence - BERT ( Reimers and Gurevych , 2019 ) and its variants ( Reimers and Gurevych , 2020 ; Thakur et al , 2021 ; Wang et al , 2021a ) , which use Siamese neural networks ( Koch et al , 2015 ; Chicco , 2021 ) . We explore several pre - trained sentence transformers models 9 for sentence representation . These models differ in ( a ) the data used for pre - training , ( b ) the main model type and it size , and ( c ) the maximum sequence length . SimCSE : SimCSE ( Gao et al , 2021 ) uses a contrastive learning to train sentence embeddings in both unsupervised and supervised settings . The former is trained to take an input sentence and reconstruct it using standard dropout as noise . The latter uses example pairs from the MNLI dataset ( Williams et al , 2018 ) with entailments serving as positive examples and contradiction serving as hard negatives for contrastive learning . We give the row sentences directly to SimCSE to get their embeddings . To avoid misleading matches between the hypothesis tokens and those in the premise title , we swap the hypothesis title tokens with a single token title from another randomly selected table of the same category . We then use the cosine similarity between SimCSE sentence embeddings to compute the final relevance score . We again use the sparsity and dynamic selection as earlier . In the study , we refer to this method as SimCSE ( Hypo - Title - Swap + Re - rank + Top - K τ ) .
The supervised evidence extraction procedure consists of three aspects : Dataset Construction . We use the annotated relevant row data ( $ 3 ) to construct a supervised extraction training dataset . Every row in the table , paired with the hypothesis , is associated with a binary label signifying whether the row is relevant or not . As before , we use the sentences from Better Paragraph Representation ( BPR ) ( Neeraja et al , 2021 ) to represent each row . Label Balancing . Our annotation , and the perturbation probing analysis of 10 , show that the number of irrelevant rows can be much larger than the relevant ones for a tablehypothesis pair . Therefore , if we use all irrelevant rows from tables as negative examples , the resulting training set would be imbalanced , with about 6× more irrelevant rows than relevant rows . We investigate several label balancing strategies by sub - sampling irrelevant rows for training . We explore the following schemes : ( a ) taking all irrelevant rows from the table without sub - sampling ( on average 6× more irrelevant rows ) referred to as Without Sample ( 6× ) , ( b ) randomly sampling unrelated rowsin the same proportion as relevant rows , referred to as Random Negative ( 1× ) , ( c ) using the unsupervised DRR ( Re - Rank + Top - S τ ) method to pick the irrelevant rows that are most similar to the hypothesis , in equal proportion as the relevant rows , referred to as Hard Negative ( 1× ) , and ( d ) same as ( c ) , except picking three times as many irrelevant rows , referred to as Hard Negative ( 3× ) 11 . Classifier Training . We train a relevant - vsirrelevant row classifier using RoBERTa LARGE 's two sentence classifier . We use RoBERTa LARGE because of its superior performance over other models in preliminary experiments , and also the fact that it is also used for the NLI classifier .
For the downstream NLI task , the function h is a two - sentence classifier whose inputs are T R ( the rows selected by g ) and the hypothesis H. We use BPR to represent T R as we did for the full table T. Since | T R | | T | , the extraction benefits larger tables ( especially in α 3 set ) which exceed the model 's token limit .
Our experiments assess the efficacy of evidence extraction ( $ 4 ) and its impact on the downstream NLI task by studying the following questions : RQ1 : What is the efficacy of unsupervised approaches for evidence extraction ? ( $ 5.2 )
First , we briefly summarize the models used in our experiments . We investigate both unsupervised ( $ 4.1 ) and supervised ( $ 4.2 ) evidence extraction methods . We use only the extracted evidence as the premise for the tabular inference task ( $ 4.3 ) . We compare both tasks against human performance . As baselines , we use the Word Mover Distance ( WMD ) of and the original DRR ( Neeraja et al , 2021 ) with Top - 4 extracted evidence rows . For DRR ( Re - Rank + Top - S τ ) , which uses static embeddings , we set the sparsity parameter S = 2 , and the dynamic row selection parameter τ = 1.0 . Our choice of S is based on the observation that in INFOTABS most ( 92 % ) instances have only one ( 54 % ) or two ( 38 % ) relevant rows . We set δ to 0.5 for all experiments . For the Sentence Transformer , we used the paraphrase - mpnet - base v2 model ( Reimers and Gurevych , 2019 ) which is a pre - trained with the mpnet - base architecture using several existing paraphrase datasets . This choice is based on performance on the development set . Both the supervised and unsupervised SimCSE models use the same parameters as DRR ( Re - Rank + Top - K τ ) . We refer to the supervised and unsupervised variants as SimCSE - Supervised and SimCSE - Unsupervised respectively . For the NLI task , we use the BPR representation over extracted evidence T R with the RoBERTa LARGE two sentence classification model . We compare the following settings : ( a ) WMD Top - 3 from , ( b ) No extraction i.e. using the full premise table with the " para " representation from , ( c ) DRR Top - 4 , ( d ) DRR ( Re - Rank + Top - 2 ( τ = 1 ) ) for training , de - velopment and test sets , ( e ) training a supervised classifier with a human oracle i.e. annotated evidence extraction as discussed in $ 3 , and using the best extraction model , i.e. supervised evidence extraction with Hard Negative ( 3× ) for the test sets , and ( f ) the human oracle across the training , development , and test sets .
Unsupervised evidence extraction . For RQ1 , Table 2 shows the performance of unsupervised methods . We see that the contextual embedding method , SimCSE - Supervised ( Hypo - Title - Swap + Re - Rank + Top - 2 ( τ = 1 ) ) , performs the best . Among the static embedding cases , DRR ( Re - Rank + Top - 2 ( τ = 1 ) ) sees substantial performance improvement over the original DRR baseline . The alignment based approach using SimAlign underperforms , especially on the α 1 and α 2 test sets . However , its performance on the α 3 data , with out of domain and longer tables , is competitive to other methods . Overall , the idea of using Top - S τ , i.e. , using the dynamic number of rows prediction and Re - Rank ( exact - match based re - ranking ) is beneficial . Previously used approaches such as DRR and WMD have low F1 - score , because of poor precision . Using Re - Rank based on exact match improves the evidence extraction recall . Furthermore , introducing sparsity with Top - S τ , i.e. considering only the Top - 2 rows ( S=2 ) and dynamic row selection ( τ = 1 ) substantially enhances evidence extraction precision . Furthermore , the zero weighting of title matches using the Hypo - Title - Swap heuristic , benefits contextualized embedding models such as SimCSE 12 . SimCSE - supervised ( Hypo - Title - Swap + Re - Rank + Top - 2 ( τ = 1 ) ) outperforms DRR ( Re - Rank + Top - 2 ( τ = 1 ) ) by 4.3 % ( α 1 ) , 2.5 % ( α 2 ) and 5.4 % ( α 3 ) absolute score . Since the table domains and the NLI reasoning involved for α 1 and α 2 are sim - ilar , so is their evidence extraction performance . However , the performance of α 3 , which contains out - of - domain and longer tables ( an average of thirteen rows , versus nine rows in α 1 and α 2 ) is relatively worse . The unsupervised approaches are still 12.69 % ( α 1 ) , 13.49 % ( α 2 ) , and 19.81 % ( α 3 ) behind the human performance , highlighting the challenges of the task . Supervised evidence extraction . For RQ2 , Table 4 shows the performance of the supervised relevant row extraction approaches that use binary classifiers trained with several sampling techniques for irrelevant rows . Overall , adding supervision is advantageous 13 . Furthermore , we observe that using the unsupervised DRR technique to extract challenging irrelevant rows , i.e. , Hard Negative , is more effective than random sampling . Indeed , using random negative examples as the irrelevant rows performs the worst . Not sampling ( 6× ) or using only one irrelevant row , namely Hard Negative ( 1× ) , also underperforms . We see that employing moderate sampling , i.e. , Hard Negative ( 3× ) , performs best across all test sets . The best supervised model with Hard Negative ( 3× ) sampling improves evidence extraction performance by 8.7 % ( α 1 ) , 10.8 % ( α 2 ) , and 4.2 % ( α 3 ) absolute score over the best unsupervised model , namely SimCSE - Supervised ( Hypo - Title - Swap + Re - Rank + Top - 2 ( τ = 1 ) ) . 14 The human oracle outperforms the best supervised model by 4.13 % ( α 1 ) and 2.65 % ( α 2 ) absolute scores - a smaller gap than the best unsupervised approach . We also observe that the supervision does not benefit the α 3 set much , where the performance gap to humans is still about 15.95 % ( only 3.80 % improvement over unsupervised approach ) . We suspect this is because of the distributional changes in α 3 set noted earlier . 13 We investigate " How much supervision is adequate ? " in Appendix A. 14 Although α2 is adversarial owing to label flipping , rendering the NLI task more difficult , both α1 and α2 have instances with the same domain tables and hypotheses with similar reasoning types , making the relevant row extraction task equally challenging . This highlights directions for future improvement via domain adaptation .
For RQ3 , we investigate how using only extracted evidence as a premise impacts the performance of the tabular NLI task . Table 3 shows the results . Compared to the baseline DRR , our unsupervised DRR ( Re - Rank + Top - 2 ( τ = 1 ) ) performs similarly for α 2 , worse by 1.12 % on α 1 , and outperforms by 0.95 % on α 3 . Using evidence extraction with the best supervised model , Hard Negative ( 3× ) , trained on human - extracted ( Oracle ) rows results in 2.68 % ( α 1 ) , 3.93 % ( α 2 ) , and 4.04 % ( α 3 ) improvements against DRR . Furthermore , using human extracted ( Oracle ) rows for both training and testing sets outperforms all models - based extraction methods . The human oracle based evidence extraction leads to largest performance improvements of 3.05 % ( α 1 ) , 4.39 % ( α 2 ) , and 6.67 % ( α 3 ) over DRR . Overall , these findings indicate that extracting evidence is beneficial for reasoning in tabular inference task . Despite using human extracted ( Oracle ) rows for both training and testing , the NLI model still falls far behind human reasoning ( Human NLI ) . This gap exists because , in addition to extracting evidence , the INFOTABS hypotheses require inference with the evidence involving common - sense and knowledge , which the NLI component does not adequately perform .
We perform an error analysis of how well our proposed supervised extraction model ( Hard Negative ( 3x ) ) performs compared to the human annotators . The model makes two types of errors : a Type I error occurs when an evidence row is marked as irrelevant , whereas Type II error occurs when an irrelevant row is marked as evidence . A Type I error will reduce the model 's precision for the extraction model , whereas a Type II error will decrease the model 's recall . Type I errors are especially concerning for the downstream NLI task . Since mislabeled evidence rows will be absent from the extracted premise , necessary evidence will be omitted , leading to inaccurate entailment labels . On the other hand , with Type II errors , when an irrelevant row is labeled as evidence , the model has to deal with from extra noise in the premise . However , all the required evidence remains . Table 5 shows a comparison of the supervised extraction ( Hard Negative ( 3x ) ) approach with the ground truth human labels on all the three test sets for both error types . On the α 3 set , Type - I and Type - II errors are substantially higher than α 1 and α 2 . This highlights the fact that on the α 3 set , the model disagrees with with humans the most . Furthermore , the ratio of Type - II over Type - I errors is much higher for α 3 . This indicates that the super - vised extraction model marks many irrelevant rows as evidence ( Type - II error ) for α 3 set . The out - ofdomain origin of α 3 tables , as well as their larger size , might be one explanation for this poor performance . Appendix C provides several examples of both types of errors .
Why Sequential Prediction ? Our choice of the sequential paradigm is motivated by the observation that it enforces a causal structure . Of course , a joint or a multi - task model may make better predictions . However , these models ignore the causal relationship between evidence selection and label prediction ( Herzig et al , 2021 ; Paranjape et al , 2020 ) . Ideally , each row is independent and , its relevance to the hypothesis can be determined on its own . In a joint or a multi - task model that exploits correlations across rows and the final label , irrelevant rows and the NLI label , can erroneously influence row selection . Future Directions . Based on the observations and discussions , we identify the future directions as follows . ( 1 ) Joint Causal Model . To build a joint or a multi - task model that follows the causal reasoning structure , significant changes in model architecture are required . Such a model would first identify important rows and then use them for NLI predictions , but without risking spurious correlations . ( 2 ) How much Supervision is Needed ? As evident from our experiments , relevant row supervision improves the evidence extraction , especially on α 1 and α 2 sets compared to unsupervised extraction . But do we need full supervision for all examples ? Is there any lower limit to supervision ? We partially answered this question in the affirmative by training the evidence extraction model with limited supervision ( semi - supervised setting ) , but a deeper analysis is needed to understand the limits . See Appendix A for details . ( 3 ) Improving Zero - shot Domain Performance . As evident from 5.2 , the evidence extraction performance of outof - domain tables in α 3 needs further improvements , setting up a domain adaptation research question as future work . ( 4 ) Finally , inspired by Neeraja et al ( 2021 ) , we may be able to add explicit knowledge to improve evidence extraction .
Tabular Reasoning Many recent studies investigate various NLP tasks on semi - structured tabular data , including tabular NLI and fact verification ( Chen et al , 2020b ; , various question answering and semantic parsing tasks ( Zhang and Balog , 2020 ; Pasupat and Liang , 2015 ; Krishnamurthy et al , 2017 ; Abbas et al , 2016 ; Sun et al , 2016 ; Chen et al , 2020c ; Lin et al , 2020 ; Zayats et al , 2021 ; Oguz et al , 2020 ; Chen et al , 2021 , inter alia ) , andtable - to - text generation ( e.g. , Parikh et al , 2020 ; Nan et al , 2021 ; Yoran et al , 2021 ; Chen et al , 2020a ) . Several strategies for representing Wikipedia relational tables are proposed , such as Ta - ble2vec ( Deng et al , 2019 ) , TAPAS ( Herzig et al , 2020 ) , TaBERT ( Yin et al , 2020 ) , TabStruc ( Zhang et al , 2020a ) , TABBIE ( Iida et al , 2021 ) , TabGCN ( Pramanick and Bhattacharya , 2021 ) and RCI ( Glass et al , 2021 ) . Yu et al ( 2018 ; and Neeraja et al ( 2021 ) study pre - training for improving tabular inference . Interpretability and Explainability Model interpretability can either be through explanations or by identifying the evidence for the predictions ( Feng et al , 2018 ; Serrano and Smith , 2019 ; Jain and Wallace , 2019 ; Wiegreffe and Pinter , 2019 ; DeYoung et al , 2020 ; Paranjape et al , 2020 ) . Additionally , NLI models ( e.g. Ribeiro et al , 2016Ribeiro et al , , 2018aZhao et al , 2018 ; Glockner et al , 2018 ; Naik et al , 2018 ; McCoy et al , 2019 ; Nie et al , 2019 ; Liu et al , 2019a ) must be subjected to numerous test sets with adversarial settings . These settings can focus on various aspects of reasoning , such as perturbed premises for evidence selection , zeroshot transferability ( α 3 ) , counterfactual premises ( Jain et al , 2021 ) , and contrasting hypotheses α 2 . Recently , Kumar and Talukdar ( 2020 ) introduced Natural - language Inference over Label - specific Explanations ( NILE ) , an NLI approach for generating labels and accompanying faithful explanations using auto - generated label - specific natural language explanations . Our work focuses on the extraction of label - independent evidence for correct inference , rather than on the generation of abstractive explanations for a given label . Comparison with Shared Tasks The Se - mEval'21 Task 9 ( Wang et al , 2021b ) and FEVEROUS'21 shared task ( Aly et al , 2021 ) are conceptually close to this work . The SemEval task focuses on statement verification and evidence extraction using relational tables from scientific articles . In this work , we focus on item evidence extraction for non - scientific Wikipedia Infobox entity tables , proposed a twostage sequential approach , and used the INFOTABS dataset which has complex reasoning and multiple adversarial tests for robust evaluation . The FEVEROUS'21 shared task focuses on verifying information using unstructured and structured evidence from open - domain Wikipedia . Our approach concerns evidence extraction from a single table rather than open - domain document , table or paragraph retrieval . Furthermore , we are only concerned with entity tables rather than relational tables or unstructured text , while the FEVEROUS data has relational tables , unstructured text , and fewer entity tables .
In this paper , we introduced the problem of Trustworthy Tabular Inference , where a reasoning model both extracts evidence from a table and predicts an inference label . We studied a two - stage approach , comprising an evidence extraction and an inference stage . We explored several unsupervised and supervised strategies for evidence extraction , several of which outperformed prior benchmarks . Finally , we showed that by using only extracted evidence as the premise , our approach outperforms previous baselines on the downstream tabular inference task .
To investigate this , we use Hard Negative ( 3x ) with RoBERTa LARGE model as our evidence extraction classifier , which is similar to the full supervision method . To simulate semi - supervision settings , we randomly sample 10 % , 20 % , 30 % , 40 % , and 50 % example instances of the train set in an incremental fashion for model training , where we repeat the random samplings three times . Figure 3 , 4 , and 5 compare the average F1 - score over three runs on the three test sets α 1 , α 2 and α 3 respectively . We discovered that adding some supervision had advantages over not having any supervision . However , we also find that 20 % supervision is adequate for reasonably good evidence extraction with only < 5 % F1 - score gap with full supervision . One key issue we observe is the lack of a visible trend due to significant variation produced by random data sub - sampling . It would be worthwhile to explore if this volatility could be reduced by strategic sampling using an unsupervised extraction model , an active learning framework , and strategic diversity maximizing sampling , which is left as future work .
Since many hypothesis sentences ( especially those with neutral labels ) require out - of - table information for inference , we introduced the option to choose out - of - table ( OOT ) pseudo rows , which are highlighted only when the hypothesis requires information that is not common ( i.e. common sense ) and missing from the table . To reduce any possible bias due to unintended associations between the NLI label and the row selections ( e.g. , using OOT for neutral examples ) , we avoid showing inference labels to the annotators 15 . To assess an annotator , we compare their annotations with the majority consensus of other annotators ' ( four ) annotations . We perform this comparison at two levels : ( a ) local - consensus - score on the most recent batch , and ( b ) cumulative - consensusscore on all batches annotated thus far . We use these consensus scores to temporarily ( local - consensus - score ) or permanently ( cumulative score ) block the poor annotators from the task . We also review the annotations manually and provide feedback with more detailed instructions and personalized examples for annotators who were making mistakes due to ambiguity in the task . We give incentives to annotators who received high consensus scores . As in previous work , we removed certain annotators ' annotations that have a poor consensus score ( cumulative score ) and published a second validation HIT to double - check each data point if necessary .
We manually inspect the Type I and Type II error instances for the supervised model and human annotation for the development set . Below , we show some of these examples where models conflict with ground - truth human annotation . We also provide a possible reason behind the model mistakes . Example III Row : The trainer of Caveat is Woody Stephens . Hypothesis : Caveat won more in winnings than it took to raise and train him . Model Prediction : Relevant Evidence Human Ground Truth : Not Relevant . Possible Reason : Model connects the ' raise and train ' term with the trainer name which is unrelated and has no connection to overall , winning races money vs spending for animal . Discussion Based on the observation from the above examples as also stated in $ 5.3 , the model fails on many examples due to its lack of knowledge and common - sense reasoning ability . One possible solution to mitigate this is by the addition of implicit and explicit knowledge on - the - fly for evidence extraction , as done for inference task by Neeraja et al ( 2021 ) .
We manually examine the human - annotated evidence in the development set . We discovered the existence of several relevant phrases / tokens which implicitly indicate the presence of evidence rows . E.g. The existence of tokens such as married , husband , lesbian , and wife in hypothesis ( H ) is very suggestive of the row Spouse being the relevant evidence . Learning such implicit relevance - based phrases and tokens connection is easy for humans and large pre - trained supervision models . It is a challenging task for similarity - based unsupervised extraction methods . Below , we show implicit relevance , indicating token and the corresponding relevant evidence rows . Relevance Indicating Phrase ( H ) Relevant Evidence Rows Key ( T )
The authors thank Bloomberg 's AI Engineering team , especially Ketevan Tsereteli , Anju Kambadur , and Amanda Stent for helpful feedback and directions . We also appreciate the useful feedback provided by Ellen Riloff and the Utah NLP group . Additionally , we appreciate the helpful inputs provided by Atreya Ghosal , Riyaz A. Bhat , Manish Srivastava , and Maneesh Singh . Vivek Gupta acknowledges support from Bloomberg 's Data Science Ph.D. Fellowship . This work was supported in part by National Science Foundation grants # 1801446 ( SaTC ) and # 1822877 ( Cyberlearning ) . Finally , we would like to express our gratitude to the reviewing team for their insightful comments .
