Lightweight Models for Multimodal Sequential Data
Human language encompasses more than just text ; it also conveys emotions through tone and gestures . We present a case study of three simple and efficient Transformer - based architectures for predicting sentiment and emotion in multimodal data . The Late Fusion model merges unimodal features to create a multimodal feature sequence , the Round Robin model iteratively combines bimodal features using cross - modal attention , and the Hybrid Fusion model combines trimodal and unimodal features together to form a final feature sequence for predicting sentiment . Our experiments show that our small models are effective and outperform the publicly released versions of much larger , state - of - the - art multimodal sentiment analysis systems .
Language is composed of three different modalities : text , audio , and video . These three modalities together make it easier for humans to convey emotion and sentiment . Thus , a machine learning model for sentiment analysis needs to learn the features and interactions of all three modalities . For example , a frown in the video can alter the emotion expressed in the text transcript , or audio intensity can help determine if a speaker is getting agitated . Multimodal learning has recently received a good deal of attention from the natural language processing community [ Sun et al , 2016 , Chen et al , 2018 , Pham et al , 2019 . The Transformer network [ Vaswani et al , 2017 ] , with its self - attention modules , has achieved strong performance in multimodal learning ; attention provides a natural way to model the relationship between pairs of modalities . In this work we investigate three small , lightweight , Transformer - based architectures for multimodal sentiment analysis and emotion recog - nition . Our first model is an implementation of the Late Fusion model commonly used as a baseline system , which assigns individual Transformer blocks to each of the three modalities for feature extraction and then combines these unimodal features to learn cross - modal interactions . The second model is an implementation of the Round Robin approach ; the model generates bimodal features by using cross - modal attention to combine pairs of modalities , one pair at a time . Our last model is a Hybrid of the early and late fusion schemes . This model merges the features extracted using a late fusion pipeline , as well as those from an early fusion pipeline , where the three modalities are concatenated and passed through a single Transformer block for feature extraction ; . We present experiments using these three models on three multimodal datasets : IEMOCAP [ Busso et al , 2008 ] , an emotion recognition dataset , and CMU - MOSI [ Zadeh et al , 2016 ] and CMU - MOSEI [ Zadeh et al , 2018b ] , two multimodal sentiment analysis datasets . Our results show that our small models are competitive with state - of - the - art models that use much more complex architectures . Our main contributions are as follows : We present three lightweight architectures for multimodal sentiment analysis that achieve comparable results to much larger , state - ofthe - art models . We analyze the effect of removing or simplifying components of state - of - the - art multimodal architectures . We conduct experiments on small training sets , demonstrating the ability of our lightweight architectures to leverage limited training data and computational resources .
We do not give an exhaustive list of prior work in multimodal sentiment analysis , but focus on recent neural approaches that achieved state - of - the - art performance at their times of publication .
The Memory Fusion Network ( MFN ) of Zadeh et al [ 2018a ] uses a separate LSTM to encode each of the three modalities and then uses attention to model cross - modal interactions for different combinations of modalities . The Recurrent Attended Variation Embedding Network ( RAVEN ) of Wang et al [ 2019 ] encodes the audio and video features using two recurrent neural networks ; these features are combined with the textual input using cross - modal attention in a Gated Modality Mixing Network . The Multi - Attention Recurrent Network ( MARN ) of Zadeh et al [ 2018c ] is an LSTM - based architecture that stores representations of each of the three modalities , which are then combined using a multi - attention block . Finally , the Multimodal Cyclic Translation Network ( MCTN ) of Pham et al [ 2019 ] produces multimodal features by translating one modality into another , learning a joint encoding in that direction , and then back - translating to learn a joint encoding in the other direction .
The Transformer network [ Vaswani et al , 2017 ] has been used widely in neural machine translation [ Tubay and Costa - jussà , 2018 , Edunov et al , 2018 , Xia et al , 2019 , Devlin et al , 2019 and has proven effective for sentiment analysis and emotion recognition . However , existing architectures are very dense compared to our three lightweight models . The Multimodal Transformer ( MuLT ) of Tsai et al [ 2019 ] modifies the Transformer block to compute cross - modal attention for two modalities at a time . It combines modalities in directed pairs , using a total of six Transformers , whose outputs are then merged into a single multimodal representation . Unlike other works , MuLT is able to handle cases where the three modalities are not aligned at the word level ; it learns soft alignments via the cross - modal attention weights for each pair of modalities . The model works well in the unaligned case , and in the aligned case , it gives state of the art performance the Happy emotion in IEMO - CAP . The Factorized Multimodal Transformer ( FMT ) of introduces Factorized Multimodal Self - Attention ( FSM ) modules , which compute self - attention over unimodal , bimodal , and trimodal inputs in parallel . FMT gives state of the art performance in the word - aligned case on CMU - MOSI and on the Sad , Angry , and Neutral emotions in IEMOCAP . We use FMT , along with the word - aligned version of MuLT , as baselines for comparison in our experiments .
The Interaction Canonical Correlation Network ( ICCN ) [ Sun et al , 2020 ] implements Deep Canonical Correlation Analysis ( DCCA ) [ Andrew et al , 2013 ] to extract bimodal features from the outer product matrix of a pair of modalities . 3 Models
We use T , A , and V , to represent the three modalities : text , audio , and video , respectively . Following the notation in [ Tsai et al , 2019 ] and , we denote the input as X T , A , V = { x T , x A , x V } where x i = [ x t , i ] for i [ T , A , V ] and t [ 1 , τ ] and τ is the length of the input sentence . Each of the three modalities has its own lowlevel features , such as the Mel spectrogram for audio or facial landmarks for video . These features are extracted at different sampling rates - one set of features per word or character for text , per millisecond for audio , and per frame for video - and thus the input sequences for the three modalities are often different . A five - thousand - millisecond audio sequence , for example , may be only a three - word sequence from a textual perspective and a 50 - frame sequence from a video perspective . We align the audio and video to the text using the timestamps provided in the text transcripts . The set of audio or video samples that correspond to a word in the transcript are combined using a series of 1D convolutional layers : X { T , A , V } = conv1D X { T , A , V } R d where d is a common feature dimension size . This procedure ensures that the input sequence length is the same across modalities .
Our three lightweight architectures are comprised of Transformer blocks [ Vaswani et al , 2017 ] , which are non - recurrent neural networks that can process sequential data . It consists of alternating attention and linear layers . The attention block of a Transformer uses multi - head attention , where each head computes scaled dot product attention : attn ( Q , K , V ) = softmax QK T √ d k V head i = attn QW Q i , KW K i , V W V i multi ( Q , K , V ) = [ head 1 ; . . . ; head h ] W O where Q , K , V represent the query , key and value ; d k is the key dimension size ; W Q i , W K i , W V i are learned projection matrices for head i ; and W O is a learned projection matrix for the attention block . In addition , Vaswani et al note that positional encodings must be added to Transformer input because there is no sequential information present in the Transformer itself : P E ( pos , 2i ) = sin ( pos/10000 2i / d model ) P E ( pos , 2i+1 ) = cos ( pos/10000 2i / d model ) X = X + P E
Figure 1 shows our Late Fusion architecture . Three unimodal Transformers learn high - level features from the low - level input features of each modality . The outputs of these unimodal Transformers are then merged together using a simple summation , rather than the merge layer used in previous work [ Tsai et al , 2019 ] , and passed to a residual network of linear layers [ Xie et al , 2017 ] for sentiment prediction . Figure 2 shows our Round Robin architecture , which is a simplification of MuTL [ Tsai et al , 2019 ] . Three cross - modal Transformers learn bimodal feaatures for ordered pairs of modalities , where the query is one modality and the key / value is the other . We use only three pairs - text query and audio key / value , audio query and video key / value , and video query and text key / valuewith bimodal information flowing in only one direction ; in contrast , MuLT uses six pairs of crossmodal Transformers , with information flowing in both directions . MuLT also uses three Transformers , one for each modality , to merge the two pairs sharing that modality as key / value ; our pairwise features are simply concatenated and passed to the output residual network . Figure 3 shows our Hybrid Fusion architecture , which uses both an early fusion approach that concatenates the inputs and passes them to a single Transformer to learn trimodal features , as well as a late fusion approach that passes each modality through a separate Transformer to learn unimodal features . The trimodal and unimodal features are concatenated together and merged using a layer of Gated Recurrent Units .
We train our models on a single NVIDIA K80 GPU . We tune hyperparameter values for our model using the validation sets provided by our evaluation datasets ; we achieve the best validation performance using 8 attention blocks per Transformer , each with 5 attention heads , and a hidden size was set to 40 . The dropout rate was set to 0.15 ; the best learning rate for IEMOCAP was 0.02 , while for CMU - MOSI and CMU - MOSEI it was 0.01 , with batch sizes of 32 , 128 , and 40 , respectively .
IEMOCAP [ Busso et al , 2008 ] consists of video recordings of 151 conversation sessions ( dialogues ) , totaling around 6k verbal interactions . This dataset is intended for multilabel emotion classification ; we evaluate on the four labeled emotions ( Happy , Sad , Angry , and Neutral ) used in previous work [ Wang et al , 2019 ] ; also following previous work , we report binary accuracy and F1 score as the evaluation metrics on this dataset . CMU - MOSI [ Zadeh et al , 2016 ] is a sentiment analysis dataset of 2199 short monologues labeled in the range [ −3 , 3 ] , with −3 being strongly negative and +3 being strongly positive . Following previous work , we report seven - class and binary accuracy , F1 score , mean absolute error , and correlation with human judgments . CMU - MOSEI [ Zadeh et al , 2018b ] is a sentiment and emotion analysis dataset of 23 K movie reviews from YouTube . As with CMU - MOSI , it is labeled in the range of [ −3 , 3 ] , and its evaluation metrics are the same as in CMU - MOSI .
Text Features : For word - level textual features we use the pretrained , 300 - dimensional , Common Crawl GloVe embeddings [ Pennington et al , 2014 ] . Audio features , including Mel - frequency cepstral coefficients and transformations thereof , as well as harmonic , percussive , and glottal source parameters . We also use COVERAP [ Degottex et al , 2014 ] to extract pitch tracking and voiced / unvoiced sloping parameters , peak slope parameters , and maximum dispersion quotients . Video Features : We extract 35 facial units using Facet [ iMotions , 2017 ] , as well as 35 facial action units and 30 facial landmark and gaze fea - tures using OpenFace [ Baltrusaitis et al , 2018 ] .
We compare our results with the state - of - the - art Multimodal Transformer ( MuLT ) 1 [ Tsai et al , 2019 ] and Factorized Multimodal Transformer ( FMT ) , as well as Memory Fusion Network ( MFN ) [ Zadeh et al , 2018a ] , Recurrent Attended Variation Embedding Network ( RAVEN ) [ Wang et al , 2019 ] , Multi - Attention Recurrent Network ( MARN ) [ Zadeh et al , 2018c ] , and Multimodal Cyclic Translation Network ( MCTN ) [ Pham et al , 2019 ] . These systems are described in Section 2 ; all attained state of the art on at least one of the evaluation datasets at their times of publication , and all use a similar feature set to our work .
We present the results of our model compared to the reported results of our baseline models in Tables 1 , 2 , and 3 . The best - performing MuLT and FMT models are extremely dense , with around 15 and 77 million parameters , respectively . In contrast , our models have between 7 - 9 million trainable parameters , depending on the architecture ; despite using about half as many parameters as MuLT , we see that our models produce comparable results . We perform fairly well on IEMOCAP , which has around 2717 training samples ; we achieve scores around 1 - 2 % below the best - performing model , FMT . Late Fusion models give state of the art results on seven - way and binary accuracy , respectively . The CMU - MOSEI dataset is much larger than IEMOCAP and CMU - MOSI , with close to 16265 training samples . Our models perform the weakest on this dataset , falling short of the state of the art models by around 2 - 3 % , suggesting that our models may be too small to learn the entire distribution . Neither MARN [ Zadeh et al , 2018c ] nor FMT reports results on CMU - MOSEI , so they are omitted from Table 3 . We also experiment with the open source code available for MuLT and FMT ( denoted by * ) . Using the hyperparameter settings provided 2 , we were nevertheless unable to match those systems ' reported performance , possibly due to differences 2 Batch size for FMT * is not given ; we use 20 , the default . resulting from random initialization . In training MuLT * and FMT * , we observe that the models are overfitting , with a mean difference of 15 - 20 % between the train and test accuracy ; in contrast , the largest train - test accuracy difference among our three models is only about 10 % . The smaller number of parameters in our model reduces the risk of overfitting on smaller datasets , while still achieving good performance on larger datasets .
We compare the training time and memory footprint of our models with MuLT * and FMT * in Table 4 on CMU - MOSEI ( the number of epochs needed for MuLT to converge , as reported by Tsai et al [ 2019 ] ) . On the smallest dataset , CMU - MOSI , training MuLT * took just over seven minutes , while FMT * took 2.5 hours . Our models train in under three minutes and outperform both MuLT * and FMT * , and this difference in training speed holds for CMU - MOSI and CMU - MOSEI as well . Thus our model , available in the supplementary materials 4 , is the fastest and best - performing multimodal sentiment system currently available for public use . We also conduct experiments on a substantially reduced IEMOCAP training subset of 1284 samples , matching the size of CMU - MOSI , which we create by randomly sampling from the full IEMO - CAP training set . Table 5 shows the results of our models , as well as MuLT * and FMT * , retrained on this smaller IEMOCAP training set , and evaluated on the full IEMOCAP test set . We see that our models , with their smaller numbers of parameters , are better able to learn from limited training data than are state - of - the - art models with double or more the number of trainable parameters .
We perform ablation experiments on our models using the IEMOCAP dataset ; ablation results for CMU - MOSI and CMU - MOSEI are omitted due to space constraints , but exhibit similar trends . Table 6 presents the results of modality ablation on the simplest Late Fusion model ; it clearly shows that unimodal and bimodal models are unable to match the performance of a full multimodal model . This demonstrates the importance of considering all modalities when analyzing spoken language , since some of the emotions or sentiment may be dependent more on the audio or the visual actions of the speaker , rather than the text . Examining the unimodal results , we see that the Text modality is the most informative for predicting Happy , Sad , and Neutral , while Audio is the most informative for Angry . However , the bimodal results do not always match the unimodal results . The best - performing bimodal model for Happy is [ V , A ] , despite Video being the worst - performing single modality , and [ T , A ] is the worst - performing bimodal model , despite both Text and Audio outperforming Video individually . Considering the other three emotions , we see that the best bimodal model varies between [ T , A ] and [ V , A ] , with [ T , V ] generally performing the worst . Table 7 shows the results of modality ablation on the Round Robin model ; as the architecture does not support unimodal experiments , only bimodal results are shown . Comparing Table 6 to Table 7 , we see that the cross - modal Transformers of the full Round Robin model are outperformed by the full Late Fusion model . However , the relative performance among modality pairs is consistent across Tables 6 and 7 . Finally , Table 8 shows the results of modality ablation on the Hybrid Fusion model , where we compare the relative contributions of the early fusion and late fusion halves of the architecture . The top of the table shows the results of reducing the early fusion half to only two modalities while retaining all three modalities in the late fusion half , and the bottom shows the results of reducing the late fusion half to two modalities while retaining all three in the early fusion half ; in both sets of experiments , the overall model has access to all three modalities , but only through either the early fusion path or the late fusion path . Surprisingly , although standalone early fusion models are outperformed by standalone late fusion models [ Tsai et al , 2019 ] , we find that a hybrid model containing a full , trimodal early fusion half is more robust to modality ablation in its late fusion half than a model with a full late fusion half is to an ablated early fusion half . Our results in this experiment also show greater variability among modality pairs . The [ T , A ] combination , which gave the best performance in the Late Fusion and Round Robin experiments , remains the strongest modality pair for the full early fusion , bimodal late fusion model . In contrast , for the bimodal early fusion , full late fusion model , [ T , A ] is outperformed by one of the two Video - based modality pairs , [ T , V ] or [ V , A ] , on each of the four emotions , suggesting that the performance gap of early versus late fusion differs across modalities .
The effect of direction on our Round Robin model is shown in Table 9 ; this experiment shows the impact of the direction of information flow across modalities within the model . Comparing our results to those of MuLT and MuLT * , we see that capturing information flow in one direction , text to audio to video and back to text , is enough for a model to give good predictions , without requiring the additional overhead of handling both directions . We can also see that the direction does matter ; the performance of the Round Robin model with information flowing in the opposite direction , from video to audio to text and back to video , is relatively poor . These results suggest that the interactions between pairs of modalities are directed .
We have presented three lightweight architectures for multimodal sentiment analysis and emotion recognition . The Late Fusion model merges unimodal features , the Round Robin model iteratively combines bimodal features , and the Hybrid Early - Late Fusion model combines early - fusion trimodal and late - fusion unimodal features . Our proposed models are much smaller in size compared to existing state - of - the - art models ; they are able to attain new state - of - the - art scores on the CMU - MOSI and CMU - MOSEI datasets on two metrics , while remaining competitive on the others . Further , our experiments analyzing the relative contribution of modalities and architecture components in our models suggest new directions for developing multimodal systems . We hope that our simple architectures for sentiment and emotion detection , currently the fastest and best - performing publicly available system , as well as the insights revealed in our experimental results , can be useful for further research in the field .
