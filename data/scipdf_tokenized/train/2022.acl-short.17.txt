The Power of Prompt Tuning for Low - Resource Semantic Parsing
Prompt tuning has recently emerged as an effective method for adapting pre - trained language models to a number of language understanding and generation tasks . In this paper , we investigate prompt tuning for semantic parsing - the task of mapping natural language utterances onto formal meaning representations . On the low - resource splits of Overnight and TOPv2 , we find that a prompt tuned T5 - xl significantly outperforms its fine - tuned counterpart , as well as strong GPT - 3 and BART baselines . We also conduct ablation studies across different model scales and target representations , finding that , with increasing model scale , prompt tuned T5 models improve at generating target representations that are far from the pre - training distribution .
With the widespread success of pre - trained language models ( LMs ; Devlin et al 2019 ; Raffel et al 2020 ; Bommasani et al 2021 ) , it becomes increasingly important to explore how such models can be adapted to downstream tasks . One adaptation method which has recently attracted much attention is prompt design ( Brown et al , 2020 ; Shin et al , 2020 ) , which modulates the behaviour of a LM through a task description and a few inputoutput examples . Brown et al ( 2020 ) show that this adaptation strategy is increasingly effective for larger LMs . However , prompt design is sensitive to the exact phrasing of the prompt , and , more importantly , performs worse than fine - tuning models on task - specific examples ( Lester et al , 2021 ) . Prompt tuning has recently arisen as a strong performing alternative adaption method ( Lester et al , 2021 ) . Rather than hand - designing discrete prompts , prompt tuning optimizes the embeddings of a number of task - specific prompt tokens . In contrast to fine - tuning , this method keeps almost all LM parameters frozen . On a set of language understanding tasks , Lester et al ( 2021 ) show that prompt tuning becomes competitive with finetuning for the largest pre - trained T5 models ( Raffel et al , 2020 ) . Li and Liang ( 2021 ) also explore a related parameter - efficient adaptation method called prefix - tuning , finding that it outperforms fine - tuning on low - resource natural language generation tasks . In this paper , we investigate prompt tuning for semantic parsing . This task is fundamentally different from the aforementioned language understanding and generation tasks , as it requires that models output formal meaning representations which do not resemble the natural language distribution seen during pre - training . In particular , we focus on the low - resource setup because examples for semantic parsing are difficult and expensive to collect ( Wang et al , 2015 ; Marzoev et al , 2020 ) . We therefore evaluate prompt tuning on two datasets : the 200 - shot version of Overnight ( Wang et al , 2015 ; Shin et al , 2021 ) and the low - resource splits TOPv2 ( Chen et al , 2020 ) . On both datasets , we compare prompt tuning T5 against fine - tuning and investigate the effect of canonicalizing the meaning representation , i.e. to what extent naturalizing the logical forms influences performance . In addition , we study the effect of T5 model scale on Overnight as well as varying data regimes on TOPv2 . Our main findings can be summarized as follows : For large T5 models , prompt tuning significantly outperforms fine - tuning in the low - data regime , resulting in an absolute improvement of 6 % and 15 % on Overnight and TOPv2 , respectively . This performance gap decreases when more training data becomes available . With growing model size , prompt tuned T5 models are increasingly capable of outputting diverse target representations ( see Figure 1 ) . On Overnight , we find that the disparity between canonical and meaning representations shrinks from 17 % to 4 % for T5 - small and T5 - xl , respectively . On TOPv2 , prompt tuned T5 - large models are much better at generating out - of - vocabulary tokens than T5 - small .
Our work is related to recent work on semantic parsing and prompt tuning , which we briefly describe below .
Semantic parsing is the task of converting a natural language utterance u = ( u 1 , . . . , u N ) to a formal meaning representation z = ( z 1 , . . . , z M ) . These meaning representations , also referred to as logical forms , can be interpreted by machines and executed in a real environment . For example , ThingTalk ( Campagna et al , 2019 ) and TOP ( Gupta et al , 2018 ) are meaning representations for executing commands of virtual assistants , while SQL is a representation for interacting with relational databases . In recent years , neural sequence - to - sequence models have become the dominant approach for semantic parsing tasks ( Dong and Lapata , 2016 ) . Canonicalization A common simplification step in semantic parsing is to canonicalize the meaning representations . That is , the meaning representation z is naturalized to a canonical form c through a grammar or set of rules . Examples of the meaning and canonical representation for Overnight and TOPv2 ( Wang et al , 2015 ; Chen et al , 2020 ) can be found in Fig . 2 . When canonical representations are available , Berant and Liang ( 2014 ) argue that semantic parsing can be seen as a paraphrase task . They propose to use a paraphrase model - using e.g. word vectors trained on Wikipedia - to find the best paraphrase of utterance u among a set of canonical utterances . They show this paraphrase model improves results over directly generating logical forms on two question - answering datasets . Marzoev et al ( 2020 ) extends this work by showing that pre - trained language models like BERT can be effective paraphrasers . While Berant and Liang ( 2014 ) ; Marzoev et al ( 2020 ) use models to score canonical utterances , Shin et al ( 2021 ) propose to constrain the generation process of autoregressive models like BART and GPT - 3 . On a number of few - shot semantic parsing tasks , they demonstrate the benefit of generating canonical representations over meaning representations .
Lester et al ( 2021 ) evaluates prompt tuning on SuperGLUE , a benchmark consisting of eight language understanding tasks . They find that prompt tuning becomes competitive with fine - tuning for the largest T5 model . Li and Liang ( 2021 ) propose prefix - tuning to adapt BART and GPT - 2 for natural language generation tasks . This method differs from Lester et al ( 2021 ) in that it prepends trainable embeddings for each layer of the language model rather than introducing token embeddings at the input layer . They demonstrate that pre - fix outperforms fine - tuning baselines . Similarly , Liu et al ( 2021 ) also show encouraging results for prompt tuning on natural language understand and generation tasks . Qin and Eisner ( 2021 ) also explores prompt tuning but for a knowledge extraction task . Inserting general adapter layers into pre - trained language models is also proposed in Houlsby et al ( 2019 ) ; Mahabadi et al ( 2021 ) . Related to our work are also other few - shot adaptation techniques like PET ( Schick and Sch√ºtze , 2021 ) . Moreover , adapter layers have also been explored in the computer vision domain ( Rebuffi et al , 2017 ; de Vries et al , 2017 ) .
To evaluate low - resource prompt tuning , we compare against fine - tuned variants of the same model on two semantic parsing datasets with canonical representations available . We compare both large and small variants of the T5 architecture on these datasets and experiment with various canonicalized representations .
Overnight The Overnight semantic parsing dataset ( Wang et al , 2015 ) consists of 13 , 682 natural utterance , canonical form , meaning representation triples split across eight domains . To simulate low - resource splits of this dataset , we follow Shin et al and create randomly subsampled splits of 200 training examples for each domain , using 20 % of the remaining data for validation . We measure and report denotation accuracy by evaluating all predicted queries using the SEMPRE toolkit ( Berant et al , 2013 ) . We repeat each experiment on Overnight with five different random splits . TOPv2 Chen et al ( 2020 ) introduce the TOPv2 dataset , a task - oriented semantic parsing dataset with eight domains , two of which come with predefined low - resource splits . The authors propose a principled way of constructing low - resource training sets , samples per intent and slot ( SPIS ) , intended to ensure equal exposure to ontology labels across domains of varying complexity . We experiment with the weather and reminder domains at the 10 , 25 , and 500 SPIS resource splits , performing five runs on each model varying the random seed . The reminder domain is the most challenging with 19 intent labels , 32 slot labels , and with 21 % of the programs having a depth greater than 2 . Weather in comparison has 7 intent labels , 11 slot labels , and no programs with depth greater than 2 .
Overnight uses a context - free synchronous grammar to generate canonical representations for the logical forms . As can be seen in Fig . 2 , these canonical representations resemble natural language .
Chen et al apply a set of simple modifications to the TOPv2 meaning representations to arrive at a canonical form used in all their experiments . Unlike Overnight , these pre - processing steps are largely small encoding differences and do not change the syntactic structure of the logical forms . We adopt all of these canonicalization steps ( except for lexicographic sorting of the semantic parse tree ) and add an ontology label shortening step . Examples of these transformations can be seen in Fig . 2 and are briefly described below . Simplify removes redundant utterance tokens unnecessary for interpreting the meaning representation . Out - of - Vocab adds the entire intent or slot label to the tokenizer as a new single tokens with a corresponding randomly initialized embedding . In - Vocab replaces the intent and slot labels with a short unique identifier representable by the pre - trained tokenizer . We perform an ablation over these canonicalization choices , repeating each experiment three times with varying random seed . For each domain , we report the average over 5 runs trained on randomly sampled splits of 200 examples for fine - tuned ( FT ) and prompt tuned ( PT ) models .
We provide training details and hyperparameters for all models in Appendix A. Below , we briefly explain the prompt - tuning methodology .
Prompt tuning , as proposed by Lester et al ( 2021 ) , prepends a sequence of continuous embeddings p = ( p 1 , . . . , p K ) to the sequence input embeddings e ( u ) = ( e ( u 1 ) , . . . , e ( u N ) ) before feeding it to a language model with parameters Œ∏ . During prompt tuning we optimize the prompt embeddings ( p 1 , . . . , p K ) exclusively , keeping the language model parameters Œ∏ and the pretrained vocabulary embeddings fixed . Note that this process still requires backpropagating gradients through the full language model . Like fine - tuning models , we maximize the likelihood of generating the output sequence z.
In ( 2020 ) . In Table 4 , we summarize the results of the canonicalization ablation study for TOPv2 .
We find that prompt tuning improves over finetuning for all large model configurations and target representations . On Overnight , prompt tuned denotation accuracy exceeds fine - tuned counterparts by up to 5 points with T5 - large and T5 - xl . For T5 - small and T5 - base , prompt tuning remains competitive ( within 1 % average accuracy ) with fine - tuning when predicting canonical forms . On TOPv2 , prompt tuning achieves an absolute improvement of 15 % mean accuracy over fine - tuning on the lowest SPIS split . This performance disparity lessens when training data increases ; however , prompt tuned T5 - large continues to beat its finetuned counterpart by 5 points at 500 SPIS and the BART - CopyPtr model by 1.4 points . Our prompt tuning models outperform previously reported results on these datasets . On Overnight , our best model - T5 - xl PT with canonical representations and constrained decodingoutperforms the BART FT model of Shin et al ( 2021 ) by 5 accuracy points , and GPT - 3 by more than 2 points . On the 25 SPIS split of TOPv2 , we see an average improvement of more than 5 points compared to the BART - CopyPTR of Chen et al ( 2020 ) .
Our main finding is that prompt tuned T5 models become better at generating meaning representations with increased model size . On Overnight , we see the absolute difference between canonical and meaning representations shrink from 17.5 points for T5 - small to 3.4 points for T5 - xl ( Table 1 ) . This gap shrinks another 18 % to 2.8 points when we apply constrained decoding to T5 - xl ( Table 2 ) . By contrast , Shin et al ( 2021 ) reports an 11.7 point difference when prompting GPT - 3 . For our finetuning baselines , we observe a small performance gap of 4 points across target representations for BART and T5 - xl , while we observe no gap for T5 - small , T5 - base , and T5 - large models . In our TOPv2 experiments we find similar evidence of large T5 model flexibility for generating sequences far from the training distribution . In particular , for our most intrusive canonicalization scheme Out - of - Vocab , which adds novel tokens to the vocabulary and leaves these embeddings un - trained , we find no significant reduction in performance for T5 - large across all data resource levels . T5 - small , in comparison , sees almost a 50 % drop in performance relative to no canonicalization ( None ) at the 10 SPIS level and continues to underperform by 33 % at the 500 SPIS level . Interestingly , we find that In - Vocab drastically reduces performance for T5 - small at the 10 SPIS level - 30.9 % vs. 43.4 % for None - but slightly outperforms it at 500 SPIS . We speculate that In - Vocab effectively anonymizes the ontol - ogy tokens , obscuring information that is useful for prediction . In low - data regimes there is not enough training data to learn the semantics of these anonymized tokens , whereas with enough data this problem vanishes .
We find that prompt tuning is an effective method for adapting language models to the semantic parsing task . Prompt tuning significantly outperforms fine - tuning in low - data regimes , and remains competitive in the fully supervised setting . We furthermore find that while canonicalizing meaning representations can slightly improve performance , the disparity between target representations decreases when prompt tuning larger T5 models . This result differs from previous work ( Shin et al , 2021 ) which suggested that pre - trained LMs are much better equipped to output canonical than meaning representations . However , a significant limitation of prompt tuning is that it takes more time to converge than fine - tuning . We believe one fruitful direction for future research is to find ways to reduce the compute required to prompt tune .
There are two main limitations of this work . The first is the limited analysis of the learned prompts . While concurrent work has shown that interpreting prompts is a difficult task , it is still an important consideration and left for future work ( Khashabi et al , 2021 ) . Secondly , training prompts on meaning representations requires substantially more compute than fine - tuning . This may exacerbate inequalities in regions where access to data and compute are similarly limited ( Ahia et al , 2021 ) .
Here we provide all model details and hyperparameters to reproduce our results . We experiment with BART and T5 ( Lewis et al , 2020 ; Raffel et al , 2020 ) , two large pre - trained encoder - decoder language models . BART is trained on the same 160 GB text dataset used to train RoBERTa ( Lewis et al , 2020 ) ( Paszke et al , 2019 ; Wolf et al , 2020 ) . Fine - tuning baseline We compare against baselines that fine - tune all parameters of BART and T5 . We train the T5 models with AdaFactor ( Shazeer and Stern , 2018 ) and BART with Adam ( Lewis et al , 2020 ; Kingma and Ba , 2015 ) . On TOPv2 , we use a learning rate of 10 ‚àí4 and batch size of 128 . On Overnight , we use a learning rate of 10 ‚àí3 and a batch size of 64 across all sizes of T5 . On both datasets , we train for 5000 epochs and perform model selection by early stopping on the validation set . Prompt tuning We follow the prompt tuning procedure proposed by Lester et al for T5 . We use 150 prompt tokens for all model sizes with a learning rate of 0.3 optimized with AdaFactor . We train for 5000 epochs on most domains , although it sometimes took as many as 20000 epochs to converge on the low - resource splits . Like the fine - tuned baseline , we perform model selection with best exact match accuracy on the validation set . We apply the same method to BART and found that it did not converge under a number of hyperparameter configurations . We therefore exclude prompt tuned BART models from our results 1 . Constrained Decoding We implement grammarconstrained decoding by building a prefix tree containing all canonical or meaning representations in the dataset as in Shin et al ( 2021 ) . When doing constrained decoding we perform a beam search with 10 beams and use the prefix tree to look up valid single token continuations of the decoded sequence .
For completeness , we provide all Overnight results in Table 5 .
Prompt tuned parameter efficiency comes at a cost : we find that prompt tuning takes significantly longer to train with early stopping than does finetuning . On the Overnight dataset , fine - tuned models typically took 250 epochs before validation performance plateaued . Our prompt tuned models frequently took more than 1000 epochs when predicting canonical representations , and up to 5 , 000 when predicting meaning representations . In Figure 3 , we show example training curves for prompt tuning and fine - tuning .
5 : Results across all model size , target representation , tuning method , and decoding method for Overnight dataset . BART , GPT - 2 , and GPT - 3 results results are included from Shin et al ( 2021 )
