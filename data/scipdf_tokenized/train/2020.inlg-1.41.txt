Transformer based Natural Language Generation for Question - Answering
This paper explores Natural Language Generation within the context of Question - Answering task . The several works addressing this task only focused on generating a short answer or a long text span that contains the answer , while reasoning over a Web page or processing structured data . Such answers ' length are usually not appropriate as the answer tend to be perceived as too brief or too long to be read out loud by an intelligent assistant . In this work , we aim at generating a concise answer for a given question using an unsupervised approach that does not require annotated data . Tested over English and French datasets , the proposed approach shows very promising results . 25 50 rank A1 / Bert / Cmbert - base A2 / Bert / flaubert - small A1 / Bert / xlm - roberta - base A1 / Bert / flaubert - base - unc A1 / Bert / xlm - mlm - enfr - 1024 A1 / Bert / xlm - roberta - large A2 / Bert / gpt2 A1 / Bert / bert - base - mlg - unc A1 / Bert / bert - base - mlg A2 / Bert / xlm - clm - enfr - 1024 A1 / Bert / xlm - clm - enfr - 1024 A2 / Bert / xlm - mlm - enfr - 1024 A2 / Bert / flaubert - large A2 / Bert / xlm - roberta - base A1 / Bert / flaubert - large A1 / Bert / flaubert - small A1 / Bert / openai - gpt A2 / Bert / Cmbert - base A2 / Bert / flaubert - base A2 / Bert / flaubert - base - unc A2 / Bert / xlm - roberta - large A1 / Bert / gpt2 A2 / Bert / bert - base - mlg - unc A2 / Bert / gpt2 - medium A1 / Bert / gpt2 - large A2 / Bert / bert - base - mlg A1 / Bert / gpt2 - medium
Question - Answering systems ( QAS ) aim at analyzing and processing user questions in order to provide relevant answers ( Hirschman and Gaizauskas , 2001 ) . The recent popularity of intelligent assistants has increased the interest in QAS which have become a key component of " Human - Machine " exchanges since they allow users to have instant answers to their questions in natural language using their own terminology without having to go through a long list of documents to find the appropriate answers . Most of the existing research work focuses on the major complexity of these systems residing in the processing and interpretation of the question that expresses the user 's need for information , without considering the representation of the answer itself . Usually , the answer is either represented by a short set of terms answering exactly the question ( case of QAS which extract answers from structured data ) , or by a text span extracted from a document which , besides the exact answer , can integrate other unnecessary information that are not relevant to the context of the question asked . The following presents two answers for Who is the thesis supervisor of Albert Einstein ? possibly generated by two systems :
Albert Einstein is a German - born theoretical physicist who developed the theory of relativity , one of the two pillars of modern physics . Given the specificity of QAS which extract answers from structured data , users generally receive only a short and limited answer to their questions as illustrated by the example above . This type of answer representation might not meet the user expectations . Indeed , the type of answer given by the first system can be perceived as too brief not recalling the context of the question . The second system returns a passage which contains information that are out of the question 's scope and might be deemed by the user as irrelevant . It is within this framework that we propose in this article an approach which allows to generate a concise answer in natural language ( e.g. The thesis superviser of Albert Einstein was Alfred Kleiner ) that shows very promising results tested over French and English questions . This approach is a component of a QAS that we proposed in Rojas Barahona et al ( 2019 ) and that we will briefly present in this article . In what follows , we detail in section 3 the approach we propose for answer generation in Natural Language and we briefly discuss the QAS developed . We present in section 4 the experiments that we have conducted to evaluate this approach .
The huge amount of information available nowadays makes the task of retrieving relevant informa - tion complex and time consuming . This complexity has prompted the development of QAS which help spare the user the search and the information filtering tasks , as it is often the case with search engines , and directly return the exact answer to a question asked in natural language . The QAS cover mainly three tasks : question analysis , information retrieval and answer extraction ( Lopez et al , 2011 ) . These tasks have been tackled in different ways , considering the knowledge bases used , the types of questions addressed ( Iida et al , 2019 ; Zayaraz et al , 2015 ; Dwivedi and Singh , 2013 ; Lopez et al , 2011 ) and the way in which the answer is presented . In this article , we particularly focus on the answer generation process . We generally notice two forms of representation addressed in literature . The answer can take the form of a paragraph selected from a set of text passages retrieved from the web ( Asai et al , 2018 ; Du and Cardie , 2018 ; Wang and Jiang , 2016 ; Wang et al , 2017 ; Oh et al , 2016 ) , as it can also be the exact answer to the question extracted from a knowledge base ( Wu et al , 2003 ; Bhaskar et al , 2013 ; Le et al , 2016 ) . Despite the abundance of work in the field of QAS , the answers generation issue has received little attention . A first approach indirectly addressing this task has been proposed in Brill et al ( 2001Brill et al ( , 2002 . Indeed , the authors aimed at diversifying the possible answer patterns by permuting the question 's words in order to maximise the number of retrieved documents that may contain the answer to the given question . Another answer representation approach based on rephrasing rules has also been proposed in Agichtein and Gravano ( 2000 ) ; Lawrence and Giles ( 1998 ) within the context of query expansion task for document retrieval and not purposely for the question - answering task . The few works that have considered this task within the QAS framework have approached it from a text summary generation perspective ( Ishida et al , 2018 ; Iida et al , 2019 ; Rush et al , 2015 ; Chopra et al , 2016 ; Nallapati et al , 2016 ; Miao and Blunsom , 2016 ; See et al , 2017 ; Oh et al , 2016 ; Sharp et al , 2016 ; . These works consist in generating a summary of a single or various text spans that contain the answer to a question . Most of these works have only considered causality questions like the ones starting with " why " and whose answers are para - graphs . To make these answers more concise , the extracted paragraphs are summed up . Other approaches ( Kruengkrai et al , 2017 ; Girju , 2003 ; Verberne et al , 2011 ; Oh et al , 2013 ) have explored this task as a classification problem that consists in predicting whether a text passage can be considered as an answer to a given question . It should be noted that these approaches only intend to diversify as much as possible the answer representation patterns to a given question in order to increase the probability of extracting the correct answer from the Web and do not focus on the answer 's representation itself . It should also be noted that these approaches are only applicable for QAS which extract answers as a text snippet and can not be applied to short answers usually extracted from knowledge bases . The work presented in Pal et al ( 2019 ) tried to tackle this issue by proposing a supervised approach that was trained on a small dataset whose questions / answers pairs were extracted from machine comprehension datasets and augmented manually which make generalization and capturing variation very limited . Our answer generation approach differs from these works as it is unsupervised , can be adapted to any type of factual question ( except for why ) and is based only on easily accessible and unannotated data . Indeed , we build upon the intuitive hypothesis that a concise answer and easily pronounced by an intelligent assistant can in fact consist of a reformulation of the question asked . This approach is a part of a QAS that we have developed in Rojas Barahona et al ( 2019 ) that extracts the answer to a question from structured data . In what follows , we detail in section 3 the approach we propose for answer generation in Natural Language and we briefly discuss the QAS developed . We present in section 4 the experiments that we have conducted to evaluate this approach . and we conclude in section 5 with the limitations noted and the perspectives considered .
The answer generation approach proposed is a component of a system which was developed in Rojas Barahona et al ( 2019 ) and which consists in a spoken conversational question - answering system which analyses and translates a question in natural language ( French or English ) in a formal representation that is transformed into a Sparql query 1 . The Sparql query helps extracting the answer to the given question from an RDF knowledge base , in our case Wikidata 2 . The extracted answer takes the form of a list of URIs or values . Although the QAS that we have developed ( Rojas Barahona et al , 2019 ) is able to find the correct answer to a question , we have noticed that its short representation is not user - friendly . Therefore , we propose an unsupervised approach which integrates the use of Transformer models such as BERT ( Devlin et al , 2019 ) and GPT ( Radford et al , 2018 ) . The choice of an unsupervised approach arises from the fact that there is no available training dataset associating a question with an exhaustive and concise answer at the same time . such dataset could have helped use an End - to - End learning neural architecture that can generate an elaborated answer to a question . This approach builds upon the fact that we have already extracted the short answer to a given question and assumes that a user - friendly answer can consist in rephrasing the question words along with the short answer . This approach is composed of two fundamental phases : The dependency analysis of the input question and the answer generation using Transformer models .
For the dependency analysis , we use an extended version of UDPipeFuture ( Straka , 2018 ) which showed its state of the art performance by becoming first in terms of the Morphology - aware Labeled Attachment Score ( MLAS ) 3 metric at the CoNLL Shared Task of dependency parsing in 2018 ( Zeman et al , 2018 ) . UDPipeFuture is a POS tagger and graph parser based dependency parser using a BiLSTM , inspired by Dozat et al ( 2017 ) . Our modification consisted in adding several contextual word embeddings ( with respect to the language ) . In order to find the best configuration we experimented with models like multilingual BERT ( Devlin et al , 2019 ) , XLM - R ( Conneau et al , 2019 ) ( for both , English and French ) , RoBERTA ( Liu et al , 2019 ) ( for English ) , FlauBERT ( Le et al , 2020 ) and CamemBERT ( Martin et al , 2019 ) ( for French ) during the training of the treebanks French - GSD and English - EWT 4 , of the Universal Dependencies project ( UD ) ( Nivre et al , 2016 ) 5 . Adding contextual word embedding increases significantly the results for all metrics , LAS , CLAS and MLAS ( cf . table 1 ) . This is the case for all languages ( of the CoNLL shared task ) , where language specific contextual embeddings or multingual ones ( as BERT or XLM - R ) improved parsing ( Heinecke , 2020 ) French ( Fr - GSD ) embeddings MLAS CLAS LAS Straka ( 2018 In order to parse simple , quiz - like questions , the training corpora of the two UD treebanks are not appropriate ( enough ) , since both treebanks do not contain many questions , if at all 6 . An explanation for bad performance on questions of parser models trained on standard UD is the fact , that in both languages , the syntax of questions differs from the syntax of declarative sentences : apart from wh question words , in English the to do periphrasis is nearly always used in questions . In French , subject and direct objects can be inversed and the est - ce que construction appears frequently . Both , the English to do periphrasis and the French est - ce que construction are absent in declarative sentences . Table 2 shows the ( much lower ) results when parsing questions using models trained only on the standard UD treebanks . In order to get a better analysis , we decided to and add this data to the basic treebanks . For English we annotated 309 questions ( plus 91 questions for validation ) from the QALD7 ( Usbeck et al , 2017 ) and QALD8 corpora 7 . For French we translated the QALD7 questions into French and formulated others ourselves ( 276 train , 66 validation ) . For the annotations we followed the general UD guidelines 8 as well as the treebank specific guidelines of En - EWT and Fr - GSD . As table 3 shows , the quality of the dependency analysis improves considerably . The contextual word embeddings CamemBERT ( for French ) and BERT ( English ) have the biggest impact . We rely on the UdpipeFuture version which we have improved with BERT ( for English ) /CamemBERT ( for French ) and which gives the best results in terms of dependency analysis , in order to proceed with the partitioning of the question into textual fragments ( also called chunks ) : Q = { c 1 , c 2 , . . . , c n } . If we take the example of the question What is the political party of the mayor of Paris ? , the set of textual fragments would be Q = { What , is , the political party of the mayor of Paris } .
During this phase , we first carry out a first test of the set Q to check whether the text fragment which contains a question marker ( exp : what , when , who etc . ) represents the subject nsubj in the analysed question . If so , we simply replace that text fragment with the answer we identified earlier . Let us take the previous example What is the political party of the mayor of Paris ? , the system automatically detects that the text fragment containing the question marker What represents the subject and will therefore be replaced directly by the exact answer The Socialist Party . Therefore , the concise answer generated will be The Socialist Party is the political party of the mayor of Paris . Otherwise , we remove the text fragment containing the question marker that we detected and we add the short answer R to Q : Q = { c 1 , c 2 , . . . , c n−1 , R } Using the text fragments set Q , we proceed with a permutation based generation of all possible answer structures that can form the sentence answering the question asked : S = { s 1 ( R , c 1 , c 2 , . . . , c n−1 ) , s 2 ( c 1 , R , c 2 , . . . , c n−1 ) , . . . , s m ( c 1 , c 2 , . . . , c n−1 , R ) } These structures will be evaluated by a Language Model ( LM ) based on Transformer models which will extract the most probable sequence of text fragments that can account for the answer to be sent to the user : structure * = s S ; p ( s ) = argmax s i S p ( s i ) Once the best structure is identified , we initiate the generation process of possible missing words . Indeed , we suppose that there could be some terms which do not necessarily appear in the question or in the short answer but which are , on the other hand , necessary to the generation of a correct grammatical structure of the final answer . This process requires that we set two parameters , the number of possible missing words and their positions within the selected structure . In this paper , we experiment the assumption that one word could be missing and that it is located before the short answer within the identified structure , as it could be the case for a missing article ( the , a , etc . ) or a preposition ( in , at , etc . ) for example . Therefore , to predict this missing word , we use BERT as the generation model ( GM ) for its ability to capture bidirectionally the context of a given word within a sentence . In case when BERT returns a non - alphabetic character sequence , we assume that the optimal structure , as predicted by the LM , does not need to be completed by an additional word . The following example illustrates the different steps of the proposed approach : Question : When did princess Diana die ? 1 . Question parsing and answer extraction using the system proposed in Rojas Barahona et al ( 2019 ) : short answer = { August 31 , 1997 }
The existing QAS test sets are more tailored to systems which generate the exact short answer to a question or more focused on the Machine Reading Comprehension task where the answer consists of a text passage from a document containing the short answer . Therefore , we have created a dataset which maps questions extracted from the QALD - 7 challenge dataset ( Usbeck et al , 2017 ) with natural language answers which were defined by a linguist and which we individually reviewed . This dataset called QUEREO consists of 150 questions with the short answers extracted by the QAS that we described above . We denote an average of three possible gold sanswers in natural language for each question . French and English versions were created for this dataset . As illustrated in figure 1 , two possible architectures of the approach proposed for answer generation have been evaluated . The first architecture A1 consists in generating all possible answer structures in order to have them evaluated afterwards by a LM which will identify the optimal answer structure to which we generate possible missing words . Architecture A2 starts with generating missing words for each structure in S which will then be evaluated by the LM . In this paper , we assume that there is only one missing word per structure . To evaluate the proposed approach , we have referred to standard metrics defined for NLG tasks such as Automatic Translation and Summarization , as they allow to assess to what extent a generated sentence is similar to the gold sentence . We con - sider three N - gram metrics ( BLEU , METEOR and ROUGE ) and the BERT score metric which exploits the pre - trained embeddings of BERT to calculate the similarity between the answer generated and the gold answer . To be able to compare the different configurations of the approach , we refer to Friedman 's test ( Milton , 1939 ) We also conducted a human evaluation study for the French and the English versions of the dataset , in which we asked 20 native speakers participants to evaluate the relevance of a generated answer ( correct or not correct ) regarding a given question while indicating the type of errors depicted ( grammar , wrong preposition , word order , extra word ( s ) , etc ) . Figure 3 presents the evaluation framework that we have implemented and provided to the participants . The results of each participant are saved in a json - file ( figure 4 ) . The inter - agreement rate between participants reached 70 % which indicates a substantial agreement . Through the human evaluation study , we wanted to explore to what extent the standard metrics are reliable to assess NLG approaches within the context of question - answering systems . Table 4 ( French dataset ) represents the obtained results for the first three best models according to the human evaluation ranking and the Friedman test ranking . We indicate between brackets each model 's rank according to the metric used . We note that the highest human accuracy score for French of about 85 % was scored with the first architecture coupled with BERT as the generation model ( GM ) and CamemBERT as the language model ( LM ) . We also notice that the architecture A1 , which considers the LM assessment of the structure before generating missing words , performs better . Surprisingly , as a generative model , the multi - Table 4 : Model ranking for French dataset according to the human evaluation study ( best in bold ) and the Friedman test ( best in yellow ) . " BT " in Column GM stands for BERT - base - multilingual - cased . In column LM we use " CmBT " for CamemBERT - base , " BT - ml - c " for BERT - base - multilingual - cased , " XRob " for XLM - RoBERTa - base , " FBT - s - c " for FlauBERT - small - cased , " FBT - b - uc " for FlauBERT - base - uncased and " clm - 1024 " for XLM - clmenfr - 1024 lingual BERT model predicts missing words better than CamemBERT for French sentences . These findings are also confirmed by the Friedman test where we can clearly see that the first ranked configuration maps the best configuration selected according to the human accuracy , with a very slight difference for the other four configurations . Let us see if that means that the four metrics are correlated with the human accuracy . According to table 6 which presents the Pearson correlation ( Benesty et al , 2009 ) of the human accuracy with the four metrics and to figure 2 which illustrates the ranking given by each evaluation metric along with the human judgement for each configuration ( i.e. configuration = GM × architecture × LM ) tested , we can clearly see that the human evaluation results are positively and strongly correlated with the BLEU , the METEOR and the BERT scores . These metrics are practically matching the human ranking and thus are obviously able to identify which configuration gives better results . The rouge metric , used for French question / answer evaluation , is moderately correlated with the human evaluation which means that we should not only rely on this metric when assessing such task . On the other hand , when the ROUGE metric is considered with the other metrics , it helps to get closer to the human judgement . Table 5 presents the results for the English dataset and shows that the best accuracy scored is about 72 % with A1 , BERT as the generative model and the Generative Pretrained Transformer ( GPT ) as the language model . According to the first three configurations , architecture A2 prevails and the GPT transformer takes over the other lan - guage models . These results are also confirmed by the Friedman test with a very slight difference on the ranking and also upheld with the correlation scores between the human assessment and each of the four metrics as shown by figure 5 and table 6 . These findings mean that we actually can rely on the use of these standard metrics to evaluate the answer generation task for question - answering . We also tried to analyse the errors indicated by the participants . As we can note from figure 6 , the most common error reported for both English and French datasets is the word order which sheds the light on a problem related to the language model assessment phase . The second most reported error addresses the generation process , whether to indicate that there are one or more missing words within the answer ( French ) or the presence of some odd words ( English ) . When trying to get an insight on the answers generated by the current intelligent systems such as Google assistant and Alexa , we noted that these systems are very accurate when extracting the correct answer to a question and can sometimes generate user - friendly answers that help recall the question context , specially with Alexa . However , we noticed that most of the answers generated by these systems are more verbose than necessary , we also found out that when addressing yes / no questions , these systems generally settle for just a yes or no without elaborating , or , on the other hand , present a text span extracted from a Web page and let the user guess the answer . Let us take for example the following question Was US president Jackson involved in a war ? Table 5 : Model ranking for English dataset according to the human evaluation study ( best in bold ) and the Friedman ranking ( best in yellow ) . In Column GM we use " BT - ml " for BERT - base - multilingual - cased and " BT " for BERTlarge - cased . In column LM " GPT " stands for for OpenAI - GPT , " GPT2 - l " for GPT2 - large , " GPT2 - m " for GPT2medium , " GPT2 " for GPT2 , " BT - b - uc " for BERT - base - uncased , " mlm - 2048 " for XLM - mlm - en - 2048 and " BT - l - c " for BERT - large - cased . [ { " ID " : " quereo_5.4 " , " QUESTION " : " Quelles sont les companies d'électronique fondées a Beijing ? " , " SHORT_ANSWER " : [ " Xiaomi " , " Lenovo " ] , " GENERATED_ANSWER " : " Les companies d'électronique fondéesà beijing sont xiao xiaomi et lenovo " , " MISSING_WORD " : " Xiao " , " EVALUATION " : " correcte " , " ERROR " : [ " aucun " ] , " COMMENT " : " " } , { " ID " : " quereo_8.8 " , " QUESTION " : " Combien de films a réalisé Park Chan - wook ? " , " SHORT_ANSWER " : [ " quatorze " ] , " GENERATED_ANSWER " : " Quatorze films a réalisé park chan - wook " , " MISSING_WORD " : " . " , " EVALUATION " : " incorrecte " , " ERROR " : [ " ordre " , " accord " ] , " COMMENT " : " " } , ... ] Here 's something I found on the Web . According to constitutioncenter.org : After the War of 1812 , Jackson led military forces against the Indians and was involved in treaties that led to the relocation of Indians . The user has to focus on the returned text fragment in order to guess that the answer to his question is actually yes . This issue was particularly noted when addressing French questions . If we also take the example How many grandchildren did Jacques Cousteau have ? the two systems answer as follows : Fabien Cousteau , Alexandra Cousteau , Philippe Cousteau Jr. , Céline Cousteau . Jacques Cousteau 's grandchildren were Philippe Cousteau Jr. , Alexandra ousteau , Céline Cousteau , and Fabien Cousteau However , the user is not asking about the names of Cousteau 's grand - children and has to guess by himself that the answer for this question is four . A more accurate answer should indicate the exact answer to the question and then elaborate Jacques Cousteau had four grand - children . But these systems perform better in case when the terms employed in the question are not necessarily relevant to the answer . If we take the example of the question who is the wife of Lance Bass , the approach that we propose will generate The wife of Lance Bass is Michael Turchin . As we can note the answer generated was not adapted to the actual answer , while the other systems are able to detect such nuance : Lance Bass is married to Michael Turchin . They have been married since 2014 . This issue has still to be addressed .
We have put forward , in this paper , an approach for Natural Language Generation within the framework of the question - answering task that considers dependency analysis and probability distribution of words sequences . This approach takes part of a question / answering system in order to help generate a user - friendly answer rather than a short one . The results obtained through a human evaluation and standard metrics tested over French and English questions are very promising and shows a good correlation with human judgement . However , we intend to put more emphasis on the Language Model choice as reported by the human study and consider the generation of more than one missing word within the answer .
