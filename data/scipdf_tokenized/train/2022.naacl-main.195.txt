Do n't sweat the small stuff , classify the rest : Sample Shielding to protect text classifiers against adversarial attacks
Deep learning ( DL ) is being used extensively for text classification . However , researchers have demonstrated the vulnerability of such classifiers to adversarial attacks . Attackers modify the text in a way which misleads the classifier while keeping the original meaning close to intact . State - of - the - art ( SOTA ) attack algorithms follow the general principle of making minimal changes to the text so as to not jeopardize semantics . Taking advantage of this we propose a novel and intuitive defense strategy called Sample Shielding . It is attacker and classifier agnostic , does not require any reconfiguration of the classifier or external resources and is simple to implement . Essentially , we sample subsets of the input text , classify them and summarize these into a final decision . We shield three popular DL text classifiers with Sample Shielding , test their resilience against four SOTA attackers across three datasets in a realistic threat setting . Even when given the advantage of knowing about our shielding strategy the adversary 's attack success rate is < = 10 % with only one exception and often < 5 % . Additionally , Sample Shielding maintains near original accuracy when applied to original texts . Crucially , we show that the ' make minimal changes ' approach of SOTA attackers leads to critical vulnerabilities that can be defended against with an intuitive sampling strategy . 1
Text classifiers have become ubiquitous . Unfortunately , they are subject to attacks from adversaries , typically executed using machine learning methods . Attackers work by making small modifications to the text that mislead the classifier . Adversarial attackers are now a growing part of the ecosystem . Like classifiers , attack algorithms have achieved strong success due to advances in machine learning / deep learning . Current text attackers , like 1 Our code and data are available at : https://github.com/JonRusert/SampleShielding TextFooler and Bert - Attack ( Li et al , 2020 ) , are able to reduce near perfect classification accuracy down to 5 % . Additionally , these attackers achieve this while perturbing ( changing ) only a small amount of the original text . This helps preserve the original meaning so that humans are able to understand the original message even though classifiers are duped . As a counter , classifier shielding techniques are being explored . One such approach is adversarial training where the classifier , assumed to have access to the attacker , uses it to generate perturbed texts - these are added to the classifier 's training data . While this leads to model resilience against that attacker it leaves the classifier open to attacks by new attackers . Other defenses involve modifying classifier structure to reduce the information an attacker can glean from it ( Goel et al , 2020 ) . However , this type of reconfiguration will not be possible if a third party classifier ( e.g. Google Perspective ) is leveraged . Even other approaches involve modifying the input text during classification time , but are currently limited to classifiers built from specific masked language models ( Zeng et al , 2021 ) or rely on external synonym datasets ( Wang et al , 2021a ) . We propose a shielding technique which is attacker - agnostic , does not require additional training / reconfiguration to the classifier , can shield any classifier , does not require an external data source , and can be used in a more realistic threat setting . We refer to this as Sample Shielding . Sample Shielding takes advantage of current constraints in SOTA attacks . Mainly , to preserve original meaning , these make the minimal changes needed to deceive the classifier . For example , BERT - Attack ( Li et al , 2020 ) only perturbs up to 16 % of text , and often far less ( e.g. 1.1 % ) for some datasets . Thus , if we would look at the 84 % to 99 % of text that is untouched our model would be more likely to classify correctly . Hence , in Sample Shielding we take many samples of the input 2 . We assess Sample Shielding under a realistic threat model where the attacker can not query a website 's classifier hundreds of times since that pattern is easily detectable by the website . We run experiments under two conditions , when the attacker has knowledge of Sample Shielding and when it does not . In both cases the attacker uses a local copy of the websites ' classifier . This is an optimistic assumption favouring the attacker and thus provides a lower bound to our results . 3 . We test against 4 SOTA text attack algorithms , 3 text datasets and 3 classifiers . When the attacker does not have knowledge of Sample Shielding , our defense reduces attack success rate from near total decimation 90 - 100 % down to 13 - 36 % , while still maintaining accuracy on original texts . When the attacker has knowledge of Sample Shielding , our defense performs even better , reducing attacks down to 1 - 10 % success rate . This is partially due to Sample Shielding 's random nature providing unreliable feedback to attackers . Our success with Sample Shielding is good news for classifiers - and it raises the bar significantly for the next generation attackers . We share code and our perturbed text collections for future research .
The typical attack strategy perturbing texts with word synonyms or character substitutions assumes to have query access to the target web site 's classifier ( W ) ( Yoo and Qi , 2021 ; Li et al , 2021a ; Ren et al , 2019 ; Li et al , 2020 ; Garg and Ramakrishnan , 2020 ; Jia et al , 2019 ; Li et al , 2019 ) . The text is modified by querying W hundreds or thousands of times , each time with a text version differing only slightly from the previouseven by just a single word ( Li et al , 2020 ; . Such a querying pattern can be easily identified as adversarial by the website and countered . Thus , practically the only way in which such an attack can take place is when the attacker owns a local classifier W ′ which is either an exact copy of W or a close enough approximation . We adopt this more realistic threat model , shown in Figure 1 . In our threat model the attacker uses feedback from its local W ′ to generate a final perturbed version that defeats W ′ or is close enough to do so . The attacker submits only this final version to the website , expecting W to make the same error . However , the website defends W using Sample Shielding : sample based pre - processing on the input text , prior to applying W . The attacker may or may not be aware of this fact . Keeping W = W ′ which is consistent with other defenses , we evaluate our defense under two conditions : 1 ) The attacker does not know that the website employs Sample Shielding pre - processing when classifying text using W . 2 ) The Sample Shielding step is leaked and the attacker incorporates it locally when using W ′ to generate the final perturbed text . We present results from experiments exploring both of these attack conditions .
Intuition . Current adversarial attackers have two goals : fool the classifier and maintain the original meaning . Since they make minimal changes , the extent of perturbation is in fact one of the reported statistics . For example , ( Li et al , 2020 ) note that their 10 % perturbation rate is far less than in previous attacks . ( Li et al , 2019 ) also focus on minimal changes ( 4 % ) needed in support of their attack success rate . Our defense approach capitalizes on this drive to make minimal changes . Specifically , in Sample Shielding , we take k samples each composed of p% of the text . We choose a p which minimizes the chance of a sample including attacked ( modified ) words , while maximizing the content available for the classifier to make a correct classification . We choose a k which is large enough to cover key information but small enough to reduce redundancy . We classify each sample and combine > 0.5 k sample predictions … I enjoyed this movie more than I thought I would . From multiple viewings it becomes especially clear how much time and energy the director put into this film . The choice for lead actor had me worried but it worked well . The twist was what really had me hooked . ... their decisions for the final classification . We explore two sampling and three decision combining methods .
Random Sampling . We randomly sample p portions of the text . We explore both sentences and words as sampled units . A visualization of random sampling is in Figure 2 . Shifting Sampling . We sample the text using a moving window of length p × length_of _ text . The first starts at the beginning of the text . The next window starts right after the previous window ends . If there is insufficient text for the last window , then it wraps back to include the beginning text .
Majority voting . This is a simple majority vote across the k samples ( Figure 2 ) . Classifier trained on sample scores from original texts ( NN ) . We train a neural network summarizer to make a final class prediction based on the k sample probabilities . Since sample ID does not carry any information , the input to the neural network is a sorted list of sample probabilities . The intent is to see if the neural network picks up on latent patterns in the probabilities that are not captured by majority voting ( see Figure 2 ) . It should be emphasized that the neural network summarizer is trained only on probabilities generated from original texts and does not consider probabilities from attacker modified texts . We use a simple feed forward neural net composed of 2 linear layers ( size 500 and 300 ) as classification summarizer . Classifier trained on sample scores from original and attacked texts ( NN - BB ) . This is similar to the previous strategy except that the training data includes scores from original texts and texts that have been modified by the attacker . Because this assumes more knowledge of the attacker we expect NN - BB to perform better than NN . The ground truth label for these modified texts is the original correct class label . 3 Experimental Setup
We examine three standard datasets in our experiments . Two have binary class labels ( Yelp , IMDB ) and the third has multi class labels ( AG News ) . These have been used in adversarial generation and defense research ( Zeng et al , 2021 ; Li et al , 2020 ) . All datasets can be found via huggingface 2 . 1 . IMDB - Movie review dataset for binary sentiment classification . 25k examples are provided for training and testing respectively . 2 . Yelp - Yelp dataset for binary sentiment classification on reviews of businesses extracted from the Yelp Dataset Challenge 3 . 560k examples are provided for training and 38k for testing . 3 . AG News - News articles from over 2000 news sources annotated by type of news : Sports , World , Business , and Science / Tech . 120k training and 7k test sets are provided . Following previous research , ( Li et al , 2020 ; we use all training data , and evaluate our method on random 1k samples of each dataset for the case where the local classifier does not employ Sample Shielding . Due to the high amount of queries used by the adversaries , we test on a subset of 100 samples for the case where the attacker 's local classifier employs Sample Shielding . 4
We test our text classifier shielding strategy against 4 state - of - the - art ( SOTA ) text classifier attack algorithms . These algorithms have shown excellent performance in causing misclassifications while still producing readable texts . We defend against 3 word based attacks : TextFooler , Bert - Attack ( Li et al , 2020 ) , PWWS ( Ren et al , 2019 ) . TextFooler leverages word embeddings for word replacements , Bert - Attack leverages BERT itself by masking words and using BERT suggestions , PWWS selects and weights word replacements from WordNet . All three use some form of greedy selection for determining which words to replace . We also defend against a character based attack algorithm , TextBugger ( Li et al , 2019 ) .
We test our shielding approach against 3 standard classifiers 5 used in previous research , e.g. ( Li et al , 2021a ; Li et al , 2020 ) : 1 . CNN - A word based CNN ( Kim , 2014 ) , with three window sizes ( 3 , 4 , 5 ) , 100 filters per window with dropout of 0.3 and Glove embeddings . 2 . LSTM - A word based bidirectional LSTM with 150 hidden units . As with the CNN a dropout of 0.3 is used and Glove embeddings are leveraged . 3 . BERT - The 12 layer BERT base model which has been fine - tuned on the corresponding dataset . These are provided by textattack via huggingface 6 .
We run experiments on the combination of the three victim classification models , three datasets , and four attack algorithms . These combinations are run on both threat model conditions ( attacker is aware/ not aware of SampleShielding ) . This leads to 72 shielding experiments . For all attacks , we leverage TextAttack framework 7 which provides classification algorithms and adversarial text generation algorithms implemented as specified in respective papers ( Morris et al , 2020 ) . In all experiments where the attacker does not use Sample Shielding 4 We share the original and perturbed texts for replicability . We note that replicability of previous defenses are limited because the identity of their randomly sampled test instances are not provided . 5 We calibrated classifier accuracies against previous research ( Li et al , 2020 ; 6 huggingface.co/textattack 7 textattack.readthedocs.io/en/latest/index.html we set k = 100 and p = 0.3 . While better performance was achieved with other values in preliminary experiments , we chose to go with a single combination of p and k for simplicity . In experiments where the attacker uses Sample Shielding pre - processing we reduce k to 30 for efficiency . Except where otherwise noted , majority voting is used to generate results . Additionally , shifting sampling ( Section 2.2.1 ) shielding typically achieved 10 - 20 points lower accuracy compared to random , thus we do not include it in the results .
We examine accuracy and Attack Success Rate : accuracy = # examples_classif ied_correctly # total_examples ( 1 ) ASR = Original Acc . − Attacked Acc . Original Acc . ( 2 ) 4 Results We first present results for the condition where the attacker is not aware of Sample Shielding based pre - processing and then the results for when the attacker also employs Sample Shielding .
Results are in Table 1 . BERT is the strongest classifier achieving 91 - 100 % accuracy on the original datasets . Attacks are highly successful against unshielded texts . TextFooler and Bert - Attack are the most successful , dropping accuracies to 0 - 5 % generally . Attacks were able to achieve strong drops with minimal amount of text perturbed ( about 10 % ) . Figure 3 shows that the average percent of words perturbed across datasets for each attack are about equal in the mid regions of the plots . For AG News , attacks are less successful against BERT ; accuracy drops to 19 % in the strongest attack ( TextFooler ) , and only to 49 % in the weakest ( TextBugger
Results are in Table 3 . As in the previous condition , classifiers perform well on original texts ( Table 1 ) with BERT often achieving the highest accuracies . In this setting , every query by an attacker requires k samples to be processed , which greatly increases attack time . Thus , we reduce k to 30 for these experiments . Sample Shielding repels attacks even when attacker uses Sample Shielding . We see that shielding is extremely successful in almost completely removing the negative effects of the attacks . For example , on the IMDB - TextFooler combination , attack success rate drops from 100 to 5 for LSTM , 100 to 1 for CNN , and 99 to 6 against BERT . The largest protection provided by Sample Shielding ( 100 % ) is for TextBugger vs CNN in IMDB . The smallest is for 85 % ( PWWS vs LSTM ) . On average the protection is 88.8 % . The recovered accuracies are only 13 to 0 percent away from the originals . These results show the power of Sample Shielding as even with knowledge of both the classifier and Sample Shielding , attacks struggle to perturb the text in a manner that causes W to fail . Furthermore , the attacks do worse with feedback from Sample Shielding . This shows the misleading nature of feedback from Sample Shielding , and unreliability when guiding attacks . 5 Additional Analysis
Increasing p raises the risk of samples containing increased amounts of perturbed text . Decreasing k raises the risk of not covering enough of the unperturbed portions of the original text . While our settings of p = 0.3 and k = 100 for our main results are reasonable values ( Table 1 , Table 2 ) they are not necessarily optimal . Optimal p. Figure 4 shows the results for all com - binations of attacks against LSTM on IMDB with word shielding as the defense , k fixed at 100 . As we increase p , we see a continued drop in accuracy which is consistent with the idea that a higher p is more likely to capture perturbed text . The optimal value range appears to be in 0.2 - 0.4 range , although we do not see large drops until 0.6 onward . We also examined the same combination on AG News ( Figure 5 ) since it 's texts are considerably shorter and found consistent results . Optimal k. Figure 6 shows results for all attacks against LSTM on IMDB with word sampling as the defense , p fixed at 0.3 . The optimal k is not as clear as p. We see clear increases after 30 samples , but then the optimal k varies depending on attack . However , we see a leveling off around 90 samples , which gives some credence to our chosen k of 100 . We also found similar results when examining the same combination on AG News ( Figure 7 ) , however , k stabilized lower ( about 50 ) .
Due to the randomness of samples , there may be concern over the consistency of Sample Shielding . To address this , we ran Sample Shielding 100 times on the IMDB attacked texts from Table 3 against BERT classifier . Each time 30 random samples were used to vote . As can be observed from Figure 8 , Sample Shielding consistently protects against attacks . Median accuracies are above 80 % dropping only to 75 % in the worst case . This points to Sample Shielding as a consistent , reliable defense .
Comparisons are limited as threat models differ . As noted earlier , other defenses assume a weaker threat model where the attacker queries the web - site 's shielded W directly . To make ours equivalent we compare SOTA results with our accuracies obtained by the attacker using W ′ alone ( with W = W ′ ) . We calculate accuracies right after the final perturbed text is generated using W ′ eliminating a followup round of W with Sample Shielding . 3 ) . While we do not know how FreeLB++ , RanMask , and similar defenses would perform with our threat model any deterministic shield would give the exact same results when the classifier is applied once again by the website .
First , in future work we will add in direct comparisons to the two closest methods to Sample Shielding ( Zeng et al , 2021 ; Wang et al , 2021a ) . They are similar in spirit as they also work off samples though these are generated differently . We have not compared with them because these two papers appeared very recently , one last revised in July ( Zeng et al , 2021 ) and the other appeared in arXiv in September 2021 ( Wang et al , 2021a ) . Second , the neural net summarizer leverages a simple linear layer . Other networks , e.g. , LSTM , maybe better at finding patterns in sequential data . In future work we will also explore layering Sample Shielding onto other defense strategies . Another limitation of our current method is that we do not measure Sample Shielding 's effectiveness on other common text tasks including Natural Language Understanding . Additionally , datasets which contain the shortest texts ( e.g. not currently tested in our experiments . Since sample shielding removes texts , it 's performance could drop for these tasks and short texts . Thus , future work will include these comparisons .
Defenses using voting . The most similar methods to our own are RanMask and RS&V both appearing within the last five months . RanMask ( Zeng et al , 2021 ) randomly masks tokens in input texts . This random masking occurs n times generating n inputs to be fed to a classifier . RS&V ( Wang et al , 2021a ) randomly replaces words in the input with synonyms . This it does k times to produce k samples which are then voted on . If the samples vote for a different label than the label produced by the unsampled input , then the text is labeled as an adversarial text . Our method is advantageous since it does not rely on specific models ( i.e. Masked Language Model ) or synonym sources . Adversarial training . Classifiers train on perturbed data , learning to identify modified versions of the original input ( Wang and Wang , 2020 ; Wang et al , 2021b ; Zhu et al , 2020 ; Li et al , 2021b ) . As an example , Gil et al ( 2019 ) propose HotFlip which uses white - box knowledge to generate adversarial attacks to train on . Specifically , they flip tokens based on the gradients of the one - hot input vectors . However , adversarial defenses are limited to known attackers . In contrast , Sample Shielding is ' plug - and - play ' as it is a pre - processing step . Other defenses . Several other shielding methods exist ( Keller et al , 2021 ; Eger et al , 2019 ; Zhu et al , 2021 ) . For example , Rodriguez and Galeano ( 2018 ) defend Perspective ( Google 's toxicity classification model ) by neutralizing adversarial inputs via a negated predicates list . Again , these defenses are restricted to contexts where specific lists may be identified , this is not so with Sample Shielding .
Sample Shielding , an intuitively designed defense which is attacker and classifier agnostic , protects effectively ; reducing ASR from 90 - 100 % down to 14 - 34 % with minimal accuracy loss ( 3 % ) in original texts . The randomness ( through sampling ) provides unreliable feedback for attackers , thus it even thwarts attackers who have query access to classifiers protected with Sample Shielding . Attack strategies will need to increase the amount of perturbation to make sure a majority of samples fail at classification . However , this will risk semantic integrity . Thus , we expect Sample Shielding to cause ripples in future adversarial attack strategies while providing text classifiers with a definite advantage .
