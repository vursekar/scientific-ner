Improving Joint Training of Inference Networks and Structured Prediction Energy Networks
Deep energy - based models are powerful , but pose challenges for learning and inference ( Belanger and McCallum , 2016 ) . Tu and Gimpel ( 2018 ) developed an efficient framework for energy - based models by training " inference networks " to approximate structured inference instead of using gradient descent . However , their alternating optimization approach suffers from instabilities during training , requiring additional loss terms and careful hyperparameter tuning . In this paper , we contribute several strategies to stabilize and improve this joint training of energy functions and inference networks for structured prediction . We design a compound objective to jointly train both costaugmented and test - time inference networks along with the energy function . We propose joint parameterizations for the inference networks that encourage them to capture complementary functionality during learning . We empirically validate our strategies on two sequence labeling tasks , showing easier paths to strong performance than prior work , as well as further improvements with global energy terms .
Energy - based modeling ( LeCun et al , 2006 ) associates a scalar compatibility measure to each configuration of input and output variables . Belanger and McCallum ( 2016 ) formulated deep energy - based models for structured prediction , which they called structured prediction energy networks ( SPENs ) . SPENs use arbitrary neural networks to define the scoring function over input / output pairs . However , this flexibility leads to challenges for learning and inference . The original work on SPENs used gradient descent for structured inference ( Belanger and McCallum , 2016 ; Belanger et al , 2017 ) . Gimpel ( 2018 , 2019 ) found improvements in both speed and accuracy by replacing the use of gradient descent with a method that trains a neural network ( called an " inference network " ) to do inference directly . Their formulation , which jointly trains the inference network and energy function , is similar to training in generative adversarial networks ( Goodfellow et al , 2014 ) , which is known to suffer from practical difficulties in training due to the use of alternating optimization ( Salimans et al , 2016 ) . To stabilize training , Tu and Gimpel ( 2018 ) experimented with several additional terms in the training objectives , finding performance to be dependent on their inclusion . Moreover , when using the approach of Tu and Gimpel ( 2018 ) , there is a mismatch between the training and test - time uses of the trained inference network . During training with hinge loss , the inference network is actually trained to do " costaugmented " inference . However , at test time , the goal is to simply minimize the energy without any cost term . Tu and Gimpel ( 2018 ) fine - tuned the cost - augmented network to match the test - time criterion , but found only minimal change from this fine - tuning . This suggests that the cost - augmented network was mostly acting as a test - time inference network by convergence , which may be hindering the potential contributions of cost - augmented inference in max - margin structured learning ( Tsochantaridis et al , 2004 ; Taskar et al , 2004 ) . In this paper , we contribute a new training objective for SPENs that addresses the above concern and also contribute several techniques for stabilizing and improving learning . We empirically validate our strategies on two sequence labeling tasks from natural language processing ( NLP ) , namely part - of - speech tagging and named entity recognition . We show easier paths to strong performance than prior work , as well as further improvements with global energy terms . We summarize our list of contributions as follows . We design a compound objective under the SPEN framework to jointly train the " trainingtime " cost - augmented inference network and test - time inference network ( Section 3 ) . We propose shared parameterizations for the two inference networks so as to encourage them to capture complementary functionality while reducing the total number of trained parameters ( Section 3.1 ) . Quantitative and qualitative analysis shows clear differences in the characteristics of the two networks ( Table 3 ) . We present three methods to streamline and stabilize training that help with both the old and new objectives ( Section 4 ) . We propose global energy terms to capture long - distance dependencies and obtain further improvements ( Section 5 ) . While SPENs have been used for multiple NLP tasks , including multi - label classification ( Belanger and McCallum , 2016 ) , part - of - speech tagging ( Tu and Gimpel , 2018 ) , and semantic role labeling ( Belanger et al , 2017 ) , they are not widely used in NLP . Structured prediction is extremely common in NLP , but is typically approached using methods that are more limited than SPENs ( such as conditional random fields ) or models that suffer from a train / test mismatch ( such as most auto - regressive models ) . SPENs offer a maximally expressive framework for structured prediction while avoiding the train / test mismatch and therefore offer great potential for NLP . However , the training and inference have deterred NLP researchers . While we have found benefit from training inference networks for machine translation in recent work ( Tu et al , 2020b ) , that work assumed a fixed , pretrained energy function . Our hope is that the methods in this paper will enable SPENs to be applied to a larger set of applications , including generation tasks in the future .
We denote the input space by X . For an input x X , we denote the structured output space by Y ( x ) . The entire space of structured outputs is denoted Y = ∪ x X Y ( x ) . A SPEN ( Belanger and McCallum , 2016 ) defines an energy function E Θ : X × Y R parameterized by Θ that computes a scalar energy for an input / output pair . At test time , for a given input x , prediction is done by choosing the output with lowest energy : y = arg min y Y ( x ) E Θ ( x , y ) ( 1 ) However , solving equation ( 1 ) requires combinatorial algorithms because Y is a structured , discrete space . This becomes intractable when E Θ does not decompose into a sum over small " parts " of y. Belanger and McCallum ( 2016 ) relaxed this problem by allowing the discrete vector y to be continuous ; Y R denotes the relaxed output space . They solved the relaxed problem by using gradient descent to iteratively minimize the energy with respect to y. The energy function parameters Θ are trained using a structured hinge loss which requires repeated cost - augmented inference during training . Using gradient descent for the repeated costaugmented inference steps is time - consuming and makes learning unstable ( Belanger et al , 2017 ) . Tu and Gimpel ( 2018 ) replaced gradient descent with a neural network trained to do efficient inference . This " inference network " A Ψ : X Y R is parameterized by Ψ and trained with the goal that A Ψ ( x ) ≈ arg min y Y R ( x ) E Θ ( x , y ) ( 2 ) When training the energy function parameters Θ , Tu and Gimpel ( 2018 ) replaced the cost - augmented inference step in the structured hinge loss from Belanger and McCallum ( 2016 ) with a costaugmented inference network F Φ : F Φ ( x ) ≈ arg min y Y R ( x ) ( E Θ ( x , y ) − ( y , y * ) ) ( 3 ) where is a structured cost function that computes the distance between its two arguments . We use L1 distance for . This inference problem involves finding an output with low energy but high cost relative to the gold standard . Thus , it is not wellaligned with the test - time inference problem . Here is the specific objective to jointly train Θ ( parameters of the energy function ) and Φ ( parameters of the cost - augmented inference network ) : min Θ max Φ x i , y i D [ ( F Φ ( x i ) , y i ) − E Θ ( x i , F Φ ( x i ) ) + E Θ ( x i , y i ) ] + ( 4 ) where D is the set of training pairs , [ h ] + = max ( 0 , h ) , and is a structured cost function that computes the distance between its two arguments . Tu and Gimpel ( 2018 ) alternatively optimized Θ and Φ , which is similar to training in generative adversarial networks ( Goodfellow et al , 2014 ) . The inference network is analogous to the generator and the energy function is analogous to the discriminator . As alternating optimization can be difficult in practice ( Salimans et al , 2016 ) , Tu & Gimpel experimented with including several additional terms in the above objective to stabilize training . After the training of the energy function , an inference network A Ψ for test - time prediction is finetuned with the goal shown in Eq . ( 2 ) . More specifically , for the fine - tuning step , we first initialize Ψ with Φ ; next , we do gradient descent according to the following objective to learn Ψ : Ψ arg min Ψ x X E Θ ( x , A Ψ ( x ) ) where X is a set of training or validation inputs . It could also be the test inputs in a transductive setting . 3 An Objective for Joint Learning of Inference Networks One challenge with the above optimization problem is that it requires training a separate inference network A Ψ for test - time prediction after the energy function is trained . In this paper , we propose an alternative that trains the energy function and both inference networks jointly . In particular , we use a " compound " objective that combines two widely - used losses in structured prediction . We first present it without inference networks : min Θ x i , y i D max y ( ( y , y i ) −E Θ ( x i , y ) + E Θ ( x i , y i ) ) + margin - rescaled hinge loss + λ max y ( −E Θ ( x i , y ) + E Θ ( x i , y i ) ) + perceptron loss ( 5 ) As indicated , this loss can be viewed as the sum of the margin - rescaled hinge and perceptron losses for SPENs . Two different inference problems are represented . The margin - rescaled hinge loss contains cost - augmented inference , shown as part of Eq . ( 3 ) . The perceptron loss contains the test - time inference problem , which is shown in Eq . ( 1 ) . Tu and Gimpel ( 2018 ) used a single inference network for solving both problems , so it was trained as a cost - augmented inference network during training and then fine - tuned as a test - time inference network afterward . We avoid this issue by training two inference networks , A Ψ for test - time inference and F Φ for cost - augmented inference : min Θ max Φ , Ψ x i , y i D [ ( F Φ ( x i ) , y i ) −E Θ ( x i , F Φ ( x i ) ) + E Θ ( x i , y i ) ] + + λ [ −E Θ ( x i , A Ψ ( x i ) ) + E Θ ( x i , y i ) ] + ( 6 ) We treat this optimization problem as a minimax game and find a saddle point for the game similar to Tu and Gimpel ( 2018 ) and Goodfellow et al ( 2014 ) . We use minibatch stochastic gradient descent and alternately optimize Θ , Φ , and Ψ. The objective for the energy parameters Θ in minibatch M is : Θ arg min Θ x i , y i M ( F Φ ( x i ) , y i ) −E Θ ( x i , F Φ ( x i ) ) + E Θ ( x i , y i ) + + λ −E Θ ( x i , A Ψ ( x i ) ) + E Θ ( x i , y i ) + When we remove 0 - truncation ( see Sec . 4.1 for the motivation ) , the objective for the inference network parameters in minibatch M is : Ψ , Φ arg max Ψ , Φ x i , y i M ( F Φ ( x i ) , y i ) − E Θ ( x i , F Φ ( x i ) ) − λE Θ ( x i , A Ψ ( x i ) )
If we were to train independent inference networks A Ψ and F Φ , this new objective could be much slower than the approach of Tu and Gimpel ( 2018 ) . However , the compound objective offers several natural options for defining joint parameterizations of the two inference networks . We consider three options which are visualized in Figure 1 and described below : separated : F Φ and A Ψ are two independent networks with their own architectures and parameters as shown in Figure 1 ( a ) . shared : F Φ and A Ψ share a " feature " network as shown in Figure 1 ( b ) . We consider this option because both F Φ and A Ψ are trained to produce output labels with low energy . However F Φ also needs to produce output labels with high cost ( i.e. , far from the gold standard ) . stacked : the cost - augmented network F Φ is a function of the output of the test - time network A Ψ and the gold standard output y. That is , F Φ ( x , y ) = q ( A Ψ ( x ) , y ) where q is a parameterized function . This is depicted in Figure 1 ( c ) . We block the gradient at A Ψ when updating Φ. For the q function in the stacked option , we use an affine transformation on the concatenation of the inference network label distribution and the gold standard one - hot vector . That is , denoting the vector at position t of the cost - augmented network output by F Φ ( x , y ) t , we have : F Φ ( x , y ) t = softmax ( W q [ A Ψ ( x ) t ; y ( t ) ] + b q ) where semicolon ( ; ) is vertical concatenation , y ( t ) ( position t of y ) is an L - dimensional one - hot vector , A Ψ ( x ) t is the vector at position t of A Ψ ( x ) , W q is an L × 2L matrix , and b q is a bias . One motivation for these parameterizations is to reduce the total number of parameters in the procedure . Generally , the number of parameters is expected to decrease when moving from separated to shared to stacked . We will compare the three options empirically in our experiments , in terms of both accuracy and number of parameters . Another motivation , specifically for the third option , is to distinguish the two inference networks in terms of their learned functionality . With all three parameterizations , the cost - augmented network will be trained to produce an output that differs from the gold standard , due to the presence of the ( ) term in the combined objective . However , Tu and Gimpel ( 2018 ) found that the trained cost - augmented network was barely affected by fine - tuning for the test - time inference objective . This suggests that the cost - augmented network was mostly acting as a test - time inference network by the time of convergence . With the stacked parameterization , however , we explicitly provide the gold standard y to the cost - augmented network , permitting it to learn to change the predictions of the test - time network in appropriate ways to improve the energy function .
We now discuss several methods that simplify and stabilize training SPENs with inference networks . When describing them , we will illustrate their impact by showing training trajectories for the Twitter part - of - speech tagging task .
Tu and Gimpel ( 2018 ) used the following objective for the cost - augmented inference network ( maximizing it with respect to Φ ) : l 0 = [ ( F Φ ( x ) , y ) − E Θ ( x , F Φ ( x ) ) + E Θ ( x , y ) ] + where [ h ] + = max ( 0 , h ) . However , there are two potential reasons why l 0 will equal zero and trigger no gradient update . First , E Θ ( the energy function , corresponding to the discriminator in a GAN ) may already be well - trained , and it can easily separate the gold standard output from the costaugmented inference network output . Second , the cost - augmented inference network ( corresponding to the generator in a GAN ) could be so poorly trained that the energy of its output is very large , leading the margin constraints to be satisfied and l 0 to be zero . In standard margin - rescaled max - margin learning in structured prediction ( Taskar et al , 2004 ; Tsochantaridis et al , 2004 ) , the cost - augmented inference step is performed exactly ( or approximately with reasonable guarantee of effectiveness ) , ensuring that when l 0 is zero , the energy parameters are well trained . However , in our case , l 0 may be zero simply because the cost - augmented inference network is undertrained , which will be the case early in training . Then , when using zero truncation , the gradient of the inference network parameters will be 0 . This is likely why Tu and Gimpel ( 2018 ) found it important to add several stabilization terms to the l 0 objective . We find that by instead removing the truncation , learning stabilizes and becomes less dependent on these additional terms . Note that we retain the truncation at zero when updating the energy parameters Θ. As shown in Figure 2 ( a ) , without any stabilization terms and with truncation , the inference network will barely move from its starting point and learning fails overall . However , without truncation , the inference network can work well even without any stabilization terms .
Tu and proposed adding a local cross entropy ( CE ) loss , which is the sum of the label cross entropy losses over all positions in the sequence , to stabilize inference network training . We similarly find this term to help speed up convergence and improve accuracy . Figure 2 ( b ) shows faster convergence to high accuracy when adding the local CE term . See Section 7 for more details .
When training SPENs with inference networks , the inference network parameters are nested within the energy function . We found that the gradient components of the inference network parameters consequently have smaller absolute values than those of the energy function parameters . So , we alternate between k ≥ 1 steps of optimizing the inference network parameters ( " I steps " ) and one step of optimizing the energy function parameters ( " E steps " ) . We find this strategy especially helpful when using complex inference network architectures . To analyze , we compute the cost - augmented loss l 1 = ( F Φ ( x ) , y ) − E Θ ( x , F Φ ( x ) ) and the margin - rescaled hinge loss With k = 1 , the setting used by Tu and Gimpel ( 2018 ) , the inference network lags behind the energy , making the energy parameter updates very small , as shown by the small norms in Fig . 3 ( c ) . The inference network gradient norm ( Fig . 3 ( d ) ) remains high , indicating underfitting . However , increasing k too much also harms learning , as evi - denced by the " plateau " effect in the l 1 curves for k = 50 ; this indicates that the energy function is lagging behind the inference network . Using k = 5 leads to more of a balance between l 1 and l 0 and gradient norms that are mostly decreasing during training . We treat k as a hyperparameter that is tuned in our experiments . l 0 = [ ( F Φ ( x ) , y ) − E Θ ( x , F Φ ( x ) ) + E Θ ( x , y ) ] + averaged There is a potential connection between our use of multiple I steps and a similar procedure used in GANs ( Goodfellow et al , 2014 ) . In the GAN objective , the discriminator D is updated in the inner loop , and they alternate between multiple update steps for D and one update step for G. In this section , we similarly found benefit from multiple steps of inner loop optimization for every step of the outer loop . However , the analogy is limited , since GAN training involves sampling noise vectors and using them to generate data , while there are no noise vectors or explicitly - generated samples in our framework .
For our sequence labeling experiments in this paper , the input x is a length - T sequence of tokens , and the output y is a sequence of labels of length T . We use y t to denote the output label at position t , where y t is a vector of length L ( the number of labels in the label set ) and where y t , j is the jth entry of the vector y t . In the original output space Y ( x ) , y t , j is 1 for a single j and 0 for all others . In the relaxed output space Y R ( x ) , y t , j can be interpreted as the probability of the tth position being labeled with label j. We then use the following energy for sequence labeling ( Tu and Gimpel , 2018 ) : E Θ ( x , y ) = − T t=1 L j=1 y t , j U j b ( x , t ) + T t=1 y t−1 W y t ( 7 ) where U j R d is a parameter vector for label j and the parameter matrix W R L×L contains label - pair parameters . Also , b ( x , t ) R d denotes the " input feature vector " for position t. We define b to be the d - dimensional BiLSTM ( Hochreiter and Schmidhuber , 1997 ) hidden vector at t. The full set of energy parameters Θ includes the U j vectors , W , and the parameters of the BiLSTM . Global Energies for Sequence Labeling . In addition to new training strategies , we also experiment with several global energy terms for sequence labeling . Eq . ( 7 ) shows the base energy , and to capture long - distance dependencies , we include global energy ( GE ) terms in the form of Eq . ( 8 ) . We use h to denote an LSTM tag language model ( TLM ) that takes a sequence of labels as input and returns a distribution over next labels . We define y t = h ( y 0 , . . . , y t−1 ) to be the distribution given the preceding label vectors ( under a LSTM language model ) . Then , the energy term is : E TLM ( y ) = − T +1 t=1 log y t y t ( 8 ) where y 0 is the start - of - sequence symbol and y T +1 is the end - of - sequence symbol . This energy returns the negative log - likelihood under the TLM of the candidate output y. Tu and Gimpel ( 2018 ) pretrained their h on a large , automatically - tagged corpus and fixed its parameters when optimizing Θ. Our approach has one critical difference . We instead do not pretrain h , and its parameters are learned when optimizing Θ. We show that even without pretraining , our global energy terms are still able to capture useful additional information . We also propose new global energy terms . Define y t = h ( y 0 , . . . , y t−1 ) where h is an LSTM TLM that takes a sequence of labels as input and returns a distribution over next labels . First , we add a TLM in the backward direction ( denoted y t analogously to the forward TLM ) . Second , we include words as additional inputs to forward and backward TLMs . We define y t = g ( x 0 , ... , x t−1 , y 0 , ... , y t−1 ) where g is a forward LSTM TLM . We define the backward version similarly ( denoted y t ) . The global energy is therefore E GE ( y ) = − T +1 t=1 log ( y t y t ) + log ( y t y t ) + γ log ( y t y t ) + log ( y t y t ) ( 9 ) Here γ is a hyperparameter that is tuned . We experiment with three settings for the global energy : GE ( a ) : forward TLM as in Tu and Gimpel ( 2018 ) ; GE ( b ) : forward and backward TLMs ( γ = 0 ) ; GE ( c ) : all four TLMs in Eq . ( 9 ) .
We consider two sequence labeling tasks : Twitter part - of - speech ( POS ) tagging ( Gimpel et al , 2011 ) and named entity recognition ( NER ; Tjong Kim Sang and De Meulder , 2003 ) . Twitter Part - of - Speech ( POS ) Tagging . We use the Twitter POS data from Gimpel et al ( 2011 ) and Owoputi et al ( 2013 ) which contain 25 tags . We use 100 - dimensional skip - gram ( Mikolov et al , 2013 ) embeddings from Tu et al ( 2017 ) . Like Tu and Gimpel ( 2018 ) , we use a BiLSTM to compute the input feature vector for each position , using hidden size 100 . We also use BiLSTMs for the inference networks . The output of the inference network is a softmax function , so the inference network will produce a distribution over labels at each position . The ∆ is L1 distance . We train the inference network using stochastic gradient descent ( SGD ) with momentum and train the energy parameters using Adam ( Kingma and Ba , 2014 ) . We also explore training the inference network using Adam when not using the local CE loss . 1 In experiments with the local CE term , its weight is set to 1 . Named Entity Recognition ( NER ) . We use the CoNLL 2003 English dataset ( Tjong Kim Sang and De Meulder , 2003 ) . We use the BIOES tagging scheme , following previous work ( Ratinov and Roth , 2009 ) , resulting in 17 NER labels . We use 100 - dimensional pretrained GloVe embeddings ( Pennington et al , 2014 ) . The task is evaluated using F1 score computed with the conlleval script . The architectures for the feature networks in the energy function and inference networks are all BiLSTMs . The architectures for tag language models are LSTMs . We use a dropout keep - prob of 0.7 for all LSTM cells . The hidden size for all LSTMs is 128 . We use Adam ( Kingma and Ba , 2014 ) and do early stopping on the development set . We use a learning rate of 5 10 −4 . Similar to above , the weight for the CE term is set to 1 . We consider three NER modeling configurations . NER uses only words as input and pretrained , fixed Tu and Gimpel ( 2018 ) . The inference network architecture is a one - layer BiLSTM . GloVe embeddings . NER+ uses words , the case of the first letter , POS tags , and chunk labels , as well as pretrained GloVe embeddings with fine - tuning . NER++ includes everything in NER+ as well as character - based word representations obtained using a convolutional network over the character sequence in each word . Unless otherwise indicated , our SPENs use the energy in Eq . ( 7 ) .
Effect of Zero Truncation and Local CE Loss . Table 1 shows results for zero truncation and the local CE term . Training fails for both tasks when using zero truncation without CE . Removing truncation makes learning succeed and leads to effective models even without using CE . However , when using the local CE term , truncation has little effect on performance . The importance of CE in prior work ( Tu and Gimpel , 2018 ) is likely due to the fact that truncation was being used . The local CE term is useful for both tasks , though it appears more helpful for tagging . 2 This may be because POS tagging is a more local task . Regardless , for both tasks , as shown in Section 4.2 , the inclusion of the CE term speeds convergence and improves training stability . For example , on NER , using the CE term reduces the number of epochs chosen by early stopping from ∼100 to ∼25 . For POS , using the CE term reduces the number of epochs from ∼150 to ∼60 . Effect of Compound Objective and Joint Parameterizations . The compound objective is the sum of the margin - rescaled and perceptron losses , and outperforms them both ( see Table 2 ) . Across all tasks , the shared and stacked parameterizations are more accurate than the previous objectives . For the separated parameterization , the performance drops slightly for NER , likely due to the larger number of parameters . The shared and stacked options have fewer parameters to train than the separated option , and the stacked version processes examples at the fastest rate during training . The top part of Table 3 shows how the performance of the test - time inference network A Ψ and the cost - augmented inference network F Φ vary when using the new compound objective . The differences between F Φ and A Ψ are larger than in the baseline configuration , showing that the two are learning complementary functionality . With the stacked parameterization , the cost - augmented network F Φ receives as an additional input the gold standard label sequence , which leads to the largest differences as the cost - augmented network can explicitly favor incorrect labels . 3 The bottom part of Table 3 shows qualitative differences between the two inference networks . On the POS development set , we count the differences between the predictions of A Ψ and F Φ when A Ψ makes the correct prediction . 4 F Φ tends to output tags that are highly confusable with those output by A Ψ . For example , it often outputs proper noun when the gold standard is common noun or vice versa . It also captures the ambiguities among adverbs , adjectives , and prepositions . Global Energies . The results are shown in Table 4 . Adding the backward ( b ) and word - augmented TLMs ( c ) improves over using only the forward TLM from Tu and Gimpel ( 2018 ) . With the global energies , our performance is comparable to several strong results ( 90.94 of Lample et al , 2016 and91.37 of Ma andHovy , 2016 ) . However , it is still lower than the state of the art ( Akbik et al , 2018 ; Devlin et al , 2019 ) , likely due to the lack of contextualized embeddings . In other work , we proposed and evaluated several other high - order energy terms for sequence labeling using our framework ( Tu et al , 2020a ) .
There are several efforts aimed at stabilizing and improving learning in generative adversarial networks ( GANs ) ( Goodfellow et al , 2014 ; Salimans et al , 2016 ; Zhao et al , 2017 ; from overcoming learning difficulties by modifying loss functions and optimization , and GANs have become more successful and popular as a result . Notably , Wasserstein GANs provided the first convergence measure in GAN training using Wasserstein distance . To compute Wasserstein distance , the discriminator uses weight clipping , which limits network capacity . Weight clipping was subsequently replaced with a gradient norm constraint ( Gulrajani et al , 2017 ) . Miyato et al ( 2018 ) proposed a novel weight normalization technique called spectral normalization . These methods may be applicable to the similar optimization problems solved in learning SPENs . Another direction may be to explore alternative training objectives for SPENs , such as those that use weaker supervision than complete structures ( Rooshenas et al , 2018 ( Rooshenas et al , , 2019Naskar et al , 2020 ) .
We contributed several strategies to stabilize and improve joint training of SPENs and inference networks . Our use of joint parameterizations mitigates the need for inference network fine - tuning , leads to complementarity in the learned inference networks , and yields improved performance overall . These developments offer promise for SPENs to be more easily applied to a broad range of NLP tasks . Future work will explore other structured prediction tasks , such as parsing and generation . We have taken initial steps in this direction , considering constituency parsing with the sequence - to - sequence model of Tran et al ( 2018 ) . Preliminary experiments are positive , 5 but significant challenges remain , specifically in defining appropriate inference network architectures to enable efficient learning .
We linearize the constituency parsing outputs , similar to Tran et al ( 2018 ) . We use the following equation plus global energy in the form of Eq . ( 8 ) as the energy function : E Θ ( x , y ) = − T t=1 L j=1 y t , j U j b ( x , t ) + T t=1 y t−1 W y t Here , b has a seq2seq - with - attention architecture identical to Tran et al ( 2018 ) . In particular , here is the list of implementation decisions . We can write b = g f where f ( which we call the " feature network " ) takes in an input sentence , passes it through the encoder , and passes the encoder output to the decoder feature layer to obtain hidden states ; g takes in the hidden states and passes them into the rest of the layers in the decoder . In our experiments , the cost - augmented inference network F Φ , test - time inference network A Ψ , and b of the energy function above share the same feature network ( defined as f above ) . The feature network ( f ) component of b is pretrained using the feed - forward local crossentropy objective . The cost - augmented inference network F Φ and the test - time inference network A Ψ are both pretrained using the feed - forward local cross - entropy objective . The seq2seq baseline achieves 82.80 F1 on the development set in our replication of Tran et al ( 2018 ) . Using a SPEN with our stacked parameterization , we obtain 83.22 F1 .
We would like to thank the reviewers for insightful comments . This research was supported in part by an Amazon Research Award to K. Gimpel .
