A Negative Case Analysis of Visual Grounding Methods for VQA
Existing Visual Question Answering ( VQA ) methods tend to exploit dataset biases and spurious statistical correlations , instead of producing right answers for the right reasons . To address this issue , recent bias mitigation methods for VQA propose to incorporate visual cues ( e.g. , human attention maps ) to better ground the VQA models , showcasing impressive gains . However , we show that the performance improvements are not a result of improved visual grounding , but a regularization effect which prevents over - fitting to linguistic priors . For instance , we find that it is not actually necessary to provide proper , humanbased cues ; random , insensible cues also result in similar improvements . Based on this observation , we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state - of - theart performance on VQA - CPv2 1 .
Visual Question Answering ( VQA ) ( Antol et al , 2015 ) , the task of answering questions about visual content , was proposed to facilitate the development of models with human - like visual and linguistic understanding . However , existing VQA models often exploit superficial statistical biases to produce responses , instead of producing the right answers for the right reasons ( Kafle et al , 2019 ) . The VQA - CP dataset showcases this phenomenon by incorporating different question type / answer distributions in the train and test sets . Since the linguistic priors in the train and test sets differ , models that exploit these priors fail on the test set . To tackle this issue , recent works have endeavored to enforce proper visual grounding , where the goal is to make models produce answers by looking at relevant visual regions ( Gan et al , 2017 ; Selvaraju et al , Figure 1 : We find that existing visual sensitivity enhancement methods improve performance on VQA - CPv2 through regularization as opposed to proper visual grounding . 2019 ; Wu and Mooney , 2019 ) , instead of exploiting linguistic priors . These approaches rely on additional annotations / cues such as human - based attention maps ( Das et al , 2017 ) , textual explanations ( Huk Park et al , 2018 ) and object label predictions ( Ren et al , 2015 ) to identify relevant regions , and train the model to base its predictions on those regions , showing large improvements ( 8 - 10 % accuracy ) on the VQA - CPv2 dataset . Here , we study these methods . We find that their improved accuracy does not actually emerge from proper visual grounding , but from regularization effects , where the model forgets the linguistic priors in the train set , thereby performing better on the test set . To support these claims , we first show that it is possible to achieve such gains even when the model is trained to look at : a ) irrelevant visual regions , and b ) random visual regions . Second , we show that differences in the predictions from the variants trained with relevant , irrelevant and random visual regions are not statistically significant . Third , we show that these methods degrade performance when the priors remain intact and instead work on VQA - CPv2 by hurting its train accuracy . Based on these observations , we hypothesize that controlled degradation on the train set al ows models to forget the training priors to improve test accuracy . To test this hypothesis , we introduce a simple regularization scheme that zeros out the ground truth answers , thereby always penalizing the model , whether the predictions are correct or incorrect . We find that this approach also achieves near state - of - the - art performance ( 48.9 % on VQA - CPv2 ) , providing further support for our claims . While we agree that visual grounding is a useful direction to pursue , our experiments show that the community requires better ways to test if systems are actually visually grounded . We make some recommendations in the discussion section .
As expected of any real world dataset , VQA datasets also contain dataset biases ( Goyal et al , 2017 ) . The VQA - CP dataset was introduced to study the robustness of VQA methods against linguistic biases . Since it contains different answer distributions in the train and test sets , VQA - CP makes it nearly impossible for the models that rely upon linguistic correlations to perform well on the test set Shrestha et al , 2019 ) .
VQA algorithms without explicit bias mitigation mechanisms fail on VQA - CP , so recent works have focused on the following solutions :
Some recent approaches employ a question - only branch as a control model to discover the questions most affected by linguistic correlations . The question - only model is either used to perform adversarial regularization ( Grand and Belinkov , 2019 ; Ramakrishnan et al , 2018 ) or to re - scale the loss based on the difficulty of the question ( Cadene et al , 2019 ) . However , when these ideas are applied to the UpDn model ( Anderson et al , 2018 ) , which attempts to learn correct visual grounding , these approaches achieve 4 - 7 % lower accuracy compared to the state - of - the - art methods .
Both Human Importance Aware Network Tuning ( HINT ) ( Selvaraju et al , 2019 ) and Self Critical Reasoning ( SCR ) ( Wu and Mooney , 2019 ) , train the network to be more sensitive towards salient image regions by improving the alignment between visual cues and gradient - based sensitivity scores . HINT proposes a ranking loss between humanbased importance scores ( Das et al , 2016 ) and the gradient - based sensitivities . In contrast , SCR does not require exact saliency ranks . Instead , it penalizes the model if correct answers are more sensitive towards non - important regions as compared to important regions , and if incorrect answers are more sensitive to important regions than correct answers .
Given a question Q and an image I , e.g. , represented by bottom - up region proposals : v ( Anderson et al , 2018 ) , a VQA model is tasked with predicting the answer a : P ( a | Q , I ) = f V QA ( v , Q ) . ( 1 )
Without additional regularization , existing VQA models such as the baseline model used in this work : UpDn ( Anderson et al , 2018 ) , tend to rely on the linguistic priors : P ( a | Q ) to answer questions . Such models fail on VQA - CP , because the priors in the test set differ from the train set .
To reduce the reliance on linguistic priors , visual sensitivity enhancement methods attempt to train the model to be more sensitive to relevant visual regions when answering questions . Following ( Wu and Mooney , 2019 ) , we define the sensitivity of an answer a with respect to a visual region v i as : S ( a , v i ) : = ( ∇ v i P ( a | I , Q ) ) T 1 . ( 2 ) Existing methods propose the following training objectives to improve grounding using S : HINT uses a ranking loss , which penalizes the model if the pair - wise rankings of the sensitivities of visual regions towards ground truth answers a gt are different from the ranks computed from the human - based attention maps . SCR divides the region proposals into influential and non - influential regions and penalizes the model if : 1 ) S ( a gt ) of a non - influential region is higher than an influential region , and 2 ) the region most influential for the correct answer has even higher sensitivity for incorrect answers . Both methods improve baseline accuracy by 8 - 10 % . Is this actually due to better visual grounding ? 4 Why Did the Performance Improve ? We probe the reasons behind the performance improvements of HINT and SCR . We first analyze if the results improve even when the visual cues are irrelevant ( Sec . 4.2 ) or random ( Sec . 4.3 ) and examine if their differences are statistically significant ( Sec . 4.4 ) . Then , we analyze the regularization effects by evaluating the performance on VQA - CPv2 's train split ( Sec . 4.5 ) and the behavior on a dataset without changing priors ( Sec . 4.6 ) . We present a new metric to assess visual grounding in Sec . 4.7 and describe our regularization method in Sec . 5 .
We compare the baseline UpDn model with HINT and SCR - variants trained on VQAv2 or VQA - CPv2 to study the causes behind the improvements . We report mean accuracies across 5 runs , where a pretrained UpDn model is fine - tuned on subsets with human attention maps and textual explanations for HINT and SCR respectively . Further training details are provided in the Appendix .
In our first experiment we studied how irrelevant visual cues performed compared to relevant ones . We fine - tune the model with irrelevant cues defined as : S irrelevant : = ( 1 − S h ) , where , S h represents the human - based importance scores . As shown in the ' Grounding using irrelevant cues ' section of Table 1 , both HINT and SCR are within 0.3 % of the results obtained from looking at relevant regions , which indicates the gains for HINT and SCR are not necessarily from looking at relevant regions .
In our next experiment we studied how random visual cues performed with HINT and SCR . We assign random importance scores to the visual regions : S rand ∼ uniform ( 0 , 1 ) . We test two variants of randomness : Fixed random regions , where 1 , both of these variants obtain similar results as the model trained with human - based importance scores . The performance improves even when the importance scores are changed every epoch , indicating that it is not even necessary to look at the same visual regions .
To test if the changes in results were statistically significant , we performed Welch 's t - tests ( Welch , 1938 ) on the predictions of the variants trained on relevant , irrelevant and random cues . We pick Welch 's t - test over the Student 's t - test , because the latter assumes equal variances for predictions from different variants . To perform the tests , we first randomly sample 5000 subsets of non - overlapping test instances . We then average the accuracy of each subset across 5 runs , obtaining 5000 values . Next , we run the t - tests for HINT and SCR separately on the subset accuracies . As shown in Table 2 , the p - values across the variants of HINT and SCR are greater than or equal to 0.3 . Using a confidence level of 95 % ( α = 0.05 ) , we fail to reject the null hypothesis that the mean difference between the paired values is 0 , showing that the variants are not statistically significantly different from each other . We also compare the predictions of HINT / SCR against baseline , and find that p - values are all zeros , showing that the differences have statistical significance . Percentage of Overlaps : To further check if the variants trained on irrelevant or random regions gain performance in a manner similar to the models trained on relevant regions , we compute the overlap between their predictions on VQA - CPv2 's test set . The percentage of overlap is defined as : % Overlap = n same n total × 100 % , where , n same denotes the number of instances where either both variants were correct or both were incorrect and n total denotes the total number of test instances . As shown in Table 2 , we compare % Overlap between different variants of HINT / SCR with baseline and against each other . We find 89.7 − 91.9 % and 89.5 − 92.0 % overlaps for different variants of HINT and SCR respectively . These high overlaps suggest that the variants are not working in fundamentally different manners .
We compare the training accuracies to analyze the regularization effects . As shown in Table 1 , the baseline method has the highest training results , while the other methods cause 6.0 − 14.0 % and 3.3−10.5 % drops in the training accuracy on VQA - CPv2 and VQAv2 , respectively . We hypothesize that degrading performance on the train set helps forget linguistic biases , which in turn helps accuracy on VQA - CPv2 's test set but hurts accuracy on VQAv2 's val set .
As observed by Selvaraju et al ( 2019 ) and as shown in Fig . 2 , we observe small improvements on VQAv2 when the models are fine - tuned on the entire train set . However , if we were to compare against the improvements in VQA - CPv2 in a fair manner , i.e. , only use the instances with visual cues while fine - tuning , then , the performance on VQAv2 drops continuously during the course of the training . This indicates that HINT and SCR help forget linguistic priors , which is beneficial for VQA - CPv2 but not for VQAv2 .
In order to quantitatively assess visual grounding , we propose a new metric called : Correctly Predicted but Improperly Grounded ( CPIG ) : % CP IG = N correct ans , improper grounding N correct ans × 100 % , which is the number instances for which the most sensitive visual region used to correctly predict the answer is not within top - 3 most relevant ground truth regions , normalized by the total number of correct predictions . HINT and SCR trained on relevant regions obtained lower CPIG values that other variants ( 70.24 % and 80.22 % respectively ) , indicating they are better than other variants at finding relevant regions . However , these numbers are still high , and show that only 29.76 % and 19.78 % of the correct predictions for HINT and SCR were properly grounded . Further analysis is presented in the Appendix .
The usage of visual cues and sensitivities in existing methods is superfluous because the results indicate that performance improves through degradation of training accuracy . We hypothesize that simple regularization that does not rely on cues or sensitivities can also achieve large performance gains for VQA - CP . To test this hypothesis , we devise a simple loss function which continuously degrades the training accuracy by training the network to always predict a score of zero for all possible answers i.e. produce a zero vector ( 0 ) . The overall loss function can be written as : L : = BCE ( P ( A ) , A gt ) + λBCE ( P ( A ) , 0 ) , where , BCE refers to the binary cross entropy loss and P ( A ) is a vector consisting of predicted scores for all possible answers . The first term is the binary cross entropy loss between model predictions and ground truth answer vector ( A gt ) , and the second term is our regularizer with a coefficient of λ = 1 . Note that this regularizer continually penalizes the model during the course of the training , whether its predictions are correct or incorrect . As shown in Table 1 , we present results when this loss is used on : a ) Fixed subset covering 1 % of the dataset , b ) Varying subset covering 1 % of the dataset , where a new random subset is sampled every epoch and c ) 100 % of the dataset . Confirming our hypothesis , all variants of our model achieve near state - of - the - art results , solidifying our claim that the performance gains for recent methods come from regularization effects . It is also interesting to note that the drop in training accuracy is lower with this regularization scheme as compared to the state - of - the - art methods . Of course , if any model was actually visually grounded , then we would expect it to improve performances on both train and test sets . We do not observe such behavior in any of the methods , indicating that they are not producing right answers for the right reasons .
While our results indicate that current visual grounding based bias mitigation approaches do not suffice , we believe this is still a good research direction . However , future methods must seek to verify that performance gains are not stemming from spurious sources by using an experimental setup similar to that presented in this paper . We recommend that both train and test accuracy be reported , because a model truly capable of visual grounding would not cause drastic drops in training accuracy to do well on the test sets . Finally , we advocate for creating a dataset with ground truth grounding available for 100 % of the instances using synthetically generated datasets Kafle et al , 2018 ; Acharya et al , 2019b ; Hudson and Manning , 2019 ; Johnson et al , 2017 ) , enabling the community to evaluate if their methods are able to focus on relevant information . Another alternative is to use tasks that explicitly test grounding , e.g. , in visual query detection an agent must output boxes around any regions of a scene that match the natural language query ( Acharya et al , 2019a ) .
Here , we showed that existing visual grounding based bias mitigation methods for VQA are not working as intended . We found that the accuracy improvements stem from a regularization effect rather than proper visual grounding . We proposed a simple regularization scheme which , despite not requiring additional annotations , rivals state - of - theart accuracy . Future visual grounding methods should be tested with a more comprehensive experimental setup and datasets for proper evaluation .
We compare four different variants of HINT and SCR to study the causes behind the improvements including the models that are fine - tuned on : 1 ) relevant regions ( state - of - the - art methods ) 2 ) irrelevant regions 3 ) fixed random regions and 4 ) variable random regions . For all variants , we fine - tune a pretrained UpDn , which was trained on either VQA - CPv2 or VQAv2 for 40 epochs with a learning rate of 10 −3 . When fine - tuning with HINT , SCR or our method , we also use the main binary cross entropy VQA loss , whose weight is set to 1 . The batch size is set to 384 for all of the experiments .
Following ( Selvaraju et al , 2019 ) , we train HINT on the subset with human - based attention maps ( Das et al , 2017 ) , which are available for 9 % of the VQA - CPv2 train and test sets . The same subset is used for VQAv2 too . The learning rate is set to 2 × 10 −5 and the weight for the HINT loss is set to 2 .
Since ( Wu and Mooney , 2019 ) reported that humanbased textual explanations ( Huk Park et al , 2018 ) gave better results than human - based attention maps for SCR , we train all of the SCR variants on the subset containing textual explanation - based cues . SCR is trained in two phases . For the first phase , which strengthens the influential objects , we use a learning rate of 5 × 10 −5 , loss weight of 3 and train the model to a maximum of 12 epochs . Then , following ( Wu and Mooney , 2019 ) , for the second phase , we use the best performing model from the first phase to train the second phase , which criticizes incorrect dominant answers . For the second phase , we use a learning rate of 10 −4 and weight of 1000 , which is applied alongside the loss term used in the first phase . The specified hyperparameters worked better for us than the values provided in the original paper . Our Zero - Out Regularizer Our regularization method , which is a binary cross entropy loss between the model predictions and a zero vector , does not use additional cues or sensitivities and yet achieves near state - of - the - art performance on VQA - CPv2 . We set the learning rate to : 2×10 −6 r , where r is the ratio of the training instances used for fine - tuning . The weight for the loss is set to 2 . We report the performance obtained at the 8 th epoch .
Following ( Selvaraju et al , 2019 ) , we report Spearman 's rank correlation between network 's sensitivity scores and human - based scores in Table A3 . For HINT and our zero - out regularizer , we use human - based attention maps . For SCR , we use textual explanation - based scores . We find that HINT trained on human attention maps has the highest correlation coefficients for both datasets . However , compared to baseline , HINT variants trained on random visual cues also show improved correlations . For SCR , we obtain surprising results , with the model trained on irrelevant cues obtaining higher correlation than that trained on relevant visual cues . As expected , applying our regularizer does not improve rank correlation . Since HINT trained on relevant cues obtains the highest correlation values , it does indicate improvement in visual grounding . However , as we have seen , the improvements in performance can not necessarily be attributed to better overlap with ground truth localizations .
Presentation of qualitative examples in visual grounding models for VQA suffers from confirmation bias i.e. , while it is possible to find qualitative samples that look at relevant regions to answer questions properly , it is also possible to find samples that produce correct answers without looking at relevant regions . We present examples for such cases in Fig . A3 . We next present a quantitative assessment of visual grounding , which does not suffer from the confirmation bias .
In order to truly assess if existing methods are using relevant regions to produce correct answers , we use our proposed metric : Correctly Predicted but Improperly Grounded ( CPIG ) . If the CPIG values are large , then it implies that large portion of correctly predicted samples were not properly grounded . Fig . A4 shows % CPIG for different variants of HINT trained on human attention - based cues , whereas Fig . A5 shows the metric for different variants of SCR trained on textual explanationbased cues . We observe that HINT and SCR trained on relevant regions have the lowest % CPIG values ( 70.24 % and 80.22 % respectively ) , indicating that they are better than other variants in finding relevant regions . However , only a small percentage of correctly predicted samples were properly grounded ( 29.76 % and 19.78 % for HINT and SCR respectively ) , even when trained on relevant cues .
Table A4 shows VQA accuracy for each answer type on VQACPv2 's test set . HINT / SCR and our regularizer show large gains in ' Yes / No ' questions . We hypothesize that the methods help forget linguistic priors , which improves test accuracy of such questions . In the train set of VQACPv2 , the answer ' no ' is more frequent than the answer ' yes ' , tempting the baseline model to answer ' yes / no ' questions with ' no ' . However , in the test set , answer ' yes ' is more frequent . Regularization effects caused by HINT / SCR and our method cause the models to weaken this prior i.e. , reduce the tendency to just predict ' no ' , which would increase accuracy at test because ' yes ' is more frequent in the test set . Next , all of the methods perform poorly on ' Number ( Num ) ' answer type , showing that methods find it difficult to answer questions that are most reliant on correct visual grounding such as : localizing and counting objects . Finally , we do not observe large improvements in ' Other ' question type , most likely due to the large number of answers present under this answer type .
We test our regularization method on random subsets of varying sizes . Fig . A6 shows the results when we apply our loss to 1 − 100 % of the training instances . Clearly , the ability to regularize the model does not vary much with respect to the size of the train subset , with the best performance occurring when our loss is applied to 1 % of the training instances . These results support our claims that it is possible to improve performance without actually performing visual grounding . Q : Is this food sweet ? A : yes Remarks : The most sensitive regions for irrelevant / random variants do not contain food , yet their answers are correct .
HINT trained on relevant cues HINT trained on irrelevant cues HINT trained on random cues Q : Has the boy worn out his jeans ? A : yes Remarks : All of the variants look at both relevant and irrelevant regions to produce correct answer . Q : Is the sport being played tennis or volleyball ? A : tennis Remarks : None of the variants look at relevant regions , and yet produce correct answer . Q : What is the swimmer doing ? A : surfing Remarks : Models trained on irrelevant / random cues do not look at the swimmer at all , yet produce correct answer . We pick samples where all variants produce correct response to the question . The first column shows ground truth regions and columns 2 - 4 show visualizations from HINT trained on relevant , irrelevant and fixed random regions respectively . Figure A5 : % CPIG for baseline and different variants of SCR and our method , computed using ground truth relevant regions taken from textual explanations ( txt ) . Train Test Figure A6 : The regularization effect of our loss is invariant with respect to the dataset size .
Acknowledgement . This work was supported in part by AFOSR grant [ FA9550 - 18 - 1 - 0121 ] , NSF award # 1909696 , and a gift from Adobe Research . We thank NVIDIA for the GPU donation . The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies or endorsements of any sponsor . We are grateful to Tyler Hayes for agreeing to review the paper at short notice and suggesting valuable edits and corrections for the paper .
