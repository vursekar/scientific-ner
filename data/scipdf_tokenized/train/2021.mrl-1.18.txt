Multilingual Code - Switching for Zero - Shot Cross - Lingual Intent Prediction and Slot Filling
Predicting user intent and detecting the corresponding slots from text are two key problems in Natural Language Understanding ( NLU ) . Since annotated datasets are only available for a handful of languages , our work focuses particularly on a zero - shot scenario where the target language is unseen during training . In the context of zero - shot learning , this task is typically approached using representations from pre - trained multilingual language models such as mBERT or by fine - tuning on data automatically translated into the target language . We propose a novel method which augments monolingual source data using multilingual code - switching via random translations , to enhance generalizability of large multilingual language models when fine - tuning them for downstream tasks . Experiments on the Mul - tiATIS++ benchmark show that our method leads to an average improvement of +4.2 % in accuracy for the intent task and +1.8 % in F1 for the slot - filling task over the state - of - the - art across 8 typologically diverse languages . We also study the impact of code - switching into different families of languages on downstream performance . Furthermore , we present an application of our method for crisis informatics using a new human - annotated tweet dataset of slot filling in English and Haitian Creole , collected during the Haiti earthquake . 1
A cross - lingual setting is typically described as a scenario in which a model trained for a particular task in one source language ( e.g. English ) should be able to generalize well to a different target language ( e.g. Japanese ) . While semi - supervised solutions ( Muis et al , 2018 ; FitzGerald , 2020 , inter alia ) assume some target language data or translators are available , a zero - shot solution ( Eriguchi et al , 2018 ; Srivastava et al , 2018 ; assumes none is available at training time . Having models that generalize well even to unseen languages is crucial for tackling real world problems such as extracting relevant information during a new disaster ( Nguyen et al , 2017 ; Krishnan et al , 2020 ) or detecting hate speech ( Pamungkas and Patti , 2019 ; Stappen et al , 2020 ) , where the target language might be of low - resource or unknown . Intent prediction and slot filling are two NLU tasks , usually solved jointly , which learn to model the intent ( sentence - level ) and slot ( word - level ) labels . Such models are currently used extensively for goal - oriented dialogue systems , such as Amazon 's Alexa , Apple 's Siri , Google Assistant , and Microsoft 's Cortana . Finding the ' intent ' behind the user 's query and identifying relevant ' slots ' in the sentence to engage in a dialogue are essential for effective conversational assistance . For example , users might want to ' play music ' given the slot labels ' year ' and ' artist ' ( Coucke et al , 2018 ) , or they may want to ' book a flight ' given the ' airport ' and ' locations ' slot labels ( Price , 1990 ) . A strong correlation between the two tasks has made jointly trained models successful ( Goo et al , 2018 ; Haihong et al , 2019 ; Hardalov et al , 2020 ; . In a cross - lingual setting , the model should be able to learn this joint task in one language and transfer knowledge to another ( Upadhyay et al , 2018 ; Schuster et al , 2019 ; . This is the premise of our work . Highly effective transformer - based multilingual models such as mBERT ( Devlin et al , 2019 ) and XLM - R ( Conneau et al , 2020a ) have found success across several multilingual tasks in recent years . In the zero - shot cross - lingual transfer setting with an unknown target language , a typical solution is to use pre - trained transformer models and fine - tune to the downstream task using the monolingual source data . However , Pires et al ( 2019 ) showed that existing transformer - based represen - Figure 1 : t - SNE plot of embeddings across the 12 multi - head attention layers of multilingual BERT . Parallelly translated sentences of MutiATIS++ dataset are still clustered according to the languages : English ( black ) , Chinese ( cyan ) , French ( blue ) , German ( green ) , and Japanese ( red ) . Figure 2 : An original example in English from MultiATIS++ dataset and its multilingually code - switched version . In the above code - switching example , the chunks are in Chinese , Punjabi , Spanish , English , Arabic , and Russian . ' atis_airfare ' represents an intent class where the user seeks price of a ticket . tations may exhibit systematic deficiencies for certain language pairs . Figure 1 also verifies that the representations across the 12 multi - head attention layers of mBERT are still not shared across languages , instead forming clearly distinguishable clusters per language . This leads to a fundamental challenge that we address in this work : enhancing the language neutrality so that the fine - tuned model is generalizable across languages for the downstream task . To this goal , we introduce a data augmentation method via multilingual codeswitching , where the original sentence in English is code - switched into randomly selected languages . For example , chunk - level code - switching creates sentences with phrases in multiple languages as shown in Figure 2 . We show that mBERT can be fine - tuned for many languages starting only with monolingual source - language data , leading to better performance in zero - shot settings . Further , we show how code - switching with languages from different language families impacts the model 's performance on individual target languages , even finding some counter - intuitive results . For instance , training on data code - switched between English and Sino - Tibetan languages is as helpful for Hindi ( an Indo - Aryan Indo - European language ) as code - switching with other Indo - Aryan languages , and Turkic languages can be helpful for both Chinese and Japanese .
We present a data augmentation method via multilingual code - switching to enhance the language neutrality of transformerbased language models such as mBERT for finetuning to a downstream NLU task of intent prediction and slot filling . b ) By studying different language families , we show how code - switching can be used to aid zero - shot cross - lingual learning for low - resource languages . c ) We release a new human - annotated tweet dataset , collected during Haiti earthquake disaster , for intent prediction and slot filling in English and Haitian Creole .
This section describes our problem definition , codeswitching algorithm , language families , and the training methodology .
Given a source ( S ) and a set of target ( T ) languages , the goal is to train a classifier using data only in the source language and predict examples from the completely unseen target languages . We assume the target language is unknown during training ( fine - tuning ) time , which makes direct translation to target infeasible . In this context , we use code - switching ( cs ) to augment the monolingual source data . Thus , the input , augmented input , and output of our problem can be defined as : lset = googletrans.languages − lT for i 1 .. k do for j 1 .. len ( X en ut ) do G cs , L cs chunks = slot_chunks ( X en ut [ j ] , y en sl [ j ] ) for c chunks do l random.choice ( lset ) t translate ( c , l ) G cs G cs ∪ t L cs L cs ∪ align_label ( c , t ) end X cs ut X cs ut ∪ G cs y cs y cs ∪ y cs [ j ] y cs sl y cs sl ∪ L cs end end Input : X S ut , y S , y S sl , l T Code - Switched Input : X cs ut , y cs , y cs sl Output : y T , y T sl predict ( X T ut ) where X ut represents sentences , y their ground truth intent classes , y sl the slot labels for the words in those sentences , and l T the set of target languages . An example sentence , its intent class , and slot labels are shown in Figure 2 .
Multilingual masked language models , such as mBERT ( Devlin et al , 2019 ) , are trained using large datasets of publicly available unlabeled corpora such as Wikipedia . Such corpora largely remain monolingual at the sentence level because the presence of intra - sentence code - switched data in written texts is likely scarce . The masked words that needed to be predicted usually are in the same language as their surrounding words . We study how code - switching can enhance the language neutrality of such language models by augmenting it with artificially code - switched data for fine - tuning it to a downstream task . Algorithm 1 explains this codeswitching process at the chunk - level . When using slot filling datasets , slot labels that are grouped by BIO ( Ramshaw and Marcus , 1999 ) tags constitute natural chunks , as shown in Figure 2 . To summarize the algorithm , we take a sentence , take each chunk from that sentence , perform a translation into a random language using Google 's NMT system ( Wu et al , 2016 ) , and align the slot labels to fit the translation , i.e. , label propagation through alignment as the translated sentence do not preserve the
Afro - Asiatic Arabic ( ar ) , Amharic ( am ) , Hebrew ( he ) , Somali ( so ) Germanic German ( de ) , Dutch ( nl ) , Danish ( da ) , Swedish ( sv ) , Norwegian ( no ) Indo - Aryan Hindi ( hi ) , Bengali ( bn ) , Marathi ( mr ) , Nepali ( ne ) , Gujarati ( gu ) , Punjabi ( pa ) Romance Spanish ( es ) , Portuguese ( pt ) , French ( fr ) , Italian ( it ) , Romanian ( ro ) Sino - Tibetan , Koreanic , & Japonic Chinese ( zh - cn ) , Japanese ( ja ) , Korean ( ko ) Turkic Turkish ( tr ) , Azerbaijani ( az ) , Uyghur ( ug ) , Kazakh ( kk ) number and order of words in the original sentence . At the chunk - level , we use a direct alignment . The BIO - tagged labels are recreated for the translated phrase based on the word tokens . More complex methods could be applied here to improve the alignment of the slot labels such as fast - align ( Dyer et al , 2013 ) or soft - align , but we leave this for future work . Code - Switching at the word - level essentially translates every word randomly , while at the sentence - level translates the entire sentence . During the experimental evaluation process , to build a language - neutral model using monolingual source ( English ) data , all eight target languages are excluded from the code - switching procedure to avoid unfair model comparisons , i.e. removing target languages ( l T ) from lset in Algorithm 1 . Complexity . The augmentation process is repeated k times per sentence producing a new augmented dataset of size k × n , where n is the size of the original dataset , i.e. space complexity of O ( k × n ) . For T translations per sentence , Algorithm 1 has a runtime complexity of O ( k × n × T ) assuming constant time for alignment . Word - level requires as many translations as the number of words but sentence - level requires only one . An increase in the dataset size also increases the training time , but the advantage is one model appropriate for many languages .
A language family is defined as a group of related languages that likely share a common ancestor . For example , Portuguese , Spanish , French , Italian , and Romanian are all derived from Latin ( Rowe and Levine , 2017 ) . We use language families to study their impact on the target languages . We augment the source language with code - switching to a particular language family . For instance , codeswitching the English dataset with Turkic language family and testing on Japanese can reveal how closely the two are aligned in the vector space of a pre - trained multilingual model . We work with 6 language groups : Afro - Asiatic ( Voegelin and Voegelin , 1976 ) , Germanic ( Harbert , 2006 ) , Indo - Aryan ( Masica , 1993 ) , Romance ( Elcock and Green , 1960 ) , and Turkic ( Johanson and Johanson , 2015 ) , also grouping Sino - Tibetan , Koreanic and Japonic ( Shafer , 1955 ; Miller , 1967 ) . 2 Germanic , Romance , and Indo - Aryan are genera of the Indo - European family . Language groups and corresponding languages are shown in Table 1 . Each group is selected based on a target language in the dataset , and the Afro - Asiatic family is added as an extra group . In experiments , lset in Algorithm 1 will be assigned languages from a specific family .
Joint training is traditionally used for intent prediction and slot filling to exploit the correlation between the two tasks . This is done by feeding the feature vectors of one model to another or by sharing layers of a neural network followed by training the tasks together . So , a standard joint model loss can be defined as a combination of intent ( L i ) and slot ( L sl ) losses . i.e. , L = αL i + βL sl , where α and β are corresponding task weights . Prior works ( Goo et al , 2018 ; Schuster et al , 2019 ; Liu and Lane , 2016 ; Haihong et al , 2019 ) that use BiL - STM or RNN are now modified to BERT - based implementations explored in more recent works Hardalov et al , 2020 ; . A standard Joint model consists of BERT outputs from the final hidden state ( classification ( CLS ) token for intent and m word tokens for slots ) fed to linear layers to get intent and slot predictions . Assuming h cls represents the CLS token and h m represents a token from the remaining word - level tokens , the BERT model outputs are defined as : p i = sof tmax ( W i h cls + b i ) p sl m = sof tmax ( W sl hm + b sl ) ∀m ( 1 ) with a multi - class cross - entropy loss 3 for both intent ( L i ) and slots ( L sl ) . We will use this model as 2 Each of the Sino - Tibetan , Koreanic , and Japonic families have a single high - resource member ( Chinese , Korean , Japanese respectively ) . We only group them as an additional interesting data point , not because we ascribe to any theories that link them typologically . 3 L = − 1 n ∑︁ n i=1 [ y log ŷ ] our baseline for joint training . Our goal will be to show that code - switching on top of joint training improves the performance . The output of Algorithm 1 will be the input used for joint training on BERT for code - switched experiments . 3 Datasets Benchmark Dataset . We use the latest multilingual benchmark dataset of MultiATIS++ , which was created by manually translating the original ATIS ( Price , 1990 ) dataset from English ( en ) to 8 other languages : Spanish ( es ) , Portuguese ( pt ) , German ( de ) , French ( fr ) , Chinese ( zh ) , Japanese ( ja ) , Hindi ( hi ) , and Turkish ( tr ) . The dataset consists of utterances for each language with an ' intent ' label for ' flight intent ' and ' slot ' labels for the word tokens in BIO format . A sample datapoint in English is shown in Figure 2 . model on the validation set . Consistent with the metrics reported for intent prediction and slot filling evaluation in the past , we also accuracy for intent and micro F1 5 to measure slot performance .
Since we assume that target language is not known before hand , Translate - Train ( TT ) method is not a suitable baseline . Rather , we set this to be an upper bound , i.e. translating to the target language and fine - tuning the model should intuitively outperform a generic model . Additionally , we add code - switching to this TT model to assess if augmentation negatively impacts its performance . The zero - shot baselines for the codeswitching experiments use an English - Only ) model , which is fine - tuned over the pre - trained mBERT separately for each task and an English - only Joint model .
Effect of Multilingual Code - Switching . Table 3 describes performance evaluation on the Multi - ATIS++ dataset . When compared to the state - ofthe - art jointly trained English - only baseline , we see a +4.2 % boost in intent accuracy and +1.8 % boost in slot F1 scores on average by augmenting the dataset via multilingual code - switching without requiring the target language . From the significance tests , except for Spanish and German , all other languages were helped by code - switching for intent detection . For slot filling , improvement on Portuguese and French is insignificant . This suggests that code - switching primarily helped languages that are morphologically more different from the source language ( English ) . For example , Hindi and Turkish have the highest intent performance improvement of +16.1 % and +9.8 % respectively . And for slots , Hindi and Chinese with +6.0 % and +4.3 % respectively . Japanese showed +4 % improvement for intent and +3.4 % for slots . The runtime of the models in Table 5 ( Appendix B ) shows that code - switching is expensive , taking up to five hours for five augmentation rounds ( k = 5 ) . This is because there are k times more data compared to the monolingual source data . Increasing the number of code - switchings ( k ) for a sentence from 5 to 50 improves the performance by +1 % , while increasing the run - time by a large margin . Hence , such tradeoffs should be considered when picking k for real - world applications where time to deployment might be of the essense . In the translate - train ( upper bound ) scenario , it is not immediately clear if augmentation helps , since data in the same language as the target are always preferable to other language or code - switched data . At a minimum , augmentation does not hinder upper - bound performance ( Table 3 ) . For both intent and slot performance , the chunklevel model remained robust across the languages . For intent , the difference between word - level and sentence - level was insignificant . For slot , sentencelevel was in par with chunk - level on average . Thus , we think that code - switching at chunk - level is safer for avoiding semantic discrepancies ( which are a danger in the word - level ) while also better encouraging intra - sentence language neutrality . Evaluation on Disaster Dataset . We found that disaster data is more challenging than the ATIS dataset for transfer learning in NLU . The predictive performance is shown in Table 4 . Code - Switching improved intent accuracy by +12.5 % and slot F1 by +2.3 % , which is quite promising considering the domain mismatch ( tweets vs airline guides ) . Joint training added +0.9 % improvement to intent accuracy , however did not seem to help slot F1 . This might imply a weaker correlation between the two tasks in real - world data , i.e. a mention of ' food ' or ' shelter ' in a tweet may not always mean that there is a ' request ' or vice - versa . The upper bound of translate - train method did not perform any better than the randomly code - switched model which seemed counter - intuitive . This might be due to the lack of strong representation for Haitian Creole in the pre - trained model , although it is similar to French , or due to the limitation of the machine translation system . Impact of Language Families . Results of language family analysis are shown in Figure 3 for the 4 languages that showed significant improvements for both intent and slots in Table 3 . The input in English is independently code - switched using 6 different language families . Note that the target language is always excluded from the group when evaluating on the same , i.e. Hindi is excluded from Indo - Aryan family when that family is being evaluated on it . Translate - train model is provided as a frame of reference and upper bound . Generally , as expected , we found that language families helped their corresponding languages , i.e. Romance helped Spanish , Germanic helped German , and so on . An exception is our loose group of Sino - Tibetan , Koreanic , and Japonic languages - for both Chinese and Japanese , languages from the Turkic language family helped more than others . On the other hand , the Sino - tibetan , Japonic , and Koreanic group helps Hindi more than other Indo - European languages . We believe this highlights the necessity for methods like the one of Xia et al ( 2020 ) that can a priori identify the best helper language or group of languages that can benefit downstream tasks for low resource languages . Control Experiments on k. Hyperparameter k controls the amount of code - switched data . k = 0 represents original size with no code - switching , k = 1 represents original size with code - switching , and k = 10 means 10 - times more code - switched data than the original . The main experiments in Table 3 use k = 5 . Figure 4 shows how varying k affects performance . For this analysis , we consider 4 target languages on which code - switching produced significant results in Table 3 on both Intent Accuracy and Slot F1 : Chinese , Japanese , Hindi , and Turkish . Intuitively , we observe that as k increases , too much code - switching becomes expensive in terms of runtime , while performance improvement slowly plateaus . For Slot F1 performance in all four cases , unlike Intent , we observe an interesting dip when k = 1 , which represents the augmentation having just one copy of codeswitching ( without the original non - code - switched data ) , as compared to k = 0 . Adding the original data to one round of code - switched data ( k = 2 ) leads to big improvements . Overall , we see improvement for both tasks , with Slot F1 plateauing earlier . Table 5 and Figure 10 in Appendix B show the impact of code - switching on training runtime , which increases as k increases . Thus , finding an optimal value of k and specific language groups are essential for downstream applications . mBERT versus XLM - R. Additional performance evaluations and benefits of code - switching on XLM - R ( Conneau et al , 2020a ) , a stronger multilingual language model , are provided in Appendix A. Note that XLM - R is trained using Common - Crawl and is likely to be exposed to some code - switched data . Thus , we focus primarily on mBERT which largely remains monolingual at the sentence - level to identify the unbiased impact of code - switching during fine - tuning . Furthermore , runtime and hyperparameter tuning along with insights into layers to freeze before training are shown in Appendix B. Error Analysis . Selecting intent classes with support > 10 , Figure 5 shows how each class is positively or negatively impacted by code - switching . Improvement was primarily on ' airfare ' , ' distance ' ' capacity ' , ' airline ' , and ' ground_service ' which had longer sentences such as ' Please tell me which airline has the most departures from Atlanta ' when compared to ' abbreviations ' and ' airport ' classes that included very short phrases like ' What does EA mean ? ' However , note that Spanish and German did not improve much , aligning with our results in Table 3 . For slot labels in Figure 6 , we selected the ones with support > 50 and that have different characteristics , e.g. ' name ' , ' code ' , etc . The overall trend in slot performance shows improvements for labels such as ' day_name ' , ' airport_code ' , and ' city_name ' and slight variations in labels such as ' fight_number ' and ' period_of_day ' , implying textual slots benefiting over numeric ones .
Cross - Lingual Transfer . Researchers have studied cross - lingual tasks in various settings such as sentiment / sequence classification ( Wan , 2009 ; Eriguchi et al , 2018 ; , named entity recognition ( Zirikly and Hagiwara , 2015 ; Tsai et Xie et al , 2018 ) , parts - of - speech tagging ( Yarowsky et al , 2001 ; Täckström et al , 2013 ; Plank and Agić , 2018 ) , and natural language understanding ( He et al , 2013 ; Upadhyay et al , 2018 ; . The methodology for most of the current approaches for cross - lingual tasks can be categorizes as : a ) multilingual representations from pre - trained or fine - tuned models such as mBERT ( Devlin et al , 2019 ) or XLM - R ( Conneau et al , 2020a ) , b ) machine translation followed by alignment ( Shah et al , 2010 ; Yarowsky et al , 2001 ; Ni et al , 2017 ) , or c ) a combination of both . Before transformer models , effective approaches included domain adversarial training to extract language - agnostic features ( Ganin et al , 2016 ; and word alignment methods such as MUSE ( Conneau et al , 2017 ) to align fastText word vectors ( Bojanowski et al , 2017 ) . Recently , Conneau et al , 2020b show that having shared parameters in the top layers of the multilingual encoders can be used to align different languages quite effectively on tasks such as XNLI ( Conneau et al , 2018 ) . Monolingual models for joint slot filling and intent prediction have used attention - based RNN ( Liu and Lane , 2016 ) and attention - based BiLSTM with a slot gate ( Goo et al , 2018 ) on benchmark datasets ( Price , 1990 ; Coucke et al , 2018 ) . These methods have shown that a joint method can enhance both tasks and slot filling can be conditioned on the learned intent . A related approach iteratively learns the relationship between the two tasks ( Haihong et al , 2019 ) . Recently , BERT - based approaches ( Hardalov et al , 2020 ; have improved results . On the other hand , cross - lingual versions of this joint task include a low - supervision based approach for Hindi and Turkish ( Upadhyay et al , 2018 ) , new datasets for Spanish and Thai ( Schuster et al , 2019 ) , and recently creating MultiATIS++ , a comprehensive dataset in 9 languages . The joint task mentioned above in a pure zero - shot setting is one of the motivations for our work . A Zero - shot is the setting where the model sees a new distribution of examples only during test ( prediction ) time ( Xian et al , 2017 ; Srivastava et al , 2018 ; Romera - Paredes and Torr , 2015 ) . Thus , in our setting , we assume that target language is unknown during training , so that our model is generalizable across multiple languages . Code - Switching . Linguistic code - switching is a phenomenon where multilingual speakers alternate between languages . Recently , monolingual models have been adapted to code - switched text in entity recognition ( Aguilar and Solorio , 2019 ) , part - ofspeech tagging ( Soto and Hirschberg , 2018 ; Ball and Garrette , 2018 ) , sentiment analysis ( Joshi et al , 2016 ) and language identification ( Mave et al , 2018 ; Yirmibeşoglu and Eryigit , 2018 ; Mager et al , 2019 ) . Recently , KhudaBukhsh et al , 2020 have proposed an approach to sample code - mixed documents using minimal supervision . Qin et al , 2020 allows randomized code - switching to include the target language , as shown in their Figure 3 . In our context for example , if the target language is German , we ensure that there is no code - switching to German during training . We consider this distinction essential to evaluate a true zero - shot learning scenario and prevent any bias when comparing with translate - and - train . present a non - zero - shot approach that performs code - switching to target languages , and Jiang et al ( 2020 ) present a code - switching based method to improve the ability of multilingual language mod - els for factual knowledge retrieval . Contemporary work by Tan and Joty , 2021 makes use of both word and phrase - level code - mixing to switch to a set of languages to perform adversarial training for XNLI . Code - switching and other data augmentation techniques have been applied to the pre - training stage in recent works ( Chaudhary et al , 2020 ; Kale and Siddhant , 2021 ; Dufter and Schütze , 2020 ) . However , pre - training is outside the scope of this work . In addition to studying cross - lingual slot filling and language families , another key distinction of our method is that we completely ignore the target language during training to represent a fully zero - shot scenario . The main advantage is that with enhanced cross - lingual generalizability , it can be deployed out - of - the - box , as our training is conducted independently of the target language .
Our study shows that augmenting the monolingual input data with multilingual code - switching via random translations at the chunk - level helps a zeroshot model to be language neutral when evaluated on unseen languages . This approach enhanced the generalizability of pre - trained language models such as mBERT when fine - tuning for downstream tasks of intent detection and slot filling . Additionally , we presented an application of this method using a new annotated dataset of disaster tweets . Further , we studied code - switching with language families and their impact on specific target languages . Addressing code - switching with language families during the pre - training phase and releasing a larger dataset of annotated disaster tweets in more languages are planned for future work .
The tweet dataset that we constructed for disaster NLU was originally released by Appen 6 , and we use it to construct slot labels in two languages : English ( en ) and Haitian Creole ( ht ) . Data statement that includes annotator guidelines for the labeling jobs and other dataset information will be provided with the implementation . From a broader impact perspective , our code and developed models are open - source and allows NLP technology to be accessible to information systems for emergency services and social scientists in quickly deploying model during disaster events .
This work was partially supported by U.S. National Science Foundation grants IIS - 1815459 , IIS - 1657379 , and 2040926 . This work was also supported in part by the grant H - 4Q21 - 009 from the Commonwealth Cyber Initiative , an investment in the advancement of cyber R&D , innovation , and workforce development ( for more information about CCI , visit www.cyberinitiative . org ) . The authors are thankful to Ming Sun and Alexis Conneau for giving valuable insights on multilingual model training , as well as to the anonymous reviewers for their constructive feedback . We also acknowledge ARGO , a research computing cluster provided by the Office of Research Computing at George Mason University , were most experiments were conducted .
We conduct an additional analysis on XLM - R ( Conneau et al , 2020a ) and compare it with mBERT ( Devlin et al , 2019 ) . The implementation is very similar in PyTorch ( Paszke et al , 2019 ) but using the pre - trained xlm - roberta - base with RobertaForSequenceClassification ( Wolf et al , 2020 ) as the XLM - R model . We observe that , setting k = 5 , XLM - R outperforms mBERT on average ( by 2 % Intent Accuracy and 1.5 % Slot F1 ) . Individually , XLM - R improved Chinese , Japanese , Portuguese , and Turkish for Intent Prediction and German , Chinese , Japanese , Portuguese , and Hindi for Slot Filling as shown in Figure 7 . We observe a trend similar to mBERT with k on XLM - R shown in Figure 8 . However , for XLM - R , we observe that randomized code - switching did not help Chinese for Intent Prediction and Hindi for Slot F1 . If codeswitched to a specific language family , instead of switching to random languages , it might improve their performance . A deeper dive into XLM - R and language families are left for future work .
For joint training with same task weights , we tuned α and β using grid search to see the strength of correlation between the tasks . For intent , the ( α , β ) combination of ( 1.0 , 0.6 ) performed well , while ( 1.0 , 1.0 ) for slots . This suggests that intent benefiting slot might be slightly more than slot benefiting intent . Additionally , during fine - tuning , freezing the layers of the transformer affected the model performance as shown in Figure 9 . Keeping the first 8 layers frozen gave the best performance . By freezing the earlier layers , the transformer can retain its most fundamental feature information gained from the massive pre - training step , and by unfreezing some top layers , it can undergo fine - tuning . Additionally , latency for training a code - switched model is shown in Table 5 and how runtime varies with an increase in k is shown in Figure 10 .
