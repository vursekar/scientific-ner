Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text
Numerical reasoning skills are essential for complex question answering ( CQA ) over text . It requires opertaions including counting , comparison , addition and subtraction . A successful approach to CQA on text , Neural Module Networks ( NMNs ) , follows the programmer - interpreter paradigm and leverages specialised modules to perform compositional reasoning . However , the NMNs framework does not consider the relationship between numbers and entities in both questions and paragraphs . We propose effective techniques to improve NMNs ' numerical reasoning capabilities by making the interpreter questionaware and capturing the relationship between entities and numbers . On the same subset of the DROP dataset for CQA on text , experimental results show that our additions outperform the original NMNs by 3.0 points for the overall F1 score .
Complex Question Answering ( CQA ) is a challenging task , requiring a model to perform compositional and numerical reasoning . Originally proposed for the visual question answering ( VQA ) task , Neural Module Networks ( NMNs ) ( Andreas et al , 2016 ) have recently been adopted to tackle the CQA problem over text ( Gupta et al , 2020 ) . The NMNs is an end - to - end differentiable model in the programmer - interpreter paradigm ( Guo et al , 2020 ; Hua et al , 2020a , b ) . Briefly , the programmer learns to map each question into a program , i.e. a sequence of neural modules , and the interpreter then " executes " the program , operationalized by modules , on the paragraph to yield the answer for different types of complex questions . NMNs achieves the best performance on a subset of the challenging DROP dataset ( Dua et al , 2019 ) and is interpertable by nature . However , NMNs ' performance advantage is not consistent , as it underperforms in some types of questions that require numerical reasoning . For instance , for date - compare questions , MTMSN ( Hu et al , 2019 ) achieves an F1 score of 85.2 1 , whereas NMNs ' performance is 82.6 . Similarly , for count questions , the F1 score is 61.6 for MTMSN and 55.7 for NMNs . This performance gap stems from two deficiencies of NMNs , which we describe below with the help of two examples in Figure 1 . Firstly , NMNs ' interpreter is oblivious to the question when executing number - related modules . For executing number - related modules , the interpreter only receives the paragraph as input , but not the question . Such a lack of direct interactions with the question impairs model performance : the entities in the question , which may also occur in the paragraph , can help locate significant and relevant numbers to produce the final answer . In the first example in Figure 1 , if the interpreter is aware of the correct event mentioned in the question ( i.e. " the Constituent Assembly being elected " ) , it can easily find the same event in the paragraph and further locate its date ( " 12 November " ) precisely . Without this knowledge , the original NMNs found the wrong event ( i.e. " dissolved the Constituent Assembly " ) , thus the wrong date ( " January 1918 " ) , leading to an incorrect answer . Secondly , NMNs disregards the relative positioning of entities and their related numbers in the paragraph . Although NMNs can learn separate distributions over numbers extracted from a paragraph , it does not have an effective mechanism to identify the number that connects to a given entity . Such an ability to recognise the association among numbers and entities is vital for learning numerical reasoning skills : the operation between numbers is meaningful only when they refer to the same entity or the same type of entities . The second example in Figure 1 illustrates the positioning of entities and their related numbers . With only a constraint on a window around an entity , the NMNs ' interpreter tends to identify the nearest number as the related one to a given entity ( " August 1996 to December 1997 " for entity " PUK and KDP later co - operated " ) , resulting in wrong predictions . Figure 1 : Two examples in the DROP ( Dua et al , 2019 ) dataset that demonstrate the deficienties of NMNs . Tokens pertinent to our discussion are highlighted in red , and their relevant numbers are highlighted in orange . Solid blue lines are predictions of our model , while dotted blue lines show the predictions of NMNs . We propose three simple and effective mechanisms to improve NMNs ' numerical reasoning capabilities . Firstly , we improve the interpreter to make it questionaware . By explicitly conditioning the execution on the question , the interpreter can exploit the information contained in the question . Secondly , we propose an intuitive constraint to better relate numbers and their corresponding entities in the paragraph . Finally , we strengthen the auxiliary loss to increase attention values of entities in closer vicinity within a sentence . Experimental results show that our modifications significantly improve NMNs ' numerical reasoning performance by up to 3.0 absolute F1 points . With minor modification , these mechanisms are simple enough to be applied to other modular approaches .
Complex Question Answering focuses on questions that require capabilities beyond multi - hop reasoning . These capabilities include numerical , logical and discrete reasoning . A number of neural models were recently proposed to address the CQA task , such as BiDAF ( Seo et al , 2017 ) , QANet ( Yu et al , 2018 ) , NMNs ( Gupta et al , 2020 ) and NumNet ( Ran et al , 2019 ) , which achieved high performance on benchmark datasets such as DROP ( Dua et al , 2019 ) . Numerical Reasoning is an essential capability for the CQA task , which is a challenging problem since the numbers and computation procedures are separately extracted and generated from raw text . Dua et al ( 2019 ) modified the output layer of QANet ( Yu et al , 2018 ) and proposed a number - aware model NAQANet that can deal with numerical questions for which the answer can not be directly extracted from the paragraph . In addition to NAQANet , NumNet ( Ran et al , 2019 ) leveraged Graph Neural Network ( GNN ) to design a number - aware deep learning model . Also leveraging GNN , Chen et al ( 2020a ) distinguished number types more precisely by adding the connection with entities and obtained better performance . Chen et al ( 2020b ) searched possible programs exhaustively based on answer numbers and employed these programs as weak supervision to train the whole model . Using dependency parsing of questions , Saha et al ( 2021 ) focused on the numerical part and obtained excellent results on different kinds of numerical reasoning questions . Neural Module Networks ( NMNs ) ( Gupta et al , 2020 ) adopts the programmer - interpreter paradigm and is a fully end - to - end differentiable model , in which the programmer ( responsible for composing programs ) and the interpreter ( responsible for soft execution ) are jointly learned . Specialised modules , such as find and find - num , are predefined to perform different types of reasoning over text and numbers . Compared with those techniques that employ GNNs ( Ran et al , 2019 ; Yu et al , 2018 ) , NMNs is highly interpretable while achieving competitive performance . More details can be found in Appendix A.
In this section , we will discuss the deficiencies of NMNs described in Section 1 and propose three techniques to overcome these problems . Considering the importance of questions while executing programs , we incorporate a question - to - paragraph alignment matrix to form a question - aware interpreter in Section 3.1 . In Section 3.2 , the correspondence between numbers and their related entities is enhanced with a simple and effective constraint on number - related modules . In Section 3.3 , we strengthen the auxiliary loss function in NMNs to further concentrate attention in the same sentence .
The interpreter in the NMNs framework is responsible for executing specialised modules given the context ( i.e. paragraph ) . For number - related modules such as " find - num " , the question is not taken into account , which limits NMNs ' performance on numerical reasoning , as information in the question is not taken into account . As an example , let us take a clear look at the " find - num " module in NMNs . find - num ( P ) T 2 . This module takes as input the distribution over paragraph tokens , and produces output an distribution over the numbers : S n ij = P i T W n P n j , ( 1 ) A n i = softmax ( S n i ) , ( 2 ) T = i P i A n i , ( 3 ) where input P and output T are distributions over paragraph tokens and numbers respectively , P is the paragraph token representations , i is the index of the i th paragraph token , n j is the index of the j th number token , and W n is a learnable matrix . Note that when computing the similarity matrix between the paragraph token P i and the number token P n j in Equation 1 , there is no interaction with the question . When the correct number types or related entities can be easily found in the question , incorporating the question in " find - num " can help narrow down the search of numbers in the paragraph . The first example in Figure 1 shows that the NMNs fails to locate the correct number as the wrong event is recognized , without interacting with the question . Inspired by this idea , we propose the question - toparagraph alignment modification to number - related modules . Specifically , the definition of " find - num " is modified as follows : find - num ( P , Q ) T n , where the additional input Q obtained from the programmer represents the distribution over question tokens , and the new output is represented by T n . Additional computational steps ( Equation 4 to 7 below ) are added after Equation 3 : S n kj = Q k T W n P n j , ( 4 ) A n k = softmax ( S n k ) , ( 5 ) T = k Q k A n k , ( 6 ) T n = λ T + ( 1−λ ) T , ( 7 ) where Q is the question token representations and k is the index of the k th question token . As can be seen from the above equations , the input of the improved " find - num " module is extended to include not only paragraph but also question token distributions instead of only the paragraph . More precisely , T is another alignment matrix between all question tokens and number tokens , using the same form of Bi - linear attention computation as T . Finally , the new distribution T n is produced by the weighted sum of T and T with an additional hyperparameters λ . Here we fix λ=0.5 so that NMNs treats the paragraph and the question equally . Other numberrelated modules are also revised in a similar way , e.g. " find - date " , " compare - num - lt - than " , " find - max - num " .
It is highly likely for a paragraph to contain multiple numbers and entities , as shown in Figure 1 . For such paragraphs , the original NMNs allows all numbers to interact with all entities in the computation of number - related modules such as " find - num " . This is detrimental to performance as , intuitively , a number far away from an entity is less likely to be related to the entity . As the second example in Figure 1 shows , NMNs connects " December 1997 " to the entity " PUK and KDP " since " 2003 " is far away from it , resulting in wrong predictions eventually . To tackle this issue , we add another computational component , the relation matrix U n , into numberrelated modules . Taking the " find - num " module as an example , the following step is added before Equation 2 when computing S n ij : S n ij = U n ij S n ij , ( 8 ) where is element - wise multiplication . In the above equation , the value of S n ij is updated with the relation matrix U n , which constrains the relationship between the i th paragraph token and j th number token . More specifically , let s t be the token index set for the t th sentence in the paragraph . Thus , if both the i th paragraph token and the j th number token belong to the same sentence , element U n ij , in row i and column j , is set to 1 , otherwise 0 : U n ij = 1 , ( i s t ) ( n j s t ) 0 , otherwise ( 9 ) By adding this matrix , the module only keeps the attention values of tokens in close vicinity within a sentence , and learns to find the related numbers that directly interact with entities . Similarly , this relation matrix U n is also applied to other number - related modules to improve performance .
Gupta et al ( 2020 ) employed an auxiliary loss to constrain the relative positioning of output tokens with respect to input tokens in the " find - num " , " find - date " and " relocate " modules . For instance , the auxiliary loss for the " find - num " module is as follows : H n loss = − m i=1 log ( Nt j=0 1 n j [ i±W ] A n ij ) , ( 10 ) where A n ij is from Equation 2 . The loss enables the model to concentrate the attention mass of output tokens within a window of size W ( e.g. W = 10 ) . However , these loss functions still allow irrelevant numbers to have spuriously high attention values . Taking the second line in Figure 1 as an example , based on the loss computation procedures , the number " December 1997 " will be also " found " and connected to the entity " PUK and KDP " in NMNs . Obviously , this irrelevant year information should not be taken into consideration . Therefore , we propose to strengthen the auxiliary loss to further concentrate attention mass to those tokens within the same sentence : H n loss = − m i=1 log ( Nt j=0 1 ( n j st ) ( i st ) A n ij ) , ( 11 ) where the s t is the token index set for the t th sentence in the paragraph . In this way , the year " 2003 " is the only consideration for the previous example .
We evaluate model performance on the same subset of the DROP dataset used by the original NMNs ( Gupta et al , 2020 ) , which contains approx . 19 , 500 QA pairs for training , 440 for validation and 1 , 700 for testing . The training procedures and hyper - parameter settings are the same as the original NMNs ( Gupta et al , 2020 ) . We report F1 and Exact Match ( EM ) scores following the literature ( Dua et al , 2019 ; Gupta et al , 2020 ) .
Table 1 shows the main results , where " original " represents the performance of the original NMNs ( Gupta et al , 2020 ) . Row 4 , " + qai+nepc+aux " , is our full model , which includes the question - aware interpreter ( + qai ) , the number - entity positional constraint ( + nepc ) , and the improved auxiliary loss ( + aux ) . It can be observed that compared to " original " , our full model achieves significantly higher performance with F1 of 80.4 and EM of 76.6 , representing an increase of 3.0 and 2.6 absolute points respectively . Besides , our significant test shows p≤0.01 .
original ( Gupta et al , 2020 ) We also conduct an ablation study to discuss the contribution of individual technique . The second line , " + qai " , is the results with the question - aware interpreter employed only . For this variant , the F1 and EM scores improve on the original baseline by 1.6 and 0.9 points respectively . With the addition of the number - entity positional constraint , " + nepc " , results show an improvement of 2.5 and 2.0 points for F1 and EM when comparing with " original " . These results show that all of the three techniques are effective in improving numerical reasoning skills for NMNs . We also report performance by subsets of different question types in Table 2 . Except for the numbercompare type , our model improves on the original NMNs across all other types of questions significantly , by at least 3.2 absolute points for F1 . In addition , our model outperforms aforementioned MTMSN ( Hu et al , 2019 )
Neural Moudule Networks ( NMNs ) represent an interpretable state - of - the - art approach to complex question answering over text . In this paper , we further improve NMNs ' numerical reasoning capabilities , by making the interpreter question - aware and placing stronger constraints on the relative positioning of entities and their related numbers . Experimental results show that our approach significantly improves NMNs ' numerical reasoning ability , with an increase in F1 of 3.0 absolute points .
In order to solve the complex question answering problem , Gupta et al ( 2020 ) proposed a Neural Module Networks ( NMNs ) model . Consisting of a programmer and an interpreter , NMNs can be more interpretable as shown in Figure 2 . As Figure 2 shows , NMNs takes the question and the paragraph as inputs . The programmer firstly maps the question into corresponding " discrete " modules in order . Then , the interpreter executes these generated modules against the corresponding paragraph to produce the final answer . Moreover , all modules are differentiable so that the whole NMNs can be trained in an end - to - end way . For hyper - parameters in our model , we do n't conduct experiments on their search trials since we employ the same settings as Gupta et al ( 2020 ) did , which can be found in Table 3 . Note that they are also the configuration to obtain the best performance . For the added parameter λ in Equation 7 , we leverage an empirical value λ=0.5 without any fine - tuning .
Due to the page limitation , we did n't include more baselines , such as NAQANet ( Dua et al , 2019 ) . After running on the same split of DROP dataset , the F1 and EM scores by NAQANet are 62.1 % and 57.9 % respectively , which are substantially lower than our results in Table 1 , by over 17 % for both scores . And we did apply these components in Section 3 to other modules , such as the " extract - argument " module ( extracts spans or tokens from paragraphs ) , and also obtained better results ( 0.5 % F1 increase ) . Besides , for different question types , their statistics on the test set can be found in Table 4 . Current NMNs ( Gupta et al , 2020 ) does not support other arithmetic datasets , since some arithmetic operations , including addition , are not supported . Extending related arithmetic modules is one of our future work , based on which the NMNs could be trained on other datsets .
This research was supported in part by the Future Fellowship FT190100039 from the Australian Research Council . The computational resources for this work were provided by the Multi - modal Australian Sci - enceS Imaging and Visualisation Environment ( MAS - SIVE ) ( www.massive.org.au ) . We would like to thank the anonymous reviewers for their useful comments to improve the manuscript .
