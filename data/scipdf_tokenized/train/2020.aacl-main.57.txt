STIL - Simultaneous Slot Filling , Translation , Intent Classification , and Language Identification : Initial Results using mBART on MultiATIS++
Slot - filling , Translation , Intent classification , and Language identification , or STIL , is a newly - proposed task for multilingual Natural Language Understanding ( NLU ) . By performing simultaneous slot filling and translation into a single output language ( English in this case ) , some portion of downstream system components can be monolingual , reducing development and maintenance cost . Results are given using the multilingual BART model ( Liu et al , 2020 ) fine - tuned on 7 languages using the MultiATIS++ dataset . When no translation is performed , mBART 's performance is comparable to the current state of the art system ( Cross - Lingual BERT by Xu et al ( 2020 ) ) for the languages tested , with better average intent classification accuracy ( 96.07 % versus 95.50 % ) but worse average slot F1 ( 89.87 % versus 90.81 % ) . When simultaneous translation is performed , average intent classification accuracy degrades by only 1.7 % relative and average slot F1 degrades by only 1.2 % relative .
Multilingual Natural Language Understanding ( NLU ) , also called cross - lingual NLU , is a technique by which an NLU - based system can scale to multiple languages . A single model is trained on more than one language , and it can accept input from more than one language during inference . In most recent high - performing systems , a model is first pre - trained using unlabeled data for all supported languages and then fine tuned for a specific task using a small set of labeled data ( Conneau and Lample , 2019 ; Pires et al , 2019 ) . Two typical tasks for goal - based systems , such as virtual assistants and chatbots , are intent classification and slot filling ( Gupta et al , 2006 ) . Though intent classification creates a language agnostic output ( the intent of the user ) , slot filling does not . Instead , a slot - filling model outputs the labels for each of input tokens from the user . Suppose the slot - filling model can handle L languages . Downstream components must therefore handle all L languages for the full system to be multilingual across L languages . Machine translation could be performed before the slot filling model at system runtime , though the latency would be fully additive , and some amount of information useful to the slotfilling model may be lost . Similarly , translation could occur after the slot - filling model at runtime , but slot alignment between the source and target language is a non - trivial task ( Jain et al , 2019 ; Xu et al , 2020 ) . Instead , the goal of this work was to build a single model that can simultaneously translate the input , output slotted text in a single language ( English ) , classify the intent , and classify the input language ( See Table 1 ) . The STIL task is defined such that the input language tag is not given to the model as input . Thus , language identification is necessary so that the system can communicate back to the user in the correct language . In all STIL cases , the output is in English . Each token is followed by its BIO - tagged slot label . The sequence of tokens and slots are followed by the intent and then the language . sification , and Language identification ( STIL ) ; ( 2 ) both non - translated and STIL results using the mBART model ( Liu et al , 2020 ) trained using a fully text - to - text data format ; and ( 3 ) public release of source code used in this study , with a goal toward reproducibility and future work on the STIL task 1 .
The Airline Travel Information System ( ATIS ) dataset is a classic benchmark for goal - oriented NLU ( Price , 1990 ; Tur et al , 2010 ) . It contains utterances focused on airline travel , such as how much is the cheapest flight from Boston to New York tomorrow morning ? The dataset is annotated with 17 intents , though the distribution is skewed , with 70 % of intents being the flight intent . Slots are labeled using the Beginning Inside Outside ( BIO ) format . ATIS was localized to Turkish and Hindi in 2018 , forming MultiATIS ( Upadhyay et al , 2018 ) , and then to Spanish , Portuguese , German , French , Chinese , and Japanese in 2020 , forming Multi - ATIS++ ( Xu et al , 2020 ) . In this work , Portuguese was excluded due to a lack of Portuguese pretraining in the publicly available mBART model , and Japanese was excluded due to a current lack of alignment between Japanese and English samples in MultiATIS++ . Hindi and Turkish data were taken from Multi - ATIS , and the training data were upsampled by 3x for Hindi and 7x for Turkish . Prior to any upsampling , there were 4 , 488 training samples for English , Spanish , German , French , and Chinese . The test sets contained 893 samples for all languages except Turkish , which had 715 samples . For English , Spanish , German , French , and Chinese , validation sets of 490 samples were used in all cases . Given the smaller data quantities for Hindi and Turkish , two training and validation set configurations were considered . The first configuration
Previous approaches for intent classification and slot filling have used either ( 1 ) separate models for slot filling , including support vector machines ( Moschitti et al , 2007 ) , conditional random fields ( Xu and Sarikaya , 2014 ) , and recurrent neural networks of various types ( Kurata et al , 2016 ) or ( 2 ) joint models that diverge into separate decoders or layers for intent classification and slot filling ( Xu and Sarikaya , 2013 ; Guo et al , 2014 ; Liu and Lane , 2016 ; Hakkani - Tür et al , 2016 ) or that share hidden states ( Wang et al , 2018 ) . In this work , a fully text - to - text approach similar to that of the T5 model was used , such that the model would have maximum information sharing across the four STIL sub - tasks . Encoder - decoder models , first introduced in 2014 ( Sutskever et al , 2014 ) , are a mainstay of neural machine translation . The original transformer model included both an encoder and a decoder ( Vaswani et al , 2017 ) . Since then , much of the work on transformers focuses on models with only an encoder pretrained with autoencoding techniques ( e.g. BERT by Devlin et al ( 2018 ) ) or auto - regressive models with only a decoder ( e.g. GPT by Radford ( 2018 ) ) . In this work , it was assumed that encoder - decoder models , such as BART ( Lewis et al , 2019 ) and T5 ( Raffel et al , 2019 ) , are the best architectural candidates given the translation component of the STIL task , as well as past state of the art advancement by encoder - decoder models on ATIS , cited above . Rigorous architectural comparisons are left to future work .
The multilingual BART ( mBART ) model architecture was used ( Liu et al , 2020 ) , as well as the pretrained mBART.cc25 model described in the same paper . The model consists of 12 encoder layers , 12 decoder layers , a hidden layer size of 1 , 024 , and 16 attention heads , yielding a parameter count of 680M. The mBART.cc25 model was trained on 25 languages for 500k steps using a 1.4 TB corpus of scraped website data taken from Common Crawl ( Wenzek et al , 2019 ) . The model was trained to reconstruct masked tokens and to rearrange scrambled sentences . SentencePiece tokenization ( Kudo and Richardson , 2018 ) was used for mBART.cc25 with a sub - word vocabulary size of 250k .
The same vocabulary as that of the pretrained model was used for this work , and SentencePiece tokenization was performed on the full sequence , including the slot tags , intent tags , and language tags . For all mBART experiments and datasets , data from all languages were shuffled together . The fairseq library was used for all experimentation ( Ott et al , 2019 ) . Training was performed on 8 Nvidia V100 GPUs ( 16 GB ) using a batch size of 32 , layer normalization for both the encoder and the decoder ( Xu et al , 2019 ) ; label smoothed cross entropy with = 0.2 ( Szegedy et al , 2016 ) ; the ADAM optimizer with β 1 = 0.9 and β 2 = 0.999 ( Kingma and Ba , 2014 ) ; an initial learning rate of 3 × 10 −5 with polynomial decay over 20 , 000 updates after 1 epoch of warmup ; attention dropout of 0.1 and dropout of 0.2 elsewhere ; and FP16 type for weights . Each model was trained for 19 epochs , which took 5 - 6 hours .
Results from the models are given in Table 3 . Statistical significance was evaluated using the Wilson method ( Wilson , 1927 ) with 95 % confidence . Xu et al ( 2020 ) Examining the first training configuration ( 1 , 496 samples for Hindi and 626 for Turkish ) , the nontranslated mBART 's macro - averaged intent classification ( 96.07 % ) outperforms Cross - Lingual BERT by Xu et al ( 2020 ) ( 95.50 % ) , but slot F1 is worse ( 89.87 % for non - translated mBART and 90.81 % for Cross - Lingual BERT ) . The differences are statistically significant in both cases .
When translation is performed ( the STIL task ) , intent classification accuracy degrades by 1.7 % relative from 96.07 % to 94.40 % , and slot F1 degrades by 1.2 % relative from 89.87 % to 88.79 % . The greatest degradation occurred for utterances involving flight number , airfare , and airport name ( in that order ) .
Adding 105 more Hindi and 12 more Turkish training examples results in improved performance for the translated , STIL mBART model . Macro - averaged intent classification improves from 94.40 % to 95.94 % , and slot F1 improves from 88.79 % to 90.10 % , both of which are statistically significant . By adding these 117 samples , the STIL mBART model matches the performance ( within confidence intervals ) of the non - translated mBART model . This finding suggests that the STIL models may require more training data than traditional , non - translated slot filling models . Additionally , by adding more Hindi and Turkish data , both the intent accuracy and the slot filling F1 improves for every individual language of the translated , STIL models , suggesting that some portion of the internal , learned representation is language agnostic . Finally , the results suggest that there is a trainingsize - dependent performance advantage in using a single output language , as contrasted with the nontranslated mBART model , for which the intent classification accuracy and slot F1 does not improve ( with statistical significance ) when using the additional Hindi and Turkish training samples .
Language identification F1 is above 99.7 % for all languages , with perfect performance in many cases . ( Qin et al , 2019 ) 97.5 Joint BERT + CRF ( Chen et al , 2019 ) 97.9 Non - translated mBART , with Perfect performance on Chinese and Hindi is unsurprising given their unique scripts versus the other languages tested .
This preliminary work demonstrates that a single NLU model can perform simultaneous slot filling , translation , intent classification , and language identification across 7 languages using MultiATIS++ . Such an NLU model would negate the need for multiple - language support in some portion of downstream system components . Performance is not irreconcilably worse than traditional slot - filling models , and performance is statistically equivalent with a small amount of additional training data . Looking forward , a more challenging dataset is needed to further develop the translation compo - nent of the STIL task . The English MultiATIS++ test set only contains 455 unique entity - slot pairs . An ideal future dataset would include freeform and varied content , such as text messages , song titles , or open - domain questions . Until then , work remains to achieve parity with English - only ATIS models .
The author would like to thank Saleh Soltan , Gokhan Tur , Saab Mansour , and Batool Haider for reviewing this work and providing valuable feedback .
