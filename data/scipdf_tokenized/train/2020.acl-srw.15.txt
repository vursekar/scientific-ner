Neural machine translation ( NMT ) has achieved impressive performance recently by using large - scale parallel corpora . However , it struggles in the low - resource and morphologically - rich scenarios of agglutinative language translation task . Inspired by the finding that monolingual data can greatly improve the NMT performance , we propose a multi - task neural model that jointly learns to perform bi - directional translation and agglutinative language stemming . Our approach employs the shared encoder and decoder to train a single model without changing the standard NMT architecture but instead adding a token before each source - side sentence to specify the desired target outputs of the two different tasks . Experimental results on Turkish - English and Uyghur - Chinese show that our proposed approach can significantly improve the translation performance on agglutinative languages by using a small amount of monolingual data .
Neural machine translation ( NMT ) has achieved impressive performance on many high - resource machine translation tasks ( Bahdanau et al , 2015 ; Luong et al , 2015a ; Vaswani et al , 2017 ) . The standard NMT model uses the encoder to map the source sentence to a continuous representation vector , and then it feeds the resulting vector to the decoder to produce the target sentence . However , the NMT model still suffers from the low - resource and morphologically - rich scenarios of agglutinative language translation tasks , such as Turkish - English and Uyghur - Chinese . Both Turkish and Uyghur are agglutinative languages with complex morphology . The morpheme structure of the word can be denoted as : prefix1 + … + prefixN + stem + suffix1 + … + suffixN ( Ablimit et al , 2010 ) . Since the suffixes have many inflected and morphological variants , the vocabulary size of an agglutinative language is considerable even in small - scale training data . Moreover , many words have different morphemes and meanings in different context , which leads to inaccurate translation results . Recently , researchers show their great interest in utilizing monolingual data to further improve the NMT model performance ( Cheng et al , 2016 ; Ramachandran et al , 2017 ; Currey et al , 2017 ) . Sennrich et al ( 2016 ) pair the target - side monolingual data with automatic back - translation as additional training data to train the NMT model . Zhang and Zong ( 2016 ) use the source - side monolingual data and employ the multi - task learning framework for translation and source sentence reordering . modify the decoder to enable multi - task learning for translation and language modeling . However , the above works mainly focus on boosting the translation fluency , and lack the consideration of morphological and linguistic knowledge . Stemming is a morphological analysis method , which is widely used for information retrieval tasks ( Kishida , 2005 ) . By removing the suffixes in the word , stemming allows the variants of the same word to share representations and reduces data sparseness . We consider that stemming can lead to better generalization on agglutinative languages , which helps NMT to capture the in - depth semantic information . Thus we use stemming as an auxiliary task for agglutinative language translation . In this paper , we investigate a method to exploit the monolingual data of the agglutinative language to enhance the representation ability of the encoder . This is achieved by training a multi - task neural model to jointly perform bi - directional translation and agglutinative language stemming , which utilizes the shared encoder and decoder . We treat stemming as a sequence generation task . Figure 1 : The architecture of the multi - task neural model that jointly learns to perform bi - directional translation between Turkish and English , and stemming for Turkish sentence .
Multi - task learning ( MTL ) aims to improve the generalization performance of a main task by using the other related tasks , which has been successfully applied to various research fields ranging from language ( Liu et al , 2015 ; Luong et al , 2015a ) , vision ( Yim et al , 2015 ; Misra et al , 2016 ) , and speech ( Chen and Mak , 2015 ; Kim et al , 2016 ) . Many natural language processing ( NLP ) tasks have been chosen as auxiliary task to deal with the increasingly complex tasks . Luong et al ( 2015b ) employ a small amount of data of syntactic parsing and image caption for English - German translation . Hashimoto et al ( 2017 ) present a joint MTL model to handle the tasks of part - of - speech ( POS ) tagging , dependency parsing , semantic relatedness , and textual entailment for English . Kiperwasser and Ballesteros ( 2018 ) utilize the POS tagging and dependency parsing for English - German machine translation . To the best of our knowledge , we are the first to incorporate stemming task into MTL framework to further improve the translation performance on agglutinative languages . Recently , several works have combined the MTL method with sequence - to - sequence NMT model for machine translation tasks . Dong et al ( 2015 ) follow a one - to - many setting that utilizes a shared encoder for all the source languages with respective attention mechanisms and multiple decoders for the different target languages . Luong et al ( 2015b ) follow a many - to - many setting that uses multiple encoders and decoders with two separate unsupervised objective functions . Zoph and Knight ( 2016 ) follow a many - to - one setting that employs multiple encoders for all the source languages and one decoder for the desired target language . Johnson et al ( 2017 ) propose a more simple method in one - to - one setting , which trains a single NMT model with the shared encoder and decoder in order to enable multilingual translation . The method requires no changes to the standard NMT architecture but instead requires adding a token at the beginning of each source sentence to specify the desired target sentence . Inspired by their work , we employ the standard NMT model with one encoder and one decoder for parameter sharing and model generalization . In addition , we build a joint vocabulary on the concatenation of the source - side and target - side words . Several works on morphologically - rich NMT have focused on using morphological analysis to pre - process the training data ( Luong et al , 2016 ; Huck et al , 2017 ; Tawfik et al , 2019 ) . Gulcehre et al ( 2015 ) segment each Turkish sentence into a sequence of morpheme units and remove any nonsurface morphemes for Turkish - English translation . Ataman et al ( 2017 ) propose a vocabulary reduction method that considers the morphological properties of the agglutinative language , which is based on the unsupervised morphology learning . This work takes inspiration from our previously proposed segmentation method ( Pan et al , 2020 ) that segments the word into a sequence of subword units with morpheme structure , which can effectively reduce language complexity .
We propose a multi - task neural model for machine translation from and into a low - resource and morphologically - rich agglutinative language . We train the model to jointly learn to perform both the bi - directional translation task and the stemming task on an agglutinative language by using the standard NMT framework . Moreover , we add an artificial token before each source sentence to specify the desired target outputs for different tasks . The architecture of the proposed model is shown in Figure 1 . We take the Turkish - English translation task as example . The " < MT > " token denotes the bilingual translation task and the " < ST > " token denotes the stemming task on Turkish sentence .
Our proposed multi - task neural model on using the source - side monolingual data for agglutinative language translation task can be applied in any NMT structures with encoder - decoder framework . In this work , we follow the NMT model proposed by Vaswani et al ( 2017 ) , which is implemented as Transformer . We will briefly summarize it here . Firstly , the Transformer model maps the source sequence = ( 1 , … , ) and the target sentence = ( 1 , … , ) into a word embedding matrix , respectively . Secondly , in order to make use of the word order in the sequence , the above word embedding matrices sum with their positional encoding matrices to generate the source - side and target - side positional embedding matrices . The encoder is composed of a stack of N identical layers . Each layer has two sub - layers consisting of the multi - head self - attention and the fully connected feed - forward network , which maps the source - side positional embedding matrix into a representation vector . The decoder is also composed of a stack of N identical layers . Each layer has three sub - layers : the multi - head self - attention , the multi - head attention , and the fully connected feed - forward network . The multi - head attention attends to the outputs of the encoder and decoder to generate a context vector . The feed - forward network followed by a linear layer maps the context vector into a vector with the original space dimension . Finally , the softmax function is applied on the vector to predict the target word sequence .
The statistics of the training , validation , and test datasets on Turkish - English and Uyghur - Chinese machine translation tasks are shown in Table 1 . For the Turkish - English machine translation , following ( Sennrich et al , 2015a ) , we use the WIT corpus ( Cettolo et al , 2012 ) and the SETimes corpus ( Tyers and Alperen , 2010 ) as the training dataset , merge the dev2010 and tst2010 as the validation dataset , and use tst2011 , tst2012 , tst2013 , tst2014 from the IWSLT as the test datasets . We also use the talks data from the IWSLT evaluation campaign 1 in 2018 and the news data from News Crawl corpora 2 in 2017 as external monolingual data for the stemming task on Turkish sentences . For the Uyghur - Chinese machine translation , we use the news data from the China Workshop on Machine Translation in 2017 ( CWMT2017 ) as the training dataset and validation dataset , use the news data from CWMT2015 as the test dataset . Each Uyghur sentence has four Chinese reference sentences . Moreover , we use the news data from the Tianshan website 3 as external monolingual data for the stemming task on Uyghur sentences .
We normalize and tokenize the experimental data . We utilize the jieba toolkit 4 to segment the Chinese sentences , we utilize the Zemberek toolkit 5 with morphological disambiguation ( Sak et al , 2007 ) and the morphological analysis tool ( Tursun et al , 2016 ) to annotate the morpheme structure of the words in Turkish and Uyghur , respectively . We use our previously proposed morphological segmentation method ( Pan et al , 2020 ) , which segments the word into smaller subword units with morpheme structure . Since Turkish and Uyghur only have a few prefixes , we combine the prefixes with stem into the stem unit . As shown in Figure 2 , the morpheme structure of the Turkish word " hecelerini " ( syllables ) is : hece + lerini . Then the byte pair encoding ( BPE ) technique ( Sennrich et al , 2015b ) is applied on the stem unit " hece " to segment it into " he@@ " and " ce@@ " . Thus the Turkish word is segmented into a sequence of subword units : he@@ + ce@@ + lerini .
Training Sentence Samples En - Tr Translation < MT > We go through initiation rit@@ es . Başla@@ ma ritüel@@ lerini yaş@@ ıyoruz . Tr - En Translation < MT > Başla@@ ma ritüel@@ lerini yaş@@ ıyoruz . We go through initiation rit@@ es . Turkish Stemming < ST > Başla@@ ma ritüel@@ lerini yaş@@ ıyoruz . Başla@@ ritüel@@ yaş@@ In this paper , we utilize the above morphological segmentation method for our experiments by applying BPE on the stem units with 15 K merge operations for the Turkish words and 10 K merge operations for the Uyghur words . The standard NMT model trained on this experimental data is denoted as " baseline NMT model " . Moreover , we employ BPE to segment the words in English and Chinese by learning separate vocabulary with 32 K merge operations . Table 2 shows the training sentence samples for multi - task neural model on Turkish - English machine translation task . In addition , to certify the effectiveness of the morphological segmentation method , we employ the pure BPE to segment the words in Turkish and Uyghur by learning a separate vocabulary with 36 K and 38 K merge operations , respectively . The standard NMT model trained on this experimental data is denoted as " general NMT model " . Table 3 shows the detailed statistics of using different word segmentation methods on Turkish , English , Uyghur , and Chinese . The " Vocab " token denotes the vocabulary size after data preprocessing . The " Avg . Len " token denotes the average sentence length .
We employ the Transformer model implemented in the Sockeye toolkit . The number of layer in both the encoder and decoder is set to N=6 , the number of head is set to 8 , and the number of hidden unit in the feed - forward network is set to 1024 . We use an embedding size of both the source and target words of 512 dimension , and use a batch size of 128 sentences . The maximum sentence length is set to 100 tokens with 0.1 label smoothing . We apply layer normalization and add dropout to the embedding and transformer layers with 0.1 probability . Moreover , we use the Adam optimizer ( Kingma and Ba , 2015 ) with an initial learning rate of 0.0002 , and save the checkpoint every 1500 updates . Model training process stops after 8 checkpoints without improvements on the validation perplexity . Following Niu et al ( 2018a ) , we select the 4 best checkpoint based on the validation perplexity values and combine them in a linear ensemble for decoding . Decoding is performed by using beam search with a beam size of 5 . We evaluate the machine translation performance by using the case - sensitive BLEU score ( Papineni et al , 2002 ) with standard tokenization .
In this paper , we select 4 neural translation models for comparison . More details about the models are shown below : General NMT Model : The standard NMT model trained on the experimental data segmented by BPE . Baseline NMT Model : The standard NMT model trained on the experimental data segmented by morphological segmentation . The following models also use this word segmentation method . Bi - Directional NMT Model : Following Niu et al ( 2018b ) , we train a single NMT model to perform bi - directional machine translation . We concatenate the bilingual parallel sentences in both directions . Since the source and target sentences come from the same language pairs , we share the source and target vocabulary , and tie their word embedding during model training . Multi - Task Neural Model : We simply use the monolingual data of the agglutinative language from the bilingual parallel sentences . We use a joint vocabulary , tie the word embedding as well as the output layer 's weight matrix .
Table 4 shows the BLEU scores of the general NMT model and baseline NMT model on machine translation task . We can observe that the baseline NMT model is comparable to the general NMT model , and it achieves the highest BLEU scores on almost all the test datasets in both directions , which indicates that the NMT baseline based on our proposed segmentation method is competitive .
Table 5 shows the BLEU scores of the baseline NMT model , bi - directional NMT model , and multi - task neural model on the machine translation task between Turkish and English . The table shows that the multi - task neural model outperforms both the baseline NMT model and bi - directional NMT model , and it achieves the highest BLEU scores on almost all the test datasets in both directions , which suggests that the multi - task neural model is capable of improving the bi - directional translation quality on agglutinative languages . The main reason is that compared with the bi - directional NMT model , our proposed multi - task neural model additionally employs the stemming task for the agglutinative language , which is effective for the NMT model to learn both the source - side semantic information and the target - side language modeling .
The university was emulating its lives . multi - task The university was imitating life . The function of epochs and perplexity values on the validation dataset in different neural translation models are shown in Figure 3 . We can see that the perplexity values are consistently lower on the multi - task neural model , and it converges rapidly . Table 6 shows a translation example for the different models on Turkish - English . We can see that the translation result of the multi - task neural model is more accurate . The Turkish word " taklit " means " imitate " in English , both the baseline NMT and bi - directional NMT translate it into a synonym " emulate " . However , they are not able to express the meaning of the sentence correctly . The main reason is that the auxiliary task of stemming forces the proposed model to focus more strongly on the core meaning of each word ( or stem ) , therefore helping the model make the correct lexical choices and capture the in - depth semantic information .
Moreover , we evaluate the multi - task neural model on using external monolingual data for Turkish stemming task . We employ the parallel sentences and the monolingual data in a 1 - 1 ratio , and shuffle them randomly before each training epoch . More details about the data are shown below : Table 7 shows the BLEU scores of the proposed multi - task neural model on using different external monolingual data . We can see that there is no obvious difference on Turkish - English translation performance by using different monolingual data , whether the data is in - domain or out - of - domain to the test dataset . However , for the English - Turkish machine translation task , which can be seen as agglutinative language generation task , using the mixed data of talks and news achieves further improvements of the BLEU scores on almost all the test datasets . The main reason is that the proposed multi - task neural model incorporates many morphological and linguistic information of Turkish rather than that of English , which mainly pays attention to the source - side representation ability on agglutinative languages rather than the target - side language modeling . We also evaluate the translation performance of the general NMT model , baseline NMT model , and multi - task neural model with external news data on the machine translation task between Uyghur and Chinese . The experimental results are shown in Table 8 . The results indicate that the multi - task neural model achieves the highest BLEU scores on the test dataset by utilizing external monolingual data for the stemming task on Uyghur sentences .
In this paper , we propose a multi - task neural model for translation task from and into a low - resource and morphologically - rich agglutinative language . The model jointly learns to perform bi - directional translation and agglutinative language stemming by utilizing the shared encoder and decoder under standard NMT framework . Extensive experimental results show that the proposed model is beneficial for the agglutinative language machine translation , and only a small amount of the agglutinative data can improve the translation performance in both directions . Moreover , the proposed approach with external monolingual data is more useful for translating into the agglutinative language , which achieves an improvement of +1.42 BLEU points for translation from English into Turkish and +1.45 BLEU points from Chinese into Uyghur . In future work , we plan to utilize other word segmentation methods for model training . We also plan to combine the proposed multi - task neural model with back - translation method to enhance the ability of the NMT model on target - side language modeling .
We are very grateful to the mentor of this paper for her meaningful feedback . Thanks three anonymous reviewers for their insightful comments and practical suggestions .
