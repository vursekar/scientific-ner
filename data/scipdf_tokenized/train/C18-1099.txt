Adversarial Multi - lingual Neural Relation Extraction
Multi - lingual relation extraction aims to find unknown relational facts from text in various languages . Existing models can not well capture the consistency and diversity of relation patterns in different languages . To address these issues , we propose an adversarial multi - lingual neural relation extraction ( AMNRE ) model , which builds both consistent and individual representations for each sentence to consider the consistency and diversity among languages . Further , we adopt an adversarial training strategy to ensure those consistent sentence representations could effectively extract the language - consistent relation patterns . The experimental results on real - world datasets demonstrate that our AMNRE model significantly outperforms the state - of - the - art models . The source code of this paper can be obtained from https://github.com/thunlp/AMNRE .
Relation extraction ( RE ) is a crucial task in NLP , which aims to extract semantic relations between entity pairs from the sentences containing them . For example , given an entity pair ( Bill Gates , Microsoft ) and a sentence " Bill Gates is the co - founder and CEO of Microsoft " , we want to figure out the relation Founder between the two entities . RE can potentially benefit many applications , such as knowledge base construction ( Zhong et al , 2015 ; Han et al , 2018 ) and question answering ( Xiang et al , 2017 ) . Recently , neural models have shown their great abilities in RE . Zeng et al ( 2014 ) introduce a convolutional neural network ( CNN ) to extract relational facts with automatically learning features from text . To address the issue of lack of data , Zeng et al ( 2015 ) incorporate multi - instance learning with a piece - wise convolutional neural network ( PCNN ) to extract relations in distantly supervised data . Because distant supervision suffer from wrong labeling problems , Lin et al ( 2016 ) further employ a sentence - level selective attention to filter out those noisy sentences in distantly supervised data and achieve state - of - the - art performance . All these neural relation extraction ( NRE ) models merely focus on extracting relational facts from mono - lingual data , ignoring the rich information in multi - lingual data . propose a multi - lingual attention - based neural relation extraction ( MNRE ) model , which considers the consistency and complementarity in multi - lingual data . MNRE builds a sentence representation for each sentence in various languages and employs a multi - lingual attention to capture the pattern consistency and complementarity among languages . Although MNRE achieves great success in multi - lingual RE , it still has some problems . MNRE learns a single representation for each sentence in various languages , which can not well capture both the consistency and diversity of relation patterns in different languages . Moreover , MNRE simply utilizes a multi - lingual attention mechanism and a global relation predictor to capture the consistent relation patterns among multiple languages . From the experimental data , we find that the sentence representations in different languages are still far from each other and linearly separable . Therefore , it is hard for the multi - To address these issues , we propose an adversarial multi - lingual NRE ( AMNRE ) model . As shown in Figure 1 , for an entity pair , we encode its corresponding sentences in various languages through neural sentence encoders . For each sentence , we build an individual representation to grasp its individual language features and a consistent representation to encode its substantially consistent features among languages . Further , we adopt an adversarial training strategy to ensure AMNRE can extract the language - consistent relation patterns from the consistent representations . Orthogonality constraints are also adopted to enhance differences between individual representations and consistent representations for each language . x1 x 2 x1 x 1 1 E I 1 E I 1 E C 1 E C x 2 x2 x 1 x2 E I 2 E C 2 E C 2 E I 2 1 2 1 1 1 2 2 2 2 1 s1 s2 In experiments , we take Chinese and English to show the effectiveness of AMNRE . The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages . And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language - consistent relation patterns .
Traditional supervised RE models ( Zelenko et al , 2003 ; Socher et al , 2012 ; Santos et al , 2015 ) heavily rely on abundant amounts of high - quality annotated data . Hence , Mintz et al ( 2009 ) propose a distantly supervised model for RE . Distant supervision aligns knowledge bases ( KBs ) and text to automatically annotate data , and thus distantly supervised models inevitably suffer from wrong labeling problems . To alleviate the noise issue , Riedel et al ( 2010 ) and Hoffmann et al ( 2011 ) propose multi - instance learning ( MIL ) mechanisms for single - label and multi - label problems respectively . Then , Zeng et al ( 2015 ) attempt to integrate neural models into distant supervision . Lin et al ( 2016 ) further propose a sentence - level attention to jointly consider all sentences containing same entity pairs for RE . The attention - based neural relation extraction ( NRE ) model has become a foundation for some recent works ( Ji et al , 2017 ; Zeng et al , 2017 ; Liu et al , 2017b ; Wu et al , 2017 ; Feng et al , 2018 ; Zeng et al , 2018 ) . Most existing RE models are devoted to extracting relations from mono - lingual data and ignore information lying in text of multiple languages . Faruqui and Kumar ( 2015 ) and Verga et al ( 2016 ) first attempt to adopt multi - lingual transfer learning for RE . However , both of these works learn predictive models on a new language for existing KBs , without fully leveraging semantic information in text . Then , construct a multi - lingual NRE ( MNRE ) model to jointly represent text of multiple languages to enhance RE . In this paper , we propose a novel multi - lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces , which can achieve more effective representations for RE . et al ( 2015 ) propose adversarial training for image classification tasks . Afterwards , Goodfellow et al ( 2014 ) propose a mature adversarial training framework and use the framework to train generative models . Adversarial networks have recently been used as methods to narrow probability distributions and proven effective in some tasks . In domain adaptation , Ganin et al ( 2016 ) and Bousmalis et al ( 2016 ) adopt adverarial training strategies to transfer the features of one source domain to its corresponding target domain .
Inspired by Ganin et al ( 2016 ) , adversarial training has also been explored in some typical NLP tasks for multi - feature fusion . Park and I m ( 2016 ) propose a multi - modal representation learning model based on adversarial training . Then , Liu et al ( 2017a ) employ adversarial training to construct a multi - task learning model for text classification by extending the original binary adversarial training to the multiclass version . And a similar adversarial framework is also adapted by to learn features from different datasets for chinese word segmentation . In this paper , we adopt adversarial training to boost feature fusion to grasp the consistency among different languages .
In this section , we introduce the overall framework of our proposed AMNRE in detail . As shown in Figure 1 , for each entity pair , AMNRE encodes its corresponding sentences in different languages into several semantic spaces to grasp their individual language patterns . Meanwhile , a unified space is also set up to encode consistent features among languages . By explicitly encoding the consistency and diversity among languages , AMNRE can achieve better extraction results in the multi - lingual scenario . For each given entity pair , we define its corresponding sentences in n different languages as T = { S 1 , . . . , S n } , where S j = { x 1 j , . . . , x | S j | j } denotes the sentence set in the j - th language . All these sentences are labeled with the relation r R by heuristical labeling algorithms in distant supervision ( Mintz et al , 2009 ) . Our model aims to learn a relation extractor by maximizing the conditional probability p ( r | T ) with the following three components : Sentence Encoder . Given a sentence and its target entity pair , we employ neural networks to encode the sentence into a embedding . In this paper , we implement the sentence encoder with both convolutional ( CNN ) and recurrent ( RNN ) architectures . Specifically , we set the encoders E I j and E C j to encode each sentence in the j - th language into its individual and consistent embeddings respectively , and expect these embeddings to capture the diversity and consistency among languages . Multi - lingual Attention . Since not all sentences are labeled correctly in distant supervision , we adopt multi - lingual attention mechanisms to capture those informative sentences . In practice , we apply language - individual and language - consistent attentions to compute local and global textual relation representations respectively for final prediction . Adversarial Training . Under the framework of AMNRE , we encode the sentences in various languages into a unified consistent semantic space . We further adopt adversarial training to ensure these sentences are well fused in the unified space after encoding so that our model can effectively extract the language - consistent relation patterns . We will introduce the three components in detail as follows .
Given a sentence x = { w 1 , w 2 , . . . } containing two entities , we apply neural architectures including both CNN and RNN to encode the sentence into a continuous low - dimensional space to capture its implicit semantics .
The input layer transforms all input words in the sentence into corresponding input embeddings by concatenating their word embeddings and position embeddings . The word embeddings are pre - trained by Skip - Gram ( Mikolov et al , 2013 ) . The position embeddings are a widely - used technique in RE proposed by Zeng et al ( 2014 ) , representing each word 's relative distances to the two entities into two k p - dimensional vectors . The input layer represents the input sentence as a k i - dimensional embedding sequence x = { w 1 , w 2 , . . . } , where k i = k w + k p ×2 , k w and k p are the dimensions of word embeddings and position embeddings respectively .
After representing the input sentence as a k i - dimensional embedding sequence , we select both CNN ( Zeng et al , 2014 ) and RNN to encode the input embedding sequence x = { w 1 , w 2 , . . . } to its sentence embedding . CNN slides a convolution kernel with the window size m to extract the k h - dimensional local features , hi = CNN w i− m−1 2 , . . . , w i+ m−1 2 . ( 1 ) A max - pooling is then adopted to obtain the final sentence embedding y as follows , [ y ] j = max { [ h1 ] j , . . . , [ hn ] j } . ( 2 ) RNN is mainly designed for modeling sequential data . In this paper , we adopt bidirectional RNN ( Bi - RNN ) to incorporate information from both sides of the sentence sequence as follows , − h i = RNN f ( xi , − h i−1 ) , − h i = RNN b ( xi , − h i+1 ) , ( 3 ) where − h i and − h i are the k h - dimensional hidden states at the position i of the forward and backward RNN respectively . RNN ( ) is the recurrent unit and we select gated recurrent unit ( GRU ) ( Cho et al , 2014 ) as the recurrent unit in this paper . We concatenate both the forward and backward hidden states as the sentence embedding y , y = [ − h n ; − h 1 ] . ( 4 ) For simplicity , we denote such a sentence encoding operation as the following equation , y = E ( x ) . ( 5 ) For each sentence x i j S j , we adopt the individual sentence encoder E I j and the consistent sentence encoder E C j to embed the sentence into its individual and consistent representations respectively , { y 1 j , y 2 j , . . . } = { E I j ( x 1 j ) , E I j ( x 2 j ) , . . . } , { ȳ 1 j , ȳ 2 j , . . . } = { E C j ( x 1 j ) , E C j ( x 2 j ) , . . . } . ( 6 )
For each given entity pair , AMNRE adopts multi - lingual selective attention mechanisms to exploit informative sentences in T . We explicitly encode languages ' consistency and diversity into individual and consistent representations , thus our attentions are more simple than those proposed in .
Since it is intuitive that each language has its own characteristic , we set language - individual attention mechanisms for different languages . In the individual semantic space of the j - th language , we assign a query vector r j to each relation r R. The attention score for each sentence in S j = { x 1 j , x 2 j , . . . } is defined as follows , α i j = exp ( r j y i j ) | S j | k=1 exp ( r j y k j ) . ( 7 ) The attention scores can be used to compute language - individual textual relation representations , sj = | S j | k=1 α k j y k j . ( 8 )
Besides language - individual attention mechanisms , we also adopt a language - consistent attention to take all sentences in all languages into consideration . In the consistent semantic space , we also assign a query vectorr to each relation r R and the attention score for each sentence is defined as follows , β i j = exp ( r ȳ i j ) n l=1 | S l | k=1 exp ( r ȳ k l ) . ( 9 ) The attention scores can be used to compute language - consistent textual relation representations , s = n l=1 | S l | k=1 β k lȳ k l . ( 10 )
With the language - individual textual relation representations { s 1 , s 2 , . . . } and the language - consistent textual relation representations , we can estimate the probability p ( r | T ) over each relation r R , p ( r | T ) = p ( r | s ) n j=1 p ( r | sj ) . ( 11 ) p ( r | s ) and p ( r | s j ) can be defined as follows , p ( r | sj ) = softmax [ Rjsj + dj ] , p ( r | s ) = softmax [ Rs + d ] , ( 12 ) where d j andd are bias vectors , R j is the specific relation matrix of the j - th language , andR is the consistent relation matrix . We define the objective function to train the relation extractor as follows , min θ Lnre ( θ ) = − l log p ( r l | T l ) , ( 13 ) where θ is all parameters in the framework . In the training phase , p ( r | T ) is computed using the labeled relations as the attention queries . In the test phase , we need to use each possible relation as attention queries to compute p ( r | T ) for relation prediction since the relations are unknown in advance .
In our framework , we encode sentences of various languages into a consistent semantic space to grasp the consistency among languages . One possible situation is that sentences of different languages are aggregated in different places of the space and linearly separable . In this case , our purpose of mining substantially consistent relation patterns in different languages is difficult to be reached . Inspired by Ganin et al ( 2016 ) , we adopt adversarial training into our framework to address this problem . In the adversarial training , we define a discriminator to estimate which kind of languages the sentences from . The probability distributions over these sentences are formalized as follows , D ( s i j ) = softmax ( MLP ( s i j ) ) , ( 14 ) where MLP is a two - layer multilayer perceptron network . Contrary to the discriminator , the consistent sentence encoders are expected to produce sentence embeddings that can not be reliably predicted by the discriminator . Hence , the adversarial training process is a min - max game and can be formalized as follows , min θ C E max θ D n j=1 | S j | i=1 log [ D ( E C j ( x i j ) ) ] j , ( 15 ) where [ ] j is the j - th value of the vector . The formula means that given a sentence of any language , the corresponding sentence encoder of its language generates the sentence embedding to confuse the discriminator . Meanwhile , the discriminator tries its best to predict the language of the sentence according to the sentence embedding . After sufficient training , the encoders and the discriminator reach a balance , and sentences of different languages containing similar semantic information can be well encoded into adjacent places of the space . In training , we optimize the following loss functions instead of Eq . 15 , min θ C E L E adv ( θ C E ) = l S j T l x i j S j log [ D ( E C j ( x i j ) ) ] j , min θ D L D adv ( θ D ) = − l S j T l x i j S j log [ D ( E C j ( x i j ) ) ] j , ( 16 ) where θ C E and θ D are all parameters of the consistent sentence encoders and the discriminator . We notice that language - individual semantics could be wrongly encoded into the consistent semantic space , and may have negative effects on extracting language - consistent features . Inspired by Bousmalis et al ( 2016 ) , we adopt orthogonality constraints to alleviate this issue . We minimize the following penalty function : min θ E L penalty ( θ E ) = n j=1 I T j Cj F , ( 17 ) where I j and C j are two matrices whose row vectors are the embeddings of sentences in the j - th language encoded by E I j and E C j respectively . θ E is parameters of the all encoders . And F is the squared Frobenius norm .
During training process , we combine the extraction and adversarial objective functions as follows , L = Lnre ( θ ) + λ1L D adv ( θ D ) + λ2L E adv ( θ C E ) + λ3L penalty ( θ E ) , ( 18 ) where λ 1 , λ 2 , and λ 3 are harmonic factors . All models are optimized using stochastic gradient descent ( SGD ) . In practice , we integrate λ 1 and λ 2 into the alternating ratio among the loss functions , and we calibrate a 1:1:5 ratio among L nre ( θ ) + λ 3 L penalty ( θ E ) , L D adv ( θ D ) and L E adv ( θ C E ) . λ 3 is set as 0.02 .
We evaluate our models on a multi - lingual relation extraction dataset developed by . We evaluate all models by the held - out evaluation following previous works ( Mintz et al , 2009 ; . In experiments , we report precision - recall curves of recall under 0.3 since we focus more on the performance of those top - ranked results . To give a complete view of the performance , we also report the area under the curve ( AUC ) .
Following the settings of previous works , we use the pre - trained word embeddings learned by Skip - Gram as the initial word embeddings . We implement the MNRE framework proposed by by ourselves . For fair comparision , we set most of the hyperparameters following . We list the best setting of hyperparameters in Table 2 .
To evaluate the effectiveness of our proposed models AMNRE - CNN and AMNRE - RNN , we compare the proposed models with various neural methods : MNRE - CNN and MNRE - RNN are multi - lingual attention - based NRE models with CNN and RNN sentence encoders respectively ; CNN - EN and RNN - EN are vanilla selective - attention NRE models trained with English data , which are the state - of - the - art models in mono - lingual RE ( Lin et al , 2016 ) ; CNN - CN and RNN - CN are trained with Chinese data ; CNN - Joint and RNN - Joint are naive joint models which predict relations by directly summing up ranking scores of both English and Chinese ; CNN - Share and RNN - Share are another naive joint models which train English and Chinese models with shared relation embeddings . The results of precision - recall curves are shown in Figure 2 and the results of AUC are shown in Table 3 . From the results , we have the following observations : ( 1 ) Both for CNN and RNN , the models jointly utilizing English and Chinese sentences outperform the models only using mono - lingual sentences . This demonstrates that the rich information in multi - lingual data is useful and can significantly enhance existing NRE models . ( 2 ) The - Joint models achieve similar performance with the - Share models , and both of them underperform the MNRE and AMNRE models . They all benefit from the multi - lingual information , but the models with multi - lingual attentions can better take advantage of multi - lingual data . It indicates that designing targeted schemes to extract rich multi - lingual information is crucial . ( 3 ) AMNRE achieves the best results among all the baseline models over the entire range of recall in Figure 2 , even as compared with MNRE . AMNRE also outperforms MNRE with 3 percentage points increasing in the AUC results . It indicates our proposed framework which explicitly encodes languageconsistent and language - individual semantics better extract multi - lingual information , and therefore lead to the significant improvement in RE performance .
To further verify that every mono - lingual RE models can benefit from our proposed framework , which explicitly consider language - consistent relation patterns , we train models with multi - lingual data and evaluate the performance of these models in the mono - lingual RE scenario . To show the results clearly , we report the precision - recall curves in Figure 3 and the AUC results in Table 4 . From the results , we can observe that : ( 1 ) As compared with the models directly learned with the mono - lingual data , the models exploiting the multi - lingual information perform better in the mono - lingual scenario . This demonstrates that there is latent consistency among languages , and grasping this consistency from multi - lingual data can provide additional information for models in each language to enhance their results in the mono - lingual scenario . ( 2 ) Our proposed models achieve the best precision over the entire range of recall and also significantly improve the AUC results as compared with both MNRE and mono - lingual RE models . It indicates that due to the consistent semantic space in our framework , language - consistent information lying in the multi - lingual data is better mined and serve the mono - lingual scenario .
We adopt an adversarial training strategy to fuse the features from different languages to extract consistent relation patterns . Orthogonality constraints are also adopted to separate the consistent and individual feature spaces . To measure the effectiveness of them , we conduct an ablation study which compares the proposed models with the similar models but without adversarial training strategy ( AMNRE - noA ) , without orthogonality constraints ( AMNRE - noO ) , and without both of them ( AMNRE - noBoth ) . The AUC results are shown in Table 5 . We can observe that both the adversarial training strategy and orthogonality constraints have significant influence on the performance of our proposed model . This demonstrates the effectiveness of adversarial training strategy and orthogonality constraints for multi - lingual RE . To give a more intuitive picture of the effect of these two mechanisms , we visualize the distribution of sentence feature embeddings encoded by the individual and consistent encoders using t - SNE ( Maaten and Hinton , 2008 ) . The results are shown in Figure 4 . Figure 4 ( a ) shows that there are obvious differences between the feature embeddings encoded from the same sentences by individual and consistent encoders . It indicates the orthogonality constraints are effective to separate the individual and consistent latent spaces . From the comparison between Figure 4 ( c ) , we can observe that the feature embeddings from different languages are wellmixed due to the adversarial training strategy . We can more easily to grasp latent consistency among languages after multi - feature fusion .
To further show the effectiveness of our proposed model to extract the language - consistent semantic information , we give an example in Table 6 . We adopt the cosine similarity to measure the similarity between sentence embeddings encoded by consistent encoders . The first sentence in the middle column is the standard Chinese translation of the left sentence , thus they share the same semantic information . We observe that in our proposed model , the feature embedding similarity between these two sentences are significantly higher than the other English sentences sharing entity pair and relational fact but differing in semantics . It indicates that sentences in different languages containing similar semantics can be indeed encoded into adjacent places of the consistent space in our framework .
In this paper , we introduce a novel adversarial multi - lingual neural relation extraction model ( AMNRE ) . AMNRE builds both individual and consistent representations for each sentence to consider the consistency and diversity of relation patterns among languages . It also employs an adversarial training strategy and orthogonality constraints to ensure the consistent representations could extract the languageconsistent features to extract relations . The experimental results on real - world datasets demonstrate that
We thank Jiacheng Zhang for his help . This work is supported by the National Natural Science Foundation of China ( NSFC No . 61621136008 , 61772302 ) and Tsinghua University Initiative Scientific Research Program ( 20151080406 ) . This research is part of the NExT++ project , supported by the National Research Foundation , Prime Minister 's Office , Singapore under its IRC@Singapore Funding Initiative .
our AMNRE could effectively encode the consistency and diversity among languages , and achieves state - of - the - art performance in relation extraction . We will explore the following directions as our future work : ( 1 ) AMNRE can be also implemented in the scenario of multiple languages , and this paper shows the effectiveness of AMNRE on the dataset with two languages ( English and Chinese ) . In the future , we will explore AMNRE in much more other languages such as French , Spanish , and so on . ( 2 ) AMNRE simply aligns the sentences with similar semantics in different languages with an adversarial training strategy . In fact , machine translation is a typical approach to align sentences in various languages . In the future , we will combine machine translation with our model to further improve the extraction performance .
