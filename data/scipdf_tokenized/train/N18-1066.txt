Polyglot Semantic Parsing in APIs
Traditional approaches to semantic parsing ( SP ) work by training individual models for each available parallel dataset of text - meaning pairs . In this paper , we explore the idea of polyglot semantic translation , or learning semantic parsing models that are trained on multiple datasets and natural languages . In particular , we focus on translating text to code signature representations using the software component datasets of Richardson and Kuhn ( 2017a , b ) . The advantage of such models is that they can be used for parsing a wide variety of input natural languages and output programming languages , or mixed input languages , using a single unified model . To facilitate modeling of this type , we develop a novel graph - based decoding framework that achieves state - of - the - art performance on the above datasets , and apply this method to two other benchmark SP tasks . * Returns the greater of two long values public static long max ( long a , long b ) 2 . ( en , Python ) Documentation max ( self , a , b ) : " " " Compares two values numerically and returns the maximum " " " 3 . ( en , Haskell ) Documentation - - | " The largest element of a non - empty structure " maximum : : forall z. Ord a a = > t a - > a 4 . ( de , PHP ) Documentation * gibt den größeren dieser Werte zurück .
Recent work by Richardson and Kuhn ( 2017a , b ) ; Miceli Barone and Sennrich ( 2017 ) considers the problem of translating source code documentation to lower - level code template representations as part of an effort to model the meaning of such documentation . Example documentation for a number of programming languages is shown in Figure 1 , where each docstring description in red describes a given function ( blue ) in the library . While capturing the semantics of docstrings is in general a difficult task , learning the translation from descriptions to formal code representations ( e.g. , formal representations of functions ) is proposed as a reasonable first step towards learning more general natural language understanding models in the software domain . Under this approach , one can view a software library , or API , as a kind of parallel translation corpus for studying text code or code text translation . Richardson and Kuhn ( 2017b ) extracted the standard library documentation for 10 popular programming languages across a number of natural languages to study the problem of text to function signature translation . Initially , these datasets were proposed as a resource for studying semantic parser induction ( Mooney , 2007 ) , or for building models that learn to translate text to formal meaning representations from parallel data . In followup work ( Richardson and Kuhn , 2017a ) , they proposed using the resulting models to do automated question - answering ( QA ) and code retrieval on target APIs , and experimented with an additional set of software datasets built from 27 open - source Python projects . As traditionally done in SP ( Zettlemoyer and Collins , 2012 ) , their approach involves learning individual models for each parallel dataset or language pair , e.g. , ( en , Java ) , ( de , PHP ) , and ( en , Haskell ) . Looking again at Figure 1 , we notice that while programming languages differ in terms of representation conventions , there is often overlap between the functionality implemented and naming in these different languages ( e.g. , the max function ) , and redundancy in the associated linguistic descriptions . In addition , each English description ( Figure 1.1 - 1.3 ) describes max differently using the synonyms greater , maximum , largest . In this case , it would seem that training models on multiple datasets , as opposed to single language pairs , might make learning more robust , and help to capture various linguistic alternatives . With the software QA application in mind , an additional limitation is that their approach does not allow one to freely translate a given description to multiple output languages , which would be useful for comparing how different programming languages represent the same functionality . The model also can not translate between natural languages and programming languages that are not observed during training . While software documentation is easy to find in bulk , if a particular API is not already documented in a language other than English ( e.g. , Haskell in de ) , it is unlikely that such a translation will appear without considerable effort by experienced translators . Similarly , many individual APIs may be too small or poorly documented to build individual models or QA applications , and will in some way need to bootstrap off of more general models or resources . To deal with these issues , we aim to learn more general text - to - code translation models that are trained on multiple datasets simultaneously . Our ultimate goal is to build polyglot translation models ( cf . Johnson et al ( 2016 ) ) , or models with shared representations that can translate any input text to any output programming language , regardless of whether such language pairs were encountered explicitly during training . Inherent in this task is the challenge of building an efficient polyglot decoder , or a translation mechanism that allows such crossing between input and output languages . A key challenge is ensuring that such a decoder generates well - formed code representations , which is not guaranteed when one simply applies standard decoding strategies from SMT and neural MT ( cf . Cheng et al ( 2017 ) ) . Given our ultimate interest in API QA , such a decoder must also facilitate monolingual translation , or being able to translate to specific output languages as needed . To solve the decoding problem , we introduce a new graph - based decoding and representation framework that reduces to solving shortest path problems in directed graphs . We investigate several translation models that work within this framework , including traditional SMT models and models based on neural networks , and report stateof - the - art results on the technical documentation task of Richardson and Kuhn ( 2017b , a ) . To show the applicability of our approach to more conventional SP tasks , we apply our methods to the Geo - Query domain ( Zelle and Mooney , 1996 ) and the Sportscaster corpus ( Chen et al , 2010 ) . These experiments also provide insight into the main technical documentation task and highlight the strengths and weaknesses of the various translation models being investigated .
Our approach builds on the baseline models introduced in Richardson and Kuhn ( 2017b ) ( see also Deng and Chrupała ( 2014 ) ) . Their work is positioned within the broader SP literature , where traditionally SMT ( Wong and Mooney , 2006a ) and parsing ( Zettlemoyer and Collins , 2009 ) methods are used to study the problem of translating text to formal meaning representations , usually centering around QA applications ( Berant et al , 2013 ) . More recently , there has been interest in using neural network approaches either in place of ( Dong and Lapata , 2016 ; Kočiský et al , 2016 ) or in combination with ( Misra and Artzi , 2016 ; Jia and Liang , 2016 ; Cheng et al , 2017 ) these traditional models , the latter idea we look at in this paper . Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github ( cf . Allamanis et al ( 2017 ) ) . Most of this recent work focuses on processing large amounts of API data in bulk ( Gu et al , 2016 ; Miceli Barone and Sennrich , 2017 ) , either for learning longer executable programs from text ( Yin and Neubig , 2017 ; Rabinovich et al , 2017 ) , or solving the inverse problem of code to text generation ( Iyer et al , 2016 ; . In contrast to our work , these studies do not look explicitly at translating to target APIs , or at non - English documentation . The idea of polyglot modeling has gained some traction in recent years for a variety of problems ( Tsvetkov et al , 2016 ) and has appeared within work in SP under the heading of multilingual SP ( Jie and Lu , 2014 ; Duong et al , 2017 ) . A related topic is learning from multiple knowledge sources or domains ( Herzig and Berant , 2017 ) , which is related to our idea of learning from multiple APIs . When building models that can translate between unobserved language pairs , we use the term zeroshot translation from Johnson et al ( 2016 ) .
Problem Formulation Throughout the paper , we refer to target code representations as API components . In all cases , components will consist of formal representations of functions , or function signatures ( e.g. , long max ( int a , int b ) ) , which include a function name ( max ) , a sequence of arguments ( int a , int b ) , and other information such as a return value ( long ) and namespace ( for more details , see Richardson ( 2018 ) ) . For a given API dataset D = { ( x i , z i ) } n i=1 of size n , the goal is to learn a model that can generate exactly a correct component sequence z = ( z 1 , .. , z | z | ) , within a finite space C of signatures ( i.e. , the space of all defined functions ) , for each input text sequence x = ( x 1 , ... , x | x | ) . This involves learning a probability distribution p ( z | x ) . As such , one can think of this underlying problem as a constrained MT task . In this section , we describe the baseline approach of Richardson and Kuhn ( 2017b ) . Technically , their approach has two components : a simple word - based translation model and task specific decoder , which is used to generate a k - best list of candidate component representations for a given input x. They then use a discriminative model to rerank the translation output using additional nonworld level features . The goal in this section is to provide the technical details of their translation approach , which we improve in Section 4 .
The translation models investigated in Richardson and Kuhn ( 2017b ) use a noisy - channel formulation where p ( z | x ) ∝ p ( x | z ) p ( z ) via Bayes rule . By assuming a uniform prior on output components , p ( z ) , the model therefore involves estimating p ( x | z ) , which under a word - translation model is computed using the following formula : p ( x | z ) = a A p ( x , a | z ) , where the summation ranges over the set of all many - to - one word alignments A from x z , with | A | equal to ( | z | + 1 ) | x | . They investigate various types of sequence - based alignment models ( Och and Ney , 2003 ) , and find that the classic IBM Model 1 outperforms more complex word models . This model factors in the following way and assumes an inde - pendent word generation process : p ( x | z ) = 1 | A | | x | j=1 | z | i=0 p t ( x j | z i ) ( 1 ) where each p t defines a multinomial distribution over a given component term z for all words x. The decoding problem for the above translation model involves finding the most likely outputẑ , which requires solving an arg max z over Equation 1 . In the general case , this problem is known to be N P - complete for the models under consideration ( Knight , 1999 ) largely due to the large space of possible predictions z. Richardson and Kuhn ( 2017b ) avoid these issues by exploiting the finiteness of the target component search space ( an idea we also pursue here and discuss more below ) , and describe a constrained decoding algorithm that runs in time O ( | C | log | C | ) . While this works well for small APIs , it becomes less feasible when dealing with large sets of APIs , as in the polyglot case , or with more complex semantic languages typically used in SP ( Liang , 2013 ) .
To improve the baseline translation approach used previously ( Section 3.1 ) , we pursue a graph based approach . Given the formulation above and the finiteness of our prediction space C , our approach exploits the fact that we can represent the complete component search space for any set of APIs as a directed acyclic finite - state automaton ( DAFSA ) , such as the one shown graphically in Figure 2 . The underlying graph is constructed by concatenating all of the component representations for each API of interest and applying standard finite - state construction and minimization techniques ( Mohri , 1996 ) . Each path in the resulting compact automaton is therefore a well - formed component representation . Using an idea from Johnson et al ( 2016 ) , we add to each component representation an artificial token that identifies the output programming language or library . For example , the two edges from the initial state 0 in Figure 2 are labeled as 2C and 2Clojure , which identify the C and Clojure programming languages respectively . All paths starting from the right of these edges are therefore valid paths in each respective programming language . The paths starting from the initial state 0 , in contrast , correspond to all valid component representations in all languages . Decoding reduces to the problem of finding a path for a given text input x. For example , given the input the ceiling of a number , we would want to find the paths corresponding to the component translations numeric math ceil arg ( in C ) and algo math ceil x ( in Clojure ) in the graph shown in Figure 2 . Using the trick above , our setup facilitates both monolingual decoding , i.e. , generating components specific to a particular output language ( e.g. , the C language via the path shown in bold ) , and polyglot decoding , i.e. , generating any output language by starting at the initial state 0 ( e.g. , C and Clojure ) . We formulate the decoding problem using a variant of the well - known single source shortest path ( SSSP ) algorithm for directed acyclic graphs ( DAGs ) ( Johnson ( 1977 ) ) . This involves a graph G = ( V , E ) ( nodes V and labeled edges E , see graph in Figure 2 ) , and taking an off - line topological sort of the graph 's vertices . Using a data structure d R | V | ( initialized as | V | , as shown in Figure 2 ) , the standard SSSP algorithm ( which is the forward update variant of the Viterbi algorithm ( Huang , 2008 ) ) works by searching forward through the graph in sorted order and finding for each node v an incoming labeled edge u , with label z , that solves the following recurrence : d ( v ) = min ( u , z ) : ( u , v , z ) E d ( u ) + w ( u , v , z ) ( 2 ) where d ( u ) is shortest path score from a unique source node b to the incoming node u ( computed recursively ) and w ( u , v , z ) is the weight of the particular labeled edge . The weight of the resulting shortest path is commonly taken to be the sum of the path edge weights as given by w , and the output translation is the sequence of labels associated with each edge . This algorithm runs in linear time over the size of the graph 's adjacency matrix ( Adj ) and can be extended to find k SSSPs . In the standard case , a weighting function w is pro - d [ V [ G ] ] , π [ V [ G ] ] N il , d [ b ] o 2 : s [ V [ G ] , n ] 0.0 Shortest path sums at each node 3 : for each vertex u ≥ b V [ G ] in sorted order do 4 : for each vertex and label ( v , z ) Adj [ u ] do 5 : score −log n i pt ( xi | z ) + s [ u , i ] 6 : if d [ v ] > score then 7 : d [ v ] score , π [ v ] u 8 : for i in 1 , .. , n do Update scores 9 : s [ v , i ] pt ( xi | z ) + s [ u , i ] 10 : return FINDPATH ( π , | V | , b ) vided by assuming a static weighted graph . In our translation context , we replace w with a translation model , which is used to dynamically generate edge weights during the SSSP search for each input x by scoring the translation between x and each edge label z encountered . Given this general framework , many different translation models can be used for scoring . In what follows , we describe two types of decoders based on lexical translation ( or unigram ) and neural sequence models . Technically , each decoding algorithm involves modifying the standard SSSP search procedure by adding an additional data structure s to each node ( see Figure 2 ) , which is used to store information about translations ( e.g. , running lexical translation scores , RNN state information ) associated with particular shortest paths . By using these two very different models , we can get insight into the challenges associated with the technical documentation translation task . As we show in Section 6 , each model achieves varying levels of success when subjected to a wider range of SP tasks , which reveals differences between our task and other SP tasks .
In our first model , we use the lexical translation model and probability function p t in Equation 1 as the weighting function , which can be learned efficiently off - line using the EM algorithm . When attempting to use the SSSP procedure to compute this equation for a given source input x , we immediately have the problem that such a computation requires a complete component representation z ( Knight and Al - Onaizan , 1998 ) . We use an approximation 1 that involves ignoring the normalizer | A | and exploiting the word independence assumption of the model , which allows us to incrementally compute translation scores for individual source words given output translations corresponding to shortest paths during the SSSP search . The full decoding algorithm in shown in Algorithm 1 , where the red highlights the adjustments made to the standard SSSP search as presented in Cormen et al ( 2009 ) . The main modification involves adding a data structure s R | V | × | x | ( initialized as 0.0 | V | × | x | at line 2 ) that stores a running sum of source word scores given the best translations at each node , which can be used for computing the inner sum in Equation 1 . For example , given an input utterance ceiling function , s 6 in Figure 2 contains the independent translation scores for words ceiling and function given the edge label numeric and p t . Later on in the search , these scores are used to compute s 7 , which will provide translation scores for each word given the edge sequence numeric math . Taking the product over any given s j ( as done in line 7 to get score ) will give the probability of the shortest path translation at the particular point j. Here , the transformation into − log space is used to find the minimum incoming path . Standardly , the data structure π can be used to retrieve the shortest path back to the source node b ( done via the FINDPATH method ) .
Our second set of models use neural networks to compute the weighting function in Equation 2 . We use an encoder - decoder model with global attention ( Bahdanau et al , 2014 ; Luong et al , 2015 ) , which has the following two components : Encoder Model The first is an encoder network , which uses a bi - directional recurrent neural network architecture with LSTM units ( Hochreiter and Schmidhuber , 1997 ) to compute a sequence of forward annotations or hidden states ( − h 1 , ... , − h | x | ) and a sequence of backward hid - 1 Details about the approx . are provided as supp . material . den states ( − h , ... , − h | x | ) for the input sequence ( x 1 , ... , x | x | ) . Standardly , each word is then represented as the concatenation of its forward and backward states : h j = [ − h j , − h j ] .
The second component is a decoder network , which directly computes the conditional distribution p ( z | x ) as follows : p ( z | x ) = | z | i=1 log p Θ ( z i | z < i , x ) ( 3 ) p Θ ( z i | z < i , x ) ∼ softmax ( f ( Θ , z < i , x ) ) ( 4 ) where f is a non - linear function that encodes information about the sequence z < i and the input x given the model parameters Θ. We can think of this model as an ordinary recurrent language model that is additionally conditioned on the input x using information from our encoder . We implement the function f in the following way : f ( Θ , z < i , x ) = W o η i + b o ( 5 ) η i = MLP ( c i , g i ) ( 6 ) g i = LSTM dec ( g i−1 , E out z i−1 , c i ) ( 7 ) where MLP is a multi - layer perceptron model with a single hidden layer , E out R | Σ dec | ×e is a randomly initialized embedding matrix , g i is the decoder 's hidden state at step i , and c i is a contextvector that encodes information about the input x and the encoder annotations . Each context vector c i in turn is a weighted sum of each annotation h j against an attention vector α i , j , or c i = | x | j=1 α i , j h j , which is jointly learned using an additional single layered multi - layer perceptron defined in the following way : α i , j ∝ exp ( e i , j ) ; e i , j = MLP ( g i−1 , h j ) ( 8 ) Lexical Bias and Copying In contrast to standard MT tasks , we are dealing with a relatively low - resource setting where the sparseness of the target vocabulary is an issue . For this reason , we experimented with integrating lexical translation scores using a biasing technique from Arthur et al ( 2016 ) . Their method is based on the following computation for each token z i : bias i =   p t ( 1 : d [ V [ G ] ] , d [ b ] o , π [ V [ G ] ] N il 2 : s [ V [ G ] ] N il Path state information 3 : s [ b ] InitState ( ) Initialize source state 4 : for each vertex u ≥ b V [ G ] in sorted order do 5 : if isinf ( d [ u ] ) then continue 6 : p s [ u ] Current state at node u , or z < i 7 : L 1 [ l ] arg max ( v 1 , ... , v k ) Adj [ u ] softmax ( f ( Θ , p , x ) )
for each vertex and label ( v , z ) L do 9 : score − log pΘ ( z | p , x ) + d [ u ] 10 : if d [ v ] > score then 11 : d [ v ] score , π [ v ] u 12 : s [ v ] UpdateState ( p , z ) 13 : return FINDPATH ( π , | V | , b ) The first matrix uses the inverse ( p t ) of the lexical translation function p t already introduced to compute the probability of each word in the target vocabulary Σ dec ( the columns ) with each word in the input x ( the rows ) , which is then weighted by the attention vector from Equation 8 . bias i is then used to modify Equation 5 in the following way : f bias ( Θ , z < i , x ) = W o η i + b o + log ( bias i + ) where is a hyper - parameter that helps to preserve numerical stability and biases more heavily on the lexical model when set lower . We also experiment with the copying mechanism from Jia and Liang ( 2016 ) , which works by allowing the decoder to choose from a set of latent actions , a j , that includes writing target words according to Equation 5 , as done standardly , or copying source words from x , or copy [ x i ] according to the attention scores in Equation 8 . A distribution is then computed over these actions using a softmax function and particular actions are chosen accordingly during training and decoding .
The full decoding procedure is shown in Algorithm 2 , where the differences with the standard SSSP are again shown in red . We change the data structure s to contain the decoder 's RNN state at each node . We also modify the scoring ( line 7 , which uses Equation 4 ) to consider only the top l edges or translations at that point , as opposed to imposing a full search . When l is set to 1 , for example , the procedure does a greedy search through the graph , whereas when l is large the procedure is closer to a full search . In general terms , the decoder described above works like an ordinary neural decoder with the difference that each decision ( i.e. , new target - side word translation ) is constrained ( in line 7 ) by the transitions allowed in the underlying graph in order to ensure wellformedness of each component output . Standardly , we optimize these models using stochastic gradient descent with the objective of finding parametersΘ that minimize the negative conditional log - likelihood of the training dataset .
Our framework facilitates both monolingual and polyglot decoding . In the first case , the decoder requires a graph associated with the output semantic language ( more details in next section ) and a trained translation model . The latter case requires taking the union of all datasets and graphs ( with artificial identifier tokens ) for a collection of target datasets and training a single model over this global dataset . In this setting , we can then decode to a particular language using the language identifiers or decode without specifying the output language . The main focus in this paper is investigating polyglot decoding , and in particular the effect of training models on multiple datasets when translating to individuals APIs or SP datasets . When evaluating our models and building QA applications , it is important to be able to generate the k best translations . This can easily be done in our framework by applying standard k SSSP algorithms ( Brander and Sinclair , 1995 ) . We use an implementation of the algorithm of Yen ( 1971 ) , which works on top of the SSSP algorithms introduced above by iteratively finding deviating or branching paths from an initial SSSP ( more details provided in supplementary materials ) .
We experimented with two main types of resources : 45 API documentation datasets and two multilingual benchmark SP datasets . In the former case , our main objective is to test whether training polyglot models ( shown as polyglot in Tables 1 - 2 ) on multiple datasets leads to an improvement when compared to training individual monolingual models ( shown as monolingual in Tables 1 - 2 ) . Experiments involving the latter datasets are meant to test the applicability of our general graph and polyglot method to related SP tasks , and are also used for comparison against our main technical documentation task . Figure 3 : Test Acc@1 for the best monolingual models ( in yellow / left ) compared with the best lexical polyglot model ( green / right ) across all 45 technical documentation datasets .
Technical API Docs The first dataset includes the Stdlib and Py27 datasets of Richardson and Kuhn ( 2017b , a ) , which are publicly available via Richardson ( 2017 ) . Stdlib consists of short description and function signature pairs for 10 programming languages in 7 languages , and Py27 contains the same type of data for 27 popular Python projects in English mined from Github . We also built new datasets from the Japanese translation of the Python 2.7 standard library , as well as the Lua stdlib documentation in a mixture of Russian , Portuguese , German , Spanish and English . Taken together , these resources consist of 79 , 885 training pairs , and we experiment with training models on Stdlib and Py27 separately as well as together ( shown as + more in Table 1 ) . We use a BPE subword encoding ( Sennrich et al , 2015 ) of both input and output words to make the representations more similar and transliterated all datasets ( excluding Japanese datasets ) to an 8 - bit latin encoding . Graphs were built by concatenating all function representations into a single word list and compiling this list into a minimized DAFSA . For our global polyglot dataset , this resulted in a graph with 218 , 505 nodes , 313 , 288 edges , and 112 , 107 paths or component representations over an output vocabulary of 9 , 324 words .
We run experiments on the GeoQuery 880 corpus using the splits from Andreas et al ( 2013 ) , which includes geography queries for English , Greek , Thai , and German paired with formal database queries , as well as a seed lexicon or NP list for each language . In addition to training models on each individual dataset , we also learn polyglot models trained on all datasets concatenated together . We also created a new mixed language test set that was built by re - placing NPs in 803 test examples with one or more NPs from a different language using the NP lists mentioned above ( see examples in Figure 4 ) . The goal in the last case is to test our model 's ability to handle mixed language input . We also ran monolingual experiments on the English Sportscaster corpus , which contains human generated soccer commentary paired with symbolic meaning representation produced by a simulation of four games . For GeoQuery graph construction , we built a single graph for all languages by extracting general rule templates from all representations in the dataset , and exploited additional information and patterns using the Geobase database and the semantic grammars used in ( Wong and Mooney , 2006b ) . This resulted in a graph with 2 , 419 nodes , 4 , 936 edges and 39 , 482 paths over an output vocabulary of 164 . For Sportscaster , we directly translated the semantic grammar provided in Chen and Mooney ( 2008 ) to a DAFSA , which resulted in a graph with 98 nodes , 86 edges and 830 paths .
For the technical datasets , the goal is to see if our model generates correct signature representations from unobserved descriptions using exact match . We follow exactly the experimental setup and data splits from Richardson and Kuhn ( 2017b ) , and measure the accuracy at 1 ( Acc@1 ) , accuracy in top 10 ( Acc@10 ) , and MRR . For the GeoQuery and Sportscaster experiments , the goal is to see if our models can generate correct meaning representations for unseen input . For GeoQuery , we follow Andreas et al ( 2013 ) in evaluating extrinsically by checking that each representation evaluates to the same answer as the gold representation when executed against the Geobase database . For Sportscaster , we evaluate by exact match to a gold representation . Richardson and Kuhn ( 2017b , a ) , or RK
We use the Foma finite - state toolkit of Hulden ( 2009 ) to construct all graphs used in our experiments . We also use the Cython version of Dynet ( Neubig et al , 2017 ) to implement all the neural models ( see supp . materials for more details ) . In the results tables , we refer to the lexical and neural models introduced in Section 4 as Lexical Shortest Path and Neural Shortest Path , where models that use copying ( + copy ) and lexical biasing ( + bias ) are marked accordingly . We also experimented with adding a discriminative reranker to our lexical models ( + rerank ) , using the approach from Richardson and Kuhn ( 2017b ) , which uses additional lexical ( e.g. , word match and alignment ) features and other phrase - level and syntax features . The goal here is to see if these additional ( mostly non - word level ) features help improve on the baseline lexical models .
Technical Documentation Results Table 1 shows the results for Stdlib and Py27 . In the monolingual case , we compare against the best performing models in Richardson and Kuhn ( 2017b , a ) . As summarized in Figure 3 , our experiments show that training polyglot models on multiple datasets can lead to large improvements over training individual models , especially on the Py27 datasets where using a polyglot model resulted in a nearly 9 % average increase in accuracy @1 . In both cases , however , the best performing lexical models are those trained only on the datasets they are evaluated on , as opposed to training on all datasets ( i.e. , + more ) . This is surprising given that training on all datasets doubles the size of the training data , and shows that adding more data does not necessarily boost performance when the additional data is from another distribution .
Acc@1 Acc@10 UBL ( Kwiatkowski et al , 2010 ) 74.2 - TreeTrans ( Jones et al , 2012 ) 76.8 - nHT ( Susanto and Lu , 2017 ) 83 . The neural models are strongly outperformed by all other models both in the monolingual and polyglot case ( only the latter results shown ) , even when lexical biasing is applied . While surprising , this is consistent with other studies on lowresource neural MT ( Zoph et al , 2016 ; Östling and Tiedemann , 2017 ) , where datasets of comparable size to ours ( e.g. , 1 million tokens or less ) typically fail against classical SMT models . This result has also been found in relation to neural AMR semantic parsing , where similar issues of sparsity are encountered ( Peng et al , 2017 ) . Even by doubling the amount of training data by training on all datasets ( results not shown ) , this did not improve the accuracy , suggesting that much more data is needed ( more discussion below ) . Beyond increases in accuracy , our polyglot models support zero - shot translation as shown in Figure 4 , which can be used for translating between unobserved language pairs ( e.g. , ( es , Clojure ) , ( ru , Haskell ) as shown in 1 - 2 ) , or for finding related functionality across different software projects ( as shown in 3 ) . These results were obtained by running our decoder model without specifying the output language . We note , however , that the decoder can be constrained to selectively translate to any specific programming language or project ( e.g. , in a QA setting ) . Future work will further investigate the decoder 's polyglot capabilities , which is currently hard to evaluate since we do not have an annotated set of function equivalences between different APIs .
Mixed GeoQuery ( de / gr ) Input : Wie hoch liegt der höchstgelegene punkt in Αλαμπάμα ? Logical Form Translation : answer ( elevation 1 ( highest ( place ( loc 2 ( stateid ( ' alabama ' ) ) ) ) ) ) Figure 4 : Examples of zero - shot translation when running in polyglot mode ( 1 - 3 , function representations shown in a conventionalized format ) , and mixed language parsing ( 4 ) . Semantic Parsing Results SP results are summarized in Table 2 . In contrast , the neural models , especially those with biasing and copying , strongly outperform all other models and are competitive with related work . In the GeoQuery case , we compare against two classic grammar - based models , UBL and TreeTrans , as well as a feature rich , neural hybrid tree model ( nHT ) . We also see that the polyglot Geo achieves the best performance , demonstrating that training on multiple datasets helps in this domain as well . In the Sportscaster case we compare against two PCFG learning approaches , where the second model ( wo - PCFG ) involves a grammar with complex wordorder constraints . The advantage of training a polyglot model is shown on the results related to mixed language parsing ( i.e. , the middle set of results ) . Here we compared against the best performing monolingual English model ( Best Mono . Model ) , which does not have a way to deal with multilingual NPs . We also find the neural model to be more robust than the lexical models with reranking . While the lexical models overall perform poorly on both tasks , the weakness of this model is particularly acute in the Sportscaster case . We found that mistakes are largely related to the ordering of arguments , which these lexical ( unigram ) models are blind to . That these models still perform reasonably well on the Geo task shows that such ordering issues are less of a factor in this domain .
Having results across related SP tasks allows us to reflect on the nature of the main technical documentation task . Consistent with recent findings ( Dong and Lapata , 2016 ) , we show that relatively simple neural sequence models are competitive with , and in some cases outperform , traditional grammar - based SP methods on bench - mark SP tasks . However , this result is not observed in our technical documentation task , in part because this problem is much harder for neural learners given the sparseness of the target data and lack of redundancy . For this reason , we believe our datasets provide new challenges for neural - based SP , and serve as a cautionary tale about the scalability and applicability of commonly used neural models to lower - resource SP problems . In general , we believe that focusing on polyglot and mixed language decoding is not only of interest to applications ( e.g , mixed language API QA ) but also allows for new forms of SP evaluation that are more revealing than only translation accuracy . When comparing the accuracy of the best monolingual Geo model and the worst performing neural polyglot model , one could mistakingly think that these models have equal abilities , though the polyglot model is much more robust and general . Moving forward , we hope that our work helps to motivate more diverse evaluations of this type .
We look at learning from multiple API libraries and datasets in the context of learning to translate text to code representations and other SP tasks . To support polyglot modeling of this type , we developed a novel graph based decoding method and experimented with various SMT and neural MT models that work in this framework . We report a mixture of positive results specific to each task and set of models , some of which reveal interesting limitations of different approaches to SP . We also introduced new API and mixed language datasets to facilitate further work on polyglot SP .
This work was supported by the German Research Foundation ( DFG ) in project D2 of SFB 732 .
