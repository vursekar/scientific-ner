{"authors": "Masato Umakoshi; Yugo Murawaki; Sadao Kurohashi", "pub_date": "", "title": "Japanese Zero Anaphora Resolution Can Benefit from Parallel Texts Through Neural Transfer Learning", "abstract": "Parallel texts of Japanese and a non-pro-drop language have the potential of improving the performance of Japanese zero anaphora resolution (ZAR) because pronouns dropped in the former are usually mentioned explicitly in the latter. However, rule-based cross-lingual transfer is hampered by error propagation in an NLP pipeline and the frequent lack of transparency in translation correspondences. In this paper, we propose implicit transfer by injecting machine translation (MT) as an intermediate task between pretraining and ZAR. We employ a pretrained BERT model to initialize the encoder part of the encoder-decoder model for MT, and eject the encoder part for finetuning on ZAR. The proposed framework empirically demonstrates that ZAR performance can be improved by transfer learning from MT. In addition, we find that the incorporation of the masked language model training into MT leads to further gains.", "sections": [{"heading": "Introduction", "text": "Figuring out who did what to whom is an essential part of natural language understanding. This is, however, especially challenging for so-called prodrop languages like Japanese and Chinese because they usually omit pronouns that are inferable from context. The task of identifying the referent of such a dropped element, as illustrated in Figure 1(a), is referred to as zero anaphora resolution (ZAR). Although Japanese ZAR saw a performance boost with the introduction of BERT (Ueda et al., 2020;Konno et al., 2020), there is still a good amount of room for improvement.\nA major barrier to improvement is the scarcity of training data. The number of annotated sentences is the order of tens of thousands or less (Kawahara et al., 2002;Hangyo et al., 2012;Iida et al., 2017), and the considerable linguistic expertise required for annotation makes drastic corpus expansion impractical.\nPrevious attempts to overcome this limitation exploit orders-of-magnitude larger parallel texts of Japanese and English, a non-pro-drop language (Nakaiwa, 1999;Furukawa et al., 2017). The key idea is that Japanese zero pronouns can be recovered from parallel texts because they are usually mentioned explicitly in English, as in Figure 1(b). If translation correspondences are identified and the anaphoric relation in English is identified, then we can identify the antecedent of the omitted argument in Japanese.\nTheir rule-based transfer from English to Japanese had met with limited success, however. It is prone to error propagation due to its dependence on word alignment, parsing, and English coreference resolution. More importantly, the great linguistic differences between the two language often lead to parallel sentences without transparent syntactic correspondences (Figure 1(c)).\nIn this paper, we propose neural transfer learning from machine translation (MT). By generating English translations, a neural MT model should be able to implicitly recover omitted Japanese pronouns, thanks to its expressiveness and large training data. We expect the knowledge gained during MT training to be transferred to ZAR. Given that state-of-the-art ZAR models are based on BERT (Ueda et al., 2020;Konno et al., 2020Konno et al., , 2021, it is a natural choice to explore intermediate task transfer learning (Phang et al., 2018;Wang et al., 2019a;Pruksachatkun et al., 2020;Vu et al., 2020): A pretrained BERT model is first trained on MT and the resultant model is then fine-tuned on ZAR. 1 A key challenge to this approach is a mismatch in model architectures. While BERT is an encoder, the dominant paradigm of neural MT is the encoder-decoder. Although both share Transformer (Vaswani et al., 2017)   The nominative argument of the underlined predicate is omitted. The goal of the task is to detect the omission and to identify its antecedent \"son\". (b) The corresponding English text. The omitted argument in Japanese is present as a pronoun in English. (c) A Japanese-English pair (Nabeshima and Brooks, 2020, p. 74) whose correspondences are too obscure for rule-based transfer. Because Japanese generally avoids having inanimate agents with animate patients, the English inanimate-subject sentence corresponds to two animate-subject clauses in Japanese, with two exophoric references to the reader (i.e., you).\nit is non-trivial to combine the two distinct architectures, with the goal to help the former.\nWe use a pretrained BERT model to initialize the encoder part of the encoder-decoder model for MT. While this technique was previously used by Imamura and Sumita (2019) and Clinchant et al. (2019), they both aimed at improving MT performance. We show that by ejecting the encoder part for use in fine-tuning (Figure 2), we can achieve performance improvements in ZAR. We also demonstrate further improvements can be brought by incorporating encoder-side masked language model (MLM) training into the intermediate training on MT.", "n_publication_ref": 18, "n_figure_ref": 4}, {"heading": "Related Work", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Zero Anaphora Resolution (ZAR)", "text": "ZAR has been extensively studied in major East Asian languages, Chinese and Korean as well as Japanese, which not only omit contextually inferable pronouns but also show no verbal agreement for person, number, or gender (Park et al., 2015;Song et al., 2020;Kim et al., 2021). While supervised learning is the standard approach to ZAR (Iida et al., 2016;Ouchi et al., 2017;Shibata and Kurohashi, 2018), training data are so small that additional resources are clearly needed. Early studies work on case frame construction from a large raw corpus (Sasano et al., 2008;Sasano and Kurohashi, 2011;Yamashiro et al., 2018), pseudo training data generation , and adversarial training (Kurita et al., 2018). These efforts are, however, overshadowed by the surprising effectiveness of BERT's pretraining (Ueda et al., 2020;Konno et al., 2020).\nAdopting BERT, recent studies seek gains through multi-task learning (Ueda et al., 2020), data augmentation (Konno et al., 2020), and an intermediate task tailored to ZAR (Konno et al., 2021). The multi-task learning approach of Ueda et al. (2020) covers verbal predicate analysis (which subsumes ZAR), and nominal predicate analysis, coreference resolution, and bridging anaphora resolution. Their method is used as a state-of-the-art baseline in our experiments. Konno et al. (2020) perform data augmentation by simply masking some tokens. They found that performance gains were achieved by selecting target tokens by part of speech. Konno et al. (2021) introduce a more elaborate masking strategy as a ZAR-specific intermediate task They spot multiple occurrences of the same noun phrase, mask one of them, and force the model to identify the pseudo-antecedent.\nOur use of parallel texts in ZAR is inspired by Nakaiwa (1999) and Furukawa et al. (2017), who identify a multi-hop link from a Japanese zero pronoun to its Japanese antecedent via English counterparts. Their rule-based methods suffer from accumulated errors and syntactically non-transparent correspondences. In addition, they do not handle inter-sentential anaphora, a non-negligible subtype of anaphora we cover in this paper.\nWhile we exploit MT to improve the performance of ZAR, the exploitation in the reverse direction has been studied. A line of research has been done on Chinese zero pronoun prediction (ZPP) with a primary aim of improving Chinese-English translation (Wang et al., 2016(Wang et al., , 2018(Wang et al., , 2019b. ZPP is different from ZAR in that it does not identify antecedents. This is understandable given that classification of zero pronouns into overt ones suffices for MT. Although Wang et al. (2019b)   open question whether MT helps ZAR as well.", "n_publication_ref": 24, "n_figure_ref": 0}, {"heading": "MT as an Intermediate Task", "text": "Inspired by the great success of the pretraining/finetuning paradigm on a broad range of tasks (Peters et al., 2018;Devlin et al., 2019), a line of research inserts an intermediate task between pretraining and fine-tuning on a target task (Phang et al., 2018;Wang et al., 2019a;Pruksachatkun et al., 2020). However, Wang et al. (2019a) found that MT used as an intermediate task led to performance degeneration in various target tasks, such as natural language inference and sentiment classification. 2 They argue that the considerable difference between MLM pretraining and MT causes catastrophic forgetting (CF). Pruksachatkun et al. (2020) suggest injecting the MLM objective during intermediate training as a possible way to mitigate CF, which we empirically test in this paper.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Use of BERT in MT", "text": "Motivated by BERT's success in a wide range of applications, some studies incorporate BERT into MT models. A straightforward way to do this is to initialize the encoder part of the encoder-decoder with pretrained BERT, but it has had mixed success at best (Clinchant et al., 2019;Zhu et al., 2020). Abandoning this approach, Zhang et al. (2020) simply use BERT as a supplier of context-aware embeddings to their own encoder-decoder model. Similarly, Guo et al. (2020) stack adapter layers on top of two frozen BERT models to use them as the encoder and decoder of a non-autoregressive MT 2 We suspect that the poor performance resulted in part from their excessively simple decoder, a single-layer LSTM.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "BERT [CLS]", "text": "[author]\n[NA]\n! [CLS] [author] [NA] ! Figure 3: ZAR as argument selection.\nmodel. However, these methods cannot be adopted for our purpose because we want BERT itself to learn from MT. Imamura and Sumita (2019) manage to maintain the straightforward approach by adopting a twostage training procedure: In the first stage, only the decoder is updated with the encoder frozen, while in the second stage, the entire model is updated. Although they offer some insights, it remains unclear how best to exploit BERT when MT is an intermediate task, not the target task.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Proposed Method", "text": "We adopt a ZAR model of Ueda et al. (2020), which adds a thin layer on top of BERT during fine-tuning to solve ZAR and related tasks (Section 3.1). Instead of directly moving from MLM pretraining to fine-tuning on ZAR, we inject MT as an intermediate task (Section 3.2). In addition, we introduce the MLM training objective during the intermediate training (Section 3.3).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "BERT-based Model for ZAR", "text": "ZAR as argument selection As illustrated in Figure 3, the basic idea behind BERT-based ZAR is that given the powerful neural encoder, the joint task of omission detection and antecedent identification can be formalized as argument selection (Shibata and Kurohashi, 2018;Kurita et al., 2018;Ueda et al., 2020). Omission detection concerns whether a given predicate has an argument for a given case (relation). If not, the model must point to the special token [NULL]. Otherwise the model must identify the antecedent of the zero pronoun by pointing either to a token in the given text or to a special token reserved for exophora. Note that by getting the entire document as the input, the model can handle inter-sentential anaphora as well as intra-sentential anaphora. In practice, the input length limitation of BERT forces us to implement a sliding window approach. Also note that in this formulation, ZAR is naturally subsumed into verbal predicate analysis (VPA), which also covers instances where the predicate and the argument have a dependency relation and only the case marker is absent.\nFormally, the probability of the token t j being the argument of the predicate t i for case c is:\nP (t j |t i , c) = exp(s c (t j , t i )) j exp(s c (t j , t i ))(1)\ns c (t j , t i ) = v tanh(W c t j + U c t i ) (2)\nwhere t i is the context-aware embedding of t i provided by BERT, W c and U c are case-specific weight matrices, and v is a weight vector shared among cases. We output t j with the highest probability.\nFor each predicate, we repeat this for the nominative (NOM), accusative (ACC), and dative (DAT) cases, and another nominative case for the double nominative construction (NOM2). ", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "MT as an Intermediate Task", "text": "Our main proposal is to use MT as an intermediate task prior to fine-tuning on ZAR. Following Imamura and Sumita (2019) and Clinchant et al. (2019), we use a pretrained BERT to initialize the encoder part of the Transformer-based encoderdecoder model while the decoder is randomly initialized. After the intermediate training on MT, we extract the encoder and move on to fine-tuning on ZAR and related tasks (Figure 2). Specifically, we test the following two procedures for intermediate training:\nOne-stage optimization The entire model is updated throughout the training.\nTwo-stage optimization In the first stage, the encoder is frozen and only the decoder is updated. In the second stage, the entire model is updated (Imamura and Sumita, 2019).", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Incorporating MLM into MT", "text": "As discussed in Section 2.2, MT as an intermediate task reportedly harms target-task performance, probably because MT forces the model to forget what it has learned from MLM pretraining (catastrophic forgetting). To overcome this problem, we incorporate the MLM training objective into MT, as suggested by Pruksachatkun et al. (2020). Specifically, we mask some input tokens on the encoder Web News # of sentences 16,038 11,276 # of zeros 30,852 27,062 side and force the model to recover the original tokens, as depicted in the center of Figure 2. Our masking strategy is the same as BERT's (Devlin et al., 2019): We choose 15% of the tokens at random and 80% of them are replaced with [MASK], 10% of them with a random token, and the rest are unchanged. The corresponding losses are added to the MT loss function.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Experiments", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets", "text": "ZAR We used two corpora in our experiments: the Kyoto University Web Document Lead Corpus (Hangyo et al., 2012) and the Kyoto University Text Corpus (Kawahara et al., 2002). Based on their genres, we refer to them as the Web and News, respectively. These corpora have been widely used in previous studies (Shibata and Kurohashi, 2018;Kurita et al., 2018;Ueda et al., 2020). They contained manual annotation for predicate-argument structures (including zero anaphora) as well as word segmentation, part-of-speech tags, dependency relations, and coreference chains. We split the datasets into training, validation, and test sets following the published setting, where the ratio was around 0.75:0.1:0.15. Key statistics are shown in Table 1.\nMT We used a Japanese-English parallel corpus of newspaper articles distributed by the Yomiuri Shimbun. 3 It consisted of about 1.3 million sentence pairs 4 with sentence alignment scores. We discarded pairs with scores of 0. Because the task of interest, ZAR, required inter-sentential reasoning, consecutive sentences were concatenated into chunks, with the maximum number of tokens equal to that of ZAR. As a result, we obtained around 373,000, 21,000, and 21,000 chunks for the training, validation, and test data, respectively.\nJapanese sentences were split into words using the morphological analyzer MeCab with the Juman dictionary (Kudo et al., 2004). 5 Both Japanese and English texts underwent subword tokenization. We used Subword-NMT (Sennrich et al., 2016) for Japanese and SentencePiece (Kudo and Richardson, 2018) for English. We used separate vocabularies for Japanese and English, with the vocabulary sizes of around 32,000 and 16,000, respectively.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Model Settings", "text": "BERT We employed a Japanese BERT model with BPE segmentation distributed by NICT. 6 It had the same architecture as Google's BERT-Base (Devlin et al., 2019): 12 layers, 768 hidden units, and 12 attention heads. It was trained on the full text of Japanese Wikipedia for approximately 1 million steps.\nMT We used the Transformer encoder-decoder architecture (Vaswani et al., 2017). The encoder was initialized with BERT while the decoder was a randomly initialized six-layer Transformer. The numbers of hidden units and heads were set to be the same as BERT's (i.e., 768 units and 12 attention heads). We adopted Adam (Kingma and Ba, 2017) as the optimizer. We set the total number of epochs to 50. In two-stage optimization, the encoder was frozen during the first 15 epochs, then the entire model was updated for the remaining 35 epochs. We set a mini-batch size to about 500. The details of hyper-parameters are given in Appendix A.\nZAR For a fair comparison with Ueda et al. (2020), we used almost the same configuration as theirs. We dealt with all subtypes of ZAR: intra-sentential anaphora, inter-sentential anaphora, and exophora. For exophora, we targeted [author], [reader], and [unspecified person]. We set the maximum sequence length to 128. 7 All documents from the Web met this limitation. In the News corpus, however, many documents exceeded the sequence length of 128. For such documents, we divided the document into multiple parts such that it had the longest preceding contexts. The evaluation of ZAR was relaxed using a gold coreference chain. The model was trained on the mixture of both corpora and evaluated on each corpus. We used almost the same", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Models", "text": "Web News  hyper-parameters as Ueda et al. (2020), which are included in Appendix B. We decided to tune the training epochs for MT since we found that it slightly affected ZAR performance. We collected checkpoints at the interval of 5 epochs out of 45 epochs, in addition to the one with the lowest validation loss. They were all trained on ZAR, and we chose the one with the highest score on the validation set. We ran the model with 3 seeds on MT and with 3 seeds on ZAR, which resulted in 9 seed combinations. We report the mean and the standard deviation of the 9 runs.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Results", "text": "Table 2 summarizes the experimental results. Our baseline is Ueda et al. (2020), who drastically outperformed previous models, thanks to BERT. +MT refers to the model with intermediate training on MT while +MT w/ MLM corresponds to the model that incorporated the MLM objective into MT. We can see that MT combined with MLM performed the best and that the gains reached 1.6 points for both the Web and News.\nTables 3 and 4 provide more detailed results. For comparison, we performed additional pretraining with ordinary MLM on the Japanese part of the parallel corpus (denoted as +MLM), because the possibility remained that the model simply took advantage of additional data. The subsequent two blocks compare one-stage (unmarked) optimization with two-stage optimization. MT yielded gains on all settings. The gains were consistent across anaphora categories. Although +MLM somehow beat the baseline, it was outperformed by most models trained on MT, ruling out the possibility that the gains were solely attributed to extra data. We can conclude that Japanese ZAR benefits from parallel texts through neural transfer learning.\nTwo-stage optimization showed mixed results. It worked for the Web but did not for the News. What is worse, its combination with MLM led to performance degeneration on both datasets.\nMLM achieved superior performance as it worked well in all settings. The gains were larger with one-stage optimization than with two-stage optimization (1.4 vs. 0.3 on the Web).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Translation of Zero Pronouns", "text": "The experimental results demonstrate that MT helps ZAR, but why does it work? Unfortunately, conventional evaluation metrics for MT (e.g., BLEU) reveal little about the model's ability to handle zero anaphora. To address this problem, Shimazu et al. (2020) and Nagata and Morishita (2020) constructed Japanese-English parallel datasets that were designed to automatically evaluate MT models with regard to the translation of Japanese zero pronouns (ZPT). We used Shimazu et al.'s dataset for its larger data size. 10 To facilitate automatic evaluation of ZPT, this dataset paired a correct English sentence with an incorrect one. All we had to do was to calculate the ratio of instances for which the model assigned higher translation scores to the correct candidates. The only difference between the two sentences involved the translation of a Japanese zero pronoun.\nTo choose the correct one, the MT model must sometimes refer to preceding sentences. As in intermediate training, multiple source sentences were fed to the model to generate multiple target sentences. We prepended as many preceding sentences as possible given the limit of 128 tokens.\nIn addition, this dataset recorded d, the sentencelevel distance between the zero pronoun in question and its antecedent. The number of instances with d = 0 was 218 while the number of remaining instances was 506. We regarded the former as the instances of intra-sentential anaphora and the latter as the instances of inter-sentential anaphora.\nWe chose the model with the best performance (i.e., one-stage optimization with MLM). For each checkpoint we collected during intermediate training, we (1) measured the ZPT accuracy and (2) finetuned it to obtain the F1 score for ZAR. As before,   Web News intra-sentential anaphora 0.758 0.763 inter-sentential anaphora 0.871 0.879  5 shows the strong positive correlations between the two performance measures, especially the very strong correlation for inter-sentential anaphora. These results were in line with our speculation that the performance gains in ZAR stemmed from the model's increased ability to translate zero pronouns.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Why Is MLM so Effective?", "text": "The MLM objective during intermediate training on MT is shown to be very effective, but why? Pruksachatkun et al. (2020) conjecture that it would mitigate catastrophic forgetting (CF), but this is not the sole explanation. In fact, Konno et al. (2020) see token masking as a way to augment data.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Methods", "text": "Web News F1 F1 +MT 70.5 -57.7 -+MT w/ masking 71.1 0.6 57.8 0.1 +MT w/ MLM 71.9 1.4 58.3 0.6 To dig into this question, we conducted an ablation study by introducing a model with token masking but without the corresponding loss function (denoted as +MT w/ masking). We assume that this model was largely deprived of the power to mitigate CF while token masking still acted as a data augmenter.\nTable 6 shows the results. Not surprisingly, +MT w/ masking was beaten by +MT w/ MLM with large margins. However, it did outperform +MT, and the gain was particularly large for the Web. The fact that the contribution of the loss function was larger than that of token masking indicates that the improvements were mainly attributed to CF mitigation, but the contribution of token masking alone should not be overlooked.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Case Studies", "text": "To gain further insights, we compared ZAR results with English translations automatically generated by the corresponding MT model. Figure 4 gives two examples. It is no great surprise that the translation quality was not satisfactory because we did not fully optimize the model for it.\nIn the exmple of Figure 4(a), MT seems to have helped ZAR. The omitted nominative argument of \"\u3042\u308a\" (is) was correctly translated as \"the school\", and the model successfully identified its antecedent \"\u5b66\u6821\" (school) while the baseline failed.\nFigure 4(b) illustrates a limitation of the proposed approach. The omitted nominative argument of the predicate, \"\u3067\" (be), points to \"\u5b9a\u5409\" (Sadakichi, the father of Jutaro). Although the model correctly translated the zero pronoun as \"He\", it failed in ZAR. This is probably because not only \"\u5b9a\u5409 (Sadakichi)\" but also \"\u9f8d\u99ac\" (Ryoma) and \"\u91cd\u592a\u90ce\" (Jutaro) can be referred to as \"He\". When disambiguation is not required to generate an overt pronoun, MT is not very helpful.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "Note on Other Pretrained Models", "text": "Due to space limitation, we have limited our focus to BERT, but for the sake of future practitioners, we would like to briefly note that we extensively tested BART  and its variants before switching to BERT. Unlike BERT, BART is an encoder-decoder model pretrained on a monolingual corpus (original) or a non-parallel multilingual corpus (mBART) . Because MT requires the encoder-decoder architecture, maintaining the model architecture between pretraining and intermediate training looked promising to us.\nWe specifically tested (1) the officially distributed mBART model, (2) a BART model we pretrained on Japanese Wikipedia, and (3) an mBART model we pretrained on Japanese and English texts. During fine-tuning, we added the ZAR argument selection layer on top of either the encoder or the decoder.\nUnfortunately, gains from MT intermediate training were marginal for these models. A more serious problem was that they came close to but rarely outperformed the strong BERT baseline. We gave up identifying the cause of poorer performance because it was extremely hard to apply comparable experimental conditions to large pretrained models.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we proposed to exploit parallel texts for Japanese zero anaphora resolution (ZAR) by inserting machine translation (MT) as an intermediate task between masked language model (MLM) pretraining and fine-tuning on ZAR. Although previous studies reported negative results on the use of MT as an intermediate task, we demonstrated that it did work for Japanese ZAR. Our analysis suggests that the intermediate training on MT simultaneously improved the model's ability to translate Japanese zero pronouns and the ZAR performance.\nWe bridged the gap between BERT-based ZAR and the encoder-decoder architecture for MT by initializing the encoder part of the MT model with a pretrained BERT. Previous studies focusing on MT reported mixed results on this approach, but again, we demonstrated its considerable positive impact on ZAR. We found that incorporating the MLM objective into the intermediate training was particularly effective. Our experimental results were consistent with the speculation that MLM mitigated catastrophic forgetting during intermediate training.\nWith neural transfer learning, we successfully revived the old idea that Japanese ZAR can benefit from parallel texts (Nakaiwa, 1999). Thanks to the astonishing flexibility of neural networks, we would probably be able to connect ZAR to other tasks through transfer learning.  The example in which MT apparently helped ZAR. The nominative zero pronoun of \"\u3042\u308a\" (is) was correctly translated as \"the school\". The model also succeeded in identifying its antecedent \"\u5b66 \u6821\" (school). (b) The example in which MT was not helpful. The model successfully translated the nominative zero pronoun of the underlined predicate, \"\u3067\" (be), as \"He\". It misidentified its antecedent, however.    ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We thank the Yomiuri Shimbun for providing Japanese-English parallel texts. We are grateful for Nobuhiro Ueda' help in setting up the baseline model. We thank the anonymous reviewers for their insightful comments.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "", "text": "Although we followed Ueda et al. (2020) with respect to hyper-parameter settings, there was one exception. Verbal predicate analysis is conventionally divided into three types: overt, covert, and zero. While Ueda et al. (2020) excluded the easiest overt type from training, we targeted all the three types because we found slight performance improvements. The overt type covers situations", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "C Results on Validation Sets", "text": "Tables 9 and 10 show the performance on the validation sets.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D Relationship between Zero Anaphora", "text": "Resolution and Zero Pronoun Translatoin    ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "On the use of BERT for neural machine translation", "journal": "", "year": "2019", "authors": "Stephane Clinchant; Kweon Woo Jung; Vassilina Nikoulina"}, {"title": "Automatic construction of a pseudoannotatedzero anaphora corpus using a bilingual corpus", "journal": "", "year": "2017", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova ; Tomohide Shibata; Daisuke Kawahara; Sadao Kuroahshi"}, {"title": "Incorporating BERT into parallel sequence decoding with adapters", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Junliang Guo; Zhirui Zhang; Linli Xu;  Hao-Ran; Boxing Wei; Enhong Chen;  Chen"}, {"title": "Building a diverse document leads corpus annotated with semantic relations", "journal": "", "year": "2012", "authors": "Masatsugu Hangyo; Daisuke Kawahara; Sadao Kurohashi"}, {"title": "NAIST text corpus: Annotating predicate-argument and coreference relations in Japanese", "journal": "Springer", "year": "2017", "authors": "Ryu Iida; Mamoru Komachi; Naoya Inoue; Kentaro Inui; Yuji Matsumoto"}, {"title": "Intrasentential subject zero anaphora resolution using multi-column convolutional neural network", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Ryu Iida; Kentaro Torisawa; Jong-Hoon Oh; Canasai Kruengkrai; Julien Kloetzer"}, {"title": "Recycling a pre-trained BERT encoder for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Kenji Imamura; Eiichiro Sumita"}, {"title": "Construction of a Japanese relevance-tagged corpus", "journal": "", "year": "2002", "authors": "Daisuke Kawahara; Sadao Kurohashi; K\u00f4iti Hasida"}, {"title": "Zero-anaphora resolution in Korean based on deep language representation model: BERT. ETRI Journal", "journal": "", "year": "2021", "authors": "Youngtae Kim; Dongyul Ra; Soojong Lim"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2017", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Pseudo zero pronoun resolution improves zero anaphora resolution", "journal": "", "year": "2021", "authors": "Ryuto Konno; Shun Kiyono; Yuichiroh Matsubayashi; Hiroki Ouchi; Kentaro Inui"}, {"title": "An empirical study of contextual data augmentation for Japanese zero anaphora resolution", "journal": "", "year": "2020", "authors": "Ryuto Konno; Yuichiroh Matsubayashi; Shun Kiyono; Hiroki Ouchi; Ryo Takahashi; Kentaro Inui"}, {"title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"title": "Applying conditional random fields to Japanese morphological analysis", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Taku Kudo; Kaoru Yamamoto; Yuji Matsumoto"}, {"title": "Neural adversarial training for semisupervised Japanese predicate-argument structure analysis", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Shuhei Kurita; Daisuke Kawahara; Sadao Kurohashi"}, {"title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"title": "Generating and exploiting large-scale pseudo training data for zero pronoun resolution", "journal": "Long Papers", "year": "2017", "authors": "Ting Liu; Yiming Cui; Qingyu Yin; Wei-Nan Zhang; Shijin Wang; Guoping Hu"}, {"title": "Multilingual denoising pre-training for neural machine translation", "journal": "", "year": "2020", "authors": "Yinhan Liu; Jiatao Gu; Naman Goyal; Xian Li; Sergey Edunov; Marjan Ghazvininejad; Mike Lewis; Luke Zettlemoyer"}, {"title": "Techniques of English-Japanese translation", "journal": "Kurosio Publishers", "year": "2020", "authors": "Kojiro Nabeshima; Michael N Brooks"}, {"title": "A test set for discourse translation from Japanese to English", "journal": "", "year": "2020", "authors": "Masaaki Nagata; Makoto Morishita"}, {"title": "Automatic extraction of rules for anaphora resolution of Japanese zero pronouns in Japanese-English machine translation from aligned sentence pairs", "journal": "Machine Translation", "year": "1999", "authors": "Hiromi Nakaiwa"}, {"title": "Neural modeling of multi-predicate interactions for Japanese predicate argument structure analysis", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Hiroki Ouchi; Hiroyuki Shindo; Yuji Matsumoto"}, {"title": "Zero object resolution in Korean", "journal": "", "year": "2015", "authors": "Arum Park; Seunghee Lim; Munpyo Hong"}, {"title": "Deep contextualized word representations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Matthew Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"title": "Bowman", "journal": "", "year": "2018", "authors": "Jason Phang; Thibault F\u00e9vry; Samuel R "}, {"title": "", "journal": "", "year": "", "authors": "Yada Pruksachatkun; Jason Phang; Haokun Liu"}, {"title": "Intermediate-task transfer learning with pretrained language models: When and why does it work?", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": " Bowman"}, {"title": "A fully-lexicalized probabilistic model for Japanese zero anaphora resolution", "journal": "", "year": "2008", "authors": "Ryohei Sasano; Daisuke Kawahara; Sadao Kurohashi"}, {"title": "A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames", "journal": "", "year": "2011", "authors": "Ryohei Sasano; Sadao Kurohashi"}, {"title": "Neural machine translation of rare words with subword units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Entitycentric joint modeling of Japanese coreference resolution and predicate argument structure analysis", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Tomohide Shibata; Sadao Kurohashi"}, {"title": "Evaluation dataset for zero pronoun in Japanese to English translation", "journal": "", "year": "2020", "authors": "Sho Shimazu; Sho Takase; Toshiaki Nakazawa; Naoaki Okazaki"}, {"title": "Joint zero pronoun recovery and resolution using multi-task learning and BERT", "journal": "", "year": "2020", "authors": "Linfeng Song; Kun Xu; Yue Zhang; Jianshu Chen; Dong Yu"}, {"title": "BERT-based cohesion analysis of Japanese texts", "journal": "", "year": "2020", "authors": "Nobuhiro Ueda; Daisuke Kawahara; Sadao Kurohashi"}, {"title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Exploring and predicting transferability across NLP tasks", "journal": "", "year": "2020", "authors": "Tu Vu; Tong Wang; Tsendsuren Munkhdalai; Alessandro Sordoni; Adam Trischler; Andrew Mattarella-Micke; Subhransu Maji; Mohit Iyyer"}, {"title": "Bowman. 2019a. Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling", "journal": "Association for Computational Linguistics", "year": "", "authors": "Alex Wang; Jan Hula; Patrick Xia; Raghavendra Pappagari; R Thomas Mccoy; Roma Patel; Najoung Kim; Ian Tenney; Yinghui Huang; Katherin Yu; Shuning Jin; Berlin Chen; Benjamin Van Durme; Edouard Grave; Ellie Pavlick; Samuel R "}, {"title": "One model to learn both: Zero pronoun prediction and translation", "journal": "", "year": "2019", "authors": "Longyue Wang; Zhaopeng Tu; Xing Wang; Shuming Shi"}, {"title": "Learning to jointly translate and predict dropped pronouns with a shared reconstruction mechanism", "journal": "", "year": "2018", "authors": "Longyue Wang; Zhaopeng Tu; Andy Way; Qun Liu"}, {"title": "A novel approach to dropped pronoun translation", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Longyue Wang; Zhaopeng Tu; Xiaojun Zhang; Hang Li; Andy Way; Qun Liu"}, {"title": "Neural Japanese zero anaphora resolution using smoothed large-scale case frames with word embedding", "journal": "", "year": "2018", "authors": "Souta Yamashiro; Hitoshi Nishikawa; Takenobu Tokunaga"}, {"title": "Chinese zero pronoun resolution with deep memory network", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Qingyu Yin; Yu Zhang; Weinan Zhang; Ting Liu"}, {"title": "Dynamic attention aggregation with bert for neural machine translation", "journal": "IEEE", "year": "2020", "authors": "Jia-Rui Zhang; Hongzheng Li; Shumin Shi; Heyan Huang; Yue Hu; Xiangpeng Wei"}, {"title": "Incorporating BERT into neural machine translation", "journal": "", "year": "2020", "authors": "Jinhua Zhu; Yingce Xia; Lijun Wu; Di He; Tao Qin; Wengang Zhou; Houqiang Li; Tieyan Liu"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: (a) An example of Japanese zero anaphora. The nominative argument of the underlined predicate is omitted. The goal of the task is to detect the omission and to identify its antecedent \"son\". (b) The corresponding English text. The omitted argument in Japanese is present as a pronoun in English. (c) A Japanese-English pair(Nabeshima and Brooks, 2020, p. 74) whose correspondences are too obscure for rule-based transfer. Because Japanese generally avoids having inanimate agents with animate patients, the English inanimate-subject sentence corresponds to two animate-subject clauses in Japanese, with two exophoric references to the reader (i.e., you).", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: Overview of the proposed method. Left: The model is pretrained with the masked language model (MLM) objective (known as BERT). Center: The pretrained BERT is used to initialize the encoder part of the encoder-decoder, which is trained on MT with the MLM objective. Right: The encoder is extracted from the MT model and is fine-tuned on ZAR and related tasks. Note that some special tokens are omitted for simplicity.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure4: Two examples of ZAR and MT. Green, blue, and orange dotted lines represent the output of the baseline model, that of ours, and the gold standard, respectively. English sentences are generated by the corresponding MT (encoder-decoder) model. (a) The example in which MT apparently helped ZAR. The nominative zero pronoun of \"\u3042\u308a\" (is) was correctly translated as \"the school\". The model also succeeded in identifying its antecedent \"\u5b66 \u6821\" (school). (b) The example in which MT was not helpful. The model successfully translated the nominative zero pronoun of the underlined predicate, \"\u3067\" (be), as \"He\". It misidentified its antecedent, however.", "figure_data": ""}, {"figure_label": "a", "figure_type": "", "figure_id": "fig_3", "figure_caption": "( a )aRelationship between ZAR and ZPT for intra-sentential on the Web test set. between ZAR and ZPT for intra-sentential on the News test set.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Relationships between ZAR and ZPT.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Relationships between ZAR and ZPT.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "as the building block, My wife got my son several toys. He especially likes the red car.", "figure_data": "(a) \u59bb\u304c\u606f\u5b50\u306b\u3044\u304f\u3064\u304b \u304a\u3082\u3061\u3083\u3092 \u8cb7\u3063\u3066\u3042\u3052\u305f \u3002! =NOM \u8d64\u3044 \u8eca\u3092\u7279\u306b\u6c17\u306b\u5165\u3063\u3066\u3044\u308b \u3002(b) wife=NOMson=DATseveraltoy=ACCbuy.GER=give.PST! =NOMredcar=ACCespeciallylike.GER=be.NPST! =NOM \u3053\u306e \u30b1\u30fc\u30d6\u30eb\u30ab\u30fc\u306b \u4e57\u308c\u3070\" =NOM\u30c0\u30a6\u30f3\u30bf\u30a6\u30f3\u306b\u884c\u304d\u307e\u3059 \u3002[reader](c)! =NOMthiscable.car=LOCride-COND\" =NOMdowntown=LOCgo-POL-NPSTThis cable car will take you downtown."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Input representations We append some special tokens at the end of the input sequence:[NULL]   for null arguments, and [author],[reader], and [unspecified person] for exophora. The special token [NA] is also supplied for the reason given in the next paragraph. As is usual for BERT, the special tokens [CLS] and [SEP] are inserted at the beginning and end of the sequence, respectively. If a predicate or argument candidate is split into two or more subwords, the initial subword is used for argument selection.Multi-task learning FollowingUeda et al. (2020), we use a single model to simultaneously perform verbal predicate analysis (VPA), nominal predicate analysis (NPA), bridging anaphora resolution (BAR), and coreference resolution (CR). NPA is a variant of VPA in which verb-like nouns serve as predicates taking arguments. BAR is a special kind of anaphora resolution in which the antecedent fills a semantic gap of the anaphor (e.g., \"price\" takes something priced as its argument). CR identifies the antecedent and anaphor that refer to the same real-world entity, with the special token [NA] reserved for nouns without coreferent mentions. All of the four tasks can be formalized as argument selection as in Eq. (1). By sharing the BERT encoder, these interrelated tasks have an influence on each other during training. In addition, case-specific weights are shared between VPA and NPA while separate weights are used for BAR and CR. During training, we compute the losses equally for the four tasks.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The numbers of sentences and zero anaphors in each corpus.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "F1 scores on the test sets. *: with two-stage optimization.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "\u00b10.716 63.9 \u00b11.27 65.1 \u00b11.14 75.8 \u00b10.764 +MT 70.5 \u00b10.410 64.0 \u00b10.868 63.8 \u00b10.536 75.4 \u00b10.565 \u00b10.511 65.3 \u00b10.830 64.6 \u00b10.479 76.1 \u00b10.633 +MT w/ MLM (Two-stage) 71.7 \u00b10.393 65.0 \u00b10.641 64.4 \u00b10.844 76.7 \u00b10.478", "figure_data": "MethodsallintraWebinterexophoraUeda et al. (2020) 970.3---+MLM 71.0 +MT w/ MLM 71.9 \u00b10.416 65.4 \u00b10.697 65.2 \u00b11.0776.7 \u00b10.468+MT (Two-stage)71.4"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Breakdown of the F1 scores with standard deviations on the Web test set. Boldfaced scores indicate the best results in the corresponding categories. One-stage optimization with the MLM objective performed the best on all categories. \u00b10.466 63.2 \u00b10.723 50.1 \u00b10.761 55.7 \u00b10.700 +MT w/ MLM (Two-stage) 57.7 \u00b10.549 63.8 \u00b10.597 50.2 \u00b10.628 56.2 \u00b11.37", "figure_data": "Methods"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Breakdown of the F1 scores with standard deviations on the News test set. Boldfaced scores indicate the best results in the corresponding categories. One-stage optimization with the MLM objective performed the best on all categories but exophora.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Pearson's correlation coefficient between ZPT accuracies and ZAR F1 scores.", "figure_data": "scores were averaged over 3 different seeds.Through the course of intermediate training, weobserved almost steady increase in ZPT accuraciesand ZAR F1 scores until around the 30th epoch (thefour figures in Appendix D). Table"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Ablation study focusing on MLM. All models were trained with one-stage optimization. w/ masking indicates token masking without the corresponding loss function.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Twenty-two students of Osaka Korean pro-Pyongyang Korean high school students watched the final at Kintetsu National High School's Hanazono Stadium on Sunday .Although the school is only about five kilometers away from the stadium , it was unable to participate in the tournament because it was treated as a special-needs school.", "figure_data": "(a) \u7b2c\u4e03\u5341\u56db\u56de \u5168\u56fd\u9ad8\u6821 \u30e9\u30b0\u30d3\u30fc \u30d5\u30c3\u30c8\u30dc\u30fc\u30eb \u5927\u4f1a\u6e96\u6c7a\u52dd\u306e \u4e94\u65e5 \u3001 \u8fd1\u9244\u82b1\u5712\u30e9\u30b0\u30d3\u30fc\u5834\u306e\u30b9\u30bf\u30f3\u30c9\u3067\u306fORD 74 CLFnationalhigh.schoolrugbyfootballtournamentsemi final=GEN 5dayKintetsu Hanazono rugby field=GENstands=LOC=TOP\u5927\u962a \u671d\u9bae \u9ad8\u7d1a \u5b66\u6821 \u30e9\u30b0\u30d3\u30fc \u90e8\u54e1\u4e8c\u5341\u4e94\u4eba\u304c \u9752\u3044 \u30a6\u30a4\u30f3\u30c9\u30d6\u30ec\u30fc\u30ab\u30fc\u59ff\u3067\u89b3\u6226\u3057\u305f \u3002Osaka Koreahighschoolrugbyclub.member25 CLF=NOMbluewind breakerappearance=INSwatch=do.PST! -NOM \u540c \u30e9\u30b0\u30d3\u30fc \u5834\u304b\u3089 \u308f\u305a\u304b \u4e00\u30fb\u4e94\u30ad\u30ed\u306e\u81f3\u8fd1\u8ddd\u96e2\u306b\u3042\u308a \u306a\u304c\u3089 \u3001!-NOMsamerugbyfield=ABLonly1.5km=GENproximatedistance=DATbe.GERbut\u300c \u5404\u7a2e\u5b66\u6821 \u300d \u6271\u3044\u306e \u305f\u3081\u5927\u4f1a\u306b\u51fa\u5834\u3067\u304d\u306a\u304b\u3063\u305f \u304c \u3001 \u4eca\u5e74 \u3088\u3046\u3084\u304f\u82b1\u5712\u3078\u306e\u5922\u304c\u5b9f\u73fe\u3057\u305f \u3002miscellaneous school treatment=GENbecause.oftournament=DAT enter=can.NEG.PSTbutthis.yearfinallyHanazono=DAT=GEN dream=NOM realize=do.PST(b) \u6c5f\u6238\u306b\u5263\u8853\u4fee\u884c\u306b\u6765\u305f\u9f8d\u99ac\u306f \u3001\u5b9a\u5409\u306e\u9053\u5834\u306b\u5165\u9580\u3059\u308b \u3002Edo=DAT swordplay training=DATcome.PST Ryoma=TOPSadakichi=GEN gym=DATenter.NPSTRyoma, who came to the Edo period [UNK] 1603-1867 [UNK] to learn swords, entered a training school run by Sakakichi.\u9053\u5834\u306e gym=GEN\u7d4c\u55b6\u306f operation=TOP son=GEN Jutaro=DAT delegate-GER=be.NPST \u606f\u5b50\u306e \u91cd\u592a\u90ce\u306b \u4efb\u305b\u3066\u3044\u308b\u3002BASELINEHe is left to his son, Shigeta.OURSGOLD! -NOM\u9f8d\u99ac\u3084\u4f50\u90a3\u306e\u6210\u9577\u3092\u3058\u3063\u3068 \u898b\u5b88\u308b\u5fc3 \u512a\u3057\u3044 \u7236\u89aa\u3067\u3082\u3042\u308b \u3002[NULL]!-NOMRyoma=andSana=GEN growth=ACC steadily watch.NPST heartkindfather be.GER also be.NPSTHe is a gentle father who watches the growth of Ryoma and Sana ."}], "doi": "10.18653/v1/D19-5611"}