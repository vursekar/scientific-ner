{"authors": "Yufei Chen; Sheng Huang; Fang Wang; Weiwei Sun; Xiaojun Wan", "pub_date": "", "title": "Neural Maximum Subgraph Parsing for Cross-Domain Semantic Dependency Analysis", "abstract": "We present experiments for cross-domain semantic dependency analysis with a neural Maximum Subgraph parser. Our parser targets 1-endpoint-crossing, pagenumber-2 graphs which are a good fit to semantic dependency graphs, and utilizes an efficient dynamic programming algorithm for decoding. For disambiguation, the parser associates words with BiLSTM vectors and utilizes these vectors to assign scores to candidate dependencies. We conduct experiments on the data sets from Se-mEval 2015 as well as Chinese CCGBank. Our parser achieves very competitive results for both English and Chinese. To improve the parsing performance on cross-domain texts, we propose a data-oriented method to explore the linguistic generality encoded in English Resource Grammar, which is a precisionoriented, hand-crafted HPSG grammar, in an implicit way. Experiments demonstrate the effectiveness of our data-oriented method across a wide range of conditions.", "sections": [{"heading": "Introduction", "text": "Semantic Dependency Parsing (SDP) is defined as the task of recovering sentence-internal bilexical semantic dependency structures, which encode predicate-argument relationships for all content words. Such sentence-level semantic analysis of text is concerned with the characterization of events and is therefore important to understand the essential meaning of a natural language sentence. With the advent of many supporting resources, SDP has become a well-defined task with a substantial body of work and comparative evaluation. (Almeida and Martins, 2015;Du et al., 2015a;Zhang et al., 2016;Peng et al., 2017;Wang et al., 2018). Two SDP shared tasks have been run as part of the 2014 and 2015 International Workshops on Semantic Evaluation (SemEval) (Oepen et al., 2014(Oepen et al., , 2015.\nThere are two key dimensions of the data-driven dependency parsing approach: decoding and disambiguation. Existing decoding approaches to syntactic or semantic analysis into bilexical dependencies can be categorized into two dominant types: transition-based (Zhang et al., 2016;Wang et al., 2018) and graph-based, i.e., Maximum Subgraph (Kuhlmann and Jonsson, 2015;Cao et al., 2017a) approaches. For disambiguation, while early work on dependency parsing focused on global linear models, e.g., structured perceptron (Collins, 2002), recent work shows that deep learning techniques, e.g., LSTM (Hochreiter and Schmidhuber, 1997), is able to significantly advance the state-of-the-art of the parsing accuracy. From the above two perspectives, i.e., the decoding and disambiguation frameworks, we find that what is still underexploited is neural Maximum Subgraph parsing for highly constrained graph classes, e.g., noncrossing graphs. In this paper, we fill this gap in the literature by developing a neural Maximum Subgraph parser.\nPrevious work showed that the 1-endpointcrossing, pagenumber-2 (1EC/P2) graphs are an appropriate graph class for modeling semantic dependency structures (Cao et al., 2017a). In this paper, we build a parser that targets 1EC/P2 graphs. Based on an efficient first-order Maximum Subgraph decoder, we implement a data-driven parser that scores arcs based on stacked bidirectional-LSTM (BiLSTM) together with a multi-layer perceptron. Using the benchmark data sets from the SemEval 2015 Task 18 (Oepen et al., 2015), our parser gives very competitive results for English semantic parsing. To test the ability for crosslingual parsing, we also conduct experiments on the Chinese CCGBank (Tse and Curran, 2010) and Enju HPSGBank (Yu et al., 2010) data. Our parser plays equally well for Chinese, resulting in an error reduction of 23.5% and 9.4% over the best published result reported in Zhang et al. (2016) and Du et al. (2015b).\nMost studies on semantic parsing focused on the in-domain setting, meaning that both training and testing data are drawn from the same domain. Even a data-driven parsing system achieves a high in-domain accuracy, it usually performs rather poorly on the out-of-domain data (Oepen et al., 2015). How to build robust semantic dependency parsers that can learn across domains remains an under-addressed problem. To improve the cross-domain parsing performance, we propose a data-oriented model to explore the linguistic generality encoded in a hand-crafted, domainindependent, linguistically-precise English grammar, namely English Resource Grammar (ERG; Flickinger, 2000). In particular, we introduce a cost-sensitive training model to learn crossdomain semantic information implicitly encoded in WikiWoods (Flickinger et al., 2010), i.e., a corpus that collects the wikipedia 1 texts as well as their automatic syntactico-semantic annotations produced by ERG. Evaluation demonstrates the usefulness of the imperfect annotations automatically created by ERG.\nOur parser is available at https://github. com/draplater/msg-parser.", "n_publication_ref": 22, "n_figure_ref": 0}, {"heading": "Semantic Dependency Parsing", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Semantic Dependency Analysis", "text": "SDP is the task of mapping a natural language sentence into a formal meaning representation in the form of a dependency graph. Figure 1 shows an Minimal Recursion Semantics (MRS; Copestake et al., 2005) reduced semantic dependency analysis (Ivanova et al., 2012). In this example, the semantic analysis is represented as a labeled directed graph in which the vertices are tokens in the sentence. The graph abstracts away from syntactic analysis (e.g., the complementizer-thatand passive construction are excluded) and includes most semantically relevant non-anaphoric local (e.g., from \"wants\" to \"Mark\") and longdistance (e.g., from \"buy\" to \"company\") dependencies. The arc labels encode linguisticallymotivated, broadly-applicable semantic relations that are grounded under the type-driven semantics. It is worth noting that semantic dependency graphs are not necessarily trees: (1) a token may be multiply headed because a word can be the arguments of more than one predicate; (2) cycles are allowed if the direction of arcs are not taken into account.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Previous Work", "text": "Some recent work on parsing targets the graphstructured semantic representations that are more general than the tree representation. Existing approaches can be categorized into two dominant types: the transition-based (Zhang et al., 2016;Wang et al., 2018) and graph-based, i.e., Maximum Subgraph (Kuhlmann and Jonsson, 2015;Cao et al., 2017a), approaches. Previous investigations on transition-based string-to-semantic-graph parsing adopt many ideas from syntactic string-totree parsing, such as how to handle crossing arcs and how to perform neural disambiguation. Zhang et al. (2016) introduced two transition systems that can generate arbitrary graphs and augmented them into practical semantic dependency parsers with a structured perceptron model. Wang et al. (2018) evaluated the effectiveness of deep learning techniques for transition-based SDP. Kuhlmann and Jonsson (2015) proposed to formulate SDP as the search for the maximum subgraphs for some particular graph classes. This proposal is called Maximum Subgraph parsing, which is a generalization of the graph-based parsing framework for syntactic parsing. For arbitrary graphs, Du et al. (2015a) proved that the secondorder Maximum Subgraph problem is an NPhard problem. Nevertheless, Almeida and Martins (2015) and Du et al. (2015a) showed that dual decomposition is a practical technique to solve the problem. Considering more restricted graph classes, Kuhlmann and Jonsson (2015) introduced a dynamic programming algorithem for parsing to noncrossing graphs. Cao et al. (2017a;2017b) showed that 1EC/P2 graphs are more suitable for describing semantic graphs than the noncrossing graphs, and they also allow low-degree dynamic programming algorithms for decoding.", "n_publication_ref": 13, "n_figure_ref": 0}, {"heading": "A Neural Maximum Subgraph Parser", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Maximum Subgraph Parsing", "text": "Usually, syntactic dependency analysis employs the tree-shaped representation. Dependency parsing, thus, can be formulated as the search for a maximum spanning tree (MST) from an arcweighted (complete) graph. For SDP where the target representation are no longer trees, Kuhlmann and Jonsson (2015) proposed to generalize the MST model to other types of subgraphs. In general, dependency parsing is formulated as the search for Maximum Subgraph regarding to a particular graph class, viz. G: Given a graph G = (V, A), find a subset A \u2286 A with maximum total weight such that the induced subgraph G = (V, A ) belongs to G. Formally, we have the following optimization problem:\nG (s) = arg max H\u2208G(s,G) SCORE(H) = arg max H\u2208G(s,G) p in H SCOREPART(s, p) (1)\nHere, G(s, G) is the set of all graphs that belong to G and are compatible with s and G. For parsing, G is usually a complete graph. SCOREPART(s, p) evaluates whether a small subgraph p of a candidate graph H is a good partial analysis for sentence s.\nFor some graph classes and some types of score functions, there exists efficient algorithms for solving (1). For example, when G is the set of noncrossing graphs and SCOREPART is limited to handle individual dependencies, (1) can be solved in cubic-time (Kuhlmann and Jonsson, 2015).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Parsing to 1EC/P2 Graphs", "text": "Previous work showed that the Maximum Subgraph framework is not only elegant in theory but also effective in practice (Kuhlmann and Jonsson, 2015;Cao et al., 2017a,b). In particular, 1EC/P2 graphs are an appropriate graph class for modeling semantic dependency structures (Cao et al., 2017a). Figure 2 presents an example to illustrate the 1-endpoint-crossing property, while Figure 3 shows a case for pagenumber-2. Below we present the formal description of the two properties that are adopted from Pitler et al. (2013) and Kuhlmann and Jonsson (2015) respectively. Definition 1 A dependency graph is 1-Endpoint-Crossing if for any edge e, all edges that cross e share an endpoint p named pencil point.\nDefinition 2 A pagenumber-k graph means it consists at most k half-planes, and arcs on each half-plane are noncrossing.\nIf G is the set of 1-endpoint-crossing graphs or more restricted 1EC/P2 graphs, the optimization problem (1) in the first-order case can be solved in quintic-time (Cao et al., 2017a) by using dynamic programming. Furthermore, ignoring one linguistically-rare structure in 1EC/P2 graphs descreases the complexity to O(n 4 ) (Cao et al., 2017a). In this paper, we implement Cao et al. Cao et al. (2017a)'s algorithm as the basis of our parser.", "n_publication_ref": 8, "n_figure_ref": 2}, {"heading": "Disambiguation with an LSTM", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "The Architecture", "text": "A semantic graph mainly consists of two parts: the structural part and the label part. The former describes the predicate-argument relation in the sentence, and the latter describes the type of this relation. In our model, the structural part and the label part are regarded as independent of each other. We use a coarse-to-fine strategy: finding the maximum unlabeled subgraph first and assigning a label for every edge in this subgraph then. The motivation is to avoid the calculation of a number of unnecessary label scores in order to improve the processing efficiency.  candidate dependencies as well as their relation types. Figure 4 shows the architecture of our system.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Dense Representations", "text": "We use words as well as POS tags as clues for scoring an individual arc. In particular, we transform all of them into continuous and dense vectors. Inspired by Costa-juss\u00e0 and Fonollosa (2016)'s work, we utilize character-based embedding for low-frequency words, i.e., words that appear more than k times in the training data, and word-based embeddings for other words. The word-based embedding module applies the common lookup-table mechanism, while the character-based word embedding w i is implemented by extracting the features (denoted as c 1 , c 2 , . . . , c n ) within a character-based BiLSTM:\nx 1 : x n = BiLSTM(c 1 : c n ) w i = x 1 + x n", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Lexical Feature Extractor", "text": "The concatenation of word embedding w i and POS-tag embedding p i of each word in specific sentence is used as the input of BiLSTMs to extract context-related feature vectors r i for each position i.\na i = w i \u2295 p i r 1 : r n = BiLSTM(a 1 : a n )", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Factorized Scoring", "text": "In our first order model, the SCORE function evaluates the preference of a semantic dependency graph by considering every bilexical relation in this graph one by one. In particular, the corresponding SCOREPART function assigns a score to a candidate arc between word i and word j using a non-linear transform from the two feature vectors, viz. r i and r j , associated to the two words:\nSCOREPART(i, j) = W 2 \u2022 ReLU(W 1,1 \u2022 r i + W 1,2 \u2022 r j + b)\nThe assignment task for dependency labels can be regarded as a classification task. Our label scoring process is similar to the prediction of dependencies:\nLABEL(i, j) = arg max W 2 \u2022 ReLU(W 1,1 \u2022 r i + W 1,2 \u2022 r j + b) + b 2\nWe can see here the two local score functions explicitly utilize the positions of a semantic head and a semantic dependent. It is similar to the firstorder factorization as defined in a number of linear parsing models, e.g., the models defined by Martins and Almeida (2014) and Cao et al. (2017a).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Training", "text": "In order to update graphs which achieve high model scores but are actually wrong, we use a margin-based approach to compute loss from the gold graph G * and the best prediction\u011c under current model. We define the loss term as:\nmax(0, \u2206(G * ,\u011c) \u2212 SCORE(G * ) + SCORE(\u011c))\nThe margin objective \u2206 measures the similarity between the gold graph G * and the prediction G. Follow Peng et al. (2017)'s approach, we define \u2206 as weighted Hamming to trade off between precision and recall.  Banarescu et al., 2013). Different from data-driven syntactic parsing, semantic parsing for the first type of annotation can leverage a precision grammar-guided model. Such a model applies a rich set of precise linguistic rules to constrain their search for a preferable syntactic or semantic analysis. In recent years, several of these linguistically motivated parsing systems achieved high performances that are comparable or even superior to the treebank-based purely data-driven parsers. For example, using ERG (Flickinger, 2000), which provides precise linguistic analyses for a broad range of phenomena, as the the core engine, PET 2 (Callmeier, 2000) and ACE 3 produce better results than all existing datadriven semantic parsers for sentences that can be parsed by ERG.\nThe main weakness of the precision grammarguided parsers is their robustness with respect to both coverage and efficiency. Even for treebanking on the newswire data, i.e., the Wall Street Journal data from Penn TreeBank, ERG lacks analyses for c.a. 11% sentences (Oepen et al., 2015). For the texts from the web, e.g., tweets, this problem is much more serious. Moreover, checking all linguistic constraints makes a grammar-guided parser too slow for many realistic NLP applications. On the contrary, light-weight, data-driven parsers usually have complementary strengthes in terms of both coverage and efficiency.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "The Parser-Oriented Model", "text": "Intuitively, a hand-crafted precision grammar, e.g., ERG, reflects highly generalized properties of a particular language and is thus highly resilient to domain shifts. Accordingly, one should expect that a precision grammar-guided parser which guarantees the a rich set of domain-independent linguistic constraints to be met can be more robust to domain shifts than a purely data-driven parser.\nIn related work for syntactic parsing, Ivanova et al. (2013) showed that the ERG-based parser was more robust to domain variation than several representative data-driven parsers. Zhang and Wang (2009) proposed to derive features from syntactic parses generated by PET to assist a data-driven dependency tree parser and observed some encouraging results for cross-domain evaluation. However, there are at least two drawbacks of their ERG-guided parser based method:\n1. A considerable number of sentences cannot benefit from ERG since PET may produce no analysis.\n2. This method fails to take parsing efficiency into account.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Our Data-Oriented Model", "text": "In this paper, we introduce a new data-oriented strategy to consume a precision grammar. The key idea is to take a grammar as an imperfect annotator: We let a precision grammar-guided parser parse large-scale raw texts in an offline way, and then utilize the automatically generated analysis as imperfect training data. Because we only need raw texts to be parsed once, even if this process takes much time, it is still reasonable. A grammarguided parser cannot parse a considerable portion of data, but this will not cause serious problems because we can take an enormous amount of sentences as annotation candidates. Just considering the wikipedia, we can collect at least dozens of millions of comparatively high-quality sentences.\nAn essential problem of this method is that such imperfect annotations bring in annotation errors which may hurt parser training. To deal with this problem, we adopted a cost-sensitive training method to train our model on the extended training data. In each epoch, we trained on imperfect corpus first and then on gold-standard corpus. When processing an imperfect sentence, we do not take a loss into consideration if the loss of this sentence is too small. In particular, if a loss of a bilexical relation between two tokens is less than 0.05, we would exclude the loss. As for label assigning, we exclude losses less than 0.5. These threshold numbers are tuned on the development data.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Set-up for the Baseline System", "text": "To evaluate neural Maximum Subgraph parsing in practice, we first conduct experiments on the three English data sets, namely DM, PAS and PSD 4 , which are from the SemEval 2015 Task18 (Oepen et al., 2015). We use the \"standard\" training, validation, and test splits to facilitate comparisons. In other words, the data splitting policy follows the shared task. In addition to English parsing, we consider Chinese SDP and use two data sets: (1) Chinese PAS data provided by SemEval 2015, and\n(2) Chinese CCGBank (Tse and Curran, 2010) to evaluate the cross-lingual ability of our model. All the SemEval data sets are publicly available from LDC (Oepen et al., 2016). We use DyNet 5 to implement our neural models. We use the automatic batch technique (Neubig et al., 2017) in DyNet to perform mini-batch gradient descent training. The batch size is 32. The detailed network hyper-parameters are summarized in Table 2. We use the same pre-trained word embedding as Kiperwasser and Goldberg (2016).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Main Results of English Parsing", "text": "Table 1 lists the parsing accuracy of our system as well as the best published results in the literature for comparison. Results from other papers are of different yet representative decoding or disambiguation frameworks. Du et al. (2015a) sion of the two linear model-based parsers is comparable or even superior to our neural parser, but the recall is far behind.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Model Ensemble", "text": "Ensemble methods have been shown very helpful to boost the accuracy of neural network based parsing. We evaluate two ensemble methods, voting and score averaging. In the voting method, each model parses the sentence to graph respectively. An edge will exist on the combined graph only if more than half output graphs of these models contain this edge. The label of this edge will be the most common label. In the score averaging method, we use averaged score parts to get a maximum graph and classify labels.\nWe choose 3/10 kind of different initial parameters to train models for ensemble. Figure 5 shows the result of the two ensemble methods. The averaging method has slightly better performance on the 3 datasets. The performance of this method on test data is shown on Table 1.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Data for Cross-Domain Experiments", "text": "Since around 2001, the ERG has been accompanied by syntactico-semantic annotations, where for each sentence an annotator has selected the intended analysis among all alternatives licensed by the grammar. This derived resource, namly Redwoods 6 (Oepen et al., 2002;Flickinger et al., 2017), is a collection of hand-annotated corpora and consists of data sets from several distinct domains. Redwoods also includes (re)treebanking results of the first 22 sections of the venerable Wall Street Journal (WSJ) text and the section of Brown Corpus in the Penn Treebank (Marcus et al., 1993). The WSJ part is also known as Deep-Bank . The Brown corpus part is used as the out-of-domain test data by Se-mEval 2015. The DM data sets for both SemEval 2014 and 2015 SDP shared tasks are based on the RedWoods corpus.\nBesides gold standard annoations, Flickinger et al. (2010) built the WikiWoods corpus 7 , which provides automatically created annotations for the texts from wikipedia. The annotations are disambiguated using the MaxEnt model trained using redwoods without DeepBank. We use a small portion of Wikiwoods, which contains 857,329 sentences in total.\nTo evaluate the (positive) impact of ERG on out-of-domain parsing, we conduct experiments on the DM data. The first group of experiments are designed to be comparable with the results obtained by various participant systems of SemEval 2015. The detailed data set-up is as follows:\n\u2022 Test Data. We use the Brown corpus section which is provided by SemEval 2015.\n\u2022 Training Data. We use three data sets for training: (1) DeepBank, (2) RedWoods and\n(3) a small portion of WikiWoods reparsed using the MaxEnt model trained on Deep-Bank. We denote this reparsed WikiWoods as WikiWoods-ACE, since the HPSG analysis is provided by the ACE parser. To extract the semantic dependency graph, we use the pydelphin tool 8 .\nFor the second group of experiments, we use the section wsj21 from the DeepBank as test data, which is the official in-domain test of the SemEval 2015. The training data includes the \"RedWoods minus DeepBank\" annotations (RedwoodsWOD for short) as well as the official WikiWoods annotations. Note that the MaxEnt model used to obtain the official WikiWoods annotations are compatible with RedwoodswWOD. Due to the diversity of the RedwoodsWOD and DeepBank sentences, this set-up can also be viewed as an outof-domain evaluation.  WikiWoods to train another model, and leave out other parts of Redwoods. The performance improvement is more remarkable when providing more data, even though such data contains annotation errors. For the second group of experiments, we use the RedwoodsWOD sentences for training and the DeepBank WSJ sentences for evaluation. For this set-up, consistent improvements of the parser quality are observed.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Results of Cross-Domain Parsing", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results of Chinese Parsing", "text": "To test the ability for cross-lingual parsing, we conduct experiments on HPSG and CCG grounded semantic analyses respectively. The HPSG grounded analysis is provided by SemEval 2015 and the underlying framework is the same to the English PAS data. The CCG grounded analysis is from Chinese CCGBank. We use the same set-up as Zhang et al. (2016). Both data sets are transformed from Chinese TreeBank with two rich sets of heuristic rules (Yu et al., 2010;Tse and Curran, 2010). Table 4 and 5  Chinese POS tagging has a great impact on parsing. In this paper, we consider two POS taggers: a symbol-refined generative HMM tagger (SR-HMM) (Huang et al., 2009) and a BiLSTM-CRF model when assisting Chinese SDG. For the neural tagging model, in addition to a BiL-STM layer for encoding words, we set a BiLSTM layer for encoding characters, which supports us to derive character-level representations for all words. In particular, vectors from the characterlevel LSTM is concatenated with the pre-trained word embedding before feeding into the other word-level BiLSTM network to capture contextual information. The final module of our CRF tagger is a linear chain CRF which scores the output sequence by factoring it in local tag bi-grams. From Table 5, we can see that POS information is very important to Chinese SDP. This phenomenon is consist with Chinese syntactic parsing, including both constituency and dependency parsing. Mandarin Chinese is recognized as a morphology-poor language: POS tags are defined mainly according to words' distributional rather than morphological properties.   \"ZDSW\" is the system that obtained the best parsing accuracy on the Chinese CCGBank data in the literature.\nthe power of the RNN architecture to learn nonlocal dependencies and thus benefit our semantic dependency parser a lot.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "Parsing sentences to linguistically-rich semantic representations is a key goal of Natural Language Understanding. We introduce a new parser for semantic dependency analysis, which combines two promising parsing techniques, i.e., decoding based on Maximum Subgraph algorithms and disambiguation based on BiLSTMs. To our knowledge, this is the first neural Maximum Subgraph parser. Our parser significantly improves state-ofthe-art accuracy on three out of total four data sets from SemEval 2015 for English/Chinese parsing and the CCGBank data for Chinese parsing. We also propose a new data-oriented method to leverage ERG, a linguistically-motivated, hand-crafted grammar, to improve cross-domain performance. Experiments demonstrate the effectiveness of taking ERG as an imperfect annotator. We think this method can be re-used for other types of datadriven semantic parsing models.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgement", "text": "This work was supported by the National Natural Science Foundation of China (61772036, 61331011) and the Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). We thank the anonymous reviewers for their helpful comments. Weiwei Sun is the corresponding author.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Lisbon: Evaluating TurboSemanticParser on Multiple Languages and Out-of-Domain Data", "journal": "", "year": "2015", "authors": "C ; Mariana S Almeida; T Andr\u00e9; F Martins"}, {"title": "Abstract Meaning Representation for Sembanking", "journal": "", "year": "2013", "authors": "Laura Banarescu; Claire Bonial; Shu Cai; Madalina Georgescu; Kira Griffitt; Ulf Hermjakob; Kevin Knight; Philipp Koehn; Martha Palmer; Nathan Schneider"}, {"title": "Pet. a platform for experimentation with efficient hpsg processing techniques", "journal": "Journal of Natural Language Engineering", "year": "2000", "authors": "Ulrich Callmeier"}, {"title": "Parsing to 1-endpoint-crossing, pagenumber-2 graphs", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Junjie Cao; Sheng Huang; Weiwei Sun; Xiaojun Wan"}, {"title": "Quasi-second-order parsing for 1-endpoint-crossing, pagenumber-2 graphs", "journal": "", "year": "2017", "authors": "Junjie Cao; Sheng Huang; Weiwei Sun; Xiaojun Wan"}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Michael Collins"}, {"title": "Minimal Recursion Semantics: An introduction", "journal": "", "year": "2005", "authors": "Ann Copestake; Dan Flickinger; Carl Pollard; Ivan A Sag"}, {"title": "Character-based neural machine translation", "journal": "Short Papers", "year": "2016", "authors": "Marta R Costa-Juss\u00e0; A R Jos\u00e9;  Fonollosa"}, {"title": "A data-driven, factorization parser for CCG dependency structures", "journal": "Long Papers", "year": "2015", "authors": "Yantao Du; Weiwei Sun; Xiaojun Wan"}, {"title": "Peking: Building semantic dependency graphs with a hybrid parser", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Yantao Du; Fan Zhang; Xun Zhang; Weiwei Sun; Xiaojun Wan"}, {"title": "On building a more efficient grammar by exploiting types", "journal": "Nat. Lang. Eng", "year": "2000", "authors": "Dan Flickinger"}, {"title": "Sustainable Development and Refinement of Complex Linguistic Annotations at Scale", "journal": "Springer", "year": "2017", "authors": "Dan Flickinger; Stephan Oepen; Emily M Bender"}, {"title": "Wikiwoods: Syntacto-semantic annotation for English wikipedia", "journal": "", "year": "2010", "authors": "Dan Flickinger; Stephan Oepen; Gisle Ytrest\u00f8l"}, {"title": "Deepbank: A dynamically annotated treebank of the wall street journal", "journal": "", "year": "2012", "authors": "Daniel Flickinger; Yi Zhang; Valia Kordoni"}, {"title": "Long Short-Term Memory", "journal": "Neural Comput", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"title": "Improving a simple bigram hmm part-of-speech tagger by latent annotation and selftraining", "journal": "", "year": "2009", "authors": "Zhongqiang Huang; Vladimir Eidelman; Mary Harper"}, {"title": "On different approaches to syntactic analysis into bi-lexical dependencies. an empirical comparison of direct, PCFG-based, and HPSG-based parsers", "journal": "", "year": "2013", "authors": "Angelina Ivanova; Stephan Oepen; Rebecca Dridan; Dan Flickinger; Lilja \u00d8vrelid"}, {"title": "Who did what to whom? A contrastive study of syntacto-semantic dependencies", "journal": "", "year": "2012", "authors": "Angelina Ivanova; Stephan Oepen; Lilja \u00d8vrelid; Dan Flickinger"}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Eliyahu Kiperwasser; Yoav Goldberg"}, {"title": "Parsing to noncrossing dependency graphs", "journal": "Transactions of the Association for Computational Linguistics", "year": "2015", "authors": "Marco Kuhlmann; Peter Jonsson"}, {"title": "Building a large annotated corpus of English: the penn treebank", "journal": "Computational Linguistics", "year": "1993", "authors": "Mitchell P Marcus; Mary Ann Marcinkiewicz; Beatrice Santorini"}, {"title": "Priberam: A turbo semantic parser with second order features", "journal": "", "year": "2014", "authors": "F T Andr\u00e9; Mariana S C Martins;  Almeida"}, {"title": "On-the-fly operation batching in dynamic computation graphs", "journal": "", "year": "2017", "authors": "Graham Neubig; Yoav Goldberg; Chris Dyer"}, {"title": "Angelina Ivanova, and Zde\u0148ka Ure\u0161ov\u00e1", "journal": "", "year": "2016", "authors": "Stephan Oepen; Marco Kuhlmann; Yusuke Miyao; Daniel Zeman; Silvie Cinkov\u00e1; Dan Flickinger"}, {"title": "Broad-coverage semantic dependency parsing", "journal": "", "year": "2015-01", "authors": "Stephan Oepen; Marco Kuhlmann; Yusuke Miyao; Daniel Zeman; Silvie Cinkov\u00e1; Dan Flickinger"}, {"title": "Semeval 2014 task 8: Broad-coverage semantic dependency parsing", "journal": "", "year": "2014", "authors": "Stephan Oepen; Marco Kuhlmann; Yusuke Miyao; Daniel Zeman; Dan Flickinger; Jan Hajic; Angelina Ivanova; Yi Zhang"}, {"title": "The lingo redwoods treebank motivation and preliminary applications", "journal": "", "year": "2002", "authors": "Stephan Oepen; Kristina Toutanova; Stuart Shieber; Christopher Manning; Dan Flickinger; Thorsten Brants"}, {"title": "Deep multitask learning for semantic dependency parsing", "journal": "Vancouver", "year": "2017", "authors": "Hao Peng; Sam Thomson; Noah A Smith"}, {"title": "Finding optimal 1-endpoint-crossing trees", "journal": "TACL", "year": "2013", "authors": "Emily Pitler; Sampath Kannan; Mitchell Marcus"}, {"title": "Chinese CCGbank: extracting CCG derivations from the penn Chinese treebank", "journal": "", "year": "2010", "authors": "Daniel Tse; James R Curran"}, {"title": "A neural transition-based approach for semantic dependency graph parsing", "journal": "", "year": "2018", "authors": "Yuxuan Wang; Wanxiang Che; Jiang Guo; Ting Liu"}, {"title": "Semiautomatically developing Chinese hpsg grammar from the penn Chinese treebank for deep parsing", "journal": "", "year": "2010", "authors": "Kun Yu; Miyao Yusuke; Xiangli Wang; Takuya Matsuzaki; Junichi Tsujii"}, {"title": "Transition-based parsing for deep dependency structures", "journal": "Computational Linguistics", "year": "2016", "authors": "Xun Zhang; Yantao Du; Weiwei Sun; Xiaojun Wan"}, {"title": "Cross-domain dependency parsing using a deep linguistic grammar", "journal": "Association for Computational Linguistics", "year": "2009", "authors": "Yi Zhang; Rui Wang"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "1Figure 1: A fragment of a semantic dependency graph.", "figure_data": ""}, {"figure_label": "23", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :Figure 3 :23Figure 2: (a, c)'s crossing edges (b, d) and (b, e) share an endpoint b.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Following Kiperwasser and Goldberg (2016)'s successful experience on syntactic tree parsing andPeng et al. (2017)'s experience on semantic graph parsing, we employ a stacked bidirectional-LSTM (BiLSTM) based model to assign scores. In our system, the BiLSTM vectors associated with the input words are utilized to calculate scores for the", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: The architecture of the network when processingHe wants to go. The upper-left nonlinear transform is used for edge scoring while the upper right one is used for label scoring.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Labeled F1 relative to different ensemble methods. Results are obtained on the development data.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_5", "figure_caption": "presents all results. Our parser significantly outperforms Zhang et al. (2016)'s Zhang et al. (2016) system on Chinese CCGBank, which achieved best reported performance.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "They are parallel with the deep syntactic analysis, and the semantic construction process of them is strictly compositional. Another type of domainindependent, sentence-level semantic annotations are based on annotators' reflection of the meanings of particular natural language sentences. No syntactic constraints on linguistic signals are introduced explicitly introduced. A representative example is Abstract Meaning Representation (AMR;", "figure_data": "4 Cross-Domain Parsing with a PrecisionGrammar and a Data-Oriented Model4.1 Precision Grammar-Guided ParsingSemantic dependency graphs like Minimal Recur-"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Du et al. ensemble 90.93 87.32 89.09 92.90 89.67 91.26 78.60 72.93 75.66 Almeida and Martins single 89.84 86.64 88.21 91.87 89.92 90.88 78.62 74.23 76.36 Peng et al.", "figure_data": "SystemDMPASPSDLPLRLFLPLRLFLPLRLFIN-DOMAINPeng et al. Wang et al. Wang et al. Ourssingle multitask single ensemble single--------90.74 90.40 90.57 92.26 92.43 92.35 76.42 76.33 76.38 --89.4 ----92.2 ----77.6 --90.4 ----92.7 ----78.5 --89.3 ----91.4 ----76.1 --90.3 ----91.7 ----78.6Ours (E[3])ensemble 92.17 91.35 91.76 93.50 92.98 93.24 78.83 77.07 77.95Ours ([E10])ensemble 92.81 91.65 92.23 93.91 93.22 93.56 79.33 78.00 78.66Du et al.ensemble 84.29 79.53 81.84 89.47 85.10 87.23 77.36 69.61 73.28OUT-OF-DOMAINAlmeida and Martins single Peng et al. single Peng et al. multitask Wang et al. single Wang et al. ensemble Ours single Ours (E[3]) ensemble 87.65 86.24 86.94 90.72 89.31 90.01 76.10 73.83 74.95 84.81 78.90 81.75 88.52 85.30 86.88 78.68 71.31 74.82 ----84.5 ----88.3 ----75.3 ----85.3 ----89.0 ----76.4 ----83.2 ----87.2 ----73.2 ----84.9 ----87.6 ----75.9 85.70 85.02 85.37 89.11 88.85 88.98 73.54 73.19 73.36Ours (E[10])ensemble 88.13 86.37 87.24 91.19 89.50 90.34 76.75 74.48 75.60Table 1: Labeled F1 on the test data from SemEval 2015.Hyper-parameterValRandomly-initialized word embedding dimension100Pre-trained word embedding dimension100Randomly-initialized character embedding dimension 100Character LSTM layers for each direction2Randomly-initialized POS-Tag embedding dimension50POS-Tag dropout0.5Batch size32BiLSTM dimension for each direction150BiLSTM layers5MLP hidden layers1MLP hidden layer dimension100"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Hyper-parameter setting of our model.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "summarizes experimental results for dif-ferent cross-domain evaluation set-ups. For the"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Labeled F1 on the DM test sets. \"S\" denotes single model, while \"E[3]\" denotes ensemble model with 3 sub-models. first group of experiments, we test the parser using different training data sets. The baseline utilizes the WSJ portion only. While more reliable training data is added, the performances increase consistently. We notice that the improvement extending the training data from DeepBank to Redwoods is quite limited for the out-of-domain evaluation. One reason is that the amount of enlarged gold standard annotations is still limited:", "figure_data": "The DeepBank training data contains 35,656 sen-tences (838,374 tokens, i.e., roughly words), whilethe additional training data contains 35,950 sen-tences (538,659 tokens). For comparison, we se-lect 480,564 sentences (5,346,703 tokens) from"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "The LSTM-based tagger can leverage Model LP LR LF Peking 84.75 82.15 83.43 Ours 85.49 84.11 84.79", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Labeled F1 on the test set of SemEval 2015 for Chinese. \"Peking\" is the participant system that obtained the best parsing accuracy for Chinese in SemEval 2015.", "figure_data": "ModelPOSLPLRLFZDSWGold82.09 81.81 81.95OursGold86.37 86.00 86.19SR-HMM80.19 80.53 80.37BiLSTM-CRF 81.13 81.74 81.43"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Labeled F1 on the test set of Chinese CCGBank.", "figure_data": ""}], "doi": ""}