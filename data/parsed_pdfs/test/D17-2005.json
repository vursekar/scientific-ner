{"authors": "Felix Stahlberg; Eva Hasler; Danielle Saunders; Bill Byrne", "pub_date": "", "title": "SGNMT -A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies", "abstract": "This paper introduces SGNMT, our experimental platform for machine translation research. SGNMT provides a generic interface to neural and symbolic scoring modules (predictors) with left-to-right semantic such as translation models like NMT, language models, translation lattices, n-best lists or other kinds of scores and constraints. Predictors can be combined with other predictors to form complex decoding tasks. SGNMT implements a number of search strategies for traversing the space spanned by the predictors which are appropriate for different predictor constellations. Adding new predictors or decoding strategies is particularly easy, making it a very efficient tool for prototyping new research ideas. SGNMT is actively being used by students in the MPhil program in Machine Learning, Speech and Language Technology at the University of Cambridge for course work and theses, as well as for most of the research work in our group.", "sections": [{"heading": "Introduction", "text": "We are developing an open source decoding framework called SGNMT, short for Syntactically Guided Neural Machine Translation. 1 The software package supports a number of well-known frameworks, including TensorFlow 2 (Abadi et al., 2016), OpenFST (Allauzen et al., 2007), Blocks/Theano (Bastien et al., 2012;van Merri\u00ebnboer et al., 2015), and NPLM (Vaswani et al., 2013). The two central concepts in the SGNMT tool are predictors and decoders. Predictors are scoring modules which define scores over the target language vocabulary given the current internal predictor state, the history, the source sentence, and external side information. Scores from multiple, diverse predictors can be combined for use in decoding.\nDecoders are search strategies which traverse the space spanned by the predictors. SGNMT provides implementations of common search tree traversal algorithms like beam search. Since decoders differ in runtime complexity and the kind of search errors they make, different decoders are appropriate for different predictor constellations.\nThe strict separation of scoring module and search strategy and the decoupling of scoring modules from each other makes SGNMT a very flexible decoding tool for neural and symbolic models which is applicable not only to machine translation. SGNMT is based on the OpenFSTbased Cambridge SMT system (Allauzen et al., 2014). Although the system is less than a year old, we have found it to be very flexible and easy for new researchers to adopt. Our group has already integrated SGNMT into most of its research work.\nWe also find that SGNMT is very well-suited for teaching and student research projects. In the 2015-16 academic year, two students on the Cambridge MPhil in Machine Learning, Speech and Language Technology used SGNMT for their dissertation projects. 3 The first project involved using SGNMT with OpenFST for applying subword models in SMT (Gao, 2016). The second project developed automatic music composition by LSTMs where WFSAs were used to define the space of allowable chord progressions in 'Bach' chorales (Tomczak, 2016  that the chorales must obey. This second project in particular demonstrates the versatility of the approach. For the current, 2016-17 academic year, SGNMT is being used heavily in two courses.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Predictors", "text": "SGNMT consequently emphasizes flexibility and extensibility by providing a common interface to a wide range of constraints or models used in MT research. The concept facilitates quick prototyping of new research ideas. Our platform aims to minimize the effort required for implementation; decoding speed is secondary as optimized code for production systems can be produced once an idea has been proven successful in the SGNMT framework. In SGNMT, scores are assigned to partial hypotheses via one or many predictors. One predictor usually has a single responsibility as it represents a single model or type of constraint. Predictors need to implement the following methods:\n\u2022 initialize(src sentence) Initialize the predictor state using the source sentence.\n\u2022 get state() Get the internal predictor state.\n\u2022 set state(state) Set the internal predictor state.\n\u2022 predict next() Given the internal predictor state, produce the posterior over target tokens for the next position.\nPredictor Description nmt Attention-based neural machine translation following . Supports Blocks/Theano (Bastien et al., 2012;van Merri\u00ebnboer et al., 2015) and TensorFlow (Abadi et al., 2016). fst\nPredictor for rescoring deterministic lattices   (Heafield et al., 2013;Stolcke et al., 2002) toolkit. nplm\nNeural n-gram language models based on NPLM (Vaswani et al., 2013). rnnlm\nIntegrates RNN language models with TensorFlow as described by Zaremba et al. (2014). forced Forced decoding with a single reference. forcedlst n-best list rescoring. bow\nRestricts the search space to a bag of words with or without repetition   \u2022 consume(token) Update the internal predictor state by adding token to the current history.\nThe structure of the predictor state and the implementations of these methods differ substantially between predictors. Tab. 2 lists all predictors which are currently implemented. Tab. 1 summarizes the semantics of this interface for three very common predictors: the neural machine translation (NMT) predictor, the (deterministic) finite state transducer (FST) predictor for lattice rescoring, and the n-gram predictor for applying n-gram language models. We also included two examples (word count and UNK count) which do not have a natural left-to-right semantic but can still be represented as predictors.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Example Predictor Constellations", "text": "SGNMT allows combining any number of predictors and even multiple instances of the same predictor type. In case of multiple predictors we combine the predictor scores in a linear model. The following list illustrates that various interesting decoding tasks can be formulated as predictor combinations.\n\u2022 nmt: A single NMT predictor represents pure NMT decoding.\n\u2022 nmt,nmt,nmt: Using multiple NMT predictors is a natural way to represent ensemble decoding (Hansen and Salamon, 1990; in our framework.\n\u2022 fst,nmt: NMT decoding constrained to an FST. This can be used for neural lattice rescoring  or other kinds of constraints, for example in the context of source side simplification in MT  or chord progressions in 'Bach' (Tomczak, 2016). The fst predictor can also be used to restrict the output of character-based or subword-unit-based NMT to a large word-level vocabulary encoded as FSA.\n\u2022 nmt,rnnlm,srilm,nplm: Combining NMT with three kinds of language models: An RNNLM (Zaremba et al., 2014), a Kneser-Ney n-gram LM (Heafield et al., 2013;Stolcke et al., 2002), and a feedforward neural network LM (Vaswani et al., 2013).  ", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Decoders", "text": "Decoders are algorithms to search for the highest scoring hypothesis. The list of predictors determines how (partial) hypotheses are scored by implementing the methods initialize(\u2022), get state(), set state(\u2022), predict next(), and consume(\u2022). The Decoder class implements versions of these methods which apply to all predictors in the list.\ninitialize(\u2022) is always called prior to decoding a new sentence. Many popular search strategies can be described via the remaining methods get state(), set state(\u2022), predict next(), and consume(\u2022). Algs. 1 and 2 show how to define greedy and beam decoding in this way. 45 Tab. 3 contains a list of currently implemented decoders. The UML diagram in Fig. 1 illustrates the relation between decoders and predictors. Algorithm 1 Greedy(src sen)\n1: initialize(src sen) 2: h \u2190 <s> 3: repeat 4:\nP \u2190predict next()\n5: (t, c) \u2190 arg max (t ,c )\u2208P c 6: h \u2190 h \u2022 t 7:\nconsume(t) 8: until t = </s> 9: return h NMT batch decoding The flexibility of the predictor framework comes with degradation in decoding time. SGNMT provides two ways of speeding up pure NMT decoding, especially on the GPU. The vanilla decoding strategy exposes the beam search implementation in Blocks (van Merri\u00ebnboer et al., 2015) which processes all active hypotheses in the beam in parallel. We also implemented a beam decoder version which decodes multiple sentences at once (batch decoding) rather than in a sequential order. Batch decoding is potentially more efficient since larger batches can make better use of GPU parallelism. The key concepts of our batch decoder implementation are:\n\u2022 We use a scheduler running on a separate CPU thread to construct large batches of computation (GPU jobs) from multiple sentences and feeding them to the jobs queue.\n\u2022 The GPU is operated by a single thread which communicates with the CPU scheduler thread via queues containing jobs. This thread is only responsible for retrieving jobs in the jobs queue, computing them, and putting them in the jobs results queue, minimizing the down-time of GPU computation.\n\u2022 Yet another CPU thread is responsible for processing the results computed on the GPU \nH next \u2190 H next \u222a (t ,c )\u2208P (h \u2022 t , c + c , s) 9:\nend for 10:\nH \u2190 \u2205 11:\nfor all (h, c, s) \u2208 n-best(H next ) do end for 16: until Best hypothesis in H ends with </s> 17: return Best hypothesis in H in the job results queue, e.g. by getting the n-best words from the posteriors. Processed jobs are sent back to the CPU scheduler where they are reassembled into new jobs. This decoder is able to translate the WMT English-French test sets news-test2012 to news-test2014 on a Titan X GPU with 911.6 words per second with the word-based NMT model described in Stahlberg et al. (2016). 6 This decoding speed seems to be slightly faster than sequential decoding with high-performance NMT decoders like Marian-NMT (Junczys-Dowmunt et al., 2016) with reported decoding speeds of 865 words per second. 7 However, batch decoding with Marian-NMT is much faster reaching over 4,500 words per second. 8 We think that these differences are mainly due to the limited multithreading support and performance in Python especially when using external libraries as opposed to the highly optimized C++ code in Marian-NMT. We did not push for even faster decoding as speed is not a major design goal of SGNMT. Note that batch decoding bypasses the predictor framework and can only be used for pure NMT decoding.\nEnsembling with models at multiple tokenization levels SGNMT allows masking predictors with alternative sets of modelling units. The conversion between the tokenization schemes of different predictors is defined with FSTs. This makes it possible to decode by combining scores from both a subword-unit (BPE) based NMT (Sennrich et al., 2016) and a word-based NMT model with character-based NMT, masking the BPE-based and word-based NMT predictors with FSTs which transduce character sequences to BPE or word sequences. Masking is transparent to the decoding strategy as predictors are replaced by a special wrapper (fsttok) that uses the masking FST to translate predict next() and consume() calls to (a series of) predictor calls with alternative tokens. The syncbeam variation of beam search compares competing hypotheses only after consuming a special word boundary symbol rather than after each token. This allows combining scores at the word level even when using models with multiple levels of tokenization. Joint decoding with different tokenization schemes has the potential of combining the benefits of the different schemes: character-and BPE-based models are able to address rare words, but word-based NMT can model long-range dependencies more efficiently.\nSystem-level combination We showed in Sec. 2.1 how to formulate NMT ensembling as a set of NMT predictors. Ensembling averages the individual model scores in each decoding step. Alternatively, system-level combination decodes the entire sentence with each model separately, and selects the best scoring complete hypothesis over all models. In our experiments, system-level combination is not as effective as en-1080), (b) a different training and test set, (c) a slightly different network architecture, and (d) words rather than subword units.\n8 https://marian-nmt.github.io/ features/ sembling but still leads to moderate gains for pure NMT. However, a trivial implementation which selects the best translation in a postprocessing step after separate decoding runs is slow. The sepbeam decoding strategy reduces the runtime of system-level combination to the single system level. The strategy applies only one predictor rather than a linear combination of all predictors to expand a hypothesis. The single predictor is linked by the parent hypothesis. The initial stack in sepbeam contains hypotheses for each predictor (i.e. system) rather than only one as in normal beam search. We report a moderate gain of 0.5 BLEU over a single system on the Japanese-English ASPEC test set (Nakazawa et al., 2016) by combining three BPE-based NMT models from  using the sepbeam decoder.\nIterative beam search Normal beam search is difficult to use in a time-constrained setting since the runtime depends on the target sentence length which is a priori not known, and it is therefore hard to choose the right beam size beforehand. The bucket search algorithm sidesteps the problem of setting the beam size by repeatedly performing small beam search passes until a fixed computational budget is exhausted. Bucket search produces an initial hypothesis very quickly, and keeps the partial hypotheses for each length in buckets. Subsequent beam search passes refine the initial hypothesis by iteratively updating these buckets.\nOur initial experiments suggest that bucket search often performs on a similar level as standard beam search with the benefit of being able to support hard time constraints. Unlike beam search, bucket search lends itself to risk-free (i.e. admissible) pruning since all partial hypotheses worse than the current best complete hypothesis can be discarded.", "n_publication_ref": 7, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "This paper presented our SGNMT platform for prototyping new approaches to MT which involve both neural and symbolic models. SGNMT supports a number of different models and constraints via a common interface (predictors), and various search strategies (decoders). Furthermore, SGNMT focuses on minimizing the implementation effort for adding new predictors and decoders by decoupling scoring modules from each other and from the search algorithm. SGNMT is actively being used for teaching and research and we welcome contributions to its development, for example by implementing new predictors for using models trained with other frameworks and tools.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "This work was supported by the U.K. Engineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1).", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "journal": "", "year": "2016", "authors": "Mart\u0131n Abadi; Ashish Agarwal; Paul Barham; Eugene Brevdo; Zhifeng Chen; Craig Citro; Greg S Corrado; Andy Davis; Jeffrey Dean; Matthieu Devin"}, {"title": "Pushdown automata in statistical machine translation", "journal": "Computational Linguistics", "year": "2014", "authors": "Cyril Allauzen; Bill Byrne; Adri\u00e0 De Gispert; Gonzalo Iglesias; Michael Riley"}, {"title": "OpenFst: A general and efficient weighted finite-state transducer library", "journal": "Springer", "year": "2007", "authors": "Cyril Allauzen; Michael Riley; Johan Schalkwyk"}, {"title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2015", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Theano: New features and speed improvements", "journal": "", "year": "2012", "authors": "Fr\u00e9d\u00e9ric Bastien; Pascal Lamblin; Razvan Pascanu; James Bergstra; Ian Goodfellow; Arnaud Bergeron; Nicolas Bouchard; David Warde-Farley; Yoshua Bengio"}, {"title": "Variable length word encodings for neural translation models. MPhil dissertation", "journal": "", "year": "2016", "authors": "Jiameng Gao"}, {"title": "Neural network ensembles", "journal": "", "year": "1990", "authors": "Lars Kai Hansen; Peter Salamon"}, {"title": "Source sentence simplification for statistical machine translation", "journal": "Computer Speech & Language", "year": "2016", "authors": "Eva Hasler; Adri\u00e0 De Gispert; Felix Stahlberg; Aurelien Waite; Bill Byrne"}, {"title": "A comparison of neural models for word ordering", "journal": "", "year": "2017", "authors": "Eva Hasler; Felix Stahlberg; Marcus Tomalin"}, {"title": "Scalable modified Kneser-Ney language model estimation", "journal": "", "year": "2013", "authors": "Kenneth Heafield; Ivan Pouzyrevsky; Jonathan H Clark; Philipp Koehn"}, {"title": "Is neural machine translation ready for deployment? a case study on 30 translation directions", "journal": "", "year": "2016", "authors": "Marcin Junczys-Dowmunt; Tomasz Dwojak; Hieu Hoang"}, {"title": "Blocks and fuel: Frameworks for deep learning", "journal": "", "year": "2015", "authors": "Dzmitry Bart Van Merri\u00ebnboer; Vincent Bahdanau; Dmitriy Dumoulin; David Serdyuk; Jan Warde-Farley; Yoshua Chorowski;  Bengio"}, {"title": "ASPEC: Asian scientific paper excerpt corpus", "journal": "", "year": "2016", "authors": "Toshiaki Nakazawa; Manabu Yaguchi; Kiyotaka Uchimoto; Masao Utiyama; Eiichiro Sumita; Sadao Kurohashi; Hitoshi Isahara"}, {"title": "Artificial Intelligence: A Modern Approach", "journal": "Pearson Education", "year": "2003", "authors": "J Stuart; Peter Russell;  Norvig"}, {"title": "Neural machine translation of rare words with subword units", "journal": "", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Efficient left-to-right hierarchical phrase-based translation with improved reordering", "journal": "", "year": "2013", "authors": "Maryam Siahbani; Anoop Baskaran Sankaran;  Sarkar"}, {"title": "Neural machine translation by minimising the Bayes-risk with respect to syntactic translation lattices", "journal": "", "year": "2017", "authors": "Felix Stahlberg; Adri\u00e0 De Gispert; Eva Hasler; Bill Byrne"}, {"title": "Syntactically guided neural machine translation", "journal": "", "year": "2016", "authors": "Felix Stahlberg; Eva Hasler; Aurelien Waite; Bill Byrne"}, {"title": "SRILM -an extensible language modeling toolkit", "journal": "", "year": "2002", "authors": "Andreas Stolcke"}, {"title": "Sequence to sequence learning with neural networks", "journal": "MIT Press", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; V Quoc;  Le"}, {"title": "", "journal": "", "year": "2016", "authors": "Marcin Tomczak"}, {"title": "Decoding with largescale neural language models improves translation", "journal": "", "year": "2013", "authors": "Ashish Vaswani; Yinggong Zhao; Victoria Fossum; David Chiang"}, {"title": "Recurrent neural network regularization", "journal": "", "year": "2014", "authors": "Wojciech Zaremba; Ilya Sutskever; Oriol Vinyals"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Reduced UML class diagram.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "). The LSTM provides the 'creativity' and the WFSA enforces constraints", "figure_data": "PredictorPredictor stateinitialize(\u2022)predict next()consume(token)NMTState vector in the GRURun encoder network toForward pass throughFeed back token toor LSTM layer of thecompute annotations.the decoder to computethe NMT network anddecoder network andthe posterior given theupdate the decoder statecurrent context vector.current decoder GRUand the context vector.or LSTM state and thecontext vector.FSTID of the current nodeLoad FST from the fileExplore all outgoingTraverse the outgoingin the FST.system, set the predic-edges of the currentedge from the currenttor state to the FST startnode and use arcnode labelled withnode.weights as scores.token and update thepredictor state to thetarget node.n-gramCurrent n-gram history Set the current n-gramReturn the LM scoresAdd token to the cur-history to the begin-of-for the current n-gramrent n-gram history.sentence symbol.history.Word count NoneEmptyReturn a cost of 1 for allEmptytokens except </s>.UNK count Number of consumedSet UNK counter to 0,For </s> use the log-IncreaseinternalUNK tokens.estimate the \u03bb parame-probability of the cur-counter by 1 if tokenter of the Poisson distri-rent number of UNKsis UNK.bution based on sourcegiven \u03bb. Use zero forsentence features.all other tokens."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Predictor operations for the NMT, FST, n-gram LM, and counting modules.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Currently implemented predictors.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": ". dfs Depth-first search. Efficiently enumerates the complete search space, e.g. for exhaustive FST-based rescoring.", "figure_data": "DecoderDescriptiongreedyGreedy decoding.beamBeam search as described in Bahdanauet al. (restarting Similar to DFS but with better admissi-ble pruning behaviour.astarA* search (Russell and Norvig, 2003).The heuristic function can be definedvia predictors.sepbeamAssociates hypotheses in the beam withonly one predictor. Efficiently approxi-mates system-level combination.syncbeam Beam search which compares hypothe-ses after consuming a special synchro-nization symbol rather than after eachiteration.bucketMultiple beam search passes with smallbeam size. Can have better pruning be-haviour than standard beam search.vanillaFast beam search decoder for (ensem-bled) NMT. This implementation issimilar to the decoder in Blocks (vanMerri\u00ebnboer et al., 2015) but can onlybe used for NMT as it bypasses the pre-dictor framework."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Currently implemented decoders.", "figure_data": ""}], "doi": ""}