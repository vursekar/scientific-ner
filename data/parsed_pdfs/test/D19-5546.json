{"authors": "Roman Grundkiewicz; Marcin Junczys-Dowmunt", "pub_date": "", "title": "Minimally-Augmented Grammatical Error Correction", "abstract": "There has been an increased interest in lowresource approaches to automatic grammatical error correction. We introduce Minimally-Augmented Grammatical Error Correction (MAGEC) that does not require any errorlabelled data. Our unsupervised approach is based on a simple but effective synthetic error generation method based on confusion sets from inverted spell-checkers. In low-resource settings, we outperform the current state-ofthe-art results for German and Russian GEC tasks by a large margin without using any real error-annotated training data. When combined with labelled data, our method can serve as an efficient pre-training technique.", "sections": [{"heading": "Introduction", "text": "Most neural approaches to automatic grammatical error correction (GEC) require error-labelled training data to achieve their best performance. Unfortunately, such resources are not easily available, particularly for languages other than English. This has lead to an increased interest in unsupervised and low-resource GEC (Rozovskaya et al., 2017;Bryant and Briscoe, 2018;Boyd, 2018;Rozovskaya and Roth, 2019), which recently culminated in the low-resource track of the Building Educational Application (BEA) shared task . 1 We present Minimally-Augmented Grammatical Error Correction (MAGEC), a simple but effective approach to unsupervised and low-resource GEC which does not require any authentic error-labelled training data. A neural sequence-to-sequence model is trained on clean and synthetically noised sentences alone. The noise is automatically created from confusion sets. Additionally, if labelled data 1 https://www.cl.cam.ac.uk/research/nl/ bea2019st is available for fine-tuning (Hinton and Salakhutdinov, 2006), MAGEC can also serve as an efficient pre-training technique.\nThe proposed unsupervised synthetic error generation method does not require a seed corpus with example errors as most other methods based on statistical error injection (Felice and Yuan, 2014) or back-translation models (Rei et al., 2017;Kasewa et al., 2018;Htut and Tetreault, 2019). It also outperforms noising techniques that rely on random word replacements (Xie et al., 2018;Zhao et al., 2019). Contrary to Ge et al. (2018) or Lichtarge et al. (2018), our approach can be easily used for effective pre-training of full encoder-decoder models as it is model-independent and only requires clean monolingual data and potentially an available spell-checker dictionary. 2 In comparison to pretraining with BERT (Devlin et al., 2019), synthetic errors provide more task-specific training examples than masking. As an unsupervised approach, MAGEC is an alternative to recently proposed language model (LM) based approaches (Bryant and Briscoe, 2018;Stahlberg et al., 2019), but it does not require any amount of annotated sentences for tuning.", "n_publication_ref": 17, "n_figure_ref": 0}, {"heading": "Minimally-augmented grammatical error correction", "text": "Our minimally-augmented GEC approach uses synthetic noise as its primary source of training data. We generate erroneous sentences from monolingual texts via random word perturbations selected from automatically created confusion sets. These are traditionally defined as sets of frequently confused words (Rozovskaya and Roth, 2010). We experiment with three unsupervised methods for generating confusion sets:  Edit distance Confusion sets consist of words with the shortest Levenshtein distance (Levenshtein, 1966) to the selected confused word.\nWord embeddings Confusion sets contain the most similar words to the confused word based on the cosine similarity of their word embedding vectors (Mikolov et al., 2013).\nSpell-breaking Confusion sets are composed of suggestions from a spell-checker; a suggestion list is extracted for the confused word regardless of its actual correctness.\nThese methods can be used to build confusion sets for any alphabetic language. 3 We find that confusion sets constructed via spell-breaking perform best (Section 4). Most context-free spell-checkers combine a weighted edit distance and phonetic algorithms to order suggestions, which produces reliable confusion sets (Table 1).\nWe synthesize erroneous sentences as follows: given a confusion set C i = {c i 1 , c i 2 , c i 3 , ...}, and the vocabulary V , we sample word w j \u2208 V from the input sentence with a probability approximated with a normal distribution N (p WER , 0.2), and perform one of the following operations: (1) substitution of w j with a random word c j k from its confusion set with probability p sub , (2) deletion of w j with p del , (3) insertion of a random word w k \u2208 V at j + 1 with p ins , and (4) swapping w j and w j+1 with p swp . When making a substitution, words within confusion sets are sampled uniformly.\nTo improve the model's capability of correcting spelling errors, inspired by Lichtarge et al. (2018); Xie et al. (2018), we randomly perturb 10% of characters using the same edit operations as above.  Character-level noise is introduced on top of the synthetic errors generated via confusion sets. A MAGEC model is trained solely on the synthetically noised data and then ensembled with a language model. Being limited only by the amount of clean monolingual data, this large-scale unsupervised approach can perform better than training on small authentic error corpora. A large amount of training examples increases the chance that synthetic errors resemble real error patterns and results in better language modelling properties.\nIf any small amount of error-annotated learner data is available, it can be used to fine-tune the pre-trained model and further boost its performance. Pre-training of decoders of GEC models from language models has been introduced by Junczys-Dowmunt et al. (2018b), we pretrain the full encoder-decoder models instead, as proposed by Grundkiewicz et al. (2019).", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Experiments", "text": "Data and evaluation Our approach requires a large amount of monolingual data that is used for generating synthetic training pairs. We use the publicly available News crawl data 5 released for the WMT shared tasks (Bojar et al., 2018). For English and German, we limit the size of the data to 100 million sentences; for Russian, we use all the available 80.5 million sentences.\nAs primary development and test data, we use the following learner corpora (Table 2):\n\u2022 English: the new W&I+LOCNESS corpus Granger, 1998) released for the BEA 2019 shared task and representing a diverse cross-section of English language;\n\u2022 German: the Falko-MERLIN GEC corpus (Boyd, 2018)   \u2022 Russian: the recently introduced RULEC-GEC dataset (Alsufieva et al., 2012;Rozovskaya and Roth, 2019) containing Russian texts from foreign and heritage speakers.\nUnless explicitly stated, we do not use the training parts of those datasets. For each language we follow the originally proposed preprocessing and evaluation settings. English and German data are tokenized with Spacy 6 , while Russian is preprocessed with Mystem (Segalovich, 2003). We additionally normalise punctuation in monolingual data using Moses scripts (Koehn et al., 2007). During training, we limit the vocabulary size to 32,000 subwords computed with SentencePiece using the unigram method (Kudo and Richardson, 2018).\nEnglish models are evaluated with ERRANT (Bryant et al., 2017) using F 0.5 ; for German and Russian, the M2Scorer with the MaxMatch metric (Dahlmeier and Ng, 2012) is used.\nSynthetic data Confusion sets are created for each language for V = 96, 000 most frequent lexical word forms from monolingual data. We use the Levenshtein distance to generate edit-distance based confusion sets. The maximum considered distance is 2. Word embeddings are computed with word2vec 7 from monolingual data. To generate spell-broken confusion sets we use Enchant 8 with Aspell dictionaries. 9 The size of confusion sets is limited to top 20 words.\nSynthetic errors are introduced into monolingual texts to mimic word error rate (WER) of about 15%, i.e. p WER = 0.15, which resembles error frequency in common ESL error corpora. When confusing a word, the probability p sub is set to 0.7, other probabilities are set to 0.1.  Training settings We adapt the recent state-ofthe-art GEC system by Junczys-Dowmunt et al. (2018b), an ensemble of sequence-to-sequence Transformer models (Vaswani et al., 2017) and a neural language model. 10 We use the training setting proposed by the authors 11 , but introduce stronger regularization: we increase dropout probabilities of source words to 0.3, add dropout on transformer self-attention and filter layers of 0.1, and use larger mini-batches with 2,500 sentences. We do not pre-train the decoder parameters with a language model and train directly on the synthetic data. We increase the size of language model used for ensembling to match the Transformer-big configuration (Vaswani et al., 2017) with 16-head self-attention, embeddings size of 1024 and feed-forward filter size size of 4096. In experiments with fine-tuning, the training hyperparameters remain unchanged.\nAll models are trained with Marian (Junczys-Dowmunt et al., 2018a). The training is continued for at most 5 epochs or until early-stopping is triggered after 5 stalled validation steps. We found that using 10,000 synthetic sentences as validation sets, i.e. a fully unsupervised approach, is as effective as using the development parts of error corpora and does not decrease the final performance.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Results and analysis", "text": "Confusion sets On English data, all proposed confusion set generation methods perform better than random word substitution (Table 3).Confusion sets based on word embeddings are the least effective, while spell-broken sets perform best at 26.66 F 0.5 . We observe further gains of +1.04 from keeping out-of-vocabulary spell-checker suggestions (OOV) and preserving consistent letter casing within confusion sets (Case).\nThe word error rate of error corpora is an useful statistic that can be used to balance precision/recall ratios (Rozovskaya and Roth, 2010;Junczys-Dowmunt et al., 2018b;Hotate et al., 2019). Increasing WER in the synthetic data from 15% to 25% increases recall at the expense of precision, but no overall improvement is observed. A noticeable recall gain that transfers to a higher F-score of 28.99 is achieved by increasing the importance of edited fragments with the edit-weighted MLE objective from Junczys-Dowmunt et al. (2018b) with \u039b = 2. We use this setting for the rest of our experiments.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Main results", "text": "We first compare the GEC systems with simple baselines using a greedy and context spell-checking (Table 4); the latter selects the best correction suggestion based on the sentence perplexity from a Transformer language model. All systems outperform the spell-checker baselines.\nOn German and Russian test sets, single MAGEC models without ensembling with a language model already achieve better performance than reported by Boyd (2018)    Roth (2019) for their systems that use authentic error-annotated data for training (Table 4b and 4c). Our best unsupervised ensemble systems that combine three Transformer models and a LM 12 outperform the state-of-the-art results for these languages by +7.0 and +11.4 F 0.5 .\nOur English models do not compete with the top systems (Grundkiewicz et al., 2019) from the BEA shared task trained on publicly available errorannotated corpora (Table 4a). It is difficult to compare with the top low-resource system from the shared task, because it uses additional parallel data from Wikipedia (Grundkiewicz and Junczys-Dowmunt, 2014), larger ensemble, and n-best list re-ranking with right-to-left models, which can be also implemented in this work.\nMAGEC systems are generally on par with the results achieved by a recent unsupervised contribution based on finite state transducers by Stahlberg et al. (2019) on the CoNLL-2014 (Dahlmeier et al., 2013) and JFLEG test sets (Napoles et al., 2017  All unsupervised systems benefit from domainadaptation via fine-tuning on authentic labelled data (Miceli Barone et al., 2017). The more authentic high-quality and in-domain training data is used, the greater the improvement, but even as few as~2,000 sentences are helpful (Fig. 1). We found that fine-tuning on a 2:1 mixture of synthetic and oversampled authentic data prevents the model from over-fitting. This is particularly visible for English which has the largest fine-tuning set (34K sentences), and the difference of 5.2 F 0.5 between finetuning with and without synthetic data is largest.", "n_publication_ref": 7, "n_figure_ref": 1}, {"heading": "Spelling and punctuation errors", "text": "The GEC task involves detection and correction of all types of error in written texts, including grammatical, lexical and orthographical errors. Spelling and punctuation errors are among the most frequent error types and also the easiest to synthesize.\nTo counter the argument that -mostly due to the introduced character-level noise and strong language modelling -MAGEC can only correct these \"simple\" errors, we evaluate it against test sets that contain either spelling and punctuation errors or all other error types; with the complement errors corrected (Table 6). Our systems indeed perform best on misspellings and punctuation errors, but are capable of correcting various error types. The disparity for Russian can be explained by the fact that it is a morphologically-rich language and we suffer from generally lower performance.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusions and future work", "text": "We have presented Minimally-Augmented Grammatical Error Correction (MAGEC), which can be effectively used in both unsupervised and lowresource scenarios. The method is model independent, requires easily available resources, and can be used for creating reliable baselines for supervised techniques or as an efficient pre-training method for neural GEC models with labelled data. We have demonstrated the effectiveness of our method and outperformed state-of-the-art results for German and Russian benchmarks, trained with labelled data, by a large margin.\nFor future work, we plan to evaluate MAGEC on more languages and experiment with more diversified confusion sets created with additional unsupervised generation methods.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Using flagship data to develop a Russian learner corpus of academic writing", "journal": "Russian Language Journal", "year": "2012", "authors": "Anna Alsufieva; Olesya Kisselev; Sandra Freels"}, {"title": "Findings of the 2018 conference on machine translation (WMT18)", "journal": "", "year": "2018", "authors": "Ond\u0159ej Bojar; Christian Federmann; Mark Fishel; Yvette Graham; Barry Haddow; Philipp Koehn; Christof Monz"}, {"title": "Using Wikipedia edits in low resource grammatical error correction", "journal": "", "year": "2018", "authors": "Adriane Boyd"}, {"title": "Language model based grammatical error correction without annotated training data", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Christopher Bryant; Ted Briscoe"}, {"title": "The BEA-2019 shared task on grammatical error correction", "journal": "", "year": "2019", "authors": "Christopher Bryant; Mariano Felice; \u00d8istein E Andersen; Ted Briscoe"}, {"title": "Automatic annotation and evaluation of error types for grammatical error correction", "journal": "", "year": "2017", "authors": "Christopher Bryant; Mariano Felice; Ted Briscoe"}, {"title": "Better evaluation for grammatical error correction", "journal": "", "year": "2012", "authors": "Daniel Dahlmeier; Hwee Tou Ng"}, {"title": "Building a large annotated corpus of learner English: The NUS corpus of learner English", "journal": "", "year": "2013", "authors": "Daniel Dahlmeier; Siew Mei Hwee Tou Ng;  Wu"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Generating artificial errors for grammatical error correction", "journal": "", "year": "2014", "authors": "Mariano Felice; Zheng Yuan"}, {"title": "Fluency boost learning and inference for neural grammatical error correction", "journal": "Long Papers", "year": "2018", "authors": "Tao Ge; Furu Wei; Ming Zhou"}, {"title": "The computer learner corpus: A versatile new source of data for SLA research", "journal": "Learner English on Computer", "year": "1998", "authors": "Sylviane Granger"}, {"title": "The WikEd error corpus: A corpus of corrective Wikipedia edits and its application to grammatical error correction", "journal": "Springer", "year": "2014", "authors": "Roman Grundkiewicz; Marcin Junczys-Dowmunt"}, {"title": "Neural grammatical error correction systems with unsupervised pre-training on synthetic data", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Roman Grundkiewicz; Marcin Junczys-Dowmunt; Kenneth Heafield"}, {"title": "Reducing the dimensionality of data with neural networks. science", "journal": "", "year": "2006", "authors": "E Geoffrey; Ruslan R Hinton;  Salakhutdinov"}, {"title": "Controlling grammatical error correction using word edit rate", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Kengo Hotate; Masahiro Kaneko; Satoru Katsumata; Mamoru Komachi"}, {"title": "The unbearable weight of generating artificial errors for grammatical error correction", "journal": "", "year": "2019", "authors": "Mon Phu; Joel Htut;  Tetreault"}, {"title": "Marian: Fast neural machine translation in C++", "journal": "", "year": "2018", "authors": "Marcin Junczys-Dowmunt; Roman Grundkiewicz; Tomasz Dwojak; Hieu Hoang; Kenneth Heafield; Tom Neckermann; Frank Seide; Ulrich Germann; Alham Fikri Aji; Nikolay Bogoychev; F T Andr\u00e9"}, {"title": "Approaching neural grammatical error correction as a low-resource machine translation task", "journal": "Long Papers", "year": "2018", "authors": "Marcin Junczys-Dowmunt; Roman Grundkiewicz; Shubha Guha; Kenneth Heafield"}, {"title": "Wronging a right: Generating better errors to improve grammatical error detection", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Sudhanshu Kasewa; Pontus Stenetorp; Sebastian Riedel"}, {"title": "Moses: Open source toolkit for statistical machine translation", "journal": "", "year": "2007", "authors": "Philipp Koehn; Hieu Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; Nicola Bertoldi; Brooke Cowan; Wade Shen; Christine Moran; Richard Zens; Chris Dyer; Ondrej Bojar; Alexandra Constantin; Evan Herbst"}, {"title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"title": "Binary codes capable of correcting deletions, insertions, and reversals", "journal": "", "year": "1966", "authors": " Vladimir I Levenshtein"}, {"title": "Weakly supervised grammatical error correction using iterative decoding", "journal": "CoRR", "year": "2018", "authors": "Jared Lichtarge; Christopher Alberti; Shankar Kumar; Noam Shazeer; Niki Parmar"}, {"title": "Regularization techniques for fine-tuning in neural machine translation", "journal": "", "year": "2017", "authors": "Antonio Valerio Miceli; Barry Barone; Ulrich Haddow; Rico Germann;  Sennrich"}, {"title": "Distributed representations of words and phrases and their compositionality", "journal": "Curran Associates, Inc", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"title": "JFLEG: A fluency corpus and benchmark for grammatical error correction", "journal": "", "year": "2017", "authors": "Courtney Napoles; Keisuke Sakaguchi; Joel Tetreault"}, {"title": "Artificial error generation with machine translation and syntactic patterns", "journal": "", "year": "2017", "authors": "Marek Rei; Mariano Felice; Zheng Yuan; Ted Briscoe"}, {"title": "Generating confusion sets for context-sensitive error correction", "journal": "", "year": "2010", "authors": "Alla Rozovskaya; Dan Roth"}, {"title": "Grammar error correction in morphologically-rich languages: The case of Russian", "journal": "Transactions of the Association for Computational Linguistics", "year": "2019", "authors": "Alla Rozovskaya; Dan Roth"}, {"title": "Adapting to learner errors with minimal supervision", "journal": "Comput. Linguist", "year": "2017", "authors": "Alla Rozovskaya; Dan Roth; Mark Sammons"}, {"title": "A fast morphological algorithm with unknown word guessing induced by a dictionary for a web search engine", "journal": "CSREA Press", "year": "2003", "authors": "Ilya Segalovich"}, {"title": "Neural grammatical error correction with finite state transducers", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Felix Stahlberg; Christopher Bryant; Bill Byrne"}, {"title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Noising and denoising natural language: Diverse backtranslation for grammar correction", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Ziang Xie; Guillaume Genthial; Stanley Xie; Andrew Ng; Dan Jurafsky"}, {"title": "Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data", "journal": "CoRR", "year": "2019", "authors": "Wei Zhao; Liang Wang; Kewei Shen; Ruoyu Jia; Jingming Liu"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Improvements from fine-tuning on subsets of W&I+LOCNESS Train. The smallest 1 16 part of the dataset contains 2,145 sentences. Averaged F-scores over 4 runs trained on different subsets of the data.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Examples of spell-broken confusion sets forEnglish, German and Russian."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Sizes of labelled corpora in no. of sentences.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Performance for different confusion sets and edit weighting techniques on W&I+LOCNESS Dev.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ": Unsupervised and fine-tuned MAGEC sys-tems for English, German and Russian, contrasted withsystems from related work and spell-checking base-lines."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ": Comparison with LM-based GEC on theCoNLL (M 2 ) and JFLEG (GLEU) test sets for unsuper-vised ( ) and supervised systems trained or fine-tunedon different amounts of labelled data."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Performance of single MAGEC w/ LM models on two groups of errors on respective development sets.", "figure_data": ""}], "doi": "10.18653/v1/W18-0529"}