{"authors": "Chong Zhang; Jieyu Zhao; Huan Zhang; Kai-Wei Chang; Cho-Jui Hsieh", "pub_date": "", "title": "Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation", "abstract": "Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these evaluations robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a \"double perturbation\" framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models' robustness and counterfactual bias in English. (1) For robustness, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed attack attains high success rates (96.0%-99.8%) in finding vulnerable examples on both original and robustly trained CNNs and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/ nlp-second-order-attack.", "sections": [{"heading": "Introduction", "text": "Recent studies show that NLP models are vulnerable to adversarial perturbations. A seemingly \"invariance transformation\" (a.k.a. adversarial perturbation) such as synonym substitutions (Alzantot et al., 2018;Zang et al., 2020) or syntax-guided paraphrasing (Iyyer et al., 2018;Huang and Chang, 2021) can alter the prediction. To mitigate the model vulnerability, robust training methods have been proposed and shown effective (Miyato et al., 2017;Jia et al., 2019;Huang et al., 2019;Zhou et al., 2020).\nx 0 =\"a deep and meaningful film (movie).\"", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "X test", "text": "x 0 =\"a short and moving film (movie).\" 73% positive (70% negative) (99% positive) 99% positive perturb Figure 1: A vulnerable example beyond the test dataset. Numbers on the bottom right are the sentiment predictions for film and movie. Blue x 0 comes from the test dataset and its prediction cannot be altered by the substitution film \u2192 movie (robust). Yellow examplex 0 is slightly perturbed but remains natural. Its prediction can be altered by the substitution (vulnerable).\nIn most studies, model robustness is evaluated based on a given test dataset or synthetic sentences constructed from templates (Ribeiro et al., 2020). Specifically, the robustness of a model is often evaluated by the ratio of test examples where the model prediction cannot be altered by semantic-invariant perturbation. We refer to this type of evaluations as the first-order robustness evaluation. However, even if a model is first-order robust on an input sentence x 0 , it is possible that the model is not robust on a natural sentencex 0 that is slightly modified from x 0 . In that case, adversarial examples still exist even if first-order attacks cannot find any of them from the given test dataset. Throughout this paper, we callx 0 a vulnerable example. The existence of such examples exposes weaknesses in models' understanding and presents challenges for model deployment. Fig. 1 illustrates an example.\nIn this paper, we propose the double perturbation framework for evaluating a stronger notion of second-order robustness. Given a test dataset, we consider a model to be second-order robust if there is no vulnerable example that can be identified in the neighborhood of given test instances ( \u00a72.2). In particular, our framework first perturbs the test set to construct the neighborhood, and then diagnoses the robustness regarding a single-word synonym substitution. Taking Fig. 2 as an example, the model is first-order robust on the input sentence x 0 (the prediction cannot be altered), but it is not second-order robust due to the existence of the vulnerable examplex 0 . Our framework is designed to identifyx 0 .\nWe apply the proposed framework and quantify second-order robustness through two second-order attacks ( \u00a73). We experiment with English sentiment classification on the SST-2 dataset (Socher et al., 2013) across various model architectures. Surprisingly, although robustly trained CNN (Jia et al., 2019) and Transformer (Xu et al., 2020) can achieve high robustness under strong attacks (Alzantot et al., 2018;Garg and Ramakrishnan, 2020) (23.0%-71.6% success rates), for around 96.0% of the test examples our attacks can find a vulnerable example by perturbing 1.3 words on average. This finding indicates that these robustly trained models, despite being first-order robust, are not second-order robust. Furthermore, we extend the double perturbation framework to evaluate counterfactual biases (Kusner et al., 2017) ( \u00a74) in English. When the test dataset is small, our framework can help improve the evaluation robustness by revealing the hidden biases not directly shown in the test dataset. Intuitively, a fair model should make the same prediction for nearly identical examples referencing different groups (Garg et al., 2019) with different protected attributes (e.g., gender, race). In our evaluation, we consider a model biased if substituting tokens associated with protected attributes changes the expected prediction, which is the average prediction among all examples within the neighborhood. For instance, a toxicity classifier is biased if it tends to increase the toxicity if we substitute straight \u2192 gay in an input sentence (Dixon et al., 2018). In the experiments, we evaluate the expected sentiment predictions on pairs of protected tokens (e.g., (he, she), (gay, straight)), and demonstrate that our method is able to reveal the hidden model biases.\nOur main contributions are: (1) We propose the double perturbation framework to diagnose the robustness of existing robustness and fairness evaluation methods. (2) We propose two second-order attacks to quantify the stronger notion of second-\nx 0x 0 x 0 x 1 negative positive\nFigure 2: An illustration of the decision boundary. Diamond area denotes invariance transformations. Blue x 0 is a robust input example (the entire diamond is green). Yellowx 0 is a vulnerable example in the neighborhood of x 0 . Redx 0 is an adversarial example tox 0 . Note: x 0 is not an adversarial example to x 0 since they have different meanings to human (outside the diamond).\norder robustness and reveal the models' vulnerabilities that cannot be identified by previous attacks.\n(3) We propose a counterfactual bias evaluation method to reveal the hidden model bias based on our double perturbation framework.", "n_publication_ref": 9, "n_figure_ref": 4}, {"heading": "The Double Perturbation Framework", "text": "In this section, we describe the double perturbation framework which focuses on identifying vulnerable examples within a small neighborhood of the test dataset. The framework consists of a neighborhood perturbation and a word substitution. We start with defining word substitutions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Existing Word Substitution Strategy", "text": "We focus our study on word-level substitution, where existing works evaluate robustness and counterfactual bias by directly perturbing the test dataset. For instance, adversarial attacks alter the prediction by making synonym substitutions, and the fairness literature evaluates counterfactual fairness by substituting protected tokens. We integrate the word substitution strategy into our framework as the component for evaluating robustness and fairness.\nFor simplicity, we consider a single-word substitution and denote it with the operator \u2295. Let X \u2286 V l be the input space where V is the vocabulary and l is the sentence length, p = (p (1) , p (2) ) \u2208 V 2 be a pair of synonyms (called patch words), X p \u2286 X denotes sentences with a single occurrence of p (1) (for simplicity we skip other sentences), x 0 \u2208 X p be an input sentence, then x 0 \u2295 p means \"substitute p (1) \u2192 p (2) in x 0 \". The result after substitution is:\nx 0 = x 0 \u2295 p.\nTaking Fig. 1 as an example, where p = (film, movie) and x 0 = a deep and meaningful film, the perturbed sentence is x 0 = a deep and meaningful movie. Now we introduce other components in our framework.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Proposed Neighborhood Perturbation", "text": "Instead of applying the aforementioned word substitutions directly to the original test dataset, our framework perturbs the test dataset within a small neighborhood to construct similar natural sentences. This is to identify vulnerable examples with respect to the model. Note that examples in the neighborhood are not required to have the same meaning as the original example, since we only study the prediction difference caused by applying synonym substitution p ( \u00a72.1).\nConstraints on the neighborhood. We limit the neighborhood sentences within a small 0 norm ball (regarding the test instance) to ensure syntactic similarity, and empirically ensure the naturalness through a language model. The neighborhood of an input sentence x 0 \u2208 X is:\nNeighbor k (x 0 ) \u2286 Ball k (x 0 ) \u2229 X natural ,(1)\nwhere\nBall k (x 0 ) = {x | x \u2212 x 0 0 \u2264 k, x \u2208 X }\nis the 0 norm ball around x 0 (i.e., at most k different tokens), and X natural denotes natural sentences that satisfy a certain language model score which will be discussed next.\nConstruction with masked language model. We construct neighborhood sentences from x 0 by substituting at most k tokens. As shown in Algorithm 1, the construction employs a recursive approach and replaces one token at a time. For each recursion, the algorithm first masks each token of the input sentence (may be the original x 0 or thex from last recursion) separately and predicts likely replacements with a masked language model (e.g., DistilBERT, Sanh et al. 2019). To ensure the naturalness, we keep the top 20 tokens for each mask with the largest logit (subject to a threshold, Line 9). Then, the algorithm constructs neighborhood sentences by replacing the mask with found tokens. We use the notationx in the following sections to denote the constructed sentences within the neighborhood.   lmin \u2190 max{L (\u03ba) , L (0) \u2212 \u03b4}; (i) denotes the ith element. We empirically set \u03ba \u2190 20 and \u03b4 \u2190 3.\nL\n10 Tnew \u2190 {t | l > lmin, (t, l) \u2208 T \u00d7 L}; 11 Xnew \u2190 {x0 | x (i) 0 \u2190 t, t \u2208 Tnew};\nConstruct new sentences by replacing the ith token.\n12 Xneighbor \u2190 Xneighbor \u222a Xnew; 13 return Xneighbor;", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Evaluating Second-Order Robustness", "text": "With the proposed double perturbation framework, we design two black-box attacks 1 to identify vulnerable examples within the neighborhood of the test set. We aim at evaluating the robustness for inputs beyond the test set.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Previous First-Order Attacks", "text": "Adversarial attacks search for small and invariant perturbations on the model input that can alter the prediction. To simplify the discussion, in the following, we take a binary classifier f (x) : X \u2192 {0, 1} as an example to describe our framework. Let x 0 be the sentence from the test set with label y 0 , then the smallest perturbation \u03b4 * under 0 norm distance is: 2\n\u03b4 * := argmin \u03b4 \u03b4 0 s.t. f (x 0 \u2295 \u03b4) = y 0 .\nHere \u03b4 = p 1 \u2295 \u2022 \u2022 \u2022 \u2295 p l denotes a series of substitutions. In contrast, our second-order attacks fix \u03b4 = p and search for the vulnerable x 0 .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Proposed Second-Order Attacks", "text": "Second-order attacks study the prediction difference caused by applying p. For notation convenience we define the prediction difference F (x; p) :\nx0 = a deep and meaningful film. p = film, movie\nx (i = 2) a short and moving film (movie). a slow and moving film (movie). a dramatic or meaningful film (movie).\n\u2022 p alters the prediction.\nx0 =\"a short and moving film (movie).\" (70% negative) 73% positive Figure 3: The attack flow for SO-Beam (Algorithm 2). Blue x 0 is the input sentence and yellowx 0 is our constructed vulnerable example (the prediction can be altered by substituting film \u2192 movie). Green boxes in the middle show intermediate sentences, and f soft (x) denotes the probability outputs for film and movie.\nX \u00d7 V 2 \u2192 {\u22121, 0, 1} by: 3 F (x; p) := f (x \u2295 p) \u2212 f (x).\n(2) Taking Fig. 1 as an example, the prediction difference forx 0 on p is F (x 0 ; p) = f (...moving movie.) \u2212 f (...moving film.) = \u22121.\nGiven an input sentence x 0 , we want to find patch words p and a vulnerable examplex 0 such that f (x 0 \u2295 p) = f (x 0 ). Follow Alzantot et al. (2018), we choose p from a predefined list of counter-fitted synonyms (Mrk\u0161i\u0107 et al., 2016) that maximizes |f soft (p (2) ) \u2212 f soft (p (1) )|. Here f soft (x) : X \u2192 [0, 1] denotes probability output (e.g., after the softmax layer but before the final argmax), f soft (p (1) ) and f soft (p (2) ) denote the predictions for the single word, and we enumerate through all possible p for x 0 . Let k be the neighborhood distance, then the attack is equivalent to solving:\nx 0 = argmax x\u2208Neighbor k (x 0 ) |F (x; p)|.\n(3)", "n_publication_ref": 2, "n_figure_ref": 2}, {"heading": "Brute-force attack (SO-Enum).", "text": "A naive approach for solving Eq. (3) is to enumerate through Neighbor k (x 0 ). The enumeration finds the smallest perturbation, but is only applicable for small k (e.g., k \u2264 2) given the exponential complexity. Beam-search attack (SO-Beam). The efficiency can be improved by utilizing the probability output, where we solve Eq. (3) by minimizing the crossentropy loss with regard to x \u2208 Neighbor k (x 0 ):\nL(x; p) := \u2212 log(1 \u2212 f min ) \u2212 log(f max ), (4)\nwhere f min and f max are the smaller and the larger output probability between f soft (x) and f soft (x \u2295 3 We assume a binary classification task, but our framework is general and can be extended to multi-class classification.\np), respectively. Minimizing Eq. ( 4) effectively leads to f min \u2192 0 and f max \u2192 1, and we use a beam search to find the best x. At each iteration, we construct sentences through Neighbor 1 (x) and only keep the top 20 sentences with the smallest L(x; p). We run at most k iterations, and stop earlier if we find a vulnerable example. We provide the detailed implementation in Algorithm 2 and a flowchart in Fig. 3. \n4 for i \u2190 1, . . . , k do 5 Xnew \u2190 x\u2208X beam Neighbor 1 (x); 6x0 \u2190 argmax x\u2208Xnew |F (x; p)|; 7 if F (x0; p) = 0 then returnx0; 8 Xnew \u2190 SortIncreasing(Xnew, L); 9 Xbeam \u2190 {X (0) new , . . . , X (\u03b2\u22121) new };\nKeep the best beam. We set \u03b2 \u2190 20. 10 return None;", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Experimental Results", "text": "In this section, we evaluate the second-order robustness of existing models and show the quality of our constructed vulnerable examples.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Setup", "text": "We follow the setup from the robust training literature (Jia et al., 2019;Xu et al., 2020) and experiment with both the base (non-robust) and robustly trained models. We train the binary sentiment classifiers on the SST-2 dataset with bag-ofwords (BoW), CNN, LSTM, and attention-based Original: 70% Negative Input Example: in its best moments , resembles a bad high school production of grease , without benefit of song .\nGenetic: 56% Positive Adversarial Example: in its best moment , recalling a naughty high school production of lubrication , unless benefit of song .\nBAE: 56% Positive Adversarial Example: in its best moments , resembles a great high school production of grease , without benefit of song .\nSO-Enum and SO-Beam (ours): 60% Negative (67% Positive) Vulnerable Example: in its best moments , resembles a bad (unhealthy) high school production of musicals , without benefit of song .\nTable 1: Sampled attack results on the robust BoW. For Genetic and BAE the goal is to find an adversarial example that alters the original prediction, whereas for SO-Enum and SO-Beam the goal is to find a vulnerable example beyond the test set such that the prediction can be altered by substituting bad \u2192 unhealthy.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "models.", "text": "Base models. For BoW, CNN, and LSTM, all models use pre-trained GloVe embeddings (Pennington et al., 2014), and have one hidden layer of the corresponding type with 100 hidden size. Similar to the baseline performance reported in GLUE , our trained models have an evaluation accuracy of 81.4%, 82.5%, and 81.7%, respectively. For attention-based models, we train a 3-layer Transformer (the largest size in ) and fine-tune a pre-trained bertbase-uncased from HuggingFace (Wolf et al., 2020 (Morris et al., 2020).\nAttack success rate (second-order). We also quantify second-order robustness through attack success rate, which measures the ratio of test examples that a vulnerable example can be found.\nTo evaluate the impact of neighborhood size, we experiment with two configurations: (1) For the small neighborhood (k = 2), we use SO-Enum that finds the most similar vulnerable example.\n(2) For the large neighborhood (k = 6), SO-Enum is not applicable and we use SO-Beam to find vulnerable examples. We consider the most challenging setup and use patch words p from the same set of counter-fitted synonyms as robust models (they are provably robust to these synonyms on the test set). We also provide a random baseline to validate the effectiveness of minimizing Eq. (4) (Appendix A.1).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Quality metrics (perplexity and similarity).", "text": "We  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "We experiment with the validation split (872 examples) on a single RTX 3090. The average running time per example (in seconds) on base LSTM is 31.9 for Genetic, 1.1 for BAE, 7.0 for SO-Enum (k = 2), and 1.9 for SO-Beam (k = 6). We provide additional running time results in Appendix A.3. Table 1 provides an example of the attack result where all attacks are successful (additional examples in Appendix A.5). As shown, our secondorder attacks find a vulnerable example by replacing grease \u2192 musicals, and the vulnerable example has different predictions for bad and unhealthy. Note that, Genetic and BAE have different objectives from second-order attacks and focus on finding the adversarial example. Next we discuss the results from two perspectives. Second-order robustness. We observe that existing robustly trained models are not second-order robust. As shown in  Furthermore, applying existing attacks on the vulnerable examples constructed by our method will lead to much smaller perturbations. As a reference, on the robustly trained CNN, Genetic attack constructs adversarial examples by perturbing 2.7 words on average (starting from the input examples). However, if Genetic starts from our vulnerable examples, it would only need to perturb a single word (i.e., the patch words p) to alter the prediction. These results demonstrate the weakness of the models (even robustly trained) for those inputs beyond the test set.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Human Evaluation", "text": "We perform human evaluation on the examples constructed by SO-Beam. Specifically, we randomly  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Evaluating Counterfactual Bias", "text": "In addition to evaluating second-order robustness, we further extend the double perturbation framework ( \u00a72) to evaluate counterfactual biases by setting p to pairs of protected tokens. We show that our method can reveal the hidden model bias.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Counterfactual Bias", "text": "In contrast to second-order robustness, where we consider the model vulnerable as long as there exists one vulnerable example, counterfactual bias focuses on the expected prediction, which is the average prediction among all examples within the neighborhood. We consider a model biased if the expected predictions for protected groups are different (assuming the model is not intended to discriminate between these groups). For instance, a sentiment classifier is biased if the expected prediction for inputs containing woman is more positive (or negative) than inputs containing man. Such bias is harmful as they may make unfair decisions based on protected attributes, for example in situations such as hiring and college admission.\nCounterfactual token bias. We study a narrow case of counterfactual bias, where counterfactual examples are constructed by substituting protected tokens in the input. A naive approach of measuring this bias is to construct counterfactual examples directly from the test set, however such evaluation may not be robust since test examples are only a small subset of natural sentences. Formally, let p be a pair of protected tokens such as (he, she) or (Asian, American), X test \u2286 X p be a test set (as in \u00a72.1), we define counterfactual token bias by:\nB p,k := E x\u2208Neighbor k (Xtest) F soft (x; p).(5)\nWe calculate Eq. (5) through an enumeration across all natural sentences within the neighborhood. 7 Here Neighbor k (X test ) = x\u2208Xtest Neighbor k (x) denotes the union of neighborhood examples (of distance k) around the test set, and F soft (x; p) :\nX \u00d7 V 2 \u2192 [\u22121, 1]\ndenotes the difference between probability outputs f soft (similar to Eq. (2)):  \nF soft (x; p) := f soft (x \u2295 p) \u2212 f soft (x). (6\n(k = 3) in X filter .\nThe model is unbiased on p if B p,k \u2248 0, whereas a positive or negative B p,k indicates that the model shows preference or against to p (2) , respectively. Fig. 4 illustrates the distribution of (x, x \u2295 p) for both an unbiased model and a biased model. The aforementioned neighborhood construction does not introduce additional bias. For instance, let x 0 be a sentence containing he, even though it is possible for Neighbor 1 (x 0 ) to contain many stereotyping sentences (e.g., contains tokens such as doctor and driving) that affect the distribution of f soft (x), but it does not bias Eq. (6) as we only care about the prediction difference of replacing he \u2192 she. The construction has no information about the model objective, thus it would be difficult to bias f soft (x) and f soft (x \u2295 p) differently.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Experimental Results", "text": "In this section, we use gender bias as a running example, and demonstrate the effectiveness of our method by revealing the hidden model bias. We provide additional results in Appendix A.4.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Setup", "text": "We evaluate counterfactual token bias on the SST-2 dataset with both the base and debiased models. We focus on binary gender bias and set p to pairs of gendered pronouns from Zhao et al. (2018a). Base Model. We train a single layer LSTM with pre-trained GloVe embeddings and 75 hidden size (from TextAttack, Morris et al. 2020). The model has 82.9% accuracy similar to the baseline performance reported in GLUE. Debiased Model. Data-augmentation with gender swapping has been shown effective in mitigating gender bias (Zhao et al., 2018a. We augment the training split by swapping all male entities with the corresponding female entities and vice-versa. We use the same setup as the base LSTM and attain 82.45% accuracy. Here \"original\" is equivalent to k = 0, \"perturbed\" is equivalent to k = 3, p is in the form of (male, female).\nMetrics. We evaluate model bias through the proposed B p,k for k = 0, . . . , 3. Here the bias for k = 0 is effectively measured on the original test set, and the bias for k \u2265 1 is measured on our constructed neighborhood. We randomly sample a subset of constructed examples when k = 3 due to the exponential complexity. Filtered test set. To investigate whether our method is able to reveal model bias that was hidden in the test set, we construct a filtered test set on which the bias cannot be observed directly. Let X test be the original validation split, we construct X filter by the equation below and empirically set = 0.005. We provide statistics in Table 5.\nX filter := {x | |F soft (x; p)| < , x \u2208 X test }.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Results", "text": "Our method is able to reveal the hidden model bias on X filter , which is not visible with naive measurements. In Fig. 5, the naive approach (k = 0) observes very small biases on most tokens (as constructed). In contrast, when evaluated by our double perturbation framework (k = 3), we are able to observe noticeable bias, where most p has a positive bias on the base model. This observed bias is in line with the measurements on the original X test (Appendix A.4), indicating that we reveal the correct model bias. Furthermore, we observe mitigated biases in the debiased model, which demonstrates the effectiveness of data augmentation.\nTo demonstrate how our method reveals hidden bias, we conduct a case study with p = (actor, actress) and show the relationship between the bias B p,k and the neighborhood distance k. We present the histograms for F soft (x; p) in Fig. 6 and plot the corresponding B p,k vs. k in the right-most panel. Surprisingly, for the base model, the bias is Figure 6: Left and Middle: Histograms for F soft (x; p) (x-axis) with p = (actor, actress). Right: The plot for the average F soft (x; p) (i.e., counterfactual token bias) vs. neighborhood distance k. Results show that the counterfactual bias on p can be revealed when increasing k.\nnegative when k = 0, but becomes positive when k = 3. This is because the naive approach only has two test examples (Table 5) thus the measurement is not robust. In contrast, our method is able to construct 141,780 similar natural sentences when k = 3 and shifts the distribution to the right (positive). As shown in the right-most panel, the bias is small when k = 1, and becomes more significant as k increases (larger neighborhood). As discussed in \u00a74.1, the neighborhood construction does not introduce additional bias, and these results demonstrate the effectiveness of our method in revealing hidden model bias.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "Related Work", "text": "First-order robustness evaluation.\nA line of work has been proposed to study the vulnerability of natural language models, through transformations such as character-level perturbations (Ebrahimi et al., 2018), word-level perturbations (Jin et al., 2019;Ren et al., 2019;Cheng et al., 2020;Li et al., 2020), prepending or appending a sequence (Jia and Liang, 2017;Wallace et al., 2019a), and generative models (Zhao et al., 2018b). They focus on constructing adversarial examples from the test set that alter the prediction, whereas our methods focus on finding vulnerable examples beyond the test set whose prediction can be altered. Robustness beyond the test set. Several works have studied model robustness beyond test sets but mostly focused on computer vision tasks. Zhang et al. (2019) demonstrate that a robustly trained model could still be vulnerable to small perturbations if the input comes from a distribution only slightly different than a normal test set (e.g., images with slightly different contrasts). Hendrycks and Dietterich (2019) study more sources of common corruptions such as brightness, motion blur and fog. Unlike in computer vision where simple image transformations can be used, in our natural language setting, generating a valid example beyond test set is more challenging because language semantics and grammar must be maintained. Counterfactual fairness. Kusner et al. (2017) propose counterfactual fairness and consider a model fair if changing the protected attributes does not affect the distribution of prediction. We follow the definition and focus on evaluating the counterfactual bias between pairs of protected tokens. Existing literature quantifies fairness on a test dataset or through templates (Feldman et al., 2015;Kiritchenko and Mohammad, 2018;May et al., 2019;. For instance, Garg et al. (2019) quantify the absolute counterfactual token fairness gap on the test set; Prabhakaran et al. ( 2019) study perturbation sensitivity for named entities on a given set of corpus. Wallace et al. (2019b); Sheng et al. (2019Sheng et al. ( , 2020 study how language generation models respond differently to prompt sentences containing mentions of different demographic groups. In contrast, our method quantifies the bias on the constructed neighborhood.", "n_publication_ref": 18, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "This work proposes the double perturbation framework to identify model weaknesses beyond the test dataset, and study a stronger notion of robustness and counterfactual bias. We hope that our work can stimulate the research on further improving the robustness and fairness of natural language models.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ethical Considerations", "text": "Intended use. One primary goal of NLP models is the generalization to real-world inputs. However, existing test datasets and templates are often not comprehensive, and thus it is difficult to evaluate real-world performance (Recht et al., 2019;Ribeiro et al., 2020). Our work sheds a light on quantifying performance for inputs beyond the test dataset and help uncover model weaknesses prior to the realworld deployment. Misuse potential. Similar to other existing adversarial attack methods (Ebrahimi et al., 2018;Jin et al., 2019;Zhao et al., 2018b), our second-order attacks can be used for finding vulnerable examples to a NLP system. Therefore, it is essential to study how to improve the robustness of NLP models against second-order attacks. Limitations. While the core idea about the double perturbation framework is general, in \u00a74, we consider only binary gender in the analysis of counterfactual fairness due to the restriction of the English corpus we used, which only have words associated with binary gender such as he/she, waiter/waitress, etc.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "A Supplemental Material", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 Random Baseline", "text": "To validate the effectiveness of minimizing Eq. (4), we also experiment on a second-order baseline that constructs vulnerable examples by randomly replacing up to 6 words. We use the same masked language model and threshold as SO-Beam such that they share a similar neighborhood. We perform the attack on the same models as Table 2, and the attack success rates on robustly trained BoW, CNN, LSTM, and Transformers are 18.8%, 22.3%, 15.2%, and 25.1%, respectively. Despite being a second-order attack, the random baseline has low attack success rates thus demonstrates the effectiveness of SO-Beam.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.2 Human Evaluation", "text": "We randomly select 100 successful attacks from SO-Beam and consider four types of examples (for a total of 400 examples): The original examples with and without synonym substitution p, and the vulnerable examples with and without synonym substitution p. For each example, we annotate the naturalness and sentiment separately as described below.\nNaturalness of vulnerable examples. We ask the annotators to score the likelihood of being an original example (i.e., not altered by computer) based on grammar correctness and naturalness, with a Likert scale of 1-5: (1) Sure adversarial example. (2) Likely an adversarial example. (3) Neutral. (4) Likely an original example. (5) Sure original example. Semantic similarity after the synonym substitution. We first ask the annotators to predict the sentiment on a Likert scale of 1-5, and then map the prediction to three categories: negative, neutral, and positive. We consider two examples to have the same semantic meaning if and only if they are both positive or negative.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.3 Running Time", "text": "We experiment with the validation split on a single RTX 3090, and measure the average running time per example. As shown in  A.4 Additional Results on Protected Tokens Fig. 7 presents the experimental results with additional protected tokens such as nationality, religion, and sexual orientation (from Ribeiro et al. ( 2020)). We use the same base LSTM as described in \u00a74.2. One interesting observation is when p = (gay, straight) where the bias is negative, indicating that the sentiment classifier tends to give more negative prediction when substituting gay \u2192 straight in the input. This phenomenon is opposite to the behavior of toxicity classifiers (Dixon et al., 2018)  In Fig. 8, we measure the bias on X test and observe positive bias on most tokens for both k = 0 and k = 3, which indicates that the model \"tends\" to make more positive predictions for examples containing certain female pronouns than male pro- nouns. Notice that even though gender swap mitigates the bias to some extent, it is still difficult to fully eliminate the bias. This is probably caused by tuples like (him, his, her) which cannot be swapped perfectly, and requires additional processing such as part-of-speech resolving (Zhao et al., 2018a). To help evaluate the naturalness of our constructed examples used in \u00a74, we provide sample sentences in Table 9 and Table 10. Bold words are the corresponding patch words p, taken from the predefined list of gendered pronouns.  Distance k = 1 97% Negative (97% Negative) it 's hampered by a lifetime-channel kind of plot and lone lead actor (actress) who is out of their depth . 56% Negative (55% Positive ) it 's hampered by a lifetime-channel kind of plot and a lead actor (actress) who is out of creative depth . 89% Negative (84% Negative) it 's hampered by a lifetime-channel kind of plot and a lead actor (actress) who talks out of their depth . 98% Negative (98% Negative) it 's hampered by a lifetime-channel kind of plot and a lead actor (actress) who is out of production depth . 96% Negative (96% Negative) it 's hampered by a lifetime-channel kind of plot and a lead actor (actress) that is out of their depth .", "n_publication_ref": 2, "n_figure_ref": 2}, {"heading": "A.5 Additional Results on Robustness", "text": "Distance k = 2 88% Negative (87% Negative) it 's hampered by a lifetime-channel cast of stars and a lead actor (actress) who is out of their depth . 96% Negative (95% Negative) it 's hampered by a simple set of plot and a lead actor (actress) who is out of their depth . 54% Negative (54% Negative) it 's framed about a lifetime-channel kind of plot and a lead actor (actress) who is out of their depth . 90% Negative (88% Negative) it 's hampered by a lifetime-channel mix between plot and a lead actor (actress) who is out of their depth . 78% Negative (68% Negative) it 's hampered by a lifetime-channel kind of plot and a lead actor (actress) who storms out of their mind .\nDistance k = 3 52% Positive (64% Positive ) it 's characterized by a lifetime-channel combination comedy plot and a lead actor (actress) who is out of their depth . 93% Negative (93% Negative) it 's hampered by a lifetime-channel kind of star and a lead actor (actress) who falls out of their depth . 58% Negative (57% Negative) it 's hampered by a tough kind of singer and a lead actor (actress) who is out of their teens . 70% Negative (52% Negative) it 's hampered with a lifetime-channel kind of plot and a lead actor (actress) who operates regardless of their depth . 58% Negative (53% Positive ) it 's hampered with a lifetime-channel cast of plot and a lead actor (actress) who is out of creative depth .  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We thank anonymous reviewers for their helpful feedback. We thank UCLA-NLP group for the valuable discussions and comments. The research is supported NSF #1927554, #1901527, #2008173  and #2048280 and an Amazon Research Award.   ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Type Predictions Text", "text": "Original 54% Positive (69% Positive ) for the most part , director anne-sophie birot 's first feature is a sensitive , overly (extraordinarily) well-acted drama . Vulnerable 53% Negative (62% Positive ) for the most part , director anne-sophie benoit 's first feature is a sensitive , overly (extraordinarily) well-acted drama . Original 73% Negative (56% Negative) the cold (colder) turkey would 've been a far better title . Vulnerable 61% Negative (62% Positive ) the cold (colder) turkey might 've been a far better title .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Original", "text": "70% Negative (65% Negative) it 's just disappointingly superficial -a movie that has all the elements necessary to be a fascinating , involving character study , but never does more than scratch the shallow (surface) . Vulnerable 52% Negative (55% Positive ) it 's just disappointingly short -a movie that has all the elements necessary to be a fascinating , involving character study , but never does more than scratch the shallow (surface) .\nOriginal 79% Negative (72% Negative) schaeffer has to find some hook on which to hang his persistently useless movies , and it might as well be the resuscitation (revival) of the middleaged character . Vulnerable 57% Negative (57% Positive ) schaeffer has to find some hook on which to hang his persistently entertaining movies , and it might as well be the resuscitation (revival) of the middleaged character .\nOriginal 64% Positive (58% Positive ) the primitive force of this film seems to bubble up from the vast collective memory of the combatants (militants) . Vulnerable 52% Positive (53% Negative) the primitive force of this film seems to bubble down from the vast collective memory of the combatants (militants) .\nOriginal 64% Positive (74% Positive ) on this troublesome (tricky) topic , tadpole is very much a step in the right direction , with its blend of frankness , civility and compassion . Vulnerable 55% Negative (56% Positive ) on this troublesome (tricky) topic , tadpole is very much a step in the right direction , losing its blend of frankness , civility and compassion .  ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Generating natural language adversarial examples", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Moustafa Alzantot; Yash Sharma; Ahmed Elgohary; Bo-Jhang Ho; Mani Srivastava; Kai-Wei Chang"}, {"title": "Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples", "journal": "", "year": "2020", "authors": "Minhao Cheng; Jinfeng Yi; Pin-Yu Chen; Huan Zhang; Cho-Jui Hsieh"}, {"title": "Measuring and mitigating unintended bias in text classification", "journal": "", "year": "2018", "authors": "Lucas Dixon; John Li; Jeffrey Sorensen; Nithum Thain; Lucy Vasserman"}, {"title": "Training verified learners with learned verifiers", "journal": "", "year": "2018", "authors": "Krishnamurthy Dvijotham; Sven Gowal; Robert Stanforth; Relja Arandjelovic; O' Brendan; Jonathan Donoghue; Pushmeet Uesato;  Kohli"}, {"title": "HotFlip: White-box adversarial examples for text classification", "journal": "Short Papers", "year": "2018", "authors": "Javid Ebrahimi; Anyi Rao; Daniel Lowd; Dejing Dou"}, {"title": "Certifying and removing disparate impact", "journal": "", "year": "2015", "authors": "Michael Feldman; A Sorelle; John Friedler; Carlos Moeller; Suresh Scheidegger;  Venkatasubramanian"}, {"title": "Counterfactual fairness in text classification through robustness", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Sahaj Garg; Vincent Perot; Nicole Limtiaco; Ankur Taly; Ed H Chi; Alex Beutel"}, {"title": "BAE: BERT-based adversarial examples for text classification", "journal": "", "year": "2020", "authors": "Siddhant Garg; Goutham Ramakrishnan"}, {"title": "Benchmarking neural network robustness to common corruptions and perturbations", "journal": "", "year": "2019", "authors": "Dan Hendrycks; Thomas Dietterich"}, {"title": "On the robustness of self-attentive models", "journal": "", "year": "2019", "authors": "Yu-Lun Hsieh; Minhao Cheng; Da-Cheng Juan; Wei Wei; Wen-Lian Hsu; Cho-Jui Hsieh"}, {"title": "Generating syntactically controlled paraphrases without using annotated parallel pairs", "journal": "", "year": "2021", "authors": "Hao Kuan; Kai-Wei Huang;  Chang"}, {"title": "Achieving verified robustness to symbol substitutions via interval bound propagation", "journal": "EMNLP-IJCNLP", "year": "2019", "authors": "Po-Sen Huang; Robert Stanforth; Johannes Welbl; Chris Dyer; Dani Yogatama; Sven Gowal"}, {"title": "Reducing sentiment bias in language models via counterfactual evaluation", "journal": "", "year": "2020", "authors": "Po-Sen Huang; Huan Zhang; Ray Jiang; Robert Stanforth; Johannes Welbl; Jack Rae; Vishal Maini; Dani Yogatama; Pushmeet Kohli"}, {"title": "Adversarial example generation with syntactically controlled paraphrase networks", "journal": "ArXiv", "year": "2018", "authors": "J Mohit Iyyer; Kevin Wieting; Luke Gimpel;  Zettlemoyer"}, {"title": "Adversarial examples for evaluating reading comprehension systems", "journal": "", "year": "2017", "authors": "Robin Jia; Percy Liang"}, {"title": "Association for Computational Linguistics", "journal": "", "year": "2017-09-09", "authors": ""}, {"title": "Certified robustness to adversarial word substitutions", "journal": "", "year": "2019", "authors": "Robin Jia; Aditi Raghunathan; Kerem G\u00f6ksel; Percy Liang"}, {"title": "Is bert really robust? a strong baseline for natural language attack on text classification and entailment", "journal": "", "year": "2019", "authors": "Di Jin; Zhijing Jin; Joey Tianyi Zhou; Peter Szolovits"}, {"title": "Examining gender and race bias in two hundred sentiment analysis systems", "journal": "", "year": "2018", "authors": "Svetlana Kiritchenko; Saif Mohammad"}, {"title": "Counterfactual fairness", "journal": "Curran Associates, Inc", "year": "2017", "authors": "J Matt; Joshua Kusner; Chris Loftus; Ricardo Russell;  Silva"}, {"title": "BERT-ATTACK: Adversarial attack against BERT using BERT", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Linyang Li; Ruotian Ma; Qipeng Guo; Xiangyang Xue; Xipeng Qiu"}, {"title": "On measuring social biases in sentence encoders", "journal": "Long and Short Papers", "year": "2019", "authors": "Chandler May; Alex Wang; Shikha Bordia; Samuel R Bowman; Rachel Rudinger"}, {"title": "Adversarial training methods for semisupervised text classification", "journal": "", "year": "2017", "authors": "Takeru Miyato; Andrew M Dai; Ian Goodfellow"}, {"title": "Textattack: A framework for adversarial attacks, data augmentation", "journal": "", "year": "2020", "authors": "John X Morris; Eli Lifland; Jin Yong Yoo; Jake Grigsby; Di Jin; Yanjun Qi"}, {"title": "Counter-fitting word vectors to linguistic constraints", "journal": "", "year": "2016", "authors": "Nikola Mrk\u0161i\u0107; \u00d3 Diarmuid; Blaise S\u00e9aghdha; Milica Thomson; Lina M Ga\u0161i\u0107; Pei-Hao Rojas-Barahona; David Su; Tsung-Hsien Vandyke; Steve Wen;  Young"}, {"title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"title": "Perturbation sensitivity analysis to detect unintended model biases", "journal": "", "year": "2019", "authors": "Ben Vinodkumar Prabhakaran; Margaret Hutchinson;  Mitchell"}, {"title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeff Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"title": "Do ImageNet classifiers generalize to ImageNet?", "journal": "Proceedings of Machine Learning Research", "year": "2019", "authors": "Benjamin Recht; Rebecca Roelofs; Ludwig Schmidt; Vaishaal Shankar"}, {"title": "Generating natural language adversarial examples through probability weighted word saliency", "journal": "", "year": "2019", "authors": "Yihe Shuhuai Ren; Kun Deng; Wanxiang He;  Che"}, {"title": "Beyond accuracy: Behavioral testing of nlp models with checklist", "journal": "", "year": "2020", "authors": "Tongshuang Marco Tulio Ribeiro; Carlos Wu; Sameer Guestrin;  Singh"}, {"title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "journal": "ArXiv", "year": "2019", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"title": "The woman worked as a babysitter: On biases in language generation", "journal": "", "year": "2019", "authors": "Emily Sheng; Kai-Wei Chang; Premkumar Natarajan; Nanyun Peng"}, {"title": "Towards controllable biases in language generation", "journal": "", "year": "2020", "authors": "Emily Sheng; Kai-Wei Chang; Premkumar Natarajan; Nanyun Peng"}, {"title": "Robustness verification for transformers", "journal": "", "year": "2020", "authors": "Zhouxing Shi; Huan Zhang; Kai-Wei Chang; Minlie Huang; Cho-Jui Hsieh"}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "journal": "", "year": "2013", "authors": "Richard Socher; Alex Perelygin; Jean Wu; Jason Chuang; Christopher D Manning; Andrew Ng; Christopher Potts"}, {"title": "Universal adversarial triggers for attacking and analyzing nlp", "journal": "", "year": "2019", "authors": "Eric Wallace; Shi Feng; Nikhil Kandpal; Matt Gardner; Sameer Singh"}, {"title": "Universal adversarial triggers for attacking and analyzing NLP", "journal": "", "year": "2019", "authors": "Eric Wallace; Shi Feng; Nikhil Kandpal; Matt Gardner; Sameer Singh"}, {"title": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2019", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"title": "", "journal": "", "year": "", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest"}, {"title": "Automatic perturbation analysis for scalable certified robustness and beyond", "journal": "", "year": "2020", "authors": "Kaidi Xu; Zhouxing Shi; Huan Zhang; Yihan Wang; Kai-Wei Chang; Minlie Huang; Bhavya Kailkhura; Xue Lin; Cho-Jui Hsieh"}, {"title": "Greedy attack and gumbel attack: Generating adversarial examples for discrete data", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Puyudi Yang; Jianbo Chen; Cho-Jui Hsieh; Jane-Ling Wang; Michael I Jordan"}, {"title": "Word-level textual adversarial attacking as combinatorial optimization", "journal": "", "year": "2020", "authors": "Yuan Zang; Fanchao Qi; Chenghao Yang; Zhiyuan Liu; Meng Zhang; Qun Liu; Maosong Sun"}, {"title": "The limitations of adversarial training and the blind-spot attack", "journal": "", "year": "2019", "authors": "Huan Zhang; Hongge Chen; Zhao Song; Duane Boning; Cho-Jui Inderjit Dhillon;  Hsieh"}, {"title": "Gender bias in contextualized word embeddings", "journal": "", "year": "2019", "authors": "Jieyu Zhao; Tianlu Wang; Mark Yatskar; Ryan Cotterell; Vicente Ordonez; Kai-Wei Chang"}, {"title": "Gender bias in coreference resolution: Evaluation and debiasing methods", "journal": "", "year": "2018", "authors": "Jieyu Zhao; Tianlu Wang; Mark Yatskar; Vicente Ordonez; Kai-Wei Chang"}, {"title": "Generating natural adversarial examples", "journal": "", "year": "2018", "authors": "Zhengli Zhao; Dheeru Dua; Sameer Singh"}, {"title": "Type Predictions Text Original 55% Positive (67% Positive ) a hamfisted romantic comedy that makes our boy (girl) the hapless facilitator of an extended cheap shot across the mason-dixon line", "journal": "", "year": "2020", "authors": "Yi Zhou; Xiaoqing Zheng; Cho-Jui Hsieh; Kai-Wei Chang; Xuanjing Huang"}, {"title": "Distance k = 1 52% Positive (66% Positive ) a hamfisted romantic comedy that makes our boy (girl) the hapless facilitator of an extended cheap shot from the mason-dixon line", "journal": "", "year": "", "authors": ""}, {"title": "Positive (79% Positive ) a hamfisted romantic comedy that makes our boy (girl) the hapless facilitator gives an extended cheap shot across the mason-dixon line", "journal": "", "year": "", "authors": ""}, {"title": "Negative (58% Positive ) a hamfisted romantic comedy that makes our boy (girl) the hapless facilitator of an extended cheap shot across the phone line", "journal": "", "year": "", "authors": ""}, {"title": "Positive (83% Positive ) a hamfisted romantic comedy that makes our boy (girl) the hapless facilitator of an extended chase shot across the mason-dixon line", "journal": "", "year": "", "authors": ""}, {"title": "Positive (81% Positive ) a hamfisted romantic comedy that makes our boy (girl) our hapless facilitator of an extended cheap shot across the mason-dixon line", "journal": "", "year": "", "authors": ""}, {"title": "Distance k = 2 85% Positive (85% Positive ) a hilarious romantic comedy that makes our boy (girl) the hapless facilitator of an emotionally cheap shot across the mason-dixon line", "journal": "", "year": "", "authors": ""}, {"title": "Positive (86% Positive ) a hamfisted romantic comedy romance makes our boy (girl) the hapless facilitator of an extended cheap delivery across the mason-dixon line", "journal": "", "year": "", "authors": ""}, {"title": "Positive (87% Positive ) a hamfisted romantic romance adventure makes our boy (girl) the hapless facilitator of an extended cheap shot across the mason-dixon line . 50% Negative (62% Positive ) a hamfisted romantic comedy that makes our boy (girl) the hapless boss of an extended cheap shot behind the mason-dixon line", "journal": "", "year": "", "authors": ""}, {"title": "77% Negative (71% Negative) a hamfisted lesbian comedy that makes our boy (girl) the hapless facilitator of an extended slap shot across the mason-dixon line", "journal": "", "year": "", "authors": ""}, {"title": "Distance k = 3 97% Positive (97% Positive ) a darkly romantic comedy romance makes our boy (girl) the hapless facilitator delivers an extended cheap shot across the mason-dixon line", "journal": "", "year": "", "authors": ""}, {"title": "Positive (74% Positive ) a hamfisted romantic comedy film makes our boy (girl) the hapless facilitator of an extended cheap shot across the production line", "journal": "", "year": "", "authors": ""}, {"title": "Positive (89% Positive ) a hamfisted romantic comedy that makes our boy (girl) the exclusive focus of an extended cheap shot across the mason-dixon line", "journal": "", "year": "", "authors": ""}, {"title": "Positive (76% Positive ) a hamfisted romantic comedy that makes our boy (girl) the hapless facilitator shoots an extended flash shot across the camera line", "journal": "", "year": "", "authors": ""}, {"title": "Positive (99% Positive ) a compelling romantic comedy that makes our boy (girl) the perfect facilitator of an extended story shot across the mason-dixon line", "journal": "", "year": "", "authors": ""}, {"title": "We only present 5 examples per k due to space constrain", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Algorithm 1 :1Neighborhood construction Data: Input sentence x0, masked language model LM, max distance k. 1 Function Neighbor k (x0): 0, . . . , len(x0) \u2212 1 do 7 T, L \u2190 LM.fillmask(x0, i); Mask ith token and return candidate tokens and corresponding logits.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "8L\u2190SortDecreasing(L);", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "9", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Algorithm 2 :2Beam-search attack (SO-Beam) Data: Input sentence x0, synonyms P, model functions F and fsoft, loss L, max distance k. 1 Function SO-Beam k (x0): 2 p \u2190 argmax p\u2208P s.t. x 0 \u2208Xp |fsoft(p (2) ) \u2212 fsoft(p (1) )|; 3 Xbeam \u2190 {x0};", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: An illustration of an unbiased model vs. a biased model. Green and gray indicate the probability of positive and negative predictions, respectively. Left: An unbiased model where the (x, x \u2295 p) pair (yellowred dots) is relatively parallel to the decision boundary. Right: A biased model where the predictions for x \u2295 p (red) are usually more negative (gray) than x (yellow).", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 5 :5Figure5: Our proposed B p,k measured on X filter . Here \"original\" is equivalent to k = 0, \"perturbed\" is equivalent to k = 3, p is in the form of (male, female).", "figure_data": ""}, {"figure_label": "7", "figure_type": "", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure 7: Additional counterfactual token bias measured on the original validation split with base LSTM.", "figure_data": ""}, {"figure_label": "8", "figure_type": "", "figure_id": "fig_8", "figure_caption": "Figure 8 :8Figure 8: Full results for gendered tokens measured on the original validation split.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "). The Transformer uses 4 attention heads and 64 hidden size, and obtains 82.1% accuracy. The BERT-base uses the default configuration and obtains 92.7% accuracy. Robust models (first-order). With the same setup as base models, we apply robust training methods to improve the resistance to word substitution attacks.Jia et al. (2019) provide a provably robust training method through Interval Bound Propagation (IBP,Dvijotham et al. 2018) for all word substitutions on BoW, CNN and LSTM.Xu et al. (2020) provide a provably robust training method on general computational graphs through a combination of forward and backward linear bound propagation, and the resulting 3-layer Transformer is robust to up to 6 word substitutions. For both works we use the same set of counter-fitted synonyms provided inJia et al. (2019). We skip BERT-base due to the lack of an effective robust training method.", "figure_data": "timization algorithm that generates both syntacti-cally and semantically similar adversarial exam-ples, by replacing words within the list of counter-fitted synonyms. (2) The BAE attack (Garg andRamakrishnan, 2020) generates coherent adversar-ial examples by masking and replacing words usingBERT. For both methods we use the implementa-tion provided by TextAttackAttack success rate (first-order). We quantifyfirst-order robustness through attack success rate,which measures the ratio of test examples that anadversarial example can be found. We use first-order attacks as a reference due to the lack of adirect baseline. We experiment with two black-boxattacks: (1) The Genetic attack (Alzantot et al.,2018; Jia et al., 2019) uses a population-based op-"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The average rates over 872 examples (100 for Genetic due to long running time). Second-order attacks achieve higher successful rate since they are able to search beyond the test set.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": ", our second-order attacks attain high success rates not only on the base models but also on the robustly trained models. For instance, on the robustly trained CNN and Transformer, SO-Beam finds vulnerable examples within a small neighborhood for around 96.0% of the test examples, even though these models have improved resistance to strong first-order attacks (success rates drop from 62.0%-74.3% to 23.0%-71.6% for Genetic and BAE).", "figure_data": "SO-EnumSO-BeamOriginalPerturbOriginalPerturbPPLPPL0PPLPPL0Base Models:BoW1682021.11662021.2CNN1702041.11662011.2LSTM1682041.11662041.2Transformer1651931.01651951.1BERT-base1702291.31682221.4Robust Models:BoW1702121.21712221.4CNN1662091.21682101.3LSTM1942511.31852601.8Transformer1702131.21652081.3"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "The quality metrics for second-order methods. We report the median perplexity (PPL) and average 0 norm distance. The original PPL may differ across models since we only count successful attacks.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "The quality metrics from human evaluation. select 100 successful attacks and evaluate both the original examples and the vulnerable examples. To evaluate the naturalness of the constructed examples, we ask the annotators to score the likelihood (on a Likert scale of 1-5, 5 to be the most likely) of being an original example based on the grammar correctness. To evaluate the semantic similarity after applying the synonym substitution p, we ask the annotators to predict the sentiment of each example,", "figure_data": "2 for moredetails.As shown in Table 4, the naturalness score onlydrop slightly after the perturbation, indicating thatour constructed vulnerable examples have similarnaturalness as the original examples. As for thesemantic similarity, we observe that 85% of theoriginal examples maintain the same meaning afterthe synonym substitution, and the correspondingratio is 71% for vulnerable examples. This indi-cates that the synonym substitution is an invariancetransformation for most examples."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "The number of original examples (k = 0) and the number of perturbed examples", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "SO-Beam runs faster than SO-Enum since it utilizes the probability output. The running time may increase if the model has improved second-order robustness.", "figure_data": "Running Time (seconds)Genetic BAE SO-Enum SO-BeamBase Models:BoW31.60.96.21.8CNN28.81.05.91.7LSTM31.91.17.01.9Transformer51.90.56.52.5BERT-base65.61.135.47.1Robust Models:BoW103.91.08.03.5CNN129.41.06.72.6LSTM116.41.110.75.3Transformer66.40.55.92.6"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "The average running time over 872 examples (100 for Genetic due to long running time).", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Number of negative and positive examples containing gay and straight in the training set.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "provides the quality metrics for first-order attacks, where we measure the GPT-2 perplexity and 0 norm distance between the input and the adversarial example. For BAE we evaluate on 872 validation examples, and for Genetic we evaluate on 100 validation examples due to the long running time.Table11shows additional attack results from", "figure_data": "GeneticBAEOriginalPerturbOriginalPerturbPPLPPL0PPLPPL0Base Models:BoW1452583.31922681.6CNN1462823.01862541.5LSTM1312382.91902631.6Transformer1372322.81852541.4BERT-base2013423.41892771.6Robust Models:BoW1321772.42142691.5CNN1362362.72112791.5LSTM1632672.52203021.6Transformer1182002.81962611.4"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "", "figure_data": ": The quality metrics for first-order attacks fromsuccessful attacks. We compare median perplexities(PPL) and average 0 norm distances.SO-Beam on base LSTM, and Table 12 shows addi-tional attack results from SO-Beam on robust CNN.Bold words are the corresponding patch words p,taken from the predefined list of counter-fitted syn-onyms."}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Additional counterfactual bias examples on base LSTM with p = (actor, actress). We only present 5 examples per k due to space constrain. 100% Negative) in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky (lunatic) to jerky to utter turkey . Vulnerable 51% Positive (65% Negative) lasting exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , but 51 ranges from quirky (lunatic) to delicious to crisp turkey . Original 97% Positive (100% Positive ) the scintillating (mesmerizing) performances of the leads keep the film grounded and keep the audience riveted . Vulnerable 91% Negative (90% Positive ) the scintillating (mesmerizing) performances of the leads keep the film grounded and keep the plot predictable . uncanny (strange) kind of laziness to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same movie . Vulnerable 80% Positive (76% Negative) it takes a uncanny (strange) kind of humour to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same movie . Original 100% Negative (100% Negative) ... the film suffers from a lack of humor ( something needed to balance (equilibrium) out the violence ) ... Vulnerable 76% Positive (86% Negative) ... the film derives from a lot of humor ( something clever to balance (equilibrium) out the violence ) ... Original 55% Positive (97% Positive ) we root for ( clara and paul ) , even like them , though perhaps it 's an emotion closer to pity (compassion) . Vulnerable 89% Negative (91% Positive ) we root for ( clara and paul ) , even like them , though perhaps it 's an explanation closer to pity (compassion) . Original 95% Negative (97% Negative) even horror fans (stalkers) will most likely not find what they 're seeking with trouble every day ; the movie lacks both thrills and humor . Vulnerable 61% Positive (59% Negative) even horror fans (stalkers) will most likely not find what they 're seeking with trouble every day ; the movie has both thrills and humor . Original 100% Positive (100% Positive ) a gorgeous , high-spirited musical from india that exquisitely mixed (blends) music , dance , song , and high drama . Vulnerable 87% Negative (81% Positive ) a dark , high-spirited musical from nowhere that loosely mixed (blends) music , dance , song , and high drama . Original 99% Negative (94% Negative) ... the movie is just a plain old (longtime) monster . Vulnerable 94% Negative (94% Positive ) ... the movie is just a pretty old (longtime) monster .", "figure_data": "TypePredictionsTextOriginal99% Positive (99% Positive )it 's a charming and sometimes (often) affecting journey .Vulnerable59% Negative (56% Positive )it 's a charming and sometimes (often) painful journey .Original99% Negative (97% Negative)unflinchingly bleak (somber) and desperateVulnerable80% Negative (79% Positive )unflinchingly bleak (somber) and mysteriousOriginal99% Positive (93% Positive )allows us to hope that nolan is poised to embark a major career (quarry)as a commercial yet inventive filmmaker .Vulnerable76% Positive (75% Negative)allows us to hope that nolan is poised to embark a major career (quarry)as a commercial yet amateur filmmaker .Original94% Positive (68% Positive )the acting , costumes , music , cinematography and sound are all astound-ing (staggering) given the production 's austere locales .Vulnerable87% Positive (66% Negative)the acting , costumes , music , cinematography and sound are largelyastounding (staggering) given the production 's austere locales .Original99% Positive (97% Positive )although laced with humor and a few fanciful touches , the film is arefreshingly serious look at young (juvenile) women .Vulnerable94% Positive (81% Negative)although laced with humor and a few fanciful touches , the film is amoderately serious look at young (juvenile) women .Original99% Negative (98% Negative)a sometimes (occasionally) tedious film .Vulnerable62% Negative (55% Positive )a sometimes (occasionally) disturbing film .Original 100% Negative (Original 89% Negative (96% Negative)it takes a"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "", "figure_data": ""}], "doi": "10.18653/v1/D18-1316"}