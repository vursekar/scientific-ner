{"authors": "Fanghua Ye; Yue Feng; Emine Yilmaz", "pub_date": "", "title": "ASSIST: Towards Label Noise-Robust Dialogue State Tracking", "abstract": "The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state tracking (DST). However, substantial noise has been discovered in its state annotations. Such noise brings about huge challenges for training DST models robustly. Although several refined versions, including MultiWOZ 2.1-2.4, have been published recently, there are still lots of noisy labels, especially in the training set. Besides, it is costly to rectify all the problematic annotations. In this paper, instead of improving the annotation quality further, we propose a general framework, named ASSIST (lAbel noiSe-robuSt dIalogue State Tracking), to train DST models robustly from noisy labels. ASSIST first generates pseudo labels for each sample in the training set by using an auxiliary model trained on a small clean dataset, then puts the generated pseudo labels and vanilla noisy labels together to train the primary model. We show the validity of ASSIST theoretically. Experimental results also demonstrate that AS-SIST improves the joint goal accuracy of DST by up to 28.16% on MultiWOZ 2.0 and 8.41% on MultiWOZ 2.4, compared to using only the vanilla noisy labels.", "sections": [{"heading": "Introduction", "text": "Task-oriented dialogue systems play an important role in helping users accomplish a variety of tasks through verbal interactions (Young et al., 2013;Gao et al., 2019). Dialogue state tracking (DST) is an essential component of the dialogue manager in pipeline-based task-oriented dialogue systems. It aims to keep track of users' intentions at each turn of the conversation (Mrk\u0161i\u0107 et al., 2017). The state information indicates the progress of the conversation and is leveraged to determine the next system action and generate the next system response (Chen et al., 2017). As shown in Figure 1, the dialogue state is typically represented as a set of (slot, value) pairs .\nHi, how may I help you? I need to book a room at autumn house.\nDefinitely, for how many people and how many nights? Just me, 3 nights. Can you also give me information on the vue cinema? Sure. It is in the city centre, and the phone number is 08451962320.\nThanks for your help. That's all I need.\n(hotel-name, autumn house) (hotel-name, autumn house) (hotel-book people, 1) (hotel-book stay, 3) (attraction-name, vue cinema) (hotel-name, autumn house) (hotel-book people, 1) (hotel-book stay, 3) (attraction-name, vue cinema)", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "Dialogue Context", "text": "Dialogue State Figure 1: An example dialogue spanning two domains.\nOn the left is the dialogue context with system responses shown in orange and user utterances in green. The dialogue state at each turn is presented on the right.\nTherefore, the problem of DST is defined as extracting the values for all slots from the dialogue context at each turn of the conversation.\nOver the past few years, DST has made significant progress, attributed to a number of publicly available dialogue datasets, such as DSTC2 , FRAMES (El Asri et al., 2017), MultiWOZ 2.0 (Budzianowski et al., 2018), Cross-WOZ , and SGD . Among these datasets, MultiWOZ 2.0 is the most popular one. So far, lots of DST models have been built on top of it Wu et al., 2019;Ouyang et al., 2020;Hu et al., 2020;Ye et al., 2021b;Lin et al., 2021).\nHowever, it has been found out that there is substantial noise in the state annotations of MultiWOZ 2.0 (Eric et al., 2020). These noisy labels may impede the training of robust DST models and lead to noticeable performance decrease (Zhang et al., 2016). To remedy this issue, massive efforts have been devoted to rectifying the annotations, and four refined versions, including MultiWOZ 2.1 (Eric et al., 2020), MultiWOZ 2.2 , MultiWOZ 2.3 (Han et al., 2020b), and MultiWOZ 2.4 (Ye et al., 2021a), have been released. Even so, there are still plenty of noisy and inconsistent la-bels. For example, in the latest version MultiWOZ 2.4, the validation set and test set have been manually re-annotated and tend to be noise-free. While the training set is still noisy, as it remains intact. In reality, it is costly and laborious to refine existing large-scale noisy datasets or collect new ones with fully precise annotations (Wei et al., 2020), let alone dialogue datasets with multiple domains and multiple turns. In view of this, we argue that it is essential to devise particular learning algorithms to train DST models robustly from noisy labels.\nAlthough loads of noisy label learning algorithms (Natarajan et al., 2013;Han et al., 2020a) have been proposed in the machine learning community, most of them target only multi-class classification (Song et al., 2020). However, as illustrated in Figure 1, the dialogue state may contain multiple labels, which makes it unstraightforward to apply existing noisy label learning algorithms to the DST task. In this paper we propose a general framework, named ASSIST (lAbel noiSe-robuSt dIalogue State Tracking), to train DST models robustly from noisy labels. ASSIST first trains an auxiliary model on a small clean dataset to generate pseudo labels for each sample in the noisy training set. Then, it leverages both the generated pseudo labels and vanilla noisy labels to train the primary model. Since the auxiliary model is trained on the clean dataset, it can be expected that the pseudo labels will help us train the primary model more robustly. Note that ASSIST is based on the assumption that we have access to a small clean dataset. This assumption is reasonable, as it is feasible to manually collect a small noise-free dataset or re-annotate a portion of a large noisy dataset.\nIn summary, our main contributions include:\n\u2022 We propose a general framework ASSIST to train robust DST models from noisy labels.\nTo the best of our knowledge, we are the first to tackle the DST problem by taking into consideration the label noise.\n\u2022 We theoretically analyze why the pseudo labels are beneficial and show that a proper combination of the pseudo labels and vanilla noisy labels can approximate the unknown true labels more accurately.\n\u2022 We conduct extensive experiments on Multi-WOZ 2.0 & 2.4. The results demonstrate that ASSIST can improve the DST performance on both datasets by a large margin.", "n_publication_ref": 16, "n_figure_ref": 2}, {"heading": "Problem Definition", "text": "In this section, we first provide the conventional definition of DST and then extend the definition to the noisy label learning scenario.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conventional Dialogue State Tracking", "text": "Let X = {(R 1 , U 1 ), . . . , (R T , U T )} denote a dialogue of T turns, where R t and U t represent the system response and user utterance at turn t, respectively. The dialogue state at turn t is defined as B t = {(s, v t )|s \u2208 S}, where S denotes the set of predefined slots and v t is the corresponding value of slot s. Following previous work Hu et al., 2020;Ye et al., 2021b), a slot in this paper refers to the concatenation of the domain name and slot name so as to include the domain information. For example, we use \"hotel-name\" to represent the slot \"name\" in the hotel domain.\nIn general, the issue of DST is defined as learning a dialogue state tracker F : X t \u2192 B t that takes the dialogue context X t as input and predicts the dialogue state B t at each turn t as accurately as possible. Here, X t represents the dialogue history up to turn t, i.e., X t = {(R 1 , U 1 ), . . . , (R t , U t )}.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Dialogue State Tracking with Noisy Labels", "text": "Conventionally, all the state labels are assumed to be correct. However, this assumption may not hold.\nIn practice, dialogue state annotations are errorprone (Han et al., 2020b). There are a couple of reasons. First, the states are usually annotated by crowdworkers to improve the labelling efficiency. Due to limited knowledge, crowdworkers cannot annotate all the states with 100% accuracy, which naturally incurs noisy labels (Han et al., 2020a). Second, the dialogue may span multiple domains, which also increases the labelling difficulty. Apparently, the noisy labels are harmful and likely to lead to sub-optimal performance. Therefore, it is crucial to take them into consideration so as to train DST models more robustly. LetB t = {(s,\u1e7d t )|s \u2208 S} denote the noisy state annotations, where\u1e7d t is the noisy label of slot s at turn t. We use B t = {(s, v t )|s \u2208 S} to denote the noise-free state annotations. Here, v t represents the true label of slot s at turn t, which is unknown. In fact, existing DST approaches are only able to learn a sub-optimal dialogue state trackerF : X t \u2192B t rather than the optimal state tracker F : X t \u2192 B t , as none of them have considered the influence of noisy labels. In this work, we aim to learn a robust state tracker F * that can better approximate F from the noisy state annotationsB t .", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Proposed Approach", "text": "We introduce a general framework ASSIST, aiming to train DST models robustly from noisy labels. We assume that a small clean dataset is accessible. Based on this dataset, ASSIST first trains an auxiliary model A. Then, it leverages A to generate pseudo labels for each sample in the noisy training set. The pseudo state annotations are represented asB t = {(s,v t )|s \u2208 S}, wherev t denotes the pseudo label of slot s at turn t. Afterwards, both the generated pseudo labels and vanilla noisy labels are exploited to train the primary model F * . That is, we intend to learn F * : X t \u2192 C(B t ,B t ), where C(B t ,B t ) is a combination ofB t andB t .\nEssentially, any existing DST models can be employed as the auxiliary model. However, these models may lead to overfitting due to the small size of the clean dataset. To tackle this issue, we propose a new simple model as the auxiliary model 1 .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Auxiliary Model Architecture", "text": "Figure 2 shows the architecture, which consists of a dialogue context semantic encoder, a slot attention module, and a slot-value matching module.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Dialogue Context Semantic Encoder", "text": "Similar to Ye et al., 2021b), we utilize the pre-trained language model BERT (Devlin et al., 2019) to encode the dialogue context X t into contextual semantic representations. Let Z t = R t \u2295 U t be the concatenation of the system response and user utterance at turn t, where \u2295 denotes the operator of sequence concatenation. Then, the dialogue context X t can be represented as\nX t = Z 1 \u2295 Z 2 \u2295 \u2022 \u2022 \u2022 \u2295 Z t .\nWe also concatenate each slot-value pair and denote the representation of the dialogue state at turn t as B t = (s,vt)\u2208Bt,vt =none s \u2295 v t , in which only non-none slots are included. B t can serve as a compact representation of the dialogue history. In view of this, we treat the previous turn dialogue state B t\u22121 as part of the input as well, which can be beneficial when X t exceeds the maximum input length of BERT. The complete input sequence to the encoder module is then denoted as:\nI t = [CLS]\u2295X t\u22121 \u2295B t\u22121 \u2295[SEP ]\u2295Z t \u2295[SEP ],\n1 We adopt existing DST models as the primary model.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "BERT BERT", "text": "[CLS]\n\u22ef \u22ef [SEP] [SEP] [CLS] \u22ef [SEP]\nSlot Attention  Let H t \u2208 R |It|\u00d7d be the semantic matrix representation of I t . Here, |I t | and d denote the sequence length of I t and the BERT output dimension, respectively. Then, we have:\nH t = BERT f inetune (I t ),\nwhere BERT f inetune means that the BERT model will be fine-tuned during the training process.\nFor each slot s and its candidate value v \u2208 V s , we employ another BERT to encode them into semantic vectors h s \u2208 R d and h v \u2208 R d . Here, V s denotes the candidate value set of slot s. Unlike the dialogue context, we leverage the pre-trained BERT without fine-tuning to embed s and v . Besides, we adopt the output vector corresponding to the special token [CLS] as an aggregated representation of slot s and value v , i.e.,\nh s = BERT [CLS] f ixed ([CLS] \u2295 s \u2295 [SEP ]), h v = BERT [CLS] f ixed ([CLS] \u2295 v \u2295 [SEP ]).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Slot Attention", "text": "The slot attention module is exploited to retrieve slot-relevant information for all the slots from the same dialogue context. The slot attention is a multihead attention (Vaswani et al., 2017). Specifically, the slot representation h s is regarded as the query vector, and the dialogue context representation H t is taken as both the key matrix and value matrix. The slot attention matches h s to the semantic vector of each word in the dialogue context and calculates the attention score, based on which the slot-specific information can be extracted. Let a s t \u2208 R d denote a d-dimensional vector representation of the related information of slot s at turn t, we obtain:\na s t = MultiHead(h s , H t , H t ). a s\nt is expected to be close to the semantic vector representation of the true value of slot s.\nConsidering that the output of BERT is normalized by layer normalization (Ba et al., 2016), we also feed a s t to a layer normalization layer, which is preceded by a linear transformation layer. The final slot-specific vector g s t \u2208 R d is calculated as: g s t = LayerNorm(Linear(a s t )).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Slot-Value Matching", "text": "The slot-value matching module is utilized to predict the value of each slot s. It first calculates the distance between the slot-specific representation g s t and the semantic representation of each candidate value v \u2208 V s , i.e., h v . Then, the candidate value with the smallest distance is selected as the prediction. The 2 norm is adopted to compute the distance. Denotingv t as the predicted value of slot s at turn t, we have:\nv t = argmin v \u2208Vs g s t \u2212 h v 2 .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Auxiliary Model Training", "text": "We leverage a small clean dataset to train the auxiliary model. Since the true labels are available, the auxiliary model is directly trained to maximize the joint probability of all slot values. The probability of the true value v t of slot s at turn t is defined as:\np(v t |X t , s) = exp (\u2212 g s t \u2212 h v t 2 ) v \u2208Vs exp (\u2212 g s t \u2212 h v 2 ) ,\nwhere h v t is the semantic representation of v t . Maximizing the joint probability \u03a0 (s,vt)\u2208Bt p(v t |X t , s) is equivalent to minimizing the following objective:\nL aux = (s,vt)\u2208Bt \u2212 log p(v t |X t , s).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Pseudo Label Generation", "text": "Our approach depends on the auxiliary model A to generate pseudo labelsB t = {(s,v t )|s \u2208 S} for each sample in the noisy training set. In this work, we treat each dialogue context X t rather than the entire dialogue as a training sample. Without loss of generality, the pseudo label generation process is denoted as follows:\nB t = A(X t , S),\nwhere X t belongs to the noisy training set.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Primary Model Training", "text": "To reduce the influence of noisy labels, we combine the generated pseudo labels and vanilla noisy labels to train the primary model.\nLetv t and\u1e7d t be the one-hot representation of the pseudo labelv t and vanilla noisy label\u1e7d t , respectively. Then, we can define the combined label as: \nv c t = \u03b1v t + (1 \u2212 \u03b1)\u1e7d t , where \u03b1(0 \u2264 \u03b1 \u2264 1)\nL pri = (s,v c t )\u2208C(Bt,Bt) \u2212 log p(v c t |X t , s) = \u03b1 (s,vt)\u2208Bt \u2212 log p(v t |X t , s) + (1 \u2212 \u03b1) (s,\u1e7dt)\u2208Bt \u2212 log p(\u1e7d t |X t , s) = \u03b1L pseudo + (1 \u2212 \u03b1)L vanilla ,\nwhere L pseudo and L vanilla correspond to the training objective of using only the pseudo labels and using only the vanilla noisy labels, respectively. By minimizing L pri , the primary model is trained to learn from the vanilla noisy labels and at the same time imitate the predictions of the auxiliary model.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Theoretical Analysis", "text": "Since the pseudo labels are generated by the auxiliary model that has been trained on a small clean dataset, it can be expected that the combined labels are able to serve as a better approximation to the unknown true labels. Let v t denote the one-hot representation of the unknown true value v t of slot s at turn t. We adopt the mean squared loss to define the approximation error of any corrupted labelsv t associated with the noisy training set D n as:\nYv = 1 |D n ||S| Xt\u2208Dn s\u2208S E Dc [ v t \u2212 v t 2 2 ],\nwhere the expectation ranges over different choices of the clean dataset D c , and | \u2022 | returns the cardinality of a set.\nNext, we show that the approximation error of the combined labels can be smaller than that of both the vanilla noisy labels and the generated pseudo labels. The details are presented in Theorem 1.\nTheorem 1. The optimal approximation error with respect to the combined labels v c t is smaller than that of the vanilla labels\u1e7d t and pseudo labelsv t , i.e., min \u03b1 Y v c < min{Y\u1e7d, Yv}.\nBy setting \u03b1 = Y\u1e7d Y\u1e7d+Yv , Y v c reaches its minimum:\nmin \u03b1 Y v c = Y\u1e7dYv Y\u1e7d + Yv .\nProof. The proof is presented in Appendix A.\nTheorem 1 indicates that if \u03b1 is set properly, the combined labels can approximate the unknown true labels more accurately. Hence, we can potentially train the primary model more robustly. Note that we cannot calculate the optimal value of \u03b1 directly.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets", "text": "We adopt MultiWOZ 2.0 (Budzianowski et al., 2018) and MultiWOZ 2.4 (Ye et al., 2021a) as the datasets in our experiments. MultiWOZ 2.0 is one of the largest publicly available multi-domain taskoriented dialogue datasets, including about 10,000 dialogues spanning seven domains. MultiWOZ 2.4 is the latest refined version of MultiWOZ 2.0. The annotations of its validation set and test set have been manually rectified. While its training set remains intact and is the same as that of MultiWOZ 2.1 (Eric et al., 2020), in which 41.34% of the state values are changed, compared to MultiWOZ 2.0.\nSince the hospital domain and police domain never occur in the test set, we use only the remaining five domains {attraction, hotel, restaurant, taxi, train} in our experiments. These domains have 30 slots in total. Considering that the validation set and test set of MultiWOZ 2.0 are noisy, we replace them with the counterparts of MultiWOZ 2.4 2 . We preprocess the datasets following (Ye et al., 2021b). We use the validation set as the small clean dataset.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Evaluation Metrics", "text": "We exploit joint goal accuracy and slot accuracy as the evaluation metrics. The joint goal accuracy is 2 Despite this change, we still call the dataset MultiWOZ 2.0 in this paper for ease of exposition. defined as the proportion of dialogue turns in which the values of all slots are correctly predicted. It is the most important metric in the DST task. The slot accuracy is defined as the average of all individual slot accuracies. The accuracy of an individual slot is calculated as the ratio of dialogue turns in which its value is correctly predicted.\nWe also propose a new evaluation metric, termed as joint turn accuracy. We define joint turn accuracy as the proportion of dialogue turns in which the values of all active slots are correctly predicted. A slot becomes active if its value is mentioned in current turn and is not inherited from previous turns. The advantage of joint turn accuracy is that it can tell us in how many turns the turn-level information is fully captured by the model.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Primary DST Models", "text": "To verify the effectiveness of the proposed framework, we apply the generated pseudo labels to three different primary models.\nSOM-DST: SOM-DST ) is an open vocabulary-based method. It treats the dialogue state as an explicit fixed-sized memory and selectively overwrites this memory at each turn. STAR: STAR (Ye et al., 2021b) is a predefined ontology-based method. It leverages a stacked slot self-attention mechanism to capture the slot dependencies automatically.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "AUX-DST:", "text": "We also test using the proposed auxiliary model as the primary model. For the sake of description, we refer to this model as AUX-DST.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Implementation Details", "text": "For the auxiliary model, the pre-trained BERT-baseuncased model is utilized as the dialogue context encoder. Another pre-trained BERT-base-uncased model with fixed weights is employed to encode the slots and their candidate values. The maximum input length of the BERT model is set to 512. The number of heads in the slot attention module is set to 4. The output dimension of the linear transformation layer is set to 768, which is the same as the dimension of the BERT outputs. Recall that the previous turn dialogue state is treated as part of the input. The ground-truth one is used during training, and the predicted one is used during testing 3 .  We train the auxiliary model on the clean validation set and the primary model on the noisy training set. When training the auxiliary model, the noisy training set is leveraged to choose the best model. For all primary models, the parameter \u03b1 is set to 0.6 on MutliWOZ 2.0 and 0.4 on MultiWOZ 2.4. More training details can be found in Appendix B.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Main Results", "text": "Table 1 presents the performance scores of the three different primary DST models on the test sets of MultiWOZ 2.0 & 2.4 when they are trained using our proposed framework ASSIST. For comparison, we also include the results when only the vanilla labels or only the pseudo labels are used to train the primary models.\nAs can be seen, ASSIST consistently improves the performance of the three primary models on both datasets. More concretely, compared to the results obtained using only the vanilla labels, AS-SIST improves the joint goal accuracy of SOM-DST, STAR, and AUX-DST on MultiWOZ 2.0 by 25.69%, 25.82%, and 28.16% absolute gains, respectively. On MultiWOZ 2.4, ASSIST also leads to 8.41%, 5.79%, and 7.77% absolute joint goal accuracy gains. From Table 1, we further observe that the performance improvements on MultiWOZ 2.4 are lower than on MultiWOZ 2.0. This is because the training set of MultiWOZ 2.4 is the same as that of MultiWOZ 2.1 (Eric et al., 2020), in which lots of annotation errors have been fixed. We also observe that all the primary models demonstrate relatively good performance when only the pseudo labels are used. From these results, it can be con-cluded that the pseudo labels are beneficial and they can help us train DST models more robustly.\nAnother observation from Table 1 is that SOM-DST tends to show comparable or even higher joint turn accuracy compared to STAR and AUX-DST, although its performance is worse in terms of joint goal accuracy and slot accuracy. This is because SOM-DST focuses on turn-active slots and copies the values for other slots from previous turns, while both STAR and AUX-DST predict the values of all slots from scratch at each turn. These results show that the joint turn accuracy can help us understand in more depth how different models behave. ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Applying STAR as the Auxiliary Model", "text": "Although any existing DST models can be adopted as the auxiliary model, we chose to propose a new simple one to reduce overfitting. In order to verify the superiority of the proposed model, we also apply STAR as the auxiliary model and compare their performance in Figure 3. We chose STAR due to its good performance, as shown in Table 1. From Figure 3, we observe that all three primary 0 0.1 0. 2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  models demonstrate higher performance on both datasets when using the proposed auxiliary model than using STAR as the auxiliary model. The results indicate that the proposed auxiliary model is able to generate pseudo labels with higher quality.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Effects of Parameter \u03b1", "text": "The parameter \u03b1 adjusts the weights of the pseudo labels and vanilla labels in the training phase. Here, we study the effects of \u03b1 by varying its value in the range of 0 to 1 with a step size of 0.1. Figure 4 shows the results of AUX-DST. As can be seen, \u03b1 plays an important role in balancing the pseudo labels and vanilla labels. The best performance is achieved when \u03b1 is set to 0.6 on MultiWOZ 2.0 and 0.4 on MultiWOZ 2.4. Since the training set of MultiWOZ 2.0 has more noisy labels than that of MultiWOZ 2.4, more emphasis should be put on its pseudo labels to obtain the best performance. It is also noted that the performance difference between MultiWOZ 2.0 and MultiWOZ 2.4 dwindles away as \u03b1 increases. This is because the vanilla labels will contribute less to the training of the primary model when \u03b1 is set to be larger.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Effects of the Size of the Clean Dataset", "text": "Considering that our proposed framework ASSIST relies on a small clean dataset to train the auxiliary model that is further leveraged to generate pseudo labels for the training set, it is valuable to explore the effects of the size of the clean dataset on the performance of the primary model. For this purpose, we vary the number of dialogues in the clean dataset from 500 to 1000 4 to generate different pseudo labels. We then combine these different pseudo labels with the vanilla labels to train the primary model AUX-DST. The results on Multi-WOZ 2.4 are reported in Figure 5. For comparison,4 There are 1000 dialogues in total in the validation set.\nwe also include the results when only the pseudo labels or only the vanilla labels are used to train the primary model. As can be seen, the size of the clean dataset has a great impact on the performance of the primary model. Apparently, fewer clean data will lead to worse performance. Nevertheless, as long as the pseudo labels are combined with the vanilla labels, the primary model can consistently demonstrate the strongest performance.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Analyses on Pseudo Labels' Quality", "text": "The previous experiments have proven the effectiveness of the generated pseudo labels in training robust DST models. In this part, we provide further analyses on the quality of the pseudo labels to gain more insights into why they can be beneficial.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Quantitative Analysis", "text": "We first investigate whether the pseudo labels are consistent with the true labels. To achieve this goal, we can compute the joint goal accuracy and joint turn accuracy of the auxiliary model on the training set. However, the true labels of the training set are unavailable. As an alternative, we treat the vanilla noisy labels as true labels (note that only a portion of the vanilla labels are noisy). In this experiment, we also vary the number of clean dialogues to train the auxiliary model. Figure 6 presents the results. As shown in Figure 6, the auxiliary model achieves higher performance when more clean dialogues are utilized to train it. If the entire validation set is used, it achieves around 50% joint goal accuracy and around 75% joint turn accuracy. Given that the vanilla noisy labels are regarded as the true labels, we can conjecture that the true performance is actually higher. This experiment shows that the pseudo labels are consistent with the unknown true labels to some extent and can serve as a good complement to the vanilla noisy labels.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Dialogue Context", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Vanilla Labels", "text": "Pseudo Labels [sys]: Sure, da vinci pizzeria is a cheap Italian restaurant in the area.\n[usr]: Would you mind making a reservation for Thursday at 17:15? (restaurant-name, da vinci pizzeria) (restaurant-book day, thursday) (restaurant-book time, 17:15) (restaurant-name, da vinci pizzeria)\n[sys]: Do you have a preferred section of town?\n[usr]: Not really, but I want free wifi and it should be 4 star.\n(hotel-internet, free) (hotel-stars, 4) (hotel-area, dontcare) (hotel-internet, free) (hotel-stars, 4)\n[usr]: I need to find out if there is a train going to stansted airport that leaves after 12:30.\n(train-arriveby, 13:03) (train-destination, stansted airport) (train-leaveat, 12:30) (train-destination, stansted airport) (train-leaveat, 12:30)\n[usr]: I am staying in the west part of Cambridge and would like to know about some places to go. (attraction-area, west) (attraction-area, west) (hotel-area, west)\nTable 2: Four dialogue snippets with their vanilla labels and the generated pseudo labels. These dialogue snippets are chosen from the training set of MultiWOZ 2.4. To save space, we only present turn-active slots and their values.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Qualitative Analysis", "text": "To intuitively understand the quality of the pseudo labels, we show four dialogue snippets with their vanilla labels and the generated pseudo labels in Table 2. As can be seen, the vanilla labels of the first two dialogue snippets are incomplete, while all the missing information is presented in the pseudo labels. For the third dialogue snippet, the vanilla labels contain an unmentioned slot-value pair \"(trainarriveby, 13:03)\". This error has also been fixed in the pseudo labels. For the last dialogue snippet, the vanilla labels are correct. However, the pseudo labels introduce an overconfident prediction of the value of slot \"hotel-area\". This case study has verified again that the pseudo labels can be utilized to fix certain errors in the vanilla labels. However, the pseudo labels may bring about some new errors. Hence, we should combine the two types of labels so as to achieve the best performance.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Pseudo Labels vs. Simple Combination", "text": "Aiming to better validate the effectiveness of the proposed framework, we also report the results when the small clean dataset is directly combined with the large noisy training set to train the primary model. We adopt AUX-DST as the primary model and show the results in Table 3. Since the clean dataset (i.e., the validation set in our experiments) is combined with the training set, all the results in Table 3 are the best ones on the test set. As can be observed, a simple combination of the noisy training set and clean dataset can lead to better results. However, the performance improvements are lower, compared to using pseudo labels (especially on MultiWOZ 2.0 due to its noisier training set). It is also observed that when both the clean dataset and the pseudo labels are utilized to train the model, even higher performance can be achieved. These results indicate that our proposed framework can make better use of the small clean dataset to train the primary model.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Error Analysis", "text": "We further investigate the error rate with respect to each slot. We adopt AUX-DST as the primary model and use AUX-DST(w/o p) to denote the case when only the vanilla labels are employed to train the model. The results on the test set of MultiWOZ 2.4 are illustrated in Figure 7, from which we can observe that the slot \"hotel-type\" has the highest error rate. Even though the error rate is reduced with the aid of the pseudo labels, it is still the highest one among all the slots. This is because the labels of this slot are confusing. It is also observed that the \"name\"-related slots have relatively high error rates. However, when the pseudo labels are used, their error rates reduce remarkably. Besides, we observe that the error rates of some slots are higher when the pseudo labels are leveraged. This is probably due to the fact that we have used the same parameter \u03b1 to combine the pseudo labels and vanilla labels of all slots. In practice, the noise rate with respect to each slot in the vanilla labels may not be exactly the same. This observation in- spires us that more advanced techniques should be developed to combine the pseudo labels and vanilla labels, which we leave as our future work.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Related Work", "text": "In this section, we briefly review related work on DST and noisy label learning.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dialogue State Tracking", "text": "Recently, DST has got an enormous amount of attention, thanks to the availability of multiple largescale multi-domain dialogue datasets such as Multi-WOZ 2.0 (Budzianowski et al., 2018), MultiWOZ 2.1 (Eric et al., 2020), RiSAWOZ (Quan et al., 2020), and SGD . The most popular datasets are MultiWOZ 2.0 and MultiWOZ 2.1, and lots of DST models have been built on top of them Wu et al., 2019;Ouyang et al., 2020;Hosseini-Asl et al., 2020;Hu et al., 2020;Ye et al., 2021b;Lin et al., 2021;Liang et al., 2021). These recent DST models can be grouped into two categories: predefined ontology-based models and open vocabulary-based models. The predefined ontology-based models treat DST as a multi-label classification problem and tend to demonstrate better performance Shan et al., 2020;Ye et al., 2021b). The open vocabulary-based models leverage either span prediction (Heck et al., 2020; or sequence generation (Wu et al., 2019;Hosseini-Asl et al., 2020) to extract slot values from the dialogue context directly.\nAlthough these DST models have made a huge success, they can only achieve sub-optimal performance, due to the lack of handling noisy labels. To the best of our knowledge, we are the first to take the noisy labels into consideration when tackling the DST problem.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Noisy Label Learning", "text": "Addressing noisy labels in supervised learning is a long-term studied problem (Fr\u00e9nay and Verleysen, 2013;Song et al., 2020;Han et al., 2020a). This issue becomes more prominent in the era of deep learning, as training deep models generally requires a lot of well-labelled data, but it is expensive and time-consuming to collect large-scale datasets with completely clean annotations. This dilemma has sparked a surge of noisy label learning methods (Hendrycks et al., 2018;Zhang and Sabuncu, 2018;Song et al., 2019;Wei et al., 2020). Even so, these methods mainly focus on multi-class classification (Song et al., 2020), which makes it not straightforward to apply them to the DST task.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this work, we have presented a general framework ASSIST, aiming to train DST models robustly from noisy labels. ASSIST leverages an auxiliary model that is trained on a small clean dataset to generate pseudo labels for the large noisy training set. The pseudo labels are combined with the vanilla labels to train the primary model. Both theoretical analysis and empirical study have verified the validity of our proposed framework. In the future, we intend to explore more advanced techniques to combine the pseudo labels and vanilla noisy labels in a better way.\nConsidering that the pseudo labels are generated by the auxiliary model that is trained on an extra small clean dataset and this clean dataset is independent of the noisy training set, we can regard the pseudo labels and vanilla labels as independent of each other. Consequently, we obtain:\nE Dc [(\u1e7d t \u2212 v t ) T (v t \u2212 v t )] = [E Dc [\u1e7d t \u2212 v t ]] T E Dc [v t \u2212 v t ] = [E Dc [\u1e7d t \u2212 v t ]] T E Dc [v t \u2212 E Dc [v t ]] = [E Dc [\u1e7d t \u2212 v t ]] T 0 = 0.\nBased on the formula above, we can now calculate the approximation error with respect to the combined label v c t of slot s as below:\nE Dc [ v c t \u2212 v t 2 2 ] = E Dc [ \u03b1v t + (1 \u2212 \u03b1)\u1e7d t \u2212 v t 2 2 ] = E Dc [ \u03b1(v t \u2212 v t ) + (1 \u2212 \u03b1)(\u1e7d t \u2212 v t ) 2 2 ] = \u03b1 2 E Dc [ v t \u2212 v t 2 2 ] + (1 \u2212 \u03b1) 2 E Dc [ \u1e7d t \u2212 v t 2 2 ],\nwhere the last equality holds because of E Dc [(\u1e7d t \u2212 v t ) T (v t \u2212 v t )] = 0. Then, we have:\nY v c = 1 |D n ||S| Xt\u2208Dn s\u2208S E Dc [ v c t \u2212 v t 2 2 ] = \u03b1 2 |D n ||S| Xt\u2208Dn s\u2208S E Dc [ v t \u2212 v t 2 2 ] + (1 \u2212 \u03b1) 2 |D n ||S| Xt\u2208Dn s\u2208S E Dc [ \u1e7d t \u2212 v t 2 2 ] = \u03b1 2 Yv + (1 \u2212 \u03b1) 2 Y\u1e7d.\nY v c reaches its minimum when \u03b1 = Y\u1e7d Y\u1e7d+Yv , and\nmin \u03b1 Y v c = Y\u1e7dYv Y\u1e7d + Yv ,\nwhich concludes the proof.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B Training Details", "text": "Note that the proposed auxiliary model is also applied as one primary model in our experiments.\nIn both cases, AdamW (Kingma and Ba, 2014) is adopted as the optimizer, and a linear schedule with warmup is created to adjust the learning rate dynamically. The peak learning rate is set to 2.5e-5. The warmup proportion is fixed at 0.1. The dropout (Srivastava et al., 2014) probability and word dropout (Bowman et al., 2016) probability are also fixed at 0.1. When taken as the auxiliary model, the model is trained for at most 30 epochs with a batch size of 8. When taken as the primary model, the batch size and training epochs are set to 8 and 12, respectively. The best model is chosen according to the performance on the validation set.\nWe apply left truncation when the input exceeds the maximum input length of BERT.\nFor SOM-DST and STAR, the default hyperparameters are adopted when they are applied as the primary model (except setting num_workers = 0).  ", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "C.1 Effects of the Distribution of the Clean Dataset", "text": "Except for the size of the clean dataset, the distribution of the clean dataset may also affect the performance of the primary model, especially when the clean dataset has a significantly different distribution from the training set. Thus, it is important to study the effects of the distribution of the clean dataset. However, we are short of clean datasets with different distributions. It is also challenging to model the distribution explicitly since the dialogue state may contain multiple labels. To address this issue, we propose to remove all the dialogues that are related to a specific domain and use only the remaining ones as the clean dataset. As thus, we can create multiple clean datasets with different distributions. The results of AUX-DST on MultiWOZ 2.4 are shown in Figure 8. As can be observed, although different clean datasets indeed lead to different performance, compared to the situation where no clean data is used (i.e., only the vanilla labels are used to train the model), all these clean datasets still bring huge performance improvements.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Acknowledgments", "text": "This project was funded by the EPSRC Fellowship titled \"Task Based Information Retrieval\" and grant reference number EP/P024289/1.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Proof of Theorem 1", "text": "Proof. Our proof is based on the bias-variance decomposition theorem 5 . For any sample X t in the noisy training set D n , the approximation error with respect to the pseudo labelv t of slot s is defined as\n], which, according to the biasvariance decomposition theorem, can be decomposed into a bias term and a variance term, i.e.,\nwhere\nIn our approach, the auxiliary model is a BERTbased model, which has more than 110M parameters. Such a complex model is expected to be able to capture all the samples in the small clean dataset D c . Therefore, we can reasonably assume that the bias term is close to zero. Then, we have:", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "", "journal": "", "year": "", "authors": "Jimmy Lei Ba; Jamie Ryan Kiros; Geoffrey E Hin"}, {"title": "Generating sentences from a continuous space", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "R Samuel; Luke Bowman; Oriol Vilnis; Andrew Vinyals; Rafal Dai; Samy Jozefowicz;  Bengio"}, {"title": "MultiWOZ -a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling", "journal": "", "year": "2018", "authors": "Pawe\u0142 Budzianowski; Tsung-Hsien Wen; Bo-Hsiang Tseng; I\u00f1igo Casanueva; Stefan Ultes; Milica Osman Ramadan;  Ga\u0161i\u0107"}, {"title": "Schema-guided multi-domain dialogue state tracking with graph attention neural networks", "journal": "", "year": "2020", "authors": "Lu Chen; Boer Lv; Chi Wang; Su Zhu; Bowen Tan; Kai Yu"}, {"title": "Deep learning for dialogue systems", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Yun-Nung Chen; Asli Celikyilmaz; Dilek Hakkani-T\u00fcr"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Frames: a corpus for adding memory to goal-oriented dialogue systems", "journal": "", "year": "2017", "authors": "Layla El Asri; Hannes Schulz; Shikhar Sharma; Jeremie Zumer; Justin Harris; Emery Fine; Rahul Mehrotra; Kaheer Suleman"}, {"title": "MultiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines", "journal": "", "year": "2020", "authors": "Mihail Eric; Rahul Goel; Shachi Paul; Abhishek Sethi; Sanchit Agarwal; Shuyang Gao; Adarsh Kumar; Anuj Goyal; Peter Ku; Dilek Hakkani-Tur"}, {"title": "A sequenceto-sequence approach to dialogue state tracking", "journal": "", "year": "2020", "authors": "Yue Feng; Yang Wang; Hang Li"}, {"title": "Classification in the presence of label noise: a survey", "journal": "", "year": "2013", "authors": "Beno\u00eet Fr\u00e9nay; Michel Verleysen"}, {"title": "Neural approaches to conversational ai. Foundations and Trends\u00ae in Information Retrieval", "journal": "", "year": "2019", "authors": "Jianfeng Gao; Michel Galley; Lihong Li"}, {"title": "From machine reading comprehension to dialogue state tracking: Bridging the gap", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Shuyang Gao; Sanchit Agarwal; Di Jin; Tagyoung Chung; Dilek Hakkani-Tur"}, {"title": "A survey of label-noise representation learning: Past, present and future", "journal": "", "year": "2020", "authors": "Bo Han; Quanming Yao; Tongliang Liu; Gang Niu; Ivor W Tsang; James T Kwok; Masashi Sugiyama"}, {"title": "Multiwoz 2.3: A multi-domain taskoriented dataset enhanced with annotation corrections and co-reference annotation", "journal": "", "year": "2020", "authors": "Ting Han; Ximing Liu; Ryuichi Takanobu; Yixin Lian; Chongxuan Huang; Wei Peng; Minlie Huang"}, {"title": "TripPy: A triple copy strategy for value independent neural dialog state tracking", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Michael Heck; Nurul Carel Van Niekerk; Christian Lubis; Hsien-Chin Geishauser; Marco Lin; Milica Moresi;  Gasic"}, {"title": "The second dialog state tracking challenge", "journal": "", "year": "2014", "authors": "Matthew Henderson; Blaise Thomson; Jason D Williams"}, {"title": "Using trusted data to train deep networks on labels corrupted by severe noise", "journal": "", "year": "2018", "authors": "Dan Hendrycks; Mantas Mazeika; Duncan Wilson; Kevin Gimpel"}, {"title": "Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue", "journal": "", "year": "", "authors": "Ehsan Hosseini-Asl; Bryan Mccann; Chien-Sheng Wu"}, {"title": "SAS: Dialogue state tracking via slot attention and slot information sharing", "journal": "", "year": "2020", "authors": "Jiaying Hu; Yan Yang; Chencai Chen; Liang He; Zhou Yu"}, {"title": "Efficient dialogue state tracking by selectively overwriting memory", "journal": "", "year": "2020", "authors": "Sungdong Kim; Sohee Yang; Gyuwan Kim; Sang-Woo Lee"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "SUMBT: Slot-utterance matching for universal and scalable belief tracking", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Hwaran Lee; Jinsik Lee; Tae-Yoon Kim"}, {"title": "Attention guided dialogue state tracking with sparse supervision", "journal": "", "year": "2021", "authors": "Shuailong Liang; Lahari Poddar; Gyuri Szarvas"}, {"title": "Knowledge-aware graph-enhanced gpt-2 for dialogue state tracking", "journal": "", "year": "2021", "authors": "Weizhe Lin; Bo-Hsian Tseng; Bill Byrne"}, {"title": "Neural belief tracker: Data-driven dialogue state tracking", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Nikola Mrk\u0161i\u0107; \u00d3 Diarmuid; Tsung-Hsien S\u00e9aghdha; Blaise Wen; Steve Thomson;  Young"}, {"title": "Learning with noisy labels", "journal": "", "year": "2013", "authors": "Nagarajan Natarajan; S Inderjit; Pradeep Dhillon; Ambuj Ravikumar;  Tewari"}, {"title": "Dialogue state tracking with explicit slot connection modeling", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Yawen Ouyang; Moxin Chen; Xinyu Dai; Yinggong Zhao; Shujian Huang; Jiajun Chen"}, {"title": "RiSAWOZ: A large-scale multidomain Wizard-of-Oz dataset with rich semantic annotations for task-oriented dialogue modeling", "journal": "", "year": "2020", "authors": "Jun Quan; Shian Zhang; Qian Cao; Zizhong Li; Deyi Xiong"}, {"title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset", "journal": "", "year": "2020", "authors": "Abhinav Rastogi; Xiaoxue Zang; Srinivas Sunkara; Raghav Gupta; Pranav Khaitan"}, {"title": "A contextual hierarchical attention network with adaptive objective for dialogue state tracking", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Yong Shan; Zekang Li; Jinchao Zhang; Fandong Meng; Yang Feng; Cheng Niu; Jie Zhou"}, {"title": "Selfie: Refurbishing unclean samples for robust deep learning", "journal": "PMLR", "year": "2019", "authors": "Hwanjun Song; Minseok Kim; Jae-Gil Lee"}, {"title": "Learning from noisy labels with deep neural networks: A survey", "journal": "", "year": "2020", "authors": "Hwanjun Song; Minseok Kim; Dongmin Park; Yooju Shin; Jae-Gil Lee"}, {"title": "Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research", "journal": "", "year": "2014", "authors": "Nitish Srivastava; Geoffrey Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"title": "Combating noisy labels by agreement: A joint training method with co-regularization", "journal": "", "year": "2020", "authors": "Hongxin Wei; Lei Feng; Xiangyu Chen; Bo An"}, {"title": "The dialog state tracking challenge series", "journal": "AI Magazine", "year": "2014", "authors": "D Jason; Matthew Williams; Antoine Henderson; Blaise Raux;  Thomson"}, {"title": "Transferable multi-domain state generator for task-oriented dialogue systems", "journal": "", "year": "2019", "authors": "Chien-Sheng Wu; Andrea Madotto; Ehsan Hosseini-Asl; Caiming Xiong; Richard Socher; Pascale Fung"}, {"title": "Multiwoz 2.4: A multi-domain task-oriented dialogue dataset with essential annotation corrections to improve state tracking evaluation", "journal": "", "year": "2021", "authors": "Fanghua Ye; Jarana Manotumruksa; Emine Yilmaz"}, {"title": "Slot selfattentive dialogue state tracking", "journal": "", "year": "2021", "authors": "Fanghua Ye; Jarana Manotumruksa; Qiang Zhang; Shenghui Li; Emine Yilmaz"}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "journal": "Proceedings of the IEEE", "year": "2013", "authors": "Steve Young; Milica Ga\u0161i\u0107; Blaise Thomson; Jason D Williams"}, {"title": "Multiwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines", "journal": "", "year": "2020", "authors": "Xiaoxue Zang; Abhinav Rastogi; Srinivas Sunkara; Raghav Gupta; Jianguo Zhang; Jindong Chen"}, {"title": "Understanding deep learning requires rethinking generalization", "journal": "", "year": "2016", "authors": "Chiyuan Zhang; Samy Bengio; Moritz Hardt; Benjamin Recht; Oriol Vinyals"}, {"title": "Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking", "journal": "", "year": "2020", "authors": "Jianguo Zhang; Kazuma Hashimoto; Chien-Sheng Wu; Yao Wang; Philip Yu; Richard Socher; Caiming Xiong"}, {"title": "Generalized cross entropy loss for training deep neural networks with noisy labels", "journal": "", "year": "2018", "authors": "Zhilu Zhang; R Mert;  Sabuncu"}, {"title": "Crosswoz: A large-scale chinese cross-domain task-oriented dialogue dataset", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Qi Zhu; Kaili Huang; Zheng Zhang; Xiaoyan Zhu; Minlie Huang"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Overall architecture of the auxiliary model. The parameters of the BERT used to encode slots and values are fixed during the training process.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Performance comparison on MultiWOZ 2.0 and MultiWOZ 2.4 by adopting STAR as the auxiliary model. We use lowercase letters in the legend to show that the models are taken as the auxiliary model.", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 6 :6Figure 6: Performance of the auxiliary model evaluated on the noisy training set of MultiWOZ 2.4.", "figure_data": ""}, {"figure_label": "8", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 8 :8Figure 8: Analyses on the effects of the distribution of the clean dataset by removing all the dialogues related to each domain. \"w/o all\" means no clean data is used.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "|X t , s) = p(v t |X t , s) \u03b1 p(\u1e7d t |X t , s) (1\u2212\u03b1) .Here, p(v t |X t , s) and p(\u1e7d t |X t , s) correspond to the probability ofv t and\u1e7d t , respectively.Let C(B t ,B t ) = {(s, v c t )|s \u2208 S} represent the combined state annotations. The training objective of the primary model is then defined as:", "figure_data": "is a parameter to balance thepseudo labels and vanilla labels. We calculate theprobability of v c t as below:p(v c t"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Performance comparison on MultiWOZ 2.0 and MultiWOZ 2.4. Note that MultiWOZ 2.0 and MultiWOZ 2.4 share the same test set in our experiments. The best scores are highlighted in bold.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": The joint goal accuracy (%) of AUX-DST onMultiWOZ 2.0 & 2.4 under different training settings.T: the noisy training set. C: the small clean dataset. P:the generated pseudo labels of the original training set.The reported scores are the best ones on the test set."}], "doi": "10.18653/v1/K16-1002"}