{"authors": "Jonathan Rusert; Padmini Srinivasan", "pub_date": "", "title": "Don't sweat the small stuff, classify the rest: Sample Shielding to protect text classifiers against adversarial attacks", "abstract": "Deep learning (DL) is being used extensively for text classification. However, researchers have demonstrated the vulnerability of such classifiers to adversarial attacks. Attackers modify the text in a way which misleads the classifier while keeping the original meaning close to intact. State-of-the-art (SOTA) attack algorithms follow the general principle of making minimal changes to the text so as to not jeopardize semantics. Taking advantage of this we propose a novel and intuitive defense strategy called Sample Shielding. It is attacker and classifier agnostic, does not require any reconfiguration of the classifier or external resources and is simple to implement. Essentially, we sample subsets of the input text, classify them and summarize these into a final decision. We shield three popular DL text classifiers with Sample Shielding, test their resilience against four SOTA attackers across three datasets in a realistic threat setting. Even when given the advantage of knowing about our shielding strategy the adversary's attack success rate is <= 10% with only one exception and often < 5%. Additionally, Sample Shielding maintains near original accuracy when applied to original texts. Crucially, we show that the 'make minimal changes' approach of SOTA attackers leads to critical vulnerabilities that can be defended against with an intuitive sampling strategy. 1   ", "sections": [{"heading": "Introduction", "text": "Text classifiers have become ubiquitous. Unfortunately, they are subject to attacks from adversaries, typically executed using machine learning methods. Attackers work by making small modifications to the text that mislead the classifier. Adversarial attackers are now a growing part of the ecosystem.\nLike classifiers, attack algorithms have achieved strong success due to advances in machine learning/deep learning. Current text attackers, like 1 Our code and data are available at: https://github.com/JonRusert/SampleShielding TextFooler  and Bert-Attack (Li et al., 2020), are able to reduce near perfect classification accuracy down to 5%. Additionally, these attackers achieve this while perturbing (changing) only a small amount of the original text. This helps preserve the original meaning so that humans are able to understand the original message even though classifiers are duped.\nAs a counter, classifier shielding techniques are being explored. One such approach is adversarial training where the classifier, assumed to have access to the attacker, uses it to generate perturbed texts -these are added to the classifier's training data. While this leads to model resilience against that attacker it leaves the classifier open to attacks by new attackers. Other defenses involve modifying classifier structure to reduce the information an attacker can glean from it (Goel et al., 2020). However, this type of reconfiguration will not be possible if a third party classifier (e.g. Google Perspective) is leveraged. Even other approaches involve modifying the input text during classification time, but are currently limited to classifiers built from specific masked language models (Zeng et al., 2021) or rely on external synonym datasets (Wang et al., 2021a). We propose a shielding technique which is attacker-agnostic, does not require additional training/reconfiguration to the classifier, can shield any classifier, does not require an external data source, and can be used in a more realistic threat setting. We refer to this as Sample Shielding.\nSample Shielding takes advantage of current constraints in SOTA attacks. Mainly, to preserve original meaning, these make the minimal changes needed to deceive the classifier. For example, BERT-Attack (Li et al., 2020) only perturbs up to 16% of text, and often far less (e.g. 1.1 %) for some datasets. Thus, if we would look at the 84% to 99% of text that is untouched our model would be more likely to classify correctly. Hence, in Sample Shielding we take many samples of the input 2. We assess Sample Shielding under a realistic threat model where the attacker cannot query a website's classifier hundreds of times since that pattern is easily detectable by the website. We run experiments under two conditions, when the attacker has knowledge of Sample Shielding and when it does not. In both cases the attacker uses a local copy of the websites' classifier. This is an optimistic assumption favouring the attacker and thus provides a lower bound to our results.\n3. We test against 4 SOTA text attack algorithms, 3 text datasets and 3 classifiers. When the attacker does not have knowledge of Sample Shielding, our defense reduces attack success rate from near total decimation 90 -100% down to 13 -36%, while still maintaining accuracy on original texts. When the attacker has knowledge of Sample Shielding, our defense performs even better, reducing attacks down to 1 -10% success rate. This is partially due to Sample Shielding's random nature providing unreliable feedback to attackers.\nOur success with Sample Shielding is good news for classifiers -and it raises the bar significantly for the next generation attackers. We share code and our perturbed text collections for future research.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Methodology", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Threat model", "text": "The typical attack strategy perturbing texts with word synonyms or character substitutions assumes to have query access to the target web site's classifier (W ) (Yoo and Qi, 2021;Li et al., 2021a;Ren et al., 2019;Li et al., 2020;Garg and Ramakrishnan, 2020;Jia et al., 2019;Li et al., 2019). The text is modified by querying W hundreds or thousands of times, each time with a text version differing only slightly from the previouseven by just a single word (Li et al., 2020;. Such a querying pattern can be easily identified as adversarial by the website and countered. Thus, practically the only way in which such an attack can take place is when the attacker owns a local classifier W \u2032 which is either an exact copy of W or a close enough approximation. We adopt this more realistic threat model, shown in Figure 1.\nIn our threat model the attacker uses feedback from its local W \u2032 to generate a final perturbed version that defeats W \u2032 or is close enough to do so. The attacker submits only this final version to the website, expecting W to make the same error. However, the website defends W using Sample Shielding: sample based pre-processing on the input text, prior to applying W . The attacker may or may not be aware of this fact. Keeping W = W \u2032 which is consistent with other defenses, we evaluate our defense under two conditions:\n1) The attacker does not know that the website employs Sample Shielding pre-processing when classifying text using W .\n2) The Sample Shielding step is leaked and the attacker incorporates it locally when using W \u2032 to generate the final perturbed text.\nWe present results from experiments exploring both of these attack conditions.", "n_publication_ref": 8, "n_figure_ref": 1}, {"heading": "Sample Shielding approach", "text": "Intuition. Current adversarial attackers have two goals: fool the classifier and maintain the original meaning. Since they make minimal changes, the extent of perturbation is in fact one of the reported statistics. For example, (Li et al., 2020) note that their 10% perturbation rate is far less than in previous attacks. (Li et al., 2019) also focus on minimal changes (4%) needed in support of their attack success rate. Our defense approach capitalizes on this drive to make minimal changes. Specifically, in Sample Shielding, we take k samples each composed of p% of the text. We choose a p which minimizes the chance of a sample including attacked (modified) words, while maximizing the content available for the classifier to make a correct classification. We choose a k which is large enough to cover key information but small enough to reduce redundancy. We classify each sample and combine > 0.5 k sample predictions \u2026 I enjoyed this movie more than I thought I would. From multiple viewings it becomes especially clear how much time and energy the director put into this film. The choice for lead actor had me worried but it worked well. The twist was what really had me hooked. ...  their decisions for the final classification. We explore two sampling and three decision combining methods.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Sampling methods", "text": "Random Sampling. We randomly sample p portions of the text. We explore both sentences and words as sampled units. A visualization of random sampling is in Figure 2. Shifting Sampling. We sample the text using a moving window of length p \u00d7 length_of _text. The first starts at the beginning of the text. The next window starts right after the previous window ends. If there is insufficient text for the last window, then it wraps back to include the beginning text.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Decision strategy", "text": "Majority voting. This is a simple majority vote across the k samples (Figure 2). Classifier trained on sample scores from original texts (NN). We train a neural network summarizer to make a final class prediction based on the k sample probabilities. Since sample ID does not carry any information, the input to the neural network is a sorted list of sample probabilities. The intent is to see if the neural network picks up on latent patterns in the probabilities that are not captured by majority voting (see Figure 2). It should be emphasized that the neural network summarizer is trained only on probabilities generated from original texts and does not consider probabilities from attacker modified texts. We use a simple feed forward neural net composed of 2 linear layers (size 500 and 300) as classification summarizer. Classifier trained on sample scores from original and attacked texts (NN-BB). This is similar to the previous strategy except that the training data includes scores from original texts and texts that have been modified by the attacker. Because this assumes more knowledge of the attacker we expect NN-BB to perform better than NN. The ground truth label for these modified texts is the original correct class label.\n3 Experimental Setup", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Datasets", "text": "We examine three standard datasets in our experiments. Two have binary class labels (Yelp, IMDB) and the third has multi class labels (AG News). These have been used in adversarial generation and defense research (Zeng et al., 2021;Li et al., 2020). All datasets can be found via huggingface 2 .\n1. IMDB -Movie review dataset for binary sentiment classification. 25k examples are provided for training and testing respectively.\n2. Yelp -Yelp dataset for binary sentiment classification on reviews of businesses extracted from the Yelp Dataset Challenge 3 . 560k examples are provided for training and 38k for testing.\n3. AG News -News articles from over 2000 news sources annotated by type of news: Sports, World, Business, and Science/Tech. 120k training and 7k test sets are provided.\nFollowing previous research, (Li et al., 2020; we use all training data, and evaluate our method on random 1k samples of each dataset for the case where the local classifier does not employ Sample Shielding. Due to the high amount of queries used by the adversaries, we test on a subset of 100 samples for the case where the attacker's local classifier employs Sample Shielding. 4", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Adversarial models", "text": "We test our text classifier shielding strategy against 4 state-of-the-art (SOTA) text classifier attack algorithms. These algorithms have shown excellent performance in causing misclassifications while still producing readable texts. We defend against 3 word based attacks: TextFooler , Bert-Attack (Li et al., 2020), PWWS (Ren et al., 2019). TextFooler leverages word embeddings for word replacements, Bert-Attack leverages BERT itself by masking words and using BERT suggestions, PWWS selects and weights word replacements from WordNet. All three use some form of greedy selection for determining which words to replace. We also defend against a character based attack algorithm, TextBugger (Li et al., 2019).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Victim classifier models", "text": "We test our shielding approach against 3 standard classifiers 5 used in previous research, e.g. (Li et al., 2021a;Li et al., 2020):\n1. CNN -A word based CNN (Kim, 2014), with three window sizes (3,4,5), 100 filters per window with dropout of 0.3 and Glove embeddings.\n2. LSTM -A word based bidirectional LSTM with 150 hidden units. As with the CNN a dropout of 0.3 is used and Glove embeddings are leveraged.\n3. BERT -The 12 layer BERT base model which has been fine-tuned on the corresponding dataset. These are provided by textattack via huggingface 6 .", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Experimental design", "text": "We run experiments on the combination of the three victim classification models, three datasets, and four attack algorithms. These combinations are run on both threat model conditions (attacker is aware/ not aware of SampleShielding). This leads to 72 shielding experiments. For all attacks, we leverage TextAttack framework 7 which provides classification algorithms and adversarial text generation algorithms implemented as specified in respective papers (Morris et al., 2020). In all experiments where the attacker does not use Sample Shielding 4 We share the original and perturbed texts for replicability. We note that replicability of previous defenses are limited because the identity of their randomly sampled test instances are not provided. 5 We calibrated classifier accuracies against previous research (Li et al., 2020; 6 huggingface.co/textattack 7 textattack.readthedocs.io/en/latest/index.html we set k = 100 and p = 0.3. While better performance was achieved with other values in preliminary experiments, we chose to go with a single combination of p and k for simplicity. In experiments where the attacker uses Sample Shielding pre-processing we reduce k to 30 for efficiency. Except where otherwise noted, majority voting is used to generate results. Additionally, shifting sampling (Section 2.2.1) shielding typically achieved 10-20 points lower accuracy compared to random, thus we do not include it in the results.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Evaluation measures", "text": "We examine accuracy and Attack Success Rate:\naccuracy = #examples_classif ied_correctly #total_examples (1) ASR = Original Acc. \u2212 Attacked Acc. Original Acc.(2)\n4 Results\nWe first present results for the condition where the attacker is not aware of Sample Shielding based pre-processing and then the results for when the attacker also employs Sample Shielding.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Condition 1: Attacker does not know about Sample Shielding", "text": "Results are in Table 1. BERT is the strongest classifier achieving 91 -100% accuracy on the original datasets. Attacks are highly successful against unshielded texts. TextFooler and Bert-Attack are the most successful, dropping accuracies to 0-5% generally. Attacks were able to achieve strong drops with minimal amount of text perturbed (about 10%). Figure 3 shows that the average percent of words perturbed across datasets for each attack are about equal in the mid regions of the plots. For AG News, attacks are less successful against BERT; accuracy drops to 19% in the strongest attack (TextFooler), and only to 49% in the weakest (TextBugger ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Condition 2: Attacker knows about Sample Shielding", "text": "Results are in Table 3. As in the previous condition, classifiers perform well on original texts (Table 1) with BERT often achieving the highest accuracies. In this setting, every query by an attacker requires k samples to be processed, which greatly increases attack time. Thus, we reduce k to 30 for these experiments.\nSample Shielding repels attacks even when attacker uses Sample Shielding. We see that shielding is extremely successful in almost completely removing the negative effects of the attacks. For example, on the IMDB -TextFooler combination, attack success rate drops from 100 to 5 for LSTM, 100 to 1 for CNN, and 99 to 6 against BERT. The largest protection provided by Sample Shielding (100%) is for TextBugger vs CNN in IMDB. The smallest is for 85% (PWWS vs LSTM). On average the protection is 88.8%. The recovered accuracies are only 13 to 0 percent away from the originals.     These results show the power of Sample Shielding as even with knowledge of both the classifier and Sample Shielding, attacks struggle to perturb the text in a manner that causes W to fail. Furthermore, the attacks do worse with feedback from Sample Shielding. This shows the misleading nature of feedback from Sample Shielding, and unreliability when guiding attacks.\n5 Additional Analysis", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Parameter search", "text": "Increasing p raises the risk of samples containing increased amounts of perturbed text. Decreasing k raises the risk of not covering enough of the unperturbed portions of the original text. While our settings of p = 0.3 and k = 100 for our main results are reasonable values (Table 1, Table 2) they are not necessarily optimal. Optimal p. Figure 4 shows the results for all com- binations of attacks against LSTM on IMDB with word shielding as the defense, k fixed at 100. As we increase p, we see a continued drop in accuracy which is consistent with the idea that a higher p is more likely to capture perturbed text. The optimal value range appears to be in 0.2 -0.4 range, although we do not see large drops until 0.6 onward.\nWe also examined the same combination on AG News (Figure 5) since it's texts are considerably shorter and found consistent results.\nOptimal k. Figure 6 shows results for all attacks against LSTM on IMDB with word sampling as the defense, p fixed at 0.3. The optimal k is not as clear as p. We see clear increases after 30 samples, but then the optimal k varies depending on attack. However, we see a leveling off around 90 samples, which gives some credence to our chosen k of 100. We also found similar results when examining the same combination on AG News (Figure 7), however, k stabilized lower (about 50). ", "n_publication_ref": 0, "n_figure_ref": 4}, {"heading": "Reliability of Sample Shielding", "text": "Due to the randomness of samples, there may be concern over the consistency of Sample Shielding.\nTo address this, we ran Sample Shielding 100 times on the IMDB attacked texts from Table 3 against BERT classifier. Each time 30 random samples were used to vote. As can be observed from Figure 8, Sample Shielding consistently protects against attacks. Median accuracies are above 80% dropping only to 75% in the worst case. This points to Sample Shielding as a consistent, reliable defense.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Comparison with other SOTA Defenses", "text": "Comparisons are limited as threat models differ.\nAs noted earlier, other defenses assume a weaker threat model where the attacker queries the web- site's shielded W directly. To make ours equivalent we compare SOTA results with our accuracies obtained by the attacker using W \u2032 alone (with W = W \u2032 ). We calculate accuracies right after the final perturbed text is generated using W \u2032 eliminating a followup round of W with Sample Shielding.  3). While we do not know how FreeLB++, RanMask, and similar defenses would perform with our threat model any deterministic shield would give the exact same results when the classifier is applied once again by the website.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Limitations/Future work", "text": "First, in future work we will add in direct comparisons to the two closest methods to Sample Shielding (Zeng et al., 2021;Wang et al., 2021a). They are similar in spirit as they also work off samples though these are generated differently. We have not compared with them because these two papers appeared very recently, one last revised in July (Zeng et al., 2021) and the other appeared in arXiv in September 2021 (Wang et al., 2021a). Second, the neural net summarizer leverages a simple linear layer. Other networks, e.g., LSTM, maybe better at finding patterns in sequential data. In future work we will also explore layering Sample Shielding onto other defense strategies.\nAnother limitation of our current method is that we do not measure Sample Shielding's effectiveness on other common text tasks including Natural Language Understanding. Additionally, datasets which contain the shortest texts (e.g.  not currently tested in our experiments. Since sample shielding removes texts, it's performance could drop for these tasks and short texts. Thus, future work will include these comparisons.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Defenses using voting. The most similar methods to our own are RanMask and RS&V both appearing within the last five months. RanMask (Zeng et al., 2021) randomly masks tokens in input texts. This random masking occurs n times generating n inputs to be fed to a classifier. RS&V (Wang et al., 2021a) randomly replaces words in the input with synonyms. This it does k times to produce k samples which are then voted on. If the samples vote for a different label than the label produced by the unsampled input, then the text is labeled as an adversarial text. Our method is advantageous since it does not rely on specific models (i.e. Masked Language Model) or synonym sources.\nAdversarial training. Classifiers train on perturbed data, learning to identify modified versions of the original input (Wang and Wang, 2020;Wang et al., 2021b;Zhu et al., 2020;Li et al., 2021b).\nAs an example, Gil et al. (2019) propose HotFlip which uses white-box knowledge to generate adversarial attacks to train on. Specifically, they flip tokens based on the gradients of the one-hot input vectors. However, adversarial defenses are limited to known attackers. In contrast, Sample Shielding is 'plug-and-play' as it is a pre-processing step.\nOther defenses. Several other shielding methods exist (Keller et al., 2021;Eger et al., 2019;Zhu et al., 2021). For example, Rodriguez and Galeano (2018) defend Perspective (Google's toxicity classification model) by neutralizing adversarial inputs via a negated predicates list. Again, these defenses are restricted to contexts where specific lists may be identified, this is not so with Sample Shielding.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "Sample Shielding, an intuitively designed defense which is attacker and classifier agnostic, protects effectively; reducing ASR from 90 -100% down to 14 -34% with minimal accuracy loss (3%) in original texts. The randomness (through sampling) provides unreliable feedback for attackers, thus it even thwarts attackers who have query access to classifiers protected with Sample Shielding. Attack strategies will need to increase the amount of perturbation to make sure a majority of samples fail at classification. However, this will risk semantic integrity. Thus, we expect Sample Shielding to cause ripples in future adversarial attack strategies while providing text classifiers with a definite advantage.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Text processing like humans do: Visually attacking and shielding NLP systems", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Steffen Eger; G\u00f6zde G\u00fcl\u015fahin; Andreas R\u00fcckl\u00e9; Ji-Ung Lee; Claudia Schulz; Mohsen Mesgar; Krishnkant Swarnkar; Edwin Simpson; Iryna Gurevych"}, {"title": "Bae: Bert-based adversarial examples for text classification", "journal": "", "year": "2020", "authors": "Siddhant Garg; Goutham Ramakrishnan"}, {"title": "White-to-black: Efficient distillation of black-box adversarial attacks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Yotam Gil; Yoav Chai; Or Gorodissky; Jonathan Berant"}, {"title": "Dndnet: Reconfiguring cnn for adversarial robustness", "journal": "", "year": "2020", "authors": "Akhil Goel; Akshay Agarwal; Mayank Vatsa; Richa Singh; Nalini K Ratha"}, {"title": "Certified robustness to adversarial word substitutions", "journal": "", "year": "2019", "authors": "Robin Jia; Aditi Raghunathan; Kerem G\u00f6ksel; Percy Liang"}, {"title": "Is bert really robust? a strong baseline for natural language attack on text classification and entailment", "journal": "", "year": "2020", "authors": "Di Jin; Zhijing Jin; Joey Tianyi Zhou; Peter Szolovits"}, {"title": "BERT-defense: A probabilistic model based on BERT to combat cognitively inspired orthographic adversarial attacks", "journal": "", "year": "2021", "authors": "Yannik Keller; Jan Mackensen; Steffen Eger"}, {"title": "Convolutional neural networks for sentence classification", "journal": "", "year": "2014", "authors": "Yoon Kim"}, {"title": "Contextualized perturbation for textual adversarial attack", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Dianqi Li; Yizhe Zhang; Hao Peng; Liqun Chen; Chris Brockett; Ming-Ting Sun; Bill Dolan"}, {"title": "Textbugger: Generating adversarial text against real-world applications", "journal": "", "year": "2019", "authors": "Jinfeng Li; Shouling Ji; Tianyu Du; Bo Li; Ting Wang"}, {"title": "BERT-ATTACK: Adversarial attack against BERT using BERT", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Linyang Li; Ruotian Ma; Qipeng Guo; Xiangyang Xue; Xipeng Qiu"}, {"title": "Searching for an effiective defender: Benchmarking defense against adversarial word substitution", "journal": "", "year": "2021", "authors": "Zongyi Li; Jianhan Xu; Jiehang Zeng; Linyang Li; Xiaoqing Zheng; Qi Zhang; Kai-Wei Chang; Cho-Jui Hsieh"}, {"title": "TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP", "journal": "", "year": "2020", "authors": "John Morris; Eli Lifland; Jin Yong Yoo; Jake Grigsby; Di Jin; Yanjun Qi"}, {"title": "Generating natural language adversarial examples through probability weighted word saliency", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Yihe Shuhuai Ren; Kun Deng; Wanxiang He;  Che"}, {"title": "Shielding google's language toxicity model against adversarial attacks", "journal": "", "year": "2018", "authors": "Nestor Rodriguez; Sergio Rojas Galeano"}, {"title": "Randomized substitution and vote for textual adversarial example detection", "journal": "ArXiv", "year": "2021", "authors": "Xiaosen Wang; Yifeng Xiong; Kun He"}, {"title": "Adversarial training with fast gradient projection method against synonym substitution based text attacks", "journal": "", "year": "2021", "authors": "Xiaosen Wang; Yichen Yang; Yihe Deng; Kun He"}, {"title": "Defense of word-level adversarial attacks via random substitution encoding", "journal": "", "year": "2020", "authors": "Zhaoyang Wang; Hongtao Wang"}, {"title": "Towards improving adversarial training of nlp models", "journal": "", "year": "2021", "authors": "Jin ; Yong Yoo; Yanjun Qi"}, {"title": "", "journal": "", "year": "2021", "authors": "Jiehang Zeng; Xiaoqing Zheng; Jianhan Xu; Linyang Li; Liping Yuan; Xuanjing Huang"}, {"title": "", "journal": "", "year": "", "authors": " Arxiv"}, {"title": "Treated: Towards universal defense against textual adversarial attacks", "journal": "ArXiv", "year": "2021", "authors": "Bin Zhu; Zhaoquan Gu; Le Wang; Zhihong Tian"}, {"title": "Freelb: Enhanced adversarial training for natural language understanding", "journal": "", "year": "2020", "authors": "Chen Zhu; Yu Cheng; Zhe Gan; Siqi Sun; Tom Goldstein; Jingjing Liu"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: Proposed shielding method. Sentences or words are sampled k times at a rate of p percent (of the input text), the k samples are classified. The probabilities are used in a majority vote for the final prediction (solid box), or are sorted and given to a Neural Net Summarizer (NN or NN-BB) to made the final prediction (dotted box).", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Average % of perturbed words for each attack. Percentages estimated by comparing words in original and perturbed texts. Since TextBugger adds whitespace in words skewing its percentage it is excluded.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Accuracy with various p values for LSTM on IMDB. Note that k is fixed to 100.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Accuracy with various p values for LSTM on AG News. Note that k is fixed to 100.", "figure_data": ""}, {"figure_label": "67", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 6 :Figure 7 :67Figure 6: Accuracy with various k values for LSTM on IMDB. Note that p is fixed to 0.3.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Input Text\u2026 I enjoyed this movie more than I thought I would. The twist was what really had me hooked. ... \u2026 \u2026 From multiple viewings it becomes especially clear how much time and energy the director put into this film. The choice for lead actor had me worried but it worked well. . ... \u2026 \u2026 enjoyed movie more than I thought I would. From multiple viewings it clear how much time and the director into this film. The choice for had me it worked well. twist was what really me hooked. ... \u2026 I enjoyed this movie more I would. From viewings clear how much time and energy into film. choice for lead had me worried but it worked well. twist really. ...", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "). In general, TextBugger, the character-based attacker, is the least effective attacker. With AG news, we see a large drop in effectiveness of sentence sampling. The average length of AG News is 43 words compared to 157 and 215 words of Yelp and IMDB respectively(Li et al., 2020). This shorter length makes it more difficult to sample enough sentences.", "figure_data": "texts. For Textfooler -CNN,Sample Shielding greatly reduces effectiveness of attacks while maintaining accuracy on orig-inal texts. The shielded classifier W maintains accuracy on original texts to within 7% of the orig-inal accuracy. Crucially, for attacked texts we see accuracy improve to between 60 and 80% (from post attack range of 0-5% generally). For example, TextFooler causes BERT's accuracy to drop from 91% to 1% for IMDB, however, Sample Shield-ing returns accuracy to 78%. In other words, thesentence sampling is only able to increase accuracy from the attacked value of 0.4 to 13.2. However, word sampling is much more effective, increasing accuracy to 77.3. Text length may be crucial when choosing between the two strategies for a dataset. Neural Network summarizer shows some im-provements over majority voting. Comparisons of majority voting and the two neural net-based de-cision strategies are in Table 2. We experimented on the two binary datasets 8 . Replacing majority voting with a simple neural net (NN) gave some-what disappointing results -accuracies stay thesame or decrease slightly in all cases except forLSTM on the Yelp dataset (increases). However,when the neural nets are trained on perturbed texts(NN-BB), we see increases. For example, CNNvs TextFooler on Yelp, the neural net increases ac-curacy from 64.9 to 72.2, reducing attack successrate from 31 to 23. Possibly a more sophisticatedneural net, such as a sequence aware LSTM, mightbetter exploit patterns in the sorted probabilities.Sample Shielding effective against both word based and character based attacks. The results show effectiveness regardless of type of attack(word or character based). For example, all 4 at-tacks bring the original accuracy of LSTM from88.3 down to \u223c0 for IMDB. However, word sam-pling brings the accuracy back up to \u223c66. This is a great reduction in attack effectiveness. Again, sim-ilar trends are seen for the other classifiers, CNNis reduced from 94.1 to \u22645.5 for Yelp, but word sampling brings it back up to 60 -70%.Word sampling outperforms sentence sampling for LSTM, CNN, sentence sampling better for BERT. For example, for CNN on IMDB, word sampling increases accuracy more than 15 pointsover sentence sampling (69.8 vs 53.3). Similartrends hold for LSTM. However, the opposite isseen for BERT classifiers. For BERT on IMDB,we see an average of 6.5 higher points for sentencesampling over word sampling. These results are notsurprising as LSTM and CNN leverage word em-beddings for classification, while BERT leveragesthe context of the entire sentence.Word sampling is more appropriate for short"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Results where attacker does not know about Sample Shielding. Shielding settings: k = 100, p = 0.3, majority voting. Acc: accuracy, ASR: success rate of attack (%), Orig. Acc.: accuracy on original texts.", "figure_data": "IMDB YelpClassifier LSTM CNN LSTM CNNSampling Strategy No Shielding 88.3 Orig. Acc. Maj. Vot. 85.1 NN 85.3 NN-BB 85.3 No Shielding 86.2 Maj. Vot. 84.7 NN 84.8 NN-BB 84.8 No Shielding 92.5 Maj. Vot. 87.8 NN 89.0 NN-BB 89 No Shielding 94.1 Maj. Vot. 88.1 NN 89.9 NN-BB 89.9TextFooler Acc. SR 0 100 66.0 25 62.5 29 65.2 26 0.1 100 69.8 19 61.7 28 69.3 20 0.3 100 65.5 29 68.7 26 69.7 25 0.8 99 64.9 31 63.2 33 72.2 23Bert-Attack Acc. SR 0 100 67.0 24 62.1 30 68.2 23 0 100 66.7 23 59.6 31 67.9 21 0.7 99 66.7 28 68.1 26 70.0 24 0.4 100 62.2 34 57.6 39 69.7 26TextBugger Acc. SR 0.3 100 66.0 25 65.4 26 66.5 25 0.3 100 71.6 17 66.7 23 72.3 16 5 95 68.5 26 73.5 21 73.5 21 5.5 94 70.4 25 69.9 26 72.9 23PWWS Acc. SR 0.1 100 65.7 26 62.4 29 67.3 24 0 100 67.8 21 60.0 30 69.6 19 1.5 98 61.9 33 63.6 31 64.9 30 2.4 97 60.2 36 57.4 39 67.6 28"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": "IMDB Yelp AG NewsClassifier LSTM CNN BERT LSTM CNN BERT LSTM CNN BERTSample Strategy No Shielding Shielding-Word No Shielding Shielding-Word No Shielding Shielding-Word No Shielding Shielding-Word No Shielding Shielding-Word No Shielding Shielding-Word No Shielding Shielding-Word No Shielding Shielding-Word No Shielding Shielding-WordOrig. Acc. 91 94 86 89 90 85 95 87 96 88 100 92 93 87 92 87 99 88TextFooler Acc. ASR 0 100 89 5 0 100 88 1 1 99 80 6 0 100 81 7 0 100 85 3 3 97 90 2 1 99 78 10 1 99 81 7 20 78 81 8Bert-Attack Acc. ASR 0 100 87 7 0 100 88 1 4 96 80 6 0 100 79 9 0 100 81 8 10 90 88 4 0 100 84 3 0 100 87 0 11 89 82 7TextBugger Acc. ASR 0 100 89 5 0 100 89 0 6 93 84 1 6 94 78 10 5 95 81 8 13 87 91 1 16 83 78 10 7 92 84 3 60 39 83 6PWWS Acc. ASR 0 100 89 5 0 100 86 3 2 98 82 4 0 100 74 15 3 97 83 6 7 93 85 8 13 86 84 3 3 97 83 5 15 85 85 3"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Sample Shielding is in the mix with current SOTA defenses in this weaker threat model. However, when deployed as designed for the realistic threat model, it wins over these other defenses by large margins (see Table", "figure_data": "provides our full results against this weakerthreat model.With BERT as base classifier for AG News,FreeLB++, an adversarial training technique (Liet al., 2021b) report accuracies of 51, 56, and 42against TextFooler, TextBugger, and Bert-Attackrespectively. RanMask (Zeng et al., 2021), whichuses random masking of words report accuracies of38, 45, and 49. In comparison, Sample Shieldingachieves 48, 55, and 38 respectively outperformingRanMask in 2 out of 3, while only a fews pointbehind FreeLB++. For IMDB, FreeLB++ reports45, 43, and 40 and RanMask reports 22, 18, and36 respectively. Equivalently, Sample Shieldingachieves 18, 34, and 31. With some wins and somelosses,"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Results of attack against local model with knowledge of Sample Shielding. For all shielding cases, k = 30, p = 0.3, and majority voting is used. Acc. is accuracy, and ASR is success rate of attack (%) and Orig. Acc. is accuracy on the original text. Note that the examples used by RanMask and FreeLB++ is not the set of dataset samples as our paper.", "figure_data": ""}], "doi": "10.18653/v1/N19-1165"}