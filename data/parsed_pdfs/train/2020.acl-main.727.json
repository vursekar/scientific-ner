{"authors": "Robik Shrestha; Kushal Kafle; Christopher Kanan; Adobe Research; Cornell Tech; Baseline Methods", "pub_date": "", "title": "A Negative Case Analysis of Visual Grounding Methods for VQA", "abstract": "Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. To address this issue, recent bias mitigation methods for VQA propose to incorporate visual cues (e.g., human attention maps) to better ground the VQA models, showcasing impressive gains. However, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to linguistic priors. For instance, we find that it is not actually necessary to provide proper, humanbased cues; random, insensible cues also result in similar improvements. Based on this observation, we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-theart performance on VQA-CPv2 1 .", "sections": [{"heading": "Introduction", "text": "Visual Question Answering (VQA) (Antol et al., 2015), the task of answering questions about visual content, was proposed to facilitate the development of models with human-like visual and linguistic understanding. However, existing VQA models often exploit superficial statistical biases to produce responses, instead of producing the right answers for the right reasons (Kafle et al., 2019).\nThe VQA-CP dataset  showcases this phenomenon by incorporating different question type/answer distributions in the train and test sets. Since the linguistic priors in the train and test sets differ, models that exploit these priors fail on the test set. To tackle this issue, recent works have endeavored to enforce proper visual grounding, where the goal is to make models produce answers by looking at relevant visual regions (Gan et al., 2017;Selvaraju et al., Figure 1: We find that existing visual sensitivity enhancement methods improve performance on VQA-CPv2 through regularization as opposed to proper visual grounding.\n2019; Wu and Mooney, 2019), instead of exploiting linguistic priors. These approaches rely on additional annotations/cues such as human-based attention maps (Das et al., 2017), textual explanations (Huk Park et al., 2018) and object label predictions (Ren et al., 2015) to identify relevant regions, and train the model to base its predictions on those regions, showing large improvements (8-10% accuracy) on the VQA-CPv2 dataset.\nHere, we study these methods. We find that their improved accuracy does not actually emerge from proper visual grounding, but from regularization effects, where the model forgets the linguistic priors in the train set, thereby performing better on the test set. To support these claims, we first show that it is possible to achieve such gains even when the model is trained to look at: a) irrelevant visual regions, and b) random visual regions. Second, we show that differences in the predictions from the variants trained with relevant, irrelevant and random visual regions are not statistically significant. Third, we show that these methods degrade performance when the priors remain intact and instead work on VQA-CPv2 by hurting its train accuracy.\nBased on these observations, we hypothesize that controlled degradation on the train set allows models to forget the training priors to improve test accuracy. To test this hypothesis, we introduce a simple regularization scheme that zeros out the ground truth answers, thereby always penalizing the model, whether the predictions are correct or incorrect. We find that this approach also achieves near state-of-the-art performance (48.9% on VQA-CPv2), providing further support for our claims.\nWhile we agree that visual grounding is a useful direction to pursue, our experiments show that the community requires better ways to test if systems are actually visually grounded. We make some recommendations in the discussion section.", "n_publication_ref": 7, "n_figure_ref": 1}, {"heading": "Related Work", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Biases in VQA", "text": "As expected of any real world dataset, VQA datasets also contain dataset biases (Goyal et al., 2017). The VQA-CP dataset  was introduced to study the robustness of VQA methods against linguistic biases. Since it contains different answer distributions in the train and test sets, VQA-CP makes it nearly impossible for the models that rely upon linguistic correlations to perform well on the test set Shrestha et al., 2019).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Bias Mitigation for VQA", "text": "VQA algorithms without explicit bias mitigation mechanisms fail on VQA-CP, so recent works have focused on the following solutions:", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Reducing Reliance on Questions", "text": "Some recent approaches employ a question-only branch as a control model to discover the questions most affected by linguistic correlations. The question-only model is either used to perform adversarial regularization (Grand and Belinkov, 2019;Ramakrishnan et al., 2018) or to re-scale the loss based on the difficulty of the question (Cadene et al., 2019). However, when these ideas are applied to the UpDn model (Anderson et al., 2018), which attempts to learn correct visual grounding, these approaches achieve 4-7% lower accuracy compared to the state-of-the-art methods.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Enhancing Visual Sensitivities", "text": "Both Human Importance Aware Network Tuning (HINT) (Selvaraju et al., 2019) and Self Critical Reasoning (SCR) (Wu and Mooney, 2019), train the network to be more sensitive towards salient image regions by improving the alignment between visual cues and gradient-based sensitivity scores. HINT proposes a ranking loss between humanbased importance scores (Das et al., 2016) and the gradient-based sensitivities. In contrast, SCR does not require exact saliency ranks. Instead, it penalizes the model if correct answers are more sensitive towards non-important regions as compared to important regions, and if incorrect answers are more sensitive to important regions than correct answers.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Existing VQA Methods", "text": "Given a question Q and an image I, e.g., represented by bottom-up region proposals: v (Anderson et al., 2018), a VQA model is tasked with predicting the answer a:\nP (a|Q, I) = f V QA (v, Q).\n(1)", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Baseline VQA Methods", "text": "Without additional regularization, existing VQA models such as the baseline model used in this work: UpDn (Anderson et al., 2018), tend to rely on the linguistic priors: P (a|Q) to answer questions. Such models fail on VQA-CP, because the priors in the test set differ from the train set.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Visual Sensitivity Enhancement Methods", "text": "To reduce the reliance on linguistic priors, visual sensitivity enhancement methods attempt to train the model to be more sensitive to relevant visual regions when answering questions. Following (Wu and Mooney, 2019), we define the sensitivity of an answer a with respect to a visual region v i as:\nS(a, v i ) := (\u2207 v i P (a|I, Q)) T 1.(2)\nExisting methods propose the following training objectives to improve grounding using S:\n\u2022 HINT uses a ranking loss, which penalizes the model if the pair-wise rankings of the sensitivities of visual regions towards ground truth answers a gt are different from the ranks computed from the human-based attention maps.\n\u2022 SCR divides the region proposals into influential and non-influential regions and penalizes the model if: 1) S(a gt ) of a non-influential region is higher than an influential region, and 2) the region most influential for the correct answer has even higher sensitivity for incorrect answers.\nBoth methods improve baseline accuracy by 8-10%. Is this actually due to better visual grounding?\n4 Why Did the Performance Improve?\nWe probe the reasons behind the performance improvements of HINT and SCR. We first analyze if the results improve even when the visual cues are irrelevant (Sec. 4.2) or random (Sec. 4.3) and examine if their differences are statistically significant (Sec. 4.4). Then, we analyze the regularization effects by evaluating the performance on VQA-CPv2's train split (Sec. 4.5) and the behavior on a dataset without changing priors (Sec. 4.6). We present a new metric to assess visual grounding in Sec. 4.7 and describe our regularization method in Sec. 5.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "We compare the baseline UpDn model with HINT and SCR-variants trained on VQAv2 or VQA-CPv2 to study the causes behind the improvements. We report mean accuracies across 5 runs, where a pretrained UpDn model is fine-tuned on subsets with human attention maps and textual explanations for HINT and SCR respectively. Further training details are provided in the Appendix.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Training on Irrelevant Visual Cues", "text": "In our first experiment we studied how irrelevant visual cues performed compared to relevant ones. We fine-tune the model with irrelevant cues defined as:\nS irrelevant := (1 \u2212 S h ),\nwhere, S h represents the human-based importance scores. As shown in the 'Grounding using irrelevant cues' section of Table 1, both HINT and SCR are within 0.3% of the results obtained from looking at relevant regions, which indicates the gains for HINT and SCR are not necessarily from looking at relevant regions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Training on Random Visual Cues", "text": "In our next experiment we studied how random visual cues performed with HINT and SCR. We assign random importance scores to the visual regions: S rand \u223c uniform(0, 1). We test two variants of randomness: Fixed random regions, where  1, both of these variants obtain similar results as the model trained with human-based importance scores. The performance improves even when the importance scores are changed every epoch, indicating that it is not even necessary to look at the same visual regions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Significance of Statistical Differences", "text": "To test if the changes in results were statistically significant, we performed Welch's t-tests (Welch, 1938) on the predictions of the variants trained on relevant, irrelevant and random cues. We pick Welch's t-test over the Student's t-test, because the latter assumes equal variances for predictions from different variants. To perform the tests, we first randomly sample 5000 subsets of non-overlapping test instances. We then average the accuracy of each subset across 5 runs, obtaining 5000 values. Next, we run the t-tests for HINT and SCR separately on the subset accuracies. As shown in Table 2, the p-values across the variants of HINT and SCR are greater than or equal to 0.3. Using a confidence level of 95% (\u03b1 = 0.05), we fail to reject the null hypothesis that the mean difference between the paired values is 0, showing that the variants are not statistically significantly different from each other. We also compare the predictions of HINT/SCR against baseline, and find that p-values are all zeros, showing that the differences have statistical significance.\nPercentage of Overlaps: To further check if the variants trained on irrelevant or random regions gain performance in a manner similar to the models trained on relevant regions, we compute the overlap between their predictions on VQA-CPv2's test set. The percentage of overlap is defined as:\n% Overlap = n same n total \u00d7 100%,\nwhere, n same denotes the number of instances where either both variants were correct or both were incorrect and n total denotes the total number of test instances. As shown in Table 2, we compare %Overlap between different variants of HINT/SCR with baseline and against each other. We find 89.7 \u2212 91.9% and 89.5 \u2212 92.0% overlaps for different variants of HINT and SCR respectively. These high overlaps suggest that the variants are not working in fundamentally different manners.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Drops in Training Accuracy", "text": "We compare the training accuracies to analyze the regularization effects. As shown in Table 1, the baseline method has the highest training results, while the other methods cause 6.0 \u2212 14.0% and 3.3\u221210.5% drops in the training accuracy on VQA-CPv2 and VQAv2, respectively. We hypothesize that degrading performance on the train set helps forget linguistic biases, which in turn helps accuracy on VQA-CPv2's test set but hurts accuracy on VQAv2's val set.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Drops in VQAv2 Accuracy", "text": "As observed by Selvaraju et al. (2019) and as shown in Fig. 2, we observe small improvements on VQAv2 when the models are fine-tuned on the entire train set. However, if we were to compare against the improvements in VQA-CPv2 in a fair manner, i.e., only use the instances with visual cues while fine-tuning, then, the performance on VQAv2 drops continuously during the course of the training. This indicates that HINT and SCR help forget linguistic priors, which is beneficial for VQA-CPv2 but not for VQAv2.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Assessment of Proper Grounding", "text": "In order to quantitatively assess visual grounding, we propose a new metric called: Correctly Predicted but Improperly Grounded (CPIG):\n%CP IG = N correct ans, improper grounding N correct ans \u00d7 100%, which is the number instances for which the most sensitive visual region used to correctly predict the answer is not within top-3 most relevant ground truth regions, normalized by the total number of correct predictions. HINT and SCR trained on relevant regions obtained lower CPIG values that other variants (70.24% and 80.22% respectively), indicating they are better than other variants at finding relevant regions. However, these numbers are still high, and show that only 29.76% and 19.78% of the correct predictions for HINT and SCR were properly grounded. Further analysis is presented in the Appendix.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Embarrassingly Simple Regularizer", "text": "The usage of visual cues and sensitivities in existing methods is superfluous because the results indicate that performance improves through degradation of training accuracy. We hypothesize that simple regularization that does not rely on cues or sensitivities can also achieve large performance gains for VQA-CP. To test this hypothesis, we devise a simple loss function which continuously degrades the training accuracy by training the network to always predict a score of zero for all possible answers i.e. produce a zero vector (0). The overall loss function can be written as:\nL := BCE(P (A), A gt ) + \u03bbBCE(P (A), 0),\nwhere, BCE refers to the binary cross entropy loss and P (A) is a vector consisting of predicted scores for all possible answers. The first term is the binary cross entropy loss between model predictions and ground truth answer vector (A gt ), and the second term is our regularizer with a coefficient of \u03bb = 1.\nNote that this regularizer continually penalizes the model during the course of the training, whether its predictions are correct or incorrect.\nAs shown in Table 1, we present results when this loss is used on: a) Fixed subset covering 1% of the dataset, b) Varying subset covering 1% of the dataset, where a new random subset is sampled every epoch and c) 100% of the dataset. Confirming our hypothesis, all variants of our model achieve near state-of-the-art results, solidifying our claim that the performance gains for recent methods come from regularization effects.\nIt is also interesting to note that the drop in training accuracy is lower with this regularization scheme as compared to the state-of-the-art methods. Of course, if any model was actually visually grounded, then we would expect it to improve performances on both train and test sets. We do not observe such behavior in any of the methods, indicating that they are not producing right answers for the right reasons.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Discussion on Proper Grounding", "text": "While our results indicate that current visual grounding based bias mitigation approaches do not suffice, we believe this is still a good research direction. However, future methods must seek to verify that performance gains are not stemming from spurious sources by using an experimental setup similar to that presented in this paper. We recommend that both train and test accuracy be reported, because a model truly capable of visual grounding would not cause drastic drops in training accuracy to do well on the test sets. Finally, we advocate for creating a dataset with ground truth grounding available for 100% of the instances using synthetically generated datasets Kafle et al., 2018;Acharya et al., 2019b;Hudson and Manning, 2019;Johnson et al., 2017), enabling the community to evaluate if their methods are able to focus on relevant information. Another alternative is to use tasks that explicitly test grounding, e.g., in visual query detection an agent must output boxes around any regions of a scene that match the natural language query (Acharya et al., 2019a).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "Here, we showed that existing visual grounding based bias mitigation methods for VQA are not working as intended. We found that the accuracy improvements stem from a regularization effect rather than proper visual grounding. We proposed a simple regularization scheme which, despite not requiring additional annotations, rivals state-of-theart accuracy. Future visual grounding methods should be tested with a more comprehensive experimental setup and datasets for proper evaluation. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Appendix", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 Training Details", "text": "We compare four different variants of HINT and SCR to study the causes behind the improvements including the models that are fine-tuned on: 1) relevant regions (state-of-the-art methods) 2) irrelevant regions 3) fixed random regions and 4) variable random regions. For all variants, we fine-tune a pretrained UpDn, which was trained on either VQA-CPv2 or VQAv2 for 40 epochs with a learning rate of 10 \u22123 . When fine-tuning with HINT, SCR or our method, we also use the main binary cross entropy VQA loss, whose weight is set to 1. The batch size is set to 384 for all of the experiments.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "HINT", "text": "Following (Selvaraju et al., 2019), we train HINT on the subset with human-based attention maps (Das et al., 2017), which are available for 9% of the VQA-CPv2 train and test sets. The same subset is used for VQAv2 too. The learning rate is set to 2 \u00d7 10 \u22125 and the weight for the HINT loss is set to 2.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "SCR", "text": "Since (Wu and Mooney, 2019) reported that humanbased textual explanations (Huk Park et al., 2018) gave better results than human-based attention maps for SCR, we train all of the SCR variants on the subset containing textual explanation-based cues. SCR is trained in two phases. For the first phase, which strengthens the influential objects, we use a learning rate of 5 \u00d7 10 \u22125 , loss weight of 3 and train the model to a maximum of 12 epochs. Then, following (Wu and Mooney, 2019), for the second phase, we use the best performing model from the first phase to train the second phase, which criticizes incorrect dominant answers. For the second phase, we use a learning rate of 10 \u22124 and weight of 1000, which is applied alongside the loss term used in the first phase. The specified hyperparameters worked better for us than the values provided in the original paper.\nOur Zero-Out Regularizer Our regularization method, which is a binary cross entropy loss between the model predictions and a zero vector, does not use additional cues or sensitivities and yet achieves near state-of-the-art performance on VQA-CPv2. We set the learning rate to:\n2\u00d710 \u22126 r\n, where r is the ratio of the training instances used for fine-tuning. The weight for the loss is set to 2. We report the performance obtained at the 8 th epoch.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A.2 Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Correlation with Ground Truth Visual Cues", "text": "Following (Selvaraju et al., 2019), we report Spearman's rank correlation between network's sensitivity scores and human-based scores in Table A3. For HINT and our zero-out regularizer, we use human-based attention maps. For SCR, we use textual explanation-based scores. We find that HINT trained on human attention maps has the highest correlation coefficients for both datasets. However, compared to baseline, HINT variants trained on random visual cues also show improved correlations. For SCR, we obtain surprising results, with the model trained on irrelevant cues obtaining higher correlation than that trained on relevant visual cues. As expected, applying our regularizer does not improve rank correlation. Since HINT trained on relevant cues obtains the highest correlation values, it does indicate improvement in visual grounding. However, as we have seen, the improvements in performance cannot necessarily be attributed to better overlap with ground truth localizations.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A Note on Qualitative Examples", "text": "Presentation of qualitative examples in visual grounding models for VQA suffers from confirmation bias i.e., while it is possible to find qualitative samples that look at relevant regions to answer questions properly, it is also possible to find samples that produce correct answers without looking at relevant regions. We present examples for such cases in Fig. A3. We next present a quantitative assessment of visual grounding, which does not suffer from the confirmation bias.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Quantitative Assessment of Grounding", "text": "In order to truly assess if existing methods are using relevant regions to produce correct answers, we use our proposed metric: Correctly Predicted but Improperly Grounded (CPIG). If the CPIG values are large, then it implies that large portion of correctly predicted samples were not properly grounded. Fig. A4 shows % CPIG for different variants of HINT trained on human attention-based cues, whereas Fig. A5 shows the metric for different variants of SCR trained on textual explanationbased cues. We observe that HINT and SCR trained on relevant regions have the lowest % CPIG values (70.24% and 80.22% respectively), indicating that they are better than other variants in finding relevant regions. However, only a small percentage of correctly predicted samples were properly grounded (29.76% and 19.78% for HINT and SCR respectively), even when trained on relevant cues.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Breakdown by Answer Types", "text": "Table A4 shows VQA accuracy for each answer type on VQACPv2's test set. HINT/SCR and our regularizer show large gains in 'Yes/No' questions. We hypothesize that the methods help forget linguistic priors, which improves test accuracy of such questions. In the train set of VQACPv2, the answer 'no' is more frequent than the answer 'yes', tempting the baseline model to answer 'yes/no' questions with 'no'. However, in the test set, answer 'yes' is more frequent. Regularization effects caused by HINT/SCR and our method cause the models to weaken this prior i.e., reduce the tendency to just predict 'no', which would increase accuracy at test because 'yes' is more frequent in the test set.\nNext, all of the methods perform poorly on 'Number (Num)' answer type, showing that methods find it difficult to answer questions that are most reliant on correct visual grounding such as: localizing and counting objects. Finally, we do not observe large improvements in 'Other' question type, most likely due to the large number of answers present under this answer type.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Accuracy versus Size of Train Set", "text": "We test our regularization method on random subsets of varying sizes. Fig. A6 shows the results when we apply our loss to 1 \u2212 100% of the training instances. Clearly, the ability to regularize the model does not vary much with respect to the size of the train subset, with the best performance occurring when our loss is applied to 1% of the training instances. These results support our claims that it is possible to improve performance without actually performing visual grounding.\nQ: Is this food sweet? A: yes Remarks: The most sensitive regions for irrelevant/random variants do not contain food, yet their answers are correct.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Ground Truth Localization", "text": "HINT trained on relevant cues HINT trained on irrelevant cues HINT trained on random cues Q: Has the boy worn out his jeans? A: yes Remarks: All of the variants look at both relevant and irrelevant regions to produce correct answer.\nQ: Is the sport being played tennis or volleyball? A: tennis Remarks: None of the variants look at relevant regions, and yet produce correct answer.\nQ: What is the swimmer doing? A: surfing Remarks: Models trained on irrelevant/random cues do not look at the swimmer at all, yet produce correct answer. We pick samples where all variants produce correct response to the question. The first column shows ground truth regions and columns 2-4 show visualizations from HINT trained on relevant, irrelevant and fixed random regions respectively.\nFigure A5: % CPIG for baseline and different variants of SCR and our method, computed using ground truth relevant regions taken from textual explanations (txt). Train Test\nFigure A6: The regularization effect of our loss is invariant with respect to the dataset size.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "", "text": "Acknowledgement. This work was supported in part by AFOSR grant [FA9550-18-1-0121], NSF award #1909696, and a gift from Adobe Research. We thank NVIDIA for the GPU donation. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies or endorsements of any sponsor. We are grateful to Tyler Hayes for agreeing to review the paper at short notice and suggesting valuable edits and corrections for the paper.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "VQD: Visual query detection in natural scenes", "journal": "Long and Short Papers", "year": "2019", "authors": "Manoj Acharya; Karan Jariwala; Christopher Kanan"}, {"title": "Tallyqa: Answering complex counting questions", "journal": "", "year": "2019", "authors": "Manoj Acharya; Kushal Kafle; Christopher Kanan"}, {"title": "Dont just assume; look and answer: Overcoming priors for visual question answering", "journal": "", "year": "2018", "authors": "Aishwarya Agrawal; Dhruv Batra; Devi Parikh; Aniruddha Kembhavi"}, {"title": "Bottom-up and top-down attention for image captioning and visual question answering", "journal": "", "year": "2018", "authors": "Peter Anderson; Xiaodong He; Chris Buehler; Damien Teney; Mark Johnson; Stephen Gould; Lei Zhang"}, {"title": "VQA: Visual question answering", "journal": "", "year": "2015", "authors": "Stanislaw Antol; Aishwarya Agrawal; Jiasen Lu; Margaret Mitchell; Dhruv Batra; C Lawrence Zitnick; Devi Parikh"}, {"title": "Rubi: Reducing unimodal biases for visual question answering", "journal": "", "year": "2019", "authors": "Remi Cadene; Corentin Dancette; Matthieu Cord; Devi Parikh"}, {"title": "Human attention in visual question answering: Do humans and deep networks look at the same regions?", "journal": "EMNLP", "year": "2016", "authors": "Abhishek Das; Harsh Agrawal; Lawrence Zitnick; Devi Parikh; Dhruv Batra"}, {"title": "Human attention in visual question answering: Do humans and deep networks look at the same regions?", "journal": "", "year": "2017", "authors": "Abhishek Das; Harsh Agrawal; Larry Zitnick; Devi Parikh; Dhruv Batra"}, {"title": "Vqs: Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation", "journal": "", "year": "2017", "authors": "Chuang Gan; Yandong Li; Haoxiang Li; Chen Sun; Boqing Gong"}, {"title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering", "journal": "", "year": "2017", "authors": "Yash Goyal; Tejas Khot; Douglas Summers-Stay; Dhruv Batra; Devi Parikh"}, {"title": "Adversarial regularization for visual question answering: Strengths, shortcomings, and side effects", "journal": "", "year": "2019", "authors": "Gabriel Grand; Yonatan Belinkov"}, {"title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "journal": "", "year": "2019", "authors": "A Drew; Christopher D Hudson;  Manning"}, {"title": "Multimodal explanations: Justifying decisions and pointing to the evidence", "journal": "", "year": "2018", "authors": "Lisa Anne Dong Huk Park; Zeynep Hendricks; Anna Akata; Bernt Rohrbach; Trevor Schiele; Marcus Darrell;  Rohrbach"}, {"title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "journal": "IEEE", "year": "2017", "authors": "Justin Johnson; Bharath Hariharan; Laurens Van Der Maaten; Li Fei-Fei; Lawrence Zitnick; Ross Girshick"}, {"title": "An analysis of visual question answering algorithms", "journal": "IEEE", "year": "2017", "authors": "Kushal Kafle; Christopher Kanan"}, {"title": "DVQA: Understanding data visualizations via question answering", "journal": "", "year": "2018", "authors": "Kushal Kafle; Brian Price; Scott Cohen; Christopher Kanan"}, {"title": "Challenges and prospects in vision and language research", "journal": "Frontiers in Artificial Intelligence", "year": "2019", "authors": "Kushal Kafle; Robik Shrestha; Christopher Kanan"}, {"title": "Data augmentation for visual question answering", "journal": "", "year": "2017", "authors": "Kushal Kafle; Mohammed Yousefhussien; Christopher Kanan"}, {"title": "Overcoming language priors in visual question answering with adversarial regularization", "journal": "", "year": "2018", "authors": "Aishwarya Sainandan Ramakrishnan; Stefan Agrawal;  Lee"}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "journal": "", "year": "2015", "authors": "Kaiming Shaoqing Ren; Ross He; Jian Girshick;  Sun"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Accuracies for HINT and SCR on VQAv2's val set, when fine-tuned either on the full train set or on the subset containing visual cues.", "figure_data": ""}, {"figure_label": "A3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure A3 :A3Figure A3: Visualizations of most sensitive visual regions used by different variants of HINT to make predictions.We pick samples where all variants produce correct response to the question. The first column shows ground truth regions and columns 2-4 show visualizations from HINT trained on relevant, irrelevant and fixed random regions respectively.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Results on VQA-CPv2 and VQAv2 datasets for the baseline UpDn, visual sensitivity enhancement methods (HINT and SCR) and our own regularization method, including the published (pub.) numbers.", "figure_data": "VQA-CPv2VQAv2TrainTestTrainValBaseline -Without visual groundingUpDn84.040.183.464.4Grounding using human-based cuesHINT pub.N/A46.7N/A63.4 1SCR pub.N/A49.5N/A62.2HINT73.948.275.761.3SCR75.949.177.961.3Grounding using irrelevant cuesHINT71.248.073.560.3SCR75.749.274.159.1Grounding using fixed random cuesHINT72.048.173.059.5SCR70.049.178.061.4Grounding using variable random cuesHINT71.948.172.959.4SCR69.649.278.161.5Regularization by zeroing out answersOurs 1% f ixed78.048.980.162.6Ours 1% var.77.648.580.062.6Ours 100%75.748.279.962.4"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "p-values from the Welch's t-tests and the percentage of overlap between the predictions (Ovp.) of different variants of HINT and SCR.", "figure_data": "Methodsp Ovp.(%)HINT variants against BaselineDefault vs. Baseline0.083.6Irrelevant vs. Baseline0.082.4Fixed Random vs. Baseline0.082.0Variable Random vs. Baseline0.081.5Among HINT variantsDefault vs Irrelevant0.389.7Default vs Fixed random0.790.9Default vs Variable random0.691.9Irrelevant vs Fixed random0.595.6Irrelevant vs Variable random0.793.9Fixed random vs Variable random 0.996.9SCR variants against BaselineDefault vs. Baseline0.085.6Irrelevant vs. Baseline0.084.2Fixed Random vs. Baseline0.080.7Variable Random vs. Baseline0.080.6Among SCR variantsDefault vs Irrelevant0.692.0Default vs Fixed random0.889.3Default vs Variable random0.689.5Irrelevant vs Fixed random0.491.7Irrelevant vs Variable random1.091.6Fixed random vs Variable random 0.496.7"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. 2019. Taking a hint: Leveraging explanations to make vision and language models more grounded. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2591-2600.", "figure_data": "Robik Shrestha, Kushal Kafle, and Christopher Kanan.2019. Answer them all! toward universal visualquestion answering models. In The IEEE Confer-ence on Computer Vision and Pattern Recognition(CVPR).Bernard L Welch. 1938. The significance of the differ-ence between two means when the population vari-ances are unequal. Biometrika, 29(3/4):350-362.Jialin Wu and Raymond Mooney. 2019. Self-criticalreasoning for robust visual question answering. InAdvances in Neural Information Processing Systems(NeurIPS), pages 8601-8611."}, {"figure_label": "A3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Results on VQA-CPv2 and VQAv2 datasets for the baseline UpDn, visual sensitivity enhancement methods (HINT and SCR) and our own regularization method, including the published (pub.) numbers.", "figure_data": "VQA-CPv2VQAv2Baseline -Without visual groundingUpDn0.01100.0155Grounding using human-based cuesHINT0.10200.1350SCR0.0340-0.0670Grounding using irrelevant cuesHINT-0.0048-0.0200SCR0.0580-0.0100Grounding using fixed random cuesHINT0.05100.0620SCR-0.0250-0.0350Grounding using variable random cuesHINT0.05700.0623SCR-0.03800.0246Regularization by zeroing out answersOurs 1% f ixed-0.1050-0.1200Ours 100%-0.0750-0.0100"}, {"figure_label": "A4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "VQA accuracy per answer-type on VQACPv2 test set.", "figure_data": "Over-NumOtherall Yes/NoBaseline -Without visual groundingUpDn40.141.112.047.2Grounding using human-based cuesHINT48.265.213.847.5SCR49.170.311.548.0Grounding using irrelevant cuesHINT48.067.213.547.1SCR49.273.411.546.4Grounding using fixed random cuesHINT48.166.913.846.9SCR49.174.712.245.1Grounding using variable random cuesHINT48.167.113.946.9SCR49.274.712.245.1Regularization by zeroing out answersOurs 1% f ixed48.969.811.347.8Ours 100%48.266.711.747.9"}], "doi": ""}