{"authors": "Ori Shapira; Ramakanth Pasunuru; Mohit Bansal; Ido Dagan; Yael Amsterdamer", "pub_date": "", "title": "Interactive Query-Assisted Summarization via Deep Reinforcement Learning", "abstract": "Interactive summarization is a task that facilitates user-guided exploration of information within a document set. While one would like to employ state of the art neural models to improve the quality of interactive summarization, many such technologies cannot ingest the full document set or cannot operate at sufficient speed for interactivity. To that end, we propose two novel deep reinforcement learning models for the task that address, respectively, the subtask of summarizing salient information that adheres to user queries, and the subtask of listing suggested queries to assist users throughout their exploration. 1  In particular, our models allow encoding the interactive session state and history to refrain from redundancy. Together, these models compose a state of the art solution that addresses all of the task requirements. We compare our solution to a recent interactive summarization system, and show through an experimental study involving real users that our models are able to improve informativeness while preserving positive user experience.", "sections": [{"heading": "Introduction", "text": "Integrating human interaction into NLP tasks has been gaining the interest of the NLP community. Human-machine cooperation can improve the general quality of results, as well as provide a higher sense of control for the targeted consumer. We focus on the task of interactive summarization (INTSUMM: Shapira et al., 2021b) which enables information exploration within a document set on a topic, by means of user-guided summarization. As illustrated in Figure 1, a user can incrementally expand on a summary by submitting requests to the system, in order to expose the information of interest within the topic. A proper exploration session demands access to all information within the document set, and fast reaction time for smooth human Figure 1: An INTSUMM system, ingesting a large document set. A user interactively submits queries in order to expand on the information. The system is required to process the full document set for comprehensive exploration, respond quickly, and expose nonredundant salient information that also complies to the input queries. See real example in Figure 5. engagement (Anderson, 2020;Attig et al., 2017). In addition, presented information must consider the session history to refrain from repetitiveness.\nWhile it is worthwhile to apply recent NLP advances that excel at extracting salient and querybiased information, those advances usually come at a cost of rather small input size limits or heavy computation time. Indeed, all previous interactive summarization systems we know of either apply traditional methods or are inadequate for real-time processing due to high latency ( \u00a72). Our goal is to overcome these obstacles, and leverage advanced methods to improve information exposure while keeping latency acceptable for interaction.\nAs depicted in Figure 1, an INTSUMM system provides an initial generic summary as an overview of the topic, after which a user can iteratively issue queries to the system for summary expansions on subtopics of interest. To support querying, the system offers a list of suggested queries, hinting at information concealed within the document set.\nWe address the INTSUMM task components through two subtasks: (1) generating the initial summary and query responses, and (2) generating lists of suggested queries. For each of the subtasks we propose a deep reinforcement learning (RL) algorithm that addresses the respective sub-task requirements. To enable comprehensive topic exploration, our models speedily process the full document set, as inspired by . Additionally, they are able to peek at session history to comply to the current state of the interaction. The model for the query-assisted summarization subtask, M Summ , incorporates the query sequence by (1) encoding a query into the contextual sentence representations, (2) attending the representations using a new query-biased variant of the maximal marginal relevance (MMR: Carbonell and Goldstein, 1998) function, and (3) a dual reward mechanism for policy optimization (Pasunuru and Bansal, 2018) which we adapt to consider both reference summaries and the query ( \u00a73). The model for the suggested queries list generation subtask, M Sugg , works at the phrase level, as opposed to the sentence level, to enable extraction of important phrases that serve as suggested queries. Similarly to M Summ , the model learns importance with consideration to session history, but without an input query -as its role is to suggest such a query ( \u00a74).\nThe models are trained on the DUC 2 2007 multidocument summarization (MDS) news-domain dataset, with adaptions for our task setting. For testing, we follow the INTSUMM evaluation framework of Shapira et al. (2021b) to run simulations, collect real user sessions, and assess the results, using DUC 2006. In principle, summary informativeness, i.e. general salience, could potentially come at the expense of query responsiveness, but importantly, our results show that our RL-based solution is able to significantly improve information exposure over the baseline of Shapira et al. (2021b), without compromising user experience ( \u00a75).", "n_publication_ref": 7, "n_figure_ref": 4}, {"heading": "Background and Related Work", "text": "Interactive summarization facilitates user-guided information navigation within document sets. The task suffered from a lack of a methodological evaluation, until Shapira et al. (2021b) formalized the INTSUMM task with a framework consisting of a benchmark, evaluation metrics, a session collection process and baseline systems. This framework, that we leverage, enables comparison and analysis of systems, allowing principled research on the task and accelerated development of algorithms.\nTo the best of our knowledge, all previous works on INTSUMM have either applied more traditional text-processing methods or require costly prepro-cessing of inputs to facilitate seamless interaction. Leuski et al. (2003) used surface-form features for processing content, and Baumel et al. (2014) adapted classic MDS algorithms like LexRank (Erkan and Radev, 2004) and KLSum (Haghighi and Vanderwende, 2009). Christensen et al. (2014) optimized discourse graphs and Shapira et al. (2017) relied on a knowledge representation, both expensively pre-generating hierarchical summaries that limit expansions to pre-prepared information selections. Hirsch et al. (2021) applied advanced coreference resolution algorithms that take several hours for preprocessing a document set.\nThe two INTSUMM baseline systems of Shapira et al. (2021b) use sentence clustering or TextRank (Mihalcea and Tarau, 2004) for summarization, sentence similarity heuristics for query-responses, and n-gram frequency or TextRank for suggested query extraction. Moreover, their query-response generators strictly consider a given query, ignoring history or global informativeness. Our proposed algorithms significantly improve information exposure over the latter baselines, using advanced deep RL methods, working in real time. We next review some recent techniques in MDS, query-focused summarization and multi-document keyphrase extraction, all of which relate to the INTSUMM task and our choice of algorithms.\nThe subtask of query-assisted summarization. Non-interactive MDS has been researched extensively, with few recent neural-based methods that can handle relatively large inputs. For example, Wang et al. (2020) use graph neural networks to globally score sentence salience, Xiao et al. (2021) summarize using Longformers (Beltagy et al., 2020), and combine a Longformer with BART (Lewis et al., 2020) and incorporate graphical representation of information.  apply deep RL for autoregressive sentence selection, and, in contrast to most other neural methods, can ingest the full document set.\nIn the query-focused summarization (QFS) task summaries are biased on a query. To accommodate a query,  use conditional selfattention to enforce dependency of the query on source words. Pasunuru et al. (2021a) and Kulkarni et al. (2021) hierarchically encode a query with the documents. These and other QFS methods require large training sets, and limit the allowed input size (Baumel et al., 2018;Laskar et al., 2020). Relatedly, incremental update summarization (Mc-Creadie et al., 2014;Lin et al., 2017) marks queryrelevant information as reported texts stream in, avoiding repeating information marked earlier. Interactivity is not a constraining factor here, yielding solutions with relatively high computation time.\nWith respect to the above related work, we develop a model inspired by , which is closest to our requirements. To facilitate an interactive setting, our model (1) enables query+history injection, (2) supports full input processing, necessary for complete information availability during exploration, (3) has low latency at inference time, and (4) requires a relatively small training set.\nThe subtask of suggested-queries list generation. Extracting suggested queries on a document set most resembles the multi-document keyphrase extraction (MDKE) task since it aims to identify salient keyphrases (Shapira et al., 2021a). MDKE was mostly addressed using traditional heuristics or graph-centrality algorithms applied over the documents (e.g. Mihalcea and Tarau, 2004;Florescu and Caragea, 2017). In contrast to MDKE, the suggested queries extraction subtask is a new paradigm that updates \"keyphrases\" with respect to session history. While previous methods for keyphrase extraction could potentially be adapted for our dynamic setting, we choose to focus in this work on a deep RL architecture for suggested queries that resonates our model for query-assisted summarization and allows sharing insights between the models.", "n_publication_ref": 23, "n_figure_ref": 0}, {"heading": "Query-Assisted Summarization Model", "text": "The subtask of query-assisted summarization covers two main components of the INTSUMM task: the generators of an initial summary and of queryresponses. The initial summary concisely specifies some central issues from the input topic (not biased on a query) to initiate the user's understanding of the topic and to motivate further exploration. Then, for each user submitted query, the query-response generator non-redundantly expands on the previously presented information with topically salient responses that are also biased around the query. We next formally define the subtask and then describe our RL model for it.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Subtask Formulation", "text": "The input to the query-assisted summarization subtask is tuple (D, q, E in , m), such that: D is a document set on a topic where the j-th sentence in the concatenation of D's documents is denoted s j ; q is a query, and can be empty (denoted _) for an unbiased generic summary; E in = {e in 1 , ..., e in k } is a sequence of sentences from D termed the history, containing texts previously output in the session; and m is the number of sentences to output. The output is sentence sequence E out = {e out 1 , ..., e out m } from D (extractive summarization). When inputting (D, _, {}, m), the output is a generic summary of m sentences, that can serve as the initial summary; and when q and E in are not empty, the output is an expansion on E in in response to q, containing new salient information biased on q.\nD is paired with a set of generic reference summaries R, which is used for training or as a part of the evaluation effort.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model Architecture", "text": "Our query-assisted summarization model, M Summ , is autoregressive, outputting the requested number of summary sentences one-by-one. At time step t, a sentence e out t is output according to the current query and an encoding of the summary-so-far E t = {e in 1 , ..., e in k , e out 1 , ..., e out t\u22121 } to prevent information repetition. At inference time, M Summ outputs the summary sentences with the given query and history (possibly empty). At train time, we emulate a session by invoking M Summ with a sequence of differing queries, Q = {q 1 , q 2 , ..., q m }, for which to generate the corresponding sequence of output sentences. I.e., output sentence e out t is biased on query q t and the summary-so-far E t at time step t. We next describe the architecture 3 of M Summ , also illustrated in Figure 2.\nSentence encoding. The first step of the model is hierarchically encoding the sentences of the document set D to obtain contextualized representation c j for sentence s j \u2200j. A CNN (Kim, 2014) encodes s j on the sentence level and then a bi-LSTM (Huang et al., 2015) forms representation c j on the document level, given the CNN encodings.\nQuery encoding. Additionally, at each time step t we prepare sentence+query representations c t j = c j \u2295 CNN(q t ), i.e., obtained by concatenating a sentence representation and the CNN-encoding of the current query. This sentence+query represen- Contextual sentence embeddings are concatenated to the current query embedding. The sentence+query representation is softly attended with a transformed query-focused MMR score, and a sentence selection distribution is obtained with a two-hop attention mechanism, considering a summary-so-far representation. A dual-reward mechanism, using the reference summaries and query, optimizes a policy to train the model for summary content quality and sentence-to-query resemblance. At inference time, an initial summary is generated with empty E in and q t -s, while for an expansion they are not empty. tation influences the relevance of a sentence with respect to the current input query.\nQuery-MMR score weighting. MMR has been shown to be effective in MDS, where information repeats across documents. It aims to select a salient sentence for a summary, that is non-redundant to previous summary sentences. We extend standard MMR so that the importance of the sentence is in regards to both the document set and the query. Formally, the query-focused MMR function defines a score m t j for each s j at time step t as follows:\nm t j = \u03bb \u2022 BISIM(s j , D, q t ) \u2212 (1 \u2212 \u03bb) \u2022 max e\u2208Et SIM(s j , e) (1) BISIM(s j , D, q t ) = \u03b2 \u2022 SIM(s j , D \u2295 ) + (1 \u2212 \u03b2) \u2022 SIM(s j , q t ) (2)\nwhere \u03bb \u2208 [0, 1] balances salience and redundancy and \u03b2 \u2208 [0, 1] balances a sentence's salience within its document set and its resemblance to the current query. SIM(x, y) measures the similarity of texts x and y, and D \u2295 is a fully concatenated version of document set D. Following findings of , SIM computes cosine similarity between the two compared texts' TF-IDF vectors. Redundancy to previous sentences is computed as the highest similarity-score against any of the previous sentences. We set \u03bb = 0.6 (following Lebanoff et al., 2018) and \u03b2 = 0.5 (see Appendix B.3).\nThe query-focused MMR scores are incorporated into M Summ by softly attending on the sentence representations with their respective translated query-focused MMR scores:\n\u00b5 t = softmax(MLP(m t ))(3)\nc t j = \u00b5 t j c t j (4)\nState representation. At time t, a representation z t of the summary-so-far is computed by applying an LSTM encoder on {c idx(e in 1 ) , ..., c idx(e in k ) , c idx(e out 1 ) , ..., c idx(e out t\u22121 ) }, i.e., on the plain sentence representations of E t , where idx(e) is the index of sentence e. Then, a state representation g t considers z t and all sentence representations with the glimpse operation (Vinyals et al., 2016):\na t j = v 1 tanh(W 1\u0109 t j + W 2 z t )(5)\n\u03b1 t = softmax(a t )(6)\ng t = j \u03b1 t j W 1\u0109 t j(7)\nwhere v 1 , W 1 and W 2 are model parameters, and a t represents the vector composed of a t j . Finally, a sentence s j at time t is assigned a selection probability softmax(p t ) j such that:\np t j = v2 tanh(W 3\u0109 t j + W 4 g t ) if sj / \u2208 Et \u2212\u221e otherwise (8)\nwhere v 2 , W 3 and W 4 are model parameters.\nReinforcement learning. As M Summ 's goal is to incrementally generate a query-assisted summary, it should strive to optimize (1) nonredundant salient-sentence extraction and (2) queryto-sentence similarity, that can be appraised with ROUGE (Lin, 2004) and text-similarity metrics, respectively. A policy gradient-based RL approach (Williams, 1992) allows optimizing on such nondifferentiable metrics. Specifically, we adopt the Advantage Actor Critic method (Mnih et al., 2016) for policy learning, and a dual-reward procedure (Pasunuru and Bansal, 2018) to alternate between the summary and query-similarity rewards. At time step t, for selected sentence e out t (based on softmax(p t )), reward r t is computed and weighted into M Summ 's loss function. The reward function alternates, from one train batch to the next, between ROUGE \u2206 (e out t , E t , R) and QSIM(e out t , q t ). The former computes the ROUGE difference before adding e t to E t and after:\nROUGE\u2206(e out t , Et, R) = ROUGE((Et \u222a e out t ) \u2295 , R) \u2212 ROUGE(E \u2295 t , R) (9)\nA larger ROUGE \u2206 value implies that e t concisely adds more information onto E t , with respect to topic reference summaries R. We use ROUGE-1 F 1 as the ROUGE function here. The querysimilarity reward function QSIM(e out t , q t ) = avg(SEMSIM(e out t , q t ), LEXSIM(e out t , q t )) (10) computes an average of semantic and lexical similarities between the selected sentence and corresponding query. SEMSIM computes the cosine similarity between the average of word embeddings (spaCy: Honnibal and Montani, 2021) of e out t and that of q t . For lexical similarity,\nLEXSIM(e out t , q t ) = avg(R p 1 (e out t , q t ), R p 2 (e out t , q t ), R p L (e out t , q t )) (11)\nis the average of ROUGE-1, 2 and L precision scores between sentence and query. By alternating between the two rewards, we train a sentenceselection policy in M Summ to balance summary informativeness and adherence to queries.\nOverall system. Our M Summ model adopts its base architecture from  (for generic MDS). Chiefly, we modify their model for handling an input query-sequence and a sentence history, and employ a different summarization reward function. The query is incorporated in the sentence representation, in the new query-focused MMR function and in the dual-reward mechanism.", "n_publication_ref": 8, "n_figure_ref": 1}, {"heading": "Model Training", "text": "Pre-training. To provide a warm start for training M Summ , a reduced version of M Summ is first pre-trained for generic extractive single-document summarization using the large-scale CNN/Daily Mail corpus (Hermann et al., 2015), as proposed by Chen and Bansal (2018) For each topic, we generate an \"oracle\" extractive summary by greedily aggregating 10 sentences from D, that maximizes the ROUGE \u2206 -1 recall against R. Then for each sentence, we extract a bi-or trigram that is most lexically-unique to the sentence, in comparison to all other sentences in D. This yields a sequence of 10 \"queries\" that could easily render the corresponding oracle summary. The intuition for this approach is that it would teach M Summ that it is worthwhile to consider a given query when selecting a sentence that is informative with respect to the reference summaries. This further assists in fulfilling the dual requirements of selecting a globally informative sentence that also adheres to the query. 4 Appendix B.3 discusses usage of different query types for training.\nValidation metric. As the interactive session progresses, a recall curve emerges, that maps the ROUGE recall score (here ROUGE-1) versus the expanding summary token-length. Once the session halts, the area under the curve indicates the efficacy of the session for information exposure. A higher value implies faster unveiling of salient information. Normalizing by the final summary length allows approximate comparability between different length sessions. We hence use the average (over topics) length-normalized area under the recall curve for validating the training progress.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Suggested Queries Extraction Model", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Subtask Formulation", "text": "We now consider the second subtask of INTSUMM: generating lists of suggested queries. The list is regenerated after every interaction, to yield queries that focus on sub-topics that were not yet explored.\nReusing the notations of M Summ in \u00a73, we define a model, M Sugg , for suggested queries list generation, that receives an input tuple (D, E in , m) (notice that a query is not needed here). Here, the jth phrase in D is denoted \u03c1 j , when the documents in D are concatenated, and accordingly, history E in is a list of phrases extracted from the session's current accumulated summary. m is the number of suggested queries to output. The model outputs phrase sequence E out = {e out 1 , e out 2 , ..., e out m } from D, accounting for history E in . As in M Summ 's setting, D is paired with a set of generic reference summaries R.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model Architecture", "text": "We adopt and adjust the architecture in \u00a73.2 for this subtask. Similar to M Summ , M Sugg selects input units one-by-one considering a history, with the main difference being the absence of query injection. Additionally, inputs and outputs are processed on the phrase-rather than the sentence level.\nPhrase and state representation. For the given document set, all noun phrases are extracted using a standard part-of-speech regular expression method (Mihalcea and Tarau, 2004;Wan and Xiao, 2008).\nWe obtain document-level contextual phrase embeddings, c j for phrase \u03c1 j , with the CNN and bi-LSTM networks, and softly attend the embeddings with a standard MMR score:\nm t j = \u03bb \u2022 SIM(\u03c1 j , D \u2295 ) \u2212 (1 \u2212 \u03bb) \u2022 max e\u2208Et SIM(\u03c1 j , e) (12)\nThe MMR-based phrase representations then pass through the glimpse attention procedure, which culminates in the phrase probability distribution for selecting the next output phrase.\nReinforcement learning. The policy in M Sugg is trained with a single reward function that measures how prominent the selected phrase is within the reference summaries, and how different it is from previously seen phrases. Formally, at time step t, the reward r t of selected phrase e out t is:\nr t = PF(e out t , R) \u2212 \u03b3 1 \u2022 PFMAX(e out t , E in , R) \u2212 \u03b3 2 \u2022 PFMAX(e out t , E t \\ E in , R) (13)\nPF(e out t , R) = avg r\u2208R (avg w\u2208e out t TF(w, r)) ( 14)\nPFMAX(e out t , L, R) = max e\u2208L PF(e out t \u2229e, R) (15) where TF(w, r) is the relative frequency of word w in reference summary r. Namely, PF computes the average term frequency of a phrase over its words and across the reference summaries, as an estimate of the phrase importance within the topic. PFMAX computes the highest PF against a list of phrases, which is used to lower the reward of a phrase that is redundant to phrases used earlier. Different weights are given to the PFMAX against the input history (\u03b3 1 ) and that of the phrases output so far (\u03b3 2 ).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Model Training", "text": "Similarly to M Summ , we first pre-train the base model to get a warm start on embedding formation and salience detection. The reduced architecture of M Summ and M Sugg for pre-training are identical.\nWe use the same DUC 2007 training data, with document sets and reference summaries, and additionally prepare three \"histories\" per topic: one empty and two non-empty. An empty history mimics generating a session's initial list of suggested queries, while a non-empty history trains the model to consider previously known information. Training with two non-empty histories per topic prepares a model for varying informational states. These are curated from a generic summary (from a trained M Summ model) that is truncated at two random sentence-lengths between 1 and 12. Overall, the model is trained on three versions of each topic, each time with a different history.\nSimilarly to M Summ , validation is guided by the average normalized area under the recall curve. Here, the accumulating r t scores from Equation 13are used as the recall of the expanding suggested queries list. I.e., a higher reward means better suggested queries are output earlier. The AUC is normalized with the total token-length of all suggested queries to mitigate for lengthy phrase extractions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We ran several experiments for the assessment of our M Summ and M Sugg models, applying the INTSUMM evaluation framework of Shapira et al. (2021b). The goals of the experiments are to compare varying configurations of our models and to evaluate against an INTSUMM baseline system. The experiments include both simulations and interactive sessions with human users.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Compared Algorithms", "text": "The M Summ model architecture ( \u00a73.2) has several configurable components: encoding the query into sentences, considering the query in the MMR function (both at train and inference time), and the dual reward mechanism. We compared several variations of these using simulations, presented in \u00a75.2.\nIn addition, we compare, both via simulations ( \u00a75.2) and real sessions ( \u00a75.3), against the (betterperforming) baseline system in (Shapira et al., 2021b), named S 2 . S 2 's initial summary algorithm is TextRank, and the query-response generator extracts sentences via lexical+semantic similarity to the query, somewhat resembling QSIM in Equation 10, fully neglecting the summary-so-far, in contrast to M Summ . S 2 's suggested queries list contains TextRank's top salient topic phrases. Since these too do not account for the summary-so-far, they are computed at the session beginning and are not updated along the session, in contrast to M Sugg .", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Simulated Experiments", "text": "The INTSUMM task involves human users by definition. Nevertheless, running on simulated query lists and session histories is pertinent for efficient system evaluation and comparison of methods.\nTo simulate the query-assisted summarization algorithms, we utilize the real sessions recorded by Shapira et al. (2021b): 3-4 user sessions on 20 topics from DUC 2006 collected with S 2 . In our simulation, each summary-so-far from a recorded session is fed as input to the system together with the following recorded user query. We then measure R recall 1\u2206 (difference of ROUGE-1 recall incurred by the query response compared with the input summary-so-far). Additionally, we use R F 1 1 (ROUGE-1 F 1 ) for initial summary informativeness. Both are measured w.r.t. the reference summaries, normalized by the output length, and averaged per session recording, and then over all sessions and topics, to get an overall system infor-mativeness score. We also measure system queryresponsiveness using the QSIM metric.\nTable 1 presents a representative partial ablation of the M Summ model. All variants were configured to output sentences of up to 30 tokens, initial summaries are 75 tokens, and query responses are 2 sentences. Configurations i-iv use the query in training, while v and vi do not. Each configuration is measured for informativeness (columns marked with \u2020), and for query-responsiveness (QSIM column). Out of configurations i-iv, config. i, where we employ all mechanisms for query inclusion, yields the best overall scores in both informativeness and query-responsiveness, despite the inherent tradeoff between the two. In the second set of configurations (v-vi), we observe that ignoring the query at train time substantially degrades queryresponsiveness, and this is expectedly further exacerbated when also ignoring the query at inference time. However, disregarding the query gives more informative expansions with respect to reference summaries, since the model was trained only to optimize content informativeness, and is less likely to sidetrack to the query-related information.\nCompared to S 2 (last row), our model significantly improves informativeness.\nQueryresponsiveness is better in the S 2 baseline since its query-response generator simply invokes a function similar to QSIM, but for the price of lower informativeness. Still, this does not lead to inferior overall user experience, see \u00a7 5.3.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Real Session Collection and Evaluation", "text": "We collect real user sessions via controlled crowdsourcing (which provides high quality work, see Appendix D) with the use of an INTSUMM web application 5 running either our M Summ +M Sugg models or the S 2 baseline algorithms, enabling a comparative assessment of the two systems. Notably, our algorithms have the low latency required for the interactive setting (Attig et al., 2017), i.e., responding almost immediately. 6 Using the DUC 2006 INTSUMM test set, we prepared two complementing user sets of 20 topics, each with 10 of the topics to be run on our system and the other 10 on the baseline. We apply the evaluation metrics of Shapira et al. (2021b)   area under the sessions' ROUGE recall curves, in a common word-length interval across all sessions and topics, which demonstrates how fast salient information is exposed in sessions.\n(2) ROUGE F 1 at the initial summary and at 250 tokens, that indicate how effectively the interactive system can generate summaries at pre-specified, comparable lengths.\n(3) Manually assigned query-responsiveness score (1 to 5 scale), which expresses how well users think the system responded to their requests. And (4) manual UMUX-Lite (Lewis et al., 2013) score for system usability (effectiveness and ease of use), where 68 is considered \"acceptable\" and 80.3 is considered \"excellent\". We also measure automatic query-responsiveness with QSIM. 7 We conducted two such comparative collection and assessment experiments, either employing M Summ configuration v or i, namely the best of the two configuration sets. In both cases, the M Sugg model used was set with \u03b3 1 = 0.5 and \u03b3 2 = 0.9 after some hyperparameter tuning (Appendix B.4). The first experiment (with configuration v) is described here, and the other in Appendix E.1.\nWe hired 6 qualified workers using the controlled crowdsourcing procedure, and collected 2-3 sessions per topic per system (111 total sessions). In the sessions, users explore their given topic by submitting queries with a common generic informational goal in mind (Appendix D).\nOverall system assessment. Table 2, presenting average scores over the collected sessions, shows that our system is significantly more effective for exposing salient information, as depicted in the first three rows. Users indicate a slight degradation in query-responsiveness of our system, consistent with QSIM scores (row 4-5). Note that the observed difference in QSIM scores, between simulations and user sessions, partly stems from the fact that they were computed over different sets of queries. The varying queries issued by the users in user sessions form a less stable query responsiveness comparison than the one in Table 1, where QSIM scores are computed using consistent queries for all systems. Despite the gap in QSIM scores between our system and S 2 in Table 2, the overall usability scores are slightly better (last row). This may suggest that users appreciate the informativeness of the produced summary even when they are aware that the summary is less biased on their queries; thus our system improves informativeness while still providing a favorable user experience.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Assessment of suggested queries functionality.", "text": "We analyzed the types of queries users submitted throughout their sessions, to assess the utility of updating suggested queries, with M Sugg , as opposed to a static list of suggestions, with S 2 . To that end, we tallied suggested query clicks and query submissions via other modes, binning the tallies to three sequential temporal segments within their respective sessions (Appendix E.3). We found that, on average, the usage of suggested query clicks increased by~13% when nearing the end of a session with M Sugg , and conversely decreased by~24% with S 2 . While the decrease in use of the static list is expected, since appealing queries are likely exhausted earlier in a session, it is encouraging to witness the usefulness of updated queries as the session progresses. This behavior suggests that the updated list contains suggested queries that are indeed engaging for learning more about the topic.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "Interactive summarization for information exploration is a task that requires compliance to user requests and session history, while comprehensively handling a large input document set. These requirements pose a challenge for advanced text processing methods due to the need for fast reaction time. We present novel deep reinforcement learning based algorithms that answer to the task requirements, improving salient information exposure while satisfying user queries and keeping user experience positive.\nWe note that while M Summ is designed for the INTSUMM task, it may potentially be serviceable for standard MDS, QFS, update summarization and combinations thereof. This can be accommodated by a proper choice of input, e.g., QFS can be addressed by giving M Summ as input a query, an empty history and target summary length. In future work, we may study the performance of our solutions for such tasks, as well as strive to further improve their performance on both ends of the INTSUMM task -selecting topically salient information and responding to user queries.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Ethical Considerations", "text": "Datasets. The DUC 2006 and 2007 datasets were obtained according to the DUC website (duc. nist.gov) requirements. It was not possible for others to reconstruct the document sets and reference summaries of the dataset from the crowdsourcing tasks.\nThe datasets are composed of new articles mainly from the late 1990s from large news outlets, compiled by NIST. All data exposed by our systems are directly extracted from those articles. For extraction, we do not intentionally add in any rules for ignoring or boosting certain information due to an opinion.\nCrowdsourcing. Due to the need for English speaking workers, a location filter was set on the Amazon Mechanical Turk (https://www. mturk.com) tasks for the US, UK and Australia. All tasks paid according to a $10 per hour wage, according to the estimated required time of each task. The payment was either paid per assignment, or as a combination with a bonus.\nCompute resources. Our M Summ and M Sugg models required between 2 and 20 hours of training (usually around 4 hours), depending on the configuration. We trained on one NVIDIA GeForce GTX 1080 Ti GPU with 11GB memory. The pretrained base model was trained once and reused in all subsequent training. Outputting at inference time is computationally cheap: M Summ runs upto about 1 second, but mostly in a few hundred milliseconds, and M Sugg runs upto about 7 seconds, but mostly in under 4 seconds. Training with a batch size of 8 used about 3GB GPU memory for M Summ , and about 9GB memory for M Sugg (since there are many more input units per document set, i.e., all noun phrases versus sentences).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B Implementation Details", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B.1 Pre-training Technicalities", "text": "To provide a warm start for training M Summ and M Sugg , a reduced version of the models, which is the same for both, is first pre-trained for generic extractive single-document summarization using the CNN/Daily Mail corpus (Hermann et al., 2015) with about 287k samples, as proposed by Chen and Bansal (2018). In this reduced model,\u0109 t j is replaced by c j in Equations 5, 7 and 8. Further-more, there is a single reward function for learning the policy, computed per selected sentence e out t as ROUGE-L F 1 w.r.t. the (single) reference summary's sentence at index t. The reduced model pre-trains the full model for contextual sentence representation and for salient-sentence selection in the single-document generic setting. This allows training M Summ and M Sugg with a relatively small dataset for their final purposes.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "B.2 Training Technicalities", "text": "Following , the pre-trained base model is the rnn-ext + RL model from Chen and Bansal (2018), and is trained like in Lebanoff et al. (2018). Both M Summ and M Sugg are further trained on our adjusted DUC 2007 data using an Adam optimizer with a learning rate of 5e-4 and no weight decay. A discount factor of 0.99 is used for the reinforcement learning rewards. The batch size was 8. Training was halted once 30 consecutive epochs did not improve the validation score.\nThe MMR function within our models uses TF-IDF vector cosine similarity for all SIM instances (in Equations 1 and 12). The TF-IDF vectorizer is initialized with the document set on which the MMR score is computed.\nAs is commonly practiced, selection of an output sentence/phrase e out t is done by sampling probability distribution p t (in Equation 8) at train time, and by extracting the maximum scoring sentence/phrase at inference time.\nThe MLP in Equation 3 transforms the MMR score with a feed-forward network with one-hidden layer of dimension 80 following .", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "B.3 Query-Assisted Summarization Model", "text": "Model configurations. The architecture of the M Summ model and its training allowed for much creativity in the configuration process. Other than the combinations mentioned in the paper in Table 1, we also experimented with other components. We list here many of the experiments, without formal results. Anecdotes are taken by looking at validation scores and some eyeballing.\n(1) The \u03b2 value in the query-focused MMR function in Equation 2, that impacts the weight of the query on a sentence versus the document set on the sentence. We tried out a few \u03b2 values and mainly noticed that a value of 0.5 kept validation results more stable across configurations, or kept training time shorter. In our experiments, to cancel out this component (both at training and inference time), we simply set \u03b2 = 1 so that the query is not considered.\n(2) Different summary reward functions. ROUGE \u2206 recall (instead of F 1 ) was also a good alternative, but gave somewhat less stable results across configurations. ROUGE (not as \u2206) was also less stable with recall and F 1 , and gave too short and irrelevant sentences with precision. We also tried sentence level ROUGE-L, like in , eventually outputting sentences that were much less compliant to queries.\n(3) Using only the query similarity reward instead of the dual reward mechanism worked surprisingly well. This may be due to the queries on which the model was trained on. These queries were very relevant to the gold reference summaries, hence possibly implicitly providing a strong signal to salient sentences within the document set. Still, this was less productive than our final choice of reward.\n(4) Adding training data (additional DUC MDS datasets) did not impact the results. Importantly, since DUC 2007 is most similar to the test DUC 2006 set, it seems to be more beneficial to include DUC 2007 in the training set.\n(5) We also tried representing the query in the input by concatenating it's raw text to each input sentence before get the sentence representations.\n(6) To represent the sentences, we also tried using average w2v vectors (Honnibal and Montani, 2021) and Sentence-BERT (Reimers and Gurevych, 2019) instead of the CNN network. These did not show any apparent improvements, and were notably expensive in terms of execution time.\n(7) For the sentence similarity in the query-MMR component, we tried w2v and Sentence-BERT representations instead of TF-IDF vectors. Similarly to (6), they did not show improvements over using TF-IDF, and were very time-costly.\n(8) Instead of the dual-reward mechanism that alternates between the two rewards from batch to batch, we also considered using a weighted average of the two rewards, consistently over all batches. Further experimentation is required on this technique for a more conclusive judgment.\nQueries used for training. The queries used for training the M Summ model can affect the way it learns to respond to a query. Seemingly, the most natural approach would be to train the model as close as possible to the model's use at inference time. This would mean training M Summ with queries from real sessions. However, a session's queries are dependent on outputs previously produced by the used system. It is therefore not certain that the sequence of queries from a different system's usage would necessarily benefit the training process when compared to a synthesized sequence of queries. I.e., it's not actually possible to train with \"real sessions\" in a conventional way.\nAlso, as stated in \u00a73.3, the synthetic queries we eventually used direct the model to select salient sentences, which can support our dual-objectives: to get a sentence that is both globally salient to the topic, as well as responsive to the query. We tried training on other query types, synthesized with various keyphrase extraction techniques, and found that our final choice of queries more consistently gave good results overall.\nSentence length. We segmented the sentences in the document sets with the NLTK 8 sentence tokenizer, and removed sentences that contain quotes in them or do not end with a period.\nDuring training we did not constrain the input sentences in any way. Some of the configuration experiments described above were done to check how the configuration might influence the length of the selected sentences. The best configurations, including the one we eventually used in our tests, tended to output somewhat longer sentences. Very long sentences are usually tedious for human readers, and we hence limited the sentences to 30 tokens at inference time. We found that this length constraint caused a slight degradation in simulation score results of our models, however still gave superior informativeness results compared to the baseline system.\nInitial summary length. Sentences are accumulated until surpassing 75 tokens. Therefore summaries are not shorter than 75 tokens, but mostly not much longer than that.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "B.4 Suggested Queries Extraction Model", "text": "Model configurations. We experimented with different configurations and hyper-parameter finetuning in the M Sugg model as well. Tuning was performed in accordance to the validation scores and generic keyphrase extraction scores on the MK-DUC-01 multi-document keyphrase extraction dataset of Shapira et al. (2021a).\n(1) In the reward function in Equation 13, we set \u03b3 1 = 0.5 and \u03b3 2 = 0.9, i.e., the preceding output phrases are more strongly accounted for than the phrases in the session history. We tested several values between 0 and 1 for both hyper-parameters.\n(2) We implemented altered versions of the reward function in Equation 13. Instead of phrase unigram-level frequency, we tried computing the full phrase frequency and computing partial phrase frequency, i.e., a maximal phrase template match within a reference summary. All functions tested were adequate overall, though our final choice of reward function was closest to the keyphrase extraction task unigram overlap metric, and gave best results overall.\n(3) We also attempted noun phrase extraction with the spaCy 9 noun chunker and named entity recognizer. This combined approach misses some noun phrases within the text, but mainly is also more computationally heavy than the simple POS regex search that we use.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Extracting phrases with regular-expression.", "text": "We extracted all noun-phrases from the document set by first mapping all tokens to their part-of-speech tags, and then applying a regularexpression chunker with regex: {(<JJ> * <NN. * >+ <IN>)? <JJ> * <NN. * >+}. These steps were accomplished with NLTK.\nPhrase length. There is no limit set on the phrase length. We tried training and inferring with a phrase length constraint of 4 words, but found that this gave worse results overall.\nHistory sentences to phrases. M Sugg works on the phrase level. Meanwhile, in our extractive interactive setting, the history is a set of sentences already presented to the reader. Therefore, when extracting phrases from D, we also link each phrase to its source sentence, and obtain E in by compiling the phrases linked from the history sentences.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C Dataset Notes", "text": "While DUC 2006 (our test set) and 2007 (our train/validation set) were originally designed for the query-focused summarization task, they contain excessive topic concentration due to their long and descriptive topic queries (Baumel et al., 2016). Hence, their reference summaries can practically be considered generic. 9 https://spacy.io/", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "D Session Collection", "text": "Controlled crowdsourcing protocol. We followed the controlled crowdsourcing protocol of Shapira et al. (2021b), which includes three steps:\n(1) a trap task for finding qualified workers; (2) practice tasks for explaining the interface and the purpose, as well as reiterating the generic information goal (see below) during exploration; (3) the session collection tasks. We used the Amazon Mechanical Turk HITs prepared by Shapira et al. (2021b).\nProcess cost. We paid $0.40 for a trap task assignment, with 400 assignments released, and $0.90 for a practice task assignment, with 28 assignments completed. The session collection assignment paid $0.70, and a bonus mainly according to the length of interaction and additional comments provided. The bonus was between $0.15 and $0.35. A total of 111 sessions were recorded from 6 high quality workers. The full process cost about $385 in total (including the Mechanical Turk fees) for the experiment including configuration v in Table 1.\nThe second round of experiments done on another variant of our system (configuration i) also included 28 practice tasks and compiled 10 final workers for a total of 180 collected sessions. Bonuses ranged from $0.10 and $0.40 on the session collection task. The full process cost of the second experiment was about $475 in total (including the Mechanical Turk fees).\nSession collection data preparation. We used the same 20 test topics as Shapira et al. (2021b), and created 2 batches of tasks. For the first batch, in alternating order of topics, 10 topics were paired with our system, and the other 10 were paired with the S 2 baseline. The other batch consisted of the complementing topic-system pairings. The workers were assigned a batch to work on such that half of the workers would work on each batch.\nUser informational goal. Since all sessions on a topic are evaluated against the same reference summaries, it is important that users aim to explore similar information. Following Shapira et al. (2021b), during practice tasks all users received a common informational goal to follow, so that the sessions are comparable. The emphasized description was: \"produce an informative summary draft text which a journalist could use to best produce an overview of the topic\".\nSessions filtering. In the first experiment, we filtered out 7 sessions that accumulated less than 250 tokens (from 2 different workers).\nIn the second experiment, 9 of the 10 workers completed at least 19 of the 20 topics One worker completed only 3 tasks and we disregarded those sessions. We also threw away 9 sessions that accumulated less than 250 tokens.\nINTSUMM user interface. We used the same user interface developed by Shapira et al. (2021b) with a small change to enable suggested query list updates after each interaction (the interface was designed for the baselines, where the suggestedquery list is static). To refrain from any possible user experience bias, we made the UI change as least apparent as possible.\nSystem response time. M Summ is able to generate summaries mostly in under a second, and M Sugg prepares the list in a few seconds. The summary expansion is hence presented to the user almost immediately after query submission, and the suggested queries list is shown shortly afterwords, before the user finishes reading the expansion. The small delay in suggested query updating is hence almost unnoticed. The baseline summarizer responds similarly fast to M Summ , making response-time difference unperceivable between the systems.\nUser feedback. Many of the users provided feedback about the session collection tasks after finishing their assignment batch. The overall impression was that there was no strong preference for either system. For example, one user wrote: \"I did not discern a consistent difference between the two systems that would result in having a clear preference.\" This kind of comment was repeated by several users. Generally, there were no explicit comments about the difference in quality of the summary outputs, and topics were mostly scored or commented on similarly between the two systems since the complexity of the topic influenced the ability of the systems to comply to the user.\nA comment in favor of updating suggested queries during interaction said: \"It was nice to have a new list as you progressed through the task, it helped me think of where to go next if I got stuck...\" This specific comment was written by a user that explored topics quite deeply. On the other hand, a user that explored more shallow liked that used suggested queries in the static list were marked: \"I did notice...the red font color on the used queries.\nThat was helpful.\" It therefore seems that updating suggested queries are more useful for lengthy exploration, but for quick navigation, the static list might naturally be enough.  We conducted two comparative session collection and analysis experiments, one using M Summ model configuration v (from Table 1), as presented in \u00a75.3 and Table 2, and another with M Summ model configuration i. As explained in \u00a75.2, these two configurations performed best, on simulations, out of their respective configuration sets.\nWe show here results of the second experiment, where we used M Summ model configuration i, with the same M Sugg model as in the first experiment. The S 2 baseline was similarly used for comparison. We also kept the same AUC length limits (106 to 250 tokens) for easy comparability to Table 2. Table 3 shows the results. Here too, while less substantially, informativeness is improved with our system without significantly harming the user experience. Overall, it seems that users were somewhat more satisfied with the INTSUMM system that uses M Summ configuration v than configuration i. Interestingly, it seems the users may have appreciated the slightly better informativeness of configuration v even if the query-responsiveness was not as good as in configuration i, as shown through the QSIM score. In addition, we see that absolute manual scores in Table 3 are lower than in Table 2, but trends are generally similar. It is common that scaling of manually supplied scores can fluctuate (e.g. Gillick and Liu, 2010).\nFigures 3 and 4 show the averaged (per topic and then over all topics) recall curves of the collected sessions in the experiment described in \u00a75.3 and above, respectively. The x-axis is the accumulating token-length of the session, and the y-axis is the ROUGE-1 recall. The points on the curve are the average interpolated values from all the sessions. The vertical dashed lines are the intersecting bounds of the sessions, from 106 tokens to 250. The area under the curve (AUC) is computed for each of the curves, and reported in the first row of Tables 2 and 3. The higher AUC scores obtained from the recall curves of our models, compared to those of the S 2 baseline, highlight the ability to expose more salient information earlier in the session.", "n_publication_ref": 6, "n_figure_ref": 1}, {"heading": "E.2 Execution Time of Systems", "text": "Systems that are made for interacting with humans must respond quickly in order to keep the user's engagement. The exact amount of time does not affect the user experience as long as it does not surpass some limit, after which the user starts losing interest or feeling irritated (Attig et al., 2017;Anderson, 2020).\nAs mentioned in Appendix D, M Summ generates summaries in under a second and M Sugg prepares the list in a few seconds. The baseline summarizer also responds in under a second. The difference between the systems is virtually unperceivable during interaction. There were no comments from the users in our experiments that stated any issue with execution time.\nFigure 3: Averaged recall curves of our system and the S 2 baseline system in the experiment described in \u00a75.3 and Table 2 (using M Summ configuration v from Table 1). The intersecting range is bounded by dashed lines (between 106 and 250 tokens).", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "E.3 Assessment of Suggested Queries Functionality", "text": "In this analysis, we assessed what modes of query submission users relied on over the course of a session. To that end, (1) we divided each session to three segments (first, second and third part of the session), and counted the types of queries. The types are \"suggested query\", \"free-text\", \"highlight\" (a span from the summary text) and \"repeat\" (repeating the last submitted query). ( 2) We then computed the percentage of each mode in each segment.\n(3) The percentages over all sessions and all topics were computed for each of the three segments.\nThis process was conducted only for sessions between 4 and 20 interactions, as the few long and short sessions often show different behavior. For the first experiment, this left 43 sessions with avg. 8.63 (std. 2.32) interactions for our system, and 50 sessions with 8.44 (2.48) interaction for S 2 . For the second experiment, it left 72 sessions with 10.24 (4.82) interactions for our system, and 74 sessions with 9.59 (4.42) interactions for S 2 .\nWe focus here on the use of suggested queries versus all other query types. In the first experiment we observe a change of +9% from the first to the third segment in our system, and -20% in S 2 . In the second experiment we see +18% and -28% in S 2 . As discussed in \u00a75.3, this suggests the effectiveness of updated suggested queries, especially by the end of a session.\nFigure 4: Averaged recall curves of our system and the S 2 baseline system in the experiment described here in Appendix E.1 and Table 3 (using M Summ configuration i from Table 1). The intersecting range is bounded by dashed lines (between 106 and 250 tokens).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "F Further Explanations on Evaluation Metrics", "text": "The normalized AUC score for the validation metric (explained in \u00a73.3) is computed over the recall curve produced from the accumulating summary expansions. Each point on the curve marks an accumulating token-length (x-axis) and an accumulating recall score (y-axis) of an interactive state, as depicted in Figures 3 and 4 (although these figures show the averaged session recall curves with bounds, whereas during validation the curve is for a single session and there are no bounds set). By computing the area under the full curve, and dividing by the full length, the normalized AUC score is obtained. The normalization gives an approximate absolute value that can be compared at different lengths (although at large length differences this is not comparable due to the decaying slope of the curve).\nThe manual query-responsiveness score, reported in Tables 2 and 3, is obtained by asking users, at the end of a session, \"During the interactive stage, how well did the responses respond to your queries?\", for which they rate on a 1-to-5 scale. The scores are averaged over the topic and then over all topics. This follows the evaluation defined in Shapira et al. (2021b).\nThe UMUX-Lite score (Lewis et al., 2013), reported in Tables 2 and 3, is obtained by asking users to rate (1-to-5) two statements at the end of a session: (1) \"The system's capabilities meet the need to efficiently collect useful information for a journalistic overview\" and (2) \"The system is easy to use\". The first question refers to the users' informational goal that they received, in order to follow a consistent objective goal during their exploration. The final score is a function of these two scores, and is used as a replacement for the popular SUS metric (Brooke, 1996) (with a much longer questionnaire), to which it shows very high correlation, thus offering a cheaper alternative. This also follows the evaluation defined in Shapira et al. (2021b).\nAll confidence intervals in Tables 1, 2 and 3 are computed as margins-of-error, on the topiclevel, over the standard error of the mean with 95% confidence. 10 The token-length values in ", "n_publication_ref": 5, "n_figure_ref": 1}, {"heading": "G A2C Policy Learning", "text": "A policy gradient-based reinforcement learning approach (Williams, 1992) allows optimizing on nondifferentiable metrics, and eliminates the exposure bias that occurs with traditional training methods, like cross-entropy, on generation tasks (Ranzato et al., 2016).\nSpecifically, we use the Advantage Actor Critic (A2C) policy gradient training method. See technical explanations in the appendix of (Chen and Bansal, 2018). At a high level, an output reward (subtracted by a baseline reward -computed on a version of the model without MMR attention) is used to weight the output selection in the loss function. In so, outputs with higher rewards increase the likelihood of those outputs and lower rewards decrease the likelihood. Since the reward function is not differentiable, it is used as a weight on the probability of the selected output, which is then given to the loss function.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "H INTSUMM Example", "text": "We show in Figure 5 an example of an INTSUMM system using the web application of Shapira et al. (2021b) and our our M Summ (configuration i from Table 1) and M Sugg models in the backend.  shows the result of clicking the \"carbon dioxide gas\" suggested query (with the query response and updated suggested queries list). Sub-figure (c) shows the result of subsequently submitting the query \"water level\". Query responses should be informative for the general topic, while also complying to the user queries. System summaries and expansions must be output fast in order to allow smooth interaction and human engagement.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their constructive comments and suggestions. This work was supported in part by Intel Labs; by the Israel Science Foundation (grants no. 2827/21 and 2015/21); by a grant from the Israel Ministry of Science and Technology; by the NSF-CAREER Award #1846185; and by a Microsoft PhD Fellowship.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "How Fast Should A Website Load?", "journal": "", "year": "2020", "authors": "Shaun Anderson"}, {"title": "System Latency Guidelines Then and Now -Is Zero Latency Really Considered Necessary?", "journal": "Springer International Publishing", "year": "2017", "authors": "Christiane Attig; Nadine Rauh; Thomas Franke; Josef F Krems"}, {"title": "Query-Chain Focused Summarization", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Tal Baumel; Raphael Cohen; Michael Elhadad"}, {"title": "Topic Concentration in Query Focused Summarization Datasets", "journal": "AAAI Press", "year": "2016", "authors": "Tal Baumel; Raphael Cohen; Michael Elhadad"}, {"title": "Query Focused Abstractive Summarization: Incorporating Query Relevance", "journal": "", "year": "2018", "authors": "Tal Baumel; Matan Eyal; Michael Elhadad"}, {"title": "Longformer: The Long-Document Transformer", "journal": "", "year": "2020", "authors": "Iz Beltagy; Matthew E Peters; Arman Cohan"}, {"title": "SUS-A quick and dirty usability scale. Usability evaluation in industry", "journal": "", "year": "1996", "authors": "John Brooke"}, {"title": "The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries", "journal": "", "year": "1998", "authors": "Jaime Carbonell; Jade Goldstein"}, {"title": "Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yen-Chun Chen; Mohit Bansal"}, {"title": "Hierarchical Summarization: Scaling Up Multi-Document Summarization", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Janara Christensen; Stephen Soderland; Gagan Bansal; Mausam "}, {"title": "LexRank: Graph-Based Lexical Centrality as Salience in Text Summarization", "journal": "Journal of Artificial Intelligence Research", "year": "2004", "authors": "G\u00fcnes Erkan; Dragomir R Radev"}, {"title": "Position-Rank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Corina Florescu; Cornelia Caragea"}, {"title": "Non-Expert Evaluation of Summarization Systems is Risky", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Dan Gillick; Yang Liu"}, {"title": "Exploring Content Models for Multi-Document Summarization", "journal": "Association for Computational Linguistics", "year": "2009", "authors": "Aria Haghighi; Lucy Vanderwende"}, {"title": "Teaching Machines to Read and Comprehend", "journal": "MIT Press", "year": "2015", "authors": "Karl Moritz Hermann; Tom\u00e1\u0161 Ko\u010disk\u00fd; Edward Grefenstette; Lasse Espeholt; Will Kay; Mustafa Suleyman; Phil Blunsom"}, {"title": "Mohit Bansal, and Ido Dagan. 2021. iFacetSum: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration", "journal": "", "year": "", "authors": "Eran Hirsch; Alon Eirew; Ori Shapira; Avi Caciularu; Arie Cattan; Ori Ernst; Ramakanth Pasunuru; Hadar Ronen"}, {"title": "Linguistic Features -spaCy Usage Documentation", "journal": "", "year": "2021", "authors": "Matthew Honnibal; Ines Montani"}, {"title": "Bidirectional LSTM-CRF Models for Sequence Tagging", "journal": "", "year": "2015", "authors": "Zhiheng Huang; Wei Xu; Kai Yu"}, {"title": "Convolutional Neural Networks for Sentence Classification", "journal": "", "year": "2014", "authors": "Yoon Kim"}, {"title": "CoMSum and SIBERT: A Dataset and Neural Model for Query-Based Multidocument Summarization", "journal": "Springer International Publishing", "year": "2021", "authors": "Sayali Kulkarni; Sheide Chammas; Wan Zhu; Fei Sha; Eugene Ie"}, {"title": "WSL-DS: Weakly Supervised Learning with Distant Supervision for Query Focused Multi-Document Abstractive Summarization", "journal": "", "year": "2020", "authors": "Enamul Md Tahmid Rahman Laskar; Jimmy Xiangji Hoque;  Huang"}, {"title": "Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization", "journal": "", "year": "2018", "authors": "Logan Lebanoff; Kaiqiang Song; Fei Liu"}, {"title": "iNeATS: Interactive Multi-Document Summarization", "journal": "", "year": "2003", "authors": "Anton Leuski; Chin-Yew Lin; Eduard Hovy"}, {"title": "Annual Meeting of the Association for Computational Linguistics", "journal": "Association for Computational Linguistics", "year": "", "authors": ""}, {"title": "UMUX-LITE: When There's No Time for the SUS", "journal": "Association for Computing Machinery", "year": "2013", "authors": "James R Lewis; Brian S Utesch; Deborah E Maher"}, {"title": "BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"title": "Annual Meeting of the Association for Computational Linguistics", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": ""}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"title": "Dmitrijs Milajevs, and Ellen Voorhees. 2017. Overview of the TREC 2017 Real-Time Summarization Track", "journal": "", "year": "", "authors": "Jimmy Lin; Salman Mohammed; Royal Sequiera; Luchen Tan; Nimesh Ghelani; Mustafa Abualsaud; Richard Mccreadie"}, {"title": "Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning", "journal": "", "year": "2020", "authors": "Yuning Mao; Yanru Qu; Yiqing Xie; Xiang Ren; Jiawei Han"}, {"title": "Incremental Update Summarization: Adaptive Sentence Selection Based on Prevalence and Novelty", "journal": "", "year": "2014", "authors": "Richard Mccreadie; Craig Macdonald; Iadh Ounis"}, {"title": "TextRank: Bringing Order into Text", "journal": "", "year": "2004", "authors": "Rada Mihalcea; Paul Tarau"}, {"title": "Conference on Empirical Methods in Natural Language Processing", "journal": "Association for Computational Linguistics", "year": "", "authors": ""}, {"title": "Asynchronous Methods for Deep Reinforcement Learning", "journal": "", "year": "2016", "authors": "Volodymyr Mnih; Adria Puigdomenech Badia; Mehdi Mirza; Alex Graves; Timothy Lillicrap; Tim Harley; David Silver; Koray Kavukcuoglu"}, {"title": "Multi-Reward Reinforced Summarization with Saliency and Entailment", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Ramakanth Pasunuru; Mohit Bansal"}, {"title": "Data Augmentation for Abstractive Query-Focused Multi-Document Summarization", "journal": "", "year": "2021", "authors": "Ramakanth Pasunuru; Asli Celikyilmaz; Michel Galley; Chenyan Xiong; Yizhe Zhang; Mohit Bansal; Jianfeng Gao"}, {"title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Ramakanth Pasunuru; Mengwen Liu; Mohit Bansal; Sujith Ravi; Markus Dreyer"}, {"title": "Sequence Level Training with Recurrent Neural Networks", "journal": "", "year": "2016", "authors": "Aurelio Marc; Sumit Ranzato; Michael Chopra; Wojciech Auli;  Zaremba"}, {"title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "journal": "", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"title": "Ramakanth Pasunuru, Ido Dagan, and Yael Amsterdamer. 2021a. Multi-Document Keyphrase Extraction: A Literature Review and the First Dataset", "journal": "", "year": "", "authors": "Ori Shapira"}, {"title": "Extending Multi-Document Summarization Evaluation to the Interactive Setting", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Ori Shapira; Ramakanth Pasunuru; Hadar Ronen; Mohit Bansal; Yael Amsterdamer; Ido Dagan"}, {"title": "Interactive Abstractive Summarization for Event News Tweets", "journal": "", "year": "2017", "authors": "Ori Shapira; Hadar Ronen; Meni Adler; Yael Amsterdamer; Judit Bar-Ilan; Ido Dagan"}, {"title": "Order Matters: Sequence to sequence for sets", "journal": "", "year": "2016", "authors": "Oriol Vinyals; Samy Bengio; Manjunath Kudlur"}, {"title": "Single Document Keyphrase Extraction Using Neighborhood Knowledge", "journal": "AAAI Press", "year": "2008", "authors": "Xiaojun Wan; Jianguo Xiao"}, {"title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Danqing Wang; Pengfei Liu; Yining Zheng; Xipeng Qiu; Xuanjing Huang"}, {"title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning", "journal": "", "year": "1992", "authors": "J Ronald;  Williams"}, {"title": "PRIMER: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization", "journal": "", "year": "2021", "authors": "Wen Xiao; Iz Beltagy; Giuseppe Carenini; Arman Cohan"}, {"title": "Conditional Self-Attention for Query-based Summarization", "journal": "", "year": "2020", "authors": "Yujia Xie; Tianyi Zhou; Yi Mao; Weizhu Chen"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: The M Summ query-assisted summarization model architecture. Contextual sentence embeddings are concatenated to the current query embedding. The sentence+query representation is softly attended with a transformed query-focused MMR score, and a sentence selection distribution is obtained with a two-hop attention mechanism, considering a summary-so-far representation. A dual-reward mechanism, using the reference summaries and query, optimizes a policy to train the model for summary content quality and sentence-to-query resemblance. At inference time, an initial summary is generated with empty E in and q t -s, while for an expansion they are not empty.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "2567", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: An INTSUMM system using the web application of Shapira et al. (2021b), with our M Summ and M Sugg models run in the backend, on one of the topics in DUC 2006 with 25 news documents about \"Global Warming\". Sub-figure (a) shows the initial summary and the initial list of suggested queries. Sub-figure (b)shows the result of clicking the \"carbon dioxide gas\" suggested query (with the query response and updated suggested queries list). Sub-figure (c) shows the result of subsequently submitting the query \"water level\". Query responses should be informative for the general topic, while also complying to the user queries. System summaries and expansions must be output fast in order to allow smooth interaction and human engagement.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Summ Model ConfigurationSimulation Results ( \u2020 = informativeness metric, R 1 = ROUGE-1)", "figure_data": "# i.Query in Encoding yesQuery in MMR yesQuery in Reward (Dual) yesQuery in MMR at Inference yes\u2020 Initial Summ Norm R F1 1 (\u00d710 \u22123 ) 3.09 (\u00b10.11)Initial Summ Token-Length 86.7 (5.6)\u2020 Expansion Norm R recall 1\u2206 (\u00d710 \u22123 ) 0.913 (\u00b10.055)Expansion Token-Len. 49.7 (2.7)QSIM Query Responsiveness 0.488 (\u00b10.021)ii.yesyesnoyes3.04 (\u00b10.12)87.4 (8.3)0.897 (\u00b10.054)49.4 (2.7)0.482 (\u00b10.022)iii.yesnonoyes3.00 (\u00b10.14)88.0 (8.7)0.892 (\u00b10.058)50.1 (2.9)0.479 (\u00b10.020)iv.noyesyesyes2.98 (\u00b10.17)85.1 (6.5)0.892 (\u00b10.057)51.3 (2.8)0.462 (\u00b10.025)v.nononoyes3.05 (\u00b10.12)85.4 (8.1)0.955 (\u00b10.046)51.8 (2.9)0.423 (\u00b10.027)vi.nononono3.05 (\u00b10.12)85.4 (8.1)0.988 (\u00b10.056)52.8 (4.0)0.311 (\u00b10.023)S 2 Baseline (Shapira et al., 2021b)2.75 (\u00b10.20)85.1 (21.8)0.799 (\u00b10.040)49.1 (2.8)0.601 (\u00b10.021): (1) The"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Simulation results on previously collected sessions, yielding a partial ablation of our M Summ model, and the results on the baseline system which was originally used to collect those sessions. Intervals at 95% confidence.", "figure_data": "MetricOursS 2 BaselineR r 1 AUC @ [106, 250] 43.42 (\u00b11.54) 40.01 (\u00b11.52) R F1 0.256 (\u00b1.011) 0.231 (\u00b10.014) 1 @ initial R F1 0.396 (\u00b1.015) 0.378 (\u00b1.015) 1 @ 250 QSIM query-resp. 0.471 (\u00b1.028) 0.623 (\u00b1.023)Manual query-resp.3.96 (\u00b10.19)4.03 (\u00b10.23)Manual UMUX-Lite78.9 (\u00b12.5)78.6 (\u00b13.4)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ": Average scores of our system and a baseline INTSUMM system on real user sessions, in an experi-ment using a different M Summ configuration (configu-ration i) compared to the experiment of Table 2 (con-figuration v). Our system exposes topical information better, while the overall user experience is not signifi-cantly harmed. Intervals at 95% confidence level."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "are averages with standard deviations. 10 E.g., see https://www.calculator.net/ standard-deviation-calculator.html", "figure_data": ""}], "doi": "10.1007/978-3-319-58475-1_1"}