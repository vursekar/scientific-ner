{"authors": "Sorcha Gilroy; Adam Lopez; Sebastian Maneth", "pub_date": "", "title": "Parsing Graphs with Regular Graph Grammars", "abstract": "Recently, several datasets have become available which represent natural language phenomena as graphs. Hyperedge Replacement Languages (HRL) have been the focus of much attention as a formalism to represent the graphs in these datasets. Chiang et al. (2013) prove that HRL graphs can be parsed in polynomial time with respect to the size of the input graph. We believe that HRL are more expressive than is necessary to represent semantic graphs and we propose the use of Regular Graph Languages (RGL; Courcelle 1991), which is a subfamily of HRL, as a possible alternative. We provide a topdown parsing algorithm for RGL that runs in time linear in the size of the input graph.", "sections": [{"heading": "Introduction", "text": "NLP systems for machine translation, summarization, paraphrasing, and other tasks often fail to preserve the compositional semantics of sentences and documents because they model language as bags of words, or at best syntactic trees. To preserve semantics, they must model semantics. In pursuit of this goal, several datasets have been produced which pair natural language with compositional semantic representations in the form of directed acyclic graphs (DAGs), including the Abstract Meaning Representation Bank (AMR; Banarescu et al. 2013), the Prague Czech-English Dependency Treebank (Haji\u010d et al., 2012), Deepbank (Flickinger et al., 2012), and the Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013). To make use of this data, we require models of graphs.\nConsider how we might use compositional semantic representations in machine translation  (Jones et al., 2012). The edge labels identify 'cat' as the object of the verb 'miss', 'Anna' as the subject of 'miss' and 'Anna' as the possessor of 'cat'. Edges whose head nodes are not attached to any other edge are interpreted as node labels.\n(Figure 1), a two-step process in which semantic analysis is followed by generation. Jones et al. (2012) observe that this decomposition can be modeled with a pair of synchronous grammars, each defining a relation between strings and graphs. Necessarily, one projection of this synchronous grammar produces strings, while the other produces graphs, i.e., is a graph grammar.\nA consequence of this representation is that the complete translation process can be realized by parsing: to analyze a sentence, we parse the input string with the string-generating projection of the synchronous grammar, and read off the synchronous graph from the resulting parse. To generate a sentence, we parse the graph, and read off the synchronous string from the resulting parse. In this paper, we focus on the latter problem: using graph grammars to parse input graphs. We call this graph recognition to avoid confusion with other parsing problems.\nRecent work in NLP has focused primarily on hyperedge replacement grammar (HRG; Drewes et al. 1997), a context-free graph grammar formalism that has been studied in an NLP context by several researchers (Chiang et al., 2013;Peng et al., 2015;Bauer and Rambow, 2016). In particular, Chiang et al. (2013) propose that HRG could be used to represent semantic graphs, and precisely characterize the complexity of a CKY-style algorithm for graph recognition from Lautemann (1990) to be polynomial in the size of the input graph. HRGs are very expressive-they can generate graphs that simulate non-context-free string languages (Engelfriet and Heyker, 1991;Bauer and Rambow, 2016). This means they are likely more expressive than we need to represent the linguistic phenomena that appear in existing semantic datasets. In this paper, we propose the use of Regular Graph Grammars (RGG; Courcelle 1991) a subfamily of HRG that, like its regular counterparts among string and tree languages, is less expressive than context-free grammars but may admit more practical algorithms. By analogy to Chiang's CKY-style algorithm for HRG. We develop an Earley-style recognition algorithm for RGLs that is linear in the size of the input graph.", "n_publication_ref": 14, "n_figure_ref": 1}, {"heading": "Regular Graph Languages", "text": "We use the following notation. If n is an integer, [n] denotes the set {1, . . . , n}. Let \u0393 be an alphabet, i.e., a finite set. Then s \u2208 \u0393 * denotes that s is a sequence of arbitrary length, each element of which is in \u0393. We denote by |s| the length of s. A ranked alphabet is an alphabet \u0393 paired with an arity mapping (i.e., a total function) rank: \u0393 \u2192 N.\nDefinition 1. A hypergraph (or simply graph) over a ranked alphabet \u0393 is a tuple G = (V G , E G , att G , lab G , ext G ) where V G is a finite set of nodes; E G is a finite set of edges (distinct from V G ); att G : E G \u2192 V *\nG maps each edge to a sequence of nodes; lab G : E G \u2192 \u0393 maps each edge to a label such that |att G (e)| = rank(lab G (e)); and ext G is an ordered subset of V G called the external nodes of G.\nWe assume that the elements of ext G are pairwise distinct, and the elements of att G (e) for each edge e are also pairwise distinct. An edge e is attached to its nodes by tentacles, each labeled by an integer indicating the node's position in att G (e) = (v 1 , . . . , v k ). The tentacle from e to v i will have label i, so the tentacle labels lie in the set [k] where k = rank(e). To express that a node v is attached to the ith tentacle of an edge e, we say vert(e, i) = v. Likewise, the nodes in ext G are labeled by their position in ext G . We refer to the ith external node of G by ext G (i) and in figures this will be labeled (i). The rank of an edge e is k if att(e) = (v 1 , . . . , v k ) (or equivalently, rank(lab(e)) = k). The rank of a hypergraph G, denoted by rank(G) is the size of ext G .\nExample 1. Hypergraph G in Figure 2 has four nodes (shown as black dots) and three hyperedges labeled a, b, and X (shown boxed). The bracketed numbers (1) and (2) denote its external nodes and the numbers between edges and the nodes are tentacle labels. Call the top node v 1 and, proceeding clockwise, call the other nodes v 2 , v 3 , and v 4 . Call its edges e 1 , e 2 and e 3 . Its definition would state att G (e 1 ) = (v 1 , v 2 ), att G (e 2 ) = (v 2 , v 3 ), att G (e 3 ) = (v 1 , v 4 , v 3 ), lab G (e 1 ) = a, lab G (e 2 ) = b, lab G (e 3 ) = X, and ext G = (v 4 , v 2 ). Definition 2. Let G be a hypergraph containing an edge e with att G (e) = (v 1 , . . . , v k ) and let H be a hypergraph of rank k with node and edge sets disjoint from those of G. The replacement of e by H is the graph\nG = G[e/H]. Its node set V G is V \u222a V H where V = V G \u2212 {v 1 , . . . , v k }. Its edge set is E G = (E G \u2212 {e}) \u222a E H . We define att G = att \u222a att H where for every e \u2208 (E G \u2212 {e}), att(e) is obtained from att G (e ) by replacing v i by the ith external node of H. Let lab G = lab \u222a lab H where lab is the restriction of lab G to edges in E G \u2212 {e}. Finally, let ext G = ext G .\nExample 2. A replacement is shown in Figure 2.", "n_publication_ref": 1, "n_figure_ref": 2}, {"heading": "Hyperedge Replacement Grammars", "text": "Definition 3. A hyperedge replacement grammar G = (N G , T G , P G , S G ) consists of ranked (disjoint) alphabets N G and T G of nonterminal and terminal symbols, respectively, a finite set P G of productions, and a start symbol S G \u2208 N G . Every production in P G is of the form X \u2192 G where G is a hypergraph over N G \u222a T G and rank(G) = rank(X).\nFor each production p : X \u2192 G, we use L(p) to refer to X (the left-hand side of p) and R(p) to refer to G (the right-hand side of p). An edge is a terminal edge if its label is terminal and a nonterminal edge if its label is nonterminal. A graph is a terminal graph if all of its edges are terminal. The terminal subgraph of a graph is the subgraph consisting of all terminal edges and their incident nodes.\nGiven a HRG G, we say that graph G immediately derives graph G , denoted G \u2192 G , iff there is an edge e \u2208 E G and a nonterminal X \u2208 N G such that lab G (e) = X and G = G[e/H], where X \u2192 H is in P G . We extend the idea of immediate derivation to its transitive closure G \u2192 * G , and say here that G derives G . For every X \u2208 N G we also use X to de- \nX a b (1) (2) G (2) c (1) a (3) d H (1) c d a (2) b a G[e/H]", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "S p : X", "text": "(1)\n1 go 1 2 I arg0 Y Z s : (1) (2) 1 2 1 arg0 arg1 X q : W Y (2) (1) 1 2 1 1 2 arg1 arg0 W t : (1) 1 want Y r : Z X (2) (1) 1 2 1 1 2 arg1 arg0 Z u : (1) 1 need\nTable 1: Productions of a HRG. The labels p, q, r, s, t, and u label the productions so that we can refer to them in the text. Note that Y can rewrite in two ways, either via production r or s.\nnote the graph consisting of a single edge e with lab(e) = X and nodes (v 1 , . . . , v rank(X) ) such that att G (e) = (v 1 , . . . , v rank(X) ), and we define the language L X (G) as\n{G | X \u2192 * G \u2227 G is terminal}. The language of G is L(G) = L S G (G).\nWe call the family of languages that can be produced by any HRG the hyperedge replacement languages (HRL).\nWe assume that terminal edges are always of rank 2, and depict them as directed edges where the direction is determined by the tentacle labels: the tentacle labeled 1 attaches to the source of the edge and the tentacle labeled 2 attaches to the target of the edge.\nExample 3. Table 1 shows a HRG deriving AMR graphs for sentences of the form 'I need to want to need to want to ... to want to go'. Figure 3 is a graph derived by the grammar. The grammar is somewhat unnatural, a point we will return to ( \u00a74).\nWe can use HRGs to generate chain graphs (strings) by restricting the form of the productions in the grammars. Figure 4 shows a HRG that produces the context-free string language a n b n . HRGs can simulate the class of mildly context-sensitive languages that is characterized, e.g., by linear context-free rewriting systems (LCFRS; Vijay-Shanker et al. 1987), where the fan-out of the LCFRS will influence the maximum rank of nonterminal required in the HRG, see (Engelfriet and Heyker, 1991).", "n_publication_ref": 2, "n_figure_ref": 2}, {"heading": "Regular Graph Grammars", "text": "A regular graph grammar (RGG; Courcelle 1991) is a restricted form of HRG. To explain the restrictions, we first require some definitions.\nDefinition 4. Given a graph G, a path in G from a node v to a node v is a sequence\n(v 0 , i 1 , e 1 , j 1 , v 1 )(v 1 , i 2 , e 2 , j 2 , v 2 ) . . . (v k\u22121 , i k , e k , j k , v k ) (1) such that v 0 = v, v k = v , and for each r \u2208 [k],\nvert(e r , i r ) = v r\u22121 and vert(e r , j r ) = v r . The length of this path is k.\nA path is terminal if every edge in the path has a terminal label. A path is internal if each v i is internal for 1 \u2264 i \u2264 k \u2212 1. Note that the endpoints v 0 and v k of an internal path can be external. Definition 5. A HRG G is a Regular Graph Grammar (or simply RGG) if each nonterminal in N G has rank at least one and for each p \u2208 P G the following hold: (C1) R(p) has at least one edge. Either it is a single terminal edge, all nodes of which are external, or each of its edges has at least one internal node.\n(C2) Every pair of nodes in R(p) is connected by a terminal and internal path.\nExample 4. The grammar in Table 1 is an RGG. Although HRGs can produce context-free languages (and beyond) as shown in Figure 4, the only string languages RGGs can produce are the regular string languages. See Figure 5 for an example of a string generating RGG. Similarly, RGGs can produce regular tree languages, but not context-free tree languages. Figure 6 shows a tree generating RGG that generates binary trees the internal nodes of which are represented by a-labeled edges, and the leaves of which are represented by b-labeled edges. Note that these two results of regularity of the string-and tree-languages generated by RGG follow from the fact that graph languages produced by RGG are MSO-definable (Courcelle, 1991), and the well-known facts that the regular string and graph languages are MSO-definable.\nX (1) a Y (1) b 1 1\nFigure 5: A RGG for a regular string language.", "n_publication_ref": 1, "n_figure_ref": 4}, {"heading": "X", "text": "(1)\nY Z We call the family of languages generated by RGGs the regular graph languages (RGLs).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "RGL Recognition", "text": "To recognize RGG, we exploit the property that every nonterminal including the start symbol has rank at least one (Definition 5), and we assume that the corresponding external node is identified in the input graph. This mild assumption may be reasonable for applications like AMR parsing, where grammars could be designed so that the external node is always the unique root. Later we relax this assumption.\nThe availability of an identifiable external node suggests a top-down algorithm, and we take in-spiration from a top-down recognition algorithm for the predictive top-down parsable grammars, another subclass of HRG (Drewes et al., 2015). These grammars, the graph equivalent of LL(1) string grammars, are incomparable to RGG, but the algorithms are related in their use of top-down prediction and in that they both fix an order of the edges in the right-hand side of each production.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Top-Down Recognition for RGLs", "text": "Just as the algorithm of Chiang et al. (2013) generalizes CKY to HRG, our algorithm generalizes Earley's algorithm (Earley, 1970). Both algorithms operate by recognizing incrementally larger subgraphs of the input graph, using a succinct representation for subgraphs that depends on an arbitrarily chosen marker node m of the input graph. For each production p of the grammar, we impose a fixed order on the edges of R(p), as in Drewes et al. (2015). We discuss this order in detail in \u00a73.2. As in Earley's algorithm, we use dotted rules to represent partial recognition of productions: X \u2192\u0113 1 . . .\u0113 i\u22121 \u2022\u0113 i . . .\u0113 n means that we have identified the edges\u0113 1 to\u0113 i\u22121 and that we must next recognize edge\u0113 i . We write\u0113 and v for edges and nodes in productions and e and v for edges and nodes in a derived graph. When the identity of the sequence is immaterial we abbreviate it as \u03b1, for example writing X \u2192 \u2022 \u03b1.\nWe present our recognizer as a deductive proof system (Shieber et al., 1995). The items of the recognizer are of the form\nName Rule Conditions PREDICT\n[b(I), p : X \u2192\u01131 . . . \u2022\u0113 i . . .\u0113n, \u03c6p][q : Y \u2192 \u03b1] [\u03c6p(\u0113i), q : Y \u2192 \u2022 \u03b1, \u03c6 0 q [ext R(q) = \u03c6p(\u0113i)]] lab(\u0113i) = Y SCAN [b(I), X \u2192\u01131 . . . \u2022\u0113 i . . .\u0113n, \u03c6p][e = edg lab(\u0113 i ) (v1, . . . , vm)] [b(I \u222a {e}), X \u2192\u01131 . . . \u2022\u0113 i+1 . . .\u0113n, \u03c6p[att(\u0113i) = (v1, . . . , vm)]] \u03c6p(\u0113i)(j) \u2208 VG \u21d2 \u03c6p(\u0113i)(j) = vert(e, j) COMPLETE [b(I), p : X \u2192\u01131 . . . \u2022\u0113 i . . .\u0113n, \u03c6p][b(J), q : Y \u2192 \u03b1 \u2022 , \u03c6q] [b(I \u222a J), X \u2192\u01131 . . . \u2022\u0113 i+1 . . .\u0113n, \u03c6p[att(\u0113i) = \u03c6p(ext R(q) )]] \u03c6p(\u0113i)(j) \u2208 VG \u21d2 \u03c6p(\u0113i)(j) = \u03c6q(ext R(q) )(j), lab(\u0113i) = Y, EI \u2229 EJ = \u2205\n[b(I), p : X \u2192\u0113 1 . . . \u2022\u0113 i . . .\u0113 n , \u03c6 p ]\nwhere I is a subgraph that has been recognized as matching\u0113 1 , . . . ,\u0113 i\u22121 ; p : X \u2192\u0113 1 , . . . ,\u0113 n is a production in the grammar with the edges in order; and \u03c6 p : E R(p) \u2192 V * G maps the endpoints of edges in R(p) to nodes in G.\nFor each production p, we number the nodes in some arbitrary but fixed order. Using this, we construct the function \u03c6 0\np : E R(p) \u2192 V * R(p) such that for\u0113 \u2208 E R(p) if att(\u0113) = (v 1 ,v 2 ) then \u03c6 0 p (\u0113) = (v 1 ,v 2 ).\nAs we match edges in the graph with edges in p, we assign the nodesv to nodes in the graph. For example, if we have an edge\u0113 in a production p such that att(\u0113) = (v 1 ,v 2 ) and we find an edge e which matches\u0113, then we update \u03c6 p to record this fact, written \u03c6 p [att(\u0113) = att(e)]. We also use \u03c6 p to record assignments of external nodes. If we assign the ith external node to v, we write \u03c6 p [ext p (i) = v]. We write \u03c6 0 p to represent a mapping with no grounded nodes.\nSince our algorithm makes top-down predictions based on known external nodes, our boundary representation must cover the case where a subgraph is empty except for these nodes. If at some point we know that our subgraph has external nodes \u03c6(\u0113), then we use the shorthand \u03c6(\u0113) rather than the full boundary representation \u03c6(\u0113), \u2205, m \u2208 \u03c6(\u0113) .\nTo keep notation uniform, we use dummy nonterminal S * \u2208 N G that derives S G via the production p 0 . For graph G, our system includes the axiom:\n[ext G , p 0 : S * \u2192 \u2022 S G , \u03c6 0 p 0 [ext R(p 0 ) = ext G ]].\nOur goal is to prove:\n[b(G), p S : S * \u2192 S G \u2022 , \u03c6 p S ]\nwhere \u03c6 p S has a single edge\u0113 in its domain which has label S G in R(p S ) and \u03c6 p S (\u0113) = ext G .\nAs in Earley's algorithm, we have three inference rules: PREDICT, SCAN and COMPLETE (Table 2). PREDICT is applied when the edge after the dot is nonterminal, assigning any external nodes that have been identified. SCAN is applied when the edge after the dot is terminal. Using \u03c6 p , we may already know where some of the endpoints of the edge should be, so it requires the endpoints of the scanned edge to match. COMPLETE requires that each of the nodes of\u0113 i in R(p) have been identified, these nodes match up with the corresponding external nodes of the subgraph J, and that the subgraphs I and J are edge-disjoint.\nWe provide a high-level proof that the recognizer is sound and complete. Proposition 1. Let G be a HRG and G a graph.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Then the goal", "text": "[b(G), p S : S * \u2192 S G \u2022 , \u03c6 p S ] can be proved from the axiom [ext G , p S : S * \u2192 \u2022 S G , \u03c6 p S [ext R(p S ) = ext G ]] if and only if G \u2208 L(G). Proof. We prove that for each X \u2208 N G , [b(G), p X : X * \u2192 X \u2022 , \u03c6 p X ] can be proved from [ext G , p X : X * \u2192 \u2022 X, \u03c6 p X [ext R(p X ) = ext G ]] if and only if G \u2208 L X (G)\nwhere the dummy nonterminal X * was added to the set of nonterminals and p X : X * \u2192 X was added to the set of productions. We prove this by induction on the number of edges in G.\nWe assume that each production in the grammar contains at least one terminal edge. If the HRG is not in this form, it can be converted into this form and in the case of RGGs they are already in this form by definition.\nBase Case: Let G consist of a single edge.\nIf: Assume G \u2208 L X (G). Since G consists of one edge, there must be a production q : X \u2192 G. Apply PREDICT to the axiom and p X : X * \u2192 X to obtain the item [\u03c6 p X (X), q :\nX \u2192 \u2022 G, \u03c6 0 q [ext G = \u03c6 p X (X)]]\n. Apply SCAN to the single terminal edge that makes up G to obtain [b(G), q : X \u2192 G \u2022 , \u03c6 q ] and finally apply COMPLETE to this and the axiom reach the goal\n[b(G), p X : X * \u2192 X, \u03c6 p X ].\nOnly if: Assume the goal can be reached from the axiom and G = e. Then the item [b(e), q : X \u2192 e, \u03c6 q ] must have been reached at some point for some q \u2208 P G . Therefore q : X \u2192 e is a production and so e = G \u2208 L X (G).\nAssumption: Assume that the proposition holds when G has fewer than k edges.\nInductive\nStep: Assume G has k edges.\nIf: Assume G \u2208 L X (G), then there is a production q : X \u2192 H where H has nonterminals Y 1 , . . . , Y n and there are graphs\nH 1 , . . . , H n such that G = H[Y 1 /H 1 ] . . . [Y n /H n ]. Each graph H i for i \u2208 [n]\nhas fewer than k edges and so we apply the inductive hypothesis to show that we can prove the items\n[b(H i ), r i : Y i \u2192 J i , \u03c6 r i ] for each i \u2208 [n]\n. By applying COMPLETE to each such item and applying SCAN to each terminal edge of H we reach the goal [b(G), p X :\nX * \u2192 X \u2022 , \u03c6 p X ].\nOnly If: Assume the goal can be proved from the axiom. Then we must have at some point reached an item of the form [b(G), q : X \u2192 H, \u03c6 q ] and that H has nonterminals Y 1 , . . . , Y n . This means that there are graphs\nH 1 , . . . , H n such that [b(H i ), p Y i : Y * i \u2192 Y i , \u03c6 p Y i ] for each i \u2208 [n] and G = H[Y 1 /H 1 ] . . . [Y n /H n ].\nSince each H i has fewer than k edges, we apply the inductive hypothesis to get that\nH i \u2208 L Y i (G) for each i \u2208 [n] and therefore G \u2208 L X (G).\nExample 5. Using the RGG in Table 1, we show how to recognize the graph in Figure 7, which can be derived by applying production s followed by production u, where the external nodes of Y are (v 3 , v 2 ). Assume the ordering of the edges in production s is arg1, arg0, Z; the top node isv 1 ; the bottom node isv 2 ; and the node on the right isv 3 ; and that the marker node is not in this subgraphwe elide reference to it for simplicity. Letv 4 be the top node of R(u) andv 5 be the bottom node of R(u). The external nodes of Y are determined top-down, so the recognize of this subgraph is triggered by this item:\n[{v 3 , v 2 }, Y \u2192 \u2022 arg1 arg0 Z, \u03c6 0 s [ext R(s) = (v 3 , v 2 )]] (2)\nwhere \u03c6 s (arg1) = (v 1 , v 3 ), \u03c6 s (arg0) = (v 1 , v 2 ), and \u03c6 s (Z) = (v 1 ).\nTable 3 shows how we can prove the item\n[ {v 3 , v 2 }, {e 3 , e 2 } , Y \u2192 arg1arg0Z \u2022 , \u03c6]\nThe boundary representation {v 3 , v 2 }, {e 3 , e 2 } in this item represents the whole subgraph shown in Figure 7. \nv 1 v 4 v 2 v 3 . . . . . . need (e1) arg0 (e2) arg1 (e3)", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Normal Ordering", "text": "Our algorithm requires a fixed ordering of the edges in the right-hand sides of each production. We will constrain this ordering to exploit the structure of RGG productions, allowing us to bound recognition complexity. If s =\u0113 1 . . .\u0113 n is an order, define s i:j =\u0113 i . . .\u0113 j . Definition 7. Let s =\u0113 1 , . . . ,\u0113 n be an edge order of a right-hand side of a production. Then s is normal if it has the following properties:\n1.\u0113 1 is connected to an external node, 2. s 1:j is a connected graph for all j \u2208 [n] 3. if\u0113 i is nonterminal, each endpoint of\u0113 i must be incident with some terminal edge\u0113 j for which j < i. Example 6. The ordering of the edges of production s in Example 5 is normal.\nArbitrary HRGs do not necessarily admit a normal ordering. For example, the graph in Figure 8 cannot satisfy Properties 2 and 3 simultaneously. However, RGGs do admit a normal ordering. \nCurrent Item Reason\n1. [{v3, v2}, Y \u2192 \u2022 arg1arg0Z, \u03c6 0 s [ext R(s) = (v3, v2)]] Equation 2 2. [ {v3, v2, v1}, {e3} , Y \u2192 arg1 \u2022 arg0Z, \u03c6s[att(arg1) = (v1, v3\n)]] SCAN: 1. and e3 = edg arg1 (v1, v3) 3. [ {v3, v2, v1}, {e3, e2} , Y \u2192 arg1arg0 \u2022 Z, \u03c6s[att(arg0) = (v1, v2)]] SCAN: 2. and e2 = edg arg0 (v1, v2)] 4.\n[(v1), Z \u2192 \u2022 need, \u03c6 0 u [ext R(u) = (v1)]] PREDICT: 3. and Z \u2192 need 5. [ {v1, v4}, {e1} , Z \u2192 need \u2022 , \u03c6u[att(need) = (v1, v4)]]\nSCAN: 4. and e1 = edg need (v1, v4)\n6. [ {v3, v2}, {e3, e2} , Y \u2192 arg1arg0Z \u2022 , \u03c6s[att(Z) = (v1)]]\nCOMPLETE: 3. and 5.\nTable 3: The steps of recognizing that the subgraph shown in Figure 7 is derived from productions r2 and u in the grammar in Table 1.\nProposition 2. If G is an RGG, for every p \u2208 P G , there is a normal ordering of the edges in R(p).\nProof. If R(p) contains a single node then it must be an external node and it must have a terminal edge attached to it since R(p) must contain at least one terminal edge. If R(p) contains multiple nodes then by C2 there must be terminal internal paths between all of them, so there must be a terminal edge attached to the external node, which we use to satisfy Property 1. To produce a normal ordering, we next select terminal edges once one of their endpoints is connected to an ordered edge, and nonterminal edges once all endpoints are connected to ordered edges, possible by C2. Therefore, Properties 2 and 3 are satisfied.\nA normal ordering tightly constrains the recognition of edges. Property 3 ensures that when we apply PREDICT, the external nodes of the predicted edge are all bound to specific nodes in the graph. Properties 1 and 2 ensure that when we apply SCAN, at least one endpoint of the edge is bound (fixed).", "n_publication_ref": 1, "n_figure_ref": 2}, {"heading": "Recognition Complexity", "text": "Assume a normally-ordered RGG. Let the maximum number of edges in the right-hand side of any production be m; the maximum number of nodes in any right-hand side of a production k; the maximum degree of any node in the input graph d; and the number of nodes in the input graph n.\nAs previously mentioned, Drewes et al. (2015) also propose a HRG recognizer which can recognize a subclass of HRG (incomparable to RGG) called the predictive top-down parsable grammars. Their recognizer in this case runs in O(n 2 ) time. A well-known bottom-up recognizing algorithm for HRG was first proposed by Lautemann (1990).\nIn this paper, the recognizer is shown to be polynomial in the size of the input graph. Later, Chiang et al. (2013) formulate the same algorithm more precisely and show that the recognizing complexity is O((3 d \u00d7 n) k+1 ) where k in their case is the treewidth of the grammar. Remark 1. The maximum number of nodes in any right-hand side of a production (k) is also the maximum number of boundary nodes for any subgraph in the recognizer.\nCOMPLETE combines subgraphs I and J only when the entire subgraph derived from Y has been recognized. Boundary nodes of J are also boundary nodes of I because they are nodes in the terminal subgraph of R(p) where Y connects. The boundary nodes of I \u222a J are also bounded by k since form a subset of the boundary nodes of I. Remark 2. Given a boundary node, there are at most (d m ) k\u22121 ways of identifying the remaining boundary nodes of a subgraph that is isomorphic to the terminal subgraph of the right-hand side of a production.\nThe terminal subgraph of each production is connected by C2, with a maximum path length of m. For each edge in the path, there are at most d subsequent edges. Hence for the k \u2212 1 remaining boundary nodes there are (d m ) k\u22121 ways of choosing them.\nWe count instantiations of COMPLETE for an upper bound on complexity (McAllester, 2002), using similar logic to (Chiang et al., 2013). The number of boundary nodes of I, J and I \u222a J is at most k. Therefore, if we choose an arbitrary node to be some boundary node of I \u222a J, there are at most (d m ) k\u22121 ways of choosing its remaining boundary nodes. For each of these nodes, there are at most (3 d ) k states of their attached boundary edges: in I, in J, or in neither. The total number of instantiations is O(n(d m ) k\u22121 (3 d ) k ), linear in the number of input nodes and exponential in the degree of the input graph. Note that in the case of the AMR dataset (Banarescu et al. 2013), the maximum node degree is 17 and the average is 2.12.\nWe observe that RGGs could be relaxed to produce graphs with no external nodes by adding a dummy nonterminal S with rank 0 and a single production S \u2192 S. To adapt the recognition algorithm, we would first need to guess where the graph starts. This would add a factor of n to the complexity as the graph could start at any node.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Discussion and Conclusions", "text": "We have presented RGG as a formalism that could be useful for semantic representations and we have provided a top-down recognition algorithm for them. The constraints of RGG enable more efficient recognition than general HRG, and this tradeoff is reasonable since HRG is very expressive-when generating strings, it can express non-context-free languages (Engelfriet and Heyker, 1991;Bauer and Rambow, 2016), far more power than needed to express semantic graphs. On the other hand, RGG is so constrained that it may not be expressive enough: it would be more natural to derive the graph in Figure 4 from outermost to innermost predicate; but constraint C2 makes it difficult to express this, and the grammar in Table 1   A possible alternative would be to consider Restricted DAG Grammars (RDG; Bj\u00f6rklund et al. 2016). Parsing for a fixed such grammar can be achieved in quadratic time with respect to the input graph. It is known that for a fixed HRG generating k-connected hypergraphs consisting of hyperedges of rank k only, parsing can be carried out in cubic time (k-HRG; (Drewes, 1993)).\nMore general than RDGs a is the class of graph languages recognized by DAG automata (DA-GAL; Blum and Drewes 2016), for which the deterministic variant provides polynomial time parsing. Note that RGGs can generate graph languages of unbounded node degree. With respect to expressive power, RDGs and k-HRGs are incomparable to RGGs. Figure 9 shows the relationships between the context-free and regular languages for strings, trees and graphs. Monadic-second order logic (MSOL; Courcelle and Engelfriet 2011) is a form of logic which when restricted to strings gives us exactly the regular string languages and when restricted to trees gives us exactly the regular tree languages. RGLs lie in the intersection of HRG and MSOL on graphs but they do not make up this entire intersection. Courcelle (1991) defined (non-constructively) this intersection to be the strongly context-free languages (SCFL). We believe that there may be other formalisms that are subfamilies of SCFL which may be useful for semantic representations. All inclusions shown in Figure 9 are strict. For instance, RGL cannot produce \"star graphs\" (one node that has edges to n other nodes), while DAGAL and HRL can produce such graphs. It is well-known that HRL and MSOL are incomparable. There is a language in RGL that is not in DAGAL, for instance, \"ladders\" (two string graphs of n nodes each, with an edge between the ith node of each string).\nAnother alternative formalism to RGG that is defined as a restriction of HRG are Tree-like Grammars (TLG; Matheja et al. 2015). They define a subclass of SCFL, i.e., they are MSO definable. TLGs have been considered for program verification, where closure under intersection of the formalism is essential. Note that RGGs are also closed under intersection. While TLG and RDG are both incomparable to RGG, they share important characteristics, including the fact that the terminal subgraph of every production is connected. This means that our top-down recognition algorithm is applicable to both. In the future we would like to investigate larger, less restrictive (and more linguistically expressive) subfamilies of SCFL. We plan to implement and evaluate our algorithm experimentally.", "n_publication_ref": 6, "n_figure_ref": 3}, {"heading": "Acknowledgments", "text": "This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh; and in part by a Google faculty research award (to AL). We thank Clara Vania, Sameer Bansal, Ida Szubert, Federico Fancellu, Antonis Anastasopoulos, Marco Damonte, and the anonymous reviews for helpful discussion of this work and comments on previous drafts of the paper.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Universal conceptual cognitive annotation (ucca)", "journal": "", "year": "2013", "authors": "Omri Abend; Ari Rappoport"}, {"title": "Abstract meaning representation for sembanking", "journal": "", "year": "2013", "authors": "Laura Banarescu; Claire Bonial; Shu Cai; Madalina Georgescu; Kira Griffitt; Ulf Hermjakob; Kevin Knight; Philipp Koehn; Martha Palmer; Nathan Schneider"}, {"title": "Hyperedge replacement and nonprojective dependency structures", "journal": "", "year": "2016-06-29", "authors": "Daniel Bauer; Owen Rambow"}, {"title": "Between a Rock and a Hard Place -Uniform Parsing for Hyperedge Replacement DAG Grammars", "journal": "Springer International Publishing", "year": "2016", "authors": "Henrik Bj\u00f6rklund; Frank Drewes; Petter Ericson"}, {"title": "Properties of regular DAG languages", "journal": "", "year": "2016-03-14", "authors": "Johannes Blum; Frank Drewes"}, {"title": "Parsing graphs with hyperedge replacement grammars", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "David Chiang; Jacob Andreas; Daniel Bauer; Karl Moritz Hermann; Bevan Jones; Kevin Knight"}, {"title": "The monadic second-order logic of graphs V: on closing the gap between definability and recognizability", "journal": "Theor. Comput. Sci", "year": "1991", "authors": "Bruno Courcelle"}, {"title": "Graph Structure and Monadic Second-Order Logic, a Language Theoretic Approach", "journal": "Cambridge University Press", "year": "2011", "authors": "Bruno Courcelle; Joost Engelfriet"}, {"title": "Np-completeness of kconnected hyperedge-replacement languages of order k", "journal": "Inf. Process. Lett", "year": "1993", "authors": "Frank Drewes"}, {"title": "", "journal": "", "year": "", "authors": ""}, {"title": "Predictive Top-Down Parsing for Hyperedge Replacement Grammars", "journal": "Springer International Publishing", "year": "2015", "authors": "Frank Drewes; Berthold Hoffmann; Mark Minas"}, {"title": "Handbook of Graph Grammars and Computing by Graph Transformation", "journal": "World Scientific", "year": "1997", "authors": "Frank Drewes; Hans-J\u00f6rg Kreowski; Annegret Habel"}, {"title": "An efficient context-free parsing algorithm", "journal": "ACM", "year": "1970", "authors": "Jay Earley"}, {"title": "", "journal": "", "year": "", "authors": ""}, {"title": "The string generating power of context-free hypergraph grammars", "journal": "Journal of Computer and System Sciences", "year": "1991", "authors": "Joost Engelfriet; Linda Heyker"}, {"title": "Deepbank : a dynamically annotated treebank of the Wall Street Journal", "journal": "", "year": "2012", "authors": "Dan Flickinger; Yi Zhang; Valia Kordoni"}, {"title": "Announcing prague czech-english dependency treebank 2.0", "journal": "", "year": "2012", "authors": "Jan Haji\u010d; Eva Haji\u010dov\u00e1; Jarmila Panevov; Petr Sgall; Ond\u0159ej Bojar; Silvie Cinkov\u00e1; Eva Fu\u010d\u00edkov\u00e1; Marie Mikulov\u00e1; Petr Pajas; Jan Popelka; Ji\u0159\u00ed Semeck\u00fd;  Jana\u0161indlerov\u00e1; Josef Jan\u0161t\u011bp\u00e1nek; Zde\u0148ka Toman; Zden\u011bk\u017eabokrtsk\u00fd Ure\u0161ov\u00e1"}, {"title": "Semanticsbased machine translation with hyperedge replacement grammars", "journal": "", "year": "2012", "authors": "Bevan Jones; Jacob Andreas; Daniel Bauer; Karl Mortiz Hermann; Kevin Knight"}, {"title": "The complexity of graph languages generated by hyperedge replacement", "journal": "", "year": "1990", "authors": "Clemens Lautemann"}, {"title": "", "journal": "Acta Informatica", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": ""}, {"title": "Tree-Like Grammars and Separation Logic", "journal": "Springer International Publishing", "year": "2015", "authors": "Christoph Matheja; Christina Jansen; Thomas Noll"}, {"title": "On the complexity analysis of static analyses", "journal": "J. ACM", "year": "2002", "authors": "David Mcallester"}, {"title": "A synchronous hyperedge replacement grammar based approach for AMR parsing", "journal": "", "year": "2015-07-30", "authors": "Xiaochang Peng; Linfeng Song; Daniel Gildea"}, {"title": "Principles and implementation of deductive parsing", "journal": "Journal of Logic Programming", "year": "1995", "authors": "M Stuart; Yves Shieber; Fernando C N Schabes;  Pereira"}, {"title": "Characterizing structural descriptions produced by various grammatical formalisms", "journal": "", "year": "1987", "authors": "K Vijay-Shanker; David J Weir; Aravind K Joshi"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Semantic machine translation using AMR(Jones et al., 2012). The edge labels identify 'cat' as the object of the verb 'miss', 'Anna' as the subject of 'miss' and 'Anna' as the possessor of 'cat'. Edges whose head nodes are not attached to any other edge are interpreted as node labels.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: The replacement of the X-labeled edge e in G by the graph H.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure3: Graph derived by grammar in Table1.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "bFigure 6: A RGG for a regular tree language.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Definition 6. (Chiang et al. 2013; Definition 6) Let I be a subgraph of a graph G. A boundary node of I is a node which is either an endpoint of an edge in G\\I or an external node of G. A boundary edge of I is an edge in I which has a boundary node as an endpoint. The boundary representation of I is the tuple b(I) = bn(I), be(I), m \u2208 I where 1. bn(I) is the set of boundary nodes of I 2. be(I) is the set of boundary edges of I 3. (m \u2208 I) is a flag indicating whether the marker node is in I. Chiang et al. (2013) prove each subgraph has a unique boundary representation, and give algorithms that use only boundary representations to compute the union of two subgraphs, requiring time linear in the number of boundary nodes; and to check disjointness of subgraphs, requiring time linear in the number of boundary edges.", "figure_data": ""}, {"figure_label": "7", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: Top left subgraph of Figure 3. To refer to nodes and edges in the text, they are labeled v1, v2, v3, e1, e2, and e3.", "figure_data": ""}, {"figure_label": "8", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: This graph cannot be normally ordered.", "figure_data": ""}, {"figure_label": "9", "figure_type": "", "figure_id": "fig_7", "figure_caption": "Figure 9 :9Figure 9: A Hasse diagram of various string, tree and graph language families. An arrow from family A to family B indicates that family A is a subfamily of family B.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The inference rules for the top-down recognizer.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "does not. Perhaps we need less expressivity than HRG but more than RGG.", "figure_data": "HRLMSOLGraphsRGL DAGALCFTLRTLTreesCFL  *RLStrings"}], "doi": "10.1007/978-3-319-30000-940"}