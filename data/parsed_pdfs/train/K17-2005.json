{"authors": "Chunting Zhou; Graham Neubig", "pub_date": "", "title": "Morphological Inflection Generation with Multi-space Variational Encoder-Decoders", "abstract": "This paper describes the CMU submission to shared task 1 of SIGMORPHON 2017. The system is based on the multi-space variational encoder-decoder (MSVED) method of Zhou and Neubig (2017), which employs both continuous and discrete latent variables for the variational encoder-decoder and is trained in a semi-supervised fashion. We discuss some language-specific errors and present result analysis.", "sections": [{"heading": "Introduction", "text": "In morphologically rich languages, different affixes (i.e. prefixes, infixes, suffixes) can be combined with the lemma to reflect various syntactic and semantic features of a word. In many areas of natural language processing (NLP) it is important that systems are able to correctly analyze and generate different morphological forms, including previously unseen forms. The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation (Chahuneau et al., 2013) and information retrieval (Darwish and Oard, 2007). Accordingly, learning morphological reinflection patterns from labeled data is an important challenge.\nThe Universal Morphological Reinflection task at SIGMORPHON 2017 (Cotterell and Sch\u00fctze, 2017) is an evaluation campaign aimed at systems that tackle the task of morphological inflection. It extends the SIGMORPHON 2016 Morphological Reinflection by conducting tasks in 52 languages instead of 10 Cotterell et al. (2016).\nIn our system submission, we utilize multispace variational encoder-decoders (MSVEDs), which are a varitional encoder-decoder with both continuous and discrete latent variables (Zhou and Neubig, 2017). The continuous latent variable is expected to reflect the lemma form of a word and the discrete variables are used to induce the desired labels of the inflected word. The whole model is trained in a semi-supervised fashion. For the supervised part we are reducing the reconstruction error of generating the inflected word given the lemma and corresponding tags. For the unsupervised part, we introduce the discrete latent variables representing the morphological tags, and train an auto-encoder over unlabeled corpora. Thus, the training objective includes both the variational lower bound on the marginal log likelihood of the observed parallel training data and the monolingual data.\nThere are two tasks in SIGMORPHON 2017, which are morphology inflection (task 1) and paradigm completion (task 2) respectively. We participated in task 1, inflection generation, in which the goal is to output the inflected form of a lemma given a set of desired morphological tags. 1 Experimental results found that our model works relatively well on the shared task 1 without extensive tuning of hyper-parameters and languagespecific features.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Methods", "text": "In this section we will detail the multi-space variational encoder-decoder model.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Notation:", "text": "In morphological reinflection, the source sequence x (s) consists of the characters in an inflected word (e.g., \"played\"), while the associated labels y (t) describe some linguistic features (e.g., y\n(t) pos = Verb, y (t)\ntense = Past) that we hope to realize in the target. The target sequence x (t) is therefore the characters of the re-inflected form of the source word (e.g., \"played\") that satisfy the linguistic features specified by y (t) . For this task, each discrete variable y (t) k has a set of possible labels (e.g. pos=V, pos=ADJ, etc) and follows a multinomial distribution.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Preliminaries: Variational Autoencoder", "text": "The variational autoencoder (Kingma and Welling, 2014) is an efficient way to handle (continuous) latent variables in neural models. We describe it briefly here, and interested readers can refer to Doersch (2016) for details. The VAE learns a generative model of the probability p(x) of observed data x. The generative process consists of first generating a continuous latent variable z conditioned on the observed data x, which is termed as the recognition model q(z|x) (encoder) and then use this latent variable to reconstruct the observation x known as the reconstruction (decoder) model p(x|z). VAE uses the variational inference to approximate the intractable posterior by learning a parametric posterior distribution for all observations.Th learning objective function is the variational lower bound on the marginal log likelihood of data:\nlog p \u03b8 (x) \u2265 E z\u223cq \u03c6 (z|x) [log p \u03b8 (x|z)]\u2212 KL(q \u03c6 (z|x)||p(z))(1)\nTo optimize the parameters with gradient descent, Kingma and Welling (2014) introduce a reparameterization trick that allows for training using simple backpropagation w.r.t. the Gaussian latent variables z.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Multi-space Variational Encoder-Decoders", "text": "There are two cases to discuss when employing the variational encoder-decoder framework for labeled sequence transduction. First, when the labels of the inflected words are known as is the format of the training data in the shared task, we don't need to bother introduction the discrete latent variables for the inflected labels. We maximize the variational lower bound on the conditional log likelihood of observing x (t) and y (t) as follows:\nlog p \u03b8 (x (t) , y (t) |x (s) ) \u2265 E z\u223cq \u03c6 (z|x (s) ) log p \u03b8 (x (t) , y (t) , z|x (s) ) q \u03c6 (z|x (s) ) = E z\u223cq \u03c6 (z|x (s) ) [log p \u03b8 (x (t) |y (t) , z) + log p \u03c0 (y (t) )]\u2212 KL(q \u03c6 (z|x (s) )||p(z)) = L l (x (t) , y (t) |x (s) )(2)\nwhich is a simple extension to the vanilla variational auto-enocders. Second, in the case of unsupervised learning or when the labels of the inflected word is not observed, we only observe a word or a pair of words and we would like to maximize the log likelihood of the observed data by marginalizing over possible morphological labels, which is consisted to the supervised case above. In this scenario, we can introduce the discrete latent variables for the inflected labels which are used to infer the labels for the target word. Then when decoding the word, we condition both on the continuous and discrete latent variables. For the variational encoder-decoder (MSVED), the variational lower bound on the conditional log likelihood is affected by the recognition model, and thus is computed as:\nlog p \u03b8 (x (t) |x (s) ) \u2265E (y (t) ,z)\u223cq \u03c6 (y (t) ,z|x (s) ,x (t) ) log p \u03b8 (x (t) , y (t) , z|x (s) ) q \u03c6 (y (t) , z|x (s) , x (t) ) =E y (t) \u223cq \u03c6 (y (t) |x (t) ) [E z\u223cq \u03c6 (z|x (s) ) [log p \u03b8 (x (t) |y (t) , z)] \u2212 KL(q \u03c6 (z|x (s) )||p(z)) + log p \u03c0 (y (t) ) \u2212 log q \u03c6 (y (t) |x (t) )] = L u (x (t) |x (s) )(3)\nWhile the unsupervised objective is trained by maximizing the following variational lower bound U(x) on the objective for unlabeled data:\nlog p \u03b8 (x) \u2265 E (y,z)\u223cq \u03c6 (y,z|x) log p \u03b8 (x, y, z) q \u03c6 (y, z|x) = E y\u223cq \u03c6 (y|x) [E z\u223cq \u03c6 (z|x) [log p \u03b8 (x|z, y)] \u2212 KL(q \u03c6 (z|x)||p(z)) + log p \u03c0 (y) \u2212 log q \u03c6 (y|x)] = U(x)(4)\nNote that when labels are not observed, the inference model q \u03c6 (y|x) has the form of a discriminative classifier, thus we can use observed labels as the supervision signal to learn a better classifier. In this case we also minimize the following cross entropy as the classification loss:\nD(x, y) = E (x,y)\u223cp l (x,y) [\u2212 log q \u03c6 (y|x)] (5)\nwhere p l (x, y) is the distribution of labeled data.\nTo sum up, the semi-supervised model (Semisup) is trained to maximize the variational lower bounds and minimize the classification crossentropy error of 5.\nL(x (s) , x (t) , y (t) , x) = \u03b1 \u2022 U(x) + L u (x (s) |x (t) ) + L l (x (t) , y (t) |x (s) ) \u2212 D(x (t) , y (t) )(6)\nThe weight \u03b1 controls the relative weight between the loss from unlabeled data and labeled data.\n3 Learning MSVED", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Learning Discrete Latent Variables", "text": "One challenge in training our model is that discrete random variables in a stochastic computation graph prevent the gradient from being backpropagated due to their non-differentiability, and marginalizing over all label combinations is also infeasible in our case.\nTo alleviate this problem, we use the recently proposed Gumbel-Softmax trick (Maddison et al., 2014;Gumbel and Lieblein, 1954) to create a differentiable estimator for categorical variables. In experiments, we start with a relatively large temperature and decrease it gradually.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Learning Continuous Latent Variables", "text": "We observe that with the vanilla implementation the KL cost quickly decreases to near zero, setting q \u03c6 (z|x) equal to standard normal distribution. In this case, the RNN decoder can easily degenerate into an RNN language model. Hence, the latent variables are ignored by the decoder and cannot encode any useful information. The latent variable z learns an undesirable distribution that coincides with the imposed prior distribution but has no contribution to the decoder. To force the decoder to use the latent variables, we take the following two approaches which are similar to Bowman et al. (2016). KL-Divergence Annealing: We add a coefficient \u03bb to the KL cost and gradually anneal it from zero to a predefined threshold \u03bb m . At the early stage of training, we set \u03bb to be zero and let the model first figure out how to project the representation of the source sequence to a roughly right point in the space and then regularize it with the KL cost. This technique can also be seen in (Ko\u010disk\u1ef3 et al., 2016;Miao and Blunsom, 2016). Input Dropout in the Decoder: Besides annealing the KL cost, we also randomly drop out the  input token with a probability of \u03b2 at each time step of the decoder during learning. The previous ground-truth token embedding is replaced with a zero vector when dropped. In this way, the RNN decoder could not fully rely on the ground-truth previous token, which ensures that the decoder uses information encoded in the latent variables.\nk a l b \u2303(x) \u00b5(x) \u270f \u21e0 N (0, 1) z <w> k k \u00e4 + y T 1 y T 2 y T 3 y T 4 .... ...... k a l b \u2303(x) \u00b5(x) \u270f \u21e0 N (0, 1) z <w> k k a Multinomial Sampling + ...... y12", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Architecture for Morphological Reinflection", "text": "The overall model architecture is shown in Fig. 1. Each character and each label is associated with a continuous vector. We employ Gated Recurrent Units (GRUs) for the encoder and decoder. We use only single directional GRUs as the encoder for the input word x (s) . u is the hidden representation of x (s) which is the last hidden state of GRUs. and is used as the input for the inference model on z. We represent \u00b5(u) and \u03c3 2 (u) as MLPs and sample z from N (\u00b5(u), diag(\u03c3 2 (u))), using z = \u00b5 + \u03c3 \u2022 , where \u223c N (0, I). Similarly, we can obtain the hidden representation of (t) and use this as input to the inference model on each label y\nx\n(t)\ni , which is also an MLP following a softmax layer to generate the categorical probabilities of target labels. Other experimental setups: We apply temperature annealing in the Gumble-Softmax with the scheme max(0.5, exp(\u22123e \u2212 5 \u2022 t)) every 2000 updates where t is the update steps. We observe   that our model is not sensitive to the temperature in this task. All hyperparameters are tuned on the validation set, and include the following: For KL cost annealing, \u03bb m is set to be 0.2 for all language settings. For character drop-out at the decoder, we empirically set \u03b2 to be 0.4 for all languages. We set the dimension of character embeddings to be 300, tag label embeddings to be 200, RNN hidden state to be 256, and latent variable z to be 150 or 100. We set \u03b1 the weight for the unsupervised loss to be 0.8. We train the model with Adadelta (Zeiler, 2012) and use early-stop with a patience of 5. Our system is an ensemble of five models and the probability vector at each time step is obtained by averaging the output probabilities from each model 5 Experiments", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Data pre-processing", "text": "Creating morphosyntactic tag maps: In our model, we treat the inference model on discrete labels in the form of discriminator, thus we need to know which label belongs to which morphosyntactic dimension. For example, V is a label of Part-of-speech-tagging. To obtain such mapping from a specific label to the morphosyntactic dimension, we leverage the Universal Morphological Feature Schema (Sylak-Glassman, 2016) and also add the missing schema from the training data to create the key-value pairs of morphosysntactic dimension and label. Then we reformat the labels provided in the data set into the key-value pairs to train a classifier for each morphosyntactic dimension.\nData Augmentation: We augment the data set in the similar way as Kann and Sch\u00fctze (2016). By doing so, the training data is not limited to the form of lemma to inflected word but can also be any word pairs that share the same lemma. This helps our model generalize better and learn the latent continuous representations more effectively. The size of training data set after augmentation scales with a factor of 2 to 20 times compared with the original one.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Monolingual WikiData:", "text": "We process the Wikipedia corpus provided by the shared task organizer as our unsupervised training data together with words in the training data. For each language, we first get the character vocabulary of the corresponding training data and only keep words in the Wiki corpus for which characters are all in the character set we obtained. All words that occur less than 20 times are eliminated. We also limit the number of words used during training to be the 50000 most frequent words.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results and Analysis", "text": "The results on the dev and test data of the 52 languages are presented in 1. We obtain a generation accuracy above 80% over more than 25% languages and an average of 87.2% for both dev and test data. The generation accuracy is almost consistent on the dev and test data except that the test data accuracy of Scottish-Gaelic drops by near 21%. We find that only a medium volume of training data is provided for Scottish-Gaelic. This may be the reason why the model trained for Scottish-Gaelic can not generalize as well as other languages.\nWe do not tune the hyper-parameters for each language manually. However, we test on different dimensions for the continuous latent variables. The dimension size we have used included 100 and 150. And we observe significant improvement by using a larger dimension size of latent variables over a portion of languages including Faroese, Lithuanian, Navajo, Scottish-gaelic, Northern-sami, Slovene, Sorani, Slovak. However, we also observe that for some languages including Finnish, German, French, etc, the performance drops signficantly after increasing the size of continuous latent variable dimension. This indicates that for different languages, the continuous space required to encode the lemma and inflected information varies from language to language. We will further investigate this in the future work.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Effect of Data Augmentation and Using", "text": "Wiki Data\nWhile our performance was reasonable, it was not as good as that presented in our previous work (Zhou and Neubig, 2017), nor was it competitive with the highest-scoring models on the shared task. In order to examine the reason for this, we performed several ablations, the results of which are presented in Tab. 3 First, we first examined the effects of data augmentation and Wiki Data for semi-supervised learning on the performance of our model. By removing the augmented data from the training set, we observe a large gain in the generation accuracy. Besides, we find that Wiki Data for semisupervised learning doesn't help much to increase the model's performance. The reasons for this will be examined further in the following section.\nWe additionally reimplemented a vanilla encoder-decoder model with attention that concatenates the input characters and target word tags together with a special token in the middle as the new input sequence to the encoder (Kann and Sch\u00fctze, 2016). The results show that the vanilla encoder-decoder works better than our  model in some cases. We suspect that since task 1 is purely an inflection task and because semi-supervised learning did not provide a particularly large benefit, a simpler model that utilizes attention may be sufficient. This is in contrast to our previous findings, where semi-supervised learning was highly effective, and the proposed model out-performed the simpler attention-based baseline.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Analysis on the Distribution of Linguistic Tags of Wiki Data and Training Data", "text": "One potential reason for the lack of effectiveness of semi-supervised training is that the semi-supervised data that we used for training was not appropriate for the task at hand, or that we were not able to use it in the most effective way. In order to do so, we analyze the distribution of linguistic tags for words from the training data in the shared task and the Wiki Data provided by the organizer, with the hypothesis that if the distribution of tags for the Wiki Data is very different from the training and test data for the shared task, our predictions may be biased away from the testing distribution by incorporating the unsupervised Wiki data. To perform this examination, we use the tag classifier trained in our model to predict the labels for each word in the Wiki Data.  The percentages of each label within each morphosyntactic dimension for Arabic and Persian are listed in Tab. 4 and Tab. 5. We found that the distribution of the linguistic tags for the Wiki Data and the training data in the shared task are not always consistent. For example, in Arabic, the distributions of predicted tags with respect to case, possession, part-of-speech, and several other classes differ significantly from the original training data. Such difference suggests that either the words in the unlabeled Wiki Data have very different characteristics than our training set, or our tag classifier is not functioning properly to identify the tags. Either case would be detrimental to semi-supervised learning. The problem is even more stark for Persian: in Persian the only labeled words in the training data are verbs, so all nonverb words in the Wiki Data will receive an incorrect analysis, which is obviously not conducive to learning anything useful. As a recommendation for the future, when performing semi-supervised learning for morphology where the labeled data only represents a subset of the phenomena in the language, it is likely necessary to first identify which of the available unlabeled data is appropriate for semi-supervised learning before applying such methods.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Case Study on Inflected Words", "text": "In Tab. 1, we notice that the performance on Latin is relatively poor compared with other languages. Latin is a highly inflected languages with three distinct genders, seven noun cases, four verb conjugations, four verb principal parts, six tenses, three persons, three moods, two voices, two aspects and two numbers. In addition to this, we found that the data set size after augmentation was only enlarged 2 times. We examine some errors made by our system on two worst performed languages Latin and Icelandic in Tab. 2. As shown in the table, we found that the inflections of Latin and Icelandic have more suffix variations from the lemma. We guess our model still lacks the ability to capture more complicated inflections for such languages. We might consider adding the dependencies between different inflections for multiple target labels in our future work.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion and Future Work", "text": "In this work, we further examine the method proposed in (Zhou and Neubig, 2017) for the shared task of SIGMORPHON 2017 on 52 languages and demonstrate the effectiveness of this approach. We will further improve our model's sophistication by investigating strategies for choosing appropriate semi-supervised data, and examining the model's performance on languages with a high inflection level.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "This work has been supported in part by an Amazon Academic Research Award. We thank Matthew Honnibal for pointing out that the data distribution of Wikipedia corpus might be biased.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Generating sentences from a continuous space", "journal": "Proceedings of CoNLL", "year": "2016", "authors": "Luke Samuel R Bowman; Oriol Vilnis;  Vinyals; M Andrew; Rafal Dai; Samy Jozefowicz;  Bengio"}, {"title": "Translating into morphologically rich languages with synthetic phrases", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Victor Chahuneau; Eva Schlinger; A Noah; Chris Smith;  Dyer"}, {"title": "The sigmorphon 2016 shared taskmorphological reinflection", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Ryan Cotterell; Christo Kirov; John Sylak-Glassman; David Yarowsky; Jason Eisner; Mans Hulden"}, {"title": "Joint semantic synthesis and morphological analysis of the derived word", "journal": "", "year": "2017", "authors": "Ryan Cotterell; Hinrich Sch\u00fctze"}, {"title": "Adapting morphology for arabic information retrieval", "journal": "Springer", "year": "2007", "authors": "Kareem Darwish; W Douglas;  Oard"}, {"title": "Tutorial on variational autoencoders", "journal": "", "year": "2016", "authors": "Carl Doersch"}, {"title": "Statistical theory of extreme values and some practical applications: a series of lectures", "journal": "US Government Printing Office Washington", "year": "1954", "authors": "Emil Julius Gumbel; Julius Lieblein"}, {"title": "Med: The lmu system for the sigmorphon 2016 shared task on morphological reinflection", "journal": "", "year": "2016", "authors": "Katharina Kann; Hinrich Sch\u00fctze"}, {"title": "Auto-encoding variational bayes", "journal": "", "year": "2014", "authors": "D P Kingma; M Welling"}, {"title": "", "journal": "", "year": "", "authors": "Tom\u00e1\u0161 Ko\u010disk\u1ef3; G\u00e1bor Melis; Edward Grefenstette; Chris Dyer; Wang Ling"}, {"title": "Semantic parsing with semi-supervised sequential autoencoders", "journal": "", "year": "2016", "authors": "Karl Moritz Hermann"}, {"title": "A* sampling", "journal": "", "year": "2014", "authors": "J Chris; Daniel Maddison; Tom Tarlow;  Minka"}, {"title": "Language as a latent variable: Discrete generative models for sentence compression", "journal": "", "year": "2016", "authors": "Yishu Miao; Phil Blunsom"}, {"title": "The composition and use of the universal morphological feature schema (unimorph schema)", "journal": "", "year": "2016", "authors": "John Sylak-Glassman"}, {"title": "Adadelta: an adaptive learning rate method", "journal": "", "year": "2012", "authors": "D Matthew;  Zeiler"}, {"title": "Multispace variational encoder-decoders for semisupervised labeled sequence transduction", "journal": "", "year": "2017", "authors": "Chunting Zhou; Graham Neubig"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Model architecture for labeled and unlabeled data. For the encoder-decoder model, only one direction from the source to target is given. The classification model is not illustrated in the diagram.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "{pos: V, N, ADJ}.. y2 2 {def: DEF, INDEF} y3 2 {num: DU, SG, PL}...", "figure_data": "Source WordReinflected FormSupervised Variational Encoder DecoderSource WordSource Wordy1y2y3...y4...\u2022 \u2022 \u2022Unsupervised Variational Auto-encoder"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Results of the ensemble system on the development ang test sets of 52 languages.", "figure_data": "LanguageDev Test LanguageDev Test LanguageDevTestLatin66.2 66.2 Navajo84.9 84.2 English93.394.6Icelandic71.7 68.1 French84.9 82.4 Lower-Sorbian 93.991.3Irish72.7 71.9 Armenian85.3 82.3 Italian94.292.6Finnish73.4 74.9 Latvian85.6 87.5 Basque95.097.0Hungarian74.5 73.6 Scottish-Gaelic 86.0 68.0 Estonian95.193.7Faroese74.8 74.5 Bulgarian86.3 86.7 Quechua95.595.5Russian75.8 76.4 Macedonian86.6 86.1 Khaling96.294.8Norwegian-Nynorsk 77.8 73.8 Northern-Sami 86.7 85.8 Hebrew96.397.5Polish78.3 78.1 Slovene87.3 87.8 Portuguese96.496.4German79.3 78.7 Danish88.1 85.4 Catalan96.996.5Swedish80.2 80.6 Arabic88.6 85.9 Urdu98.497.9Romanian80.3 78.6 Sorani89.6 87.8 Persian98.698.7Lithuanian80.6 81.6 Slovak89.6 87.9 Bengali99.099.0Serbo-Croatian81.1 79.6 Turkish90.4 90.3 Welsh99.099.0Norwegian-Bokmal 81.2 82 Dutch91.2 88.9 Haida99.097.0Czech83.1 81.9 Albanian91.9 91.3 Hindi99.999.6Kurmanji83.4 83.8 Georgian92.5 92.3Ukrainian84.5 84.0 Spanish92.5 92.8 Average87.18 86.21Table 1: Language Src WordTgt LabelsGold TgtOursLatintrygon largio compensoPos=N;Case=ABL;Num=PL Mood=SBJV;Num=PL;Per=2;Tense=PST;Asp=PRF;Pos=V Mood=SBJV;Num=SG;Per=3;Tense=PST;Asp=PFV;Pos=V compens\u0101verit compenserit tr\u0233g\u014dnibus tryg\u00f5nibus larg\u012bviss\u1ebdtis largiss\u0113tisIcelandich\u00e1spil gallabuxur lestPos=N;Def=DEF;Case=GEN;Num=PL Pos=N;Def=INDF;Case=GEN;Num=SG Pos=N;Def=DEF;Case=GEN;Num=SG Lh\u00e1spilanna gallabuxna lestarinnarh\u00e1splanna gallab\u00f6lur lestsins"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Examples of incorrect inflection generation words on the dev data.", "figure_data": "Language SettingsDev Acc. (Single Model.)Icelandicvanilla Encoder-Decoder + attention, w/o data augmentation our model w/o data augmentation and Wiki our model (full)81.0 78.6 71.7Latinvanilla Encoder-Decoder + attention, w/o data augmentation our model w/o data augmentation and Wiki our model (full)74.6 66.6 66.2Persianvanilla Encoder-Decoder + attention, w/o data augmentation our model w/o data augmentation and Wiki our model (full)99.6 99.6 98.6Arabicvanilla Encoder-Decoder + attention, w/o data augmentation our model w/o data augmentation and Wiki our model (full)90.7 91.3 88.6"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Ablation experiments on the effects of data augmentation and WikiData.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "The distribution of morphosyntactic tags for Arabic on Wikipedia and the shared task training data respectively. The linguistic tag classifier has an average accuracy of 93.36% on the Dev data.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The distribution of morphosyntactic tags for Persian on Wikipedia and the shared task training data respectively. The linguistic tag classifier has an average accuracy of 95.26% on the Dev data.", "figure_data": ""}], "doi": ""}