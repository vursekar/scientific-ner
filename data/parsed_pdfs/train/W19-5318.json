{"authors": "Jeremy Gwinnup; Grant Erdmann; Timothy Anderson", "pub_date": "", "title": "The AFRL WMT19 Systems: Old Favorites and New Tricks", "abstract": "This paper describes the Air Force Research Laboratory (AFRL) machine translation systems and the improvements that were developed during the WMT19 evaluation campaign. This year, we refine our approach to training popular neural machine translation toolkits, experiment with a new domain adaptation technique and again measure improvements in performance on the Russian-English language pair.", "sections": [{"heading": "Introduction", "text": "As part of the 2019 Conference on Machine Translation (Bojar et al., 2019) news-translation shared task, the AFRL Human Language Technology team participated in the Russian-English portion of the competition. We build on our strategies from last year (Gwinnup et al., 2018), adding additional language ID based data processing and optimizing subword segmentation strategies. For Russian-English we again submitted an entry comprising our best systems trained with Marian (Junczys-Dowmunt et al., 2018), Sockeye (Hieber et al., 2017) with Elastic Weight Consolidation (EWC) (Thompson et al., 2019), OpenNMT (Klein et al., 2018), and Moses (Koehn et al., 2007) combined using the Jane system combination method (Freitag et al., 2014).", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Data and Preprocessing", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Data Preparation", "text": "We used and preprocess data as outlined in Gwinnup et al. (2018). For all systems trained, we applied either byte-pair encoding (BPE) (Sennrich et al., 2016) or SentencePiece (Kudo and Richardson, 2018) subword strategies to address the vocabulary-size problem.\nFor this year, we also employed a language ID filtering step for the BPE-based systems. Using the pre-built language ID model developed by the authors of fastText (Joulin et al., 2016a,b), we developed a utility that examined the source and target sentence pairs and discarded that pair if either side fell below 0.8 1 probability of the desired language. We applied this filtering to all provided parallel corpora, removing 33.7% of lines. This process was particularly effective when used to filter the Paracrawl corpus where 57.1% of lines were removed. Pre and post-filtering line counts for various corpora are shown in Table 1.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Corpus", "text": "Total Retained   A comparison with the organizer-provided parallel training data used in our WMT18 system (which is largely the same as the provided parallel data for WMT19 in the Russian-English language pair) on baseline Marian transformer systems with identical training conditions show that aggressive language ID based filtering yields an approximate +1 BLEU point improvement as measured by SacreBLEU (Post, 2018). These results are shown in Table 2.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Exploration of Byte-Pair Encoding Merge Sizes", "text": "One of the problems faced when addressing the closed-vocabulary problem is the granularity of the subword units either produced by SentencePiece or BPE. To that end, we examined varying the number of BPE merge operations in order to determine an optimal setting to maximize performance for the Russian-English language pair. For the OpenNMT-based systems, a vocabulary size of 32k entries was employed during training of a SentencePiece segmentation model 2 . This vocabulary size was determined empirically from the training data.\nAlternatively, for the BPE-based systems, we systematically examined varying sizes of BPE merge operations and vocabulary sizes in 10k increments from 30k to 80k. Results in Table 3 show that 40k BPE merge operations perform best across all test sets decoded for this language pair. All subsequent Marian experiments in this work utilize this 40k BPE training corpus.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "MT Systems", "text": "This year, we focused system-building efforts on the Marian, Sockeye, OpenNMT, and Moses toolkits, having explored a variety of parameters, data, and conditions. While most of our experimentation builds off of previous years' efforts, we did examine domain adaptation via continued training, including Elastic Weight Consolidation (EWC) (Thompson et al., 2019).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Marian", "text": "As with last year's efforts, we train multiple Marian (Junczys-Dowmunt et al., 2018) models with both University of Edinburgh's \"bi-deep\"  and Google's transformer (Vaswani et al., 2017) architectures. Network hyperparameters are the same as detailed in Gwinnup et al. (2018). We again use newstest2014 as the validation set during training.\nUtilizing the best-performing BPE parameters from Section 2.2, we first trained a baseline system in each of the two network architectures, noting the Transformer system's better performance of +0.82 BLEU on average across decoded test sets. An additional six distinct transformer models were then independently 3 trained for use in ensemble decoding. We then ensemble decoded test sets with all eight models.\nMarian typically assigns each model used in ensemble decoding a feature weight of 1.0; thus each model contributes equally to the decoding process. Borrowing from our Moses training approach, we utilize a multi-iteration decode and optimize feature weights using the \"Expected Corpus BLEU\" (ECB) metric with the Drem optimizer (Erdmann and Gwinnup, 2015). We experimented using newstest2014 and newstest2017 as tuning sets -2017 did not help performance, but using 2014 did improve performance by up to +0.9 BLEU 4 over the non-tuned ensemble.\nScores for all the above-mentioned systems are shown in Table 4. The best-performing ensemble (ensemble tune14) was used in system combination.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Sockeye", "text": "For our Sockeye (Hieber et al., 2017) systems, we experimented with continued training (Luong and Manning, 2015;Sennrich et al., 2015) -a means to specialize a model in a new domain after a period of training on a general domain. One downside of utilizing continued training is the model adapts \"too-well\" to the new domain at the expense of performance in the original domain (Freitag and Al-Onaizan, 2016). One method to mitigate this performance drop is to prevent certain parameters of the network from changing with Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017). Thompson et al. (2019)    ference in style and content. Here, we created a news subdomain corpus from the newstest2014 through newstest2017 test sets. The intuition is that more current events will be discussed in these test sets than the remainder of the provided training corpora, allowing better adaptation of new events in the newest test sets (newstest2018 and newstest2019.)\nWe first trained a baseline transformer system using the best-performing BPE parameters from Section 2.2, 512-dimension word embeddings, 6 layer encoder and decoder, 8 attention heads, label smoothing and transformer attention dropout of 0.1. We then continue-train a model on the adaptation set described above. We also followed the Sockeye EWC training procedure, producing a model more resilient to overfitting due to continued training. Results for these systems are shown in Table 5.\nWe see that the baseline Sockeye transformer model performs similarly to the baseline singlemodel Marian transformer system shown in Table 4. The continued-training system (con't train) system predictably overfit on newstest2014 as expected, since that test set is a part of the adaptation set. Likewise, performance on the out-ofdomain newstest2018 also dropped as a result of overfitting. The best-performing EWC system 5 5 EWC applied with weight-decay of 0.001 and learning-actually improved performance on 2018 with lesspronounced overfitting on 2014.  For system combination outlined later in Section 4, we decoded test sets with an ensemble of the four highest-scoring model checkpoints from the best EWC training run.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "OpenNMT-T", "text": "Our first Open-NMT system was trained using the Transformer architecture with the default \"Trans-formerBig\" settings as described in Vaswani et al. (2017): 6 layers of 1024 units, 16 attention heads. Dropout rates of 0.3 for layers and 0.1 for attention heads and relu's. Training data for this system utilized the training corpus from our WMT17 Russian-English system (Gwinnup et al., 2017) consisting of provided parallel and backtranslated rate of 0.00001 data. This data was then processed with a joint 32k word vocabulary SentencePiece model.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "OpenNMT-G", "text": "For our second OpenNMT system, we first trained language-specific, 32k word vocabularies using SentencePiece. WMT news test data from all years except 2014 and 2017 were used to train Senten-cePiece. These data, with the addition of the language ID filtered ParaCrawl corpus outlined in Section 2.1, were used for training the system. WMT news test data from 2014 was used for validation. OpenNMT-tf was used to create the system, using the stock \"Transformer\" model.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Moses", "text": "As in previous years, we trained a phrase-based Moses (Koehn et al., 2007) system with the same data as the Marian system outlined in Section 3.1 in order to provide diversity for system combination. This system employed a hierarchical reordering model (Galley and Manning, 2008) and 5-gram operation sequence model (Durrani et al., 2011). The 5-gram English language model was trained with KenLM on all permissable monolingual English news-crawl data. The BPE model used was applied to both the parallel training data and the language modeling corpus. System weights were tuned with the Drem (Erdmann and Gwinnup, 2015) optimizer using the \"Expected Corpus BLEU\" (ECB) metric.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "System Combination", "text": "Jane system combination (Freitag et al., 2014) was employed to combine outputs from the best systems from each approach outlined above. Individual component system and final combination scores are shown in Table 6 for cased, detokenized BLEU and BEER 2.0 (Stanojevi\u0107 and Sima'an, 2014) .", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Submission Systems", "text": "We submitted the final 5-system combination outlined in Section 4 and the four-checkpoint EWC ensemble detailed in Section 3.2 to the Russian-English portion of the WMT19 news task evaluation. Selected newstest2019 automatic scores from the WMT Evaluation Matrix 6 are shown in Table 7.   ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We presented a series of improvements to our Russian-English systems, including improved preprocessing and domain adaptation. Clever remixing of older techniques from the phrasebased MT era enabled improvements in ensembled neural decoding. Lastly, we performed system combination to leverage benefits from these new techniques and favorite approaches from previous years.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Findings of the 2019 conference on machine translation (wmt19)", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ond\u0159ej Bojar; Christian Federmann; Mark Fishel; Yvette Graham; Barry Haddow; Matthias Huck; Philipp Koehn; Christof Monz; Mathias M\u00fcller; Matt Post"}, {"title": "A joint sequence translation model with integrated reordering", "journal": "", "year": "2011", "authors": "Nadir Durrani; Helmut Schmid; Alexander Fraser"}, {"title": "Drem: The AFRL submission to the WMT15 tuning task", "journal": "", "year": "2015", "authors": "Grant Erdmann; Jeremy Gwinnup"}, {"title": "Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government. Cleared for public release on 12", "journal": "", "year": "2019-06", "authors": ""}, {"title": "Proc. of the Tenth Workshop on Statistical Machine Translation", "journal": "", "year": "", "authors": ""}, {"title": "Fast Domain Adaptation for Neural Machine Translation", "journal": "", "year": "2016", "authors": "Markus Freitag; Yaser Al-Onaizan"}, {"title": "Jane: Open source machine translation system combination", "journal": "", "year": "2014", "authors": "Markus Freitag; Matthias Huck; Hermann Ney"}, {"title": "A simple and effective hierarchical phrase reordering model", "journal": "", "year": "2008", "authors": "Michel Galley; Christopher D Manning"}, {"title": "The AFRL WMT18 systems: Ensembling, continuation and combination", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jeremy Gwinnup; Tim Anderson; Grant Erdmann; Katherine Young"}, {"title": "The AFRL-MITLL WMT17 systems: Old, new, borrowed, BLEU", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Jeremy Gwinnup; Timothy Anderson; Grant Erdmann; Katherine Young; Michaeel Kazi; Elizabeth Salesky; Brian Thompson; Jonathan Taylor"}, {"title": "Sockeye: A toolkit for neural machine translation", "journal": "", "year": "2017", "authors": "Felix Hieber; Tobias Domhan; Michael Denkowski; David Vilar; Artem Sokolov; Ann Clifton; Matt Post"}, {"title": "Matthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov. 2016a. Fasttext.zip: Compressing text classification models", "journal": "", "year": "", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski"}, {"title": "Bag of tricks for efficient text classification", "journal": "", "year": "2016", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Tomas Mikolov"}, {"title": "Marian: Fast neural machine translation in C++", "journal": "", "year": "2018", "authors": "Marcin Junczys-Dowmunt; Roman Grundkiewicz; Tomasz Dwojak; Hieu Hoang; Kenneth Heafield; Tom Neckermann; Frank Seide; Ulrich Germann; Alham Fikri Aji; Nikolay Bogoychev; F T Andr\u00e9; Alexandra Martins;  Birch"}, {"title": "Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell", "journal": "Proceedings of the National Academy of Sciences", "year": "2017", "authors": "James Kirkpatrick; Razvan Pascanu; Neil Rabinowitz; Joel Veness; Guillaume Desjardins; Andrei A Rusu; Kieran Milan; John Quan; Tiago Ramalho"}, {"title": "OpenNMT: Neural machine translation toolkit", "journal": "", "year": "2018", "authors": "Guillaume Klein; Yoon Kim; Yuntian Deng; Vincent Nguyen; Jean Senellart; Alexander Rush"}, {"title": "Moses: Open source toolkit for statistical machine translation", "journal": "", "year": "2007", "authors": "Philipp Koehn; Hieu Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; Nicola Bertoldi; Brooke Cowan; Wade Shen; Christine Moran; Richard Zens; Chris Dyer; Ond\u0159ej Bojar; Alexandra Constantin; Evan Herbst"}, {"title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"title": "Stanford Neural Machine Translation Systems for Spoken Language Domain", "journal": "", "year": "2015", "authors": "Minh-Thang Luong; Christopher D Manning"}, {"title": "Deep architectures for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Antonio Valerio Miceli Barone; Jind\u0159ich Helcl; Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "A call for clarity in reporting BLEU scores", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Matt Post"}, {"title": "The University of Edinburgh's neural MT systems for WMT17", "journal": "", "year": "2017", "authors": "Rico Sennrich; Alexandra Birch; Anna Currey; Ulrich Germann; Barry Haddow; Kenneth Heafield; Antonio Valerio Miceli; Philip Barone;  Williams"}, {"title": "Improving neural machine translation models with monolingual data", "journal": "", "year": "2015", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Neural machine translation of rare words with subword units", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Fitting sentence level translation evaluation with many dense features", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Milo\u0161 Stanojevi\u0107; Khalil Sima'an"}, {"title": "Overcoming catastrophic forgetting during domain adaptation of neural machine translation", "journal": "", "year": "2019", "authors": "Brian Thompson; Jeremy Gwinnup; Huda Khayrallah; Kevin Duh; Philipp Koehn"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}], "figures": [{"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Training corpus total and retained lines afterfastText filteringtestsetwmt18preproc wmt19filtnewstest201433.034.1newstest201528.629.6newstest201628.429.4newstest201730.831.8newstest201826.927.9"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": Test set comparison for non-filtered WMT18training corpus and filtered WMT19 training corpusmeasured by SacreBLEU."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Cased, detokenized BLEU for various test sets and BPE merge-value treatments. Best scores for each test set are denoted with bold text.", "figure_data": "Systemnewstest2014 newstest2015 newstest2016 newstest2017 newstest2018single bi-deep32.729.028.731.327.0single transformer34.129.629.431.827.9untuned ensemble36.231.630.534.229.7ensemble tune1735.331.130.234.229.7ensemble tune1437.131.331.234.530.5"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ": Sockeye system scores for newstest2014(in-domain) and newstest2018 (out-of-domain) testsets for various training conditions measured in Sacre-BLEU."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "http://matrix.statmt.org", "figure_data": "SystemBLEU BEER1. Marian30.47 0.59952. Sockeye EWC 29.43 0.59683. OpenNMT-T26.22 0.57374. OpenNMT-G30.05 0.60175. Moses27.33 0.5836Syscomb-532.12 0.6072"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "System combination and input system scores measured in cased, detokenized BLEU and BEER on the newstest2018 test set.", "figure_data": "SystemBLEU BEERafrl-syscomb1937.20.627afrl-ewc34.30.613"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Final submission system scores measured in cased BLEU and BEER on the newstest2019 test set.", "figure_data": ""}], "doi": "10.1073/pnas.1611835114"}