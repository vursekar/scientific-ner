{"authors": "Xiangpeng Wei; Heng Yu; Yue Hu; Rongxiang Weng; Weihua Luo; Rong Jin", "pub_date": "", "title": "Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation", "abstract": "The principal task in supervised neural machine translation (NMT) is to learn to generate target sentences conditioned on the source inputs from a set of parallel sentence pairs, and thus produce a model capable of generalizing to unseen instances. However, it is commonly observed that the generalization performance of the model is highly influenced by the amount of parallel data used in training. Although data augmentation is widely used to enrich the training data, conventional methods with discrete manipulations fail to generate diverse and faithful training samples. In this paper, we present a novel data augmentation paradigm termed Continuous Semantic Augmentation (CSANMT), which augments each training instance with an adjacency semantic region that could cover adequate variants of literal expression under the same meaning. We conduct extensive experiments on both rich-resource and low-resource settings involving various language pairs, including WMT14 English\u2192{German,French}, NIST Chinese\u2192English and multiple low-resource IWSLT translation tasks. The provided empirical evidences show that CSANMT sets a new level of performance among existing augmentation techniques, improving on the state-of-theart by a large margin. 1   ", "sections": [{"heading": "Introduction", "text": "Neural machine translation (NMT) is one of the core topics in natural language processing, which aims to generate sequences of words in the target language conditioned on the source inputs (Sutskever et al., 2014;Cho et al., 2014;Vaswani et al., 2017). In the common supervised setting, the training objective is to learn a transformation from the source space to the target space X \u2192 Y : f (y|x; \u0398) with the usage of parallel data. In this way, NMT models are expected to 1 The core codes are contained in Appendix E. be capable of generalizing to unseen instances with the help of large scale training data, which poses a big challenge for scenarios with limited resources.\nTo address this problem, various methods have been developed to leverage abundant unlabeled data for augmenting limited labeled data (Sennrich et al., 2016a;Cheng et al., 2016;Hoang et al., 2018;Song et al., 2019). For example, backtranslation (BT) (Sennrich et al., 2016a) makes use of the monolingual data on the target side to synthesize large scale pseudo parallel data, which is further combined with the real parallel corpus in machine translation task. Another line of research is to introduce adversarial inputs to improve the generalization of NMT models towards small perturbations (Iyyer et al., 2015;Fadaee et al., 2017;Wang et al., 2018;Cheng et al., 2018;Gao et al., 2019). While these methods lead to significant boosts in translation quality, we argue that augmenting the observed training data in the discrete space inherently has two major limitations.\nFirst, augmented training instances in discrete space are lack diversity. We still take BT as an example, it typically uses beam search (Sennrich et al., 2016a) or greedy search (Lample et al., 2018a,c) to generate synthetic source sentences for each target monolingual sentence. The above two search strategies are approximate algorithms to identify the maximum a-posteriori (MAP) output , and thus favor the most frequent one in case of ambiguity.  proposed a sampling strategy from the output distribution to alleviate this issue, but this method typically yields synthesized data with low quality. While some extensions (Wang et al., 2018;Imamura et al., 2018;Khayrallah et al., 2020;Nguyen et al., 2020) augment each training instance with multiple literal forms, they still fail to cover adequate variants under the same meaning.\nSecond, it is difficult for augmented texts in dis-crete space to preserve their original meanings. In the context of natural language processing, discrete manipulations such as adds, drops, reorders, and/or replaces words in the original sentences often result in significant changes in semantics. To address this issue, Gao et al. (2019) and Cheng et al. (2020) instead replace words with other words that are predicted using language model under the same context, by interpolating their embeddings. Although being effective, these techniques are limited to word-level manipulation and are unable to perform the whole sentence transformation, such as producing another sentence by rephrasing the original one so that they have the same meaning.\nIn this paper, we propose Continuous Semantic Augmentation (CSANMT), a novel data augmentation paradigm for NMT, to alleviate both limitations mentioned above. The principle of CSANMT is to produce diverse training data from a semantically-preserved continuous space. Specifically, (1) we first train a semantic encoder via a tangential contrast, which encourages each training instance to support an adjacency semantic region in continuous space and treats the tangent points of the region as the critical states of semantic equivalence. This is motivated by the intriguing observation made by recent work showing that the vectors in continuous space can easily cover adequate variants under the same meaning (Wei et al., 2020a).\n(2) We then introduce a Mixed Gaussian Recurrent Chain (MGRC) algorithm to sample a cluster of vectors from the adjacency semantic region. (3) Each of the sampled vectors is finally incorporated into the decoder by developing a broadcasting integration network, which is agnostic to model architectures. As a consequence, transforming discrete sentences into the continuous space can effectively augment the training data space and thus improve the generalization capability of NMT models.\nWe evaluate our framework on a variety of machine translation tasks, including WMT14 English-German/French, NIST Chinese-English and multiple IWSLT tasks. Specifically, CSANMT sets the new state of the art among existing augmentation techniques on the WMT14 English-German task with 30.94 BLEU score. In addition, our approach could achieve comparable performance with the baseline model with the usage of only 25% of training data. This reveals that CSANMT has great potential to achieve good results with very few data. Furthermore, CSANMT demonstrates consistent improvements over strong baselines in low resource scenarios, such as IWSLT14 English-German and IWSLT17 English-French.", "n_publication_ref": 23, "n_figure_ref": 0}, {"heading": "Framework", "text": "Problem Definition Supposing X and Y are two data spaces that cover all possible sequences of words in source and target languages, respectively. We denote (x, y) \u2208 (X , Y) as a pair of two sentences with the same meaning, where x = {x 1 , x 2 , ..., x T } is the source sentence with T tokens, and y = {y 1 , y 2 , ..., y T \u2032 } is the target sentence with T \u2032 tokens. A sequence-tosequence model is usually applied to neural machine translation, which aims to learn a transformation from the source space to the target space X \u2192 Y : f (y|x; \u0398) with the usage of parallel data. Formally, given a set of observed sentence pairs\nC = {(x (n) , y (n) )} N n=1\n, the training objective is to maximize the log-likelihood:\nJ mle (\u0398) = E (x,y)\u223cC log P (y|x; \u0398) .(1)\nThe log-probability is typically decomposed as: log P (y|x; \u0398) =\nT \u2032\nt=1 log P (y t |y <t , x; \u0398), where \u0398 is a set of trainable parameters and y <t is a partial sequence before time-step t.\nHowever, there is a major problem in the common supervised setting for neural machine translation, that is the number of training instances is very limited because of the cost in acquiring parallel data. This makes it difficult to learn an NMT model generalized well to unseen instances. Traditional data augmentation methods generate more training samples by applying discrete manipulations to unlabeled (or labeled) data, such as back-translation or randomly replacing a word with another one, which usually suffer from the problems of semantic deviation and the lack of diversity.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Continuous Semantic Augmentation", "text": "We propose a novel data augmentation paradigm for neural machine translation, termed continuous semantic augmentation (CSANMT), to better generalize the model's capability to unseen instances. We adopt the Transformer (Vaswani et al., 2017) model as a backbone, and the framework is shown in Figure 1. In this architecture, an extra semantic encoder translates the source x and the target sentence y to real-value vectors r x = \u03c8(x; \u0398 \u2032 ) and r y = \u03c8(y; \u0398 \u2032 ) respectively, where \u03c8(\u2022; \u0398 \u2032 ) is the forward function of the semantic encoder parameterized by \u0398 \u2032 (parameters other than \u0398). \n\u2200(x, y) \u2208 (X , Y) : r x = r y .\nBesides, an adjacency semantic region \u03bd(r x , r y ) in the semantic space describes adequate variants of literal expression centered around each observed sentence pair (x, y).\nIn our scenario, we first sample a series of vectors (denoted by R) from the adjacency semantic region to augment the current training instance, that is R = {r (1) ,r (2) , ...,r (K) }, wherer (k) \u223c \u03bd(r x , r y ). K is the hyperparameter that determines the number of sampled vectors. Each sampler (k) is then integrated into the generation process through a broadcasting integration network:\not = W1r (k) + W2ot + b,(2)\nwhere o t is the output of the self-attention module at position t. Finally, the training objective in Eq.\n(1) can be improved as\nJ mle (\u0398) =E (x,y)\u223cC,r (k) \u2208R log P (y|x,r (k) ; \u0398) . (3)\nBy augmenting the training instance (x, y) with diverse samples from the adjacency semantic region, the model is expected to generalize to more unseen instances. To this end, we must consider such two problems: (1) How to optimize the semantic encoder so that it produces a meaningful adjacency semantic region for each observed training pair.  (i) , y (i) ).\n(2) How to obtain samples from the adjacency semantic region in an efficient and effective way.\nIn the rest part of this section, we introduce the resolutions of these two problems, respectively.\nTangential Contrastive Learning We start from analyzing the geometric interpretation of adjacency semantic regions. The schematic diagram is illustrated in Figure 2. Let (x (i) , y (i) ) and (x (j) , y (j) ) are two instances randomly sampled from the training corpora. For (x (i) , y (i) ), the adjacency semantic region \u03bd(r\nx (i) , r y (i)\n) is defined as the union of two closed balls that are centered by r x (i) and r y (i) , respectively. The radius of both balls is\nd =\u2225 r x (i) \u2212 r y (i) \u2225 2\n, which is also considered as a slack variable for determining semantic equivalence. The underlying interpretation is that vectors whose distances from r x (i) (or r y (i) ) do not exceed d, are semantically-equivalent to both r x (i) and r y (i) . To make \u03bd(r x (i) , r y (i) ) conform to the interpretation, we employ a similar method as in (Zheng et al., 2019; to optimize the semantic encoder with the tangential contrast. Specifically, we construct negative samples by applying the convex interpolation between the current instance and other ones in the same training batch for instance comparison. And the tangent points (i.e., the points on the boundary) are considered as the critical states of semantic equivalence. The training objective is formulated as:\nJ ctl (\u0398 \u2032 ) =E (x (i) ,y (i) )\u223cB log e s r x (i) ,r y (i) e s r x (i) ,r y (i) + \u03be , \u03be = |B| j&j\u0338 =i e s r y (i) ,r y \u2032(j) + e s r x (i) ,r x \u2032(j) ,(4)\nwhere B indicates a batch of sentence pairs randomly selected from the training corpora C, and s(\u2022) is the score function that computes the cosine similarity between two vectors. The negative samples r x \u2032(j) and r y \u2032(j) are designed as the following interpolation:\nr x \u2032(j) = r x (i) + \u03bbx(r x (j) \u2212 r x (i) ), \u03bbx \u2208 ( d d \u2032 x , 1], r y \u2032(j) = r y (i) + \u03bby(r y (j) \u2212 r y (i) ), \u03bby \u2208 ( d d \u2032 y , 1],(5)\nwhere\nd \u2032 x =\u2225 r x (i) \u2212 r x (j) \u2225 and d \u2032 y =\u2225 r y (i) \u2212 r y (j) \u2225.\nThe two equations in Eq. ( 5) set up when d \u2032\nx and d \u2032 y are larger than d respectively, or else r x \u2032(j) = r x (j) and r y \u2032(j) = r y (j) . According to this design, an adjacency semantic region for the i-th training instance can be fully established by interpolating various instances in the same training batch. We follow  to adaptively adjust the value of \u03bb x (or \u03bb y ) during the training process, and refer to the original paper for details.\nMGRC Sampling To obtain augmented data from the adjacency semantic region for the training instance (x, y), we introduce a Mixed Gaussian Recurrent Chain (denoted by MGRC) algorithm to design an efficient and effective sampling strategy. As illustrated in Figure 3, we first transform the bias vectorr = r y \u2212 r x according to a predefined scale vector \u03c9, that is \u03c9 \u2299r, where \u2299 is the element-wise product operation. Then, we construct a novel sampler = r + \u03c9 \u2299r for augmenting the current instance, in which r is either r x or r y . As a consequence, the goal of the sampling strategy turns into find a set of scale vectors, i.e. {\u03c9 (1) , \u03c9 (2) , ..., \u03c9 (K) }. Intuitively, we can assume that \u03c9 follows a distribution with universal or Gaussian forms, despite the latter demonstrates better results in our experience. Formally, we design a", "n_publication_ref": 7, "n_figure_ref": 3}, {"heading": "Algorithm 1 MGRC Sampling", "text": "Input: The representations of the training instance (x, y),\ni.e. rx and ry. Output: A set of augmented samples R = {r (1) ,r (2) , ...,r (K) } 1: Normalizing the importance of each element inr = ry \u2212 rx:\nWr = |r|\u2212min(|r|) max(|r|)\u2212min(|r|) 2: Set k = 1, \u03c9 (1) \u223c N (0, diag(W 2 r )),r (1) = r + \u03c9 (1) \u2299 (ry \u2212 rx) 3: Initialize the set of samples as R = {r (1) }. 4: while k \u2264 (K \u2212 1) do 5: k \u2190 k + 1 6:\nCalculate the current scale vector: \u03c9 (k) \u223c p(\u03c9|\u03c9 (1) , \u03c9 (2) , ..., \u03c9 (k\u22121) according to Eq. (6). 7:\nCalculate the current sample:r (k) = r + \u03c9 (k) \u2299 (ry \u2212 rx).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "8:", "text": "R \u2190 R {r (k) }. 9: end while mixed Gaussian distribution as follow:\n\u03c9 (k) \u223c p(\u03c9|\u03c9 (1) , \u03c9 (2) , ..., \u03c9 (k\u22121) ), p = \u03b7N 0, diag(W 2 r ) + (1.0 \u2212 \u03b7)N 1 k \u2212 1 k\u22121 i=1 \u03c9 (i) , 1 . (6)\nThis framework unifies the recurrent chain and the rejection sampling mechanism. Concretely, we first normalize the importance of each dimension inr as W r = |r|\u2212min (|r|) max(|r|)\u2212min(|r|) , the operation | \u2022 | takes the absolute value of each element in the vector, which means the larger the value of an element is the more informative it is. Thus N (0, diag(W 2 r )) limits the range of sampling to a subspace of the adjacency semantic region, and rejects to conduct sampling from the uninformative dimensions. Moreover,\nN ( 1 k\u22121 k\u22121 i=1 \u03c9 (i)\n, 1) simulates a recurrent chain that generates a sequence of reasonable vectors where the current one is dependent on the prior vectors. The reason for this design is that we expect that p in Eq. ( 6) can become a stationary distribution with the increase of the number of samples, which describes the fact that the diversity of each training instance is not infinite. \u03b7 is a hyperparameter to balance the importance of the above two Gaussian forms. For a clearer presentation, Algorithm 1 summarizes the sampling process.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Training and Inference", "text": "The training objective in our approach is a combination of J mle (\u0398) in Eq. (3) and J ctl (\u0398 \u2032 ) in Eq. (4). In practice, we introduce a two-phase training procedure with mini-batch losses. Firstly, we train the semantic encoder from scratch using the task-specific data, i.e. \u0398 \u2032 * = argmax \u0398 \u2032 J ctl (\u0398 \u2032 ). Secondly, we optimize the encoder-decoder model by maximizing the log-likelihood, i.e. \u0398 * = argmax \u0398 J mle (\u0398), and fine-tune the semantic encoder with a small learning rate at the same time.\nDuring inference, the sequence of target words is generated auto-regressively, which is almost the same as the vanilla Transformer (Vaswani et al., 2017). A major difference is that our method involves the semantic vector of the input sequence for generation: y * t = argmax yt P (\u2022|y <t , x, r x ; \u0398), where r x = \u03c8(x; \u0398 \u2032 ). This module is plug-in-use as well as is agnostic to model architectures.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We first apply CSANMT to NIST Chinese-English (Zh\u2192En), WMT14 English-German (En\u2192De) and English-French (En\u2192Fr) tasks, and conduct extensive analyses for better understanding the proposed method. And then we generalize the capability of our method to low-resource IWSLT tasks. Training Details. We implement our approach on top of the Transformer (Vaswani et al., 2017). The semantic encoder is a 4-layer transformer encoder with the same hidden size as the backbone model. Following sentence-bert (Reimers and Gurevych, 2019), we average the outputs of all positions as the sequence-level representation. The learning rate for finetuning the semantic encoder at the second training stage is set as 1e \u2212 5. All experiments are performed on 8 V100 GPUs. We accumulate the gradient of 8 iterations and update the models with a batch of about 65K tokens. The hyperparameters K and \u03b7 in MGRC sampling are tuned on the validation set with the range of K \u2208 {10, 20, 40, 80} and \u03b7 \u2208 {0.15, 0.30, 0.45, 0.6, 0.75, 0.90}. We use the default setup of K = 40 for all three tasks, \u03b7 = 0.6 for both Zh\u2192En and En\u2192De while \u03b7 = 0.45 for En\u2192Fr. For evaluation, the beam size and length penalty are set to 4 and 0.6 for the En\u2192De as well as En\u2192Fr, while 5 and 1.0 for the Zh\u2192En task.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Settings", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Main Results", "text": "Results of Zh\u2192En. Table 1 shows the results on the Chinese-to-English translation task. From the results, we can conclude that our approach outperforms existing augmentation strategies such as back-translation (Sennrich et al., 2016a;Wei et al., 2020a) and switchout (Wang et al., 2018) by a large margin (up to 3.63 BLEU), which verifies that augmentation in continuous space is more effective than methods with discrete manipulations. Compared to the approaches that replace words in the embedding space (Cheng et al., 2020), our approach also demonstrates superior performance, which reveals that sentence-level augmentation with continuous semantics works better on generalizing to unseen instances. Moreover, compared to the vanilla Transformer, our approach consistently   achieves promising improvements on five test sets.\nResults of En\u2192De and En\u2192Fr. From Table 2, our approach consistently performs better than existing methods (Sennrich et al., 2016a;Wang et al., 2018;Wei et al., 2020a;Cheng et al., 2020), yielding significant gains (0.65\u223c1.76 BLEU) on the En\u2192De and En\u2192Fr tasks. An exception is that Nguyen et al. (2020) achieved comparable results with ours via multiple forward and backward NMT models, thus data diversification intuitively demonstrates lower training efficiency. Moreover, we observe that CSANMT gives 30.16 BLEU on the En\u2192De task with the base setting, significantly outperforming the vanilla Transformer by 2.49 BLEU points. Our approach yields a further improvement of 0.68 BLEU by equipped with the wider architecture, demonstrating superiority over the standard Transformer by 2.15 BLEU. Similar observations can be drawn for the En\u2192Fr task.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Analysis", "text": "Effects of K and \u03b7. Figure 4 illustrates how the hyper-parameters K and \u03b7 in MGRC sampling affect the translation quality. From Figures 4(a)-4(c), we can observe that gradually increasing the number of samples significantly improves BLEU scores, which demonstrates large gaps between K = 10 and K = 40. However, assigning larger values (e.g., 80) to K does not result in further improvements among all three tasks. We conjecture that the reasons are two folds: (1) it is fact that the diversity of each training instance is not infinite and thus MGRC gets saturated is inevitable with K increasing. (2) MGRC sampling with a scaled item (i.e., W r ) may degenerate to traverse in the same place. This prompts us to design more sophisticated algorithms in future work. In our experiments, we default set K = 40 to achieve a balance between the training efficiency and translation quality. Figure 4(d) shows the effect of \u03b7 on validation sets, which balances the importance of two Gaussian forms during the sampling process. The setting of \u03b7 = 0.6 achieves the best results on both the Zh\u2192En and En\u2192De tasks, and \u03b7 = 0.45 consistently outperforms other values on the En\u2192Fr task.", "n_publication_ref": 1, "n_figure_ref": 2}, {"heading": "Lexical diversity and semantic faithfulness.", "text": "We demonstrate both the lexical diversity (measured by TTR= num. of types num. of tokens ) of various trans-  lations and the semantic faithfulness of machine translated ones (measured by BLEURT with considering human translations as the references) in Table 4. It is clear that CSANMT substantially bridge the gap of the lexical diversity between translations produced by human and machine. Meanwhile, CSANMT shows a better capability on preserving the semantics of the generated translations than Transformer. We intuitively attribute the significantly increases of BLEU scores on all datasets to these two factors. We also have studied the robustness of CSANMT towards noisy inputs and the translationese effect, see Appendix D for details. Effect of the semantic encoder. We introduce two variants of the semantic encoder to investigate its performance on En\u2192De validation set. Specifically, ( 1) we remove the extra semantic encoder and construct the sentence-level representations by averaging the sequence of outputs of the vanilla sentence encoder. (2) We replace the default 4-layer semantic encoder with a large pre-trained model (PTM) (i.e., XLM-R (Conneau et al., 2020)). The results are reported in Table 3. Comparing line 2 with line 3, we can conclude that an extra semantic encoder is necessary for constructing the universal continuous space among different languages. Moreover, when the large PTM is incorporated, our approach yields further improvements, but it causes massive computational overhead.\nComparison between discrete and continuous augmentations. To conduct detailed compar-   isons between different augmentation methods, we asymptotically increase the training data to analyze the performance of them on the En\u2192De translation. As in Figure 5, our approach significantly outperforms the back-translation method on each subset, whether or not extra monolingual data (Sennrich et al., 2016a) is introduced. These results demonstrate the stronger ability of our approach than discrete augmentation methods on generalizing to unseen instances with the same set of observed data points. Encouragingly, our approach achieves comparable performance with the baseline model with only 25% of training data, which indicates that our approach has great potential to achieve good results with very few data. Effect of MGRC sampling and tangential contrastive learning. To better understand the effectiveness of the MGRC sampling and the tangential contrastive learning, we conduct detailed ablation studies in Table 5. The details of four variants with different objectives or sampling strategies are shown in Appendix C. From the results, we can observe that both removing the recurrent dependence and replacing the Gaussian forms with uniform distributions make the translation quality decline, but the former demonstrates more drops. We also have tried the training objectives with other forms, such as variational inference and cosine similarity, to optimize the semantic encoder. However, the BLEU score drops significantly.\nTraining Cost and Convergence.  shows the evolution of BLEU scores during training. It is obvious that our method performs consistently better than both the vanilla Transformer and the back-translation method at each iteration (except for the first 10K warm-up iterations, where the former one has access to less unique training data than the latter two due to the K times over-sampling). For the vanilla Transformer, the BLEU score reaches its peak at about 52K iterations. In comparison, both CSANMT and the back-translation method require 75K updates for convergence. In other words, CSANMT spends 44% more training costs than the vanilla Transformer, due to the longer time to make the NMT model converge with augmented training instances. This is the same as the back-translation method. Word prediction accuracy. Figure 7 illustrates the prediction accuracy of both frequent and rare words. As expected, CSANMT generalizes to rare words better than the vanilla Transformer, and the gap of word prediction accuracy is as large as 16%. This indicates that the NMT model alleviates the probability under-estimation of rare words via continuous semantic augmentation.  Baselines. In contrast to the vanilla Transformer, CSANMT involves with approximate 20% additional parameters. In this section, we further compare against the baselines with increased amounts of parameters, and investigate the performance of CSANMT equipped with much stronger baselines (e.g. deep and scale Transformers Wei et al., 2020b)). From the results on WMT14 testsets in Table 6, we can observe that CSANMT still outperforms the vanilla Transformer (by more than 1.2 BLEU) under the same amount of parameters, which shows that the additional parameters are not the key to the improvement. Moreover, CSANMT yields at least 0.9 BLEU gains equipped with much stronger baselines. For example, the scale Transformer , which originally gives 29.3 BLEU in the En\u2192De task, now gives 31.37 BLEU with our continuous semantic augmentation strategy. It is important to mention that our method can help models to achieve further improvement, even if they are strong enough.", "n_publication_ref": 2, "n_figure_ref": 2}, {"heading": "Low-Resource Machine Translation", "text": "We further generalize the capability of the proposed CSANMT to various low-resource machine translation tasks, including IWSLT14 English-German and IWSLT17 English-French. The details of the datasets and model configurations can be found in Appendix B. Table 7 shows the results of different models. Compared to the vanilla Transformer, the proposed CSANMT improve the BLEU scores of the two tasks by 2.7 and 2.9 points, respectively.  This result indicates that the claiming of the continuous semantic augmentation enriching the training corpora with very limited observed instances.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Data Augmentation (DA) Kobayashi, 2018;Gao et al., 2019;Khayrallah et al., 2020;Pham et al., 2021) has been widely used in neural machine translation. The most popular one is the family of back-translation (Sennrich et al., 2016a;Nguyen et al., 2020), which utilizes a target-to-source model to translate monolingual target sentences back into the source language. Besides, constructing adversarial training instances with diverse literal forms via word replacing or embedding interpolating (Wang et al., 2018;Cheng et al., 2020) is beneficial to improve the generalization performance of NMT models.\nVicinal Risk Minimization (VRM) (Chapelle et al., 2000) is another principle of data augmentation, in which DA is formalized as extracting additional pseudo samples from the vicinal distribution of observed instances. Typically the vicinity of a training example is defined using datasetdependent heuristics, such as color (scale, mixup) augmentation (Simonyan and Zisserman, 2014;Krizhevsky et al., 2012;Zhang et al., 2018) in computer vision and adversarial augmentation with manifold neighborhoods (Ng et al., 2020;Cheng et al., 2021) in NLP. Our approach relates to VRM that involves with an adjacency semantic region as the vicinity manifold for each training instance.\nSentence Representation Learning is a well investigated area with dozens of methods (Kiros et al., 2015;. In recent years, the methods built on large pre-trained models (Devlin et al., 2019;Conneau et al., 2020) have been widely used for learning sentence level representations (Reimers and Gurevych, 2019;Huang et al., 2019;Yang et al., 2019). Our work is also related to the methods that aims at learning the uni-versal representation (Zhang et al., 2016;Schwenk and Douze, 2017; for multiple semantically-equivalent sentences in NMT. In this context, contrastive learning has become a popular paradigm in NLP (Kong et al., 2020;Clark et al., 2020;Gao et al., 2021). The most related work are  and Chi et al. (2021), which suggested transforming cross-lingual sentences into a shared vector by contrastive objectives.", "n_publication_ref": 26, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We propose a novel data augmentation paradigm CSANMT, which involves with an adjacency semantic region as the vicinity manifold for each training instance. This method is expected to make more unseen instances under generalization with very limited training data. The main components of CSANMT consists of the tangential contrastive learning and the Mixed Gaussian Recurrent Chain (MGRC) sampling. Experiments on both rich-and low-resource machine translation tasks demonstrate the effectiveness of our method.\nIn the future work, we would like to further study the vicinal risk minimization with the combination of multi-lingual aligned scenarios and large-scale monolingual data, and development it as a pure data augmentator merged into the vanilla Transformer. We use the Stanford segmenter (Tseng et al., 2005) for Chinese word segmentation and apply the script tokenizer.pl of Moses (Koehn et al., 2007) for English, German and French tokenization. We measure the performance with the 4gram BLEU score (Papineni et al., 2002). Both the case-sensitive tokenized BLEU (compued by multi-bleu.pl) and the detokenized sacrebleu 3 (Post, 2018) are reported on the En\u2192De and En\u2192Fr tasks. The case-insensitive BLEU is reported on the Zh\u2192En task.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "B Low-Resource Machine Translation", "text": "For the low-resource scenario, we choose the IWSLT14 English-German (En\u2192De) and IWSLT17 English-French (En\u2192Fr) tasks.\nDatasets. For IWSLT14 En\u2192De, there are 160k sentence pairs for training and 7584 sentence pairs for validation. As in previous work (Ranzato et al., 2016;Zhu et al., 2020), the concatenation of dev2010, dev2012, test2010, test2011 and test2012 is used as the test set. For IWSLT17 En\u2192Fr, there are 236k sentence pairs for training and 10263 for validation. The concatenation of test2010, test2011, test2012, test2013, test2014 and test2015 is used as the test set. We use a joint source and target vocabulary with 10k byte-pair-encoding (BPE) types (Sennrich et al., 2016b) for above two tasks.\nModel Settings. The model configuration is transformer_iwslt, representing a 6-layer model with embedding size 512 and FFN layer dimension 1024. We train all models using the Adam optimizer with adaptive learning rate schedule (warm-up step with 4K) as in (Vaswani et al., 2017). During inference, we use beam search with a beam size of 5 and length penalty of 1.0.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "C Variants with Different Objectives or", "text": "Sampling Strategies  \n(i) ,r x \u2032(j) \u03c9 (k) \u223c \u03b7N 0, diag(W 2 r ) + (1.0 \u2212 \u03b7)N 0, 1 2 ditto \u03c9 (k) \u223c \u03b7U \u2212 W r , W r + (1.0 \u2212 \u03b7)U \u0101 \u2212 1, 1 \u2212\u0101 where\u0101 = 1 k\u22121 k\u22121 i=1 \u03c9 (i) 3 E (x (i) ,y (i) )\u223cB \u2212 KL p(r x (i) ) \u2225 q(r x (i) , r y (i) ) r x = \u00b5 + \u03f5 \u2299 \u03c3\nwhere p(r x (i) ) \u223c N (\u00b5, \u03c3 2 ) and q(r\nx (i) , r y (i) ) \u223c N (\u00b5 \u2032 , \u03c3 \u20322 )\nwhere \u03f5 is a standard Gaussian noise  \n4 E (x (i) ,y (i) )\u223cB r T x (i) r y (i) \u2225r x (i) \u2225\u2022\u2225r y (i) \u2225 \u03c9 (k) \u223c \u03b7N 0, diag(W 2 r ) + (1.0 \u2212 \u03b7)N 1 k\u22121 k\u22121 i=1 \u03c9 (i) , 1", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D Robustness on Noisy Inputs and Translationese", "text": "In this section, we study the robustness of our CSANMT towards both noisy inputs and the translationese effect (Volansky et al., 2013) on new-stest2014 for the WMT14 English-German task.\nNoisy Inputs. Inspired by (Gao et al., 2019), we construct noisy test sets via several strategies described as follows:\n\u2022 Original: the original testset without any manipulations;\n\u2022 WS: word swap, randomly swap words in nearby positions within a window size 3 (Artetxe et al., 2018;Lample et al., 2018b);\n\u2022 WD: word dropout, randomly drop words with a ratio of 15% (Iyyer et al., 2015;Lample et al., 2018b);\n\u2022 WR: word replace, randomly replace word tokens with a placeholder token (e.g.,\n[UNK]) (Xie et al., 2017) or with a relevant (measured by the similarity of word embeddings) alternative . The replacement ratio also is 15%. \u2022 natural source \u2192 translationese target (X \u2192 Y * );\n\u2022 translationese source \u2192 natural target (X * \u2192 Y);\n\u2022 round-trip translationese source \u2192 translationese target (X * * \u2192 Y * ), where X \u2192 Y * \u2192 X * * .\nResults. As shown in Table 9, our approach shows better robustness over two baseline methods across various artificial noises. Moreover, CSANMT consistently outperforms the baseline in all three translationese scenarios, the same is true for back-translation. However, Edunov et al. (2020) shows that BT improves only in the X * \u2192 Y scenario. Our explanation for the inconsistency is that BT without monolingual data in our setting benefits from the natural parallel data to deal with the translationese sources. ", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We would like to thank all of the anonymous reviewers (during ARR Oct. and ARR Dec.) for the helpful comments. We also thank Baosong Yang and Dayiheng Liu for their instructive suggestions and invaluable help.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Details of Rich-Resource Datasets", "text": ")) codes with 60K merge operations to build two vocabularies comprising 47K Chinese sub-words and 30K English sub-words. For the En\u2192De task, we employ the popular WMT14 dataset, which consists of approximately 4.5M sentence pairs for training. We select newstest2013 as the validation set and newstest2014 as the test set. All sentences had been jointly byte-pair-encoded with 32K merge operations, which results in a shared source-target vocabulary of about 37K tokens. For the En\u2192Fr task, we use the significantly larger WMT14 dataset consisting of 36M sentence pairs. The combination of {newstest2012, 2013} was used for model selection and the experimental results were reported on newstest2014.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Unsupervised neural machine translation", "journal": "", "year": "2018", "authors": "Mikel Artetxe; Gorka Labaka; Eneko Agirre; Kyunghyun Cho"}, {"title": "Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil", "journal": "", "year": "2018", "authors": "Daniel Cer; Yinfei Yang; Sheng-Yi Kong; Nan Hua; Nicole Limtiaco; Rhomni St; Noah John; Mario Constant; Steve Guajardo-Cespedes;  Yuan"}, {"title": "Vicinal risk minimization", "journal": "MIT Press", "year": "2000", "authors": "Olivier Chapelle; Jason Weston; L\u00e9on Bottou; Vladimir Vapnik"}, {"title": "Robust neural machine translation with doubly adversarial inputs", "journal": "", "year": "2019", "authors": "Yong Cheng; Lu Jiang; Wolfgang Macherey"}, {"title": "AdvAug: Robust adversarial augmentation for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Yong Cheng; Lu Jiang; Wolfgang Macherey; Jacob Eisenstein"}, {"title": "Towards robust neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yong Cheng; Zhaopeng Tu; Fandong Meng; Junjie Zhai; Yang Liu"}, {"title": "Self-supervised and supervised joint training for resource-rich machine translation", "journal": "PMLR", "year": "2021", "authors": "Yong Cheng; Wei Wang; Lu Jiang; Wolfgang Macherey"}, {"title": "Semi-supervised learning for neural machine translation", "journal": "", "year": "2016", "authors": "Yong Cheng; Wei Xu; Zhongjun He; Wei He; Hua Wu; Maosong Sun; Yang Liu"}, {"title": "InfoXLM: An information-theoretic framework for cross-lingual language model pre-training", "journal": "", "year": "2021", "authors": "Zewen Chi; Li Dong; Furu Wei; Nan Yang; Saksham Singhal; Wenhui Wang; Xia Song; Xian-Ling Mao; Heyan Huang; Ming Zhou"}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merri\u00ebnboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"}, {"title": "ELECTRA: pretraining text encoders as discriminators rather than generators", "journal": "", "year": "2020", "authors": "Kevin Clark; Minh-Thang Luong; Quoc V Le; Christopher D Manning"}, {"title": "Unsupervised cross-lingual representation learning at scale", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Understanding back-translation at scale", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Sergey Edunov; Myle Ott; Michael Auli; David Grangier"}, {"title": "On the evaluation of machine translation systems trained with back-translation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Sergey Edunov; Myle Ott; Marc'aurelio Ranzato; Michael Auli"}, {"title": "Data augmentation for low-resource neural machine translation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Marzieh Fadaee; Arianna Bisazza; Christof Monz"}, {"title": "Soft contextual data augmentation for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Fei Gao; Jinhua Zhu; Lijun Wu; Yingce Xia; Tao Qin; Xueqi Cheng; Wengang Zhou; Tie-Yan Liu"}, {"title": "SimCSE: Simple contrastive learning of sentence embeddings", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Tianyu Gao; Xingcheng Yao; Danqi Chen"}, {"title": "Dual learning for machine translation", "journal": "", "year": "2016-12-05", "authors": "Di He; Yingce Xia; Tao Qin; Liwei Wang; Nenghai Yu; Tie-Yan Liu; Wei-Ying Ma"}, {"title": "Revisiting self-training for neural sequence generation", "journal": "", "year": "2020", "authors": "Junxian He; Jiatao Gu; Jiajun Shen; Marc'aurelio Ranzato"}, {"title": "Iterative backtranslation for neural machine translation", "journal": "", "year": "2018", "authors": "Duy Vu Cong; Philipp Hoang; Gholamreza Koehn; Trevor Haffari;  Cohn"}, {"title": "Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Haoyang Huang; Yaobo Liang; Nan Duan; Ming Gong; Linjun Shou; Daxin Jiang; Ming Zhou"}, {"title": "Enhancement of encoder and attention using target monolingual corpora in neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Kenji Imamura; Atsushi Fujita; Eiichiro Sumita"}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Mohit Iyyer; Varun Manjunatha; Jordan Boyd-Graber; Hal Daum\u00e9; Iii "}, {"title": "Simulated multiple reference training improves low-resource machine translation", "journal": "", "year": "2020", "authors": "Huda Khayrallah; Brian Thompson; Matt Post; Philipp Koehn"}, {"title": "Skip-thought vectors", "journal": "", "year": "2015", "authors": "Ryan Kiros; Yukun Zhu; R Russ; Richard Salakhutdinov; Raquel Zemel; Antonio Urtasun; Sanja Torralba;  Fidler"}, {"title": "Contextual augmentation: Data augmentation by words with paradigmatic relations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Sosuke Kobayashi"}, {"title": "Moses: Open source toolkit for statistical machine translation", "journal": "", "year": "2007", "authors": "Philipp Koehn; Hieu Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; Nicola Bertoldi; Brooke Cowan; Wade Shen; Christine Moran; Richard Zens; Chris Dyer; Ond\u0159ej Bojar; Alexandra Constantin; Evan Herbst"}, {"title": "A mutual information maximization perspective of language representation learning", "journal": "", "year": "2020", "authors": "Lingpeng Kong; Cyprien De Masson D'autume; Lei Yu; Wang Ling; Zihang Dai; Dani Yogatama"}, {"title": "Imagenet classification with deep convolutional neural networks", "journal": "", "year": "2012", "authors": "Alex Krizhevsky; Ilya Sutskever; Geoffrey E Hin"}, {"title": "Unsupervised machine translation using monolingual corpora only", "journal": "", "year": "2018", "authors": "Guillaume Lample; Alexis Conneau; Ludovic Denoyer; Marc'aurelio Ranzato"}, {"title": "Unsupervised machine translation using monolingual corpora only", "journal": "", "year": "2018", "authors": "Guillaume Lample; Alexis Conneau; Ludovic Denoyer; Marc'aurelio Ranzato"}, {"title": "Phrasebased & neural unsupervised machine translation", "journal": "", "year": "2018", "authors": "Guillaume Lample; Myle Ott; Alexis Conneau; Ludovic Denoyer; Marc'aurelio Ranzato"}, {"title": "SSMBA: Self-supervised manifold based data augmentation for improving out-of-domain robustness", "journal": "", "year": "2020", "authors": "Nathan Ng; Kyunghyun Cho; Marzyeh Ghassemi"}, {"title": "Data diversification: A simple strategy for neural machine translation", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Xuan-Phi Nguyen; Shafiq Joty; Kui Wu; Ai Ti Aw"}, {"title": "Scaling neural machine translation", "journal": "", "year": "2018", "authors": "Myle Ott; Sergey Edunov; David Grangier; Michael Auli"}, {"title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"title": "Yiming Yang, and Graham Neubig. 2021. Meta back-translation", "journal": "", "year": "", "authors": "Hieu Pham; Xinyi Wang"}, {"title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2014", "authors": "Karen Simonyan; Andrew Zisserman"}, {"title": "Mass: Masked sequence to sequence pretraining for language generation", "journal": "", "year": "2019", "authors": "Kaitao Song; Xu Tan; Tao Qin; Jianfeng Lu; Tie-Yan Liu"}, {"title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014-12-08", "authors": "Ilya Sutskever; Oriol Vinyals; Quoc V Le"}, {"title": "Certain language skills in children; their development and interrelationships", "journal": "", "year": "1957", "authors": " Mildred C Templin"}, {"title": "A conditional random field word segmenter for sighan bakeoff", "journal": "", "year": "2005", "authors": "Huihsin Tseng; Pichuan Chang; Galen Andrew; Daniel Jurafsky; Christopher Manning"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"title": "On the features of translationese. Digital Scholarship in the Humanities", "journal": "", "year": "2013", "authors": "Vered Volansky; Noam Ordan; Shuly Wintner"}, {"title": "Learning deep transformer models for machine translation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Qiang Wang; Bei Li; Tong Xiao; Jingbo Zhu; Changliang Li; Derek F Wong; Lidia S Chao"}, {"title": "SwitchOut: an efficient data augmentation algorithm for neural machine translation", "journal": "", "year": "2018", "authors": "Xinyi Wang; Hieu Pham; Zihang Dai; Graham Neubig"}, {"title": "On learning universal representations across languages", "journal": "", "year": "2021", "authors": "Xiangpeng Wei; Rongxiang Weng; Yue Hu; Luxi Xing; Heng Yu; Weihua Luo"}, {"title": "Uncertaintyaware semantic augmentation for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Xiangpeng Wei; Heng Yu; Yue Hu; Rongxiang Weng; Luxi Xing; Weihua Luo"}, {"title": "Multiscale collaborative deep models for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Xiangpeng Wei; Heng Yu; Yue Hu; Yue Zhang; Rongxiang Weng; Weihua Luo"}, {"title": "Google's neural machine translation system: Bridging the gap between human and machine translation", "journal": "", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; V Quoc; Mohammad Le; Wolfgang Norouzi; Maxim Macherey; Yuan Krikun; Qin Cao; Klaus Gao;  Macherey"}, {"title": "Data noising as smoothing in neural network language models", "journal": "", "year": "2017", "authors": "Ziang Xie; I Sida; Jiwei Wang; Daniel Li; Aiming L\u00e9vy; Dan Nie; Andrew Y Jurafsky;  Ng"}, {"title": "Sentence-level agreement for neural machine translation", "journal": "", "year": "2019", "authors": "Mingming Yang; Rui Wang; Kehai Chen; Masao Utiyama; Eiichiro Sumita; Min Zhang; Tiejun Zhao"}, {"title": "Learning semantic textual similarity from conversations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yinfei Yang; Steve Yuan; Daniel Cer; Sheng-Yi Kong; Noah Constant; Petr Pilar; Heming Ge; Yun-Hsuan Sung; Brian Strope; Ray Kurzweil"}, {"title": "Universal sentence representation learning with conditional masked language model", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Ziyi Yang; Yinfei Yang; Daniel Cer; Jax Law; Eric Darve"}, {"title": "Variational neural machine translation", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Biao Zhang; Deyi Xiong; Jinsong Su; Hong Duan; Min Zhang"}, {"title": "mixup: Beyond empirical risk minimization", "journal": "", "year": "2018", "authors": "Hongyi Zhang; Moustapha Cisse; Yann N Dauphin; David Lopez-Paz"}, {"title": "Hardness-aware deep metric learning", "journal": "", "year": "2019", "authors": "Wenzhao Zheng; Zhaodong Chen; Jiwen Lu; Jie Zhou"}, {"title": "Incorporating bert into neural machine translation", "journal": "", "year": "2020", "authors": "Jinhua Zhu; Yingce Xia; Lijun Wu; Di He; Tao Qin; Wengang Zhou; Houqiang Li; Tieyan Liu"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: The diagram of formulating the adjacency semantic region for the sentence pair (x (i) , y (i) ).", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure3: The geometric diagram of the proposed MGRC sampling. r x and r y are the representations of the source sentence x and the target sentence y, respectively. To construct the augmented sample, a straightforward idea is that: (1) transform the norm or the direction ofr = r y \u2212 r x , formulated as \u03c9 \u2299r (e.g., the black dashed arrow), in which each element \u03c9 i \u2208 [\u22121, 1], and (2) combine r x (or r y ) and the transformation \u03c9 \u2299r a\u015d r x = r x + \u03c9 \u2299r (i.e., the red dashed arrow).", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Datasets. For the Zh\u2192En task, the LDC corpus is taken into consideration, which consists of 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words, respectively. The NIST 2006 dataset is used as the validation set for selecting the best model, and NIST 2002 (MT02), 2003 (MT03), 2004 (MT04), 2005 (MT05), 2008 (MT08) are used as the test sets. For the En\u2192De task, we employ the popular WMT14 dataset, which consists of approximately 4.5M sentence pairs for training. We select newstest2013 as the validation set and newstest2014 as the test set. For the En\u2192Fr task, we use the significantly larger WMT14 dataset consisting of 36M sentence pairs. The combination of {newstest2012, 2013} was used for model selection and the experimental results were reported on newstest2014. Refer to Appendix A for more details.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Effects of K and \u03b7 on validation sets. (a), (b) and (c) depict the BLEU curves with different values of K on Zh\u2192En, En\u2192De and En\u2192Fr, respectively. (d) demonstrates the performances of \u03b7 with different values.", "figure_data": ""}, {"figure_label": "67", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 6 :Figure 7 :67Figure 6: BLEU curves over iterations on the WMT14 English\u2192German test set. Note that back-translation is initialized from the vanilla Transformer.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Method #Params. Valid. MT02 MT03 MT04 MT05 MT08 Avg. Transformer, base (our implementation) 84M 45.09 45.63 45.07 46.59 45.84 36.18 43.86 Back-translation (Sennrich et al., 2016a) * 84M 46.71 47.22 46.86 47.36 46.65 36.69 44.96 SwitchOut (Wang et al., 2018) * 84M 46.13 46.72 45.69 47.08 46.19 36.47 44.43 SemAug (Wei et al.Cheng et al., 2020) -49.26 49.03 47.96 48.86 49.88 39.63 47.07 CSANMT, base 96M 50.46 49.65 48.84 49.80 50.40 41.63 48.06 Table 1: BLEU scores [%] on Zh\u2192En translation. \"Params.\" denotes the number of parameters (M=million). \" * \" indicates the results obtained by our implementation, we construct multiple pseudo sources for each target during back-translation but rather introducing extra monolingual corpora as in(Wei et al., 2020a) for fairer comparisons.", "figure_data": ", 2020a)86M---49.15 49.21 40.94-AdvAug ("}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Table 2: BLEU scores [%] on the WMT14 En\u2192De and En\u2192Fr tasks. \" * \" indicates the results obtained by our implementation, which is the same in Table1. \" \u2020 \" denote estimate values. We further compare against the baselines with increased amounts of parameters, and investigate the performance of CSANMT equipped with much stronger baselines (e.g. deep and scale TransformersWei et al., 2020b)) in Sec. 3.3.", "figure_data": "ModelWMT 2014 En\u2192DeWMT 2014 En\u2192Fr#Params. BLEU SacreBLEU #Params. BLEU SacreBLEUTransformer, base (our implementation)62M 27.6726.867M 40.5338.5Transformer, big (our implementation)213M 28.7927.7222M 42.3640.3Back-Translation (Sennrich et al., 2016a)  *213M 29.2528.2222M 41.7339.7SwitchOut (Wang et al., 2018)  *213M 29.1828.1222M 41.6239.6SemAug (Wei et al., 2020a)221M 30.29-230M 42.92-AdvAug (Cheng et al., 2020)\u2020 65M 29.57----Data Diversification (Nguyen et al., 2020)\u2020 1260M 30.70-\u2020 1332M 43.70-CSANMT, base74M 30.1629.280M 42.4040.3CSANMT, big265M 30.9429.8274M 43.6841.6"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "95% 20.32% 11.76% 0.570 0.635 0.696 CSANMT 7.13% 21.26% 12.91% 0.581 0.684 0.739", "figure_data": "TTRBLEURT ScoreZhDeFrZhDeFrHuman7.58% 22.08% 13.98%---Trans.6."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "TTR (Type-Token-Ratio)(Templin, 1957) and BLEURT scores of Zh\u2192En and En\u2192X translations produced by Human, vanilla Transformer (written as Trans.), and CSANMT. \"Human\" translations mean the references contained in the standard test sets. Refer to Appendix D for the results on robustness test sets.", "figure_data": "# ObjectiveSamplingBLEU1 Default tangential CTLDefault MGRC30.162 Default tangential CTL MGRC w/o recurrent chain 29.643 Default tangential CTLMGRC w/ uniform dist.29.784 Variational InferenceGaussian sampling28.075 Cosine similarityDefault MGRC28.19"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ": BLEU scores [%] on WMT14 testsets forthe English-German (En\u2192De) and English-French(En\u2192Fr) tasks. Superscript  \u2020 denotes the numbers arereported from the paper, others are based on our runs.\"-\" means omitted results because of the limitations ofGPU resources. \"10 layers\" means that we constructthe Transformer with a 10-layer encoder, thus it has thesame amount of parameters as our model."}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "BLEU scores [%] on the IWSLT tasks. For fairer comparison, all the models are implemented by ourselves using the same backbone, and the extra monolingual corpora is not introduced into back-translation.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level training with recurrent neural networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.", "figure_data": "Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processingand the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages3982-3992. Association for Computational Linguis-tics.Holger Schwenk and Matthijs Douze. 2017. Learn-ing joint multilingual sentence representations withneural machine translation. In Proceedings of the2nd Workshop on Representation Learning for NLP,pages 157-167, Vancouver, Canada. Association forComputational Linguistics.Rico Sennrich, Barry Haddow, and Alexandra Birch.2016a. Improving neural machine translation modelswith monolingual data. In Proceedings of the 54thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 86-96.Association for Computational Linguistics.Rico Sennrich, Barry Haddow, and Alexandra Birch.2016b. Neural machine translation of rare wordswith subword units. In Proceedings of the 54th An-nual Meeting of the Association for ComputationalLinguistics, pages 1715-1725, Berlin, Germany. As-sociation for Computational Linguistics."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Variants Training Objective for the Semantic EncoderSampling Strategy for Obtaining Augmented Samples1 E (x (i) ,y (i) )\u223cB log", "figure_data": "describes the details of four variants (intro-duced in Table 5, from row 2 to row 5) with differ-ent objectives or sampling strategies: (1) defaulttangential CTL in Eq. (4) + MGRC w/o recurrentdependence, (2) default tangential CTL in Eq. (4) +MGRC w uniform distribution, (3) variational infer-ence (Zhang et al., 2016) + Gaussian sampling, and(4) cosine similarity + default MGRC sampling."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "The variants of the training objective for the semantic encoder as well as the sampling strategy for obtaining augmented samples.", "figure_data": "ModelNoisy InputsTranslationeseOriginal WS WR X \u2192 Y  Transformer (our implementation) WD 27.67 15.33 18.59 16.98 32.8228.5639.04Back-Translation (our implementation)29.2517.20 20.44 18.7133.0729.7339.86CSANMT30.1620.14 23.76 21.6634.6230.7041.64"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "BLEU scores [%] on noisy inputs and the translationese effect, in the WMT14 En\u2192De setup.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "E Codes of tangential contrastive learning and MGRC sampling E.1 Tangential Contrastive Learning # src_embedding: [batch_size, 1, hidden_size] # trg_embedding: [batch_size, 1, hidden_size] def get_ctl_loss(src_embedding, trg_embedding, dynamic_coefficient): batch_size = tf.shape(src_embedding)[0] def get_ctl_logits(query, keys): # expand_query: [batch_size, batch_size, hidden_size] # expand_keys: [batch_size, batch_size, hidden_size] # the current ref is the positive key, while others in the training batch are negative ones expand_query = tf.tile(query, [1, batch_size, 1]) expand_keys = tf.tile(tf.transpose(keys, [1,0,2]), [batch_size, 1, 1]) # distances between queries and positive keys d_pos = tf.sqrt(tf.reduce_sum(tf.pow(query -keys, 2.0), axis=-1)) # [batch_size, 1] d_pos = tf.tile(d_pos, [1, batch_size]) # [batch_size, batch_size] d_neg = tf.sqrt(tf.reduce_sum(tf.pow(expand_query -expand_keys, 2..expand_dims(tf.range(batch_size, dtype=tf.int32), axis=1) labels = tf.one_hot(labels, depth=2 * batch_size, on_value=1.0, off_value=0.0) cross_entropy_fn = tf.nn.softmax_cross_entropy_with_logits loss = tf.reduce_mean(cross_entropy_fn(logits=logits, labels=labels)) return loss", "figure_data": "0), axis=-1)) # [batch_size, batch_size]lambda_coefficient = (d_pos / d_neg) ** dynamic_coefficienthardness_masks = tf.cast(tf.greater(d_neg, d_pos), dtype=tf.float32)hard_keys =(expand_query + tf.expand_dims(lambda_coefficient, axis=2) * (expand_keys -expand_query)) * \\tf.expand_dims(hardness_masks, axis=2) + expand_keys * tf.expand_dims(1.0 -hardness_masks, axis=2)logits = tf.matmul(query, hard_keys, transpose_b=True) # [batch_size, 1, batch_size]return logitslogits_src_trg = get_ctl_logits(src_embedding, trg_embedding)logits_trg_src = get_ctl_logits(trg_embedding, src_embedding) + tf.expand_dims(tf.matrix_band_part(tf.ones([batch_size,batch_size]), 0, 0) * -1e9, axis=1)logits = tf.concat([logits_src_trg, logits_trg_src], axis=2) # [batch_size, 1, 2 * batch_size]labels = tfE.2 MGRC Sampling# src_embedding: [batch_size, hidden_size]# trg_embedding: [batch_size, hidden_size]# default: K=20 and eta = 0.6def mgmc_sampling(src_embedding, trg_embedding, K, eta):batch_size = tf.shape(src_embedding)[0]def get_samples(x_vector, y_vector):bias_vector = y_vector -x_vectorW_r = (tf.abs(bias_vector) -tf.reduce_min(tf.abs(bias_vector), axis=1, keep_dims=True)) / \\(tf.reduce_max(tf.abs(bias_vector), 1, keep_dims=True) -tf.reduce_min(tf.abs(bias_vector), 1, keep_dims=True))# initializing the set of samplesR = []omega = tf.random_normal(tf.shape(bias_vector), 0, W_r)sample = x_vector + tf.multiply(omega, bias_vector)R.append(sample)for i in xrange(1, K):chain = [tf.expand_dims(item, axis=1) for item in R[:i]]average_omega = tf.reduce_mean(tf.concat(chain, axis=1), axis=1)omega = eta * tf.random_normal(tf.shape(bias_vector), 0, W_r) + \\(1.0 -eta) * tf.random_normal(tf.shape(bias_vector), average_omega, 1.0)sample = x_vector + tf.multiply(omega, bias_vector)R.append(sample)return Rx_sample = get_samples(src_embedding, trg_embedding)y_sample = get_samples(trg_embedding, src_embedding)return x_sample.extend(y_sample)"}], "doi": "10.18653/v1/2020.acl-main.253"}