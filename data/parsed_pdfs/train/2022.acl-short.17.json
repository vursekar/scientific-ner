{"authors": "Nathan Schucher; Siva Reddy;  Harm De Vries; Servicenow Research", "pub_date": "", "title": "The Power of Prompt Tuning for Low-Resource Semantic Parsing", "abstract": "Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks. In this paper, we investigate prompt tuning for semantic parsing-the task of mapping natural language utterances onto formal meaning representations. On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines. We also conduct ablation studies across different model scales and target representations, finding that, with increasing model scale, prompt tuned T5 models improve at generating target representations that are far from the pre-training distribution.", "sections": [{"heading": "Introduction", "text": "With the widespread success of pre-trained language models (LMs; Devlin et al. 2019;Raffel et al. 2020;Bommasani et al. 2021), it becomes increasingly important to explore how such models can be adapted to downstream tasks. One adaptation method which has recently attracted much attention is prompt design (Brown et al., 2020;Shin et al., 2020), which modulates the behaviour of a LM through a task description and a few inputoutput examples. Brown et al. (2020) show that this adaptation strategy is increasingly effective for larger LMs. However, prompt design is sensitive to the exact phrasing of the prompt, and, more importantly, performs worse than fine-tuning models on task-specific examples (Lester et al., 2021).\nPrompt tuning has recently arisen as a strong performing alternative adaption method (Lester et al., 2021). Rather than hand-designing discrete prompts, prompt tuning optimizes the embeddings of a number of task-specific prompt tokens. In contrast to fine-tuning, this method keeps almost all LM parameters frozen. On a set of language understanding tasks, Lester et al. (2021) show that prompt tuning becomes competitive with finetuning for the largest pre-trained T5 models (Raffel et al., 2020). Li and Liang (2021) also explore a related parameter-efficient adaptation method called prefix-tuning, finding that it outperforms fine-tuning on low-resource natural language generation tasks.\nIn this paper, we investigate prompt tuning for semantic parsing. This task is fundamentally different from the aforementioned language understanding and generation tasks, as it requires that models output formal meaning representations which do not resemble the natural language distribution seen during pre-training. In particular, we focus on the low-resource setup because examples for semantic parsing are difficult and expensive to collect (Wang et al., 2015;Marzoev et al., 2020). We therefore evaluate prompt tuning on two datasets: the 200-shot version of Overnight (Wang et al., 2015;Shin et al., 2021) and the low-resource splits TOPv2 (Chen et al., 2020). On both datasets, we compare prompt tuning T5 against fine-tuning and investigate the effect of canonicalizing the meaning representation, i.e. to what extent naturalizing the logical forms influences performance. In addition, we study the effect of T5 model scale on Overnight as well as varying data regimes on TOPv2. Our main findings can be summarized as follows:\n\u2022 For large T5 models, prompt tuning significantly outperforms fine-tuning in the low-data regime, resulting in an absolute improvement of 6% and 15% on Overnight and TOPv2, respectively. This performance gap decreases when more training data becomes available.\n\u2022 With growing model size, prompt tuned T5 models are increasingly capable of outputting diverse target representations (see Figure 1). On Overnight, we find that the disparity between canonical and meaning representations shrinks from 17% to 4% for T5-small and T5-xl, respectively. On TOPv2, prompt tuned T5-large models are much better at generating out-of-vocabulary tokens than T5-small.", "n_publication_ref": 16, "n_figure_ref": 1}, {"heading": "Related work", "text": "Our work is related to recent work on semantic parsing and prompt tuning, which we briefly describe below.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Semantic Parsing", "text": "Semantic parsing is the task of converting a natural language utterance u = (u 1 , . . . , u N ) to a formal meaning representation z = (z 1 , . . . , z M ). These meaning representations, also referred to as logical forms, can be interpreted by machines and executed in a real environment. For example, ThingTalk (Campagna et al., 2019) and TOP (Gupta et al., 2018) are meaning representations for executing commands of virtual assistants, while SQL is a representation for interacting with relational databases. In recent years, neural sequence-to-sequence models have become the dominant approach for semantic parsing tasks (Dong and Lapata, 2016).\nCanonicalization A common simplification step in semantic parsing is to canonicalize the meaning representations. That is, the meaning representation z is naturalized to a canonical form c through a grammar or set of rules. Examples of the meaning and canonical representation for Overnight and TOPv2 (Wang et al., 2015;Chen et al., 2020) can be found in Fig. 2.\nWhen canonical representations are available, Berant and Liang (2014) argue that semantic parsing can be seen as a paraphrase task. They propose to use a paraphrase model-using e.g. word vectors trained on Wikipedia-to find the best paraphrase of utterance u among a set of canonical utterances. They show this paraphrase model improves results over directly generating logical forms on two question-answering datasets. Marzoev et al. (2020) extends this work by showing that pre-trained language models like BERT can be effective paraphrasers. While Berant and Liang (2014); Marzoev et al. (2020) use models to score canonical utterances, Shin et al. (2021) propose to constrain the generation process of autoregressive models like BART and GPT-3. On a number of few-shot semantic parsing tasks, they demonstrate the benefit of generating canonical representations over meaning representations.", "n_publication_ref": 10, "n_figure_ref": 1}, {"heading": "Prompt-tuning", "text": "Lester et al. (2021) evaluates prompt tuning on SuperGLUE, a benchmark consisting of eight language understanding tasks. They find that prompt tuning becomes competitive with fine-tuning for the largest T5 model. Li and Liang (2021) propose prefix-tuning to adapt BART and GPT-2 for natural language generation tasks. This method differs from Lester et al. (2021) in that it prepends trainable embeddings for each layer of the language model rather than introducing token embeddings at the input layer. They demonstrate that pre-fix outperforms fine-tuning baselines. Similarly, Liu et al. (2021) also show encouraging results for prompt tuning on natural language understand and generation tasks. Qin and Eisner (2021) also explores prompt tuning but for a knowledge extraction task. Inserting general adapter layers into pre-trained language models is also proposed in Houlsby et al. (2019); Mahabadi et al. (2021). Related to our work are also other few-shot adaptation techniques like PET (Schick and Sch\u00fctze, 2021). Moreover, adapter layers have also been explored in the computer vision domain (Rebuffi et al., 2017;de Vries et al., 2017).", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Experiments", "text": "To evaluate low-resource prompt tuning, we compare against fine-tuned variants of the same model on two semantic parsing datasets with canonical representations available. We compare both large  and small variants of the T5 architecture on these datasets and experiment with various canonicalized representations.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets", "text": "Overnight The Overnight semantic parsing dataset (Wang et al., 2015) consists of 13,682 natural utterance, canonical form, meaning representation triples split across eight domains. To simulate low-resource splits of this dataset, we follow Shin et al. and create randomly subsampled splits of 200 training examples for each domain, using 20% of the remaining data for validation. We measure and report denotation accuracy by evaluating all predicted queries using the SEMPRE toolkit (Berant et al., 2013). We repeat each experiment on Overnight with five different random splits.\nTOPv2 Chen et al. (2020) introduce the TOPv2 dataset, a task-oriented semantic parsing dataset with eight domains, two of which come with predefined low-resource splits. The authors propose a principled way of constructing low-resource training sets, samples per intent and slot (SPIS), intended to ensure equal exposure to ontology labels across domains of varying complexity. We experiment with the weather and reminder domains at the 10, 25, and 500 SPIS resource splits, performing five runs on each model varying the random seed. The reminder domain is the most challenging with 19 intent labels, 32 slot labels, and with 21% of the programs having a depth greater than 2. Weather in comparison has 7 intent labels, 11 slot labels, and no programs with depth greater than 2.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Canonicalized Representations", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Overnight", "text": "Overnight uses a context-free synchronous grammar to generate canonical representations for the logical forms. As can be seen in Fig. 2, these canonical representations resemble natural language.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "TOPv2", "text": "Chen et al. apply a set of simple modifications to the TOPv2 meaning representations to arrive at a canonical form used in all their experiments. Unlike Overnight, these pre-processing steps are largely small encoding differences and do not change the syntactic structure of the logical forms. We adopt all of these canonicalization steps (except for lexicographic sorting of the semantic parse tree) and add an ontology label shortening step. Examples of these transformations can be seen in Fig. 2 and are briefly described below.\nSimplify removes redundant utterance tokens unnecessary for interpreting the meaning representation.\nOut-of-Vocab adds the entire intent or slot label to the tokenizer as a new single tokens with a corresponding randomly initialized embedding.\nIn-Vocab replaces the intent and slot labels with a short unique identifier representable by the pre-trained tokenizer.\nWe perform an ablation over these canonicalization choices, repeating each experiment three times with varying random seed.  For each domain, we report the average over 5 runs trained on randomly sampled splits of 200 examples for fine-tuned (FT) and prompt tuned (PT) models.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Models", "text": "We provide training details and hyperparameters for all models in Appendix A. Below, we briefly explain the prompt-tuning methodology.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Prompt Tuning", "text": "Prompt tuning, as proposed by Lester et al. (2021), prepends a sequence of continuous embeddings p = (p 1 , . . . , p K ) to the sequence input embeddings e(u) = (e(u 1 ), . . . , e(u N )) before feeding it to a language model with parameters \u03b8. During prompt tuning we optimize the prompt embeddings (p 1 , . . . , p K ) exclusively, keeping the language model parameters \u03b8 and the pretrained vocabulary embeddings fixed. Note that this process still requires backpropagating gradients through the full language model. Like fine-tuning models, we maximize the likelihood of generating the output sequence z.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Results", "text": "In  (2020). In Table 4, we summarize the results of the canonicalization ablation study for TOPv2.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Prompt tuning vs fine tuning", "text": "We find that prompt tuning improves over finetuning for all large model configurations and target representations. On Overnight, prompt tuned denotation accuracy exceeds fine-tuned counterparts by up to 5 points with T5-large and T5-xl. For T5-small and T5-base, prompt tuning remains competitive (within 1% average accuracy) with fine-tuning when predicting canonical forms. On TOPv2, prompt tuning achieves an absolute improvement of 15% mean accuracy over fine-tuning on the lowest SPIS split. This performance disparity lessens when training data increases; however, prompt tuned T5-large continues to beat its finetuned counterpart by 5 points at 500 SPIS and the BART-CopyPtr model by 1.4 points.\nOur prompt tuning models outperform previously reported results on these datasets. On Overnight, our best model-T5-xl PT with canonical representations and constrained decodingoutperforms the BART FT model of Shin et al. (2021) by 5 accuracy points, and GPT-3 by more than 2 points. On the 25 SPIS split of TOPv2, we see an average improvement of more than 5 points compared to the BART-CopyPTR of Chen et al. (2020).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Canonical vs meaning representations", "text": "Our main finding is that prompt tuned T5 models become better at generating meaning representations with increased model size. On Overnight, we see the absolute difference between canonical and meaning representations shrink from 17.5 points   for T5-small to 3.4 points for T5-xl (Table 1). This gap shrinks another 18% to 2.8 points when we apply constrained decoding to T5-xl (Table 2). By contrast, Shin et al. (2021) reports an 11.7 point difference when prompting GPT-3. For our finetuning baselines, we observe a small performance gap of 4 points across target representations for BART and T5-xl, while we observe no gap for T5-small, T5-base, and T5-large models.\nIn our TOPv2 experiments we find similar evidence of large T5 model flexibility for generating sequences far from the training distribution. In particular, for our most intrusive canonicalization scheme Out-of-Vocab, which adds novel tokens to the vocabulary and leaves these embeddings un-trained, we find no significant reduction in performance for T5-large across all data resource levels. T5-small, in comparison, sees almost a 50% drop in performance relative to no canonicalization (None) at the 10 SPIS level and continues to underperform by 33 % at the 500 SPIS level.\nInterestingly, we find that In-Vocab drastically reduces performance for T5-small at the 10 SPIS level-30.9% vs. 43.4% for None-but slightly outperforms it at 500 SPIS. We speculate that In-Vocab effectively anonymizes the ontol-  ogy tokens, obscuring information that is useful for prediction. In low-data regimes there is not enough training data to learn the semantics of these anonymized tokens, whereas with enough data this problem vanishes.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We find that prompt tuning is an effective method for adapting language models to the semantic parsing task. Prompt tuning significantly outperforms fine-tuning in low-data regimes, and remains competitive in the fully supervised setting. We furthermore find that while canonicalizing meaning representations can slightly improve performance, the disparity between target representations decreases when prompt tuning larger T5 models. This result differs from previous work (Shin et al., 2021) which suggested that pre-trained LMs are much better equipped to output canonical than meaning representations. However, a significant limitation of prompt tuning is that it takes more time to converge than fine-tuning. We believe one fruitful direction for future research is to find ways to reduce the compute required to prompt tune.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Ethical Considerations and Limitations", "text": "There are two main limitations of this work. The first is the limited analysis of the learned prompts.\nWhile concurrent work has shown that interpreting prompts is a difficult task, it is still an important consideration and left for future work (Khashabi et al., 2021). Secondly, training prompts on meaning representations requires substantially more compute than fine-tuning. This may exacerbate inequalities in regions where access to data and compute are similarly limited (Ahia et al., 2021).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A Models", "text": "Here we provide all model details and hyperparameters to reproduce our results. We experiment with BART and T5 (Lewis et al., 2020;Raffel et al., 2020), two large pre-trained encoder-decoder language models. BART is trained on the same 160GB text dataset used to train RoBERTa (Lewis et al., 2020) (Paszke et al., 2019;Wolf et al., 2020).\nFine-tuning baseline We compare against baselines that fine-tune all parameters of BART and T5. We train the T5 models with AdaFactor (Shazeer and Stern, 2018) and BART with Adam (Lewis et al., 2020;Kingma and Ba, 2015). On TOPv2, we use a learning rate of 10 \u22124 and batch size of 128. On Overnight, we use a learning rate of 10 \u22123 and a batch size of 64 across all sizes of T5. On both datasets, we train for 5000 epochs and perform model selection by early stopping on the validation set.\nPrompt tuning We follow the prompt tuning procedure proposed by Lester et al. for T5. We use 150 prompt tokens for all model sizes with a learning rate of 0.3 optimized with AdaFactor. We train for 5000 epochs on most domains, although it sometimes took as many as 20000 epochs to converge on the low-resource splits. Like the fine-tuned baseline, we perform model selection with best exact match accuracy on the validation set. We apply the same method to BART and found that it did not converge under a number of hyperparameter configurations. We therefore exclude prompt tuned BART models from our results 1 .\nConstrained Decoding We implement grammarconstrained decoding by building a prefix tree containing all canonical or meaning representations in the dataset as in Shin et al. (2021). When doing constrained decoding we perform a beam search with 10 beams and use the prefix tree to look up valid single token continuations of the decoded sequence.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "B Results", "text": "For completeness, we provide all Overnight results in Table 5.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B.1 Training Times", "text": "Prompt tuned parameter efficiency comes at a cost: we find that prompt tuning takes significantly longer to train with early stopping than does finetuning. On the Overnight dataset, fine-tuned models typically took 250 epochs before validation performance plateaued. Our prompt tuned models frequently took more than 1000 epochs when predicting canonical representations, and up to 5,000 when predicting meaning representations. In Figure 3, we show example training curves for prompt tuning and fine-tuning.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": " ", "text": "5\n: Results across all model size, target representation, tuning method, and decoding method for Overnight dataset. BART, GPT-2, and GPT-3 results results are included from Shin et al. (2021) ", "n_publication_ref": 1, "n_figure_ref": 0}], "references": [{"title": "The low-resource double bind: An empirical study of pruning for low-resource machine translation", "journal": "", "year": "2021", "authors": "Orevaoghene Ahia; Julia Kreutzer; Sara Hooker"}, {"title": "Semantic parsing on Freebase from question-answer pairs", "journal": "", "year": "2013", "authors": "J Berant; A Chou; R Frostig; P Liang"}, {"title": "Semantic parsing via paraphrasing", "journal": "Long Papers", "year": "2014", "authors": "Jonathan Berant; Percy Liang"}, {"title": "", "journal": "", "year": "", "authors": "Rishi Bommasani; Drew A Hudson; Ehsan Adeli; Russ Altman; Simran Arora;  Sydney Von Arx; S Michael"}, {"title": "", "journal": "", "year": "", "authors": "Jeannette Bernstein; Antoine Bohg; Emma Bosselut; Erik Brunskill; Shyamal Brynjolfsson; Dallas Buch; Rodrigo Card; Niladri Castellon; Annie S Chatterji"}, {"title": "On the opportunities and risks of foundation models", "journal": "CoRR", "year": "2021", "authors": "Kathleen Chen; Jared Quincy Creel; Dorottya Davis; Chris Demszky; Moussa Donahue; Esin Doumbouya; Stefano Durmus; John Ermon; Kawin Etchemendy; Li Ethayarajh; Chelsea Fei-Fei; Trevor Finn; Lauren Gale; Karan Gillespie; Noah D Goel; Shelby Goodman; Neel Grossman; Tatsunori Guha; Peter Hashimoto; John Henderson; Daniel E Hewitt; Jenny Ho; Kyle Hong; Jing Hsu; Thomas Huang; Saahil Icard; Dan Jain; Pratyusha Jurafsky; Siddharth Kalluri; Geoff Karamcheti; Fereshte Keeling; Omar Khani; Pang Wei Khattab; Mark S Koh; Ranjay Krass; Rohith Krishna;  Kuditipudi"}, {"title": "Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners", "journal": "", "year": "", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"title": "Genie: A generator of natural language semantic parsers for virtual assistant commands", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Giovanni Campagna; Silei Xu; Mehrad Moradshahi; Richard Socher; Monica S Lam"}, {"title": "Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing", "journal": "", "year": "2020", "authors": "Xilun Chen; Asish Ghoshal; Yashar Mehdad; Luke Zettlemoyer; Sonal Gupta"}, {"title": "Modulating early visual processing by language", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Florian Harm De Vries; Jeremie Strub; Hugo Mary; Olivier Larochelle; Aaron C Pietquin;  Courville"}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Language to logical form with neural attention", "journal": "Long Papers", "year": "2016", "authors": "Li Dong; Mirella Lapata"}, {"title": "Semantic parsing for task oriented dialog using hierarchical representations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Sonal Gupta; Rushin Shah; Mrinal Mohit"}, {"title": "Parameter-efficient transfer learning for nlp", "journal": "PMLR", "year": "2019", "authors": "Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin De Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly"}, {"title": "Tushar Khot, Ashish Sabharwal, and Yejin Choi. 2021. PROMPT WAYWARDNESS: The Curious Case of Discretized Interpretation of Continuous Prompts", "journal": "", "year": "", "authors": "Daniel Khashabi; Shane Lyu; Sewon Min; Lianhui Qin; Kyle Richardson; Sameer Singh; Sean Welleck; Hannaneh Hajishirzi"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "journal": "", "year": "2021", "authors": "Brian Lester; Rami Al-Rfou; Noah Constant"}, {"title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "journal": "Long Papers", "year": "2021", "authors": "Lisa Xiang; Percy Li;  Liang"}, {"title": "Zhilin Yang, and Jie Tang. 2021. Gpt understands, too", "journal": "", "year": "", "authors": "Xiao Liu; Yanan Zheng; Zhengxiao Du; Ming Ding; Yujie Qian"}, {"title": "Compacter: Efficient lowrank hypercomplex adapter layers", "journal": "", "year": "2021", "authors": "James Rabeeh Karimi Mahabadi; Sebastian Henderson;  Ruder"}, {"title": "Unnatural language processing: Bridging the gap between synthetic and natural language data", "journal": "", "year": "2004", "authors": "Alana Marzoev; M Frans Samuel Madden; Michael J Kaashoek; Jacob Cafarella;  Andreas"}, {"title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga; Alban Desmaison; Andreas Kopf; Edward Yang; Zachary Devito; Martin Raison; Alykhan Tejani; Sasank Chilamkurthy; Benoit Steiner; Lu Fang; Junjie Bai; Soumith Chintala"}, {"title": "Learning how to ask: Querying lms with mixtures of soft prompts", "journal": "", "year": "2021", "authors": "Guanghui Qin; Jason Eisner"}, {"title": "", "journal": "", "year": "", "authors": "Colin Raffel; Noam Shazeer"}, {"title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Wei Zhou; Peter J Li;  Liu"}, {"title": "Learning multiple visual domains with residual adapters", "journal": "", "year": "2017", "authors": "S-A Rebuffi; H Bilen; A Vedaldi"}, {"title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference", "journal": "", "year": "2021", "authors": "Timo Schick; Hinrich Sch\u00fctze"}, {"title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost", "journal": "PMLR", "year": "2018", "authors": "Noam Shazeer; Mitchell Stern"}, {"title": "Constrained Language Models Yield Few-Shot Semantic Parsers", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Richard Shin; Christopher Lin; Sam Thomson; Charles Chen; Subhro Roy; Adam Emmanouil Antonios Platanios; Dan Pauls; Jason Klein; Benjamin Eisner;  Van Durme"}, {"title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Taylor Shin; Yasaman Razeghi; Robert L Logan; I V ; Eric Wallace; Sameer Singh"}, {"title": "Building a Semantic Parser Overnight", "journal": "Long Papers", "year": "2015", "authors": "Yushi Wang; Jonathan Berant; Percy Liang"}, {"title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander Lhoest;  Rush"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: We show that the T5 prompt tuning performance difference between target representations shrinks as the number of parameters increase, with constrained decoded T5-xl achieving close to performance parity.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Examples from the TOPv2 and Overnight datasets with the corresponding canonicalization schemes.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Prompt and fine-tuned exact match validation accuracy on the Overnight blocks domain. Fine-tuned models can quickly reach peak validation accuracy regardless of target representation. Prompt tuned models can take thousands of epochs to converge when predicting meaning representations.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Unconstrained denotation accuracy for all models (with unconstrained decoding) on the Overnight dataset.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ", we report Overnight results across fourT5 model scales and two target representations.In Table 2, we add constrained decoding (see Ap-pendix A) to our best performing T5 model andcompare against previously reported Overnight re-"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Constrained denotation accuracy for all models on the Overnight dataset. For each domain, we report the average over 5 runs trained on randomly sampled splits of 200 examples. \u2020 denotes results reported byShin et al. (2021). * indicates performance on subsampled test set.", "figure_data": "SPISModelMethodReminderWeatherAverage10T5-largeFT0.3920.5790.486PT0.5670.7000.63425BART-CopyPtrFT0.5570.7160.637T5-largeFT0.5020.6830.593PT0.6420.7390.691500BART-CopyPtrFT0.7190.8490.784T5-largeFT0.6490.8460.748PT0.7490.8470.798"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Average exact match accuracies (5 runs) for different low-resource splits of the TOPv2 dataset. BART-CopyPtr results fromChen et al. (2020).", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ": Exact match accuracies (3 runs) on TOPv2Weather domain for different meaning representationcanonicalization choices (bold indicates best exactmatch accuracy at that resource level), Sm. and Lg.refer to T5-small and T5-large, respectively."}], "doi": "10.3115/v1/P14-1133"}