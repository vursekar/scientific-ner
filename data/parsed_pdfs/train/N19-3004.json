{"authors": "Jiatong Li; Kai Zheng; Hua Xu; Qiaozhu Mei; Yue Wang", "pub_date": "", "title": "The Strength of the Weakest Supervision: Topic Classification Using Class Labels", "abstract": "When developing topic classifiers for realworld applications, we begin by defining a set of meaningful topic labels. Ideally, an intelligent classifier can understand these labels right away and start classifying documents. Indeed, a human can confidently tell if a news article is about science, politics, sports, or none of the above, after knowing just the class labels. We study the problem of training an initial topic classifier using only class labels. We investigate existing techniques for solving this problem and propose a simple but effective approach. Experiments on a variety of topic classification data sets show that learning from class labels can save significant initial labeling effort, essentially providing a \"free\" warm start to the topic classifier.", "sections": [{"heading": "Introduction", "text": "When developing topic classifiers for real-world tasks, such as news categorization, query intent detection, and user-generated content analysis, practitioners often begin by crafting a succinct definition, or a class label, to define each class. Unfortunately, these carefully written class labels are completely ignored by supervised topic classification models. Given a new task, these models typically require a significant amount of labeled documents to reach even a modest initial performance. In contrast, a human can readily understand new topic categories by reading the class definitions and making connections to prior knowledge. Labeling initial examples for every new task can be time-consuming and laborintensive, especially in resource-constrained domains like medicine and law. Therefore it is desirable if a topic classifier can proactively interpret class labels before the training starts, giving itself a \"warm start\". An imperfect initial model can always be fine-tuned with more labeled documents. As conceptually shown in Figure 1, a warm start can reduce the total number of training labels for a classifier to reach certain performance level.\nIn this work, we study algorithms that can initialize a topic classifier using class labels only. Since class labels are the starting point of any topic classification task, they can be viewed as the earliest hence weakest supervision signal. We propose a simple and effective approach that combines word embedding and naive Bayes classification. On six topic classification data sets, we evaluate a suite of existing approaches and the proposed approach. Experimental results show that class labels can train a topic classifier that generalizes as well as a classifier trained on hundreds to thousands of labeled documents.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Related Work", "text": "Text retrieval. Classifying documents by short labels can be viewed as evaluating textual similarity between a document and a label. Baeza-Yates et al. (2011) called this approach \"naive text classification\". Treating labels as search queries, we can classify a document into a class if it best matches the label of that class. Well-studied text retrieval methods, such as vector space models and probabilistic models (Croft et al., 2010), can produce matching scores. To mitigate vocabulary mismatch, such a classifier can be further enhanced by self-training: the classifier assigns pseudo labels to top-ranked documents as done in pseudo relevance feedback (Rocchio, 1965), and updates itself using those labels.\nSemi-supervised learning. Our problem setting can be seen as an extreme case of weak supervision: we only use class labels as the (noisy) supervision signal, and nothing else. If we view class labels as \"labeled documents\", one from each class, and to-be-classified documents as unlabeled documents, then we cast the problem as semisupervised learning (Zhu, 2006). Self-training is one such technique: a generative classifier is trained using only class labels, and then teaches itself using its own predictions on unlabeled data. If we view class labels as \"labeled features\", then we expect the classifier to predict a class when a document contains the class label words. For instance, Druck et al. (2008) proposed generalized expectation criteria that uses feature words (class labels) to train a discriminative classifier. Jagarlamudi et al. (2012) and Hingmire and Chakraborti (2014) proposed Seeded LDA to incorporate labeled words/topics into statistical topic modeling. The inferred document-topic mixture probabilities can be used to classify documents.\nZero-shot learning aims to classify visual objects from a new class using only word descriptions of that class (Socher et al., 2013). It first learns visual features and their correspondence with word descriptions, and then constructs a new classifier by composing learned features. Most research on zero-shot learning focuses on image classification, but the same principle applies to text classification as well (Pushp and Srivastava, 2017). Our proposed method constructs a new classifier by composing learned word embeddings in a probabilistic manner. Since the new classifier transfers semantic knowledge in word embedding to topic classification tasks, it is broadly related to transfer learning (Pan and Yang, 2010). The main difference is that in transfer learning the information about the new task is in the form of labeled data, not class definition words.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Proposed Method", "text": "Let a test document x be a sequence of words (w 1 , \u2022 \u2022 \u2022 , w j , \u2022 \u2022 \u2022 ), and a class topic description y be a sequence of words\nd y = (w 1 , \u2022 \u2022 \u2022 , w y , \u2022 \u2022 \u2022 ).\nAll words are in vocabulary V . We propose a generative approach, where the predictive probabil-ity p(y|x) \u221d p(x|y)p(y). Generative approaches tends to perform well when training data is scarce, which is the case in our setting.\nWe assume there exists weak prior knowledge on which classes are popular and which are rare. We can then construct rough estimatesp(y) using simple heuristics as described in (Schapire et al., 2002). It distributes probability mass q evenly among majority classes, and 1 \u2212 q evenly among minority classes. We treat the most frequent class as the majority class, the rest as minority classes, and q = 0.7 in our experiments.\nBy interpreting class topic description as words, we obtainp(x|y) = p(x|d y ). We assume that the d y expresses a noisy-OR relation of the words it contains (Oni\u015bko et al., 2001). Up to first-order approximation:\np(x|d y ) = 1 \u2212 wy\u2208dy (1 \u2212 p(x|w y )) \u2248 wy\u2208dy p(x|w y ),(1)\nwhere each w y is a word in the class topic description d y . Further, we assume that words in document x are conditionally independent given a label word w y (na\u00efve Bayes assumption):\np(x|w y ) = w j \u2208x p(w j |w y ).\n(2)\nCombining ( 1) and ( 2), the document likelihood i\u015d p(x|y) = wy\u2208dy w j \u2208x p(w j |w y ).\nTo this end, we need a word association model p(w 1 |w 2 ), \u2200w 1 , w 2 \u2208 V . It can be efficiently learned by word embedding algorithms. The skipgram algorithm  learns vector representations of words, such that for words w 1 , w 2 , their vectors u w 1 , v w 2 approximate the conditional probability 1\np(w 1 |w 2 ) = exp u w 1 v w 2 w\u2208V exp (u w v w 2 ) .(4)\nCombining ( 3) with ( 4), the document likelihood become\u015d\np(x|y) = wy\u2208dy exp \uf8eb \uf8ed w j \u2208x u w j v wy \u2212 C wy \uf8f6 \uf8f8 ,\nwhere C wy = log w\u2208V exp u w v wy is independent of document x and only related to label word w y , therefore can be precomputed and stored to save computation. Finally, we construct an generative classifier a\u015d p(y|x) \u221dp(x|y)p(y). We call this method word embedding na\u00efve Bayes (WENB).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Continued Training", "text": "The proposed method produces pseudo label\u015d p(y|x j ) for unlabeled documents {x j } m j=1 . When true labels {(x i , y i )} n i=1 are available, we can train a new discriminative logistic regression classifier p \u03b8 (y|x) using both true and pseudo labels (\u03b8 is the model parameter):\nJ(\u03b8) = n i=1 y\u2208Y \u22121 {y i =y} log p \u03b8 (y|x i ) + \u03bb \u03b8 2 + \u00b5 m j=1 y\u2208Y \u2212p(y|x j ) log p \u03b8 (y|x j ) .(5)\nTo find the balance of pseudo vs. true labels in (5), we search the hyperparameter \u00b5 on a 5point grid {10 \u22122 , 10 \u22121 , 0.4, 0.7, 1}. We expect pseudo labels to have comparable importance as true labels when n is small (fine granularity for \u00b5 \u2208 [10 \u22121 , 1]), and their importance will diminish as n gets large (\u00b5 = 10 \u22122 ). \u00b5 is automatically selected such that it gives the best 5-fold crossvalidation accuracy on n true labels.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We compare a variety of methods on six topic classification data sets. The goals are (1) to study the best classification performance achievable using class labels only, and (2) to estimate the equivalent amount of true labels needed to achieve the same warm-start performance.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Compared Methods", "text": "Retrieval-based methods. We use language modeling retrieval function with Dirichlet smoothing (Zhai and Lafferty, 2001) (\u00b5 = 2500) to match a document to class labels (IR). The top 10 results are then used as pseudo-labeled documents to retrain three classifiers: IR+Roc: a Rocchio classifier (\u03b1 = 1, \u03b2 = 0.5, \u03b3 = 0); IR+NB: a multinomial naive Bayes classifier (Laplace smoothing, \u03b1 = 0.01); IR+LR a logistic regression classifier (linear kernel, C = 1).\nSemi-supervised methods. ST-0: the initial self-training classifier using class labels as \"training documents\" (multinomial na\u00efve Bayes, Laplace smoothing \u03b1 = 0.01). ST-1: ST-0 retrained on 10 most confident documents predicted by itself. GE: a logistic regression classifier trained using generalized expectation criteria (Druck et al., 2008). Class labels are used as labeled features. sLDA: a supervised topic model trained using seeded LDA (Jagarlamudi et al., 2012). Besides k seeded topics (k is the number of classes), we use an extra topic to account for other content in the corpus.\nWord embedding-based methods. Cosine: a centroid-based classifier, where class definitions and documents are represented as average of word vectors. WENB: The proposed method (Section 3). WENB+LR: a logistic regression classifier trained only on pseudo labels produced by WENB (Section 3.1, n = 0).\nFor general domain tasks, we take raw text from English Wikipedia, English news crawl (WMT, 2014), and 1 billion word news corpus (Chelba et al., 2013) to train word vectors. For medical domain tasks, we take raw text from MEDLINE abstracts (NLM, 2018) to train word vectors. We find 50-dimensional skip-gram word vectors perform reasonably well in the experiments.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Data Sets", "text": "We consider six topic classification data sets with different document lengths and application domains. Table 1 summarizes basic statistics of these data sets. Table 4 and     Three short text data sets are (1) Wiki Titles: Wikipedia article titles sampled from 15 main categories (Wikipedia Main Topic). (2) News Titles: The UCI news title data set (Lichman, 2013). (3) Y Questions: User-posted questions in Yahoo Answers (Yahoo Language Data, 2007).\nThree long text data sets are (1) 20 News: The well-known 20 newsgroup data set. (2) Reuters. The Reuters-21578 data set (Lewis). We take the articles from the 10 largest topics. (3) Med WSD: The MeSH word sense disambiguation (WSD) data set (Jimeno-Yepes et al., 2011).\nEach WSD task aims to tell the sense (meaning) of an ambiguous term in a MEDLINE abstract. For instance, the term \"cold\" may refer to Low Temperature, Common Cold, or Chronic Obstructive Lung Disease, depending on its context. These senses are used as the class labels. We use 198 ambiguous words with at least 100 labeled abstracts in the data set, and report the average statistics over 198 independent classification tasks.\nAlthough no true labels are used for training, some methods require unlabeled data for retrieval, pseudo-labeling, and re-training. We split unlabeled data into 5 folds, using 4 folds to \"train\" a classifier and 1 fold for test. We use macroaveraged F 1 as the performance metric because not all data sets have a balanced class distribution.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Results and Discussion", "text": "Label savings. Table 2 shows that overall, class labels can train text classifiers remarkably better than majority guess. This is no small feat considering that the classifier has not seen any labeled documents yet. Such performance gain essentially comes \"for free\", as any text classification task has to start by defining classes. In Table 3, we report the number of true labels needed for a logistic regression model to achieve the same performance as WENB+LR. The most significant savings happen on short documents: class labels are equivalent to hundreds to thousands of labeled documents at the beginning of the training process.\nEffect of document length. On short documents (Wiki Titles, News Titles, Y Questions), leveraging unlabeled data does not help with most semi-supervised methods due to severe vocabulary mismatch. The proposed methods (WENB and WENB+LR) show robust performance, because pretrained word vectors can capture semantic similarity even without any word overlap between a class label and a document. This prior knowledge is essential when documents are short. On long documents (20 News, Reuters, Med WSD), leveraging unlabeled data helps, since long documents have richer content and are more likely to contain not only label words themselves, but also other topic-specific words. Retrieval-based and semi-supervised methods are able to learn these words by exploiting intra-document word co-occurrences.\nPerformance of other methods. Learning from class labels themselves provides very limited help (IR and ST-0). Using class labels as search queries and labeled documents are closely related: IR and ST-0 perform similarly; so do IR+NB and ST-1. When using class labels as search queries,   2 shows a salient warm-start effect on a balanced binary classification task in 20 News. The weight \u00b5 of pseudo labels increases when true labels are few (initial classifier as an informative prior). As expected, \u00b5 decreases when true labels become abundant.\nFigure 3 shows another binary classification task in 20 News where the warm-start effect is limited. Correspondingly, \u00b5 quickly diminishes as more true labels are available. With 100 or more true labels, pseudo labels have a negligible weight (\u00b5 = 10 \u22122 ). In machine learning terms, these pseudo labels specify an incorrect prior that the model should quickly forget, so that it will not hinder the overall learning process. A closer investigation reveals that the word vector for mideast (the class label of one topic in Figure 3) is not well-trained. This is because in general text corpus, the word mideast is rather infrequent compared to commonly used alternatives, such as middle east. The word vector of mideast is surrounded by other infrequent words or misspellings (such as hizballah, jubeir, saudis, isreal) as opposed to more frequent and relevant ones (such as israel, israeli, saudi, arab). Since WENB uses the semantic knowledge in word vectors to infer pseudo labels, the quality of class label word vectors will affect the pseudo label accuracy.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "Conclusion and Future Directions", "text": "We studied the problem of training topic classifiers using only class labels. Experiments on six data sets show that class labels can save a significant amount of labeled examples in the beginning. Retrieval-based and semi-supervised methods tend to perform better on long documents, while the proposed method performs better on short documents.\nThis study opens up many interesting avenues for future work. First, we introduce a new perspective on text classification: can we build a text classifier by just providing a short description of each class? This is a more challenging (but more user-friendly) setup than standard supervised classification. Second, future work can investigate tasks such as sentiment and emotion classification, which are more challenging than topic classification tasks. Third, the two approaches -leveraging unlabeled data (retrievalbased and semi-supervised methods) and leveraging pretrained models (the proposed method)could be combined to give robust performance on both short and long documents. Finally, we can invite users into the training loop: in addition to labeling documents, users can also revise the class definitions to improve the classifier.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their helpful comments. This work was in part supported by the National Library of Medicine under grant number 2R01LM010681-05. Qiaozhu Mei's work was supported in part by the National Science Foundation under grant numbers 1633370 and 1620319. Yue Wang would like to thank the support of the Eleanor M. and Frederick G. Kilgour Research Grant Award by the UNC-CH School of Information and Library Science.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "", "journal": "WMT 2014 English News Crawl", "year": "2014", "authors": ""}, {"title": "8.3.2 Naive Text Classification, chapter 8", "journal": "Addison-Wesley", "year": "2011", "authors": "Ricardo Baeza-Yates; Berthier De Ara\u00fajo Neto Ribeiro"}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "journal": "", "year": "2013", "authors": "Ciprian Chelba; Tomas Mikolov; Mike Schuster; Qi Ge"}, {"title": "Retrieval Models", "journal": "Addison-Wesley Reading", "year": "2010", "authors": "Bruce Croft; Donald Metzler; Trevor Strohman"}, {"title": "Learning from labeled features using generalized expectation criteria", "journal": "ACM", "year": "2008", "authors": "Gregory Druck; Gideon Mann; Andrew Mccallum"}, {"title": "Topic labeled text classification: a weakly supervised approach", "journal": "ACM", "year": "2014", "authors": "Swapnil Hingmire; Sutanu Chakraborti"}, {"title": "Incorporating lexical priors into topic models", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "Jagadeesh Jagarlamudi; Hal Daum\u00e9; Iii ; Raghavendra Udupa"}, {"title": "Exploiting mesh indexing in medline to generate a data set for word sense disambiguation", "journal": "BMC bioinformatics", "year": "2011", "authors": "J Antonio; Bridget T Jimeno-Yepes; Alan R Mcinnes;  Aronson"}, {"title": "", "journal": "", "year": "", "authors": "David D Lewis"}, {"title": "of california, school of information and computer science", "journal": "", "year": "2013", "authors": "M Lichman"}, {"title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"title": "Learning bayesian network parameters from small data sets: Application of noisy-or gates", "journal": "International Journal of Approximate Reasoning", "year": "2001", "authors": "Agnieszka Oni\u015bko; J Marek; Hanna Druzdzel;  Wasyluk"}, {"title": "A survey on transfer learning", "journal": "IEEE Transactions on knowledge and data engineering", "year": "2010", "authors": "Qiang Sinno Jialin Pan;  Yang"}, {"title": "Train once, test anywhere: Zeroshot learning for text classification", "journal": "", "year": "2017", "authors": ""}, {"title": "Relevance feedback in information retrieval, report no. ISR-9 to the National Science Foundation", "journal": "", "year": "1965", "authors": "J J Rocchio"}, {"title": "Incorporating prior knowledge into boosting", "journal": "", "year": "2002", "authors": "Marie Robert E Schapire; Mazin Rochery; Narendra Rahim;  Gupta"}, {"title": "Zero-shot learning through cross-modal transfer", "journal": "", "year": "2013", "authors": "Richard Socher; Milind Ganjoo; D Christopher; Andrew Manning;  Ng"}, {"title": "Category:Main topic classifications", "journal": "", "year": "", "authors": " Wikipedia Main Topic"}, {"title": "Yahoo! Answers Comprehensive Questions and Answers version 1.0 (multi part", "journal": "", "year": "2007", "authors": ""}, {"title": "A study of smoothing methods for language models applied to ad hoc information retrieval", "journal": "", "year": "2001", "authors": "Chengxiang Zhai; John Lafferty"}, {"title": "Semi-supervised learning literature survey", "journal": "", "year": "2006", "authors": "Xiaojin Zhu"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Learning from class labels can give \"warm start\" to a classifier, accelerating the learning process.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Continued training behavior: Atheism vs. Autos. Colored band: \u00b11 standard deviation.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Continued training behavior: Medical vs. Mideast. Colored band: \u00b11 standard deviation.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "5 in the appendix show actual class labels used in each data set.", "figure_data": "Data setAvg word/doc # classes # docsWiki Titles3.1 (1.1)1530,000News Titles6.7 (9.5)4422,937Y Questions 5.0 (2.6)101,460,00020 News101.6 (438.5)2018,846Reuters76.5 (117.3)108,246Med WSD202.8 (46.6)2/task190/task"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "Wiki Titles News Titles Y Questions 20 NewsReutersMed WSDMajority guess .8313.261.82.486.4734.20IR3.14 (.25)14.20 (.06)6.15 (.06)19.57 (.95)8.37 (.55)52.99 (.64)IR+Roc2.93 (.24)14.20 (.06)8.35 (1.12)25.09 (.93)19.33 (1.87)59.89 (.54)IR+NB5.44 (.53)32.98 (2.13) 14.45 (.45)30.45 (1.46) 62.59 (2.43)82.12 (.41)IR+LR3.26 (.30)13.44 (.10)7.38 (2.08)34.76 (1.50) 6.48 (.07)68.35 (.38)ST-03.16 (.32)16.03 (.16)6.15 (.02)19.49 (.98)6.79 (.17)69.11 (.26)ST-15.62 (.29)24.34 (.36)10.02 (.49)22.91 (1.29) 55.77 (1.62)82.97 (.56)GE9.55 (.90)14.54 (.08)31.72 (.05)48.71 (.41)21.65 (27.36) 62.63 (.37)sLDA7.07 (0.97)51.16 (8.10) 40.98 (2.61)24.80 (4.98) 30.61 (4.80)69.81 (1.09)Cosine27.67 (.59)33.49 (.11)31.16 (.03)26.19 (.75)6.56 (.16)32.65 (.19)WENB26.70 (.48)63.02 (.10)44.89 (.06)32.23 (.48)34.99 (1.99)68.27 (.20)WENB+LR24.88 (.39)63.76 (.11)45.69 (.09)30.57 (.71)32.04 (1.44)62.57 (.19)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Macro-averaged F1 (%) of compared methods on different data sets. The numbers are \"mean (standard deviation)\" of 5-fold cross validation. Top two numbers in each column are highlighted in boldface.", "figure_data": "Data set# of labelsWiki Titles1500News Titles200Y Questions 1500-200020 News100-200Reuters100-200Med WSD20/task \u00d7 198 tasks"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": Number of true labels needed for a logisticregression classifier to achieve the same performanceas \"WENB+LR\"."}], "doi": ""}