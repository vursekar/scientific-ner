{"authors": "Fangyu Liu; Ehsan Shareghi; Zaiqiao Meng; Marco Basaldella; Nigel Collier", "pub_date": "", "title": "Self-Alignment Pretraining for Biomedical Entity Representations", "abstract": "Despite the widespread success of selfsupervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SAPBERT, a pretraining scheme that selfaligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipelinebased hybrid systems, SAPBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without taskspecific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BIOBERT, SCIBERT and PUB-MEDBERT, our pretraining scheme proves to be both effective and robust. 1   ", "sections": [{"heading": "Introduction", "text": "Biomedical entity 2 representation is the foundation for a plethora of text mining systems in the medical domain, facilitating applications such as literature search (Lee et al., 2016), clinical decision making (Roberts et al., 2015) and relational knowledge discovery (e.g. chemical-disease, drug-drug and protein-protein relations, Wang et al. 2018). The heterogeneous naming of biomedical concepts * Work conducted prior to joining Amazon. 1 For code and pretrained models, please visit: https: //github.com/cambridgeltl/sapbert.\n2 In this work, biomedical entity refers to the surface forms of biomedical concepts, which can be a single word (e.g. fever), a compound (e.g. sars-cov-2) or a short phrase (e.g. abnormal retinal vascular development). poses a major challenge to representation learning. For instance, the medication Hydroxychloroquine is often referred to as Oxichlorochine (alternative name), HCQ (in social media) and Plaquenil (brand name).", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "PUBMEDBERT + SAPBERT", "text": "PUBMEDBERT\nMEL addresses this problem by framing it as a task of mapping entity mentions to unified concepts in a medical knowledge graph. 3 The main bottleneck of MEL is the quality of the entity representations (Basaldella et al., 2020). Prior works in this domain have adopted very sophisticated text pre-processing heuristics (D'Souza and Ng, 2015;Kim et al., 2019;Ji et al., 2020;Sung et al., 2020) which can hardly cover all the variations of biomedical names. In parallel, self-supervised learning has shown tremendous success in NLP via leveraging the masked language modelling (MLM) objective to learn semantics from distributional representations Liu et al., 2019). Domain-specific pretraining on biomedical corpora (e.g. BIOBERT, Lee et al. 2020 and BIOMEGA-TRON, Shin et al. 2020) have made much progress in biomedical text mining tasks. Nonetheless, representing medical entities with the existing SOTA pretrained MLMs (e.g. PUBMEDBERT, Gu et al. 2020) as suggested in Fig. 1 (left) does not lead to a well-separated representation space.\nTo address the aforementioned issue, we propose to pretrain a Transformer-based language model on the biomedical knowledge graph of UMLS (Bodenreider, 2004), the largest interlingua of biomedical ontologies. UMLS contains a comprehensive collection of biomedical synonyms in various forms (UMLS 2020AA has 4M+ concepts and 10M+ synonyms which stem from over 150 controlled vocabularies including MeSH, SNOMED CT, RxNorm, Gene Ontology and OMIM). 4 We design a selfalignment objective that clusters synonyms of the same concept. To cope with the immense size of UMLS, we sample hard training pairs from the knowledge base and use a scalable metric learning loss. We name our model as Self-aligning pretrained BERT (SAPBERT).\nBeing both simple and powerful, SAPBERT obtains new SOTA performances across all six MEL benchmark datasets. In contrast with the current systems which adopt complex pipelines and hybrid components (Xu et al., 2020;Ji et al., 2020;Sung et al., 2020), SAPBERT applies a much simpler training procedure without requiring any pre-or post-processing steps. At test time, a simple nearest neighbour's search is sufficient for making a prediction. When compared with other domain-specific pretrained language models (e.g. BIOBERT and SCIBERT), SAPBERT also brings substantial improvement by up to 20% on accuracy across all tasks. The effectiveness of the pretraining in SAP-BERT is especially highlighted in the scientific language domain where SAPBERT outperforms previous SOTA even without fine-tuning on any MEL datasets. We also provide insights on pretraining's impact across domains and explore pretraining with fewer model parameters by using a recently introduced ADAPTER module in our training scheme.\nFigure 2: The distribution of similarity scores for all sampled PUBMEDBERT representations in a minibatch. The left graph shows the distribution of + andpairs which are easy and already well-separated. The right graph illustrates larger overlap between the two groups generated by the online mining step, making them harder and more informative for learning.", "n_publication_ref": 15, "n_figure_ref": 2}, {"heading": "Method: Self-Alignment Pretraining", "text": "We design a metric learning framework that learns to self-align synonymous biomedical entities. The framework can be used as both pretraining on UMLS, and fine-tuning on task-specific datasets. We use an existing BERT model as our starting point. In the following, we introduce the key components of our framework.\nFormal Definition. Let (x, y) \u2208 X \u00d7 Y denote a tuple of a name and its categorical label. For the self-alignment pretraining step, X \u00d7 Y is the set of all (name, CUI 5 ) pairs in UMLS, e.g. (Remdesivir, C4726677); while for the finetuning step, it is formed as an entity mention and its corresponding mapping from the ontology, e.g. (scratchy throat, 102618009). Given any pair of tuples (x i , y i ), (x j , y j ) \u2208 X \u00d7 Y, the goal of the self-alignment is to learn a function f (\u2022; \u03b8) : X \u2192 R d parameterised by \u03b8. Then, the similarity f (x i ), f (x j ) (in this work we use cosine similarity) can be used to estimate the resemblance of x i and x j (i.e., high if x i , x j are synonyms and low otherwise). We model f by a BERT model with its output [CLS] token regarded as the representation of the input. 6 During the learning, a sampling procedure selects the informative pairs of training samples and uses them in the pairwise metric learning loss function (introduced shortly).\nOnline Hard Pairs Mining. We use an online hard triplet mining condition to find the most informative training examples (i.e. hard positive/negative pairs) within a mini-batch for efficient training, Fig. 2. For biomedical entities, this step can be particularly useful as most examples can be easily classified while a small set of very hard ones cause the most challenge to representation learning. 7 We start from constructing all possible triplets for all names within the mini-batch where each triplet is in the form of (x a , x p , x n ). Here x a is called anchor, an arbitrary name in the minibatch; x p a positive match of x a (i.e. y a = y p ) and x n a negative match of x a (i.e. y a = y n ). Among the constructed triplets, we select out all triplets that violate the following condition:\nf (x a ) \u2212 f (x p ) 2 < f (x a ) \u2212 f (x n ) 2 + \u03bb, (1)\nwhere \u03bb is a pre-set margin. In other words, we only consider triplets with the negative sample closer to the positive sample by a margin of \u03bb. These are the hard triplets as their original representations were very far from correct. Every hard triplet contributes one hard positive pair (x a , x p ) and one hard negative pair (x a , x n ). We collect all such positive & negative pairs and denote them as P, N . A similar but not identical triplet mining condition was used by Schroff et al. (2015) for face recognition to select hard negative samples. Switching-off this mining process, causes a drastic performance drop (see Tab. 2).\nLoss Function. We compute the pairwise cosine similarity of all the BERT-produced name representations and obtain a similarity matrix S \u2208 R |X b |\u00d7|X b | where each entry S ij corresponds to the cosine similarity between the i-th and j-th names in the mini-batch b. We adapted the Multi-Similarity loss (MS loss, Wang et al. 2019), a SOTA metric learning objective on visual recognition, for learning from the positive and negative pairs:\nL = 1 |X b | |X b | i=1 1 \u03b1 log 1 + n\u2208N i e \u03b1(S in \u2212 ) + 1 \u03b2 log 1 + p\u2208P i e \u2212\u03b2(S ip \u2212 ) ,(2)\nwhere \u03b1, \u03b2 are temperature scales; is an offset applied on the similarity matrix; P i , N i are indices of positive and negative samples of the anchor i. 8 While the first term in Eq. 2 pushes negative pairs away from each other, the second term pulls positive pairs together. This dynamic allows for a re-calibration of the alignment space using the semantic biases of synonymy relations. The MS loss leverages similarities among and between positive and negative pairs to re-weight the importance of the samples. The most informative pairs will receive more gradient signals during training and thus can better use the information stored in data.", "n_publication_ref": 5, "n_figure_ref": 1}, {"heading": "Experiments and Discussions", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Setups", "text": "Data Preparation Details for UMLS Pretraining. We download the full release of UMLS 2020AA version. 9 We then extract all English entries from the MRCONSO.RFF raw file and convert all entity names into lowercase (duplicates are removed). Besides synonyms defined in MRCONSO.RFF, we also include tradenames of drugs as synonyms (extracted from MRREL.RRF). After pre-processing, a list of 9,712,959 (name, CUI) entries is obtained. However, random batching on this list can lead to very few (if not none) positive pairs within a mini-batch. To ensure sufficient positives present in each mini-batch, we generate offline positive pairs in the format of (name 1 , name 2 , CUI) where name 1 and name 2 have the same CUI label. This can be achieved by enumerating all possible combinations of synonym pairs with common CUIs. For balanced training, any concepts with more than 50 positive pairs are randomly trimmed to 50 pairs. In the end we obtain a training list with 11,792,953 pairwise entries.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "UMLS Pretraining Details.", "text": "During training, we use AdamW (Loshchilov and Hutter, 2018) with a learning rate of 2e-5 and weight decay rate of 1e-2. Models are trained on the prepared pairwise UMLS data for 1 epoch (approximately 50k iterations) with a batch size of 512 (i.e., 256 pairs per mini-batch). We train with Automatic Mixed Precision (AMP) 10 provided in PyTorch 1.7.0. This takes approximately 5 hours on our machine (con-4231 scientific language social media language model NCBI BC5CDR-d BC5CDR-c MedMentions AskAPatient COMETA @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 @1 @5  the improvement comparing to the base model (the deeper the more). Bottom: SAPBERT vs. SOTA results. Blue and red denote unsupervised and supervised models. Bold and underline denote the best and second best results in the column. \" \u2020 \" denotes statistically significant better than supervised SOTA (T-test, \u03c1 < 0.05). On COMETA, the results inside the parentheses added the supervised SOTA's dictionary back-off technique (Basaldella et al., 2020). \"-\": not reported in the SOTA paper. \"OOM\": out-of-memory (192GB+).\nfigurations specified in App. \u00a7B.4). For other hyperparameters used, please view App. \u00a7C.2.\nEvaluation Data and Protocol. We experiment on 6 different English MEL datasets: 4 in the scientific domain (NCBI, Dogan et al. 2014;BC5CDR-c and BC5CDR-d, Li et al. 2016;MedMentions, Mohan and Li 2018) and 2 in the social media domain (COMETA, Basaldella et al. 2020 andAskAPatient, Limsopatham andCollier 2016). Descriptions of the datasets and their statistics are provided in App. \u00a7A. We report Acc @1 and Acc @5 (denoted as @1 and @5) for evaluating performance. In all experiments, SAPBERT denotes further pretraining with our self-alignment method on UMLS. At the test phase, for all SAPBERT models we use nearest neighbour search without further fine-tuning on task data (unless stated otherwise). Except for numbers reported in previous papers, all results are the average of five runs with different random seeds.\nFine-Tuning on Task Data. The red rows in Tab. 1 are results of models (further) fine-tuned on the training sets of the six MEL datasets. Similar to pretraining, a positive pair list is generated through traversing the combinations of mention and all ground truth synonyms where mentions are from the training set and ground truth synonyms are from the reference ontology. We use the same optimiser and learning rates but train with a batch size of 256 (to accommodate the memory of 1 GPU). On scientific language datasets, we train for 3 epochs while on AskAPatient and COMETA we train for 15 and 10 epochs respectively. For BIOSYN on social media language datasets, we empirically found that 10 epochs work the best. Other configurations are the same as the original BIOSYN paper.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Main Results and Analysis", "text": "*BERT + SAPBERT (Tab. 1, top). We illustrate the impact of SAPBERT pretraining over 7 existing BERT-based models (*BERT = {BIOBERT, PUBMEDBERT, ...}). SAPBERT obtains consistent improvement over all *BERT models across all datasets, with larger gains (by up to 31.0% absolute Acc @1 increase) observed in the social media domain. While SCIBERT is the leading model before applying SAPBERT, PUBMEDBERT+SAPBERT performs the best afterwards.\nSAPBERT vs. SOTA (Tab. 1, bottom). We take PUBMEDBERT+SAPBERT (w/wo fine-tuning) and compare against various published SOTA results (see App. \u00a7C.1 for a full listing of 10 baselines) which all require task supervision. For the scientific language domain, the SOTA is BIOSYN (Sung et al., 2020). For the social media domain, the SOTA are Basaldella et al. (2020) and GEN-RANK (Xu et al., 2020) on COMETA and AskAPatient respectively. All these SOTA methods combine BERT with heuristic modules such as tf-idf, string matching and information retrieval system (i.e. Apache Lucene) in a multi-stage manner.\nMeasured by Acc @1 , SAPBERT achieves new SOTA with statistical significance on 5 of the 6 datasets and for the dataset (BC5CDR-c) where SAPBERT is not significantly better, it performs on par with SOTA (96.5 vs. 96.6). Interestingly, on scientific language datasets, SAPBERT outperforms SOTA without any task supervision (fine-tuning mostly leads to overfitting and performance drops). On social media language datasets, unsupervised SAPBERT lags behind supervised SOTA by large margins, highlighting the well-documented complex nature of social media language (Baldwin et al., 2013;Collier, 2015, 2016;Basaldella et al., 2020;Tutubalina et al., 2020). However, after fine-tuning on the social media datasets (using the MS loss introduced earlier), SAPBERT outperforms SOTA significantly, indicating that knowledge acquired during the selfaligning pretraining can be adapted to a shifted domain without much effort.\nThe ADAPTER Variant. As an option for parameter efficient pretraining, we explore a variant of SAPBERT using a recently introduced training module named ADAPTER (Houlsby et al., 2019). While maintaining the same pretraining scheme with the same SAPBERT online mining + MS loss, instead of training from the full model of PUBMEDBERT, we insert new ADAPTER layers between Transformer layers of the fixed PUBMEDBERT, and only train the weights of these ADAPTER layers. In our experiments, we use the enhanced ADAPTER configuration by Pfeiffer et al. (2020). We include two variants where trained parameters are 13.22% and 1.09% of the full SAPBERT variant. The ADAPTER variant of SAPBERT achieves comparable performance to full-model-tuning in scientific datasets but lags behind in social media datasets, Tab. 1. The results indicate that more parameters are needed in pretraining for knowledge transfer to a shifted domain, in our case, the social media datasets.\nThe Impact of Online Mining (Eq. (1)). As suggested in Tab. 2, switching off the online hard pairs mining procedure causes a large performance drop in @1 and a smaller but still significant drop in @5. This is due to the presence of many easy and already well-separated samples in the mini-batches. These uninformative training examples dominated the gradients and harmed the learning process.  Integrating SAPBERT in Existing Systems. SAPBERT can be easily inserted into existing BERT-based MEL systems by initialising the systems with SAPBERT pretrained weights. We use the SOTA scientific language system, BIOSYN (originally initialised with BIOBERT weights), as an example and show the performance is boosted across all datasets (last two rows, Tab. 1).", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We present SAPBERT, a self-alignment pretraining scheme for learning biomedical entity representations. We highlight the consistent performance boost achieved by SAPBERT, obtaining new SOTA in all six widely used MEL benchmarking datasets. Strikingly, without any fine-tuning on task-specific labelled data, SAPBERT already outperforms the previous supervised SOTA (sophisticated hybrid entity linking systems) on multiple datasets in the scientific language domain. Our work opens new avenues to explore for general domain self-alignment (e.g. by leveraging knowledge graphs such as DBpedia). We plan to incorporate other types of relations (i.e., hypernymy and hyponymy) and extend our model to sentence-level representation learning. In particular, our ongoing work using a combination of SAPBERT and ADAPTER is a promising direction for tackling sentence-level tasks. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Evaluation Datasets Details", "text": "We divide our experimental datasets into two categories (1) scientific language datasests where the data is extracted from scientific papers and (2) social media language datasets where the data is coming from social media forums like Reddit.com.\nFor an overview of the key statistics, see Tab. 3.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 Scientific Language Datasets", "text": "NCBI disease (Dogan et al., 2014) is a corpus containing 793 fully annotated PubMed abstracts and 6,881 mentions. The mentions are mapped into the MEDIC dictionary (Davis et al., 2012). We denote this dataset as \"NCBI\" in our experiments.\nBC5CDR (Li et al., 2016) consists of 1,500 PubMed articles with 4,409 annotated chemicals, 5,818 diseases and 3,116 chemical-disease interactions. The disease mentions are mapped into the MEDIC dictionary like the NCBI disease corpus.\nThe chemical mentions are mapped into the Comparative Toxicogenomics Database (CTD) (Davis et al., 2019) chemical dictionary. We denote the disease and chemical mention sets as \"BC5CDRd\" and \"BC5CDR-c\" respectively. For NCBI and BC5CDR we use the same data and evaluation protocol by Sung et al. (2020). 11 MedMentions (Mohan and Li, 2018) is a verylarge-scale entity linking dataset containing over 4,000 abstracts and over 350,000 mentions linked to UMLS 2017AA. According to Mohan and Li (2018), training TAGGERONE , a very popular MEL system, on a subset of MedMentions require >900 GB of RAM. Its massive number of mentions and more importantly the used reference ontology (UMLS 2017AA has 3M+ concepts) make the application of most MEL systems infeasible. However, through our metric learning formulation, SAPBERT can be applied on MedMentions with minimal effort.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "A.2 Social-Media Language Datasets", "text": "AskAPatient (Limsopatham and Collier, 2016) includes 17,324 adverse drug reaction (ADR) annotations collected from askapatient.com blog posts. The mentions are mapped to 1,036 medical concepts grounded onto SNOMED-CT (Donnelly, 2006) and AMT (the Australian Medicines Terminology). For this dataset, we follow the 10-fold evaluation protocol stated in the original paper. 12 COMETA (Basaldella et al., 2020) is a recently released large-scale MEL dataset that specifically focuses on MEL in the social media domain, containing around 20k medical mentions extracted from health-related discussions on reddit.com.\nMentions are mapped to SNOMED-CT. We use the \"stratified (general)\" split and follow the evaluation protocol of the original paper. 13", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "B Model & Training Details B.1 The Choice of Base Models", "text": "We list all the versions of BERT models used in this study, linking to the specific versions in Tab. 5. Note that we exhaustively tried all official variants of the selected models and the best performing ones are chosen. All BERT models refer to the BERT Base architecture in this paper.  S denotes the set of all surface forms / synonyms of all concepts in C; M denotes the set of mentions / queries. COMETA (s.g.) and (z.g.) are the stratified (general) and zeroshot (general) split respectively. model NCBI BC5CDR-d BC5CDR-c MedMentions AskAPatient COMETA @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 SIEVE-BASED (D'Souza and Ng, 2015) 84.7 -84.1 -90.7 ---WORDCNN (Limsopatham and Collier, 2016) --------81.4 ---WORDGRU+TF-IDF (Tutubalina et al., 2018) --------85.7 ---TAGGERONE  87.7 -88.9 -94.1 -OOM OOM ----NORMCO (Wright et al., 2019) 87.8 -88.0 ---------BNE (Phan et al., 2019) 87.7 -90.6 -95.8 -------BERTRANK (Ji et al., 2020) 89.  ", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "B.2 Comparing Loss Functions", "text": "We use COMETA (zeroshot general) as a benchmark for selecting learning objectives. Note that this split of COMETA is different from the stratified-general split used in Tab. 4. It is very challenging (so easy to see the difference of the performance) and also does not directly affect the model's performance on other datasets. The results are listed in Tab. 6. Note that online mining is switched on for all models here.  (Basaldella et al., 2020) 64.6 74.6 NCA loss (Goldberger et al., 2005) 65.2 77.0 Lifted-Structure loss (Oh Song et al., 2016) 62.0 72.1 InfoNCE (Oord et al., 2018;He et al., 2020) 63.3 74.2 Circle loss (Sun et al., 2020) 66.7 78.7\nMulti-Similarity loss (Wang et al., 2019) 67.2 80.3  Schumacher et al. (2020) for clinical concept linking. InfoNCE has been very popular in selfsupervised learning and contrastive learning (Oord et al., 2018;He et al., 2020). Lifted-Structure loss (Oh Song et al., 2016) and NCA loss (Goldberger et al., 2005) are two very classic metric learning objectives. Multi-Similarity loss (Wang et al., 2019) and Circle loss (Sun et al., 2020) are two recently proposed metric learning objectives and have been considered as SOTA on large-scale visual recognition benchmarks.", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "B.3 Details of ADAPTERs", "text": "In Tab. 7 we list number of parameters trained in the three ADAPTER variants along with full-modeltuning for easy comparison.\nBIOBERT (Lee et al., 2020) https://huggingface.co/dmis-lab/biobert-v1.1 BLUEBERT (Peng et al., 2019) https://huggingface.co/bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12 CLINICALBERT (Alsentzer et al., 2019) https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT SCIBERT (Beltagy et al., 2019) https://huggingface.co/allenai/scibert_scivocab_uncased UMLSBERT (Michalopoulos et al., 2020) https://www.dropbox.com/s/qaoq5gfen69xdcc/umlsbert.tar.xz?dl=0 PUBMEDBERT (Gu et al., 2020) https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext  ", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "B.4 Hardware Configurations", "text": "All our experiments are conducted on a server with specifications listed in Tab. 8.  C Other Details", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C.1 The Full Table of Supervised Baseline Models", "text": "The full table of supervised baseline models is provided in Tab. 4.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C.2 Hyper-Parameters Search Scope", "text": "Tab. 9 lists hyper-parameter search space for obtaining the set of used numbers. Note that the chosen hyper-parameters yield the overall best performance but might be sub-optimal on any single dataset. Also, we balanced the memory limit and model performance.\nC.3 A High-Resolution Version of Fig. 1 We show a clearer version of t-SNE embedding visualisation in Fig. 3.  ", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "PUDMEDBERT + SAPBERT", "text": "PUDMEDBERT Figure 3: Same as Fig. 1 in the main text, but generated with a higher resolution.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Acknowledgements", "text": "We thank the three reviewers and the Area Chair for their insightful comments and suggestions. FL is supported by Grace & Thomas C.H. Chan Cambridge Scholarship. NC and MB would like to", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Publicly available clinical BERT embeddings", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Emily Alsentzer; John Murphy; William Boag; Wei-Hung Weng; Di Jindi; Tristan Naumann; Matthew Mcdermott"}, {"title": "How noisy social media text, how diffrnt social media sources?", "journal": "", "year": "2013", "authors": "Timothy Baldwin; Paul Cook; Marco Lui; Andrew Mackinlay; Li Wang"}, {"title": "COMETA: A corpus for medical entity linking in the social media", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Marco Basaldella; Fangyu Liu; Ehsan Shareghi; Nigel Collier"}, {"title": "SciB-ERT: A pretrained language model for scientific text", "journal": "", "year": "2019", "authors": "Iz Beltagy; Kyle Lo; Arman Cohan"}, {"title": "The unified medical language system (UMLS): integrating biomedical terminology", "journal": "Nucleic Acids Research", "year": "2004", "authors": "Olivier Bodenreider"}, {"title": "The comparative toxicogenomics database: update", "journal": "Nucleic Acids Research", "year": "2019", "authors": "Allan Peter Davis; Cynthia J Grondin; Robin J Johnson; Daniela Sciaky; Roy Mcmorran; Jolene Wiegers; C Thomas; Carolyn J Wiegers;  Mattingly"}, {"title": "MEDIC: a practical disease vocabulary used at the comparative toxicogenomics database", "journal": "", "year": "2012", "authors": "Allan Peter Davis; C Thomas;  Wiegers; Carolyn J Michael C Rosenstein;  Mattingly"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "NCBI disease corpus: a resource for disease name recognition and concept normalization", "journal": "Journal of Biomedical Informatics", "year": "2014", "authors": "Robert Rezarta Islamaj Dogan; Zhiyong Leaman;  Lu"}, {"title": "SNOMED-CT: The advanced terminology and coding system for eHealth", "journal": "Studies in health technology and informatics", "year": "2006", "authors": "Kevin Donnelly"}, {"title": "Sieve-based entity linking for the biomedical domain", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "D' Jennifer; Vincent Souza;  Ng"}, {"title": "Neighbourhood components analysis", "journal": "", "year": "2005", "authors": "Jacob Goldberger; Geoffrey E Hinton; T Sam; Russ R Roweis;  Salakhutdinov"}, {"title": "Jianfeng Gao, and Hoifung Poon. 2020. Domainspecific language model pretraining for biomedical natural language processing", "journal": "", "year": "", "authors": "Yu Gu; Robert Tinn; Hao Cheng; Michael Lucas; Naoto Usuyama; Xiaodong Liu; Tristan Naumann"}, {"title": "Momentum contrast for unsupervised visual representation learning", "journal": "", "year": "2020", "authors": "Kaiming He; Haoqi Fan; Yuxin Wu; Saining Xie; Ross Girshick"}, {"title": "Parameter-efficient transfer learning for NLP", "journal": "PMLR", "year": "2019-06", "authors": "Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin De Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly"}, {"title": "BERTbased ranking for biomedical entity normalization", "journal": "AMIA Summits on Translational Science Proceedings", "year": "2020", "authors": "Zongcheng Ji; Qiang Wei; Hua Xu"}, {"title": "A neural named entity recognition and multi-type normalization tool for biomedical text mining", "journal": "IEEE Access", "year": "2019", "authors": "Donghyeon Kim; Jinhyuk Lee; Chan Ho So; Hwisang Jeon; Minbyul Jeong; Yonghwa Choi; Wonjin Yoon; Mujeen Sung; Jaewoo Kang"}, {"title": "Tag-gerOne: joint named entity recognition and normalization with semi-markov models", "journal": "Bioinformatics", "year": "2016", "authors": "Robert Leaman; Zhiyong Lu"}, {"title": "BioBERT: a pretrained biomedical language representation model for biomedical text mining", "journal": "Bioinformatics", "year": "2020", "authors": "Jinhyuk Lee; Wonjin Yoon; Sungdong Kim; Donghyeon Kim; Sunkyu Kim; Chan Ho So; Jaewoo Kang"}, {"title": "BEST: next-generation biomedical entity search tool for knowledge discovery from biomedical literature", "journal": "PloS one", "year": "2016", "authors": "Sunwon Lee; Donghyeon Kim; Kyubum Lee; Jaehoon Choi; Seongsoon Kim; Minji Jeon; Sangrak Lim; Donghee Choi; Sunkyu Kim; Aik-Choon Tan"}, {"title": "BioCreative V CDR task corpus: a resource for chemical disease relation extraction", "journal": "Database", "year": "2016", "authors": "Jiao Li; Yueping Sun; J Robin; Daniela Johnson; Chih-Hsuan Sciaky; Robert Wei; Allan Peter Leaman; Carolyn J Davis;  Mattingly; C Thomas; Zhiyong Wiegers;  Lu"}, {"title": "Adapting phrase-based machine translation to normalise medical terms in social media messages", "journal": "", "year": "2015", "authors": "Nut Limsopatham; Nigel Collier"}, {"title": "Normalising medical concepts in social media texts by learning semantic representation", "journal": "", "year": "2016", "authors": "Nut Limsopatham; Nigel Collier"}, {"title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Decoupled weight decay regularization", "journal": "", "year": "2018", "authors": "Ilya Loshchilov; Frank Hutter"}, {"title": "Visualizing data using t-SNE", "journal": "Journal of machine learning research", "year": "2008-11", "authors": "Laurens Van Der Maaten; Geoffrey Hinton"}, {"title": "Umlsbert: Clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathesaurus", "journal": "", "year": "2020", "authors": "George Michalopoulos; Yuanxin Wang; Hussam Kaka; Helen Chen; Alex Wong"}, {"title": "MedMentions: A large biomedical corpus annotated with UMLS concepts", "journal": "", "year": "2018", "authors": "Sunil Mohan; Donghui Li"}, {"title": "Deep metric learning via lifted structured feature embedding", "journal": "", "year": "2016", "authors": "Hyun Oh Song; Yu Xiang; Stefanie Jegelka; Silvio Savarese"}, {"title": "Representation learning with contrastive predictive coding", "journal": "", "year": "2018", "authors": "Aaron Van Den Oord; Yazhe Li; Oriol Vinyals"}, {"title": "Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets", "journal": "", "year": "2019", "authors": "Yifan Peng; Shankai Yan; Zhiyong Lu"}, {"title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer", "journal": "", "year": "2020", "authors": "Jonas Pfeiffer; Ivan Vuli\u0107; Iryna Gurevych; Sebastian Ruder"}, {"title": "Robust representation learning of biomedical names", "journal": "", "year": "2019", "authors": "Aixin Minh C Phan; Yi Sun;  Tay"}, {"title": "Overview of the trec 2015 clinical decision support track", "journal": "", "year": "2015", "authors": "Kirk Roberts; S Matthew; Ellen M Simpson; William R Voorhees;  Hersh"}, {"title": "Facenet: A unified embedding for face recognition and clustering", "journal": "", "year": "2015", "authors": "Florian Schroff; Dmitry Kalenichenko; James Philbin"}, {"title": "Clinical concept linking with contextualized neural representations", "journal": "", "year": "2020", "authors": "Elliot Schumacher; Andriy Mulyar; Mark Dredze"}, {"title": "BioMegatron: Larger biomedical domain language model", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Hoo-Chang Shin; Yang Zhang; Evelina Bakhturina; Raul Puri; Mostofa Patwary; Mohammad Shoeybi; Raghav Mani"}, {"title": "Circle loss: A unified perspective of pair similarity optimization", "journal": "", "year": "2020", "authors": "Yifan Sun; Changmao Cheng; Yuhan Zhang; Chi Zhang; Liang Zheng; Zhongdao Wang; Yichen Wei"}, {"title": "Medical concept normalization in social media posts with recurrent neural networks", "journal": "Journal of Biomedical Informatics", "year": "2018", "authors": "Elena Tutubalina; Zulfat Miftahutdinov; Sergey Nikolenko; Valentin Malykh"}, {"title": "Probing pretrained language models for lexical semantics", "journal": "", "year": "2020", "authors": "Ivan Vuli\u0107; Maria Edoardo; Robert Ponti; Goran Litschko; Anna Glava\u0161;  Korhonen"}, {"title": "Multi-similarity loss with general pair weighting for deep metric learning", "journal": "", "year": "2019", "authors": "Xun Wang; Xintong Han; Weilin Huang; Dengke Dong; Matthew R Scott"}, {"title": "A comparison of word embeddings for the biomedical natural language processing", "journal": "Journal of Biomedical Informatics", "year": "2018", "authors": "Yanshan Wang; Sijia Liu; Naveed Afzal; Majid Rastegar-Mojarad; Liwei Wang; Feichen Shen; Paul Kingsbury; Hongfang Liu"}, {"title": "Normco: Deep disease normalization for biomedical knowledge base construction", "journal": "", "year": "2019", "authors": "Dustin Wright; Yannis Katsis; Raghav Mehta; Chun-Nan Hsu; ; Devlin"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The t-SNE (Maaten and Hinton, 2008) visualisation of UMLS entities under PUBMEDBERT (BERT pretrained on PubMed papers) & PUBMED-BERT+SAPBERT (PUBMEDBERT further pretrained on UMLS synonyms). The biomedical names of different concepts are hard to separate in the heterogeneous embedding space (left). After the self-alignment pretraining, the same concept's entity names are drawn closer to form compact clusters (right).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ":This table compares PUBMED-BERT+SAPBERT's performance with and withoutonline hard mining on COMETA (zeroshot general)."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "acknowledge funding from Health Data Research UK as part of the National Text Analytics project.", "figure_data": "Dongfang Xu, Zeyu Zhang, and Steven Bethard. 2020.A generate-and-rank framework with semantic typeregularization for biomedical concept normalization.In Proceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics, pages8452-8464."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "This table contains basic statistics of the MEL datasets used in the study. C denotes the set of concepts;", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "A list of baselines on the 6 different MEL datasets, including both scientific and social media language ones. The last row collects reported numbers from the best performing models. \" * \" denotes results produced using official released code. \"-\" denotes results not reported in the cited paper. \"OOM\" means out-of-memoery.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "This table compares loss functions used for SAPBERT pretraining. Numbers reported are on COMETA (zeroshot general). The cosine loss was used by Phan et al. (2019) for learning UMLS synonyms for LSTM models. The max-margin triplet loss was used by Basaldella et al. (2020) for training MEL models. A very similar (though not identical) hinge-loss was used by", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "This table lists the URL of models used in this study.", "figure_data": "methodreduction rate #params#params #params in BERTADAPTER 13%114.47M13.22%ADAPTER 1%160.60M1.09%full-model-tuning-109.48M100%"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "This table compares number of parameters trained in ADAPTER variants and also full-modeltuning.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Hardware specifications of the used machine.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "He et al., 2020) * , 0.5 (Oord et al., 2018)} m in Circle loss {0.25 (Sun et al., 2020) * , 0.4 (Sun et al., 2020)} \u03b3 in Circle loss{80(Sun et al., 2020), 256(Sun et al., 2020) ", "figure_data": "hyper-parameterssearch spacelearning rate for pretraining & fine-tuning SAPBERT{1e-4, 2e-5  *  , 5e-5, 1e-5, 1e-6}pretraining batch size{128, 256, 512  *  , 1024}pretraining training iterations{10k, 20k, 30k, 40k, 50k (1 epoch)  *  , 100k (2 epochs)}fine-tuning epochs on scientific language datasets{1, 2, 3  *  , 5}fine-training epochs on AskAPatient{5, 10, 15  *  , 20}fine-training epochs on COMETA{5, 10  *  , 15, 20}max_seq_length of BERT tokenizer{15, 20, 25  *  , 30}\u03bb in Online Mining{-0.05, -0.1, -0.2  *  , -0.3}\u03b1 in MS loss{1, 2 (Wang et al., 2019)  *  , 3}\u03b2 in MS loss{40, 50 (Wang et al., 2019)  *  , 60}in MS loss{0.5  *  , 1 (Wang et al., 2019)}\u03b1 in max-margin triplet loss{0.05, 0.1, 0.2 (Basaldella et al., 2020)  *  , 0.3}softmax scale in NCA loss{1 (Goldberger et al., 2005), 5, 10, 20  *  , 30}\u03b1 in Lifted-Structured loss{0.5  *  , 1 (Oh Song et al., 2016)}\u03c4 (temperature) in InfoNCE{0.07 ("}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "This table lists the search space for hyper-parameters used. * means the used ones for reporting results.", "figure_data": ""}], "doi": "10.18653/v1/W19-1909"}