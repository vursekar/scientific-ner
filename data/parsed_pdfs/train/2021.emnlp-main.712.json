{"authors": "Asahi Ushio; Jose Camacho-Collados; Steven Schockaert", "pub_date": "", "title": "Distilling Relation Embeddings from Pre-trained Language Models", "abstract": "Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more finegrained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically generated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, even without any task-specific fine-tuning. 1   ", "sections": [{"heading": "Introduction", "text": "One of the most widely studied aspects of word embeddings is the fact that word vector differences capture lexical relations (Mikolov et al., 2013a). While not being directly connected to downstream performance on NLP tasks, this ability of word embeddings is nonetheless important. For instance, understanding lexical relations is an important prerequisite for understanding the meaning of compound nouns . Moreover, the ability of word vectors to capture semantic relations has enabled a wide range of applications beyond NLP, including flexible querying of relational databases (Bordawekar and Shmueli, 2017), schema match-1 Source code to reproduce our experimental results and the model checkpoints are available in the following repository: https://github.com/asahi417/relbert ing (Fernandez et al., 2018), completion and retrieval of Web tables (Zhang et al., 2019), ontology completion (Bouraoui and Schockaert, 2019) and information retrieval in the medical domain (Arguello Casteleiro et al., 2020). More generally, relational similarity (or analogy) plays a central role in computational creativity (Goel, 2019), legal reasoning (Ashley, 1988;Walton, 2010), ontology alignment (Raad and Evermann, 2015) and instance-based learning (Miclet et al., 2008).\nGiven the recent success of pre-trained language models (Devlin et al., 2019;Liu et al., 2019;Brown et al., 2020), we may wonder whether such models are able to capture lexical relations in a more faithful or fine-grained way than traditional word embeddings. However, for language models (LMs), there is no direct equivalent to the word vector difference. In this paper, we therefore propose a strategy for extracting relation embeddings from pre-trained LMs, i.e. vectors encoding the relationship between two words. On the one hand, this will allow us to gain a better understanding of how well lexical relations are captured by these models. On the other hand, this will also provide us with a practical method for obtaining relation embeddings in applications such as the ones mentioned above.\nSince it is unclear how LMs store relational knowledge, rather than directly extracting relation embeddings, we first fine-tune the LM, such that relation embeddings can be obtained from its output. To this end, we need a prompt, i.e. a template to convert a given word pair into a sentence, and some training data to fine-tune the model. To illustrate the process, consider the word pair Paris-France. As a possible input to the model, we could use a sentence such as \"The relation between Paris and France is <mask>\". Note that our aim is to find a strategy that can be applied to any pair of words, hence the way in which the input is represented needs to be sufficiently generic. We then fine-tune the LM such that its output corresponds to a relation embedding. To this end, we use a crowdsourced dataset of relational similarity judgements that was collected in the context of SemEval 2012 Task 2 (Jurgens et al., 2012). Despite the relatively small size of this dataset, we show that the resulting fine-tuned LM allows us to produce high-quality relation embeddings, as confirmed in our extensive evaluation in analogy and relation classification tasks. Importantly, this also holds for relations that are of a different nature than those in the SemEval dataset, showing that this process allows us to distill relational knowledge that is encoded in the pre-trained LM, rather than merely generalising from the examples that were used for fine-tuning.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Probing LMs for Relational Knowledge Since the introduction of transformer-based LMs, a large number of works have focused on analysing the capabilities of such models, covering the extent to which they capture syntax (Goldberg, 2019;Saphra and Lopez, 2019;Hewitt and Manning, 2019;van Schijndel et al., 2019;Jawahar et al., 2019;Tenney et al., 2019), lexical semantics (Ethayarajh, 2019;Bommasani et al., 2020;Vulic et al., 2020), and various forms of factual and commonsense knowledge (Petroni et al., 2019;Forbes et al., 2019;Davison et al., 2019;Zhou et al., 2020;Talmor et al., 2020;Roberts et al., 2020), among others. The idea of extracting relational knowledge from LMs, in particular, has also been studied. For instance, Petroni et al. (2019) use BERT for link prediction. To this end, they use a manually defined prompt for each relation type, in which the tail entity is replaced by a <mask> token. To complete a knowledge graph triple such as (Dante, born-in, ?) they create the input \"Dante was born in <mask>\" and then look at the predictions of BERT for the masked token to retrieve the correct answer. It is notable that BERT is thus used for extracting relational knowledge without any fine-tuning. This clearly shows that a substantial amount of factual knowledge is encoded in the parameters of pre-trained LMs. Some works have also looked at how such knowledge is stored. Geva et al. (2020) argue that the feed-forward layers of transformer-based LMs act as neural memories, which would suggest that e.g. \"the place where Dante is born\" is stored as a property of Florence. Dai et al. (2021) present further evidence of this view. What is less clear, then, is whether relations themselves have an explicit representation, or whether transformer models essentially store a propositionalised knowledge graph. The results we present in this paper suggest that common lexical relations (e.g. hypernymy, meronymy, has-attribute), at least, must have some kind of explicit representation, although it remains unclear how they are encoded.\nAnother notable work focusing on link prediction is (Bosselut et al., 2019), where GPT is fine-tuned to complete triples from commonsense knowledge graphs, in particular ConceptNet (Speer et al., 2017) and ATOMIC . While their model was able to generate new knowledge graph triples, it is unclear to what extent this is achieved by extracting commonsense knowledge that was already captured by the pre-trained GPT model, or whether this rather comes from the ability to generalise from the training triples. For the ConceptNet dataset, for instance, Jastrz\u0119bski et al. (2018) found that most test triples are in fact minor variations of training triples. In this paper, we also rely on fine-tuning, which makes it harder to determine to what extent the pre-trained LM already captures relational knowledge. We address this concern by including relation types in our evaluation which are different from the ones that have been used for fine-tuning.\nUnsupervised Relation Discovery Modelling how different words are related is a long-standing challenge in NLP. An early approach is DIRT (Lin and Pantel, 2001), which encodes the relation between two nouns as the dependency path connecting them. Their view is that two such dependency paths are similar if the sets of word pairs with which they co-occur are similar. Hasegawa et al. (2004) cluster named entity pairs based on the bag-of-words representations of the contexts in which they appear. Along the same lines, Yao et al. (2011) proposed a generative probabilistic model, inspired by LDA (Blei et al., 2003), in which relations are viewed as latent variables (similar to topics in LDA). Turney (2005) proposed a method called Latent Relational Analysis (LRA), which uses matrix factorization to learn relation embeddings based on co-occurrences of word pairs and dependency paths. Matrix factorization is also used in the Universal Schema approach from Riedel et al. (Riedel et al., 2013), which jointly models the contexts in which words appear in a corpus with a given set of relational facts.\nThe aforementioned works essentially represent the relation between two words by summarising the contexts in which these words co-occur. In recent years, a number of strategies based on distributional models have been explored that rely on similar intuitions but go beyond simple vector operations of word embeddings. 2 For instance, Jameel et al. (2018) introduced a variant of the GloVe word embedding model, in which relation vectors are jointly learned with word vectors. In SeVeN (Espinosa-Anke and Schockaert, 2018) and RELATIVE (Camacho-Collados et al., 2019), relation vectors are computed by averaging the embeddings of context words, while pair2vec  uses an LSTM to summarise the contexts in which two given words occur, and Washio and Kato (2018) learn embeddings of dependency paths to encode word pairs. Another line of work is based on the idea that relation embeddings should facilitate link prediction, i.e. given the first word and a relation vector, we should be able to predict the second word (Marcheggiani and Titov, 2016;Simon et al., 2019). This idea also lies at the basis of the approach from Soares et al. ( 2019), who train a relation encoder by fine-tuning BERT (Devlin et al., 2019) with a link prediction loss. However, it should be noted that they focus on learning relation vectors from individual sentences, as a pre-training task for applications such as few-shot relation extraction. In contrast, our focus in this paper is on characterising the overall relationship between two words.", "n_publication_ref": 32, "n_figure_ref": 0}, {"heading": "RelBERT", "text": "In this section, we describe our proposed relation embedding model (RelBERT henceforth). To obtain a relation embedding for given a word pair (h, t), we first convert it into a sentence s, called the prompt. We then feed the prompt through the LM and average the contextualized embeddings (i.e. the output vectors) to get the relation embedding of (h, t). These steps are illustrated in Figure 1 and explained in more detail in the following.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Prompt Generation", "text": "Manual Prompts A basic prompt generation strategy is to rely on manually created templates, 2 Interestingly, Roller and Erk (2016) showed that the direct concatenation of distributional word vectors in isolation can effectively identify Hearst Patterns (Hearst, 1992). which has proven effective in factual knowledge probing (Petroni et al., 2019) and text classification (Schick and Sch\u00fctze, 2021;Tam et al., 2021;Le Scao and Rush, 2021), among many others. To test whether manually generated templates can be effective for learning relation embeddings, we will consider the following five templates:  (Bouraoui et al., 2020;Jiang et al., 2020).\nLearned Prompts The choice of prompt can have a significant impact on an LM's performance. Since it is difficult to generate manual prompts in a systematic way, several strategies for automated generation of task-specific prompts have been proposed, e.g. based on mining patterns from a corpus (Bouraoui et al., 2020), paraphrasing (Jiang et al., 2020), training an additional LM for template generation (Haviv et al., 2021;Gao et al., 2020), and prompt optimization (Shin et al., 2020;Liu et al., 2021). In our work, we focus on the latter strategy, given its conceptual simplicity and its strong reported performance on various benchmarks. Specifically, we consider AutoPrompt (Shin et al., 2020) and P-tuning (Liu et al., 2021). Note that both methods rely on training data. We will use the same training data and loss function that we use for fine-tuning the LM; see Section 3.2.\nAutoPrompt initializes the prompt as a fixedlength template:\nT = (z 1 , . . . , z \u03c0 , [h], z \u03c0+1 , . . . , z \u03c0+\u03c4 , [t], z \u03c0+\u03c4 +1 , . . . , z \u03c0+\u03c4 +\u03b3 )(1)\nwhere \u03c0, \u03c4 , \u03b3 are hyper-parameters which determine the length of the template. The tokens of the form z i are called trigger tokens. These tokens are initialized as <mask>. The method then iteratively finds the best token to replace each mask, based on the gradient of the task-specific loss function. 3 P-tuning employs the same template initialization as AutoPrompt but its trigger tokens are newly introduced special tokens with trainable embeddings\u00ea 1:\u03c0+\u03c4 +\u03b3 , which are learned using a taskspecific loss function while the LM's weights are frozen.", "n_publication_ref": 16, "n_figure_ref": 0}, {"heading": "Fine-tuning the LM", "text": "To fine-tune the LM, we need training data and a loss function. As training data, we assume that, for a number of different relation types r, we have access to examples of word pairs (h, t) that are instances of that relation type. The loss function is based on the following intuition: the embeddings of word pairs that belong to the same relation type should be closer together than the embeddings of pairs that belong to different relations. In particular, we use the triplet loss from Schroff et al. (2015) and the classification loss from Reimers and Gurevych (2019), both of which are based on this intuition.\nTriplet Loss We draw a triplet from the relation dataset by selecting an anchor pair a = (h a , t a ), a positive example p = (h p , t p ) and a negative example n = (h n , t n ), i.e. we select word pairs a, p, n such that a and p belong to the same relation type while n belongs to a different relation type. Let us write x a , x p , x n for the corresponding relation embeddings. Each relation embedding is produced by the same LM, which is trained to make the distance between x a and x p smaller than the distance between x a and x n . Formally, this is accomplished using the following triplet loss function:\nL t = max 0, x a \u2212 x p \u2212 x a \u2212 x n + \u03b5 3\nWe note that in most implementations of AutoPrompt the vocabulary to sample trigger tokens is restricted to that of the training data. However, given the nature of our training data (i.e., pairs of words and not sentences), we consider the full pre-trained LM's vocabulary.\nwhere \u03b5 > 0 is the margin and \u2022 is the l 2 norm.\nClassification Loss Following SBERT (Reimers and Gurevych, 2019), we use a classifier to predict whether two word pairs belong to the same relation. The classifier is jointly trained with the LM using the negative log likelihood loss function:\nL c = \u2212 log(g(x a , x p )) \u2212 log(1 \u2212 g(x a , x n )) where g(u, v) = sigmoid(W \u2022 [u \u2295 v \u2295 |v \u2212 u|] T ) with W \u2208 R 3\u00d7d , u, v \u2208 R d , |\n\u2022 | the element-wise absolute difference, and \u2295 concatenation.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Experimental Setting", "text": "In this section, we explain our experimental setting to train and evaluate RelBERT.  triples are from different relations). Figure 2 illustrates this idea. Note how the effective batch size thus increases quadratically, while the number of vectors that needs to be encoded by the LM remains unchanged. In our setting, this leads to an additional 13500 triples per relation. Similar in-batch negative sampling has been shown to be effective in information retrieval (Karpukhin et al., 2020;Gillick et al., 2019). Third, we also construct training triples by considering the 10 high-level categories as relation types. In this case, we choose two positive examples from different relations that belong to the same category, along with a positive example from a relation from a different category. We add 5040 triples of this kind for each of the 10 categories.\nTraining RelBERT training consists of two phases: prompt optimization (unless a manually defined prompt is used) and language model finetuning. First we optimize the prompt over the training set with the triplet loss L t while the parameters of the LM are frozen. Subsequently, we fine-tune the LM with the resulting prompt, using the sum of the triplet loss L t and the classification loss L c over the same training set. We do not use the classification loss during the prompt optimisation, as that would involve training the classifier while optimizing the prompt. We select the best hyper-parameters of the prompting methods based on the final loss over the validation set. In particular, when manual prompts are used, we choose the best template among the five candidates described in Section 3.1. For AutoPrompt and Ptuning, we consider all combinations of \u03c0 \u2208 {8, 9}, \u03c4 \u2208 {1, 2}, \u03b3 \u2208 {1, 2}. We use RoBERTa (Liu et al., 2019) as our main LM, where the initial weights were taken from the roberta-large model checkpoint shared by the Huggingface transformers model hub (Wolf et al., 2020). We use the Adam optimizer (Kingma and Ba, 2014) with learn-ing rate 0.00002, batch size 64 and we fine-tune the model for 1 epoch. For AutoPrompt, the top-50 tokens are considered and the number of iterations is set to 50. In each iteration, one of the input tokens is re-sampled and the loss is re-computed across the entire training set. 4 For P-tuning, we train the weights that define the trigger embeddings (i.e. the weights of the input vectors and the parameters of the LSTM) for 2 epochs. Note that we do not tune RelBERT on any task-specific training or validation set. We thus use the same relation embeddings across all the considered evaluation tasks.", "n_publication_ref": 5, "n_figure_ref": 1}, {"heading": "Evaluation Tasks", "text": "We evaluate RelBERT on two relation-centric tasks: solving analogy questions (unsupervised) and lexical relation classification (supervised).\nAnalogy Questions We consider the task of solving word analogy questions. Given a query word pair, the model is required to select the relationally most similar word pair from a list of candidates. To solve this task, we simply choose the candidate whose RelBERT embedding has the highest cosine similarity with the RelBERT embedding of the query pair. Note that this task is completely unsupervised, without the need for any training or tuning. We use the five analogy datasets that were considered by Ushio et al. (2021): the SAT analogies dataset (Turney et al., 2003), the U2 and U4 analogy datasets, which were collected from an educational website 5 , and datasets that were derived 6 from BATS (Gladkova et al., 2016) and the Google analogy dataset (Mikolov et al., 2013b). These five datasets consist of tuning and testing fragments. In particular, they contain 37/337 (SAT), 24/228 (U2), 48/432 (U4), 50/500 (Google), and 199/1799 (BATS) questions for validation/testing. As there is no need to tune RelBERT on task-specific data, we only use the test fragments. For SAT, we will also report results on the full dataset (i.e. the testing fragment and tuning fragment combined), as this allows us to compare the performance with published results. We will refer to this full version of the SAT dataset as SAT \u2020. Lexical Relation Classification We consider the task of predicting which relation a given word pair belongs to. To solve this task, we train a multi-layer perceptron (MLP) which takes the (frozen) RelBERT embedding of the word pair as input. We consider the following widelyused multi-class relation classification benchmarks: K&H+N (Nec\u015fulescu et al., 2015), BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016b), EVALution (Santus et al., 2015), and CogALex-V Subtask 2 (Santus et al., 2016a). Table 1 shows the size of the training, validation and test sets for each of the relation classification dataset. The hyperparameters of the MLP classifier are tuned on the validation set of each dataset. Concretely, we tune the learning rate from [0.001, 0.0001, 0.00001] and the hidden layer size from [100,150,200]. CogALex-V only has testing fragments so for this dataset we employ the default configuration of Scikit-Learn (Pedregosa et al., 2011), which uses a 100-dimensional hidden layer and is optimized using Adam with a learning rate of 0.001. These datasets focus on the following lexical relations: co-hyponymy (cohyp), hypernymy (hyp), meronymy (mero), possession (poss), synonymy (syn), antonymy (ant), attribute (attr), event, and random (rand).", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Baselines", "text": "As baselines, we consider two standard word embedding models: GloVe (Pennington et al., 2014) and FastText (Bojanowski et al., 2017), where word pairs are represented by the vector difference of their word embeddings (diff ). 7 For the classification experiments, we also consider the concatena-\n7\nVector difference is the most common method for encoding relations, and has been shown to be the most reliable in the context of word analogies (Hakami and Bollegala, 2017). tion of the two word embeddings (cat) and their element-wise multiplication 8 (dot). We furthermore experiment with two pre-trained word pair embedding models: pair2vec  (pair) and RELATIVE (Camacho-Collados et al., 2019) (rel). For these word pair embeddings, as well as for RelBERT, we concatenate the embeddings from both directions, i.e. (h, t) and (t, h). For the analogy questions, two simple statistical baselines are included: the expected random performance and a strategy based on point-wise mutual information (PMI) Church and Hanks (1990). In particular, the PMI score of a word pair is computed using the English Wikipedia, with a fixed window size of 10. We then choose the candidate pair with the highest PMI as the prediction. Note that this PMI-based method completely ignores the query pair. We also compare with the published results from Ushio et al. (2021), where a strategy is proposed to solve analogy questions by using LMs to compute an analogical proportion score. In particular, a four-word tuple (a, b, c, d) is encoded using a custom prompt and perplexity based scoring strategies are used to determine whether the word pair (a, b) has the same relation as the word pair (c, d). Finally, for the SAT \u2020 dataset, we compare with the published results from GPT-3 (Brown et al., 2020), LRA (Turney, 2005) and SuperSim (Turney, 2013); for relation classification we report the published results of the LexNet (Shwartz et al., 2016) and SphereRE  relation classification models, taking the results from the latter publication. We did not reproduce these latter methods in similar conditions as our work, and hence they are not fully comparable. More-over, these approaches are a different nature, as the aim of our work is to provide universal relation embeddings instead of task-specific models.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Results", "text": "In this section, we present our main experimental results, testing the relation embeddings learned by RelBERT on analogy questions (Section 5.1) and relation classification (Section 5.2).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Analogy Questions", "text": "Table 2 shows the accuracy on the analogy benchmarks. The RelBERT models substantially outperform the baselines on all datasets, except for the Google analogy dataset. 9 Comparing the different prompt generation approaches, we can see that, surprisingly, the manual prompt consistently outperforms the automatically-learned prompt strategies. It can furthermore be noted that the other two relation embedding methods (i.e. pair2vec and REL-ATIVE) perform poorly in this unsupervised task. The analogical proportion score from Ushio et al. (2021) also underperforms RelBERT, even when tuned on dataset-specific tuning data.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Lexical Relation Classification", "text": "Table 3 summarizes the results of the lexical relation classification experiments, in terms of macro and micro averaged F1 score. The RelBERT models achieve the best results on all datasets except for BLESS and K&H+N, where the performance of all models is rather close. We observe a particularly large improvement over the word embedding and SotA models on the EVALution dataset. When comparing the different prompting strategies, we again find that the manual prompts perform surprisingly well, although the best results are now obtained with learned prompts in a few cases. 9 The Google analogy dataset has been shown to be biased toward word similarity and therefore to be well suited to word embeddings (Linzen, 2016;Rogers et al., 2017 ", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Analysis", "text": "To better understand how relation embeddings are learned, in this section we analyze the model's performance in more detail.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Training Data Overlap", "text": "In our main experiments, RelBERT is trained using the SemEval 2012 Task 2 dataset. This dataset contains a broad range of semantic relations, including hypernymy and meronymy relations. This raises an important question: Does RelBERT provide us with a way to extract relational knowledge from the parameters of the   As a further analysis, Table 5 shows a breakdown of the Google and BATS analogy results, showing the average performance on each of the top-level categories from these datasets. 10 While RelBERT is outperformed by FastText on the morphological relations, it should be noted that the differences are small, while such relations are of a very different nature than those from the SemEval dataset. This confirms that RelBERT is able to model a broad range of relations, although it can be expected that better results would be possible by including task-specific training data into the fine-tuning process (e.g. including morphological relations for tasks where such relations matter).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Target", "text": "Nearest Neighbors barista:coffee baker:bread, brewer:beer, bartender:cocktail, winemaker:wine, bartender:drink, baker:cake bag:plastic bottle:plastic, bag:leather, container:plastic, box:plastic, jug:glass, bottle:glass duck:duckling chicken:chick, pig:piglet, cat:kitten, ox:calf, butterfly:larvae, bear:cub cooked:raw raw:cooked, regulated:unregulated, sober:drunk, loaded:unloaded, armed:unarmed, published:unpublished chihuahua:dog dachshund:dog, poodle:dog, terrier:dog, chinchilla:rodent, macaque:monkey, dalmatian:dog dog:dogs cat:cats, horse:horses, pig:pigs, rat:rats, wolf:wolves, monkey:monkeys spy:espionage pirate:piracy, robber:robbery, lobbyist:lobbying, scout:scouting, terrorist:terrorism, witch:witchcraft ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Language Model Comparison", "text": "Figure 3 compares the performance of RelBERT with that of the vanilla pre-trained RoBERTa model (i.e. when only the prompt is optimized). As can be seen, the fine-tuning process is critical for achieving good results. In Figure 3, we also compare the performance of our main RelBERT model, which is based on RoBERTa, with versions that were instead initialized with BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2019). 11 RoBERTa clearly outperforms the other two LMs, which is in accordance with findings from the literature suggesting that RoBERTa captures more semantic knowledge Warstadt et al., 2020).", "n_publication_ref": 4, "n_figure_ref": 2}, {"heading": "Qualitative Analysis", "text": "To give further insight into the nature of RelBERT embeddings, Table 6 shows the nearest neighbors of some selected word pairs from the evaluation datasets. To this end, we computed RelBERT relation vectors for all pairs in the Wikipedia pretrained RELATIVE vocabulary (over 1M pairs). 12 The neighbors are those word pairs whose Rel-BERT embedding has the highest cosine similarity within the full pair vocabulary. As can be seen, the neighbors mostly represent word pairs that are relationally similar, even for morphological relations (e.g. dog:dogs), which are not present in the SemEval dataset. A more extensive qualitative analysis, including a comparison with RELATIVE, is provided in the appendix.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We have proposed a strategy for learning relation embeddings, i.e. vector representations of pairs of words which capture their relationship. The main idea is to fine-tune a pre-trained language model using the relational similarity dataset from SemEval 2012 Task 2, which covers a broad range of semantic relations. In our experimental results, we found the resulting relation embeddings to be of high quality, outperforming state-of-the-art methods on several analogy and relation classification benchmarks. Among the models tested, we obtained the best results with RoBERTa, when using manually defined templates for encoding word pairs. Importantly, we found that high-quality relation embeddings can be obtained even for relations that are unlike those from the SemEval dataset, such as morphological and encyclopedic relations. This suggests that the knowledge captured by our relation embeddings is largely distilled from the pre-trained language model, rather than being acquired during training. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Additional Experimental Results", "text": "In this section, we show additional experimental results that complement the main results of the paper.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 Vanilla LM Comparison", "text": "We show comparisons of versions of RelBERT with optimized prompt with/without finetuning. Figure 4 shows the absolute accuracy drop from RelBERT (i.e. the model with fine-tuning) to the vanilla RoBERTa model (i.e. without fine-tuning) with the same prompt. In all cases, the accuracy drop for the models without fine-tuning is substantial.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "A.2 Comparison with ALBERT & BERT", "text": "We use RoBERTa in our main experiments and here we train RelBERT with ALBERT and BERT instead, and evaluate them on both of the analogy and relation classification tasks. Table 7 shows the accuracy on the analogy questions, while Table 8 shows the accuracy on the relation classification task. In both tasks, we can confirm that RoBERTa achieves the best performance within the LMs, by a relatively large margin in most cases.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.3 Word Embeddings", "text": "Table 9 shows additional results of word embeddings on analogy test together with RelBERT results. We concatenate the RELATIVE and pair2vec vectors with the word vector difference. However, this does not lead to better results.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B Experimental Details and Model Configurations", "text": "In this section, we explain models' configuration in the experiments, and details on RelBERT's training time.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B.1 Prompting Configuration", "text": "Table 10 shows the best prompt configuration based on the validation loss for the SemEval 2012 Task 2 dataset in our main experiments using RoBERTa.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B.2 MLP Configuration in Relation Classification", "text": "Table 11 shows the best hyperparameters in the validation set of the MLPs for relation classification.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B.3 Training Time", "text": "Training a single RelBERT model with a custom prompt takes about half a day on two V100 GPUs. Additionally, to achieve prompt by AutoPrompt  technique takes about a week on a single V100, while P-tuning takes 3 to 4 hours, also on a single V100.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C Implementation Details of AutoPrompt", "text": "All the trigger tokens are initialized by mask tokens and updated based on the gradient of a loss function L t . Concretely, let us denote the loss value with template T as L t (T ). The candidate set for the j th trigger is derived b\u1ef9\nW j = top-k w\u2208W e T w \u2207 j L t (T ) (2)\nwhere the gradient is taken with respect to j th trigger token and e w is the input embedding for the word w. Then we evaluate each token based on the loss function as\nz j = argmin w\u2208W j L t (rep(T, j, w))(3)\nwhere rep(T, j, w) replaces the j th token in T by w and j is randomly chosen. We ignore any candidates that do not improve current loss value to further enhance the prompt quality.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D Additional Analysis", "text": "In this section, we analyze our experimental results based on prediction breakdown and provide an extended qualitative analysis.   ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D.1 Prediction Breakdown", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D.2 Qualitative Analysis", "text": "Tables 13 shows the nearest neighbors of a number of selected word pairs, in terms of their RelBERT and RELATIVE embeddings. In both cases cosine similarity is used to compare the embeddings and the pair vocabulary of the RELATIVE model is used to determine the universe of candidate neighbors.\nThe results for the RelBERT embeddings show their ability to capture a wide range of relations. In most cases the neighbors make sense, despite the fact that many of these relations are quite different from those in the SemEval dataset that was used for training RelBERT. The results for RELATIVE are in general much noisier, suggesting that REL-ATIVE embeddings fail to capture many types of relations. This is in particular the case for the morphological examples, although various issues can be observed for the other relations as well.   ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "Jose Camacho-Collados acknowledges support from the UKRI Future Leaders Fellowship scheme.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Semantic deep learning: Prior knowledge and a type of four-term embedding analogy to acquire treatments for well-known diseases", "journal": "JMIR Med Inform", "year": "2020", "authors": "Mercedes Arguello Casteleiro; Julio Des Diz; Nava Maroto; Maria Jesus Fernandez Prieto; Simon Peters; Chris Wroe; Carlos Sevillano Torrado; Diego Maseda Fernandez; Robert Stevens"}, {"title": "Arguing by analogy in law: A case-based model", "journal": "Springer", "year": "1988", "authors": "D Kevin;  Ashley"}, {"title": "How we BLESSed distributional semantic evaluation", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Marco Baroni; Alessandro Lenci"}, {"title": "Latent dirichlet allocation", "journal": "Journal of Machine Learning Research", "year": "2003", "authors": "David M Blei; Andrew Y Ng; Michael I Jordan"}, {"title": "Enriching word vectors with subword information", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Piotr Bojanowski; Edouard Grave; Armand Joulin; Tomas Mikolov"}, {"title": "Interpreting pretrained contextualized representations via reductions to static embeddings", "journal": "", "year": "2020", "authors": "Rishi Bommasani; Kelly Davis; Claire Cardie"}, {"title": "Using word embedding to enable semantic queries in relational databases", "journal": "", "year": "2017", "authors": "Rajesh Bordawekar; Oded Shmueli"}, {"title": "COMET: Commonsense transformers for automatic knowledge graph construction", "journal": "", "year": "2019", "authors": "Antoine Bosselut; Hannah Rashkin; Maarten Sap; Chaitanya Malaviya; Asli Celikyilmaz; Yejin Choi"}, {"title": "Inducing relational knowledge from bert", "journal": "", "year": "2020", "authors": "Zied Bouraoui; Jose Camacho-Collados; Steven Schockaert"}, {"title": "Automated rule base completion as bayesian concept induction", "journal": "", "year": "2019", "authors": "Zied Bouraoui; Steven Schockaert"}, {"title": "", "journal": "", "year": "", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal"}, {"title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "", "year": "", "authors": "Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin; Scott Gray"}, {"title": "A latent variable model for learning distributional relation vectors", "journal": "", "year": "2019", "authors": "Jose Camacho-Collados; Luis Espinosa-Anke; Jameel Shoaib; Steven Schockaert"}, {"title": "Word association norms, mutual information, and lexicography", "journal": "Computational linguistics", "year": "1990", "authors": "Kenneth Church; Patrick Hanks"}, {"title": "Knowledge neurons in pretrained transformers", "journal": "", "year": "2021", "authors": "Damai Dai; Li Dong; Yaru Hao; Zhifang Sui; Furu Wei"}, {"title": "Commonsense knowledge mining from pretrained models", "journal": "", "year": "2019", "authors": "Joe Davison; Joshua Feldman; Alexander M Rush"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "SeVeN: Augmenting word embeddings with unsupervised relation vectors", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Luis Espinosa-Anke; Steven Schockaert"}, {"title": "How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings", "journal": "", "year": "2019", "authors": "Kawin Ethayarajh"}, {"title": "Seeping semantics: Linking datasets using word embeddings for data discovery", "journal": "IEEE 34th International Conference on Data Engineering", "year": "2018", "authors": "Raul Castro Fernandez; Essam Mansour; A Abdulhakim; Ahmed Qahtan; Ihab Elmagarmid; Samuel Ilyas; Mourad Madden; Michael Ouzzani; Nan Stonebraker;  Tang"}, {"title": "Do neural language representations learn physical commonsense?", "journal": "", "year": "2019", "authors": "Maxwell Forbes; Ari Holtzman; Yejin Choi"}, {"title": "Making pre-trained language models better few-shot learners", "journal": "", "year": "2020", "authors": "Tianyu Gao; Adam Fisch; Danqi Chen"}, {"title": "Transformer feed-forward layers are key-value memories", "journal": "", "year": "2020", "authors": "Mor Geva; Roei Schuster; Jonathan Berant; Omer Levy"}, {"title": "Learning dense representations for entity retrieval", "journal": "", "year": "2019", "authors": "Daniel Gillick; Sayali Kulkarni; Larry Lansing; Alessandro Presta; Jason Baldridge; Eugene Ie; Diego Garcia-Olano"}, {"title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't", "journal": "", "year": "2016", "authors": "Anna Gladkova; Aleksandr Drozd; Satoshi Matsuoka"}, {"title": "Computational design, analogy, and creativity", "journal": "Springer", "year": "2019", "authors": "Ashok Goel"}, {"title": "Assessing bert's syntactic abilities", "journal": "", "year": "2019", "authors": "Yoav Goldberg"}, {"title": "Compositional approaches for representing relations between words: A comparative study. Knowledge-Based Systems", "journal": "", "year": "2017", "authors": "Huda Hakami; Danushka Bollegala"}, {"title": "Discovering relations among named entities from large corpora", "journal": "", "year": "2004", "authors": "Takaaki Hasegawa; Satoshi Sekine; Ralph Grishman"}, {"title": "BERTese: Learning to speak to BERT", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Adi Haviv; Jonathan Berant; Amir Globerson"}, {"title": "Automatic acquisition of hyponyms from large text corpora", "journal": "", "year": "1992", "authors": "Marti A Hearst"}, {"title": "A structural probe for finding syntax in word representations", "journal": "", "year": "2019", "authors": "John Hewitt; Christopher D Manning"}, {"title": "Unsupervised learning of distributional relation vectors", "journal": "", "year": "2018", "authors": "Shoaib Jameel; Zied Bouraoui; Steven Schockaert"}, {"title": "Commonsense mining as knowledge base completion? a study on the impact of novelty", "journal": "", "year": "2018", "authors": "Stanislaw Jastrz\u0119bski; Dzmitry Bahdanau; Seyedarian Hosseini; Michael Noukhovitch; Yoshua Bengio; Jackie Chi Kit Cheung"}, {"title": "What does BERT learn about the structure of language", "journal": "", "year": "2019", "authors": "Ganesh Jawahar; Beno\u00eet Sagot; Djam\u00e9 Seddah"}, {"title": "How can we know what language models know?", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020-06", "authors": "Zhengbao Jiang; Frank F Xu"}, {"title": "Compositional word-pair embeddings for cross-sentence inference", "journal": "Long and Short Papers", "year": "2019", "authors": "Mandar Joshi; Eunsol Choi; Omer Levy; Daniel Weld; Luke Zettlemoyer"}, {"title": "SemEval-2012 task 2: Measuring degrees of relational similarity", "journal": "", "year": "2012", "authors": "David Jurgens; Saif Mohammad; Peter Turney; Keith Holyoak"}, {"title": "Dense passage retrieval for open-domain question answering", "journal": "", "year": "2020", "authors": "Vladimir Karpukhin; Barlas Oguz; Sewon Min; Patrick Lewis; Ledell Wu; Sergey Edunov; Danqi Chen; Wen-Tau Yih"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "ALBERT: A lite BERT for self-supervised learning of language representations", "journal": "", "year": "2019", "authors": "Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut"}, {"title": "How many data points is a prompt worth? arXiv e-prints", "journal": "", "year": "2021", "authors": "Le Teven; Alexander M Scao;  Rush"}, {"title": "Linguistically-informed transformations (LIT): A method for automatically generating contrast sets", "journal": "", "year": "2020", "authors": "Chuanrong Li; Lin Shengshuo; Zeyu Liu; Xinyi Wu; Xuhui Zhou; Shane Steinert-Threlkeld"}, {"title": "DIRT -discovery of inference rules from text", "journal": "", "year": "2001", "authors": "Dekang Lin; Patrick Pantel"}, {"title": "Issues in evaluating semantic spaces using word analogies", "journal": "", "year": "2016", "authors": "Tal Linzen"}, {"title": "Zhilin Yang, and Jie Tang. 2021. GPT understands, too", "journal": "", "year": "", "authors": "Xiao Liu; Yanan Zheng; Zhengxiao Du; Ming Ding; Yujie Qian"}, {"title": "RoBERTa: A robustly optimized BERT pretraining approach", "journal": "", "year": "1907", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Discretestate variational autoencoders for joint discovery and factorization of relations", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Diego Marcheggiani; Ivan Titov"}, {"title": "Analogical dissimilarity: definition, algorithms and two experiments in machine learning", "journal": "Journal of Artificial Intelligence Research", "year": "2008", "authors": "Laurent Miclet; Sabri Bayoudh; Arnaud Delhay"}, {"title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"title": "Linguistic regularities in continuous space word representations", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Tomas Mikolov; Yih Wen-Tau; Geoffrey Zweig"}, {"title": "Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Silvia Nec\u015fulescu; Sara Mendes; David Jurgens; N\u00faria Bel; Roberto Navigli"}, {"title": "Scikit-learn: Machine learning in python", "journal": "Journal of Machine Learning Research", "year": "2011", "authors": "Fabian Pedregosa; Ga\u00ebl Varoquaux; Alexandre Gramfort; Vincent Michel; Bertrand Thirion; Olivier Grisel; Mathieu Blondel; Peter Prettenhofer; Ron Weiss; Vincent Dubourg; Jake Vanderplas; Alexandre Passos; David Cournapeau; Matthieu Brucher; Matthieu Perrot; \u00c9douard Duchesnay"}, {"title": "GloVe: Global vectors for word representation", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"title": "Association for Computational Linguistics", "journal": "", "year": "2019", "authors": "Fabio Petroni; Tim Rockt\u00e4schel; Sebastian Riedel; Patrick Lewis; Anton Bakhtin; Yuxiang Wu; Alexander Miller"}, {"title": "The role of analogy in ontology alignment: A study on LISA. Cognitive Systems Research", "journal": "", "year": "2015", "authors": "Elie Raad; Joerg Evermann"}, {"title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"title": "Relation extraction with matrix factorization and universal schemas", "journal": "", "year": "2013", "authors": "Sebastian Riedel; Limin Yao; Andrew Mccallum; Benjamin M Marlin"}, {"title": "How much knowledge can you pack into the parameters of a language model", "journal": "", "year": "2020", "authors": "Adam Roberts; Colin Raffel; Noam Shazeer"}, {"title": "The (too many) problems of analogical reasoning with word vectors", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Anna Rogers; Aleksandr Drozd; Bofang Li"}, {"title": "Relations such as hypernymy: Identifying and exploiting hearst patterns in distributional vectors for lexical entailment", "journal": "", "year": "2016", "authors": "Stephen Roller; Katrin Erk"}, {"title": "The CogALex-V shared task on the corpus-based identification of semantic relations", "journal": "", "year": "2016", "authors": "Enrico Santus; Anna Gladkova; Stefan Evert; Alessandro Lenci"}, {"title": "Nine features in a random forest to learn taxonomical semantic relations", "journal": "", "year": "2016", "authors": "Enrico Santus; Alessandro Lenci; Tin-Shing Chiu; Qin Lu; Chu-Ren Huang"}, {"title": "EVALution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models", "journal": "", "year": "2015", "authors": "Enrico Santus; Frances Yung; Alessandro Lenci; Chu-Ren Huang"}, {"title": "Atomic: An atlas of machine commonsense for ifthen reasoning", "journal": "", "year": "2019", "authors": "Maarten Sap; Emily Ronan Le Bras; Chandra Allaway; Nicholas Bhagavatula; Hannah Lourie; Brendan Rashkin;  Roof; A Noah; Yejin Smith;  Choi"}, {"title": "Understanding learning dynamics of language models with SVCCA", "journal": "", "year": "2019", "authors": "Naomi Saphra; Adam Lopez"}, {"title": "Exploiting cloze-questions for few-shot text classification and natural language inference", "journal": "", "year": "2021", "authors": "Timo Schick; Hinrich Sch\u00fctze"}, {"title": "Facenet: A unified embedding for face recognition and clustering", "journal": "", "year": "2015", "authors": "Florian Schroff; Dmitry Kalenichenko; James Philbin"}, {"title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Taylor Shin; Yasaman Razeghi; Robert L Logan; I V ; Eric Wallace; Sameer Singh"}, {"title": "Improving hypernymy detection with an integrated path-based and distributional method", "journal": "", "year": "2016", "authors": "Vered Shwartz; Yoav Goldberg; Ido Dagan"}, {"title": "Unsupervised information extraction: Regularizing discriminative approaches with relation distribution losses", "journal": "", "year": "2019", "authors": "\u00c9tienne Simon; Vincent Guigue; Benjamin Piwowarski"}, {"title": "Matching the blanks: Distributional similarity for relation learning", "journal": "", "year": "2019", "authors": " Livio Baldini; Nicholas Soares; Jeffrey Fitzgerald; Tom Ling;  Kwiatkowski"}, {"title": "Conceptnet 5.5: An open multilingual graph of general knowledge", "journal": "", "year": "2017", "authors": "Robert Speer; Joshua Chin; Catherine Havasi"}, {"title": "2020. oLMpics-on what language model pre-training captures", "journal": "Transactions of the Association for Computational Linguistics", "year": "", "authors": "Alon Talmor; Yanai Elazar; Yoav Goldberg; Jonathan Berant"}, {"title": "Shashank Srivastava, and Colin Raffel. 2021. Improving and simplifying pattern exploiting training", "journal": "", "year": "", "authors": "Derek Tam; Mohit Rakesh R Menon;  Bansal"}, {"title": "What do you learn from context? probing for sentence structure in contextualized word representations", "journal": "", "year": "2019", "authors": "Ian Tenney; Patrick Xia; Berlin Chen; Alex Wang; Adam Poliak; R Thomas Mccoy; Najoung Kim; Benjamin Van Durme; Samuel R Bowman; Dipanjan Das; Ellie Pavlick"}, {"title": "Measuring semantic similarity by latent relational analysis", "journal": "", "year": "2005", "authors": "D Peter;  Turney"}, {"title": "Domain and function: A dualspace model of semantic relations and compositions", "journal": "Journal of Artificial Intelligence Research", "year": "2012", "authors": "D Peter;  Turney"}, {"title": "Distributional semantics beyond words: Supervised learning of analogy and paraphrase", "journal": "Transactions of the Association for Computational Linguistics", "year": "2013", "authors": "D Peter;  Turney"}, {"title": "Combining independent modules in lexical multiple-choice problems", "journal": "", "year": "2003", "authors": "D Peter; Michael L Turney; Jeffrey Littman; Victor Bigham;  Shnayder"}, {"title": "BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?", "journal": "", "year": "2021", "authors": "Asahi Ushio; Luis Espinosa-Anke; Steven Schockaert; Jose Camacho-Collados"}, {"title": "Quantity doesn't buy quality syntax with neural language models", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Aaron Marten Van Schijndel; Tal Mueller;  Linzen"}, {"title": "Integrating multiplicative features into supervised distributional methods for lexical entailment", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Tu Vu; Vered Shwartz"}, {"title": "Probing pretrained language models for lexical semantics", "journal": "", "year": "2020", "authors": "Ivan Vulic; Maria Edoardo; Robert Ponti; Goran Litschko; Anna Glavas;  Korhonen"}, {"title": "Similarity, precedent and argument from analogy", "journal": "Artificial Intelligence and Law", "year": "2010", "authors": "Douglas Walton"}, {"title": "SphereRE: Distinguishing lexical relations with hyperspherical relation embeddings", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Chengyu Wang; Xiaofeng He; Aoying Zhou"}, {"title": "Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually)", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Alex Warstadt; Yian Zhang; Xiaocheng Li; Haokun Liu; Samuel R Bowman"}, {"title": "Filling missing paths: Modeling co-occurrences of word pairs and dependency paths for recognizing lexical semantic relations", "journal": "", "year": "2018", "authors": "Koki Washio; Tsuneaki Kato"}, {"title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger;  Drame"}, {"title": "Structured relation discovery using generative models", "journal": "", "year": "2011", "authors": "Limin Yao; Aria Haghighi; Sebastian Riedel; Andrew Mccallum"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Pipeline to transform the word pair (h, t) to the relation embedding x.", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "4. 11RelBERT Training Dataset We use the platinum ratings from Se-mEval 2012 Task 2 (Jurgens et al., 2012) as our training dataset for RelBERT. This dataset covers 79 fine-grained semantic relations, which are grouped in 10 categories. For each of the 79 relations, the dataset contains a typicality score for a number of word pairs (around 40 on average), indicating to what extent the word pair is a prototypical instance of the relation. We treat the top 10 pairs (i.e. those with the highest typicality score) as positive examples of the relation, and the bottom 10 pairs as negative examples. We use 80% of these positive and negative examples for training RelBERT (i.e. learning the prompt and fine-tuning the LM) and 20% for validation. Constructing Training Triples We rely on three different strategies for constructing training triples. First, we obtain triples by selecting two positive examples of a given relation type (i.e. from the top-10 pairs) and one negative example (i.e. from the bottom-10 pairs). We construct 450 such triples per relation. Second, we construct triples by using two positive examples of the relation and one positive example from another relation (which is assumed to correspond to a negative example). In particular, for efficiency, we use the anchors and positive examples of the other triples from the same batch as negative examples (while ensuring that these", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Batch augmentation where the original batch with m samples is augmented with 2m(m\u22121) samples.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "On SAT \u2020, RelBERT outperforms LRA, which represents the state-of-the-art in the zero-shot setting, i.e. in the setting where no training data from the SAT dataset is used. RelBERT moreover outperforms GPT-3 in the few-shot setting, despite not using any training examples. In contrast, GPT-3 encodes a number of training examples as part of the prompt.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Test accuracy (%) on analogy dataset of the vanilla RoBERTa model (i.e. without fine-tuning) and variants of RelBERT with different language models. Each variant uses the best manual prompt based on the SemEval tuning data.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure4: Test accuracy drop of the vanilla models without fine-tuning (measured in terms of absolute percentage points in comparison with RelBERT) on analogy datasets.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_6", "figure_caption": "-4, 150) (1e-3, 200) (1e-4, 150) CogA (1e-3, 100) (1e-3, 100) (1e-3, 100) EVAL (1e-4, 100) (1e-3, 200) (1e-4, 100) K&H (1e-4, 150) (1e-4, 150) (1e-4, 200) ROOT (1e-5, 200) (1e-4, 100) (1e-4", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Number of instances for each relation type across training/validation/test sets of all lexical relation classification datasets.", "figure_data": "BLESSCogALexEVALutionK&H+NROOT09Random8,529/609/3,008 2,228/3,059-18,319/1,313/6,746 4,479/327/1,566Meronym2,051/146/746163/224218/13/86755/48/240-Event2,657/212/955----Hypernym924/63/350255/382 1,327/94/4593,048/202/1,0422,232/149/809Co-hyponym2,529/154/882--18,134/1,313/6,3492,222/162/816Attribute1,892/143/696-903/72/322--Possession--377/25/142--Antonym-241/360 1,095/90/415--Synonym-167/235759/50/277--"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": ").", "figure_data": "ModelSAT \u2020 SAT U2U4 Google BATSRandom20.0 20.0 23.6 24.2 25.0 25.0PMI23.3 23.1 32.9 39.1 57.4 42.7LRA56.4-----SuperSim54.8-----GPT-3 (zero) 53.7-----GPT-3 (few) 65.2*-----RELATIVE24.9 24.6 32.5 27.1 62.0 39.0pair2vec33.7 34.1 25.4 28.2 66.6 53.8GloVe48.9 47.8 46.5 39.8 96.0 68.7FastText49.7 47.8 43.0 40.7 96.6 72.0Analogical Proportion Score\u2022 GPT-241.4 35.9 41.2 44.9 80.4 63.5\u2022 BERT32.6 32.9 32.9 34.0 80.8 61.5\u2022 RoBERTa49.6 42.4 49.1 49.1 90.8 69.7Analogical Proportion Score (tuned)\u2022 GPT-257.8* 56.7* 50.9* 49.5* 95.2* 81.2*\u2022 BERT42.8* 41.8* 44.7* 41.2* 88.8* 67.9*\u2022 RoBERTa55.8* 53.4* 58.3* 57.4* 93.6* 78.4*RelBERT\u2022 Manual69.5 70.6 66.2 65.3 92.4 78.8\u2022 AutoPrompt 61.0 62.3 61.4 63.0 88.2 74.6\u2022 P-tuning54.0 55.5 58.3 55.8 83.4 72.1"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Macro/micro F1 score (%) for lexical relation classification.", "figure_data": "BLESS CogALexEVALK&H+N ROOT09rand 93.7 (+0.3) 94.3 (-0.2)-97.9 (+0.2) 91.2 (-0.1)mero 89.8 (+1.4) 72.9 (+2.7) 69.2 (+1.9) 74.5 (+5.4)-event 86.5 (-0.3)----hyp 94.1 (+0.8) 60.9 (-0.7) 61.7 (-1.5) 93.5 (+5.0) 83.0 (-0.4)cohyp 96.4 (+0.3)--97.8 (+1.2) 97.4 (-0.5)attr92.6 (+0.3)-84.7 (+1.6)--poss--67.1 (-0.2)--ant-76.8 (-2.6) 81.3 (-0.9)--syn-49.9 (-0.6) 53.6 (+2.7)--macro 92.2 (+0.5) 71.0 (-0.2) 69.3 (+0.9) 90.9 (+2.9) 90.5 (-0.4)micro 92.5 (+0.4) 86.9 (-0.1) 70.2 (+0.6) 97.2 (+1.0) 90.7 (-0.3)"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Per-class F1 score of RelBERT trained without hypernymy relations and the absolute difference with respect to the original model (parentheses), along with the macro and micro averaged F1 for each dataset (%).includes the hypernymy relation. Hypernymy is of particular interest, as it can be found across all the considered lexical relation classification datasets, which is itself a reflection of its central importance in lexical semantics. In Table4, we report the difference in performance compared to the original RelBERT model (i.e. the model that was fine-tuned on the full SemEval training set). As can be seen, the overall changes in performance are small, and the new version actually outperforms the original RelBERT model on a few datasets. In particular, hypernymy is still modelled well, confirming that RelBERT is able to generalize to unseen relations.", "figure_data": "ModelGoogle Mor SemBATS Mor Sem LexFastText95.4 98.190.4 71.1 33.8Manual89.8 95.887.0 66.2 75.1AutoPrompt90.5 85.185.3 59.8 68.0P-tuning87.4 78.182.9 60.9 61.8Table 5: Test accuracy for the high-level categories ofBATS and Google, comparing FastText and RelBERT."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Nearest neighbors of selected word pairs, in terms of cosine similarity between RelBERT embeddings. Candidate word pairs are taken from the RELATIVE pair vocabulary.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ": Test accuracy (%) of ALBERT, BERT, andRoBERTa on analogy datasets."}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": "shows a detailed break-down of the BATSresults."}, {"figure_label": "89", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Macro/micro F1 score (%) for lexical relation classification of ALBERT, BERT, and RoBERTa. Test accuracy (%) on analogy datasets (SAT \u2020 refers to the full SAT dataset).", "figure_data": "ModelSAT \u2020 SAT U2U4 Google BATSGloVediff diff +rel diff +pair48.9 47.8 46.5 47.8 45.9 40.4 46.9 35.4 35.1 33.8 29.4 30.696.0 87.6 78.068.7 67.3 56.3FastTextdiff diff +rel diff +pair49.7 47.8 43.0 40.7 37.3 35.9 39.5 35.6 33.4 33.5 27.2 28.796.6 85.8 75.472.0 67.5 52.1RelBERTManual AutoPrompt 61.0 62.3 61.4 63.0 69.5 70.6 66.2 65.3 P-tuning 54.0 55.5 58.3 55.892.4 88.2 83.478.8 74.6 72.1ModelPrompt\u03c0 \u03c4 \u03b3 template typeManual---3BERTAutoPrompt 8 2 3-P-tuning8 2 2-Manual---4ALBERTAutoPrompt 8 3 3-P-tuning8 2 3-Manual---4RoBERTaAutoPrompt 9 2 2-P-tuning9 3 2-"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Best prompting configuration.", "figure_data": ""}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Best MLP configuration for the relation classification experiment. Each entry shows the learning rate and hidden layer size. Note that CogALex uses the default configuration due to the lack of validation set.Table 12: Break-down of BATS results per relation type. , restaurant:kitchen, restaurant:grill, nightclub:open, pub:bar, night:concert restaurant:waitress restaurant:waiter, diner:waitress, bar:bartender, hospital:nurse, courthouse:clerk, office:clerk coincidentally:first, ironically:first, ironically:name, notably:three, however:new, instance:character car:garage car:pit, plane:hangar, auto:garage, baby:crib, yacht:harbour, aircraft:hangarshelter:house, elevator:building, worker:mine, worker:factory, plane:hangar, horse:stable ice:melt snow:melt, glacier:melt, ice:drift, crust:melt, polar ice:melt, ice:thaw glacier:melt, snow:melt, water:freeze, crack:form, ice:surface, ice:freeze dolphin:swim squid:swim, salmon:swim, shark:swim, fish:swim, horse:run, frog:leap fisherman:fish, fisherman:catch, must:protect, diver:underwater, dog:human, scheme:make flower:fragrant orchid:fragrant, cluster:fragrant, jewel:precious, jewel:valuable, soil:permeable, vegetation:abundant flower:greenish, flower:white, flower:yellowish, flower:creamy, flower:pale yellow, flower:arrange coconut:milk coconut:oil, goat:milk, grape:juice, palm:oil, olive:oil, camel:milk dry:powder, mix:sugar, candy:chocolate, cook:fry, butter:oil, milk:coffee bag:plastic bottle:plastic, bag:leather, container:plastic, box:plastic, jug:glass, bottle:glass tube:glass, bottle:plastic, typically:plastic, frame:steel, shoe:leather, wire:metal duck:duckling chicken:chick, pig:piglet, cat:kitten, ox:calf, butterfly:larvae, bear:cub adult:young, worker:queen, queen:worker, bird:fly, chick:adult, female:mat Gender man:woman men:women, male:female, father:mother, boy:girl, hero:heroine, king:queen man:boy, woman:child, child:youth, officer:crew, bride:groom, child:teen Antonymy cooked:raw raw:cooked, regulated:unregulated, sober:drunk, loaded:unloaded, armed:unarmed, published:unpublished annual:biennial, raw:cook, herb:subshrub, aquatic:semi, shrub:small, fry:grill normal:abnormal ordinary:unusual, usual:unusual, acceptable:unacceptable, stable:unstable, rational:irrational, legal:illegal acute:chronic, mouse:human, negative:positive, fat:muscle, cell:tissue, motor:sensory Meronymy helicopter:rotor helicopter:rotor blades, helicopter:wing, bicycle:wheel, motorcycle:wheel, airplane:engine, plane:engine aircraft:engine, engine:crankshaft, landing gear:wheel, engine:camshaft, rotor:blade, aircraft:wing bat:wing butterfly:wing, eagle:wing, angel:wing, cat:paw, lion:wings, fly:wing mouse:tail, dog:like, dragon:like, human:robot, leopard:spot, cat:like beer:alcohol wine:alcohol, cider:alcohol, soda:sugar, beer:liquor, beer:malt, lager:alcoho steel:carbon, cider:alcohol, humidity:average, rate:average, household:non, consume:beer oxygen:atmosphere helium:atmosphere, hydrogen:atmosphere, nitrogen:atmosphere, methane:atmosphere, carbon:atmosphere carbon dioxide:atmosphere, cloud:atmosphere, methane:atmosphere, nitrogen:soil, gas:atmosphere Hypernymy chihuahua:dog dachshund:dog, poodle:dog, terrier:dog, chinchilla:rodent, macaque:monkey, dalmatian:dog julie:katy, tench:pike, catfish:pike, sunfish:perch, salmonid:salmon, raw:marinate pelican:bird toucan:bird, puffin:bird, egret:bird, peacock:bird, grouse:bird, pigeon:bird drinking:contaminate, drinking:source, pelican:distinctive, boiling:pour, aquifer:table, fresh:source tennis:sport hockey:sport, soccer:sport, volleyball:sport, cricket:sport, golf:sport, football:sport hockey:sport, golf:sport, badminton:sport, boxing:sport, rowing:sport, volleyball:sport Morphology dog:dogs cat:cats, horse:horses, pig:pigs, rat:rats, wolf:wolves, monkey:monkeys shepherd:dog, landrace:breed, like:dog, farm:breed, breed:animal, captive:release tall:tallest strong:strongest, short:shortest, smart:smartest, weak:weakest, big:biggest, small:smallest rank:world, summit:nato, redistricting:district, delegation:congress, debate:congress spy:espionage pirate:piracy, robber:robbery, lobbyist:lobbying, scout:scouting, terrorist:terrorism, witch:witchcraft group:call, crime:criminal, action:involve, cop:police, action:one, group:make", "figure_data": "RelationFastText Manual AutoPrompt P-tuningUK city:county33.328.928.940.0animal:shelter44.488.977.884.4animal:sound80.086.782.275.6animal:young53.362.264.451.1Encyclopediccountry:capital country:language82.2 93.337.8 51.117.8 55.635.6 51.1male:female88.960.055.662.2name:nationality60.073.351.140.0name:occupation86.775.675.677.8things:color88.997.888.991.1antonyms:binary26.764.468.977.8antonyms:gradable44.488.993.388.9hypernyms:animals44.491.180.055.6hypernyms:misc42.271.160.064.4Lexicalhyponyms:misc meronyms:member31.1 44.455.6 68.955.6 48.948.9 53.3meronyms:part31.177.871.155.6meronyms:substance26.775.666.753.3synonyms:exact17.880.071.166.7synonyms:intensity28.977.864.453.3adj+ly95.684.488.982.2adj+ness100.097.893.397.8adj:comparative100.097.8100.091.1adj:superlative97.8100.093.3100.0noun+less77.8100.097.8100.0over+adj75.684.480.082.2un+adj60.097.891.197.8verb 3pSg:v+ed100.075.684.468.9verb inf:3pSg100.093.391.184.4Morphologicalverb inf:v+ed100.091.191.188.9verb inf:v+ing100.097.897.895.6verb v+ing:3pSg97.882.268.968.9verb v+ing:v+ed97.886.782.284.4verb+able97.893.380.084.4verb+er95.6100.095.695.6verb+ment95.677.877.862.2verb+tion84.477.866.768.9noun:plural78.787.688.869.7re+verb75.662.286.766.7"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Nearest neighbors of selected word pairs, in terms of cosine similarity between RelBERT embeddings. Candidate word pairs are taken from the RELATIVE pair vocabulary.", "figure_data": ""}], "doi": "10.1162/tacl_a_00051"}