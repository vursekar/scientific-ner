{"authors": "Kartik Chandra; Chuma Kabaghe; Gregory Valiant", "pub_date": "", "title": "Beyond Laurel/Yanny: An Autoencoder-Enabled Search for Polyperceivable Audio", "abstract": "The famous \"laurel/yanny\" phenomenon references an audio clip that elicits dramatically different responses from different listeners. For the original clip, roughly half the population hears the word \"laurel,\" while the other half hears \"yanny.\" How common are such \"polyperceivable\" audio clips? In this paper we apply ML techniques to study the prevalence of polyperceivability in spoken language. We devise a metric that correlates with polyperceivability of audio clips, use it to efficiently find new \"laurel/yanny\"-type examples, and validate these results with human experiments. Our results suggest that polyperceivable examples are surprisingly prevalent, existing for >2% of English words. 1   ", "sections": [{"heading": "Introduction", "text": "How robust is human sensory perception, and to what extent do perceptions differ between individuals? In May 2018, an audio clip of a man speaking the word \"laurel\" received widespread attention because a significant proportion of listeners confidently reported hearing not the word \"laurel,\" but rather the quite different sound \"yanny\" (Salam and Victor, 2018). At first glance, this suggests that the decision boundaries for speech perception vary considerably among individuals. The reality is more surprising: almost everyone has a decision boundary between the sounds \"laurel\" and \"yanny,\" without a significant \"dead zone\" separating these classes. The audio clip in question lies close to this decision boundary, so that if the clip is slightly perturbed (e.g. by damping certain frequencies or slowing down the playback rate), individuals switch from confidently perceiving \"laurel\" to confidently perceiving \"yanny,\" with the exact point of switching varying slightly from person to person.\nHow common is this phenomenon? Specifically, what fraction of spoken language is \"polyperceivable\" in the sense of evoking a multimodal response in a population of listeners? In this work, we provide initial results suggesting a significant density of spoken words that, like the original \"laurel/yanny\" clip, lie close to unexpected decision boundaries between seemingly unrelated pairs of words or sounds, such that individual listeners can switch between perceptual modes via a slight perturbation.\nThe clips we consider consist of audio signals synthesized by the Amazon Polly speech synthesis system with a slightly perturbed playback rate (i.e. a slight slowing-down of the clip). Though the resulting audio signals are not \"natural\" stimuli, in the sense that they are very different from the result of asking a human to speak slower (see Section 5), we find that they are easy to compute and reliably yield compelling polyperceivable instances. We encourage future work to investigate the power of more sophisticated perturbations, as well as to consider natural, ecologically-plausible perturbations.\nTo find our polyperceivable instances, we (1) devise a metric that correlates with polyperceivability, (2) use this metric to efficiently sample candidate audio clips, and (3) evaluate these candidates on human subjects via Amazon Mechanical Turk. We present several compelling new examples of the \"laurel/yanny\" effect, and we encourage readers to listen to the examples included in the supplementary materials (also available online at https://theory.stanford.edu/ valiant/polyperceivable/index.html).\nFinally, we estimate that polyperceivable clips can be made for >2% of English words.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Method", "text": "To investigate polyperceivability in everyday auditory input, we searched for audio clips of single spoken words that exhibit the desired effect. Our method consisted of two phases: (1) sample a large number of audio clips that are likely to be polyperceivable, and (2) collect human perception data on those clips using Amazon Mechanical Turk to identify perceptual modes and confirm polyperceivability.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Sampling clips", "text": "To sample clips that were likely candidates, we trained a simple autoencoder for audio clips of single words synthesized using the Amazon Polly speech synthesis system. Treating the autoencoder's low-dimensional latent space as a proxy for perceptual space, we searched for clips that travel through more of the space as the playback rate is slowed from 1.0\u00d7 to 0.6\u00d7. Intuitively, a longer path through encoder space should correspond to a more dramatic change in perception as the clip is slowed down (Section 3 presents some data supporting this).\nConcretely, we computed a score S proportional to the length of the curve swept by the encoder E in latent space as the clip is slowed down, normalized by the straight-line distance traveled: that is, we define S(c) = 0.6\u00d7 r=1.0\u00d7 ||dE(c,r)/dr||dr ||E(c,0.6\u00d7)\u2212E(c,1.0\u00d7)|| . Then, with probability proportional to e 0.2\u2022S , we importancesampled 200 clips from the set of audio clips of the top 10,000 English words, each spoken by all 16 voices offered by Amazon Polly (spanning American, British, Indian, Australian, and Welsh accents, and male and female voices). The distributions of S in the population and our sample is shown in Figure 2.\nAutoencoder details Our autoencoder operates on one-second audio clips sampled at 22,050 Hz, which are converted to spectrograms with a window size of 256 and then flattened to vectors in R 90,000 . The encoder is a linear map to R 512 with ReLU activations, and the decoder is a linear map back to R 90,000 space with pointwise squaring. We used an Adam optimizer with lr=0.01, training on a corpus of 16,000 clips (randomly resampled to between 0.6x and 1.0x the original speed) for 70 epochs with a batch size of 16 (\u2248 8 hours on an AWS c5.4xlarge EC2 instance).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Mechanical Turk experiments", "text": "Each Mechanical Turk worker was randomly assigned 25 clips from our importance-sampled set of 200. Each clip was slowed to either 0.9x, 0.75x, or 0.6x the original rate. Workers responded with a perceived word and a confidence score for each clip. We collected responses from 574 workers, all of whom self-identified as US-based native English speakers. This yielded 14,370 responses (\u2248 72 responses per clip).\nNext, we manually reviewed these responses and selected the most promising clips for a second round with only 11 of the 200 clips. Note that because these selections were made by manual review (i.e. listening to clips ourselves), there is a chance we passed over some polyperceivable clips -this means that our computations in Section 3 are only a conservative lower bound. For this round, we also included clips of the 5 words identified by Guan and Valiant (2019), 12 potentially-polyperceivable words we had found in earlier experiments, and \"laurel\" as controls. We collected an additional 3,950 responses among these 29 clips (\u2248 136 responses per clip) to validate that they were indeed polyperceivable.\nFinally, we took the words associated with these 29 clips and produced a new set of clips using each of the 16 voices, for a total of 464 clips. We collected 4,125 responses for this last set (\u2248 3 responses for each word/voice/rate combination).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "Are the words we found polyperceivable? To identify cases where words had multiple perceptual \"modes,\" we looked for clusters in the distribution of responses for each of the 29 candidate words. Concretely, we treated responses as \"bags of phonemes\" and then applied K-means. Though this rough heuristic discards information about the order of phonemes within a word, it works sufficiently well for clustering, especially since most of our words have very few syllables (more sophisticated models of phonetic similarity exist, but they would not change our results).\nWe found that the largest cluster typically contained the original word and rhymes, whereas other clusters represented significantly different perceptual modes. Some examples of clusters and their relative frequency are available in  the prevalance of alternate modes among our clips increases.\nHow prevalent are polyperceivable words? Of our initial sample of 200 words, 11 ultimately yielded compelling demonstrations. To compute the prevalence of polyperceivable words in the population of the top 10k words, we have to account for the importance sampling weights we used when sampling in Section 2.1. After scaling each word's contribution by the inverse of the probability of including that word in our nonuniform sample of 200, we conclude that polyperceivable clips exist for at least 2% of the population: that is, of the 16 voices under consideration, at least one yields a polyperceivable clip for >2% of the top 10k English words.\nWe emphasize that this is a conservative lower bound, because it assumes that there were no other polyperceivable words in the 200 words we sampled, besides the 11 that we selected for the second round. We did not conduct an exhaustive search among those 200 words, instead focusing our Mechanical Turk resources on only the most promising candidates.  Is S a good metric? We consider the metric S to be successful because it allowed us to efficiently find several new polyperceivable instances. If the 200 words were sampled uniformly instead of being importance-sampled based on S, we would only have found 4 polyperceivable words in expectation (2% of 200). Thus, importance sampling increased our procedure's recall by almost 3\u00d7.\nFor a more quantitative understanding, we analyzed the relationship between \"autoencoder path length\" S and \"perceptual path length\" T . Our measure T of \"perceptual path length\" for a clip is change in average distance between source word and response as we slow the clip down from 0.75\u00d7 to 0.6\u00d7. As with clustering above, distance is measured in bag-of-phonemes space. For each word, we computed the correlation between S and T among the 16 voices (both S and T vary significantly across voices). For all but 5 of our 29 words these metrics correlated positively, though with varying strength (Figure 3). This suggests that S indeed correlates with polyperceivability.\n4 Discussion: Why study quirks of human perception in an ACL paper?\nPerceptual instability in human sensory systems offers insight into ML systems. The question of what fraction of natural inputs lie close to decision boundaries for trained ML systems has received enormous attention. The surprising punchline that has emerged over the past decade is that most natural examples (including points in the training set) actually lie extremely close to unexpected decision boundaries. For most of these points, a tiny but carefully-crafted perturbation can lead the ML system to change the label. Such perturbations are analogous to the slight perturbation in playback speed for the polyperceivable clips we consider. In the ML literature, these perturbations, referred to as \"adversarial examples\" seem pervasive across complex ML systems (Szegedy et al., 2013;Goodfellow et al., 2014;Nguyen et al., 2015;Moosavi-Dezfooli et al., 2016;Madry et al., 2017;Raghunathan et al., 2018;Athalye et al., 2017). While the initial work on adversarial examples focused on computer vision, more recent work shows the presence of such examples across other settings, including reinforcement learning (Huang et al., 2017), reading comprehension (Jia and Liang, 2017), and speech recognition (Carlini and Wag-ner, 2018;Qin et al., 2019). Studying perceptual illusions would provide a much-needed reference when evaluating ML systems in these domains. For vision tasks, for example, human vision provides the only evidence that current ML models are far from optimal in terms of robustness to adversarial examples. However, while humans are certainly not as susceptible to adversarial examples as ML systems, we lack quantified bounds on human robustness. More broadly, understanding which systems (both biological and ML) have decision boundaries that lie surprisingly close to many natural inputs may inform our sense of what settings are amenable to adversarially robust models, and what settings inherently lead to vulnerable classifiers.\nPerceptual instability in ML systems offers insight into human sensory systems. Recent research on adversarial robustness of ML models has provided a trove of new tools and perspectives for probing classifiers and exploring the geometry of decision boundaries. These tools cannot directly be applied to study the decision boundaries of biological classifiers (e.g. we cannot reasonably do \"gradient descent\" on human subjects). However, using standard data-driven deep learning techniques to model human perceptual systems can allow us to apply these techniques by proxy.\nAn example can be found in the study of \"transferability.\" Adversarial examples crafted to fool a specific model often also fool other models, even those trained on disjoint training sets (Papernot et al., 2016a;Tram\u00e8r et al., 2017;Liu et al., 2016). This prompts the question of whether adversarial examples crafted for an ML model might also transfer to humans. Recent surprising work by Elsayed et al. (2018) explores this question for vision. Humans were shown adversarial examples trained for an image classifier for \u2248 70ms, and asked to choose between the correct label and the classifier's (incorrect) predicted label. Humans selected the incorrect label more frequently when shown adversarial examples than when shown unperturbed images. Similarly, Hong et al. (2014) trained a lowdimensional representation of \"perceptual space,\" and used the decision boundaries of the model to find images that confused human subjects.", "n_publication_ref": 16, "n_figure_ref": 1}, {"heading": "Related work", "text": "An enormous body of work from cognitive sciences communities explores the quirks of human/animal sensory systems (Fahle et al., 2002). These works often have the explicit goal of exploring isolated \"illusions\" that provide insights into our perceptual systems (Davis and Johnsrude, 2007;Fritz et al., 2005). However, there are few efforts to quantify the extent to which \"typical\" instances are polyperceivable or lie close to decision boundaries. Miller (1981) studies the effect of speaking rate on how listeners perceive phonemes. The perceptual shifts studied therein are between phonetically adjacent perceptions (e.g. \"pip\" vs. \"peep\") rather than dramatically different perceptions (e.g. \"laurel\" vs. \"yanny\"). The \"perturbation\" of increasing human speaking rate is much more complex than simply linearly scaling the playback rate of an audio clip. Speaking-rate induced shifts also seem to hold more universally across voices, as opposed to the polyperceivable instances we examine.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Future work", "text": "Priming effects It is possible to use additional stimuli to alter perceptions of the \"laurel/yanny\" audio clip. For example, Bosker (2018) demonstrates the ability to control a listener's perception by \"priming\" them with a carefully crafted recording before the polyperceivable clip is played. Similarly, Guan and Valiant (2019) investigated the \"McGurk effect\" (McGurk and MacDonald, 1976), where what one \"sees\" affects what one \"hears.\" The work estimated the fraction of spoken words that, when accompanied by a carefully designed video of a human speaker, would be perceived as significantly different words by listeners. Such phenomena raise questions about how our autoencoder-based method can be extended to search for \"priming-sensitive\" polyperceivability.\nSecurity implications Just as adversarial examples for DNNs have security implications (Papernot et al., 2016b;Carlini and Wagner, 2017;Liu et al., 2016), so too might adversarial examples for sensory systems. For example, if a video clip of a politician happens to be polyperceivable, an adversary could lightly edit it with potentially significant ramifications. A thorough treatment of such security implications is left to future work.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we leveraged ML techniques to study polyperceivability in humans. By modeling perceptual space as the latent space of an autoencoder, we were able to discover dozens of new polyper-ceivable instances, which were validated with Mechanical Turk experiments. Our results indicate that polyperceivability is surprisingly prevalent in spoken language. More broadly, we suggest that the study of perceptual illusions can offer insight into machine learning systems, and vice-versa.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "We would like to thank Melody Guan for early discussions on this project, and the anonymous reviewers for their thoughtful suggestions. This research was supported by a seed grant from Stanford's HAI Institute, NSF award AF-1813049 and ONR Young Investigator Award N00014-18-1-2295.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "", "journal": "", "year": "2017", "authors": "Anish Athalye; Logan Engstrom; Andrew Ilyas; Kevin Kwok"}, {"title": "Putting laurel and yanny in context", "journal": "The Journal of the Acoustical Society of America", "year": "2018", "authors": "Rutger Hans;  Bosker"}, {"title": "Towards evaluating the robustness of neural networks", "journal": "IEEE", "year": "2017", "authors": "Nicholas Carlini; David Wagner"}, {"title": "Audio adversarial examples: Targeted attacks on speech-totext", "journal": "IEEE", "year": "2018", "authors": "Nicholas Carlini; David Wagner"}, {"title": "Hearing speech sounds: top-down influences on the interface between audition and speech perception", "journal": "Hearing research", "year": "2007", "authors": "H Matthew; Ingrid S Johnsrude Davis"}, {"title": "Adversarial examples that fool both computer vision and time-limited humans", "journal": "", "year": "2018", "authors": "Gamaleldin Elsayed; Shreya Shankar; Brian Cheung; Nicolas Papernot; Alexey Kurakin; Ian Goodfellow; Jascha Sohl-Dickstein"}, {"title": "Perceptual learning", "journal": "MIT Press", "year": "2002", "authors": "Manfred Fahle; Tomaso Poggio; A Tomaso;  Poggio"}, {"title": "Differential dynamic plasticity of a1 receptive fields during multiple spectral tasks", "journal": "Journal of Neuroscience", "year": "2005", "authors": "B Jonathan; Mounya Fritz;  Elhilali;  Shihab;  Shamma"}, {"title": "Explaining and harnessing adversarial examples", "journal": "", "year": "2014", "authors": "J Ian; Jonathon Goodfellow; Christian Shlens;  Szegedy"}, {"title": "A surprising density of illusionable natural speech", "journal": "", "year": "2019", "authors": "Y Melody; Gregory Guan;  Valiant"}, {"title": "Large-scale characterization of a universal and compact visual perceptual space", "journal": "", "year": "2014", "authors": "Ha Hong; Ethan Solomon; Dan Yamins; James J Dicarlo"}, {"title": "Adversarial attacks on neural network policies", "journal": "", "year": "2017", "authors": "Sandy Huang; Nicolas Papernot; Ian Goodfellow; Yan Duan; Pieter Abbeel"}, {"title": "Adversarial examples for evaluating reading comprehension systems", "journal": "EMNLP", "year": "2017", "authors": "Robin Jia; Percy Liang"}, {"title": "Delving into transferable adversarial examples and black-box attacks", "journal": "", "year": "2016", "authors": "Yanpei Liu; Xinyun Chen; Chang Liu; Dawn Song"}, {"title": "Towards deep learning models resistant to adversarial attacks", "journal": "", "year": "2017", "authors": "Aleksander Madry; Aleksandar Makelov; Ludwig Schmidt; Dimitris Tsipras; Adrian Vladu"}, {"title": "Hearing lips and seeing voices", "journal": "Nature", "year": "1976", "authors": "Harry Mcgurk; John Macdonald"}, {"title": "Effects of speaking rate on segmental distinctions. Perspectives on the study of speech", "journal": "", "year": "1981", "authors": "L Joanne;  Miller"}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "journal": "", "year": "2016", "authors": "Alhussein Seyed-Mohsen Moosavi-Dezfooli; Pascal Fawzi;  Frossard"}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "journal": "", "year": "2015", "authors": "Anh Nguyen; Jason Yosinski; Jeff Clune"}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "journal": "", "year": "2016", "authors": "Nicolas Papernot; Patrick Mcdaniel; Ian Goodfellow"}, {"title": "The limitations of deep learning in adversarial settings", "journal": "IEEE", "year": "2016", "authors": "Nicolas Papernot; Patrick Mcdaniel; Somesh Jha; Matt Fredrikson; Ananthram Berkay Celik;  Swami"}, {"title": "Imperceptible, robust, and targeted adversarial examples for automatic speech recognition", "journal": "", "year": "2019", "authors": "Yao Qin; Nicholas Carlini; Ian Goodfellow; Garrison Cottrell; Colin Raffel"}, {"title": "Certified defenses against adversarial examples", "journal": "", "year": "2018", "authors": "Aditi Raghunathan; Jacob Steinhardt; Percy Liang"}, {"title": "Yanny or laurel? how a sound clip divided america. The New York Times", "journal": "", "year": "2018", "authors": "Maya Salam; Daniel Victor"}, {"title": "Intriguing properties of neural networks", "journal": "", "year": "2013", "authors": "Christian Szegedy; Wojciech Zaremba; Ilya Sutskever; Joan Bruna; Dumitru Erhan; Ian Goodfellow; Rob Fergus"}, {"title": "The space of transferable adversarial examples", "journal": "", "year": "2017", "authors": "Florian Tram\u00e8r; Nicolas Papernot; Ian Goodfellow; Dan Boneh; Patrick Mcdaniel"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Distribution of path lengths (the S metric) in the population (top 10k English words, all 16 voices) and our sample of 200.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ", and the relative cluster sizes as a function of playback rate are shown in Figure1. As the rate is perturbed,", "figure_data": "Perceived soundPlayback rate0.90\u00d7 0.75\u00d7 0.60\u00d7laurel/lauren/moral/floral0.860.640.19manly/alley/marry/merry/mary0.00.030.35thrilling0.630.470.33flowing/throwing0.340.500.58settle0.650.250.33civil0.320.640.48claimed/claim/climbed0.580.340.11framed/flam(m)ed/friend/ find0.330.520.43leg0.500.310.10lake0.460.340.14growing/rowing0.500.470.26brewing/booing/boeing0.190.230.26third0.400.100.10food/foot0.180.290.13idly/ideally0.380.300.03natalie0.250.270.09fiend0.220.340.32themed0.110.170.24bologna/baloney/bellany0.260.000.00(good)morning0.030.280.77thumb0.660.740.79fem(me)/firm0.060.100.12frank/flank0.720.960.43strength0.080.000.15round0.530.380.65world0.030.000.14"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}], "doi": ""}