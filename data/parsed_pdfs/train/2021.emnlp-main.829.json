{"authors": "Fran\u00e7ois Lagunas; Ella Charlaix; Victor Sanh; Alexander M Rush", "pub_date": "", "title": "Block Pruning For Faster Transformers", "abstract": "Pre-training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up inference. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine-tuning. We find that this approach learns to prune out full components of the underlying model, such as attention heads. Experiments consider classification and generation tasks, yielding among other results a pruned model that is a 2.4x faster, 74% smaller BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models in speed and pruned models in size.", "sections": [{"heading": "Introduction", "text": "Pre-trained transformer models are the standard for NLP tasks in both classification and generation tasks (Devlin et al., 2019;Lewis et al., 2020). The recent trend is for models to continue to grow in size while yielding improved performance on standard benchmarks (Rosset, 2020). This development highlights the need to reduce the storage size and increase the efficiency of pre-trained models.\nPruning methods have shown to be extremely effective at reducing the storage size of models fine-tuned for a specific task. Approaches such as magnitude pruning (Han et al., 2015), L0 regularization (Louizos et al., 2018), lottery ticket hypothesis (Frankle and Carbin, 2018), diff pruning (Guo et al., 2020), and movement pruning (Sanh et al., 2020) have demonstrated remarkable reductions in model size. Movement pruning produces 77% savings in parameter storage for a 1% drop in accuracy on SQuAD v1.1. However, these models yield very little actual efficiency benefits, as to run them in standard hardware often requires reconstructing the original dense shape.\nOn the other hand distillation methods have been more effective at producing faster models as has been shown by DistilBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2019) or MobileBERT (Sun et al., 2020). These approaches utilize targeted distillation to produce smaller models with a dense structure that is fast on standard hardware. However without careful engineering and size selection these models are much larger than pruned ones.\nIn this work, we target closing this gap through block pruning. Unlike pruning individual parameters, this approach encourages pruning that can be optimized on dense hardware. It is a less rigid approach than row or column-based pruning typically used in structured approaches (McCarley, 2019), which have been difficult to apply effectively to transformers. We integrate this approach with Movement pruning (Sanh et al., 2020), a simple method for pruning pre-trained models during fine-tuning. The final method 1 has few additional hyperparameters or training requirements.\nExperiments consider a large variety of different benchmark datasets comparing accuracy and efficiency. We find a surprising result that despite utilizing sub-row square blocks during training, the approach learns to eliminate full components of the model, effectively dropping a large number of attention heads. This effect allows the model to achieve speedups even beyond standard structured pruning of feed-forward layers. Results show a 2.4x speedup on SQuAD v1.1 with a 1% drop of F1, and a 2.3x speedup on QQP with a 1% loss of F1. Experiments on summarization also show a 1.39x speedup for an average of 2 points drop on all ROUGE metrics on CNN/DailyMail, and for a reduction of decoder weights of 3.5x. 1 Available at https://github.com/ huggingface/nn_pruning", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "Related Work", "text": "There has been a growing interest in the compression of pre-trained language models. We consider three varieties of methods: distillation, pruning, and structured pruning.\nKnowledge distillation, introduced by Hinton et al. (2015), is a popular compression technique. Researchers have applied this method to a variety of NLP models (Tang et al., 2019;Sun et al., 2019;Turc et al., 2019). Distillation has been used to obtain significantly smaller BERT models achieving competitive performances. Sanh et al. (2019) distills BERT into shallower students during the pre-training stage and optionally during the finetuning stage. MobileBERT (Sun et al., 2020) and TinyBERT (Jiao et al., 2019) are obtained thanks to a layer-wise distillation strategy. While the distillation of former is task-agnostic, the one used to obtain the latter is task-specific.\nOther previous work has focused on unstructured pruning (LeCun et al., 1989;Han et al., 2015;Frankle and Carbin, 2018). When targeting transformer models, it is typical to select the weights to prune based on their magnitude (Gordon et al., 2020), or by computing an importance score using a firstorder method (Sanh et al., 2020). While these methods allow for a significant reduction in model size, specialized hardware is required to make use of the resulting unstructured sparse matrices in order to speed up inference.\nIn contrast, structured pruning removes coherent groups of weights (Murray and Chiang, 2015;See et al., 2016;Joulin et al., 2016;Fan et al., 2020;Sajjad et al., 2020). Recent works (Michel et al., 2019;Voita et al., 2019) show that some heads can be removed without significant degradation in performance, leading to the conclusion that most heads provide redundant information. Other authors have worked on combining matrix factorization and weight pruning. While Mao et al. (2020) combine SVD-based matrix factorization with unstructured pruning,  use structured pruning in order to reduce the rank. Related to our approach, Kim and Awadalla (2020) and McCarley (2019) both apply structured pruning on the heads of the multi-head attention (MHA) and on the inner-layer nodes of the feed-forward network (FFN). The former uses predefined pruning ratios, shared across all layers, in order to select the modules to prune after sorting them given an importance score. McCarley (2019) compares dif-ferent methods to compute the prunable module masks and find L0 regularization to perform the best.", "n_publication_ref": 22, "n_figure_ref": 0}, {"heading": "Background", "text": "Starting with a transformer model with parameters \u03b8, our goal is to produce a set of parameters \u03b8 that are both fine-tuned for a specific end-task and smaller in such a way that inference can be efficiently computed on parallel hardware.\nThe two largest lines in the transformer parameter budget are the feed-forward network sublayer (FFN) and the multi-head attention sub-layer (MHA). The FFN parameters consist of two matrices (W 1 and W 2 ) of transposed shape R d model \u00d7d ff and R d ff \u00d7d model where d model is the hidden size and d ff d model is the inner size. These are used in the standard fashion by the network. The MHA parameters consist of 4 projection matrices (W q , W k , W v and W o ) of size R d model \u00d7d model (query, key, value, out). These are used to project the hidden vector to and from the component attention parts. In implementations, this projection is made with the matrices in their folded tensor form In standard fine-tuning, starting from \u03b8, we optimize the loss L (for instance, cross-entropy for classification):\narg min \u03b8 L(\u03b8 )\nScore-based pruning methods (Ramanujan et al., 2019) modify the model by introducing score parameters S for each parameter i and replace the original parameter matrices with a masked version W = W M (S). For instance, in the simplest version of magnitude pruning, the mask would just zero-out parameters with low absolute values.\nMovement pruning (Sanh et al., 2020) is a scorebased pruning approach that encourages the model to optimize these score parameters. Specifically, we focus on the soft-movement variant of movement pruning that sets M (S) = 1(S > \u03c4 ) for a threshold parameter \u03c4 , and optimizes a regularized objective,\narg min \u03b8 ,S L(\u03b8 ) + \u03bb \u03c3(S)\nwhere \u03bb is a hyper-parameter, A = i,j A i,j and \u03c3 is the sigmoid function.\nThis pruning objective encourages the model to fine-tune the parameters while lowering the scores of unimportant parameters and thus encouraging more sparsity. In order to train through the threshold, a straight-through estimator (Bengio et al., 2013) is used.\nMovement pruning, combined with distillation, has shown to be a very effective method to reduce the number of parameters in an existing model yielding 94% pruning in our tests for a F1 of 87.5 on SQuAD v1.1 (BERT-base is 88.5). This results in significantly smaller models than distillation alone. However, even with this sparsity level, the model is not substantially faster when run on most standard hardware that cannot significantly take advantage of this style of sparse matrix-vector product.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Model: Block Movement Pruning", "text": "In this work, we extend movement pruning to work on blocks of local parameters. Specifically, each matrix in the transformer is partitioned into fixedsized blocks. This setting goes beyond the arbitrary pruning of unstructured methods, with the goal of encouraging the data locality closer to what would be needed for efficiency. 2 Our approach is extremely simple. For each parameter matrix W \u2208 R M \u00d7N , we assume a fixedsized block structure (M , N ). Each of these blocks acts as an individual group in the regularization with a shared score parameter derived from the corresponding score matrix S \u2208 R M/M \u00d7N/N . Computing the masked weight is done by expanding the thresholded values, i.e.\nW i,j = W i,j * M (S) i/M , j/N\nAs in past work, this model is trained with distillation to match the performance of a teacher model.\nUnlike other distillation approaches that require fully specifying the new model structure, our method only requires the size and shapes of the blocks, i.e. the set of (M , N ) for each parameter matrix in the model. If blocks are too large, then they are difficult to prune, but if they are too small they do not support efficient inference.\nTo reduce the search space, we will limit ourselves to test (M , N ) att and (M , N ) ff : the same block size will be used for all layers for attention weights W q , W k , W v and W o on one hand, and for the feed-forward weights W 1 and W 2 on the other hand. We split the movement pruning regularization term into:\n\u03bb att \u03c3(S att ) + \u03bb ffn \u03c3(S ffn )\nThis allows us to take into account the difference in terms of gradient received by the score parameters.\nTo reduce further the search space, we will test on two kinds of blocks:\n\u2022 (32, 32) : square blocks (Block)\n\u2022 (1, d model ) and (d model , 1) : dimension pruning on paired FFN rows and columns (Dim)\nThese block sizes allow for efficient models: blocks of size at least (16, 16) are efficient to compute with appropriate GPU kernels, whereas full rows, columns or heads can be entirely removed from the matrix: the remaining matrix is then dense.\nWe also include two additional baseline block types used to verify the approach:\n\u2022 (2 n , 2 n ), n \u2208 [2, 5] : smaller power of two square block sizes to study the impact of size on performance (Block)\n\u2022 ( d model n heads , d model ) : for attention heads (Heads)\nThe first considers small blocks, and the second considers very large functional blocks.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "We conduct experiments on five (English) tasks commonly used to evaluate pre-trained language models: question answering (SQuAD v1.1 Rajpurkar et al., 2016) and (SQuAD v2 Rajpurkar et al., 2018), natural language inference (MNLI Williams et al., 2018), sentence similarity (QQP Chen et al., 2018), sentiment classification (SST-2 Socher et al., 2013) and abstractive summarization (CNN/DailyMail Hermann et al., 2015). These datasets respectively contain 87k, 130k, 392k, 363k, 67k and 287k training examples, and are downloaded from the Hugging Face datasets hub. SQuAD is formulated as a span-extraction task, MNLI and QQP are sentence pairs classification tasks, SST-2 is a sentence classification task and CNN/DailyMail (\"CNN\") is formulated as a conditional generation task. We report the performance on the development set as measured by the accuracy for MNLI and SST-2, F1 for QQP, the exact match (EM) and F1 for SQuAD and ROUGE for CNN/DailyMail.\nWe experiment with task-specific pruning of transformer language models. We use BERT (Devlin et al., 2019)  We compare against several baselines. Movement pruning is a fully unstructured approach and gives an upper bound on the sparsity trade-offs we hope to achieve, even if it provides little speed benefit. We also compare our results against state-ofthe-art approaches developed for fast inference of transformer-based language models. DistilBERT (Sanh et al., 2019) is obtained by distilling through pre-training a pre-trained BERT into a smaller model. TinyBERT (Jiao et al., 2019) distills a finetuned model while using data augmentation. Mo-bileBERT (Sun et al., 2020) is the result of a large architecture search. dBART (Shleifer and Rush, 2020) is obtained by arbitrarily copying equally spaced layers of a large model to a smaller one. To measure inference speed on GPU, we use a 24GB 3090 RTX and an Intel i7 CPU, using a large batch size (128) for evaluation and using PyTorch CUDA timing primitives. We measure the speed of other models in this same setup. Results may be different from original papers, as latency and throughput characteristics are different for each platform. We also provide the number of parameters in the linear layers of the Transformer layers for each of our models and for the reference ones: as the linear layers represent most of the FLOPS, this is a good proxy for the computation required and to some extent for the compute time, when the model characteristics are equivalent.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Resources and Reproducibility", "text": "We are using a minimal set of hyperparameters. The ratio of \u03bb att and \u03bb ffn is fixed by the relative sizes. We performed a few experiments with differ-  ent values fixed manually for these parameters, but their influence is minor.\nThe main hyperparameter is the number of training epochs. For SQuAD v1.1, we are using 20 epochs instead of typically 2 for BERT models. This means a fine-tuning is taking about 12h with our method instead of 45mn with a standard finetuning setup. This number has to be large enough to let pruning happen slowly enough for a given task. A warming up phase and a post-pruning cooldown phase are helpful, but their exact length has not a large impact on final performance. We believe the training time is less important than the inference time for energy consideration, as inference is performed repeatedly. Our method is optimizing inference by a large factor: the training energy is potentially recouped by a large margin with inference savings.\nFinally, the checkpoints created during the experiments are available on an AWS S3 bucket, with their metadata and training parameters, totaling 3TB of data, to facilitate reproduction of our results and to make it possible to study further the behavior of those models. Code for experiments, analysis, and tools to prepare the present paper are available on GitHub (see Appendix A).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Pruning Methods", "text": "The pruning approaches are shown in Table 1.\nBlock pruning use square block sizes throughout all the linear layers, as an extension of the original movement pruning for which the block size is 1.\nHybrid pruning jointly removes hidden dimensions in feed-forward layers W 1 and W 2 , using movement pruning to create the dimension mask. This corresponds to full rows or columns in the parameter matrices. The pruned W 1 and W 2 can then be \"compacted\" to become fully dense: we perform dense operations on cropped matrices. For the attention layers, pruning only some rows or columns in W q , W k , W v and W o can not be practically exploited. This is because the structure of the computation makes the additional cost of resizing the tensor inefficient. We, therefore, use square block pruning on the attention layer, with a block size of (32, 32) which showed the best tradeoff between performance and accuracy.\nStruct pruning uses the same methods for FFN layers but aims to remove model attention heads directly. To do so, we choose a block size on attention that equals the head size while still using the same soft movement pruning strategy. For this approach, we use a \u03bb att equals to 1/32, as there are 32 times more parameters than in an attention block than in a feed-forward dimension.\nWhen Block Pruning does not fully remove a component such as an attention head, as shown in Figure 1, we cannot speed up the model. But we can reclaim some of the performance at no speed cost and at marginal cost on sparsity by making use of those zero weights.\nHybrid Filled pruning allows the model to reinitialize these reclaimed weights uniformly at random and continue fine-tuning the smaller model for a few steps. We also explore \"rewinding\" (Frankle and Carbin, 2018) by identifying weights that should not be pruned (because they are part of a non-empty attention head) and re-fine-pruning the pre-trained model: the first run marks the attention heads that were not pruned, and the second uses this information to create a positive mask of weights that are protected from pruning. We did not find a significant difference between the two methods. The results presented here do not use rewinding.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Experiments", "text": "Main Results We begin by observing the highlevel impact of the different pruning methods. Figure 1 shows the effect on attention and feedforward layers for the different block pruning methods. We find that all the different block sizes learn to prune out entire dimensions in the FFN layers. Interestingly we find that the block methods can also learn to remove entire heads from the MHA. This pruning pattern makes it possible to remove entire heads from the model during inference. For this reason, we focus on the Hybrid approach as our main method, which can both eliminate feed- forward dimensions while using blocks to remove attention heads gradually.\nResults on SQuAD are shown in Figure 2, which compares our approach for speed and density to baseline BERT-Base tuned models such as TinyBERT-6 and DistilBERT (MobileBERT is discussed below). The main result is that the Hybrid Pruning model is as fast as the baseline and approaches the same accuracy while at the same time producing significantly smaller models in terms of density. Moving to the Hybrid Filled model leads to a further gain in speed at a small cost in model density. For instance, for the same F1 performance of 87.5, Hybrid Filled models display a 2.5x speedup against 1.88 for TinyBERT. TinyBERT and Distil-BERT have 50% of BERT's encoder parameters, whereas Hybrid Filled models have 25% BERT parameters for the same level of accuracy.\nThe figures also include two intrinsic baselines: our reimplementation of Movement pruning and pure Block pruning. We find that our implementation of Movement pruning is highly effective at producing sparse models (even leading to a small increase in accuracy) but does not produce significant speedups. Square Block pruning does better, but not as well as hybrid blocks.\nTable 2 gives a full comparison of models with different compression rates. As linear layers represent a very large part of the flops of a transformer model, this compression rate is actually a good measure of the maximum achievable speedup. This number is much higher than the actually measured speedup. This indicates that our setup for measur-    BERT performance on other tasks.\nComparison with MobileBERT All methods can be improved further using a larger teacher model. For these experiments, we compare with MobileBERT, which uses a BERT-large teacher and reaches an F1 of 90.0 on SQuAD v1.1 on its fastest version. It should be noted that Mo-bileBERT makes use of additional optimizations not present in the original BERT-large we are using: LayerNorms are replaced by purely linear NoNorms, and GeLUs are replaced by ReLUs. For these experiments, we use a BERT-large teacher to perform meaningful comparisons, using our best method Hybrid Filled.\nFigure 2 shows that we have comparable results on SQuAD v1.1, with a simpler optimization approach: we get a slightly better model (F1=90.3) for the same speedup of 1.6x, and we get a speedup of 2.2x at BERT-base accuracy (F1=88.5). We observe that using a large teacher is beneficial even at high levels of pruning: up to 80% of sparsity, the resulting student model has better accuracy for the same number of parameters when using a BERTlarge teacher instead of a base one. This trend reverses after this point: a larger teacher is detrimental to accuracy when the student is very heavily pruned.\nEncoder-Decoder Finally, we apply these methods to two encoder-decoder architectures, BARTbase and BART-large for the task of summarization. For these architectures, the decoder parameters are responsible for a majority of the computational costs, so these are our main focus. Voita et al. (2019) observed that for machine translation models, encoder heads were much easier to prune than decoder ones. We found similar results, e.g. for identical \u03bb att and \u03bb ffn , the encoder was systematically more pruned than the decoder, for both MHA and FFN sub-layers. In order to increase speedup gain, we applied twice as much weight on the decoder compression, which resulted in even pruning ratios among the encoder and decoder.\nTable 4 shows the main results. We see that Hybrid pruning leads to large decoder compression ratios (3.4 on BART-base and 3.5 BART-large) with only a small drop in ROUGE score. Speedups reach 1.4 times of the original speed. (Given the large decoder compression rates, we would expect larger speedups to be possible with further engineering of the inference.)\nThere is less comparable work for pre-trained encoder-decoders. We compare our approach with a distillation-based approach dBART (Shleifer and Rush, 2020). This approach yields a similar speedup gain with a smaller drop in performance but less sparsity. For models of comparable sizes (158M for our Hybrid NT vs 176M for dBART-6-6), we observe a drop of 0.7 in R2 and 0.4 in RL against 0.9 in R2 and 1.3 in RL for dBART-6-6. As with encoder-only models, the two approaches could likely be combined to yield even faster, smaller models. 3", "n_publication_ref": 2, "n_figure_ref": 3}, {"heading": "Analysis", "text": "Large Model Pruning To test that this approach scales to large models, we apply Hybrid pruning on BERT-large on SQuAD v1.1. We observe similar results: a 18% dense BERT-large has a F1 of 90.2, with a speedup of 3.2x compared to BERT-large with a F1 of 93.2. This pruned model is actually   faster than a BERT-base model (Table 5). We can compare Hybrid Pruning of SQuAD v2 BERT-large models with the results of the structured pruning method described in McCarley (2019). For a 17% dense model, we obtain a F1 of 82.6, whereas structured pruning gets a 25% dense model with a F1 of 81.5. This result is in line with Li et al. (2020): the larger the model, the more pruning is effective. When pruning a larger model, the final model is actually better than a smaller one with the same absolute number of parameters.\nBlock Size Influence Figure 3 shows the impact of different block sizes on Block pruning: pruning is done on attention layers and FFNs with the same square block size, from (4, 4) to (32, 32), with a BERT-base teacher. We can see that we reach the BERT-base original F1 for all block sizes from 4 to 32, but with a speedup that increases with the block size. The maximum reachable speedup without F1 drop is 1.3 for a block size of 32. But when some drop of F1 is allowed, the speedup increases quickly with the block size and plateau when reaching 16. We then reach a speedup of 1.75 for an F1 drop of 2% and a block size of 32. We also note that, with the original Movement Pruning method, we see some speedup due to full dimension pruning. This likely comes from our improved set of hyper-parameters (compared to the original paper), allowing us to remove some empty rows and columns in the FFN layers. However we see that using blocks leads to a significant speed improvement compared to Movement Pruning.\nQuantization Quantization is often of critical importance for practical applications. We, therefore, wanted to check that our networks could be subjected to quantization without significant loss of accuracy, especially when considering the issues that could arise with the high level of sparsity of some FFNs. Table 6 shows the results of full 8-bit quantization tests on our models. These indicate that the method is compatible with quantization, and the models using quantization on top of our pruning method achieve very high gains in terms of size (as well as speed).", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Impact of Distillation", "text": "We report experimental results with the addition of a teacher distillation step as previous work showed this boosts movement pruning at little cost. In this section, we conduct an ablation study to evaluate the impact of distillation using a BERT-base teacher.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model", "text": "Compress EM F1   As shown in Table 7, combining hybrid pruning with distillation always performs better than pruning alone, but that it is not critical for the approach to work. The distillation effect is larger for smaller datasets such as SST-2, which are prone to over-fitting. We believe that the regularization brought by pruning and distillation counters overfitting caused by the additional number of steps needed for pruning.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We have shown that we can extract small pruned models that are at an equivalent or better than distilled networks. This approach can be done during fine-tuning and not pre-training. The method does not resort to techniques such as data augmentation or architecture search, and it works on a diverse set of tasks and base models. As better and larger models are published at an increasing pace, we can rely on a simple and robust method to accelerate them on specific tasks without sacrificing accuracy and distribute these models easily while keeping most of the original model accuracy.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Impact", "text": "We expect the method presented here to contribute to the reduction of the compute resources and energy needed to perform natural language tasks, while preserving the original model performance. It will contribute additionally to alleviating privacy concerns: smaller models running on user devices instead of server-side allow more information to stay private. This is especially relevant when considering the large anticipated demand for such NLP applications in the near future.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Reproducibility & Hyper-Parameters", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Code", "text": "The complete code to run the experiments, analyze the results and finally create the figures and tables in this paper is available on the Hugging Face nn_pruning repository, at https://github.com/huggingface/nn_pruning.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Hyperparameters", "text": "The hyperparameters of the experiments are available as JSON files (one file per task) in the same repository: each entry contains all the information to fine-tune and prune the model, its evaluation results, and detailed statistics about its final sparsity.\nFor example, the SQuAD V1 checkpoints referenced in this paper are listed with the hyperparameters and related information.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Checkpoints", "text": "Some of the models we produced during this research can be used directly from the Hugging Face model hub.\nThe other models and the checkpoints, including the intermediary ones that were saved during training, are available on Amazon S3.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B Additional Data Block Shape & Head pruning", "text": "We show here the effect of the pattern on the head number reduction: using block instead of row/column pruning leads to a much larger number of pruned heads while improving accuracy, here on the SST-2 task.\nWe are using Block Movement pruning for each model, with different block patterns, pruning only the attention layers. Compression measures the reduction of the number of non-zero parameters in attention linear layers, whereas head compression measures the reduction of the number of complete non-zero heads.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Pruning Methods Comparison", "text": "We select speed as our main metric to compare with other techniques, as it is the major practical measure of inference efficiency. On this metric, we decided to compare our models to the best models available i.e. the distilled models (MobileBERT, TinyBERT), even though the method is different, as they are the strongest \"speed/accuracy\" baseline available.\nIn Table 9 we compare  with TinyBERT (Jiao et al., 2019) and MobileBERT (Sun et al., 2020  We compare as well to Hybrid pruning, with and without a teacher, with the unstructured methods from Sanh et al. (2020) (the original Movement Pruning method we are using) and Gordon et al. (2020) ", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "", "text": "The authors would like to thank the anonymous reviewers, the Hugging Face team for the support, Nvidia for providing us some hardware for evaluation, and finally the open-source community for the numerous tools which made this research possible.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "journal": "CoRR", "year": "2013", "authors": "Yoshua Bengio; Nicholas L\u00e9onard; Aaron C Courville"}, {"title": "Quora question pairs", "journal": "", "year": "2018", "authors": "Zihan Chen; Hongbo Zhang; Xiaoji Zhang; Leqi Zhao"}, {"title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019-06-02", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Reducing transformer depth on demand with structured dropout", "journal": "", "year": "2020-04-26", "authors": "Angela Fan; Edouard Grave; Armand Joulin"}, {"title": "The lottery ticket hypothesis: Training pruned neural networks", "journal": "CoRR", "year": "2018", "authors": "Jonathan Frankle; Michael Carbin"}, {"title": "Compressing BERT: studying the effects of weight pruning on transfer learning", "journal": "", "year": "2002", "authors": "Mitchell A Gordon"}, {"title": "Parameter-efficient transfer learning with diff pruning", "journal": "CoRR", "year": "2020", "authors": "Demi Guo; Alexander M Rush; Yoon Kim"}, {"title": "Learning both weights and connections for efficient neural networks", "journal": "CoRR", "year": "2015", "authors": "Song Han; Jeff Pool; John Tran; William J Dally"}, {"title": "Teaching machines to read and comprehend", "journal": "", "year": "2015-12-07", "authors": "Karl Moritz Hermann; Tom\u00e1s Kocisk\u00fd; Edward Grefenstette; Lasse Espeholt; Will Kay; Mustafa Suleyman; Phil Blunsom"}, {"title": "Distilling the knowledge in a neural network", "journal": "CoRR", "year": "2015", "authors": "Geoffrey E Hinton; Oriol Vinyals; Jeffrey Dean"}, {"title": "Tinybert: Distilling BERT for natural language understanding", "journal": "", "year": "1909", "authors": "Xiaoqi Jiao; Yichun Yin; Lifeng Shang; Xin Jiang; Xiao Chen; Linlin Li; Fang Wang; Qun Liu"}, {"title": "Fasttext.zip: Compressing text classification models", "journal": "CoRR", "year": "2016", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Matthijs Douze; Herv\u00e9 J\u00e9gou; Tom\u00e1s Mikolov"}, {"title": "Fastformers: Highly efficient transformer models for natural language understanding", "journal": "", "year": "2020", "authors": "Jin Young; Hany Hassan Kim;  Awadalla"}, {"title": "Optimal brain damage", "journal": "Morgan Kaufmann", "year": "1989-11-27", "authors": "Yann Lecun; John S Denker; Sara A Solla"}, {"title": "BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "Association for Computational Linguistics", "year": "2020-07-05", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"title": "Train large, then compress: Rethinking model size for efficient training and inference of transformers", "journal": "", "year": "2002", "authors": "Zhuohan Li; Eric Wallace; Sheng Shen; Kevin Lin; Kurt Keutzer; Dan Klein; Joseph E Gonzalez"}, {"title": "Learning sparse neural networks through l_0 regularization", "journal": "", "year": "2018-04-30", "authors": "Christos Louizos; Max Welling; Diederik P Kingma"}, {"title": "Ladabert: Lightweight adaptation of BERT through hybrid model compression", "journal": "", "year": "2004", "authors": "Yihuan Mao; Yujing Wang; Chufan Wu; Chen Zhang; Yang Wang; Yaming Yang; Quanlu Zhang; Yunhai Tong; Jing Bai"}, {"title": "Structured pruning of a bert-based question answering model", "journal": "", "year": "2019", "authors": "J S Mccarley"}, {"title": "Are sixteen heads really better than one? CoRR, abs", "journal": "", "year": "1905", "authors": "Paul Michel; Omer Levy; Graham Neubig"}, {"title": "Autosizing neural networks: With applications to ngram language models", "journal": "", "year": "2015-09-17", "authors": "Kenton Murray; David Chiang"}, {"title": "Know what you don't know: Unanswerable questions for squad", "journal": "Association for Computational Linguistics", "year": "2018-07-15", "authors": "Pranav Rajpurkar; Robin Jia; Percy Liang"}, {"title": "Squad: 100, 000+ questions for machine comprehension of text", "journal": "The Association for Computational Linguistics", "year": "2016-11-01", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"title": "What's hidden in a randomly weighted neural network? CoRR, abs", "journal": "", "year": "1911", "authors": "Mitchell Vivek Ramanujan; Aniruddha Wortsman; Ali Kembhavi; Mohammad Farhadi;  Rastegari"}, {"title": "Turing-nlg: A 17-billionparameter language model by microsoft", "journal": "Microsoft Research Blog", "year": "2020", "authors": "Corby Rosset"}, {"title": "Poor man's BERT: smaller and faster transformer models. CoRR, abs", "journal": "", "year": "2004", "authors": "Hassan Sajjad; Fahim Dalvi; Nadir Durrani; Preslav Nakov"}, {"title": "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter", "journal": "CoRR", "year": "2019", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"title": "Movement pruning: Adaptive sparsity by finetuning. CoRR, abs", "journal": "", "year": "2005", "authors": "Victor Sanh; Thomas Wolf; Alexander M Rush"}, {"title": "Compression of neural machine translation models via pruning", "journal": "ACL", "year": "2016-08-11", "authors": "Abigail See; Minh-Thang Luong; Christopher D Manning"}, {"title": "Pre-trained summarization distillation. CoRR, abs", "journal": "", "year": "2010", "authors": "Sam Shleifer; Alexander M Rush"}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "journal": "ACL", "year": "2013-10", "authors": "Richard Socher; Alex Perelygin; Jean Wu; Jason Chuang; Christopher D Manning; Andrew Y Ng; Christopher Potts"}, {"title": "Patient knowledge distillation for BERT model compression", "journal": "", "year": "2019-11-03", "authors": "Siqi Sun; Yu Cheng; Zhe Gan; Jingjing Liu"}, {"title": "Mobilebert: a compact task-agnostic BERT for resource-limited devices", "journal": "", "year": "2004", "authors": "Zhiqing Sun; Hongkun Yu; Xiaodan Song; Renjie Liu; Yiming Yang; Denny Zhou"}, {"title": "Distilling taskspecific knowledge from BERT into simple neural networks. CoRR, abs", "journal": "", "year": "1903", "authors": "Raphael Tang; Yao Lu; Linqing Liu; Lili Mou; Olga Vechtomova; Jimmy Lin"}, {"title": "Well-read students learn better: The impact of student initialization on knowledge distillation", "journal": "", "year": "2019", "authors": "Iulia Turc; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned", "journal": "", "year": "2019", "authors": "Elena Voita; David Talbot; Fedor Moiseev; Rico Sennrich; Ivan Titov"}, {"title": "Structured pruning of large language models", "journal": "", "year": "2019", "authors": "Ziheng Wang; Jeremy Wohlwend; Tao Lei"}, {"title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "Long Papers", "year": "2018-06-01", "authors": "Adina Williams; Nikita Nangia; Samuel R "}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "model where n heads is the number of attention heads.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "(an encoder-only Transformer language model with 110M parameters, among which 85M are part of the linear layers present in the Transformer layers) for sentence classification and question answering (340M and 227M respectively for BERT-large), and BART (Lewis et al., 2020) (an encoder-decoder language model with 139M parameters, among which 99M are part of the linear layers present in the Transformer layers) for summarization (406M and 353M for BART-large).", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure 1: Pruning patterns on SQuAD v1.1: blue is preserved, pink is pruned. Attention heads are delimited for clarity.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure2: Comparison on SQuAD v1.1 of model F1 against speed and density (BERT-base reference). For each pruning method the pruned model is BERT-base, but different regularization values give different final sparsity levels. This translates into a tradeoff curve between accuracy and speedup specific to the method. Distilled networks (Mobile|Tiny|Distil)BERT are given as references. The higher the curve, the most accurate the model is for a given speed.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: SQuAD v1.1 with Block Pruning: Influence of block size on F1.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Summary of pruning methods. Dim blocks correspond to row and column blocks for the FFN.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "SQuAD v1.1 pruned models with base/large teacher: Encoder Linear Parameters Count, Compression rate and Speed gain relative to BERT-base, F1 and Exact Match. LT stands for Large Teacher.", "figure_data": "ing speed may underestimate the actual speedupone could obtain with those pruned models withspecialized implementations. Hybrid Filled reachesa 2.25x speedup under minimal loss in accuracy.Struct pruning targeting MHA blocks directly canbe even faster but leads to a stronger degradationin accuracy.Table 3 shows the comparison between Tiny-BERT and a Hybrid pruned model of the samespeed on several others tasks. Hybrid Pruning per-forms better on SQuAD v1.1, and approaches Tiny-"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": BART pruned models fine-tuned on CNN:encoder-decoder linear parameters count, Decodercompression rate (DCp) and Speed gain (one forwardprediction) relative to BART-large/base, dev ROUGEscores.  \u2020 denotes test ROUGE scores taken fromShleifer and Rush (2020). NT stands for No Teacher.ModelSizeCompr. Speed F1EMSQuAD v1.1BERT-large227M 1.001.093.2 86.9BERT-base85M2.663.188.5 80.8Hybrid54M4.162.991.0 84.6Hybrid41M5.593.290.2 83.7SQuAD v2BERT-large227M 1.001.085.8 82.8StructuredQA 57M4.00-81.5 -Hybrid38M5.88-82.6 79.7"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "BERT-large SQuAD pruned models. Reference speed is BERT-large.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Results of pruned BERT-base on SQuAD v1.1 dev Exact Match and F1 score with 8-bit quantization.", "figure_data": "Dataset Size Hybrid Hybrid NTQQP24M87.6187.1721M87.1487.0010M86.8286.27SST-270M93.2392.2042M91.9790.7118M90.6089.79"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ": Distillation ablation study of BERT-base onQQP and SST-2 dev of a BERT-base teacher. F1 and ac-curacy scores reported for QQP and SST-2 respectively.NT stands for No Teacher."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Head pruning method comparison on SST-2", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": ").", "figure_data": "MethodSpeed SST2 MRPC STS-B QNLIWang et al.<1.5x 92.09 88.6188.1889.05TinyBERT2x93.187.383.790.4MobileBERT 4x92.888.884.490.6"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_15", "figure_caption": ", and with Sajjad et al. (2020) (dropping full layers), in Table10.", "figure_data": "ModelSpd Cp MNLIQQPSST-2(m/mm)F1/AccAccBERT base 1184.5/85.1 88.1/91.1 92.7Mvmt NT\u223c 1 1080.7/81.1 87.1/90.5 -Hybrid NT 3.51079.4/79.9 86.0/89.3 87.0Mvmt\u223c 1 1081.2/81.8 86.8/90.2 -Hybrid3.51080.4/81.1 86.4/89.8 89.7Hybrid NT 34.5 81.7/81.8 87.0/90.3 89.8Hybrid34.5 82.7/82.8 87.4/90.6 90.6Sajjad< 2281.1/ --/90.490.3Gordon-282.6/--/90.390.8Hybrid NT 1.6283.2/83.3 87.2/90.4 90.7Hybrid1.6283.7/84.1 88.3/91.3 92.0"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Pruning Methods Comparison. Mvmt is Movement Pruning Sanh et al. (2020), Sajjad is Sajjad et al. (2020), Gordon is Gordon et al. (2020). NT is for No Teacher. Spd is speed multiplier, Cp is for parameters compression rate.", "figure_data": ""}], "doi": "10.18653/v1/n19-1423"}