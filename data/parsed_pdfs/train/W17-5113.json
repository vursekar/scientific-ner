{"authors": "Alfio Ferrara; Stefano Montanelli; Georgios Petasis", "pub_date": "", "title": "Unsupervised Detection of Argumentative Units though Topic Modeling Techniques", "abstract": "In this paper we present a new unsupervised approach, \"Attraction to Topics\" -A2T , for the detection of argumentative units, a sub-task of argument mining. Motivated by the importance of topic identification in manual annotation, we examine whether topic modeling can be used for performing unsupervised detection of argumentative sentences, and to what extend topic modeling can be used to classify sentences as claims and premises. Preliminary evaluation results suggest that topic information can be successfully used for the detection of argumentative sentences, at least for corpora used in the evaluation. Our approach has been evaluated on two English corpora, the first of which contains 90 persuasive essays, while the second is a collection of 340 documents from user generated content.", "sections": [{"heading": "Introduction", "text": "Argument mining involves the automatic discovery of argument components (i.e. claims, premises) and the argumentative relations (i.e. supports, attacks) among these components in texts. Primarily aiming to extract arguments from texts in order to provide structured data for computational models of argument and reasoning engines (Lippi and Torroni, 2015a), argument mining has additionally the potential to support applications in various research fields, such as opinion mining (Goudas et al., 2015), stance detection (Hasan and Ng, 2014), policy modelling (Florou et al., 2013;Goudas et al., 2014), legal information systems (Palau and Moens, 2009), etc.\nArgument mining is usually addressed as a pipeline of several sub-tasks. Typically the first sub-task is the separation between argumentative and non-argumentative text units, which can be performed at various granularity levels, from clauses to several sentences, usually depending on corpora characteristics. Detection of argumentative units (AU) 1 , as discussed in Section 2, is typically modeled as a fully-supervised classification task, either a binary one, where units are separated in argumentative and non-argumentative ones with argumentative ones to be subsequently classified in claims and premises as a second step, or as a multi-class one, where identification of argumentative units and classification into claims and premises are performed as a single step. According to a recent survey (Lippi and Torroni, 2015a), the performance of proposed approaches depends on highly engineered and sophisticated, manually constructed, features.\nHowever, fully-supervised approaches rely on manually annotated datasets, the construction of which is a laborious, costly, and error-prone process, requiring significant effort from human experts. At the same time, reliance on sophisticated features may hinder the generalisation of an approach to new corpora types and domains (Lippi and Torroni, 2015a). The removal of manual supervision through exploitation of unsupervised approaches is a possible solution to both of the aforementioned problems.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Motivations of our work", "text": "Topics seem to be related to the task of argument mining, at least for some types of corpora, as topic identification frequently appears as a step in the process of manual annotation of arguments in texts (Stab and Gurevych, 2014a). However, despite its apparent importance in manual annotation, only a small number of studies have examined the inclusion of topic information in sub-tasks of argument mining. Habernal and Gurevych (2015) have included sentiment and topic information as features for classifying sentences as claims, premises, backing and non-argumentative units. A less direct exploitation of topic information has been presented in (Nguyen and Litman, 2015), where topics have been used to extract lexicons of argument and domain words, which can provide evidence regarding the existence of argument components.\nIn this paper we propose \"Attraction to Topics\" -A2T , an unsupervised approach based on topic modeling techniques for detecting argumentative discourse units at sentence-level granularity (a sub-task known as \"argumentative sentence detection\"). The goals of A2T are twofold. On the one side, A2T enforces identification of sentences that contain argument components, by also distinguishing them from the non-argumentative sentences that do not contain argument components. On the other side, A2T classifies the discovered argumentative sentences according to their role, as major claims, claims, and premises.\nThe rest of the paper is organized as follows: Section 2 presents an overview of approaches related to argument mining focusing on the detection of argumentative units, while Section 3 presents our approach on applying topic modeling for identifying sentences that contain argument components. Section 4 presents our experimental setting and evaluation results, with Section 5 concluding this paper and proposing some directions for further research.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Related work", "text": "Almost all argument mining frameworks proposed so far employ a pipeline of stages, each of which is addressing a sub-task of the argument mining problem (Lippi and Torroni, 2015a). The segmentation of text into argumentative units is typically the first sub-task encountered in such an argument mining pipeline, aiming to segment texts into argumentative and non-argumentative text units (i.e. segments that do contain or do not contain argument components, such as claims or premises). The granularity of argument components is text-dependant. For example, in Wikipedia articles studied in (Rinott et al., 2015), argument components spanned from less than a sentence to more than a paragraph, although 90% of the cases was up to 3 sentences, with 95% of components being comprised of whole sentences.\nSeveral approaches address the identification of argumentative units at the sentence level, a subtask known as \"argumentative sentence detection\", which typically models the task as a binary classification problem. Employing machine learning and a set of features representing sentences, the goal is to discard sentences that are not part (or do not contain a component) of an argument. As reported also by Lippi and Torroni (2015a), the vast majority of existing approaches employ \"classic, off-the-self\" classifiers, while most of the effort is devoted to highly engineered features. A plethora of learning algorithms have been applied on the task, including Naive Bayes (Moens et al., 2007;Park and Cardie, 2014), Support Vector Machines (SVM) (Mochales and Moens, 2011;Rooney et al., 2012;Park and Cardie, 2014;Stab and Gurevych, 2014b;Lippi and Torroni, 2015b), Maximum Entropy (Mochales and Moens, 2011), Logistic Regression (Goudas et al., 2014(Goudas et al., , 2015Levy et al., 2014), Decision Trees and Random Forests (Goudas et al., 2014(Goudas et al., , 2015Stab and Gurevych, 2014b).\nHowever, approaches addressing this task in a semi-supervised or unsupervised manner are still scarce. In (Petasis and Karkaletsis, 2016) an unsupervised approach is presented, which addresses the sub-task of identifying the main claim in a document by exploiting evidence from an extractive summarization algorithm, TextRank (Mihalcea and Tarau, 2004). In an attempt to study the overlap between graph-based approaches and approaches targeting extractive summarization with argument mining, evaluation results suggest a positive effect on the sub-task, achieving an accuracy of 50% on the corpus compiled by Hasan and Ng (2014) from online debate forums and on a corpus of persuasive essays (Stab and Gurevych, 2014a). Regarding semi-supervised approaches, Habernal and Gurevych (2015) propose new unsupervised features that exploit clustering of unlabeled argumentative data from debate portals based on word embeddings, outperforming several baselines. This work employs also topic modeling as one of its features, by including as features the distributions of sentences from LDA (Blei et al., 2003).\nTopic modeling has been mainly exploited for identification of argumentative relations and for extraction of argument and domain lexicons. In Lawrence et al. (2014), LDA is used to decide whether a proposition can be attached to its previous proposition in order to identify non directional relations among propositions detected through classifiers based on words and part-ofspeech tags. LDA has been also used to mine lexicons of argument (words that are topic independent) and domain words (Nguyen and Litman, 2015), by post-processing document topics generated by LDA. These lexicons have been used as features for supervised approaches for argument mining (Nguyen and Litman, 2016a,b). However, to the best of our knowledge, no prior approach has applied topic modeling to argumentative sentence detection in an unsupervised setting, which is the featuring aspect of the proposed A2T approach presented in the following.", "n_publication_ref": 26, "n_figure_ref": 0}, {"heading": "Topic modeling for argument mining", "text": "Given a document corpus, topic modeling techniques can be employed to discover the most representative topics throughout the corpus, and to provide an assignment of documents to topics, meaning that the higher is the assignment value of a document to a certain topic, the higher is the probability that the document is \"focused\" on that topic.\nThe idea of A2T is that an argumentative unit is a sentence highly focused on a specific topic, namely a sentence with high assignment value to a certain topic and low assignment value to the other topics. To this end, A2T introduces the notion of attraction with the aim at recognizing the sentences highly focused on specific topics, that represent the recognized argumentative units. In the following, the A2T approach and related techniques are described in detail.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A2T approach", "text": "The schema of the A2T approach is shown in Figure 1. Consider a corpus of texts C = {c 1 , . . . , c n }, where a text c i \u2208 C is a sequence of sentences, like for example an essay, a web page/post, or a scientific paper. The ultimate goal of the A2T approach is to derive a set of argumentative units U = { s 1 , c, l , . . . , s h , c, l }, where", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Corpus of Texts ( C )", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Argumentative", "text": "Units ( U )", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Sentence Extraction Attraction Evaluation Topic Modelling", "text": "Sentence Index ( S )\nFigure 1: Schema of the A2T approach s i is a sentence containing an argumentative unit, c is the text containing s, and l is the argumentative role expressed by the unit (e.g., major claim, claim, premise). The A2T approach is articulated in the following activities:\nSentence extraction. A2T approach is characterized by the use of topic modeling at sentencelevel granularity. For this reason, a pre-processing step of the corpus C is enforced based on conventional techniques for sentence tokenization, words tokenization, normalization, and indexing (Manning et al., 2008). The result is a sentence set\nS = { \u2212 \u2192 s 1 , c, pos 1 , . . . , \u2212 \u2192 s m , c\n, pos m }, where \u2212 \u2192 s i is the vector representation of the sentence s i and c, pos are text and position in the text where the sentence appears, respectively. The sentence set is stored in a sentence index for efficient access of S elements.\nTopic modeling. The set of extracted sentences S is used as the document corpus on which topic modeling is applied. The result of this activity is twofold. First, topic modeling returns a set of topics T = {t 0 , . . . , t k } representing the latent variables that are most representative for the sentences S. Second, topic modeling returns a distribution of sentences over topics \u03b8 = {\u03b8 s 1 , . . . , \u03b8 sm }. In particular, \u03b8 s i = [p(t 0 |s i ), . . . , p(t k |s i )] is the probability distribution of the sentence s i over the set of topics T , where p(t j |s i ) represents the probability of the topic t j given the sentence s i (i.e., the so-called assignment value of s i to t j ).\nAttraction evaluation. The notion of attraction is introduced to measure the degree of focus that characterizes sentences with respect to the emerged topics. To this end, the distribution of sentences over topics \u03b8 is exploited with the aim at determining the best topic assignment for each sentence of S. The result is an attraction set A = { s 1 , a 1 , . . . , s m , a m } where s i is a sentence of S and a i is its corresponding attraction value.\nSentence labeling. By exploiting the attraction set A, labeling has the goal to determine the sentences of S that are more focused on a specific topic, according to the hypothesis that those sentences are the argumentative units. In a basic scenario, labeling consists in distinguishing between sentences that are argumentative units (l = au) and sentences that are not argumentative units (l = au). In a more articulated scenario, labeling consists in assigning a role to sentences that are recognized as argumentative units. For instance, it is possible to distinguish argumentative-unit sentences that are claims (l = cl), major claims (l = mc), or premises (l = pr). A sentence s recognized as argumentative unit is inserted in the final set U with the assigned label and it is returned as a result of A2T .", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "A2T techniques", "text": "In A2T , the sentence extraction step is enforced by relying on standard techniques for representing documents in terms of feature vectors and bag of words (using tf-idf as weighting scheme) (Castano et al., 2017). Probabilistic topic modeling is exploited to enforce the subsequent topic modeling step. Probabilistic topic models are a suite of algorithms whose aim is to discover the hidden thematic structure in large archives of documents, namely sentences in A2T . The idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words (Blei et al., 2003). Probabilistic topic modeling algorithms infer the distribution \u03b8 of documents over topics and the distribution \u03c6 of words over topics, by sampling from the bag of words of each document. In our approach, we choose to exploit the Hierarchical Dirichlet Process (HDP). With respect to other algorithms (such as LDA), HDP has the advantage to provide the optimal number of topics instead of requiring to set such a number as input (Teh et al., 2006).\nAttraction evaluation. The notion of attraction is introduced in A2T to capture the intuition that argumentative units are related to the distribution of sentences over topics. Consider a set of sentences S and the distribution \u03b8 of sentences over the set of topics T . The more the distribution \u03b8 s i of a sentence s i over the topics is unequal, the more s i is focused on a topic, thus suggesting s i as a possible argumentative unit. A further feature that attraction aims to capture is that argumentative units often appear either at the beginning or at the end of texts. The attraction a i of a sentence s i is calculated as follows:\na i = K\u03d5 s i + (1 \u2212 K) \u03c1 s i s j \u2208c \u03c1 s j , \u03d5 s i = max(\u03b8 s i\n) is a measure of how much s i is focused on a topic and \u03c1 s i = \u03b1f (pos i ) 2 + \u03b2f (pos i ) + \u03b3 is a parabolic function over the position of the sentence in c. In particular, given L(c) as the number of sentences in c, f\n(pos i ) = L(c)\n2 \u2212 pos i such that f (pos i ) is higher when s i appears either at the beginning or at the end of c. The parameters \u03b1, \u03b2, \u03b3 determine the shape of \u03c1 s i . K \u2208 [0, 1] is a constant value used to balance the role of focus and position in calculating the attraction. The attraction a i can be interpreted as the probability of a sentence s i to contain an argumentative unit. According to this interpretation, given s i , also the contiguous sentences s i\u22121 and s i+1 have a chance to be argumentative units. As a result, given the calculated attraction set A, we update the attraction values a i through an interpolation mechanism based on the Savitzky-Golay smoothing filter (SGF) (Savitzky and Golay, 1964), so that A := SGF (A).\nIn Figure 2, an example of attraction evaluation is provided by showing the values of \u03d5, \u03c1, attraction, and interpolated attraction for all the sentences within one considered student essays included in the corpus from (Stab and Gurevych, 2014a) (see Section 4). Sentence labeling. Sentence labeling has the goal to turn attraction values into labeled categories. Consider a set of possible labels L = {l 1 , . . . , l g }, each one denoting a possible argumentative role that can be assigned to a sentence. Given a set of attraction values A, a thresholdbased mechanism is enforced to assign labels to sentences according to the following scheme:\na i < \u03c4 1 : s i \u2190 l 1 \u03c4 1 \u2264 a i < \u03c4 2 : s i \u2190 l 2 . . . . . . . . . a i \u2265 \u03c4 g\u22121 : s i \u2190 l g\nwhere \u03c4 1 < \u03c4 2 < ... < \u03c4 g\u22121 (\u03c4 1 , . . . \u03c4 g\u22121 \u2208 (0, 1]) are prefixed threshold values. The result of sentence labeling is a partition of S into g categories with associated labels.\nIn the experiments, we discuss two different strategies for sentence labeling. The first one is a two-class labeling strategy where the possible labels for a sentence are argumentative unit (au) and non-argumentative unit (au. The second strategy is a multi-class labeling in which the possible labels of a sentence are non-argumentative unit au, premise (pr), claim (cl), and major claim (mc).", "n_publication_ref": 5, "n_figure_ref": 1}, {"heading": "Experimental results", "text": "For evaluation of the proposed A2T approach, we have used two English corpora. The first corpus (C1 in the following) is a collection of 90 student persuasive essays (Stab and Gurevych, 2014a) which has been manually annotated with major claims (one per essay), claims and premises at the clause level. In addition, the corpus contains manual annotations of argumentative relations, where the claims and premises are linked, while claims are linked to the major claim either with a support or an attack relation. Interannotation agreement has been measured to unitized alpha (Krippendorff, 2004) \u03b1 U = 0.724. These 90 essays consist of a total of 1, 675 sentences (from which 19.3% contain no argument components), with an average length of 18.61 \u00b1 7 sentences per essay, while the 5.4% of sentences contain a major claim, 26.4% contain a claim, and 61.1% contain a premise.\nThe second corpus (C2 in the following) has been compiled and manually annotated as described in (Habernal and Gurevych, 2017). This corpus focuses on user generated content, including user comments, forum posts, blogs, and newspaper articles, covering several thematic domains from educational controversies, such as homeschooling, private vs. public schools, or singlesex education. Containing in total 340 documents, the corpus has been manually annotated with an argument scheme based on extended Toulmin's model, involving claims, premises, and backing, rebuttal, refutation argument units. The corpus contains documents of various sizes, with a mean size of 11.44 \u00b1 11.70 sentences per document, while the inter-annotator agreement was measured as \u03b1 U = 0.48. The corpus consists of 3,899 sentences, from which 2,214 sentences (57%) contain no argument components.\nBoth corpora have been preprocessed with NLTK (Loper and Bird, 2002) in order to identify tokens and sentences. Then, each sentence was annotated as argumentative or non-argumentative, depending on whether it contained an argument unit (i.e. a text fragment annotated as major claim, claim, or premise). In addition, each argumentative sentence was further annotated with one of major claim, claim, and premise, based on the type of the contained argumentative unit. For the second corpus, which utilizes a richer argument scheme, we have considered backing, rebuttal and refutation units as premises. This second corpus does not contain units annotated as major claims. The following three tasks have been executed:\n\u2022 Task 1: Argumentative sentence identification -given a sentence, classify whether or not it contains an argument component. \u2022 Task 2: Major claim identification -given a argumentative sentence, classify whether or not it contains a major claim. \u2022 Task 3: Argumentative sentence classification -given a sentence, classify the sentence as major claim, claim, premise, or nonargumentative.\nBaseline. As a baseline for comparison against our approach, we created a probabilistic classifier of sentences which evaluates the probability p(l = au|s i ) as follows. Given the text c containing L(c) sentences s i , let be \u03b6 c \u223c Dir(\u03b1) the probability distribution of the sentences in c, such that \u03b6 s i c \u223c p(l = au|s i ). The L(c) parameters \u03b1 used to generate \u03b6 c are defined such that \u03b1 i = L(c)\n2 \u2212 pos i . The rationale of this procedure is to bias the random assignment of a sentence to the au label in favor of sentences appearing either in the beginning or in the end of a text. This bias attempts to model empirical evi-dence that in several types of documents, the density of argumentative units in various sections of documents depends on the structure of documents. The beginning and end of a document are expected to contain argumentative units in structured documents like news, scientific publications, or argumentative essays (Stab and Gurevych, 2017), where major claims and supporting premises are frequently found in the beginning of documents, with documents frequently ending with repeating the major claims and supporting evidence.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Task 1: Argumentative sentence identification", "text": "The goal of Task 1 is to associate each sentence of the corpora to a label in L = {au, au} by following a two-class labeling strategy (see Section 3). As a first experiment, we performed sentence labeling with different threshold ranging from 0 to 1 with step 0.05. In Figure 3, we report the precision, recall, and F1-measure for A2T and for the baseline. In addition, we report also the results of applying sentence labeling based on \u03d5 and \u03c1 (the components of attraction) separately. The parameter K for attraction calculation has been set to 0.5. Since A2T is an unsupervised method, there is no easy way to define the threshold parameter \u03c4 , which has been empirically defined to \u03c4 = 0.3. The different behavior of A2T with respect to the baseline is shown in the confusion matrices reported in Figures 4 and 5.\nFrom Figure 3, we can see that A2T is significantly better than the baseline, especially for the C1 corpus. A characteristic of this corpus is that argumentative units are frequently located in the introduction or the conclusion of an essay, which is also reflected by the baseline that achieved an F1-measure of 0.35 for a threshold of \u03c4 = 0.05 (with the baseline being particularly precise, suggesting that argumentative units are very frequently at the beginning and end of essays). Both components of attraction (\u03d5 and \u03c1) perform well, with the topic component \u03d5 being slightly better than position information \u03c1, both in precision and recall. The results are similar for corpus C2, with A2T surpassing the baseline, although A2T advantage in precision is smaller. As shown in the confusion matrix of Figure 5, the main source of error is the large number of false positives for the au class, proposing more argumentative units than what have been manu-ally identified in corpus C2. This can be attributed to the sparseness of argumentative units in the C2 corpus, with almost 60% of the sentences being non-argumentative.", "n_publication_ref": 0, "n_figure_ref": 4}, {"heading": "Task 2: Major claim identification", "text": "As a second experiment, we exploited probabilities associated with sentences to perform a ranked evaluation. In particular, we calculated two measures, namely P that is the area the under the precision-recall curve and R that is the area under the receiver operating characteristic (ROC) curve.\nIn this experiments, we used different criteria for defining the true labels: in P CM , an annotated sentence in the corpus is considered a true argumentative unit if it is either a premise, a claim, or a major claim; in CM only claims and major claims are taken as valid au; in M only major claims are taken into account. Results are reported in Table 1. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Task 3: Argumentative sentence classification", "text": "The goal of Task 3 is to associate each sentence of the corpora to a label in L = {au, pr, cl, mc} by following a multi-class labeling strategy (see Section 3). In particular, we adopted the thresholds [0.1, 0.3, 0.5]. This task is challenging since it is required to distinguish the different role played in argumentation by sentences that are often very similar from the terminological point of view. The confusion matrix for corpus C1 is shown in Figure 6, while Figure 7 shows the confusion matrix for corpus C2. Both A2T and the baseline achieve low results, but the accuracy of A2T is 0.3 against the 0.1 of the baseline. From Figure 6 we see that A2T achieved good results for premises, and quite good results for claims, although distinguishing between claims and premises is challenging for the A2T approach. In particular, the role it may be useful to work also on semantic relations holding among sentences. This is actually one of the future tasks in our research work. Another specific challenge emerges when we consider the corpus C2. Indeed, C2 contains a limited number of argumentative sentences with respect to the corpus size. In this case, since we analyze all the sentences according to their bag of words, we tend to overestimate the number of argumentative units, collecting a relatively high number of false positives.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "Lessons learned from error analysis", "text": "A first evidence emerging from the analysis of confusion matrices for both corpora C1 and C2 is that the role of sentences is strictly dependent on the type of documents. C1 contains structured essays of various topics, while C2 provides conversational texts extracted from blogs and chats. In the first case, the number of argumentative units is higher than in the second one. In particular, for C2 we overestimated the probability of sentences to be an argumentative unit. This is mainly due to the fact that those sentences contain words that are semantically related to the main topic of the conversation although they are not playing a role in the argumentation. An example is the following sentence, taken from a document associated with the topic \"school\": \"why do some parents not think their kids can attain?\". The sentence is clearly part of a conversation and it has been annotated as a non argumentative unit because it is a question. However, since it contains words that are relevant for the topic (i.e., parents, kids, attain), A2T associates the sentence with a good level of attraction, labeling it as a premise. In order to address this kind of false positives, we aim in our future work to study the dependency relations among sentences in text (such as questionanswers) to the goal of achieving a better insight of the sentences role.\nA second lesson learned from error analysis concerns the distinction between claims and premises. This confusion is evident especially when dealing with corpus C1. An example is given by the following two sentences, taken from an essay about the role of sports in favor of peace.\n\u2022 (s1) for example, when Irak was hardly struck by the second gulf war, its citizens tried to catch any incoming news about the footballworld cup through their portable receivers.\n\u2022 (s2) thus, world sports events strongly participate in eventually pulling back people towards friendship and peace\nThe sentence (s1) has been annotated as a premise, while (s2) as a claim. In our classification, they are both claims. The reason is that they both contain topic-related words and their position in text is similar. The main distinction is the presence of the expression \"for example\" in the first sentence which qualifies it as a premise. To this end, in our future work we aim at adding some special words (such as \"for example\", \"therefore\") in the background knowledge of the classifier, in order to improve the capability of discriminating premises and claims.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Concluding remarks", "text": "In this paper, we present the \"Attraction to Topics\" -A2T unsupervised approach for detecting argumentative discourse units, at sentence-level granularity. Motivated by the observation that topic information is frequently employed as a sub-task in the process of manual annotation of arguments, we propose an approach that exploits topic modeling techniques in order to identify argumentative units. Since manual supervision is not required, A2T has the potential to be applicable on documents of various genres and domains. Preliminary evaluation results on two different corpora are promising. First, A2T performs significantly better than the baseline on argumentative sentence detection on both corpora. Second, A2T exhibits good results for classifying argumentative sentences as major claims, claims, premises, and non-argumentative units, at least for the first corpus, which has a low rate of non-argumentative sentences ( 20%).\nRegarding directions for further research, there are several axes that can be explored. Evaluation on a larger set of annotation corpora will provide enhanced insights about the performance of the proposed approach on different document types. Our preliminary results showed that despite good recall on multiple corpora, achieving also good precision can be a challenging task in documents where argumentative units are sparse, and false positives can be an issue. In this context, we would like to also exploit other types of relations, and extend our method with other kinds of similarities over sentences.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Latent dirichlet allocation", "journal": "J. Mach. Learn. Res", "year": "2003", "authors": "David M Blei; Andrew Y Ng; Michael I Jordan"}, {"title": "Exploratory analysis of textual data streams", "journal": "Future Generation Computer Systems", "year": "2017", "authors": "Silvana Castano; Alfio Ferrara; Stefano Montanelli"}, {"title": "Argument extraction for supporting public policy formulation", "journal": "", "year": "2013-08-08", "authors": "Eirini Florou; Stasinos Konstantopoulos; Antonis Koukourikos; Pythagoras Karampiperis"}, {"title": "Argument extraction from news, blogs, and social media", "journal": "Springer International Publishing", "year": "2014-05-15", "authors": "Theodosis Goudas; Christos Louizos"}, {"title": "Argument extraction from news, blogs, and the social web", "journal": "International Journal on Artificial Intelligence Tools", "year": "2015", "authors": "Theodosis Goudas; Christos Louizos"}, {"title": "Exploiting debate portals for semi-supervised argumentation mining in user-generated web discourse", "journal": "", "year": "2015", "authors": "Ivan Habernal; Iryna Gurevych"}, {"title": "Argumentation mining in user-generated web discourse", "journal": "Computational Linguistics", "year": "2017", "authors": "Ivan Habernal; Iryna Gurevych"}, {"title": "Why are you taking this stance? identifying and classifying reasons in ideological debates", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Saidul Kazi; Vincent Hasan;  Ng"}, {"title": "Measuring the reliability of qualitative text analysis data", "journal": "Quality and Quantity", "year": "2004", "authors": "Klaus Krippendorff"}, {"title": "Mining arguments from 19th century philosophical texts using topic based modelling", "journal": "", "year": "2014", "authors": "John Lawrence; Chris Reed; Colin Allen; Simon Mcalister; Andrew Ravenscroft"}, {"title": "Context dependent claim detection", "journal": "", "year": "2014-08-23", "authors": "Ran Levy; Yonatan Bilu; Daniel Hershcovich; Ehud Aharoni; Noam Slonim"}, {"title": "Sanjay Modgil, and Nir Oren, editors, Theory and Applications of Formal Argumentation: Third International Workshop, TAFA", "journal": "Springer International Publishing", "year": "2015-07-25", "authors": "Marco Lippi; Paolo Torroni"}, {"title": "Contextindependent claim detection for argument mining", "journal": "", "year": "2015", "authors": "Marco Lippi; Paolo Torroni"}, {"title": "Nltk: The natural language toolkit", "journal": "", "year": "2002", "authors": "Edward Loper; Steven Bird"}, {"title": "Introduction to information retrieval", "journal": "Cambridge university press Cambridge", "year": "2008", "authors": "D Christopher; Prabhakar Manning; Hinrich Raghavan;  Sch\u00fctze"}, {"title": "Textrank: Bringing order into texts", "journal": "", "year": "2004", "authors": "Rada Mihalcea; Paul Tarau"}, {"title": "Argumentation mining", "journal": "Artificial Intelligence and Law", "year": "2011", "authors": "Raquel Mochales; Marie-Francine Moens"}, {"title": "Automatic detection of arguments in legal texts", "journal": "ACM", "year": "2007", "authors": "Marie-Francine Moens; Erik Boiy"}, {"title": "Extracting argument and domain words for identifying argument components in texts", "journal": "Denver, Colorado, USA. The Association for Computational Linguistics", "year": "2015-06-04", "authors": "Huy Nguyen; Diane J Litman"}, {"title": "Contextaware argumentative relation mining", "journal": "", "year": "2016", "authors": "Huy Nguyen; Diane J Litman"}, {"title": "Improving argument mining in student essays by learning and exploiting argument indicators versus essay topics", "journal": "AAAI Press", "year": "2016-05-16", "authors": "Huy Nguyen; Diane J Litman"}, {"title": "Argumentation mining: The detection, classification and structure of arguments in text", "journal": "ACM", "year": "2009", "authors": "Raquel Mochales Palau; Marie-Francine Moens"}, {"title": "Identifying appropriate support for propositions in online user comments", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Joonsuk Park; Claire Cardie"}, {"title": "From argument diagrams to argumentation mining in texts: A survey", "journal": "", "year": "2013", "authors": "Andreas Peldszus; Manfred Stede"}, {"title": "", "journal": "Inform. Nat. Intell", "year": "", "authors": "J Cogn"}, {"title": "Identifying argument components through textrank", "journal": "", "year": "2016", "authors": "Georgios Petasis; Vangelis Karkaletsis"}, {"title": "Show me your evidence -an automatic method for context dependent evidence detection", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Ruty Rinott; Lena Dankin; Carlos Alzate Perez; Mitesh M Khapra; Ehud Aharoni; Noam Slonim"}, {"title": "Applying kernel methods to argumentation mining", "journal": "AAAI Press", "year": "2012-05-23", "authors": "Niall Rooney; Hui Wang; Fiona Browne"}, {"title": "Smoothing and differentiation of data by simplified least squares procedures", "journal": "Analytical chemistry", "year": "1964", "authors": "Abraham Savitzky;  Golay"}, {"title": "Annotating argument components and relations in persuasive essays", "journal": "", "year": "2014", "authors": "Christian Stab; Iryna Gurevych"}, {"title": "Identifying argumentative discourse structures in persuasive essays", "journal": "", "year": "2014", "authors": "Christian Stab; Iryna Gurevych"}, {"title": "Parsing argumentation structures in persuasive essays", "journal": "Computational Linguistics", "year": "2017", "authors": "Christian Stab; Iryna Gurevych"}, {"title": "", "journal": "", "year": "", "authors": ""}, {"title": "Hierarchical dirichlet processes", "journal": "Journal of the American Statistical Association", "year": "2006", "authors": "Yee Whye Teh; Michael I Jordan; J Matthew; David M Beal;  Blei"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2: Attraction evaluation for the sentences of a considered text", "figure_data": ""}, {"figure_label": "3457", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :Figure 4 :Figure 5 :Figure 7 :3457Figure 3: Precision, Recall and F1-measure with different thresholds", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Area under the precision-recall (P) and the ROC (R) curves", "figure_data": "C1C2PPCM CMMPCM CMA2T0.790.31 0.080.260.19\u03d50.840.29 0.060.190.1\u03c10.680.29 0.090.240.19Baseline0.680.31 0.110.160.06RPCM CMMPCM CMA2T0.40.52 0.620.70.76\u03d50.520.51 0.530.580.57\u03c10.160.52 0.770.690.77Baseline0.160.53 0.790.310.18"}], "doi": "10.1007/978-3-319-07064-323"}