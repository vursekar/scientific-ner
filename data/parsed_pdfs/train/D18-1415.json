{"authors": "Weikang Wang; Jiajun Zhang; Han Zhang; Mei-Yuh Hwang; Chengqing Zong; Zhifei Li", "pub_date": "", "title": "A Teacher-Student Framework for Maintainable Dialog Manager", "abstract": "Reinforcement learning (RL) is an attractive solution for task-oriented dialog systems. However, extending RL-based systems to handle new intents and slots requires a system redesign. The high maintenance cost makes it difficult to apply RL methods to practical systems on a large scale. To address this issue, we propose a practical teacherstudent framework to extend RL-based dialog systems without retraining from scratch. Specifically, the \"student\" is an extended dialog manager based on a new ontology, and the \"teacher\" is existing resources used for guiding the learning process of the \"student\". By specifying constraints held in the new dialog manager, we transfer knowledge of the \"teacher\" to the \"student\" without additional resources. Experiments show that the performance of the extended system is comparable to the system trained from scratch. More importantly, the proposed framework makes no assumption about the unsupported intents and slots, which makes it possible to improve RL-based systems incrementally.", "sections": [{"heading": "Introduction", "text": "With the flourish development of virtual personal assistants (e.g., Amazon Alexa and Google Assistant), task-oriented dialog systems, which can help users accomplish tasks naturally, have been a focal point in both academic and industry research. In the early work, the task-oriented dialog system is merely a set of hand-crafted mapping rules defined by experts. This is referred to as a rule-based system. Although rule-based systems often have acceptable performance, they are inconvenient and difficult to be optimized. Recently, reinforcement learning approaches have been applied to optimize dialog systems through interaction with a user simulator or employed real users online (Ga\u0161i\u0107 et al., 2011;Su et al., 2016a;Li et al., 2016, Figure 1: An example of a task-oriented dialog after the system comes online. The user is confused because the \"confirm\" intent has not been considered in the deployed system. Dialog rules should be embedded in a new system to handle such situations. 2017b). It has been proven that RL-based dialog systems can abandon hand-crafted dialog manager and achieve more robust performance than rulebased systems (Young et al., 2013).\nTypically, the first step of building RL-based dialog systems is defining a user model 1 and necessary system actions to complete a specific task (e.g., seek restaurants information or book hotels). Based on such ontology, developers can extract dialog features and train the dialog manager model in an interaction environment. Such systems work well if real users are consistent with the predefined user model. However, as shown in Fig. 1, the unanticipated actions 2 of real users will lead to a poor user experience.\nIn this situation, the original system should be extended to support new user actions based on user feedback. However, adding new intents or slots will change the predefined ontology. As a consequence, developers need to extract additional dialog features based on new ontology. Besides, new system actions may be added to deal with new user actions. The network architecture of the new system and the original one will be different. The new system can not inherit the parameters from the old one directly. It will make the original dialog manager model invalid. Therefore, developers have to retrain the new system by interacting with users from scratch. Though there are many methods to train a RL-based dialog manager efficiently (Su et al., 2016a(Su et al., , 2017Lipton et al., 2017;Chen et al., 2017), the unmaintainable RL-based dialog systems will still be put on the shelf in real-world applications (Paek and Pieraccini, 2008;Paek, 2006).\nTo alleviate this problem, we propose a teacherstudent framework to maintain the RL-based dialog manager without training from scratch. The idea is to transfer the knowledge of existing resources to a new dialog manager. Specifically, after the system is deployed, if developers find some intents and slots missing before, they can define a few simple dialog rules to handle such situations. For example, under the condition shown in Fig. 1, a reasonable strategy is to inform the user of the location of this restaurant. Then we encode information of such hand-crafted logic rules into the new dialog manager model. Meanwhile, user logs and dialog policy of the original system can guide the new system to complete tasks like the original one. Under the guidance of the \"teacher\" (logic rules, user logs, and original policy), we can reforge an extended dialog manager (the \"student\") without a new interaction environment. We conduct a series of experiments with simulated and real users on restaurant domain. The extensive experiments demonstrate that our method can overcome the problem brought by the unpredictable user behavior after deployment. Owing to reuse of existing resources, our framework saves time in designing new interaction environments and retraining RL-based systems from scratch. More importantly, our method does not make any assumptions about the unsupported intents and slots. So the system can be incrementally extended once developers find new intents and slots that are not taken into account before. As far as we know, we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically.", "n_publication_ref": 10, "n_figure_ref": 3}, {"heading": "Related Work", "text": "Dialog Manager The dialog manager of taskoriented dialog systems, which consists of a state tracker and a dialog policy module, controls the dialog flow. Recently, deep reinforcement learning (Mnih et al., 2013(Mnih et al., , 2015 has been applied to optimize the dialog manager in an \"endto-end\" way, including deep Q-Network (Lipton et al., 2017;Li et al., 2017b;Peng et al., 2017;Zhao and Eskenazi, 2016) and policy gradient methods (Williams et al., 2017;Su et al., 2016b;Dhingra et al., 2017). RL methods have shown great potential in building a robust dialog system automatically. However, RL-based approaches are rarely used in real-world applications because of the maintainability problem (Paek and Pieraccini, 2008;Paek, 2006). To extend the domain of dialog systems, Ga\u0161ic et al. (2014) explicitly defined kernel functions between the belief states that come from different domains. However, defining an appropriate kernel function is nontrivial when the ontology has changed drastically. Shah et al. (2016) proposed to integrate turnlevel feedback with a task-level reward signal to learn how to handle new user intents. This approach alleviates the problem that arises from the difference between training and deployment phases. But it still fails when the developers have not considered all user actions in advance. Lipton et al. (2017) proposed to use BBQ-Networks to extend the domain. However, similar to Shah et al. (2016), the BBQ-Networks have reserved a few bits in the feature vector for new intents and slots. And system actions for handling new user actions have been considered in the original system design. This assumption is not practical enough. Compared to the existing domain extension methods, our work addresses a more practical problem: new intents and slots are unknown to the original system. If we need to extend the dialog system, we should design a new network architecture to represent new user actions and take new system actions into account. Knowledge Distillation Our proposed framework is inspired by recent work in knowledge distillation (Bucilu et al., 2006;Ba and Caruana, 2014;Li et al., 2014). Knowledge distillation means training a compact model to mimic a larger teacher model by approximating the function learned by the teacher. Hinton et al. (2015) introduced knowledge distillation to transfer knowledge from ", "n_publication_ref": 19, "n_figure_ref": 0}, {"heading": "RL-based Dialog Manager", "text": "Before going to the details of our method, we provide some background on the RL-based dialog manager in this section. Fig. 2 shows an overview of such dialog manager. We describe each of the parts briefly below.\nFeature Extraction At the t-th turn of a dialog, the user input u t is parsed into domain specific intents and slots to form a semantic frame a u t by a language understanding (LU) module. o u t and o s t\u22121 are the one-hot representations of such semantic frames for the current user input and the last system output respectively. Alternatively, o u t can be a simple n-grams representation of u t . But the vocabulary size is relatively large in real-world applications. It will yield slow convergence in the absence of a LU module. Based on the slot-value pair output with the highest probability, a query is sent to a database to retrieve user requested information. o db t is the one-hot representation of the database result. As a result, the observable information x t is the concatenation of o u t , o s t\u22121 and o db t . State Representation Based on the extracted feature vector x t and previous internal state s t\u22121 , recurrent neural networks (RNNs) are used to infer the latent representation of dialog state s t at step t. Current state s t can be interpreted as the summary of dialog history h t up to current step. Dialog Policy Next, the dialog state representation s t is fed into a policy network. The output \u03c0(a|h t ; \u03b8) of the policy network is a probability distribution over a predefined system action set A s . Lastly, the system samples an action a s t \u2208 A s based on \u03c0(a|h t ; \u03b8) and receives a new observation x t+1 with an assigned reward r t . The policy parameters \u03b8 can be learned by maximizing the expected discounted cumulative rewards:\nJ (\u03b8) = E T \u2212t k=0 \u03b3 k r t+k (1)\nwhere T is the maximal step, and \u03b3 is the discount factor. Usually the parameters \u03b8 can be iteratively updated by policy gradient (Williams, 1992) approach. The policy gradient can be empirically estimated as:\n\u2207 \u03b8 J (\u03b8) \u2248 1 N N i=1 T t=1 \u2207 \u03b8 log \u03c0(a s i,t |hi,t; \u03b8)(Gi,t \u2212b) (2)\nwhere N is the number of sampled episodes in a batch, G i,t = T \u2212t k=0 \u03b3 k r i,t+k is the sum of discounted reward at step t in the episode i, and b is a baseline to estimate the average reward of current policy.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Notations and Problem Definition", "text": "Let A u and A s denote the supported user and system action sets in the original system design respectively. u t denotes the user input in the t-th turn. The LU module converts u t into a domain specific intent and associated slots to form a user action a u t \u2208 A u . The system will return an action a s t \u2208 A s according to the dialog manager \u03c0(\u03b8). Note that not all user actions are taken into account at the beginning of system design. After deployment, the developers can find that some user actions A u new cannot be handled by the original system based on the human-machine interaction logs D. Generally speaking, A u new consists of new intents and slots. Our goal is to extend the original system to support the new user action set A u = A u \u222aA u new . The extended dialog manager and new system action set are denoted as \u03c0(\u03b8 ) and A s respectively. To handle new user actions, more system actions may be added to the new system. It means that A s is a subset of A s .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Approach", "text": "Fig. 3 shows two kinds of strategies to extend the original system. The first strategy requires a new interaction environment. However, building a user simulator or hiring real users once the system needs to be extended is costly and impractical in real-world applications. By contrast, our method enhances the reuse of existing resources. The basic idea is to use the existing user logs, original dialog policy model and logic rules (\"teacher\") to guide the learning process of a new dialog manager (\"student\"). Without an expensive interaction environment, the developers can maintain RL-based dialog systems as efficiently and straightforwardly as in rule-based systems.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Distill Knowledge from the Original System", "text": "Although the ontology of the new system is different from the original one, the extended dialog manager can still reuse dialog policy of the illconsidered system circuitously. Given user logs D and the original dialog manager \u03c0(\u03b8), we define a loss L(\u03b8 ; D, \u03b8) to minimize the difference between new dialog manager \u03c0(\u03b8 ) and the old one:\nL(\u03b8 ; D, \u03b8) = d\u2208D |d| t=1 KL( \u03c0(a|ht; \u03b8) || \u03c0(a|ht; \u03b8 ) ) (3)\nwhere \u03c0(a|h t ; \u03b8) and \u03c0(a|h t ; \u03b8 ) are the policy distributions over A s and A s given the same dialog history h t . |d| means turns of a specific dialog d \u2208 D. To deal with unsupported user actions, A s will be a subset of A s . As a result, the KL term in equation ( 3) can be defined as follows:\nKL( \u03c0(a|ht; \u03b8) || \u03c0(a|ht; \u03b8 ) ) = |As| k=1 \u03c0(a k |ht; \u03b8) log\u03c0(a k |ht; \u03b8) \u2212 log\u03c0(a k |ht; \u03b8 )(4)\nAs the original policy parameters \u03b8 are fixed, the loss function in equation ( 3) can be rewritten as:\nL(\u03b8 ; D, \u03b8) = \u2212 d\u2208D |d| t=1 |As| k=1 \u03c0(a k |ht; \u03b8)log\u03c0(a k |ht; \u03b8 )(5)\nThis objective will transfer knowledge of the original system to the \"student\" at the turn level. Under the guidance of the original system, the extended system will be equipped with the primary strategy to complete a task.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Distill Knowledge from Logic Rules", "text": "It's easy for the developers to give logic rules on the system responses to handle new user actions.\nFor example, if users ask to confirm a slot, the system should inform the value of that slot immediately. Note that these system actions which handle new user actions may not exist in the old model. It means the architecture of the new system is different from the old one.\nWe define a set of logic constraints R = {(h l , a l )} L l=1 , where h l \u2208 H R indicates the dialog context condition in the l-th rule, and a l \u2208 A s is the corresponding system action. The number of logic rules L is equal to the number of new user actions. These rules can be seen as triggers: if dialog context h t in current turn t meets the context condition h l defined in logic rules, then the system should execute a l . In our work, we use the output of the LU module to judge whether the current dialog context meets the condition defined by logic rules. An alternative method is simple rules matching. To distill the knowledge of rules to a new system, we define a loss function L(\u03b8 ; D, R) to embed such constraints in the new system:\nL(\u03b8 ; D, R) = \u2212 d\u2208D |d| t=1 h l \u2208H R 1{ht = h l } \u00d7 |A s | k=1 1{a k = a l } log \u03c0(a k |ht; \u03b8 ) (6)\nWhere 1{\u2022} is the indicate function. Equation ( 6) suggests the new dialog manager \u03c0(\u03b8 ) will be penalized if it violates the instructions defined by the dialog rules. Note that, for simplicity, we assume these rules are absolutely correct and mutually exclusive. Although this hypothesis may lead to a non-optimal dialog system, these rules define reasonable system actions to corresponding dialog contexts. It implies that the new system can be further refined by reinforcement learning once a new interaction environment is available.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Extension of Dialog Manager", "text": "In the absence of a new training environment, learning is made possible by exploiting structure that holds in the new dialog manager. On one hand, we expect the new system can complete tasks like the original one. On the other hand, it should satisfy the constraints defined by dialog rules. So, the learning objective of new dialog manager \u03c0(\u03b8 ) can be defined as follows:\nL(\u03b8 ; D, \u03b8, R) = L(\u03b8 ; D, R) if ht \u2208 HR ; L(\u03b8 ; D, \u03b8) else (7)\nWhen the dialog context h t in the t-th turn satisfies a condition defined in H R , we distill knowledge of rules into the new system. Otherwise, we distill knowledge of the original system into the new one. Instead of retraining from scratch, developers can extend RL-based systems by reusing existing resources.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments", "text": "To evaluate our method, we conduct experiments on a dialog system extension task of restaurant domain.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Domain", "text": "The dialog system provides restaurant information in Beijing. The database we use includes 2988 restaurants. This domain consists of 8 slots (name, area, price range, cuisine, rating, number of comments, address and phone number) in which the first four slots (inform slots) can be used for searching the desirable restaurant and all of these slots (request slots) can be asked by users. In each dialog, the user has a goal containing a set of slots, indicating the constraints and requests from users. For example, an inform slot, such as \"inform(cuisine=Sichuan cuisine)\", indicates the user finding a Sichuan restaurant, and a request slot, such as \"request(area)\", indicates the user asking for information from the system (Li et al., 2016(Li et al., , 2017bPeng et al., 2017).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Measurements", "text": "A main advantage of our approach is that the unconsidered user actions can be handled in the extended system. In addition to traditional measurements (e.g., success rate, average turns and average reward), we define an objective measurement called \"Satis.\" (user satisfaction) to verify this feature in the simulated evaluation. \"Satis.\"\nindicates the rate at which the system takes reasonable actions in unsupported dialog situations. It can be calculated as follows:\nSatis. = d\u2208D |d| t=1 L l=1 1{ht = h l }1{a s t = a l } d\u2208D |d| t=1 L l=1 1{ht = h l } (8)\nwhere h t and a s t are the dialog history and system action in the t-th turn, h l and a l are dialog context condition and corresponding system action defined in the l-th rules. Intuitively, an unreasonable system reply will frustrate users and low \"Satis.\" indicates a poor user experience.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Sim1", "text": "Sim2 LU Error Rate Succ. Turn Reward Satis. Succ. Turn Reward Satis.   Although \"Satis.\" is obtained based on our handcrafted dialog rules, it approximately measures the subjective experience of real users after system deployment.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "User Simulator", "text": "Training RL-based dialog systems requires a large number of interactions with users. It's common to use a user simulator to train RL-based dialog systems in an online fashion (Pietquin and Dutoit, 2006;Scheffler and Young, 2002;Li et al., 2016).\nAs a consequence, we construct an agenda-based user simulator, which we refer to as Sim1, to train the original RL-based system. The user action set of Sim1 is denoted as A u , which includes such intents 4 : \"hello\", \"bye\", \"inform\", \"deny\", \"negate\", \"affirm\", \"request\", \"reqalts\" and \"null\". The slots of Sim1 are shown in section 6.1. In each turn, the user action consists of a intent and slots and we append the value of slots according to the user goal.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Implementation of the Original System", "text": "For the original RL-based dialog system, a feature vector x t of size 191 is extracted. This vector is the concatenation of encodings of LU results, the previous system reply, database results and the current turn number. The LU module is implemented with an SVM 5 for intent detection and a CRF 6 for slot filling. The language generation module is implemented by a rule-based method. The hidden dialog state representation is inferred by a GRU (Chung et al., 2014). We set the hidden states of the GRU to be 120. The policy network is implemented as a Multilayer Perceptron (MLP) with one hidden layer. The size of the hidden layer is 80. The output dimension of policy network is 15, which corresponds to the number of system actions. To encourage shorter interaction, we set a small per-turn negative reward R turn = \u22121. The maximal turn is set to be 40. If the user goal is satisfied, the policy will be encouraged by a large positive reward R succ = 10; otherwise the policy will be penalized by a negative reward R f ail = \u221210. Discounted factor \u03b3 = 0.9.\nThe baseline b of current policy is estimated on sampled episodes in a batch. The batch size N is set to be 32. Adadelta (Zeiler, 2012) method is used to update model parameters. The original system S 1 is trained by interacting with Sim1.\nAfter about 2400 interactions, the performance of S 1 starts to converge.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Simulated Evaluation", "text": "To evaluate our approach, we design another user simulator, which we denote as Sim2, to simulate the unpredictable real customers. The user action set of Sim2 is denoted as A u . The difference between A u and A u is reflected on the domain specific intents 7 . Specifically, in addition to the intents of Sim1, A u includes the \"confirm\" intent.\nThe difference in user action sets will result in different interaction strategies between Sim1 and Sim2. To verify whether a recommended restaurant meets his (her) constraints, Sim1 can only request what the value of a specific slot is, but Sim2 can request or confirm. After obtaining the original system S 1 , we deploy it to interact with Sim1 and Sim2 respectively, under different LU error rates (Li et al., 2017a). In each condition, we simulate 3200 episodes to obtain the performance.  details of the test performance. Table 2 shows the statistics of turns when S 1 interacts with Sim2.\nAs shown in Table 1, S 1 achieves higher dialog success rate and rewards when testing with Sim1. When interacting with Sim2, nearly half of the responses to unsupported user actions are not reasonable. Notice even though Sim2 contains new user actions, some of the new actions might be appropriately handled by S 1 . It may be due to the robustness of our RL-based system. But it's far from being desired. The unpredictable real user behavior in the deployment stage will lead to a poor user experience in real-world applications. It proves the importance of a maintainable system.\nTo maintain the original system, we define a few simple logic rules to handle unsupported user actions: if users confirm the value of a slot in current turn, the system should inform users of that value. These rules 8 are intuitive and reasonable to handle queries such as \"Is this restaurant located in Zhongguancun?\". There are four slots 9 that can be used for confirmation, so we define four logic rules in all. Due to the change in ontology, we add a new status in dialog features to represent the \"confirm\" intent of users. It leads to a change in the model architecture of extended dialog manager. Then we distill knowledge of the S 1 and logic rules into the extended system. No additional data is used to obtain the extended system.\nFor comparison, we retrain another new system (contrast system) from scratch by interacting 8 In the practical dialog system, we can inject more complex logic rules and take dialog history into account. These rules are not limited to question/answer mapping. 9 They are \"name\", \"area\", \"price range\" and \"cuisine\".\nwith Sim2. After about 2600 interactions with Sim2, the performance of contrast system starts to converge. Note that in order to build the contrast system, the developers need to redesign a new user simulator or hire real users. It's expensive and impractical in industrial applications. Then we simulate 3200 interactions with Sim2 to obtain its performance. Fig. 4 illustrates the performance of different systems.\nAs can be seen, the extended system performs better than the original system in terms of dialog success rate and \"Satis.\". This is to a large degree attributed to the consideration of new user actions. Fig. 4(a) shows that the contrast system achieves higher dialog success rate than the extended system. But the gap is negligible. However, the contrast system is trained from scratch under a new interaction environment and the extended system is trained by transferring knowledge of the original system and logic rules. To train the contrast system, about 2600 episodes are sampled by interacting with a new interaction environment. But no additional data is used to train the extended system.\nIn Fig. 4(b), the \"Satis.\" of the extended system is slightly higher than the contrast system. This is due to the fact that the extended system learns how to deal with new user actions from logic rules but the contrast system obtains dialog policy by exploring the environment. As a result, the contrast system learns a more flexible dialog policy than the extended system 10 . However, the \"Satis.\" has a bias to the suboptimal rules,   Left column shows the dialog context condition; Right column shows the corresponding system action. We define 14 rules in all to handle newfound intents and slots shown in Table 3. rather than the optimal policy gained from the environment. It suggests the extended system can be further refined by reinforcement learning once a new interaction environment is available.", "n_publication_ref": 3, "n_figure_ref": 3}, {"heading": "Human Evaluation", "text": "In any case, the developers can't guarantee all user actions are considered. Fortunately, our method makes no assumptions about the new user actions and new dialog model architecture. As a result, the system can be extended over multiple iterations.\nTo evaluate this characteristic, we deploy the extended system 11 in section 6.5 to interact with real human users. Users are given a goal sampled from our corpus for reference. To elicit more complex situations, they are encouraged to interact with our system by new intents and slots related to the restaurant domain. At the end of each dialog, they are asked to give a subjective rating on the scale from 1 to 5 based on the naturalness of the system (1 is the worst, 5 is the best.). After filtering dialog sessions unrelated to our task, we collect 315 episodes in total. Table 3 shows the details of the user logs. As shown in Table 3, after deployment, there are a few slots 11 The extended system in the simulated evaluation will be the original system in our human evaluation.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Original System", "text": "Extended System and intents unseen before. For example, users may ask for the discount information or take a taxi to the restaurant. To represent the new intents and slots, the dimension of extracted dialog features is extended to 236. Meanwhile, the number of system actions is extended to 29 to handle new user actions. To deal with the newfound user actions, we define 14 rules in total. Table 4 shows the details of new defined logic rules. Then we distill the knowledge of the original system and logic rules into a new system. Fig. 5 shows the comparison in user ratings. The extended system significantly gets a higher subjective rating than the original one. It proves that the extended system can give reasonable responses in unseen dialog situations.\nTable 5 shows sample dialogs from the original system and extended system with real users. We can see that the extended system is much more coherent since it takes new user actions into account. It inspires us that a complicated RL-based dialog system can start from a simple one and be improved incrementally based on our framework. This design pattern will be much more efficient if the interaction environment is hard to access.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "In this paper, we present a practical solution to maintain RL-based dialog systems without retraining from scratch. By reusing existing resources, developers can extend the RL-based dialog manager to deal with unpredictable user actions after deployment. Furthermore, our method holds no constraints on the architecture of the new system and retains the key benefit of data-driven methods. It means the new system can be maintained iteratively and further refined by RL once a new interaction environment is available. The sim-", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Original System", "text": "Extended System User: I want to find a Sichuan restaurant.\nUser: I want to find a Sichuan restaurant in System: Where do you want to go? Zhongguancun area. User: Zhongguancun area.\nSystem: \"Hot Charge\" is a nice Sichuan restaurant in System: \"Hot Charge\" is a nice Sichuan restaurant in in Zhongguancun area. Zhongguancun area.\nUser: What's the discount of this restaurant? User: Can you tell me the discount?\nSystem: This restaurant is offering 10% off. System: The average cost is 42 CNY.\nUser: Is there a waiting line? User: Tell me the discount! System: Currently there are 5 parties waiting in line. System: \"Hot Charge\" is a nice Sichuan restaurant in User: Can you book a table for me? Zhongguancun area.\nSystem: Ok. User: Bye.\nUser: Thanks. System: Bye.\nSystem: You are welcome. Subjective Rating = 2\nSubjective Rating = 5\nTable 5: Sample dialogs of the original and extended systems in the human evaluation. The original system will collapse when encountering unseen slots and intents. After extension, the new system can handle more user actions.\nulation and human experiments show our proposed framework guarantees the maintainability and scalability in RL-based systems, which are necessary for any industrial application.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "The research work described in this paper has been supported by the National Key Research and Development Program of China under Grant No. 2017YFB1002103 and also supported by the Natural Science Foundation of China under Grant No. 61333018.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Do deep nets really need to be deep?", "journal": "", "year": "2014", "authors": "Jimmy Ba; Rich Caruana"}, {"title": "", "journal": "", "year": "2006", "authors": "Cristian Bucilu; Rich Caruana; Alexandru Niculescu-Mizil"}, {"title": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining", "journal": "ACM", "year": "", "authors": ""}, {"title": "Affordable on-line dialogue policy learning", "journal": "", "year": "2017", "authors": "Cheng Chang; Runzhe Yang; Lu Chen; Xiang Zhou; Kai Yu"}, {"title": "Agent-aware dropout dqn for safe and efficient on-line dialogue policy learning", "journal": "", "year": "2017", "authors": "Lu Chen; Xiang Zhou; Cheng Chang; Runzhe Yang; Kai Yu"}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "journal": "", "year": "2014", "authors": "Junyoung Chung; Caglar Gulcehre; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Towards end-to-end reinforcement learning of dialogue agents for information access", "journal": "", "year": "2017", "authors": "Bhuwan Dhingra; Lihong Li; Xiujun Li; Jianfeng Gao; Yun-Nung Chen; Faisal Ahmed; Li Deng"}, {"title": "On-line policy optimisation of spoken dialogue systems via live interaction with human subjects", "journal": "IEEE", "year": "2011", "authors": "Milica Ga\u0161i\u0107; Filip Jur\u010d\u00ed\u010dek; Blaise Thomson; Kai Yu; Steve Young"}, {"title": "Incremental on-line adaptation of pomdp-based dialogue managers to extended domains", "journal": "", "year": "2014", "authors": "Milica Ga\u0161ic; Dongho Kim; Pirros Tsiakoulis; Catherine Breslin; Matthew Henderson; Martin Szummer; Blaise Thomson; Steve Young"}, {"title": "Dialog state tracking challenge 2 & 3", "journal": "", "year": "2013", "authors": "Matthew Henderson; Blaise Thomson; Jason Williams"}, {"title": "Distilling the knowledge in a neural network", "journal": "", "year": "2015", "authors": "Geoffrey Hinton; Oriol Vinyals; Jeff Dean"}, {"title": "Harnessing deep neural networks with logic rules", "journal": "", "year": "2016", "authors": "Zhiting Hu; Xuezhe Ma; Zhengzhong Liu; Eduard Hovy; Eric Xing"}, {"title": "Learning small-size dnn with outputdistribution-based criteria", "journal": "", "year": "2014", "authors": "Jinyu Li; Rui Zhao; Jui-Ting Huang; Yifan Gong"}, {"title": "Investigation of language understanding impact for reinforcement learning based dialogue systems", "journal": "", "year": "2017", "authors": "Xiujun Li; Yun-Nung Chen; Lihong Li; Jianfeng Gao; Asli Celikyilmaz"}, {"title": "A user simulator for task-completion dialogues", "journal": "", "year": "2016", "authors": "Xiujun Li; C Zachary; Bhuwan Lipton; Lihong Dhingra; Jianfeng Li; Yun-Nung Gao;  Chen"}, {"title": "End-to-end task-completion neural dialogue systems", "journal": "", "year": "2017", "authors": "Xuijun Li; Yun-Nung Chen; Lihong Li; Jianfeng Gao"}, {"title": "Bbq-networks: Efficient exploration in deep reinforcement learning for task-oriented dialogue systems", "journal": "", "year": "2017", "authors": "Zachary Lipton; Xiujun Li; Jianfeng Gao; Lihong Li; Faisal Ahmed; Li Deng"}, {"title": "Iterative policy learning in end-to-end trainable task-oriented neural dialog models", "journal": "", "year": "2017", "authors": "Bing Liu; Ian Lane"}, {"title": "Playing atari with deep reinforcement learning", "journal": "", "year": "2013", "authors": "Volodymyr Mnih; Koray Kavukcuoglu; David Silver; Alex Graves; Ioannis Antonoglou; Daan Wierstra; Martin Riedmiller"}, {"title": "Humanlevel control through deep reinforcement learning", "journal": "Nature", "year": "2015", "authors": "Volodymyr Mnih; Koray Kavukcuoglu; David Silver; Andrei A Rusu; Joel Veness; G Marc; Alex Bellemare; Martin Graves; Andreas K Riedmiller; Georg Fidjeland;  Ostrovski"}, {"title": "Reinforcement learning for spoken dialogue systems: Comparing strengths and weaknesses for practical deployment", "journal": "", "year": "2006", "authors": "Tim Paek"}, {"title": "Automating spoken dialogue management design using machine learning: An industry perspective", "journal": "", "year": "2008", "authors": "Tim Paek; Roberto Pieraccini"}, {"title": "Composite task-completion dialogue policy learning via hierarchical deep reinforcement learning", "journal": "", "year": "2017", "authors": "Baolin Peng; Xiujun Li; Lihong Li; Jianfeng Gao; Asli Celikyilmaz; Sungjin Lee; Kam-Fai Wong"}, {"title": "A probabilistic framework for dialog simulation and optimal strategy learning", "journal": "IEEE Transactions on Audio, Speech, and Language Processing", "year": "2006", "authors": "Olivier Pietquin; Thierry Dutoit"}, {"title": "Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning", "journal": "", "year": "2002", "authors": "Konrad Scheffler; Steve Young"}, {"title": "Interactive reinforcement learning for taskoriented dialogue management", "journal": "", "year": "2016", "authors": "Pararth Shah; Dilek Hakkani-T\u00fcr; Larry Heck"}, {"title": "Labelfree supervision of neural networks with physics and domain knowledge", "journal": "", "year": "2017", "authors": "Russell Stewart; Stefano Ermon"}, {"title": "Sample-efficient actor-critic reinforcement learning with supervised data for dialogue management", "journal": "", "year": "2017", "authors": "Pei-Hao Su; Pawel Budzianowski; Stefan Ultes; Milica Gasic; Steve Young"}, {"title": "Online active reward learning for policy optimisation in spoken dialogue systems", "journal": "", "year": "2016", "authors": "Pei-Hao Su; Milica Gasic; Nikola Mrk\u0161i\u0107; Lina M Rojas Barahona; Stefan Ultes; David Vandyke; Tsung-Hsien Wen; Steve Young"}, {"title": "Continuously learning neural dialogue management", "journal": "", "year": "2016", "authors": "Pei-Hao Su; Milica Gasic; Nikola Mrksic; Lina Rojas-Barahona; Stefan Ultes; David Vandyke; Tsung-Hsien Wen; Steve Young"}, {"title": "Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", "journal": "", "year": "2017", "authors": "D Jason; Kavosh Williams; Geoffrey Asadi;  Zweig"}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "journal": "Machine learning", "year": "1992", "authors": "J Ronald;  Williams"}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "journal": "Proceedings of the IEEE", "year": "2013", "authors": "Steve Young; Milica Ga\u0161i\u0107; Blaise Thomson; Jason D Williams"}, {"title": "Adadelta: an adaptive learning rate method", "journal": "", "year": "2012", "authors": "D Matthew;  Zeiler"}, {"title": "Prior knowledge integration for neural machine translation using posterior regularization", "journal": "", "year": "2017", "authors": "Jiacheng Zhang; Yang Liu; Huanbo Luan; Jingfang Xu; Maosong Sun"}, {"title": "Annual Meeting of the Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"title": "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning", "journal": "", "year": "2016", "authors": "Tiancheng Zhao; Maxine Eskenazi"}], "figures": [{"figure_label": "3", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 3 :3Figure 3: Two kinds of strategies to extend the original system. (a) means redesigning and retraining a new system in an expensive interaction environment from scratch and (b) means transferring knowledge from existing resources to a new system. The network in red indicates the old system based on the original ontology. The networks in gray and purple represent the initialized and trained dialog manager models based on a new ontology respectively. On account of the change in ontology, the extended system has a different network architecture. The dash lines in (a) and (b) show the ability of a new model derives from various sources.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: Performance of different systems under simulation. The original, extended and contrast systems are shown in red, blue and purple bars. We test these systems by interacting with Sim2.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: Distribution of user ratings.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Performance of the original system when interacting with different user simulators. LU error means simulating slot errors and intent errors in different rates. Succ.: success rate, Turn: average turns, Reward: average reward.", "figure_data": "LU Error Rate0.000.050.100.20Total New User Actions25857 26166 27077 28385 1853 1859 1998 1912"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Statistics of turns when S 1 interacts with Sim2.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "shows the"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Details of the real user logs. Users are encouraged to interact with the original system by unsupported intents and slots. We find there are 14 user actions unseen before.", "figure_data": "Dialog ConditionSystem ActiontakeTaxi bookTable inform unseen slots request unseen slots confirm unseen slots offer information of slots API call API call recommend a restaurant offer information of slots"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Different types of rules for new user actions.", "figure_data": ""}], "doi": ""}