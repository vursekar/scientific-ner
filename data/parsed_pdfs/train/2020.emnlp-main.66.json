{"authors": "Chien-Sheng Wu; Steven Hoi; Richard Socher; Caiming Xiong; Salesforce Research", "pub_date": "", "title": "TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue", "abstract": "The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream taskoriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.", "sections": [{"heading": "Introduction", "text": "Pre-trained models with self-attention encoder architectures (Devlin et al., 2018; have been commonly used in many NLP applications. Such models are self-supervised based on a massive scale of general text corpora, such as English Wikipedia or books (Zhu et al., 2015). By further fine-tuning these representations, breakthroughs have been continuously reported for various downstream tasks, especially natural language understanding.\nHowever, previous work (Rashkin et al., 2018;Wolf et al., 2019) shows that there are some deficiencies in the performance to apply fine-tuning on conversational corpora directly. One possible reason could be the intrinsic difference of linguistic patterns between human conversations and writing text, resulting in a large gap of data distributions (Bao et al., 2019). Therefore, pre-training dialogue language models using chit-chat corpora from social media, such as Twitter or Reddit, has been recently investigated, especially for dialogue response generation (Zhang et al., 2019) and retrieval (Henderson et al., 2019b). Although these opendomain dialogues are diverse and easy-to-get, they are usually short, noisy, and without specific chatting goals.\nOn the other hand, a task-oriented dialogue has explicit goals (e.g. restaurant reservation or ticket booking) and many conversational interactions. But each dataset is usually small and scattered because obtaining and labeling such data is time-consuming. Moreover, a task-oriented dialogue has explicit user and system behaviors where a user has his/her goal, and a system has its belief and database information, which makes the language understanding component and dialogue policy learning more important than those chit-chat scenarios.\nThis paper aims to prove this hypothesis: selfsupervised language model pre-training using taskoriented corpora can learn better representations than existing pre-trained models for task-oriented downstream tasks. We emphasize that what we care about the most is not whether our pre-trained model can achieve state-of-the-art results on each downstream task since most of the current best models are built on top of pre-trained models, and ours can easily replace them. We avoid adding too many additional components on top of the pre-training architecture when fine-tuning in our experiments.\nWe collect and combine nine human-human and multi-turn task-oriented dialogue corpora to train a task-oriented dialogue BERT (TOD-BERT). In total, there are around 100k dialogues with 1.4M utterances across over 60 different domains. Like BERT (Devlin et al., 2018), TOD-BERT is formulated as a masked language model and uses a deep bidirectional Transformer (Vaswani et al., 2017) encoder as its model architecture. Unlike BERT, TOD-BERT incorporates two special tokens for user and system to model the corresponding dialogue behavior. A contrastive objective function of response selection task is combined during pretraining stage to capture response similarity. We select BERT because it is the most widely used model in NLP research recently, and our unified datasets can be easily applied to pre-train any existing language models.\nWe test TOD-BERT on task-oriented dialogue systems on four core downstream tasks, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. What we observe is: TOD-BERT outperforms BERT and other strong baselines such as GPT-2 (Radford et al., 2019) and DialoGPT (Zhang et al., 2019) on all the selected downstream tasks, which further confirms its effectiveness for improving dialogue language understanding. We find that response contrastive learning is beneficial, but it is currently overlooked not well-investigated in dialogue pretraining research. More importantly, TOD-BERT has a stronger few-shot ability than BERT on each task, suggesting that it can reduce the need for expensive human-annotated labels. TOD-BERT can be easily leveraged and adapted to a new taskoriented dialogue dataset. Our source code and data processing are released to facilitate future research on pre-training and fine-tuning of task-oriented dialogue 1 .", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Related Work", "text": "General Pre-trained Language Models, which are trained on massive general text such as Wikipedia and BookCorpus, can be roughly divided into two categories: uni-directional or bidirectional attention mechanisms. GPT (Radford et al., 2018) and GPT-2 (Radford et al., 2019) are representatives of uni-directional language models using a Transformer decoder, where the objective is to maximize left-to-right generation likelihood. These models are commonly applied in natural language generation tasks. On the other hand, BERT (Devlin et al., 2018), RoBERTa , and their variances are pre-trained using a Transformer encoder with bi-directional token prediction. These models are usually evaluated on classification tasks such as GLUE benchmark (Wang et al., 2018) or span-based question answering tasks (Ra-1 github.com/jasonwu0731/ToD-BERT jpurkar et al., 2016). Some language models can support both unidirectional and bi-directional attention, such as UniLM (Dong et al., 2019). Conditional language model pre-training is also proposed. For example, CTRL (Keskar et al., 2019) is a conditional Transformer model, trained to condition on control codes that govern style, content, and task-specific behavior. Recently, multi-task language model pretraining with unified sequence-to-sequence generation is proposed. Text-to-text Transformer (T5) (Raffel et al., 2019) unifies multiple text modeling tasks and achieves the promising results in various NLP benchmarks.\nDialogue Pre-trained Language Models are mostly trained on open-domain conversational data from Reddit or Twitter for dialogue response generation. Transfertransfo (Wolf et al., 2019) achieves good performance on ConvAI-2 dialogue competition using GPT-2. DialoGPT (Zhang et al., 2019) is an extension of GPT-2 that is pre-trained on Reddit data for open-domain response generation. Con-veRT (Henderson et al., 2019a) pre-trained a dual transformer encoder for response selection task on large-scale Reddit (input, response) pairs. PLATO (Bao et al., 2019) uses both Twitter and Reddit data to pre-trained a dialogue generation model with discrete latent variables. All of them are designed to cope with the response generation task for opendomain chatbots.\nPretraining for task-oriented dialogues, on the other hand, has few related works. Budzianowski and Vuli\u0107 (2019) first apply the GPT-2 model to train on response generation task, which takes system belief, database result, and last dialogue turn as input to predict next system responses. It only uses one dataset to train its model because few public datasets have database information available. Henderson et al. (2019b) pre-trained a response selection model for task-oriented dialogues. They first pre-train on Reddit corpora and then fine-tune on target dialogue domains, but their training and fine-tuning code is not released. Peng et al. (2020) focus on the natural language generation (NLG) task, which assumes dialogue acts and slot-tagging results are given to generate a natural language response. Pre-training on a set of annotated NLG corpora can improve conditional generation quality using a GPT-2 model.\nName # Dialogue # Utterance Avg. Turn # Domain MetaLWOZ  37,884 432,036 11.4 47 Schema (Rastogi et al., 2019) 22,825 463,284 20.3 17 Taskmaster (Byrne et al., 2019) 13,215 303,066 22.9 6 MWOZ (Budzianowski et al., 2018) 10,420 71,410 6.9 7 MSR-E2E  10,087 74,686 7.4 3 SMD (Eric and Manning, 2017) 3,031 15,928 5.3 3 Frames (Asri et al., 2017) 1,369 19,986 14.6 3 WOZ (Mrk\u0161i\u0107 et al., 2016) 1,200 5,012 4.2 1 CamRest676  676 2,744 4.1 1 ", "n_publication_ref": 21, "n_figure_ref": 0}, {"heading": "Method", "text": "This section discusses each dataset used in our taskoriented pre-training and how we process the data.\nThen we introduce the selected pre-training base model and its objective functions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets", "text": "We collect nine different task-oriented datasets which are English, human-human and multi-turn. In total, there are 100,707 dialogues, which   dialogue competition.\n\u2022 Schema (Rastogi et al., 2019): Schema-guided dialogue has 22,825 dialogues and provides a challenging testbed for several tasks, in particular, dialogue state tracking. Each schema is a set of tracking slots, and each domain could have multiple possible schemas. This allows a single dialogue system to support many services and facilitates the simple integration of new services without requiring much training data. The Schema dataset is used as the dialogue state tracking task for DSTC8  dialogue competition.\n\u2022 Taskmaster (Byrne et al., 2019): This dataset includes 13,215 dialogues comprising six do-mains, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures.\nOne is a two-person Wizard of Oz approach that one person acts like a robot, and the other is a self-dialogue approach in which crowdsourced workers wrote the entire dialog themselves. It has 22.9 average conversational turns in a single dialogue, which is the longest among all taskoriented datasets listed.\n\u2022 MWOZ (Budzianowski et al., 2018): Multi-Domain Wizard-of-Oz dataset contains 10,420 dialogues over seven domains, and it has multiple domains in a single dialogue. It has a detailed description of the data collection procedure, user goal, system act, and dialogue state labels. Different from most of the existing corpora, it also provides full database information.\n\u2022 MSR-E2E : Microsoft end-toend dialogue challenge has 10,087 dialogues in three domains, movie-ticket booking, restaurant reservation, and taxi booking. It also includes an experiment platform with built-in simulators in each domain.\n\u2022 SMD (Eric and Manning, 2017): Stanford multidomain dialogue is an in-car personal assistant dataset, comprising 3,301 dialogues and three domains: calendar scheduling, weather information retrieval, and point-of-interest navigation.\nIt is designed to smoothly interface with knowledge bases, where a knowledge snippet is attached with each dialogue as a piece of simplified database information.\n\u2022 Frames (Asri et al., 2017): This dataset comprises 1,369 human-human dialogues with an average of 14.6 turns per dialogue, where users are given some constraints to book a trip and assistants who search a database to find appropriate trips. Unlike other datasets, it has labels to keep track of different semantic frames, which is the decision-making behavior of users throughout each dialogue.\n\u2022 WOZ (Mrk\u0161i\u0107 et al., 2016) and Cam-Rest676 (Wen et al., 2016): These two corpora use the same data collection procedure and same ontology from DSTC2 (Henderson et al., 2014). They are one of the first task-oriented dialogue datasets that use Wizard of Oz style with text input instead of speech input, which improves the model's capacity for the semantic understanding instead of its robustness to automatic speech recognition errors.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "TOD-BERT", "text": "We train our TOD-BERT based on BERT architecture using two loss functions: masked language modeling (MLM) loss and response contrastive loss (RCL). Note that the datasets we used can be used to pre-train any existing language model architecture, and here we select BERT because it is the most widely used model in NLP research.\nWe use the BERT-base uncased model, which is a transformer self-attention encoder (Vaswani et al., 2017) with 12 layers and 12 attention heads with its hidden size d B = 768.\nTo capture speaker information and the underlying interaction behavior in dialogue, we add two special tokens, [USR] and [SYS], to the bytepair embeddings (Mrk\u0161i\u0107 et al., 2016). We prefix the special token to each user utterance and system response, and concatenate all the utterances in the same dialogue into one flat sequence, as shown in Figure 1. For example, for a dialogue D = {S 1 , U 1 , . . . , S n , U n }, where n is the number of dialogue turns and each S i or U i contains a sequence of words, the input of the pre-training model is processed as \"[SYS] S 1 [USR] U 1 . . . \" with standard positional embeddings and segmentation embeddings.\nMasked language modeling is a common pretraining strategy for BERT-like architectures, in which a random sample of tokens in the input sequence is selected and replaced with the special token [MASK]. The MLM loss function is the crossentropy loss on predicting the masked tokens. In the original implementation, random masking and replacement are performed once in the beginning and saved for the training duration. Here we conduct token masking dynamically during batch training. TOD-BERT is initialized from BERT, a good starting parameter set, then is further pre-trained on those task-oriented corpora. The MLM loss function is\nL mlm = \u2212 M m=1 log P (x m ),(1)\nwhere M is the total number of masked tokens and P (x m ) is the predicted probability of the token x m over the vocabulary size.\nResponse contrastive loss can also be used for dialogue language modeling since it does not require any additional human annotation. Pretraining with RCL can bring us several advantages: 1) we can learn a better representation for the [CLS] token, as it is essential for all the downstream tasks, and 2) we encourage the model to capture underlying dialogue sequential order, structure information, and response similarity. Unlike the original next sentence prediction (NSP) objective in BERT pre-training, which concatenates two segments A and B to predict whether they are consecutive text with binary classification, we apply a dual-encoder approach (Henderson et al., 2019a) and simulate multiple negative samples. We first draw a batch of dialogues {D 1 , . . . , D b } and split each dialogue at a randomly selected turn t. For example, D 1 will be separated into two segments, one is the context {S 1 1 , U 1 1 , . . . , S 1 t , U 1 t } and the other is the response {S 1 t+1 }. We use TOD-BERT to encode all the contexts and their corresponding responses separately.\nAfterwards, we have a context matrix C \u2208 R b\u00d7d B and a response matrix R \u2208 R b\u00d7d B by taking the output [CLS] representations from the b dialogues. We treat other responses in the same batch as randomly selected negative samples. The RCL objective function is\nL rcl = \u2212 b i=1 log M i,i , M = Softmax(CR T ) \u2208 R b\u00d7b .\n(2)\nIncreasing batch size to a certain amount can obtain better performance on downstream tasks, especially for the response selection. The Softmax function normalizes the vector per row. In our setting, increasing batch size also means changing the positive and negative ratio in the contrastive learning. Batch size is a hyper-parameter that may be limited by hardware. We also try different negative sampling strategies during pre-training such as local sampling (Saeidi et al., 2017), but do not observe significant change compared to random sampling.\nOverall pre-training loss function is the weighted-sum of L mlm and L rcl , and in our experiments, we simply sum them up. We gradually reduce the learning rate without a warm-up period. We train TOD-BERT with AdamW (Loshchilov and Hutter, 2017) optimizer with a dropout ratio of 0.1 on all layers and attention weights. GELU activation functions (Hendrycks and Gimpel, 2016) is used. Models are early-stopped using perplexity scores of a held-out development set, with mini-batches containing 32 sequences of maximum length 512 tokens. Experiments are conducted on two NVIDIA Tesla V100 GPUs.", "n_publication_ref": 8, "n_figure_ref": 1}, {"heading": "Downstream Tasks", "text": "We care the most in this paper whether TOD-BERT, a pre-trained language model using aggregated taskoriented corpora, can show any advantage over BERT. Therefore, we avoid adding too many additional components on top of its architecture when fine-tuning on each downstream task. Also, we use the same architecture with a similar number of parameters for a fair comparison. All the model parameters are updated with a gradient clipping to 1.0 using the same hyper-parameters during finetuning. We select four crucial task-oriented downstream tasks to evaluate: intent recognition, dialogue state tracking, dialogue act prediction, and response selection. All of them are core components in modularized task-oriented systems .\nIntent recognition task is a multi-class classification problem, where we input a sentence U and models predict one single intent class over I possible intents.\nP int = Softmax(W 1 (F (U ))) \u2208 R I , (3\n)\nwhere F is a pre-trained language model and we use its [CLS] embeddings as the output representation. W 1 \u2208 R I\u00d7d B is a trainable linear mapping.\nThe model is trained with cross-entropy loss between the predicted distributions P int and the true intent labels.\nDialogue state tracking can be treated as a multi-class classification problem using a predefined ontology. Unlike intent, we use dialogue history X (a sequence of utterances) as input and a model predicts slot values for each (domain, slot) pair at each dialogue turn. Each corresponding value v j i , the i-th value for the j-th (domain, slot) pair, is passed into a pre-trained model and fixed its representation during training.\nS j i = Sim(G j (F (X)), F (v j i )) \u2208 R 1 , (4\n)\nwhere Sim is the cosine similarity function, and S j \u2208 R |v j | is the probability distribution of the j-th (domain, slot) pair over its possible values. G j is the slot projection layer of the j slot, and the number of layers |G| is equal to the number of (domain, slot) pairs. The model is trained with cross-entropy loss summed over all the pairs. Dialogue act prediction is a multi-label classification problem because a system response may contain multiple dialogue acts, e.g., request and inform at the same time. Model take dialogue history as input and predict a binary result for each possible dialogue act:\nA = Sigmoid(W 2 (F (X))) \u2208 R N ,(5)\nwhere W 2 \u2208 R d B \u00d7N is a trainable linear mapping, N is the number of possible dialogue acts, and each value in A is between [0, 1] after a Sigmoid layer.\nThe model is trained with binary cross-entropy loss and the i-th dialogue act is considered as a triggered dialogue act if A i > 0.5.\nResponse selection is a ranking problem, aiming to retrieve the most relative system response from a candidate pool. We use a dual-encoder strategy (Henderson et al., 2019b) and compute similarity scores between source X and target Y ,\nr i = Sim(F (X), F (Y i )) \u2208 R 1 ,(6)\nwhere Y i is the i-th response candidate and r i is its cosine similarity score. Source X can be truncated, and we limit the context lengths to the most recent 256 tokens in our experiments. We randomly sample several system responses from the corpus as negative samples. Although it may not be a true negative sample, it is common to train a ranker and evaluate its results (Henderson et al., 2019a).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Evaluation Datasets", "text": "We pick up several datasets, OOS, DSTC2, GSIM, and MWOZ, for downstream evaluation. The first three corpora are not included in the pre-trained task-oriented datasets. For MWOZ, to be fair, we do not include its test set dialogues during the pretraining stage. Details of each evaluation dataset are discussed in the following:\n\u2022 OOS (Larson et al., 2019): The out-of-scope intent dataset is one of the largest annotated intent datasets, including 15,100/3,100/5,500 samples for the train, validation, and test sets, respectively. It covers 151 intent classes over ten domains, including 150 in-scope intent and one outof-scope intent. The out-of-scope intent means that a user utterance that does not fall into any of the predefined intents. Each of the intents has 100 training samples.\n\u2022 DSTC2 (Henderson et al., 2014): DSTC2 is a human-machine task-oriented dataset that may include a certain system response noise. It has 1,612/506/1117 dialogues for train, validation, and test sets, respectively. We follow  to map the original dialogue act labels to universal dialogue acts, which results in 9 different system dialogue acts.\n\u2022 GSIM (Shah et al., 2018a): GSIM is a humanrewrote machine-machine task-oriented corpus, including 1500/469/1039 dialogues for the train, validation, and test sets, respectively. We combine its two domains, movie and restaurant domains, into one single corpus. It is collected by Machines Talking To Machines (M2M) (Shah et al., 2018b) approach, a functionality-driven process combining a dialogue self-play step and a crowdsourcing step. We map its dialogue act labels to universal dialogue acts , resulting in 6 different system dialogue acts.\n\u2022 MWOZ (Budzianowski et al., 2018): MWOZ is the most common benchmark for task-oriented dialogues, especially for dialogue state tracking. It has 8420/1000/1000 dialogues for train, validation, and test sets, respectively. Across seven different domains, in total, it has 30 (domain, slot) pairs that need to be tracked in the test set. We use its revised version MWOZ 2.1, which has the same dialogue transcripts but with cleaner state label annotation.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Results", "text": "For each downstream task, we first conduct the experiments using the whole dataset, and then we simulate the few-shot setting to show the strength of our TOD-BERT. We run at least three times with different random seeds for each few-shot experiment to reduce data sampling variance, and we report its mean and standard deviation for these limited data scenarios. We investigate two versions of TOD-BERT; one is TOD-BERT-mlm that only uses MLM loss during pre-training, and the other is TOD-BERT-jnt, which is jointly trained with the MLM and RCL objectives. We compare TOD-BERT with BERT and other baselines, including two other strong pre-training models GPT-2 (Radford et al., 2019) and DialoGPT (Zhang et al., 2019). For a GPT-based model, we use mean pooling of its hidden states as its output representation, which we found it is better than using only the last token.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Linear Probe", "text": "Before fine-tuning each pre-trained models, we first investigate their feature extraction ability by probing their output representations. Probing methods are proposed to determine what information is carried intrinsically by the learned embeddings (Tenney et al., 2019). We probe the output representation using one single-layer perceptron on top of a \"fixed\" pre-trained language model and only finetune that layer for a downstream task with the same hyper-parameters. Table 3 shows the probing results of domain classification on MWOZ, intent identification on OOS, and dialogue act prediction on MWOZ. TOD-BERT-jnt achieves the highest performance in this setting, suggesting its representation contains the most useful information.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Intent Recognition", "text": "TOD-BERT outperforms BERT and other strong baselines in one of the largest intent recognition\nModel Acc (all) Acc (in) Acc (out) Recall (out)\n1-Shot BERT 29.3% \u00b1 3.4% 35.7% \u00b1 4.1% 81.3% \u00b1 0.4% 0.4% \u00b1 0.3% TOD-BERT-mlm 38.9% \u00b1 6.3% 47.4% \u00b1 7.6% 81.6% \u00b1 0.2% 0.5% \u00b1 0.2% TOD-BERT-jnt 42.5% \u00b1 0.1% 52.0% \u00b1 0.1% 81.7% \u00b1 0.1% 0.1% \u00b1 0.1% 10-Shot BERT 75.5% \u00b1 1.1% 88.6% \u00b1 1.1% 84.7% \u00b1 0.3% 16.5% \u00b1 1.7% TOD-BERT-mlm 76.6% \u00b1 0.8% 90.5% \u00b1 1.2% 84.3% \u00b1 0.2% 14.0% \u00b1 1.3% TOD-BERT-jnt 77.3% \u00b1 0.5% 91.0% \u00b1 0.5% 84.5% \u00b1 0.4% 15.3% \u00b1 2.1%   datasets, as shown in Table 2. We evaluate accuracy on all the data, the in-domain intents only, and the out-of-scope intent only. Note that there are two ways to predict out-of-scope intent, one is to treat it as an additional class, and the other is to set a threshold for prediction confidence. Here we report the results of the first setting. TOD-BERTjnt achieves the highest in-scope and out-of-scope accuracy. Besides, we conduct 1-shot and 10-shot experiments by randomly sampling one and ten utterances from each intent class in the training set. TOD-BERT-jnt has 13.2% all-intent accuracy improvement and 16.3% in-domain accuracy improvement compared to BERT in the 1-shot setting.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dialogue State Tracking", "text": "Two evaluation metrics are commonly used in dialogue state tracking task: joint goal accuracy and slot accuracy. The joint goal accuracy compares the predicted dialogue states to the ground truth at each dialogue turn. The ground truth includes slot values for all the possible (domain, slot) pairs. The output is considered as a correct prediction if and only if all the predicted values exactly match its ground truth values. On the other hand, the slot accuracy individually compares each (domain, slot, value) triplet to its ground truth label.\nIn Table 5, we compare BERT to TOD-BERTjnt on the MWOZ 2.1 dataset and find the latter has 2.4% joint goal accuracy improvement. Since the original ontology provided by Budzianowski et al. (2018) is not complete (some labeled values are not included in the ontology), we create a new ontology of all the possible annotated values. We also list several well-known dialogue state trackers as reference, including DSTReader , HyST , TRADE , and ZSDST (Rastogi et al., 2019). We also report the few-shot experiments using 1%, 5%, 10%, and 25% data. Note that 1% of data has around 84 dialogues. TOD-BERT outperforms BERT in all the setting, which further show the strength of task-oriented dialogue pre-training.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Dialogue Act Prediction", "text": "We conduct experiments on three different datasets and report micro-F1 and macro-F1 scores for the dialogue act prediction task, a multi-label classification problem. For the MWOZ dataset, we remove the domain information from the original system dialogue act labels. For example, the \"taxi-inform\" will be simplified to \"inform\". This process reduces the number of possible dialogue acts from 31 to 13. For DSTC2 and GSIM corpora, we follow  to apply universal dialogue act mapping that maps the original dialogue act labels to a general dialogue act format, resulting in 9 and 6 unique system dialogue acts in DSTC2 and GSIM, respectively. We run two other baselines, MLP and RNN, to further show the strengths of BERT-based MWOZ (13) DSTC2 ( 9) GSIM (6) micro-F1 macro-F1 micro-F1 macro-F1 micro-F1 macro-F1 1% Data BERT 84.0% \u00b1 0.6% 66.7% \u00b1 1.7% 77.1% \u00b1 2.1% 25.8% \u00b1 0.8% 67.3% \u00b1 1.4% 26.9% \u00b1 1.0% TOD-BERT-mlm 87.5% \u00b1 0.6% 73.3% \u00b1 1.5% 79.6% \u00b1 1.0% 26.4% \u00b1 0.5% 82.7% \u00b1 0.7% 35.7% \u00b1 0.3% TOD-BERT-jnt 86.9% \u00b1 0.2% 72.4% \u00b1 0.8% 82.9% \u00b1 0.4% 28.0% \u00b1 0.1% 78.4% \u00b1 3.2% 32.9% \u00b1 2.1% 10% Data BERT 89.7% \u00b1 0.2% 78.4% \u00b1 0.3% 88.2% \u00b1 0.7% 34.8% \u00b1 1.3% 98.4% \u00b1 0.3% 45.1% \u00b1 0.2% TOD-BERT-mlm 90.1% \u00b1 0.2% 78.9% \u00b1 0.1% 91.8% \u00b1 1.7% 39.4% \u00b1 1.7% 99.2% \u00b1 0.1% 45.6% \u00b1 0.1% TOD-BERT-jnt 90.2% \u00b1 0.2% 79.6% \u00b1 0.7% 90.6% \u00b1 3.2% 38.8% \u00b1 2.2% 99.3% \u00b1 0.1% 45.7% \u00b1 0.0%  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model Joint Acc", "text": "Slot Acc 1% Data BERT 6.4% \u00b1 1.4% 84.4% \u00b1 1.0% TOD-BERT-mlm 9.9% \u00b1 0.6% 86.6% \u00b1 0.5% TOD-BERT-jnt 8.0% \u00b1 1.0% 85.3% \u00b1 0.4% 5% Data BERT 19.6% \u00b1 0.1% 92.0% \u00b1 0.5% TOD-BERT-mlm 28.1% \u00b1 1.6% 93.9% \u00b1 0.1% TOD-BERT-jnt 28.6% \u00b1 1.4% 93.8% \u00b1 0.3% 10% Data BERT 32.9% \u00b1 0.6% 94.7% \u00b1 0.1% TOD-BERT-mlm 39.5% \u00b1 0.7% 95.6% \u00b1 0.1% TOD-BERT-jnt 37.0% \u00b1 0.1% 95.2% \u00b1 0.1% 25% Data BERT 40.8% \u00b1 1.0% 95.8% \u00b1 0.1% TOD-BERT-mlm 44.0% \u00b1 0.4% 96.4% \u00b1 0.1% TOD-BERT-jnt 44.3% \u00b1 0.3% 96.3% \u00b1 0.2%  models. The MLP model simply takes bag-of-word embeddings to make dialogue act prediction, and the RNN model is a bi-directional GRU network.\nIn Table 4, one can observe that in full data scenario, TOD-BERT consistently works better than BERT and other baselines, no matter which datasets or which evaluation metrics. In the fewshot experiments, TOD-BERT-mlm outperforms BERT by 3.5% micro-F1 and 6.6% macro-F1 on MWOZ corpus in the 1% data scenario. We also found that 10% of training data can achieve good performance that is close to full data training. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Response Selection", "text": "To evaluate response selection in task-oriented dialogues, we follow the k-to-100 accuracy, which is becoming a research community standard (Yang et al., 2018;Henderson et al., 2019a). The k-of-100 MWOZ DSTC2 GSIM 1-to-100 3-to-100 1-to-100 3-to-100 1-to-100 3-to-100 1% Data BERT 7.8% \u00b1 2.0% 20.5% \u00b1 4.4% 3.7% \u00b1 0.6% 9.6% \u00b1 1.3% 4.0% \u00b1 0.4% 10.3% \u00b1 1.1% TOD-BERT-mlm 13.0% \u00b1 1.1% 34.6% \u00b1 0.4% 12.5% \u00b1 6.7% 24.9% \u00b1 10.7% 7.2% \u00b1 4.0% 15.4% \u00b1 8.0% TOD-BERT-jnt --37.5% \u00b1 0.6% 55.9% \u00b1 0.4% 12.5% \u00b1 0.9% 26.8% \u00b1 0.8% 10% Data BERT 20.9% \u00b1 2.6% 45.4% \u00b1 3.8% 8.9% \u00b1 2.3% 21.4% \u00b1 3.1% 9.8% \u00b1 0.1% 24.4% \u00b1 1.2% TOD-BERT-mlm 22.3% \u00b1 3.2% 48.7% \u00b1 4.0% 19.0% \u00b1 16.3% 33.8% \u00b1 20.4% 11.2% \u00b1 2.5% 26.0% \u00b1 2.7% TOD-BERT-jnt --49.7% \u00b1 0.3% 66.6% \u00b1 0.1% 23.0% \u00b1 1.0% 42.6% \u00b1 1.0% Table 6: Response selection evaluation results on three corpora for 1%, 10% and full data setting. We report 1-to-100 and 3-to-100 accuracy, which is similar to recall1 and recall@3 given 100 candidates.\nmetric is computed using a random batch of 100 examples so that responses from other examples in the same batch can be used as random negative candidates. This allows us to be compute the metric across many examples in batches efficiently. While it is not guaranteed that the random negatives will indeed be \"true\" negatives, the 1-of-100 metric still provides a useful evaluation signal. During inference, we run five different random seeds to sample batches and report the average results.\nIn Table 6, we conduct response selection experiments on three datasets, MWOZ, DSTC2, and GSIM. TOD-BERT-jnt achieves 65.8% 1-to-100 accuracy and 87.0% 3-to-100 accuracy on MWOZ, which surpasses BERT by 18.3% and 11.5%, respectively. The similar results are also consistently observed in DSTC2 and GSIM datasets, and the advantage of the TOD-BERT-jnt is more evident in the few-shot scenario. We do not report TOD-BERT-jnt for MWOZ few-shot setting because it is not fair to compare them with others as the full MWOZ training set is used for response contrastive learning during pre-training stage. The response selection results are sensitive to the training batch size since the larger the batch size the harder the prediction. In our experiments, we set batch size equals to 25 for all the models.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Visualization", "text": "In Figure 2, we visualize the embeddings of BERT, TOD-BERT-mlm, and TOD-BERT-jnt given the same input from the MWOZ test set. Each sample point is a system response representation, which is passed through a pre-trained model and reduced its high-dimension features to a two-dimension point using the t-distributed stochastic neighbor embedding (tSNE) for dimension reduction. Since we know the true domain and dialogue act labels for each utterance, we use different colors to represent different domains and dialogue acts. As one can observe, TOD-BERT-jnt has more clear group boundaries than TOD-BERT-mlm, and two of them are better than BERT.\nTo analyze the results quantitatively, we run Kmeans, a common unsupervised clustering algorithms, on top of the output embeddings of BERT and TOD-BERT. We set K for K-means equal to 10 and 20. After the clustering, we can assign each utterance in the MWOZ test set to a predicted class. We then compute the normalized mutual information (NMI) between the clustering result and the actual domain label for each utterance.\nHere is what we observe: TOD-BERT consistently achieves higher NMI scores than BERT. For K=10, TOD-BERT has a 0.143 NMI score, and BERT only has 0.094. For K=20, TOD-BERT achieves a 0.213 NMI score, while BERT has 0.109.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "We propose task-oriented dialogue BERT (TOD-BERT) trained on nine human-human and multiturn task-oriented datasets across over 60 domains. TOD-BERT outperforms BERT on four dialogue downstream tasks, including intention classification, dialogue state tracking, dialogue act prediction, and response selection. It also has a clear advantage in the few-shot experiments when only limited labeled data is available. TOD-BERT is easy-to-deploy and will be open-sourced, allowing the NLP research community to apply or fine-tune any task-oriented conversational problem. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "", "text": "A Appendices  ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "", "journal": "", "year": "", "authors": "Layla El Asri; Hannes Schulz; Shikhar Sharma; Jeremie Zumer"}, {"title": "Frames: A corpus for adding memory to goal-oriented dialogue systems", "journal": "", "year": "2017", "authors": "Kaheer Mehrotra;  Suleman"}, {"title": "Plato: Pre-trained dialogue generation model with discrete latent variable", "journal": "", "year": "2019", "authors": "Siqi Bao; Huang He; Fan Wang; Hua Wu"}, {"title": "Hello, it's gpt-2-how can i help you? towards the use of pretrained language models for task-oriented dialogue systems", "journal": "", "year": "2019", "authors": "Pawe\u0142 Budzianowski; Ivan Vuli\u0107"}, {"title": "Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling", "journal": "", "year": "2018", "authors": "Pawe\u0142 Budzianowski; Tsung-Hsien Wen; Bo-Hsiang Tseng; Inigo Casanueva; Stefan Ultes; Milica Osman Ramadan;  Ga\u0161i\u0107"}, {"title": "Taskmaster-1: Toward a realistic and diverse dialog dataset", "journal": "", "year": "2019", "authors": "Bill Byrne; Karthik Krishnamoorthi; Chinnadhurai Sankar; Arvind Neelakantan; Daniel Duckworth; Semih Yavuz; Ben Goodrich; Amit Dubey; Andy Cedilnik; Kyu-Young Kim"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Unified language model pre-training for natural language understanding and generation", "journal": "", "year": "2019", "authors": "Li Dong; Nan Yang; Wenhui Wang; Furu Wei; Xiaodong Liu; Yu Wang; Jianfeng Gao; Ming Zhou; Hsiao-Wuen Hon"}, {"title": "Keyvalue retrieval networks for task-oriented dialogue", "journal": "", "year": "2017", "authors": "Mihail Eric; D Christopher;  Manning"}, {"title": "Dialog state tracking: A neural reading comprehension approach", "journal": "", "year": "2019", "authors": "Shuyang Gao; Abhishek Sethi; Sanchit Aggarwal; Tagyoung Chung; Dilek Hakkani-Tur"}, {"title": "Hyst: A hybrid approach for flexible and accurate dialogue state tracking", "journal": "", "year": "2019", "authors": "Rahul Goel; Shachi Paul; Dilek Hakkani-T\u00fcr"}, {"title": "Convert: Efficient and accurate conversational representations from transformers", "journal": "", "year": "2019", "authors": "Matthew Henderson; I\u00f1igo Casanueva; Nikola Mrk\u0161i\u0107; Pei-Hao Su; Ivan Vuli\u0107"}, {"title": "The second dialog state tracking challenge", "journal": "", "year": "2014", "authors": "Matthew Henderson; Blaise Thomson; Jason D Williams"}, {"title": "Training neural response selection for task-oriented dialogue systems", "journal": "", "year": "2019", "authors": "Matthew Henderson; Ivan Vuli\u0107; Daniela Gerz; I\u00f1igo Casanueva; Pawe\u0142 Budzianowski; Sam Coope; Georgios Spithourakis; Tsung-Hsien Wen; Nikola Mrk\u0161i\u0107; Pei-Hao Su"}, {"title": "Gaussian error linear units (gelus)", "journal": "", "year": "2016", "authors": "Dan Hendrycks; Kevin Gimpel"}, {"title": "Ctrl: A conditional transformer language model for controllable generation", "journal": "", "year": "2019", "authors": "Bryan Nitish Shirish Keskar;  Mccann; R Lav; Caiming Varshney; Richard Xiong;  Socher"}, {"title": "Xiaoxue Zang, Srinivas Sunkara, and Raghav Gupta. 2019. The eighth dialog system technology challenge", "journal": "", "year": "", "authors": "Seokhwan Kim; Michel Galley; Chulaka Gunasekara; Adam Atkinson Sungjin Lee; Baolin Peng; Hannes Schulz; Jianfeng Gao; Jinchao Li; Mahmoud Adada; Minlie Huang; Luis Lastras; Jonathan K Kummerfeld; Walter S Lasecki; Chiori Hori; Anoop Cherian; Tim K Marks; Abhinav Rastogi"}, {"title": "An evaluation dataset for intent classification and out-of-scope prediction", "journal": "", "year": "2019", "authors": "Stefan Larson; Anish Mahendran; J Joseph; Christopher Peper; Andrew Clarke; Parker Lee; Jonathan K Hill; Kevin Kummerfeld;  Leach; A Michael; Lingjia Laurenzano;  Tang"}, {"title": "Multi-domain task-completion dialog challenge", "journal": "", "year": "2019", "authors": "Sungjin Lee; Hannes Schulz; Adam Atkinson; Jianfeng Gao; Kaheer Suleman; Layla El Asri; Mahmoud Adada; Minlie Huang; Shikhar Sharma; Wendy Tay; Xiujun Li"}, {"title": "Microsoft dialogue challenge: Building end-to-end task-completion dialogue systems", "journal": "", "year": "2018", "authors": "Xiujun Li; Sarah Panda; ) Jj (jingjing; Jianfeng Liu;  Gao"}, {"title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "", "journal": "", "year": "2017", "authors": "Ilya Loshchilov; Frank Hutter"}, {"title": "Neural belief tracker: Data-driven dialogue state tracking", "journal": "", "year": "2016", "authors": "Nikola Mrk\u0161i\u0107; O Diarmuid; Tsung-Hsien S\u00e9aghdha; Blaise Wen; Steve Thomson;  Young"}, {"title": "Towards universal dialogue act tagging for task-oriented dialogues", "journal": "", "year": "2019", "authors": "Shachi Paul; Rahul Goel; Dilek Hakkani-T\u00fcr"}, {"title": "Few-shot natural language generation for task-oriented dialog", "journal": "", "year": "2020", "authors": "Baolin Peng; Chenguang Zhu; Chunyuan Li; Xiujun Li; Jinchao Li; Michael Zeng; Jianfeng Gao"}, {"title": "Improving language understanding by generative pre-training", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan"}, {"title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "", "year": "2019", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "journal": "", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"title": "Towards empathetic opendomain conversation models: A new benchmark and dataset", "journal": "", "year": "2018", "authors": "Eric Michael Hannah Rashkin; Margaret Smith; Y-Lan Li;  Boureau"}, {"title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset", "journal": "", "year": "2019", "authors": "Abhinav Rastogi; Xiaoxue Zang; Srinivas Sunkara; Raghav Gupta; Pranav Khaitan"}, {"title": "The effect of negative sampling strategy on capturing semantic similarity in document embeddings", "journal": "", "year": "2017", "authors": "Marzieh Saeidi; Ritwik Kulkarni; Theodosia Togia; Michele Sama"}, {"title": "Bootstrapping a neural conversational agent with dialogue self-play, crowdsourcing and on-line reinforcement learning", "journal": "", "year": "2018", "authors": "Pararth Shah; Dilek Hakkani-Tur; Bing Liu; Gokhan Tur"}, {"title": "Building a conversational agent overnight with dialogue self-play", "journal": "", "year": "2018", "authors": "Pararth Shah; Dilek Hakkani-T\u00fcr; Gokhan T\u00fcr; Abhinav Rastogi; Ankur Bapna; Neha Nayak; Larry Heck"}, {"title": "What do you learn from context? probing for sentence structure in contextualized word representations", "journal": "", "year": "2019", "authors": "Ian Tenney; Patrick Xia; Berlin Chen; Alex Wang; Adam Poliak; Thomas Mccoy; Najoung Kim; Benjamin Van Durme; R Samuel; Dipanjan Bowman;  Das"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"title": "A networkbased end-to-end trainable task-oriented dialogue system", "journal": "", "year": "2016", "authors": "David Tsung-Hsien Wen; Nikola Vandyke; Milica Mrksic; Lina M Gasic; Pei-Hao Rojas-Barahona; Stefan Su; Steve Ultes;  Young"}, {"title": "Transfertransfo: A transfer learning approach for neural network based conversational agents", "journal": "", "year": "2019", "authors": "Thomas Wolf; Victor Sanh; Julien Chaumond; Clement Delangue"}, {"title": "Transferable multi-domain state generator for task-oriented dialogue systems", "journal": "", "year": "2019", "authors": "Chien-Sheng Wu; Andrea Madotto; Ehsan Hosseini-Asl; Caiming Xiong; Richard Socher; Pascale Fung"}, {"title": "Learning semantic textual similarity from conversations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yinfei Yang; Steve Yuan; Daniel Cer; Sheng-Yi Kong; Noah Constant; Petr Pilar; Heming Ge; Yun-Hsuan Sung; Brian Strope; Ray Kurzweil"}, {"title": "Dialogpt: Large-scale generative pre-training for conversational response generation", "journal": "", "year": "2019", "authors": "Yizhe Zhang; Siqi Sun; Michel Galley; Yen-Chun Chen; Chris Brockett; Xiang Gao; Jianfeng Gao; Jingjing Liu; Bill Dolan"}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "journal": "", "year": "2015", "authors": "Yukun Zhu; Ryan Kiros; Rich Zemel; Ruslan Salakhutdinov; Raquel Urtasun; Antonio Torralba; Sanja Fidler"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Dialogue pre-training based on Transformer encoder with user and system special tokens. Two objective functions are used: masked language modeling and response contrastive learning.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: The tSNE visualization of BERT, TOD-BERT-mlm and TOD-BERT-jnt representations of system responses in the MWOZ test set. Different colors in the left-hand column mean different domains, and in the right-hand column represent different dialogue acts.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: The tSNE visualization of BERT and TOD-BERT representations of system responses in MWOZ test set. Different colors mean different dialogue slots.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Data statistics for task-oriented dialogue datasets.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Intent recognition results on the OOS dataset, one of the largest intent corpus. Models with * are reported fromLarson et al. (2019).", "figure_data": "DomainIntentDialogue Act(acc)(acc)(F1-micro)GPT2 63.5%74.7%85.7%DialoGPT 63.0%65.7%84.2%BERT 60.5%71.1%85.3%TOD-BERT-mlm 63.9%70.7%83.5%TOD-BERT-jnt 68.7% 77.8%86.2%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Dialogue act prediction results on three different datasets. The numbers reported are the micro and macro F1 scores, and each dataset has different numbers of dialogue acts.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Dialogue state tracking results on MWOZ 2.1. We report joint goal accuracy and slot accuracy for the full data setting and the simulated few-shot settings.", "figure_data": ""}], "doi": "10.3115/v1/W14-4337"}