Specializing Multilingual Language Models : An Empirical Study
Pretrained multilingual language models have become a common tool in transferring NLP capabilities to low - resource languages , often with adaptations .
In this work , we study the performance , extensibility , and interaction of two such adaptations : vocabulary augmentation and script transliteration .
Our evaluations on part - of - speech tagging , universal dependency parsing , and named entity recognition in nine diverse low - resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low - resource settings .
Research in natural language processing is increasingly carried out in languages beyond English .
This includes high - resource languages with abundant data , as well as low - resource languages , for which labeled ( and unlabeled ) data is scarce .
In fact , many of the world 's languages fall into the latter category , even some with a high number of speakers .
This presents unique challenges compared to high - resource languages : effectively modeling low - resource languages involves both accurately tokenizing text in such languages and maximally leveraging the limited available data . 
One common approach to low - resource NLP is the multilingual paradigm , in which methods that have shown success in English are applied to the union of many languages ' data , 1 enabling transfer between languages .
For instance , multilingual contextual word representations ( CWRs ) from language models ( Devlin et al , 2019 ; Huang et al , 2019 ; Lample and Conneau , 2019 , inter alia ) are conventionally " pretrained " on large multilingual 1 Within the multilingual paradigm , a distinction is sometimes made between massively multilingual methods , which consider tens or hundreds of languages ; and polyglot methods , which use only a handful .
In this paper , all mentions of " multilingual " refer to the former .
corpora before being " finetuned " directly on supervised tasks ; this pretraining - finetuning approach is derived from analogous monolingual models ( Devlin et al , 2019 ; Liu et al , 2019 ; .
However , considering the diversity of the world 's languages and the great data imbalance among them , it is natural to question whether the current multilingual paradigm can be improved upon for low - resource languages . 
Indeed , past work has demonstrated that it can .
For instance , Wu and Dredze ( 2020 ) find that multilingual models often lag behind non - contextualized baselines for the lowest - resource languages in their training data , drawing into question their utility in such settings .
Conneau et al ( 2020a ) posit that this phenomenon is a result of limited model capacity , which proves to be a bottleneck for sufficient transfer to low - resource languages .
In fact , with multilingual models only being pretrained on a limited set of languages , most of the world 's languages are unseen by the model .
For such languages , the performance of such models is even worse ( Chau et al , 2020 ) , due in part to the diversity of scripts across the world 's languages ( Muller et al , 2021 ; Pfeiffer et al , 2021b ; Rust et al , 2021 ) as compared to the models ' Latin - centricity ( Ács , 2019 ) . 
Nonetheless , there have been multiple attempts to remedy this discrepancy by specializing 2 a multilingual model to a given target low - resource language , from which we take inspiration .
Among them , Chau et al ( 2020 ) augment the model 's vocabulary to more effectively tokenize text , then pretrain on a small amount of data in the target language ; they report significant performance improvements on a small set of low - resource languages .
In a similar vein , Muller et al ( 2021 ) propose to transliterate text in the target language to Latin script to be better tokenized by the existing model , followed by additional pretraining ; they observe mixed results and note that transliteration quality may be a confounding factor .
We hypothesize that these two methods can serve as the basis for improvements in modeling a broad set of low - resource languages . 
In this work , we study the effectiveness , extensibility , and interaction of these two approaches to specialization : the vocabulary augmentation technique of Chau et al ( 2020 ) and the script transliteration method of Muller et al ( 2021 ) .
We verify the performance of vocabulary augmentation on three tasks in a diverse set of nine low - resource languages across three different scripts , especially on non - Latin scripts ( 2 ) and find that these gains are associated with improved vocabulary coverage of the target language .
We further observe a negative interaction between vocabulary augmentation and transliteration in light of a broader framework for specializing multilingual models , while noting that vocabulary augmentation offers an appealing balance of performance and cost ( 3 ) .
Overall , our results highlight several possible directions for future study in the low - resource setting .
Our code , data , and hyperparameters are publicly available .
3
We begin by revisiting the Vocabulary Augmentation method of Chau et al ( 2020 ) , which we recast more generally in light of recent work ( 2.1 ) .
We evaluate their claims on three different tasks , using a diverse set of languages in multiple scripts ( 2.2 ) , and find that the results hold to an even more pronounced degree in unseen low - resource languages with non - Latin scripts ( 2.3 ) .
Following Chau et al ( 2020 ) , we consider how to apply the pretrained multilingual BERT model ( MBERT ; Devlin et al , 2019 ) to a target lowresource language , for which both labeled and unlabeled data is scarce .
This model has produced strong CWRs for many languages ( Kondratyuk and Straka , 2019 , inter alia ) and has been the starting model for many studies on low - resource languages ( Muller et al , 2021 ; Pfeiffer et al , 2020 ; .
MBERT covers the languages with the 104 largest Wikipedias , and it uses this data to con - struct a wordpiece vocabulary ( Wu et al , 2016 ) and train its transformer - based architecture ( Vaswani et
al , 2017 )
.
Although low - resource languages are slightly oversampled , high - resource languages still dominate both the final pretraining data and the vocabulary ( Ács , 2019 ; Devlin et al , 2019 ) .
Chau et al ( 2020 ) note that target low - resource languages fall into three categories with respect to MBERT 's pretraining data : the lowest - resource languages in the data ( Type 1 ) , completely unseen low - resource languages ( Type 2 ) , and low - resouce languages with more representation ( Type 0 ) .
4 Due to their poor representation in the vocabulary , Type 1 and Type 2 languages achieve suboptimal tokenization and higher rates of the " unknown " wordpiece 5 when using MBERT out of the box .
This hinders the model 's ability to capture meaningful patterns in the data , resulting in reduced data efficiency and degraded performance . 
We note that this challenge is exacerbated when modeling languages written in non - Latin scripts .
MBERT 's vocabulary is heavily Latin - centric ( Ács , 2019 ; Muller et al , 2021 ) , resulting in a significantly larger portion of non - Latin scripts being represented with " unknown " tokens ( Pfeiffer et al , 2021b ) and further limiting the model 's ability to generalize .
In effect , MBERT 's low initial performance on such languages can be attributed to its inability to represent the script itself . 
To alleviate the problem of poor tokenization , Chau et al ( 2020 ) propose to specialize MBERT using Vocabulary Augmentation ( VA ) .
Given unlabeled data in the target language , they train a new wordpiece vocabulary on the data , then select the 99 most common wordpieces in the new vocabulary that replace " unknown " tokens under the original vocabulary .
They then add these 99 wordpieces to the original vocabulary and continue pretraining MBERT on the unlabeled data for additional steps .
They further describe a tiered variant ( TVA ) , in which a larger learning rate is used for the embeddings of these 99 new wordpieces .
VA yields strong gains over unadapted multilingual language models on dependency parsing in four low - resource languages with Latin scripts .
How - ever , no evaluation has been performed on other tasks or on languages with non - Latin scripts , which raises our first research question : RQ1 : Do the conclusions of Chau et al ( 2020 ) hold for other tasks and for languages with non - Latin scripts ? 
We can view VA and TVA as an instantation of a more general framework of vocabulary augmentation , shared by other approaches to using MBERT in low - resource settings .
Given a new vocabulary V , number of wordpieces n , and learning rate multiplier a , the n most common wordpieces in V are added to the original vocabulary .
Additional pretraining is then performed , with the embeddings of the n wordpieces taking on a learning rate a times greater than the overall learning rate .
For VA , we set n = 99 and a = 1 , while we treat a as a hyperparameter for TVA .
The related E - MBERT method of sets
n = | V | and a = 1 .
Investigating various other instantiations of this framework is an interesting research direction , though it is out of the scope of this work .
We expand on the dependency parsing evaluations of Chau et al ( 2020 ) by additionally considering named entity recognition and part - of - speech tagging .
We follow Kondratyuk and Straka ( 2019 ) and compute the CWR for each token as a weighted sum of the activations at each MBERT layer .
For dependency parsing , we follow the setup of Chau et al ( 2020 ) and Muller et al ( 2021 ) and use the CWRs as input to the graph - based dependency parser of Dozat and Manning ( 2017 ) .
For named entity recognition , the CWRs are used as input to a CRF layer , while part - of - speech tagging uses a linear projection atop the representations .
In all cases , the underlying CWRs are finetuned during downstream task training , and we do not add an additional encoder layer above the transformer outputs .
We train models on five different random seeds and report average scores and standard errors .
We select a set of nine typologically diverse lowresource languages for evaluation , including three of the original four used by Chau et al ( 2020 ) .
These languages use three different scripts and are chosen based on the availability of labeled datasets and their exemplification of the three language types identified by Chau et al ( 2020 ) .
Of the lan - guages seen by MBERT , all selected Type 0 languages are within the 45 largest Wikipedias , while the remaining Type 1 languages are within the top 100 .
The Type 2 languages , which are excluded from MBERT , are all outside of the top 150 .
6 Additional information about the evaluation languages is given in Tab .
1 . Unlabeled Datasets Following Chau et al ( 2020 ) , we use articles from Wikipedia as unlabeled data for additional pretraining in order to reflect the original pretraining data .
We downsample full articles from the largest Wikipedias to be on the order of millions of tokens in order to simulate a low - resource unlabeled setting , and we remove sentences that appear in the labeled validation or test sets . 
Labeled Datasets For dependency parsing and part - of - speech tagging , we use datasets and train / test splits from Universal Dependencies ( Nivre et al , 2020 ) , version 2.5 ( Zeman et al , 2019 ) .
POS tagging uses language - specific partof - speech tags ( XPOS ) to evaluate understanding of language - specific syntactic phenomena .
The Belarusian treebank lacks XPOS tags for certain examples , so we use universal part - of - speech tags instead .
Dependency parsers are trained with gold word segmentation and no part - of - speech features .
Experiments with named entity recognition use the WikiAnn dataset ( Pan et al , 2017 ) , following past work ( Muller et al , 2021 ; Pfeiffer et al , 2020 ;
Wu and Dredze , 2020 ) .
Specifically , we use the balanced train / test splits of ( Rahimi et al , 2019 ) .
We note that UD datasets were unavailable for Meadow Mari , and partitioned WikiAnn datasets were missing for Wolof .
To measure the effectiveness of VA , we benchmark it against unadapted MBERT , as well as directly pretraining MBERT on the unlabeled data without modifying the vocabulary ( Chau et al , 2020 ; Muller et al , 2021 ; Pfeiffer et al , 2020 ) .
Following Chau et al ( 2020 ) , we refer to the latter approach as language - adaptive pretraining ( LAPT ) .
We also evaluate two monolingual baselines that are trained on our unlabeled data : fastText embeddings ( FASTT ; Bojanowski et al , 2017 ) , which represent a static word vector approach ; and a BERT model trained from scratch ( BERT ) .
For ( Liu et al , 2019 ) with a language - specific SentencePiece tokenizer ( Kudo and Richardson , 2018 ) .
For a fair comparison to VA , we use the same task - specific architectures and modify only the input representations .
To pretrain LAPT and VA models , we use the code of Chau et al ( 2020 ) , who modify the pretraining code of Devlin et al ( 2019 ) to only use the masked language modeling ( MLM ) loss .
To generate VA vocabularies , we train a new vocabulary of size 5000 and select the 99 wordpieces that replace the most unknown tokens .
We train with a fixed linear warmup of 1000 steps .
To pretrain BERT models , we use the HuggingFace Transformers library ( Wolf et al , 2020 ) .
Following Muller et al ( 2021 ) , we train a half - sized RoBERTa model with six layers and 12 attention heads .
We use a byte - pair vocabulary of size 52000 and a linear warmup of 1 epoch .
For LAPT , VA , and BERT , we train for up to 20 epochs total , selecting the highest - performing epoch based on validation masked language modeling loss .
FASTT models are trained with the skipgram model for five epochs , with the default hyperparameters of Bojanowski et al ( 2017 ) . 
Training of downstream parsers and taggers follows Chau et al ( 2020 ) and Kondratyuk and Straka ( 2019 ) , with an inverse square - root learning rate decay and linear warmup , and layer - wise gradual unfreezing and discriminative finetuning .
Models are trained with AllenNLP , version 2.1.0 , for up to 200 epochs with early stopping based on validation performance .
We choose batch sizes to be the maximum that allows for successful training on one GPU .
Tab . 2 presents performance of the different input representations on POS tagging , dependency parsing , and named entity recognition .
VA achieves strong results across all languages and tasks and is the top performer in the majority of them , suggesting that augmenting the vocabulary addresses MBERT 's limited vocabulary coverage of the target language and is beneficial during continued pretraining . 
The relative gains that VA provides appear to correlate not only with language type , as in the findings of Chau et al ( 2020 ) , but also with each language 's script .
For instance , in Vietnamese , which is a Type 0 Latin script language , the improvements from VA are marginal at best , reflecting the Latindominated pretraining data of MBERT .
Irish , the Type 1 Latin script language , is only slightly more receptive .
However , Type 0 languages in Cyrillic and Arabic scripts , which are less represented in MBERT 's pretraining data , are more receptive to VA , with VA even outperforming all other methods for Urdu .
This trend is amplified in the Type 2 languages , as the improvements for Maltese and Wolof are small but significant .
However , they are dwarfed in magnitude by those of Uyghur , where VA achieves up to a 57 % relative error reduction over LAPT .
This result corroborates the findings of both Chau et al ( 2020 ) and Muller et al ( 2021 ) and answers RQ1 .
Prior to specialization , MBERT is especially poorly equipped to handle unseen lowresource languages and languages in non - Latin scripts due to its inability to model the script itself .
In such cases , specialization via VA is beneficial , providing MBERT with explicit signal about the target language and script while maintaining its language - agnostic insights .
On the other hand , this also motivates additional investigation into reme - dies for the script imbalance at a larger scale , e.g. , more diverse pretraining data .
We perform further analysis to investigate VA 's patterns of success .
Concretely , we hypothesize that VA significantly improves the tokenizer 's coverage of target languages where it is most successful .
Inspired by Ács ( 2019 ) , Chau et al ( 2020 ) , and Rust et al ( 2021 ) , we quantify tokenizer coverage using the percentage of tokens in the raw text that yield unknown wordpieces when tokenized with a given vocabulary ( " UNK token percentage " ) .
These are tokens whose representations contain at least partial ambiguity due to the inclusion of the unknown wordpiece . 
Tab .
3 presents the UNK token percentage for each dataset using the MBERT vocabulary , averaged over each script and language type .
This vocabulary is used in LAPT and represents the baseline level of vocabulary coverage .
We also include the change in the UNK token percentage between the MBERT and VA vocabularies , which quantifies the coverage improvement .
Both sets of values are juxtaposed against the average change in task - specific performance from LAPT to VA , representing the effect of augmenting the vocabulary on task - specific performance . 
We observe that off - the - shelf MBERT already at - tains relatively high vocabulary coverage for Type 0 and 1 languages , as well as languages written in Latin and Cyrillic scripts .
On the other hand , up to one - fifth of the tokens in Arabic languages and one - sixth of those in Type 2 languages yield an unknown wordpiece .
For these languages , there is great room for increasing tokenizer coverage , and VA indeed addresses this more tangible need .
This aligns with the task - specific performance improvements for each group and helps to explain our results in 2.3 . 
It is notable that VA does not always eliminate the issue of unknown wordpieces , even in languages for which MBERT attains high vocabulary coverage .
This suggests that the remaining unknown wordpieces in these languages are more sparsely distributed ( i.e. , they represent low frequency sequences ) , while the unknown wordpieces in languages with lower vocabulary coverage represent sequences that occur more commonly .
As a result , augmenting the vocabulary in such languages quickly improves coverage while associating these commonly occurring sequences with each other , which benefits the overall tokenization quality . 
We further explore the association between the improvements in vocabulary coverage and taskspecific performance in Fig .
1 .
Although we do not find that languages from the same types or scripts form clear clusters , we nonetheless observe a loose correlation between the two factors in question and see that VA delivers greater performance gains on Type 2 and Arabic - script languages compared to their Type 0/1 and Latin - script counterparts , respectively .
To quantify the strength of this association , we also compute the language - level Spearman correlation between the change in UNK token percentage on the unlabeled dataset 7 from the MBERT to VA vocabulary and the task - specific performance improvements from LAPT to VA .
The resulting ρ - values - 0.29 for NER , 0.56 for POS tagging , and 0.81 for UD parsing - suggest that this set of factors is meaningful for some tasks , though additional and more fine - grained analysis in future work should give a more complete explanation . 
3 Mix - in Specialization : VA and Transliteration We now expand on the observation made in 2.3 regarding the difficulties that MBERT encounters when faced with unseen low - resource languages in non - Latin scripts because of its inability to model the script .
Having observed that VA is beneficial in such cases , we now investigate the interaction between this method and another specialization approach that targets this problem .
Specifically , we consider the transliteration methods of Muller et al ( 2021 ) , in which unseen low - resource languages in non - Latin scripts are transliterated into the Latin script , often using transliteration schemes inspired by the Latin orthographies of languages related to the target language .
They hypothesize that the increased similarity in the languages ' writing systems , combined with MBERT 's overall Latin - centricity , provides increased opportunity for crosslingual transfer . 
We can view transliteration as a inverted form of vocabulary augmentation : instead of adapting the model to the needs of the data , the data is adjusted to meet the assumptions of the model .
Furthermore , the transliteration step is performed prior to pretraining MBERT on additional unlabeled data in the target language , the same stage at which VA is performed .
In both cases , the ultimate goal is identical : improving tokenization and more effectively using available data .
We can thus view transliteration and VA as two instantiations of a more general mix - in paradigm for model specialization , whereby various transformations ( mix - ins ) are applied to the data and/or model prior to performing additional pretraining .
These mix - ins target different components of the experimental pipeline , which naturally raises our second research question : RQ2 : How do the VA and transliteration mix - ins for MBERT compare and interact ?
To test this research question , we apply transliteration and VA in succession and evaluate their compatibility .
Given unlabeled data in the target language , we first transliterate it into Latin script , which decreases but does not fully eliminate the issue of unseen wordpieces .
We then perform VA , generating the vocabulary for augmentation based on the transliterated data . 
We evaluate on Meadow Mari and Uyghur , which are Type 2 languages where transliteration was successfully applied by Muller et al ( 2021 ) .
To transliterate the data , we use the same methods as Muller et al ( 2021 ) :
Meadow Mari uses the transliterate 8 package , while Uyghur uses a linguistically - motivated transliteration scheme 9 aimed at associating Uyghur with Turkish .
We use the same training scheme , model architectures , and baselines as in 2.2 , the only difference being the use of transliterated data .
This includes directly pretraining on the unlabeled data ( LAPT ) , which is comparable to the highest - performing transliteration models of Muller et al ( 2021 ) .
Although our initial investigation of VA in 2 also included non - Type 2 languages of other scripts , we omit them from our investigation based on the finding of Muller et al ( 2021 ) that transliterating higherresource languages into Latin scripts is not beneficial .
Tab . 4 gives the results of our transliteration mix - in experiments .
For the MBERT - based models , both VA and transliteration provide strong improvements over their respective baselines .
Specifically , the improvements from LAPT to VA and LAPT to LAPT with transliteration are most pronounced .
This verifies the independent results of Chau et al ( 2020 ) and Muller et al ( 2021 ) and suggests that in the non - Latin low - resource setting , unadapted additional pretraining is insufficient , but that the mix - in stage between initial and additional pretraining is amenable to performance - improving modifications .
Unsurprisingly , transliteration provides no consistent improvement to the monolingual baselines , since the noisy transliteration process removes information without improving crosslingual alignment .
However , VA and transliteration appear to interact negatively .
Although VA with transliteration i m - proves over plain VA for Uyghur POS tagging and dependency parsing , it still slightly underperforms LAPT with transliteration for the latter .
For the two NER experiments , VA with transliteration lags both methods independently .
One possible explanation is that transliteration into Latin script serves as implicit vocabulary augmentation , with embeddings that have already been updated during the initial pretraining stage ; as a result , the two sources of augmentation conflict .
Alternatively , since the transliteration process merges certain characters that are distinct in the original script , VA may augment the vocabulary with misleading character clusters .
Either way , additional vocabulary augmentation is generally not as useful when combined with transliteration , answering RQ2 . 
Nonetheless , additional investigation into the optimal amount of vocabulary augmentation might yield a configuration that is consistently complementary to transliteration and is an interesting direction for future work .
Furthermore , designing linguistically - informed transliteration schemes like those devised by Muller et al ( 2021 ) for Uyghur requires large amounts of time and domain knowledge .
VA 's fully data - driven nature and relatively comparable performance suggest that it achieves an appealing balance between performance gain and implementation difficulty .
Our work follows a long line of studies investigating the performance of multilingual language models like MBERT in various settings .
The exact source of such models ' crosslingual ability is contested : early studies attributed MBERT 's success to vocabulary overlap between languages ( Cao et al , 2020 ;
Pires et al , 2019 ; Wu and Dredze , 2019 ) but subsequent studies find typological similarity and parameter sharing to be better explanations ( Conneau et al , 2020b ; K et al , 2020 ) .
Nonetheless , past work has consistently highlighted the limitations of multilingual models in the context of low - resource languages .
Conneau et al ( 2020a ) highlight the tension between crosslingual transfer and per - language model capacity , which poses a challenge for low - resource languages that require both .
Indeed , Wu and Dredze ( 2020 ) find that MBERT is unable to outperform baselines in the lowest - resource seen languages .
Our experiments build off these insights , which motivate the development of methods for adapting MBERT to target low - resource languages . 
Adapting Language Models Several prior studies have proposed methods for adapting pretrained models to a downstream task .
The simplest of these is to perform additional pretraining on unlabeled data in the target language ( Chau et al , 2020 ;
Muller et al , 2021 ; Pfeiffer et al , 2020 ) , which in turn builds off similar approaches for domain adaptation ( Gururangan et al , 2020 ; Han and Eisenstein , 2019 ) .
Recent work uses one or more of these additional pretraining stages to specifically train modular adapter layers for specific tasks or languages , with the goal of maintaining a language - agnostic model while improving performance on individual languages ( Pfeiffer et al , 2020 ( Pfeiffer et al , , 2021aVidoni et al , 2020 ) .
However , as Muller et al ( 2021 ) note , the typological diversity of the world 's languages ultimately limits the viability of this approach . 
On the other hand , many adaptation techniques have focused on improving representation of the target language by modifying the model 's vocabulary or tokenization schemes ( Chung et al , 2020 ;
Clark et al , 2021 ; Wang et al , 2021 ) .
This is wellmotivated :
Artetxe et
al ( 2020 ) emphasize representation in the vocabulary as a key factor for effective crosslingual transfer , while Rust et al ( 2021 ) find that MBERT 's tokenization scheme for many languages is subpar .
Pfeiffer et al ( 2021b ) further observe that for languages with unseen scripts , a large proportion of the language is mapped to the generic " unknown " wordpiece , and they propose a matrix factorization - based approach to improve script representation .
Wang et al ( 2020 ) extend MBERT 's vocabulary with an entire new vocabulary in the target language to facilitate zero - shot transfer to low - resource languages from English .
The present study most closely derives from Chau et al ( 2020 ) , who select 99 wordpieces with the greatest amount of coverage to augment MBERT 's vocabulary while preserving the remainder ; and Muller et al ( 2021 ) , who transliterate target language data into Latin script to improve vocabulary coverage .
We deliver new insights on the effectiveness and applicability of these methods .
We explore the interactions between vocabulary augmentation and script transliteration for specializing multilingual contextual word representations in low - resource settings .
We confirm vocabulary augmentation 's effectiveness on multiple languages , scripts , and tasks ; identify the mix - in stage as amenable to specialization ; and observe a negative interaction between vocabulary augmentation and script transliteration .
Our findings highlight several open questions in model specialization and low - resource natural language processing at large , motivating further study in this area . 
Future directions for investigation are manifold .
In particular , our results in this work unify the separate findings of past works , which use MBERT as a case study ; a natural continuation would extend these methods to a broader set of multilingual models , such as mT5 ( Xue et al , 2021 ) and XLM - R ( Conneau et al , 2020a ) , in order to obtain a clearer understanding of the factors behind specialization methods ' patterns of success .
While we intentionally choose a set of small unlabeled datasets to evaluate on a setting applicable to the vast majority of the world 's low - resource languages , we acknowl - edge great variation in the amount of unlabeled data available in different languages .
Continued study on the applicability of these methods to datasets of different sizes is an important future step .
An interesting direction of work is to train multilingual models on data where script respresentation is more balanced , which might also allow for different output scripts for transliteration .
Given that the mix - in stage is an effective opportunity to specialize models to target languages , constructing mix - ins at both the data and model level that are complementary by design has potential to be beneficial .
Finally , future work might shed light on the interaction between different configurations of the adaptations studied here ( e.g. , the number of wordpiece types used in vocabulary augmentation ) .
We thank Jungo Kasai , Phoebe Mulcaire , and members of UW NLP for their helpful comments on preliminary versions of this paper .
We also thank Benjamin Muller for insightful discussions and providing details about transliteration methods and baselines .
Finally , we thank the anonymous reviewers for their helpful remarks .
