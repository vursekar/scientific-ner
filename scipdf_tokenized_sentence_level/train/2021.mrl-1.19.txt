Analysis of Zero - Shot Crosslingual Learning between English and Korean for Named Entity Recognition
This paper presents a English - Korean parallel dataset that collects 381 K news articles where 1 , 400 of them , comprising 10 K sentences , are manually labeled for crosslingual named entity recognition ( NER ) .
The annotation guidelines for the two languages are developed in parallel , that yield the inter - annotator agreement scores of 91 and 88 % for English and Korean respectively , indicating sublime quality annotation in our dataset .
Three types of crosslingual learning approaches , direct model transfer , embedding projection , and annotation projection , are used to develop zero - shot Korean NER models .
Our best model gives the F1 - score of 51 % that is very encouraging , considering the extremely distinct natures of these two languages .
This is pioneering work that explores zero - shot crosslingual learning between English and Korean and provides rich parallel annotation for a core NLP task such as named entity recognition .
Crosslingual representation learning aims to derive embeddings for words ( or sentences ) from multiple languages that can be projected into a shared vector space ( Conneau et al , 2018 ; Schuster et al , 2019b ; Conneau and Lample , 2019 ) .
One important application of crosslingual embeddings has been found for transferring models trained on a high - resource language to a low - resource one ( Lin et al , 2019 ;
Schuster et al , 2019a ; Artetxe and Schwenk , 2019 ) .
The latest multilingual transformer encoders such as BERT ( Devlin et al , 2019 ) and XLM ( Conneau et al , 2020 ) have made it possible to develop robust crosslingual models through zero - shot learning that requires no labeled training data on the target side ( Jebbara and Cimiano , 2019 ; Chidambaram et al , 2019 ; Chi et al , 2020 ) .
However , these approaches tend not to work as well for languages whose words can not be easily aligned .
Our team is motivated to create a rich crosslingual resource between English and Korean , which are largely different in nature as English is known to be rigid - order , morphologically - poor , and head - initial whereas Korean is flexible - order , morphologicallyrich , and head - final ( Choi et al , 1994 ; Han et al , 2002 ; Hong , 2009 ) .
Creation of a high quality parallel dataset to facilitate crosslingual research can reduce the gap between these two languages , and advance NLP techniques in both languages . 
This paper provides a comprehensive analysis of crosslingual zero - shot learning in English and Korean .
We first create a new dataset comprising a large number of parallel sentences and annotate them for named entity recognition ( NER ; Sec . 3 ) .
We then adapt the crosslingual approaches and build NER models in Korean through zeroshot learning ( Sec . 4 ) .
All models are experimented on our dataset and thoroughly compared to evaluate the feasibility of this work ( Sec . 5 ) .
Our results are promising although depicting few challenges in zero - shot learning for English and Korean ( Sec . 6 ) .
The contributions of this work can be summarized as follows : To create a crosslingual dataset that enables to develop robust zero - shot NER models in Korean .
To present a new data selection scheme that can notably improve zero - shot model performance .
To provide a comparative analysis among several crosslingual approaches and establish the initial foundation of this research .
For crosslingual representation alignment , Artetxe et al ( 2016 ) and Smith et al ( 2017 ) suggested orthogonality constraints on the embedding transformation that led to better quality translation .
Aldarmaki and Diab ( 2019 ) derived a context - aware crosslingual mapping from a parallel corpus using word alignment .
Schuster et al ( 2019b ) aligned word embeddings from multilingual transformer encoders using context independent embedding anchors .
Recent works based on multilingual pretrained language model aligns representations between languages in a unsupervised fashion .
Devlin et al ( 2019 ) proposed multilingual BERT that generates contextualized word embeddings for multiple languages in one vector space by simply sharing all languages ' vocabulary .
Conneau and Lample ( 2019 ) extends mBERT by introducing bilingual data and an extra pretraining task ( Translation Language Modeling ) .
Luo et al ( 2021 ) adds a crossattention module into the Transformer encoder to explicitly build the interdependence between langauges . 
For cross - lingual NER , Ni et al ( 2017 ) presented weakly supervised crosslingual models using annotation and representation projection .
Huang et al ( 2019 ) made an empirical analysis of how sequential order and multilingual embeddings are used in crosslingual NER .
Artetxe and Schwenk ( 2019 ) presented multilingual transfer models that used few - shot learning adapting supervising BEA , ranking and retraining for massive transfer .
Wu and Dredze ( 2019 ) and Wu et al ( 2020 ) directly transfers the NER model trained on the source language to the target language using crosslingual representations from multilingual encoders ( Direct model transfer ) . 
3 English - Korean Crosslingual Dataset 3.1 Data Collection AI Open Innovation Hub ( AI Hub ) is an integration platform operated by the Korea National Information Society Agency that provides data , software , and computing resources for AI research .
It has released the Korean - English AI Training Text Corpus ( KEAT ) 1 containing 1.6 M English - Korean parallel sentences from various sources such as news media , government website / journal , law & administration , conversation and etc .
For the present study , 800 K parallel sentences from the news portion of this corpus are extracted .
Since KEAT is not organized into documents , each sentence is composed independently although it comes with the URL of its original source .
Thus , we group all sentences into news articles based on the URLs .
Although there exist news articles with single sentence after the grouping process , we still include them in the train set in order to make full use of the parallel sentences provided , which will be used to train the word alignment model and the transformation matrix in Section 5 .
As a result , 757 , 697 sentences are selected , that are composed into 381 , 173 news articles , to create our English - Korean crosslingual dataset . 
The news articles can be categorized into 9 sections : Business , Lifestyle , Science / Technology , Society , Sports , World , Regional , and Others .
Among those , 200 articles are randomly sampled from each of the first 7 categories for our annotation in Section 3.4 and they are split into 50/50 to create the development and test sets for our experiments in Section 5 .
Table 1 describes the statistics of our dataset .
All sections are uniformly distributed in DEV and TST , enabling to conduct comparative studies among these sections .
Table 3 : The statistics of manually annotated named entities on the parallel sentences in the DEV and TST sets .
The numbers in the parentheses indicate the percentages of the corresponding tags for each set .
EN / KR : # of entities in the English / Korean sentences respectively , E ∩ K : # of entities existing in both English and Korean sentences .
ELIT 2 using the Flair model trained on OntoNotes ( Pradhan et al , 2013 ) .
Korean sentences are tagged by a CRF - based model adapting KoBERT ( Korean BERT ) 3 trained on the corpus distributed by Cheon and Kim ( 2018 ) .
Note that the named entity types pseudo - annotated on the Korean sentences do n't match with those of the English sentences for now , which will be matched in Section 3.4 in the case of DEV and TST .
In addition , Korean sentences are processed by the Mecab morphological analyzer 4 that produces more linguistically sounding tokens than SentencePiece ( Kudo and Richardson , 2018 ) in KoBERT .
All named entities from the CRF tagger are then remapped to the tokens produced by the Mecab analyzer using heuristics so they can better reflect the previous morphology work in Korean ( Hong , 2009 ) .
Words in every parallel sentence pair , tokenized by the ELIT and Mecab analyzers , are aligned by GIZA++ , that has been adapted by many prior crosslingual studies ( Och and Ney , 2003 ) . 
Table 2 shows the statistics of pseudo - annotated named entities in our dataset .
The detailed descriptions of these tags are provided in Appendix A.1 .
The overall statistics are comparable between English and Korean , 2.5 and 2.3 entities per sentence , respectively .
GPE e , the 3rd most frequent tag in English , is not supported by the Korean tagger but rather tagged as ORG k or LOC k , explaining why the numbers of these two tags in Korean are much greater than those of ORG e and LOC e , respectively .
We conduct a team of graduate students majoring in Data Science to manually tag named entities on all parallel sentences in the DEV and TST sets by taking the following 3 steps : 5 1 .
For English , the pseudo - annotated entities are revised by the OntoNotes named entity guidelines ( BBN , 2014 ; Maekawa , 2018 ) , and missing entities are annotated as necessary . 
2 . For Korean , the pseudo - annotated entities are revised to match the English tagset , and missing entities are annotated as necessary . 
3 . Let E = { e 1 , . . . , e n } and K = { k 1 , . . .
, k m } be the lists of entities from Steps 1 and 2 for a English and Korean sentence pair , respectively .
Every entity pair ( e i , k j ) is linked in our dataset if e i is the translation of k j . 
Note that every article in DEV and TST consists of at least 5 sentences with at least 2 named entities . 
Table 3 shows the statistics of the gold annotation .
Out of 22 , 367 and 21 , 892 named entities annotated in English and Korean sentences , 20 , 300 of them are linked across the languages ( above 90 % ) .
To estimate the inter - annotator agreement , 10 news articles from each of the first 7 sections in Table 1 are randomly picked and double annotated ; the rest of DEV and TST are single annotated and sample checked .
Table 4 shows the Cohen 's kappa scores measured for the English and Korean annotation .
The high labeled matching scores of 90.9 and 88.3 are achieved for those two languages respectively , implying that the single annotation in this dataset is expected to be of high quality as well .
A couple of challenges are found during the parallel annotation .
First , subjects are obligatory in English for most sentence forms whereas Korean is a prodrop language so that entities in the subject position can be missing in Korean but not in English , which explains the greater number of entities in English .
Second , certain inflectional morphemes in Korean can be dropped without violating the grammar , that often makes the labeling ambiguous .
For instance , the literal translation of " Korean Church " would be " 한국 ( Korea ) + 의 ( 's )
교회 ( Church ) "
, although it is the standard practice to drop " 의 ( 's ) " in this case such that it becomes " 한국 ( Korea )
교회 ( Church ) " .
Given this translation , the annotator can be easily confused to annotate " 한국 ( Korea ) " as a geopolitical entity ( GPE ) instead of a nationality ( NORP ) , which may lead to annotation disagreement .
Additional analytics by news sections and entity types are described in Appendix A.6 4 Zero - shot Crosslingual Learning
Three crosslingual learning approaches are adapted to develop zero - shot Korean models .
One is direct model transfer method following Wu and Dredze ( 2019 ) .
We reproduce the previous work which finetunes mBERT on English NER dataset and transfers the trained model to a target language , in our case , Korean .
We fine - tune on OntoNotes , whereas the previous work fine - tuned on CoNLL 2003 NER dataset .
The other two approaches that will be experimented are embedding preojection and annotation projection following Ni et al ( 2017 ) , although some modules in the implementation are updated or added : the encoders used to derive the embeddings from sentences , the word alignment tool , the training data selection scheme heuristics .
Figure 1 illustrates an overview of two crosslingual learning approaches adapted to develop zero - shot Korean NER models .
One is embedding projection ( R1 ) that takes a labeled English sentence ( R2 ) and generates English embeddings , ( R3 ) which are fed into an orthogonal mapping ( R4 ) then transformed into Korean embeddings ( Section 4.2 ) .
The other is annotation projection ( A1 ) that aligns words across the two languages and pseudo - annotates the Korean sentence , ( A2 ) which are fed into an encoder ( A3 ) to generate Korean embeddings ( Section 4.3 ) .
The Korean embeddings generated by individual approaches are fed into a trainer to build the Korean NER models .
No manual annotation is added to the Korean data ; thus they both are zero - shot learning .
Let X , Y R n×d be parallel matrices between the source and target languages ,
where n is the number of parallel terms ( words or sentences ) in those two languages .
Let x
i , y
i R 1×d be the i'th rows in X and Y , which are the embeddings of the i'th terms in the source and target languages respectively , that refer to the same content .
Then , the transformation matrix W R d×d can be found by minimizing the distance between XW and Y as follows : argmin W XW − Y s.t .
W T
W
=
I This optimization can be achieved by singular value decomposition as proposed by Artetxe et
al ( 2016 ) , where U , V R d×p , Σ R p×p : W =
U V T s.t . X T Y
= U ΣV T The transformation matrix W is used to convert any English embedding e i into a Korean embedding k i in Figure 1
such that e i
W =
k i
≈ k j where k j is the embedding from the Korean encoder that can be aligned with e i .
The NER model is trained on only English sentences represented by the transformed embeddings k * and a pseudo - label annotated with an existing English NER model .
During decoding , the model takes Korean sentences represented by the encoded embeddings k * and makes the predictions . 
Given the latest contextualized encoders that generate different embeddings for the same word type by contexts ( Peters et al , 2018 ; Devlin et al , 2019 ; Liu et al , 2019 ) , the size of X and Y is as large as the number of all aligned words in the training data .
It is worth saying that the transformed embedding space may be similar to the actual encoded space in the target language ; however , the word order is still preserved as in the source language .
Therefore , the model is limited to learn sequence information of the target language , which can be an issue for languages with very different word orderings .
Let S = { S 1 , . . .
, S n } and T = { T 1 , . . .
, T n } be lists of sentences in the source and target languages , and ( S i , T i ) be the i'th pair of parallel sentences in those two languages .
Let S
i = { s i1 , . . .
, s in } and T i = { t i1 , . . .
, t i m } where s i and t
i are the i'th word in S and T .
Then , annotation projection can be performed as proposed by Ni et al ( 2017 ) : 1 .
Pseudo - label S ∀i S using an existing model in the source language , in our case , ELIT ( 3.2 ) . 
2 . Pseudo - align words in every ( S i , T i ) using an existing tool , in our case , GIZA++ ( 3.2 ) .
If a consecutive word span S j , k i = { s ij , .. , s ik } is pseudo - labeled as the entity type as well as pseudo - aligned with a span T a ,
b i = { t ia , .. , t ib } , T a ,
b i is also pseudo - labeled with . 
The quality of pseudo annotation hugely depends on the performance of word alignment , which is generally not robust for the case of distant language pairs such as English and Korean .
Thus , we propose a few constraints to filter out noisy annotation . 
Entity Matching Let ψ be a boolean .
If ψ = F , all parallel sentences in ( S , T ) are used for training .
If ψ = T , ( S i , T i ) is selected for training only if all named entities in S i are properly labeled in T i by the above projection approach . 
Relative Frequency Let e be an entity term such as " 도널드 트럼프 ( Donald Trump ) " in Figure 1 . Let L e be a set of entity types pseudo - annotated for all occurrences of e in the target language .
Then , the relative frequency P ( | e ) for L e and e can be measured as follows , where COUNT ( , e ) is the number of occurrences for e being labeled as : P ( | e )
= COUNT ( , e ) 
Le COUNT ( , e ) 
Impurity Let F e be a set of unique terms in the source language that are pseudo - aligned with the term e labeled as in the target language such that | F e | ≤ COUNT ( l , e ) .
Then , the impurity M ( , e ) is measured as follows where α is a smoothing factor : M ( , e )
= | F e | COUNT ( l , e )
+
α The relative frequency P ( | e ) and the impurity M ( , e ) are used to assess pseudo - annotation reliability .
E i = { ( 1 , e 1 ) , .. , ( q , e q ) } be a list of all ( entity term , label ) pairs in the target sentence T i .
For each T i T , the following two scores , f ( T i ) and g ( T i ) , are measured to estimate the reliability of the pseudo - annotation in T i : 
f ( T i )
= ∀ ( l , e )
E i P ( | e )
|
E i | g ( T i )
= ∀ ( l , e )
E i M
( | e )
|
E i | Given the annotation reliability metrics , our data selection scheme heuristic is as follows : 
f ( T i ) ≥ φ ; g ( T i ) ≤ γ ; | E i | ≥ µ ; ψ = T | F Only the target sentences satisfying all of the above constraints are used for training given the hyperparameters
ψ , α , φ , γ , and µ.
The experimental settings of direct model transfer approach are identical with Wu and Dredze ( 2019 ) .
We freeze the bottom n layers ( including n ) of mBERT , where layer 0 is the embedding layer . 
The cases of n are { - 1 , 0 , 3 , 6 , 9 } , where - 1 denotes fine - tuning all layers in mBERT .
For word - level classification , a simple linear classification layer with softmax is added on mBERT .
The hyperparameters we experiment on are the combitations of batch size { 16 , 32 } , learning rate { 2e - 5 , 3e - 5 , 5e - 5 } , and number of max epochs { 3 , 4 } .
For embedding projection and annotation projection , two types of transformer encoders , mBERT ( Devlin et al 2019 )
Two types of transformation matrices are derived by the embedding projection method ( Section 4.2 ) . 
One is a word - level matrix and the other is a sentence - level matrix .
To evaluate the zero - shot Korean NER model performance ( Table 6 ) when different size of parallel sentences are available , we use different subsets of sentences of increasing sizes ( 0 , 1 K , 10 K , 100 K , 200 K , 400 K , 747 K ; 0 to total # of sentences in TRN ) .
Size 0 means the embeddings from source language are not transformed when fed into the NER model for training .
Word embeddings from the last hidden layer of each transformer encoder are extracted .
For every parallel sentence pair , let X i and Y
i be lists of word embeddings of the i'th sentence extracted from the last layer in the source and target encoders , respectively .
Only embeddings for words that find alignments are included in X i and Y i .
If multiple words in the source language , s i and s j , are aligned to one word , t k , in the target language ( e.g. , United States 미국 in Figure 1 ) , the embeddings of t k are duplicated and added to Y i and
vice versa s.t . | X * | =
| Y * | .
Let x
ij and y ij be the j'th embeddings in X i and Y
i that are guaranteed to be the embeddings of aligned words ; thus , X i and Y i are completely in parallel . 
For the word - level transformation matrix W w , X i and Y i from all parallel sentences are appended together to create X w i and Y
w i respectively such that X w i W
w ≈ Y
w i .
Sentence embeddings are simply created by averaging the word embeddings of parallel source and target sentences .
Let X i and Y
i be lists of word embeddings of the i'th sentence extracted from the last layer in the source and target encoders .
Note that words in X i and Y
i are not aligned , thus no duplications of word embedding unlike X i and Y i .
For the sentence - level matrix W s , the average em - beddings of X i ,
Y
i are appended to create X s i and Y
s i
such that X s i
W s
≈ Y s
i .
For each sentence in the source language , embeddings from last hidden layer are transformed by W w | s and fed into the NER model for training ( Section 5.5 ) .
The annotation projection is performed to generate the pseudo - annotated Korean dataset ( Section 4.3 ) .
The following 5 hyperparameters are tuned to filter out noisy annotation for training , where ψ , α , and γ are newly introduced by our work : ψ : if True , keep only sentences whose entities are completely matching between the two languages .
α : the smoothing factor to measure the impurity . φ : retain sentences whose annotation reliability scores by relative frequency ≥ this threshold . 
γ : retain sentences whose annotation reliability scores by impurity ≤ this threshold .
 µ : retain sentences that contain named entities whose quantities are ≥ this cutoff . 
Once the pseudo - annotation is created , all Korean sentences are encoded by mBERT to generate Korean embeddings that are fed into the NER model .
For embedding and annotation projections , a bidirectional LSTM - based NER tagger using a CRF decoder is adapted to build our NER models ( Lample et al , 2016 ) .
Details of the hyperparameters are described in Appendix A.3
Table 5 shows the best result of direct model transfer , mBERT fine - tuned on OntoNotes NER dataset and evaluated on our Korean TST set .
All scores are reported in a form of mean ( ± standard deviation ) after three developments .
The best model is built under the setting when all layers including the embedding layer of mBERT are fine - tuned .
6 shows the zero - shot results from the embedding projection models in Section 5.3 .
Both mBERT and XLM - R models showed a performance improvement over 2 % with embedding transformation .
F1 score improves 2.47 % ( 36.67 % to 39.14 % ) and 2.32 % ( 39.36 % to 41.68 % ) for mBERT and XLM - R , respectively .
Both models showed the best performance with embedding transformation matrix made of 200k w .
The number of parallel sentences used for training transformation matrix has a considerable impact on the Zero - shot learning .
Table 7 shows the results from the annotation projection models with various configurations .
About 9 % gain is shown by the best model using only the entity matching constraint ψ that effectively filters out 55 % of the training data ( row 3 ) .
A relative score of 50.25 % is achieved by the model using only 21 % of the training data , implying that a fair amount of noisy annotation is produced by the annotation projection approach . 
The overall results show that the Annotation Projection approach achieves the best performance , implying that considering the word order of the target language is critical in cross - lingual learning , especially in the case of distant language pairs .
We expect further improvement of the annotation projection approach when adapting a more accurate word alignment tool or a data selection scheme , which we will further investigate . 
6 Analysis
Given the results of the best models for embedding and annotation projection approaches ( Section 5.6 ) , a total of 105 parallel sentences ( 15 pairs per news section ) are randomly selected for error analysis .
7 Table 8 shows the distributions of the 5 error types .
error types in both models .
For example , the Korean entity " 10개 ( 10 things ) " comprises the quantity " 10 " and the metric " 개 ( things ) " that is a generic measure word in Korean , whereas in English just write " 10 " .
The grammatical difference between English and Korean , where Korean uses measure words for quantifying the classes of objects while English does not in general , makes it difficult to accurately predict under the zero - shot learning setting .
Wrong Label occurs frequently across all models when dealing with entities referring to nationality .
As mentioned in Section 3.5 , a single word in Korean can entail the meaning of both nationality and country .
This overloaded word - sense characteristic makes entities that actually refer to nationality be mislabeled as GPE , which should have been labeled as NORP .
This paper presents a multilingual dataset that allows researchers to conduct crosslingual research between English and Korean .
Our dataset contains high - quality annotation of named entities on parallel sentences from seven popular news sections .
Given this dataset , Korean NER models are built by zero - shot learning using multilingual encoders .
Our data selection scheme for annotation projection significantly improves the NER performance although it is still suboptimal .
Our error analysis depicts unique characteristics in Korean that make it hard for zero - shot learning , challenges that we need to overcome in the future work .
8
There are 18 named entity tags annotated in the OntoNotes 5.0 as follows ( Pradhan et al , 2013 ) : 9 CARDINAL : Numerical terms not categorized in other categorizations .
Numbers that indicate ages are included . 
DATE : Absolute or relative dates or periods .
The period should last longer than ' TIME ' .
General expressions of dates are included too such as ' few months ' , ' that day ' , ' Next season ' and ' First quarter ' . 
EVENT :
It means an official or widely known event , war , exhibition .
Official events include ministerial meetings , general elections , presidential elections , exams ( SAT ) , and prayers ( U.S. national breakfast prayer ) .
Social phenomena also include ( Brexit ) for widely known events . 
FAC : Objectives referring to facilities include buildings , airports , highways and bridge names . 
GPE : An object referring to a place or location , including the name of a country and the name of an administrative district , such as a city or state . 
LANGUAGE :
Any named language . 
LAW : Named documents made into laws . 
LOC : Refers to the name of a place or location that does not belong to GPE .
It also includes expressions covering the entire location of mountains , rivers , ocean names and Europe , Asia , etc . MONEY :
Monetary values including units . 
NORP :
It refers to nationality , religious groups and political groups ( party ) . 
ORDINAL : All ordinal numbers such as first and second . 
ORG : It refers a community / group of people gathered together .
For example , the name of the company , the name of the school , and the name of the sports team . 
9 https://catalog.ldc.upenn.edu/docs/ LDC2013T19 / OntoNotes - Release - 5.0.pdf PERCENT :
Percentage expressions with % symbol or the word ' percent ' . 
PERSON : Referring to a last name or full name of a particular person .
It also includes nicknames for non - human creatures and characters in cartoons , dramas and movies . 
PRODUCT :
Vehicles , Weapons , foods .
IT services ( including SNS ) and medicine names are included . 
QUANTITY : Measurements as of weights or distances such as km , kg and etc . TIME :
This tag indicates time expressions smaller than a day .
This tag includes certain time indication , amount of time or any other expressions related to time .
Even though an entity does not have numeral expressions but only words related to time ( for instance , ' noon ' ) , the words are tagged as ' Time ' . 
WORK_OF_ART : Titles of books , songs , TV programs and art pieces .
Title of games , awards , theories , records are included .
There are 10 tags annotated in the copus distributed by the Korea Maritime and Ocean University : 10 DAT : Absolute dates .
Public holidays and day of the week is included . 
DUR : Duration of incidents .
Academically clarified periods such as Cretaceous period are also included . 
LOC : The name of a country and the name of an administrative district , such as a city or state .
Words representing certain locations such as tour spot and stadium is also included .
When location word becomes compound nouns with other words , it is not included . 
MNY : Monetary values including units .
Bitcoin is not included . 
NOH : Any numerical expressions such as measurements of heights , temperatures , weights .
Ordinal numbers are included . 
ORG : A group consisting of 2 or more people .
The name of the company , the name of the school , and the name of the sports team . 
PER : Personal name including first and last name .
Any name referring to living things and nicknames for non - human creatures and characters in cartoons , dramas and movies are included . 
PNT : Percentage expressions with % symbol or the word ' percent ' . 
POH : Product name , medicine , game , event , meeting , movies , songs , drama series , TV channels , daily and weekly magazines , emails , phone numbers are included . 
TIM : This tag indicates time expressions smaller than a day .
This tag includes certain time indication , amount of time or any other expressions related to time .
Even though an entity does not have numeral expressions but only words related to time ( for instance , ' noon ' ) , the words are tagged as ' Time ' .
Table 9 shows the performance of the NER model in the ELIT toolkit on the English development and evaluation sets in our dataset .
The task specific NER model used : a 2 - layer Bi - LSTM with a hidden size of 768 followed by a CRF layer .
A dropout rate of 0.5 is applied on the input and the output of the Bi - LSTM .
Adam with default parameters and a learning rate of 0.0001 are used for optimization .
We trained the model for 10 epoch with a batch size of 32 , and evaluate the model per a epoch . 
A.4 Comparison of NER performances ( Zero - shot VS Existing ) 
We compare our best performing Zero - shot Korean NER model with the existing Korean NER model 11 on TST .
Since the number of named entity types that each model covers are different , named entity tags are mapped based on the definition of the tags .
Our named entities are more fine - grained , which makes multiple tags ( Zero - shot side ) be mapped to one tag ( Existing side ) .
Named entity tags that can not be mapped are discarded in both gold labels and predicted labels , thus not considered in the evaluation of the models .
Our zero - shot model yields a better performance than the existing model although it may be difficult to directly compare the two models .
In the case of the existing Korean model , the low performance may be caused by the different annotation scheme between the datasets .
In the case of our zero - shot model , the improvement of the performance are seen due to the coarse - grained named entities after the mapping .
Table 11b describes the proportions of news sections per entity type .
CARDINAL , ORDINAL , and EVENT appear the most in Sports that involves many game events and statistics .
DATE , ORG , and QUANTITY show fairly even proportions in every section as they are elemental to a variety of topics .
GPE , LOC , and NORP give high proportions to both Politics and World .
MONEY and PERCENT appear the most in Business that often deals with monetary issues .
PERSON show high proportions in Sports and Politics as discussed above .
FAC takes good portions in Lifestyle , Business , and World , which often mention facilities that people encounter daily ( e.g. , airports , bridges ) .
TIME appears the most in Sports and Society that are full of dynamic events and issues .
LANGUAGE is mostly found in Society although the sample size is too small to generalize .
LAW , PRODUCT , and WORK_OF_ART appear the most in Politics , Sci / Tech , and Lifestyle , that focus on legal issues , tech products , and entertainment ( e.g. , music , movies , shows ) , respectively .
This work was partly supported by the Institute of Information and Communications Technology Planning and Evaluation ( IITP ) grant funded by the Korean government ( MSIT ) ( No . 2020 - 0 - 01361 , Artificial Intelligence Graduate School Program ( Yonsei University ) ) .
