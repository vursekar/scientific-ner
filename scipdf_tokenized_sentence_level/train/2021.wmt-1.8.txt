Ensembling of Distilled Models from Multi - task Teachers for Constrained Resource Language Pairs
This paper describes our submission to the constrained track of WMT21 shared news translation task .
We focus on the three relatively low resource language pairs Bengali ↔ Hindi , English ↔ Hausa and Xhosa ↔ Zulu .
To overcome the limitation of relatively low parallel data we train a multilingual model using a multitask objective employing both parallel and monolingual data .
In addition , we augment the data using back translation .
We also train a bilingual model incorporating back translation and knowledge distillation then combine the two models using sequence - to - sequence mapping .
We see around 70 % relative gain in BLEU point for En ↔ Ha and around 25 % relative improvements for Bn ↔ Hi and Xh ↔ Zu compared to bilingual baselines .
Neural machine translation ( NMT ) witnessed a lot of success in the past few years especially for high resource languages ( Vaswani et al , 2017 ) .
Improving the quality of low resource languages is still challenging .
Some of the popular techniques are adding high resource helper languages as in multilingual neural machine translation ( MNMT )
( Dong et al , 2015 ; Firat et al , 2016 ;
Ha et
al , 2016 ; Johnson et al , 2017 ; Arivazhagan et al , 2019 ) , using monolingual data including pre - training ( Liu et al , 2020 ) , multi - task learning ( Wang et al , 2020 ) , back translation ( Sennrich et al , 2016 ) or any combination of these methods ( Barrault et al , 2020 ) and system combination of multiple systems ( Liu et al , 2018 ) . 
This paper describes the Microsoft Egypt Development Center ( EgDC ) submission to the WMT21 shared news translation task for three low resource language pairs ( six directions ) , Bengali ↔ Hindi ( Bn ↔ Hi ) , English ↔ Hausa ( En ↔ Ha ) and Xhosa ↔ Zulu ( Xh ↔ Zu ) .
We focus on the constrained track because it is easier to compare different systems and it is always possible to improve performance by adding more data .
The main features of our approach are as follows : Using a recently proposed multitask and multilingual learning framework to benefit from monolingual data in both the source and target languages ( Wang et al , 2020 ) .
Using knowledge distillation ( Freitag et al , 2017 ) to create bilingual baselines from the original multilingual model and combining it with the multilingual model . 
The paper is organized as follows .
Section 2 gives an overview of the data used in the constrained scenario , followed by section 3 that gives a detailed description of our approach .
Section 4 presents our experimental evaluation .
Finally , our findings are summarized in Section 5 .
Following the constrained track , we use bitext data provided in WMT21 for the following pairs : Bengali ↔ Hindi , English ↔ Hausa , Xhosa ↔ Zulu and English ↔ German .
Statistics of the parallel data used for the three pairs in addition to the German helper are shown in Table 1 .
We also use monolingual data for all previously mentioned languages provided in WMT21 for techniques such as multi - task training and back - translation .
Statistics of the monolingual data used for the 6 languages in addition to the German helper are shown in Table 2 .
For very low resource languages , Hausa , Xhosa and Zulu , we use all the available monolingual data , e.g. NewsCrawl + CommonCrawl + Extended CommonCrawl for Hausa , and Extended Common - Crawl for both Xhosa and Zulu .
For relatively high resource languages , Bengali , Hindi , English and German , we only use a subset of the provided data mostly from NewsCrawl due to its high - quality .
In addition to the NewsCrawl monolingual subset , we add a sampled subset from CommonCrawl to
For Bengali , English , Hindi and German , we apply fastText 1 language identification on the monolingual data to remove sentences which are not predicted as the expected language .
We do the same for Hausa , Xhosa and Zulu using Polyglot 2 because fastText does not cover these three languages .
The resulting size of the monolingual data of each language is shown in Table 2 .
The final MT system in each direction is an ensemble of two NMT models comprising a bilingual model ( one for each of the six primary directions ) and a multilingual model trained to provide translations for 8 directions ( the six primary directions plus English ↔ German ) .
The multilingual system uses a recently proposed multitask framework for training ( Wang et al , 2020 ) .
We describe the individual systems in Subsection 3.1 .
This is followed by presenting our system combination techniques in Subsection 3.2 .
Finally we present the architecture of the submitted system highlighting our design decisions in Subsection 3.3 .
This subsection describes the individual systems and their training leading to the proposed system combination strategy in the following subsection .
We first build bilingual models for the six primary directions using the data shown in Table 1 except the English ↔ German .
These serve as baselines to compare to the developed systems .
The models use a transformer base architecture comprising 6 encoder and 6 decoder layers and a 24 K joint vocabulary built for Bengali ↔ Hindi , a 8 K joint vocabulary built for English ↔ Hausa and a 4 K joint vocabulary built for Xhosa ↔ Zulu using sentencepiece ( Kudo and Richardson , 2018 ) to learn these subword units to tokenize the sentences .
In addition to the baseline bilingual models , we use knowledge distilled ( KD ) data and back - translated ( BT ) data generated from a multilingual model to build another set of bilingual models for each of the six primary directions .
This multilingual model is described below .
The purpose of these models is to participate in the ensemble along with the multilingual models .
The latter bilingual models follow the same transformer base architecture and joint vocabulary used in the baseline bilingual models . 
The multilingual model combines the 8 translation directions shown in Table 1 .
These are the six primary directions plus English ↔ German as a helper .
The latter is mainly used to improve generation on the English centric directions .
The model uses a 64 K joint vocabulary constructed using sentencepiece ( Kudo and Richardson , 2018 ) from a subset of the monolingual data of each language as described in Section 2 .
The transformer model has 12 encoder and 6 decoder layers .
In addition , a multitask objective is used during training to make use of monolingual data .
The objective comprises the usual parallel data likelihood referred to as MT , a masked language model ( MLM ) at the encoder and a denoising auto - encoder ( DAE ) ( similar to mBART ( Liu et al , 2020 ) ) at the decoder side .
The latter two objectives help leverage monolingual data for both the encoder and the decoder sides .
The three objectives are combined using different proportions according to a schedule during the training .
Please refer to ( Wang et al , 2020 ) for details . 
To summarize we build the following models : Bilingual models trained using parallel data in Table 1 for the 6 primary directions .
These are mainly used as baselines .
Multilingual models trained using a multitask objective using parallel and monolingual data and comprising 8 directions . 
Bilingual models trained using KD and BT data generated using our best multilingual model .
These are combined with the best multilingual model as described in 3.2 .
System combination or ensembling is known to improve the performance over individual systems . 
There are many ways to create an ensemble ( Liu et al , 2018 ;
Dabre et al , 2019 ) .
For example , individual models obtained from different checkpoints during the same training or by training models sharing the same vocab and architecture using different data or simply different random seeds can be combined using model averaging techniques .
Here , we opt to combine different models since it generally leads to better performance because different models tend to be more complementary .
To this end , we propose a simple and effective method to combine completely different architectures .
The proposed method could be also used in conjunction with checkpoint and model averaging for further gains , but we have n't tried this in our experiments due to time limitations .
The basic idea of our combination is very simple .
Assume we have the translation pair x y where y is the reference translation .
The output of model 1 is the pair x y1 and the output of model 2 is the pair x y2 .
This can be generalized to multiple systems but we limited our combination to only two models .
We train a new model that takes the set of hypotheses ( possibly augmented by the source sentence ) from the two models to generate the target sentence .
Thus this model combines the outputs of two models in the ensemble to produce a translation closer to the original target sentence
i.e. < HY P >
y1 < HY P > y2 y. We also experimented with adding the source to the input i.e. < SRC
> x < HY P
>
y1 < HY P > y2 y which led to around 0.3 BLEU improvement for Ha En , but we have n't tried on other pairs due to time limitation .
All combination models use 6 layers encoder and decoder and a 64 K vocabulary similar to the multilingual system .
These combination models use the full bitext and dev data provided in WMT21 as shown in Table 1 .
The system combination is outlined in Figure 1 .
This ensembling technique can be thought of as providing both system combination and post - editing capabilities .
Our overall system is depicted in Figure 2 1 following the temperature - based strategy in ( Arivazhagan et al , 2019 ) to balance the training data in different resource languages using T = 5 .
We pick the best system and use it to back translate the selected monolingual data .
For most pairs , as detailed in Section 4 , we find that M T + DAE and M T + M LM + DAE are quite close .
Therefore , we use the M T + DAE to do back translation for all submitted 6 pairs .
We use beam search with beam size = 5 when generating the synthetic back - translated data .
Once we get the back - translated data ( called BT 1 ) we add it to our parallel and monolingual data and build a new multilingual model called M T + DAE + BT 1 .
We tag the back - translated data with < BT > tag at beginning of each source sentence
so the model can differentiate between the genuine parallel and backtranslated data quality .
The resulting model is used to regenerate the back - translated data ( called BT 2 ) and to knowledge distill the bitext ( called KD ) .
The latter two data sets are augmented and used to build a bilingual system ( called M T + KD + BT 2 ) .
We upsample the KD data set and the upsampling ratio is selected based on parameter sweeping and validating the resulting improvement on the validation set .
Finally , the latter bilingual model is combined with our final multilingual model using the method in Section 3.2 to create our submission .
In this section , we describe the results of our intermediate and final systems .
We report Sacre - BLEU ( Post , 2018 ) on the validation set released in WMT21 , and both SacreBLEU and COMET ( Rei et al , 2020 ) using the available implementation 3 on the official test set released in WMT21 .
The results for the six submitted language pairs are in Tables 3 - 5 .
The first row in each table shows the bilingual baseline which performs relatively poor due to the limited amount of parallel data for each pair .
This is followed by the four multilingual systems with different objectives .
It is clear that adding a monolingual objective brings nice improvements for all language pairs .
The M T + DAE and M T + M LM + DAE perform closely for all language pairs indicating that target monolingual data is most important .
The next two rows show the results of adding back - translated data to the multilingual model and a bilingual baseline using back - translated and knowledge distilled data generated from the best multilingual model .
As expected adding back translation brings significant improvement to all language pairs .
Also using the multilingual model to create data for a bilingual model shows excellent results that outperform the multilingual model .
Finally , the ensemble , as expected , performs better than the individual models .
The significant difference between reported improvements in Ha ↔ En and other directions shows the effectiveness of adding De ↔ En parallel and monolingual data that helps English centric directions more than other directions .
We evaluated the final submitted systems on the official test set released in WMT21 as shown in Table 6 .
This paper describes our submission to the constrained track of WMT21 .
We focus on the three relatively low resource language pairs Bn ↔ Hi , En ↔ Ha and Xh ↔ Zu .
To overcome the limitation of relatively low parallel data we train a multilingual model using a multitask objective recently proposed in ( Wang et al , 2020 ) .
In addition , we augment the data using back translation .
We also use the resulting multilingual model to create a bilingual model incorporating back translation and knowledge distillation .
Finally , we combine the two models , using a flexible sequence - to - sequence approach , to yield our submitted systems .
We see large gains up to 8 - 10 BLEU points for En ↔ Ha and nice improvements of up to 2 - 3 BLEU points for Bn ↔ Hi and Xh ↔ Zu .
