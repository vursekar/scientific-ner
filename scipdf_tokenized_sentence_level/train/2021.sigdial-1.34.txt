Where Are We in Discourse Relation Recognition ?
Discourse parsers recognize the intentional and inferential relationships that organize extended texts .
They have had a great influence on a variety of NLP tasks as well as theoretical studies in linguistics and cognitive science .
However it is often difficult to achieve good results from current discourse models , largely due to the difficulty of the task , particularly recognizing implicit discourse relations .
Recent developments in transformer - based models have shown great promise on these analyses , but challenges still remain .
We present a position paper which provides a systematic analysis of the state of the art discourse parsers .
We aim to examine the performance of current discourse parsing models via gradual domain shift : within the same corpus , on in - domain texts , and on out - of - domain texts , and discuss the differences between the transformer - based models and the previous models in predicting different types of implicit relations both interand intra - sentential .
We conclude by describing several shortcomings of the existing models and a discussion of how future work should approach this problem .
Discourse analysis is a crucial analytic level in NLP .
In natural language discourse , speakers and writers often rely on implicit inference to signal the kind of contribution they are making to the conversation , as well as key relationships that justify their point of view .
While early AI literature is full of case studies suggesting that this inference is complex , open - ended and knowledge - heavy ( e.g. , Charniak ( 1973 ) ; Schank and Abelson ( 1977 ) ) , recent work on computational discourse coherence offers a different approach .
Take the following example from Pitler and Nenkova ( 2008 ) : ( 1 ) " Alice thought the story was predictable .
She found it boring .
" This discourse shows the classic pattern of implicit information .
The overall point is that Alice had a negative opinion of the story : the underlying explanation is that the story was not interesting because it had no surprises .
But given available lexical resources and sentiment detection methods , we can capture such inferences systematically by recognizing that they follow common general patterns , known as " discourse relations " , and are guided by shallow cues .
An example of an instance in which discourse analysis can produce insights that may be missed by employing other NLP methods is this example from Taboada ( 2016 ) , where without discourse relations it may be difficult to capture sentiment : ( 2 ) " While this book is totally different from any other book he has written to date , it did not disappoint me at all .
" This represents a Concession relation according to both Rhetorical Structure Theory and the Penn Discourse Treebank ( where it is notated as Comparison . Concession ) , resolving the incongruity of the first clause being negative and the second clause being positive by illustrating how the negative statement in the subordinate clause is reversed by the positive one in the main clause .
The importance of discourse has led to active research based on predicting what coherence relations are present in text based on shallow information .
The predicted relations are then used to draw inferences from the text .
The value of predicting the semantic classes of coherence relations has been demonstrated in several applications , including sentiment analysis ( Marcu , 2000 ;
Bhatia et al , 2015 ) , machine comprehension ( Narasimhan and Barzilay , 2015 ) , summarization ( Cohan et al , 2018 ; Marcu , 1999 ; Xu et
al , 2019 ; Kikuchi et al , 2014 ) , and predicting instructor intervention in an online course discussion forum ( Chandrasekaran et al , 2017 ) .
However , it is still the case that few works have so far found discourse relations as key features ( Zhong et al , 2020 ) .
We argue that one reason for this gap between theory and empirical evidence is the quality of the parsers exacerbated by the distributional shifts in the texts they need to apply to . 
The necessity of discourse research has resulted in several shared tasks ( Xue et al , 2015 ( Xue et al , , 2016 and corpora development in multiple languages ( Zeyrek and Webber , 2008 ;
Meyer et al , 2011 ; Danlos et al , 2012 ; Zhou et
al , 2014 ; Zeyrek et al , 2020 ) .
Yet shallow discourse parsing is a very difficult task ; more than 10 years after the introduction of the Penn Discourse Treebank ( Eleni Miltsakaki , 2004 ) , performance for English implicit discourse relation recognition has gone from 40.2 F - 1 ( Lin et al , 2009 ) to 47.8 ( Lee et al , 2020 ) , less than 8 percentage points ; a similar story could be said about the relation prediction performance of RST parsers .
Such performance hinders the wider application of parsers .
If downstream tasks are to use predicted relation senses , the data to which the systems are applied is typically different from their training data - the Wall Street Journal ( WSJ ) in a 3 - year window - to varying degrees .
This tends to further aggravate the low performance observed .
As a result , often we find that adding parsed discourse relations into models are unhelpful . 
Although domain difference is a recognized issue in shallow discourse parsing by existing work ( Braud et al , 2017 ; Liu et al , 2016 ) , we still have little understanding of the types of distributional shift that matter and by how much , even within one language .
This position paper seeks to shed some light on our current state in discourse parsing in English .
Surprisingly , we found that parsers have some issues even within the same news source as the training set ( WSJ ) ; the differences in accuracy were not significant between indomain and out - of - domain data for the qualitative examples that we looked at , although the distribution of errors tend to be different .
This differs from other NLP tasks such as entity recognition , where training on data in the target domain increased the F1 score by over 20 points ( Bamman et al , 2019 ) . 
We further found that parsers perform differently on implicit discourse relations held within vs. across sentences .
We believe these findings are strong evidence for the sensitivity of existing models to distributional shift in terms of both linguistic structure and vocabulary . 
Additionally , as part of our evaluation , we asked linguists to perform manual annotation , which allowed us to evaluate the accuracy of these parsers on plain , unlabeled text , and gain some insight about the mistakes made by the parsers .
During the annotation process , we uncovered information that can guide future research , including but not limited to the critical role of context for implicit discourse sense classification .
We discuss this need for context , hypothesize what scenarios may cause two arguments to need additional context , and provide some examples for which this is the case .
We urge future researchers to consider developing contextaware models for shallow discourse parsing moving forward .
We release our dataset to facilitate further discourse analysis under domain shift .
1
There are various frameworks for studying inferential links between discourse segments , from local shallow relations between discourse segments in PDTB ( Rashmi Prasad , 2008 ) to hierarchical constituent structures in RST ( Carlson et al , 2003 ) or discourse graphs in Segmented Discourse Representation Theory ( SDRT )
( Asher et al , 2003 ) and the Discourse Graphbank ( Wolf and Gibson , 2005 ) . 
Rhetorical Structure Theory ( RST ) ( Mann and Thompson , 1987 ) provides a hierarchical structure for analyzing text that describes relations between text spans known as elementary discourse units ( EDUs ) .
The RST Discourse Treebank ( Carlson et al , 2003 ) contains 385 Wall Street Journal articles from the Penn Treebank ( Marcus et al , 1993 ) which have been split into elementary discourse units and annotated according to Rhetorical Structure Theory , where discourse relations are annotated in a tree structure across the whole document .
A full list of these relations can be found in Carlson and Marcu ( 2001 ) . 
The Penn Discourse Treebank ( PDTB ) ( Eleni Miltsakaki , 2004 ; Rashmi Prasad , 2008 ; Prasad et al , 2018 ) , which also uses Penn Treebank Wall Street Journal articles , contains discourse relations annotated in a shallow , non - hierarchical manner .
For each relation between two arguments , each argument and the discourse connective ( word or phrase that indicates the discourse relation ) are labeled .
The PDTB also annotates whether a relation is explicit or non - explicit , the latter type of which has three subtypes : Implicit , AltLex , and EntRel .
In this paper , we focus on implicit relations , where a connective can be inserted between the two arguments that indicates a discourse relation .
These relations are considered extremely challenging for discourse parsers to automatically identify . 
There is a need to examine the performance of the proposed discourse parsers , their representational choices , their generalizability , and interpretability both across domains , distributions , and frameworks .
One recently developed framework is the PDTB - 3 .
Since its release in 2019 , several papers have evaluated the performance of implicit sense classifiers on this new corpus , which includes newly annotated intra - sentential implicit discourse relations .
In addition to proposing a new evaluation framework for PDTB , Kim et al ( 2020 ) evaluate the performance of pretrained encoders for implicit sense classification on the PDTB - 2 and the PDTB - 3 .
Liang et al ( 2020 ) identify locating the position of relations as a new challenge in the PDTB - 3 , due to the significantly increased number of intra - sentential implicit relations annotated . 
Techniques of discourse parsing range from supervised Mabona et al , 2019 ; Lin et al , 2019 ; Zhang et al , 2020 ; Kobayashi et al , 2020 ) and weakly supervised and unsupervised approaches ( Lee et al , 2020 ; Nishida and Nakayama , 2020 ; Kurfalı andÖstling , 2019 ) ; recent developments such as word / contextual embeddings have improved parser performance , although not as significantly as other tasks ( Shi and Demberg , 2019 ; Chen et al , 2019 )
Yet most works have made simplifying assumptions concerning the linguistic annotations for practical purposes that affect their evaluation and generality .
For instance , most shallow discourse parsers use only the argument pairs to determine the discourse sense without considering further context .
Additionally , in RST parsing , standard practice involves classifying only the 18 top - level RST classes ( Hernault et al , 2010 ; Feng and Hirst , 2014 ; Morey et al , 2017 ) .
Thus , all Elaboration relations are lumped together , making it a huge class .
We reveal findings about these assumptions in Section 4 . Other works evaluating discourse parsers include DiscoEval ( Chen et al , 2019 ) , a test suite of evaluation tasks that test the effectiveness of different sentence encoders for discourse parsers , and an i m - proved evaluation protocol for the PDTB - 2 ( Kim et al , 2020 ) .
In contrast , our work aims to analyze and evaluate existing discourse parsers via gradual domain shift .
We provide a comparative genrebased analysis on distributionally shifted text data and present a qualitative analysis of the impact of the practical choices that these models make while doing discourse parsing across frameworks . 
3 Where are we in discourse parsing ?
Data .
We start by focusing on possible distributional shifts in a shallow parser 's application , by considering different linguistic types of implicit discourse relations ( inter - vs intra - sentential ) ( Liang et al , 2020 ) .
To do this , we evaluate performance on the PDTB - 2 and PDTB - 3 , as well as the intrasentential relations in the PDTB - 3 specifically . 
We then evaluate the performance of three widely used or state - of - the - art models under gradual shift of the domain of texts , noting that users who would want to use a parser will be applying it on data that varies linguistically to different degrees from the parser 's training data ( a fixed 3 - year window of WSJ articles ) .
The data we examine is : WSJ texts outside of the Penn Treebank , other news texts , and the GUM corpus ( Zeldes , 2017 ) .
Note that none of these texts contain gold PDTB annotations , and only the GUM corpus contains gold RST annotations . 
Setup .
To examine the impact of changing the linguistic distribution by introducing intra - sentential discourse relations , we run the model developed by Chen et al ( 2019 ) using the same train - test split as the authors and training / testing on discourse senses which contain 10 or more examples .
To get results for the PDTB - 2 , we train and test the model on the PDTB - 2 ; to get results for the PDTB - 3 and intrasentential relations in the PDTB - 3 , we train the model on the PDTB - 3 and evaluate its performance on both of these sets . 
To parse plain - text documents for PDTB relations , we use the Wang and Lan ( 2015 ) parser as our end - to - end parser and the Chen et al ( 2019 ) DiscoEval parser as our implicit sense classifier .
The former is needed in order to parse unlabeled text , and the latter is a more accurate BERT - based implicit sense classifier ( implicit sense classification is the most difficult PDTB parsing task ) .
To evaluate these parsers , we look at quantitative as - pects of their output ( e.g. the distributions ) and qualitative aspects ( manual annotation and inspection of parser output ) . 
For our RST experiments , we use the state - ofthe - art ( Wang et al , 2017 ) parser .
We evaluate the performance of this parser on the standard RST Discourse Treebank test set with a 90 - 10 split ( 347 training documents and 38 test documents ) .
We also evaluate it on the gold labels from the GUM corpus ( but trained on the RST ) .
Because GUM is annotated with 20 different discourse relations which do not precisely map to the conventional 18 types used in the Wang et al ( 2017 ) parser , we map the ones that do n't match these types or the more fine - grained relations in the following manner , following Braud et
al ( 2017 ) : preparation to BACK - GROUND , justify and motivation to EXPLANA - TION , and solutionhood to TOPIC - COMMENT . 
For the plain - text news articles from outside of the PDTB corpus , we mirror the PDTB experiments on these documents by parsing them with the ( Wang et al , 2017 ) parser , then examining the resulting distributions and manually inspecting the parser output .
Transformer - based models perform better on linguistically different intra - sentential relations than they do on inter - sentential relations .
As mentioned above , we aim to examine the results of distributional shifts in both vocabulary and linguistic structure .
Here , we look at shifts in linguistic structure , namely , inter - vs. intra - sentence implicit discourse relations ( Hobbs , 1985 ) .
The latter was introduced in the PDTB - 3 ( Liang et al , 2020 ) from which we show the following example : ( 3 ) ...
Exxon Corp. built the plant but ( I m - plicit = then ) closed it in 1985 Unlike the inter - sentence relations that were annotated across adjacent sentences , implicit intrasentence relations do not occur at well - defined positions , but rather between varied types of syntactic constituents .
Additionally , they often co - occur with explicit relations . 
Table 1 shows the accuracies of the base and large BERT model ( Chen et al , 2019 ) on the implicit relations in the two versions of the PDTB .
The results on the PDTB - 3 are significantly better than those of the PDTB - 2 , and the model tested on the PDTB - 3 intra - sentential relations significantly outperformed both ( p<0.01 , t>11.172 ) .
This mirrors the results found from running the baseline model in Liang et al ( 2020 ) on the PDTB - 2 , PDTB - 3 , and PDTB - 3 intra - sentential relations . 
Figure 1 shows the accuracy of the Wang et al ( 2017 ) parser on the inter - sentential and intrasentential relations in the RST , respectively .
For the inter - sentential relations , we sampled only the relations between two sentences to have a " fairer " comparison ( it is well known that performance suffers on higher levels of the RST tree ) .
As with the PDTB , these results show a significant improvement in performance when run on only the intra - sentential relations compared to only the intersentential relations . 
These results drive home the influence of the linguistic and structural differences between intra - and inter - sentence implicit relations on the performance of the parsers .
We initially found this surprising since intra - sentence ones contain arguments with less information than their ( full - sentence ) intersentence counterparts .
However , one explanation for this is that , while looking for relations within sentence boundaries is a problem that has been very explored , and to some extent solved , in various NLP tasks ( e.g. syntactic parsing ) , there are not as many rules regarding relations that occur across sentence boundaries .
Regardless of the cause , these results illustrate that future shallow discourse parsers may benefit from accounting for such linguistic differences explicitly . 
Parsers struggle to identify implicit relations from less frequent classes .
The second distributional shift we examine is a shift in vocabulary .
In order to capture this , we measure the performance across several domain shifts from the PDTB - 2 using three datasets : WSJ articles from the COHA corpus ( Davies , 2012 ) , other news articles from COHA , and the GUM corpus ( Zeldes , 2017 ) .
The WSJ articles are completely within the domain of the PDTB , but more shifted in timeline than the PDTB test set .
The other news articles are in - domain as well , but not from the same source Figure 1 : F - 1 scores for running the Wang et al RST parser on the RST Discourse Treebank for inter - sentential ( yellow ) and intra - sentential ( blue ) relations ( * denotes that this relation was not included in the set of inter - sentential relations ) .
We can see from this graph that the performance of the parser was improved for the intra - sentential relations compared to the inter - sentential relations .
publication , and thus may be linguistically different .
The GUM corpus , our out - of - domain dataset , contains data from eight domains : Academic , Bio , Fiction , Interview , News , Travel , How - to guides , and Forum Discussions .
It contains gold RST annotations but no PDTB annotations . 
To quantitatively evaluate the performance of these parsing models , we examine the distribution of the parser predictions and how frequently different senses are predicted .
From this , we noticed that only 5 out of the 16 PDTB - 2 level 2 senses were predicted at all by the Wang and Lan parser , and only 7 out of 16 were predicted by the DiscoEval parser .
Of these classes , several were predicted less than 2 % of the time ( Table 6 ) . 
We can also see that in Tables 2 and 3 , the Wang et al parser predicted at least 38.7 % Contingency .
Cause for all datasets and the DiscoEval parser predicted at least 44 % Contingency .
Cause , although these percentages were often much higher .
Because only 24.9 % of the total relations contained in the PDTB are Contingency , this overrepresentation of Contingency .
Cause in the predictions indicates a strong bias towards Contingency .
Indeed , many of the errors found during annotation occurred when the parser predicted Contingency .
Cause , the most common level 2 sense , over a less represented class such as Comparison .
Contrast ; the precision for Contingency .
Cause was 0.33 , 0.14 , and 0.33 for WSJ articles , non - WSJ news articles , and the GUM corpus respectively .
This likely contributed to the low accuracy for these documents . 
These results show us that if PDTB parsers are run on plain text documents , whether in - domain or slightly shifted , the results are likely to be overconfident with majority classes and unlikely to predict minority classes .
We also obtained the predicted distributions of the RST relations ( Table 4 ) on the COHA news articles ; we examined these results for the set of WSJ articles as well as the other news articles .
We found that relations that are highly represented in the RST Discourse Treebank such as Elaboration , Attribution , and Same Unit were predicted much more frequently than they appear in the RST .
However , more minority classes were represented in 5 , where the results of the parsers are compared to the ground truth labels by the annotators . 
Across the three corpora , the annotators noticed that in many cases the relation type was labeled as EntRel or NoRel when it should n't have been , or vice versa .
This led to discourse senses being predicted for relations that did not have a discourse sense and vice versa .
The parsers also often had issues with argument segmentation .
For the GUM corpus , segmentation was especially an issue in the travel genre , where headers or captions would be labeled as part of an argument . 
As is shown in Table 5 , the percentage of implicit relations that the parsers got right on the second level appeared to decrease on average as the domain shifted .
However , this was a very slight decrease ; they had roughly the same level of accuracy across all datasets , which was very low .
In fact , for all parsing models and datasets , a larger percentage of relations was predicted completely incorrectly . 
The results of running the state - of - the - art Wang et al ( 2017 ) parser on the gold labels of the RST and GUM corpus are shown in Figure 2 .
These results make it clear that the RST parser performs much worse on out - of - domain data than it does on RST corpus data .
This is expected ; it unsurprisingly does not generalize as well for text outside of its domain as for the news text contained within the corpus test set due to a change in vocabulary .
However , in order for discourse parsers to be useful for applications outside of the news domain , models that can more easily adapt to the target domain must be developed . 0 - 2 % 1 2 2 2 3 3 2 - 5 % 1 0 0 2 2 2 > 5 % 3 3 3 3 3 3
While inspecting the results of the annotations , we found several helpful phenomena for developing future models , including observations regarding the role of context in shallow discourse parsing and errors that current RST parsers are making .
For the qualitative analysis , we ask two annotators ( a faculty member and a graduate student from linguistics departments ) to provide annotations for the data , as none of the texts contain gold PDTB labels and only the GUM corpus contains gold RST labels .
The annotators were trained on , and provided with , the PDTB 2.0 annotation manual ( Prasad et al , 2007 ) . 
In order for the annotators to annotate this corpus , discourse relations were randomly chosen from Wall Street Journal articles , other news articles , and the GUM corpus .
64 of these discourse relations were implicit , and are the only ones reported in this paper .
The annotators were given the sentence ( s ) containing both arguments , with the arguments labeled , and they also had access to the article text if they ever needed to reference back to it .
To assess the inter - rater agreement , we determine Cohen 's κ value ( Cohen , 1960 ) .
We randomly selected 25 samples from the PDTB and assigned each to the annotators .
We obtained a Cohen 's κ of 0.88 , which indicates almost perfect agreement .
More context than the two arguments is needed to determine the correct discourse relation in many cases One potential way to mitigate the impact of domain shift on the performance of shallow discourse parsers is to incorporate context .
With a few exceptions ( Dai and Huang , 2018 ; Shi and Demberg , 2019 ; Zhang et al , 2021 ) , existing models for shallow discourse parsing mostly do not Figure 3 : RST parse tree containing a segment of the relations that were examined in the qualitative analysis .
The discourse sense labels on this tree that were examined in our analysis are marked red and green , where green is correct and red is incorrect use input beyond the two adjacent sentences that comprise the arguments of the relation ( Kishimoto et al , 2020 ; Chen et al , 2019 ) .
We found that only considering these two sentences is not sufficient even for our expert linguist annotators .
Specifically , while annotating the PDTB , the annotators found several examples where , when they looked at the larger context behind the arguments and the sentences where the arguments were contained , their annotations changed .
Below , we describe a few examples that demonstrate the mistakes that can be made without the full context and their implications : ( 4 ) In this northern latitude it does n't get dark in summer until about 10:30 p.m. so lighting is operate except at some crazy time like 11:45 at night , whenever there is power , unless they have stand - by diesel generators .
There 's a year 's supply of diesel oil here . 
This example is from the Wall Street Journal .
At first glimpse , one would think to annotate this as Contingency .
Factual present condition , but this does not capture the full context , which is shown below : ( 5 ) One housewife says : " With an electric kitchen I have to do my whole day 's cook - ing the day before - and that during a couple of hours , not knowing from one minute to the next what time the power is coming on . "
In this northern latitude it does n't get dark in summer until about 10:30 p.m. so lighting is operate except at some crazy time like 11:45 at night , whenever there is power , unless they have stand - by diesel generators .
There 's a year 's supply of diesel oil here . 
The additional context , that people in the country described are dealing with electricity issues despite there being a year 's worth of diesel supply , is now made clear in this passage .
Thus we can conclude that the correct relation here is Comparison .
Contrast .
Without getting this context and just seeing the two sentences in which the arguments are contained , it is difficult to discern this as an annotator .
This shows that by just getting exposure to the two arguments , without additional context , the sense may be marked incorrectly .
The Wang and Lan ( 2015 ) parser and the DiscoEval parser both predicted this incorrectly , with the Wang and Lan ( 2015 ) parser predicting it as Contingency .
Cause and the BERT parser predicting it as Expansion .
Conjunction .
Similarly , the following example , also contained in this passage , has a different true annotation than one would think from only seeing the arguments : ( 6 ) One housewife says : " With an electric kitchen I have to do my whole day 's cooking the day before - and that during a couple of hours , not knowing from one minute to the next what time the power is coming on . "
In this northern latitude it does n't get dark in summer until about 10:30 p.m. so lighting is operate except at some crazy time like 11:45 at night , whenever there is power , unless they have stand - by diesel generators . 
The relation may be deemed as Expansion .
Instantiation .
However , by reading the full text , it is clear that it should be labeled as Contingency .
Cause .
Like the last example , a clearer view of the full text is needed to determine the proper annotation , not simply the two arguments .
These observations provide insights as to why contextual embeddings with document context such as the next sentence prediction task helps with implicit discourse relation classification ( Shi and Demberg , 2019 ) .
More generally , we believe future work on discourse parsing should look beyond only the arguments of a relation because of the different interpretations one would give when taking the relation in vs. out of context .
We believe that argument pairs with low specificity and one or more pronouns may be especially in need of this extra context , but more experimentation will have to be done to confirm this hypothesis . 
Attachment issues tend to occur throughout the RST parse tree , and relations are often misclassified as Same - Unit and Elaboration .
Regarding insights for the RST Discourse Treebank , a piece of the RST tree for this paragraph can be seen in 3 .
Here , the EDU " One housewife says " should attach to the EDU after it , " With an electric kitchen I have to do my whole day 's cooking the day before " .
However , it instead attaches to EDUs from the preceding sentences , which is incorrect , as these two sentences do not contain what the housewife says .
We saw several other attachment issues in the text , including a couple where the attachment should go up / down by several levels .
We also saw several instances of the relation being incorrectly tagged as Same - Unit or Elaboration , some of which can be seen in the diagram . 
Attachment issues are a particular problem for RST parsing due to its hierarchical nature ; one at - tachment issue can lead to error propagation where the accuracy of the attachments further in the tree is impacted by that of the current one .
Reducing this error is of the utmost importance for future parsers .
Discourse parsing for text has seen a recent surge in experimental approaches .
In this work we presented a detailed analysis of the performance of the state of the art discourse parsers and analysed their weaknesses and strength .
The conclusions drawn above from these experiments make it clear that discourse parsing , though it has come a long way in the past decade or so , still has a long way to go , particularly with respect to parsing on out - ofdomain texts and addressing issues of class imbalances , although the BERT - based model has made some improvements in this area .
Additionally , we investigated how and when PDTB - 3 can help in improving the prediction of intra - sentential implicit relations . 
There are several promising future directions for the area of discourse parsing .
A model that detects intra - sentential implicit relations is necessary in order to be able to parse on the PDTB - 3 .
Exploring new neural parsing strategies is also a must .
We observed that neural parsers are ignorant about what they do not know and overconfident when they make uninformed predictions .
Quantifying prediction uncertainty directly by training the model to output high uncertainty for the data samples close to class boundaries can results in parsers that can make better decisions .
One takeaway of our empirical analysis was the importance of the role of context in identifying the correct discourse relations .
This observation suggests the need for new computational experiments that can identify the right context window that is required for the model to accurately predict relations . 
Another useful direction is designing models that can learn discourse relations on their own without the help of annotated corpora .
There are several unsupervised models ( Kobayashi et al , 2019 ; Nishida and Nakayama , 2020 ) that are used for determining the structure of discourse parse trees but few that infer the relations themselves .
We would like to thank the reviewers , Diane Litman and Matthew Stone for providing helpful feedback for this work .
