You Do n't Know My Favorite Color : Preventing Dialogue Representations from Revealing Speakers ' Private Personas
Social chatbots , also known as chit - chat chatbots , evolve rapidly with large pretrained language models .
Despite the huge progress , privacy concerns have arisen recently : training data of large language models can be extracted via model inversion attacks .
On the other hand , the datasets used for training chatbots contain many private conversations between two individuals .
In this work , we further investigate the privacy leakage of the hidden states of chatbots trained by language modeling which has not been well studied yet .
We show that speakers ' personas can be inferred through a simple neural network with high accuracy .
To this end , we propose effective defense objectives to protect persona leakage from hidden states .
We conduct extensive experiments to demonstrate that our proposed defense objectives can greatly reduce the attack accuracy from 37.6 % to 0.5 % .
Meanwhile , the proposed objectives preserve language models ' powerful generation ability .
Social chatbots have been widely used to benefit many applications from answering factual questions to showing emotional companionship .
With recent progress in large pretrained language models ( Radford et al , 2019 ; Yang et al , 2019 ) , some attempts ( Wolf et al , 2019 ;
Ham et al , 2020 ; Shen et al , 2021 ; Sevegnani et al , 2021 ; Gu et al , 2021b ) are made to build chatbots based on large generative language models ( LMs ) .
To train such LM - based chatbots , private conversations are collected .
Unfortunately , large language models tend to memorize training data and some private data can be recovered from models ( Pan et al , 2020 ; Carlini et al , 2021 ) .
Besides such memorization problems , " overlearning " on simple training objectives can reveal sensitive attributes indirectly related to the learning task ( Song and Shmatikov , 2020 ) .
LM - based social chatbots essentially inherit the privacy issues of general LMs and the overlearning problem . 
For example , as Figure 1 shows , when using a fine - tuned GPT - 2 as the encoder and decoder of an LM - based social chatbot , if the learned representation of each utterance can be obtained by an adversary , then the adversary can build a classifier to predict the persona information based on the representation .
As shown by the example , for five out of 14 utterances , the attacker can successfully predict the persona , which can be harmful if the users ( speakers of the utterances ) do not prefer to reveal the persona information .
Thus , in practice , when deploying such kinds of chatbots in real applications , we should first make sure that no private information can be leaked by the models . 
To systematically study the privacy issues in LMbased social chatbots , there are several challenges .
First , there is no existing data that can be used to quantify how much private information is revealed by an LM .
Second , there has been no existing work showing how to attack utterance - level representations to obtain sensitive information .
Third , there has been no existing LM - based chatbot that can defend against persona inference attacks , and no study shows how to protect both known and unknown persona attributes . 
In this paper , to address the above challenges , we use the fine - tuned GPT - 2 as our chatbot .
We first collect a dataset by aligning personas with corresponding utterances in PersonaChat dataset ( Zhang et al , 2018 ) .
Then we show that " overlearning " can happen for LM - based chatbots to reveal personas of speakers .
We build a single external multi - layer perception ( MLP ) attacker model to perform black - box persona inference attacks on the utterance - level embeddings .
With no access to parameters of the chatbot , the attacker model can infer speakers ' personas with 37.59 % accuracy over 4 , 332 personas .
The high accuracy of the attacker model implies that the utterance - level embeddings have potential vulnerabilities to reveal                 
 


       Figure 1 : Black - box persona inference attacks ( over 4 , 332 personas ) on a dialog .
Every representation of the utterance , which is based on the last hidden state of GPT - 2 , is attacked without defense ( column of " Attacks on LM " ) and with defense ( column of " Attacks on the defensed LM " ) .
If the model can predict the persona of the speaker based on the observed representation , then we regard it as a successful attack ; otherwise , unsuccessful .
In practice , when deploying a model , a robust model which will reveal nothing of the encoded utterances is expected . 
speakers ' private persona attributes .
Thus , it is necessary to improve training algorithms to address such overlearning issues .
Finally , we apply defense learning strategies on the GPT - 2 to prevent such black - box attacks .
We combine proposed KL divergence loss ( KL loss ) with mutual information loss ( MI loss ) ( Song et al , 2019 ) as additional defense objectives to train the GPT - 2 and decrease the attacker 's persona inference accuracy to 0.53 % .
Our contributions can be summarized as follows : 1 1 ) :
To the best of our knowledge , we are the first to disclose and analyze the persona inference attack for LM - based chatbots and treat it as a privacy risk . 2 ) :
We propose an effective defensive training algorithm to prevent dialog representations from leaking personas of the corresponding speakers by uniform distribution approximation and mutual information minimization . 
3 ) : We conduct extensive experiments to quantify both privacy and utility of proposed defense mechanisms .
Besides solving the persona leakage issue , the proposed training algorithm has nearly no negative influence on utility .
Language models trained on private data suffer privacy risks of revealing sensitive information .
Previous researches mainly considered black - box attacks that assumed attackers only had access to 1 Code is publicly available at https://github . com / HKUST - KnowComp / Persona_leakage_and _ defense_in_GPT - 2 . inputs and outputs of language models .
Carlini et
al ( 2021 ) performed black - box model inversion attack on GPT - 2 through descriptive prompts with beam search .
Lehman et al ( 2021 ) examined BERT pretrained on Electronic Health Records via blank filling and model probing to recover Personal Health Information .
Furthermore , given black - box access to a language model 's pre - train and finetune stages , Zanella - Béguelin et al ( 2020 ) showed that sensitive sequences of the fine - tuning dataset can be extracted .
For the distributed client - server setup , Malekzadeh et al ( 2021 ) considered the sensitive attribute leakage from the server side with honest - but - curious ( HBC ) classifiers . 
What is worse , for an LM - based chatbot , its training conversations are prone to include more private attributes than other commonly - used corpora for language modeling like BooksCorpus ( Zhu et al , 2015 ) and Wikipedia .
Tigunova et al ( 2019 ) proposed Hidden Attribute Model ( HAM ) to extract professions and genders of speakers from various dialog datasets .
further applied Attribute Extractor to generate speakers ' attribute triplets flexibly and suggested downstream tasks based on the triplets .
Pan et al ( 2020 ) exploited embeddings of language models to recover inputs ' digits and keywords .
Though the setup of this work is similar to ours , they merely consider simple cases of data recovery with given rules and suffer great utility degradation to obtain optimal defense performance .
For our work , there is no fixed pattern or rule for the model input .
Instead of finding key - words or recovering digits , we aim to infer more complicated private attributes from such embeddings .
Moreover , our proposed defenses have almost no influence on the utility .
In this section , we illustrate black - box persona inference attacks on GPT - 2 and our defense strategies .
In Section 3.1 , we first give the problem formulation .
Then we describe the attack in Section 3.2 .
We assume that there is a GPT - 2 based chatbot f pretrained on private conversations D. Only language modeling is used to train the chatbot : 
L f ( u ; θ f )
= − | u | i=1 log ( Pr ( wi | c , w0 , w1 , ... , wi−1 ) ) , ( 1 ) where f refers to the LM - based chatbot with given utterance u = { w 0 , w 1 , ... ,
w | u | −1 } and previous context c. An adversary owns one external annotated dialog dataset D a = { ( U 1 , s 1 ) , ( U 2 , s 2 ) , ... , ( U n , s n ) } with n conversations where U i indicates a list of utterances { u i1 , u i2 , ... , u in i } of i - th conversation and s i corresponds to a list of sensitive personas { s i1 , s i2 , ... , s in i } for corresponding utterance .
Each persona s kj is an integer that can be mapped to its persona according to a predefined dictionary and 0
≤ s kj ≤ C − 1 where C is the total number of predefined persona attributes .
The goal of the adversary is to infer speakers ' personas s from utterances ' embeddings f ( u ) where u and s refer to any utterance and its persona label .
The persona inference attack can be viewed as a supervised classification task .
For the black - box attack setup , the adversary can only query the target dialog model f with access to embeddings of adversary 's inputs and can not access or modify model parameters θ f .
As shown in the left part of Figure 2 , the adversary tries to build its attacker model
A with its external data D a and dialog model f .
The persona predictor 's output A ( f ( u ) ) is the estimated probability distribution over C persona attributes .
Its loss function L A exploits cross - entropy between the predicted distribution and ground truth distribution that can be formulated as : LA ( u kj , s kj ; θA )
= CE ( A ( f ( u kj ) ) , s kj ) , ( 2 ) where CE refers to cross - entropy loss between persona label s kj and A ( f ( u kj ) ) . 
A well - performed persona predictor A can cause great privacy threats .
For machine learning as a service ( MLaaS ) , A can be applied to perform a man - in - the - middle attack on the application programming interfaces .
Moreover , even if the raw data are protected and the transmission channel is secure , a curious service provider can train its attacker A to collect personas of service users .
The LM training objective in Equation 1 only considers the utility of chatbots .
In later experiment sections , we show that LM brings severe overlearning issues .
Ideally , to achieve an optimal privacypreserving chatbot against persona inference attacks , the probability distribution of the attacker model
A should be close to the uniform distribution .
That is , the adversary can not improve its inference accuracy from posterior estimation A ( f ( u ) ) and the accuracy is no better than making random guesses on the persona attributes .
Moreover , the constraints on privacy should have minor degradation on the utility to maintain the strong generation ability of chatbots . 
Following the intuition that the adversary can not obtain better results than a random guess , in Section 4.1 , we propose KL loss that aims to flatten the persona predictor 's estimated distribution .
Based on minimizing the mutual information between hidden states f ( u ) of chatbots and private persona attributes s , we propose MI loss in Section 4.2 .
Lastly , we show the overall training objective in Section 4.3 .
KL loss aims to minimize the Kullback - Leibler divergence between A ( f ( u ) ) and the uniform distribution .
It flattens the distribution of A ( f ( u ) )
so that the adversary can not gain any useful knowledge after training attacker model
A.
The KL divergence between the uniform distribution and A ( f ( u ) ) can be formulated as : DKL ( UNI | |
A ( f ( u ) ) )
= − 1 C C−1 k=0 log ( CPr ( k |
f ( u ) , θA ) ) ,
( 3 ) where UNI indicates the uniform distribution and k indicates the k - th persona label of C labels .
For optimization , we can leave out constant terms and the logarithm ( Mireshghallah et al , 2021 ) ⃝ and the attacking stage is marked by 2 ⃝. Both language modeling and defender objectives are jointly trained for the defense to optimize the GPT - 2 model .
After GPT - 2 's training stage 1 ⃝ is finished , parameters of GPT - 2 are all frozen and then the attacking stage 2 ⃝ starts .
The defender shares the same architecture as the attacker and uses L kl with L mi as defense objectives . 
the following loss function : LD ( u ; θA )
= − 1 C C−1 k=0 Pr ( k | f ( u ) , θA ) . 
However , from the perspective of defenders , they have no access to attacker model
A and its parameters .
Instead , they can build their own persona predictor as a fake attacker .
More specifically , they may mimic the adversary to annotate a dataset D ′ a and a persona predictor A p .
Then the KL loss becomes : L kl ( u ; θA p , θ f )
= −
1 C C−1 k=0 Pr ( k | f ( u ) , θA p ) , ( 5 ) 
where parameters of the chatbot θ f and the fake attacker θ Ap are updated via KL loss .
The intuition is to train the chatbot together with a fake attacker to prevent model overlearning by flattening the attacker model 's distribution .
The privacy constraint requires that hidden representations should not reveal the persona attributes . 
In other words , given any utterance u and persona s behind the utterance u , we want to minimize the mutual information between f ( u ) and s : 
min θ f I ( f ( u ) ; s ) .
( 6 ) Following the derivation in Song et al ( 2019 )
and , the upper bound can be formulated as : I ( f ( u ) ; s ) ≤ E q ( f ( u ) )
DKL ( q ( s | f ( u ) )
|
| p ( s ) ) , ( 7 ) where p ( s ) can be any distribution for s , q ( x ) refers to probability distribution of model f parameterized by θ f and f ( u ) is assumed to be sampled from the conditional distribution q ( f ( u )
|
x , s ) .
However , q ( s | f ( u ) ) is hard to estimate .
Instead , we use p Ψ
( s |
f
( u ) )
to approximate q ( s | f ( u ) ) via minimizing their KL divergence and then we can obtain the following lower bound ( Song et al , 2019 ) : E q ( f ( u ) )
DKL ( q ( s | f ( u ) )
|
| p ( s ) )
≥ E q ( f ( u ) )
[ log pΨ ( s | f ( u ) )
− log p ( s ) ] .
( 8 ) Therefore , our objective in Equation 6 can be formulated as an adversarial training objective : min θ f max Ψ E q ( f ( u ) )
[ log pΨ ( s | f ( u ) )
− log p ( s ) ] .
( 9 ) log p ( s ) is independent of f ( u ) , and we may leave this term out in Equation 9 : min θ f max Ψ E q ( f ( u ) )
[ log pΨ ( s | f ( u ) ) ] .
( 10 ) Then , Equation 10 illustrates an adversarial game between an adversary p Ψ who manages to infer s from f ( u ) and a defender who modifies θ f to protect s from persona inference attack .
Adversarial training is widely used to protect sensitive features in natural language processing ( Elazar and Goldberg , 2018 ;
Coavoux et al , 2018 ; Li
et al , 2018 ) .
Using the persona predictor model
A p with softmax activation to learn p Ψ , we obtain the final objective for the defender : min θ Ap max θ f CE
( Ap ( f ( u ) ) , s ) .
( 11 ) We can rewrite Equation 11 into two losses : L mi1 ( u kj , s kj ; θ Ap )
= CE ( A p ( f ( u kj ) ) , s kj ) and L mi2 ( u kj , s kj ; θ f )
= −CE ( A p ( f ( u kj ) ) , s kj ) for the fake adversary and the chatbot respectively . 
Then our MI loss can be formulated as : Lmi = λ0Lmi1 + Lmi2 , ( 12 ) where λ 0 controls the ratio between two the fake attacker
A p and the defensed chatbot f .
The right part of Figure 2 illustrates how the chatbot is trained to address the black - box attack .
The loss function for the defender combines KL loss , MI loss and LM loss .
Notice that the fake adversary objective in MI loss violates KL loss which tries to make the distribution of A p flatten .
Our proposed loss assigns more weights to the KL loss : L = L f + λ1L kl
+ λ2Lmi , ( 13 ) where λ 1 and λ 2 are hyper - parameters and λ 1 ≥ 10λ 2 to flatten the distribution of A p .
Though the chatbot trained with overall loss L still can not interfere training process of A during black - box attacks , L aims to mitigate persona overlearning issues of f to address such persona inference attacks .
In this section , we conduct experiments to evaluate the performance of privacy and utility for the proposed defense learning strategies .
In Section 5.1 , we give our experimental settings in detail .
In Section 5.2 , we show the attacking performance with and without defense .
In Section 5.3 , we perform ablation study on defense objectives .
In Section 5.4 , we use automatic metrics to evaluate chatbots ' utility .
We conduct various attack setups in Section 5.5 and perform a case study in Section 5.6 .
Dataset .
To train the GPT - 2 as our chatbot , we use the DialoGPT pretrained on Reddit comment chains .
Then we use PersonaChat dataset ( Zhang et al , 2018 ) to fine - tune the GPT - 2 .
To obtain annotated dataset D a for the adversary , we align personas to corresponding utterances through positive ( utterance , persona ) pairs provided in Dialogue NLI ( Welleck et al , 2019 ) dataset .
For those utterances with no annotations , we assign label −1 to them .
We reshuffle the dataset to balance the label distribution among train / val / test datasets with the ratio of 8 : 1 : 1 .
We first let the attacker and defender share the same training data .
In later sections , we will separate the annotated data for the adversary and defender with no overlap .
A summary statistics of D a is shown in Table 1 .
Attacker model .
In our experiment , we use a 2 - layer neural network with cross - entropy loss as the attacker model .
The attacker model exploits the final layer embedding of the last token " < | endof - text | > " from the GPT - 2 as model input .
We also try other attacker model architectures ( transformer block based attackers ) and input embeddings ( average of all embeddings in the final layer of GPT - 2 ) , but the attacking performance is worse than the 2 - layer model mentioned above . 
Evaluation Metrics .
The evaluation metrics are based on privacy and utility .
For privacy , we use persona inference accuracy and weighted F1score to evaluate the attacker 's performance .
We also use Bayesian Privacy ( BP )
( Gu et al , 2021a ) to quantify the attacker 's privacy loss for the estimated persona distribution .
Top - k accuracy is reported in the Appendix .
For utility , we apply BERTScore , Distinct ( Li et al , 2016 ) , BLEU
( Papineni et al , 2002 ) and perplexity ( PPL ) as evaluation metrics .
BERTScore and BLEU measure similarity between generated outputs and ground truth while Distinct ( Dist ) focuses on diversity .
Perplexity shows the uncertainty when the LM model fits the data .
Attacks without Defense .
We list the attacking performance of A in multiple scenarios shown in Acc refers to test persona inference accuracy .
F1 uses weighted average F1 - score .
Max - Ratio indicates the ratio that the most frequent prediction shares among all predictions .
The worse the attack model performs , the better privacy protection can be achieved . 
distribution , then it can randomly guess over 4 , 332 labels ( Random Pred ) .
Otherwise the adversary can perform Best Guess that only guesses the most frequent persona in the dataset .
LM indicates the attacker performance that only language modeling objective is applied to train the chatbot without any defense mechanism .
From the table , the test persona inference accuracy on the LM achieves 37.59 % while guessing on the label with most occurrences merely has 0.72 % accuracy .
That is , the black - box persona inference attack has 52× the accuracy of guessing .
The huge performance gap between the attacker model and the baseline guess method indicates that simple language modeling objective has serious overlearning issues that unintentionally capture private personas of speakers . 
Attacks on the Defensed LM .
To avoid the persona overlearning issue , we use additional defense objectives illustrated in Section 4 . LM+KL+MI utilizes language modeling , KL loss and MI loss in Equation 13 to train the GPT - 2 .
As demonstrated in Table 2 , the attacker performance on LM+KL+MI significantly reduces the attacking accuracy from 37.59 % to 0.53 % and F1 - score drops from 0.37 to nearly 0 .
This defense mechanism can even outperform Best Guess in terms of privacy protection .
That is , even if the adversary annotates its own dataset to train an attacker model , the attacking performance is still worse than simply guessing the most frequent label .
As a result , the black - box persona prediction attack becomes useless after applying the defenses for the chatbot .
The adversary can not obtain any speaker 's persona from the embedding f ( u ) by training A. To learn why the proposed defenses work so well , we further examine the ratio of the most frequent predicted label ( Max - Ratio ) among all pre - dictions .
The accuracy of Best Guess reveals that the most frequent label in the test set has a ratio of 0.72 % .
After applying KL loss and MI loss , the attacker model tends to make predictions on a single label .
For LM+KL+MI , the Max - Ratio even occupies 81.87 % predictions .
This implies that the proposed defense strategies may have the potential to fool the attacker model to make wrong predictions on a single slot .
We will further investigate this implication in later sections . 
Overall , the above experiment demonstrates that our proposed defense learning strategies can effectively mitigate the persona overlearning issue and avoid black - box persona inference attacks .
To show the effectiveness of proposed KL loss and MI loss and how they affect the performance of black - box persona inference attacks , we consider the inclusion and exclusion of proposed defense objectives .
The result is shown in Table 2 . LM+KL indicates the GPT - 2 is trained with language modeling and KL loss .
LM+MI applies language modeling and MI loss .
From the table , it can be seen that LM+KL , LM+MI and LM+KL+MI are all able to reduce the test accuracy of the attacks .
The KL loss is weaker from the perspective of defense , but it tends to flatten the estimated persona distribution with much smaller Max - Ratio .
The LM+MI shares similar test accuracy and F1 - score with LM+KL+MI , but nearly all predictions are made on a single persona label with a ratio of 99.84 % .
This suggests that MI loss causes the attacker model to predict all labels on a single persona attribute .
After KL loss is applied on LM+KL+MI , the Max - Ratio drops to 81.87 % . 
As discussed earlier , high Max - Ratio may also cause privacy leakage .
Suppose the adversary knows the persona with Max - Ratio , then it can improve its guess by not predicting this persona , which is a threat for fewer persona labels ( for example , binary classification ) .
These results verify that KL loss introduces flatter estimation and MI loss is more effective against persona overlearning , which conforms to our intuition of their objectives in Section 4 .
Besides privacy , utility is another key objective to train a chatbot .
Several automatic metrics are considered to evaluate the generation performance .
For generation , we use Table 4 : Evaluation on the privacy for 8 clusters .
Unseen shows the results only for the first 3 persona labels that defender has never seen .
Overall refers to the results on all 8 labels .
Acc and Max - Ratio are measured in % .
BP u corresponds to Bayesian Privacy loss on the uniform distribution .
Still , the worse the attack model performs , the better privacy protection can be achieved . 
the second speaker ( Human B in Figure 1 ) with all previous turns as context .
Then we compared the generated model outputs with ground truth replies .
We use Dist - 1 and Dist - 2 to count ratios of distinct unigrams and bigrams .
BLEU - 1 , BLEU - 2 and BLEU - 4 are applied to evaluate generation similarity with ground truth .
Due to the one - to - many nature of chit - chats , the BLEU is not adequate to compare generated responses with ground truth .
Hence , we adapt Precision , Recall and Precision of BERTScore to measure the similarity in the embedding space . 
The evaluation result is shown in Table 3 , where same models from Table 2 are evaluated .
The result indicates that adding KL loss will increase the perplexity greatly from 14.8 to 28.9 .
After combining KL loss with MI loss , its perplexity decreases to 19.674 .
A plausible explanation is that KL loss confuses the persona predictor and indirectly increases the uncertainty of the GPT - 2 .
All GPT - 2 models have relatively low BLEU scores due to the one - to - many mapping between contexts and responses .
For Distinct and BERTScore , there are only minor differences between LM and defensed LMs .
Though the uncertainty increases after applying KL loss and MI loss , it does no harm to the quality of generation .
In summary , there is almost no negative influence on the utility after applying the proposed defense strategies .
Attacks on Imbalanced Data Distribution .
Previous black - box attacks usually assume that the annotated dataset D a must share similar data distri - bution with the defender 's training data .
To examine the performance of defense strategies on unseen personas , we assign the adversary 's dataset D a with labels that the defender can not acquire .
We split data with 500 persona labels that are uniquely held by the adversary .
The defender owns 8 , 031 conversations with persona labels ranging from 500 to 4 , 331 while the adversary holds 2 , 376 dialogues with persona labels ranging from 0 to 4 , 331 .
For testing , 500 conversations with persona labels ranging from 0 to 4 , 331 are used . 
Under imbalanced data distribution , the attack on the defensed LM has Acc 0.47 % , F1 1.90e - 3 and Max - Ratio 94.06 % .
The persona inference accuracy is still very low and the attacker model tends to predict more on a single persona label than the balanced data distribution setup .
This result shows that the proposed overall loss can also prevent black - box persona inference attacks on unseen personas .
It also verifies previous suggestions that combining LM loss with MI loss may fool the attacker model to make wrong predictions . 
Attacks on Fewer Persona Labels .
The above experiments are based on 4 , 332 persona labels .
In fact , many personas share similar meanings and can be further clustered .
Besides , to better evaluate privacy loss for the estimated distribution , a smaller label space is preferred .
Therefore , it is necessary to consider defense performance on a smaller label space .
We use Sentence - BERT ( Reimers and Gurevych , 2020 ) to embed all persona sentences and perform k - means clustering on the embeddings to obtain 8 clusters .
We manually checked these clusters and classified them as cars , food , animals For both conversations , the " context " is fixed and used as the first four utterances .
Then the bot and the user start interactive conversations with the " context " .
Since there is no gold standard , the results are annotated by the authors . 
( pets ) , family information , hobbies , jobs , personal information and music tastes respectively .
To evaluate how the clustering performs , we randomly sample 100 utterances with clustered labels and invite two volunteers to inspect those samples .
Both of them agree on 90 % of the clustered annotations . 
After manual inspection of the remaining 10 % annotations , the clustering error rate is 8 % .
Following previous imbalanced data split , we assign data in the first 3 clusters only to the adversary to make the data distribution imbalanced .
Here , the defender owns 6 , 654 conversations with persona labels ranging from 3 to 7 while the adversary holds 3 , 753 dialogues with persona labels ranging from 0 to 7 .
For testing , 500 conversations with persona labels ranging from 0 to 7 are used . 
The attacking performance for both unseen labels and all labels is displayed in Table 4 .
BP u measures the KL divergence D KL
( F 0 | |
A ( f ( u ) ) ) where F 0 refers to uniform distribution .
For imbalanced data distribution with a small label space , our proposed defenses can still achieve much lower attack accuracy than LM on both Unseen and Overall .
However , for Overall , LM+KL+MI has higher accuracy with a lower F1 - score compared with two baselines .
This indicates that proposed defenses fail to protect privacy as we desired in the baselines .
For BP u , LM+KL+MI are around 10 times smaller than LM .
It means that after applying defense objectives , the attacker 's estimated distribution is much closer to the uniform distribution .
Thus the effectiveness of the KL loss is verified .
In addition , Max - Ratio with 8 clusters on Unseen is smaller than 4 , 332 labels even though the distribution of 8 clusters is obviously tighter .
Still , the Max - Ratio of 58.15 % accounts for a much larger fraction than other predictions .
In summary , the above results imply that for the smaller label space , our proposed defense objectives are still effective even on unseen persona labels .
In Figure 3 , we give an example of the persona inference attack , where conversations are generated between the chatbot and the user with the given context .
We manually mark True / False on the predicted results .
As shown in the figure , there are several successful attacks on LM and no correct prediction on the defensed LM .
For attacks on LM , speakers ' hobbies and jobs can be inferred .
For incorrect predictions , the attacker model can still predict context - aware personas .
After applying proposed defense learning strategies , the predicted personas become irrelevant with context and mostly predict " My favorite color is blue . "
In fact , it is the most frequent prediction for LM+KL+MI over 4 , 332 persona labels .
This attack example illustrates that our defense objectives can prevent the black - box persona inference attack from inferring relevant personas .
In this paper , we show that LM - based chatbots tend to reveal personas of speakers and propose effective defense objectives to prevent GPT - 2 from overlearning .
Unlike other works that suffer from utility degradation , our defense learning strategies do no harm to the powerful generation ability of LM - based chatbots .
We conduct extensive experiments to evaluate both privacy and utility .
We perform black - box persona inference attacks under various setups to demonstrate the robustness of proposed defense learning strategies .
In addition , we use automatic metrics to show that proposed defense learning strategies maintain the utility .
For future work , we suggest working on flattening the distributions of attacker models .
We declare that all authors of this paper acknowledge the ACM Code of Ethics and honor the code of conduct .
This work essentially considers blackbox attacks on the private persona attributes and proposes effective learning strategies to prevent chatbots from overlearning private personas . 
Dataset .
During our dataset collection , all the conversations and personas are collected from publicly available datasets including PersonaChat and DNLI .
All the speakers are anonymized and no identifiable personal information is included . 
Model .
For training our LM - based chatbots , we follow standard methods .
We are well aware of the bias issue inside current language models .
In the future , if there are other fairer language models , we will extend our defenses on them .
formation about personas .
However , its attacking performance is poor .
The poor performance implies our proposed defense learning strategies may obfuscate Attacker for estimating single sample f ( u ) and finally make the wrong prediction .
To show an intuition view on utility , we provide one generation sample shown in Figure 5 .
Both LM and LM+KL+MI are able to generate fluent and proper relies .
Moreover , they tend to maintain coherence with previous contexts .
For example , it is mentioned in the context that Human B is a vegan and both chatbots respond that they do not eat meat for the food preference .
This generation example shows that proposed defense learning objectives preserve the model utility .
Here , we give two more examples of the persona inference attacks in Table 6 .
The first example shows one successful defense .
For the second example , both attackers with and without defense fail to predict the ground truth persona .
Still , we can see that LM+KL+MI predicts personas that are irrelevant to the context .
However , LM 's output " I know how to play the guitar . " is much closer to the context about music and instruments .
Without any defense , the above examples show that the attacker model can still predict context - aware personas even if its predictions are wrong .
After applying the proposed defenses , the attacker model can not predict meaningful personas relevant to the context .
Previous experiments mainly consider accuracy as the evaluation metric .
In this section , we use top - k accuracy for the black - box persona inference attacks to measure privacy protection .
As shown in Table 5 , our defense is much more robust than LM when k ≤ 50 .
When k is larger than 500 , the defense degrades rapidly as k increases .
This result implies that the ground truth personas mostly lie in the top 2 , 000 predictions even if the defense is applied .
For a smaller k , our proposed defense learning strategies are still effective .
The authors of this paper were supported by the NSFC Fund ( U20B2053 ) from the NSFC of China , the RIF ( R6020 - 19 and R6021 - 20 ) and the GRF ( 16211520
For each conversation , the utterances are concatenated by the special token " < | endoftext | > " to train the GPT - 2 .
To decode outputs from GPT - 2 , we apply the Nucleus Sampling ( Holtzman et al , 2020 ) method .
We set top - p = 0.9 with a temperature coefficient 0.9 to sample words from the GPT - 2 .
For optimization , we set 2 AdamW optimizers ( Loshchilov and Hutter , 2019 ) for the chatbot and the persona predictor respectively .
The learning rate is 3e - 5 with linear warm - up and decay .
For hyper - parameters , we set λ 0 = 1 , λ 1 = 10 and λ 2 = 1 .
To make predictions on personas , the arg max function is used for the estimated distribution of persona predictors .
However , the internal distribution conveys crucial information about how the persona predictors estimate f ( u ) .
We follow the setup of imbalanced data split of 8 clusters in Section 5.5 to examine persona predictors of attacker A and fake attacker
A p . 
Figure 4 shows the data distribution of the test set and average distribution after softmax activation over the 8 labels for attacker A and defender A p .
For attacker A , we consider the attack on LM and LM+KL+MI .
The defender A p tends to have a large difference with Data and tries to flatten its distribution among its own training set ( the last 5 labels ) .
This behavior conforms to the KL loss 's objective that aims to flatten the distribution and deviate from the ground truth distribution .
For attacker A , distributions of both LM and LM+KL+MI seem close to the ground truth distribution .
This indicates that the attacker model
A can still learn statistical in -
