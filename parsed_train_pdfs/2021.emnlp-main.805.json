{"authors": "Soumya Sanyal; Xiang Ren", "pub_date": "", "title": "Discretized Integrated Gradients for Explaining Language Models", "abstract": "As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the model's output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the gradients computed at the interpolated points and consequently, the quality of the generated explanations. Here we propose Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space, yielding more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encourage reproducible research 1 .", "sections": [{"heading": "Introduction", "text": "In the past few years, natural language processing has seen tremendous progress, largely due to strong performances yielded by pre-trained language models (Devlin et al., 2019;Radford et al., 2019;Brown et al., 2020). But even with this impressive performance, it can still be difficult to understand the underlying reasoning for the preferred predictions leading to distrust among end-users (Lipton, 2018). Hence, improving model interpretability has become a central focus in the community with an increasing effort in developing methods that can explain model behaviors (Ribeiro et al., 2016;Binder et al., 2016;Li et al., 2016;Sundararajan et  2017; Shrikumar et al., 2017;Lundberg and Lee, 2017;Murdoch et al., 2018).\nExplanations in NLP are typically represented at a word-level or phrase-level by quantifying the contributions of the words or phrases to the model's prediction by a scalar score. These explanation methods are commonly referred as attributionbased methods (Murdoch et al., 2018;Ancona et al., 2018). Integrated Gradients (IG) (Sundararajan et al., 2017) is a prominent attribution-based explanation method used due to the many desirable explanation axioms and ease of gradient computation. It computes the partial derivatives of the model output with respect to each input feature as the features are interpolated along a straight-line path from the given input to a baseline value. For example, say we want to compute the attribution for the word \"good\" in the sentence \"the movie was good!\" using IG. The straight-line interpolation path used by IG is depicted in green in Figure 1. Here, the baseline word is defined as the \"<pad>\" embedding and the green squares are the intermediate interpolation points in the embedding space.\nWhile this method can be used for attributing inputs in both continuous (e.g., image, audio, etc.) and discrete (e.g., text, molecules, etc.) domains (Sundararajan et al., 2017), their usage in the dis-crete domain has some limitations. Since the interpolation is done along a straight-line path joining the input word embedding and the baseline embedding (\"<pad>\" in Figure 1), the interpolated points are not necessarily representative of the discrete word embedding distribution. Specifically, let a dummy word embedding space be defined by the words represented by black dots in Figure 1. Then we can see that some of the green squares can be very far-off from any original word in the embedding space. Since the underlying language model is trained to effectively work with the specific word embedding space as input, using these out-of-distribution green interpolated samples as intermediate inputs to calculate gradients can lead to sub-optimal attributions.\nTo mitigate these limitations, we propose a Discretized integrated gradients (DIG) formulation by relaxing the constraints of searching for interpolation points along a straight-line path. Relaxing this linear-path constraint leads to a new constraint on the interpolation paths in DIG that points along the path should be monotonically situated between the input word embedding and the baseline embedding. Hence, in DIG, our main objective is to monotonically interpolate between the input word embedding and baseline such that the intermediate points are close to real data samples. This would ensure that the interpolated points are more representative of the word embedding distribution, enabling more faithful model gradient computations. To this end, we propose two interpolation strategies that search for an optimal anchor word embedding in the real data space and then modify it such that it lies monotonically between the input word and baseline (see Fig. 1 for an illustration).\nWe apply DIG using our proposed interpolation algorithms to generate attributions for three pre-trained language models -BERT (Devlin et al., 2019), DistilBERT (Sanh et al., 2020), and RoBERTa (Liu et al., 2019), each fine-tuned separately on three sentiment classification datasets -SST2 (Socher et al., 2013), IMDB (Maas et al., 2011), and Rotten Tomatoes (Pang and Lee, 2005). We find that our proposed interpolation strategies achieve a superior performance compared to integrated gradients and other gradient-based baselines on eight out of the nine settings across different metrics. Further, we also observe that on average, end-users find explanations provided by DIG to be more plausible justifications of model behavior than the explanations from other baselines.", "n_publication_ref": 21, "n_figure_ref": 4}, {"heading": "Method", "text": "In this section, we first describe our proposed Discretized integrated gradients (DIG) and the desirable explanation axioms satisfied by it. Then we describe an interpolation algorithm that leverages our DIG in discrete textual domains. Please refer to Appendix A for a brief introduction of the attribution-based explanation setup and the integrated gradients method.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Discretized integrated gradients", "text": "Below, we define our DIG formulation that allows interpolations along non-linear paths:\nDIG i (x) = x i x k i =x i \u2202F x k \u2202x i dx k i .(1)\nHere, x k i refers to the i th dimension of the k th interpolated point between input x and baseline x and F is a neural network. The only constraint on x k i 's is that each interpolation should be monotonic between x i and x i , i.e., \u2200j, k \u2208 {1, ..., m}; j < k,\nx i \u2264 x j i \u2264 x k i \u2264 x i if x i \u2264 x i , x i \u2265 x j i \u2265 x k i \u2265 x i otherwise. (2\n)\nHere m is the total number of steps for interpolation. This constraint is essential because it allows approximating the integral in Eq. 1 using Riemann summation 2 which requires monotonic paths. We note that the interpolation points used by IG naturally satisfy this constraint since they lie along a straight line joining x and x . The key distinction of our formulation from IG is that DIG is agnostic of any fixed step size parameter \u03b1 and thus allows non-linear interpolation paths in the embedding space. The integral approximation of DIG is defined as follows:\nDIG approx i (x) = \u03a3 m k=1 \u2202F x k \u2202x i \u00d7 x k+1 i \u2212 x k i ,(3)\nwhere m is the total number of steps considered for the approximation.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Axioms satisfied by DIG", "text": "As described in prior works (Sundararajan et al., 2017;Shrikumar et al., 2017), a good explanation w w2 w6 w1 w5 w3 w4 w' c (a) DIG-GREEDY w w2 [7] w6 [8] w1 [5] w5 [12] w3 [10] w4 [20] w' c (b) DIG-MAXCOUNT Figure 2: Overview of paths used in DIG and IG. The gray region is the neighborhood of w. Green line depicts the straight-line path used by IG. Left: In DIG-GREEDY, we first monotonize each word in the neighborhood (red arrow) and the word closest to its corresponding monotonic point is selected as the anchor (w 5 since the red arrow of w 5 has the smallest magnitude). Right: In DIG-MAXCOUNT we select the word with the highest number of monotonic dimensions (count shown in [.]) as the anchor word (w 4 ), followed by changing the non-monotonic dimensions of w 4 (red arrow to c). Repeating this iteratively gives the non-linear blue path for DIG with the red stars as interpolation points. Please refer to Section 2.1 for more details. Figure best viewed in color. algorithm should satisfy certain desirable axioms which justify the use of the algorithm for generating model explanations. Similar to IG, DIG also satisfies many such desirable axioms. First, DIG satisfies Implementation Invariance which states that attributions should be identical for two functionally equivalent models. Two models are functionally equivalent if they have the same output for the same input, irrespective of any differences in the model's internal implementation design. Further, DIG satisfies Completeness which states that the sum of the attributions for an input should add up to the difference between the output of the model at the input and the baseline, i.e., i DIG i (x) = F (x)\u2212F (x ). This ensures that if F (x ) \u2248 0 then the output is completely attributed to the inputs. Thirdly, DIG satisfies Sensitivity which states that attributions of inputs should be zero if the model does not depend (mathematically) on the input. Please refer to Appendix B for further comparisons of DIG with IG.", "n_publication_ref": 9, "n_figure_ref": 1}, {"heading": "Interpolation algorithm", "text": "Here, we describe our proposed interpolation algorithm that searches for intermediate interpolation points between the input word embedding and the baseline embedding. Once we have the desired interpolation points, we can use Equation 3 to compute the word attributions similar to the IG algorithm. Please refer to Section A.2 for more details about application of IG to text. Design Consideration. First, we discuss the key design considerations we need to consider of our interpolation algorithm. Clearly, our interpolation points need to satisfy the monotonicity constraints defined in Equation 2 so that we can use the Riemann sum approximation of DIG. Hence, we need to ensure that every intermediate point lies in a monotonic path. Also, the interpolation points should lie close to the original words in the embedding space to ensure that the model gradients faithfully define the model behavior. Now, we define the notion of closeness for our specific use-case of explaining textual models. To calculate how far the interpolated words are from some true word embedding in the vocabulary, we can compute the distance of the interpolated point from the nearest word in the vocabulary. We define this as the word-approximation error (WAE). More specifically, if w k denotes the k th interpolation point for a word w, then its word-approximation error along the interpolated path is defined as:\nWAE w = 1 m m k=1 min x\u2208V dist(w k \u2212 x),(4)\nwhere V is the embedding matrix of all the words in the vocabulary. WAE of a sentence is the average WAE of all words in the sentence. Intuitively, minimizing WAE will ensure that the interpolated points are close to some real word embedding in the vocabulary which in turn ensures that output gradients of F are not computed for some out-ofdistribution unseen embedding points.\nWe observe that to minimize WAE without the monotonic constraints defined in Section 2.1, one can define some heuristic to search for interpolation points that belong to the set V (i.e., select words from the vocabulary as interpolation points), leading to a zero WAE. Motivated by this, for a given input word embedding, we first search for an anchor word from the vocabulary that can be considered as the next interpolation point. Since the anchor point need not be monotonic w.r.t. the given input, we then optimally perturb the dimensions of the anchor word so that they satisfy the monotonicity constraints in Equation 2. This perturbed point becomes our first interpolation. For subsequent interpolation points, we repeat the above steps using the previous anchor and perturbed points. Formally, we break our interpolation algorithm into two parts:\n(i) ANCHORSEARCH: In this step, given the initial word embedding w, we search for an anchor word embedding a \u2208 V .\n(ii) MONOTONIZE: This step takes the anchor embedding a and modifies its dimensions to create a new embedding c such that all dimensions of c are monotonic between the input w and the baseline w .\nOverall, given an initial input word embedding w and a baseline embedding w , our interpolation algorithm interpolates points from w to w (which is in decreasing order of k in Eq. 3). It proceeds by calling ANCHORSEARCH on w to get an anchor word a. Then, it applies MONOTONIZE on a to get the monotonic embedding c. This is our first interpolated point (in reverse order), i.e., c = w m\u22121 . Now, the a becomes the new w for the next iteration and the process continues till m steps. Next, we describe in detail our specific formulations of the MONOTONIZE and ANCHORSEARCH algorithms.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "MONOTONIZE:", "text": "In this step, given an anchor word embedding a, we modify the non-monotonic dimensions of a such that they become monotonic w.r.t. w and w . The monotonic dimensions of a vector a is given by:\nM a = {j | w j \u2264 a j \u2264 w j , j \u2208 {1, ..., D}} \u222a {j | w j \u2265 a j \u2265 w j , j \u2208 {1, ..., D}},\nwhere D is the word embedding dimension. The number of monotonic dimensions is given by the size of the set defined as |M a |. Thus, the non-monotonic dimensions M a is the set complement of the monotonic dimensions, i.e., M a = {1, ..., D} \u2212 M a , where the subtraction is the setdiff operation. Let the final monotonic vector be c. We define the MONOTONIZE operations as follows:\nc[M a ] \u2190 a[M a ], c[M a ] \u2190 w[M a ] \u2212 1 m \u00d7 (w[M a ] \u2212 w [M a ]),\nwhere m is the total number of interpolation points we want to select in the path. It can be easily seen that c is monotonic w.r.t. w and w according to the definition in Equation 2.\nANCHORSEARCH: First, we preprocess the word embedding in V to find the top-K nearest neighbor for each word. We consider this neighborhood for candidate anchor selection. Let us denote the Kneighbors for a word w by KN N V (w). We define two heuristics to search for the next anchor word: GREEDY and MAXCOUNT.\nIn the GREEDY heuristic, we first compute the monotonic embedding corresponding to each word in the neighborhood KN N V (w) using the MONO-TONIZE step. Then, we select the anchor word a that is closest to its corresponding monotonic embedding obtained from the above step. This can be thought of as minimizing the WAE metric for a single interpolated word. The key intuition here is to locally optimize for smallest perturbations at each iterative selection step. This heuristic is depicted in Figure 2a and the algorithm is presented in Algorithm 1 in Appendix.\nIn the MAXCOUNT heuristic, we select the anchor a as the word in KN N V (w) with the highest number of monotonic dimensions. Precisely, the anchor is given by:\na = arg max a \u2208KN N V (w) |M a |.\nThe intuition of this heuristic is that the vector with highest number of monotonic dimensions would require the minimum number of dimensions being perturbed in the MONOTONIZE step and hence, would be close to a word in the vocabulary. This heuristic is depicted in Figure 2b and the algorithm is presented in Algorithm 2 in Appendix.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Experimental Setup", "text": "In this section, we describe the datasets and models used for evaluating our proposed algorithm.     (Wolf et al., 2020b).\nMethod DistilBERT RoBERTa BERT LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 Grad*\nMethod DistilBERT RoBERTa BERT LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 Grad*\nLanguage Models. We use pre-trained BERT (Devlin et al., 2019), DistilBERT (Sanh et al., 2020), and RoBERTa (Liu et al., 2019) text classification models individually fine-tuned for SST2, IMDB, and RT datasets. 4 The fine-tuned checkpoints used are provided by the HuggingFace library (Wolf et al., 2020a).\nEvaluation Metrics. Following prior literature, we use the following three automated metrics:\n\u2022 Log-odds (LO) score (Shrikumar et al., 2017) is defined as the average difference of the negative logarithmic probabilities on the predicted class before and after masking the top k% words with zero padding. Lower scores are better.\n\u2022 Comprehensiveness (Comp) score (DeYoung et al., 2020) is the average difference of the change in predicted class probability before and after removing the top k% words. Similar to Log-odds, this measures the influence of the top-attributed words on the model's prediction. Higher scores are better.\n\u2022 Sufficiency (Suff) score (DeYoung et al., 2020) is defined as the average difference of the change in predicted class probability before and after keeping only the top k% words. This measures the adequacy of the top k% attributions for model's prediction.\nPlease refer to Appendix C for more details about the evaluation metrics. We use k = 20% in our experiments. In Appendix D we further analyze the effect of changing top-k% on the metrics. Additionally, we use our proposed word-approximation error (WAE) metric to compare DIG with IG.  \nMethod DistilBERT RoBERTa BERT LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 LO \u2193 Comp \u2191 Suff \u2193 WAE \u2193 Grad*", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Performance Comparison", "text": "We compare DIG with four representative gradientbased explanation methods -Gradient*Input (Grad*Inp) (Shrikumar et al., 2016), DeepLIFT (Shrikumar et al., 2017), GradShap (Lundberg and Lee, 2017), and integrated gradients (Sundararajan et al., 2017). For the IMDB and RT datasets, we randomly sample a subset of 2,000 reviews from the public test sets to compare the different methods, due to computation costs. For the SST2 dataset, we use the complete set of 1,821 test sentences. The results are shown in Tables 1, 2, and 3 for SST2, IMDB, and Rotten Tomatoes respectively.\nComparison with baselines. First, we observe that across the nine different settings we studied (three language models per dataset), DIG consistently outperforms the baselines on eight of the settings. This is valid for all the metrics. We also note that the WAE metric is lower for all variants of DIG compared to IG. This validates that our proposed interpolation strategies for DIG is able to considerably reduce the word-approximation error in the interpolated paths and consistently improving performance on all three explanation evaluation metrics considered.\nComparison between variants of DIG. Second, we observe that on average, DIG-GREEDY performs better than DIG-MAXCOUNT. Specifically, we find that DIG-MAXCOUNT doesn't outperform DIG-GREEDY by significantly large margins on any setting (while the opposite is true for one setting -RoBERTa fine-tuned on IMDB dataset). This could be because the DIG-GREEDY strategy ensures that the monotonic point c is always close to the anchor a due to the locally greedy selection at each step which is not explicitly guaranteed by DIG-MAXCOUNT. But overall, we do not find any  Analysis. Finally, though we are able to achieve good reductions in WAE, we note that the WAE for our interpolation algorithms are not close to zero yet. This leaves some scope to design better interpolation algorithms in future. Moreover, we find that the average Pearson correlation between log-odds and WAE is 0.32 and the correlation is 0.45 if we consider the eight settings where we outperform IG. We discuss the correlations of all the settings in Appendix E. While this suggests a weak correlation between the two metrics, it is hard to comment if there is a causality between the two. This is partially because we believe selection of interpolation points should also take the semantics of the perturbed sentences into consideration, which we don't strongly enforce in our strategies. Hence, we think that constraining interpolations in a semantically meaningful way is a promising direction to explore.  ", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Human Evaluation", "text": "To further understand the impact of our algorithm on end users, we conduct human evaluations of explanations from our method and the two top baselines -IG and GradShap. We perform the study on the DistilBERT model fine-tuned on SST2 dataset and the BERT model fine-tuned on Rotten Tomatoes dataset. Further, we select the best variant of DIG on each dataset for explanation comparisons. First, we pick 50 sample sentences from each dataset with lengths between 5 and 25 words for easier visualizations. Then, we convert the attributions from each method into word highlights, whose intensity is determined by the magnitude of the attributions. Finally, we show the highlighted sentence and the model's predicted label to the annotators and ask them to rank the explanations on a scale of 1-3, \"1\" being the most comprehensive explanation that best justifies the prediction.\nFigure 3 shows the mean rank of each explanation algorithm across the two datasets. We find that DIG has a significantly lower mean rank compared to IG (p < .001 on both SST2 and Rotten Tomatoes 5 ). Thus, we conclude that explanations generated by DIG are also trustworthy according to humans. Please refer to Appendix G for visualizations and discussion on explanations generated by our methods.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Performance Analysis", "text": "In this section, we report the ablation of AN-CHORSEARCH and the effect of path density on DIG. Please refer to Appendix F for ablations on neighborhood size and discussions on computational complexity.\nAblation Study on ANCHORSEARCH. We ablate our methods with two random variants -DIG-RANDOMANCHOR and DIG- 5 We compute the p\u2212value using Wilcoxon signed-rank test. RANDOMNEIGHBOR, in which the AN-CHORSEARCH step uses a random anchor selection heuristic. Specifically, in DIG-RANDOMANCHOR, the anchor is selected randomly from the complete vocabulary. Thus, this variant just ensures that the selected anchor is close to some word in the vocabulary which is not necessarily in the neighborhood.\nIn contrast, the DIG-RANDOMNEIGHBOR selects the anchor randomly from the neighborhood without using our proposed heuristics MAXCOUNT or GREEDY. The log-odds metrics of IG, the two ablations, and our best variant of DIG for DistilBERT fine-tuned individually on all three datasets are reported in Table 4. We report 5-seed average for the randomized baselines. We observe that DIG-RANDOMANCHOR improves upon IG on all three datasets. This shows that generating interpolation points close to the words in the vocabulary improve the explanation quality. Further, we observe that DIG-RANDOMNEIGHBOR improves upon DIG-RANDOMANCHOR on log-odds metric. One reason could be that the words in a neighborhood are more semantically relevant to the original word, leading to more coherent perturbations for evaluating model gradients. Finally, we observe that, on average, our proposed method is better compared to selecting a random anchor in the neighborhood. This shows that our search strategies MAXCOUNT and GREEDY are indeed helpful.\nEffect of Increasing Path Density. In integrated gradients, the completeness axiom (Section 2.2) is used to estimate if the integral approximation (Equation 6) error is low enough. This error is denoted as the Delta % error. If the error is high, users can increase the number of interpolation points m.\nWhile DIG also satisfies the completeness axiom,  error reduction by increasing m is infeasible. This is because increasing m in Equation 3 implicitly changes the integral path rather than increasing the density. Hence, to achieve an error reduction in DIG, we up-sample the interpolation path P = {w, w 1 , w 2 , . . . , w m\u22122 , w } with an up-sampling factor (f ) of one as follows:\nP 1 = {w, w+w 1 2 , w 1 , w 1 +w 2 2 , . . . , w m\u22122 +w 2 , w },\ni.e., we insert the mean of two consecutive points to the path. This essentially doubles the density of points in the path. Similarly, P 2 can be obtained by up-sampling P 1 , etc. DIG(m, f = 0) refers to the standard DIG with no up-sampling.\nGiven that we have two hyperparameters m and f that determine the overall path density, we analyze the effect of each of these in Figure 4 and Table 5 respectively. The results are shown for DIG-MAXCOUNT applied on DistilBERT model finetuned on SST2 dataset. In Figure 4, we observe that as m increases, the Delta % of IG decreases as expected. But the trend is opposite for DIG. As discussed above, for DIG, the path length increases with increasing m, and hence, we attribute this trend to increasing difficulty in effectively approximating the integral for longer paths. Next, in Table 5, we observe that as the up-sampling factor f increases, the Delta % consistently decreases. We also find that our up-sampling strategy does not increase the WAE by a significant amount with increasing f , which is desirable. Thus, this confirms that our up-sampling strategy is a good substitute of increasing m for IG to effectively reduce the integral approximation error Delta %. Following Sundararajan et al. (2017), we choose a threshold of 5% average Delta to select the hyperparameters. For more discussions, please refer to Appendix F.1.", "n_publication_ref": 2, "n_figure_ref": 2}, {"heading": "Related Works", "text": "There has been an increasing effort in developing interpretability algorithms that can help understand a neural network model's behavior by explaining their predictions (Doshi-Velez and Kim, 2017;Gilpin et al., 2019). Attributions are a post-hoc explanation class where input features are quantified by scalar scores indicating the magnitude of contribution of the features toward the predicted label. Explanation algorithms that generate attributions can be broadly classified into two categories -model-agnostic algorithms, like LIME (Ribeiro et al., 2016), Input occlusion (Li et al., 2016), Integrated gradients 6 (Sundararajan et al., 2017), SHAP (Lundberg and Lee, 2017), etc. and model-dependent algorithms, like LRP (Binder et al., 2016), DeepLIFT (Shrikumar et al., 2017), CD (Murdoch et al., 2018), ACD (Singh et al., 2019), SOC (Jin et al., 2020), etc. While the model-agnostic algorithms can be used as blackbox explanation tools that can work for any neural network architecture, for the latter, one needs to understand the network's architectural details to implement the explanation algorithm. Typically, model-dependent algorithms require specific layer decomposition rules (Ancona et al., 2018;Murdoch et al., 2018) which needs to be defined for all the components in the model. Model-agnostic methods usually work directly with the model outputs and gradients which are universally available.\nDue to the many desirable explanation axioms and ease of gradient computation, there has been several extensions of integrated gradients. For example, Miglani et al. (2020) study the effect of saturation in the saliency maps generated by integrated gradients. Merrill et al. (2019) extend integrated gradients to certain classes of discontinuous functions in financial domains. Further, Jha et al. (2020) use KNNs and auto-encoders to learn latent paths for RNAs. Different from prior work, our focus here is to improve integrated gradients specifically for the discrete textual domain. While the idea of learning latent paths for text data is quite interesting, it brings a significant amount of challenge in successfully modeling such a complex latent space and hence, we leave this for future work.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we proposed Discretized integrated gradients (DIG) which is effective in explaining models working with discrete text data. Further, we proposed two interpolation strategies -DIG-GREEDY and DIG-MAXCOUNT that generate non-linear interpolation paths for word embedding space. Finally, we established the effectiveness of DIG over integrated gradients and other gradientbased baselines through experiments on multiple language models and datasets. We also conduct human evaluations and find that DIG enhances human trust on model predictions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Preliminaries", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 Attribution-based Explanations", "text": "Attribution-based explanations generate a scalar score for a given input feature that indicates the contribution (or importance) of that feature towards particular label (Ancona et al., 2018). Formally, let x = [x 1 , . . . , x N ] \u2208 R N be an input to a model which produces an output y = [y 1 , . . . , y C ], where C is the total number of labels. For a given label (usually the label predicted by the model), attribution-based explanation methods compute the contribution R c = [R c 1 , . . . , R c N ] \u2208 R N of each feature.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A.2 Integrated gradients", "text": "Integrated gradients (IG) (Sundararajan et al., 2017) for an input x along the i th dimension is defined as follows:\nIG i (x) = (x i \u2212 x i ) \u00d7 1 \u03b1=0 \u2202F (x +\u03b1\u00d7(x\u2212x )) \u2202x i\nd\u03b1.\n(5) Here, F is the neural network, x is a baseline embedding, and \u03b1 is the step size. Simply put, integrated gradients algorithm works by sampling points at a uniform spacing along a straight-line between the input and the baseline, and summing the model's gradient at the inputs for each interpolated points. To compute this integral efficiently, the authors propose a Riemann summation approximation defined below:\nIG approx i (x) = (xi \u2212 x i ) \u00d7 \u03a3 m k=1 \u2202F (x + k m \u00d7(x\u2212x ))) \u2202x i \u00d7 1 m ,(6)\nwhere m is the total number of steps considered for the approximation. Next, we briefly describe how IG is used to explain a model's prediction which takes a sentence as input (for example, the model can be a text classification network). Let S = [w 0 ..w n ] be a sentence of length n and w i be the i th word embedding of the sentence. Also, let F be a text-classification model, i.e., y = F (S). Then, IG calculates the attribution for each dimension of a word embedding w i . The interpolation points required for Equation 6 are generated by linearly interpolating the word embedding between w i and a baseline word embedding (usually chosen as the pad embedding). Then, using Eq. 6, the attribution for the i th dimension of w is calculated. The final word attribution is the sum of the attributions for each dimension of the word embedding.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "B Comparison with Integrated gradients and Path methods", "text": "It is easy to see that the approximation of integrated gradients is a special case of DIG. Note that the k th linear interpolation of the i th dimension of input x for IG can be represented as:\nx k i = x i + k m \u00d7 (x i \u2212 x i ).(7)\nSubstituting Eq. 7 in Eq. 3 gives us Eq. 6. Sundararajan et al. (2017) define path methods as the general form of integrated gradients that are applicable for all monotonic paths between the input and the baseline. Our DIG approach is a reformulation of the path method where the paths are not necessarily parameterized by \u03b1, making it more applicable for discrete data domain. Hence, DIG also satisfies all the theoretical properties applicable for path methods -Implementation Invariance, Sensitivity, Linearity, and Completeness. We refer the readers to Proposition 2 in Sundararajan et al. (2017) for more technical details.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "C Evaluation Metrics", "text": "In this section, we redefine the evaluation metrics and state the formulations for each of them. In this work, we use the following three automated metrics:\n\u2022 Log-odds (LO) score (Shrikumar et al., 2017) is defined as the average difference of the negative logarithmic probabilities on the predicted class before and after masking the top k% features with zero padding. Given the attribution scores generated by an explanation algorithm, we select the top k% words based on their attributions replace them with zero padding. More concretely, for a dataset with N sentences, it is defined as:\nlog \u2212 odds(k) = 1 N N i=1 log p \u0177 | x (k) i p (\u0177 | x i ) ,\nwhere\u0177 is the predicted class, x i is the i th sentence, and\nx (k) i\nis the modified sentence with top k% words replaced with zero padding. Lower scores are better.\n\u2022 Comprehensiveness (Comp) score (DeYoung et al., 2020) is the average difference of the change in predicted class probability  before and after removing the top k% features. Similar to Log-odds, this measures the influence of the top-attributed words on the model's prediction. It is defined as:\nComp(k) = 1 N N i=1 p(\u0177 | x (k) i ) \u2212 p(\u0177 | x i ).\nHere\nx (k)\ni denotes the modified sentence with top k% words deleted from the sentence. Higher scores are better.\n\u2022 Sufficiency (Suff) score (DeYoung et al., 2020) is defined as the average difference of the change in predicted class probability before and after keeping only the top k% features. This measures the adequacy of the top k% attributions for model's prediction. It is defined in a similar fashion as comprehensiveness, except the x (k)\ni is defined as the sentence containing only the top k% words. Lower scores are better.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "D Effect of top-k in evaluation metrics", "text": "In Figure 5, we visualize the effect of changing top-k% on log-odds, comprehensiveness, and sufficiency metrics for DistilBERT model fine-tuned on the SST2 dataset. We compare the two variants of our method: DIG-GREEDY and DIG-MAXCOUNT with Integrated Gradients. We observe that our method outperforms IG for all values of k. Specifically, we note that the gap between DIG and IG is initially non-existent but then gradually increases with increasing k in Figure 5 (a) and eventually saturates. This shows that although IG might be equally good as DIG at finding the top-5% important words, the explanations from IG are significantly misaligned from true model behavior for higher top-k values.   ", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "E Correlation between Log-odds and WAE", "text": "We compute the Pearson correlation between logodds and WAE for each dataset + LM pair. For this, we consider the metric values for IG, DIG-GREEDY, and DIG-MAXCOUNT and report the correlations for each setting in Table 6. We observe that, there is a strong correlation on average for DistilBERT. For BERT and RoBERTa we find a weak positive and negative correlation respectively.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "F Ablation Studies", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "F.1 Effect of increasing path density", "text": "Here, we report the detailed analysis of the effect of increasing m and f in Tables 7 and 8 respectively. In Table 7, we report the Log-odds score along with Delta %. We do not note any consistent trend in Log-odds with increasing m for both IG and DIG.\nThe results of IG suggest that, as long as the Delta % is sufficiently low, decreasing Delta % any further doesn't impact the explanations very significantly. Further, in ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "F.2 Effect of increasing neighborhood size", "text": "In this section, we study the effect of increasing the neighborhood size in DIG. The results are shown in Table 9. We observe a clear decreasing trend in Delta % with increasing neighborhood size, but there is no clear trend on Log-odds or WAE. Hence, we believe that the neighborhood size has little impact on the explanation quality, but we should still ensure sufficiently low Delta.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "F.3 Discussion on computational complexity", "text": "In this section, we briefly discuss the computational complexity of our proposed interpolation strategies. The algorithms for DIG-GREEDY and DIG-MAXCOUNT are presented in Algorithms 1 and 2 respectively. From there, we observe that both our algorithms have a running time complexity of O(nmK), where n is the number of words, m is the number of interpolation points, and K is the KN N V neighborhood size. While it is computationally feasible to parallelize the loops corresponding to n and K, the same cannot be said for the loop corresponding to m because we select the interpolation points iteratively. Although we empirically find in Section F.1 that a small number of interpolation points are sufficient to calculate the explanations, we believe this bottleneck can be further tackled through efficient design of noniterative search algorithms. We leave this for future works.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "G Visualizations of explanations", "text": "In this section, we present some interesting sentence visualizations based on explanations from DIG and IG for SST2 dataset in Figure 6. We show the sentence visualization and the model's predicted sentiment for the sentence for each explanation algorithm. In the visualizations, the red highlighted words denote positive attributions and blue denotes negative attributions. That is, the explanation model suggests that the red highlighted words support the predicted label whereas the blue ones oppose (or undermine) the prediction. We observe that in many cases, DIG is able to highlight more plausible explanations. For example, in sentence pairs 1-7, clearly the DIG highlights are more inline with the model prediction. But we want to emphasize that it does not mean that our method always produces more plausible highlights. For example, for sentences 8-10, we observe that highlights from IG are more plausible than those of DIG. Hence, this shows that, while it could be a good exercise to visualize the attributions as a sanity check, we should rely more on automated metrics and human evaluations to correctly compare explanation algorithms.   6: Some example visualizations of attributions from DIG and IG for the DistilBERT model fine-tuned on SST2 dataset. The sentence visualization is followed by model's sentiment prediction for the sentence. Here, the red highlighted words denote positive attributions and blue denotes negative attributions. For more details, please refer to Appendix G", "n_publication_ref": 0, "n_figure_ref": 2}], "references": [{"title": "Towards better understanding of gradient-based attribution methods for deep neural networks", "journal": "", "year": "2018-04-30", "authors": "Marco Ancona; Enea Ceolini; Cengiz \u00d6ztireli; Markus Gross"}, {"title": "Layer-wise relevance propagation for neural networks with local renormalization layers", "journal": "Springer", "year": "2016", "authors": "Alexander Binder; Gr\u00e9goire Montavon; Sebastian Lapuschkin; Klaus-Robert M\u00fcller; Wojciech Samek"}, {"title": "Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "", "year": "", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "ERASER: A benchmark to evaluate rationalized NLP models", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Jay Deyoung; Sarthak Jain; Nazneen Fatema Rajani; Eric Lehman; Caiming Xiong; Richard Socher; Byron C Wallace"}, {"title": "Towards a rigorous science of interpretable machine learning", "journal": "", "year": "2017", "authors": "Finale Doshi; - Velez; Been Kim"}, {"title": "Explaining explanations: An overview of interpretability of machine learning", "journal": "", "year": "2019", "authors": "Leilani H Gilpin; David Bau; Ben Z Yuan; Ayesha Bajwa; Michael Specter; Lalana Kagal"}, {"title": "Enhanced integrated gradients: improving interpretability of deep learning models using splicing codes as a case study", "journal": "Genome Biology", "year": "2020", "authors": "Anupama Jha; Joseph K Aicher; Matthew R Gazzara; Deependra Singh; Yoseph Barash"}, {"title": "Towards hierarchical importance attribution: Explaining compositional semantics for neural sequence models", "journal": "", "year": "2020-04-26", "authors": "Xisen Jin; Zhongyu Wei; Junyi Du; Xiangyang Xue; Xiang Ren"}, {"title": "Understanding neural networks through representation erasure", "journal": "", "year": "2016", "authors": "Jiwei Li; Will Monroe; Dan Jurafsky"}, {"title": "The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue", "journal": "", "year": "2018", "authors": "Zachary C Lipton"}, {"title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du An"}, {"title": "A unified approach to interpreting model predictions", "journal": "", "year": "2017-12-04", "authors": "M Scott; Su-In Lundberg;  Lee"}, {"title": "Learning word vectors for sentiment analysis", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Andrew L Maas; Raymond E Daly; Peter T Pham; Dan Huang; Andrew Y Ng; Christopher Potts"}, {"title": "Generalized integrated gradients: A practical method for explaining diverse ensembles", "journal": "", "year": "2019", "authors": "John Merrill; Geoff Ward; Sean Kamkar; Jay Budzik; Douglas Merrill"}, {"title": "Bilal Alsallakh, Miguel Martin, and Orion Reblitz-Richardson. 2020. Investigating saturation effects in integrated gradients", "journal": "", "year": "", "authors": "Vivek Miglani; Narine Kokhlikyan"}, {"title": "Beyond word importance: Contextual decomposition to extract interactions from lstms", "journal": "", "year": "2018-04-30", "authors": "W ; James Murdoch; Peter J Liu; Bin Yu"}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "journal": "", "year": "2005", "authors": "Bo Pang; Lillian Lee"}, {"title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeff Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"title": "why should I trust you?\": Explaining the predictions of any classifier", "journal": "ACM", "year": "2016-08-13", "authors": "Sameer Marco T\u00falio Ribeiro; Carlos Singh;  Guestrin"}, {"title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "journal": "", "year": "2020", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"title": "Learning important features through propagating activation differences", "journal": "PMLR", "year": "2017-08-11", "authors": "Avanti Shrikumar; Peyton Greenside; Anshul Kundaje"}, {"title": "Not just a black box: Learning important features through propagating activation differences", "journal": "", "year": "2016", "authors": "Avanti Shrikumar; Peyton Greenside; Anna Shcherbina; Anshul Kundaje"}, {"title": "Hierarchical interpretations for neural network predictions", "journal": "", "year": "2019-05-06", "authors": "Chandan Singh; W James Murdoch; Bin Yu"}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "journal": "", "year": "2013", "authors": "Richard Socher; Alex Perelygin; Jean Wu; Jason Chuang; Christopher D Manning; Andrew Ng; Christopher Potts"}, {"title": "Axiomatic attribution for deep networks", "journal": "PMLR", "year": "2017-08-11", "authors": "Mukund Sundararajan; Ankur Taly; Qiqi Yan"}, {"title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger;  Drame"}, {"title": "Fran\u00e7ois Lagunas, Lysandre Debut, Morgan Funtowicz", "journal": "", "year": "", "authors": "Thomas Wolf; Quentin Lhoest; Yacine Patrick Von Platen; Mariama Jernite; Julien Drame; Julien Plu; Clement Chaumond; Clara Delangue; Abhishek Ma; Suraj Thakur; Joe Patil; Teven Le Davison; Victor Scao; Canwen Sanh; Nicolas Xu; Angie Patry; Simon Mcmillan-Major; Sylvain Brandeis;  Gugger"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An illustration of paths used in IG and DIG. IG uses a straight line interpolation with points as depicted by green squares. In contrast, DIG uses a nonlinear path (shown in blue) with interpolation points (red stars) lying close to words in the embedding space.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Result of human evaluation on DistilBERT model fine-tuned on SST2 dataset and BERT model fine-tuned on Rotten Tomatoes dataset. A lower mean rank means higher trustworthy explanation algorithm. For more details, refer to Section 4.2", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Effect of increasing number of interpolation points m on IG and DIG.", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Algorithm 1 :1DIG-GREEDY Input :Sentence s = [w 1 , w 2 , . . . w n ], k-nearest neighbor graph for the vocabulary KN N V , number of interpolation points m Output :Interpolations1 points = [ ] n * m 2 for i \u2190 1 to n do 3 for j \u2190 1 to m do 4 dists = { } 5 for k \u2190 1 to K do 6 nbr \u2190 KN N V (w i )[k]7 c \u2190 MO N O T O N I Z E(nbr, w i ) 8 dists[nbr] \u2190 Distance(nbr, c ) 9 end for 10 a \u2190 arg min a \u2208dists dists[a ] 11 c \u2190 MO N O T O N I Z E(a, w i ) DIG-MAXCOUNT Input :Sentence s = [w 1 , w 2 , . . . w n ], k-nearest neighbor graph for the vocabulary KN N V , number of interpolation points m Output :Interpolations 1 points = [ ] n * m 2 for i \u2190 1 to n do 3 for j \u2190 1 to m do 4 a \u2190 arg max a \u2208KN N V (w i ) |M a | 5 c \u2190 MO N O T O N I Z E(a, w i )", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": "FigureFigure6: Some example visualizations of attributions from DIG and IG for the DistilBERT model fine-tuned on SST2 dataset. The sentence visualization is followed by model's sentiment prediction for the sentence. Here, the red highlighted words denote positive attributions and blue denotes negative attributions. For more details, please refer to Appendix G", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Comparison of variants of DIG with baselines on three LMs fine-tuned on SST2 dataset. '-' denotes that the WAE metric is not computable for that setting. We observe that DIG outperforms the baselines on all three LMs. Please refer to Section 4.1 for more details.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Comparison of variants of DIG with baselines on three LMs fine-tuned on IMDB dataset. We observe that DIG outperforms the baselines on DistilBERT and BERT models. Please refer to Section 4.1 for more details.", "figure_data": "Datasets. The SST2 (Socher et al., 2013) datasethas 6920/872/1821 example sentences in thetrain/dev/test sets. The task is binary classifica-tion into positive/negative sentiment. The IMDB(Maas et al., 2011) dataset has 25000/25000 exam-ple reviews in the train/test sets with similar binarylabels for positive and negative sentiment. Sim-ilarly, the Rotten Tomatoes (RT) (Pang and Lee,2005) dataset has 5331 positive and 5331 negativereview sentences. We use the processed datasetmade available by HuggingFace Dataset library 3"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Comparison of variants of DIG with baselines on three LMs fine-tuned on Rotten Tomatoes dataset. We observe that DIG outperforms the baselines on all three LMs. Please refer to Section 4.1 for more details.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": ".217 \u00b1 0.024 -0.834 \u00b1 0.021 -0.474 \u00b1 0.003 DIG-RANDOMNEIGHBOR -1.247 \u00b1 0.013 -0.854 \u00b1 0.015 -0.460 \u00b1 0.010", "figure_data": "MethodSST2IMDBRTIG-0.950-0.446-0.424DIG-RANDOMANCHOR -1DIG (best)-1.259-0.878-0.501"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ":Comparison of DIG with two abla-tion variants -DIG-RANDOMANCHOR and DIG-RANDOMNEIGHBOR on the DistilBERT model. We re-port 5-seed average log-odds score for the randomizedmethods. Please refer to Section 4.3 for more details."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Factor f Log-Odds \u2193 WAE \u2193 Delta % \u2193", "figure_data": "0-1.2590.2274.9261-1.2290.2303.7282-1.1840.2322.7523-1.1810.2331.862"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Effect of up-sampling a path by a factor f on Delta % for DIG using m = 30.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Pearson correlation between log-odds and WAE metrics for different dataset+LM settings. Please refer to Appendix E for more details.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Effect of increasing number of interpolation points m on Delta % for IG and DIG. Please refer to Appendix F.1 for more details.", "figure_data": "Up-sampling factor f Log-Odds \u2193 WAE \u2193 Delta % \u2193DIG (m = 30, f = 0)-1.2590.2274.926DIG (m = 30, f = 1)-1.2290.2303.728DIG (m = 30, f = 2)-1.1840.2322.752DIG (m = 30, f = 3)-1.1810.2331.862"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "", "figure_data": ": Effect of up-sampling a path by a factor f onDelta % for DIG. For more details, refer to AppendixF.1."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "", "figure_data": ", we report the WAE metrics toemphasize that our up-sampling strategy doesn'tincrease the WAE by a significant amount, whichis desirable. Also, we note a consistent increase(although marginally) in Log-odds with decreasingDelta %. But per our previous observations on IG,we believe these changes do not imply a causalrelation between the two."}], "doi": "10.18653/v1/N19-1423"}