{"authors": "Yatin Chaudhary; Pankaj Gupta; Hinrich Sch\u00fctze", "pub_date": "", "title": "BioNLP-OST 2019 RDoC Tasks: Multi-grain Neural Relevance Ranking Using Topics and Attention Based Query-Document-Sentence Interactions", "abstract": "This paper presents our system details and results of participation in the RDoC Tasks of BioNLP-OST 2019. Research Domain Criteria (RDoC) construct is a multi-dimensional and broad framework to describe mental health disorders by combining knowledge from genomics to behaviour. Non-availability of RDoC labelled dataset and tedious labelling process hinders the use of RDoC framework to reach its full potential in Biomedical research community and Healthcare industry. Therefore, Task-1 aims at retrieval and ranking of PubMed abstracts relevant to a given RDoC construct and Task-2 aims at extraction of the most relevant sentence from a given PubMed abstract. We investigate (1) attention based supervised neural topic model and SVM for retrieval and ranking of PubMed abstracts and, further utilize BM25 and other relevance measures for re-ranking, (2) supervised and unsupervised sentence ranking models utilizing multi-view representations comprising of query-aware attention-based sentence representation (QAR), bag-of-words (BoW) and TF-IDF. Our best systems achieved 1st rank and scored 0.86 mAP and 0.58 macro average accuracy in Task-1 and Task-2 respectively.", "sections": [{"heading": "Introduction", "text": "The scientific research output of the biomedical community is becoming more sub-domain specialized and increasing at a faster pace. Most of the biomedical domain knowledge is in the form of unstructured text data. Natural Language Processing (NLP) techniques such as relation extraction and information retrieval have enabled us to effectively mine relevant information from a large corpus. These techniques have significantly reduced the time and effort required for knowledge min-* : Equal Contribution ing and information extraction from past scientific studies and electronic health reports (EHR).\nInformation Retrieval (IR) is the process of retrieving relevant information from an unstructured text corpus, which satisfies a given query/requirement, for example Google search, email search, database search etc. This is generally achieved by converting the query and the document collection into an external representation which by preserving the important semantical information can reduce the IR processing time. This external representation can be generated using either statistical approach i.e., word counts or distributed semantical approach i.e., word embeddings. Therefore, there is a motivation to develop such IR system which can understand the specialized sub-domain language and domainspecific jargon of biomedical domain and assist researchers and medical professionals by effectively and efficiently retrieving most relevant information given a query.\nRDoC Tasks aims at exploring information retrieval (IR) and information extraction (IE) tasks on selected abstracts from PubMed dataset. While Task-1 aims to rank abstracts i.e., coarse granularity, Task-2 aims to rank sentences i.e., fine granularity and hence the term multi-grain. An RDoC construct combines information from multiple sources like genomics, symptoms, behaviour etc. and therefore, is a much broader way of describing mental health disorders than symptoms based approach. Table 1 shows the association between PubMed abstracts and RDoC constructs depending on the semantic knowledge of the highlighted content words. Both of these tasks aim in the direction of ease of accessibility of PubMed abstracts labelled with diverse RDoC constructs so that this information can reach its full potential and can be of help to biomedical researchers and healthcare professionals.  (PMID). Highlighted words (blue and red) in each abstract shows content words which together provide the semantic understanding of the corresponding RDoC constructs.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Task Description and Contributions", "text": "RDoc-IR Task-1: The task aims at retrieving and ranking the PubMed abstracts (within each of the eight clusters) that are relevant for the RDoC construct (i.e, a query) related to the cluster in the abstract appears. The training data consists of abstracts (title + sentences) each annotated with one or more RDoC constructs. Test data consists of abstracts without annotation and the goal is to submit a ranked lists of relevant articles for each medical domain RDoC construct.\nRDoc-IE Task-2 The task aims at extracting the most relevant sentence from each PubMed abstract for the corresponding RDoC construct. The input consists of an abstract (title t and sentences s) for an RDoC construct q. The training data consists of abstracts each annotated with one RDoC construct and the most relevant sentence. Test data contains abstracts relevant for RDoC constructs and the goal is to submit a list of predicted most relevant sentence for each abstract.\nOur Contributions: Following are our multifold contributions in this paper:\n(1) RDoC-IR Task-1: We perform document (or abstract) ranking in two steps, first using supervised neural topic model and SVM. Moreover, we have introduced attentions in supervised neural topic model, along with pre-trained word embeddings from several sources. Then, we re-rank documents using BM25 and similarity scores between query and query-aware attention-based document representation.\nComparing with other participating systems in the shared task, our submission is ranked 1 st with a mAP score of 0.86.\n(2) RDoC-IE Task-2: We have addressed the sentence ranking task by introducing unsupervised and supervised sentence ranking schemes. Moreover, we have employed multi-view representations consisting of bag-of-words, TF-IDF and query-aware attention-based sentence representation via enhanced query-sentence interactions. We have also investigated relevance of title with the sentences and coined ways to incorporate both query-sentence and title-sentence relevance scores in ranking sentences with an abstract.\nComparing with other participating systems in the shared task, our submission is ranked 1 st with a macro average accuracy of 0.58. Our code is available at https://github.com/ YatinChaudhary/RDoC_Task.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Methodology", "text": "In this section, we first describe representing a query, sentence and document using local and distributed representation schemes. We further describe enhanced query-document (query-title and query-content) and query-sentence interactions to compute query-aware document or sentence representations for Task-1 and Task-2, respectively. Finally, we discuss the application of supervised neural topic modeling in ranking documents for task 1 and introduce unsupervised and supervised sentence rankers for Task-2.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Query, Sentence and Document Vectors", "text": "In this paper, we deal with texts of different lengths in form of query, sentence and document. In this section, we describe the way we represent the different texts.\nBag-of-words (BoW) and Term frequencyinverse document frequency (TF-IDF): We use two the local representation schemes: BoW and   TF-IDF (Manning et al., 2008) to compute sentence/document vectors.\n\ua788 v 2 v 1 v 2 v i \ua788 v D h(v) ... h e (v)\nh 2 (v <2 ) h i (v <i ) U v ... \u03b1 D \u03b1 2 \u03b1 1 V i = p(V i | v <i ) \ua788 .\nEmbedding Sum Representation (ESR): Word embeddings (Mikolov et al., 2013;Pennington et al., 2014) have been successfully used in computing distributed representation of text snippets (short or long). In ESR scheme, we employ the pre-trained word embeddings from FastText (Bojanowski et al., 2017) and word2vec (Mikolov et al., 2013). To represent a text (query, sentence or document), we compute the sum of (pre-trained) word vectors of each word in the text. E.g., ESR for a document d with D words can be computed as:\nESR(d) = d = D i=1 e(d i )\nwhere, e \u2208 R E is the pre-trained embedding vector of dimension E for the word d i .\nQuery-aware Attention-based Representation (QAR) for Documents and Sentences: Unlike ESR, we reward the maximum matches between a query and document by computing density of matches between them, similar to McDonald et al. (2018). In doing so, we introduce a weighted sum of word vectors from pre-trained embeddings and therefore, incorporate importance/attention of certain words in document (or sentence) that appear in the query text.\nFor an enhanced query-aware attention based document (or sentence) representation, we first compute an histogram a i (d) \u2208 R D of attention weights for each word k in the document d (or sentence s) relative to the ith query word q i , using cosine similarity:\na i (d) = [a i,k ] D k=1 where, a i,k = e(q i ) T e(d k ) ||e(q i )|| ||e(d k )||\nfor each kth word in the document d. Here, e(w) refers to an embedding vector of the word w.\nWe then compute an query-aware attentionbased representation \u03a6 i (d) of document d from the viewpoint of ith query word by summing the word vectors of the document, weighted by their attention scores a i (d):\n\u03a6 i (d) = D k=1 a i,k (d) e(d k ) = a i (d) [e(d k )] D k=1\nwhere is an element-wise multiplication operator.\nNext, we compute density of matches between several words in query and the document by summing each of the attention histograms a i for all the query terms i. Therefore, the query-aware document representation for a document (or sentence) relative to all query words in q is given by:\nQAR(d) = \u03a6 q (d) = |q| i \u03a6 i (d) (1)\nSimilarly, a query-aware sentence representation \u03a6 q (s) and query-aware title representation \u03a6 q (t) can be computed for the sentence s and document title t, respectively.\nFor query representation, we use ESR scheme as q = |q| i=1 e(w i ). Figure 2 illustrates the computation of queryaware attention-based sentence representation.", "n_publication_ref": 6, "n_figure_ref": 1}, {"heading": "Document Neural Topic Models", "text": "Topic models (TMs) (Blei et al., 2003) have shown to capture thematic structures, i.e., topics appearing within the document collection. Beyond interpretability, topic models can extract latent document representation that is used to perform document retrieval. Recently, Gupta et al. (2019a) and Gupta et al. (2019b) have shown that the neural network-based topic models (NTM) outperform LDA-based topic models (Blei et al., 2003;Srivastava and Sutton, 2017) in terms of generalization, interpretability and document retrieval.\nIn order to perform document classification and retrieval, we have employed supervised version of neural topic model with extra features and further introduced word-level attention in a neural topic model, i.e. in DocNADE (Larochelle and Lauly, 2012;Gupta et al., 2019a).\nSupervised NTM (SupDocNADE): Document Neural Autoregressive Distribution Estimator (DocNADE) is a neural network based topic model that works on bag-of-words (BoW) representation to model a document collection in a language modeling fashion.\nConsider a document d, represented as v = [v 1 , ..., v i , ..., v D ] of size D, where v i \u2208 {1, ..., Z} is the index of ith word in the vocabulary and Z is the vocabulary size. DocNADE models the joint distribution p(v) of document v by decomposing p(v) into autoregressive conditional of each word v i in the document, i.e., p(v)\n= D i=1 p(v i |v <i ), where v <i \u2208 {v 1 , ..., v i\u22121 }.\nAs shown in Figure 1 (left), DocNADE computes each autoregressive conditional p(v i |v <i ) using a feed forward neural network for i \u2208 {1, ..., D} as,\np(v i = w|v <i ) = exp(b w + U w,: h(v <i )) w exp(b w + U w ,: h(v <i )) h i (v <i ) = f (c + j<i W :,v j )\nwhere, f (\u2022) is a non-linear activation function, W \u2208 R H\u00d7Z and U \u2208 R Z\u00d7H are encoding and decoding matrices, c \u2208 R H and b \u2208 R Z are encoding and decoding biases, H is the number of units in latent representation h i (v <i ). Here, h i (v <i ) contains information of words preceding the word v i . For a document v, the log-likelihood L(v) and latent representation h(v) are given as,\nL unsup (v) = D i=1 log p(v i |v <i ) (2) h(v) = f (c + D i=1 W :,v i )(3)\nHere, L(v) is used to optimize the topic model in unsupervised fashion and h(v) encodes the topic proportion. See Gupta et al. (2019a) for further details on training unsupervised DocNADE.\nHere, we extend the unsupervised version to DocNADE with a hybrid cost L hybrid (v), consisting of a (supervised) discriminative training cost p(y = q|v) along with an unsupervised generative cost p(v) for a given query q and associated document v:\nL hybrid (v) = L sup (v) + \u03bb \u2022 L unsup (v)(4)\nwhere \u03bb \u2208 [0, 1]. The supervised cost is given by:\nL sup (v) = p(y = q|v) = softmax(d + S h(v))\nHere, S \u2208 R L\u00d7H and d \u2208 R L are output matrix and bias, L is the total number of unique RDoC constructs (i.e., unique query labels). Supervised Attention-based NTM (a-SupDocNADE): Observe in equation 3 that the DocNADE computes document representation h(v) via aggregation of word embedding vectors without considering attention over certain words. However, certain content words own high important, especially in classification task. Therefore, we have introduced attention-based embedding aggregation in supDocNADE (Figure 1, left):\nh(v) = f (c + D i=1 \u03b1 i W :,v i ) (5)\nHere, \u03b1 i is an attention score of each word i in the document v, learned via supervised training. Additionally, we incorporate extra word features, such as pre-trained word embeddings from several sources: FastText (E f ast ) (Bojanowski et al., 2017) and word2vec (E word2vec ) (Mikolov et al., 2013). We introduce these features by concatenating h e (v) with h(v) in the supervised portion of the a-supDocNADE model, as  Therefore, the classification portion of a-supDocNADE with additional features is given by:\nh e (v) = f c + D i=1 \u03b1 i (E f ast :,v i + E word2vec :,v i )(6\np(q|v) = softmax(d + S \u2022 concat(h(v), h e (v)))\nwhere, S \u2208 R H \u00d7L and H = H + E f ast + E word2vec .", "n_publication_ref": 10, "n_figure_ref": 2}, {"heading": "Traditional Methods for IR", "text": "BM25: A ranking function proposed by Robertson and Zaragoza ( 2009) is used to estimate the relevance of a document for a given query.\nBM25-Extra: The relevance score of BM-25 is combined with four extra features: (1) percentage of query words with exact match in the document, (2) percentage of query words bigrams matched in the document, (3) IDF weighted document vector for feature #1, and (4) IDF weighted document vector for feature #2. Therefore, BM25-Extra returns a vector of 5 scores.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "System Description for RDoC Task-1", "text": "RDoC Task-1 aims at retrieving and ranking of PubMed abstracts (title and content) that are relevant for 8 RDoC constructs. Participants are provided with 8 clusters, each with a RDoC construct label and required to rank abstracts within each cluster based on their relevance to the corresponding cluster label. Each cluster contains abstracts relevant to its RDoC construct, while some (or most) of the abstracts are noisy in the sense that they belong to a different RDoC construct. Ideally, the participants are required to rank abstracts in each of the clusters by determining their relevance with the RDoC construct of the cluster in which they appear.\nTo address the RDoc Task-1, we learn a mapping function between latent representation h(v) of a document (i.e.., abstract) v and its RDoC construct, i.e., query words q in a supervised fashion. In doing so, we have employed supervised classifiers, especially supervised neural topic model a-supDocNADE (section 3.2) for document ranking. We treat q as label and maximize p(q|v) leading to maximize L hybrid (v) in a-supDocNADE model.\nAs demonstrated in Figure 1 (right), we perform document ranking in two steps:\n(1) Document Relevance Ranking: We build a supervised classifier using all the training documents and their corresponding labels (RDoC constructs), provided with the training set. At the test time, we compute prediction probability score p(CID = q|v test (CID))) of the label=CID for each test document v test (CID) in the cluster, CID. This prediction probability (or confidence score) is treated as a relevance score of the document for the RDoC construct of the cluster. Figure 1(right) shows that we perform document ranking using the probability scores (col-2) of the RDoC construct (e.g. loss) within the cluster C1.\nObserve that a test document with least confidence for a cluster are ranked lower within the cluster and thus, improving mean average precision (mAP). Additionally, we also show the predicted RDoC construct in col-1 by the supervised classifier.\n(2) Document Relevance Re-ranking: Secondly, we re-ranked each document v (ti-tle+abstract) within each cluster (with label q) using unsupervised ranking, where the relevance scores are computed as: (a) reRank(BM25-Extra): sum each of the 5 relevance scores to get the final relevance, and (b) reRank(QAR): cosine-similarity(QAR(v), q).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "System Description for RDoC Task-2", "text": "The RDoC Task-2 aims at extracting the most relevant sentence from each of the PubMed abstract for the corresponding RDoC construct. Each abstract consists of title t and sentences s with an RDoC construct q.\nTo address RDoc Task-2, we first compute multi-view representation: BoW, TF-IDF and QAR (i.e., \u03a6 q (s j )) for each sentence s j in an abstract d. On other hand, we compute ESR representation for RDoC construct (query q) and title t of the abstract d to obtain q and t, respectively. Figure 2 and section 3.1 describe the computation of these representations. We then use the representations (\u03a6 q (s j ), t and q) to compute a relevance scores of a sentence s j relative to q and/or t via unsupervised and supervised ranking schemes, discussed in the following section.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Unsupervised Sentence Ranker", "text": "As shown in Figure 2, we first extract representations: \u03a6 q (s j ), t and q for the sentence s j query q and title t. During ranking sentences within an abstract for the given RDoC construct q, we also consider title t in computing the relevance score for each sentence relative to q and t. It is inspired from the fact that the title often contains relevant terms (or words) appearing in sentence(s) of the document (or abstract). On top, we observe that q is a very short text and non-descriptive, leading to minimal text overlap with s.\nWe compute two relevance scores: r q and r t for a sentence s j with respect to a query q and title t, respectively. r q = sim( q, \u03a6 q (s j )) and r t = sim( t, \u03a6 q (s j ))\nNow, we devise two ways to combine the rele-vance scores r q and r t in unsupervised paradigm:\nversion1: r unsup 1 = r q \u2022 r q + r t \u2022 r t\nObserve that the relevance scores are weighted by itself. However, the task-2 expects a higher importance to the relevance score r q over q t . Therefore, we coin the following weighting scheme to give higher importance to r q only if it is higher than r t otherwise we compute a weight factor r t for r t .\nversion2: r unsup 2 = r q \u2022 r q + r t \u2022 r t\nwhere r t is compute as:\nr t = (r t > r q )|r t \u2212 r q |\nThe relevance score r unsup 2 is effective in ranking sentences when a query and sentence does not overlap. In such a scenario, a sentence is scored by title, penalized by a factor of |r t \u2212 r q |.\nAt the end, we obtain a final relevance score r unsup f for a sentence s j by summing the relevance scores of BM25-Extra and r unsup 1 or r unsup 2 .", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Supervised Sentence Ranker", "text": "Beyond unsupervised ranking, we further investigate sentence ranking in supervised paradigm by introducing a distance metric between the query (or title) and sentence vectors.\nFigure 2 describes the computation of relevance score for a sentence s j using a supervised sentence ranker scheme. Like the unsupervised ranker (section 3.5.1), the supervised ranker also employs vector representations: \u03a6 q (s j ), t and q. Using the projection matrix G, we then apply a projection to each of the representation to obtain \u03a6 p q (s j ), t p and q p . Here, the operator \u2297 performs concatenation of the projected vector with its input via residual connection. Next, we apply a Manhattan distance metric to compute similarity (or relevance) scores, following :\nr sup = exp \u2212 ||(\u03a6 p q (s j ), q p ) + \u03b2 (\u03a6 p q (s j ), t p )|| 2\nwhere \u03b2 \u2208 [0, 1] controls the relevance of title, determined by cross-validation. A final relevance score r sup f \u2208 [0, 1] is computed by feeding a vector [r sup , r sup siamese , BM25-extra] into a supervised linear regression, which is trained end-to-end by minimizing mean squared error between the r sup f and {0, 1}, i.e., 1 when the sentence s j is relevant to query q. Here, r sup siamese refers to a relevance   Best mAP score for each model is marked in bold.\n(reRank #1: \"reRank(BM25-Extra)\"; reRank #2: \"reRank(QAR)\"; reRank #3: \"reRank(BM25-Extra) + reRank(QAR)\") score computed between q and s j via Siamese-LSTM .\nTo perform sentence ranking within an abstract for a given RDoC construct q, the relevance score r sup f (or r unsup f ) is computed for all the sentences and a sentence with the highest score is extracted.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Experiments and Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Data Statistics and Experimental Setup", "text": "Dataset Description: Dataset for RDoC Tasks contains a total of 266 PubMed abstracts labelled with 8 RDoC constructs in a single label fashion. Number of abstracts for each RDoC construct is described in Table 2, where first row describes the statistics for all abstracts and second & third row shows the split of those abstracts into training and development sets maintaining a 80-20 ratio for each RDoC construct. For Task-1, each PubMed abstract contains its associated title, PubMed ID (PMID) and label (RDoC construct). In addition for Task-2, each PubMed abstract also contains a list of most relevant sentences from that abstract. Final evaluation test data for Task-1 & Task-2 contains 999 & 244 abstracts respectively.\nWe use \"RegexpTokenizer\" from scikit-learn to tokenize abstracts and lower-cased all tokens. After this, we remove those tokens which occur in less than 3 abstracts and also remove stopwords (using nltk). For computing BM25-Extra relevance score, we use unprocessed raw text of sentences and titles.\nExperimental Setup: As the training dataset labelled with RDoC constructs is very small, we use an external source of semantical knowledge by incorporating pretrained distributional word embeddings (Zhang et al., 2019) from FastText model (Bojanowski et al., 2017) trained on the entire corpus of PubMed and MIMIC III Clinical notes (Johnson et al., 2016). Similarly, we also use pretrained word embeddings (Moen and Ananiadou, 2013) from word2vec model (Mikolov et al., 2013) trained on PubMed and PMC abstracts. We create 3 folds * of train/dev splits for cross-validation.\nRDoC Task-1: For DocNADE topic model, we use latent representation of size 50. We use pretrained FastText embeddings of size 300 and pretrained word2vec embeddings of size 200. For SVM, we use Bag-of-words (BoW) representation of abstracts with radial basis kernel function. PubMed abstracts are provided in eight different clusters, one for each RDoC construct, for final test set evaluation.\nRDoC Task-2: We use pretrained FastText embeddings to compute query-aware sentence representation of a sentence (\u03a6 q (s j )), title ( t) and query ( q) representations. We also train Replicated-Siamese-LSTM  model with input as sentence and query pair i.e., (s j , q) and label as 1 if s j is relevant otherwise 0. We use \u03b2 \u2208 {0, 1}.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Results: RDoC Task-1", "text": "Table 3 shows the performance of supervised Document Ranker models i.e, a-supDocNADE and SVM, for Task-1. SVM achieves a classification accuracy of 0.947 and mean average precision  . It shows that an intruder/noisy abstract (Gold Label: Loss) is assigned higher probability than the abstracts with same Gold Label as the cluster. But, using re-ranking with BM25-Extra (reRank(BM25-Extra)) relevance score assigns lowest relevance to the intruder abstract.\n(mAP) of 0.992 by ranking the abstracts in their respective clusters using the supervised prediction probabilities (p(q|v)). After that, we use three different relevance scores: (1) reRank(BM25-Extra),\n(2) reRank(QAR) and ( 3) reRank(BM25-Extra) + reRank(QAR), for re-ranking of the abstracts in their respective clusters. It is to be noted that the ranking mAP of the clusters using prediction probabilities is already the best possible i.e., the intruder abstracts (abstracts with different label (RDoC construct) than the cluster label) are at the bottom of the ranked clusters. Therefore, re-ranking of these clusters would not achieve a better score. Similarly, we train a-supDocNADE model with three different settings: (1) random weight initialization, (2) incorporating FastText embeddings (h e (v)) and (3) incorporating Fast-Text and word2vec embeddings (h e (v)). By using the pretrained embeddings, the classification accuracy increases from 0.912 to 0.965, this shows that distributional pretrained embeddings carry significant semantic knowledge. Furthermore, re-ranking using reRank(BM25-Extra) and reRank(QAR) further results in the improvement of mAP score (0.994 vs 0.983) by shifting the intruder documents at the bottom of each impure cluster.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Analysis: RDoC Task-1", "text": "Table 4 shows an impure \"Potential Threat Anxiety\" cluster of abstracts containing an intruder abstract with label (RDoC construct) \"Loss\". When this cluster is ranked on the basis of predic-  tion probabilities (p(q|v)), then \"Loss\" abstract is ranked third from the bottom and it degrades the mAP score of the retrieval system. But after re-ranking this cluster using reRank(BM25-Extra) relevance score, the \"Loss\" abstract is ranked at the bottom, thus maximizing the mAP score. Therefore, re-ranking with BM25-Extra on top of ranking with p(q|v) is, evidently, a robust abstract/document ranking technique.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results: RDoC Task-2", "text": "Table 5 shows results for Task-2 using three unsupervised and two supervised sentence ranker models. For unsupervised model, using reRank(BM25-Extra) relevance score between a query (q), label (RDoC construct) of an abstract, and all the sentences (s j ) in an abstract, we get an macroaverage accuracy (MAA) of 0.631. However, using version1 and version2 models (see Fig 2), we achieve a MAA score of 0.701 and 0.526 respectively. Higher accuracy of version1 model suggests that title (t) of an abstract also contains the essential information regarding the most relevant sentence. For supervised model, we get an MAA score of 0.772 and 0.737 by setting \u03b2 = 0 & 1 in supervised relevance score (r sup f ) equation in section 3.5.2. Hence, for supervised sentence ranker model, title (t) is playing a negative influence in correctly identifying the relevance (r sup f ) of different sentences. Furthermore, we combine the knowledge of unsupervised and supervised sentence rankers by creating multiple ensembles (majority voting) of the predictions from different models. We achieve the highest MAA score of 0.789 by combining the predictions of (1) reRank(BM25-Extra), (2) version1, and (3) r sup f with \u03b2 = 0. Notice that all the proposed supervised and unsupervised sentence ranking mod-  --\nWe found that nurses experience a grieving process similar to those directly suffering from perinatal loss.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "#6 Relevant", "text": "Table 7: RDoC Task-2 analysis: This table shows that the most relevant sentence predicted using reRank(BM25-Extra) is actually not a relevant sentence, but Ensemble {#1, #2, #4} (Table 5) predicts the correct sentence as the most relevant.\nels (except [#3]) outperform tranditional ranking models, e.g., reRank(BM25-Extra) in terms of query-document relevance score.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Analysis: RDoC Task-2", "text": "Table 7 shows that the most relevant sentence predicted by reRank(BM25-Extra) is actually a non-relevant sentence. But an ensemble of predictions from both unsupervised and supervised ranker models correctly predicts the relevant sentence. This suggests that complementary knowledge of different models is able to capture the relevance of sentences on different scales and majority voting among them is, evidently, a robust sentence ranking technique.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results: RDoC Task 1 & 2 on Test set", "text": "Table 6 shows the final evaluation scores of different competing systems for both the RDoC Task-1 & Task-2 on final test set. Observe that our submission (MIC-CIS) scored a mAP score of 0.86 and MAA of 0.58 in Task-1 and Task-2, respectively. Notice that we outperform the second best system by 20.83% (0.58 vs 0.48) margin in Task2.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In conclusion, both supervised neural topic model and SVM can effectively perform ranking of PubMed abstracts in a given cluster based on the prediction probabilities. However, a further reranking using BM25-Extra or query-aware sentence representation (QAR) has proven to maximize the mAP score by correctly assigning the lowest relevance score to the intruder abstracts. Also, unsupervised and supervised sentence ranker models using query-title-sentence interactions outperform the traditional BM25-Extra based ranking model by a significant margin.\nIn future, we would like to introduce complementary feature representation via hidden vectors of LSTM jointly with topic models and would like to further investigate the interpretability (Gupta et al., 2015; of the proposed neural ranking models in the sense that one can extract salient patterns determining relationship between query and text. Another promising direction would be introduce abstract information, such as part-of-speech and named entity tags (Lample et al., 2016;Gupta et al., 2016) to augment information retrieval (IR).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Acknowledgment", "text": "This research was supported by Bundeswirtschaftsministerium (bmwi.de), grant 01MD19003E (PLASS (plass.io)) at Siemens AG -CT Machine Intelligence, Munich Germany.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Latent dirichlet allocation", "journal": "J. Mach. Learn. Res", "year": "2003", "authors": "David M Blei; Andrew Y Ng; Michael I Jordan"}, {"title": "Enriching word vectors with subword information", "journal": "TACL", "year": "2017", "authors": "Piotr Bojanowski; Edouard Grave; Armand Joulin; Tomas Mikolov"}, {"title": "Replicated siamese LSTM in ticketing system for similarity learning and retrieval in asymmetric texts", "journal": "", "year": "2018-08-20", "authors": "Pankaj Gupta; Bernt Andrassy; Hinrich Sch\u00fctze"}, {"title": "Document informed neural autoregressive topic models with distributional prior", "journal": "", "year": "2019-01-27", "authors": "Pankaj Gupta; Yatin Chaudhary; Florian Buettner; Hinrich Sch\u00fctze"}, {"title": "Texttovec: Deep contextualized neural autoregressive topic models of language with distributed compositional prior", "journal": "", "year": "2019-05-06", "authors": "Pankaj Gupta; Yatin Chaudhary; Florian Buettner; Hinrich Sch\u00fctze"}, {"title": "Deep Learning Methods for the Extraction of Relations in Natural Language Text", "journal": "", "year": "2015", "authors": "Pankaj Gupta; Thomas Runkler; Heike Adel; Bernt Andrassy; Hans-Georg Zimmermann; Hinrich Sch\u00fctze"}, {"title": "LISA: explaining recurrent neural network judgments via layer-wise semantic accumulation and example to pattern transformation", "journal": "", "year": "2018-11-01", "authors": "Pankaj Gupta; Hinrich Sch\u00fctze"}, {"title": "Table filling multi-task recurrent neural network for joint entity and relation extraction", "journal": "", "year": "2016-12-11", "authors": "Pankaj Gupta; Hinrich Sch\u00fctze; Bernt Andrassy"}, {"title": "Mimic-iii, a freely accessible critical care database", "journal": "Scientific data", "year": "2016", "authors": "E W Alistair;  Johnson; J Tom; Lu Pollard; H Lehman Shen; Mengling Li-Wei; Mohammad Feng; Benjamin Ghassemi; Peter Moody; Leo Anthony Szolovits; Roger G Celi;  Mark"}, {"title": "Neural architectures for named entity recognition", "journal": "", "year": "2016-06-12", "authors": "Guillaume Lample; Miguel Ballesteros; Sandeep Subramanian; Kazuya Kawakami; Chris Dyer"}, {"title": "A neural autoregressive topic model", "journal": "", "year": "2012", "authors": "Hugo Larochelle; Stanislas Lauly"}, {"title": "Introduction to information retrieval", "journal": "Cambridge University Press", "year": "2008", "authors": "Christopher D Manning; Prabhakar Raghavan; Hinrich Sch\u00fctze"}, {"title": "Deep relevance ranking using enhanced document-query interactions", "journal": "", "year": "2018-10-31", "authors": "Ryan Mcdonald; George Brokos"}, {"title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013-12-05", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Gregory S Corrado; Jeffrey Dean"}, {"title": "Distributional semantics resources for biomedical text processing", "journal": "", "year": "2013", "authors": "Tapio Salakoski Sophia Spfgh Moen;  Ananiadou"}, {"title": "Glove: Global vectors for word representation", "journal": "", "year": "2014-10-25", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"title": "The probabilistic relevance framework: BM25 and beyond", "journal": "Foundations and Trends in Information Retrieval", "year": "2009", "authors": "E Stephen; Hugo Robertson;  Zaragoza"}, {"title": "Autoencoding variational inference for topic models", "journal": "", "year": "2017-04-24", "authors": "Akash Srivastava; Charles A Sutton"}, {"title": "Biowordvec, improving biomedical word embeddings with subword information and mesh", "journal": "Scientific data", "year": "2019", "authors": "Yijia Zhang; Qingyu Chen; Zhihao Yang; Hongfei Lin; Zhiyong Lu"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: (Left) DocNADE Topic Model: Blue colored lines signify parameter sharing, \u03b1 attention weights are used to compute latent document representation h(v); (Right) RDoC Task-1 system architecture, where the numbered arrow (1) denotes the flow. Col-1 indicates \"predicted label\" by the DocRanker and Col-2 indicates \"prediction probability\" (p(q|v)). \"Features\" inside DocRanker indicates FastText and word2vec pretrained embeddings.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: RDoC Task-2 System Architecture for Supervised and Unsupervised sentence ranking, consisting of: Query-aware Representation, Supervised and Unsupervised sentence rankers for computing the final relevance scores r sup f and r unsup f", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "RDoC construct -This table shows two PubMed abstracts labelled with two different RDoC construct and PubMed ID", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "v 1 \ua788p(v)d 1 (tr) Training Documents LossTest DocumentClustersd 1 (te) d 2 (te) d 3 (te) C1 :: Loss...C8 :: ATFh 1 (v <1 ) Wp(q|v)d 2 (tr) d 3 (tr) d 4 (tr) d 5 (tr) ...ATF PTA Arousal ATF ...(2) Rank Documents within each Cluster based on the prediction probability of the corresponding cluster ID DocRanker { DocNADE, SVM } + Features (3) Ranked Documents based on the Prediction probabilities of the (1) Building Classification based Document Ranker corresponding cluster IDSd N (tr)Lossd 1 (te) d 3 (te)Loss Loss...d 3 (te) d 2 (te)ATF ATFd 2 (te)PTAd 1 (te)LossC1 :: LossC8 :: ATF"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Data statistics -# of PubMed abstracts belonging to each RDoC construct in different data partitions.", "figure_data": "(L1: \"Acute Threat Fear\"; L2: \"Arousal\"; L3: \"Cir-cadian Rhythms\"; L4: \"Frustrative Nonreward\"; L5:\"Loss\"; L6: \"Potential Threat Anxiety\"; L7: \"SleepWakefulness\"; L8: \"Sustained Threat\")ModelFeatureClassification AccuracyRanking mAPSVMBoW0.9470.992Re-rankingreRank #1-0.992withreRank #2-0.992cluster labelreRank #3-0.992Random init0.9120.930a-supDocNADE+ FastText0.9470.949+ BioNLP0.9650.983Re-rankingreRank #1-0.985withreRank #2-0.994cluster labelreRank #3-0.994"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ": RDoC Task-1 analysis: Ranking ofPubMed abstracts within \"Potential Threat Anxiety(PTA)\" cluster using supervised prediction probabil-ities (p(q|v))"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ": RDoC Task-2 results (on development set):Performance of unsupervised and supervised sentencerankers (Figure 2) under different configurations. Bestscores for each model is marked in bold."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": ".92 1.00 0.73 0.78 0.94 1.00 0.63 0.86 0.74 0.73 0.47 0.37 0.74 0.59 0.60 0.42 0.58 Javad Rafiei Asl 0.89 0.93 1.00 0.69 0.74 0.87 1.00 0.64 0.85 0.68 0.62 0.47 0.34 0.56 0.32 0.50 0.36 0.48 Ramya Tekumalla 0.83 0.91 1.00 0.67 0.71 0.89 1.00 0.64 0.83 0.37 0.12 0.10 0.11 0.26 0.15 0.17 0.14 0.18 Daniel Laden 0.67 0.84 1.00 0.61 0.61 0.81 0.98 0..46 0.70 0.43 0.26 0.41 0.33 0.47 0.46", "figure_data": "TeamL1L2RDoC Task-1 (Official Results) L3 L4 L5 L6 L7L8 mAPL1L2RDoC Task-2 (Official Results) L3 L4 L5 L6 L7L8 MAAMIC-CIS0.85 041 0.74---------Shyaman Jayasundara---------0.47 0.42 0.60 0.29 0.62 0.38 0.57 0.47 0.48Fei Li---------0.58 0"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "RDoC Tasks official results -performance on test set of different competing systems. Best score in each column is marked in bold. (Refer to Table 2 for header notations) (mAP: \"Mean Average Precision\"; MAA: Macro-Average Accuracy)", "figure_data": "PubMed Abstract(PMID: \"23386529\"; RDoC construct: \"Loss\")Most Relevant SentenceSentenceGold(using reRank(BM25-Extra))IDLabelNurses are expected to care forgrieving women and families#1Not relevantsuffering from perinatal loss.Most Relevant Sentence(using Ensemble {#1, #2, #4})"}], "doi": ""}