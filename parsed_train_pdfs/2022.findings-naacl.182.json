{"authors": "Kurt Shuster; Jack Urbanek; Arthur Szlam; Jason Weston", "pub_date": "", "title": "Am I Me or You? State-of-the-Art Dialogue Models Cannot Maintain an Identity", "abstract": "State-of-the-art dialogue models still often stumble with regards to factual accuracy and self-contradiction. Anecdotally, they have been observed to fail to maintain character identity throughout discourse; and more specifically, may take on the role of their interlocutor. In this work we formalize and quantify this deficiency, and show experimentally through human evaluations that this is indeed a problem. In contrast, we show that discriminative models trained specifically to recognize who is speaking can perform well; and further, these can be used as automated metrics. Finally, we evaluate a wide variety of mitigation methods, including changes to model architecture, training protocol, and decoding strategy. Our best models reduce mistaken identity issues by nearly 65% according to human annotators, while simultaneously improving engagingness. Despite these results, we find that maintaining character identity still remains a challenging problem.", "sections": [{"heading": "Introduction", "text": "The exchange of stories from one's past, or descriptions of activities in one's present, are a fundamental part of human discourse. Trustworthy human conversationalists keep their stories roughly straight within a conversation. An interlocutor taking on your own stories and persona as theirs is especially jarring and unnatural. However, despite the improvements in state-of-the-art open-domain dialogue modeling, both in terms of distributional accuracy metrics like perplexity, and subjectively in terms of human judgements (Adiwardana et al., 2020;Roller et al., 2021), interactions with those agents reveal that they cannot keep their stories straight. In particular, they are likely to take on the role of their interlocutor; for example, if an agent's partner says they are a software engineer, the agent is likely to say it is a software engineer too (Roller et al., 2021), or worse, appropriate their partners just told tale of a trip to NAACL as their own. Some   (Roller et al., 2021) and fine-tuned on LIGHT (Urbanek et al., 2019). The bold words in red highlight the model mistaking its identity for its partner's. (Top) The model believes it is a thief, rather than a guest. (Bottom) The model believes it is a hunter rather than a helper. Token probabilities are given at the position of the mistake for the two names. example failure cases are given in Table 1, where models incorrectly take on the name, role or activities of their partner instead of their assigned role. These failures are related to the general problems of repetition in language models (Holtzman et al., 2020), the weak influence of word order (Sinha et al., 2021) and inability to avoid contradictions (Nie et al., 2021).\nIn this work we formalize and quantify this behavior, show that to some extent it can be detected automatically with a specifically trained classifier, and then study a wide variety of mitigations. These include multi-objective training, unlikelihood training , classifier-assisted re-ranking based generation, and several forms modifying the attention mechanisms of the decoder in a sequence to sequence model. Our best methods can reduce mistaken identity issues by 65%, while simultaneously improving inconversation engagingness; indeed, our models that can stick to their role in conversation are judged by humans to be significantly more engaging than their baseline counterparts. Despite these advances, we find that there is still considerable space to improve these results further in future work.\nWe make publicly available both our trained models and code to reproduce results 1 .", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Role-Playing in Open-Domain Dialogue Much recent work has explored training open-domain dialogue models on large and small dialogue corpora, with the former imbuing raw conversational ability and the latter providing necessary conversational skills. Most crowd-sourced datasets require acting out a role to some capacity in conversation (though indeed Mazar\u00e9 et al. (2018) study extraction of roles from raw data). Some involve providing persona lines that a model must assume throughout the conversation (Zhang et al., 2018;; others require more subtle \"roles\", such as a listener (Rashkin et al., 2019), or a teacher and student (Dinan et al., 2019b;Gopalakrishnan et al., 2019;Zhou et al., 2018;Komeili et al., 2021). Zheng et al. (2020) explore using a discriminative model to predict whether model responses contain similarity with their persona, similar to methods we employ in our work.\nConsistency in Open-Domain Dialogue A common paradigm in the state of the art of open-domain dialogue involves concatenating all relevant contextual information as input to a sequence to sequence neural model (e.g., transformers (Vaswani et al., 2017)) to obtain a conditioned response. Such models can yield human-like and engaging responses (Adiwardana et al., 2020;Roller et al., 2021). Nevertheless, various consistency issues still plague such models. Recent studies have indicated that hallucination of incorrect knowledge is still far from a solved issue Santhanam et al., 2021), with some proposing specific datasets and tools for measuring precisely the levels of this undesired attribute . Another clear example of failure is the short-term memory of state-of-the-art models , sometimes due to the lack of long-form training data or long-context models but often due to simply the modeling itself.\nTo address consistency issues, a variety of methods have been explored. In the context of knowledge-grounded dialogue, different ways to attend most effectively over provided contextual information have been explored (Zheng and Zhou, 2019;Ye et al., 2020;Prabhumoye et al., 2021;Wang et al., 2019). These works find that considering factual documents separately (in some capacity) improves model grounding. We explore such methods, but in the context of character identity.\nAnother general problem is that of contradictions. Nie et al. (2021) collect a dataset of contradictions in dialogue, and train classifiers that help re-rank model outputs at inference time;  explore unlikelihood training  to reduce repetition and contradiction, among other undesired traits, in model generations. The character identity issue we study in this work can be seen as an important class of contradictions, but to the best of our knowledge, has not been explicitly focused on.", "n_publication_ref": 17, "n_figure_ref": 0}, {"heading": "Methods", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Problem Setting", "text": "We consider a two-party chat setting. The context provided to a model includes: (i) the name of its character and the partner's character; (ii) an extended description of its own character; (iii) and, information about the area in which the conversation takes place. The responsibility of the model is to engage its conversational partner, with no other goal prescribed; however, it should stay within character and within the bounds of the defined setting.\nWe operate in the context of LIGHT (Urbanek et al., 2019), consisting of grounded fantasy roleplaying game conversations. The LIGHT environment involves humans and models interacting with thousands of objects in hundreds of locations, all while assuming the roles of one of hundreds of characters. The dataset consists of roughly 8.5k dialogues spanning 111k utterances. It is an ideal setting for this study because of the rich and varied personas with explicit backstories.\nTo quantify the character identity problem, we take a state-of-the-art dialogue agent (specifically, BlenderBot (Roller et al., 2021)) fine-tuned on the LIGHT dialogue dataset and ask human annotators if the agent mistakes its identity based on its utterances in context. The agent conditions its response on the LIGHT context and prior utterances in the dialogue history. We see in Table 4 that in roughly 6.5 percent of utterances the model mistakes its identity; this corresponds to a mistake in approximately 35 percent of conversations.\nBlenderBot uses a Byte-Level BPE tokenizer (Radford et al., 2019); an artifact from the Blender-Bot pre-training is that it only considers 128 such tokens in the past, and thus has no mechanism for recovering truncated information about the LIGHT context in later conversational turns. Our second baseline lengthens the input context to 1024 BPE tokens, which allows the entire context for every example to fit into the truncation length of the model; we follow methods employed in  to extend the positional embeddings of the model. We see in Table that this actually makes the problem worse, resulting in 7.4 percent of utterances with mistaken identity (corresponding to a failure in approximately 38 percent of conversations).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Measuring Role-Playing Accuracy: RPA", "text": "We first define a metric, role-playing accuracy (RPA), to denote how often a model's responses are \"in-character\"; by this, we mean how often the model's response could feasibly be said by their character, given their assigned character identity. Measuring RPA is a non-trivial task for a variety of reasons. First, some conversations involve pairs that can reasonably say similar things (priest vs. priestess, man vs. woman, wizard vs. witch). Second, opening lines are often more generic (\"hello\", \"how fare your travels today\"), so either character can say it in conversation. The third reason stems from the data that we study; we are relying on crowdsourced data in which humans are required to portray their characters. Some crowdworkers may be better than others, and there may be some noise in the dataset in which, e.g., a horse may proclaim its love for a queen, or a knight may discuss at length the kingdom's tax collecting.\nGiven the difficulties above, our primary measure of RPA involves human annotation of model responses, specifically evaluating whether a candidate response fits a given model's character. We thus have human crowdworkers chat with each model in a LIGHT setting; each is given a character and asked to role-play, while the human an-notates each model response, determining whether the model is in character: we denote this metric as \"Mistaken Identity\" in our experiments, and other utterance-level annotations are collected. Further details regarding human evaluation are outlined in Section 4.7.\nDespite the efficacy of human evaluation, it is both costly and slow; as a proxy, we thus train models specifically designed to identify whether a candidate response from a model fits the model's role, and denote these as \"RPA Classifiers\". We employ poly-encoder transformers (Humeau et al., 2020) to learn this metric, and structure the task as a ranking one; the model receives the LIGHT setting and prior utterances of dialogue as input, as well as the response currently under consideration, and the model must choose the correct character from a fixed set of candidates. We also explore RPA classifiers trained on all partially complete sequences of labels, such that the classifiers can determine the character speaking without requiring the full utterance; we call these left-to-right (LTR) RPA classifiers. Further details about how our RPA classifiers are built are given in Appendix B.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Mitigations", "text": "In this section we describe several strategies for improving the role-playing accuracy of dialogue agents, specifically ways to improve our transformer baselines.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Re-ranking Model Outputs via RPA", "text": "We can employ an RPA classifier in response generation by using it to rank candidate model outputs.\nUtterance Re-ranking: Given a set of candidate responses, the RPA classifier can re-score the set and return the response yielding the highest probability of staying in character (according to the RPA score on the complete candidate generations). The dialogue models employ beam-search to generate responses, and the candidates for re-ranking are the beams within beam-search. We also try nucleus sampling (Holtzman et al., 2020) and delayed beam-search (Massarelli et al., 2020) to see whether more diverse candidates have any effect. Partial And Complete Efficient Re-ranking (PACER): Re-ranking only the final beam candidates may be suboptimal because it is well known that those candidates are not very diverse (Kulikov et al., 2019), meaning there may not be any good candidates to choose from in this final set. In order to generate utterances that agree with our classifiers, a possible improvement is to generate the utterance such that partial generations also agree with the classifier when generating left-to-right, ensuring that good candidates are surfaced. With access to LTR RPA classifiers, we can apply re-ranking to partial sequences.\nUnfortunately, re-ranking at every step of beam search, for every token, requires significant computation, such as in the recent FUDGE method (Yang and Klein, 2021). FUDGE re-scores tokens at each decoding step by multiplying the classifier probability with each token probability, and renormalizing, which is used for control tasks with lightweight classifiers in order to be tractable.\nIn our proposed approach, called PACER, we re-score candidate tokens, for each beam, according to the probability that their inclusion yields the appropriate character classification, and then finally re-rank the complete candidate beams. To make this efficient, we crucially score only a small proportion of decoding steps (e.g., 5% of token positions) as well as for only a few candidate rescored tokens (e.g., top 10 only). We can control these hyperparameters to explore the speed vs. accuracy trade-off.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Unlikelihood", "text": "We explore utilizing an unlikelihood (UL) loss   While training on the LIGHT dataset with standard NLL loss, with some fixed probability we consider a candidate model generation for UL loss. The full generation is sent to the RPA classifier; if the generation is classified as coming from the incorrect character, we examine each partial generated sequence of the output, and send these sequences to the LTR RPA classifier to determine whether the candidate partial sequences match the model's character. We apply UL loss to tokens that yield the wrong character classification.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Multi-objective Training", "text": "The RPA classifiers utilize the LIGHT setting and prior utterances of dialogue history to determine which character generates a candidate response.\nWe hypothesize that the generation models themselves should be able to pick out and utilize these components as well. However, the RPA classifier models are trained explicitly for this task, whereas the seq2seq models are trained only to generate a plausible continuation of a dialogue history.\nWe thus explore a setup in which the generation models are trained to identify the speaker of an utterance as well. To do this, we use the output representations from the model (either encoder + decoder, or decoder only) as inputs to n M O additional transformer layers, where we vary n M O \u2208 {0, 2}. The final outputs are used to compute a character score, similarly to the RPA classifier.\nThe model can then be trained piece-wise. After initializing the model weights with those trained on the LIGHT response generation task, we then train only the extra layers with only the character classification objective; once the classifier achieves suitable performance on the task, we can begin to back-propagate the character classification objective multi-tasking with the dialogue task itself to the generation model directly, in the hope that the model learns to update its internal representations of the context and/or the decoded response.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Expanded Decoder Attention", "text": "Maintaining identity relies on the model's capacity to understand which inputs from the conversational history are pertinent when generating a continuation of the preceding dialogue. In a standard, opendomain chit-chat scenario, the model has free reign to decide which elements of the context it would like to condition on when generating a response, as we are dealing with a nearly unconstrained output space (so long as the output follows plausibly from the input). In LIGHT, however, we want to emphasize certain components of the context more so than others; specifically, when role-playing as a character, we want the model to always be reminded of its role, so that it can conditionally generate an optimal response while staying in character. In this lens, one can view the task as \"grounding\" on one's character information when conversing.\nProfile Grounding Inspired by models demonstrating good performance in knowledge-grounded dialogue (Zheng and Zhou, 2019;Ye et al., 2020;Prabhumoye et al., 2021;Wang et al., 2019), we propose a simple extension to the transformer seq2seq architecture, specifically the decoder, to ensure the model knows to condition on the pro-file. The standard transformer decoder first uses self-attention over the decoded response, and then cross-attention over the encoder outputs. We add a third attention step, expanded attention, that attends again over an extracted subset of the input context (encoded separately from the normal context). We explore various subsets of the context to determine which are most important for both RPA and other automated metrics, and call this method \"Profile\" grounding as the subsets generally include the character and role description. We utilize the exact same (shared) parameters for both the normal cross-attention and the expanded attention; thus, model size is not affected.\nAutomated Grounding Instead of directly telling the model what to re-attend to, we also explore whether the model can learn to do this automatically, based on its own (or other) representations of the context. The first method we consider is examining the decoder attention weights. Specifically, we use the attention weights from the decoder over the full context to choose k tokens to re-attend to. This operation is done on a per-layer basis, and thus allows different decoder layers to re-attend to (potentially different) components of the input.\nThe second method we consider is a trainable mask; this involves feeding the encoded context through a \"mask\" layer to select various tokens to re-attend to. Specifically, we feed the context through a linear projection layer followed by a softmax to select the top-k tokens. This set of tokens is then re-encoded by the encoder and fed to the decoder as the expanded attention context. Finally, we explore using the classifier attention weights over the context from the RPA classifier itself. Intuitively, the RPA classifier has learned what components of the input are necessary for determining which character is speaking; if we look at these attention weights when considering the model's character, we know what the classifier thinks is important to use.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Combined Methods", "text": "We also consider combining expanded attention with re-ranking methods, or with multi-objective training, to see if the combination can improve results. For the latter we use the automated grounding trainable mask method.   4 Experimental Results", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "RPA Classifiers", "text": "We first assess the quality of our RPA classifiers. We measure hits@1/427, where the model must correctly identify the character speaking out of 427 characters from the validation set, comparing the standard and left-to-right (LTR) models in Table 2. We experiment with either 0, 4, or All prior context utterances. The LTR classifiers perform nearly as well as the full classifiers on the full datasplit, and outperform them on the LTR split. Given the robustness of the LTR RPA classifiers, we use this model for computing RPA throughout the remaining results, unless otherwise specified. Further results are given in Appendix Table 10.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Baseline Generation Performance", "text": "We next train baseline models for the dialogue generation task itself. Performance on the LIGHT dataset test split for our baseline models can be found in  detailed training and optimization specifications are given in Appendix A.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "RPA Re-ranker Performance", "text": "Table 4 gives results for RPA-based re-ranking of generation models. Automated results show a slight bump in F1 on the LIGHT valid set, and indeed a bump in RPA. Including the intra-generation re-ranking with PACER yields an even higher RPA score. Table 3 contains the results of varying the candidate tokens re-ranked per intra-generation step (#Toks) and number of partial re-ranking steps (Freq), both in terms of generation metrics/RPA and relative computational cost compared to reranking. Increasing # of toks or increasing the frequency can lead to improved F1 and RPA, but with significant latency increase for too high values (e.g. over 11x when applying re-ranking for every partial step using the top 10 tokens each time). Applying both partial and final complete ranking helps performance. Note that re-ranker models use the same model to re-rank that is being used to measure RPA afterwards, making that metric biased. Hence, human evaluations are required for this model, which will be detailed in Section 4.7, and which will indicate that re-ranking does in fact help.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Unlikelihood", "text": "Results of unlikelihood (UL) training are also given in Table 4. We apply UL loss to the 128-truncation model in two different ways: (1) Top-1: apply the loss on the token that yields the most incorrect partial sequence RPA classification; (2) All: apply the loss to all tokens that yield an incorrect RPA classification on partial sequences. The RPA UL methods suffer compared to the baselines in terms of PPL and F1, yet they retain similar RPA metrics. We hypothesize that while the UL loss can adjust the model to refrain from generating outof-character responses, there are still far too many other tokens that may yield similar outcomes that are not penalized. Table 12 in Appendix D includes similar results with the 1024-truncation model.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Multi-Objective Training", "text": "Multi-objective training results are in Table 5, where the base model is a 1024-truncation model. We measure generation metrics in terms of RPA (with PPL and F1 in Table 13 in Appendix E), and classification metrics in terms of Hits@1/427 as before. The model is able to predict the appropriate character using either the decoder outputs or the en-coder+decoder outputs. hits@1 for the best model), this does not translate to substantial RPA improvements over the baseline.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Expanded Attention", "text": "Profile Grounding Expanding the decoder attention yields significant gains across all automated metrics, as seen in Table 6 for a 1024-truncate model (and in Table 15 in Appendix F for a 128truncate model). As a baseline we explore simply re-attending to the full context again; this indeed improves metrics across the board for the shortcontext model, but the long-context model actually suffers. However, both models improve substantially over the baseline when including the full LIGHT context without the dialogue history, and attention over sub-components of the LIGHT context still yields strong improvements.\nTo see how much this expanded attention matters, we explored varying the number of rounds r \u2208 {1, 2, 3} of expanded attention, i.e., how many times the model attends to this additional context. In Table 6, we also see that a second expanded attention round yields even better results, but performance drops off after applying a third round.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Automated Grounding", "text": "We show results for the automated grounding of expanded attention in Table 7. Attempting to use the decoder attention weights to select expanded attention context yields no additional benefits, which is not surprising: if the model could identify the pertinent components of the input beforehand, it would not require a reattention. The trainable mask does not yield any benefits either. However, using the RPA classifier attention weights to inform the model which tokens to re-attend to yields improved performance across all three metrics compared to the baseline, and PPL   is nearly the same as profile grounding (12.19 vs. 12.18), while RPA trails slightly behind (91.11 vs. 91.79). We also include the usage of the bottom-k tokens from the classifier weights to emphasize that there is indeed signal from the top-k, as using the bottom tokens does not help.\nAutomated Grounding + Multi-Objective Table 5 shows that combining automated grounding with the multi-objective task yields higher hits@1 compared to not using the trainable mask, especially in the first stage of multi-objective training. However, RPA scores are only fractionally better than the baseline. Appendix E includes results across more settings (see Table 13 and Table 14).\nExpanded Attention + RPA Re-ranking The expanded attention and RPA re-ranker methods can also both be applied to obtain effective models. Results are in Table 4; indeed, the combination yields the highest F1 and RPA scores.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Human Evaluations", "text": "We performed human evaluation on our models.\nFor each model we collected 100 human-model conversations, set up similarly to the original LIGHT dataset conversations. During the conversation, crowdworkers were asked to annotate the model's response for the following attributes: 1) Mistaken Identity: your partner says something that would imply they believe they're someone other than who they're noted to be; 2) Contradiction: your partner says something that contradicts something they've said before; 3) Wrong Location: your partner says something that would imply they believe they are in a different location than the provided one; 4) Unrelated: your partner says something that doesn't follow the previous turns; and 5) Repetitive: your partner says something they've already said, or are driving the conversation in circles. Utterances that do not contain any of the negative attributes are denoted \"all good\". Finally, we collect an engagingness score on a scale of 1-5 at the end of the conversation. More details in Appendix I. Results are given in Correlations between automatic metrics and human evaluations are measured in Appendix K, where we find that RPA and mistaken identity are indeed strongly correlated.\n5 Qualitative Analysis", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Re-rankers & Generation Settings", "text": "We further explored three decoding settings: standard beam-search, delayed beam search (Massarelli et al., 2020) and nucleus sampling (Holtzman et al., 2020), both in a re-ranking setting and not. When considering performance on automated metrics (provided in Table 20 in the Appendix), we see that generation settings other than beam search, when using a re-ranker, yield lower F1 scores but higher RPA scores, as the RPA re-ranker has more diversity of candidate responses from which to choose; however, these methods perform worse in human evaluations, with nucleus sampling reranking yielding far more problems and far lower engagingness ratings. Qualitative analysis of outputs on the test set are in Appendix J.1.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Classifier Failure Modes", "text": "We note that the human dialogue data is classified as being \"in character\" only 92.8% of the time on the validation set by the LTR RPA classifier. We examine the scenarios in which the classifier is incorrect, with example input/output pairs in Table 21 in the Appendix. First, there are instances where either character could have said the output response (row 1). Second, there are instances where there are not enough clues in the context to provide an estimation of who said the response, for example at the beginning of the conversation (row 2). And, there are still some small amount of instances that the classifier simply fails (row 3).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model Failure Modes", "text": "We analyze the results of turn annotation to understand what failure modes contribute to mistaken identity. A full list of such modes is in Table 16; the baseline model most often mistakes its partner for itself (i.e., the model thinks it is talking to itself).\nOther common failures include the model thinking that it is its partner's character, or emulating irrelevant characteristics.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Per-Turn Character Accuracy Analysis", "text": "We consider the RPA of various models when evaluated across the turns of conversation. Intuitively, baseline models would suffer as the conversation goes on for a variety of reasons (character roles are truncated out of context, more input yields noisier outputs, etc.). Appendix Figure 1 shows the perturn results for a set of representative models. The human outputs are most often correct on the first turn, with gradual RPA decay throughout the conversation. The 128-truncate baseline, as expected, suffers a dramatic performance drop after the first couple of turns. Meanwhile, with the profile expanded attention, we see near-human performance, with better RPA in later turns. Including RPA reranking improves dramatically over all turns.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Expanded Attention Visualization", "text": "To gain some insight into what is happening with the expanded attention, we mapped out the attention between context and response tokens for both a baseline model with no expanded attention, and a model with profile expanded attention. Figures 4  and 5 in the Appendix display the heat maps for an example context and response, with details on heat map construction given in Appendix M. We find that the baseline model spreads its attention out across both the LIGHT context and the dialogue history, with the majority of the attention looking at overlapping words in the context and the response and almost no attention on the character names. The expanded attention model concentrates on the recent dialogue history heavily in the first level of attention, and then concentrates on pertinent words in the context related to the character information (i.e., the character names) in the second round of attention.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "In this work we explored the problem of maintaining one's character in open dialogue, and showed that state-art-of-the-art models have a fundamental weakness in this regard. We provided a clear framing of the problem and showed one can build automatic metrics (RPA) that evaluate models using a classifier. We then explored a variety of methods throughout this paper. While a wide variety of well-known techniques, such as multi-objective or unlikelihood training, have little impact, we found that expanded attention and re-ranking are two approaches that can help to a degree, and their combination also improves results. Our introduced method PACER performs well and may be suitable for other tasks beyond the focus of this paper. Nevertheless, our best methods still lag behind human (crowdworker) performance in several regards, e.g. 1.34% vs. 2.23% in terms of mistaken identity per turn, or 5% vs. 14.7% per conversation. Therefore considerable progress still has to be made on this challenging problem.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ethical Considerations", "text": "Limitations We note in the conclusion that the problem is not solved; our best models still lag behind human performance in maintaining character identity. All results are tested in the LIGHT environment, comprising open-domain dialogue within constrained settings with assigned characters. The application of these methods to other role-playing (or otherwise) settings is left for future work, though we believe that such methods could be beneficial outside of LIGHT.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Potential Risks", "text": "We provide methods for mitigating mistaken identity in dialogue models. It follows that such methods yield models that are more convincingly role-playing as a given character. With more convincingly in-character models, someone with bad intentions could have a model imitate realworld people without consent, or worse, can say negative/harmful things while impersonating someone else. We note that our methods are orthogonal to improvements in dialogue safety (Xu et al., 2020;Dinan et al., 2019a), and so can be used in tandem to mitigate these potential risks.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Scientific Artifacts", "text": "We make use of LIGHT in this work (Urbanek et al., 2019) (released under CC-BY license), an English-language crowdsourced dataset. We also plan to release the code and models (will be released under MIT license), with the intended use being for others (and ourselves) to reproduce and build upon the research discussed in this paper.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A Training Details", "text": "All models are trained with the ParlAI 2 framework (Miller et al., 2017). Due to the large number of experimental setups and computational cost, we do not consider multiple training runs.\nBase Models RPA classifier Poly-encoders are initialized with the 622M parameter models from Roller et al. (2021); we also use this architecture for dialogue response (retrieval) models which we also evaluate (see Table 19). All generative models are initialized with BlenderBot, also from Roller et al. (2021), a 2.7B parameter transformer encoder/decoder model. Each model was pre-trained on 1.5B training examples from pushshift.io Reddit (Baumgartner et al., 2020), with BlenderBot additionally fine-tuned on the BST tasks (see Roller et al. (2021) for more details), before training on LIGHT.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "RPA Classifiers", "text": "The RPA classifier models are trained with a cross-entropy loss over the correct label, with 99 random negatives chosen from the training set; we ensured that each character in conversation showed up in the set of candidate labels. The models were trained with a batch size of 16 on 4 32GB GPUs, with early stopping on the validation set according to valid accuracy. We used the Adam optimizer (Kingma and Ba, 2015) with weight decay (Loshchilov and Hutter, 2019), sweeping over learning rates {1e \u2212 5, 5e \u2212 6}.\nGenerative Models All variants of generative models were trained using 8 32GB GPUs, with early stopping on perplexity on the validation set. We used the Adam optimizer, sweeping over learning rates {1e \u2212 5, 7e \u2212 6}, training with a batch size of 128 for the short-truncation models, and 32 for the long-truncation models. For the multiobjective models, we used the same loss (and negative-sampling) setup as the RPA classifiers for the character accuracy objective. During inference, unless otherwise specified, we generated using beam-search with beam size of 10, enforcing a minimum length of 20, and with tri-gram blocking with respect to both the context and the current generation.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "B RPA Classifier Training", "text": "We build the training data for the RPA classifiers from the LIGHT dataset. The input is a concate-2 https://parl.ai", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dataset", "text": "Train Valid Test LIGHT (Urbanek et al.,   nation of (1) the LIGHT context (set of characters, setting, etc.); (2) a fixed number of previous utterances in the conversation; and (3) a candidate utterance from any point later in the conversation (a special token separates the candidate utterance from the prior context). We experiment with either 0, 4, or N \u2212 2 prior utterances (dubbed \"All\" in relevant tables), where N is the total number of utterances (N \u2212 2 allows the last turn for each speaker to be a candidate utterance). The left-toright (LTR) data split is built similarly, except each example i becomes w i examples, where w i is the number of tokens in the candidate utterance for example i. Statistics of the training dataset are given in Table 8. Suppose we choose n as the number of prior utterances to include in the input, and let us denote D = 8538 to represent all the dialogues in the LIGHT train split, and U = 110877 to represent all the utterances in those dialogues. For the RPA classification dataset, each dialogue is presented twice, once from each character's POV. For any value 0 < n < N \u2212 1, we build out several examples from several slices of each conversation. Suppose we have dialogue d i with N utterances {u 0 , u 1 , ..., u N }. To build the training data from dialogue d i , we select all continuous subsets of n utterances within d i , forming contexts\nc i = {u i , ..., u i+n } \u2200 0 \u2264 i \u2264 N \u2212 i\nThen, we look at all N \u2212 i utterances following utterance u i+n , and use these as target utterances in the task. The goal of this is to build the model to be robust to dataset artifacts; without this modification, the model could trivially pick out the character just by looking at the number of alternating utterances.   These measures force the model to fully understand the task and react accordingly.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "C RPA Classifier Performance: Additional Results", "text": "In Table 10, we see how each RPA classifier performs on the various datasplits, varying the number of prior utterances used during training and evaluation. Each model performs best on the split on which it was trained (the highlighted numbers).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C.1 Left-to-Right Dynamic Classification", "text": "We find that the left-to-right RPA classifiers are correctly sensitive to per-token perturbations in the input, and can accurately predict the speaker at the token level. In Table 11, we give an example where the classifier changes its character prediction, depending on the candidate utterance.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D Unlikelihood: Additional Results", "text": "In Table 12, we compare UL models across different truncation lengths; the same story applies to the 1024-truncation models. We additionally include a third method, Random-3, where we apply the loss randomly to 3 tokens that yield incorrect RPA classifications. This method performs about the same as the Top-1 method, but the RPA is lower, indicating that the Top-1 method at least is providing some signal.\nE Multi-Objective: Additional Results", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "E.1 Perplexity & F1", "text": "Table 13 displays full PPL and F1 scores corresponding to the models in Table 5.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "E.2 Multi-Objective + Automated Grounding", "text": "In Table 14, we see how, when using either the encoder+decoder or just the decoder outputs, we do not require additional multi-objective layers (as we did in the non-automated-grounding case).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "F Expanded Attention: Additional Results", "text": "We provide results for both the 128-truncate and 1024-truncate models with profile grounding in Table 15. Trends remain the same for both models.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "G Full Valid Results", "text": "Table 17 includes results on the LIGHT validation set for models in Table 4.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "H Retrieval Re-rankers", "text": "We evaluated a Poly-encoder baseline model with an RPA re-ranker as well. The Poly-encoder scores utterances from the full training set as candidates, and the candidates for re-ranking are the top-k ranked utterances; results are in Table 18. Retrieval models benefit dramatically from the re-ranking, improving to almost 99% RPA as measured by the LTR classifier. As the candidate responses for retrieval models come from the set of all training utterances, and due to overlap between the set of characters appearing in the train and valid sets, we can examine how often the model output was originally spoken by its partner's character; this can be seen as a proxy for mistaken identity. We find that the re-ranker reduces the amount of time that the model returns a message its partner said, indicating some viable and promising results.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "I Full Human Evaluation Results", "text": "In Table 19, we display the full results of human evaluations across all dimensions. We note that the Poly-encoder model is best at not mistaking location or being repetitive, but this is expected given its retrieving over human-written utterances. In Setting: Turquoise Shore, Shore A beautiful turquoise color water by the shore. It is filled with many gems and gold.         Figure 2, we show a screenshot of the instructions for the evaluation task provided to crowdworkers on Amazon Mechanical Turk.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "J Generation Settings", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "J.1 Test Output Analysis", "text": "We provide qualitative analysis of the various generation methods below.\nNo Re-ranking When examining the baseline with no re-ranking, we found that nucleus sampling can help when beam search does not work; however, both can go out of character the farther one goes in conversation.\nBeam Search Re-rankers The beam outputs in standard beam search are at times too similar, in which case re-ranking does next to nothing, unless a viable response is available.\nNucleus Sampling Re-rankers Using nucleus setting in a re-ranking setup yields more diverse choices to choose from; however, sometimes the model simply does not address *any* character within the conversation.\nDelayed Beam Search Re-rankers This strikes a nice balance between sensible outputs from beam search and diversity from nucleus sampling.\nMixed-Decoding Re-ranker Using mixed decoding (re-ranking several decoding schemes) can work quite well, as it is a nice blend of different generation methods.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "J.1.1 Turn Annotation Analysis", "text": "Qualitative analysis of the turn annotation results are in Table 16. We generally found that beam search fails the vast majority of the time when the model thinks that it is talking to itself ; i.e., it confuses its partner for its own character. The rerankers can help shift the hallucination away from this regime.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "J.2 Automated Metrics", "text": "We experiment with various generation settings, with or without re-rankers; results are in Table 20.\nFor the baseline and re-ranker models, beam search yields the highest F1 scores; RPA can be improved with the other inference methods when combined with a re-ranker. We believe this may be due to the higher diversity of candidate responses generated from those methods.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "K Human + Automatic Eval Correlation", "text": "We analyze the correlation between human annotations and the automatic metrics collected on the LIGHT validation set, as shown in Figure 3; we note some interesting trends:\nPerplexity perplexity appears to be positively correlated with mistaken identity, and negatively correlated with engagingness. So, perplexity is  a good indicator of how fluent and engaging the model is in conversation, and can indirectly point to a better understanding of the role-playing task. An important note is that we only tested this amongst models of the same size, and only for the models we tested, so it is not clear that larger models will necessarily bring improvements.\nF1 F1 word overlap is positively correlated with engagingness as well, so F1 may be a good proxy of model performance. Correlation with mistaken identity is negative here, implying that better F1 corresponds with better role-playing ability. However, we note that F1 is not a catch-all metric (Liu et al., 2016).\nRPA RPA appears to be strongly negatively correlated with mistaken identity, indicating that it is indeed a good measure of the model's ability to stay in character. It is weakly negatively correlated with the other issues, and is somewhat positively correlated with engagingness as well. These correlations give us confidence that our RPA classifiers are adequately measuring role-playing ability within models.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "L Per-Turn Analysis, Expanded", "text": "In Figure 1, we see RPA results across turns of conversation for a wider variety of models.\nHuman The human outputs are most often correct on the first turn, with gradual decay of accuracy throughout the conversation (according to RPA).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Vanilla & Long Context", "text": "The vanilla baseline suffers a pretty dramatic drop off after the first couple of turns; the long-context model achieves slightly higher character accuracy overall but we see similar drop offs farther down the conversation.\nRPA UL The unlikelihood models seem to recover somewhat in the initial turns of conversation, however later turns still yield sharp drop offs in RPA.\nMulti-objective Similarly to the UL case, we see the most gains in initial turns compare to the vanilla baselines; however, we see even more dramatic drop offs towards the end of the conversation.\nExpanded Attention With profile grounding, we see near-human performance, with even better performance towards the end of the conversation. The automatic grounding improves over the baseline but is slightly worse than profile grounding. Combining automated grounding with multi-objective training leads to some benefits in earlier turns, but later turns still suffer.\nRe-ranking Although we're using the same RPA classifier to both re-ranker and score the model outputs, it is still interesting to examine on which turns the re-ranker benefits the model the most. We see in the last set of graphs that beam re-ranking Figure 4: Vanilla Attention. The speaker here is the mermaid, whose partner is a sea-witch. The last utterance from the sea-witch is, \"What are you doing on the turquoise shore?\". The mermaid responds, \"I've been catching waves with the dolphins all morning. What kind of victims do you expect to find in a tranquil place like this?\". The vanilla model spreads its attention across the whole context; blue boxes at the top are attentions over the character descriptions, while the bottom box is attention over the word \"victims\".\nFigure 5: Profile Expanded Attention. The speaker here is the mermaid, whose partner is a sea-witch. The last utterance from the sea-witch is, \"What are you doing on the turquoise shore?\". The mermaid responds, \"I've been catching waves with the dolphins all morning. What kind of victims do you expect to find in a tranquil place like this?\". Left original attention over the full context; Right expanded attention over the additional context. The top two boxes are the partner name and self name; the bottom box on the left refers to \"victims\", and on the right refers to the \"dolphins\".", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "", "text": "seems to be most helpful in later turns, where other models generally drop off in efficacy.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "M Expanded Attention Visualization", "text": "To build the heat maps in Figures 4 and 5, we look at the maximum attention applied per-head, and the maximum weight applied across the model decoder layers; other combinations were considered (mean per-head, mean over layers or last layer) and yielded similar findings. The speaker is the mermaid, whose partner is a sea-witch. The last utterance from the sea-witch is, \"What are you doing on the turquoise shore?\". The mermaid responds, \"I've been catching waves with the dolphins all morning. What kind of victims do you expect to find in a tranquil place like this?\"", "n_publication_ref": 0, "n_figure_ref": 1}], "references": [{"title": "Towards a human-like open-domain chatbot", "journal": "", "year": "2020", "authors": "Daniel Adiwardana; Minh-Thang Luong; David R So; Jamie Hall; Noah Fiedel; Romal Thoppilan; Zi Yang; Apoorv Kulshreshtha; Gaurav Nemade; Yifeng Lu"}, {"title": "", "journal": "", "year": "", "authors": "Jason Baumgartner; Savvas Zannettou; Brian Keegan; Megan Squire; Jeremy Blackburn"}, {"title": "Build it break it fix it for dialogue safety: Robustness from adversarial human attack", "journal": "", "year": "2019", "authors": "Emily Dinan; Samuel Humeau; Bharath Chintagunta; Jason Weston"}, {"title": "The second conversational intelligence challenge (ConvAI2)", "journal": "Springer International Publishing", "year": "2020", "authors": "Emily Dinan; Varvara Logacheva; Valentin Malykh; Alexander Miller; Kurt Shuster; Jack Urbanek; Douwe Kiela; Arthur Szlam; Iulian Serban; Ryan Lowe; Shrimai Prabhumoye; Alan W Black; Alexander Rudnicky; Jason Williams; Joelle Pineau; Mikhail Burtsev; Jason Weston"}, {"title": "Wizard of wikipedia: Knowledge-powered conversational agents", "journal": "", "year": "2019-05-06", "authors": "Emily Dinan; Stephen Roller; Kurt Shuster; Angela Fan; Michael Auli; Jason Weston"}, {"title": "Dilek Hakkani-T\u00fcr, and Amazon Alexa AI. 2019. Topical-chat: Towards knowledge-grounded open-domain conversations", "journal": "", "year": "", "authors": "Karthik Gopalakrishnan; Behnam Hedayatnia; Qinglang Chen; Anna Gottardi; Sanjeev Kwatra; Anu Venkatesh; Raefer Gabriel"}, {"title": "The curious case of neural text degeneration", "journal": "", "year": "2020-04-26", "authors": "Ari Holtzman; Jan Buys; Li Du; Maxwell Forbes; Yejin Choi"}, {"title": "Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring", "journal": "", "year": "2020-04-26", "authors": "Samuel Humeau; Kurt Shuster; Marie-Anne Lachaux; Jason Weston"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015-05-07", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Internet-augmented dialogue generation", "journal": "", "year": "2021", "authors": "Mojtaba Komeili; Kurt Shuster; Jason Weston"}, {"title": "Importance of search and evaluation strategies in neural dialogue modeling", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ilia Kulikov; Alexander Miller; Kyunghyun Cho; Jason Weston"}, {"title": "Don't say that! making inconsistent dialogue unlikely with unlikelihood training", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Margaret Li; Stephen Roller; Ilia Kulikov; Sean Welleck; Y-Lan Boureau; Kyunghyun Cho; Jason Weston"}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "journal": "", "year": "2016", "authors": "Chia-Wei Liu; Ryan Lowe; Iulian Serban; Mike Noseworthy; Laurent Charlin; Joelle Pineau"}, {"title": "2021. A token-level reference-free hallucination detection benchmark for free-form text generation", "journal": "", "year": "", "authors": "Tianyu Liu; Yizhe Zhang; Chris Brockett; Yi Mao; Zhifang Sui; Weizhu Chen; Bill Dolan"}, {"title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"title": "How decoding strategies affect the verifiability of generated text", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Luca Massarelli; Fabio Petroni; Aleksandra Piktus; Myle Ott; Tim Rockt\u00e4schel; Vassilis Plachouras; Fabrizio Silvestri; Sebastian Riedel"}, {"title": "Training millions of personalized dialogue agents", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Pierre-Emmanuel Mazar\u00e9; Samuel Humeau; Martin Raison; Antoine Bordes"}, {"title": "ParlAI: A dialog research software platform", "journal": "", "year": "2017", "authors": "Alexander Miller; Will Feng; Dhruv Batra; Antoine Bordes; Adam Fisch; Jiasen Lu; Devi Parikh; Jason Weston"}, {"title": "2021. I like fish, especially dolphins: Addressing contradictions in dialogue modeling", "journal": "Long Papers", "year": "", "authors": "Yixin Nie; Mary Williamson; Mohit Bansal; Douwe Kiela; Jason Weston"}, {"title": "Focused attention improves documentgrounded generation", "journal": "Online. Association for Computational Linguistics", "year": "2021-11", "authors": "Kazuma Shrimai Prabhumoye; Yingbo Hashimoto; Alan W Zhou; Ruslan Black"}, {"title": "Language models are unsupervised multitask learners", "journal": "OpenAI Blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"title": "Towards empathetic opendomain conversation models: A new benchmark and dataset", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Eric Michael Hannah Rashkin; Margaret Smith; Y-Lan Li;  Boureau"}, {"title": "Recipes for building an open-domain chatbot", "journal": "", "year": "2021", "authors": "Stephen Roller; Emily Dinan; Naman Goyal; Da Ju; Mary Williamson; Yinhan Liu; Jing Xu; Myle Ott; Eric Michael Smith; Y-Lan Boureau; Jason Weston"}, {"title": "Rome was built in 1776: A case study on factual correctness in knowledge-grounded response generation", "journal": "", "year": "2021", "authors": "Sashank Santhanam; Behnam Hedayatnia; Spandana Gella; Aishwarya Padmakumar; Seokhwan Kim; Yang Liu; Dilek Hakkani-Tur"}, {"title": "Retrieval augmentation reduces hallucination in conversation", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Kurt Shuster; Spencer Poff; Moya Chen; Douwe Kiela; Jason Weston"}, {"title": "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Koustuv Sinha; Robin Jia; Dieuwke Hupkes; Joelle Pineau; Adina Williams; Douwe Kiela"}, {"title": "Learning to speak and act in a fantasy text adventure game", "journal": "", "year": "2019", "authors": "Jack Urbanek; Angela Fan; Siddharth Karamcheti; Saachi Jain; Samuel Humeau; Emily Dinan; Tim Rockt\u00e4schel; Douwe Kiela; Arthur Szlam; Jason Weston"}, {"title": "Attention is all you need", "journal": "", "year": "2017-12-04", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"title": "Improving conditioning in contextaware sequence to sequence models", "journal": "", "year": "2019", "authors": "Xinyi Wang; Jason Weston; Michael Auli; Yacine Jernite"}, {"title": "Neural text generation with unlikelihood training", "journal": "", "year": "2020-04-26", "authors": "Sean Welleck; Ilia Kulikov; Stephen Roller; Emily Dinan; Kyunghyun Cho; Jason Weston"}, {"title": "Recipes for safety in open-domain chatbots", "journal": "", "year": "2020", "authors": "Jing Xu; Da Ju; Margaret Li; Y-Lan Boureau; Jason Weston; Emily Dinan"}, {"title": "Beyond goldfish memory: Long-term open-domain conversation", "journal": "", "year": "2021", "authors": "Jing Xu; Arthur Szlam; Jason Weston"}, {"title": "FUDGE: Controlled text generation with future discriminators", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Kevin Yang; Dan Klein"}, {"title": "Knowledge-grounded response generation with deep attentional latent-variable model", "journal": "Computer Speech & Language", "year": "2020", "authors": "Kai-Lin Hao-Tong Ye; Shang-Yu Lo; Yun-Nung Su;  Chen"}, {"title": "Personalizing dialogue agents: I have a dog, do you have pets too?", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Saizheng Zhang; Emily Dinan; Jack Urbanek; Arthur Szlam; Douwe Kiela; Jason Weston"}, {"title": "Enhancing conversational dialogue models with grounded knowledge", "journal": "", "year": "2019", "authors": "Wen Zheng; Ke Zhou"}, {"title": "A pre-training based personalized dialogue generation model with persona-sparse data", "journal": "", "year": "2020", "authors": "Yinhe Zheng; Rongsheng Zhang; Minlie Huang; Xiaoxi Mao"}, {"title": "A dataset for document grounded conversations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Kangyan Zhou; Shrimai Prabhumoye; Alan W Black"}, {"title": "Model Contradiction Mistaken Mistaken Off-Topic Repetitive All-Good Clean Mistaken Avg. Identity Location Convo Identity Engagingness In Convo Human -1", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": " Multi-Objective"}, {"title": "Profile Grounding (1024, 2 Rounds over ABCD)", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Setting:Barn, Farm Partner: Thief Self: Guest Guest: Hail who goes there?! Thief: Hello friend I mean you no harm Model Response Guest: I am not your friend, I am your thief! I am here to steal from you! Character Token Probabilities Thief: 0.39 Guest: 0.23 Setting: Castle Kitchens, Inside Palace Partner: Hunter Self: Helper Hunter: Hi Helper: Hello. What brings you to the kitchen? Hunter: I am chopping some meat that is to be sold. Helper: Could you spare me a piece? I'm starving. Hunter: It's 3 cents a pound, if you have 3 cents then yeah Model Response Helper: I don't have any money. I'm just a poor hunter trying to make a living. Character Token Probabilities Hunter: 0.39 Helper: 0.12", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "When n = N \u2212 1, where N is the length of a conversation, then we have roughly 2D training examples. When n = 0, we have roughly 2U training examples.", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure 1: Per-turn RPA classifications, for a variety of models. Error bars show the difference between the model's RPA value and the human's RPA value.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Instructions provided to annotators in human evaluations.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Correlation between human evaluations and automated metrics computed on the test set.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Failures of a 2.7B parameter transformer pretrained on 1.5B examples from a large dialogue corpus", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "to force the modelto stay in character during training. Unlikelihoodtraining works as a counter to the standard maxi-mum likelihood (MLE) training of language mod-els; while"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "RPA classifier performance on the validation set, comparing a partial-sequence trained model (\"LTR\") to one trained only on full sequences (\"Full\"). Models were trained with 4 prior utterances of context.", "figure_data": "Re-rankerParams # Toks Freq.F1 RPACostNone0015.8 88.41xComplete-only 0016.0 93.01.1xPartial-only105%15.9 88.61.3xPartial-only1033%158 91.14.2xPartial-only10100% 15.6 93.6 11.2xPartial-only505%15.9 88.93.0xPACER PACER10 105% 33%16.1 93.3 15.9 94.61.2x 3.8xPACER10100% 15.8 96.3 11.5x"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Models (128-truncated) evaluated with various re-ranking schemes on the validation set. Cost is relative speed compared to no re-ranking at all.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": ", with results on the validation set in Table17in the Appendix. While lengthening the context from 128 to 1024 tokens yields better perplexity, the model obtains worse F1 scores and does not improve much if at all on role playing ability, both when measured by the RPA classifiers and in human evaluations (see also Table19). Further", "figure_data": "Model Human Baselines 128-Truncate Vanilla Baseline 1024-Truncate Vanilla Baseline Re-rankers 128-Truncate Baseline + RPA Re-ranker 128-Truncate Baseline + PACER Modified Training Objectives RPA Unlikelihood (Top-1 Token) RPA Unlikelihood (All Tokens) Multi-Objective (Vanilla, Dec. Only) Expanded Attention Methods Profile (128, 2 rounds over ABC) Profile (1024, 2 rounds over ABCD) Automated (1024, Classifier Attn) Automated + MO (1024, Dec. Only) Expanded Attention + Re-ranker Methods Profile (128) + RPA Re-ranker Profile (128) + PACERAutomatic Metrics PPL\u2193 F1\u2191 RPA\u2191 Mistaken All-Good\u2191 Human Evaluations Mis. Id. Engaging\u2191 Identity\u2193 in Conv.\u2193 --92.68 1.34% -5.0% -12.64 15.69 87.61 6.45% 76.0% 35.1% 4.04 12.43 15.68 87.71 7.35% 75.0% 38.4% 4.16 -15.87 92.09 5.56% 80.3% 34.7% 4.14 -15.85 92.78 4.27% 73.9% 33.7% 3.96 13.10 15.18 87.48 7.13% 72.8% 39.4% 3.87 13.31 14.77 88.07 10.51% 67.7% 43.0% 3.87 12.86 15.67 87.67 10.00% 74.8% 49.0% 4.21 12.37 15.74 91.70 4.82% 81.6% 28.4% 4.18 12.23 15.66 92.18 4.00% 83.8% 23.8% 4.34 12.27 15.75 90.93 5.51% 76.0% 29.1% 4.04 13.01 15.52 88.95 4.43% 78.6% 23.0% 4.12 -15.88 95.16 2.23% 84.4% 14.7% 4.24 -15.79 95.31 4.07% 85.7% 24.5% 4.32"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Automated metrics and human evaluations for various models considered throughout the paper on the LIGHT test set. RPA (Role-Playing Accuracy) is measured by the 4-utterance LTR classifier, see Sec. 3.2. The human evaluations are per utterance, except for Engaging and Mistaken Identity in Conversation (with the latter indicating % of conversations with mistaken identity).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Dialogue Hist. 1 12.47 15.82 88.34 ABCD 1 12.18 16.01 91.82 ABCD 2 12.17 15.95 92.60", "figure_data": "Expanded Attention Human None ABCD + ABCD ABC ABC ABC AB A Br 0 0 12.35 15.85 88.42 1024-Truncate Model PPL F1 RPA --92.80 3 12.19 15.99 91.73 1 12.22 15.94 91.83 2 12.24 15.99 92.24 3 12.25 15.93 92.25 1 12.27 15.87 90.97 1 12.30 15.80 89.13 1 12.34 15.76 89.46"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ": Models trained with expanded attention (pro-file grounding), evaluated on the valid set. Expanded attention input: A = Self Persona, B = Self Name, C = Partner Name, D = Setting Description. We also vary the number of rounds r of expanded attention.Exp. Attn. GroundingPPLF1 RPAHuman--92.80None12.35 15.85 88.42Profile Ground Best (2 rounds) 12.17 15.95 92.60 Profile Ground Best (1 round) 12.18 16.08 91.79 Profile Ground Random 12.43 15.74 87.62Decoder Attn.12.39 15.40 87.59Trainable Mask12.40 15.75 88.43Classifier Attn. (top-k)12.19 15.90 91.11Classifier Attn. (bottom-k)12.31 15.89 88.71"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": ", with more detailsresults (and comparison with a retrieval baseline)in Table 19 in the Appendix. The baseline modeldisplays mistaken identity 6.45% of the time, andhas an average engagingness score of 4.04. Longercontext increases engagingness to 4.16 but also in-creases mistaken identity. Unlikelihood and multi-objective training similarly increase mistaken iden-tity. The successful methods, then, are the beam re-ranking methods and the expanded attention mod-els. The long-context beam re-ranker decreasesmistaken identity to 4.81%, while the profile ex-panded attention model improves to 4%, and hasthe best engagingness of 4.34. Combining RPA Re-ranking with expanded attention yields the lowestmistaken identity (2.38%), while adding PACERleads to the highest all-good percentage (85.7%)."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Number of training, valid, and test examples for the LIGHT dataset, as well as the RPA training splits (both normal and LTR).", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "RPA classifier performance on the valida-", "figure_data": "tion set, as measured by Hits@1/427 and Hits@1/2 (all characters and participant characters as candidates, re-spectively). Each model is trained and evaluated with that # of prior utterances.# Train-time Prior Utterances Without LIGHT Context Hits@1/427 # Eval Prior Utterances 0 4All010.3518.5817.7142.1087.3184.35All7.02 With LIGHT Context 81.2685.70077.6466.2058.61431.0486.4884.90All32.5482.7389.26"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "RPA classifier performance on the validation set, as measured by hits@1/427. Highlighted numbers indicate models evaluated on the split on which they were trained.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Character 1: Sea Witch. I am a sea witch. I pray on young sailors who hope to find adventure and treasures on the open sea. I lure them in with magic spells and promise of riches. Character 2: Mermaid. I am one of the most beautiful mermaids to live in the sea. I like to watch the other sea creatures swim by me, including dolphins, who are my favorite creatures because they are so friendly. I fear the people who live on land because they hunt my kind Classified Utterance: Hey there Mermaid! Long time, no see. Classified Utterance: Hey there Sea Witch! Long time, no see.", "figure_data": "Correct Speaker: Sea Witch Word Predicted Speaker Confidence Hey sea witch 0.5156Correct Speaker: Mermaid Word Predicted Speaker Confidence Hey sea witch 0.5156theresea witch0.5467theresea witch0.5467Mermaid! sea witch0.9978seamermaid0.9968witch mermaid1.000Longsea witch0.9981Long mermaid1.000time,sea witch0.9979Time mermaid1.000nosea witch0.9982nomermaid1.000see.sea witch0.9985see.mermaid1.000"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Left-to-right dynamic classification examples. A candidate utterance is shown, along with the classifier's predictions at each partial decoded sequence. Left: The true next utterance in the dialogue, with the RPA classifier's predictions and confidence token by token. Right: A perturbed utterance. If we switch the name being addressed, the model switches its predictions immediately.", "figure_data": "Unlikelihood Method Human None (128) None (1024) 128-Truncation RPA UL: Top-1 Token RPA UL: All tokens RPA UL: Random-3 1024-Truncation RPA UL: Top-1 Token 12.49 15.66 88.12 PPL F1 RPA --92.80 12.54 15.80 88.54 12.35 15.85 88.42 13 15.35 88.54 12.86 15.28 88.86 12.99 15.37 87.85 RPA UL: All tokens 12.57 15.83 88.06"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Models trained with unlikelihood loss, evaluated on the valid set. We vary the tokens to which we apply UL loss.", "figure_data": "Input Human None Multi-Objective n M O N/A 0 Dec. only 2 Dec. only 2 Enc+Dec 2 Enc+Dec 2 Multi-Objective + Automated Expanded Attention Stage PPL F1 --0 12.4 15.9 1 12.4 15.9 2 12.8 16.0 1 12.4 15.9 2 12.5 15.8 Dec. Only 0 1 13.2 15.7 Dec. Only 0 2 12.9 15.9 Enc+Dec 2 1 12.9 15.8 Enc+Dec 2 2 12.7 15.8RPA 92.8 88.4 88.4 87.7 88.4 88.8 89.1 89.1 88.4 89.1Hits@1 39.3 87.4 70.9 71.6 86.4 89.1 83.3 88.5"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Models trained with varying multi-objective setups, evaluated on the valid set. Models are initialized from a (1024-truncation) model fine-tuned on LIGHT.", "figure_data": "Inputn M O StagePPLF1 RPA Hits@1/427Human0--92.80None0012.35 15.85 88.42Dec. Only 0113.22 15.66 89.0886.37Dec. Only 0 Enc+Dec 02 112.92 15.88 89.10 13.24 15.55 88.8389.10 85.78Enc+Dec Enc+Dec0 22 113.44 15.61 89.29 12.94 15.80 88.3989.22 83.25Enc+Dec2212.69 15.77 89.0588.49"}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "Models trained with varying multi-objective + automated grounding setups, evaluated on the valid set. The base model in all cases is initialized from a generation model fine-tuned on LIGHT.", "figure_data": "Exp. Attn. Human None ABCD+ ABCD ABCD ABCD ABC ABC ABC AB A Br 0 0 1 1 2 3 1 2 3 1 1 1128-Truncate Model PPL F1 RPA --92.80 12.59 15.80 88.28 12.23 15.87 90.59 12.25 15.97 90.94 12.23 15.89 90.83 12.26 15.81 90.44 12.33 15.82 91.50 12.31 16.03 92.03 12.33 15.90 91.59 12.42 15.92 90.31 12.46 16.05 90.22 12.53 15.85 89.851024-Truncate Model PPL F1 RPA --92.80 12.35 15.85 88.42 12.47 15.82 88.34 12.18 16.01 91.82 12.17 15.95 92.60 12.19 15.99 91.73 12.22 15.94 91.83 12.24 15.99 92.24 12.25 15.93 92.25 12.27 15.87 90.97 12.30 15.80 89.13 12.34 15.76 89.46"}, {"figure_label": "15", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "", "figure_data": ": Models trained with expanded attention (pro-file grounding), evaluated on the valid set. Expanded attention input: A = Self Persona, B = Self Name, C = Partner Name, D = Setting Description, + = dialogue history. We also vary the number of rounds r of ex-panded attention."}, {"figure_label": "16", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Turn annotation analysis of RPA Re-rankers.", "figure_data": "Model Human Baselines 128-Truncate Vanilla Baseline 1024-Truncate Vanilla Baseline Re-rankers 128-Truncate Baseline + Re-ranker 128-Truncate Baseline + PACER RPA UL (Top-1 Token) RPA UL (All Tokens) Multi-Objective (Vanilla, Dec. Only) Expanded Attention Methods Profile Grounding (128, 2 Rounds over ABC) Profile Grounding (1024, 2 Rounds over ABCD) 12.17 15.95 92.60 PPL F1 RPA --92.80 12.54 15.80 88.54 12.35 15.85 88.42 -16.14 92.99 -16.13 93.31 13.00 15.35 88.54 12.86 15.28 88.86 12.78 16.00 87.71 12.31 16.03 92.03 Automated Grounding (1024, Classifier Attn.) 12.19 15.90 91.11 Automated Grounding + MO (1024 Dec. Only) 12.92 15.88 89.10 Expanded Attention + Re-ranker Methods Profile (128) + RPA Re-ranker -16.21 95.62 Profile (128) + PACER -16.18 95.82"}, {"figure_label": "17", "figure_type": "table", "figure_id": "tab_24", "figure_caption": "Validation statistics for various models considered throughout the paper.", "figure_data": "Metric RPA (normal) RPA (LTR) % Partner Said ResponseBaseline Re-ranker 85.47 94.29 86.31 99.76 3.20 2.02"}, {"figure_label": "18", "figure_type": "table", "figure_id": "tab_25", "figure_caption": "Retrieval models with character output rerankers; performance on the validation set.", "figure_data": ""}, {"figure_label": "19", "figure_type": "table", "figure_id": "tab_26", "figure_caption": "Human evaluations. Annotators chatting with models were asked to annotate whether model utterances contained any of the problem attributes listed, with \"All-Good\" indicating that there were no issues. \"Clean Convo\" is the percentage of conversations without any issues.", "figure_data": ""}, {"figure_label": "20", "figure_type": "table", "figure_id": "tab_27", "figure_caption": "Performance on the LIGHT valid set for the baseline models with different generation settings, with or without re-rankers. All settings use tri-gram blocking with respect to the context and current generation, and have a minimum length of 20. We set topp = 0.3 for Nucleus sampling, topk = 50 for Top-K sampling, and use a beam-delay of 10 with topk = 10 for delayed beam search.", "figure_data": ""}], "doi": "10.18653/v1/D19-1461"}