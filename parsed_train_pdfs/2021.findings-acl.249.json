{"authors": "Qiao Cheng; Juntao Liu;  Xiaoye Qu; Jin Zhao; Jiaqing Liang; Zhefeng Wang; Baoxing Huai; Nicholas Jing Yuan; Yanghua Xiao; Elliot See; Dallas Elliot See; St Louis; Yang Jima", "pub_date": "", "title": "HacRED: A Large-Scale Relation Extraction Dataset Toward Hard Cases in Practical Applications", "abstract": "Relation extraction (RE) is an essential topic in natural language processing and has attracted extensive attention. Current RE approaches achieve fantastic results on common datasets, while they still struggle on practical applications. In this paper, we analyze the above performance gap, the underlying reason of which is that practical applications intrinsically have more hard cases. To make RE models more robust on such practical hard cases, we propose a case-oriented construction framework to build a Hard Case Relation Extraction Dataset (HacRED). The proposed HacRED consists of 65,225 relational facts annotated from 9,231 documents with sufficient and diverse hard cases. Notably, HacRED is one of the largest Chinese document-level RE datasets and achieves a high 96% F1 score on data quality. Furthermore, we apply the stateof-the-art RE models on this dataset and conduct a thorough evaluation. The results show that the performance of these models is far lower than humans, and RE applying on practical hard cases still requires further efforts. Ha-cRED is publicly available at https://github. com/qiaojiim/HacRED.", "sections": [{"heading": "Introduction", "text": "Relation extraction (RE) is one of the core NLP tasks and plays an increasingly important role in knowledge graph completion (Bordes et al., 2013) and question answering (Dong et al., 2015). RE aims to extract structured relational facts, i.e., triples such as (Bill Gates, founder_of, Microsoft) from plain texts. Recently, various models (Zeng et al., 2018;Takanobu et al., 2019;Fu et al., 2019; have been proposed to identify the relational facts and achieved state-of-theart (SOTA) performance, among which the latest  method CasRel achieves notable 91.8% F1 score on WebNLG (Gardent et al., 2017) and 89.6% on NYT (Riedel et al., 2010).\nHowever, can these seemingly fantastic results prove that the current RE models are powerful enough to perform well in practical applications.\nTo answer the question, we employ CasRel on 300 randomly selected samples of WebNLG and the same number of data from practical DuIE 1 . The F1 scores under these scenarios drop significantly from 89.3% to 62.8%. As illustrated in Figure 1, CasRel extracts correct triples (Elliot See, place_of_birth, Dallas) and (Elliot See, place_of_death, St. Louis) in WebNLG where keywords such as born and died explicitly express the relation information. In contrast, Cas-Rel fails to extract triples such as (Yang Jima, graduate_from, Communication University of China) where no keywords like graduate are mentioned. The most significant reason why CasRel performs well on WebNLG but struggles on practical data is that more challenging instances which we refer to as hard cases exist in the practical applications. Moreover, according to the statistics of entity description documents in CN-DBpedia , at least 40.1% relational facts can only be extracted from hard cases. Therefore, relation extraction from hard cases can not be neglected and demands more attention.\nAlthough many datasets (Li et al., 2016;Yao et al., 2019) have been proposed for RE, they rarely analyze the performance gap and focus on the hard cases. In order to make models robust on hard cases and more fit practical scenarios, in this paper, we aim to build a RE dataset with sufficient hard cases. To this end, we propose a case-oriented construction framework based on the challenging instances and build a Hard Case Relation Extraction Dataset (HacRED). Specifically, we first obtain general, massive, and various contexts as well as relational facts from CN-DBpedia to construct a distantly supervised dataset. The crucial part is to distinguish hard cases from abundant data. Therefore, we formulate nine indicators through systematic analysis of hard cases to quantify them. Then, we conduct feature engineering based on the valid indicators. Afterwards, a classifier is trained for distinguishing the desired hard cases. Finally, we develop a crowdsourcing platform with a novel three-stage annotation strategy and effective aggregation method CrowdTruth2.0 (Dumitrache et al., 2018) to guarantee the data size and quality.\nIn total, HacRED consists of 9,231 instances with 26 predefined relations and 9 types of entities. To the best of our knowledge, it is one of the largest document-level RE benchmark. Moreover, HacRED contains sufficient and diverse hard cases in line with practice. We conduct extensive experiments and systematic error analysis of SOTA models on HacRED. A sharp performance drop on HacRED compared to the existing benchmarks proves that RE in practical applications remains an open problem and still requires further research.\nTo recap, our main contributions are three-fold:\n\u2022 We first analyze the performance gap between popular datasets and practical applications, and therefore construct one of the largest Chinese document-level RE dataset which contains sufficient and diverse hard cases to improve the evaluation for complex RE tasks.\n\u2022 We propose a case-oriented construction framework to build RE dataset toward spe-cial cases. Meanwhile, we design a novel three-stage annotation method applicable for crowdsourcing of complex RE.\n\u2022 We systematically evaluate the current mainstream RE models on HacRED and justify its effectiveness in depth.\n2 Related Work", "n_publication_ref": 10, "n_figure_ref": 1}, {"heading": "Datasets for Relation Extraction", "text": "A series of datasets have been built for RE as of late, which have extraordinarily advanced the improvement of RE systems. RE datasets such as SemEval-2010 Task 8 (Hendrickx et al., 2009) and ACE05 are constructed through human annotation with relatively limited relation types and size. A large-scale dataset TACRED (Zhang et al., 2017) is obtained via crowdsourcing to satisfy the training of data-hungry models.\nAs RE applications differ much in various scenarios, constructing datasets aimed at specific targets is a popular trend in RE. DocRED (Yao et al., 2019) is constructed to accelerate the research on document-level RE. To meet the challenges of fewshot RE, FewRel (Han et al., 2018) as well as FewRel 2.0 (Gao et al., 2019) have been presented. RELX (Koksal and Ozgur, 2020) is a benchmark for cross-lingual RE. Jia et al. (2020) propose the task of interpersonal RE in dyadic dialogues and further construct a corresponding dataset called DDRel.\nCompared with previous RE datasets, HacRED is derived from the analysis of the performance gap between popular datasets and practical applications. It targets towards promoting the RE models to extract information from the complex contexts.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Models for Relation Extraction", "text": "Recently, many exciting works have been proposed to solve the RE tasks. (1)Joint Model: NovelTagging (Zheng et al., 2017) first formulates the task as a sequence labeling problem and presents a novel tagging schema to jointly extract entities and relations. CopyRE (Zeng et al., 2018) extracts triples based on a sequence-to-sequence structure and integrates the copy mechanism for entity generation. GraphRel (Fu et al., 2019) uses graph convolutional network (GCN) to capture features of words and text. CasRel  is different from the past and is able to extract more triples by learning relationspecific entity taggers. (2)Pipeline Model: PURE (Zhong and Chen, 2020) is a simple pipelined approach which learns an entity model and a relation model independently. DGCNN-BERT is a powerful pipeline method that first identifies multiple relations and then labels the head and tail entities given a relation. It achieves 89.3 F1 scores and has won the champion in the Competition of DuIE held by Baidu Inc. (3)Document-level Relation Classification Models: LSR (Nan et al., 2020) is a model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. GAIN (Zeng et al., 2020) introduces a path reasoning mechanism based on a heterogeneous mention-level graph and an entity-level graph. ATLOP  proposes two techniques, adaptive thresholding and localized context pooling. SSAN (Xu et al., 2021) designs several transformations to incorporate mention structural dependencies for document-level relation classification (DocRC).", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Easy Cases vs. Hard Cases", "text": "To analyze where models struggle in practical instances and distinguish the hard cases, we conduct a manual exploratory analysis on the errorprone instances of SOTA models (CGCN, CasRel, DGCNN-BERT) on NYT, DuIE and industry data. Then we formulate the potential causes of the errors with nine indicators illustrated as follows: Text Length. We notice that models tend to fail on instances with longer text. The experiments of Alt et al. (2020) also reflect that RE models get a relatively higher error rate with the length of sentence greater than 30 in TACRED. Argument Distance. We observe that the performance of the models declines when the arguments (i.e., head and tail entity mentions) are far away, especially in inter-sentence RE. Distractors. Extracting triples in contexts with linguistic distractors is tough for current models. For example, drop out will contribute to wrong relation graduate_from between entity mentions with PERSON and SCHOOL type. Reasoning. Reasoning is needed to extract the relation mentioned implicitly in the text. Recent work suggests that future researchers consider incorporating common sense knowledge or improved causal modules in RE tasks (Han et al., 2018). Homogeneous Entities. The context contains multiple homogeneous entity mentions with iden-  Similar Relations. Models struggle to identify the correct relation among those semantically similar ones concurrently mentioned in context. A sharp decrease is also found in few-shot RE when selecting N similar relations on N-way K-shot settings (Han et al., 2020).\nLong-tail Relations. Only a handful instances are available for long-tail relations in common datasets. Current data-hungry models struggle to learn the semantic patterns on these relations.\nMultiple Triples. Models always get a poor performance on the instances with numerous triples.\nOverlapping Triples. Different triples involve the identical entity mentions. Many existing models can not well handle the EntityPairOverlap and SingleEntityOverlap (Zeng et al., 2018) instances.\nTable 1 provides various examples from NYT and corresponding hard case indicators. In Table 2, the proportion growing on the error instances reflects the gap between existing datasets and practical data, which also proves the effectiveness of these indicators.  ", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "HacRED Dataset Construction", "text": "The overall architecture of the proposed caseoriented construction framework is illustrated in Figure 2. Different from previous works (Zhang et al., 2017, Zaporojets et al., 2020 which start crowdsourcing annotation straight after the data collection stage, we introduce additional stages of hard case feature engineering and target instance prediction. Moreover, we design a novel three-stage annotation method and employ CrowdTruth2.0.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Data Collection", "text": "To avoid data bias to high-frequency entities and relations, we first obtain about 5 million plain texts and 800 thousand triples from CN-DBpedia. The abundant texts and triples contribute to a more reasonable distribution. We use fine-grained named entity recognition (NER) toolkit TexSmart (Zhang et al., 2020) and entity linking (Chen et al., 2018) to align mentioned entities in texts to those in triples. Finally, we construct a distantly supervised dataset D ds with 1.6 million instances, where we select challenging instances in the following steps.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Hard Case Feature Engineering and Seed Selection", "text": "To build a dataset toward practical hard cases, we systematically formulate the nine indicators of hard cases (refer to Section 3) and introduce measurements to quantify them. For example, we calculate the Argument Distance as the number of tokens between the head and tail entity mentions in the text. More details of feature engineering are described in Appendix A. After hard case oriented feature engineering, we discard the instances in D ds without any indicator of hard cases. The remaining part forms a hard case candidate dataset D with about 108 thousand instances. We randomly sample 3,500 instances from D and ask experts to select the hard cases given the context and features. Specifically, if an instance with multiple hard case indicators or with only one indicator but selected by all three experts based on their expertise, it is regarded as a hard case. To further evaluate the quality of selected hard cases, we utilize DGCNN-BERT to test the selected and unselected data. If the F1 score drops \u03b4=10% on the hard cases, we reserve the data to constitute the high quality seeds of hard case D p . The remaining data is easy case D n . In total, we obtain 1,431 seeds of hard cases.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Classifier Training and Hard Case Prediction", "text": "It is impossible to manually select all instances to construct a large-scale dataset. So we utilize a classifier to recall more hard cases similar to the seed samples selected by experts. The classifiers consist of three categories: (1) Decision tree (Quinlan, 1986); (2) Deep classifiers by positive negative (PN) learning (Rakhlin, 2016); (3) Deep classifiers by positive unlabeled (PU) learning (Kiryo et al., 2017;du Plessis et al., 2015). First of all, we adopt the decision tree to make the classifier aware of the indicators explicitly. Then, we form the representation vector as recommended in Baldini Soares et al. ( 2019 et al., 1998) and BiLSTM (Hochreiter and Schmidhuber, 1997), to capture the context information.\nMore training details can be found in Appendix B. We ensemble multiple classifiers by weighted average and distinguish hard cases with high confidence in the original massive unlabeled dataset. Besides, we directly select instances by implicit semantic patterns to explore more hard cases fitting the indicator of Reasoning which is not well quantified by the auxiliary features. Finally, we obtain the dataset D hc ready for annotation.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Crowdsourcing", "text": "To make instances in D hc fully and accurately labeled, we develop a novel three-stage RE annotation platform taking the following two aspects into consideration: (1) Heavy workload of annotating all information at once results in growing negative feedback as the task goes on; (2) Aggregated method, such as majority vote (Dumitrache et al., 2018), is insufficient for complicated and openended tasks. To relieve the pressure of workers, we divide the whole task into three partitions consisting of Relation Annotation, Entity Annotation, and Triple Annotation. Moreover, we utilize patterns and toolkits to provide high-quality recommendations in each stage for higher recall. To capture the label disagreement more thoroughly among workers, we employ CrowdTruth2.0 (Dumitrache et al., 2018), which models the quality of workers, documents, and annotations.\nIn short, in the Relation Annotation, workers select the missed relations or delete wrong recom-mended ones. When all relations are annotated, NER toolkit recommends multiple entity mentions with the corresponding type based on schema information. Workers also need to append new entity mentions or delete incorrect ones in the Entity Annotation. As for Triple Annotation, workers verify the correctness of a candidate triples automatically generated by permutation of entity arguments and relations based on schema. Note that every input data in the three stage is assigned to three different annotators and aggregated by CrowdTruth2.0. Detailed annotation process is in Appendix D.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Experiments", "text": "In this section, we first compare our HacRED with existing datasets. Then we re-evaluate the SOTA RE models on HacRED and systematically analyze their abilities on different experiment settings. At last, we demonstrate the effectiveness of HacRED via a case study.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Data Analysis", "text": "In this section, we analyze various aspects of common RE datasets and HacRED. Data Size. As shown in Table 3, HacRED has a greater average number of words, entities, and triples in each text than all of the sentencelevel datasets. Thus we regard HacRED as a document-level RE dataset. Compared with the document-level datasets, DocRED aims at common document-level RE but not consider performance gaps and various hard cases in practical scenarios. BC5CDR is specially designed for biomedical domain. By contrast, we are the first to analyze the performance gap between popular datasets and practical applications, and propose HacRED which focuses on different kinds of hard cases in general domain. Besides, HacRED is larger in scale and contains much more various relational facts than BC5CDR and DocRED but with lower duplicated triples ratio. Data Distribution. We calculate three global statistic metrics about data distribution of common datasets and HacRED.      and the relation while low-frequency mentions are neglected. All these three aspects reveal the unreasonable data distribution of common datasets.\nIn comparison, we observe a more reasonable data distribution in HacRED from Table 4 and  Table 5. HacRED has a low ratio of duplicate triples and contains various relational facts, which addresses semantic bias. No biased relation existing in HacRED reduces the risk of selection bias. The proportion of top 20% relations promotes the alleviation of relation bias on HacRED. The more comparison of overall data distribution can be found in Appendix E. Data Quality. We evaluate the quality of HacRED through both automatic metrics and human evaluation. Specifically, we first compute the average unit quality score (UQS), annotation quality score (AQS), and worker quality score (WQS) of the whole 9,231 instances. UQS, AQS and WQS are proposed by CrowdTruth2.0 (Appendix F provides more calculation details). The closer these  scores are to 1, the higher quality of the crowdsourcing is. Meanwhile, we randomly sample 400 instances from HacRED and compute the precision, recall, and F1 score with annotations based on the revision of humans. The evaluation scores are reported in Table 7. From this table, our Ha-cRED achieves a considerable annotation quality. As a comparison, NYT contains about 31% noise instances (Riedel et al., 2010) and TACRED has poor annotation quality (Alt et al., 2020).\nHard Case Types. We group the randomly sampled 400 instances into nine categories as shown in Table 6. The proportions of different kinds of instances reflect that HacRED contains a various range of hard cases, which evaluates models comprehensively for practical applications.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Model Evaluation", "text": "As DGCNN-BERT has been used in the main process of construction, we evaluate other strong RE models including joint RE models, pipeline RE models, and DocRC models on HacRED. First, we limit the relation set within 20 types both in Ha-cRED and DuIE, and then separate a part of instances in DuIE to form the contrastive easy case dataset D ec . We carry out the equivalent substitution of hard cases in HacRED for easy ones in D ec in different proportions. Figure 3 shows the F1 curve of the performances w.r.  in performance. The SOTA model CasRel still outperforms other joint models and achieves great F1 on 100% D ec . However, the performance drops on data with more complex instances. We notice that F1 value of easy cases is generally greater than that of hard cases in different substitution ratio settings, which illustrates that RE models indeed struggle when tackling hard cases. Note that by combining HacRED with easy cases in existing datasets, it is easy to simulate diverse practical scenarios.\nIn addition, we split HacRED into train, dev, and test sets with 6231, 1500, 1500 instances respectively. The precision, recall, and F1 score of the three major categories of models are shown in Table 8. The joint and pipeline learning strategies do not contribute to a great F1 on triple extraction. For the NER task, PURE has a separate entity model but results in a 30.61% F1 when all entities in a document are considered, including entities with no positive relation labels. This also reflects the challenge to obtain complete entity information in practical scenarios. On the other hand, the relation classification performances of DocRC models are far from satisfactory. The results suggest that existing models have remarkably poor performance on HacRED compared with humans (Table 9), which indicates that RE applicable for practical hard cases still requires further research.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Human Performance", "text": "We randomly select 200 contexts from test set and ask three volunteers to extract relational facts in an end-to-end manner. Schema information like entity type set as well as relation set is provided but no entity mentions. As for relation classification task, three volunteers select the relation, including NA regarded as negative, of the given entity pair. As demonstrated in Table 9, humans fulfill excellent results which indicate the possible ceiling performance on HacRED.   ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Detailed Analysis", "text": "In this section, we give insight into the abilities of current mainstream joint models when tackling different kinds of hard cases and propose some research indications as well. As it is hard to obtain complete entity information in practical scenarios, we do not consider DocRC models in this section that entity information is provided as input.\nMultiple Triples. Table 11 shows the F1 score of existing models when extracting from texts with different number of triples. The performance of NovelTagging and CopyRE decreases as the number of triples increases, which indicates that the novel tagging schema and multiple decoder mechanism are not able to address the challenge of Multiple Triples. Since GraphRel predicts relations for all word pairs and CasRel learns separate entity tagger for different relations, these two models alleviate this problem. An interesting point is that the performance of GraphRel and CasRel rises as the number of triples increases when the triples number is less than 16, indicating that these two models work well in texts with number of triples nearing the average. However, all models get F1 score below average when text mentions have more than 16 triples.  BiLSTM-based neural models like NovelTagging and CopyRE. The performance improvement on CasRel suggests the powerfulness of BERT encoder in the long-distance context. Homogeneous Entities and Similar Relations.\nSince the text mentions multiple homogeneous entities and semantically similar relations, models are required to distinguish the fine-grained difference of the context to extract the correct triples. The first two columns in Table 10 have similar results, which indicates that the contexts with homogeneous entities and similar relations are as challenging as the long-distance contexts.\nLong-tail Relations. We observe a dramatic decrease on the instances with long-tail relational triples. As long-tail relations are common in realworld scenarios, a more efficient learning method is required to make RE models applicable for practical applications.\nOverlapping Triples. CasRel achieves a better performance on extracting overlapping triples. This proves the effectiveness of cascade binary tagging strategy by first identifying the head mention and then extract the corresponding tail mention given a relation. Specifically, the F1 scores of overlapping head and tail mentions are 66.38% and 47.44% respectively. Similarly, results of the two above metrics in CopyRE are 13.31% and 3.57%. The relative higher performance on overlapping head mentions than tail mentions also suggests that the order of extracting arguments could have effect on the results.\nDistractor and Reasoning. We manually select instances with Distractor and Reasoning indicators in HacRED because they cooccur frequently in corpus. As illustrated in Table 10, we observe a drop of the F1. This suggests that models are vulnerable to this kind of instances. However, there are lots of texts with distractions or implicit expression, which needs reasoning, and even common sense. The model design should take the reasoning mechanism into consideration in the future work.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Case Study", "text": "As shown in Figure 4, the text mentions multiple organization entities and similar relations including graduate_from and affiliation_of.\nThe incorrect triple (Lu, graduate_from, Yanjing University) extracted by CasRel represents that models struggle to capture fine-grained semantic information. The distractive phrases study for a doctorate could result in the incorrect extraction (Wu, graduate_from, University of Chicago), which can be rectified by comprehending the context of before finishing his doctoral dissertation.\nReasoning is needed to extract the triple (Wu, af filiation_of, Yanjing University) since he worked as a professor in the organization.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "In order to effectively evaluate the RE models and accelerate the research of practical RE, we first analyze the performance gap between popular datasets and practical applications. Therefore, we construct a large-scale and high-quality Ha-cRED with reasonable data distribution and sufficient hard cases. To focus on the practical challenging cases, we propose a case-oriented construction framework. We also design a novel annotation method to guarantee the quality of Ha-cRED. Finally, we conduct extensive experiments and analyze the abilities of SOTA models from various aspects, which provides a deeper understanding of RE models and inspiration for further improvement.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A More Examples of Hard Cases in NYT", "text": "In  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C Details of Classifier Training", "text": "A decision tree is learned by the auxilliary features calculated in stage 2. For deep models, we concatenate multiple embeddings and auxilliary features to make up the input. We add special tokens to mark the border of each entity and generate the representation vector as recommended in Baldini Soares et al. (2019). We assign a label 1 to   each instance in D p and \u22121 in D n . The deep models output the probability of the instance belonging to hard cases and are optimized with the binary cross entropy loss objective. To start PU learning, we sample from D to form a unlabeled dataset D u and set the hyperparameter \u03c0 p = 0.41 estimated by the proportion of hard cases selected by experts. We implement nnPU (Kiryo et al., 2017) which is efficient for massive data and deep learning and use J nnpu as the optimized objective,\nJ nnpu = \u03c0 p \u2022E p(x|y=1) [l(g(x))]+ max{0, E p(x) [l(\u2212g(x))]\u2212 \u03c0 p \u2022E p(x|y=1) [l(\u2212g(x))]} (1)\nwhere \u03c0 p = p(y = 1), g is decision function, l is surrogate loss function. We choose the double hinge loss l = max(\u2212z, max(0, 1 2 \u2212 1 2 z)) proposed by (du Plessis et al., 2015).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "D Three-stage Annotation Method", "text": "We illustrate the three-stage annotation method. Given the context in Figure 5, director, cast_member, and adapted_by is appended to the annotation of Stage 1 by relational pattern. Crowdsourcing workers select the missing relation such as author. When all relation mentions are annotated, NER toolkit recommend multiple entity mentions with the corresponding type. Workers need to select the highlighted words that are not covered by entity recommendation in the Stage 2. After stage 2, all mentions in context with specific type are obtained. As the example shown in Figure 5, given the target entity type of PERSON, platform recommends the candidates including PERSON-1 to PERSON-4. Workers select highlighted words PERSON-5 which is missed. In the final stage, we generate the candidate triples automatically by permutation of arguments and relations based on triple schema. Due to the relation director connects arguments with entity type PERSON and FILM, we generate the triple (PERSON-2, director, FILM) and ask annotator to verify the correctness. Note that we employ the powerful quality control method crowdtruth2.0 in every stages to prevent error propagation. As a result, all triples marked as valid are saved.\nE Calculation of the UQS, AQS, and WQS Metrics in CrowdTruth2.0\nWe give the details of the calculation in data quality evaluation. We calculate the three metric unit quality score (UQS), annotation quality score (AQS), and worker quality score (WQS) by CrowdTruth2.0 (Dumitrache et al., 2018) on the whole 9,231 instances in HacRED proposed as follows, where W 1 , W 2 is the weight of the iteration method and is initialized as one, u is the unit for annotation,a is one annotation given a unit, i, j denotes the different workers. We straightforward report the average of these metrics in Section 5.1.\nU QS(u) = \u2211 i,j W1(i, j, u)W QS(i)W QS(j) \u2211 i,j W QS(i)W QS(j)\n(2) AQS(a) = \u2211 i,j W QS(i)W QS(j)Pa(i|j) \u2211 i,j W QS(i)W QS(j)\n(3) W1(i, j, u)W QS(j)U QS(u) \u2211 j,u W QS(j)U QS(u)\nW QS(i) = W U A(i)W W A(i) W U A(i) = \u2211 u W2(u, i)U QS(u) \u2211 u U QS(u) W W A(i) = \u2211 j,u\n(4)", "n_publication_ref": 2, "n_figure_ref": 2}, {"heading": "Acknowledgements", "text": "We would like to thank the anonymous reviewers for their thoughtful and constructive comments.\nThis ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "TACRED revisited: A thorough evaluation of the TACRED relation extraction task", "journal": "", "year": "2020", "authors": "Christoph Alt; Aleksandra Gabryszak; Leonhard Hennig"}, {"title": "Matching the blanks: Distributional similarity for relation learning", "journal": "Association for Computational Linguistics", "year": "2019", "authors": " Livio Baldini; Nicholas Soares; Jeffrey Fitzgerald; Tom Ling;  Kwiatkowski"}, {"title": "Translating embeddings for modeling multi-relational data", "journal": "", "year": "2013", "authors": "Antoine Bordes; Nicolas Usunier; Alberto Garc\u00eda-Dur\u00e1n; J Weston; Oksana Yakhnenko"}, {"title": "Short text entity linking with fine-grained topics", "journal": "", "year": "2018", "authors": "Lihan Chen; Jiaqing Liang; Chenhao Xie; Y Xiao"}, {"title": "Question answering over Freebase with multicolumn convolutional neural networks", "journal": "Long Papers", "year": "2015", "authors": "Li Dong; Furu Wei; Ming Zhou; Ke Xu"}, {"title": "Crowdtruth 2.0: quality metrics for crowdsourcing with disagreement", "journal": "", "year": "2018", "authors": "Anca Dumitrache; Oana Inel; Lora Aroyo; Benjamin Timmermans; Chris Welty"}, {"title": "GraphRel: Modeling text as relational graphs for joint entity and relation extraction", "journal": "", "year": "2019", "authors": "Tsu-Jui Fu; Peng-Hsuan Li; Wei-Yun Ma"}, {"title": "FewRel 2.0: Towards more challenging few-shot relation classification", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Tianyu Gao; Xu Han; Hao Zhu; Zhiyuan Liu; Peng Li; Maosong Sun; Jie Zhou"}, {"title": "Creating training corpora for NLG micro-planners", "journal": "", "year": "2017", "authors": "Claire Gardent; Anastasia Shimorina; Shashi Narayan; Laura Perez-Beltrachini"}, {"title": "More data, more relations, more context and more openness: A review and outlook for relation extraction", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Xu Han; Tianyu Gao; Yankai Lin; Hao Peng; Yaoliang Yang; Chaojun Xiao; Zhiyuan Liu; Peng Li; Jie Zhou; Maosong Sun"}, {"title": "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation", "journal": "", "year": "2018", "authors": "Xu Han; Hao Zhu; Pengfei Yu; Ziyun Wang; Yuan Yao; Zhiyuan Liu; Maosong Sun"}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "journal": "", "year": "2009", "authors": "Iris Hendrickx; S Kim; Zornitsa Kozareva; Preslav Nakov; \u00d3 Diarmuid; Sebastian S\u00e9aghdha; M Pad\u00f3; Lorenza Pennacchiotti; S Romano;  Szpakowicz"}, {"title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"title": "Ddrel: A new dataset for interpersonal relation classification in dyadic dialogues", "journal": "ArXiv", "year": "2020", "authors": "Qi Jia; Hongru Huang; K Q Zhu"}, {"title": "Positiveunlabeled learning with non-negative risk estimator", "journal": "", "year": "2017", "authors": "Ryuichi Kiryo; Gang Niu"}, {"title": "2020. The relx dataset and matching the multilingual blanks for cross-lingual relation classification", "journal": "", "year": "", "authors": "Abdullatif Koksal; A Ozgur"}, {"title": "Gradient-based learning applied to document recognition", "journal": "Proceedings of the IEEE", "year": "1998", "authors": "Yann Lecun; L\u00e9on Bottou; Yoshua Bengio; Patrick Haffner"}, {"title": "Biocreative v cdr task corpus: a resource for chemical disease relation extraction", "journal": "Database: The Journal of Biological Databases and Curation", "year": "2016", "authors": "J Li; Yueping Sun; Robin J Johnson; Daniela Sciaky; Chih-Hsuan Wei; Robert Leaman; A P Davis; C Mattingly; Thomas C Wiegers; Zhiyong Lu"}, {"title": "Reasoning with latent structure refinement for document-level relation extraction", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Guoshun Nan; Zhijiang Guo; Ivan Sekulic; Wei Lu"}, {"title": "Convex formulation for learning from positive and unlabeled data", "journal": "", "year": "2015", "authors": "Marthinus Christoffel Du Plessis; Gang Niu; Masashi Sugiyama"}, {"title": "Induction of decision trees. Machine learning", "journal": "", "year": "1986", "authors": "J ; Ross Quinlan"}, {"title": "Convolutional neural networks for sentence classification", "journal": "GitHub", "year": "2016", "authors": "A Rakhlin"}, {"title": "Modeling relations and their mentions without labeled text", "journal": "", "year": "2010", "authors": "Sebastian Riedel; Limin Yao; Andrew Mccallum"}, {"title": "A hierarchical framework for relation extraction with reinforcement learning", "journal": "ArXiv", "year": "2019", "authors": "Ryuichi Takanobu; Tianyang Zhang; Jiexi Liu; Minlie Huang"}, {"title": "A novel cascade binary tagging framework for relational triple extraction", "journal": "", "year": "2020", "authors": "Zhepei Wei; Jianlin Su; Yue Wang; Yuan Tian; Yi Chang"}, {"title": "Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction", "journal": "ArXiv", "year": "2021", "authors": "Benfeng Xu; Qiangwen Wang; Yajuan Lyu; Yong Zhu; Zhendong Mao"}, {"title": "Cndbpedia: A never-ending chinese knowledge extraction system", "journal": "", "year": "2017", "authors": "Bo Xu; Yong Xu; Jiaqing Liang; Chenhao Xie; Bin Liang; Wanyun Cui; Y Xiao"}, {"title": "DocRED: A large-scale document-level relation extraction dataset", "journal": "", "year": "2019", "authors": "Yuan Yao; Deming Ye; Peng Li; Xu Han; Yankai Lin; Zhenghao Liu; Zhiyuan Liu; Lixin Huang; Jie Zhou; Maosong Sun"}, {"title": "Dwie: an entity-centric dataset for multi-task document-level information extraction", "journal": "", "year": "2009", "authors": "J Klim Zaporojets; Chris Deleu; Thomas Develder;  Demeester"}, {"title": "Double graph based reasoning for documentlevel relation extraction", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Shuang Zeng; Runxin Xu; Baobao Chang; Lei Li"}, {"title": "Extracting relational facts by an end-to-end neural model with copy mechanism", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Xiangrong Zeng; Daojian Zeng; Shizhu He; Kang Liu; Jun Zhao"}, {"title": "Texsmart: A text understanding system for fine-grained ner and enhanced semantic analysis", "journal": "", "year": "2020", "authors": "Haisong Zhang; Lemao Liu; Haiyun Jiang; Yangming Li; Enbo Zhao; Kun Xu; Linfeng Song; Suncong Zheng; Botong Zhou; Jianchen Zhu; Xiao Feng; Tao Chen; Tao Yang; Dong Yu; Feng Zhang; Zhanhui Kang; Shuming Shi"}, {"title": "Positionaware attention and supervised data improve slot filling", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Yuhao Zhang; Victor Zhong; Danqi Chen; Gabor Angeli; Christopher D Manning"}, {"title": "Joint extraction of entities and relations based on a novel tagging scheme", "journal": "ArXiv", "year": "2017", "authors": "Suncong Zheng; Feng Wang; Hongyun Bao; Yuexing Hao; Peng Zhou; Bo Xu"}, {"title": "A frustratingly easy approach for joint entity and relation extraction. ArXiv, abs", "journal": "", "year": "2010", "authors": "Zexuan Zhong; Danqi Chen"}, {"title": "Document-level relation extraction with adaptive thresholding and localized context pooling. ArXiv, abs", "journal": "", "year": "2010", "authors": "Wenxuan Zhou; Kevin Huang; Tengyu Ma; J Huang"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Triples", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Cases and corresponding triples in WebNLG and practical applications.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_3", "figure_caption": "FILM\u2026\u2026adaptedFigure 5 :5Figure 5: The illustration of three-stage annotation method.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Text 1: \"...\" said Joseph Bastianich, who owns Del Posto with his mother, Lidia Bastianich, and the chef, Three entity mentions with the same type of PERSON are mentioned in the text and the word mother may lead to wrong prediction children.", "figure_data": "Mario Batali.Annotation: NAPrediction: children_ofIndicators: Distractor, Homogeneous EntitiesInterpretation: Text 2: ... Lieberman, who was defeated by the polit-ical upstart Ned Lamont in Connecticut's Democraticprimary earlier this month.Annotation: place_livedPrediction: place_of_birthIndicators: Similar RelationsInterpretation: The relation place_lived andplace_of_birth are similar in semantics.Text 3: One of the most brutal tyrants of recent his-tory, Saddam Hussein unleashed devastating regionalwars and reduced oil-rich Iraq to a claustrophobic po-lice state.Annotation: nationalityPrediction: place_of_deathIndicators: ReasoningInterpretation: Reasoning is required to get the rela-tion nationality based on the context that Husseinis the tyrants of Iraq."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Examples of hard cases in NYT. The head and tail mentions are colored accordingly. tical types. We observe the high error rate in relations like children and parents when the text mentions different entities with type PERSON.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The case-oriented construction framework of building HacRED which consists of four stages. The right part correspondingly describes each stage. Through the construction, the texts and triples are established.", "figure_data": "Knowledge BaseHard Case Exploratory\u2460CN-DBpediatext ; triples text ; triples text ; triples\u2026Analysis\u2460Data Collection\u2461candidate hard casesHard Case Indicators FilterHard Case Feature\u2461Engineering andSeed Selectioneasy cases\u2462Target Instances Prediction\u2462\u2463Annotation\u2463NER ToolkitRelational PatternText Microsoft \u2026Triples .. , founder_of, .. .. , found_time, ..Crowd Truth 2.0Crowd Truth 2.0recommendation Entity AnnotationCrowd Truth 2.0recommendation Relation Annotation\u2026\u2026Figure 2: IndicatorWebNLGDuIEoriginal error original errorText Length1839332Argument Distance1230517Distractors15413Reasoning-319Homogeneous Ent.2341921Similar Rel.9542717Long-tail Rel.15-2Multiple Triples1759893Overlapping Triples25641633"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": ") and utilize classical PN learning on D p and D n to train the basic classifiers. Since the easy cases are extremely diverse and D n can not represent the entire distribution of easy cases, we leverage the massive unlabeled data in D ds by introducing PU learning to improve the generalization of hard cases classification. Besides, we train deep models based on different neural network structures, including CNN (LeCun", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "show the results. Specifically, 84.29% of the triples in NYT and 91.20% in WebNLG are duplicate, which results in a bias to high-frequency triples of same entity pairs (known as semantic bias for models). For example, (Beijing, capital_of, China) occurs frequently in corpus and models still extract this triple from Beijing is a historic city in China. Mean-", "figure_data": "Dataset# Text# Relation # Triple # FactAvg. Sent.Avg. Word  \u2021Avg. Ent.Avg. Triplesentence-level datasetSemEval1013,4341013,43410,2511.017.42.01.0NYT66,19424104,339 16,3872.137.82.21.6WebNLG6,22217114,4851,2752.524.03.152.3TACRED106,2644121,7735,9761.033.22.01.0document-level datasetBC5CDR1,50013,1162,4347.4188.019.52.1DocRED5,0539663,42756,3548.0198.326.212.5HacRED9,2312667,04765,2255.0126.610.87.4"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Statistics of common RE datasets and HacRED. Note that the Avg.Word is computed at word-level vocabulary, which means (China), two characters in Chinese, is regarded as one word. The average length of documents at character-level is 204.2 in HacRED.", "figure_data": "DatasetDuplicated TriplesBiased RelationsTop 20% Relation Triplessentence-level datasetSemEval1023.69%0.00%44.92%NYT84.29%58.33%98.93%WebNLG91.20%94.74%77.57%TACRED72.55%9.52%91.33%document-level datasetBC5CDR21.89%--DocRED11.15%12.50%71.46%HacRED2.72%0.00%49.96%"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": "#Rel, #T riples of top20% Rel #T riples, respec-tively. If the highest-frequency mention is involved inmore than 10% triples of the given relation, we regardit as a biased relation.DatasetRelation ExampleHighest-frequency Mention (Ratio)WebNLGcounty_seatTexas (72.73%)NYTperson_profession Bavetta (50.00%)DocREDsister_cityChipilo (35.29%)HacREDdynastyTang (4.20%)"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Example of relations which could lead to selection bias in WebNLG, NYT, and DocRED.", "figure_data": "In Ha-"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Statistics about the proportion of instances fitting different hard case indicators on HacRED.", "figure_data": "CrowdTruth 2.0Human (%)Avg. UQS \u2191 0.9373Precision 97.29Avg. AQS \u2191 0.9446Recall94.64Avg. WQS \u2191 0.9557F195.94"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Results of different quality metrics on Ha-cRED.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Model performance on HacRED test set(%). NER results are computed based on the entities involved in the gold triples of each instance.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Human performance (%).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "The F1 curve of the model performance on different mix ratios of hard and easy cases.", "figure_data": "5.68 11.5 31.33 48.786.57 14.48 27.61 48.07 F1 curve on entire mixed dataset 9.18 16.62 26.79 51.1 14.41 22.62 26.39 53.65 23.05 28.57 33.08 54.86 44.83 45.43 56.45 64.07 NovelTagging CopyRE GraphRel CasRel0 10 20 30 40 50 60 70 80 900.00.2 31.3 34.87 Substitution Ratio 0.4 0.6 28.54 58.1 33.37 31.14 63.58 39.77 36.5 63.56 42.39 0.8 41.48 64.79 F1 curve on easy cases NovelTagging CopyRE GraphRel CasRel1.0 45.43 56.45 64.070 10 20 30 40 50 60 70 80 900.0 5.68 11.5 31.33 48.780.2 5.18 12.99 27.56 48.89 F1 curve on hard cases 0.4 0.6 0.8 Substitution Ratio 5.27 6.47 9.55 13.36 12.92 22.75 13.98 20.69 26.21 48.46 47.08 42.77 NovelTagging CopyRE GraphRel CasRel1.0Figure 3: ModelText Length Argument DistanceHomo. Ent. Similar Rel.Long-tail Rel.Overlapping TriplesDistractor ReasoningOverallNovelTagging4.994.331.723.999.235.31CopyRE5.473.901.286.597.3011.12GraphRel30.1527.820.0834.6729.8132.65CasRel45.3445.6013.5453.3444.0048.85"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "F1 score on HacRED instances with different indicators of hard cases (%).", "figure_data": ""}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "F1 score on HacRED test set with different number of triples (%).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Wu graduated from Manchester College ... and went to University of Chicago to study for a doctorate ... President Lu invited him to teach western literature at Yanjing University. Wu resolutely came back to homeland and became a professor before finishing his doctoral dissertation.", "figure_data": "Case in HacREDWu, graduate_from, Manchester College Lu, affiliation_of, Yanjing University Wu, affiliation_of, Yanjing University Annotations Hard Case Indicators \u2026 Predictions ... Homogeneous Entities, Similar Relations, Distractor, Reasoning Wu, graduate_from, University of Chicago Lu, graduate_from, Yanjing University \u2026Figure 4: An example of hard cases in HacRED withmultiple indicators."}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Sixten Ehrling, ..., and directed the conducting programs at the Juilliard School and ... Multiple Triples, Overlapping Triples Interpretation: The text mentions multiple triples and entities such as Ethiopia are involved in different triples.", "figure_data": ", we provide additional error-prone ex-amples in NYT that fit other indicators of practicalhard cases including Text Length, Argument Dis-tance, Multiple Triples, and Overlapping Triples.We have illustrated the instances with other indica-tors in Section 3.Text 1: Annotation: affiliation ofPrediction: major shareholder ofIndicators: Text Length, Argument DistanceInterpretation: The text contains many words and thedistance between head and tail mention is much far.There is no indicating phrases such as work in directlyrevealing the relation affiliation of.Text 2: Though officials in Addis Ababa , Ethiopia'scapital , ...Annotation: administrativedivisions,contains, capitalPrediction: capital ofIndicators:"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "Examples of hard cases in NYT. The head and tail mentions are colored accordingly.B Details of Feature EngineeringWe calculate the Text Length and Argument Distance as the number of tokens in the text and between the head and tail entity mentions. Homogeneous Entities are measured by the NER results of TexSmart and equal to number of entities with same NER tag. The measurements of Distractors, Similar Relations are based on pre-defined schemas and auxiliary information, part of which is shown in Table13. Multiple Triples and Overlapping Triples are computed by the triples from DS. As reasoning can not be implicitly quantified, we suppose the deep neural models to capture the features of context.", "figure_data": ""}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Examples of pre-defined schemas and simple auxiliary informations to measured the indicator_distractor and similar_rels. Experts define some implicit expressions such as receive a degree reveals the relation gradu-ated_from and distractive phrases like ex-wife for spouse.", "figure_data": ""}], "doi": "10.18653/v1/2020.acl-main.142"}