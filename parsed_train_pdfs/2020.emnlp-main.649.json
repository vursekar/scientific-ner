{"authors": "Rishi Bommasani; Claire Cardie", "pub_date": "", "title": "Intrinsic Evaluation of Summarization Datasets", "abstract": "High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or post hoc. Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees. In spite of this, data quality has gone largely unquestioned for many recent summarization datasets. We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets. We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the datasets employed. Further, we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples.", "sections": [{"heading": "Introduction", "text": "Data understanding is fundamentally important in natural language processing (NLP); for data-driven learning-based methods (e.g. neural networks), the quality of the training data bounds the quality of models learned using it. Therefore, understanding this data is necessary in order to ensure that models learn to perform a given task correctly.\nUnderstanding data is a multidimensional problem. One line of inquiry has demonstrated why prominent datasets are insufficiently challenging: many data examples can be solved by alternative heuristics that do not encode an approach that is faithful to the task (McCoy et al., 2019). From the perspective of datasets, several works have shown that standard datasets in areas such as visual question answering Kafle and Kanan, 2017), natural language inference (Gururangan et al., 2018;Poliak et al., 2018), and reading comprehension (Kaushik and Lipton, 2018) contain annotation artifacts that often give rise to these spurious correlations or reasoning shortcuts. Data understanding can also inform scientific and ethical decision-making (Bender and Friedman, 2018;Gebru et al., 2018;Mitchell et al., 2019) with recent work studying how social biases encoded in training data propagate to learned models (Zhao et al., 2019;Tan and Celis, 2019).\nIn this work, we extend these efforts towards the setting of summarization. We find this to be particularly timely since several summarization datasets have been released in recent years with little discussion of data quality. While prior work on evaluating NLP datasets has focused on their difficulty, transparency, or bias, we consider broadly the overall quality of the dataset -in our case, for the task of summarization. 1 Our central insight is that desirable properties of a summary can be readily estimated by adapting and applying existing NLP methods. With this in mind, we present a multiaspect large-scale study of summarization datasets that dissects summarization into 5 properties that are evaluated across 10 datasets spanning multiple summarization domains. Our analysis reveals that our metrics can serve as lightweight detectors of generically low quality examples. Most strikingly, we show that quantifiable aspects of summarization datasets are inconsistent with their use by the NLP community in several instances.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Motivation", "text": "Quality assurance for data. Nuanced understanding of data is requisite for drawing sound scientific conclusions. In particular, without evaluating for the quality and accuracy of data used to test models, it is impossible to be certain that progress is being made and that successive iterations of models truly make progress on the underlying task or linguistic phenomena of interest.\nWithin NLP, iconic datasets such as the Penn Treebank (Marcus et al., 1993) have sustained subareas such as language modelling, part-of-speech tagging, and syntactic parsing for years due to the painstaking annotation efforts put into making these high-fidelity resources. And in the context of summarization, initial datasets, such as those produced during the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) evaluations, implemented fine-grained verification of data quality. 2 In part due to the emergence of data-hungry modelling techniques, the demands for larger datasets often render quality assurance procedures of this standard to be impractical and infeasible. Nonetheless, several recent natural language understanding datasets (Bowman et al., 2015;Rajpurkar et al., 2016;Suhr et al., 2017) institute explicit qualitycontrol procedures in crowd-sourcing dataset construction (Zaidan and Callison-Burch, 2011;Yan et al., 2014;Callison-Burch et al., 2015), such as using additional annotators to validate annotations (c.f. Geva et al., 2019). In the sibling subfield of machine translation, which often shares similar modelling challenges and evaluation regimes as summarization due to the shared nature of being sequence-to-sequence natural language generation tasks, the annual WMT conference 3 consistently furnishes high quality data. In summary, ensuring data quality is both crucial and challenging. And in comparison with other subareas of NLP, we argue that summarization has lagged behind in rigorously ensuring the quality of widely-used datasets.\nRelating data quality and model quality. The correctness and quality of data inherently bounds what can be learned from the data about the task of interest. From an information-theoretic perspective, this can be made fully formal as follows: For fully learning-based methods, especially those with weak/minimal inductive biases such as neural networks, I(S; A) is approximately zero. While I(S; P ) may be greater than zero (e.g. language modelling pretraining provides statistical information that may facilitate a model to avoid a priori unlikely summaries), standard pretraining regimes such as large-scale language modelling over generic text corpora (Devlin et al., 2019;Raffel et al., 2019) are likely insufficient to meaningfully learn to summarize. Under these assumptions, the mutual information between S and M is critically upper-bounded in terms of I (S; T ). We hypothesize that the quality of the training dataset T is highly correlated with its mutual information with respect to the summarization task S, I(S; T ). One size does not fit all. Sp\u00e4rck Jones (1999) famously argued that summarization systems should be understood conditional on the context in which they will be used. In recent years, the field has significantly departed from this perspective and primarily studied \"general-purpose summarization\" (Kryscinski et al., 2019), which she denounced as ignis fatuus. With our work, we adopt the perspective that for all datasets it is strictly preferable to have all properties quantified; it is the responsibility of practitioners building summarization systems to accurately weight different metrics based on their ultimate goals and use cases. As such, we refrain from providing prescriptive domain-agnostic or context-agnostic notions of summarization.", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Metrics", "text": "In this work, we evaluate the quality of a dataset by aggregating scores for each example in the dataset. We conjecture that for many NLP tasks, estimating the quality of a particular data example is of similar complexity as correctly performing the task on the example. 5 Nevertheless, for summarization, our insight is that various aspects of a summarization example (a document-summary pair) can be reliably estimated by re-purposing existing NLP methods. We are guided by pioneering work (Luhn, 1958;Edmundson, 1969;Mani, 1999) that defined core properties of summarization systems and influential sub-sequent work (Radev et al., 2002;Nenkova, 2006;Nenkova and McKeown, 2012;Peyrard, 2019a) that refined and extended these properties. From this literature, we specifically study compression, topic similarity, abstractivity, redundancy, and semantic coherence as these properties are of recurring and sustained interest. 6 For each abstract property, numerous concrete methods can be proposed to quantify it. In Appendix A, we describe alternatives we considered and detail how we decided which methods performed best. We restrict discussion to the bestperforming approaches in the main paper. Notation. Our metrics will assume indexed sets D, S such that summary S i \u2208 S summarizes document D i \u2208 D. The length in words of a sequence s is |s| and the length in sentences is s . Each metric assigns a value x \u2208 [0, 1] to every (D i , S i ) where 1 is the maximal score and example-level scores are averaged to yield a dataset-level score. Compression. We quantify compression at the word (w) and sentence (s) levels:\nCMP w (D i , S i ) = 1 \u2212 |S i | |D i | (1) CMP s (D i , S i ) = 1 \u2212 S i D i (2)\nTopic Similarity. We learn a topic model M on training corpus T with k topics using LDA (Blei et al., 2003) and quantify topic similarity by comparing the inferred topic distributions \u03b8 D i |M , \u03b8 S i |M for a given summary and document:\nTS (D i , S i ) = 1 \u2212 JS(\u03b8 D i |M , \u03b8 S i |M ) (3)\nwhere JS is the Jensen-Shannon distance. We set k = 20 and T = D. Abstractivity. Grusky et al. (2018) introduced fragments F(D i , S i ), which are greedily-matched spans shared between D i and S i . We quantify abstractivity as a normalized function of the aggregate fragment length; our definition generalizes the definition of Grusky et al. (2018). We set p = 1.\nABS p (D i , S i ) = 1 \u2212 f \u2208F (D i ,S i ) |f | p |S i | p (4)\nRedundancy. ROUGE (Lin, 2004) implicitly penalizes redundancy but underestimates its detrimental impacts (Chaganty et al., 2018). However, we find that ROUGE is effective for identifying redundancy given the definitional focus on overlapping spans. We quantify redundancy as the average ROUGE-L 6 Different names and interpretations have been given for these properties in the literature. We revisit this in Appendix A in discussing alternate metrics.\nF -score for all pairs of distinct sentences in the summary.\nRED (S i ) = mean (x,y)\u2208S i \u00d7S i ,x =y ROUGE (x, y) (5)\nSemantic Coherence. We evaluate the semantic coherence of multi-sentence summaries by predicting the probability of each successive sentence conditioned on the previous one using a powerful language model, BERT (Devlin et al., 2019), pretrained with precisely this objective.\nSC (S i ) = ||S|| j=2 1 BERT(S j i | S j\u22121 i ) ||S i || \u2212 1 (6)\n4 Data\nWe study the following 10 summarization datasets that have been frequently used in recent years. 7 Table 1 contains standard dataset statistics in the upper half and our aspect-level scores in the lower half; datasets are grouped by domain. CNN-DM (Hermann et al., 2015;Nallapati et al., 2016) is a dataset composed of CNN and Daily Mail news articles with summaries that are a concatenated list of highlight bullet points.\nNYT (Sandhaus, 2008) is a dataset of curated New York Times articles paired with abstracts written by library scientists. NWS (Grusky et al., 2018) is the Newsroom dataset of news articles drawn from 38 top English publishers paired with multi-sentence summaries written by the original authors and editors. GW (Graff and Cieri, 2003) is the Gigaword headline generation dataset that some refer to as a summarization dataset (Rush et al., 2015;Chopra et al., 2016). Examples in the dataset are drawn from seven news sources and are the article prefix paired with its headline.\nXSum (Narayan et al., 2018) is an extreme summarization dataset where BBC articles are paired with single-sentence summaries written generally by the author of the article that tries to motivate the BBC audience to read the article by answering \"What is the article about?\". PeerRead (Kang et al., 2018) is a dataset of paper drafts from top-tier computer science venues as well as arXiv. 8 Consistent with its use in the summarization community, we consider the full introduction to be the source document and the ab-stract to be the target summary.\nPubMed (Cohan et al., 2018) is a dataset of papers drawn from the biomedical and life sciences. Unlike PeerRead, the full paper is taken as the document but the summary is still specified as the abstract. TL;DR (V\u00f6lske et al., 2017) is a dataset of userwritten articles from the social media platform Reddit along with the author-provided courtesy summaries that tend to be multi-sentence. V\u00f6lske et al. (2017) applied a series of preprocessing procedures to filter out bot-generated content. AMI (Carletta et al., 2005) is a dataset of transcribed meetings, some which are naturally occurring and the rest of which are elicited, with handannotated summaries. The transcription process has multiple steps that are described extensively by Carletta et al. (2005). Various additional data provided within the AMI dataset is neglected in this work. MovieScript (Gorinski and Lapata, 2015) is a dataset of movie scripts drawn from the Script-Base corpus that are aligned with user-written summaries sourced either from Wikipedia or IMDB.\nVarious additional data provided within the Movi-eScript dataset is neglected in this work.", "n_publication_ref": 31, "n_figure_ref": 0}, {"heading": "Results and Analysis", "text": "Compression scores quantitatively disambiguate summarization tasks.\nConcretely, we observe GW has the lowest compression scores and while GW is sometimes described as a summarization dataset (Rush et al., 2015;Chopra et al., 2016), it is better seen as a headline generation dataset that is more in the style of sentence compression (as is suggested by S i = D i = 1). Conversely, AMI and Movi-eScript achieve the highest scores by a substantial margin and are long-document summarization datasets. Classifying new summarization datasets accurately may prove useful given that successful methods from one domain often do not extend to another and this shortcoming in generalization can be attributed to the differences in compression requirements (Cohan et al., 2018). Given the goals stated in the XSum dataset paper, TL;DR may be a better choice than XSum. In particular, Narayan et al. (2018) introduce XSum as a large dataset that legitimately requires abstraction. While XSum is more abstractive than other News datasets (barring GW) and is relatively large, TL;DR displays greater abstractivity, similar length summaries, and is 15 times larger. That said, Narayan et al. (2018) explore topic-oriented strategies in their work and such methods may be better suited to XSum given the TS scores. CNN-DM and NYT are suboptimal for studying abstractive/extractive systems respectively. Several recent works (See et al., 2017;Paulus et al., 2018;Li et al., 2018) have used CNN-DM to build and evaluate abstractive systems. Conversely, NYT has been used to build extractive systems (Hong and Nenkova, 2014;Li et al., 2016). Given our findings, we find both of these trends to be inconsistent with dataset properties and suboptimal given other preferable datasets for these purposes: CNN-DM is one of the least abstractive datasets and there are larger and more extractive alternatives to NYT such as NWS. Especially in the case of CNN-DM, we note that training learning-based systems (e.g. neural methods) using data with limited abstractivity implies the resulting summarizers will be limited in their ability to generate genuinely abstractive text. This is validated by empirical findings as both See et al. (2017) and Zhang et al. (2018) observe limited abstractivity in abstractive systems trained on CNN-DM. In light of this, we argue systems should be characterized as abstractive or not based on their empirical behavior rather than their theoretical capability. 9 CNN-DM is not a representative benchmark for summarization as a whole. Recent work (Kryscinski et al., 2019;Raffel et al., 2019) has explicitly portrayed CNN-DM as the benchmark dataset for summarization; the field has implicitly done this for several years (Kryscinski et al., 2019). While there is clear value in evaluating pretrained representations on summarization datasets, we caution against using CNN-DM as a stand-in for the entire summarization subfield. Instead, we suggest using a diverse group of datasets and not reducing a highly heterogeneous subfield to a single dataset. While this adds additional overhead, this cost is necessary to draw meaningful conclusions about the impact of advances on summarization broadly given the pronounced diversity in summarization datasets (Table 1). Post-processing methods for mitigating redundancy may be needed for practical systems. While evaluation on standard datasets using ROUGE  may not penalize for this, redundancy is clearly undesirable (Carbonell and Goldstein, 1998;Peyrard, 2019a) and existing datasets (and thereby systems learned using that data) display significant amounts of redundancy in their gold-standard summaries (exceptions are datasets with short summaries where cross-sentence redundancy is constrained to be low). Specifically, Nenkova (2006) argues that redundancy is a clear inhibitor for practical application of summarization systems. Consequently, post hoc methods that reduce redundancy after initial evaluation may be useful in generating summaries that are suitable for human users.\nSemantic coherence captures observable variation in summary coherence.\nWe observe that the Scientific summaries (which are abstracts of published papers) are clearly more coherent than the author-generated summaries in TL;DR, the fragmented summaries in AMI, and the concatenated bullet-point summaries in CNN-DM. We find that this distinction is captured by the SC measure using BERT. Quantifying semantic coherence is especially important given that the coherence of reference summaries will inform the coherence of system summaries, especially for learning-based approaches. Akin to what we discuss for abstractivity, See et al. (2017) and Paulus et al. (2018) both demonstrate that neural summarizers generate incoherent summaries despite achieving high ROUGE scores.", "n_publication_ref": 20, "n_figure_ref": 0}, {"heading": "Pairwise Correlations", "text": "While the properties we evaluate for do not exhaust all aspects of summarization that may be of interest, it is unclear to what extent different measures overlap in judgments. To quantify this, in  we report pairwise correlations for every pair of metrics. In each case, the value reported is the Spearman rank correlation coefficient \u03c1 computed between the length 10 vectors containing the scores for each dataset. 10 \u03c1 = 1 indicates perfect positive correlation (which is why we see this for all diagonal entries) and \u03c1 < 0 indicates the metrics are anti-correlated. Unsurprisingly, the compression metrics are strongly correlated with each other. We further observe that redundancy and topic similarity are correlated whereas abstractivity is anti-correlated with both. In particular, when summaries are considerably redundant, we qualitatively observe that the repeated content in the summary was both important and repeated in the context of the reference document. As a result, this may explain why redundancy and abstractivity are anti-correlated as this would suggest that highly redundant summaries are highly extractive. Additionally, since we measure topic similarity using LDA and unigram count statistics, it is not surprising that extractions may correlate with high topic similarity. In part, this may suggest a deficiency of our measure of topic similarity to accurately consider references to the same topic using substantially different words.\nWe also observe that semantic coherence patterns similarly to redundancy. In particular, while we find the semantic coherence scores are appropriate for most examples we manually inspected, this suggests that BERT relies upon word-level overlaps in making next-sentence judgments (similar to behaviors seen in other sentence-pair tasks such as natural language inference, c.f Gururangan et al., 2018)", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Detecting Low Quality Examples", "text": "To complement our quantitative dataset-level analysis, we conduct a qualitative study of individual examples by examining outliers. For each (dataset, metric) pair, we sample 10 examples from both the top and bottom 10% of examples for that metric and in that dataset.\nSince manually considering all of the 1080 examples was not feasible, we began by examining the sampled examples for topic similarity, redundancy, and semantic coherence. Our hypothesis was that example quality would positively correlate with coherence and topic similarity and negatively correlate with redundancy. We found this hypothesis to be validated by our observations as we found that examples with low coherence, low topic similarity, or high redundancy scores were generally low quality examples. Every example which we judged to be low quality demonstrated at least one of the following defects:\n\u2022 The summary contains critical disfluencies that severely hinder accurate processing. 11\n\u2022 The summary excludes unambiguously critical information from the reference document.\n\u2022 Crucial information in the summary does not appear in the reference document and is not general knowledge.\n\u2022 Substantial fractions of the summary involve entities, relations, or events that are ambiguous and that we could not resolve from the 11 We invoked this condition fairly judiciously as we observed that the domain of summaries also could influence the fluency of summaries in terms of grammaticality. In particular, we unsurprisingly found that academic papers in the Science domain generally have highly grammatical summaries whereas the bullet-point summaries in CNN-DM and the author-written summaries in TL;DR often were ungrammatical but still sufficiently clear to be interpreted correctly. summary alone. In particular, accurate interpretation of the summary would require also reading the reference document to resolve various coreferring expressions; the summary is not self-contained. 12 \u2022 The summary is entirely inappropriate as a summary of the reference document. For example, the summary only discusses an event with no obvious relationship to the contents of the reference document.\n\u2022 The summary includes an entire sentence or long phrase describing something that appears in the main document but that is clearly an auxiliary detail. We flagged examples as low quality due to this condition quite conservatively, only using it when we could come to no basis for why the sentence/phrase should appear in the summary.\nOn the other hand, we did not find any systematic defects in examples with high coherence, high topic similarity, or low redundancy scores. Instead, almost all of these examples were satisfactory.\nFor the remaining two properties (compression measured by CMP w , abstractivity measured by ABS 1 ), we analyzed all of the associated 400 examples. What we observed is that many of these examples tended to be generically low quality and we quantify this in Table 3. Since this analysis may be difficult to replicate and involves subjective decisions about example quality, we comprehensively enumerate all example IDs we use in Table 8.\nTable 4 shows a representative subset of the low quality examples we found in our analysis. We provide further examples in Appendix C and Figures 1-9. Compression. Minimally compressed summaries in NYT, NWS, TL;DR, and PubMed often are supplementary information to the document rather than a summary of it; in some cases, we believe this is due to errors in alignment in dataset construction/release. On the other hand, heavily compressed summaries in NWS and XSum often are just category labels (e.g. Sports), in TL;DR are   usually attention-grabbers, and in NYT are nearexact duplicates of reference documents, which themselves are letters to the editor.\nAbstractivity. Manual inspection reveals highly abstractive summaries in NYT and NWS generally are exceedingly vague or are entirely unrelated to the original document. Highly abstractive summaries in PeerRead are often translated to English from the reference document's language and discuss results that do not appear in the introduction but likely appear later in the paper. Conversely, extremely extractive summaries in NWS and NYT often are just the lede and cannot be understood without the reference document. However, in most other instances, the lede is an effective summary for examples drawn from the News domain.\nWithin the context of our sample of examples, we find that eight of the ten summarization datasets (all but AMI, MovieScript) contain at least 8% low quality examples, the majority contain at least 14% low quality examples, and that these low quality examples can be detected using our compression and abstractivity metrics. For the worst-offending TL;DR dataset, we conservatively estimate at least 20% of examples are of substantially subpar quality. In general, we find that the low quality TL;DR \"summaries\" we detect often serve a different rhetorical purpose than summarization (e.g. attention grabbing, responding to a previous post that is not available in the dataset, sarcasm/humor).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Dataset Analysis. As an alternative to automated evaluation, Chen et al. (2016) and Yatskar (2019) conduct human evaluations of standard datasets in reading comprehension and question answering. In some cases, dataset creators perform manual analyses of the data they introduce (e.g. Sandhaus (2008) and Grusky et al. (2018) for the NYT and Newsroom corpora, respectively). Automated and human evaluation provide complementary benefits with respect to their scalability and reliability. Even in the context of human evaluations, we advocate that automatic metrics can be useful in guiding the exploration of data and informing subsampling procedures that provide fine-grained insights. Quality Estimation. Our work bears resemblance both in name and structure to work on quality estimation. Quality estimation, often centered on natural language generation, is the task of measuring system-generated output quality (Paetzold and Specia, 2016;Yuan and Sharoff, 2020). It is closely related to work on unsupervised or reference-free evaluation (Napoles et al., 2016;Ethayarajh and Sadigh, 2020). Within the context of summarization, the special case of quality estimation regarding factual consistency/faithfulness has been of recent interest (Wang et al., 2020;Maynez et al., 2020;Durmus et al., 2020) since neural abstractive summarizers have been shown to hallucinate/misrepresent facts (See et al., 2017). In comparison to these settings, our metrics make no use of labelled data (even in training) and are entirely intrinsic/unsupervised. Summarization Practices. Several analyses and critiques exist for different aspects of the summarization pipeline. From a modelling perspective, Zhang et al. (2018) assess whether abstractive systems are truly abstractive, Kedzie et al. (2018) evaluate content selection policies in a variety of methods, and Mao et al. (2020) assess the facetlevel performance of extractive summarizers. From an evaluation perspective, several works have discussed the shortcomings of ROUGE/automated evaluation (Liu and Liu, 2008;Chaganty et al., 2018;Hashimoto et al., 2019;Peyrard, 2019b) as well proposed alternative metrics for summarization or natural language generation more broadly (Clark et al., 2019;Zhang et al., 2020;Sellam et al., 2020).\nTwo recent works are highly related to our own. Kryscinski et al. (2019) provide a critical reevaluation of summarization research. Most relevant to our work, they show that web-scraped datasets, specifically CNN-DM and NWS, contain a nontrivial fraction of examples (approx. 3.5%) with HTML artifacts (which can be easily detected/removed). Jung et al. (2019) provide an aspect-level evaluation of both summarization datasets and systems. In their work, the dataset analyses center on biases in the data (e.g. positional biases, which are often seen in news summarization), which is reminiscent of the annotation artifacts seen in other NLP tasks (Gururangan et al., 2018;Niven and Kao, 2019).", "n_publication_ref": 26, "n_figure_ref": 0}, {"heading": "Discussion", "text": "Open Problems and Future Directions. Our results demonstrate that a sizeable fraction of examples in most summarization datasets are low quality. However, it remains open whether modellers should simply prune these examples, manually/automatically attempt to correct them, or model them without change. We do note that research in the machine learning and learning theory communities shows that models both theoretically and empirically do substantially worse when trained using low quality examples, even when the examples are not strictly adversarially chosen (Klivans et al., 2009;Biggio et al., 2012;Koh et al., 2018). These concerns are further compounded by the evidence of Belinkov and Bisk (2018) that neural models for natural language generation are not robust to naturally noisy data.\nOur metrics may be repurposed to rank examples in designing curricula for curriculum learning ap-proaches (Bengio et al., 2009). Alternatively, they can serve as additional metrics for the (possibly unsupervised) evaluation of summarization systems, potentially mitigating deficiencies in standard metrics, such as ROUGE, by directly penalizing redundancy and semantic incoherence.\nLimitations. In this work, we restrict ourselves to single-document single-reference English language summarization datasets. While the datasets we study constitute a considerable fraction of dataset usage in the summarization community, several multi-document summarization datasets have been introduced (e.g. Fabbri et al., 2019;Antognini and Faltings, 2020) and multi-reference summarization datasets have often been argued to be desirable due to under-constrained nature of the summarization task (Kryscinski et al., 2019) and the ideal evaluation paradigm for ROUGE (Lin, 2004). Beyond English, both large summarization datasets (Nguyen and Daum\u00e9 III, 2019;Varab and Schluter, 2020) and more general language resources/technologies (Joshi et al., 2020) are less available, which may heighten the need for data quality assurance.\nMore broadly, the measures that we introduce are automated, and therefore non-human, judgments of the quality of summarization data. Therefore, we only envision these measures to be useful as inexpensive first-order approximations of aspectlevel summary quality rather than bona fide replacements for human evaluation. Additionally, since we principally envision applying these metrics to datasets, we make no efforts to make these metrics robust to adversarially-crafted data and they are likely quite susceptible to adversarial attack.", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this work, we demonstrate that various aspects of summarization datasets can be intrinsically evaluated for. We specifically show this for 5 properties across 10 popular datasets, uncovering that dataset use is sometimes incongruous with the attributes of the underlying data. We also find that some aspectlevel estimators may be surprisingly effective at detecting low quality dataset examples. Our findings suggest that more intentional and deliberate decisions should be made in selecting summarization datasets for downstream modelling research and that further scrutiny should be placed upon summarization datasets released in the future.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Reproducibility", "text": "All code is made publicly available. 13 Exhaustive reproducibility details, including how to access all datasets, are provided in Appendix B. We fully adhere to the EMNLP 2020 Reproducibility guidelines, addressing all relevant checklist items.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A Alternative Metrics", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 Compression", "text": "For compression, we found sentence-level compression to be a naturally motivated metric given that many extractive systems are constrained to extract sentence-length sequence. We also considered byte-level compression as an alternative to word-level compression (as computational length constraints have sometimes been used in evaluation instead of word length constraints). We found the results to be highly correlated with word-level compression and to not be further revealing (and bytes may be inherently less interpretable for NLP when compared with words). We also considered only considering content words, motivated by literature in topic modelling (Schofield et al., 2017) that has considered removing stopwords and other such lexical categories. These results were also highly correlated with the original word-level compression results and we did not find any discerning trends in looking at individual examples.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A.2 Topic Similarity", "text": "In the main paper, we compute topic similarity using the Jensen-Shannon distance. We initially considered the Kullback-Leibler (KL) divergence. While the JS distance and/or divergence has been more frequently used in the context of similarity in topic modelling, the KL divergence is also frequently considered. Intuitively and under some interpretations, the asymmetry of the KL divergence may be desirable as the extent to which a summary is topically similar to a document may not be the same as the extent to which a document is topically similar to a summary. In spite of this, in viewing the results using KL, we found that the measure lacked discriminative power in disambiguating examples we believed were more topically similar than others. We qualitatively found the judgments via the JS distance to be accurate. That said, the judgments between the measures tended to be highly correlated as the Spearman rank correlation coefficient was \u03c1 \u2265 0.7 for all topic modelling settings and in most cases exceeded 0.8.\nWe also considered a topic model learned using both the documents and summaries D \u222a S and just the documents D. Both are natural choices, with using the documents being more general in some sense as the topic similarity of a summary should be able to be assigned without requiring the summary collection. We further considered several choices for the number of topics as well. In Table 5, we report the full results for all pairs of (training corpus T , # topics k) for all (T , k) \u2208 {D \u222a S, D} \u00d7 {10, 20, 50, 100}. In all cases, the number of training examples is truncated to 20000 (hence 10000 summaries and 10000 documents when using the training corpus of D \u222a S). We fix the number of training documents across datasets to attempt to control for the confound of larger datasets inducing higher quality topic models. We did not observe significant changes in the result by relaxing this (i.e. using the full datasets instead of just 20000 examples).\nWe find that there is significant variation in crossdataset rankings with respect to these two parameters. We chose to report the results corresponding to k = 20, T = D. We chose the value for k based on qualitative judgments about topic quality for CNN-DM, PeerRead, and AMI, as we considered these to be a diverse subset of all 10 datasets. The topics we observed were highly disjoint and reasonably aligned with our intuitions about what sensible topics should be. We chose the value for T based on the generality referenced previously. While the results are substantially different for D versus D \u222a S, we did not find any consistent and interpretable discriminative properties between the two.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.3 Abstractivity", "text": "Our general framework for quantifying abstractivity is derived from Grusky et al. (2018). We considered p \u2208 {1, 2, 3, 4} initially and found p = 1 to be the most informative regarding abstractivity. In particular, we find that for increasing p, useful conclusions about abstractivity are inherently masked by the dominance of the |S i | p denominator in the definition. We report the scores for ABS 2 in Table 6.\nWe also considered the natural extensions to ABS 3 and ABS 4 but we found that the normalization dominates any deviation in the scores and all datasets essentially receive a score of 1. We also considered other forms of normalization (i.e. normalizing ABS 2 in the style of the L 2 norm/the style of generalized p-norms) in initial experiments but found no substantial differences.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A.4 Redundancy", "text": "In the main paper, we compute redundancy scores for each distinct sentence pair using ROUGE-L Fmeasure and then average these individual values to get a score for the entire summary. Alternatively, we considered other ROUGE scores (specifically ROUGE-1 and ROUGE-2) as well as max pooling the sentence pair scores. We report these results below in Table 7.\nWe do not observe significant changes with the specific ROUGE metric considered (i.e. a Spearman \u03c1 of 1.0 which indicates a perfect correlation in the case of max pooling across the ROUGE variants). We do see substantial differences between averaging and max pooling; we find that max pooling turns out to precisely correlate (\u03c1 = 1.0) with the average summary length. This is somewhat expected, given that the max-pooled redundancy estimates doesn't inherently control for summary length. We therefore chose to report redundancy scores using averaging as we also qualitatively found them to be more useful and characteristic, especially for datasets such as AMI and the Scientific datasets as max pooling was overly aggressive. While the nuances of the specific ROUGE variant did not significantly impact trends in redundancy scores, we chose to report the ROUGE-L scores in the main paper as we (highly subjectively) found the values to be most interpretable/consistent with values we would have assigned.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.5 Semantic Coherence", "text": "We evaluate for semantic coherence between successive pairs of sentences, exploiting the auxiliary training objective of BERT beyond its masked language modeling objective. In particular, we were especially interested in this given that many systems are designed with explicit handling of sentence boundaries (e.g. more extractive systems first rank extractive sentences and then order a thresholded subset) and datasets such as CNN-DM, which are artificially concatenated, may not be inherently coherent across sentence-boundaries.   Our observations regarding the measure of coherence provided by BERT's next-sentence predictions seem to contradict existing findings. In particular,  introduce RoBERTa as a direct followup study to BERT and find that the next-sentence prediction objective is not an effective pretraining objective for improving representations for natural language understanding; Yang et al. (2019) also provide similar evidence. However, our findings do not contest these conclusions but instead suggest that, nonetheless, BERT is a strong next-sentence predictor and that these predictions are still useful for measuring coherence across sentences. While we considered word or subword measures of coherence, we did not consider alternative pretrained models that are pretrained on other objectives related to inter-sentence coherence such as ALBERT (Lan et al., 2020). Given the findings of Lan et al. (2020, \u00a74.6), it seems likely that the sentence order prediction task they use may be more effective for measuring semantic coherence. Concurrent work by Prabhumoye et al. (2020) also substantiates the usefulness of BERT-based nextsentence prediction for measuring coherence and ranking sentences orders.\nThat said, semantic coherence could also be evaluated using (neural) language models, especially in light of results suggest they may be consistent with human judgments regarding grammaticality and acceptability (Chowdhury and Zamparelli, 2018;Warstadt et al., 2019). We did consider this and found language modeling scores (e.g. surprisal) assigned via a pretrained high-quality causal lan-guage model (GPT-2) to be inconsistent with our human judgments. We believe language modeling scores in this sense are likely highly sensitive to the domain (and even within-domain effects, e.g. lexical variation for XSum which is fairly limited given all articles are sourced from the BBC whereas for Newsroom the variation is greater given the heterogeneous group of publishers with more diversified writing styles).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "B Reproducibility Details", "text": "We provide precise and comprehensive details discussing all data, preprocessing and modelling decisions. All code will be made publicly available as noted in the main paper.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B.1 Dataset Sources", "text": "We use the versions of GW and CNN-DM dataset released by Gehrmann et al. (2018). 14 Sentence boundary tokens inserted by Gehrmann et al. (2018) to improve summarization quality were removed to ensure fair comparison in our work. An important distinction in the use of the CNN-DM dataset for modeling is whether the entity-anonymized or non-anonymized version was used. This copy is non-anonymized and it is important to consider the stability of our metrics under this anonymization. We used the released version of the NYT dataset directly as it was released via LDC.  We use the released version of the TL;DR dataset provided by the authors of V\u00f6lske et al. (2017). 16 We use a version of the NWS dataset that was released via private communication with the authors of Grusky et al. (2018). We have verified with the authors that the data can be requested with the platform they released in their original work. 17 For all remaining datasets, we use the version released by Jung et al. (2019). 18 All of our conventions in using these five datasets follow their work.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "B.2 Data Preprocessing", "text": "All datasets were first filtered to remove examples where either the document or summary was empty. We found only examples in CNN-DM failed this criterion and this constituted less than 0.1% 114 287227 of the dataset. All results were reported then on the standard training set if we were aware of a standard split used consistently in the summarization system literature. Splits in the case of datasets sourced from the work of Jung et al. (2019) followed their work. In all cases, the training set was at least 80% of the full data collection, so we expect results to generalize to the portions of the collection that were not considered assuming splits were constructed by sampling uniformly at random (we did not verify this).\nSentence-level tokenization was performed using NLTK (Loper and Bird, 2002). Word-level tokenization was performed using SpaCy (Honnibal and Montani, 2017).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "B.3 Topic Similarity", "text": "We lowercase all terms, remove stopwords using the list specified in NLTK (Loper and Bird, 2002), and lemmatize using SpaCy (Honnibal and Montani, 2017). We only retain words tagged with a POS category in {NOUN, ADJ, VERB, ADV} by the SpaCy POS tagger. We use LDA (Blei et al., 2003) to learn all topic models and rely on the implementation in Gensim (\u0158eh\u016f\u0159ek and Sojka, 2010) based on specification of Hoffman et al. (2010). All hyperparameters are set as default and we discussed the number of topics k and training corpus T in \u00a7A.2 with the results in the main paper using k = 20 and T = D where T is truncated to be at most 20000 documents. We compute the Jensen-Shannon distance using SciPy (Virtanen et al., 2020).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "B.4 Abstractivity", "text": "Fragments (Grusky et al., 2018) were computed using the scripts released in that work for the purposes of estimating abstractivity. In the case of the NWS dataset, the authors already provide fragment-related scores which we use without recomputing these values.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "B.5 Redundancy", "text": "We make use of the native Python reimplementation of ROUGE (Lin, 2004), easy-rouge. 19 All scores reported in the main paper use ROUGE-L and use the computed F -measure score.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "B.6 Semantic Coherence", "text": "We compute semantic coherence by predicting the probability of a sentence conditional on the preceding sentence using BERT. BERT was pretrained with exactly this objective (beyond its masked language modeling objective) and we use the released model as-is with no further fine-tuning. We use the bert-base-uncased model along with the associated tokenizer that was implemented in PyTorch (Paszke et al., 2017) by HuggingFace in the transformers repository. 20", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "B.7 Efficiency", "text": "All metrics reported in the main paper can be computed over all datasets in less than 10 ten hours on a single CPU. The only model with a nontrivial number of parameters used in this work is the bert-base-uncased models we use in measuring semantic coherence. We refer readers to Devlin et al. (2019) for more details and to the HuggingFace implementation we reference previously.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "C Detecting Low-Quality Examples", "text": "In the main paper, we briefly discuss how we discovered that several of our metrics can serve the dual purpose of detecting generally low quality examples for example that achieve extreme scores. Figures 1 through 9 are several examples we found to be representative of the general structure of low quality examples for a given metric. In some cases, the trends are highly dataset-specific whereas in others they are more general. To facilitate reproducibility efforts, we provide all examples IDs we studied for each (dataset, metric) in Table 8.\nOriginal Text (truncated): Let us, in the beginning, give a word of cordial praise to the American publishers of these splendid volumes. The undertaking, in the first place, was an intellectual compliment to the country. It was based on the faith that there is in this country enough of philosophy and scholarship to justify a new and complete edition of . . . Summary: Let us, in the beginning, give a word of cordial praise to the American publishers of these splendid volumes. The undertaking, in the first place, was an intellectual compliment to the country.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Detector: Extremely Low Abstraction", "text": "Figure 1: Dataset: NWS. This summary simply is the lede and we do not find it to be a useful summary for readers not familiar with the full context of the article. We hypothesize that such a summary may have been useful for members of a newsroom communicating information about the article to the other (given their intimate familiarity with the article) but this likely is inappropriate as a summary in most settings.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "D Mutual Information Bounds", "text": "The entropy of a random variable X is defined as:\nH(X) \u2212 x p(x) log 2 p(x)\nOriginal Text (truncated): A FULL-SERVICE hotel and conference center is to go up in the Lafayette Yard area of Trenton, giving the city a hotel for the first time since the 1980's and bringing to an end its unenviable distinction as the only state capital without lodging for visitors . . .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Summary: Acquest", "text": "Detector: Extremely Low Abstraction Figure 2: Dataset: NYT. This summary simply conveys no useful information to someone who has not also read the reference document and simply is a word copied from the source document. It appears to be a label rather than a summary.\nOriginal Text (truncated): a l\u00f3gica\u00e9 o estudo dos princ\u00edpios e crit\u00e9iros de infer\u00eancias e demonstra\u00e7\u00f5es v\u00e1lidas. um sistema l\u00f3gico\u00e9 composto por tr\u00eas partes: a sintaxe (ou nota\u00e7\u00e3o), . . .", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Summary (truncated)", "text": ": logic is the science of correct inferences and a logical system is a tool to prove assertions in a certain logic in a correct way . . .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Detector: Extremely High Abstraction", "text": "Figure 3: Dataset: PeerRead. This summary simply is not in the same language and hence achieves a very high abstractivity.\nOriginal Text (truncated): from russia with love\"screenplay byrichard maibaumadapted byjohanna harwoodbased on the novel byian fleming . . .", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Summary: final", "text": "Detector: Extremely High Abstraction  NYT. This summary is unlikely to be informative to someone who has not read the reference document and is more of a categorization/label than a summary. This is similar to the previous NYT example given.\nThe conditional entropy of X given Y is defined as:  Original Text (truncated): Brodie (the dog) was neglected, and ended up with serious anger and health issues concerning his skin and allergies. My boyfriend adopted him . . . Summary: Onions.\nH(X | Y ) y p(", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Detector: Extremely High Compression", "text": "Figure 8: Dataset: TL;DR. We observe this trend quite frequently in TL;DR. Specifically, since authors on the social discussion platform Reddit choose to provide these summaries at their discretion, we often find the \"summaries\" are attention-grabbing and serve a starkly different rhetorical purpose from how summaries are generally conceived.\nThe mutual information between random variables X and Y is defined as: I(X; Y ) H(X) \u2212 H(X | Y ) The entropy measures the uncertainty in the probability mass/density function of a random variable. As such, the mutual information measures how Original Text (truncated): these are external links and will open in a new window1908 -king carlos and eldest son assassinated in lisbon. second son manuel becomes king. 1910 -king manuel ii abdicates amid revolution . . .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Summary: a chronology of key events :", "text": "Detector: Extremely High Compression Figure 9: Dataset: XSum. We observe this trend quite frequently in XSum. For articles that are essentially timelines or other types of chronologies discussing historic events diachronically (which forms a small but distinctive section of the writing style of BBC from our analysis), the summary extracted to accompany it is generally this string or a slightly altered version. We argue this summary is fairly unhelpful (and is likely fairly uninteresting to test models on; simple rule-based filtering made be preferable to avoid overestimating performance on this dataset because of these examples). much the entropy of X is reduced by (on average) due to the observation of Y . Intuitively, the claim is that the uncertainty about the summarization task that is reduced by the model (which is uniquely determined by its training data, pretraining data, and architecture) is at most what can be cumulatively reduced by the training data, pretraining data, and inductive biases encoded in the model's architecture.\nOur hypothesis is that I(S; A) is small for learning-based models with minimal inductive biases, such as neural networks. Further, we hypothesize that while I(S; P ) is likely nontrivial for popular pretraining regimes, the dominant term on the right-hand side is likely I(S; T ). We do note that this second hypothesis may be false given the partial evidence of GPT-3 (Brown et al., 2020) and the successes it enjoys in few-shot learning due to pretraining at unprecedented scale. However, no evaluation is conducted on summarization data in that work.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Acknowledgments", "text": "We thank Anna Huang for her help with analyzing the data. We thank Ge Gao, Esin Durmus, and members of the Cornell and Stanford NLP groups for their valuable advice. We especially thank the reviewers and area chairs for their articulate and constructive feedback.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "E Additional Statistics", "text": "In the main paper, we report the average score for each metric on each dataset. To complement reporting the mean, we report the standard deviation for each metric on each dataset in Table 9.  ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "GameWik-iSum: a novel large multi-document summarization dataset", "journal": "", "year": "2020", "authors": "Diego Antognini; Boi Faltings"}, {"title": "Synthetic and natural noise both break neural machine translation", "journal": "", "year": "2018", "authors": "Yonatan Belinkov; Yonatan Bisk"}, {"title": "Data statements for natural language processing: Toward mitigating system bias and enabling better science", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Emily M Bender; Batya Friedman"}, {"title": "Curriculum learning", "journal": "", "year": "2009", "authors": "Yoshua Bengio; J\u00e9r\u00f4me Louradour; Ronan Collobert; Jason Weston"}, {"title": "Poisoning attacks against support vector machines", "journal": "Omnipress", "year": "2012", "authors": "Battista Biggio; Blaine Nelson; Pavel Laskov"}, {"title": "Latent dirichlet allocation", "journal": "J. Mach. Learn. Res", "year": "2003", "authors": "David M Blei; Andrew Y Ng; Michael I Jordan"}, {"title": "A large annotated corpus for learning natural language inference", "journal": "", "year": "2015", "authors": "R Samuel; Gabor Bowman; Christopher Angeli; Christopher D Potts;  Manning"}, {"title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": "Tom B Brown; Nick Benjamin Pickman Mann; Melanie Ryder; Jean Subbiah; Prafulla Kaplan; Arvind Dhariwal; Pranav Neelakantan; Girish Shyam; Amanda Sastry; Sandhini Askell; Ariel Agarwal; G Herbert-Voss; Tom Kr\u00fcger; Rewon Henighan; Aditya Child; Daniel M Ramesh; Jeffrey Ziegler; Clemens Wu; Christopher Winter; Mark Hesse; Eric J Chen; Mateusz Sigler; Scott Litwin;  Gray"}, {"title": "Association for Computational Linguistics", "journal": "", "year": "2015", "authors": "Chris Callison; - Burch; Lyle Ungar; Ellie Pavlick"}, {"title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries", "journal": "ACM", "year": "1998", "authors": "Jaime Carbonell; Jade Goldstein"}, {"title": "The ami meeting corpus: A pre-announcement", "journal": "Springer", "year": "2005", "authors": "Jean Carletta; Simone Ashby; Sebastien Bourban; Mike Flynn; Mael Guillemot; Thomas Hain; Jaroslav Kadlec; Vasilis Karaiskos; Wessel Kraaij; Melissa Kronenthal"}, {"title": "The price of debiasing automatic metrics in natural language evalaution", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Arun Chaganty; Stephen Mussmann; Percy Liang"}, {"title": "A thorough examination of the CNN/daily mail reading comprehension task", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Danqi Chen; Jason Bolton; Christopher D Manning"}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Sumit Chopra; Michael Auli; Alexander M Rush"}, {"title": "RNN simulations of grammaticality judgments on long-distance dependencies", "journal": "", "year": "2018", "authors": "Absar Shammur; Roberto Chowdhury;  Zamparelli"}, {"title": "Sentence mover's similarity: Automatic evaluation for multi-sentence texts", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Elizabeth Clark; Asli Celikyilmaz; Noah A Smith"}, {"title": "A discourse-aware attention model for abstractive summarization of long documents", "journal": "", "year": "2018", "authors": "Arman Cohan; Franck Dernoncourt; Soon Doo; Trung Kim; Seokhwan Bui; Walter Kim; Nazli Chang;  Goharian"}, {"title": "Elements of Information Theory", "journal": "Wiley-Interscience", "year": "2006", "authors": "M Thomas; Joy A Cover;  Thomas"}, {"title": "A repository of corpora for summarization", "journal": "", "year": "2018", "authors": "Franck Dernoncourt; Mohammad Ghassemi; Walter Chang"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Lecture notes for statistics 311/electrical engineering 377", "journal": "", "year": "2019", "authors": "John C Duchi"}, {"title": "Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization", "journal": "", "year": "2020", "authors": "Esin Durmus; He He; Mona Diab"}, {"title": "New methods in automatic extracting", "journal": "J. ACM", "year": "1969", "authors": "H P Edmundson"}, {"title": "Bleu neighbors: A reference-less approach to automatic evaluation. ArXiv, abs", "journal": "", "year": "2004", "authors": "Kawin Ethayarajh; Dorsa Sadigh"}, {"title": "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Alexander Fabbri; Irene Li; Tianwei She; Suyi Li; Dragomir Radev"}, {"title": "Datasheets for datasets", "journal": "ArXiv", "year": "2018", "authors": "Timnit Gebru; Jamie Morgenstern; Briana Vecchione; Jennifer Wortman Vaughan; Hanna M Wallach; Hal Daum\u00e9; Kate Crawford"}, {"title": "Bottom-up abstractive summarization", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Sebastian Gehrmann; Yuntian Deng; Alexander Rush"}, {"title": "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets", "journal": "", "year": "2019", "authors": "Mor Geva; Yoav Goldberg; Jonathan Berant"}, {"title": "Movie script summarization as graph-based scene extraction", "journal": "", "year": "2015", "authors": "Philip John Gorinski; Mirella Lapata"}, {"title": "", "journal": "English Gigaword. Philadelphia, Pennsylvania. Linguistic Data Consortium", "year": "2003", "authors": "David Graff; Christopher Cieri"}, {"title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies", "journal": "", "year": "2018", "authors": "Max Grusky; Mor Naaman; Yoav Artzi"}, {"title": "Annotation artifacts in natural language inference data", "journal": "", "year": "2018", "authors": "Swabha Suchin Gururangan; Omer Swayamdipta; Roy Levy; Samuel Schwartz; Noah A Bowman;  Smith"}, {"title": "Unifying human and statistical evaluation for natural language generation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Tatsunori Hashimoto; Hugh Zhang; Percy Liang"}, {"title": "Teaching machines to read and comprehend", "journal": "Curran Associates, Inc", "year": "2015", "authors": "Karl Moritz Hermann; Tomas Kocisky; Edward Grefenstette; Lasse Espeholt; Will Kay; Mustafa Suleyman; Phil Blunsom"}, {"title": "Online learning for latent dirichlet allocation", "journal": "Curran Associates Inc", "year": "2010", "authors": "Matthew D Hoffman; David M Blei; Francis Bach"}, {"title": "Improving the estimation of word importance for news multidocument summarization", "journal": "", "year": "2014", "authors": "Kai Hong; Ani Nenkova"}, {"title": "2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing", "journal": "", "year": "", "authors": "Matthew Honnibal; Ines Montani"}, {"title": "The state and fate of linguistic diversity and inclusion in the nlp world", "journal": "", "year": "2020", "authors": "Pratik Joshi; Sebastin Santy; Amar Budhiraja; Kalika Bali; Monojit Choudhury"}, {"title": "Earlier isn't always better: Subaspect analysis on corpus and system biases in summarization", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Taehee Jung; Dongyeop Kang; Lucas Mentch; Eduard Hovy"}, {"title": "Visual question answering: Datasets, algorithms, and future challenges. Computer Vision and Image Understanding", "journal": "", "year": "2017", "authors": "Kushal Kafle; Christopher Kanan"}, {"title": "A dataset of peer reviews (PeerRead): Collection, insights and NLP applications", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Dongyeop Kang; Waleed Ammar; Bhavana Dalvi; Madeleine Van Zuylen; Sebastian Kohlmeier; Eduard Hovy; Roy Schwartz"}, {"title": "How much reading does reading comprehension require? a critical investigation of popular benchmarks", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Divyansh Kaushik; Zachary C Lipton"}, {"title": "Content selection in deep learning models of summarization", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Chris Kedzie; Kathleen Mckeown; Hal Daum\u00e9; Iii "}, {"title": "Learning halfspaces with malicious noise", "journal": "Journal of Machine Learning Research", "year": "2009", "authors": "Adam R Klivans; Philip M Long; Rocco A Servedio"}, {"title": "Stronger data poisoning attacks break data sanitization defenses", "journal": "ArXiv", "year": "2018", "authors": "Pang Wei Koh; Jacob Steinhardt; Percy Liang"}, {"title": "Neural text summarization: A critical evaluation", "journal": "", "year": "2019", "authors": "Wojciech Kryscinski; Nitish Shirish Keskar; Bryan Mc-Cann; Caiming Xiong; Richard Socher"}, {"title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "journal": "", "year": "2020", "authors": "Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut"}, {"title": "The role of discourse units in near-extractive summarization", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Jessy Junyi; Kapil Li; Amanda Thadani;  Stent"}, {"title": "Improving neural abstractive document summarization with structural regularization", "journal": "", "year": "2018", "authors": "Wei Li; Xinyan Xiao; Yajuan Lyu; Yuanzhuo Wang"}, {"title": "ROUGE: A package for automatic evaluation of summaries", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"title": "Correlation between ROUGE and human evaluation of extractive meeting summaries", "journal": "", "year": "2008", "authors": "Feifan Liu; Yang Liu"}, {"title": "RoBERTa: A robustly optimized BERT pretraining approach", "journal": "", "year": "1907", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Nltk: The natural language toolkit", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Edward Loper; Steven Bird"}, {"title": "The automatic creation of literature abstracts", "journal": "IBM Journal of Research and Development", "year": "1958", "authors": "H P Luhn"}, {"title": "Advances in Automatic Text Summarization", "journal": "MIT Press", "year": "1999", "authors": "Inderjeet Mani"}, {"title": "Facet-aware evaluation for extractive summarization", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Yuning Mao; Liyuan Liu; Qi Zhu; Xiang Ren; Jiawei Han"}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "journal": "Computational Linguistics", "year": "1993", "authors": "Mitchell P Marcus; Beatrice Santorini; Mary Ann Marcinkiewicz"}, {"title": "On faithfulness and factuality in abstractive summarization", "journal": "", "year": "2020", "authors": "Joshua Maynez; Shashi Narayan; Bernd Bohnet; Ryan Mcdonald"}, {"title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "journal": "", "year": "2019", "authors": "Tom Mccoy; Ellie Pavlick; Tal Linzen"}, {"title": "Rewriting the Past: Assessing the Field through the Lens of Language Generation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Kathleen Mckeown"}, {"title": "Model cards for model reporting", "journal": "ACM", "year": "2019", "authors": "Margaret Mitchell; Simone Wu; Andrew Zaldivar; Parker Barnes; Lucy Vasserman; Ben Hutchinson; Elena Spitzer; Deborah Inioluwa; Timnit Raji;  Gebru"}, {"title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond", "journal": "", "year": "2016", "authors": "Ramesh Nallapati; Bowen Zhou;  Cicero Dos Santos; Bing Aglar Gul\u00e7ehre;  Xiang"}, {"title": "SIGNLL Conference on Computational Natural Language Learning", "journal": "Association for Computational Linguistics", "year": "", "authors": ""}, {"title": "There's no comparison: Referenceless evaluation metrics in grammatical error correction", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Courtney Napoles; Keisuke Sakaguchi; Joel Tetreault"}, {"title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Shashi Narayan; Shay B Cohen; Mirella Lapata"}, {"title": "Understanding the Process of Multi-document Summarization: Content Selection, Rewriting and Evaluation", "journal": "", "year": "2006", "authors": ""}, {"title": "A survey of text summarization techniques", "journal": "Springer", "year": "2012", "authors": "Ani Nenkova; Kathleen Mckeown"}, {"title": "Global voices: Crossing borders in automatic news summarization", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Khanh Nguyen; Hal Daum\u00e9; Iii "}, {"title": "Probing neural network comprehension of natural language arguments", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Timothy Niven; Hung-Yu Kao"}, {"title": "SimpleNets: Quality estimation with resource-light neural networks", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Gustavo Paetzold; Lucia Specia"}, {"title": "Automatic differentiation in PyTorch", "journal": "", "year": "2017", "authors": "Adam Paszke; Sam Gross; Soumith Chintala; Gregory Chanan; Edward Yang; Zachary Devito; Zeming Lin; Alban Desmaison; Luca Antiga; Adam Lerer"}, {"title": "A deep reinforced model for abstractive summarization", "journal": "", "year": "2018", "authors": "Romain Paulus; Caiming Xiong; Richard Socher"}, {"title": "A simple theoretical model of importance for summarization", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Maxime Peyrard"}, {"title": "Studying summarization evaluation metrics in the appropriate scoring range", "journal": "", "year": "2019", "authors": "Maxime Peyrard"}, {"title": "Hypothesis only baselines in natural language inference", "journal": "", "year": "2018", "authors": "Adam Poliak; Jason Naradowsky; Aparajita Haldar; Rachel Rudinger; Benjamin Van Durme"}, {"title": "Topological sort for sentence ordering", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Ruslan Shrimai Prabhumoye; Alan W Salakhutdinov;  Black"}, {"title": "Introduction to the special issue on summarization", "journal": "Computational Linguistics", "year": "2002", "authors": "R Dragomir; Eduard Radev; Kathleen Hovy;  Mckeown"}, {"title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "ArXiv", "year": "2019", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"title": "SQuAD: 100,000+ questions for machine comprehension of text", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"title": "Software Framework for Topic Modelling with Large Corpora", "journal": "Valletta, Malta. ELRA", "year": "2010", "authors": "Petr Radim\u0159eh\u016f\u0159ek;  Sojka"}, {"title": "A neural attention model for abstractive sentence summarization", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Alexander M Rush; Sumit Chopra; Jason Weston"}, {"title": "The New York Times Annotated Corpus", "journal": "Linguistic Data Consortium", "year": "2008", "authors": "Evan Sandhaus"}, {"title": "Pulling out the stops: Rethinking stopword removal for topic models", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Alexandra Schofield; M\u00e5ns Magnusson; David Mimno"}, {"title": "Get to the point: Summarization with pointergenerator networks", "journal": "Long Papers", "year": "2017", "authors": "Abigail See; J Peter; Christopher D Liu;  Manning"}, {"title": "BLEURT: Learning robust metrics for text generation", "journal": "", "year": "2020", "authors": "Thibault Sellam; Dipanjan Das; Ankur Parikh"}, {"title": "Automatic summarizing: factors and directions", "journal": "", "year": "1999", "authors": "Karen Sp\u00e4rck Jones"}, {"title": "A corpus of natural language for visual reasoning", "journal": "Short Papers", "year": "2017", "authors": "Alane Suhr; Mike Lewis; James Yeh; Yoav Artzi"}, {"title": "Assessing social and intersectional biases in contextualized word representations", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Yi Chern Tan; L Elisa Celis"}, {"title": "DaNewsroom: A large-scale Danish summarisation dataset", "journal": "", "year": "2020", "authors": "Daniel Varab; Natalie Schluter"}, {"title": "", "journal": "Nature Methods", "year": "", "authors": "Pauli Virtanen; Ralf Gommers; Travis E Oliphant; Matt Haberland; Tyler Reddy; David Cournapeau; Evgeni Burovski; Pearu Peterson; Warren Weckesser; Jonathan Bright; J St\u00e9fan; Matthew Van Der Walt; Joshua Brett; K Jarrod Wilson; Nikolay Millman;  Mayorov; R J Andrew; Eric Nelson; Robert Jones; Eric Kern;  Larson; \u0130lhan Carey; Yu Polat; Eric W Feng; Jake Moore; Denis Vand Erplas; Josef Laxalde; Robert Perktold; Ian Cimrman; E A Henriksen;  Quintero; R Charles; Anne M Harris; Ant\u00f4nio H Archibald; Fabian Ribeiro;  Pedregosa"}, {"title": "TL;DR: Mining Reddit to learn automatic summarization", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Michael V\u00f6lske; Martin Potthast; Shahbaz Syed; Benno Stein"}, {"title": "Asking and answering questions to evaluate the factual consistency of summaries", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Alex Wang; Kyunghyun Cho; Mike Lewis"}, {"title": "Neural network acceptability judgments", "journal": "Transactions of the Association for Computational Linguistics", "year": "2019", "authors": "Alex Warstadt; Amanpreet Singh; Samuel R "}, {"title": "Are two heads better than one? crowdsourced translation via a two-step collaboration of non-professional translators and editors", "journal": "", "year": "2014", "authors": "Rui Yan; Mingkun Gao; Ellie Pavlick; Chris Callison-Burch"}, {"title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; Ruslan Salakhutdinov; Quoc V Le"}, {"title": "A qualitative comparison of CoQA, SQuAD 2.0 and QuAC", "journal": "Long and Short Papers", "year": "2019", "authors": "Mark Yatskar"}, {"title": "Sentence level human translation quality estimation with attentionbased neural networks", "journal": "", "year": "2020", "authors": "Yu Yuan; Serge Sharoff"}, {"title": "Crowdsourcing translation: Professional quality from non-professionals", "journal": "", "year": "2011", "authors": "Omar F Zaidan; Chris Callison-Burch"}, {"title": "On the abstractiveness of neural document summarization", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Fangfang Zhang; Jin-Ge Yao; Rui Yan"}, {"title": "Yin and yang: Balancing and answering binary visual questions", "journal": "", "year": "2016", "authors": "Peng Zhang; Yash Goyal; Douglas Summers-Stay; Dhruv Batra; Devi Parikh"}, {"title": "BERTScore: Evaluating Text Generation with BERT", "journal": "", "year": "2020", "authors": "Tianyi Zhang; Varsha Kishore; Felix Wu; Kilian Q Weinberger; Yoav Artzi"}, {"title": "Gender bias in contextualized word embeddings", "journal": "Long and Short Papers", "year": "2019", "authors": "Jieyu Zhao; Tianlu Wang; Mark Yatskar; Ryan Cotterell; Vicente Ordonez; Kai-Wei Chang"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "denotes the mutual information, S denotes understanding of the underlying summarization task and M denotes a model learned using summarization training data T , additional pre-training data P , and the model's architecture A.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 5 :5Figure5: Dataset: NYT. This summary is unlikely to be informative to someone who has not read the reference document and is more of a categorization/label than a summary. This is similar to the previous NYT example given.", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 6 :6Figure6: Dataset: NWS. This summary has a negative compression score and, in this case, this seems to indicate the summaries and documents were extracted inaccurately using the scraper ofGrusky et al. (2018).", "figure_data": ""}, {"figure_label": "7", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 7 :7Figure7: Dataset: NYT. Similar to the previous example, this summary has a negative compression score and, in this case, this seems to indicate the summaries and documents were created/aligned incorrectly inSandhaus (2008).", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": "In the main paper, we state the following inequality: the mutual information, S denotes understanding of the underlying summarization task and M denotes a model learned using summarization training data T , additional pretraining data P , and the model's architecture A.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "CMPw CMPs TS ABS1 REDSCCMPw10.733 -0.188 -0.406 -0.179 -0.321CMPs 0.73310.042 -0.297 0.036 0.0TS-0.188 0.0421-0.564 0.75 0.643ABS1 -0.406 -0.297 -0.5641-0.429 -0.214RED -0.179 0.036 0.75 -0.42910.321SC-0.3210.00.643 -0.214 0.3211"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Pairwise correlations measured using Spearman \u03c1 coefficient between metrics studied in this work.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Upper half: Percent of examples sampled from the top (\u2191) 10% for the given metric that were low quality. Lower half: Percent of examples sampled from the bottom (\u2193) 10% for the given metric that were low quality. Brodie (the dog) was neglected . . . health issues concerning his skin. . . . Onions PeerRead ABS1 \u2191 a l\u00f3gica\u00e9 o estudo dos princ\u00edpios e crit\u00e9iros de infer\u00eancia . . . logic is the science of correct inferences . . . Let us, in the beginning, give a word of cordial praise to the . . . Let us, in the beginning, give a word of cordial praise to the . . .", "figure_data": "DatasetMetricDocumentSummaryTL;DR CMPw \u2191 NWS CMPw \u2193c Telegraph Media Group Limited 2016David Moyes has returned to former club Manchester United . . .TL;DRABS1 \u2193"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Representative low quality examples in the given dataset from the top (\u2191) or bottom (\u2193) 10% of examples for the given metric. Due to space constraints, some examples are abridged and shorter examples were preferred in selecting representatives. Additional examples are provided in Appendix C and Figures 1-9.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Alternative methods for estimating redundancy. Results in main paper are equivalent to those in the row corresponding to 20 and D.", "figure_data": "NewsScientificSocial Media MeetingScriptCNN-DM NYT NWS GW XSum PeerRead PubMedTL;DRAMIMovieScriptABS10.1350.249 0.191 0.334 0.3460.2010.1220.3840.1840.147ABS20.9320.917 0.762 0.862 0.9530.9430.9830.9320.9950.983"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Alternative methods for estimating abstractivity. Results in the main paper are for ABS 1 .", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Alternative methods for estimating redundancy. Results in main paper are equivalent to those in the row corresponding to mean and ROUGE-L.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Figure 4: Dataset: MovieScript. This summary simply bears no clear relationship with the reference document and therefore repeats no words and achieves maximal abstractivity.", "figure_data": "Original Text: BASEBALL American League BALTIMORE ORIOLES -Agreed to terms with INF-OF Mark McLemore on a minor league contract.BOSTON RED SOX -Named Dale Sveum third base coach.Summary: Sports transactionsDetector: Extremely High Abstraction"}], "doi": "10.1162/tacl_a_00041"}