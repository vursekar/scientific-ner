{"authors": "Jing Li; Shangping Zhong; Kaizhi Chen", "pub_date": "", "title": "MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset", "abstract": "Question Answering (QA) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Traditional Chinese Medicine Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40% to 55% on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems. 1   ", "sections": [{"heading": "Introduction", "text": "As a branch of the QA task, Biomedical Question Answering (BQA) enables effectively perceiving, accessing, and understanding complex biomedical knowledge by innovative applications, which makes BQA an important QA application in the biomedical domain (Jin et al., 2021). Such a task has recently attracted considerable attention from the NLP community (Zweigenbaum, 2003;He et al., 2020b;Jin et al., 2020), but is still confronted with the following three key challenges:\n1 https://github.com/Judenpech/MLEC-QA\n(1) Most work attempt to build BQA systems with deep learning and neural network techniques (Ben Abacha et al., 2017, 2019bPampari et al., 2018) and are thus data-hungry. However, annotating large-scale biomedical question-answer pairs with high quality is prohibitively expensive. As a result, current expert-annotated BQA datasets are small in size. (2) Multi-choice QA is a typical format type of BQA dataset. Most previous work focus on such format type of datasets in which contents are in the field of clinical medicine (Zhang et al., 2018b;Jin et al., 2020) and consumer health (Zhang et al., 2017(Zhang et al., , 2018aHe et al., 2019;Tian et al., 2019). However, there are many other specialized sub-fields in biomedicine that have not been studied before (e.g., Stomatology).\n(3) Ideal BQA systems should not only focus on raw text data, but also fully utilize various types of biomedical resources, such as images and tables. Unfortunately, most BQA datasets are either texts (Tsatsaronis et al., 2015;Pampari et al., 2018;Jin et al., 2019) or images (Lau et al., 2018;Ben Abacha et al., 2019a;He et al., 2020a); as a result, BQA datasets that are composed by fusing different biomedical resources are relatively limited.\nTo push forward the variety of BQA datasets, we present MLEC-QA, the largest-scale Chinese multi-choice BQA dataset. Questions in MLEC-QA are collected from the National Medical Licensing Examination in China (NMLEC) 2 , which are carefully designed by human experts to evaluate professional knowledge and skills for those who want to be medical practitioners in China. The NMLEC has a total number of 24 categories of exams, but only five of them have the written exams in Chinese. Every year, only around 18-22% of applicants can pass one of these exams, showing the complexity and difficulty of passing them even for skilled humans.\nThere are three main properties of MLEC-QA: (1) MLEC-QA is the largest-scale Chinese multichoice BQA dataset, containing 136,236 questions with extra materials (images or tables), Table 1 shows an example. (2) MLEC-QA first covers the following biomedical sub-fields: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Traditional Chinese Medicine Combined with Western Medicine (denoted as Chinese Western Medicine). Only one (Clinic) of them has been studied in previous research. (3) MLEC-QA provides extra labels of five question types (A1, A2, A3/A4 and B1) for each question, and an in-depth analysis of the most frequent reasoning types of the questions in MLEC-QA, such as lexical matching, multi-sentence reading and concept summary, etc. Detailed analysis can be found in Section 3.2. Examples of sub-fields and question types are summarized in Table 2. We set each example of five question types corresponding to one of the subfields due to page limits.  As an attempt to solve MLEC-QA and provide strong baselines, we implement eight representative control methods and open-domain QA methods by a two-stage retriever-reader framework: (1) A retriever finding documents that (might) contain an answer from a large collection of documents. We adopt Chinese Wikipedia dumps 3 as our information sources, and use a distributed search and analytics engine, ElasticSearch 4 , as the document store and document retriever. (2) A reader finding the answer in given documents retrieved by the retriever. We fine-tune five pre-trained language models for machine reading comprehension as the reader. Experimental results show that even the current best model can only achieve accuracies of 53%, 44%, 40%, 55%, and 50% on the five categories of subsets: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Chinese Western Medicine, respectively. The models especially perform poorly on questions that require understanding comprehensive biomedical concepts and handling complex reasoning.  In summary, the major contributions of this paper are threefold:\n\u2022 We present MLEC-QA, the largest-scale Chinese multi-choice BQA dataset with extra materials, and it first covers five biomedical sub-fields, only one of which has been studied in previous research.\n\u2022 We conduct an in-depth analysis on MLEC-QA, revealing that both comprehensive biomedical knowledge and sophisticated reasoning ability are required to answer questions. \u2022 We implement eight representative methods as baselines and show the performance of existing methods on MLEC-QA, and provide an outlook for future research directions.", "n_publication_ref": 19, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Open-Domain BQA The Text REtrieval Conference (TREC) (Voorhees and Tice, 2000) has triggered the open-domain BQA research. At the time, most traditional BQA systems were employing complex pipelines with question processing, document/passage retrieval, and answer processing modules. Examples of such systems include EPoCare (Niu et al., 2003), MedQA (Yu et al., 2007;Terol et al., 2007;Wang et al., 2007) and AskHERMES (Cao et al., 2011). With the introduction of various BQA datasets that are focused on specific biomedical topics, such as BioASQ (Tsatsaronis et al., 2015), emrQA (Pampari et al., 2018) and PubMedQA (Jin et al., 2019), pioneered by Chen et al. (2017), the modern open-domain BQA systems largely simplified the traditional BQA pipeline to a two-stage retriever-reader framework by combining information retrieval and machine reading comprehension models (Ben Abacha et al., 2017, 2019b. Moreover, the extensive use of medical images (e.g., CT) and tables (e.g., laboratory examination) has improved results in real-world clinical scenarios, making the BQA a task lying at the intersection of Computer Vision (CV) and NLP. However, most BQA models focus on either texts or images (Lau et al., 2018;Ben Abacha et al., 2019a;He et al., 2020a); as a result, BQA datasets that are composed by fusing different biomedical resources are relatively limited.\nOpen-Domain Multi-Choice BQA Datasets With rapidly increasing numbers of consumers asking health-related questions on online medical consultation websites, cMedQA (Zhang et al., 2017(Zhang et al., , 2018a, webMedQA (He et al., 2019) and ChiMed (Tian et al., 2019) exploit patient-doctor QA data to build consumer health QA datasets. However, the quality problems in such datasets are that the answers are written by online-doctors and the data itself has intrinsic noise. By contrast, medical licensing examinations, which are designed by human medical experts, often take the form of multi-choice questions, and contain a significant number of questions that require comprehensive biomedical knowledge and multiple reasoning ability. Such exams are the perfect data source to push the development of BQA systems. Several datasets have been released that exploit such naturally existing BQA data, which are summarized in Table 3. Collecting from the Spain public healthcare specialization examination, HEAD-QA (Vilares and G\u00f3mez-Rodr\u00edguez, 2019) contains multichoice questions from six biomedical categories, including Medicine, Pharmacology, Psychology, Nursing, Biology and Chemistry. NLPEC (Li et al., 2020) collects 21.7k multi-choice questions with human-annotated answers from the National Licensed Pharmacist Examination in China, but only a small number of sample data is available for public use.\nLast but not least, clinical medicine, as one of the 24 categories in NMLEC, has been previously studied by MedQA (Zhang et al., 2018b) and MEDQA (Jin et al., 2020). However, the former did not release any data or code, and the latter only focused on clinical medicine with 34k questions in their cross-lingual studies, questions with images or tables were not included, and none of the remaining categories in MLEC-QA were studied.   Basically, as shown in Table 2 and Table 4, the questions in MLEC-QA are divided into five types including:\n\u2022 A1: single statement question; \u2022 B1: similar to A1, with a group of options shared in multiple questions;\n\u2022 A2: questions accompanied by a clinical scenario; \u2022 A3: similar to A2, with information shared among multiple independent questions; \u2022 A4: similar to A3, with information shared among multiple questions, new information can be gradually added. We further classify these questions into Knowledge Questions (KQ) and Case Questions (CQ), where KQ (A1+B1) focus on the definition and comprehension of biomedical knowledge, while CQ (A2+A3/A4) require analysis and practical application for real-world medical scenarios. Both types of questions require multiple reasoning ability to answer.  For the Train/Dev/Test split, randomly splitting may cause data imbalance because the number of the five question types are various from each other (e.g., A1 is far more than others). To ensure that the subsets have the same distribution of the question types, we split the data based on the question types, with 80% training, 10% development, and 10% test. The overall statistics of the MLEC-QA dataset are summarized in Table 5. We can see that the length of the questions and the vocabulary size in Clinic are larger than the rest of the subsets, explaining that clinical medicine may involve more medical subjects than other specialties.  ", "n_publication_ref": 22, "n_figure_ref": 0}, {"heading": "Reasoning Types of the Questions", "text": "Since the annual examination papers are designed by a team of healthcare experts who try to follow the similar reasoning types distribution. To better understand our dataset, we manually inspected 10 sets of examination papers (2 sets for each subfield), and summarize the most frequent reasoning types of the questions from MLEC-QA and previous works (Lai et al., 2017;Zhong et al., 2020).\nThe examples are shown in Table 6. Notably, the \"Evidence\" is well-organized by us to show how models need to handle these reasoning issues to achieve promising performance in MLEC-QA.\nThe definition of reasoning types of the questions are as follows:\nLexical Matching This type of question is common and the simplest. The retrieved documents are highly matched with the question, the correct answer exactly matches a span in the document. As shown in the example, the model only needs to check which option is matched with.\nMulti-Sentence Reading Unlike lexical matching, where questions and correct answers can be found within a single sentence, multi-sentence reading requires models reading multiple sentences to gather enough information to generate answers.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Concept Summary", "text": "The correct options for this type of question do not appear directly in the documents. It requires the model to understand and summarize the question relevant concepts after reading the documents. As shown in the example, the model needs to understand and summarize the relevant mechanism of \"Thermoregulation\", and infer that when an obstacle arises in thermoregulation, the body temperature will not be able to maintain a relatively constant level, that is, it will rise with the increase of ambient temperature.\nNumerical Calculation This type of question involves logical reasoning and arithmetic operations related to mathematics. As shown in the example, the model first needs to judge the approximate age of month according to the height of the infant, and then reverse calculate the age of months according to the height formula of infants 7~12 months old to obtain the age in months: (68 -65) / 1.5 + 6 = 8.\nMulti-Hop Reasoning This type of question requires several steps of logical reasoning over mul-\ntiple documents to answer. As shown in the example, the patient's hemoglobin (HB) value is low, indicating that the patient has anemia, and the supply of iron should be increased in their diet. The model needs to compare the iron content of each option: the iron content of C, D and E is low and that of A, B is high, but B is not easily absorbed, so the best answer is A.\nReasoning Type Example ( * represents the correct answer)", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Lexical Matching", "text": "The main hallmark of peritonitis is: A. Significant abdominal distension B. Abdominal mobility dullness C. Bowel sounds were reduced or absent D. Severe abdominal cramping * E. Peritoneal irritation signs Evidence:\nThe hallmark signs of peritonitis are peritoneal irritation signs, i.e., tenderness, muscle tension, and rebound tenderness.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Multi-Sentence Reading", "text": "Which is wrong in the following narrative relating to the appendix: The purpose of thermoregulation is to maintain body temperature in the normal range. In hyperthermic environments, the thermoregulatory center is dysfunctional and cannot maintain the body's balance of heat production and heat dissipation, so the body temperature is increased by the influence of ambient temperature.\nA.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Numerical Calculation", "text": "A normal infant, weighing 7.5kg and measuring 68cm in length. Bregma 1.0cm, head circumference 44cm. Teething 4. Can sit alone and can pick up pellets with a hallux and forefinger. The most likely age of the infant is: * A. 8 months B. 24 months C. 18 months D. 12 months E. 5 months Evidence: A normal infant measured 65cm at 6 months and 75cm at 1 year of age. The infant's 7 to 12 month length is calculated as: length = 65 + (months of age -6) x 1.5.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Multi-Hop Reasoning", "text": "6-month-old female infant, artificial feeding mainly, physical examination revealed a low hemoglobin (HB) value, the dietary supplement that should be mainly added is: * A. Liver paste B. Egg yolk paste C. Tomato paste D. Rice paste E. Apple puree Evidence:\n(1) Low HB value indicates anemia tendency. Iron deficiency anemia is the most important and common type of anemia in China.\n(2) Iron supply should be increased in diet. (3) Liver paste is rich in iron. (4) The iron content of egg yolk paste is lower than that of liver paste, and it is not easy to be absorbed. (5) The iron content of tomato paste, rice paste and apple puree is lower than that of liver paste. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Document Retriever", "text": "Both examination counseling books and Wikipedia have been used as the source of supporting materials in previous research (Zhong et al., 2020;Jin et al., 2020;Vilares and G\u00f3mez-Rodr\u00edguez, 2019). However, because examination counseling books are designed to help examinees pass the examination, knowledge is highly simplified and summarized; even the easily confused knowledge points are compared. Using examination counseling books as information sources may make the retriever-reader more likely to exploit shallow text matching, and complex reasoning is seldom involved.\nTherefore, to help better understand the improvement coming from future models, we adopt Chinese Wikipedia dumps as our information sources, which contain a wealth of information (over 1 million articles) of real-world facts. Building upon the whole Chinese Wikipedia data, we use a distributed search and analytics engine, Elas-ticSearch, as the document store and document retriever, which supports very fast full-text searches. The similarity scoring function used in Elasticsearch is the BM25 algorithm (Robertson and Zaragoza, 2009), which measures the relevance of documents to a given search query. As defined in Appendix C, the larger this BM25 score, the stronger the relevance between document and query.\nSpecifically, for each question Q i and each candidate option O ij where j \u2208 {A, B, C, D, E}, we define Q i O ij = Q i + O ij as a search query to Elasticsearch and is repeated for all options. The document with the highest BM25 score returned by each query is selected as supporting materials for the next stage machine reading comprehension task.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Control Methods", "text": "In general, each option should have the same correct rate for multi-choice questions, but in fact, the order in which the correct options appear is not completely random, and the more the number of options, the lower the degree of randomization (Poundstone, 2014). Given the complex nature of multi-choice tasks, we employ three control methods to ensure a fair comparison among various open-domain QA models.\nRandom A \u2032 = Random(O). For each question, an option is randomly chosen as the answer from five candidate options. We perform this experiment five times and average the results as the baseline of the Random method.\nConstant A \u2032 = Constant j (O), where j \u2208 {A, B, C, D, E}. For each question, the j th option is always chosen as the answer to obtain the accuracy distribution of five candidate options.\nMixed A \u2032 = M ixed(O).\nIncorporating the previous experiences of NMLEC and multi-choice task work (Vilares and G\u00f3mez-Rodr\u00edguez, 2019), the Mixed method simulates how humans solving uncertain questions, and consists of the following three strategies: (1) the correct rate of choosing \"All of the options above is correct/incorrect\" is much higher than the other options. (2) Supposing the length of options is roughly equal, only one option is obviously longer with more detailed and specific descriptions, or is obviously shorter than the other options, then choose this option. (3) The correct option tends to appear in the middle of candidate options. The three strategies are applied in turn. If any strategy matches, then the option that matches the strategy is chosen as the answer.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Fine-Tuning Pre-Trained Language Models", "text": "We apply an unified framework UER-py (Zhao et al., 2019) to fine-tuning pre-trained language models on the machine reading comprehension task as our reader. We consider the following five pre-trained language models: Chinese BERT-Base (denoted as BERT-Base) and Multilingual Uncased BERT-Base (denoted as BERT-Base-Multilingual) (Devlin et al., 2019), Chinese BERT-Base with whole word masking and pre-trained over larger corpora (denoted as BERT-wwm-ext) (Cui et al., 2019), and the robustly optimized BERTs: Chinese RoBERTa-wwm-ext and Chinese RoBERTa-wwm-ext-large (Cui et al., 2019). Specifically, given the i th question Q i , retrieved question relevant documents D i , and a candidate option O ij , where j \u2208 {A, B, C, D, E}. The input sequence for the framework is constructed by concatenating [CLS], tokens in D i , [SEP], tokens in Q i , [SEP], tokens in an option O ij , and [SEP], where [CLS] is the classifier token, and [SEP] is the sentence separator in pre-trained language models. We pass each of the five options in turn, and the model outputs the hidden state representation S ij \u2208 R 1\u00d7H of the input sequence, then performs the classification and output an unnormalized log probability P ij \u2208 R of each option O ij being correct by P ij = S ij W T , where W \u2208 R 1\u00d7H is the weight matrix. Finally, we pass the unnormalized log probabilities of each option through a softmax layer and obtain the option with the highest probability as the predicted answer A \u2032 i .", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Experiments", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Settings", "text": "We conduct detailed experiments and analyses to investigate the performance of control methods and open-domain QA methods on MLEC-QA. As shown in Figure 2, we implement a two-stage retriever-reader framework: (1) a retriever first retrieves question relevant documents from Chinese Wikipedia using ElasticSearch, (2) and then a reader employs machine reading comprehension models to generate answers in given documents retrieved by the retriever. For the reader, all machine reading comprehension models are trained with 12 epochs, an initial learning rate of 2e-6, a maximum sequence length of 512, a batch size of 5. The parameters are selected based on the best performance on the development set, and we keep the default values for the other hyper-parameters (Devlin et al., 2019). We use accuracy as the metric to evaluate different methods, and provide baseline results, as well as human pass mark (60%) instead of human performance due to the wide variations exist in human performance, from almost full marks to cannot even pass the exam. ", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Retrieval Performance", "text": "The main drawbacks of the Chinese Wikipedia database in biomedicine are that it is not comprehensive and thorough, that is, it may not provide complete coverage of all subjects. To evaluate whether retrieved documents can cover enough evidence to answer questions, we sampled 5%\n(681) questions from the development sets of five categories using stratified random sampling, and manually annotate each question by five medical experts with 3 labels: (1) Exactly Match (EM): the retrieved documents exactly match the question.\n(2) Partial Match (PM): the retrieved documents partially match the question, can be confused with the correct options or are incomplete.\n(3) Mismatch (MM): the retrieved documents do not match the question at all. Table 7 lists the performance of the retrieval strategy as well as the results of the annotation for KQ and CQ questions on five subsets. From the table, we make the following observations. First, most retrieved documents indicate PM with the questions, while the matching rates of EM and MM achieve maximums of 20.83% (CWM) and 50% (PH), respectively. Second, the matching rate of CQ is higher than KQ in most subsets as CQ are usually related to simpler concepts, and use more words to describe questions, which leads to easier retrieval. By contrast, KQ usually involve more complex concepts that may not be included in the Chinese Wikipedia database. Therefore, the mismatching rate of KQ is significantly higher than that of CQ. Third, among different subsets, the performance in the subset Cli achieves the best as clinical medicine is more \"general\" to retrieve compare with other specialties. Whereas the performance in the subset PH achieves the worst as the Public Health is usually related to \"confusing concepts\", which leads to poor retrieval performance.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Baseline Results", "text": "Tables 8 and Figure 3 show the performance of baselines as well as the performance on KQ and CQ questions. As we can see, among control methods, the correct option has a slight tendency to appear in the middle (C and D) of candidate options, but the margins are small. The performance of the Mixed method is slightly better than a random guess, which indicates that the flexible use of  guessing skills may add wings to the tiger as humans can exclude some certain wrong options, but if the cart before the horse is reversed, it is impossible to pass the exam only through opportunistic guessing. RoBERTa-wwm-ext-large and BERTwwm-ext perform better than other models on five subsets. However, even the best-performing model can only achieve accuracies between 40% to 55% on five subsets, so there is still a gap to pass the exams. Comparing the performance between KQ and CQ questions, most models achieve better performance on CQ, which is positively correlated with CQ's better retrieval performance. Among different subsets, the subset TCM is the easiest (54.95%) one to answer across the board, while the subset PH is the hardest (40.04%), which does not totally correspond to their retrieval performance as shown in Table 7. The possible reason is that the diagnosis and treatment of diseases in traditional Chinese medicine are characterized by \"Homotherapy for Heteropathy\", that is, treating different diseases with the same method, which may result in some patterns or mechanisms that can be used by the models to reach such results.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Comparative Analysis", "text": "Given that we use the Chinese Wikipedia database as our information sources and apply a two-stage retriever-reader framework, the reason for such poor baseline performance could come from both our information sources and the retriever-reader framework.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Information", "text": "Sources Both books and Wikipedia have been used as the information sources in previous research. One of our subsets, Clinic, has been studied by MEDQA (Jin et al., 2020) as a subset (MCMLE) for cross-lingual research. MEDQA uses 33 medical textbooks as their information sources and the evaluation result shows that their collected text materials can provide enough information to answer all the questions in MCMLE. We compare the best model (RoBERTa-wwm-ext-large) performance on both datasets as shown in Table 9. Notably, questions in MCMLE have four candidate options due to one of the wrong options being deleted. Therefore, the random accuracy on MCMLE is higher than ours.\nFrom the results we can see that even with 100% covered materials, the best model can only  achieve 16.88% higher accuracy on the test set than ours, which indicates that using Wikipedia as information sources is not that terrible compared with medical books, and the main reason for baseline performance may come from machine reading comprehension models that lack sophisticated reasoning ability.\nRetriever-Reader We also perform an experiment that sampled 5% (92) questions from the development set of Public Health, and manually annotate each question by a medical expert to determine whether that can exactly or partially match with the top K retrieved documents, as shown in Table 10. Notably, the actual number of retrieved documents is 5\u00d7K as we define Q i O ij = Q i +O ij as a search query and is repeated for all options.\nFrom the results, we can see that more documents even bring more noise instead, as the best match documents have already been fetched in the top 1 documents. It indicates that the poor performance of machine reading comprehension models is coming from the insufficiency of reasoning ability rather than the number of retrieved documents.     In order to benefit researchers on improving the open-domain QA models, and also make advances for Biomedical Question Answering (BQA) systems, we present MLEC-QA, the largest-scale Chinese multi-choice BQA dataset to date.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "F.2 LANGUAGE VARIETY", "text": "The data is represented in simplified Chinese (zh-Hans-CN), and collect from the 2006 to 2020 NM-LEC, as well as practice exercises from the Internet.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "F.3 SPEAKER DEMOGRAPHIC", "text": "Since the data is designed by a team of anonymous human healthcare experts, we are not able to directly reach them for inclusion in this dataset and thus could not be asked for demographic information. It is expected that most of the speakers come from China with professionals working in the area of biomedicine, and speak Chinese as a native language. No direct information is available about age and gender distribution.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "F.4 ANNOTATOR DEMOGRAPHIC", "text": "The experiments involve annotations from 5 medical experts with at least have a master's degree and have passed the NMLEC. They ranged in age from 28 45 years, included 3 men and 2 women, all come from China and speak Chinese as a native language.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "F.5 SPEECH SITUATION", "text": "All questions in MLEC-QA are collected from the National Medical Licensing Examination in China (NMLEC), which are carefully designed by human experts to evaluate professional knowledge and skills for those who want to be medical practitioners in China. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their insightful comments and suggestions. This work is supported by the National Natural Science Foundation of China (NSFC No. 61972187).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Source of Data Collection", "text": "For five subsets in MLEC-QA, we collect 2006 to 2020 Sprint Paper for the National Medical Licensing Examination -Tianjin Science and Technology Press in PDF format, and then converted them into digital format via Optical Character Recognition (OCR). We manually checked and corrected the OCR results with confidence less than 0.99 to ensure the quality of our dataset. We also scraped practice exercises from offcn (http://www.offcn.com/yixue/yszg/), which are freely accessible online for public usage.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B Data Structure", "text": "The data structure below describe the JSON file representation in MLEC-QA.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "{", "text": "\"qid\":The question ID, \"qtype\":[\"A1 \", \"B1 \", \"A2 \", \"A3/A4 \"], \"qtext\":Description of the question, \"qimage\":Image or table path (if any), \"options\":{ \"A\":Description of the option A, \"B\":Description of the option B, \"C\":Description of the option C, \"D\":Description of the option D, \"E\":Description of the option E }, \"answer\":[\"A\", \"B\", \"C\", \"D\", \"E\"] }", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C BM25 Score Function", "text": "The BM25 algorithm is defined as:\nwhere q i is the i th query term of a query Q, f (q i , D) is q i 's term frequency in the document D, |D| is the length of the document D in words, and avgdl is the average document length in the text collection from which documents are drawn. b determines the effects of the length of the document on the average length. k1 is a variable which helps determine term frequency saturation characteristics. By default, b, k1 has a value of 0.75, 1.2 in Elasticsearch, respectively. IDF (q i ) is the Inverse Document Frequency (IDF) weight of the query term q i . It is usually computed as:\nwhere N is the total number of documents in the collection, and n(q i ) is the number of documents containing q i . ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Overview of the medical question answering task at TREC", "journal": "", "year": "2017", "authors": "Asma Ben Abacha; Eugene Agichtein; Yuval Pinter; Dina Demner-Fushman"}, {"title": "Proceedings of the Twenty-Sixth Text REtrieval Conference", "journal": "NIST", "year": "2017-11-15", "authors": " Liveqa"}, {"title": "VQA-Med: Overview of the medical visual question answering task at ImageCLEF", "journal": "", "year": "2019-09-09", "authors": "Asma Ben Abacha; Sadid A Hasan; V Vivek; Joey Datla; Dina Liu; Henning Demner-Fushman;  M\u00fcller"}, {"title": "Bridging the gap between consumers' medication questions and trusted answers", "journal": "IOS Press", "year": "2019-08-30", "authors": "Asma Ben Abacha; Yassine Mrabet; Mark Sharp; Travis R Goodwin; Sonya E Shooshan; Dina Demner-Fushman"}, {"title": "AskHERMES: An online question answering system for complex clinical questions", "journal": "J. Biomed. Informatics", "year": "2011", "authors": "Yonggang Cao; Feifan Liu; Pippa Simpson; Lamont D Antieau; Andrew S Bennett; James J Cimino; John W Ely; Hong Yu"}, {"title": "Reading Wikipedia to answer opendomain questions", "journal": "Long Papers", "year": "2017", "authors": "Danqi Chen; Adam Fisch; Jason Weston; Antoine Bordes"}, {"title": "Pre-Training with Whole Word Masking for Chinese BERT. ArXiv", "journal": "", "year": "2019", "authors": "Yiming Cui; W Che; T Liu; B Qin; Ziqing Yang; S Wang; G Hu"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Applying deep matching networks to Chinese medical question answering: A study and a dataset", "journal": "BMC Medical Informatics and Decision Making", "year": "2019", "authors": "Junqing He; Mingming Fu; Manshu Tu"}, {"title": "PathVQA: 30000+ questions for medical visual question answering. CoRR, abs", "journal": "", "year": "2003", "authors": "Xuehai He; Yichen Zhang; Luntian Mou; Eric P Xing; Pengtao Xie"}, {"title": "Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition", "journal": "", "year": "2020", "authors": "Yun He; Ziwei Zhu; Yin Zhang; Qin Chen; James Caverlee"}, {"title": "Hanyi Fang, and Peter Szolovits. 2020. What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams", "journal": "", "year": "", "authors": "Di Jin; Eileen Pan; Nassim Oufattole; Wei-Hung Weng"}, {"title": "PubMedQA: A dataset for biomedical research question answering", "journal": "", "year": "2019", "authors": "Qiao Jin; Bhuwan Dhingra; Zhengping Liu; William Cohen; Xinghua Lu"}, {"title": "Biomedical Question Answering: A Comprehensive Review", "journal": "", "year": "2021", "authors": "Qiao Jin; Zheng Yuan; Guangzhi Xiong; Qianlan Yu; Chuanqi Tan; Mosha Chen; Songfang Huang; Xiaozhong Liu; Sheng Yu"}, {"title": "RACE: Large-scale ReAding comprehension dataset from examinations", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Guokun Lai; Qizhe Xie; Hanxiao Liu; Yiming Yang; Eduard Hovy"}, {"title": "A dataset of clinically generated visual questions and answers about radiology images", "journal": "Scientific Data", "year": "2018", "authors": "Jason J Lau; Soumya Gayen; Asma Ben Abacha; Dina Demner-Fushman"}, {"title": "Towards medical machine reading comprehension with structural knowledge and plain text", "journal": "", "year": "2020", "authors": "Dongfang Li; Baotian Hu; Qingcai Chen; Weihua Peng; Anqi Wang"}, {"title": "PKUSEG: A toolkit for multi-domain chinese word segmentation. CoRR, abs", "journal": "", "year": "1906", "authors": "Ruixuan Luo; Jingjing Xu; Yi Zhang; Xuancheng Ren; Xu Sun"}, {"title": "Answering clinical questions with role identification", "journal": "Association for Computational Linguistics", "year": "2003", "authors": "Yun Niu; Graeme Hirst; Gregory Mcarthur; Patricia Rodriguez-Gianolli"}, {"title": "emrQA: A large corpus for question answering on electronic medical records", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Anusri Pampari; Preethi Raghavan; Jennifer Liang; Jian Peng"}, {"title": "Rock Breaks Scissors. Little Brown &amp", "journal": "Co", "year": "2014", "authors": "William Poundstone"}, {"title": "The Probabilistic Relevance Framework: BM25 and Beyond", "journal": "Foundations and Trends in Information Retrieval", "year": "2009", "authors": "Stephen Robertson; Hugo Zaragoza"}, {"title": "A knowledge based method for the medical question answering problem", "journal": "Comput. Biol. Medicine", "year": "2007", "authors": "Rafael M Terol; Patricio Mart\u00ednez-Barco; Manuel Palomar"}, {"title": "ChiMed: A Chinese medical corpus for question answering", "journal": "", "year": "2019", "authors": "Yuanhe Tian; Weicheng Ma; Fei Xia; Yan Song"}, {"title": "Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Arti\u00e8res, Axel-Cyrille Ngonga Ngomo", "journal": "", "year": "2015", "authors": "George Tsatsaronis; Georgios Balikas; Prodromos Malakasiotis; Ioannis Partalas; Matthias Zschunke; Michael R Alvers; Dirk Weissenborn; Anastasia Krithara"}, {"title": "An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition", "journal": "BMC Bioinform", "year": "", "authors": ""}, {"title": "HEAD-QA: A healthcare dataset for complex reasoning", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "David Vilares; Carlos G\u00f3mez-Rodr\u00edguez"}, {"title": "The TREC-8 question answering track", "journal": "", "year": "2000", "authors": "Ellen M Voorhees; Dawn M Tice"}, {"title": "Automatic clinical question answering based on UMLS relations", "journal": "IEEE Computer Society", "year": "2007-10-29", "authors": "Weiming Wang; Dawei Hu; Min Feng; Liu Wenyin"}, {"title": "Development, implementation, and a cognitive evaluation of a definitional question answering system for physicians", "journal": "J. Biomed. Informatics", "year": "2007", "authors": "Hong Yu; Minsuk Lee; David R Kaufman; John W Ely; Jerome A Osheroff; George Hripcsak; James J Cimino"}, {"title": "Multi-scale attentive interaction networks for chinese medical question answer selection", "journal": "IEEE Access", "year": "2018", "authors": "S Zhang; X Zhang; H Wang; L Guo; S Liu"}, {"title": "Chinese medical question answer matching using end-to-end characterlevel multi-scale CNNs", "journal": "Applied Sciences", "year": "2017", "authors": "Sheng Zhang; Xin Zhang; Hui Wang; Jiajun Cheng; Pei Li; Zhaoyun Ding"}, {"title": "Medical exam question answering with large-scale reading comprehension", "journal": "AAAI Press", "year": "2018-02-02", "authors": "Xiao Zhang; Ji Wu; Zhiyang He; Xien Liu; Ying Su"}, {"title": "UER: An open-source toolkit for pretraining models", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Zhe Zhao; Hui Chen; Jinbin Zhang; Xin Zhao; Tao Liu; Wei Lu; Xi Chen; Haotang Deng; Qi Ju; Xiaoyong Du"}, {"title": "The Thirty-Second Innovative Applications of Artificial Intelligence Conference", "journal": "", "year": "2020-02-07", "authors": "Haoxi Zhong; Chaojun Xiao; Cunchao Tu; Tianyang Zhang; Zhiyuan Liu; Maosong Sun"}, {"title": "Question answering in biomedicine", "journal": "", "year": "2003", "authors": "Pierre Zweigenbaum"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Examples of extra materials.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "The appendiceal artery is the terminal artery B. Appendiceal tissues contain abundant lymphoid follicles C. Periumbilical pain at appendicitis onset visceral pain * D. Resection of the appendix in adults will impair the body's immune function E. There are argyrophilic cells in the deep part of the appendiceal mucosa, which are associated with carcinoid tumorigenesis Evidence: (1) The appendiceal artery is a branch of the ileocolic artery and is a terminal artery without collaterals; (2) The appendix is a lymphoid organ[...]Therefore, resection of the adult appendix does not compromise the body's immune function; (3) The nerves of the appendix are supplied by sympathetic fibers[...]belonging to visceral pain; (4) Argyrophilic cells are found in the appendiceal mucosa and are the histological basis for the development of appendiceal carcinoids. Concept Summary The main hallmark of thermoregulatory disorders in hyperthermic environments is: A. Developed syncope B. Developed shock C. Dry heat of skin * D. Increased body temperature E. Decreased body temperature Evidence:", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Overview of the two-stage retriever-reader framework on MLEC-QA.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Performance in accuracy (%) on KQ (solid lines) and CQ (dashed lines) questions.", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_5", "figure_caption": "F. 6 F6TEXT CHARACTERISTICS The topics include in MLEC-QA are in 5 biomedical sub-fields: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Traditional Chinese Medicine Combined with Western Medicine.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Examples of sub-fields and question types in MLEC-QA. The Chinese version is in Appendix D.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Comparison of MLEC-QA with existing opendomain multi-choice BQA datasets. No* indicates a small number of sample data is available. Extra indicates if the dataset provides extra material to answer questions. TCM), and Chinese Western Medicine (CWM). After removing duplicated or incomplete questions (e.g., some options missing), there are 136,236 questions in MLEC-QA, and each question contains five candidate options with one correct/best option and four incorrect or partially correct options. We describe in detail the JSON data structure of MLEC-QA in Appendix B.MLEC-QA contains 1,286 questions with extra materials that provide additional information to answer correctly. As shown in Figure1, the extra materials are all in a graphical format with various types, such as ECG, table of a patient's condition record, formula, CT, line graph, explanatory drawing, etc. We include these questions with extra materials in MLEC-QA to facilitate future BQA explorations on the crossover studies of CV and NLP, although we will not exploit them in this work due to the various specifics involved in extra materials.", "figure_data": "3 MLEC-QA Dataset3.1 Data Collection"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Statistics of question types in MLEC-QA, where \"Extra\" indicates number of questions with extra materials. Only A1, A2, and B1 are used in the examination of Chinese Western Medicine.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Examples of reasoning types of the questions in MLEC-QA. The Chinese version is in Appendix E.", "figure_data": "4 MethodsNotation We represent MLEC-QA task as:(D, Q, O, A), where Q"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": ": Matching rate (%) of retrieved documents thatexactly match, partial match or mismatch with the ques-tions in the MLEC-QA dataset."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "19.61 19.41 19.43 19.70 20.13 20.17 20.11 19.69  20.16 Constant Option [A] 17.12 18.05 16.82 17.01 16.09 17.80 19.05 20.60 18.68 20.56 Option [B] 20.30 20.93 20.95 20.53 21.49 21.36 22.52 20.70 20.70 21.20 Option [C] 21.66 21.23 22.31 20.95 21.65 20.23 22.29 22.35 23.39 21.27", "figure_data": "CliStoPHTCMCWMMethodDevTestDevTestDevTestDevTestDevTestRandom19.73 Option [D] 22.53 21.38 20.64 22.76 22.68 21.84 19.80 20.67 21.00 19.51Option [E]18.40 18.41 19.28 18.75 18.09 18.77 16.33 15.68 16.22 17.46Mixed24.40 24.68 24.46 24.42 23.97 23.68 22.07 22.38 23.62 23.25BERT-Base47.26 48.30 40.53 40.08 38.99 37.40 48.51 49.14 44.32 45.14BERT-wwm-ext50.27 50.89 43.26 42.05 41.75 40.04 54.57 54.94 49.89 50.04BERT-Base-Multilingual46.61 47.68 39.85 38.76 36.61 36.70 45.50 46.61 42.26 42.86RoBERTa-wwm-ext49.94 51.97 41.14 40.88 38.40 38.91 50.45 49.82 47.38 46.00RoBERTa-wwm-ext-large 53.25 53.22 44.92 43.75 39.10 38.75 47.99 48.65 50.49 50.11Human Pass Mark60"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Performance of baselines in accuracy (%) on the MLEC-QA dataset.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "", "figure_data": ": Comparison of best model (RoBERTa-wwm-ext-large) performance on MEDQA and our MLEC-QA dataset."}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Matching rate (%) of Top K retrieved documents that exactly or partial match with the questions in the Public Health subset.6 ConclusionWe present the largest-scale Chinese multi-choice BQA dataset, MLEC-QA, which contains five biomedical subfields with extra materials (images or tables) annotated by human experts: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Chinese Western Medicine. Such questions correspond to examinations (NMLEC) to access the qualifications of medical practitioners in the Chinese healthcare system, and require specialized domain knowledge and multiple reasoning abilities to be answered. We implement eight representative control methods and opendomain QA methods by a two-stage retrieverreader framework as baselines. The experimental results demonstrate that even the current best approaches cannot achieve good performance on MLEC-QA. We hope MLEC-QA can benefit researchers on improving the open-domain QA models, and also make advances for BQA systems.", "figure_data": ""}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Chinese version of Table 2.", "figure_data": "E Chinese version of examples ofreasoning typesReasoning TypeExample ( *  represents the correct answer)Lexical MatchingA. D. Evidence:B.* E.C.A.B.C.* D.Multi-Sentence ReadingE. Evidence: (1)(2)[...](3)[...](4)A.B.C.* D.E.ConceptEvidence:Summary7.5kg68cm1.0cm44cm4Numerical* A. 8B. 24C. 18D. 12E. 5CalculationEvidence:665cm 175cm7\u223c12= 65 + (-6) x 1.56Hb* A.B.C.D.E.Multi-HopEvidence:Reasoning(1)Hb(2)(3)(4)(5)"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Chinese version of Table6.", "figure_data": ""}], "doi": "10.18653/v1/P17-1171"}