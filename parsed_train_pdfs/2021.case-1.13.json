{"authors": "Pawan Kalyan; Duddukunta Sashidhar Reddy; Adeep Hande; Ruba Priyadharshini; Ratnasingam Sakuntharaj; Bharathi Raja Chakravarthi", "pub_date": "", "title": "IIITT at CASE 2021 Task 1: Leveraging Pretrained Language Models for Multilingual Protest Detection", "abstract": "In a world abounding in constant protests resulting from events like a global pandemic, climate change, religious or political conflicts, there has always been a need to detect events/protests before getting amplified by news media or social media. This paper demonstrates our work on the sentence classification subtask of multilingual protest detection in CASE@ACL-IJCNLP 2021. We approached this task by employing various multilingual pre-trained transformer models to classify if any sentence contains information about an event that has transpired or not. Furthermore, we performed soft voting over the models, achieving the best results among the models, accomplishing a macro F1-Score of 0.8291, 0.7578, and 0.7951 in English, Spanish, and Portuguese, respectively. The source codes for our systems are published 1 .", "sections": [{"heading": "Introduction", "text": "The recent surge in social media users has led many people to express their opinions on various global issues. These opinions travel far and wide within a matter of seconds (Hossny et al., 2018). This can influence many people and may engage public movements (Won et al., 2017a). Therefore, there is a definite need to detect these protests and analyse them to know the significant areas of disinterest.\nBeing a free and easy to use platform, social media has become a part of our day to day life. It incorporates people of different ages, gender, location, religions, background, and so on. The enormous number of rich and diversified users results in an enormous amount of information being generated, which is helpful in many ways (Kapoor et al., 2018). Some of this even contains private information about the users, which others could misuse. Cases were also found where certain users were being targeted and harassed by people using this platform, a common scenario in cyberbullying (Abaido, 2020).\nSocial media plays a crucial role in amplifying these protests and movements (Won et al., 2017b). It enables political groups and protesters to organise protest movements and share information. It acts as a platform for the people who are underrepresented by giving a voice to them. It also offers new opportunities for people to engage in activism, political resistance, and protest outside the political groups and civic institutions. Thus, it has a social impact on everyone (Pulido et al., 2018). It is to be noted that social media, similar to news media, plays a vital role in its social and political events worldwide (Holt et al., 2013). For the above reasons, we can state that social media plays a crucial role in most worldwide events.\nThe English language is widely regarded as the first Lingua Franca. Statistically, it is one of the most widely spoken languages globally, having official status in over 53 countries (Crystal, 2008). Over 400 million people speak English as their primary language and widely spoken in the United States and the United Kingdom. BlackLivesMatter (Dave et al., 2020), EarthDay (Rome, 2010) are some of the major protests that have occurred in these countries. Espa\u00f1ol commonly referred to as Spanish, is spoken by over 360 million people worldwide, with most of its speakers residing in Mexico, Argentina, Spain. 15-M Movement (Casero-Ripoll\u00e9s andFeenstra, 2012) andYoSoy132 (Garc\u00eda andTrer\u00e9, 2014) are some of the recent protests where people have been vocal about in the Spanish language. Portuguese has over 220 million native speakers. Brazil, Portugal, Angola are some of the major countries where this language is spoken. Protests like Racism Kills, May 68 (Ross, 2008) are the recent ones that occurred in the Portuguese language.\nThe recent upheavals of protests are due to so-Sentence Language Label Fabius ran against Royal for the presidential nomination in 2007. English Event He planned to start a race war. English Event Metro police intervened and the fire was put out. English Not-event Pero no es\u00e9se el mayor problema. Spanish Event La Argentina retroceder\u00eda un paso todos los d\u00edas. Spanish Event Carri\u00f3 no objet\u00f3 que se trat\u00f3 de un secuestro. Spanish Not-event Os servidores do Piau\u00ed est\u00e3o em greve h\u00e1 17 dias.\nPortuguese Not-event E uma nova experi\u00eancia mobilizat\u00f3ria.\nPortuguese Event E decidiram ir\u00e0s aulas e passar o dia de saia.\nPortuguese Not-event cial media, youth, exaggeration of certain events. (Basile and Caselli, 2020). Any early detection of mass protest detection through social media platforms such as Facebook, Twitter, and Instagram to help minimizing the aftermath of the protests (Wilson, 2017). This has motivated Natural Language Processing (NLP) researchers to develop NLP systems to generalize on data coming from diverse sources to leverage the NLP systems to more realistic environments (B\u00fcy\u00fck\u00f6z et al., 2020). Hence, there is a need to develop NLP systems that could be generalized to any protest/events (Peng et al., 2013), which has motivated us to participate in the shared task for multilingual protest detection (H\u00fcrriyetoglu et al., 2019a The objective of the task is to identify if any sentence talks about any mentions of protests or events in three languages, namely, English, Spanish, and Portuguese. Hence, we treat this as a sequence classification task. The rest of the paper is organized as follows, Section 2 presents previous work on protest detection and analysis. Section 3 entails a comprehensive analysis of the dataset used for our cause. Next, section 4 gives a detailed description of the models used for the multilingual event detection. Finally, section 5 analyses the results obtained, and Section 6 concludes our work while discussing the potential directions for future work.", "n_publication_ref": 20, "n_figure_ref": 0}, {"heading": "Related Work", "text": "The need to detect events that could lead to protests is of prime interest to sociologists and governments (Danilova et al., 2016). There are several active ongoing projects for socio-political event systems such as KEDS (Kansas Event Data System) (Schrodt and Hall, 2006), CAMEO (Conflict and Mediation Event Observation) (Gerner et al., 2002), and several other databases for protest de-tection systems (Danilova, 2015). These methods have focused on news data as they have traditionally been the most reliant source of events. Protest detection has been one of the major issues in the context of social and political (Ettinger et al., 2017). Papanikolaou and Papageorgiou (2020) presented a computational social science methodology to analyse protests in Greece.  constructed a corpus of protest events comprising various language sources from various countries. Several systems were submitted to the CLEF ProtestNews Track that consisted of three shared tasks, primarily aimed at identifying and extracting event information spanning to multiple countries (H\u00fcrriyetoglu et al., 2019b.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Dataset", "text": "This dataset comprises 26,208 sentences in three languages, namely English, Spanish, and Portuguese. The dataset consists of two classes:\n\u2022 Event: The sentence indicates an event of the past.\n\u2022 Not-event: The sentence does not talk about any event.\nThe volume of sequences indicating Not-event is higher in contrast to that of the Event label. Therefore, the dataset distribution is quite imbalanced. We can also notice that the number of English samples exceeds that of Spanish and Portuguese ones. Refer to  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Methodology", "text": "We used pretrained transformer-based models for identifying if a sentence talks about an event or not. The models that were used are BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and DistilBERT (Sanh et al., 2019). Even though there are 3 different languages, we used a single model for all three due to memory constraints and reduced training time. We fine-tuned these models for sequence classification. Soft Voting is done on all these models to produce the respective final outputs for the languages. In soft voting, each classifier predicts that a specific data point belongs to the particular target class. A weighted sum of the predictions is done based on the importance of the classifier (all models have equal weights). The overall prediction is chosen as the target with the greatest sum of the weighted probability, thus winning the vote (Beyeler, 2017;Hande et al., 2021).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "BERT", "text": "Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is a pretrained language model which was created with the objective that fine-tuning a pretrained model yields better performance. BERT's pretraining phase includes two tasks. Firstly, Masked Language Modeling (MLM) is where certain words are randomly masked in a sequence. About 15% of the words in a sequence is masked. The model then attempts to predict the masked words. Secondly, Next Sentence Prediction (NSP), where the model has an additional loss function, NSP loss, indicates if the second sequence follows the first one. Around 50% of the inputs are a pair, and they randomly chose the other 50. Here, we use a bert-base-multilingual-cased (Pires et al., 2019) trained on top of 104 languages in the largest Wikipedia corpus. This model has 12 layers, 12 Attention heads with over 179 million parameters. ", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "DistilmBERT", "text": "DistilBERT (Sanh et al., 2019) is the distilled version of BERT. DistilBERT employs a triple loss language modelling, where it integrates cosine distance loss with knowledge distillation. DistilBERT has 40% fewer parameters than BERT but still promises 97% of the latter's performance. It is also 60% faster than BERT. In this system, we used a cased multilingual DistilBERT model as they are three different languages. For our cause, we finetune distilbert-base-multilingual-cased, which is distilled from the mBERT checkpoint. The model has 6 layers, 768 dimensions, and 12 Attention heads, totalizing about 134 million parameters.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "RoBERTa", "text": "Robustly Optimized BERT (RoBERTa) (Liu et al., 2019) follows the same architecture of BERT while differing in the pretraining strategy. It is pretrained with MLM as its objective where the model tries to predict the masked words. RoBERTa model is trained on the vast English Wikipedia and CC-News datasets. The NSP is not employed as a pretraining strategy, and the tokens are dynamically masked, making the model slightly different to BERT. During tokenization, RoBERTa follows byte-pair encoding (BPE) (Gall\u00e9, 2019) as opposed to WordPiece employed in BERT. We use robertabase, a pretrained language model consisting of 12 layers, 768 hidden, 12 attention heads, and 125 million parameters.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "System Description", "text": "For our system, we fine-tune the pretrained models discussed in Section 4.1, 4.2, and 4.3. We combine the three datasets as the number of samples for Spanish and Portuguese are quite low. After combining the models, we split the validation set accordingly, maintaining the split's ratio and tabulating the results on the concatenated dataset in Table3. The embeddings are extracted from these models to be fed as input to the LSTM layer, (Hochreiter and Schmidhuber, 1997) as shown in Figure1. The resulting output is fed into a global average pooling layer (Lin et al., 2014) and then passed into fully connected layers, followed by a sigmoid activation function to obtain the resulting probability score for the input sentences. The same parameters are used for all three models. A dropout layer (Srivastava et al., 2014) is also added in between the fully connected layers for regularization.\nRefer Table 4 for the parameters used in the model.    English (22,825). We also believe that our approach of combining datasets could have influenced the performance of the low support datasets.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "The need to develop automated systems to detect any event is an active protest has constantly been increasing because of the escalation of social media users and several platforms to support them. In this paper, we have explored several multilingual language models to classify if a given sentence talks about an event that has happened (Event) or not (Not-event) in three languages. Our work primarily focuses on fine-tuning language models and feeding them to an architecture we created. We also observe that the problem of class imbalance has had a significant impact on the performance of the models. The soft voting approach has achieved macro F1-Scores of 0.8291, 0.7578, and 0.7951 for English, Spanish, and Portuguese, respectively. For future work, we intend to explore class weighting techniques and semi-supervised approaches to improve our performance.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Cyberbullying on social media platforms among university students in the united arab emirates", "journal": "International journal of adolescence and youth", "year": "2020", "authors": "Ghada Abaido"}, {"title": "Protest event detection: When task-specific models outperform an event-driven method", "journal": "Springer International Publishing", "year": "2020", "authors": "Angelo Basile; Tommaso Caselli"}, {"title": "Machine Learning for OpenCV", "journal": "Packt Publishing Ltd", "year": "2017", "authors": "Michael Beyeler"}, {"title": "Analyzing ELMo and DistilBERT on sociopolitical news classification", "journal": "", "year": "2020", "authors": "Berfu B\u00fcy\u00fck\u00f6z; Ali H\u00fcrriyetoglu; Arzucan\u00f6zg\u00fcr "}, {"title": "The 15-m movement and the new media: A case study of how new themes were introduced into spanish political discourse", "journal": "", "year": "2012", "authors": "Andreu Casero; -Ripoll\u00e9s ; Ram\u00f3n Feenstra"}, {"title": "Two thousand million? English Today", "journal": "", "year": "2008", "authors": "David Crystal"}, {"title": "A pipeline for multilingual protest event selection and annotation", "journal": "", "year": "2015", "authors": "Vera Danilova"}, {"title": "Multilingual protest event data collection with gate", "journal": "Springer International Publishing", "year": "2016", "authors": "Vera Danilova; Svetlana Popova; Mikhail Alexandrov"}, {"title": "Black lives matter protests and risk avoidance: The case of civil unrest during a pandemic. Working Paper 27408", "journal": "National Bureau of Economic Research", "year": "2020", "authors": "M Dhaval;  Dave; Kyutaro Andrew I Friedson;  Matsuzawa; J Joseph; Samuel Sabia;  Safford"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Towards linguistically generalizable NLP systems: A workshop and shared task", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Allyson Ettinger; Sudha Rao; Hal Daum\u00e9; Iii ; Emily M Bender"}, {"title": "Investigating the effectiveness of BPE: The power of shorter sequences", "journal": "", "year": "2019", "authors": "Matthias Gall\u00e9"}, {"title": "The# yosoy132 movement and the struggle for media democratization in mexico", "journal": "Convergence", "year": "2014", "authors": "Rodrigo G\u00f3mez Garc\u00eda; Emiliano Trer\u00e9"}, {"title": "The creation of cameo (conflict and mediation event observations): An event data framework for a post cold war world", "journal": "", "year": "2002", "authors": "J Deborah;  Gerner; A Philip; Omur Schrodt; Rajaa Yilmaz;  Abu-Jabr"}, {"title": "Domain identification of scientific articles using transfer learning and ensembles", "journal": "Springer International Publishing", "year": "2021", "authors": "Adeep Hande; Karthik Puranik; Ruba Priyadharshini; Bharathi Raja Chakravarthi"}, {"title": "Long short-term memory", "journal": "Neural Computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"title": "Age and the effects of news media attention and social media use on political interest and participation: Do social media function as leveller?", "journal": "European Journal of Communication", "year": "2013", "authors": "Kristoffer Holt; Adam Shehata; Jesper Str\u00f6mb\u00e4ck; Elisabet Ljungberg"}, {"title": "Enhancing keyword correlation for event detection in social networks using svd and k-means: Twitter case study. Social Network Analysis and Mining", "journal": "", "year": "2018", "authors": "Ahmad Hany Hossny; Terry Moschuo; Grant Osborne; Lewis Mitchell; Nick Lothian"}, {"title": "Multilingual protest news detectionshared task 1, case 2021", "journal": "ACL", "year": "2021", "authors": "Ali H\u00fcrriyetoglu; Osman Mutlu; Erdem Farhana Ferdousi Liza; Ritesh Y\u00f6r\u00fck; Shyam Kumar;  Ratan"}, {"title": "A task set proposal for automatic protest information collection across multiple countries", "journal": "Springer International Publishing", "year": "2019", "authors": "Ali H\u00fcrriyetoglu; Erdem Y\u00f6r\u00fck; Deniz Y\u00fcret; Burak Agr\u0131 Yoltar; F\u0131rat G\u00fcrel; Osman Duru\u015fan;  Mutlu"}, {"title": "Overview of clef 2019 lab protestnews: Extracting protests from news in a cross-context setting", "journal": "Springer International Publishing", "year": "2019", "authors": "Ali H\u00fcrriyetoglu; Erdem Y\u00f6r\u00fck; Deniz Y\u00fcret; Burak Agr\u0131 Yoltar; F\u0131rat G\u00fcrel; Osman Duru\u015fan; Arda Mutlu;  Akdemir"}, {"title": "Automated extraction of socio-political events from news (AESPEN): Workshop and shared task report", "journal": "", "year": "2020", "authors": "Ali H\u00fcrriyetoglu; Vanni Zavarella; Hristo Tanev; Erdem Y\u00f6r\u00fck; Ali Safaya; Osman Mutlu"}, {"title": "Cross-Context News Corpus for Protest Event-Related Knowledge Base Construction", "journal": "Data Intelligence", "year": "2021", "authors": "Ali H\u00fcrriyetoglu; Erdem Y\u00f6r\u00fck; Osman Mutlu; F\u0131rat Duru\u015fan; Deniz Agr\u0131 Yoltar; Burak Y\u00fcret;  G\u00fcrel"}, {"title": "Advances in social media research: Past, present and future", "journal": "Information Systems Frontiers", "year": "2018", "authors": "Kawal Kapoor; Kuttimani Tamilmani; Nripendra Rana; Pushp Patil; Yogesh Dwivedi; Sridhar Nerur"}, {"title": "Network in network", "journal": "", "year": "2014", "authors": "Min Lin; Qiang Chen; Shuicheng Yan"}, {"title": "", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Protest event analysis: A longitudinal analysis for Greece", "journal": "", "year": "2020", "authors": "Konstantina Papanikolaou; Haris Papageorgiou"}, {"title": "A generalizable nlp framework for fast development of pattern-based biomedical relation extraction systems", "journal": "BMC Bioinformatics", "year": "2013", "authors": "Yifan Peng; Manabu Torii; Cathy H Wu; K Vijay-Shanker"}, {"title": "How multilingual is multilingual BERT?", "journal": "", "year": "2019", "authors": "Telmo Pires; Eva Schlinger; Dan Garrette"}, {"title": "Social impact in social media: A new method to evaluate the social impact of research", "journal": "PLOS ONE", "year": "2018", "authors": "Cristina M Pulido; Gisela Redondo-Sama; Teresa Sord\u00e9-Mart\u00ed; Ramon Flecha"}, {"title": "The genius of earth day. Environmental", "journal": "History", "year": "2010", "authors": "Adam Rome"}, {"title": "May'68 and its Afterlives", "journal": "University of Chicago Press", "year": "2008", "authors": "Kristin Ross"}, {"title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "journal": "ArXiv", "year": "2019", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"title": "Twenty years of the kansas event data system project. The political methodologist", "journal": "", "year": "2006", "authors": "A Philip; Blake Schrodt;  Hall"}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "journal": "Journal of Machine Learning Research", "year": "2014", "authors": "Nitish Srivastava; Geoffrey E Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov"}, {"title": "Detecting mass protest through social media. Social media and society", "journal": "", "year": "2017", "authors": "S Wilson"}, {"title": "Protest activity detection and perceived violence estimation from social media images", "journal": "", "year": "2017", "authors": "Donghyeon Won; C Zachary; Jungseock Steinert-Threlkeld;  Joo"}, {"title": "Protest activity detection and perceived violence estimation from social media images", "journal": "CoRR", "year": "2017", "authors": "Donghyeon Won; Zachary C Steinert-Threlkeld; Jungseock Joo"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Precision (P), recall (R), and F1-Score of the models on the validation set; M(P), M(R), and M(F1) are the Macro averages of precision, recall, and F1-Score respectively", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Examples of the dataset indicating events of the past and not-events.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "System Architecture based on BERT(Devlin et al., 2019) ", "figure_data": "DenseEventNot-eventLayer(1 Neuron)Dropout.....DenseLayer(128Neurons)Global Average Pooling.....CT1T2TNPretrainedLanguageModel.....E[CLS]E1E2.....EN[CLS]Tok 1Tok 2.....Tok NFigure 1: Language English Spanish PortugueseNot-event18,6022,291901Event4,223450281Total22,2852,7411,182"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Parameters used for training the Models", "figure_data": "5 Results and AnalysisAll pretrained language models are fine-tuned inGoogle Colab 2 for ten epochs. We use the Tensor-flow implementation of the models 3 on the Hug-gingface transformers library 4 . We compare themacro F1-Scores of our fine-tuned models on thevalidation set, which were created by splitting thegiven dataset. The remaining split is the trainingdata. The validation set contains samples from allthree languages. It has 4,387 Not-event sequencesand 963 Event sequences making a combined totalof 5,350. The results are shown in Table3.We fine-tuned BERT, DistilBERT, and RoBERTamodels on the training set. We have combined the"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Macro F1-Scores on the Test Set One of the reasons for the poor performance of the models is the imbalance in the distribution of the classes. In the dataset, there are 21,794 Not-event sentences and only 4,954 Event ones. The models performed very well in the majority class and poorly in the minority class. Having more Event samples could have certainly helped the model in distinguishing better among the classes. Based on the performance of soft voting on the validation set, we have used the same for the test set. The results for the test set are shown in Table5. The reason for relatively low scores of Spanish and Portuguese could be due to the inadequate support of the training set (2,741 and 1,182) instead of", "figure_data": "three language corpora into a single corpus com-prising of all the three languages together. Themain intention towards using a multilingual modelis that the representations learnt during one lan-guage's pretraining would help the other. We canobserve that DistilBERT achieved a better F1-Scoreamong the models mentioned in the previous sec-tions. RoBERTa gave the lowest score among these.The reason could be that the RoBERTa model wasnot multilingual, unlike the other two; however, itstill managed to get a score very close to the BERTmodel. It is imperative that performing soft vot-ing on all three models has managed to increasethe score."}], "doi": "10.1080/02673843.2019.1669059"}