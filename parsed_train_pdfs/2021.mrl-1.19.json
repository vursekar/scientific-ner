{"authors": "Jongin Kim; Nayoung Choi; Seunghyun S Lim; Jungwhan Kim; Soojin Chung; Hyunsoo Woo; Min Song; Jinho D Choi", "pub_date": "", "title": "Analysis of Zero-Shot Crosslingual Learning between English and Korean for Named Entity Recognition", "abstract": "This paper presents a English-Korean parallel dataset that collects 381K news articles where 1,400 of them, comprising 10K sentences, are manually labeled for crosslingual named entity recognition (NER). The annotation guidelines for the two languages are developed in parallel, that yield the inter-annotator agreement scores of 91 and 88% for English and Korean respectively, indicating sublime quality annotation in our dataset. Three types of crosslingual learning approaches, direct model transfer, embedding projection, and annotation projection, are used to develop zero-shot Korean NER models. Our best model gives the F1-score of 51% that is very encouraging, considering the extremely distinct natures of these two languages. This is pioneering work that explores zero-shot crosslingual learning between English and Korean and provides rich parallel annotation for a core NLP task such as named entity recognition.", "sections": [{"heading": "Introduction", "text": "Crosslingual representation learning aims to derive embeddings for words (or sentences) from multiple languages that can be projected into a shared vector space (Conneau et al., 2018;Schuster et al., 2019b;Conneau and Lample, 2019). One important application of crosslingual embeddings has been found for transferring models trained on a high-resource language to a low-resource one (Lin et al., 2019;Schuster et al., 2019a;Artetxe and Schwenk, 2019). The latest multilingual transformer encoders such as BERT (Devlin et al., 2019) and XLM (Conneau et al., 2020) have made it possible to develop robust crosslingual models through zero-shot learning that requires no labeled training data on the target side (Jebbara and Cimiano, 2019;Chidambaram et al., 2019;Chi et al., 2020). However, these approaches tend not to work as well for languages whose words cannot be easily aligned. Our team is motivated to create a rich crosslingual resource between English and Korean, which are largely different in nature as English is known to be rigid-order, morphologically-poor, and head-initial whereas Korean is flexible-order, morphologicallyrich, and head-final (Choi et al., 1994;Han et al., 2002;Hong, 2009). Creation of a high quality parallel dataset to facilitate crosslingual research can reduce the gap between these two languages, and advance NLP techniques in both languages.\nThis paper provides a comprehensive analysis of crosslingual zero-shot learning in English and Korean. We first create a new dataset comprising a large number of parallel sentences and annotate them for named entity recognition (NER; Sec. 3). We then adapt the crosslingual approaches and build NER models in Korean through zeroshot learning (Sec. 4). All models are experimented on our dataset and thoroughly compared to evaluate the feasibility of this work (Sec. 5). Our results are promising although depicting few challenges in zero-shot learning for English and Korean (Sec. 6). The contributions of this work can be summarized as follows:\n\u2022 To create a crosslingual dataset that enables to develop robust zero-shot NER models in Korean.\n\u2022 To present a new data selection scheme that can notably improve zero-shot model performance.\n\u2022 To provide a comparative analysis among several crosslingual approaches and establish the initial foundation of this research.", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "Related Work", "text": "For crosslingual representation alignment, Artetxe et al. (2016) and Smith et al. (2017) suggested orthogonality constraints on the embedding transformation that led to better quality translation. Aldarmaki and Diab (2019) derived a context-aware crosslingual mapping from a parallel corpus using word alignment. Schuster et al. (2019b) aligned  word embeddings from multilingual transformer encoders using context independent embedding anchors. Recent works based on multilingual pretrained language model aligns representations between languages in a unsupervised fashion. Devlin et al. (2019) proposed multilingual BERT that generates contextualized word embeddings for multiple languages in one vector space by simply sharing all languages' vocabulary. Conneau and Lample (2019) extends mBERT by introducing bilingual data and an extra pretraining task (Translation Language Modeling). Luo et al. (2021) adds a crossattention module into the Transformer encoder to explicitly build the interdependence between langauges.\nFor cross-lingual NER, Ni et al. (2017) presented weakly supervised crosslingual models using annotation and representation projection. Huang et al. (2019) made an empirical analysis of how sequential order and multilingual embeddings are used in crosslingual NER. Artetxe and Schwenk (2019) presented multilingual transfer models that used few-shot learning adapting supervising BEA, ranking and retraining for massive transfer. Wu and Dredze (2019) and Wu et al. (2020) directly transfers the NER model trained on the source language to the target language using crosslingual representations from multilingual encoders (Direct model transfer).\n3 English-Korean Crosslingual Dataset 3.1 Data Collection AI Open Innovation Hub (AI Hub) is an integration platform operated by the Korea National Information Society Agency that provides data, software, and computing resources for AI research. It has released the Korean-English AI Training Text Corpus (KEAT) 1 containing 1.6M English-Korean parallel sentences from various sources such as news media, government website/journal, law & administration, conversation and etc. For the present study, 800K parallel sentences from the news portion of this corpus are extracted.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Data Preprocessing", "text": "Since KEAT is not organized into documents, each sentence is composed independently although it comes with the URL of its original source. Thus, we group all sentences into news articles based on the URLs. Although there exist news articles with single sentence after the grouping process, we still include them in the train set in order to make full use of the parallel sentences provided, which will be used to train the word alignment model and the transformation matrix in Section 5. As a result, 757,697 sentences are selected, that are composed into 381,173 news articles, to create our English-Korean crosslingual dataset.\nThe news articles can be categorized into 9 sections: Business, Lifestyle, Science/Technology, Society, Sports, World, Regional, and Others. Among those, 200 articles are randomly sampled from each of the first 7 categories for our annotation in Section 3.4 and they are split into 50/50 to create the development and test sets for our experiments in Section 5. Table 1 describes the statistics of our dataset. All sections are uniformly distributed in DEV and TST, enabling to conduct comparative studies among these sections.  Table 3: The statistics of manually annotated named entities on the parallel sentences in the DEV and TST sets. The numbers in the parentheses indicate the percentages of the corresponding tags for each set. EN/KR: # of entities in the English/Korean sentences respectively, E \u2229 K: # of entities existing in both English and Korean sentences.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Pseudo Annotation", "text": "ELIT 2 using the Flair model trained on OntoNotes (Pradhan et al., 2013). Korean sentences are tagged by a CRF-based model adapting KoBERT (Korean BERT) 3 trained on the corpus distributed by Cheon and Kim (2018). Note that the named entity types pseudo-annotated on the Korean sentences don't match with those of the English sentences for now, which will be matched in Section 3.4 in the case of DEV and TST. In addition, Korean sentences are processed by the Mecab morphological analyzer 4 that produces more linguistically sounding tokens than SentencePiece (Kudo and Richardson, 2018) in KoBERT. All named entities from the CRF tagger are then remapped to the tokens produced by the Mecab analyzer using heuristics so they can better reflect the previous morphology work in Korean (Hong, 2009). Words in every parallel sentence pair, tokenized by the ELIT and Mecab analyzers, are aligned by GIZA++, that has been adapted by many prior crosslingual studies (Och and Ney, 2003).\nTable 2 shows the statistics of pseudo-annotated named entities in our dataset. The detailed descriptions of these tags are provided in Appendix A.1. The overall statistics are comparable between English and Korean, 2.5 and 2.3 entities per sentence, respectively. GPE e , the 3rd most frequent tag in English, is not supported by the Korean tagger but rather tagged as ORG k or LOC k , explaining why the numbers of these two tags in Korean are much greater than those of ORG e and LOC e , respectively.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Parallel Annotation", "text": "We conduct a team of graduate students majoring in Data Science to manually tag named entities on all parallel sentences in the DEV and TST sets by taking the following 3 steps: 5 1. For English, the pseudo-annotated entities are revised by the OntoNotes named entity guidelines (BBN, 2014;Maekawa, 2018), and missing entities are annotated as necessary.\n2. For Korean, the pseudo-annotated entities are revised to match the English tagset, and missing entities are annotated as necessary.\n3. Let E = {e 1 , . . . , e n } and K = {k 1 , . . . , k m } be the lists of entities from Steps 1 and 2 for a English and Korean sentence pair, respectively. Every entity pair (e i , k j ) is linked in our dataset if e i is the translation of k j .\nNote that every article in DEV and TST consists of at least 5 sentences with at least 2 named entities.\nTable 3 shows the statistics of the gold annotation. Out of 22,367 and 21,892 named entities annotated in English and Korean sentences, 20,300 of them are linked across the languages (above 90%). To estimate the inter-annotator agreement, 10 news articles from each of the first 7 sections in Table 1 are randomly picked and double annotated; the rest of DEV and TST are single annotated and sample checked. Table 4 shows the Cohen's kappa scores measured for the English and Korean annotation. The high labeled matching scores of 90.9 and 88.3 are achieved for those two languages respectively, implying that the single annotation in this dataset is expected to be of high quality as well.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Analytics by Languages", "text": "A couple of challenges are found during the parallel annotation. First, subjects are obligatory in English for most sentence forms whereas Korean is a prodrop language so that entities in the subject position can be missing in Korean but not in English, which explains the greater number of entities in English. Second, certain inflectional morphemes in Korean can be dropped without violating the grammar, that often makes the labeling ambiguous. For instance, the literal translation of \"Korean Church\" would be \"\ud55c\uad6d(Korea)+\uc758('s) \uad50\ud68c(Church)\", although it is the standard practice to drop \"\uc758('s)\" in this case such that it becomes \"\ud55c\uad6d(Korea) \uad50\ud68c(Church)\". Given this translation, the annotator can be easily confused to annotate \"\ud55c\uad6d(Korea)\" as a geopolitical entity (GPE) instead of a nationality (NORP), which may lead to annotation disagreement. Additional analytics by news sections and entity types are described in Appendix A.6 4 Zero-shot Crosslingual Learning", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Overview of Approaches", "text": "Three crosslingual learning approaches are adapted to develop zero-shot Korean models. One is direct model transfer method following Wu and Dredze (2019). We reproduce the previous work which finetunes mBERT on English NER dataset and transfers the trained model to a target language, in our case, Korean. We fine-tune on OntoNotes, whereas the previous work fine-tuned on CoNLL 2003 NER dataset. The other two approaches that will be experimented are embedding preojection and annotation projection following Ni et al. (2017), although some modules in the implementation are updated or added: the encoders used to derive the embeddings from sentences, the word alignment tool, the training data selection scheme heuristics. Figure 1 illustrates an overview of two crosslingual learning approaches adapted to develop zero-shot Korean NER models. One is embedding projection (R1) that takes a labeled English sentence (R2) and generates English embeddings, (R3) which are fed into an orthogonal mapping (R4) then transformed into Korean embeddings (Section 4.2). The other is annotation projection (A1) that aligns words across the two languages and pseudo-annotates the Korean sentence, (A2) which are fed into an encoder (A3) to generate Korean embeddings (Section 4.3). The Korean embeddings generated by individual approaches are fed into a trainer to build the Korean NER models. No manual annotation is added to the Korean data; thus they both are zero-shot learning. ", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Embedding Projection", "text": "Let X, Y \u2208 R n\u00d7d be parallel matrices between the source and target languages, where n is the number of parallel terms (words or sentences) in those two languages. Let x i , y i \u2208 R 1\u00d7d be the i'th rows in X and Y , which are the embeddings of the i'th terms in the source and target languages respectively, that refer to the same content. Then, the transformation matrix W \u2208 R d\u00d7d can be found by minimizing the distance between XW and Y as follows:\nargmin W XW \u2212 Y s.t. W T W = I\nThis optimization can be achieved by singular value decomposition as proposed by Artetxe et al. (2016), where\nU, V \u2208 R d\u00d7p , \u03a3 \u2208 R p\u00d7p : W = U V T s.t. X T Y = U \u03a3V T\nThe transformation matrix W is used to convert any English embedding e i into a Korean embedding k i in Figure 1 such that e i \u2022 W = k i \u2248 k j where k j is the embedding from the Korean encoder that can be aligned with e i . The NER model is trained on only English sentences represented by the transformed embeddings k * and a pseudo-label annotated with an existing English NER model. During decoding, the model takes Korean sentences represented by the encoded embeddings k * and makes the predictions.\nGiven the latest contextualized encoders that generate different embeddings for the same word type by contexts (Peters et al., 2018;Devlin et al., 2019;Liu et al., 2019), the size of X and Y is as large as the number of all aligned words in the training data. It is worth saying that the transformed embedding space may be similar to the actual encoded space in the target language; however, the word order is still preserved as in the source language. Therefore, the model is limited to learn sequence information of the target language, which can be an issue for languages with very different word orderings.", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "Annotation Projection", "text": "Let S = {S 1 , . . . , S n } and T = {T 1 , . . . , T n } be lists of sentences in the source and target languages, and (S i , T i ) be the i'th pair of parallel sentences in those two languages. Let S i = {s i1 , . . . , s in } and T i = {t i1 , . . . , t im } where s i and t i are the i'th word in S and T . Then, annotation projection can be performed as proposed by Ni et al. (2017):\n1. Pseudo-label S \u2200i \u2208 S using an existing model in the source language, in our case, ELIT ( \u00a73.2).\n2. Pseudo-align words in every (S i , T i ) using an existing tool, in our case, GIZA++ ( \u00a73.2). If a consecutive word span S j,k i = {s ij , .., s ik } is pseudo-labeled as the entity type as well as pseudo-aligned with a span T a,b i = {t ia , .., t ib }, T a,b i is also pseudo-labeled with .\nThe quality of pseudo annotation hugely depends on the performance of word alignment, which is generally not robust for the case of distant language pairs such as English and Korean. Thus, we propose a few constraints to filter out noisy annotation.\nEntity Matching Let \u03c8 be a boolean. If \u03c8 = F, all parallel sentences in (S, T) are used for training. If \u03c8 = T, (S i , T i ) is selected for training only if all named entities in S i are properly labeled in T i by the above projection approach.\nRelative Frequency Let e be an entity term such as \"\ub3c4\ub110\ub4dc \ud2b8\ub7fc\ud504 (Donald Trump)\" in Figure 1.\nLet L e be a set of entity types pseudo-annotated for all occurrences of e in the target language. Then, the relative frequency P ( |e) for \u2208 L e and e can be measured as follows, where COUNT( , e) is the number of occurrences for e being labeled as :\nP ( |e) = COUNT( , e)\n\u2208Le COUNT( , e)\nImpurity Let F e be a set of unique terms in the source language that are pseudo-aligned with the term e labeled as in the target language such that |F e | \u2264 COUNT(l, e). Then, the impurity M ( , e) is measured as follows where \u03b1 is a smoothing factor:\nM ( , e) = |F e | COUNT(l, e) + \u03b1\nThe relative frequency P ( |e) and the impurity M ( , e) are used to assess pseudo-annotation reliability.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Ann. Reliability Let", "text": "E i = {( 1 , e 1 ), .., ( q , e q )}\nbe a list of all (entity term, label) pairs in the target sentence T i . For each T i \u2208 T, the following two scores, f (T i ) and g(T i ), are measured to estimate the reliability of the pseudo-annotation in T i :\nf (T i ) = \u2200(l,e)\u2208E i P ( |e) |E i | g(T i ) = \u2200(l,e)\u2208E i M ( |e) |E i |\nGiven the annotation reliability metrics, our data selection scheme heuristic is as follows:\nf (T i ) \u2265 \u03c6; g(T i ) \u2264 \u03b3; |E i | \u2265 \u00b5; \u03c8 = T|F\nOnly the target sentences satisfying all of the above constraints are used for training given the hyperparameters \u03c8, \u03b1, \u03c6, \u03b3, and \u00b5.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Direct Model Transfer", "text": "The experimental settings of direct model transfer approach are identical with Wu and Dredze (2019). We freeze the bottom n layers (including n) of mBERT, where layer 0 is the embedding layer.\nThe cases of n are {-1, 0, 3, 6, 9}, where -1 denotes fine-tuning all layers in mBERT. For word-level classification, a simple linear classification layer with softmax is added on mBERT. The hyperparameters we experiment on are the combitations of batch size {16, 32}, learning rate {2e-5, 3e-5, 5e-5}, and number of max epochs {3, 4}.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Multilingual Encoders", "text": "For embedding projection and annotation projection, two types of transformer encoders, mBERT (Devlin et al. 2019) ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Embedding Projection", "text": "Two types of transformation matrices are derived by the embedding projection method (Section 4.2).\nOne is a word-level matrix and the other is a sentence-level matrix. To evaluate the zero-shot Korean NER model performance (Table 6) when different size of parallel sentences are available, we use different subsets of sentences of increasing sizes(0, 1K, 10K, 100K, 200K, 400K, 747K; 0 to total # of sentences in TRN). Size 0 means the embeddings from source language are not transformed when fed into the NER model for training. Word embeddings from the last hidden layer of each transformer encoder are extracted. For every parallel sentence pair, let X i and Y i be lists of word embeddings of the i'th sentence extracted from the last layer in the source and target encoders, respectively. Only embeddings for words that find alignments are included in X i and Y i . If multiple words in the source language, s i and s j , are aligned to one word, t k , in the target language (e.g., United States \u2192 \ubbf8\uad6d in Figure 1), the embeddings of t k are duplicated and added to Y i and vice versa s.t. |X * | = |Y * |. Let x ij and y ij be the j'th embeddings in X i and Y i that are guaranteed to be the embeddings of aligned words; thus, X i and Y i are completely in parallel.\nFor the word-level transformation matrix W w , X i and Y i from all parallel sentences are appended together to create X w i and Y w i respectively such that X w i \u2022 W w \u2248 Y w i . Sentence embeddings are simply created by averaging the word embeddings of parallel source and target sentences. Let X i and Y i be lists of word embeddings of the i'th sentence extracted from the last layer in the source and target encoders. Note that words in X i and Y i are not aligned, thus no duplications of word embedding unlike X i and Y i . For the sentence-level matrix W s , the average em-beddings of X i , Y i are appended to create X s i and Y s i such that X s i \u2022 W s \u2248 Y s i . For each sentence in the source language, embeddings from last hidden layer are transformed by W w|s and fed into the NER model for training (Section 5.5).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Annotation Projection", "text": "The annotation projection is performed to generate the pseudo-annotated Korean dataset (Section 4.3). The following 5 hyperparameters are tuned to filter out noisy annotation for training, where \u03c8, \u03b1, and \u03b3 are newly introduced by our work:\n\u2022 \u03c8: if True, keep only sentences whose entities are completely matching between the two languages.\n\u2022 \u03b1: the smoothing factor to measure the impurity.\n\u2022 \u03c6: retain sentences whose annotation reliability scores by relative frequency \u2265 this threshold.\n\u2022 \u03b3: retain sentences whose annotation reliability scores by impurity \u2264 this threshold.\n\u2022 \u00b5: retain sentences that contain named entities whose quantities are \u2265 this cutoff.\nOnce the pseudo-annotation is created, all Korean sentences are encoded by mBERT to generate Korean embeddings that are fed into the NER model.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "NER Model", "text": "For embedding and annotation projections, a bidirectional LSTM-based NER tagger using a CRF decoder is adapted to build our NER models (Lample et al., 2016). Details of the hyperparameters are described in Appendix A.3", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Results", "text": "Table 5 shows the best result of direct model transfer, mBERT fine-tuned on OntoNotes NER dataset and evaluated on our Korean TST set. All scores are reported in a form of mean (\u00b1 standard deviation) after three developments. The best model is built under the setting when all layers including the embedding layer of mBERT are fine-tuned.    6 shows the zero-shot results from the embedding projection models in Section 5.3. Both mBERT and XLM-R models showed a performance improvement over 2% with embedding transformation. F1 score improves 2.47% (36.67% to 39.14%) and 2.32% (39.36% to 41.68%) for mBERT and XLM-R, respectively. Both models showed the best performance with embedding transformation matrix made of 200k w . The number of parallel sentences used for training transformation matrix has a considerable impact on the Zero-shot learning.  Table 7 shows the results from the annotation projection models with various configurations. About 9% gain is shown by the best model using only the entity matching constraint \u03c8 that effectively filters out 55% of the training data (row 3). A relative score of 50.25% is achieved by the model using only 21% of the training data, implying that a fair amount of noisy annotation is produced by the annotation projection approach.\nThe overall results show that the Annotation Projection approach achieves the best performance,  implying that considering the word order of the target language is critical in cross-lingual learning, especially in the case of distant language pairs. We expect further improvement of the annotation projection approach when adapting a more accurate word alignment tool or a data selection scheme, which we will further investigate.\n6 Analysis", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Error Distribution", "text": "Given the results of the best models for embedding and annotation projection approaches (Section 5.6), a total of 105 parallel sentences (15 pairs per news section) are randomly selected for error analysis. 7 Table 8 shows the distributions of the 5 error types.   error types in both models. For example, the Korean entity \"10\uac1c (10 things)\" comprises the quantity \"10\" and the metric \"\uac1c (things)\" that is a generic measure word in Korean, whereas in English just write \"10\". The grammatical difference between English and Korean, where Korean uses measure words for quantifying the classes of objects while English does not in general, makes it difficult to accurately predict under the zero-shot learning setting. Wrong Label occurs frequently across all models when dealing with entities referring to nationality. As mentioned in Section 3.5, a single word in Korean can entail the meaning of both nationality and country. This overloaded word-sense characteristic makes entities that actually refer to nationality be mislabeled as GPE, which should have been labeled as NORP.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Error Analysis", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "OR G D AT E P E R S ON C AR D I NAL MONE Y OR I D NAL GP E E VE NT P E R C E NT", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "This paper presents a multilingual dataset that allows researchers to conduct crosslingual research between English and Korean. Our dataset contains high-quality annotation of named entities on parallel sentences from seven popular news sections. Given this dataset, Korean NER models are built by zero-shot learning using multilingual encoders. Our data selection scheme for annotation projection significantly improves the NER performance although it is still suboptimal. Our error analysis depicts unique characteristics in Korean that make it hard for zero-shot learning, challenges that we need to overcome in the future work. 8", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Appendix", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 Named Entity Tagsets", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1.1 English NER Tagset", "text": "There are 18 named entity tags annotated in the OntoNotes 5.0 as follows (Pradhan et al., 2013): 9\n\u2022 CARDINAL: Numerical terms not categorized in other categorizations. Numbers that indicate ages are included.\n\u2022 DATE: Absolute or relative dates or periods. The period should last longer than 'TIME'. General expressions of dates are included too such as 'few months', 'that day', 'Next season' and 'First quarter'.\n\u2022 EVENT: It means an official or widely known event, war, exhibition. Official events include ministerial meetings, general elections, presidential elections, exams (SAT), and prayers (U.S. national breakfast prayer). Social phenomena also include (Brexit) for widely known events.\n\u2022 FAC: Objectives referring to facilities include buildings, airports, highways and bridge names.\n\u2022 GPE: An object referring to a place or location, including the name of a country and the name of an administrative district, such as a city or state.\n\u2022 LANGUAGE: Any named language.\n\u2022 LAW: Named documents made into laws.\n\u2022 LOC: Refers to the name of a place or location that does not belong to GPE. It also includes expressions covering the entire location of mountains, rivers, ocean names and Europe, Asia, etc.\n\u2022 MONEY: Monetary values including units.\n\u2022 NORP: It refers to nationality, religious groups and political groups (party).\n\u2022 ORDINAL: All ordinal numbers such as first and second.\n\u2022 ORG: It refers a community / group of people gathered together. For example, the name of the company, the name of the school, and the name of the sports team.\n9 https://catalog.ldc.upenn.edu/docs/ LDC2013T19/OntoNotes-Release-5.0.pdf\n\u2022 PERCENT: Percentage expressions with % symbol or the word 'percent'.\n\u2022 PERSON: Referring to a last name or full name of a particular person. It also includes nicknames for non-human creatures and characters in cartoons, dramas and movies.\n\u2022 PRODUCT: Vehicles, Weapons, foods. IT services (including SNS) and medicine names are included.\n\u2022 QUANTITY: Measurements as of weights or distances such as km, kg and etc.\n\u2022 TIME: This tag indicates time expressions smaller than a day. This tag includes certain time indication, amount of time or any other expressions related to time. Even though an entity does not have numeral expressions but only words related to time (for instance, 'noon'), the words are tagged as 'Time'.\n\u2022 WORK_OF_ART: Titles of books, songs, TV programs and art pieces. Title of games, awards, theories, records are included.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1.2 Korean NER Tagset", "text": "There are 10 tags annotated in the copus distributed by the Korea Maritime and Ocean University: 10\n\u2022 DAT: Absolute dates. Public holidays and day of the week is included.\n\u2022 DUR: Duration of incidents. Academically clarified periods such as Cretaceous period are also included.\n\u2022 LOC: The name of a country and the name of an administrative district, such as a city or state. Words representing certain locations such as tour spot and stadium is also included. When location word becomes compound nouns with other words, it is not included.\n\u2022 MNY: Monetary values including units. Bitcoin is not included.\n\u2022 NOH: Any numerical expressions such as measurements of heights, temperatures, weights. Ordinal numbers are included.\n\u2022 ORG: A group consisting of 2 or more people. The name of the company, the name of the school, and the name of the sports team.\n\u2022 PER: Personal name including first and last name. Any name referring to living things and nicknames for non-human creatures and characters in cartoons, dramas and movies are included.\n\u2022 PNT: Percentage expressions with % symbol or the word 'percent'.\n\u2022 POH: Product name, medicine, game, event, meeting, movies, songs, drama series, TV channels, daily and weekly magazines, emails, phone numbers are included.\n\u2022 TIM: This tag indicates time expressions smaller than a day. This tag includes certain time indication, amount of time or any other expressions related to time. Even though an entity does not have numeral expressions but only words related to time (for instance, 'noon'), the words are tagged as 'Time'.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.2 English NER Performance", "text": "Table 9 shows the performance of the NER model in the ELIT toolkit on the English development and evaluation sets in our dataset.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.3 Experimental Settings", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.3.1 Task specific NER model for Embedding and Annotation Projections", "text": "The task specific NER model used : a 2-layer Bi-LSTM with a hidden size of 768 followed by a CRF layer. A dropout rate of 0.5 is applied on the input and the output of the Bi-LSTM. Adam with default parameters and a learning rate of 0.0001 are used for optimization. We trained the model for 10 epoch with a batch size of 32, and evaluate the model per a epoch.\nA.4 Comparison of NER performances (Zero-shot VS Existing)\nWe compare our best performing Zero-shot Korean NER model with the existing Korean NER model 11 on TST.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.4.1 Comparison Method", "text": "Since the number of named entity types that each model covers are different, named entity tags are mapped based on the definition of the tags. Our named entities are more fine-grained, which makes multiple tags (Zero-shot side) be mapped to one tag (Existing side). Named entity tags that cannot be mapped are discarded in both gold labels and predicted labels, thus not considered in the evaluation of the models.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.4.2 Comparison Result", "text": "Our zero-shot model yields a better performance than the existing model although it may be difficult to directly compare the two models. In the case of the existing Korean model, the low performance may be caused by the different annotation scheme between the datasets. In the case of our zero-shot model, the improvement of the performance are seen due to the coarse-grained named entities after the mapping.  Table 11b describes the proportions of news sections per entity type. CARDINAL, ORDINAL, and EVENT appear the most in Sports that involves many game events and statistics. DATE, ORG, and QUANTITY show fairly even proportions in every section as they are elemental to a variety of topics. GPE, LOC, and NORP give high proportions to both Politics and World. MONEY and PERCENT appear the most in Business that often deals with monetary issues. PERSON show high proportions in Sports and Politics as discussed above. FAC takes good portions in Lifestyle, Business, and World, which often mention facilities that people encounter daily (e.g., airports, bridges). TIME appears the most in Sports and Society that are full of dynamic events and issues. LANGUAGE is mostly found in Society although the sample size is too small to generalize. LAW, PRODUCT, and WORK_OF_ART appear the most in Politics, Sci/Tech, and Lifestyle, that focus on legal issues, tech products, and entertainment (e.g., music, movies, shows), respectively.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "This work was partly supported by the Institute of Information and Communications Technology Planning and Evaluation (IITP) grant funded by the Korean government (MSIT) (No. 2020-0-01361, Artificial Intelligence Graduate School Program (Yonsei University)).", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Context-Aware Cross-Lingual Mapping", "journal": "", "year": "2019", "authors": "Hanan Aldarmaki; Mona Diab"}, {"title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance", "journal": "", "year": "2016", "authors": "Mikel Artetxe; Gorka Labaka; Eneko Agirre"}, {"title": "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond", "journal": "", "year": "2019", "authors": "Mikel Artetxe; Holger Schwenk"}, {"title": "OntoNotes Named Entity Guidelines Version 14.0. Raytheon BBN Technologies", "journal": "", "year": "2014", "authors": " Bbn"}, {"title": "Definition of Korean Named-Entity Task", "journal": "", "year": "2018", "authors": "Min-Ah Cheon; Jae-Hoon Kim"}, {"title": "Finding Universal Grammatical Relations in Multilingual BERT", "journal": "", "year": "2020", "authors": "Ethan A Chi; John Hewitt; Christopher D Manning"}, {"title": "Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model", "journal": "", "year": "2019", "authors": "Muthu Chidambaram; Yinfei Yang; Daniel Cer; Steve Yuan; Yunhsuan Sung; Brian Strope; Ray Kurzweil"}, {"title": "KAIST Tree Bank Project for Korean: Present and Future Development", "journal": "", "year": "1994", "authors": " Key-Sun; Young S Choi; Young G Han; Oh W Han;  Kwon"}, {"title": "Unsupervised Cross-lingual Representation Learning at Scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Crosslingual Language Model Pretraining", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Alexis Conneau; Guillaume Lample"}, {"title": "XNLI: Evaluating Cross-lingual Sentence Representations", "journal": "", "year": "2018", "authors": "Alexis Conneau; Ruty Rinott; Guillaume Lample; Adina Williams; Samuel Bowman; Holger Schwenk; Veselin Stoyanov"}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Penn Korean Treebank: Development and Evaluation", "journal": "", "year": "2002", "authors": "Chung-Hye Han; Na-Rae Han; Eon-Suk Ko; Martha Palmer; Heejong Yi"}, {"title": "21st Century Sejong Project Results and Tasks (21\uc138\uae30 \uc138\uc885 \uacc4\ud68d \uc0ac\uc5c5 \uc131\uacfc \ubc0f \uacfc \uc81c). In New Korean Life (\uc0c8\uad6d\uc5b4\uc0dd\ud65c). National Institute of Korean Language", "journal": "", "year": "2009", "authors": "Yoonpyo Hong"}, {"title": "What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical Analysis", "journal": "", "year": "2019", "authors": "Xiaolei Huang; Jonathan May; Nanyun Peng"}, {"title": "Zero-Shot Cross-Lingual Opinion Target Extraction", "journal": "Long and Short Papers", "year": "2019", "authors": "Soufian Jebbara; Philipp Cimiano"}, {"title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing", "journal": "", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"title": "Neural Architectures for Named Entity Recognition", "journal": "", "year": "2016", "authors": "Guillaume Lample; Miguel Ballesteros; Sandeep Subramanian; Kazuya Kawakami; Chris Dyer"}, {"title": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "journal": "", "year": "", "authors": ""}, {"title": "Antonios Anastasopoulos, Patrick Littell, and Graham Neubig", "journal": "", "year": "2019", "authors": "Yu-Hsiang Lin; Chian-Yu Chen; Jean Lee; Zirui Li; Yuyan Zhang; Mengzhou Xia; Shruti Rijhwani; Junxian He; Zhisong Zhang; Xuezhe Ma"}, {"title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv", "journal": "", "year": "1907", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "VECO: Variable and flexible cross-lingual pretraining for language understanding and generation", "journal": "Long Papers", "year": "2021", "authors": "Fuli Luo; Wei Wang; Jiahao Liu; Yijia Liu; Bin Bi; Songfang Huang; Fei Huang; Luo Si"}, {"title": "Annotation guidelines for named entities version 1.1. Technical report, Search Results Web results National Institute of Information and Communications Technology", "journal": "", "year": "2018", "authors": "Emi Maekawa"}, {"title": "Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection", "journal": "Long Papers", "year": "2017", "authors": "Jian Ni; Georgiana Dinu; Radu Florian"}, {"title": "A systematic comparison of various statistical alignment models", "journal": "Computational linguistics", "year": "2003", "authors": "Josef Franz; Hermann Och;  Ney"}, {"title": "Deep Contextualized Word Representations", "journal": "Long Papers", "year": "2018", "authors": "Matthew Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"title": "Towards Robust Linguistic Analysis using OntoNotes", "journal": "", "year": "2013", "authors": "Alessandro Sameer Pradhan; Nianwen Moschitti; Hwee Tou Xue; Anders Ng; Olga Bj\u00f6rkelund; Yuchen Uryupina; Zhi Zhang;  Zhong"}, {"title": "Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog", "journal": "Long and Short Papers", "year": "2019", "authors": "Sebastian Schuster; Sonal Gupta; Rushin Shah; Mike Lewis"}, {"title": "Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing", "journal": "", "year": "2019", "authors": "Tal Schuster; Ori Ram; Regina Barzilay; Amir Globerson"}, {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "journal": "CoRR", "year": "2017", "authors": "L Samuel;  Smith; H P David; Steven Turban; Nils Y Hamblin;  Hammerla"}, {"title": "Enhanced meta-learning for cross-lingual named entity recognition with minimal resources", "journal": "", "year": "2020", "authors": "Qianhui Wu; Zijia Lin; Guoxin Wang; Hui Chen; F B\u00f6rje; Biqing Karlsson; Chin-Yew Huang;  Lin"}, {"title": "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT", "journal": "", "year": "2019", "authors": "Shijie Wu; Mark Dredze"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The overview of crosslingual methods, embedding projection ( \u00a74.2) and annotation projection ( \u00a74.3). The blue and red coded words and embeddings represent the PERSON and GPE entities, respectively. AP:Annotation Projection, EP:Embedding Projection", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Comparison of entity type distribution of Wrong Range between EP and AP. Only entity types that have errors over 2 times are included in the chart.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Comparison of entity type pair distribution of Wrong Label between EP and AP. Only entity type pairs that have errors over 2 times are included in the chart.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Business Lifestyle Politics Sci/Tech Society Sports World Regional Others Total The number of parallel sentences by each category. The number of parallel news articles by each category.", "figure_data": "TRN109,464132,60692,87755,420 164,414 52,214 70,48757,49012,881747,853DEV666733728718711715763--5,034TST676709687680686650722--4,810Total110,806134,04894,29256,818 165,811 53,579 71,97257,49012,881757,697(a) Business Lifestyle Politics Sci/Tech Society Sports World Regional OthersTotalTRN65,90163,13141,23327,92476,450 34,662 29,76534,5106,197379,773DEV100100100100100100100--700TST100100100100100100100--700Total66,10163,33141,43328,12476,650 34,862 29,96534,5106,197381,173(b)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The statistics of our English-Korean dataset. TRN/DEV/TST: the training/development/evaluation sets.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "English sentences are tokenized and tagged with named entities by the open-source NLP tool called The number of pseudo-annotated named entities in the English sentences. WOA: WORK_OF_ART.", "figure_data": "CARDNIALeDATEeEVENTeFACeGPEeLANGUAGEeLAWeLOCeMONEYeTRN207,618 (10.6) 281,936 (14.5)29,548 (1.5) 39,579 (2.0) 315,014 (16.2)3,050 (0.2)11,821 (0.6) 27,865 (1.4) 26,751 (1.4)DEV1,314 (11.6)1,534 (13.6)139 (1.2)146 (1.3)1,845 (16.3)21 (0.2)32 (0.3)151 (1.3)123 (1.1)TST1,220 (11.1)1,552 (14.1)155 (1.4)151 (1.4)1,751 (15.9)29 (0.3)50 (0.5)154 (1.4)136 (1.2)Total210,152 (10.7) 285,022 (14.5)29,842 (1.5) 39,876 (2.0) 318,610 (16.2)3,100 (0.2)11,903 (0.6) 28,170 (1.4) 27,010 (1.4)NORPeORDINALeORGePERCENTePERSONePRODUCTeQUANTITYeTIMEeWOAeTRN68,700 (3.5)82,270 (4.2) 394,226 (20.2)8,339 (0.4) 352,918 (18.1)12,170 (0.6)16,736 (0.9) 37,003 (1.9) 35,193 (1.8)DEV503 (4.4)517 (4.6)2,286 (20.2)61 (0.5)2,168 (19.2)70 (0.6)62 (0.6)148 (1.3)203 (1.8)TST513 (4.7)443 (4.0)2,264 (20.6)52 (0.5)2,049 (18.6)91 (0.8)47 (0.4)156 (1.4)205 (1.9)Total69,716 (3.5)83,230 (4.2) 398,776 (20.2)8,452 (0.4) 357,135 (18.1)12,331 (0.6)16,845 (0.9) 37,307 (1.9) 35,601 (1.8)(a) DATkDURkLOCkMNYkNOHkORGkPERkPNTkPOHkTIMkTRN156,013 (9.0) 41,651 (2.4) 235,179 (13.6) 37,538 (2.2) 285,898 (16.5) 478,830 (27.6) 312,578 (18.0) 37,767 (2.2) 136,731 (7.9) 12,894 (0.7)DEV752 (7.4)267 (2.6)959 (9.5)159 (1.6)1,807 (17.9)3,231 (32.0)1,909 (18.9)220 (2.2)748 (7.4)49 (0.5)TST741 (7.5)257 (2.6)947 (9.6)174 (1.8)1,626 (16.5)3,142 (31.9)1,818 (18.5)249 (2.5)850 (8.6)40 (0.4)Total157,506 (9.0) 42,175 (2.4) 237,085 (13.5) 37,871 (2.2) 289,331 (16.5) 485,203 (27.6) 316,305 (18.0) 38,236 (2.2) 138,329 (7.9) 12,983 (0.7)(b) The number of pseudo-annotated named entities in the Korean sentences."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The statistics of pseudo-annotated named entities by each tag in our English-Korean dataset. The numbers in the parentheses indicate the percentages of the corresponding tags for each set.", "figure_data": "CARDNIALDATEEVENTFACGPELANGUAGELAWLOCMONEYEN1,235 (10.9) 1,496 (13.2)190 (1.7)160 (1.4) 1,755 (15.5)25 (0.2)39 (0.3) 150 (1.3) 156 (1.4)KR1,359 (12.3) 1,381 (12.5)186 (1.7)158 (1.4) 1,674 (15.1)21 (0.2)39 (0.4) 141 (1.3) 159 (1.4)E \u2229 K1,124 (10.9) 1,324 (12.9)173 (1.7)154 (1.5) 1,566 (15.2)21 (0.2)34 (0.3) 135 (1.3) 156 (1.5)NORPORDINALORGPERCENTPERSONPRODUCTQUANTITYTIMEWOAEN625 (5.5)461 (4.1) 2,217 (19.6)191 (1.7) 2,118 (18.7)157 (1.4)65 (0.6) 146 (1.3) 147 (1.3)KR540 (4.9)351 (3.2) 2,249 (20.3)199 (1.8) 2,129 (19.2)155 (1.4)67 (0.6) 140 (1.3) 142 (1.3)E \u2229 K529 (5.1)329 (3.2) 2,042 (19.8)189 (1.8) 2,048 (19.9)145 (1.4)65 (0.6) 132 (1.3) 131 (1.3)(a) Statistics of the development set (DEV).CARDNIALDATEEVENTFACGPELANGUAGELAWLOCMONEYEN1,117 (10.1) 1,511 (13.7)207 (1.9)161 (1.5) 1,701 (15.4)29 (0.3)57 (0.5) 154 (1.4) 165 (1.5)KR1,253 (11.6) 1,406 (13.0)205 (1.9)159 (1.5) 1,635 (15.1)26 (0.2)52 (0.5) 147 (1.4) 172 (1.6)E \u2229 K1,018 (10.2) 1,336 (13.4)196 (2.0)151 (1.5) 1,517 (15.2)25 (0.3)51 (0.5) 137 (1.4) 164 (1.6)NORPORDINALORGPERCENTPERSONPRODUCTQUANTITYTIMEWOAEN621 (5.6)397 (3.6) 2,159 (19.6)215 (2.0) 2,012 (18.2)172 (1.6)52 (0.5) 148 (1.3) 156 (1.4)KR509 (4.7)287 (2.7) 2,196 (20.3)223 (2.0) 2,017 (18.7)176 (1.6)51 (0.5) 133 (1.2) 155 (1.4)E \u2229 K501 (5.0)274 (2.7) 1,980 (19.8)214 (2.1) 1,955 (19.5)161 (1.6)50 (0.5) 132 (1.3) 141 (1.4)(b) Statistics of the evaluation set (TST)."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "English KoreanUnlabeled92.790.4Labeled90.988.3"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "and XLM-RoBERTa (Conneau et al. 2020) are considered. Both mBERT and XLM-R are further pretrained on the training data (TRN in Table1) by individually feeding 1.5M sentences in both English and Korean.6    ", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The best result of the baseline model on TST", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Zero-shot NER results on TST using the embedding projection models in \u00a75.3. mBERT/XLM-R w|s : the English embeddings from mBERT/XLM-R are transformed by W w|s . 0 means not transformed.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Zero-shot NER results on TST using the annotation projection models in \u00a75.4. T: number of parallel sentence pairs used for training.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Distributions of 5 error types from the samples.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Performance of the English NER model.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "NER performances of Zero-shot Korean model and Existing Korean model on TSTNo Extraction occurs when the model does not extract words as a named entity although they are annotated in the gold data.Gold: \uad6d\ud68c (National Assembly) ORG Auto: \uad6d\ud68c (National Assembly) Analytics by News Sections and Entity TypesTable 11a describes the proportions of entity types per news section on DEV and TST combined. For Lifestyle, Politics, and Sports, PERSON is the most frequent entity type since many topics in these sections are centered around famous people (e.g., someone's biography, politicians, sports players). ORG on the other hand is the most frequent entity type for Business, Society, and Sci/Tech although those entities in Society refer to social groups while they generally refer to industrial companies in the other sections. GPE is the most and the 2nd-most frequent entity types for World and Politics where these entities refer to countries or regions in World but they are often related to geographical relationships between political figures in Politics.", "figure_data": "A.5 NER Error TypesUnderlined words indicate entity boundaries, fol-lowed by their TAGs:No Annotation occurs when the model ex-tracts non-entity words as a named entity althoughthey are not annotated in the gold data.Gold: \uc2e0\ud765\uad6d (Emerging Country)Auto:: \uc2e0\ud765\uad6d (Emerging Country) ORG"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Distribution comparisons between manually annotated entities and news sections in the English dataset (DEV and TST combined). Numbers in the parentheses indicate the percentages of the corresponding tags.", "figure_data": ""}], "doi": "10.18653/v1/W19-4330"}