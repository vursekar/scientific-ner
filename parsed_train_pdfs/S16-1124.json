{"authors": "Miloslav Konop\u00edk; Ond\u0159ej Pra\u017e\u00e1k; David Steinberger; Tom\u00e1\u0161 Brychc\u00edn", "pub_date": "", "title": "UWB at SemEval-2016 Task 2: Interpretable Semantic Textual Similarity with Distributional Semantics for Chunks", "abstract": "We introduce a system focused on solving SemEval 2016 Task 2 -Interpretable Semantic Textual Similarity. The system explores machine learning and rule-based approaches to the task. We focus on machine learning and experiment with a wide variety of machine learning algorithms as well as with several types of features. The core of our system consists in exploiting distributional semantics to compare similarity of sentence chunks. The system won the competition in 2016 in the \"Gold standard chunk scenario\". We have not participated in the \"System chunk scenario\".", "sections": [{"heading": "Introduction", "text": "The goal of the Interpretable Semantic Textual Similarity task is to go deeper with the assessment of semantic textual similarity of sentence pairs. It is requested to add an explanatory layer that offers a deeper insight into the sentence similarities. The sentences are split into chunks and the first goal is to find corresponding chunks (with respect to their meanings) among the compared sentences. When the corresponding chunks are known, the chunks are annotated with their similarity scores and their relation types (e.g. equivalent, more specific, etc).\nThe task follows a pilot task from the preceding SemEval 2015 competition (Agirre et al., 2015). The best performing systems adopted various approaches, (Banjade et al., 2015) relied on handcrafter rules, (Karumuri et al., 2015) employed a classifier for relation types and they associated each relation with a precomputed similarity score and (H\u00e4nig et al., 2015) extended their word alignment algorithm for the task.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Math notation", "text": "The data consist of sentence pairs S a i and S b i , where a denotes the first item of the pair, b denotes the second item of the pair and i indexes the sentences (for simiplicity we further omit i for sentences). We perceive a sentence S a to be an ordered set of chunks CH a j \u2208 S a and the chunks to be ordered sets of words w k \u2208 CH a j (and analogically for sentence S b ).\nNext we define two functions: sim(CH a i , CH b j ) \u2208 {0, 1, 2, 3, 4, 5} for chunk similarity and rel(CH a i , CH b j ) \u2208 TYPE for chunk relation type.\nThe possible types are: TYPE = {EQUI, OPPO, SPE1, SPE2, SIMI, REL}. These are the main types. All these types can have two modifiers (FACT, POL). The modifiers are optionally attached to the main types. For example, you can generate SPE1 FACT. For more information, please see the annotation guidelines 1 .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "System Overview", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Preprocessing", "text": "As a first step of our approach we perform the following text preprocessing:\n\u2022 Stopwords removal -we mark the words found in a predefined list of 32 stopwords.\n\u2022 Special character removal we remove special characters that violate the tokenization. E.g. in one of the datasets, dots, commas, quotation marks and other punctuation characters were present in tokens.\n\u2022 Lowercasing -we remove casing from the words.\n\u2022 Lemmatization -we find lemmas with the Stanford CoreNLP tool .\nOur preprocessing rather adds new information and does not modify the original information. Thus, the original word and all the generated variants are always available. In this way, we can generate the output file with identical words (including the special characters) from the input. The dataset are already tokenized.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Chunk Semantic Similarity", "text": "The core of our system is based upon computing semantic similarity of sentence chunks. More precisely, we are looking for the best estimation of sim(CH a i , CH b j ). The sim score should describe semantic similarity of a given chunk pairthe higher score the more easily both chunks can be replaced with each other without chaining the meaning of both sentences. The similarity score ranges from 0 to 5, where 0 is the lowest similarity and 5 is the highest similarity. Eg. the sim(\"a new laptop\", \"a new notebook\") = 5 and sim(\"a new laptop\", \"an old rock\") = 0. We use the chunk similarity as a feature in our machine learning approach (Section 3) and as a metric in our unsupervised approach (Section 4).\nOur attempts to estimate the sim function are based upon estimating semantic similarity of individual words and compiling them into one number for a given chunk pair. We experiment with Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) for estimating similarity of words. We compile all the word similarities in one number that reflects semantic similarity of whole chunks via the following methods: 1) the vector composition method and 2) an adapted method for constructing vectors called lexical semantic vectors.\nVector composition requires that the semantics of words is described by vectors. E.g. we have vectors for all words m i : \u2200w i \u2208 CH a j and n i : \u2200w i \u2208 CH b k in two given chunks CH a j and CH b k . The vectors for words in each chunk are summed (or averaged) to obtain one vector for each chunk: m = i ( m i ) and n = j ( n j ). The vectors are then compared with cosine distance: sim(CH a j , CH b k ) = cos(\u03b8) = m\u2022 n m| n| . Lexical semantic vectors were originally introduced in (Li et al., 2006). We have made two modifications. We do not weight words with their information content and we use methods for distributional semantics (Word2Vec and GloVe) rather than semantic networks. The modified method is explained here. First of all, we create a combined vocabulary of all unique words from chunks CH a k and\nCH b l : L = unique(CH a k \u222a CH b l ).\nThen we take all words from vocabulary L: w i \u2208 L and look for maximal similarities with words from chunks a and b, respectively. This way we get vectors m and n containing maximal similarities of chunk words and words from the combined vocabulary:\nm i = max j:1\u2264j\u2264|CH a k | sim(w i , w j ) : \u2200w i \u2208 L n i = max j:1\u2264j\u2264|CH b l | sim(w i , w j ) : \u2200w i \u2208 L\n(1) where m i and n i are elements of vectors m and n.\nIn order to obtain similarity of a chunk pair we compare their respective vectors with the cosine similarity similarly to the previous approach. The principle of the method is illustrated by the example in figure 1. iDF weighting. We assume that some words are more important than others. In order to reflect this assumption, we try to weight the vectors with iDF weighting. We compute the iDF weights on the articles from English wikipedia text data (Wikipedia dump from March 7, 2015).", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Machine Learning Approach", "text": "The main effort of our team was focused on the machine learning approach to the task. We divided the task into to three classification / regression tasks: \u2022 Alignment binary classification -we decide whether two given chunks should be aligned with each other.\n\u2022 Score classification / regression -we experiment with both classification and regression of the chunks similarity score.\n\u2022 Type classification -we classify all aligned pairs of chunks into a predefined set of types -see Section 1.1.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Classifiers", "text": "We experiment with the following classifiers: Maximum Entropy Classifier (Berger et al., 1996), Support Vector Machines Classifier (Cortes and Vapnik, 1995), Multilayer perceptron and Voted perceptron neural networks (Freund and Schapire, 1999) and with Decision / regression tree learning (Breiman et al., 1984). We employ the following two frameworks: Brainy (Konkol, 2014) and Weka (Hall et al., 2009).", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Features", "text": "We divide the employed features into four categories: lexical, syntactic, semantic, external.\nLexical features consist of the following features: word base form overlap, word lemma overlap, chunk length difference, word sentence positions difference.\nSyntactic features contains closest common parent comparison (we compute the closest common parent of all words for each chunk in the parse tree and retrieve the name of the parent node), parse tree path comparison (we compute the path from the root of the sentence to the chunk). POS (Part Of Speech) count difference (e.g. differences in counts of nouns, adjectives, verbs, etc). POS tagging and syntactic parsing are performed with Stanford CoreNLP .\nSemantic features are described in Section 2.2. Additionally, some members of our team participated in the STS task (task 1) of the SemEval 2016 (Brychc\u00edn and Svoboda, 2016) and they annotated the semantic similarity of the whole sentences with their system for us. This score is used as one feature.\nExternal features consist of the WordNet -Lin similarity metric (Lin, 1998) and the paraphrase database (Ganitkevitch et al., 2013) feature.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Post-processing", "text": "The alignment of chunks is generated by the binary classification of all possible chunk pairs. If one chunk is aligned with multiple chunks in the other sentence, these chunks should be merged into one chunk. Also, impossible multiple chunks to multiple chunks alignments are generated in some cases (e.g. two chunks from the first sentence belong a chunk in the second sentence but one of the two chunks from the first sentence belong also to a different chunk in the second sentence). These cases are resolved with few hand crafted rules.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Rule-based Approach", "text": "We attempt to solve the task with a rule-based approach as well. First, we define the similarity of chunks as described in Section 2.2. The similarity is then used for the chunk alignment. We employ an algorithm inspired by the IBM word model II for machine translation (Brown et al., 1993). We iterate over all chunks from sentence S a and find the chunk with maximal similarity from sentence S b . More chunks from sentence S a can be aligned to one chunk in the sentence S b . In this way, we obtain N:1 mapping. Then, we do the same with the reversed order of sentences and get the 1:M mapping. Then, we compare the mappings and take the one with the highest overall similarity. In this way, it is ensured that we generate only valid mappings (unlike in the previous case of machine learning -see Section 3.3).\nThe relation types are then determined by an extremely simple algorithm:\n\u2022 If the similarity is 5, then the relation type is EQUI.\n\u2022 If the similarity is 4 or 3 and chunks contain the same amount of words, then the relation is SIMI.\n\u2022 If the similarity is 4 or 3, then chunk with more words is more specific.\n\u2022 If the similarity is 2 or 1, then the relation is SIMI.\n\u2022 If the similarity is 0, then the relation type is NOALI.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental setup", "text": "Machine learning approach We employ the following classifiers and classification frameworks:\n\u2022 Alignment binary classification -Voted perceptron (Weka).\n\u2022 Score classification -Maximum entropy (Brainy).\n\u2022 Type classification -Support vector machines (Brainy).\nThese classifiers perform best on the evaluation datasets.\nWe achieved the best results for estimating chunk similarity with Word2Vec and the modified lexical semantic vectors -see Section 2.2.\nWe experimented with reduced feature set (word overlap, word positions difference, POS tags difference, semantic similarity, global semantic similarity, paraphrase database) -run 1 and with all featuresrun 3. The run 1 contains the optimal combination of features. Since this combination is established on evaluation datasets it does not need to be optimal for the test datasets. To increase our chances in the completion, we also run the system with all features -run 3.\nWe use the provided annotated evaluation dataset (Images, Headlines, Answer students) for training the models. We train three models, each for one dataset. For development, we use the 10-fold crossvalidation. For final test runs, we train the three models on evaluation datasets and run the system on the corresponding test datasets (e.g. Images evaluation dataset based model is used to annotate Images test data). We do not neither modify the original datasets nor annotate any additional data.\nRule-based approach There are little options in this approach. Again, we have achieved the best results for estimating chunk similarity with Word2Vec and the modified lexical semantic vectors -see Section 2.2. We set the threshold for the similarity score to 2.5. All lower values are set to 0. This is the run 2.\nIndividual setting for different dataset We restrained from setting individual configurations for different datasets. The setup is completely identical for all datasets.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "In this section, we summarize the official results for the SemEval 2016 competition -see table 1. The results are calculated for the following dataset: Headlines, Images and Answer students. The results show F1 scores for chunk alignment (Ali), determination of the relation type (Type), chunk similarity score (Score) and combination of relation type and score similarity (T+S). The bold numbers are the overall best scores. We participated only in the gold standard chunk scenario.\nThe results clearly show that the unsupervised run 2 perform much worse than the supervised runs 1 and 3. We expected that. However, it is worth of noticing that the unsupervised alignment algorithm inspired by machine translation alignment placed quite well. In fact, it is newer looses more than 3% from the best alignment score in all datasets. The overall rank of the run 2 places in the top half among all system with exception of the answer student dataset. The poor performance of the run 2 on this dataset is most likely caused by the fact that the hand-crafted rules were prepared for the images and  headlines datasets and they are clearly not applicable on the answer student which is substantially different.\nThe runs 1 and 3 perform very similarly. The optimized feature set of the run 1 helps especially in the answer student dataset. However, the differences between these runs are too small and they can be caused by chance. It is worth of noticing that the run 1 is not the best one in any of the datasets and it still wins in the overall results table. The reason is that it provides the most consistent results among all other runs of all systems in the competition.\nIn order to provide additional information about the features effectiveness, we have evaluated them on the final test datasets. In many cases, the obtained results are not conclusive. On some datasets, the features help slightly on others they even decrease the performance. However, the following three features have significant influence on the final results: modified lexical semantic vectors (+3% of the mean of T+S F1 scores), shared words (+2%), POS tags difference (+2%). The modified lexical semantic vectors method performed better than vector composition by 1% for the machine learning approach and by 2% for the rule-based approach in average. By optimizing the feature set, we were able to increase the mean score to 0.6484 of T+S F1 measure.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "The machine learning approach with combination of methods for the distributional semantics (Word2Vec and GloVe) proved to be very capable of solving the advanced task of Interpretable Semantic Textual Similarity. We have chosen not to tune the system for individual datasets but to tune it for the task as a whole. The modified lexical semantic vectors approach seems to be an attractive alternative to the more traditional vector composition.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "This publication was supported by the project LO1506 of the Czech Ministry of Education, Youth and Sports and by Grant No. SGS-2016-018 Data and Software Engineering for Advanced Applications. Computational resources were provided by the CESNET LM2015042 and the CERIT Scientific Cloud LM2015085, provided under the programme \"Projects of Large Research, Development, and Innovations Infrastructures\".", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability", "journal": "", "year": "2015-06", "authors": "Eneko Agirre; Carmen Banea; Claire Cardie; Daniel Cer; Mona Diab; Aitor Gonzalez-Agirre; Weiwei Guo; Inigo Lopez-Gazpio; Montse Maritxalar; Rada Mihalcea; German Rigau; Larraitz Uria; Janyce Wiebe"}, {"title": "Nerosim: A system for measuring and interpreting semantic textual similarity", "journal": "", "year": "2015-06", "authors": "Rajendra Banjade; Nabin Bikram Niraula; Vasile Maharjan; Dan Rus; Mihai Stefanescu; Dipesh Lintean;  Gautam"}, {"title": "A maximum entropy approach to natural language processing", "journal": "Computational Linguistics", "year": "1996-03", "authors": "Adam L Berger; Vincent J Della Pietra; Stephen A Della Pietra"}, {"title": "Classification and Regression Trees", "journal": "Wadsworth and Brooks", "year": "1984", "authors": "Leo Breiman; Jerome Friedman; Richard A Olshen; Charles Stone"}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "journal": "Computational Linguistics", "year": "1993-06", "authors": "F Peter;  Brown; J Della Vincent; Stephen A Pietra; Robert L Della Pietra;  Mercer"}, {"title": "Uwb at semeval-2016 task 1: Semantic textual similarity using lexical, syntactic, and semantic information", "journal": "", "year": "2016-06", "authors": "Tom\u00e1\u0161 Brychc\u00edn; Luk\u00e1\u0161 Svoboda"}, {"title": "Supportvector networks", "journal": "Mach. Learn", "year": "1995-09", "authors": "Corinna Cortes; Vladimir Vapnik"}, {"title": "Large margin classification using the perceptron algorithm. Machine Learning", "journal": "", "year": "1999-12", "authors": "Yoav Freund; Robert E Schapire"}, {"title": "PPDB: The paraphrase database", "journal": "", "year": "2013-06", "authors": "Juri Ganitkevitch; Benjamin Van Durme; Chris Callison-Burch"}, {"title": "The weka data mining software: An update", "journal": "SIGKDD Explorations Newsletter", "year": "2009-11", "authors": "Mark Hall; Eibe Frank; Geoffrey Holmes; Bernhard Pfahringer; Peter Reutemann; Ian H Witten"}, {"title": "Exb themis: Extensive feature extraction from word alignments for semantic textual similarity", "journal": "", "year": "2015-06", "authors": "Christian H\u00e4nig; Robert Remus; Xose De La Puente"}, {"title": "Umduluthblueteam: Svcsts -a multilingual and chunk level semantic similarity system", "journal": "Association for Computational Linguistics", "year": "2015-06", "authors": ""}, {"title": "Brainy: A machine learning library", "journal": "Springer International Publishing", "year": "2014", "authors": "Michal Konkol"}, {"title": "Sentence similarity based on semantic nets and corpus statistics", "journal": "IEEE Trans. on Knowl. and Data Eng", "year": "2006-08", "authors": "Yuhua Li; David Mclean; Zuhair A Bandar; James D O'shea; Keeley Crockett"}, {"title": "Extracting collocations from text corpora", "journal": "", "year": "1998", "authors": "Dekang Lin"}, {"title": "The Stanford CoreNLP natural language processing toolkit", "journal": "", "year": "2014", "authors": "Christopher D Manning; Mihai Surdeanu; John Bauer; Jenny Finkel; Steven J Bethard; David Mcclosky"}, {"title": "Efficient estimation of word representations in vector space", "journal": "CoRR", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An example of the modified lexical semantic vectors method.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Official system evaluation.", "figure_data": ""}], "doi": ""}