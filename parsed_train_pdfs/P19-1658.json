{"authors": "Ting-Yao Hsu; Chieh-Yang Huang; Yen-Chia Hsu; Kenneth Huang", "pub_date": "", "title": "Visual Story Post-Editing", "abstract": "We introduce the first dataset for human edits of machine-generated visual stories and explore how these collected edits may be used for the visual story post-editing task. The dataset, VIST-Edit 1 , includes 14,905 humanedited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions. We establish baselines for the task, showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models. We also discuss the weak correlation between automatic evaluation scores and human ratings, motivating the need for new automatic metrics.", "sections": [{"heading": "Introduction", "text": "Professional writers emphasize the importance of editing. Stephen King once put it this way: \"to write is human, to edit is divine.\" (King, 2000) Mark Twain had another quote: \"Writing is easy. All you have to do is cross out the wrong words.\" (Twain, 1876) Given that professionals revise and rewrite their drafts intensively, machines that generate stories may also benefit from a good editor. Per the evaluation of the first Visual Storytelling Challenge (Mitchell et al., 2018), the ability of an algorithm to tell a sound story is still far from that of a human. Users will inevitably need to edit generated stories before putting them to real uses, such as sharing on social media.\nWe introduce the first dataset for human edits of machine-generated visual stories, VIST-Edit, and explore how these collected edits may be used for the task of visual story post-editing (see Figure 1). The original visual storytelling (VIST) task, as introduced by Huang et al. (2016), takes a sequence of five photos as input and generates a short story describing the photo sequence. Huang et al. also released the VIST dataset, containing 20,211 photo sequences, aligned to human-written stories. On the other hand, the automatic postediting task revises the story generated from visual storytelling models, given both a machinegenerated story and a photo sequence. Automatic post-editing treats the VIST system as a black box that is fixed and not modifiable. Its goal is to correct systematic errors of the VIST system and leverage the user edit data to improve story quality.\nIn this paper, we (i) collect human edits for machine-generated stories from two different state-of-the-art models, (ii) analyze what people edited, and (iii) advance the task of visual story post-editing. In addition, we establish baselines for the task, and discuss the weak correlation between automatic evaluation scores and human ratings, motivating the need for new metrics.", "n_publication_ref": 5, "n_figure_ref": 1}, {"heading": "Related Work", "text": "The visual story post-editing task is related to (i) automatic post-editing and (ii) stylized visual captioning. Automatic post-editing (APE) revises the text generated typically from a machine translation (MT) system, given both the source sentences and translated sentences. Like the proposed VIST post-editing task, APE aims to correct the systematic errors of MT, reducing translator workloads and increasing productivity (Astudillo et al., 2018). Recently, neural models have been applied to APE in a sentence-to-sentence manner (Libovick\u1ef3 et al., 2016;Junczys-Dowmunt and Grundkiewicz, 2016), differing from previous phrase-based models that translate and reorder phrase segments for each sentence, such as (Simard et al., 2007;B\u00e9chara et al., 2011). More sophisticated sequence-to-sequence models with the attention mechanism were also introduced (Junczys-Dowmunt and Grundkiewicz, 2017;Libovick\u1ef3 and Helcl, 2017). While this line of work is relevant and encouraging, it has not explored much in a creative writing context. It is noteworthy that Roemmele et al. previously developed an online system, Creative Help, for collecting human edits for computer-generated narrative text (Roemmele and Gordon, 2018b). The collected data could be useful for story APE tasks.\nVisual story post-editing could also be considered relevant to style transfer on image captions. Both tasks take images and source text (i.e., machine-generated stories or descriptive captions) as inputs and generate modified text (i.e., postedited stories or stylized captions). End-to-end neural models have been applied to the transfer styles of image captions. For example, StyleNet, an encoder-decoder-based model trained on paired images and factual captions together with an unlabeled stylized text corpus, can transfer descriptive image captions to creative captions, e.g., humorous or romantic (Gan et al., 2017). Its advanced version with an attention mechanism, SemStyle, was also introduced (Mathews et al., 2018). In this paper, we adopt the APE approach to treat preand post-edited stories as parallel data instead of the style transfer approach that omits this parallel relationship during model training.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Dataset Construction & Analysis", "text": "Obtaining Machine-Generated Visual Stories This VIST-Edit dataset contains visual stories gen- erated by two state-of-the-art models, GLAC and AREL. GLAC (Global-Local Attention Cascading Networks) (Kim et al., 2018) achieved the highest human evaluation score in the first VIST Challenge (Mitchell et al., 2018). We obtain the pre-trained GLAC model provided by the authors via Github and run it on the entire VIST test set and obtain 2,019 stories. AREL (Adversarial REward Learning)  was the earliest available implementation online, and achieved the highest METEOR score on public test set in the VIST Challenge. We also acquire a small set of human edits for 962 AREL's stories generated using VIST test set, collected by Hsu et al. (2019).\nCrowdsourcing Edits For each machinegenerated visual story, we recruit five crowd workers from Amazon Mechanical Turk (MTurk) to revise it (at $0.12/HIT,) respectively. We instruct workers to edit the story \"as if these were your photos, and you would like using this story to share your experience with your friends.\" We also ask workers to stick with the photos of the original story so that workers would not ignore the machine-generated story and write a new one from scratch. Figure 2 shows the interface. For GLAC, we collect 2,019 \u00d7 5 = 10,095 edited stories in total; and for AREL, 962 \u00d7 5 = 4,810 edited stories have been collected by Hsu et al. (2019).\nData Post-processing We tokenize all stories using CoreNLP (Manning et al., 2014) and replace all people names with generic [male/female] tokens. Each of GLAC and AREL set is released as training, validation, and test following an 80%, 10%, 10% split, respectively.", "n_publication_ref": 5, "n_figure_ref": 1}, {"heading": "What do people edit?", "text": "We analyze human edits for GLAC and AREL. First, crowd workers systematically increase lexical diversity. We use type-token ratio (TTR), the ratio between the number of word types and the number of tokens, to estimate the lexical diversity of a story (Hardie and McEnery, 2006). Figure 3 shows significant (p<.001, paired t-test) positive shifts of TTR for both AREL and GLAC, which confirms the findings in Hsu et al. (2019). Figure 3 also indicates that GLAC generates stories with higher lexical diversity than that of AREL. Second, people shorten AREL's stories but lengthen GLAC's stories. We calculate the average number of Part-Of-Speech (POS) tags for tokens in each story using the python NLTK (Bird et al., 2009) package, as shown in Table 1. We also find that the average number of tokens in an AREL story (43.0, SD=5.0) decreases (41.9, SD=5.6) after human editing, while that of GLAC (35.0, SD=4.5) increases (36.7, SD=5.9). Hsu has observed that people often replace \"determiner/article + noun\" phrases (e.g., \"a boy\") with pronouns (e.g., \"he\") in AREL stories (2019). However, this observation cannot explain the story lengthening in GLAC, where each story on average has an increased 0.9 nouns after editing. Given the average per-story edit distances (Levenshtein, 1966;Damerau, 1964) for AREL (16.84, SD=5.64) and GLAC (17.99, SD=5.56) are similar, this difference is unlikely to be caused by deviation in editing amount.  Deleting extra words requires much less time than other editing operations (Popovic et al., 2014). Per Figure 3, AREL's stories are much more repetitive. We further analyze the type-token ratio for nouns (T T R noun ) and find AREL generates duplicate nouns. The average T T R noun of an AREL's story is 0.76 while that of GLAC is 0.90. For reference, the average T T R noun of a human-written story (the entire VIST dataset) is 0.86. Thus, we hypothesize workers prioritized their efforts in deleting repetitive words for AREL, resulting in the reduction of story length.", "n_publication_ref": 6, "n_figure_ref": 2}, {"heading": "Baseline Experiments", "text": "We report baseline experiments on the visual story post-editing task in Table 2. AREL's post-editing models are trained on the augmented AREL training set and evaluated on the AREL test set of VIST-Edit, and GLAC's models are tested using GLAC sets, too. Figure 4 shows examples of the output. Human evaluations (Table 2) indicate that the post-editing model improves visual story quality.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Methods", "text": "Two neural approaches, Long short-term memory (LSTM) and Transformer, are used as baselines, where we experiment using (i) text only (T) and (ii) both text and images (T+I) as inputs.\nLSTM An LSTM seq2seq model is used (Sutskever et al., 2014). For the text-only setting, the original stories and the human-edited stories are treated as source-target pairs. For the text-image setting, we first extract the image features using the pre-trained ResNet-152 model  and represent each image as a 2048-dimensional vector. We then apply a dense layer on image features in order to both fit its dimension to the word embedding and learn the adjusting transformation. By placing the image features in front of the sequence of text embedding, the input sequence becomes a matrix \u2208 R (5+len)\u00d7dim , where len is the text sequence length, 5 means 5 photos, and dim is the dimension of the word embedding. The input sequence with both image information and text information is then encoded by LSTM, identical as in the text-only setting.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Transformer (TF)", "text": "We also use the Transformer architecture (Vaswani et al., 2017)   ), Written-by-a-Human (\"This story sounds like it was written by a human.\"), Visually-Grounded, and Detailed. We take the average of the five judgments as the final score for each story. LSTM(T) improves all aspects for stories by AREL, and improves \"Focus\" and \"Human-like\" aspects for stories by GLAC.\nenriched embedding. It is noteworthy that the position encoding is only applied on text embedding. The input matrix \u2208 R (len+5)\u00d7dim is then passed into the Transformer as in the text-only setting.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experimental Setup and Evaluation", "text": "Data Augmentation In order to obtain sufficient training samples for neural models, we pair lessedited stories with more-edited stories of the same photo sequence to augment the data. In VIST-Edit, five human-edited stories are collected for each photo sequence. We use the human-edited stories that are less edited -measured by its Normalized Damerau-Levenshtein distance (Levenshtein, 1966;Damerau, 1964) to the original story -as the source and pair them with the stories that are more edited (as the target.) This data augmentation strategy gives us in total fifteen ( 5 2 +5 = 15) training samples given five human-edited stories.\nHuman Evaluation Following the evaluation procedure of the first VIST Challenge (Mitchell et al., 2018), for each visual story, we recruit five human judges on MTurk to rate it on six aspects (at $0.1/HIT.) We take the average of the five judgments as the final scores for the story. Table 2 shows the results. The LSTM using text-only input outperforms all other baselines. It improves all six aspects for stories by AREL, and improves \"Focus\" and \"Human-like\" aspects for stories by GLAC. These results demonstrate that a relatively small set of human edits can be used to boost the story quality of an existing large VIST model. Table 2 also suggests that the quality of a post-edited story is heavily decided by its pre-edited version. Even after editing by human editors, AREL's stories still do not achieve the quality of pre-edited stories by GLAC. The inefficacy of image features and Transformer model might be caused by the small size of VIST-Edit. It also requires further research to develop a post-editing model in a multimodal context.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Discussion", "text": "Automatic evaluation scores do not reflect the quality improvements. APE for MT has been using automatic metrics, such as BLEU, to benchmark progress (Libovick\u1ef3 et al., 2016). However, classic automatic evaluation metrics fail to capture the signal in human judgments for the proposed visual story post-editing task. We first use the human-edited stories as references, but all the automatic evaluation metrics generate lower scores when human judges give a higher rating (Table 3  We then switch to use the human-written stories (VIST test set) as references, but again, all the automatic evaluation metrics generate lower scores even when the editing was done by human (Table 4.)   Table 5 further shows the Spearman rank-order correlation \u03c1 between the automatic evaluation scores (sum of all six aspects) and human judgment calculated using different data combination. In row { of Table 5, the reported correlation \u03c1 of METEOR is consistent with the findings in Huang et al. (2016), which suggests that METEOR could be useful when comparing among stories generated by the same visual storytelling model. However, when comparing among machine-edited stories (row y and |), among pre-and post-edited stories (row z and }), or among any combinations of them (row~, and ), all metrics result in weak correlations with human judgments. These results strongly suggest the need of a new automatic evaluation metric for visual story postediting task. Some new metrics have recently been introduced using linguistic (Roemmele and Gor-don, 2018a) or story features (Purdy et al., 2018) to evaluate story automatically. More research is needed to examine whether these metrics are useful for story post-editing tasks too.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "VIST-Edit, the first dataset for human edits of machine-generated visual stories, is introduced. We argue that human editing on machinegenerated stories is unavoidable, and such edited data can be leveraged to enable automatic postediting. We have established baselines for the task of visual story post-editing, and have motivated the need for a new automatic evaluation metric.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Proceedings of the amta 2018 workshop on translation quality estimation and automatic postediting", "journal": "", "year": "2018", "authors": "Ram\u00f3n Astudillo; Jo\u00e3o Gra\u00e7a; Andr\u00e9 Martins"}, {"title": "Statistical post-editing for a statistical mt system", "journal": "", "year": "2011", "authors": "Hanna B\u00e9chara; Yanjun Ma; Josef Van Genabith"}, {"title": "Natural language processing with Python: analyzing text with the natural language toolkit", "journal": "Reilly Media, Inc", "year": "2009", "authors": "Steven Bird; Ewan Klein; Edward Loper"}, {"title": "A technique for computer detection and correction of spelling errors", "journal": "Communications of the ACM", "year": "1964", "authors": "J Fred;  Damerau"}, {"title": "Stylenet: Generating attractive visual captions with styles", "journal": "", "year": "2017", "authors": "C Gan; Z Gan; X He; J Gao; L Deng"}, {"title": "", "journal": "Statistics", "year": "2006", "authors": "Andrew Hardie; Tony Mcenery"}, {"title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"title": "On how users edit computer-generated visual stories", "journal": "ACM", "year": "2019", "authors": "Ting-Yao Hsu; Yen-Chia Hsu; K Ting-Hao;  Huang"}, {"title": "Visual storytelling", "journal": "", "year": "2016", "authors": "Ting-Hao Kenneth Huang; Francis Ferraro; Nasrin Mostafazadeh; Ishan Misra; Aishwarya Agrawal; Jacob Devlin; Ross Girshick; Xiaodong He; Pushmeet Kohli; Dhruv Batra"}, {"title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "journal": "", "year": "2016", "authors": "Marcin Junczys; - Dowmunt; Roman Grundkiewicz"}, {"title": "An exploration of neural sequence-tosequence architectures for automatic post-editing", "journal": "", "year": "2017", "authors": "Marcin Junczys; - Dowmunt; Roman Grundkiewicz"}, {"title": "Glac net: Glocal attention cascading networks for multiimage cued story generation", "journal": "", "year": "2018", "authors": "Taehyeong Kim; Min-Oh Heo; Seonil Son; Kyoung-Wha Park; Byoung-Tak Zhang"}, {"title": "On writing: A memoir ofthe craft", "journal": "Scrihner", "year": "2000", "authors": "Stephen King"}, {"title": "Binary codes capable of correcting deletions, insertions, and reversals", "journal": "", "year": "1966", "authors": " Vladimir I Levenshtein"}, {"title": "Attention strategies for multi-source sequence-to-sequence learning", "journal": "", "year": "2017", "authors": "Jind\u0159ich Libovick\u1ef3; Jind\u0159ich Helcl"}, {"title": "Cuni system for wmt16 automatic post-editing and multimodal translation tasks", "journal": "", "year": "2016", "authors": "Jind\u0159ich Libovick\u1ef3; Jind\u0159ich Helcl; Marek Tlust\u1ef3; Ond\u0159ej Bojar; Pavel Pecina"}, {"title": "The Stanford CoreNLP natural language processing toolkit", "journal": "", "year": "2014", "authors": "Christopher D Manning; Mihai Surdeanu; John Bauer; Jenny Finkel; Steven J Bethard; David Mc-Closky"}, {"title": "Semstyle: Learning to generate stylised image captions using unaligned text", "journal": "", "year": "2018", "authors": "Alexander Mathews; Lexing Xie; Xuming He"}, {"title": "Proceedings of the first workshop on storytelling", "journal": "", "year": "2018", "authors": "Margaret Mitchell; Francis Ferraro; Ishan Misra"}, {"title": "Relations between different types of post-editing operations, cognitive effort and temporal effort", "journal": "", "year": "2014", "authors": "Maja Popovic; Arle Lommel; Aljoscha Burchardt"}, {"title": "Predicting generated story quality with quantitative measures", "journal": "", "year": "2018", "authors": "Christopher Purdy; Xinyu Wang; Larry He; Mark Riedl"}, {"title": "Linguistic features of helpfulness in automated support for creative writing", "journal": "", "year": "2018", "authors": "Melissa Roemmele; Andrew Gordon"}, {"title": "Automated assistance for creative writing with an rnn language model", "journal": "ACM", "year": "2018", "authors": "Melissa Roemmele; Andrew S Gordon"}, {"title": "Statistical phrase-based post-editing", "journal": "", "year": "2007", "authors": "Michel Simard; Cyril Goutte; Pierre Isabelle"}, {"title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; Quoc V Le"}, {"title": "1876. The Adventures of Tom Sawyer", "journal": "American Publishing Company", "year": "", "authors": "Mark Twain"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "No metrics are perfect: Adversarial reward learning for visual storytelling", "journal": "", "year": "2018", "authors": "Xin Wang; Wenhu Chen; Yuan-Fang Wang; William Yang Wang"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: A machine-generated visual story (a) (by GLAC), its human-edited (b) and machine-edited (c) (by LSTM) version.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Interface for visual story post-editing. An instruction (not shown to save space) is given and workers are asked to stick with the plot of the original story.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure3: KDE plot of type-token ratio (TTR) for pre-/post-edited stories. People increase lexical diversity in machine-generated stories for both AREL and GLAC.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Average number of tokens with each POS tag per story. (\u2206: the differences between post-and preedit stories. NUM is omitted because it is nearly 0. Numbers are rounded to one decimal place.)", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "as baseline. The text-only setup and image feature extraction are identical to that of LSTM. For Transformer, the image features are attached at the end of the sequence of text embedding to form an image-", "figure_data": "ARELGLACEdited ByFocus Coherence Share Human Grounded DetailedFocus Coherence Share Human Grounded DetailedN/A3.4873.751 3.7633.7463.6023.7613.8783.908 3.9303.8173.8643.938TF (T)3.4333.705 3.6413.6563.6193.6313.7173.773 3.8633.6723.7653.795TF (T+I)3.5423.693 3.6763.6433.5483.6723.7343.759 3.7863.6223.7583.744LSTM (T)3.5513.800 3.7713.7513.6313.8103.8943.896 3.8643.8483.7513.897LSTM (T+I)3.4973.734 3.7463.7423.5733.7553.8153.872 3.8473.8133.7503.869Human3.5923.870 3.8563.8853.7793.8784.0034.057 4.0723.9763.9944.068"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Human evaluation results. Five human judges on MTurk rate each story on the following six aspects, using a 5-point Likert scale (from Strongly Disagree to Strongly Agree): Focus, Structure and Coherence, Willingto-Share (\"I Would Share\"", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Average evaluation scores for AREL stories, using the human-edited stories as references. All the automatic evaluation metrics generate lower scores when human judges give a higher rating.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Average evaluation scores on GLAC stories, using human-written stories as references. All the automatic evaluation metrics generate lower scores even when the editing was done by human.", "figure_data": "Figure 4: Example stories generated by baselines.Spearman rank-order correlation \u03c1Data IncludesBLEU4 METEOR ROUGE Skip-Thoughtsx AREL.110.099.063.062y LSTM-Edited AREL.106.109.067.205z x+y.095.092.059.116{ GLAC.222.203.140.151| LSTM-Edited GLAC.163.176.138.087} {+|.196.194.148.116x+{.091.086.059.088y+|.089.103.067.101x+y+{+|.090.096.069.094"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ": Spearman rank-order correlation \u03c1 betweenthe automatic evaluation scores (sum of all six as-pects) and human judgment. When comparing amongmachine-edited stories (y and |), among pre-andpost-edited stories (z and }), or among any combina-tions of them (~, and ), all metrics result in weakcorrelations with human judgments."}], "doi": "10.1109/CVPR.2017.108"}