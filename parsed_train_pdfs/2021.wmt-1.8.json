{"authors": "Amr Hendy; Esraa A Gad; Mohamed Abdelghaffar; Jailan S Elmosalami; Mohamed Afify; Ahmed Y Tawfik; Hany Hassan Awadalla", "pub_date": "", "title": "Ensembling of Distilled Models from Multi-task Teachers for Constrained Resource Language Pairs", "abstract": "This paper describes our submission to the constrained track of WMT21 shared news translation task. We focus on the three relatively low resource language pairs Bengali \u2194 Hindi, English \u2194 Hausa and Xhosa \u2194 Zulu. To overcome the limitation of relatively low parallel data we train a multilingual model using a multitask objective employing both parallel and monolingual data. In addition, we augment the data using back translation. We also train a bilingual model incorporating back translation and knowledge distillation then combine the two models using sequence-to-sequence mapping. We see around 70% relative gain in BLEU point for En \u2194 Ha and around 25% relative improvements for Bn \u2194 Hi and Xh \u2194 Zu compared to bilingual baselines.", "sections": [{"heading": "Introduction", "text": "Neural machine translation (NMT) witnessed a lot of success in the past few years especially for high resource languages (Vaswani et al., 2017). Improving the quality of low resource languages is still challenging. Some of the popular techniques are adding high resource helper languages as in multilingual neural machine translation (MNMT) (Dong et al., 2015;Firat et al., 2016;Ha et al., 2016;Johnson et al., 2017;Arivazhagan et al., 2019), using monolingual data including pre-training (Liu et al., 2020), multi-task learning (Wang et al., 2020), back translation (Sennrich et al., 2016) or any combination of these methods (Barrault et al., 2020) and system combination of multiple systems (Liu et al., 2018).\nThis paper describes the Microsoft Egypt Development Center (EgDC) submission to the WMT21 shared news translation task for three low resource language pairs (six directions), Bengali \u2194 Hindi (Bn \u2194 Hi), English \u2194 Hausa (En \u2194 Ha) and Xhosa \u2194 Zulu (Xh \u2194 Zu). We focus on the constrained track because it is easier to compare different systems and it is always possible to improve performance by adding more data. The main features of our approach are as follows:\n\u2022 Using a recently proposed multitask and multilingual learning framework to benefit from monolingual data in both the source and target languages (Wang et al., 2020).\n\u2022 Using knowledge distillation (Freitag et al., 2017) to create bilingual baselines from the original multilingual model and combining it with the multilingual model.\nThe paper is organized as follows. Section 2 gives an overview of the data used in the constrained scenario, followed by section 3 that gives a detailed description of our approach. Section 4 presents our experimental evaluation. Finally, our findings are summarized in Section 5.", "n_publication_ref": 13, "n_figure_ref": 0}, {"heading": "Data", "text": "Following the constrained track, we use bitext data provided in WMT21 for the following pairs: Bengali \u2194 Hindi, English \u2194 Hausa, Xhosa \u2194 Zulu and English \u2194 German. Statistics of the parallel data used for the three pairs in addition to the German helper are shown in Table 1. We also use monolingual data for all previously mentioned languages provided in WMT21 for techniques such as multi-task training and back-translation. Statistics of the monolingual data used for the 6 languages in addition to the German helper are shown in Table 2. For very low resource languages, Hausa, Xhosa and Zulu, we use all the available monolingual data, e.g. NewsCrawl + CommonCrawl + Extended CommonCrawl for Hausa, and Extended Common-Crawl for both Xhosa and Zulu. For relatively high resource languages, Bengali, Hindi, English and German, we only use a subset of the provided data mostly from NewsCrawl due to its high-quality. In addition to the NewsCrawl monolingual subset, we add a sampled subset from CommonCrawl to   ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Data Filtering", "text": "For Bengali, English, Hindi and German, we apply fastText 1 language identification on the monolingual data to remove sentences which are not predicted as the expected language. We do the same for Hausa, Xhosa and Zulu using Polyglot 2 because fastText does not cover these three languages. The resulting size of the monolingual data of each language is shown in Table 2.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "System Architecture", "text": "The final MT system in each direction is an ensemble of two NMT models comprising a bilingual model (one for each of the six primary directions) and a multilingual model trained to provide translations for 8 directions (the six primary directions plus English \u2194 German). The multilingual system uses a recently proposed multitask framework for training (Wang et al., 2020). We describe the individual systems in Subsection 3.1. This is followed by presenting our system combination techniques in Subsection 3.2. Finally we present the architecture of the submitted system highlighting our design decisions in Subsection 3.3.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Individual Systems", "text": "This subsection describes the individual systems and their training leading to the proposed system combination strategy in the following subsection. We first build bilingual models for the six primary directions using the data shown in Table 1 except the English \u2194 German. These serve as baselines to compare to the developed systems. The models use a transformer base architecture comprising 6 encoder and 6 decoder layers and a 24K joint vocabulary built for Bengali \u2194 Hindi, a 8K joint vocabulary built for English \u2194 Hausa and a 4K joint vocabulary built for Xhosa \u2194 Zulu using sentencepiece (Kudo and Richardson, 2018) to learn these subword units to tokenize the sentences. In addition to the baseline bilingual models, we use knowledge distilled (KD) data and back-translated (BT) data generated from a multilingual model to build another set of bilingual models for each of the six primary directions. This multilingual model is described below. The purpose of these models is to participate in the ensemble along with the multilingual models. The latter bilingual models follow the same transformer base architecture and joint vocabulary used in the baseline bilingual models.\nThe multilingual model combines the 8 translation directions shown in Table 1. These are the six primary directions plus English \u2194 German as a helper. The latter is mainly used to improve generation on the English centric directions. The model uses a 64K joint vocabulary constructed using sentencepiece (Kudo and Richardson, 2018) from a subset of the monolingual data of each language as described in Section 2. The transformer model has 12 encoder and 6 decoder layers. In addition, a multitask objective is used during training to make use of monolingual data. The objective comprises the usual parallel data likelihood referred to as MT, a masked language model (MLM) at the encoder and a denoising auto-encoder (DAE) (similar to mBART (Liu et al., 2020)) at the decoder side. The latter two objectives help leverage monolingual data for both the encoder and the decoder sides. The three objectives are combined using different proportions according to a schedule during the training. Please refer to (Wang et al., 2020) for details.\nTo summarize we build the following models:\n\u2022 Bilingual models trained using parallel data in Table 1 for the 6 primary directions. These are mainly used as baselines.\n\u2022 Multilingual models trained using a multitask objective using parallel and monolingual data and comprising 8 directions.\n\u2022 Bilingual models trained using KD and BT data generated using our best multilingual model. These are combined with the best multilingual model as described in 3.2.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "System Combination", "text": "System combination or ensembling is known to improve the performance over individual systems.\nThere are many ways to create an ensemble (Liu et al., 2018;Dabre et al., 2019). For example, individual models obtained from different checkpoints during the same training or by training models sharing the same vocab and architecture using different data or simply different random seeds can be combined using model averaging techniques. Here, we opt to combine different models since it generally leads to better performance because different models tend to be more complementary. To this end, we propose a simple and effective method to combine completely different architectures. The proposed method could be also used in conjunction with checkpoint and model averaging for further gains, but we haven't tried this in our experiments due to time limitations. The basic idea of our combination is very simple. Assume we have the translation pair x \u2192 y where y is the reference translation. The output of model 1 is the pair x \u2192 y1 and the output of model 2 is the pair x \u2192 y2. This can be generalized to multiple systems but we limited our combination to only two models. We train a new model that takes the set of hypotheses (possibly augmented by the source sentence) from the two models to generate the target sentence. Thus this model combines the outputs of two models in the ensemble to produce a translation closer to the original target sentence i.e. < HY P > y1 < HY P > y2 \u2192 y.We also experimented with adding the source to the input i.e. < SRC > x < HY P > y1 < HY P > y2 \u2192 y which led to around 0.3 BLEU improvement for Ha \u2192 En, but we haven't tried on other pairs due to time limitation. All combination models use 6 layers encoder and decoder and a 64K vocabulary similar to the multilingual system. These combination models use the full bitext and dev data provided in WMT21 as shown in Table 1. The system combination is outlined in Figure 1. This ensembling technique can be thought of as providing both system combination and post-editing capabilities.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Overall System", "text": "Our overall system is depicted in Figure 2 1 following the temperature-based strategy in (Arivazhagan et al., 2019) to balance the training data in different resource languages using T = 5. We pick the best system and use it to back translate the selected monolingual data. For most pairs, as detailed in Section 4, we find that M T + DAE and M T + M LM + DAE are quite close. Therefore, we use the M T + DAE to do back translation for all submitted 6 pairs. We use beam search with beam size = 5 when generating the synthetic back-translated data. Once we get the back-translated data (called BT 1 ) we add it to our parallel and monolingual data and build a new multilingual model called M T +DAE +BT 1 . We tag the back-translated data with <BT> tag at beginning of each source sentence so the model can differentiate between the genuine parallel and backtranslated data quality. The resulting model is used to regenerate the back-translated data (called BT 2 ) and to knowledge distill the bitext (called KD). The latter two data sets are augmented and used to build a bilingual system (called M T +KD +BT 2 ). We upsample the KD data set and the upsampling ratio is selected based on parameter sweeping and validating the resulting improvement on the validation set. Finally, the latter bilingual model is combined with our final multilingual model using the method in Section 3.2 to create our submission.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Experimental Results", "text": "In this section, we describe the results of our intermediate and final systems. We report Sacre- BLEU (Post, 2018) on the validation set released in WMT21, and both SacreBLEU and COMET (Rei et al., 2020) using the available implementation 3 on the official test set released in WMT21. The results for the six submitted language pairs are in Tables 3-5. The first row in each table shows the bilingual baseline which performs relatively poor due to the limited amount of parallel data for each pair. This is followed by the four multilingual systems with different objectives. It is clear that adding a monolingual objective brings nice improvements for all language pairs. The M T + DAE and M T + M LM + DAE perform closely for all language pairs indicating that target monolingual data is most important. The next two rows show the results of adding back-translated data to the multilingual model and a bilingual baseline using back-translated and knowledge distilled data generated from the best multilingual model. As expected adding back translation brings significant improvement to all language pairs. Also using the multilingual model to create data for a bilingual model shows excellent results that outperform the multilingual model. Finally, the ensemble, as expected, performs better than the individual models. The significant difference between reported improvements in Ha \u2194 En and other directions shows the effectiveness of adding De \u2194 En parallel and monolingual data that helps English centric directions more than other directions. We evaluated the final submitted systems on the official test set released in WMT21 as shown in Table 6.    ", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Summary", "text": "This paper describes our submission to the constrained track of WMT21. We focus on the three relatively low resource language pairs Bn \u2194 Hi, En \u2194 Ha and Xh \u2194 Zu. To overcome the limitation of relatively low parallel data we train a multilingual model using a multitask objective recently proposed in (Wang et al., 2020). In addition,   we augment the data using back translation. We also use the resulting multilingual model to create a bilingual model incorporating back translation and knowledge distillation. Finally, we combine the two models, using a flexible sequence-to-sequence approach, to yield our submitted systems. We see large gains up to 8-10 BLEU points for En \u2194 Ha and nice improvements of up to 2-3 BLEU points for Bn \u2194 Hi and Xh \u2194 Zu.", "n_publication_ref": 1, "n_figure_ref": 0}], "references": [{"title": "Massively multilingual neural machine translation in the wild: Findings and challenges", "journal": "", "year": "2019", "authors": "Naveen Arivazhagan; Ankur Bapna; Orhan Firat; Dmitry Lepikhin; Melvin Johnson; Maxim Krikun; Mia Xu Chen; Yuan Cao; George Foster; Colin Cherry; Wolfgang Macherey; Zhifeng Chen; Yonghui Wu"}, {"title": "Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation", "journal": "", "year": "", "authors": "Lo\u00efc Barrault; Magdalena Biesialska; Ond\u0159ej Bojar; Marta R Costa-Juss\u00e0; Christian Federmann; Yvette Graham; Roman Grundkiewicz; Barry Haddow; Matthias Huck; Eric Joanis; Tom Kocmi; Philipp Koehn; Chi-Kiu Lo; Nikola Ljube\u0161i\u0107; Christof Monz; Makoto Morishita; Masaaki Nagata; Toshiaki Nakazawa"}, {"title": "Enabling multi-source neural machine translation by concatenating source sentences in multiple languages", "journal": "", "year": "2019", "authors": "Raj Dabre; Fabien Cromieres; Sadao Kurohashi"}, {"title": "Multi-task learning for multiple language translation", "journal": "", "year": "2015", "authors": "Daxiang Dong; Hua Wu; W He; Dianhai Yu; Haifeng Wang"}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Orhan Firat; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Ensemble distillation for neural machine translation", "journal": "", "year": "2017", "authors": "Markus Freitag; Yaser Al-Onaizan; Baskaran Sankaran"}, {"title": "Toward multilingual neural machine translation with universal encoder and decoder", "journal": "", "year": "2016-01", "authors": "Thanh-Le Ha; ; ; Alexander Waibel"}, {"title": "Google's multilingual neural machine translation system: Enabling zero-shot translation", "journal": "", "year": "2017", "authors": "Melvin Johnson; Mike Schuster; Quoc V Le; Maxim Krikun; Yonghui Wu; Zhifeng Chen; Nikhil Thorat; Fernanda Vi\u00e9gas; Martin Wattenberg; Greg Corrado; Macduff Hughes; Jeffrey Dean"}, {"title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"title": "Multilingual denoising pre-training for neural machine translation", "journal": "", "year": "2020", "authors": "Yinhan Liu; Jiatao Gu; Naman Goyal; Xian Li; Sergey Edunov; Marjan Ghazvininejad; Mike Lewis; Luke Zettlemoyer"}, {"title": "A comparable study on model averaging, ensembling and reranking in nmt", "journal": "Springer International Publishing", "year": "2018", "authors": "Yuchen Liu; Long Zhou; Yining Wang; Yang Zhao; Jiajun Zhang; Chengqing Zong"}, {"title": "A call for clarity in reporting bleu scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"title": "COMET: A neural framework for MT evaluation", "journal": "", "year": "2020", "authors": "Ricardo Rei; Craig Stewart; Ana C Farinha; Alon Lavie"}, {"title": "Improving neural machine translation models with monolingual data", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"title": "Multi-task learning for multilingual neural machine translation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Yiren Wang; Chengxiang Zhai; Hany Hassan"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": ". The first module shows the data input where language identification (LID) is used to filter the monolingual data. As mentioned in Section 2.1 we use fastText and polyglot for LID depending on the language. We first build bilingual baselines which are not shown in the figure. Then as shown in the second module, we build 4 multilingual systems using different task objectives as follows: M T, M T + M LM, M T + DAE and M T + M LM + DAE trained on the 8 directions shown in Table", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: The system combination component used for our experiments.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "3 https://github.com/Unbabel/COMET", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Bitext data used for bilingual and multilingualsystems. For each language pair, we use all availablesources released in WMT21Language# of sentences Raw CleanedBengali53.8M53.3MEnglish75M73.5MGerman111.2M 109.9MHausa10.8M6.2MHindi60.2M59.8MXhosa1.6M950KZulu2M1.4M"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": Monolingual data used for multi-task trainingand back-translationavoid biasing into the news domain especially forBengali \u2194 Hindi and Xhosa \u2194 Zulu whose targetevaluation domain come from Wikipedia content."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Results of Ha-En and En-Ha systems.", "figure_data": "We re-"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results of Bn-Hi and Hi-Bn systems. We report SacreBLEU scores on the validation set provided in WMT21", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Figure 2: The overall system flow used for our experiments", "figure_data": "SystemXh-Zu Zu-Xhbilingual baseline8.007.60multi. MT7.537.47+ MLM7.237.02+ DAE8.538.24+ MLM + DAE8.207.80multi. MT + DAE + BT 19.068.86bilingual MT + KD + BT 29.809.17ensemble10.009.30"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Results of Xh-Zu and Zu-Xh systems. We report SacreBLEU scores on the validation set provided in WMT21", "figure_data": "Translation direction BLEU COMETHa \u2192 En17.130.149En \u2192 Ha16.130.086Bn \u2192 Hi21.080.532Hi \u2192 Bn10.930.411Xh \u2192 Zu9.940.180Zu \u2192 Xh9.250.299"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Results of the submitted systems. We report SacreBLEU and COMET scores on the official test set provided in WMT21. For COMET, we use the recommended model \"wmt20-comet-da\".", "figure_data": ""}], "doi": "10.18653/v1/N16-1101"}