{"authors": "Wei Song; Ziyao Song; Ruiji Fu; Lizhen Liu; Miaomiao Cheng; Ting Liu", "pub_date": "", "title": "Discourse Self-Attention for Discourse Element Identification in Argumentative Student Essays", "abstract": "This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays. Specifically, we focus on two issues. First, we propose structural sentence positional encodings to explicitly represent sentence positions. Second, we propose to use inter-sentence attentions to capture sentence interactions and enhance sentence representation. We conduct experiments on two datasets: a Chinese dataset and an English dataset. We find that (i) sentence positional encodings can lead to a large improvement for identifying discourse elements; (ii) a structural relative positional encoding of sentences shows to be most effective; (iii) inter-sentence attention vectors are useful as a kind of sentence representation for identifying discourse elements.", "sections": [{"heading": "Introduction", "text": "Discourse describes how a document is organized. This paper focuses on the task of discourse element identification (DEI) in argumentative student essays. Discourse elements represent the function and contribution of every discourse unit to the discourse. Burstein et al. (2003) formulate discourse elements as 5 categories: introduction, thesis, main idea, supporting and conclusion, while argument components such as major claim, claim and premise are used as discourse elements in argumentation structure parsing in persuasive essays (Stab and Gurevych, 2014). DEI can benefit automated essay scoring in many aspects: modeling organization, inferring topics and opinions or used as features for scoring systems (Attali and Burstein, 2006;Burstein et al., 2001;Persing et al., 2010;Song et al., 2020).\nDespite its importance, DEI is challenging. First, the ambiguity of sentences makes learning models difficult to distinguish some discourse elements. For example, the thesis is defined as expressing the central claim of the author and the main ideas support the thesis from specific aspects. However, it is hard to distinguish them from their content and style.\nSecond, the discourse element of a specific sentence depends on context. As a result, considering individual sentences only would have difficulties in identifying discourse elements. The relations and relatedness among multiple sentences should be explored.\nThird, the data imbalance problem is serious, e.g., the number of elaboration sentences could be 10 times more than the number of thesis sentences. The minority discourse elements (such as thesis, main ideas or major claim) are harder to be recalled although they have important roles in many scenarios, e.g., evaluating the organization of essays (Attali and Burstein, 2006).\nIn this paper, we propose a method to explicitly model sentence positions and relations to improve discourse element identification in argumentative student essays. Our idea is partially motivated by the self-attention mechanism such as (Vaswani et al., 2017). Self-attention is usually applied to capture dependencies between words. We aim to apply self-attention mechanism to describe relations between sentences.\nOn one hand, position information is important for DEI to give clues on discourse elements beyond content and style, because authors usually hold some conventions to organize content. Position is one of the most useful feature classes in feature-based DEI (Burstein et al., 2003;Stab and Gurevych, 2014). Previous neural network models usually cast DEI as a classification or sequence labeling task and do not explicitly model position information. Motivated by the positional encoding of words, we propose a simple structural positional encoding strategy for a sentence by considering its relative position in its essay, relative position of its paragraph in its essay, and its relative position within its paragraph.\nOn the other hand, relatedness among sentences may also indicate properties of discourse elements. For example, thesis sentences should have close relations to the whole essay; main ideas usually locate in similar positions and have high relatedness. Relatedness between discourse elements has shown to be an important indicator of essay coherence (Higgins et al., 2004). We compute inter-sentence attention vectors to represent either element-wise or content-wise relations to other sentences, which bring in additional information beyond individual sentences and enhance sentence representation without extra information.\nExperiments show that the proposed approach can get considerable improvements compared with feature-based and neural network based baselines on a Chinese dataset and obtain competitive results compared with the state-of-the-art method on an English dataset. The structural positional encodings of sentences show effectiveness to achieve obvious overall improvements. The inter-sentence attention vectors enhance sentence representation helping identify discourse elements as well.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Related Work", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Discourse Element Identification", "text": "DEI could be seen as a subtask in discourse structure analysis. It aims to identify discourse elements, determine their functions and establish relationships among them in an argumentative text.\nTypical tasks in this line include argumentative zoning (Teufel et al., 1999) , argumentation mining (Mochales and Moens, 2011;Lippi and Torroni, 2016) and analyzing argumentative student essays (Burstein et al., 2003;Stab and Gurevych, 2014). Argumentative zoning identifies arguments in scientific articles (Teufel et al., 1999;Guo et al., 2010). Argumentation mining aims to identify argument components and relations from legal texts (Palau and Moens, 2009;Mochales and Moens, 2011) or argumentative texts (Stab and Gurevych, 2014;.\nThe solutions to these tasks usually adopt similar machine learning methods but use domain related features. The methods could be roughly classified into the following categories.\nClassification based methods cast DEI as a classification problem. Various classifiers have been tested, such as SVM (Stab and Gurevych, 2014), decision trees (Burstein et al., 2003(Burstein et al., , 2001 and naive Bayes, maximum entropy model (Moens et al., 2007;Palau and Moens, 2009). Sequence labeling based methods exploit contextual information for DEI with conditional random fields (Hirohata et al., 2008;Song et al., 2015) or recurrent neural networks .\nEstablishing relations between sentences is often viewed as a classification tasks as well (Stab and Gurevych, 2014). Parsing based methods are also adopted to build more complex structures with techniques like ILP  or RST style parsing (Peldszus and Stede, 2015).\nFeature engineering. Some common features are shared across these tasks, including syntactic, lexical, semantic and discourse relations. There are also domain related features to further boost the performance. Mochales and Moens (2011) designed special features for argumentation mining in legal texts. Nguyen and Litman (2015) identified claims based on domain words. Lippi and Torroni (2015) modeled syntactic structures for content independent claim detection based on tree kernels.\nOur work is mostly related to DEI in argumentative student essays (Burstein et al., 2003;Stab and Gurevych, 2014), which is useful for qualifying essay organization (Persing et al., 2010), argumentation (Persing and Ng, 2016;Wachsmuth et al., 2016) and general writing (Burstein et al., 2003;Ong et al., 2014;Song et al., 2014). The major feature classes proposed by Burstein et al. (2003) and Stab and Gurevych (2014) are used to build a baseline. The features include: position, cue words, lexical features (main verbs, adverbs and connectives) and structural features (such as number of clauses). Some of these features are based on manually collected lexicons.\nDeep Learning Methods have achieved great success in many NLP tasks.  proposed neural argumentation mining models based on sequence tagging or dependency parsing. It exploits inter-sentence relations but needs sophisticated language processing.  exploited CNN and LSTM for classifying sentences to identify claims from different domains. It mainly depends on the content of components but does not sufficiently model positions and exploit inter-sentence relatedness.", "n_publication_ref": 32, "n_figure_ref": 0}, {"heading": "Attention Mechanism for Discourse Representation", "text": "Attention mechanism was first introduced by (Bahdanau et al., 2015) in the encoder-decoder framework. Attention has the ability to learn important regions within a context and has been widely adopted in deep learning. Liu and Lapata (2018) proposed a structured attention mechanism to derive a tree over a text, akin to an RST discourse tree. Ferracane et al. (2019) evaluated the model, however, found multiple negative results. Attention mechanism has also been applied for RST parsing and its applications (Li et al., 2016;Ji and Smith, 2017;Huber and Carenini, 2019) but it is mostly used for capturing local semantic interactions.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Self-Attention Mechanism", "text": "Vaswani et al. ( 2017) proposed the self-attention mechanism and achieved state of the art results in many NLP tasks. Since then, self-attention has drawn increasing interests due to flexibility in modeling long range interactions.\nSelf-attention ignores word order in a sentence. As a result, position representations are developed to cooperate with self-attention. In addition to the sinusoidal position representation proposed by Vaswani et al. (2017), there are also other variations to bias the selection of attentive regions (Shen et al., 2018;Shaw et al., 2018;Yang et al., 2019). In NLP, self-attention is mostly applied to sequential structures such as a sequence of words. Mihaylov and Frank (2019) proposed a discourse-aware selfattention encoder for reading comprehension on narrative texts, where event chains, discourse relations and coreference relations are used for connecting sentences. Self-attention can be also extended to 2d-dimensions for image processing (Parmar et al., 2018) and lattice inputs (Sperber et al., 2019).", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Baseline", "text": "We use Hierarchical BiLSTM (HBiLSTM), which is similar to (Yang et al., 2016), as the base model to model sentence and discourse level representations.\nThe task is to assign discourse element labels y = (y 1 , ..., y n ) to sentences (x 1 , ..., x n ) in a text, where x i , 1 \u2264 i \u2264 n, is a sentence of a sequence of words and y i \u2208 Y, Y is a set of pre-defined discourse elements.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Sentence Representation Layer", "text": "A sequence of words x = {w 1 , ..., w N } is modeled with a RNN encoder and is converted into a sequence of hidden states H = {h 1 , ..., h N }. The hidden state at the i-th step is\nh i = f (e (w i ) , h i\u22121 ) , (1\n)\nwhere f is a RNN unit, e(w i ) \u2208 R d is the embedding of a word, and h i\u22121 is the hidden state of the previous step. The whole sequence could be represented as a fixed length vector\nc = \u03c6({h 1 , \u2022\u2022\u2022, h N })\nto represent the semantic of a sentence, where \u03c6(\u2022) is a function to summarize hidden states.\nIn this work, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) is used as the RNN unit and the sequence is encoded in a Bidirectional way that a hidden state\nh i = [ \u2212 \u2192 h i ; \u2190 \u2212 h i ]\nis the concatenation of the corresponding hidden states from both directions. The summarization function \u03c6(\u2022) could be based on the attention mechanism.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Discourse Representation Layer", "text": "In the discourse element layer, we feed sentence representations C = (c 1 , ..., c n ) \u2208 R d\u00d7n to a BiL-STM and use a nonlinear layer to map semantic representations to discourse element representations, D = tanh(BiLSTM (C)).\n(2)", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Inference Layer", "text": "Finally, we use a linear and a softmax layer to predict the discourse element of every sentence,\nY = softmax (linear(D)) ,(3)\nwhere Y \u2208 R |Y|\u00d7n refers to the probabilities of every sentence over discourse element categories. The baseline mainly exploits interactions between adjacent sentences, but long distance interactions and sentence positions are not explicitly considered, which may be also important to determine the function of sentences in argumentative discourse.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Discourse Self-Attention", "text": "We propose the Discourse Self-Attention (DiSA) layer to improve the baseline by explicitly modeling sentence positions and inter-sentence interactions. The architecture is illustrated in Figure 1.\nThe sentences in an essay are converted to sentence embeddings C through the BiLSTM encoder introduced in Section 3.1, which are used as the input of DiSA. DiSA explicitly represents sentence positions, which are integrated with the content representations of sentences to get element representations. DiSA also has an inter-sentence attention module to get both element-wise and content-wise attention vectors of sentences to capture sentence interactions. The attention vectors and element representations are concatenated and fed to a linear layer and a softmax layer for prediction.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Sentence Positional Encodings (SPE)", "text": "Discourse elements in argumentative essays are sensitive to their positions. For example, introduction mostly comes before thesis or main ideas and main ideas may occur more often at the beginnings or endings of paragraphs.\nFigure 2 shows an essay with 7 sentences and 4 paragraphs as an example. We consider three types of sentence positions for positional encoding.\n\u2022 Global position: The index of a sentence is used to describe its position where we assume sentences in an essay form a sequence.\n\u2022 Paragraph position: An essay has multiple paragraphs. The position of the paragraph that contains the sentence is also important.\n\u2022 Local position: The position of the sentence in its paragraph is informative as well.\nWe adopt a relative positional encoding approach. We compute the relative positions for the above three position types. For example, the relative global position of the i-th\n(i \u2265 1) sentence in an essay E is pos global (i) = i |E| , (4\n)\nwhere |E| is the number of sentences.\nTo integrate with sentence representations, we expand pos global (i) to a vector of the same dimension d of the distributed sentence representations by duplicating its value to every dimension, noted as pos global (i) \u2208 R d . The relative paragraph position representation pos para (i) and relative local position representation pos local (i) are computed in the same way.  The final position representation pos(i) is formulated as a liner combination of the three relative position representations, i.e.,\npos(i) = t\u2208{global,local,para} \u03b2 t pos t (i),(5)\nwhere {\u03b2 t } are parameters to be learnt in training. The element representation of the i-th sentence is\ne i = tanh(BiLSTM (C i + pos(i))). (6)", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Inter-Sentence Attention (ISA)", "text": "Self-Attention relates elements at different positions by computing attention between every pair of elements. An attention function is to map a query and a set of key-value pairs to an output. The queries Q, keys K and values V are vectors. We define Q, K \u2208 R d k \u00d7n and d k is the dimension. The attention is computed as\n\u03b1 = Attn(Q, K) = softmax( QK T \u221a d k ). (7\n)\nThe output is computed as a weighted sum of the values, i.e., \u03b1V. Here, we are interested in the attention vectors rather than the weighted output, because an attention vector reflects the relatedness of a given sentence to every other sentence. We propose the inter-sentence attention (ISA) by applying self-attention to sentence semantic representations C and discourse element representations E = {e i }.\n\u2022 Element Self-Attention (ElemSA): ElemSA models relations among discourse elements.\nWe use E to get Q and K, Q = EW Q , K = EW K , where W Q , W K \u2208 R d\u00d7d k . We do not use the normalized attention vectors as shown in Equation 7to capture relative relatedness. Instead, we use \u03b1 e = tanh( QK T \u221a d k\n) as attention vectors.\n\u2022 Content Self-Attention (ContSA): ContSA explores content relatedness to model sentence interactions. Similarly to ElemSA, we use the sentence semantic representations C to compute the ContSA vector \u03b1 c . The parameters are independent from ElemSA.\nAdaptive Maxpooling Different essays have different number of sentences. To have a fixed-length attention vector, we borrow the idea of spatial pyramid pooling from image processing (He et al., 2015). It can maintain relatedness information by maxpooling \u03b1 e and \u03b1 c in local bins. These bins have sizes proportional to the number of an essay's sentences so that the number of bins is fixed regardless of the essay length. We set the number of bins to 1, 2, 4 and 8, respectively. The resulted representations can be seen as descriptions of the relatedness of a sentence to different zones of its essay. These representations are concatenated so that the dimension of the pooled attention vectors \u03b1 c , \u03b1 e is 1+2+4+8=15. Finally, the prediction is made according to\nY = softmax linear([\u03b1 e ; \u03b1 c ; E]) , (8\n)\nwhere \u03b1 c , \u03b1 e and E are concatenated.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Datasets", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "The Chinese Dataset", "text": "The construction of the Chinese Dataset mainly follows the definition and taxonomy of discourse elements proposed by Burstein et al. (2003). Specifically, we consider the following discourse elements:  \u2022 Introduction The role of introduction is to introduce background or attract readers' attention before making claims.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "\u2022 Thesis", "text": "The thesis express the central claim of an author with respect to the essay's topic.\n\u2022 Main Idea The ideas establish foundational ideas or aspects that are related to the thesis.\n\u2022 Evidence The evidence elements provide examples or other evidence that are used to support main ideas and thesis.\n\u2022 Elaboration The elaboration elements further explain main ideas or provide reasons, but contain no examples or other evidence.\n\u2022 Conclusion The conclusion sentence is the extension of the central argument, summarizes the full text, and echos the thesis of the essay.\n\u2022 Other Other elements refer to the ones that do not match the above classes.\nThe dataset has 1,230 argumentative essays written by high school students, covering diverse topics. These essays were collected from a website LeleKetang. 1 We asked two annotators from the literal arts college to assign discourse elements to sentences from these essays according to a manual. The annotators discussed to reach a consensus and refined the manual for several rounds. We use one annotator's annotation as the gold answer, and the other's annotation as the prediction, and compute the F1 scores to measure the agreement, which is shown in Figure 3.\nTable 1 shows the basic statistics of the dataset. The distribution of discourse elements is imbalanced. Elaboration and evidence sentences are [To conclude, art could play an active role in improving the quality of people's lives, ]s 1 [ but I think that governments should attach heavier weight to other social issues such as education and housing needs]s 2 [because those are the most essential ways enable to make people a decent life.]s 3   .\nmany more than thesis and main idea sentences. The type of other sentence accounts for a very small percentage of the dataset. The test dataset is 10% of the whole dataset.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "The English Dataset", "text": "We also use the English student essay dataset released by . This dataset marks argument components, i.e., major claim, claim, and premise, at clause level. Table 2 shows an example sentence. The consecutive words in bold form three components, corresponding to claim, major claim and premise, respectively.\nBecause our model is at sentence level, we convert the original annotations to sentence level. First, an essay is split into sentences by NLTK. Then if a sentence contains only one argument component, we annotate this sentence as the type of this component; if a sentence contains more than one argument component, we further separate it into multiple sentences to ensure that each sentence has only one argument. The beginning of a new sentence is from the end of the last component. The end of a new sentence is the end of the component it contains. As shown in Table 2, three sentences s 1 , s 2 and s 3 are generated from the original example sentence. If a sentence does not have any argument component, its label is other. Table 3 shows the basic statistics of the converted dataset.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiment", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Settings", "text": "The max length of sentences is set to 40 words. Sentences are padded or truncated according to this length. The Tencent pre-trained word embeddings (Song et al., 2018) were used for experiments on the Chinese dataset. The dimension of word embeddings is 200. The Bert tokenizer and embeddings were used for experiments on the English dataset. The dimension of all the BiLSTM hidden layers is 256 on Chinese dataset, and 128 on English dataset. So is the dimension of d k . The dimension of the attention vectors is 15. The optimizer is stochastic gradient descent (SGD) with a learning rate 0.1. The best models were selected for all settings based on the results on the validation data, which is 10% of the training data.\nWe use accuracy (Acc.) and macro-F1 as evaluation metrics.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Comparisons", "text": "We compare with the following systems.\n\u2022 Feature-based. We adapt features from previous feature-based methods (Burstein et al., 2003;Stab and Gurevych, 2014;Song et al., 2015) to build a feature-based CRF model.\n\u2022 HBiLSTM. The baseline described in Section 3 uses two BiLSTM layers to encode word sequences and sentences.\n\u2022 BERT. We fine-tune BERT on training data to train a sentence classifier, because the lengths of many Chinese essays exceed the length constraint of BERT and it is expensive to train BERT-like models at discourse level.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Results on the Chinese Dataset", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "System Comparisons", "text": "Table 4 shows the performance of the baselines and DiSA. We can see that HBiLSTM performs even worse than feature-based approach. HBiL-STM has a low macro-F1 score, indicating that it has difficulties in identifying particular discourse elements. The two end-to-end models do not consider position information and interactions among sentences. The performance of BERT is worse than HBiLSTM. This verifies that sequence modeling is more proper than single sentence classification for this task. DiSA achieves the best performance   on all metrics, with a large improvement compared with the baselines.\nFigure 3 further illustrates system performance on identifying specific discourse elements. The human performance is also measured by considering one annotator's annotation as the answer, and the other one's as the prediction.\nThe discourse elements that HBiLSTM is unable to accurately identify are thesis and main idea. Despite their importance for understanding a text, their scale is obviously smaller than other discourse elements, which may bring in obstacles for datadriven approaches.\nFeature-based method performs better than HBiLSTM on identifying thesis and main idea. But it heavily relies on feature-engineering such as manually collected discourse markers and cue words. It does not perform well on identifying evidence due to the difficulties in designing related features.\nDiSA is also an end-to-end model the same as HBiLSTM but performs much better. We will discuss the impacts of positional encoding and intersentence attention in Section 6.3.2 and 6.3.3.\nCompared with the feature-based method, DiSA has comparable performance on identifying thesis but has superior results on identifying main idea (9% higher in F1 score) and evidence (21% higher in F1 score).  ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Analysis of Positional Encodings", "text": "This part investigates the effect of sentence positional encodings. We compare our relative sentence positional encoding (relativeSPE) with two other encoding strategies which are previously used for word sequences. Sinusoidal indicates the sinusoidal positional encoding which is used in Transformer (Vaswani et al., 2017). PosEmbedding uses a distributed vector to represent an absolute position. The position embeddings are learned during training. Each of the above three strategies is applied for modeling global position, local position and paragraph position, which are then combined according to Equation 5. Table 5 lists the results of using different SPEs and modeling different positions. RelativeSPE performs best with improvements of 2-3% macro-F1 score compared with Sinusoidal and PosEmbedding. Without SPE, the metrics drop at least 6.2% compared with using any SPE strategy, and 8.6% compared with relativeSPE. If we explicitly add only pos global , the results even decrease. Perhaps recurrent neural networks such as LSTM naturally capture sequential positional information. However, encoding paragraph position (pos para ) and local position (pos local ) largely improves the performance. This indicates that proper structural positional encodings can exploit richer discourse structures than sequential structures.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Analysis of Inter-Sentence Attention", "text": "Table 6 shows the effects of removing intersentence attention (ISA) components from DiSA. We can see that both ElemSA and ContSA can make contributions, and ElemSA seems to have a larger effect on macro-F1 score. Removing ISA, the accuracy and the macro-F1 score decreases 1.8% and 2.2%.\nRemind that ISA uses attention vectors as representations rather than the final output \u03b1V in the self-attention module.   The result is not good. This indicates that semantic relation among sentences is more important for DEI than the specific meaning of sentences.\nWe further analyze ISA's impact on specific discourse elements. As shown in Table 7, ISA affects the identification of the minority discourse element thesis most. It also benefits identifying evidence which is not a minority discourse element. Thesis sentences often relate to other sentences from different essay zones, while evidence sentences mainly provide facts or examples so they often relate to local context in content. ISA helps capture such patterns. The performance on other types also increases with different degrees.\nAnyway, ISA provides a way to build useful representations by exploiting relations between sentences in the same text without any extra burden.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results on the English Dataset", "text": "Table 8 and Table 9 show main experimental results on the English dataset.\nThe second column of Table 8 shows the results on distinguishing four component types (i.e., major claim, claim, premise, other). DiSA outperforms the baselines with a large margin on both accuracy and macro-F1. Again, removing SPE leads to a large performance decrease.     , where Joint-Best incorporates relation identification as an auxiliary task.\nThe third column of Table 8 shows the comparison to the best results from . DiSA does not perform competitively based on the distributed representation only, because the baseline uses some strong hand-crafted features, especially the component position features, which rely on the correct argument component information. Thus we build a feature vector by incorporating the indicator features and a component position feature: number of preceding and following components in paragraph, out of 8 categories of features introduced in . The vector is concatenated with the distributed representation. This combination obtains improvements, outperforms Single-Best results, and achieves close performance compared with Joint-Best, which considers argumentative relation identification as an auxiliary task. We also attempt to apply the same strategy for the Chinese task. But the improvement is negligible. The reason may be that the indicator phrases used in Chinese essays is much less than in English essays. The English dataset heavily relies on phrases signaling beliefs or argumentative discourse connectors .\nTable 9 shows the macro-F1 scores of DiSA on identifying specific argument components. Without the ISA module, the identification of major claims and claims would decline by 3% and 1.4% absolute F1 score, respectively. This is consistent with the experimental results on the Chinese dataset. As a result, the effectiveness of the SPE and ISA can be verified on both the Chinese and the English datasets.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We presented a method DiSA to identify discourse elements in argumentative student essays by explicitly modeling structural positions and inter-  sentence relations. The structural positional encoding considers relative positions of the sentence and its paragraph. Moreover, we use inter-sentence attention vectors to capture sentence relations in content and function. Experiments on a Chinese dataset and an English dataset show that (i) although it is simple, the positional encoding largely improves the performance. This indicates that modeling structural positions is feasible and important to analyze the role of sentences; (ii) discourse elements could be better identified with the help of inter-sentence attention vectors, especially the minority ones and the ones that have distinct relation patterns to other sentences. In future, we plan to evaluate DiSA on other discourse analysis tasks.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "This work is supported by the National Natural Science Foundation of China (Nos. 61876113, 61876112), Beijing Natural Science Foundation (No. 4192017), Support Project of Highlevel Teachers in Beijing Municipal Universities in the Period of 13th Five-year Plan (CIT&TCD20170322) and Capital Building for Sci-Tech Innovation-Fundamental Scientific Research Funds. Lizhen Liu is the corresponding author.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Automated essay scoring with e-rater v. 2. The Journal of Technology", "journal": "Learning and Assessment", "year": "2006", "authors": "Yigal Attali; Jill Burstein"}, {"title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2015", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Enriching automated essay scoring using discourse marking", "journal": "", "year": "2001", "authors": "Jill Burstein; Karen Kukich; Susanne Wolff; Chi Lu; Martin Chodorow"}, {"title": "Finding the write stuff: Automatic identification of discourse structure in student essays", "journal": "IEEE Intelligent Systems", "year": "2003", "authors": "Jill Burstein; Daniel Marcu; Kevin Knight"}, {"title": "What is the essence of a claim? cross-domain claim identification", "journal": "", "year": "2017", "authors": "Johannes Daxenberger; Steffen Eger; Ivan Habernal; Christian Stab; Iryna Gurevych"}, {"title": "Neural end-to-end learning for computational argumentation mining", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Steffen Eger; Johannes Daxenberger; Iryna Gurevych"}, {"title": "Evaluating discourse in structured text representations", "journal": "", "year": "2019", "authors": "Elisa Ferracane; Greg Durrett; Junyi Jessy Li; Katrin Erk"}, {"title": "Identifying the information structure of scientific abstracts: an investigation of three different schemes", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Yufan Guo; Anna Korhonen; Maria Liakata; Ilona Silins Karolinska; Lin Sun; Ulla Stenius"}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2015", "authors": "X Kaiming He; Shaoqing Zhang; Jian Ren;  Sun"}, {"title": "Evaluating multiple aspects of coherence in student essays", "journal": "", "year": "2004", "authors": "Derrick Higgins; Jill Burstein; Daniel Marcu; Claudia Gentile"}, {"title": "Identifying sections in scientific abstracts using conditional random fields", "journal": "", "year": "2008", "authors": "Kenji Hirohata; Naoaki Okazaki; Sophia Ananiadou; Mitsuru Ishizuka"}, {"title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"title": "Predicting discourse structure using distant supervision from sentiment", "journal": "", "year": "2019", "authors": "Patrick Huber; Giuseppe Carenini"}, {"title": "Neural discourse structure for text categorization", "journal": "", "year": "2017", "authors": "Yangfeng Ji; A Noah;  Smith"}, {"title": "Discourse parsing with attention-based hierarchical neural networks", "journal": "", "year": "2016", "authors": "Qi Li; Tianshi Li; Baobao Chang"}, {"title": "Contextindependent claim detection for argument mining", "journal": "", "year": "2015", "authors": "Marco Lippi; Paolo Torroni"}, {"title": "Argument mining from speech: Detecting claims in political debates", "journal": "", "year": "2016", "authors": "Marco Lippi; Paolo Torroni"}, {"title": "Learning structured text representations", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Yang Liu; Mirella Lapata"}, {"title": "Discourseaware semantic self-attention for narrative reading comprehension", "journal": "", "year": "2019", "authors": "Todor Mihaylov; Anette Frank"}, {"title": "Argumentation mining", "journal": "Artificial Intelligence and Law", "year": "2011", "authors": "Raquel Mochales; Marie-Francine Moens"}, {"title": "Automatic detection of arguments in legal texts", "journal": "ACM", "year": "2007", "authors": "Marie-Francine Moens; Erik Boiy"}, {"title": "Extracting argument and domain words for identifying argument components in texts", "journal": "", "year": "2015", "authors": "Huy Nguyen; Diane Litman"}, {"title": "Ontology-based argument mining and automatic essay scoring", "journal": "", "year": "2014", "authors": "Nathan Ong; Diane Litman; Alexandra Brusilovsky"}, {"title": "Argumentation mining: the detection, classification and structure of arguments in text", "journal": "ACM", "year": "2009", "authors": "Raquel Mochales Palau; Marie-Francine Moens"}, {"title": "International Conference on Machine Learning", "journal": "", "year": "2018", "authors": "Niki Parmar; Ashish Vaswani; Jakob Uszkoreit; Lukasz Kaiser; Noam Shazeer; Alexander Ku; Dustin Tran"}, {"title": "Joint prediction in mst-style discourse parsing for argumentation mining", "journal": "", "year": "2015", "authors": "Andreas Peldszus; Manfred Stede"}, {"title": "Modeling organization in student essays", "journal": "", "year": "2010", "authors": "Isaac Persing; Alan Davis; Vincent Ng"}, {"title": "End-to-end argumentation mining in student essays", "journal": "", "year": "2016", "authors": "Isaac Persing; Vincent Ng"}, {"title": "Self-attention with relative position representations", "journal": "", "year": "2018", "authors": "Peter Shaw; Jakob Uszkoreit; Ashish Vaswani"}, {"title": "Disan: Directional self-attention network for rnn/cnn-free language understanding", "journal": "", "year": "2018", "authors": "Tao Shen; Tianyi Zhou; Guodong Long; Jing Jiang; Shirui Pan; Chengqi Zhang"}, {"title": "Discourse element identification in student essays based on global and local cohesion", "journal": "", "year": "2015", "authors": "Wei Song; Ruiji Fu; Lizhen Liu; Ting Liu"}, {"title": "Hierarchical multi-task learning for organization evaluation of argumentative student essays", "journal": "", "year": "2020", "authors": "Wei Song; Ziyao Song; Lizhen Liu; Ruiji Fu"}, {"title": "Directional skip-gram: Explicitly distinguishing left and right context for word embeddings", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yan Song; Shuming Shi; Jing Li; Haisong Zhang"}, {"title": "Applying argumentation schemes for essay scoring", "journal": "", "year": "2014", "authors": "Yi Song; Michael Heilman; Beata Beigman Klebanov; Paul Deane"}, {"title": "Self-attentional models for lattice inputs", "journal": "", "year": "2019", "authors": "Matthias Sperber; Graham Neubig; Ngoc-Quan Pham; Alex Waibel"}, {"title": "Identifying argumentative discourse structures in persuasive essays", "journal": "", "year": "2014", "authors": "Christian Stab; Iryna Gurevych"}, {"title": "Parsing argumentation structures in persuasive essays", "journal": "Computational Linguistics", "year": "2017", "authors": "Christian Stab; Iryna Gurevych"}, {"title": "Argumentative zoning: Information extraction from scientific text", "journal": "Citeseer", "year": "1999", "authors": "Simone Teufel"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Using argument mining to assess the argumentation quality of essays", "journal": "", "year": "2016", "authors": "Henning Wachsmuth; Al Khalid; Benno Khatib;  Stein"}, {"title": "Convolutional selfattention networks", "journal": "", "year": "2019", "authors": "Baosong Yang; Longyue Wang; Derek F Wong; Lidia S Chao; Zhaopeng Tu"}, {"title": "Hierarchical attention networks for document classification", "journal": "", "year": "2016", "authors": "Zichao Yang; Diyi Yang; Chris Dyer; Xiaodong He; Alex Smola; Eduard Hovy"}], "figures": [{"figure_label": "12", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :Figure 2 :12Figure 1: The architecture of Discourse Self-Attention.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: F1 scores on identifying specific discourse elements.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Basic statistics of the Chinese dataset.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "A sentence from the dataset of, with clause level component annotations (in bold), are split into three individual sentences s 1 , s 2 and s 3 .", "figure_data": "ElementTrainTest Total%Major Claim598153751 10.3Claim1,202304 1,506 20.6Premise3,023809 3,832 52.3Other999232 1,231 16.8Total5,822 1,498 7,320# essays32280402Avg. #sentences per essay19Avg. #words per sentence20"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "System comparisons.    ", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The effects of different SPEs.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "also lists the performance that \u03b1V is used to replace attention vectors.", "figure_data": "ISA TypeAcc. macro-F1DiSA0.6810.657\u2212 ContSA0.6750.646\u2212 ElemSA0.6770.647\u2212 ISA0.6630.635ISA with \u03b1V 0.6180.600"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "The effects of inter-sentence attention (ISA).", "figure_data": "ModelDiSA \u2212 ISA\u2206Introduction 0.7960.792\u22120.4%Thesis0.3830.338 \u22124.5%Main Idea0.5770.573\u22120.4%Evidence0.6270.578 \u22124.9%Elaboration0.6890.677\u22121.2%Conclusion0.8680.850 \u2212 1.8%"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Macro-F1 scores on identifying specific discourse elements.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "conducted argument component classification experiments (classifying a component into major claim, claim and premise) by assuming that argument components have been correctly distinguished from other parts. To compare with their results, during training, the other type is removed from the label set and only the losses over non-other sentences are accumulated.", "figure_data": "4 classes3 classesModelAcc.macro-F1Acc.macro-F1BERT0.6730.596--HBiLSTM0.6800.501--DiSA0.7720.6990.8060.742DiSA -SPE0.6870.5290.7100.534DiSA+Feature--0.8390.807Eger et al. (2017)---0.730Single-Best---0.773Joint-Best--0.8500.826"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Comparisons on the English dataset. Single-BEST and Joint-Best indicate the best results reported in", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Macro-F1 scores on identifying specific argument components on the English dataset.", "figure_data": ""}], "doi": "10.18653/v1/P17-1002"}