{"authors": "Akash Gautam; Debanjan Mahata; Rakesh Gosangi; Rajiv Ratn", "pub_date": "", "title": "Semi-Supervised Iterative Approach for Domain-Specific Complaint Detection in Social Media", "abstract": "In this paper, we present a semi-supervised bootstrapping approach to detect product or service related complaints in social media. Our approach begins with a small collection of annotated samples which are used to identify a preliminary set of linguistic indicators pertinent to complaints. These indicators are then used to expand the dataset. The expanded dataset is again used to extract more indicators. This process is applied for several iterations until we can no longer find any new indicators. We evaluated this approach on a Twitter corpus specifically to detect complaints about transportation services. We started with an annotated set of 326 samples of transportation complaints, and after four iterations of the approach, we collected 2,840 indicators and over 3,700 tweets. We annotated a random sample of 700 tweets from the final dataset and observed that nearly half the samples were actual transportation complaints. Lastly, we also studied how different features based on semantics, orthographic properties, and sentiment contribute towards the prediction of complaints.", "sections": [{"heading": "Introduction", "text": "Social media has lately become one of the primary venues where users express their opinions about various products and services. These opinions are extremely useful in understanding the user's perceptions and sentiment about these services. They are also useful in identifying potential defects (Abrahams et al., 2012) and thus critical to the execution of downstream customer service responses. Therefore, automatic detection of user complaints on social media could prove beneficial to both the clients and the service providers. To build such detection systems, we could employ supervised approaches that would typically require a large corpus of labeled training samples. However, labeling social media posts that capture complaints about a particular service is challenging because of their low prevalence and also the vast amounts of inevitable noise (Kietzmann et al., 2011;Lee, 2018). Additionally, social media platforms are also likely to be plagued with redundancy, where the posts are rephrased or structurally morphed before being re-posted (Ellison et al., 2011;Harrigan et al., 2012).\nPrior work in event detection (Ritter et al., 2012) has demonstrated that simple linguistic indicators (phrases or n-grams) can be useful in the accurate discovery of events in social media. Though user complaints are not the same as events, more of a speech act (Preotiuc-Pietro et al., 2019), we posit that similar indicators can be used in complaint detection. To pursue this hypothesis, we propose a semi-supervised iterative approach to identify social media posts that complain about a specific service.\nIn our approach, we first begin with a small, manually curated dataset containing samples of social media posts complaining about a service. We then identify linguistic indicators (phrases or n-grams) that serve as strong evidence of this phenomenon. These indicators are then used to extract more posts from the unannotated corpus. This newly obtained data is then used to create a new set of indicators. This process is repeated until it reaches a certain convergence point. Since the set of indicators is growing after each iteration, they are re-evaluated continuously in terms of their relevance. This process is similar to the mutual bootstrapping approach for information extraction proposed in (Riloff et al., 2003).\nWe employ this approach to the problem of complaint detection for transportation services on Twitter. Transportation and its related logistic services are critical aspects of every economy as they account for nearly 40% of the value of international trade (Rodrigue, 2007). As with most businesses (Gallaugher and Ransbotham, 2010;Gottipati et al., 2018), transportation also often relies on social media to ascertain feedback and initiate appropriate responses (Stelzer et al., 2016(Stelzer et al., , 2014. In our experimental work, we started with an annotated set of 326 samples of transportation complaints, and after four iterations of the approach, we collected 2,840 indicators and over 3,700 tweets. We annotated a random sample of 700 tweets from the final dataset and observed that over 47% of the samples were actual transportation complaints. We also characterize the performance of basic classification algorithms on this dataset. In doing so, we also study how different linguistic features contribute to the performance of a supervised model in this domain.\nThe main contributions of this paper are as follows:\n\u2022 We propose a semi-supervised iterative approach to collect user complaints about a service from social media platforms.\n\u2022 We evaluate the proposed approach for the problem of complaint detection for transportation services on Twitter.\n\u2022 We annotate a random sample of the resulting dataset to establish that nearly half the tweets were actual complaints.\n\u2022 We release a curated dataset for the task of traffic-related complaint detection in social media 1 .\n\u2022 Lastly, we characterize the performance of basic classification algorithms on the dataset.", "n_publication_ref": 13, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Complaints are often considered dialogue acts used to express a mismatch between the expectation and reality (Olshtain and Weinbach, 1985). The problem of complaint detection is of great interest to the marketing and research teams of various service providers. Previous works on complaint identification have applied text mining with LDA and sentiment analysis on user-generated content Duan et al., 2013). Prior works have also focused on leveraging data streamed from social media platforms for outage and complaint detection as they are publicly available (Augustine et al., 2012;Kursar and Gopinath, 2013). (Yang et al., 2019) inspected customer support dialogue for support. Different complaint expressions have been explored by analyzing variations across cultures (Cohen and Olshtain, 1993), sociodemographic traits (Boxer, 1993) and temporal representations (Raghavan, 2014). However, mentioned works on user-generated content have focused on static data repositories only. These have not been robust to linguistic variations (Shah and Zimmermann, 2017) and morphological changes (Abdul-Mageed and Korayem, 2010). Our pipeline builds on linguistic identifiers to expand on lexical cues in order to identify complaint relevant posts.\nResearches have proposed many semisupervised architectures for identification of events pertaining to societal and civil unrest (Hua et al., 2013), using speech modality (Serizel et al., 2018;Wu et al., 2014;Zhang et al., 2017) and Hidden Markov Models (Zhang, 2005). These have been documented to give better performance as compared against their counterparts (Lee et al., 2017;Zheng et al., 2017) with minimal intervention (Rahimi et al., 2018). For our analysis, the semi-supervised approach has been preferred as opposed to supervised ones because: (a) usage of supervised approach relies on carefully choosing the training set making it cumbersome and less attractive for practical use (Watanabe, 2018) and (b) imbalance between the subjective and objective classes lead to poor performance (Yu et al., 2015).", "n_publication_ref": 20, "n_figure_ref": 0}, {"heading": "Methods and Data", "text": "Our proposed approach begins with a large corpus of transport-related tweets and a small set of annotated complaints. We use this labeled data to create a set of seed indicators that drive the rest of our iterative complaint detection process.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Seed Data", "text": "We focused our experimentation over the period of November 2018 to December 2018. Our first step towards creating a corpus of transportrelated tweets is to identify linguistic markers related to the transport domain. To this end, we scraped random posts from transport-related web forums 2 . These forums involve users discussing their grievances and raising awareness about a wide array of transportation-related issues. We then processed this data to extract words and phrases (unigrams, bigrams, and trigrams) with high tf-idf scores. We then had human annotators prune them further to remove duplicates and irrelevant items. This resulted in a lexicon of 75 unique phrases. Some examples include cabs, discount, tickets, underground, luggage, transit, parking, neighborhood, downtown, traffic, Uber.\nWe used Twitter's public streaming API to query for tweets that contained any of the 75 phrases over the chosen time range. We then excluded non-English tweets and any tweets with less than two tokens. This resulted in a collection of 19,300 tweets. We will refer to this collection as corpus C. We chose a random sample of 1,500 tweets from this collection for human annotation. We employed two human annotators to identify traffic-related complaints from these 1,500 tweets. Following are some high-level details of the annotation task.\nWe instructed the annotators to identify any tweets that contain first-hand accounts of a complaint or a grievance related to a public/private mode of transport. Following is a sample tweet from this instruction: \"@[UserHandle] can you please make sure that compartment A-6 is at least clean before public use.\" We also instructed them to identify tweets that provide verifiable sources of information (news) about transport-related services. Sample tweet: \"4 hour jam in [place] area due to rain and poor management of traffic police.\". Lastly, we also explicitly asked them to exclude tweets that contain announcements or advertisements about transportation services. Sample tweet: \"Please use [name] cabs, you will get 60% discount on your first 3 rides.\"\nThe two annotators worked independently, and when we finally tallied their responses, we observed that they had an inter-annotator agreement rate of \u03ba = 0.81 (Cohen kappa). In cases where the annotators disagreed, the labels were resolved through a discussion. After the disagreements were resolved, the final seed dataset had 326 samples of traffic-related complaints. We will refer to this as T s . Table 1 shows some examples of tweets that were annotated as complaints.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Iterative Complaint Detection", "text": "Our proposed iterative approach is summarized in Algorithm 1. First, we use the seed data T s to build a set of linguistic indicators I for complaints.\nWe then use these indicators to get potential new complaints T l from the corpus C. We merge T s and T l to build our new dataset. We then use this new dataset to extract a new set of indicators I l . The indicators are combined with the original indicators I to extract the next version of T l . This process is repeated until we can no longer find any new indicators. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Extracting linguistic indicators", "text": "As shown in Algorithm 1, extracting linguistic indicators (n-grams) is one of the most important steps in the process. These indicators are critical to identifying tweets that are most likely domainspecific complaints. We employ two different approaches for extracting these indicators. For seed data, T s , which is annotated, we just select n-grams with the highest tf-idf scores. In our experimental work, T s had 326 annotated tweets. We identified 50 n-grams with the highest tf-idf scores to initialize I. Some examples include: problem, station, services, toll-fee, reply, fault, provide information, driver, district, passenger. In subsequent iterations, when we are handling unannotated samples, we use a more advanced domain relevance criterion for extracting the indicators. When extracting indicators from T l , which is not annotated, it is possible that there could be frequently occurring phrases that are not necessarily indicative of complaints. These phrases could lead to a concept drift in subsequent iterations. To avoid these digressions, we use a measure of domain relevance when selecting indicators. This is defined as the ratio of the frequency of an n-gram in T l to that of in T r . T r is a collection of randomly chosen tweets that do not intersect with C. In our experimental work, we defined T r as a random sample of 5,000 tweets from a different time range than that of C. We also wanted to quantitatively en-Samples of transport-related complaints. 1. No metro fares will be reduced, but proper fare structure needs to be introduced .... right?. 2. It takes [name] govt. longer to refund charges, but it took them a few mins to remove that bus stop. You can't erase the problem[name]. 3. I tried to lodge a complaint on [url] but see the results. Sir if 8 A.C's are not working in this coach , why have you attached that coach.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "4.", "text": "[name] Is that for when people can't travel due to your staff having to strike to keep everyone safe? Or perhaps short formed trains that you cant get on. sure that the lexicon in T r is different from that of C. Namely, we calculated the cosine similarity between the two datasets in the tf-idf space. The cosine similarity at a value of 0.028 was statistically significant with a Pearson correlation coefficient value 0.012 (p-value 0.0034) (Schober et al., 2018).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Selection of tweets", "text": "Given a set of indicators I, the process of selecting tweets from corpus C is fairly straightforward. It only requires to identify all the tweet that contains any of the indicators. The only caveat here is to reduce the redundancy in the dataset. For this, we just filtered out tweets that have a cosine similarity of more than 0.85 with any other tweet in the tf-idf space (Albakour et al., 2013). This process also helped remove tweets, which are exact matches, sub-strings, or differing by some punctuation. Removal of these redundant tweets also helps in diversifying the lexicon for subsequent iterations.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Complaints dataset", "text": "Our iterative approach converged in four rounds, after which it did not extract any new indicators. Figure 1 shows the counts of indicators and the number of tweets after each iteration. After four iterations, this approach chose 3,732 tweets and generated 2,840 unique indicators. We also manually inspected the indicators chosen during the process. We observed that only indicators with a domain relevance score of \u2265 2.5 were chosen for subsequent iterations. Table 2 provides a few examples of strong and weak indicators acquired after the first iteration. In this figure, strong indicators are those with a domain relevance score \u2265 2.5.\nWe chose a random set of 700 tweets from the final complaints dataset T and annotated them manually to help understand the quality. We used the same guidelines as discussed in section 3.1 and also employed the same annotators as before. The anno-  tators once again obtained a high agreement score of \u03ba = 0.83. After resolving the disagreements, we observed that 332 tweets were labeled as complaints. This accounts for 47.4% of the sampled 700 tweets. This demonstrates that nearly half the tweets selected by our semi-supervised approach were traffic-related complaints. This is a significantly higher proportion in the original seed data T s , where only 21.7% were actual complaints.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Modeling", "text": "We conducted a series of experiments to understand if we can automatically build simple machine learning models to detect complaints. These experiments also helped us evaluate the quality of the final dataset. Additionally, this experimental work also studies how different types of linguistic features contribute to the detection of social media complaints. For these experiments, we used the annotated sample of 700 posts as a test dataset. We built our training dataset by selecting another 2,000 posts from the original corpus C, and anno-   tated them once again per guidelines discussed in section 3.1. In this sample, we observed that the annotators had similar agreements scores of \u03ba = 0.79, and there were 702 instances of complaints.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Features", "text": "We also wanted to understand the predictive power of different types of linguistic features towards the detection of complaints. These features can be broadly broken down into four groups. (i) The first group of features are based on simple semantic properties such as n-grams, word embeddings, and part of speech tags. (ii) The second group of features are based on pre-trained sentiment models or lexicons. (iii) The third group of features use orthographic information such as hashtags, user mentions, and intensifiers. (iv) The last group of features again use pre-trained models or lexicons associated with request, which is a closely related speech act (\u0160v\u00e1rov\u00e1, 2008).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Semantic features", "text": "We experimented with four different semantic features: Unigrams: Each tweet (Wallach, 2006) is represented as sparse vector of tf-idf values correspond-ing to the constituent tokens.\nWord2Vec Clusters: We follow the same approach as in (Preo\u0163iuc-Pietro et al., 2015), where words are clustered using pair-wise similarities in Word2Vec space (Mikolov et al., 2013). Each tweet is then represented as a distribution over these clusters; the values are proportional to the number of tokens belonging to a cluster. These clusters have previously been demonstrated to have great interpretability (Preo\u0163iuc-Pietro et al., 2015Zou et al., 2016).\nPOS Tags: We used the Stanford POS Tagger (Manning et al., 2014) to represent tweets as a dense frequency vector over five main POS tags: nouns, adjectives, adverbs, verbs, pronouns.\nPronoun Types: Pronouns are often used in complaints and suggestions to reveal personal involvement or to add intensity to an opinion (Claridge, 2007;Meinl, 2013). We identify various pronoun types (first person, second person, third person, demonstrative, indefinite) using dictionaries and use their counts as features.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Sentiment features", "text": "We expect sentiment to contribute strongly towards the prediction of complaints. We experiment with two pre-trained models: Stanford Sentiment (Socher et al., 2013) and VADER (Hutto and Gilbert, 2014). Namely, we use the scores predicted by these models as representations of tweets. Likewise, we also experiment with two sentiment lexicons: MPQA (Wilson et al., 2005), NRC (Mohammad et al., 2013) for assigning sentiment scores to tweets.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Orthographic features", "text": "Our first set of orthographic feature uses counts of URLs, hashtags, user mentions, and special symbols used in the post. The second set of orthographic features try to identify potential intensifiers such as capitalization and repeated use of exclamation or question marks. These types of intensifiers are often used to express anger or strong opinions (Meinl, 2013).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Request features", "text": "A request is a speech act very closely related to complaints. Often, the main motivation behind a complaint on a social media platform is to get a correction or reparation from the service providers (Blum-Kulka and Olshtain, 1984). We use the model presented in (Danescu-Niculescu-Mizil et al., 2013) to detect if a given tweet is a request. Requests might also often include polite phrases in expectation of better service. They are coded using various dictionaries e.g, downgraders (little), down-toners (just), hedges (somewhat). Apology markers have the same effect as politeness markers, they may include greetings at the start (Good Morning), direct start (e.g so), subjunctive phrases (could you) (\u0160v\u00e1rov\u00e1, 2008). We utilize pre-defined dictionaries to determine the presence of politeness identifiers along with the politeness score of the tweet based on the model in (Danescu-Niculescu-Mizil et al., 2013).", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Results", "text": "We trained a logistic regression model for complaint detection using each one of the features described in section 4.1. Table 3 summarizes the results in terms of accuracy and macro averaged F1-score. The best performing model is based on unigrams, with an accuracy of 75.3%. There is not a significant difference in the performance of different sentiment models. It is also interesting to observe that simple features like the counts of different pronoun types and counts of intensifiers have strong predictive ability. Overall, we observe that most of the features studied here have some ability to predict complaints.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion and Future Work", "text": "In this paper, we presented a semi-supervised iterative approach for the detection of complaints in social media platforms. The process begins with a small sample of annotated examples, and then iteratively builds more linguistic identifiers to expand the dataset. We evaluated this approach on the domain of transportation on Twitter, starting with a sample of 326 annotated tweets. After four iterations, we were able to construct a corpus with over 3,700 tweets. Annotation of random samples established that nearly half the tweets were actual complaints. We evaluated the predictive power based on semantic, orthographic, and sentiment features. We observed that complaint is a complex speech act, which is related to many other linguistic properties.\nAutomatic detection of complaints is not only useful to service providers as feedback; it could also prove helpful in improving service providers' operations and in downstream applications such as developing chat-bots. Additionally, it could also be of interest to linguists in understanding how humans express grievances and criticism.\nThis proposed methodology could be applied to many other products or services to detect complaints. This would only additionally require some lexicons and a small annotated dataset. We also expect it would be fairly straightforward to adapt this technique to many other types of speech acts. Further investigation is necessary to understand how this method compares against supervised or completely unsupervised techniques.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Automatic identification of subjectivity in morphologically rich languages: the case of arabic. Computational approaches to subjectivity and sentiment analysis", "journal": "", "year": "2010", "authors": ""}, {"title": "Vehicle defect discovery from social media", "journal": "Decision Support Systems", "year": "2012", "authors": "Jian Alan S Abrahams; Alan Jiao; Weiguo Wang;  Fan"}, {"title": "On sparsity and drift for effective real-time filtering in microblogs", "journal": "ACM", "year": "2013", "authors": "M Albakour; Craig Macdonald; Iadh Ounis"}, {"title": "Outage detection via real-time social stream analysis: leveraging the power of online complaints", "journal": "", "year": "2012", "authors": "Eriq Augustine; Cailin Cushing; Alex Dekhtyar; Kevin Mcentee; Kimberly Paterson; Matt Tognetti"}, {"title": "Requests and apologies: A cross-cultural study of speech act realization patterns (ccsarp)", "journal": "", "year": "1984", "authors": "Shoshana Blum-Kulka; Elite Olshtain"}, {"title": "Social distance and speech behavior: The case of indirect complaints", "journal": "Journal of pragmatics", "year": "1993", "authors": ""}, {"title": "The superlative in spoken english", "journal": "Brill", "year": "2007", "authors": "Claudia Claridge"}, {"title": "The production of speech acts by efl learners", "journal": "Tesol Quarterly", "year": "1993", "authors": "D Andrew; Elite Cohen;  Olshtain"}, {"title": "No country for old members: User lifecycle and linguistic change in online communities", "journal": "ACM", "year": "2013", "authors": "Cristian Danescu-Niculescu-Mizil; Robert West; Dan Jurafsky; Jure Leskovec; Christopher Potts"}, {"title": "Mining online user-generated content: using sentiment analysis technique to study hotel service quality", "journal": "IEEE", "year": "2013", "authors": "Wenjing Duan; Qing Cao; Yang Yu; Stuart Levy"}, {"title": "Negotiating privacy concerns and social capital needs in a social media environment", "journal": "Springer", "year": "2011", "authors": "Jessica Nicole B Ellison; Charles Vitak; Rebecca Steinfield; Cliff Gray;  Lampe"}, {"title": "Social media and customer dialog management at starbucks", "journal": "MIS Quarterly Executive", "year": "2010", "authors": "John Gallaugher; Sam Ransbotham"}, {"title": "Text analytics approach to extract course improvement suggestions from students' feedback", "journal": "", "year": "2018", "authors": "Swapna Gottipati; Venky Shankararaman; Jeff Rongsheng Lin"}, {"title": "Influentials, novelty, and social contagion: The viral power of average friends, close communities, and old news", "journal": "Social Networks", "year": "2012", "authors": "Nicholas Harrigan; Palakorn Achananuparp; Ee-Peng Lim"}, {"title": "Sted: semi-supervised targeted-interest event detectionin in twitter", "journal": "", "year": "2013", "authors": "Ting Hua; Feng Chen; Liang Zhao; Chang-Tien Lu; Naren Ramakrishnan"}, {"title": "Vader: A parsimonious rule-based model for sentiment analysis of social media text", "journal": "", "year": "2014", "authors": "J Clayton; Eric Hutto;  Gilbert"}, {"title": "Social media? get serious! understanding the functional building blocks of social media", "journal": "Business horizons", "year": "2011", "authors": "Kristopher Jan H Kietzmann;  Hermkens; P Ian; Bruno S Mc-Carthy;  Silvestre"}, {"title": "Validating customer complaints based on social media postings", "journal": "", "year": "2013", "authors": "Brian Kursar; Jayadev Gopinath"}, {"title": "Social media analytics for enterprises: Typology, methods, and processes. Business Horizons", "journal": "", "year": "2018", "authors": "In Lee"}, {"title": "Adverse drug event detection in tweets with semi-supervised convolutional neural networks", "journal": "", "year": "2017", "authors": "Kathy Lee; Ashequl Qadir; A Sadid; Vivek Hasan; Aaditya Datla; Joey Prakash; Oladimeji Liu;  Farri"}, {"title": "An investigation of brand-related user-generated content on twitter", "journal": "Journal of Advertising", "year": "2017", "authors": "Xia Liu; Alvin C Burns; Yingjian Hou"}, {"title": "The stanford corenlp natural language processing toolkit", "journal": "", "year": "2014", "authors": "Christopher Manning; Mihai Surdeanu; John Bauer; Jenny Finkel; Steven Bethard; David Mcclosky"}, {"title": "Electronic complaints: an empirical study on British English and German complaints on eBay", "journal": "Frank & Timme GmbH", "year": "2013", "authors": "E Marja;  Meinl"}, {"title": "Efficient estimation of word representations in vector space", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"title": "Nrc-canada: Building the stateof-the-art in sentiment analysis of tweets", "journal": "", "year": "2013", "authors": "M Saif; Svetlana Mohammad; Xiaodan Kiritchenko;  Zhu"}, {"title": "Complaints-A study of speech act behavior among native and nonnative speakers of Hebrew", "journal": "", "year": "1985", "authors": "Elite Olshtain; Liora Weinbach"}, {"title": "Mihaela Gaman, and Nikolaos Aletras. 2019. Automatically identifying complaints in social media", "journal": "", "year": "", "authors": "Daniel Preotiuc-Pietro"}, {"title": "An analysis of the user occupational class through twitter content", "journal": "", "year": "2015", "authors": "Daniel Preo\u0163iuc-Pietro; Vasileios Lampos; Nikolaos Aletras"}, {"title": "Beyond binary labels: political ideology prediction of twitter users", "journal": "Long Papers", "year": "2017", "authors": "Daniel Preo\u0163iuc-Pietro; Ye Liu; Daniel Hopkins; Lyle Ungar"}, {"title": "Medical event timeline generation from clinical narratives", "journal": "", "year": "2014", "authors": "Preethi Raghavan"}, {"title": "Semi-supervised user geolocation via graph convolutional networks", "journal": "", "year": "2018", "authors": "Afshin Rahimi; Trevor Cohn; Timothy Baldwin"}, {"title": "Learning subjective nouns using extraction pattern bootstrapping", "journal": "", "year": "2003", "authors": "Ellen Riloff; Janyce Wiebe; Theresa Wilson"}, {"title": "Open domain event extraction from twitter", "journal": "ACM", "year": "2012", "authors": "Alan Ritter; Oren Etzioni; Sam Clark"}, {"title": "Transportation and globalization", "journal": "", "year": "2007", "authors": "Jean-Paul Rodrigue"}, {"title": "Correlation coefficients: appropriate use and interpretation", "journal": "", "year": "2018", "authors": "Patrick Schober; Christa Boer; Lothar A Schwarte"}, {"title": "", "journal": "Anesthesia & Analgesia", "year": "", "authors": ""}, {"title": "Large-scale weakly labeled semi-supervised sound event detection in domestic environments", "journal": "", "year": "2018", "authors": "Romain Serizel; Nicolas Turpault; Hamid Eghbal-Zadeh; Ankit Parag Shah"}, {"title": "Multimodal analysis of user-generated multimedia content", "journal": "Springer", "year": "2017", "authors": "Rajiv Shah; Roger Zimmermann"}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "journal": "", "year": "2013", "authors": "Richard Socher; Alex Perelygin; Jean Wu; Jason Chuang; D Christopher; Andrew Manning; Christopher Ng;  Potts"}, {"title": "Using customer feedback in public transportation systems", "journal": "IEEE", "year": "2014", "authors": "Anselmo Stelzer; Frank Englert; Stephan H\u00f6rold; Cindy Mayas"}, {"title": "Improving service quality in public transportation systems using automated customer feedback", "journal": "Transportation Research Part E: Logistics and Transportation Review", "year": "2016", "authors": "Anselmo Stelzer; Frank Englert; Stephan H\u00f6rold; Cindy Mayas"}, {"title": "Politeness markers in spoken language", "journal": "", "year": "2008", "authors": " Jana\u0161v\u00e1rov\u00e1"}, {"title": "Topic modeling: beyond bag-of-words", "journal": "ACM", "year": "2006", "authors": "M Hanna;  Wallach"}, {"title": "Newsmap: A semi-supervised approach to geographical news classification", "journal": "Digital Journalism", "year": "2018", "authors": "Kohei Watanabe"}, {"title": "Recognizing contextual polarity in phraselevel sentiment analysis", "journal": "", "year": "2005", "authors": "Theresa Wilson; Janyce Wiebe; Paul Hoffmann"}, {"title": "Zeroshot event detection using multi-modal fusion of weakly supervised concepts", "journal": "", "year": "2014", "authors": "Shuang Wu; Sravanthi Bondugula; Florian Luisier; Xiaodan Zhuang; Pradeep Natarajan"}, {"title": "Detecting customer complaint escalation with recurrent neural networks and manuallyengineered features", "journal": "", "year": "2019", "authors": "Wei Yang; Luchen Tan; Chunwei Lu; Anqi Cui; Han Li; Xi Chen; Kun Xiong; Muzi Wang; Ming Li; Jian Pei"}, {"title": "A semisupervised learning approach for microblog sentiment classification", "journal": "", "year": "2015", "authors": "Z Yu; R K Wong; C Chi; F Chen"}, {"title": "Revealing event saliency in unconstrained video collection", "journal": "", "year": "2017", "authors": "Dingwen Zhang; Junwei Han; Lu Jiang; Senmao Ye; Xiaojun Chang"}, {"title": "Mining inter-entity semantic relations using improved transductive learning", "journal": "Full Papers", "year": "2005", "authors": "Zhu Zhang"}, {"title": "Semi-supervised event-related tweet identification with dynamic keyword generation", "journal": "", "year": "2017", "authors": "Xin Zheng; Aixin Sun; Sibo Wang; Jialong Han"}, {"title": "On infectious intestinal disease surveillance using social media content", "journal": "ACM", "year": "2016", "authors": "Bin Zou; Vasileios Lampos; Russell Gorton; Ingemar J Cox"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Algorithm 1 :1Iterative Complaint Detection Given: Corpus: C, Seed data: T s Get indicators I from T s T = T s Complaint Detection loop Step 1: Select set T l from C using I Step 2: T = T \u222a T l Step 3: Get indicators I l from T Step 4: I = I \u222a I l Step 4: C = C \u2212 T l", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Number of indicators and tweets collected after each iteration.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Sample tweets annotated as transport-related complaints.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Examples of some strong and weak indicators. The numbers in brackets denote the respective domain relevance score.", "figure_data": "FeatureAccuracy(%) F1-scoreSemantic FeaturesUnigrams75.30.70POS Tags70.10.66Word2Vec cluster72.10.67Pronoun Types69.60.65Sentiment FeaturesMPQA68.20.61NRC67.90.59VADER68.00.62Stanford Sentiment68.70.63Orthographic FeaturesTextual Meta-data69.30.62Intensifiers72.50.67Request FeaturesRequest Model70.10.66Politeness Markers70.40.63"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Predictive accuracy and F1-score associated with different types of features.", "figure_data": ""}], "doi": "10.1109/SmartCity.2015.94"}