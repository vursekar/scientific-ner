{"authors": "", "pub_date": null, "title": "", "abstract": "Neural machine translation (NMT) has achieved impressive performance recently by using large-scale parallel corpora. However, it struggles in the low-resource and morphologically-rich scenarios of agglutinative language translation task. Inspired by the finding that monolingual data can greatly improve the NMT performance, we propose a multi-task neural model that jointly learns to perform bi-directional translation and agglutinative language stemming. Our approach employs the shared encoder and decoder to train a single model without changing the standard NMT architecture but instead adding a token before each source-side sentence to specify the desired target outputs of the two different tasks. Experimental results on Turkish-English and Uyghur-Chinese show that our proposed approach can significantly improve the translation performance on agglutinative languages by using a small amount of monolingual data.", "sections": [{"heading": "Introduction", "text": "Neural machine translation (NMT) has achieved impressive performance on many high-resource machine translation tasks (Bahdanau et al., 2015;Luong et al., 2015a;Vaswani et al., 2017). The standard NMT model uses the encoder to map the source sentence to a continuous representation vector, and then it feeds the resulting vector to the decoder to produce the target sentence.\nHowever, the NMT model still suffers from the low-resource and morphologically-rich scenarios of agglutinative language translation tasks, such as Turkish-English and Uyghur-Chinese. Both Turkish and Uyghur are agglutinative languages with complex morphology. The morpheme structure of the word can be denoted as: prefix1 + \u2026 + prefixN + stem + suffix1 + \u2026 + suffixN (Ablimit et al., 2010). Since the suffixes have many inflected and morphological variants, the vocabulary size of an agglutinative language is considerable even in small-scale training data. Moreover, many words have different morphemes and meanings in different context, which leads to inaccurate translation results.\nRecently, researchers show their great interest in utilizing monolingual data to further improve the NMT model performance (Cheng et al., 2016;Ramachandran et al., 2017;Currey et al., 2017). Sennrich et al. (2016) pair the target-side monolingual data with automatic back-translation as additional training data to train the NMT model. Zhang and Zong (2016) use the source-side monolingual data and employ the multi-task learning framework for translation and source sentence reordering.  modify the decoder to enable multi-task learning for translation and language modeling. However, the above works mainly focus on boosting the translation fluency, and lack the consideration of morphological and linguistic knowledge.\nStemming is a morphological analysis method, which is widely used for information retrieval tasks (Kishida, 2005). By removing the suffixes in the word, stemming allows the variants of the same word to share representations and reduces data sparseness. We consider that stemming can lead to better generalization on agglutinative languages, which helps NMT to capture the in-depth semantic information. Thus we use stemming as an auxiliary task for agglutinative language translation.\nIn this paper, we investigate a method to exploit the monolingual data of the agglutinative language to enhance the representation ability of the encoder. This is achieved by training a multi-task neural model to jointly perform bi-directional translation and agglutinative language stemming, which utilizes the shared encoder and decoder. We treat stemming as a sequence generation task. Figure 1: The architecture of the multi-task neural model that jointly learns to perform bi-directional translation between Turkish and English, and stemming for Turkish sentence.", "n_publication_ref": 10, "n_figure_ref": 1}, {"heading": "Multi-Task Neural Model for Agglutinative Language Translation", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Multi-task learning (MTL) aims to improve the generalization performance of a main task by using the other related tasks, which has been successfully applied to various research fields ranging from language (Liu et al., 2015;Luong et al., 2015a), vision (Yim et al., 2015;Misra et al., 2016), and speech (Chen and Mak, 2015;Kim et al., 2016). Many natural language processing (NLP) tasks have been chosen as auxiliary task to deal with the increasingly complex tasks. Luong et al. (2015b) employ a small amount of data of syntactic parsing and image caption for English-German translation. Hashimoto et al. (2017) present a joint MTL model to handle the tasks of part-of-speech (POS) tagging, dependency parsing, semantic relatedness, and textual entailment for English. Kiperwasser and Ballesteros (2018) utilize the POS tagging and dependency parsing for English-German machine translation. To the best of our knowledge, we are the first to incorporate stemming task into MTL framework to further improve the translation performance on agglutinative languages. Recently, several works have combined the MTL method with sequence-to-sequence NMT model for machine translation tasks. Dong et al. (2015) follow a one-to-many setting that utilizes a shared encoder for all the source languages with respective attention mechanisms and multiple decoders for the different target languages. Luong et al. (2015b) follow a many-to-many setting that uses multiple encoders and decoders with two separate unsupervised objective functions. Zoph and Knight (2016) follow a many-to-one setting that employs multiple encoders for all the source languages and one decoder for the desired target language. Johnson et al. (2017) propose a more simple method in one-to-one setting, which trains a single NMT model with the shared encoder and decoder in order to enable multilingual translation.\nThe method requires no changes to the standard NMT architecture but instead requires adding a token at the beginning of each source sentence to specify the desired target sentence. Inspired by their work, we employ the standard NMT model with one encoder and one decoder for parameter sharing and model generalization. In addition, we build a joint vocabulary on the concatenation of the source-side and target-side words.\nSeveral works on morphologically-rich NMT have focused on using morphological analysis to pre-process the training data (Luong et al., 2016;Huck et al., 2017;Tawfik et al., 2019). Gulcehre et al. (2015) segment each Turkish sentence into a sequence of morpheme units and remove any nonsurface morphemes for Turkish-English translation. Ataman et al. (2017) propose a vocabulary reduction method that considers the morphological properties of the agglutinative language, which is based on the unsupervised morphology learning. This work takes inspiration from our previously proposed segmentation method (Pan et al., 2020) that segments the word into a sequence of subword units with morpheme structure, which can effectively reduce language complexity.", "n_publication_ref": 19, "n_figure_ref": 0}, {"heading": "Multi-Task Neural Model", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Overview", "text": "We propose a multi-task neural model for machine translation from and into a low-resource and morphologically-rich agglutinative language. We train the model to jointly learn to perform both the bi-directional translation task and the stemming task on an agglutinative language by using the standard NMT framework. Moreover, we add an artificial token before each source sentence to specify the desired target outputs for different tasks. The architecture of the proposed model is shown in Figure 1. We take the Turkish-English translation task as example. The \"<MT>\" token denotes the bilingual translation task and the \"<ST>\" token denotes the stemming task on Turkish sentence.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Neural Machine Translation (NMT)", "text": "Our proposed multi-task neural model on using the source-side monolingual data for agglutinative language translation task can be applied in any NMT structures with encoder-decoder framework. In this work, we follow the NMT model proposed by Vaswani et al. (2017), which is implemented as Transformer. We will briefly summarize it here.   Firstly, the Transformer model maps the source sequence = ( 1 , \u2026 , ) and the target sentence = ( 1 , \u2026 , ) into a word embedding matrix, respectively. Secondly, in order to make use of the word order in the sequence, the above word embedding matrices sum with their positional encoding matrices to generate the source-side and target-side positional embedding matrices. The encoder is composed of a stack of N identical layers. Each layer has two sub-layers consisting of the multi-head self-attention and the fully connected feed-forward network, which maps the source-side positional embedding matrix into a representation vector.\nThe decoder is also composed of a stack of N identical layers. Each layer has three sub-layers: the multi-head self-attention, the multi-head attention, and the fully connected feed-forward network. The multi-head attention attends to the outputs of the encoder and decoder to generate a context vector. The feed-forward network followed by a linear layer maps the context vector into a vector with the original space dimension. Finally, the softmax function is applied on the vector to predict the target word sequence.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experiment", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dataset", "text": "The statistics of the training, validation, and test datasets on Turkish-English and Uyghur-Chinese machine translation tasks are shown in Table 1.\nFor the Turkish-English machine translation, following (Sennrich et al., 2015a), we use the WIT corpus (Cettolo et al., 2012) and the SETimes corpus (Tyers and Alperen, 2010) as the training dataset, merge the dev2010 and tst2010 as the validation dataset, and use tst2011, tst2012, tst2013, tst2014 from the IWSLT as the test datasets. We also use the talks data from the IWSLT evaluation campaign 1 in 2018 and the news data from News Crawl corpora 2 in 2017 as external monolingual data for the stemming task on Turkish sentences.\nFor the Uyghur-Chinese machine translation, we use the news data from the China Workshop on Machine Translation in 2017 (CWMT2017) as the training dataset and validation dataset, use the news data from CWMT2015 as the test dataset. Each Uyghur sentence has four Chinese reference sentences. Moreover, we use the news data from the Tianshan website 3 as external monolingual data for the stemming task on Uyghur sentences.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Data Preprocessing", "text": "We normalize and tokenize the experimental data. We utilize the jieba toolkit 4 to segment the Chinese sentences, we utilize the Zemberek toolkit 5 with morphological disambiguation (Sak et al., 2007) and the morphological analysis tool (Tursun et al., 2016) to annotate the morpheme structure of the words in Turkish and Uyghur, respectively.\nWe use our previously proposed morphological segmentation method (Pan et al., 2020), which segments the word into smaller subword units with morpheme structure. Since Turkish and Uyghur only have a few prefixes, we combine the prefixes with stem into the stem unit. As shown in Figure 2, the morpheme structure of the Turkish word \"hecelerini\" (syllables) is: hece + lerini. Then the byte pair encoding (BPE) technique (Sennrich et al., 2015b) is applied on the stem unit \"hece\" to segment it into \"he@@\" and \"ce@@\". Thus the Turkish word is segmented into a sequence of subword units: he@@ + ce@@ + lerini.", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "Task", "text": "Training Sentence Samples En-Tr Translation <MT> We go through initiation rit@@ es. Ba\u015fla@@ ma rit\u00fcel@@ lerini ya\u015f@@ \u0131yoruz. Tr-En Translation <MT> Ba\u015fla@@ ma rit\u00fcel@@ lerini ya\u015f@@ \u0131yoruz. We go through initiation rit@@ es. Turkish Stemming <ST> Ba\u015fla@@ ma rit\u00fcel@@ lerini ya\u015f@@ \u0131yoruz. Ba\u015fla@@ rit\u00fcel@@ ya\u015f@@  In this paper, we utilize the above morphological segmentation method for our experiments by applying BPE on the stem units with 15K merge operations for the Turkish words and 10K merge operations for the Uyghur words. The standard NMT model trained on this experimental data is denoted as \"baseline NMT model\". Moreover, we employ BPE to segment the words in English and Chinese by learning separate vocabulary with 32K merge operations. Table 2 shows the training sentence samples for multi-task neural model on Turkish-English machine translation task.\nIn addition, to certify the effectiveness of the morphological segmentation method, we employ the pure BPE to segment the words in Turkish and Uyghur by learning a separate vocabulary with 36K and 38K merge operations, respectively. The standard NMT model trained on this experimental data is denoted as \"general NMT model\". Table 3 shows the detailed statistics of using different word segmentation methods on Turkish, English, Uyghur, and Chinese. The \"Vocab\" token denotes the vocabulary size after data preprocessing. The \"Avg.Len\" token denotes the average sentence length.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Training and Evaluation Details", "text": "We employ the Transformer model implemented in the Sockeye toolkit . The number of layer in both the encoder and decoder is set to N=6, the number of head is set to 8, and the number of hidden unit in the feed-forward network is set to 1024. We use an embedding size of both the source and target words of 512 dimension, and use a batch size of 128 sentences. The maximum sentence length is set to 100 tokens with 0.1 label smoothing. We apply layer normalization and add dropout to the embedding and transformer layers with 0.1 probability. Moreover, we use the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 0.0002, and save the checkpoint every 1500 updates.\nModel training process stops after 8 checkpoints without improvements on the validation perplexity. Following Niu et al. (2018a), we select the 4 best checkpoint based on the validation perplexity values and combine them in a linear ensemble for decoding. Decoding is performed by using beam search with a beam size of 5. We evaluate the machine translation performance by using the case-sensitive BLEU score (Papineni et al., 2002) with standard tokenization.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Neural Translation Models", "text": "In this paper, we select 4 neural translation models for comparison. More details about the models are shown below: General NMT Model: The standard NMT model trained on the experimental data segmented by BPE. Baseline NMT Model: The standard NMT model trained on the experimental data segmented by morphological segmentation. The following models also use this word segmentation method. Bi-Directional NMT Model: Following Niu et al. (2018b), we train a single NMT model to perform bi-directional machine translation. We concatenate the bilingual parallel sentences in both directions. Since the source and target sentences come from the same language pairs, we share the source and target vocabulary, and tie their word embedding during model training. Multi-Task Neural Model: We simply use the monolingual data of the agglutinative language from the bilingual parallel sentences. We use a joint vocabulary, tie the word embedding as well as the output layer's weight matrix.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results and Discussion", "text": "Table 4 shows the BLEU scores of the general NMT model and baseline NMT model on machine translation task. We can observe that the baseline NMT model is comparable to the general NMT model, and it achieves the highest BLEU scores on almost all the test datasets in both directions, which indicates that the NMT baseline based on our proposed segmentation method is competitive.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Using Original Monolingual Data", "text": "Table 5 shows the BLEU scores of the baseline NMT model, bi-directional NMT model, and multi-task neural model on the machine translation task between Turkish and English. The table shows that the multi-task neural model outperforms both the baseline NMT model and bi-directional NMT model, and it achieves the highest BLEU scores on almost all the test datasets in both directions, which suggests that the multi-task neural model is capable of improving the bi-directional translation quality on agglutinative languages. The main reason is that compared with the bi-directional NMT model, our proposed multi-task neural model additionally employs the stemming task for the agglutinative language, which is effective for the NMT model to learn both the source-side semantic information and the target-side language modeling. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "bidirectional", "text": "The university was emulating its lives. multi-task The university was imitating life. The function of epochs and perplexity values on the validation dataset in different neural translation models are shown in Figure 3. We can see that the perplexity values are consistently lower on the multi-task neural model, and it converges rapidly.\nTable 6 shows a translation example for the different models on Turkish-English. We can see that the translation result of the multi-task neural model is more accurate. The Turkish word \"taklit\" means \"imitate\" in English, both the baseline NMT and bi-directional NMT translate it into a synonym \"emulate\". However, they are not able to express the meaning of the sentence correctly. The main reason is that the auxiliary task of stemming forces the proposed model to focus more strongly on the core meaning of each word (or stem), therefore helping the model make the correct lexical choices and capture the in-depth semantic information.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Using External Monolingual Data", "text": "Moreover, we evaluate the multi-task neural model on using external monolingual data for Turkish stemming task. We employ the parallel sentences and the monolingual data in a 1-1 ratio, and shuffle them randomly before each training epoch. More details about the data are shown below:   Table 7 shows the BLEU scores of the proposed multi-task neural model on using different external monolingual data. We can see that there is no obvious difference on Turkish-English translation performance by using different monolingual data, whether the data is in-domain or out-of-domain to the test dataset. However, for the English-Turkish machine translation task, which can be seen as agglutinative language generation task, using the mixed data of talks and news achieves further improvements of the BLEU scores on almost all the test datasets. The main reason is that the proposed multi-task neural model incorporates many morphological and linguistic information of Turkish rather than that of English, which mainly pays attention to the source-side representation ability on agglutinative languages rather than the target-side language modeling.\nWe also evaluate the translation performance of the general NMT model, baseline NMT model, and multi-task neural model with external news data on the machine translation task between Uyghur and Chinese. The experimental results are shown in Table 8. The results indicate that the multi-task neural model achieves the highest BLEU scores on the test dataset by utilizing external monolingual data for the stemming task on Uyghur sentences.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusions", "text": "In this paper, we propose a multi-task neural model for translation task from and into a low-resource and morphologically-rich agglutinative language. The model jointly learns to perform bi-directional translation and agglutinative language stemming by utilizing the shared encoder and decoder under standard NMT framework. Extensive experimental results show that the proposed model is beneficial for the agglutinative language machine translation, and only a small amount of the agglutinative data can improve the translation performance in both directions. Moreover, the proposed approach with external monolingual data is more useful for translating into the agglutinative language, which achieves an improvement of +1.42 BLEU points for translation from English into Turkish and +1.45 BLEU points from Chinese into Uyghur.\nIn future work, we plan to utilize other word segmentation methods for model training. We also plan to combine the proposed multi-task neural model with back-translation method to enhance the ability of the NMT model on target-side language modeling.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "We are very grateful to the mentor of this paper for her meaningful feedback. Thanks three anonymous reviewers for their insightful comments and practical suggestions. ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Uyghur morpheme-based language models and ASR", "journal": "", "year": "2010", "authors": "Mijit Ablimit; Graham Neubig; Masato Mimura; Shinsuke Mori; Tatsuya Kawahara; Askar Hamdulla"}, {"title": "Linguistically motivated vocabulary reduction for neural machine translation from Turkish to English", "journal": "Journal of the Prague Bulletin of Mathematical Linguistics", "year": "2017", "authors": "Matteo Duygu Ataman; Marco Negri; Marcello Turchi;  Federico"}, {"title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2014", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"title": "WIT3: Web inventory of transcribed and translated talks", "journal": "", "year": "2012", "authors": "Mauro Cettolo; Christian Girardi; Marcello Federico"}, {"title": "Multitask learning of deep neural networks for lowresource speech recognition", "journal": "", "year": "2015", "authors": "Dongpeng Chen; Brian Kan-Wing Mak"}, {"title": "Semi-Supervised learning for neural machine translation", "journal": "", "year": "2016", "authors": "Yong Cheng; Wei Xu; Zhongjun He; Wei He; Hua Wu; Maosong Sun; Yang Liu"}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "journal": "", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merrienboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"}, {"title": "Copied monolingual data improves low-resource neural machine translation", "journal": "", "year": "2017", "authors": "Anna Currey; Antonio Valerio Miceli; Kenneth Barone;  Heafield"}, {"title": "Using targetside monolingual data for neural machine translation through multi-task learning", "journal": "", "year": "2017", "authors": "Tobias Domhan; Felix Hieber"}, {"title": "Multi-task learning for multiple language translation", "journal": "", "year": "2015", "authors": "Daxiang Dong; Hua Wu; Wei He; Dianhai Yu; Haifeng Wang"}, {"title": "On using monolingual corpora in neural machine translation", "journal": "", "year": "2015", "authors": "Caglar Gulcehre; Orhan Firat; Kelvin Xu; Kyunghyun Cho"}, {"title": "A joint manytask model: Growing a neural network for multiple NLP tasks", "journal": "", "year": "2017", "authors": "Kazuma Hashimoto; Caiming Xiong; Yoshimasa Tsuruoka; Richard Socher"}, {"title": "Sockeye: A toolkit for neural machine translation", "journal": "", "year": "2017", "authors": "Felix Hieber; Tobias Domhan; Michael Denkowski; David Vilar; Artem Sokolov; Ann Clifton; Matt Post"}, {"title": "Target-side word segmentation strategies for neural machine translation", "journal": "", "year": "2017", "authors": "Matthias Huck; Simon Riess; Alexander Fraser"}, {"title": "On using very large target vocabulary for neural machine translation", "journal": "", "year": "2015", "authors": "S\u00e9bastien Jean; Kyunghyun Cho; Roland Memisevic; Yoshua Bengio"}, {"title": "Google's multilingual neural machine translation system: Enabling zero-shot translation", "journal": "", "year": "2017", "authors": "Melvin Johnson; Mike Schuster; Quoc V Le; Maxim Krikun; Yonghui Wu; Zhifeng Chen; Nikhil Thorat; Fernanda Vi\u00e9gas; Martin Wattenberg; Greg Corrado; Macduff Hughes; Jeffrey Dean"}, {"title": "Joint CTC-Attention based end-to-end speech recognition using multi-task learning", "journal": "", "year": "2016", "authors": "Suyoun Kim; Takaaki Hori; Shinji Watanabe"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "Diederik Kingma; Jimmy Ba"}, {"title": "Scheduled multi-task learning: From syntax to translation", "journal": "", "year": "2018", "authors": "Eliyahu Kiperwasser; Miguel Ballesteros"}, {"title": "Technical issues of crosslanguage information retrieval: A review", "journal": "Journal of the Information Processing and Management", "year": "2005", "authors": "Kazuaki Kishida"}, {"title": "", "journal": "", "year": "", "authors": ""}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "journal": "", "year": "2015", "authors": "Xiaodong Liu; Jianfeng Gao; Xiaodong He; Li Deng; Kevin Duh; Ye-Yi Wang"}, {"title": "Effective approaches to Attentionbased neural machine translation", "journal": "", "year": "2015", "authors": "Minh-Thang Luong; Hieu Pham; Christopher D Manning"}, {"title": "Multi-task sequence to sequence learning", "journal": "", "year": "2015", "authors": "Minh-Thang Luong; V Quoc; Ilya Le; Oriol Sutskever; Lukasz Vinyals;  Kaiser"}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "journal": "", "year": "2016", "authors": "Minh-Thang Luong; Christopher D Manning"}, {"title": "Cross-Stitch networks for multi-task learning", "journal": "", "year": "2016", "authors": "Ishan Misra; Abhinav Shrivastava; Abhinav Gupta; Martial Hebert"}, {"title": "Multi-task neural models for translating between styles within and across languages", "journal": "", "year": "2018", "authors": "Xing Niu; Sudha Rao; Marine Carpuat"}, {"title": "Bi-Directional neural machine translation with synthetic parallel data", "journal": "", "year": "2018", "authors": "Xing Niu; Michael Denkowski; Marine Carpuat"}, {"title": "Unsupervised pretraining for sequence to sequence learning", "journal": "", "year": "2017", "authors": "Prajit Ramachandran; Peter Liu; Quoc Le"}, {"title": "Morphological word segmentation on agglutinative languages for neural machine translation", "journal": "", "year": "2020", "authors": "Yirong Pan; Xiao Li; Yating Yang; Rui Dong"}, {"title": "BLEU: A method for automatic evaluation of machine translation", "journal": "", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Weijing Zhu"}, {"title": "Morphological disambiguation of Turkish text with perceptron algorithm", "journal": "", "year": "2007", "authors": "Ha\u00b8sim Sak; Tunga G\u00fcng\u00f6r; Murat Sara\u00e7lar"}, {"title": "Improving neural machine translation models with monolingual data", "journal": "", "year": "2015", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Neural machine translation of rare words with subword units", "journal": "", "year": "2015", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Controlling politeness in neural machine translation via side constraints", "journal": "", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; Quoc Vv Le"}, {"title": "Morphology-aware word-segmentation in dialectal Arabic adaptation of neural machine translation", "journal": "", "year": "2019", "authors": "Ahmed Tawfik; Mahitab Emam; Khaled Essam; Robert Nabil; Hany Hassan"}, {"title": "A semi-supervised tag-transitionbased Markovian model for Uyghur morphology analysis", "journal": "", "year": "2016", "authors": "Eziz Tursun; Debasis Ganguly; Turghun Osman; Yating Yang; Ghalip Abdukerim; Junlin Zhou; Qun Liu"}, {"title": "South-East European Times: A parallel corpus of the Balkan languages", "journal": "", "year": "2010", "authors": "M Francis;  Tyers;  Murat Alperen"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"title": "Rotating your face using multi-task deep neural network", "journal": "", "year": "2015", "authors": "Junho Yim; Heechul Jung; Byungin Yoo; Changkyu Choi; Dusik Park; Junmo Kim"}, {"title": "Exploiting source-side monolingual data in neural machine translation", "journal": "", "year": "2016", "authors": "Jiajun Zhang; Chengqing Zong"}, {"title": "Multi-source neural translation", "journal": "", "year": "2016", "authors": "Barret Zoph; Kevin Knight"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: The example of morphological segmentation method for the word in Turkish.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: The function of epochs (x-axis) and perplexity (y-axis) values on the validation dataset in different neural translation models for the translation task.", "figure_data": ""}, {"figure_label": ":", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Original Data ::The monolingual data comes from the original bilingual parallel sentences. Talks Data: The monolingual data contains talks. News Data: The monolingual data contains news. Talks and News Mixed Data: The monolingual data contains talks and news in a 3:4 ratio as the same with the original bilingual parallel sentences.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences, China 2 University of Chinese Academy of Sciences, China 3 Xinjiang Laboratory of Minority Speech and Language Information Processing, China panyirong15@mails.ucas.ac.cn {xiaoli, yangyt, dongrui}@ms.xjb.ac.cn", "figure_data": "Training dataBilingual DataMonolingual Data<MT> + English sentenceTurkish sentence<MT> + Turkish sentenceEncoder-Decoder FrameworkEnglish sentence<ST> + Turkish sentencestem sequence"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The statistics of the training, validation, and test datasets on Turkish-English and Uyghur-Chinese machine translation tasks. The \"# Src\" denotes the number of the source tokens, and the \"# Trg\" denotes the numbers of the target tokens.", "figure_data": "bir dilin son hecelerini kendisiyle birlikte mezaraMorpheme Segmentationhece+ler+i+niStem+Combined Suffixhece+leriniApply BPE on Stemhe@@+ce@@+lerini"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": The training sentence samples for multi-taskneural model on Turkish-English machine translationtask. We add \"<MT>\" and \"<ST>\" before each sourcesentence to specify the desired target outputs fordifferent tasks.Lang Method # Merge Vocab Avg.LenTrMorph15K36,468 28TrBPE36K36,040 22EnBPE32K31,306 25UyMorph10K38,164 28UyBPE38K38,292 21ChBPE32K40,835 19"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "The BLEU scores of the general NMT model and baseline NMT model on the machine translation task between Turkish and English.", "figure_data": "Task Modeltst11 tst12 tst13 tst14Tr-baseline26.48 27.02 27.91 26.33Enbi-26.21 27.17 28.68 26.90directionalmulti-task 26.82 27.96 29.16 27.98En-baseline14.85 15.93 15.45 15.93Trbi-15.08 16.20 16.25 16.56directionalmulti-task 15.65 17.10 16.35 16.41Table 5: The BLEU scores of the baseline NMT model,bi-directional NMT model, and multi-task neuralmodel on the machine translation task between Turkishand English."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "A translation example for the different NMT models on Turkish-English.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "En original 26.82 27.96 29.16 27.98 talks 26.55 27.94 29.13 28.02 news 26.47 28.18 28.89 27.40 mixed 26.60 27.93 29.58 27.32 En-Tr original 15.65 17.10 16.35 16.", "figure_data": "TaskDatatst11 tst12 tst13 tst14Tr-41talks15.57 16.97 16.22 16.91news15.67 17.19 16.26 16.69mixed15.96 17.35 16.55 16.89Table 7: The BLEU scores of the multi-task neuralmodel on using external monolingual data of talks data,news data, and mixed data.TaskModelBLEUUy-Chgeneral NMT model35.12baseline NMT model35.46multi-task neural model with36.47external monolingual dataCh-Uygeneral NMT model21.00baseline NMT model21.57multi-task neural model with23.02external monolingual data"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "The BLEU scores of the general NMT model, baseline NMT model, and the multi-task neural model with external monolingual data on Uyghur-Chinese and Chinese-Uyghur machine translation tasks.", "figure_data": ""}], "doi": "10.1016/j.ipm.2004.06.007"}