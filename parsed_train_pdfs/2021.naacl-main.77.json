{"authors": "Siqi Sun; Yen-Chun Chen; Linjie Li; Shuohang Wang; Yuwei Fang; Jingjing Liu", "pub_date": "", "title": "LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval", "abstract": "Multimodal pre-training has propelled great advancement in vision-and-language research. These large-scale pre-trained models, although successful, fatefully suffer from slow inference speed due to enormous computation cost mainly from cross-modal attention in Transformer architecture. When applied to reallife applications, such latency and computation demand severely deter the practical use of pre-trained models. In this paper, we study Image-text retrieval (ITR), the most mature scenario of V+L application, which has been widely studied even prior to the emergence of recent pre-trained models. We propose a simple yet highly effective approach, LightningDOT that accelerates the inference time of ITR by thousands of times, without sacrificing accuracy. LightningDOT removes the time-consuming cross-modal attention by pre-training on three novel learning objectives, extracting feature indexes offline, and employing instant dot-product matching with further re-ranking, which significantly speeds up retrieval process. In fact, Light-ningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that consume 1000\u00d7 magnitude of computational hours. 1   ", "sections": [{"heading": "Introduction", "text": "Image-text retrieval (ITR) has been widely studied as a staple benchmark task in both NLP and computer vision communities. Traditional ITR search engines typically deploy ranking-based models built upon visual-semantic embedding matching (Faghri et al., 2017;Huang et al., 2018) or deep cross-modal fusion with attention mechanism (Lee et al., 2018;Li et al., 2020a,b). Earliest works (Kiros et al., 2014;Faghri et al., 2017  (a) Early work (Faghri et al., 2017) using dot product to learn the similarity between global image features and global text features. (b) Later study (Lee et al., 2018) applying cross-attention between the features of each region and each word. (c) Pre-trained V+L models  with deep Transformer. (d) LightningDOT without cross-attention. CMR, SMRM and VMLM refer to different pre-training tasks, which will be introduced later in method section.  employ separate image encoder (e.g., CNN) and text encoder (e.g., RNN), the embeddings from which are then measured by doc product for similarity matching (Figure 1(a)).\nLater studies (Lee et al., 2018Wang et al., 2019; improve this paradigm by employing advanced region-level visual encoder (e.g., Faster-RCNN) and applying cross-attention between word features and region features for multimodal fusion (Figure 1(b)).\nWith the advent of Transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019), crossmodal retrieval tasks are more recently dominated by vision-and-language (V+L) pre-trained models, such as ViLBERT , UNITER , OSCAR (Li et al., 2020b), and VILLA . Large-scale pre-trained models learned from massive corpus of image-text pairs can power heterogeneous downstream tasks that take diverse modalities as inputs (e.g., text, image, video, audio). These models benefit from the self-attention mechanism in Transformer architecture, learning joint image+text embeddings through pre-training objectives such as masked language modeling (MLM) and masked region modeling (MRM) (Figure 1(c)).\nHowever, the very ingredient that engenders the success of these pre-trained models, crossmodal attention between two modalities (through self-attention), also destines the inevitable latency and huge computation cost in training and deploying such massive-scale models. For example, UNITER  builds upon 12/24 Transformer layers, and trains over 10 million image+text pairs. The inference time of such large models with 110 million parameters is 48 seconds on average for text query from COCO dataset (Chen et al., 2015), not scalable in real-life applications serving millions of queries per second.\nTo make real-time ITR possible with low latency, we ask a bold question: can we go back to the beginning, reverting to simple dot product for efficient cross-modal retrieval? To make this retro experiment feasible, we rely on Transformer to pre-train high-quality image and text encoders, but use efficient dot product for multimodal fusion instead of computationally heavy self-attention. To still facilitate effective cross-modal embedding learning, we use a special [CLS] token on both encoders, which transfers the learned embedding from the other modality (Figure 1(d)). We name this new paradigm LightningDOT, for its lightening speed benefiting from dot product computation.\nBy removing the time-consuming cross-attention between modalities, the model can learn visualsemantic embeddings without extensive matching between each image-text pair during inference, as used in existing pre-trained models Li et al., 2020b;. Further, by eliminating the dependency on real-time computation over image-text pairs, we can compute all image and text embeddings independently offline just for once, and reuse these embeddings as cached indexes for new queries on the fly (Figure 2).\nFor model training, we propose three learning objectives to jointly train two Transformer blocks: Image Encoder and Language Encoder. Specifically, Visual-embedding fused MLM (namely VMLM) and Semantic-embedding fused MRM (namely SMRM) ensure cross-modal information is harnessed even without cross-modality self-attention. A cross-modal retrieval objective (namely CMR) encourages the model to learn multimodal fusion through pre-training. To maintain competitive model performance, we further introduce a reranking mechanism to bring back the benefit of cross-attention methods.\nIn summary, LightningDOT is designed with late fusion to learn visual-semantic embeddings. Experiments on popular ITR benchmarks show that LightningDOT is 600/1900 times faster than existing pre-trained models on Flickr30k/COCO, while achieving new state-of-the-art results. When retrieving from larger candidate pool (>120K images), LightningDOT is 23,000 times faster. To the best of our knowledge, this is the first known effort on improving V+L model efficiency.", "n_publication_ref": 16, "n_figure_ref": 5}, {"heading": "Related Work", "text": "V+L Pre-training Inspired by the success of Transformer-based (Vaswani et al., 2017) language model pre-training (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Lan et al., 2020;Clark et al., 2020), vision-andlanguage pre-training (Huang et al., 2020b;Su et al., 2020;Li et al., 2020bLi et al., , 2019a has become the prevailing paradigm in learning multimodal representations, with strong results on tasks such as image-text retrieval (Kiros et al., 2014), visual question answering (Antol et al., 2015) and referring expression comprehension (Yu et al., 2016). Exemplary works include two-stream (Tan and Bansal, 2019; and single-stream models Li et al., 2020a;. Multi-task learning  and adversarial training  are also explored. This family of pre-training methods aims for general-purpose V+L without computation cost consideration. To the best of our knowledge, our work is the first known effort on pre-training visualsemantic embedding that enables low-latency realtime cross-modal retrieval. Ours is concurrent work with CLIP (Radford et al., 2021).\nImage-Text Retrieval Early cross-modal embedding works (Kiros et al., 2014;Faghri et al., 2017) focus on using a twostream model to learn a unified visual-semantic embedding, with progressive improvement on two popular benchmarks: Flickr30K (Plummer et al., 2015) and COCO (Chen et al., 2015). Later methods with cross-attention (Lee et al., 2018Wang et al., 2019; become more popular, with significant performance gain.  Pre-trained V+L models also fall into this category. By exploiting large-scale image-text datasets, pretrained V+L models further push the performance on Flickr30K and COCO. Although achieving high recall, cross-attention requires excessive computation cost during inference that cannot be overlooked. 2 In this work, inspired by dense retrieval in text retrieval domain (Guu et al., 2020;Karpukhin et al., 2020;Xiong et al., 2020;Mao et al., 2020;Lewis et al., 2020), we propose a more efficient attention-less framework. With pre-training, our model achieves better performance while being significantly faster than cross-modal attention methods. Note that the proposed approach is orthogonal to model compression techniques that reduce the number of layers/parameters (Sun et al., 2019;Jiao et al., 2020), since we do not reduce the number of parameters from the UNITER baseline. These two approaches can be combined to further boost the speed, which is an interesting future work direction.", "n_publication_ref": 30, "n_figure_ref": 0}, {"heading": "LightningDOT Framework", "text": "In this section, we present the proposed Light-ningDOT framework, which consists of two deep Transformers as image and language encoders. We first introduce three tasks designed to pre-train the model, then present our inference pipeline from offline feature extraction to online instant retrieval.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model Pre-training", "text": "We denote the Transformer-based (Vaswani et al., 2017) image encoder and language encoder by f \u03b8 V and f \u03b8 L , respectively (\u03b8 V , \u03b8 L are learnable parameters). Given a dataset of paired image and text {(i, t)}, we first extract region features v = {v 0 , v 1 , . . . , v N } (v j \u2208 R dv , N is the number of regions) for image i, along with bounding box positions of regions via a pre-trained Faster- RCNN (Ren et al., 2015;Anderson et al., 2018). 3 The image encoder f \u03b8 V encodes this sequence of image regions into a d-dimensional space\nf \u03b8 V (v) = h = {h 0 , . . . , h N } (h j \u2208 R d ).\nThe corresponding text t is tokenized into sub-word units and projected into high-dimensional feature vectors w = {w 0 , w 1 , ..., w T } (w j \u2208 R dw , T is the number of tokens) following Devlin et al. (2019). 4 Similarly, the text encoding process can be written as f \u03b8 L (w) = z = {z 0 , . . . , z T } (z j \u2208 R d ). We regard the output [CLS] embedding h 0 as global image representation, and z 0 as global text representation. Following sections discuss how to jointly train these two encoders to learn strong visual-semantic embeddings, through three pre-training objectives. ", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Visual-embedding", "text": "L MLM (t) = \u2212 log P \u03b8 L (w m |w \\m ) = \u2212 1 M M k=1 log P \u03b8 mlm (w m k |z m k ) ,(1)\nwhere \u03b8 mlm is the additional parameters introduced to map hidden states z to word probabilities. Under the V+L setting, the textual input is usually highly correlated with the image. To leverage this cross-modal relation, we propose visualembedding fused MLM (VMLM), in which the paired image i is considered as additional input when training the model to reconstruct masked tokens in sentence t. The loss function of VMLM can be formulated as:\nL VMLM (t, i) = \u2212 log P \u03b8 (w m |w \\m , i) = \u2212 1 M M k=1 log P \u03b8 mlm (w m k |z m k + h 0 ) ,(2)\nwhere \u03b8 = {\u03b8 V , \u03b8 L } and the word probabilities P \u03b8 are conditioned on the corresponding image i via the global image representation h 0 . Although VMLM takes a similar mathematical form to the MLM task proposed in UNITER, they differ in two main aspects: 1) LightningDOT uses two separate encoders (h 0 is computed by f \u03b8 V ); and 2) visual dependency is explicitly injected to text representations (z m k + h 0 ), instead of implicitly learned through cross-modal attention.\nSemantic-embedding Fused Masked Region Modeling (SMRM) Recent works on V+L pretraining Tan and Bansal, 2019) have shown that mask-then-reconstruct pre-training on image regions also helps image+text embedding learning. Similar to MLM, Masked Region Modeling (MRM) is supervised by:\nL MRM (i) = D \u03b8mrm (v m , f \u03b8 V (v \\m )) = 1 M M k=1 D \u03b8mrm (v m k , h m k ) ,(3)\nwhere D can be any differentiable distance function. Among the variants of MRM, we consider Masked Region Feature Regression (MRFR) with L2 distance and Masked Region Classification with KL-Divergence (MRC-kl), due to their proven success in learning V+L representations . 6 In MRFR, the L 2 distance between two feature vectors x and y is defined as:\nD \u03b8 fr (x, y) = k x k \u2212 g \u03b8 fr (y k ) 2 2 ,\nwhere \u2022 2 denotes L 2 -norm, and g \u03b8 fr (\u2022) is a learnable Multi-layer Perceptron (MLP) with parameters \u03b8 fr . The KL-divergence D KL in MRC-kl measures distance between two probability distributions:\nD \u03b8mrc (x, y) = k D KL (c(x k ) || g \u03b8mrc (y k )) ,\nwhere \u03b8 mrc is the parameters of a trainable MLP that maps feature vector x k to the object class distribution c(x k ) predicted by Faster R-CNN.\nTo incorporate language information encoded in the paired text, we extend MRM to Semanticembedding fused MRM (SMRM), where the global text representation z 0 is exploited when reconstructing masked regions.\nL SMRM (i, t) = D \u03b8mrm (v m , f \u03b8 V (v \\m ), t) = 1 M M k=1 D \u03b8mrm (v m k , h m k + z 0 ) .(4)\nThe specific variants SMRFR and SMRC-kl can be derived using the corresponding distance function, which is omitted for simplicity. Note that both the cross-modal fusion introduced in Eqn.\n(2) and Eqn. (4) uses simple addition without introducing extra parameters from their uni-modal counterpart. Moreover, the extra parameters \u03b8 mlm and \u03b8 mrm is not needed at downstream inference so will not slow down the retrieval.\nCross-modal Retrieval Objective (CMR) Beyond image or text focused reconstructive objectives, we also propose a new pre-training task, Cross-modal Retrieval (CMR), to leverage the paired information between image and text. With this learning objective, the model is optimized to promote high similarity score for a matched imagesentence pair (i, t) and vice versa. The similarity score between query t and image i is defined as:\nS(t, i) = z 0 , h 0 ,(5)\nwhere \u2022, \u2022 denotes the inner product between two vectors, and h 0 and z 0 are the output [CLS] embeddings from image encoder f \u03b8 V and language encoder f \u03b8 L , respectively.  In order to capture both image-retrieval and textretrieval supervision signals in a single forwardbackward pass, we propose a bi-directional variant of contrastive loss. Given any matched image-text pair (i, t), we treat text t as the query, sample n \u2212 1 negative images {i 2 , i 3 , . . . , i n }, and then compute the objective function as:\nL (t) IR = \u2212 log e S(t,i 1 ) n k=1 e S(t,i k ) ,\nwhere t 1 := t. Similarly, we take image i as query (i 1 := i), sample n \u2212 1 negative text, and compute:\nL (i) TR = \u2212 log e S(i,t 1 )\nn k=1 e S(i,t k ) to optimize for text retrieval.\nFollowing Henderson et al. (2017); Gillick et al. (2019); Karpukhin et al. (2020), we use in-batch negatives to avoid the actual sampling of a negative image or text: given a batch of n positive image-text pairs B = {(i 1 , t 1 ), . . . , (i n , t n )}, we use all other images from within the batch as negatives ({i j } , where j \u2208 {1, 2, . . . , n} and j = k) for every positive pair (i k , t k ), and vice versa for negative text. The final CMR loss for batch B is:\nL CMR (B) = 1 2n n k=1 L (i k ) TR + L (t k ) IR .(6)\nAn illustration of L CMR is presented in Figure 3. 7 Through joint pre-training with CMR, VMLM and SMRM, the visual-semantic embeddings learned from image encoder and language encoder can be readily applied to downstream tasks. During finetuning stage, we directly adopt CMR loss to supervise the training process. 7 The whole similarity matrix can be computed efficiently with one batched matrix multiplication call. This operation can take advantage of GPU hardware with Tensor Cores for faster training.", "n_publication_ref": 7, "n_figure_ref": 1}, {"heading": "Real-time Inference", "text": "For simplicity, we take text-to-image retrieval as an example to introduce the real-time inference pipeline (Figure 2 Offline Feature Extraction Image retrieval task requires the model to rank every image i in an image database I based on its similarity to a text query t. In LightningDOT, we first apply the image encoder f \u03b8 V to all images in I, and cache the resulting global image representations {h (Johnson et al., 2019) in memory for later use. Note that the entire image-to-index process, including Faster-RCNN feature extraction and Transformer encoding, can all be conducted offline. Therefore, for every new query t at real time, the cached index can be reused for maximum inference time saving.\n(i) 0 \u2208 R d |i \u2208 I} into an index\nOnline Retrieval During inference, given a text query t, we encode it with the language encoder \u03b8 L , and then compute its similarity score to the embedding of every image in I (stored in memory index) via Eqn (5). Finally, the images will be ranked by their similarity scores, from the highest to lowest. In practice, people are more interested in top-K retrieval, with a list of K images I t satisfying:\nI t := {i m k } K k=1 , where S(t, i m 1 ) \u2265 S(t, i m 2 ) \u2265 \u2022 \u2022 \u2022 \u2265 S(t, i m K ) and S(t, i m K ) \u2265 S(t, i) \u2200i \u2208 (I \\ I t ) . (7\n)\nThis optimization problem has been well studied, and we use FAISS (Johnson et al., 2019) to solve it in our implementation. It is worth noting that in order to apply fast search, the similarity function has to be decomposable. Therefore, we choose the simple dot product as S instead of a more complicated neural network function. Similarly, for text retrieval, the same architecture can be applied by simply pre-computing the embedding for all sentences and using an image as query instead.\nRe-ranking To further improve retrieval accuracy, we propose a two-stage approach by adopting an optional re-ranking model. In the first stage, we use LightningDOT to retrieve top-M images (or texts), where M is an integer much smaller R@1 R@5 R@10 R@1 R@5 R@10 AR R@1 R@5 R@10 R@1 R@5 R@10 AR  ", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Experiments", "text": "This section discusses our experiments on pretraining and evaluating LightningDOT on downstream ITR benchmarks.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets and Metrics", "text": "For pre-training, we use pre-processed data provided by , including 4.2 million 8 The computation time of LightningDOT is negligible compared to that of UNITER. Therefore, the empirical speed is proportional to the number of pairs UNITER has to rank: constant M for LightningDOT + UNITER vs. the whole database (index) size for UNITER only. images with 9.5 million associated captions from COCO (Chen et al., 2015), VG (Krishna et al., 2017), Conceptual Captions (Sharma et al., 2018), and SBU captions (Ordonez et al., 2011).\nFor evaluation, we use Flickr30k (Plummer et al., 2015) and COCO (Lin et al., 2014) datasets, which include 31K/123K images, respectively, each associated with 5 human-written captions. Following (Faghri et al., 2017), we split COCO into 114K/5K/5K and Flickr30K into 29K/1k/1k images for train, validation and test.\nDownstream performance is measured by recall at K (R@K) for both image and text retrieval tasks. We also use an additional metric \"AR\", the average of R@K for all K across both image and sentence retrieval tasks.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Results on Flickr30K and COCO", "text": "We compare the proposed approach with state-ofthe-art methods (with and without pre-training) and report the results in Table 1. Without crossattention, our method outperforms non-pre-training approaches by large margins on all metrics. Specifically, our model improves over CAAN  (SOTA method with cross-attention) by 3.3% (73.5 vs. 70.2) on COCO and 9.5% (89.3 vs. 79.8) on Flickr30K in terms of AR. When compared with methods without cross-attention (VSE++ (Faghri et al., 2017) and SCO (Huang et al., 2018)), LightningDOT achieves nearly Model COCO Full (123K Images) Flickr30K Full (31K Images)\nText Retrieval Image Retrieval Text Retrieval Image Retrieval R@5 R@10 R@20 R@5 R@10 R@20 AR R@5 R@10 R@20 R@5 R@10 R@20 AR    (Lee et al., 2018). LightningDOT with/without UNITER-base re-ranker is significantly faster.\n20-point gain on AR. Although LightningDOT achieves slightly lower AR than UNITER (pretraining method with cross-attention), with 3.5/1.1 points drop on Flickr30K/COCO, it is 600/1900 \u00d7 faster than UNITER during inference time.\nWe further apply second-stage re-ranking, and use UNITER to score top-M retrieved image-text pairs from LightningDOT to obtain the final top-K ranked lists. With re-ranking, LightningDOT achieves an instant performance lift, surpassing UNITER on both benchmarks, while still 46-95 times faster than UNITER. With an even stronger re-ranker OSCAR, LightningDOT achieves similar results to the state-of-the-art performance on COCO.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Speed & Space Improvement", "text": "To demonstrate the efficiency of LightningDOT, we use UNITER-base as baseline to compare inference speed. We also compare with a more lightweight cross-attention method SCAN (Lee et al., 2018), which uses GRU (Chung et al., 2014) instead of a 12-layer Transformer. All methods are tested on a single TITAN RTX GPU, with batch size of 400. As shown in Table 3, SCAN is \u223c1.9\u00d7 faster than UNITER-base across both benchmarks, as the computational cost of GRU is much cheaper than that of Transformer (performance drop is significant though). However, the speedup from SCAN is limited, as it computes cross-attention between each query and all images. On the other hand, LightningDOT is 639\u00d7 faster than UNITER on Flickr30K. When tested with 5 times more im-ages in COCO, the speedup from LightningDOT is 1927\u00d7. Even with re-ranking, LightningDOT is still much more efficient than UNITER-base (46\u00d7 faster on Flickr30K and 95\u00d7 faster on COCO).\nTo mimic a real-life scenario for image retrieval, where the candidate pool contains hundreds of thousands of images, we combine all images from training, validation and test set to form a larger candidate pool. Note that models are still trained on the training set. Although the number of text queries remain the same, the number of candidate images scales up by >20\u00d7, where cross-attention methods immediately become impractical. We refer this setting on both benchmarks as Flickr30k-full (31k) and COCO-full (123k). Our algorithm is 6,591\u00d7 faster on Flickr30k-full and 23,869\u00d7 faster on COCO-full, which clearly shows the advantage of LightningDOT and its potential in real-world applications. With re-ranking, LightningDOT is still more than 1,000\u00d7 and 2,000\u00d7 faster on Flickr30kfull and COCO-full, respectively. In general, for other re-rankers such as OSCAR, our algorithm can approximately speed up inference by N images /M times, where N images is the number of candidate images, and M is number of re-ranked images from top-M retrieved results by LightningDOT.\nSimilarly, we construct a full setting for text retrieval by combining all text queries from training, validation and test set. Results are summarized in Table 2. Considering the size of candidate pool has become more than 20\u00d7 larger, we adopt recall at top 5, 10, 50 as evaluation metrics. Our method achieves reasonably good performance, with AR of 44.4 on COCO and 70.2 on Flickr30K. Re-ranking further lifts AR to 56.4 and 76.2. Results from UNITER or SCAN are not included as the computation of pairwise scores is extremely expensive, given the excessive amount of retrieval candidates. While LightningDOT only takes minutes to evaluate, UNITER-base is estimated to take about 28 days 9 to evaluate under the full setting for both Text Retrieval Image Retrieval Method R@1 R@5 R@10 R@1 R@5 R@10 AR  ", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Text Retrieval", "text": "Image Retrieval LightningDOT R@1 R@5 R@10 R@1 R@5 R@10 AR  image retrieval and text retrieval.\nIn addition, We compare all models with the same setting: cache as much as possible for fastest speed, where our model outperforms others in both speed and space on image retrieval. The proposed algorithm maps each image to a 768-dimensional vector, which only consumes about 300Mb storage space for the whole COCO dataset. For crossattention models such as SCAN, UNITER or OS-CAR, they also need to cache image features, which typically requires to save a 36 x 2048 dimensional vector per image, and it consumes about 28GB storage space for COCO dataset.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ablation Studies", "text": "We conduct ablation studies on Flickr30K (Table 4) and compare LightningDOT (L4) against 3 ablated instances: (i)\"R-CNN only\" (L1): image representations are extracted from Faster R-CNN directly, with no image encoder applied; (ii) \"+Image Encoder\" (L2): regional features are encoded with a 12-layer Transformer as the image encoder; (iii) \"+PT \u2020 \" (L3): our model is pre-trained with MLM+MRM+CMR, then finetuned on Flickr30K. Note that the difference between MLM vs. VMLM and MRM vs. SMRM is whether the predictions of masked tokens (regions) rely on infused embeddings from the other modality.  Multi30K and COCO datasets. We compare with task-specific methods: S-LIWE (Wehrmann et al., 2019), MULE , SMALR (Burns et al., 2020), pre-trained method M 3 P (Huang et al., 2020a)  Results show that \"R-CNN only\" is not sufficient in learning good image representations for ITR task, while image encoder with Transformer architecture can effectively learn contextualized image representations, hence achieving better performance. Pre-trained models (L3-4) generally achieve better performance, compared to nonpretrained models (L1-2). Comparing \"+PT \u2020 \" to the full instance of LightningDOT, dependency on the other modality in VMLM and SMRM brings universal performance lift across all metrics. This indicates that these cross-modal dependencies introduced by VMLM and SMRM are effective in learning the association between image and text inputs.\nIn addition, we investigate the effectiveness of each pre-training task in Table 5. Comparing to baseline without pre-training, pre-training with CMR alone lifts +1.4 on AR. Pre-training with all three tasks achieves the best performance, indicating that the learning of contextualized word and region representations promotes better global alignment between image and text, and these three pre-training tasks work collaboratively to yield better visual-semantic embeddings.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Multilingual Image-Text Retrieval", "text": "We further report results on multilingual image-text retrieval tasks. Specially, we evaluate Lightning-DOT under the translate-test setting, which is to translate the test captions in other languages to English by leveraging Machine Translation (MT) tool. 10 Note that our method is only trained on English captions, without exploiting the original or translated captions from multilingual benchmarks. We consider two benchmarks: Multi30K (Elliott et al., 2016(Elliott et al., , 2017Barrault et al., 2018) with captions in German, French and Czech; and COCO Japanese (Yoshikawa et al., 2017) and Chinese (Li et al., 2019b).\nAverage Recall (AR) is used as the evaluation metric. Meta-Ave, the average of AR over different languages across two benchmarks, is used as a global metric. More details on multilingual ITR benchmarks are included in Appendix.\nWe compare LightningDOT against 3 task-specific methods: S-LIWE (Wehrmann et al., 2019), MULE  and SMALR (Burns et al., 2020), which all exploit captions in different languages to learn multilingual or language-agnostic word embeddings. We also compare with a pre-trained model M 3 P (Huang et al., 2020a), which is alternatively pre-trained with image-caption pairs labeled in English and cross-lingual corpus in 100 different languages. Note that all methods discussed above are trained/finetuned on captions in different languages. For fair comparison, we report performance of UNITER under the same translate-test setting, which is finetuned with English captions only and tested on translated captions.\nTable 6 shows similar trends of performance improvements as on English benchmarks. Compared to both state-of-the-art task-specific methods and pre-trained models, LightningDOT under translatetest setting achieves new state of the art on most languages and establishes a strong baseline for future study on these multilingual benchmarks.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Qualitative Examples", "text": "We show an example of image retrieval results here at figure 4 for query as \"Sky view of a blue and yellow biplane flying near each other\". In addition to the ground truth image in the red rectangle, all the 10 images retrieved by our model are valid retrieval since multiple keywords (\"sky\", \"blue\", \"yellow\", \"airplane\", \"near\") are captured for each image. Please see the appendix A.4 for more examples.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we propose a pre-training framework that learns joint visual-semantic embedding without any cross-attention between modalities. Light-ningDOT outperforms previous state of the art, while significantly speeding up inference time by 600-2000\u00d7 on Flickr30K and COCO image-text retrieval benchmarks. Future work includes extending the efficient training framework to other V+L tasks.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.2 Multilingual Image-Text Retrieval Benchmarks", "text": "When evaluating on ITR under the multilingual setting, we consider two benchmarks: Multi30K (Elliott et al., 2016(Elliott et al., , 2017Barrault et al., 2018) and COCO Japanese (Yoshikawa et al., 2017) and Chinese (Li et al., 2019b). Multi30K is constructed by manually translating English captions in Flickr30K (Plummer et al., 2015) to German, French, and Czech. Each image in Multi30K is paired with 5 captions in German, 1 caption in French and Czech. We adopt the same train/val/test split as in Flickr30K. COCO Japanese (Yoshikawa et al., 2017) collected 820K Japanese captions for 165K COCO images (Lin et al., 2014). We use the same train/dev/test splits for COCO Japanese as in Karpathy and Fei-Fei (2015), and present results on the 1K test set. Similarly, Li et al. (2019b) collected 1-2 Chinese captions per image for 20K COCO images to build COCO Chinese. We follow the original split defined in Li et al. (2019b).", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "A.3 Inference Time", "text": "We present the detailed inference time of UNITERbase, SCAN the proposed LightningDOT and LightningDOT with UNITER-base re-ranker in Table 7, measured by seconds/query. UNITER clearly is the slowest, as the 12-layer Transformer model inference needs to be run between each query and all images. Comparing between Flickr30k-test and COCO-test, its inference time scales up linearly with the number of images. With the lightweight GRU (Chung et al., 2014), SCAN is \u223c1.9\u00d7 faster than UNITER. Across all settings, LightningDOT is significantly faster than both cross-attention methods (UNITER-base and SCAN). When adding UNITER-base as the re-ranker, our method slows down by \u223c10, but still achieves decent speedup.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A.4 More Qualitative Examples", "text": "We show several qualitative results of image retrieval (top-10). All results are retrieved from COCO-Full dataset (123k images in total). Our model can well understand the underlying semantic meaning. For example, \"romantic\" only appears twice in the whole COCO dataset annotations, yet the top retrieved images are all topic-related (Figure 5). With multiple keywords, our model attempts to retrieve the combinations of them (if not all). For example, for the query \"blue girl boy ball\" with four keywords, our model retrieves images  Figure 5: Retrieved top-10 images for query \"romantic\".\nFigure 6: Retrieved top-10 images for query \"blue girl boy ball\" that capture at least three keywords (Figure 6).\nWe also present image retrieval results where the text query is sampled from COCO dataset. We randomly sample 3 queries and present the results as below (ground truth on the top, retrieved top-10 images at the bottom). Clearly, our model retrieves related images from the full dataset. ", "n_publication_ref": 0, "n_figure_ref": 4}, {"heading": "A Appendix", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 Implementation Details", "text": "To further facilitate the reproductivity of our proposed method, we include more details about the choice of model size and hyper-parameters for both pre-training and fine-tuning.\nThe model dimensions are set to (L=12, H=768, A=12) for both image encoder and language encoder, where L is the number of stacked Transformer blocks; H stands for hidden activation dimension, and A is the number of attention heads. The total number of parameters in LightningDOT is 220M. Pre-training and finetuning learn the parameters of both encoders. During inference, with offline representation caching, only the forwarding pass with one encoder from the query modality will be performed online.\nFor both pre-training and finetuning, AdamW (Loshchilov and Hutter, 2019) is used to optimize the model training, with \u03b2 1 =0.9, \u03b2 2 =0.98. We adopt a learning rate warmup strategy, where the learning rate is linearly increased during the first 10% of training steps, followed by a linear decay to 0. We set the L2 weight decay to be 0.01.\nDuring pre-training, we follow UNITER  to randomly sample 1 task per minibatch update. 11 Our best model is pre-trained on VMLM+SMRM+CRM for 300,000 optimization steps. We set the batch size to 10240 per GPU (batch size is specified by #tokens + #regions, as in UNITER). Pre-training experiments are conducted on 8\u00d7 V100 GPUs with 6-step gradient accumulation, and the learning rate is set to be 5e-5. For ablation studies presented in Table 5, the ablated instances of our model are pre-trained for 30k steps on COCO dataset (Lin et al., 2014) only, and the same choice of learning rate and batch size are applied as in the best pre-training setting.\nFor finetuning, we set batch size n to 96 (n is in examples, instead of the sequence length of tokens and regions), and search learning rate from {1e-5, 2e-5, 5e-5}. We select models based on their AR on the validation set. The best learning rate is 5e-5 for COCO and 1e-5 for Flickr30K. Our models are trained for 15 epochs on Flickr30k, and 20 epochs on COCO. For re-ranking, we choose k from {20, 50}.", "n_publication_ref": 3, "n_figure_ref": 0}], "references": [{"title": "Bottom-up and top-down attention for image captioning and visual question answering", "journal": "", "year": "2018", "authors": "Peter Anderson; Xiaodong He; Chris Buehler; Damien Teney; Mark Johnson; Stephen Gould; Lei Zhang"}, {"title": "Vqa: Visual question answering", "journal": "", "year": "2015", "authors": "Stanislaw Antol; Aishwarya Agrawal; Jiasen Lu; Margaret Mitchell; Dhruv Batra; Lawrence Zitnick; Devi Parikh"}, {"title": "Findings of the third shared task on multimodal machine translation", "journal": "", "year": "2018", "authors": "Lo\u00efc Barrault; Fethi Bougares; Lucia Specia; Chiraag Lala; Desmond Elliott; Stella Frank"}, {"title": "Learning to scale multilingual representations for visionlanguage tasks", "journal": "", "year": "2020", "authors": "Andrea Burns; Donghyun Kim; Derry Wijaya; Kate Saenko; Bryan A Plummer"}, {"title": "Microsoft coco captions: Data collection and evaluation server", "journal": "", "year": "2015", "authors": "Xinlei Chen; Hao Fang; Tsung-Yi Lin; Ramakrishna Vedantam; Saurabh Gupta; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"title": "Uniter: Universal image-text representation learning", "journal": "", "year": "2020", "authors": "Yen-Chun Chen; Linjie Li; Licheng Yu; Ahmed El Kholy; Faisal Ahmed; Zhe Gan; Yu Cheng; Jingjing Liu"}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "journal": "", "year": "2014", "authors": "Junyoung Chung; Caglar Gulcehre; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Electra: Pre-training text encoders as discriminators rather than generators", "journal": "", "year": "2020", "authors": "Kevin Clark; Minh-Thang Luong; V Quoc; Christopher D Le;  Manning"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Findings of the second shared task on multimodal machine translation and multilingual image description", "journal": "", "year": "2017", "authors": "Desmond Elliott; Stella Frank; Lo\u00efc Barrault; Fethi Bougares; Lucia Specia"}, {"title": "Multi30K: Multilingual English-German image descriptions", "journal": "", "year": "2016", "authors": "Desmond Elliott; Stella Frank; Khalil Sima'an; Lucia Specia"}, {"title": "Vse++: Improving visualsemantic embeddings with hard negatives", "journal": "", "year": "2017", "authors": "Fartash Faghri; J David; Jamie Ryan Fleet; Sanja Kiros;  Fidler"}, {"title": "Large-scale adversarial training for vision-and-language representation learning", "journal": "", "year": "2020", "authors": "Zhe Gan; Yen-Chun Chen; Linjie Li; Chen Zhu; Yu Cheng; Jingjing Liu"}, {"title": "Learning dense representations for entity retrieval", "journal": "", "year": "2019", "authors": "Dan Gillick; Sayali Kulkarni; Larry Lansing; Alessandro Presta; Jason Baldridge; Eugene Ie; Diego Garcia-Olano"}, {"title": "Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models", "journal": "", "year": "2018", "authors": "Jiuxiang Gu; Jianfei Cai; R Shafiq; Li Joty; Gang Niu;  Wang"}, {"title": "Realm: Retrievalaugmented language model pre-training", "journal": "", "year": "2020", "authors": "Kelvin Guu; Kenton Lee; Zora Tung; Panupong Pasupat; Ming-Wei Chang"}, {"title": "Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply", "journal": "", "year": null, "authors": "Matthew Henderson; Rami Al-Rfou; Brian Strope; Yun-Hsuan Sung; L\u00e1szl\u00f3 Luk\u00e1cs; Ruiqi Guo"}, {"title": "Dongdong Zhang, Xin Liu, and Ming Zhou. 2020a. M3p: Learning universal representations via multitask multilingual multimodal pre-training", "journal": "", "year": "", "authors": "Haoyang Huang; Lin Su; Di Qi; Nan Duan; Edward Cui; Taroon Bharti; Lei Zhang; Lijuan Wang; Jianfeng Gao; Bei Liu; Jianlong Fu"}, {"title": "Learning semantic concepts and order for image and sentence matching", "journal": "", "year": "2018", "authors": "Yan Huang; Qi Wu; Chunfeng Song; Liang Wang"}, {"title": "Pixel-bert: Aligning image pixels with text by deep multi-modal transformers", "journal": "", "year": "2020", "authors": "Zhicheng Huang; Zhaoyang Zeng; Bei Liu; Dongmei Fu; Jianlong Fu"}, {"title": "Tinybert: Distilling bert for natural language understanding", "journal": "", "year": "2020", "authors": "Xiaoqi Jiao; Yichun Yin; Lifeng Shang; Xin Jiang; Xiao Chen; Linlin Li; Fang Wang; Qun Liu"}, {"title": "Billion-scale similarity search with gpus", "journal": "IEEE Transactions on Big Data", "year": "2019", "authors": "Jeff Johnson; Matthijs Douze; Herv\u00e9 J\u00e9gou"}, {"title": "Deep visualsemantic alignments for generating image descriptions", "journal": "", "year": "2015", "authors": "Andrej Karpathy; Li Fei-Fei"}, {"title": "Dense passage retrieval for open-domain question answering", "journal": "", "year": "2020", "authors": "Vladimir Karpukhin; Barlas Oguz; Sewon Min; Ledell Wu; Sergey Edunov; Danqi Chen; Wentau Yih"}, {"title": "MULE: Multimodal Universal Language Embedding", "journal": "", "year": "2020", "authors": "Donghyun Kim; Kuniaki Saito; Kate Saenko; Stan Sclaroff; Bryan A Plummer"}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "journal": "", "year": "2014", "authors": "Ryan Kiros; Ruslan Salakhutdinov; Richard S Zemel"}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "journal": "", "year": "2017", "authors": "Ranjay Krishna; Yuke Zhu; Oliver Groth; Justin Johnson; Kenji Hata; Joshua Kravitz; Stephanie Chen; Yannis Kalantidis; Li-Jia Li; David A Shamma"}, {"title": "Albert: A lite bert for self-supervised learning of language representations", "journal": "", "year": "2020", "authors": "Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut"}, {"title": "Stacked cross attention for image-text matching", "journal": "", "year": "2018", "authors": "Kuang-Huei Lee; Xi Chen"}, {"title": "Learning visual relation priors for image-text matching and image captioning with neural scene graph generators", "journal": "", "year": "2019", "authors": "Kuang-Huei Lee; Hamid Palangi; Xi Chen; Houdong Hu; Jianfeng Gao"}, {"title": "Tim Rockt\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks", "journal": "", "year": "", "authors": "Patrick Lewis; Ethan Perez; Aleksandara Piktus; Fabio Petroni; Vladimir Karpukhin; Naman Goyal; Heinrich K\u00fcttler; Mike Lewis; Wen-Tau Yih"}, {"title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining", "journal": "", "year": "2020", "authors": "Gen Li; Nan Duan; Yuejian Fang; Daxin Jiang; Ming Zhou"}, {"title": "Visualbert: A simple and performant baseline for vision and language", "journal": "", "year": "2019", "authors": "Liunian Harold Li; Mark Yatskar; Da Yin; Cho-Jui Hsieh; Kai-Wei Chang"}, {"title": "Coco-cn for cross-lingual image tagging, captioning, and retrieval", "journal": "IEEE Transactions on Multimedia", "year": "2019", "authors": "Xirong Li; Chaoxi Xu; Xiaoxu Wang; Weiyu Lan; Zhengxiong Jia; Gang Yang; Jieping Xu"}, {"title": "Oscar: Object-semantics aligned pre-training for vision-language tasks", "journal": "", "year": "2020", "authors": "Xiujun Li; Xi Yin; Chunyuan Li; Xiaowei Hu; Pengchuan Zhang; Lei Zhang; Lijuan Wang; Houdong Hu; Li Dong; Furu Wei"}, {"title": "Microsoft coco: Common objects in context", "journal": "", "year": "2014", "authors": "Tsung-Yi Lin; Michael Maire; Serge Belongie; James Hays; Pietro Perona; Deva Ramanan; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks", "journal": "", "year": "2019", "authors": "Jiasen Lu; Dhruv Batra; Devi Parikh; Stefan Lee"}, {"title": "12-in-1: Multi-task vision and language representation learning", "journal": "", "year": "2020", "authors": "Jiasen Lu; Vedanuj Goswami; Marcus Rohrbach; Devi Parikh; Stefan Lee"}, {"title": "Generation-augmented retrieval for open-domain question answering", "journal": "", "year": "2020", "authors": "Yuning Mao; Pengcheng He; Xiaodong Liu; Yelong Shen; Jianfeng Gao; Jiawei Han; Weizhu Chen"}, {"title": "Im2text: Describing images using 1 million captioned photographs", "journal": "", "year": "2011", "authors": "Vicente Ordonez; Girish Kulkarni; Tamara L Berg"}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models", "journal": "", "year": "2015", "authors": "A Bryan; Liwei Plummer; Chris M Wang; Juan C Cervantes; Julia Caicedo; Svetlana Hockenmaier;  Lazebnik"}, {"title": "Learning transferable visual models from natural language supervision", "journal": "", "year": "2021", "authors": "Alec Radford; Jong Wook Kim; Chris Hallacy; Aditya Ramesh; Gabriel Goh; Sandhini Agarwal; Girish Sastry; Amanda Askell; Pamela Mishkin; Jack Clark"}, {"title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "journal": "", "year": "2015", "authors": "Kaiming Shaoqing Ren; Ross He; Jian Girshick;  Sun"}, {"title": "Neural machine translation of rare words with subword units", "journal": "", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning", "journal": "", "year": "2018", "authors": "Piyush Sharma; Nan Ding; Sebastian Goodman; Radu Soricut"}, {"title": "Vl-bert: Pretraining of generic visual-linguistic representations", "journal": "", "year": "2020", "authors": "Weijie Su; Xizhou Zhu; Yue Cao; Bin Li; Lewei Lu; Furu Wei; Jifeng Dai"}, {"title": "Patient knowledge distillation for bert model compression", "journal": "", "year": "2019", "authors": "Siqi Sun; Yu Cheng; Zhe Gan; Jingjing Liu"}, {"title": "Lxmert: Learning cross-modality encoder representations from transformers", "journal": "", "year": "2019", "authors": "Hao Tan; Mohit Bansal"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Learning two-branch neural networks for image-text matching tasks", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2018", "authors": "Liwei Wang; Yin Li"}, {"title": "Camp: Cross-modal adaptive message passing for text-image retrieval", "journal": "", "year": "2019", "authors": "Zihao Wang; Xihui Liu; Hongsheng Li; Lu Sheng; Junjie Yan; Xiaogang Wang; Jing Shao"}, {"title": "Languageagnostic visual-semantic embeddings", "journal": "", "year": "2019", "authors": "Jonatas Wehrmann; Douglas M Souza; Mauricio A Lopes; Rodrigo C Barros"}, {"title": "Approximate nearest neighbor negative contrastive learning for dense text retrieval", "journal": "", "year": "2020", "authors": "Lee Xiong; Chenyan Xiong; Ye Li; Kwok-Fung Tang; Jialin Liu; Paul Bennett; Junaid Ahmed; Arnold Overwijk"}, {"title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; R Russ; Quoc V Salakhutdinov;  Le"}, {"title": "STAIR captions: Constructing a large-scale Japanese image caption dataset", "journal": "", "year": "2017", "authors": "Yuya Yoshikawa; Yutaro Shigeto; Akikazu Takeuchi"}, {"title": "Modeling context in referring expressions", "journal": "", "year": "2016", "authors": "Licheng Yu; Patrick Poirson; Shan Yang; Alexander C Berg; Tamara L Berg"}, {"title": "Context-aware attention network for imagetext retrieval", "journal": "", "year": "2020", "authors": "Qi Zhang; Zhen Lei; Zhaoxiang Zhang; Stan Z Li"}, {"title": "Unified vision-language pre-training for image captioning and vqa", "journal": "", "year": "2020", "authors": "Luowei Zhou; Hamid Palangi; Lei Zhang; Houdong Hu; Jason J Corso; Jianfeng Gao"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Evolution of Image-Text Retrieval (ITR) paradigm.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: An overview of our proposed framework. (a) LightningDOT is pre-trained with Sementic-embedding Fused Mask Region Modeling (SMRM), Visual-embedding Fused Mask Language Modeling (VMLM) and Cross-modal Retrieval (CMR).(b) LightningDOT ITR pipeline (image retrieval as an example). Similarities between input textual query and image candidates are computed via dot product. During inference, image representations can be computed offline, and a re-ranker can be applied for better accuracy, still with significant speedup.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Fused Masked Language Modeling (VMLM) Masked Language Modeling (MLM) pre-training is first proposed by Devlin et al. (2019), where 15% of the words are masked 5 and the model is trained to reconstruct the masked words. Formally, we denote w m = {w m 1 , . . . , w m M } as masked tokens, where m \u2208 N M is the set of masked indices of size M , randomly sampled from a natural number N. w \\m are the unmasked words. MLM can be optimized by minimizing the negative log-likelihood:", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: An illustration of the proposed CMR Loss. Note that positive pairs lie in the diagonal of the matrix.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": "(b)): (i) Offline image feature extraction and encoding; (ii) Online retrieval with text query; and (iii) Online re-ranking with topretrieved images. Text retrieval is conducted in a symmetric manner.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 4 :4Figure 4: Retrieved top 10 images from the query \"Sky view of a blue and yellow biplane flying near each other.\" The ground truth is in the red rectangle.", "figure_data": ""}, {"figure_label": "79", "figure_type": "", "figure_id": "fig_7", "figure_caption": "Figure 7 :Figure 9 :79Figure 7: Retrieved top 10 images from the query \"A man and a little boy on skis on a ski hill.\" (Top picture is the ground truth.)", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "VSE++ * 41.3 69.2 81.2 30.3 59.1 72.4 58.9 52.9 80.5 87.2 39.6 70.1 79.5 68.3 SCO * 42.8 72.3 83.0 33.1 62.9 75.5 61.6 55.5 82.0 89.3 41.1 70.5 81.1 69.", "figure_data": "9GXN42.0-84.7 31.7-74.6-56.8-89.6 41.5-80.0-SCAN-single46.4 77.4 87.2 34.4 63.7 75.7 64.1 67.9 89.0 94.4 43.9 74.2 82.8 75.4R-SCAN45.4 77.9 87.9 36.2 65.6 76.7 65.0 66.3 90.6 96.0 51.4 77.8 84.9 77.8CAMP50.1 82.1 89.7 39.0 68.9 80.2 68.3 68.1 89.7 95.2 51.5 77.1 85.3 77.8CAAN52.5 83.3 90.9 41.2 70.3 82.9 70.2 70.1 91.6 97.2 52.8 79.0 87.9 79.8ViLBERT----------58.2 84.9 72.8-Unicoder-VL62.3 87.1 92.8 46.7 76.0 85.3 75.0 86.2 86.3 99.0 71.5 90.9 94.9 88.1UNITER-base64.4 87.4 93.1 50.3 78.5 87.2 76.8 85.9 97.1 98.8 72.5 92.3 95.9 90.4UNITER-large65.7 88.6 93.8 52.9 79.9 88.0 78.1 86.9 98.1 99.2 75.5 94.0 96.6 91.7OSCAR73.5 92.2 96.0 57.5 82.8 89.8 82.0 -------LightningDOT  *60.1 85.1 91.8 45.8 74.6 83.8 73.5 83.9 97.2 98.6 69.9 91.1 95.2 89.3+UNITERbase Re-Ranker 64.6 87.6 93.5 50.3 78.7 87.5 77.0 86.5 97.5 98.9 72.6 93.1 96.1 90.8+UNITERlarge Re-Ranker 65.7 89.0 93.7 53.0 80.1 88.0 78.2 87.2 98.3 99.0 75.6 94.0 96.5 91.8+OSCAR Re-Ranker74.2 92.4 96.0 57.4 82.7 89.9 82.1 -------"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": "-"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "LightningDOT 40.1 51.0 62.0 28.2 37.4 47.8 44.4 69.6 78.9 86.1 51.8 62.3 72.3 70.2 + Re-Ranker-base 47.9 58.5 67.8 35.7 45.2 55.2 51.7 74.2 81.7 88.2 56.9 66.7 75.6 73.9 + Re-Ranker-large 48.0 59.0 68.9 37.3 46.8 56.4 52.7 75.1 83.9 90.5 60.1 69.5 78.3 76.2", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Results on the extreme retrieval setting of full Flickr30k and full COCO datasets.", "figure_data": "Method#images SCAN Ours +Re-rankerFlickr30K-test 1,000 1.8\u00d7639\u00d746\u00d7COCO-test5,000 1.9\u00d7 1,927\u00d795\u00d7Flickr30K-full 31,014 1.8\u00d7 6,591\u00d71,255\u00d7COCO-full123,287 1.9\u00d7 23,869\u00d7 2,235\u00d7"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ": Speedup w.r.t. UNITER-base. We compare Light-ningDOT (Ours) and +Re-Ranker, plus a lightweight cross-attention method SCAN"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "R-CNN only 62.2 85.9 91.1 42.0 70.9 80.3 72.1 +Image Encoder 73.4 92.5 95.6 59.5 84.5 90.3 82.6 +PT \u2020 83.5 96.4 98.7 68.6 90.5 94.8 88.8 LightningDOT 85.2 96.4 98.7 69.9 90.4 94.5 89.2", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Ablation studies on model design over Flickr30K validation set. PT \u2020 indicates pre-training with MLM+MRM+CMR, while LightningDOT is pre-trained with VMLM+SMRM+CMR.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": ": Ablation studies on pre-training tasks overFlickr30K validation set after finetuning on the correspondingtraining set. All pre-training experiments are conducted onCOCO dataset only. PT is short for pre-training. PT(CMR)refers to pre-training using CMR task only, and PT(All) refersto pre-training with all of the three tasks."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Evaluation on multilingual image-text retrieval over", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Image retrieval time cost measured by computation time (in seconds) for each query. The computation time for UNITER and SCAN is roughly linear to #images. Numbers with * are estimated by running time on test set.", "figure_data": ""}], "doi": ""}