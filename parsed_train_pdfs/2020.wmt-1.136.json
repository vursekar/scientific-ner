{"authors": "Keshaw Singh", "pub_date": "", "title": "Adobe AMPS's Submission for Very Low Resource Supervised Translation Task at WMT20", "abstract": "In this paper, we describe our systems submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transformer-based neural machine translation model trained on original training bitext. We also conduct several experiments with backtranslation using limited monolingual data in our postsubmission work and include our results for the same. In one such experiment, we observe jumps of up to 2.6 BLEU points over the primary system by pretraining on a synthetic, backtranslated corpus followed by fine-tuning on the original parallel training data.", "sections": [{"heading": "Introduction", "text": "This paper describes our submissions to the shared task on Very Low Resource Supervised Machine Translation at WMT 2020. The task involved a single language pair: Upper Sorbian-German. We submit supervised neural machine translation (NMT) systems for both translation directions, Upper Sorbian\u2192German and German\u2192Upper Sorbian.\nNMT models (Sutskever et al., 2014;Bahdanau et al., 2015;Cho et al., 2014a) have achieved stateof-the-art performance on benchmark datasets for multiple language pairs. A big advantage of such systems over phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) models is that they can be trained end-to-end. The bulk of the development, however, has been limited to a handful of high-resource language pairs. The primary reason is that training a well-performing NMT system requires a large amount of parallel training data, which means a lot of equivalent investment in terms of resources. Koehn and Knowles (2017) show that when compared to PBSMT approaches, NMT models need more training data to achieve the same level of performance. 1 One of the most popular ways to increase the amount of parallel training data for supervised training is backtranslation (Sennrich et al., 2016a). We utilize this approach to improve upon the performance of our baseline models.\nAll of our systems follow the Transformer architecture (Vaswani et al., 2017). Our primary system is a supervised NMT model trained on the original training bitext. We also report our results on experiments with backtranslation, which were completed post the shared task and hence not a part of our primary submissions. We use the backtranslated data in two distinct ways -as a standalone parallel corpus, and to create a combined parallel corpus by mixing in a 1:1 ratio with the provided training data. We also report the performance of fine-tuned models originally trained only on the backtranslated data. In the following sections, we begin by briefly describing the Transformer architecture and backtranslation. We then discuss our experimental setup as well as our experiments with backtranslation. We conclude with a discussion of our results and possible future work.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Related Work", "text": "The Transformer model is the dominant architecture within current NMT models due to its superior performance on several language pairs. While still a sequence-to-sequence (Sutskever et al., 2014) model composed of an encoder and a decoder, Transformer models are highly parallelizable thanks to being composed purely of feedforward and self-attention layers rather than recurrent layers (Hochreiter and Schmidhuber, 1997;Cho et al., 2014b). The reader is encouraged to read the original paper (Vaswani et al., 2017) to gain a deeper understanding of the model. We adopt the Transformer base architecture available under the fairseq 2 (Ott et al., 2019) library for all our models.\nHowever, NMT models are known to be datahungry (Koehn and Knowles, 2017); their performance improves sharply with the availability of more parallel training data. Except for a few language pairs (e.g. English-German), most have little to no such data available. On the other hand, a far greater number of languages have a decent amount of monolingual data available online (e.g. Wikipedia).\nTo address this issue of lack of parallel data, Sennrich et al. (2016a) introduced the concept of backtranslation. It involves creating a synthetic parallel corpus by translating sentences from the target-side monolingual data to the source language and making corresponding pairs. A baseline target\u2192source model (PBSMT or NMT), trained with limited data, is generally used for this purpose. It enables the use of large corpora of monolingual data for several languages, the size of which is typically orders of magnitude larger than any corresponding bitext available. What is notable is that only the sourceside data is synthetic in such a scenario and the target-side still corresponds to original monolingual data.\nSome studies (Poncelas et al., 2018;Popel, 2018) have investigated the effects of varying the amount of backtranslated data as a proportion of the total training corpus, including training only on the synthetic dataset as a standalone corpus. We follow some of the related experiments conducted by Kocmi and Bojar (2019) on Gujarati-English (another low-resource pair) with a few exceptions. Besides, we also report performance when pretraining solely on the synthetic corpus following by finetuning on either original or mixed data. While not quite the same, one could think of this approach as having some similarities with transfer learning (Zoph et al., 2016) as well as domain adaptation (Luong and Manning, 2015;Freitag and Al-Onaizan, 2016) for machine translation. There has also been work on using sampling (Edunov et al., 2018) for generating backtranslations, but we stick to using beam search in this work.", "n_publication_ref": 13, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dataset", "text": "We used the complete parallel training corpus for our primary systems. In addition, we also made use of monolingual data from each language for 2 https://github.com/pytorch/fairseq two purposes -learning Byte Pair Encodings (BPE) (Sennrich et al., 2016b) and backtranslation. For Upper Sorbian (hsb), we used the monolingual corpora provided by the Sorbian Institute and by the Witaj Sprachzentrum. To control the quality of the backtranslated data, we chose not to use the data scraped from the web. For the German (de) side, we made use of the News Crawl 3 2009 dataset, as it is large enough to satisfy the requirements for our experiments.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Data Preprocessing", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Source", "text": "No. of sentences hsb-de, bitext 58,389 hsb, monolingual 540,994 de, monolingual 2,000,000 Moses toolkit (Koehn et al., 2007) was used for tokenization and punctuation normalization for all data. Before doing any additional preprocessing, we learned separate truecaser models using the toolkit. For this purpose, we took first 500K sentences from each of the monolingual corpora and aggregated them with the corresponding portion from the training bitext. After tokenizing and truecasing, we joined the parallel training corpus with the same monolingual data. We learned joint BPE 4 with 32K merge operations over this corpus and applied them to the parallel training data to get vocabularies for each language. Additionally, we used the clean-corpus-n.perl script within Moses to filter out sentences from the parallel corpus with more than 250 subwords as well as sentence length ratio over 1.5 in either direction. Final corpus statistics are presented in Table 1.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Training", "text": "Our primary system is a Transformer base model, trained on the parallel training corpus for both translation directions till 60 epochs. We keep most of the hyperparameters to their default values in fairseq. More precisely, we chose Adam (Kingma and Ba, 2015) as the optimizer and Adam betas were set to 0.9 and 0.98, respectively. The maximum number of tokens in each batch was set to 4096. Learning rate was set to 0.0005, with an inverse squared root decay schedule and 4000 steps of warmup updates. Label smoothing was set to 0.1 and dropout to 0.3. Label-smoothed cross-entropy was used as the training criterion. We trained all our models for a fixed number of epochs, determined separately for each system, and chose the last checkpoint for reporting BLEU (Papineni et al., 2002) scores on the test sets.\nAll training was done using a single NVIDIA P100 GPU. Due to the small amount of parallel training data, each epoch of training took about 90 seconds on average for the primary system.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Additional Backtranslation Experiments", "text": "In this section, we report our post-submission work on using monolingual data for backtranslation. We took the raw monolingual data that we describe in Section 3.1 and backtranslated with our primary submission models for the respective translation directions, i.e., hsb\u2192de for Upper Sorbian data and de\u2192hsb for German data. We used fairseq-generate function with a beam size of 5 for this purpose. Once again, we limited the number of subwords in each sentence to 250. Finally, we took all sentence pairs for backtranslated Upper Sorbian corpus and the first two million sentence pairs for the German corpus. Table 1 indicates the size of the backtranslated corpora by original language. For further experiments, we name the datasets as follows:\n\u2022 auth: Processed original training data.\n\u2022 synth: Backtranslated de\u2192hsb and hsb\u2192de corpora.\n\u2022 mixed: Augmented training data obtained by mixing auth with a portion of synth in 1:1 ratio, providing a total of 116,778 sentence pairs.\nWe define the following systems for making use of the backtranslated data. Note that the first system only differs from the primary system in the number of training epochs completed.\n\u2022 auth-from-scratch: This system has the same settings as the primary system. It was trained on the auth corpus till 80 epochs (as opposed to 60 for primary).\n\u2022 mixed-from-scratch: We trained models on mixed data from scratch for 40 epochs. 5\n\u2022 synth-from-scratch: Models were trained only on the synth datasets. To adjust for the difference in the size of the respective backtranslated corpora, we trained hsb\u2192de system for 10 epochs and de\u2192hsb system for 30 epochs.\n\u2022 synth-auth-finetune: We took the models trained via the previous system and fine-tuned them on auth data for 20 epochs in each translation direction.\n\u2022 synth-mixed-finetune: Same as the last model, except that fine-tuning was done on mixed data.\nFine-tuning was carried out by loading pretrained checkpoints and adding extra training flags in reset-optimizer and reset-lr-scheduler.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "The systems were evaluated on the blind test set (newstest2020) using automated metrics; no human evaluation was done. Table 2 shows cased BLEU scores for various systems. Our primary systems achieved a BLEU score of 47.6 for Upper Sorbian\u2192German and 45.2 for German\u2192Upper Sorbian translation. We achieved an improvement of 0.3 and 0.4 BLEU points, respectively, by training further till 80 epochs in each direction. We also evaluated a third system, synth-auth-finetune, as described in Section 4, which provided a jump of 2.6 points in BLEU score over the primary system for Upper Sorbian\u2192German and 2.5 for German\u2192Upper Sorbian.\nIn addition to evaluating on blind test sets, we also report BLEU scores on the development test set in the same table. Two outcomes are worth highlighting:\n\u2022 Model trained only on synth data for German\u2192Upper Sorbian translation matched the performance of a similar model trained on the authentic bitext.\n\u2022 Best results were obtained by fine-tuning a model trained on synth data with either auth or mixed.\nThe second result is notable since the regime of pretraining followed by fine-tuning improves the BLEU scores by up to 4 points on this test set when compared to training only on the original bitext. Moreover, while the model trained on synth was not able to match the performance of that trained on auth for Upper Sorbian\u2192German, it still provides the same benefits as German\u2192Upper Sorbian model when fine-tuned further. Looking at the small improvements achieved by using only the mixed corpus for training, increasing its size by combining upsampled auth data with more synth data might lead to even further jumps in the BLEU scores.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we described our Transformer model for supervised machine translation for Upper Sorbian-German language pair. We take note of relatively high BLEU scores achieved by our primary systems (and those of other participants) on this low-resource language pair, which could relate to the high quality of the training corpus. We also report results and takeaways from several experiments with backtranslated data completed post the shared task. A key result is matching the performance of a system trained on the original bitext with one trained on a limited amount of synthetic, backtranslated data. Domain mismatch and a difference in the quality of monolingual corpus might have prevented the system from achieving a similar result in the other direction. We notice big improvements in performance over the primary systems by following a \"pretraining then fine-tuning\" regime.\nAn interesting future work would be to measure the applicability of this approach to other lowresource language pairs. Additional systems could be added as well. For instance, models trained on mixed data and fine-tuned on auth data might provide a meaningful comparison. Prior work (Ding et al., 2019) has shown that the number of BPE merge operations has a significant effect on the performance of NMT systems. This work was pointed out during the review process and should be an avenue for further improvement of the model performance.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "The author would like to thank his manager for supporting this project, and the anonymous reviewers for their thoughtful comments which helped improve the presentation of this work.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2015", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merri\u00ebnboer; Dzmitry Bahdanau; Yoshua Bengio"}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merri\u00ebnboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"}, {"title": "A call for prudent choice of subword merge operations in neural machine translation", "journal": "", "year": "2019", "authors": "Shuoyang Ding; Adithya Renduchintala; Kevin Duh"}, {"title": "Understanding back-translation at scale", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Sergey Edunov; Myle Ott; Michael Auli; David Grangier"}, {"title": "Fast domain adaptation for neural machine translation", "journal": "", "year": "2016", "authors": "Markus Freitag; Yaser Al-Onaizan"}, {"title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "CUNI submission for low-resource languages in WMT news 2019", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Tom Kocmi; Ond\u0159ej Bojar"}, {"title": "Moses: Open source toolkit for statistical machine translation", "journal": "", "year": "2007", "authors": "Philipp Koehn; Hieu Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; Nicola Bertoldi; Brooke Cowan; Wade Shen; Christine Moran; Richard Zens; Chris Dyer; Ond\u0159ej Bojar; Alexandra Constantin; Evan Herbst"}, {"title": "Six challenges for neural machine translation", "journal": "Vancouver. Association for Computational Linguistics", "year": "2017", "authors": "Philipp Koehn; Rebecca Knowles"}, {"title": "Statistical phrase-based translation", "journal": "", "year": "2003", "authors": "Philipp Koehn; Franz J Och; Daniel Marcu"}, {"title": "Stanford neural machine translation systems for spoken language domains", "journal": "", "year": "2015", "authors": "Minh-Thang Luong; Christopher D Manning"}, {"title": "fairseq: A fast, extensible toolkit for sequence modeling", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Myle Ott; Sergey Edunov; Alexei Baevski; Angela Fan; Sam Gross; Nathan Ng; David Grangier; Michael Auli"}, {"title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"title": "Investigating backtranslation in neural machine translation", "journal": "", "year": "2018", "authors": "A Poncelas;  Shterionov;  Way;  De Buy; P Wenniger;  Passban"}, {"title": "Machine translation using syntactic analysis", "journal": "", "year": "2018", "authors": "Martin Popel"}, {"title": "Improving neural machine translation models with monolingual data", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Neural machine translation of rare words with subword units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Sequence to sequence learning with neural networks", "journal": "Curran Associates, Inc", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; Quoc V Le"}, {"title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"title": "Transfer learning for low-resource neural machine translation", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Barret Zoph; Deniz Yuret; Jonathan May; Kevin Knight"}], "figures": [{"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": ""}], "doi": "10.3115/v1/W14-4012"}