{"authors": "Sarah Sarabadani", "pub_date": "", "title": "Detection of Adverse Drug Reaction mentions in tweets using ELMo", "abstract": "This paper describes the models used by our team in SMM4H 2019 shared task (Weissenbacher et al., 2019). We submitted results for subtasks 1 and 2. For task 1 which aims to detect tweets with Adverse Drug Reaction (ADR) mentions we used ELMo embeddings which is a deep contextualized word representation able to capture both syntactic and semantic characteristics. For task 2, which focuses on extraction of ADR mentions, first the same architecture as task 1 was used to identify whether or not a tweet contains ADR. Then, for tweets positively classified as mentioning ADR, the relevant text span was identified by similarity matching with 3 different lexicon sets.", "sections": [{"heading": "Introduction and task description", "text": "Twitter is an ever-growing store of daily generated data. Given the huge number of tweets talking about drug-related issues, social media mining is applicable to areas such as pharmacovigilance (Lee et al., 2017;Nikfarjam et al., 2015;Ginn et al., 2014;Freifeld et al., 2014;Bian et al., 2012).\nTasks 1 and 2 focuses on detecting tweets with ADR and identifying location of mentions. We are provided with 25,672 tweets (2,374 positive and 23,298 negative) and approximately 5,000 unlabeled tweets as a validation set. For the second task, a subset of 2,367 tweets from the first task was provided (1,212 positive and 1,155 negative). The evaluation data comprises 1,000 tweets (~500 positive, ~500 negative).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Preprocessing", "text": "Stop words and punctuations were removed from tweets and all drug names found in the FDA's Approved Drug Products list 1 were replaced by the word \"drug\". Word stemming and tokenization were performed using nltk python library.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Methods", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "task 1", "text": "For this task, we used 4 deep learning models. The architecture of the first 3 models were relatively similar, differing in the embedding layer.\nThe first model involves character embedding with dimension equal to the total number of unique characters in training set including emojis. The output of this layer is fed to a series of 6 convolutional neural network layers (CNNs) with ReLU activation. Each CNN used 256 filters, with a filter size of 7 for the first two layers and 3 for the rest. Max pooling with size 3 was used for the first two and last CNNs. The CNNs' output was fed into a bidirectional LSTM (Bi-LSTM) with 2*200 units, whose output was flattened to feed into two dense layers. We used two fully connected layers with 1024 units each, ReLU activation, and dropout of 0.5. Finally, we used a dense layer with size two and softmax activation. We used Adam as the optimizer and binary cross-entropy as the loss function. The model was trained with 10 epochs and batch size of 128.\nThe second architecture was identical to the first, except the first layer was a word embedding using GloVe 2 pre-trained on Twitter data with embedding dimension of 100.\nThe third model was a concatenation of word and character embeddings. We combined the Bi-LSTM output of the first and second models and then applied dense layers as before.\nAfter building the above models, we tried to improve the outcomes by adding layers and features. We used a multi-head self-attention with an attention width of 15 and ReLU activation. We also explored the effect of sentiment features. Since the data classes were imbalanced, we tried to make class sizes equal by downsampling and upsampling. In downsampling, samples from the majority class (tweets without ADR mentions) were randomly sampled without replacement. In upsampling we did the opposite, adding samples from the minority class with replacement. None of these strategies substantially altered our baseline results.\nIn our final model, we used ELMo (Peters et al., 2018) (Embeddings from Language Models) with 1024 dimensions. In contrast to traditional word embeddings such as GloVe and word2vec, ELMo assigns each word to a vector as a function of the entire sentence containing that word. Therefore, the same word can have different embeddings depending on its context. Since ELMo already captures character-level information under the hood, we decided to encircle the complexity inside the embedding layer and used only two additional dense layers with 256 and 2 units, using ReLU and softmax activations, respectively.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Methods for task 2", "text": "To identify the text spans of ADR mentions, first the model developed for task 1 was used to determine whether each tweet mentions an ADR. Then the similarity between each tweet and 3 different lexicon sets (Nikfarjam et al. 3 , MedDRA (Medical Dictionary for Regulatory Activities) 4 , and CHV (Consumer Health Vocabulary) 5 ) was measured.\nTo calculate similarity, each tweet and lexicon was converted to a set of word stems. Since similarity measures such as cosine or Jaccard are highly affected by other non-ADR words, we defined similarity as the percent of word stems of a lexicon that exist in a tweet. For each tweet, only lexicons with a 100% match were kept.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Results, discussion, and next steps", "text": "Among all architectures, the best results came from ELMo embedding (F1 = 0.64). Therefore, we only submitted ELMo results with 5, 10, and 15 epochs. The model performed less well for the validation set (F1 = 0.41), below the average F1 score of 0.50 among all teams, which might result from overfitting. Using more sophisticated architecture after the embedding layer might improve performance.\nSince task 2's performance depends strongly on task 1, we also scored lower on this task compared to the team average (0.40 vs. 0.54). Since ADR phrases and tweets do not always lexically match, approaches such as named entity recognition (NER) might perform better.\nOther approaches to improve performance: Task 1:\n\u2022 Try other embeddings such as BERT I would also like to show my gratitude to Peter Leimbigler for comments that greatly improved the manuscript.\nFinally, special thanks go to Alfred Whitehead for supporting me to participate in this challenge.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Towards large-scale twitter mining for drug-related adverse events", "journal": "ACM", "year": "2012-10", "authors": "Jiang Bian; Umit Topaloglu; Fan Yu"}, {"title": "Digital drug safety surveillance: monitoring pharmaceutical products in twitter", "journal": "", "year": "2014", "authors": "Clark C Freifeld; John S Brownstein; Christopher M Menone; Wenjie Bao; Ross Filice"}, {"title": "Mining Twitter for adverse drug reaction mentions: a corpus and classification benchmark", "journal": "", "year": "2014-05", "authors": "Rachel Ginn; Pranoti Pimpalkhute; Azadeh Nikfarjam; Apurv Patki; O' Karen; Abeed Connor; Karen Sarker; Graciela Smith;  Gonzalez"}, {"title": "Adverse drug event detection in tweets with semi-supervised convolutional neural networks", "journal": "", "year": "2017-04", "authors": "Kathy Lee; Ashequl Qadir; A Sadid; Vivek Hasan; Aaditya Datla; Joey Prakash; Oladimeji Liu;  Farri"}, {"title": "Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features", "journal": "Journal of the American Medical Informatics Association", "year": "2015", "authors": "Azadeh Nikfarjam; Abeed Sarker; Karen O 'connor; Rachel Ginn; Graciela Gonzalez"}, {"title": "Deep contextualized word representations", "journal": "", "year": "2018", "authors": "Matthew E Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"title": "Overview of the Fourth Social Media Mining for Health (SMM4H) Shared Task at ACL 2019", "journal": "", "year": "", "authors": "Davy Weissenbacher; Abeed Sarker; Arjun Magge; Ashlynn Daughton; O' Karen; Michael Connor; Graciela Paul;  Gonzalez-Hernandez"}], "figures": [{"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Add part of speech (POS) tags", "figure_data": "\u2022 Experimentwithmorecomplexarchitectures after the ELMo layer\u2022 Add topic modeling and tweet clusterfeaturesTask 2:\u2022 Search Twitter for keywords from lexiconsets to augment the training set with newtweets which mention ADRs\u2022 Try NERAcknowledgmentI would like to thank Maheedhar Kolla whoprovided insight and expertise that significantlyassisted this work."}], "doi": ""}