{"authors": "Yonatan Bitton; Gabriel Stanovsky; Roy Schwartz; Michael Elhadad", "pub_date": "", "title": "Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA", "abstract": "Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets  quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified. While most contrast sets were created manually, requiring intensive annotation effort, we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Our method computes the answer of perturbed questions, thus vastly reducing annotation cost and enabling thorough evaluation of models' performance on various semantic aspects (e.g., spatial or relational reasoning). We demonstrate the effectiveness of our approach on the popular GQA dataset (Hudson and Manning, 2019) and its semantic scene graph image representation. We find that, despite GQA's compositionality and carefully balanced label distribution, two strong models drop 13-17% in accuracy on our automatically-constructed contrast set compared to the original validation set. Finally, we show that our method can be applied to the training set to mitigate the degradation in performance, opening the door to more robust models. 1   ", "sections": [{"heading": "Introduction", "text": "NLP benchmarks typically evaluate in-distribution generalization, where test sets are drawn i.i.d from a distribution similar to the training set. Recent works showed that high performance on test sets sampled in this manner is often achieved by exploiting systematic gaps, annotation artifacts, lexical cues and other heuristics, rather than learning meaningful task-related signal. As a result, 1 Our contrast sets and code are available at https://github.com/yonatanbitton/ AutoGenOfContrastSetsFromSceneGraphs.\nFigure 1: Illustration of our approach based on an example from the GQA dataset. Top: QA pairs and an image annotated with bounding boxes from the scene graph. Bottom: relations among the objects in the scene graph. First line at the top is the original QA pair, while the following 3 lines show our pertubated questions: replacing a single element in the question (a fence) with other options (a wall, men, an elephant), leading to a change in the output label. For each QA pair, the LXMERT predicted output is shown.\nthe out-of-domain performance of these models is often severely deteriorated (Jia and Liang, 2017;Ribeiro et al., 2018;Gururangan et al., 2018;Geva et al., 2019;McCoy et al., 2019;Feng et al., 2019;Stanovsky et al., 2019). Recently, Kaushik et al. (2019) and  introduced the contrast sets approach to probe out-of-domain generalization. Contrast sets are constructed via minimal modifications to test inputs, such that their label is modified. For example, in Fig. 1, replacing \"a fence\" with \"a wall\", changes the answer from \"Yes\" to \"No\". Since such perturbations introduce minimal additional semantic complexity, robust models are expected to perform similarly on the test and contrast sets. However, a range of NLP models severely degrade in performance on contrast sets, hinting that they do not generalize well . Except two recent exceptions for textual datasets (Li et al., 2020;Rosenman et al., 2020), contrast sets have so far been built manually, requiring extensive human effort and expertise.\nIn this work, we propose a method for automatic generation of large contrast sets for visual question answering (VQA). We experiment with the GQA dataset (Hudson and Manning, 2019). GQA includes semantic scene graphs (Krishna et al., 2017) representing the spatial relations between objects in the image, as exemplified in Fig. 1. The scene graphs, along with functional programs that represent the questions, are used to balance the dataset, thus aiming to mitigate spurious dataset correlations. We leverage the GQA scene graphs to create contrast sets, by automatically computing the answers to question perturbations, e.g., verifying that there is no wall near the puddle in Fig. 1.\nWe create automatic contrast sets for 29K samples or \u224822% of the validation set. We manually verify the correctness of 1,106 of these samples on Mechanical Turk. Following, we evaluate two leading models, LXMERT (Tan and Bansal, 2019) and MAC (Hudson and Manning, 2019) on our contrast sets, and find a 13-17% reduction in performance compared to the original validation set. Finally, we show that our automatic method for contrast set construction can be used to improve performance by employing it during training. We augment the GQA training set with automatically constructed training contrast sets (adding 80K samples to the existing 943K in GQA), and observe that when trained with it, both LXMERT and MAC improve by about 14% on the contrast sets, while maintaining their original validation performance.\nOur key contributions are: (1) We present an automatic method for creating contrast sets for VQA datasets with structured input representations; (2) We automatically create contrast sets for GQA, and find that for two strong models, performance on the contrast sets is lower than on the original validation set; and (3) We apply our method to augment the training data, improving both models' performance on the contrast sets.", "n_publication_ref": 14, "n_figure_ref": 4}, {"heading": "Automatic Contrast Set Construction", "text": "To construct automatic contrast sets for GQA we first identify a large subset of questions requiring specific reasoning skills ( \u00a72.1). Using the scene graph representation, we perturb each question in a manner which changes its gold answer ( \u00a72.2). Finally, we validate the automatic process via crowdsourcing ( \u00a72.3).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Identifying Recurring Patterns in GQA", "text": "The questions in the GQA dataset present a diverse set of modelling challenges, as exemplified in Table 1, including object identification and grounding, spatial reasoning and color identification. Following the contrast set approach, we create perturbations testing whether models are capable of solving questions which require this skill set, but that diverge from their training distribution.\nTo achieve this, we identify commonly recurring question templates which specifically require such skills. For example, to answer the question \"Are there any cats near the boat?\" a model needs to identify objects in the image (cats, boat), link them to the question, and identify their relative position.\nWe identify six question templates, testing various skills (Table 1). We abstract each question template with a regular expression which identifies the question types as well as the physical objects, their attributes (e.g., colors), and spatial relations. Overall, these regular expressions match 29K questions in the validation set (\u224822%), and 80K questions in the training set (\u22488%).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Perturbing Questions with Scene Graphs", "text": "We design a perturbation method which guarantees a change in the gold answer for each question template. For example, looking at Fig. 2, for the question template are there X near the Y? (e.g., \"Is there any fence near the players?\"), we replace either X or Y with a probable distractor (e.g.\" replace \"fence\" with \"trees\").\nWe use the scene graph to ensure that the answer to the question is indeed changed. In our example, this would entail grounding \"players\" in the question to the scene graph (either via exact match or several other heuristics such as hard-coded lists of synonyms or co-hyponyms), locating its neighbors, and verifying that none of them are \"trees.\" We then apply heuristics to fix syntax (e.g., changing from singular to plural determiner, see Appendix A.3), and verify that the perturbed sample Are there any cats near the boat? \u2192 Is there any bush near the boat? Is the X Rel the Y?\nIs the boy to the right of the man? \u2192 Is the boy to the left of the man? Is the X Rel the Y?\nIs the boy to the right of the man? \u2192 Is the zebra to the right of the man? does not already exist in GQA. The specific perturbation is performed per question template. In question templates with two objects (X and Y), we replace X with X', such that X' is correlated with Y in other GQA scene graphs. In question templates with a single object X, we replace X with a textually-similar X'. For example in the first row in Table 1 we replace dishwasher with dishes. Our perturbation code is publicly available. This process may yield an arbitrarily large number of contrasting samples per question, as there are many candidates for replacing objects participating in questions. We report experiments with up to 1, 3 and 5 contrasting samples per question.\nIllustrating the perturbation process. Looking at Fig. 1, we see the scene-graph information: objects have bounding-boxes around them in the image (e.g., zebra); Objects have attributes (wood is an attribute of the fence object); and there are relationships between the objects (the puddle is to the right of the zebra, and it is near the fence). The original (question, answer) pair is (\"is there a fence near the puddle?\", \"Yes\"). We first identify the question template by regular expressions: \"Is there X near the Y\", and isolate X=fence, Y=puddle. The answer is \"Yes\", so we know that X is indeed near Y. We then use the existing information given in the scene-graph. We search for X' that is not near Y. To achieve this, we sample a random object (wall), and verify that it doesn't exist in the set of scenegraph objects. This results in a perturbed example \"Is there a wall near the puddle?\", and now the ground truth is computed to be \"No\". Consider a different example: (\"Is the puddle to the left of the zebra?\", \"Yes\"). We identify the question template \"Is the X Rel the Y\", where X=puddle, Rel=to the left, Y=zebra. The answer is \"Yes\". Now we can easily change Rel'=to the right, resulting in the (question, answer) pair (\"Is the puddle to the right of the zebra?\", \"No\").\nWe highlight the following: (1) This process is done entirely automatically (we validate it in Section 2.3); (2) The answer is deterministic given the information in the scene-graph; (3) We do not produce unanswerable questions. If we couldn't find an alternative atom for which the presuppositions hold, we do not create the perturbed (question, answer) pair; (4) Grounding objects from the question to the scene-graph can be tricky. It can involve exact match, number match (dogs in the question, and dog in the scene-graph), hyponyms (animal in the question, and dog in the scene-graph), and synonyms (motorbike in the question, and motorcycle in the scene-graph). The details are in the published code; (5) The only difference between the original and the perturbed instance is a single atom: an object, relationship, or attribute.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Validating Perturbed Instances", "text": "To verify the correctness of our automatic process, we sampled 553 images, each one with an original and perturbed QA pair for a total of 1,106 instances (\u22484% of the validation contrast pairs). The (image, question) pairs were answered independently by human annotators on Amazon Mechanical Turk (see Fig. 3 in Appendix A.4), oblivious to whether the question originated from GQA or from our automatic contrast set. We found that the workers were able to correctly answer 72.3% of the perturbed questions, slightly lower than their performance on the original questions (76.6%). 2 We observed high agreement between annotators (\u03ba = 0.679).\nOur analysis shows that the human performance difference between the perturbed questions and the original questions can be attributed to the scene Figure 2: GQA image (left) with example perturbations for different question templates (right). Each perturbation aims to change the label in a predetermined manner, e.g., from \"yes\" to \"no\".", "n_publication_ref": 1, "n_figure_ref": 2}, {"heading": "Model", "text": "Training  graph annotation errors in the GQA dataset: 3.5% of the 4% difference is caused by a discrepancy between image and scene graph (objects appearing in the image and not in the graph, and vice versa). Examples are available in Fig. 5 in Appendix A.5.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Experiments", "text": "We experiment with two top-performing GQA models, MAC (Hudson and Manning, 2018) and LXMERT (Tan and Bansal, 2019), 3 to test their generalization on our automatic contrast sets, leading to various key observations. Models struggle with our contrast set.  had the smallest performance drop, potentially because the models performance on such multi-class, subjective questions is relatively low to begin with.\nTraining on perturbed set leads to more robust models. Previous works tried to mitigate spurious datasets biases by explicitly balancing labels during dataset construction (Goyal et al., 2017;Zhu et al., 2016;Zhang et al., 2016) or using adversarial filtering (Zellers et al., 2018(Zellers et al., , 2019. In this work we take an inoculation approach (Liu et al., 2019)   This allows us to measure the contrast consistency  of our contrast set, defined as the percentage of the contrast sets for which a model's predictions are correct for all examples in the set (including the original example). For example, in Fig. 1 the set size is 4, and only 2/4 predictions are correct. We experiment with 1, 3, and 5 augmentations per question with the LXMERT model trained on the original GQA training set. Our results (Table 4) show that sampling more objects leads to similar accuracy levels for the LXMERT model, indicating that quality of our contrast sets does not depend on the specific selection of replacements. However, we observe that consistency drops fast as the size of the contrast sets per QA instance grows, indicating that model success on a specific instance does not mean it can generalize robustly to perturbations.", "n_publication_ref": 7, "n_figure_ref": 1}, {"heading": "Discussion and Conclusion", "text": "Our results suggest that both MAC and LXMERT under-perform when tested out of distribution. A remaining question is whether this is due to model architecture or dataset design.  claim that both of these models are prone to fail on compositional generalization because they do not decompose the problem into smaller sub-tasks. Our results support this claim. On the other hand, it is possible that a different dataset could prevent these models from finding shortcuts. Is there a dataset that can prevent all shortcuts? Our automatic method for creating contrast sets allows us to ask those questions, while we believe that future work in better training mechanisms, as suggested in  and Jin et al. (2020), could help in making more robust models.\nWe proposed an automatic method for creating contrast sets for VQA datasets that use annotated scene graphs. We created contrast sets for the GQA dataset, which is designed to be compositional, balanced, and robust against statistical biases. We observed a large performance drop between the original and augmented sets. As our contrast sets can be generated cheaply, we further augmented the GQA training data with additional perturbed questions, and showed that this improves models' performance on the contrast set. Our proposed method can be extended to other VQA datasets.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A Appendix Ethical Considerations", "text": "We created contrast sets automatically, and verified their correctness via the crowdsourcing annotation of a sample of roughly 1K instances. Section 2.3 describes the annotation process on Amazon Mechanical Turk. The images and original questions were sampled from the public GQA dataset (Hudson and Manning, 2019), in the English language. Fig. 3 in Appendix A.4 provides example of the annotation task. Overall, the crowdsourcing task resulted in \u22486 hours of work, which paid an average of 11USD per hour per annotator.\nReproducibility The augmentations were performed with a MacBook Pro laptop. Augmentations for the validation data takes < 1 hour per question template, and for the training data < 3 hours per question template. Overall process, < 24 hours.\nThe experiments have been performed with the public implementations of MAC (Hudson and Manning, 2018) and LXMERT (Tan and Bansal, 2019), models: https: //github.com/airsplay/lxmert, https://github.com/stanfordnlp/ mac-network/.\nThe configurations were modified to not include the validation set in the training process. The experiments were performed with a Linux virtual machine with a NVIDIA's Tesla V100 GPU. The training took \u223c1-2 days in each model. Validation took \u223c 30 minutes.", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "A.1 Generated Contrast Sets Statistics", "text": "Table 5 reports the basic statistics of automatic contrast sets generation method when applied on the GQA validation dataset. It shows the overall number of images and QA pairs that matched the 6 question types we identified. Tables 6 shows the statistics per question type, indicating how productive each augmentation method is. Tables 7 and  8 ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.3 Linguistic Heuristics for Questions Generation", "text": "For each question type, we select an object in the image scene graph, and update the question by substituting the reference to this object by another object. When substituting one object by another, we need to adjust the question to keep it fluent. Table 10 shows the specific linguistic rules we verify when performing this substitution.\nA.4 Annotation Task for Verifying Generated Contrast Sets Fig. 3 shows the annotation task that is shown to Turkers to validate the QA pairs generated by our method.     ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "A.5 Examples", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "We thank the reviewers for the helpful comments and feedback. We thank the authors of GQA for building the dataset, and the authors of LXMERT and MAC for sharing their code and making it usable. This work was supported in part by the Center for Interdisciplinary Data Science Research at the Hebrew University of Jerusalem, and research gifts from the Allen Institute for AI.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Linguistic rule Explanation Examples", "text": "Singular vs. plural\nIf the noun is singular and countable: add \"a\" or \"an\" If needed, replace \"Are\" and \"Is\" \"a fence\", \"men\" \"a boy\", \"an elephant\" Definite vs. indefinite Do not change definite articles to indefinite articles, and vice versa \"is there any fence near the boy\" suggests that there is a boy in the scene graph, which is not always correct", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "General vs. specific", "text": "Meaning can be changed When replacing to general or specific terms \"Cats in the image\" =>\"Animals in the image\", \"Animals not in the image\" =>\"cats not in the image\",\nThe opposite directions not necessarily holds Countable vs. uncountable If the noun is uncountable, do not add \"a\" or \"an\" \"A cat\", \"water\"\nTable 10: Partial linguistic rules to notice using our method. ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Latent compositional representations improve systematic generalization in grounded question answering", "journal": "", "year": "2020", "authors": "Ben Bogin; Sanjay Subramanian; Matt Gardner; Jonathan Berant"}, {"title": "Misleading failures of partial-input baselines", "journal": "", "year": "2019", "authors": "Eric Shi Feng; Jordan Wallace;  Boyd-Graber"}, {"title": "Evaluating models' local decision boundaries via contrast sets", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Matt Gardner; Yoav Artzi; Victoria Basmov; Jonathan Berant; Ben Bogin; Sihao Chen; Pradeep Dasigi; Dheeru Dua; Yanai Elazar; Ananth Gottumukkala; Nitish Gupta; Hannaneh Hajishirzi; Gabriel Ilharco; Daniel Khashabi; Kevin Lin; Jiangming Liu; Nelson F Liu; Phoebe Mulcaire; Qiang Ning; Sameer Singh; Noah A Smith; Sanjay Subramanian; Reut Tsarfaty; Eric Wallace; Ally Zhang; Ben Zhou"}, {"title": "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets", "journal": "", "year": "2019", "authors": "Mor Geva; Yoav Goldberg; Jonathan Berant"}, {"title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "journal": "", "year": "2017", "authors": "Yash Goyal; Tejas Khot; Douglas Summers-Stay; Dhruv Batra; Devi Parikh"}, {"title": "Annotation artifacts in natural language inference data", "journal": "", "year": "2018", "authors": "Swabha Suchin Gururangan; Omer Swayamdipta; Roy Levy; Samuel Schwartz; Noah A Bowman;  Smith"}, {"title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "journal": "", "year": "2019", "authors": "A Drew; Christopher D Hudson;  Manning"}, {"title": "Compositional attention networks for machine reasoning", "journal": "", "year": "2018", "authors": "Arad Drew; Christopher D Hudson;  Manning"}, {"title": "Adversarial examples for evaluating reading comprehension systems", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Robin Jia; Percy Liang"}, {"title": "Visually grounded continual learning of compositional phrases", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Xisen Jin; Junyi Du; Arka Sadhu; Ram Nevatia; Xiang Ren"}, {"title": "Learning the difference that makes a difference with counterfactually-augmented data", "journal": "", "year": "2019", "authors": "Divyansh Kaushik; Eduard Hovy; Zachary Lipton"}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "journal": "International journal of computer vision", "year": "2017", "authors": "Ranjay Krishna; Yuke Zhu; Oliver Groth; Justin Johnson; Kenji Hata; Joshua Kravitz; Stephanie Chen; Yannis Kalantidis; Li-Jia Li; David A Shamma"}, {"title": "Linguistically-informed transformations (LIT): A method for automatically generating contrast sets", "journal": "", "year": "2020", "authors": "Chuanrong Li; Lin Shengshuo; Zeyu Liu; Xinyi Wu; Xuhui Zhou; Shane Steinert-Threlkeld"}, {"title": "Inoculation by fine-tuning: A method for analyzing challenge datasets", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Nelson F Liu; Roy Schwartz; Noah A Smith"}, {"title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "journal": "", "year": "2019", "authors": "Tom Mccoy; Ellie Pavlick; Tal Linzen"}, {"title": "Semantically equivalent adversarial rules for debugging NLP models", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Sameer Marco Tulio Ribeiro; Carlos Singh;  Guestrin"}, {"title": "Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Alon Shachar Rosenman; Yoav Jacovi;  Goldberg"}, {"title": "Evaluating gender bias in machine translation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Gabriel Stanovsky; Noah A Smith; Luke Zettlemoyer"}, {"title": "LXMERT: Learning cross-modality encoder representations from transformers", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Hao Tan; Mohit Bansal"}, {"title": "SWAG: A large-scale adversarial dataset for grounded commonsense inference", "journal": "", "year": "2018", "authors": "Rowan Zellers; Yonatan Bisk; Roy Schwartz; Yejin Choi"}, {"title": "HellaSwag: Can a machine really finish your sentence?", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Rowan Zellers; Ari Holtzman; Yonatan Bisk; Ali Farhadi; Yejin Choi"}, {"title": "Yin and yang: Balancing and answering binary visual questions", "journal": "", "year": "2016", "authors": "Peng Zhang; Yash Goyal; Douglas Summers-Stay; Dhruv Batra; Devi Parikh"}, {"title": "Visual7w: Grounded question answering in images", "journal": "", "year": "2016", "authors": "Yuke Zhu; Oliver Groth; Michael Bernstein; Li Fei-Fei"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Thebat the batter is holding has what color? Brown \u2192 The helmet has what color? Blue Is there any fence near the players? Yes \u2192 Are there any trees near the players? No Do you see either bakers or photographers? No \u2192 Do you see either spectators or photographers? Yes Is the catcher to the right of an umpire? No \u2192 Is the catcher to the right of a batter? Yes Is the catcher to the right of an umpire? No \u2192 Is the catcher to the left of an umpire? Yes", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "(a) First case example -multiple objects Augmented question: On which side of the photo are the bananas? Expected answer: right \"bananas\" are annotated in green text color in the right side of the image, but it also appears in additional locations (b) Second case example -missing annotation Augmented question: Do you see either a brown chair or couch in this picture? Expected answer: No We can see a couch in the left side of the image which is not annotated in the scene graph (c) Third case example -incorrect annotation Augmented question: Do you see either any windows or fences? Expected answer: Yes We can see an incorrect annotation of \"windows\" on the person shirt in azure text color.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: Scene graph annotation mistakes", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "On which side is the X? Relational (left vs. right) On which side is the dishwasher? \u2192 On which side are the dishes?What color is the X?Color identification What color is the cat?\u2192 What color is the jacket?Do you see X or Y? Compositions Do you see laptops or cameras?\u2192 Do you see headphones or cameras? Are there X near the Y? Spatial, relational", "figure_data": "Question templateTested attributesExample"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Question templates with original question examples, and generated perturbations modifying the answer. Italic text indicates variables, bold text indicates the perturbed atoms.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "shows that despite GQA's emphasis on datasetbalance and compositionality, both MAC andLXMERT degraded on the contrast set: MAC64.9% \u2192 51.5% and LXMERT 83.9% \u2192 67.2%,compared to only 4% degradation in human perfor-mance. Full breakdown of the results by templateis shown in Table 3. As expected, question tem-plates that reference two objects (X and Y ) resultin larger performance drop compared to those con-taining a single object (X). Questions about colors3 MAC and LXMERT are the top two models in the GQAleaderboard with a public implementation as of the timeof submission: https://github.com/airsplay/lxmert and https://github.com/stanfordnlp/mac-network/."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ": Model accuracy on the original and augmentedvalidation set by question template for a maximum 5augmentations per instance."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Accuracy and consistency results for the LXMERT model on different contrast set sizes.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "shows the same statistics for the GQA Training dataset.", "figure_data": "# Aug. QA pairsMax 1Max 3Max 5# Images10,69610,69610,696# QA pairs132,062 132,062 132,062# Aug. QA pairs12,96226,18932,802# Aug. images6,1666,1666,166% Aug. images57.6%57.6%57.6%% Aug. QA pairs9.8%19.8%24.8%"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Validation data augmentation statistics", "figure_data": "Question template# Aug. QA pairsMax 1 Max 3 Max 5On which side is the X?2,5164,8895,617What color is the X?4,608 10,424 12,414Are there X near the Y?3828671,320Do you see X or Y?1,5064,5147,516Is the X Rel the Y?7661,3141,392Is the X Rel the Y?1,4171,4161,416"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ": Augmentation statistics per question templatefor the validation dataA.2 Models Performance Breakdown byQuestion Type and Number ofAugmentations"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "shows the breakdown of the performance of the MAC and LXMERT models per question type, on both the original GQA validation set and on the augmented contrast sets on validation.The LXMERT model has two stages of training: pre-training on several datasets (which includes GQA training and validation data) and fine-tuning. To avoid inflating results on the validation data, we re-trained the pre-training stage without the GQA data, and fine-tuned on the training sets. Table2. We discovered lower performance on the original set (-\u223c5%) with both models, but the same improvement on the augmented set (+\u223c10).", "figure_data": "# Images72,140# QA pairs943,000# Aug. QA pairs89,936# Aug. images43,463% Aug. images60.2%% Aug. QA pairs9.5%"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Training data augmentation statistics", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Augmentation statistics per question template for the training data", "figure_data": "Original DatasetAug. datasetMax 1Max 3Max 5Size MAC LXMERT MAC LXMERTSize MAC LXMERTSize MAC LXMERTOn which side is the X?2,53868%94%56%79%4,92757%80%5,66257%81%What color is the X?4,65449%69%48%62% 10,50649%62% 12,49849%62%Are there X near the Y?38285%98%72%84%86769%80%1,32066%79%Do you see X or Y?1,50688%95%53%63%4,20553%64%6,67953%65%Is the X Rel the Y?76685%96%42%67%1,31444%69%1,39244%69%Is the X Rel the Y?1,41771%93%38%55%1,41738%55%1,41738%55%Overall11,26365%84%50%66% 23,23651%67% 28,96852%67%"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Model accuracy by question template and maximum number of augmentations. Italic text indicates variables, bold text indicates the perturbed atoms.", "figure_data": ""}], "doi": "10.18653/v1/P19-1554"}