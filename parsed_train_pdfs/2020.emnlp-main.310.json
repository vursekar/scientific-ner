{"authors": "Deyu Zhou; Xuemeng Hu; Rui Wang", "pub_date": "", "title": "Neural Topic Modeling by Incorporating Document Relationship Graph", "abstract": "Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community. In this paper, we propose Graph Topic Model (GTM), a GNN based neural topic model that represents a corpus as a document relationship graph. Documents and words in the corpus become nodes in the graph and are connected based on document-word cooccurrences. By introducing the graph structure, the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution. Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach.", "sections": [{"heading": "Introduction", "text": "Probabilistic topic models (Blei, 2012) are tools for discovering main themes from large corpora. The popular Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants (Lin and He, 2009;Zhao et al., 2010;Zhou et al., 2014) are effective in extracting coherent topics in an interpretable manner, but usually at the cost of designing sophisticated and model-specific learning algorithm. Recently, neural topic modeling that utilizes neuralnetwork-based black-box inference has been the main research direction in this field. Notably, NVDM (Miao et al., 2016) employs variational autoencoder (VAE) (Kingma and Welling, 2013) to model topic inference and document generation. Specifically, NVDM consists of an encoder inferring topics from documents and a decoder generating documents from topics, where the latent topics are constrained by a Gaussian prior. Srivastava and Sutton (2017) argued that Dirichlet distribution is a more appropriate prior for topic modeling than Gaussian in NVDM and proposed ProdLDA that approximates the Dirichlet prior with logistic normal. There are also attempts that directly enforced a Dirichlet prior on the document topics. W-LDA (Nan et al., 2019) models topics in the Wasserstein autoencoders (Tolstikhin et al., 2017) framework and achieves distribution matching by minimizing their Maximum Mean Discrepancy (MMD) (Gretton et al., 2012), while adversarial topic model (Wang et al., 2019a(Wang et al., ,b, 2020 directly generates documents from the Dirichlet prior and such a process is adversarially trained with a discriminator under the framework of Generative Adversarial Network (GAN) (Goodfellow et al., 2014).\nRecently, due to the effectiveness of Graph Neural Networks (GNNs) (Li et al., 2015;Kipf and Welling, 2016;Zhou et al., 2018) in embedding graph structures, there is a surge of interests of applying GNN to natural language processing tasks (Yasunaga et al., 2017;Song et al., 2018;Yao et al., 2019). For example, GraphBTM (Zhu et al., 2018) is a neural topic model that incorporates the graph representation of a document to capture biterm cooccurrences in the document. To construct the graph, a sliding window over the document is employed and all word pairs in the window are connected.\nA limitation of GraphBTM is that only word relationships are considered while ignoring document relationships. Since a topic is possessed by a subset of documents in the corpus, we believe that the topical neighborhood of a document, i.e., documents with similar topics, would help determine the topics of a document. To this end, we propose Graph Topic Model (GTM), a neural topic model that a corpus is represented as a document relationship graph where documents and words in the corpus are nodes and they are connected based on document-word co-occurrences. In GTM, the topical representation of a document node is aggregated from its multi-hop neighborhood, including both document and word nodes, using Graph Convolutional Network (GCN) (Kipf and Welling, 2016). As GCN is able to capture high-order neighborhood relationships, GTM is essentially capable of modeling both word-word and doc-doc relationships. In specific, the relationships between relevant documents are established by their shared words, which is desirable for topic modeling as documents belonging to one topic typically have similar word distributions.\nThe main contributions of the paper are:\n\u2022 We propose GTM, a novel topic model that incorporates document relationship graph to enrich document and word representations.\n\u2022 We extensively experimented on three datasets and the results demonstrate the effectiveness of the proposed approach.\n2 Graph Topic Model", "n_publication_ref": 22, "n_figure_ref": 0}, {"heading": "Graph Representation of the Corpus", "text": "We represent the whole corpus D with an undirected graph G = (N , E), where N and E are nodes and edges in the graph respectively. To model both words and documents, each of them is represented as a node n i \u2208 N , which gives rise to N = V + D nodes in total, where V is the size of vocabulary V and D is the number of documents in corpus D. An edge (n i , n j ) indicates the relevance of node n i and n j , whose weight is determined by\nA i,j = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 TF-IDF ij , i \u2208 D and j \u2208 V TF-IDF ji , i \u2208 V and j \u2208 D 1, i = j 0, otherwise(1)\nwhere A is the adjacency matrix of G and TF-IDF ij denotes the max-normalized TF-IDF (Term Frequency-Inverse Document Frequency) weight of word j in document i. Besides self-connections, we only apply positive weights to edges between documents and words, while rely on the model to capture higher-order relationships, e.g. doc-doc and word-word relationships, by applying graph convolutions on graph G.\nI X X T I \uf8ee \uf8f0 \uf8f9 \uf8fb E\u1e90 Z Dirichlet(\u03b1) GX L rec (X,X) MMD(P Z , Q\u1e90 )\nFigure 1: The framework of GTM. Circles denote neural networks. X, I,\u1e90,X, Z are the TF-IDF matrix of the corpus, an identity matrix, latent topics of all documents, reconstructed word weights and topic distributions drawn from the Dirichlet prior respectively. L rec (X,X) and MMD(P Z , Q Z ) are training objectives.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Model Architecture", "text": "The proposed GTM consists of an encoder E and a decoder G. The framework is shown in Figure 1, and we detail the architecture in the following.\nThe encoder network E maps nodes in G to their topic distributions by iteratively applying graph convolution to the node features. Following (Kipf and Welling, 2016), the layer-wise propagation rule of the graph convolution at layer l + 1 \u2208 [1, L] is defined as\nH (l+1) = \u03c3(D \u2212 1 2 AD \u2212 1 2 H (l) W (l) ) (2)\nwhere A \u2208 R N \u00d7N is the adjacency matrix of G,\nD ii = j A ij , W (l) \u2208 R d (l) \u00d7d (l+1\n) is a layerspecific weight matrix where d (l) is the output size of layer l, and \u03c3 denotes an activation function that is LeakyReLU (Maas et al., 2013) in this paper.\nH (l) \u2208 R N \u00d7d (l)\nis the activations of all nodes at layer l and H (0) i is the embedding of node i. At each encoder layer, what the graph convolution does is aggregating node features from a node's first-order neighborhood, which consequently enlarges the receptive field of the central node and enables the information propagation between relevant nodes. After successively applying L graph convolution layers, the encoding of a node essentially involves its L th -order neighborhood. With L \u2265 2, doc-doc and word-word relationships are naturally captured in the topic inference process.\nWe also add a batch normalization (Ioffe and Szegedy, 2015) after each graph convolution. After the graph encoding, a softmax is further applied to the node features of a document to produce a multinomial topic distribution\u1e91 \u2208 R K , where K is the topic number.\nBased on the inferred topic distribution\u1e91, the decoder network G tries to restore the original document representations. To achieve this goal, we employ a 2-layer MLP with LeakyReLU activation and batch normalization in the first layer. The output of the MLP decoder is then softmax-normalized to generate a word distributionx \u2208 R V .\nThe decoder is also used to interpret topics. In this case, we feed to the decoder an identity matrix I \u2208 R K\u00d7K , and the decoder output G(I) i is the word distribution of the i-th topic.", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Training Objective", "text": "Based on the Wasserstein Autoencoder (Tolstikhin et al., 2017) framework, the training objective of GTM is to minimize the document reconstruction loss when the latent topic space is constrained by a prior distribution. The reconstruction loss is defined as\nL rec (X,X) = \u2212E(x logx),(3)\nwhere x denotes the TF-IDF of a document andx is the reconstructed word distribution corresponding to x. we use TF-IDF as the reconstruction target since TF-IDF basically preserves the relative importance of words and reduces some background noise that may hurt topic modeling, e.g., stop words. We impose a Dirichlet prior, the conjugate prior of the multinomial distribution, to the latent topic distributions. Following W-LDA (Nan et al., 2019), we achieve this goal by minimizing the Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) between the distribution Q\u1e90 of inferred topic distributions\u1e91 and the Dirichlet prior P Z from which we draw multinomial noises z:\nMMD(PZ , Q\u1e90 ) = 1 m(m \u2212 1) i =j k(z (i) , z (j) )+ 1 n(n \u2212 1) i =j k(\u1e91 (i) ,\u1e91 (j) ) \u2212 2 mn i,j k(z (i) ,\u1e91 (j) ), (4\n)\nwhere m and n are the number of samples from Z and\u1e90 respectively (m and n are batch sizes and they are equal in our experiments), and k : Z\u00d7Z \u2192 R is the kernel function. We use the information diffusion kernel (Lebanon and Lafferty, 2003) as in W-LDA:\nk(z, z ) = exp(\u2212 arccos 2 ( K i=1 z i z i )), (5)\nwhich is sensitive to points near the simplex boundary and thus more suitable for the sparse topic distributions.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We evaluate our model on three datasets: 20Newsgroups consisting of 11,259 documents, Grolier consisting of 29,762 documents, and NYTimes consisting of 99,992 documents. We use the preprocessed 20Newsgroups of (Srivastava and Sutton, 2017), and preprocessed Grolier and NYTimes of (Wang et al., 2019a). We compare the performance of our model with LDA (Blei et al., 2003), NVDM (Miao et al., 2016), ProdLDA (Srivastava and Sutton, 2017), GraphBTM (Zhu et al., 2018), ATM (Wang et al., 2019a) and W-LDA (Nan et al., 2019) using topic coherence measures (R\u00f6der et al., 2015). To quantify the understandability of the extracted topics, a topic coherence measure aggregates the relatedness scores of the topic words (topweighted words) of each topic, where the word relatedness scores are estimated based on word co-occurrence statistics on a large external corpus. For example, the NPMI coherence measure (Aletras and Stevenson, 2013) applies a sliding window of size 10 over the Wikipedia corpus to calculate NPMI (Bouma, 2009) for word pairs. We use three topic coherence measures in our experiments: C A (Aletras and Stevenson, 2013), C P (R\u00f6der et al., 2015), and NPMI. The topic coherence scores are calculated using Palmetto (R\u00f6der et al., 2015) 1 .   (20,30,50,75,100). Bold values indicate the best performing model under the corresponding dataset/metric setting. We use 2 graph convolution layers with output dimensions of 100 and K respectively in the encoder. The hidden size of the decoder is also set to 100. We use the RMSProp (Hinton et al., 2012) optimizer with a learning rate of 0.01 to train the model for 100 epochs. Since the training datasets scale up to 100K documents, i.e., 100K document nodes in the graph, it is hard to do batch training on a single GPU given the large memory requirements. We solve this issue by mini-batching the datasets and feeding to the model a subgraph consisting of 1000 document nodes and all word nodes at a training step, which results in efficient training (The training time increases almost linearly with the number of documents) and makes it possible to apply our model to even bigger datasets.\nThe topic coherence results on the three datasets are shown in Table 1, where each value is the average of 5 topic number settings: 20, 30, 50, 75, 100. From Table 1, we can observe that our proposed GTM is the best-performing model under all dataset/metric settings. W-LDA, ATM, LDA, and GraphBTM alternately achieve the second-best but they are always under-performed compared to our model. As described in section 2, GTM is an extension to W-LDA with the main difference that GTM models topics in a larger context and incorporates more global information with the graph encoder. Therefore the improvements of GTM over W-LDA indicate the effectiveness of such information for topic modeling. We only experimented GraphBTM on 20Newsgroups because only 20Newsgroups preserves the sequential information that is necessary for GraphBTM to build graphs. GraphBTM performs well on the C A metric, which is reasonable since C A is a coherence measure based on a small sliding window of size 5 and consequently prefers models concentrating on a smaller context like GraphBTM. However, GraphBTM fails to achieve a high C P or NPMI score, which uses a bigger window (70 and 10 respectively).\nTo explore how topic coherence results vary w.r.t. different topic numbers, we present in Figure 2 the topic coherence scores under different topic  numbers settings. It can be observed in Figure 2 that GTM enjoys the best overall performance, achieving the highest scores in most settings. LDA has a slightly higher NPMI score on 20Newsgroups dataset with 75 and 100 topics, nevertheless, GTM outperforms all baseline models with a relatively large margin on other settings of 20Newsgroups. NVDM is apparently the worst-performing model, while performances of models other than GTM and NVDM are not so consistent. Notably, W-LDA, GraphBTM, and LDA obtain the second-best overall C P, C A, and NPMI scores respectively. Another observation from Figure 2 is that GTM performs better on smaller topics, probably due to the fact that topics become more discriminative against each other when the topic number is small.\nTo gain an intuitive impression on the discovered topics, we present in Table 2 4 topics corresponding to 4 out of 20 ground-truth categories of 20Newsgroups. It can be observed that the topics discovered by GTM are more coherent and interpretable, containing few off-topic words. As a comparison, GraphBTM's rec.autos topic mixes up automobiles and criminals, W-LDA's misc.forsale topic is difficult to identify with too many offtopic words, while LDA can not distinguish between rec.autos and misc.forsale well thus recog-nizes them as the same topic. It can be observed that GTM learns more discriminative topics by examining topic words from overlapping topics, e.g. rec.autos and misc.forsale.", "n_publication_ref": 18, "n_figure_ref": 3}, {"heading": "Conclusion", "text": "We have introduced Graph Topic Model, a neural topic model that incorporates corpus-level neighboring context using graph convolutions to enrich document representations and facilitate the topic inference. Both quantitative and qualitative results are presented in the experiments to demonstrate the effectiveness of the proposed approach. In the future, we would like to extend GTM to corpora with explicit doc-doc interactions, e.g., scientific documents with citations or social media posts with user relationships. Replacing GCN in GTM with more advanced graph neural networks is another promising research direction.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous reviewers for insightful comments and helpful suggestions. This work was funded in part by the National Key Research and Development Program of China (2016YFC1306704) and the National Natural Science Foundation of China (61772132).", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Evaluating topic coherence using distributional semantics", "journal": "", "year": "2013", "authors": "Nikolaos Aletras; Mark Stevenson"}, {"title": "Probabilistic topic models", "journal": "Communications of the ACM", "year": "2012", "authors": "David M Blei"}, {"title": "Latent dirichlet allocation", "journal": "J. Mach. Learn. Res", "year": "2003", "authors": "David M Blei; Andrew Y Ng; Michael I Jordan"}, {"title": "Normalized (pointwise) mautual information in collocation extraction", "journal": "", "year": "2009", "authors": "G Bouma"}, {"title": "Generative adversarial nets", "journal": "Curran Associates, Inc", "year": "2014", "authors": "Ian Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio"}, {"title": "A kernel two-sample test", "journal": "J. Mach. Learn. Res", "year": "2012", "authors": "Arthur Gretton; Karsten M Borgwardt; Malte J Rasch; Bernhard Sch\u00f6lkopf; Alexander Smola"}, {"title": "Neural networks for machine learning lecture 6a overview of mini-batch gradient descent", "journal": "", "year": "2012", "authors": "Geoffrey Hinton; Nitish Srivastava; Kevin Swersky"}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "journal": "", "year": "2015", "authors": "Sergey Ioffe; Christian Szegedy"}, {"title": "Autoencoding variational bayes", "journal": "", "year": "2013", "authors": "P Diederik; Max Kingma;  Welling"}, {"title": "Semisupervised classification with graph convolutional networks", "journal": "", "year": "2016", "authors": "N Thomas; Max Kipf;  Welling"}, {"title": "Information diffusion kernels", "journal": "MIT Press", "year": "2003", "authors": "Guy Lebanon; John D Lafferty"}, {"title": "Gated graph sequence neural networks", "journal": "", "year": "2015", "authors": "Yujia Li; Daniel Tarlow; Marc Brockschmidt; Richard Zemel"}, {"title": "Joint sentiment/topic model for sentiment analysis", "journal": "ACM", "year": "2009", "authors": "Chenghua Lin; Yulan He"}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "journal": "Citeseer", "year": "2013", "authors": "L Andrew;  Maas; Y Awni; Andrew Y Hannun;  Ng"}, {"title": "Neural variational inference for text processing", "journal": "", "year": "2016", "authors": "Yishu Miao; Lei Yu; Phil Blunsom"}, {"title": "Topic modeling with Wasserstein autoencoders", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Feng Nan; Ran Ding; Ramesh Nallapati; Bing Xiang"}, {"title": "Exploring the space of topic coherence measures", "journal": "ACM", "year": "2015", "authors": "Michael R\u00f6der; Andreas Both; Alexander Hinneburg"}, {"title": "A graph-to-sequence model for AMRto-text generation", "journal": "Long Papers", "year": "2018", "authors": "Linfeng Song; Yue Zhang; Zhiguo Wang; Daniel Gildea"}, {"title": "Autoencoding variational inference for topic models", "journal": "", "year": "2017", "authors": "Akash Srivastava; Charles Sutton"}, {"title": "", "journal": "", "year": "2017", "authors": "Ilya Tolstikhin; Olivier Bousquet; Sylvain Gelly; Bernhard Schoelkopf"}, {"title": "Neural topic modeling with bidirectional adversarial training", "journal": "", "year": "2020", "authors": "Rui Wang; Xuemeng Hu; Deyu Zhou; Yulan He; Yuxuan Xiong; Chenchen Ye; Haiyang Xu"}, {"title": "ATM: Adversarial-neural topic model. Information Processing & Management", "journal": "", "year": "2019", "authors": "Rui Wang; Deyu Zhou; Yulan He"}, {"title": "Open event extraction from online text using a generative adversarial network", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Rui Wang; Deyu Zhou; Yulan He"}, {"title": "Graph convolutional networks for text classification", "journal": "", "year": "2019", "authors": "Liang Yao; Chengsheng Mao; Yuan Luo"}, {"title": "Graph-based neural multi-document summarization", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Michihiro Yasunaga; Rui Zhang; Kshitijh Meelu; Ayush Pareek; Krishnan Srinivasan; Dragomir Radev"}, {"title": "Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Xin Zhao; Jing Jiang; Hongfei Yan; Xiaoming Li"}, {"title": "A simple Bayesian modelling approach to event extraction from twitter", "journal": "Short Papers", "year": "2014", "authors": "Deyu Zhou; Liangyu Chen; Yulan He"}, {"title": "Graph neural networks: A review of methods and applications", "journal": "", "year": "2018", "authors": "Jie Zhou; Ganqu Cui; Zhengyan Zhang; Cheng Yang; Zhiyuan Liu; Lifeng Wang; Changcheng Li; Maosong Sun"}, {"title": "GraphBTM: Graph enhanced autoencoded variational inference for biterm topic model", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Qile Zhu; Zheng Feng; Xiaolin Li"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Topic coherence scores (C P, C A, NPMI) w.r.t. topic numbers on 20Newsgroups (20NG), NYTimes (NYT), and Grolier.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Average topic coherence of 5 topic number settings", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Model Topics GTM cancer medicine patient treatment medical disease md health hospital investigation satellite mission space launch lunar spacecraft shuttle orbit nasa flight car honda bmw engine ford saturn dealer turbo rear model ticket send mail price credit sale offer receive list customer GraphBTM cancer hus md medical health disease patient mission laboratory culture probe mission spacecraft lunar shuttle orbit nasa solar satellite space car bike cop road hit gas insurance fbi guy lot car buy mouse scsi engine card audio pc windows faster village turkish armenia azerbaijan troops militia greek lebanon armenian greece W-LDA msg food patient disease study science one treatment doctor scientific space launch nasa satellite ground mission shuttle use rocket orbit car dog road ride speed light drive bike go front condition sale offer shipping sell excellent car speaker cd include LDA use drug cause effect medical study disease patient doctor treatment space launch earth nasa mission system orbit satellite design moon car buy price sale new engine offer model dealer car buy sell price sale new engine offer model dealer", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Discovered topics that are most similar to 4 ground-truth categories (sci.med, sci.space, rec.autos, misc.forsale) on 20Newsgroups with topic number 50. Italics are manually labeled off-topic words.", "figure_data": ""}], "doi": "10.1145/2133806.2133826"}