{"authors": "Sven Buechel; Susanna R\u00fccker; Udo Hahn", "pub_date": "", "title": "Learning and Evaluating Emotion Lexicons for 91 Languages", "abstract": "Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. Code and data are available at github.com/JULIELab/MEmoLon archived under", "sections": [{"heading": "Introduction", "text": "An emotion lexicon is a lexical repository which encodes the affective meaning of individual words (lexical entries). Most simply, affective meaning can be encoded in terms of polarity, i.e., the distinction whether an item is considered as positive, negative, or neutral. This is the case for many well-known resources such as WORDNET-AFFECT (Strapparava and Valitutti, 2004), SENTIWORD-NET (Baccianella et al., 2010), or VADER (Hutto and Gilbert, 2014). Yet, an increasing number of researchers focus on more expressive encodings for affective states inspired by distinct lines of work in psychology Buechel and Hahn, 2017;Sedoc et al., 2017;Abdul-Mageed and Ungar, 2017;Bostan and Klinger, 2018;Mohammad, 2018;Troiano et al., 2019).\nPsychologists, on the one hand, value such lexicons as a controlled set of stimuli for designing experiments, e.g., to investigate patterns of lexical access or the structure of memory Monnier and Syssau, 2008). NLP researchers, on the other hand, use them to augment the emotional loading of word embeddings (Yu et al., 2017;Khosla et al., 2018), as additional input to sentence-level emotion models so that the performance of even the most sophisticated neural network gets boosted (Mohammad and Bravo-Marquez, 2017;De Bruyne et al., 2019), or rely on them in a keyword-spotting approach when no training data is available, e.g., for studies dealing with historical language stages .\nAs with any kind of manually curated resource, the availability of emotion lexicons is heavily restricted to only a few languages whose exact number varies depending on the variables under scrutiny. For example, we are aware of lexicons for 15 languages that encode the emotional variables of Valence, Arousal, and Dominance (see Section 2). This number leaves the majority of the world's (less-resourced) languages without such a dataset. In case such a lexicon exists for a particular language, it is often severely limited in size, sometimes only comprising some hundreds of entries (Davidson and Innes-Ker, 2014). Yet, even the largest lexicons typically cover only some ten thousands of words, still leaving out major portions of the emotion-carrying vocabulary. This is especially true for languages with complex morphology or productive compounding, such as Finnish, Turkish, Czech, or German. Finally, the diversity of emotion representation schemes adds another layer of complexity. While psychologists and NLP researchers alike find that different sets of emotional variables are complementary to each other (Stevenson et al., 2007;Pinheiro et al., 2017;Barnes et al., 2019;De Bruyne et al., 2019), manually creating emotion lexicons for every language and every emotion representation scheme is virtually impossible.\nWe here propose an approach based on crosslingual distant supervision to generate almost arbitrarily large emotion lexicons for any target language and emotional variable, provided the following requirements are met: a source language emotion lexicon covering the desired variables, a bilingual word translation model, and a target language embedding model. By fulfilling these preconditions, we can automatically generate emotion lexicons for 91 languages covering ratings for eight emotional variables and hundreds of thousands of lexical entries each. Our experiments reveal that our method is on a par with state-of-the-art monolingual approaches and compares favorably with (sometimes even outperforms) human reliability.", "n_publication_ref": 19, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Representing Emotion. Whereas research in NLP has focused for a very long time almost exclusively on polarity, more recently, there has been a growing interest in more informative representation structures for affective states by including different groups of emotional variables (Bostan and Klinger, 2018). Borrowing from distinct schools of thought in psychology, these variables can typically be subdivided into dimensional vs. discrete approaches to emotion representation (Calvo and Mac Kim, 2013). The dimensional approach assumes that emotional states can be composed out of several foundational factors, most noticeably Valence (corresponding to polarity), Arousal (measuring calmness vs. excitement), and Dominance (the perceived degree of control in a social situation); VAD, for short (Bradley and Lang, 1994). Conversely, the discrete approach assumes that emotional states can be reduced to a small, evolutionary motivated set of basic emotions (Ekman, 1992). Although the exact division of the set has been subject of hot debates, recently constructed datasets (see Section 4) most often cover the categories of Joy, Anger, Sadness, Fear, and Disgust; BE5, for short. Plutchik's Wheel of Emotion takes a middle ground between those two positions by postulating emotional categories which are yet grouped into opposite pairs along different levels of intensity (Plutchik, 1980).\nAnother dividing line between representational approaches is whether target variables are encoded in terms of (strict) class-membership or scores for numerical strength. In the first case, emotion analysis translates into a (multi-class) classification problem, whereas the latter turns it into a regression problem . While our proposed methodology is agnostic towards the chosen emotion format, we will focus on the VAD and BE5 formats here, using numerical ratings (see the examples in Table 1) due to the widespread availability of such data. Accordingly, this paper treats word emotion prediction as a regression problem. 1.6 7.4 2.7 1.2 2.9 3.3 3.9 2.5 nuclear 4.3 7.3 4.1 1.4 2.2 1.9 3.2 1.6 ownership 5.9 4.4 7.5 2.1 1.4 1.2 1.4 1.3  Fear,and Disgust [BE5]. VAD uses 1-to-9 scales (\"5\" encodes the neutral value) and BE5 1-to-5 scales (\"1\" encodes the neutral value).\nBuilding Emotion Lexicons. Usually, the ground truth for affective word ratings (i.e., the assignment of emotional values to a lexical item) is acquired in a questionnaire study design where subjects (annotators) receive lists of words which they rate according to different emotion variables or categories. Aggregating individual ratings of multiple annotators then results in the final emotion lexicon (Bradley and Lang, 1999). Recently, this workflow has often been enhanced by crowdsourcing (Mohammad and Turney, 2013) and best-worst scaling (Kiritchenko and Mohammad, 2016).\nAs a viable alternative to manual acquisition, such lexicons can also be created by automatic means (Bestgen, 2008;K\u00f6per and Schulte im Walde, 2016;Shaikh et al., 2016), i.e., by learning to predict emotion labels for unseen words. Researchers have worked on this prediction problem for quite a long time. Early work tended to focus on word statistics, often in combination with linguistic rules (Hatzivassiloglou and McKeown, 1997;Turney and Littman, 2003). More recent approaches focus heavily on word embeddings, either using semi-supervised graph-based approaches Hamilton et al., 2016;Sedoc et al., 2017) or fully supervised methods (Rosenthal et al., 2015;Li et al., 2017;Rothe et al., 2016;Du and Zhang, 2016). Most important for this work, Buechel and Hahn (2018b) report on near-human performance using a combination of FASTTEXT vectors and a multi-task feed-forward network (see Section 4). While this line of work can add new words, it does not extend lexicons to other emotional variables or languages.\nA relatively new way of generating novel labels is emotion representation mapping (ERM), an annotation projection that translates ratings from one emotion format into another, e.g., mapping VAD labels into BE5, or vice versa (Hoffmann et al., 2012;Hahn, 2016, 2018a;Alarc\u00e3o and Fonseca, 2017;Landowska, 2018;Zhou et al., 2020;Park et al., 2019). While our work uses ERM to add additional emotion variables to the source lexicon, ERM alone can neither increase the coverage of a lexicon, nor adapt it to another language.\nTranslating Emotions. The approach we propose is strongly tied to the observation by Leveau et al. (2012) and Warriner et al. (2013) who found-comparing a large number of existing emotion lexicons of different languages-that translational equivalents of words show strong stability and adherence to their emotional value. Yet, their work is purely descriptive. They do not exploit their observation to create new ratings, and only consider manual rather than automatic translation.\nMaking indirect use of this observation, Mohammad and Turney (2013) offer machine-translated versions of their NRC Emotion Lexicon. Also, many approaches in cross-lingual sentiment analysis (on the sentence-level) rely on translating polarity lexicons (Abdalla and Hirst, 2017;Barnes et al., 2018). Perhaps most similar to our work, Chen and Skiena (2014) create (polarity-only) lexicons for 136 languages by building a multilingual word graph and propagating sentiment labels through that graph. Yet, their method is restricted to high frequency words-their lexicons cover between 12 and 4,653 entries, whereas our approach exceeds this limit by more than two orders of magnitude.\nOur methodology also resembles previous work which models word emotion for historical language stages (Cook and Stevenson, 2010;Hamilton et al., 2016;Hellrich et al., 2018;Li et al., 2019). Work in this direction typically comes up with a set of seed words with assumingly temporally stable affective meaning (our work assumes stability against translation) and then uses distributional methods to derive emotion ratings in the target language stage. However, gold data for the target language (stage) is usually inaccessible, often preventing evaluation against human judgment. In contrast, we here propose several alternative evaluation set-ups as an integral part of our methodology.", "n_publication_ref": 36, "n_figure_ref": 0}, {"heading": "A Novel Approach to Lexicon Creation", "text": "Our methodology integrates (1) cross-lingual generation and expansion of emotion lexicons and (2) their evaluation against gold and silver standard data. Consequently, a key aspect of our workflow design is how data is split into train, dev, and test sets at different points of the generation process. Figure 1 gives an overview of our framework including a toy example for illustration.\nLexicon Generation. We start with a lexicon (Source) of arbitrary size, emotion format 1 and source language which is partitioned into train, dev, and test splits denoted by Source-train, Source-dev, and Source-test, respectively. Next, we leverage a bilingual word translation model between source and desired target language to build the first target-side emotion lexicon denoted as TargetMT. Source words are translated according to the model, whereas target-side emotion labels are simply copied from the source to the target (see Section 2). Entries are assigned to train, dev, or test set according to their source-side assignment (cf. Figure 1). The choice of our translation service (see below) ensures that each source word receives exactly one translation.\nTargetMT is then used as the distant supervisor to train a model that predicts word emotions based on target-side word embeddings. TargetMT-train and TargetMT-dev are used to fit model parameters and optimize hyperparameters, respectively, whereas TargetMT-test is held out for later evaluation. Once finalized, the model is used to predict new labels for the words in TargetMT, resulting in a second target-side emotion lexicon denoted TargetPred. Our rationale for doing so is that a reasonably trained model should generalize well  (4.3, 7.3))   test   (Terrorismus, (1.6, 7.4) : Schematic view on the methodology for generating and evaluating an emotion lexicon for a given target language based on source language supervision. Included is a toy example starting with an English VA lexicon (sunshine, nuclear, terrorism and the associated numerical scores for Valence and Arousal) and resulting in an extended German lexicon which incorporates translated entries with altered VA scores and additional entries originating from the embedding model with newly learned scores.\nover the entire TargetMT lexicon because it has access to the target-side embedding vectors. Hence, it may mitigate some of the errors which were introduced in previous steps, either by machine translation or by assuming that sourceand target-side emotion are always identical. We validate this assumption in Section 6. We also predict ratings for all the words in the embedding model, leading to a large number of new entries.\nThe splits are defined as follows: let M T train , M T dev , and M T test denote the set of words in train, dev, and test split of TargetMT, respectively. Likewise, let P train , P dev , and P test denote the splits of TargetPred and let E denote the set of words in the embedding model. Then\nP train := M T train P dev := M T dev \\ M T train P test := (M T test \u222a E) \\ (M T dev \u222a M T train )\nThe above definitions help clarify the way we address polysemy. 2 Ambiguity on the target-side 2 In short, our work evades this problem by dealing with lexical entries exclusively on the type-rather than the senselevel. From a lexicological perspective, this may seem like a strong assumption. From a modeling perspective, however, it appears almost obvious as it aligns well with the major components of our methodology, i.e., lexicons, embeddings, and translation. The lexicons we work with follow the design of behavioral experiments: a stimulus (word type) is given to may result in multiple source entries translating to the same target-side word. 3 This circumstance leads to \"partial duplicates\" in TargetMT, i.e., groups of entries with the same word type but different emotion values (because they were derived from distinct Source entries). Such overlap could do harm to the integrity of our evaluation since knowledge may \"leak\" from training to validation phase, i.e., by testing the model on words it has already seen during training, although with distinct emotion labels. The proposed data partitioning eliminates such distortion effects. Since partial duplicates receive the same embedding vector, the prediction model assigns the same emotion value to both, thus merging them in TargetPred.\nEvaluation Methodology. The main advantage of the above generation method is that it allows us to create large-scale emotion lexicons for languages a subject and the response (rating) is recorded. The absence of sense-level annotation simplifies the mapping between lexicon and embedding entries. While sense embeddings form an active area of research (Camacho-Collados and Pilehvar, 2018;Chi and Chen, 2018), to the best of our knowledge, type-level embeddings yield state-of-the-art performance in downstream applications.\n3 Source-side polysemy, in contrast to its target-side counterpart, is less of a problem, because we receive only a single candidate during translation. This may result in cases where the translation misaligns with the copied emotion value in TargetMT. Yet, the prediction step partly mitigates such inconsistencies (see Section 6).\nfor which gold data is lacking. But if that is the case, how can we assess the quality of the generated lexicons? Our solution is to propose two different evaluation scenarios-a gold evaluation which is a strict comparison against human judgment, meaning that it is limited to languages where such data (denoted TargetGold) is available, and a silver evaluation which substitutes human judgments by automatically derived ones (silver standard) which is feasible for any language in our study. The rationale is that if both, gold and silver evaluation, strongly agree with each other, we can use one as proxy for the other when no target-side gold data exists (examined in Section 6).\nNote that our lexicon generation approach consists of two major steps, translation and prediction. However, these two steps are not equally important for each generated entry in TargetPred. Words, such as German Sonnenschein for which a translational equivalent already exists in the Source (\"sunshine\"; see Figure 1), mainly rely on translation, while the prediction step acts as an optional refinement procedure. In contrast, the prediction step is crucial for words, such as Erdbeben, whose translational equivalents (\"earthquake\") are missing in the Source. Yet, these words also depend on the translation step for producing training data. These considerations are important for deciding which words to evaluate on. We may choose to base our evaluation on the full TargetPred lexicon, including words from the training set-after all, the word emotion model does not have access to any target-side gold data. The problem with this approach is that it merges words that mainly rely on translation, because their equivalents are in the Source, and those which largely depend on prediction, because they are taken from the embedding model. In this case, generalizability of evaluation results becomes questionable. Thus, our evaluation methodology needs to fulfill the following two requirements: (1) evaluation must not be performed on translational equivalents of the Source entries to which the model already had access during training (e.g., Sonnenschein and nuklear in our example from Figure 1); but, on the other hand, (2) a reasonable number of instances must be available for evaluation (ideally, as many as possible to increase reliability). The intricate cross-lingual train-dev-test set assignment of our generation methodology is in place so that we meet these two requirements.  In particular, for our silver evaluation, we intersect TargetMT-test with TargetPred-test and compute the correlation of these two sets individually for each emotion variable. Pearson's r will be used as correlation measure throughout this paper. Establishing a test set at the very start of our workflow, Source-test, assures that there is a relatively large overlap between the two sets and, by extension, that our requirements for the evaluation are met.\nThe gold evaluation is a somewhat more challenging case, because we can, in general, not guarantee that the overlap of a TargetGold lexicon with TargetPred-test will be of any particular size. For this reason, the words of the embedding model are added to TargetPred-test (see above), maximizing the expected overlap with TargetGold. In practical terms, we intersect TargetGold with TargetPred-test and compute the variable-wise correlation between these sets, in parallel to the silver evaluation. A complementary strategy for maximizing overlap, by exploiting dependencies between published lexicons, is described below.", "n_publication_ref": 3, "n_figure_ref": 4}, {"heading": "Experimental Setup", "text": "Gold Lexicons and Data Splits. We use the English emotion lexicon from Warriner et al. (2013) as first part of our Source dataset. This popular resource comprises about 14k entries in VAD format collected via crowdsourcing. Since manually gathered BE5 ratings are available only for a subset of this lexicon (Stevenson et al., 2007), we add BE5 ratings from Buechel and Hahn (2018a) who used emotion representation mapping (see Section 2) to convert the existing VAD ratings, showing that this is about as reliable as human annotation.\nAs apparent from the previous section, a crucial aspect for applying our methodology is the design of the train-dev-test split of the Source because it directly impacts the amount of words we can test our lexicons on during gold evaluation. In line with these considerations, we choose the lexical items which are already present in ANEW (Bradley and Lang, 1999) as Source-test set. ANEW is the precursor to the version later distributed by Warriner et al. (2013); it is widely used and has been adapted to a wide range of languages. With this choice, it is likely that a resulting TargetPred-test set has a large overlap with the respective TargetGold lexicon. As for the TargetGold lexicons, we included every VA(D) and BE5 lexicon we could get hold of with more than 500 entries. This resulted in 26 datasets covering 12 quite diverse languages (see Table 2). Note that we also include English lexicons in the gold evaluation. In these cases, no translation will be carried out (Source is identical to TargetMT) so that only the expansion step is validated. Appendix A.1 gives further details on data preparation.\nTranslation. We used the GOOGLE CLOUD TRANSLATION API 4 to produce word-to-word translation tables. This is a commercial service, total translation costs amount to 160 EUR. API calls were performed in November 2019.\nEmbeddings. We use the fastText embedding models from Grave et al. (2018) trained for 157 languages on the respective WIKIPEDIA and the respective part of COMMONCRAWL. These resources not only greatly facilitate our work but also increase comparability across languages. The restriction to \"only\" 91 languages comes from intersecting the ones covered by the vectors with the languages covered by the translation service.\nModels. Since our proposed methodology is agnostic towards the chosen word emotion model, we will re-use models from the literature. In particular, we will rely on the multi-task learning feed-forward network (MTLFFN) worked out by Buechel and Hahn (2018b). This network constitutes the current state of the art for monolingual emotion lexicon creation (expanding an existing lexicon for a given language) for many of the datasets in Table 2.\nThe MTLFFN has two hidden layers of 256 and 128 units, respectively, and takes pre-trained embedding vectors as input. Its distinguishing feature is that hidden layer parameters are shared between the different emotion target variables, thus constituting a mild form of multi-task learning (MTL). We apply MTL to VAD and BE5 variables individually (but not between both groups), thus training two distinct emotion models per language, following the outcome of a development experiment. Details are given in Appendix A.2 together with the remainder of the model specifications.\nBeing aware of the infamous instability of neural approaches (Reimers and Gurevych, 2017), we also employ a ridge regression model, an L 2 regularized version of linear regression, as a more robust, yet also powerful baseline (Li et al., 2017).", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Results", "text": "The size of the resulting lexicons (a complete list is provided in Table 8 in the Appendix) ranges from roughly 100k to more than 2M entries mainly depending on the vocabulary of the respective embeddings. We want to point out that not every single entry should be considered meaningful because of noise in the embedding vocabulary caused by typos and tokenization errors. However, choosing the \"best\" size for an emotion lexicon necessarily translates into a quality-coverage trade-off for which there is no general solution. Instead, we release the full-size lexicons and leave it to prospective users to apply any sort of filtering they deem appropriate. Silver Evaluation. Figure 2 displays the results of our silver evaluation. Languages (x-axis) are sorted by their average performance over all variables (not shown in the plot; tabular data given in the Appendix). As can be seen, the evaluation results for English are markedly better than for any other language. This is not surprising since no (potentially error-prone) machine translation was performed. Apart from that, performance remains relatively stable across most of the languages and starts degrading more quickly only for the last third of them. In particular, for Valence-typically the easiest variable to predict-we achieve a strong performance of r > .7 for 56 languages. On the other hand, for Arousal-typically, the most difficult one to predict-we achieve a solid performance of r > .5 for 55 languages. Dominance and the discrete emotion variables show performance trajectories swinging between these two extremes. We assume that the main factors for explaining performance differences between languages are the quality of the translation and embedding models which, in turn, both depend on the amount of available text data (parallel or monolingual, respectively).\nComparing MTLFFN and ridge baseline, we find that the neural network reliably outperforms the linear model. On average over all languages and variables, the MTL models achieve 6.7%-points higher Pearson correlation. Conversely, ridge regression outperforms MTLFFN in only 15 of the total 728 cases (91 languages \u00d7 8 variables).\nGold Evaluation. Results for VAD variables on gold data are given in Table 3. As can be seen, our lexicons show a good correlation with human judgment and do so robustly, even for less-resourced languages, such as Indonesian (id), Turkish (tr), or Croatian (hr), and across affective variables. Perhaps the strongest negative outliers are the Arousal results for the two Chinese datasets (zh), which are likely to result from the low reliability of the gold ratings (see below).   Buechel and Hahn (2018b). Shared words between TargetGold and TargetPred-test; (%): percentage relative to TargetGold; Mn (all): mean over all datasets; Mn (vs. monolingual): mean over datasets with comparative results.\nWe compare these results against those from Buechel and Hahn (2018b) which were acquired on the respective TargetGold dataset in a monolingual fashion using 10-fold cross-validation (10-  CV). We admit that those results are not fully comparable to those presented here because we use fixed splits rather than 10-CV. Nevertheless, we find that the results of our cross-lingual set-up are more than competitive, outperforming the monolingual results from Buechel and Hahn (2018b) in 17 out of 30 cases (mainly for Valence and Dominance, less often for Arousal). This is surprising since we use an otherwise identical model and training procedure. We conjecture that the large size of the English Source lexicon, compared to most TargetGold lexicons, more than compensates for error-prone machine translation. Table 4 shows the results for BE5 datasets which are in line with the VAD results. Regarding the ordering of the emotional variables, again, we find Valence to be the easiest one to predict, Arousal the hardest, whereas basic emotions and Dominance take a middle ground.\nComparison against Human Reliability. We base this analysis on inter-study reliability (ISR), a rather strong criterion for human performance. ISR is computed, per variable, as the correlation between the ratings from two distinct annotation studies (Warriner et al., 2013). Hence, this analysis is restricted to languages where more than one gold lexicon exists per emotion format. We intersect the entries from both gold standards as well as the respective TargetPred-test set and compute the correlation between all three pairs of lexicons. If our lexicon agrees more with one of the gold standards than the two gold standards agree with each other, we consider this as an indicator for superhuman reliability (Buechel and Hahn, 2018b).\nAs shown in Table 5, our lexicons are often competitive with human reliability for Valence (especially for English and Chinese), but outperform  human reliability in 4 out of 6 cases for Arousal, and in the single test case for Dominance. There are no cases of overlapping gold standards for BE5.\nG o l d 1 G o l d 2 S h a r e d E m o G 1 v s G 2 G 1 v s P r G 2 v", "n_publication_ref": 5, "n_figure_ref": 1}, {"heading": "Methodological Assumptions Revisited", "text": "This section investigates patterns in prediction quality across languages, validating design decisions of our methodology.\nTranslation vs. Prediction. Is it beneficial to predict new ratings for the words in TargetMT rather than using them as final lexicon entries straight away? For each TargetGold lexicon (cf.\nTable 2), we intersect its word material with that in TargetMT and TargetPred. Then, we compute the correlation between TargetPred and TargetMT with the gold standard. This analysis was done on the respective train sets because using TargetMT rather than TargetPred is only an option for entries known at training time.\nTable 6 depicts the results of this comparison averaged over all gold lexicons. As hypothesized, the TargetPred lexicons agree, on average, more with human judgment than the TargetMT lexicons, suggesting that the word emotion model acts as a value-adding post-processor, partly mitigating rating inconsistencies introduced by mere translation of the lexicons. The observation holds for each individual emotion variable with particularly large benefits for Arousal, where the postprocessed TargetPred lexicons are on average  14%-points better compared to the translation-only TargetMT lexicons. This seems to indicate that lexical Arousal is less consistent between translational equivalents compared to other emotional meaning components like Valence and Sadness, which appear to be more robust against translation.\nV a l A r o D o m J o y A n g S a d F e a D i\nGold vs. Silver Evaluation. How meaningful is silver evaluation without gold data? We compute the Pearson correlation between gold and silver evaluation results across languages per emotion variable. For languages where we consider multiple datasets during gold evaluation, we first average the gold evaluation results for each emotion variable. As can be seen from Table 7, the correlation values range between r = .91 for Joy and r = .27 for Disgust. This relatively large dispersion is not surprising when we take into account that we correlate very small data series (for Valence and Arousal there are just 12 languages for which both gold and silver evaluation results are available; for BE5 there are only 5 such languages). However, the mean over all correlation values in Table 7 is .64, indicating that there is a relatively strong correlation between both types of evaluation. This suggests that the silver evaluation may be used as a rather reliable proxy of lexicon quality even in the absence of language-specific gold data. Table 7: Agreement between gold and silver evaluation across languages in Pearson's r relative to the number of applicable languages (\"#Lg\").", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "Emotion lexicons are at the core of sentiment analysis, a rapidly flourishing field of NLP. Yet, despite large community efforts, the coverage of existing lexicons is still limited in terms of languages, size, and types of emotion variables. While there are techniques to tackle these three forms of sparsity in isolation, we introduced a methodology which allows us to cope with them simultaneously by jointly combining emotion representation mapping, machine translation, and embedding-based lexicon expansion.\nOur study is \"large-scale\" in many respects. We created representationally complex lexiconscomprising 8 distinct emotion variables-for 91 languages with up to 2 million entries each. The evaluation of the generated lexicons featured 26 manually annotated datasets spanning 12 diverse languages. The predicted ratings showed consistently high correlation with human judgment, compared favorably with state-of-the-art monolingual approaches to lexicon expansion and even surpassed human inter-study reliability in some cases.\nThe sheer number of test sets we used allowed us to validate fundamental methodological assumptions underlying our approach. Firstly, the evaluation procedure, which is integrated into the generation methodology, allows us to reliably estimate the quality of resulting lexicons, even without target language gold standard. Secondly, our data suggests that embedding-based word emotion models can be used as a repair mechanism, mitigating poor target-language emotion estimates acquired by simple word-to-word translation.\nFuture work will have to deepen the way we deal with word sense ambiguity by way of exchanging the simplifying type-level approach our current work is based on with a semantically more informed sense-level approach. A promising direction would be to combine a multilingual sense inventory such as BABELNET (Navigli and Ponzetto, 2012) with sense embeddings (Camacho-Collados and Pilehvar, 2018).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A Appendices", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 Data Preparation", "text": "The exact design of the Source train-dev-test split is as follows: All entries (words plus ratings) from all splits are taken from Warriner et al. (2013). The data was then partitioned based on the overlap with the two precursory versions by Bradley and Lang (1999) (the original ANEW) and Bradley and Lang (2010) (an early extended version of ANEW roughly twice as large). Source-test was built by intersecting the lexicon from Warriner et al. (2013) with the original ANEW. A similar process was applied for Source-dev: we intersected the words from Warriner et al. (2013) and Bradley and Lang (2010) and removed the ones present in Source-test. Lastly, Source-train is made up by all words from Warriner et al. (2013) which are neither in Source-test nor in Source-dev. The reason why the ratings in Source are taken exclusively from Warriner et al. (2013) is that these are distributed under a more permissive license compared to their precursors.\nWe removed multi-token entries (e.g., boa constrictor) and entries with upper case characters (e.g., Budweiser) from all data splits of Source, thus restricting the lexicon to single-token, nonproper noun entries to make it more suitable for word embedding-based research. All splits combined have 13,791 entries (train: 11,463, dev: 1,296, test: 1,032), thus removing less than 1% from the original lexicon. 5 Regarding the remaining gold standards, the only cases which needed additional preparation or cleansing steps were zh1  and zh2 (Yao et al., 2017). zh1 was created and is distributed using traditional Chinese characters, whereas the embedding model by Grave et al. (2018) employs simplified ones. Therefore, we converted zh1 into simplified characters using GOOGLE TRANSLATE 6 prior to evaluation.\nWhile manually examining the zh2 lexicon, we noticed several cases where the ratings seemed rather counter-intuitive (e.g., seemingly positive words which received very negative ratings). We contacted the authors who confirmed the problem and sent us a corrected version. We did not find any such problems in the second version. We consulted with a Chinese native speaker for both of these procedures regarding the zh1 and zh2 lexicons.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "A.2 Model Training and Implementation", "text": "Training of the MTLFFN model closely followed the procedure specified by Buechel and Hahn (2018b): For each language, the model was trained for roughly 15k iterations (exactly 168 epochs) with a batch size of 128 using the Adam optimizer (Kingma and Ba, 2015) with learning rate 10 \u22123 , and .5 dropout on the hidden layers and .2 on the input layer. As nonlinear activation function we used leaky ReLU with \"leakage\" of 0.01.\nEmbedding vectors are the only model input. They have 300 dimensions for every language, independent of their respective training data size (Grave et al., 2018). Since the automatic translation of Source is not guaranteed to result in single-word translations, we use the following workaround to derive embedding vectors for multi-token translations: If the translation as a whole cannot be found in the embedding model, the multi-token term gets split up into its constituent parts, using spaces, apostrophes or hyphens as separators. Each substring is looked up in the embedding model, the averaged vector is taken as input. If no substring is recognized, we use the zero vector instead. We also use the zero vector for single-token entries in TargetMT that are missing in the embeddings.\nSince Buechel and Hahn (2018b) considered only VAD but not BE5 datasets, we conducted a development experiment on the TargetMT-dev sets for all 91 languages where we assessed whether MTL is advantageous for BE5 variables as well, or for a combination of VAD and BE5 variables. We found that MTL improved performance when applied separately among all VAD and BE5 variables. Yet, when jointly learning all eight emotion variables, the results were somewhat inconclusive. Performance increased for BE5, but decreased for VAD. Hence, for lexicon creation, we took a cautious approach and trained two separate models per language, one for VAD, the other for BE5. An analysis of MTL across VAD and BE5 is left for future work.\nThe MTLFFN model is implemented in PY-TORCH, adapting part of the TENSORFLOW code from Buechel and Hahn (2018b). The ridge regression baseline model is implemented with SCIKIT-LEARN (Pedregosa et al., 2011) ", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their helpful suggestions and comments, and Tinghui Duan, JULIE LAB, for assisting us with the Chinese gold data. This work was partially funded by the German Federal Ministry for Economic Affairs and Energy (funding line \"Big Data in der makro\u00f6konomischen Analyse\" [Big data in macroeconomic analysis]; Fachlos 2; GZ 23305/003#002).", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Crosslingual sentiment analysis without (good) translation", "journal": "", "year": "2017-11-27", "authors": "Mohamed Abdalla; Graeme Hirst"}, {"title": "EmoNet: Fine-grained emotion detection with gated recurrent neural networks", "journal": "", "year": "2017-07-30", "authors": "Muhammad Abdul; -Mageed ; Lyle H Ungar"}, {"title": "Identifying emotions in images from valence and arousal ratings. Multimedia Tools and Applications", "journal": "", "year": "2017", "authors": "M Soraia; Manuel J Alarc\u00e3o;  Fonseca"}, {"title": "SENTIWORDNET 3.0: An enhanced lexical resource for sentiment analysis and opinion mining", "journal": "", "year": "2010-05-17", "authors": "Stefano Baccianella; Andrea Esuli; Fabrizio Sebastiani"}, {"title": "Bilingual sentiment embeddings: Joint projection of sentiment across languages", "journal": "", "year": "2018-07-15", "authors": "Jeremy Barnes; Roman Klinger; Sabine Schulte Im Walde"}, {"title": "Lexicon information in neural sentiment analysis: a multi-task learning approach", "journal": "", "year": "2019-09-30", "authors": "Jeremy Barnes; Samia Touileb; Lilja \u00d8vrelid; Erik Velldal"}, {"title": "Building affective lexicons from specific corpora for automatic sentiment analysis", "journal": "", "year": "2008-05-28", "authors": "Yves Bestgen"}, {"title": "An analysis of annotated corpora for emotion classification in text", "journal": "", "year": "2018-08-20", "authors": "Laura-Ana-Maria Bostan; Roman Klinger"}, {"title": "Measuring emotion: The Self-Assessment Manikin and the semantic differential", "journal": "Journal of Behavior Therapy and Experimental Psychiatry", "year": "1994", "authors": "Margaret M Bradley; Peter J Lang"}, {"title": "Affective norms for English words (ANEW): Stimuli, instruction manual and affective ratings", "journal": "", "year": "1999", "authors": "Margaret M Bradley; Peter J Lang"}, {"title": "Affective norms for English words (ANEW): Stimuli, instruction manual and affective ratings", "journal": "", "year": "2010", "authors": "Margaret M Bradley; Peter J Lang"}, {"title": "Discrete Emotion Norms for Nouns: Berlin Affective Word List (DENN-BAWL)", "journal": "Behavior Research Methods", "year": "2011", "authors": "Benny B Briesemeister; Lars Kuchinke; Arthur M Jacobs"}, {"title": "Emotion analysis as a regression problem: Dimensional models and their implications on emotion representation and metrical evaluation", "journal": "", "year": "2016-08-29", "authors": "Sven Buechel; Udo Hahn"}, {"title": "EMOBANK: Studying the impact of annotation perspective and representation format on dimensional emotion analysis", "journal": "", "year": "2017-04-03", "authors": "Sven Buechel; Udo Hahn"}, {"title": "Emotion representation mapping for automatic lexicon construction (mostly) performs on human level", "journal": "", "year": "2018-08-20", "authors": "Sven Buechel; Udo Hahn"}, {"title": "Word emotion induction for multiple languages as a deep multi-task learning problem", "journal": "", "year": "2018-06-01", "authors": "Sven Buechel; Udo Hahn"}, {"title": "Feelings from the past: Adapting affective lexicons for historical emotion analysis", "journal": "", "year": "2016", "authors": "Sven Buechel; Johannes Hellrich; Udo Hahn"}, {"title": "Emotions in text: Dimensional and categorical models", "journal": "Computational Intelligence", "year": "2013", "authors": "A Rafael; Sunghwan Mac Calvo;  Kim"}, {"title": "From word to sense embeddings: A survey on vector representations of meaning", "journal": "Journal of Artificial Intelligence Research", "year": "2018", "authors": ""}, {"title": "Building sentiment lexicons for all major languages", "journal": "", "year": "2014-06-23", "authors": "Yanqing Chen; Steven Skiena"}, {"title": "CLUSE : Cross-Lingual Unsupervised Sense Embeddings", "journal": "", "year": "2018-11-04", "authors": "Chung Ta-; Yun-Nung Chi;  Chen"}, {"title": "Automatically identifying changes in the semantic orientation of words", "journal": "", "year": "2010-05-17", "authors": "Paul Cook; Suzanne Stevenson"}, {"title": "Affective and concreteness norms for 3,022 Croatian words", "journal": "Quarterly Journal of Experimental Psychology", "year": "2019", "authors": "Marc Bojana\u0107oso; Pilar Guasch; Jos\u00e9 Antonio Ferr\u00e9;  Hinojosa"}, {"title": "Valence and arousal norms for Swedish affective words", "journal": "Lund Psychological Reports", "year": "2014", "authors": ""}, {"title": "Joint emotion label space modelling for affect lexica", "journal": "", "year": "2019", "authors": "Pepa Luna De Bruyne; Isabelle Atanasova;  Augenstein"}, {"title": "Aicyber's system for IALP 2016 Shared Task: Character-enhanced word vectors and boosted neural networks", "journal": "", "year": "2016-11-21", "authors": "Steven Du; Xi Zhang"}, {"title": "An argument for basic emotions", "journal": "Cognition and Emotion", "year": "1992", "authors": "Paul Ekman"}, {"title": "Moved by words: Affective ratings for a set of", "journal": "", "year": "2017", "authors": "Pilar Ferr\u00e9; Marc Guasch; Natalia Mart\u00ednez-Garc\u00eda; Isabel Fraga; Jos\u00e9 Antonio Hinojosa"}, {"title": "Spanish words in five discrete emotion categories", "journal": "Behavior Research Methods", "year": "", "authors": ""}, {"title": "Learning word vectors for 157 languages", "journal": "", "year": "2018-05-07", "authors": "Edouard Grave; Piotr Bojanowski; Prakhar Gupta; Armand Joulin; Tomas Mikolov"}, {"title": "Inducing domain-specific sentiment lexicons from unlabeled corpora", "journal": "", "year": "2016-11-01", "authors": "William L Hamilton; Kevin Clark; Jure Leskovec; Dan Jurafsky"}, {"title": "Predicting the semantic orientation of adjectives", "journal": "Association for Computational Linguistics", "year": "1997-07-07", "authors": "Vasileios Hatzivassiloglou; Kathleen R Mckeown"}, {"title": "JESEME: Interleaving semantics and emotions in a Web service for the exploration of language change phenomena", "journal": "", "year": "2018-08-20", "authors": "Johannes Hellrich; Sven Buechel; Udo Hahn"}, {"title": "Affective norms of 875 Spanish words for five discrete emotional categories and two emotional dimensions", "journal": "Behavior Research Methods", "year": "2016", "authors": "Jos\u00e9 Antonio Hinojosa; Natalia Mart\u00ednez-Garc\u00eda; Cristina Villalba-Garc\u00eda; Uxia Fern\u00e1ndez-Folgueiras; Alberto S\u00e1nchez-Carmona; Miguel Angel Pozo; Pedro R Montoro"}, {"title": "Mapping discrete emotions into the dimensional space: An empirical approach", "journal": "", "year": "2012-10-14", "authors": "Holger Hoffmann; Andreas Scheck; Timo Schuster; Steffen Walter; Kerstin Limbrecht-Ecklundt; Harald C Traue; Henrik Kessler"}, {"title": "Affective processing within 1/10th of a second: High arousal is necessary for early facilitative processing of negative but not positive words. Cognitive, Affective", "journal": "& Behavioral Neuroscience", "year": "2009", "authors": "Markus J Hofmann; Lars Kuchinke; Sascha Tamm; Melissa L ; -H V\u00f5; Arthur M Jacobs"}, {"title": "VADER: A parsimonious rule-based model for sentiment analysis of social media text", "journal": "", "year": "2014-06-01", "authors": "J Clayton; Eric Hutto;  Gilbert"}, {"title": "Affective Norms for 4900 Polish Words Reload (ANPW R): Assessments for valence, arousal, dominance, origin, significance, concreteness, imageability and, age of acquisition", "journal": "Frontiers in Psychology", "year": "2016", "authors": "K Kamil;  Imbir"}, {"title": "Leipzig Affective Norms for German: A reliability study", "journal": "Behavior Research Methods", "year": "2010", "authors": "Philipp Kanske; Sonja A Kotz"}, {"title": "Turkish emotional word norms for arousal, valence, and discrete emotion categories", "journal": "Psychological Reports", "year": "2018-12-04", "authors": "Aycan Kapucu; Asl\u0131 K\u0131l\u0131\u00e7; Y\u0131ld\u0131z\u00f6zk\u0131l\u0131\u00e7 ; Bengisu Sar\u0131baz"}, {"title": "AFF2VEC : Affect-enriched distributional word representations", "journal": "", "year": "2018-08-20", "authors": "Sopan Khosla; Niyati Chhaya; Kushal Chawla"}, {"title": "ADAM: A method for stochastic optimization", "journal": "", "year": "2015-05-07", "authors": "Diederik Kingma; Jimmy Ba"}, {"title": "Capturing reliable fine-grained sentiment associations by crowdsourcing and best-worst scaling", "journal": "", "year": "2016-06-12", "authors": "Svetlana Kiritchenko; M Saif;  Mohammad"}, {"title": "Automatically generated affective norms of abstractness, arousal, imageability and valence for", "journal": "", "year": "2016", "authors": "Maximilian K\u00f6per; Sabine Schulte Im Walde"}, {"title": "LREC 2016 -Proceedings of the 10th International Conference on Language Resources and Evaluation", "journal": "", "year": "2016", "authors": "German Lemmas"}, {"title": "Towards new mappings between emotion representation models", "journal": "Applied Sciences", "year": "2018", "authors": "Agnieszka Landowska"}, {"title": "Validating an interlingual metanorm for emotional analysis of texts", "journal": "Behavior Research Methods", "year": "2012", "authors": "Nicolas Leveau; Sandra Jhean-Larose; Guy Denhi\u00e8re; Ba-Linh Nguyen"}, {"title": "Inferring affective meanings of words from word embedding", "journal": "IEEE Transactions on Affective Computing", "year": "2017", "authors": "Minglei Li; Qin Lu; Yunfei Long; Lin Gui"}, {"title": "The MACROSCOPE: A tool for examining the historical structure of language", "journal": "Behavior Research Methods", "year": "2019", "authors": "Ying Li; Tomas Engelthaler; Cynthia S Q Siew; Thomas T Hills"}, {"title": "Obtaining reliable human ratings of valence, arousal, and dominance for", "journal": "", "year": "2018", "authors": "Saif Mohammad"}, {"title": "English words", "journal": "", "year": "2018", "authors": ""}, {"title": "WASSA-2017 Shared Task on Emotion Intensity", "journal": "", "year": "2017-09-08", "authors": "Saif Mohammad; Felipe Bravo-Marquez"}, {"title": "SEMEVAL-2018 Task 1: Affect in Tweets", "journal": "", "year": "2018-06-05", "authors": "Saif Mohammad; Felipe Bravo-Marquez; Mohammad Salameh; Svetlana Kiritchenko"}, {"title": "Crowdsourcing a word-emotion association lexicon", "journal": "Computational Intelligence", "year": "2013", "authors": "M Saif; Peter D Mohammad;  Turney"}, {"title": "Semantic contribution to verbal short-term memory: Are pleasant words easier to remember than neutral words in serial recall and serial recognition? Memory & Cognition", "journal": "", "year": "2008", "authors": "Catherine Monnier; Arielle Syssau"}, {"title": "The adaptation of the Affective Norms for English Words (ANEW) for Italian", "journal": "Behavior Research Methods", "year": "2014", "authors": "Maria Montefinese; Ettore Ambrosini; Beth Fairfield; Nicola Mammarella"}, {"title": "Norms of valence, arousal, dominance, and age of acquisition for 4,300 Dutch words", "journal": "Behavior Research Methods", "year": "2013", "authors": "Agnes Moors; Jan De Houwer; Dirk Hermans; Sabine Wanmaker; Kevin Van Schie; Anne-Laura Van Harmelen; Maarten De Schryver; Jeffrey De Winne; Marc Brysbaert"}, {"title": "BABELNET: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network", "journal": "Artificial Intelligence", "year": "2012", "authors": "Roberto Navigli; Simone Paolo Ponzetto"}, {"title": "Affective lexicon creation for the Greek language", "journal": "", "year": "2016-05-23", "authors": "Elisavet Palogiannidi; Polychronis Koutsakis; Elias Iosif; Alexandros Potamianos"}, {"title": "Toward dimensional emotion detection from categorical emotion annotations", "journal": "", "year": "2019", "authors": "Sungjoon Park; Jiseon Kim; Jaeyeol Jeon; Heeyoung Park; Alice Oh"}, {"title": "SCIKIT-LEARN: Machine learning in PYTHON", "journal": "Journal of Machine Learning Research", "year": "2011", "authors": "Fabian Pedregosa; Ga\u00ebl Varoquaux; Alexandre Gramfort; Vincent Michel; Bertrand Thirion; Olivier Grisel; Mathieu Blondel; Peter Prettenhofer; Ron Weiss; Vincent Dubourg"}, {"title": "Minho Affective Sentences (MAS): Probing the roles of sex, mood, and empathy in affective ratings of verbal stimuli", "journal": "Behavior Research Methods", "year": "2017", "authors": "Ana P Pinheiro; Marcelo Dias; Jo\u00e3o Pedrosa; Ana P Soares"}, {"title": "A general psychoevolutionary theory of emotion", "journal": "Academic Press", "year": "1980", "authors": "Robert Plutchik"}, {"title": "The Spanish adaptation of ANEW (Affective Norms for English Words)", "journal": "Behavior Research Methods", "year": "2007", "authors": "Jaime Redondo; Isabel Fraga"}, {"title": "Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging", "journal": "", "year": "2017-09-09", "authors": "Nils Reimers; Iryna Gurevych"}, {"title": "Nencki Affective Word List (NAWL): The cultural adaptation of the Berlin Affective Word List-Reloaded (BAWL-R) for Polish. Behavior Research Methods", "journal": "", "year": "2015", "authors": "Monika Riegel; Ma\u0142gorzata Wierzba; Marek Wypych; Katarzyna \u0141ukasz\u017curawski; Anna Jednor\u00f3g; Artur Grabowska;  Marchewka"}, {"title": "Sentiment Analysis in Twitter", "journal": "", "year": "2015-06-04", "authors": "Sara Rosenthal; Preslav I Nakov; Svetlana Kiritchenko; M Saif; Alan Mohammad; Veselin Ritter;  Stoyanov"}, {"title": "Ultradense word embeddings by orthogonal transformation", "journal": "", "year": "2016-06-12", "authors": "Sascha Rothe; Sebastian Ebert; Hinrich Sch\u00fctze"}, {"title": "ANGST: Affective Norms for German Sentiment Terms, derived from the Affective Norms for English Words", "journal": "Behavior Research Methods", "year": "2014", "authors": "David S Schmidtke; Tobias Schr\u00f6der; Arthur M Jacobs; Markus Conrad"}, {"title": "Predicting emotional word ratings using distributional representations and signed clustering", "journal": "Association for Computational Linguistics", "year": "2017-04-03", "authors": "Jo\u00e3o Sedoc; Daniel Preo\u0163iuc-Pietro; Lyle H Ungar"}, {"title": "ANEW+ : Automatic expansion and validation of Affective Norms of Words lexicons in multiple languages", "journal": "", "year": "2016-05-23", "authors": "Samira Shaikh; Kit Cho; Tomek Strzalkowski; Laurie Feldman; John Lien; Ting Liu; George Aaron Broadwell"}, {"title": "Affective meaning, concreteness, and subjective frequency norms for Indonesian words", "journal": "Frontiers in Psychology", "year": "1907", "authors": "Agnes Sianipar; Ton Pieter Van Groenestijn;  Dijkstra"}, {"title": "Pinheiro, Alberto Sim\u00f5es, and Carla Sofia Frade. 2012. The adaptation of the Affective Norms for English Words (ANEW) for European Portuguese", "journal": "Behavior Research Methods", "year": "", "authors": "Ana Paula Soares; Montserrat Comesa\u00f1a; Ana P "}, {"title": "Norms for 10,491 Spanish words for five discrete emotions: Happiness, disgust, anger, fear, and sadness. Behavior Research Methods", "journal": "", "year": "2018", "authors": "Hans Stadthagen-Gonz\u00e1lez; Pilar Ferr\u00e9; Miguel A P\u00e9rez-S\u00e1nchez; Constance Imbault; Jos\u00e9 Antonio Hinojosa"}, {"title": "Norms of valence and arousal for 14,031 Spanish words", "journal": "Behavior Research Methods", "year": "2017", "authors": "Hans Stadthagen-Gonz\u00e1lez; Constance Imbault; Miguel A P\u00e9rez-S\u00e1nchez; Marc Brysbaert"}, {"title": "Characterization of the Affective Norms for English Words by discrete emotional categories", "journal": "Behavior Research Methods", "year": "2007", "authors": "Ryan A Stevenson; Joseph A Mikels; Thomas W James"}, {"title": "WORDNET-AFFECT: An affective extension of WORDNET", "journal": "", "year": "2004-05-24", "authors": "Carlo Strapparava; Alessandro Valitutti"}, {"title": "Crowdsourcing and validating event-focused emotion corpora for German and English", "journal": "", "year": "2019-07-28", "authors": "Enrica Troiano; Sebastian Pad\u00f3; Roman Klinger"}, {"title": "Measuring praise and criticism: Inference of semantic orientation from association", "journal": "ACM Transactions on Information Systems", "year": "2003", "authors": "D Peter; Michael L Turney;  Littman"}, {"title": "The Berlin Affective Word List Reloaded (BAWL-R). Behavior Research Methods", "journal": "", "year": "2009", "authors": "Melissa L.-H V\u00f5; Markus Conrad; Lars Kuchinke; Karolina Urton; Markus J Hofmann; Arthur M Jacobs"}, {"title": "Community-based weighted graph model for valence-arousal prediction of affective words", "journal": "IEEE/ACM Transactions on Audio, Speech, and Language Processing", "year": "2016", "authors": "Jin Wang; Liang-Chih Yu; K Robert Lai; Xuejie Zhang"}, {"title": "Norms of valence, arousal, and dominance for 13,915 English lemmas. Behavior Research", "journal": "Methods", "year": "2013", "authors": "Amy Beth Warriner; Victor Kuperman; Marc Brysbaert"}, {"title": "Basic emotions in the Nencki Affective Word List (NAWL BE): New method of classifying emotional stimuli", "journal": "", "year": "2015", "authors": "Ma\u0142gorzata Wierzba; Monika Riegel; Marek Wypych; Katarzyna Jednor\u00f3g; Pawe\u0142 Turnau; Anna Grabowska; Artur Marchewka"}, {"title": "Norms of valence, arousal, concreteness, familiarity, imageability, and context availability for 1,100 Chinese words", "journal": "Behavior Research Methods", "year": "2017", "authors": "Zhao Yao; Jia Wu; Yanyan Zhang; Zhenhong Wang"}, {"title": "Building Chinese affective resources in valence-arousal dimensions", "journal": "", "year": "2016-06-12", "authors": "Liang-Chih Yu; Lung-Hao Lee; Shuai Hao; Jin Wang; Yunchao He; Jun Hu; K Robert Lai; Xuejie Zhang"}, {"title": "Refining word embeddings for sentiment analysis", "journal": "", "year": "2017-09-09", "authors": "Liang-Chih Yu; Jin Wang; K Robert Lai; Xuejie Zhang"}, {"title": "2020. Fine-grained facial expression analysis using dimensional emotion model", "journal": "", "year": "2020-01-23", "authors": "Feng Zhou; Shu Kong; Charless C Fowlkes; Tao Chen; Baiying Lei"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "V a l A r o D o m J o y A n g S a d F e a D i s sunshine 8.1 5.3 5.4 4.2 1.2 1.3 1.3 1.2 terrorism", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure1: Schematic view on the methodology for generating and evaluating an emotion lexicon for a given target language based on source language supervision. Included is a toy example starting with an English VA lexicon (sunshine, nuclear, terrorism and the associated numerical scores for Valence and Arousal) and resulting in an extended German lexicon which incorporates translated entries with altered VA scores and additional entries originating from the embedding model with newly learned scores.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Silver evaluation results in Pearson's r. Languages (x-axis) are sorted according to mean correlation.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Sample entries from our English source lexicon described via eight emotional variables: Valence, Arousal, Dominance [VAD], and Joy, Anger, Sadness,", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Lexicons used for gold evaluation. IDs consist of the respective ISO 639-1 language code plus a cardinal number to distinguish different datasets, if needed; the format of emotion Encoding is specified and Size gives the number of lexical entries per lexicon.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Mn (vs. monolingual)  .87 (.84) .68 (.70) .74(.74)    ", "figure_data": "IDShared (%) ValAroDomen11032100 .94 (.87) .76 (.67) .88 (.76)en21034100 .92 (.92) .71 (.73) .78 (.82)es161259 .91 (.88) .71 (.70) .82 (.83)es2768554 .79 (.82) .64 (.74) -es336341 .91.73-de167767 .89 (.87) .78 (.80) .68 (.74)de2232980 .75.64-de391691 .80.67-pl1227146 .83 (.74) .74 (.70) .60 (.69)pl2138147 .82.61-zh1168560 .84 (.85) .56 (.63) -zh270163 .84.44-it66058 .89 (.86) .63 (.65) .76 (.75)pt64562 .89 (.86) .71 (.71) .75 (.73)nl206448 .85 (.79) .58 (.74) -id69646 .84 (.80) .64 (.60) .63 (.58)el63361 .86.50.74tr172135 .75.57-hr133144 .81.66-Mn (all).85.65.74"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Gold evaluation results for VAD (Valence, Arousal, Dominance) in Pearson's r. Parentheses give comparative monolingual results from", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": Gold evaluation results for BE5 (Joy, Anger,Sadness, Fear, Disgust) in Pearson's r. Shared wordsbetween TargetGold and TargetPred-test;(%): percentage relative to TargetGold; Mean overall datasets."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Comparison against human performance. Correlation between two gold standards, Gold1 and Gold2, with each other (G1vsG2), as well as with our lexicons TargetPred-test (G1vsPr and G2vsPr) relative to Emotional variable and Shared number of words.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Pred .871 .652 .733 .767 .734 .692 .728 .650 MT .796 .515 .613 .699 .677 .636 .654 .579 Diff .076 .137 .119 .068 .057 .056 .074 .071", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Quality of TargetMT vs. TargetPred in terms of average Pearson correlation over all languages and gold standards. Diff := Pred \u2212 MT.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "using default parameters.", "figure_data": "No.ISOFull NameSizeValAroDomJoyAngSadFeaDisMean1enEnglish2,000,004.94.76.88.90.91.90.89.89.882esSpanish2,001,183.89.70.80.83.86.85.82.81.823itItalian2,001,137.88.69.81.82.85.84.82.81.814deGerman2,000,507.89.66.81.82.84.82.80.81.815svSwedish2,000,980.87.64.80.82.84.82.81.80.806ptPortuguese2,001,078.86.70.78.78.83.81.78.82.797idIndonesian2,002,221.85.67.79.78.82.80.79.77.798huHungarian2,000,975.86.67.79.80.82.79.77.79.799frFrench2,001,517.85.65.79.78.82.81.78.81.7810fiFinnish2,000,841.86.64.79.81.82.78.77.80.7811roRomanian2,001,501.85.65.78.78.82.81.79.78.7812csCzech2,001,203.84.64.77.78.82.80.79.79.7813plPolish2,001,460.85.63.78.80.82.80.78.78.7814nlDutch2,000,721.85.64.78.77.80.79.77.78.7715noNorwegian (Bokm\u00e5l)2,000,876.84.63.77.78.82.78.78.78.7716trTurkish2,002,489.84.62.78.78.80.80.75.77.7717ruRussian2,001,317.82.64.75.80.81.77.77.77.7718elGreek2,001,704.82.63.76.78.80.78.77.78.7719ukUkrainian2,001,261.83.63.77.78.80.77.76.77.7620etEstonian2,001,125.83.59.75.77.81.78.77.78.7621caCatalan2,001,538.84.60.80.77.79.78.76.74.7622daDanish2,000,654.84.61.77.78.79.77.73.79.7623lvLatvian1,642,923.82.63.75.76.79.78.76.77.7624ltLithuanian2,001,306.83.63.77.75.79.77.75.76.7625bgBulgarian2,001,391.82.60.76.75.77.77.73.76.7426heHebrew2,001,984.80.62.72.76.78.76.74.75.7427zhChinese2,001,799.79.60.75.72.77.75.75.73.7328mkMacedonian1,356,402.82.54.75.77.76.73.72.74.7329afAfrikaans883,464.80.58.74.76.75.74.71.74.7330tlTagalog716,272.80.56.76.70.77.76.74.72.7331skSlovak2,001,221.80.60.75.74.74.73.71.73.7232sqAlbanian1,169,697.80.57.73.75.75.75.72.72.7233azAzerbaijani2,002,146.81.60.73.74.75.73.70.71.7234mnMongolian608,598.78.57.73.71.78.72.74.74.7235hyArmenian2,001,329.80.52.72.75.77.73.71.73.7236eoEsperanto2,001,575.77.55.71.72.76.74.73.73.7137slSlovenian1,992,272.81.54.75.74.74.70.70.72.7138hrCroatian2,001,570.78.56.71.72.74.71.71.73.7139glGalician1,336,256.78.53.72.72.76.74.71.71.7140srSerbian2,002,395.76.57.71.72.74.70.70.73.7041arArabic2,003,155.78.53.70.70.75.72.71.74.7042faPersian2,003,533.77.58.70.70.74.73.70.70.7043msMalay1,213,397.75.58.69.69.72.70.65.73.6944mrMarathi848,549.74.54.68.70.74.70.69.71.6945kaGeorgian1,567,232.76.52.72.70.72.71.70.66.6946jaJapanese2,003,306.72.58.67.68.71.70.70.68.6847hiHindi1,879,196.76.56.68.69.73.64.65.72.6848isIcelandic945,214.76.55.70.68.70.69.68.64.6749kkKazakh1,981,562.72.53.65.67.73.69.67.70.6750koKorean2,002,600.74.57.69.67.67.66.65.69.6751beBelarusian1,715,582.73.49.66.68.71.67.67.70.6652bnBengali1,471,709.74.50.67.67.70.67.67.66.6653knKannada1,747,421.70.47.65.67.71.68.67.68.6554cyWelsh502,006.72.51.67.64.69.65.64.66.6555urUrdu1,157,969.69.52.61.63.70.65.64.68.6456taTamil2,002,514.70.51.66.64.66.66.63.64.6457euBasque1,828,013.70.46.66.64.68.67.64.64.6458mlMalayalam2,002,920.67.51.62.63.67.67.62.61.6359guGujarati557,270.69.46.62.61.67.65.63.64.6260siSinhalese812,356.66.48.59.65.67.62.63.65.6261teTelugu1,880,585.69.46.62.60.65.63.61.65.6162neNepali580,582.68.44.62.63.65.63.61.62.6163tgTajik508,617.67.38.64.57.65.65.60.60.6064viVietnamese2,008,605.65.47.58.59.65.59.58.62.5965paEastern Punjabi403,997.67.37.61.59.64.61.58.62.5966bsBosnian1,124,938.63.43.60.57.64.61.61.60.5867kyKirghiz751,902.65.37.61.56.64.62.59.60.5868gaIrish321,249.64.47.59.58.61.61.59.55.5869fyWest Frisian530,054.61.43.54.53.60.59.55.58.5670uzUzbek833,860.60.38.55.56.57.56.54.53.5371swSwahili391,312.59.34.57.52.59.58.57.51.5372jvJavanese518,634.58.45.53.53.56.58.54.49.5373psPashto300,927.58.40.56.52.55.54.55.49.5374amAmharic308,109.56.31.52.48.53.54.52.47.4975lbLuxembourgish642,504.53.37.47.45.55.52.50.51.4976suSundanese327,533.54.36.47.45.53.52.48.52.4877thThai2,006,540.51.38.45.50.49.46.45.49.4778kmKhmer247,498.51.39.44.49.51.44.45.48.4679sdSindhi139,063.47.35.39.41.50.49.50.46.4580yiYiddish205,727.49.34.40.43.50.47.45.44.4481myBurmese339,628.49.36.42.43.49.45.46.43.4482laLatin1,088,139.47.33.40.39.47.46.43.44.4283mtMaltese204,630.47.32.44.38.43.40.39.38.4084gdScottish Gaelic150,694.45.36.39.40.36.36.35.33.3885soSomali177,405.40.22.35.36.44.41.41.38.3786mgMalagasy415,050.40.32.36.34.41.37.36.36.3787htHaitian118,302.39.22.33.30.42.42.37.38.3588kuKurdish (Kurmanji)395,645.37.22.33.33.34.33.31.35.3289cebCebuano2,006,001.34.22.29.34.36.32.33.34.3290coCorsican108,035.29.24.27.27.32.30.29.30.2991yoYoruba156,764.24.08.19.18.24.21.21.26.20"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Overview of generated emotion lexicons with silver evaluation results; sorted by Mean performance over the eight emotional variables.", "figure_data": ""}], "doi": "10.5281/zenodo.3779901"}