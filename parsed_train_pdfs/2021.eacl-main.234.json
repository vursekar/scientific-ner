{"authors": "Xiangyang Mou; Mo Yu; Shiyu Chang; Yufei Feng; Li Zhang; Hui Su", "pub_date": "", "title": "Complementary Evidence Identification in Open-Domain Question Answering", "abstract": "This paper proposes a new problem of complementary evidence identification for opendomain question answering (QA). The problem aims to efficiently find a small set of passages that covers full evidence from multiple aspects as to answer a complex question. To this end, we proposes a method that learns vector representations of passages and models the sufficiency and diversity within the selected set, in addition to the relevance between the question and passages. Our experiments demonstrate that our method considers the dependence within the supporting evidence and significantly improves the accuracy of complementary evidence selection in QA domain.", "sections": [{"heading": "Introduction", "text": "In recent years, significant progress has been made in the field of open-domain question answering (Chen et al., 2017;Wang et al., 2017Wang et al., , 2018Min et al., 2018;Asai et al., 2019). Very recently, some works turn to deal with a more challenging task of asking complex questions (Welbl et al., 2018;Yang et al., 2018) from the open-domain text corpus. In the open-domain scenario, one critical challenge raised by complex questions is that each question may require multiple pieces of evidence to get the right answer, while the evidence usually scatters in different passages. Examples in Figure 1 shows two types of questions that require evidence from multiple passages.\nTo deal with the challenging multi-evidence questions, an open-domain QA system should be able to (1) efficiently retrieve a small number of passages that cover the full evidence; and (2) accurately extract the answer by jointly considering the candidate evidence passages. While there have been several prior works in the latter direction (Wang et al., 2017; Figure 1: Examples of complex questions involving two facts of a person. Different facts are color-coded. P# are all relevant passages, while only the ones with solid-line boxes are the true supporting passages. Lin et al., 2018), the solutions to the first problem still rely on traditional or neural information retrieval (IR) approaches, which solely measure the relevance between the question and each individual paragraph, and will highly possibly put the wrong evidence to the top. 1 For example in Figure 1 (top), P1 and P2 are two candidate evidence passages that are closely related to the question but only cover the same unilateral fact required by the question, therefore leading us to the wrong answer Newton.\nThis paper formulates a new problem of complementary evidence identification for answering complex questions. The key idea is to consider the problem as measuring the properties of the selected passages, more than the individual relevance. Specifically, we hope the selected passages can serve as a set of spanning bases that supports the question. The selected passage set thus should satisfy the properties of (1)relevancy, i.e., they should be closely related to the question; (2) diversity, i.e., they should cover diverse information given the coverage property is satisfied; (3) compactness, i.e., the number of passages to satisfy the above properties should be minimal. With these three defined properties, we hope to both improve the selective accuracy and encourage the interpretability of the evidence identification. Note that complementary evidence identification in QA is different from Search Result Diversification (SRD) in IR on their requirement of compactness. The size of the selected set is constrained in QA tasks by the capability of downstream reasoning models and practically needs to be a small value, whereas it is not the case in SRD.\nTo achieve the above goals, a straightforward approach is to train a model that evaluates each subset of the candidate passages, e.g., by concatenating passages in any subsets. However, this approach is highly inefficient since it requires to encode O(K L ) passage subsets, where K is the total number of candidates and L is the maximum size of subsets. Thus, a practical complementary evidence identification method needs to be computationally efficient. This is especially critical when we use heavy models like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), where passage encoding is time and memory consuming.\nTo this end, we propose an efficient method to select a set of spanning passages that is sufficient and diverse. The core idea is to represent questions and passages in a vector space and define the measures of our criterion in the vector space. For example, in the vector space, sufficiency can be defined as a similarity between the question vector and the sum of selected passage vectors, measured by a cosine function with a higher score indicating a closer similarity; and diversity can be defined as 1 distance between each pair of passages. By properly training the passage encoder with a loss function derived by the above terms, we expect the resulted vector space satisfies the property that the complementary evidence passages lead to large scores. In addition, our method only encodes each passage in the candidate set once, which is more efficient than the naive solution mentioned above. To evaluate the proposed method, we use the multi-hop QA dataset HotpotQA (the full wiki setting) since the ground-truth of evidence passages are provided.\nExperiments show that our method significantly improves the accuracy of complementary evidence selection.", "n_publication_ref": 13, "n_figure_ref": 3}, {"heading": "Proposed Method", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Task Definition", "text": "Given a question q and a mixture set of paragraphs P = P + \u222a P \u2212 with some paragraphs p \u2208 P + relevant to q and some p \u2208 P \u2212 irrelevant. Our goal is to select a small subset of paragraphs P sel \u2282 P, such that every p \u2208 P sel satisfies p \u2208 P + (relevancy), and all p \u2208 P sel can jointly cover all the information asked by q (complementary). The off-the-shelf models select relevant paragraphs independently, thus usually cannot deal with the complementary property. The inner dependency among the selected P sel needs to be considered, which will be modeled in the remaining of the section.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model and Training", "text": "Vector Space Modeling We apply BERT model to estimate the likelihood of a paragraph p being the supporting evidence to the question q, denoted as P (p|q). Let q and p i denote the input texts of a question and a passage. We feed q and the concatenation of q and p i into the BERT model, and use the hidden states of the last layer to represent q and p i in vector space, denoted as q and p i respectively. A fully connected layer f (\u2022) followed by sigmoid activation is added to the end of the BERT model, and outputs a scalar P (p i |q) to estimate how relevant the paragraph p i is to the question. Note that in our implementation p i is based on both q and p i , but we omit the condition on q for simplicity.\nComplementary Conditions Previous works extract evidence paragraphs according to P (p|q), which is estimated on each passage separately without considering the dependency among selected paragraphs. To extract complementary evidence, we propose that the selected passages P sel should satisfy the following conditions that intuitively encourage each selected passage to be a basis to support the question:\n\u2022 Relevancy: P sel should have a high probability of p i \u2208P sel P (p i |q);\n\u2022 Diversity: P sel should cover passages as diverse as possible, which can be measured by the average distance between any pairs in P sel , e.g., maximizing i,j\u2208{i,j|p i ,p j \u2208P sel ,i =j} 1 (p i , p j ). Here 1 (\u2022, \u2022) denotes L 1 distance;\n\u2022 Compactness: P sel should optimize the aforementioned conditions while the size being minimal. In this work we constrain the compactness by fixing |P sel | and meanwhile maximizing cos( i\u2208{i|p i \u2208P sel } p i , q). We use cos(\u2022, \u2022) to encourage the collection of evidence covers what needed by the question.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Training with Complementary Regularization", "text": "We propose a new supervised training objective to learn the BERT encoder for QA that optimizes the previous conditions. Note that in this work we assume a set of labeled training examples are available, i.e., the ground truth annotations contain complementary supporting paragraphs. Recently there was a growing in such datasets (Yang et al., 2018;Yao et al., 2019), due to the increasing interest in model explainability. Also, such supervision signals can also be obtained with distant supervision.\nFor each training instance (q, P), we define\n{p i } + = {p i }, \u2200i \u2208 {i|p i \u2208 P + } (1) {p i } \u2212 = {p i }, \u2200i \u2208 {i|p i \u2208 P \u2212 } (2) {p i } = {p i } + \u222a {p i } \u2212 (3)\nDenoting y p i = 1 if p i \u2208 P + and y p i = 0 if p i \u2208 P \u2212 , we have the following training objective function:\nL({pi}; q; y) = Lsup({pi}; q; y)\n+ \u03b1L d ({pi} + ) + \u03b2Lc({pi}; q; y)(4)\nwhere\nLsup({pi}; q; y) = \u2212 i yp i log(f (pi)),(5)\nL d ({pi} + ) = p i ,p j ,i =j (1 \u2212 1(pi, p j )).(6)\nLc({pi}; q; y) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 \u2212 cos(q, i pi), if \u03a0p i yp i = 1 max(0, cos(q, i pi) \u2212 \u03b3), if \u03a0p i yp i = 0 (7)\nwhere \u03b1 and \u03b2 are the hyperparameter weights and 1 (\u2022, \u2022) denotes L1 loss between two input vectors. Eq 5 is the cross-entropy loss corresponding to relevance condition; Eq 6 regularizes the diversity condition; Eq 7 is the cosine-embedding loss 2 for the compactness condition and \u03b3 > 0 is the margin to encourage data samples with better question coverage.\n2 Refer to CosineEmbeddingLoss in PyTorch.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Inference via Beam Search", "text": "Score Function During inference, we use the following score function to find the best paragraph combination:\ng(P sel ; q; {pi}) = p i P (pi|q) + \u03b1 cos( p i pi, q) + \u03b2 p i ,p j ,i =j 1(pi, pj)(8)\nwhere \u03b1 and \u03b2 are hyperparameters similar to Eq 4. Note that our approach requires to encode each passage in P only once for each question, resulting in an O(K) time complexity of encoding (K = |P|); and the subset selection is performed in the vector space, which is much more efficient than selecting subsets before encoding.\nBeam Search In a real-world application, there is usually a large candidate set of P, e.g., retrieved passages for q via a traditional IR system. Our algorithm requires O(K) time encoding, and O(K L ) time scoring in vector space when ranking all the combinations in L candidates. Thus when K becomes large, it is still inefficient even when L = 2.\nWe resort to beam search to deal with scenarios with large Ks. The details can be found in Appendix A.\n3 Experiments", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Settings", "text": "Datasets Considering the prerequisite of sentence-level evidence annotations, we evaluate our approach on two datasets, a synthetic dataset MNLI-12 and a real application HotpotQA-50. Data sampling is detailed in Appendix B.\n\u2022 MNLI-12 is constructed based on the textual entailment dataset MNLI (Williams et al., 2018), in order to verify the ability of our method in finding complementary evidence. In original MNLI, each premise sentence corresponds to three hypotheses sentences: entailment, neutral and contradiction.\nTo generate complementary pairs for each premise sentence, we split each hypothesis sentence into two segments. The goal is to find the segment combination that entails premise sentence, and our dataset, by definition, ensures that only the combination of two segments from the entailment hypothesis can entail the premise, not any of its subset or other combinations. The original train/dev/test splits from MNLI are used.\n\u2022 HotpotQA-50 is based on the open-domain setting of the multi-hop QA benchmark HotpotQA (Yang et al., 2018). The original task requires to find evidence passages from abstract paragraphs of all Wikipedia pages to support a multi-hop question. For each q, we collect 50 relevant passages based on bigram BM25 (Godbole et al., 2019). Two positive evidence passages to each question are provided by human annotators as the ground truth.\nNote that there is no guarantee that P 50 covers both evidence passages here. We use the original development set from HotpotQA as our test set and randomly split a subset from the original training set as our development set.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Settings", "text": "Baseline We compare with the BERT passage ranker (Nie et al., 2019) that is commonly used on open-domain QA including HotpotQA. The baseline uses the same BERT architecture as our approach described in Section 2.2, but is trained with only the relevancy loss (Eq 5) and therefore only consider the relevancy when selecting evidence.\nWe also compare the DRN model from (Harel et al., 2019) which is designed for the SRD task. Their ensemble system first finds the most relevant evidence to the given question, and then select the second diverse evidence using their score function. The major differences from our method are that (1) they train two separate models for evidence selection; (2) they do not consider the compactness among the evidences. It is worth mentioning that we replace their LSTM encoder with BERT encoder for fair comparison.\nMetric During the evaluation we make each method output its top 2 ranked results 3 (i.e. the top 1 ranked pair from our method) as the prediction. The final performance is evaluated by exact match (EM), i.e., whether both true evidence passages are covered, and the F1 score on the test sets.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Results", "text": "In the experiments, we have M = 3, N = 4 for MNLI-12 and M = 4, N = 5 for HotpotQA-50 with our method. The values are selected according to development performance. We follow the settings and hyperparameters used in (Harel et al., 2019) for the DRN model.  many pieces of true evidences enclosed by the complete set of candidate passages where our proposed ranker selects from. For HotpotQA dataset, we use a bi-gram BM25 ranker to collect top 50 relevant passages and build the basis for the experiments 4 , which inevitably leads some of the true evidences to be filtered out and makes its upper-bound less than 100%. For the artificial MNLI-12 dataset, all the true evidences are guaranteed to be included.\nTable 1 shows that our method achieves significant improvements on both datasets. On HotpotQA-50, all systems have low EM scores, because of the relatively low recall of the BM25 retrieval. Only 35.49% of the samples in the test set contain both ground-truth evidence passages. On MNLI-12, the EM score is around 50%. This is mainly because the segments are usually much shorter than a paragraph, with an average length of 7 words. Therefore it is more challenging in matching the q with the p i s. Specifically, both our method and the BERT baseline surpass the DRN model on all datasets and metrics, which results from our question-conditioned passage encoding approach. Our defined vector space proves beneficial to model the complementation among the evidence with respect to a given question. The ablation study of our loss function further illustrates that the diversity and the compactness terms efficiently bring additional 20%/30% increase in EM score on two datasets and consequently raise the F1 score by about 8/6 absolute points.\nFigure 2 gives examples about how our model improves over the baseline. Our method can successfully select complementary passages while the baselines only select passages that look similar to the question. A more interesting example is given at the bottom where the top-50 only covers one supporting passage. The BERT baseline selects two incorrect passages that cover identical part of facts required by the question and similarly the DRN baseline select a relevant evidence and an irrelevant evidence, while our method scores lower the second passage that does not bring new information, and reaches a supporting selection. A similar situation contributes to the majority of improvement on one-supporting-evidence data sample in HotpotQA-50.\nInference Speed Our beam search with score function brings slight overheads to the running time. On HotpotQA-50, it takes 1,990 milliseconds (ms) on average to obtain the embeddings of all passages for one data sample whereas our vector-based complementary selection only adds an extra 2 ms which can be negligible compared to the encoding time.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Future Work", "text": "The latest dense retrieval methods (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) show promising results on efficient inference on the full set of Wikipedia articles, which allows to skip the initial standard BM25 retrieval and avoid the significant loss during the pre-processing step. Our proposed approach is able to directly cooperate with these methods as we all work in the vector space. Therefore, the extension to dense retrieval can be naturally the next step of our work.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In the paper, we propose a new problem of complementary evidence identification and define the criterion of complementary evidence in vector space. We further design an algorithm and a loss function to support efficient training and inference for complementary evidence selection. Compared to the baseline, our approach improves more than 20% and remains to scale well to the computationally complex cases.  In both examples, the DRN baseline first finds the most relevant evidence to the question (left) and then select a diverse one (right); the BERT baseline model selected the top-2 most relevant passages (P1, P2) to the question regardless of their complementation; whereas our model made the selection (P1, P3) with consideration of both relevance and evidence sufficiency. Note that, in the bottom example, one of the ground-truth supporting passages and the answer were excluded when building the dataset.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Complementary Evidence Selection via Beam Search", "text": "For efficient inference when L = 2, we start to select the top-N (N K) most relevant passages. Then we score the combinations between each passage pair in the top-N set and another top-M set. This reduces the complexity from O(K 2 ) to O(M N ). M is a hyperparameter corresponding to the beam size. In a more general setting with L \u2265 2, we have an algorithm with the complexity of O((L \u2212 1)M N ) instead of O(K L ), which is shown in algorithm 1.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Algorithm 1: Complementary Evidence Selection via Beam Search", "text": "Data: Vector representation of question (q), vector representation of all the N passages {pn} ({pn}); the maximum number of passage to select (L); the beam size (M ); a vector of weights for all regularization terms \u03bb. Result: The top ranked complementary passages. / * Predict the probability P (p i ) of being a supporting passage for each passage B Data Sampling  In original MNLI, each premise sentence P corresponds to one entailment EP , one neutral NP and one contradiction CP . We take the premise P as q, and split each of its corresponding hypotheses into two segments with a random cutting point near the middle of the sentence, resulting in a total of 6 segments {E 1 P , E 2 P , N 1 P , N 2 P , C 1 P , C 2 P }. Mixing them with the 6 segments corresponding to another premise X, we can finally have P + = {E 1 P , E 2 P } and P \u2212 = {N 1 P , N 2 P , C 1 P , C 2 P , E 1 X , E 2 X , N 1 X , N 2 X , C 1 X , C 2 X }. Consequently, we sample one positive and eight negative pairs respectively from P + and P \u2212 . A pair like {E 1 P , C 2 X } is considered as negative. To ensure the segments are literally meaningful, each segment is guaranteed to be longer than 5 words.\np i given q * / 1 for i \u2208 [1, N ] do 2 P (p i ) \u2190 f (q, p i );\nHotpotQA In HotpotQA, the true supporting paragraphs of each question q are given. Therefore, we can easily form P + and P \u2212 and sample positive and negative pairs of paragraphs respectively from P + and P \u2212 . A special pair that contains one true supporting paragraph and one non-supporting paragraph is considered as a negative pair.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Learning to retrieve reasoning paths over wikipedia graph for question answering", "journal": "", "year": "2019", "authors": "Akari Asai; Kazuma Hashimoto; Hannaneh Hajishirzi; Richard Socher; Caiming Xiong"}, {"title": "Reading wikipedia to answer open-domain questions", "journal": "", "year": "2017", "authors": "Danqi Chen; Adam Fisch; Jason Weston; Antoine Bordes"}, {"title": "Simple and effective multi-paragraph reading comprehension", "journal": "", "year": "2018", "authors": "Christopher Clark; Matt Gardner"}, {"title": "Think you have solved question answering? try arc, the ai2 reasoning challenge", "journal": "", "year": "2018", "authors": "Peter Clark; Isaac Cowhey; Oren Etzioni; Tushar Khot; Ashish Sabharwal; Carissa Schoenick; Oyvind Tafjord"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Multi-step entity-centric information retrieval for multi-hop question answering", "journal": "", "year": "2019", "authors": "Ameya Godbole; Dilip Kavarthapu; Rajarshi Das; Zhiyu Gong; Abhishek Singhal; Hamed Zamani; Mo Yu; Tian Gao; Xiaoxiao Guo; Manzil Zaheer"}, {"title": "Realm: Retrievalaugmented language model pre-training", "journal": "", "year": "2020", "authors": "Kelvin Guu; Kenton Lee; Zora Tung; Panupong Pasupat; Ming-Wei Chang"}, {"title": "Learning novelty-aware ranking of answers to complex questions", "journal": "", "year": "2019", "authors": "Shahar Harel; Sefi Albo; Eugene Agichtein; Kira Radinsky"}, {"title": "Dense passage retrieval for open-domain question answering", "journal": "", "year": "2020", "authors": "Vladimir Karpukhin; Barlas Oguz; Sewon Min; Ledell Wu; Sergey Edunov; Danqi Chen; Wentau Yih"}, {"title": "Latent retrieval for weakly supervised open domain question answering", "journal": "", "year": "2019", "authors": "Kenton Lee; Ming-Wei Chang; Kristina Toutanova"}, {"title": "Denoising distantly supervised open-domain question answering", "journal": "Long Papers", "year": "2018", "authors": "Yankai Lin; Haozhe Ji; Zhiyuan Liu; Maosong Sun"}, {"title": "Compositional questions do not necessitate multi-hop reasoning", "journal": "", "year": "2019", "authors": "Sewon Min; Eric Wallace; Sameer Singh; Matt Gardner; Hannaneh Hajishirzi; Luke Zettlemoyer"}, {"title": "Efficient and robust question answering from minimal context over documents", "journal": "", "year": "2018", "authors": "Sewon Min; Victor Zhong; Richard Socher; Caiming Xiong"}, {"title": "Revealing the importance of semantic retrieval for machine reading at scale", "journal": "", "year": "2019", "authors": "Yixin Nie; Songhe Wang; Mohit Bansal"}, {"title": "Deep contextualized word representations", "journal": "Long Papers", "year": "2018", "authors": "Matthew Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"title": "Do multi-hop readers dream of reasoning chains? arXiv preprint", "journal": "", "year": "2019", "authors": "Haoyu Wang; Mo Yu; Xiaoxiao Guo; Rajarshi Das; Wenhan Xiong; Tian Gao"}, {"title": "R 3: Reinforced ranker-reader for open-domain question answering", "journal": "", "year": "2018", "authors": "Shuohang Wang; Mo Yu; Xiaoxiao Guo; Zhiguo Wang; Tim Klinger; Wei Zhang; Shiyu Chang; Gerry Tesauro; Bowen Zhou; Jing Jiang"}, {"title": "Evidence aggregation for answer re-ranking in open-domain question answering", "journal": "", "year": "2017", "authors": "Shuohang Wang; Mo Yu; Jing Jiang; Wei Zhang; Xiaoxiao Guo; Shiyu Chang; Zhiguo Wang; Tim Klinger; Gerald Tesauro; Murray Campbell"}, {"title": "Constructing datasets for multi-hop reading comprehension across documents", "journal": "", "year": "2018", "authors": "Johannes Welbl; Pontus Stenetorp; Sebastian Riedel"}, {"title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering", "journal": "", "year": "2018", "authors": "Zhilin Yang; Peng Qi; Saizheng Zhang; Yoshua Bengio; W William; Ruslan Cohen; Christopher D Salakhutdinov;  Manning"}, {"title": "Docred: A large-scale document-level relation extraction dataset", "journal": "", "year": "2019", "authors": "Yuan Yao; Deming Ye; Peng Li; Xu Han; Yankai Lin; Zhenghao Liu; Zhiyuan Liu; Lixin Huang; Jie Zhou; Maosong Sun"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "AcknowledgmentAspecial thank to Rensselaer-IBM Artificial Intelligence Research Collaboration (RPI-AIRC) for providing externship and other supports. This work is funded by Cognitive and Immersive Systems Lab (CISL), a collaboration between IBM and RPI, and also a center in IBM's AI Horizons Network.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Gain from complementary selection.In both examples, the DRN baseline first finds the most relevant evidence to the question (left) and then select a diverse one (right); the BERT baseline model selected the top-2 most relevant passages (P1, P2) to the question regardless of their complementation; whereas our model made the selection (P1, P3) with consideration of both relevance and evidence sufficiency. Note that, in the bottom example, one of the ground-truth supporting passages and the answer were excluded when building the dataset.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "3 end 44Rank the passages by P (p i ); 5 Pspan = [] 6 Pick M passages with top P (p i ) into Pspan;7 for depth \u2208 [2, L] do 8 P span = [] ; 9 for j \u2208 [1, M ] do / * P j is a selected subset,sj is the corresponding score * / 10 Pop the j-th tuple (P j , sj) from Pspan; 11 for n \u2208 [1, N ] do 12 if The set P j \u222a {pn} is covered by P span then 13 continue 14 end / * rn is the regulation increases by adding pn to P j * / 15 Put (P j \u222a {pn}, sj + P (pn) + \u03bbrn) into P span ; 16 if More than M tuples added based on P j", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "shows the performance. The upper-bound measures how3  There is only one positive pair of evidences for each question.", "figure_data": "SystemHotpotQA-50 EM F1MNLI-12 EM F1Baseline Ranker 16.67 41.2941.6167.57DRN + BERT1.0335.376.2046.07Our Method20.15 49.1053.8173.18Upper-Bound35.49 61.08 100.00 100.00"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Model Evaluation (%). The upper-bound indicates the amount of true evidences contained by all candidate passages. The baseline ranker is a BERT ranker trained only with relevancy loss.", "figure_data": ""}], "doi": ""}