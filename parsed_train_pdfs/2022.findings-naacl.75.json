{"authors": "Elazar Gershuni; Yuval Pinter", "pub_date": "", "title": "Restoring Hebrew Diacritics Without a Dictionary", "abstract": "We demonstrate that it is feasible to accurately diacritize Hebrew script without any human-curated resources other than plain diacritized text. We present NAKDIMON, a two-layer character-level LSTM, that performs on par with much more complicated curationdependent systems, across a diverse array of modern Hebrew sources. The model is accompanied by a training set and a test set, collected from diverse sources.", "sections": [{"heading": "Introduction", "text": "The vast majority of modern Hebrew texts are written in a letter-only version of the Hebrew script, one which omits the diacritics present in the full diacritized, or dotted variant. 1 Since most vowels are encoded via diacritics, the pronunciation of words in the text is left underspecified, and a considerable mass of tokens becomes ambiguous. This ambiguity forces readers and learners to infer the intended reading using syntactic and semantic context, as well as common sense (Bentin and Frost, 1987;Abu-Rabia, 2001). In NLP systems, recovering such signals is difficult, and indeed their performance on Hebrew tasks is adversely affected by the presence of undotted text (Shacham and Wintner, 2007;Goldberg and Elhadad, 2010;Tsarfaty et al., 2019).\nAs an example, the sentence in Table 1 (a) will be resolved by a typical reader as (b) in most reasonable contexts, knowing that the word \"softly\" may characterize landings. In contrast, an automatic system processing Hebrew text may not be as sensitive to this kind of grammatical knowledge and instead interpret the undotted token as the more 'The plane landed congratulations' Table 1: An example of an undotted Hebrew text (a) (written right to left) which can be interpreted in at least two different ways (b,c), dotted and pronounced differently, but only (b) makes grammatical sense.\nfrequent word in (c), harming downstream performance.\nOne possible way to overcome this problem is by adding diacritics to undotted text, or dotting, implemented using data-driven algorithms trained on dotted text. Obtaining such data is not trivial, even given correct pronunciation: the standard Tiberian diacritic system contains several sets of identicallyvocalized forms, so while most Hebrew speakers easily read dotted text, they are unable to produce it. Moreover, the process of manually adding diacritics in either handwritten script or through digital input devices is mechanically cumbersome. Thus, the overwhelming majority of modern Hebrew text is undotted, and manually dotting it requires expertise. The resulting scarcity of available dotted text in modern Hebrew contrasts with Biblical and Rabbinical texts which, while dotted, manifest a very different language register. This state of affairs allows individuals and companies to offer dotting as paid services, either by experts or automatically, e.g. the Morfix engine by Melingo. 2 Such usage practices also force a disconnect in the NLP pipeline, requiring an API call into an external service whose parameters cannot be updated.\nExisting computational approaches to dotting are manifested as complex, multi-resourced systems which perform morphological analysis on the undotted text and look undotted words up in handcrafted dictionaries as part of the dotting process. Dicta's Nakdan (Shmidman et al., 2020), the current state-of-the-art, applies such methods in addition to applying multiple neural networks over different levels of the text, requiring manual annotation not only for dotting but also for morphology. Among the resources it uses are a diacritized corpus of 3M tokens and a POS-tagged corpus of 300K tokens. Training the model takes several weeks. 3 In this work, we set out to simplify the dotting task as much as possible to standard modules. We introduce a large corpus of semi-automatically dotted Hebrew, collected from various sources, and use it to train an RNN-based model. Our system, NAKDIMON, accepts the undotted character sequence as its input, consults no external resources or lexical components, and produces diacritics for each character, resulting in dotted text whose quality is comparable to that of the commercial Morfix, on both character-level and word-level accuracy. Our model is easy to integrate within larger systems that perform end-to-end Hebrew processing tasks, as opposed to the existing proprietary dotters. To our knowledge, this is the first attempt at a \"light\" model for Hebrew dotting since early HMM-based systems (Kontorovich, 2001;Gal, 2002).\nWe introduce a novel test set for Modern Hebrew dotting, derived from larger and more diverse sources than existing datasets. In experiments over our dataset, we show that our system is particularly useful in the main use case of modern dotting, which is to convey the desired pronunciation to a reader, and that the errors it makes should be more easily detectable by non-professionals than Dicta's. 4\n2 Task and Datasets", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Dotting as Sequence Labeling", "text": "The input to the dotting task consists of a sequence of characters. Each of the characters is assigned three values, from three separate diacritic categories: one category for the dot distinguishing shin ( \u202b)\ufb2a\u202c from sin ( \u202b,)\ufb2b\u202c two consonants sharing a base character \u202b;\u05e9\u202c another for the presence of dagesh/mappiq, a central dot affecting pronunciation of some consonants, e.g. \u202b\ufb44\u202c /p/ from \u202b\u05e4\u202c /f/, but also present elsewhere; and one for all other diacritic marks, which mostly determine vocalization, e.g. \u202b\u05d3\u202c /da/ vs. \u202b\u05d3\u202c /de/. Diacritics of different categories may co-occur on single letters, e.g. , or may be absent altogether.\nFull script Hebrew script written without intention of dotting typically employs a compensatory variant known colloquially as full script (ktiv male, \u202b\u05de\u05dc\u05d0\u202c \u202b,)\u05db\u05ea\u05d9\u05d1\u202c which adds instances of the letters \u202b\u05d9\u202c and \u202b\u05d5\u202c in some places where they can aid pronunciation, but are incompatible with the rules for dotted script. In our formulation of dotting as a sequence tagging problem, and in collecting our test set from raw text, these added letters may conflict with the dotting standard. For the sake of input integrity, and unlike some other systems, we opt not to remove these characters, but instead employ a dotting policy consistent with full script. See Appendix A for further details.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Training corpora", "text": "Dotted modern Hebrew text is scarce, since speakers usually read and write undotted text, with the occasional diacritic added for disambiguation when context does not suffice. As we are unaware of legally-obtainable dotted modern corpora, we use a combination of dotted pre-modern texts as well as automatically and semi-automatically dotted modern sources to train NAKDIMON:\nThe PRE-MODERN portion is obtained from two main sources: A combination of late pre-modern text from Project Ben-Yehuda, mostly texts from the late 19th century and the early 20th century; 5 rabbinical texts from the medieval period, the most important of which is Mishneh Torah (obtained from Project Mamre); 6 and 23 short stories from the short story project. 7 This portion contains roughly 1.81M Hebrew tokens, most of which are dotted, with a varying level of accuracy, varying dotting styles, and varying degree of similarity to Modern Hebrew.\nThe AUTOMATIC portion contains 547 short stories taken from the short story project. The stories are dotted using Dicta without manual validation.  The corpus contains roughly 1.27M Hebrew tokens.\nLastly, the MODERN portion contains manually collected text in Modern Hebrew, mostly from undotted sources, which we dot using Dicta and follow up by manually fixing errors, either using Dicta's API or via automated scripts which catch common mistakes. We made an effort to collect a diverse set of sources: news, opinion columns, paragraphs from books, short stories, Wikipedia articles, governmental publications, blog posts and forums expressing various domains and voices, and more. Our MODERN corpus contains roughly 326K Hebrew tokens, and is much more consistent and similar to the expectation of a native Hebrew speaker than the PRE-MODERN or the AUTOMATIC corpora, and more accurately dotted than the AU-TOMATIC corpus. The sources and statistics of this dataset are presented in Table 2.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "New test set", "text": "Shmidman et al. (2020) provide a benchmark dataset for dotting modern Hebrew documents. However, it is relatively small and non-diverse: all 22 documents in the dataset originate in a single source, namely Hebrew Wikipedia articles.\nTherefore, we created a new test set 8 from a larger variety of texts, including high-quality Wikipedia articles and edited news stories, as well as user-generated blog posts. This set consists of ten documents from each of eleven sources (5x Dicta's test set), and totals 20,474 Hebrew tokens, roughly 3.5x Dicta's. We use the same technique and style for dotting this corpus as we do for the MODERN corpus ( \u00a72.2), but the documents were 3 Nakdimon NAKDIMON embeds the input characters and passes them through a two-layer Bi-LSTM (Hochreiter and Schmidhuber, 1997). The LSTM output is fed into a single linear layer, which then feeds three linear layers, one for each diacritic category (see \u00a72). Each character then receives a prediction for each category independently and all predicted marks are added to it as output.\nDecoding is performed greedily, with no validation of readability or any other dependence between character-level decisions.\nThe input is pre-processed by removing all but Hebrew characters, spaces and punctuation; digits are converted to a dedicated symbol, as are Latin characters. All existing diacritic marks are stripped, and each document is split into chunks bounded at whitespace, ignoring sentence boundaries.\nWe train NAKDIMON first over PRE-MODERN, then over the AUTOMATIC corpus, and then by over the MODERN corpus. During training, the loss is the sum of the cross-entropy loss from all three categories. Trivial decisions, such as the label for the shin/sin diacritic for any non-\u202b\u05e9\u202c letter, are masked.\nTuning experiments are detailed in Appendix B; an evaluation of a preliminary version of NAKDI-MON over the Dicta test set is in Appendix C, and Hyperparameters are detailed in Appendix D.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We compare the performance of NAKDIMON on our new test set ( \u00a72.3) against Dicta, 9 Snopi, 10 and Morfix (Kamir et al., 2002). as well as a MA-JORITY baseline which returns the most common dotting for each word seen in our full training set.\nMetrics We report four metrics: decision accuracy (DEC) is computed over the entire set of individual possible decisions: dagesh/mappiq for letters that allow it, sin/shin dot for the letter \u202b,\u05e9\u202c and all other diacritics for letters that allow them; character accuracy (CHA) is the portion of characters in the text that end up in their intended final form (which may combine two or three decisions, e.g. dagesh + vowel); word accuracy (WOR) is the portion of words with no mistakes; and vocalization  ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Results", "text": "We provide document-level macro-averaged accuracy percentage results for a single run over our test set in Table 3. All systems, except Snopi, substantially outperform the majority-dotting baseline on all metrics. NAKDIMON outperforms Morfix on character-level metrics but not on word-level metrics, mostly since Morfix ignores certain words altogether, incurring errors on multiple characters. We note the substantial improvement our model achieves on the VOC metric compared to the WOR metric: 18.43% of word-level errors are attributable to vocalization-agnostic dotting, compared to 13.80% for Dicta and 10.41% for Snopi (but 20.91% for Morfix). Considering that the central use case for dotting modern Hebrew text is to facilitate pronunciation to learners and for reading, and that undotted homograph ambiguity typically comes with pronunciation differences, we believe this measure to be no less important than WOR.\nResults on Dicta's test set (Shmidman et al., 2020) are presented in Appendix C.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Error analysis", "text": "In Table 4 we present examples of words dotted incorrectly, or correctly, only by NAKDIMON, compared with Morfix and Dicta. The largest category for NAKDIMON-only errors (\u223c18% of 90 sampled) are ones where a fused preposition+determiner character is dotted to only include the preposition, perhaps due to its inability to detect the explicit determiner clitic \u202b\u05d4\u202c in neighboring words, on which the complex systems apply morphological segmentation. In other cases (\u223c15%), NAKDIMON creates 11 These are: the sin/shin dot, vowel distinctions across the a/e/i/o/u/null sets, and dagesh in the \u202b/\u05d1\u202c \u202b/\u05db\u202c \u202b\u05e4\u202c characters. We do not distinguish between kamatz gadol/kamatz katan, and schwa is assumed to always be null.  unreadable vocalization sequences, as it has no lexical component and is decoded greedily. These types of errors are more friendly to the typical use cases of a dotting system, as they are likely to stand out to a reader. In contrast, a large portion of cases where only NAKDIMON was correct (\u223c13% of 152) are foreign names and terms. This may be the result of such words not yet appearing in dictionaries, or not being easily separable from an adjoining clitic, while character-level information can capture pronunciation patterns from similar words (e.g. \u202b\u05b6\u05e4\ufb4b\u202c \u202b\u05b6\u05dc\u202c \u202b\u05d8\u202c 'telephone', for the example \u202b.)\u05d4\u05d0\u05d9\u05d9\u05e4\u05d5\u202c OOVs To further quantify the strengths of NAKDIMON's architecture and training abilities, we evaluate the systems' results pertaining only to those words in the test set which do not appear in our training sets. We follow common practice by calling them OOVs (\"out of vocabulary\"), but emphasize that NAKDIMON does not consult an explicit vocabulary, and the other systems are not evaluated against their own vocabularies (which are unknown to us).\nWe find that NAKDIMON's performance on this subset is substantially worse compared with the other systems than on the full set: 15 percentage points below Dicta and seven below Morfix on the VOC metric (see full results in Appendix C).\nThese results might be counter-intuitive considering the proven utility of character-level models in OOV contexts (e.g., Plank et al., 2016), and so we offer several possible explanations: First, many \"OOVs\" consist in fact of known words coupled with an unseen combination of prefix clitics and/or suffix possessive markers, which other systems explicitly remove using morphological analyzers before dotting. Second, mirroring the last finding from the overall analysis, some \"OOVs\" are proper names which appear in dictionaries but are absent from the training set, due to corpus effects such as time and domain, or simply chance.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Existing work on diacritizing Hebrew is not common, and all efforts build on word-level features. Kontorovich (2001) trains an HMM on a vocalized and morphologically-tagged portion of the Hebrew Bible containing 30,743 words, and evaluates the result on a test set containing 2,852 words, achieving 81% WOR accuracy. Note that Biblical Hebrew is very different from Modern Hebrew in both vocabulary, grammatical structure, and diacritization, and also has many words with unique diacritization. In our system, we exclude the Bible altogether from the training set, as its inclusion actively hurts performance on the validation set, which consists of Modern Hebrew.\nTomer (2012) designs a diacritization system for Hebrew verbs consisting of a combination of a verb inflection system, a syllable boundary detector, and an SVM model for classifying verb inflection paradigms. The focus on verbs in a type-level setup makes this work incomparable to ours or to others in this survey.\nIn Arabic, diacritization serves a comparable purpose to that in Hebrew, but not exclusively: most diacritic marks differentiate consonantal phonemes from each other, e.g. /b/ vs. /t/ (which only the sin/shin dot does in Hebrew), whereas vocalization marks are in a one-to-one relationship with their phonetic realizations, e.g. only the fatha as in /ba/ encodes the /a/ vowel. Dictionary-less Arabic diacritization has been attempted using a 3-layer Bi-LSTM (Belinkov and Glass, 2015). Abandah et al. (2015) use a Bi-LSTM where characters are assigned either one or more diacritic symbols. Our system differs from theirs by virtue of separating the diacritization categories. Mubarak et al. (2019) tackled Arabic diacritization as a sequence-to-sequence problem, tasking the model with reproducing the characters as well as the marks. Zalmout and Habash (2017) have made the case against RNN-only systems, arguing for the importance of morphological analyzers in Arabic NLP systems. We concede that well-curated systems may perform better than uncurated ones, particularly on low-resource languages such as Hebrew, but we note that they are difficult to train for individual use cases and are burdensome to incorporate within larger systems.\nDiacritics restoration in Latin-based scripts, applicable mostly to European languages, forms a substantially different problem from the one in Hebrew given the highly lexicalized nature of diacritic usage in these languages and the very low rate of characters requiring diacritics. The state-of-theart systems in such languages employ transformer models in a sequence-to-sequence setup (N\u00e1plava et al., 2021;Stankevi\u010dius et al., 2022), supplanting character-RNN sequence prediction architectures reminiscent of ours (N\u00e1plava et al., 2018). Indeed, the authors of this latter work note the only non-European in their dataset, Vietnamese, as a special outlier.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "Learning directly from plain diacritized text can go a long way, even with relatively limited resources. NAKDIMON demonstrates that a simple architecture for diacritizing Hebrew text as a sequence tagging problem can achieve performance on par with much more complex systems. We also introduce and release a corpus of dotted Hebrew text, as well as a source-balanced test set.\nIn the future, we wish to evaluate the utility of dotting as a feature for downstream tasks such as question answering, machine translation, and speech generation, taking advantage of the fact that our simplified model can be easily integrated in an end-to-end Hebrew processing system.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ethical Considerations", "text": "We collected the data for our training set and test sets from open online sources, while making sure their terms allow research application and privacy is not impugned. NAKDIMON's architecture does not encourage memorization of training data and the system is not trained for generating text.\nWe consider a main use case for our system to be assisting Hebrew learners in reading. We therefore expect NAKDIMON to facilitate life in Israel for immigrants still struggling with Hebrew, among other underprivileged groups. Automatic dotting can increase inclusion in Hebrew-prominent societies for literacy-challenged individuals, and derivative improvements in text-to-speech applications can assist those with impaired vision. Lastly, dotting can help researchers with limited understanding of Hebrew access resources in the language.\nHebrew is a gendered language. Orthographically, in many cases the lack of dots masks gender ambiguity, allowing both masculine and feminine readings for a given word (e.g. \u05b0 \u202b\u05b0\ufb4a\u202c \u202b\u05b7\u05d7\u202c \u202b\u05dc\u202c \u202b\ufb2a\u202c / \u05b8 \u202b\u05b0\ufb4a\u202c \u202b\u05b7\u05d7\u202c \u202b\u05dc\u202c \u202b\ufb2a\u202c 'you.fem sent' / 'you.masc sent'). While wellperforming automatic dotting can help alleviate these ambiguities and reduce the amount of potentially prejudiced readings, we recognize the large body of work on gender bias in NLP (Blodgett et al., 2020), including in Hebrew NLP (Moryossef et al., 2019), and the findings that an imbalanced training set may result in an even more skewed distribution of gender bias in applications (Zhao et al., 2017). We believe our unlexicalized approach is more robust to such bias compared with other systems, and have already started quantifying and addressing these issues as we find them in ongoing work. In the meantime, we offer this paragraph as a disclaimer.  ", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "A Full Script Reconciliation", "text": "We apply the following resolution tactics for added letters in undotted text: (a) We almost never remove or add letters to the original text (unless it is completely undiacritizable). (b) We keep dagesh in letters that follow a shuruk which replaces a kubuts, and similarly for yod (hirik male replacing hirik haser). (c) When we have double vav or double yod, the second letter is usually left undotted, except when it is impossible to have the correct vocalization this way.\nResolving ktiv haser discrepancies from Morfix outputs is done by adding missing vowel letters, or removing superfluous vowel letters, in such a way that would not count as an error if it is correct according to Academy regulations.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B Development Experiments", "text": "We tried to further improve NAKDIMON by initializing its parameters from a language model trained to predict masked characters in a large undotted Wikipedia corpus (440MB, 30% mask rate), but were only able to achieve an improvement of 0.07%. Attempted architectural modifications, including substituting a Transformer (Vaswani et al., 2017) for the LSTM; adding a CRF layer to the decoding process; and adding a residual connection between the character LSTM layers, yielded no substantial benefits in these experiments. Similarly, varying the number of LSTM layers between 2 and 5 (keeping the total number of parameters roughly constant, close to the 5,313,223 parameters of our final model) had little to no impact on the accuracy on the validation set.  ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "C Dicta Test Set", "text": "We present results for the Dicta test set in Table 5. In order to provide fair comparison and to preempt overfitting on this test data, we ran this test in a preliminary setup on a variant of NAKDIMON which was not tuned or otherwise unfairly trained. This system, NAKDIMON 0 , differs from our final variant in three main aspects: it is not trained on the Dicta portion of our training corpus ( \u00a72.2), it is not trained on the AUTOMATIC corpus, and it employs a residual connection between the two character Bi-LSTM layers. Testing on the Dicta test set required some minimal evaluation adaptations resulting from encoding constraints (for example, we do not distinguish between kamatz katan and kamatz gadol). Thus, we copy the results reported in", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We would like to thank Avi Shmidman for details about Dicta's Nakdan and other suggestions. We thank Sara Gershuni for lengthy and fruitful discussions, and for her linguistic insights and advice. We thank Yoav Goldberg, Reut Tsarfaty, Ian Stewart, Sarah Wiegreffe, Kyle Gorman and many anonymous reviewers for their comments and suggestions in discussions and on earlier drafts.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Automatic diacritization of Arabic text using recurrent neural networks", "journal": "", "year": "2015", "authors": "Gheith Abandah; Alex Graves; Balkees Al-Shagoor; Alaa Arabiyat; Fuad Jamour; Majid Al-Taee"}, {"title": "The role of vowels in reading semitic scripts: Data from Arabic and Hebrew", "journal": "Reading and Writing", "year": "2001", "authors": "Salim Abu-Rabia"}, {"title": "Arabic diacritization with recurrent neural networks", "journal": "", "year": "2015", "authors": "Yonatan Belinkov; James Glass"}, {"title": "Processing lexical ambiguity and visual word recognition in a deep orthography", "journal": "Memory & Cognition", "year": "1987", "authors": "Shlomo Bentin; Ram Frost"}, {"title": "Language (technology) is power: A critical survey of \"bias\" in NLP", "journal": "", "year": "2020", "authors": " Su Lin; Solon Blodgett; Hal Barocas; Iii Daum\u00e9; Hanna Wallach"}, {"title": "An HMM approach to vowel restoration in Arabic and Hebrew", "journal": "Association for Computational Linguistics", "year": "2002", "authors": " Ya"}, {"title": "Easy-first dependency parsing of modern Hebrew", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Yoav Goldberg; Michael Elhadad"}, {"title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"title": "A comprehensive NLP system for modern standard Arabic and modern Hebrew", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Dror Kamir; Naama Soreq; Yoni Neeman"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Problems in Semitic NLP: Hebrew vocalization using HMMs", "journal": "", "year": "2001", "authors": "Leonid Kontorovich"}, {"title": "Filling gender & number gaps in neural machine translation with black-box context injection", "journal": "", "year": "2019", "authors": "Amit Moryossef; Roee Aharoni; Yoav Goldberg"}, {"title": "Highly effective Arabic diacritization using sequence to sequence modeling", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Hamdy Mubarak; Ahmed Abdelali; Hassan Sajjad; Younes Samih; Kareem Darwish"}, {"title": "Diacritics restoration using bert with analysis on czech language", "journal": "", "year": "2021", "authors": "Jakub N\u00e1plava; Milan Straka; Jana Strakov\u00e1"}, {"title": "Diacritics restoration using neural networks", "journal": "", "year": "2018", "authors": "Jakub N\u00e1plava; Milan Straka; Pavel Stra\u0148\u00e1k; Jan Haji\u010d"}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Barbara Plank; Anders S\u00f8gaard; Yoav Goldberg"}, {"title": "Morphological disambiguation of Hebrew: A case study in classifier combination", "journal": "", "year": "2007", "authors": "Danny Shacham; Shuly Wintner"}, {"title": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)", "journal": "", "year": "", "authors": ""}, {"title": "Nakdan: Professional Hebrew diacritizer", "journal": "", "year": "2020", "authors": "Avi Shmidman; Shaltiel Shmidman; Moshe Koppel; Yoav Goldberg"}, {"title": "Cyclical learning rates for training neural networks", "journal": "IEEE", "year": "2017", "authors": "N Leslie;  Smith"}, {"title": "Monika Briedien\u0117, and Tomas Krilavi\u010dius. 2022. Correcting diacritics and typos with ByT5 transformer model", "journal": "", "year": "", "authors": "Lukas Stankevi\u010dius; Mantas Luko\u0161evi\u010dius"}, {"title": "Automatic Hebrew Text Vocalization", "journal": "", "year": "2012", "authors": "Eran Tomer"}, {"title": "What's wrong with Hebrew NLP? and how to make it right", "journal": "", "year": "2019", "authors": "Reut Tsarfaty; Shoval Sadde; Stav Klein; Amit Seker"}, {"title": "", "journal": "", "year": "", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Illia Kaiser;  Polosukhin"}, {"title": "Don't throw those morphological analyzers away just yet: Neural morphological disambiguation for Arabic", "journal": "", "year": "2017", "authors": "Nasser Zalmout; Nizar Habash"}, {"title": "Men also like shopping: Reducing gender bias amplification using corpus-level constraints", "journal": "", "year": "2017", "authors": "Jieyu Zhao; Tianlu Wang; Mark Yatskar; Vicente Ordonez; Kai-Wei Chang"}, {"title": "We see that the untuned NAKDIMON 0 performs on par with the proprietary Morfix, which uses word-level dictionary data, consistent with our with 27,681 tokens, on which Dicta performs at 91.56% WOR accuracy. In our chosen setup, we train NAKDIMON over PRE-MODERN for a single epoch, followed by two epochs over the AUTOMATIC corpus, and then by three epochs over the MODERN corpus. We optimize using Adam (Kingma and Ba", "journal": "", "year": "2014", "authors": " Shmidman"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: WOR error rate on validation set as a function of training set size vs. Dicta, over five runs. Other metrics show similar trends.", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 11Figure1shows the favorable effect of training NAKDIMON over an increasing amount of MOD-ERN text.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Data sources for our MODERN Hebrew training set. Rows marked with * were automatically dotted via the Dicta API and corrected manually. Rows with \u2020 were dotted at low quality, requiring manual correction. The rest were available with professional dotting.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "90.01 84.87 86.19 SNOPI 91.29 85.84 76.45 78.91 MORFIX 96.84 94.92 90.38 92.39 DICTA 97.95 96.77 94.11 94.92 NAKDIMON 97.91 96.37 89.75 91.64", "figure_data": "SystemDECCHAWORVOCMAJORITY93.79"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Document-level macro % accuracy. accuracy (VOC) is the portion of words where any dotting errors do not cause incorrect pronunciation among mainstream Israeli Hebrew speakers.11    ", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Context Correct Incorrect . \u202b\u05d1\u05e2\u05d9\u05e0\u05d9\u05d9\u202c \u202b\u05dc\u05d4\u202c \u202b\u05dc\u05d4\u05e1\u05ea\u05db\u05dc\u202c \u202b\u05d5\u05e6\u05e8\u05d9\u202c . . . \u202b\u05d9\u202c \u202b\u05d9\u202c \u202b\u05b5\u05d9\u05e0\u202c \u202b\u05b8\u05e2\u202c \u202b\ufb31\u202c \u202b\u05d9\u202c \u202b\u05d9\u202c \u202b\u05b5\u05d9\u05e0\u202c \u202b\u05b0\u05e2\u202c \u202b\ufb31\u202c '. . . and we need to look her in the eyes (/in eyes).' . . . \u202b\u05d1\u05e1\u05d1\u05dc\u05e0\u05d5\u05ea\u202c \u202b\u05dc\u202c \u202b\u05d9\u05e2\u05e0\u05d5\u202c . . . \u05b8 \u202b\u05dc\u202c \u05b0 \u202b\u05dc\u202c '. . . you.sg.f (/unreadable) will be answered patiently. . . ' . . . \u202b\u05d4\u05e8\u05d0\u05e9\u05d5\u05e0\u05d9\u202c \u202b\u05d4\u05d0\u05d9\u05d9\u05e4\u05d5\u202c \u202b\u05de\u05e9\u05ea\u05de\u05e9\u05d9\u202c . . . \u202b\u05d9\u05e4\ufb4b\u202c \u202b\u05b7\u05d9\u202c \u202b\u05b8\u05d0\u202c \u202b\u05d4\u202c \u202b\u05b4\u05d9\u05d9\ufb44\ufb4b\u202c \u202b\u05b8\u05d0\u202c \u202b\u05d4\u202c '. . . the first iPhone (/ee-pon) users. . . '", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Examples of words dotted incorrectly (top) or correctly (bottom) only by NAKDIMON.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Document-level macro % accuracy on the test set from Shmidman et al. (2020) and on our new test set. We cannot report our full NAKDIMON's performance on the former, as we use the test set for parts of its training. MAJALL is reported as MAJORITY in the main text; MAJMOD only considers text in the MODERN portion of our training set.", "figure_data": ""}], "doi": "10.1007/s10032-015-0242-2"}