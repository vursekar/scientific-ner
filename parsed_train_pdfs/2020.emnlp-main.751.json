{"authors": "Manik Bhandari; Pranav Gour; Atabak Ashfaq; Pengfei Liu; Graham Neubig", "pub_date": "", "title": "Re-evaluating Evaluation in Text Summarization", "abstract": "Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not -for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both systemlevel and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive):", "sections": [{"heading": "Introduction", "text": "In text summarization, manual evaluation, as exemplified by the Pyramid method (Nenkova and Passonneau, 2004), is the gold-standard in evaluation. However, due to time required and relatively high cost of annotation, the great majority of research papers on summarization use exclusively automatic evaluation metrics, such as ROUGE (Lin, 2004) , JS-2 (Louis and Nenkova, 2013), S3 (Peyrard et al., 2017), BERTScore (Zhang et al., 2020), Mover-Score (Zhao et al., 2019) etc. Among these metrics, ROUGE is by far the most popular, and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of baseline and proposed methods. To characterize the relative goodness of evaluation metrics, it is necessary to perform metaevaluation (Graham, 2015;Lin and Och, 2004), where a dataset annotated with human judgments (e.g. TAC 1 2008 (Dang and Owczarzak, 2008)) is used to test the degree to which automatic metrics correlate therewith.\nHowever, the classic TAC meta-evaluation datasets are now 6-12 years old 2 and it is not clear whether conclusions found there will hold with modern systems and summarization tasks. Two earlier works exemplify this disconnect: (1) Peyrard (2019) observed that the human-annotated summaries in the TAC dataset are mostly of lower quality than those produced by modern systems and that various automated evaluation metrics strongly disagree in the higher-scoring range in which current systems now operate. (2) Rankel et al. (2013) observed that the correlation between ROUGE and human judgments in the TAC dataset decreases when looking at the best systems only, even for systems from eight years ago, which are far from today's state-of-the-art.\nConstrained by few existing human judgment datasets, it remains unknown how existing metrics behave on current top-scoring summarization systems. In this paper, we ask the question: does the rapid progress of model development in summarization models require us to re-evaluate the evaluation process used for text summarization? To this end, we create and release a large benchmark for meta-evaluating summarization metrics including:\n\u2022 Outputs from 25 top-scoring extractive and abstractive summarization systems on the CNN/DailyMail dataset. \u2022 Automatic evaluations from several evaluation metrics including traditional metrics (e.g. ROUGE) and modern semantic matching metrics (e.g. BERTScore, MoverScore).\nAbility of metrics to Observations on existing human judgments (TAC) Observations on new human judgments (CNNDM)\nExp-I: evaluate all systems? (Sec. 4.1) MoverScore and JS-2 outperform all other metrics. ROUGE-2 outperforms all other metrics. MoverScore and JS-2 performs worse both in extractive (only achieved nearly 0.1 Pearson correlation) and abstractive summaries.\nExp-II: evaluate top-k systems? (Sec. 4.2) As k becomes smaller, ROUGE-2 de-correlates with humans. For extractive and abstractive systems, ROUGE-2 highly correlates with humans. For evaluating a mix of extractive and abstractive systems, all metrics de-correlate.\nExp-III: compare 2 systems? (Sec. 4.3) MoverScore and JS-2 outperform all other metrics. ROUGE-2 is the most reliable for abstractive systems while ROUGE-1 is most reliable for extractive systems.\nExp-IV: evaluate summaries? (Sec. 4.4) (1) MoverScore and JS-2 outperform all other metrics. (2) Metrics have much lower correlations when evaluating summaries than systems.\n(1) ROUGE metrics outperform all other metrics. (2) For extractive summaries, most metrics are better at evaluating summaries than systems. For abstractive summaries, some metrics are better at summary level, others are better at system level. \u2022 Manual evaluations using the lightweight pyramids method (Shapira et al., 2019), which we use as a gold-standard to evaluate summarization systems as well as automated metrics.\nUsing this benchmark, we perform an extensive analysis, which indicates the need to re-examine our assumptions about the evaluation of automatic summarization systems. Specifically, we conduct four experiments analyzing the correspondence between various metrics and human evaluation. Somewhat surprisingly, we find that many of the previously attested properties of metrics found on the TAC dataset demonstrate different trends on our newly collected CNNDM dataset, as shown in Tab. 1. For example, MoverScore is the best performing metric for evaluating summaries on dataset TAC, but it is significantly worse than ROUGE-2 on our collected CNNDM set. Additionally, many previous works (Novikova et al., 2017;Peyrard et al., 2017;Chaganty et al., 2018) show that metrics have much lower correlations at comparing summaries than systems. For extractive summaries on CNNDM, however, most metrics are better at comparing summaries than systems.\nCalls for Future Research These observations demonstrate the limitations of our current bestperforming metrics, highlighting (1) the need for future meta-evaluation to (i) be across multiple datasets and (ii) evaluate metrics on different application scenarios, e.g. summary level vs. system level (2) the need for more systematic metaevaluation of summarization metrics that updates with our ever-evolving systems and datasets, and (3) the potential benefit to the summarization community of a shared task similar to the WMT 3 Metrics Task in Machine Translation, where systems and metrics co-evolve.\n3 http://www.statmt.org/wmt20/", "n_publication_ref": 16, "n_figure_ref": 0}, {"heading": "Preliminaries", "text": "In this section we describe the datasets, systems, metrics, and meta evaluation methods used below. -2008, 2009(Dang and Owczarzak, 2008, 2009 are multi-document, multi-reference summarization datasets. Human judgments are available on for the system summaries submitted during the TAC-2008, TAC-2009 shared tasks. CNN/DailyMail (CNNDM) (Hermann et al., 2015;Nallapati et al., 2016) is a commonly used summarization dataset that contains news articles and associated highlights as summaries. We use the version without entities anonymized.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Datasets", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "TAC", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Representative Systems", "text": "We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), Ban-ditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum , HIBERT (Zhang et al., 2019b), Bert-Sum-Ext (Liu and Lapata, 2019a), CNN-Transformer-BiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN;Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), twoStageRL (Zhang et al., 2019a), pre-SummAbs (Liu and Lapata, 2019b), preSummAbsext (Liu and Lapata, 2019b) BART (Lewis et al., 2019) and Semsim (Yoon et al., 2020) as abstractive systems. In total, we use 14 abstractive system outputs for each document in the CNNDM test set.", "n_publication_ref": 23, "n_figure_ref": 0}, {"heading": "Evaluation Metrics", "text": "We examine eight metrics that measure the agreement between two texts, in our case, between the system summary and reference summary. BERTScore (BScore) measures soft overlap between contextual BERT embeddings of tokens between the two texts 4 (Zhang et al., 2020). MoverScore (MScore) applies a distance measure to contextualized BERT and ELMo word embeddings 5 (Zhao et al., 2019). Sentence Mover Similarity (SMS) applies minimum distance matching between text based on sentence embeddings (Clark et al., 2019). Word Mover Similarity (WMS) measures similarity using minimum distance matching between texts which are represented as a bag of word embeddings 6 (Kusner et al., 2015). JS divergence (JS-2) measures Jensen-Shannon divergence between the two text's bigram distributions 7 (Lin et al., 2006). ROUGE-1 and ROUGE-2 measure overlap of unigrams and bigrams respectively 8 (Lin, 2004). ROUGE-L measures overlap of the longest common subsequence between two texts (Lin, 2004). We use the recall variant of all metrics (since the Pyramid method of human evaluations is inherently recall based) except MScore which has no specific recall variant.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Correlation Measures", "text": "Pearson Correlation is a measure of linear correlation between two variables and is popular in metaevaluating metrics at the system level (Lee Rodgers, 1988). We use the implementation given by Virtanen et al. (2020). William's Significance Test is a means of calculating the statistical significance of differences in correlations for dependent variables (Williams, 1959;Graham and Baldwin, 2014). This is useful for us since metrics evaluated on the same dataset are not independent of each other.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Meta Evaluation Strategies", "text": "There are two broad meta-evaluation strategies: summary-level and system-level. Setup: For each document d i , i \u2208 {1 . . . n} in a dataset D, we have J system outputs, where the outputs can come from (1) extractive systems (Ext), (2) abstractive systems (Abs) or (3) a union of both (Mix). Let s ij , j \u2208 {1 . . . J} be the j th summary of the i th document, m i be a specific metric and K be a correlation measure.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Summary Level", "text": "Summary-level correlation is calculated as follows:\nK sum m 1 m 2 = 1 n n i=1 K [m 1 (s i1 ) . . . m 1 (s iJ )], [m 2 (s i1 ) . . . m 2 (s iJ )] .(1)\nHere, correlation is calculated for each document, among the different system outputs of that document, and the mean value is reported.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "System Level", "text": "System-level correlation is calculated as follows:\nK sys m 1 m 2 = K 1 n n i=1 m1(si1) . . . 1 n n i=1 m1(siJ ) , 1 n n i=1 m2(si1) . . . 1 n n i=1 m2(siJ ) .(2)\nAdditionally, the \"quality\" of a system sys j is defined as the mean human score received by it i.e.\nHScore\nsys j mean = 1 n n i=1 humanScore(s ij ). (3)", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Collection of Human Judgments", "text": "We follow a 3-step process to collect human judgments: (1) we collect system-generated summaries on the most-commonly used summarization dataset, CNNDM;\n(2) we select representative test samples from CNNDM and (3) we manually evaluate system-generated summaries of the aboveselected test samples.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "System-Generated Summary Collection", "text": "We collect the system-generated summaries from 25 top-scoring systems, 9 covering 11 extractive and 14 abstractive systems (Sec. 2.2) on the CNNDM dataset. We organize our collected generated summaries into three groups based on system type:\n\u2022 CNNDM Abs denotes collected output summaries from abstractive systems. \u2022 CNNDM Ext denotes collected output summaries from extractive systems. \u2022 CNNDM Mix is the union of the two.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Representative Sample Selection", "text": "Since collecting human annotations is costly, we sample 100 documents from CNNDM test set (11,490 samples) and evaluate system generated summaries of these 100 documents. We aim to include documents of varying difficulties in the representative sample. As a proxy to the difficulty of summarizing a document, we use the mean score received by the system generated summaries for the document. Based on this, we partition the CNNDM test set into 5 equal sized bins and sample 4 documents from each bin. We repeat this process for 5 metrics (BERTScore, MoverScore, R-1, R-2, R-L) obtaining a sample of 100 documents. This methodology is detailed in Alg. 1 in Sec. A.1.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Human Evaluation", "text": "In text summarization, a \"good\" summary should represent as much relevant content from the input document as possible, within the acceptable length limits. Many human evaluation methods have been proposed to capture this desideratum (Nenkova and Passonneau, 2004;Chaganty et al., 2018;Fan et al., 2018;Shapira et al., 2019). Among these, Pyramid (Nenkova and Passonneau, 2004) is a reliable and widely used method, that evaluates content selection by (1) exhaustively obtaining Semantic Content Units (SCUs) from reference summaries, (2) weighting them based on the number of times they are mentioned and (3) scoring a system summary based on which SCUs can be inferred.\nRecently, Shapira et al. (2019) extended Pyramid to a lightweight, crowdsourcable method -LitePyramids, which uses Amazon Mechanical Turk 10 (AMT) for gathering human annotations. LitePyramids simplifies Pyramid by (1) allowing crowd workers to extract a subset of all possible SCUs and (2) eliminating the difficult task of merging duplicate SCUs from different reference summaries, instead using SCU sampling to simulate frequency-based weighting.\nBoth Pyramid and LitePyramid rely on the presence of multiple references per document to assign importance weights to SCUs. However in the CNNDM dataset there is only one reference summary per document. We therefore adapt the LitePyramid method for the single-reference setting as follows. SCU Extraction The LitePyramids annotation instructions define a Semantic Content Unit (SCU) as a sentence containing a single fact written as briefly and clearly as possible. Instead, we focus on shorter, more fine-grained SCUs that contain at most 2-3 entities. This allows for partial content overlap between a generated and reference summary, and also makes the task easy for workers. Tab. 2 gives an example. We exhaustively extract (up to 16) SCUs 11 from each reference summary. Requiring the set of SCUs to be exhaustive increases the complexity of the SCU generation task, and hence instead of relying on crowd-workers, we create SCUs from reference summaries ourselves. In the end, we obtained nearly 10.5 SCUs on average from each reference summary. System Evaluation During system evaluation the full set of SCUs is presented to crowd workers. Workers are paid similar to Shapira et al. (2019), scaling the rates for fewer SCUs and shorter summary texts. For abstractive systems, we pay $0.20 per summary and for extractive systems, we pay $0.15 per summary since extractive summaries are more readable and might precisely overlap with SCUs. We post-process system output summaries before presenting them to annotators by true-casing the text using Stanford CoreNLP (Manning et al., 2014) and replacing \"unknown\" tokens with a special symbol \"2\" (Chaganty et al., 2018).\nTab. 2 depicts an example reference summary, system summary, SCUs extracted from the reference summary, and annotations obtained in evaluating the system summary. Annotation Scoring For robustness (Shapira et al., 2019), each system summary is evaluated by 4 crowd workers. Each worker annotates up to 16 SCUs by marking an SCU \"present\" if it can be (a) Reference Summary: Bayern Munich beat Porto 6 -1 in the Champions League on Tuesday. Pep Guardiola's side progressed 7 -4 on aggregate to reach semi-finals. Thomas Muller scored 27th Champions League goal to pass Mario Gomez. Muller is now the leading German scorer in the competition. After game Muller led the celebrations with supporters using a megaphone.\n(b) System Summary (BART, Lewis et al. (2019)): Bayern Munich beat Porto 6 -1 at the Allianz Arena on Tuesday night. Thomas Muller scored his 27th Champions League goal. The 25 -year -old became the highest -scoring German since the tournament took its current shape in 1992. Bayern players remained on the pitch for some time as they celebrated with supporters.\n(c) SCUs with corresponding evaluations:\n\u2022 Bayern Munich beat Porto.\n\u2022 Bayern Munich won 6 -1.\n\u2022 Bayern Munich won in Champions League. inferred from the system summary or \"not present\" otherwise. We obtain a total of 10,000 human annotations (100 documents \u00d7 25 systems \u00d7 4 workers).\nFor each document, we identify a \"noisy\" worker as one who disagrees with the majority (i.e. marks an SCU as \"present\" when majority thinks \"not present\" or vice-versa), on the largest number of SCUs. We remove the annotations of noisy workers and retain 7,742 annotations of the 10,000. After this filtering, we obtain an average inter-annotator agreement (Krippendorff's alpha (Krippendorff, 2011)) of 0.66. 12 Finally, we use the majority vote to mark the presence of an SCU in a system summary, breaking ties by the class, \"not present\".", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Experiments", "text": "Motivated by the central research question: \"does the rapid progress of model development in summarization models require us to re-evaluate the evaluation process used for text summarization?\" We use the collected human judgments to meta-evaluate current metrics from four diverse viewpoints, measuring the ability of metrics to: (1) evaluate all systems;\n(2) evaluate top-k strongest systems; (3) compare two systems; (4) evaluate individual summaries. We find that many previously attested properties of metrics observed on TAC exhibit different trends on the new CNNDM dataset.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Exp-I: Evaluating All Systems", "text": "Automatic metrics are widely used to determine where a new system may rank against existing state-of-the-art systems. Thus, in meta-evaluation studies, calculating correlation of automatic metrics with human judgments at the system level is a commonly-used setting (Novikova et al., 2017;Bojar et al., 2016;Graham, 2015). We follow this setting and specifically, ask two questions: Can metrics reliably compare different systems? To answer this we observe the Pearson correlation between different metrics and human judgments in Fig. 2, finding that:\n(1) MoverScore and JS-2, which were the best performing metrics on TAC, have poor correlations with humans in comparing CNNDM Ext systems.\n(2) Most metrics have high correlations on the TAC-2008 dataset but many suffer on TAC-2009, especially ROUGE based metrics. However, ROUGE metrics consistently perform well on the collected CNNDM datasets.\nAre some metrics significantly better than others in comparing systems? Since automated metrics calculated on the same data are not independent, we must perform the William's test (Williams, 1959) to establish if the difference in correlations between metrics is statistically significant (Graham and Baldwin, 2014). In Fig. 1 we report the pvalues of William's test. We find that Figure 1: p-value of William's Significance Test for the hypothesis \"Is the system on left (y-axis) significantly better than system on top (x-axis)\". 'BScore' refers to BERTScore and 'MScore' refers to MoverScore. A dark green value in cell (i, j) denotes metric m i has a significantly higher Pearson correlation with human scores compared to metric m j (p-value < 0.05). 13 '-' in cell (i, j) refers to the case when Pearson correlation of m i with human scores is less that of m j (Sec. 4.1). (1) MoverScore and JS-2 are significantly better than other metrics in correlating with human judgments on the TAC datasets.\n(2) However, on CNNDM Abs and CNNDM Mix, R-2 significantly outperforms all others whereas on CNNDM Ext none of the metrics show significant improvements over others.\nTakeaway: These results suggest that metrics run the risk of overfitting to some datasets, highlighting the need to meta-evaluate metrics for modern datasets and systems. Additionally, there is no one-size-fits-all metric that can outperform others on all datasets. This suggests the utility of using different metrics for different datasets to evaluate systems e.g. MoverScore on TAC-2008, JS-2 on TAC-2009 and R-2 on CNNDM datasets.", "n_publication_ref": 5, "n_figure_ref": 3}, {"heading": "Exp-II: Evaluating Top-k Systems", "text": "Most papers that propose a new state-of-the-art system often use automatic metrics as a proxy to human judgments to compare their proposed method against other top scoring systems. However, can metrics reliably quantify the improvements that one high quality system makes over other competitive systems? To answer this, instead of focusing on all of the collected systems, we evaluate the correlation between automatic metrics and human judg-ments in comparing the top-k systems, where top-k are chosen based on a system's mean human score (Eqn. 3). 14 Our observations are presented in Fig. 3. We find that:\n(1) As k becomes smaller, metrics de-correlate with humans on the TAC-2008 and CNNDM Mix datasets, even getting negative correlations for small values of k (Fig. 8a, 8c). Interestingly, SMS, R-1, R-2 and R-L improve in performance as k becomes smaller on CNNDM Ext.\n(2) R-2 had negative correlations with human judgments on TAC-2009 for k < 50, however it remains highly correlated with human judgments on CNNDM Abs for all values of k. Takeaway: Metrics cannot reliably quantify the improvements made by one system over others, especially for the top few systems across all datasets. Some metrics, however, are well suited for specific datasets, e.g. JS-2 and R-2 are reliable indicators of improvements on TAC-2009 and CNNDM Abs respectively.", "n_publication_ref": 1, "n_figure_ref": 2}, {"heading": "Exp-III: Comparing T wo-Systems", "text": "Instead of comparing many systems (Sec. 4.1, 4.2) ranking two systems aims to test the discriminative power of a metric, i.e., the degree to which the metric can capture statistically significant differences between two summarization systems. We analyze the reliability of metrics along a useful dimension: can metrics reliably say if one system is significantly better than another? Since we only have 100 annotated summaries to compare any two systems, sys 1 and sys 2 , we use paired bootstrap resampling, to test with statistical sig- nificance if sys 1 is better than sys 2 according to metric m (Koehn, 2004;Dror et al., 2018). We take all J 2 pairs of systems and compare their mean human score (Eqn. 3) using paired bootstrap resampling. We assign a label y true = 1 if sys 1 is better than sys 2 with 95% confidence, y true = 2 for viceversa and y true = 0 if the confidence is below 95%. We treat this as the ground truth label of the pair (sys 1 , sys 2 ). This process is then repeated for all metrics, to get a \"prediction\", y m pred from each metric m for the same J 2 pairs. If m is a good proxy for human judgments, the F1 score (Goutte and Gaussier, 2005) between y m pred and y true should be high. We calculate the weighted macro F1 score for all metrics and view them in Fig. 4.\nWe find that ROUGE based metrics perform moderately well in this task. R-2 performs the best on CNNDM datasets. While on the TAC 2009 dataset, JS-2 achieves the highest F1 score, its performance is low on CNNDM Ext. Takeaway: Different metrics are better suited for different datasets. For example, on the CNNDM datasets, we recommend using R-2 while, on the TAC datasets, we recommend using JS-2.", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Exp-IV: Evaluating Summaries", "text": "In addition to comparing systems, real-world application scenarios also require metrics to reliably compare multiple summaries of a document. For example, top-scoring reinforcement learning based summarization systems (B\u00f6hm et al., 2019) and the current state-of-the-art extractive system (Zhong et al., 2020) heavily rely on summary-level reward  scores to guide the optimization process.\nIn this experiment, we ask the question: how well do different metrics perform at the summary level, i.e. in comparing system summaries generated from the same document? We use Eq. 1 to calculate Pearson correlation between different metrics and human judgments for different datasets and collected system outputs. Our observations are summarized in Fig. 5. We find that:\n(1) As compared to semantic matching metrics, R-1, R-2 and R-L have lower correlations on the TAC datasets but are strong indicators of good summaries especially for extractive summaries on the CNNDM dataset.\n(2) Notably, BERTScore, WMS, R-1 and R-L have negative correlations on TAC-2009 but perform moderately well on other datasets including CNNDM.\n(3) Previous meta-evaluation studies (Novikova et al., 2017;Peyrard et al., 2017;Chaganty et al., 2018) conclude that automatic metrics tend to correlate well with humans at the system level but have poor correlations at the instance (here summary) level. We find this observation only holds on TAC-2008. Some metrics' summary-level correlations can outperform system-level on the CNNDM dataset as shown in Fig. 7b (bins below y = 0). Notably, MoverScore has a correlation of only 0.05 on CNNDM Ext at the system level but 0.74 at the summary level. Takeaway: Meta-evaluations of metrics on the old TAC datasets show significantly different trends than meta-evaluation on modern systems and datasets. Even though some metrics might be good at comparing summaries, they may point in the wrong direction when comparing systems. Moreover, some metrics show poor generalization ability to different datasets (e.g. BERTScore on TAC-2009 vs other datasets). This highlights the need for empirically testing the efficacy of different automatic metrics in evaluating summaries on multiple datasets.", "n_publication_ref": 5, "n_figure_ref": 2}, {"heading": "Related Work", "text": "This work is connected to the following threads of topics in text summarization. Human Judgment Collection Despite many approaches to the acquisition of human judgment (Chaganty et al., 2018;Nenkova and Passonneau, 2004;Shapira et al., 2019;Fan et al., 2018), Pyramid (Nenkova and Passonneau, 2004) has been a mainstream method to meta-evaluate various automatic metrics. Specifically, Pyramid provides a robust technique for evaluating content selection by exhaustively obtaining a set of Semantic Content Units (SCUs) from a set of references, and then scoring system summaries on how many SCUs can be inferred from them. Recently, Shapira et al. (2019) proposed a lightweight and crowdsourceable version of the original Pyramid, and demonstrated it on the DUC 2005 (Dang, 2005) and 2006 (Dang, 2006) multi-document summarization datasets. In this paper, our human evaluation methodology is based on the Pyramid (Nenkova and Passonneau, 2004) and LitePyramids (Shapira et al., 2019) techniques. Chaganty et al. ( 2018) also obtain human evaluations on system summaries on the CNNDM dataset, but with a focus on language quality of summaries. In comparison, our work is focused on evaluating content selection. Our work also covers more systems than their study (11 extractive + 14 abstractive vs. 4 abstractive).", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Meta-evaluation with Human Judgment", "text": "The effectiveness of different automatic metrics -ROUGE-2 (Lin, 2004), ROUGE-L (Lin, 2004), ROUGE-WE (Ng and Abrecht, 2015), JS-2 (Louis and Nenkova, 2013) and S3 (Peyrard et al., 2017) is commonly evaluated based on their correlation with human judgments (e.g., on the TAC-2008 (Dang and Owczarzak, 2008) and TAC-2009 (Dang andOwczarzak, 2009) datasets). As an important supplementary technique to metaevaluation, Graham (2015) advocate for the use of a significance test, William's test (Williams, 1959), to measure the improved correlations of a metric with human scores and show that the popular variant of ROUGE (mean ROUGE-2 score) is sub-optimal. Unlike these works, instead of proposing a new metric, in this paper, we upgrade the meta-evaluation environment by introducing a sizeable human judgment dataset evaluating current top-scoring systems and mainstream datasets. And then, we re-evaluate diverse metrics at both systemlevel and summary-level settings. (Novikova et al., 2017) also analyzes existing metrics, but they only focus on dialog generation.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Implications and Future Directions", "text": "Our work not only diagnoses the limitations of current metrics but also highlights the importance of upgrading the existing meta-evaluation testbed, keeping it up-to-date with the rapid development of systems and datasets. In closing, we highlight some potential future directions: (1) The choice of metrics depends not only on different tasks (e.g, summarization, translation) but also on different datasets (e.g., TAC, CNNDM) and application scenarios (e.g, system-level, summary-level). Future works on meta-evaluation should investigate the effect of these settings on the performance of metrics.\n(2) Metrics easily overfit on limited datasets. Multidataset meta-evaluation can help us better understand each metric's peculiarity, therefore achieving a better choice of metrics under diverse scenarios. (3) Our collected human judgments can be used as supervision to instantiate the most recentlyproposed pretrain-then-finetune framework (originally for machine translation) (Sellam et al., 2020), learning a robust metric for text summarization.  ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A Appendices", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "We sincerely thank all authors of the systems that we used in this work for sharing their systems' outputs.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Association for Computational Linguistics", "journal": "", "year": "2016", "authors": "Ond\u0159ej Bojar; Yvette Graham; Amir Kamran; Milo\u0161 Stanojevi\u0107"}, {"title": "Ido Dagan, and Iryna Gurevych. 2019. Better rewards yield better summaries: Learning to summarise without references", "journal": "", "year": "", "authors": "Florian B\u00f6hm; Yang Gao; Christian M Meyer; Ori Shapira"}, {"title": "The price of debiasing automatic metrics in natural language evaluation", "journal": "", "year": "2018", "authors": "Stephen Arun Tejasvi Chaganty; Percy Mussman;  Liang"}, {"title": "Fast abstractive summarization with reinforce-selected sentence rewriting", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yen-Chun Chen; Mohit Bansal"}, {"title": "Sentence mover's similarity: Automatic evaluation for multi-sentence texts", "journal": "", "year": "2019", "authors": "Elizabeth Clark; Asli Celikyilmaz; Noah A Smith"}, {"title": "Overview of the tac 2008 update summarization task", "journal": "TAC", "year": "2008", "authors": "Hoa Dang; Karolina Owczarzak"}, {"title": "Overview of the tac 2009 summarization track", "journal": "TAC", "year": "2009", "authors": "Hoa Dang; Karolina Owczarzak"}, {"title": "Overview of duc", "journal": "HLT/EMNLP", "year": "2005", "authors": "Hoa Trang Dang"}, {"title": "Overview of duc", "journal": "", "year": "2006", "authors": "Hoa Trang Dang"}, {"title": "Unified language model pre-training for natural language understanding and generation", "journal": "", "year": "2019", "authors": "Li Dong; Nan Yang; Wenhui Wang; Furu Wei; Xiaodong Liu; Yu Wang; Jianfeng Gao; Ming Zhou; Hsiao-Wuen Hon"}, {"title": "Bandit-Sum: Extractive summarization as a contextual bandit", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yue Dong; Yikang Shen; Eric Crawford; Jackie Chi Kit Herke Van Hoof;  Cheung"}, {"title": "The hitchhiker's guide to testing statistical significance in natural language processing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Rotem Dror; Gili Baumer; Segev Shlomov; Roi Reichart"}, {"title": "Controllable abstractive summarization", "journal": "", "year": "2018", "authors": "Angela Fan; David Grangier; Michael Auli"}, {"title": "Bottom-up abstractive summarization", "journal": "", "year": "2018", "authors": "Sebastian Gehrmann; Yuntian Deng; Alexander Rush"}, {"title": "A probabilistic interpretation of precision, recall and f-score, with implication for evaluation", "journal": "Springer", "year": "2005", "authors": "Cyril Goutte; Eric Gaussier"}, {"title": "Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE", "journal": "", "year": "2015", "authors": "Yvette Graham"}, {"title": "Testing for significance of increased correlation with human judgment", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Yvette Graham; Timothy Baldwin"}, {"title": "Teaching machines to read and comprehend", "journal": "", "year": "2015", "authors": "Karl Moritz Hermann; Tomas Kocisky; Edward Grefenstette; Lasse Espeholt; Will Kay; Mustafa Suleyman; Phil Blunsom"}, {"title": "Content selection in deep learning models of summarization", "journal": "", "year": "2018", "authors": "Chris Kedzie; Kathleen Mckeown; Hal Daume; Iii "}, {"title": "Statistical significance tests for machine translation evaluation", "journal": "", "year": "2004", "authors": "Philipp Koehn"}, {"title": "Computing krippendorff's alpha-reliability", "journal": "", "year": "2011", "authors": "Klaus Krippendorff"}, {"title": "From word embeddings to document distances", "journal": "", "year": "2015", "authors": "Matt Kusner; Yu Sun; Nicholas Kolkin; Kilian Weinberger"}, {"title": "Thirteen ways to look at the correlation coefficient. The American Statistician", "journal": "", "year": "1988", "authors": "W Alan Lee Rodgers"}, {"title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2019", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Ves Stoyanov; Luke Zettlemoyer"}, {"title": "Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out", "journal": "", "year": "2004", "authors": "Chin-Yew Lin"}, {"title": "An information-theoretic approach to automatic evaluation of summaries", "journal": "", "year": "2006", "authors": "Chin-Yew Lin; Guihong Cao; Jianfeng Gao; Jian-Yun Nie"}, {"title": "ORANGE: a method for evaluating automatic evaluation metrics for machine translation", "journal": "", "year": "2004", "authors": "Chin-Yew Lin; Franz Josef Och"}, {"title": "Text summarization with pretrained encoders", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Yang Liu; Mirella Lapata"}, {"title": "Text summarization with pretrained encoders", "journal": "", "year": "2019", "authors": "Yang Liu; Mirella Lapata"}, {"title": "Automatically assessing machine summary content without a gold standard", "journal": "Computational Linguistics", "year": "2013", "authors": "Annie Louis; Ani Nenkova"}, {"title": "The Stanford CoreNLP natural language processing toolkit", "journal": "", "year": "2014", "authors": "Christopher D Manning; Mihai Surdeanu; John Bauer; Jenny Finkel; Steven J Bethard; David Mc-Closky"}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "journal": "", "year": "2016", "authors": "Ramesh Nallapati; Bowen Zhou;  Cicero Dos Santos; Bing Glar Gul\u00e7ehre;  Xiang"}, {"title": "Ranking sentences for extractive summarization with reinforcement learning", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Shashi Narayan; Shay B Cohen; Mirella Lapata"}, {"title": "Evaluating content selection in summarization: The pyramid method", "journal": "", "year": "2004", "authors": "Ani Nenkova; Rebecca Passonneau"}, {"title": "Better summarization evaluation with word embeddings for ROUGE", "journal": "Association for Computational Linguistics", "year": "1925", "authors": "Jun-Ping Ng; Viktoria Abrecht"}, {"title": "Why we need new evaluation metrics for NLG", "journal": "", "year": "2017", "authors": "Jekaterina Novikova; Ond\u0159ej Du\u0161ek; Amanda Cercas Curry; Verena Rieser"}, {"title": "Studying summarization evaluation metrics in the appropriate scoring range", "journal": "", "year": "2019", "authors": "Maxime Peyrard"}, {"title": "Learning to score system summaries for better content selection evaluation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Maxime Peyrard; Teresa Botschen; Iryna Gurevych"}, {"title": "Exploring the limits of transfer learning with a unified text-to", "journal": "", "year": "2019", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"title": "A decade of automatic content evaluation of news summaries: Reassessing the state of the art", "journal": "Short Papers", "year": "2013", "authors": "A Peter; John M Rankel; Hoa Trang Conroy; Ani Dang;  Nenkova"}, {"title": "Get to the point: Summarization with pointergenerator networks", "journal": "Long Papers", "year": "2017", "authors": "Abigail See; J Peter; Christopher D Liu;  Manning"}, {"title": "Bleurt: Learning robust metrics for text generation", "journal": "", "year": "2020", "authors": "Thibault Sellam; Dipanjan Das; Ankur P Parikh"}, {"title": "Crowdsourcing lightweight pyramids for manual summary evaluation", "journal": "Long and Short Papers", "year": "2019", "authors": "Ori Shapira; David Gabay; Yang Gao; Hadar Ronen; Ramakanth Pasunuru; Mohit Bansal"}, {"title": "", "journal": "Nature Methods", "year": "", "authors": "Pauli Virtanen; Ralf Gommers; Travis E Oliphant; Matt Haberland; Tyler Reddy; David Cournapeau; Evgeni Burovski; Pearu Peterson; Warren Weckesser; Jonathan Bright; J St\u00e9fan; Matthew Van Der Walt; Joshua Brett; K Jarrod Wilson; Nikolay Millman;  Mayorov; R J Andrew; Eric Nelson; Robert Jones; Eric Kern;  Larson; \u0130lhan Carey; Yu Polat; Eric W Feng; Jake Moore; Denis Vand Erplas; Josef Laxalde; Robert Perktold; Ian Cimrman; E A Henriksen;  Quintero; R Charles; Anne M Harris; Ant\u00f4nio H Archibald; Fabian Ribeiro;  Pedregosa"}, {"title": "Xipeng Qiu, and Xuanjing Huang. 2020. Heterogeneous graph neural networks for extractive document summarization", "journal": "", "year": "", "authors": "Danqing Wang; Pengfei Liu; Yining Zheng"}, {"title": "Regression analysis", "journal": "Wiley", "year": "1959", "authors": "Evan J Williams"}, {"title": "Learning by semantic similarity makes abstractive summarization better", "journal": "", "year": "2020", "authors": "Wonjin Yoon; Yoon Sun Yeo; Minbyul Jeong; Bong-Jun Yi; Jaewoo Kang"}, {"title": "Pretraining-based natural language generation for text summarization", "journal": "", "year": "2019", "authors": "Haoyu Zhang; Yeyun Gong; Yu Yan; Nan Duan; Jianjun Xu; Ji Wang; Ming Gong; Ming Zhou"}, {"title": "Bertscore: Evaluating text generation with bert", "journal": "", "year": "2020", "authors": "Tianyi Zhang; Varsha Kishore; Felix Wu; Kilian Q Weinberger; Yoav Artzi"}, {"title": "Neural latent extractive document summarization", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Xingxing Zhang; Mirella Lapata; Furu Wei; Ming Zhou"}, {"title": "HIBERT: Document level pre-training of hierarchical bidirectional transformers for document summarization", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Xingxing Zhang; Furu Wei; Ming Zhou"}, {"title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance", "journal": "", "year": "2019", "authors": "Wei Zhao; Maxime Peyrard; Fei Liu; Yang Gao; Christian M Meyer; Steffen Eger"}, {"title": "Xipeng Qiu, and Xuanjing Huang. 2020. Extractive summarization as text matching", "journal": "", "year": "", "authors": "Ming Zhong; Pengfei Liu; Yiran Chen; Danqing Wang"}, {"title": "Searching for effective neural extractive summarization: What works and what's next", "journal": "", "year": "2019", "authors": "Ming Zhong; Pengfei Liu; Danqing Wang; Xipeng Qiu; Xuan-Jing Huang"}, {"title": "Neural storyline extraction model for storyline generation from news articles", "journal": "Long Papers", "year": "2018", "authors": "Deyu Zhou; Linsen Guo; Yulan He"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "\u2022Bayern Munich won on Tuesday. \u2022 Bayern Munich is managed by Pep Guardiola. \u00d7 \u2022 Bayern Munich progressed in the competition. \u2022 Bayern Munich reached semi-finals. \u00d7 \u2022 Bayern Munich progressed 7 -4 on aggregate. \u00d7 \u2022 Thomas Muller scored 27th Champions League goal. \u2022 Thomas Muller passed Mario Gomez in goals. \u00d7 \u2022 Thomas Muller is now the leading German scorer in the competition. \u2022 After the game Thomas Muller led the celebrations. \u00d7 \u2022 Thomas Muller led the celebrations using a megaphone. \u00d7 Table 2: Example of a summary and corresponding annotation. (a) shows a reference summary from the representative sample of the CNNDM test set. (b) shows the corresponding system summary generated by BART, one of the abstractive systems used in the study. (c) shows the SCUs (Semantic Content Units) extracted from (a) and the \"Present( )\"/\"Not Present(\u00d7)\" marked by crowd workers when evaluating (b).", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: System-level Pearson correlation between metrics and human scores (Sec. 4.1).", "figure_data": ""}, {"figure_label": "34", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 3 :Figure 4 :34Figure 3: System-level Pearson correlation with humans on top-k systems (Sec. 4.2).", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": "-2 R-L WMS SMS JS-2 BERTScore MoverScore (a) Summary-level Pearson correlation with human scores. Difference between system-level and summary-level Pearson correlation.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Pearson correlation between metrics and human judgments across different datasets (Sec. 4.4).", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: System-level Kendall correlation between metrics and human scores.", "figure_data": ""}, {"figure_label": "78", "figure_type": "", "figure_id": "fig_7", "figure_caption": "Figure 7 :Figure 8 :78Figure 7: Kendall correlation between metrics and human judgements across different datasets.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Summary of our experiments, observations on existing human judgments on the TAC, and contrasting observations on newly obtained human judgments on the CNNDM dataset. Please refer to Sec. 4 for more details.", "figure_data": ""}], "doi": "10.18653/v1/W16-2302"}