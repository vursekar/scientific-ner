{"authors": "Hui Su; Xiaoyu Shen; Zhou Xiao; Zheng Zhang; Ernie Chang; Cheng Zhang; Cheng Niu; Jie Zhou; Forrest Gump", "pub_date": "", "title": "MovieChats: Chat like Humans in a Closed Domain", "abstract": "Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can ever be claimed. In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of moviedomain chatbots. We propose a unified, readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval. The model is first pretrained on the huge general-domain data, then finetuned on our corpus. We show this simple neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules. On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones. We further analyze the limits of our work and point out potential directions for future work 1 .", "sections": [{"heading": "Introduction", "text": "Being able to converse like humans in a closed domain is a precondition before an intelligent opendomain chatbot, which further requires transiting among various domains, can be designed Su et al., 2020). Nonetheless, even if constrained in a specific domain, current chatbots are still far from satisfactory. Unlike task-oriented systems that can be relatively well-resolved with handcrafted templates, human conversations feature a complex mixture of QA, chitchat, recommendation, etc. without pre-specified goals or conversational patterns (Dodge et al., 2016;Akasaki and Kaji, 2017;. Selecting proper domain knowledge to support response generation at all the different situations is challenging (Milward and Beveridge, 2003;Shen et al., 2019). In this work, we direct our focus to the movie domain and present a large-scale, crowdsourced Chinese dataset with fine-grained annotations in hope of boosting the study towards a human-like closed-domain chatbot.\nA variety of dialogue datasets with grounded domain knowledge have already been proposed. However, they are collected either through (1) online forum crawling (Dodge et al., 2016;Ghazvininejad et al., 2018;Liu et al., 2018;Zhou et al., 2018a;, which are noisy, multi-party, mostly contain only single-exchange QA, or (2) crowdsourced (Zhu et al., 2017;Zhou et al., 2018b;Moon et al., 2019;, which are small-scale and often created in an overconstrained setting like teacher-student (Moghe et al., 2018). Even for datasets crowd-sourced in unconstrained scenarios, suggestive domain knowledge is provided for humans before an utterance is provided. This would inevitably prompt humans to utilize these knowledge deliberately, yielding unnatural conversations simply connecting the knowledge (Dinan et al., 2019;. We show examples from other datasets in Appendix Table 10. In comparison, our dataset has the following advantages:\n1. Natural: Crowdworkers chat in a free environment without further constraint or prompt in order to mimic the human daily conversations to the largest extent.\n2. Large-scale: It covers 270k human dialogues with over 3M utterances, which is at least one order of magnitude larger than all the other crowd-sourced datasets.\n3. Annotated: Utterances are labeled with entity information and dialogue acts classified into 15 fine-grained aspects, based on which linked into different types of knowledge.\nDifferent from previous crowd-sourced works, our annotation process is conducted posteriori so that it will not interfere with human conversations, e.g., prompt them to overuse suggested knowledge.\nBuilt upon our dataset, we propose a simple unified language model approach to push the limits of movie-domain chatbots. The model is first pretrained on 2.2B words collected from various general-domain conversational resources, then finetuned on the movie dataset with additional knowledge and dialogue acts incorporated. We pool all components like intent prediction and knowledge retrieval into a sequence prediction task and solve them with a unified language model architecture. It avoids designing complex systems for individual components separately and all subtasks can be easily trained simultaneously (Hosseini-Asl et al., 2020;Peng et al., 2020). We show our simple unified approach outperforms strong baselines for each separate subtask. Knowledge retrieval, dialogue acts prediction and general-domain pretrain benefit from each other and altogether bring improvement to the generation quality. In the online interactive test, our best model succeeds at chatting with humans for 11.4 turns without being detected to be a machine, outperforming even commercial chatbots Mitsuku 2 and Microsoft XiaoIce 3 which further rely on complex rules. By analyzing the limitations of our model, we find it especially has difficulty at dealing with in-depth discussions over long turns. Future research can consider employing larger knowledge base or explicit state tracking.\nIn summary, our main contributions are (1) presenting a high-quality, large-scale Chinese conversational corpus with fine-grained annotations in the movie domain to benefit future study, (2) showing that a simple unified neural model trained on the high-quality dataset can approach human performance and even outperform commercial systems replying on complex rules, and (3) studying the shortcomings of current techniques, providing suggestive directions for future research.", "n_publication_ref": 18, "n_figure_ref": 0}, {"heading": "Dataset Construction", "text": "The dataset construction consist of (1) crowdsourcing the dialogues, (2) annotating dialog acts and entities and ( 3) linking utterances into grounded knowledge. We explain these three steps in order and present the dataset statistics in the end.\nDialogue Crowd-sourcing We obtain the dialogue dataset through a two-phase Wizard-of-Ozstyle collection (Kelley, 1984;Dahlb\u00e4ck et al., 1993). In the first phase, we run small-scale pilot studies and examine the quality of collected conversations. Based on the examination, we created tutorials and qualification tests. They are used to train and qualify crowd-workers for the second phase. During this second phase, we consistently monitor the collected dialogue datasets and perform periodic quality check on samples from every individual work pairs. If more than 5% from one pair are considered invalid, their collections will be removed. Before a conversation started, two workers are paired and a movie is chosen agreed by both 4 . We constrain at least one of them to have watched the movie to make sure the conversation is contentful 5 . The annotators are especially instructed to (1) behave naturally as in daily life, (2) avoid dirty words and (3) talk differently in each conversation. Duplicate conversations will be removed if more than 70% of their contents are overlapped. To encourage diverse movies, we further set an upper limit to forbid one movie from being talked about for more than 100 times.\nThe whole collecting process lasts two months. In the end, 245 participants are involved with 66,424 movies being talked about in total.\nDialogue Act and Entity Annotation Following prior work, we base our annotation schema on the ISO 24617-2 standard (Bunt et al., 2010(Bunt et al., , 2012. Table 1 shows our annotation schema, counts, descriptions, and brief examples. The dialogue acts (DAs) are organized in a hierarchical structure. The first layer makes distinctions on three concepts: objective facts, recommendations and subjective feelings. Each concept can either be either requested or informed during the conversation. We further define an \"Other\" class to include actions that do not belong to any of the three concepts, like some general non-contentful greetings or echos. The second layer includes 15 finer-grained aspects covering most popular topics being discussed. Every first-layer DA (except Other) will be further group it into one of these 15 aspects, e.g., the de- tailed DA of the first example in Table 1 will be request fact director. If one utterance contains multiple dialogue acts, we order the dialogue acts based on their turn of appearance in the utterance. As for the named entity recognition, we labeled 5 kinds of entities: movie names, director, actor, type and role (first 5 aspects).\nTo speed up the annotation process, we first define a set of handcrafted regular expressions, which covers most frequent patterns at each class, to train a DA and NER classifier . The annotators are instructed to post-correct the auto-labeled dialogues instead of doing everything from scratch. The classifiers are trained with online learning (Sahoo et al., 2018) to keep improving the accuracy and lower down the frequency of post-correction in consequence. As we observe, this semi-automated way significantly speeds up the labeling process. All the dataset is finished labeling within three weeks with 188 annotators involved.\nKnowledge Linkage We extract fact knowledge from the structured table in Douban Movie 6 , a popular Chinese platform for movies. The knowledge is organized in the form of key-value pairs, where the key corresponds to the 15 aspects defined by us. Some aspects, like lines or music, are not directly available from the structured table. We extract these missing information from other sources and combine it into our knowledge base. For utterances labeld as inform/request fact, we link them to the key-value pairs from the same aspect. Apart from the objective knowledge, we also crawl movie comments from Douban Movie to support the generation of responses expressing subjective feelings. These comments can be a good supplementary to provide knowledge that can be hardly organized in the structured form (Moghe et al., 2018). For utterances labeled as inform/request feeling, we compare them with Douban comments from the same movie and compute the similarity score based on weighted average of edit distance, Jaccard distance, tf-idf, sentence vector cosine similarity, common words and entities. Each utterance is linked to the most similar comment with a threshold cutoff. In the end, 51.7% of the utterances about feelings have grounded comments. For utterances about recommendations, we simply ground them to the men-  tioned movie entities 7 , and no grounded knowledge is linked for utterances labeled as Other. An example of our annotation is presented in Table 1.  ", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Model Architecture", "text": "Language models have demonstrated impressive performance as a universal learner across NLP tasks (Shen et al., 2017;Peters et al., 2018;Radford et al., 2019;Brown et al., 2020). Inspired by this, our dialogue generation model is implemented as a Transformer-based language model like GPT2 (Radford et al., 2019;. It contains a pipeline process of movie tracker, intent prediction, knowledge retrieval and text gener- 7 We only consider recommending movies as for the DA about recommendation. Recommending other aspects require assembling recommendation systems of different domains, which is beyond the scope of this paper.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Context", "text": "[context] dialogue context  ation. Unlike in traditional task-oriented systems where subtasks are decomposed separately, we opt for a simple and unified approach by casting all subtasks into sequence prediction. A special token is injected in the beginning to indicate which subtask to perform (Hosseini-Asl et al., 2020;Peng et al., 2020). Table 3 shows the schema representation for different components. The condition and the target are concatenated into a single sequence and then fed into the language model to train. For example, the task of predicting the intent given the dialogue context will be transformed into \" ", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Intent Prediction", "text": "The intent prediction is also cast as a sequence prediction task. Compared with the traditional way of multi-label classification, casting it as sequence prediction is better at addressing the coexistence of multiple DAs and capturing the sequential dependencies among the hierarchy (Raffel et al., 2019;Vedula et al., 2020). For example, to predict the DAs of the 4th utterance in Figure 1, the sequence fed to the language model will be \"[context] dialogue context [intent] inform, feeling, plot, request, fact, plot\". By this means, before predicting a DA, the model can condition on both the dialogue context and its previous DAs to improve the accuracy.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Knowledge Retrieval", "text": "The knowledge retrieval component is similar to the classical DSSM model (Huang et al., 2013). We replace the MLP with our language model encoder to get the embedding for knowledge. Note that we only select knowledge from the current movie, which can be obtained from the movie tracker, so it is possible to  \" will be fed to the language model to generate the response. To make it consistent with the pretrained general-domain dialogue, the position embedding of the decoded response will skip the concatenated intent and knowledge and directly follow the dialogue context. We find this beneficial when combined with pretrained models. The objective also follows the pretrained model mixing maximum lilkelihood and unlikelihood training.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experiment Setting", "text": "We tokenize the Text in the unit of Chinese characters and keep all unique non-Chinese unique tokens appearing for more than 5 times. The whole vocabulary contains 13,317 words. We train our model on 24 Nvidia V100 GPUs (32GB) with three different model sizes as shown in  PyTorch (Paszke et al., 2019).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Results and Analysis", "text": "Automatic Evaluation In Table 5, we report the perplexity, BLEU scores and distinct uni/bigrams for three model sizes. To investigate the effects of incorporating annotations and pretraining, we start from a basic model which trains from scratch on our movie corpus. At each time, we add one more condition to see its influence. The results show a clear tendency of gradual improvement as more conditions are added to the training. Adding knowledge especially boosts the performance, which is understandable considering movie-domain chats usually contain many movie-specific rare names.\nWithout knowledge grounding, it can hardly predict the correct tokens. Pretraining on general-domain conversations can improve both the overlap with ground truth. The distinct uni/bigrams also consistently increase, implying the model can learn useful patterns in the pretrained corpus to enrich its generations in the movie domain. In unseen testset, the performance generally drops for all, especially for models without knowledge grounding as they have to make up facts and comments for totally unseen movies in the training set. Table 6 measures the accuracy of predicting dialogue act (DA), aspect and movie tracker of our model. Our models are all pretrained with general-domain corpus beforehand. Apart from being trained only to predict the individual tasks, we include the results where all subtasks are cotrained end-to-end in the last line. We compare our models with the Chinese RoBERTa (Liu et    Table 7 measures the performance of retrieving fact knowledge, movie comments and recommen-dation respectively. We report the hit@1 and hit@5 scores for them (Zhang et al., 2018). We compare our model with a random baseline, bag-ofword (BOW) and the Bert (Devlin et al., 2019) model (we pass sentences through Bert and derive a fixed-sized vector by averaging the outputs from the second-to-last layer (May et al., 2019)). The BOW and Bert model are finetuned with our knowledge linkage annotations. We find that our unified model again outperforms all baseline approaches. Adding the DA as a condition further helps. Fact retrieval has the highest hit rate as it is well structured and easy to match. Recommendation, on the other hand, is very hard to predict. As an accurate recommendation system is clearly beyond the scope of this paper, it is understandable that our simple way fails to provide satisfying recommendations.  Human Evaluation Automatically evaluating dialogue systems are known to be extremely hard (Liu et al., 2016;. We further conduct a set of static and interactive human evaluations. We focus on evaluate the machine-generated response from four perspectives. Apart from the oft-used metrics ( 1) Sensibleness (Sens) and ( 2) Engagement (Enga) for open-domain chatbots, we further evaluate on ( 3) Factuality (Fact) and ( 4) Informativeness (Info) to see if models can actively provide informative responses based on movie facts. Details are in Appendix B. As evaluating factuality requires specific movie knowledge, this metric is only evaluated by the same person who produced the dialogue. The other metrics are evaluated by 3 workers each. Table 8 shows the agreement scores. The agreement is reasonable considering the evaluations are subjective. The results are the majority votes of the binary scores.\nIn the static evaluation, we sample 300 responses for each model from the test set (mixing seen and unseen). The responses can come from any turn in a conversation. We show the results in Figure 2. Our largest model with 762M is clearly preferred by human evaluators on almost all metrics and approaches human performance. By training a larger model and increasing the training size, the gap might be further closed.\nIn the interactive evaluation, humans can chat with any topic but restricted in the movie domain. We conduct an online Turing test where one side is always a human participant not aware whom he is talking with. The other side could be either Mitsuku, XiaoIce 13 , our model (762M with pretraining) or a real human. Mitsuku interacts in English, so we hire only English native speakers for the experiment. We collect 100 conversations for all models. Humans can stop interacting once they (1) find the other side is a machine or (2) reaches the maximum turn of 20. Responses from all models are later passed to the third party to judge the scores. The results are shown on the right of Figure 2. Our model outperforms Mitsuku and XiaoIce by a large margin. As Mitsuku and XiaoIce are designed to be open-domain chatbots, restricting to be on the movie domain will give our model some natural advantage. We can also notice that Mitsuku and XiaoIce almost never produce fake facts. The cost is the extremely low ratio of informative responses since they tend to behave over-safely and will only answer it when they are 100% sure. Our model is closer to humans in that sense. It will converse actively at some risk of containing fact errors. Figure 3: Change of SEA and FIA as the turn proceeds. 13 We use its chat service through Weibo. It will sometimes generate responses containing keywords like \"XiaoIce\". We manually replace it to prevent disclosing its identity.  Distance from Human Performance In the interactive evaluation, compared with human performance, our model loses a bit on sensibleness and factuality but wins on the other two. To investigate where our model fails, figure 3 visualizes the change of SSA (Sensibleness-Engagement average) and FIA (Factuality-Informativeness average) when the conversational turn proceeds. A good chatbot should balance well these skills (Adiwardana et al., 2020). SEA can reflect how it behaves as a general chatbot while FIA can better test its capability at incorporating domain knowledge. We can see a clear trend of decrease for all models. As for human performance, however, the score is quite consistent across turn rounds, implying a large improvement space for current models to deal with multi-turn context.\nIn figure 4, we further show the \"dying distribution\" of our model, namely, in which DA our model fails to pass the Turing test and thereby \"dies\". Unsurprisingly, we can see the system fails mostly when informing facts or feelings. Only a small portion are from non-grounded chitchats (other). This suggests the most crucial bottleneck lies in the interaction with movie-specific knowledge and seamlessly incorporating it into the response generation. We show some snippets of interactions with our model in Table 9. The first two are failing cases labeled by humans as not factual and sensible. We can see the model struggles at replying to too specific facts. This is understandable since our knowledge base only provide short introductions and cannot cover all what happened in the movie. The second case shows its shortcoming at handing long-range consistency. It still recommends the current movie when the user asks about \"which other movie\". Employing larger knowledge bases and explicitly tracking the states by a checklist (Kiddon et al., 2016) might potentially alleviate both issue. We also provide examples for controllable generations where the DA and aspect are manually assigned. As observed, the model shows decent performance at fitting both the dialogue con-text and specified conditions. This can be helpful when finer-grained control is needed.", "n_publication_ref": 8, "n_figure_ref": 5}, {"heading": "Conclusion", "text": "We present MovieChats: a movie-domain chatbot built upon a large-scale, high-quality conversational corpus with fine-grained annotations. The model can be trained end-to-end with a simple unified language model architecture. We show that our model, powered by well-defined knowledge grounding, is able to approach human performance in some perspective, though still lagged behind when it comes to dealing with detailed knowledge or long-turn consistency. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ours", "text": "A: \u5a01\u5c14\u53f2\u5bc6\u65af\u6f14\u6280\u771f\u7684\u5f88\u68d2 (Will Smith's acting skill is really good). B: \u4ed6\u7684\u5f53\u5e78\u798f\u6765\u6572\u95e8\u592a\u7ecf\u5178\u4e86 (His The Pursuit of Happyness is a classic). A: \u4e00\u76f4\u90fd\u6302\u5728\u7535\u5f71\u6392\u884c\u699c\u9760\u524d\u7684\u4f4d\u7f6e (That' always among top ranked movies). B: \u55ef\u55ef\uff0c\u8fd9\u90e8\u7535\u5f71\u771f\u7684\u5f88\u52b1\u5fd7\u554a (Yes, it's really motivational). A: \u5a01\u5c14\u53f2\u5bc6\u65af\u4e5f\u6f14\u51fa\u4e86\u5f88\u60e8\u7684\u611f\u89c9\u4e86 (Will Smith plays like he is a real tragedy). B: \u6f14\u6280\u7279\u522b\u597d (Yes, he acts pretty well). ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Dataset Collection", "text": "Table 10 shows examples comparing our dataset and the others. As observed, forum conversations are mostly single-turn QA or comments. Current crowd-sourced datasets are either collected on constrained scenarios (the scenario in (Zhou et al., 2018b) fixed the roles in a conversation as one introducer and one listener), or unconstrained but prompting people to deliberately connect knowledge. Our dataset simulates real-life conversations to the largest extent.\nWe classify the utterances into one of 15 aspects.  ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "B Human Evaluation", "text": "As for the four human evaluation metrics. The first two will focus only on the conversational backbones without considering domain knowledge. The second two will check if the responses can provide informative and correct responses powered by domain knowledge. The detailed definitions of them are:", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledge", "text": "We thank anonymous reviewers and the dialogue system team at Wechat AI for their valuable comments. Xiaoyu Shen was funded by IMPRS-CS fellowship. Ernie Chang is supported by SFB 248 \"Foundations of Perspicuous Software Systems\" (E2).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "", "text": "All the three metrics are evaluated by three crow-workers each except for factuality. As evaluating factuality requires in-depth knowledge about one movie, it is quite difficult for random human evaluators to judge them. Even if we filter to only keep people who have watched a movie, it is hard to guarantee they can recall all the scenes in the movie. Therefore, the factuality check is only done by the person who performed this dialogue. In the static evaluation, it is evaluated by the annotator who produced the reference response. In the interactive evaluation, it is evaluated by the person who chat with the bot. It is nevertheless not accurate though. However, if the bot can cheat the human into believing its false information, it can also somehow be considered a \"success\". We provide examples for guiding the human evaluators in Table 12.\nTable 13 shows some interactive examples with humans. We observe Mitsuku can XiaoIce perform decently in single-turn exchanges but strongly struggle at understanding multi-turn user intents. Most conversations stop at turn 4 and will not move on.   ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Towards a human-like open-domain chatbot", "journal": "", "year": "2020", "authors": "Daniel Adiwardana; Minh-Thang Luong; David R So; Jamie Hall; Noah Fiedel; Romal Thoppilan; Zi Yang; Apoorv Kulshreshtha; Gaurav Nemade; Yifeng Lu"}, {"title": "Chat detection in an intelligent assistant: Combining taskoriented and non-task-oriented spoken dialogue systems", "journal": "", "year": "2017", "authors": "Satoshi Akasaki; Nobuhiro Kaji"}, {"title": "", "journal": "", "year": "", "authors": "Benjamin Tom B Brown; Nick Mann; Melanie Ryder; Jared Subbiah; Prafulla Kaplan; Arvind Dhariwal; Pranav Neelakantan; Girish Shyam;  Sastry"}, {"title": "Towards an iso standard for dialogue act annotation", "journal": "", "year": "2010", "authors": "Harry Bunt; Jan Alexandersson; Jean Carletta; Jae-Woong Choe; Alex Chengyu Fang; Koiti Hasida; Kiyong Lee; Volha Petukhova; Andrei Popescu-Belis; Laurent Romary"}, {"title": "Iso 24617-2: A semantically-based standard for dialogue annotation", "journal": "", "year": "2012", "authors": "Harry Bunt; Jan Alexandersson; Jae-Woong Choe; Alex Chengyu Fang; Koiti Hasida; Volha Petukhova; Andrei Popescu-Belis; David R Traum"}, {"title": "Pre-training with whole word masking for chinese bert", "journal": "", "year": "2019", "authors": "Yiming Cui; Wanxiang Che; Ting Liu; Bing Qin; Ziqing Yang; Shijin Wang; Guoping Hu"}, {"title": "Wizard of oz studies: why and how", "journal": "", "year": "1993", "authors": "Nils Dahlb\u00e4ck; Arne J\u00f6nsson; Lars Ahrenberg"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Wizard of wikipedia: Knowledge-powered conversational agents", "journal": "", "year": "2019", "authors": "Emily Dinan; Stephen Roller; Kurt Shuster; Angela Fan; Michael Auli; Jason Weston"}, {"title": "Evaluating prerequisite qualities for learning end-to-end dialog systems", "journal": "", "year": "2016", "authors": "Jesse Dodge; Andreea Gane; Xiang Zhang; Antoine Bordes; Sumit Chopra; Alexander Miller; Arthur Szlam; Jason Weston"}, {"title": "Neural approaches to conversational ai. Foundations and Trends R in Information Retrieval", "journal": "", "year": "2019", "authors": "Jianfeng Gao; Michel Galley; Lihong Li"}, {"title": "A knowledge-grounded neural conversation model", "journal": "", "year": "2018", "authors": "Marjan Ghazvininejad; Chris Brockett; Ming-Wei Chang; Bill Dolan; Jianfeng Gao; Yih Wen-Tau; Michel Galley"}, {"title": "Negative training for neural dialogue response generation", "journal": "", "year": "2019", "authors": "Tianxing He; James Glass"}, {"title": "Deep neural network approach for the dialog state tracking challenge", "journal": "", "year": "2013", "authors": "Matthew Henderson; Blaise Thomson; Steve Young"}, {"title": "The curious case of neural text degeneration", "journal": "", "year": "2020", "authors": "Ari Holtzman; Jan Buys; Li Du; Maxwell Forbes; Yejin Choi"}, {"title": "Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue", "journal": "", "year": "", "authors": "Ehsan Hosseini-Asl; Bryan Mccann; Chien-Sheng Wu"}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "journal": "", "year": "2013", "authors": "Po-Sen Huang; Xiaodong He; Jianfeng Gao; Li Deng; Alex Acero; Larry Heck"}, {"title": "An iterative design methodology for user-friendly natural language office information applications", "journal": "", "year": "1984", "authors": "F John;  Kelley"}, {"title": "Globally coherent text generation with neural checklist models", "journal": "", "year": "2016", "authors": "Chlo\u00e9 Kiddon; Luke Zettlemoyer; Yejin Choi"}, {"title": "Don't say that! making inconsistent dialogue unlikely with unlikelihood training", "journal": "", "year": "2019", "authors": "Margaret Li; Stephen Roller; Ilia Kulikov; Sean Welleck; Y-Lan Boureau; Kyunghyun Cho; Jason Weston"}, {"title": "Learning to select knowledge for response generation in dialog systems", "journal": "AAAI Press", "year": "2019", "authors": "Rongzhong Lian; Min Xie; Fan Wang; Jinhua Peng; Hua Wu"}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "journal": "", "year": "2016", "authors": "Chia-Wei Liu; Ryan Lowe; Iulian Serban; Mike Noseworthy; Laurent Charlin; Joelle Pineau"}, {"title": "Knowledge diffusion for neural dialogue generation", "journal": "", "year": "2018", "authors": "Shuman Liu; Hongshen Chen; Zhaochun Ren; Yang Feng; Qun Liu; Dawei Yin"}, {"title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "On measuring social biases in sentence encoders", "journal": "", "year": "2019", "authors": "Chandler May; Alex Wang; Shikha Bordia; Samuel Bowman; Rachel Rudinger"}, {"title": "Ontologybased dialogue systems", "journal": "", "year": "2003", "authors": "David Milward; Martin Beveridge"}, {"title": "Towards exploiting background knowledge for building conversation systems", "journal": "", "year": "2018", "authors": "Nikita Moghe; Siddhartha Arora; Suman Banerjee; Mitesh M Khapra"}, {"title": "Opendialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs", "journal": "", "year": "2019", "authors": "Seungwhan Moon; Pararth Shah; Anuj Kumar; Rajen Subba"}, {"title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga"}, {"title": "Shahin Shayandeh, Lars Liden, and Jianfeng Gao. 2020. Soloist: Few-shot task-oriented dialog with a single pretrained auto-regressive model", "journal": "", "year": "", "authors": "Baolin Peng; Chunyuan Li; Jinchao Li"}, {"title": "Deep contextualized word representations", "journal": "", "year": "2018", "authors": "E Matthew; Mark Peters; Mohit Neumann; Matt Iyyer; Christopher Gardner; Kenton Clark; Luke Lee;  Zettlemoyer"}, {"title": "Conversing by reading: Contentful neural conversation with on-demand machine reading", "journal": "", "year": "2019", "authors": "Lianhui Qin; Michel Galley; Chris Brockett; Xiaodong Liu; Xiang Gao; Bill Dolan; Yejin Choi; Jianfeng Gao"}, {"title": "Language models are unsupervised multitask learners", "journal": "OpenAI Blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "", "year": "2019", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"title": "Online deep learning: learning deep neural networks on the fly", "journal": "", "year": "2018", "authors": "Doyen Sahoo; Quang Pham; Jing Lu; C H Steven;  Hoi"}, {"title": "Estimation of gap between current language models and human performance", "journal": "", "year": "2017", "authors": "Xiaoyu Shen; Youssef Oualil; Clayton Greenberg; Mittul Singh; Dietrich Klakow"}, {"title": "Nexus network: Connecting the preceding and the following in dialogue generation", "journal": "", "year": "2018", "authors": "Xiaoyu Shen; Hui Su; Wenjie Li; Dietrich Klakow"}, {"title": "Select and attend: Towards controllable content selection in text generation", "journal": "", "year": "2019", "authors": "Xiaoyu Shen; Jun Suzuki; Kentaro Inui; Hui Su; Dietrich Klakow; Satoshi Sekine"}, {"title": "Dialogue generation with gan", "journal": "", "year": "2018", "authors": "Hui Su; Xiaoyu Shen; Pengwei Hu; Wenjie Li; Yun Chen"}, {"title": "Diversifying dialogue generation with non-conversational text", "journal": "", "year": "2020", "authors": "Hui Su; Xiaoyu Shen; Sanqiang Zhao; Zhou Xiao; Pengwei Hu; Randy Zhong; Cheng Niu; Jie Zhou"}, {"title": "Annual Meeting of the Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"title": "Open intent extraction from natural language interactions", "journal": "", "year": "2020", "authors": "Nikhita Vedula; Nedim Lipka; Pranav Maneriker; Srinivasan Parthasarathy"}, {"title": "Proactive human-machine conversation with explicit conversation goal", "journal": "", "year": "2019", "authors": "Wenquan Wu; Zhen Guo; Xiangyang Zhou; Hua Wu; Xiyuan Zhang; Rongzhong Lian; Haifeng Wang"}, {"title": "Personalizing dialogue agents: I have a dog, do you have pets too?", "journal": "Long Papers", "year": "2018", "authors": "Saizheng Zhang; Emily Dinan; Jack Urbanek; Arthur Szlam; Douwe Kiela; Jason Weston"}, {"title": "Dialogpt: Large-scale generative pre-training for conversational response generation", "journal": "", "year": "2019", "authors": "Yizhe Zhang; Siqi Sun; Michel Galley; Yen-Chun Chen; Chris Brockett; Xiang Gao; Jianfeng Gao; Jingjing Liu; Bill Dolan"}, {"title": "Commonsense knowledge aware conversation generation with graph attention", "journal": "AAAI Press", "year": "2018", "authors": "Hao Zhou; Tom Young; Minlie Huang; Haizhou Zhao; Jingfang Xu; Xiaoyan Zhu"}, {"title": "Kdconv: A chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation", "journal": "ACL", "year": "2020", "authors": "Hao Zhou; Chujie Zheng; Kaili Huang; Minlie Huang; Xiaoyan Zhu"}, {"title": "A dataset for document grounded conversations", "journal": "", "year": "2018", "authors": "Kangyan Zhou; Shrimai Prabhumoye; Alan W Black"}, {"title": "Flexible end-to-end dialogue system for knowledge grounded conversation", "journal": "", "year": "2017", "authors": "Wenya Zhu; Kaixiang Mo; Yu Zhang; Zhangbin Zhu; Xuezheng Peng; Qiang Yang"}, {"title": "Who directed the movie Inception?\", the bot replies \"Steven Spielberg! I like him so much!!\" The answer is wrong but still considered sensible given the context. 2. Engagement: If the response is engaging for the user to follow. The definition is generally similar to diversity, or interestingness metric used for open-domain chitchat. If the bot simply passively replies messages like \"That's good\" or \"No idea", "journal": "", "year": "", "authors": ""}, {"title": "Factuality: Factuality checks the information correctness in case the model fabricate wrong facts. This is orthogonal to the above metrics. The response can be factual but not sensible or the other way around. The same for Engagement", "journal": "", "year": "", "authors": ""}, {"title": "That's so interesting, I've never heard of that!", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An example of our dataset. The annotations include dialogue act, aspects and movie tracker and grounded knowledge.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2: Results of static/interactive human evaluation.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Dying distribution of DAs in interactive evaluation.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "(Any inspirational movies? Thanks everyone). B: \u300a\u5f53\u5e78\u798f\u6765\u6572\u95e8\u300b (The Pursuit of Happyness). Crowd-sourced (Constrained) Zhou et al. (2018b) A: Hey have you seen the inception? B: No, I have not but have heard of it. What is it about ? A: It's about extractors that perform experiments using. . . Crowd-sourced (unconstrained) Zhou et al. (2020) A: \u77e5\u9053\u91cd\u5e86\u68ee\u6797\u8fd9\u4e2a\u7535\u5f71\u5417 (Do you know the movie Chungking Express)? B: \u77e5\u9053\u554a\uff0c\u662f\u738b\u5bb6\u536b\u5bfc\u6f14\u7684 (Yes, it's directed by Wong Kar-Wai). A: \u5177\u4f53\u662f\u54ea\u5e74\u4e0a\u6620\u7684\u4f60\u8fd8\u8bb0\u5f97\u5417\uff1f (Remember which year it was on)? B: \u8bb0\u5f97\uff0c\u662f\u57281994\u5e7407\u670814\u65e5 (Yes, July 14th, 1994). . .", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Counts, type of linked knowledge, descriptions and examples of the dialogue acts. Examples are shown only in translated English for space limit. Each dialogue act (except other) is paired with one of the 15 aspects. Details descriptions and statistics of the aspects are in Appendix Table11.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "A schematic representation of the different components. Upper parts are the conditions and lower parts are the targets. The condition and target are concatenated into a single sequence then fed into the language model.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "[context]  dialogue context [intent] DA sequence\", where the DA sequence will be predicted conditioned on\"[context] dialogue context [intent]\".General-domain PretrainThe model is first pretrained on a mixed general-domain conversational corpus crawled from various sources like douban 8 , tieba 9 , zhihu 10 and weibo 11 . The pretrained corpus covers 468M conversations with 2.2B words. Each training instance is processed in the form of \"[context] dialogue context [response] response\" where response will be predicted given \"[context] dialogue context [response]\". The objective is a mixture of maximum likelihood and unlikelihood training(He and Glass, 2019;, which we find help reduce repeated and incoherent generations as observed inAdiwardana et al. (2020). The unlikelihood training minimizes the likelihood of 1) randomly sampled responses from the corpus and 2) repeated bigrams from the previous generated tokens.", "figure_data": "Movie Tracker The movie tracker is like the be-lief state tracker in task-oriented systems (Hender-son et al."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Architecture hyperparameters for the 3 model sizes.run exact softmax over all knowledge candidates and maximize the likelihood of the ground truth. We condition on both the dialogue context and the intent. The sequence fed to the language model would be \"[context] dialogue context [intent] DA sequence [retrieve] knowledge\" where knowledge will be predicted. When an utterance is not linked with any knowledge, it will predict a None token.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ". The batch"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "The perplexity, BLEU score and distinct uni/bi-grams of our model with different sizes. Results are reported for seen/unseen movies respectively. Each line adds one more condition on top of the above one.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Accuracy of dialogue act, aspect and movie tracker.", "figure_data": "2019), which has achieved SOTA performance onChinese NLU tasks. We use its optimized versionfor Chinese which is pretrained by masking fullChinese words (Cui et al., 2019) 12 . RoBERTadoes the movie tracking by performing softmaxover the recognized named entities. For dialogueacts, we deal with DA1 (inform/request/other) andDA2(fact/feeling/recommend) separately. As canbe observed, our simple unified approach for allsubtasks does not come at the expense of the accu-racy. When all subtasks are co-trained, the perfor-mance is further boosted, indicating the multitasktraining paradigm can benefit individual tasks.ModelFact Comment Hit@1 Hit@1Recommend Hit@1/5Random0.0540.011 0.000 0.000BOW0.1350.048 0.011 0.014Bert0.4060.067 0.048 0.075117M0.4020.053 0.045 0.071345M0.4550.071 0.047 0.072762M0.5180.120 0.053 0.086+ DA0.5430.158 0.066 0.104"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Hit rates of knowledge retrieval.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Crowd worker agreement scores.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "MovieChats[inform feeling role] [\u53d8\u5f62\u91d1\u521a\u91cc\u5927\u9ec4\u8702\u6700\u53ef\u7231] \u662f\u554a\u5927\u9ec4\u8702\u771f\u7684\u592a\u592a\u592a\u53ef\u7231\u4e86\u53ed(Yes, Bumblebee is sooo cute) User Input \u53ef\u4e0d\u561b\u5077\u5077\u628a\u8f66\u505c\u5728\u597d\u98ce\u666f\u7684\u5730\u65b9\u5e2e\u5c71\u59c6\u64a9\u59b9(Exactly, stopped the car in a nice view and helped Sam hit on girls) \u54c8\u5229\u6ce2\u7279. . . ] \u54c8\u5229\u6ce2\u7279\u662f\u8c01\u6f14\u7684\u5440(Who played the role of Harry Potter)", "figure_data": "1User Input\u53d8\u5f62\u91d1\u521a\u770b\u4e86\u6ca1\uff0c\u8d85\u9177\u7684\u5462(Have you watched Transformers? So cool)MovieChats[Others] [None] \u5f53\u7136\u770b\u8fc7\u5566(Sure)User Input\u6211\u6700\u559c\u6b22\u5927\u9ec4\u8702\u4e86(I like Bumblebee so much)MovieChats[inform feeling plot] [None] \u54c8\u54c8\uff0c\u9738\u5929\u864e\u5f53\u65f6\u4e5f\u5728(Haha, Decepticons were there, too.)[factuality=0]2User Input\u4e09\u50bb\u5927\u95f9\u5b9d\u83b1\u575e\u628a\u6211\u770b\u54ed\u4e86(I was moved into tears by 3 idiots)MovieChats[inform feeling others] [\u611f\u4eba\u7684\u5f71\u7247\uff0c\u770b\u4e86\u5f88\u591a\u904d] \u6211\u4e5f\u662f\uff0c\u770b\u4e86\u4e00\u767e\u904d(Me too, have watched it one hundred times)User Input\u7537\u4e3b\u662f\u771f\u7684\u5f88\u806a\u660e(The leading actor is really smart.)MovieChats[inform feeling role] [. . . \u4e3b\u89d2\u6562\u4e8e\u5bf9\u4f20\u7edf\u6559\u80b2\u8bf4\u4e0d. . . ] \u5f88\u559c\u6b22\u7537\u4e3b\uff0c\u4f20\u7edf\u6559\u80b2\u7cca\u5f04\u4e0d\u4e86\u4ed6(I like him very much. Heis not fooled by the traditional educational system.)User Input\u6240\u4ee5\u5b66\u6821\u6559\u80b2\u6a21\u5f0f\u771f\u7684\u5f88\u91cd\u8981(So the educational pattern at school is very important)MovieChats[inform feeling others] [None] \u793e\u4f1a\u8981\u8fdb\u6b65\uff0c\u6559\u80b2\u5c31\u8981\u4e0d\u65ad\u9769\u65b0(Education must keep going for social progress)User Input\u7537\u4e3b\u662f\u8c01\u6f14\u7684(Who plays the actor in the movie)MovieChats[inform fact actor] [actor: \u963f\u7c73\u5c14\u2022\u6c57. . . ] \u963f\u7c73\u5c14\u2022\u6c57\uff0c\u975e\u5e38\u51fa\u540d\u7684(Aamir Khan, very famous)User Input\u4ed6\u8fd8\u6f14\u8fc7\u4ec0\u4e48\u7535\u5f71(Which other movie did he play)MovieChats[inform fact name] [name: \u4e09\u50bb\u5927\u95f9\u5b9d\u83b1\u575e] \u4e09\u50bb\u5927\u95f9\u5b9d\u83b1\u575e\uff0c\u4e5f\u5f88\u597d\u770b(3 idiots, also a nice movie)[sensibleness=0]3User Input\u6211\u6700\u8fd1\u5728\u91cd\u6e29\u54c8\u5229\u6ce2\u7279\u7cfb\u5217(I'm reviewing Happy Potter series recently.)MovieChats[others] [None] \u4f60\u559c\u6b22\u770b\u5417(Do you like it)MovieChats[inform feeling others] [\u7b2c\u4e00\u90e8\u597d\u770b\u554a ] \u7b2c\u4e00\u90e8\u662f\u6211\u6700\u559c\u6b22\u7684(The first part is my favorite)MovieChats[inform feeling role] [\u5c0f\u54c8\u5229\u989c\u503c\u592a\u9ad8\u4e86] \u559c\u6b22\u5c0f\u54c8\u5229\uff0c\u90a3\u65f6\u5019\u989c\u503c\u592a\u9ad8\u4e86(I like little Harry, he was so cute that time)MovieChats[request fact role] [role:"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Examples of interactions with our model. First two are failing cases from the interactive evaluation. Last one shows how the response can be controlled by feeding different DA and aspects.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Comparison between our dataset and others.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "The definitions, counts, and examples of them are shown in Table 11. When annotating the corpus, tutorials and examples are provided to the annotators, we show some examples of in the following tables. All the examples are provided only in Chinese as that is the native language among annotators. Release time of a movie. I remember it was in 1997, still at high school.", "figure_data": "AspectsCount(%) DescriptionExample\u7535\u5f71\u540d(name)17.41 Name of a movie.Its name is so funny\u5bfc\u6f14(director)2.44 Director of a movie.Who played Jack in the movie?\u6f14\u5458(actor)9.24 Actor of a movie.I wonder who picked him as the leading actor.\u7c7b\u578b(type)7.12 Type of a movie.I don't really like horror movies.\u89d2\u8272(role)10.01 Role of a movie.I like Bumblebee, he is cute.\u533a\u57df(region)2.95 Region of a movie.Is it a Japanese movie?\u4e0a\u6620\u65f6\u95f4(time) 2.02 \u5267\u60c5(plot) 24.30 Plot of a movie.What happened to him?\u53f0\u8bcd(line)1.51 Line of a movie.That line becomes a popular word that year.\u5956\u9879(awards)1.17 Award of a movie.It won a lot of awards that year.\u7968\u623f(gross)1.00 Gross of a movie.I doubt if its gross will be satisfying.\u8bc4\u5206(rating)4.04 Rating of a movie.It's clearly the best mivie I've watched.\u8d44\u6e90(website)1.25 Website of a movie.You can watch it on iqiyi.com!\u97f3\u4e50(music)2.19 Music in the movie.Its theme song is \"my heart will go on\".\u5176\u4ed6(other)12.31 Other aspects.I like her costume so much!"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Counts, descriptions and examples of the aspects. Examples are shown only in translated English for space limit.", "figure_data": ""}], "doi": "10.18653/v1/2020.acl-main.634"}