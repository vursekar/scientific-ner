{"authors": "Ahmet\u00fcst\u00fcn Arianna; Bisazza Gosse; Bouma Gertjan Van Noord; Ria Basmov; John Bauer; Sandra Bellato; Kepa Bengoetxea; Yevgeni Berzak; Ahmad Bhat; Adriane Boyd; Aljoscha Burchardt; Marie Can- Dito; Bernard Caron; Gauthier Caron; Cebiroglu Eryigit; Massimiliano Flavio; Giuseppe G A Cecchini;  Celano; Savas Slavom\u00edr\u010d\u00e9pl\u00f6; Fabricio Cetin; Jinho Chalub; Yongseok Choi; Jayeol Cho; Silvie Chun; Aur\u00e9lie Cinkov\u00e1; Agr\u0131 Collomb; Miriam \u00c7\u00f6ltekin; Marine Connor; Elizabeth Courtin; Marie-Catherine Davidson;  De Marneffe; Peter Dickerson; Kaja Dirix; Tim- Othy Dobrovoljc; Kira Dozat; Puneet Droganova; Marhaba Dwivedi; Ali Eli; Binyam Elkahky; Toma\u017e Ephrem; Aline Erjavec; Rich\u00e1rd Etienne; Hector Farkas; Jennifer Fernandez Alcalde; Fre- Itas Foster; Katar\u00edna Gajdo\u0161ov\u00e1; Daniel Galbraith; Mar- Cos Garcia; Moa G\u00e4rdenfors; Sebastian Garza; Kim Gerdes; Filip Ginter; Iakes Goenaga; Koldo Gojenola; Memduh G\u00f6k\u0131rmak; Yoav Goldberg; Xavier G\u00f3mez Guinovart; Berta Gonz\u00e1les; Matias Grioni; Normunds Gr\u016bz\u012btis; Bruno Guillaume; C\u00e9line Guillot-Barbance; Nizar Habash; Jan Haji\u010d; Linh H\u00e0 M\u1ef9; Na-Rae Han; Kim Harris; Dag Haug; Barbora Hladk\u00e1; Jaroslava Hlav\u00e1\u010dov\u00e1; Florinel Hociung; Petter Hohle; Jena Hwang; Radu Ion; Elena Irimia; O L\u00e1j\u00edd\u00e9 Ishola; Tom\u00e1\u0161 Jel\u00ednek; Anders Johannsen; Fredrik J\u00f8rgensen; H\u00fcner Ka\u015f\u0131kara; Sylvain Kahane; Hi- Roshi Kanayama; Jenna Kanerva; Boris Katz; Tolga Kayadelen; Jessica Kenney; V\u00e1clava Kettnerov\u00e1; Jesse Kirchner; Kamil Kopacewicz; Natalia Kot- Syba; Simon Krek; Sookyoung Kwak; Veronika Laippala; Lorenzo Lambertino; Lucia Lam", "pub_date": "", "title": "UDapter: Language Adaptation for Truly Universal Dependency Parsing", "abstract": "Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, crosslanguage interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and lowresource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success. 1   ", "sections": [{"heading": "Introduction", "text": "Monolingual training of a dependency parser has been successful when relatively large treebanks are available (Kiperwasser and Goldberg, 2016;Dozat and Manning, 2017). However, for many languages, treebanks are either too small or unavailable. Therefore, multilingual models leveraging Universal Dependency annotations (Nivre et al., 2018) have drawn serious attention (Zhang and Barzilay, 2015;Ammar et al., 2016;Kondratyuk and Straka, 2019). Multilingual approaches learn generalizations across languages and share information between them, making it possible to parse a target language without supervision in that language. Moreover, multilingual models can be faster to train and easier to maintain than a large set of monolingual models.\nHowever, scaling a multilingual model over a high number of languages can lead to sub-optimal results, especially if the training languages are typologically diverse. Often, multilingual neural models have been found to outperform their monolingual counterparts on low-and zero-resource languages due to positive transfer effects, but underperform for high-resource languages (Johnson et al., 2017;Arivazhagan et al., 2019;Conneau et al., 2020), a problem also known as \"the curse of multilinguality\". Generally speaking, a multilingual model without language-specific supervision is likely to suffer from over-generalization and perform poorly on high-resource languages due to limited capacity compared to the monolingual baselines, as verified by our experiments on parsing.\nIn this paper, we strike a good balance between maximum sharing and language-specific capacity in multilingual dependency parsing. Inspired by recently introduced parameter sharing techniques (Platanios et al., 2018;Houlsby et al., 2019), we propose a new multilingual parser, UDapter, that learns to modify its language-specific parameters including the adapter modules, as a function of language embeddings. This allows the model to share parameters across languages, ensuring generalization and transfer ability, but also enables language-specific parameterization in a single multilingual model. Furthermore, we propose not to learn language embeddings from scratch, but to leverage a mix of linguistically curated and predicted typological features as obtained from the URIEL language typology database  which supports 3718 languages including all languages represented in UD. While the importance of typological features for cross-lingual parsing is known for both non-neural (Naseem et al., 2012;Zhang and Barzilay, 2015) and neural approaches (Ammar et al., 2016;Scholivet et al., 2019), we are the first to use them effectively as direct input to a neural parser, without manual selection, over a large number of languages in the context of zero-shot parsing where gold POS labels are not given at test time. In our model, typological features are crucial, leading to a substantial LAS increase on zero-shot languages and no loss on high-resource languages when compared to the language embeddings learned from scratch.\nWe train and test our model on the 13 syntactically diverse high-resource languages that were used by Kulmizev et al. (2019), and also evaluate it on 30 genuinely low-resource languages. Results show that UDapter significantly outperforms stateof-the-art monolingual (Straka, 2018) and multilingual (Kondratyuk and Straka, 2019) parsers on most high-resource languages and achieves overall promising improvements on zero-shot languages.\nContributions We conduct several experiments on a large set of languages and perform thorough analyses of our model. Accordingly, we make the following contributions: 1) We apply the idea of adapter tuning (Rebuffi et al., 2018;Houlsby et al., 2019) to the task of universal dependency parsing.\n2) We combine adapters with the idea of contextual parameter generation (Platanios et al., 2018), leading to a novel language adaptation approach with state-of-the art UD parsing results. 3) We provide a simple but effective method for conditioning the language adaptation on existing typological language features, which we show is crucial for zero-shot performance.", "n_publication_ref": 21, "n_figure_ref": 0}, {"heading": "Previous Work", "text": "This section presents the background of our approach.\nMultilingual Neural Networks Early models in multilingual neural machine translation (NMT) designed dedicated architectures (Dong et al., 2015;Firat et al., 2016) whilst subsequent models, from Johnson et al. (2017) onward, added a simple language identifier to the models with the same architecture as their monolingual counterparts. More recently, multilingual NMT models have focused on maximizing transfer accuracy for low-resource language pairs, while preserving high-resource language accuracy (Platanios et al., 2018;Neubig and Hu, 2018;Aharoni et al., 2019;Arivazhagan et al., 2019), known as the (positive) transfer -(negative) interference trade-off. Another line of work builds massively multilingual pre-trained language mod-els to produce contextual representation to be used in downstream tasks (Devlin et al., 2019;Conneau et al., 2020). As the leading model, multilingual BERT (mBERT) 2 (Devlin et al., 2019) which is a deep self-attention network, was trained without language-specific signals on the 104 languages with the largest Wikipedias. It uses a shared vocabulary of 110K WordPieces (Wu et al., 2016), and has been shown to facilitate cross-lingual transfer in several applications (Pires et al., 2019;Wu and Dredze, 2019). Concurrently to our work, Pfeiffer et al. (2020) have proposed to combine language and task adapters, small bottleneck layers (Rebuffi et al., 2018;Houlsby et al., 2019), to address the capacity issue which limits multilingual pre-trained models for cross-lingual transfer.\nCross-Lingual Dependency Parsing The availability of consistent dependency treebanks in many languages Nivre et al., 2018) has provided an opportunity for the study of cross-lingual parsing. Early studies trained a delexicalized parser (Zeman and Resnik, 2008; on one or more source languages by using either gold or predicted POS labels (Tiedemann, 2015) and applied it to target languages. Building on this, later work used additional features such as typological language properties (Naseem et al., 2012), syntactic embeddings (Duong et al., 2015), and cross-lingual word clusters (T\u00e4ckstr\u00f6m et al., 2012). Among lexicalized approaches, Vilares et al. (2016) learns a bilingual parser on a corpora obtained by merging harmonized treebanks. Ammar et al. (2016) trains a multilingual parser using multilingual word embeddings, token-level language information, language typology features and fine-grained POS tags. More recently, based on mBERT (Devlin et al., 2019), zero-shot transfer in dependency parsing was investigated (Wu and Dredze, 2019;Tran and Bisazza, 2019). Finally Kondratyuk and Straka (2019) trained a multilingual parser on the concatenation of all available UD treebanks.\nLanguage Embeddings and Typology Conditioning a multilingual model on the input language is studied in NMT (Ha et al., 2016;Johnson et al., 2017), syntactic parsing (Ammar et al., 2016) and language modeling (\u00d6stling and Tiedemann, 2017). The goal is to embed language information in real-valued vectors in order to enrich internal representations with input language for multilingual models. In dependency parsing, several previous studies (Naseem et al., 2012;Zhang and Barzilay, 2015;Ammar et al., 2016;Scholivet et al., 2019) have suggested that typological features are useful for the selective sharing of transfer information. Results, however, are mixed and often limited to a handful of manually selected features (Fisch et al., 2019;Ponti et al., 2019). As the most similar work to ours, Ammar et al. (2016) uses typological features to learn language embeddings as part of training, by augmenting each input token and parsing action representation. Unfortunately though, this technique is found to underperform the simple use of randomly initialized language embeddings ('language IDs'). Authors also reported that language embeddings hurt the performance of the parser in zero-shot experiments (Ammar et al., 2016, footnote 30). Our work instead demonstrates that typological features can be very effective if used with the right adaptation strategy in both supervised and zero-shot settings. Finally, Lin et al. (2019) use typological features, along with properties of the training data, to choose optimal transfer languages for various tasks, including UD parsing, in a hard manner. By contrast, we focus on a soft parameter sharing approach to maximize generalizations within a single universal model.", "n_publication_ref": 41, "n_figure_ref": 0}, {"heading": "Proposed Model", "text": "In this section, we present our truly universal dependency parser, UDapter. UDapter consists of a biaffine attention layer stacked on top of the pretrained Transformer encoder (mBERT). This is similar to (Wu and Dredze, 2019;Kondratyuk and Straka, 2019), except that our mBERT layers are interleaved with special adapter layers inspired by Houlsby et al. (2019). While mBERT weights are frozen, biaffine attention and adapter layer weights are generated by a contextual parameter generator (Platanios et al., 2018) that takes a language embedding as input and is updated while training on the treebanks.\nNote that the proposed adaptation approach is not restricted to dependency parsing and is in principle applicable to a range of multilingual NLP tasks. We will now describe the components of our model.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Biaffine Attention Parser", "text": "The top layer of UDapter is a graph-based biaffine attention parser proposed by Dozat and Manning (2017). In this model, an encoder generates an internal representation r i for each word; the decoder takes r i and passes it through separate feedforward layers (MLP), and finally uses deep biaffine attention to score arcs connecting a head and a tail:\nh (head) i = MLP (head) (r i ) (1) h (tail) i = MLP (tail) (r i )(2)\ns (arc) = Biaffine(H (head) , H (tail) )(3)\nSimilarly, label scores are calculated by using a biaffine classifier over two separate feedforward layers. Finally, the Chu-Liu/Edmonds algorithm (Chu, 1965;Edmonds, 1967) is used to find the highest scoring valid dependency tree.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Transformer Encoder with Adapters", "text": "To obtain contextualized word representations, UDapter uses mBERT. For a token i in sentence S, BERT builds an input representation w i composed by summing a WordPiece embedding x i (Wu et al., 2016) and a position embedding f i . Each w i \u2208 S is then passed to a stacked self-attention layers (SA) to generate the final encoder representation r i :\nw i = x i + f i (4) r i = SA (w i ; \u0398 (ad) ) (5\n)\nwhere \u0398 (ad) denotes the adapter modules. During training, instead of fine-tuning the whole encoder network together with the task-specific top layer, we use adapter modules (Rebuffi et al., 2018;Stickland and Murray, 2019;Houlsby et al., 2019), or simply adapters, to capture both task-specific and language-specific information. Adapters are small modules added between layers of a pre-trained network. In adapter tuning, the weights of the original network are kept frozen, whilst the adapters are trained for a downstream task. Tuning with adapters was mainly suggested for parameter efficiency but they also act as an information module for the task or the language to be adapted (Pfeiffer et al., 2020). In this way, the original network serves as a memory for the language(s). In UDapter, following Houlsby et al. (2019), two bottleneck adapters with two feedforward projections and a GELU nonlinearity (Hendrycks and Gimpel, 2016) are inserted into each transformer layer as shown in Figure 1. We apply adapter tuning for two reasons: 1) Each adapter module consists of only few parameters and allows to use contextual parameter generation (CPG; see \u00a7 3.3) with a reasonable number of trainable parameters. 3 2) Adapters enable taskspecific as well as language-specific adaptation via CPG since it keeps backbone multilingual representations as memory for all languages in pre-training, which is important for multilingual transfer.\nF 1 1 0 0 1 0 0 1 1 0 1 0 1 1 Biaffine Attention (for", "n_publication_ref": 7, "n_figure_ref": 1}, {"heading": "Contextual Parameter Generator", "text": "To control the amount of sharing across languages, we generate trainable parameters of the model using a contextual parameter generator (CPG) function inspired by Platanios et al. (2018). CPG enables UDapter to retain high multilingual quality without losing performance on a single language, during multi-language training. We define CPG as a function of language embeddings. Since we only train adapters and the biaffine attention (i.e. adapter tuning), the parameter generator is formalized as {\u03b8 (ad) , \u03b8 (bf ) } g (m) (l e ) where g (m) denotes the parameter generator with language embedding l e , and \u03b8 (ad) and \u03b8 (bf ) denote the parameters of adapters and biaffine attention respectively. We implement CPG as a simple linear transform of a language embedding, similar to Platanios et al. (2018), so that weights of adapters in the encoder and biaffine attention are generated by the dot product of language embeddings:\ng (m) (l e ) = (W (ad) , W (bf) ) \u2022 l e (6)\n3 Due to CPG, the number of adapter parameters is multiplied by language embedding size, resulting in a larger model compared to the baseline (more details in Appendix A.1).\nwhere l e \u2208 R M , W (ad) \u2208 R P (ad) \u00d7M , W (bf) \u2208 R P (bf) \u00d7M , M is the language embedding size, P (ad) and P (bf) are the number of parameters for adapters and biaffine attention respectively. 4 An important advantage of CPG is the easy integration of existing task or language features.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Typology-Based Language Embeddings", "text": "Soft sharing via CPG enables our model to modify its parsing decisions depending on a language embedding. While this allows UDapter to perform well on the languages in training, even if they are typologically diverse, information sharing is still a problem for languages not seen during training (zero-shot learning) as a language embedding is not available. Inspired by Naseem et al. (2012) and Ammar et al. (2016), we address this problem by defining language embeddings as a function of a large set of language typological features, including syntactic and phonological features. We use a multi-layer perceptron MLP (lang) with two feedforward layers and a ReLU nonlinear activation to compute a language embedding l e :\nl e = MLP (lang) (l t ) (7)\nwhere l t is a typological feature vector for a language consisting of all 103 syntactic, 28 phonological and 158 phonetic inventory features from the URIEL language typology database     (Lewis et al., 2015) and Glottolog (Hammarstr\u00f6m et al., 2020). As many feature values are not available for each language, we use the values predicted by  using a k-nearest neighbors approach based on average of genetic, geographical and feature distances between languages. For the encoder, we use BERT-multilingualcased together with its WordPiece tokenizer. Since dependency annotations are between words, we pass the BERT output corresponding to the first wordpiece per word to the biaffine parser. We apply the same hyper-parameter settings as Kondratyuk and Straka (2019). Additionally, we use 256 and 32 for adapter size and language embedding size respectively. In our approach, pre-trained BERT weights are frozen, and only adapters and biaffine attention are trained, thus we use the same learning rate for the whole network by applying an inverse square root learning rate decay with linear warmup (Howard and Ruder, 2018). Appendix A.1 gives the hyper-parameter details.\nBaselines We compare UDapter to the current state of the art in UD parsing: [1] UUparser+BERT (Kulmizev et al., 2019), a graph-based BLSTM parser (de Lhoneux et al., 2017;Smith et al., 2018) using mBERT embeddings as additional features.\n[2] UDpipe (Straka, 2018), a monolingually trained multi-task parser that uses pretrained word embeddings and character representations. [3] UDify (Kondratyuk and Straka, 2019), the mBERTbased multi-task UD parser on which our UDapter is based, but originally trained on all language treebanks from UD. UDPipe scores are taken from Kondratyuk and Straka (2019).\nTo enable a direct comparison, we also re-train UDify on our set of 13 high-resource languages both monolingually (one treebank at a time; monoudify) and multilingually (on the concatenation of languages; multi-udify). Finally, we evaluate two variants of our model: 1) Adapter-only has only task-specific adapter modules and no languagespecific adaptation, i.e. no contextual parameter generator; and 2) UDapter-proxy is trained without typology features: a separate language embedding is learnt from scratch for each in-training language, and for low-resource languages we use one from the same language family, if available, as proxy representation.\nImportantly, all baselines are either trained for a single language, or multilingually without any language-specific adaptation. By comparing UDapter to these parsers, we highlight its unique character that enables language specific parameterization by typological features within a multilingual framework for both supervised and zero-shot learning setup.", "n_publication_ref": 13, "n_figure_ref": 0}, {"heading": "Results", "text": "Overall, UDapter outperforms the monolingual and multilingual baselines on both high-resource and zero-shot languages. Below, we elaborate on the detailed results.\nHigh-resource Languages Labelled Attachement Scores (LAS) on the high-resource set are given in Table 1. UDapter consistently outperforms both our monolingual and multilingual baselines in all languages, and beats the previous work, setting a new state of the art, in 9 out of 13 languages. Statistical significance testing 8 applied between UDapter and multi/mono-udify confirms that UDapter's performance is significantly better than the baselines in 11 out of 13 languages (all except en and it).  Among directly comparable baselines, multiudify gives the worst performance in the typologically diverse high-resource setting. This multilingual model is clearly worse than its monolingually trained counterparts mono-udify: 83.0 vs 86.0. This result resounds with previous findings in multilingual NMT (Arivazhagan et al., 2019) and highlights the importance of language adaptation even when using high-quality sentence representations like those produced by mBERT.\nTo understand the relevance of adapters, we also evaluate a model which has almost the same architecture as multi-udify except for the adapter modules and the tuning choice (frozen mBERT weights). Interestingly, this adapter-only model considerably outperforms multi-udify (85.0 vs 83.0), indicating that adapter modules are also effective in multilingual scenarios.\nFinally, UDapter achieves the overall best results, with consistent gains over both multi-udify and adapter-only, showing the importance of linguistically informed adaptation even for in-training languages.\nLow-Resource Languages Average LAS on the 30 low-resource languages are shown in column lr-avg of Table 1. Overall, UDapter slightly outperforms the multi-udify baseline (36.5 vs 36.3), which shows the benefits of our approach on both in-training and zero-shot languages. For a closer look, Table 2 provides individual results for the 18 representative languages in our low-resource set. Here we find a mixed picture: UDapter outperforms multi-udify on 13 out of 18 languages 9 . Achieving improvements in the zero-shot parsing 9 LAS scores for all 30 languages are given in Appendix A.2. By significance testing, UDapter is significantly better than multi-udify on 16/30 low-resource languages, which is shown in  setup is very difficult, thus we believe this result is an important step towards overcoming the problem of positive/negative transfer trade-off. Indeed, UDapter-proxy results show that choosing a proxy language embedding from the same language family underperforms UDapter, apart from not being available for many languages. This indicates the importance of typological features in our approach (see \u00a7 5.2 for further analysis).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Analysis", "text": "In this section, we further analyse UDapter to understand its impact on different languages, and the importance of its various components.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Which languages improve most?", "text": "Figure 2 presents the LAS gain of UDapter over the multi-udify baseline for each high-resource language along with the respective treebank training size. To summarize, the gains are higher for languages with less training data. This suggests that in UDapter, useful knowledge is shared among intraining languages, which benefits low resource languages without hurting high resource ones.\nFor zero-shot languages, the difference between the two models is small compared to high-resource languages (+1.2 LAS). While it is harder to find a trend here, we notice that UDapter is typically beneficial for the languages not present in the mBERT training corpus: it outperforms multi-udify in 13 out of 22 (non-mBERT) languages. This suggests that typological feature-based adaptation leads to improved sentence representations when the pretrained encoder has not been exposed to a language. ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "How much gain from typology?", "text": "UDapter learns language embeddings from syntactic, phonological and phonetic inventory features.\nA natural alternative to this choice is to learn language embeddings from scratch. For a comparison, we train a model where, for each in-training language, a separate language embedding (of the same size: 32) is initialized randomly and learned end-toend. For the zero-shot languages we use the average, or centroid, of all in-training language embeddings. As shown in Figure 4a, on the high-resource set, the models with and without typological features achieve very similar average LAS (87.3 and 87.1 respectively). On zero-shot languages, however, the use of centroid embedding performs very poorly: 9.0 vs 36.5 average LAS score over 30 languages. As already discussed in \u00a7 4.1 (Table 2), using a proxy language embedding belonging to the same family as the test language, when available, also clearly underperforms UDapter. These results confirm our expectation that a model can learn reliable language embeddings for in-training languages, however typological signals are required to obtain a robust parsing quality on zero-shot languages.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "How does UDapter represent languages?", "text": "We start by analyzing the projection weights assigned to different typological features by the first layer of the language embedding network (see eq. 7). Figure 4b shows the averages of normalized syntactic, phonological and phonetic inventory feature weights. Although dependency parsing is a syntactic task, the network does not only utilize syntactic features, as also observed by Lin et al. (2019), but exploits all available typological features to learn its representations. Next, we plot the language representations learned in UDapter by using t-SNE (van der Maaten and Hinton, 2008), which is similar to the analysis carried out by Ponti et al. (2019, figure 8) using the language vectors learned by Malaviya et al. (2017). Figure 5 illustrates 2D vector spaces generated for the typological feature vectors l t (A) and the language embeddings l e learned by UDapter with or without typological features (B and C respectively). The benefits of using typological features can be understood by comparing A and B: During training, UDapter learns to project URIEL features to language embeddings in a way that is optimal for in-training language parsing quality. This leads to a different placement of the high-resource languages (red points) in the space, where many linguistic similarities are preserved (e.g. Hebrew and Arabic; European languages except Basque) but others are overruled (Japanese drifting away from Korean). Looking at the low-resource languages (blue points) we find that typologically similar languages tend to have similar embeddings to the closest highresource language in both A and B. In fact, most groupings of genetically related languages, such as the Indian languages (hi-cluster) or the Uralic ones (fi-cluster) are largely preserved across these two spaces.\nComparing B and C where language embeddings are learned from scratch, the absence of typological features leads to a seemingly random space with no linguistic similarities (e.g. Arabic far away from Hebrew, Korean closer to English than to Japanese, etc.) and, therefore, no principled way to represent additional languages.\nTaken together with the parsing results of \u00a7 4.1, these plots suggest that UDapter embeddings strike a good balance between a linguistically motivated representation space and one solely optimized for in-training language accuracy.", "n_publication_ref": 4, "n_figure_ref": 3}, {"heading": "Is CPG really essential?", "text": "In section 4.1 we observed that adapter tuning alone (that is, without CPG) improved the multilingual baseline in the high-resource languages, but worsened it considerably in the zero-shot setup. By contrast, the addition of CPG with typological features led to the best results over all languages. But could we have obtained similar results by simply increasing the adapter size? For instance, in multilingual MT, increasing overall model capacity of an already very large and deep architecture can be a powerful alternative to more sophisticated parameter sharing approaches (Arivazhagan et al., 2019).\nTo answer this question we train another adapteronly model with doubled size (2048 instead of the 1024 used in the main experiments).\nAs seen in 3a, increase in model size brings a slight gain to the high-resource languages, but actually leads to a small loss in the zero-shot setup. This shows that adapters enlarge the per-language capacity for in-training languages, but at the same time they hurt generalization and zero-shot transfer. By contrast, UDapter including CPG which increases the model size by language embeddings (see Appendix A.1 for details), outperforms both adapter-only models, confirming once more the importance of this component.\nFor our last analysis (Fig. 3b), we study soft parameter sharing via CPG on different portions of the network, namely: only on the adapter modules 'cpg (adapters)' versus on both adapters and biaffine attention 'cpg (adap.+biaf.)' corresponding to the full UDapter. Results show that most of the gain in the high-resource languages is obtained by only applying CPG on the multilingual encoder. On the other hand, for the low-resource languages, typological feature based parameter sharing is most important in the biaffine attention layer. We leave further investigation of this result to future work.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "We have presented UDapter, a multilingual dependency parsing model that learns to adapt languagespecific parameters on the basis of adapter modules (Rebuffi et al., 2018;Houlsby et al., 2019) and the contextual parameter generation (CPG) method (Platanios et al., 2018) which is in principle applicable to a range of multilingual NLP tasks. While adapters provide a more general tasklevel adaptation, CPG enables language-specific adaptation, defined as a function of language embeddings projected from linguistically curated typological features. In this way, the model retains high per-language performance in the training data and achieves better zero-shot transfer.\nUDapter, trained on a concatenation of typologically diverse languages (Kulmizev et al., 2019), outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, which reflects its strong balance between per-language capacity and maximum sharing. Finally, the analyses we performed on the underlying characteristics of our model show that typological features are crucial for zero-shot languages.    (Kondratyuk and Straka, 2019) for all low-resource languages. '*' shows languages not present in mBERT training data. Additionally, ( \u2020) indicates languages where no significant difference between UDapter and multi-udify by significance testing. For udapter-proxy, chosen proxy language is given between brackets. CTR means centroid language embedding. models are trained separately so the total number of parameters for 13 languages is 2.5B (13x191M).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "A.2 Zero-Shot Results", "text": "Table 4 shows LAS scores on all 30 low-resouce languages for UDapter, original UDify (Kondratyuk and Straka, 2019), and re-trained 'multiudify'. Languages with '*' are not included in mBERT training data. Note that original UDify is trained on all available UD treebanks from 75 languages. For the zero-shot languages, we obtained original UDify scores by running the pre-trained model.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A.3 Language Details", "text": "Details of training and zero-shot languages such as language code, data size (number of sentences), and family are given in Table 5 and Table 6.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "Arianna Bisazza was partly funded by the Netherlands Organization for Scientific Research (NWO) under project number 639.021.646. We would like to thank the Center for Information Technology of the University of Groningen for providing access to the Peregrine HPC cluster and the anonymous reviewers for their helpful comments.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Massively multilingual neural machine translation", "journal": "Long and Short Papers", "year": "2019", "authors": "Roee Aharoni; Melvin Johnson; Orhan Firat"}, {"title": "Many languages, one parser", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Waleed Ammar; George Mulcaire; Miguel Ballesteros; Chris Dyer; Noah A Smith"}, {"title": "Massively multilingual neural machine translation in the wild: Findings and challenges", "journal": "", "year": "1907", "authors": "Naveen Arivazhagan; Ankur Bapna; Orhan Firat; Dmitry Lepikhin; Melvin Johnson; Maxim Krikun; Mia Xu Chen; Yuan Cao; George Foster; Colin Cherry; Wolfgang Macherey; Zhifeng Chen; Yonghui Wu"}, {"title": "On the shortest arborescence of a directed graph", "journal": "Scientia Sinica", "year": "1965", "authors": "Yoeng-Jin Chu"}, {"title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Multi-task learning for multiple language translation", "journal": "", "year": "2015", "authors": "Daxiang Dong; Hua Wu; Wei He; Dianhai Yu; Haifeng Wang"}, {"title": "Deep biaffine attention for neural dependency parsing", "journal": "", "year": "2017", "authors": "Timothy Dozat; D Christopher;  Manning"}, {"title": "WALS Online. Max Planck Institute for Evolutionary Anthropology", "journal": "", "year": "2013", "authors": "Matthew S Dryer; Martin Haspelmath"}, {"title": "A neural network model for low-resource Universal Dependency parsing", "journal": "", "year": "2015", "authors": "Long Duong; Trevor Cohn; Steven Bird; Paul Cook"}, {"title": "Optimum branchings", "journal": "Journal of Research of the national Bureau of Standards B", "year": "1967", "authors": ""}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "journal": "", "year": "2016", "authors": "Orhan Firat; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Working hard or hardly working: Challenges of integrating typology into neural dependency parsers", "journal": "", "year": "2019", "authors": "Adam Fisch; Jiang Guo; Regina Barzilay"}, {"title": "Toward multilingual neural machine translation with universal encoder and decoder", "journal": "", "year": "2016-01", "authors": "Thanh-Le Ha"}, {"title": "Martin Haspelmath, and Sebastian Bank. 2020. Glottolog 4.3. Max Planck Institute for the Science of Human History", "journal": "", "year": "", "authors": "Harald Hammarstr\u00f6m; Robert Forkel"}, {"title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units", "journal": "", "year": "2016", "authors": "Dan Hendrycks; Kevin Gimpel"}, {"title": "Parameter-efficient transfer learning for nlp", "journal": "", "year": "2019", "authors": "Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin De Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly"}, {"title": "Universal language model fine-tuning for text classification", "journal": "Long Papers", "year": "2018", "authors": "Jeremy Howard; Sebastian Ruder"}, {"title": "Google's multilingual neural machine translation system: Enabling zero-shot translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Melvin Johnson; Mike Schuster; Quoc V Le; Maxim Krikun; Yonghui Wu; Zhifeng Chen; Nikhil Thorat; Fernanda Vi\u00e9gas; Martin Wattenberg; Greg Corrado; Macduff Hughes; Jeffrey Dean"}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Eliyahu Kiperwasser; Yoav Goldberg"}, {"title": "75 languages, 1 model: Parsing Universal Dependencies universally", "journal": "", "year": "2019", "authors": "Dan Kondratyuk; Milan Straka"}, {"title": "Deep contextualized word embeddings in transitionbased and graph-based dependency parsing -a tale of two parsers revisited", "journal": "", "year": "2019", "authors": "Artur Kulmizev; Johannes Miryam De Lhoneux; Elena Gontrum; Joakim Fano;  Nivre"}, {"title": "Ethnologue: Languages of the world [eighteenth", "journal": "SIL International", "year": "2015", "authors": "Paul Lewis; F Gary; C D Simons;  Fennig"}, {"title": "Parameter sharing between dependency parsers for related languages", "journal": "", "year": "2018", "authors": "Johannes Miryam De Lhoneux; Isabelle Bjerva; Anders Augenstein;  S\u00f8gaard"}, {"title": "From raw text to Universal Dependencies -look, no tags!", "journal": "", "year": "2017", "authors": "Yan Miryam De Lhoneux; Ali Shao; Eliyahu Basirat; Sara Kiperwasser; Yoav Stymne; Joakim Goldberg;  Nivre"}, {"title": "Antonios Anastasopoulos, Patrick Littell, and Graham Neubig", "journal": "", "year": "2019", "authors": "Yu-Hsiang Lin; Chian-Yu Chen; Jean Lee; Zirui Li; Yuyan Zhang; Mengzhou Xia; Shruti Rijhwani; Junxian He; Zhisong Zhang; Xuezhe Ma"}, {"title": "URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors", "journal": "Short Papers", "year": "2017", "authors": "Patrick Littell; David R Mortensen; Ke Lin; Katherine Kairis; Carlisle Turner; Lori Levin"}, {"title": "Visualizing data using t-sne", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "Laurens Van Der Maaten; Geoffrey Hinton"}, {"title": "Learning language representations for typology prediction", "journal": "", "year": "2017", "authors": "Chaitanya Malaviya; Graham Neubig; Patrick Littell"}, {"title": "Universal Dependency annotation for multilingual parsing", "journal": "", "year": "2013", "authors": "Ryan Mcdonald; Joakim Nivre; Yvonne Quirmbach-Brundage; Yoav Goldberg; Dipanjan Das; Kuzman Ganchev; Keith Hall; Slav Petrov; Hao Zhang; Oscar T\u00e4ckstr\u00f6m"}, {"title": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "journal": "Short Papers", "year": "", "authors": ""}, {"title": "PHOIBLE 2.0. Max Planck Institute for the Science of Human History", "journal": "", "year": "2019", "authors": ""}, {"title": "Selective sharing for multilingual dependency parsing", "journal": "Long Papers", "year": "2012", "authors": "Tahira Naseem; Regina Barzilay; Amir Globerson"}, {"title": "Rapid adaptation of neural machine translation to new languages", "journal": "", "year": "2018", "authors": "Graham Neubig; Junjie Hu"}, {"title": "Faculty of Mathematics and Physics", "journal": "", "year": "", "authors": " Tiana Lando; Alexei Septina Dian Larasati; John Lavrentiev; Ph\u00f4\u00eang Lee; Alessandro L\u00ea H`\u00f4ng; Saran Lenci; Herman Lertpradit;  Leung; Ying Cheuk; Josie Li; Keying Li; Kyungtae Li; Nikola Lim; Olga Ljube\u0161i\u0107; Olga Loginova; Teresa Lyashevskaya; Vivien Lynn; Aibek Macketanz; Michael Makazhanov; Christopher Mandl; Ruli Manning; C\u0203t\u0203lina Manurung; David M\u0203r\u0203nduc; Katrin Mare\u010dek;  Marheinecke; Andr\u00e9 H\u00e9ctor Mart\u00ednez Alonso; Jan Martins; Yuji Ma\u0161ek; Ryan Matsumoto; Gustavo Mc-Donald; Niko Mendon\u00e7a; Margarita Miekka; Anna Misirpashayeva; C\u0203t\u0203lin Missil\u00e4; Yusuke Mititelu; Simonetta Miyao; Amir Montemagni; Laura Moreno More; Keiko Sophie Romero; Shinsuke Mori; Bjartur Mori; Bohdan Mortensen; Kadri Moskalevskyi; Yugo Muischnek; Kaili Murawaki; Pinkey M\u00fc\u00fcrisep; Juan Ignacio Navarro Nainwani; Anna Hor\u00f1iacek; Gunta Nedoluzhko; L\u00f4\u00eang Ne\u0161pore-B\u0113rzkalne;  Nguy\u02dc\u00ean Thi;  Huy`\u00ean Nguy\u02dc\u00ean Thi; Vitaly Minh; Rattima Nikolaev; Hanna Nitisaroj; Stina Nurmi; Ad\u00e9dayo Ojala; Mai Ol\u00fa\u00f2kun; Petya Omura;  Osenova; Lilja Robert\u00f6stling; Niko \u00d8vrelid; Elena Partanen; Marco Pascual; Agnieszka Passarotti; Guilherme Patejuk; Siyao Paulino-Passos; Cenel-Augusto Peng; Guy Perez; Slav Perrier; Jussi Petrov; Emily Piitulainen; ; Pitler; Marat M Yan; Zhuoran Yavrumyan;  Yu; Amir Zden\u011bk\u017eabokrtsk\u00fd; Daniel Zeldes;  Zeman"}, {"title": "Continuous multilinguality with language vectors", "journal": "Short Papers", "year": "2017", "authors": "J\u00f6rg Robert\u00f6stling;  Tiedemann"}, {"title": "Mad-x: An adapter-based framework for multi-task cross-lingual transfer", "journal": "", "year": "2020", "authors": "Jonas Pfeiffer; Ivan Vuli\u0107; Iryna Gurevych; Sebastian Ruder"}, {"title": "How multilingual is multilingual BERT?", "journal": "", "year": "2019", "authors": "Telmo Pires; Eva Schlinger; Dan Garrette"}, {"title": "Contextual parameter generation for universal neural machine translation", "journal": "", "year": "2018", "authors": "Mrinmaya Emmanouil Antonios Platanios; Graham Sachan; Tom Neubig;  Mitchell"}, {"title": "Modeling language variation and universals: A survey on typological linguistics for natural language processing", "journal": "Computational Linguistics", "year": "2019", "authors": " Edoardo Maria Ponti; O' Helen; Yevgeni Horan; Ivan Berzak; Roi Vuli\u0107; Thierry Reichart; Ekaterina Poibeau; Anna Shutova;  Korhonen"}, {"title": "Udapi: Universal API for Universal Dependencies", "journal": "", "year": "2017", "authors": "Martin Popel; Zden\u011bk\u017eabokrtsk\u00fd ; Martin Vojtek"}, {"title": "Efficient parametrization of multidomain deep neural networks", "journal": "", "year": "2018", "authors": "Hakan Sylvestre-Alvise Rebuffi; Andrea Bilen;  Vedaldi"}, {"title": "Typological features for multilingual delexicalised dependency parsing", "journal": "", "year": "2019", "authors": "Manon Scholivet; Franck Dary; Alexis Nasr; Carlos Benoit Favre;  Ramisch"}, {"title": "82 treebanks, 34 models: Universal Dependency parsing with multi-treebank models", "journal": "", "year": "2018", "authors": "Aaron Smith; Bernd Bohnet; Joakim Miryam De Lhoneux; Yan Nivre; Sara Shao;  Stymne"}, {"title": "Bert and pals: Projected attention layers for efficient adaptation in multi-task learning", "journal": "", "year": "2019", "authors": "Asa Cooper Stickland; Iain Murray"}, {"title": "UDPipe 2.0 prototype at CoNLL 2018 UD shared task", "journal": "", "year": "2018", "authors": "Milan Straka"}, {"title": "Target language adaptation of discriminative transfer parsers", "journal": "", "year": "2013", "authors": "Oscar T\u00e4ckstr\u00f6m; Ryan Mcdonald; Joakim Nivre"}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "journal": "", "year": "2012", "authors": "Oscar T\u00e4ckstr\u00f6m; Ryan Mcdonald; Jakob Uszkoreit"}, {"title": "Cross-lingual dependency parsing with Universal Dependencies and predicted PoS labels", "journal": "", "year": "2015", "authors": "J\u00f6rg Tiedemann"}, {"title": "Zero-shot dependency parsing with pre-trained multilingual sentence representations", "journal": "", "year": "2019", "authors": "Ke Tran; Arianna Bisazza"}, {"title": "One model, two languages: training bilingual parsers with harmonized treebanks", "journal": "Short Papers", "year": "2016", "authors": "David Vilares; Carlos G\u00f3mez-Rodr\u00edguez; Miguel A Alonso"}, {"title": "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT", "journal": "", "year": "2019", "authors": "Shijie Wu; Mark Dredze"}, {"title": "Google's neural machine translation system: Bridging the gap between human and machine translation", "journal": "", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; V Quoc; Mohammad Le; Wolfgang Norouzi; Maxim Macherey; Yuan Krikun; Qin Cao; Klaus Gao;  Macherey"}, {"title": "Crosslanguage parser adaptation between related languages", "journal": "", "year": "2008", "authors": "Daniel Zeman; Philip Resnik"}, {"title": "Hierarchical low-rank tensors for multilingual transfer parsing", "journal": "", "year": "2015", "authors": "Yuan Zhang; Regina Barzilay"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Difference in LAS between UDapter and multi-udify in the high-resource setting. Diamonds indicate the amount of sentences in the corresponding treebank.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Impact of different UDapter components on parsing performance (LAS): (a) adapters and adapter layer size, (b) application of contextual parameter generation to different portions of the network. In (b) the model named 'cpg (adap.+biaf.)' coincides with the full UDapter.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: (a) Impact of language typology features on parsing performance (LAS). (b) Average of normalized feature weights obtained from linear projection layer of the language embedding network.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Vector spaces for (A) language-typology feature vectors taken from URIEL, (B) language embeddings learned from typological features by UDapter, and (C) language embeddings learned without typological features. High-and low-resource languages are indicated by red and blue dots respectively. Highlighted clusters in A and B denote sets of genetically related languages.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "uuparser-bert [1] 81.8 87.6 79.8 83.9 85.9 90.8 91.7 92.1 84.2 91.0 86.9 64.9 83.4 84.9 udpipe [2] 82.9 87.0 82.9 87.5 86.9 91.8 91.5 93.7 84.2 92.3 86.6 67.6 80.5 85.8 udify [3] 82.9 88.5 81.0 82.1 88.1 91.5 93.7 92.1 74.3 93.1 89.1 67.4 83.8", "figure_data": "aren eu fihe hiitjako rusvtrzhHR-AVG LR-AVGPrevious work:85.234.1Monolingually trained (one model per language):mono-udify83.5 89.4 81.3 87.3 87.9 91.1 93.1 92.5 84.2 91.9 88.0 66.0 82.486.0-Multilingually trained (one model for all languages):multi-udify80.1 88.5 76.4 85.1 84.4 89.3 92.0 90.0 78.0 89.0 86.2 62.9 77.883.035.3adapter-only82.8 88.3 80.2 86.9 86.2 90.6 93.1 91.6 81.3 90.8 88.4 66.0 79.485.032.9udapter84.4 89.7 83.3 89.0 88.8 92.0 93.5 92.8 85.9 92.2 90.3 69.6 83.287.336.5"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Labelled attachment scores (LAS) on high-resource languages for baselines and UDapter. Last two columns show average LAS of 13 high-resource (HR-AVG) and 30 low-resource (LR-AVG) languages respectively.", "figure_data": "Previous work results are reported from (Kulmizev et al., 2019) [1] and (Kondratyuk and Straka, 2019) [2,3].be br* bxr* cy fo* gsw* hsb* kk koi* krl* mdf* mr olo* pcm* sa* tl yo* yue* AVGmulti-udify80.1 60.5 26.1 53.6 68.6 43.6 53.2 61.9 20.8 49.2 24.8 46.4 42.1 36.1 19.4 62.7 41.2 30.5 45.2udapter-proxy 69.9 ---64.1 23.7 44.4 45.1 -45.6 -29.6 41.1 -15.1 --24.5 -udapter79.3 58.5 28.9 54.4 69.2 45.5 54.2 60.7 23.1 48.4 26.6 44.4 43.3 36.7 22.2 69.5 42.7 32.8 46.2"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": "60 70 80 90multi-udify adapter-only (1024) adapter-only (2048) udapter504030HRLR"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Together with the additional adapter size and language embedding size that are picked manually by parsing accuracy, hyperparameters are given in Table3. Note that, to give a fair chance to the adapter-only baseline (see \u00a74), we used 1024 as adapter size unlike that of the final UDapter (256). For fair comparison, mono-udify and multi-udify are re-trained on the concatenation of 13 high-resource languages for only dependency parsing. Besides, we did not use a layer attention for both our model and the baselines.Training Time and Model size Comparing to UDify, UDapter has a similar training time. An epoch over the full training set takes approximately 27 and 30 minutes in UDify and UDapter respectively on a Tesla V100 GPU.In terms of number of trainable parameters, UDify has 191M total number of parameters whereas UDapter uses 550M parameters in total, 302M for adapters (32x9.4M) and 248M for biaffine attention (32x7.8M), since the parameter generator network (CPG) multiplies the tensors with language embedding size (32). Note that for multilingual training, UDapter's parameter cost depends only on language embedding size regardless of number of languages, therefore it highly scalable with an increasing number of languages for larger experiments. Finally, monolingual UDify orig.udify multi-udify udapter udap.-proxy", "figure_data": ": Hyper-parameter setting A.1 Experimental Details A Appendix Implementation UDapter's implementation is based on UDify (Kondratyuk and Straka, 2019). We use the same hyper-parameters setting opti-mized in UDify without applying a new hyper-parameter search. aii* akk* am* be bho*( \u2020) bm* br* bxr* cy fo* gsw* gun*( \u2020) hsb* kk kmr*( \u2020) koi* kpv*( \u2020) krl* mdf* mr myv*( \u2020) olo* pcm*( \u2020) sa* ta ( \u2020) te ( \u2020) tl wbp* yo yue* avg9.1 4.4 2.6 81.8 35.9 7.9 39.0 26.7 42.7 59.0 39.7 6.0 62.7 63.6 20.2 22.6 12.9 41.7 19.4 67.0 16.6 33.9 31.5 19.4 71.4 83.4 41.4 6.7 22.0 31.0 34.18.4 4.5 2.8 80.1 37.2 8.9 60.5 26.1 53.6 68.6 43.6 8.5 53.2 61.9 11.2 20.8 12.4 49.2 24.7 46.4 19.1 42.1 36.1 19.4 46.0 71.2 62.7 9.6 41.2 30.5 35.314.3 8.2 5.9 79.3 37.3 8.1 58.5 28.9 54.4 69.2 45.5 8.4 54.2 60.7 12.1 23.1 12.5 48.4 26.6 44.4 19.2 43.3 36.7 22.2 46.1 71.1 69.5 12.1 42.7 32.8 36.58.2 (ar) 9.1 (ar) 1.1 (ar) 69.9 (ru) 35.9 (hi) 3.1 (CTR) 14.3 (CTR) 9.1 (CTR) 9.8 (CTR) 64.1 (sv) 23.7 (en) 2.1 (CTR) 44.4 (ru) 45.1 (tr) 4.7 (CTR) 6.5 (CTR) 4.7 (CTR) 45.6 (fi) 8.7 (CTR) 29.6 (hi) 6.3 (CTR) 41.1 (fi) 5.6 (CTR) 15.1 (hi) 12.3 (CTR) 23.1 (CTR) 14.1 (CTR) 4.8 (CTR) 10.5 (CTR) 24.5 (zh) 20.4"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "LAS results of UDapter and UDify models", "figure_data": ""}], "doi": "10.18653/v1/N19-1388"}