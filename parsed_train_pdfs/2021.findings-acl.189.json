{"authors": "Jie Zhou; Yuanbin Wu; Qin Chen; Xuanjing Huang; Liang He", "pub_date": "", "title": "Attending via both Fine-tuning and Compressing", "abstract": "Though being a primary trend for enhancing interpretability of neural networks, attention mechanism's reliability and validity are still under debate. In this paper, we try to purify attention scores to obtain a more faithful explanation of downstream models. Specifically, we propose a framework consisting of a learner and a compressor, which performs finetuning and compressing iteratively to enhance the performance and interpretability of the attention mechanism. The learner focuses on learning better text representations to achieve good decisions by fine-tuning, while the compressor aims to perform compressions over the representations to retain the most useful clues for explanations with a Variational information bottleneck ATtention (VAT) mechanism. Extensive experiments on eight benchmark datasets show the great advantages of our proposed approach in terms of both performance and interpretability.", "sections": [{"heading": "Introduction", "text": "Attention mechanisms (Bahdanau et al., 2014) have achieved great success in various natural language processing (NLP) tasks. They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels. The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models (Mullenbach et al., 2018;Xie et al., 2017;Xu et al., 2015). Li et al. (2016) pointed the view: \"Attention provides an important way to explain the workings of neural models\". Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model.\nThe basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally verified. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, and there are a lot of controversy regarding to the result explanations (Wiegreffe and Pinter, 2019;Jain and Wallace, 2019). Moreover, we find that though the attention mechanism can help improve the performance for text classification in our experiments, it may focus on the irrelevant information. For example, in the sentence \"A very funny movie.\", the long short-term memory model with standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word \"movie\", making the result difficult to explain.\nIn general, the attention weights are only optimized to encode the task-relevant information while are not restricted to imitate human behavior. In order to enhance the interpretability of the attention mechanism, recent studies turn to integrate the human provided explanation signals into the attention models.  regularized the attention weights with a small amount of word-level annotations. Barrett et al. (2018); Bao et al. (2018) improved the explanation of attention by aligning explanations with human-provided rationales. These methods rely on additional labour consuming labelling for enhancing explanations, which is hard to extend to other datasets or tasks.\nIn this paper, we aim to train a more efficient and effective interpretable attention model without any pre-defined annotations or pre-collected explanations. Specifically, we propose a framework consisting of a learner and a compressor, which enhances the performance and interpretability of the attention model for text classification 1 . The learner learns text representations by fine-tuning the encoder. Regarding to the compressor, we are motivated by the effectiveness of the information bottleneck (IB) (Tishby et al., 1999) to enhance performance (Li and Eisner, 2019) or detect important features (Bang et al., 2019;Chen and Ji, 2020;Jiang et al., 2020;Schulz et al., 2020), and present a Variational information bottleneck ATtention (VAT) mechanism using IB to keep the most relevant clues and forget the irrelevant ones for better attention explanations. In particular, IB is integrated into attention to minimize the mutual information (MI) with the input while preserving as much MI as possible with the output, which provides more accurate and reliable explanations by controlling the information flow.\nTo evaluate the effectiveness of our proposed approach, we adapt two advanced neural models (LSTM and BERT) within the framework and conduct experiments on eight benchmark datasets. The experimental results show that our adapted models outperform the standard attention-based models over all the datasets. Moreover, they exhibit great advantages with respect to interpretability by both qualitative and quantitative analyses. Specifically, we obtain significant improvements by applying our model to the semi-supervised word-level sentiment detection task, which detects the sentiment words based on attention weights via only sentencelevel sentiment label. In addition, we provide the case studies and text representation visualization to have an insight into how our model works.\nThe main contributions of this work are summarized as follows.\n\u2022 We propose a novel framework to enhance the performance and interpretability of the attention models, where a learner is used to learn good representations by fine-tuning and a compressor is used to obtain good attentive weights by compressing iteratively.\n\u2022 We present a Variational information bottleneck ATtention (VAT) mechanism for the compressor, which performs compression over the text representation to keep the task related information while reduce the irrelevant noise via information bottleneck.\n\u2022 Extensive experiments show the great advantages of our models within the proposed framework, and we perform various qualitative and quantitative analyses to shed light on why our models work in both performance and interpretability.", "n_publication_ref": 17, "n_figure_ref": 0}, {"heading": "Related Work", "text": "In this section, we survey related attention mechanisms (Bahdanau et al., 2014) and review the most relevant studies on information bottleneck (IB) (Tishby et al., 1999). Attention has been proved can help explain the internals of neural models (Li et al., 2016;Wiegreffe and Pinter, 2019) though it is limited (Jain and Wallace, 2019). Many researchers try to improve the interpretability of the attention mechanisms.  leveraged small amounts of word-level annotations to regularize attention. Kim et al. (2017) introduced a structured attention mechanism to learn attention variants from explicit probabilistic semantics. Barrett et al. (2018); Bao et al. (2018) aligned explanations with human-provided rationales to improve the explanation of attention. Unlike these methods that require prior attributions or human explanations, the VAT method enforces the attention to learn the vital information while filter the noise via IB.\nA series of studies motivate us to utilize IB to improve the explanations of attention mechanisms. Li and Eisner (2019) compressed the pre-trained embedding (e.g., BERT, ELMO), remaining only the information that helps a discriminative parser through variational IB. Zhmoginov et al. (2019) utilized the IB approach to discover the salient region. Some works (Jiang et al., 2020;Chen et al., 2018;Guan et al., 2019;Schulz et al., 2020;Bang et al., 2019) proposed to identify vital features or attributions via IB. Moreover, Chen and Ji (2020) designed a variational mask strategy to delete the useless words in the text. As far as we are aware, we are the first ones to leverage IB into attention mechanisms to train more interpretable attention with better accuracy.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Our Approach", "text": "In this section, we introduce our framework consisting of a learner and a compressor with a Variational information bottleneck ATtenttion (VAT) mechanism. Given an attention-based neural network model, we formulate our idea within the framework of variational information bottleneck (VIB) (Tishby et al., 1999). Our framework aims to improve the attention's interpretalility with better performance by restricting the attention to capture the crucial words while filter the useless information. ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Overview", "text": "Our framework is composed of a learner and a compressor, which performs fine-tuning and compressing iteratively (Figure 1). The learner aims to learn a task-specific contextual word representation by fine-tuning. The compressor enforces the model to learn task-relevant information while reduce irrelevant information via IB. We iteratively perform the learner and compressor (fine-tuning and compressing) to improve each other.\nLearner. We adopt a basic attention-based neural network model as a learner to learn representations of the words based on the good attention weights learned by the compressor. The model is optimized by cross-entropy loss to learn the label-relevant information. In this phase, we fix the attention's parameters so that the model will focus on updating the encoder to learn word representations.\nCompressor. To restrict the attention to capture the vital information while reduce the noise, we integrate IB into attention mechanisms to compress the text attentive representation. We fix the encoder's parameters so that the model will focus on learning the attention weights based on current representations obtained from the learner.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Basic Attention Model (Learner)", "text": "In this section, we describe our learner, which is an attention-based neural network model. First, given a text T \" tw 1 , w 2 , ..., w |T | u, where |T | is the length of text T , we feed it into an encoder with a First, we obtain the input text's word representations X via an encoder trained by the learner. Then, we calculate Z by compressing the text representation R that is the weighted sum of X based on the attention \u03b1, while remaining the maximum information to judge Y by inputting Z into a MLP classifier for predicting. word embedding layer. We adopt LSTM and BERT models as our encoder, and other models can also be applied to our framework. We obtain the contextaware word representations x \" rx 1 , x 2 , ..., x |T | s, where x i is the hidden vector of the word w i .\nx \" encoderpT, \u03b8 encoder q,\n(\n)1\nwhere \u03b8 encoder is the parameters of the encoder.\nBased on the contextual word representations, attention mechanism (Bahdanau et al., 2014) 2 is utilized to capture the important parts in the text and obtain the text representation R, which is calculated as, R \"\nn \u00ff i\"1 \u03b1 i x i \u03b1 i \" softmaxpv J a tanhpW a x i qq (2)\nwhere \u03b8 attention \" tv a , W a u is the trainable parameters of the attention, which is not updated in this step to learn the word representation x based the good attention learned by the compressor. \u03b1 \" r\u03b1 1 , \u03b1 2 , ..., \u03b1 |T | s is the attention weights. Finally, we input the text representation R into a multi-layer perceptron (MLP) to predict the probability. The cross-entropy loss is used to optimize the model.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Variational Information Bottleneck Attention (Compressor)", "text": "The learner optimizes the sentence representations by minimizing the cross-entropy loss, which does not restrict the model to ignore the useless information. Thus, we compress sentence representations R into a latent representation Z that retains most useful information to infer the label Y . We propose to accomplish this by integrating VIB into the attention mechanism (Figure 2).\nTo ensure Z contains maximum ability to predict Y (IpZ; Y q) while has the least redundant information form R (\u00b4IpZ; Rq), we use the standard IB theory (Tishby et al., 1999)  where KLr\u00a8}\u00a8s represents Kullback-Leibler divergence. Specifically, we regard ppyq as constant and then minimize E p \u03b8 py,zq rlog q \u03c6 py | zqs. Since we must first sample r to sample y, z from p \u03b8 pr, y, zq, the lower bound of IpZ; Y q is computed as,\nIpZ; Y q \u011b E ppr,yq rE p \u03b8 pz|rq rlog q \u03c6 py | zqss (5)\nWe calculate the upper bound of IpZ; Rq by replacing p \u03b8 pzq with a variational distribution r \u03c8 pzq, 3 We give the main steps as follows and the detailed derivation is provided in supplementary materials.\n4 Y \u00d1 R \u00d1 Z: Y and Z are independent given R. Then, we obtain the lower bound L of IB by substituting Equation 5 and 7 into Equation 3:\nL \" E ppr,yq rE p \u03b8 pz|rq rlog q \u03c6 py | zq\u015b \u03b2\u00a8KLrp \u03b8 pz | rq}r \u03c8 pzqss (8)\nThe first component in L is to keep the most useful information in p \u03b8 pz|rq for inferring y, while the second one is to regularize p \u03b8 pz|rq with a predefined prior distribution r \u03c8 pzq (e.g., Gaussian distribution). To compute p \u03b8 pz|rq, we adopt the reparametrization trick for multivariate Gaussians (Rezende et al., 2014), which obtains the gradient of parameters that derive z from a random noise .\nz \" u`\u03c3 d , \" N p0, Iq(9)\nwhere d means element-wise multiplication. u and \u03c3 denote the mean and covariance defined by two functions of R, where R \" \u03b1\u00a8x that is learned based on attention. In particular, two MLP are used to predict u and \u03c3.\nFinally, we input the z into a MLP to predict q \u03c6 py | zq and optimize the attention's parameter via Equation 8.    ", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Implementation Details", "text": "For LSTM-based models, we use GloVe embedding (Pennington et al., 2014) with 300-dimension to initialize the word embedding and fine-tune it during the training. We randomly initialize all outof-vocabulary words and weights with the uniform distribution U p\u00b40.1, 0.1q. For the BERT-based models, we fine-tune pre-trained BERT-base model. ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experiments", "text": "First, we perform our models and baselines on eight benchmark datasets and visualize the text representation to verify the effectiveness of VAT (Section 5.1). Second, to further investigate our VAT model, we adopt two popular explanation metrics for quantitative evaluation (Section 5.2). Third, we apply our models to semi-supervision sentiment detection task to evaluate the explanation of our model (Section 5.3). Fourth, we explore the influence of our iteration strategy in Section 5.4 and provide case studies in Section 5.5. For the limitation of the space, we may only list the results on parts of the datasets in some cases since the conclusions are similar for other datasets. The complete results are presented in the supplementary materials.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Main Results", "text": "We report the accuracy of our VAT and baselines based on LSTM and BERT (Table 2). From these results, we find the following observations: 1) our models (LSTM/BERT-VAT) outperform all the corresponding baselines over all the eight datasets, which denotes the effectiveness of our VAT on both LSTM and BERT-based models; 2) compared with attention-based models (LSTM/BERT-ATT), our models obtain better results. It indicates reducing the irrelevant information in input via VAT can improve the performance of the models. Furthermore, we visualize the sentence representations obtained from LSTM/BERT-ATT and -VAT models (Figure 3). We randomly select 1000 samples from the test set for each dataset. We can find that our VAT model can reduce the distance of the samples in a class and add the distance of the samples in different classes. For example, it is hard to split the positive samples from the negative ones based on the representations obtained from LSTM-ATT for the IMDB dataset, while the divider line based on our VAT is clear. These ob-    servations show our VAT model can learn a better task-specific representation by enforcing the model to reduce the task-irrelevant information.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Quantitative Evaluation", "text": "In this section, we evaluate our VAT model using two metrics, AOPC and post-hoc accuracy, which are widely used for explanations (Chen and Ji, 2020). Note that well-trained LSTM/BERT-base is used for evaluating the performance of classification.\nAOPC. To evaluate the faithfulness of explanations to our models, we adopt the area over the perturbation curve (AOPC) (Nguyen, 2018;Samek et al., 2016) metric. It calculates the average change of accuracy over test data by deleting top K words via attentive weights. The larger the value of AOPC, the better the explanations of the models.\nTable 3 displays the results with K \" 5. We compare our models with random and basic attention-based models. From the results, we observe that: 1) basic attention-based models (LSTM/BERT-ATT) can find the important words in the sentence to some extent. Comparing with random (Random), LSTM/BERT-ATT obtains significant improvement; 2) Our models (LSTM/BERT-VAT) outperform the standard attention-based models. It indicates that integrating VIB into the attention mechanism can help improve the interpretability of the models by filtering the useless information; 3) BERT model is sensitive to the context; deleting the words will destroy the semantic information of the sentence and significantly affect the model's performance.\nWe also explore the influence of top-K (Figure 4). Intuitively, the more words we delete, the larger accuracy the models reduce. Our models reduce more performance than random and attention-based   models. For the IMDB dataset, when deleting top 20 words (average length is 268), the accuracy reduces about 19 points for our LSTM-VAT model while it is about 2 points for the random model.\nPost-hoc Accuracy. We also adopt the post-hoc accuracy (Chen et al., 2018) to evaluate the influence of task-specific essential words on the performance of LSTM-based and BERT-based models.\nFor each test sample, we select the top K words based on their attentive weights as input to make a prediction and compare it with the ground truth. Table 4 presents the performance with K \" 5. First, it is interesting to find that the post-hoc accuracy with five most important words on Sbuj dataset (89.10) is even better than the original sentence (89.00). Additionally, we obtain comparable results with only five words for SST-1, SST-2, and Twitter datasets. These show that our model can reduce the noise information since most of the words are useless for predictions in some cases. Second, for BERT-based models, the context words are also important for classification even though they may not be task-specific.\nSimilarly, we investigate the influence of top-K for post-hoc (Figure 5). The LSTM-base model with top-10 words selected by our LSTM-VAT model can achieve comparable results with the original samples in most cases. Additionally, for the IMDB dataset, the accuracy of LSTM-base with one word selected by our VAT model is even better than the one with 20 words selected randomly.", "n_publication_ref": 4, "n_figure_ref": 2}, {"heading": "Semi-Supervised Word-Level Sentiment Detection", "text": "We perform semi-supervised word-level sentiment detection in Twitter (Rosenthal et al., 2015(Rosenthal et al., , 2014 to evaluate the interpretability of our VAT. This task requires to detect the sentiment words in a tweet via the sentiment polarity of the whole tweet. In the following example from the dataset, positive words (\"good\" and \"fantastic\") are marked with a bold font and the overall polarity of the tweet is positive: Good morning becky! Thursday is going to be fantastic!\nWe use the SemEval 2013 Twitter dataset, which contains word-level sentiment annotation. We remove the samples with the neutral sentiment. We report word-level precision, recall, and F-measure for evaluating the models (Table 5), the same as . Note that we select the top-K (we set it as 1 and 5 here) words according to the attention weights as the sentiment words.\nWe compare our VAT model with random and attention-based models. The results show attentionbased models can capture the important words in the text, to a certain extent. Since our VAT can reduce irrelevant information, it performs better than the standard attention model. Also, LSTM-based models outperform BERT-based models for this task in most cases. It is because that BERT learns much semantic information from the text, and context information plays a vital role in prediction.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Influence of Iteration", "text": "We propose to train the learner and compressor iteratively so that the learner optimizes the word representations based on the good attention, and the compressor optimizes the attention based on the good word representations. To have a deep look at how it works, we first provide our VAT model's accuracy with different iterations (Table 6). From the results, we can find that the model's performance will improve at first, then it will converge.\nPositive Negative P@1 R@1 F1@1 P@5 R@5 F1@5 P@1 R@1 F1@1 P@5 R@     Also, we draw change of the sentence representation with different iterations (Figure 6). Similarly, we observe that fine-tuning and compressing iteratively can improve the sentence representations. The samples with the same class are close, and the samples with different classes have a large distance.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Case Studies", "text": "To understand why our proposed VAT model is more effective than the standard attention-based model, we visualize two examples of LSTM-based models using attention heatmaps (Figure 7). First, the standard attention-based LSTM model focuses on the wrong words (e.g., \"this\", \"work\") even though it predicts the right sentiment while our VAT model finds the correct words (e.g., \"admired\", \"lot\"). It indicates integrating IB into attention can help it focus on the key words and reduce the noisy information. Second, our proposed model can also improve the attention's performance by capturing the critical words accurately. For example, in the sentence \"That sucks if you have to take the sats tomorrow.\", our model predicts the right class label by attending the words \"sucks\" and \"have to.\"", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Conclusions and Future Work", "text": "This paper proposes a VAT-based framework to improve the performance and interpretability of attentions via both fine-tuning and compressing. The experimental results on eight benchmark datasets for text classification verify the effectiveness of our models within this framework. In addition, we apply the framework for sentiment detection, which further demonstrates the superiority in terms of interpretability. It is also interesting to find that training the models by fine-tuning and compressing iteratively is effective to improve the text representations. In the future, we will investigate the effectiveness of our proposed attention framework for other tasks and areas, such as machine translation and visual question answering.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgement", "text": "The authors wish to thank the reviewers for their helpful comments and suggestions. This research is (partially) supported by NSFC (62076097), STCSM (18ZR1411500), the Fundamental Research Funds for the Central Universities. This research is also funded by the Science and Technology Commission of Shanghai Municipality (19511120200 & 20511105102). The computation is performed in ECNU Multifunctional Platform for Innovation (001). The corresponding authors are Yuanbin Wu and Liang He.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2014", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Explaining a black-box using deep variational information bottleneck approach", "journal": "", "year": "2019", "authors": "Seojin Bang; Pengtao Xie; Heewook Lee; Wei Wu; Eric Xing"}, {"title": "Deriving machine attention from human rationales", "journal": "", "year": "2018", "authors": "Yujia Bao; Shiyu Chang; Mo Yu; Regina Barzilay"}, {"title": "Sequence classification with human attention", "journal": "", "year": "2018", "authors": "Maria Barrett; Joachim Bingel; Nora Hollenstein; Marek Rei; Anders S\u00f8gaard"}, {"title": "Learning variational word masks to improve the interpretability of neural text classifiers", "journal": "", "year": "2020", "authors": "Hanjie Chen; Yangfeng Ji"}, {"title": "Learning to explain: An information-theoretic perspective on model interpretation", "journal": "", "year": "2018", "authors": "Jianbo Chen; Le Song; Martin Wainwright; Michael Jordan"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Towards a deep and unified understanding of deep neural models in nlp", "journal": "PMLR", "year": "2019", "authors": "Chaoyu Guan; Xiting Wang; Quanshi Zhang; Runjin Chen; Di He; Xing Xie"}, {"title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"title": "Attention is not explanation", "journal": "", "year": "2019", "authors": "Sarthak Jain; C Byron;  Wallace"}, {"title": "Inserting information bottleneck for attribution in transformers", "journal": "", "year": "2020", "authors": "Zhiying Jiang; Raphael Tang; Ji Xin; Jimmy Lin"}, {"title": "Structured attention networks", "journal": "", "year": "2017-04-24", "authors": "Yoon Kim; Carl Denton; Luong Hoang; Alexander M Rush"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Understanding neural networks through representation erasure", "journal": "", "year": "2016", "authors": "Jiwei Li; Will Monroe; Dan Jurafsky"}, {"title": "Specializing word embeddings (for parsing) by information bottleneck", "journal": "", "year": "2019-11-03", "authors": "Lisa Xiang; Jason Li;  Eisner"}, {"title": "Learning question classifiers", "journal": "", "year": "2002", "authors": "Xin Li; Dan Roth"}, {"title": "Learning word vectors for sentiment analysis", "journal": "", "year": "2011", "authors": "Andrew Maas; Raymond E Daly; T Peter; Dan Pham;  Huang; Y Andrew; Christopher Ng;  Potts"}, {"title": "Explainable prediction of medical codes from clinical text", "journal": "Long Papers", "year": "2018", "authors": "James Mullenbach; Sarah Wiegreffe; Jon Duke; Jimeng Sun; Jacob Eisenstein"}, {"title": "Comparing automatic and human evaluation of local explanations for text classification", "journal": "Long Papers", "year": "2018", "authors": "Dong Nguyen"}, {"title": "Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales", "journal": "", "year": "2005", "authors": "Bo Pang; Lillian Lee"}, {"title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"title": "Zero-shot sequence labeling: Transferring knowledge from sentences to tokens", "journal": "Long Papers", "year": "2018", "authors": "Marek Rei; Anders S\u00f8gaard"}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "journal": "", "year": "2014", "authors": "Danilo Jimenez Rezende; Shakir Mohamed; Daan Wierstra"}, {"title": "SemEval-2015 task 10: Sentiment analysis in Twitter", "journal": "", "year": "2015", "authors": "Sara Rosenthal; Preslav Nakov; Svetlana Kiritchenko; Saif Mohammad; Alan Ritter; Veselin Stoyanov"}, {"title": "SemEval-2014 task 9: Sentiment analysis in Twitter", "journal": "", "year": "2014", "authors": "Sara Rosenthal; Alan Ritter; Preslav Nakov; Veselin Stoyanov"}, {"title": "Evaluating the visualization of what a deep neural network has learned", "journal": "", "year": "2016", "authors": "Wojciech Samek; Alexander Binder; Gr\u00e9goire Montavon; Sebastian Lapuschkin; Klaus-Robert M\u00fcller"}, {"title": "Restricting the flow: Information bottlenecks for attribution", "journal": "", "year": "2020-04-26", "authors": "Karl Schulz; Leon Sixt; Federico Tombari; Tim Landgraf"}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "journal": "", "year": "2013", "authors": "Richard Socher; Alex Perelygin; Jean Wu; Jason Chuang; D Christopher;  Manning; Y Andrew; Christopher Ng;  Potts"}, {"title": "The information bottleneck method", "journal": "", "year": "1999", "authors": "Naftali Tishby; C Fernando; William Pereira;  Bialek"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Attention is not not explanation", "journal": "", "year": "2019", "authors": "Sarah Wiegreffe; Yuval Pinter"}, {"title": "An interpretable knowledge transfer model for knowledge base completion", "journal": "Long Papers", "year": "2017", "authors": "Qizhe Xie; Xuezhe Ma; Zihang Dai; Eduard Hovy"}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "journal": "", "year": "2015", "authors": "Kelvin Xu; Jimmy Ba; Ryan Kiros; Kyunghyun Cho; Aaron Courville; Ruslan Salakhudinov; Rich Zemel; Yoshua Bengio"}, {"title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}, {"title": "Information-bottleneck approach to salient region discovery", "journal": "", "year": "2019", "authors": "Andrey Zhmoginov; Ian Fischer; Mark Sandler"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: The framework.The learner aims to learn the good text representation X by fine-tuning, and the compressor aims to learn good attention weights by compressing the attentive representations to capture the important words while forget the redundant information via VAT. The blue circles mean the corresponding parameters of the modules are fixed.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: The architecture of our VAT (Compressor). First, we obtain the input text's word representations X via an encoder trained by the learner. Then, we calculate Z by compressing the text representation R that is the weighted sum of X based on the attention \u03b1, while remaining the maximum information to judge Y by inputting Z into a MLP classifier for predicting.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "BaselinesWe compare our model with two kinds of models, basic models (LSTM/BERT-base) and attention-based models (LSTM/BERT-ATT). LSTM-base takes the max-pooling of the LSTM's hidden vectors as text representation. For BERTbase, the \"[CLS]\" representation is obtained as the sentence representation. LSTM-ATT model is a standard attention-based LSTM model that has the same structure as the learner. We obtain the BERT-ATT by replacing the LSTM encoder with BERT in LSTM-ATT. Our models are marked with VAT (LSTM-VAT, BERT-VAT), which integrate VIB into attention-based neural models.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Visualization of text representation obtained from LSTM/BERT-ATT and LSTM/BERT-VAT. We use t-SNE to transfer 100/768-dimensional feature space into two-dimensional space.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: The influence of Top-K for LSTM/BERTbased models in terms of AOPC.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: The influence of Top-K for LSTM-based models in terms of post-hoc.", "figure_data": ""}, {"figure_label": "67", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 6 :Figure 7 :67Figure 6: Visualization of text representation obtained from LSTM/BERT-VAT with different iterations. We use t-SNE to transfer 100/768-dimensional feature space into two-dimensional space.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Ip\u00a8;\u00a8q means the mutual information and \u03b2 is a coefficient to balance two components. The main challenge is to estimate the lower bound for IpZ; Y q and the upper bound for IpZ; Rq.3   The joint probability p \u03b8 pr, y, zq can be factored as pprq\u00a8ppy | rq\u00a8p \u03b8 pz | rq based on the independence assumption4  . By replacing the conditional distribution p \u03b8 py | zq with a variational approximation q \u03c6 py | zq, we obtain a lower bound of IpZ; Y q. q \u03c6 py | zq is a simple classifier that runs on a compressed text representation z.", "figure_data": "and define the objectivefunction as:max \u03b1IpZ; Y q\u00b4\u03b2\u00a8IpZ; Rq(3)where IpZ;Y q hkkkkkkkkkkkkkikkkkkkkkkkkkkjower bound hkkkkkkkkkkkkkikkkkkkkkkkkkkjE p \u03b8 py,zq rlogp \u03b8 py | zq ppyqs\u00b4l E p \u03b8 py,zq rlogq \u03c6 py | zq ppyqs"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "bound hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj E pprq rE p \u03b8 pz|rq rlog p \u03b8 pz | rq r \u03c8 pzq ss\u00b4I pZ;Rq hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj E pprq rE p \u03b8 pz|rq rlog p \u03b8 pz | rq ppzq ss \"E pprq rKLpppzq}r \u03c8 pzqqs \u011b 0 (6) The upper bound of IpZ; Rq is computed as, IpZ; Rq \u010f E pprq rE p \u03b8 pz|rq rlog p \u03b8 pz | rq r \u03c8 pzq ss \" E pprq rKLrp \u03b8 pz | rq}r \u03c8 pzqss", "figure_data": "upper (7)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The statistics information of the datasets, where Class is the number of the class, Length is average text length, and #train/#dev/#test counts the number of samples in the train/dev/test sets.", "figure_data": "IMDB SST-1 SST-2YelpAG NewsTrecSubjTwitter AverageLSTM-base88.7945.2085.45 95.1091.9190.00 89.0071.2582.09LSTM-ATT88.1646.2984.73 95.0691.8891.00 90.8070.7582.33LSTM-VAT88.9847.4286.22 95.3292.0492.80 91.1071.6283.19BERT-base91.9051.4491.60 96.0793.5296.60 96.5075.2886.61BERT-ATT91.8151.1391.16 97.2093.4196.40 96.2074.8486.52BERT-VAT92.1151.9991.98 97.3693.7197.20 96.7077.1387.27"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "The main results of text classification.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The results of AOPC.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "The results of post-hoc accuracy.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "5 F1@5 Random 14.88 4.78 6.56 14.59 23.34 16.06 20.52 5.61 8.19 17.18 23.68 17.97 LSTM-ATT 58.70 26.04 32.73 30.30 54.17 34.70 47.13 15.74 21.39 28.24 42.04 30.33 LSTM-VAT 65.20 29.38 36.60 33.04 58.40 37.77 60.00 21.42 28.76 32.70 49.19 35.35 BERT-ATT 46.44 16.52 21.82 33.13 52.52 35.66 37.74 9.19 13.46 30.82 39.65 30.23 BERT-VAT 55.24 20.62 26.90 37.26 58.39 40.09 43.83 11.15 16.20 36.42 44.55 35.30", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "The results of semi-supervision word-level sentiment detection in twitter.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "71.62 70.96 70.67 71.06 70.98 IMDB 88.16 88.98 88.22 88.84 88.14 88.60 BERT-VAT Twitter 74.84 75.26 77.71 77.13 76.68 76.76 IMDB 91.81 92.06 92.11 92.09 91.92 91.96", "figure_data": "Dataset 012345LSTM-VATTwitter 70.75"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "The accuracy with different iteration number with our LSTM/BERT-VAT model.", "figure_data": ""}], "doi": ""}