{"authors": "Kalyani Roy; Vineeth Kumar Balapanuru; Tapas Nayak; Pawan Goyal", "pub_date": "", "title": "Investigating the Generative Approach for Question Answering in E-Commerce", "abstract": "Many e-commerce websites provide Productrelated Question Answering (PQA) platform where potential customers can ask questions related to a product, and other consumers can post an answer to that question based on their experience. Recently, there has been a growing interest in providing automated responses to product questions. In this paper, we investigate the suitability of the generative approach for PQA. We use state-of-the-art generative models proposed by Deng et al. (2020) and Lu et al. (2020) for this purpose. On closer examination, we find several drawbacks in this approach: (1) input reviews are not always utilized significantly for answer generation, (2) the performance of the models is abysmal while answering the numerical questions, (3) many of the generated answers contain phrases like \"I do not know\" which are taken from the reference answer in training data, and these answers do not convey any information to the customer. Although these approaches achieve a high ROUGE score, it does not reflect upon these shortcomings of the generated answers. We hope that our analysis will lead to more rigorous PQA approaches, and future research will focus on addressing these shortcomings in PQA.", "sections": [{"heading": "Introduction", "text": "With the increase in e-commerce shopping, customer-generated product queries are also growing. Manually answering the questions in real-time is infeasible, and also some questions go unanswered for an extended period. It is necessary to answer the user queries in the e-commerce business automatically. The user reviews are a vast source of information with diverse opinions, and they can be used to answer user queries. Earlier works on product question answering (PQA) focus on retrieval-based approaches and binary answer prediction tasks. McAuley and Yang (2016); Fan et al. (2019);  aim to predict the answer as \"yes/no\" based on the relevant reviews, customer ratings, aspects in the reviews, etc. Retrieval-based approaches try to find the most relevant review snippet as the answer (Chen et al., 2019a) and use a ranked list of review snippets as the response for a given question . With the success of machine translation (Sutskever et al., 2014) and summarization (See et al., 2017), the PQA approaches are shifting towards natural answer generation from relevant product reviews (Gao et al., 2019;Chen et al., 2019b;Deng et al., 2020;Lu et al., 2020;Gao et al., 2021). In this work, we analyse the answer generated from state-of-the-art generative models OAAG (Deng et al., 2020) and CHIME (Lu et al., 2020) in detail beyond their traditional scores on popular metrics such as ROUGE (Lin, 2004). We find that despite achieving a good score on these metrics, generated answers have several drawbacks that can lead to user dissatisfaction.  (Ni et al., 2019;He and McAuley, 2016) includes users' reviews along with a rating of the product given by the same user. The Product ID is used to align the question with its reviews.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "PQA Models", "text": "We use Opinion-aware Answer Generation (OAAG) model (Deng et al., 2020) and Crosspassage Hierarchical Memory Network (CHIME) model (Lu et al., 2020) for our analysis. Following the generative approach, these two models achieve state-of-the-art performance on the Amazon Question Answering dataset. There are thousands of products in each category in the Amazon Product Review dataset, and each product has thousands of reviews. All the reviews may not be relevant for a particular query, and therefore, to answer a product-related question, models need to filter out the irrelevant reviews first. OAAG and CHIME use the BM25 algorithm to retrieve and rank all the review snippets of a product, and the top k relevant snippets (we use top 10 reviews snippets) for that question are taken as the premise of the answer.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "OAAG Model", "text": "Upon retrieving the relevant reviews, OAAG uses an encoder-decoder model for answer generation. OAAG encodes the question and each review corresponding to that question using a Bi-LSTM (Hochreiter and Schmidhuber, 1997) network. They apply a co-attention mechanism over these encodings to get the question and review representations. They utilize the ratings of the retrieved reviews to mine the general opinion about the question using the attention mechanism. Finally, they employ a multi-view pointer-generator network that copies words from the question as well as from the reviews and fuses the opinion by re-weighting the attention scores of the words in reviews to generate an opinionated answer. They report ROUGE-based scores to compare the model performance against the previous approaches (Chen et al., 2019b;Gao et al., 2019).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "CHIME Model", "text": "CHIME uses a transformer-based encoder-decoder model to generate the response. It extends pretrained XLNet (Yang et al., 2019) with an auxiliary memory module that consists of two components: the context memory, and the answer memory. Given a question with K review passages, it creates K training instances, each consisting of the question, a review passage, and the reference answer. Each training instance is fed into an XLNet encoder to get the hidden representations that are used to update the two memories. The context memory mechanism sequentially reads the review passages and gathers the cross-passage evidences to identify the most prominent opinion in reviews. The answer memory works as a buffer to gradually refine the generated answers after reading each (question, review passage) pair. After reading the last review, the answer memory is fed to the decoder to get a final response.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Research Questions", "text": "We empirically analyse the OAAG model with dynamic fusion and CHIME model to answer the following research questions: RQ1 : Are the retrieved review snippets significantly utilized for generating the answers? RQ2 : Is the model performing similarly for a heterogeneous group of questions? RQ3 : Is the generative model biased towards more frequently occurring phrases? RQ4 : Can ROUGE capture the correctness of generated answers?", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We use two product categories, namely, Home&Kitchen and Sports&Outdoors for our analysis from the dataset mentioned in Section 2.1 after combining the question-answer and review dataset with the Product IDs. We will denote the two categories as Home and Sports, respectively. We use the same data split from OAAG 1 to retrain the models. Since there is no validation dataset, we take the 10% of the train data as validation data. Table A.1 in the Appendix shows the details of training, validation, and test split. We keep all the hyper-parameters the same as the OAAG and CHIME. We train all the OAAG models for 20 epochs and CHIME models for 3 epochs, and the model that performs the best on the validation set is used to evaluate the test set. We evaluate the model with ROUGE metric and report the F1 scores for ROUGE-1 (R1) and ROUGE-L (RL), which measure the word overlap and the longest common sequence between the reference answer and the generated answer, respectively. We obtain the ROUGE scores using rouge-score 2 package. Both the models use the BM25 algorithm to retrieve relevant reviews using the questions in the test dataset. We refer to this test setting as BM25Q.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Analysis & Discussion", "text": "For answering RQ1, at inference time, we replace these reviews with four sets of review snippets: (i) TrainA: We use BM25 to find the closest question to the test question in the train data, and we take the answer of it as the generated answer. (ii) Ran-domOD: We randomly choose the review snippets from any other product of that category except the product for which the question is asked. (iii) Ran-domID: We randomly select review snippets from the review sentences of that particular product. (iv) BM25QA: We retrieve the review snippets using the BM25 algorithm that uses the question and reference answer in the test dataset. OAAG uses the opinion along with the reviews. We also select the opinion of the corresponding review sentence while replacing the reviews. Both the models utilize the top 10 reviews for training and evaluation. Table 1 shows the result of this experiment. The TrainA does not utilize either of the models to generate the answer. It shows the answer from the most similar train question, and its performance is competitive with other methods, especially in Home. In both the categories, the performance of both the models is almost similar in RandomOD and RandomID. RandomID shows marginally better performance than RandomOD for OAAG. For CHIME, BM25Q performs the best in both categories. For OAAG, BM25QA performs the best in Home while in Sports, BM25QA performs the best in R1, and BM25Q performs the best in RL, but the difference is minute. The results are quite surprising: the performance of the models is very similar when the answers are generated with random reviews vs. when the answers are generated with the reviews obtained from BM25. Hence, it is not clear if the model is effectively utilizing the retrieved review snippets.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Answer to RQ2 (Models' performance on heterogeneous questions)", "text": "Different types of questions are asked on the Amazon product page like numerical, \"yes/no\", descriptive. The generative model may not be suitable for answering all kinds of questions. So, we categorize the questions as template-based and descriptive.  For template-based questions, the answer can be yes or no without any explanation. We filter the questions where the answer starts with 'yes', 'yeah', 'no', 'nope' and mark these as templatebased questions. Both categories contain \u223c75% descriptive questions. Table 2 summarizes the result of the template-based and generative questions. Both models' performance in descriptive questions is better than the template-based questions.\nFurthermore, we categorized the questions into numerical and non-numerical questions. We consider a question to be numerical if there are numbers in the question or in the reference answer. The test datasets of both the categories have \u223c19% numerical questions. The OAAG model performs better in answering non-numerical questions, while CHIME performs better in answering numerical questions. Although the ROUGE scores are close in numerical and non-numerical questions for both the models, on analyzing the numerical answers, we find that the words in generated and reference answers might match, but the numbers generally do not match. 3 We present some examples of numerical questions with their answers in Table A.4 of Appendix.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Answer to RQ3 (Bias in model)", "text": "We observe that some phrases are frequently occurring in the reference answers as well as in the generated answers. We find that in the training data of both categories, \u223c2.4% of the reference answers start with the phrase \"I don't think so\", but 12.29% of responses in Sports and 35.64% responses in Home begin with this phrase. This \u223c2.4% repetition of the same phrase in the training data makes the generative model biased towards this phrase.  Many of the reference answers in the training data contain \"I don't know\", \"I have no idea\", \"I can't say\". These kinds of answers do not give any meaningful information to the user. Together, we denote these phrases as IDK. On analysis of the dataset, we find that in Sports, there are 3.04%, 2.9%, and 6.9% IDK phrases in train dataset answers, test dataset answers, and generated answers, respectively. In Home, the answers in the train and test dataset contain 3.64% and 3.60% IDK phrases, respectively, but 16.31% of the answers are generated as IDK phrases. So, in the generated answers, the appearance of IDK phrases has increased by three to five folds which clearly shows that the model is biased towards these frequent phrases.\nTo see the effect of these phrases on the models, we remove the questions from the training dataset which have IDK in their reference answer 4 and retrain the models. We denote this model as BM25Q-IDK. Table 3 shows the result of BM25Q, the model trained on the original training data, and BM25Q-IDK. Home had 16.31% and Sports had only 6.9% IDK phrases, and consequently, when the IDK phrases are removed, it has more impact on Home which drops the bias towards these phrases and improves the ROUGE score, whereas, in Sports, BM25Q and BM25Q-IDK have close ROUGE scores due to lesser IDK phrases in the generated answer.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Answer to RQ4 (Correctness of generated response)", "text": "For answering RQ4, we look into the generated response with high R1 scores and check their correct-Question is this box made of polypropylene? can photos and mementos be safely stored in it ? Ref. Ans.\ni can't comment on the material it is made of, but the top does not stay on tight. it is not a decent storage container for anything requiring a top or to be airtight. OAAG.\nit says \"made in china\" on the bottom of the box . it says \"made in china\" on the bottom of the box . hope that helps . (26.22) CHIME.\nyes, it is polypropylene and can be stored in the box if you want to store them in a safe environment that is not toxic to you and/or your pets or food they will be exposed (27.77)", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Question", "text": "What is the width at the base? Ref. Ans.\nWidth across the bottom/base is approximately 3 inches. OAAG.\nThe width of the top of the base is about 1.5 inches . the base of the pitcher is 9 inches. the top of the pitcher is about 1.5 inche.' (25.00) CHIME.\nit\u015b about 12 \" wide at the base and about 10.5 inches deep (26.08) ness with respect to the reference answer. In OAAG model, 15.36% predictions in Home and 13.34% predictions in Sports have R1 score above 20. We manually analyse the reference and generated answers of randomly chosen 100 question-answers with a high ROUGE score (> 20), and we find that 54% are answered incorrectly. In CHIME model, 46.87% predictions in Home and 46.15% predictions in Sports have R1 score above 20 and 56% of 100 randomly chosen question-answer pairs (whose ROUGE score > 20) turn out to be incorrect. Table 4 shows two examples where the generated responses result in high R1 scores, but the answers are incorrect. Both models predict irrelevant answers in the first question, and the predicted dimension is incorrect in the second question. It shows that it is not possible to infer from ROUGE scores if the generated answer is accurate to the reference answer, i.e., the word count overlap is not an indicator of an accurate answer. We show some more cases with high R1 scores in Tables A.2 and A.3 in the Appendix.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we extensively analyze the generative approach of question-answering in e-commerce using a state-of-the-art OAAG model (Deng et al., 2020) and CHIME model (Lu et al., 2020). We find many shortcomings which need to be addressed for a reliable PQA system. We try to address four re-search questions related to the generative approach for PQA, such as how the models utilize the reviews, how it performs on diverse question types, whether it is biased toward frequent phrases in training data, and the correctness of the generated response. We hope that our analysis will lead to more rigorous PQA research.  A.2 Generated Answers with High R1 Score R1 score. In Table A.2, in the first and the second example, the generated answers are exactly the opposite of the reference answers. In the third example, the question was about sweating of the bottle and straw cover, but the answer does not address any of these. In the fourth example, the answer is ambivalent. The last example contains a frequently occurring phrase \"I don't know\" with a very high R1 score.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Sports", "text": "Similarly, in the case of Table A.3, the second and the fifth examples have high R1 scores, but the generated answers are exactly opposite of the reference answers. In the first question, the model wrongly predicts that it would melt and bubble up a little in the microwave, and in the third question, it predicts an entirely different answer. The response to the fourth question is \"I don't know,\" which is a frequently occurring phrase.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "", "text": "it 's about 3 oz . and it 's just as strong as any other clip i 've seen OAAG:\nit 's about 7.5 \u2032\u2032 long . it 's a great knife . it 's a great deal . it 's a great knife and i love it . it 's a great deal . CHIME:\ni'm not sure about the exact weight but it is very light and is very easy to use with ease . Question:\ndoes this fit a 2 year old ? Ref. Ans.:\nit fit my two year old who has a pretty normal size head . the multiple thicknesses of pads that are included really make it a great long term helmet ! OAAG:\nyes it will , it 's a very tight fit . i do n't think it would be too big for a 2 \u2032\u2032 2 \u2032\u2032 . it 's a great helmet .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "CHIME:", "text": "i bought this for my son and he loves it so much he bought another one for his 2 year-bean . ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.3 Examples of Numerical Questions", "text": "We present some examples of numerical questions with their answers in Table A.4. In the first example, the generated answer is right, but none of the answers are correct for the rest of the questions.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Answer identification from product reviews for user questions by multi-task attentive networks", "journal": "", "year": "2019", "authors": "Long Chen; Ziyu Guan; Wei Zhao; Wanqing Zhao; Xiaopeng Wang; Zhou Zhao; Huan Sun"}, {"title": "Review-driven answer generation for product-related questions in e-commerce", "journal": "", "year": "2019", "authors": "Shiqian Chen; Chenliang Li; Feng Ji; Wei Zhou; Haiqing Chen"}, {"title": "Opinion-aware answer generation for review-driven question answering in e-commerce", "journal": "", "year": "2020", "authors": "Yang Deng; Wenxuan Zhang; Wai Lam"}, {"title": "Reading customer reviews to answer product-related questions", "journal": "", "year": "2019", "authors": "Chao Miao Fan; Mingming Feng; Ping Sun; Haifeng Li;  Wang"}, {"title": "Meaningful answer generation of e-commerce question-answering", "journal": "ACM Transactions on Information Systems", "year": "2021", "authors": "Shen Gao; Xiuying Chen; Z Ren; Dongyan Zhao; Rui Yan"}, {"title": "Product-aware answer generation in e-commerce question-answering", "journal": "", "year": "2019", "authors": "Shen Gao; Zhaochun Ren; Yihong Zhao; Dongyan Zhao; Dawei Yin; Rui Yan"}, {"title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering", "journal": "", "year": "2016", "authors": "Ruining He; Julian J Mcauley"}, {"title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"title": "ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"title": "CHIME: Cross-passage hierarchical memory network for generative review question answering", "journal": "", "year": "2020", "authors": "Junru Lu; Gabriele Pergola; Lin Gui; Binyang Li; Yulan He"}, {"title": "Addressing complex and subjective product-related queries with customer reviews", "journal": "", "year": "2016", "authors": "Julian Mcauley; Alex Yang"}, {"title": "Justifying recommendations using distantly-labeled reviews and fine-grained aspects", "journal": "", "year": "2019", "authors": "Jianmo Ni; Jiacheng Li; Julian Mcauley"}, {"title": "Get to the point: Summarization with pointergenerator networks", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Abigail See; J Peter; Christopher D Liu;  Manning"}, {"title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; V Quoc;  Le"}, {"title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; R Russ; Quoc V Salakhutdinov;  Le"}, {"title": "Review-aware answer prediction for product-related questions incorporating aspects", "journal": "", "year": "2018", "authors": "Qian Yu; Wai Lam"}, {"title": "Responding E-commerce product questions via exploiting QA collections and reviews", "journal": "", "year": "2018", "authors": "Qian Yu; Wai Lam; Zihao Wang"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "5. 11Answer to RQ1 (Utilization of retrieved review for generating the answers)", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "11.99 14.44 11.91 RandomOD 14.25 11.38 14.04 11.53 RandomID 14.71 11.69 14.42 11.85 BM25QA 15.09 11.97 14.53 11.93 CHIME BM25Q 18.53 13.19 18.99 13.84 RandomOD 18.10 12.87 17.83 13.11 RandomID 17.95 12.81 17.98 13.17 BM25QA 17.99 12.84 17.85 13.11 Table 1: Performance of the OAAG and CHIME models with various sets of review snippets.", "figure_data": "SportsHomeR1RLR1RLTrainA13.01 10.13 14.36 11.35BM25Q15.01OAAG"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Performance of OAAG and CHIME models on template-based, descriptive categories of questions.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ": Performance of OAAG and CHIME models when trained with and without phrases that are not mean-ingful (IDK phrases)."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Examples of generated answers with high R1 score, but having incorrect answer. The R1 score is mentioned within the parenthesis.", "figure_data": ""}, {"figure_label": "A", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ".1: Dataset statistics.Question: Ref. Ans.: Gen. Ans.: Question: Ref. Ans.: Gen. Ans.: Question: Ref. Ans.: Gen. Ans.: Question: Ref. Ans.: Gen. Ans.: Question: Ref. Ans.: Gen. Ans.:does the sight come with batteries ? if not what batteries do i need to buy ? the sight did come with a battery , but if needed it would be a cr2032 . no , it does not come with a batteries . you need to buy a separate batteries . it 's a great scope for the money but it 's not worth it . (31.82) is it lined ? it does n't have a liner in it , but still makes for a very nice light winter coat ... no , it is not lined . it is a very nice jacket and i do n't think it 's too big for it . it 's a great deal . (22.22) does the bottle sweat ? and , does the straw have a cover ? does it sweat ? a little . not hardcore . just a little condensation on the bottom . and the straw does n't have a cover ... no , it does not have a sweat straw . it is a very good straw and i have n't had any issues with it . i have n't had any issues with it . (24.62) is it wood ? it is gorgeous , and yes it is wood no it is not wood . it 's a very nice piece of furniture . i would n't buy it again . it 's a very nice piece of furniture . (22.22) how many watt ? i do n't know but i am disappointed in mine . i wanted it to keep my coffee hot but it only keeps it warm . i do n't know , but i do n't know if it does n't work for me . i 've had mine for over a year and it works great . (39.29)Table A.2: Examples of answers generated by the OAAG model with high R1 scores. The R1 scores are within the parenthesis."}, {"figure_label": "A", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Ref. Ans.:   it never has with me and i have a high power microwave. really love to freeze in them. no ice buildup in them . Gen. Ans.:ihave not tried it in the microwave but it does melt and bubble a little when you press on. Ref. Ans.: they do come in red, white and cream colors. whoever you buy it from, email them right after you place your order so you have a better chance of getting red . Gen. Ans.: i don't know if you can get a red one but it is a great product and is easy to use with any size pasta or sauce you want to make in your kitchen . (27.27) Question: is the under side a refrigerator as well or just storage ? Ref. Ans.: its a storage not a refrigerator you can only store cups or water bottles . Gen. Ans.: it's a refrigerator, but you can put it in the freezer or in your car to cool off water and then use it for storage .(35.00) Table A.3: Examples of answers generated by the CHIME model with high R1 scores. The R1 scores are within the parenthesis.", "figure_data": "Question:will it melt or bubble in the microwave ?Question: Ref. Ans.: Gen. Ans.: Question: Ref. Ans.: Gen. Ans.: Question:(32.55) does this actually comes with storage box as shown which is not in description ? mine did not come with a storage box. and, it was not a very good knife. it grinds rather than cuts. yes, it comes with a storage box as shown in picture.it is very easy to use and easy to take apart and put back together (38.29) does it have an attachment to clean windows? there is a wider flat piece that you could use on windows... it works nicely . no, it does not have an attachment to clean windows but you can use a hose to do so if you need to clean them with water or some-thing similar like that in your kitchen .(20.40) could i have a red one ?.2 and Table A.3 shows some exampleswhere the generated answers have a high (> 20)"}], "doi": "10.1145/3289600.3290971"}