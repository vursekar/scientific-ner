{"authors": "Jianing Zhou; Suma Bhat", "pub_date": "", "title": "Paraphrase Generation: A Survey of the State of the Art", "abstract": "This paper focuses on paraphrase generation, which is a widely studied natural language generation task in NLP. With the development of neural models, paraphrase generation research has exhibited a gradual shift to neural methods in the recent years. This has provided architectures for contextualized representation of an input text and generating fluent, diverse and human-like paraphrases. This paper surveys various approaches to paraphrase generation with a main focus on neural methods.", "sections": [{"heading": "Introduction", "text": "Paraphrases are texts that convey the same meaning while using different words or sentence structures. The generation of paraphrases is a longstanding problem for natural language learning. For example, the question How do I improve my English could be equivalently phrased as What is the best way to learn English. Paraphrasing can be play an important role in language understanding tasks, such as question answering (Dong et al., 2017;Zhu et al., 2017), machine translation (Seraj et al., 2015;Thompson and Post, 2020a), and semantic parsing (Berant and Liang, 2014;. And it is also a good way for data augmentation (Kumar et al., 2019;Gao et al., 2020). Given a sentence, paraphrase generation aims to create its paraphrases that can have a different wording or different structure from the original sentence, while preserving the original meaning.\nThe focus of paraphrase generation has exhibited a gradual shift from classical approaches to more advanced neural approaches in the recent years with the rapid development of various neural models. Neural models have changed the traditional way paraphrase generation is performed and also provided new directions and architectures for the NLP community.\nWhile several surveys on the traditional methods and limited neural methods for paraphrase gener-", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Sentences", "text": "Paraphrases How do I improve my English What is the best way to learn English", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "How far is Earth from Sun", "text": "What is the distance between Sun and Earth if at any time in the preparation of this product the integrity of this container is compromised it should not be used .\nthis container should not be used if the product is compromised at any time in preparation . ation have been published (Metzler et al., 2011;Gupta and Krzy\u017cak, 2020), there is no thorough and comprehensive survey on neural methods for paraphrase generation. To our best knowledge, this is the first survey on neural methods for paraphrase generation. Therefore, our goal in this paper is to provide a timely survey on paraphrase generation, with a main focus on neural methods.\nIn the following section, we will first introduce the most frequently used datasets for paraphrase generation (Section 2). Then we list the traditional evaluation metrics in Section 3. In Section 4, we present some of the traditional approaches that were used before the neural methods. Neural models, the main focus of this paper, will be discussed in Section 5. After introducing all the methods, we compare the performance of the different models for paraphrase generation in Section 6. Finally, we identify some research gaps in paraphrase generation.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Datasets", "text": "In this section, we describe several datasets that have been extensively used for paraphrase generation.\nPPDB The paraphrase database (Ganitkevitch et al., 2013)   WikiAnswer This dataset (Fader et al., 2013) contains approximately 18 million word-aligned question pairs that are paraphrases. The word alignments provided by this dataset also relate the synonyms in the paraphrase sentences. However, all the sentences provided in this dataset are questions, which restricts the paraphrases to only questions.\nMSCOCO MSCOCO (Lin et al., 2014)  ParaNMT ParaNMT ) is a dataset of more than 50 million English-English sentential paraphrase pairs. The pairs were generated automatically by using back-translation to translate the non-English side of a large Czech-English parallel corpus. Owing to its recency, it has not been used widely.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Evaluation Methods", "text": "Two general types of evaluation metrics are commonly used to evaluate paraphrase generation: automatic evaluation and human evaluation.\nAutomatic Evaluation Several automatic evaluation metrics are used for the evaluation of paraphrase generation. The widely-used metrics include (1) BLEU (Papineni et al., 2002), which was originally developed to evaluate machine translation systems;\n(2) METEOR (Denkowski and Lavie, 2014), which aims to address BLEU's weakness of being unable to measure semantic equivalents when applied to low-resource languages and has a better correlation with human judgment at the sentence/segment level than BLEU; (3) ROUGE (Lin, 2004), a recall-based evaluation metric originally developed for text summarization, has also been used to evaluate paraphrase generation. Its versions, ROUGE-N (computing the n-gram recall) and ROUGE-L (focusing on the longest common subsequence) are mostly used. (4) TER (Snover et al., 2006), which was also developed to evaluate machine translation. It measures the number of edits that a human translator would have to perform to change a translation so it exactly matches a reference translation. A TER score is a value in the range of 0-1, but is frequently presented as a percentage, where lower is better.\nHuman Evaluation Due to the fact that automatic evaluation metrics mainly focus on the ngram overlaps instead of meaning, human evaluation is used to provide a more accurate and qualitative evaluation of the generated output. In human evaluation, human annotators are asked to score generated paraphrases along multiple dimensions of quality such as similarity, clarity, and fluency.\nOwing to the manual annotation efforts, human evaluation is naturally more costly compared to automatic evaluation, but more representative of the quality of the generated output.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Traditional Approaches", "text": "In this section, some traditional approaches without neural models will be introduced.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Rule-Based Approaches", "text": "Rule-based paraphrase generation approaches build on hand-crafted or automatically collected paraphrase rules. In the early works, these rules were mainly hand-crafted (McKeown, 1983). Due to the significant manual efforts, some researchers have sought to collect paraphrase rules automatically (Lin and Pantel, 2001;Barzilay and Lee, 2003). However, the limitation of the extracting methods has led to the generation of long and complex paraphrase patterns, in turn impacting performance.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Thesaurus-Based Approaches", "text": "This approach usually generates paraphrases by substituting some words in the source sentences with their synonyms extracted from a thesaurus (Bolshakov and Gelbukh, 2004;Kauchak and Barzilay, 2006). Thesaurus-based approaches proceed by first extracting all synonyms from a thesaurus for the words to be replaced. Then the optimal candidate is selected according to the context in the source sentence. Although simple and effective, this approach is severely limited by the diversity of the generated paraphrases.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "SMT-Based Approach", "text": "This approach is based on statistical machine translation (SMT) and is motivated by the fact that paraphrase generation can be seen as a special case of machine translation (i.e., monolingual machine translation). A machine translation model normally finds a best translation\u00ea of a text in language f to a text in language e by utilizing a statistical translation model p(f |e) and a language model p(e):\ne = arg max e\u2208e * p(f |e)p(e) Applying this idea to paraphrase generation, such a model will find a best paraphraset of a text in the source side s to a text in the target side t obtained as,t = arg max t\u2208t * p(s|t)p(t) For instance, (Wubben et al., 2010) constructed a large-scale parallel corpus containing paraphrases collected from the headlines that appeared in Google News. Then they trained a Phrase-Based Machine Translation model (PBMT) (Koehn et al., 2007) on their parallel corpus using the MOSES package. The trained PBMT is finally used to generate paraphrases.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Neural Approaches", "text": "Early works on paraphrasing mainly focused on template-based or statistical machine translation approaches. However, the matching of templates and modeling of a statistical translation model are both challenging tasks. With the recent advances of neural networks, especially the sequence-to-sequence framework, Seq2Seq models were first use for paraphrase generation by (Prakash et al., 2016). Their work inspired the wide use of neural models for paraphrase generation. Below we introduce the main approaches based on neural models that are used for paraphrase generation.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Encoder-Decoder Architecture", "text": "Currently, most of the existing paraphrase generation models are based on sequence-to-sequence models consisting of an encoder and a decoder. The encoder will encode the source texts into a contextualized vector representation along with a list of vector representations capturing the semantics of each word and context. Then, the decoder will generate paraphrases based on the vectors given by the encoder.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Encoding Side", "text": "The main purpose of encoding is to extract the semantic information for the decoder to generate paraphrases. With the development of various neural models, researchers also have multiple choices for the encoder.\nEncoder With a consistent goal of learning better abstract contextualized representation of the input text, several architectures have been explored by researchers. (Prakash et al., 2016) first utilized a seq2seq model implemented as recurrent neural networks-long short term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997)-to process long sequences. A nonvolutional neural network (CNN) has also been used to construct seq2seq models as a CNN has fewer parameters and thus is faster to train (Vizcarra and Ochoa-Luna, 2020). The Transformer model (Vaswani et al., 2017) has shown state-of-the-art performance on multiple text generation tasks. Due to the Transformer's improved ability to capture long-range dependencies in sentences,  utilized a Transformer to construct their seq2seq model. More recently, large language models using transformer architectures have achieved state-ofthe-art results for many NLP tasks while using less supervised data than before. Therefore, some researchers also utilized large pretrained language models such as GPT-2 (Radford et al., 2019) and BART (Lewis et al., 2020) as their encoder-decoder framework (Witteveen and Andrews, 2019;Hegde and Patil, 2020;Garg et al., 2021).", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Decoding Side", "text": "At the decoding side, the contextualized representation is used at each decoding step with the vector representation of previously generated words. Finally, a distribution over the vocabulary is obtained and the word with highest probability will be generated. This method is greedy decoding. Besides, a more commonly used method called beam search (Wiseman and Rush, 2016) is used, which identifies the k-best paths up to current timestep during decoding.\nHowever, greedy decoding and beam search methods are both generic approaches for all text generation tasks without a specific focus on paraphrase generation. Therefore, with the goal of generating paraphrases and avoiding the words existing in the source sentences, a few blocking mechanisms have been proposed to prevent the decoder from generating the same words in the source sentences. This is also a way to guarantee the diversity of the generated paraphrases and prevent the models from directly copying the input into the output paraphrases (Niu et al., 2020;Thompson and Post, 2020b).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Improvements Based on", "text": "Encoder-Decoder Architecture\nThe numerous attempts that have been made to improve the Encoder-Decoder architecture for paraphrase generation can be broadly categorized into two types based on their focus: A. Model-focused; and B. Attribute-focused. Next we introduce them respectively with more fine-grained divisions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A. Model-focused", "text": "Model-focused improvements only aim to utilize various mechanisms to enhance the encoder or the decoder without paying special attention to the attributes of the generated paraphrases (e.g., granularity level such as word-level, phrase-level and sentence-level).\nAttention The Attention mechanism (Bahdanau et al., 2015) enables the decoder to focus on some words/phrases that are of high relevance when generating a word. First, a weight for each token in the source sequence in each timestep is computed to indicate the importance, emphasizing the important information from the input and de-emphasizing the unimportant information. Given the weight distribution over all the tokens in the source sequence, this extra input vector, the context vector, is provided to the decoder..\nCopy To counter the effect of rare and outof-vocabulary words in neural sequence models, (Vinyals et al., 2015) proposed a pointer network.\nA pointer network copies an element from the input sequence directly into the output. Similarly, copy mechanism copies a span of elements from the input sequence decided by attention mechanism directly into the output. With copy mechanism, the decoder is able to determine whether a generate mode or a copy mode should be used at each timestep. First introduced by Gu et al. (2016) for abstractive summarization, Cao et al. (2017) haev also applied the copy mechanism to paraphrase generation. Despite the advantage of generating well-formed paraphrases by using the copy mechanism, it leads to the undesirable consequence of making a paraphrase contain many of the phrases in the original sentence and limits diversity. This calls for a controlled use of the copy mechanism during paraphrase generation.  Gupta et al. (2017) implemented the encoder and decoder with LSTMs, whereas the transformer is utilized by Roy and Grangier (2019).\nReinforcement Learning As pointed out by (Ranzato et al., 2015), a well-known problem of the encoder-decoder architecture is exposure bias: the decoding of current word is conditioned on the gold references during training but on the generated output from the last timestep during testing. Therefore, the error might be accumulated and propagated when testing. Another problem lies in the mismatch between the training goal and the evaluation metrics. While the generated paraphrases are finally evaluated automatically using the previously mentioned metrics, the network is trained to maximize the probability of generating the reference paraphrases. Therefore, minimizing the training loss might not correspond to optimizing the evaluation metric. To address this limitation, reinforcement learning (RL) is leveraged. RL aims to train an agent to interact with the environment with the goal of maximizing its reward. Toward finding an optimal policy, RL can be used to maximize the reward indicated as a desired evaluation metric or a combination of multiple desired metrics. Rather than minimizing loss (the conventional approach),  first utilized RL to maximize the reward given by an evaluator which outputs a real value to represent the matching degree between two sentences as paraphrases of each other. Other reward functions have been explored by researchers, including ROUGE score, perplexity score and language fluency (Siddique et al., 2020;.\nGenerative adversarial networks (GAN) Proposed by Goodfellow et al. (2014), GANs consist of generators and discriminators, where generators try to generate realistic outputs that match the real distribution and discriminators try to distinguish between the samples generated by generators and the samples that are real. GAN is originally trained by minimax optimization proposed in (Goodfellow et al., 2014). However, when GAN is applied in text generation, the traditional training method cannot be used because generating discrete words is non-differentiable. Therefore, the idea of policy gradient (Sutton et al., 1999) is leveraged to solve this problem (Yu et al., 2017). With policy gradient applied, discriminators act like the reward function in RL. Moreover, different discriminators can provide different desired rewards and thus equip the model with the capacity to generating text with different conditions. Here, a model is usually trained in an adversarial way: generators and discriminators are first pretrained, then generators are trained to maximize the loss of the fixed discriminators, then generators are fixed and discriminators are again trained to minimize the loss by provided the real samples and the samples generated by the fixed generators. For the task of paraphrase generation, different discriminators are designed to distinguish between generated samples and real samples, paraphrases and non-paraphrases (Yang et al., 2019;Vizcarra and Ochoa-Luna, 2020).", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "B. Attribute-focused", "text": "For attribute-focused improvements, their purpose is to improve the quality of generated paraphrases in some specific aspects such as diversity and also provide control over some attributes of generated paraphrases such as syntax and granularity level. These attribute-focused works usually use the previously mentioned models as their backbone models.\nBased on the backbone models, different mechanisms are applied for different focuses.\nDiversity Attempts focusing on diversity aim to generate multiple diverse paraphrases for a given sentence. Some works control diversity by provid-  ing control signals to the decoder. Random pattern embeddings are used by . (Kumar et al., 2019) utilized a submodular mechanism to maximize submodular functions measuring fidelity and diversity. ,  and (Cao and Wan, 2020) all generate diverse paraphrases by providing the decoder with different latent patterns as control signal. Furthermore, (Cao and Wan, 2020) also incorporated their model with a diversity loss to control diversity.  use RL with multiple reward functions to generate diverse paraphrases. One of the reward functions computes ROUGE score between a generated sentence and original sentence, which can focus on the word variations and diversity. Acting like a reward function in RL, discriminators naturally can be used to provide control over some desired attributes. (Qian et al., 2019) utilized multiple generators in GAN to generate multiple diverse paraphrases. A generator discriminator is used to distinguish sentences generated by different generators and guarantee the generated paraphrases are diverse enough.\nWord-Level Works on word-level paraphrasing mainly focus on generating paraphrases by replacing original words in the source texts with synonyms. Some works leveraged external linguistic knowledge (Cao et al., 2017;Lin et al., 2020). (Cao et al., 2017) utilized an alignment table capturing many synonym mappings based on the IBM Model (Chahuneau et al., 2013). (Lin et al., 2020) utilized WordNet (Miller, 1995 to retrieve synonyms.\nOther works instead proposed special mechanisms to learn a mapping of synonyms (Ma et al., 2018;Fu et al., 2019). For example, (Ma et al., 2018) utilized retrieved-based method to learn such a mapping. (Fu et al., 2019) incorporates a novel latent bag-of-word mechanism into seq2seq model for content planning, which mainly provides candidate synonyms for words in the source texts. However, generating paraphrases only on a word-level makes the quality and diversity of generated paraphrases limited. Therefore, paraphrasing has also been studied on other granularity level, e.g. syntax level.\nSyntax Works in this category explore methods to provide control over the syntax of generated paraphrases. Basically, all the methods used by previous works can be split into two classes: 1. Explicit Control and 2. Implicit Control. Methods in the first class first encode the syntax tree of an exemplar sentence into a list of vector representations and then feed them into decoder at each timestep when decoding (Iyyer et al., 2018;Chen et al., 2019;Goyal and Durrett, 2020;Kumar et al., 2020). These methods can provide explicit control over the syntax of generated paraphrases and thus has better interpretability. The second class of methods will first learn a distribution over syntax information by VAE. Then a latent syntax variable sampled from the learned distribution will be fed into decoder at each decoding step . Although the control provided by this method is implicit, it does not require exemplar sentences and also can group multiple related syntax under the same latent assignment.\nMulti-Level Focusing on a single granularity level of paraphrasing still makes generated paraphrases limited. Therefore, researchers also explore methods to combine multiple granularity levels together. Such attempts equip their model with the capacity of generating synonyms, substituting phrases and also rearrange sentential structures (Li et al., 2019;Huang et al., 2019;Kazemnejad et al.,   2020). By using multiple encoders, (Li et al., 2019) and (Kazemnejad et al., 2020) both enable their models to capture paraphrasing patterns on different granularity levels. (Huang et al., 2019) instead utilized the help of external linguistic knowledge from the paraphrase database (Ganitkevitch et al., 2013) to retrieve and learn word-level and phraselevel paraphrases. With different methods, both of them successfully combine multiple granularity levels together when generating paraphrases.\n6 State-of-the-Art Performance\nTable 3 shows the ROUGE and BLEU scores of state-of-the-art performance on some most frequently used evaluation corpus in recent years: MSCOCO and Quora. Due to the facts that different metrics are used in different works, different datasets are used in different works and many of them did not release their codes, Table 3 is not fully filled. However, with most of the table filled, we can still have some observations worth mentioning. First, the use of attention mechanism achieves a close performance on Quora but has a better performance on MSCOCO (row 1 and 2). Similarly, the simple application of VAE also achieves a close performance on Quora but further improves the performance on MSCOCO (row4). With the copy mechanism, the Seq2Seq model is able to retain some words and thus yields a much better results (row 3). Transformer (row 5) outperforms all the Seq2Seq-based models without copy mechanism (row 1,2,4,6), which shows the advantages of Transformer and meanwhile also proves the effectiveness of copy mechanism.\nSecond, a model that employs RL (row 7) has a great advantage for generating better paraphrases because of the reward provided. Therefore, a well designed optimization goal plays an important role in the task of paraphrase generation.\nThird, a novel decoding algorithm based on large pretrained language models helps to generate better paraphrases at the word level (row 8) because of the strength of large pretrained language models and the synonyms learned by decoding algorithm.\nFourth, the attempts to improve paraphrase generation with a special focus on combining multiple granularity levels also yield good performance (row 9,10). When learning to generate paraphrase in word level, phrase level and sentence level at the same time, their models improve the performance on multiple metrics compared with their backbone Transformer model (row 5).\nFinally, incorporating syntax control into paraphrase generation will also yield better results at word level and sentence level (row 11,12). Compared with implicit control (row 11), explicit control has a much better performance (row 12) based on Quora.\nIt should be noted that most of the works utilize two datasets for experiments (as shown in Table 4) with one of them focusing on question paraphrases and the other focusing on general sentence paraphrases. Quora is the most popular dataset for question paraphrases. However, for corpus focusing on general sentnece paraphrases, different works have different choices among MSCOCO, Twitter URL and ParaNMT. MSCOCO is more preferred for less noise compared with Twitter URL and ParaNMT. Therefore, a combination of MSCOCO and Quora is more reasonable.\nFor evaluation metrics, BLEU is the most frequently used one. However, as proposed by (Niu et al., 2020) (Niu et al., 2020).\nbecause of \"curse of BLEU on paraphrase evaluation\". As shown in Table 5, examples with low BLEU scores might include both relatively good and bad paraphrasing because BLEU scores only measure the overlap between outputs and references. However, a generated paraphrase might still be a good paraphrase even it is not same with the reference. Therefore, for evaluation, it is better to combine automatic evaluation metrics and human evaluation together for a more comprehensive evaluation.", "n_publication_ref": 28, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "Although recent neural models have shown great advances, state-of-the-art results are still not satisfactory enough. Therefore, more advanced paraphrasing models still need to be explored. Below we discuss several potential directions of research that we believe are worth studying.\nPretrained language models Virtually all recent work related to the application of pretrained language models on paraphrase generation is quite naive. Therefore, we could combine the large pretrained language models with other mechanisms, for example reinforcement learning, VAE and GAN.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Multi-level controllable paraphrase generation", "text": "Most recent works on multi-level paraphrase generation only focus on word-level paraphrasing and phrase-level paraphrasing. However, more granularity levels can be incorporated. We believe it is worthwhile to study the combination of various levels, including word-level, phrase-level, syntaxlevel and sentence-level.\nTransfer learning With the goal of generating different surfaces of given sentences while preserving the meaning, text summarization, text simplification and paraphrase generation are essentially similar. Therefore, one could utilize transfer learning of these three tasks to improve the performance.\nStylistic paraphrase generation Currently, word-and phrase-substitution in paraphrase gener-ation cannot be carefully controlled. Therefore, it is hard to control the style of generated paraphrases. We believe it is worthwhile to explore methods of incorporating specific styles into generated paraphrases. For instance, by controlling the types of words and phrases, we can incorporate metaphor and idiomatic expressions into paraphrases (Zhou et al., 2021b,a), which could also help to enhance creativity and diversity of generated paraphrases.\nEvaluation metrics As stated in Section 6, BLEU scores and other automatic evaluation metrics based on similar principle are not good enough to evaluate paraphrase generation. Thus there is a need for better automatic evaluation methods. One possible method is to utilize paraphrase identification in the automatic evaluation metrics to explicitly provide an evaluation of if the generated sentence and input sentence are paraphrases.", "n_publication_ref": 1, "n_figure_ref": 0}], "references": [{"title": "Towards diverse paraphrase generation using multi-class wasserstein gan", "journal": "", "year": "2019", "authors": "Zhecheng An; Sicong Liu"}, {"title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2015", "authors": "Dzmitry Bahdanau; Kyung Hyun Cho; Yoshua Bengio"}, {"title": "Learning to paraphrase: An unsupervised approach using multiple-sequence alignment", "journal": "", "year": "2003", "authors": "Regina Barzilay; Lillian Lee"}, {"title": "Semantic parsing via paraphrasing", "journal": "Long Papers", "year": "2014", "authors": "Jonathan Berant; Percy Liang"}, {"title": "Synonymous paraphrasing using wordnet and internet", "journal": "Springer", "year": "2004", "authors": "A Igor; Alexander Bolshakov;  Gelbukh"}, {"title": "Unsupervised dual paraphrasing for two-stage semantic parsing", "journal": "", "year": "2020", "authors": "Ruisheng Cao; Su Zhu; Chenyu Yang; Chen Liu; Rao Ma; Yanbin Zhao; Lu Chen; Kai Yu"}, {"title": "Divgan: Towards diverse paraphrase generation via diversified generative adversarial network", "journal": "", "year": "2020", "authors": "Yue Cao; Xiaojun Wan"}, {"title": "Joint copying and restricted generation for paraphrase", "journal": "", "year": "2017", "authors": "Ziqiang Cao; Chuwei Luo; Wenjie Li; Sujian Li"}, {"title": "Translating into morphologically rich languages with synthetic phrases", "journal": "", "year": "2013", "authors": "Victor Chahuneau; Eva Schlinger; A Noah; Chris Smith;  Dyer"}, {"title": "Controllable paraphrase generation with a syntactic exemplar", "journal": "", "year": "2019", "authors": "Mingda Chen; Qingming Tang; Sam Wiseman; Kevin Gimpel"}, {"title": "A semantically consistent and syntactically variational encoder-decoder framework for paraphrase generation", "journal": "", "year": "2020", "authors": "Wenqing Chen; Jidong Tian; Liqiang Xiao; Hao He; Yaohui Jin"}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "journal": "", "year": "2014", "authors": "Michael Denkowski; Alon Lavie"}, {"title": "Learning to paraphrase for question answering", "journal": "", "year": "2017", "authors": "Li Dong; Jonathan Mallinson; Siva Reddy; Mirella Lapata"}, {"title": "Paraphrase-driven learning for open question answering", "journal": "Long Papers", "year": "2013", "authors": "Anthony Fader; Luke Zettlemoyer; Oren Etzioni"}, {"title": "Paraphrase generation with latent bag of words", "journal": "", "year": "2019", "authors": "Yao Fu; Yansong Feng; John P Cunningham"}, {"title": "Ppdb: The paraphrase database", "journal": "", "year": "2013", "authors": "Juri Ganitkevitch; Benjamin Van Durme; Chris Callison-Burch"}, {"title": "Paraphrase augmented task-oriented dialog generation", "journal": "", "year": "2020", "authors": "Silin Gao; Yichi Zhang; Zhijian Ou; Zhou Yu"}, {"title": "Unsupervised contextual paraphrase generation using lexical control and reinforcement learning", "journal": "", "year": "2021", "authors": "Sonal Garg; Sumanth Prabhu; Hemant Misra; G Srinivasaraghavan"}, {"title": "Generative adversarial nets", "journal": "Curran Associates, Inc", "year": "2014", "authors": "Ian Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio"}, {"title": "Neural syntactic preordering for controlled paraphrase generation", "journal": "", "year": "2020", "authors": "Tanya Goyal; Greg Durrett"}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "journal": "Long Papers", "year": "2016", "authors": "Jiatao Gu; Zhengdong Lu; Hang Li; O K Victor;  Li"}, {"title": "A deep generative framework for paraphrase generation", "journal": "", "year": "2017", "authors": "Ankush Gupta; Arvind Agarwal; Prawaan Singh; Piyush Rai"}, {"title": "An empirical evaluation of attention and pointer networks for paraphrase generation", "journal": "Springer", "year": "2020", "authors": "Varun Gupta; Adam Krzy\u017cak"}, {"title": "Unsupervised paraphrase generation using pre-trained language models", "journal": "", "year": "2020", "authors": "Chaitra Hegde; Shrikumar Patil"}, {"title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"title": "Dictionary-guided editing networks for paraphrase generation", "journal": "", "year": "2019", "authors": "Shaohan Huang; Yu Wu; Furu Wei; Zhongzhi Luan"}, {"title": "Adversarial example generation with syntactically controlled paraphrase networks", "journal": "Long Papers", "year": "2018", "authors": "Mohit Iyyer; John Wieting; Kevin Gimpel; Luke Zettlemoyer"}, {"title": "Paraphrasing for automatic evaluation", "journal": "", "year": "2006", "authors": "David Kauchak; Regina Barzilay"}, {"title": "Paraphrase generation by learning how to edit from samples", "journal": "", "year": "2020", "authors": "Amirhossein Kazemnejad; Mohammadreza Salehi; Mahdieh Soleymani Baghshah"}, {"title": "Semi-supervised learning with deep generative models", "journal": "", "year": "2014", "authors": "P Diederik; Danilo J Kingma; Shakir Rezende; Max Mohamed;  Welling"}, {"title": "Moses: Open source toolkit for statistical machine translation", "journal": "", "year": "2007", "authors": "Philipp Koehn; Hieu Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; Nicola Bertoldi; Brooke Cowan; Wade Shen; Christine Moran; Richard Zens"}, {"title": "Syntax-guided controlled generation of paraphrases", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Ashutosh Kumar; Kabir Ahuja; Raghuram Vadapalli; Partha Talukdar"}, {"title": "Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation", "journal": "", "year": "2019", "authors": "Ashutosh Kumar; Satwik Bhattamishra; Manik Bhandari; Partha Talukdar"}, {"title": "A continuously growing dataset of sentential paraphrases", "journal": "", "year": "2017", "authors": "Wuwei Lan; Siyu Qiu; Hua He; Wei Xu"}, {"title": "Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"title": "Paraphrase generation with deep reinforcement learning", "journal": "", "year": "2018", "authors": "Zichao Li; Xin Jiang; Lifeng Shang; Hang Li"}, {"title": "Decomposable neural paraphrase generation", "journal": "", "year": "2019", "authors": "Zichao Li; Xin Jiang; Lifeng Shang; Qun Liu"}, {"title": "Rouge: A package for automatic evaluation of summaries", "journal": "", "year": "2004", "authors": "Chin-Yew Lin"}, {"title": "Discovery of inference rules for question-answering", "journal": "Natural Language Engineering", "year": "2001", "authors": "Dekang Lin; Patrick Pantel"}, {"title": "Microsoft coco: Common objects in context", "journal": "Springer", "year": "2014", "authors": "Tsung-Yi Lin; Michael Maire; Serge Belongie; James Hays; Pietro Perona; Deva Ramanan; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"title": "Integrating linguistic knowledge to sentence paraphrase generation", "journal": "", "year": "2020", "authors": "Zibo Lin; Ziran Li; Ning Ding; Hai-Tao Zheng; Ying Shen; Wei Wang; Cong-Zhi Zhao"}, {"title": "2020. A learning-exploring method to generate diverse paraphrases with multi-objective deep reinforcement learning", "journal": "", "year": "", "authors": "Mingtong Liu; Erguang Yang; Deyi Xiong; Yujie Zhang; Yao Meng; Changjian Hu; Jinan Xu; Yufeng Chen"}, {"title": "Query and output: Generating words by querying distributed word representations for paraphrase generation", "journal": "", "year": "2018", "authors": "Shuming Ma; Xu Sun; Wei Li; Sujian Li; Wenjie Li; Xuancheng Ren"}, {"title": "Paraphrasing questions using given and new information", "journal": "American Journal of Computational Linguistics", "year": "1983", "authors": "Kathleen Mckeown"}, {"title": "An empirical evaluation of data-driven paraphrase generation techniques", "journal": "", "year": "2011", "authors": "Donald Metzler; Eduard Hovy; Chunliang Zhang"}, {"title": "Wordnet: a lexical database for english", "journal": "Communications of the ACM", "year": "1995", "authors": "A George;  Miller"}, {"title": "Nitish Shirish Keskar, and Caiming Xiong. 2020. Unsupervised paraphrase generation via dynamic blocking", "journal": "", "year": "", "authors": "Tong Niu; Semih Yavuz; Yingbo Zhou; Huan Wang"}, {"title": "Bleu: a method for automatic evaluation of machine translation", "journal": "", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"title": "Neural paraphrase generation with stacked residual lstm networks", "journal": "", "year": "2016", "authors": "Aaditya Prakash; A Sadid; Kathy Hasan; Vivek Lee; Ashequl Datla; Joey Qadir; Oladimeji Liu;  Farri"}, {"title": "Exploring diverse expressions for paraphrase generation", "journal": "", "year": "2019", "authors": "Lihua Qian; Lin Qiu; Weinan Zhang; Xin Jiang; Yong Yu"}, {"title": "Language models are unsupervised multitask learners", "journal": "OpenAI blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"title": "Sequence level training with recurrent neural networks", "journal": "", "year": "2015", "authors": "Aurelio Marc; Sumit Ranzato; Michael Chopra; Wojciech Auli;  Zaremba"}, {"title": "Unsupervised paraphrasing without translation", "journal": "", "year": "2019", "authors": "Aurko Roy; David Grangier"}, {"title": "Improving statistical machine translation with a multilingual paraphrase database", "journal": "", "year": "2015", "authors": "Maryam Ramtin Mehdizadeh Seraj; Anoop Siahbani;  Sarkar"}, {"title": "Unsupervised paraphrasing via deep reinforcement learning", "journal": "", "year": "2020", "authors": "Samet Ab Siddique; Vagelis Oymak;  Hristidis"}, {"title": "A study of translation edit rate with targeted human annotation", "journal": "Citeseer", "year": "2006", "authors": "Matthew Snover; Bonnie Dorr; Richard Schwartz; Linnea Micciulla; John Makhoul"}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "journal": "Citeseer", "year": "1999", "authors": "S Richard; David A Sutton;  Mcallester; P Satinder; Yishay Singh;  Mansour"}, {"title": "Automatic machine translation evaluation in many languages via zero-shot paraphrasing", "journal": "", "year": "2020", "authors": "Brian Thompson; Matt Post"}, {"title": "Paraphrase generation as zero-shot multilingual translation: Disentangling semantic similarity from lexical and syntactic diversity", "journal": "", "year": "2020", "authors": "Brian Thompson; Matt Post"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Pointer networks", "journal": "", "year": "2015", "authors": "Oriol Vinyals; Meire Fortunato; Navdeep Jaitly"}, {"title": "Paraphrase generation via adversarial penalizations", "journal": "", "year": "2020", "authors": "Gerson Vizcarra; Jose Ochoa-Luna"}, {"title": "A task in a suit and a tie: paraphrase generation with semantic augmentation", "journal": "", "year": "2019", "authors": "Su Wang; Rahul Gupta; Nancy Chang; Jason Baldridge"}, {"title": "Paranmt-50m: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations", "journal": "Long Papers", "year": "2018", "authors": "John Wieting; Kevin Gimpel"}, {"title": "Sequence-to-sequence learning as beam-search optimization", "journal": "", "year": "2016", "authors": "Sam Wiseman; Alexander M Rush"}, {"title": "Paraphrasing with large language models", "journal": "", "year": "2019", "authors": "Sam Witteveen; Martin Andrews"}, {"title": "Paraphrase generation as monolingual translation: Data and evaluation", "journal": "", "year": "2010", "authors": " Sander Wubben;  Van Den; Emiel Bosch;  Krahmer"}, {"title": "D-page: Diverse paraphrase generation", "journal": "", "year": "2018", "authors": "Qiongkai Xu; Juyan Zhang; Lizhen Qu; Lexing Xie; Richard Nock"}, {"title": "An endto-end generative architecture for paraphrase generation", "journal": "", "year": "2019", "authors": "Qian Yang; Dinghan Shen; Yong Cheng; Wenlin Wang; Guoyin Wang; Lawrence Carin"}, {"title": "Seqgan: Sequence generative adversarial nets with policy gradient", "journal": "", "year": "2017", "authors": "Lantao Yu; Weinan Zhang; Jun Wang; Yong Yu"}, {"title": "PIE: A parallel idiomatic expression corpus for idiomatic sentence generation and paraphrasing", "journal": "", "year": "2021", "authors": "Jianing Zhou; Hongyu Gong; Suma Bhat"}, {"title": "From solving a problem boldly to cutting the gordian knot: Idiomatic text generation", "journal": "", "year": "2021", "authors": "Jianing Zhou; Hongyu Gong; Srihari Nanniyur; Suma Bhat"}, {"title": "Knowledge-based question answering by jointly generating, copying and paraphrasing", "journal": "", "year": "2017", "authors": "Shuguang Zhu; Xiang Cheng; Sen Su; Shuang Lang"}], "figures": [{"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Examples of paraphrases from available datasets for paraphrase generation.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Highlights of primarily used paraphrase generation datasets. Gold Size represents the size of the subset used for paraphrase generation when the original dataset was not used for generation. Length is the the average number of words per sentence and Char Len is the average number of characters per sentence.", "figure_data": "phrase patterns that capture meaning-preservingsyntactic transformations. Each paraphrase pair inPPDB contains a set of associated scores includingparaphrase probabilities and monolingual distribu-tional similarity scores. Despite its size and variety,because this dataset only contains phrasal and lexi-cal paraphrases without any sentence paraphrases,it has recently fallen out of use."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "State-of-the-art performance on Quora dataset and MSCOCO dataset.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Datasets and evaluation metrics used by different works on paraphrase generation. Twitter represents Twitter URL corpus. Only(Prakash et al., 2016) used PPDB. Therefore, we did not include PPDB into this table.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": ", current automatic evaluation metrics are limited for evaluating paraphrase generation", "figure_data": "BLEU Quality TargetGenerated ParaphraseHighHigha picture of someone taking a picture of herself a woman taking a picture with a cell phone.HighLowa batter swinging a bat at a baseballa batter swinging a baseball at a batLowHigha man in sunglasses laying on a green kayak.the man lying on a boat in the water.LowLowpeople on a gold course enjoy a few gamesa group of peaple walking"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Samples of generated paraphrases and their quality. Selected from", "figure_data": ""}], "doi": "10.18653/v1/2021.mwe-1.5"}