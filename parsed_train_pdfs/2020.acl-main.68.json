{"authors": "Piji Li; Haisong Zhang; Xiaojiang Liu; Shuming Shi", "pub_date": "", "title": "Rigid Formats Controlled Text Generation", "abstract": "Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation. 1   ", "sections": [{"heading": "Introduction", "text": "Recent years have seen the tremendous progress in the area of natural language generation especially benefiting by the neural network models such as Recurrent Neural Networks (RNN) or Convolutional Neural Networks (CNN) based sequence-tosequence (seq2seq) frameworks (Bahdanau et al.,1 Code: http://github.com/lipiji/SongNet Let me not to the marriage of true minds Admit impediments, love is not love", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Which alters when it alteration finds", "text": "Or bends with the remover to remove.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Lyrics SongCi", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Sonnet", "text": "Figure 1: Examples of text with rigid formats. In lyrics, the syllables of the lyric words must align with the tones of the notation. In SongCi and Sonnet, there are strict rhyming schemes and the rhyming words are labeled in red color and italic font.\n2014; Gehring et al., 2017), Transformer and its variants (Vaswani et al., 2017;, pre-trained auto-regressive language models such as XLNet  and GPT2 (Radford et al., 2019), etc. Performance has been improved significantly in lots of tasks such as machine translation Vaswani et al., 2017), dialogue systems (Vinyals and Le, 2015;Shang et al., 2015;Li, 2020), text summarization (Rush et al., 2015;Li et al., 2017;See et al., 2017), story telling (Fan et al., 2018;See et al., 2019), poetry writing (Zhang and Lapata, 2014;Lau et al., 2018;Liao et al., 2019), etc.\nGenerally, most of the above mentioned tasks can be regarded as free text generation, which means that no constraints on the format and structure, say the number of words and rhyming rules. Note that tasks of dialogue generation and story telling are almost in an open-ending generation style as long as the generated content is relevant with the conditional input text. Although there are formats constraints on the poetry text, the proposed models just treat the formats as kind of latent information and let the model capture this feature implicitly during training (Liao et al., 2019). The model trained on the five-character quatrain corpus cannot generate seven-character verses. Moreover, it is impossible to trigger these models to generate satisfying results according to arbitrary new defined formats.\nIn practice we will confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet (say Shakespeare's Sonnets (Shakespeare, 2000)), SongCi (a kind of Ci. Ci is a type of lyric poetry in the tradition of Classical Chinese poetry. 2 , SongCi is the Ci created during Song dynasty), etc., and some examples are illustrated in Figure 1. The typical characteristics of these text can be categorized into three folds: (1) The assembling of text must comply fully with the predefined rigid formats. Assume that the music score is composed, then the lyricist must fill the lyric content strictly tally with the schemes lie in the notation. Take partial of song \"Edelweiss\" as shown in the first row of Figure 1 as example, the syllables of the lyric words must align with the tones of the notation. The second row of Figure 1 depicts the content of a SongCi created based on the CiPai of \"Bu Suan Zi\". Given the CiPai, the number of characters and the syntactical structure of the content are also defined (e.g., the number of characters of each clause: 5, 5. 7, 5. 5, 5. 7, 5.). (2) The arrangement of the content must obey the defined rhyming schemes. For example, all the final words (words in red color and italic font) of the SongCi content in Figure1 are rhyming (the spelling of each word is: \"zhu\", \"yu\", \"du\", and \"gu\".). The example in the third row of Figure 1 comes from Shakespeare's \"Sonnet 116\" (Shakespeare, 2000), the first four sentences. Usually, the rhyming schemes of Shakespeare's Sonnets is \"ABAB CDCD EFEF GG\" 3 . In the example, the rhyming words in scheme \"ABAB\" are \"minds\", \"love\", \"finds\", and \"remove\". (3) Even though the format is rigid, the sentence integrity must always be guaranteed. Incomplete sentence such as \"love is not the\" is inappropriate.\nTo the best of our knowledge, text generation based on the predefined rigid formats constraints has not been well investigated yet. In this work, we propose a simple and elegant framework named SongNet to address this challenging problem. The backbone of the framework is a Transformer-based auto-regressive language model. Considering the three folds characteristics mentioned above, we introduce sets of tailor-designed indicating symbols to improve the modeling performance, especially for the robustness of the format, rhyme, as well as sentence integrity. We improve the attention mechanism to impel the model to capture the future information on the format to further enhance sentence integrity. Inspired by BERT (Devlin et al., 2019) and GPT (Radford et al., 2018(Radford et al., , 2019, a pretraining and fine-tuning framework is designed to further improve the generation quality. To verify the performance of our framework, we collect two corpora, SongCi and Sonnet, in Chinese and English respectively. Extensive experiments on the collected datasets demonstrate that our proposed framework can generate satisfying results in terms of both the tailor-designed automatic metrics including format accuracy, rhyming accuracy, sentence integrity, as well as the human evaluation results on relevance, fluency, and style.\nIn summary, our contributions are as follows:\n\u2022 We propose to tackle a new challenging task: rigid formats controlled text generation. A pre-training and fine-tuning framework named SongNet is designed to address the problem. \u2022 Sets of symbols are tailor-designed to improve the modeling performance. We improve the attention mechanism to impel the model to capture the future information to further enhance the sentence integrity. \u2022 To verify the performance of our framework SongNet, we collect two corpora, SongCi and Sonnet, in Chinese and English respectively. We design several automatic evaluation metrics and human evaluation metrics to conduct the performance evaluation. \u2022 Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results given arbitrary formats, including the cold-start formats or even the formats newly defined by ourselves.", "n_publication_ref": 21, "n_figure_ref": 5}, {"heading": "Task Definition", "text": "The task of rigid formats controlled text generation is defined as follows:  Input: a rigid format C \u2208 C:\nC = {c 0 c 1 c 2 c 3 , c 0 c 1 c 2 c 3 c 4 c 5 .} (1)\nwhere C is the set of all possible formats. Note that we can define arbitrary new formats not restricted to the ones pre-defined in the corpus, thus |C| \u2192 \u221e.\nFormat token c i denotes a place-holder symbol of C which need to be translated into a real word token. Format C contains 10 words plus two extra punctuation characters \",\" and \".\"\nOutput: a natural language sentence Y \u2208 Y which tally with the defined format C: Y = love is not love, bends with the remover to remove.\nwhere the example sentences are extracted from the Shakespeare's Sonnets (Shakespeare, 2000). From the result Y we can observe that the count of words is 10 which is consistent with the format C. The punctuation characters \",\" and \".\" are also correct. Thus, we claim that it is a 100% format accuracy result. Also, since the two clause sentences are complete, we can get a good sentence integrity score. If C is defined on the literary genres of SongCi or Sonnet which have rhyming constraints, the rhyming performance should be evaluated as well. Recall that C can be arbitrary and flexible, thus we can rebuild a new format C based on the generated result Y by masking partial content, say C = {c 0 c 1 c 2 love, c 0 c 1 c 2 c 3 c 4 remove.}, then we may obtain better results by re-generating based on C . We name this operation as polishing.\nFinally, the target of this problem is to find a mapping function G to conduct the rigid formats controlled text generation:\nY = G(C)(2)\n3 Framework Description", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Overview", "text": "As shown in Figure 2, the backbone of our framework is a Transformer-based auto-regressive language model. The input can be the whole token sequences of samples from SongCi or Sonnet. We tailor-design several sets of indicating symbols to enhance the performance in terms of accuracy on format, rhyme, and sentence integrity. Specifically, symbols C = {c i } are introduced for format and rhyming modeling; Intra-position symbols P = {p i } are designed to represent the local positions of the tokens within each sentence aiming to improve the rhyming performance and the sentence integrity. Segment symbols S = {s i } are employed to identify the sentence border to further improve the sentence quality. Attention mechanism is improved to impel the model to capture the future format information such as the sentence ending markers. Similar to BERT (Devlin et al., 2019) and GPT (Radford et al., 2018(Radford et al., , 2019, pre-training and fine-tuning paradigm is utilized to boost the performance of the original models.", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Details", "text": "We use two sentences (as shown in Figure 1) \"love is not love, ..., bends with the remover to remove\" extracted from the Shakespeare's Sonnets (Shakespeare, 2000) as examples to describe the details of our framework SongNet. Since our basic model is a Transformer-based auto-regressive language model, during training, the input is \" bos love is not love, /s ..., bends with the remover to remove. /s \", and the corresponding output is a left-shifting version of the input (tokenized, and we ignore \"...\" for convenience and clarity):\nlove is not love , /s bends with the remover to remove . /s eos where /s denotes the clause or sentence separator, and eos is the ending marker of the whole sequence. The target of our framework is to conduct the formats controlled text generation. Therefore, the indicating symbols for format and rhyme as well as the sentence integrity are designed based on the target output sequence. Format and Rhyme Symbols:\nC = {c 0 , c 0 , c 0 , c 2 , c 1 , /s c 0 , c 0 , c 0 , c 0 , c 0 , c 2 , c 1 , /s , eos } (3)\nwhere we use {c 0 } to represent the general tokens; {c 1 } depict the punctuation characters; {c 2 } represent the rhyming tokens \"love\" and \"remove\". /s and eos are kept. Intra-Position Symbols:\nP = {p 4 , p 3 , p 2 , p 1 , p 0 , /s p 6 , p 5 , p 4 , p 3 , p 2 , p 1 , p 0 , /s , eos }(4)\n{p i } denote the local positions of tokens within the same clause or sentence. Note that we align the position symbol indices in a descending order. The aim is to improve the sentence integrity by impelling the symbols capture the sentence dynamic information, precisely, the sense to end a sequence. For example, {p 0 } usually denote punctuation characters, thus {p 1 } should be the ending words of sentences. Segment Symbols:\nS = {s 0 , s 0 , s 0 , s 0 , s 0 , /s s 1 , s 1 , s 1 , s 1 , s 1 , s 1 , s 1 , /s , eos } (5)\nwhere s i is the symbol index for sentence i. The purpose is to enhance the interactions between different sentences in different positions by defining the sentence index features.\nDuring training, all the symbols as well as the input tokens are fed into the transformer-based language model. Contrast to Transformer (Vaswani et al., 2017), BERT (Devlin et al., 2019), and GPT2 (Radford et al., 2019), we modify the traditional attention strategies slightly to fit our problem.\nSpecifically, for the input, we first obtain the representations by summing all the embeddings of the input tokens and symbols, as shown in the red solid box of Figure 2:\nH 0 t = E wt + E ct + E pt + E st + E gt (6)\nwhere 0 is the layer index and t is the state index. E * is the embedding vector for input * . w t is the real token at position t. c, p, and s are three pre-defined symbols. g is the global position index same as position symbols used in Transformer (Vaswani et al., 2017). Moreover, the state at time t need to know some future information to grasp the global sequence dynamic information. For example, the model may want to know if it should close the decoding progress by generating the last word and a punctuation character to end the sentence. To represent the global dynamic information, we introduce another variable F 0 by only summing the pre-defined symbols as shown in the blue dash box of Figure 2:\nF 0 t = E ct + E pt + E st (7)\nAfter processing the input, two blocks of attention mechanisms are introduced to conduct the feature learning procedure. The first block is a masking multi-head self-attention component, and the second block is named global multi-head attention. Masking Multi-Head Self-Attention:\nC 1 t = LN FFN(C 1 t ) + C 1 t C 1 t = LN SLF-ATT(Q 0 t , K 0 \u2264t , V 0 \u2264t ) + H 0 t Q 0 = H 0 W Q K 0 , V 0 = H 0 W K , H 0 W V (8)\nwhere SLF-ATT(\u2022), LN(\u2022), and FFN(\u2022) represent self-attention mechanism, layer normalization, and feed-forward network respectively. Note that we only use the states whose indices \u2264 t as the attention context. After obtaining C 1 t from Equation ( 8), we feed it into the second attention block to capture the global dynamic information from F 0 . Global Multi-Head Attention:\nH 1 t = LN FFN(H 1 t ) + H 1 t H 1 t = LN GLOBAL-ATT(Q 1 t , K 1 , V 1 ) + C 1 t Q 1 = C 1 W Q K 1 , V 1 = F 0 W K , F 0 W V (9)\nWe can observe that all the context information from F 0 are considered. This is the reason why we name it as \"global attention\" and why the input real token information E wt is NOT considered. Then the calculation of the unified first model layer is finished. We can iteratively apply these two attention blocks on the whole L model layers until obtain the final representations H L . Note that H is renewed layerly, however the global variable F 0 is fixed. Finally, the training objective is to minimize the negative log-likelihood over the whole sequence:\nL nll = \u2212 n t=1 log P (y t |y <t )(10)", "n_publication_ref": 5, "n_figure_ref": 3}, {"heading": "Pre-training and Fine-tuning", "text": "Although our framework can be trained purely on the training dataset of the target corpus, usually the scale of the corpus is limited. For example, there are only about 150 samples in the corpus of Shakespeare's Sonnets (Shakespeare, 2000). Therefore, we also design a pre-training and fine-tuning framework to further improve the generation quality.\nRecall that in the task definition in Section 2, we claim that our model owns the ability of refining and polishing. To achieve this goal, we adjust the masking strategy used in BERT (Devlin et al., 2019) to our framework according to our definitions. Specifically, we randomly (say 20%) select partial of the original content and keep them not changed when building the format symbols C. For example, we will get a new symbol set C for the example sentences:\nC = {c0, c0, c0, love, c1, /s bends, c0, c0, c0, c0, remove, c1, /s , eos }\nwhere \"love\", \"bends\" and \"remove\" are kept in the format C . After the pre-training stage, we can conduct the fine-tuning procedure directly on the target corpus without adjusting any model structure.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Generation", "text": "We can assign any format and rhyming symbols C to control the generation. Given C, we will obtain P and S automatically. And the model can conduct generation starting from the special token bos iteratively until meet the ending marker eos . Both beam-search algorithm (Koehn, 2004) and truncated top-k sampling (Fan et al., 2018;Radford et al., 2019) method are utilized to conduct the decoding.\n4 Experimental Setup", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Settings", "text": "The parameter size of our model are fixed in both the pre-training stage and the fine-tuning stage. The number of layers L = 12, and hidden size is 768. We employ 12 heads in both the masking multihead self-attention block and the global attention block. Adam (Kingma and Ba, 2014) optimization method with Noam learning-rate decay strategy and 10,000 warmup steps is employed to conduct the pre-training.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets", "text": "We conduct all the experiments on two collected corpus with different literary genres: SongCi and Sonnet, in Chinese and English respectively. The statistic number are shown in Table 3. We can see that Sonnet is in small size since we only utilize the samples from the Shakespeare's Sonnets (Shakespeare, 2000). Since SongCi and Sonnet are in different languages, thus we conduct the pre-training procedure on two large scale corpus in the corresponding languages respectively. For Chinese, we collect Chinese Wikipedia (1700M Characters) and a merged Chinese News (9200M Characters) corpus from the Internet. We did not conduct the word segmenting operations on the Chinese datasets, which means that we just use the characters to build the vocabulary, and the size is 27681. For English, same as BERT, we employ English Wikipedia (2400M words) and BooksCorpus (980M words) (Zhu et al., 2015) to conduct the pre-training. We did not use BPE operation (Sennrich et al., 2015) on this corpus considering the format controlling purpose. We keep the most frequent 50,000 words to build the vocabulary.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Evaluation Metrics", "text": "Besides PPL and Distinct (Li et al., 2016), we also tailor-design several metrics for our task to conduct the evaluation for format, rhyme, and sentence integrity. Format Assume that there are m sentences defined in the format C = {C s 1 , C s 2 , ..., C s m }, and the generated results Y contains n sentences Y = {Y s 1 , Y s 2 , ..., Y s n }. Without loss of generality, we align C and Y from the beginning, and calculate the format quality according to the following rules:\n(1) the length difference\n||C s i | \u2212 |Y s i || \u2264 \u03b4;\n(2) the punctuation characters must be same. For SongCi, we let \u03b4 = 0 and rule (2) must be conforming.     For Sonnet, we relax the condition where we let \u03b4 = 1 and ignore rule (2). Assume that the number of format-correct sentences is n , then we can obtain Precision p = n /n, Recall r = n /m, and F1-measure. We report both the Macro-F1 and Micro-F1 in the results tables.\nModel PPL\u2193 Diversity (Distinct) \u2191 VAL TEST MA-D-1 MI-D-1 MA-D-2 MI-D-2 GPT2 w/\nRhyme For SongCi, usually, there is only one group of rhyming words in one sample. As the example shown in Table 1, the pronunciation of the red rhyming words are \"zhu\", \"y\u00fc\", \"du\", and \"gu\" respectively, and the rhyming phoneme is \"u\". For the generated samples, we first use the tool pinyin 4 to get the pronunciations (PinYin) of the words in the rhyming positions, and then conduct the evaluation. For Shakespeare's Sonnets corpus, the rhyming rule is clear \"ABAB CDCD EFEF GG\" and there are 7 groups of rhyming tokens. For the generated samples, we employ the CMU Pronouncing Dictionary 5 (Speech@CMU, 1998) to obtain the phonemes of the words in the rhyming positions. For example, the phonemes for word \"asleep\" and \"steep\" are ['AH0', 'S', 'L', 'IY1', 'P'] and ['S', 'T', 'IY1', 'P'] respectively. And then we can conduct the evaluation by counting the overlapping units from both the original words and the extracted phonemes group by group. We report the Macro-F1 and Micro-F1 numbers in the results tables as well.\nIntegrity Since the format in our task is strict and  rigid, thus the number of words to be predicted is also pre-defined. Our model must organize the language using the limited positions, thus sentence integrity may become a serious issue. For example, the integrity of \"love is not love . /s \" is much better than\"love is not the . /s \". To conduct the evaluation of sentence integrity, we design a straightforward method by calculating the prediction probability of the punctuation characters before /s given the prefix tokens:\nModel PPL\u2193 Diversity (Distinct) \u2191 VAL TEST MA-D-1 MI-D-1 MA-D-2 MI-D-2\nIntegrity = 2 \u2212 1 |Y | |Y | i=1 log(P (y i punc |y i 0 ,y i 1 ,...,y i <punc ))(11)\nwhere Y is the generated sequence of sentences. Smaller integrity metric value indicates higher sentence quality. To achieve this goal, we conduct pre-trainings for two GPT2 (Radford et al., 2019) models on the large scale Chinese corpus and English corpus respectively. Then we utilize the GPT2 models to conduct the evaluation for sentence integrity. Human Evaluations For SongCi, we sampled 50 samples for 25 CiPais. For Sonnet, the whole 27 samples in the test set are selected for human evaluation. We recruit three helpers to score the Relevance, Fluency, and Style. The rating criteria are as follows: Relevance: +2: all the sentences are relevant to the same topic; +1: partial sentences are relevant; 0: not relevant at all. Fluency: +2: fluent; +1: readable but with some grammar mistakes; 0: unreadable. Style: +2: match with SongCi or Sonnet genres; +1: partially match; 0: mismatch.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Comparison Methods", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "S2S", "text": "Sequence-to-sequence framework with attention mechanism . We regard the format and rhyme symbols C as the input sequence, and the target as the output sequence. GPT2 We fine-tune the GPT2 models (the pretraining versions are used for sentence integrity evaluation) on SongCi and Sonnet respectively. SongNet Out proposed framework with both the per-training and fine-tuning stages.\nWe also conduct ablation analysis to verify the performance of the defined symbols as well as the variants of model structures.\n\u2022 SongNet (only pre-tuning) Without the finetuning stage. \u2022 SongNet (only fine-tuning) Without the pretraining stage. \u2022 SongNet-GRU Employ GRU  to replace Transformer as the core structure.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Cases of Generated Results", "text": "SongNet-SongCi CiPai: Zhe Gu Tian, Format: 7. 7. 7, 7. 3, 3. 7. what lies, for when you are not that , \\ no one in this and that can see me lies!\nTable 5: Cases of the generated results for SongCi and Sonnet respectively. For SongCi, the number in Format (e.g., 3,5,7) denotes the number of tokens in one sentence. The rhyming words are labeled in red color and italic font following is the Pinyin. (Since cases are provided to confirm the format consistency, thus we did not conduct translation for the Chinese samples. Translation for Chinese poetry is also a challenging task.)", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Cases of Generated Results Given the Formats with Partial Content", "text": "SongNet-SongCi CiPai: Bu Suan Zi, Format: 5, 5. 7, 5. 5, 5. 7, 5. though all thy love with thy hearts , thou still are lacking of my dead ; if thy love love is lost to your love and parts , and yet mine own heart can be buried . so many are ill or in tear, hath not this time that we will make their eye , for that which lies not well hath now appear, no longer nor the world that holds thee lie ! for if it would be buried in my live , or by the earth of mine was gone , then my own parts as my body and mine give , may not be so far beyond thine alone : so far as thee and this world view find thee , then mine life be far enough from all thee and no me .\nFormat C _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ (1) (2) Format C _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ (1) (2) SongNet-Sonnet _ _ _ _\nTable 6: Cases of the generated results given the formats with partial pre-defined content. Format token \" \" needs to be translated to real word token.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results and Discussions", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "Please note that we mainly employ top-k sampling method (Fan et al., 2018;Radford et al., 2019) to conduct the generation, and we let k = 32 here. The parameter tuning of k is described in Section 5.3.  on SongCi. The main reason is that Sonnet only contains 100 samples in the training set as shown in Table 3. Therefore, the model cannot capture sufficient useful features especially for the rhyming issue.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Ablation Analysis", "text": "We conduct ablation study on corpus SongCi and the experimental results are depicted in Table 4. It should note that all the models are purely trained on SongCi corpus without any pre-training stages.\nFrom the results we can conclude that the introduced symbols C, P , and S indeed play crucial roles in improving the overall performance especially on the metrics of format, rhyme, and sentence integrity. Even though some of the components can not improve the performance simultaneously on all the metrics, the combination of them can obtain the best performance.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Parameter Tuning", "text": "Since we employ top-k sampling as our main decoding strategy, thus we design several experiments to conduct the parameter tuning on k. We let k to be 1, 5, 10, 20, 50, 500 respectively. We also provide the beam-search (beam=5) results for comparing and reference. The parameter tuning results are depicted in Figure 3. From the results we can observe that large k can increase the diversity of the results significantly. But the Rhyme accuracy and the sentence integrity will drop simultaneously. Therefore, in the experiments we let k = 32 to obtain a trade-off between the diversity and the general quality.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Human Evaluation", "text": "For human evaluation, we just conduct the judging on the results generated by our final model SongNet. From the result we can observe that the results on corpus SongCi is much better than the ones on corpus Sonnet, which is because the corpus scale is different. And the the small scale also lead to dramatically dropping on all the metrics.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Case Analysis", "text": "Table 5 depicts several generated cases for SongCi and Sonnet respectively. For SongCi, the formats (CiPai) are all cold-start samples which are not in the training set or even newly defined. Our model can still generate high quality results on the aspects of format, rhyme as well as integrity. However, for corpus Sonnet, even though the model can generate 14 lines text, the quality is not as good as SongCi due to the insufficient training-set (only 100 samples). We will address this interesting and challenging few-shot issue in the future.\nIn addition, we mentioned that our model has the ability of refining and polishing given the format C which contains some fixed text information. The examples of the generated results under this setting are shown in Table 6, which show that our model SongNet can generate satisfying results especially on SongCi.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We propose to tackle a challenging task called rigid formats controlled text generation. A pre-training and fine-tuning framework SongNet is designed to address the problem. Sets of symbols are tailordesigned to improve the modeling performance for format, rhyme, and sentence integrity. Extensive experiments conducted on two collected corpora demonstrate that our framework generates significantly better results in terms of both automatic metrics and human evaluations given arbitrary cold start formats.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2014", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "journal": "", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merrienboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"}, {"title": "Transformer-xl: Attentive language models beyond a fixed-length context", "journal": "", "year": "2019", "authors": "Zihang Dai; Zhilin Yang; Yiming Yang; W William; Jaime Cohen;  Carbonell; V Quoc; Ruslan Le;  Salakhutdinov"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Hierarchical neural story generation", "journal": "Long Papers", "year": "2018", "authors": "Angela Fan; Mike Lewis; Yann Dauphin"}, {"title": "Convolutional sequence to sequence learning", "journal": "", "year": "2017", "authors": "Jonas Gehring; Michael Auli; David Grangier; Denis Yarats; Yann N Dauphin"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Pharaoh: a beam search decoder for phrase-based statistical machine translation models", "journal": "Springer", "year": "2004", "authors": "Philipp Koehn"}, {"title": "Deep-speare: A joint neural model of poetic language, meter and rhyme", "journal": "", "year": "2018", "authors": "Trevor Jey Han Lau; Timothy Cohn; Julian Baldwin; Adam Brooke;  Hammond"}, {"title": "A diversity-promoting objective function for neural conversation models", "journal": "", "year": "2016", "authors": "Jiwei Li; Michel Galley; Chris Brockett; Jianfeng Gao; Bill Dolan"}, {"title": "An empirical investigation of pre-trained transformer language models for open-domain dialogue generation", "journal": "", "year": "2020", "authors": "Piji Li"}, {"title": "Deep recurrent generative decoder for abstractive text summarization", "journal": "", "year": "2017", "authors": "Piji Li; Wai Lam; Lidong Bing; Zihao Wang"}, {"title": "Gpt-based generation for classical chinese poetry", "journal": "", "year": "2019", "authors": "Yi Liao; Yasheng Wang; Qun Liu; Xin Jiang"}, {"title": "Improving language understanding with unsupervised learning", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan"}, {"title": "Language models are unsupervised multitask learners", "journal": "OpenAI Blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"title": "A neural attention model for abstractive sentence summarization", "journal": "", "year": "2015", "authors": "Sumit Alexander M Rush; Jason Chopra;  Weston"}, {"title": "Get to the point: Summarization with pointergenerator networks", "journal": "Long Papers", "year": "2017", "authors": "Abigail See; J Peter; Christopher D Liu;  Manning"}, {"title": "Do massively pretrained language models make better storytellers? arXiv preprint", "journal": "", "year": "2019", "authors": "Abigail See; Aneesh Pappu; Rohun Saxena; Akhila Yerukola; Christopher D Manning"}, {"title": "Neural machine translation of rare words with subword units", "journal": "", "year": "2015", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Shakespeare's sonnets", "journal": "Yale University Press", "year": "2000", "authors": "William Shakespeare"}, {"title": "Neural responding machine for short-text conversation", "journal": "Long Papers", "year": "2015", "authors": "Lifeng Shang; Zhengdong Lu; Hang Li"}, {"title": "Carnegie-mellon university pronouncing dictionary for american english", "journal": "", "year": "1998", "authors": "@ Speech;  Cmu"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "A neural conversational model", "journal": "", "year": "2015", "authors": "Oriol Vinyals; Quoc Le"}, {"title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; Ruslan Salakhutdinov; Quoc V Le"}, {"title": "Chinese poetry generation with recurrent neural networks", "journal": "", "year": "2014", "authors": "Xingxing Zhang; Mirella Lapata"}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "journal": "", "year": "2015", "authors": "Yukun Zhu; Ryan Kiros; Rich Zemel; Ruslan Salakhutdinov; Raquel Urtasun; Antonio Torralba; Sanja Fidler"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: The framework of our proposed model.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "\u2022SongNet w/o C Remove the format and rhyme symbols C. \u2022 SongNet w/o P Remove the intra-position symbols P . \u2022 SongNet w/o S Remove the sentence segment symbols S. \u2022 SongNet w/ inverse-P Arrange the intraposition indices in ascending order instead of the descending order.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Parameter tuning of k on the metrics of Rhyme, Integrity, and Micro-Dist-2.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Automatic evaluation results on SongCi", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ": Automatic evaluation results on SonnetCorpus #Train #Dev #Test #VocabSongCi 19,244 8479625310Sonnet10027272801"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Statistics of the datasets SongCi and Sonnet.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "hide this from my eyes, \\ from this white mine eyes all fals, \\ where is the good fortune, in me, \\ that hath no excuse, no excuse? what is that which can mask the true love \\ and for whom is this true love more? \\ the one, which shall save the poor my eye, \\ from the false truth of my judgment?", "figure_data": "7, 7.(qian)(nuan)(can)(man)(guan)(yuan)(ban)(ban)CiPai: Bu Suan Zi, Format: 5, 5. 7, 5. 5, 5. 7, 5.(chu)(tu)(yu)(zhu)CiPai: Self-Defined, Format: 3, 3, 5. 3, 3, 5. 7, 7.(tian)(xian)(yuan)CiPai: Self-Defined, Format: 9. 9. 9. 9.(han)(xian)(jian)(shan)SongNet-Sonnet how do you hold such a thing like this, \\ when my eyes are so not black? \\ but how can i show myself, so strange, \\ that all this black is white?where am i to"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Table2depict the experimental results of SongNet as well as the baseline methods S2S and GPT2 on corpus SongCi and Sonnet respectively. It is obvious that our pre-training and fine-tuning framework SongNet obtain the best per-formance on most of the automatic metrics. Especially on the metric of Format accuracy, SongNet can even obtain a 98%+ value which means that our framework can conduct the generation rigidly matching with the pre-defined formats. On the metric of PPL, Rhyme accuracy, and sentence integrity, SongNet also performs significantly better in a large gap than the baseline methods such as S2S and GPT2 as well as the model variants only with the pre-training or fine-tuning stage.Another observation is that some of the results on corpus Sonnet are not as good as the results", "figure_data": "ModelRelevance Fluency StyleSongNet-SongCi1.361.452.00SongNet-Sonnet0.580.420.83"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Human evaluation results.", "figure_data": ""}], "doi": ""}