{"authors": "Kevin Stowe; \u2666 Prasetya; Ajie Utama; Iryna Gurevych", "pub_date": "", "title": "IMPLI: Investigating NLI Models' Performance on Figurative Language", "abstract": "Natural language inference (NLI) has been widely used as a task to train and evaluate models for language understanding. However, the ability of NLI models to perform inferences requiring understanding of figurative language such as idioms and metaphors remains understudied. We introduce the IMPLI (Idiomatic and Metaphoric Paired Language Inference) dataset, an English dataset consisting of paired sentences spanning idioms and metaphors. We develop novel methods to generate 24k semiautomatic pairs as well as manually creating 1.8k gold pairs. We use IMPLI to evaluate NLI models based on RoBERTa fine-tuned on the widely used MNLI dataset. We then show that while they can reliably detect entailment relationship between figurative phrases with their literal counterparts, they perform poorly on similarly structured examples where pairs are designed to be non-entailing. This suggests the limits of current NLI models with regard to understanding figurative language and this dataset serves as a benchmark for future improvements in this direction. 1 * The work was done while the second author was still affiliated with the UKP Lab at TU Darmstadt.", "sections": [{"heading": "Introduction", "text": "Understanding figurative language (i.e., that in which the intended meaning of the utterance differs from the literal compositional meaning) is a particularly difficult area in NLP (Shutova, 2011;Veale et al., 2016), but is essential for proper natural language understanding. We consider here two types of figurative language: idioms and metaphors. Idioms can be viewed as non-compositional multiword expressions (Jochim et al., 2018), and have been historically difficult for NLP systems. For instance, sentiment systems struggle with multiword expressions in which individual words do not directly contribute to the sentiment (Sag et al., 2002).", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Idioms", "text": "Jamie was pissed off this afternoon. \u2192 Jamie was irritated this afternoon There's a marina down in the docks. There's a marina down under scrutiny.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Metaphors", "text": "The hearts of men were softened. \u2192 The men were made kindler and gentler.\nThe gun kicked into my shoulder. The mule kicked into my shoulder. Metaphors involve linking conceptual properties of two or more domains, and are known to be pervasive in everyday language (Lakoff and Johnson, 1980;Stefanowitsch and Gries, 2008;Steen et al., 2010). Recent work has shown that these types of figurative language are impactful across a broad array of NLP tasks (see \u00a72.1).\nLarge-scale pre-training and transformer-based architectures have yielded increasingly powerful language models (Vaswani et al., 2017;Devlin et al., 2019;Liu et al., 2019). However, relatively little work has explored these models' representations of figurative and creative language. NLI datasets have widely been used for evaluating the performance of language models (Dagan et al., 2006;Bowman et al., 2015a;Williams et al., 2018), but there are insufficient figurative language datasets in which a literal sentence is linked to a corresponding figurative counterpart that are large enough to be suitable for evaluating NLI. Due to the creative nature of human language, creating a dataset of diverse, high-quality literal/figurative pairs is time-consuming and difficult.\nTo address this gap, we build a new English dataset of paired expressions designed to be leveraged to explore model performance via NLI. Our dataset, IMPLI (Idiomatic/Metaphoric Paired Language Inference), is comprised of both silver pairs, which are built using semi-automated methods ( \u00a73.1), as well as hand-written gold pairs ( \u00a73.4), crafted to reflect both entailment and nonentailment scenarios. Each pair consists of a sentence containing a figurative expression (idioms/metaphors) and a literal counterpart, designed to be either entailed or non-entailed by the figurative expression (Table 1 shows some examples).\nOur contribution thus consists of three key parts:\n\u2022 We create a new IMPLI dataset consisting of 24,029 silver and 1,831 gold sentence pairs consisting of idiomatic and metaphoric phrases that result in both entailment and nonentailment relationship (see Table 2). \u2022 We evaluate language models in an NLI setup,\nshowing that metaphoric language is surprisingly easy, while non-entailing idiomatic relationships remain extremely difficult. \u2022 We evaluate model performance in a number of experiments, showing that incorporating idiomatic expressions into the training data is less helpful than expected, and that idioms that can occur more in more flexible syntactic contexts tend to be easier to classify.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Background", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Figurative Language and NLP", "text": "Figurative language includes idioms, metaphors, metonymy, hyperbole, and more. Critically, figurative language is that in which speaker meaning (what the speaker intends to accomplish through an utterance) differs from the literal meaning of that utterance. This leads to problems in NLP systems if they are trained mostly on literal data, as their representations for particular words and/or phrases will not reflect their figurative intended meanings. Figurative language has a significant impact on many NLP tasks. Metaphoric understanding has been shown to be necessary for proper machine translation (Mao et al., 2018;Mohammad et al., 2016). Sentiment analysis also relies critically on figurative language: irony and sarcasm can reverse the polarity of a sentence, while metaphors and idioms may make more subtle changes in the speaker meaning (Ghosh et al., 2015). Political discourse tasks including bias, misinformation, and political framing detection benefit from joint learning with metaphoricity (Huguet Cabot et al., 2020). Figurative language engendered by creativity on social media also poses difficulty for many NLP tasks including identifying depression symptoms (Yadav et al., 2020;Iyer et al., 2019) and hate speech detection (Lemmens et al., 2021).\nWe are here focused on idioms and metaphors. There is currently a gap in diagnostic datasets for idioms, and our work fills this gap. There exist some relevant metaphoric resources (see \u00a72.2); metaphors are known to be extremely common and important to understanding figurative language, our resource serves to build upon this work.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "NLI and related challenges", "text": "Natural language inference is the task of predicting, given two fragments of text, whether the meaning of one (premise) entails the other (hypothesis) (Dagan et al., 2006). The task is formulated as a 3-way classification problem, in which the premise and hypothesis pairs are labeled as entailment, contradiction, or neutral, if their relationship could not be directly inferred (Bowman et al., 2015b). NLI has been widely used as an evaluation task for language understanding, and there have been a large number of challenging datasets, which have been used to further our understanding of the capabilities of language models (Wang et al., 2018.\nPaired data for figurative language is relatively sparse, and there is a gap in the diagnostic datasets used for NLI in this area. Previous work includes the literal/metaphoric paraphrases of Mohammad et al. (2016) and Bizzoni and Lappin (2018), although both contain only hundreds of samples, insufficient for proper model training and evaluation. With regard to NLI, early work proposed the task of textual entailment as a way of understanding metaphor processing capabilities Agerri, 2008). Poliak et al. (2018) build a dataset for diverse NLI, which includes some creative language such as puns, albeit making no claims with regard to figurativeness. Zhou et al. (2021) build a dataset consisting of paired idiomatic and literal expressions. They begin with a set of 823 idiomatic expressions yielding 5,170 sentences, and had annotators manually rewrite sentences containing these idioms as literal expressions. We expand on this methodology by having annotators only correct definitions for the idioms themselves and use these definitions to automatically generate the literal interpretations of the idioms by replacing them into appropriate contexts: this allows us to scale up to over 24k silver sentences. We also expand beyond paraphrasing by incorporating both entailment and non-entailment   Their dataset consists of figurative/literal pairs recast from previously developed simile and metaphor datasets, along with a parallel dataset between ironic and non-ironic rephrasing. This sets the groundwork for figurative NLI, but the dataset is relatively small outside of the irony domain, and the non-entailments are generated purely by replacing words with their antonyms, restricting the novelty of the hypotheses. Their dataset is relatively easy for NLI models; here we show that figurative language can be challenging, particularly with regard to non-entailments. Zhou et al. (2021) and Chakrabarty et al. (2021a) provide invaluable resources for figurative NLI; our works aims to covers gaps in a number of areas. First, we generate a large number of both entailment and non-entailment pairs, allowing for better evaluation of adversarial non-entailing examples. Second, our silver methods allow for rapid development of larger scale data, allowing for model training and evaluation. We show that while entailment pairs are relatively easy (accuracy scores ranging from .86 to .89), the non-entailment pairs are exceedingly challenging, with the roberta-large model achieving accuracy scores ranging from .311 to .539.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Building a Dataset", "text": "Our IMPLI dataset is built from idiomatic and metaphoric sentences paired with entailing and nonentailing counterparts, from both silver pairs ( \u00a73.1) and manually written sentences ( \u00a73.4). For our purposes, we follow McCoy et al. (2019) in conflating the neutral and contradiction categories into a nonentailment label. We then label every pair as either entailment (\u2192) or non-entailment ( ).\nDue to the difficult nature of the task and to avoid issues with crowdsourcing (Bowman et al., 2020), we employed expert annotators. We used two fluent English speakers, both graduate students in linguistics with strong knowledge in figurative language, paid at a rate of $20/hr. For each method below, we ran pilot studies, incorporated annotator feedback and iteratively assessed the viability of identifying and generating appropriate expressions. As the annotators were working on generating new expressions, agreement was not calculated: we instead assessed the quality of the resulting expressions (see Section 3.3). Table 2 contains an overview of the different entailment and non-entailment types collected (Detail examples are also provided in Appendix D).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Silver pairs", "text": "First, we explore a method for generating silver pairs using annotators to create phrase definitions which can be inserted automatically into relevant contexts, yielding a large number of possible entailment and non-entailment pairs that differ only with regard to the relevant phrase. Our procedure hinges on a key assumption: for any given figurative phrase, we can generate a contextually independent literal paraphrase. We then replace the original expression with the literal paraphrase, following the assumption that the figurative expression necessarily entails its literal paraphrase: He's stuck in bed, which is his hard cheese. \u2192 He's stuck in bed, which is his bad luck.\nConversely, in contexts where the original phrase is used literally, replacing it with the literal paraphrase should yield a non-entailment relation.\nSwitzerland is famous for six cheeses, sometimes referred to as hard cheeses. The sailors all worked under scrutiny.\nThe sailors all worked in the docks  Switzerland is famous for six cheeses, sometimes referred to as bad luck.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Idioms", "text": "To build idiomatic pairs, we use three corpora that contain sentences with idiomatic expressions (IEs) labelled as either figurative or literal. 2 These are the MAGPIE Corpus (Haagsma et al., 2020), the PIE Corpus (Adewumi et al., 2021), and the SemEval 2013 Task 5 (Korkontzelos et al., 2013). We collect the total set of IEs that are present in these corpora. We then extract definitions for these using freely available online idiom dictionaries. 3 These definitions are often faulty, incomplete, or improperly formatted. We employed annotators to make manual corrections. The annotators were given the original IE as well as the definition extracted from the dictionary. The annotators were asked to ensure that the dictionary definition given was (1) a correct literal interpretation and (2) fit syntactically in the same environments as the original IE. If the definition met both of these criteria, the IE can be replaced by its definition to yield an entailment pair. If either criterion was not met, annotators were asked to minimally update the definition so that it satisfied the requirements.\nIn total this process yielded 697 IE definitions. We then used the above corpora, replacing these definitions into the original sentences (see Figure 1). We use the figurative/literal labels from the as right as rain  original corpora: replacing them into figurative contexts yields entailment relations, while replacing them into contexts where the phrase is meant literally then yields non-entailments.", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "Adversarial Definitions", "text": "As a second method for generating non-entailment pairs, we asked annotators to write novel, adversarial definitions for IEs. Given a particular phrase, they were instructed to invent a new meaning for the IE that was not entailed by the true meaning, but which seemed reasonable presuming they had never heard the original IE. Some examples of this process are shown in Table 3. We then replace these adversarial definitions into figurative sentences from the corpora. This yields pairs where the premise is an idiom used figuratively, and the hypothesis is a sentence that attempts to rephrase the idiom literally, but does so incorrectly, thus yielding non-entailments (Figure 2).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Metaphors", "text": "Metaphors are handled in a similar way: we start with a collection of minimal metaphoric expressions (MEs). These are subject-verb-object and adjective-noun constructions from Tsvetkov et al. (2014). Each is annotated as being either literal or metaphoric, along with an example sentence. We passed these MEs directly to annotators, who were then instructed to replace a word in the ME so that it would be considered literal in a neutral context. They ran through the airport to board their flight.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Figurative sentences", "text": "Figure 3: Metaphor entailment generation. Pairs are generated using annotator-defined literal translations substituted into metaphoric contexts.\n2. hard truth \u2192 unpleasant truth 3. hairy problem \u2192 difficult problem These can then be replaced in a similar fashion: we start with the original figurative sentence, replace the ME with the literal replacements, and the result is an entailing pair with the metaphoric sentence entailing the literal.\nWe apply this procedure to the dataset of Tsvetkov et al. (2014), yielding 100 metaphoric/literal NLI entailment pairs. We then take a portion of the Common Crawl dataset 4 , and identify sentences that contain these original MEs. We identify sentences that contain the words from the metaphoric phrase, and replace the metaphoric word itself with its literal counterpart. This yields 645 additional silver pairs.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Postprocessing", "text": "For all silver methods, we also employ syntactic postprocessing to overcome a number of hurdles. First, phrases used idiomatically often follow different syntactic patterns than when used literally.\nOriginal: These point out of this world, but where to is not made clear. Replaced: *These point wonderful, but where to is not made clear. This phrase in literal contexts functions syntactically as a prepositional phrase, while idiomatically it is used as an adjective. When replaced with the definition \"wonderful\" in a literal context, we get a grammatically incoherent sentence. Second, phrases in their literal usage often do not form full constituents, due to the string-matching approach of the original datasets. Many literal usages of these phrases are thus incompatible with the defined replacement.\n\u2022 I think [this one has to die] for the other one to live. \u2022 Turn in [the raw edges] of both seam allowances towards each other and match the folded edges.\nTo avoid these issues, we ran syntactic parsing on the definition and the expression within each context, requiring that the expression in context begins with the same part of speech as the definition and that it does not end inside of another phrase.\nAdditionally, for each replacement, we ensured that the verb conjugation matched the context. For this, we identified the conjugation in the context, and used a de-lemmatization script to conjugate the replacement verb to match the original.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Additional Issues", "text": "In implementing and analyzing this procedure, we noted a number of practical issues. First, a large number of the MEs provided are actually idiomatic or proverbial: the focus word does not actually contribute to the metaphor, but rather the entire expression is necessary. Similarly, we found that replacing individual parts of MEs is often insufficient to fully remove the metaphoric meaning. We iterated over possible solutions to circumvent these issues and found that it is best to simply skip instances for which a replacement does not yield a feasible literal interpretation.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Evaluating Pair Quality", "text": "In order for these automatically created pairs to be useful for NLI-based evaluation, they need to be of sufficiently high quality. As the annotators were generating novel definitions and pairs, rather than inter-annotator agreement, we instead evaluate the quality of the resulting pairs by testing whether the automatically generated pairs contained the appropriate entailment relation. For this task, each annotator was given 100 samples for each general category of silver generations (idiomatic entailments, idiomatic non-entailments, and metaphoric entailments). They were asked if the entailment relation between the two sentences was as expected. An expert than adjudicated disagreements to determine the final percentage of valid pairs.\nTo evaluate the syntactic validity of the generated pairs, we additionally ran the Stanford PCFG dependency parser (Klein and Manning, 2003)   the pairs. Per previous work in NLI (Williams et al., 2018), we evaluate the proportion of sentences for which the root node is S. Table 4 shows the results. The semi-supervised examples evoked the correct entailment relation between %88 and %97 of the time: while there is still noise present, this indicates the effectiveness of the proposed methods. With regard to syntax, we see S node roots for between 82% and %90 of the sentences: within the range of the SNLI performance (74%-88%), and slightly behind the MNLI (91%-98%). We find that the generated hypotheses are not significantly different in quality than the premises. This indicates that the method for generation preserves the original syntax.\nThese methods allow us to quickly generate a substantial number of high-quality pairs to evaluate NLI systems on figurative language. However, they may introduce additional bias as we employ a number of restrictions in order to ensure syntactic and semantic compatibility, and we lack full nonentailment pairs for metaphoric data. We therefore expand our dataset with manually generated pairs.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Manual Creation of Gold Pairs", "text": "To create gold pairs, annotators were given a figurative sentence along with the focus of the figurative expression: for idioms, this is the IE; for metaphors, the focus word of the metaphor. For idioms, we used the MAGPIE dataset to collect contextually figurative expressions. For metaphors, we collected metaphoric sentences from the VUA Metaphor Corpus (Steen et al., 2010), the metaphor dataset of (Mohammad et al., 2016), and instances from the Gutenberg poetry corpus (Jacobs, 2018) annotated for metaphoricity (Chakrabarty et al., 2021b;Stowe et al., 2021) . Annotators were instructed to rewrite the sentence literally. This was done by removing or rephrasing the figurative component of the sentence. This yields gold standard paraphrases for idiomatic and metaphoric contexts.\nWe then asked annotators to write non-entailed hypotheses for each premise. They were encour-aged to keep as much of the original utterance as possible, ensuring high lexical overlap, while removing the main figurative element of the sentence. For idioms, this comes from adding or adjusting words to force a literal reading of the idiom:\n\u2022 The old girl finally kicked the bucket.\nThe girl kicked the bucket on the right.\nFor metaphors, this typically involves keeping the same phrasing while adapting the sentence to have a different, non-metaphoric meaning.\n\u2022 You must adhere to the rules.\nYou must adhere the rules to the wall.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Antonyms", "text": "Previous work in NLI has employed the technique of replacing words in the literal sentences with their antonyms to yield non-entailing pairs (Chakrabarty et al., 2021a). We replicate this process for idioms: for the manually elicited definitions, we replace key words as determined by annotators with their antonyms. This yields sentences which negate the original figurative meaning and are thus suitable non-entailment pairs. Previous work found this antonym replacement for figurative language remains relatively easy for NLI systems, which we can additionally explore with regard to idioms.\nThese manual annotations provide a number of concrete benefits. First, they are not restricted to individual words or phrases (excluding antonyms): the figurative components can be rewritten freely, allowing for diverse, interesting pairs. Second, they are written by experts, ensuring higher quality than the automatic annotations, which may be noisy.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experiments / Results", "text": "Using the IMPLI dataset, we aim to answer a series of questions via NLI pertaining to language models' ability to understand and represent figurative language accurately. These questions are: Our dataset provides unique advantages in addressing these research questions that cover gaps  in previous work: it contains a large number of both entailments and non-entailments and is large enough to be used for training the models.\nIdioms Metaphors Model MNLI MNLI-MM \u2192 S S l S d \u2192 G G a G \u2192 S \u2192 G G roberta-", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "R1: pre-trained Model Performance", "text": "We obtain baseline NLI models by fine-tuning roberta-base and roberta-large models on the MNLI dataset (Williams et al., 2018), with entailments as the positive class and all others as the negative and evaluate them on their original test sets as well as IMPLI. 5 Due to variance in neural model performance (Reimers and Gurevych, 2017), we take the mean score over 5 runs using different seeds.\nWe report results in Table 5. We observe that idiomatic entailments are relatively easy to classify, with accuracy scores over .84. Non-entailments were much more challenging. Silver pairs generated through adversarial definitions were especially difficult: the pairs contain high lexical overlap, and in many cases the premise and hypotheses are semantically similar. The replacement into literal samples were easier, as the idiomatic definition clashes more starkly with the original premise, making non-entailment predictions more likely. Consistent with Chakrabarty et al. (2021a)'s work in metaphors, non-entailment through antonym replacement is easiest for idioms: the antonymic relationship can be a marker for non-entailment, despite the high word overlap.\nWith regard to metaphors, silver entailment pairs are relatively easy. Manual pairs are more challenging but are still much easier than idioms. This is supported by the fact that metaphors are common in everyday language: these models have likely seen the same (or similar) metaphors in training. Our findings show that in fact metaphoricity may not be particularly challenging for deep pre-trained models, as they are able to effectively capture the metaphoric entailment relations. The roberta-large model performs better for metaphoric expressions than roberta-base, but the difference on other partitions is relatively small. 6 We also find that lexical overlap plays a significant role here as noted by previous work (McCoy et al., 2019): sentences with high overlap tend to be classified as entailments regardless of the true label (for more, see Appendix B).\nWe note that the manual pairs tend to be more difficult for both idioms and metaphors: these pairs can be more flexible and creative, whereas the silver pairs are restricted to more regular patterns.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "R2: Incorporating Idioms into Training", "text": "To evaluate incorporating idioms into training, we then split the idiom data by idiomatic phrase types, keeping a set of IEs separate as test data to assess whether the model can learn to correctly handle novel, unseen phrases. Our goal is to assess whether poor performance is due to models' not containing these expressions in training, or because their ability to represent figurative language inherently limited. We hypothesize that the noncompositional nature of these types of figuration should lead to poor performance on unseen phrases, even if the model is trained on other idiomatic data.\nFor each task, we split the data into 10 folds by IE and incrementally incorporate these folds into the original MNLI for training, leaving one fold out for testing. We experiment with incorporating all training data for both labels, as well as using only entailment or non-entailment samples. We then evaluate our results on the entire test set, as well as the entailment and non-entailment partitions.\nFigure 4 shows the results, highlighting that additional training data yields only small improvements. Pairs with non-entailment relations remain exceedingly difficult, with performance capping out at only slightly better than chance. As hypothesized, additional training data is only somewhat effective in improving language models' idiomatic capabilities; this is not sufficient to overcome difficulties from literal usages of idiomatic phrases and adversarial definitions, indicating that idiomatic language remains difficult for pre-trained language models to learn to represent. R3: Syntactic Flexibility Finally, we assess models' representation of idiomatic compositionality. Nunberg et al. (1994) indicate that there are two general types of idioms: \"idiomatic phrases\", which exhibit limited flexibility and generally occur only in a single surface form, and \"idiomatically combining expressions\" or ICEs, in which the constituent elements of the idiom carry semantic meaning which can influence their syntactic properties, allowing them to be more syntactically flexible.\nFor example, in the idiom spill the beans, we can map the spilling activity to divulging of information, and the beans to the information. Because this expression has semantic mappings to figurative meaning for its syntactic constituents, Nunberg et al. (1994) argue that it can be more syntactically flexible, allowing for expressions like the beans that were spilled by Martha to maintain idiomatic meaning. For fixed expressions such as kick the bucket, no syntactic constituents map directly to the figurative meaning (\"die\"). We then expect less syntactic flexibility, and thus the bucket that was kicked by John loses its idiomatic meaning.\nWe hypothesize that model performance will be correlated with the degree to which a given idiom type is flexible: more fixed expressions may be easier, as they are seen in regular, fixed patterns that the models can memorize, while more flexible ICEs will be more difficult, as they can appear in different patterns, cases, and word order, often even mixing in with other constituents. To test this, we define an ICE score as the percentage of times a phrase occurs in our test data in a form that does not match its original base form. Higher percentages mean the phrase occurs more frequently in a non-standard form, acting as a measure for the syntactic flexibility of the expression. We assessed the performance of the roberta-base model for each idiom type, evaluating Spearman correlations between performance and idioms' ICE scores.\nWe found no correlation between ICE scores and performance for entailments, nor for adversarial definition non-entailments (r = .004/.45, p = .921/.399, see Appendix C). However, we do see a weak but significant correlation (r = .188, p = 0.016) with non-entailments from literal contexts: the model performs better when the phrases are more flexible, contrary to our initial hypothesis. One possible explanation is that the model memorizes a specific figurative meanings for each fixed expression, disregarding the possibility of these words being used literally. When the expression is used in a literal context, the model then still assumes the figurative meaning, resulting in errors on non-entailment samples. The ICEs are more fluid, and thus the model is less likely to have a concrete representation for the given phrase: it is better able to reason about the context and interacting words within the expression, making it easier to distinguish the entailing and non-entailing samples.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Conclusions and Future Work", "text": "In this work, we introduce the IMPLI dataset, which we then use to evaluate NLI models' capabilities on figurative language. We show that while widely used MNLI models handle entailment admirably and metaphoric expressions are relatively easy, nonentailment idiomatic relationships are more difficult. Additionally, adding idiom-specific training data fails to alleviate poor performance for nonentailing pairs. This highlights how currently language models are inherently limited in representing some figurative phenomena and can provide a target for future model improvements.\nFor future work, we aim to expand our data collection processes to new data sources. Our dataset creation procedure relies on annotated samples and definitions: as more idiomatic and metaphoric resources become available, this process is broadly extendable to create new figurative/literal pairs. Additionally, we only explore this data for evaluating NLI systems: this data could also be used for other parallel data tasks such as figurative language interpretation (Shutova, 2013;Su et al., 2017) and figurative paraphrase generation. As natural language generation often relies on training or fine-tuning models with paired sentences, this data could be a valuable resource for figurative language generation systems.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A Model Hyperparameters", "text": "We use a fixed set of hyperparameters for all NLI fine-tuning experiments: learning rate of 1e \u22125 , batch size 32, and maximum input length of 128 tokens. The models are trained for 3 epochs. We used the HuggingFace implementation of the models (Wolf et al., 2020).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "B Lexical Overlap", "text": "Previous research shows that NLI systems exploit cues based on lexical overlap, predicting entailment for overlapping sentences (McCoy et al., 2019;Nie et al., 2019). Our dataset consists mostly of pairs with high overlap: this could explain why the nonentailment sections are more difficult. We thus evaluate system predictions for our datasets as a function of lexical overlap. Figure 5 shows densitybased histograms of the results, comparing overlap via Levenshtein distance (Levenshtein, 1965) for correctly and incorrectly classified pairs.\nOur data contains higher overlap than the MNLI data, with the bulk of the density falling on minimally distant pairs. We also note a distinct difference between our entailment and non-entailment pairs: non-entailments contain extremely high overlap and are frequently misclassified in these cases where the distance is small, matching previous reports for NLI tasks: lexical overlap is a key artifact for entailment, and this reliance persists when classifying idiomatic pairs.", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "C Syntactic Flexibility Correlations", "text": "Figure 6 shows correlations between ICE scores (determined by frequency of occurences of a given IE outside of its normal form) and roberta-base model performance on that IE.   ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "D Dataset Examples", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "", "text": "It would be good to roll in a difficult situation all over. Pour in the soup.\nPour in trouble. There's a marina down in the docks.\nThere's a marina down under scrutiny.\n( The mule kicked back into my shoulder. This was conveniently encapsulated on the first try.\nThis was conveniently encapsulated in the first battle. On their tracks his eyes were fastened.\nOn their tracks his hands were fastened. ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Potential idiomatic expression (pie)-english: Corpus for classes of idioms", "journal": "", "year": "2021", "authors": "P Tosin; Saleha Adewumi; Roshanak Javed; Aparajita Vadoodi; Konstantina Tripathy; Foteini Nikolaidou; Marcus Liwicki;  Liwicki"}, {"title": "Metaphor in textual entailment", "journal": "", "year": "2008", "authors": "Rodrigo Agerri"}, {"title": "Textual entailment as an evaluation framework for metaphor resolution: A proposal", "journal": "College Publications", "year": "2008", "authors": "Rodrigo Agerri; John Barnden; Mark Lee; Alan Wallington"}, {"title": "Predicting human metaphor paraphrase judgments with deep neural networks", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yuri Bizzoni; Shalom Lappin"}, {"title": "A large annotated corpus for learning natural language inference", "journal": "", "year": "2015", "authors": "R Samuel; Gabor Bowman; Christopher Angeli; Christopher D Potts;  Manning"}, {"title": "A large annotated corpus for learning natural language inference", "journal": "", "year": "2015", "authors": "R Samuel; Gabor Bowman; Christopher Angeli; Christopher D Potts;  Manning"}, {"title": "New protocols and negative results for textual entailment data collection", "journal": "", "year": "2020", "authors": "R Samuel; Jennimaria Bowman; Livio Baldini Palomaki; Emily Soares;  Pitler"}, {"title": "Figurative language in recognizing textual entailment", "journal": "", "year": "2021", "authors": "Tuhin Chakrabarty; Debanjan Ghosh; Adam Poliak; Smaranda Muresan"}, {"title": "MERMAID: Metaphor generation with symbolism and discriminative decoding", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Tuhin Chakrabarty; Xurui Zhang; Smaranda Muresan; Nanyun Peng"}, {"title": "The pascal recognising textual entailment challenge", "journal": "Springer Berlin Heidelberg", "year": "2006", "authors": "Oren Ido Dagan; Bernardo Glickman;  Magnini"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "SemEval-2015 task 11: Sentiment analysis of figurative language in Twitter", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Aniruddha Ghosh; Guofu Li; Tony Veale; Paolo Rosso; Ekaterina Shutova; John Barnden; Antonio Reyes"}, {"title": "MAGPIE: A large corpus of potentially idiomatic expressions", "journal": "", "year": "2020", "authors": "Hessel Haagsma; Johan Bos; Malvina Nissim"}, {"title": "The Pragmatics behind Politics: Modelling Metaphor, Framing and Emotion in Political Discourse", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Pere-Llu\u00eds Huguet Cabot; Verna Dankers; David Abadi; Agneta Fischer; Ekaterina Shutova"}, {"title": "Figurative usage detection of symptom words to improve personal health mention detection", "journal": "", "year": "2019", "authors": "Adith Iyer; Aditya Joshi; Sarvnaz Karimi; Ross Sparks; Cecile Paris"}, {"title": "The Gutenberg English poetry corpus: exemplary quantitative narrative analyses", "journal": "Frontiers in Digital Humanities", "year": "2018", "authors": "M Arthur;  Jacobs"}, {"title": "SLIDE -a sentiment lexicon of common idioms", "journal": "European Language Resources Association", "year": "2018", "authors": "Charles Jochim; Francesca Bonin; Roy Bar-Haim; Noam Slonim"}, {"title": "Accurate unlexicalized parsing", "journal": "Association for Computational Linguistics", "year": "2003", "authors": "Dan Klein; Christopher D Manning"}, {"title": "SemEval-2013 task 5: Evaluating phrasal semantics", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Ioannis Korkontzelos; Torsten Zesch; Fabio Massimo Zanzotto; Chris Biemann"}, {"title": "Metaphors We Live By", "journal": "University of Chicago Press", "year": "1980", "authors": "George Lakoff; Mark Johnson"}, {"title": "Improving hate speech type and target detection with hateful metaphor features", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Jens Lemmens; Ilia Markov; Walter Daelemans"}, {"title": "Binary codes capable of correcting deletions, insertions, and reversals", "journal": "", "year": "1965", "authors": " Vladimir Iosifovich Levenshtein"}, {"title": "RoBERTa: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Word embedding and WordNet based metaphor identification and interpretation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Rui Mao; Chenghua Lin; Frank Guerin"}, {"title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "journal": "", "year": "2019", "authors": "Tom Mccoy; Ellie Pavlick; Tal Linzen"}, {"title": "Metaphor as a medium for emotion: An empirical study", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Saif Mohammad; Ekaterina Shutova; Peter Turney"}, {"title": "Analyzing compositionality-sensitivity of nli models", "journal": "", "year": "2019", "authors": "Yixin Nie; Yicheng Wang; Mohit Bansal"}, {"title": "", "journal": "Idioms. Language", "year": "1994", "authors": "Geoffrey Nunberg; Ivan A Sag; Thomas Wasow"}, {"title": "Hypothesis only baselines in natural language inference", "journal": "", "year": "2018", "authors": "Adam Poliak; Jason Naradowsky; Aparajita Haldar; Rachel Rudinger; Benjamin Van Durme"}, {"title": "Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Nils Reimers; Iryna Gurevych"}, {"title": "Multiword expressions: A pain in the neck for nlp", "journal": "Springer Berlin Heidelberg", "year": "2002", "authors": "A Ivan; Timothy Sag; Francis Baldwin; Ann Bond; Dan Copestake;  Flickinger"}, {"title": "Metaphor identification as interpretation", "journal": "", "year": "2013", "authors": "Ekaterina Shutova"}, {"title": "Computational approaches to figurative language", "journal": "", "year": "2011", "authors": "V Ekaterina;  Shutova"}, {"title": "A Method for Linguistic Metaphor Identification: From MIP to MIPVU", "journal": "John Benjamins", "year": "2010", "authors": "Gerard J Steen; Aletta G Dorst; J Berenike Herrmann; Anna Kaal; Tina Krennmayr; Trijntje Pasma"}, {"title": "Corpus-Based Approaches to Metaphor and Metonymy", "journal": "De Gruyter Mouton", "year": "2008", "authors": "Anatol Stefanowitsch; Stefan Th;  Gries"}, {"title": "Metaphor generation with conceptual mappings", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Kevin Stowe; Tuhin Chakrabarty; Nanyun Peng; Smaranda Muresan; Iryna Gurevych"}, {"title": "Automatic detection and interpretation of nominal metaphor based on the theory of meaning", "journal": "Neurocomputing", "year": "2017", "authors": "Chang Su; Shuman Huang; Yijiang Chen"}, {"title": "Metaphor detection with cross-lingual model transfer", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Yulia Tsvetkov; Leonid Boytsov; Anatole Gershman; Eric Nyberg; Chris Dyer"}, {"title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Metaphor: A computational perspective", "journal": "Morgan and Claypool", "year": "2016", "authors": "Tony Veale; Ekatarina Shutova; Beata Beigman Klebanov"}, {"title": "Superglue: A stickier benchmark for general-purpose language understanding systems", "journal": "Curran Associates Inc", "year": "2019", "authors": "Alex Wang; Yada Pruksachatkun; Nikita Nangia; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "Long Papers", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger;  Drame"}, {"title": "Identifying depressive symptoms from tweets: Figurative language enabled multitask learning framework", "journal": "", "year": "2020", "authors": "Shweta Yadav; Jainish Chauhan; Joy Prakash Sain; Krishnaprasad Thirunarayan; Amit Sheth; Jeremiah Schumm"}, {"title": "PIE: A parallel idiomatic expression corpus for idiomatic sentence generation and paraphrasing", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Jianing Zhou; Hongyu Gong; Suma Bhat"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Fig. Type", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure1: Idiomatic definition replacement. Pairs are generated using corrected dictionary definitions, substituted into figurative (left) and literal (center) sentences.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "1. drop prices \u2192 reduce prices", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "\u2022R1: How well do pre-trained models perform on figurative entailments and nonentailments? \u2022 R2: Does adding idiomatic pairs into the training data affect model performance? \u2022 R3: Does the flexibility of idiomatic expressions affect model performance?", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: R2: Training. Performance of the roberta-base models as more idiom examples are added to the training data.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: R2: Lexical Overlap. Classification performance by lexical overlap. The x-axis shows Levenshtein distance; the y-axis shows stacked density of correctly and incorrectly tagged pairs. The IMPLI non-entailments contain extremely high overlap, and are thus frequently misclassified as entailment.", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: R3: Syntactic Flexibility. Performance of idiom types compared to their syntactic flexibility (based on ICE score defined in R3), with Spearman coefficient correlations r and significance values p. The middle figure is non-entailments based on replacement in literal context; the right is those based on adversarial definitions. Further right on the x-axis indicates greater flexibility.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Examples of entailment (\u2192) and nonentailment pairs ( ) from the IMPLI dataset.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": Dataset Summary: Overview of entailments/non-entailment types in IMPLI. (\u2192) denotes entailments,( ) non-entailments. Note that the descriptions are simplified: some intermediate steps are omitted (see  \u00a73.1).pairs to enable NLI-based evaluation.Similar to this work, Chakrabarty et al. (2021a)build a dataset for NLI based on figurative lan-guage."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Sampled hand-written adversarial definitions.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Valid pairs. Percentage of valid pairs, syntactically and with regard to the intended entailments, of automatic data generation.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "base  .878  .876  .848 .539 .409 .890 .771 .311 .947 .818 .818  roberta-large .899  .899  .866 .536 .418 .889 .777 .348 .936 .871 .840    ", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "R1: Model accuracy. Accuracy on MNLI and IMPLI pairs, divided into silver (S) and gold (G) datasets. S l Silver non-entailment based on replacement in literal contexts, S d Silver non-entailment based on adversarial definitions, G a Gold non-entailment based on antonyms.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "shows examples from each type of pair generation.", "figure_data": "5386"}], "doi": "10.18653/v1/W18-0906"}