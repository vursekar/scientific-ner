{"authors": "Zuchao Li; Shexia He; Zhuosheng Zhang; Hai Zhao", "pub_date": "", "title": "Joint Learning of POS and Dependencies for Multilingual Universal Dependency Parsing", "abstract": "This paper describes the system of team LeisureX in the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Our system predicts the part-of-speech tag and dependency tree jointly. For the basic tasks, including tokenization, lemmatization and morphology prediction, we employ the official baseline model (UDPipe). To train the low-resource languages, we adopt a sampling method based on other richresource languages. Our system achieves a macro-average of 68.31% LAS F1 score, with an improvement of 2.51% compared with the UDPipe.", "sections": [{"heading": "Introduction", "text": "The goal of Universal Dependencies (UD) (Nivre et al., 2016;Zeman et al., 2017) is to develop multilingual treebank, whose annotations of morphology and syntax are cross-linguistically consistent. In this paper, we describe our system for the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies (Zeman et al., 2018), and we focus only on the subtasks of part-of-speech (POS) tagging and dependency parsing. For the intermediate steps, including tokenization, lemmatization and morphology prediction, we tackle them by the official baseline model (UDPipe) 1 .\nDependency parsing that aims to predict the existence and type of linguistic dependency relations between words, is a fundamental part in natural language processing (NLP) tasks (Li et al., 2018c;. Many referential natural language processing studies (Zhang et al., 2018;Cai et al., 2018;Li et al., 2018b;Wang et al., 2018;Qin et al., 2017) can also contribute to the universal dependency parsing system. Universal dependency parsing focuses on learning syntactic dependency structure over many typologically different languages, even low-resource languages in a real-world setting. Within the dependency parsing literature, there are two dominant techniques, graph-based (McDonald et al., 2005;Ma and Zhao, 2012;Kiperwasser and Goldberg, 2016; and transition-based parsing (Nivre, 2003;Dyer et al., 2015;. Graph-based dependency parsers enjoy the advantage of the global search which learns the scoring functions for all possible parsing trees to find the globally highest scoring one while transition-based dependency parsers build dependency trees from left to right incrementally, which makes the series of multiple choice decisions locally.\nIn our system, we adopt the transition-based dependency parsing in view of its relatively lower time complexity. Our system implements universal dependency parsing based on the stack-pointer networks (STACKPTR) parser introduced by (Ma et al., 2018). Furthermore, previous work (Straka et al., 2016;Nguyen et al., 2017) showed that POS tags are helpful to dependency parsing. In particular, (Nguyen et al., 2017) pointed out that parsing performance could be improved by the merit of accurate POS tags and the context of syntactic parse tree could help resolve POS ambiguities. Therefore, we seek to jointly learn POS tagging and dependency parsing.\nAs Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) have shown significant representational effectiveness to a wide range of NLP tasks, we leverage bidirectional LSTMs (BiLSTM) to learn shared representations for both POS tagging and dependency parsing. In addition, to train the low-resource languages, we adopt a sampling method based on other richresource languages.\nIn terms of all the above model improvement, compared to the UDPipe baseline, our system achieves a macro-average of 68.31% LAS F1 score, with an improvement of 2.51% in this task.", "n_publication_ref": 20, "n_figure_ref": 0}, {"heading": "Our Model", "text": "In this section, we describe our joint model 2 for POS tagging and dependency parsing in the CoNLL 2018 Shared Task, which is built on the STACKPTR parser introduced by (Ma et al., 2018). Our model is mainly composed of three components, the representation (Section 2.1), POS tagger (Section 2.2) and dependency parser (Section 2.3). Figure 1 illustrates the overall model.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Representation", "text": "Representation is a key component in various NLP models, and good representations should ideally model both complex characteristics and linguistic contexts. In our system, we follow the bidirectional LSTM-CNN architecture (BiLSTM-CNNs) (Chiu and Nichols, 2016;Ma and Hovy, 2016), where CNNs encode word information into character-level representation and BiLSTM models context information of each word.\nCharacter Level Representation Though word embedding is popular in many existing parsers, they are not ideal for languages with high out-ofvocabulary (OOV) ratios. Hence, our system introduces the character-level (Li et al., 2018a) representation to address the challenge. Formally, given a word w = {BOW, c 1 , c 2 , ..., c n , EOW }, where two special BOW (begin-of-word) and EOW (end-of-word) tags indicate the begin and end positions respectively, we use the CNN to extract character-level representation as follows:\ne c = M axP ool(Conv(w))\nwhere the CNN is similar to the one in (Chiu and Nichols, 2016), but we use only characters as the inputs to CNN, without character type features.\nWord Level Representation Word embedding is a standard component of most state-of-the-art NLP architectures. Due to their ability to capture syntactic and semantic information of words from large scale unlabeled texts, we pre-train the word embeddings from the given training dataset by word2vec (Mikolov et al., 2013) toolkit. For low-resource languages without available training data, we sample the training dataset from similar languages to generate a mixed dataset.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "POS Tagger", "text": "To enrich morphological information, we also incorporate UPOS tag embeddings into the representation. Therefore, we jointly predict the UPOS tag in our system. The architecture for the POS tagger in our model is almost identical to that of the parser . The tagger uses a BiLSTM over the concatenation of word embeddings and character embeddings:\ns pos i = BiLST M pos (e w i e c i )\nThen we calculate the probability of tag for each type using affine classifiers as follows:\nh pos i = M LP pos (s pos i ) r pos i = W pos h pos i + b pos y pos i = arg max(r i )\nThe tag classifier is trained jointly using crossentropy losses that are summed together with the dependency parser loss during optimization.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Context-sensitive Representation", "text": "In order to integrate contextual information, we concatenate the character embedding e c , pre-trained word embedding e w and UPOS tag embedding e pos , then feed them into the BiLSTM. We take the bidirectional vectors at the final layer as the contextsensitive representation:\n\u2212 \u2192 s i = LST M f orward (e w i e c i e pos i ) \u2190 \u2212 s i = LST M backward (e w i e c i e pos i ) s i = \u2212 \u2192 s i \u2190 \u2212 s i\nNotably, we use the UPOS tag from the output of our POS tagging model. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dependency Parsing", "text": "The universal dependency parsing component of our system is built on the current state-of-the-art approach STACKPTR, which combines pointer networks (Vinyals et al., 2015) with an internal stack for tracking the status of depth-first search. It benefits from the global information of the sentence and all previously derived subtree structures, and removes the left-to-right restriction in classical transition-based parsers.\nThe STACKPTR parser mainly consists of two parts: encoder and decoder. The encoder based on BiLSTM-CNNs architecture takes the sequence of tokens and their POS tags as input, then encodes it into encoder hidden state s i . The internal stack \u03c3 is initialized with dummy ROOT. For decoder (a uni-directional RNN), it receives the input from last step and outputs decoder hidden state h t . The pointer neural network takes the top element w h in the stack \u03c3 at each timestep t as current head to select a specific child w c with biaffine attention mechanism  for attention score function in all possible head-dependent pairs. Then the child w c will be pushed onto the stack \u03c3 for next step when c = h, otherwise it indicates that all children of the current head h have been selected, therefore the head w h will be popped out of the stack \u03c3. The attention scoring function used is given as follows and the pointer neural network uses a t as pointer to select the child element:\ne t i = h T t Ws i + U T h t + V T s i + b a t = sof tmax(e t )\nMore specifically, the decoder maintains a list of available words in test phase. For each head h at each decoding step, the selected child will be removed from the list to make sure that it cannot be selected as a child of other head words.\nGiven a dependency tree, there may be multiple children for a specific head. This results in more than one valid selection for each time step, which might confuse the decoder. To address this problem, the parser introduces an inside-outside order to utilize second-order sibling information, which has been proven to be an important feature for parsing process (McDonald and Pereira, 2006;Koo and Collins, 2010). To utilize the secondorder information, the parser replaces the input of decoder from s i as follows:\n\u03b2 i = s s \u2022 s h \u2022 s i\nwhere s and h indicate the sibling and head index of node i, \u2022 is the element-wise sum operation to ensure no additional model parameters.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Loss Function", "text": "The training objective of pur system is to learn the probability of UPOS tags P \u03b8 pos (y pos |x) and the dependency trees P \u03b8 dep (y dep |x, y pos ). Given a sentence x, the probabilities are factorized as:\nP \u03b8 pos (y pos |x) = k i=1 P \u03b8 pos (p i |x) y pos = arg max ypos\u2208Ypos (P \u03b8 pos (y pos |x)) P \u03b8 dep (y dep |x, y pos ) = k i=1 P \u03b8 dep (p i |p <i , x, y pos ) = k i=1 l i j=1 P \u03b8 dep (c i,j |c i,<j , p <i , x, y pos )\nwhere \u03b8 pos and \u03b8 dep represent the model parameters respectively. p <i denotes the preceding dependency paths that have already been generated. c i,j represents the j th word in p i and c i,j denotes all the proceeding words on the path p i .\nTherefore, the whole loss is the sum of three objectives:\nLoss = Loss pos + Loss arc + Loss label\nwhere the Loss pos , Loss arc and Loss label are the conditional likehood of their corresponding target, using the cross-entropy loss. Specifically, we train a dependency label classifier following , which takes the dependency head-child pair as input features.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "System Implements", "text": "Our system focuses on three targets: the UPOS tag, dependency arc and dependency relation. Therefore, we rely on the UDPipe model (Straka Treebank Sampling Breton KEB English, Irish Czech PUD Czech PDT English PUD English EWT Faroese OFT Norwegian, English, Danish, Swedish, German, Dutch Finnish PUD Finnish TDT Japanese Modern Japanese GSD Naija NSC English Swedish PUD Swedish Talbanken Thai PUD English, Chinese, Hindi, Vietnamese For treebanks with non-empty training dataset (including treebanks whose training set is very small), we utilize the baseline model UDPipe trained on corresponding treebank, which has been provided by the organizer. For the remaining nine treebanks without training data, we construct the train dataset by sampling from the other training datasets according to the language similarity inspired by (Zhao et al., 2009(Zhao et al., , 2010Wang et al., , 2016, as detailed in Table 1.\nOur system adopts the hyper-parameter configuration in (Ma et al., 2018), with a few exceptions. We initialize word vectors with 50-dimensional pretrained word embeddings, 100-dimensional tag embeddings and 512-dimensional recurrent states (in each direction). Our system drops embeddings and hidden states independently with 33% probability. We optimize with Adam (Kingma and Ba, 2015), setting the learning rate to 1e \u22123 and \u03b2 1 = \u03b2 2 = 0.9. Moreover, we train models for up to 100 epochs with batch size 32 on 3 NVIDIA GeForce GTX 1080Ti GPUs with 200 to 500 sentences per second and occupying 2 to 3 GB graphic memory each model. A full run over the test datasets on the TIRA virtual machine (Potthast et al., 2014) takes about 12 hours.  with absolute gains (1.28-3.08%) on average LAS, UAS, MLAS and CLAS. These results show that our joint model could improve the performance of universal dependency parsing. Surprisingly, in the case of POS tagging, our joint model obtains lower averaged accuracy in both UPOS and XPOS. The possible reason for performance degradation may be that we select all hyper-parameters based on English and do not tune them individually. Furthermore, we also compare the performance of our system with the baseline and the best scorer on big treebanks (Table 3), PUD treebanks (Table 4), low-resource languages (Table 5), respectively.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Results", "text": "Since our model applies the baseline model for tokenization and segmentation, we show all results of focused metrics on each treebank in Table 6. In addition, we compare our model with the best and the average results of top ten models on each treebank, using LAS F1 for the evaluation metric, as shown in Figure 2.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "In this paper, we describe our system in the CoNLL 2018 shared task on UD parsing. Our system uses a transition-based neural network architecture for dependency parsing, which predicts the UPOS tag and dependencies jointly. Combining pointer networks with an internal stack to track the status of the top-down, depth-first search in the parsing decoding procedure, the STACKPTR parser is able to capture information from the whole sentence and all the previously derived subtrees, removing the left-to-right restriction in classical transition-based parsers, while maintaining     ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Deep enhanced representation for implicit discourse relation recognition", "journal": "", "year": "2018", "authors": "Hongxiao Bai; Hai Zhao"}, {"title": "A full end-to-end semantic role labeler, syntacticagnostic over syntactic-aware?", "journal": "", "year": "2018", "authors": "Jiaxun Cai; Shexia He; Zuchao Li; Hai Zhao"}, {"title": "Named entity recognition with bidirectional LSTM-CNNs", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "P C Jason; Eric Chiu;  Nichols"}, {"title": "Deep biaffine attention for neural dependency parsing", "journal": "", "year": "2017", "authors": "Timothy Dozat; D Christopher;  Manning"}, {"title": "Stanford's graph-based neural dependency parser at the conll 2017 shared task", "journal": "", "year": "2017", "authors": "Timothy Dozat; Peng Qi; Christopher D Manning"}, {"title": "Transitionbased dependency parsing with stack long shortterm memory pages", "journal": "", "year": "2015", "authors": "Chris Dyer; Miguel Ballesteros; Wang Ling; Austin Matthews; Noah A Smith"}, {"title": "Syntax for semantic role labeling, to be, or not to be", "journal": "Long Papers", "year": "2018", "authors": "Shexia He; Zuchao Li; Hai Zhao; Hongxiao Bai"}, {"title": "Long short-term memory", "journal": "Neural Computation", "year": "1997", "authors": "Sepp Hochreiter; Jrgen Schmidhuber"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "journal": "", "year": "2016", "authors": "Eliyahu Kiperwasser; Yoav Goldberg"}, {"title": "Efficient thirdorder dependency parsers", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Terry Koo; Michael Collins"}, {"title": "Neural character-level dependency parsing for Chinese", "journal": "", "year": "2018", "authors": "Haonan Li; Zhisong Zhang; Yuqi Ju; Hai Zhao"}, {"title": "Seq2seq dependency parsing", "journal": "", "year": "2018", "authors": "Zuchao Li; Jiaxun Cai; Shexia He; Hai Zhao"}, {"title": "A unified syntax-aware framework for semantic role labeling", "journal": "", "year": "2018", "authors": "Zuchao Li; Shexia He; Jiaxun Cai; Zhuosheng Zhang; Hai Zhao; Gongshen Liu; Linlin Li; Luo Si"}, {"title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF", "journal": "", "year": "2016", "authors": "Xuezhe Ma; Eduard Hovy"}, {"title": "Stackpointer networks for dependency parsing", "journal": "Long Papers", "year": "2018", "authors": "Xuezhe Ma; Zecong Hu; Jingzhou Liu; Nanyun Peng; Graham Neubig; Eduard Hovy"}, {"title": "Fourth-order dependency parsing", "journal": "", "year": "2012", "authors": "Xuezhe Ma; Hai Zhao"}, {"title": "Online large-margin training of dependency parsers", "journal": "Association for Computational Linguistics", "year": "2005", "authors": "Ryan Mcdonald; Koby Crammer; Fernando Pereira"}, {"title": "Online learning of approximate dependency parsing algorithms", "journal": "Association for Computational Linguistics", "year": "2006", "authors": "Ryan Mcdonald; Fernando Pereira"}, {"title": "Efficient estimation of word representations in vector space", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"title": "A novel neural network model for joint pos tagging and graph-based dependency parsing", "journal": "", "year": "2017", "authors": "Mark Dat Quoc Nguyen; Mark Dras;  Johnson"}, {"title": "An efficient algorithm for projective dependency parsing", "journal": "", "year": "2003", "authors": "Joakim Nivre"}, {"title": "Universal Dependencies v1: A multilingual treebank collection", "journal": "Portoro", "year": "2016", "authors": "Joakim Nivre; Marie-Catherine De Marneffe; Filip Ginter; Yoav Goldberg; Jan Haji\u010d; Christopher Manning; Ryan Mcdonald; Slav Petrov; Sampo Pyysalo; Natalia Silveira; Reut Tsarfaty; Daniel Zeman"}, {"title": "Improving the reproducibility of PAN's shared tasks: Plagiarism detection, author identification, and author profiling", "journal": "", "year": "2014", "authors": "Martin Potthast; Tim Gollub; Francisco Rangel; Paolo Rosso; Efstathios Stamatatos; Benno Stein"}, {"title": "Adversarial connectiveexploiting networks for implicit discourse relation classification", "journal": "", "year": "2017", "authors": "Lianhui Qin; Zhisong Zhang; Hai Zhao; Zhiting Hu; Eric Xing"}, {"title": "UD-Pipe: trainable pipeline for processing CoNLL-U files performing tokenization, morphological analysis, POS tagging and parsing", "journal": "Portoro", "year": "2016", "authors": "Milan Straka; Jan Haji\u010d; Jana Strakov\u00e1"}, {"title": "Pointer networks", "journal": "", "year": "2015", "authors": "Oriol Vinyals; Meire Fortunato; Navdeep Jaitly"}, {"title": "Converting continuous-space language models into ngram language models with efficient bilingual pruning for statistical machine translation", "journal": "ACM Transactions on Asian and Low-Resource Language Information Processing", "year": "2016", "authors": "Rui Wang; Masao Utiyama; Isao Goto; Eiichiro Sumita; Hai Zhao; Bao-Liang Lu"}, {"title": "Bilingual continuousspace language model growing for statistical machine translation", "journal": "IEEE/ACM Transactions on Audio, Speech, and Language Processing", "year": "2015", "authors": "Rui Wang; Hai Zhao; Bao-Liang Lu; Masao Utiyama; Eiichiro Sumita"}, {"title": "Graphbased bilingual word embedding for statistical machine translation", "journal": "ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)", "year": "2018", "authors": "Rui Wang; Hai Zhao; Sabine Ploux; Bao-Liang Lu; Masao Utiyama; Eiichiro Sumita"}, {"title": "", "journal": "", "year": "", "authors": "Daniel Zeman; Jan Haji\u010d; Martin Popel; Martin Potthast; Milan Straka; Filip Ginter; Joakim Nivre"}, {"title": "Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies", "journal": "", "year": "2018", "authors": "Slav Petrov"}, {"title": "Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"title": "Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies", "journal": "", "year": "2017", "authors": "Daniel Zeman; Martin Popel; Milan Straka; Jan Haji\u010d; Joakim Nivre; Filip Ginter; Juhani Luotolahti; Sampo Pyysalo; Slav Petrov; Martin Potthast; Francis Tyers; Elena Badmaeva; Memduh G\u00f6k\u0131rmak; Anna Nedoluzhko; Silvie Cinkov\u00e1; Jan Haji\u010d Jr; Jaroslava Hlav\u00e1\u010dov\u00e1; V\u00e1clava Kettnerov\u00e1; Zde\u0148ka Ure\u0161ov\u00e1; Jenna Kanerva; Stina Ojala; Anna Missil\u00e4; Christopher Manning; Sebastian Schuster; Siva Reddy; Dima Taji; Nizar Habash; Herman Leung; Marie-Catherine De Marneffe; Manuela Sanguinetti; Maria Simi; Hiroshi Kanayama"}, {"title": "Stack-based multi-layer attention for transition-based dependency parsing", "journal": "", "year": "2017", "authors": "Zhirui Zhang; Shujie Liu; Mu Li; Ming Zhou; Enhong Chen"}, {"title": "Subword-augmented embedding for cloze reading comprehension", "journal": "", "year": "2018", "authors": "Zhuosheng Zhang; Yafang Huang; Hai Zhao"}, {"title": "How large a corpus do we need: Statistical method versus rulebased method", "journal": "Training (M)", "year": "2010", "authors": "Hai Zhao; Yan Song; Chunyu Kit"}, {"title": "Cross language dependency parsing using a bilingual lexicon", "journal": "", "year": "2009", "authors": "Hai Zhao; Yan Song; Chunyu Kit; Guodong Zhou"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The joint model for POS tagging and dependency parsing.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": ": Language substitution for treebanks with-out training dataet al., 2016) to provide a pipeline from raw textto basic dependency structures, including a tok-enizer, tagger and the dependency predictor."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "reports the official evaluation results ofour system in several metrics of treebanks fromthe CoNLL 2018 shared task (?). For depen-dency parsing, our model outperforms the baseline"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Results on all treebanks.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Results on big treebank only.", "figure_data": "ResultsOurs Baseline BestLAS61.05 66.63 74.20MLAS41.95 51.75 58.75BLEX50.60 54.87 63.25UAS67.88 71.22 78.42CLAS57.34 61.29 69.86UPOS82.45 85.23 87.51XPOS35.66 54.27 55.98Morphological features 78.89 83.41 87.05Morphological tags34.68 50.32 51.90Lemmas82.24 83.37 85.76Sentence segmentation 75.53 75.53 76.04Word segmentation92.61 92.61 94.57Tokenization92.61 92.61 94.57"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results on PUD treebank only.", "figure_data": "ResultsOurs Baseline BestLAS17.16 17.1727.89MLAS3.43 3.446.13BLEX7.63 7.6313.98UAS30.07 30.0839.23CLAS13.42 13.4222.18UPOS45.17 45.2061.07XPOS54.68 54.2354.73Morphological features 38.03 38.0348.95Morphological tags25.86 25.7225.91Lemmas54.25 54.2564.42Sentence segmentation 65.99 65.9967.50Word segmentation84.95 84.9593.38Tokenization85.76 85.7693.34"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Results on low-resource languages only. LAS F1 score per treebank. For comparison, we include the best official result and the average of the top ten results on each treebank.", "figure_data": "BestOurs80Avg.Top106040200af_afriboomsar_padtbg_btbbr_kebbxr_bdtca_ancoracs_caccs_fictreecs_pdtcs_pudcu_proielda_ddtde_gsdel_gdten_ewten_gumen_linesen_pudes_ancoraet_edteu_bdtfa_serajifi_ftbfi_pudfi_tdtfo_oftfro_srcmffr_gsdfr_sequoiafr_spokenga_idtgl_ctggl_treegalgot_proielgrc_perseusgrc_proielhe_htbhi_hdtbhr_sethsb_ufalhu_szegedBestOurs80Avg.Top106040200hy_armtdpid_gsdit_isdtit_postwitaja_gsdja_modernkk_ktbkmr_mgko_gsdko_kaistla_ittbla_perseusla_proiellv_lvtbnl_alpinonl_lassysmallno_bokmaalno_nynorskno_nynorskliapcm_nscpl_lfgpl_szpt_bosquero_rrtru_syntagrusru_taigask_snksl_ssjsl_sstsme_giellasr_setsv_linessv_pudsv_talbankenth_pudtr_imstug_udtuk_iuur_udtbvi_vtbzh_gsdFigure 2: linear parsing steps. Furthermore, our model issingle instead of ensemble, and it does not uti-lize lemmas or morphological features. Resultsshow that our system achieves 68.31% in macro-averaged LAS F1-score on the official blind test.Further improvements could be obtained by multi-lingual embeddings and adopting ensemble meth-ods."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Performances of focused metrics on each treebank.", "figure_data": ""}], "doi": "10.18653/v1/K18-2006"}