{"authors": "Hongyuan Lu; Wai Lam; Hong Cheng; Helen M Meng", "pub_date": "", "title": "Partner Personas Generation for Dialogue Response Generation", "abstract": "Incorporating personas information allows diverse and engaging responses in dialogue response generation. Unfortunately, prior works have primarily focused on self personas and have overlooked the value of partner personas. Moreover, in practical applications, the availability of the gold partner personas is often not the case. This paper attempts to tackle these issues by offering a novel framework that leverages automatic partner personas generation to enhance the succeeding dialogue response generation. Our framework employs reinforcement learning with a dedicatedly designed critic network for reward judgement. Experimental results from automatic and human evaluations indicate that our framework is capable of generating relevant, interesting, coherent and informative partner personas, even compared to the ground truth partner personas. This enhances the succeeding dialogue response generation, which surpasses our competitive baselines that condition on the ground truth partner personas.", "sections": [{"heading": "Introduction", "text": "Building informative and engaging dialogue agents Roller et al., 2021) is a popular research direction within the area of natural language processing. For the sake of engagement, diverse and consistent responses (Song et al., 2020(Song et al., , 2021 are important factors, and personas information  gives rise to both. There are two types of personas, namely self persona and partner persona. The former refers to a self profile consisting of several sentences representing the dialogue agents. Such a persona allows producing consistent responses rather than solely relying on the personas that are randomly learned and embedded in the model parameters (Kim et al., 2020). The latter refers to a profile that represents the users. Leveraging such partner personas has been empirically shown to be helpful for dialogue response selection (Gu et al., 2021).\nUnfortunately, the existence of partner personas suffers from the cold start (Schein et al., 2002;Zhang et al., 2014; at the beginning of the conversation. Most of the works, if not all, (Li et al., 2016b;Mazar\u00e9 et al., 2018;Gu et al., 2019;Zhao et al., 2019;Madotto et al., 2019;Majumder et al., 2020;Wu et al., 2020a;Song et al., 2020) have been either overlooking partner personas or simply focusing on the impractical situation where partner personas guarantee to exist. In contrast, our work does not suffer from the practical issue when partner personas are missing during inference, and our proposed framework surpasses the baseline that conditions on the ground truth partner personas.\nTo our knowledge, this is the first attempt to formulate partner personas generation for improved performance on the downstream dialogue response generation. Our work is motivated by the underlying hypothesis that partner personas generation is plausible given the self personas and dialogue context. Automatic and human evaluation results support the hypothesis and indicate that generated personas are even more interesting than the ground truth, which improves the downstream dialogue response generation. This paper thus paves the way to exploit partner personas generation (PPG) for dialogue response generation (DRG).\nWe propose a novel framework composed of three major components, namely a personas generator, a dialogue response generator and a critic network. The personas generator generates partner personas, which the dialogue response generator conditions on. We employ reinforcement learning with a critic network that propagates the reward back to the generators for joint training.\nPrior works have investigated partner persona retrieval . The human-constructed ground truth personas serve as the upper bound for such retrieval-based systems, and we argue that the ground truth is not coherent and diverse enough. Interestingly, we observe that the generative counterpart proposed in our framework generates relevant, informative and coherent partner personas, which further improves the succeeding dialogue response generation. It follows another advantage that our framework does not need an external database to retrieve from (Madotto et al., 2020;.\nOne close work to ours is a multi-task framework for meta-learning (Lee et al., 2021) that uses personas reconstruction as an auxiliary task to improve response consistency. The differences are that theirs does not differentiate between self personas and partner personas, while ours does. Theirs indicates an improvement over personality consistency, while ours report improvements for the overall quality. We conduct an empirical comparison with their model by reconstructing the partner personas. Experimental results indicate that such a multi-task model does not work well in our problem setting. Very recently,  formulates personas generation as a Seq2Seq task for improved downstream response generation via multi-task learning. In contrast, our work leverages reinforcement learning to jointly train the partner personas generator and the response generator.\nAutomatic and human evaluation results indicate that our framework can generate partner personas that are more diverse and interesting than the ground truth partner personas and generate more diverse and engaging responses than the baseline conditioned on ground truth partner personas. 1 2 Related Work", "n_publication_ref": 18, "n_figure_ref": 0}, {"heading": "Personalized Dialgoue Generation", "text": "Conditioning on personas helps to produce informative and engaging responses. The most wellknown multi-turn dialogue dataset conditioned on personal profiles is PERSONACHAT , in which two crowdsourcers converse and find more about each other. The community has proposed many methods to better utilize self personas. Mazar\u00e9 et al. (2018) employs a pre-training stage based on dedicatedly extracted large-scale persona-based dialogues. Zhao et al. (2019) fuses information in personas and dialogue context into individual contextualized representations by attending to different parts of both. Gu et al. (2019) exploits the interaction between personas, dialogue context and response to improve retrieval-based dialogue agents. Madotto et al. (2019) leverages meta-learning with several dialogues of the current speakers to enhance response personality. Welleck et al. (2019) releases a dataset for measuring dialogue consistency. Song et al. (2020) employs a multi-stage pipeline to improve response personality by response rewriting. Lee et al. (2021) uses multi-task learning for improved personality consistency in the meta-learning scenario. Gu et al. (2021) employs four different strategies for personas fusing to leverage both self persona. However, most of these works focus on exploiting self personas rather than partner personas, and they assume the existance of the gold partner personas. Li et al. (2014) leverages distant supervision to classify the spouse, education and job information from user twitters. Wu et al. (2020b) proposes a twostaged profile extractor that extracts attributes before extracting the underlying relationship.  proposes to categorize the profile extraction task into two different difficulties, namely 'extraction' and 'inference', and they leverage a GPT-based generator to extract user profiles. These works have formulated user profile extraction as a classification task that conditions on an input sentence, and they aim at better profile extraction. In contrast, we propose to formulate personas generation to be conditioned dialogue input to be jointly trained with response generation. While ground truth personas serve as the upper bound for these user profile extractors, we empirically demonstrate that our reinforcement learning algorithm surpasses the response model conditioned on the ground truth partner personas. As supported by our human evaluation, we believe the underlying reason is that our model can leverage pre-trained generators to generate coherent and relevant partner personas.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "User Profile Extraction", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Reinforcement Learning", "text": "Reinforcement learning (RL), or specifically, policy gradient methods (Williams, 1992), have been frequently adopted to both task-oriented dialogue agents (Roman Roman et al., 2020;Deng et al., 2021) or open-domain chitchat agents (Li et al., 2016c;Saleh et al., 2020). It can either propagate non-differentiable loss (Cai et al., 2019a) or optimize an expert reward such as ease of answering (Li et al., 2016c). It also adopts a scenario where a user simulator and a dialogue agent interact, and an Figure 1: An example of the inference flow that shows the generated partner personas and the incorporation of partner personas generation into response generation.\nFigure 2: The illustrated reinforcement learning strategy that directly backpropagates the response-related rewards from the critic network to the partner personas generator and the dialogue response generator. expert reward function can be defined to assign the goodness to each response generated (Roman Roman et al., 2020).", "n_publication_ref": 8, "n_figure_ref": 2}, {"heading": "Proposed Framework", "text": "We propose a novel framework composed of three major components, namely a partner personas generator, a dialogue response generator and a reinforcement learning component with a critic network. Figure 1 depicts the inference flow of our setting. The input dialogue context with self persona is first fed into the partner personas generator.\nThe generated partner personas output is then concatenated with the dialogue context and the self personas as the input into the dialogue response generator. In the beginning, we train our partner personas generator and dialogue response generator under supervised learning. In the training stage, we use the ground truth partner personas to train the dialogue response generator, and we replace it with generated partner personas in the inference stage. After the supervised learning stage, the second stage is a reinforcement learning stage which jointly optimizes both partner personas generator and dialogue response generator as depicted in Figure 2 to train the partner personas generator under the reward signal that is relevant to dialogue response generation as well as fine-tuning dialogue response generator trained on the generated partner personas. 2 Particularly, we employ a dedicatedly designed critic network that receives generated partner personas and generated dialogue responses as the input and output a reward that measures the relevance between the generated personas and responses and propagates back to the generators.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Partner Personas Generation (PPG)", "text": "A Seq2Seq neural network (Sutskever et al., 2014) is adopted as our partner personas generator for the task of partner personas generation (PPG). The concatenation of dialogue context c and self personas s is fed as an input into the partner personas generator. The personas generator then outputs an approximated partner personasp conditioned on the input that maximises the following likelihood:\nP (p | s, c) = T t=1 P (p t |p 1 , ...,p t\u22121 , s, c),\nwhere T represents the length of the generated partner personas andp t represents the word at the position t that has been inferenced.\nFor training, the ground truth partner personas p is used and we train our generator to maximise the likelihood P (p | s, c). We generate the complete partner personas profiles in an one-off shot for all the dialogue samples.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Dialogue Response Generation (DRG)", "text": "We also adopt a Seq2Seq neural network for the task of dialogue response generation (DRG). During inference, the concatenation of dialogue context c, self personas s, and generated partner personasp is fed as an input into the dialogue response generator. The response generator then outputs a dialogue responser conditioned on the input, which maximises the conditional likelihood: P (r | s,p, c).\nFor training, the ground truth partner personas p and the ground truth dialogue responses r are used.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Reinforcement Learning (RL)", "text": "We employ a critic network to compute the reinforcement learning rewards for our generators. We use a binary classifier as critic by extracting training instances (s, r, L = 1), 3 (s A , r A , L = 1) and (s B , r B , L = 1). Then we can derive two negative samples as: (s A , r B , L = 0) and (s B , r A , L = 0). Thereafter, we fine-tune on a binary classifier to be used as our critic in RL on the training partition by minimizing the binary cross-entropy loss:\n\u2212Llog(P (L | s, r))\u2212(1\u2212L) log(1 \u2212 P (L | s, r)),\nwhere the binary label L indicates whether the response is relevant to the personas.\nWe then use this classifier acting as a critic network that outputsL, conditioned on the generated partner personasp and generated responser. The predicted binary labelL is then converted to a reward R. R is a positive reward whenL = 1, and R is a negative reward whenL = 0. We empirically set the reward R for RL to {1, -1} for both PPG and DRG. We then update our RL agents with the following gradients:\n\u2206\u03b8 PPG = \u2212R\u25bd \u03b8 PPG log P (p | s, c)\nfor the partner personas generator (PPG), and for the dialogue response generator (DRG):\n\u2206\u03b8 DRG = \u2212R\u25bd \u03b8 DRG log P (r | s,p, c)\nBy formulating a reward that measures the relevance between generated partner personas and generated dialogue response, we are motivated by the following objectives:\n\u2022 Further fine-tune the partner personas generator to generate personas that benefits the downstream dialogue response generation.\n\u2022 Further fine-tune the dialogue response generator trained with ground-truth partner personas to adapt to noisy partner personas generated by the partner personas generator.\nAs mentioned in Section 3.1, the first motivation is that we are generating the complete personas profile. However, some of them can be irrelevant and unhelpful for the next-turn dialogue response generation. It could be challenging for the partner personas generator alone to identify which personas could be helpful. Therefore, we design such a reward to train the personas generator to learn to generate a set of personas that is more helpful for the downstream dialogue response generation.\nOur second motivation is that the dialogue response generator has not been exposed to the generated partner personas. We would like to fine-tune the response generator to mitigate the potential traininginference discrepancy. Experimental results indicate that our design empirically works well. The previous work from Cai et al. (2019a) employed critic network for RL loss backpropagation. The major difference is that their critic is trained in an adversarial manner (Li et al., 2018) to pick up the gold response among other negative candidates. Also, their critic network conditions only on the dialogue response but not on the generated skeleton. In contrast, we aim for improved response generation with a classifier conditioning on both the generated personas and the generated response.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Evaluation Metrics", "text": "For both PPG and DRG, perplexity (PPL) is reported to measure the intrinsic performance with the ground truth output (Roller et al., 2021). We adopt well-known sequence evaluation metrics weighted BLEU (Papineni et al., 2002) and Fmeasure for ROUGE-L (Lin, 2004) as the extrinsic evaluations. For PPG, we also report Distinct-N with N={1,2} to measure the response diversity (Li et al., 2016a;Cai et al., 2019b;Gao et al., 2019) with the ratio of distinct unigrams/bigrams against total number of unigrams/bigrams generated.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dataset", "text": "We conduct experiments on the PERSONACHAT , the most well-known multiturn dialogue dataset conditioned on personas. We follow the train/valid/test split from the PARLAI  platform (Miller et al., 2017) that contains about 65,000/7,800/7,500 instances respectively. Each instance contains about 8 utterances on average and about 4 traits for each of the self and partner personas. We denote the dataset with this original personas as PERSONACHAT-ORI. Later the original personas have been manually scrutinized by rephrasing, generalizing or specializing, which we denote as PERSONACHAT-REV. We apply the same preprocessing operation to both datasets. To train the critic for RL, we collected about 130,000 instances from the train split with equally distributed positive and negative samples. During early experiments, we found that feeding all traits yields lower performance. Retrieving Top-3 relevant partner personas using BM25 (Robertson and Walker, 1994) yields the best performance on the original personas.\nGPT-2 This is a comparison model fine-tuned on GPT-2 (Radford et al., 2019). We build the same three E2E systems described above, and the best model is selected, the third one.\nTRANSFERTRANSFO A comparison model built with a Transformer-based model pre-trained on gen-eral domain corpus, which is then fine-tuned on PERSONACHAT .\nPERCVAE This is a comparison model that employs a memory-augmented architecture incorporated with conditional variational autoencoder that exploits persona information .\nPAML This is a comparison model that leverages several dialogues collected from the same speaker to enhance response personality via metalearning (Madotto et al., 2019). As the authors did not conduct experiments on the PERSONACHAT-REV and no preprocessing scripts are provided for the revised personas, we only report the results of their model on the PERSONACHAT-ORI only.\nMTL w/ Personas Reconstruction This is a multi-task learning (MTL) comparison model (Lee et al., 2021) trained to maximise the objective:\n\u03b1L PPG + (1 \u2212 \u03b1)L DRG ,\nwhere L PPG represents the auxiliary PPG likelihood, and L DRG represents the DRG likelihood. \u03b1 is weight tuned over the validation set, and both tasks condition on dialogue context and self personas and share the same model parameters.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dialogue Response Generation Results", "text": "We build our baselines, the partner personas generator and the dialogue response generator based on a state-of-the-art pre-trained dialogue model DIALOGPT  for parameters Table 2: Case studies that compare our framework against the baseline with the complete partner personas as well as the human response. We present the preceding partner utterance as dialogue context, and we give the most salient ground truth partner personas (Gold Partner) and generated partner personas (Generated Partner) for clarity.\ninitialization. More implementation details can be found in Appendx B.\nThe dialogue response generation results are presented in Table 1. Our framework with reinforcement learning attains the best over all the metrics on both PERSONACHAT-ORI and PERSONACHAT-REV. This supports the usefulness of our framework, which generates reasonable personas and effectively enhances the succeeding dialogue response generation, through the use of RL.\nAlthough TRANSFERTRANSFO attains a better score on the PPL than the fine-tuned GPT-2, GPT-2 have better extrinsic scores than TRANSFER-TRANSFO. GPT-2 also has better overall scores than the E2E baselines without the complete partner personas. However, it is surpassed by the E2E baseline with the complete partner personas during training and inference.\nThe E2E baseline with the complete ground truth partner personas attains better scores on all of the metrics than our remaining baselines. Our framework with RL succeeds the performance of such a competitive baseline for both PERSONACHAT-ORI and PERSONACHAT-REV, indicating our proposed framework's robustness against paraphrasal.\nThe multi-task learning comparison model (Lee et al., 2021) produces less promising results. Concretely, we postulate that the nature of PPG and DRG largely differs. The textual format of partner personas always initiates with first-person sentence starters, while dialogue responses are more general, ranging from greetings to goodbyes. Therefore, it could be hard to capture both in a single model. ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Cold Start", "text": "Cold start is a common problem in recommender systems (Schein et al., 2002;Zhang et al., 2014;. This also applies to dialogue systems, as the partner personas are commonly missing in early turns. We conduct an analysis on the baselines and our framework when N turns are available where N={1,2,3}, using PERSONACHAT-ORI. As demonstrated in Figure 3, all the methods attain a better PPL when N increases, which indicates the existence of the cold start. This is also the case for the baseline with ground truth personas, and we postulate that it fails to learn how to use partner personas during cold start due to the lack of clues. Our framework effectively mitigates the cold start problem and attains the best among them for all N.   our framework successfully recognizes that the partner is asking specifically for metallica. It then conditions on the generated personas to generate a much more entailed response than the baseline. The human response expresses negatively and thus seems less engaging. In the second case, our framework recognizes that the partner has a garden. It then talks about the garden rather than the irrelevant response from the baseline that we postulate is misled by the 'large' adjective in the dialogue context. The human response is potentially sarcastic if the partner is not joking, while our generation does not have such issue. For the third case, the baseline produces a response that could be potentially offensive, which could be biased by the word 'violent' in the dialogue context. In contrast, our framework recognizes the identity of the partner to generate a response without such an issue. The human response tends to raise a new topic and is less relevant. For the fourth case, we observe that the annotator sometimes converses based on the partner profile rather than his own traits. In this case, the annotator (Dialogue Context) said that he has many pets, which is not in his own traits (Gold Partner). Rather, his conversation partner expressed his passion for animals in previous dialogue contexts. We postulate that the annotator attempted to engage the conversation by conditioning his partner personas and telling a relevant joke. Our PPG can recognize this, which further tweaks the model output to talk about dogs and cats rather than the dog only. These cases validate that leveraging partner personas is beneficial, and our framework can generate reasonable partner personas, which is not even in the ground truth.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Case Study on Dialogue Response Generation", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Partner Personas Generation Results", "text": "Table 4 presents the quality measurements of the generated partner personas from our PPG with no RL. We observe that our models have much higher Distinct-N scores as the number of unique words in the generated output is much higher than the ground truth test personas. Compared to the ground truth personas that are limited sets of traits, our generator can leverage the power of pre-trained models for better diversity. The remaining metrics also report reasonable scores, suggesting the plausbility to formulate personas generation as a Seq2Seq task.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Case Study on Partner Personas Generation", "text": "Table 3 presents generated partner personas using PERSONACHAT-ORI. As depicted, our PPG can generate reasonable partner personas which are relevant to the ground truth partner personas. It sometimes gives a reasonable generation which is even not in the ground truth partner personas. In the first case, the generator successfully identifies the partner as being an army ranger. It then becomes rather positive than a violent person as given in the ground truth personas. Conditioning on such positive contents can give a positive response. In the second case, it recognizes the partner as a gym person, and imagines that the partner drinks protein and life weights, which is not in the ground truth personas. In the third case, the generator generates coherent personas, saying that the partner would drink beer and eat food while watching football, which is also not in the ground truth. We postulate that personas could be semantically closer to each other when they frequently co-occur in the training set. Our PPG then tends to generate more coherent   personas by learning such semantical relationship. Since our generated personas are relevant and coherent, we postulate it as the underlying reason why our method gives a better generalization to DRG. In contrast, as demonstrated by Table 3, ground truth personas tend to be more like discrete collections of traits. This could be the reason why some of our generated partner personas could beat the ground truth, which is also supported by our human evaluation in Section 5.6. This is a potential benefit of our approach compared to sentence-level user profile extraction (Li et al., 2014;Wu et al., 2020b; that is upper bounded by the discrete ground truth. We present more examples in Table 8 in the Appendix.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Human Evaluation", "text": "We hired experienced annotators who have degrees relevant to English Linguistics to conduct evaluation on PERSONACHAT-ORI. For both DRG and PPG, we present a questionnaire composed of 800 questions with randomly sampled 200 test instances to three annotators who compare model outputs under A/B testing. As in Zou et al. (2021) and ACUTE-Evals (Li et al., 2020), annotators follow the criteria which we present in Appendix D.  trained under RL surpasses the E2E model that leverages both training and inference ground truth partner personas from all the aspects. Table 6 presents the human evaluation results on PPG. We observe that our PPG generates personas that are more coherent and interesting than the ground truth, which align with the facts observed in Section 5.4 and Section 5.5 indicating that our generated partner personas are more coherent and diverse.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Ablation Study", "text": "We conduct an ablation study on PERSONACHAT-ORI as reported in Table 7 to present the performance of our framework when one of the components is frozen during RL. The result indicates that our proposed framework yields the best result when both of the components are actively trained under RL. We also notice that scaling the RL reward for either PPG or DRG by 10 leads to minor decrease in the performance. Further scaling deteriorates the quality of response generation.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "Our novel framework incorporates partner personas generation into dialogue response generation. It effectively mitigates the problem that partner personas are not available in practical applications as well as the cold start problem during early conversation. The experimental results with both automatic and human evaluation demonstrate that our framework generates coherent, diverse, interesting and engaging partner personas, even compared to the ground truth partner personas. We employ reinforcement learning with a dedicatedly designed critic network that boosts the response generation by conditioning on the generated personas. Automatic and human evaluation results indicate that our response generator surpasses our competitive baselines that condition on the ground truth partner personas. Extensive case studies demonstrate that our framework can generate satisfying dialogue responses and partner personas.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Ethics Statement", "text": "The PERSONACHAT dataset used in this work is well-known and widely used. In our view, there is no known ethical issue with its usage. Large-scale pre-trained models are also employed, but they are widely known to be subject to potential problems such as generating offensiveness context. With its use, our partner personas generator could generate unseen personas, which are also subject to potential offensive generation. An offensiveness check can be incorporated to alleviate this problem for actual usage (Baheti et al., 2021).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "B Implementation Details", "text": "For supervised phase, we set Adam (Kingma and Ba, 2015) as our optimizer, with hyperparameters \u03b7 = 5e\u22124, \u03b2 1 = 0.9, \u03b2 2 = 0.999, \u03f5 = 1e\u22128. The models are fine-tuned for 2 epochs. For RL phase, we set Adam as our optimizer, with \u03b7 = 5e\u22126, \u03b2 1 = 0.9, \u03b2 2 = 0.999, \u03f5 = 1e\u22128. We update the model parameters every 20 training instances and validate the model performance every 50 updates. DistilBERT  is used to initialize the model parameters for the critic network. We set Adam as our optimizer, with hyperparameters \u03b7 = 5e\u22126, \u03b2 1 = 0.9, \u03b2 2 = 0.999, \u03f5 = 1e\u22128. We fine-tune the critic for 1 epoch, and we freeze it empirically during RL. All the experiments are conducted based on the TRANSFORMERS library from HUGGINGFACE (Wolf et al., 2020).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "C Analysis on Dialogue Reponse Generation", "text": "We present the progressive change of the testing perplexity for DRG and PPG on PERSONACHAT-ORI in Figure 4. 4 We observe that they improve D Human Evaluation Criteria \u2022 (Appropriateness): \"Who is more appropriate given the previous dialogue context?\"\n\u2022 (Informativeness): \"Who is more diverse instead of null answers such as I do not know?\"\n\u2022 (Engagingness): \"Who would you prefer to talk with for a long conversation?\"\n\u2022 (Human-likeness): \"Which speaker do you think sounds more like a real person?\"\n\u2022 (Coherence): \"Which persona contains traits that are more coherent to each other?\"\n\u2022 (Interestingness): \"Which persona is more interesting and diverse?\"\nThe first four are from the existing work Zou et al., 2021) and we propose the last two for evaluating PPG. We report the first four for DRG, and we report the last four for PPG.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "E Dataset Limitations", "text": "Our work uses an off-the-shelf persona-based conversational dataset PERSONACHAT , which is collected and built by crowdsourcing to converse based on a fake set of discrete traits.\nThere is no personal information and hence no ethics concern, but this might result in limited usefulness as there could be discrepancies between the collected samples and real-life conversation. It is also more expensive to collect real data. However, PERSONACHAT has been widely used by the community as a standard dataset. Many well-known persona-based datasets suffer from the same problem (Urbanek et al., 2019) as widely known.\nAlthough Mazar\u00e9 et al. (2018) proposed a useful method to collect large-scale persona-based dialogue datasets by extracting persona from user comments with classifiers trained on revised personas from PERSONACHAT which can improve the model performance on PERSONACHAT. For legal reasons, they did not release this dataset at the time of writing. Similarly, Zheng et al. (2019) proposed a persona-based dialogue dataset with diversified traits, but it is not currently online readily available. Zhong et al. (2020) has followed the approach suggested by Mazar\u00e9 et al. (2018) to build an empathetic conversation dataset based on personas.  8: More generated personas. We highlight in pink for informativeness and in yellow for coherence. However, their main focus is to investigate the impact of personas on empathetic dialogue generation. Therefore, we choose to follow the community to investigate our method on the most well-known dataset, PERSONACHAT.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "F Computing Infrastructure", "text": "We run all our experiments on a single NVIDIA TI-TAN RTX with 24GB GPU memory. Fine-tuning the generators for 2 epochs as we have done on our preprocessed PERSONACHAT train split consumes about 3-4 hours. Fine-tuning our critic classifier for 1 epoch consumes about 1 hour. Our RL phase consumes about 15 hours to achieve the best validation loss before being early stopped. We report averaged results from 3 runs for our dialogue response generation and partner personas generation results reported in Table 1, Table 4 and Table 7. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "This research/paper was supported by the Center for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission's InnoHK scheme.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Just say no: Analyzing the stance of neural dialogue generation in offensive contexts", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Ashutosh Baheti; Maarten Sap; Alan Ritter; Mark Riedl"}, {"title": "Skeletonto-response: Dialogue generation guided by retrieval memory", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Deng Cai; Yan Wang; Wei Bi; Zhaopeng Tu; Xiaojiang Liu; Wai Lam; Shuming Shi"}, {"title": "Retrievalguided dialogue response generation via a matchingto-generation framework", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Deng Cai; Yan Wang; Wei Bi; Zhaopeng Tu; Xiaojiang Liu; Shuming Shi"}, {"title": "Unified conversational recommendation policy learning via graph-based reinforcement learning", "journal": "", "year": "2021", "authors": "Yang Deng; Yaliang Li; Fei Sun; Bolin Ding; Wai Lam"}, {"title": "Generating multiple diverse responses for short-text conversation", "journal": "", "year": "2019", "authors": "Jun Gao; Wei Bi; Xiaojiang Liu; Junhui Li; Shuming Shi"}, {"title": "Dually interactive matching network for personalized response selection in retrieval-based chatbots", "journal": "", "year": "2019", "authors": "Jia-Chen Gu; Zhen-Hua Ling; Xiaodan Zhu; Quan Liu"}, {"title": "Partner matters! an empirical study on fusing personas for personalized response selection in retrieval-based chatbots", "journal": "Association for Computing Machinery", "year": "2021", "authors": "Jia-Chen Gu; Hui Liu; Zhen-Hua Ling; Quan Liu; Zhigang Chen; Xiaodan Zhu"}, {"title": "2020. Will I sound like me? improving persona consistency in dialogues through pragmatic selfconsciousness", "journal": "", "year": "", "authors": "Hyunwoo Kim; Byeongchang Kim; Gunhee Kim"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Generating personalized dialogue via multitask meta-learning", "journal": "", "year": "2021", "authors": "Yang Jing;  Lee; Woon Seng Kong Aik Lee;  Gan"}, {"title": "Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning", "journal": "CoRR", "year": "2018", "authors": "Dianqi Li; Qiuyuan Huang; Xiaodong He; Lei Zhang; Ming-Ting Sun"}, {"title": "A diversity-promoting objective function for neural conversation models", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Jiwei Li; Michel Galley; Chris Brockett; Jianfeng Gao; Bill Dolan"}, {"title": "A persona-based neural conversation model", "journal": "Long Papers", "year": "2016", "authors": "Jiwei Li; Michel Galley; Chris Brockett; Georgios Spithourakis; Jianfeng Gao; Bill Dolan"}, {"title": "Deep reinforcement learning for dialogue generation", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Jiwei Li; Will Monroe; Alan Ritter; Dan Jurafsky; Michel Galley; Jianfeng Gao"}, {"title": "Weakly supervised user profile extraction from twitter", "journal": "", "year": "2014", "authors": "Jiwei Li; Alan Ritter; Eduard Hovy"}, {"title": "Don't say that! making inconsistent dialogue unlikely with unlikelihood training", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Margaret Li; Stephen Roller; Ilia Kulikov; Sean Welleck; Y-Lan Boureau; Kyunghyun Cho; Jason Weston"}, {"title": "ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons", "journal": "", "year": "2019", "authors": "Margaret Li; Jason Weston; Stephen Roller"}, {"title": "", "journal": "", "year": "", "authors": " Corr"}, {"title": "Seamlessly unifying attributes and items: Conversational recommendation for cold-start users", "journal": "ACM Trans. Inf. Syst", "year": "2021", "authors": "Shijun Li; Wenqiang Lei; Qingyun Wu; Xiangnan He; Peng Jiang; Tat-Seng Chua"}, {"title": "ROUGE: A package for automatic evaluation of summaries", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"title": "You impress me: Dialogue generation via mutual persona perception", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Qian Liu; Yihong Chen; Bei Chen; Jian-Guang Lou; Zixuan Chen; Bin Zhou; Dongmei Zhang"}, {"title": "Learning knowledge bases with parameters for task-oriented dialogue systems", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Andrea Madotto; Samuel Cahyawijaya; Yan Genta Indra Winata; Zihan Xu; Zhaojiang Liu; Pascale Lin;  Fung"}, {"title": "Personalizing dialogue agents via meta-learning", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Andrea Madotto; Zhaojiang Lin; Chien-Sheng Wu; Pascale Fung"}, {"title": "Like hiking? you probably enjoy nature: Personagrounded dialog with commonsense expansions", "journal": "", "year": "2020", "authors": "Harsh Bodhisattwa Prasad Majumder; Taylor Jhamtani; Julian Berg-Kirkpatrick;  Mcauley"}, {"title": "Training millions of personalized dialogue agents", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Pierre-Emmanuel Mazar\u00e9; Samuel Humeau; Martin Raison; Antoine Bordes"}, {"title": "ParlAI: A dialog research software platform", "journal": "", "year": "2017", "authors": "Alexander Miller; Will Feng; Dhruv Batra; Antoine Bordes; Adam Fisch; Jiasen Lu; Devi Parikh; Jason Weston"}, {"title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeff Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "journal": "", "year": "1994", "authors": "Stephen Robertson; Steve Walker"}, {"title": "Recipes for building an open-domain chatbot", "journal": "", "year": "2021", "authors": "Stephen Roller; Emily Dinan; Naman Goyal; Da Ju; Mary Williamson; Yinhan Liu; Jing Xu; Myle Ott; Eric Michael Smith; Y-Lan Boureau; Jason Weston"}, {"title": "RMM: A recursive mental model for dialogue navigation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Yonatan Homero Roman Roman; Jesse Bisk; Asli Thomason; Jianfeng Celikyilmaz;  Gao"}, {"title": "Hierarchical reinforcement learning for open-domain dialog", "journal": "", "year": "2020", "authors": "Abdelrhman Saleh; Natasha Jaques; Asma Ghandeharioun; Judy Shen; Rosalind Picard"}, {"title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "journal": "CoRR", "year": "2019", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"title": "Methods and metrics for cold-start recommendations", "journal": "Association for Computing Machinery", "year": "2002", "authors": "Andrew I Schein; Alexandrin Popescul; Lyle H Ungar; David M Pennock"}, {"title": "BoB: BERT over BERT for training persona-based dialogue models from limited personalized data", "journal": "Long Papers", "year": "2021", "authors": "Haoyu Song; Yan Wang; Kaiyan Zhang; Wei-Nan Zhang; Ting Liu"}, {"title": "2020. Generate, delete and rewrite: A three-stage framework for improving persona consistency of dialogue generation", "journal": "", "year": "", "authors": "Haoyu Song; Yan Wang; Wei-Nan Zhang; Xiaojiang Liu; Ting Liu"}, {"title": "Exploiting persona information for diverse generation of conversational responses", "journal": "", "year": "2019", "authors": "Haoyu Song; Wei-Nan Zhang; Yiming Cui; Dong Wang; Ting Liu"}, {"title": "Sequence to sequence learning with neural networks", "journal": "MIT Press", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; V Quoc;  Le"}, {"title": "Learning to Speak and Act in a Fantasy", "journal": "", "year": "2019", "authors": "Jack Urbanek; Angela Fan; Siddharth Karamcheti; Saachi Jain; Samuel Humeau; Emily Dinan; Tim Rockt\u00e4schel; Douwe Kiela; Arthur Szlam; Jason Weston"}, {"title": "Extracting and Inferring Personal Attributes from Dialogue", "journal": "CoRR", "year": "2021", "authors": "Zhilin Wang; Xuhui Zhou; Rik Koncel-Kedziorski; Alex Marin; Fei Xia"}, {"title": "Dialogue natural language inference", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Sean Welleck; Jason Weston; Arthur Szlam; Kyunghyun Cho"}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "journal": "Mach. Learn", "year": "1992", "authors": "Ronald J Williams"}, {"title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"title": "TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents. CoRR, abs", "journal": "", "year": "1901", "authors": "Thomas Wolf; Victor Sanh; Julien Chaumond; Clement Delangue"}, {"title": "Guiding variational response generator to exploit persona", "journal": "", "year": "2020", "authors": "Bowen Wu; Mengyuan Li; Zongsheng Wang; Yifu Chen; Derek F Wong; Qihang Feng; Junhong Huang; Baoxun Wang"}, {"title": "Getting to know you: User attribute extraction from dialogues", "journal": "", "year": "2020", "authors": "Chien-Sheng Wu; Andrea Madotto; Zhaojiang Lin; Peng Xu; Pascale Fung"}, {"title": "Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters", "journal": "CoRR", "year": "2021", "authors": "Yan Xu; Etsuko Ishii; Zihan Liu; Genta Indra Winata; Dan Su; Andrea Madotto; Pascale Fung"}, {"title": "Addressing cold start in recommender systems: A semi-supervised co-training algorithm", "journal": "Association for Computing Machinery", "year": "2014", "authors": "Mi Zhang; Jie Tang; Xuchen Zhang; Xiangyang Xue"}, {"title": "Personalizing dialogue agents: I have a dog, do you have pets too?", "journal": "Australia. Association for Computational Linguistics", "year": "2018", "authors": "Saizheng Zhang; Emily Dinan; Jack Urbanek; Arthur Szlam; Douwe Kiela; Jason Weston"}, {"title": "DIALOGPT : Large-scale generative pre-training for conversational response generation", "journal": "", "year": "2020", "authors": "Yizhe Zhang; Siqi Sun; Michel Galley; Yen-Chun Chen; Chris Brockett; Xiang Gao; Jianfeng Gao; Jingjing Liu; Bill Dolan"}, {"title": "A documentgrounded matching network for response selection in retrieval-based chatbots", "journal": "", "year": "2019", "authors": "Xueliang Zhao; Chongyang Tao; Wei Wu; Can Xu; Dongyan Zhao; Rui Yan"}, {"title": "Personalized Dialogue Generation with Diversified Traits", "journal": "CoRR", "year": "2019", "authors": "Yinhe Zheng; Guanyi Chen; Minlie Huang; Song Liu; Xuan Zhu"}, {"title": "Towards persona-based empathetic conversational models", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Peixiang Zhong; Chen Zhang; Hao Wang; Yong Liu; Chunyan Miao"}, {"title": "Learning to Predict Persona Information for Dialogue Personalization without Explicit Persona Description", "journal": "", "year": "2021", "authors": "Wangchunshu Zhou; Qifei Li; Chenle Li"}, {"title": "", "journal": "", "year": "", "authors": " Corr"}, {"title": "Thinking clearly, talking fast: Concept-guided non-autoregressive generation for open-domain dialogue systems", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Yicheng Zou; Zhihua Liu; Xingwu Hu; Qi Zhang"}, {"title": "Generated Partner Personas", "journal": "", "year": "", "authors": ""}, {"title": "I am a shy person, but I love to sing. Until recently, I ve never been able to sing in front of anyone. Anyways, I decided to give it a try and participaed in an audition for a talent show . My shyness made me panick and I didn t show up", "journal": "", "year": "", "authors": "A Personas"}, {"title": "Personas B: I play the violin. I am married with 5 kids. I am nurse. I met my husband when I was a freshman in college", "journal": "", "year": "", "authors": ""}, {"title": "Personas C: I am a soccer player. I am a goalie. My number is 42. Nike cleats are my favorite. I joined a new team last month", "journal": "", "year": "", "authors": ""}, {"title": "Personas D: I have two kids, ages 2 and 6. I am from sterling heights, michigan . My favorite movie is titanic . I work part time at aldis. My husband owns a small auto repair shop", "journal": "", "year": "", "authors": ""}, {"title": "Personas E: I am a retired computer programmer. I have one grandson and one daughter. I just turned 77. I love animals. I like watching british tv shows and movies", "journal": "", "year": "", "authors": ""}, {"title": "Personas F: I like to go hunting. I like to remodel homes. I like to shoot a bow. My favorite holiday is halloween . I like to go shopping with my daughters", "journal": "", "year": "", "authors": ""}, {"title": "I have a large cd collection. I collect stamps. Favorite band is the beetles . I like vintage furniture", "journal": "", "year": "", "authors": "G Personas"}, {"title": "Personas H: I like to drink wine. I enjoy reading history books. I am a teacher. I love to write stories while sitting in the grass in my back yard. I grew up in new hampshire", "journal": "", "year": "", "authors": ""}, {"title": "Personas I: I am retired. I stay active. I have eight grandchildren. I have good health", "journal": "", "year": "", "authors": ""}, {"title": "I m a student. I like to go out to eat. I like listening to other rap music too. One of my favorite artists is drake . A hobby of mine is the drums", "journal": "", "year": "", "authors": "J Personas"}, {"title": "Personas K: I have two children. I like to go on walks. I am from mexico. I used to be a chef, but I am a teacher now", "journal": "", "year": "", "authors": ""}, {"title": "Personas L: I like to do all my shopping at walmart . I m deathly terrified of heights. I prefer to live where the weather s cold. Winter s my favorite time of the year. I m really excited to see how game of thrones ends", "journal": "", "year": "", "authors": ""}, {"title": "Personas M: I like to cook. I am a foodie. I love to chat with my friends. I love kids and dogs. I like to go shopping with my daughters", "journal": "", "year": "", "authors": ""}, {"title": "We will be traveling to niagra falls for our honeymoon. We are getting married in a park", "journal": "", "year": "", "authors": "N Personas"}, {"title": "My favorite color is red. I have 2 dogs as pets. I leave the dogs home when I visit my parents. I love dogs. I work as a veterinarian s assistant", "journal": "", "year": "", "authors": "O Personas"}, {"title": "Personas P: I am a musician. I wish I could spend more time at home. I like to write my own songs. I have taken formal music lessons since i was 5", "journal": "", "year": "", "authors": ""}, {"title": "Personas Q: I love comics. I love reading. I ve started creating my own comics and presenting them to publishers. I decided to publish my creations on internet. I ve been rejected several times and thought of giving up with this", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "4. 22Baselines and Comparison Models E2E w/o Partner Personas This is an end-to-end (E2E) response generator without partner persona. E2E w/ Partner Personas in Training With partial ground truth partner personas for training only. E2E w/ Partner Personas in Training and Inference With ground truth partner personas for training and inference.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Analysis for the cold start problem with limited dialogue turns available. Note that all of these baselines are fine-tuned on DIALOGPT.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Change of the testing PPL during RL.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Automatic evaluation results on PERSONACHAT-ORI and PERSONACHAT-REV. Perplexity (PPL) attains a better quality with lower values and the remaining metrics attain a better quality with higher values.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Partner PersonasGround Truth Partner Personas I am an army ranger. I secretly love my long deployments, because it gets me away from conventional life. I have a wife and two kids back in the states. I would be honored to give my life for my country. I am not afraid to die. I am in the army. I am serving in South Korea. I was born in puerto rico. I am a violent person. I drink protein powder with nothing but water. I like to watch mma. My prized possession is a bowie knife. I life weights, but I never do squats.I am happy being single and alone. I only drink water. I go to the gym a days a week. I do not want children. I work in labor and delivery.I like to watch football. My friends like watching it too. Its great fun. We drink beer and eat food. I love watching football on Sunday. I have three dogs. My favroutie food is cheese piazza. I am a hair stylist.", "figure_data": "Generated"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Case studies to show that our generated personas are relevant, informative and coherent.", "figure_data": "ModelPPL\u2193BRD-1D-2Gold Our PPG 56.2 2.99 22.5 0.012 0.042 ---0.003 0.008Gold Our PPG-111-1.34 20.8 0.013 0.042 -0.004 0.009"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results for PPG. Upper for PERSONACHAT-", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Human evaluation results of dialogue response generation in winning percentages. \u2021 indicates the results as passing a two-tailed binomial significance test with p < 0.001.", "figure_data": "CriteriaGround Truth Our FrameworkCoherence4159  \u2021Interestingness3961  \u2021Engagingness4456  \u2020Human-likeness4753  \u2020"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Human evaluation results of PPG in winning percentages. \u2020 and \u2021 indicates the results as passing a two-tailed binomial significance test with p < 0.05 and p < 0.001 respectively.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": "presents the human evaluation resultson dialogue response generation. Our framework"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Ablation study for our proposed framework on PERSONACHAT-ORI.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": ""}], "doi": "10.18653/v1/N19-1124"}