{"authors": "Qingkai Zeng; Mengxia Yu; Wenhao Yu; Tianwen Jiang; Meng Jiang", "pub_date": "", "title": "Validating Label Consistency in NER Data Annotation", "abstract": "Data annotation plays a crucial role in ensuring your named entity recognition (NER) projects are trained with the correct information to learn from. Producing the most accurate labels is a challenge due to the complexity involved with annotation. Label inconsistency between multiple subsets of data annotation (e.g., training set and test set, or multiple training subsets) is an indicator of label mistakes. In this work, we present an empirical method to explore the relationship between label (in-)consistency and NER model performance. It can be used to validate the label consistency (or catch the inconsistency) in multiple sets of NER data annotation. In experiments, our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets (with 26.7% and 5.4% label mistakes). It validated the consistency in the corrected version of both datasets.", "sections": [{"heading": "Introduction", "text": "Named entity recognition (NER) is one of the foundations of many downstream tasks such as relation extraction, event detection, and knowledge graph construction. NER models require vast amounts of labeled data to learn and identify patterns that humans cannot continuously. It is really about getting accurate data to train the models. When end-to-end neural models achieve excellent performance on NER in various domains (Lample et al., 2016;Liu et al., 2018;Luan et al., 2018;Zeng et al., , 2021, building useful and challenging NER benchmarks, such as CoNLL03, WNUT16, and SCIERC, contributes significantly to the research community.\nData annotation plays a crucial role in building benchmarks and ensuring NLP models are trained with the correct information to learn from (Luan et al., 2018;. Producing the necessary annotation from any asset at scale is a challenge, mainly because of the complexity involved with annotation. Getting the most accurate labels demands time and expertise.\nLabel mistakes can hardly be avoided, especially when the labeling process splits the data into multiple sets for distributed annotation. The mistakes cause label inconsistency between subsets of annotated data (e.g., training set and test set or multiple training subsets). For example, in the CoNLL03 dataset (Sang and De Meulder, 2003), a standard NER benchmark that has been cited over 2,300 times, label mistakes were found in 5.38% of the test set (Wang et al., 2019). Note that the stateof-the-art results on CoNLL03 have achieved an F1 score of \u223c .93. So even if the label mistakes make up a tiny part, they cannot be negligible when researchers are trying to improve the results further. In the work of Wang et al., five annotators were recruited to correct the label mistakes. Compared to the original test set results, the corrected test set results are more accurate and stable.\nHowever, two critical issues were not resolved in this process: i) How to identify label inconsistency between the subsets of annotated data? ii) How to validate that the label consistency was recovered by the correction?\nAnother example is SCIERC (Luan et al., 2018) (cited \u223c50 times) which is a multi-task (including NER) benchmark in AI domain. It has 1,861 sentences for training, 455 for dev, and 551 for test. When we looked at the false predictions given by SCIIE which was a multi-task model released along with the SCIERC dataset, we found that as many as 147 (26.7% of the test set) sentences were not properly annotated. (We also recruited five annotators and counted a mistake when all the annotators report it.) Three examples are given in Table 1: two of them have wrong entity types; the third has a wrong span boundary. As shown in the experiments section, after the correction, the NER performance becomes more accurate and stable.\nTable 1: Three examples to compare original and corrected annotation in the test set of the SCIERC dataset. If the annotation on the test set consistently followed the \"codebook\" that was used to annotate training data, the entities in the first two examples would be labelled as \"Task\" (not \"Method\") for sure.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Original Examples", "text": "Corrected Examples Starting from a DP-based solution to the [traveling salesman problem]Method, we present a novel technique ...", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Starting from a DP-based solution to the [traveling salesman", "text": "problem]Task, we present a novel technique ... FERRET utilizes a novel approach to [Q/A]Method known as predictive questioning which attempts to identify ... FERRET utilizes a novel approach to [Q/A]Task known as predictive questioning which attempts to identify ... The goal of this work is the enrichment of [human-machine interactions]Task in a natural language environment. The goal of this work is the [enrichment of human-machine interactions]Task in a natural language environment. We sample three exclusive subsets (of size x) from the training set (orange, green, and blue). We use one subset as the new test set (orange). We apply the SCIIE NER model on the new test set. We build three new training sets: i) \"TrainTest\" (blue-red), ii) \"PureTrain\" (green-blue), iii) \"TestTrain\" (red-blue). Results on SCIERC show that the test set (red) is less predictive of training samples (orange) than the training set itself (blue or green). This was not observed on two other datasets.\nBesides the significant correction on the SCI-ERC dataset, our contributions in this work are as follows: i) an empirical, visual method to identify the label inconsistency between subsets of annotated data (see Figure 1), ii) a method to validate the label consistency of corrected data annotation (see Figure 2). Experiments show that they are effective on the CoNLL03 and SCIERC datasets.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Proposed Methods", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A method to identify label inconsistency", "text": "Suppose the labeling processes on two parts of annotated data were consistent. They are likely to be equivalently predictive of each other. In other words, if we train a model with a set of samples from either part A or part B to predict a different set from part A, the performance should be similar. Take SCIERC as an example. We were wondering whether the labels in the test set were consistent with those in the training set. Our method to identify the inconsistency is presented in Figure 1.\nWe sample three exclusive subsets (of size x) from the training set. We set x = 550 according to the size of the original test set. We use one of the subsets as the new test set. Then we train the SCIIE NER model (Luan et al., 2018) to perform on the new test set. We build three new training sets to feed into the model:\n\u2022 \"TrainTest\": first fed with one training subset and then the original test set; \u2022 \"PureTrain\": fed with two training subsets;\n\u2022 \"TestTrain\": first fed with the original test set and then one of the training subsets. Results show that \"TestTrain\" performed the worst at the early stage because the quality of the original test set is not reliable. In \"TrainTest\" the performance no longer improved when the model started being fed with the original test set. \"Pure-Train\" performed the best. All the observations conclude that the original test set is less predictive of training samples than the training set itself. It may be due to the issue of label inconsistency. Moreover, we do not have such observations on two other datasets, WikiGold and WNUT16.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "A method to validate label consistency after correction", "text": "After we corrected the label mistakes, how could we empirically validate the recovery of label consistency? Again, we use a subset of training data as the new test set. We evaluate the predictability of the original wrong test subset, the corrected test subset, and the rest of the training set. We expect to see that the wrong test subset delivers weaker performance and the other two sets make comparable good predictions. Figure 2 illustrates this idea. Take SCIERC as an example. Suppose we corrected z of y + z sentences in the test set. The original wrong test subset (\"Mistake\") and the corrected test subset (\"Correct\") are both of size z.\nHere z = 147 and the original good test subset y = 404 (\"Test\"). We sampled three exclusive subsets of size x, y, and w = 804 from the training set (\"Train\"). We use the first subset (of size x) as the new test set. We build four new training sets and feed into the SCIIE model. Each new training set has y + w + z = 1, 355 sentences.\n\u2022 \"TestTrainMistake\"/\"TestTrainCorrect\": the original good test subset, the third sampled training subset, and the original wrong test subset (or the corrected test subset); \u2022 \"PureTrainMistake\"/\"PureTrainCorrect\": the second and third sampled training subsets and the original wrong test subset (or the corrected test subset); \u2022 \"MistakeTestTrain\"/\"CorrectTestTrain\": the original wrong test subset (or the corrected test subset), the original good test subset, and the third sampled training subset; \u2022 \"MistakePureTrain\"/\"CorrectPureTrain\": the original wrong test subset (or the corrected test subset) and the second and third sampled training subsets.\nResults show that the label mistakes (i.e., original wrong test subset) hurt the model performance  whenever being fed at the beginning or later. The corrected test subset delivers comparable performance with the original good test subset and the training set. This demonstrates the label consistency of the corrected test set with the training set.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Experiments", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results on SCIERC", "text": "The visual results of the proposed methods have been presented in Section 2. Here we deploy five state-of-the-art NER models to investigate their performance on the corrected SCIERC dataset. The NER models are BiLSTM-CRF (Lample et al., 2016), LM-BiLSTM-CRF (Liu et al., 2018), singletask and multi-task SCIIE (Luan et al., 2018), and multi-task DyGIE (Luan et al., 2019).\nAs shown in Table 2, all NER models deliver better performance on the corrected SCIERC than the original dataset. So the training set is more consistent with the fixed test set than the original wrong test set. In future work, we will explore more baselines in the leaderboard.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Results on CoNLL03", "text": "Based on the correction contributed by (Wang et al., 2019), we use the proposed method to justify label inconsistency though the label mistakes take \"only\" 5.38%. It also validates the label consistency after recovery. Figure 3(a) shows that starting with the wrong labels in the original test set makes the performance worse than starting with the training set or the good test subset. After label correction, this issue is fixed in Figure 3(b).", "n_publication_ref": 1, "n_figure_ref": 2}, {"heading": "Related Work", "text": "NER is typically cast as a sequence labeling problem and solved by models integrate LSTMs, CRF, and language models (Lample et al., 2016;Liu et al., 2018;Zeng et al., 2019. Another idea is to generate span candidates and predict their type. Span-based models have been proposed with multitask learning strategies (Luan et al., 2018(Luan et al., , 2019. The multiple tasks include concept recognition, relation extraction, and co-reference resolution.\nResearchers notice label mistakes in many NLP tasks (Manning, 2011;Wang et al., 2019;Eskin, 2000;Kv\u0207to\u0148 and Oliva, 2002). For instance, it is reported that the bottleneck of the POS tagging task is the consistency of the annotation result (Manning, 2011). People tried to detect label mistakes automatically and minimize the influence of noise in training. The mistake re-weighting mechanism is effective in the NER task (Wang et al., 2019). We focus on visually evaluating the label consistency.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We presented an empirical method to explore the relationship between label consistency and NER model performance. It identified the label inconsistency of test data in SCIERC and CoNLL03 datasets (with 26.7% and 5.4% label mistakes). It validated the label consistency in multiple sets of NER data annotation on two benchmarks, CoNLL03 and SCIERC.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "This work is supported by National Science Foundation IIS-1849816 and CCF-1901059.   ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Detecting errors within a corpus using anomaly detection", "journal": "Association for Computational Linguistics", "year": "2000", "authors": " Eleazar Eskin"}, {"title": "Biomedical knowledge graphs construction from conditional statements", "journal": "IEEE/ACM transactions on computational biology and bioinformatics", "year": "2020", "authors": "Tianwen Jiang; Qingkai Zeng; Tong Zhao; Bing Qin; Ting Liu; V Nitesh; Meng Chawla;  Jiang"}, {"title": "(semi-)automatic detection of errors in PoS-tagged corpora", "journal": "", "year": "2002", "authors": "Pavel Kv\u0207to\u0148; Karel Oliva"}, {"title": "Neural architectures for named entity recognition", "journal": "", "year": "2016", "authors": "Guillaume Lample; Miguel Ballesteros; Sandeep Subramanian; Kazuya Kawakami; Chris Dyer"}, {"title": "Empower sequence labeling with task-aware neural language model", "journal": "", "year": "2018", "authors": "Liyuan Liu; Jingbo Shang; Xiang Ren; Frank Fangzheng Xu; Huan Gui; Jian Peng; Jiawei Han"}, {"title": "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction", "journal": "EMNLP", "year": "2018", "authors": "Yi Luan; Luheng He; Mari Ostendorf; Hannaneh Hajishirzi"}, {"title": "A general framework for information extraction using dynamic span graphs", "journal": "", "year": "2019", "authors": "Yi Luan; Dave Wadden; Luheng He; Amy Shah; Mari Ostendorf; Hannaneh Hajishirzi"}, {"title": "Part-of-speech tagging from 97% to 100%: is it time for some linguistics?", "journal": "", "year": "2011", "authors": "D Christopher;  Manning"}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "journal": "", "year": "2003", "authors": "F Erik; Fien Sang;  De Meulder"}, {"title": "Crossweigh: Training named entity tagger from imperfect annotations", "journal": "EMNLP-IJCNLP", "year": "2019", "authors": "Zihan Wang; Jingbo Shang; Liyuan Liu; Lihao Lu; Jiacheng Liu; Jiawei Han"}, {"title": "Identifying referential intention with heterogeneous contexts", "journal": "", "year": "2020", "authors": "Wenhao Yu; Mengxia Yu; Tong Zhao; Meng Jiang"}, {"title": "Enhancing taxonomy completion with concept generation via fusing relational representations", "journal": "", "year": "2021", "authors": "Qingkai Zeng; Jinfeng Lin; Wenhao Yu; Jane Cleland-Huang; Meng Jiang"}, {"title": "Faceted hierarchy: A new graph type to organize scientific concepts and a construction method", "journal": "", "year": "2019", "authors": "Qingkai Zeng; Mengxia Yu; Wenhao Yu; Jinjun Xiong; Yiyu Shi; Meng Jiang"}, {"title": "Tritrain: Automatic pre-fine tuning between pretraining and fine-tuning for sciner", "journal": "", "year": "2020", "authors": "Qingkai Zeng; Wenhao Yu; Mengxia Yu; Tianwen Jiang; Tim Weninger; Meng Jiang"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Identifying label inconsistency of test set with training set: We sample three exclusive subsets (of size x) from the training set (orange, green, and blue). We use one subset as the new test set (orange). We apply the SCIIE NER model on the new test set. We build three new training sets: i) \"TrainTest\" (blue-red), ii) \"PureTrain\" (green-blue), iii) \"TestTrain\" (red-blue). Results on SCIERC show that the test set (red) is less predictive of training samples (orange) than the training set itself (blue or green). This was not observed on two other datasets.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Validating label consistency in corrected test set: We corrected z of y + z sentences in the test set. We sampled three exclusive subsets of size x, y, and w from the training set. We use the first subset (of size x) as the new test set. We build four new training sets as shown in the figure and feed them into the SCIIE model (at the top of the figure). Results show that the label mistakes (red parts of the curves on the left) do hurt the performance no matter fed at the beginning or later; and the corrected test set performs as well as the training set (on the right).", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Identifying label inconsistency and validating the consistency in the original & corrected CoNLL03.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Five NER models perform consistently better on the corrected SCIERC than on the original dataset. 47.95 52.64 56.13 48.07 51.79 LM-BiLSTM-CRF 62.78 58.20 60.40 59.15 57.15 58.13 SCIIE-single 71.20 62.88 66.79 65.77 60.90 63.24 SCIIE-multi 72.66 63.22 67.61 67.66 61.72 64.56 DyGIE-multi 69.64 67.02 68.31 65.09 65.28 65.18", "figure_data": "MethodCorrected SCIERC Original SCIERC P R F1 P R F1BiLSTM-CRF58.35"}], "doi": "10.26615/978-954-452-056-4_002"}