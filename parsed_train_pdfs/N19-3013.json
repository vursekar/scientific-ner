{"authors": "Sarah Gupta; Anthony Dipadova", "pub_date": "", "title": "Deep Learning and Sociophonetics: Automatic Coding of Rhoticity Using Neural Networks", "abstract": "Automated extraction methods are widely available for vowels (Rosenfelder et al., 2014), but automated methods for coding rhoticity have lagged far behind. R-fulness versus rlessness (in words like park, store, etc.) is a classic and frequently cited variable (Labov, 1966), but it is still commonly coded by human analysts rather than automated methods. Human-coding requires extensive resources and lacks replicability, making it difficult to compare large datasets across research groups (Yaeger-Dror et al., 2008;Heselwood et al., 2008). Can reliable automated methods be developed to aid in coding rhoticity? In this study, we use Neural Networks/Deep Learning, training our model on 208 Boston-area speakers.", "sections": [{"heading": "Introduction", "text": "Despite advances in automation for phonetic alignment and extraction of vowel formants, there is still no reliable automated method for classifying r-dropping, that is, whether a given word is pronounced with an /r/ in words like park (pahk), start (staht), and so on. R-dropping, also known as non-rhotic speech, is an important sociolinguistic variable in modern dialect research. But unfortunately most researchers continue to depend on human judgments (Nagy and Irwin, 2010;Becker, 2009;Nagy and Roberts, 2004), which is an inconsistent and time-consuming method that lacks replicability. Turning to the field of machine learning, our deep learning approach investigates a new way to distinguish rhotic versus non-rhotic pronunciations in recorded data. This is the first study to use neural networks to classify rhotic versus non-rhotic speech.\nAlthough human-coding requires extensive resources and lacks consistency and replicability (Yaeger-Dror et al., 2008;Heselwood et al., 2008), making it difficult to compare large datasets across different research groups, it is the only method we have right now. How soon will computers be able to quickly and reliably code rhoticity up to this standard? In terms of other machine learning approaches, McLarty, Jones, and Hall work on this challenge using Support Vector Machines (SVMs) (Mclarty et al., 2018). The present study uses Neural Networks/Deep Learning, one of the most effective and fastest-growing approaches in machine-learning. To our knowledge, this is the first attempt to use neural networks for automatic coding of any sociophonetic variable.\nThis new method was developed using audio recordings from over 200 New England speakers from Boston, Maine, and central New Hampshire (Stanford, forthcoming), and is here compared to other work on rhoticity (Heselwood et al., 2008;Mclarty et al., 2018). In what ways can neural networks be effective tools in assisting the coding of rhoticity? To what level can they perform compared to traditional coding methods and other approaches?", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Background", "text": "The phoneme /r/ has been particularly difficult to pin down because it may be articulated in different ways, yet still produce the same acoustic signal. As most phoneticians have come to agree, F3 is one of the primary acoustic correlates of rhoticity (Espy-Wilson et al., 2000;Hagiwara, 1995;Thomas, 2011). The general consensus is that the F3 measurement for /r/ is lower than that of other non-rhotic vowels, but reliable standards for coding rhoticity are lacking.\nIn this paper, rhoticity will refer to post-vocalic realizations of the phoneme /r/ which do not occur before other vowels. For example, rhotic tokens of interest would include park and father but not marry. British phonetician John Wells used the term \"rhotic\", which has been subsequently considered in the field as one of the most defining traits of varieties of English (Wells, 1982).\nRhotic and non-rhotic dialects have been widely studied as they relate to sociolinguistic features of location, age, gender, and socioeconomic status. However, we are still reliant on human analysts to make judgements of rhotic vs. non-rhotic speech, which can require a lot of time and money. Despite advances in many areas of computational linguistics, there is still not an accurate way to determine rhoticity based on acoustic components alone; a human must judge for themselves whether or not an /r/ has been dropped. As expected, this is not highly replicable as different speakers may perceive things differently especially when it comes to dialects that are not so clear-cut (Yaeger-Dror et al., 2008). For this reason, an automated way to determine rhotic/non-rhotic tokens would be especially helpful in these contexts.\n3 Other work 3.1 Heselwood, Plug, and Tickle Heselwood et al. (2008) extracted formant data from the spectrograms on the Bark scale -usually, formant data F2/F3 is reported on the Hertz scale. The Bark scale more closely correlates to human perception of sounds, that is, on a logarithmic scale rather than absolute. After conversion, F2 was labeled Z2 and F3 was labeled Z3, and a series of perceptual experiments were performed to ascertain rhoticity thresholds. Note that it was conducted for the purposes of perceptual research rather than coding applications.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "McLarty, Jones, and Hall", "text": "Mclarty et al. ( 2018) trained a Support Vector Machine (SVM) on pre-vocalic /r/ and vowels, and their approach did quite well in classifying prevocalic /r/s. They then took this pre-trained model and applied it to classifying postvocalic /r/ tokens, which classified 84% as vowels, and 15% as /r/. As they describe, this is likely because all postvocalic segments still contain vowel-like properties; furthermore, their training set excluded postvocalic /r/ so the accuracy is expected to decrease.\nHowever, their method did not perform as well in comparison to humans. On tokens where there was no ground truth, humans only agreed with the SVM classification about 55% of the time.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Methods", "text": "In this initial study, we used Boston-area field recordings of 208 speakers, 100 tokens per speaker (107 women/101 men, born 1915-1997). These on-the-street interviews ( 15-20 minutes each) are typical sociolinguistic recordings in terms of speech styles (word-list, sentences, reading passage, free speech) and occasional background noise. We chose to omit free speech because its token variability between speakers would present another challenging factor, leaving us with recordings where participants were reading (word-list, sentences, passage). Given word transcriptions, we used the Montreal Forced Aligner (McAuliffe et al., 2017) and modified Praat scripts (DiCanio, 2014;Koops, 2013) to align and extract vowel+(r) sequences, e.g., park, short. However, note that because non-rhotic dialects are less common, and some of our recordings had background noise, it could be possible that alignments were not perfect for all of our tokens.\nTwo human analysts listened to recordings and judged each vowel+(r) token as r-ful or r-less. The human analysts agreed on 89.9% of the tokens, similar to human agreement elsewhere (Nagy and Irwin, 2010). Like other studies, we omitted tokens when the human analysts disagreed ( 10%). So overall, 1700 tokens were discarded because of speaker disagreement, and 6500 rhotic tokens and 5300 non-rhotic tokens remained for analysis.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Preliminary Investigations", "text": "In early testing, we attempted classification into rful, r-less, and unknown, but this did not provide strong results so we simplified to a binary classification. From the beginning of this project, we knew we wanted to use a machine learning approach, so before using neural networks we tried some easier classifiers. However, we did not get encouraging results. For example, our Random Forest Classifier only gave about 54% accuracy. When we tried simpler neural networks, these gave much more promising results to we chose to pursue this method.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Data Extraction and Model Specifications", "text": "Following standard methods of Automatic Speech Recognition, we converted the audio to 12 Mel-Frequency-Cepstral-Coefficients (MFCCs). We used the 12 MFCCs, similar to Mclarty et al.. For each vowel+(r) sequence, we normalized across the length to extract 100 time-points per token, as shown in figure 1. In the training, MFCCs were more effective than traditional sociophonetic /r/ correlates F2 and F3 (Thomas, 2011). These samples were used in the model architecture as shown in figure 1, where there are 100 samples for each vowel + /r/ sequence. The Gated Recurrent Unit is shown in more detail in figure 2, where we can see the input from the previous timestep and layer, and how this is filtered through gates using tanh and sigmoid activation functions. Importantly, no work on coding rhoticity has made use of Recurrent Neural Networks, and we believe our methods are a promising step. We used Gated Recurrent Units Chung et al., 2014) to train our system to classify vowel+(r) tokens as r-ful or r-less. Following standard methods in machine-learning, we split the data in order to train with 80% of the data and test with 20%.\nWe chose hyperparameters based on a grid search using 3-fold cross validation (only 3 due to the small dataset). We saved the test set to validate results. The hidden layer size was 50 nodes, and dense layer size was 200 nodes. For regularization we used a kernel L2 regularization for the dense layer and we used both activation L2 and Recurrent L2 for the GRU layer. All of the alphas for this regularization are 0.01. The optimization method was RMSprop, and the learning rate was 0.001.", "n_publication_ref": 2, "n_figure_ref": 2}, {"heading": "Results", "text": "In figure 3, we see the Normalized Confusion Matrix, which summarizes our results by lining up true labels and predicted labels for our rhotic and non-rhotic tokens. We consider this binary classification either rhotic (positive) or non-rhotic (negative). In this way we can see the proportion of true positives (predicted to be rhotic and indeed truly rhotic), false positive (predicted to be rhotic but actually non-rhotic), true negative (predicted to be non-rhotic and actually non-rhotic), and false negative (predicted to be non-rhotic and actually rhotic). In deciding which model to use, we tried a few different configurations. We used the sampled MFCCs (as described earlier, figure 1) as well as Bark measurements that were extracted also at 100 time-points across the vowel. Because our MFCC data is multi-dimensional and time-dependent, we wanted to see how a Convolutional Neural Network would perform (table 1), but it turned out not to be as high in performance as our earlier model.\nFigure 4 shows the Receiver Operating Characteristic (ROC) for our model (created using scikitlearn), which is fairly good by machine learning standards. The Area Under the Curve (AUC, as noted in Table 1) is 0.892, and as evident from the graph, is much closer to 1. Our system had 81.1% accuracy with the human analysts in judging tokens as r-less or r-ful, scoring 0.829 for F-measure.  We also used the Heselwood et al. approach (section 3.1) of classifying front or back vowels to see how accurately it would perform on the same test dataset. This classification gave an average speaker accuracy of 63.3% and an average token accuracy of 62.1% (Table 2), much lower than our best model's overall accuracy (i.e. average across all tokens) of 81.1% (Table 1).\nAverage Speaker Accuracy 63.3% Average Token Accuracy 62.1% ", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "Discussion", "text": "The initial results of this study are promising. Our results are quite strong, as shown by the metrics in Table 1. When testing the Heselwood et al. approach (Table 2), it only predicted correctly approximately 60% of the time; our model performs significantly better, at an accuracy of 81.1% (Table 1). It seems that we are also slightly better at predicting rhotic tokens than non-rhotic (Figure 3), which likely has to do with the fact that we have more rhotic tokens in total.\nWe aimed to reach human levels -considering that analyst agreement is 89.9% for our dataset (as mentioned above), our accuracy of 81.1% is quite good. However, these numbers are not strictly comparable as we discarded tokens that proved difficult for human analysts.\nIn future development of this method, we want to consider any sources of error on our part. For example, some audio and text files could be misaligned so we might consider hand-correcting these alignments. However, the nature of the neural network could correct for this in that it learns to forget irrelevant or noisy data. By gathering more data, we would expect that our accuracy would improve and eventually reach a plateau where additional speakers would not affect anything.\nAdditionally, a study that involves cross-corpus analysis could provide greater insight into how this model might be applicable on a larger scale, and how well our model actually performs. Furthermore, if we had 3 analysts rather than 2, we could have used a majority vote for classifying tokens, and would not have to discard tokens where rhoticity was ambiguous.\nA shortcoming of this study is that it only involves speech that is elicited through readingideally future studies would involve free speech in order to use more natural speech. R-dropping is a crucial sociolinguistic variable for English dialect research in the US Northeast, Great Britain, Australia, New Zealand, Singapore, and other locations. Our neural network model takes a significant step toward automation of this key variable. In the future, we will continue optimizing and improving our model. Other groups have studied automated methods for coding sociolinguistic variables (Yuan and Liberman, 2011;Bailey, 2016), and there are great ideas to be found in these works. When automated methods for rhoticity reach the accuracy level of humans, along with consistency and full replicability, this will open the floodgates to large amounts of /r/ data and greatly expand sociolinguistic knowledge of dialect variation around the world, efficiently allowing studies to be replicated across research groups.", "n_publication_ref": 2, "n_figure_ref": 1}], "references": [{"title": "Automatic Detection of Sociolinguistic Variation Using Forced Alignment. University of Pennsylvania Working Papers in Linguistics", "journal": "", "year": "2016", "authors": "George Bailey"}, {"title": "/r/ and the construction of place identity on New York City's Lower East Side", "journal": "Journal of Sociolinguistics", "year": "2009", "authors": "Kara Becker"}, {"title": "Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation", "journal": "EMNLP", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merrienboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"}, {"title": "Empirical evaluation of Gated Recurrent Neural Networks on sequence modeling", "journal": "", "year": "2014", "authors": "Junyoung Chung; Caglar Gulcehre; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Combine intervals", "journal": "", "year": "2014", "authors": "Christian Dicanio"}, {"title": "Acoustic modeling of American English /r/. The Journal of the", "journal": "Acoustical Society of America", "year": "2000", "authors": "Y Carol; Suzanne E Espy-Wilson; Michel Boyce; Shrikanth Jackson; Abeer Narayanan;  Alwan"}, {"title": "Acoustic Realizations of American /r/ as Produced by Women and Men", "journal": "", "year": "1995", "authors": "Robert Hagiwara"}, {"title": "Assessing rhoticity using auditory, acoustic and psychoacoustic methods", "journal": "", "year": "2008", "authors": "Barry Heselwood; Leendert Plug; Alison Tickle"}, {"title": "Praat script for extracting vowel formants", "journal": "", "year": "2013", "authors": "Chris Koops"}, {"title": "The Social Stratification of English", "journal": "", "year": "1966", "authors": "William Labov"}, {"title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi", "journal": "", "year": "2017", "authors": "Michael Mcauliffe; Michaela Socolof; Sarah Mihuc; Michael Wagner; Morgan Sonderegger"}, {"title": "Corpus-Based Sociophonetic Approaches to Postvocalic R-lessness in African American Language", "journal": "American Speech", "year": "2018", "authors": "Jason Mclarty; Taylor Jones; Christopher Hall"}, {"title": "Boston (r): Neighbo(r)s nea(r) and fa(r). Language Variation and Change", "journal": "", "year": "2010", "authors": "Naomi Nagy; Patricia Irwin"}, {"title": "", "journal": "New England: Phonology", "year": "2004", "authors": "Naomi Nagy; Julie Roberts"}, {"title": "FAVE (Forced Alignment and Vowel Extraction)", "journal": "", "year": "2014", "authors": "Ingrid Rosenfelder; Josef Fruehwald; Keelan Evanini; Scott Seyfarth; Kyle Gorman; Hilary Prichard; Jiahong Yuan"}, {"title": "New England English: Large-scale acoustic sociophonetics and dialectology", "journal": "Oxford University Press", "year": "", "authors": "N James;  Stanford"}, {"title": "Sociophonetics: An introduction", "journal": "Palgrave Macmillan", "year": "2011", "authors": "Erik Thomas"}, {"title": "Accents of English", "journal": "Cambridge University Press", "year": "1982", "authors": "J C Wells"}, {"title": "", "journal": "NWAV", "year": "2008", "authors": "Malcah Yaeger-Dror; Tyler Kendall; Paul Foulkes; Dominic Watt; Jillian Eddie ; Philip Harrison; Colleen Kavenagh"}, {"title": "Automatic detection of g-dropping in American English using forced alignment", "journal": "", "year": "2011", "authors": "Jiahong Yuan; Mark Liberman"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Model architecture.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Gated Recurrent Unit (GRU) architecture.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Normalized Confusion Matrix.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Receiver Operating Characteristic.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Metrics showing the performance of different models -our top performing model was using GRUs with MFCCs as input (as described previously).", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Heselwood et al. approach on test dataset (using Bark thresholds Z2 and Z3)", "figure_data": ""}], "doi": "10.1111/j.1467-9841.2009.00426.x"}