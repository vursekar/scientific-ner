{"authors": "Jitin Krishnan; Antonios Anastasopoulos; Hemant Purohit; Huzefa Rangwala", "pub_date": "", "title": "Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent Prediction and Slot Filling", "abstract": "Predicting user intent and detecting the corresponding slots from text are two key problems in Natural Language Understanding (NLU). Since annotated datasets are only available for a handful of languages, our work focuses particularly on a zero-shot scenario where the target language is unseen during training. In the context of zero-shot learning, this task is typically approached using representations from pre-trained multilingual language models such as mBERT or by fine-tuning on data automatically translated into the target language. We propose a novel method which augments monolingual source data using multilingual code-switching via random translations, to enhance generalizability of large multilingual language models when fine-tuning them for downstream tasks. Experiments on the Mul-tiATIS++ benchmark show that our method leads to an average improvement of +4.2% in accuracy for the intent task and +1.8% in F1 for the slot-filling task over the state-of-the-art across 8 typologically diverse languages. We also study the impact of code-switching into different families of languages on downstream performance. Furthermore, we present an application of our method for crisis informatics using a new human-annotated tweet dataset of slot filling in English and Haitian Creole, collected during the Haiti earthquake. 1   ", "sections": [{"heading": "Introduction", "text": "A cross-lingual setting is typically described as a scenario in which a model trained for a particular task in one source language (e.g. English) should be able to generalize well to a different target language (e.g. Japanese). While semi-supervised solutions (Muis et al., 2018;FitzGerald, 2020, inter alia) assume some target language data or translators are available, a zero-shot solution (Eriguchi et al., 2018;Srivastava et al., 2018; assumes none is available at training time. Having models that generalize well even to unseen languages is crucial for tackling real world problems such as extracting relevant information during a new disaster (Nguyen et al., 2017;Krishnan et al., 2020) or detecting hate speech (Pamungkas and Patti, 2019;Stappen et al., 2020), where the target language might be of low-resource or unknown.\nIntent prediction and slot filling are two NLU tasks, usually solved jointly, which learn to model the intent (sentence-level) and slot (word-level) labels. Such models are currently used extensively for goal-oriented dialogue systems, such as Amazon's Alexa, Apple's Siri, Google Assistant, and Microsoft's Cortana. Finding the 'intent' behind the user's query and identifying relevant 'slots' in the sentence to engage in a dialogue are essential for effective conversational assistance. For example, users might want to 'play music' given the slot labels 'year' and 'artist' (Coucke et al., 2018), or they may want to 'book a flight' given the 'airport' and 'locations' slot labels (Price, 1990). A strong correlation between the two tasks has made jointly trained models successful (Goo et al., 2018;Haihong et al., 2019;Hardalov et al., 2020;. In a cross-lingual setting, the model should be able to learn this joint task in one language and transfer knowledge to another (Upadhyay et al., 2018;Schuster et al., 2019;. This is the premise of our work.\nHighly effective transformer-based multilingual models such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020a) have found success across several multilingual tasks in recent years. In the zero-shot cross-lingual transfer setting with an unknown target language, a typical solution is to use pre-trained transformer models and fine-tune to the downstream task using the monolingual source data . However, Pires et al. (2019) showed that existing transformer-based represen-Figure 1: t-SNE plot of embeddings across the 12 multi-head attention layers of multilingual BERT. Parallelly translated sentences of MutiATIS++ dataset are still clustered according to the languages: English (black), Chinese (cyan), French (blue), German (green), and Japanese (red).\nFigure 2: An original example in English from MultiATIS++ dataset and its multilingually code-switched version. In the above code-switching example, the chunks are in Chinese, Punjabi, Spanish, English, Arabic, and Russian. 'atis_airfare' represents an intent class where the user seeks price of a ticket. tations may exhibit systematic deficiencies for certain language pairs. Figure 1 also verifies that the representations across the 12 multi-head attention layers of mBERT are still not shared across languages, instead forming clearly distinguishable clusters per language. This leads to a fundamental challenge that we address in this work: enhancing the language neutrality so that the fine-tuned model is generalizable across languages for the downstream task. To this goal, we introduce a data augmentation method via multilingual codeswitching, where the original sentence in English is code-switched into randomly selected languages. For example, chunk-level code-switching creates sentences with phrases in multiple languages as shown in Figure 2. We show that mBERT can be fine-tuned for many languages starting only with monolingual source-language data, leading to better performance in zero-shot settings.\nFurther, we show how code-switching with languages from different language families impacts the model's performance on individual target languages, even finding some counter-intuitive results. For instance, training on data code-switched between English and Sino-Tibetan languages is as helpful for Hindi (an Indo-Aryan Indo-European language) as code-switching with other Indo-Aryan languages, and Turkic languages can be helpful for both Chinese and Japanese.", "n_publication_ref": 18, "n_figure_ref": 4}, {"heading": "Contributions: a)", "text": "We present a data augmentation method via multilingual code-switching to enhance the language neutrality of transformerbased language models such as mBERT for finetuning to a downstream NLU task of intent prediction and slot filling. b) By studying different language families, we show how code-switching can be used to aid zero-shot cross-lingual learning for low-resource languages. c) We release a new human-annotated tweet dataset, collected during Haiti earthquake disaster, for intent prediction and slot filling in English and Haitian Creole.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Methodology", "text": "This section describes our problem definition, codeswitching algorithm, language families, and the training methodology.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Problem Definition", "text": "Given a source (S) and a set of target (T) languages, the goal is to train a classifier using data only in the source language and predict examples from the completely unseen target languages. We assume the target language is unknown during training (fine-tuning) time, which makes direct translation to target infeasible. In this context, we use code-switching (cs) to augment the monolingual source data. Thus, the input, augmented input, and output of our problem can be defined as: \n\u2190 \u2205 lset = googletrans.languages \u2212 lT for i \u2208 1.. k do for j \u2208 1.. len(X en ut ) do G cs \u2190 \u2205, L cs \u2190 \u2205 chunks = slot_chunks(X en ut [j], y en sl [j]) for c \u2208 chunks do l \u2190 random.choice(lset) t \u2190 translate(c, l) G cs \u2190 G cs \u222a t L cs \u2190 L cs \u222a align_label(c, t) end X cs ut \u2190 X cs ut \u222a G cs y cs \u2190 y cs \u222a y cs [j] y cs sl \u2190 y cs sl \u222a L cs end end Input: X S ut , y S , y S sl , l T Code-Switched Input: X cs ut , y cs , y cs sl Output: y T , y T sl \u2190 predict(X T ut )\nwhere X ut represents sentences, y their ground truth intent classes, y sl the slot labels for the words in those sentences, and l T the set of target languages. An example sentence, its intent class, and slot labels are shown in Figure 2.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Multilingual Code-Switching", "text": "Multilingual masked language models, such as mBERT (Devlin et al., 2019), are trained using large datasets of publicly available unlabeled corpora such as Wikipedia. Such corpora largely remain monolingual at the sentence level because the presence of intra-sentence code-switched data in written texts is likely scarce. The masked words that needed to be predicted usually are in the same language as their surrounding words. We study how code-switching can enhance the language neutrality of such language models by augmenting it with artificially code-switched data for fine-tuning it to a downstream task. Algorithm 1 explains this codeswitching process at the chunk-level. When using slot filling datasets, slot labels that are grouped by BIO (Ramshaw and Marcus, 1999) tags constitute natural chunks, as shown in Figure 2. To summarize the algorithm, we take a sentence, take each chunk from that sentence, perform a translation into a random language using Google's NMT system (Wu et al., 2016), and align the slot labels to fit the translation, i.e., label propagation through alignment as the translated sentence do not preserve the", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Group Name Languages", "text": "Afro-Asiatic Arabic (ar), Amharic (am), Hebrew (he), Somali (so) Germanic German (de), Dutch (nl), Danish (da), Swedish (sv), Norwegian (no) Indo-Aryan Hindi (hi), Bengali (bn), Marathi (mr), Nepali (ne), Gujarati (gu), Punjabi (pa) Romance Spanish (es), Portuguese (pt), French (fr), Italian (it), Romanian (ro) Sino-Tibetan, Koreanic, & Japonic Chinese (zh-cn), Japanese (ja), Korean (ko) Turkic Turkish (tr), Azerbaijani (az), Uyghur (ug), Kazakh (kk) number and order of words in the original sentence.\nAt the chunk-level, we use a direct alignment. The BIO-tagged labels are recreated for the translated phrase based on the word tokens. More complex methods could be applied here to improve the alignment of the slot labels such as fast-align (Dyer et al., 2013) or soft-align , but we leave this for future work. Code-Switching at the word-level essentially translates every word randomly, while at the sentence-level translates the entire sentence. During the experimental evaluation process, to build a language-neutral model using monolingual source (English) data, all eight target languages are excluded from the code-switching procedure to avoid unfair model comparisons, i.e. removing target languages (l T ) from lset in Algorithm 1.\nComplexity. The augmentation process is repeated k times per sentence producing a new augmented dataset of size k \u00d7 n, where n is the size of the original dataset, i.e. space complexity of O(k \u00d7 n). For T translations per sentence, Algorithm 1 has a runtime complexity of O(k \u00d7 n \u00d7 T ) assuming constant time for alignment. Word-level requires as many translations as the number of words but sentence-level requires only one. An increase in the dataset size also increases the training time, but the advantage is one model appropriate for many languages.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Language Families", "text": "A language family is defined as a group of related languages that likely share a common ancestor. For example, Portuguese, Spanish, French, Italian, and Romanian are all derived from Latin (Rowe and Levine, 2017). We use language families to study their impact on the target languages. We augment the source language with code-switching to a particular language family. For instance, codeswitching the English dataset with Turkic language family and testing on Japanese can reveal how closely the two are aligned in the vector space of a pre-trained multilingual model. We work with 6 language groups: Afro-Asiatic (Voegelin and Voegelin, 1976), Germanic (Harbert, 2006), Indo-Aryan (Masica, 1993), Romance (Elcock and Green, 1960), and Turkic (Johanson and Johanson, 2015), also grouping Sino-Tibetan, Koreanic and Japonic (Shafer, 1955;Miller, 1967). 2 Germanic, Romance, and Indo-Aryan are genera of the Indo-European family. Language groups and corresponding languages are shown in Table 1. Each group is selected based on a target language in the dataset, and the Afro-Asiatic family is added as an extra group. In experiments, lset in Algorithm 1 will be assigned languages from a specific family.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Joint Training", "text": "Joint training is traditionally used for intent prediction and slot filling to exploit the correlation between the two tasks. This is done by feeding the feature vectors of one model to another or by sharing layers of a neural network followed by training the tasks together. So, a standard joint model loss can be defined as a combination of intent (L i ) and slot (L sl ) losses. i.e., L = \u03b1L i + \u03b2L sl , where \u03b1 and \u03b2 are corresponding task weights. Prior works (Goo et al., 2018;Schuster et al., 2019;Liu and Lane, 2016;Haihong et al., 2019) that use BiL-STM or RNN are now modified to BERT-based implementations explored in more recent works Hardalov et al., 2020;. A standard Joint model consists of BERT outputs from the final hidden state (classification (CLS) token for intent and m word tokens for slots) fed to linear layers to get intent and slot predictions. Assuming h cls represents the CLS token and h m represents a token from the remaining word-level tokens, the BERT model outputs are defined as :\np i = sof tmax(W i h cls + b i ) p sl m = sof tmax(W sl hm + b sl ) \u2200m (1)\nwith a multi-class cross-entropy loss 3 for both intent (L i ) and slots (L sl ). We will use this model as 2 Each of the Sino-Tibetan, Koreanic, and Japonic families have a single high-resource member (Chinese, Korean, Japanese respectively). We only group them as an additional interesting data point, not because we ascribe to any theories that link them typologically.\n3\nL = \u2212 1 n \u2211\ufe01 n i=1 [y log \u0177]\nour baseline for joint training. Our goal will be to show that code-switching on top of joint training improves the performance. The output of Algorithm 1 will be the input used for joint training on BERT for code-switched experiments.\n3 Datasets Benchmark Dataset. We use the latest multilingual benchmark dataset of MultiATIS++ , which was created by manually translating the original ATIS (Price, 1990) dataset from English (en) to 8 other languages: Spanish (es), Portuguese (pt), German (de), French (fr), Chinese (zh), Japanese (ja), Hindi (hi), and Turkish (tr). The dataset consists of utterances for each language with an 'intent' label for 'flight intent' and 'slot' labels for the word tokens in BIO format. A sample datapoint in English is shown in Figure 2.  model on the validation set. Consistent with the metrics reported for intent prediction and slot filling evaluation in the past, we also accuracy for intent and micro F1 5 to measure slot performance.", "n_publication_ref": 6, "n_figure_ref": 1}, {"heading": "Baselines & Upper Bound", "text": "Since we assume that target language is not known before hand, Translate-Train (TT)  method is not a suitable baseline. Rather, we set this to be an upper bound, i.e. translating to the target language and fine-tuning the model should intuitively outperform a generic model. Additionally, we add code-switching to this TT model to assess if augmentation negatively impacts its performance. The zero-shot baselines for the codeswitching experiments use an English-Only ) model, which is fine-tuned over the pre-trained mBERT separately for each task and an English-only Joint model .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results & Discussion", "text": "Effect of Multilingual Code-Switching. Table 3 describes performance evaluation on the Multi-ATIS++ dataset. When compared to the state-ofthe-art jointly trained English-only baseline, we see a +4.2% boost in intent accuracy and +1.8% boost in slot F1 scores on average by augmenting the dataset via multilingual code-switching without requiring the target language. From the significance tests, except for Spanish and German, all other languages were helped by code-switching for intent detection. For slot filling, improvement on Portuguese and French is insignificant. This suggests that code-switching primarily helped languages that are morphologically more different from the source language (English). For example, Hindi and Turkish have the highest intent performance improvement of +16.1% and +9.8% respectively. And for slots, Hindi and Chinese with +6.0% and +4.3% respectively. Japanese showed +4% improvement for intent and +3.4% for slots.\nThe runtime of the models in Table 5 (Appendix B) shows that code-switching is expensive, taking up to five hours for five augmentation rounds (k = 5). This is because there are k times more data compared to the monolingual source data. Increasing the number of code-switchings (k) for a sentence from 5 to 50 improves the performance by +1%, while increasing the run-time by a large margin. Hence, such tradeoffs should be considered when picking k for real-world applications where time to deployment might be of the essense.\nIn the translate-train (upper bound) scenario, it is not immediately clear if augmentation helps, since data in the same language as the target are always preferable to other language or code-switched data. At a minimum, augmentation does not hinder upper-bound performance (Table 3).\nFor both intent and slot performance, the chunklevel model remained robust across the languages. For intent, the difference between word-level and sentence-level was insignificant. For slot, sentencelevel was in par with chunk-level on average. Thus,   we think that code-switching at chunk-level is safer for avoiding semantic discrepancies (which are a danger in the word-level) while also better encouraging intra-sentence language neutrality.\nEvaluation on Disaster Dataset. We found that disaster data is more challenging than the ATIS dataset for transfer learning in NLU. The predictive performance is shown in Table 4. Code-Switching improved intent accuracy by +12.5% and slot F1 by +2.3%, which is quite promising considering the domain mismatch (tweets vs airline guides).\nJoint training added +0.9% improvement to intent accuracy, however did not seem to help slot F1. This might imply a weaker correlation between the two tasks in real-world data, i.e. a mention of 'food' or 'shelter' in a tweet may not always mean that there is a 'request' or vice-versa. The upper bound of translate-train method did not perform any better than the randomly code-switched model which seemed counter-intuitive. This might be due to the lack of strong representation for Haitian Creole in the pre-trained model, although it is similar to French, or due to the limitation of the machine translation system.\nImpact of Language Families. Results of language family analysis are shown in Figure 3 for the 4 languages that showed significant improvements for both intent and slots in Table 3. The input in English is independently code-switched using 6 different language families. Note that the target language is always excluded from the group when evaluating on the same, i.e. Hindi is excluded from Indo-Aryan family when that family is being evaluated on it. Translate-train model is provided as a frame of reference and upper bound. Generally, as expected, we found that language families helped their corresponding languages, i.e. Romance helped Spanish, Germanic helped German, and so on. An exception is our loose group of Sino-Tibetan, Koreanic, and Japonic languages -for both Chinese and Japanese, languages from the Turkic language family helped more than others. On the other hand, the Sino-tibetan, Japonic, and Koreanic group helps Hindi more than other Indo-European languages. We believe this highlights the necessity  for methods like the one of Xia et al. (2020) that can a priori identify the best helper language or group of languages that can benefit downstream tasks for low resource languages.\nControl Experiments on k. Hyperparameter k controls the amount of code-switched data. k = 0 represents original size with no code-switching, k = 1 represents original size with code-switching, and k = 10 means 10-times more code-switched data than the original. The main experiments in Table 3 use k = 5. Figure 4 shows how varying k affects performance. For this analysis, we consider 4 target languages on which code-switching produced significant results in Table 3 on both Intent Accuracy and Slot F1: Chinese, Japanese, Hindi, and Turkish. Intuitively, we observe that as k increases, too much code-switching becomes expensive in terms of runtime, while performance improvement slowly plateaus. For Slot F1 performance in all four cases, unlike Intent, we observe an interesting dip when k = 1, which represents the augmentation having just one copy of codeswitching (without the original non-code-switched data), as compared to k = 0. Adding the original data to one round of code-switched data (k = 2) leads to big improvements. Overall, we see improvement for both tasks, with Slot F1 plateauing earlier. Table 5 and Figure 10 in Appendix B show the impact of code-switching on training runtime, which increases as k increases. Thus, finding an optimal value of k and specific language groups are essential for downstream applications.\nmBERT versus XLM-R. Additional performance evaluations and benefits of code-switching on XLM-R (Conneau et al., 2020a), a stronger multilingual language model, are provided in Appendix A. Note that XLM-R is trained using Common-Crawl and is likely to be exposed to some code-switched data. Thus, we focus primarily on mBERT which largely remains monolingual at the sentence-level to identify the unbiased impact of code-switching during fine-tuning. Furthermore, runtime and hyperparameter tuning along with insights into layers to freeze before training are shown in Appendix B.\nError Analysis. Selecting intent classes with support > 10, Figure 5 shows how each class is positively or negatively impacted by code-switching. Improvement was primarily on 'airfare', 'distance' 'capacity', 'airline', and 'ground_service' which had longer sentences such as 'Please tell me which airline has the most departures from Atlanta' when compared to 'abbreviations' and 'airport' classes that included very short phrases like 'What does EA mean?' However, note that Spanish and German did not improve much, aligning with our results in Table 3. For slot labels in Figure 6, we selected the ones with support > 50 and that have different characteristics, e.g. 'name', 'code', etc. The overall trend in slot performance shows improvements for labels such as 'day_name', 'airport_code', and 'city_name' and slight variations in labels such as 'fight_number' and 'period_of_day', implying textual slots benefiting over numeric ones.", "n_publication_ref": 2, "n_figure_ref": 5}, {"heading": "Related Work", "text": "Cross-Lingual Transfer. Researchers have studied cross-lingual tasks in various settings such as sentiment/sequence classification (Wan, 2009;Eriguchi et al., 2018;, named entity recognition (Zirikly and Hagiwara, 2015; Tsai  et Xie et al., 2018), parts-of-speech tagging (Yarowsky et al., 2001;T\u00e4ckstr\u00f6m et al., 2013;Plank and Agi\u0107, 2018), and natural language understanding (He et al., 2013;Upadhyay et al., 2018;. The methodology for most of the current approaches for cross-lingual tasks can be categorizes as: a) multilingual representations from pre-trained or fine-tuned models such as mBERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020a), b) machine translation followed by alignment (Shah et al., 2010;Yarowsky et al., 2001;Ni et al., 2017), or c) a combination of both . Before transformer models, effective approaches included domain adversarial training to extract language-agnostic features (Ganin et al., 2016; and word alignment methods such as MUSE (Conneau et al., 2017) to align fastText word vectors (Bojanowski et al., 2017). Recently, Conneau et al., 2020b show that having shared parameters in the top layers of the multilingual encoders can be used to align different languages quite effectively on tasks such as XNLI (Conneau et al., 2018). Monolingual models for joint slot filling and intent prediction have used attention-based RNN (Liu and Lane, 2016) and attention-based BiLSTM with a slot gate (Goo et al., 2018) on benchmark datasets (Price, 1990;Coucke et al., 2018). These methods have shown that a joint method can enhance both tasks and slot filling can be conditioned on the learned intent. A related approach iteratively learns the relationship between the two tasks (Haihong et al., 2019) . Recently, BERT-based approaches (Hardalov et al., 2020; have improved results. On the other hand, cross-lingual versions of this joint task include a low-supervision based approach for Hindi and Turkish (Upadhyay et al., 2018), new datasets for Spanish and Thai (Schuster et al., 2019), and recently  creating MultiATIS++, a comprehensive dataset in 9 languages. The joint task mentioned above in a pure zero-shot setting is one of the motivations for our work. A Zero-shot is the setting where the model sees a new distribution of examples only during test (prediction) time (Xian et al., 2017;Srivastava et al., 2018;Romera-Paredes and Torr, 2015). Thus, in our setting, we assume that target language is unknown during training, so that our model is generalizable across multiple languages.\nCode-Switching. Linguistic code-switching is a phenomenon where multilingual speakers alternate between languages. Recently, monolingual models have been adapted to code-switched text in entity recognition (Aguilar and Solorio, 2019), part-ofspeech tagging (Soto and Hirschberg, 2018;Ball and Garrette, 2018), sentiment analysis (Joshi et al., 2016) and language identification (Mave et al., 2018;Yirmibe\u015foglu and Eryigit, 2018;Mager et al., 2019). Recently, KhudaBukhsh et al., 2020 have proposed an approach to sample code-mixed documents using minimal supervision. Qin et al., 2020 allows randomized code-switching to include the target language, as shown in their Figure 3. In our context for example, if the target language is German, we ensure that there is no code-switching to German during training. We consider this distinction essential to evaluate a true zero-shot learning scenario and prevent any bias when comparing with translate-and-train.  present a non-zero-shot approach that performs code-switching to target languages, and Jiang et al. (2020) present a code-switching based method to improve the ability of multilingual language mod-els for factual knowledge retrieval. Contemporary work by Tan and Joty, 2021 makes use of both word and phrase-level code-mixing to switch to a set of languages to perform adversarial training for XNLI. Code-switching and other data augmentation techniques have been applied to the pre-training stage in recent works (Chaudhary et al., 2020;Kale and Siddhant, 2021;Dufter and Sch\u00fctze, 2020). However, pre-training is outside the scope of this work. In addition to studying cross-lingual slot filling and language families, another key distinction of our method is that we completely ignore the target language during training to represent a fully zero-shot scenario. The main advantage is that with enhanced cross-lingual generalizability, it can be deployed out-of-the-box, as our training is conducted independently of the target language.", "n_publication_ref": 41, "n_figure_ref": 1}, {"heading": "Conclusion & Future Work", "text": "Our study shows that augmenting the monolingual input data with multilingual code-switching via random translations at the chunk-level helps a zeroshot model to be language neutral when evaluated on unseen languages. This approach enhanced the generalizability of pre-trained language models such as mBERT when fine-tuning for downstream tasks of intent detection and slot filling. Additionally, we presented an application of this method using a new annotated dataset of disaster tweets. Further, we studied code-switching with language families and their impact on specific target languages. Addressing code-switching with language families during the pre-training phase and releasing a larger dataset of annotated disaster tweets in more languages are planned for future work.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ethical Considerations", "text": "The tweet dataset that we constructed for disaster NLU was originally released by Appen 6 , and we use it to construct slot labels in two languages: English (en) and Haitian Creole (ht). Data statement that includes annotator guidelines for the labeling jobs and other dataset information will be provided with the implementation. From a broader impact perspective, our code and developed models are open-source and allows NLP technology to be accessible to information systems for emergency services and social scientists in quickly deploying model during disaster events.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "This work was partially supported by U.S. National Science Foundation grants IIS-1815459, IIS-1657379, and 2040926. This work was also supported in part by the grant H-4Q21-009 from the Commonwealth Cyber Initiative, an investment in the advancement of cyber R&D, innovation, and workforce development (for more information about CCI, visit www.cyberinitiative. org). The authors are thankful to Ming Sun and Alexis Conneau for giving valuable insights on multilingual model training, as well as to the anonymous reviewers for their constructive feedback. We also acknowledge ARGO, a research computing cluster provided by the Office of Research Computing at George Mason University, were most experiments were conducted.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A mBERT versus XLM-R", "text": "We conduct an additional analysis on XLM-R (Conneau et al., 2020a) and compare it with mBERT (Devlin et al., 2019). The implementation is very similar in PyTorch (Paszke et al., 2019) but using the pre-trained xlm-roberta-base with RobertaForSequenceClassification (Wolf et al., 2020) as the XLM-R model. We observe that, setting k = 5, XLM-R outperforms mBERT on average (by 2% Intent Accuracy and 1.5% Slot F1). Individually, XLM-R improved Chinese, Japanese, Portuguese, and Turkish for Intent Prediction and German, Chinese, Japanese, Portuguese, and Hindi for Slot Filling as shown in Figure 7. We observe a trend similar to mBERT with k on XLM-R shown in Figure 8. However, for XLM-R, we observe that randomized code-switching did not help Chinese for Intent Prediction and Hindi for Slot F1. If codeswitched to a specific language family, instead of switching to random languages, it might improve their performance. A deeper dive into XLM-R and language families are left for future work. ", "n_publication_ref": 4, "n_figure_ref": 2}, {"heading": "B Hyperparameter Tuning & Runtime", "text": "For joint training with same task weights, we tuned \u03b1 and \u03b2 using grid search to see the strength of correlation between the tasks. For intent, the (\u03b1, \u03b2) combination of (1.0, 0.6) performed well, while (1.0, 1.0) for slots. This suggests that intent benefiting slot might be slightly more than slot benefiting intent. Additionally, during fine-tuning, freezing the layers of the transformer affected the model performance as shown in Figure 9. Keeping the first 8 layers frozen gave the best performance. By freezing the earlier layers, the transformer can retain its most fundamental feature information gained from the massive pre-training step, and by unfreezing some top layers, it can undergo fine-tuning. Additionally, latency for training a code-switched model is shown in Table 5 and how runtime varies with an increase in k is shown in Figure 10.  ", "n_publication_ref": 0, "n_figure_ref": 2}], "references": [{"title": "From english to code-switching: Transfer learning with strong morphological clues", "journal": "", "year": "2019", "authors": "Gustavo Aguilar; Thamar Solorio"}, {"title": "Part-of-speech tagging for code-switched, transliterated texts without explicit language identification", "journal": "", "year": "2018", "authors": "Kelsey Ball; Dan Garrette"}, {"title": "Enriching word vectors with subword information", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Piotr Bojanowski; Edouard Grave; Armand Joulin; Tomas Mikolov"}, {"title": "Dict-mlm: Improved multilingual pre-training using bilingual dictionaries", "journal": "", "year": "2020", "authors": "Aditi Chaudhary; Karthik Raman; Krishna Srinivasan; Jiecao Chen"}, {"title": "Bert for joint intent classification and slot filling", "journal": "", "year": "2019", "authors": "Qian Chen; Zhu Zhuo; Wen Wang"}, {"title": "Adversarial deep averaging networks for cross-lingual sentiment classification", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Xilun Chen; Yu Sun; Ben Athiwaratkun; Claire Cardie; Kilian Weinberger"}, {"title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Word translation without parallel data", "journal": "", "year": "2017", "authors": "Alexis Conneau; Guillaume Lample; Marc'aurelio Ranzato; Ludovic Denoyer; Herv\u00e9 J\u00e9gou"}, {"title": "Xnli: Evaluating crosslingual sentence representations", "journal": "", "year": "2018", "authors": "Alexis Conneau; Guillaume Lample; Ruty Rinott; Adina Williams; R Samuel; Holger Bowman; Veselin Schwenk;  Stoyanov"}, {"title": "Emerging cross-lingual structure in pretrained language models", "journal": "", "year": "2020", "authors": "Alexis Conneau; Shijie Wu; Haoran Li; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces", "journal": "", "year": "2018", "authors": "Alice Coucke; Alaa Saade; Adrien Ball; Th\u00e9odore Bluche; Alexandre Caulier; David Leroy; Cl\u00e9ment Doumouro; Thibault Gisselbrecht; Francesco Caltagirone; Thibaut Lavril"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Identifying elements essential for bert's multilinguality", "journal": "", "year": "2020", "authors": "Philipp Dufter; Hinrich Sch\u00fctze"}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "journal": "", "year": "2013", "authors": "Chris Dyer; Victor Chahuneau; Noah A Smith"}, {"title": "The romance languages", "journal": "Faber & Faber London", "year": "1960", "authors": "Denis William; John N Elcock;  Green"}, {"title": "Zeroshot cross-lingual classification using multilingual neural machine translation", "journal": "", "year": "2018", "authors": "Akiko Eriguchi; Melvin Johnson; Orhan Firat; Hideto Kazawa; Wolfgang Macherey"}, {"title": "Stil-simultaneous slot filling, translation, intent classification, and language identification: Initial results using mbart on multi-atis++", "journal": "", "year": "2020", "authors": "G M Jack;  Fitzgerald"}, {"title": "Domain-adversarial training of neural networks", "journal": "The Journal of Machine Learning Research", "year": "2016", "authors": "Yaroslav Ganin; Evgeniya Ustinova; Hana Ajakan; Pascal Germain; Hugo Larochelle; Fran\u00e7ois Laviolette; Mario Marchand; Victor Lempitsky"}, {"title": "Slot-gated modeling for joint slot filling and intent prediction", "journal": "", "year": "2018", "authors": "Guang Chih-Wen Goo; Yun-Kai Gao; Chih-Li Hsu; Tsung-Chieh Huo; Keng-Wei Chen; Yun-Nung Hsu;  Chen"}, {"title": "A novel bi-directional interrelated model for joint intent detection and slot filling", "journal": "", "year": "2019", "authors": "E Haihong; Peiqing Niu; Zhongfu Chen; Meina Song"}, {"title": "The Germanic Languages", "journal": "Cambridge University Press", "year": "2006", "authors": "Wayne Harbert"}, {"title": "Enriched pre-trained transformers for joint slot filling and intent detection", "journal": "", "year": "2020", "authors": "Momchil Hardalov; Ivan Koychev; Preslav Nakov"}, {"title": "Multi-style adaptive training for robust cross-lingual spoken language understanding", "journal": "IEEE", "year": "2013", "authors": "Xiaodong He; Li Deng; Dilek Hakkani-Tur; Gokhan Tur"}, {"title": "Multilingual factual knowledge retrieval from pretrained language models", "journal": "", "year": "2020", "authors": "Zhengbao Jiang; Antonios Anastasopoulos; Jun Araki; Haibo Ding; Graham Neubig"}, {"title": "The Turkic Languages. Routledge", "journal": "", "year": "2015", "authors": "Lars Johanson; \u00c9va \u00c1gnes Csat\u00f3 Johanson"}, {"title": "Towards sub-word level compositions for sentiment analysis of hindi-english code mixed text", "journal": "", "year": "2016", "authors": "Aditya Joshi; Ameya Prabhu; Manish Shrivastava; Vasudeva Varma"}, {"title": "Mixout: A simple yet effective data augmentation scheme for slotfilling", "journal": "Springer", "year": "2021", "authors": "Mihir Kale; Aditya Siddhant"}, {"title": "Harnessing code switching to transcend the linguistic barrier", "journal": "", "year": "2020", "authors": "Shriphani Ashiqur R Khudabukhsh; Jaime G Palakodety;  Carbonell"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Unsupervised and interpretable domain adaptation to rapidly filter social web data for emergency services", "journal": "", "year": "2020", "authors": "Jitin Krishnan; Hemant Purohit; Huzefa Rangwala"}, {"title": "Attention-based recurrent neural network models for joint intent detection and slot filling", "journal": "", "year": "2016", "authors": "Bing Liu; Ian Lane"}, {"title": "Subword-level language identification for intra-word code-switching", "journal": "", "year": "2019", "authors": "Manuel Mager; \u00d6zlem \u00c7etinoglu; Katharina Kann"}, {"title": "The indo-aryan languages", "journal": "Cambridge University Press", "year": "1993", "authors": "P Colin;  Masica"}, {"title": "Language identification and analysis of codeswitched social media text", "journal": "", "year": "2018", "authors": "Deepthi Mave; Suraj Maharjan; Thamar Solorio"}, {"title": "The Japanese Language", "journal": "University of Chicago Press Chicago", "year": "1967", "authors": "Roy Andrew Miller"}, {"title": "Low-resource cross-lingual event type detection via distant supervision with minimal effort", "journal": "", "year": "2018", "authors": "Aldrian Obaja Muis; Naoki Otani; Nidhi Vyas; Ruochen Xu; Yiming Yang; Teruko Mitamura; Eduard Hovy"}, {"title": "Robust classification of crisis-related data on social networks using convolutional neural networks", "journal": "ICWSM", "year": "2017", "authors": "Kamla Dat Tien Nguyen;  Al-Mannai; R Shafiq; Hassan Joty; Muhammad Sajjad; Prasenjit Imran;  Mitra"}, {"title": "Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection", "journal": "Long Papers", "year": "2017", "authors": "Jian Ni; Georgiana Dinu; Radu Florian"}, {"title": "Cross-domain and cross-lingual abusive language detection: A hybrid approach with deep learning and a multilingual lexicon", "journal": "", "year": "2019", "authors": "Wahyu Endang; Viviana Pamungkas;  Patti"}, {"title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga; Alban Desmaison; Andreas Kopf; Edward Yang; Zachary Devito; Martin Raison; Alykhan Tejani; Sasank Chilamkurthy; Benoit Steiner; Lu Fang; Junjie Bai; Soumith Chintala"}, {"title": "How multilingual is multilingual bert?", "journal": "", "year": "2019", "authors": "Telmo Pires; Eva Schlinger; Dan Garrette"}, {"title": "Distant supervision from disparate sources for low-resource partof-speech tagging", "journal": "", "year": "2018", "authors": "Barbara Plank; \u017deljko Agi\u0107"}, {"title": "Evaluation of spoken language systems: The atis domain", "journal": "", "year": "1990-06-24", "authors": "Patti Price"}, {"title": "Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp", "journal": "", "year": "2020", "authors": "Libo Qin; Minheng Ni; Yue Zhang; Wanxiang Che"}, {"title": "Text chunking using transformation-based learning", "journal": "Springer", "year": "1999", "authors": "A Lance; Mitchell P Ramshaw;  Marcus"}, {"title": "An embarrassingly simple approach to zero-shot learning", "journal": "", "year": "2015", "authors": "Bernardino Romera; - Paredes; Philip Torr"}, {"title": "A concise introduction to linguistics. Routledge", "journal": "", "year": "2017", "authors": "Bruce Rowe; Diane Levine"}, {"title": "Cross-lingual transfer learning for multilingual task oriented dialog", "journal": "", "year": "2019", "authors": "Sebastian Schuster; Sonal Gupta; Rushin Shah; Mike Lewis"}, {"title": "Classification of the sino-tibetan languages", "journal": "", "year": "1955", "authors": "Robert Shafer"}, {"title": "Synergy: a named entity recognition system for resource-scarce languages such as swahili using online machine translation", "journal": "", "year": "2010", "authors": "Rushin Shah; Bo Lin; Anatole Gershman; Robert Frederking"}, {"title": "Joint part-ofspeech and language id tagging for code-switched data", "journal": "", "year": "2018", "authors": "Victor Soto; Julia Hirschberg"}, {"title": "Zero-shot learning of classifiers from natural language quantification", "journal": "Long Papers", "year": "2018", "authors": "Shashank Srivastava; Igor Labutov; Tom Mitchell"}, {"title": "Cross-lingual zero-and few-shot hate speech detection utilising frozen transformer language models and axel", "journal": "", "year": "2020", "authors": "Lukas Stappen; Fabian Brunn; Bj\u00f6rn Schuller"}, {"title": "Token and type constraints for cross-lingual part-of-speech tagging", "journal": "Transactions of the Association for Computational Linguistics", "year": "2013", "authors": "Oscar T\u00e4ckstr\u00f6m; Dipanjan Das; Slav Petrov; Ryan Mc-Donald; Joakim Nivre"}, {"title": "Code-mixing on sesame street: Dawn of the adversarial polyglots", "journal": "", "year": "2021", "authors": "Samson Tan; Shafiq Joty"}, {"title": "Cross-lingual named entity recognition via wikification", "journal": "", "year": "2016", "authors": "Chen-Tse Tsai; Stephen Mayhew; Dan Roth"}, {"title": "2018. (almost) zero-shot cross-lingual spoken language understanding", "journal": "IEEE", "year": "", "authors": "Shyam Upadhyay; Manaal Faruqui; Gokhan T\u00fcr; Hakkani-T\u00fcr Dilek; Larry Heck"}, {"title": "Classification and index of the world's languages", "journal": "", "year": "1976", "authors": "Charles Frederick Voegelin; Florence Marie Voegelin"}, {"title": "Co-training for cross-lingual sentiment classification", "journal": "", "year": "2009", "authors": "Xiaojun Wan"}, {"title": "Transformers: State-of-theart natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Julien Chaumond; Lysandre Debut; Victor Sanh; Clement Delangue; Anthony Moi; Pierric Cistac; Morgan Funtowicz; Joe Davison; Sam Shleifer"}, {"title": "Google's neural machine translation system", "journal": "", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; V Quoc; Mohammad Le; Wolfgang Norouzi; Maxim Macherey; Yuan Krikun; Qin Cao; Klaus Gao;  Macherey"}, {"title": "Predicting performance for natural language processing tasks", "journal": "", "year": "2020", "authors": "Mengzhou Xia; Antonios Anastasopoulos; Ruochen Xu; Yiming Yang; Graham Neubig"}, {"title": "Zero-shot learning-the good, the bad and the ugly", "journal": "", "year": "2017", "authors": "Yongqin Xian; Bernt Schiele; Zeynep Akata"}, {"title": "Neural crosslingual named entity recognition with minimal resources", "journal": "", "year": "2018", "authors": "Jiateng Xie; Zhilin Yang; Graham Neubig; A Noah; Jaime G Smith;  Carbonell"}, {"title": "End-to-end slot alignment and recognition for crosslingual nlu", "journal": "", "year": "2020", "authors": "Weijia Xu; Batool Haider; Saab Mansour"}, {"title": "Alternating language modeling for cross-lingual pre-training", "journal": "", "year": "2020", "authors": "Jian Yang; Shuming Ma; Dongdong Zhang; Shuangzhi Wu; Zhoujun Li; Ming Zhou"}, {"title": "Inducing multilingual text analysis tools via robust projection across aligned corpora", "journal": "", "year": "2001", "authors": "David Yarowsky; Grace Ngai; Richard Wicentowski"}, {"title": "Detecting code-switching between turkish-english language pair", "journal": "", "year": "2018", "authors": "Zeynep Yirmibe\u015foglu; G\u00fcl\u015fen Eryigit"}, {"title": "Multilingual seq2seq training with similarity loss for cross-lingual document classification", "journal": "", "year": "2018", "authors": "Katherine Yu; Haoran Li; Barlas Oguz"}, {"title": "Crosslingual transfer of named entity recognizers without parallel corpora", "journal": "Short Papers", "year": "2015", "authors": "Ayah Zirikly; Masato Hagiwara"}], "figures": [{"figure_label": "3", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 3 :3Figure 3: Impact of different language groups on the target languages.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: Performance as k (augmentation rounds) increases (on mBERT).", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: Impact of code-switching on intent classes.", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 6 :6Figure 6: Impact of code-switching on slot labels.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Input: X en ut , y en , y en sl , lT Output: X cs ut , y cs , y cs sl X cs ut \u2190 \u2205, y cs \u2190 \u2205, y cs sl", "figure_data": "Algorithm 1: Data Augmentation via Mul-tilingual Code-Switching (Chunk-Level)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Selected language families to evaluate their impact on a target language.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "presents the dataset statistics for the benchmark dataset of MultiATIS++ as well as for the newly constructed dataset for disaster NLU.", "figure_data": "4 Experimental SetupWe use the traditional cross-lingual task settingwhere each experiment consists of a source lan-guage and a target language. A model is trainedon the source data (English) and evaluated on thetarget data (8 other languages). For code-switchingexperiments, the English dataset is augmented withmultilingual code-switching before training. Ourimplementation is in PyTorch (Paszke et al., 2019)and we use the pre-trained bert-base-multilingual-uncased with BertForSequenceClassification (Wolfet al."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "95.48 94.51 84.43 \u2660 76.48 \u2660 94.15 \u2660 94.89 \u2660 85.37 \u2660 78.04 \u2660 87.92", "figure_data": "Intent Acc.mesdezhjaptfrhitrAVGEnglish-Only Baseline*194.42 94.2979.5373.7592.9093.8667.0669.7183.19Jointen\u2212only Baseline*195.03 94.5180.5473.5793.4893.3373.5371.0584.38Word-level CS  \u2020194.18 93.9281.6775.4892.5494.1881.1974.2285.92Sentence-level CS194.60 93.5381.2175.0193.1093.2482.3775.1186.02Chunk-level CS (CCS)195.12 95.2783.8874.2794.2093.4882.7377.5187.06Jointen\u2212only* + CCS1Upper BoundTranslate-Train (TT)*894.02 93.8490.2184.1995.6694.5485.0885.7990.42JointT T *894.16 94.2491.5685.9895.7595.0186.4584.9591.01JointT T * + CCS895.48 95.4191.6087.1795.3494.6087.9485.9391.68Slot F1mesdezhjaptfrhitrAVGEnglish-Only Baseline*196.16 96.7383.1278.8195.6395.4077.0588.0988.87Jointen\u2212only Baseline*196.12 96.7684.9579.6095.7695.7677.6388.9289.44Word-level CS  \u2020195.81 96.3385.4679.3396.2795.0879.1086.8689.28Sentence-level CS196.57 96.9286.3279.5296.6595.8481.9489.8490.45Chunk-level CS (CCS)196.68 96.8287.1080.0096.4696.3180.9591.6090.51Jointen\u2212only* + CCS196.09 96.56 88.61 \u2660 82.28 \u266096.0195.9482.28 \u2660 90.45 \u2660 91.03Upper BoundTranslate-Train (TT)*896.89 96.0493.4885.2996.3596.0282.0391.2192.16JointT T *896.92 95.6693.6487.8496.1195.9582.9891.1592.53JointT T * + CCS896.98 96.2793.3785.8795.8895.4482.0091.3192.14"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": "Intent Acc.htEnglish-Only Baseline* 56.12Translate-Train (TT)62.58Chunk-level CS (CCS)63.15Jointen\u2212only* + CCS63.73Slot F1htEnglish-Only Baseline* 68.72Translate-Train (TT)69.96Chunk-level CS (CCS)70.27Jointen\u2212only* + CCS70.02"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ""}], "doi": "10.18653/v1/2020.acl-main.536"}