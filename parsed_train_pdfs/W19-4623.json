{"authors": "Pavel P\u0159ib\u00e1\u0148; Stephen Taylor", "pub_date": "", "title": "ZCU-NLP at MADAR 2019: Recognizing Arabic Dialects", "abstract": "In this paper, we present our systems for the MADAR Shared Task: Arabic Fine-Grained Dialect Identification. The shared task consists of two subtasks. The goal of Subtask-1 (S-1) is to detect an Arabic city dialect in a given text and the goal of Subtask-2 (S-2) is to predict the country of origin of a Twitter user by using tweets posted by the user. In S-1, our proposed systems are based on language modelling. We use language models to extract features that are later used as an input for other machine learning algorithms. We also experiment with recurrent neural networks (RNN), but these experiments showed that simpler machine learning algorithms are more successful. Our system achieves 0.658 macro F 1 -score and our rank is 6 th out of 19 teams in S-1 and 7 th in S-2 with 0.475 macro F 1 -score.", "sections": [{"heading": "Introduction", "text": "The Madar shared tasks (Bouamor et al., 2019) are a follow-up to Salameh's  work with the synthetic corpus of Bouamor (Bouamor et al., 2014) and Salameh's work with tweets based on the corpus. Two corpora are provided, a six-city corpus of travel sentences rendered into the dialects of five cities and MSA 1 , and a 25-city + MSA corpus using a smaller number of sentences. In the first task, test data is classified as one of the 25 cities or MSA. For the second task, the organizers chose training, development and test tweet-sets for download from Twitter. The tweets are from 21 Arabic countries, and the goal is to determine, for each tweet author, the country of origin.\nFor S-1 we did not use any external data, only data provided by the shared task organizers. 1 ", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Modern Standard Arabic", "text": "The organizers provided training and development data 2 consisting of sentences in different dialects with a label denoting the corresponding dialect.\nThe training data contain 41K sentences and development data contain 5.2K sentences. Organizers also provided additional data with Arabic sentences in seven dialects.\nS-2 uses a corpus of tweets. Twitter does not permit the organizers to distribute tweets, only the user ids and tweet ids. Every participant must arrange with Twitter to download the tweets themselves, and because tweets are subject to deletion over time, it is possible that each participant's version of the corpus and test is unique.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Related Work", "text": "The Arabic dialects have a common written form and unified literary tradition, so it seems most logical to distinguish dialects on the basis of acoustics, and there is a fair amount of work there, including Hanani et al. (2013Hanani et al. ( , 2015; .\n-- Biadsy et al. (2009) distinguish four Arabic dialects and MSA based on (audio) phone sequences; the phones were obtained by phone recognizers for English, German, Japanese, Hindi, Mandarin, Spanish, and three different MSA phone-recognizer implementations. The dialects were distinguished by phoneme sequences, and the results of classifications based on each phonerecognizer were combined using a logistic regression classifier. They train on 150 hours per dialect of telephone recordings. They report 61% accuracy on 5-second segments, and 84% accuracy on 120 second segments. Zaidan and Callison-Burch (2011) describe building a text corpus, based on reader commen-tary on newspaper websites, with significant dialect content; the goal is to provide a corpus to improve machine translation for Arabic dialects. They used Amazon Mechanical Turk to provide annotation for a portion of the corpus. Zaidan and Callison-Burch (2014) describe the same work in greater detail, including dialect classifiers they built using the Mechanical Turk data for classes and origin metadata as additional features. They say these classifiers are 'approaching human quality. ' ElFardy and Diab (2013) classify EGY 3 and MSA sentences from the Zaidan and Callison-Burch (2011) corpus, that is, from text. Not only is this a binary task, but orthographic hints, including repeated long vowels, emojis and multiple punctuations, give strong clues of the register, and hence whether MSA is being employed. They do a number of experiments comparing various preprocessing schemes and different training sizes, ranging from 2-28 million tokens. They achieve 80% -86% accuracy for all of their attempts. Malmasi et al. (2015) do Arabic dialect identification from text corpora, including the Multi-Dialect Parallel Corpus of Arabic (Bouamor et al., 2014) and the Arabic Online Commentary database (Zaidan and Callison-Burch, 2011). Hanani et al. (2015) perform recognition of several Palestinian regional accents, evaluating four different acoustic models, achieving 81.5% accuracy for their best system, an I-vector framework with 64 Gaussian components.  developed the corpus on which the DSL Arabic shared task is based. Their own dialect detection efforts depended largely on acoustical cues.\nArabic dialect recognition appeared in the 2016 edition of the VarDial workshop's shared task (Malmasi et al., 2016). The shared task data was text-only.\nThe best classifiers (Malmasi et al., 2016;Ionescu and Popescu, 2016) for the shared task performed far below the best results reported by some of the preceding researchers, in particular  which used some of the same data.\nPart of the reason must be that the amount of training data for the workshop is much smaller than that used by some of the other researchers; the workshop data also did not include the audio recordings on which the transcripts are based. 3 ", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "Egyptian dialect", "text": "The absence of audio was remedied for the 2017 and 2018 VarDial workshops, (Zampieri et al., 2017(Zampieri et al., , 2018 However, the five dialects plus MSA targeted by the VarDial shared task comprise a small fraction of Arabic's dialectical variation. Salameh et al. ) use a corpus  which differentiates between twentyfive different cities and MSA. This still doesn't address urban rural divides, but it begins to reflect more realistic diversity.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Overview", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Language Modelling", "text": "In S-1, both of our systems used for the official submission take as an input language model features. In our case the objective of a language model in its simplest form is to predict probability p(S) of sentence S which is composed from strings (words or character n-grams) s 1 , s 2 . . . s N , where N is a number of strings in the sentence. The probability estimation of p(S) can be computed as a product of conditional probabilities p(s i |h i ) of its strings s 1 , s 2 . . . s N , where h i is a history of a string s i . The probability of string s i is conditioned by history h i i.e. n \u2212 1 preceding strings s i\u2212n+1 , s i\u2212n+2 , . . . s i\u22121 which can be rewritten as s i\u22121 i\u2212n+1 . The resulting formula for the p(S) estimation looks as follows:\np(S) = N i=1 p(s i |h i ) = N i=1 p(s i |s i\u22121 i\u2212n+1 ) (1)\nThe conditioned probability p(s i |h i ) can be estimated with Maximum Likelihood Estimate (MLE) which is defined as:\np M LE (s i |h i ) = c(s i\u2212n+1 , s i\u2212n+2 . . . s i ) c(s i\u2212n+1 , s i\u2212n+2 . . . s i\u22121 ) (2)\nwhere c(s i\u2212n+1 , s i\u2212n+2 . . . s i ) is a number of occurrences of string s i with history h i and c(s i\u2212n+1 , s i\u2212n+2 . . . s i\u22121 ) is a number of occurrences of history h i . These counts are taken from a training corpus.\nWe followed Salameh ) in using the kenlm language modelling tool (Heafield et al., 2013). kenlm doesn't have an option to use character n-grams instead of words, so in order to get character-based language models, we prepared input files with characters separated by spaces. Instead of encoding space as a special word, we surrounded words with a <w></w>pair. This enables noticing strings which occur at the beginning or end of a word (as would a special sequence for space) but reduces the possible amount of inter-word information which the language model can keep for a given order, the parameter which indicates to kenlm the largest n-gram to index. We used order 5 for all our kenlm language models. We prebuilt models for each dialect. We prepared six directories, each containing word or character models for each dialect in one of the three corpora.\nWe wrote a LangModel class which quacks like a sklearn classifier, that is, it supports fit(), predict(), and predict proba(), but its choices are based on a directory of language models. predict() returns the dialect name whose model gives the highest score. predict proba() provides a list of languagemodel-score features, adjusted to probabilities.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Subtask-1 System Description", "text": "In this section we describe our models 4 . We submitted results for the S-1 from two systems -Tortuous Classifier and Neural Network Classifier.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Tortuous Classifier", "text": "This submission uses a jumble of features and classifiers, most from the sklearn module (Buitinck et al., 2013). The final classifier is a hard voting classifier with three input streams:\n1. Soft voting classifier on: on language-model-scores for character and language models on the corpus-6 language models and character language models for the corpus-26 language models.\n2. Support vector machine, svm.SVC( gamma='scale', kernel = 'poly', degree = 2) with the same features as item 1e.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Multinomial naive Bayes classifier using", "text": "word and char language model features for corpus-6 and corpus-26 features, tfid vectorized word 1-2grams, and tfid vectorized char 3-5grams.\nThe classifier did better on the development data, suggesting that it is over-fitted, but the language model features, which are the most predictive, also did better on the development data.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Neural Network Classifier", "text": "We experimented with several neural networks.\nOur model for the S-1 submission uses as input 26 features which correspond to one of our 26 pretrained dialect language models. Each feature represents the probability of a given sentence for one language model. The probability scores measure how close each sentence is to the dialect. We train Multilayer Perceptron (MLP) with one hidden (dense) layer with 400 units. The output of the hidden layer is passed to a final fullyconnected softmax layer. The output of the softmax layer is a probability distribution over all 26 classes. The class with the highest probability is predicted as a final output of our model. As an activation function in the hidden layer of the MLP a Rectified Linear Unit (ReLu) is employed.\nWe also tried to combine character n-gram features with the language model features. The input is a sequence of first 200 character n-grams of a given text. Each sequence of character n-grams is used as a separate input followed by a randomly initialized embedding layer and then two layers of Bidirectional LSTM (BiLSTM) (Graves and Schmidhuber, 2005) with 64 units are employed (see Figure 1).\nThe output vector of the BiLSTM layers is concatenated with the language model features and this concatenated vector is passed to the MLP layer with 400 units (the same as described above). All models were implemented by using Keras (Chollet et al., 2015) with TensorFlow backend (Abadi et al., 2015) ", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Neural Netwok Model Training", "text": "We tune all hyperparameters on the development data. We train our model with Adam (Kingma and Ba, 2014) optimizer with learning rate 0.01 and without any dropout. The number of epochs is 800 and we do not use mini-batches or dropout regularization technique. The model with these hyperparameters achieves the best result (0.661 macro F 1 -score) on the development data and was used for the final submission.\nWe also experimented with the n-gram inputs. We tried a different number of character n-grams and we achieve the best result (0.555 macro F 1score) on the development data using three inputs -character unigrams, bigrams and trigrams, with learning rate 0.005, mini-batches of size 256 for 11 epochs and with the Adam optimizer.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Subtask-2 System Description", "text": "Our tortuous classifier did less well on the tweet data, so we used a simpler classifier.\nThe features are the kenlm language model scores for the 21 countries, computed for each of the training tweets, then exponentiated and normalized to sum to 1. The tweets are classified using y_test = KNeighborsClassifier (n_neighbors=31) .fit (X_train,y_train) .predict(X_test)\nThe users are predicted based on the plurality prediction for all of their tweets, that is, the country to which the largest number of their tweets were assigned. There were a significant number of tweets unavailable, about 10% in the training and development sets, and 12% in the test set. After the submissions had closed we experimented with eliminating the unavailable and non-Arabic tweets from True labels 127 0 0 12 3 4 1 7 0 0 30 1 0 0 8 0 1 0 0 0 0 3 1 0 1 1 0 160 1 1 0 1 0 0 8 0 0 1 7 1 1 4 1 4 2 2 2 1 0 1 1 1 0 3 149 4 16 1 0 0 1 10 1 1 1 2 1 8 0 0 0 0 0 0 2 0 0 0 9 0 2 118 2 1 0 6 2 8 9 7 0 2 18 3 0 0 2 0 2 7 1 0 1 0 0 0 21 4 134 1 1 0 0 19 1 4 1 3 4 2 0 0 0 0 3 1 0 0 1 0 2 0 0 5 2 127 25 1 3 1 1 3 0 1 0 4 3 3 1 0 9 2 5 2 0 0 1 1 1 2 1 29 128 1 2 0 1 6 0 1 0 0 8 2 3 0 5 2 3 2 1 0 8 1 0 7 2 1 1 129 2 3 18 0 0 1 9 3 0 1 2 1 3 5 0 0 2 1 2 5 0 6 9 3 1 1 131 2 1 8 0 4 2 0 0 0 2 0 6 1 4 1 11 0 6 4 19 3 37 2 0 0 2 100 2 1 1 5 1 9 0 0 0 0 3 0 1 0 2 2 16 0 1 16 4 4 2 17 2 3 108 4 1 2 10 0 0 0 0 0 0 8 1 0 1 0 1 1 0 4 2 2 3 1 1 1 0 150 1 8 4 5 0 1 3 1 4 2 1 1 2 1 0 7 1 1 2 0 0 0 0 2 1 4 136 3 0 2 0 1 2 31 1 1 1 3 0 1 1 1 1 6 4 0 0 2 5 3 3 14 1 123 1 6 0 1 3 0 10 5 5 1 3 1 7 2 1 26 2 1 2 7 5 1 1 6 0 2 121 2 0 0 1 0 2 9 1 0 1 0  0 2 3 8 10 1 1 1 0 1 1 3 0 4 2 147 1 12 0 0 0 0 1 0 2 0  1 0 0 0 0 3 8 0 1 0 0 0 0 1 0 0 167 1 1 0 6 4 5 0 1 1  1 7 2 2 0 1 0 0 2 1 1 0 0 0 1 9 1 158 4 0 7 1 1 0 1 0  2 3 1 3 1 3 1 1 5 1 3 15 1 8 1 10 0 32 83 1 16 4 4 0 1 0  1 7 0 3 0 0 0 0 2 0 1 1 36 1 2 1 0 0 0 139 0 1 0 3 1 1  2 0 0 3 0 2 2 3 6 5 0 13 1 18 1 1 2 9 7 0 115 2 7 0 1 0  3 3 1 12 3 2 2 5 3 2 7 10 0 5 12 2 1 3 2 0 7 109 4 1 1 0  1 1 1 2 1 6 2 1 3 1 1 7 0 9 1 3 4 3 1 0 6 4 140 0 2 0  2 6 0 1 1 2 3 0 3 1 2 0 2 0 2 0 0 0 2 1 3 1 0 146 0 22  1 3 0 2 3 0 0 2 9 2 0 4 0 2 2 4 0 0 1 1 4 1 1 4 151 3  1  training and testing and choosing Saudi Arabia (which is the origin for the plurality of tweets at 36%) for users with no remaining tweets. This improved tweet classification accuracy by about 5%, but actually decreased user classification accuracy on the development set.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Results", "text": "For the Subtask-1 we achieved 0.658 macro F 1score on the test data, sixth among nineteen submissions with the Tortuous Classifier. The Neural Network Classifier achieved a macro F 1 -score of 0.648 on the test data. For the Subtask-2 we submitted a single entry. It ranked 7 th among 9 submissions with 0.475 macro F 1 -score.\nFigure 2 shows that many of the errors are geographically plausible. For example, ASWan ALXandria and CAIro are all in Egypt, and each has a sizeable chunk of mistaken identity for the others. Similarly, DAMascus, ALEppo, AMMan, BEIrut, JERusalem which are all 'Levantine' and only a few hundred miles apart.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "This paper presents an automatic approach for Arabic dialect detection in the MADAR Shared Task. Our proposed systems for the Subtask-1 use language model features. Our experiments showed that simpler machine learning algorithms outperform RNN using language model features. Subtask-2 turned out to be more challenging because Tweets, which are real-world wild data, are more difficult to process than systematically prepared texts.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "This work has been partly supported by Grant No. SGS-2019-018 Processing of heterogeneous data and its specialized applications, and was partly supported from ERDF \"Research and Development of Intelligent Components of Advanced Technologies for the Pilsen Metropolitan Area (InteCom)\". Access to computing and storage facilities owned by parties and projects contributing to the National Grid Infrastructure MetaCentrum provided under the programme \"Projects of Large Research, Development, and Innovations Infrastructures\" (CESNET LM2015042), is greatly appreciated.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "", "journal": "Oriol Vinyals", "year": "", "authors": "Mart\u00edn Abadi; Ashish Agarwal; Paul Barham; Eugene Brevdo; Zhifeng Chen; Craig Citro; Greg S Corrado; Andy Davis; Jeffrey Dean; Matthieu Devin; Sanjay Ghemawat; Ian Goodfellow; Andrew Harp; Geoffrey Irving; Michael Isard; Yangqing Jia; Rafal Jozefowicz; Lukasz Kaiser; Manjunath Kudlur; Josh Levenberg"}, {"title": "Automatic dialect detection in Arabic broadcast speech", "journal": "", "year": "2016", "authors": "Ahmed Ali; Najim Dehak; Patrick Cardinal; Sameer Khurana; Sree Harsha Yella; James Glass; Peter Bell; Steve Renals"}, {"title": "Spoken arabic dialect identification using phonotactic modeling", "journal": "", "year": "2009", "authors": "Fadi Biadsy; Julia Hirschberg; Nizar Habash"}, {"title": "A multidialectal parallel corpus of Arabic", "journal": "", "year": "2014", "authors": "Houda Bouamor; Nizar Habash; Kemal Oflazer"}, {"title": "The madar arabic dialect corpus and lexicon", "journal": "", "year": "2018", "authors": "Houda Bouamor; Nizar Habash; Mohammad Salameh; Wajdi Zaghouani; Owen Rambow; Dana Abdulrahim; Ossama Obeid; Salam Khalifa; Fadhl Eryani; Alexander Erdmann; Kemal Oflazer"}, {"title": "The MADAR Shared Task on Arabic Fine-Grained Dialect Identification", "journal": "", "year": "2019", "authors": "Houda Bouamor; Sabit Hassan; Nizar Habash"}, {"title": "API design for machine learning software: experiences from the scikit-learn project", "journal": "", "year": "2013", "authors": "Lars Buitinck; Gilles Louppe; Mathieu Blondel; Fabian Pedregosa; Andreas Mueller; Olivier Grisel; Vlad Niculae; Peter Prettenhofer; Alexandre Gramfort; Jaques Grobler; Robert Layton; Jake Vanderplas; Arnaud Joly; Brian Holt; Ga\u00ebl Varoquaux"}, {"title": "", "journal": "", "year": "2015", "authors": "Fran\u00e7ois Chollet"}, {"title": "Sentence level dialect identification in Arabic", "journal": "", "year": "2013", "authors": "Heba Elfardy; Mona Diab"}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "journal": "", "year": "2005", "authors": "Alex Graves; J\u00fcrgen Schmidhuber"}, {"title": "Palestinian Arabic regional accent recognition", "journal": "", "year": "2015", "authors": "Abualsoud Hanani; Hanna Basha; Yasmeen Sharaf; Stephen Taylor"}, {"title": "Human and computer recognition of regional accents and ethnic groups from British english speech", "journal": "Computer Speech and Language", "year": "2013", "authors": "Abualsoud Hanani; Martin J Russell; Michael J Carey"}, {"title": "Scalable modified kneser-ney language model estimation", "journal": "", "year": "2013", "authors": "Kenneth Heafield; Ivan Pouzyrevsky; Jonathan H Clark; Philipp Koehn"}, {"title": "UnibucKernel: An Approach for Arabic Dialect Identification Based on Multiple String Kernels", "journal": "", "year": "2016", "authors": "Tudor Radu; Marius Ionescu;  Popescu"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Lei Kingma;  Ba"}, {"title": "Arabic dialect identification using a parallel multidialectal corpus", "journal": "", "year": "2015", "authors": "Shervin Malmasi; Eshrag Refaee; Mark Dras"}, {"title": "Discriminating between similar languages and arabic dialect identification: A report on the third dsl shared task", "journal": "VarDial", "year": "2016", "authors": "Shervin Malmasi; Marcos Zampieri; Nikola Ljubei; Preslav Nakov; Ahmed Ali; Jrg Tiedemann"}, {"title": "Fine-grained arabic dialect identification", "journal": "", "year": "2018", "authors": "Mohammad Salameh; Houda Bouamor; Nizar Habash"}, {"title": "The Arabic Online Commentary dataset: An annotated dataset of informal Arabic with high dialectal content", "journal": "", "year": "2011", "authors": "Omar F Zaidan; Chris Callison-Burch"}, {"title": "Arabic dialect identification", "journal": "Computational Linguistics", "year": "2014", "authors": "Omar F Zaidan; Chris Callison-Burch"}, {"title": "Findings of the vardial evaluation campaign 2017", "journal": "", "year": "2017", "authors": "Marcos Zampieri; Shervin Malmasi; Nikola Ljubei; Preslav Nakov; Ahmed Ali; Jrg Tiedemann; Yves Scherrer; Nomi Aepli"}, {"title": "Language identification and morphosyntactic tagging: The second VarDial evaluation campaign", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Marcos Zampieri; Shervin Malmasi; Preslav Nakov; Ahmed Ali; Suwon Shon; James Glass; Yves Scherrer; Tanja Samard\u017ei\u0107; Nikola Ljube\u0161i\u0107; J\u00f6rg Tiedemann; Chris Van Der Lee; Stefan Grondelaers; Nelleke Oostdijk; Dirk Speelman;  Van Den; Ritesh Bosch; Bornini Kumar; Mayank Lahiri;  Jain"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Language model scores adjusted to probabilities, for word-based language models of the corpus 26 dialects (d) Language model scores adjusted to probabilities, for char-based language models of the corpus 26 dialects (e) Multinomial naive Bayes classifier", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Neural network model architecture", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Tortuous Classifier confusion matrix", "figure_data": ""}], "doi": "10.21437/Interspeech.2016-1297"}