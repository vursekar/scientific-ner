{"authors": "Boli Chen; Xin Huang; Lin Xiao; Liping Jing", "pub_date": "", "title": "Hyperbolic Capsule Networks for Multi-Label Classification", "abstract": "Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling). Such operations limit the performance. For instance, a multi-label document may contain several concepts. In this case, one vector can not sufficiently capture its salient and discriminative content. Thus, we propose Hyperbolic Capsule Networks (HYPERCAPS) for Multi-Label Classification (MLC), which have two merits. First, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents. Second, Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks. To efficiently handle large-scale MLC datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing. Extensive experiments are conducted on four benchmark datasets. Compared with the state-of-the-art methods, HY-PERCAPS significantly improves the performance of MLC especially on tail labels.", "sections": [{"heading": "Introduction", "text": "The main difference between Multi-Class Classification (MCC) and Multi-Label Classification (MLC) is that datasets in MCC have only serval mutually exclusive classes, while datasets in MLC contain much more correlated labels. MLC allows label co-occurrence in one document, which indicates that the labels are not disjointed. In addition, a large fraction of the labels are the infrequently occurring tail labels (Bhatia et al., 2015), which is also referred as the power-law label distribution. Figure 1 illustrates the label distribution of EUR-LEX57K (Chalkidis et al., 2019). A multi-label document usually has serval head and tail labels, and hence contain several concepts about both its head and tail labels simultaneously.\nRecent works for text classification, such as CNN-KIM (Kim, 2014) and FASTTEXT (Joulin et al., 2017), focus on encoding a document into a fixed-length vector as the distributed document representation (Le and Mikolov, 2014). These encoding based deep learning methods use simple operations (e.g. pooling) to aggregate features extracted by neural networks and construct the document vector representation. A Fully-Connected (FC) layer is usually applied upon the document vector to predict the probability of each label. And each row in its weight matrix can be interpreted as a label vector representation (Du et al., 2019b). In this way, the label probability can be predicted by computing the dot product between label and document vectors, which is proportional to the scalar projection of the label vector onto the document vector as shown in Figure 2. For example, label \"movie\" should have the largest scalar projection onto a document about \"movie\". However, even the learned label representation of \"music\" can be distinguished from \"movie\", it may also have a large scalar projection onto the document.\nMoreover, multi-label documents always contain several concepts about multiple labels, such as a document about \"sport movie\". Whereas the document vector representation is identical to all the labels, and training instances for tail labels are inadequate compared to head labels. The imbalance between head and tail labels makes it hard for the FC layer to make prediction, especially on tail labels. In this case, one vector can not sufficiently capture its salient and discriminative content. Therefore, the performance of constructing the document vector representation via simple aggregation operations is limited for MLC.\nCapsule networks (Sabour et al., 2017;Yang et al., 2018a) has recently proposed to use dynamic routing in place of pooling and achieved better performance for classification tasks. In fact, capsules are fine-grained features compared to the distributed document representation, and dynamic routing is a label-aware feature aggregation procedure. (Zhao et al., 2019) improves the scalability of capsule networks for MLC. However, they only use CNN to construct capsules, which capture local contextual information (Wang et al., 2016). Effectively learning the document information about multiple labels is crucial for MLC. Thus we propose to connect CNN and RNN in parallel to capture both local and global contextual information, which would be complementary to each other. Nevertheless, Euclidean capsules necessitate designing a non-linear squashing function.\nInspired by the hyperbolic representation learning methods which demonstrate that the hyper-bolic space has more representation capacity than the Euclidean space (Nickel and Kiela, 2017;Ganea et al., 2018a), Hyperbolic Capsule Networks (HYPERCAPS) is proposed. Capsules are constrained in the hyperbolic space which does not require the squashing function. Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner. Moreover, in order to fit the large label set of MLC and improve the scalability of HYPERCAPS, adaptive routing is presented to adjust the number of capsules participated in the routing procedure.\nThe main contributions of our work are therefore summarized as follows:\n\u2022 We propose to connect CNN and RNN in parallel to simultaneously extract local and global contextual information, which would be complementary to each other.\n\u2022 HYPERCAPS with HDR are formulated to aggregate features in a label-aware manner, and hyperbolic capsules benefits from the representation capacity of the hyperbolic space.\n\u2022 Adaptive routing is furthermore presented to improve the scalability of HYPERCAPS and fit the large label set of MLC.\n\u2022 Extensive experiments on four benchmark MLC datasets demonstrate the effectiveness of HYPER-CAPS, especially on tail labels.", "n_publication_ref": 12, "n_figure_ref": 2}, {"heading": "Preliminaries", "text": "In order to make neural networks work in the hyperbolic space, formalism of the M\u00f6bius gyrovector space is adopted (Ganea et al., 2018b).\nAn n-dimensional Poincar\u00e9 ball B n is a Riemannian manifold defined as B n = {x \u2208 R n | x < 1}, with its tangent space around p \u2208 B n denoted as T p B n and the conformal factor as \u03bb\np := 2 1\u2212 p 2 . The exponential map exp p : T p B n \u2192 B n for w \u2208 T p B n \\ {0} is consequently defined as exp p (w) = p \u2295 (tanh( \u03bb p 2 w ) w w ). (1)\nTo work with hyperbolic capsules, M\u00f6bius operations in the Poincar\u00e9 ball also need to be formulated.\nM\u00f6bius addition for u, v \u2208 B n is defined as\nu \u2295 v = (1+2 u,v + v 2 )u+(1\u2212 u 2 )v 1+2 u,v + u 2 v 2 ,(2)\nwhere \u2022, \u2022 denotes the Euclidean inner product.\nThus M\u00f6bius summation can be formulated as\nn M i=m p i = p m \u2295 \u2022 \u2022 \u2022 \u2295 p n , p i \u2208 B n .(3)\nM\u00f6bius scalar multiplication for k \u2208 R and p \u2208 B n \\ {0} is defined as\nk \u2297 p = tanh(k tanh \u22121 ( p )) p p .(4\n)\nAnd k \u2297 p = 0 when p = 0 \u2208 B n .\nThe definition of M\u00f6bius matrix-vector multiplication for M \u2208 R m\u00d7n and p \u2208 B n when M p = 0 is as follows\nM \u2297 p = tanh( M p p tanh \u22121 ( p )) M p M p . (5) And M \u2297 p = 0 when M p = 0.\nHDR is developed based on these operations.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Local and Global Hyperbolic Capsules", "text": "Neural networks are generally used as effective feature extractors for text classification. Kernels of CNN can be used to capture local n-gram contextual information at different positions of a text sequence, while hidden states of RNN can represent global long-term dependencies of the text (Wang et al., 2016). Hence, we propose to obtain the combination of local and global hyperbolic capsules by connecting CNN and RNN in parallel, which would be complementary to each other.\nGiven a text sequence of a document with T word tokens x = [x 1 , . . . , x T ], pre-trained wdimensional word embeddings (e.g. GLOVE (Pennington et al., 2014)) are used to compose word vector representations E = [e 1 , . . . , e T ] \u2208 R T \u00d7w , upon which CNN and RNN connected in parallel are used to construct local and global hyperbolic capsules in the Poincar\u00e9 ball. Figure 3 illustrates the framework for HYPERCAPS.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Local Hyperbolic Capsule Layer", "text": "N-gram kernels K \u2208 R k\u00d7w with different window size k are applied on the local region of the word representations E t:t+k\u22121 \u2208 R k\u00d7w to construct the local features as\nl t = \u03d5(K \u2022 E t:t+k\u22121 ),(6)\nwhere \u2022 denotes the element-wise multiplication and \u03d5 is a non-linearity (e.g. ReLU). For simplicity, the bias term is omitted.\nWith totally d channels, the local hyperbolic capsules at position t can be constructed as\nl t = exp 0 ([l (1) t , . . . , l (d) t ]) \u2208 B d . (7)\nTherefore, a k-gram kernel with 1 stride can construct T \u2212k+1 local hyperbolic capsules. The local hyperbolic capsule set is denoted as {u 1 , . . . , u L }.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Global Hyperbolic Capsule Layer", "text": "Bidirectional GRU (Chung et al., 2014) is adopted to incorporate forward and backward global contextual information and construct the global hyperbolic capsules. Forward and backward hidden states at time-step t are obtained by\n\u2212 \u2192 h t = GRU( \u2212\u2212\u2192 h t\u22121 , e t ), \u2190 \u2212 h t = GRU( \u2190\u2212\u2212 h t+1 , e t ).(8)\nEach of the total 2T hidden states can be taken as a global hyperbolic capsule using the exponential map, i.e. \u2212 \u2192 g t = exp 0 ( \u2212 \u2192 h t ), and equally for the backward capsules. The global hyperbolic capsule set is denoted as {u 1 , . . . , u G }.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Hyperbolic Compression Layer", "text": "As discussed in (Zhao et al., 2019), the routing procedure is computational expensive for a large number of capsules. Compressing capsules into a smaller amount can not only relieve the computational complexity, but also merge similar capsules and remove outliers. Therefore, hyperbolic compression layer is introduced. Each compressed local hyperbolic capsule is calculated as a weighted M\u00f6bius summation over all the local hyperbolic capsules. For instance,\nu l = M u k \u2208{u 1 ,...,u L } r k \u2297 u k \u2208 B d ,(9)\nwhere r k is a learnable weight parameter. And likewise for compressing global hyperbolic capsules. Let set {u 1 , . . . , u P } denote the compressed local and global hyperbolic capsules together, which are then aggregated in a label-aware manner via HDR.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Hyperbolic Dynamic Routing", "text": "The purpose of Hyperbolic Dynamic Routing (HDR) is to iteratively aggregate local and global hyperbolic capsules into label-aware hyperbolic capsules, whose activations stand for probabilities of the labels. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Label-Aware Hyperbolic Capsules", "text": "With the acquirement of the compressed local and global hyperbolic capsule set {u 1 , . . . , u P } in layer , let {v 1 , . . . , v Q } denote the label-aware hyperbolic capsule set in the next layer +1, where Q equals to the number of labels.\nFollowing (Sabour et al., 2017), the compressed hyperbolic capsules are firstly transformed into a set of prediction capsules {\u00fb j|1 , . . . ,\u00fb j|P } for the j-th label-aware capsule, each of them is calculated by\u00fb\nj|i = W ij \u2297 u i \u2208 B d ,(10)\nwhere W ij is a learnable parameter. Then v j is calculated as a weighted M\u00f6bius summation over all the prediction capsules by\nv j = M u j|i \u2208{\u00fb j|1 ,...,\u00fb j|P } c ij \u2297\u00fb j|i ,(11)\nwhere c ij denotes the coupling coefficient that indicates the connection strength between\u00fb j|i and v j . The coupling coefficient c ij is iteratively updated during the HDR procedure and computed by the routing softmax\nc ij = exp(b ij ) k exp(b ik ) ,(12)\nwhere the logits b ij are the log prior probabilities between capsule i and j, which are initialized as 0.\nOnce the label-aware hyperbolic capsules are produced, each b ij is then updated by\nb ij = b ij + K(d B (v j ,\u00fb j|i )),(13)\nwhere d B (\u2022, \u2022) denotes the Poincar\u00e9 distance, which can be written as\nd B (u, v) = cosh \u22121 (1 + 1 2 \u03bb u \u03bb v u \u2212 v 2 ). (14\n)\nAnd K is a Epanechnikov kernel function (Wand and Jones, 1994) with\nK = \u03b3 \u2212 x, x \u2208 [0, \u03b3) 0, x \u2265 \u03b3 (15\n)\nwhere \u03b3 is the maximum Poincar\u00e9 distance between two points in the Poincar\u00e9 ball, which is d B (p, 0) with p = 1 \u2212 ( = 10 \u22125 ) to avoid numerical errors. HDR is summarized in Algorithm 1. Different from the routing procedure described in (Sabour et al., 2017), HDR does not require the squashing function since all the hyperbolic capsules are constrained in the Poincar\u00e9 ball.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Adaptive Routing", "text": "The large amount of labels in MLC is one major source of the computational complexity for the routing procedure. Since most of the labels are unrelated to a document, calculating the label-aware hyperbolic capsules for all the unrelated labels is redundant. Therefore, encoding based adaptive routing layer is used to efficiently decide the candidate labels for the document.\nThe adaptive routing layer produces the candidate probability of each label by Initialize \u2200i, j : b ij \u2190 0\nc = \u03c3(W c 1 T e i \u2208E e i + b c ),(16)\n3: for r iterations do 4:\nfor all capsule i in layer and capsule j in layer + 1:\nc ij \u2190 softmax(b ij )\nEq. 12 5:\nfor all capsule j in layer ( + 1):\nv j \u2190 M i c ij \u2297\u00fb j|i 6:\nfor all capsule i in layer and capsule j in layer + 1:\nb ij \u2190 b ij + K(d B (v j ,\u00fb j|i )) 7: return v j\nwhere \u03c3 denotes the Sigmoid function. W c and the bias b c are learnable parameters updated by minimizing the binary cross-entropy loss (Liu et al., 2017)\nL c = \u2212 Q j=1 y j log(c j ) + (1 \u2212 y j )log(1 \u2212 c j ) , (17\n)\nwhere c j \u2208 [0, 1] is the j-th element in c and y j \u2208 {0, 1} denotes the ground truth about label j. The adaptive routing layer selects the candidate labels during test. Label-aware hyperbolic capsules are then constructed via HDR to predict probabilities of these candidate labels.\nDuring the training process, negative sampling is used to improve the the scalability of HYPERCAPS. Let N + denote the true label set and N \u2212 denote the set of randomly selected negative labels, the loss function is derived as\nL f = \u2212 j\u2208N + log(a j ) + j\u2208N \u2212 log(1 \u2212 a j ) , (18\n)\nwhere a j = \u03c3(d B (v j , 0)) is activations of the j-th label-aware capsules, which is proportional to the distance from the origin of the Poincar\u00e9 ball.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experiments", "text": "The proposed HYPERCAPS is evaluated on four benchmark datasets with various label number from 54 to 4271. We compare with the state-of-the-art methods in terms of widely used metrics. Performance on tail labels is also compared to demonstrate the superiority of HYPERCAPS for MLC. An ablation test is also carried out to analyse the contribution of each component of HYPERCAPS.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "Datasets Experiments are carried out on four publicly available MLC datasets, including the small-scale AAPD (Yang et al., 2018b) and RCV1 (Lewis et al., 2004), the large-scale ZHIHU 1 and EUR-LEX57K (Chalkidis et al., 2019). Labels are divided into head and tail sets according to their number of training instances, i.e. labels have less than average number of training instances are divided into the tail label set. Their statistics can be found in Table 1.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Evaluation metrics", "text": "We use the rank-based evaluation metrics which have been widely adopted for MLC tasks (Bhatia et al., 2015;Liu et al., 2017), i.e. Precision@k (P@k for short) and nDCG@k, which are respectively defined as\nP@k = 1 k j\u2208rank k (a) y j ,(19)\nnDCG@k = j\u2208rank k (a) y j /log(j + 1) min(k, y 0 ) j=1 1/log(j + 1) ,(20)\nwhere y j \u2208 {0, 1} denotes the the ground truth about label j, rank k (a) denotes the indices of the candidate label-aware hyperbolic capsules with k largest activations in descending order, and y 0 is the true label number for the document instance. The final results are averaged over all the test instances.\nBaselines To demonstrate the effectiveness of HYPERCAPS on the benchmark datasets, six comparative text classification methods are chosen as the baselines. FASTTEXT (Joulin et al., 2017) is a representative encoding-based method which use average pooling to construct document representations and MLP to make the predictions. SLEEC (Bhatia et al., 2015) is a typical label-embedding method for MLC, which uses k-nearest neighbors search to predict the labels. XML-CNN (Liu et al., 2017) employs CNN as local n-gram feature extractors and a dynamic pooling technique as aggregation method. SGM (Yang et al., 2018b) applies the seq2seq model with attention mechanism, which takes the global contextual information. REGGNN (Xu et al., 2019) uses a combination of CNN and LSTM with a dynamic gate that controls the information from these two parts. NLP-CAP (Zhao et al., 2019) is a capsule-based approach for MLC, which reformulates the routing algorithm. NLP-CAP use only CNN to construct capsules, and it applies the squashing function onto capsules.\nImplementation Details All the words are converted to lower case and padding is used to handle the various lengths of the text sequences. Maximum length of AAPD, RCV1 and EUR-LEX57K is set to 500, while maximum length of ZHIHU is 50. To compose the word vector representations, pre-trained 300-dimensional GLOVE (Pennington et al., 2014) word embeddings are used for AAPD, RCV1 and EUR-LEX57K, while ZHIHU uses its specified 256-dimensional word embeddings. The dimension of the Poincar\u00e9 ball is set to 32 with a radius 1 \u2212 ( = 10 \u22125 ) to avoid numerical errors. Multiple one-dimensional convolutional kernels (with window sizes of 2, 4, 8) are applied in the local hyperbolic capsule layer. The number of compressed local and global hyperbolic capsules is 128. Adaptive routing layer is not applied on the small-scale datasets AAPD and RCV1. The maximum candidate label number is set to 200 for the large-scale datasets ZHIHU and EUR-LEX57K. For the baselines, hyperparameters recommended by their authors are adopted.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Experimental Results", "text": "The proposed HYPERCAPS is evaluated on the four benchmark datasets by comparing with the six baselines in terms of P@k and nDCG@k with k = 1, 3, 5. Results on all the labels averaged over the test instances are shown in Table 2. nDCG@1 is omitted since it gives the same value as P@1.\nIt is notable that HYPERCAPS obtains competitive results on the four datasets. The encoding-based FASTTEXT is generally inferior to the other baselines as it applies the average pooling on word vector representations, which ig-  nores word order for the construction of document representations. The typical MLC method SLEEC takes advantage of label correlations by embedding the label co-occurrence graph. However, SLEEC uses TF-IDF vectors to represent documents, thus word order is also ignored. XML-CNN uses a dynamic pooling technique to aggregate the local contextual features extracted by CNN, while SGM uses attention mechanism to aggregate the global contextual features extracted by LSTM. REGGNN is generally superior to both of them as it combines the local and global contextual information dynamically and takes label correlations into consideration using a regularized loss. However, the two capsulebased methods NLP-CAP and HYPERCAPS consistently outperform all the other methods owing to dynamic routing, which aggregates the fine-grained capsule features in a label-aware manner.\nMoreover, NLP-CAP only uses CNN to extract the local contextual information, while HYPER-CAPS benefits from the parallel combination of local and global contextual information. In addi-tion, NLP-CAP applies the non-linear squashing function for capsules in the Euclidean space, while HDR is designed for hyperbolic capsules, which take advantage of the representation capacity of the hyperbolic space. Therefore, HYPERCAPS outperforms NLP-CAP as expected. This result further confirms that the proposed HYPERCAPS with HDR is effective to learn the label-aware hyperbolic capsules for MLC.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Performance on Tail Labels", "text": "In MLC, tail labels have low occurring frequency and hence are hard to predict compared to head labels. The performance on tail labels of the four benchmark datasets is evaluated in terms of nDCG@k with k = 1, 3, 5. Figure 4 shows the results of the five deep learning based MLC methods, i.e. XML-CNN, SGM, REGGNN, NLP-CAP and HYPERCAPS. nDCG@1 is smaller than nDCG@3 on AAPD, RCV1 and ZHIHU since most of their test instances contain less than three tail labels. It is remarkable that HYPERCAPS outperforms all the other methods on tail labels.\nREGGNN takes advantage of the local and global contextual information and label correlations, thus it outperforms XML-CNN and SGM. The two capsule-based methods NLP-CAP and HYPERCAPS are both superior to the other methods, which indicates that the label-aware dynamic routing is effective for the prediction on tail labels. In addition, the fact that HYPERCAPS significantly improves the prediction performance compared to NLP-CAP implies that the representation capacity of the hyperbolic space and the combination of local and global contextual information are helpful for learning on tail labels. The results demonstrate the superiority of the proposed HYPERCAPS on tail labels for MLC.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Ablation Test", "text": "An ablation test would be informative to analyze the effect of varying different components of the proposed HYPERCAPS, which can be taken apart as local Euclidean capsules only (denoted as L), global Euclidean capsules only (denoted as G), a combination of the local and global Euclidean capsules (denoted as L + G), and a combination of the local and global hyperbolic capsules (denoted as L + G + H). Euclidean capsules (in L, G and L + G) are aggregated via the origin dynamic routing (Sabour et al., 2017), while hyperbolic capsules (in L + G + H) are aggregated via our HDR.\nFigure 5 shows the results on EUR-LEX57K in terms of P@k with k = 1, 3, 5. In order to make the comparison fair, the number of total compressed capsules is equally set to 256 for all the four models. Adaptive routing is also applied with the maximum candidate label number set equally to 200. Generally, the proposed combination of local and global contextual information contributes to the effectiveness of the model (L + G). Therefore, it is practical to combine the local and global contextual information via dynamic routing. HDR furthermore improves the performance by making use of the representation capacity of the hyperbolic space. Overall, each of the components benefits the performance of HYPERCAPS for MLC.\nIn summary, extensive experiments are carried out on four MLC benchmark datasets with various scales. The results demonstrate that the proposed HYPERCAPS can achieve competitive performance compared with the baselines. In particular, effectiveness of HYPERCAPS is shown on tail labels. The ablation test furthermore confirms that the combination of local and global contextual information is practical and HYPERCAPS benefits from the representation capacity of the hyperbolic space.\n6 Related Work", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Multi-Label Classification", "text": "Multi-label classification (MLC) aims at assigning multiple relevant labels to one document. The MLC label set is large compared to Multi-class classification (MCC). Besides, the correlations of labels (e.g. hierarchical label structures (Banerjee et al., 2019)) and the existence of tail labels make MLC a hard task (Bhatia et al., 2015).\nAs data sparsity and scalability issues arise with the large number of labels, XML-CNN (Liu et al., 2017) employs CNN as efficient feature extractor, whereas it ignores label correlations, which are often used to deal with tail labels. The traditional MLC method SLEEC (Bhatia et al., 2015) makes use of label correlations by embedding the label co-occurrence graph. The seq2seq model SGM (Yang et al., 2018b) uses the attention mechanism to consider the label correlations, while REGGNN (Xu et al., 2019) applies a regularized loss specified for label co-occurrence. REGGNN additionally chooses to dynamically combine the local and global contextual information to construct document representations.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Capsule Networks", "text": "Capsule networks are recently proposed to address the representation limitations of CNN and RNN. The concept of capsule is first introduced by (Hinton et al., 2011). (Sabour et al., 2017) replaces the scalar output features of CNN with vector capsules and pooling with dynamic routing. (Hinton et al., 2018) proposes the EM algorithm based routing procedure between capsule layers. (Gong et al., 2018) proposes to regard dynamic routing as an information aggregation procedure, which is more effective than pooling. (Yang et al., 2018a) and (Du et al., 2019a) investigate capsule networks for text classification. (Zhao et al., 2019) then presents a capsule compression method and reformulates the routing procedure to fit for MLC.\nOur work is different from the predecessors as we design the Hyperbolic Dynamic Routing (HDR) to aggregate the parallel combination of local and global contextual information in form of hyperbolic capsules, which are constrained in the hyperbolic space without the requirement of non-linear squashing function. In addition, adaptive routing is proposed to improve the scalability for large number of labels.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Hyperbolic Deep Learning", "text": "Recent research on representation learning (Nickel and Kiela, 2017) indicates that hyperbolic space is superior to Euclidean space in terms of representation capacity, especially in low dimension. (Ganea et al., 2018b) generalizes operations for neural networks in the Poincar\u00e9 ball using formalism of M\u00f6bius gyrovector space. Some works lately demonstrate the superiority of the hyperbolic space for serval natural language processing tasks, such as textual entailment (Ganea et al., 2018a), machine translation (Gulcehre et al., 2019) and word embedding (Tifrea et al., 2019). Our work presents the Hyperbolic Capsule Networks (HYPERCAPS) for MLC.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We present the Hyperbolic Capsule Networks (HYPERCAPS) with Hyperbolic Dynamic Routing (HDR) and adaptive routing for Multi-Label Classification (MLC). The proposed HYPERCAPS takes advantage of the parallel combination of finegrained local and global contextual information and label-aware feature aggregation method HDR to dynamically construct label-aware hyperbolic capsules for tail and head labels. Adaptive routing is additionally applied to improve the scalability of HYPERCAPS by controlling the number of capsules during the routing procedure. Extensive experiments are carried out on four benchmark datasets. Results compared with the state-of-the-art methods demonstrate the superiority of HYPERCAPS, especially on tail labels. As recent works explore the superiority of hyperbolic space to Euclidean space for serval natural language processing tasks, we intend to couple with the hyperbolic neural networks (Ganea et al., 2018b) and the hyperbolic word embedding method such as POINCAR\u00c9GLOVE (Tifrea et al., 2019) in the future.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "This work was supported in part by the National Natural Science Foundation of China under Grant 61822601, 61773050, and 61632004; the Beijing Natural Science Foundation under Grant Z180006; National Key Research and Development Program (2017YFC1703506); the Fundamental Research Funds for the Central Universities (2019JBZ110). We thank the anonymous reviewers for their valuable feedback.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": " ", "text": "Table 1\n), i.e. labels have less than average number of training instances are tail labels. We observe that this division generally follows the Pareto Principle, as nearly 80% of labels are divided into the tail label set.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Hierarchical transfer learning for multi-label text classification", "journal": "", "year": "2019", "authors": "Siddhartha Banerjee; Cem Akkaya; Francisco Perez-Sorrosal; Kostas Tsioutsiouliklis"}, {"title": "Sparse local embeddings for extreme multi-label classification", "journal": "", "year": "2015", "authors": "Kush Bhatia; Himanshu Jain; Purushottam Kar; Manik Varma; Prateek Jain"}, {"title": "Large-scale multi-label text classification on EU legislation", "journal": "", "year": "2019", "authors": "Ilias Chalkidis; Emmanouil Fergadiotis"}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "journal": "", "year": "2014", "authors": "Junyoung Chung; Caglar Gulcehre; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Investigating capsule network and semantic feature on hyperplanes for text classification", "journal": "", "year": "2019", "authors": "Chunning Du; Haifeng Sun; Jingyu Wang; Qi Qi; Jianxin Liao; Chun Wang; Bing Ma"}, {"title": "Explicit interaction model towards text classification", "journal": "", "year": "2019", "authors": "Cunxiao Du; Zhaozheng Chin; Fuli Feng; Lei Zhu; Tian Gan; Liqiang Nie"}, {"title": "Hyperbolic entailment cones for learning hierarchical embeddings", "journal": "", "year": "2018", "authors": "Octavian Ganea; Gary Becigneul; Thomas Hofmann"}, {"title": "Hyperbolic neural networks. In Advances in neural information processing systems 31", "journal": "", "year": "2018", "authors": "Octavian Ganea; Gary Becigneul; Thomas Hofmann"}, {"title": "Information aggregation via dynamic routing for sequence encoding", "journal": "", "year": "2018", "authors": "Jingjing Gong; Xipeng Qiu; Shaojing Wang; Xuanjing Huang"}, {"title": "Hyperbolic attention networks. In International Conference on Learning Representations", "journal": "", "year": "2019", "authors": "Caglar Gulcehre; Misha Denil; Mateusz Malinowski; Ali Razavi; Razvan Pascanu; Karl Moritz Hermann; Peter Battaglia; Victor Bapst; David Raposo; Adam Santoro; Nando De Freitas"}, {"title": "Transforming auto-encoders", "journal": "Springer", "year": "2011", "authors": "Alex Geoffrey E Hinton; Sida D Krizhevsky;  Wang"}, {"title": "Matrix capsules with EM routing", "journal": "", "year": "2018", "authors": "Sara Geoffrey E Hinton; Nicholas Sabour;  Frosst"}, {"title": "Bag of tricks for efficient text classification", "journal": "Short Papers", "year": "2017", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Tomas Mikolov"}, {"title": "Convolutional neural networks for sentence classification", "journal": "", "year": "2014", "authors": "Yoon Kim"}, {"title": "Distributed representations of sentences and documents", "journal": "", "year": "2014", "authors": "Quoc Le; Tomas Mikolov"}, {"title": "Rcv1: A new benchmark collection for text categorization research", "journal": "Journal of machine learning research", "year": "2004-04", "authors": "D David; Yiming Lewis; Tony G Yang; Fan Rose;  Li"}, {"title": "Deep learning for extreme multilabel text classification", "journal": "", "year": "2017", "authors": "Jingzhou Liu; Wei-Cheng Chang; Yuexin Wu; Yiming Yang"}, {"title": "Poincar\u00e9 embeddings for learning hierarchical representations", "journal": "", "year": "2017", "authors": "Maximillian Nickel; Douwe Kiela"}, {"title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"title": "Dynamic routing between capsules", "journal": "", "year": "2017", "authors": "Sara Sabour; Nicholas Frosst; Geoffrey E Hinton"}, {"title": "Poincare glove: Hyperbolic word embeddings", "journal": "", "year": "2019", "authors": "Alexandru Tifrea; Gary Becigneul; Octavian-Eugen Ganea"}, {"title": "Kernel smoothing", "journal": "Chapman and Hall/CRC", "year": "1994", "authors": "P Matt; Chris Wand;  Jones"}, {"title": "Combination of convolutional and recurrent neural network for sentiment analysis of short texts", "journal": "", "year": "2016", "authors": "Xingyou Wang; Weijie Jiang; Zhiyong Luo"}, {"title": "Gated neural network with regularized loss for multi-label text classification", "journal": "", "year": "2019", "authors": "Yunlai Xu; Xiangying Ran; Wei Sun; Xiangyang Luo; Chongjun Wang"}, {"title": "Investigating capsule networks with dynamic routing for text classification", "journal": "", "year": "2018", "authors": "Min Yang; Wei Zhao; Jianbo Ye; Zeyang Lei; Zhou Zhao; Soufei Zhang"}, {"title": "SGM: Sequence generation model for multi-label classification", "journal": "", "year": "2018", "authors": "Pengcheng Yang; Xu Sun; Wei Li; Shuming Ma; Wei Wu; Houfeng Wang"}, {"title": "Towards scalable and reliable capsule networks for challenging NLP applications", "journal": "", "year": "2019", "authors": "Wei Zhao; Haiyun Peng; Steffen Eger; Erik Cambria; Min Yang"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The power-law label distribution of EUR-LEX57K with Y-axis on log-scale. Division is based on average number of training instances.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Illustration of the FC layer in the encoding based methods.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Illustration of HYPERCAPS framework.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Results on tail labels in nDCG@k.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Results of ablation test on EUR-LEX57K in P@k. L denotes local capsules, G denotes global capsules, H denotes HDR.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Statistics of the datasets: N train and N test are the numbers of training and test instances, W train and W test are their average word numbers, L is the average label number per instance, I is the average number of training instances per label, #H and #T are the numbers of head and tail labels, H and T are their average number of training instances respectively.", "figure_data": "DatasetN trainN testW train W testLI#HH#TTAAPD49,3566,484163.34 164.14 2.41 2,199.03 17 5,002.2337911.08RCV123,149781,265 259.47 269.23 3.21 715.5027 2,209.4476184.76ZHIHU2,699,969 299,997 38.1435.56 2.32 3,165.92 442 7,144.31 1,557 2,036.54EUR-LEX57K51,0006,000726.46 725.37 5.0653.45711273.72 3,5609.46Algorithm 1 Hyperbolic Dynamic Routing1: procedure HDR(\u00fb j|i , r, )2:"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Results on all the labels in P@k and nDCG@k, bold face indicates the best of each line.", "figure_data": "DatasetMetricFASTTEXT SLEEC XML-CNN SGM REGGNN NLP-CAP HYPERCAPSP@175.3375.8576.3177.9079.9281.7585.37P@353.8354.3654.4155.7657.3159.6361.89AAPDP@537.5737.8937.8338.5839.5041.9742.51nDCG@371.2271.5472.1273.7375.7778.4081.64nDCG@575.7875.9876.3978.0580.0383.7085.87P@195.4095.3596.8695.3796.5397.0597.10P@379.9679.5181.1181.3681.6981.2782.04RCV1P@555.6455.0656.0753.0656.2356.3357.06nDCG@390.9590.4592.2291.7692.2892.4793.03nDCG@591.6890.9792.6390.6992.6793.1193.66P@149.4050.2249.6850.3250.6753.7356.50P@331.5032.2132.2731.8332.4333.8335.77ZHIHUP@523.2323.8124.1723.9524.2325.1026.27nDCG@346.5247.5746.6546.9047.9748.8950.61nDCG@549.1650.3449.6050.4750.7051.1952.89P@186.1889.4385.3389.1190.4690.8391.42P@373.1876.7374.4078.0379.2980.7282.18EUR-LEX57K P@560.1563.5961.2165.0265.8369.1470.53nDCG@377.4280.9878.5982.3083.4584.1386.05nDCG@573.2176.9674.3678.5079.4081.9183.28"}], "doi": ""}