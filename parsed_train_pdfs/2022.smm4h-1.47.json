{"authors": "Orest Xherija; Hojoon Choi", "pub_date": "", "title": "CompLx@SMM4H'22: In-domain pretrained language models for detection of adverse drug reaction mentions in English tweets", "abstract": "The paper describes the system that team CompLx developed for sub-task 1a of the Social Media Mining for Health 2022 (#SMM4H) Shared Task. We finetune a RoBERTa model, a pretrained, transformer-based language model, on a provided dataset to classify English tweets for mentions of Adverse Drug Reactions (ADRs), i.e. negative side effects related to medication intake. With only a simple finetuning, our approach achieves competitive results, significantly outperforming the average score across submitted systems. We make the model checkpoints 1 and code 2 publicly available. We also create a web application 3 to provide a userfriendly, readily accessible interface for anyone interested in exploring the model's capabilities.", "sections": [{"heading": "Introduction", "text": "The Shared Task (Weissenbacher et al., 2022) of the 2022 Social Media Mining for Health Applications (#SMM4H) workshop proposed ten sub-tasks in the domain of social media mining for health monitoring and surveillance. From the perspective of Natural Language Processing (NLP), these tasks present a considerable challenge since the nature of social media posts requires dealing with both a significant level of language variation (informal and colloquial expressions, ambiguity, multilingual posts) and data sparsity, as well as a widespread presence of noise such as misspellings of clinical concepts and syntactic errors.\nIn the 2022 instantiation of the #SMM4H Shared Task, our team participated in: (i) sub-task 1a, the classification of English tweets containing mentions of Adverse Drug Reactions (ADRs) (Magge et al., 2021), (ii) sub-task 3, the classification of English tweets (3a) and WebMD reviews (3b) contain-1 https://huggingface.co/orestxherija/roberta-base-adr-smm4h2022\n2 https://github.com/orestxherija/CompLx-SMM4H2022 3 https://huggingface.co/spaces/orestxherija/adr-mentionclassifier ing mentions of changes in medication treatments, and (iii) sub-task 8, the classification of English tweets self-reporting chronic stress. In this paper we primarily describe our approach for task 1a, as that constituted the major focus of our efforts.\nTo address these challenges, we finetune a variant of a RoBERTa (Liu et al., 2019) model, a transformer-based (Vaswani et al., 2017) language model pretrained on approximately 128 million tweets (Loureiro et al., 2022) on each sub-task's provided dataset. Without any domain adaptation efforts (apart from standard finetuning on the downstream task) or hyperparameter optimizations, the model outperforms the average of all submissions for sub-task 1a by a 9% absolute difference in F1score.\nIn the following sections, we introduce the subtasks' datasets, describe the model architecture and training setup, report our results, and conclude with a discussion of related research and potential avenues for future work.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Datasets", "text": "In Section [1] we provided a brief summary of each sub-task in which we participated. For each of them, participants were given access to a labeled training and validation set, as well as an unlabeled evaluation set that was used to determined the final performance of the submitted systems.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Sentence", "text": "Label vyvanse make me so hyper and creative and i think of so many tweets ADR feed an ocd vyvanse and cover him in crayons\nNo ADR trazodone has screwed up my sleep schedule. its helping tho. ADR on medication-related keywords for label assignment is going to be problematic: both the first and the second example contain the medication term \"vyvanse\" but they have been assigned different labels, \"ADR\" and \"No ADR\" respectively. This motivates the use of a modeling approach that leverages the overall semantic content of the sentence, rather than keyword matching with individual constituents.\n3 Modeling Approach", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model Architecture", "text": "The establishment of language modeling as the pretraining step in the transfer learning pipeline revolutionized modern NLP with models such as ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018) and, most notably, transformerbased language models such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). In recent years, there have been intensive efforts in the research community to produce ever-larger transformer-based pretrained language models that are trained using a variety of datasets, transformermodel architectures, training objectives and optimization techniques. This should come as no surprise, since such language models have dominated virtually all NLP leaderboards, most notably GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019).\nConsidering this overwhelming success, we opt for a RoBERTa (Liu et al., 2019) model 4 that has been trained on approximately 128 million tweets (Loureiro et al., 2022). Our exact modeling approach is depicted in Figure [1]. We opt for a model that has been trained on an in-domain corpus, namely tweets, as transfer learning has been shown to yield improved results when there is indomain pretraining (Gururangan et al., 2020). We do not use any text normalization steps.\n4 https://huggingface.co/cardiffnlp/twitter-roberta-base-mar2022", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Training Regime", "text": "We train the model to minimize the negative log-likelihood loss using back-propagation with stochastic gradient descent and a mini-batch size of 16. To monitor model performance, we use the train/validation split provided by the organizers. For optimization, we use the AdamW optimizer (Loshchilov and Hutter, 2019) with gradient clipping (Pascanu et al., 2013) and a linear scheduler with no warm-up. We use FP-16 mixed precision (Micikevicius et al., 2018) training (and inference) in order to afford a larger batch size and increased training speed. To optimize GPU use by minimizing the amount of memory allocated for padding tokens, we use dynamic padding and length-based batching in the sense of (Skinner, 2018). Finally, we employ label smoothing (Szegedy et al., 2016) with a smoothing factor of 0.1.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Hyperparameters", "text": "As mentioned in Section [1], we do not experiment with hyperparameter tuning but rather keep the default parameters of the Trainer API in the Hugging Face transformers library. More specifically, we use \u03b2 1 = 0.9, \u03b2 2 = 0.999 and \u03f5 = 10 \u22128 for the AdamW optimizer parameter values and a learning rate of 0.00005. We train the models for a maximum of 25 epochs with an early stopping patience level set to 0.001 for 3 epochs. Finally, we set a maximum sequence length of 128 since input sentences are generally short and we would like to avoid consuming GPU memory for padding tokens.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments and Results", "text": "In this section, we give a brief description of the system we used to conduct our experiments, share our results and provide a brief discussion.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Setup", "text": "The model was developed using the PyTorch (Paszke et al., 2019)  machine with an Intel Core i9-9820X CPU @ 3.30GHz and a NVIDIA GeForce RTX 2080 Ti GPU with 11GB of memory.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Results", "text": "Table [3] summarizes the performance of our approach in the validation set for each sub-task. Note that in this set of experiments, the validation set was used both during training (e.g. for early stopping or selection of batch size) as well as for the reporting of the systems' performance. Table [4] summarizes the performance of our approach in the evaluation set for each sub-task. The organizers chose to disclose to each team only their respective score along with the average score of all submitted systems. Our system performed considerably better than the average in sub-task 1a and surpassed the existing state-of-the-art F1-score of 0.63 reported in (Magge et al., 2021). Performance was considerably poorer for sub-tasks 3 and 8. As mentioned in     4: Results on the evaluation set for sub-tasks 1a, 3 and 8. Average score of all participating systems in parentheses. Metric is F1-score for class 1.\nSection [1], our main efforts were dedicated to subtask 1a and the system developed did not transfer well to the remaining sub-tasks.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Conclusion and Future Directions", "text": "We demonstrated that a RoBERTa model (Liu et al., 2019) pretrained on approximately 128 million tweets performs very competitively when finetuned on English tweet classification for ADRs. Using only a standard finetuning approach, our model obtained competitive results, outperforming the average of all submissions for sub-task 1 by a 9% absolute difference in F1-score. This constitutes yet another testament of the fact that large pre-trained language models have rightfully become the default approach in virtually all NLP tasks.\nWith respect to potential future work, there is a large collection of available options. Text classification and, more generally, binary classification is one of the oldest and most widely researched topics in NLP. Most approaches aiming to improve performance of classification models can be broadly categorized into three groups, depending on the segment of the machine learning workflow that they are targeting. Data augmentation methods typically target the initial part of the workflow, the data, aiming to increase the quantity, quality and diversity of the training dataset to ensure that model performance is robust to small syntactic or semantic perturbations in the inputs. Transformations acting directly on strings, such random token insertions or deletions, synonym/antonym replacements and related techniques (Wei and Zou, 2019;Karimi et al., 2021, inter alia) have shown significant performance improvements, especially in lowresource scenarios much like the one in this shared task.\nA second approach, evidently a natural extension of the previous technique, would be to target vector encodings of the tokens and/or documents that are produced by the various layers of the neural networks. We can distinguish two different approaches here: (i) improve the language model backbone during the pretraining phase, or (ii) improve the weights of the language model backbone during finetuning. The research community has devoted intensive efforts in the former approach, as can be observed by the ever-increasing list of transformerbased pretrained language models (Devlin et al., 2019;Joshi et al., 2020;Kitaev et al., 2020;Raffel et al., 2020;Brown et al., 2020, inter multi alia) released. Model size, in terms of total number of trainable parameters, has been consistently shown to correlate strongly with downstream performance, so opting for a larger pretrained model would be a reasonable first steps towards more transferable vector representations (and hence improved performance) in the downstream task. The latter approach would include domain adaptation techniques, such as continued self-supervised pretraining followed by supervised finetuning, which has been shown (Gururangan et al., 2020) to consistently lead to superior results relative to direct finetuning.\nFinally, one could aim to improve performance by modifying aspects of the objective function. (Hui and Belkin, 2021), in an extensive series of experiments, show that the established practice of using a cross-entropy loss for classification is not well-founded and show through a variety of diverse experiments that a square loss can, in many cases, significantly improve performance.", "n_publication_ref": 10, "n_figure_ref": 0}], "references": [{"title": "Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners", "journal": "Curran Associates, Inc", "year": "", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "2020. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": "Ana Suchin Gururangan; Swabha Marasovi\u0107; Kyle Swayamdipta; Iz Lo; Doug Beltagy; Noah A Downey;  Smith"}, {"title": "Universal Language Model Fine-tuning for Text Classification", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jeremy Howard; Sebastian Ruder"}, {"title": "Evaluation of neural architectures trained with square loss vs crossentropy in classification", "journal": "", "year": "2021", "authors": "Like Hui; Mikhail Belkin"}, {"title": "Span-BERT: Improving Pre-training by Representing and Predicting Spans", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Mandar Joshi; Danqi Chen; Yinhan Liu; Daniel S Weld; Luke Zettlemoyer; Omer Levy"}, {"title": "AEDA: An Easier Data Augmentation Technique for", "journal": "", "year": "2021", "authors": "Akbar Karimi; Leonardo Rossi; Andrea Prati"}, {"title": "Text Classification", "journal": "Association for Computational Linguistics", "year": "", "authors": ""}, {"title": "Reformer: The Efficient Transformer", "journal": "", "year": "2020", "authors": "Nikita Kitaev; Lukasz Kaiser; Anselm Levskaya"}, {"title": "", "journal": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": "1907", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Decoupled Weight Decay Regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"title": "TimeLMs: Diachronic Language Models from Twitter", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Daniel Loureiro; Francesco Barbieri; Leonardo Neves; Luis Espinosa Anke; Jose Camacho-Collados"}, {"title": "DeepADEMiner: a deep learning pharmacovigilance pipeline for extraction and normalization of adverse drug event mentions on twitter", "journal": "Journal of the American Medical Informatics Association", "year": "2021", "authors": "Arjun Magge; Elena Tutubalina; Zulfat Miftahutdinov; Ilseyar Alimova; Anne Dirkson; Suzan Verberne; Davy Weissenbacher; Graciela Gonzalez-Hernandez"}, {"title": "Mixed Precision Training", "journal": "", "year": "2018", "authors": "Paulius Micikevicius; Sharan Narang; Jonah Alben; Gregory Diamos; Erich Elsen; David Garcia; Boris Ginsburg; Michael Houston; Oleksii Kuchaiev; Ganesh Venkatesh; Hao Wu"}, {"title": "On the Difficulty of Training Recurrent Neural Networks", "journal": "", "year": "2013", "authors": "Razvan Pascanu; Tomas Mikolov; Yoshua Bengio"}, {"title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga; Alban Desmaison; Andreas Kopf; Edward Yang; Zachary Devito; Martin Raison; Alykhan Tejani; Sasank Chilamkurthy; Benoit Steiner; Lu Fang; Junjie Bai; Soumith Chintala"}, {"title": "Deep Contextualized Word Representations", "journal": "Long Papers", "year": "2018", "authors": "Matthew E Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"title": "Improving Language Understanding by Generative Pre-Training", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimha; Tim Salimans; Ilya Sutskever"}, {"title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"title": "Product Categorization with LSTMs and Balanced Pooling Views", "journal": "ACM", "year": "2018", "authors": "Michael Skinner"}, {"title": "Rethinking the Inception Architecture for Computer Vision", "journal": "", "year": "2016", "authors": "Christian Szegedy; Vincent Vanhoucke; Sergey Ioffe; Jon Shlens; Zbigniew Wojna"}, {"title": "Attention is All you Need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "journal": "", "year": "2019", "authors": "Alex Wang; Yada Pruksachatkun; Nikita Nangia; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jason Wei; Kai Zou"}, {"title": "", "journal": "Arjun Magge", "year": "", "authors": "Davy Weissenbacher; Ari Z Klein; Luis Gasc\u00f3; Darryl Estrada-Zavala; Martin Krallinger; Yuting Guo; Yao Ge; Abeed Sarker; Ana Lucia Schmidt; Raul Rodriguez-Esteban; Mathias Leddin"}, {"title": "Overview of the Seventh Social Media Mining for Health Applications (#SMM4H) Shared Tasks at COLING 2022", "journal": "", "year": "2022", "authors": "Juan M Banda; Vera Davydova; Elena Tutubalina; Graciela Gonzalez-Hernandez"}, {"title": "Transformers: State-of-the-Art Natural Language Processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Illustration of modeling approach (inference step): the string input is tokenized and the tokens are passed through the language model backbone so as to obtain contextualized (vector) representations of the tokens. The vector associated with the [CLS] token is passed through a feed-forward layer and the logit outputs are used to decide the sample's class label, \"ADR\" or \"No ADR\".", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Number of samples per split per task.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Selection of samples from training set of sub-task 1a.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}], "doi": "10.18653/v1/N19-1423"}