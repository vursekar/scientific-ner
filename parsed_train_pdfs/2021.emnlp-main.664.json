{"authors": "Alessandro Raganato; Ra\u00fal V\u00e1zquez; Mathias Creutz; J\u00f6rg Tiedemann", "pub_date": "", "title": "An Empirical Investigation of Word Alignment Supervision for Zero-Shot Multilingual Neural Machine Translation", "abstract": "Zero-shot translations is a fascinating feature of Multilingual Neural Machine Translation (MNMT) systems. These MNMT models are usually trained on English-centric data, i.e. English either as the source or target language, and with a language label prepended to the input indicating the target language. However, recent work has highlighted several flaws of these models in zero-shot scenarios where language labels are ignored and the wrong language is generated or different runs show highly unstable results. In this paper, we investigate the benefits of an explicit alignment to language labels in Transformer-based MNMT models in the zero-shot context, by jointly training one cross attention head with word alignment supervision to stress the focus on the target language label. We compare and evaluate several MNMT systems on three multilingual MT benchmarks of different sizes, showing that simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language, improving the zero-shot performance overall. Moreover, as an additional advantage, we find that our alignment supervision leads to more stable results across different training runs.", "sections": [{"heading": "Introduction", "text": "Multilingual Neural Machine Translation (MNMT) focuses on translation between multiple language pairs through a single optimized neural model, and has been explored from different angles witnessing a rapid progress in recent years (Arivazhagan et al., 2019b;Dabre et al., 2020;Lin et al., 2021). Besides the great flexibility MNMT models offer, they are also highlighted by their so called zero-shot translation capabilities, i.e., translating between all combinations of languages available in the training data, including those with no parallel data seen at training time (Ha et al., 2016;Firat et al., 2016;Johnson et al., 2017). Many studies have investigated this feature, focusing on the impact of both, the model architecture design (Arivazhagan et al., 2019a; and data pre-processing (Lee et al., 2017;Wang et al., 2019;Rios et al., 2020;. Broadly speaking, MNMT architectures are categorized according to their degree of parameter sharing, from fully shared (Johnson et al., 2017) to the use of language-specific components (V\u00e1zquez et al., 2020;Escolano et al., 2021;Zhang et al., 2021). The Johnson et al. (2017) MNMT model is widely used, due to its simplicity and good translation quality. It uses the fully shared parameters setting, and relies on appending an artificial language label to each input sentence to indicate the target language. While this method allows for zeroshot translation, several works have highlighted two major flaws: i) its failure to reliably generalize to unseen language pairs, ending up with the so called off-target issue, where the language label is ignored and the wrong target language is produced as a result , ii) its lack of stability in translation results between different training runs (Rios et al., 2020).\nIn this work, we investigate the role of guided alignment in the Johnson et al. (2017) setting, by jointly training one cross attention head to explicitly focus on the target language label. We show that alignment supervision mitigates the off-target translation issue in the zero-shot case. Our method improves the zero-shot translation performance and results in more stable results across different training runs.", "n_publication_ref": 17, "n_figure_ref": 0}, {"heading": "Methodology", "text": "Alignment Methods. Given a bitext B src = (s 1 , ..., s j , ..., s N ) and B trg = (t 1 , ..., t i , ..., t M ) where B src is a sentence in the source language and B trg is its translation in the target language, an alignment A is a mapping of words between B src and B trg (Tiedemann, 2011), formally defined as \nA \u2286 {(j, i) : j = 1, ..., N ; i = 1, ..., M } (1)\nWe study three different settings: (a) standard word alignment between corresponding words, (b) alignments between all target words and the language label in the input string, and (c) the union between the former two. Figure 1 shows an example of those approaches. To produce word alignments between parallel sentences, i.e., Figure 1 (a), we use the awesome-align tool (Dou and Neubig, 2021), a recent work that leverages multilingual BERT (Devlin et al., 2019) to extract the links. 1 Models. To train Many-to-Many MNMT models, we use a 6-layer Transformer architecture (Vaswani et al., 2017), prepending a language label in the input to indicate the target language (Johnson et al., 2017). Following Garg et al. (2019), given an alignment matrix AM M,N and an attention matrix computed by a cross attention head AH M,N , for each target word i, we use the following cross-entropy loss L a to minimize the Kullback-Leibler divergence between AH and AM :\nL a (AH, AM ) = \u2212 1 M M i=1 N j=1 AM i,j log(AH i,j )\n(2) The overall loss L is:\nL = L t + \u03b3L a (AH, AM ) (3)\nwhere L t is the standard NLL translation loss, and \u03b3 is a hyperparameter. We use \u03b3 = 0.05, supervising only one cross attention head at the third last layer. 2 Given the sparse nature of the alignments, we replace the softmax operator in the cross attention head with the \u03b1-entmax function (Peters et al., 2019;Correia et al., 2019). Entmax allows sparse attention weights for any \u03b1 > 1. Following Peters et al. ( 2019), we use \u03b1=1.5.", "n_publication_ref": 8, "n_figure_ref": 2}, {"heading": "Experimental Setup", "text": "We use three highly multilingual MT benchmarks:\n\u2022 TED Talks (Qi et al., 2018). An Englishcentric parallel corpus with 10M training sentences across 116 translation directions. Following Aharoni et al. (2019), we evaluate on a total of 16 language directions, while as zeroshot test we evaluate on 4 language pairs.\n\u2022 WMT-2018 (Bojar et al., 2018). 3 A parallel dataset provided by the WMT-2018 shared task on news translation. We use all available language pairs, i.e. 14, up to 5M training sentences for each language pair. We evaluate the models on the test sets of the shared task, i.e. newstest2018. As there are no zero-shot test sets provided by the competition, we use the test portion from the Tatoeba-challenge (Tiedemann, 2020), 4 in all possible language pair combinations included in the challenge.\n\u2022 OPUS-100 ). An Englishcentric multi-domain benchmark, built upon the OPUS parallel text collection (Tiedemann, 2012) Aharoni et al. (2019), and 2 its variant with a 1.5-entmax function on the cross attention heads as in Correia et al. (2019). The labels (a), (b), (c) denote the use of different alignment supervision (see Section 2). \"#Param.\": trainable parameter number. \"EN -> X ( 16)\" and \"X-> EN ( 16)\": average BLEU scores for English to Non-English languages and for Non-English languages to English on 16 language pairs respectively. \"BLEU zero (4)\" and \"ACC zero (4)\": average BLEU scores and target language identification accuracy over 4 zero-shot language directions. We report average BLEU and accuracy scores, plus the standard deviation over 3 training runs with different random seeds.\nlanguage pair. It provides supervised translation test data for 188 language pairs, and zero-shot evaluation data for 30 pairs.\nFollowing related work (Aharoni et al., 2019;, we apply joint Byte-Pair Encoding (BPE) segmentation (Sennrich et al., 2016;Kudo and Richardson, 2018), with a shared vocabulary size of 32K symbols for TED Talks and 64K for WMT-2018 and OPUS-100. As evaluation measure, we use tokenized BLEU (Papineni et al., 2002) to be comparable with Aharoni et al. (2019) for the TED Talks benchmark, and SACREBLEU 5 (Post, 2018) for WMT-2018 and OPUS-100. 6 As an additional evaluation, we report the target language identification accuracy score for the zeroshot cases , called ACC zero . We use fasttext as a language identification tool (Joulin et al., 2017), counting how many times the translation language matches the reference target language.\nThe Transformer models follow the base setting of Vaswani et al. (2017), with three different random seeds in each run. All of them are trained on the Many-to-Many English-centric scenario, i.e., on the concatenation of the training data having English either as the source or target language. Details about data and model settings in the Appendix.\n5 Signature: BLEU+case.mixed+numrefs.1+smooth.exp+ tok.{13a,ja-mecab-0.996-IPA,zh}+version.1.5.0 6 We report average BLEU over all test sets. Scores for each language pair are available in the supplementary material.", "n_publication_ref": 18, "n_figure_ref": 0}, {"heading": "Results and Discussion", "text": "Throughout this section we refer to our baseline MNMT models by the labels 1 and 2 , while 3 , 4 , and 5 mark the models trained with the auxiliary alignment supervision task, (a), (b), (c) from Figure 1 respectively (see Section 2). TED Talks. Table 1 shows the results on the TED Talks benchmark. Regarding translation quality on the language pairs seen during training (EN \u2192 X and X \u2192 EN columns), average BLEU scores from all models end up in the same ballpark. In contrast, zero-shot results vary across the board, with 5 attaining the best performance, with almost 2 BLEU points better than its baseline 2 . Moreover, 5 considerably improves target language identification accuracy (ACC zero ), with more stable results, i.e. lower standard deviation, than counterparts. Surprisingly, the addition of alignment supervision (a) and (b) as an auxiliary task has an overall detrimental effect on the zero-shot performance, even though model 4 results in more stable results than 2 .\nWMT-2018. Table 2 reports the results on the WMT-2018 benchmark. As expected, in a highresource scenario bilingual baselines are hard to beat. Among multilingual models, the overall performance follows a similar trend as before. Enriching the model with alignment supervision (c) results in the best system overall, with an improvement of more than 3 BLEU points in the zero-shot    (2020). MATT denotes the use of merged attention (Zhang et al., 2019). LALN and LALT indicate the use of language-aware components. Average BLEU, target language identification accuracy and standard deviation of 3 training runs.\ntestbed compared to baseline 2 , and with stable results across three training runs (standard deviations of 0.12 and 0.82).", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "OPUS-100.", "text": "As one can see from Table 3, we confirm the positive effect of adding the alignment strategy (c) both as translation quality and as a mechanism to produce stable results even in a highly multilingual setup, i.e., training on 198 language directions. The average score over 30 zeroshot language pairs is low but the individual results range from 0.3 to 17.5 BLEU showing the potentials of multilingual models in this challenging data set as well. 7 Even though the results from our best model still lag behind models with languagespecific components, i.e. MATT+LALN+LALT from , we note that our results demonstrate the positive effect of alignment on zero-shot translation. 8 Overall, our experiments show consistent results across different benchmarks, providing quantitative evidence on the utility of guided alignment in highly multilingual MT scenarios. Supervising a single cross attention head with the alignment method (c) substantially reduces the instability between training runs, mitigating the off-target translation issue in the zero-shot evaluation. Zero-shot improvements, i.e. BLEU zero and ACC zero , are large in two benchmarks out of three, i.e. Ted Talks and WMT-2018, and with a similar trend in OPUS-100. We also note that performance differences may be related to the different data sizes (see Appendix A). TED Talks is a rather small and imbalanced multilingual dataset with 116 language directions with a total of 10M training sentences, while WMT-2018 and OPUS-100 comprise 14 language pairs for a total of 47.8M training sentences, and 110M training sentences for 198 language pairs, respectively. We plan on investigating the impact of the training size and the resulting alignments on the zero-shot test sets further in future work.\nLimitations Finally, we highlight that we have focused on a quantitative evaluation on Englishcentric MNMT benchmarks only, therefore we lack a comprehensive evaluation on complete MNMT benchmarks including training data without English as source and target language (Freitag and Firat, 2020;Rios et al., 2020;Tiedemann, 2020;Goyal et al., 2021).", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Conclusions and Future Work", "text": "In this work we present an empirical comparative evaluation of integrating different alignment methods in Transformer-based models for highly multilingual English-centric MT setups. Our extensive evaluation over three alignment variants shows that adding alignment supervision between corresponding words and the language label consistently improves the stability of the models, resulting in stable performance across different runs and mitigating the off-target translation issue in the zero-shot scenario. We believe that our work will pave the way for designing new and better multilingual MT models to improve their generalization in zero-shot setups.\nAs future work, we intend to analyze the quality of the learned alignments and their effect on the other attention weights in both supervised and zeroshot evaluation data (Raganato and Tiedemann, 2018;Tang et al., 2018;Mare\u010dek and Rosa, 2019;Voita et al., 2019). Finally, we plan to explore other mechanisms to inject prior knowledge to better handle zero-shot translations (Deshpande and Narasimhan, 2020;Song et al., 2020).", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "A Data and Model details", "text": "A.1 Data TED Talks (Qi et al., 2018). This parallel corpus includes 59 language pairs from and to English. It is a highly imbalanced benchmark, ranging from less than 4K up to 215K training sentences. We use the same languages as Aharoni et al. (2019) for both supervised testing and zero-shot evaluation. As supervised test sets, we use {Azerbeijani, Belarusian, Galician, Slovak, Arabic, German, Hebrew, Italian}\u2194English. As zero-shot test sets, we use Arabic\u2194French, and Ukrainian\u2194Russian.\nWMT-2018 (Bojar et al., 2018). We use training and testing data as provided by the WMT 2018 news translation task organizers. The benchmark contains a total of 14 language pairs: {Chinese, Czech, Estonian, Finnish, German, Russian, Turkish}\u2194English. For training, we use up to 5M parallel sentences per language pair, with Turkish\u2194English, Estonian\u2194English, and Finnish\u2194English, having only 200K, 1M, and 2.7M training sentences, respectively. For zeroshot test sets, we use the test data from Tiedemann (2020), using the following 24 language directions: Czech \u2194 German, German \u2194 Russian, German \u2194 Chinese, Finnish \u2194 German, Finnish \u2194 Turkish, Russian \u2194 Finnish, Russian \u2194 Chinese, Turkish \u2194 Chinese, Czech \u2194 Russian, German \u2194 Turkish, Estonian \u2194 Russian, Russian \u2194 Turkish OPUS-100 . OPUS-100 is a recent benchmark consisting of 55M Englishcentric sentence pairs covering 100 languages. The data is collected from movie subtitles, GNOME documentation, and the Bible. Out of 99 language pairs, 44 have 1M sentences, 73 have at least 100K sentences, and 95 at least 10K. It provides also zero-shot test sets, pairing the following languages: Arabic, Chinese, Dutch, French, German, and Russian.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "A.2 Model hyperparameters", "text": "We use the OpenNMT-py framework (Klein et al., 2017), and the Transformer base model setting (Vaswani et al., 2017). Specifically, we use 6 layers for the encoder and the decoder, 512 as model dimension, and 2048 as hidden dimension.  We applied 0.1 as dropout for both residual layers and attention weights, using the Adam optimizer (Kingma and Ba, 2015) with \u03b21 = 0.9, and \u03b22 = 0.998, with learning rate set at 3 and 40K warmup steps as in Aharoni et al. (2019). We train the models with three random seeds each, for 200K training steps for the TED Talks and WMT-2018 benchmarks, while for 500K training steps for the OPUS-100. To speed up training, we use halfprecision, i.e., FP16.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "This work is part of the FoTran project, funded by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 771113). The authors gratefully acknowledge the support of the CSC -IT Center for Science, Finland, for computational resources. Finally, We would also like to acknowledge NVIDIA and their GPU grant.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Massively multilingual neural machine translation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Roee Aharoni; Melvin Johnson; Orhan Firat"}, {"title": "The missing ingredient in zeroshot neural machine translation", "journal": "", "year": "2019", "authors": "Naveen Arivazhagan; Ankur Bapna; Orhan Firat; Roee Aharoni; Melvin Johnson; Wolfgang Macherey"}, {"title": "Massively multilingual neural machine translation in the wild: Findings and challenges", "journal": "", "year": "2019", "authors": "Naveen Arivazhagan; Ankur Bapna; Orhan Firat; Dmitry Lepikhin; Melvin Johnson; Maxim Krikun; Mia Xu Chen; Yuan Cao; George Foster; Colin Cherry"}, {"title": "Findings of the 2018 conference on machine translation (WMT18)", "journal": "", "year": "2018", "authors": "Ond\u0159ej Bojar; Christian Federmann; Mark Fishel; Yvette Graham; Barry Haddow; Philipp Koehn; Christof Monz"}, {"title": "Adaptively sparse transformers", "journal": "", "year": "2019", "authors": "M Gon\u00e7alo; Vlad Correia; Andr\u00e9 F T Niculae;  Martins"}, {"title": "A survey of multilingual neural machine translation", "journal": "ACM Computing Surveys (CSUR)", "year": "2020", "authors": "Raj Dabre; Chenhui Chu; Anoop Kunchukuttan"}, {"title": "Guiding attention for self-supervised learning with transformers", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Ameet Deshpande; Karthik Narasimhan"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Word alignment by fine-tuning embeddings on parallel corpora", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Yi Zi; Graham Dou;  Neubig"}, {"title": "Multilingual machine translation: Closing the gap between shared and language-specific encoder-decoders", "journal": "", "year": "2021", "authors": "Carlos Escolano; Marta R Costa-Juss\u00e0; A R Jos\u00e9; Mikel Fonollosa;  Artetxe"}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Orhan Firat; Baskaran Sankaran; Yaser Al-Onaizan; Fatos T Yarman Vural; Kyunghyun Cho"}, {"title": "Complete multilingual neural machine translation", "journal": "", "year": "2020", "authors": "Markus Freitag; Orhan Firat"}, {"title": "Jointly learning to align and translate with transformer models", "journal": "", "year": "2019", "authors": "Sarthak Garg; Stephan Peitz; Udhyakumar Nallasamy; Matthias Paulik"}, {"title": "The flores-101 evaluation benchmark for low-resource and multilingual machine translation", "journal": "", "year": "2021", "authors": "Naman Goyal; Cynthia Gao; Vishrav Chaudhary; Peng-Jen Chen; Guillaume Wenzek; Da Ju; Sanjana Krishnan; Marc'aurelio Ranzato; Francisco Guzman; Angela Fan"}, {"title": "Toward multilingual neural machine translation with universal encoder and decoder", "journal": "", "year": "2016", "authors": "Thanh-Le Ha; Jan Niehues; Alexander Waibel"}, {"title": "Google's multilingual neural machine translation system: Enabling zero-shot translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Melvin Johnson; Mike Schuster; Quoc V Le; Maxim Krikun; Yonghui Wu; Zhifeng Chen; Nikhil Thorat; Fernanda Vi\u00e9gas; Martin Wattenberg; Greg Corrado; Macduff Hughes; Jeffrey Dean"}, {"title": "Bag of tricks for efficient text classification", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Tomas Mikolov"}, {"title": "Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR)", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "OpenNMT: Opensource toolkit for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Guillaume Klein; Yoon Kim; Yuntian Deng; Jean Senellart; Alexander Rush"}, {"title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"title": "Fully character-level neural machine translation without explicit segmentation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Jason Lee; Kyunghyun Cho; Thomas Hofmann"}, {"title": "Learning language specific sub-network for multilingual machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Zehui Lin; Liwei Wu; Mingxuan Wang; Lei Li"}, {"title": "From balustrades to pierre vinken: Looking for syntax in transformer self-attentions", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "David Mare\u010dek; Rudolf Rosa"}, {"title": "A systematic comparison of various statistical alignment models", "journal": "Computational Linguistics", "year": "2003", "authors": "Josef Franz; Hermann Och;  Ney"}, {"title": "BLEU: A method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"title": "Sparse sequence-to-sequence models", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ben Peters; Vlad Niculae; Andr\u00e9 F T Martins"}, {"title": "Improving zero-shot translation with language-independent constraints", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ngoc-Quan Pham; Jan Niehues; Thanh-Le Ha; Alexander Waibel"}, {"title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"title": "When and why are pre-trained word embeddings useful for neural machine translation?", "journal": "", "year": "2018", "authors": "Ye Qi; Devendra Sachan; Matthieu Felix; Sarguna Padmanabhan; Graham Neubig"}, {"title": "Fixed encoder self-attention patterns in transformer-based machine translation", "journal": "", "year": "2020", "authors": "Alessandro Raganato; Yves Scherrer; J\u00f6rg Tiedemann"}, {"title": "An analysis of encoder representations in transformerbased machine translation", "journal": "", "year": "2018", "authors": "Alessandro Raganato; J\u00f6rg Tiedemann"}, {"title": "Subword segmentation and a single bridge language affect zero-shot neural machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Annette Rios; Mathias M\u00fcller; Rico Sennrich"}, {"title": "Neural machine translation of rare words with subword units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Alignment-enhanced transformer for constraining nmt with pre-specified translations", "journal": "", "year": "2020", "authors": "Kai Song; Kun Wang; Heng Yu; Yue Zhang; Zhongqiang Huang; Weihua Luo; Xiangyu Duan; Min Zhang"}, {"title": "An analysis of attention mechanisms: The case of word sense disambiguation in neural machine translation", "journal": "", "year": "2018", "authors": "Gongbo Tang; Rico Sennrich; Joakim Nivre"}, {"title": "Bitext alignment", "journal": "Synthesis Lectures on Human Language Technologies", "year": "2011", "authors": "J\u00f6rg Tiedemann"}, {"title": "Parallel data, tools and interfaces in OPUS", "journal": "", "year": "2012", "authors": "J\u00f6rg Tiedemann"}, {"title": "The tatoeba translation challenge -realistic data sets for low resource and multilingual MT", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "J\u00f6rg Tiedemann"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "A systematic study of inner-attention-based sentence representations in multilingual neural machine translation", "journal": "Computational Linguistics", "year": "2020", "authors": "Ra\u00fal V\u00e1zquez; Alessandro Raganato; Mathias Creutz; J\u00f6rg Tiedemann"}, {"title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Elena Voita; David Talbot; Fedor Moiseev; Rico Sennrich; Ivan Titov"}, {"title": "Multilingual neural machine translation with soft decoupled encoding", "journal": "", "year": "2019", "authors": "Xinyi Wang; Hieu Pham; Philip Arthur; Graham Neubig"}, {"title": "Multi-task learning for multilingual neural machine translation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Yiren Wang; Chengxiang Zhai; Hany Hassan"}, {"title": "Language tags matter for zero-shot neural machine translation", "journal": "", "year": "2021", "authors": "Liwei Wu; Shanbo Cheng; Mingxuan Wang; Lei Li"}, {"title": "Share or not? learning to schedule language-specific capacity for multilingual translation", "journal": "", "year": "2021", "authors": "Biao Zhang; Ankur Bapna; Rico Sennrich; Orhan Firat"}, {"title": "Improving deep transformer with depth-scaled initialization and merged attention", "journal": "", "year": "2019", "authors": "Biao Zhang; Ivan Titov; Rico Sennrich"}, {"title": "Improving massively multilingual neural machine translation and zero-shot translation", "journal": "", "year": "2020", "authors": "Biao Zhang; Philip Williams; Ivan Titov; Rico Sennrich"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: English \u2192 German example sentence with different alignment methods. Alignments in (a) show word alignments between corresponding words in the two languages, (b) our introduced alignments between all target words and the input language label, and (c) the union of the two.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ". It covers a total of 198 language directions, with up to 1M training sentence per ID Model #Param. EN \u2192 X (16) X \u2192 EN (16) BLEU zero (4) ACC zero (4) \u00b10.25 27.21 \u00b10.38 10.02 \u00b11.50 \u00b10.11 27.37 \u00b10.19 11.94 \u00b10.86 97.25 \u00b1 2.66 Table 1: Results on the Many-to-Many TED Talks benchmark. The baselines consist of 1 our replication of the standard 6-layer Transformer model by", "figure_data": "Aharoni et al. (2019)-103 473M 20.1129.979.17-Aharoni et al. (2019)93M 19.5428.03--1Transformer93M 18.93 \u00b10.15 27.56 \u00b10.256.81 \u00b10.8672.38 \u00b1 7.1821 + 1.5-entmax93M 18.90 87.81 \u00b1 8.8032 + (a)93M 18.99 \u00b10.07 27.58 \u00b10.128.38 \u00b15.3773.12 \u00b141.1442 + (b)93M 18.98 \u00b10.08 27.48 \u00b10.136.35 \u00b10.8765.01 \u00b1 6.1052 + (c)93M 19.06"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "ID Model #Param. EN \u2192 X (7) X \u2192 EN (7) BLEU zero (24) ACC zero (24) Transformer 127M 15.18 \u00b10.54 18.39 \u00b10.65 9.78 \u00b10.61 74.17 \u00b14.78 2 1 + 1.5-entmax 127M 15.17 \u00b10.41 18.33 \u00b10.56 8.55 \u00b10.61", "figure_data": "Transformer, Bilingual 127M 18.2819.25--165.31 \u00b14.4632 + (a) 127M 11.99 \u00b10.37 16.42 \u00b10.73 6.38 \u00b10.8373.78 \u00b17.8442 + (b) 127M 15.46 \u00b10.16 18.66 \u00b10.31 11.72 \u00b10.7685.64 \u00b13.3752 + (c) 127M 15.50 \u00b10.18 18.70 \u00b10.23 11.98 \u00b10.1285.68 \u00b10.82"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Results on the Many-to-Many WMT-2018 benchmark. Average BLEU, target language identification accuracy and standard deviation of 3 training runs. \u00b10.07 26.69 \u00b10.09 18.51 \u00b10.18 25.39 \u00b10.01 4.73 \u00b10.16 32.00 \u00b10.96", "figure_data": "IDModel #Param. EN \u2192 X (94) X \u2192 EN (94) EN \u2192 X (4) X \u2192 EN (4) BLEU zero (30) ACC zero (30)Transformer, Bilingual  \u2020 110M --20.2821.23--Transformer+MATT  \u2020 141M 20.7729.1516.0824.154.7139.40MATT+LALN+LALT  \u2020 173M 22.8629.4919.2524.535.4151.401Transformer 142M 18.50 \u00b10.08 26.85 \u00b10.13 18.37 \u00b10.39 25.70 \u00b10.05 4.59 \u00b10.2130.91 \u00b12.0521 + 1.5-entmax 142M 18.47 \u00b10.15 26.83 \u00b10.14 18.42 \u00b10.38 25.67 \u00b10.10 4.39 \u00b10.8630.51 \u00b15.6232 + (a) 142M 17.80 \u00b10.23 26.21 \u00b10.40 17.53 \u00b10.34 25.18 \u00b10.39 3.96 \u00b10.4328.95 \u00b12.6142 + (b) 142M 18.56 \u00b10.04 26.91 \u00b10.18 18.32 \u00b10.36 25.47 \u00b10.10 4.63 \u00b10.4831.05 \u00b15.9352 + (c) 142M 18.63"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "#Lang. #Train. #Zero-shot pairs sent. lang. pairs", "figure_data": "TED Talks11610M4WMT-20181447M24OPUS-100198 110M30"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Benchmark statistics: number of language pairs used for training, total number of training sentences, and number of language pairs for zero-shot evaluation.", "figure_data": ""}], "doi": "10.18653/v1/N19-1388"}