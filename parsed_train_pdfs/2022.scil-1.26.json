{"authors": "Alex Shilen; Colin Wilson", "pub_date": "", "title": "Learning Input Strictly Local Functions: Comparing Approaches with Catalan Adjectives", "abstract": "", "sections": [{"heading": "Introduction", "text": "Input strictly local (ISL) functions are a class of subregular transductions that have well-understood mathematical and computational properties and that are sufficiently expressive to account for a wide variety of attested morphological and phonological patterns (e.g., Chandlee, 2014;Chandlee, 2017;. In this study, we compared several approaches to learning ISL functions: the ISL function learning algorithm (ISLFLA; Chandlee, 2014; and the classic OSTIA learner to which it is related (Oncina et al., 1993); the Minimal Generalization Learner (MGL; Hayes, 2002, 2003); and a novel deep neural network model presented here (DNN-ISL).\nThe four models were evaluated on their ability to learn the mapping from feminine singular (fem.sg.) to masculine singular (masc.sg.) surface forms of Catalan adjectives (and, separately, from provided underlying representations to fem.sg. and masc.sg. surface forms). The mappings to masc.sg. forms in Catalan involve several phonological modifications at the right edge of the word (e.g., Mascar\u00f3, 1976), the empirical focus of our study. The relevant processes include obstruent devoicing and strengthening (e.g., [rOZ@] fem.sg. \u2192 [rOtS] masc.sg. 'red'), post-tonic n-Deletion (e.g., [san@] \u2192 [sa] 'healthy'), and cluster simplification (e.g., [blaNk@] \u2192 [blaN] 'white'). There are opaque, counterfeeding interactions among some of the processes (e.g., [f@kund@] \u2192 [f@kun] / *[f@ku] 'fertile'), consistent with the idea that the mappings are input-rather than output-determined (see . A small number of apparent lexical exceptions to the typical modification pattern (e.g., [blan@] \u2192 [blan] / *[bla] 'soft') are problematic for ISL learners that assume perfect homogeneity.\nOur main findings were that the DNN-ISL learner achieved high accuracy on the Catalan data, with MGL coming in a close second, while ISLFLA and OSTIA performed much worse -either failing to learn any mapping at all or predicting the correct output for less than 5% of held-out cases, even when lexical exceptions were removed from the data (see Table 1).", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Data", "text": "The FestCat project (Bonafonte et al., 2008) provides broad transcriptions for more than 53,000 adjectival surface forms in two major dialects of Catalan. We considered the Central Catalan forms and restricted our data to the nearly 6,500 lemmas that are also attested in a subtitle lexicon (Boada et al., 2020). While our main focus was on learning, we also developed a hand-written ISL transducer for the mapping to masc.sg. forms that is highly accurate (> 98% correct), along with custom code to derive plausible underlying representations from masc.sg. \u223c fem.sg. pairs.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Models", "text": "For the purposes of this abstract, we assume familiarity with ISLFLA, OSTIA, and MGL. We verified that the implementation of MGL learns only ISL phonological rules -rules conditioned on local phonological context in the result of a morphological operation such as affixation or truncation -a connection that has not previously been made in the literature.\nThe deep neural network model proposed here (DNN-ISL) also applies morphological operations followed by phonological modifications, the latter being implemented with weighted constraints rather than rules. A phonological constraint as learned by DNN-ISL is defined by: a three-segment featural pattern specifying the input context to which the constraint applies; a preference for one type of modification applied to the center segment of the context (i.e., deletion, epenthesis before/after, or feature change); target output features in the case of epenthesis or change; and a real-valued strength.\nEach constraint computes the degree to which its context matches every three-segment window in the input (i.e., it applies a novel feature based convolution operation to the input) and imposes its preferred modification in proportion to the degree of match and its strength. These preferences are summed over constraints for each input position and applied to the positions independently to derive the phonological output. The parameters of the constraints are straightforwardly interpretable and visualizable as real-valued phonological feature coefficients, modification-type logits, and strengths. The model is fully differentiable and was trained with the Adagrad optimizer on small mini-batches for 20 epochs.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "We evaluated all four models on the same training/validation/testing data, as summarized in Table 1. ISLFLA and OSTIA were unable to learn accurate mappings except when the fem.sg. and masc.sg. forms were artificially trimmed to their final VC * (V) sequences -a strong, languagespecific bias to attend to changes at the end of the word that the other models did not require. Results for larger training splits, and for mapping from URs to SRs, were similar. The errors made by DNN-ISL mostly involved underapplication of deletion (e.g., *[blaNk]).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Contributions & future directions", "text": "In summary, we evaluated four learning models on an ISL phonological mapping (with a small number of exceptions) found in a large, realistic body of natural language data. The models that have proofs of learnability and efficiency, ISLFLA and OSTIA, performed much worse than models that currently lack such theoretical guarantees but share the inductive bias for ISL patterns. The results highlight the need for further empirical and formal study of highperforming subsymbolic models such as DNN-ISL, and extension of our model to output-based patterns and learning of underlying representations. We plan to release our processed data, hand-written ISL transducer, and model implementations.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "Thanks to Coleman Haley and Marina Bedny for helpful discussion of this research, which was supported by NSF grant BCS-1941593 to CW.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Modeling English past tense intuitions with minimal generalization", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Adam Albright; Bruce Hayes"}, {"title": "Rules vs. analogy in English past tenses: A computational/experimental study", "journal": "Cognition", "year": "2003", "authors": "Adam Albright; Bruce Hayes"}, {"title": "SUBTLEX-CAT: Subtitle word frequencies and contextual diversity for Catalan", "journal": "", "year": "2020", "authors": "Roger Boada; Marc Guasch; Juan Haro; Josep Demestre; Pilar Ferr\u00e9"}, {"title": "Corpus and voices for Catalan speech synthesis", "journal": "", "year": "2008", "authors": "Antonio Bonafonte; Jordi Adell; Ignasi Esquerra; Silvia Gallego; Asunci\u00f3n Moreno; Javier P\u00e9rez"}, {"title": "Strictly Local Phonological Processes", "journal": "", "year": "2014", "authors": "Jane Chandlee"}, {"title": "Computational locality in morphological maps", "journal": "Morphology", "year": "2017", "authors": "Jane Chandlee"}, {"title": "Learning strictly local subsequential functions", "journal": "Transactions of the Association for Computational Linguistics", "year": "2014", "authors": "Jane Chandlee; R\u00e9mi Eyraud; Jeffrey Heinz"}, {"title": "Strict locality and phonological maps", "journal": "Linguistic Inquiry", "year": "2018", "authors": "Jane Chandlee; Jeffrey Heinz"}, {"title": "Input strictly local opaque maps", "journal": "Phonology", "year": "2018", "authors": "Jane Chandlee; Jeffrey Heinz; Adam Jardine"}, {"title": "Catalan Phonology and the Phonological Cycle", "journal": "", "year": "1976", "authors": "Joan Mascar\u00f3"}, {"title": "Learning subsequential transducers for pattern recognition interpretation tasks", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "1993", "authors": "Jos\u00e9 Oncina; Pedro Garc\u00eda; Enrique Vidal"}], "figures": [{"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Mean model accuracy for mapping from fem.sg. to masc.sg. in 10 independent runs with random splits (20% train, 10% validation, 70% test) of all Catalan adjective data or the 'regular' subset with exceptions omitted. ISLFLA failed to learn a transducer in each run, as indicated by '-', returning \"Insufficient data\" (seeChandlee  2014:117). The Baseline model simply subtracted the feminine suffix -@. *OSTIA and ISLFLA performed better when input and output forms were trimmed to their final VC * (V) sequences, as shown after the slash, but ISLFLA still failed to learn a transducer in 4/10 of the runs on the full data set. SDs were lower than .04 in most cells. Perfect performance on the training data, shown here for OSTIA only, is available to any model with sufficient memory.", "figure_data": "Data set Data split DNN-ISL MGL OSTIAISLFLABaselineAlltrain val test.95 .95 .95.79 .79 .801.0 / .84  *  -/ .89  *  (n=6) .02 / .79  *  -/ .83  *  (n=6) .02 / .79  *  -/ .83  *  (n=6).38 .39 .39Regulartrain val test.98 .98 .98.98 .97 .971.0 / 1.0  *  .04 / .98  *  .03 / .98  *-/1.0  *  -/ .96  *  -/ .96  *.41 .41 .41"}], "doi": ""}