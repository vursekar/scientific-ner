{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f59939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f22d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ed523758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "949fef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import ConfusionMatrixDisplay as cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ff420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46ae7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_cased\").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2348b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll = open(\"nick_merged.conll\",\"rt\", encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5cf6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (conll.split('\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72b38601",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "labels = []\n",
    "\n",
    "failed_lines = 0\n",
    "total_lines = 0\n",
    "for paragraph in text:\n",
    "    if len(paragraph) == 0:\n",
    "        continue\n",
    "    pg_text = []\n",
    "    pg_labels = []\n",
    "    for line in paragraph.split('\\n'):\n",
    "        total_lines += 1\n",
    "        try:\n",
    "            pg_text.append(line.split(' ')[0])\n",
    "            pg_labels.append(line.split(' ')[1])\n",
    "        except:\n",
    "            failed_lines += 1\n",
    "            continue\n",
    "    X.append(' '.join(pg_text).strip())\n",
    "    Y.append(pg_labels)\n",
    "    labels += pg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c02395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.sort(pd.Series(labels).unique())\n",
    "label_to_ind = {}\n",
    "ind_to_label = {}\n",
    "for i in range(len(unique_labels)):\n",
    "    label_to_ind[unique_labels[i]] = i\n",
    "    ind_to_label[i] = unique_labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6357f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ind_to_label, open(\"ind_to_label.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d072acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y)):\n",
    "    for j in range(len(Y[i])):\n",
    "        Y[i][j] = label_to_ind[Y[i][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d34dd855",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = []\n",
    "token_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "66900329",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    tokens = tokenizer(X[i].split(' '))['input_ids']\n",
    "    pg_tokens = [101]\n",
    "    pg_labels = [label_to_ind['O']]\n",
    "    for j in range(len(tokens)):\n",
    "        pg_tokens += tokens[j][1:-1]\n",
    "        pg_labels += [Y[i][j] for k in range(len(tokens[j]) - 2)]\n",
    "    pg_tokens += [102]\n",
    "    pg_labels += [label_to_ind['O']]\n",
    "    token_ids.append(pg_tokens)\n",
    "    token_labels.append(pg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5f47c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dbc5c769",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n"
     ]
    }
   ],
   "source": [
    "last_hidden_X = []\n",
    "labels = []\n",
    "for i in range(len(token_ids)):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    last_hidden_X.append(model(torch.tensor(token_ids[i][:512], dtype=torch.long).cuda().reshape(1, -1))['last_hidden_state'][0].detach())\n",
    "    labels += (token_labels[i][:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6c42d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_X = torch.cat(last_hidden_X).cuda()\n",
    "cat_Y = torch.tensor(labels, dtype = torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f2c4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(cat_X)*.7)\n",
    "train_X = cat_X[:train_ind]\n",
    "val_X = cat_X[train_ind:]\n",
    "train_Y = cat_Y[:train_ind]\n",
    "val_Y = cat_Y[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c5aaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_X, train_Y)\n",
    "val_data = TensorDataset(val_X, val_Y)\n",
    "train_loader = DataLoader(train_data, batch_size = 32, shuffle = True)\n",
    "val_loader = DataLoader(val_data, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "203990ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "fname = \"../data/test_data/anlp-sciner-test.txt\"\n",
    "\n",
    "text = open(fname,\"rt\", encoding=\"utf-8\").read().split('\\n')\n",
    "\n",
    "ind_converter = {}\n",
    "token_ids = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    ind_converter[i] = {}\n",
    "    tokens = tokenizer(text[i].split(' '))['input_ids']\n",
    "    pg_tokens = [101]\n",
    "    out_ind = 1\n",
    "    for j in range(len(tokens)):\n",
    "        ind_converter[i][j] = []\n",
    "        for k in range(len(tokens[j]) - 2):\n",
    "            ind_converter[i][j].append(out_ind)\n",
    "            out_ind += 1\n",
    "        pg_tokens += tokens[j][1:-1]\n",
    "    pg_tokens += [102]\n",
    "    token_ids.append(pg_tokens)\n",
    "\n",
    "last_hidden_X = []\n",
    "for i in range(len(token_ids)):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    if len(token_ids[i]) <= 512:\n",
    "        last_hidden_X.append(model(torch.tensor(token_ids[i][:512], dtype=torch.long).cuda().reshape(1, -1))['last_hidden_state'][0].detach())\n",
    "    else:\n",
    "        pred1 = model(torch.tensor(token_ids[i][:512], dtype=torch.long).cuda().reshape(1, -1))['last_hidden_state'][0].detach()\n",
    "        pred2 = model(torch.tensor(token_ids[i][-512:], dtype=torch.long).cuda().reshape(1, -1))['last_hidden_state'][0].detach()\n",
    "        pred = torch.cat([pred1, pred2[-(len(token_ids[i]) - 512):]])\n",
    "        last_hidden_X.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81f7d6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a4d06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = torch.cat(last_hidden_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36207666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84018, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f766bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_X, open(\"test_X.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44f3699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(last_hidden_X, open(\"last_hidden_X.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
