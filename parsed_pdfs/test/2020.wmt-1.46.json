{"authors": "Lovish Madaan; Soumya Sharma; Parag Singla", "pub_date": "", "title": "Transfer Learning for Related Languages: Submissions to the WMT20 Similar Language Translation Task", "abstract": "In this paper, we describe IIT Delhi's submissions to the WMT 2020 task on Similar Language Translation for four language directions: Hindi \u2194 Marathi and Spanish \u2194 Portuguese. We try out three different model settings for the translation task and select our primary and contrastive submissions on the basis of performance of these three models. For our best submissions, we fine-tune the mBART model (Liu et al., 2020) on the parallel data provided for the task. The pre-training is done using self-supervised objectives on a large amount of monolingual data for many languages. Overall, our models are ranked in the top four of all systems for the submitted language pairs, with first rank in Spanish \u2192 Portuguese.", "sections": [{"heading": "Introduction", "text": "Machine Translation (MT) is currently tackled using rule-based methods (RBMT) (Charoenpornsawat et al., 2002), phrase-based statistical methods (SMT) (Koehn et al., 2003) and neural methods (NMT) (Cho et al., 2014;Sutskever et al., 2014;Bahdanau et al., 2015;Vaswani et al., 2017).\nNMT has achieved high translation quality for several language pairs (Bojar et al., 2018;Barrault et al., 2019), but this level of performance usually requires large amounts of aligned data in the order of millions of sentence pairs. For low and medium resource languages, SMT performs better than NMT (Koehn and Knowles, 2017;Sennrich and Zhang, 2019). SMT also shows better performance when there is a domain mismatch between the train and test datasets, which is typical of low and medium resource language pairs. In these settings, NMT performance can be boosted by leveraging additional monolingual data to enforce various types of constraints or increasing the training data using back-translation. These methods can be particularly helpful if the source and target languages in MT are closely related and share language structure and alphabet. Recently, pre-training methods for sequence-to-sequence (seq2seq) models have been introduced like MASS (Song et al., 2019a), XLM (Conneau and Lample, 2019), BART (Lewis et al., 2019), and mBART (Liu et al., 2020). These methods show significant gains in downstream tasks like NMT, summarization, natural language inference (NLI), etc. In this paper, we focus on the transfer learning capabilities in NMT for the task of translation between related languages where parallel data is scarce.\nIIT Delhi participated in the WMT 2020 Shared task on Similar Language Translation for four language directions: Hindi (hi) \u2194 Marathi (mr) and Spanish (es) \u2194 Portuguese (pt). The first language pair is low resource and second is medium resource in terms of the parallel data available for the task. Refer to Table 2 for the classification.\nWe fine-tuned the pre-trained mBART model (Liu et al., 2020) on the parallel data provided for the task. mBART gives better performance than SMT models even when the parallel data is very limited. mBART is pre-trained on 25 languages, which contain Hindi and Spanish, but not Marathi and Portuguese. mBART is able to leverage transfer learning capabilities even for those languages that are originally not present during the pre-training phase. The fine-tuned mBART architecture forms our best submissions for both language pairs: hi \u2194 mr and es \u2194 pt. The rankings obtained by us in each of the language directions are listed in Table 1 The results and analysis are detailed in Section 5. We finally conclude in Section 6.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Background", "text": "SMT is tackled by building a phrase table from the aligned parallel data. The target side translation is then generated by matching the most appropriate phrases in the source sentence conditioned on the target side language model along with a reordering model (Koehn et al., 2003).\nNMT is modeled using Encoder-Decoder models (Cho et al., 2014;Sutskever et al., 2014;Bahdanau et al., 2015), with the Transformer model (Vaswani et al., 2017) achieving state-of-the-art on many MT problems. But these models' reliance on large aligned parallel data for the source and target languages makes them unsuitable for low/medium resource language pairs (Koehn and Knowles, 2017). Some of the previous works in these settings to improve NMT performance are described below:", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Multilingual NMT", "text": "Instead of using only two languages (source and target) for training an NMT model, using multiple languages has been shown to help in low resource scenarios. For example, it might be the case that a certain pair of languages have very little parallel data between them, but there exists a third language with abundant parallel data with the original two languages. This third language acts as a pivot and helps in improving NMT between the two languages (Aharoni et al., 2019;Gu et al., 2018;Liu et al., 2020;Zhang et al., 2020).", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Back-Translation", "text": "Back-Translation Hoang et al., 2018) increases the amount of training data by using monolingual corpus along with partially-trained NMT models on the limited parallel data. Pseudo-parallel corpus for each direction is first obtained by generating the translations of the monolingual data for each language using the partially-trained MT models on the limited parallel data. Using these pseudoparallel corpora, the partially-trained NMT models are then trained further for some number of steps. In this way, millions of pseudo-parallel sentence pairs can be generated to improve NMT models because of the abundance of monolingual data. Another version of using back-translation is the copying mechanism. Currey et al. (2017) proposes to copy the target side monolingual data on the source side to create additional data without modifying the training regimen for NMT. This helps the model to generate fluent translations.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Pre-trained Language Models", "text": "For NMT, the first step is the random initialization of model weights in both the encoder and decoder. Instead of random initialization, NMT models can be initialized by pre-training parts of the model (Conneau and Lample, 2019;Edunov et al., 2019), or pre-training the complete seq2seq model (Ramachandran et al., 2017;Song et al., 2019b;Liu et al., 2020). These pre-training methods leverage different kinds of masking techniques and the pretraining objective is to predict these masked tokens, similar to BERT (Devlin et al., 2019). Denoising auto-encoding can also be used where a sentence is corrupted by various noising techniques and the pre-training objective is to generate the original uncorrupted sentence as in BART (Lewis et al., 2019) and mBART (Liu et al., 2020).", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Incorporating Linguistic Information in NMT", "text": "There also have been works to improve low/medium resource NMT by adding linguistic information either using data augmentation (Currey and Heafield, 2019), subword embedding augmentation  , or architectural changes (Eriguchi et al., 2017). This helps the model to not only learn the alignment between source and target language spaces, but also syntax structure like dependency parse, part of speech, etc. This helps in making the target side translations more fluent and conforming to the structure of the language. We do not explore this direction in this paper.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "System Overview", "text": "We experimented with three different settings for hi \u2194 mr as listed below.\nSMT This phrase-based system leverages both monolingual and parallel data provided for the task. We use Moses (Koehn et al., 2007) for training the SMT systems.\nNMT (Transformer) For this, we used the standard Transformer large architecture from Vaswani et al. (2017) for training on the parallel data provided for the task.\nNMT (mBART) mBART (Liu et al., 2020) is a large Transformer pre-trained on monolingual data for 25 languages. The pre-training objective for mBART is seq2seq de-noising for natural text as in BART (Lewis et al., 2019). mBART provides a general-purpose pre-trained Transformer for any downstream task. It has been shown to give significant improvements over the random initialization for NMT and is the current state-of-the-art for many low resource language pairs. Implementation Details mBART uses a shared subword vocabulary of 250K tokens for all the 25 languages present in the pre-training. We use the same vocabulary for Marathi and Portuguese also, even though they were not used during the pre-training phase. Marathi shares its subword vocabulary with languages like Hindi and Nepali in mBART, and Portuguese shares with Spanish, Italian and other European languages present in mBART. The percentage of unknown tokens [UNK] in Marathi and Portuguese parallel datasets is less than 0.003% when using the shared mBART vocabulary.\nAdditionally, the mBART architecture requires language specific token at the end of each input sequence to provide the language specific context for the decoder. Since Marathi and Portuguese were not present during the pre-training phase, we use the token corresponding to the second most related language present in mBART pre-training for specifying the context at the time of decoding in each case. For Marathi, we used the Nepali language token and for Portuguese, we used the Italian language token. We could not use Spanish language token for Portuguese because we are doing translations to and from Spanish.   ", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We use hi \u2194 mr and es \u2194 pt language pairs for our experiments.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets & Preprocessing", "text": "Because of the constrained nature of the shared task, we only use the parallel data provided for this task. We removed the empty instances for both language pairs (< 2000 instances). For es \u2194 pt, we do not use 'WikiTitles v2' part of the parallel data for training because of very short sentences in the dataset. The cleaned parallel dataset statistics are provided in Table 2.\nPreprocessing We use sentence piece tokenization (Kudo and Richardson, 2018) for generating the source and target sequences for the NMT architectures. For the standard Transformer, we train a sentence piece model using 40K subword tokens for hi \u2194 mr. For mBART, we use Liu et al. (2020)'s pre-trained 1 sentence piece model comprising of 250K subword tokens as the vocabulary.\nFor the SMT model on hi \u2194 mr, we also use the monolingual data provided for this task. We extract 5 Million monolingual sentences each for Hindi and Marathi after deduplication and use this set for training the language models. We use Moses (Koehn et al., 2007) for all tokenization / detokenization scripts.   Lample et al. (2018). We used Moses (Koehn et al., 2007) and Giza++ with standard settings to train the SMT model in both directions.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "NMT (Transformer)", "text": "We use the large Transformer from Vaswani et al. (2017) with 8 encoder and decoder layers and replicate all the parameters from . The number of parameters in the model are approximately 248 Million and it takes \u223c26 hours on 4 Nvidia V100 (32 GB) GPUs.\nNMT (mBART) For this, we use 12 Transformer encoder and decoder layers, with total number of model parameters \u223c611 Million. We use the pretrained mBART for initializing the model weights.\nWe follow the recommendations of Liu et al. (2020) for the hyperparameter settings. We stop the training after 25K gradient updates for the model. These updates take \u223c35 hours on 4 Nvidia V100 (32 GB) GPUs.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Evaluation", "text": "We use case-insensitive BLEU scores (Papineni et al., 2002) calculated using sacreBLEU 2 (Post, 2018). These scores are calculated on the validation set to decide our primary and contrastive submissions. For evaluating performance on the test set, the organizers use BLEU, TER (Snover et al., 2006), and RIBES (Isozaki et al., 2010).", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Results and Analysis", "text": "Results Table 3 shows our results on the test set for our primary and contrastive submissions. We observed the performance of our three model settings on the validation set, and we selected the mBART model as our primary submission and SMT model as the contrastive submission for hi \u2194 mr. Similarly, the mBART model forms our primary submission for es \u2194 pt. Table 4 lists our final results on this shared task. We also list the BLEU scores for the submission that got first rank in each of the language directions. Since the test sets were hidden at the time of submission, we do not report our numbers on the standard Transformer architecture.\nAnalysis Even though Marathi and Portuguese are not present during the pre-training phase of mBART, fine-tuning on these languages provides significant boosts over SMT and standard Transformer. This shows that some level of language independent multilingual embeddings are present in the pre-trained model weights which can be exploited for the transfer task.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Discussion and Conclusion", "text": "We have participated in the Similar Language Translation task on four language directions. We have shown that pre-trained models can help in low and medium resource NMT. Our best system uses the pre-trained mBART model (Liu et al., 2020) and fine-tunes on the parallel data provided for the specific translation task. Our results demonstrate that pre-training can help even when the language used for fine-tuning is not present during pre-training. One direction of future work is to add linguistic information during the pre-training phase to get more fluent translations. When this information is not available directly (especially for low resource languages), pre-training on a related high resource language with syntax information can help low resource languages also. by the DARPA Explainable Artificial Intelligence (XAI) Program with number N66001-17-2-4032, Visvesvaraya Young Faculty Fellowships by Govt. of India and IBM SUR awards. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of the funding agencies.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We thank the IIT Delhi HPC facility 3 for the computational resources. We are also thankful to Ganesh Ramakrishnan and Pawan Goyal for initial discussions on the project. Parag Singla is supported", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Massively multilingual neural machine translation", "journal": "Long and Short Papers", "year": "2019-06-02", "authors": "Roee Aharoni; Melvin Johnson; Orhan Firat"}, {"title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2015-05-07", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Findings of the 2019 conference on machine translation (WMT19)", "journal": "", "year": "2019-08-01", "authors": "Lo\u00efc Barrault; Ondrej Bojar; Marta R Costa-Juss\u00e0; Christian Federmann; Mark Fishel; Yvette Graham; Barry Haddow; Matthias Huck; Philipp Koehn; Shervin Malmasi; Christof Monz; Mathias M\u00fcller"}, {"title": "Findings of the 2018 conference on machine translation (WMT18)", "journal": "", "year": "2018-10-31", "authors": "Ondrej Bojar; Christian Federmann; Mark Fishel; Yvette Graham; Barry Haddow; Philipp Koehn; Christof Monz"}, {"title": "Improving translation quality of rule-based machine translation", "journal": "", "year": "2002", "authors": "Paisarn Charoenpornsawat; Virach Sornlertlamvanich; Thatsanee Charoenporn"}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "journal": "", "year": "2014-10-25", "authors": "Kyunghyun Cho; Bart Van Merrienboer; Dzmitry Bahdanau; Yoshua Bengio"}, {"title": "Crosslingual language model pretraining", "journal": "", "year": "2019-12-14", "authors": "Alexis Conneau; Guillaume Lample"}, {"title": "Copied monolingual data improves low-resource neural machine translation", "journal": "", "year": "2017-09-07", "authors": "Anna Currey; Antonio Valerio Miceli; Kenneth Barone;  Heafield"}, {"title": "Incorporating source syntax into transformer-based neural machine translation", "journal": "Association for Computational Linguistics", "year": "2019-08-01", "authors": "Anna Currey; Kenneth Heafield"}, {"title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019-06-02", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Pre-trained language model representations for language generation", "journal": "Association for Computational Linguistics", "year": "2019-06-02", "authors": "Sergey Edunov; Alexei Baevski; Michael Auli"}, {"title": "Understanding back-translation at scale", "journal": "", "year": "2018-10-31", "authors": "Sergey Edunov; Myle Ott; Michael Auli; David Grangier"}, {"title": "Learning to parse and translate improves neural machine translation", "journal": "Association for Computational Linguistics", "year": "2017-07-30", "authors": "Akiko Eriguchi; Yoshimasa Tsuruoka; Kyunghyun Cho"}, {"title": "Universal neural machine translation for extremely low resource languages", "journal": "Association for Computational Linguistics", "year": "2018-06-01", "authors": "Jiatao Gu; Hany Hassan; Jacob Devlin; O K Victor;  Li"}, {"title": "Iterative backtranslation for neural machine translation", "journal": "", "year": "2018-07-20", "authors": "Cong Duy Vu Hoang; Philipp Koehn; Gholamreza Haffari; Trevor Cohn"}, {"title": "Automatic evaluation of translation quality for distant language pairs", "journal": "ACL", "year": "2010-10", "authors": "Hideki Isozaki; Tsutomu Hirao; Kevin Duh; Katsuhito Sudoh; Hajime Tsukada"}, {"title": "Moses: Open source toolkit for statistical machine translation", "journal": "", "year": "2007", "authors": "Philipp Koehn; Hieu Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; Nicola Bertoldi; Brooke Cowan; Wade Shen; Christine Moran; Richard Zens"}, {"title": "Six challenges for neural machine translation", "journal": "", "year": "2017-08-04", "authors": "Philipp Koehn; Rebecca Knowles"}, {"title": "Statistical phrase-based translation", "journal": "", "year": "2003", "authors": "Philipp Koehn; Franz J Och; Daniel Marcu"}, {"title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "", "year": "2018-10-31", "authors": "Taku Kudo; John Richardson"}, {"title": "Phrase-based & neural unsupervised machine translation", "journal": "EMNLP", "year": "2018", "authors": "Guillaume Lample; Myle Ott; Alexis Conneau; Ludovic Denoyer; Marc'aurelio Ranzato"}, {"title": "BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "", "year": "1910", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"title": "Multilingual denoising pre-training for neural machine translation", "journal": "", "year": "2001", "authors": "Yinhan Liu; Jiatao Gu; Naman Goyal; Xian Li; Sergey Edunov; Marjan Ghazvininejad; Mike Lewis; Luke Zettlemoyer"}, {"title": "Scaling neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Myle Ott; Sergey Edunov; David Grangier; Michael Auli"}, {"title": "Bleu: a method for automatic evaluation of machine translation", "journal": "ACL", "year": "2002-07-06", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018-10-31", "authors": "Matt Post"}, {"title": "Unsupervised pretraining for sequence to sequence learning", "journal": "Association for Computational Linguistics", "year": "2017-09-09", "authors": "Prajit Ramachandran; Peter J Liu; V Quoc;  Le"}, {"title": "Linguistic input features improve neural machine translation", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Rico Sennrich; Barry Haddow"}, {"title": "Improving neural machine translation models with monolingual data", "journal": "", "year": "2016-08-07", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Revisiting lowresource neural machine translation: A case study", "journal": "Long Papers", "year": "2019-07-28", "authors": "Rico Sennrich; Biao Zhang"}, {"title": "A study of translation edit rate with targeted human annotation", "journal": "", "year": "2006", "authors": "Matthew Snover; Bonnie Dorr; Richard Schwartz; Linnea Micciulla; John Makhoul"}, {"title": "Proceedings of Association for Machine Translation in the Americas", "journal": "", "year": "2006", "authors": ""}, {"title": "MASS: masked sequence to sequence pre-training for language generation", "journal": "PMLR", "year": "2019-06", "authors": "Kaitao Song; Xu Tan; Tao Qin; Jianfeng Lu; Tie-Yan Liu"}, {"title": "MASS: masked sequence to sequence pre-training for language generation", "journal": "PMLR", "year": "2019-06", "authors": "Kaitao Song; Xu Tan; Tao Qin; Jianfeng Lu; Tie-Yan Liu"}, {"title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014-12-08", "authors": "Ilya Sutskever; Oriol Vinyals; V Quoc;  Le"}, {"title": "Attention is all you need", "journal": "", "year": "2017-12-09", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"title": "Improving massively multilingual neural machine translation and zero-shot translation. CoRR, abs", "journal": "", "year": "2004", "authors": "Biao Zhang; Philip Williams; Ivan Titov; Rico Sennrich"}], "figures": [{"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Dataset statistics. First is low resource pair (# train < 1 Million) and second is medium resource (1 Million < # train < 10 Million).", "figure_data": "Modelhi -mr\u2190\u2192SMT18.74 14.91mBART 24.53 15.14"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "BLEU scores on Hindi \u2194 Marathi on the test set for our primary and contrastive submissions.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "IIT Delhi (ours) 24.53 15.14 32.84 32.69 Rank 1 24.53 18.26 33.82 32.69", "figure_data": "Submissionhi -mres -pt\u2190\u2192\u2190\u2192"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Hindi -Marathi and Spanish -Portuguese BLEU scores on the test dataset of the Similar Language Translation Task. Our submission scores are bolded when they match the first ranked submission.", "figure_data": "4.2 Model Architectures & TrainingSMT We generate a phrase table for the SMTmodel using the code provided by"}], "doi": "10.18653/v1/n19-1388"}