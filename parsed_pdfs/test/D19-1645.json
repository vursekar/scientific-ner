{"authors": "Kosuke Akimoto; Takuya Hiraoka; Kunihiko Sadamasa; Mathias Niepert", "pub_date": "", "title": "Cross-Sentence N -ary Relation Extraction using Lower-Arity Universal Schemas", "abstract": "Most existing relation extraction approaches exclusively target binary relations, and n-ary relation extraction is relatively unexplored. Current state-of-the-art n-ary relation extraction method is based on a supervised learning approach and, therefore, may suffer from the lack of sufficient relation labels. In this paper, we propose a novel approach to cross-sentence n-ary relation extraction based on universal schemas. To alleviate the sparsity problem and to leverage inherent decomposability of n-ary relations, we propose to learn relation representations of lower-arity facts that result from decomposing higher-arity facts. The proposed method computes a score of a new nary fact by aggregating scores of its decomposed lower-arity facts. We conduct experiments with datasets for ternary relation extraction and empirically show that our method improves the n-ary relation extraction performance compared to previous methods.", "sections": [{"heading": "Introduction", "text": "Relation extraction is a core natural language processing task which is concerned with the extraction of relations between entities from text. It has numerous applications ranging from question answering (Xu et al., 2016) to automated knowledge base construction (Dong et al., 2014).\nWhile the vast majority of existing research focuses on extracting binary relations, there exists only few recent approaches to extract n-ary relations, that is, relations among n \u2265 2 entities (Li et al., 2015;Ernst et al., 2018). In n-ary relation extraction, relation mentions tend to span multiple sentences more frequently as n increases. Thus, Peng et al. (2017) recently extended the problem to cross-sentence n-ary relation extraction in which n-ary relations are extracted from multiple sentences. As a motivating example, consider the following text from Wikipedia: \"Revis started off the 2009 season matched up against some of football's best wide receivers. In Week 1, he helped limit Houston Texans Pro-bowler Andre Johnson to four receptions for 35 yards.\" In this example, two sentences collectively describes that Andre Johnson is a player of the football team the Texans during 2009 season, and thus we need cross-sentence information to correctly extract this ternary interaction among the three entities, i.e. Player(Andre Johnson, Texans, 2009 season).\nPrevious methods (Peng et al., 2017;Song et al., 2018) capture cross-sentence n-ary relation mentions by representing texts with a document graph which consists of both intra-and cross-sentence links between words. With this graphical representation, they applied graph neural networks to predict ternary relations in the medical domain. However, these methods train the neural networks in a supervised manner using distant supervision (Mintz et al., 2009) and, therefore, may suffer from the lack of sufficient positive labels when a well-populated knowledge base is not available.\nOn the other hand, for binary relation extraction, the problem of insufficient positive labels can be mitigated with universal schemas . In a universal schema approach, textual representations (surface patterns) of entities and their relations are encoded into the same vector space as the canonical knowledge base relations. Thus, semantically similar surface patterns can share information of relation labels in a semisupervised manner. This reduces the amount of required labeled training data. Applying the universal schema approach to n-ary (n > 2) relation extraction is, however, not straight-forward due to the sparsity of higher-order relation mentions among a specific set of n > 2 entities. 1 This is be-cause the universal schema approach  and its extensions (Toutanova et al., 2015;Verga et al., 2016Verga et al., , 2017 utilize co-occurring patterns of relation types between specific pair of entities. Also, prior work has only addressed binary relations, and it is not trivial to define surface patterns among n > 2 entities and to encode these patterns into a vector representation.\nTo mitigate the aforementioned sparsity problem and utilize existing encoders for binary and unary surface patterns, we propose to train universal schema models on more dense lower-arity (unary and binary) facts instead of original sparse n-ary facts. Since most n-ary relations can be decomposed into a set of k-ary relations (k = 1, 2) which are implied by the n-ary relation, 2 we can easily acquire lower-arity facts by decomposing n-ary facts. Our model learns representations of these lower-arity relations using the universal schema framework, and predicts new n-ary facts by aggregating scores of lower-arity facts.\nTo evaluate the proposed method, we create new cross-sentence n-ary relation extraction datasets with multiple ternary relations. 3 The new datasets contain more entity tuples with known relational facts appeared in a knowledge base than the existing dataset (Peng et al., 2017), and, therefore, these datasets can be used to more effectively evaluate methods which predict relation labels for each individual entity tuple. We show empirically that by jointly training lower-arity models and an nary score aggregation model, the proposed method improves the performance of n-ary relation extraction. To the best of our knowledge, this is the first attempt to apply universal schemas to n-ary relation extraction, taking advantage of the compositionality of higher-arity facts.", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "Task Definition and Notation", "text": "The cross-sentence n-ary relation extraction task (Peng et al., 2017) is defined as follows. Let E be a set of entities, R KB be a set of relation types of an external knowledge base KB, and O KB = { r, (e 1 , ..., e n ) : r(e 1 , ..., e n ) \u2208 KB, r \u2208 2 For example, the ternary relation AwardedFor(director, movie, award) can be decomposed into the binary relations DirectorOf(director, movie) and WonAward(director, award). Note that a similar idea is introduced in (Ernst et al., 2018) as partial facts or partial patterns.\n3 Our codes and datasets are available at https://github.com/aurtg/ nary-relation-extraction-decomposed.  R KB , e i \u2208 E} be the set of facts in KB. We collect a set of candidate entity tuples among which KB relation r \u2208 R KB possibly holds. 4 Here, all entities in each candidate tuple (e 1 , ..., e n ) are mentioned in the same text section T in a given set of documents. We define a set of these entity mentions as O text = { T, (e 1 , ..., e n ) :\ne i \u2208 E is mentioned in T }.\nHere, text section T is a (short) span in a document which can describes relational facts among entities. In the cross-sentence n-ary relation extraction task, text section T can contain multiple sentences. In this paper, following (Peng et al., 2017), we define M consecutive sentences (M \u2265 1) which contain n target entities as a text section in the crosssentence n-ary relation extraction task. We use the term \"relation\" to refer to both relations r \u2208 R KB and sections T .\nThe goal of the cross-sentence n-ary relation extraction task is to predict new facts r, (e 1 , ..., e n ) /\n\u2208 O KB for relation r \u2208 R KB given O = O KB \u222a O text , where n \u2265 2.\n3 Proposed Method", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Lower-Arity Facts", "text": "To alleviate the sparsity problem of facts among n entities (n > 2) and to utilize well-studied encoders for binary and unary surface patterns, we decompose a set of original n-ary facts, O, into a set of unary facts O 1 and a set of binary facts O 2 (Figure 1).\nUnary Facts: Given an n-ary fact r, (e 1 , ..., e k ) \u2208 O, we decompose it into a set of n unary facts { r (k) , e k : k = 1, ..., n}, where r (k) is a tentative unary relation w.r.t. the k-th argument of the original relation r. If r is a KB relation, we define unary relation r (k) as a new canonicalized relation. If r is section T , we define unary relation r (k) as a tuple r (k) = (T, pos(e k )), where pos(e k ) is a set of word position indices of entity e k in section T (Figure 2). We denote a set of all decomposed unary facts by O 1 . Intuitively, these unary relations represent semantic roles or types of corresponding arguments of the original relation r .\nBinary Facts: Given an n-ary fact r, (e 1 , ..., e k ) \u2208 O, we decompose it into a set of n(n \u2212 1) binary facts { r (k,l) , (e k , e l ) : k, l = 1, ..., n, k = l}, where r (k,l) is a tentative binary relation between the k-th and l-th argument of the original relation r. If r is a KB relation, we define binary relation r (k,l) as a new canonicalized relation. If r is a section T , we represent it by the shortest path between e k and e l on the document graph (Quirk and Poon, 2017) of T (Figure 2), and denote it by path(T ; e k , e l ). We denote the set of all decomposed binary facts by O 2 .", "n_publication_ref": 5, "n_figure_ref": 3}, {"heading": "Lower-Arity Relation Representations", "text": "We learn a vector representation v(r) \u2208 R dr for each unary or binary relation in O 1 or O 2 . For r (k) or r (k,l) derived from a KB relation, we represent it by a trainable parameter vector. On the other hand, for the one derived from a textual relation, we use the following encoders to compute its representations.\nUnary encoder: For an unary textual relation r (k) = (T, pos(e k )), we represent each section T by a sequence of word vectors and use a bidirectional LSTM (Bi-LSTM) (Schuster and Paliwal, 1997) to compute a hidden representation h l \u2208 R dr at each word position l. Following recent works He et al., 2018;Lee et al., 2017), we aggregate h l within a phrase of entity e k to compute v(T (k) ). We use elementwise mean as aggregation function:\nv(r (k) ) = mean({h l : l \u2208 pos(e k )}). (1)\nBinary encoder: For a binary textual relation r (k,l) = path(T ; e k , e l ), we represent each token (word or edge label) in path(T ; e k , e l ) by an embedding vector (Toutanova et al., 2015;Verga et al., 2016). We use a Bi-LSTM to compute a hidden representation h l \u2208 R dr at each token position l, and max-pool along the path to compute the relation representation: v(T (k,l) ) = max({h l : l = 1, ..., L}). (2)", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Learning Relation Representations", "text": "We follow Verga et al. (2017) to train relation representations ( \u00a73.2). We define a score \u03b8 r,p for each lower-arity fact r, p \u2208 O 1 \u222a O 2 , and minimize the following loss (3) for each arity i = 1, 2.\nHere, placeholder p refers to either an entity (if r, p \u2208 O 1 ) or an entity tuple (if r, p \u2208 O 2 ), and we simply refer to both as entity tuple. The loss functions contrast a score of an original fact r, p + \u2208 O i and those of K sampled negative facts r, p \u2212 k / \u2208 O i . We sample negative facts by randomly replacing entity tuple p + in the original fact by different entity tuples\np \u2212 k . L i = E r,p + \u2208O i r,p \u2212 k / \u2208O i [\u2212 log( exp(\u03b8 r,p + ) exp(\u03b8 r,p + )+ k exp(\u03b8 r,p \u2212 k ) )].\n(3) The score of fact r, p is defined as \u03b8 r,p = v(r) T v(p; r). Entity tuple representations v(p; r) are computed with a weighted average of the representations {v(r ) : r \u2208 V (p)} as shown in ( 4) and ( 5) where a(r , r; V (p)) is the attention weight for each relation r \u2208\nV (p). 5 v(p; r) = r \u2208V (p) a(r , r; V (p))v(r ), a(r , r; V (p)) = exp(v(r ) T v(r)) r \u2208V (p) exp(v(r ) T v(r)) . (4\n) V (p) = {r : r, e k \u2208 O 1 } (if p = e k ) {r : r, (e k , e l ) \u2208 O 2 } (if p = (e k , e l ))\n. (5)", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Aggregating Lower-Arity Scores", "text": "To predict n-ary facts of KB relation r \u2208 R KB , we compute its score \u03b8 r,(e 1 ,...,en) by aggregating lower-arity scores as in (6), where w (\u2022) r is a positive scalar weight defined for each KB relation which sum to one: k w\n(k) r + k =l w (k,l) r = 1.\nWe can set all weights w (k) r and w (k,l) r to 1/n 2 , or train these weights to give higher scores to positive n-ary facts by minimizing additional loss function L n . Note that L n directly contrasts n-ary scores associated with KB relations r \u2208 R KB in a more supervised manner than both L 1 and L 2 . 6 (6)\nL n = E r,p + \u2208O KB r,p \u2212 / \u2208O KB [max(0, 1 \u2212 \u03b8 r,p + + \u03b8 r,p \u2212 )](7)\nThe overall loss function is now L = L 1 + L 2 + \u03b1L n . By changing \u03b1, we can balance the semisupervised effect of lower-arity universal schemas (L 1 , L 2 ) and that of the supervision with n-ary relation labels (L n ).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments 4.1 Dataset", "text": "The cross-sentence n-ary relation extraction dataset from Peng et al. (2017) contains only 59 distinct ternary KB facts including the train and test set. Since our proposed method and universal schemas baselines predict KB relations for each entity tuple instead of each surface pattern, the number of known facts of KB relations is crucial to reliably evaluate and compare these methods. Thus, we created two new n-ary cross-sentence relation extraction datasets (dubbed with Wiki-90k and WF-20k) that contain more known facts retrieved from public knowledge bases.\nTo create the Wiki-90k and WF-20k datasets, we used Wikidata and Freebase respectively as external knowledge bases. Since these knowledge bases store only binary relational facts, we defined multiple ternary relations by combining a few binary relations. 7,8 For both datasets, we collected paragraphs from the English Wikipedia, and used Stanford CoreNLP (Manning et al., 2014) to extract dependency and co-reference links. Entity mentions are detected using DBpedia Spotlight (Daiber et al., 2013). We followed (Peng et al., 2017) to extract co-occurring entity tuples and their surface patterns, that is, we selected tuples which occurred in a minimal span within at most M \u2264 3 consecutive sentences. Entity tuples without a known KB relation are subsampled, since the number of such tuples are too large. We randomly partitioned all entity tuples into train, development (dev), and test sets. (Song et al., 2018): The state-of-the-art crosssentence n-ary relation extraction method proposed by Song et al. (2018) represents each surface pattern by the concatenation of entity vectors from the last layer of a Graph State LSTM, a variant of a graph neural network. The concatenated vector is then fed into a classifier to predict the relation label. Since their method directly predicts a relation label for each surface pattern, it is more robust to the sparsity of surface patterns among a specific higher arity entity tuple. However, due to their purely supervised training objective, its performance may degrade if the number of available training labels is small.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Baselines", "text": "Universal schemas: We compared our method with semi-supervised methods based on universal schemas (Toutanova et al., 2015;Verga et al., 2017). In our experiments, we used the same encoder as (Song et al., 2018) to encode each surface pattern. 9 We tested two types of scoring functions, Model F and Model E, as in (Toutanova et al., 2015). 10,11 ", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Evaluation", "text": "We compared the methods in the held-out evaluation as in (Mintz et al., 2009) and report (weighted) mean average precision (MAP) . Unless otherwise noted, reported values are average values over six experiments, in which network parameters are randomly initialized. All reported p-values are calculated based on Wilcoxon rank sum test (Wilcoxon, 1945) with", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Method", "text": "Wiki-90k WF-20k average weighted average weighted Proposed 0.584 0.634 0.821 0.842 (Song et al., 2018) 0.471 0.536 0.639 0.680 (Toutanova et al., 2015) with Graph State LSTM (Song et al., 2018) (Verga et al., 2017) with Graph State LSTM (Song et al., 2018)   multiple-test adjustment using Holm's method (Holm, 1979).", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Results", "text": "Table 1 illustrates the performance of each method. 12 Compared to the baseline methods, our proposed method achieves higher weighted MAP for both datasets. Interestingly, Model F performs well in Verga et al. (2017) baseline, while it shows low performance in Toutanova et al. (2015) baseline.\nAblation Study: Table 2 illustrates the performance of various settings of our proposed method. U,B, and N stand for using the loss functions L 1 , L 2 , and \u03b1L n respectively. In the result, U+B performs significantly better (p < 0.005) than U and B, and this shows effectiveness of combining scores of both binary facts and unary facts. On the other hand, there was no significant difference between U+B+N and N (p > 0.9). Note that we used all positive labels in this experiment, that is, sufficient amount of positive labels are used for calculating the loss N.\nData efficiency: Furthermore, we also investigated the influence of the training data size (the number of positive labels) of our proposed method and baseline methods. 13 Here, \u03b1 = \u221e stands for optimizing L n instead of L 1 + L 2 + \u03b1L n . As shown in Figure 3, \u03b1 = 1 achieved higher performance than \u03b1 = \u221e, showing that introducing lower-arity semi-supervised loss (L 1 +L 2 ) improves the performance for dataset with few positive labels. On the other hand, the lower performance of \u03b1 = 0 compared to \u03b1 = 0.1, 1 suggests that information of higher-arity facts introduced from L n is benefitial for n-ary relation extraction.", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Conclusion and Future Works", "text": "We proposed a new method for cross-sentence nary relation extraction that decomposes sparse n- 12 For the proposed method, we set \u03b1 = 10. 13 In this experiment, we conducted four experiments per each setting and set K = 10.   ary facts into dense unary and binary facts. Experiments on two datasets with multiple ternary relations show that our proposed method can statistically significantly improve over previous works, which suggests the effectiveness of using unary and binary interaction among entities in surface patterns.\nHowever, as Fatemi et al. (2019) suggests, there exists cases in which reconstructing n-ary facts from decomposed binary facts induces false positives. Tackling this issue is one important future research direction.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "We thank all the EMNLP reviewers and Daniel Andrade for their valuable comments and suggestions to improve the paper.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Improving efficiency and accuracy in multilingual entity extraction", "journal": "ACM", "year": "2013", "authors": "Joachim Daiber; Max Jakob; Chris Hokamp; Pablo N Mendes"}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "journal": "", "year": "2014-08-24", "authors": "Evgeniy Xin Luna Dong; Geremy Gabrilovich; Wilko Heitz; Ni Horn; Kevin Lao; Thomas Murphy; Shaohua Strohmann; Wei Sun;  Zhang"}, {"title": "Highlife: Higher-arity fact harvesting", "journal": "", "year": "2018", "authors": "Patrick Ernst; Amy Siu; Gerhard Weikum"}, {"title": "Knowledge hypergraphs: Extending knowledge graphs beyond binary relations", "journal": "", "year": "2019", "authors": "Bahare Fatemi; Perouz Taslakian; David Vazquez; David Poole"}, {"title": "Jointly predicting predicates and arguments in neural semantic role labeling", "journal": "Short Papers", "year": "2018", "authors": "Luheng He; Kenton Lee; Omer Levy; Luke Zettlemoyer"}, {"title": "A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics", "journal": "", "year": "1979", "authors": "Sture Holm"}, {"title": "End-to-end neural coreference resolution", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Kenton Lee; Luheng He; Mike Lewis; Luke Zettlemoyer"}, {"title": "Improvement of n-ary relation extraction by adding lexical semantics to distant-supervision rule learning", "journal": "SCITEPRESS -Science and Technology Publications", "year": "2015", "authors": "Hong Li; Sebastian Krause; Feiyu Xu; Andrea Moro; Hans Uszkoreit; Roberto Navigli"}, {"title": "The stanford corenlp natural language processing toolkit", "journal": "", "year": "2014", "authors": "Christopher Manning; Mihai Surdeanu; John Bauer; Jenny Finkel; Steven Bethard; David Mcclosky"}, {"title": "Distant supervision for relation extraction without labeled data", "journal": "Association for Computational Linguistics", "year": "2009", "authors": "Mike Mintz; Steven Bills; Rion Snow; Daniel Jurafsky"}, {"title": "Cross-sentence n-ary relation extraction with graph lstms", "journal": "", "year": "2017", "authors": "Nanyun Peng; Hoifung Poon; Chris Quirk; Kristina Toutanova; Wen-Tau Yih"}, {"title": "Distant supervision for relation extraction beyond the sentence boundary", "journal": "", "year": "2017", "authors": "Chris Quirk; Hoifung Poon"}, {"title": "Relation extraction with matrix factorization and universal schemas", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Sebastian Riedel; Limin Yao; Andrew Mccallum; Benjamin M Marlin"}, {"title": "Bidirectional recurrent neural networks", "journal": "IEEE Transactions on Signal Processing", "year": "1997", "authors": "M Schuster; K K Paliwal"}, {"title": "N-ary relation extraction using graphstate lstm", "journal": "", "year": "2018", "authors": "Linfeng Song; Yue Zhang; Zhiguo Wang; Daniel Gildea"}, {"title": "Representing text for joint embedding of text and knowledge bases", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Kristina Toutanova; Danqi Chen; Patrick Pantel; Hoifung Poon; Pallavi Choudhury; Michael Gamon"}, {"title": "Multilingual relation extraction using compositional universal schema", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Patrick Verga; David Belanger; Emma Strubell; Benjamin Roth; Andrew Mccallum"}, {"title": "Generalizing to unseen entities and entity pairs with row-less universal schema", "journal": "Long Papers", "year": "2017", "authors": "Patrick Verga; Arvind Neelakantan; Andrew Mc-Callum"}, {"title": "Individual comparisons by ranking methods", "journal": "Biometrics bulletin", "year": "1945", "authors": "Frank Wilcoxon"}, {"title": "Question answering on freebase via relation extraction and textual evidence", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Kun Xu; Siva Reddy; Yansong Feng; Songfang Huang; Dongyan Zhao"}, {"title": "Universal schema for entity type prediction", "journal": "ACM", "year": "2013", "authors": "Limin Yao; Sebastian Riedel; Andrew Mccallum"}, {"title": "Graph convolution over pruned dependency trees improves relation extraction", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yuhao Zhang; Peng Qi; Christopher D Manning"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An overview of the proposed method.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: An example of decomposed textual facts.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "k,l) ,(e k ,e l ) .", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Weighted MAP on test data with missing labels (Wiki-90k).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Mean average precisions (MAPs) on test data.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Ablation study: mean average precisions (MAPs) on dev data (Wiki-90k).", "figure_data": ""}], "doi": "10.1145/2506182.2506198"}