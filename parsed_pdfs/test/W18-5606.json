{"authors": "Lina Rojas-Barahona; Bo-Hsiang Tseng; Yinpei Dai; Clare Mansfield; Osman Ramadan; Stefan Ultes; Michael Crawford; Milica Ga\u0161i\u0107", "pub_date": "", "title": "Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy", "abstract": "In recent years, we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks, such as similarity, entailment and sentiment analysis. Here we introduce a new task: understanding of mental health concepts derived from Cognitive Behavioural Therapy (CBT). We define a mental health ontology based on the CBT principles, annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations. Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform non-deep-learning models in this difficult task. This understanding module will be an essential component of a statistical dialogue system delivering therapy.", "sections": [{"heading": "Introduction", "text": "Promotion of mental well-being is at the core of the action plan on mental health 2013-2020 of the World Health Organisation (WHO) (World Health Organization, 2013) and of the European Pact on Mental Health and Well-being of the European Union (EU high-level conference: Together for Mental Health and Well-being, 2008). The biggest potential breakthrough in fighting mental illness would lie in finding tools for early detection and preventive intervention (Insel and Scholnick, 2006). The WHO action plan stresses the importance of health policies and programmes that not only meet the need of people affected by mental disorders but also protect mental well-being. The emphasis is on early evidence-based non-pharmacological intervention, avoiding institutionalisation and medicalisation. What is particularly important for successful intervention is the frequency with which the therapy can be accessed (Hansen et al., 2002). This gives automated systems a huge advantage over conventional therapies, as they can be used continuously with marginal extra cost. Health assistants that can deliver therapy, have gained great interest in recent years (Bickmore et al., 2005;Fitzpatrick et al., 2017). These systems however are largely based on hand-crafted rules. On the other hand, the main research effort in statistical approaches to conversational systems has focused on limited-domain information seeking dialogues (Schatzmann et al., 2006;Geist and Pietquin, 2011;Gasic and Young, 2014;Fatemi et al., 2016;Li et al., 2016;Williams et al., 2017).\nIn this paper we introduce a new task: understanding of mental health concepts derived from Cognitive Behavioural Therapy (CBT). We present an ontology that is formulated according to Cognitive Behavioural Therapy principles. We label a high quality mental health corpus, which exhibits targeted psychological phenomena. We use the whole unlabelled dataset to train distributed representations of words and sentences. We then investigate two approaches for classifying the user input according to the defined ontology. The first model involves a convolutional neural network (CNN) operating over distributed words representations. The second involves a gated recurrent network (GRU) operating over distributed representation of sentences. Our models perform significantly better than chance and for instances with a large number of data they reach the inter-annotator agreement. This understanding module will be an essential component of a statistical dialogue system delivering therapy.\nThe paper is organised as follows. In Section 2 we give a brief background of the statistical approach to dialogue modelling, focusing on dialogue ontology and natural language understanding. In Section 3 we review related work in the area of automated mental-health assistants. The sections that follow represent the main contribution of this work: a CBT ontology in Section 4, a labelled dataset in Section 5, and models for language understanding in Section 6. We present the results in Section 7 and our conclusion in Section 8.", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Background", "text": "A dialogue system can be treated as a trainable statistical model suitable for goal-oriented information seeking dialogues (Young, 2002). In these dialogues, the user has a clear goal that he or she is trying to achieve and this involves extracting particular information from a back-end database. A structured representation of the database, the ontology is a central element of a dialogue system. It defines the concepts that the dialogue system can understand and talk about. Another critical component is the natural language understanding unit, which takes textual user input and detects presence of the ontology concepts in the text.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Dialogue ontology", "text": "Statistical approaches to dialogue modelling have been applied to relatively simple domains. These systems interface databases of up to 1000 entities where each entity has up to 20 properties, i.e. slots (Cuay\u00e1huitl, 2009). There has been a significant amount of work in spoken language understanding focused on exploiting large knowledge graphs in order to improve coverage (T\u00fcr et al., 2012;Heck et al., 2013). Despite these efforts, little work has been done on mental health ontologies for supporting cognitive behavioural therapy on dialogue systems. Available medical ontologies follow a symptom-treatment categorisation and are not suitable for dialogue or natural language understanding (Bluhm, 2017;Hofmann, 2014;Wang et al., 2018).", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Natural language understanding", "text": "Within a dialogue system, a natural language understanding unit extracts meaning from user sentences. Both classification (Mairesse et al., 2009) and sequence-to-sequence (Yao et al., 2014;Mesnil et al., 2015) models have been applied to address this task.\nDeep learning architectures that exploit distributed word-vector representations have been successfully applied to different tasks in natural language understanding, such as semantic role labelling, semantic parsing, spoken language un-derstanding, sentiment analysis or dialogue belief tracking (Collobert et al., 2011;Kim, 2014;Kalchbrenner et al., 2014;Le and Mikolov, 2014a;Rojas Barahona et al., 2016;Mrk\u0161i\u0107 et al., 2017).\nIn this work we consider understanding of mental health concepts of as a classification task. To facilitate this process, we use distributed representations.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Related work", "text": "The aim of building an automated therapist has been around since the first time researchers attempted to build a dialogue system (Weizenbaum, 1966). Automated health advice systems built to date typically rely on expert coded rules and have limited conversational capabilities (Rojas-Barahona and Giorgino, 2009;Vardoulakis et al., 2012;Ring et al., 2013;Riccardi, 2014;DeVault et al., 2014;Ring et al., 2016). One particular system that we would like to highlight is an affectively aware virtual therapist (Ring et al., 2016). This system is based on Cognitive Behavioural Therapy and the system behaviour is scripted using VoiceXML. There is no language understanding: the agent simply asks questions and the user selects answers from a given list. The agent is however able to interpret hand gestures, posture shifts, and facial expressions. Another notable system (De-Vault et al., 2014) has a multi-modal perception unit which captures and analyses user behaviour for both behavioural understanding and interaction. The measurements contribute to the indicator analysis of affect, gesture, emotion and engagement. Again, no statistical language understanding takes place and the behaviour of the system is scripted. The system does not provide therapy to the user but is rather a tool that can support healthcare decisions (by human healthcare professionals).\nThe Stanford Woebot chat-bot proposed by (Fitzpatrick et al., 2017) is designed for delivering CBT to young adults with depression and anxiety. It has been shown that the interaction with this chat-bot can significantly reduce the symptoms of depression when compared to a group of people directed to a read a CBT manual. The conversational agent appears to be effective in engaging the users. However, the understanding component of Woebot has not been fully described. The dialogue decisions are based on decision trees. At each node, the user is expected to choose one of several predefined responses. Limited language understanding was in-troduced at specific points in the tree to determine routing to subsequent conversational nodes. Still, one of the main deficiencies reported by the trial participants in (Fitzpatrick et al., 2017) was the inability to converse naturally. Here we address this problem by performing statistical natural language understanding.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "CBT ontology", "text": "To define the ontology we draw from principles of Cognitive Behavioural Therapy (CBT). This is one of the best studied psychotherapeutic interventions, and the most widely used psychological treatment for mental disorders in Britain (Bhasi et al., 2013). There is evidence that CBT is more effective than other forms of psychotherapy (Tolin, 2010). Unlike other, longer-term, forms of therapy such as psychoanalysis, CBT can have a positive effect on the client within a few sessions. Also, due to it being highly structured, it is more easily amenable by computer interpretation. This is why we adopted CBT as the basis of our work.\nCognitive Behavioural Therapy is derived from Cognitive Therapy model theory (Beck, 1976;Beck et al., 1979), which postulates that our emotions and behaviour are influenced by the way we think and by how we make sense of the world. The idea is that, if the client changes the way he or she thinks about their problem, this will in turn change the way he or she feels, and behaves.\nA major underlying principle of CBT is the idea of cognitive distortions, and the value in challenging them. In CBT, clients are helped to test their assumptions and views of the world in order to check if they fit with reality. When clients learn that their perceptions and interpretations are distorted or unhelpful they then work on correcting them. Within the realm of cognitive distortion, CBT identifies a number of specific self-defeating thought processes, or thinking errors. There is a core of around 10 to 15 thinking errors, with their exact titles having some fluidity. A strong component of CBT is teaching clients to be able to recognize and identify the thinking errors themselves, and ultimately discard the negative thought processes and 're-think' their problems.\nWe consider the main analytical step in this therapy: an adequate decoding of these 'thinking error' concepts, and the identification of the key emotion(s) and the situational context of a particular problem. Therefore, our ontology consists of think-ing errors, emotions, and situations.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Thinking errors", "text": "Notwithstanding slight variations in number and terminology, the list of thinking errors is fairly well standardised in the CBT literature. We present one such list in Table 1. However, it is important to note that there is a fair degree of overlap between different thinking errors, for example, between Jumping to Negative Conclusions and Fortune Telling, or between Disqualifying the Positives and Mental Filtering. In addition, within the data used -and as is likely to be the case in any data of spontaneous expressions of psychological upset -a single problem can exhibit several thinking errors simultaneously. Thus, the situation is much more challenging than in simple information-seeking dialogues, where ontologies are typically clearly defined and there is no or very little overlap between concepts.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Emotions", "text": "In addition to thinking errors, we define a set of emotions. We mainly focus on negative emotions, relevant to people in psychological distress. In CBT, emotions tend to be divided into positive and negative, or helpful/healthy and unhelpful/ unhealthy emotions (Branch and Willson, 2010). The set of emotions for this work evolved over time in the early days of annotation. Although we initally agreed to focus on 'unhealthy' emotions, as defined by CBT, there seemed also to be a place for the 'healthy' emotion Grief/sadness. Overall, the list of emotions used was drawn from a number of sources, including CBT literature, the annotators' own knowledge of what they work with in psychological therapy, and the common emotions that were seen emerging from the data early on in the process. Note that more than one emotion might be expressed within an individual problem -for example Depression and Loneliness. The list of emotions is given in Table 2.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Situations", "text": "While our main emphasis was on thinking errors and emotions, we also defined a small set of situations. The list of situations again evolved during the early days of annotation, with a longer original list being reduced down, for simplicity. Again, it is possible for more than one situation (for example Work and Relationships) to apply to a single problem. The considered situations are given in Table 3.    ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "The corpus", "text": "The corpus consists of 500K written posts that users anonymously posted on the Koko platform 1 . This platform is based on the peer-to-peer therapy proposed by (Morris et al., 2015). In this set-up, a user anonymously posts their problem (referred to 1 https://itskoko.com/ as the problem) and is prompted to consider their most negative take on the problem (referred to as the negative take). Subsequently, peers post responses that attempt to offer a re-think and give a more positive angle on the problem. When first developed, this peer-to-peer framework was shown to be more efficacious than expressive writing, an intervention that is known to improve physical and emotional well-being (Morris et al., 2015). Since then, the app developed by Koko has collected a very large number of posts and associated responses. Initially, any first-time Koko user would be given a short introductory tutorial in the art of 're-thinking'/'re-framing' problems (based on CBT principles), before being able to use the platform. This however changed over time, as the age of the users decreased, and a different tutorial, emphasizing empathy and optimism, was used (less CBT-based than the 're-thinking'). Most of the data annotated in this study was drawn from the earlier phase. Figure 1 gives an annotated post example.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Annotation", "text": "A subset of posts was annotated by two psychological therapists using a web annotation tool that we developed. The annotation tool allowed annotators to have a quick view of the posts, showing up to 50 posts per page, to navigate through posts, to check pending posts and to annotate them by adding or removing thinking errors, emotions and situations. All annotations were stored in a MySQL database. Initially 1000 posts were analysed. These were used to define the ontology. Then 4035 posts were labelled with thinking errors, emotions and situations. It takes an experienced psychological therapist about one minute to annotate one post. Note that the same post can exhibit multiple thinking errors, emotions and situations, which makes the whole process more complex. We randomly selected 50 posts and calculated the inter-annotator agreement. The inter-annotator agreement was calculated using a contingency table for thinking error, emotion and situation, showing agreement and disagreement between the two annotators. Then, Cohen's kappa was calculated discounting the possibility that the agreement may happen by chance. The result is shown in  due to the unbounded number of thinking errors per post. In other words, the annotators typically have three or four thinking errors in common but one of them might have detected one or two more.\nStill, the agreement is much higher than chance, so we think that while challenging, it is possible to build a classifier for this task. The distributions of labelled posts with multiple sub-categories for three super-categories are shown in Figure 2 6 Deep learning model", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Distributed representations", "text": "The task of decoding thinking errors and emotions is closely related to the task of sentiment analysis. In sentiment analysis we are concerned with positive or negative sentiment expressed in a sentence. Detecting thinking errors or emotions could be perceived as detecting different kinds of negative sentiment. Distributed representations of words, sentences and documents have gained success in sentiment detection and similarity tasks (Le and Mikolov, 2014a;Maas et al., 2011;Kiros et al., 2015). A key advantage of these representations is that they can be obtained in an unsupervised manner, thus allowing exploitation of large amounts of unlabelled data. This is precisely what we have in our set-up, where only a small portion of our posts is labelled. We utilise GloVe (Pennington et al., 2014) word vectors, which have previously achieved competitive results in a similarity task. We train the word vectors on the whole dataset and then use a convolutional neural network (CNN) to extract features from posts where words are represented as vectors.\nWe also consider distributed representation of sentences. A particularly competitive model is the skip-thought model, which is obtained from an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage (Kiros et al., 2015). On similarity tasks it outperfoms the simpler doc2vec model (Le and Mikolov, 2014a). An approach that represents vectors by weighted averages of word vectors and then modifies them using PCA and SVD outperforms skipthought vectors (Arora et al., 2017). This method however does not do well on a sentiment analysis task due to down-weighting of words like \"not\". As these often appear in our corpus, we chose skipthought vectors for investigation here.\nThe skip-thought model allows a dense representation of the utterance. We train skip-thought vectors using the method described in (Kiros et al., 2015). The automatically generated post shown in Fig 3 demonstrates that skip-thought vectors can convey the sentiment well in accordance to context. We then train a gated recurrent unit (GRU) network using the skip-thoughts as input.\ni 'm so depressed . i 'm worthless . No one likes me i 'm try being nice but . No light at every point i 'm unpopular and i 'm a <NUM> year old potato . my most negative take is that i 'll never know how to be as socially as a quiet girl. i will stop talking to how fragile is and be any ways of normal people . Figure 3: An example of a generated post using skipthought vectors initialised with \"I'm so depressed\".", "n_publication_ref": 8, "n_figure_ref": 2}, {"heading": "Convolutional neural network model", "text": "The convolutional neural network (CNN) model is preferred over a recurrent neural network (RNN) model, because the posts are generally too long for an RNN to maintain memory over words. The convolutional neural network (CNN) used in this work is inspired by (Kim, 2014) and operates over pre-trained GloVe embeddings of dimensionality d. As shown in Fig 4, the network has two inputs, one for the problem and the other for the negative take. These are represented as two tensors. A convolutional operation involves a filter w \u2208 R ld which is applied to l words to produce the feature map. Then, a max-pooling operation is applied to produce two vectors: p for problem and n for negative take. The reason for this is that the negative take is usually a summary of the post, carrying stronger sentiment (see Figure 1). We use a gating mechanism to combine p and n as follows:\ng = \u03c3(W p p + W n n + b) (1) h = g p + (1 \u2212 g) n\n(2) Here, \u03c3 is the sigmoid function, W p , W n and W are weight matrices, b is a bias term, 1 is a vector of ones, is the element-wise product, and g is the output of the gating mechanism. The extracted feature h is then processed with a one-layer fullyconnected neural network (FNN) to perform binary classification. The model is illustrated in Fig 4. ", "n_publication_ref": 1, "n_figure_ref": 3}, {"heading": "Gated recurrent unit model", "text": "We use the gated recurrent unit (GRU) model to process skip-thought sentence vectors, for two reasons. First, most posts contain less than 5 sentences, so a recurrent neural network is more suitable than a convolutional neural network. Second, since our corpus only comprises very limited labelled data, a GRU should perform better than a long short-term memory (LSTM) network as it has less parameters.\nDenote each post as P = {s 1 , s 2 , ..., s t , ...}, where s t is the t th sentence in post P . First, we use an already trained GRU to extract skip-thought embeddings e t from the sentences s t . Then, taking the sequence of sentence vectors {e 1 , e 2 , ..., e t , ...} as input, another GRU is used as follows:\nz t = \u03c3(W z h t\u22121 + U z e t + b z ) (3) r t = \u03c3(W r h t\u22121 + U r e t + b r )(4)\nh t = tanh(W(r t h t\u22121 ) + Ue t + b h ) (5) h t = z t h t\u22121 + (1 \u2212 z t ) h t (6) W z , U z , W r , U r , W, U are recurrent weight ma- trices, b z , b r , b h are\nbias terms, is the elementwise dot product, and \u03c3 is the sigmoid function.\nFinally, the last hidden state h T is fed into a FNN with one hidden layer of the same size as input. The model is illustrated in Fig 5. ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Training set-up", "text": "We first train 100 and 300 dimensions for both GloVe embeddings and skip-thought embeddings using the same mechanism as in (Pennington et al., 2014;Kiros et al., 2015). In some posts the length of sentences is very large, so we bound the length at 50 words. We do not treat the problem separately from the negative take as the GRU will anyway put more importance on the information that comes last. We split the labelled data in a 8 : 1 : 1 ratio for training, validation and testing in a 10-fold cross validation for both GRU and CNN training. A distinct network is trained for each concept, i. e. one for thinking errors, one for emotions and one for situations. The hidden size of the FNN is 150.\nTo tackle the data bias problem, we utilise oversampling. Different ratios (1:1, 1:3, 1:5, 1:7) of positive and negative samples are explored.\nWe used filter windows of 2, 3, and 4 with 50 feature maps for the CNN model. For the GRU model, the hidden size is set at 150, so that both models have comparable number of parameters. Mini-batches of size 24 are used and gradients are clipped with maximum norm 5. We initialise the learning rate as 0.001 with a decay rate of 0.986 every 10 steps. The non-recurrent weights with a truncated normal distribution (0, 0.01), and the recurrent weights with orthogonal initialisation (Saxe et al., 2013). To overcome over-fitting, we employ dropout with rate 0.8 and l2-normalisation. Both models were trained with Adam algorithm and implemented in Tensorflow (Girija, 2016).", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Baselines", "text": "For rule-based models, we chose a chance classifier and a majority classifier, where all the posts are treated as positive examples for each class. In addition, we trained two non-deep-learning models, the logistic regression (LR) model and the Support Vector Machine (SVM). Both of them take the bag-of-words feature as input and implemented in sklearn (Pedregosa et al., 2011). For completeness, we also trained 100 and 300 dimensions PV-DM document embeddings (Le and Mikolov, 2014b) as the distributed representations of the posts using the gensim toolkit (\u0158eh\u016f\u0159ek and Sojka, 2010), and employ FNNs to do the classification, the hidden size is set as 800 to ensure parameters of all deep learning models comparable. All the baseline models are trained with the same set-up as described in section 6.4.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Analysis", "text": "Table 5 gives the average F1 scores and the average F1 scores weighted with the frequency of CBT labels for all models under the oversampling ratio 1:1. It shows that GloVe word vectors with CNN achieves the best performance both in 100 and 300 dimensions.  Table 6 shows the F1-measure of the compared models that detect thinking errors, emotions and situations under the 1 : 1 oversampling ratio. We only include the results of the best performing models, SVMs, CNNs and GRUs, due to limited space. The results show that both models outperform SVM-BOW in larger embedding dimensions. Although SVM-BOW is comparable to 100 dimensional GRU-Skip-thought in terms on average F1, in all other cases CNN-GloVe and GRU-Skipthought overshadow SVM-BOW. We also find that CNN-GloVe on average works better than GRU-Skip-thought, which is expected as the space of words is smaller in comparison to the space of sentences so the word vectors can be more accurately trained. While the CNN operating on 100 dimensional word vectors is comparable to the CNN operating on 300 dimensional word vectors, the GRU-Skip-thought tends to be worse on 100 dimensional skip-thoughts, suggesting that sentence vectors generally need to be of a higher dimension to represent the meaning more accurately than word vectors.\nTable 7 shows a more detailed analysis of the 300 dimensional CNN-GloVe performance, where both precision and recall are presented, indicating that oversampling mechanism can help overcome the data bias problem. To illustrate the capabilities of this model, we give samples of two posts and their predicted and true labels in Figure 6, which shows that our model discerns the classes reasonably well even in some difficult cases.  While oversampling is essential for both models, GRU-Skip-thought is less sensitive to lower oversampling ratios, suggesting that skip-thoughts can already capture sentiment on the sentence level. Therefore, including only a limited ratio of positive samples is sufficient to train the classifier. Instead, models using word vectors need more positive data to learn sentence sentiment features.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "We presented an ontology based on the principles of Cognitive Behavioural Therapy. We then annotated data that exhibits psychological problems and computed the inter-annotator agreement.\nWe found that classifying thinking errors is a difficult task as suggested by the low inter-annotator agreement. We trained GloVe word embeddings and skip-thought embeddings on 500K posts in an unsupervised fashion and generated distributed representations both of words and of sentences. We  then used the GloVe word vectors as input to a CNN and the skip-thought sentence vectors as input to a GRU. The results suggest that both models significantly outperform a chance classifier for all thinking errors, emotions and situations with CNN-GloVe on average achieving better results. Areas of future investigation include richer dis-tributed representations, or a fusion of distributed representations from word-level, sentence-level and document-level, to acquire more powerful semantic features. We also plan to extend the current ontology with its focus on thinking errors, emotions and situations to include a much lager number of concepts. The development of a statistical system delivering therapy will moreover require further research on other modules of a dialogue system.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "This work was funded by EPSRC project Natural speech Automated Utility for Mental health (NAUM), award reference EP/P017746/1. The authors would also like to thank anonymous reviewers for their valuable comments.\nThe code is available at https://github.com/YinpeiDai/NAUM", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "A simple but tough-to-beat baseline for sentence embedding", "journal": "", "year": "2017", "authors": "Y Arora; T Liang;  Ma"}, {"title": "Cognitive Therapy of Depression", "journal": "Guildford Press", "year": "1979", "authors": "A T Beck; J Rush; B Shaw; G Emery"}, {"title": "Second Round of the National Audit of Psychological Therapies for Anxiety and Depression", "journal": "NAPT", "year": "2013", "authors": "Charissa Bhasi; Rohanna Cawdron; Melissa Clapp; Jeremy Clarke; Mike Crawford; Lorna Farquharson; Elizabeth Hancock; Miranda Heneghan; Rachel Marsh; Lucy Palmer"}, {"title": "Establishing the computer-patient working alliance in automated health behavior change interventions", "journal": "Patient education and counseling", "year": "2005", "authors": "Timothy Bickmore; Amanda Gruber; Rosalind Picard"}, {"title": "The need for new ontologies in psychiatry", "journal": "Philosophical Explorations", "year": "2017", "authors": "Robyn Bluhm"}, {"title": "Cognitive Behavioural Therapy for Dummies", "journal": "Wiley", "year": "2010", "authors": "R Branch; R Willson"}, {"title": "Natural language processing (almost) from scratch", "journal": "Journal of Machine Learning Research", "year": "2011-08", "authors": "Ronan Collobert; Jason Weston; L\u00e9on Bottou; Michael Karlen; Koray Kavukcuoglu; Pavel Kuksa"}, {"title": "Hierarchical reinforcement learning for spoken dialogue systems", "journal": "", "year": "2009", "authors": "Heriberto Cuay\u00e1huitl"}, {"title": "Simsensei kiosk: A virtual human interviewer for healthcare decision support", "journal": "", "year": "2014", "authors": "D Devault;  Artstein;  Ben;  Dey;  Fast; K Gainer;  Georgila;  Gratch; M Hartholt;  Lhommet;  Lucas;  Marsella;  Morbini;  Nazarian; G Scherer;  Stratou;  Suri;  Traum;  Wood;  Xu; L-P Rizzo;  Morency"}, {"title": "level conference: Together for Mental Health and Well-being", "journal": "European Pact on Mental Health and Well-being", "year": "2008", "authors": ""}, {"title": "Policy networks with two-stage training for dialogue systems", "journal": "", "year": "2016", "authors": "Mehdi Fatemi; Layla El Asri; Hannes Schulz; Jing He; Kaheer Suleman"}, {"title": "Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (woebot): a randomized controlled trial", "journal": "JMIR mental health", "year": "2017", "authors": "Kathleen Kara Fitzpatrick; Alison Darcy; Molly Vierhile"}, {"title": "Gaussian processes for pomdp-based dialogue manager optimization", "journal": "IEEE/ACM Transactions on", "year": "2014", "authors": "M Gasic; S Young"}, {"title": "Managing Uncertainty within the KTD Framework", "journal": "", "year": "2011", "authors": "M Geist;  Pietquin"}, {"title": "Tensorflow: Largescale machine learning on heterogeneous distributed systems", "journal": "", "year": "2016", "authors": "Girija Sanjay Surendranath"}, {"title": "The psychotherapy dose-response effect and its implications for treatment delivery services", "journal": "Clinical Psychology: Science and Practice", "year": "2002", "authors": "Nathan B Hansen; Michael J Lambert; Evan M Forman"}, {"title": "Leveraging knowledge graphs for web-scale unsupervised semantic parsing", "journal": "", "year": "2013", "authors": "P Larry; Dilek Heck; G\u00f6khan Hakkani-T\u00fcr;  T\u00fcr"}, {"title": "Toward a cognitive-behavioral classification system for mental disorders", "journal": "Behavior Therapy", "year": "2014", "authors": "Stefan Hofmann"}, {"title": "Cure therapeutics and strategic prevention: raising the bar for mental health research", "journal": "Molecular Psychiatry", "year": "2006", "authors": "E M Tr Insel;  Scholnick"}, {"title": "A convolutional neural network for modelling sentences", "journal": "", "year": "2014", "authors": "Nal Kalchbrenner; Edward Grefenstette; Phil Blunsom"}, {"title": "Convolutional neural networks for sentence classification", "journal": "", "year": "2014", "authors": "Yoon Kim"}, {"title": "Skip-thought vectors. NIPS", "journal": "", "year": "2015", "authors": "R Kiros; Y Zhu; R Salakhutdinov; R S Zemel; A Torralba; R Urtasun; S Fidler"}, {"title": "Distributed representations of sentences and documents", "journal": "", "year": "2014", "authors": "Quoc Le; Tomas Mikolov"}, {"title": "Distributed representations of sentences and documents", "journal": "", "year": "2014", "authors": "Quoc Le; Tomas Mikolov"}, {"title": "Deep reinforcement learning for dialogue generation", "journal": "", "year": "2016", "authors": "Jiwei Li; Will Monroe; Alan Ritter; Dan Jurafsky"}, {"title": "Learning word vectors for sentiment analysis", "journal": "", "year": "2011", "authors": "L Andrew; Raymond E Maas;  Daly; T Peter; Dan Pham;  Huang; Y Andrew; Christopher Ng;  Potts"}, {"title": "Spoken language understanding from unaligned data using discriminative classification models", "journal": "", "year": "2009", "authors": "F Mairesse; M Ga\u0161i\u0107; F Jur\u010d\u00ed\u010dek; S Keizer; B Thomson; K Yu; S Young"}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "journal": "IEEE Transactions on Audio, Speech, and Language Processing", "year": "2015", "authors": "Gr\u00e9goire Mesnil; Yann Dauphin; Kaisheng Yao; Yoshua Bengio; Li Deng; Dilek Hakkani-Tur; Xiaodong He; Larry Heck; Gokhan Tur; Dong Yu; Geoffrey Zweig"}, {"title": "Efficacy of a Web-Based, Crowdsourced Peer-To-Peer Cognitive Reappraisal Platform for Depression: Randomized Controlled Trial", "journal": "J Med Internet Res", "year": "2015", "authors": " Rr Morris; S M Schueller; R W Picard"}, {"title": "Neural belief tracker: Data-driven dialogue state tracking", "journal": "", "year": "2017", "authors": "Nikola Mrk\u0161i\u0107; Diarmuid\u00f3 S\u00e9aghdha; Tsung-Hsien Wen; Blaise Thomson; Steve Young"}, {"title": "Scikit-learn: Machine learning in Python", "journal": "Journal of Machine Learning Research", "year": "2011", "authors": "F Pedregosa; G Varoquaux; A Gramfort; V Michel; B Thirion; O Grisel; M Blondel; P Prettenhofer; R Weiss; V Dubourg; J Vanderplas; A Passos; D Cournapeau; M Brucher; M Perrot; E Duchesnay"}, {"title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"title": "Software Framework for Topic Modelling with Large Corpora", "journal": "Valletta, Malta. ELRA", "year": "2010", "authors": "Petr Radim\u0159eh\u016f\u0159ek;  Sojka"}, {"title": "Towards healthcare personal agents", "journal": "ACM", "year": "2014", "authors": "Giuseppe Riccardi"}, {"title": "Addressing loneliness and isolation in older adults: Proactive affective agents provide better support", "journal": "", "year": "2013", "authors": "Lazlo Ring; Barbara Barry; Kathleen Totzke; Timothy Bickmore"}, {"title": "Humaine Association Conference on Affective Computing and Intelligent Interaction, ACII '13", "journal": "", "year": "", "authors": ""}, {"title": "An affectively aware virtual therapist for depression counseling", "journal": "", "year": "2016", "authors": "Lazlo Ring; Timothy Bickmore; Paola Pedrelli"}, {"title": "Exploiting sentence and context representations in deep neural models for spoken language understanding", "journal": "", "year": "2016", "authors": "Lina M Rojas Barahona; M Gasic; N Mrk\u0161i\u0107; P-H Su; S Ultes; T-H Wen; S Young"}, {"title": "Adaptable dialog architecture and runtime engine (adarte): A framework for rapid prototyping of health dialog systems", "journal": "I. J. Medical Informatics", "year": "2009", "authors": "Lina Maria Rojas-Barahona; Toni Giorgino"}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "journal": "", "year": "2013", "authors": "M Andrew; James L Saxe; Surya Mcclelland;  Ganguli"}, {"title": "A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies", "journal": "KER", "year": "2006", "authors": "J Schatzmann;  Weilhammer; S Mn Stuttle;  Young"}, {"title": "Is cognitivebehavioral therapy more effective than other therapies? A meta-analytic review", "journal": "Clinical Psychology Review", "year": "2010", "authors": "D F Tolin"}, {"title": "Exploiting the semantic web for unsupervised natural language semantic parsing", "journal": "", "year": "2012", "authors": "G\u00f6khan T\u00fcr; Minwoo Jeong; Ye-Yi Wang; Dilek Hakkani-T\u00fcr; Larry P Heck"}, {"title": "Designing relational agents as long term social companions for older adults", "journal": "Springer", "year": "2012", "authors": "L P Vardoulakis; L Ring; B Barry; C Sidner; T Bickmore"}, {"title": "Clinical information extraction applications: A literature review", "journal": "Journal of Biomedical Informatics", "year": "2018", "authors": "Y Wang; L Wang; M Rastegar-Mojarad; S Moon; F Shen; N Afzal; S Liu; Y Zeng; S Mehrabi; S Sohn; H Liu"}, {"title": "Eliza, a computer program for the study of natural language communication between man and machine", "journal": "ACM", "year": "1966", "authors": "Joseph Weizenbaum"}, {"title": "Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", "journal": "", "year": "2017", "authors": "Jason D Williams; Kavosh Asadi; Geoffrey Zweig"}, {"title": "World Health Organization", "journal": "", "year": "2013", "authors": ""}, {"title": "Spoken language understanding using long short-term memory neural networks", "journal": "", "year": "2014", "authors": "Kaisheng Yao; Baolin Peng; Yu Zhang; Dong Yu; G Zweig; Yangyang Shi"}, {"title": "Talking to Machines (Statistically Speaking)", "journal": "", "year": "2002", "authors": " Sj Young"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An example of an annotated Koko post.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Distribution of posts for each category.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: CNN with gating mechanism.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: GRU with skip-thought vectors.", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure6: predictions of posts by 300 dim CNN-GloVe Figure7gives the comparative performance of two models under different oversampling ratios.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Taxonomy for thinking errors and how they are exhibited.", "figure_data": "EmotionFrequency Exhibited by ...Anger (/frustration) 14.76%Feelings of frustration, annoyance, irritation, resentment, fury, outrageAnxiety63.12%Any expression of fear, worry or anxietyDepression20.72%Feeling down, hopeless, joyless, negative about self and/or life in generalGrief/sadness5.70%Feeling sad, upset, bereft in relation to a major lossGuilt3.37%Feeling blameworthy for a wrongdoing or something not doneHurt19.88%Feeling wounded and/or badly treatedJealousy3.12%Antagonistic feeling towards another either wish to be like or to have what they haveLoneliness7.41%Feeling of alone-ness, isolation, friendlessness, not understood by anyoneShame5.68%Feeling distress, humiliation, disgrace in relation to own behaviour or feelings"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Taxonomy for emotions and how they are exhibited.", "figure_data": "SituationFrequencyBereavement2.65%Existential21.93%Health10.61%Relationships 67.58%School/College 8.28%Work6.10%Other5.53%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Taxonomy for situations.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "The main reason for the low agreement in thinking errors (61%) is", "figure_data": "ConceptThinking errorSituationEmotionKappa0.61 \u00b1 0.090.92 \u00b1 0.080.90 \u00b1 0.07"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Cohen's kappa with a 95% confidence interval", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ": F1 scores for all models with 1:1 oversampling"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "F1 score of the models trained with embeddings with dimensionality of 300 and 100 respectively.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "739\u00b10.007 0.884\u00b10.005 0.805\u00b10.006 0.729\u00b10.012 Depression 0.538\u00b10.010 0.708\u00b10.005 0.611\u00b10.008 0.813\u00b10.010 Hurt 0.428\u00b10.005 0.620\u00b10.004 0.506\u00b10.005 0.763\u00b10.011 Anger 0.313\u00b10.005 0.491\u00b10.000 0.383\u00b10.004 0.769\u00b10.012 Loneliness 0.479\u00b10.010 0.643\u00b10.008 0.549\u00b10.009 0.923\u00b10.006 Grief 0.437\u00b10.013 0.490\u00b10.000 0.462\u00b10.008 0.937\u00b10.005 Shame 0.219\u00b10.008 0.378\u00b10.004 0.277\u00b10.007 0.891\u00b10.007 Jealousy 0.170\u00b10.002 0.296\u00b10.012 0.216\u00b10.005 0.935\u00b10.006 Guilt 0.221\u00b10.014 0.378\u00b10.008 0.279\u00b10.014 0.936\u00b10.008 Relationships 0.847\u00b10.005 0.912\u00b10.007 0.878\u00b10.006 0.829\u00b10.011 Existential 0.516\u00b10.008 0.700\u00b10.004 0.594\u00b10.007 0.789\u00b10.009 Health 0.520\u00b10.010 0.668\u00b10.005 0.585\u00b10.008 0.900\u00b10.006 School College 0.570\u00b10.009 0.821\u00b10.008 0.673\u00b10.009 0.934\u00b10.004 Other 0.209\u00b10.004 0.331\u00b10.007 0.256\u00b10.005 0.894\u00b10.007 Work 0.601\u00b10.015 0.733\u00b10.006 0.661\u00b10.011 0.955\u00b10.003 Bereavement 0.567\u00b10.029 0.733\u00b10.008 0.639\u00b10.021 0.979\u00b10.002 Jumping to negative conclusions 0.643\u00b10.005 0.775\u00b10.004 0.703\u00b10.005 0.711\u00b10.009 179\u00b10.003 0.345\u00b10.007 0.236\u00b10.004 0.871\u00b10.009 Comparing 0.257\u00b10.009 0.253\u00b10.009 0.255\u00b10.009 0.952\u00b10.003", "figure_data": "labelprecisionrecallF1 scoreaccuracyAnxiety 0.Fortune telling 0.486\u00b10.006 0.737\u00b10.004 0.585\u00b10.006 0.733\u00b10.010Black and white0.330\u00b10.003 0.625\u00b10.003 0.432\u00b10.003 0.657\u00b10.011Low frustration tolerance0.222\u00b10.005 0.531\u00b10.002 0.313\u00b10.005 0.631\u00b10.028Catastrophising0.291\u00b10.005 0.509\u00b10.000 0.371\u00b10.004 0.796\u00b10.012Mind-reading0.343\u00b10.008 0.540\u00b10.002 0.419\u00b10.006 0.783\u00b10.014Labelling0.376\u00b10.004 0.597\u00b10.003 0.462\u00b10.004 0.853\u00b10.007Emotional reasoning0.241\u00b10.006 0.417\u00b10.004 0.306\u00b10.006 0.748\u00b10.017Over-generalising0.337\u00b10.009 0.548\u00b10.002 0.418\u00b10.008 0.808\u00b10.014Inflexibility0.162\u00b10.002 0.336\u00b10.006 0.218\u00b10.003 0.807\u00b10.012Blaming0.218\u00b10.002 0.381\u00b10.005 0.277\u00b10.003 0.841\u00b10.009Disqualifying the positive0.125\u00b10.002 0.365\u00b10.008 0.187\u00b10.003 0.808\u00b10.016Mental filtering0.087\u00b10.001 0.386\u00b10.009 0.141\u00b10.002 0.741\u00b10.026Personalising0."}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Precision, recall, F1 score and accuracy for 300 dim CNN-GloVe with oversampling ratio 1:1 Figure7: Weighted AVG. F1 for different models", "figure_data": ""}], "doi": ""}