{"authors": "Bo Chen; Xiaotao Gu; Yufeng Hu; Siliang Tang; Guoping Hu; Yueting Zhuang; Xiang Ren; Classifier Kl", "pub_date": "", "title": "Improving Distantly-supervised Entity Typing with Compact Latent Space Clustering", "abstract": "Recently, distant supervision has gained great success on Fine-grained Entity Typing (FET). Despite its efficiency in reducing manual labeling efforts, it also brings the challenge of dealing with false entity type labels, as distant supervision assigns labels in a contextagnostic manner. Existing works alleviated this issue with partial-label loss, but usually suffer from confirmation bias, which means the classifier fit a pseudo data distribution given by itself. In this work, we propose to regularize distantly supervised models with Compact Latent Space Clustering (CLSC) to bypass this problem and effectively utilize noisy data yet. Our proposed method first dynamically constructs a similarity graph of different entity mentions; infer the labels of noisy instances via label propagation. Based on the inferred labels, mention embeddings are updated accordingly to encourage entity mentions with close semantics to form a compact cluster in the embedding space, thus leading to better classification performance. Extensive experiments on standard benchmarks show that our CLSC model consistently outperforms state-of-the-art distantly supervised entity typing systems by a significant margin.", "sections": [{"heading": "Introduction", "text": "Recent years have seen a surge of interests in fine-grained entity typing (FET) as it serves as an important cornerstone of several nature language processing tasks including relation extraction (Mintz et al., 2009), entity linking (Raiman and Raiman, 2018), and knowledge base completion (Dong et al., 2014). To reduce manual efforts in labelling training data, distant supervision (Mintz et al., 2009) has been widely adopted by recent FET systems. With the help of an external knowledge base (KB), an entity mention is first Figure 1: T-SNE visualization of the mention embeddings generated by NFETC (left) and CLSC (right) on the BBN dataset. Our model (CLSC) clearly groups mentions of the same type into a compact cluster.\nlinked to an existing entity in KB, and then labeled with all possible types of the KB entity as supervision. However, despite its efficiency, distant supervision also brings the challenge of outof-context noise, as it assigns labels in a context agnostic manner. Early works usually ignore such noise in supervision (Ling and Weld, 2012;Shimaoka et al., 2016), which dampens the performance of distantly supervised models.\nTowards overcoming out-of-context noise, two lines of work have been proposed to distantly supervised FET. The first kind of work try to filter out noisy labels using heuristic rules (Gillick et al., 2014). However, such heuristic pruning significantly reduces the amount of training data, and thus cannot make full use of distantly annotated data. In contrast, the other thread of works try to incorporate such imperfect annotation by partiallabel loss (PLL). The basic assumption is that, for a noisy mention, the maximum score associated with its candidate types should be greater than the scores associated with any other non-candidate types (Ren et al., 2016a;Abhishek et al., 2017;Xu and Barbosa, 2018). Despite their success, PLLbased models still suffer from Confirmation Bias by taking its own prediction as optimization objective in the next step. Specifically, given an entity mention, if the typing system selected a wrong type with the maximum score among all candidates, it will try to further maximize the score of the wrong type in following optimization epoches (in order to minimize PLL), thus amplifying the confirmation bias. Such bias starts from the early stage of training, when the typing model is still very suboptimal, and can accumulate in training process. Related discussion can be also found in the setting of semi-supervised learning (Lee et al., 2006;Laine and Aila, 2017;Tarvainen and Valpola, 2017).\nIn this paper, we propose a new method for distantly supervised fine-grained entity typing. Enlightened by (Kamnitsas et al., 2018), we propose to effectively utilize imperfect annotation as model regularization via Compact Latent Space Clustering (CLSC). More specifically, our model encourages the feature extractor to group mentions of the same type as a compact cluster (dense region) in the representation space, which leads to better classification performance. For training data with noisy labels, instead of generating pseudo supervision by the typing model itself, we dynamically construct a similarity-weighted graph between clean and noisy mentions, and apply label propagation on the graph to help the formation of compact clusters. Figure 1 demonstrates the effectiveness of our method in clustering mentions of different types into dense regions. In contrast to PLL-based models, we do not force the model to fit pseudo supervision generated by itself, but only use noisy data as part of regularization for our feature extractor layer, thus avoiding bias accumulation.\nExtensive experiments on standard benchmarks show that our method consistently outperforms state-of-the-art models. Further study reveals that, the advantage of our model over the competitors gets even more significant as the portion of noisy data rises.", "n_publication_ref": 14, "n_figure_ref": 2}, {"heading": "Problem Definition", "text": "Fine-grained entity typing takes a corpus and an external knowledge base (KB) with a type hierarchy Y as input. Given an entity mention (i.e., a sequence of token spans representing an entity) in the corpus, our task is to uncover its corresponding type-path in Y based on the context. By applying distant supervision, each mention is first linked to an existing entity in KB, and then labeled with all its possible types. Formally, a labeled corpus can be represented as\ntriples D = {(m i , c i , Y i )} n i=1\n, where m i is the i-th mention, c i is the context of m i , Y i is the set of candidate types of m i . Note that types in Y i can form one or more type paths. In addition, we denote all terminal (leaf) types of each type path in Y i as the target type set Y t i (e.g., for Y i = {artist, teacher, person}, Y t i = {artist, teacher}). This setting is also adopted by (Xu and Barbosa, 2018).\nAs each entity in KB can have several type paths, out-of-context noise may exist when Y i contains type paths that are irrelevant to m i in context c i . In this work, we argue triples where Y i contains only one type path (i.e., |Y t i | = 1) as clean data. Other triples are treated as noisy data, where Y i contains both the true type path and irrel- evant type paths. Noisy data usually takes a considerable portion of the entire dataset. The major challenge for distantly supervised typing systems is to incorporate both clean and noisy data to train high-quality type classifiers.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "The Proposed Approach", "text": "Overview. The basic assumptions of our idea are:\n(1) all mentions belong to the same type should be close to each other in the representation space because they should have similar context, (2) similar contexts lead to the same type. For clean data, we compact the representation space of the same type to comply (1). For noisy data, given assumption (2), we infer the their type distributions via label propagation and candidate types constrain.\nFigure 2 shows the overall framework of the proposed method. Clean data is used to train classifier and feature extractor end-to-endly, while noisy data is only used in CLSC regularization. Formally, given a batch of samples {(m i , c i , Y t i )} B i=1 , we first convert each sample (m i , c i ) into a real-valued vector z i via a feature extractor z((m i , c i ); \u03b8 z ) parameterized by \u03b8 z . Then a type classifier g(z i ; \u03b8 g ) parameterized by \u03b8 g gives the posterior P (y|z i ; \u03b8 g ). By incorporating CLSC regularization in the objective function, we encourage the feature extractor z to group mentions of the same type into a compact cluster, which facilitates classification as is shown in Figure 1. Noisy data enhances the formation of compact clusters with the help of label propagation.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Feature Extractor", "text": "Figure 3 illustrates our feature extractor. For fair comparison, we adopt the same feature extraction pipeline as used in (Xu and Barbosa, 2018). The feature extractor is composed of an embedding layer and two encoders which encode mentions and contexts respectively. Embedding Layer: The output of this layer is a concatenation of word embedding and word position embedding. We use the popular 300dimensional word embedding supplied by (Pennington et al., 2014) to capture the semantic information and random initialized position embedding (Zeng et al., 2014) to acquire information about the relation between words and the mentions.\nFormally, Given a word embedding matrix W word of shape d w \u00d7 |V |, where V is the vocabulary and d w is the size of word embedding, each column of W word represents a specific word w in V . We map each word w j in (m i , c i ) to a word embedding w d j \u2208 R dw . Analogously, we get the word position embedding w p j \u2208 R dp of each word according to the relative distance between the word and the mention, we only use a fixed length context here. The final embedding of the j-th word is w\nE j = [w d j , w p j ].\nMention Encoder: To capture lexical level information of mentions, an averaging mention encoder and a LSTM mention encoder (Hochreiter and Schmidhuber, 1997) is applied to encode mentions. Given m i = (w s , w s+1 , \u2022 \u2022 \u2022 , w e ), the aver-aging mention representation r a i \u2208 R dw is :\nr a i = 1 e \u2212 s + 1 e j=s w d j (1)\nBy applying a LSTM over an extended mention (w s\u22121 , w s , w s+1 , \u2022 \u2022 \u2022 , w e , w e+1 ), we get a sequence\n(h s\u22121 , h s , h s+1 , \u2022 \u2022 \u2022 , h e , h e+1 ). We use h e+1 as LSTM mention representation r l i \u2208 R d l . The final mention representation is r m i = [r a i , r l i ] \u2208 R dw+d l . Context Encoder: A bidirectional LSTM with d l hidden units is employed to encode embedding se- quence (w E s\u2212W , w E s\u2212W+1 , \u2022 \u2022 \u2022 , w E e+W ): \u2212 \u2192 h j =LST M ( \u2212\u2212\u2192 h j\u22121 , w E j\u22121 ) \u2190 \u2212 h j =LST M ( \u2190\u2212\u2212 h j\u22121 , w E j\u22121 ) h j =[ \u2212 \u2192 h j \u2295 \u2190 \u2212 h j ](2)\nwhere \u2295 denotes element-wise plus. Then, the word-level attention mechanism computes a score \u03b2 i,j over different word j in the context c i to get the final context representation r c i :\n\u03b1 j =w T tanh(h j ) \u03b2 i,j = exp(\u03b1 j ) k exp(\u03b1 k ) r c i = j \u03b2 i,j h i,j(3)\nWe use r i = [r m i , r c i ] \u2208 R dz = R dw+d l +d l as the feature representation of (m i , c i ) and use a Neural Networks q over r i to get the feature vector z i . q has n layers with h n hidden units and use ReLu activation.", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "Compact Latent Space Clustering for Distant Supervision", "text": "The overview of CLSC regularization is exhibited in Figure 4, which includes three steps: dynamic graph construction (Figure 4c), label propagation (Figure 4d, e) and Markov chains (Figure 4g). The idea of compact clustering for semisupervised learning is first proposed by (Kamnitsas et al., 2018). The basic idea is to encourage mentions of the same type to be clustered into a dense region in the embedding space. We introduce more details of CLSC for distantly supervised FET in following sections. Dynamic Graph Construction: We start by creating a fully connected graph G over the batch of samples Z = {z i } B i=1 , as shown in Figure 4c 1 . Each node of G is a feature representation z i , while the distance between nodes is defined by a scaled dot-product distance function (Vaswani et al., 2017):\nA ij =exp( z T i z j \u221a d z ), \u2200z i , z j \u2208 Z A =exp( Z T Z \u221a d z )(4)\nEach entry A ij measures the similarity between z i and z j , A \u2208 R B\u00d7B can be viewed as the weighted adjacency matrix of G.\nLabel Propagation: The end goal of CLSC is to cluster mentions of the same type to a dense region. For mentions which have more than one labeled types, we apply label propagation (LP) on G to estimate their type distribution. Formally, we denote \u03a6 \u2208 R B\u00d7K as the label propagation posterior of a training batch.\nThe original label propagation proposed by (Zhu and Ghahramani, 2002) uses a transition matrix H to model the probability of a node i propagating its type posterior \u03c6 i = P (y i |x i ) \u2208 R K to the other nodes. Each entry of the transition matrix H \u2208 R B\u00d7B is defined as:\nH ij = A ij / b A ib (5)\nThe original label propagation algorithm is defined as:\n1. Propagate the label by transition matrix H, \u03a6 (t+1) = H\u03a6 (t) 2. Clamp the labeled data to their true labels.\nRepeat from step 1 until \u03a6 converges\nIn this work \u03a6 (0) is randomly initialized 2 . Unlike unlabeled data in semi-supervised learning, distantly labeled mentions in FET have a limited set of candidate types. Based on this observation, We assume that (m i , c i ) can only transmit and receive probability of types in Y t i no matter it is noisy data or clean data. Formally, define a B \u00d7 K indicator matrix M \u2208 R B\u00d7K , where M ij = 1 if type j in Y t i otherwise 0, where B is the batch size and K is the number of types. Our clamping step relies on M as is shown in Figure 4d:\n\u03a6 (t+1) ij \u2190 \u03a6 (t+1) ij M ij / k \u03a6 (t+1) ik M ik (6)\nFor convenience, we iterate through these two steps S lp times, S lp is a hyperparameter.\nCompact Clustering: The LP posterior \u03a6 = \u03a6 (S lp +1) is used to judge the label agreement between samples. In the desired optimal state, transition probabilities between samples should be uniform inside the same class, while be zero between different classes. Based on this assumption, the desirable transition matrix T \u2208 R B\u00d7B is defined as:\nT ij = K k=1 \u03a6 ik \u03a6 jk m k , m k = B b=1 \u03a6 bk (7)\nm k is a normalization term for class k. Transition matrix H derived from z((m i , c i ); \u03b8 z ) should be in keeping with T . Thus we minimize the cross entropy between T and H:\nL 1\u2212step = \u2212 1 B 2 B i=1 B j=1 T ij log(H ij ) (8)\nFor instance, if T ij is close to 1, H ij needs to be bigger, which results in the growth of A ij and finally optimize \u03b8 z (Eq.4). The loss L 1\u2212step has largely described the regularization we use in z((m i , c i ); \u03b8 z ) for compression clustering.\nIn order to keep the structure of existing clusters, (Kamnitsas et al., 2018) proposed an extension of L 1\u2212step to the case of Markov chains with multiple transitions between samples, which should remain within a single class. The extension maximizes probability of paths that only traverse among samples belong to one class. Define E \u2208 R B\u00d7B as:\nE = \u03a6 T \u03a6 (9)\nE ij measures the label similarities between z i and z j , which is used to mask the transition between different clusters. The extension is given by:\nH (1) =H H (s) =(H E) (s\u22121) H =(H E)H (s\u22121) ,(10)\nwhere is Hadamard Product, and H (s) ij is the probability of a Markov process to transit from node i to node j after s \u2212 1 steps within the same class. The extended loss function models paths of different length s between samples on the graph:\nL clsc = \u2212 1 S m 1 B 2 Sm s=1 B i=1 B j=1 T ij log(H (s) ij ).\n(11) For S m = 1, L clsc = L 1\u2212step . By minimizing the cross entropy between T and H (s) (Eq.11), L clsc compact paths of different length between samples within the same class. Here, S m is a hyper-parameter to control the maximum length of Markov chain. L clsc is added to the final objective function as regularization to encourage compact cluttering.", "n_publication_ref": 4, "n_figure_ref": 6}, {"heading": "Overall Objective", "text": "Given the representation of a mention, the type posterior is given by a standard softmax classifier parameterized by \u03b8 g :\nP (\u0177 i |z i ; \u03b8 g ) = sof tmax(W c z i + b c ),(12)\nwhere W c \u2208 R K\u00d7dz is a parameter matrix, b \u2208 R K is the bias vector, where K is the number of types. The predicted type is then given byt i = argmax y i P (\u0177 i |z i ; \u03b8 g ).\nOur loss function consists of two parts. L sup is supervision loss defined by KL divergence:\nL sup = \u2212 1 B c Bc i=1 K k=1 y ik log(P (y i |z i ; \u03b8 g )) k\n(13) Here B c is the number of clean data in a training batch, K is the number of target types. The regularization term is given by L clsc . Hence, the overall loss function is:\nL f inal = L sup + \u03bb clsc \u00d7 L clsc (14\n)\n\u03bb clsc is a hyper parameter to control the influence of CLSC.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dataset", "text": "We evaluate our method on two standard benchmarks: OntoNotes and BBN:\n\u2022 OntoNotes: The OntoNotes dataset is composed of sentences from the Newswire part of OntoNotes corpus (Weischedel et al., 2013). (Gillick et al., 2014) annotated the training part with the aid of DBpedia spotlight (Daiber et al., 2013), while the test data is manually annotated. \u2022 BBN: The BBN dataset is composed of sentences from Wall Street Journal articles and is manually annotated by (Weischedel and Brunstein, 2005). (Ren et al., 2016a) regenerated the training corpus via distant supervision.\nIn this work we use the preprocessed datasets provided by (Abhishek et al., 2017;Xu and Barbosa, 2018). Table 2 shows detailed statistics of the datasets.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Compared Methods", "text": "We compare the proposed method with several state-of-the-art FET systems 3 :\n\u2022 Attentive (Shimaoka et al., 2016) uses an attention based feature extractor and doesn't distinguish clean from noisy data; \u2022 AFET (Ren et al., 2016a) trains label embedding with partial label loss; \u2022 AAA (Abhishek et al., 2017) learns joint representation of mentions and type labels; \u2022 PLE+HYENA/FIGER (Ren et al., 2016b) proposes heterogeneous partial-label embedding for label noise reduction to boost typing systems. We compare two PLE models with HYENA (Yogatama et al., 2015) and FIGER (Ling and Weld, 2012) as the base typing system respectively; \u2022 NFETC (Xu and Barbosa, 2018) trains neural fine-grained typing system with hierarchy-aware loss. We compare the performance of the NFETC model with two different loss functions: partial-label loss and PLL+hierarchical loss. We denote the two variants as NFETC and NFETC hier respectively; \u2022 NFETC-CLSC is the proposed model in this work. We use the NFETC model as our base model, based on which we apply Compact Latent Space Clustering Regularization as described in Section 3.2; Similarly, we report results produced by using both KLdivergense-based loss (NFETC-CLSC) and KL+hierarchical loss (NFETC-CLSC hier ).", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Evaluation Settings", "text": "For evaluation metrics, we adopt strict accuracy, loose macro, and loose micro F-scores widely used in the FET task (Ling and Weld, 2012). To fine tuning the hyper-parameters, we randomly sampled 10% of the test set as a development set for both datasets. With the fine-tuned hyperparameter as mentioned in 4.4, we run the model five times and report the average strict accuracy, macro F1 and micro F1 on the test set.   ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Hyper Parameters", "text": "We search the hyper parameter of Ontonotes and BBN respectively via Hyperopt proposed by (Bergstra et al., 2013). Hyper parameters are shown in Appendix A. We optimize the model via Adam Optimizer. The full hyper parameters includes the learning rate lr, the dimension d p of word position embedding, the dimension d l of the mention encoder's output (equal to the dimension of the context encoder's ourput), the input dropout keep probability p i and output dropout keep probability p o for LSTM layers (in context encoder and LSTM mention encoder), the L2 regularization parameter \u03bb, the factor of hierarchical loss normalization \u03b1 (\u03b1 > 0 means use the normalization), BN (whether using Batch normalization), the max step S lp of the label propagation, the max length S m of Markov chain, the influence parameter \u03bb clsc of CLSC, the batch size B, the number n of hidden layers in q and the number h n of hidden units of the hidden layers. We implement all models using Tensorflow 4 .", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Performance comparison and analysis", "text": "Table 1 shows performance comparison between the proposed CLSC model and state-of-the-art FET systems. On both benchmarks, the CLSC model achieves the best performance in all three metrics. When focusing on the comparison between NFETC and CLSC, we have following observation:\n\u2022 Compact Latent Space Clustering shows its effectiveness on both clean data and noisy data. By applying CLSC regularization on the basic NFETC model, we observe consistent and significant performance boost; \u2022 Hierarchical-aware loss shows significant advantage on the OntoNotes dataset, while showing insignificant performance boost on the BBN dataset. This is due to different distribution of labels on the test set. The proportion of terminal types of the test set is 69% for the BBN dataset, while is only 33% on the OntoNotes dataset. Thus, applying hierarchical-aware loss on the BBN dataset brings little improvement; \u2022 Both algorithms are able to utilize noisy data to improve performance, so we would like to further study their performance in different noisy scenarios in following discussions.  By principle, with sufficient amount of clean training data, most typing systems can achieve satisfying performance. To further study the robustness of the methods to label noise, we compare their performance with the presence of 25%, 20%, 15%, 10% and 5% clean training data and all noisy training data. Figure 5 shows the performance curves as the proportion of clean data drops. As it reveals, the CLSC model consistently wins in the comparison. The advantage is especially clear on the BBN dataset, which offers less amount of training data. Note that, with only 27.9% of training data (when only leaving 5% clean data) on the BBN dataset, the CLSC model yield a comparable result with the NFETC model trained on full data. This comparison clearly shows the superiority of our approach in the effectiveness of utilizing noisy data.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Ablation: Do Markov Chains improve typing performance?", "text": "Table 3 shows the performance of CLSC with onestep transition (L 1\u2212step ) and with Markov Chains (L clsc ) as described in Section 3.2. Results show that the use of Markov Chains does bring improvement to the overall performance, which is consistent with the model intuition.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Named entity Recognition (NER) has been excavated for a long time (Collins and Singer, 1999;, which classifies coarsegrained types (e.g. person, location). Recently, (Nagesh and Surdeanu, 2018a,b) applied ladder network (Rasmus et al., 2015) to coarse-grained entity classification in a semi-supervised learning fashion. (Ling and Weld, 2012) proposed Fine-Grained Entity Recognition (FET). They used distant supervision to get training corpus for FET.\nEmbedding techniques was applied to learn feature representations since (Yogatama et al., 2015;Dong et al., 2015). (Shimaoka et al., 2016) introduced attention mechanism for FET to capture informative words. (Xin et al., 2018a) used the TransE entity embeddings (Bordes et al., 2013) as the query vector of attention.\nEarly works ignore the out-of-context noise, (Gillick et al., 2014) proposed context dependent FET and use three heuristics to clean the noisy labels with the side effect of losing training data.\nTo utilize noisy data, (Ren et al., 2016a) distinguished the loss function of noisy data from clean data via partial label loss (PLL). (Abhishek et al., 2017;Xu and Barbosa, 2018) proposed variants of PLL, which still suffer from confirmation bias. (Xu and Barbosa, 2018) proposed hierarchical loss to handle over-specific noise. On top of AFET, (Ren et al., 2016b) proposed a method PLE to reduce the label noise, which lead to a great success in FET. Because label noise reduction is separated from the learning of FET, there might be error propagation problem. Recently, (Xin et al., 2018b) proposed utilizing a pretrained language model measures the compatibility between context and type names, and use it to repel the interference of noisy labels. However, the compatibility got by language model may not be right and type information is defined by corpus and annotation guidelines rather than type names as is mentioned in (Azad et al., 2018). In addition, there are some work about entity-level typing which aim to figure out the types of entities in KB (Yaghoobzadeh and Sch\u00fctze, 2015;Jin et al., 2018).  ", "n_publication_ref": 19, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we propose a new method for distantly supervised fine-grained entity typing, which leverages imperfect annotations as model regularization via Compact Latent Space Clustering (CLSC). Experiments on two standard benchmarks demonstrate that our method consistently outperforms state-of-the-art models. Further study reveals our method is more robust than the former state-of-the-art approach as the portion of noisy data rises. The proposed method is general for other tasks with imperfect annotation. As a part of future investigation, we plan to apply the approach to other distantly supervised tasks, such as relation extraction.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Fine-grained entity type classification by jointly learning representations and label embeddings", "journal": "", "year": "2017", "authors": "Abhishek Abhishek; Ashish Anand; Amit Awekar"}, {"title": "A unified labeling approach by pooling diverse datasets for entity typing", "journal": "", "year": "2018", "authors": "Balaji Amar Prakash Azad; Ashish Ganesan; Amit Anand;  Awekar"}, {"title": "Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms", "journal": "Citeseer", "year": "2013", "authors": "James Bergstra; Dan Yamins; David D Cox"}, {"title": "Translating embeddings for modeling multirelational data", "journal": "", "year": "2013", "authors": "Antoine Bordes; Nicolas Usunier; Alberto Garcia-Duran; Jason Weston; Oksana Yakhnenko"}, {"title": "Unsupervised models for named entity classification", "journal": "", "year": "1999", "authors": "Michael Collins; Yoram Singer"}, {"title": "Improving efficiency and accuracy in multilingual entity extraction", "journal": "ACM", "year": "2013", "authors": "Joachim Daiber; Max Jakob; Chris Hokamp; Pablo N Mendes"}, {"title": "A hybrid neural model for type classification of entity mentions", "journal": "", "year": "2015", "authors": "Li Dong; Furu Wei; Hong Sun; Ming Zhou; Ke Xu"}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "journal": "ACM", "year": "2014", "authors": "Xin Dong; Evgeniy Gabrilovich; Geremy Heitz; Wilko Horn; Ni Lao; Kevin Murphy; Thomas Strohmann; Shaohua Sun; Wei Zhang"}, {"title": "Contextdependent fine-grained entity type tagging", "journal": "", "year": "2014", "authors": "Dan Gillick; Nevena Lazic; Kuzman Ganchev; Jesse Kirchner; David Huynh"}, {"title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"title": "Attributed and predictive entity embedding for fine-grained entity typing in knowledge bases", "journal": "", "year": "2018", "authors": "Hailong Jin; Lei Hou; Juanzi Li; Tiansi Dong"}, {"title": "Semi-supervised learning via compact latent space clustering", "journal": "PMLR", "year": "2018", "authors": "Konstantinos Kamnitsas; Daniel Castro; Loic Le Folgoc; Ian Walker; Ryutaro Tanno; Daniel Rueckert; Ben Glocker; Antonio Criminisi; Aditya Nori"}, {"title": "Temporal ensembling for semi-supervised learning", "journal": "", "year": "2017", "authors": "Samuli Laine; Timo Aila"}, {"title": "Fine-grained named entity recognition using conditional random fields for question answering", "journal": "Springer", "year": "2006", "authors": "Changki Lee; Yi-Gyu Hwang; Hyo-Jung Oh; Soojong Lim; Jeong Heo; Chung-Hee Lee; Hyeon-Jin Kim; Ji-Hyun Wang; Myung-Gil Jang"}, {"title": "Fine-grained entity recognition", "journal": "", "year": "2012", "authors": "Xiao Ling;  Daniel S Weld"}, {"title": "The stanford corenlp natural language processing toolkit", "journal": "", "year": "2014", "authors": "Christopher Manning; Mihai Surdeanu; John Bauer; Jenny Finkel; Steven Bethard; David Mcclosky"}, {"title": "Distant supervision for relation extraction without labeled data", "journal": "Association for Computational Linguistics", "year": "2009", "authors": "Mike Mintz; Steven Bills; Rion Snow; Dan Jurafsky"}, {"title": "An exploration of three lightly-supervised representation learning approaches for named entity classification", "journal": "", "year": "2018", "authors": "Ajay Nagesh; Mihai Surdeanu"}, {"title": "Keep your bearings: Lightly-supervised information extraction with ladder networks that avoids semantic drift", "journal": "", "year": "2018", "authors": "Ajay Nagesh; Mihai Surdeanu"}, {"title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"title": "Deeptype: Multilingual entity linking by neural type system evolution", "journal": "", "year": "2018-02-02", "authors": "Jonathan Raiman; Olivier Raiman"}, {"title": "Semisupervised learning with ladder networks", "journal": "", "year": "2015", "authors": "Antti Rasmus; Mathias Berglund; Mikko Honkala; Harri Valpola; Tapani Raiko"}, {"title": "Afet: Automatic finegrained entity typing by hierarchical partial-label embedding", "journal": "", "year": "2016", "authors": "Xiang Ren; Wenqi He; Meng Qu; Lifu Huang; Ji Heng; Jiawei Han"}, {"title": "Label noise reduction in entity typing by heterogeneous partial-label embedding", "journal": "ACM", "year": "2016", "authors": "Xiang Ren; Wenqi He; Meng Qu; Clare R Voss; Heng Ji; Jiawei Han"}, {"title": "An attentive neural architecture for fine-grained entity type classification", "journal": "", "year": "2016", "authors": "Sonse Shimaoka; Pontus Stenetorp; Kentaro Inui; Sebastian Riedel"}, {"title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results", "journal": "", "year": "2017", "authors": "Antti Tarvainen; Harri Valpola"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Bbn pronoun coreference and entity type corpus. Linguistic Data Consortium", "journal": "", "year": "2005", "authors": "Ralph Weischedel; Ada Brunstein"}, {"title": "Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia", "journal": "", "year": "2013", "authors": "Ralph Weischedel; Martha Palmer; Mitchell Marcus; Eduard Hovy; Sameer Pradhan; Lance Ramshaw; Nianwen Xue; Ann Taylor; Jeff Kaufman; Michelle Franchini"}, {"title": "Improving neural fine-grained entity typing with knowledge attention", "journal": "", "year": "2018", "authors": "Ji Xin; Yankai Lin; Zhiyuan Liu; Maosong Sun"}, {"title": "Put it back: Entity typing with language model enhancement", "journal": "", "year": "2018", "authors": "Ji Xin; Hao Zhu; Xu Han; Zhiyuan Liu; Maosong Sun"}, {"title": "Neural finegrained entity type classification with hierarchyaware loss", "journal": "", "year": "2018", "authors": "Peng Xu; Denilson Barbosa"}, {"title": "Corpus-level fine-grained entity typing using contextual information", "journal": "", "year": "2015", "authors": "Yadollah Yaghoobzadeh; Hinrich Sch\u00fctze"}, {"title": "Embedding methods for fine grained entity type classification", "journal": "Short Papers", "year": "2015", "authors": "Dani Yogatama; Daniel Gillick; Nevena Lazic"}, {"title": "Relation classification via convolutional deep neural network", "journal": "", "year": "2014", "authors": "Daojian Zeng; Kang Liu; Siwei Lai; Guangyou Zhou; Jun Zhao"}, {"title": "Learning from labeled and unlabeled data with label propagation", "journal": "", "year": "2002", "authors": "Xiaojin Zhu; Zoubin Ghahramani"}], "figures": [{"figure_label": "3", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 3 :3Figure 3: The architecture of feature extractor z((m i , c i ); \u03b8 z )", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: A demonstration of the CLSC process. (a) represents the feature extraction step; (b)\u2192(h) shows the traditional type classification process (each color represents one candidate type), where suboptimal classifiers make predictions for each mention and misclassifies A into the Blue type; (c)\u2192(d)\u2192(e)\u2192(f)\u2192(g) demonstrates the process of CLSC as described in Section 3. Through label propagation and compact clustering, our model is able to group mentions of the same type into a dense region and leaves clear separation boundaries in sparse regions.", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_2", "figure_caption": "4. 66How robust are the methods to the proportion of noisy data?", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Performance comparison between NFETC-CLSC and NFETC by removing 75%-95% clean data.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "S2: Defense Secretary William Cohen says if a formal investigation shows it was a", "figure_data": "Candidate Type/person/artist/actor/person/political_figure/person/legalMaine Republican William Cohen said the plan mightviolate the assassination bansup L sup L sup Lclsc L clsc LFigure 2: The overall framework of CLSC. We calculate classification loss only on clean data, while regularize thefeature extractor with CLSC using both clean and noisy data."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Performance comparision of FET systems on the two datasets.", "figure_data": "OntoNotes BBN#types8947Max hierarchy depth32#mentions-train25324186078#mentions-test896312845%clean mentions-train 73.1375.92%clean mentions-test94.00100Average |Y t i |1.401.26"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Detailed statistics of the two datasets.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "The comparison of L 1\u2212step and L clsc on BBN.", "figure_data": ""}], "doi": ""}