{"authors": "Leyi Wang; Rui Xia", "pub_date": "", "title": "Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision", "abstract": "Sentiment lexicon is an important tool for identifying the sentiment polarity of words and texts. How to automatically construct sentiment lexicons has become a research topic in the field of sentiment analysis and opinion mining. Recently there were some attempts to employ representation learning algorithms to construct a sentiment lexicon with sentiment-aware word embedding. However, these methods were normally trained under documentlevel sentiment supervision. In this paper, we develop a neural architecture to train a sentiment-aware word embedding by integrating the sentiment supervision at both document and word levels, to enhance the quality of word embedding as well as the sentiment lexicon. Experiments on the SemEval 2013-2016 datasets indicate that the sentiment lexicon generated by our approach achieves the state-of-the-art performance in both supervised and unsupervised sentiment classification, in comparison with several strong sentiment lexicon construction methods.", "sections": [{"heading": "Introduction", "text": "Sentiment lexicon is a set of words (or phrases) each of which is assigned with a sentiment polarity score. Sentiment lexicon plays an important role in many practical sentiment analysis and opinion mining tasks. There were some manually annotated universal sentiment lexicons such as General Inquireer (GI) and HowNet. However, due to the ubiquitous domain diversity and absence of domain prior knowledge, the automatic construction technique for domain-specific sentiment lex- *\nThe corresponding author of this paper.\nicons has become a challenging research topic in the field of sentiment analysis and opinion mining (Wang and Xia, 2016).\nThe early work employed unsupervised learning for sentiment lexicon construction. They normally labelled a set of seed words at first, and then learned the polarity of each candidate word, based on either word conjunction relations (e.g., constellation and transition in texts) (Hatzivassiloglou and McKeown, 1997), or the word co-occurrence information (such as pointwise mutual information, PMI) (Turney, 2002), between the candidate word and the seed words. However, the unsupervised manner showed limited effect in sentiment prediction, and the performance greatly depends on the quality of the seed words.\nTo fully exploit the sentiment labeling information in texts, a series of supervised learning methods was further proposed to learn the sentiment lexicons. For example, Mohammad et al. (2013) proposed to construct sentiment lexicons by calculating PMI between the word and the distantly supervised sentiment labels (such as emoticons) in tweets and the word's sentiment orientation (SO). The resulting lexicons obtained the best results in SemEval 2013. More advanced representation learning models were also utilized, with the aim to construct the sentiment lexicons with efficient word embeddings (Tang et al., 2014a;Hamilton et al., 2016;Vo and Zhang, 2016). The traditional representation learning framework such as Word2Vec only captures the syntactic information in the texts, but ignores the sentiment relations between words. Therefore, some researchers attempted to add sentiment supervision into the network structure, in order to train a sentimentaware word embedding. For example, Tang et al. (2014a) exploited a dedicated neural architecture to integrate document-level sentiment supervision and the syntactic knowledge for representation learning. The sentiment-aware word embedding is then used to construct a sentiment lexicon. Vo and Zhang (2016) proposed to learn a two-dimensional sentiment representation based on a simple neural network. The sentiment lexicons generated by their approach obtained better performance to predict the tweet sentiment labels, in comparison with the PMI-based method (Mohammad et al., 2013).\nAlthough these supervised learning methods can to some extent exploit the sentiment labeling information in the texts and can learn a sentiment-aware word embedding, the manner of using document-level sentiment supervision suffers from some complex linguistic phenomena such as negation, transition and comparative degree, and hence unable to capture the fine-grained sentiment information in the text. For example, in the following tweet \"Four more fake people added me. Is this why people don't like Twitter? :( \", the document-level sentiment label is negative, but there is a positive word \"like\" in the text. In representation learning, the embeddings of words are summed up to represent the document, and the word \"like\" will be falsely associated with the negative sentiment label. Such linguistic phenomena occur frequently in review texts, and makes sentiment-aware word representation learning less effective.\nTo address this problem, in this paper, we propose a new representation learning framework called HSSWE, to learn sentiment-aware word embeddings based on hierarchical sentiment supervision. In HSSWE, the learning algorithm is supervised under both document-level sentiment labels and word-level sentiment annotations (e.g., labeling \"like\" as a positive word). By leveraging the sentiment supervision at both document and word level, our approach can avoid the sentiment learning flaws caused by coarse-grained document-level supervision by incorporating finegrained word-level supervision, and improve the quality of sentiment-aware word embedding. Finally, following Tang et al. (2014a), a simple classifier was constructed to obtain the domainspecific sentiment lexicon by using word embeddings as inputs.\nThe main contributions of this work are as follows:\n1. To the best of our knowledge, this is the first work that learns the sentiment-aware word representation under supervision at both document and word levels.\n2. Our approach supports several kinds of wordlevel sentiment annotations such as 1) predefined sentiment lexicon; 2) PMI-SO lexicon with hard sentiment annotation; 3) PMI-SO lexicon with soft sentiment annotation. By using PMI-SO dictionary as word-level sentiment annotation, our approach is totally corpus-based, without any external resource.\n3. Our approach obtains the state-of-the-art performance in comparison with several strong sentiment lexicon construction methods, on the benchmark SemEval 2013-2016 datasets for twitter sentiment classification.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Related Work", "text": "In general, sentiment lexicons construction can be classified into two categories, dictionary-based methods and corpus-based methods. Dictionary-based methods generally integrate predefined resources, such as WordNet, to construct sentiment lexicons. Hu and Liu (2004) exploited WordNet for sentiment lexicon construction. They first labelled two sets of seed words by polarities, then extended the sets by adding the synonyms for each word to the same set and antonyms to the other. For a given new word, Kim and Hovy (2004) introduced a Naive Bayes model to predict the polarities with .the synonym set obtained from WordNet as features. Kamps et al. (2004) investigated a graph-theoretic model of WordNet's synonymy relation and measured the sentiment orientation by distance between each candidate word and the seed words with different polarities. Heerschop et al. (2011) proposed a method to propagate the sentiment of seed set words through semantic relations of WordNet.\nCorpus-based approaches originate from the latent relation hypothesis: \"Pairs of words that cooccur in similar patterns tend to have similar semantic and sentiment relations\" (Turney, 2008).\nThe primary corpus-based method made the use of PMI. Turney (2002) built a sentiment lexicon by calculating PMI between the candidate word and seed words. The difference of the PMI score between positive and negative seed words is finally used as the sentiment orientation (SO) of each candidate word (Turney, 2002). Many variants of PMI were proposed afterwards, for example, positive pointwise mutual information (PPMI), second order co-occurrence PMI (SOC-PMI), etc. Hamilton et al. (2016) proposed to build a sentiment lexicon by a propagation method. The key of this method is to build a lexical graph by calculating the PPMI between words. Instead of calculating the PMI between words, Mohammad et al. (2013) proposed to use emoticons as distant supervision and calculate the PMI between words and the distant class labels, and obtained sound performance for tweet sentiment classification.\nThe latest corpus-based approaches normally utilize the up-to-date machine learning models (e.g. neural networks) to first learn a sentimentaware distributed representation of words, based on which the sentiment lexicon is then constructed. There were many word representation learning methods such as NNLM (Bengio et al., 2003) and Word2Vec (Mikolov et al., 2013). However, they mainly consider the syntactic relation of words in the context but ignore the sentiment information. Some work were later proposed to deal with this problem by incorporating the sentiment information during representation learning. For example, Tang et al. (2014a) adapted a variant of skip-gram model, which can learn the sentiment information based on distant supervision. Furthermore, Tang et al. (2014b) proposed a new neural network approach called SSWE to train sentimentaware word representation. Vo and Zhang (2016) exploited a simple and fast neural network to train a 2-dimensional representation. Each dimension is explicitly associated with a sentiment polarity.\nThe sentiment-aware word representation in these methods was normally trained based on only document-level sentiment supervision. In contrast, the learning algorithm in our approach is supervised under both document-level and wordlevel sentiment supervision.", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Our Approach", "text": "Our approach is comprised of three base modules: (1) Word-level sentiment learning and annotation; (2) Sentiment-aware word embedding learning; (3) Sentiment lexicon construction.\nOur approach depends on document-level sentiment labels. The tweet corpus provides a cheap way to get document-level sentiment annotation, owing to the distant sentiment supervision. But it should be noted that our approach is feasible for any corpus provided with document-level sentiment labels (not merely tweets).\nThe first module of our method aims to learn the pseudo sentiment distribution for each word and use it as word-level sentiment annotations to supervise word embedding learning.\nIn the second module, we learn the sentimentaware embeddings for each word in corpus, based on hierarchical sentiment supervision.\nIn the last module, we construct a sentiment lexicon by using the sentiment-aware word embeddings as the basis.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Learning Word-Level Sentiment Supervision", "text": "In addition to use a pre-defined sentiment lexicon for word-level annotations, we also propose to learn the word-level sentiment supervision, based on PMI and SO.\n(1) PMI and SO Given a corpus with document-level class labels. We first compute the PMI score between each word t and two class labels\nP M I(t, +) = log p(+|t) p(+) ,(1)\nP M I(t, \u2212) = log p(\u2212|t) p(\u2212) ,(2)\nwhere + and \u2212 denote the positive and negative document-level class labels, respectively.\nSecond, we compute the SO score for each word t:\nSO(t) = P M I(t, +) \u2212 P M I(t, \u2212). (3)\nWe call {t, SO(t)} as PMI-SO dictionary. The PMI-SO dictionary was widely used as a corpusbased sentiment lexicon for sentiment classification. By contrast, in our approach, it is the first step to learn the sentiment-aware word representation. Our approach supports two kinds of wordlevel sentiment annotations: 1) PMI-SO dictionary with hard sentiment annotation; 2) PMI-SO dictionary with soft sentiment annotation. The bias of document-level softmax layer \u03b8t\nWeight of word-level softmax layer \u03b8 d\nWeight of document-level softmax layer p(c|et)\nThe sentiment distribution of word t predicted by our model p(c|de)\nThe sentiment distribution of document d predicted by our model p(c|t)\nThe word-level sentiment annotation of word t with respect to class \u0109 p(c|d)\nThe document-level sentiment annotation of document d with respect to class c \n[p(\u2212|t),p(+|t)] = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 [0, 1], if SO(t) > 0 [1, 0], if SO(t) < 0 random{[1, 0]or[0, 1]}, otherwise .(4)\n(3) PMI-SO lexicon with soft sentiment annotation \"Soft sentiment annotation\" means that the annotation is given by the probability of two sentiment polarities, rather than the class label. We first use the sigmoid function to map the SO score to the range of a probability, and then define\n[p(\u2212|t),p(+|t)] = [1 \u2212 \u03c3(SO(t)), \u03c3(SO(t))](5)\nas the PMI-SO soft sentiment distribution of the word t.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Learning Sentiment-aware Word Representation under Hierarchical Sentiment Supervision", "text": "Till now we have obtained both document and word-level sentiment annotations, in the next step, we propose a neural network framework to learn the sentiment-aware word representation by integrating the sentiment supervision at both word and document granularities. We call it \"hierarchical sentiment supervision\". The architecture of our model is shown in Figure 1. We denote the corpus as D = {d 1 , d 2 , ..., d N } where N is the size of the corpus. Suppose d k is k-th document in D, and t i represents the i-th word in a document d. The parameters used in our neural network are described in Table 1.\nWe construct a embedding matrix C \u2208 R V \u00d7M , of which each row represents the embedding of a word in the vocabulary, where V is the size of the vocabulary and M is the dimension of word embedding. We randomly initialize each element of matrix C with a normal distribution.\n(1) Word-Level Sentiment Supervision\nWe use the word-level sentiment annotation [p(\u2212|t),p(+|t)] provided in Section 3.1 to supervise word representation learning at the word level.\nFor each word in document d, we map it to a continuous representation as e \u2208 C and feed e into our model to predict the sentiment distribution of the input word:\np(c|e) = sof tmax(\u03b8 t \u2022 e + b t ).(6)\nThe cost function is defined as the average cross entropy that measures the difference between the sentiment distribution predicted in our model and the sentiment annotations at the word level:\nf word = \u2212 1 T N k=1 t\u2208d k c\u2208{+,\u2212}p (c|t) log p(c|e t )(7)\nwhere T is the number of words in corpus.\n(2) Document-Level Sentiment Supervision\nWe use the document-level sentiment annotations to supervise word representation learning at the document level.\nIn order to obtain a continuous representation of a document d, we simply use the average embedding of words in d as de:\nde = 1 |d| t\u2208d e t .(8)\nWe feed de into our model to predict the sentiment probability: t i is the i-th word in d. And e t i represents the embedding of the word t i . We take de, the average embedding of [e t 1 , e t 2 , . . . , e tn ], as the representation of document d. We get each embedding of words in d as input to predict its sentiment polarities. We also take de as input to predict the sentiment for document d one time per epoch.\np(c|de) = sof tmax(\u03b8 d \u2022 de + b d ).(9)\nSimilarly, the cost function is defined as average cross entropy that measures the difference between the sentiment distribution predicted in our model and the sentiment annotation at the document level:\nf doc = \u2212 1 N N k=1 c\u2208{+,\u2212}p (c|d k ) log p(c|de k ) (10) wherep(c|d k ) is the sentiment annotation of doc- ument d k .p(c|d k ) = 1 denotes the class label of d k is positive, otherwisep(c|d k ) = 0.\n(3) Word and Document-Level Joint Learning\nIn order to learn the sentiment-aware word representation at both word and document levels, we integrate the cost function of two levels in a weighted combination way. The final cost function is defined as follows:\nf = \u03b1f word + (1 \u2212 \u03b1)f doc (11\n)\nwhere \u03b1 is a tradeoff parameter(0 \u2264 \u03b1 \u2264 1). The weight of f word can be increased by choosing a lager value of \u03b1.\nWe train our neural model with stochastic gradient descent and use AdaGrad (Duchi et al., 2011) to update the parameters.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "From Sentiment Representation to Sentiment Lexicon", "text": "In this part, we follow the method proposed by Tang et al. (2014a) to build a classifier to convert the sentiment-aware word representation learned in Section 3.2 to a sentiment lexicon. The word representation is the input of the classifier and word sentiment polarity is the output.\nFirstly, we utilize the embedding of 125 positive and 109 negative seed words manually labelled by Tang et al. (2014a)   Thirdly, a traditional logistic regression classifier is trained by using the embeddings of extended sentiment words as the inputs. The sentiment score of a word is the difference between its positive and negative probabilities.\nFinally, the sentiment lexicon can be collected by using the classifier to predict the other words' sentiment score.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Experiment Study", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets and Settings", "text": "We utilize the public distant-supervision corpus 2 (Go et al., 2009) to learn our lexicons. We set M , the dimension of embedding, as 50. The learning rate is 0.3 for stochastic gradient descent optimizer. We tune the hyper-parameter \u03b1 in the training process.\nWe evaluate the sentiment lexicons in both supervised and unsupervised sentiment classification tasks, on the SemEval 2013-2016 datasets. The statistics of evaluation datasets are shown in Table 2.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Supervised Sentiment Classification Evaluation:", "text": "To evaluate the effect of the sentiment lexicon in supervised sentiment classification, we report the supervised sentiment classification performance by using some pre-defined lexicon features. We follow (Mohammad et al., 2013) to extract the lexicon features as follows:\n\u2022 Total count of words in the tweet score of which is greater than 0;\n\u2022 Total count of words in the tweet score of which is less than 0;\n\u2022 The sum of scores for all word great than 0;\n2 http://help.sentiment140.com/for-students\n\u2022 The sum of scores for all word less than 0;\n\u2022 The max score greater than 0;\n\u2022 The min score less than 0;\n\u2022 Non-zero score of the last positive word in the tweet;\n\u2022 Non-zero score of the last negative word in the tweet.\nWe report the performance of SVM by using these lexicon features. The LIBSVM 3 toolkit is used with a linear kernel and the penalty parameter is set as the default value. The metric is F 1 score.\nUnsupervised Sentiment Classification Evaluation: For unsupervised sentiment classification, we sum up the scores of all sentiment words in the document, according to the sentiment lexicon. If the sum is greater than 0, the document will be considered as positive, otherwise negative. The unsupervised learning evaluation metric is accuracy.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "(External) Comparison with Public Lexicons", "text": "We compare our HSSWE method with four sentiment lexicons generated by the related work proposed in recent years:\n\u2022 Sentiment140 was constructed by Mohammad et al. (2013) on tweet corpus based on PMI between each word and the emoticons.\n\u2022 HIT was constructed by Tang et al. (2014a) with a representation learning approach.\n\u2022 NN was constructed by Vo and Zhang (2016) with a neural network method.\n\u2022 ETSL refers to SemEval-2015 English Twitter Sentiment Lexicon 4 (Rosenthal et al., 2015;Kiritchenko et al., 2014), which is done using Best-Worst Scaling.\nNote that Tang et al. (2014a)   all the comparison experiments on the complete benchmark datasets.\nSupervised Sentiment Classification: We first report the supervised sentiment classification F 1 score of five compared methods on the Semeval 2013-2016 datasets in Table 3. It can be seen that our HSSWE method gets the best result on all four datasets. It outperforms Sentiment140, HIT, NN and ETSL 1.7, 2.8, 1.9, and 3.2 percentages on the average of four datasets. The improvements are significant according to the paired t-test.\nUnsupervised Sentiment Classification: We then report the unsupervised sentiment classification accuracy of five methods on the Semeval 2013-2016 datasets in Table 4. In can be seen that HSSWE obtains the best performance on Semeval 2013-2015. On the Semeval 2016 dataset, it is slightly lower than ETSL. Across four datasets, the average accuracy of HSSWE is 6.6, 3.1, 9.6 and 0.94 higher than Sentiment140, HIT, NN and ET-SL, respectively.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "(Internal) Comparison within the Model", "text": "In order to further verify the effectiveness of our method and analyze which part of our model contributes the most, we carried out the internal comparison within our model. We design the following two simplified versions of our model for comparison:\n\u2022 PMI-SO denotes a PMI-SO based sentiment lexicon with soft sentiment annotation learned in Section 3.1.\n\u2022 Doc-Sup denotes the neural network system with only document-level sentiment supervision. It equals to HSSWE when \u03b1 = 0.\nActually, HSSWE can be viewed as a \"combination\" of PMI-SO and Doc-Sup. In Tables 5 and  6, we report the comparison results on supervised and unsupervised sentiment classification respectively.\nSupervised Sentiment Classification: As is shown in Table 5, two basic models PMI-SO and Doc-Sup show similar performance. They have distinct superiority across different datasets. But both are significantly lower than HSSWE. It shows that by combing the supervision at both document and word levels, it can indeed improve the quality of sentiment-aware word embedding and the subsequent sentiment lexicon.\nUnsupervised Sentiment Classification: As is shown in Table 6, the conclusions are similar with that in supervised sentiment classification: HSS-WE achieves the significantly better performance.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Word-level Sentimnt Annotation: Hard vs. Soft", "text": "In Section 3.1, we introduce two kinds of wordlevel sentiment annotation, i.e., soft and hard sentiment annotation. We now compare two methods. The results are reported in Tables 5 and  6 ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Tuning the Parameter \u03b1", "text": "In this section, we discuss the tradeoff between two parts of supervisions by turning the tradeoff parameter \u03b1. When \u03b1 is 0, HSSWE only benefits from the document-level sentiment supervision and when \u03b1 is 1, HSSWE benefits from only word-level sentiment supervision. We observe that HSSWE performs better when \u03b1 is in the range of [0.45,0.55]. By integrating two component parts of sentiment supervision, HSSWE has significant superiority over that learned from either one.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Lexicon Analysis", "text": "In order to gain more insight of our model and observe the effectiveness of the sentiment lexicon, in  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we proposed to construct sentiment lexicons based on a sentiment-aware word representation learning approach. In contrast to traditional methods normally learned based on only the document-level sentiment supervision. We proposed word representation learning via hierarchical sentiment supervision, i.e., under the supervi-sion at both word and document levels. The wordlevel supervision can be provided based on either predefined sentiment lexicons or the learned PMI-SO based sentiment annotation of words. A wide range of experiments were conducted on several benchmark sentiment classification datasets. The results indicate that our method is quite effective for sentiment-aware word representation, and the sentiment lexicon generated by our approach beats the state-of-the-art sentiment lexicon construction approaches.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "The work was supported by the Natural Science Foundation of China (No. 61672288), and the Natural Science Foundation of Jiangsu Province for Excellent Young Scholars (No. BK20160085).", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "A neural probabilistic language model", "journal": "Journal of Machine Learning Research", "year": "2003-02", "authors": "Yoshua Bengio; R\u00e9jean Ducharme; Pascal Vincent; Christian Jauvin"}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "journal": "Journal of Machine Learning Research", "year": "2011-07", "authors": "John Duchi; Elad Hazan; Yoram Singer"}, {"title": "Twitter sentiment classification using distant supervision", "journal": "", "year": "2009", "authors": "Alec Go; Richa Bhayani; Lei Huang"}, {"title": "Inducing domain-specific sentiment lexicons from unlabeled corpora", "journal": "", "year": "2016", "authors": "William L Hamilton; Kevin Clark; Jure Leskovec; Dan Jurafsky"}, {"title": "Predicting the semantic orientation of adjectives", "journal": "", "year": "1997", "authors": "Vasileios Hatzivassiloglou;  Kathleen R Mckeown"}, {"title": "Sentiment lexicon creation from lexical resources", "journal": "", "year": "2011", "authors": "Bas Heerschop; Alexander Hogenboom; Flavius Frasincar"}, {"title": "Mining and summarizing customer reviews", "journal": "", "year": "2004", "authors": "Minqing Hu; Bing Liu"}, {"title": "Using wordnet to measure semantic orientations of adjectives", "journal": "", "year": "2004", "authors": "Jaap Kamps; Maarten Marx; J Robert; Maarten Mokken;  De Rijke"}, {"title": "Determining the sentiment of opinions", "journal": "", "year": "2004", "authors": "Min Soo; Eduard Kim;  Hovy"}, {"title": "Sentiment analysis of short informal texts", "journal": "Journal of Artificial Intelligence Research", "year": "2014", "authors": "Svetlana Kiritchenko; Xiaodan Zhu; Saif M Mohammad"}, {"title": "Efficient estimation of word representations in vector space", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"title": "Nrc-canada: Building the state-ofthe-art in sentiment analysis of tweets", "journal": "", "year": "2013", "authors": "M Saif; Svetlana Mohammad; Xiaodan Kiritchenko;  Zhu"}, {"title": "Semeval-2015 task 10: Sentiment analysis in twitter", "journal": "", "year": "2015", "authors": "Sara Rosenthal; Preslav Nakov; Svetlana Kiritchenko; Saif Mohammad; Alan Ritter; Veselin Stoyanov"}, {"title": "Building large-scale twitter-specific sentiment lexicon : A representation learning approach", "journal": "", "year": "2014", "authors": "Duyu Tang; Furu Wei; Bing Qin; Ming Zhou; Ting Liu"}, {"title": "Learning sentimentspecific word embedding for twitter sentiment classification", "journal": "", "year": "2014", "authors": "Duyu Tang; Furu Wei; Nan Yang; Ming Zhou; Ting Liu; Bing Qin"}, {"title": "Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews", "journal": "", "year": "2002", "authors": "D Peter;  Turney"}, {"title": "The latent relation mapping engine: Algorithm and experiments", "journal": "Journal of Artificial Intelligence Research", "year": "2008", "authors": "D Peter;  Turney"}, {"title": "Dont count, predict! an automatic approach to learning sentiment lexicons for short text", "journal": "", "year": "2016", "authors": "Tin Duy; Yue Vo;  Zhang"}, {"title": "A survey on automatical construction methods of sentiment lexicons", "journal": "Acta Automatica Sinica", "year": "2016", "authors": "Ke Wang; Rui Xia"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "The word-level sentiment annotation is represented as [p(\u2212|t),p(+|t)]. We employ the following two ways to obtain [p(\u2212|t),p(+|t)].(2) PMI-SO lexicon with hard sentiment annotationNotations Description et The embedding of word t de The document representation of d bt The bias of word-level softmax layer b d", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure1: The Architecture of our Neural Network. Given a document d, represented as [t 1 , t 2 , . . . , t n ]. t i is the i-th word in d. And e t i represents the embedding of the word t i . We take de, the average embedding of [e t 1 , e t 2 , . . . , e tn ], as the representation of document d. We get each embedding of words in d as input to predict its sentiment polarities. We also take de as input to predict the sentiment for document d one time per epoch.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: HSSWE (soft) with different \u03b1", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The parameters used in our neural network.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Statistics of Evaluation Datasets 2014a), we did not extend the neutral words.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Supervised Evaluation for External Comparison (F 1 Score)", "figure_data": "LexiconSemeval2013 Semeval2014 Semeval2015 Semeval2016 AverageSentiment1400.72080.74160.69350.69280.7122HIT0.75660.79220.71280.72820.7474NN0.69030.72800.65070.65850.6819ETSL0.76750.82260.75050.73650.7693HSSWE0.77340.85390.76690.72060.7787"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Unsupervised Evaluation for External Comparison (Accuracy)", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Supervised Evaluation for Internal Comparison (F 1 Score), where HSSWE (hard) and HSSWE (soft) utilize the PMI-SO lexicon with hard sentiment annotation and soft sentiment annotation at the word level, respectively.", "figure_data": "LexiconSemeval2013 Semeval2014 Semeval2015 Semeval2016 AverageDoc-Sup0.72520.82940.73910.68590.7449HSSWE (soft)0.77340.85390.76690.72060.7787HSSWE (hard)0.74180.83950.76330.70110.7614"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Unsupervised Evaluation for Internal Comparison (Accuracy)", "figure_data": "0.750.74Macro-F1 Score0.71 0.72 0.73SemEval2013 SemEval2014 SemEval2015 SemEval20160.700.690.0 0.680.20.4alpha0.60.81.0"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "we extract the positive sentimen-", "figure_data": "WordsHSSWEPMI-SODoc-Supwell0.57400.54300.7898better0.58370.5358 (F)0.8440best0.94550.68230.9639fit0.2894-0.5594 (F)0.6076unreasonable-0.2441-0.84210.5275 (F)boreddddd-0.1137-0.71420.4843 (F)sickkkk-0.3892-0.76920.1323 (F)overplayed-0.13900.5000 (F) 0.8448 (F)"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Sentiment Lexicon Analysis, where score with (F) means falsely predicted polarity or strength. t score of some representative words learned by different methods. The positive scores are supposed to be: best>better>well. HSSWE captures such comparative sentiment strength but PMI-SO does not. We further observe that in many cases where the results of PMI-SO and Doc-Sup are inconsistent (e.g., Doc-Sup incorrectly predicts \"unreasonable\", \"boreddddd\" and \"sickkk\" as positive words, but PMI-SO predicts them correctly; PMI-SO incorrectly predicts \"fit\" but Doc-Sup predicts it correctly.), HSSWE often yield the correct results. It shows the advantages of hierarchical sentiment supervision. HSSWE can also correct the sentiment prediction where both PMI-SO and Doc-Sup are inefficient (e.g., \"overplayed\").", "figure_data": ""}], "doi": ""}