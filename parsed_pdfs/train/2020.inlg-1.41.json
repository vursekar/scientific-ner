{"authors": "Imen Akermi; Johannes Heinecke; Fr\u00e9d\u00e9ric Herledan", "pub_date": "", "title": "Transformer based Natural Language Generation for Question-Answering", "abstract": "This paper explores Natural Language Generation within the context of Question-Answering task. The several works addressing this task only focused on generating a short answer or a long text span that contains the answer, while reasoning over a Web page or processing structured data. Such answers' length are usually not appropriate as the answer tend to be perceived as too brief or too long to be read out loud by an intelligent assistant. In this work, we aim at generating a concise answer for a given question using an unsupervised approach that does not require annotated data. Tested over English and French datasets, the proposed approach shows very promising results. 25 50 rank A1/Bert/Cmbert-base A2/Bert/flaubert-small A1/Bert/xlm-roberta-base A1/Bert/flaubert-base-unc A1/Bert/xlm-mlm-enfr-1024 A1/Bert/xlm-roberta-large A2/Bert/gpt2 A1/Bert/bert-base-mlg-unc A1/Bert/bert-base-mlg A2/Bert/xlm-clm-enfr-1024 A1/Bert/xlm-clm-enfr-1024 A2/Bert/xlm-mlm-enfr-1024 A2/Bert/flaubert-large A2/Bert/xlm-roberta-base A1/Bert/flaubert-large A1/Bert/flaubert-small A1/Bert/openai-gpt A2/Bert/Cmbert-base A2/Bert/flaubert-base A2/Bert/flaubert-base-unc A2/Bert/xlm-roberta-large A1/Bert/gpt2 A2/Bert/bert-base-mlg-unc A2/Bert/gpt2-medium A1/Bert/gpt2-large A2/Bert/bert-base-mlg A1/Bert/gpt2-medium", "sections": [{"heading": "Introduction", "text": "Question-Answering systems (QAS) aim at analyzing and processing user questions in order to provide relevant answers (Hirschman and Gaizauskas, 2001). The recent popularity of intelligent assistants has increased the interest in QAS which have become a key component of \"Human-Machine\" exchanges since they allow users to have instant answers to their questions in natural language using their own terminology without having to go through a long list of documents to find the appropriate answers.\nMost of the existing research work focuses on the major complexity of these systems residing in the processing and interpretation of the question that expresses the user's need for information, without considering the representation of the answer itself. Usually, the answer is either represented by a short set of terms answering exactly the question (case of QAS which extract answers from structured data), or by a text span extracted from a document which, besides the exact answer, can integrate other unnecessary information that are not relevant to the context of the question asked. The following presents two answers for Who is the thesis supervisor of Albert Einstein? possibly generated by two systems :", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Alfred Kleiner", "text": "Albert Einstein is a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics.\nGiven the specificity of QAS which extract answers from structured data, users generally receive only a short and limited answer to their questions as illustrated by the example above. This type of answer representation might not meet the user expectations. Indeed, the type of answer given by the first system can be perceived as too brief not recalling the context of the question. The second system returns a passage which contains information that are out of the question's scope and might be deemed by the user as irrelevant.\nIt is within this framework that we propose in this article an approach which allows to generate a concise answer in natural language (e.g. The thesis superviser of Albert Einstein was Alfred Kleiner) that shows very promising results tested over French and English questions. This approach is a component of a QAS that we proposed in Rojas Barahona et al. (2019) and that we will briefly present in this article.\nIn what follows, we detail in section 3 the approach we propose for answer generation in Natural Language and we briefly discuss the QAS developed. We present in section 4 the experiments that we have conducted to evaluate this approach.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Related Work", "text": "The huge amount of information available nowadays makes the task of retrieving relevant informa-tion complex and time consuming. This complexity has prompted the development of QAS which help spare the user the search and the information filtering tasks, as it is often the case with search engines, and directly return the exact answer to a question asked in natural language.\nThe QAS cover mainly three tasks: question analysis, information retrieval and answer extraction (Lopez et al., 2011). These tasks have been tackled in different ways, considering the knowledge bases used, the types of questions addressed (Iida et al., 2019;Zayaraz et al., 2015;Dwivedi and Singh, 2013;Lopez et al., 2011) and the way in which the answer is presented. In this article, we particularly focus on the answer generation process.\nWe generally notice two forms of representation addressed in literature. The answer can take the form of a paragraph selected from a set of text passages retrieved from the web (Asai et al., 2018;Du and Cardie, 2018;Wang and Jiang, 2016;Wang et al., 2017;Oh et al., 2016), as it can also be the exact answer to the question extracted from a knowledge base (Wu et al., 2003;Bhaskar et al., 2013;Le et al., 2016).\nDespite the abundance of work in the field of QAS, the answers generation issue has received little attention. A first approach indirectly addressing this task has been proposed in Brill et al. (2001Brill et al. ( , 2002. Indeed, the authors aimed at diversifying the possible answer patterns by permuting the question's words in order to maximise the number of retrieved documents that may contain the answer to the given question. Another answer representation approach based on rephrasing rules has also been proposed in Agichtein and Gravano (2000); Lawrence and Giles (1998) within the context of query expansion task for document retrieval and not purposely for the question-answering task.\nThe few works that have considered this task within the QAS framework have approached it from a text summary generation perspective (Ishida et al., 2018;Iida et al., 2019;Rush et al., 2015;Chopra et al., 2016;Nallapati et al., 2016;Miao and Blunsom, 2016;See et al., 2017;Oh et al., 2016;Sharp et al., 2016;. These works consist in generating a summary of a single or various text spans that contain the answer to a question. Most of these works have only considered causality questions like the ones starting with \"why\" and whose answers are para-graphs. To make these answers more concise, the extracted paragraphs are summed up.\nOther approaches (Kruengkrai et al., 2017;Girju, 2003;Verberne et al., 2011;Oh et al., 2013) have explored this task as a classification problem that consists in predicting whether a text passage can be considered as an answer to a given question.\nIt should be noted that these approaches only intend to diversify as much as possible the answer representation patterns to a given question in order to increase the probability of extracting the correct answer from the Web and do not focus on the answer's representation itself. It should also be noted that these approaches are only applicable for QAS which extract answers as a text snippet and cannot be applied to short answers usually extracted from knowledge bases. The work presented in Pal et al. (2019) tried to tackle this issue by proposing a supervised approach that was trained on a small dataset whose questions/answers pairs were extracted from machine comprehension datasets and augmented manually which make generalization and capturing variation very limited.\nOur answer generation approach differs from these works as it is unsupervised, can be adapted to any type of factual question (except for why) and is based only on easily accessible and unannotated data. Indeed, we build upon the intuitive hypothesis that a concise answer and easily pronounced by an intelligent assistant can in fact consist of a reformulation of the question asked. This approach is a part of a QAS that we have developed in Rojas Barahona et al. (2019) that extracts the answer to a question from structured data.\nIn what follows, we detail in section 3 the approach we propose for answer generation in Natural Language and we briefly discuss the QAS developed. We present in section 4 the experiments that we have conducted to evaluate this approach. and we conclude in section 5 with the limitations noted and the perspectives considered.", "n_publication_ref": 32, "n_figure_ref": 0}, {"heading": "NLG Approach for Answer Generation", "text": "The answer generation approach proposed is a component of a system which was developed in Rojas Barahona et al. (2019) and which consists in a spoken conversational question-answering system which analyses and translates a question in natural language (French or English) in a formal representation that is transformed into a Sparql query 1 .\nThe Sparql query helps extracting the answer to the given question from an RDF knowledge base, in our case Wikidata 2 . The extracted answer takes the form of a list of URIs or values.\nAlthough the QAS that we have developed (Rojas Barahona et al., 2019) is able to find the correct answer to a question, we have noticed that its short representation is not user-friendly. Therefore, we propose an unsupervised approach which integrates the use of Transformer models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018). The choice of an unsupervised approach arises from the fact that there is no available training dataset associating a question with an exhaustive and concise answer at the same time. such dataset could have helped use an End-to-End learning neural architecture that can generate an elaborated answer to a question.\nThis approach builds upon the fact that we have already extracted the short answer to a given question and assumes that a user-friendly answer can consist in rephrasing the question words along with the short answer. This approach is composed of two fundamental phases: The dependency analysis of the input question and the answer generation using Transformer models.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Dependency parsing", "text": "For the dependency analysis, we use an extended version of UDPipeFuture (Straka, 2018) which showed its state of the art performance by becoming first in terms of the Morphology-aware Labeled Attachment Score (MLAS) 3 metric at the CoNLL Shared Task of dependency parsing in 2018 (Zeman et al., 2018). UDPipeFuture is a POS tagger and graph parser based dependency parser using a BiLSTM, inspired by Dozat et al. (2017).\nOur modification consisted in adding several contextual word embeddings (with respect to the language). In order to find the best configuration we experimented with models like multilingual BERT (Devlin et al., 2019), XLM-R (Conneau et al., 2019) (for both, English and French), RoBERTA (Liu et al., 2019) (for English), FlauBERT (Le et al., 2020) and CamemBERT (Martin et al., 2019) (for French) during the training of the treebanks French-GSD and English-EWT 4 , of the Universal Dependencies project (UD) (Nivre et al., 2016) 5 . Adding contextual word embedding increases significantly the results for all metrics, LAS, CLAS and MLAS (cf. table 1). This is the case for all languages (of the CoNLL shared task), where language specific contextual embeddings or multingual ones (as BERT or XLM-R) improved parsing (Heinecke, 2020) French (Fr-GSD) embeddings MLAS CLAS LAS Straka ( 2018  In order to parse simple, quiz-like questions, the training corpora of the two UD treebanks are not appropriate (enough), since both treebanks do not contain many questions, if at all 6 .\nAn explanation for bad performance on questions of parser models trained on standard UD is the fact, that in both languages, the syntax of questions differs from the syntax of declarative sentences: apart from wh question words, in English the to do periphrasis is nearly always used in questions. In French, subject and direct objects can be inversed and the est-ce que construction appears frequently. Both, the English to do periphrasis and the French est-ce que construction are absent in declarative sentences. Table 2 shows the (much lower) results when parsing questions using models trained only on the standard UD treebanks.\nIn order to get a better analysis, we decided to  and add this data to the basic treebanks.\nFor English we annotated 309 questions (plus 91 questions for validation) from the QALD7 (Usbeck et al., 2017) and QALD8 corpora 7 . For French we translated the QALD7 questions into French and formulated others ourselves (276 train, 66 validation). For the annotations we followed the general UD guidelines 8 as well as the treebank specific guidelines of En-EWT and Fr-GSD.\nAs table 3 shows, the quality of the dependency analysis improves considerably. The contextual word embeddings CamemBERT (for French) and BERT (English) have the biggest impact.  We rely on the UdpipeFuture version which we have improved with BERT (for English)/CamemBERT (for French) and which gives the best results in terms of dependency analysis, in order to proceed with the partitioning of the question into textual fragments (also called chunks): Q = {c 1 , c 2 , . . . , c n }.\nIf we take the example of the question What is the political party of the mayor of Paris?, the set of textual fragments would be Q = {What, is, the political party of the mayor of Paris }.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Answer generation", "text": "During this phase, we first carry out a first test of the set Q to check whether the text fragment which contains a question marker (exp: what, when, who etc.) represents the subject nsubj in the analysed question. If so, we simply replace that text fragment with the answer we identified earlier. Let us take the previous example What is the political party of the mayor of Paris?, the system automatically detects that the text fragment containing the question marker What represents the subject and will therefore be replaced directly by the exact answer The Socialist Party. Therefore, the concise answer generated will be The Socialist Party is the political party of the mayor of Paris.\nOtherwise, we remove the text fragment containing the question marker that we detected and we add the short answer R to Q: Q = {c 1 , c 2 , . . . , c n\u22121 , R} Using the text fragments set Q, we proceed with a permutation based generation of all possible answer structures that can form the sentence answering the question asked:\nS = {s 1 (R, c 1 , c 2 , . . . , c n\u22121 ), s 2 (c 1 , R, c 2 , . . . , c n\u22121 ),\n. . . , s m (c 1 , c 2 , . . . , c n\u22121 , R)} These structures will be evaluated by a Language Model (LM) based on Transformer models which will extract the most probable sequence of text fragments that can account for the answer to be sent to the user:\nstructure * = s \u2208 S; p(s) = argmax s i \u2208S p(s i )\nOnce the best structure is identified, we initiate the generation process of possible missing words. Indeed, we suppose that there could be some terms which do not necessarily appear in the question or in the short answer but which are, on the other hand, necessary to the generation of a correct grammatical structure of the final answer.\nThis process requires that we set two parameters, the number of possible missing words and their positions within the selected structure. In this paper, we experiment the assumption that one word could be missing and that it is located before the short answer within the identified structure, as it could be the case for a missing article (the, a, etc.) or a preposition (in, at, etc.) for example.\nTherefore, to predict this missing word, we use BERT as the generation model (GM) for its ability to capture bidirectionally the context of a given word within a sentence. In case when BERT returns a non-alphabetic character sequence, we assume that the optimal structure, as predicted by the LM, does not need to be completed by an additional word. The following example illustrates the different steps of the proposed approach:\nQuestion: When did princess Diana die?\n1. Question parsing and answer extraction using the system proposed in Rojas Barahona et al. ( 2019):\nshort answer = {August 31, 1997} ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments and Evaluation", "text": "The existing QAS test sets are more tailored to systems which generate the exact short answer to a question or more focused on the Machine Reading Comprehension task where the answer consists of a text passage from a document containing the short answer. Therefore, we have created a dataset which maps questions extracted from the QALD-7 challenge dataset (Usbeck et al., 2017) with natural language answers which were defined by a linguist and which we individually reviewed. This dataset called QUEREO consists of 150 questions with the short answers extracted by the QAS that we described above. We denote an average of three possible gold sanswers in natural language for each question. French and English versions were created for this dataset.  As illustrated in figure 1, two possible architectures of the approach proposed for answer generation have been evaluated. The first architecture A1 consists in generating all possible answer structures in order to have them evaluated afterwards by a LM which will identify the optimal answer structure to which we generate possible missing words. Architecture A2 starts with generating missing words for each structure in S which will then be evaluated by the LM. In this paper, we assume that there is only one missing word per structure.\nTo evaluate the proposed approach, we have referred to standard metrics defined for NLG tasks such as Automatic Translation and Summarization, as they allow to assess to what extent a generated sentence is similar to the gold sentence. We con-sider three N-gram metrics (BLEU, METEOR and ROUGE) and the BERT score metric which exploits the pre-trained embeddings of BERT to calculate the similarity between the answer generated and the gold answer. To be able to compare the different configurations of the approach, we refer to Friedman's test (Milton, 1939)   We also conducted a human evaluation study for the French and the English versions of the dataset, in which we asked 20 native speakers participants to evaluate the relevance of a generated answer (correct or not correct) regarding a given question while indicating the type of errors depicted (grammar, wrong preposition, word order, extra word(s), etc). Figure 3 presents the evaluation framework that we have implemented and provided to the participants. The results of each participant are saved in a json-file (figure 4). The inter-agreement rate between participants reached 70% which indicates a substantial agreement. Through the human evaluation study, we wanted to explore to what extent the standard metrics are reliable to assess NLG approaches within the context of question-answering systems.\nTable 4 (French dataset) represents the obtained results for the first three best models according to the human evaluation ranking and the Friedman test ranking. We indicate between brackets each model's rank according to the metric used.\nWe note that the highest human accuracy score for French of about 85% was scored with the first architecture coupled with BERT as the generation model (GM) and CamemBERT as the language model (LM). We also notice that the architecture A1, which considers the LM assessment of the structure before generating missing words, performs better. Surprisingly, as a generative model, the multi- Table 4: Model ranking for French dataset according to the human evaluation study (best in bold) and the Friedman test (best in yellow). \"BT\" in Column GM stands for BERT-base-multilingual-cased. In column LM we use \"CmBT\" for CamemBERT-base, \"BT-ml-c\" for BERT-base-multilingual-cased, \"XRob\" for XLM-RoBERTa-base, \"FBT-s-c\" for FlauBERT-small-cased, \"FBT-b-uc\" for FlauBERT-base-uncased and \"clm-1024\" for XLM-clmenfr-1024 lingual BERT model predicts missing words better than CamemBERT for French sentences. These findings are also confirmed by the Friedman test where we can clearly see that the first ranked configuration maps the best configuration selected according to the human accuracy, with a very slight difference for the other four configurations. Let us see if that means that the four metrics are correlated with the human accuracy. According to table 6 which presents the Pearson correlation (Benesty et al., 2009) of the human accuracy with the four metrics and to figure 2 which illustrates the ranking given by each evaluation metric along with the human judgement for each configuration (i.e. configuration = GM \u00d7 architecture \u00d7 LM) tested, we can clearly see that the human evaluation results are positively and strongly correlated with the BLEU, the METEOR and the BERT scores. These metrics are practically matching the human ranking and thus are obviously able to identify which configuration gives better results. The rouge metric, used for French question/answer evaluation, is moderately correlated with the human evaluation which means that we should not only rely on this metric when assessing such task. On the other hand, when the ROUGE metric is considered with the other metrics, it helps to get closer to the human judgement.\nTable 5 presents the results for the English dataset and shows that the best accuracy scored is about 72% with A1, BERT as the generative model and the Generative Pretrained Transformer (GPT) as the language model. According to the first three configurations, architecture A2 prevails and the GPT transformer takes over the other lan-guage models. These results are also confirmed by the Friedman test with a very slight difference on the ranking and also upheld with the correlation scores between the human assessment and each of the four metrics as shown by figure 5 and table 6.\nThese findings mean that we actually can rely on the use of these standard metrics to evaluate the answer generation task for question-answering.\nWe also tried to analyse the errors indicated by the participants. As we can note from figure 6, the most common error reported for both English and French datasets is the word order which sheds the light on a problem related to the language model assessment phase. The second most reported error addresses the generation process, whether to indicate that there are one or more missing words within the answer (French) or the presence of some odd words (English).\nWhen trying to get an insight on the answers generated by the current intelligent systems such as Google assistant and Alexa, we noted that these systems are very accurate when extracting the correct answer to a question and can sometimes generate user-friendly answers that help recall the question context, specially with Alexa. However, we noticed that most of the answers generated by these systems are more verbose than necessary, we also found out that when addressing yes/no questions, these systems generally settle for just a yes or no without elaborating, or, on the other hand, present a text span extracted from a Web page and let the user guess the answer. Let us take for example the following question Was US president Jackson involved in a war? Table 5: Model ranking for English dataset according to the human evaluation study (best in bold) and the Friedman ranking (best in yellow). In Column GM we use \"BT-ml\" for BERT-base-multilingual-cased and \"BT\" for BERTlarge-cased. In column LM \"GPT\" stands for for OpenAI-GPT, \"GPT2-l\" for GPT2-large, \"GPT2-m\" for GPT2medium, \"GPT2\" for GPT2, \"BT-b-uc\" for BERT-base-uncased, \"mlm-2048\" for XLM-mlm-en-2048 and \"BT-l-c\" for BERT-large-cased.\n[ { \"ID\": \"quereo_5.4\", \"QUESTION\":\n\"Quelles sont les companies d'\u00e9lectronique fond\u00e9es a Beijing ?\", \"SHORT_ANSWER\":\n[ \"Xiaomi\", \"Lenovo\" ], \"GENERATED_ANSWER\":\n\"Les companies d'\u00e9lectronique fond\u00e9es\u00e0 beijing sont xiao xiaomi et lenovo\", \"MISSING_WORD\": \"Xiao\", \"EVALUATION\": \"correcte\", \"ERROR\":\n[ \"aucun\" ], \"COMMENT\": \"\" }, { \"ID\": \"quereo_8.8\", \"QUESTION\":\n\"Combien de films a r\u00e9alis\u00e9 Park Chan-wook ?\", \"SHORT_ANSWER\":\n[ \"quatorze\" ], \"GENERATED_ANSWER\":\n\"Quatorze films a r\u00e9alis\u00e9 park chan-wook\", \"MISSING_WORD\": \".\", \"EVALUATION\": \"incorrecte\", \"ERROR\":\n[ \"ordre\", \"accord\" ], \"COMMENT\": \"\" }, ... ]    Here's something I found on the Web. According to constitutioncenter.org: After the War of 1812, Jackson led military forces against the Indians and was involved in treaties that led to the relocation of Indians.\nThe user has to focus on the returned text fragment in order to guess that the answer to his question is actually yes. This issue was particularly noted when addressing French questions. If we also take the example How many grandchildren did Jacques Cousteau have ? the two systems answer as follows:\nFabien Cousteau, Alexandra Cousteau, Philippe Cousteau Jr., C\u00e9line Cousteau. Jacques Cousteau's grandchildren were Philippe Cousteau Jr., Alexandra ousteau, C\u00e9line Cousteau, and Fabien Cousteau However, the user is not asking about the names of Cousteau's grand-children and has to guess by himself that the answer for this question is four.\nA more accurate answer should indicate the exact answer to the question and then elaborate Jacques Cousteau had four grand-children. But these systems perform better in case when the terms employed in the question are not necessarily relevant to the answer. If we take the example of the question who is the wife of Lance Bass, the approach that we propose will generate The wife of Lance Bass is Michael Turchin. As we can note the answer generated was not adapted to the actual answer, while the other systems are able to detect such nuance:\nLance Bass is married to Michael Turchin. They have been married since 2014.\nThis issue has still to be addressed.", "n_publication_ref": 4, "n_figure_ref": 5}, {"heading": "Conclusion and perspectives", "text": "We have put forward, in this paper, an approach for Natural Language Generation within the framework of the question-answering task that considers dependency analysis and probability distribution of words sequences. This approach takes part of a question/answering system in order to help generate a user-friendly answer rather than a short one. The results obtained through a human evaluation and standard metrics tested over French and English questions are very promising and shows a good correlation with human judgement. However, we intend to put more emphasis on the Language Model choice as reported by the human study and consider the generation of more than one missing word within the answer.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Snowball: Extracting relations from large plain-text collections", "journal": "", "year": "2000", "authors": "Eugene Agichtein; Luis Gravano"}, {"title": "Multilingual extractive reading comprehension by runtime machine translation", "journal": "", "year": "2018", "authors": "Akari Asai; Akiko Eriguchi; Kazuma Hashimoto; Yoshimasa Tsuruoka"}, {"title": "Yiteng Huang, and Israel Cohen", "journal": "Springer", "year": "2009", "authors": "Jacob Benesty; Jingdong Chen"}, {"title": "A hybrid question answering system for Multiple Choice Question (MCQ)", "journal": "", "year": "2013", "authors": "Pinaki Bhaskar; Somnath Banerjee; Partha Pakray; Samadrita Banerjee; Sivaji Bandyopadhyay; Alexander Gelbukh"}, {"title": "An analysis of the AskMSR question-answering system", "journal": "ACL", "year": "2002", "authors": "Eric Brill; Susan Dumais; Michele Banko"}, {"title": "Data-intensive question answering", "journal": "", "year": "2001", "authors": "Eric Brill; Jimmy Lin; Michele Banko; Susan Dumais; Andrew Ng"}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "journal": "", "year": "2016", "authors": "Sumit Chopra; Michael Auli; Alexander M Rush"}, {"title": "Unsupervised Cross-lingual Representation Learning at Scale", "journal": "", "year": "2019", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grace; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Stanford's Graph-based Neural Dependency Parser at the CoNLL 2017 Shared Task", "journal": "Vancouver", "year": "2017", "authors": "Timothy Dozat; Peng Qi; Christopher D Manning"}, {"title": "Harvesting paragraph-level question-answer pairs from Wikipedia", "journal": "", "year": "2018", "authors": "Xinya Du; Claire Cardie"}, {"title": "Research and reviews in question answering system", "journal": "", "year": "2013", "authors": "K Sanjay; Vaishali Dwivedi;  Singh"}, {"title": "Automatic detection of causal relations for question answering", "journal": "ACL", "year": "2003", "authors": "Roxana Girju"}, {"title": "Hybrid enhanced Universal Dependencies parsing", "journal": "", "year": "2020", "authors": "Johannes Heinecke"}, {"title": "Natural language question answering: the view from here", "journal": "natural language engineering", "year": "2001", "authors": "Lynette Hirschman; Robert Gaizauskas"}, {"title": "Exploiting background knowledge in compact answer generation for why-questions", "journal": "", "year": "2019", "authors": "Ryu Iida; Canasai Kruengkrai; Ryo Ishida; Kentaro Torisawa; Jong-Hoon Oh; Julien Kloetzer"}, {"title": "Semi-distantly supervised neural model for generating compact answers to open-domain why questions", "journal": "", "year": "2018", "authors": "Ryo Ishida; Kentaro Torisawa; Jong-Hoon Oh; Ryu Iida; Canasai Kruengkrai; Julien Kloetzer"}, {"title": "Improving event causality recognition with multiple background knowledge sources using multi-column convolutional neural networks", "journal": "", "year": "2017", "authors": "Canasai Kruengkrai; Kentaro Torisawa; Chikara Hashimoto; Julien Kloetzer; Jong-Hoon Oh; Masahiro Tanaka"}, {"title": "Context and page analysis for improved web search", "journal": "IEEE Internet computing", "year": "1998", "authors": "Steve Lawrence; C. Lee Giles"}, {"title": "Alexandre Allauzen, Beno\u00eet Crabb\u00e9, Laurent Besacier, and Didier Schwab. 2020. FlauBERT: Unsupervised Language Model Pre-training for French", "journal": "", "year": "", "authors": "Hang Le; Lo\u00efc Vial; Jibril Frej; Vincent Segonne; Maximin Coavoux; Benjamin Lecouteux"}, {"title": "Answer extraction based on merging score strategy of hot terms", "journal": "Chinese Journal of Electronics", "year": "2016", "authors": "Juan Le; Chunxia Zhang; Zhendong Niu"}, {"title": "", "journal": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Mandar Du; Danqi Jingfei Adn Joshi; Mike Chen; Luke Lewis; Veselin Zettlemoyer;  Stoyanov"}, {"title": "Is question answering fit for the semantic web?: a survey", "journal": "", "year": "2011", "authors": "Vanessa Lopez; Victoria Uren; Marta Sabou; Enrico Motta"}, {"title": "\u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah, and Beno\u00eet Sagot", "journal": "", "year": "2019", "authors": "Louis Martin; Benjamin Muller; Pedro Javier Ortiz Su\u00e1rez; Yoann Dupont; Laurent Romary"}, {"title": "Language as a latent variable: Discrete generative models for sentence compression", "journal": "", "year": "2016", "authors": "Yishu Miao; Phil Blunsom"}, {"title": "A correction: The use of ranks to avoid the assumption of normality implicit in the analysis of variance", "journal": "Journal of the American Statistical Association", "year": "1939", "authors": "Friedman Milton"}, {"title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond", "journal": "", "year": "2016", "authors": "Ramesh Nallapati; Bowen Zhou;  Cicero Dos Santos; Bing Aglar Gul\u00e7ehre;  Xiang"}, {"title": "Universal Dependency Evaluation", "journal": "", "year": "2017", "authors": "Joakim Nivre; Chiao-Ting Fang"}, {"title": "Universal Dependencies v1: A Multilingual Treebank Collection", "journal": "", "year": "2016", "authors": "Joakim Nivre; Marie-Catherine De Marneffe; Filip Ginter; Yoav Goldberg; Yoav Goldberg; Jan Haji\u010d; Manning Christopher; D ; Ryan Mcdonald; Slav Petrov; Sampo Pyysalo"}, {"title": "A semi-supervised learning approach to whyquestion answering", "journal": "", "year": "2016", "authors": "Jong-Hoon Oh; Kentaro Torisawa; Chikara Hashimoto; Ryu Iida; Masahiro Tanaka; Julien Kloetzer"}, {"title": "Why-question answering using intra-and inter-sentential causal relations", "journal": "", "year": "2013", "authors": "Jong-Hoon Oh; Kentaro Torisawa; Chikara Hashimoto; Motoki Sano; Kiyonori Stijn De Saeger;  Ohtake"}, {"title": "Answering naturally: Factoid to full length answer generation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Vaishali Pal; Manish Shrivastava; Irshad Bhat"}, {"title": "Improving language understanding by generative pre-training", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan"}, {"title": "Emmanuel Mory, and Fr\u00e9d\u00e9ric Herl\u00e9dan. 2019. Spoken Conversational Search for General Knowledge", "journal": "", "year": "", "authors": "Lina M Rojas Barahona; Pascal Bellec; Beno\u00eet Besset; Martinho Dos Santos; Johannes Heinecke; Munshi Asadullah; Olivier Leblouch; Jean-Yves Lancien; G\u00e9raldine Damnati"}, {"title": "A neural attention model for abstractive sentence summarization", "journal": "", "year": "2015", "authors": "Sumit Alexander M Rush; Jason Chopra;  Weston"}, {"title": "", "journal": "", "year": "2016", "authors": "Santos Cicero Dos; Ming Tan; Bing Xiang; Bowen Zhou"}, {"title": "Hard Time Parsing Questions: Building a QuestionBank for French", "journal": "", "year": "2016", "authors": "Djam\u00e9 Seddah; Marie Candito"}, {"title": "Get to the point: Summarization with pointer-generator networks", "journal": "", "year": "2017", "authors": "Abigail See; J Peter; Christopher D Liu;  Manning"}, {"title": "Creating causal embeddings for question answering with minimal supervision", "journal": "", "year": "2016", "authors": "Rebecca Sharp; Mihai Surdeanu; Peter Jansen; Peter Clark; Michael Hammond"}, {"title": "UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task", "journal": "", "year": "2018", "authors": "Milan Straka"}, {"title": "Improved representation learning for question answer matching", "journal": "", "year": "2016", "authors": "Ming Tan; Bing Cicero Dos Santos; Bowen Xiang;  Zhou"}, {"title": "", "journal": "", "year": "2017", "authors": "Ricardo Usbeck; Axel-Cyrille Ngonga Ngomo; Bastian Haarmann; Anastasia Krithara; Michael R\u00f6der; Giulio Napolitano"}, {"title": "Semantic Web Challenges", "journal": "Springer International Publishing", "year": "", "authors": ""}, {"title": "Learning to rank for why-question answering", "journal": "Information Retrieval", "year": "2011", "authors": "Suzan Verberne; Daphne Hans Van Halteren; Stephan Theijssen; Lou Raaijmakers;  Boves"}, {"title": "Machine comprehension using match-lstm and answer pointer", "journal": "", "year": "2016", "authors": "Shuohang Wang; Jing Jiang"}, {"title": "A joint model for question answering and question generation", "journal": "", "year": "2017", "authors": "Tong Wang; Xingdi Yuan; Adam Trischler"}, {"title": "Question answering by pattern matching, web-proofing, semantic form proofing", "journal": "", "year": "2003", "authors": "Min Wu; Xiaoyu Zheng; Michelle Duan; Ting Liu; Tomek Strzalkowski; S Albany"}, {"title": "Concept relation extraction using na\u00efve bayes classifier for ontologybased question answering systems", "journal": "Journal of King Saud University-Computer and Information Sciences", "year": "2015", "authors": "Godandapani Zayaraz"}, {"title": "Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies", "journal": "", "year": "2018", "authors": "Daniel Zeman; Jan Haji\u010d; Martin Popel; Martin Potthast; Milan Straka; Filip Ginter; Joakim Nivre; Slav Petrov"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Experiment framework", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "which allows to detect the performance variation of different configurations of a model evaluated by several metrics based on the average ranks.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Correlation assessment between human evaluation and the Bleu, Meteor, Rouge and Bert scores -French Q/A (\"CmBert\" stands for CamemBERT)", "figure_data": ""}, {"figure_label": "4405", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 : 40 rankFigure 5 :4405Figure 4: Extract of a human evaluation result", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Distribution of generation errors", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Dependency Analysis for English and French (UD v2.2) using different contextual word embeddings, best results in bold", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Dependency Analysis of questions using models trained on the standard UD treebanks annotate additional sentences (quiz-like questions)", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Dependency analysis of questions using models trained on enriched UD treebanks", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Pearson Correlation of the four metrics with the human evaluation/judgement", "figure_data": "% of all errors20 40 60English French0extra wordsgrammar missing wordsprepo-sitionsorder worderror categories"}], "doi": "10.1007/978-3-642-00296-0_5"}