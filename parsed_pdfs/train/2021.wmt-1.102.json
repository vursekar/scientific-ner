{"authors": "Chrysoula Zerva; Daan Van Stigt; Ricardo Rei; Ana C Farinha; Pedro G Ramos; Jos\u00e9 G C De Souza; Taisiya Glushkova; Miguel Vera; F\u00e1bio Kepler; Andr\u00e9 F T Martins; Instituto Superior T\u00e9cnico; Instituto De Telecomunica\u00e7\u00f5es", "pub_date": "", "title": "IST-Unbabel 2021 Submission for the Quality Estimation Shared Task", "abstract": "We present the joint contribution of IST and Unbabel to the WMT 2021 Shared Task on Quality Estimation. Our team participated on two tasks: Direct Assessment and Post-Editing Effort, encompassing a total of 35 submissions. For all submissions, our efforts focused on training multilingual models on top of OpenKiwi predictor-estimator architecture, using pre-trained multilingual encoders combined with adapters. We further experiment with and uncertainty-related objectives and features as well as training on out-ofdomain direct assessment data.", "sections": [{"heading": "Introduction", "text": "Quality estimation (QE) is the task of evaluating a translation system's quality without access to reference translations (Blatz et al., 2004;Specia et al., 2018). This paper describes the joint contribution of Instituto Superior T\u00e9cnico (IST) and Unbabel to the WMT21 Quality Estimation shared task (Specia et al., 2021), where systems were submitted to two tasks: 1) sentence-level direct assessment; 2) word-and sentence-level post-editing effort. This year's submission combines several ideas built on top of the OpenKiwi framework. Motivated by the mixture of blind and seen language pairs in the test sets, we experimented with extensions that would allow us to train multilingual models that maintain good generalization ability and are robust to the presence of epistemic and aleatoric uncertainty.\nFor both tasks we trained and submitted an ensemble of multilingual models. All submitted models follow the predictor-estimator architecture (Kim and Lee, 2016;Kim et al., 2017) and use pretrained models for feature extraction. Also, we fine-tune all models on the provided QE data using stacked adapter layers (Pfeiffer et al., 2020). * The first three authors have equal contribution.\nWe show that we can thus achieve comparable performance across language pairs while minimising the number of trainable parameters (see Table 1). Furthermore, we experimented with different types of uncertainty-related information to leverage it's benefits, improving performance and robustness of the submitted systems (see \u00a73.1.1). All related code extensions will be publicly available.\nOur main contributions are:\n\u2022 We build on our OpenKiwi architecture by exploring adapter layers (Houlsby et al., 2019;Pfeiffer et al., 2020) for quality estimation as these demonstrated to be less amenable to overfitting while presenting the same or superior quality performance than fine-tuning the whole base pre-trained model for different NLP tasks (He et al., 2021).\n\u2022 We incorporate different types of uncertainty into our architectures. We make use of the glass-box features (Fomicheva et al., 2020) extracted from the NMT models, the aleatoric (data) uncertainty derived from the human annotations and the epistemic (model) uncertainty (Hora, 1996;Kiureghian and Ditlevsen, 2009;Huellermeier and Waegeman, 2021) that originates from the QE model.\n\u2022 We show that training the QE models on additional out-of-domain direct assessment (DA) data gives considerable gains in performance for the new language pairs from the blind test sets.", "n_publication_ref": 13, "n_figure_ref": 0}, {"heading": "Quality Estimation Tasks", "text": "In this year's shared task edition we submitted models for the first two tasks:\n1. Task 1: sentence-level direct assessment 2. Task 2: word-and sentence level post-editing effort, comprising of two subtasks: a) predicting the HTER score of the translated sentence (hypothesis); and b) predicting OK/BAD tags for the words and gaps (both in source and translation)\nWe note that this year, both tasks 1 and 2 provided additional blind test sets with language pairs that were not included in the data made available for training/development, providing an interesting challenge and motivating multilingual and generalisable approaches.\n3 Implemented Systems", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Task 1", "text": "For Task 1 our final submission consisted of an ensemble of two different multilingual models, that differ in the way they process the input source (original sentence) and hypothesis (machine translation). Both models are based on the predictor-estimator architecture, using different pre-trained models to extract features and different training approaches to optimise for the QE task.\nThe key idea explored with our first model (denoted by M1 variations in the experiments), revolved around pursuing highly generalisable multilingual models, robust to overfitting. To this end, we train a cross-lingual transformer (XLM-RoBERTa (Conneau et al., 2020)) on large, multilingual data with direct assessments and then use adapters (Houlsby et al., 2019;Pfeiffer et al., 2020) to adapt to the domain specific data of the QE task with minimal training effort. In line with our efforts for good generalisation, we use only task-specific adapters and refrain from using specific adapters for each language pair. For these experiments we build on the OpenKiwi architecture (Kepler et al., 2019), using a pre-trained xlm-roberta-large encoder as a feature predictor. The source and hypothesis sentences are jointly encoded with hypothesis first. Then, source and hypothesis features are generated using average pooling over the hypothesis embeddings and forwarded to the estimator module which corresponds to a feed-forward layer. Figure 1 provides the general architecture 1 The model was first trained on the direct assessment data provided in the Metrics shared tasks (Mathur et al., 2020), as described in \u00a73.1.2. Upon training, the XML-R encoder is frozen and the the model is fine-tuned on sentence regression with the task-specific data, using stacked adapters. We hence manage to maintain a low number of trainable parameters during fine-tuning and minimize training time while learning to predict task-specific sentence scores.\nFor the second model (denoted by M2-KL-G-MCD) we aimed to explore the potential of a large pre-trained multilingual model (trained with MT objectives). We use the mBART (Liu et al., 2020) encoder-decoder architecture to encode the source and force-decode the hypothesis. We specifically use the mBART50 model (Tang et al., 2020) which is trained with multilingual finetuning on 50 languages, including all languages of interest for the QE 2021 task. We obtain the features by averaging the decoder embeddings and concatenating with the <eos> token of the sequence. The estimator part of the model consists of a bottleneck feed-forward layer that reduces the dimensionality of the decoder output, and is concatenated with a vector with additional glass-box features from the NMT models (see \u00a73.1.1). The combined vector is then forwarded to a feed-forward estimator and the full model is fine-tuned on the task specific QE data. Apart from the glass-box features we experimented further with methods that allow the model to be more robust towards the underlying uncertainty of its predictions. We elaborate that in the next section. Figure 2 provides a general architecture of the M2 model variations.", "n_publication_ref": 8, "n_figure_ref": 2}, {"heading": "Learning from uncertainty", "text": "Multiple neural models are involved in the process of obtaining and scoring machine translations, which naturally leads to several sources of uncertainty. These sources can be very informative and useful for MT evaluation. In this work we try to consider three types of uncertainty: (1) uncertainty of the NMT models used to obtain the hypotheses, (2) data (aleatoric) uncertainty for which we use the inter-annotator disagreement as a proxy, and (3) uncertainty of the MT evaluation model itself.\nNMT model uncertainty The idea of extracting uncertainty-related features from the MT systems in order to estimate the quality of their predictions, was originally introduced by Fomicheva et al. (2020). This glass-box approach to QE is mostly focusing on capturing epistemic uncertainty, and the proposed features are extracted either using Monte Carlo (MC) dropout on the NMT or using the output probability distributions obtained from a standard deterministic MT system. In our last year's submission (Moura et al., 2020) the integration of such features proved to be effective, thus we decided to incorporate it into our new model as well. We list the extracted features below:\n\u2022 TP sentence average of word translation probability -of MT output generated in different stochastic passes.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Aleatoric uncertainty", "text": "The noise and complexity of the training data is a source of predictive uncertainty in itself, referred to as data or aleatoric uncertainty (Kiureghian and Ditlevsen, 2009). This uncertainty is often reflected in the disagreement between human annotations for the same sourcehypothesis segment (Cohn and Specia, 2013;Fornaciari et al., 2021). We hypothesize that the direct assessments can be better modelled as normally distributed scores rather than a single score, and that a model trained to predict this distribution (mean and standard deviation) could provide better quality estimates 2 . We formalise this as a KL divergence objective, using the closed form solution to estimate the KL divergence between the target distribution p(x) = N (\u00b5 1 , \u03c3 1 ) and the predicted distribution q(x) = N (\u00b5 2 , \u03c3 2 ), as shown in Eq. 1.\nKL(p||q) = log \u03c3 2 \u03c3 1 + \u03c3 2 1 + (\u00b5 1 \u2212 \u00b5 2 ) 2 2\u03c3 2 2 \u2212 1 2 (1)\nwhere we take the mean and standard deviation (std) of the direct assessment z_scores as the target (ground truth proxy) values p. This way, we account for the annotator disagreement (reflected in the std value) during learning. QE epistemic uncertainty We use MC dropout (Gal and Ghahramani, 2016) to account for the uncertainty of the QE model. Specifically, we enable dropout during inference and run multiple forward runs over each test instance. Thus we obtain a distribution of quality predictions for each instance instead of a single point estimate. We use the estimated mean of the distribution as our predicted quality estimate. MC dropout has been shown to improve predictive accuracy and perform on par or even better compared to deep ensembles for MT evaluation tasks (Glushkova et al., 2021). It thus allows us to simulate ensembling in a cheap and effective way, without the need to train multiple checkpoints.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Out-of-domain direct assessment data", "text": "The QE data is relatively limited, making it harder to train multilingual models with a large number of parameters without over-fitting. Thus, as explained in \u00a73.1 we aimed to investigate whether we could obtain models that generalise better and are more robust to noise and out-of-distribution data by training the XLM-RoBERTa model first on a larger -yet noisier and out-of-domain dataset. To that end we leverage the data provided for the past Metrics shared tasks, which covers the language pairs used in this year's QE task, including the blind tests for which we had no in-domain data available. Altogether, it encompasses 30 language pairs from the news domain (versus 7 in the QE dataset). We provide more detailed statistics for each language pair of the Metrics data in Appendix C. We refer to experiments using the model initially trained on the Metrics data as M1M-. We also show that using the trained XLM-RoBERTa encoder from the M1M model can prove beneficial for the predictions on post-edited data of Task 2 (see Table 3).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Task 2", "text": "For Task 2 we submitted an ensemble of two variations of the first model (M1-ADAPT and M1M-ADAPT) presented for Task 1 (see \u00a73.1). In both cases, we use multi-task training and a feedforward for each output types: hypothesis word tags, hypothesis gap tags, source word tags, and sentence regression (on HTER scores). Both variations use a pre-trained XLM-RoBERTa (large) encoder to extract features as described for Task 1, but differ in the training of the encoder. In the first case we use the pre-trained model 3 and finetune on the QE data using stacked adapters. In the second variation we swap the original pre-trained model with the XLM-RoBERTa model that has been trained on the Metrics data as described in \u00a73.1.2. We note that the two variations favor different language pairs, hence we combine multiple checkpoints from each variation (ranging training steps). We use the test-20 split of the data to optimise the hyper-parameters and following this approach we use the estimated top-3 checkpoints from each variation using the combined dataset 4 and the top checkpoint for the non-augmented model trained exclusively on the train set, resulting in total 7 checkpoints in our final ensemble.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Results", "text": "We present the performance of the implemented models on the test-20 dataset.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Task 1", "text": "The results can be seen in Tables 1 and 2. In line with the shared task guidelines we treat Pearson r as the primary performance metric and select the submitted models accordingly. We can observe, that while on average the M1 model and its variations outperform the M2 model, their performance is comparable, and M2-KL-G-MCD can even outperform M1M-ADAPT for specific language pairs, hence it made sense to combine them in the final ensemble. We can also see that fine-tuning the M1 model on the Metrics data, results in performance gains for the majority of the language pairs. Specifically, even applying the M1M directly, without further fine-tuning on QE data, achieves competitive performance for most pairs, which further improves upon fine-tuning. It helps in increasing the performance on the blind sets (denoted as zeroshot in the Appendix B  assessments for each segment. Thus, the difference in target score range and distribution could affect the magnitude of predicted scores and the distance to the ground truth values, which is reflected in the MAE and RMSE metrics. These findings, further supported by the results on Task 2, is a first step in exploring the underlying connection and bridging the gap between the Metrics and Quality Estimation shared tasks.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Task 2", "text": "The results can be seen in Table 3. Similarly to Task 1, the primary evaluation metric for the sentence level sub-task of Task 2 is the Pearson r coefficient,   2: Results for Task 1 with the M2 predictorestimator (mBART) and different uncertainty handling additions. \"KL\" signifies the incorporation of KL loss, \"G\"the incorporation of glass-box features and MCD the addition of MC dropout. ML stands for MULTILIN-GUAL, showing the performance averaged over all language pairs. Underlined numbers indicate the best result for each language pair and evaluation metric. Bold systems were selected for the final ensemble.\nwhile the word level sub-task is evaluated using the Matthews correlation coefficient (MCC, (Matthews, 1975)) as the primary performance indicator.\nWe can see that while HTER scores do not always correlate highly with DAs (see Table 4), the use of the M1M model encoder that was trained on large data with direct assessments can still prove useful. Indeed, when fine-tuning on the Task2 data, the model using the M1M encoder (M1M-ADAPT in the table 3) provides a performance boost for the Pearson correlation in most language pairs, and competitive performance for the rest. Based on these results, we deem it worthwhile to include checkpoints trained with this configuration in the ensemble estimating that they will contribute in higher performance, especially on the blind test sets. This can be further confirmed when   ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Conclusions", "text": "We presented a joint contribution of IST and Unbabel to the WMT 2021 QE shared task. Our submissions are ensembles of multilingual checkpoints extending the OpenKiwi framework. We found adapter-tuning to be suitable for fine-tuning OpenKiwi on the QE tasks data and less prone to overfitting. We showed that pre-training on large, out-of-domain annotated data can prove beneficial both for the direct assessment and the postediting QE tasks. We also demonstrated that handling uncertainty-related sources of information improves the performance when integrated into the QE system. For Task 2 we do multi-task training based on the models from the previous task and use multiple checkpoints to create the submitted ensemble.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Hyperparameters", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 M1", "text": "In  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.2 M2", "text": "In Table 6 is an excerpt of the training configuration used for training the M2 models using the mBART encoder-decoder:", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B Evaluation on test set of WMT21", "text": "We present the performance of the submitted ensembles on the TEST-21 dataset as calculated in the official QE results 6 for each task and sub-task. We also provide the comparison with the organisers' baseline.  The results for Task1 on TEST-21 are presented in Table 7.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B.2 Task 2: HTER prediction at sentence-level", "text": "The results for Task2 on TEST-21TEST-21 are presented in Table 8, showing the performance for the sentence level, HTER score predictions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B.3 Task 2: Word-level prediction", "text": "The results for Task2 on TEST-21 are presented in Table 9, showing the performance for the word tag predictions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C Statistics on the Metrics data", "text": "We present below (Tables 10 and 11) the statistics on the Metrics data used to train the M1M model on direct assessments.   ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "We are grateful to Alon Lavie and Craig Stewart for their valuable feedback and discussions. This work was supported by the P2020 programs MAIA (contract 045909) and Unbabel4EU (contract 042671), by the European Research Council (ERC StG Deep-SPIN 758969), and by the Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia through contract UIDB/50008/2020.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Confidence estimation for machine translation", "journal": "", "year": "2004", "authors": "John Blatz; Erin Fitzgerald; George Foster; Simona Gandrabur; Cyril Goutte; Alex Kulesza"}, {"title": "Modelling annotator bias with multi-task Gaussian processes: An application to machine translation quality estimation", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Trevor Cohn; Lucia Specia"}, {"title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Unsupervised quality estimation for neural machine translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Marina Fomicheva; Shuo Sun; Lisa Yankovskaya; Fr\u00e9d\u00e9ric Blain; Francisco Guzm\u00e1n; Mark Fishel; Nikolaos Aletras; Vishrav Chaudhary; Lucia Specia"}, {"title": "Beyond black & white: Leveraging annotator disagreement via soft-label multi-task learning", "journal": "", "year": "2021", "authors": "Tommaso Fornaciari; Alexandra Uma; Silviu Paun; Barbara Plank; Dirk Hovy; Massimo Poesio"}, {"title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "journal": "", "year": "2016", "authors": "Yarin Gal; Zoubin Ghahramani"}, {"title": "Uncertainty-Aware Machine Translation Evaluation", "journal": "", "year": "2021", "authors": "Taisiya Glushkova; Chrysoula Zerva; Ricardo Rei; Andr\u00e9 F T Martins"}, {"title": "On the effectiveness of adapterbased tuning for pretrained language model adaptation", "journal": "Long Papers", "year": "2021", "authors": "Ruidan He; Linlin Liu; Hai Ye; Qingyu Tan; Bosheng Ding; Liying Cheng; Jiawei Low; Lidong Bing; Luo Si"}, {"title": "Aleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management. Reliability Engineering & System Safety", "journal": "", "year": "1996", "authors": "C Stephen;  Hora"}, {"title": "Parameter-efficient transfer learning for NLP", "journal": "PMLR", "year": "2019", "authors": "Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin De Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly"}, {"title": "Aleatoric and epistemic uncertainty in machine learning : an introduction to concepts and methods", "journal": "", "year": "2021", "authors": "Eyke Huellermeier; Willem Waegeman"}, {"title": "OpenKiwi: An open source framework for quality estimation", "journal": "", "year": "2019", "authors": "Fabio Kepler; Jonay Tr\u00e9nous; Marcos Treviso; Miguel Vera; Andr\u00e9 F T Martins"}, {"title": "A recurrent neural networks approach for estimating the quality of machine translation output", "journal": "", "year": "2016", "authors": "Hyun Kim; Jong-Hyeok Lee"}, {"title": "Predictor-estimator using multilevel task learning with stack propagation for neural quality estimation", "journal": "", "year": "2017", "authors": "Hyun Kim; Jong-Hyeok Lee; Seung-Hoon Na"}, {"title": "Aleatory or epistemic? does it matter? Structural Safety", "journal": "", "year": "2009", "authors": "Armen Der Kiureghian; Ove Ditlevsen"}, {"title": "The meteor metric for automatic evaluation of machine translation", "journal": "", "year": "2009", "authors": "Alon Lavie; Michael Denkowski"}, {"title": "Multilingual denoising pre-training for neural machine translation", "journal": "", "year": "2020", "authors": "Yinhan Liu; Jiatao Gu; Naman Goyal; Xian Li; Sergey Edunov; Marjan Ghazvininejad; Mike Lewis; Luke Zettlemoyer"}, {"title": "Results of the WMT20 metrics shared task", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Nitika Mathur; Johnny Wei; Markus Freitag; Qingsong Ma; Ond\u0159ej Bojar"}, {"title": "Comparison of the predicted and observed secondary structure of t4 phage lysozyme", "journal": "Biochimica et Biophysica Acta (BBA)-Protein Structure", "year": "1975", "authors": "W Brian;  Matthews"}, {"title": "ISTunbabel participation in the WMT20 quality estimation shared task", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Jo\u00e3o Moura; Miguel Vera; Fabio Daan Van Stigt; Andr\u00e9 F T Kepler;  Martins"}, {"title": "AdapterHub: A framework for adapting transformers", "journal": "", "year": "2020", "authors": "Jonas Pfeiffer; Andreas R\u00fcckl\u00e9; Clifton Poth; Aishwarya Kamath; Ivan Vuli\u0107; Sebastian Ruder; Kyunghyun Cho; Iryna Gurevych"}, {"title": "2021. Findings of the wmt 2021 shared task on quality estimation", "journal": "", "year": "", "authors": "Lucia Specia; Fr\u00e9d\u00e9ric Blain; Marina Fomicheva; Chrysoula Zerva; Zhenhao Li; Vishrav Chaudhary; Andr\u00e9 F T Martins"}, {"title": "Quality estimation for machine translation", "journal": "Synthesis Lectures on Human Language Technologies", "year": "2018", "authors": "Lucia Specia; Carolina Scarton; Gustavo Henrique Paetzold"}, {"title": "Multilingual translation with extensible multilingual pretraining and finetuning", "journal": "", "year": "2020", "authors": "Yuqing Tang; Chau Tran; Xian Li; Peng-Jen Chen; Naman Goyal; Vishrav Chaudhary; Jiatao Gu; Angela Fan"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: General architecture of M1 model variations. Word tag prediction is used only for Task 2.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: General architecture of M2 model variations.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "\u2022Softmax-Ent sentence average of softmax output distribution entropy \u2022 Sent-Std sentence standard deviation of word probabilities \u2022 D-TP average TP across N(N = 30) stochastic forward-passes \u2022 D-Var variance of TP across N stochastic forward-passes \u2022 D-Combo combination of D-TP and D-Var defined by 1 \u2212 D \u2212 T P/D \u2212 V ar\u2022 D-Lex-Sim lexical similarity -measured by METEOR score(Lavie and Denkowski, 2009) ", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Results for Task 1 with the M1 predictorestimator (XLM-RoBERTa) and different training/finetuning approaches. M1M is the M1 model trained on the Metrics dataset and M#-ADAPT signifies a model fine-tuned on the QE data with adapters. ML stands for MULTILINGUAL, showing the performance averaged over all language pairs. Underlined numbers indicate the best result for each language pair and evaluation metric. Bold systems were selected for the final ensemble.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ": Results for Task 2 with the M1 predictor-estimator (XLM-RoBERTa) and different training/fine-tuning approaches. M1M is the M1 model trained onthe Metrics dataset and M#-ADAPT signifies a modelfine-tuned on the QE data with adapters. ML stands forMULTILINGUAL, showing the performance averagedover all language pairs. Underlined numbers indicatethe best result for each language pair and evaluationmetric. Bold systems were selected for the final ensem-ble.inspecting the results for the blind sets (en-cs,en-ja, km-en and ps-en) in the official resultson test-21 as shown in Appendix B.lpTRAINDEVTEST-20EN-DE -0.1654 -0.4032 -0.3850EN-ZH -0.2947 -0.1895 -0.1932ET-EN -0.5464 -0.5850 -0.5995NE-EN -0.4527 -0.5004 -0.4558RO-EN -0.5887 -0.7932 -0.7880RU-EN -0.5358 -0.5055 -0.5152SI-EN-0.3916 -0.4384 -0.4125"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Pearson correlation between the z_mean of the direct assessments for the QE Task 1 data and the HTER score for the post edits in QE Task 2 data.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "is an excerpt of the training configuration used for training OpenKiwi for our M1 models. Note that the configurations follow the configuration file format of OpenKiwi and any additional configurations are identical to the ones proposed in the sample configuration file of the github repository 5 .", "figure_data": "Systembatch_size2Encoderhidden_size1024Decoderbottleneck_size1024dropout0.05hidden_size1024Optimizerclass_nameadamencoder_learning_rate0.0001learning_rate_decay1.0learning_rate_decay_start0learning_rate0.0001Trainertraining_steps2180early_stop_patience10validation_steps0.5gradient_accumulation_steps 4gradient_max_norm1.0"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Hyperparameters for M1 models", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Hyperparameters for M2 models", "figure_data": "B.1 Task 1: Direct Assessments prediction atsentence-level"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Statistics for the WMT 15 to 20 Direct Assessments corpus into-English language pairs. EN-RU EN-CS EN-DE EN-FI EN-LV EN-TR EN-ZH EN-ET EN-LT EN-GU EN-KK EN-JA EN-PL EN-TA", "figure_data": "Total tuples637716090555352309245810517166830133768959692482199573105067886Avg. tokens (reference) 22.4823.4823.9617.720.4519.747.2618.8320.6122.0719.211.424.5419.84Avg. tokens (source)24.525.822423.2124.9924.228.8124.2324.0924.324.1325.225.3325.15Avg. tokens (MT)22.142323.8417.8121.1819.247.5318.9620.6222.3919.712.2923.1919.18"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Statistics for the WMT 15 to 20 Direct Assessments corpus from-English language pairs.", "figure_data": ""}], "doi": "10.18653/v1/2020.acl-main.747"}