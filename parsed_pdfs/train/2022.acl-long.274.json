{"authors": "Aliva Das; Xinya Du; Barry Wang; Kejian Shi; Jiayuan Gu; Thomas Porter; Claire Cardie", "pub_date": "", "title": "Automatic Error Analysis for Document-level Information Extraction", "abstract": "Document-level information extraction (IE) tasks have recently begun to be revisited in earnest using the end-to-end neural network techniques that have been successful on their sentence-level IE counterparts. Evaluation of the approaches, however, has been limited in a number of dimensions. In particular, the precision/recall/F1 scores typically reported provide few insights on the range of errors the models make. We build on the work of Kummerfeld and Klein (2013) to propose a transformation-based framework for automating error analysis in document-level event and (N-ary) relation extraction. We employ our framework to compare two state-of-theart document-level template-filling approaches on datasets from three domains; and then, to gauge progress in IE since its inception 30 years ago, vs. four systems from the MUC-4 (1992) evaluation. 1   ", "sections": [{"heading": "Introduction", "text": "Although information extraction (IE) research has almost uniformly focused on sentence-level relation and event extraction (Grishman, 2019), the earliest research in the area formulated the task at the document level. Consider, for example, the first large-scale (for the time) evaluations of IE systems -e.g. MUC-3 (1991) and MUC-4 (1992). Each involved a complex document-level event extraction task: there were 24 types of events, over a dozen event arguments (or roles) to be identified for each event; documents could contain zero to tens of events, and extracting argument entities (or role fillers) required noun phrase coreference resolution to ensure interpretability for the end-user (e.g. to ensure that multiple distinct mentions of the same entity in the output were not misinterpreted as references to distinct entities).\nThe task was challenging: information relevant for a single event could be scattered across the document or repeated in multiple places; relevant information might need to be shared across multiple events; information regarding different events could be intermingled. In Figure 1, for example, the DISEASE \"Newcastle\" is mentioned well before its associated event is mentioned (via the triggering phrase \"the disease has killed\"); that same mention of \"Newcastle\" must again be recognized as the DISEASE in a second event; and the COUNTRY of the first event (\"Honduras\") appears only in the sentence describing the second event.\nIn fact, the problem of document-level information extraction has only recently begun to be revisited (Quirk and Poon, 2017;Jain et al., 2020;Du et al., 2021b,a;Li et al., 2021;Du, 2021;Yang et al., 2021) in part in an attempt to test the power of end-to-end neural network techniques that have been so successful on their sentence-level counterparts. 2 Evaluation, however, has been limited in a number of ways.\nFirst, despite the relative complexity of the task, approaches are only evaluated with respect to their overall performance scores (e.g. precision, recall, and F1). Even though scores at the role level are sometimes included, no systematic analysis or characterization of the types of errors that occur is typically done. The latter is needed to determine strategies to improve performance, to obtain more informative cross-system and cross-genre comparisons, and to identify and track broader advances in the field as the underlying approaches evolve. To date, for example, there has been no attempt to directly compare the error landscape and distribution of [Trigger] [Trigger] Input Document ", "n_publication_ref": 10, "n_figure_ref": 1}, {"heading": "Error Statistics", "text": "The Agriculture ministers of El Salvador and Honduras ... to control the spread of disease affecting poultry, like the virus Newcastle[Disease].\nUrrutia ... to study the Newcastle outbreak. The disease has killed close to half a million Honduran chickens[Victims] in recent weeks.\nHonduras[Country] said this week it would halt the importation of chickens and eggs from Guatemala[Country], where the disease has been detected earlier, and .....", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "--", "text": "Figure 1: The document-level extraction task from the ProMED dataset on disease outbreaks (left) and the automatic error analysis process (right). Our system performs a set of transformations on the predicted templates to convert them into the corresponding gold standard templates. Transformation steps are mapped to corresponding error types to produce informative error statistics.\nnewly developed neural IE methods with that of the largely hand-crafted systems of the 1990s.\nIn this work, we first introduce a framework for automating error analysis for document-level event and relation extraction, casting both as instances of a general role-filling, or template-filling task (Jurafsky and Martin, 2021). Our approach converts predicted system outputs into their gold standard counterparts through a series of template-level transformations (Figure 2) and then maps combinations of transformations into a collection of IE-based error types. Examples of errors include duplicates, missing and spurious role fillers, missing and spurious templates, and incorrect role and template assignments for fillers. (See Figure 3 for the full set).\nNext, we employ the error analysis framework in a comparison of two state-of-the-art documentlevel neural template-filling approaches, DyGIE++  and GTT (Du et al., 2021b), across three template-filling datasets (SciREX, ProMED (Patwardhan and Riloff, 2009) 3 , and MUC-4).\nFinally, in an attempt to gauge progress in the information extraction field over the past 30 years, we employ the framework to compare the performance of four of the original MUC-4 systems with the two newer deep-learning approaches to documentlevel IE. 4 We find that (1) the best of the early IE models -which strikes a better balance between precision and recall -outperforms modern models that exhibit much higher precision and much lower recall; (2) the modern neural models make more mistakes on scientific vs. news-oriented texts, and missing role fillers is universally the largest source of errors; and (3) modern models have clear advantages over the early IE systems in terms of accurate span extraction, while the early systems make fewer mistakes assigning role fillers to their roles.", "n_publication_ref": 3, "n_figure_ref": 3}, {"heading": "Related Work", "text": "Aside from the original MUC-4 evaluation scoring reports (Chinchor, 1991), which included counts of missing and spurious role filler errors, there have been very few attempts at understanding the types of errors made by IE systems and grounding those errors linguistically. Valls-Vargas et al. (2017) proposed a framework for studying how different errors propagate through an IE system; however, the framework can only be used for pipelined systems, not end-to-end ones.\nOn the other hand, automated error analysis with linguistically motivated error types has been used in other sub-fields of NLP such as machinetranslation (Vilar et al., 2006;Zhou et al., 2008;Farr\u00fas et al., 2010;Kholy and Habash, 2011;Zeman et al., 2011;Popovi\u0107 and Ney, 2011), coreference resolution (Uryupina, 2008;Kummerfeld and Klein, 2013;Martschat and Strube, 2014;Martschat et al., 2015) and parsing (Kummerfeld et al., 2012). Recently, generalized automated error analysis frameworks involving human-in-the-loop testing like Errudite (Wu et al., 2019), CHECK- LIST (Ribeiro et al., 2020), CrossCheck (Arendt et al., 2021), and AllenNLP Interpret (Wallace et al., 2019) have successfully been applied to tasks like machine comprehension and relation extraction (Alt et al., 2020). Closest to our work are Kummerfeld et al. (2012) and Kummerfeld and Klein (2013), which use model-agnostic transformationbased mapping approaches to automatically obtain error information in the predicted structured output.", "n_publication_ref": 20, "n_figure_ref": 0}, {"heading": "Template-Filling Task Specification and Evaluation", "text": "As in Jurafsky and Martin (2021), we will refer to document-level information extraction tasks as template-filling tasks and use the term going forward to refer to both event extraction and documentlevel relation extraction tasks. Given a document, D, and an IE template specification consisting of a predetermined list of roles R 1 , R 2 , ..., R i associated with each type of relevant event for the task of interest, the goal for template filling is to extract from D, one output template, T for every relevant event/relation e 1 , e 2 , . . . , e n present in the document. Notably, in the general case, n \u2265 0 and is not specified as part of the input. In each output template, its roles are filled with the corresponding role filler(s), which can be inferred or extracted from the document depending on the predetermined role types. We consider two role types here: 5 Set-fill roles, which must be filled with exactly one role filler from a finite set supplied in the template specification. An example of a set-fill role in Figure 1 is STATUS, which can be confirmed, possible, or suspected.\nString-fill roles, whose role filler(s) are spans extracted from the document, or left empty if no corresponding role filler is found in the document. VICTIMS, DISEASE and COUNTRY are string-fill roles in Figure 1. Some string-fill roles allow multiple fillers; for example, there might be more than one VICTIMS. Importantly, for document-level template filling, exactly one string should be included for each role filler entity (typically a canonical mention of the entity), i.e. coreferent mentions of the same entity are not permitted.\nEvaluation. We use the standard (exact-match) F1 score (Chinchor, 1991) to evaluate the output 5 There are potentially more role types depending on the dataset (e.g. normalized dates, times, locations); we will not consider those here.\nproduced by a template-filling system:\nF 1 = 2 \u2022 Precision \u2022 Recall Precision + Recall", "n_publication_ref": 4, "n_figure_ref": 2}, {"heading": "Methodology: Automatic Transformations for Error Analysis", "text": "Similar to the work of Kummerfeld and Klein (2013), our error analysis approach is systemagnostic, i.e. it only uses system output and does not consider intermediate system decisions. This allows for error analysis and comparison across different kinds of systems -end-to-end or pipeline; neural or pattern-based. Given inputs consisting of the system-predicted templates and gold standard templates (i.e. desired output) for every document in the target dataset, our error analysis tool operates in three steps. For each document, 1. Perform an optimized mapping of the associated predicted templates and gold templates.\n2. Apply a pre-defined set of transformations to convert each system-predicted template into the desired gold template, keeping track of the transformations applied.\n3. Map the changes made in the conversion process to an IE-based set of error types.\nWe describe each step in detail in the subsections below.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Optimized Matching", "text": "The first stage of the error analysis tool involves matching each system-predicted template to the best-matching gold template for each document in the dataset. In particular, the overall F1 score for a given document can vary based on how a predicted template is individually matched with a gold template (or left unmatched). Specifically, for each document, we recursively generate all possible template matchings -where each predicted template is matched (if possible) to a gold template. In particular, for a document with P predicted templates and G gold templates, the total number of possible template matchings is:  Note that template matching can result in unmatched predicted templates (Spurious Templates), as well as unmatched gold templates (Missing Templates).\n1 + P 1 G + P 2 G(G \u2212 1) + ... + G! (G \u2212 P )! , if G \u2212 P \u2265 0 1 + P 1 G + P 2 G(G \u2212 1) + ... + P G G!, if G \u2212 P < 0 = min(P,G) i=0 P i G! (G \u2212 i)!\nNext, for each predicted-gold pair in a template matching, we iterate through all its roles and recursively generate all possible mention matchings, in each of which a predicted role filler is matched (if possible) to a set of coreferent gold role fillers. Similar to template matching, the process of mention matching can also result in unmatched predicted role fillers (Spurious Role Fillers) and unmatched coreferent sets of gold role fillers (Missing Role Fillers).\nThrough the process, each predicted role filler increases the denominator of the total precision by 1, and each set of coreferent gold role fillers increases the denominator of total recall by 1. Whenever there is a matched mention pair in which the predicted role filler has an exact match to an element of the set of coreferent gold role fillers, this adds 1 to the numerator of both precision and recall. These counts are calculated for each template matching.\nUsing precision and recall, the total F1 score across all the slots/roles is calculated and the template matching with the highest total F1 score is chosen. If there are ties, the template matching with the fewest errors is chosen (see Section 4.3).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Transformations", "text": "The second part of the error analysis tool involves changing the predicted templates to the desired gold templates with the help of a fixed set of transformations as detailed below.\n\u2022 Alter Span transforms a role filler into the gold role filler with the lowest span comparison score (SCS). The tool provides two options for computing the SCS between two spans, and each depends only on the starting and ending indices of the spans. 6 SCS can be interpreted as distance and is 0 between two identical spans, and 1 for non-overlapping spans. The two modes are given as follows:\na) absolute: This mode captures the (positive) distance between the starting (and ending) character offsets of spans x and y in the document, and scales that value by the sum of the lengths of x and y, capping it at a maximum of 1.\nSCS = max 1, |xstart\u2212ystart|+|x end \u2212y end | length(x)+length(y) b) geometric mean:\nThis mode captures the degree of disjointedness between spans x and y by dividing the length of the overlap between the two spans with respect to each of their lengths, multiplying those two fractions, and subtracting the final result from 1.\nIf si is the length of the intersection of x and y, and neither x nor y have length 0, SCS is calculated as shown below; otherwise, SCS is 1.\noverlap = min(x end , y end ) \u2212 max(x start , y start ) si = max (0, overlap) SCS = 1 \u2212 si 2 length(x) * length(y)\nThus, if the predicted role filler is an exact match for the gold role filler, the SCS is 0. If there is some overlap between the spans, the SCS is between 0 and 1 (not inclusive), and if there is no overlap between the spans, the SCS is 1. The order of comparison of the spans doesn't change the SCS score for both modes.\nAs the absolute mode is less sensitive to changes in span indices as compared to the geometric mean, we chose geometric mean for our analysis, as tiny changes in index positions result in a bigger change in the SCS score.\n\u2022 Alter Role changes the role of a role filler to another role within the same template.\n\u2022 Remove Duplicate Role Filler removes a role filler that is coreferent to an already matched role filler.\n\u2022 Remove Cross Template Spurious Role Filler removes a role filler that would be correct if present in another template (in the same role).\n\u2022 Remove Spurious Role Filler removes a role filler that has not been mentioned in any of the gold templates for a given document.\n\u2022 Introduce Role Filler introduces a role filler that was not present in the predicted template but was required to be present in the matching gold template.\n\u2022 Remove Template removes a predicted template that could not be matched to any gold template for a given document.\n\u2022 Introduce Template introduces a template that can be matched to an unmatched gold template for a given document.\nFor a given document, all singleton Alter Span and Alter Role transformations, as well as sets of Alter Span + Alter Role transformations, are applied first. The other transformations are applied in the order in which they were detected, which is dependent on the order of predicted and gold template pairs in the optimized matching and the order of the slots/roles in the template.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Error Type Mappings", "text": "The transformations in Section 4.2 are mapped onto a set of IE-specific error types as shown in Figure 3. In some cases, a single transformation maps onto a single error, while in others a sequence of transformations is associated with a single error. Full details are in Appendix A.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Document-level IE Datasets", "text": "Our experiments employ three document-level information extraction datasets. We briefly describe each below. Dataset statistics are summarized in Table 1.\nMUC-4 (MUC-4, 1992) consists of newswire describing terrorist incidents in Latin America provided by the FBIS (Federal Broadcast Information Services). We converted the optional templates to required templates and removed the subtypes of the incidents as done in previous work (Chambers, 2013;Du et al., 2021b) so that the dataset is transformed into standardized templates. The roles chosen from the MUC-4 dataset are PERPIND (individual perpetrator), PERPORG (organization perpetrator), TARGET (physical target), VICTIM (human target), and WEAPON which are all string-fill roles, as well as INCIDENT  We focus specifically on its 4-ary relation extraction subtask. The roles present in each relation are MATERIAL (DATASET), METRIC, TASK, and METHOD which are all string-fills. We convert the dataset from its original format to templates for our models, and remove individual role fillers (entities) that have no mentions in the text. 11 We also remove any duplicate templates. 12 During preprocessing, we remove malformed words longer than 25 characters, as the majority of these consist of concatenated words that are not present in the corresponding text.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "IE Modeling Details", "text": "In our experiments, we train and test two neuralbased IE models, described briefly below, on the MUC-4, ProMED, and SciREX datasets. Note that   (2019) for the SciREX dataset and 11 tokens for the ProMED dataset. We use bert-base-cased and allenai/scibert_scivocab_uncased for the base BERT and SciBERT models respectively, which both have a maximum input sequence length of 512 tokens.\nTo aggregate entities detected by DyGIE++ into templates, we use a clustering algorithm. For the SciREX dataset, we adopt a heuristic approach that assumes there is only one template per document, and in that template, we assign the named entities predicted by DyGIE++ for a document to the predicted role types. For the ProMED dataset, we use a different clustering heuristic that ensures that each template has exactly one role filler for the COUNTRY and DISEASE roles, as detailed in the dataset annotation guidelines. Also, since STATUS has the value confirmed in the majority of the templates, every template predicted has its STATUS assigned as confirmed.\nGTT is an end-to-end document-level templategenerating model. For the MUC-4 and SciREX datasets, GTT is run for 20 epochs, while for ProMED it is run for 36 epochs, to adjust for the smaller size of the dataset. All other hyperparameters are set as in Du et al. (2021b). We use the same BERT and SciBERT base models as described in the DyGIE++ architecture above, both with a maximum input sequence length of 512 tokens.\nThe computational budget and optimal hyperparameters for these models can be found in Ap-pendix sections D and E, respectively.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Experimental Results and Analysis", "text": "We first discuss the results of DyGIE++ and GTT on SciREX, ProMED, and MUC-4; and then examine the performance of these newer neural models on the 1992 MUC-4 dataset vs. a few of the bestperforming IE systems at the time.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "DyGIE++ vs. GTT", "text": "Table 2 shows the results of evaluating DyGIE++ and GTT on the SciREX, ProMED, and MUC-4 datasets. We can see that all models perform substantially worse on scientific texts (ProMED, SciREX) as compared to news (MUC-4), likely because the model base is pretrained for generalpurpose NLP applications (BERT) or there are not enough examples of scientific-style text in the pretraining corpus (SciBERT). In addition, models seem to perform better on the news-style ProMED dataset than the scientific-paper-based long-text SciREX dataset. This can be explained by the fact that all four models handle a maximum of 512 tokens as inputs, while the average length of a SciREX document is 5401 tokens. Thus, a majority of the text is truncated and, hence, unavailable to the models.\nNevertheless, we see an increase in F1 scores for all SciBERT-based models when compared to their BERT counterparts for the SciREX dataset. The same trend is seen for DyGIE++ for ProMED, but not for GTT. This can be explained by the fact that GTT (SciBERT) has more Missing Template errors than GTT (BERT). So even if GTT (SciBERT) performs better on the scientific slot VICTIMS, i.e. it extracts more scientific information, it does not identify relevant events as well as GTT (BERT), reducing the F1 score across the remaining slots.\nFrom the error count results in Figure 4, we see that GTT makes fewer Missing Template errors than DyGIE++ on the MUC-4 dataset (86 vs. 97). However, there is no significant difference   in the number of missing templates between the two models on the ProMED and SciREX datasets. This could be because DyGIE++ is prone to overgeneration -there are significantly more Spurious Role Filler and Spurious Template errors as compared to the results of GTT. Since we use heuristics that create templates based on the extracted role fillers, this increases the probability that there was a possible match to a gold template, reducing the number of Missing Template Errors.\nWe can also conclude that DyGIE++ is worse at coreference resolution when compared to GTT as DyGIE++ makes more Duplicate Role Filler errors across all datasets.\nOverall, we find that the major source of error for both GTT and DyGIE++ across all the datasets is missing recall in the form of Missing Role Filler and Missing Template errors.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Early IE Models vs. DyGIE++ and GTT", "text": "Table 3 presents the precision, recall, and F1 performance on the MUC-4 dataset for early models from 1992 alongside those of the more recent DyGIE++ and GTT models. We summarize key findings below.\nThe best of the early models (GE NLToolset) performs better than either of the modern models. It does so by doing a better job balancing precision and recall, whereas GTT and DyGIE++ exhibit much higher precision and much lower recall. Table 4: Span Errors in early models. The differences between the predicted mention and its best gold mention match according to our analysis tool are in bold.\nThe early models have more span errors than the modern DyGIE++ and GTT models. The representative kinds of span errors from the 1992 model outputs are shown in Table 4. One interesting difference between the span errors in the early models and the modern models is that the predicted mentions include longer spans with more information than is indicated in the best gold mention match. Some could be due to errors in dataset annotation; for example, maoist shining path group versus shining path but a significant number of the span errors occur as the early models seem to extract the entire sentence or clause which contains the desired role filler mention. The modern models tend to leave off parts of the desired spans, and if they do predict larger spans than required, are only off by a few words.\nThe early models have fewer Missing Template and Missing Role Filler errors as compared to the modern models. However, the former also have more Spurious Template and Spurious Role Filler errors than the latter, indicating these models mitigate the issue of Missing Templates through over-generation.\nThe early models have fewer Incorrect Role errors as compared to modern models. However, since all the models make relatively few such errors, it suggests that role classification for predicted mentions is not a major problem for modern models.\nThe main source of error for both early and modern models is missing recall due to missing templates and missing role fillers. This strongly suggests future systems can maximize their performance by being less conservative in role filler detection and focusing on improvement of the recall, even at the expense of potentially decreasing some precision.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Limitations and Future Work", "text": "This work explores subtypes of Spurious Role Filler errors extensively, however, we would like to further analyze Missing Role Filler and templatelevel errors for more fine-grained error subtypes and the linguistic reasons behind why they occur.\nDue to the pairwise comparisons between all predicted and gold mentions in a role for all pairs of predicted and gold templates in an example, the error analysis tool is slow when the number of both the predicted and gold templates as well as the number of role fillers in the templates is high. Thus, we would also like to improve the time complexity of our template (and mention) matching algorithms using an approach like bipartite matching (Yang et al., 2021).\nCurrently, the error analysis tool reports exact match precision/recall/F1 which is more suitable for string-fill roles. We would like to extend the tool to further analyze set-fill roles by implementing metrics such as false-positive rate.\nWe used a limited number of models in this paper as we aimed to develop and test the usability of our error analysis tool. In the future, however, we would like to test our tool on a wider range of models, in addition to running more experiments in order to reach more generalizable conclusions about the behavior of IE models.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "As new models for information extraction continue to be developed, we find that their predicted error types contain insights regarding their shortcomings. Analyzing error patterns within model predictions in a more fine-grained manner beyond scores provided by commonly used metrics is important for the progress of the field. We introduce a framework for the automatic categorization of model prediction errors for document-level IE tasks. We used the tool to analyze the errors of two state-of-theart models on three datasets from varying domains and compared the error profiles of these models to four of the earliest systems in the field on a dataset from that era. We find that state-of-the-art models, when compared to the earlier manual feature-based models, perform better at span extraction but worse at template detection and role assignment. With a better balance between precision and recall, the best early model outperforms the relatively highprecision, low-recall modern models. Missing role fillers remain the main source of errors, and scientific corpora are the most difficult for all systems, suggesting that improvements in these areas should be a priority for future system development.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B Example Error Types with ProMED", "text": "We also provide example error types with the ProMED dataset.   ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D Computational Budget", "text": "The GTT (BERT) model on the MUC-4 dataset took 1 hour and 21 minutes to train and around 11 minutes to test on Google Colab (GPU).\nThe GTT (BERT) model on the ProMED dataset took around 24 minutes to train and 4 minutes to test, while the GTT (SciBERT) model on the ProMED dataset took around 13 minutes to train and 4 minutes to test, both on Google Colab (GPU). The DyGIE++ (BERT) model on the ProMED dataset took around 50 minutes to train, while the DyGIE++ (SciBERT) model on the ProMED dataset took around 1 hour and 30 minutes to train, both on a NVIDIA V100 GPU.\nFor the SciREX dataset, it took around 10-20 minutes to run the GTT (BERT) and GTT (SciBERT) models on a NVIDIA V100 GPU. It is worth noting that since the GTT model embeds all inputs before training and SciREX documents are extremely long, more than 25 GB of memory needs to be allocated at the embedding phrase. The training process has normal memory usage. The DyGIE++ (BERT) model took around 2 hours to train, while the DyGIE++ (SciBERT) model took around 4 hours to train, both on a NVIDIA V100 GPU.\nOur error analysis tool can be run completely on a CPU and takes a couple of minutes to run, depending on the size of the dataset and the predicted outputs.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "E Hyperparameters and Model Configurations", "text": "We did not run the DyGIE++ model on the MUC-4 dataset as the model output was made available to us by Xinya Du.      ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers and Ellen Riloff for their helpful comments(!) and Sienna Hu for converting the 1992 model outputs to a format compatible with our error analysis tool. Our research was supported, in part, by NSF CISE Grant 1815455 and the Cornell CS Department CSURP grants for undergraduate research.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Detailed Error Types Mappings", "text": "The specific list of transformations applied in the error correction process.\n(1) Span Error. Each singleton Alter Span transformation is mapped to a Span Error. A Span Error occurs when a predicted role filler becomes an exact match to the a gold role filer only upon span alteration.\n(2) Duplicate Role Filler. Each singleton Remove Duplicate Role Filler transformation is mapped to a Duplicate Role Filler error. A Duplicate Role Filler error occurs when a spurious role filler is coreferent to an already matched role filler and is treated as a separate entity. This happens when the system fails at coreference resolution.\n(3) Duplicate Partially Matched Role Filler (Spurious). Same as (2) above, but with an added Alter Span transformation applied first to account for partial matching. This happens when the system fails at correct span extraction and coreference resolution.\n(4) Spurious Role Filler. Each singleton Remove Spurious Role Filler transformation is mapped to a Spurious Role Filler error. A Spurious Role Filler error occurs when a mention is extracted from the text with no connection to the gold templates.\n(5) Missing Role Filler. Each singleton Introduce Role Filler transformation is mapped to a Missing Role Filler error. A Missing Role Filler error occurs when a role filler is present in the gold template but not the predicted template for a given role.\n(6) Incorrect Role. Each singleton Alter Role transformation is mapped to an Incorrect Role. An Incorrect Role occurs when a spurious role filler is assigned to the incorrect role within the same template, i.e. the role filler would have been correct if present filled in another slot/role in the same template. This happens when the system fails at correct role assignment.\n(7) Incorrect Role + Partially Matched Filler. Same as (4) above, but with an added Alter Span transformation applied first to account for partial matching. This happens when the system fails at correct span extraction and role assignment.\n(8) Wrong Template for Role Filler. Each singleton Remove Cross Template Spurious Role Filler transformation is mapped to a Wrong Template for Role Filler error. A Wrong Template for Role Filler occurs when a spurious role filler in one template can be assigned to the correct role in another template, i.e. it would be correct if it had been placed in another template. This happens when the system fails at correct event assignment.\n(9) Wrong Template for Partially Matched Role Filler. Same as (6) above, but with an added Alter Span transformation applied first to account for partial matching. This happens when the system fails at correct span extraction and event assignment.\n(10) Wrong Template + Wrong Role. An Alter Role and Remove Cross Template Spurious Role Filler transformation are applied to the same predicted role filler in that order to be mapped to a Wrong Template + Wrong Role error. A Wrong Template + Wrong Role error occurs when a spurious role filler can be assigned to another role in another template. This happens when the system fails at correct role assignment and event assignment.\n(11) Wrong Template + Wrong Role + Partially Matched Filler. Same as (8) above, but with an added Alter Span transformation applied first to account for partial matching. This happens when the system fails at correct span extraction, role assignment and event assignment.\n( ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Removing unmentioned entities sometimes eliminates differences between templates. This results in some templates becoming identical or making some templates contain information that is a subset of the information", "journal": "", "year": "", "authors": " According To Jain"}, {"title": "Pooled contextualized embeddings for named entity recognition", "journal": "", "year": "2019", "authors": "Tanja References Alan Akbik; Roland Bergmann;  Vollgraf"}, {"title": "Contextual string embeddings for sequence labeling", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Alan Akbik; Duncan Blythe; Roland Vollgraf"}, {"title": "TACRED revisited: A thorough evaluation of the TACRED relation extraction task", "journal": "", "year": "2020", "authors": "Christoph Alt; Aleksandra Gabryszak; Leonhard Hennig"}, {"title": "CrossCheck: Rapid, reproducible, and interpretable model evaluation", "journal": "", "year": "2021", "authors": "Dustin Arendt; Zhuanyi Shaw; Prasha Shrestha; Ellyn Ayton; Maria Glenski; Svitlana Volkova"}, {"title": "Event schema induction with a probabilistic entity-driven model", "journal": "", "year": "2013", "authors": "Nathanael Chambers"}, {"title": "MUC-3 evaluation metrics", "journal": "", "year": "1991-05-21", "authors": "Nancy Chinchor"}, {"title": "Towards More Intelligent Extraction of Information from Documents", "journal": "", "year": "2021", "authors": "Xinya Du"}, {"title": "Event extraction by answering (almost) natural questions", "journal": "", "year": "2020", "authors": "Xinya Du; Claire Cardie"}, {"title": "GRIT: Generative role-filler transformers for document-level event entity extraction", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Xinya Du; Alexander Rush; Claire Cardie"}, {"title": "Template filling with generative transformers", "journal": "", "year": "2021", "authors": "Xinya Du; Alexander Rush; Claire Cardie"}, {"title": "Linguisticbased evaluation criteria to identify statistical machine translation errors", "journal": "", "year": "2010", "authors": "Mireia Farr\u00fas; Marta R Costa-Juss\u00e0; Jos\u00e9 B Mari\u00f1o; Jos\u00e9 A R Fonollosa"}, {"title": "Twenty-five years of information extraction", "journal": "Natural Language Engineering", "year": "2019", "authors": "Ralph Grishman"}, {"title": "SciREX: A challenge dataset for document-level information extraction", "journal": "", "year": "2020", "authors": "Sarthak Jain; Madeleine Van Zuylen; Hannaneh Hajishirzi; Iz Beltagy"}, {"title": "Speech and language processing", "journal": "", "year": "2021", "authors": "Daniel Jurafsky; James H Martin"}, {"title": "Automatic error analysis for morphologically rich languages", "journal": "", "year": "2011", "authors": "Ahmed Kholy; Nizar Habash"}, {"title": "Parser showdown at the Wall Street corral: An empirical investigation of error types in parser output", "journal": "", "year": "2012", "authors": "Jonathan K Kummerfeld; David Hall; James R Curran; Dan Klein"}, {"title": "Errordriven analysis of challenges in coreference resolution", "journal": "", "year": "2013", "authors": "Jonathan K Kummerfeld; Dan Klein"}, {"title": "Documentlevel event argument extraction by conditional generation", "journal": "", "year": "2021", "authors": "Sha Li; Ji Heng; Jiawei Han"}, {"title": "A joint neural model for information extraction with global features", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Ying Lin; Heng Ji; Fei Huang; Lingfei Wu"}, {"title": "A general framework for information extraction using dynamic span graphs", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Yi Luan; Dave Wadden; Luheng He; Amy Shah; Mari Ostendorf; Hannaneh Hajishirzi"}, {"title": "Analyzing and visualizing coreference resolution errors", "journal": "", "year": "2015", "authors": "Sebastian Martschat; Thierry G\u00f6ckel; Michael Strube"}, {"title": "Recall error analysis for coreference resolution", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Sebastian Martschat; Michael Strube"}, {"title": "Third Message Understanding Conference", "journal": "", "year": "1991", "authors": ""}, {"title": "Fourth message understanding conference (MUC-4)", "journal": "", "year": "", "authors": ""}, {"title": "A unified model of phrasal and sentential evidence for information extraction", "journal": "Association for Computational Linguistics", "year": "2009", "authors": "Siddharth Patwardhan; Ellen Riloff"}, {"title": "Towards automatic error analysis of machine translation output", "journal": "Computational Linguistics", "year": "2011", "authors": "Maja Popovi\u0107; Hermann Ney"}, {"title": "Distant supervision for relation extraction beyond the sentence boundary", "journal": "Long Papers", "year": "2017", "authors": "Chris Quirk; Hoifung Poon"}, {"title": "Beyond accuracy: Behavioral testing of NLP models with CheckList", "journal": "", "year": "2020", "authors": "Tongshuang Marco Tulio Ribeiro; Carlos Wu; Sameer Guestrin;  Singh"}, {"title": "Error analysis for learningbased coreference resolution", "journal": "", "year": "2008", "authors": "Olga Uryupina"}, {"title": "Error analysis in an automated narrative information extraction pipeline", "journal": "IEEE Transactions on Computational Intelligence and AI in Games", "year": "2017", "authors": "Josep Valls-Vargas; Jichen Zhu; Santiago Onta\u00f1\u00f3n"}, {"title": "Error analysis of statistical machine translation output", "journal": "", "year": "2006", "authors": "David Vilar; Jia Xu; Luis Fernando; D' Haro; Hermann Ney"}, {"title": "Entity, relation, and event extraction with contextualized span representations", "journal": "", "year": "2019", "authors": "David Wadden; Ulme Wennberg; Yi Luan; Hannaneh Hajishirzi"}, {"title": "AllenNLP interpret: A framework for explaining predictions of NLP models", "journal": "", "year": "2019", "authors": "Eric Wallace; Jens Tuyls; Junlin Wang; Sanjay Subramanian; Matt Gardner; Sameer Singh"}, {"title": "Errudite: Scalable, reproducible, and testable error analysis", "journal": "", "year": "2019", "authors": "Tongshuang Wu; Marco Tulio Ribeiro; Jeffrey Heer; Daniel Weld"}, {"title": "Document-level event extraction via parallel prediction networks", "journal": "Long Papers", "year": "2021", "authors": "Hang Yang; Dianbo Sui; Yubo Chen; Kang Liu; Jun Zhao; Taifeng Wang"}, {"title": "Addicter: What is wrong with my translations?", "journal": "", "year": "2011-01", "authors": "Daniel Zeman; Mark Fishel"}, {"title": "Extracting entities and events as a single task using a transition-based neural model", "journal": "", "year": "2019-08-10", "authors": "Junchi Zhang; Yanxia Qin; Yue Zhang; Mengchi Liu; Donghong Ji"}, {"title": "Graph convolution over pruned dependency trees improves relation extraction", "journal": "", "year": "2018", "authors": "Yuhao Zhang; Peng Qi; Christopher D Manning"}, {"title": "Diagnostic evaluation of machine translation systems using automatically constructed linguistic check-points", "journal": "", "year": "2008", "authors": "Ming Zhou; Bo Wang; Shujie Liu; Mu Li; Dongdong Zhang; Tiejun Zhao"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Automatic transformations to convert predicted templates (on the left) to gold templates (on the right). Arrows represent transformations. Colored circles represent role filler entity mentions. Dupl. stands for duplicate.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "TYPE which is a set-fill role with six possible role fillers: attack, kidnapping, bombing, arson, robbery, and forced work stoppage. As seen in Table1, 44.59% of the documents have no templates, which makes the identification of relevant vs. irrelevant documents critical to the success of any IE model for this dataset. fill role with confirmed, possible, and suspected as the possible role filler options.SciREX(Jain et al., 2020) consists of annotated computer science articles from Papers with Code 10 .", "figure_data": "ProMED 8 (Patwardhan and Riloff, 2009) consistsof just 125 annotated tuning examples and 120annotated test examples, describing global diseaseoutbreaks by subject matter experts from ProMED.We use the tuning data as training data and reserve10% of the test data, i.e. 12 examples, to create a de-velopment/validation set. 19.83% of the documentsin the dataset have no templates. The roles that weextract from the dataset are STATUS, COUNTRY,DISEASE, and VICTIMS. DISEASE, VICTIMS,and COUNTRY are string-fill roles 9 ; STATUS isa set-"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Dataset Statistics. A relevant document has one or more templates.", "figure_data": "to create the training data for both the DyGIE++and GTT models, we use the first mention of eachrole filler in the document as the mention to beextracted.DyGIE++ with Clustering We use DyGIE++-a span-based, sentence-level extraction model-to identify role fillers in the document andassociate them with certain role types. Duringtraining, the maximum span length enumeratedby the model is set to 8 tokens as in Wadden et al."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": F1 Scores for the Neural Models on SciREX,ProMED, and MUC-4PrecisionRecallF1GE NLToolset56.69%52.09% 54.29%NYU PROTEUS34.23%31.28%32.69%SRI FASTUS48.47%38.42%42.86%UMass CIRCUS48.62%39.04%43.30%GTT (BERT)63.18%40.02%49.00%DyGIE++ (BERT)61.90%36.33%45.79%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ": Precision, Recall, and F1 scores for models onthe MUC-4 dataset. The first four models were devel-oped in 1992, while the last two models are recent anduse neural-based methods."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Some examples of the Error Types taken from the ProMED dataset. For each template, in every role, the role fillers within brackets refer to the same entity, while role fillers in different brackets refer to different entities. The text in bold black indicates the error in the prediction.C Precision, Recall, and F1 Scores for All Models on all Three DatasetsWe also provide additional precision, recall scores along with the F1 scores.", "figure_data": "ModelsSciREXProMEDMUC-4DyGIE++ (BERT)27.85 / 18.83 / 22.47 51.13 / 26.62 / 35.01 61.90 / 36.33 / 45.79DyGIE++ (SciBERT) 30.47 / 21.76 / 25.39 52.55 / 29.94 / 38.15-GTT (BERT)52.86 / 13.53 / 21.54 68.58 / 33.09 / 44.64 63.18 / 40.02 / 49.00GTT (SciBERT)53.68 / 18.65 / 27.68 64.68 / 32.16 / 42.96-"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Precision, Recall and F1 Scores (%).", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "GTT on the MUC-4 dataset", "figure_data": "GTT (BERT)GTT (SciBERT)Hyperparameter NameValueValuenumber of GPUs11number of TPU cores00max_grad_norm1.01.0gradient_accumulation_steps11seed11base_modelbert_base_uncasedallenai_ scibert_scivocab_uncasedlearning_rate5e-055e-05weight_decay0.00.0adam_epsilon1e-081e-08warmup_steps00num_train_epochs3636train_batch_size11eval_batch_size11max_seq_length_src435435max_seq_length_tgt7575threshold80.080.0"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "GTT Models on the ProMED dataset 3974", "figure_data": "GTT (BERT)GTT (SciBERT)Hyperparameter NameValueValuenumber of GPUs11number of TPU cores00max_grad_norm1.01.0gradient_accumulation_steps11seed11base_modelbert_base_uncasedallenai_ scibert_scivocab_uncasedlearning_rate5e-055e-05weight_decay0.00.0adam_epsilon1e-081e-08warmup_steps00num_train_epochs2020train_batch_size11eval_batch_size11max_seq_length_src435435max_seq_length_tgt7575threshold80.080.0"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "GTT Models on the SciREX dataset", "figure_data": "DyGIE++ (BERT) DyGIE++ (SciBERT)Hyperparameter NameValueValuenumber of GPUs11max_span_width1111base_modelbert_base_casedallenai_ scibert_scivocab_casedlearning_rate5e-045e-04patience55num_train_epochs2020train_batch_size3232num_dataloader_workers22max seq length512512ner loss weight1.01.0relation loss weight0.00.0coreference loss weight0.20.2events loss weight0.00.0target tasknerner"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "DyGIE++ Models on the ProMED dataset", "figure_data": "DyGIE++ (BERT) DyGIE++ (SciBERT)Hyperparameter NameValueValuenumber of GPUs11max_span_width88base_modelbert_base_casedallenai_ scibert_scivocab_casedlearning_rate5e-045e-04patience55num_train_epochs2020train_batch_size3232num_dataloader_workers22max seq length512512ner loss weight1.01.0relation loss weight0.00.0coreference loss weight0.20.2events loss weight0.00.0target tasknerner"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "", "figure_data": ""}], "doi": "10.18653/v1/N19-1078"}