{"authors": "Caroline Pasquer; Agata Savary; Carlos Ramisch; Jean-Yves Antoine", "pub_date": "", "title": "Seen2Unseen at PARSEME Shared Task 2020: All Roads do not Lead to Unseen Verb-Noun VMWEs", "abstract": "We describe the Seen2Unseen system that participated in edition 1.2 of the PARSEME shared task on automatic identification of verbal multiword expressions (VMWEs). The identification of VMWEs that do not appear in the provided training corpora (called unseen VMWEs) -with a focus here on verb-noun VMWEs -is based on mutual information and lexical substitution or translation of seen VMWEs. We present the architecture of the system, report results for 14 languages, and propose an error analysis.", "sections": [{"heading": "Introduction", "text": "The identification of multiword expressions (MWEs) such as spill the beans is a challenging problem (Baldwin and Kim, 2010;Constant et al., 2017), all the more so for verbal MWEs (VMWEs) subject to morphological (spill the bean) and syntactic variability (the beans were spilled ). The PARSEME shared task (PST) provided training, development and test corpora (hereafter Train, Dev, and Test) manually annotated for VMWEs. 1 Our system aimed at identifying every VMWE in Test which also appears in Train or Dev, including possible morphological or syntactic variants (henceforth seen VMWEs) or not present in Train/Dev (unseen VMWEs). Unseen VMWE identification, the main focus of this PST edition, is harder than seen VMWE identification, as shown by previous results (Ramisch et al., 2018).\nWe submitted two systems: Seen2Seen (closed track) and Seen2Unseen (open track). Seen2Unseen relies on Seen2Seen for the identification of seen VMWEs and has an additional module for unseen ones. Its best global unseen F-score (i.e. not only for verb-noun constructions) was obtained for Hindi (42.66) and it reached 25.36 in French, which was our main focus. Despite the lower global MWE-based F1score of Seen2Unseen (63.02) compared to Seen2Seen (66.23), we describe the former (Sec. 2), analyse its interesting negative results (Sec. 3), and conclude with ideas for future work (Sec. 4).", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "System Description", "text": "While describing the architecture of our system, we use the notions of a VMWE token (its occurrence in running text) and a VMWE type (abstraction over all occurrences of a given VMWE), as introduced by Savary et al. (2019b). We represent VMWE types as multisets of lemmas and POS. 2 Our system uses a mixture of discovery and identification methods, as defined by Constant et al. (2017). Namely, VMWE discovery consists in generating lists of MWE types out of context, while VMWE identification marks VMWE tokens in running text. The system is freely available online (https://gitlab.com/ cpasquer/st_2020).\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/.\n1 http://hdl.handle.net/11234/1-3367 2 VMWEs are represented as multisets (i.e. bags of elements with repetition allowed), since the same lemma and/or POS can occur twice, as in appeler un chat un chat 'to call a cat a cat'\u21d2'to call a spade a spade'.\nSeen2Seen in a nutshell Seen2Seen is a VMWE identification system dedicated to only those VMWEs which have been previously seen in the training data. Its detailed description is provided in Pasquer et al. (2020), but a brief overview is included here to make the current paper self-contained. Seen2Seen extracts lemma combinations of VMWEs seen in Train, looking for the same combinations (within one sentence) in Test, with an expected high recall. To improve precision, up to eight independent criteria can be used:\n(1) component lemmas should be disambiguated by their POS, (2) components should appear in specific orders (e.g. the determiner before the noun), (3) the order of \"gap\" words possibly occurring between components is also considered, (4) components should not be too far from each other in a sentence, (5) closer components are preferred over distant ones, (6) components should be syntactically connected, (7) nominal components should appear with a previously seen inflection, and (8) nested VMWEs should be annotated as in Train. We select the combination of criteria with maximal performance on Dev among all 2 8 = 256 possibilities. The candidates remaining after applying the criteria are annotated as VMWEs. This relatively simple system relying on morphosyntactic filters and tuned for 8 parameters was evaluated on 11 languages of the PARSEME shared task 1.1 (Ramisch et al., 2018). Seen2Seen outperformed the best systems not only on seen (F=0.8276), but even on all seen and unseen VMWEs (F=0.6653). 3 In edition 1.2 of the PARSEME shared task, Seen2Seen scored best (out of 2) in the global ranking of the closed track and second (out of 9) across both tracks. It outperformed 6 other open track systems, notably those using complex neural architectures and contextual word embeddings. We believe that these competitive results are due to carefully taking the nature of VMWEs into account (Savary et al., 2019a). Since Seen2Seen, by design, does not account for unseen VMWEs, its score in this category is very low (F=1.12). 4 Therefore, it was later extended with a VMWE discovery module. Seen2Unseen is precisely this extended system. It relies on Seen2Seen for seen VMWEs and on discovery methods described below for unseen VMWEs.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "From Seen2Seen to Seen2Unseen", "text": "We assume that seen VMWEs could help identify unseen ones by using (i) lexical variation, tolerated by some VMWEs (e.g. take a bath/shower), and (ii) translation, e.g. (FR) prendre d\u00e9cision 'take decision' = (PL) podejmowa\u0107 decyzj\u0119 = (PT) tomar decis\u00e3o = (SV) fatta beslut. 5 We also expect seen and unseen VMWEs to share characteristics, such as the distance between components or their syntactic dependency relations, e.g. nouns often being objects of verbs. The categories that should benefit from our strategy are, mainly, light-verb constructions (LVCs) containing nouns and, in some cases, verbal idioms (VIDs). These categories are universal, so our method can be applied to the 14 languages of the PST. Since LVCs are often verb-noun pairs, Seen2Unseen quasiexclusively focuses on them. 6 Consequently, we do not aim at exhaustively identifying unseen VMWEs, but at determining to what extent seen verb-noun VMWEs can help us discover new unseen ones.\nResources In addition to the PST Train, Dev and Test corpora, we used the CoNLL 2017 shared task parsed corpora, hereafter CoNLL-ST (Ginter et al., 2017). 7 The CoNLL-ST corpora were preferred over the PST-provided parsed corpora because they are conveniently released with pre-trained 100-dimensional word2vec embeddings for the 14 languages of the PST, which we used to generate lexical variants. Additionally, we used a free library to implement translation towards French and Italian. 8 We automatically translated all VMWEs in the other 13 languages into French (resp. Italian), privileged due to the availability of two Wikitionary-based lexicons in the same format for both languages. 3 In this paragraph we refer to macro-averaged MWE-based F-scores. 4 The score is not null due to different implementations of unseen VMWEs in the evaluation script and in Seen2Seen. 5 Languages are referred to with their PST identifier: e.g. FR for French. 6 We also model inherently reflexive verbs with cranberry words, i.e. verbs which never occur without a reflexive pronoun, e.g. (FR) s'\u00e9vanouir vs. *\u00e9vanouir. With 1 VMWE discovered in Portuguese and 3 in French, this module is omitted here.\n7 http://hdl.handle.net/11234/1-1989 8 Googletrans: https://pypi.org/project/googletrans, implementing the Google Translate API. 9 For French: http://redac.univ-tlse2.fr/lexicons/glaff_en.html, for Italian: http://redac. univ-tlse2.fr/lexiques/glaffit.html 10 In case of multiple POS or lemmas, the most frequent verb-noun combination in CoNLL-ST was selected.\nUnseen VMWE identification To support identification of unseen VMWEs we use a combination of semi-supervised discovery and identification methods: lexical replacement, translation and statistical ranking. For a language L, let SeenV N L be the set of all seen LVC and VID types having exactly one verb and one noun (and any number of components with other POS tags). Let each type in SeenV N L be linked with its manually annotated occurrences in Train. This set is used in the following steps: 2 Translation: By translating seen VMWE types in one language we obtain a list of VMWE type candidates in another language:\n\u2022 T RAN S L is built only for French and Italian, and is empty for other languages. T RAN S F R (resp.\nT RAN S IT ) contains automatic translations of each VMWE in SeenV N L , with L = FR (resp. L = IT), into French (resp. Italian). We eliminate translations which do not contain exactly one verb and one noun (and possible components of other POS), e.g. due to a wrong translation. For the remaining translations, we keep only the verb and the noun lemmas.\n3 Statistical ranking: This approach is based on statistical characteristics of both seen VMWEs and unseen VMWE candidates. We first calculate 3 sets of features for the whole SeenV N L list:\n\u2022 Dist L is the maximal verb-noun distance for all VMWE tokens occurring at least twice in SeenV N L . This should help eliminate candidates whose components are too distant in a sentence.\n\u2022 P L Dep (Dep V , Dep N )\nis the ratio of VMWE tokens in SeenV N L in which the incoming dependencies of the verb and of the noun are Dep V and Dep N . For instance, P F R Dep (root, obj) is higher than P F R Dep (root, nsubj) because, in French, active voice (e.g. rendre une visite 'pay a visit') is more frequent than passive voice (e.g. malediction fut lanc\u00e9e 'curse was cast'). We thus favour the most commonly observed VMWE dependencies. \u2022 P L Dist (i) is the ratio of VMWE tokens in SeenV N L in which the number of words inserted between the verb and the noun is i. For instance, P F R Dist (0) = 0.46, i.e. occurrences in which the verb and the noun are contiguous represent 46% of SeenV N F R . This ratio tends to decrease as i increases: P F R Dist (2) = 0.11, P F R Dist (5) = 0.006, etc. Candidates whose number of intervening words i has higher P L Dist (i) likely are true VMWEs. Given these characteristics of seen VMWEs, we proceed to extracting and ranking unseen VMWE candidates. Namely, Cand L is the list of all occurrences of verb-noun pairs in Test such that: (i) the verb and the noun are directly connected by a syntactic dependency, (ii) the distance between the verb and the noun does not exceed Dist L , and (iii) the verb and the noun never co-occur with a direct dependency link in Train or in Dev. The latter condition excludes both seen VMWEs (already covered by Seen2Seen) and verb-noun constructions not annotated as VMWEs in Train or Dev, i.e. being no VMWEs, e.g. (FR) avoir an 'have year' in elle a quinze ans 'she is 15 years old '. Cand L is then ranked by considering statistical properties. For each candidate c in Cand L , we calculate three measures:\n\u2022 P (c) is the estimated joint dependency-and distance-based probability. Suppose that i is the number of words inserted between c's verb and noun, and their incoming dependencies are Dep V and Dep N , respectively. Then, P (c) =  21) 0 (0) 0 (0) 0 (0) SIM L 0 (0) 0 (0) 0.45 (11) 0.17 ( 6) 0 (0) 0 (0) 0 (0) RAN K L 0.19 ( 101  \u2022 AM I(c) is the augmented mutual information of c's type in the CoNLL-ST corpus. MWEs are known to have a Zipfian distribution and to often mix very frequent words with very rare ones. AMI is designed specifically to address this phenomenon, so as to leverage the rarely occurring expressions or components (Zhang et al., 2009): AM I(x, y) = log 2  c) . Cand L is then ranked by RR(c). We keep n top-ranked candidates, where n is estimated by scaling the number (provided the organizers) of VIDs and LVCs in Test -when all the expressions annotated as seen during the Seen2Seen phase have been eliminated -by the recall of our method on Dev on the target constructions (unseen verb-noun LVCs and VIDs). 12 This n-best list is called RAN K L n . 4 Identification proper: In step 3 we obtain a list of unseen VMWE candidate tokens Cand L extracted from Test. The aim of identification is to discriminate among true and false VMWEs on this list. Statistical ranking and retaining top-n candidates is one possible statistically-based criterion. But we hypothesise that some candidates whose rank is worse than n, notably due to data sparseness, can still be correct if they result from lexical replacement or translation of seen VMWEs. Therefore, every c in Cand L is annotated as an LVC if c belongs to RAN K L n or if c's type belongs to M IX L \u222a SIM L \u222a T RAN S L .\nP L Dep (Dep V , Dep N ) \u00d7 P L Dist (i).\nP (x,y) P (x)P (y)(1\u2212 P (x,y) P (x) )(1\u2212 P (x,y) P (y) ) \u2022 RR(c)", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Results", "text": "Although Seen2Unseen uses 4 lists of candidates, here we analyse their contribution separately, that is, we use one list at a time in step 4 above. We report unseen MWE/token-based precision. 13 Sec. 3.1 analyses the impact of M IX L , SIM L and RAN K L n , while Sec. 3.2 discusses T RAN S L for French.\n3.1 Impact of M IX L , SIM L and RAN K L n As shown in Table 1, using M IX L alone leads to precision values above 0.29 for 7 languages out of 14. Conversely, RAN K L alone mostly leads to values below 0.22 (except for Hindi with P = 0.46). The precision using SIM L alone reaches a maximum of 0.45 for Basque. The error analysis below suggests ways to improve precision. In French, using M IX F R alone yields 21 candidates in Test. Among the 5 false positives, there is one literal reading (faire dessin 'make drawing'), one omitted VMWE (recevoir aide 'receive help') and three other verb-noun pairs that could have been disregarded (being coincidental occurrences) if we had taken into account not only the existence of the syntactic dependency but also its nature (e.g. nous avons VERB cinq points \u00e0 l'ordre NOUN.xcomp du jour 'we have five items on the agenda').\nThis major problem for M IX L is shared by SIM L , but a specific drawback with SIM L is that not all words that occur in similar contexts are actually similar. Indeed, we obtain relevant generated unseen verb-noun pairs, including synonyms, antonyms and hyponyms, but also irrelevant ones. We should therefore either use more reliable resources, such as synonym/antonym dictionaries, and/or disregard frequent verbs (to have, to do, etc.). For these frequent verbs, the more reliable equivalences obtained by M IX L compared to SIM L should be preferred (faire 'do' M IX F R = subir 'suffer' vs. faire 'do' SIM F R = passer 'pass'). Indeed, as shown in Table 1, over 5 languages with M IX L and SIM L candidates, 4 exhibit a better precision and higher number of candidates for M IX L .\nIn French, by dividing n by 4 in RAN K F R n , the precision would have increased from 0.19 to 0.45 (18 VMWEs over 40 candidates). In other words, using RAN K L n in step 4 can slightly increase recall but causes a drop in precision, unless n is low. Hindi appears as an exception: no negative impact is observed with RAN K HI n due to a bias in the corpora (compound mentioned in the dependency label).\n3.2 Impact of T RAN S L : (IT) Traduttore, traditore 'translator, traitor'?\nWith translational equivalences, we hypothesized that T RAN S L would lead to situations such as:\n\u2022 exact matches: (PT) cometer crime 'commit a crime' \u2192 (FR) commettre crime ,\n\u2022 partial matches leading to VMWEs nonetheless: (PT) causar problema 'cause problem' \u2192 (FR) causer ennui, instead of causer probl\u00e8me, \u2022 no match, but another VMWE: (PT) ter destaque 'highlight' \u2192 (FR) mettre en \u00e9vidence.\n\u2022 literal, non-fluent or ambiguous translations (Constant et al., 2017): (PT) jogar o toalha 'throw the towel '\u21d2'give up' \u2192 (FR) jeter la serviette instead of jeter l'\u00e9ponge 'throw the sponge', \u2022 non-existing VMWEs in the target language: (TR) el atma \u2192 (FR) lancer main 'throw hand '\nWe focus on French due to the high number of candidates in T RAN S F R . In Test-FR, among the 44 annotated verb-noun candidates using T RAN S F R alone, 18 are actually VMWEs and 3 partially correspond to VMWEs due to omitted determiners, yielding an unseen MWE-based precision of 0.41 and an unseen token-based precision value of 0.48. These 21 candidates are mainly provided by Greek (10 vs. 6 from PT and 0 from IT or RO). Thus, the size of the training corpora may have more influence on the probability to obtain good translations than the source language family.\nThe 23 false positives include (i) 13 candidates that can be VMWEs or not depending on the context, including coincidental co-occurrences, literal readings and errors in the manually annotated reference Test corpus, and (ii) 10 candidates that are not VMWEs, whatever the context, e.g. the inchoative commencer recherche 'start research ' (from Hebrew) or payer taxe 'pay tax'(from (PL) uiszcza\u0107 op\u0142at\u0119).\nConsequently, translation may be a clue to discover unseen VMWEs, since 78% of Cand F R \u2229 T RAN S F R are VMWEs out of context, but barely half of them were manually annotated in context. As highlighted above, a restriction to the most frequent VMWE syntactic relations could help filter out coincidental occurrences corresponding to 39% of false positives (e.g. lancer la balle \u00e0 la main OBL:MOD 'throw the ball with the hand ').", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Conclusions and Future Work", "text": "We proposed an error analysis for our system Seen2Unseen dedicated to unseen verb-noun VMWE identification. It reveals that lexical variation and translation can produce valid unseen VMWEs but their ambiguity in context must be solved: we should take into account both the dependency labels (to avoid coincidental occurrences) and the probability of the verb to be light in Train (to avoid frequent co-ocurrences like fumer cigarette 'smoke cigarette'). Using contextual rather than non-contextual word embeddings might also be helpful, even if computationally more intensive. We could also combine T RAN S L and M IX L \u222a SIM L by applying lexical substitution to the translated VMWEs.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "This work was funded by the French PARSEME-FR grant (ANR-14-CERA-0001). We are grateful to Guillaume Vidal for his prototype, and to the anonymous reviewers for their useful comments.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Multiword expressions", "journal": "CRC Press", "year": "2010", "authors": "Timothy Baldwin; Su Nam Kim"}, {"title": "Multiword expression processing: A survey", "journal": "Computational Linguistics", "year": "2017", "authors": "Mathieu Constant; G\u00fcl\u015fen Eryigit; Johanna Monti; Lonneke Van Der; Carlos Plas; Michael Ramisch; Amalia Rosner;  Todirascu"}, {"title": "CoNLL 2017 shared taskautomatically annotated raw texts and word embeddings. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (\u00daFAL), Faculty of Mathematics and Physics", "journal": "", "year": "2017", "authors": "Filip Ginter; Jan Haji\u010d; Juhani Luotolahti; Milan Straka; Daniel Zeman"}, {"title": "Verbal multiword expression identification: Do we need a sledgehammer to crack a nut?", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Caroline Pasquer; Agata Savary; Carlos Ramisch; Jean-Yves Antoine"}, {"title": "Edition 1.1 of the PARSEME Shared Task on Automatic Identification of Verbal Multiword Expressions", "journal": "", "year": "2018", "authors": "Carlos Ramisch; Silvio Ricardo Cordeiro; Agata Savary; Veronika Vincze; Archna Verginica Barbu Mititelu; Maja Bhatia; Marie Buljan; Polona Candito; Voula Gantar; Tunga Giouli; Abdelati G\u00fcng\u00f6r; Uxoa Hawwari; Jolanta I\u00f1urrieta; Simon Kovalevskait\u0117; Timm Krek; Chaya Lichte; Johanna Liebeskind;  Monti"}, {"title": "Without lexicons, multiword expression identification will never fly: A position statement", "journal": "Association for Computational Linguistics", "year": "2019-08", "authors": "Agata Savary; Silvio Cordeiro; Carlos Ramisch"}, {"title": "Literal Occurrences of Multiword Expressions: Rare Birds That Cause a Stir", "journal": "", "year": "2019-04", "authors": "Agata Savary; Silvio Ricardo Cordeiro; Timm Lichte; Carlos Ramisch; Voula Uxoa I Nurrieta;  Giouli"}, {"title": "Augmented mutual information for multi-word extraction", "journal": "International Journal of Innovative Computing, Information and Control", "year": "2009", "authors": "Wen Zhang; Taketoshi Yoshida; Tu Bao Ho; Xijin Tang"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "9 These lexicons were used to lemmatize and POS-tag automatic translations, e.g. (PT) firmar contrato 'sign contract' translation \u2212 \u2212\u2212\u2212\u2212\u2212\u2212 \u2192 (FR) a sign\u00e9 un contrat lemma,P OS \u2212 \u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2192 signer VERB contrat NOUN . 10", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "is the reciprocal rank combining the two indicators above. Let rank P (c) and rank AM I (c) be the ranks of c in Cand L according to the values of P (c) and AM I(c) with P (c) > 0 and AM I(c) > 0. Then RR(c) = 1 rank P (c) + 1 rank AM I (", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Lexical replacement: The idea is to observe lexical variability of seen VMWEs and to generate on this basis new potential VMWEs. Let LV C L V var contain LVC types in SeenV N L that tolerate variation in verbs, e.g. accomplir/effectuer VERB mission NOUN 'fulfil/perform mission'. Similarly, let LV C L N var contain LVCs types with variation in nouns, e.g. accomplir VERB mission/t\u00e2che NOUN 'fulfil mission/task'. Then we define two sets of candidates: \u2022 M IX L combines each verb in LV C L V var with each noun in LV C L N var to predict new combinations. e.g. effectuer t\u00e2che 'perform task'. \u2022 SIM L contains VMWEs from LV C L V var (resp. LV C L N var ) where we replace the verb (resp. noun) by its closest verb (resp. noun) according to cosine similarity in CoNLL-ST word embeddings. 11", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": ") 0.05 (228) 0.09 (329) 0.19 (159) 0.21 (137) 0.04 (129) 0.46 (273)", "figure_data": "ListITPLPTROSVTRZHM IX L0 (0)0.40 (20)0.29 (35)0 (0)0 (2)0.48 (21)0.29 (7)SIM L0 (0)0.33 (3)0.20 (20)0 (0)0 (0)0.33 (6)0 (0)RAN K L 0.11 (163) 0.14 (164) 0.08 (225) 0.03 (422) 0.21 (100) 0.15 (214)0 (32)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Unseen MWE-based precision (and number of predicted VMWEs) in Test for the 14 languages L, when using only M IX L , SIM L or RAN K L lists.", "figure_data": ""}], "doi": ""}