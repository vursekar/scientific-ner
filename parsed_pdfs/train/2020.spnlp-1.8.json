{"authors": "Lifu Tu; Richard Yuanzhe Pang; Kevin Gimpel", "pub_date": "", "title": "Improving Joint Training of Inference Networks and Structured Prediction Energy Networks", "abstract": "Deep energy-based models are powerful, but pose challenges for learning and inference (Belanger and McCallum, 2016). Tu and Gimpel (2018) developed an efficient framework for energy-based models by training \"inference networks\" to approximate structured inference instead of using gradient descent. However, their alternating optimization approach suffers from instabilities during training, requiring additional loss terms and careful hyperparameter tuning. In this paper, we contribute several strategies to stabilize and improve this joint training of energy functions and inference networks for structured prediction. We design a compound objective to jointly train both costaugmented and test-time inference networks along with the energy function. We propose joint parameterizations for the inference networks that encourage them to capture complementary functionality during learning. We empirically validate our strategies on two sequence labeling tasks, showing easier paths to strong performance than prior work, as well as further improvements with global energy terms.", "sections": [{"heading": "Introduction", "text": "Energy-based modeling (LeCun et al., 2006) associates a scalar compatibility measure to each configuration of input and output variables. Belanger and McCallum (2016) formulated deep energy-based models for structured prediction, which they called structured prediction energy networks (SPENs). SPENs use arbitrary neural networks to define the scoring function over input/output pairs. However, this flexibility leads to challenges for learning and inference. The original work on SPENs used gradient descent for structured inference (Belanger and McCallum, 2016;Belanger et al., 2017). Gimpel (2018, 2019) found improvements in both speed and accuracy by replacing the use of gradient descent with a method that trains a neural network (called an \"inference network\") to do inference directly. Their formulation, which jointly trains the inference network and energy function, is similar to training in generative adversarial networks (Goodfellow et al., 2014), which is known to suffer from practical difficulties in training due to the use of alternating optimization (Salimans et al., 2016). To stabilize training, Tu and Gimpel (2018) experimented with several additional terms in the training objectives, finding performance to be dependent on their inclusion.\nMoreover, when using the approach of Tu and Gimpel (2018), there is a mismatch between the training and test-time uses of the trained inference network. During training with hinge loss, the inference network is actually trained to do \"costaugmented\" inference. However, at test time, the goal is to simply minimize the energy without any cost term. Tu and Gimpel (2018) fine-tuned the cost-augmented network to match the test-time criterion, but found only minimal change from this fine-tuning. This suggests that the cost-augmented network was mostly acting as a test-time inference network by convergence, which may be hindering the potential contributions of cost-augmented inference in max-margin structured learning (Tsochantaridis et al., 2004;Taskar et al., 2004).\nIn this paper, we contribute a new training objective for SPENs that addresses the above concern and also contribute several techniques for stabilizing and improving learning. We empirically validate our strategies on two sequence labeling tasks from natural language processing (NLP), namely part-of-speech tagging and named entity recognition. We show easier paths to strong performance than prior work, as well as further improvements with global energy terms. We summarize our list of contributions as follows.\n\u2022 We design a compound objective under the SPEN framework to jointly train the \"trainingtime\" cost-augmented inference network and test-time inference network (Section 3).\n\u2022 We propose shared parameterizations for the two inference networks so as to encourage them to capture complementary functionality while reducing the total number of trained parameters (Section 3.1). Quantitative and qualitative analysis shows clear differences in the characteristics of the two networks (Table 3).\n\u2022 We present three methods to streamline and stabilize training that help with both the old and new objectives (Section 4).\n\u2022 We propose global energy terms to capture long-distance dependencies and obtain further improvements (Section 5).\nWhile SPENs have been used for multiple NLP tasks, including multi-label classification (Belanger and McCallum, 2016), part-of-speech tagging (Tu and Gimpel, 2018), and semantic role labeling (Belanger et al., 2017), they are not widely used in NLP. Structured prediction is extremely common in NLP, but is typically approached using methods that are more limited than SPENs (such as conditional random fields) or models that suffer from a train/test mismatch (such as most auto-regressive models). SPENs offer a maximally expressive framework for structured prediction while avoiding the train/test mismatch and therefore offer great potential for NLP. However, the training and inference have deterred NLP researchers. While we have found benefit from training inference networks for machine translation in recent work (Tu et al., 2020b), that work assumed a fixed, pretrained energy function.\nOur hope is that the methods in this paper will enable SPENs to be applied to a larger set of applications, including generation tasks in the future.", "n_publication_ref": 16, "n_figure_ref": 0}, {"heading": "Background", "text": "We denote the input space by X . For an input x \u2208 X , we denote the structured output space by Y(x). The entire space of structured outputs is denoted Y = \u222a x\u2208X Y(x). A SPEN (Belanger and McCallum, 2016) defines an energy function E \u0398 : X \u00d7 Y \u2192 R parameterized by \u0398 that computes a scalar energy for an input/output pair. At test time, for a given input x, prediction is done by choosing the output with lowest energy:\ny = arg min y\u2208Y(x) E \u0398 (x, y)(1)\nHowever, solving equation ( 1) requires combinatorial algorithms because Y is a structured, discrete space. This becomes intractable when E \u0398 does not decompose into a sum over small \"parts\" of y. Belanger and McCallum (2016) relaxed this problem by allowing the discrete vector y to be continuous; Y R denotes the relaxed output space.\nThey solved the relaxed problem by using gradient descent to iteratively minimize the energy with respect to y. The energy function parameters \u0398 are trained using a structured hinge loss which requires repeated cost-augmented inference during training. Using gradient descent for the repeated costaugmented inference steps is time-consuming and makes learning unstable (Belanger et al., 2017). Tu and Gimpel (2018) replaced gradient descent with a neural network trained to do efficient inference. This \"inference network\" A \u03a8 : X \u2192 Y R is parameterized by \u03a8 and trained with the goal that\nA \u03a8 (x) \u2248 arg min y\u2208Y R (x) E \u0398 (x, y)(2)\nWhen training the energy function parameters \u0398, Tu and Gimpel (2018) replaced the cost-augmented inference step in the structured hinge loss from Belanger and McCallum (2016) with a costaugmented inference network F \u03a6 :\nF \u03a6 (x) \u2248 arg min y\u2208Y R (x) (E \u0398 (x, y) \u2212 (y, y * )) (3)\nwhere is a structured cost function that computes the distance between its two arguments. We use L1 distance for . This inference problem involves finding an output with low energy but high cost relative to the gold standard. Thus, it is not wellaligned with the test-time inference problem.\nHere is the specific objective to jointly train \u0398 (parameters of the energy function) and \u03a6 (parameters of the cost-augmented inference network):\nmin \u0398 max \u03a6 x i ,y i \u2208D [ (F \u03a6 (x i ), y i ) \u2212 E \u0398 (x i , F \u03a6 (x i )) + E \u0398 (x i , y i )] + (4)\nwhere D is the set of training pairs, [h] + = max(0, h), and is a structured cost function that computes the distance between its two arguments. Tu and Gimpel (2018) alternatively optimized \u0398 and \u03a6, which is similar to training in generative adversarial networks (Goodfellow et al., 2014). The inference network is analogous to the generator and the energy function is analogous to the discriminator. As alternating optimization can be difficult in practice (Salimans et al., 2016), Tu & Gimpel experimented with including several additional terms in the above objective to stabilize training.\nAfter the training of the energy function, an inference network A \u03a8 for test-time prediction is finetuned with the goal shown in Eq. (2). More specifically, for the fine-tuning step, we first initialize \u03a8 with \u03a6; next, we do gradient descent according to the following objective to learn \u03a8:\n\u03a8 \u2190 arg min \u03a8 x\u2208X E \u0398 (x, A \u03a8 (x))\nwhere X is a set of training or validation inputs. It could also be the test inputs in a transductive setting.\n3 An Objective for Joint Learning of Inference Networks\nOne challenge with the above optimization problem is that it requires training a separate inference network A \u03a8 for test-time prediction after the energy function is trained. In this paper, we propose an alternative that trains the energy function and both inference networks jointly. In particular, we use a \"compound\" objective that combines two widely-used losses in structured prediction. We first present it without inference networks:\nmin \u0398 x i ,y i \u2208D max y ( (y, y i )\u2212E \u0398 (x i , y)+E \u0398 (x i , y i )) + margin-rescaled hinge loss + \u03bb max y (\u2212E \u0398 (x i , y) + E \u0398 (x i , y i )) + perceptron loss (5)\nAs indicated, this loss can be viewed as the sum of the margin-rescaled hinge and perceptron losses for SPENs. Two different inference problems are represented. The margin-rescaled hinge loss contains cost-augmented inference, shown as part of Eq. (3). The perceptron loss contains the test-time inference problem, which is shown in Eq. (1). Tu and Gimpel ( 2018) used a single inference network for solving both problems, so it was trained as a cost-augmented inference network during training and then fine-tuned as a test-time inference network afterward. We avoid this issue by training two inference networks, A \u03a8 for test-time inference and F \u03a6 for cost-augmented inference:\nmin \u0398 max \u03a6,\u03a8 x i ,y i \u2208D [ (F \u03a6 (x i ), y i )\u2212E \u0398 (x i , F \u03a6 (x i ))+E \u0398 (x i , y i )] + + \u03bb [\u2212E \u0398 (x i , A \u03a8 (x i ))+E \u0398 (x i , y i )] +(6)\nWe treat this optimization problem as a minimax game and find a saddle point for the game similar to Tu and Gimpel (2018) and Goodfellow et al. (2014).\nWe use minibatch stochastic gradient descent and alternately optimize \u0398, \u03a6, and \u03a8. The objective for the energy parameters \u0398 in minibatch M is:\n\u0398 \u2190 arg min \u0398 x i ,y i \u2208M (F \u03a6 (x i ), y i )\u2212E \u0398 (x i , F \u03a6 (x i ))+E \u0398 (x i , y i ) + + \u03bb \u2212E \u0398 (x i , A \u03a8 (x i ))+E \u0398 (x i , y i ) +\nWhen we remove 0-truncation (see Sec. 4.1 for the motivation), the objective for the inference network parameters in minibatch M is:\n\u03a8,\u03a6 \u2190 arg max \u03a8,\u03a6 x i ,y i \u2208M (F \u03a6 (x i ), y i )\u2212 E \u0398 (x i , F \u03a6 (x i )) \u2212 \u03bbE \u0398 (x i , A \u03a8 (x i ))", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Joint Parameterizations", "text": "If we were to train independent inference networks A \u03a8 and F \u03a6 , this new objective could be much slower than the approach of Tu and Gimpel (2018). However, the compound objective offers several natural options for defining joint parameterizations of the two inference networks. We consider three options which are visualized in Figure 1 and described below:\n\u2022 separated: F \u03a6 and A \u03a8 are two independent networks with their own architectures and parameters as shown in Figure 1(a). \u2022 shared: F \u03a6 and A \u03a8 share a \"feature\" network as shown in Figure 1(b). We consider this option because both F \u03a6 and A \u03a8 are trained to produce output labels with low energy. However F \u03a6 also needs to produce output labels with high cost (i.e., far from the gold standard). \u2022 stacked: the cost-augmented network F \u03a6 is a function of the output of the test-time network A \u03a8 and the gold standard output y. That is,\nF \u03a6 (x, y) = q(A \u03a8 (x), y)\nwhere q is a parameterized function. This is depicted in Figure 1(c). We block the gradient at A \u03a8 when updating \u03a6.\nFor the q function in the stacked option, we use an affine transformation on the concatenation of the inference network label distribution and the gold standard one-hot vector. That is, denoting the vector at position t of the cost-augmented network output by F \u03a6 (x, y) t , we have:\nF \u03a6 (x, y) t = softmax(W q [A \u03a8 (x) t ; y(t)] + b q )\nwhere semicolon (;) is vertical concatenation, y(t) (position t of y) is an L-dimensional one-hot vector, A \u03a8 (x) t is the vector at position t of A \u03a8 (x), W q is an L \u00d7 2L matrix, and b q is a bias. One motivation for these parameterizations is to reduce the total number of parameters in the procedure. Generally, the number of parameters is expected to decrease when moving from separated to shared to stacked. We will compare the three options empirically in our experiments, in terms of both accuracy and number of parameters.\nAnother motivation, specifically for the third option, is to distinguish the two inference networks in terms of their learned functionality. With all three parameterizations, the cost-augmented network will be trained to produce an output that differs from the gold standard, due to the presence of the (\u2022) term in the combined objective. However, Tu and Gimpel (2018) found that the trained cost-augmented network was barely affected by fine-tuning for the test-time inference objective. This suggests that the cost-augmented network was mostly acting as a test-time inference network by the time of convergence. With the stacked parameterization, however, we explicitly provide the gold standard y to the cost-augmented network, permitting it to learn to change the predictions of the test-time network in appropriate ways to improve the energy function.", "n_publication_ref": 2, "n_figure_ref": 4}, {"heading": "Training Stability and Effectiveness", "text": "We now discuss several methods that simplify and stabilize training SPENs with inference networks. When describing them, we will illustrate their impact by showing training trajectories for the Twitter part-of-speech tagging task.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Removing Zero Truncation", "text": "Tu and Gimpel (2018) used the following objective for the cost-augmented inference network (maximizing it with respect to \u03a6):\nl 0 = [ (F \u03a6 (x), y) \u2212 E \u0398 (x, F \u03a6 (x)) + E \u0398 (x, y)] +\nwhere [h] + = max(0, h). However, there are two potential reasons why l 0 will equal zero and trigger no gradient update. First, E \u0398 (the energy function, corresponding to the discriminator in a GAN) may already be well-trained, and it can easily separate the gold standard output from the costaugmented inference network output. Second, the cost-augmented inference network (corresponding to the generator in a GAN) could be so poorly trained that the energy of its output is very large, leading the margin constraints to be satisfied and l 0 to be zero.\nIn standard margin-rescaled max-margin learning in structured prediction (Taskar et al., 2004;Tsochantaridis et al., 2004), the cost-augmented inference step is performed exactly (or approximately  with reasonable guarantee of effectiveness), ensuring that when l 0 is zero, the energy parameters are well trained. However, in our case, l 0 may be zero simply because the cost-augmented inference network is undertrained, which will be the case early in training. Then, when using zero truncation, the gradient of the inference network parameters will be 0. This is likely why Tu and Gimpel (2018) found it important to add several stabilization terms to the l 0 objective. We find that by instead removing the truncation, learning stabilizes and becomes less dependent on these additional terms. Note that we retain the truncation at zero when updating the energy parameters \u0398.\nAs shown in Figure 2(a), without any stabilization terms and with truncation, the inference network will barely move from its starting point and learning fails overall. However, without truncation, the inference network can work well even without any stabilization terms.", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Local Cross Entropy (CE) Loss", "text": "Tu and  proposed adding a local cross entropy (CE) loss, which is the sum of the label cross entropy losses over all positions in the sequence, to stabilize inference network training. We similarly find this term to help speed up convergence and improve accuracy. Figure 2(b) shows faster convergence to high accuracy when adding the local CE term. See Section 7 for more details.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Multiple Inference Network Update Steps", "text": "When training SPENs with inference networks, the inference network parameters are nested within the energy function. We found that the gradient components of the inference network parameters consequently have smaller absolute values than those of the energy function parameters. So, we alternate between k \u2265 1 steps of optimizing the inference network parameters (\"I steps\") and one step of optimizing the energy function parameters (\"E steps\"). We find this strategy especially helpful when using complex inference network architectures.\nTo analyze, we compute the cost-augmented loss l 1 = (F \u03a6 (x), y) \u2212 E \u0398 (x, F \u03a6 (x)) and the margin-rescaled hinge loss With k = 1, the setting used by Tu and Gimpel (2018), the inference network lags behind the energy, making the energy parameter updates very small, as shown by the small norms in Fig. 3(c). The inference network gradient norm (Fig. 3(d)) remains high, indicating underfitting. However, increasing k too much also harms learning, as evi- denced by the \"plateau\" effect in the l 1 curves for k = 50; this indicates that the energy function is lagging behind the inference network. Using k = 5 leads to more of a balance between l 1 and l 0 and gradient norms that are mostly decreasing during training. We treat k as a hyperparameter that is tuned in our experiments.\nl 0 = [ (F \u03a6 (x), y) \u2212 E \u0398 (x, F \u03a6 (x)) + E \u0398 (x, y)] + averaged\nThere is a potential connection between our use of multiple I steps and a similar procedure used in GANs (Goodfellow et al., 2014). In the GAN objective, the discriminator D is updated in the inner loop, and they alternate between multiple update steps for D and one update step for G. In this section, we similarly found benefit from multiple steps of inner loop optimization for every step of the outer loop. However, the analogy is limited, since GAN training involves sampling noise vectors and using them to generate data, while there are no noise vectors or explicitly-generated samples in our framework.", "n_publication_ref": 2, "n_figure_ref": 2}, {"heading": "Energies for Sequence Labeling", "text": "For our sequence labeling experiments in this paper, the input x is a length-T sequence of tokens, and the output y is a sequence of labels of length T . We use y t to denote the output label at position t, where y t is a vector of length L (the number of labels in the label set) and where y t,j is the jth entry of the vector y t . In the original output space Y(x), y t,j is 1 for a single j and 0 for all others. In the relaxed output space Y R (x), y t,j can be interpreted as the probability of the tth position being labeled with label j. We then use the following energy for sequence labeling (Tu and Gimpel, 2018):\nE \u0398 (x, y) = \u2212 T t=1 L j=1 y t,j U j b(x, t) + T t=1 y t\u22121 W y t (7\n)\nwhere U j \u2208 R d is a parameter vector for label j and the parameter matrix W \u2208 R L\u00d7L contains label-pair parameters. Also, b(x, t) \u2208 R d denotes the \"input feature vector\" for position t. We define b to be the d-dimensional BiLSTM (Hochreiter and Schmidhuber, 1997) hidden vector at t. The full set of energy parameters \u0398 includes the U j vectors, W , and the parameters of the BiLSTM.\nGlobal Energies for Sequence Labeling. In addition to new training strategies, we also experiment with several global energy terms for sequence labeling. Eq. ( 7) shows the base energy, and to capture long-distance dependencies, we include global energy (GE) terms in the form of Eq. (8). We use h to denote an LSTM tag language model (TLM) that takes a sequence of labels as input and returns a distribution over next labels. We define y t = h(y 0 , . . . , y t\u22121 ) to be the distribution given the preceding label vectors (under a LSTM language model). Then, the energy term is:\nE TLM (y) = \u2212 T +1 t=1 log y t y t (8\n)\nwhere y 0 is the start-of-sequence symbol and y T +1 is the end-of-sequence symbol. This energy returns the negative log-likelihood under the TLM of the candidate output y. Tu and Gimpel (2018) pretrained their h on a large, automatically-tagged corpus and fixed its parameters when optimizing \u0398. Our approach has one critical difference. We instead do not pretrain h, and its parameters are learned when optimizing \u0398. We show that even without pretraining, our global energy terms are still able to capture useful additional information.\nWe also propose new global energy terms. Define y t = h(y 0 , . . . , y t\u22121 ) where h is an LSTM TLM that takes a sequence of labels as input and returns a distribution over next labels. First, we add a TLM in the backward direction (denoted y t analogously to the forward TLM). Second, we include words as additional inputs to forward and backward TLMs. We define y t = g(x 0 , ..., x t\u22121 , y 0 , ..., y t\u22121 ) where g is a forward LSTM TLM. We define the backward version similarly (denoted y t ). The global energy is therefore\nE GE (y) = \u2212 T +1 t=1 log(y t y t ) + log(y t y t ) + \u03b3 log(y t y t ) + log(y t y t ) (9)\nHere \u03b3 is a hyperparameter that is tuned. We experiment with three settings for the global energy: GE(a): forward TLM as in Tu and Gimpel (2018); GE(b): forward and backward TLMs (\u03b3 = 0); GE(c): all four TLMs in Eq. (9).", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "We consider two sequence labeling tasks: Twitter part-of-speech (POS) tagging (Gimpel et al., 2011) and named entity recognition (NER; Tjong Kim Sang and De Meulder, 2003).\nTwitter Part-of-Speech (POS) Tagging. We use the Twitter POS data from Gimpel et al. (2011) and Owoputi et al. (2013) which contain 25 tags. We use 100-dimensional skip-gram (Mikolov et al., 2013) embeddings from Tu et al. (2017). Like Tu and Gimpel (2018), we use a BiLSTM to compute the input feature vector for each position, using hidden size 100. We also use BiLSTMs for the inference networks. The output of the inference network is a softmax function, so the inference network will produce a distribution over labels at each position. The \u2206 is L1 distance. We train the inference network using stochastic gradient descent (SGD) with momentum and train the energy parameters using Adam (Kingma and Ba, 2014). We also explore training the inference network using Adam when not using the local CE loss. 1 In experiments with the local CE term, its weight is set to 1.\nNamed Entity Recognition (NER). We use the CoNLL 2003 English dataset (Tjong Kim Sang and De Meulder, 2003). We use the BIOES tagging scheme, following previous work (Ratinov and Roth, 2009), resulting in 17 NER labels. We use 100-dimensional pretrained GloVe embeddings (Pennington et al., 2014). The task is evaluated using F1 score computed with the conlleval script. The architectures for the feature networks in the energy function and inference networks are all BiLSTMs. The architectures for tag language models are LSTMs. We use a dropout keep-prob of 0.7 for all LSTM cells. The hidden size for all LSTMs is 128. We use Adam (Kingma and Ba, 2014) and do early stopping on the development set. We use a learning rate of 5 \u2022 10 \u22124 . Similar to above, the weight for the CE term is set to 1. We consider three NER modeling configurations. NER uses only words as input and pretrained, fixed   Tu and Gimpel (2018). The inference network architecture is a one-layer BiLSTM.\nGloVe embeddings. NER+ uses words, the case of the first letter, POS tags, and chunk labels, as well as pretrained GloVe embeddings with fine-tuning. NER++ includes everything in NER+ as well as character-based word representations obtained using a convolutional network over the character sequence in each word. Unless otherwise indicated, our SPENs use the energy in Eq. (7).", "n_publication_ref": 13, "n_figure_ref": 0}, {"heading": "Results and Analysis", "text": "Effect of Zero Truncation and Local CE Loss.\nTable 1 shows results for zero truncation and the local CE term. Training fails for both tasks when using zero truncation without CE. Removing truncation makes learning succeed and leads to effective models even without using CE. However, when using the local CE term, truncation has little effect on performance. The importance of CE in prior work (Tu and Gimpel, 2018) is likely due to the fact that truncation was being used.\nThe local CE term is useful for both tasks, though it appears more helpful for tagging. 2 This may be because POS tagging is a more local task. Regardless, for both tasks, as shown in Section 4.2, the inclusion of the CE term speeds convergence and improves training stability. For example, on NER, using the CE term reduces the number of epochs chosen by early stopping from \u223c100 to \u223c25. For POS, using the CE term reduces the number of epochs from \u223c150 to \u223c60.\nEffect of Compound Objective and Joint Parameterizations. The compound objective is the sum of the margin-rescaled and perceptron losses, and outperforms them both (see Table 2). Across all tasks, the shared and stacked parameterizations are more accurate than the previous objectives. For the separated parameterization, the performance drops slightly for NER, likely due to the larger number of parameters. The shared and stacked options have fewer parameters to train than the separated option, and the stacked version processes examples at the fastest rate during training.\nThe top part of Table 3 shows how the performance of the test-time inference network A \u03a8 and the cost-augmented inference network F \u03a6 vary when using the new compound objective. The differences between F \u03a6 and A \u03a8 are larger than in the baseline configuration, showing that the two are learning complementary functionality. With the stacked parameterization, the cost-augmented network F \u03a6 receives as an additional input the gold standard label sequence, which leads to the largest differences as the cost-augmented network can explicitly favor incorrect labels. 3 The bottom part of Table 3 shows qualitative differences between the two inference networks. On the POS development set, we count the differences between the predictions of A \u03a8 and F \u03a6 when A \u03a8 makes the correct prediction. 4 F \u03a6 tends to output tags that are highly confusable with those output by A \u03a8 . For example, it often outputs proper noun when the gold standard is common noun or vice versa. It also captures the ambiguities among adverbs, adjectives, and prepositions.\nGlobal Energies. The results are shown in Table 4. Adding the backward (b) and word-augmented TLMs (c) improves over using only the forward TLM from Tu and Gimpel (2018). With the global energies, our performance is comparable to several strong results (90.94 of Lample et al., 2016 and91.37 of Ma andHovy, 2016). However, it is still lower than the state of the art (Akbik et al., 2018;Devlin et al., 2019), likely due to the lack of contextualized embeddings. In other work, we proposed and evaluated several other high-order energy terms for sequence labeling using our framework (Tu et al., 2020a).", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Related Work", "text": "There are several efforts aimed at stabilizing and improving learning in generative adversarial networks (GANs) (Goodfellow et al., 2014;Salimans et al., 2016;Zhao et al., 2017;    from overcoming learning difficulties by modifying loss functions and optimization, and GANs have become more successful and popular as a result. Notably, Wasserstein GANs  provided the first convergence measure in GAN training using Wasserstein distance. To compute Wasserstein distance, the discriminator uses weight clipping, which limits network capacity. Weight clipping was subsequently replaced with a gradient norm constraint (Gulrajani et al., 2017). Miyato et al. (2018) proposed a novel weight normalization technique called spectral normalization. These methods may be applicable to the similar optimization problems solved in learning SPENs. Another direction may be to explore alternative training objectives for SPENs, such as those that use weaker supervision than complete structures (Rooshenas et al., 2018(Rooshenas et al., , 2019Naskar et al., 2020).", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Conclusions and Future Work", "text": "We contributed several strategies to stabilize and improve joint training of SPENs and inference networks. Our use of joint parameterizations mitigates the need for inference network fine-tuning, leads to complementarity in the learned inference networks, and yields improved performance overall. These developments offer promise for SPENs to be more easily applied to a broad range of NLP tasks. Future work will explore other structured prediction tasks, such as parsing and generation. We have taken initial steps in this direction, considering constituency parsing with the sequence-to-sequence model of Tran et al. (2018). Preliminary experiments are positive, 5 but significant challenges remain, specifically in defining appropriate inference network architectures to enable efficient learning.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A Appendices", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 Constituency Parsing Experiments", "text": "We linearize the constituency parsing outputs, similar to Tran et al. (2018). We use the following equation plus global energy in the form of Eq. (8) as the energy function:\nE \u0398 (x, y) = \u2212 T t=1 L j=1 y t,j U j b(x, t) + T t=1 y t\u22121 W y t\nHere, b has a seq2seq-with-attention architecture identical to Tran et al. (2018). In particular, here is the list of implementation decisions.\n\u2022 We can write b = g \u2022 f where f (which we call the \"feature network\") takes in an input sentence, passes it through the encoder, and passes the encoder output to the decoder feature layer to obtain hidden states; g takes in the hidden states and passes them into the rest of the layers in the decoder. In our experiments, the cost-augmented inference network F \u03a6 , test-time inference network A \u03a8 , and b of the energy function above share the same feature network (defined as f above).\n\u2022 The feature network (f ) component of b is pretrained using the feed-forward local crossentropy objective. The cost-augmented inference network F \u03a6 and the test-time inference network A \u03a8 are both pretrained using the feed-forward local cross-entropy objective.\nThe seq2seq baseline achieves 82.80 F1 on the development set in our replication of Tran et al. (2018). Using a SPEN with our stacked parameterization, we obtain 83.22 F1.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We would like to thank the reviewers for insightful comments. This research was supported in part by an Amazon Research Award to K. Gimpel.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Contextual string embeddings for sequence labeling", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Alan Akbik; Duncan Blythe; Roland Vollgraf"}, {"title": "Wasserstein generative adversarial networks", "journal": "", "year": "2017", "authors": "Mart\u00edn Arjovsky; Soumith Chintala; L\u00e9on Bottou"}, {"title": "Structured prediction energy networks", "journal": "", "year": "2016", "authors": "David Belanger; Andrew Mccallum"}, {"title": "End-to-end learning for structured prediction energy networks", "journal": "", "year": "2017", "authors": "David Belanger; Bishan Yang; Andrew Mccallum"}, {"title": "The NXT-format Switchboard Corpus: a rich resource for investigating the syntax, semantics, pragmatics and prosody of dialogue. Language resources and evaluation", "journal": "", "year": "2010", "authors": "Sasha Calhoun; Jean Carletta; Jason M Brenier; Neil Mayo; Dan Jurafsky; Mark Steedman; David Beaver"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Part-of-speech tagging for twitter: Annotation, features, and experiments", "journal": "", "year": "2011", "authors": "Kevin Gimpel; Nathan Schneider; O' Brendan; Dipanjan Connor; Daniel Das; Jacob Mills; Michael Eisenstein; Dani Heilman; Jeffrey Yogatama; Noah A Flanigan;  Smith"}, {"title": "Generative adversarial nets", "journal": "", "year": "2014", "authors": "Ian Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio"}, {"title": "Improved training of Wasserstein GANs", "journal": "", "year": "2017", "authors": "Ishaan Gulrajani; Faruk Ahmed; Martin Arjovsky; Vincent Dumoulin; Aaron C Courville"}, {"title": "Long short-term memory", "journal": "Neural Computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Neural architectures for named entity recognition", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Guillaume Lample; Miguel Ballesteros; Sandeep Subramanian; Kazuya Kawakami; Chris Dyer"}, {"title": "A tutorial on energy-based learning", "journal": "MIT Press", "year": "2006", "authors": "Yann Lecun; Sumit Chopra; Raia Hadsell; Marc'aurelio Ranzato; Fu-Jie Huang"}, {"title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Xuezhe Ma; Eduard Hovy"}, {"title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"title": "Spectral normalization for generative adversarial networks", "journal": "", "year": "2018", "authors": "Takeru Miyato; Toshiki Kataoka; Masanori Koyama; Yuichi Yoshida"}, {"title": "Energy-based reranking: Improving neural machine translation using energy-based models", "journal": "", "year": "2020", "authors": "Subhajit Naskar; Amirmohammad Rooshenas; Simeng Sun; Mohit Iyyer; Andrew Mccallum"}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Olutobi Owoputi; O' Brendan; Chris Connor; Kevin Dyer; Nathan Gimpel; Noah A Schneider;  Smith"}, {"title": "Glove: Global vectors for word representation", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"title": "Design challenges and misconceptions in named entity recognition", "journal": "", "year": "2009", "authors": "Lev Ratinov; Dan Roth"}, {"title": "Training structured prediction energy networks with indirect supervision", "journal": "", "year": "2018", "authors": "Amirmohammad Rooshenas; Aishwarya Kamath; Andrew Mccallum"}, {"title": "Searchguided, lightly-supervised training of structured prediction energy networks", "journal": "", "year": "2019", "authors": "Amirmohammad Rooshenas; Dongxu Zhang; Gopal Sharma; Andrew Mccallum"}, {"title": "Improved techniques for training GANs", "journal": "", "year": "2016", "authors": "Tim Salimans; Ian Goodfellow; Wojciech Zaremba; Vicki Cheung; Alec Radford; Xi Chen"}, {"title": "Max-margin Markov networks", "journal": "", "year": "2004", "authors": "Ben Taskar; Carlos Guestrin; Daphne Koller"}, {"title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition", "journal": "", "year": "2003", "authors": "Erik F Tjong; Kim Sang; Fien De Meulder"}, {"title": "Parsing speech: a neural approach to integrating lexical and acoustic-prosodic information", "journal": "Long Papers", "year": "2018", "authors": "Trang Tran; Shubham Toshniwal; Mohit Bansal; Kevin Gimpel; Karen Livescu; Mari Ostendorf"}, {"title": "Support vector machine learning for interdependent and structured output spaces", "journal": "", "year": "2004", "authors": "Ioannis Tsochantaridis; Thomas Hofmann; Thorsten Joachims; Yasemin Altun"}, {"title": "Learning approximate inference networks for structured prediction", "journal": "", "year": "2018", "authors": "Lifu Tu; Kevin Gimpel"}, {"title": "Benchmarking approximate inference methods for neural structured prediction", "journal": "Long and Short Papers", "year": "2019", "authors": "Lifu Tu; Kevin Gimpel"}, {"title": "Learning to embed words in context for syntactic tasks", "journal": "", "year": "2017", "authors": "Lifu Tu; Kevin Gimpel; Karen Livescu"}, {"title": "An exploration of arbitrary-order sequence labeling via energy-based inference networks", "journal": "", "year": "2020", "authors": "Lifu Tu; Tianyu Liu; Kevin Gimpel"}, {"title": "ENGINE: Energy-based inference networks for non-autoregressive machine translation", "journal": "", "year": "2020", "authors": "Lifu Tu; Richard Yuanzhe Pang; Sam Wiseman; Kevin Gimpel"}, {"title": "Energy-based generative adversarial network", "journal": "", "year": "2017", "authors": "Jake Junbo; Micha\u00ebl Zhao; Yann Mathieu;  Le-Cun"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Joint parameterizations for cost-augmented inference network F \u03a6 and test-time inference network A \u03a8 .", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Adding CE loss (without truncation).", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Part-of-speech tagging training trajectories. The three curves in each setting correspond to different random seeds. (a) Without the local CE loss, training fails when using zero truncation. (b) The CE loss reduces the number of epochs for training. Tu and Gimpel (2018) used zero truncation and CE during training.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "over all training pairs (x, y) after each set of I steps. The I steps update \u03a8 and \u03a6 to maximize these losses. Meanwhile the E steps update \u0398 to minimize these losses. Figs. 3(a) and (b) show l 1 and l 0 during training for different numbers (k) of I steps for every one E step. Fig. 3(c) shows the norm of the energy parameters after the E steps, and Fig. 3(d) shows the norm of \u2202E \u0398 (x,A \u03a8 ) \u2202\u03a8 after the I steps.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: POS training trajectories with different numbers of I steps. The three curves in each setting correspond to different random seeds. (a) cost-augmented loss after I steps; (b) margin-rescaled hinge loss after I steps; (c) gradient norm of energy function parameters after E steps; (d) gradient norm of test-time inference network parameters after I steps.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Test results for POS and NER for several SPEN configurations. Results with * correspond to the setting of", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "). Progress in training GANs has come largely", "figure_data": "POSNERNER+acc. (%) |T ||I|speed F1 (%) |T ||I|speed F1 (%)BiLSTM88.8166K 166K-84.9 239K 239K-89.3SPENs with inference networks (Tu and Gimpel, 2018):margin-rescaled89.4333K 166K-85.2 479K 239K-89.5perceptron88.6333K 166K-84.4 479K 239K-89.0SPENs with inference networks, compound objective, CE, no zero truncation (this paper):separated89.7500K 166K6685.0 719K 239K3289.8shared89.8339K 166K7885.6 485K 239K3890.1stacked89.8335K 166K9285.6 481K 239K4690.1"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Test results for POS and NER. |T | is the number of trained parameters; |I| is the number of parameters needed during inference. Training speeds (examples/second) are shown for joint parameterizations to compare them in terms of efficiency. Best setting (best performance with fewest parameters and fastest training) is in bold.", "figure_data": "POSNERA\u03a8 \u2212 F\u03a6A\u03a8 \u2212 F\u03a6margin-rescaled0.20separated2.20.4compoundshared1.90.5stacked2.61.7test-time (A\u03a8)cost-augmented (F\u03a6)common nounproper nounproper nouncommon nouncommon nounadjectiveproper nounproper noun + possessiveadverbadjectiveprepositionadverbadverbprepositionverbcommon nounadjectiveverb"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Top: differences in accuracy/F1 between test-time inference networks A \u03a8 and cost-augmented networks F \u03a6 (on development sets). The \"marginrescaled\" row uses a SPEN with the local CE term and without zero truncation, where A \u03a8 is obtained by finetuning F \u03a6 as done byTu and Gimpel (2018). Bottom: most frequent output differences between A \u03a8 and F \u03a6 on the development set.", "figure_data": "NER NER+ NER++margin-rescaled85.289.590.2compound, stacked, CE, no truncation85.690.190.8+ global energy GE(a)85.890.290.7+ global energy GE(b)85.990.290.8+ global energy GE(c)86.390.491.0"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "NER test F1 scores with global energy terms.", "figure_data": ""}], "doi": "10.18653/v1/N19-1423"}