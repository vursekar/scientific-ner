{"authors": "Ethan C Chau; Noah A Smith", "pub_date": "", "title": "Specializing Multilingual Language Models: An Empirical Study", "abstract": "Pretrained multilingual language models have become a common tool in transferring NLP capabilities to low-resource languages, often with adaptations. In this work, we study the performance, extensibility, and interaction of two such adaptations: vocabulary augmentation and script transliteration. Our evaluations on part-of-speech tagging, universal dependency parsing, and named entity recognition in nine diverse low-resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low-resource settings.", "sections": [{"heading": "Introduction", "text": "Research in natural language processing is increasingly carried out in languages beyond English. This includes high-resource languages with abundant data, as well as low-resource languages, for which labeled (and unlabeled) data is scarce. In fact, many of the world's languages fall into the latter category, even some with a high number of speakers. This presents unique challenges compared to high-resource languages: effectively modeling low-resource languages involves both accurately tokenizing text in such languages and maximally leveraging the limited available data.\nOne common approach to low-resource NLP is the multilingual paradigm, in which methods that have shown success in English are applied to the union of many languages' data, 1 enabling transfer between languages. For instance, multilingual contextual word representations (CWRs) from language models (Devlin et al., 2019;Huang et al., 2019;Lample and Conneau, 2019, inter alia) are conventionally \"pretrained\" on large multilingual 1 Within the multilingual paradigm, a distinction is sometimes made between massively multilingual methods, which consider tens or hundreds of languages; and polyglot methods, which use only a handful. In this paper, all mentions of \"multilingual\" refer to the former. corpora before being \"finetuned\" directly on supervised tasks; this pretraining-finetuning approach is derived from analogous monolingual models (Devlin et al., 2019;Liu et al., 2019;. However, considering the diversity of the world's languages and the great data imbalance among them, it is natural to question whether the current multilingual paradigm can be improved upon for low-resource languages.\nIndeed, past work has demonstrated that it can. For instance, Wu and Dredze (2020) find that multilingual models often lag behind non-contextualized baselines for the lowest-resource languages in their training data, drawing into question their utility in such settings. Conneau et al. (2020a) posit that this phenomenon is a result of limited model capacity, which proves to be a bottleneck for sufficient transfer to low-resource languages. In fact, with multilingual models only being pretrained on a limited set of languages, most of the world's languages are unseen by the model. For such languages, the performance of such models is even worse (Chau et al., 2020), due in part to the diversity of scripts across the world's languages (Muller et al., 2021;Pfeiffer et al., 2021b;Rust et al., 2021) as compared to the models' Latin-centricity (\u00c1cs, 2019).\nNonetheless, there have been multiple attempts to remedy this discrepancy by specializing 2 a multilingual model to a given target low-resource language, from which we take inspiration. Among them, Chau et al. (2020) augment the model's vocabulary to more effectively tokenize text, then pretrain on a small amount of data in the target language; they report significant performance improvements on a small set of low-resource languages. In a similar vein, Muller et al. (2021) propose to transliterate text in the target language to Latin script to be better tokenized by the existing model, followed by additional pretraining; they observe mixed results and note that transliteration quality may be a confounding factor. We hypothesize that these two methods can serve as the basis for improvements in modeling a broad set of low-resource languages.\nIn this work, we study the effectiveness, extensibility, and interaction of these two approaches to specialization: the vocabulary augmentation technique of Chau et al. (2020) and the script transliteration method of Muller et al. (2021). We verify the performance of vocabulary augmentation on three tasks in a diverse set of nine low-resource languages across three different scripts, especially on non-Latin scripts ( \u00a72) and find that these gains are associated with improved vocabulary coverage of the target language. We further observe a negative interaction between vocabulary augmentation and transliteration in light of a broader framework for specializing multilingual models, while noting that vocabulary augmentation offers an appealing balance of performance and cost ( \u00a73). Overall, our results highlight several possible directions for future study in the low-resource setting. Our code, data, and hyperparameters are publicly available. 3", "n_publication_ref": 17, "n_figure_ref": 0}, {"heading": "Revisiting Vocabulary Augmentation", "text": "We begin by revisiting the Vocabulary Augmentation method of Chau et al. (2020), which we recast more generally in light of recent work ( \u00a72.1). We evaluate their claims on three different tasks, using a diverse set of languages in multiple scripts ( \u00a72.2), and find that the results hold to an even more pronounced degree in unseen low-resource languages with non-Latin scripts ( \u00a72.3).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Method Overview", "text": "Following Chau et al. (2020), we consider how to apply the pretrained multilingual BERT model (MBERT; Devlin et al., 2019) to a target lowresource language, for which both labeled and unlabeled data is scarce. This model has produced strong CWRs for many languages (Kondratyuk and Straka, 2019, inter alia) and has been the starting model for many studies on low-resource languages (Muller et al., 2021;Pfeiffer et al., 2020;. MBERT covers the languages with the 104 largest Wikipedias, and it uses this data to con-struct a wordpiece vocabulary (Wu et al., 2016) and train its transformer-based architecture (Vaswani et al., 2017). Although low-resource languages are slightly oversampled, high-resource languages still dominate both the final pretraining data and the vocabulary (\u00c1cs, 2019;Devlin et al., 2019). Chau et al. (2020) note that target low-resource languages fall into three categories with respect to MBERT's pretraining data: the lowest-resource languages in the data (Type 1), completely unseen low-resource languages (Type 2), and low-resouce languages with more representation (Type 0). 4 Due to their poor representation in the vocabulary, Type 1 and Type 2 languages achieve suboptimal tokenization and higher rates of the \"unknown\" wordpiece 5 when using MBERT out of the box. This hinders the model's ability to capture meaningful patterns in the data, resulting in reduced data efficiency and degraded performance.\nWe note that this challenge is exacerbated when modeling languages written in non-Latin scripts. MBERT's vocabulary is heavily Latin-centric (\u00c1cs, 2019;Muller et al., 2021), resulting in a significantly larger portion of non-Latin scripts being represented with \"unknown\" tokens (Pfeiffer et al., 2021b) and further limiting the model's ability to generalize. In effect, MBERT's low initial performance on such languages can be attributed to its inability to represent the script itself.\nTo alleviate the problem of poor tokenization, Chau et al. (2020) propose to specialize MBERT using Vocabulary Augmentation (VA). Given unlabeled data in the target language, they train a new wordpiece vocabulary on the data, then select the 99 most common wordpieces in the new vocabulary that replace \"unknown\" tokens under the original vocabulary. They then add these 99 wordpieces to the original vocabulary and continue pretraining MBERT on the unlabeled data for additional steps. They further describe a tiered variant (TVA), in which a larger learning rate is used for the embeddings of these 99 new wordpieces. VA yields strong gains over unadapted multilingual language models on dependency parsing in four low-resource languages with Latin scripts. How-ever, no evaluation has been performed on other tasks or on languages with non-Latin scripts, which raises our first research question: RQ1: Do the conclusions of Chau et al. (2020) hold for other tasks and for languages with non-Latin scripts?\nWe can view VA and TVA as an instantation of a more general framework of vocabulary augmentation, shared by other approaches to using MBERT in low-resource settings. Given a new vocabulary V , number of wordpieces n, and learning rate multiplier a, the n most common wordpieces in V are added to the original vocabulary. Additional pretraining is then performed, with the embeddings of the n wordpieces taking on a learning rate a times greater than the overall learning rate. For VA, we set n = 99 and a = 1, while we treat a as a hyperparameter for TVA. The related E-MBERT method of  sets n = |V | and a = 1. Investigating various other instantiations of this framework is an interesting research direction, though it is out of the scope of this work.", "n_publication_ref": 16, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We expand on the dependency parsing evaluations of Chau et al. (2020) by additionally considering named entity recognition and part-of-speech tagging. We follow Kondratyuk and Straka (2019) and compute the CWR for each token as a weighted sum of the activations at each MBERT layer. For dependency parsing, we follow the setup of Chau et al. (2020) and Muller et al. (2021) and use the CWRs as input to the graph-based dependency parser of Dozat and Manning (2017). For named entity recognition, the CWRs are used as input to a CRF layer, while part-of-speech tagging uses a linear projection atop the representations. In all cases, the underlying CWRs are finetuned during downstream task training, and we do not add an additional encoder layer above the transformer outputs. We train models on five different random seeds and report average scores and standard errors.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Languages and Datasets", "text": "We select a set of nine typologically diverse lowresource languages for evaluation, including three of the original four used by Chau et al. (2020). These languages use three different scripts and are chosen based on the availability of labeled datasets and their exemplification of the three language types identified by Chau et al. (2020). Of the lan-guages seen by MBERT, all selected Type 0 languages are within the 45 largest Wikipedias, while the remaining Type 1 languages are within the top 100. The Type 2 languages, which are excluded from MBERT, are all outside of the top 150. 6 Additional information about the evaluation languages is given in Tab. 1.\nUnlabeled Datasets Following Chau et al. (2020), we use articles from Wikipedia as unlabeled data for additional pretraining in order to reflect the original pretraining data. We downsample full articles from the largest Wikipedias to be on the order of millions of tokens in order to simulate a low-resource unlabeled setting, and we remove sentences that appear in the labeled validation or test sets.\nLabeled Datasets For dependency parsing and part-of-speech tagging, we use datasets and train/test splits from Universal Dependencies (Nivre et al., 2020), version 2.5 (Zeman et al., 2019). POS tagging uses language-specific partof-speech tags (XPOS) to evaluate understanding of language-specific syntactic phenomena. The Belarusian treebank lacks XPOS tags for certain examples, so we use universal part-of-speech tags instead. Dependency parsers are trained with gold word segmentation and no part-of-speech features. Experiments with named entity recognition use the WikiAnn dataset (Pan et al., 2017), following past work (Muller et al., 2021;Pfeiffer et al., 2020;Wu and Dredze, 2020). Specifically, we use the balanced train/test splits of (Rahimi et al., 2019). We note that UD datasets were unavailable for Meadow Mari, and partitioned WikiAnn datasets were missing for Wolof.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Baselines", "text": "To measure the effectiveness of VA, we benchmark it against unadapted MBERT, as well as directly pretraining MBERT on the unlabeled data without modifying the vocabulary (Chau et al., 2020;Muller et al., 2021;Pfeiffer et al., 2020). Following Chau et al. (2020), we refer to the latter approach as language-adaptive pretraining (LAPT). We also evaluate two monolingual baselines that are trained on our unlabeled data: fastText embeddings (FASTT; Bojanowski et al., 2017), which represent a static word vector approach; and a BERT model trained from scratch (BERT). For   (Liu et al., 2019) with a language-specific SentencePiece tokenizer (Kudo and Richardson, 2018). For a fair comparison to VA, we use the same task-specific architectures and modify only the input representations.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Implementation Details", "text": "To pretrain LAPT and VA models, we use the code of Chau et al. (2020), who modify the pretraining code of Devlin et al. (2019) to only use the masked language modeling (MLM) loss. To generate VA vocabularies, we train a new vocabulary of size 5000 and select the 99 wordpieces that replace the most unknown tokens. We train with a fixed linear warmup of 1000 steps. To pretrain BERT models, we use the HuggingFace Transformers library (Wolf et al., 2020). Following Muller et al. (2021), we train a half-sized RoBERTa model with six layers and 12 attention heads. We use a byte-pair vocabulary of size 52000 and a linear warmup of 1 epoch. For LAPT, VA, and BERT, we train for up to 20 epochs total, selecting the highest-performing epoch based on validation masked language modeling loss. FASTT models are trained with the skipgram model for five epochs, with the default hyperparameters of Bojanowski et al. (2017).\nTraining of downstream parsers and taggers follows Chau et al. (2020) and Kondratyuk and Straka (2019), with an inverse square-root learning rate decay and linear warmup, and layer-wise gradual unfreezing and discriminative finetuning. Models are trained with AllenNLP, version 2.1.0 , for up to 200 epochs with early stopping based on validation performance. We choose batch sizes to be the maximum that allows for successful training on one GPU.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Results", "text": "Tab. 2 presents performance of the different input representations on POS tagging, dependency parsing, and named entity recognition. VA achieves strong results across all languages and tasks and is the top performer in the majority of them, suggesting that augmenting the vocabulary addresses MBERT's limited vocabulary coverage of the target language and is beneficial during continued pretraining.\nThe relative gains that VA provides appear to correlate not only with language type, as in the findings of Chau et al. (2020), but also with each language's script. For instance, in Vietnamese, which is a Type 0 Latin script language, the improvements from VA are marginal at best, reflecting the Latindominated pretraining data of MBERT. Irish, the Type 1 Latin script language, is only slightly more receptive. However, Type 0 languages in Cyrillic and Arabic scripts, which are less represented in MBERT's pretraining data, are more receptive to VA, with VA even outperforming all other methods for Urdu. This trend is amplified in the Type 2 languages, as the improvements for Maltese and Wolof are small but significant. However, they are dwarfed in magnitude by those of Uyghur, where VA achieves up to a 57% relative error reduction over LAPT. This result corroborates the findings of both Chau et al. (2020) and Muller et al. (2021) and answers RQ1. Prior to specialization, MBERT is especially poorly equipped to handle unseen lowresource languages and languages in non-Latin scripts due to its inability to model the script itself. In such cases, specialization via VA is beneficial, providing MBERT with explicit signal about the target language and script while maintaining its language-agnostic insights. On the other hand, this also motivates additional investigation into reme-  dies for the script imbalance at a larger scale, e.g., more diverse pretraining data.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Analysis", "text": "We perform further analysis to investigate VA's patterns of success. Concretely, we hypothesize that VA significantly improves the tokenizer's coverage of target languages where it is most successful. Inspired by \u00c1cs (2019), Chau et al. (2020), and Rust et al. (2021), we quantify tokenizer coverage using the percentage of tokens in the raw text that yield unknown wordpieces when tokenized with a given vocabulary (\"UNK token percentage\"). These are tokens whose representations contain at least partial ambiguity due to the inclusion of the unknown wordpiece.\nTab. 3 presents the UNK token percentage for each dataset using the MBERT vocabulary, averaged over each script and language type. This vocabulary is used in LAPT and represents the baseline level of vocabulary coverage. We also include the change in the UNK token percentage between the MBERT and VA vocabularies, which quantifies the coverage improvement. Both sets of values are juxtaposed against the average change in task-specific performance from LAPT to VA, representing the effect of augmenting the vocabulary on task-specific performance.\nWe observe that off-the-shelf MBERT already at-tains relatively high vocabulary coverage for Type 0 and 1 languages, as well as languages written in Latin and Cyrillic scripts. On the other hand, up to one-fifth of the tokens in Arabic languages and one-sixth of those in Type 2 languages yield an unknown wordpiece. For these languages, there is great room for increasing tokenizer coverage, and VA indeed addresses this more tangible need. This aligns with the task-specific performance improvements for each group and helps to explain our results in \u00a72.3.\nIt is notable that VA does not always eliminate the issue of unknown wordpieces, even in languages for which MBERT attains high vocabulary coverage. This suggests that the remaining unknown wordpieces in these languages are more sparsely distributed (i.e., they represent low frequency sequences), while the unknown wordpieces in languages with lower vocabulary coverage represent sequences that occur more commonly. As a result, augmenting the vocabulary in such languages quickly improves coverage while associating these commonly occurring sequences with each other, which benefits the overall tokenization quality.\nWe further explore the association between the improvements in vocabulary coverage and taskspecific performance in Fig. 1. Although we do not find that languages from the same types or scripts form clear clusters, we nonetheless observe a loose  correlation between the two factors in question and see that VA delivers greater performance gains on Type 2 and Arabic-script languages compared to their Type 0/1 and Latin-script counterparts, respectively. To quantify the strength of this association, we also compute the language-level Spearman correlation between the change in UNK token percentage on the unlabeled dataset 7 from the MBERT to VA vocabulary and the task-specific performance improvements from LAPT to VA. The resulting \u03c1-values -0.29 for NER, 0.56 for POS tagging, and 0.81 for UD parsing -suggest that this set of factors is meaningful for some tasks, though additional and more fine-grained analysis in future work should give a more complete explanation.\n3 Mix-in Specialization: VA and Transliteration\nWe now expand on the observation made in \u00a72.3 regarding the difficulties that MBERT encounters when faced with unseen low-resource languages in non-Latin scripts because of its inability to model the script. Having observed that VA is beneficial in such cases, we now investigate the interaction between this method and another specialization approach that targets this problem. Specifically, we consider the transliteration methods of Muller et al. (2021), in which unseen low-resource languages in non-Latin scripts are transliterated into the Latin script, often using transliteration schemes inspired by the Latin orthographies of languages related to the target language. They hypothesize that the increased similarity in the languages' writing systems, combined with MBERT's overall Latin-centricity, provides increased opportunity for crosslingual transfer.\nWe can view transliteration as a inverted form of vocabulary augmentation: instead of adapting the model to the needs of the data, the data is adjusted to meet the assumptions of the model. Furthermore, the transliteration step is performed prior to pretraining MBERT on additional unlabeled data in the target language, the same stage at which VA is performed. In both cases, the ultimate goal is identical: improving tokenization and more effectively using available data. We can thus view transliteration and VA as two instantiations of a more general mix-in paradigm for model specialization, whereby various transformations (mix-ins) are applied to the data and/or model prior to performing additional pretraining. These mix-ins target different components of the experimental pipeline, which naturally raises our second research question:\nRQ2: How do the VA and transliteration mix-ins for MBERT compare and interact?", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Method and Experiments", "text": "To test this research question, we apply transliteration and VA in succession and evaluate their compatibility. Given unlabeled data in the target language, we first transliterate it into Latin script, which decreases but does not fully eliminate the issue of unseen wordpieces. We then perform VA, generating the vocabulary for augmentation based on the transliterated data.\nWe evaluate on Meadow Mari and Uyghur, which are Type 2 languages where transliteration was successfully applied by Muller et al. (2021). To transliterate the data, we use the same methods as Muller et al. (2021): Meadow Mari uses the transliterate 8 package, while Uyghur uses  a linguistically-motivated transliteration scheme 9 aimed at associating Uyghur with Turkish. We use the same training scheme, model architectures, and baselines as in \u00a72.2, the only difference being the use of transliterated data. This includes directly pretraining on the unlabeled data (LAPT), which is comparable to the highest-performing transliteration models of Muller et al. (2021). Although our initial investigation of VA in \u00a72 also included non-Type 2 languages of other scripts, we omit them from our investigation based on the finding of Muller et al. (2021) that transliterating higherresource languages into Latin scripts is not beneficial.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Results", "text": "Tab. 4 gives the results of our transliteration mix-in experiments. For the MBERT-based models, both VA and transliteration provide strong improvements over their respective baselines. Specifically, the improvements from LAPT to VA and LAPT to LAPT with transliteration are most pronounced. This verifies the independent results of Chau et al. (2020) and Muller et al. (2021) and suggests that in the non-Latin low-resource setting, unadapted additional pretraining is insufficient, but that the mix-in stage between initial and additional pretraining is amenable to performance-improving modifications. Unsurprisingly, transliteration provides no consistent improvement to the monolingual baselines, since the noisy transliteration process removes information without improving crosslingual alignment. However, VA and transliteration appear to interact negatively. Although VA with transliteration im-proves over plain VA for Uyghur POS tagging and dependency parsing, it still slightly underperforms LAPT with transliteration for the latter. For the two NER experiments, VA with transliteration lags both methods independently. One possible explanation is that transliteration into Latin script serves as implicit vocabulary augmentation, with embeddings that have already been updated during the initial pretraining stage; as a result, the two sources of augmentation conflict. Alternatively, since the transliteration process merges certain characters that are distinct in the original script, VA may augment the vocabulary with misleading character clusters. Either way, additional vocabulary augmentation is generally not as useful when combined with transliteration, answering RQ2.\nNonetheless, additional investigation into the optimal amount of vocabulary augmentation might yield a configuration that is consistently complementary to transliteration and is an interesting direction for future work. Furthermore, designing linguistically-informed transliteration schemes like those devised by Muller et al. (2021) for Uyghur requires large amounts of time and domain knowledge. VA's fully data-driven nature and relatively comparable performance suggest that it achieves an appealing balance between performance gain and implementation difficulty.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Our work follows a long line of studies investigating the performance of multilingual language models like MBERT in various settings. The exact source of such models' crosslingual ability is contested: early studies attributed MBERT's success to vocabulary overlap between languages (Cao et al., 2020;Pires et al., 2019;Wu and Dredze, 2019)  but subsequent studies find typological similarity and parameter sharing to be better explanations (Conneau et al., 2020b;K et al., 2020). Nonetheless, past work has consistently highlighted the limitations of multilingual models in the context of low-resource languages. Conneau et al. (2020a) highlight the tension between crosslingual transfer and per-language model capacity, which poses a challenge for low-resource languages that require both. Indeed, Wu and Dredze (2020) find that MBERT is unable to outperform baselines in the lowest-resource seen languages. Our experiments build off these insights, which motivate the development of methods for adapting MBERT to target low-resource languages.\nAdapting Language Models Several prior studies have proposed methods for adapting pretrained models to a downstream task. The simplest of these is to perform additional pretraining on unlabeled data in the target language (Chau et al., 2020;Muller et al., 2021;Pfeiffer et al., 2020), which in turn builds off similar approaches for domain adaptation (Gururangan et al., 2020;Han and Eisenstein, 2019). Recent work uses one or more of these additional pretraining stages to specifically train modular adapter layers for specific tasks or languages, with the goal of maintaining a language-agnostic model while improving performance on individual languages (Pfeiffer et al., 2020(Pfeiffer et al., , 2021aVidoni et al., 2020). However, as Muller et al. (2021) note, the typological diversity of the world's languages ultimately limits the viability of this approach.\nOn the other hand, many adaptation techniques have focused on improving representation of the target language by modifying the model's vocabulary or tokenization schemes (Chung et al., 2020;Clark et al., 2021;Wang et al., 2021). This is wellmotivated: Artetxe et al. (2020) emphasize representation in the vocabulary as a key factor for effective crosslingual transfer, while Rust et al. (2021) find that MBERT's tokenization scheme for many languages is subpar. Pfeiffer et al. (2021b) further observe that for languages with unseen scripts, a large proportion of the language is mapped to the generic \"unknown\" wordpiece, and they propose a matrix factorization-based approach to improve script representation. Wang et al. ( 2020) extend MBERT's vocabulary with an entire new vocabulary in the target language to facilitate zero-shot transfer to low-resource languages from English. The present study most closely derives from Chau et al. (2020), who select 99 wordpieces with the greatest amount of coverage to augment MBERT's vocabulary while preserving the remainder; and Muller et al. (2021), who transliterate target language data into Latin script to improve vocabulary coverage. We deliver new insights on the effectiveness and applicability of these methods.", "n_publication_ref": 24, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We explore the interactions between vocabulary augmentation and script transliteration for specializing multilingual contextual word representations in low-resource settings. We confirm vocabulary augmentation's effectiveness on multiple languages, scripts, and tasks; identify the mix-in stage as amenable to specialization; and observe a negative interaction between vocabulary augmentation and script transliteration. Our findings highlight several open questions in model specialization and low-resource natural language processing at large, motivating further study in this area.\nFuture directions for investigation are manifold. In particular, our results in this work unify the separate findings of past works, which use MBERT as a case study; a natural continuation would extend these methods to a broader set of multilingual models, such as mT5 (Xue et al., 2021) and XLM-R (Conneau et al., 2020a), in order to obtain a clearer understanding of the factors behind specialization methods' patterns of success. While we intentionally choose a set of small unlabeled datasets to evaluate on a setting applicable to the vast majority of the world's low-resource languages, we acknowl-edge great variation in the amount of unlabeled data available in different languages. Continued study on the applicability of these methods to datasets of different sizes is an important future step. An interesting direction of work is to train multilingual models on data where script respresentation is more balanced, which might also allow for different output scripts for transliteration. Given that the mix-in stage is an effective opportunity to specialize models to target languages, constructing mix-ins at both the data and model level that are complementary by design has potential to be beneficial. Finally, future work might shed light on the interaction between different configurations of the adaptations studied here (e.g., the number of wordpiece types used in vocabulary augmentation).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We thank Jungo Kasai, Phoebe Mulcaire, and members of UW NLP for their helpful comments on preliminary versions of this paper. We also thank Benjamin Muller for insightful discussions and providing details about transliteration methods and baselines. Finally, we thank the anonymous reviewers for their helpful remarks.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Exploring BERT's vocabulary", "journal": "", "year": "2019", "authors": "Judit \u00c1cs"}, {"title": "On the cross-lingual transferability of monolingual representations", "journal": "", "year": "2020", "authors": "Mikel Artetxe; Sebastian Ruder; Dani Yogatama"}, {"title": "Enriching word vectors with subword information", "journal": "TACL", "year": "2017", "authors": "Piotr Bojanowski; Edouard Grave; Armand Joulin; Tomas Mikolov"}, {"title": "Multilingual alignment of contextual word representations", "journal": "", "year": "2020", "authors": "Steven Cao; Nikita Kitaev; Dan Klein"}, {"title": "Parsing with multilingual BERT, a small corpus, and a small treebank", "journal": "", "year": "2020", "authors": "Ethan C Chau; Lucy H Lin; Noah A Smith"}, {"title": "Improving multilingual models with language-clustered vocabularies", "journal": "", "year": "2020", "authors": "Dan Hyung Won Chung;  Garrette; Jason Kiat Chuan Tan;  Riesa"}, {"title": "Canine: Pre-training an efficient tokenization-free encoder for language representation", "journal": "", "year": "2021", "authors": "Jonathan H Clark; Dan Garrette; Iulia Turc; John Wieting"}, {"title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Emerging cross-lingual structure in pretrained language models", "journal": "", "year": "2020", "authors": "Alexis Conneau; Shijie Wu; Haoran Li; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Deep biaffine attention for neural dependency parsing", "journal": "", "year": "2017", "authors": "Timothy Dozat; Christopher D Manning"}, {"title": "AllenNLP: A deep semantic natural language processing platform", "journal": "", "year": "2018", "authors": "Matt Gardner; Joel Grus; Mark Neumann; Oyvind Tafjord; Pradeep Dasigi; Nelson F Liu; Matthew Peters; Michael Schmitz; Luke Zettlemoyer"}, {"title": "2020. Don't stop pretraining: Adapt language models to domains and tasks", "journal": "", "year": "", "authors": "Ana Suchin Gururangan; Swabha Marasovi\u0107; Kyle Swayamdipta; Iz Lo; Doug Beltagy; Noah A Downey;  Smith"}, {"title": "Unsupervised domain adaptation of contextualized embeddings for sequence labeling", "journal": "", "year": "2019", "authors": "Xiaochuang Han; Jacob Eisenstein"}, {"title": "Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks", "journal": "", "year": "2019", "authors": "Haoyang Huang; Yaobo Liang; Nan Duan; Ming Gong; Linjun Shou; Daxin Jiang; Ming Zhou"}, {"title": "Cross-lingual ability of multilingual BERT: An empirical study", "journal": "", "year": "2020", "authors": "K Karthikeyan; Zihan Wang; Stephen Mayhew; Dan Roth"}, {"title": "75 languages, 1 model: Parsing universal dependencies universally", "journal": "", "year": "2019", "authors": "Dan Kondratyuk; Milan Straka"}, {"title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"title": "Crosslingual language model pretraining", "journal": "", "year": "2019", "authors": "Guillaume Lample; Alexis Conneau"}, {"title": "", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Beno\u00eet Sagot, and Djam\u00e9 Seddah. 2021. When being unseen from mBERT is just the beginning: Handling new languages with multilingual language models", "journal": "", "year": "", "authors": "Benjamin Muller; Antonios Anastasopoulos"}, {"title": "Universal Dependencies v2: An evergrowing multilingual treebank collection", "journal": "", "year": "2020", "authors": "Joakim Nivre; Marie-Catherine De Marneffe; Filip Ginter; Jan Haji\u010d; Christopher D Manning; Sampo Pyysalo"}, {"title": "Cross-lingual name tagging and linking for 282 languages", "journal": "", "year": "2017", "authors": "Xiaoman Pan; Boliang Zhang; Jonathan May; Joel Nothman; Kevin Knight; Heng Ji"}, {"title": "Deep contextualized word representations", "journal": "", "year": "2018", "authors": "Matthew Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"title": "AdapterFusion: Non-destructive task composition for transfer learning", "journal": "", "year": "2021", "authors": "Jonas Pfeiffer; Aishwarya Kamath; Andreas R\u00fcckl\u00e9; Kyunghyun Cho; Iryna Gurevych"}, {"title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer", "journal": "", "year": "2020", "authors": "Jonas Pfeiffer; Ivan Vuli\u0107; Iryna Gurevych; Sebastian Ruder"}, {"title": "Unks everywhere: Adapting multilingual language models to new scripts", "journal": "", "year": "2021", "authors": "Jonas Pfeiffer; Ivan Vuli\u0107"}, {"title": "How multilingual is multilingual BERT?", "journal": "", "year": "2019", "authors": "Telmo Pires; Eva Schlinger; Dan Garrette"}, {"title": "Massively multilingual transfer for NER", "journal": "", "year": "2019", "authors": "Afshin Rahimi; Yuan Li; Trevor Cohn"}, {"title": "How good is your tokenizer? on the monolingual performance of multilingual language models", "journal": "", "year": "2021", "authors": "Phillip Rust; Jonas Pfeiffer; Ivan Vuli\u0107; Sebastian Ruder; Iryna Gurevych"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"title": "Orthogonal language and task adapters in zero-shot cross-lingual transfer", "journal": "", "year": "2020", "authors": "Marko Vidoni; Ivan Vuli\u0107; Goran Glava\u0161"}, {"title": "Multi-view subword regularization", "journal": "", "year": "2021", "authors": "Xinyi Wang; Sebastian Ruder; Graham Neubig"}, {"title": "Extending multilingual BERT to lowresource languages", "journal": "", "year": "2020", "authors": "Zihan Wang; K Karthikeyan; Stephen Mayhew; Dan Roth"}, {"title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"title": "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT", "journal": "", "year": "2019", "authors": "Shijie Wu; Mark Dredze"}, {"title": "Are all languages created equal in multilingual BERT?", "journal": "", "year": "2020", "authors": "Shijie Wu; Mark Dredze"}, {"title": "Google's neural machine translation system", "journal": "Oriol Vinyals", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; Quoc V Le; Mohammad Norouzi; Wolfgang Macherey; Maxim Krikun; Yuan Cao; Qin Gao; Klaus Macherey; Jeff Klingner; Apurva Shah; Melvin Johnson; Xiaobing Liu; \u0141ukasz Kaiser; Stephan Gouws; Yoshikiyo Kato; Taku Kudo; Hideto Kazawa; Keith Stevens; George Kurian; Nishant Patil; Wei Wang ; Greg; Macduff Corrado; Jeffrey Hughes;  Dean"}, {"title": "Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer", "journal": "", "year": "", "authors": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant"}, {"title": "", "journal": "Aur\u00e9lie Collomb, \u00c7agr\u0131 \u00c7\u00f6ltekin", "year": "", "authors": "Daniel Zeman; Joakim Nivre; Mitchell Abrams; No\u00ebmi Aepli; \u017deljko Agi\u0107; Lars Ahrenberg; Gabriel\u0117 Aleksandravi\u010di\u016bt\u0117; Lene Antonsen; Katya Aplonova; Maria Jesus Aranzabe; Gashaw Arutie; Masayuki Asahara; Luma Ateyah; Mohammed Attia; Aitziber Atutxa; Liesbeth Augustinus; Elena Badmaeva; Miguel Ballesteros; Esha Banerjee; Sebastian Bank; Victoria Verginica Barbu Mititelu; Colin Basmov; John Batchelor; Sandra Bauer; Kepa Bellato; Yevgeni Bengoetxea;  Berzak; Ahmad Irshad; Riyaz Ahmad Bhat; Erica Bhat; Eckhard Biagetti; Agn\u0117 Bick; Rogier Bielinskien\u0117; Victoria Blokland; Lo\u00efc Bobicev; Emanuel Borges Boizou; Carl V\u00f6lker; Cristina B\u00f6rstell; Gosse Bosco; Sam Bouma; Adriane Bowman; Kristina Boyd; Aljoscha Brokait\u0117; Marie Burchardt; Bernard Candito; Gauthier Caron; Tatiana Caron; G\u00fcl\u015fen Cavalcanti; Flavio Cebiroglu Eryigit; Giuseppe G A Massimiliano Cecchini;  Celano; Savas Slavom\u00edr\u010d\u00e9pl\u00f6; Fabricio Cetin; Jinho Chalub; Yongseok Choi; Jayeol Cho; Alessandra T Chun; Silvie Cignarella;  Cinkov\u00e1"}, {"title": "Tiina Puolakainen, Sampo Pyysalo, Peng Qi, Andriela R\u00e4\u00e4bis, Alexandre Rademaker, Loganathan Ramasamy, Taraka Rama, Carlos Ramisch, Vinit Ravishankar, Livy Real, Siva Reddy", "journal": "", "year": "", "authors": "Toma\u017e Erina; Aline Erjavec; Wograine Etienne; Rich\u00e1rd Evelyn; Hector Farkas; Jennifer Fernandez Alcalde; Cl\u00e1udia Foster; Kazunori Freitas; Katar\u00edna Fujita; Daniel Gajdo\u0161ov\u00e1; Marcos Galbraith; Moa Garcia; Sebastian G\u00e4rdenfors; Kim Garza; Filip Gerdes; Iakes Ginter; Koldo Goenaga; Memduh Gojenola; Yoav G\u00f6k\u0131rmak; Xavier G\u00f3mez Goldberg; Berta Gonz\u00e1lez Guinovart; Bernadeta Saavedra; Matias Grici\u016bt\u0117; Normunds Grioni; Bruno Gr\u016bz\u012btis; C\u00e9line Guillaume; Nizar Guillot-Barbance; Jan Habash; Jan Haji\u010d; Mika Haji\u010d Jr; Linh H\u00e0 H\u00e4m\u00e4l\u00e4inen; Na-Rae M\u1ef9; Kim Han; Dag Harris; Johannes Haug; Felix Heinecke; Barbora Hennig; Jaroslava Hladk\u00e1; Florinel Hlav\u00e1\u010dov\u00e1; Petter Hociung; Jena Hohle; Takumi Hwang; Radu Ikeda; Elena Ion; O Irimia; Tom\u00e1\u0161 Ishola; Anders Jel\u00ednek; Fredrik Johannsen; Markus J\u00f8rgensen; H\u00fcner Juutinen; Andre Ka\u015f\u0131kara; Nadezhda Kaasen;  Kabaeva; Hiroshi Sylvain Kahane; Jenna Kanayama; Boris Kanerva; Tolga Katz; Jessica Kayadelen; V\u00e1clava Kenney; Jesse Kettnerov\u00e1; Elena Kirchner; Arne Klementieva; Kamil K\u00f6hn; Natalia Kopacewicz; Jolanta Kotsyba; Simon Kovalevskait\u0117; Sookyoung Krek; Veronika Kwak; Lorenzo Laippala; Lucia Lambertino; Tatiana Lam;  Lando; Alexei Septina Dian Larasati; John Lavrentiev; Phuong L\u00ea Lee; Alessandro H\u1ed3ng; Saran Lenci; Herman Lertpradit;  Leung; Ying Cheuk; Josie Li; Keying Li; Kyungtae Li; Maria Lim; Yuan Liovina; Nikola Li; Olga Ljube\u0161i\u0107; Olga Loginova; Teresa Lyashevskaya; Vivien Lynn; Aibek Macketanz; Michael Makazhanov; Christopher Mandl; Ruli Manning; C\u0203t\u0203lina Manurung; David M\u0203r\u0203nduc; Katrin Mare\u010dek;  Marheinecke; Andr\u00e9 H\u00e9ctor Mart\u00ednez Alonso; Jan Martins; Yuji Ma\u0161ek; Ryan Matsumoto; Sarah Mcdonald; Gustavo Mcguinness; Niko Mendon\u00e7a; Margarita Miekka; Anna Misirpashayeva; C\u0203t\u0203lin Missil\u00e4; Maria Mititelu; Yusuke Mitrofan; Simonetta Miyao; Amir Montemagni; Laura Moreno More; Keiko Sophie Romero; Tomohiko Mori; Shinsuke Morioka; Shigeki Mori; Bjartur Moro; Bohdan Mortensen; Kadri Moskalevskyi; Robert Muischnek; Yugo Munro; Kaili Murawaki; Pinkey M\u00fc\u00fcrisep; Juan Ignacio Navarro Nainwani; Anna Hor\u00f1iacek; Gunta Nedoluzhko;  Ne\u0161pore-B\u0113rzkalne;  Luong Nguy\u1ec5n Thi;  Huy\u1ec1n Nguy\u1ec5n Thi; Yoshihiro Minh; Vitaly Nikaido; Rattima Nikolaev; Hanna Nitisaroj; Stina Nurmi; Atul Ojala;  Kr;  Ojha; Mai Ad\u00e9day`o . Ol\u00fa\u00f2kun; Petya Omura; Jana Osenova ; Lan Straka; Alane Strnadov\u00e1; Umut Suhr; Shingo Sulubacak; Zsolt Suzuki; Dima Sz\u00e1nt\u00f3; Yuta Taji; Fabio Takahashi; Takaaki Tamburini; Isabelle Tanaka; Guillaume Tellier; Liisi Thomas; Trond Torga;  Trosterud"}, {"title": "LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (\u00daFAL), Faculty of Mathematics and Physics", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "(\u2191) 19.2 % (\u2191) 16.9 % (\u2191) -16.1 % (\u2193) -17.0 % (\u2193) -15.5 % (\u2193) +5.84 (\u2191) +8.49 (\u2191) +6.22 (\u2191)", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure 1: Relationship between the change in UNK token percentage on task data and the change in task performance, from (MBERT/LAPT to VA), with a 1-degree line of best fit. All vocabulary values are computed on the respective training sets.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Language overview and unlabeled dataset statistics: number of sentences, number of tokens, and average wordpieces per token under the original MBERT vocabulary.", "figure_data": "BERT, we follow Muller et al. (2021) and train asix-layer RoBERTa model"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "68.84 \u00b1 7.16 88.86 \u00b1 0.37 86.87 \u00b1 2.55 89.68 \u00b1 2.15 89.45 \u00b1 1.37 90.81 \u00b1 0.31 81.84 \u00b1 1.15 87.48 \u00b1 0.55 85.48 BERT 91.00 \u00b1 0.30 94.48 \u00b1 0.10 90.36 \u00b1 0.20 92.61 \u00b1 0.10 90.87 \u00b1 0.13 89.88 \u00b1 0.13 84.73 \u00b1 0.13 87.71 \u00b1 0.31 90.20 MBERT 94.57 \u00b1 0.45 96.98 \u00b1 0.08 91.91 \u00b1 0.25 94.01 \u00b1 0.17 78.07 \u00b1 0.22 91.77 \u00b1 0.18 88.97 \u00b1 0.10 93.04 \u00b1 0.20 91.16 LAPT 95.74 \u00b1 0.44 97.15 \u00b1 0.04 93.28 \u00b1 0.19 95.76 \u00b1 0.09 79.88 \u00b1 0.27 92.18 \u00b1 0.16 89.64 \u00b1 0.20 94.58 \u00b1 0.13 92.28 VA 95.28 \u00b1 0.51 97.20 \u00b1 0.06 93.33 \u00b1 0.16 96.33 \u00b1 0.09 91.49 \u00b1 0.13 92.24 \u00b1 0.16 89.49 \u00b1 0.22 94.48 \u00b1 0.20 93.73 (a) POS tagging (accuracy). *Belarusian uses universal POS tags. 35.81 \u00b1 2.24 84.03 \u00b1 0.41 65.58 \u00b1 1.21 68.45 \u00b1 1.40 54.52 \u00b1 1.02 79.33 \u00b1 0.25 54.91 \u00b1 0.79 70.39 \u00b1 1.39 64.13 BERT 45.77 \u00b1 1.35 84.61 \u00b1 0.27 64.02 \u00b1 0.49 65.92 \u00b1 0.45 60.34 \u00b1 0.27 78.07 \u00b1 0.22 54.70 \u00b1 0.27 60.12 \u00b1 0.39 64.19 MBERT 71.83 \u00b1 0.90 91.62 \u00b1 0.23 71.68 \u00b1 0.62 76.63 \u00b1 0.35 47.70 \u00b1 0.44 81.45 \u00b1 0.26 64.58 \u00b1 0.42 76.24 \u00b1 0.83 72.72 LAPT 72.77 \u00b1 1.12 92.08 \u00b1 0.31 74.79 \u00b1 0.12 81.53 \u00b1 0.37 50.67 \u00b1 0.34 81.78 \u00b1 0.44 66.15 \u00b1 0.41 80.34 \u00b1 0.14 75.01 VA 73.22 \u00b1 1.23 91.90 \u00b1 0.20 74.35 \u00b1 0.22 82.00 \u00b1 0.31 67.55 \u00b1 0.17 81.88 \u00b1 0.25 65.64 \u00b1 0.12 80.22 \u00b1 0.41 77.09", "figure_data": "Rep.BE* (0)BG (0)GA (1)MT (2)UG (2)UR (0)VI (0)WO (2) Avg.Rep.BE (0)BG (0)GA (1)MT (2)UG (2)UR (0)VI (0)WO (2) Avg.(b) UD parsing (LAS).Rep.BE (0)BG (0)GA (1)MT (2)UG (2)UR (0)VI (0)MHR (2) Avg.FASTT84.26 \u00b1 0.86 87.98 \u00b1 0.76 67.21 \u00b1 4.30 33.53 \u00b1 17.89-92.85 \u00b1 2.04 85.57 \u00b1 1.98 35.28 \u00b1 13.81 60.84BERT88.08 \u00b1 0.62 90.31 \u00b1 0.20 76.58 \u00b1 0.9854.64 \u00b1 3.51 61.54 \u00b1 3.70 94.04 \u00b1 0.55 88.08 \u00b1 0.1554.17 \u00b1 2.88 75.93MBERT 91.13 \u00b1 0.07 92.56 \u00b1 0.09 82.82 \u00b1 0.5761.86 \u00b1 2.60 50.76 \u00b1 1.86 94.60 \u00b1 0.34 92.13 \u00b1 0.2761.85 \u00b1 3.25 78.46LAPT91.61 \u00b1 0.74 92.96 \u00b1 0.13 84.13 \u00b1 0.7881.53 \u00b1 2.33 56.76 \u00b1 4.91 95.17 \u00b1 0.29 92.41 \u00b1 0.1559.17 \u00b1 5.15 81.72"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Average UNK token percentage under the MBERT vocabulary (left); change in UNK token percentage from MBERT to VA vocabularies (center); and average task performance change from LAPT to VA (right). Averages are computed overall and within each script and language type, with comparisons to the overall average; all UNK token percentages are computed on the respective training sets for illustration. Note that Uyghur accounts for a large portion of the behavior of the Type 2/Arabic rows.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "\u2192 48.45(-5.72)  61.54 \u2192 63.05 (+1.51) 90.87 \u2192 90.76 (-0.09) 60.34 \u2192 60.08 (-0.26) MBERT 61.85 \u2192 63.84 (+1.99) 50.76 \u2192 56.80 (+6.04) 78.07 \u2192 91.34 (+13.27) 47.70 \u2192 65.85 (+18.15) LAPT 59.17 \u2192 63.68 (+4.51) 56.76 \u2192 67.57 (+10.81) 79.88 \u2192 92.59 (+12.71) 50.67 \u2192 69.39 (+18.72) VA 64.23 \u2192 63.19 (-1.04) 68.93 \u2192 67.10 (-1.83) 91.49 \u2192 92.64 (+1.15) 67.55 \u2192 68.58", "figure_data": "Rep.MHR (NER)UG (NER)UG (POS)UG (UD)FASTT35.28 \u2192 41.32 (+6.04)-89.45 \u2192 89.03(-0.42) 54.52 \u2192 54.45(-0.07)BERT54.17,"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Comparison of model performance before and after transliteration. Bolded results are the maximum for each language-task pair. -indicates that a model did not converge.", "figure_data": ""}], "doi": "10.18653/v1/2020.acl-main.421"}