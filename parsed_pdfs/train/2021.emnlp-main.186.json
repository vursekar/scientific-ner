{"authors": "Shaopeng Lai; Ante Wang; Fandong Meng; Jie Zhou; Yubin Ge; Jiali Zeng; Junfeng Yao; Degen Huang; Jinsong Su", "pub_date": "", "title": "Improving Graph-based Sentence Ordering with Iteratively Predicted Pairwise Orderings", "abstract": "Dominant sentence ordering models can be classified into pairwise ordering models and set-to-sequence models. However, there is little attempt to combine these two types of models, which inituitively possess complementary advantages. In this paper, we propose a novel sentence ordering framework which introduces two classifiers to make better use of pairwise orderings for graph-based sentence ordering (Yin et al., 2019(Yin et al., , 2021. Specially, given an initial sentence-entity graph, we first introduce a graph-based classifier to predict pairwise orderings between linked sentences. Then, in an iterative manner, based on the graph updated by previously predicted highconfident pairwise orderings, another classifier is used to predict the remaining uncertain pairwise orderings. At last, we adapt a GRN-based sentence ordering model (Yin et al., 2019(Yin et al., , 2021 on the basis of final graph. Experiments on five commonly-used datasets demonstrate the effectiveness and generality of our model. Particularly, when equipped with BERT (Devlin et al., 2019) and FHDecoder (Yin et al.,  2020), our model achieves state-of-the-art performance. Our code is available at https:// github.com/DeepLearnXMU/IRSEG.", "sections": [{"heading": "Introduction", "text": "With the rapid development and increasing applications of natural language processing (NLP), modeling text coherence has become a significant task, since it can provide beneficial information for understanding, evaluating and generating multi-sentence texts. As an important subtask, sentence ordering aims at recovering unordered sentences back to naturally coherent paragraphs. It is required to deal with logic and syntactic consistency, and has increasingly attracted attention due to its wide applications on several tasks such as text generation (Konstas and Lapata, 2012;Holtzman et al., 2018) Recently, inspired by the great success of deep learning in other NLP tasks, researchers have resorted to neural sentence ordering models, which can be classified into: pairwise ordering models Agrawal et al., 2016;Li and Jurafsky, 2017;Moon et al., 2019;Kumar et al., 2020;Prabhumoye et al., 2020;Zhu et al., 2021) and set-to-sequence models (Gong et al., 2016;Nguyen and Joty, 2017;Logeswaran et al., 2018;Mohiuddin et al., 2018;Cui et al., 2018;Yin et al., 2019;Oh et al., 2019;Yin et al., 2020;Cui et al., 2020;Yin et al., 2021). Generally, the former predicts the relative orderings between pairwise sentences, which are then leveraged to produce the final ordered sentence sequence. Its advantage lies in the lightweight pairwise ordering predictions, since the predictions only depend on the semantic representations of involved sentences. By contrast, the latter is mainly based on an encoder-decoder framework, where an encoder is first used to learn contexualized sentence representations by considering other sentences, and then a decoder, such as pointer network (Vinyals et al., 2015a), outputs ordered sentences.\nOverall, these two kinds of models have their own strengths, which are complementary to each other. To combine their advantages, Yin et al. (2020) propose FHDecoder that is equipped with three pairwise ordering prediction modules to enhance the pointer network decoder. Along this line, Cui et al. (2020) introduce BERT to exploit the deep semantic connection and relative orderings between sentences and achieve SOTA performance when equipped with FHDecoder. However, there still exist two drawbacks: 1) their pairwise ordering predictions only depend on involved sentence pairs, without considering other sentences in the same set; 2) their one-pass pairwise ordering predictions are relatively rough, ignoring distinct difficulties in predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited.\nIn this paper, we propose a novel iterative pairwise ordering prediction framework which introduces two classifiers to make better use of pairwise orderings for graph-based sentence ordering (Yin et al., 2019(Yin et al., , 2021. As an extension of Sentence-Enity Graph Recurrent Network (SE-GRN) (Yin et al., 2019(Yin et al., , 2021, our framework enriches the graph representation with iteratively predicted orderings between pairwise sentences, which further benefits the subsequent generation of ordered sentences. The basic intuitions behind our work are two-fold. First, learning contextual sentence representations is helpful to predict pairwise orderings. Second, difficulties of predicting ordering vary with respect to different sentence pairs. Thus, it is more reasonable to first predict the orderings of pairwise sentences easily to be predicted, and then leverage these predicted orderings to refine the predictions for other pairwise sentences.\nConcretely, we propose two graph-based classifiers to iteratively conduct ordering predictions for pairwise sentences. The first classifier takes the sentence-entity graph (SE-Graph) (Yin et al., 2019(Yin et al., , 2021 as input and yields relative orderings of linked sentences via corresponding probabilities. Next, in an iterative manner, the second classifier enriches the previous graph representation by converting high-value probabilities into the weights of the corresponding edges, and then reconduct graph encoding to predict orderings for the other pairwise sentences. Based on the final weighted graph representation, we adapt SE-GRN to construct a graph-based sentence ordering model, of which the decoder is also a pointer network.\nTo the best of our knowledge, our work is the first to exploit pairwise orderings to enhance the graph encoding for graph-based set-to-squence sentence ordering. To investigate the effectiveness of our framework, we conduct extensive experiments on several commonly-used datasets. Experimental results and in-depth analyses show that our model enhanced with some proposed technologies (Devlin et al., 2019;Yin et al., 2020) achieves the state-of-the-art performance.", "n_publication_ref": 29, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Early studies mainly focused on exploring humandesigned features for sentence ordering (Lapata, 2003;Barzilay and Lee, 2004;Lapata, 2005, 2008;Elsner and Charniak, 2011;Guinaudeau and Strube, 2013). Recently, neural network based sentence ordering models have become dominant , consisting of the following two kinds of models:\n1) Pairwise models. Generally, they first predict the pairwise orderings between sentences and then use them to produce the final sentence order via ranking algorithms Agrawal et al., 2016;Li and Jurafsky, 2017;Kumar et al., 2020;Prabhumoye et al., 2020;Zhu et al., 2021). For example,  first framed sentence ordering as a ranking task conditioned on pairwise scores. Agrawal et al. (2016) conducted the same experiments as  in the task of image caption storytelling. Similarly, Li and Jurafsky (2017)  2) Set-to-sequence Models. Basically, these models are based on an encoder-decoder framework, where the encoder is used to obtain sentence representations and then the decoder produces ordered sentences progressively. Among them, both Gong et al. (2016) and Logeswaran et al. (2018) Cui et al. (2018) proposed ATTOrderNet that uses self-attention mechanism to learn sentence representations. Inspired by the successful applications of graph neural network (GNN) in many NLP tasks Xue et al., 2019;, Yin et al. (2019Yin et al. ( , 2021 represented input sentences with a unified SE-Graph and then applied GRN to learn sentence representations. Very recently, we notice that Chowdhury et al. (2021) proposes a BART-based sentence ordering model. Please note that our porposed framework is compatible with BART (Lewis et al., 2020). For  (Yin et al., 2019(Yin et al., , 2021. example, we can easily adapt the BART encoder as our sentence encoder.\nWith similar motivation with ours, that is, to combine advantages of above-mentioned two kinds of models, Yin et al. (2020) introduced three pairwise ordering predicting modules (FHDecoder) to enhance the pointer network decoder of ATTOrder-Net. Recently, Cui et al. (2020) proposed BERSON that is also equipped with FHDecoder and utilizes BERT to exploit the deep semantic connection and relative ordering between sentences.\nHowever, significantly different from them, we borrow the idea from the mask-predict framework (Gu et al., 2018;Ghazvininejad et al., 2019;Deng et al., 2020) to progressively incorporate pairwise ordering information into SE-Graph, which is the basis of our graph-based sentence ordering model. To the best of our knowledge, our work is the first attempt to explore iteratively refined GNN for sentence ordering.", "n_publication_ref": 27, "n_figure_ref": 0}, {"heading": "Background", "text": "In this section, we give a brief introduction to the SE-GRN (Yin et al., 2019(Yin et al., , 2021, which is selected as our baseline due to its competitive performance. As shown in Figure 1, SE-GRN is composed of a Bi-LSTM sentence encoder, GRN  paragraph encoder, and a pointer network (Vinyals et al., 2015b) decoder. o i . As illustrated in the middle of Figure 1, each input sentence set is rep-resented as an undirected sentence-entity graph G = (V , E), where V ={v i } I i=1 \u222a{v j } J j=1 and E ={e i,i } I,I i=1,i =1 \u222a{\u0113 i,j } I,J i=1,j=1 \u222a{\u00ea j,j } J,J j=1,j =1 represent the nodes and edges respectively. Here, nodes include sentence nodes (such as v i ) and entity nodes (such asv j ), and each edge is 1) sentencesentence edge (ss-edge, such as e i,i ) linking two sentences having the same entity; or 2) sentenceentity edge (se-edge, such as\u0113 i,j ) connecting an entity to a sentence that contains it. Each se-edge is assigned with a label including subject, object or other, based on the syntactic role of its involved entity; or 3) entity-entity edge (ee-edge, such as\u00ea j,j ) connecting two semantic related entities. Besides, a virtual global node connecting to all nodes is introduced to capture global information effectively.", "n_publication_ref": 3, "n_figure_ref": 2}, {"heading": "Sentence-Entity Graph", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Paragraph Encoding with GRN", "text": "Node representations of each sentence and each entity are first initialized with the concatenation of bidirectional last states of the Bi-LSTM sentence encoder and the corresponding GloVe word embedding, respectively. Then, a GRN is adapted to encode the above sentence-entity graph, where node states are updated iteratively. During the process of updating hidden states, the messages for each node are aggregated from its adjacent nodes. Specifically, the sentence-level message m (l) i and entity-level messagem (l) i for a sentence s i are defined as follows:\nm (l) i = v i \u2208N i w(\u03ba (l-1) i , \u03ba (l-1) i )\u03ba (l-1) i , m(l\n) i = v j \u2208N iw (\u03ba (l-1) i , (l-1) j , rij) (l-1) j ,(1)\nwhere \u03ba (l-1) i and (l-1) j stand for the neighboring sentence and entity representations of the i-th sentence node v i at the (l \u2212 1)-th layer, N i andN i denote the sets of neighboring sentences and entities of v i , and both w( * ) andw( * ) are gating functions with single-layer networks, involving associated node states and edge label r ij (if any).\nAfterwards, \u03ba (l-1) i is updated by concatenating its original representation \u03ba (0) i , the messages from neighbours (m (l) i andm (l) i ) and the global state g (l-1) via GRU:\n\u03be (l) i = [\u03ba (0) i ; m (l) i ;m (l) i ; g (l-1) ], \u03ba (l) i = GRU(\u03be (l) i , \u03ba (l-1) i\n).\n(\n)2\nSimilar to updating sentence nodes, each entity state (l-1) j is updated based on its word embedding emb j , hidden states of its connected sentence nodes (such as \u03ba (l-1) i ), and g (l-1) :\nm (l) j = v i \u2208N jw ( (l-1) j , \u03ba (l-1) i , rij)\u03ba (l-1) i , m (l) j = v j \u2208N jw ( (l-1) j , (l-1) j ) (l-1) j , \u03be (l) j = [embj; m (l) j ;m (l) j ; g (l-1) ],(l)\nj = GRU(\u03be (l) j , (l-1) j ).(3)\nFinally, the messages from both sentence and entity states are used to update global state g (l-1) via\ng (l) = GRU( 1 |V | v i \u2208V \u03ba (l-1) i , 1 |V | v j \u2208V (l-1) j , g (l-1) ). (4)\nThe above updating process is iterated for L times. Usually, the top hidden states are considered as fine-grained graph representations, which will provide dynamical context for the decoder via attention mechanism.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Decoding with Pointer Network", "text": "Given the learned hidden states {\u03ba (L) i } and g (L) , the prediction procedure for order o can be formalized as follows:\nP (o |K (L) ) = I t=1 P (o t |o <t , K (L) o t\u22121 ), P (o t |o <t , K (L) o t\u22121 ) = softmax(q T tanh(W h d t + U K (L) o t\u22121 )), h d t = LSTM(h d t\u22121 , \u03ba (0) o t\u22121\n).\nHere, q, W and U are learnable parameters, K and the decoder hidden state at the t-th time step, which is initialized by g (L) as t=0, respectively.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Our Framework", "text": "In this section, we give a detailed description to our framework. As shown in Figure 2, we first introduce two graph-based classifiers to construct an iteratively refined sentence-entity graph (IRSE-Graph). It is a weighted version of SE-Graph, where pairwise ordering inforamtion is iteratively incorporated to update ss-edge weights. Then, we adapt the conventional GRN to establish a neural sentence ordering model based on the final IRSE-Graph.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "The Definition of IRSE-Graph", "text": "As an extension of SE-Graph, IRSE-Graph can be denoted as G=(V ,E,W ), where V and E share the same definitions with those of SE-Graph. Particularly, in IRSE-Graph, each ss-edge e i,i is a directed one with a weight w i,i \u2208W indicating the probability of sentence s i occurring before sentence s i . Meanwhile, there must exist a corresponding ssedge e i ,i with the weight w i ,i =1\u2212w i,i denoting the probability of s i appearing after s i . For example, in Figure 2, for two linked sentence nodes v 1 and v 2 , there exist two ss-edges e 1,2 and e 2,1 with weights w 1,2 and w 2,1 respectively, both of which are iteratively updated during constructing IRSE-Graph.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Constructing IRSE-Graph", "text": "Inspired by Gui et al. (2020), we successively introduce two classifiers -initial classifier and iterative classifier to construct IRSE-Graph. Both classifiers are constructed using slightly adapted GRN and utilized to deal with different scenarios, respectively. In this way, we can fully exploit the potential of iterative classifier to predict better pairwise orderings. We will give a detail introduction to the slightly adapted GRN in Section \u00a74.3.\nTo better understand the procedure of constructing IRSE-Graph, we provide the details in Algorithm 1. During this procedure, pairwise orderings are iteratively predicted and gradually incorporated to refine IRSE-Graph. Here we introduce a set VP (k) to collect sentence node pairs with uncertain pairwise orderings at the k-th iteration.\nFirst, we bulid an initial classifier based on the initial IRSE-Graph, where the learned sentence representations are used to predict pairwise orderings between any two linked sentences only once (Lines 2-6). Note that in the initial IRSE-Graph, all weights of ss-edges are set to 0.5. In this case, IRSE-Graph degrades to the conventional SE-Graph. Concretely, for any two linked sentence nodes v i and v i , we concatenate their vector representations \u03ba i and \u03ba i as [\u03ba i ; \u03ba i ] and [\u03ba i ; \u03ba i ], which are fed into an MLP classifier to obtain two probabilities. Then, we normalize and convert these two probabilities into ss-edge weights w i,i and w i ,i . If both w i,i and w i ,i are within a prefixed interval [\u03b4 min , \u03b4 max ], we consider (v i , v i ) as a sentence node pair with uncertain pairwise ordering and add it into VP (0) . Moreover, we replace both w i,i and w i ,i with 0.5, indicating that they will be repredicted in the next iteration.\nIn the following, we also construct an iterative classifier based on IRSE-Graph. However, in an easy-to-hard manner, we use iterative classifier to perform pairwise ordering predictions, where the ss-edge weights of IRSE-Graph are continously updated with previously-predicted pairwise orderings with high confidence (Lines 13-26). By doing so, graph representations can be continously refined for better subsequent predictions. More specifically, the k-th iteration of this classifier mainly involve three steps:\nIn\nStep 1, based on the current IRSE-Graph, we employ the adapted GRN to conduct graph encoding to learn sentence representations (Line 15).\nIn\nStep 2, on the top of learned sentence representations, we stack an MLP classifier to predict pairwise orderings for sentence node pairs in VP (k) (Lines 16-19). Likewise, we collect sentence node pairs with uncertain pairwise orderings to form VP (k+1) , and reassign their corresponding ss-edge weights as 0.5, so as to avoid the negative effect of these uncertain ss-edge weights during the next Algorithm 1 The procedure of constructing IRSE-Graph Input: the initial IRSE-Graph: G=(V , E, W ) with all w i,i =0; two thresholds: \u03b4min, \u03b4max Output: the final IRSE-Graph: G = (V , E, W ) 1: VP (0) \u2190 \u2205 2: {\u03bai} I i=1 \u2190 GRN(G) 3: for any linked sentence node pair (vi, v i ) && i<i do 4:\nw i,i \u2190 InitialClassifer([\u03bai; \u03ba i ]) 5: w i ,i \u2190 InitialClassifer([\u03ba i ; \u03bai]) 6: w i,i , w i ,i \u2190 Normalize(w i,i , w i ,i ) 7:\nif \u03b4min \u2264 w i,i \u2264 \u03b4max then 8:\nVP (0) \u2190 VP (0) \u222a{(vi, v i )} 9:\nw i,i \u2190 0.5, w i ,i \u2190 0.5 10:\nend if 11: end for 12: k \u2190 0 13: repeat 14: iteration (Lines 20-23).\nVP (k+1) \u2190 \u2205 15: {\u03bai} I i=1 \u2190 GRN(G) 16: for (vi, v i ) \u2208 VP (k) do 17: w i,i \u2190 IterativeClassifer([\u03bai; \u03ba i ]) 18: w i ,i \u2190 IterativeClassifer([\u03ba i ; \u03bai]) 19: w i,i , w i ,i \u2190 Normalize(w i,i , w i ,i ) 20: if \u03b4min \u2264 w i,i \u2264 \u03b4max then 21: VP (k+1) \u2190VP (k+1) \u222a{(vi, v i )} 22: w i,i \u2190 0.5, w i ,i \u2190 0.5 23: end if 24: end for 25: k \u2190 k + 1 26: until VP (k+1) ==VP (k) || VP (k) == \u2205 27: return G\nIn Step 3, if VP (k+1) is equal to VP (k) or empty, we believe the learning of IRSE-Graph G has converged and thus return it (Lines 26-27).\nAlthough both of our classifiers are constructed using IRSE-Graph, their training procedures are slightly different. As for initial classifier, we directly train it on the initial IRSE-Graph without any pairwise ordering information (all ss-edge weights are set to 0.5). By contrast, we train iterative classifier on IRSE-Graph with partial pairwise orderings. To enable iterative classifier generalizable to any IRSE-Graph with partial predicted pairwise orderings, we first set all ss-edge weights to 1 or 0 according to their ground-truth pairwise orderings, and then train the classifier to correctly predict pari-wise orderings for other pairs. Concretely, if s i appears before s i , we set w i,i =1 and w i ,i =0, vice versa. For example, in the left part of Figure 3, the ground-truth sentence sequence is s 1 ,s 2 ,s 3 ,s 4 , and thus we assign the ss-edge weights of linked sentence node pairs (v 1 , v 2 ), (v 3 , v 2 ), (v 3 , v 4 ), (v 2 , v 4 ) as follows: w 1,2 =1, w 2,3 =1, w 2,4 =1, w 3,4 =1, and w 2,1 =0, w 3,2 =0, w 4,2 =0, w 4,3 =0.\nMoreover, to enhance the robustness of the iterative classifier, we randomly select a certain ratio \u03b7 of sentence pairs and assign their ss-edges with incorrect weights. Let us revisit Figure 3, for the randomly selected sentence node pair (v 1 , v 2 ), we assign ss-edges weights w 1,2 and w 2,1 with randomly generated noisy values 0.3 and 0.7 respectively. In this way, we expect that iterative classifier can conduct correct predictions even given incorrect previously-predicted pairwise orderings.", "n_publication_ref": 4, "n_figure_ref": 2}, {"heading": "IRSE-Graph Sentence Ordering Model", "text": "Finally, following the conventional SE-GRN (Yin et al., 2019(Yin et al., , 2021, we construct a graph-based sentence ordering model. Note that the above two classifiers and our sentence ordering model are all based on IRSE-Graph rather than the conventional SE-Graph, which makes the standard GRN unable to be applied directly. To deal with this issue, we slightly adapt GRN to utilize pairwise ordering information for graph encoding. Specifically, we adapt Equation 1 to incorporate ss-edge weights into the message aggregation of sentence-level nodes:\nm (l) i = v i \u2208N i w i,i \u2022 w(\u03ba (l-1) i , \u03ba (l-1) i )\u03ba (l-1) i , w(\u03ba (l-1) i , \u03ba (l-1) i ) = \u03c3(Wg[\u03ba (l-1) i ; \u03ba (l-1) i ]).(6)\nHere \u03c3 denotes sigmoid function and W g is learnable parameter matrix. Equation 6 expresses that the sentence-level aggregation should consider not only the semantic representations of the two involved sentences, but also the relative ordering between them. In addition, other Equations are the same as those of conventional GRN, which have been described in Section \u00a73.2.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Experiment", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Setup", "text": "Datasets. Following previous work (Yin et al., 2020;Cui et al., 2018;Yin et al., 2021), we carry out experiments on five benchmark datasets:\n\u2022 SIND, ROCStory. SIND  is a visual storytelling dataset and ROCStory (Mostafazadeh et al., 2016) is about commonsense stories. Both two datasets are composed of 5-sentence stories and randomly split by 8:1:1 for the training/validation/test sets. \u2022 NIPS Abstract, AAN Abstract, arXiv Abstract. These three datasets consist of abstracts from research papers, which are collected from NIPS, ACL anthology and arXiv, respectively (Radev et al., 2016; (Yin et al., 2021) for our model and its variants. Specifically, we apply 100-dimensional GloVe word embeddings, and set the sizes of Bi-LSTM hidden states, sentence node states, and entity node states as 512, 512 and 150, respectively. The recurrent step of GRN is 3. We empirically set thresholds \u03b4 min and \u03b4 max as 0.2 and 0.8, and set \u03b7 as 20%, 15%, 25%, 15%, 15% according to accuracies of initial classifier on validation sets. Besides, we individually set the coefficient \u03bb (See Equation 18in (Yin et al., 2020)) as 0.5, 0.5, 0.2, 0.4, 0.5 on the five datasets. We adopt Adadelta (Zeiler, 2012) with = 10 \u22126 , \u03c1 = 0.95 and initial learning rate 1.0 as the optimizer. We employ L2 weight decay with coefficient 10 \u22125 , batch size of 16 and dropout rate of 0.5.\nWhen constructing our model based on BERT, we use the same settings as (Cui et al., 2020). Concretely, we set sizes of hidden states and node states to 768, the learning rate of BERT as 3e-3, the batch size as 16, 32, 128, 128, 64 for the five datasets.\nBaselines. To demonstrate the effectiveness of our model (IRSE-GRN), we compare it with SE-GRN (Yin et al., 2021). Besides, we report the performance of following sentence ordering models: 1) Pairwise models: Pairwise Model , RankTxNet (Kumar et al., 2020), andB-TSort (Prabhumoye et al., 2020), ConsGraph (Zhu et al., 2021); 2) Set-to-sequence models: HAN (Wang and Wan, 2019), LSTM+PtrNet (Gong et al., 2016), V-LSTM+PtrNet (Logeswaran et al., 2018), ATTOrderNet (Cui et al., 2018), TGCM (Oh et al., 2019), SE-GRN (Yin et al., 2019), SE-GRN (Yin et al., 2021)  2020) and BERSON (Cui et al., 2020). Furthermore, to examine the compatibility of other technologies with our model, we report the performance of IRSE-GRN equipped with some effective components: 1) IRSE-GRN+FHDecoder. In this variant, we equip our model with FHDecoder (Yin et al., 2020), where pairwise ordering information is incorporated; 2) IRSE-GRN+BERT+FHDecoder. In addition to FHDecoder, we construct the sentence encoder based on BERT, where the mean-pooling outputs of all learned word representations are used to initialize sentence nodes. Evaluation Metrics. Following previous work (Oh et al., 2019;Cui et al., 2020;Prabhumoye et al., 2020;Zhu et al., 2021;Yin et al., 2021), we use the following three metrics: 1) Kendall's Tau (\u03c4 ): Formally, this metric is calculated as 1-2\u00d7(number of inversions)/ I 2 , where I denotes the sequence length and number of inversions is the number of pairs in the predicted sequence with incorrect relative order (Lapata, 2003); 2) Perfect Match Ratio (PMR): This metric calculates the ratio of samples where the entire sequence is correctly predicted ; 3) Accuracy (Acc): This metric measures the percentage of sentences, whose absolute positions are correctly predicted (Logeswaran et al., 2018).", "n_publication_ref": 28, "n_figure_ref": 0}, {"heading": "Pairwise Ordering", "text": "Since pairwise ordering plays a crucial role in our proposed framework, we first compare the performance of different classifiers on various datasets.  the predictions of pairwise orderings.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Main Results", "text": "Table 1 reports the overall experimental results of sentence ordering. When incorporating BERT and FHDecoder into IRSE-GRN, our model achieves SOTA performance on most of datasets. Besides, we arrive at the following conclusions: First, IRSE-GRN significantly surpasses SE-GRN on all datasets (bootstrapping test, p<0.01), indicating that iteratively refining graph representations indeed benefit the ordering of input sentences.\nSecond, IRSE-GRN+FHDecoder exhibits better performance than IRSE-GRN and all non-BERT baselines, which are shown above the upper dotted line of Table 1, across datasets in different domains. Therefore, we confirm that our framework is orthogonal to the current approach exploiting pairwise ordering information for decoder.\nThird, when constructing our model based on BERT, IRSE-GRN+BERT+FHDecoder also outperforms all BERT-based baselines, such as Cons-Graph, BERSON, achieving SOTA performance. It can be known that our proposed framework is also effective when combining with pretrained language model. Finally, we note that IRSE-GRN+BERT+FH-Decoder gains relatively marginal improvement on SIND and ROCStory, and performs worse than BERSON in PMR on SIND. We speculate that there exist less ss-edges on these two datasets, resulting in that our proposed framework can not achieve its full potential. Specifically, average edge numbers of SIND and ROCStory are 2.85 and 5.66 respectively, far fewer than 16.60, 10.86 and 16.73 on NIPS Abstract, ANN Abstract and arXiv Abstract.\nBesides, since it is a challenge to order longer paragraphs, we investigate the Kendall's \u03c4 of our models and SE-GRN with respect to different sentence numbers, as shown in Figure 4. Overall, all models degrade with the increase of sentence number. However, our model and its two enhanced versions always exhibit better performance than SE-GRN.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Predictions of the First and Last Sentences", "text": "As mentioned in previous studies (Gong et al., 2016;Cui et al., 2018;Oh et al., 2019), the first and last sentences are very important in a paragraph. Following these studies, we compare models by conducting experiments to predict the first and last sentences. As displayed in Table 3, IRSE-GRN surpasses all non-BERT baselines, and IRSE-GRN+BERT+ FHDecoder wins against BERTSON. These results are consistent with those reported in Table 1, further demonstrating the effectiveness of our model.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Ablation Study", "text": "We conduct several experiments to investigate the impacts of our proposed components on ROCstory dataset and arXiv dataset which are the two largest   datasets. All results are provided in Table 4, where we draw the following conclusions: First, using only iterative classifier, IRSE-GRN(w/o initial classifier) performs worse than IRSE-GRN. This result proves that iterative classifier fails to predict well from scratch and the pairwise ordering predicted by initial classifier is beneficial to construct a well-formed graph representation for iterative classifier.\nSecond, when the iteration number k is set as 1, the performance of IRSE-GRN decreases. Moreover, if we remove iterative classifier, the performance of IRSE-GRN becomes even worse. Therefore, we confirm that the iterative predictions of pairwise ordering indeed benefit the learning of graph representations.\nFinally, the result in the last line indicates that removing noisy weights leads to a significant performance drop. It suggests that the utilization of noisy weights is useful for the training of iterative classifier, which makes our model more robust.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Summary Coherence Evaluation", "text": "Following previous studies (Barzilay and Lapata, 2005;Nayeem and Chali, 2017)   spect the validity of our proposed framework via multi-document summarization. Concretely, we train different neural sentence ordering models on a large-scale summarization corpus (Fabbri et al., 2019), and then individually use them to reorder the small-scale summarization data of DUC2004 (Task2). Finally, we use coherence probability proposed by (Nayeem and Chali, 2017) to evaluate the coherence of summaries. In this group of experiments, we conduct experiments using different weights: 0.5 and 0.8, as implemented in (Nayeem and Chali, 2017) and (Yin et al., 2020) respectively.\nThe results are reported in Table 5. We can observe that the summaries reordered by IRSE-GRN and its variants achieve higher coherence probabilities than baseline, verifying the effectiveness of our proposed framework in the downstream task.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Further Experiment Results", "text": "To provide more experimental results, we summarize the runtime on the validation sets and the numbers of parameters for our enhanced models and baseline SE-GRN in Table 6.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this work, we propose a novel sentence ordering framework that makes better use of pairwise orderings for graph-based sentence ordering. Specifically, we introduce two classifiers to iteratively predict pairwise orderings, which are gradually incorporated into the graph as edge weights. Then, based on this refined graph, we construct a graph-based sentence ordering model. Experiments on five datasets demonstrate not only the superiority of our model over baselines, but also the compatibility to other modules utilizing pairwise ordering information. Moreover, when equipped with BERT and FHDecoder, our enhanced model achieves SOTA performance across datasets.\nIn the future, we plan to explore more effective GNN for sentence ordering. In particular, we will improve our model by iteratively merging nodes to refine the graph representation.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgment", "text": "The project was supported by National Key Research and Development Program of China (No. 2020AAA0108004), National Natural Science Foundation of China (No. 61672440), Natural Science Foundation of Fujian Province of China (No. 2020J06001), and Youth Innovation Fund of Xiamen (No. 3502Z20206059). We also thank the reviewers for their insightful comments.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Sort story: Sorting jumbled images and captions into stories", "journal": "", "year": "2016", "authors": "Harsh Agrawal; Arjun Chandrasekaran; Dhruv Batra; Devi Parikh; Mohit Bansal"}, {"title": "Inferring strategies for sentence ordering in multidocument news summarization", "journal": "Journal of Artificial Intelligence Research", "year": "2002", "authors": "Regina Barzilay; Noemie Elhadad; Kathleen R M-Ckeown"}, {"title": "Modeling local coherence: An entity-based approach", "journal": "", "year": "2005", "authors": "Regina Barzilay; Mirella Lapata"}, {"title": "Modeling local coherence: An entity-based approach", "journal": "Computational Linguistics", "year": "2008", "authors": "Regina Barzilay; Mirella Lapata"}, {"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "journal": "", "year": "2004", "authors": "Regina Barzilay; Lillian Lee"}, {"title": "Neural sentence ordering", "journal": "", "year": "2016", "authors": "Xinchi Chen; Xipeng Qiu; Xuanjing Huang"}, {"title": "Reformulating sentence ordering as conditional text generation", "journal": "", "year": "2021", "authors": "Somnath Basu; Roy Chowdhury; Faeze Brahman; Snigdha Chaturvedi"}, {"title": "Deep attentive sentence ordering network", "journal": "", "year": "2018", "authors": "Baiyun Cui; Yingming Li; Ming Chen; Zhongfei Zhang"}, {"title": "Bert-enhanced relational sentence ordering network", "journal": "", "year": "2020", "authors": "Baiyun Cui; Yingming Li; Zhongfei Zhang"}, {"title": "Length-controllable image captioning", "journal": "", "year": "2020", "authors": "Chaorui Deng; Ning Ding; Mingkui Tan; Qi Wu"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "J Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Extending the entity grid with entity-specific features", "journal": "", "year": "2011", "authors": "Micha Elsner; Eugene Charniak"}, {"title": "Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model", "journal": "", "year": "2019", "authors": "Alexander R Fabbri; Irene Li; Tianwei She; Suyi Li; Dragomir R Radev"}, {"title": "Mask-predict: Parallel decoding of conditional masked language models", "journal": "", "year": "2019", "authors": "Marjan Ghazvininejad; Omer Levy; Yinhan Liu; Luke Zettlemoyer"}, {"title": "End-to-end neural sentence ordering using pointer network", "journal": "", "year": "2016", "authors": "Jingjing Gong; Xinchi Chen; Xipeng Qiu; Xuanjing Huang"}, {"title": "Nonautoregressive neural machine translation", "journal": "", "year": "2018", "authors": "Jiatao Gu; James Bradbury; Caiming Xiong; O K Victor; Richard Li;  Socher"}, {"title": "Yeyun Gong, and Xuanjing Huang. 2020. Uncertainty-aware label refinement for sequence labeling", "journal": "", "year": "", "authors": "Tao Gui; Jiacheng Ye; Qi Zhang; Zhengyan Li; Zichu Fei"}, {"title": "Graphbased local coherence modeling", "journal": "", "year": "2013", "authors": "Camille Guinaudeau; Michael Strube"}, {"title": "Learning to write with cooperative discriminators", "journal": "", "year": "2018", "authors": "Ari Holtzman; Jan Buys; Maxwell Forbes; Antoine Bosselut; David Golub; Yejin Choi"}, {"title": "Michel Galley, and Margaret Mitchell. 2016. Visual storytelling. In NAACL", "journal": "", "year": "", "authors": "Ting-Hao Huang; Francis Ferraro; Nasrin Mostafazadeh; Ishan Misra; Aishwarya Agrawal; Jacob Devlin; Ross Girshick; Xiaodong He; Pushmeet Kohli; Dhruv Batra; C Lawrence Zitnick; Devi Parikh; Lucy Vanderwende"}, {"title": "Unsupervised concept-to-text generation with hypergraphs", "journal": "", "year": "2012", "authors": "Ioannis Konstas; Mirella Lapata"}, {"title": "Deep attentive ranking networks for learning to order sentences", "journal": "", "year": "2020", "authors": "Pawan Kumar; Dhanajit Brahma; Harish Karnick; Piyush Rai"}, {"title": "Probabilistic text structuring: Experiments with sentence ordering", "journal": "", "year": "2003", "authors": "Mirella Lapata"}, {"title": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"title": "Neural net models of open-domain discourse coherence", "journal": "", "year": "2017", "authors": "Jiwei Li; Dan Jurafsky"}, {"title": "Sentence ordering and coherence modeling using recurrent neural networks", "journal": "", "year": "2018", "authors": "Lajanugen Logeswaran; Honglak Lee; Dragomir R Radev"}, {"title": "Coherence modeling of asynchronous conversations: A neural entity grid approach", "journal": "", "year": "2018", "authors": " Muhammad Tasnim Mohiuddin; R Shafiq; Dat Tien Joty;  Nguyen"}, {"title": "A unified neural coherence model", "journal": "", "year": "2019", "authors": "Tasnim Han Cheol Moon;  Mohiuddin; R Shafiq; Xu Joty;  Chi"}, {"title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "journal": "", "year": "2016", "authors": "Nasrin Mostafazadeh; Nathanael Chambers; Xiaodong He; Devi Parikh; Dhruv Batra; Lucy Vanderwende; Pushmeet Kohli; James Allen"}, {"title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents", "journal": "", "year": "2012", "authors": "Ramesh Nallapati; Feifei Zhai; Bowen Zhou"}, {"title": "Extract with order for coherent multi-document summarization", "journal": "", "year": "2017", "authors": "Yllias Mir Tafseer Nayeem;  Chali"}, {"title": "A neural local coherence model", "journal": "", "year": "2017", "authors": "Shafiq Rayhan Joty Dat Tien Nguyen"}, {"title": "Topic-guided coherence modeling for sentence ordering by preserving global and local information", "journal": "", "year": "2019", "authors": "Byungkook Oh; Seungmin Seo; Cheolheon Shin; Eunju Jo; Kyong-Ho Lee"}, {"title": "Topological sort for sentence ordering", "journal": "", "year": "2020", "authors": "Ruslan Shrimai Prabhumoye; Alan W Salakhutdinov;  Black"}, {"title": "A bibliometric and network analysis of the field of computational linguistics", "journal": "Journal of the Association for Information Science and Technology", "year": "2016", "authors": "R Dragomir; Mark Thomas Radev; Bryan R Joseph; Pradeep Gibson;  Muthukrishnan"}, {"title": "Semantic neural machine translation using AMR", "journal": "Transactions of the Association for Computational Linguistics", "year": "2019", "authors": "Linfeng Song; Daniel Gildea; Yue Zhang; Zhiguo Wang; Jinsong Su"}, {"title": "Structural information preserving for graph-to-text generation", "journal": "", "year": "2020", "authors": "Linfeng Song; Ante Wang; Jinsong Su; Yue Zhang; Kun Xu; Yubin Ge; Dong Yu"}, {"title": "Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks", "journal": "", "year": "2018", "authors": "Linfeng Song; Zhiguo Wang; Mo Yu; Yue Zhang; Radu Florian; Daniel Gildea"}, {"title": "Pointer networks", "journal": "", "year": "2015", "authors": "Oriol Vinyals; Meire Fortunato; Navdeep Jaitly"}, {"title": "Pointer networks", "journal": "", "year": "2015", "authors": "Oriol Vinyals; Meire Fortunato; Navdeep Jaitly"}, {"title": "Hierarchical attention networks for sentence ordering", "journal": "", "year": "2019", "authors": "Tianming Wang; Xiaojun Wan"}, {"title": "Neural collective entity linking based on recurrent random walk network learning. In IJCAI", "journal": "", "year": "2019", "authors": "Mengge Xue; Weiming Cai; Jinsong Su; Linfeng Song; Yubin Ge; Yubao Liu; Bin Wang"}, {"title": "An external knowledge enhanced graphbased neural network for sentence ordering", "journal": "Journal of Artificial Intelligence Research", "year": "2021", "authors": "Yongjing Yin; Shaopeng Lai; Linfeng Song; Chulun Zhou; Xianpei Han; Junfeng Yao; Jinsong Su"}, {"title": "Enhancing pointer network for sentence ordering with pairwise ordering predictions", "journal": "", "year": "2020", "authors": "Yongjing Yin; Fandong Meng; Jinsong Su; Yubin Ge; Linfeng Song; Jie Zhou; Jiebo Luo"}, {"title": "Graph-based neural sentence ordering", "journal": "", "year": "2019", "authors": "Yongjing Yin; Linfeng Song; Jinsong Su; Jiali Zeng; Chulun Zhou; Jiebo Luo"}, {"title": "ADADELTA: an adaptive learning rate method", "journal": "", "year": "2012", "authors": "Matthew D Zeiler"}, {"title": "Sentencestate lstm for text representation", "journal": "", "year": "2018", "authors": "Yue Zhang; Qi Liu; Linfeng Song"}, {"title": "Neural sentence ordering based on constraint graphs", "journal": "", "year": "2021", "authors": "Yutao Zhu; Kun Zhou; Jian-Yun Nie; Shengchao Liu; Zhicheng Dou"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "investigated the effectiveness of discriminative and generative models on ordering pairs of sentences in small domains. Moon et al. (2019) proposed a unified model that incorporates sentence grammar, pairwise coherence and global coherence into a common neural framework. Recently, Prabhumoye et al. (2020) and Zhu et al. (2021) employed ranking techniques to find the right order of the sentences under the constraint of the predicted pairwise sentence ordering;", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure1: The architecture of SE-GRN model(Yin et al., 2019(Yin et al., , 2021.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "The SE-GRN takes I sentences s = [s o 1 , . . . , s o I ] as input and tries to predict their correct ordero * = [o * 1 , . . . , o * I ].At first, each sentence s o i is fed into a Bi-LSTM sentence encoder, where the concatenation of the last hidden states in two directions is used as the context-aware sentence representation \u03ba(0)   ", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: The architecture of our model during inference. IRSE-Graph is a weighted graph representation, of which weights of ss-edges are iteratively refined by iterative classifier. Note that we construct the sentence ordering model based on the final IRSE-Graph.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": "L) o t\u22121 and h d t denote the sentence representations with predicted order \u03ba (L) o 1 , . . . , \u03ba (L) o t\u22121", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 3 :3Figure 3: Introducing noisy ss-edge weights into IRSE-Graph.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 4 :4Figure 4: The Kendall's \u03c4 of different models with respect to different sentence numbers on the arXiv abstract test set.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Main results on the sentence ordering task, where \u2020 indicates previously reported scores. Please note that RankTxNet, B-TSort and ConsGraph are pairwise models based on BERT, and the previous SOTA BERSON is also based on BERT and equipped with FHDecoder.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "shows the experimental results. Obviously,the utilization of iterative classifier further benefits"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "The accuracies of our two classifiers on five test datasets.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Gong et al., 2016) \u2020 74.66 53.30 90.47 66.49 ATTOrderNet (Cui et al., 2018) \u2020 76.00 54.42 91.00 68.08 SE-GRN (Yin et al., 2019) \u2020 78.12 56.68 92.28 70.45 SE-GRN (Yin et al., 2021) 79.01 57.27 92.23 70.46 ATTOrderNet+FHDecoder (Yin et al., 2020) \u2020 78.08 57.32 92.76 71.49 TGCM (Oh et al., 2019) \u2020", "figure_data": "ModelSINDarXiv Abstracthead tail headtailPairwise Model (Chen et al., 2016)  \u2020--84.85 62.37LSTM+PtrNet (78.98 56.24 92.46 69.45RankTxNet (Kumar et al., 2020)  \u202080.32 59.68 92.97 69.13B-Tsort (Prabhumoye et al., 2020)  \u202078.06 58.36--ConsGraph (Zhu et al., 2021)  \u202079.80 60.44--BERSON (Cui et al., 2020)  \u202084.95 64.87 94.75 76.69IRSE-GRN78.62 59.11 94.46 80.97IRSE-GRN+FHDecoder82.87 64.15 96.09 85.04IRSE-GRN+BERT+FHDecoder86.21 67.14 98.23 88.33"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The ratios of correctly predicting first and last sentences on arXiv Abstract and SIND. \u2020 indicates previously reported scores.", "figure_data": "ModelROCStoryarXiv AbstractPairwise \u03c4PMR Pairwise \u03c4PMRIRSE-GRN92.23 77.43 46.38 86.82 84.22 56.85w/o initial classifier88.96 77.31 45.06 77.90 81.03 51.65iterative number k=191.73 77.21 46.13 86.22 83.89 56.03w/o iterative classifier 87.59 75.98 44.14 84.09 83.24 55.05w/o noise90.42 77.06 46.02 80.45 82.23 53.10"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Ablation study on the impacts of our proposed components on ROCStory dataset and arXiv abstract dataset.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "The runtime on the validation sets and the numbers of parameters for our enhanced models and baseline.", "figure_data": "ModelCoherenceSE-GRN (Yin et al., 2021)46.71 59.47IRSE-GRN47.48 60.01IRSE-GRN+FHDecoder49.84 61.81IRSE-GRN+BERT+FHDecoder 51.01 62.87"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Coherence probabilities of summaries reordered by different models using weights of 0.8 (left) and 0.5 (right).", "figure_data": ""}], "doi": ""}