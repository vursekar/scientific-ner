{"authors": "Yixuan Su; \u2666 Deng Cai; Yan Wang; David Vandyke; Simon Baker; Piji Li; \u2660 Nigel Collier", "pub_date": "", "title": "Non-Autoregressive Text Generation with Pre-trained Language Models", "abstract": "Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a NAG model to greatly improve performance. Additionally, we devise mechanisms to alleviate the two common problems of vanilla NAG models: the inflexibility of prefixed output length and the conditional independence of individual token predictions. Lastly, to further increase the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including text summarization, sentence compression and machine translation. Experimental results show that our model significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component. 1   ", "sections": [{"heading": "Introduction", "text": "Autoregressive generation (AG) models achieve state-of-the-art performance on a wide range of text generation tasks, such as machine translation (Vaswani et al., 2017) and text summarization (Rush et al., 2015). Such models generate a token sequence in a left-to-right, token-by-token fashion. The prediction for the next token is conditioned on all previously generated tokens. This characteristic makes it impossible to parallelize the computational overhead for token predictions in different positions, which leads to a relatively high latency in inference. On the other hand, non-autoregressive generation (NAG) models (Gu et al., 2018) have emerged as a promising alternative due to their fast inference speed. NAG models omit the sequential dependencies within the output-side sequence and predict tokens in all positions simultaneously once the output length has been determined beforehand. While NAG models enjoy full parallelism and faster inference, the generation quality of NAG models often lags behind their autoregressive counterparts.\nIn this work, we explore the potential of largescale pre-trained language models for improving the performance of non-autoregressive generation. Specifically, we utilize BERT (Devlin et al., 2019) as the backbone for NAG modelling and extend the architecture of BERT with a CRF output layer (Lafferty et al., 2001; for better capturing the output-side dependencies.\nIn addition, we analyze two significant limitations that NAG models currently suffer from: (1) the inflexibility of prefixed output length, and (2) the conditional independence of individual token predictions. Accordingly, we devise two solutions to these two problems.\nFirst, prior NAG models require the output length to be determined before token generation, thus an extra module for output length prediction is always required. Nevertheless, the most likely length from the prediction module is not necessarily the best-suited one for the token generation model. To this end, previous works (Gu et al., 2018;Ma et al., 2019) usually rely on length-parallel decoding (LPD)  for performance enhancement; that is, generating and re-ranking the results from different output length candidates. In this work, we propose a simple and elegant decoding mechanism that lets the model determine the output length on-the-fly. Specifically, our model dynamically adjusts the output sequence length via emitting an [eos] token at any output position to indicate the ending of the generated sequence. Therefore, we can avoid the additional efforts of output length prediction and results re-ranking.\nSecond, most existing NAG models assume the token predictions in different positions are conditionally independent. As a consequence, they often tend to generate results that are ungrammatical with repetitions (Wang et al., 2019b). To alleviate this problem, we propose a context-aware learning objective which impels the model to output different tokens at adjacent positions, thereby reducing the possibility of repetitive generation.\nFurthermore, for tasks like text summarization, the output sequence (summary) is known to be shorter than the source sequence (article). In such cases, to further improve the model's inference efficiency, we introduce a new ratio-first decoding strategy. Specifically, instead of performing inference on all source-side hidden states, ratio-first generates the result only based on a subset of source hidden states. The subset size is jointly determined by the source length T and a predefined ratio \u03b1 that is set based on our prior knowledge from the data statistics. In the experiments, we show that ratio-first can significantly improve the inference speed while maintaining the generation quality.\nWe evaluate the proposed model on three typical text generation tasks, including text summarization, sentence compression and machine translation. Experimental results show that our model significantly outperforms many strong non-autoregressive baselines, and even performs competitively with several strong autoregressive models. In addition, we conduct extensive analysis experiments to study the effect of individual proposed components.\nIn summary, our contributions are: (1) We propose a novel framework that utilizes BERT for text generation under the non-autoregressive generation paradigm; (2) We propose a decoding mechanism that allows the model to dynamically determine the output length, and a new context-aware learning objective that reduces errors stemming from the output-side conditional independence assumption;\n(3) We introduce a ratio-first decoding strategy that further improve the model's inference efficiency.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Background", "text": "Autoregressive generation (AG) models generate sequences based on a left-to-right factorization. As shown in Figure 1, given the source sequence X, the target sequence Y with length T is generated via a chain of conditional probabilities based on the left-to-right sequential dependencies as:\np(Y|X) = T i=1 p(y i |y <i , X),(1)\nwhere y <i denotes the tokens before the i-th step. This property of autoregressive factorization makes the generation process hard to be parallelized as the result is generated token by token. Unlike AG models, non-autoregressive (NAG) models generate sequences without modelling the output-side dependencies. As shown in Figure 1, given the prespecified output length T , the probability of the target sequence Y is then modelled as:\np(Y|X) = T i=1 p(y i |X, i, T ).\n(2)\nWith this conditional independence assumption, NAG models can fully parallelize their generation process, which significantly improves the inference speed. However, it has been shown that, the choice of the prespecified output length has a notable impact on the model's generation quality (Gu et al., 2018). In addition, the removal of output-side sequential dependency also causes the generation quality of NAG models to be inferior to their autoregressive counterparts (Wang et al., 2019b).", "n_publication_ref": 2, "n_figure_ref": 2}, {"heading": "Proposed Model", "text": "In this section, we give a detailed explanation of the proposed model. First, we describe how to utilize BERT as a non-autoregressive generation model. Then we discuss the decoding mechanism which allows the model to determine the output length dynamically. Finally, we introduce the new ratiofirst decoding strategy which further improves the model's decoding efficiency. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model Architecture", "text": "The architecture of the proposed model is presented in Figure 2, in which the embedding layer and the stack of transformer layers are initialized with BERT (Devlin et al., 2019).\nInput Representation Following the setup of BERT, we first append a [cls] and a [sep] token on both sides of the source sequence. Then we attach a number of [pad] tokens at the end of source sequence to make its length equal to the predefined maximum size (e.g., 256). Thus we can make sure the source length is longer than or equal to the output length. As a special case, for tasks like text summarization where the source is known to be longer than the target, we do not attach the [pad] tokens when constructing the input.\nTransformer Layers Given the source sequence X, it is processed by a stack of N transformer (Vaswani et al., 2017) layers. Formally, the Multi-Head Attention is defined as MultiHead(Q, K, V), where Q, K, V denotes the query, key and value respectively. The computation of the first transformer layer is then defined as:\nV (1) = MultiHead(E(X), E(X), E(X)), (3\n)\nO (1) = FFN(V (1) ),(4)\nFFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 , (5\n)\nwhere E(X) = T E(X) + P E(X) in which T E(\u2022) denotes the token embedding and P E(\u2022) denotes the position embedding. For other layers:\nV (n) = MultiHead(O (n\u22121) , O (n\u22121) , O (n\u22121) ),(6)\nO (n) = FFN(V (n) ),(7)\nwhere n = 2, ..., N and N is the total number of transformer layers. The final sequence representation H \u2208 R T \u00d7d model is the output states of BERT from the last layer, where T is the source sequence length and d model is the model size.\nCRF Layer Then, H is passed through a linearchain CRF (Lafferty et al., 2001). Under the CRF framework, the likelihood of the target sequence Y with length T is then modelled as:\nP CRF (Y|X) = e S(X,Y) Y e S(X,Y ) = 1 Z(X) exp( T i=1 \u03a6 y i (h i ) + T i=2 t(y i\u22121 , y i )),(8)\nwhere Z(X) is the normalizing factor and \u03a6 y i (h i ) denotes the label score of y i at position i. In practice, \u03a6 is parameterized by a neural network that maps the BERT output state h i into the label (vocabulary) space. The t(y i\u22121 , y i ) = T y i\u22121 ,y i denotes the transition score from label y i\u22121 to y i where T \u2208 R |V |\u00d7|V | is the transition matrix.\nApproximation In the context of text generation, the size of the label space (vocabulary size) |V | is typically large, e.g., 32k. Therefore, it is intractable to directly model the transition matrix T and the normalizing factor Z(X). To this end, we adopt the techniques proposed by  to approximate these two terms. Specifically, the full transition matrix is approximated by the product of two low-rank matrices T = E 1 E T 2 , where E 1 , E 2 \u2208 R |V |\u00d7d and d is much smaller than |V |. To compute the normalizing factor Z(X), at each time step, instead of searching through all possible paths, the number of candidates is heuristically truncated to a predefined beam size k. We refer readers to the original paper for further details.", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Output Length Determination", "text": "In this section, we describe how to let the model determine the output sequence length by itself. Our basic idea is that we want the model to dynamically stop generation via emitting a special [eos] token. To achieve this, during training, we manually append two consecutive [eos] tokens to the end of the target sequence, as shown in the top left part of Figure 2. In this way, the model can learn a deterministic transition behaviour between two [eos] states, meaning that t([eos], [eos]) = max v\u2208V t([eos], v). This is because, during training, the model never sees a transition ([eos], v), where v = [eos].\nDuring inference, the result\u1ef8 is acquired as Y = arg max Y S(X, Y ), where the CRF scoring function S(X, Y ) in Equation ( 8) can be decomposed as:\nS(X, Y ) = T i=1 \u03a6 y i (h i ) + T i=2 t(y i\u22121 , y i ) = \u03a6 y 1 (h 1 ) initial state + T i=2 { label score \u03a6 y i (h i ) + transition score t(y i\u22121 , y i ) state transition }. (9)\nOnce the decoded trajectory enters the [eos] state, the state transition term in S(X, Y ) will be dominated by the transition score term t([eos], [eos]). As a result, the model will keep transitioning to [eos] in the remaining steps. An example is provided in the right part of Figure 2, from which we can see that, at step 5, the decoded trajectory enters the [eos] state and remains at it in the rest of the generation process. In this way, our model can dynamically control the length of output sequence by entering the [eos] state during the generation process. After the entire generation process is completed, the final output sequence can be obtained by removing all generated [eos] tokens.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Ratio-First Decoding", "text": "We note that the outputs of BERT can be divided into two subsets. The first subset ranges from the beginning to the position where the first [eos] is emitted, and the second subset is the rest. For example, in Figure 2, the first subset are those corresponding to the output sequence \"y (1) y (2) y (3) y (4) [eos]\". As for the second part, we can see that it has little effect on the final output and removing it should not change the result. This indicates that it suffices to only consider the beginning part of BERT outputs for improving the inference speed. Especially, for tasks like summarization where the target is known to be shorter than the source sequence, we are safe to only use the first [\u03b1 \u2022 T ] outputs of BERT to perform inference. Here T denotes the source length, \u03b1 \u2208 (0.0, 1.0) is set based on the data statistics and [\u2022] is the integer rounding operation. Formally, given the source sequence X, the ratio-first decoding is defined as\nY = arg max Y F(X, Y , \u03b1), = arg max Y { [\u03b1\u2022T ] i=1 \u03a6 y i (h i ) + [\u03b1\u2022T ] i=2 t(y i\u22121 , y i )}.(10)\nWhen \u03b1 = 1.0, ratio-first degenerates to the standard decoding strategy in CRF-based models.\nIt should be noted that, [\u03b1 \u2022 T ] only constrains the maximum length of the generated result, and the actual output length (after removing the generated [eos] tokens) is still decided by the model itself. In the experiment section, we demonstrate that ratio-first can notably improve the inference speed whilst maintaining the generation quality.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Learning", "text": "Due to the conditional independence approximation on output tokens, NAG models often tend to generate repeated tokens (Wang et al., 2019b). One way to alleviate this problem is to introduce implicit dependencies on the output side. In this work, we propose to use the unlikelihood formulation of Welleck et al. (2020) in the context of NAG, where we define the set of negative candidate as the surrounding tokens within a predefined context window c. Formally, given the source sequence X and the target sequence Y with length T , the proposed context-aware objective is defined as:\nL CA (Y|X) = \u2212 T i=1 {log p \u03b8 (y i |h i ; X) + l CA (i)}, l CA (i) = j=i+c j=i\u2212c,y j =y i log(1.0 \u2212 p \u03b8 (y j |h i ; X)),(11)\nwhere h i is the model output state at position i. At position i, the proposed objective maximizes the probability of token y i while minimizing the probabilities of the surrounding tokens. In this way, it discourages the model from generating repetitive tokens at different time steps.\nThe overall learning objective is then defined as\nL CRF = \u2212 log P CRF (Y|X), L = L CRF + \u03bb \u2022 L CA ,(12)\nwhere \u03bb controls the importance of different loss terms and P CRF (Y|X) is described in Equation ( 8).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Non-Autoregressive generation was first introduced by Gu et al. (2018) to reduce the inference latency in machine translation. Recent works in this area have investigated ways to mitigate the tradeoff between the decoding speed and generation quality. Gu et al. (2018) utilized fertility as latent variables for better translation performance. Wang et al. (2019b) proposed two auxiliary objectives for better modelling the output states and solving the under-translation problem. To better model the intermediate alignments between source and target sides, Ma et al. (2019) proposed a model based on the generative flow framework. Ghazvininejad et al. (2019) proposed to use a masked language objective to train the NAG model. During inference, starting from a fully masked sequence, the output is generated in an iterative refinement manner. Recently,  proposed to incorporate a conditional random field into the decoder of a NAG model for better modelling the outputside dependencies. Our work is different from prior works in two aspects: (1) we directly utilize a pretrained language model (BERT) to perform nonautoregressive generation;\n(2) our model can dynamically generate the output sequence without the need of prespecified output length.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We evaluate the proposed model on three typical text generation tasks: (1) text summarization; (2) sentence compression and (3) machine translation.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "We implement the proposed model with PyTorch (Paszke et al., 2017). The BERT model we use is the Huggingface implementation (Wolf et al., 2019) (bert-base-uncased). To approximate the transition matrix in the CRF layer, we set the dimension d of matrices E 1 and E 2 as 32. For the normalizing factor Z(X), we set the predefined beam size k as 256. As for the overall learning objective, we set the window size c as 3 and \u03bb as 1.0. In training, we use Adam optimizer (Kingma and Ba, 2015). To measure the relative speedup, we follow the standard setup which runs inference for each individual example separately. The model's inference speed is computed by averaging the results of test cases. For a fair comparison, we measure the inference speed of all models on the same platform.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Text Summarization", "text": "Text summarization aims to automatically generate a compact summary that retains the most important content of the original text document (Nenkova and McKeown, 2012). In this experiment, we use the Gigawords dataset (Rush et al., 2015) as our benchmark. For evaluation, standard metrics including ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L (R-L) (Lin, 2004) are reported. We compare our model with several representative and the latest NAG models, including NAG-NMT (Gu et al., 2018), NAR-REG (Wang et al., 2019b) and NAG-CRF . Following previous works, during training, we train a length predictor to predict the output length. During inference, for each NAG baseline, we adopt the length-parallel decoding strategy (LPD-k) , that is, generating k results using the top-k possible output length predictions from the length predictor. The results are then re-ranked by a transformer model to get the final ouput. In the experiment, we report the results of different NAG baselines using LPD-9 decoding. In addition, to better examine the effect of using BERT in NAG models, we add a BNAG-CRF baseline which adopts the same structure of the NAG-CRF model but using BERT as the encoder. We also compare our model with several strong autoregressive models, which are Luong-NMT (Luong et al., 2015), Pointer-Generator (See et al., 2017), DRGD (Li et al., 2017) and Concept Pointer (Wang et al., 2019a). To measure the relative inference speedup, we include transformer as a baseline model.\nThe results are shown in Table 1, from which we can see that, by using length-parallel decoding, the performance of all NAG baselines can be notably improved. However, such procedure significantly increases the inference latency. In contrast,  our model can self-determine the output length without any re-ranking process. As shown in the results, our model outperforms the best NAG baseline (with LPD) and achieves performances that are comparable with several strong AG models.\nComparing the results of BNAG-CRF and NAG-CRF, we can see that incorporating BERT as encoder helps to improve the model performance. Nonetheless, our model still outperforms BNAG-CRF with LPD-9 decoding. This is because the dynamic length decoding mechanism allows our model to generate results with optimal length, leading to stronger model performances.\nFinally, we analyze the proposed ratio-first decoding. From the results, we observe a moderate performance drop when using ratio-first (\u03b1 = 0.3). It comes from the fact that, for some input documents with length T , the reference summary is longer than [\u03b1 \u2022 T ]. In such cases, ratio-first fails to generate the complete reference summary, leading to the drop of performance. On the other hand, we can see that, ratio-first can notably improve the inference speedup. With \u03b1 = 0.3, our model achieves the highest inference speedup while still outperforms all compared NAG models.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Sentence Compression", "text": "Sentence compression aims at compressing a long sentence into a short one by deleting redundant words. In this experiment, we use the Google sentence compression dataset (Filippova and Altun, 2013) as our benchmark. For evaluation, we use  the standard token-kept-F1 (F1) score. In addition, We also report the results of other standard metrics including ROUGE-1, ROUGE-2 and ROUGE-L.\nModels F1 R-1 R-2 R-L\nWe compare the proposed model with the same NAG baselines as in the previous experiment. We also compare our model with several strong autoregressive models, including Bi-LSTM-Dep (Filippova et al., 2015), Tagger and Tagger+ILP , HiSAN-Dep and HiSAN (Kamigaito et al., 2018). To measure the inference speedup, we include transformer as a baseline model.\nThe results are presented in Table 2, from which we see that our model outperforms the best reported NAG baseline (with LPD) in terms of both the generation quality and inference speed. Comparing with the strong autoregressive models, our model can achieve competitive performance with a over 8.42\u00d7 inference speed up. We also report the results of our model using the ratio-first decoding strategy. By setting \u03b1 as 0.7, it achieves a 10.00\u00d7 inference speedup while still outperforming other compared NAG baselines.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Machine Translation", "text": "Machine translation aims at translating text from the source language to the target language. In this task, we use the IWSLT14 German-to-English (DE-EN) dataset as our benchmark. Following previous works, we use the sequence-level knowledge distillation (Gu et al., 2018) during training. For evaluation, we report results in BLEU scores (Papineni et al., 2002). In this experiment, we use the BERT model in German language.\nWe compare our model with a range of strong   NAG models, including NAG-NMT (Gu et al., 2018), ENAG-E and ENAG-P (Guo et al., 2019), NAG-REG (Wang et al., 2019b), NAG-CRF  and BNAG-CRF. For each NAG baseline, we also report the results using LPD-9 decoding. In addition, we compare our model with several strong autoregressive models, including LSTM-based (Wu et al., 2016), CNN-based (Gehring et al., 2017) and transformer model. The results are shown in Table 3, from which we see that our model outperforms the best NAG baseline (with LPD) in terms of both the generation quality and inference speedup. Additionally, we also report the results using the ratio-first decoding. By setting \u03b1 as 0.8, the inference speedup can be further boosted to 13.92\u00d7 while the generation quality is still higher than the best NAG baseline.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Further Analysis", "text": "In this section, we present further discussions and empirical analysis of the proposed model.  nents, the overall performance decreases. By removing BERT from the model, we observe notable drop across all metrics. This shows that the knowledge of BERT is an important factor of the model's strong performance. Comparing with results in Table 1, it still outperforms vanilla NAG-CRF and performs comparably with NAG-CRF using LPD decoding, which demonstrates the merit of the proposed dynamic length decoding mechanism. Another interesting finding is that, by only removing the CRF layer, the most notable drop is observed on the bigram-level metric . This shows that the bigram-level dependencies on the output side are mainly captured by the CRF module. In addition, by removing both BERT and CRF, all metrics further decrease. This confirms that each of these two components positively contributes to the model's overall performance.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "BERT & CRF", "text": "Context-Aware Objective In this part, we study the effect of the context-aware objective. As described in Equation (11), it aims at alleviating the problem of repetitive generation. To give a quantitative analysis, we use the measurement of sentencelevel repetition (Welleck et al., 2020) to compute the ratio of duplicate n-grams (rep-n) in the generated result. This metric is defined as\nrep-n(Y) = 100 \u00d7 (1.0 \u2212 |unique n-grams(Y)| |n-grams(Y)| ).\n(13) For each generated result, rep-n is 0.0 when it has no repeating n-grams. The final result is computed by averaging over the entire evaluation set.\nWe conduct experiments on Gigawords dataset to evaluate the n-gram repetitions ranging from uni-gram to 4-gram. The results are shown in Table 5, where w/o CA means the model is trained without using context-aware objective and R-L denotes the model's ROUGE-L score. Additionally, we also show the results from transformer model for a direct comparison. Comparing the two variants of our model, we see that training with context-aware objective leads to a 42% drop on rep-3 metric (0.427 vs 0.741) and a 64% drop on rep-4 metric (0.106 vs 0.295). The ROUGE-L results also indicate that  the reduction in token repetition can effectively improve the model generation quality.\nDynamic Length Determination Next, we examine the importance of the model's ability to dynamically determine the length of the generated output. To this end, we train another model variant by removing the two [eos] tokens from the target sequence. In this way, the model is not able to self-determine the output length throughout the generation process. To perform inference, we use length-parallel decoding (LPD) with different number of length candidates. Formally, for each length candidate l, the model generates the result\u1ef8 as\nY = arg max Y { l i=1 \u03a6 y i (h i ) + l i=2 t(y i\u22121 , y i )}.(14)\nThe final result is acquired by re-ranking the generated results with a transformer model.\nWe conduct experiments on the IWSLT14 DE-EN dataset in which we try a different number of length candidates, including top-1, top-5 and top-10. The results are shown in Table 6, from which we can see, as the number of length candidates increases, the model performance increases as well. The reason is that a larger candidates set is more likely to contain the best-suited length for the generation model, leading to better performance. However, such decoding procedure inevitably increases the required computation overhead. We can see that, when setting k as 10, the inference speedup decreases from 11.84\u00d7 to 6.01\u00d7. In contrast, our proposed model is able to determine the optimal output length by itself. Without any re-ranking process, it outperforms the model with LPD-10 decoding and achieves the inference speedup that is comparable with the model using LPD-1 decoding.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Ratio-First Decoding", "text": "We are also interested in the effect of the ratio-first decoding strategy. To provide a quantitative analysis, we perform inference on the Gigawords dataset using ratio-first with different \u03b1. The experimental results with different \u03b1 are presented in Figure 3. It can be observed that, when \u03b1 reaches 0.3, the model approximately  achieves its optimal performance. At the same time, a notable improvement can be observed in terms of the inference speedup (6.72\u00d7 \u2192 9.31\u00d7). Now we illustrate why the near optimal performance can be achieved when \u03b1 reaches 0.3. In Figure 4, we present the distribution of the target/source length ratio of every data instance in the Gigawords dataset. We can see that, for most cases, the ratio between the target length T and source length T is less than 0.3. Recall the definition of ratio-first decoding in Equation ( 10), the [\u03b1 \u2022 T ] constrains the maximum length of the generated result. Therefore, once we have a prior knowledge on the data statistic, we can easily choose a proper \u03b1 that both improves the inference speed whilst maintaining the generation quality. In this case, a proper \u03b1 could be 0.3 which is demonstrated by the results in Figure 3 and 4. By setting different \u03b1, ratio-first provides us an explicit way to control the balance between the inference speed and the generation quality. This property of ratio-first is especially favorable in real-life scenarios where the inference speed is the highest concern.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "Conclusion", "text": "In this work, we explored the potential of BERT in various text generation tasks under the NAG framework. To address problems from NAG models previously having a prefixed output length, we devised a decoding mechanism which enables the model to determine the output length dynamically. To reduce errors stemming from the assumption of conditional independence of output tokens, we proposed a context-aware objective as well as using a CRF decoding. Furthermore, to maximize the inference speed advantage of our model, we introduced a ratio-first decoding strategy. We evaluated our model on three benchmark datasets and the results show that our model significantly outperforms many strong NAG baselines and performs comparably to many strong AG models.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "The authors wish to thank Jialu Xu, Guanlin Li, Xing Wang for their insightful discussions and support. Many thanks to our anonymous reviewers for their suggestions and comments.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019-06-02", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Sentence compression by deletion with lstms", "journal": "", "year": "2015-09-17", "authors": "Katja Filippova; Enrique Alfonseca; Carlos A Colmenares; Lukasz Kaiser; Oriol Vinyals"}, {"title": "Overcoming the lack of parallel data in sentence compression", "journal": "", "year": "2013-10", "authors": "Katja Filippova; Yasemin Altun"}, {"title": "Convolutional sequence to sequence learning", "journal": "", "year": "2017-08-11", "authors": "Jonas Gehring; Michael Auli; David Grangier; Denis Yarats; Yann N Dauphin"}, {"title": "Mask-predict: Parallel decoding of conditional masked language models", "journal": "", "year": "2019-11-03", "authors": "Marjan Ghazvininejad; Omer Levy; Yinhan Liu; Luke Zettlemoyer"}, {"title": "Nonautoregressive neural machine translation", "journal": "", "year": "2018-04-30", "authors": "Jiatao Gu; James Bradbury; Caiming Xiong; O K Victor; Richard Li;  Socher"}, {"title": "Non-autoregressive neural machine translation with enhanced decoder input", "journal": "", "year": "2019-01-27", "authors": "Junliang Guo; Xu Tan; Di He; Tao Qin; Linli Xu; Tie-Yan Liu"}, {"title": "Higher-order syntactic attention network for longer sentence compression", "journal": "", "year": "2018-06-01", "authors": "Hidetaka Kamigaito; Katsuhiko Hayashi; Tsutomu Hirao; Masaaki Nagata"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015-05-07", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "journal": "", "year": "2001-06-28", "authors": "John D Lafferty; Andrew Mccallum; Fernando C N Pereira"}, {"title": "Deep recurrent generative decoder for abstractive text summarization", "journal": "", "year": "2017-09-09", "authors": "Piji Li; Wai Lam; Lidong Bing; Zihao Wang"}, {"title": "Rouge: A package for automatic evaluation of summaries", "journal": "", "year": "2004", "authors": "Chin-Yew Lin"}, {"title": "Effective approaches to attention-based neural machine translation", "journal": "", "year": "2015-09-17", "authors": "Thang Luong; Hieu Pham; Christopher D Manning"}, {"title": "Flowseq: Nonautoregressive conditional sequence generation with generative flow", "journal": "", "year": "2019-11-03", "authors": "Xuezhe Ma; Chunting Zhou; Xian Li; Graham Neubig; Eduard H Hovy"}, {"title": "A survey of text summarization techniques", "journal": "", "year": "2012", "authors": "Ani Nenkova; Kathleen R Mckeown"}, {"title": "Bleu: a method for automatic evaluation of machine translation", "journal": "", "year": "2002-07-06", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"title": "Automatic differentiation in pytorch", "journal": "", "year": "2017", "authors": "Adam Paszke; Sam Gross; Soumith Chintala; Gregory Chanan; Edward Yang; Zachary Devito; Zeming Lin; Alban Desmaison; Luca Antiga; Adam Lerer"}, {"title": "A neural attention model for abstractive sentence summarization", "journal": "", "year": "2015-09-17", "authors": "Alexander M Rush; Sumit Chopra; Jason Weston"}, {"title": "Get to the point: Summarization with pointergenerator networks", "journal": "Long Papers", "year": "2017-07-30", "authors": "Abigail See; J Peter; Christopher D Liu;  Manning"}, {"title": "Fast structured decoding for sequence models", "journal": "", "year": "2019-12-14", "authors": "Zhiqing Sun; Zhuohan Li; Haoqing Wang; Di He; Zi Lin; Zhi-Hong Deng"}, {"title": "Attention is all you need", "journal": "", "year": "2017-12-09", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"title": "Can syntax help? improving an lstm-based sentence compression model for new domains", "journal": "", "year": "2017-07-30", "authors": "Liangguo Wang; Jing Jiang; Hai Leong Chieu; Chen Hui Ong; Dandan Song; Lejian Liao"}, {"title": "Concept pointer network for abstractive summarization", "journal": "", "year": "2019-11-03", "authors": "Wenbo Wang; Yang Gao; Heyan Huang; Yuxiang Zhou"}, {"title": "Non-autoregressive machine translation with auxiliary regularization", "journal": "", "year": "2019-01-27", "authors": "Yiren Wang; Fei Tian; Di He; Tao Qin; Chengxiang Zhai; Tie-Yan Liu"}, {"title": "Imitation learning for nonautoregressive neural machine translation", "journal": "", "year": "2019-07-28", "authors": "Bingzhen Wei; Mingxuan Wang; Hao Zhou; Junyang Lin; Xu Sun"}, {"title": "Neural text generation with unlikelihood training", "journal": "", "year": "2020-04-26", "authors": "Sean Welleck; Ilia Kulikov; Stephen Roller; Emily Dinan; Kyunghyun Cho; Jason Weston"}, {"title": "Huggingface's transformers: State-of-the-art natural language processing", "journal": "ArXiv", "year": "2019", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R'emi Louf; Morgan Funtowicz; Jamie Brew"}, {"title": "Google's neural machine translation system: Bridging the gap between human and machine translation", "journal": "Oriol Vinyals", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; Quoc V Le; Mohammad Norouzi; Wolfgang Macherey; Maxim Krikun; Yuan Cao; Qin Gao; Klaus Macherey; Jeff Klingner; Apurva Shah; Melvin Johnson; Xiaobing Liu; Lukasz Kaiser; Stephan Gouws; Yoshikiyo Kato; Taku Kudo; Hideto Kazawa; Keith Stevens; George Kurian; Nishant Patil; Wei Wang"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: (a) Autoregressive; (b) Non-Autoregressive", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: The overall illustration of the proposed model: During training, the model parameters are only updated on the positions of the target sequence. During inference, once the decoded trajectory (colored in red) gets into the [eos] state, it will only transit to the [eos] state in the remaining steps. The final result is obtained by removing the generated [eos] tokens from the entire decoded trajectory.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Experiment results on Gigawords dataset using ratio-first decoding with different \u03b1.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: The distribution of target/source length ratio of the training and test set in Gigawords dataset.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Results on Gigawords dataset, where b in the transformer baseline stands for beam search size.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Results on sentence compression task", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ": Results on IWSLT14 De-En dataset. The num-bers in () are results using length-parallel decoding.BERT CRFR-1R-2R-L35.05 16.48 33.28\u00d732.41 14.19 30.53\u00d732.16 11.33 30.34\u00d7\u00d727.028.8125.25"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Ablation study on Gigawords dataset.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Evaluation results on n-gram repetitions.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Results comparison on IWSLT14 dataset", "figure_data": ""}], "doi": ""}