{"authors": "Meng Zhang; Minghao Wu; Pengfei Li; Liangyou Li; Qun Liu", "pub_date": "", "title": "NoahNMT at WMT 2021: Dual Transfer for Very Low Resource Supervised Machine Translation", "abstract": "This paper describes the NoahNMT system submitted to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation. The system is a standard Transformer model equipped with our recent technique of dual transfer. It also employs widely used techniques that are known to be helpful for neural machine translation, including iterative backtranslation, selected finetuning, and ensemble. The final submission achieves the top BLEU for three translation directions.", "sections": [{"heading": "Introduction", "text": "In this paper, we describe the NoahNMT system submitted to one of the WMT 2021 shared tasks. The shared task features both unsupervised machine translation and very low resource supervised machine translation. As our core technique is mainly suitable for low resource supervised machine translation, we participated in four translation directions between Chuvash-Russian (chv-ru) and Upper Sorbian-German (hsb-de).\nOur core technique is called dual transfer (Zhang et al., 2021), which belongs to the family of transfer learning. It transfers from both high resource neural machine translation model and pretrained language model to improve the quality of low resource machine translation. During the preparation for the shared task, we conducted additional experiments that supplement the original paper, including the choice of parent language, the validation of Transformer big model, and the usage of dual transfer along with iterative back-translation.\nIn addition, we also applied proven techniques to strengthen the quality of our system, including selected finetuning and ensemble. Our final submission achieves the top BLEU on the blind test sets for three translation directions: chv\u2192ru, ru\u2192chv, and hsb\u2192de.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Approach", "text": "In this section, we describe the techniques used in our system. Interested readers are encouraged to check out the original papers for further details.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dual Transfer", "text": "We reproduced the illustration of dual transfer from the original paper (Zhang et al., 2021), as shown in Figure 1. This illustration shows the case of general transfer, where the high resource translation direction is A\u2192B, and the low resource translation direction is P\u2192Q. As discussed in the original paper, in many cases, it is possible to use shared target transfer (B=Q) or shared source transfer (A=P). Taking chv\u2192ru as an example, we can choose en\u2192ru as the high resource translation direction, resulting in an instance of shared target transfer. In this shared task, when training the high resource translation model, we always initialize the shared language side with the pretrained language model BERT (Devlin et al., 2019).", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Iterative Back-Translation", "text": "Iterative back-translation (Hoang et al., 2018) is an extension of back-translation (Sennrich et al., 2016a). It can exploit both sides of monolingual data of a language pair, and produces translation models for both directions, which is suitable for this shared task.\nThe initial models for generating synthetic parallel data are produced by using dual transfer with low resource authentic parallel data. In each iteration of iterative back-translation, we use the latest model to greedily decode a disjoint subset of 4m monolingual sentences 1 to generate synthetic parallel data. Then a new model is trained on a mixture of authentic and synthetic parallel data. With the use of dual transfer, model training can start from [A] PLM emb.\n[A] PLM body A and B mono.\n(1)\n[P] PLM emb.\n[A] PLM body P and Q mono.\n(2)\n[A] NMT encoder emb.\n[A] NMT encoder body\n[B] NMT decoder emb.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A\u2192B parallel", "text": "(3)\n[P] NMT encoder emb.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Selected Finetuning", "text": "Selected finetuning aims to deal with the domain difference that may exist between the test set and the training set. Given the source side of the test set, we try to select similar source sentences from the training set, and then finetune the translation model on the selected subset of training sentence pairs. We use BM25 (Robertson and Zaragoza, 2009) to calculate the similarity between two sentences for retrieval. The BM25 score between a query sentence Q and a sentence D in the corpus for parent language chv\u2192ru BLEU kk 18.47 en 18.61 retrieval C is given by\ns (D, Q) = L Q i=1 IDF (q i ) \u2022 (k + 1) \u2022 TF (q i , D) k \u2022 1 \u2212 b + b \u2022 L D Lavg + TF (q i , D) ,\nwhere the query sentence Q is a sequence of\nL Q subwords {q i } L Q i=1 , IDF (q i ) is the Inverse Docu- ment Frequency for q i in the corpus C, TF (q i , D)\nis the Term Frequency for q i in the sentence D, L D is the length of the sentence D, L avg is the average length of the corpus C, k and b are hyperparameters, which are set as 1.5 and 0.75, respectively.\nBased on the BM25 score, we calculate the similarity between a source test sentence (as the query sentence) and the source sentences in the training set to obtain the top 500 sentences. After performing the selection for all the source test sentences, we merge them and remove duplicates to obtain the set for finetuning.   3 Experimental Setup", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Data", "text": "We collected allowed data for the involved languages and followed the same preprocessing pipeline of punctuation normalization and tokenization, using scripts from Moses 2 . The English monolingual data came from the English original side of ru-en back-translated news 3 , but its automatic translation to Russian was discarded. The provided Chuvash-Russian dictionary was not used. Each language was encoded with byte pair encoding (BPE) (Sennrich et al., 2016b). The BPE codes and vocabularies were learned on each language's monolingual data, and then used to segment parallel data. We used 32k merge operations for all languages. After BPE segmentation, we discarded sentences with more than 128 subwords, and cleaned parallel data with length ratio 1.5. Training data statistics is provided in Table 1. Note that we experimented with Kazakh (kk) data (Section 4.1), but did not use it for our final submission. Evaluation on test sets is given by SacreBLEU 4 (Post, 2018), after BPE removal and detokenization.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Hyperparameters", "text": "We use Transformer (Vaswani et al., 2017) as our translation model, but with slight modifications that follow the implementation of BERT 5 . The absolute position embeddings are also learned as in BERT. The encoder and decoder embeddings are independent because each language manages its own vocabulary, but we tie the decoder input and output embeddings (Press and Wolf, 2017). We apply dropout with probability 0.1. We use LazyAdam as the optimizer. Learning rate warms up for 16,000 steps and then follows inverse square root decay.\nThe peak learning rate is 5 \u00d7 10 \u22124 for parent translation models, and 1 \u00d7 10 \u22124 for child translation models. Early stopping occurs when the validation BLEU does not improve for 10 checkpoints. We set checkpoint frequency to 2,000 updates for parent translation models and 1,000 updates for child translation models. The batch size is 6,144 tokens per GPU and 8 NVIDIA V100 GPUs are used. Hyperparameters for BERT are the same as in the original paper (Zhang et al., 2021).\nFor selected finetuning, we use stochastic gradient descent as the optimizer, and the learning rate is 1 \u00d7 10 \u22125 . We finetune for 10,000 updates, and save a checkpoint every 100 updates. The checkpoint with the highest validation BLEU is kept.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "The Choice of Parent Language", "text": "In our preliminary experiments, we found it beneficial to use a closely related language as the parent language. It is clear that there are several factors that should be taken into account, such as the degree of closeness, and the amount of resource for training the parent model. For Upper Sorbian, Czech (cs) is closely related to it, and Czech-German has a good amount of parallel data, so we directly choose Czech as the parent language.\nChuvash, however, is a rather isolated language in the Turkic family. The closest language with usable data is Kazakh (kk), but the amount of parallel data for Kazakh-Russian is relatively small, and we found it to be quite noisy. Therefore, we considered    using English (en) as the parent language of Chuvash. Even though English is unrelated to Chuvash and they use different scripts, English-Russian has more parallel data that can guarantee the quality of the parent model. We conducted an experiment with Transformer base. Results in Table 2 indicate that English can serve as an eligible parent for Chuvash. Considering that we plan to use Transformer big for which data amount is likely to play a more important role, we decided to use English as the parent language for Chuvash.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "The Effect of Transformer Big", "text": "The original paper (Zhang et al., 2021) evaluated dual transfer only with Transformer base. In this shared task, we scale up to Transformer big. We also face a more realistic setting where the monolingual data for the low resource languages (chv and hsb) are quite scarce. Therefore it is worth testing the effect of scaling up. Results in Table 3 show that Transformer big brings consistent improvements. We also report the runtime of each step in dual transfer for NMT chv\u2192ru with Transformer big in Table 4 for reference, but the numbers can vary depending on implementation and data size. In the following experiments and our final submission, we use Transformer big models.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Iterative Back-Translation", "text": "We ran five iterations of iterative back-translation. Results are shown in Table 5. The best BLEU scores are attained with two or three iterations. Another observation is that iterative back-translation brings larger improvements for chv\u2192ru and hsb\u2192de than ru\u2192chv and de\u2192hsb. This is probably because the monolingual data for chv and hsb are small in quantity.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Selected Finetuning", "text": "We only use selected finetuning for the chv-ru pair because parallel data for hsb-de is scarce. In order to test the effect of selected finetuning, we start from the models of Iteration 2 in Table 5. Results in Table 6 indicate that selected finetuning gives modest improvements.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ensemble", "text": "We validate the effectiveness of ensemble on hsb\u2192de and de\u2192hsb, by performing ensemble decoding from the five models from iterative back-translation. Results in Table 7 demonstrate that ensemble gives BLEU improvements of about 0.8.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Final Submission", "text": "For chv\u2192ru and ru\u2192chv, we perform selected finetuning starting from the best models from iterative back-translation (Iteration 2 for chv\u2192ru, Iteration 3 for ru\u2192chv). Note that the selected training subsets are different from those in Section 4.4 because the selection is based on the source side of the blind test sets. We finetune five times with different random seeds for model ensemble. For hsb\u2192de and de\u2192hsb, we ensemble the five models from iterative back-translation.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we describe a series of experiments that contribute to our submission to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation. These experiments, as well as the good results of the final submission, show that dual transfer can work in synergy with several widely used techniques in realistic scenarios.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Iterative Back-Translation for Neural Machine Translation", "journal": "", "year": "2018", "authors": "Duy Vu Cong; Philipp Hoang; Gholamreza Koehn; Trevor Haffari;  Cohn"}, {"title": "A Call for Clarity in Reporting BLEU Scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"title": "Using the Output Embedding to Improve Language Models", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Ofir Press; Lior Wolf"}, {"title": "The Probabilistic Relevance Framework: BM25 and Beyond", "journal": "Foundations and Trends in Information Retrieval", "year": "2009", "authors": "Stephen Robertson; Hugo Zaragoza"}, {"title": "Improving Neural Machine Translation Models with Monolingual Data", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Attention is All you Need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Two Parents, One Child: Dual Transfer for Low-Resource Neural Machine Translation", "journal": "", "year": "2021", "authors": "Meng Zhang; Liangyou Li; Qun Liu"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "[Figure 1 :1Figure 1: Dual transfer from pretrained language model and high resource A\u2192B neural machine translation to low resource P\u2192Q neural machine translation. Dashed lines represent initialization. Parameters in striped blocks are frozen in the corresponding step, while other parameters are trainable. Different colors represent different languages. Data used in each step is also listed.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Training data statistics.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Test set BLEU for chv\u2192ru, when the parent language is either kk or en (i.e. the parent translation direction is either kk\u2192ru or en\u2192ru). The translation model is Transformer base.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Test set BLEU for the four translation directions, using either Transformer base or Transformer big for dual transfer. *: The parent translation direction is ru\u2192kk, and we did not train a Transformer base with ru\u2192en as the parent, though the resulting ru\u2192chv BLEU scores should be close based on the experiment in Section 4.1.", "figure_data": "runtime (hours)BERT en143BERT chv54NMT en\u2192ru52NMT chv\u2192ru14"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Runtime of each step in dual transfer for NMT chv\u2192ru with Transformer big.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Test set BLEU for the four translation directions with iterative back-translation. Iteration 0 is the Transformer big model in Table3. Best BLEU scores are in bold.", "figure_data": "methodchv\u2192ru ru\u2192chvbefore selected finetuning20.4217.69after selected finetuning20.5518.03"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Test set BLEU to show the effect of selected finetuning.", "figure_data": "modelhsb\u2192de de\u2192hsbbest single57.7257.47ensemble58.5458.28"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Test set BLEU to show the effect of ensemble.", "figure_data": ""}], "doi": "10.18653/v1/N19-1423"}