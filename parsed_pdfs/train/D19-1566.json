{"authors": "Dushyant Singh Chauhan; Shad Akhtar; Asif Ekbal; Pushpak Bhattacharyya", "pub_date": "", "title": "Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis", "abstract": "In recent times, multi-modal analysis has been an emerging and highly sought-after field at the intersection of natural language processing, computer vision, and speech processing. The prime objective of such studies is to leverage the diversified information, (e.g., textual, acoustic and visual), for learning a model. The effective interaction among these modalities often leads to a better system in terms of performance. In this paper, we introduce a recurrent neural network based approach for the multi-modal sentiment and emotion analysis. The proposed model learns the inter-modal interaction among the participating modalities through an auto-encoder mechanism. We employ a context-aware attention module to exploit the correspondence among the neighboring utterances. We evaluate our proposed approach for five standard multi-modal affect analysis datasets. Experimental results suggest the efficacy of the proposed model for both sentiment and emotion analysis over various existing state-of-the-art systems.", "sections": [{"heading": "Introduction", "text": "In recent past, the world has witnessed tremendous growth of various social media platforms, e.g., YouTube, Instagram, Twitter, Facebook, etc. People treat these platforms as a communication medium and freely express themselves with the help of a diverse set of input sources, e.g. videos, images, audio, text etc. The amount of information produced daily through these mediums are enormous, and hence, the research on multi-modal information processing has attracted attention to the researchers and developers. A video is a multimodal input which provides visual, acoustic, and textual information.\nThe motivation of multi-modal sentiment and emotion analysis lies in fact to leverage the varieties of (often distinct) information from multiple sources for building more efficient systems. For some cases, text can provide a better clue for the prediction, whereas for the others, acoustic or visual sources can be more informative. Similarly, in some situations, a combination of two or more information sources together ensures better and unambiguous classification decision. For example, only text \"shut up\" can not decide the mood of a person but acoustic (tone of a person) and visual (expression of a person) can reveal the exact mood. Similarly, for some instances visual features such as gesture, postures, facial expression etc. have important roles to play in determining the correctness of the system. However, effectively combining this information is a nontrivial task that researchers often have to face (Poria et al., 2016;Ranganathan et al., 2016;Lee et al., 2018).\nTraditionally, 'text' has been the key factor in any Natural Language Processing (NLP) tasks, including sentiment and emotion analysis. However, with the recent emergence of social media platforms, an interdisciplinary study involving text, visual and acoustic features have drawn a great interest among the research community. Expressing the feelings and emotions through a video is much convenient than the text for a user, and it is the best source to extract all multi-modal information. Not only the visual, it also provides other information such as acoustic and textual representation of spoken language. Additionally, a single video can have multiple utterances based on a speaker's pause (speech bounded by breaths) with different sentiments and emotions. The sentiments and emotions of an utterance often have interdependence on the other contextual utterances. Independently classifying such an utterance poses several challenges to the underlying problem. In contrast, multi-modal sentiment and emotion analysis take inputs from more than one sources e.g. text, visual, acoustic for the analysis. Effectively fusing this diverse information is non-trivial and poses several challenges to the underlying problem.\nIn our current work, we propose an end-to-end Context-aware Interactive Attention (CIA) based recurrent neural network for sentiment and emotion analysis. We aim to leverage the interaction between the modalities to increase the confidence of individual task in prediction. The main contributions of our current research are as follows:\n(1) We propose an Inter-modal Interactive Module (IIM) that aims to learn the interaction among the diverse and distinct features of the input modalities, i.e., text, acoustic and visual; (2) We employ a Context-aware Attention Module (CAM) that identifies and assigns the weights to the neighboring utterances based on their contributing features. It exploits the interactive representations of pairwise modalities to learn the attention weights, and\n(3) We present new state-of-the-arts for five benchmark datasets for both sentiment and emotion predictions.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Different reviews in (Arevalo et al., 2017;Poria et al., 2016Poria et al., , 2017bGhosal et al., 2018;Morency et al., 2011a;Zadeh et al., 2018a;Mihalcea, 2012;Lee et al., 2018;Tsai et al., 2018) suggest that multi-modal sentiment and emotion analysis are relatively new areas as compared to uni-modal analysis. Feature selection (fusion) is a challenging and important task for any multi-modal analysis. Poria et al. (2016) proposed a multi-kernel learning based feature selection method for multimodal sentiment and emotion recognition. A convolutional deep belief network (CDBN) is proposed in (Ranganathan et al., 2016) to learn salient multi-modal features of low-intensity expressions of emotions, whereas Lee et al. (2018) introduced a convolutional attention network to learn multimodal feature representation between speech and text data for multi-modal emotion recognition.\nA feature level fusion vector was built, and then a Support Vector Machine (SVM) classifier was used to detect the emotional duality and mixed emotional experience in (Patwardhan, 2017). Similar work on feature-level fusion based on self-attention mechanism is reported in (Hazarika et al., 2018). Fu et al. (2017) introduced an enhanced sparse local discriminative canoni-cal correlation analysis approach (En-SLDCCA) to learn the multi-modal shared feature representation. Tzirakis et al. (2017) introduced a Long Short Term Memory (LSTM) based end-to-end multi-modal emotion recognition system in which convolutional neural network (CNN) and a deep residual network are used to capture the emotional content for various styles of speaking, robust features. Poria et al. (2017a) presented a literature survey on various affect dimensions e.g., sentiment analysis, emotion analysis, etc., for the multi-modal analysis. A multi-modal fusion-based approach is proposed in (Blanchard et al., 2018) for sentiment classification. The author used exclusively high-level fusion of visual and acoustic features to classify the sentiment. Zadeh et al. (2016) presented the multi-modal dictionary-based technique to capture the interaction between spoken words and facial expression better when expressing the sentiment. In another work,  proposed a Tensor Fusion Network (TFN) to capture the inter-modality and intra-modality dynamics between the multi-modalities (i.e., text, visual, and acoustic). These works did not take contextual information into account. Poria et al. (2017b) introduced an Long Short Term Memory (LSTM) based framework for sentiment classification which uses contextual information to capture interrelationships between the utterances. In another work, Poria et al. (2017c) proposed a user opinion based framework to combine all the multi-modal inputs (i.e., visual, acoustic, and textual) by applying a multi-kernel learning-based approach. Contextual inter-modal attention mechanism was not explored in much details until recently. Zadeh et al. (2018a) introduced a multi-attention blocks based model for multi-modal sentiment classification but did not account for contextual information, whereas Ghosal et al. (2018) proposed a contextual inter-modal attention based framework for multi-modal sentiment classification. Recently, Zadeh et al. (2018c) introduced the largest multimodal dataset namely CMU-MOSEI for sentiment and emotion analysis. Author effectively fused the multi-modality inputs i.e., text, visual, and acoustic through a dynamic fusion graph and reported competitive performance w.r.t. various state-ofthe-art systems for both sentiment and emotion analysis. Very recently, Akhtar et al. (2019) in-troduced an attention based multi-task learning framework for sentiment and emotion classification on the CMU-MOSEI dataset.\nIn comparison to the existing systems, our proposed approach aims to exploits the interaction between the input modalities through an autoencoder based inter-modal interactive module. The interactive module learns the joint representation for the participating modalities, which are further utilized to capture the contributing contextual utterances in a context-aware attention module.\n3 Context-aware Interactive Attention (CIA) Affect Analysis\nIn this section, we describe our proposed approach for the effective fusion of multi-modal input sources. We propose an end-to-end Contextaware Interactive Attention (CIA) based recurrent neural network for sentiment and emotion analysis. As discussed earlier, one of the main challenges for multi-modal information analysis is to exploit the interaction among the input modalities. Therefore, we introduce an Inter-modal Interactive Module (IIM) that aims to learn the interaction between any two modalities through an auto-encoder like structure. For the text-acoustic pair of modalities, we aim to decode the acoustic representation through the encoded textual representation. After training of IIM, we extract the encoded representation for further processing. We argue that the encoded representation learns the interaction between the text and acoustic modalities. Similarly, we compute the interaction among all the other pairs (i.e., acoustic-text, text-visual, visualtext, acoustic-visual, and visual-acoustic). Next, we extract the sequential pattern of the utterances through a Bi-directional Gated Recurrent Unit (Bi-GRU) (Cho et al., 2014)). For each pair of modalities, the two representations denoting the interactions between them are combined through a mean operation. For an instance, we compute the mean of the text-acoustic and acoustic-text representations for text and acoustic modalities. The mean operation ensures that the network utilizes the two distinct representations by keeping the minimal dimension.\nIn our network, we, additionally, learn the interaction among the modalities through a feedforward network. At first, all the three modalities are passed through a separate Bi-GRU. Then, pair-Algorithm 1 Inter-modal Interactive Module for Multi-modal Sentiment and Emotion Recognition (IIM-MMSE)\nprocedure IIM-MMSE(t, v, a) for i \u2208 1, ..., K do K = #modalities for j \u2208 1, ..., K do \u2200x, y \u2208 [T, V, A], x = y and i \u2264 j C x i y j \u2190 IIM (x i , y j ) C x i y j \u2190 biGRU (C x i y j ) C x i \u2190 biGRU (x i ) for i, j \u2208 1, ..., K do \u2200x, y \u2208 [T, V, A], and x = y M x i ,y j \u2190 M ean(C x i y j , C y i x j ) cat x i ,y j \u2190 Concatenate(C x i , C y j ) BI x i ,y j \u2190 F ullyConnected(cat x i ,y j ) A x i ,y j \u2190 CAM (M x i ,y j , BI x i ,y j ) Rep \u2190 [A T V , A T A , A AV ] polarity \u2190 Sent(Rep)/Emo(Rep) return polarity Algorithm 2 Inter-Modal Interactive Module (IIM) procedure IIM(X, Y ) C XY \u2190 IIM Encoder (X, Y ) Y \u2190 IIM Decoder (C XY ) loss \u2190 cross entropy( Y , Y ) Backpropagation to update the weights return C XY Algorithm 3 Context-aware Attention Module (CAM) procedure CAM(M, BI) P \u2190 M.BI T Cross product for i, j \u2208 1, ..., u do u = #utterances N (i, j) \u2190 e P (i,j) u k=1 e P (i,k) O \u2190 N.BI A \u2190 O M Multiplicative gating. return A\nwise concatenation is performed over the output of Bi-GRU and passed through a fully-connected layer to extract the bi-modal interaction (BI). Further, we employ a Context-aware Attention Module (CAM) to exploit the correspondence among the neighboring utterances. The inputs to the CAM are the two representations for each pair of modalities, e.g., mean representation M T A and bi-modal interaction BI T A for the text-acoustic pair. The attention module assists the network in attending the contributing features by putting weights to the current and the neighboring utterances in a video. In the end, the pair-wise (i.e., text-acoustic, text-visual, and acoustic-visual) attended representations are concatenated and fed to an output layer for the prediction.\nWe depict and summarize the proposed approach in Figure 1 and Algorithm 1, 2, and 3. The source code is available at http://www.iitp. ac.in/\u02dcai-nlp-ml/resources.html.", "n_publication_ref": 27, "n_figure_ref": 1}, {"heading": "Context-aware Attention Module (CAM)", "text": "Since the utterances in a video are the split units of the break/pause of the speech, their emotions (or sentiments) often have relations with their neighboring utterances. Therefore, knowledge of the emotions (or, sentiments) of the neighboring utterances is an important piece of information and has the capability to derive the prediction of an utterance, if the available inputs are insufficient for the correct prediction.\nOur proposed context-aware attention module leverages the contextual information. For each utterance in a video, we compute the attention weights of all the neighboring utterances based on their contributions in predicting the current utterance. It ensures that the network properly utilizes the local contextual information of an utterance as well as the global contextual information of a video together. The aim is to compute the interactive attention weights utilizing a softmax activation for each utterance in the video. Next, we apply a multiplicative gating mechanism following the work of Dhingra et al. (2016). The attentive representation is, then, forwarded to the upper layers for further processing. We summarize the process of CAM in Algorithm3.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Inter-modal Interactive Module (IIM)", "text": "One of the key objectives of the multi-modal analysis is to fuse the available input modalities effectively. In general, different modalities represent distinct features despite serving a common goal. For example, in multi-modal sentiment analysis all the three modalities, i.e., text, acoustic, and visual, aim to predict the expressed polarity of an utterance. The distinctive features in isolation might create an ambiguous scenario for a network to learn effectively. Therefore, we introduce an auto-encoder based inter-modal interactive module whose objective is to learn the interaction between two distinct modalities to serve a common goal. The IIM encodes the feature representation of one modality (say, text), and aims to decode it into the feature representation of another modality (say, acoustic). Similar to an auto-encoder where the input and output are conceptually the same (or closely related), in our case the input and output feature representations of two modalities also intuitively serve a common goal. After training of IIM, the encoded vector signifies a joint representation of the two modalities, which can be further utilized in the network.\nAs the proposed architecture in Figure 1 depicts, our proposed model is an end-to-end system, which takes multi-modal raw features for each utterance in a video and predicts an output. We also train our proposed IIM in the combined framework. For any pair of modalities, e.g., text-visual, the encoded vector in IIM receives two gradients of errors, i.e., one error from the IIM output (visual) l 1 and another from the task-specific label l 2 . We aggregate the errors (l 1 + l 2 ) at the encoded vector and backpropagate it to the input (text). Thus, the weights in the encoder part will adjust according to the desired task-specific label as well. However, in contrast, the decoder part does not have such information. Therefore, we employ another IIM to capture the interaction between the visual-text. This time, the visual features are aware of the desired label during the interaction with textual features. A conceptual diagram, depicting the gradient flow in IIM for the text and visual modalities, is shown in Figure 2. ", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Datasets, Experiments and Analysis", "text": "In this section, we present our experimental results along with necessary analysis. We also compare our obtained results with several state-of-the-art systems.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets", "text": "For the evaluation of our proposed approach, we employ five multi-modal benchmark datasets 1 covering two affect analysis tasks, i.e., sentiment and emotion. Table 1: Results of sentiment and emotion analysis for the proposed approach. T: Text, V: Visual, A: Acoustic. Weighted accuracy as a metric is chosen due to unbalanced samples across various emotions and it is also in line with the other existing works (Zadeh et al., 2018c ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Setups", "text": "The above datasets offer different dimension of sentiment analysis. We define the following setups for our experiments.\n\u2022 Two-class (pos and neg) classification: MO-SEI, MOSI, ICT-MMMO, and MOUD.\n\u2022 Three-class (pos, neu, and neg) classification: YouTube.\n\u2022 Five-class (strong pos, weak pos, neu, weak neg, and strong neg) classification: MOSEI.\n\u2022 Seven-class (strong pos, moderate pos, weak pos, neu, weak neg, moderate neg, and strong neg) classification: MOSEI and MOSI.\n\u2022 Intensity prediction: MOSEI and MOSI.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments", "text": "We implement our proposed model on the Pythonbased Keras deep learning library. As the evaluation metric, we employ accuracy (weighted accu-racy (Tong et al., 2017)) and F1-score for the classification problems, while for the intensity prediction task, we compute Pearson correlation scores and mean-absolute-error (MAE).\nWe evaluate our proposed CIA model on five benchmark datasets i.e., MOUD, MOSI, YouTube, ICT-MMMO, and MOSEI. For all the datasets, we perform grid search to find the optimal hyperparameters (c.f. Table 4). Though we push for a generic hyper-parameter configuration for all datasets, in some cases, a different choice of the parameter has a significant effect. Therefore, we choose different parameters for different datasets for our experiments. Details of hyper-parameters for different datasets are depicted in Table 4.\nWe use different activation functions for the various modules in our model. We use tanh as the activation function for the inter-modal interactive module (IIM), while we employ ReLu for the context-aware attention module. For each dataset, we use Adam as optimizer.\nIn this paper, we address three multi-modal affective analysis problems, namely i.e., sentiment classification (S C ), sentiment intensity (S I ) and emotion classification (E C ). We use softmax as a classifier for sentiment classification, while optimizing the categorical cross-entropy as a loss function. In comparison, we use sigmoid for prediction and binary cross-entropy as the loss function for the emotion classification. As the emotions in the dataset are multi-labeled, we apply a threshold over the predicted sigmoid outputs for each emotion and consider all the emotions as present whose respective values are above the threshold. We cross-validate and optimize both   We evaluate our proposed approach for all the possible input combinations i.e., uni-modal (T, A, V), bi-modal (T+V, T+A, A+V) and tri-modal (T+V+A). We depict our obtained results in Table 1. For MOSEI dataset, with tri-modal inputs, our proposed system reports 79.02% F1-score and 62.97% weighted-accuracy for emotion classification. For sentiment classification, we obtain 78.23%, 80.37%, 49.15% and 50.14% as F1-score for two-class, five-class and seven-class, respectively. For sentiment intensity prediction task, our proposed system yields MAE and Pearson score of 0.683 and 0.594, respectively. We also observe that the proposed approach yields better performance for the tri-modal inputs than the bi-modal and uni-modal input combinations. This improvement implies that our proposed CIA architecture utilizes the interaction among the input modalities very effectively. Furthermore, for the other datasets, i.e., MOSI, ICT-MMMO, YouTube, and MOUD, we also observe a similar phenomenon as well (c.f. Table 1).\nTo show that our proposed IIM module, indeed, learns the interaction among the distinct modalities, we also perform an ablation study of the proposed CIA architecture. Consequently, we omit the IIM module from our architecture and compute the self-attention on the pair-wise fully-connected representations for the prediction. We observe that, for all the datasets, the performance of this modified architecture (i.e., CIA -IIM) is constantly inferior (with 1% to 7% F-score points) to the proposed CIA architecture. This performance degradation suggests that the IIM module is, indeed, an important component of our proposed architecture. In Table 3, we depict the evaluation results for both-with and without IIM.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Comparative Analysis", "text": "In this section, we present our comparative studies against several existing and recent state-ofthe-art systems. For each dataset, we report three best systems for the comparisons 2 . In particular, we compare with the following systems: Bag of Feature -Multimodal Sentiment Analysis (BoF-MSA) (Blanchard et al., 2018), Memory Fusion Network (MFN) (Zadeh et al., 2018b) (Zadeh et al., 2018c), Tensor Fusion Network (TFN) , Random Forest (RF) (Breiman, 2001), Support Vector Machine (Zadeh et al., 2016), Multi-Attention Recurrent Network (MARN) (Zadeh et al., 2018a), Dynamic Fusion Graph (DFG) (Zadeh et al., 2018c), Multi Modal Multi Utterance-Bimodal Attention (MMMU-BA) (Ghosal et al., 2018), Bi-directional Contextual LSTM (BC-LSTM) (Poria et al., 2017b) and Multimodal Factorization Model (MFM) (Tsai et al., 2018).\nWe show the comparative results in Table 5a and Table 5b for emotion and sentiment analysis, respectively. We observe that the proposed CIA framework yields better performance against the state-of-the-art for all the cases. For emotion classification, our proposed approach achieves approximately 3 and 0.6 percentage higher F1-   (Zadeh et al., 2018c). \u2020 Values are taken from (Tsai et al., 2018). Significance T-test (< 0.05) signifies that the obtained results are statistically significant over the existing systems with 95% confidence score.\nscore and weighted accuracy, respectively, than the state-of-the-art DFG (Zadeh et al., 2018c) system. Furthermore, we also see improvements for most of the individual emotion classes as well.\nIn sentiment analysis (c.f. Table 5b), for all the five datasets and different experimental setups, the proposed CIA framework obtains the improved accuracies for the classification tasks. For intensity prediction, our proposed framework yields lesser mean-absolute-error with high Pearson correlation scores. On average, we observe 1 to 5% improvement in accuracy values in comparison to the next best systems. Similarly, for the intensity prediction task, we report approximately 0.03 and 0.04 points improvement in mean-absolute-error and Pearson score, respectively.\nWe perform statistical significance test (paired T-test) on the obtained results and observe that performance improvement in the proposed model over the state-of-the-art is significant with 95% confidence (i.e., p-value< 0.05).", "n_publication_ref": 13, "n_figure_ref": 0}, {"heading": "Error Analysis", "text": "We analyze our proposed CIA model to understand the importance of the baseline framework CIA-IIM. We study the predictions of both the models and observe that the proposed CIA framework improves the predictions of the baseline CIA-IIM model. It indicates that the CIA framework, indeed, learns the interaction among the input modalities, and the model effectively exploits this interaction for better judgment. In Table 6, we list the utterances of a CMU-MOSEI video along with their correct and predicted labels for both the proposed and baseline systems.\nThe video in Table 6 has 4 utterances, out of which the correct sentiments of three utterances (i.e., u 1 , u 3 , and u 4 ) are positive, while one utterance (i.e., u 2 ) is negative. We observe that our proposed CIA model predicts all the 4 utterances correctly, while the CIA-IIM mis-classify the sentiments of the utterances, u 2 and u 3 .\nWe also analyze the context-aware attention module (CAM) with the help of heatmaps of the attention weights. The heatmaps, as depicted in Figure 3, represent the contributing utterances in the neighbourhood for the classification of each utterance. Figures 3a, 3b and 3c show the heatmaps of the pair-wise modality interaction of the proposed model CIA. In Figure 3a, each cell(i,j) of    the heatmap signifies the weights of utterance 'j' for the classification of utterance 'i'. For the utterance u 4 , the model puts more attention weights on the u 2 and u 3 of the text-visual interactions, while for the text-acoustic interaction the model assigns higher weights to the u 4 utterance itself. Similarly, the model assigns the least weight to the u 1 utterance, whereas the utterance u 3 gets the highest weights. We argue that the proposed CAM module captures the diversity in the input modalities of the contextual utterances for the correct prediction.\nFor emotion prediction, the CIA model captures all the emotions correctly, while the CIA-IIM framework fails to predict the correct emotions of the utterances, u 2 and u 3 . For the same video, we also show the attention heatmaps for emotion in Figure 3. For the utterance u 2 , our proposed model (CIA) captures the emotion class 'sad' as the CAM module assigns higher attention weights on the utterances u 2 and u 3 in Figure 3d, u 4 in Figure 3e, and u 2 in Figure 3f. Since the system finds the contributing neighbours as utterances u 2 , u 3 and u 4 for various combinations, we argue that it utilizes the information of these utterances -which all express the 'sad' emotion -for the correct prediction of utterance u 2 as 'sad'.", "n_publication_ref": 0, "n_figure_ref": 6}, {"heading": "Conclusion", "text": "In this paper, we have proposed a Context-aware Interactive Attention framework that aims to capture the interaction between the input modalities for the multi-modal sentiment and emotion prediction. We employed a contextual attention module to learn the contributing utterances in the neighborhood by exploiting the interaction among the input modalities. We evaluate our proposed approach on five standard multi-modal datasets. Experiments suggest the effectiveness of the proposed model over various existing systems, for both sentiment and emotion analysis, as we obtained new state-of-the-art for all five datasets.\nIn current work, we undertook the problem of sentiment and emotion analysis for a single-party utterances. In future, we would like to extend our work towards the multi-party dialogue.\n6 Acknowledgment", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "", "text": "The research reported here is partially supported by SkyMap Global India Private Limited. Asif Ekbal acknowledges the Young Faculty Research Fellowship (YFRF), supported by Visvesvaraya PhD scheme for Electronics and IT, Ministry of Electronics and Information Technology (MeitY), Government of India, being implemented by Digital India Corporation (formerly Media Lab Asia).", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Multi-task learning for multimodal emotion recognition and sentiment analysis", "journal": "Long and Short Papers", "year": "2019", "authors": "Dushyant Md Shad Akhtar; Deepanway Chauhan; Soujanya Ghosal;  Poria"}, {"title": "Gated multimodal units for information fusion", "journal": "", "year": "2017", "authors": "John Arevalo; Thamar Solorio; Manuel Montes-Y G\u00f3mez; Fabio A Gonz\u00e1lez"}, {"title": "Getting the subtext without the text: Scalable multimodal sentiment classification from visual and acoustic modalities", "journal": "", "year": "2018", "authors": "Nathaniel Blanchard; Daniel Moreira; Aparna Bharati; Walter Scheirer"}, {"title": "Random forests. Machine Learning", "journal": "", "year": "2001", "authors": "Leo Breiman"}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "journal": "CoRR", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merrienboer; Dzmitry Bahdanau; Yoshua Bengio"}, {"title": "Gated-attention readers for text comprehension", "journal": "", "year": "2016", "authors": "Bhuwan Dhingra; Hanxiao Liu; Zhilin Yang; W William; Ruslan Cohen;  Salakhutdinov"}, {"title": "Multimodal shared features learning for emotion recognition by enhanced sparse local discriminative canonical correlation analysis. Multimedia Systems", "journal": "", "year": "2017", "authors": "Jiamin Fu; Qirong Mao; Juanjuan Tu; Yongzhao Zhan"}, {"title": "Contextual inter-modal attention for multi-modal sentiment analysis", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Deepanway Ghosal; Shad Md; Dushyant Akhtar; Soujanya Chauhan;  Poria"}, {"title": "Self-attentive feature-level fusion for multimodal emotion detection", "journal": "", "year": "2018-04-10", "authors": "Devamanyu Hazarika; Sruthi Gorantla; Soujanya Poria; Roger Zimmermann"}, {"title": "Convolutional attention networks for multimodal emotion recognition from speech and text data", "journal": "", "year": "2018", "authors": "Chan Woo Lee; Ye Kyu; Jihoon Song; Woo Yong Jeong;  Choi"}, {"title": "Multimodal sentiment analysis", "journal": "", "year": "2012-07-12", "authors": "Rada Mihalcea"}, {"title": "Towards multimodal sentiment analysis: harvesting opinions from the web", "journal": "", "year": "2011-11-14", "authors": "Louis-Philippe Morency; Rada Mihalcea; Payal Doshi"}, {"title": "Towards multimodal sentiment analysis: Harvesting opinions from the web", "journal": "ACM", "year": "2011", "authors": "Louis-Philippe Morency; Rada Mihalcea; Payal Doshi"}, {"title": "Deep multimodal fusion for persuasiveness prediction", "journal": "ACM", "year": "2016", "authors": "Behnaz Nojavanasghari; Deepak Gopinath; Jayanth Koushik; Tadas Baltru\u0161aitis; Louis-Philippe Morency"}, {"title": "Multimodal mixed emotion detection", "journal": "IEEE", "year": "2017", "authors": "S Amol;  Patwardhan"}, {"title": "Utterance-level multimodal sentiment analysis", "journal": "", "year": "2013", "authors": "Ver\u00f3nica P\u00e9rez-Rosas; Rada Mihalcea; Louis-Philippe Morency"}, {"title": "A review of affective computing: From unimodal analysis to multimodal fusion", "journal": "", "year": "2017", "authors": "Soujanya Poria; Erik Cambria; Rajiv Bajpai; Amir Hussain"}, {"title": "Context-dependent sentiment analysis in user-generated videos", "journal": "", "year": "2017", "authors": "Soujanya Poria; Erik Cambria; Devamanyu Hazarika; Navonil Majumder; Amir Zadeh; Louis-Philippe Morency"}, {"title": "Convolutional mkl based multimodal emotion recognition and sentiment analysis", "journal": "IEEE", "year": "2016", "authors": "Soujanya Poria; Iti Chaturvedi; Erik Cambria; Amir Hussain"}, {"title": "Ensemble application of convolutional neural networks and multiple kernel learning for multimodal sentiment analysis", "journal": "Neurocomputing", "year": "2017", "authors": "Soujanya Poria; Haiyun Peng; Amir Hussain; Newton Howard; Erik Cambria"}, {"title": "Extending long short-term memory for multi-view structured learning", "journal": "Springer International Publishing", "year": "2016", "authors": "Louis-Philippe Shyam Sundar Rajagopalan; Tadas Morency; Roland Baltrusaitis;  Goecke"}, {"title": "Multimodal emotion recognition using deep learning architectures", "journal": "", "year": "2016", "authors": "Shayok Hiranmayi Ranganathan; Sethuraman Chakraborty;  Panchanathan"}, {"title": "", "journal": "IEEE Winter Conference on", "year": "", "authors": ""}, {"title": "Combating Human Trafficking with Multimodal Deep Models", "journal": "", "year": "2017", "authors": "Edmund Tong; Amir Zadeh; Cara Jones; Louis-Philippe Morency"}, {"title": "Learning factorized multimodal representations", "journal": "", "year": "2018", "authors": ""}, {"title": "End-to-end multimodal emotion recognition using deep neural networks", "journal": "IEEE Journal of Selected Topics in Signal Processing", "year": "2017", "authors": "Panagiotis Tzirakis; George Trigeorgis; A Mihalis;  Nicolaou; W Bj\u00f6rn; Stefanos Schuller;  Zafeiriou"}, {"title": "Youtube movie reviews: Sentiment analysis in an audio-visual context", "journal": "IEEE Intelligent Systems", "year": "2013", "authors": "Martin W\u00f6llmer; Felix Weninger; Tobias Knaup; Bj\u00f6rn Schuller; Congkai Sun; Kenji Sagae; Louis-Philippe Morency"}, {"title": "Multi-attention recurrent network for human communication comprehension", "journal": "", "year": "2018", "authors": "A Zadeh; S Liang; P Poria;  Vij; L P Cambria;  Morency"}, {"title": "Multimodal Sentiment Intensity Analysis in Videos: Facial Gestures and Verbal Messages", "journal": "IEEE Intelligent Systems", "year": "2016", "authors": "A Zadeh; R Zellers; E Pincus; L P Morency"}, {"title": "Tensor fusion network for multimodal sentiment analysis", "journal": "", "year": "2017", "authors": "Amir Zadeh; Minghai Chen; Soujanya Poria; Erik Cambria; Louis-Philippe Morency"}, {"title": "Soujanya Poria, Erik Cambria, and Louis-Philippe Morency", "journal": "AAAI Press", "year": "2018-02-02", "authors": "Amir Zadeh; Paul Pu Liang; Navonil Mazumder"}, {"title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Amir Zadeh; Paul Pu Liang; Soujanya Poria; Erik Cambria; Louis-Philippe Morency"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Overall architecture of the proposed Context-aware Interactive Attention framework.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Difference between the interaction modules of text-visual and visual-text pairs. l 1 & l 2 are the losses from the IIM and output labels, respectively.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": ",Deep Fusion -Deep Neural Network (DF-DNN) (Nojavanasghari et al., 2016), Multi View -LSTM (MV-LSTM) (Rajagopalan et al., 2016), Early Fusion -LSTM (EF-LSTM)", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Heatmap analysis of MOSEI dataset where (a), (b) & (c) represents the Contextual Attention weights for TV, TA and AV for sentiment and (d), (e) & (f) are contextual attention weights for TV, TA and AV for emotion.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "T 77.66 60.13 77.22 79.45 48.50 48.87 0.695 0.572 77.43 77.69 37.75 0.996 0.658 71.80 77.50 48.27 49.15 72.24 73.58 A 76.14 57.68 74.64 77.27 45.02 44.97 0.782 0.433 59.30 59.32 23.17 1.423 0.165 77.90 78.34 43.12 44.06 45.46 60.37 V 75.03 57.59 69.13 75.04 42.22 42.51 0.804 0.317 50.48 51.31 23.03 1.465 0.122 78.28 78.75 49.79 50.84 62.47 62.76 T+V 78.01 57.10 77.31 79.51 48.98 49.84 0.685 0.592 77.70 78.13 38.19 0.946 0.673 80.10 81.25 53.11 53.91 78.40 79.24 T+A 77.53 60.69 77.80 80.06 48.66 49.08 0.694 0.579 78.98 79.15 39.35 0.952 0.671 78.99 80.01 48.82 49.37 72.93 74.52 A+V 76.60 59.47 74.76 77.38 45.32 45.87 0.775 0.437 49.30 53.49 24.19 1.464 0.189 79.12 81.25 51.93 52.54 64.41 65.09 T+A+V 79.02 62.97 78.23 80.37 49.15 50.14 0.683 0.594 79.54 79.88 38.92 0.914 0.689 81.47 82.75 55.13 55.93 82.07 82.41", "figure_data": "MOSEIMOSIICT-MMMOYouTubeMOUDEmotionSentimentSentimentSentimentSentimentSentimentF1W-AccF1A 2A 5A 7MAErF1A 2A 7MAErF1A 2F1A 3F1A 21. YouTube (Morency et al., 2011b): TheYouTube opinion dataset contains 269 prod-uct reviews utterances across 47 videos.There are 169, 41, and 59 utterances in train-ing, validation, and test set, respectively.a col-lection of 2199 opinion utterances annotatedwith the sentiment class. There are 1284 ut-terances in the training set, 229 utterances inthe validation set, and 686 utterances in thetest set.5. CMU-MOSEI (Zadeh et al., 2018c): CMU-MOSEI is the largest dataset among all theabove. It has 3,229 videos which compriseof approximately 23,000 utterances. Eachutterance is associated with one sentimentvalue and six emotions (i.e., anger (4903utterances), disgust (4028 utterance), fear(1850 utterance), happy (12135 utterance),"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "CIA 79.02 62.97 78.23 80.37 49.15 50.14 0.683 0.594 79.54 79.88 38.92 0.914 0.689 81.47 82.75 55.13 55.93 82.07 82.41 CIA -IIM 77.81 61.86 77.69 79.53 48.02 49.16 0.714 0.566 39.32 55.24 34.40 0.941 0.652 80.10 81.25 50.02 52.54 78.14 78.30", "figure_data": "MOSEIMOSIICT-MMMOYouTubeMOUDModalityEmotionSentimentSentimentSentimentSentimentSentimentF1W-AccF1A 2A 5A 7MAErF1A 2A 7MAErF1A 2F1A 3F1A 2"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Ablation results for IIM module.the evaluation metrics, i.e., F1-score and weighted accuracy, and set the threshold as 0.4 and 0.18, respectively.", "figure_data": "Parameters MOUD MOSI YouTube MMMO MOSEIBi-GRU50N0.3D200N100N 0.5DFC50N200N 0.5D100NActivationsReLu as activation for our model tanh as activation in IIMOutputSoftmax (S C ), tanh (S I ) & Sigmoid (E C )OptimizerAdam (lr=0.001)IIM LossMean Square Error (MSE)Model LossCross-entropy (Classification) & MSE (Intensity)Threshold0.4 (F1) & 0.18 (W-Acc) for multi label in E CBatch16Epochs50"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Hyper-parameters for our experiments where N, D, S C , S I and E C stands for #neurons, dropout, sentiment classification, sentiment intensity and emotion classification respectively.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Comparative results. Values are taken from", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "these critics argue that the welfare state breeds dependence and incompetence among those who receive it they", "figure_data": "SentimentEmotionUtterancesActual CIA\u2212IIM CIA ActualCIA\u2212IIM CIA1 PosPosPos Happy, Sad HappyHappy, Sad2 argue that it creates social pathologies such as single parentNegPosNeg SadHappySadfamilies excess fertility and laziness3 some argue that people who receive welfare benefits cannotPosNegPos Fear, SadSadFear, Sadspend their benefits rationally and4 and then lastly some people on the moral side argue that noth-PosPosPos SadHappy, Sad Sading should be given to a person without requiring a reciprocalobligation from that person so"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Comparison between proposed CIA and CIA\u2212IIM frameworks in MOSEI dataset. Few cases where CIA framework performs better than the CIA\u2212IIM framework. Red text signifies error in classification.", "figure_data": ""}], "doi": "10.18653/v1/N19-1034"}