{"authors": "Saurabh Srivastava; Prerna Khurana; Vartika Tewari", "pub_date": "", "title": "Identifying Aggression and Toxicity in Comments using Capsule Network", "abstract": "Aggression and related activities like trolling, hate speech etc. involve toxic comments in various forms. These are common scenarios in today's time and websites react by shutting down their comment sections. To tackle this, an algorithmic solution is preferred to human moderation which is slow and expensive. In this paper, we propose a single model capsule network with focal loss to achieve this task which is suitable for production environment. Our model achieves competitive results over other strong baseline methods, which show its effectiveness and that focal loss exhibits significant improvement in such cases where class imbalance is a regular issue. Additionally, we show that the problem of extensive data preprocessing, data augmentation can be tackled by capsule networks implicitly. We achieve an overall ROC AUC of 98.46 on Kaggletoxic comment dataset and show that it beats other architectures by a good margin. As comments tend to be written in more than one language, and transliteration is a common problem, we further show that our model handles this effectively by applying our model on TRAC shared task dataset which contains comments in code-mixed Hindi-English.", "sections": [{"heading": "Introduction", "text": "In today's time, with an ever increasing penetration of social media, news portals, blogs, QnA forums, and other websites that allow user interaction, users often end up inviting comments that are nasty, harrasing, insulting, toxic etc. This can have adverse effects on users, who then become victims of cyberbullying or online harrasment. An online survey carried out by the Pew Research Centre in 2017 states that 4 in 10 Americans have personally experienced online harrasment. Strikingly, 1 in 5 Americans have witnessed severe form of online harrasment like physical threats, stalking, sexual harrasment etc. There are several challenges associated with solving this kind of problem. First being the problem of class imbalance found in the dataset. Since such type of comments are sparse in nature, they introduce skewness in the dataset. There are several ways to handle this problem, however, we choose a more recent technique which modifies the standard cross entropy loss function known as Focal Loss (Lin et al., 2017). We will briefly describe how it helps in improving classifier performance. The next problem we want to address is that of data preprocessing. This is the most time consuming task and requires a good understanding of the data. However, we wish to minimise this process so as to have a good model with minimal preprocessing of the data. Another frequently observed challenge is transliteration, which is often observed, especially, when we are working with text data from social networking websites. Users tend to speak in more than one language in the same statement. This leads to several out of vocabulary or OOV words for which the model would not have any word embedding. We use randomly initialised word embeddings in such a case and show how they can be trained during model training procedure such that it results in clusters of OOV words which have similar meaning in Hindi. We propose to tackle all the above described challenges using a single model as opposed to ensemble of several other models, which is a common practice in such competitive challenges. We also show that our proposed model can converge really quickly, hence the model can be trained in lesser time. This is essential when the model has to be deployed in a production environment where it requires retraining periodically.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Early works in automated detection of abusive language made use of basic machine learning like Tf-Idf (Yin et al., 2009), SVM (Warner and Hirschberg, 2012), Naive Bayes, random forests, or logistic regression over a bag-of-ngrams and achieved limited success. Newer approaches include solving problems using deep learning architectures like CNNs (Kim, 2014;Zhang et al., 2015;Conneau et al., 2017b;Park and Fung, 2017) which just focus on spatial patterns or LSTMs which treat text as sequences (Tai et al., 2015;Mousa and Schuller, 2017). Another popular approach completely ignores the order of words but focuses on their compositions as a collection, like probabilistic topic modeling (Blei et al., 2003;Mcauliffe and Blei, 2008) and Earth Movers Distance based modeling (Kusner et al., 2015;Ye et al., 2017). Recently Capsule Network (Sabour et al., 2017) has been used in text classification (Zhao et al., 2018).It makes use of the dynamic routing process to alleviate the disturbance of some noise capsules which may contain background information such as stop words and words that are unrelated to specific categories and show that capsule networks achieves significant improvement over strong baseline methods. As we focus to solve the problem of toxic comments and cyberbullying, we are confronted with the issue of large class imbalance. We use focal loss (Lin et al., 2017) to tackle it as it prevents the vast number of easy negatives from overwhelming the detector during training. Also, in the online space people tend to talk using different languages in the same comment and often use transliteration. We show that our model is suitable for such data as well.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Capsule Net for Classification", "text": "Proposed Model: The model proposed in (Zhao et al., 2018) has been used for the experimentation with an inclusion of Focal Loss (Lin et al., 2017) as a loss function to address the class imbalance problem. In our experiments we have compared performances of CNNs and RNNs as feature extractors and found that sentence representation obtained from RNNs performs better than representations obtained after applying convolution operation, although CNNs tends to perform better on short texts. The model consists of four layers:\n(i) Word Embedding Layer: We represent every comment x i , as a sequence of one-hot encoding of its words, x i = (w 1 , w 2 , ...w n ) of length n max , which is the maximum length of the comment, with zero padding. Such a sequence becomes the input to the embedding layer. To represent word tokens several ideas like sparse representation or dense representation (Collobert and Weston, 2008;Bengio et al., 2003) have been proposed.\n(ii) Feature Extraction Layer: This layer has been used to extract either n-grams feature at different position of a sentence through different filters (CNNs) or long term temporal dependencies within the sentence (RNNs). We use RNNs as feature extractors in our final model.\n(iii) Capsule Layer: The Capsule layer is primarily composed of two sub-layers Primary Capsule Layer and Convolutional Capsule Layer. The primary capsule layer is supposed to capture the instantiated parameters of the inputs, for example, in case of texts local order of words and their semantic representation. Suppose we have\u00ea number of feature extractors, then the input to the Primary capsule layer will be Z \u2208 R n\u00d7\u00ea (where n is the number of timesteps in RNNs). The primary capsules transform a scalar-output feature detector to vector-valued capsules to capture the instantiated features. Let d be the dimension of each capsule, then for each capsule p i \u2208 R d , where p denotes instantiated parameters set of a capsule (Sabour et al., 2017), we have\np i = g (W Z i + b)\n, where Z i is captured by RNNs in the feature extractor layer. Here, g is the nonlinear squash function which shrinks the small vectors to around 0 and large vectors around 1.\n(iv) The Convolutional Capsule: The Conv layers capsules output a local grid of vectors to capsules in earlier layers using different transformation matrices for each capsule and grids member (Sabour et al., 2017).Capsule networks are trained using a dynamic routing algorithm that overlooks words that are not important or unrelated in the text, like stopwords and name mentions.    \nF L(p t ) = \u2212\u03b1 t (1 \u2212 p t ) \u03b3 log(p t ), where p t = { p if y = 1 1 \u2212 p else \u03b3 is", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Experiments", "text": "In this section we attempt to describe different models that we have used for the classification process.\nWe seek to answer the following questions: (1) Is combination of Capsules and focal loss the new apotheosis for toxic comment classification problems? (2) Can capsules solve the problem of OOV and transliteration implicitly ?", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Kaggle Toxic Comment Classification:", "text": "Recently, Kaggle hosted a competition named Toxic Comment Classification. This dataset has been contributed by Conversation AI, which is a research initiative founded by Jigsaw and Google. The task was comprised of calculating the log-likelihood of a sentence for the six classes, i.e., given a sentence calculate the probability of it belonging to six classes. The six different classes were toxic, severe toxic, obscene, threat, insult and identity hate.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "TRAC dataset:", "text": "\"First Shared Task on Aggression Identification\" released a dataset for Aggression Identification. The task was to classify the comments into one of the three different classes Overtly Aggressive, Covertly Aggressive, and Non-aggressive. The train data was given in English and Hindi, where some of the comments in Hindi dataset were transliterated to English.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Data Preprocessing", "text": "For all our experiments, to show efficacy of our approach we kept the preprocessing as minimal as possible. Apart from word lowerization, tokenization, and punctuation removal we didn't perform any other activity.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Baseline Algorithms", "text": "We evaluate and compare our model with several strong baseline methods including: LSTM with Maxpool (Lai et al., 2015), Attention networks (Raffel and Ellis, 2015), Pre-trained LSTMs (Dai and Le, 2015), Hierarchical ConvNet (Conneau et al., 2017a), Bi-LSTM with Skip-connections, variation of CNN-LSTM (Wang et al., 2016), CNN-multifilter (Kim, 2014), Bi-LSTM with xgboost and logistic regression. We experiment with these models on three datasets. The models were first evaluated on Kaggle competition for Toxic Comment Classification. All the model parameters and attributes were decided on the basis of our best performing model, and were kept same for the rest of experimentations and datasets.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Model Training", "text": "For all our experiments we have used pre-trained embeddings for each word token obtained from (Joulin et al., 2016). We have also exploited (Pennington et al., 2014), (Mikolov et al., 2013), random and manually trained embeddings for initialization. After experimentation, fasttext embeddings with dimension of 300 were found to perform better than rest of the initialization process. In our experiments we observed that RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014) as an optimizer works well for training RNNs and CNNs respectively and used this throughout. The learning rate was kept between [.1 and .001]. For CNNs, number of Kernels was chosen from the range [128,256,512] and the LSTM units were selected from the range [128,256]. In all of our experiments with the proposed model only a single layer for feature extraction was used. Number of capsules was varied from [8,10,16], the vector length of 8 for each capsule was found to be the best, and the dropout values for RNNs were taken as per suggestions from (Zaremba et al., 2014). The \u03b1 and \u03b3 values in focal loss were experimented for [1.5, 2, 2.5, 3, 3.5] and [.2, .25, .3] and finally \u03b1 = 2 and \u03b3=0.25 were taken.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Results and Discussions", "text": "The proposed CapsNet architecture was able to beat other strong baseline algorithms with reasonable difference in accuracies with minimal preprocessing. We demonstrated that using focal loss along with CapsNet gave us .25 raise in the ROC-AUC for Kaggle's toxic comment classification and 1.39 and .80 gain in F1 scores on TRAC shared task dataset in English, from Facebook and Twitter comments respectively. All of our experiments were performed on NVIDIA Quadro M1200 4096 MB GPU system with 32 GB RAM and Intel i7 processor. The model took almost 33 minutes for an epoch to train which was faster in comparison with other models, with exception to the models using CNNs as feature extractors. For example, the second best performing model, which uses Pre-trained LSTM embeddings takes more than a day for the autoencoder to train and further 39+ minutes for each epoch. Hence, we can say that our model is viable for production environment. We have tested the capability of the architecture to handle the OOV words or misspelled words. For this we used TRAC shared dataset, initialised the word embeddings randomly and trained the model for classification process. Next, we enabled the embeddings to be changed during training process which is mentioned as dynamic channel in (Kim, 2014) to let the model learn new embeddings. After training, we took the weights of embedding layer and plotted these embedings using Tensorboard (Abadi et al., 2015). From figure 2 we can see that the model is able to minimise the distance between the misspelled word and is able to capture the relationship between transliterated words as in Table :2. We found that total of 3 clusters were formed after the experiment as shown in Fig: 2b. We investigated these clusters and found that some of the highly used words in the comments belonged to certain classes. For example, one of the cluster contained more of neutral words, another cluster contained highly aggressive and abusive words, and the third cluster contained some toxic words along with place and country names related to one's origin which were used in some foul comments.\nWe show the capability of our model to tackle the problem of overfitting, as observed during training we see the model to have comparatively lower difference in training and validation loss than other models. Same can be seen from Fig: 2a the loss margin difference doesn't change . We have shown that, not only our model has performed well on the classification task, it also has ability to generalise well and can learn good representation for word tokens.  ", "n_publication_ref": 2, "n_figure_ref": 3}, {"heading": "Conclusion and Future Work", "text": "In this work, we have proposed to automatically detect toxicity and aggression in comments, we show that with minimal preprocessing techniques we are able to achieve a good model performance and demonstrated how OOV words and semantic sense are learnt implicitly with random initialisation. We show the effectiveness of our proposed model against strong benchmark algorithms and that it outperforms others. In this work, we did basic preprocessing of the data, however in future we intend to explore more preprocessing techniques for the dataset, like data augmentation using translation approaches and methods to deal with mispelled words. We further would examine the results of capsule net by visualising which words or phrases does the model correctly recognises for classification as opposed to benchmark algorithms. Also, we would like to examine the usage of focal loss with the rest of the baseline models.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.4 Hierarchical ConvNets", "text": "Convolutional Neural Networks are known to perform well on short texts (Yin et al., 2017), in (Conneau et al., 2017a) authors proposed to concatenate representation at different levels of input sentence. The model was claimed to capture hierarchical abstractions of the input sentence. For our experiments, we fixed 128 kernels of size 2, 3, 4, 4 at 4 different levels. These values were decided after the experiments with different number of kernels and their sizes.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A.5 Bi-LSTMS with skip connections", "text": "In one of our experiments, the summary vector obtained from LSTMs was concatenated with the vector obtained after appying Max Over Time on the hidden state representation of the input. The intuition behind this was that, by passing most relevant features along with summary of the input to the softmax layer may enhance the clasification process. From the experiments we obtained competetive results using this model.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.6 Pre-trained LSTMs", "text": "In (Dai and Le, 2015), authors claimed that by pretraining LSTMs on some related task as Auto-Encoder or as a Language Model, could optimize the stability of the LSTMs training process. The authors reported improvenemt in error rates by good margin in many tasks like, text classification on 20 Newsgroup, IMDB etc. For our experiments we gathered many related datasets like all of Wikimedia datasets (Wulczyn et al., 2017), TRAC shared dataset, IMDB movie reviews dataset. An autoencoder was trained on these datasets and the LSTMs from the encoder part were extracted and used in the classifcation task.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A Baseline Algorithm Descriptions", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.1 CNN Multifiter", "text": "The idea of applying CNNs for text classification was proposed in (Kim, 2014), where authors applied filters of different length to extract N-gram features from text. The authors tried static and dynamic embedding channels and concluded that the model with combination of both outperformed others. For our setting we found that filters of length [2,3,4] have outperformed other filter sizes, we tried various combinations from range [2,5]. For activations we used Leaky ReLU, and performed Batch Normalization to stablize the data.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "A.2 CNN LSTM", "text": "A joint architecture of CNNs and RNNs were proposed in (Wang et al., 2016), where the authors tried combination of CNNs with different RNNs like GRUs and LSTMs. In our experiment, we again used Leaky ReLU for CNNs activations, filter size of 3 was fixed for the experiments to decide the dropout values and other hyperparameters tuning.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A.3 Bi-directional LSTM with maxpool", "text": "In (Lai et al., 2015), authors took Max Over Time on the RNN representation of the input. Their model RNN outperformed other models in 3 out of 4 datasets. In our experiments, we fixed LSTM units to be 51, and rest of the parameters were decided on the basis of validation-data experiments.", "n_publication_ref": 1, "n_figure_ref": 0}], "references": [{"title": "Tensor-Flow: Large-scale machine learning on heterogeneous systems", "journal": "", "year": "2015", "authors": "Mart\u00edn Abadi; Ashish Agarwal; Paul Barham; Eugene Brevdo; Zhifeng Chen; Craig Citro; Greg S Corrado; Andy Davis; Jeffrey Dean; Matthieu Devin; Sanjay Ghemawat; Ian Goodfellow; Andrew Harp; Geoffrey Irving; Michael Isard; Yangqing Jia; Rafal Jozefowicz; Lukasz Kaiser; Manjunath Kudlur; Josh Levenberg"}, {"title": "A neural probabilistic language model", "journal": "Journal of machine learning research", "year": "2003-02", "authors": "Yoshua Bengio; R\u00e9jean Ducharme; Pascal Vincent; Christian Jauvin"}, {"title": "Latent dirichlet allocation", "journal": "Journal of machine Learning research", "year": "2003-01", "authors": "M David;  Blei; Y Andrew; Michael I Jordan Ng"}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "journal": "ACM", "year": "2008", "authors": "Ronan Collobert; Jason Weston"}, {"title": "Supervised learning of universal sentence representations from natural language inference data", "journal": "", "year": "2017", "authors": "Alexis Conneau; Douwe Kiela; Holger Schwenk; Loic Barrault; Antoine Bordes"}, {"title": "Very deep convolutional networks for text classification", "journal": "", "year": "2017", "authors": "Alexis Conneau; Holger Schwenk; Lo\u00efc Barrault; Yann Lecun"}, {"title": "Semi-supervised sequence learning", "journal": "", "year": "2015", "authors": "M Andrew; Quoc V Dai;  Le"}, {"title": "Bag of tricks for efficient text classification", "journal": "", "year": "2016", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Tomas Mikolov"}, {"title": "Convolutional neural networks for sentence classification", "journal": "", "year": "2014", "authors": "Yoon Kim"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "From word embeddings to document distances", "journal": "", "year": "2015", "authors": "Matt Kusner; Yu Sun; Nicholas Kolkin; Kilian Weinberger"}, {"title": "Recurrent convolutional neural networks for text classification", "journal": "", "year": "2015", "authors": "Siwei Lai; Liheng Xu; Kang Liu; Jun Zhao"}, {"title": "Focal loss for dense object detection", "journal": "", "year": "2017-10-22", "authors": "Tsung-Yi Lin; Priya Goyal; Ross B Girshick"}, {"title": "Supervised topic models", "journal": "", "year": "2008", "authors": "D Jon; David M Mcauliffe;  Blei"}, {"title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"title": "Contextual bidirectional long short-term memory recurrent neural network language models: A generative approach to sentiment analysis", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Amr Mousa; Bj\u00f6rn Schuller"}, {"title": "One-step and two-step classification for abusive language detection on twitter", "journal": "", "year": "2017", "authors": "Ji Ho; Park ; Pascale Fung"}, {"title": "Global vectors for word representation", "journal": "EMNLP", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"title": "Feed-forward networks with attention can solve some long-term memory problems", "journal": "", "year": "2015", "authors": "Colin Raffel; P W Daniel;  Ellis"}, {"title": "Dynamic routing between capsules", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Sara Sabour; Nicholas Frosst; Geoffrey E Hinton; ; I Guyon; U V Luxburg; S Bengio; H Wallach; R Fergus; S Vishwanathan; R Garnett"}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "journal": "", "year": "2015", "authors": "Kai Sheng Tai; Richard Socher; Christopher D Manning"}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning", "journal": "", "year": "2012", "authors": "Tijmen Tieleman; Geoffrey Hinton"}, {"title": "Combination of convolutional and recurrent neural network for sentiment analysis of short texts", "journal": "", "year": "2016", "authors": "Xingyou Wang; Weijie Jiang; Zhiyong Luo"}, {"title": "Detecting hate speech on the world wide web", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "William Warner; Julia Hirschberg"}, {"title": "Ex machina: Personal attacks seen at scale", "journal": "", "year": "2017", "authors": "Ellery Wulczyn; Nithum Thain; Lucas Dixon"}, {"title": "Determining gains acquired from word embedding quantitatively using discrete distribution clustering", "journal": "", "year": "2017", "authors": "Jianbo Ye; Yanran Li; Zhaohui Wu; Z James; Wenjie Wang; Jia Li;  Li"}, {"title": "", "journal": "", "year": "", "authors": "Dawei Yin; Brian D Davison; Zhenzhen Xue; Liangjie Hong"}, {"title": "Comparative study of cnn and rnn for natural language processing", "journal": "", "year": "2017", "authors": "Wenpeng Yin; Katharina Kann; Mo Yu; Hinrich Sch\u00fctze"}, {"title": "Recurrent neural network regularization", "journal": "", "year": "2014", "authors": "Wojciech Zaremba; Ilya Sutskever; Oriol Vinyals"}, {"title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}, {"title": "Investigating capsule networks with dynamic routing for text classification", "journal": "", "year": "2018", "authors": "Wei Zhao; Jianbo Ye; Min Yang; Zeyang Lei; Suofei Zhang; Zhou Zhao"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "the focusing parameter which smoothens the rate at which easy examples are down weighted and, \u03b1 is the weight assigned to the rare class.", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: CapsNet with LSTMs as feature extractor", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 2", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Comparison of several deep learning approaches with Capsule Net on the three datasetsFocal Loss: To handle the class imbalance problem, we have used Focal Loss which is given by the following formula :", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Example of handling misspelt words and transliteration. NN : Nearest Neighbour", "figure_data": ""}], "doi": ""}