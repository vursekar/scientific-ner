{"authors": "Sopan Khosla; Justin Lovelace; Ritam Dutt; Adithya Pratapa", "pub_date": "", "title": "Team JARS: DialDoc Subtask 1 -Improved Knowledge Identification with Supervised Out-of-Domain Pretraining", "abstract": "In this paper, we discuss our submission for DialDoc subtask 1. The subtask requires systems to extract knowledge from FAQ-type documents vital to reply to a user's query in a conversational setting. We experiment with pretraining a BERT-based question-answering model on different QA datasets from MRQA, as well as conversational QA datasets like CoQA and QuAC. Our results show that models pretrained on CoQA and QuAC perform better than their counterparts that are pretrained on MRQA datasets. Our results also indicate that adding more pretraining data does not necessarily result in improved performance. Our final model, which is an ensemble of AlBERT-XL pretrained on CoQA and QuAC independently, with the chosen answer having the highest average probability score, achieves an F1-Score of 70.9% on the official test-set.", "sections": [{"heading": "Introduction", "text": "Question Answering (QA) involves constructing an answer for a given question in either an extractive or an abstractive manner. QA systems are central to other Natural Language Processing (NLP) applications like search engines, and dialogue. Recently, QA based solutions have also been proposed to evaluate factuality (Wang et al., 2020) and faithfulness (Durmus et al., 2020) of abstractive summarization systems.\nIn addition to popular QA benchmarks like SQuAD (Rajpurkar et al., 2016), and MRQA-2019 (Fisch et al., 2019), we have seen QA challenges that require reasoning over human dialogue. Some notable examples being QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019). These datasets require the model to attend to the entire dialogue context in the process of retrieving an answer. In this work, we are interesting in building a QA system to help with human dialogue. Feng et al. (2020) introduced a new dataset of goal-oriented dialogues (Doc2Dial) that are grounded in the associated documents. Each sample in the dataset consists of an information-seeking conversation between a user and an agent where agent's responses are grounded in FAQ-like webpages. DialDoc shared task derives its training data from the Doc2Dial dataset and proposes two subtasks which require the participants to (1) identify the grounding knowledge in form of document span for the next agent turn; and (2) generate the next agent response in natural language.\nIn this paper, we describe our solution to the subtask 1. This subtask is formulated as a span selection problem. Therefore, we leverage a transformerbased extractive question-answering model Lan et al., 2019) to extract the relevant spans from the document. We pretrain our model on different QA datasets like SQuAD, different subsets of MRQA-2019 training set, and conversational QA datasets like CoQA and QuAC. We find that models pretrained on out-of-domain QA datasets substantially outperform the baseline. Our experiments suggest that conversational QA datasets are more useful than MRQA-2019 data or its subsets. In the following sections, we first present an overview of the DialDoc shared task ( \u00a72), followed by our system description ( \u00a73) and a detailed account of our experimental results, and ablation studies ( \u00a74, \u00a75).", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "DialDoc Shared Task Dataset", "text": "Dataset used in the DialDoc shared-task is derived from Doc2Dial dataset (Feng et al., 2020), a new dataset with goal-oriented document-grounded dialogue. It includes a set of documents and conversations between a user and an agent grounded in the associated document. The authors provide annotations for dialogue acts for each utterance in the dialogue flow, along with the span in the document that acts as the reference of it.\nThe dataset shared during the shared task was divided into train/validation/testdev/test splits. Train and validation splits were provided to the participants to facilitate model development. During phase 1, the models were evaluated on testdev whereas, the final ranking was done on the performance on the test set.\nPre-processing Using the pre-processing scripts provided by the task organizers, we converted the Doc2Dial dataset into SQuAD v2.0 format with questions containing the latest user utterance as well as all previous turns in the conversation. This is in line with previous work from (Feng et al., 2020) which showed that including the entire conversational history performs better than just considering the current user utterance. Dialogue context is concatenated with the latest user utterance in the reverse time order.\nThe output of this pre-processing step consisted of 20431 training, 3972 validation, 727 testdev, and 2824 test instances.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "System Description", "text": "As discussed earlier, subtask 1 of DialDoc shared task is formulated as a span selection problem. Therefore, in order to learn to predict the correct span, we use an extractive question-answering setup.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Question-Answering Model", "text": "We pass the pre-processed training data through a QA model that leverages a transformer encoder to contextually represent the question (dialogue history) along with the context (document). Since the grounding document is often longer than the maximum input sequence length for transformers, we follow (Feng et al., 2020) and truncate the documents in sliding windows with a stride. The document trunk and the dialogue history are passed through the transformer encoder to create contextual representations for each token in the input. To extract the beginning and the ending positions of the answer span within the document, the encoded embeddings are sent to a linear layer to output two logits that correspond to the probability of the position being the start and end position of the answer span. The training loss is computed using the Cross-Entropy loss function. We use the huggingface transformers toolkit in all of our experiments.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Pretraining", "text": "Recent work (Gururangan et al., 2020) has shown that multi-phase domain adaptive pretraining of transformer-based encoders on related datasets (and tasks) benefits the overall performance of the model on the downstream task. Motivated by this, we experimented with further pretraining the QA model on different out-of-domain QA datasets to gauge its benefits on Doc2Dial (Table 1).  ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "In this section, we discuss our experimental setup in detail.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Pretraining Datasets", "text": "Firstly, we briefly describe the different datasets used for the continual pretraining of our transformer-based QA models.\nMRQA-19 Shared task (Fisch et al., 2019)   CoQA (Reddy et al., 2019). 2 For both datasets, we filter out samples which do not adhere to SQuADlike extractive QA setup (e.g. yes/no questions) or have a context length of more than 5000 characters. Table 1 presents the size of the different pretraining datasets after the removal of non-extractive QA samples.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Evaluation Metrics", "text": "The shared-task relies on Exact Match (EM) and F1 metrics to evaluate the systems on subtask 1. To compute these scores, we use the metrics for SQuAD from huggingface. 3", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Hyperparameters", "text": "We use default parameters set by the subtask baseline provided by the authors. 4 However, we reduce the training per-device batch-size to 2 to accommodate the large models on an Nvidia Geforce GTX 1080 Ti 12GB GPU. We stop the continual out-ofdomain supervised pretraining after 2 epochs.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Results", "text": "We now present the results for different experimental setups we tried for DialDoc subtask 1.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Pretraining on Different QA Datasets", "text": "Our first set of results portray the differential benefits of different out-of-domain QA datasets when used to pretrain the transformer encoder.  Experiments with bert-base-uncased on the validation set (Table 2) portray that pretraining on different QA datasets is indeed beneficial. Datasets like SQuAD, NewsQA, and NaturalQuestions are more useful than SearchQA, and Trivi-aQA. However, pretraining on complete MRQA-2019 training set does not outperform the individual datasets suggesting that merely introducing more pretraining data might not result in improved performance. Furthermore, conversational QA datasets like CoQA and QuAC, which are more similar in their setup to DialDoc, perform substantially better than any of the other MRQA-2019 training datasets.\nWe observe similar trends with larger transformers (Table 3). Models pretrained on QuAC or CoQA outperform those pretrained on SQuAD. However, combining CoQA and QuAC during pretraining does not seem to help with the performance on validation or testdev split.\nAnalyzing Different Transformer Variants Table 3 also contains the results for experiments where albert-xl is used to encode the questioncontext pair. We find that albert-xl-based models outperform their bert counterparts on validation set. However, they do not generalize well to the Testdev set, which contains about 30% of the test instances but is much smaller than validation set in size (727 samples in testdev vs 3972 in validation set).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results on test set", "text": "We only submitted our best performing models on the official test set due to a constraint on the number of submissions. Contrary to the trends for testdev phase, albert-xl models trained on conversational QA datasets perform the best. albert-xl + QuAC is the best-performing single model according to the EM metric (EM = 52.60), whereas albert-xl + CoQA performs the best on F1 metric (F 1 = 69.48) on the test set.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ensembling", "text": "We perform ensembling over the outputs of the model variants to obtain a single unified ranked list. For a given question Q, we produce 20 candidate spans, along with a corresponding probability score ps. We compute rank-scores rs for the answer-spans at rank r as rs = 1 log 2 (r+1) . We then aggregate the information of the answer spans for the model variants using the following techniques. Frequent: We chose the answer span which was the most frequent across the model variants. Rank Score : We chose the answer span which was the highest average rank score. Probability Score: We chose the answer span which was the highest average probability score.\nWe observe empirically that ensembling using the probability score performs the best and hence we report the results of ensembling using the probability score (E) in Table 3.\nWe observe the highest gains after ensembling the outputs of all the 5 model variants on the validation test and test-dev set. However, the best performance on the test set was achieved by ensembling over the albert-xl models pre-trained independently on CoQA and QuAC (EM = 53.5, F 1 = 70.9). This was the final submission for our team.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Informed Data Selection", "text": "We investigate the disparate impact of pretraining on different MRQA-19 datasets on the Doc2Dial shared task. Specifically, we explored factors such as answer length, relative position of the answer in the context, question length, and context length in Table 4. We observe that the SQuAD, NewsQA, and NaturalQuestions (NQ) has compartaively longer answers than the other datasets. However, we do not observe a noticeable difference in terms of question length, context length or relative position of the answer in the context, with respect to the other datasets.  We also use the dataset of Li and Roth (2002) to train a BERT classifier to predict answer type of a question with 97% accuracy. The coarse-answer types are DESC (Description), NUM (Numerical), ENT (Entity), HUM (Person), LOC (Location) and ABBR (Abbreviation). We use the classifier to gauge the distribution of answer types on MRQA datasets and Doc2Dial. We observe from Figure 2 that a majority of questions in Doc2Dial require a descriptive answer. These DESC type questions are more prevelant in SQuAD, NewsQA, and NQ, which might explain their efficacy.\nTo ascertain the benefit of intelligent sampling, we pretrain on a much smaller subset of the SQuAD, NewsQA, and NaturalQuestions dataset, which we obtain via intelligent sampling. We select questions which satisfy one of the following criteria, (i) the answer length of the question is \u2265 50, (ii) the question includes 'how' or 'why' question word or (iii) the answer type of the question is 'DESC'. Overall, the size of the selected sample is only 20% of the original dataset, yet achieves a higher EM score than the combined dataset as seen in Table 2. Yet, surprisingly, the performance is lower than each of the individual dataset.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "Our submission to the DialDoc subtask 1 performs continual pretraining of a transformer-based encoder on out-of-domain QA datasets. Experiments with different QA datasets suggest that conversational QA datasets like CoQA and QuAC are highly beneficial as their setup is substantially similar to Doc2Dial, the downstream dataset of interest. Our final submission ensembles two AlBERT-XL models independently pretrained on CoQA and QuAC and achieves an F1-Score of 70.9% and EM-Score of 53.5% on the competition test-set.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Impact Statement", "text": "In this work, we tackle the task of question answering (QA) for English language text. While we believe that the proposed methods can be effective in other languages, we leave this exploration for future work. We also acknowledge that QA systems suffer from bias (Li et al., 2020), which often lead to unintended real-world consequences. For the purpose of the shared task, we focused solely on the modeling techniques, but a study of model bias in our systems is necessary.", "n_publication_ref": 1, "n_figure_ref": 0}], "references": [{"title": "QuAC: Question answering in context", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Eunsol Choi; He He; Mohit Iyyer; Mark Yatskar; Wentau Yih; Yejin Choi; Percy Liang; Luke Zettlemoyer"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Searchqa: A new qa dataset augmented with context from a search engine", "journal": "ArXiv", "year": "2017", "authors": "Matthew Dunn; Levent Sagun; Mike Higgins; V U G\u00fcney; Volkan Cirik; Kyunghyun Cho"}, {"title": "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization", "journal": "", "year": "2020", "authors": "Esin Durmus; He He; Mona Diab"}, {"title": "2020. doc2dial: A goal-oriented document-grounded dialogue dataset", "journal": "", "year": "", "authors": "Song Feng; Hui Wan; Chulaka Gunasekara; Siva Patel; Sachindra Joshi; Luis Lastras"}, {"title": "MRQA 2019 shared task: Evaluating generalization in reading comprehension", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Adam Fisch; Alon Talmor; Robin Jia; Minjoon Seo; Eunsol Choi; Danqi Chen"}, {"title": "2020. Don't stop pretraining: Adapt language models to domains and tasks", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": "Ana Suchin Gururangan; Swabha Marasovi\u0107; Kyle Swayamdipta; Iz Lo; Doug Beltagy; Noah A Downey;  Smith"}, {"title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension", "journal": "Vancouver", "year": "2017", "authors": "Mandar Joshi; Eunsol Choi; Daniel Weld; Luke Zettlemoyer"}, {"title": "Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics", "journal": "", "year": "2019", "authors": "Tom Kwiatkowski; Jennimaria Palomaki; Olivia Redfield; Michael Collins; Ankur Parikh; Chris Alberti; Danielle Epstein; Illia Polosukhin; Matthew Kelcey; Jacob Devlin; Kenton Lee; Kristina N Toutanova; Llion Jones; Ming-Wei Chang; Andrew Dai; Jakob Uszkoreit; Quoc Le; Slav Petrov"}, {"title": "Albert: A lite bert for self-supervised learning of language representations", "journal": "", "year": "2019", "authors": "Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut"}, {"title": "UNQOVERing stereotyping biases via underspecified questions", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Tao Li; Daniel Khashabi; Tushar Khot"}, {"title": "Learning question classifiers", "journal": "", "year": "2002", "authors": "Xin Li; Dan Roth"}, {"title": "SQuAD: 100,000+ questions for machine comprehension of text", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"title": "CoQA: A conversational question answering challenge", "journal": "Transactions of the Association for Computational Linguistics", "year": "2019", "authors": "Siva Reddy; Danqi Chen; Christopher D Manning"}, {"title": "NewsQA: A machine comprehension dataset", "journal": "", "year": "2017", "authors": "Adam Trischler; Tong Wang; Xingdi Yuan; Justin Harris; Alessandro Sordoni; Philip Bachman; Kaheer Suleman"}, {"title": "Asking and answering questions to evaluate the factual consistency of summaries", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Alex Wang; Kyunghyun Cho; Mike Lewis"}, {"title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Zhilin Yang; Peng Qi; Saizheng Zhang; Yoshua Bengio; William Cohen; Ruslan Salakhutdinov; Christopher D Manning"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Distribution of Question Words for MRQA.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Distribution of Answer Types for MRQA.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Statistics (domain, # samples) for different QA datasets used for continual pre-training.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "focused on evaluating the generalizability of QA systems. They developed a training set that combined examples from 6 different QA datasets and developed evaluation splits using 12 other QA datasets. We explored the effectiveness of pretraining on the entire MRQA training set as well on each of the 6 training datasets: SQuAD(Rajpurkar et al., 2016), NewsQA(Trischler et al., 2017), Nat-uralQuestions(Kwiatkowski et al., 2019), Hot-potQA(Yang et al., 2018), SearchQA(Dunn et al., 2017), and TriviaQA (Joshi et al., 2017).", "figure_data": "QA DatasetValidationEM F1Doc2Dial42.1 57.8+ SQuAD45.0 60.3+ NewsQA45.5 59.8+ NaturalQuestions (NQ)44.2 59.9+ HotpotQA43.0 58.0+ SearchQA42.3 57.5+ TriviaQA43.1 58.0+ MRQA-19 (Train)43.4 58.9+ SQuAD + NewsQA + NQ43.0 59.2+ SQuAD + NewsQA + NQ (IS) 43.8 59.4+ QuAC46.4 60.3+ CoQA47.7 66.0"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": Performance (EM (%), F1 (%)) ofbert-base-uncased on DialDoc validation setwhen further pretrained on different QA datasets."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ": Performance (EM (%), F1 (%)) of largetransformer-based QA models on DialDoc validationand testdev set when further pretrained on different QA"}], "doi": "10.18653/v1/D18-1241"}