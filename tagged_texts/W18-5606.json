[{"text": "Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis ( LOUHI 2018 ) , pages 44\u201354 Brussels , Belgium , October 31 , 2018 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics44Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy Lina Rojas - Barahona1 , Bo - Hsiang Tseng1 , Yinpei Dai1 , Clare Mans\ufb01eld2 Osman Ramadan1 , Stefan Ultes1 ,", "entities": []}, {"text": "Michael Crawford3and Milica", "entities": []}, {"text": "Ga \u02c7si\u00b4c1 1University of Cambridge,2CM Insight,3Imperial College London mg436@cam.ac.uk", "entities": []}, {"text": "Abstract In recent years , we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks , such as similarity , entailment and sentiment analysis .", "entities": [[34, 36, "TaskName", "sentiment analysis"]]}, {"text": "Here we introduce a new task : understanding of mental health concepts derived from Cognitive Behavioural Therapy ( CBT ) .", "entities": [[18, 19, "DatasetName", "CBT"]]}, {"text": "We de\ufb01ne a mental health ontology based on the CBT principles , annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations .", "entities": [[5, 6, "MethodName", "ontology"], [9, 10, "DatasetName", "CBT"]]}, {"text": "Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings signi\ufb01cantly outperform non - deep - learning models in this dif\ufb01cult task .", "entities": [[12, 14, "TaskName", "word embeddings"], [15, 17, "TaskName", "sentence embeddings"]]}, {"text": "This understanding module will be an essential component of a statistical dialogue system delivering therapy .", "entities": []}, {"text": "1 Introduction Promotion of mental well - being is at the core of the action plan on mental health 2013\u20132020 of the World Health Organisation ( WHO ) ( World Health Organization , 2013 ) and of the European Pact on Mental Health and Well - being of the European Union ( EU high - level conference : Together for Mental Health and Well - being , 2008 ) .", "entities": []}, {"text": "The biggest potential breakthrough in \ufb01ghting mental illness would lie in \ufb01nding tools for early detection and preventive intervention ( Insel and Scholnick , 2006 ) .", "entities": []}, {"text": "The WHO action plan stresses the importance of health policies and programmes that not only meet the need of people affected by mental disorders but also protect mental well - being .", "entities": []}, {"text": "The emphasis is on early evidence - based non - pharmacological intervention , avoiding institutionalisation and medicalisation .", "entities": []}, {"text": "What is particularly important for successful intervention is the frequency with which the therapy can be accessed ( Hansen et al . , 2002 ) .", "entities": []}, {"text": "Thisgives automated systems a huge advantage over conventional therapies , as they can be used continuously with marginal extra cost .", "entities": []}, {"text": "Health assistants that can deliver therapy , have gained great interest in recent years ( Bickmore et al . , 2005 ;", "entities": []}, {"text": "Fitzpatrick et al . , 2017 ) .", "entities": []}, {"text": "These systems however are largely based on hand - crafted rules .", "entities": []}, {"text": "On the other hand , the main research effort in statistical approaches to conversational systems has focused on limited - domain information seeking dialogues ( Schatzmann et al . , 2006 ; Geist and Pietquin , 2011 ; Gasic and Young , 2014 ; Fatemi et al . , 2016 ; Li et al . , 2016 ; Williams et al . , 2017 ) .", "entities": []}, {"text": "In this paper we introduce a new task : understanding of mental health concepts derived from Cognitive Behavioural Therapy ( CBT ) .", "entities": [[20, 21, "DatasetName", "CBT"]]}, {"text": "We present an ontology that is formulated according to Cognitive Behavioural Therapy principles .", "entities": [[3, 4, "MethodName", "ontology"]]}, {"text": "We label a high quality mental health corpus , which exhibits targeted psychological phenomena .", "entities": []}, {"text": "We use the whole unlabelled dataset to train distributed representations of words and sentences .", "entities": []}, {"text": "We then investigate two approaches for classifying the user input according to the de\ufb01ned ontology .", "entities": [[14, 15, "MethodName", "ontology"]]}, {"text": "The \ufb01rst model involves a convolutional neural network ( CNN ) operating over distributed words representations .", "entities": []}, {"text": "The second involves a gated recurrent network ( GRU ) operating over distributed representation of sentences .", "entities": [[8, 9, "MethodName", "GRU"]]}, {"text": "Our models perform signi\ufb01cantly better than chance and for instances with a large number of data they reach the inter - annotator agreement .", "entities": []}, {"text": "This understanding module will be an essential component of a statistical dialogue system delivering therapy .", "entities": []}, {"text": "The paper is organised as follows .", "entities": []}, {"text": "In Section 2 we give a brief background of the statistical approach to dialogue modelling , focusing on dialogue ontology and natural language understanding .", "entities": [[19, 20, "MethodName", "ontology"], [21, 24, "TaskName", "natural language understanding"]]}, {"text": "In Section 3 we review related work in the area of automated mental - health assistants .", "entities": []}, {"text": "The sections that", "entities": []}, {"text": "45follow represent the main contribution of this work : a CBT ontology in Section 4 , a labelled dataset in Section 5 , and models for language understanding in Section 6 .", "entities": [[10, 11, "DatasetName", "CBT"], [11, 12, "MethodName", "ontology"]]}, {"text": "We present the results in Section 7 and our conclusion in Section 8 . 2 Background A dialogue system can be treated as a trainable statistical model suitable for goal - oriented information seeking dialogues ( Young , 2002 ) .", "entities": []}, {"text": "In these dialogues , the user has a clear goal that he or she is trying to achieve and this involves extracting particular information from a back - end database .", "entities": []}, {"text": "A structured representation of the database , the ontologyis a central element of a dialogue system .", "entities": []}, {"text": "It de\ufb01nes the concepts that the dialogue system can understand and talk about .", "entities": []}, {"text": "Another critical component is the natural language understanding unit , which takes textual user input and detects presence of the ontology concepts in the text .", "entities": [[5, 8, "TaskName", "natural language understanding"], [20, 21, "MethodName", "ontology"]]}, {"text": "2.1 Dialogue ontology Statistical approaches to dialogue modelling have been applied to relatively simple domains .", "entities": [[2, 3, "MethodName", "ontology"]]}, {"text": "These systems interface databases of up to 1000 entities where each entity has up to 20properties , i.e. slots ( Cuay \u00b4 ahuitl , 2009 ) .", "entities": []}, {"text": "There has been a significant amount of work in spoken language understanding focused on exploiting large knowledge graphs in order to improve coverage ( T \u00a8ur et", "entities": [[9, 12, "TaskName", "spoken language understanding"], [16, 18, "TaskName", "knowledge graphs"]]}, {"text": "al . , 2012 ; Heck et al . , 2013 ) .", "entities": []}, {"text": "Despite these efforts , little work has been done on mental health ontologies for supporting cognitive behavioural therapy on dialogue systems .", "entities": []}, {"text": "Available medical ontologies follow a symptom - treatment categorisation and are not suitable for dialogue or natural language understanding ( Bluhm , 2017 ; Hofmann , 2014 ; Wang et al . , 2018 ) .", "entities": [[16, 19, "TaskName", "natural language understanding"]]}, {"text": "2.2 Natural language understanding Within a dialogue system , a natural language understanding unit extracts meaning from user sentences .", "entities": [[1, 4, "TaskName", "Natural language understanding"], [10, 13, "TaskName", "natural language understanding"]]}, {"text": "Both classi\ufb01cation ( Mairesse et al . , 2009 ) and sequence - to - sequence ( Yao et al . , 2014 ; Mesnil et al . , 2015 ) models have been applied to address this task .", "entities": []}, {"text": "Deep learning architectures that exploit distributed word - vector representations have been successfully applied to different tasks in natural language understanding , such as semantic role labelling , semantic parsing , spoken language un - derstanding , sentiment analysis or dialogue belief tracking ( Collobert et al . , 2011 ; Kim , 2014 ; Kalchbrenner et al . , 2014 ; Le and Mikolov , 2014a ; Rojas Barahona et al . , 2016 ;", "entities": [[18, 21, "TaskName", "natural language understanding"], [28, 30, "TaskName", "semantic parsing"], [37, 39, "TaskName", "sentiment analysis"]]}, {"text": "Mrk \u02c7si\u00b4c et al . , 2017 ) .", "entities": []}, {"text": "In this work we consider understanding of mental health concepts of as a classi\ufb01cation task .", "entities": []}, {"text": "To facilitate this process , we use distributed representations .", "entities": []}, {"text": "3 Related work The aim of building an automated therapist has been around since the \ufb01rst time researchers attempted to build a dialogue system ( Weizenbaum , 1966 ) .", "entities": []}, {"text": "Automated health advice systems built to date typically rely on expert coded rules and have limited conversational capabilities ( RojasBarahona and Giorgino , 2009 ; Vardoulakis et al . , 2012 ; Ring et al . , 2013 ; Riccardi , 2014 ; DeVault et al . , 2014 ; Ring et al . , 2016 ) .", "entities": []}, {"text": "One particular system that we would like to highlight is an affectively aware virtual therapist ( Ring et al . , 2016 ) .", "entities": []}, {"text": "This system is based on Cognitive Behavioural Therapy and the system behaviour is scripted using V oiceXML .", "entities": []}, {"text": "There is no language understanding : the agent simply asks questions and the user selects answers from a given list .", "entities": [[7, 8, "DatasetName", "agent"]]}, {"text": "The agent is however able to interpret hand gestures , posture shifts , and facial expressions .", "entities": [[1, 2, "DatasetName", "agent"]]}, {"text": "Another notable system ( DeVault et al . , 2014 ) has a multi - modal perception unit which captures and analyses user behaviour for both behavioural understanding and interaction .", "entities": []}, {"text": "The measurements contribute to the indicator analysis of affect , gesture , emotion and engagement .", "entities": [[12, 13, "DatasetName", "emotion"]]}, {"text": "Again , no statistical language understanding takes place and the behaviour of the system is scripted .", "entities": []}, {"text": "The system does not provide therapy to the user but is rather a tool that can support healthcare decisions ( by human healthcare professionals ) .", "entities": []}, {"text": "The Stanford Woebot chat - bot proposed by ( Fitzpatrick et al . , 2017 ) is designed for delivering CBT to young adults with depression and anxiety .", "entities": [[20, 21, "DatasetName", "CBT"]]}, {"text": "It has been shown that the interaction with this chat - bot can signi\ufb01cantly reduce the symptoms of depression when compared to a group of people directed to a read a CBT manual .", "entities": [[31, 32, "DatasetName", "CBT"]]}, {"text": "The conversational agent appears to be effective in engaging the users .", "entities": [[2, 3, "DatasetName", "agent"]]}, {"text": "However , the understanding component of Woebot has not been fully described .", "entities": []}, {"text": "The dialogue decisions are based on decision trees .", "entities": []}, {"text": "At each node , the user is expected to choose one of several prede\ufb01ned responses .", "entities": []}, {"text": "Limited language understanding was in-", "entities": []}, {"text": "46troduced at speci\ufb01c points in the tree to determine routing to subsequent conversational nodes .", "entities": []}, {"text": "Still , one of the main de\ufb01ciencies reported by the trial participants in ( Fitzpatrick et al . , 2017 ) was the inability to converse naturally .", "entities": [[25, 26, "DatasetName", "converse"]]}, {"text": "Here we address this problem by performing statistical natural language understanding .", "entities": [[8, 11, "TaskName", "natural language understanding"]]}, {"text": "4 CBT ontology To de\ufb01ne the ontology we draw from principles of Cognitive Behavioural Therapy ( CBT ) .", "entities": [[1, 2, "DatasetName", "CBT"], [2, 3, "MethodName", "ontology"], [6, 7, "MethodName", "ontology"], [16, 17, "DatasetName", "CBT"]]}, {"text": "This is one of the best studied psychotherapeutic interventions , and the most widely used psychological treatment for mental disorders in Britain ( Bhasi et al . , 2013 ) .", "entities": []}, {"text": "There is evidence that CBT is more effective than other forms of psychotherapy ( Tolin , 2010 ) .", "entities": [[4, 5, "DatasetName", "CBT"]]}, {"text": "Unlike other , longer - term , forms of therapy such as psychoanalysis , CBT can have a positive effect on the client within a few sessions .", "entities": [[14, 15, "DatasetName", "CBT"]]}, {"text": "Also , due to it being highly structured , it is more easily amenable by computer interpretation .", "entities": []}, {"text": "This is why we adopted CBT as the basis of our work .", "entities": [[5, 6, "DatasetName", "CBT"]]}, {"text": "Cognitive Behavioural Therapy is derived from Cognitive Therapy model theory ( Beck , 1976 ; Beck et al . , 1979 ) , which postulates that our emotions and behaviour are in\ufb02uenced by the way we think and by how we make sense of the world .", "entities": []}, {"text": "The idea is that , if the client changes the way he or she thinks about their problem , this will in turn change the way he or she feels , and behaves .", "entities": []}, {"text": "A major underlying principle of CBT is the idea of cognitive distortions , and the value in challenging them .", "entities": [[5, 6, "DatasetName", "CBT"]]}, {"text": "In CBT , clients are helped to test their assumptions and views of the world in order to check if they \ufb01t with reality .", "entities": [[1, 2, "DatasetName", "CBT"]]}, {"text": "When clients learn that their perceptions and interpretations are distorted or unhelpful they then work on correcting them .", "entities": []}, {"text": "Within the realm of cognitive distortion , CBT identi\ufb01es a number of speci\ufb01c self - defeating thought processes , or thinking errors .", "entities": [[7, 8, "DatasetName", "CBT"]]}, {"text": "There is a core of around 10 to 15 thinking errors , with their exact titles having some \ufb02uidity .", "entities": []}, {"text": "A strong component of CBT is teaching clients to be able to recognize and identify the thinking errors themselves , and ultimately discard the negative thought processes and \u2018 re - think \u2019 their problems .", "entities": [[4, 5, "DatasetName", "CBT"]]}, {"text": "We consider the main analytical step in this therapy : an adequate decoding of these \u2018 thinking error \u2019 concepts , and the identi\ufb01cation of the key emotion(s ) and the situational context of a particular problem .", "entities": []}, {"text": "Therefore , our ontology consists of think - ing errors , emotions , and situations .", "entities": [[3, 4, "MethodName", "ontology"]]}, {"text": "4.1 Thinking errors Notwithstanding slight variations in number and terminology , the list of thinking errors is fairly well standardised in the CBT literature .", "entities": [[22, 23, "DatasetName", "CBT"]]}, {"text": "We present one such list in Table 1 .", "entities": []}, {"text": "However , it is important to note that there is a fair degree of overlap between differentthinking errors , for example , between Jumping to Negative Conclusions andFortune Telling , or between Disqualifying the Positives andMental Filtering .", "entities": []}, {"text": "In addition , within the data used \u2013 and as is likely to be the case in any data of spontaneous expressions of psychological upset \u2013 a single problem can exhibit several thinking errors simultaneously .", "entities": []}, {"text": "Thus , the situation is much more challenging than in simple information - seeking dialogues , where ontologies are typically clearly de\ufb01ned and there is no or very little overlap between concepts .", "entities": []}, {"text": "4.2 Emotions In addition to thinking errors , we de\ufb01ne a set of emotions .", "entities": []}, {"text": "We mainly focus on negative emotions , relevant to people in psychological distress .", "entities": []}, {"text": "In CBT , emotions tend to be divided into positive and negative , or helpful / healthy and unhelpful/ unhealthy emotions ( Branch and Willson , 2010 ) .", "entities": [[1, 2, "DatasetName", "CBT"]]}, {"text": "The set of emotions for this work evolved over time in the early days of annotation .", "entities": []}, {"text": "Although we initally agreed to focus on \u2018 unhealthy \u2019 emotions , as de\ufb01ned by CBT , there seemed also to be a place for the \u2018 healthy \u2019 emotion Grief / sadness .", "entities": [[15, 16, "DatasetName", "CBT"], [29, 30, "DatasetName", "emotion"]]}, {"text": "Overall , the list of emotions used was drawn from a number of sources , including CBT literature , the annotators \u2019 own knowledge of what they work with in psychological therapy , and the common emotions that were seen emerging from the data early on in the process .", "entities": [[16, 17, "DatasetName", "CBT"]]}, {"text": "Note that more than one emotion might be expressed within an individual problem \u2013 for example Depression andLoneliness .", "entities": [[5, 6, "DatasetName", "emotion"]]}, {"text": "The list ofemotions is given in Table 2 . 4.3 Situations While our main emphasis was on thinking errors andemotions , we also de\ufb01ned a small set of situations .", "entities": []}, {"text": "The list of situations again evolved during the early days of annotation , with a longer original list being reduced down , for simplicity .", "entities": []}, {"text": "Again , it is possible for more than one situation ( for exampleWork andRelationships ) to apply to a single problem .", "entities": []}, {"text": "The considered situations are given in Table 3 .", "entities": []}, {"text": "47Thinking Error Frequency Exhibited by ...", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "Black and white ( or all or nothing ) thinking 20:82%Only seeing things in absolutes No shades of grey Blaming 8:05%Holding others responsible for your pain Not seeking to understand your own responsibility in situation Catastrophising 11:87%Magnifying a ( sometimes minor ) negative event into potential disaster Comparing 3:27 % Making dissatis\ufb01ed comparison of self versus others Disqualifying the positive 6:15%Dismissing / discounting positive aspects of a situation or experience Emotional reasoning 13:31 %", "entities": []}, {"text": "Assuming feelings represent fact .", "entities": []}, {"text": "Fortune telling 25:70 % Predicting how things will be , unduly negatively Jumping to negative conclusions 44:16%Anticipating something will turn out badly , with little evidence to support it Labelling 10:51%Using negative , sometimes highly coloured , language to describe self or other Ignoring complexity of people Low frustration tolerance \u201d I ca n\u2019t bear it\u201d16:03%Assuming something is intolerable , rather than dif\ufb01cult to tolerate or a temporary discomfort In\ufb02exibility \u201d should / need / ought\u201d8:08%Having rigid beliefs about how things or people must or ought to be Mental \ufb01ltering 5:50%Focusing on the negative Filtering out all positive aspects of a situation Mind - reading 14:60%Assuming others think negative things or have negative motives and intentions Over - generalising 12:69%Generalising negatively , using words like always , nobody , never , etc Personalising 5:85%Interpreting events as being related to you personally and overlooking other factors Table 1 : Taxonomy for thinking errors and how they are exhibited .", "entities": []}, {"text": "Emotion Frequency Exhibited by ...", "entities": []}, {"text": "Anger ( /frustration ) 14:76%Feelings of frustration , annoyance , irritation , resentment , fury , outrage Anxiety 63:12 % Any expression of fear , worry or anxiety Depression 20:72%Feeling down , hopeless , joyless , negative about self and/or life in general Grief / sadness 5:70%Feeling sad , upset , bereft in relation to a major loss Guilt 3:37%Feeling blameworthy for a wrongdoing or something not done Hurt 19:88 %", "entities": [[57, 58, "MetricName", "loss"]]}, {"text": "Feeling wounded and/or badly treated Jealousy 3:12%Antagonistic feeling towards another either wish to be like or to have what they have Loneliness 7:41%Feeling of alone - ness , isolation , friendlessness , not understood by anyone Shame 5:68%Feeling distress , humiliation , disgrace in relation to own behaviour or feelings Table 2 : Taxonomy for emotions and how they are exhibited .", "entities": []}, {"text": "Situation Frequency Bereavement 2:65 % Existential 21:93 % Health 10:61 % Relationships 67:58 % School / College 8:28 % Work 6:10 % Other 5:53 % Table 3 : Taxonomy for situations .", "entities": []}, {"text": "5", "entities": []}, {"text": "The corpus The corpus consists of 500 K written posts that users anonymously posted on the Koko platform1 .", "entities": []}, {"text": "This platform is based on the peer - to - peer therapy proposed by ( Morris et al . , 2015 ) .", "entities": []}, {"text": "In this set - up , a user anonymously posts their problem ( referred to 1https://itskoko.com/as the problem ) and is prompted to consider their most negative take on the problem ( referred to as thenegative take ) .", "entities": []}, {"text": "Subsequently , peers post responses that attempt to offer a re - think and give a more positive angle on the problem .", "entities": []}, {"text": "When \ufb01rst developed , this peer - to - peer framework was shown to be more ef\ufb01cacious than expressive writing , an intervention that is known to improve physical and", "entities": []}, {"text": "48 thinking errorsjumping to negative conclusionsdisqualifying the positiveemotionsanxietyshameProblem : I agreed to go on a last - minute business trip to Seoul .", "entities": []}, {"text": "Right now I 'm overweight and feel gross .", "entities": []}, {"text": "We 're staying in a really fancy area , and I 'm afraid people will think I 'm fat and disgusting .", "entities": []}, {"text": "Negative take : I \u2019m afraid I will be the grossest , ugliest person there.situationsworkhealthFigure 1 : An example of an annotated Koko post .", "entities": []}, {"text": "emotional well - being ( Morris et al . , 2015 ) .", "entities": []}, {"text": "Since then , the app developed by Koko has collected a very large number of posts and associated responses .", "entities": []}, {"text": "Initially , any \ufb01rst - time Koko user would be given a short introductory tutorial in the art of \u2018 re - thinking\u2019/\u2018re - framing \u2019 problems ( based on CBT principles ) , before being able to use the platform .", "entities": [[30, 31, "DatasetName", "CBT"]]}, {"text": "This however changed over time , as the age of the users decreased , and a different tutorial , emphasizing empathy and optimism , was used ( less CBT - based than the \u2018 re - thinking \u2019 ) .", "entities": [[28, 29, "DatasetName", "CBT"]]}, {"text": "Most of the data annotated in this study was drawn from the earlier phase .", "entities": []}, {"text": "Figure 1 gives an annotated post example .", "entities": []}, {"text": "5.1 Annotation A subset of posts was annotated by two psychological therapists using a web annotation tool that we developed .", "entities": []}, {"text": "The annotation tool allowed annotators to have a quick view of the posts , showing up to 50 posts per page , to navigate through posts , to check pending posts and to annotate them by adding or removing thinking errors , emotions andsituations .", "entities": []}, {"text": "All annotations were stored in a MySQL database .", "entities": []}, {"text": "Initially 1000 posts were analysed .", "entities": []}, {"text": "These were used to de\ufb01ne the ontology .", "entities": [[6, 7, "MethodName", "ontology"]]}, {"text": "Then 4035 posts were labelled with thinking errors , emotions andsituations .", "entities": []}, {"text": "It takes an experienced psychological therapist about one minute to annotate one post .", "entities": []}, {"text": "Note that the same post can exhibit multiple thinking errors , emotions andsituations , which makes the whole process more complex .", "entities": []}, {"text": "We randomly selected 50 posts and calculated the inter - annotator agreement .", "entities": []}, {"text": "The inter - annotator agreement was calculated using a contingency table for thinking error , emotion and situation , showing agreement and disagreement between the two annotators .", "entities": [[15, 16, "DatasetName", "emotion"]]}, {"text": "Then , Cohen \u2019s kappa was calculated discounting the possibility that the agreement may happen by chance .", "entities": []}, {"text": "The result is shown in Table 4 .", "entities": []}, {"text": "The main reason for the low agreement in thinking errors ( 61 % ) isConcept Thinking error Situation Emotion Kappa 0:61\u00060:09 0 : 92\u00060:08 0 : 90\u00060:07 Table 4 : Cohen \u2019s kappa with a 95 % con\ufb01dence interval due to the unbounded number of thinking errors per post .", "entities": [[21, 22, "DatasetName", "0"], [24, 25, "DatasetName", "0"]]}, {"text": "In other words , the annotators typically have three or four thinking errors in common but one of them might have detected one or two more .", "entities": []}, {"text": "Still , the agreement is much higher than chance , so we think that while challenging , it is possible to build a classi\ufb01er for this task .", "entities": []}, {"text": "The distributions of labelled posts with multiple sub - categories for three super - categories are shown in Figure 2 Figure 2 : Distribution of posts for each category .", "entities": []}, {"text": "6 Deep learning model 6.1 Distributed representations The task of decoding thinking errors andemotions is closely related to the task of sentiment analysis .", "entities": [[21, 23, "TaskName", "sentiment analysis"]]}, {"text": "In sentiment analysis we are concerned with positive or negative sentiment expressed in a sentence .", "entities": [[1, 3, "TaskName", "sentiment analysis"]]}, {"text": "Detecting thinking errors or emotions could be perceived as detecting different kinds of negative sentiment .", "entities": []}, {"text": "Distributed representations of words , sentences and documents have gained success in sentiment detection and similarity tasks ( Le and Mikolov , 2014a ; Maas et", "entities": []}, {"text": "al . , 2011 ; Kiros et al . , 2015 )", "entities": []}, {"text": ".", "entities": []}, {"text": "A key advantage of these representations is that they can be obtained in an unsupervised manner , thus allowing exploitation of large amounts of unlabelled data .", "entities": []}, {"text": "This is precisely what we have in our set - up , where only a small portion of our posts is labelled .", "entities": []}, {"text": "We utilise GloVe ( Pennington et al . , 2014 ) word vectors , which have previously achieved competitive results in a similarity task .", "entities": [[2, 3, "MethodName", "GloVe"]]}, {"text": "We train the word vectors on the whole dataset and then use a convolutional neural network ( CNN ) to extract features from posts where words are represented as vectors .", "entities": []}, {"text": "We also consider distributed representation of sentences .", "entities": []}, {"text": "A particularly competitive model is the skip - thought model , which is obtained from an encoder - decoder model that tries to reconstruct the surrounding sentences of an encoded passage", "entities": []}, {"text": "49(Kiros et al . , 2015 ) .", "entities": []}, {"text": "On similarity tasks it outperfoms the simpler doc2vec model ( Le and Mikolov , 2014a ) .", "entities": []}, {"text": "An approach that represents vectors by weighted averages of word vectors and then modi\ufb01es them using PCA and SVD outperforms skipthought vectors ( Arora et al . , 2017 ) .", "entities": [[16, 17, "MethodName", "PCA"], [18, 19, "DatasetName", "SVD"]]}, {"text": "This method however does not do well on a sentiment analysis task due to down - weighting of words like \u201c not \u201d .", "entities": [[9, 11, "TaskName", "sentiment analysis"]]}, {"text": "As these often appear in our corpus , we chose skipthought vectors for investigation here .", "entities": []}, {"text": "The skip - thought model allows a dense representation of the utterance .", "entities": []}, {"text": "We train skip - thought vectors using the method described in ( Kiros et al . , 2015 ) .", "entities": []}, {"text": "The automatically generated post shown in Fig 3 demonstrates that skip - thought vectors can convey the sentiment well in accordance to context .", "entities": []}, {"text": "We then train a gated recurrent unit ( GRU ) network using the skip - thoughts as input .", "entities": [[4, 7, "MethodName", "gated recurrent unit"], [8, 9, "MethodName", "GRU"]]}, {"text": "i 'm so depressed .", "entities": []}, {"text": "i 'm worthless .", "entities": []}, {"text": "No one likes me   i 'm try being nice but .", "entities": []}, {"text": "No light at every point i 'm   unpopular and i 'm a < NUM > year old potato .", "entities": []}, {"text": "my   most negative take is that i 'll never know how to be   as socially as a quiet girl .", "entities": []}, {"text": "i will stop talking to how   fragile is and be any ways of normal people .", "entities": []}, {"text": "Figure 3 : An example of a generated post using skipthought vectors initialised with \u201d I \u2019m so depressed \u201d .", "entities": []}, {"text": "6.2 Convolutional neural network model", "entities": []}, {"text": "The convolutional neural network ( CNN ) model is preferred over a recurrent neural network ( RNN ) model , because the posts are generally too long for an RNN to maintain memory over words .", "entities": []}, {"text": "The convolutional neural network ( CNN ) used in this work is inspired by ( Kim , 2014 ) and operates over pre - trained GloVe embeddings of dimensionality d.", "entities": [[25, 27, "MethodName", "GloVe embeddings"]]}, {"text": "As shown in Fig 4 , the network has two inputs , one for the problem and the other for the negative take .", "entities": []}, {"text": "These are represented as two tensors .", "entities": []}, {"text": "A convolutional operation involves a \ufb01lter w2Rldwhich is applied to lwords to produce the feature map .", "entities": []}, {"text": "Then , a max - pooling operation is applied to produce two vectors : pforproblem andnfornegative take .", "entities": []}, {"text": "The reason for this is that the negative take is usually a summary of the post , carrying stronger sentiment ( see Figure 1 ) .", "entities": []}, {"text": "We use a gating mechanism to combine pandnas follows : g=\u001b(Wpp+Wnn+b ) ( 1 ) h = g \f p+ ( 1\u0000g ) \f n ( 2 ) Here,\u001bis the sigmoid function , Wp , WnandW are weight matrices , bis a bias term , 1is a vector of ones , \f is the element - wise product , and gisthe output of the gating mechanism .", "entities": []}, {"text": "The extracted feature his then processed with a one - layer fullyconnected neural network ( FNN ) to perform binary classi\ufb01cation .", "entities": []}, {"text": "The model is illustrated in Fig 4 . problem negative takep nhFNNIlikeagirl ....... brokeupdenotes gating mechanism Figure 4 : CNN with gating mechanism .", "entities": []}, {"text": "6.3 Gated recurrent unit model We use the gated recurrent unit ( GRU ) model to process skip - thought sentence vectors , for two reasons .", "entities": [[1, 4, "MethodName", "Gated recurrent unit"], [8, 11, "MethodName", "gated recurrent unit"], [12, 13, "MethodName", "GRU"]]}, {"text": "First , most posts contain less than 5 sentences , so a recurrent neural network is more suitable than a convolutional neural network .", "entities": []}, {"text": "Second , since our corpus only comprises very limited labelled data , a GRU should perform better than a long short - term memory ( LSTM ) network as it has less parameters .", "entities": [[13, 14, "MethodName", "GRU"], [19, 24, "MethodName", "long short - term memory"], [25, 26, "MethodName", "LSTM"]]}, {"text": "Denote each post as P = fs1;s2;:::;s t;:::g , wherestis thetthsentence in post P. First , we use an already trained GRU to extract skip - thought embeddings etfrom the sentences st .", "entities": [[21, 22, "MethodName", "GRU"]]}, {"text": "Then , taking the sequence of sentence vectors fe1;e2;:::;et;:::gas input , another GRU is used as follows : zt=\u001b(Wzht\u00001+Uzet+bz ) ( 3 ) rt=\u001b(Wrht\u00001+Uret+br ) ( 4 ) ~ht= tanh ( W(rt \f ht\u00001 ) + Uet+bh)(5 )", "entities": [[12, 13, "MethodName", "GRU"]]}, {"text": "ht = zt \f ht\u00001 + ( 1\u0000zt ) \f ~ht ( 6 ) Wz;Uz;Wr;Ur;W;Uare recurrent weight matrices , bz;br;bhare bias terms , \f is the elementwise dot product , and \u001bis the sigmoid function .", "entities": []}, {"text": "Finally , the last hidden state hTis fed into a FNN with one hidden layer of the same size as input .", "entities": []}, {"text": "The model is illustrated in Fig 5 .", "entities": []}, {"text": "Figure 5 : GRU with skip - thought vectors .", "entities": [[3, 4, "MethodName", "GRU"]]}, {"text": "506.4 Training set - up We \ufb01rst train 100and300dimensions for both GloVe embeddings and skip - thought embeddings using the same mechanism as in ( Pennington et al . , 2014 ; Kiros et al . , 2015 ) .", "entities": [[11, 13, "MethodName", "GloVe embeddings"]]}, {"text": "In some posts the length of sentences is very large , so we bound the length at50 words .", "entities": []}, {"text": "We do not treat the problem separately from the negative take as the GRU will anyway put more importance on the information that comes last .", "entities": [[13, 14, "MethodName", "GRU"]]}, {"text": "We split the labelled data in a 8 : 1 : 1 ratio for training , validation and testing in a 10 - fold cross validation for both GRU and CNN training .", "entities": [[28, 29, "MethodName", "GRU"]]}, {"text": "A distinct network is trained for each concept , i. e. one for thinking errors , one for emotions and one forsituations .", "entities": []}, {"text": "The hidden size of the FNN is 150 .", "entities": []}, {"text": "To tackle the data bias problem , we utilise oversampling .", "entities": []}, {"text": "Different ratios ( 1:1 , 1:3 , 1:5 , 1:7 ) of positive and negative samples are explored .", "entities": []}, {"text": "We used \ufb01lter windows of 2,3 , and 4with 50 feature maps for the CNN model .", "entities": []}, {"text": "For the GRU model , the hidden size is set at 150 , so that both models have comparable number of parameters .", "entities": [[2, 3, "MethodName", "GRU"], [19, 22, "HyperparameterName", "number of parameters"]]}, {"text": "Mini - batches of size 24are used and gradients are clipped with maximum norm 5 .", "entities": []}, {"text": "We initialise the learning rate as 0:001with a decay rate of 0:986 every 10 steps .", "entities": [[3, 5, "HyperparameterName", "learning rate"], [8, 10, "HyperparameterName", "decay rate"]]}, {"text": "The non - recurrent weights with a truncated normal distribution ( 0;0:01 ) , and the recurrent weights with orthogonal initialisation ( Saxe et al . , 2013 ) .", "entities": []}, {"text": "To overcome over-\ufb01tting , we employ dropout with rate 0:8andl2 - normalisation .", "entities": []}, {"text": "Both models were trained with Adam algorithm and implemented in Tensor\ufb02ow ( Girija , 2016 ) .", "entities": [[5, 6, "MethodName", "Adam"]]}, {"text": "7 Results 7.1 Baselines For rule - based models , we chose a chance classi\ufb01er and a majority classi\ufb01er , where all the posts are treated as positive examples for each class .", "entities": []}, {"text": "In addition , we trained two non - deep - learning models , the logistic regression ( LR ) model and the Support Vector Machine ( SVM ) .", "entities": [[14, 16, "MethodName", "logistic regression"], [22, 25, "MethodName", "Support Vector Machine"], [26, 27, "MethodName", "SVM"]]}, {"text": "Both of them take the bag - of - words feature as input and implemented in sklearn ( Pedregosa et al . , 2011 ) .", "entities": []}, {"text": "For completeness , we also trained 100 and 300 dimensions PV - DM document embeddings ( Le and Mikolov , 2014b ) as the distributed representations of the posts using thegensim toolkit ( \u02c7Reh\u02dau\u02c7rek and Sojka , 2010 ) , and employ FNNs to do the classi\ufb01cation , the hidden size is set as 800 to ensure parameters of all deep learning models comparable .", "entities": []}, {"text": "All the baseline models are trained with the same set - up as described insection 6.4 .", "entities": []}, {"text": "7.2 Analysis Table 5 gives the average F1 scores and the average F1 scores weighted with the frequency of CBT labels for all models under the oversampling ratio 1:1 .", "entities": [[6, 8, "MetricName", "average F1"], [11, 13, "MetricName", "average F1"], [19, 20, "DatasetName", "CBT"]]}, {"text": "It shows that GloVe word vectors with CNN achieves the best performance both in 100 and 300 dimensions .", "entities": [[3, 4, "MethodName", "GloVe"]]}, {"text": "Model A VG .", "entities": []}, {"text": "F1 Weighted A VG F1", "entities": [[0, 2, "MetricName", "F1 Weighted"], [4, 5, "MetricName", "F1"]]}, {"text": "Chance 0.203\u00060.008 0.337\u00060.008 Majority 0.24\u00060.000 0.432\u00060.000 LR - BOW 0.330\u00060.011 0.479\u00060.008 SVM - BOW 0.403\u00060.000 0.536\u00060.000 FNN - DocVec-100d 0.339\u00060.006 0.502\u00060.005", "entities": [[11, 12, "MethodName", "SVM"]]}, {"text": "FNN - DocVec-300d 0.349\u00060.007 0.508\u00060.005 GRU - SkipThought-100d", "entities": [[5, 6, "MethodName", "GRU"]]}, {"text": "0.401\u00060.005 0.558\u00060.004 GRU - SkipThought-300d 0.423\u00060.005 0.570\u00060.004 CNN - GloVe-100d", "entities": [[2, 3, "MethodName", "GRU"]]}, {"text": "0:443\u00060:007 0.576\u00060.005", "entities": []}, {"text": "CNN - GloVe-300d 0:442\u00060:007 0:578\u00060:006 Table 5 : F1 scores for all models with 1:1 oversampling Table 6 shows the F1 - measure of the compared models that detect thinking errors , emotions and situations under the 1 : 1 oversampling ratio .", "entities": [[8, 9, "MetricName", "F1"], [20, 21, "MetricName", "F1"]]}, {"text": "We only include the results of the best performing models , SVMs , CNNs and GRUs , due to limited space .", "entities": []}, {"text": "The results show that both models outperform SVM - BOW in larger embedding dimensions .", "entities": [[7, 8, "MethodName", "SVM"]]}, {"text": "Although SVM - BOW is comparable to 100 dimensional GRU - Skip - thought in terms on average F1 , in all other cases CNN - GloVe and GRU - Skipthought overshadow SVM - BOW .", "entities": [[1, 2, "MethodName", "SVM"], [9, 10, "MethodName", "GRU"], [17, 19, "MetricName", "average F1"], [26, 27, "MethodName", "GloVe"], [28, 29, "MethodName", "GRU"], [32, 33, "MethodName", "SVM"]]}, {"text": "We also \ufb01nd that CNN - GloVe on average works better than GRUSkip - thought , which is expected as the space of words is smaller in comparison to the space of sentences so the word vectors can be more accurately trained .", "entities": [[6, 7, "MethodName", "GloVe"]]}, {"text": "While the CNN operating on 100 dimensional word vectors is comparable to the CNN operating on 300 dimensional word vectors , the GRU - Skip - thought tends to be worse on 100 dimensional skip - thoughts , suggesting that sentence vectors generally need to be of a higher dimension to represent the meaning more accurately than word vectors .", "entities": [[22, 23, "MethodName", "GRU"]]}, {"text": "Table 7 shows a more detailed analysis of the 300 dimensional CNN - GloVe performance , where both precision and recall are presented , indicating that oversampling mechanism can help overcome the data bias problem .", "entities": [[13, 14, "MethodName", "GloVe"]]}, {"text": "To illustrate the capabilities of this model , we give samples of two posts and their predicted and true labels in Figure 6 , which shows that our model discerns the classes reasonably well even in some dif\ufb01cult cases .", "entities": []}, {"text": "51Freq . SVM - BOW100d 300d Num . CNN - Glove GRU - Skip - thought CNN - Glove GRU - Skip - thought Emotion Anxiety 2547 0.798 \u00060.000 0.805\u00060.003 0.805\u00060.002 0.805\u00060.006 0:816\u00060:002", "entities": [[2, 3, "MethodName", "SVM"], [11, 12, "MethodName", "GRU"], [19, 20, "MethodName", "GRU"]]}, {"text": "Depression 836 0.564 \u00060.000 0.605\u00060.003 0.568\u00060.001 0:611\u00060:008 0.578\u00060.005", "entities": []}, {"text": "Hurt 802 0.448 \u00060.000 0.505\u00060.007 0.483\u00060.003 0:506\u00060:005 0.496\u00060.006 Anger", "entities": []}, {"text": "595 0.375 \u00060.001 0.389\u00060.009 0.384\u00060.007 0.383\u00060.004 0:425\u00060:007", "entities": []}, {"text": "Loneliness 299 0.558 \u00060.000 0.495\u00060.008 0.445\u00060.007 0:549\u00060:009 0.457\u00060.005", "entities": []}, {"text": "Grief 230 0.433 \u00060.005", "entities": []}, {"text": "0.462\u00060.010", "entities": []}, {"text": "0.373\u00060.008 0:462\u00060:008", "entities": []}, {"text": "0.382\u00060.005", "entities": []}, {"text": "Shame 229 0.220 \u00060.000 0:304\u00060:011 0.243\u00060.004 0.277\u00060.007 0.254\u00060.004 Jealousy 126 0.217 \u00060.000 0:228\u00060:012 0.159\u00060.004 0.216\u00060.005 0.216\u00060.009", "entities": []}, {"text": "Guilt 136 0.252 \u00060.000 0:295\u00060:012 0.186\u00060.007 0.279\u00060.014", "entities": []}, {"text": "0.225\u00060.008 A VG .", "entities": []}, {"text": "F1 score for Emotion 0.429\u00060.001 0:454\u00060:008 0.405\u00060.005 0:454\u00060:007 0.428\u00060.006", "entities": [[0, 2, "MetricName", "F1 score"]]}, {"text": "Situation Relationships 2727 0.861 \u00060.000 0.871\u00060.003", "entities": []}, {"text": "0.886\u00060.001 0.878\u00060.006 0:889\u00060:003", "entities": []}, {"text": "Existential 885 0.556 \u00060.000 0.591\u00060.002 0:600\u00060:005 0.594\u00060.007 0.599\u00060.006 Health 428 0.476 \u00060.000 0:589\u00060:003 0.555\u00060.005 0.585\u00060.008 0.587\u00060.006 School College 334 0.633 \u00060.000 0.670\u00060.004 0.641\u00060.003 0.673\u00060.009 0:680\u00060:002", "entities": []}, {"text": "Other 223 0.196 \u00060.001 0.255\u00060.011 0.241\u00060.008 0.256\u00060.005 0:281\u00060:006", "entities": []}, {"text": "Work 246 0.651 \u00060.000 0:663\u00060:004 0.572\u00060.006 0.661\u00060.011 0.639\u00060.006", "entities": []}, {"text": "Bereavement 107 0.602 \u00060.000 0.637\u00060.021 0.402\u00060.024 0:639\u00060:021 0.493\u00060.011", "entities": []}, {"text": "A VG .", "entities": []}, {"text": "F1 score for Situation 0.568\u00060.000 0.611\u00060.007 0.557\u00060.007", "entities": [[0, 2, "MetricName", "F1 score"]]}, {"text": "0:612\u00060:010 0.595\u00060.006", "entities": []}, {"text": "Thinking Error Jumping tonegative conclusions 1782 0.590 \u00060.000 0.696\u00060.004 0.685\u00060.004 0:703\u00060:005", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "0.687\u00060.002 Fortune telling 1037 0.458 \u00060.000 0:595\u00060:002", "entities": []}, {"text": "0.558\u00060.004 0.585\u00060.006 0.564\u00060.005", "entities": []}, {"text": "Black andwhite 840 0.395 \u00060.000 0.431\u00060.002 0.437\u00060.004 0.432\u00060.003 0:441\u00060:003 Low frustration tolerance 647 0.318 \u00060.000 0.322\u00060.007 0.330\u00060.003 0.313\u00060.005 0:336\u00060:001", "entities": []}, {"text": "Catastrophising 479 0.352 \u00060.000 0:375\u00060:002", "entities": []}, {"text": "0.358\u00060.005 0.371\u00060.004 0.364\u00060.003", "entities": []}, {"text": "Mind - reading 589 0.360 \u00060.000 0.404\u00060.005 0.353\u00060.011 0:419\u00060:006 0.356\u00060.007", "entities": []}, {"text": "Labelling 424 0.399 \u00060.001 0.453\u00060.007 0.335\u00060.004 0:462\u00060:004 0.373\u00060.002 Emotional reasoning 537 0.290 \u00060.000 0:319\u00060:007", "entities": []}, {"text": "0.285\u00060.005 0.306\u00060.006 0.293\u00060.008 Over - generalising 512 0.405 \u00060.001 0.405\u00060.006 0.375\u00060.004 0:418\u00060:008 0.389\u00060.004 In\ufb02exibility 326 0.202 \u00060.001 0.203\u00060.014 0.188\u00060.007 0:218\u00060:003", "entities": []}, {"text": "0.208\u00060.005 Blaming 325 0.209 \u00060.001 0:304\u00060:007", "entities": []}, {"text": "0.264\u00060.002 0.277\u00060.003 0.274\u00060.004 Disqualifying thepositive 248 0.146 \u00060.000 0.194\u00060.007 0.176\u00060.005 0.187\u00060.003 0:195\u00060:005", "entities": []}, {"text": "Mental \ufb01ltering 222 0.088 \u00060.000 0.142\u00060.007 0.150\u00060.001 0.141\u00060.002 0:155\u00060:003", "entities": []}, {"text": "Personalising 236 0.212 \u00060.000 0.230\u00060.012 0.220\u00060.005 0:236\u00060:004 0.221\u00060.005", "entities": []}, {"text": "Comparing 132 0.242 \u00060.000 0:289\u00060:014 0.177\u00060.008 0.255\u00060.009 0.227\u00060.007", "entities": []}, {"text": "A VG .", "entities": []}, {"text": "F1 score for Thinking Error 0.311\u00060.000 0:358\u00060:007 0.326\u00060.005 0.355\u00060.0050", "entities": [[0, 2, "MetricName", "F1 score"], [4, 5, "MetricName", "Error"]]}, {"text": "0.339 \u00060.004 A VG .", "entities": []}, {"text": "F1 score 0.403 \u00060.000 0:443\u00060:007", "entities": [[0, 2, "MetricName", "F1 score"]]}, {"text": "0.401\u00060.005 0.442\u00060.007 0.423\u00060.005", "entities": []}, {"text": "A VG .", "entities": []}, {"text": "F1 score weighted with Freq . 0.536 \u00060.000 0.576\u00060.005", "entities": [[0, 2, "MetricName", "F1 score"]]}, {"text": "0.558\u00060.004 0:578\u00060:006 0.570\u00060.004 Table 6 : F1 score of the models trained with embeddings with dimensionality of 300 and 100 respectively .", "entities": [[6, 8, "MetricName", "F1 score"]]}, {"text": "Figure 6 : predictions of posts by 300 dim CNN - GloVe Figure 7 gives the comparative performance of two models under different oversampling ratios .", "entities": [[11, 12, "MethodName", "GloVe"]]}, {"text": "While oversampling is essential for both models , GRU - Skip - thought is less sensitive to lower oversampling ratios , suggesting that skip - thoughts can already capture sentiment on the sentence level .", "entities": [[8, 9, "MethodName", "GRU"]]}, {"text": "Therefore , including only a limited ratio of positive samples is suf\ufb01cient to train the classi\ufb01er .", "entities": []}, {"text": "Instead , models using word vectors need more positive data to learn sentence sentiment features .", "entities": []}, {"text": "8 Conclusion We presented an ontology based on the principles of Cognitive Behavioural Therapy .", "entities": [[5, 6, "MethodName", "ontology"]]}, {"text": "We then annotated data that exhibits psychological problems and computed the inter - annotator agreement .", "entities": []}, {"text": "We found that classifying thinking errors is a dif\ufb01cult task as suggested by the low inter - annotator agreement .", "entities": []}, {"text": "We trained GloVe word embeddings and skip - thought embeddings on 500 K posts in an unsupervised fashion and generated distributed representations both of words and of sentences .", "entities": [[2, 3, "MethodName", "GloVe"], [3, 5, "TaskName", "word embeddings"]]}, {"text": "We", "entities": []}, {"text": "52label precision recall F1 score accuracy Anxiety 0.739 \u00060.007 0.884\u00060.005 0.805\u00060.006 0.729\u00060.012 Depression 0.538 \u00060.010 0.708\u00060.005 0.611\u00060.008 0.813\u00060.010", "entities": [[3, 5, "MetricName", "F1 score"], [5, 6, "MetricName", "accuracy"]]}, {"text": "Hurt 0.428 \u00060.005 0.620\u00060.004 0.506\u00060.005 0.763\u00060.011 Anger 0.313 \u00060.005", "entities": []}, {"text": "0.491\u00060.000 0.383\u00060.004 0.769\u00060.012", "entities": []}, {"text": "Loneliness 0.479 \u00060.010 0.643\u00060.008 0.549\u00060.009 0.923\u00060.006 Grief 0.437 \u00060.013 0.490\u00060.000", "entities": []}, {"text": "0.462\u00060.008 0.937\u00060.005", "entities": []}, {"text": "Shame 0.219 \u00060.008 0.378\u00060.004 0.277\u00060.007 0.891\u00060.007 Jealousy 0.170 \u00060.002 0.296\u00060.012 0.216\u00060.005 0.935\u00060.006", "entities": []}, {"text": "Guilt 0.221 \u00060.014 0.378\u00060.008 0.279\u00060.014 0.936\u00060.008 Relationships 0.847 \u00060.005", "entities": []}, {"text": "0.912\u00060.007 0.878\u00060.006", "entities": []}, {"text": "0.829\u00060.011", "entities": []}, {"text": "Existential 0.516 \u00060.008 0.700\u00060.004 0.594\u00060.007 0.789\u00060.009", "entities": []}, {"text": "Health 0.520 \u00060.010 0.668\u00060.005 0.585\u00060.008 0.900\u00060.006 School College 0.570 \u00060.009 0.821\u00060.008 0.673\u00060.009 0.934\u00060.004 Other 0.209 \u00060.004 0.331\u00060.007 0.256\u00060.005 0.894\u00060.007 Work 0.601 \u00060.015 0.733\u00060.006", "entities": []}, {"text": "0.661\u00060.011 0.955\u00060.003", "entities": []}, {"text": "Bereavement 0.567 \u00060.029 0.733\u00060.008 0.639\u00060.021 0.979\u00060.002 Jumping tonegative conclusions 0.643 \u00060.005 0.775\u00060.004 0.703\u00060.005", "entities": []}, {"text": "0.711\u00060.009", "entities": []}, {"text": "Fortune telling 0.486 \u00060.006 0.737\u00060.004 0.585\u00060.006 0.733\u00060.010", "entities": []}, {"text": "Black andwhite 0.330 \u00060.003 0.625\u00060.003 0.432\u00060.003 0.657\u00060.011 Low frustration tolerance 0.222 \u00060.005 0.531\u00060.002", "entities": []}, {"text": "0.313\u00060.005", "entities": []}, {"text": "0.631\u00060.028", "entities": []}, {"text": "Catastrophising 0.291 \u00060.005", "entities": []}, {"text": "0.509\u00060.000", "entities": []}, {"text": "0.371\u00060.004 0.796\u00060.012 Mind - reading 0.343 \u00060.008 0.540\u00060.002 0.419\u00060.006 0.783\u00060.014 Labelling 0.376 \u00060.004 0.597\u00060.003 0.462\u00060.004 0.853\u00060.007", "entities": []}, {"text": "Emotional reasoning 0.241 \u00060.006 0.417\u00060.004 0.306\u00060.006 0.748\u00060.017 Over - generalising 0.337 \u00060.009 0.548\u00060.002 0.418\u00060.008 0.808\u00060.014 In\ufb02exibility 0.162 \u00060.002", "entities": []}, {"text": "0.336\u00060.006 0.218\u00060.003 0.807\u00060.012", "entities": []}, {"text": "Blaming 0.218 \u00060.002 0.381\u00060.005 0.277\u00060.003 0.841\u00060.009 Disqualifying thepositive 0.125 \u00060.002 0.365\u00060.008 0.187\u00060.003 0.808\u00060.016 Mental \ufb01ltering 0.087", "entities": []}, {"text": "\u00060.001 0.386\u00060.009 0.141\u00060.002", "entities": []}, {"text": "0.741\u00060.026 Personalising 0.179 \u00060.003 0.345\u00060.007 0.236\u00060.004 0.871\u00060.009", "entities": []}, {"text": "Comparing 0.257 \u00060.009 0.253\u00060.009 0.255\u00060.009 0.952\u00060.003 Table 7 : Precision , recall , F1 score and accuracy for 300 dim CNN - GloVe with oversampling ratio 1:1 Figure 7 : Weighted A VG . F1 for different models then used the GloVe word vectors as input to a CNN and the skip - thought sentence vectors as input to a GRU .", "entities": [[9, 10, "MetricName", "Precision"], [13, 15, "MetricName", "F1 score"], [16, 17, "MetricName", "accuracy"], [22, 23, "MethodName", "GloVe"], [34, 35, "MetricName", "F1"], [41, 42, "MethodName", "GloVe"], [60, 61, "MethodName", "GRU"]]}, {"text": "The results suggest that both models signi\ufb01cantly outperform a chance classi\ufb01er for all thinking errors , emotions andsituations with CNNGloVe on average achieving better results .", "entities": []}, {"text": "Areas of future investigation include richer dis - tributed representations , or a fusion of distributed representations from word - level , sentence - level and document - level , to acquire more powerful semantic features .", "entities": []}, {"text": "We also plan to extend the current ontology with its focus on thinking errors , emotions andsituations to include a much lager number of concepts .", "entities": [[7, 8, "MethodName", "ontology"]]}, {"text": "The development of a statistical system delivering therapy will moreover require further research on other modules of a dialogue system .", "entities": []}, {"text": "Acknowledgements This work was funded by EPSRC project Natural speech Automated Utility for Mental health ( NAUM ) , award reference EP / P017746/1 .", "entities": []}, {"text": "The authors would also like to thank anonymous reviewers for their valuable comments .", "entities": []}, {"text": "The code is available at https://github.com/YinpeiDai/NAUM", "entities": []}, {"text": "References S. Arora , Y .", "entities": []}, {"text": "Liang , and T. Ma . 2017 .", "entities": []}, {"text": "A simple but tough - to - beat baseline for sentence embedding .", "entities": [[10, 12, "TaskName", "sentence embedding"]]}, {"text": "In ICLR .", "entities": []}, {"text": "A.T. Beck .", "entities": []}, {"text": "1976 .", "entities": []}, {"text": "Cognitive Therapy and the Emotional Disorders .", "entities": []}, {"text": "New York , International Universities Press .", "entities": []}, {"text": "53A.T. Beck , J. Rush , B. Shaw , and G Emery .", "entities": []}, {"text": "1979 .", "entities": []}, {"text": "Cognitive Therapy of Depression .", "entities": []}, {"text": "New York , Guildford Press .", "entities": []}, {"text": "Charissa Bhasi , Rohanna Cawdron , Melissa Clapp , Jeremy Clarke , Mike Crawford , Lorna Farquharson , Elizabeth Hancock , Miranda Heneghan , Rachel Marsh , and Lucy Palmer .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Second Round of the National Audit of Psychological Therapies for Anxiety and Depression ( NAPT ) .", "entities": []}, {"text": "Timothy Bickmore , Amanda Gruber , and Rosalind Picard .", "entities": []}, {"text": "2005 .", "entities": []}, {"text": "Establishing the computer \u2013 patient working alliance in automated health behavior change interventions .", "entities": []}, {"text": "Patient education and counseling , 59(1):21\u201330 .", "entities": []}, {"text": "Robyn Bluhm .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "The need for new ontologies in psychiatry .", "entities": []}, {"text": "Philosophical Explorations , 20(2):146 \u2013 159 .", "entities": []}, {"text": "R. Branch and R. Willson .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Cognitive Behavioural Therapy for Dummies .", "entities": []}, {"text": "Wiley .", "entities": []}, {"text": "Ronan Collobert , Jason Weston , L \u00b4 eon Bottou , Michael Karlen , Koray Kavukcuoglu , and Pavel Kuksa . 2011 .", "entities": []}, {"text": "Natural language processing ( almost ) from scratch .", "entities": []}, {"text": "Journal of Machine Learning Research , 12(Aug):2493\u20132537 .", "entities": []}, {"text": "Heriberto Cuay \u00b4 ahuitl .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Hierarchical reinforcement learning for spoken dialogue systems .", "entities": [[0, 3, "TaskName", "Hierarchical reinforcement learning"], [4, 7, "TaskName", "spoken dialogue systems"]]}, {"text": "Ph.D. thesis , University of Edinburgh , Edinburgh .", "entities": []}, {"text": "D DeVault , R Artstein , G Ben , T Dey , E Fast , A Gainer , K Georgila , J Gratch , A Hartholt , M Lhommet , G Lucas , S Marsella , F Morbini , A Nazarian , S Scherer , G Stratou , A Suri , D Traum , R Wood , Y Xu , A Rizzo , and L - P Morency .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Simsensei kiosk :", "entities": []}, {"text": "A virtual human interviewer for healthcare decision support .", "entities": []}, {"text": "In International Conference on Autonomous Agents and Multiagent Systems .", "entities": []}, {"text": "EU high - level conference : Together for Mental Health and Well - being .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "European Pact on Mental Health and Well - being .", "entities": []}, {"text": "Mehdi Fatemi , Layla El Asri , Hannes Schulz , Jing He , and Kaheer Suleman .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Policy networks with two - stage training for dialogue systems .", "entities": []}, {"text": "In Proceedings of SIGDIAL .", "entities": []}, {"text": "Kathleen Kara Fitzpatrick , Alison Darcy , and Molly Vierhile .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent ( woebot ): a randomized controlled trial .", "entities": [[18, 19, "DatasetName", "agent"]]}, {"text": "JMIR mental health , 4(2 ) .", "entities": []}, {"text": "M. Gasic and S. Young .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Gaussian processes for pomdp - based dialogue manager optimization .", "entities": [[0, 2, "TaskName", "Gaussian processes"]]}, {"text": "Audio , Speech , and Language Processing , IEEE / ACM Transactions on , 22(1):28\u201340.M Geist and O Pietquin .", "entities": [[10, 11, "DatasetName", "ACM"]]}, {"text": "2011 .", "entities": []}, {"text": "Managing Uncertainty within the KTD Framework .", "entities": []}, {"text": "In Proceedings of the Workshop on Active Learning and Experimental Design , Sardinia ( Italy ) .", "entities": [[6, 8, "TaskName", "Active Learning"]]}, {"text": "Sanjay Surendranath Girija .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Tensor\ufb02ow : Largescale machine learning on heterogeneous distributed systems .", "entities": []}, {"text": "Nathan B. Hansen , Michael J. Lambert , and Evan M. Forman .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "The psychotherapy dose - response effect and its implications for treatment delivery services .", "entities": []}, {"text": "Clinical Psychology : Science and Practice , 9(3):329\u2013343 .", "entities": []}, {"text": "Larry P Heck , Dilek Hakkani - T \u00a8ur , and G \u00a8okhan T \u00a8ur .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Leveraging knowledge graphs for web - scale unsupervised semantic parsing .", "entities": [[1, 3, "TaskName", "knowledge graphs"], [7, 10, "TaskName", "unsupervised semantic parsing"]]}, {"text": "In Proceedings of Interspeech , pages 1594\u20131598 .", "entities": []}, {"text": "Stefan Hofmann .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Toward a cognitive - behavioral classi\ufb01cation system for mental disorders .", "entities": []}, {"text": "Behavior Therapy , 45(4):576 \u2013 587 .", "entities": []}, {"text": "TR Insel and EM Scholnick .", "entities": [[3, 4, "MetricName", "EM"]]}, {"text": "2006 .", "entities": []}, {"text": "Cure therapeutics and strategic prevention : raising the bar for mental health research .", "entities": []}, {"text": "Molecular Psychiatry , 11(1):11 - 17 .", "entities": []}, {"text": "Nal Kalchbrenner , Edward Grefenstette , and Phil Blunsom .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A convolutional neural network for modelling sentences .", "entities": []}, {"text": "arXiv preprint arXiv:1404.2188 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Yoon Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional neural networks for sentence classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1408.5882 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "R. Kiros , Y .", "entities": []}, {"text": "Zhu , R. Salakhutdinov , R.S. Zemel , A. Torralba , R. Urtasun , and S. Fidler . 2015 .", "entities": []}, {"text": "Skip - thought vectors .", "entities": []}, {"text": "NIPS .", "entities": []}, {"text": "Quoc Le and Tomas Mikolov .", "entities": []}, {"text": "2014a .", "entities": []}, {"text": "Distributed representations of sentences and documents .", "entities": []}, {"text": "In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32 , ICML\u201914 , pages II\u20131188 \u2013 II\u20131196 .", "entities": []}, {"text": "JMLR.org .", "entities": []}, {"text": "Quoc Le and Tomas Mikolov .", "entities": []}, {"text": "2014b .", "entities": []}, {"text": "Distributed representations of sentences and documents .", "entities": []}, {"text": "In International Conference on Machine Learning , pages 1188\u20131196 .", "entities": []}, {"text": "Jiwei Li , Will Monroe , Alan Ritter , and Dan Jurafsky .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Deep reinforcement learning for dialogue generation .", "entities": [[4, 6, "TaskName", "dialogue generation"]]}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Andrew L Maas , Raymond E Daly , Peter T Pham , Dan Huang , Andrew Y Ng , and Christopher Potts . 2011 .", "entities": []}, {"text": "Learning word vectors for sentiment analysis .", "entities": [[4, 6, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 49th annual meeting of the association for computational linguistics : Human language technologies - volume 1 , pages 142\u2013150 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "F. Mairesse , M. Ga \u02c7si\u00b4c , F. Jur \u02c7c\u00b4\u0131\u02c7cek , S. Keizer , B. Thomson , K. Yu , and S. Young .", "entities": []}, {"text": "2009 .", "entities": []}, {"text": "Spoken language understanding from unaligned data using discriminative classi\ufb01cation models .", "entities": [[0, 3, "TaskName", "Spoken language understanding"]]}, {"text": "In Proceedings of ICASSP .", "entities": []}, {"text": "54Gr\u00b4egoire Mesnil , Yann Dauphin , Kaisheng Yao , Yoshua Bengio , Li Deng , Dilek Hakkani - Tur , Xiaodong He , Larry Heck , Gokhan Tur , Dong Yu , and Geoffrey Zweig . 2015 .", "entities": []}, {"text": "Using recurrent neural networks for slot \ufb01lling in spoken language understanding .", "entities": [[8, 11, "TaskName", "spoken language understanding"]]}, {"text": "IEEE Transactions on Audio , Speech , and Language Processing , 23(3):530\u2013539 .", "entities": []}, {"text": "RR Morris , Schueller SM , and Picard RW . 2015 .", "entities": [[0, 1, "DatasetName", "RR"]]}, {"text": "Ef\ufb01cacy of a Web - Based , Crowdsourced Peer - ToPeer Cognitive Reappraisal Platform for Depression : Randomized Controlled Trial .", "entities": []}, {"text": "J Med Internet Res , 17(3 ) .", "entities": []}, {"text": "Nikola Mrk \u02c7si\u00b4c , Diarmuid \u00b4 O S\u00b4eaghdha , Tsung - Hsien Wen , Blaise Thomson , and Steve Young . 2017 .", "entities": []}, {"text": "Neural belief tracker : Data - driven dialogue state tracking .", "entities": [[7, 10, "TaskName", "dialogue state tracking"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1777\u20131788 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "F. Pedregosa , G. Varoquaux , A. Gramfort , V .", "entities": []}, {"text": "Michel , B. Thirion , O. Grisel , M. Blondel , P. Prettenhofer , R. Weiss , V .", "entities": []}, {"text": "Dubourg , J. Vanderplas , A. Passos , D. Cournapeau , M. Brucher , M. Perrot , and E. Duchesnay . 2011 .", "entities": []}, {"text": "Scikit - learn : Machine learning in Python .", "entities": []}, {"text": "Journal of Machine Learning Research , 12:2825\u20132830 .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher D. Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532\u20131543 .", "entities": []}, {"text": "Radim \u02c7Reh\u02dau\u02c7rek and Petr Sojka .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Software Framework for Topic Modelling with Large Corpora .", "entities": []}, {"text": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks , pages 45\u201350 , Valletta , Malta .", "entities": []}, {"text": "ELRA .", "entities": []}, {"text": "http://is.muni.cz/ publication/884893 / en .", "entities": []}, {"text": "Giuseppe Riccardi .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Towards healthcare personal agents .", "entities": []}, {"text": "In Proceedings of the 2014 Workshop on Roadmapping the Future of Multimodal Interaction Research Including Business Opportunities and Challenges , RFMIR \u2019 14 , pages 53\u201356 , New York , NY , USA . ACM .", "entities": [[34, 35, "DatasetName", "ACM"]]}, {"text": "Lazlo Ring , Barbara Barry , Kathleen Totzke , and Timothy Bickmore .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Addressing loneliness and isolation in older adults : Proactive affective agents provide better support .", "entities": []}, {"text": "In Proceedings of the 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction , ACII \u2019 13 , pages 61\u201366 , Washington , DC , USA .", "entities": []}, {"text": "IEEE Computer Society .", "entities": []}, {"text": "Lazlo Ring , Timothy Bickmore , and Paola Pedrelli .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "An affectively aware virtual therapist for depression counseling .", "entities": []}, {"text": "In CHI 2016 Computing and Mental Health Workshop .", "entities": []}, {"text": "Lina M. Rojas Barahona , M. Gasic , N. Mrk \u02c7si\u00b4c , P - H Su , S. Ultes , T - H Wen , and S. Young .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Exploitingsentence and context representations in deep neural models for spoken language understanding .", "entities": [[9, 12, "TaskName", "spoken language understanding"]]}, {"text": "In Proceedings of COLING 2016 , the 26th International Conference on Computational Linguistics : Technical Papers , pages 258\u2013267 , Osaka , Japan .", "entities": []}, {"text": "The COLING 2016 Organizing Committee .", "entities": []}, {"text": "Lina Maria Rojas - Barahona and Toni Giorgino . 2009 .", "entities": []}, {"text": "Adaptable dialog architecture and runtime engine ( adarte ): A framework for rapid prototyping of health dialog systems .", "entities": []}, {"text": "I. J. Medical Informatics , 78(Supplement-1):S56 \u2013 S68 .", "entities": []}, {"text": "Andrew M Saxe , James L McClelland , and Surya Ganguli .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks .", "entities": []}, {"text": "arXiv preprint arXiv:1312.6120 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "J Schatzmann , K Weilhammer , MN Stuttle , and S Young .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "A Survey of Statistical User Simulation Techniques for Reinforcement - Learning of Dialogue Management Strategies .", "entities": [[12, 14, "TaskName", "Dialogue Management"]]}, {"text": "KER , 21(2):97 \u2013 126 .", "entities": []}, {"text": "D.F. Tolin .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Is cognitivebehavioral therapy more effective than other therapies ?", "entities": []}, {"text": "A meta - analytic review .", "entities": []}, {"text": "Clinical Psychology Review , 30:710\u2013720 .", "entities": []}, {"text": "G\u00a8okhan T \u00a8ur , Minwoo Jeong , Ye - Yi Wang , Dilek Hakkani - T \u00a8ur , and Larry P Heck .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Exploiting the semantic web for unsupervised natural language semantic parsing .", "entities": [[8, 10, "TaskName", "semantic parsing"]]}, {"text": "In Proceedings of Interspeech .", "entities": []}, {"text": "L.P. Vardoulakis , L. Ring , B. Barry , C. Sidner , and T. Bickmore .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Designing relational agents as long term social companions for older adults .", "entities": []}, {"text": "In Yukiko Nakano , Michael Neff , Ana Paiva , and Marilyn Walker , editors , Intelligent Virtual Agents , volume 7502 of Lecture Notes in Computer Science , pages 289\u2013302 .", "entities": []}, {"text": "Springer Berlin Heidelberg .", "entities": []}, {"text": "Y .", "entities": []}, {"text": "Wang , L. Wang , M. Rastegar - Mojarad , S. Moon , F. Shen , N. Afzal , S. Liu , Y .", "entities": []}, {"text": "Zeng , S. Mehrabi , S. Sohn , and H. Liu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Clinical information extraction applications : A literature review .", "entities": []}, {"text": "Journal of Biomedical Informatics , 77:34 \u2013 49 .", "entities": []}, {"text": "Joseph Weizenbaum .", "entities": []}, {"text": "1966 .", "entities": []}, {"text": "Eliza , a computer program for the study of natural language communication between man and machine .", "entities": []}, {"text": "ACM , 9(1):36\u201345 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Jason D. Williams , Kavosh Asadi , and Geoffrey Zweig .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Hybrid code networks : practical and ef\ufb01cient end - to - end dialog control with supervised and reinforcement learning .", "entities": []}, {"text": "In Proceedings of ACL .", "entities": []}, {"text": "World Health Organization .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Mental health action plan 2013 - 2020 .", "entities": []}, {"text": "Kaisheng Yao , Baolin Peng , Yu Zhang , Dong Yu , G. Zweig , and Yangyang Shi . 2014 .", "entities": []}, {"text": "Spoken language understanding using long short - term memory neural networks .", "entities": [[0, 3, "TaskName", "Spoken language understanding"], [4, 9, "MethodName", "long short - term memory"]]}, {"text": "In Spoken Language Technology Workshop ( SLT ) , 2014 IEEE , pages 189\u2013194 .", "entities": []}, {"text": "SJ Young .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Talking to Machines ( Statistically Speaking ) .", "entities": []}, {"text": "In Proceedings of ICSLP .", "entities": []}]