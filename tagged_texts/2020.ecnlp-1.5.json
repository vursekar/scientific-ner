[{"text": "Proceedings of the 3rd Workshop on e - Commerce and NLP ( ECNLP 3 ) , pages 35\u201339 Online , July 10 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics35Using Large Pretrained Language Models for Answering User Queries from Product Speci\ufb01cations Kalyani Roy1 , Smit Shah1\u0003 , Nithish Pai2 , Jaidam Ramtej2 , Prajit Prashant Nadkarn2 , Jyotirmoy Banerjee2 , Pawan Goyal1,and Surender Kumar2 1Indian Institute of Technology Kharagpur 2Flipkart kroy@iitkgp.ac.in , smitsunny11@gmail.com , fnithish.p , jaidam.ramtej , prajit.pn g@ \ufb02ipkart.com , jyoban@gmail.com , pawang@cse.iitkgp.ac.in ,", "entities": [[6, 9, "TaskName", "Pretrained Language Models"]]}, {"text": "surender.k@\ufb02ipkart.com Abstract While buying a product from the e - commerce websites , customers generally have a plethora of questions .", "entities": []}, {"text": "From the perspective of both the e - commerce service provider as well as the customers , there must be an effective question answering system to provide immediate answers to the user queries .", "entities": [[22, 24, "TaskName", "question answering"]]}, {"text": "While certain questions can only be answered after using the product , there are many questions which can be answered from the product speci\ufb01cation itself .", "entities": []}, {"text": "Our work takes a \ufb01rst step in this direction by \ufb01nding out the relevant product speci\ufb01cations , that can help answering the user questions .", "entities": []}, {"text": "We propose an approach to automatically create a training dataset for this problem .", "entities": []}, {"text": "We utilize recently proposed XLNet and BERT architectures for this problem and \ufb01nd that they provide much better performance than the Siamese model , previously applied for this problem ( Lai et al . , 2018 ) .", "entities": [[4, 5, "MethodName", "XLNet"], [6, 7, "MethodName", "BERT"]]}, {"text": "Our model gives a good performance even when trained on one vertical and tested across different verticals .", "entities": []}, {"text": "1 Introduction Product speci\ufb01cations are the attributes of a product .", "entities": []}, {"text": "These speci\ufb01cations help a user to easily identify and differentiate products and choose the one that matches certain speci\ufb01cations .", "entities": []}, {"text": "There are more than 80million products across 80 + product categories on Flipkart1 .", "entities": []}, {"text": "The 6largest categories are -Mobile , AC , Backpack , Computer , Shoes , and Watches .", "entities": []}, {"text": "A large fraction of user queries ( \u001820%)2 can be answered with the speci\ufb01cations .", "entities": []}, {"text": "Product speci\ufb01cations would be helpful in providing instant responses to questions newly posed by users about \u0003Work done while author was at IIT Kharagpur .", "entities": []}, {"text": "1Flipkart Pvt Ltd. is an e - commerce company based in Bangalore , India .", "entities": [[1, 2, "MethodName", "Pvt"]]}, {"text": "2We randomly sampled 1500 questions from all these verticals except Mobile and manually annotated them as to whether these can be answered through product speci\ufb01cations .", "entities": []}, {"text": "Figure 1 : Snapshot of a product with its speci\ufb01cations .", "entities": []}, {"text": "the corresponding product .", "entities": []}, {"text": "Consider a question \u201c What is the fabric of this bag ? \u201d", "entities": []}, {"text": "This new question can be easily answered by retrieving the speci\ufb01cation \u201c Material \u201d as the response .", "entities": []}, {"text": "Fig .", "entities": []}, {"text": "1 depicts this scenario .", "entities": []}, {"text": "Most of the recent works on product related queries on e - commerce leverage the product reviews to answer the questions ( Gao et al . , 2019 ; Zhao et", "entities": []}, {"text": "al . , 2019 ; McAuley and Yang , 2016 )", "entities": []}, {"text": ".", "entities": []}, {"text": "Although reviews are a rich source of data , they are also subject to personal experiences .", "entities": []}, {"text": "People tend to give many reviews on some products and since it is based upon their personal experience , the opinion is also diverse .", "entities": []}, {"text": "This creates a massive volume and range of opinions and thus makes review systems dif\ufb01cult to navigate .", "entities": []}, {"text": "Sometimes products do not even have any reviews that can be used to \ufb01nd an answer , also the reviews do not mention the speci\ufb01cations a lot , but mainly deal with the experience .", "entities": []}, {"text": "So , there are several reasons why product speci\ufb01cations might be a useful source of information to answer product - related queries which does not involve user experience to \ufb01nd an answer .", "entities": []}, {"text": "As the speci\ufb01cations are readily available , users can get the response instantly .", "entities": []}, {"text": "36Dataset Products Questions Avg .", "entities": []}, {"text": "Specs Mobile 1,175 260,529 55 AC 300 16,545 35 Backpack 300 16,878 17 Computer 300 93,589 60 Shoes 300 5,812 10 Watches 300 21,392 50 Table 1 : Statistics of 6 largest categories .", "entities": []}, {"text": "This paper attempts to retrieve the product speci\ufb01cations that would answer the user queries .", "entities": []}, {"text": "While solving this problem , our key contributions are as follows - ( i ) We demonstrate the success of XLNet on \ufb01nding product speci\ufb01cations that can help answering product related queries .", "entities": [[20, 21, "MethodName", "XLNet"]]}, {"text": "It beats the baseline Siamese method by 0:14\u00000:31points in HIT@1 .", "entities": []}, {"text": "( ii ) We utilize a method to automatically create a large training dataset using a semisupervised approach , that was used to \ufb01ne - tune XLNet and other models .", "entities": [[26, 27, "MethodName", "XLNet"]]}, {"text": "( iii ) While we trained onMobile vertical , we tested on different verticals , namely , AC , Backpack , Computer , Shoes , Watches , which show promising results .", "entities": []}, {"text": "2 Background and Related Work In recent years , e - commerce product question answering ( PQA ) has received a lot of attention .", "entities": [[13, 15, "TaskName", "question answering"]]}, {"text": "Yu et al .", "entities": []}, {"text": "( 2018 ) present a framework to answer product related questions by retrieving a ranked list of reviews and they employ the Positional Language Model ( PLM ) to create the training data .", "entities": []}, {"text": "Chen et al .", "entities": []}, {"text": "( 2019 ) apply a multi - task attentive model to identify plausible answers .", "entities": []}, {"text": "Lai et al .", "entities": []}, {"text": "( 2018 ) propose a Siamese deep learning model for answering questions regarding product speci\ufb01cations .", "entities": []}, {"text": "The model returns a score for a question and speci\ufb01cation pair .", "entities": []}, {"text": "McAuley and Yang ( 2016 ) exploit product reviews for answer prediction via a Mixture of Expert ( MoE ) model .", "entities": []}, {"text": "This MoE model makes use of a review relevance function and an answer prediction function .", "entities": []}, {"text": "It assumes that a candidate answer set containing the correct answers is available for answer selection .", "entities": [[14, 16, "TaskName", "answer selection"]]}, {"text": "Cui et al .", "entities": []}, {"text": "( 2017 ) develop a chatbot for e - commerce sites known as SuperAgent .", "entities": [[5, 6, "TaskName", "chatbot"]]}, {"text": "SuperAgent considers question answer collections , reviews and speci\ufb01cations when answering questions .", "entities": []}, {"text": "It selects the best answer from multiple data sources .", "entities": []}, {"text": "Language representation models like BERT ( Devlin et al . , 2019 ) and XLNet ( Yang et al . , 2019 ) are pre - trained on vast amounts of text and then \ufb01ne - tuned on task - speci\ufb01c labelled data .", "entities": [[4, 5, "MethodName", "BERT"], [14, 15, "MethodName", "XLNet"]]}, {"text": "The resulting models have achievedstate of the art in many natural language processing tasks including question answering .", "entities": [[15, 17, "TaskName", "question answering"]]}, {"text": "Dzendzik et al .", "entities": []}, {"text": "( 2019 ) employ BERT to answer binary questions by utilizing customer reviews .", "entities": [[4, 5, "MethodName", "BERT"]]}, {"text": "In this paper , unlike some of the previous works ( Lai et al . , 2018 ; Chen et al . , 2019 ) on PQA that solely rely on human annotators to annotate the training instances , we propose a semi - supervised method to label training data .", "entities": []}, {"text": "We leverage the product speci\ufb01cations to answer user queries by using BERT and XLNet .", "entities": [[11, 12, "MethodName", "BERT"], [13, 14, "MethodName", "XLNet"]]}, {"text": "3 Problem Statement Here , we formalize the problem of answering user queries from product speci\ufb01cations .", "entities": []}, {"text": "Given a questionQabout a product Pand the list of Mspeci\ufb01cationsfs1,s2 , ... , sMgofP , our objective is to identify the speci\ufb01cation sithat can help answer Q. Here , we assume that the question is answerable from speci\ufb01cations .", "entities": []}, {"text": "4 Model Architecture Our goal is to train a classi\ufb01er that takes a question and a speci\ufb01cation as input ( e.g. , \u201c Color Code Black \u201d ) and predicts whether the speci\ufb01cation is relevant to the question .", "entities": []}, {"text": "We take Siamese architecture ( Lai et al . , 2018 ) as our baseline method .", "entities": []}, {"text": "We \ufb01ne - tune BERT and XLNet for this classi\ufb01cation task .", "entities": [[4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "XLNet"]]}, {"text": "Siamese : We train a 100 - dimensional word2vec embedding on the whole corpus ( all questions and speci\ufb01cations as shown in Table 1 . ) to get the input word representation .", "entities": []}, {"text": "In the Siamese model , the question and speci\ufb01cation is passed through a Siamese Bi - LSTM layer .", "entities": [[16, 17, "MethodName", "LSTM"]]}, {"text": "Then we use max - pooling on the contextual representations to get the feature vectors of the question and speci\ufb01cation .", "entities": []}, {"text": "We concatenate the absolute difference and hadamard product of these two feature vectors and feed it to two fully connected layers of dimension 50 and 25 , subsequently .", "entities": []}, {"text": "Finally , the softmax layer gives the relevance score .", "entities": [[3, 4, "MethodName", "softmax"]]}, {"text": "BERT andXLNet : The architecture we use for \ufb01ne - tuning BERT and XLNet is the same .", "entities": [[0, 1, "MethodName", "BERT"], [11, 12, "MethodName", "BERT"], [13, 14, "MethodName", "XLNet"]]}, {"text": "We begin with the pre - trained BERT Baseand XLNet Base model .", "entities": [[7, 8, "MethodName", "BERT"], [9, 10, "MethodName", "XLNet"]]}, {"text": "To adapt the models for our task , we introduce a fully - connected layer over the \ufb01nal hidden state corresponding to the [ CLS ] input token .", "entities": []}, {"text": "During \ufb01ne - tuning , we optimize the entire model end - to - end , with the additional softmax classi\ufb01er parametersW2RK\u0002H , whereHis the dimen-", "entities": [[19, 20, "MethodName", "softmax"]]}, {"text": "37Dataset # que - spec Answer type ( in % ) pairs Num Y / N Other AC 3693 0.27 0.52 0.21 Backpack 2693 0.29 0.48 0.23 Computer 2718 0.04 0.78 0.18 Shoes 999 0.09 0.49 0.42 Watches 1700 0.17 0.59 0.24 Table 2 : Test datasets statistics .", "entities": []}, {"text": "sion of the hidden state vectors and Kis the number of classes .", "entities": []}, {"text": "5 Experimental Setup 5.1 Dataset Creation The Statistics for the 6 largest categories used in this paper are shown in Table 1 , containing a snapshot of product details up to January 2019 .", "entities": []}, {"text": "Except for mobiles , for other domains , 300 products were sampled .", "entities": []}, {"text": "As the number of question - speci\ufb01cation pairs is huge , manually labelling a suf\ufb01ciently large dataset is a tedious task .", "entities": []}, {"text": "So , we propose a semisupervised method to create a large training dataset using Dual Embedding Space model ( DESM ) ( Mitra et al . , 2016 ) .", "entities": []}, {"text": "Suppose a product PhasSspeci\ufb01cations and Qquestions .", "entities": []}, {"text": "For a question qi2Qand a speci\ufb01cationsj2S , we \ufb01nd dual embedding score DUAL ( qi;sj)using Equation 1 , where tqandts denote the vectors for the question and speci\ufb01cation terms , respectively .", "entities": []}, {"text": "We consider ( qi;sj ) pair positive if DUAL ( qi;sj)\u0015\u0012and negative if DUAL ( qi;sj)<\u0012. DUAL ( qi;sj ) = 1 jqijX tq2qitqTsj ktqkksjk(1 ) where sj=1", "entities": []}, {"text": "jsjjX ts2sjts ktsk(2 )", "entities": []}, {"text": "We takeMobile dataset to create labelled training data since most of the questions come from this vertical .", "entities": []}, {"text": "We choose the threshold value ( \u0012 ) which gives the best accuracy on manually labelled balanced validation dataset consisting of 380question and speci\ufb01cation pairs .", "entities": [[12, 13, "MetricName", "accuracy"], [17, 19, "DatasetName", "validation dataset"]]}, {"text": "We train a word2vec ( Mikolov et al . , 2013 ) model on our training dataset to get the embeddings of the words .", "entities": []}, {"text": "The word2vec model learns two weight matrices during training .", "entities": []}, {"text": "The matrix corresponding to the input space and the output space is denoted as IN and OUT word embedding space respectively .", "entities": []}, {"text": "Dataset Model HIT@1 HIT@2 HIT@3 ACBERT-380 0.05 0.09 0.14 XLNet-380 0.20 0.32 0.39 Siamese 0.38 0.53 0.61 BERT 0.62 0.77 0.81 XLNet 0.69 0.77 0.80 BackpackBERT-380 0.17 0.27 0.34 XLNet-380 0.27 0.41 0.48 Siamese 0.35 0.53 0.65 BERT 0.50 0.66 0.69 XLNet 0.49 0.67 0.70 ComputerBERT-380 0.14 0.16 0.22 XLNet-380 0.06 0.16 0.18 Siamese 0.5 0.6 0.72 BERT 0.68 0.80 0.90 XLNet 0.70 0.86 0.92 ShoesBERT-380 0.22 0.40 0.55 XLNet-380 0.25 0.45 0.60 Siamese 0.42 0.55 0.62 BERT 0.60 0.72 0.84 XLNet 0.63 0.77 0.88 WatchesBERT-380 0.05 0.09 0.15 XLNet-380 0.24 0.36 0.45 Siamese 0.42 0.65 0.69 BERT 0.54 0.60 0.74 XLNet 0.60 0.76 0.84 Table 3 : Performance comparison of different models .", "entities": [[17, 18, "MethodName", "BERT"], [21, 22, "MethodName", "XLNet"], [37, 38, "MethodName", "BERT"], [41, 42, "MethodName", "XLNet"], [57, 58, "MethodName", "BERT"], [61, 62, "MethodName", "XLNet"], [77, 78, "MethodName", "BERT"], [81, 82, "MethodName", "XLNet"], [97, 98, "MethodName", "BERT"], [101, 102, "MethodName", "XLNet"]]}, {"text": "Word2vec leverages only the input embeddings ( IN ) , but discards the output embeddings ( OUT ) , whereas DESM utilizes both IN and OUT embeddings .", "entities": []}, {"text": "To compute the DUAL score of a question and speci\ufb01cation , we take OUT - OUT vectors as it gives the best validation accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}, {"text": "We \ufb01nd that for\u0012= 0:34 , we gain maximum accuracy value of 0:72on the validation set .", "entities": [[9, 10, "MetricName", "accuracy"]]}, {"text": "This creates a labelled training datasetDwith 57;138positive pairs and 655;290negative pairs .", "entities": []}, {"text": "For training , we take all the positive data from Dand we randomly sample an equal number of negative examples from D. To create the test datasets , domain experts manually annotate the correct speci\ufb01cation for a question .", "entities": []}, {"text": "As the test datasets come from different verticals , there is no product in common with the training set .", "entities": []}, {"text": "The details of different test datasets are shown in Table 2 .", "entities": []}, {"text": "We analyze the questions in the test datasets and \ufb01nd that the questions can be roughly categorized into three classes - numerical , yes / no and others based upon the answer type of the questions .", "entities": []}, {"text": "For a question , we have a number of speci\ufb01cations and only one of them is correct .", "entities": []}, {"text": "5.2 Training and Evaluation We split the Mobile dataset into 80 % and 20 % as training set and development set , respectively .", "entities": []}, {"text": "The Siamese model is trained for 20 epoch with Stochastic Gradient Descent optimizer and learn-", "entities": [[9, 12, "MethodName", "Stochastic Gradient Descent"], [12, 13, "HyperparameterName", "optimizer"]]}, {"text": "38Question Siamese BERT XLNet Is it single core or multi core?processor name core i3 internal mic single digital microphonenumber of cores 2 processor variant 7100u processor name core i3 processor name core i3 os architecture 64 bit number of cores 2 processor brand intel Does 16 inch laptop \ufb01t in to it?depth 13 inch compatible laptop size 15.4 inchcompatible laptop size 15.4 inch width 9 inch laptop sleeve", "entities": [[2, 3, "MethodName", "BERT"], [3, 4, "MethodName", "XLNet"]]}, {"text": "no depth 13 inch height 19 inch depth 13 inch height 19 inch Table 4 : Top three speci\ufb01cations returned by different models for two questions .", "entities": []}, {"text": "Correct speci\ufb01cation is highlighted in bold .", "entities": []}, {"text": "ing rate 0.01 .", "entities": []}, {"text": "The \ufb01ne - tuning of BERT and XLNet is done with the same experimental settings as given in the original papers .", "entities": [[5, 6, "MethodName", "BERT"], [7, 8, "MethodName", "XLNet"]]}, {"text": "In all the models , we minimize the cross - entropy loss while training .", "entities": [[11, 12, "MetricName", "loss"]]}, {"text": "BERT-380 and XLNet-380 models are \ufb01ne - tuned on the 380 labeled validation dataset that was used for creating the training dataset in Section 5.1 .", "entities": [[12, 14, "DatasetName", "validation dataset"]]}, {"text": "During evaluation , we sort the question speci\ufb01cation pairs according to their relevance score .", "entities": []}, {"text": "From this ranked list , we compute whether the correct speci\ufb01cation appears within top k;k2f1;2;3 g positions .", "entities": []}, {"text": "The ratio of correctly identi\ufb01ed speci\ufb01cations in top 1,2 , and 3positions to the total number of questions is denoted as HIT@1 , HIT@2 and HIT@3 respectively .", "entities": []}, {"text": "6 Results and Discussion Table 3 shows the performance of the models on different datasets3 .", "entities": []}, {"text": "BERT-380 and XLNet-380 perform very poorly , but when we use the train dataset created with DESM , there is a large boost in the models \u2019 performance and it shows the effectiveness of our semi - supervised method in generating labeled dataset .", "entities": []}, {"text": "Both BERT and XLNet outperform the baseline Siamese model ( Lai et al . , 2018 ) by a large margin , and retrieve the correct speci\ufb01cation within top 3 results for most of the queries .", "entities": [[1, 2, "MethodName", "BERT"], [3, 4, "MethodName", "XLNet"]]}, {"text": "For Backpack andAC , both BERT and XLNet are very competitive .", "entities": [[5, 6, "MethodName", "BERT"], [7, 8, "MethodName", "XLNet"]]}, {"text": "XLNet outperforms BERT in Computer , Shoes , and Watches .", "entities": [[0, 1, "MethodName", "XLNet"], [2, 3, "MethodName", "BERT"]]}, {"text": "Only in HIT@1 of AC , BERT has surpassed XLNet with 0:07points .", "entities": [[6, 7, "MethodName", "BERT"], [9, 10, "MethodName", "XLNet"]]}, {"text": "We see that all the models have performed better in Computer compared to the other datasets .", "entities": []}, {"text": "Computer has the highest percentage of yes / no questions and this might be one of the reasons , as some questions might have word overlap with correct speci\ufb01cation .", "entities": []}, {"text": "Table 4 shows the top three speci\ufb01cations returned by different models for some 3Unsupervised DUAL embedding model gave very similar results to Siamese model , and is not reported.questions .", "entities": []}, {"text": "We see that Siamese architecture returns results which look similar to na \u00a8\u0131ve word match , and retrieve wrong speci\ufb01cations .", "entities": []}, {"text": "On the other hand , BERT and XLNet are able to retrieve the correct speci\ufb01cations .", "entities": [[5, 6, "MethodName", "BERT"], [7, 8, "MethodName", "XLNet"]]}, {"text": "Error Analysis : We assume that for each question , there is only one correct speci\ufb01cation , but the correct answer may span multiple speci\ufb01cations and our models can not provide a full answer .", "entities": [[0, 1, "MetricName", "Error"]]}, {"text": "For example , in Backpack dataset , the dimension of the backpack , i.e. , its height , weight , depth is de\ufb01ned separately .", "entities": []}, {"text": "So , when user queries about the dimension , only one speci\ufb01cation is provided .", "entities": []}, {"text": "Some speci\ufb01cations are given in one unit , but users want the answer in another unit , e.g. , \u201c what is the width of this bag in cms ? \u201d .", "entities": []}, {"text": "Since the speci\ufb01cation is given in inches , the models show the answer in inches .", "entities": []}, {"text": "So , the answer is related , but not exactly correct .", "entities": []}, {"text": "Users sometimes want to know the difference between certain speci\ufb01cation types , what is meant by some speci\ufb01cations .", "entities": []}, {"text": "For example , consider the questions \u201c what is the difference between inverter and non - inverter AC ? \u201d , \u201c what is meant by water resistant depth ? \u201d .", "entities": []}, {"text": "While we can \ufb01nd the type of inverter , the water resistant depth of a watch etc . from speci\ufb01cations , the de\ufb01nition of the speci\ufb01cation is not given .", "entities": []}, {"text": "As we have generated train data labels in semi - supervised fashion , it also contributes to inaccurate classi\ufb01cation in some cases .", "entities": []}, {"text": "7 Conclusion and Future Work", "entities": []}, {"text": "In this paper , we proposed a method to label training data with little supervision .", "entities": []}, {"text": "We demonstrated that large pretrained language models such as BERT and XLNet can be \ufb01ne - tuned successfully to obtain product speci\ufb01cations that can help answer user queries .", "entities": [[4, 7, "TaskName", "pretrained language models"], [9, 10, "MethodName", "BERT"], [11, 12, "MethodName", "XLNet"]]}, {"text": "We also achieve reasonably good results even while testing on different verticals .", "entities": []}, {"text": "We would like to extend our method to take into", "entities": []}, {"text": "39account multiple speci\ufb01cations as an answer .", "entities": []}, {"text": "We also plan to develop a classi\ufb01er to identify which questions can not be answered from the speci\ufb01cations .", "entities": []}, {"text": "References Long Chen , Ziyu Guan , Wei Zhao , Wanqing Zhao , Xiaopeng Wang , Zhou Zhao , and Huan Sun .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Answer identi\ufb01cation from product reviews for user questions by multi - task attentive networks .", "entities": []}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 33 , pages 45\u201352 .", "entities": []}, {"text": "Lei Cui , Shaohan Huang , Furu Wei , Chuanqi Tan , Chaoqun Duan , and Ming Zhou .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "SuperAgent :", "entities": []}, {"text": "A customer service chatbot for e - commerce websites .", "entities": [[3, 4, "TaskName", "chatbot"]]}, {"text": "pages 97\u2013102 .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Daria Dzendzik , Carl V ogel , and Jennifer Foster .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Is it dish washer safe ?", "entities": []}, {"text": "automatically answering \u201c yes / no \u201d questions using customer reviews .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Student Research Workshop , pages 1\u20136 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shen Gao , Zhaochun Ren , Yihong Zhao , Dongyan Zhao , Dawei Yin , and Rui Yan . 2019 .", "entities": []}, {"text": "Productaware answer generation in e - commerce questionanswering .", "entities": [[1, 3, "TaskName", "answer generation"]]}, {"text": "In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining , WSDM \u2019 19 , page 429\u2013437 , New York , NY , USA .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Association for Computing Machinery .", "entities": []}, {"text": "Tuan Lai , Trung Bui , Sheng Li , and Nedim Lipka .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A simple end - to - end question answering model for product information .", "entities": [[7, 9, "TaskName", "question answering"]]}, {"text": "In Proceedings of the First Workshop on Economics and Natural Language Processing , pages 38\u201343 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Julian McAuley and Alex Yang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Addressing complex and subjective product - related queries with customer reviews .", "entities": []}, {"text": "In Proceedings of the 25th International Conference on World Wide Web , WWW \u2019 16 , page 625\u2013635 , Republic and Canton of Geneva , CHE .", "entities": []}, {"text": "International World Wide Web Conferences Steering Committee .", "entities": []}, {"text": "Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg Corrado , and Jeffrey Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributed represen - tations of words and phrases and their compositionality .", "entities": []}, {"text": "In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 , NIPS\u201913 , page 3111\u20133119 , Red Hook , NY , USA .", "entities": []}, {"text": "Curran Associates Inc.", "entities": []}, {"text": "Bhaskar Mitra , Eric Nalisnick , Nick Craswell , and Rich Caruana .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A dual embedding space model for document ranking .", "entities": [[6, 8, "TaskName", "document ranking"]]}, {"text": "arXiv preprint arXiv:1602.01137 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Zhilin Yang , Zihang Dai , Yiming Yang , Jaime Carbonell , Russ R Salakhutdinov , and Quoc V Le . 2019 .", "entities": []}, {"text": "Xlnet :", "entities": [[0, 1, "MethodName", "Xlnet"]]}, {"text": "Generalized autoregressive pretraining for language understanding .", "entities": []}, {"text": "In Advances in neural information processing systems , pages 5754\u20135764 .", "entities": []}, {"text": "Qian Yu , Wai Lam , and Zihao Wang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Responding e - commerce product questions via exploiting QA collections and reviews .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , pages 2192\u20132203 , Santa Fe , New Mexico , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Jie Zhao , Ziyu Guan , and Huan Sun .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Riker : Mining rich keyword representations for interpretable product question answering .", "entities": [[9, 11, "TaskName", "question answering"]]}, {"text": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery Data Mining , KDD \u2019 19 , page 1389\u20131398 , New York , NY , USA .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Association for Computing Machinery .", "entities": []}]