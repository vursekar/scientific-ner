[{"text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 6225\u20136231 , Hong Kong , China , November 3\u20137 , 2019 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics6225Cross - Sentence N - ary Relation Extraction using Lower - Arity Universal Schemas Kosuke Akimoto1 , Takuya Hiraoka1 , Kunihiko Sadamasa1 , Mathias Niepert2 1Security Research Laboratories , NEC Corporation 2NEC Laboratories Europe fk - akimoto@ab , t - hiraoka@ce , k - sadamasa@az g.jp.nec.com mathias.niepert@neclab.eu", "entities": [[10, 12, "TaskName", "Relation Extraction"]]}, {"text": "Abstract Most existing relation extraction approaches exclusively target binary relations , and n - ary relation extraction is relatively unexplored .", "entities": [[3, 5, "TaskName", "relation extraction"], [15, 17, "TaskName", "relation extraction"]]}, {"text": "Current state - of - the - art n - ary relation extraction method is based on a supervised learning approach and , therefore , may suffer from the lack of suf\ufb01cient relation labels .", "entities": [[11, 13, "TaskName", "relation extraction"]]}, {"text": "In this paper , we propose a novel approach to cross - sentence n - ary relation extraction based on universal schemas .", "entities": [[16, 18, "TaskName", "relation extraction"]]}, {"text": "To alleviate the sparsity problem and to leverage inherent decomposability of n - ary relations , we propose to learn relation representations of lower - arity facts that result from decomposing higher - arity facts .", "entities": []}, {"text": "The proposed method computes a score of a new nary fact by aggregating scores of its decomposed lower - arity facts .", "entities": []}, {"text": "We conduct experiments with datasets for ternary relation extraction and empirically show that our method improves the n - ary relation extraction performance compared to previous methods .", "entities": [[7, 9, "TaskName", "relation extraction"], [20, 22, "TaskName", "relation extraction"]]}, {"text": "1 Introduction Relation extraction is a core natural language processing task which is concerned with the extraction of relations between entities from text .", "entities": [[2, 4, "TaskName", "Relation extraction"]]}, {"text": "It has numerous applications ranging from question answering ( Xu et", "entities": [[6, 8, "TaskName", "question answering"]]}, {"text": "al . , 2016 ) to automated knowledge base construction ( Dong et al . , 2014 ) .", "entities": []}, {"text": "While the vast majority of existing research focuses on extracting binary relations , there exists only few recent approaches to extract n - ary relations , that is , relations among n\u00152entities ( Li et al . , 2015 ; Ernst et al . , 2018 ) .", "entities": []}, {"text": "In n - ary relation extraction , relation mentions tend to span multiple sentences more frequently as nincreases .", "entities": [[4, 6, "TaskName", "relation extraction"]]}, {"text": "Thus , Peng et al .", "entities": []}, {"text": "( 2017 ) recently extended the problem to cross - sentence n - ary relation extraction in which n - ary relations are extracted from multiple sentences .", "entities": [[14, 16, "TaskName", "relation extraction"]]}, {"text": "As a motivating example , consider the following text from Wikipedia : \u201c Revisstarted off the 2009 season matched up against some of football \u2019s best wide receivers .", "entities": []}, {"text": "In Week 1 , he helped limit Houston Texans Pro - bowler Andre Johnson to four receptions for 35 yards . \u201d", "entities": [[7, 8, "DatasetName", "Houston"]]}, {"text": "In this example , two sentences collectively describes that Andre Johnson is a player of the football team the Texans during 2009 season , and thus we need cross - sentence information to correctly extract this ternary interaction among the three entities , i.e. Player ( Andre Johnson , Texans , 2009 season ) .", "entities": []}, {"text": "Previous methods ( Peng et al . , 2017 ; Song et al . , 2018 ) capture cross - sentence n - ary relation mentions by representing texts with a document graph which consists of both intra- and cross - sentence links between words .", "entities": []}, {"text": "With this graphical representation , they applied graph neural networks to predict ternary relations in the medical domain .", "entities": [[16, 18, "DatasetName", "medical domain"]]}, {"text": "However , these methods train the neural networks in a supervised manner using distant supervision ( Mintz et al . , 2009 ) and , therefore , may suffer from the lack of suf\ufb01cient positive labels when a well - populated knowledge base is not available .", "entities": []}, {"text": "On the other hand , for binary relation extraction , the problem of insuf\ufb01cient positive labels can be mitigated with universal schemas ( Riedel et al . , 2013 ) .", "entities": [[6, 9, "TaskName", "binary relation extraction"]]}, {"text": "In a universal schema approach , textual representations ( surface patterns ) of entities and their relations are encoded into the same vector space as the canonical knowledge base relations .", "entities": []}, {"text": "Thus , semantically similar surface patterns can share information of relation labels in a semisupervised manner .", "entities": []}, {"text": "This reduces the amount of required labeled training data .", "entities": []}, {"text": "Applying the universal schema approach to n - ary ( n > 2 ) relation extraction is , however , not straight - forward due to the sparsity of higher - order relation mentions among a speci\ufb01c set of n>2entities.1This is be1In our Wiki-90k dataset ( see x4.1 ) , only 12.5 % of ternary entity tuples have at least two relations among the entities , while 77.3 % of entities appear at least twice .", "entities": [[14, 16, "TaskName", "relation extraction"]]}, {"text": "6226cause the universal schema approach ( Riedel et al . , 2013 ) and its extensions ( Toutanova et al . , 2015 ; Verga et al . , 2016 , 2017 ) utilize co - occurring patterns of relation types between speci\ufb01c pair of entities .", "entities": []}, {"text": "Also , prior work has only addressed binary relations , and it is not trivial to de\ufb01ne surface patterns among n >", "entities": []}, {"text": "2entities and to encode these patterns into a vector representation .", "entities": []}, {"text": "To mitigate the aforementioned sparsity problem and utilize existing encoders for binary and unary surface patterns , we propose to train universal schema models on more dense lower - arity ( unary and binary ) facts instead of original sparse n - ary facts .", "entities": []}, {"text": "Since most n - ary relations can be decomposed into a set of k - ary relations ( k= 1;2 ) which are implied by the n - ary relation,2we can easily acquire lower - arity facts by decomposing n - ary facts .", "entities": []}, {"text": "Our model learns representations of these lower - arity relations using the universal schema framework , and predicts new n - ary facts by aggregating scores of lower - arity facts .", "entities": []}, {"text": "To evaluate the proposed method , we create new cross - sentence n - ary relation extraction datasets with multiple ternary relations.3The new datasets contain more entity tuples with known relational facts appeared in a knowledge base than the existing dataset ( Peng et al . , 2017 ) , and , therefore , these datasets can be used to more effectively evaluate methods which predict relation labels for each individual entity tuple .", "entities": [[15, 17, "TaskName", "relation extraction"]]}, {"text": "We show empirically that by jointly training lower - arity models and an nary score aggregation model , the proposed method improves the performance of n - ary relation extraction .", "entities": [[28, 30, "TaskName", "relation extraction"]]}, {"text": "To the best of our knowledge , this is the \ufb01rst attempt to apply universal schemas to n - ary relation extraction , taking advantage of the compositionality of higher - arity facts .", "entities": [[20, 22, "TaskName", "relation extraction"]]}, {"text": "2 Task De\ufb01nition and Notation The cross - sentence n - ary relation extraction task ( Peng et", "entities": [[12, 14, "TaskName", "relation extraction"]]}, {"text": "al . , 2017 ) is de\ufb01ned as follows .", "entities": []}, {"text": "Let Ebe a set of entities , RKBbe a set of relation types of an external knowledge base KB , and OKB = fhr;(e1;:::;en)i : r(e1;:::;en)2KB;r2 2For example , the ternary relation AwardedFor ( director;movie;award ) can be decomposed into the binary relations DirectorOf ( director;movie ) and WonAward ( director;award ) .", "entities": []}, {"text": "Note that a similar idea is introduced in ( Ernst et al . , 2018 ) as partial facts orpartial patterns .", "entities": []}, {"text": "3Our codes and datasets are available at https://github.com/aurtg/ nary - relation - extraction - decomposed .", "entities": []}, {"text": "Figure 1 : An overview of the proposed method .", "entities": []}, {"text": "nmod compound acl compound Figure 2 : An example of decomposed textual facts .", "entities": []}, {"text": "RKB;ei2Eg be the set of facts in KB .", "entities": []}, {"text": "We collect a set of candidate entity tuples among which KB relation r2 R KBpossibly holds.4Here , all entities in each candidate tuple ( e1;:::;en ) are mentioned in the same text section Tin a given set of documents .", "entities": []}, {"text": "We de\ufb01ne a set of these entity mentions as Otext = fhT;(e1;:::;en)i :", "entities": []}, {"text": "ei2Eis mentioned in Tg .", "entities": []}, {"text": "Here , text section T is a ( short ) span in a document which can describes relational facts among entities .", "entities": []}, {"text": "In the cross - sentence n - ary relation extraction task , text sectionTcan contain multiple sentences .", "entities": [[8, 10, "TaskName", "relation extraction"]]}, {"text": "In this paper , following ( Peng et al . , 2017 ) , we de\ufb01ne Mconsecutive sentences ( M\u00151 ) which containntarget entities as a text section in the crosssentencen - ary relation extraction task .", "entities": [[33, 35, "TaskName", "relation extraction"]]}, {"text": "We use the term \u201c relation \u201d to refer to both relations r2R KB and sections T.", "entities": [[12, 13, "DatasetName", "r2R"]]}, {"text": "The goal of the cross - sentence n - ary relation extraction task is to predict new facts hr;(e1;:::;en)i=2O KBfor relationr2R", "entities": [[10, 12, "TaskName", "relation extraction"]]}, {"text": "KBgiven O = OKB[O text , wheren\u00152 .", "entities": []}, {"text": "3 Proposed Method 3.1 Lower - Arity Facts To alleviate the sparsity problem of facts among nentities ( n > 2 ) and to utilize well - studied encoders for binary and unary surface patterns", "entities": []}, {"text": ", we 4It is allowed that multiple KB relations hold among the same set of entities .", "entities": []}, {"text": "6227decompose a set of original n - ary facts , O , into a set of unary factsO1and a set of binary facts O2 ( Figure 1 ) .", "entities": []}, {"text": "Unary Facts : Given an n - ary fact hr;(e1;:::;ek)i 2 O , we decompose it into a set ofnunary factsfhr(k);eki : k= 1;:::;ng , wherer(k)is a tentative unary relation w.r.t .", "entities": []}, {"text": "the k - th argument of the original relation r. Ifris a KB relation , we de\ufb01ne unary relation r(k)as a new canonicalized relation .", "entities": []}, {"text": "If ris sectionT , we de\ufb01ne unary relation r(k)as a tupler(k)= ( T;pos ( ek ) ) , wherepos(ek)is a set of word position indices of entityekin sectionT(Figure 2 ) .", "entities": []}, {"text": "We denote a set of all decomposed unary facts by O1 .", "entities": []}, {"text": "Intuitively , these unary relations represent semantic roles or types of corresponding arguments of the original relationr(Yao et al . , 2013 ) .", "entities": []}, {"text": "Binary Facts : Given an n - ary fact hr;(e1;:::;ek)i 2 O , we decompose it into a set of n(n\u00001)binary facts fhr(k;l);(ek;el)i :", "entities": []}, {"text": "k;l = 1;:::;n;k6 = lg , wherer(k;l)is a tentative binary relation between thek - th andl - th argument of the original relation r. Ifris a KB relation , we de\ufb01ne binary relation r(k;l)as a new canonicalized relation .", "entities": []}, {"text": "If ris a sectionT , we represent it by the shortest path betweenekandelon the document graph ( Quirk and Poon , 2017 ) of T(Figure 2 ) , and denote it bypath(T;ek;el ) .", "entities": []}, {"text": "We denote the set of all decomposed binary facts by O2 .", "entities": []}, {"text": "3.2 Lower - Arity Relation Representations We learn a vector representation v(r)2Rdrfor each unary or binary relation in O1orO2 .", "entities": []}, {"text": "For r(k)orr(k;l)derived from a KB relation , we represent it by a trainable parameter vector .", "entities": []}, {"text": "On the other hand , for the one derived from a textual relation , we use the following encoders to compute its representations .", "entities": []}, {"text": "Unary encoder : For an unary textual relation r(k)= ( T;pos ( ek ) ) , we represent each section Tby a sequence of word vectors and use a bidirectional LSTM ( Bi - LSTM )", "entities": [[29, 31, "MethodName", "bidirectional LSTM"], [34, 35, "MethodName", "LSTM"]]}, {"text": "( Schuster and Paliwal , 1997 ) to compute a hidden representation hl2Rdrat each word position l.", "entities": []}, {"text": "Following recent works ( Zhang et al . , 2018 ; He et al . , 2018 ; Lee et al . , 2017 ) , we aggregate hlwithin a phrase of entityekto compute v(T(k ) ) .", "entities": []}, {"text": "We use elementwise mean as aggregation function : v(r(k ) )", "entities": []}, {"text": "= mean(fhl : l2pos(ek)g):(1 )", "entities": []}, {"text": "Binary encoder : For a binary textual relationr(k;l)=path(T;ek;el ) , we represent each token ( word or edge label ) in path(T;ek;el)by an embedding vector ( Toutanova et al . , 2015 ; Verga et", "entities": []}, {"text": "al . , 2016 )", "entities": []}, {"text": ".", "entities": []}, {"text": "We use a Bi - LSTM to compute a hidden representation h0 l2Rdrat each token positionl , and max - pool along the path to compute the relation representation : v(T(k;l ) )", "entities": [[5, 6, "MethodName", "LSTM"]]}, {"text": "= max(fh0 l : l= 1;:::;Lg):(2 )", "entities": []}, {"text": "3.3 Learning Relation Representations We follow Verga et al .", "entities": []}, {"text": "( 2017 ) to train relation representations ( x3.2 ) .", "entities": []}, {"text": "We de\ufb01ne a score \u0012hr;pifor each lower - arity fact hr;pi2O 1[O 2 , and minimize the following loss ( 3 ) for each arity i= 1;2 .", "entities": [[18, 19, "MetricName", "loss"]]}, {"text": "Here , placeholder prefers to either an entity ( if hr;pi2O 1 ) or an entity tuple ( if hr;pi2O 2 ) , and we simply refer to both as entity tuple .", "entities": []}, {"text": "The loss functions contrast a score of an original fact hr;p+i 2 Oiand those of Ksampled negative factshr;p\u0000 ki=2Oi .", "entities": [[1, 2, "MetricName", "loss"]]}, {"text": "We sample negative facts by randomly replacing entity tuple p+in the original fact by different entity tuples p\u0000", "entities": []}, {"text": "k.", "entities": []}, {"text": "Li = E hr;p+i2Oi hr;p\u0000 ki=2Oi[\u0000log(exp(\u0012hr;p+i ) exp(\u0012hr;p+i)+P kexp(\u0012hr;p\u0000 ki ) ) ] : ( 3 ) The score of fact hr;piis de\ufb01ned as \u0012hr;pi= v(r)Tv(p;r ) .", "entities": []}, {"text": "Entity tuple representations v(p;r ) are computed with a weighted average of the representationsfv(r0 ) :", "entities": []}, {"text": "r02V(p)gas shown in ( 4 ) and ( 5 ) where a(r0;r;V(p))is the attention weight for each relation r02V(p).5 v(p;r )", "entities": []}, {"text": "= X r02V(p)a(r0;r;V(p))v(r0 ) ; a(r0;r;V(p ) )", "entities": []}, {"text": "= exp(v(r0)Tv(r))P r002V(p)exp(v(r00)Tv(r)):(4 ) V(p ) =(", "entities": []}, {"text": "fr : hr;eki2O 1 g ( ifp = ek ) fr :", "entities": []}, {"text": "hr;(ek;el)i2O 2g(ifp= ( ek;el)):(5 ) 3.4 Aggregating Lower - Arity Scores To predictn - ary facts of KB relation r2R KB , we compute its score \u0012hr;(e1;:::;en)iby aggregating lower - arity scores as in ( 6 ) , where w(\u0001 ) ris a positive scalar weight de\ufb01ned for each KB relation which sum to one :P kw(k ) r+P k6 = lw(k;l ) r=", "entities": [[19, 20, "DatasetName", "r2R"]]}, {"text": "1 . 5During training , as in ( Verga et al . , 2017 ) , we aggregate sub - sampled Mrelations from V(p)ifjV(p)j > M .", "entities": []}, {"text": "We set M= 2for all experiments .", "entities": []}, {"text": "6228We can set all weights w(k ) randw(k;l ) rto1 = n2 , or train these weights to give higher scores to positiven - ary facts by minimizing additional loss functionLn .", "entities": [[29, 30, "MetricName", "loss"]]}, {"text": "Note thatLndirectly contrasts n - ary scores associated with", "entities": []}, {"text": "KB relations r2R KBin a more supervised manner than both L1andL2.6 \u0012hr;(e1;:::;en)i= nX k=1w(k ) r\u0012hr(k);eki+X k=1;:::;n l=1;:::;n k6 = lw(k;l ) r\u0012hr(k;l);(ek;el)i : ( 6 ) Ln=", "entities": [[2, 3, "DatasetName", "r2R"]]}, {"text": "E hr;p+i2O KB hr;p\u0000i=2O KB[max(0;1\u0000\u0012hr;p+i+\u0012hr;p\u0000i ) ] ( 7 ) The overall loss function is now L = L1+L2 + \u000b Ln .", "entities": [[12, 13, "MetricName", "loss"]]}, {"text": "By changing \u000b , we can balance the semisupervised effect of lower - arity universal schemas ( L1;L2 ) and that of the supervision with n - ary relation labels ( Ln ) .", "entities": []}, {"text": "4 Experiments 4.1 Dataset The cross - sentence n - ary relation extraction dataset from Peng et", "entities": [[11, 13, "TaskName", "relation extraction"]]}, {"text": "al .", "entities": []}, {"text": "( 2017 ) contains only 59 distinct ternary KB facts including the train and test set .", "entities": []}, {"text": "Since our proposed method and universal schemas baselines predict KB relations for each entity tuple instead of each surface pattern , the number of known facts of KB relations is crucial to reliably evaluate and compare these methods .", "entities": []}, {"text": "Thus , we created two new n - ary cross - sentence relation extraction datasets ( dubbed with Wiki-90k andWF-20k ) that contain more known facts retrieved from public knowledge bases .", "entities": [[12, 14, "TaskName", "relation extraction"]]}, {"text": "To create the Wiki-90k and WF-20k datasets , we used Wikidata and Freebase respectively as external knowledge bases .", "entities": []}, {"text": "Since these knowledge bases store only binary relational facts , we de\ufb01ned multiple ternary relations by combining a few binary relations.7;8For both datasets , we collected paragraphs from the English Wikipedia , and used Stanford CoreNLP ( Manning et al . , 2014 ) to 6The loss function ( 7 ) performs better than a loglikelihood based loss , \u0000log\u001b(\u0012hr;p+i\u0000\u0012hr;p\u0000i ) .", "entities": [[46, 47, "MetricName", "loss"], [57, 58, "MetricName", "loss"]]}, {"text": "7See the appendix A for details about de\ufb01ned ternary relations .", "entities": []}, {"text": "8For about half of the de\ufb01ned ternary relations , combined original binary relations in KB are different from decomposed binary relations of the ternary relations in the proposed method ( x3.1).extract dependency and co - reference links .", "entities": []}, {"text": "Entity mentions are detected using DBpedia Spotlight ( Daiber et al . , 2013 ) .", "entities": [[5, 6, "DatasetName", "DBpedia"]]}, {"text": "We followed ( Peng et al . , 2017 ) to extract co - occurring entity tuples and their surface patterns , that is , we selected tuples which occurred in a minimal span within at mostM\u00143consecutive sentences .", "entities": []}, {"text": "Entity tuples without a known KB relation are subsampled , since the number of such tuples are too large .", "entities": []}, {"text": "We randomly partitioned all entity tuples into train , development ( dev ) , and test sets .", "entities": []}, {"text": "4.2 Baselines ( Song et al . , 2018 ): The state - of - the - art crosssentencen - ary relation extraction method proposed by Song et al .", "entities": [[21, 23, "TaskName", "relation extraction"]]}, {"text": "( 2018 ) represents each surface pattern by the concatenation of entity vectors from the last layer of a Graph State LSTM , a variant of a graph neural network .", "entities": [[21, 22, "MethodName", "LSTM"]]}, {"text": "The concatenated vector is then fed into a classi\ufb01er to predict the relation label .", "entities": []}, {"text": "Since their method directly predicts a relation label for each surface pattern , it is more robust to the sparsity of surface patterns among a speci\ufb01c higher arity entity tuple .", "entities": []}, {"text": "However , due to their purely supervised training objective , its performance may degrade if the number of available training labels is small .", "entities": []}, {"text": "Universal schemas : We compared our method with semi - supervised methods based on universal schemas ( Toutanova et al . , 2015 ; Verga et al . , 2017 ) .", "entities": []}, {"text": "In our experiments , we used the same encoder as ( Song et al . , 2018 ) to encode each surface pattern.9We tested two types of scoring functions , Model F and Model E , as in ( Toutanova et al . , 2015).10;11 4.3 Evaluation We compared the methods in the held - out evaluation as in ( Mintz et al . , 2009 ) and report ( weighted ) mean average precision ( MAP ) ( Riedel et al . , 2013 ) .", "entities": [[73, 75, "MetricName", "average precision"], [76, 77, "DatasetName", "MAP"]]}, {"text": "Unless otherwise noted , reported values are average values over six experiments , in which network parameters are randomly initialized .", "entities": []}, {"text": "All reported p - values are calculated based on Wilcoxon rank sum test ( Wilcoxon , 1945 ) with 9Using linear projection instead of simple concatenation did not improve performance in our preliminary experiments .", "entities": []}, {"text": "10It is not trivial to apply Model E scoring function with Verga et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2017 ) method , since their aggregation method calculates a representation for each row , i.e. entity tuple .", "entities": []}, {"text": "11DistMult scoring function in ( Toutanova et al . , 2015 ) showed poor performance compared to the other two scoring functions in our preliminary experiments .", "entities": []}, {"text": "6229MethodWiki-90k WF-20k average weighted average weighted Proposed 0.584 0.634 0.821 0.842 ( Song et al . , 2018 ) 0.471 0.536 0.639 0.680 ( Toutanova et al . , 2015 ) with Graph State LSTM ( Song et al . , 2018 ) encoder Model F 0.240 0.262 0.341 0.380 Model E 0.399 0.414 0.725 0.752 ( Verga et al . , 2017 ) with Graph State LSTM ( Song et al . , 2018 ) encoder Model F 0.443 0.482 0.610 0.653 bold : p\u00140:01 Table 1 : Mean average precisions ( MAPs ) on testdata .", "entities": [[34, 35, "MethodName", "LSTM"], [67, 68, "MethodName", "LSTM"]]}, {"text": "multiple - test adjustment using Holm \u2019s method ( Holm , 1979 ) .", "entities": []}, {"text": "4.4 Results Table 1 illustrates the performance of each method.12Compared to the baseline methods , our proposed method achieves higher weighted MAP for both datasets .", "entities": [[21, 22, "DatasetName", "MAP"]]}, {"text": "Interestingly , Model F performs well in Verga et al .", "entities": []}, {"text": "( 2017 ) baseline , while it shows low performance in Toutanova", "entities": []}, {"text": "et al .", "entities": []}, {"text": "( 2015 ) baseline .", "entities": []}, {"text": "Ablation Study : Table 2 illustrates the performance of various settings of our proposed method .", "entities": []}, {"text": "U , B , and Nstand for using the loss functions L1 , L2 , and \u000b Lnrespectively .", "entities": [[9, 10, "MetricName", "loss"]]}, {"text": "In the result , U+B performs signi\ufb01cantly better ( p < 0:005 ) than U andB , and this shows effectiveness of combining scores of both binary facts and unary facts .", "entities": []}, {"text": "On the other hand , there was no signi\ufb01cant difference between U+B+N andN(p > 0:9 ) .", "entities": []}, {"text": "Note that we used all positive labels in this experiment , that is , suf\ufb01cient amount of positive labels are used for calculating the loss N. Data ef\ufb01ciency : Furthermore , we also investigated the in\ufb02uence of the training data size ( the number of positive labels ) of our proposed method and baseline methods.13Here ,", "entities": [[24, 25, "MetricName", "loss"]]}, {"text": "= 1stands for optimizingLninstead ofL1+L2 + \u000b Ln .", "entities": []}, {"text": "As shown in Figure 3 , \u000b = 1 achieved higher performance than \u000b = 1 , showing that introducing lower - arity semi - supervised loss ( L1+L2 ) improves the performance for dataset with few positive labels .", "entities": [[26, 27, "MetricName", "loss"]]}, {"text": "On the other hand , the lower performance of \u000b = 0compared to \u000b = 0:1;1suggests that information of higher - arity facts introduced fromLnis bene\ufb01tial for n - ary relation extraction .", "entities": [[30, 32, "TaskName", "relation extraction"]]}, {"text": "5 Conclusion and Future Works We proposed a new method for cross - sentence nary relation extraction that decomposes sparse n12For the proposed method , we set \u000b = 10 .", "entities": [[15, 17, "TaskName", "relation extraction"]]}, {"text": "13In this experiment , we conducted four experiments per each setting and set K= 10 .Setting average weighted U 0.363 0.404 B 0.411 0.452 N 0.646 0.685 U+B 0.521 0.552 U+B+N 0.646 0.682 U+B+N ( K= 10 ) 0.650 0.689 U+B+N ( K= 5 ) 0.592 0.640 U+B+N ( \ufb01xwr= 1 ) 0.645 0.679 Default hyperparameter : K= 20 Table 2 : Ablation study : mean average precisions ( MAPs ) on devdata ( Wiki-90k ) .", "entities": []}, {"text": "0.0 0.1 0.2 0.3 0.4 Ratio of remaining positive labels in training data0.00.20.40.6Weighted MAP \u03b1= 0 \u03b1= 0.1 \u03b1= 1 \u03b1=\u221eModel F ( Verga et al . , 2017 )", "entities": [[13, 14, "DatasetName", "MAP"], [15, 16, "DatasetName", "0"]]}, {"text": "Model E ( Toutanova et al . , 2015 ) ( Song et al . , 2018 )", "entities": []}, {"text": "Figure 3 : Weighted MAP on test data with missing labels ( Wiki-90k ) .", "entities": [[4, 5, "DatasetName", "MAP"]]}, {"text": "ary facts into dense unary and binary facts .", "entities": []}, {"text": "Experiments on two datasets with multiple ternary relations show that our proposed method can statistically signi\ufb01cantly improve over previous works , which suggests the effectiveness of using unary and binary interaction among entities in surface patterns .", "entities": []}, {"text": "However , as Fatemi et al . ( 2019 ) suggests , there exists cases in which reconstructing n - ary facts from decomposed binary facts induces false positives .", "entities": []}, {"text": "Tackling this issue is one important future research direction .", "entities": []}, {"text": "Acknowledgements We thank all the EMNLP reviewers and Daniel Andrade for their valuable comments and suggestions to improve the paper .", "entities": []}, {"text": "References Joachim Daiber , Max Jakob , Chris Hokamp , and Pablo N. Mendes .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Improving ef\ufb01ciency and accuracy in multilingual entity extraction .", "entities": [[3, 4, "MetricName", "accuracy"]]}, {"text": "In Proceedings of the 9th International Conference on Semantic Systems , I - SEMANTICS \u2019 13 , pages 121 \u2013 124 .", "entities": []}, {"text": "ACM .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Xin Luna Dong , Evgeniy Gabrilovich , Geremy Heitz , Wilko Horn , Ni Lao , Kevin Murphy , Thomas Strohmann , Shaohua Sun , and Wei Zhang .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Knowledge vault : A web - scale approach to probabilistic knowledge fusion .", "entities": []}, {"text": "In The 20th ACM", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "6230SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD \u2019 14 , New York , NY , USA - August 24 - 27 , 2014 , pages 601\u2013610 .", "entities": []}, {"text": "Patrick Ernst , Amy Siu , and Gerhard Weikum .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Highlife :", "entities": []}, {"text": "Higher - arity fact harvesting .", "entities": []}, {"text": "In Proceedings of the 2018 World Wide Web Conference , WWW \u2019 18 , pages 1013\u20131022 .", "entities": []}, {"text": "International World Wide Web Conferences Steering Committee .", "entities": []}, {"text": "Bahare Fatemi , Perouz Taslakian , David Vazquez , and David Poole .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Knowledge hypergraphs : Extending knowledge graphs beyond binary relations .", "entities": [[4, 6, "TaskName", "knowledge graphs"]]}, {"text": "arXiv preprint arXiv:1906.00137 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Luheng He , Kenton Lee , Omer Levy , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Jointly predicting predicates and arguments in neural semantic role labeling .", "entities": [[7, 10, "TaskName", "semantic role labeling"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 364\u2013369 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sture Holm .", "entities": []}, {"text": "1979 .", "entities": []}, {"text": "A simple sequentially rejective multiple test procedure .", "entities": []}, {"text": "Scandinavian journal of statistics , pages 65\u201370 .", "entities": []}, {"text": "Kenton Lee , Luheng He , Mike Lewis , and Luke Zettlemoyer . 2017 .", "entities": []}, {"text": "End - to - end neural coreference resolution .", "entities": [[6, 8, "TaskName", "coreference resolution"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 188\u2013197 . Association for Computational Linguistics .", "entities": []}, {"text": "Hong Li , Sebastian Krause , Feiyu Xu , Andrea Moro , Hans Uszkoreit , and Roberto Navigli .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Improvement of n - ary relation extraction by adding lexical semantics to distant - supervision rule learning .", "entities": [[5, 7, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the International Conference on Agents and Arti\ufb01cial Intelligence - Volume 2 , ICAART 2015 , pages 317\u2013324 .", "entities": []}, {"text": "SCITEPRESS Science and Technology Publications , Lda .", "entities": []}, {"text": "Christopher Manning , Mihai Surdeanu , John Bauer , Jenny Finkel , Steven Bethard , and David McClosky .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The stanford corenlp natural language processing toolkit .", "entities": []}, {"text": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 55\u201360 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mike Mintz , Steven Bills , Rion Snow , and Daniel Jurafsky . 2009 .", "entities": []}, {"text": "Distant supervision for relation extraction without labeled data .", "entities": [[3, 5, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing , pages 1003 \u2013 1011 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nanyun Peng , Hoifung Poon , Chris Quirk , Kristina Toutanova , and Wen - tau Yih . 2017 .", "entities": []}, {"text": "Cross - sentence n - ary relation extraction with graph lstms .", "entities": [[6, 8, "TaskName", "relation extraction"]]}, {"text": "Transactions of the Association for Computational Linguistics , 5:101\u2013115.Chris Quirk and Hoifung Poon . 2017 .", "entities": []}, {"text": "Distant supervision for relation extraction beyond the sentence boundary .", "entities": [[3, 5, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 1 , Long Papers , pages 1171\u20131182 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sebastian Riedel , Limin Yao , Andrew McCallum , and Benjamin M. Marlin .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Relation extraction with matrix factorization and universal schemas .", "entities": [[0, 2, "TaskName", "Relation extraction"]]}, {"text": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 74\u201384 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "M. Schuster and K.K. Paliwal .", "entities": []}, {"text": "1997 .", "entities": []}, {"text": "Bidirectional recurrent neural networks .", "entities": []}, {"text": "IEEE Transactions on Signal Processing , 45(11):2673\u20132681 .", "entities": []}, {"text": "Linfeng Song , Yue Zhang , Zhiguo Wang , and Daniel Gildea .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "N - ary relation extraction using graphstate lstm .", "entities": [[3, 5, "TaskName", "relation extraction"], [7, 8, "MethodName", "lstm"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2226\u20132235 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kristina Toutanova , Danqi Chen , Patrick Pantel , Hoifung Poon , Pallavi Choudhury , and Michael Gamon . 2015 .", "entities": []}, {"text": "Representing text for joint embedding of text and knowledge bases .", "entities": []}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1499\u20131509 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Patrick Verga , David Belanger , Emma Strubell , Benjamin Roth , and Andrew McCallum . 2016 .", "entities": []}, {"text": "Multilingual relation extraction using compositional universal schema .", "entities": [[1, 3, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 886\u2013896 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Patrick Verga , Arvind Neelakantan , and Andrew McCallum . 2017 .", "entities": []}, {"text": "Generalizing to unseen entities and entity pairs with row - less universal schema .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 1 , Long Papers .", "entities": []}, {"text": "Frank Wilcoxon .", "entities": []}, {"text": "1945 .", "entities": []}, {"text": "Individual comparisons by ranking methods .", "entities": []}, {"text": "Biometrics bulletin , 1(6):80\u201383 .", "entities": []}, {"text": "Kun Xu , Siva Reddy , Yansong Feng , Songfang Huang , and Dongyan Zhao .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Question answering on freebase via relation extraction and textual evidence .", "entities": [[0, 2, "TaskName", "Question answering"], [5, 7, "TaskName", "relation extraction"]]}, {"text": "InProceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2326\u20132336 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Limin Yao , Sebastian Riedel , and Andrew McCallum .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Universal schema for entity type prediction .", "entities": [[4, 6, "TaskName", "type prediction"]]}, {"text": "InProceedings of the 2013 Workshop on Automated", "entities": []}, {"text": "6231Knowledge Base Construction , AKBC \u2019 13 , pages 79\u201384 , New York , NY , USA . ACM .", "entities": [[18, 19, "DatasetName", "ACM"]]}, {"text": "Yuhao Zhang , Peng Qi , and Christopher D. Manning .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Graph convolution over pruned dependency trees improves relation extraction .", "entities": [[1, 2, "MethodName", "convolution"], [7, 9, "TaskName", "relation extraction"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2205\u20132215 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}]