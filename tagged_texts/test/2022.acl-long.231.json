[{"text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1 : Long Papers , pages 3268 - 3283 May 22 - 27 , 2022 c", "entities": []}, {"text": "2022 Association for Computational Linguistics Right for the Right Reason : Evidence Extraction for Trustworthy Tabular Reasoning Vivek Gupta1\u0003 , Shuo Zhang2 , Alakananda Vempala2 , Yujie He2 , Temma Choji2 , Vivek Srikumar1 1University of Utah,2Bloomberg , { vgupta , svivek}@cs.utah.edu , { szhang611 , avempala , yhe247 , tchoji}@bloomberg.net Abstract When pre - trained contextualized embeddingbased models developed for unstructured data are adapted for structured tabular data , they perform admirably .", "entities": []}, {"text": "However , recent probing studies show that these models use spurious correlations , and often predict inference labels by focusing on false evidence or ignoring it altogether .", "entities": []}, {"text": "To study this issue , we introduce the task of Trustworthy Tabular Reasoning , where a model needs to extract evidence to be used for reasoning , in addition to predicting the label .", "entities": []}, {"text": "As a case study , we propose a twostage sequential prediction approach , which includes an evidence extraction and an inference stage .", "entities": []}, {"text": "First , we crowdsource evidence row labels and develop several unsupervised and supervised evidence extraction strategies for I NFOTABS , a tabular NLI benchmark .", "entities": []}, {"text": "Our evidence extraction strategy outperforms earlier baselines .", "entities": []}, {"text": "On the downstream tabular inference task , using only the automatically extracted evidence as the premise , our approach outperforms prior benchmarks .", "entities": []}, {"text": "1 Introduction Reasoning on tabular or semi - structured knowledge is a fundamental challenge for today \u2019s natural language processing ( NLP ) systems .", "entities": []}, {"text": "Two recently created tabular Natural language Inference ( NLI ) datasets , TabFact ( Chen et al . , 2020b ) on Wikipedia relational tables and INFOTABS(Gupta et al . , 2020 ) on Wikipedia Infoboxes help study the question of inferential reasoning over semi - structured tables .", "entities": [[4, 7, "TaskName", "Natural language Inference"], [12, 13, "DatasetName", "TabFact"]]}, {"text": "Today \u2019s state - of - the - art for NLI over unstructured text uses contextualized embeddings ( e.g. , Devlin et", "entities": []}, {"text": "al . , 2019 ; Liu et al . , 2019b ) .", "entities": []}, {"text": "When adapted for tabular NLI by \ufb02attening tables into synthetic sentences using heuristics , these models achieve remarkable performance on the datasets .", "entities": []}, {"text": "However , a recent study ( Gupta et al . , 2021 ) demonstrates that these models fail to reason prop\u0003Work done during an internship at BloombergBreakfast in America Relevant Released429 March 19794H3 Recorded3;4May - December 19783;4H2 , H3 Studio", "entities": []}, {"text": "The Village Recorder in Los Angeles3 Genre Pop , Art Rock , Soft Rock Length246:062H1 Label A&M Producer1Peter Henderson , Supertramp1H1 H1 : Supertramp produced1an album that was less than an hour long2 .", "entities": []}, {"text": "H2 :", "entities": []}, {"text": "Most of Breakfast in America was recorded3 in the last month of 19783 .", "entities": [[2, 3, "DatasetName", "Breakfast"]]}, {"text": "H3 :", "entities": []}, {"text": "Breakfast in America was released4the same month recording ended4 .", "entities": [[0, 1, "DatasetName", "Breakfast"]]}, {"text": "Figure 1 : A semi - structured premise ( the table \u2018 Breakfast in America \u2019 ) example from ( Gupta et al . , 2020 ) .", "entities": [[12, 13, "DatasetName", "Breakfast"]]}, {"text": "Hypotheses H1 are entailed by it , H2 is neither entailed nor contradictory , and H3 is a contradiction .", "entities": []}, {"text": "The Relevant column shows the hypotheses that use the corresponding row .", "entities": []}, {"text": "The colored text ( and superscripts ) in the table and hypothesis highlights relevance token level alignment .", "entities": []}, {"text": "erly on the semi - structured inputs in many cases .", "entities": []}, {"text": "For example , they can ignore relevant rows , and ( a ) focus on the irrelevant rows ( Neeraja et al . , 2021 ) , ( b ) use only the hypothesis sentence ( Poliak et al . , 2018 ; Gururangan et al . , 2018 ) , or ( c ) knowledge acquired during pre - training ( Jain et al . , 2021 ; Gupta et al . , 2021 ) .", "entities": []}, {"text": "In essence , they use spurious correlations between irrelevant rows , the hypothesis , and the inference label to predict labels .", "entities": []}, {"text": "This paper argues that existing NLI systems optimized solely for label prediction can not be trusted .", "entities": []}, {"text": "It is not suf\ufb01cient for a model to be merely Right", "entities": []}, {"text": "but also Right for the Right Reasons .", "entities": []}, {"text": "In particular , at least identifying the relevant elements of inputs as the \u2018 Right Reasons \u2019 is essential for trustworthy reasoning1 .", "entities": []}, {"text": "We address this issue by introducing 1We argue that a reasoning system can be deemed trustworthy only if it exposes how its decisions are made , thus admitting veri\ufb01cation of the reasons for its decisions.3268", "entities": []}, {"text": "the task of Trustworthy Tabular Inference , where the goal is to extract relevant rows as evidence and predict inference labels .", "entities": []}, {"text": "To illustrate this task , consider an example from theINFOTABSdataset in Figure 1 , which shows a premise table and three hypotheses .", "entities": []}, {"text": "The \ufb01gure also marks the rows needed to make decisions about each hypothesis , and also indicates the relevant tokens for each hypothesis .", "entities": []}, {"text": "For trustworthy tabular reasoning , in addition to predicting the label ENTAIL forH1,CONTRADICT forH2andNEUTRAL forH3 , the model should also identify the evidence rows \u2014 namely , the rows Producer and Length for hypothesis H1,Recorded for hypothesis H2,Released andRecorded for hypothesis H3 .", "entities": []}, {"text": "As a \ufb01rst step , we propose a two - stage sequential prediction approach for the task , comprising of an evidence extraction stage , followed by an inference stage .", "entities": []}, {"text": "In the evidence extraction stage , the model extracts the necessary information needed for the second stage .", "entities": []}, {"text": "In the inference stage , the NLI model uses only the extracted evidence as the premise for the label prediction task .", "entities": []}, {"text": "We explore several unsupervised evidence extraction approaches for INFOTABS .", "entities": [[8, 9, "DatasetName", "INFOTABS"]]}, {"text": "Our best unsupervised evidence extraction method outperforms a previously developed baseline by 4.3 % , 2.5 % and 5.4 % absolute score on the three test sets .", "entities": []}, {"text": "For supervised evidence extraction , we annotate the INFOTABStraining set ( 17 K table - hypothesis pairs with 1740 unique tables ) with relevant rows following the methodology of Gupta et al . ( 2021 ) , and then train a RoBERTa LARGE classi\ufb01er .", "entities": [[41, 42, "MethodName", "RoBERTa"]]}, {"text": "The supervised model improves the evidence extraction performance by 8.7 % , 10.8 % , and 4.2 % absolute scores on the three test sets over the unsupervised approach .", "entities": []}, {"text": "Finally , for the full inference task , we demonstrate that our two - stage approach with best extraction , outperforms the earlier baseline by 1.6 % , 3.8 % , and 4.2 % on the three test sets .", "entities": []}, {"text": "In summary , our contributions are as follows2 :", "entities": []}, {"text": "\u2022We introduce the problem of trustworthy tabular reasoning and study a two - stage prediction approach that \ufb01rst extracts evidence and then predicts the NLI label .", "entities": []}, {"text": "\u2022We investigate a variety of unsupervised evidence extraction techniques .", "entities": []}, {"text": "Our unsupervised approach for evidence extraction outperforms the previous methods .", "entities": []}, {"text": "2The updated dataset , along with associated code , is available athttps://tabevidence.github.io/ .\u2022We", "entities": []}, {"text": "enrich the INFOTABStraining set with evidence rows , and develop a supervised extractor that has near - human performance .", "entities": []}, {"text": "\u2022We demonstrate that our two - stage technique with best extraction outperforms all the prior benchmarks on the downstream NLI task .", "entities": []}, {"text": "2 Task Formulation We begin by introducing the task and the datasets we use .", "entities": []}, {"text": "Tabular Inference is a reasoning task that , like conventional NLI ( Dagan et al . , 2013 ; Bowman et al . , 2015 ; Williams et al . , 2018 ) , asks whether a natural language hypothesis can be inferred from a tabular premise .", "entities": []}, {"text": "Concretely , given a premise table T withmrowsfr1;r2;:::;rmg , and a hypothesis sentence H , the task maps them to ENTAIL ( E ) , CONTRADICT ( C ) orNEUTRAL ( N ) .", "entities": []}, {"text": "We can denote the mapping as f(T;H)!y ( 1 ) where , y2fE , N , Cg .", "entities": []}, {"text": "For example , for the tabular premise in Figure 1 , the model should predict E , C , andNfor the hypotheses H1,H2 , and H3 , respectively .", "entities": []}, {"text": "Trustworthy Tabular Inference is a table reasoning problem that seeks not just the NLI label , but also relevant evidence from the input table that supports the label prediction .", "entities": []}, {"text": "We use TR , asubset of T , to denote the relevant rows or evidence .", "entities": []}, {"text": "Then , the task is de\ufb01ned as follows .", "entities": []}, {"text": "f(T;H)!fTR;yg ( 2 ) In our example table , this task will also indicate the evidence rows TRofProducer andLength for hypothesis H1,Recorded for hypothesis H2 , and Released andRecorded for hypothesis H3 .", "entities": []}, {"text": "While the notion of evidence is well - de\ufb01ned for theENTAIL andCONTRADICT labels , the NEUTRAL label requires explanation .", "entities": []}, {"text": "To decide on the NEUTRAL label , one must \ufb01rst search for relevant rows ( if any ) , i.e. , identify evidence in the premise tables .", "entities": []}, {"text": "In fact , this is a causally correct sequential approach .", "entities": []}, {"text": "Indeed , INFOTABShas multiple neutral hypotheses that are partly entailed by the table ; if any part of a hypothesis contradicts the table , then the inference label should be CONTRADICT .", "entities": []}, {"text": "For example , in our example table , the premise table indicates that the album was recorded in 1978 , emphasizing the importance of the Recorded row for3269", "entities": []}, {"text": "the hypothesis H2 .", "entities": []}, {"text": "For NEUTRAL examples , we refer to any such pertinent rows as evidence .", "entities": []}, {"text": "Dataset Details .", "entities": []}, {"text": "There are several datasets for tabular NLI : TabFact , INFOTABS , and the SemEval\u201921 Task 9 ( Wang et al . , 2021b ) and the FEVEROUS\u201921 shared task ( Aly et al . , 2021 ) datasets .", "entities": [[8, 9, "DatasetName", "TabFact"], [10, 11, "DatasetName", "INFOTABS"]]}, {"text": "We use the INFOTABSdata in this work .", "entities": []}, {"text": "It contains \ufb01ner - grained annotation ( e.g. , TabFact lacks NEUTRAL hypotheses ) and more complex reasoning than the others3 .", "entities": [[9, 10, "DatasetName", "TabFact"]]}, {"text": "The dataset consists of 23;738", "entities": []}, {"text": "premisehypothesis pairs collected via crowdsourcing on Amazon MTurk .", "entities": []}, {"text": "The tabular premises are based on2;540Wikipedia Infoboxes representing twelve diverse domains , and the hypotheses are short statements paired with NLI labels .", "entities": []}, {"text": "All tables contain atitlefollowed by two columns ( cf .", "entities": []}, {"text": "Figure 1 ) ; the left columns are keys and the right ones are values ) .", "entities": []}, {"text": "In addition to the train and development sets , the data includes multiple test sets , some of which are adversarial : \u000b 1represents a standard test set that is both topically and lexically similar to the training data ; \u000b 2hypotheses are designed to be lexically adversarial4 ; and \u000b 3tables are drawn from topics unavailable in the training set .", "entities": []}, {"text": "The dev and test set , comprising of 7200 table - hypothesis pairs , were recently extended with crowdsourced evidence rows ( Gupta et al . , 2021 ) .", "entities": []}, {"text": "As one of our contributions , we describe the evidence rows annotation for the training set in the next Section 3 . 3 Crowdsource Evidence Extraction This section describes the process of using Amazon MTurk to annotate evidence rows for the 16;538 premise - hypothesis pairs that make the training set ofINFOTABS .", "entities": []}, {"text": "We followed the protocol of Gupta et al .", "entities": []}, {"text": "( 2021 ): one table and three distinct hypotheses formed a HIT .", "entities": []}, {"text": "For each of the hypotheses , \ufb01ve annotators would select the evidence rows .", "entities": []}, {"text": "We divide the tasks equally into 110 batches , each batch having 51 HITs each having three examples .", "entities": []}, {"text": "To reduce bias induced by a link between the NLI label and row selection , we do not reveal the labels to the annotators .", "entities": []}, {"text": "The quality control details are provided in the Appendix \u00a7 B. In total , we collected 81,282 annotations from 3As per Gupta et al .", "entities": []}, {"text": "( 2020 ) , 33 % of examples in INFOTABS involve multiple rows .", "entities": [[9, 10, "DatasetName", "INFOTABS"]]}, {"text": "The dataset covers all the reasoning types present in the Glue ( Wang et al . , 2018 ) and SuperGlue ( Wang et al . , 2019 ) benchmarks.4i.e .", "entities": [[20, 21, "DatasetName", "SuperGlue"]]}, {"text": "minimally perturbing hypothesis to \ufb02ipped ENTAIL toCONTRADICT label and vice - versa .", "entities": []}, {"text": "Agreement Range Percentage ( % ) Poor < 0 0.27 Slight 0.01 \u2013 0.20 1.61 Fair 0.21 \u2013 0.40 5.69 Moderate 0.41 - 0.60 13.89 Substantial 0.61 - 0.80 22.92 Perfect 0.81 - 1.00 55.61 Overall mean 0.79 s.t.d . 0.23 Table 1 : Examples ( % ) for each Fleiss \u2019 Kappa score bucket .", "entities": [[8, 9, "DatasetName", "0"]]}, {"text": "90 distinct annotators .", "entities": []}, {"text": "Overall , twenty \ufb01ve annotators completed over 1000 tasks , corresponding to 87.75 % of the examples , indicating a tail distribution with the annotations .", "entities": []}, {"text": "Overall , 16,248 training set table - hypothesis pairs were successfully labeled with the evidence rows5 .", "entities": []}, {"text": "On average , we obtain 89.49 % F1 - score with equal precision and recall for annotation agreement when compared with majority vote .", "entities": [[7, 10, "MetricName", "F1 - score"]]}, {"text": "Furthermore , 85 % examples have an F1 - score of > 80 % , and 62 % examples have an F1 - score of > 90 % .", "entities": [[7, 10, "MetricName", "F1 - score"], [21, 24, "MetricName", "F1 - score"]]}, {"text": "Around 60 % examples have either perfect ( 100 % ) precision or recall , and 42 % have both .", "entities": []}, {"text": "Table 1 reports the Fleiss \u2019 Kappa score with annotation percentage .", "entities": []}, {"text": "The average Kappa score is 0.79 with standard deviation of 0.236 .", "entities": []}, {"text": "Choice of Semi - structured Data .", "entities": []}, {"text": "The rows of an Infobox table are semantically distinct , though all connected to the title entity .", "entities": []}, {"text": "Each row can be considered a separate and uniquely distinct source of information about the title entity .", "entities": []}, {"text": "Because of this property , the problem of evidence extraction is well - formed as relevant row selection .", "entities": []}, {"text": "The same is not valid for unstructured text , whose units of information may be tokens , phrases , sentences or entire paragraphs , and is typically unavailable ( Ribeiro et al . , 2020 ; Goel et", "entities": []}, {"text": "al . , 2021 ; Mishra et al . , 2021 ; Yin et al . , 2021 ) .", "entities": []}, {"text": "4 Trustworthy Tabular Inference Trustworthy inference has an intrinsic sequential causal structure : extract evidence \ufb01rst , then predict the inference label using the extracted evidence data , knowledge / common sense , and perhaps formal reasoning ( Herzig et al . , 2021 ;", "entities": []}, {"text": "Paranjape et al . , 2020)7 .", "entities": []}, {"text": "To operationalize this intuition , we chose a two - stage sequential approach which consists of an evidence extraction followed by the NLI classi5We exclude certain example pairings from our training sets since they could not achieve satisfactory agreement after adding more annotators or have label imbalance issues i.e. more the required number of neutrals.6We also manually examined hypothesis phrases that signal relevant rows .", "entities": []}, {"text": "See Appendix D for details.7See more details discussion in $ 7.3270", "entities": []}, {"text": "Figure 2 : High level \ufb02owchart showing our approach for evidence extraction and trustworthy tabular inference .", "entities": []}, {"text": "\ufb01cation , as shown in Figure 2 . Notation .", "entities": []}, {"text": "The function fin Eq . 2 can be rewritten with functions gandh , f (: )", "entities": []}, {"text": "= g(:),h\u000eg (: ) , as f(T;H )", "entities": []}, {"text": "= fg(T;H),h(g(T;H);H)g(3 )", "entities": []}, {"text": "Here , gextracts the evidence rows TRsubset of T , andhuses the extracted evidence TRand the hypothesis H to predict the inference label y , as g(T;H)!TR h(TR;H)!y(4 )", "entities": []}, {"text": "To obtainf , we need to de\ufb01ne the functions gand h , and a \ufb02exible representation of a semi - structured table T. To represent a table T , we use the Better Paragraph Representation ( BPR ) heuristic of Neeraja et", "entities": []}, {"text": "al . ( 2021 ) .", "entities": []}, {"text": "BPR uses hand - crafted rules based on the table category and entity type \u2019s of the row values ( e.g. , boolean and date ) to convert each row to a sentence , consisting of table title , key and values .", "entities": []}, {"text": "This representation outperforms the original \u201c para \u201d representation technique of Gupta et al .", "entities": []}, {"text": "( 2020 ) .", "entities": []}, {"text": "We explore unsupervised ( $ 4.1 ) and supervised ( $ 4.2 ) methods for the evidence row extractor g. 4.1 Unsupervised Evidence Extraction The unsupervised approaches extract Top - K rows are based on relevance scores , where K is a hyperparameter .", "entities": []}, {"text": "We use the cosine similarity between the row and the hypothesis sentence representations to score rows .", "entities": []}, {"text": "We study three ways to de\ufb01ne relevance described next .", "entities": []}, {"text": "4.1.1", "entities": []}, {"text": "Using Static Embeddings Inspired by the Distracting RowRemoval ( DRR ) heuristic of Neeraja et al .", "entities": [[3, 4, "DatasetName", "Inspired"]]}, {"text": "( 2021 ) , we propose DRR ( Re - Rank + Top - S \u001c ) , which uses fastText ( Joulinet al . , 2016 ; Mikolov et", "entities": [[20, 21, "MethodName", "fastText"]]}, {"text": "al . , 2018 ) based static embeddings to measure sentence similarity .", "entities": []}, {"text": "We employ three modi\ufb01cations to improve DRR .", "entities": []}, {"text": "Re - Rank ( \u000e):We observed that the raw similarity scores ( i.e. , using only fastText ) for some valid evidence rows could be low , despite exact wordlevel lexical matching with the row \u2019s keyandvalues .", "entities": [[16, 17, "MethodName", "fastText"]]}, {"text": "We augmented the scores by \u000efor each exact match to incentivize precise matches .", "entities": [[7, 9, "MetricName", "exact match"]]}, {"text": "Sparse Extraction ( S ): For most instances , the number of relevant rows ( K ) is much lower than the total number of rows ( m ) ; most examples have only one or two relevant rows .", "entities": []}, {"text": "We constrained the sparsity in the extraction by capping the value of K to S \u001c m. Dynamic Selection ( \u001c ): We use a threshold \u001c  to select rows dynamically Top - K \u001c based on the hypothesis , rather than always selecting \ufb01xed K rows .", "entities": []}, {"text": "We only select rows whose similarity ( after Re - Ranking ) to the hypothesis sentence representations is greater than a threshold \u001c .", "entities": []}, {"text": "We adopt this strategy because ( a ) the number of rows in the premise table can vary across examples , and ( b ) different hypotheses may require a differing number of evidence rows .", "entities": []}, {"text": "4.1.2 Using Word Alignments This approach consists of two parts ( a ) aligning rows and hypothesis words , and ( b ) then computing cosine similarity between the aligned words .", "entities": []}, {"text": "Speci\ufb01cally , we use the SimAlign ( Jalili Sabet et al . , 2020 ) method for word - level alignment .", "entities": []}, {"text": "SimAlign uses static and contextualized embeddings without parallel training data to get word alignments .", "entities": []}, {"text": "Among the approaches explored by SimAlign , we use the Match ( mwmf ) method , which uses maximum- weight maximal matching in the bipartite weighted network formed by the word level similarity matrix .", "entities": []}, {"text": "Our choice of this approach over the other greedy methods ( Itermax and Argmax ) is motivated by the fact that it \ufb01nds the global optimum matching , while the other two do not .", "entities": []}, {"text": "After alignment , we normalize the sum of cosine similarities of RoBERTa LARGE token", "entities": [[11, 12, "MethodName", "RoBERTa"]]}, {"text": "embeddings8to derive the relevance score .", "entities": []}, {"text": "Furthermore , because all rows use the same title , we assign title matching terms zero weight .", "entities": []}, {"text": "This paper refers to this method as SimAlign ( Match ( mwmf ) ) .", "entities": []}, {"text": "8We use the average BPE token embeddings as the word embeddings.3271", "entities": [[4, 5, "MethodName", "BPE"]]}, {"text": "4.1.3 Using Contextualised Embeddings The approach we saw in $ 4.1.2 de\ufb01nes rowhypothesis similarity using word alignments .", "entities": []}, {"text": "As an alternative , we can directly compute similarities between the contextualised sentence embeddings of rows and the hypothesis .", "entities": [[12, 14, "TaskName", "sentence embeddings"]]}, {"text": "We explore two options here .", "entities": []}, {"text": "Sentence Transformer : We use Sentence - BERT ( Reimers and Gurevych , 2019 ) and its variants ( Reimers and Gurevych , 2020 ; Thakur et al . , 2021 ; Wang et", "entities": [[1, 2, "MethodName", "Transformer"], [7, 8, "MethodName", "BERT"]]}, {"text": "al . , 2021a ) , which use Siamese neural networks ( Koch et al . , 2015 ; Chicco , 2021 ) .", "entities": []}, {"text": "We explore several pre - trained sentence transformers models9for sentence representation .", "entities": []}, {"text": "These models differ in ( a ) the data used for pre - training , ( b ) the main model type and it size , and ( c ) the maximum sequence length .", "entities": []}, {"text": "SimCSE : SimCSE ( Gao et al . , 2021 ) uses a contrastive learning to train sentence embeddings in both unsupervised and supervised settings .", "entities": [[0, 1, "MethodName", "SimCSE"], [2, 3, "MethodName", "SimCSE"], [13, 15, "MethodName", "contrastive learning"], [17, 19, "TaskName", "sentence embeddings"]]}, {"text": "The former is trained to take an input sentence and reconstruct it using standard dropout as noise .", "entities": []}, {"text": "The latter uses example pairs from the MNLI dataset ( Williams et al . , 2018 ) with entailments serving as positive examples and contradiction serving as hard negatives for contrastive learning .", "entities": [[7, 8, "DatasetName", "MNLI"], [30, 32, "MethodName", "contrastive learning"]]}, {"text": "We give the row sentences directly to SimCSE to get their embeddings .", "entities": [[7, 8, "MethodName", "SimCSE"]]}, {"text": "To avoid misleading matches between the hypothesis tokens and those in the premise title , we swap the hypothesis title tokens with a single token title from another randomly selected table of the same category .", "entities": []}, {"text": "We then use the cosine similarity between SimCSE sentence embeddings to compute the \ufb01nal relevance score .", "entities": [[7, 8, "MethodName", "SimCSE"], [8, 10, "TaskName", "sentence embeddings"]]}, {"text": "We again use the sparsity and dynamic selection as earlier .", "entities": []}, {"text": "In the study , we refer to this method as SimCSE ( Hypo - Title - Swap + Re - rank + Top - K \u001c ) .", "entities": [[10, 11, "MethodName", "SimCSE"]]}, {"text": "4.2 Supervised Evidence Extraction The supervised evidence extraction procedure consists of three aspects : ( a ) Dataset construction , ( b ) Label balancing , and ( c ) Classi\ufb01er training .", "entities": []}, {"text": "Dataset Construction .", "entities": []}, {"text": "We use the annotated relevant row data ( $ 3 ) to construct a supervised extraction training dataset .", "entities": []}, {"text": "Every row in the table , paired with the hypothesis , is associated with a binary label signifying whether the row is relevant or not .", "entities": []}, {"text": "As before , we use the sentences from Better 9https://www.sbert.netParagraph Representation ( BPR ) ( Neeraja et al . , 2021 ) to represent each row .", "entities": []}, {"text": "Label Balancing .", "entities": []}, {"text": "Our annotation , and the perturbation probing analysis of Gupta et al .", "entities": []}, {"text": "( 2021)10 , show that the number of irrelevant rows can be much larger than the relevant ones for a tablehypothesis pair .", "entities": []}, {"text": "Therefore , if we use all irrelevant rows from tables as negative examples , the resulting training set would be imbalanced , with about 6\u0002more irrelevant rows than relevant rows .", "entities": []}, {"text": "We investigate several label balancing strategies by sub - sampling irrelevant rows for training .", "entities": []}, {"text": "We explore the following schemes : ( a ) taking all irrelevant rows from the table without sub - sampling ( on average 6\u0002more irrelevant rows ) referred to as Without Sample ( 6\u0002 ) , ( b ) randomly sampling unrelated rowsin the same proportion as relevant rows , referred to as Random Negative ( 1\u0002 ) , ( c ) using the unsupervised DRR ( Re - Rank + Top - S \u001c ) method to pick the irrelevant rows that are most similar to the hypothesis , in equal proportion as the relevant rows , referred to as Hard Negative ( 1\u0002 ) , and ( d ) same as ( c ) , except picking three times as many irrelevant rows , referred to as Hard Negative ( 3\u0002)11 .", "entities": []}, {"text": "Classi\ufb01er Training .", "entities": []}, {"text": "We train a relevant - vsirrelevant row classi\ufb01er using RoBERTa LARGE \u2019s two sentence classi\ufb01er .", "entities": [[9, 10, "MethodName", "RoBERTa"]]}, {"text": "We use RoBERTa LARGE because of its superior performance over other models in preliminary experiments , and also the fact that it is also used for the NLI classi\ufb01er .", "entities": [[2, 3, "MethodName", "RoBERTa"]]}, {"text": "4.3 Natural Language Inference For the downstream NLI task , the function his a two - sentence classi\ufb01er whose inputs are TR(the rows selected by g ) and the hypothesis H. We use BPR to represent TRas we did for the full table T. Since |TR| \u001c |T| , the extraction bene\ufb01ts larger tables ( especially in \u000b 3set ) which exceed the model \u2019s token limit .", "entities": [[1, 4, "TaskName", "Natural Language Inference"]]}, {"text": "5 Experimental Evaluation Our experiments assess the ef\ufb01cacy of evidence extraction ( $ 4 ) and its impact on the downstream NLI task by studying the following questions : \u2022RQ1 : What is the ef\ufb01cacy of unsupervised approaches for evidence extraction ?", "entities": []}, {"text": "( $ 5.2 ) 10Tabular probing using row deletion , row - value updation , row permutation , and row insertion.11We explored other selection ratios too , take rows with rank till 5\u0002,2\u0002 , and 4\u0002 , but discovered that their performance is equivalent to ( a ) , ( b ) , and ( c ) respectively.3272", "entities": []}, {"text": "Category Unsupervised Methods \u000b 1 \u000b 2 \u000b 3 Baseline WMD ( Gupta et al . , 2020 ) 29.42 30.13 28.23 DRR ( Neeraja et al . , 2021 ) 33.36 35.72 33.38 Static Embedding DRR ( Re - Rank + Top-2 ( \u001c = 1 ) ) 71.49 73.28 63.41 Alignment SimAlign ( Match ( mwmf ) ) 58.98 61.53 66.33 Sentence - Transformer ( paraphrase - mpnet - base - v2 ) 67.37 69.88 63.36 Contextualised SimCSE - Unsupervised ( Hypo - Title - Swap + Re - Rank + Top-2 ( \u001c = 1))72.93 70.88 66.33 Embedding SimCSE - Supervised ( Hypo - Title - Swap + Re - Rank + Top-2 ( \u001c = 1 ) ) 75.79 75.74 68.81 Human Oracle ( Gupta et al . , 2021 ) 88.62 89.23 88.56 Table 2 : F1 - scores of the unsupervised evidence extraction methods .", "entities": [[65, 66, "MethodName", "Transformer"], [69, 70, "MethodName", "mpnet"], [79, 80, "MethodName", "SimCSE"], [101, 102, "MethodName", "SimCSE"], [141, 142, "MetricName", "F1"]]}, {"text": "\u2022RQ2 : Is supervision bene\ufb01cial ?", "entities": []}, {"text": "Is it helpful to use hard negatives from unsupervised approaches for supervised training ?", "entities": []}, {"text": "( $ 5.2 ) .", "entities": []}, {"text": "\u2022RQ3 : Does evidence extraction enhance the downstream tabular inference task ?", "entities": []}, {"text": "( $ 5.3 ) 5.1 Experimental Setup First , we brie\ufb02y summarize the models used in our experiments .", "entities": []}, {"text": "We investigate both unsupervised ( $ 4.1 ) and supervised ( $ 4.2 ) evidence extraction methods .", "entities": []}, {"text": "We use only the extracted evidence as the premise for the tabular inference task ( $ 4.3 ) .", "entities": []}, {"text": "We compare both tasks against human performance .", "entities": []}, {"text": "As baselines , we use the WordMover Distance ( WMD ) of Gupta et al .", "entities": []}, {"text": "( 2020 ) and the original DRR ( Neeraja et al . , 2021 ) with Top-4 extracted evidence rows .", "entities": []}, {"text": "For DRR ( Re - Rank + Top - S \u001c ) , which uses static embeddings , we set the sparsity parameterS= 2 , and the dynamic row selection parameter \u001c = 1:0 .", "entities": []}, {"text": "Our choice of S is based on the observation that in INFOTABSmost ( 92 % ) instances have only one ( 54 % ) or two ( 38 % ) relevant rows .", "entities": []}, {"text": "We set \u000eto0:5for all experiments .", "entities": []}, {"text": "For the Sentence Transformer , we used the paraphrase - mpnet - base v2 model ( Reimers and Gurevych , 2019 ) which is a pre - trained with the mpnet - base architecture using several existing paraphrase datasets .", "entities": [[3, 4, "MethodName", "Transformer"], [10, 11, "MethodName", "mpnet"], [30, 31, "MethodName", "mpnet"]]}, {"text": "This choice is based on performance on the development set .", "entities": []}, {"text": "Both the supervised and unsupervised SimCSE models use the same parameters as DRR ( Re - Rank + Top - K \u001c ) .", "entities": [[5, 6, "MethodName", "SimCSE"]]}, {"text": "We refer to the supervised and unsupervised variants as SimCSE - Supervised and SimCSEUnsupervised respectively .", "entities": [[9, 10, "MethodName", "SimCSE"]]}, {"text": "For the NLI task , we use the BPR representation over extracted evidence TRwith the RoBERTa LARGE two sentence classi\ufb01cation model .", "entities": [[15, 16, "MethodName", "RoBERTa"]]}, {"text": "We compare the following settings : ( a ) WMD Top3 from Gupta et", "entities": []}, {"text": "al . ( 2020 ) , ( b ) No extraction i.e. using the full premise table with the \u201c para \u201d representation from Gupta et al .", "entities": []}, {"text": "( 2020 ) , ( c ) DRR Top-4 , ( d ) DRR ( Re - Rank + Top-2 ( \u001c = 1 ) ) for training , de - velopment and test sets , ( e ) training a supervised classi\ufb01er with a human oracle i.e. annotated evidence extraction as discussed in $ 3 , and using the best extraction model , i.e. supervised evidence extraction with Hard Negative ( 3\u0002 ) for the test sets , and ( f ) the human oracle across the training , development , and test sets .", "entities": []}, {"text": "5.2 Results of Evidence Extraction Unsupervised evidence extraction .", "entities": []}, {"text": "For RQ1 , Table 2 shows the performance of unsupervised methods .", "entities": []}, {"text": "We see that the contextual embedding method , SimCSE - Supervised ( Hypo - Title - Swap + Re - Rank + Top-2 ( \u001c = 1 ) ) , performs the best .", "entities": [[8, 9, "MethodName", "SimCSE"]]}, {"text": "Among the static embedding cases , DRR ( Re - Rank + Top2 ( \u001c = 1 ) ) sees substantial performance improvement over the original DRR baseline .", "entities": []}, {"text": "The alignment based approach using SimAlign underperforms , especially on the \u000b 1and \u000b 2test sets .", "entities": []}, {"text": "However , its performance on the \u000b 3data , with out of domain and longer tables , is competitive to other methods .", "entities": []}, {"text": "Overall , the idea of using Top - S \u001c , i.e. , using the dynamic number of rows prediction and Re - Rank ( exact - match based re - ranking ) is bene\ufb01cial .", "entities": []}, {"text": "Previously used approaches such as DRR and WMD have low F1 - score , because of poor precision .", "entities": [[10, 13, "MetricName", "F1 - score"]]}, {"text": "UsingRe - Rank based on exact match improves the evidence extraction recall .", "entities": [[5, 7, "MetricName", "exact match"]]}, {"text": "Furthermore , introducing sparsity with Top - S \u001c , i.e. considering only the Top-2 rows ( S=2 ) and dynamic row selection ( \u001c = 1 ) substantially enhances evidence extraction precision .", "entities": []}, {"text": "Furthermore , the zero weighting of title matches using the Hypo - Title - Swap heuristic , bene\ufb01ts contextualized embedding models such as SimCSE12 .", "entities": []}, {"text": "SimCSE - supervised ( Hypo - Title - Swap + ReRank + Top-2 ( \u001c = 1 ) ) outperforms DRR ( Re - Rank + Top-2 ( \u001c = 1 ) ) by 4.3 % ( \u000b 1 ) , 2.5 % ( \u000b 2 ) and 5.4 % ( \u000b 3 ) absolute score .", "entities": [[0, 1, "MethodName", "SimCSE"]]}, {"text": "Since the table domains and the NLI reasoning involved for \u000b 1and \u000b 2are sim12For static embedding models , the effect of Hypo - Title - Swap was insigni\ufb01cant3273", "entities": []}, {"text": "Category Evidence Extraction Train Set Evidence Extraction Test Set \u000b 1 \u000b 2 \u000b 3 WMD ( Gupta et al . , 2020 ) WMD ( Gupta et al . , 2020 ) 70.38 62.55 61.33 Baseline No Extraction ( Gupta et al . , 2020 )", "entities": []}, {"text": "No Extraction ( Gupta et al . , 2020 ) 74.88 65.55 64.94 DRR ( Neeraja et al . , 2021 ) DRR ( Neeraja et al . , 2021 ) 75.78 67.22 64.88 Unsupervised DRR ( Re - Rank + Top-2 ( \u001c = 1 ) ) DRR ( Re - Rank + Top-2 ( \u001c = 1 ) )", "entities": []}, {"text": "74.66 67.38 65.83 Supervised Oracle Supervised ( 3\u0002Hard Negative ) 77.34 71.15 68.92 Human Oracle Oracle ( Gupta et al . , 2021 ) 78.83 71.61 71.55 Human Human NLI ( Gupta et al . , 2020 )", "entities": []}, {"text": "Human NLI ( Gupta et al . , 2020 ) 84.04 83.88 79.33 Table 3 : Tabular NLI performance with the extracted relevant rows as the premise .", "entities": []}, {"text": "ilar , so is their evidence extraction performance .", "entities": []}, {"text": "However , the performance of \u000b 3 , which contains out - of - domain and longer tables ( an average of thirteen rows , versus nine rows in \u000b 1and \u000b 2 ) is relatively worse .", "entities": []}, {"text": "The unsupervised approaches are still 12.69 % ( \u000b 1 ) , 13.49 % ( \u000b 2 ) , and 19.81 % ( \u000b 3 ) behind the human performance , highlighting the challenges of the task .", "entities": []}, {"text": "Supervised evidence extraction .", "entities": []}, {"text": "For RQ2 , Table 4 shows the performance of the supervised relevant row extraction approaches that use binary classi\ufb01ers trained with several sampling techniques for irrelevant rows .", "entities": []}, {"text": "Overall , adding supervision is advantageous13 .", "entities": []}, {"text": "Furthermore , we observe that using the unsupervised DRR technique to extract challenging irrelevant rows , i.e. , Hard Negative , is more effective than random sampling .", "entities": []}, {"text": "Indeed , using random negative examples as the irrelevant rows performs the worst .", "entities": []}, {"text": "Not sampling ( 6\u0002 ) or using only one irrelevant row , namely Hard Negative ( 1\u0002 ) , also underperforms .", "entities": []}, {"text": "We see that employing moderate sampling , i.e. , Hard Negative ( 3\u0002 ) , performs best across all test sets .", "entities": []}, {"text": "The best supervised model with Hard Negative ( 3\u0002 ) sampling improves evidence extraction performance by 8.7 % ( \u000b 1 ) , 10.8 % ( \u000b 2 ) , and 4.2 % ( \u000b 3 ) absolute score over the best unsupervised model , namely SimCSE - Supervised ( Hypo - Title - Swap + Re - Rank + Top-2 ( \u001c = 1)).14The human oracle outperforms the best supervised model by 4.13 % ( \u000b 1 ) and 2.65 % ( \u000b 2 ) absolute scores \u2014 a smaller gap than the best unsupervised approach .", "entities": [[46, 47, "MethodName", "SimCSE"]]}, {"text": "We also observe that the supervision does not bene\ufb01t the \u000b 3 set much , where the performance gap to humans is still about 15.95 % ( only 3.80 % improvement over unsupervised approach ) .", "entities": []}, {"text": "We suspect this is because of the distributional changes in \u000b 3set noted earlier .", "entities": []}, {"text": "13We investigate \u201c How much supervision is adequate ? \" in Appendix A.14Although \u000b 2is adversarial owing to label \ufb02ipping , rendering the NLI task more dif\ufb01cult , both \u000b 1 and \u000b 2have instances with the same domain tables and hypotheses with similar reasoning types , making the relevant row extraction task equally challenging .", "entities": []}, {"text": "This highlights directions for future improvement via domain adaptation .", "entities": [[7, 9, "TaskName", "domain adaptation"]]}, {"text": "Sampling ( Ratio ) \u000b 1 \u000b 2 \u000b 3 Random Negative ( 1\u0002)69.42 71.94 54.12 Hard Negative ( 1\u0002 ) 80.88 84.37 68.28 No Sampling ( 6\u0002 ) 83.76 85.41 71.26 Hard Negative ( 3\u0002 ) 84.49 86.58 72.61 Human Oracle 88.62 89.23 88.56 Table 4 : F1 - scores of supervised evidence extractors .", "entities": [[48, 49, "MetricName", "F1"]]}, {"text": "5.3 Results of Natural Language Inference For RQ3 , we investigate how using only extracted evidence as a premise impacts the performance of the tabular NLI task .", "entities": [[3, 6, "TaskName", "Natural Language Inference"]]}, {"text": "Table 3 shows the results .", "entities": []}, {"text": "Compared to the baseline DRR , our unsupervised DRR ( Re - Rank + Top-2 ( \u001c = 1 ) ) performs similarly for \u000b 2 , worse by 1.12 % on \u000b 1 , and outperforms by 0.95%on \u000b 3 .", "entities": []}, {"text": "Using evidence extraction with the best supervised model , Hard Negative ( 3\u0002 ) , trained on human - extracted ( Oracle ) rows results in 2.68 % ( \u000b 1 ) , 3.93 % ( \u000b 2 ) , and 4.04 % ( \u000b 3 ) improvements against DRR .", "entities": []}, {"text": "Furthermore , using human extracted ( Oracle ) rows for both training and testing sets outperforms all models - based extraction methods .", "entities": []}, {"text": "The human oracle based evidence extraction leads to largest performance improvements of 3.05 % ( \u000b 1 ) , 4.39 % ( \u000b 2 ) , and 6.67 % ( \u000b 3 ) over DRR .", "entities": []}, {"text": "Overall , these \ufb01ndings indicate that extracting evidence is bene\ufb01cial for reasoning in tabular inference task .", "entities": []}, {"text": "Despite using human extracted ( Oracle ) rows for both training and testing , the NLI model still falls far behind human reasoning ( Human NLI ) ( Gupta et al . , 2020 ) .", "entities": []}, {"text": "This gap exists because , in addition to extracting evidence , the INFOTABShypotheses require inference with the evidence involving common - sense and knowledge , which the NLI component does not adequately perform.3274", "entities": []}, {"text": "6 Evidence Extraction : Human versus Model We perform an error analysis of how well our proposed supervised extraction model ( Hard Negative(3x ) ) performs compared to the human annotators .", "entities": []}, {"text": "The model makes two types of errors : a Type I error occurs when an evidence row is marked as irrelevant , whereas Type II error occurs when an irrelevant row is marked as evidence .", "entities": []}, {"text": "A Type I error will reduce the model \u2019s precision for the extraction model , whereas a Type II error will decrease the model \u2019s recall .", "entities": []}, {"text": "Type I errors are especially concerning for the downstream NLI task .", "entities": []}, {"text": "Since mislabeled evidence rows will be absent from the extracted premise , necessary evidence will be omitted , leading to inaccurate entailment labels .", "entities": []}, {"text": "On the other hand , with Type II errors , when an irrelevant row is labeled as evidence , the model has to deal with from extra noise in the premise .", "entities": []}, {"text": "However , all the required evidence remains .", "entities": []}, {"text": "Table 5 shows a comparison of the supervised extraction ( Hard Negative ( 3x ) ) approach with the ground truth human labels on all the three test sets for both error types .", "entities": []}, {"text": "On the \u000b 3set , Type - I and Type - II errors are substantially higher than \u000b 1and \u000b 2 .", "entities": []}, {"text": "This highlights the fact that on the \u000b 3set , the model disagrees with with humans the most .", "entities": []}, {"text": "Furthermore , the ratio of Type - II over Type - I errors is much higher for \u000b 3 .", "entities": []}, {"text": "This indicates that the superTest Set Type - I Type - II Ratio ( II / I ) Total \u000b 1 312 430 1.38 742 \u000b 2 286 358 1.25 644 \u000b 3 508 1053 2.07 1561 Table 5 : Type - I and Type - II error of best supervised evidence extraction model .", "entities": []}, {"text": "vised extraction model marks many irrelevant rows as evidence ( Type - II error ) for \u000b 3set .", "entities": []}, {"text": "The out - ofdomain origin of \u000b 3tables , as well as their larger size , might be one explanation for this poor performance .", "entities": []}, {"text": "Appendix \u00a7 C provides several examples of both types of errors .", "entities": []}, {"text": "7 Discussion Why Sequential Prediction ?", "entities": []}, {"text": "Our choice of the sequential paradigm is motivated by the observation that it enforces a causal structure .", "entities": []}, {"text": "Of course , a joint or a multi - task model may make better predictions .", "entities": []}, {"text": "However , these models ignore the causal relationship between evidence selection and label prediction ( Herzig et al . , 2021 ; Paranjape et al . ,2020 ) .", "entities": []}, {"text": "Ideally , each row is independent and , its relevance to the hypothesis can be determined on its own .", "entities": []}, {"text": "In a joint or a multi - task model that exploits correlations across rows and the \ufb01nal label , irrelevant rows and the NLI label , can erroneously in\ufb02uence row selection .", "entities": []}, {"text": "Future Directions .", "entities": []}, {"text": "Based on the observations and discussions , we identify the future directions as follows .", "entities": []}, {"text": "( 1 ) Joint Causal Model .", "entities": []}, {"text": "To build a joint or a multi - task model that follows the causal reasoning structure , signi\ufb01cant changes in model architecture are required .", "entities": []}, {"text": "Such a model would \ufb01rst identify important rows and then use them for NLI predictions , but without risking spurious correlations .", "entities": []}, {"text": "( 2 ) How much Supervision is Needed ?", "entities": []}, {"text": "As evident from our experiments , relevant row supervision improves the evidence extraction , especially on \u000b 1and \u000b 2sets compared to unsupervised extraction .", "entities": []}, {"text": "But do we need full supervision for all examples ?", "entities": []}, {"text": "Is there any lower limit to supervision ?", "entities": []}, {"text": "We partially answered this question in the af\ufb01rmative by training the evidence extraction model with limited supervision ( semi - supervised setting ) , but a deeper analysis is needed to understand the limits .", "entities": []}, {"text": "See Appendix A for details .", "entities": []}, {"text": "( 3 ) Improving Zero - shot Domain Performance .", "entities": []}, {"text": "As evident from \u00a7 5.2 , the evidence extraction performance of outof - domain tables in \u000b 3needs further improvements , setting up a domain adaptation research question as future work .", "entities": [[24, 26, "TaskName", "domain adaptation"]]}, {"text": "( 4 ) Finally , inspired by Neeraja et al .", "entities": []}, {"text": "( 2021 ) , we may be able to add explicit knowledge to improve evidence extraction .", "entities": []}, {"text": "8 Comparison with Related Work Tabular Reasoning Many recent studies investigate various NLP tasks on semi - structured tabular data , including tabular NLI and fact veri\ufb01cation ( Chen et al . , 2020b ; Gupta et al . , 2020 ; Zhang and Balog , 2019 ) , various question answering and semantic parsing tasks ( Zhang and Balog , 2020 ; Zhang et", "entities": [[50, 52, "TaskName", "question answering"], [53, 55, "TaskName", "semantic parsing"]]}, {"text": "al . , 2020b ; Pasupat and Liang , 2015 ; Krishnamurthy et al . , 2017 ; Abbas et al . , 2016 ; Sun et al . , 2016 ; Chen et al . , 2020c ; Lin et al . , 2020 ; Zayats et al . , 2021 ; Oguz et", "entities": []}, {"text": "al . , 2020 ; Chen et al . , 2021 , inter alia ) , and table - to - text generation ( e.g. , Parikh et al . , 2020 ; Li et al . , 2021 ; Nan et al . , 2021 ; Yoran et al . , 2021 ; Chen et al . , 2020a ) .", "entities": [[17, 23, "TaskName", "table - to - text generation"]]}, {"text": "Several strategies for representing Wikipedia relational tables are proposed , such as Table2vec ( Deng et al . , 2019 ) , TAPAS ( Herzig et al . , 2020 ) , TaBERT ( Yin et al . , 2020 ) , TabStruc ( Zhang3275", "entities": [[14, 17, "DatasetName", "Deng et al"], [22, 23, "MethodName", "TAPAS"], [32, 33, "MethodName", "TaBERT"]]}, {"text": "et al . , 2020a ) , TABBIE ( Iida et al . , 2021 ) , TabGCN ( Pramanick and Bhattacharya , 2021 ) and RCI ( Glass et al . , 2021 ) .", "entities": [[7, 8, "MethodName", "TABBIE"]]}, {"text": "Yu et al .", "entities": []}, {"text": "( 2018 , 2021 ) ;", "entities": []}, {"text": "Eisenschlos et al . ( 2020 ) and Neeraja et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2021 ) study pre - training for improving tabular inference .", "entities": []}, {"text": "Interpretability and Explainability Model interpretability can either be through explanations or by identifying the evidence for the predictions ( Feng et al . , 2018 ; Serrano and Smith , 2019 ; Jain and Wallace , 2019 ; Wiegreffe and Pinter , 2019 ; DeYoung et al . , 2020 ; Paranjape et al . , 2020 ) .", "entities": []}, {"text": "Additionally , NLI models ( e.g. Ribeiro et al . , 2016 , 2018a , b ; Zhao et al . , 2018 ; Iyyer et al . , 2018 ; Glockner et al . , 2018 ; Naik et al . , 2018 ; McCoy et al . , 2019 ; Nie et al . , 2019 ; Liu et al . , 2019a ) must be subjected to numerous test sets with adversarial settings .", "entities": []}, {"text": "These settings can focus on various aspects of reasoning , such as perturbed premises for evidence selection ( Gupta et al . , 2021 ) , zeroshot transferability ( \u000b 3 ) , counterfactual premises ( Jain et al . , 2021 ) , and contrasting hypotheses \u000b 2 .", "entities": []}, {"text": "Recently , Kumar and Talukdar ( 2020 ) introduced Natural - language Inference over Label - speci\ufb01c Explanations ( NILE ) , an NLI approach for generating labels and accompanying faithful explanations using auto - generated label - speci\ufb01c natural language explanations .", "entities": [[2, 3, "DatasetName", "Kumar"]]}, {"text": "Our work focuses on the extraction of label - independent evidence for correct inference , rather than on the generation of abstractive explanations for a given label .", "entities": []}, {"text": "Comparison with Shared Tasks The SemEval\u201921", "entities": []}, {"text": "Task 9 ( Wang et al . , 2021b ) and FEVEROUS\u201921 shared task ( Aly et al . , 2021 ) are conceptually close to this work .", "entities": []}, {"text": "The SemEval task focuses on statement veri\ufb01cation and evidence extraction using relational tables from scienti\ufb01c articles .", "entities": []}, {"text": "In this work , we focus on item evidence extraction for non - scienti\ufb01c Wikipedia Infobox entity tables , proposed a twostage sequential approach , and used the INFOTABS dataset which has complex reasoning and multiple adversarial tests for robust evaluation .", "entities": [[28, 29, "DatasetName", "INFOTABS"]]}, {"text": "The FEVEROUS\u201921 shared task focuses on verifying information using unstructured and structured evidence from open - domain Wikipedia .", "entities": []}, {"text": "Our approach concerns evidence extraction from a single table rather than open - domain document , table or paragraph retrieval .", "entities": []}, {"text": "Furthermore , we are only concerned with entity tables rather than relational tables or unstructured text , while the FEVEROUSdata has relational tables , unstructured text , and fewer entity tables .", "entities": []}, {"text": "9 Conclusion and Future Work", "entities": []}, {"text": "In this paper , we introduced the problem of Trustworthy Tabular Inference , where a reasoning model both extracts evidence from a table and predicts an inference label .", "entities": []}, {"text": "We studied a two - stage approach , comprising an evidence extraction and an inference stage .", "entities": []}, {"text": "We explored several unsupervised and supervised strategies for evidence extraction , several of which outperformed prior benchmarks .", "entities": []}, {"text": "Finally , we showed that by using only extracted evidence as the premise , our approach outperforms previous baselines on the downstream tabular inference task .", "entities": []}, {"text": "Acknowledgements The authors thank Bloomberg \u2019s AI Engineering team , especially Ketevan Tsereteli , Anju Kambadur , and Amanda Stent for helpful feedback and directions .", "entities": []}, {"text": "We also appreciate the useful feedback provided by Ellen Riloff and the Utah NLP group .", "entities": []}, {"text": "Additionally , we appreciate the helpful inputs provided by Atreya Ghosal , Riyaz A. Bhat , Manish Srivastava , and Maneesh Singh .", "entities": []}, {"text": "Vivek Gupta acknowledges support from Bloomberg \u2019s Data Science Ph.D. Fellowship .", "entities": []}, {"text": "This work was supported in part by National Science Foundation grants # 1801446 ( SaTC ) and # 1822877 ( Cyberlearning ) .", "entities": []}, {"text": "Finally , we would like to express our gratitude to the reviewing team for their insightful comments .", "entities": []}, {"text": "References Faheem Abbas , M. K. Malik , M. Rashid , and Rizwan Zafar .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Wikiqa \u2014 a question answering system on wikipedia using freebase , dbpedia and infobox .", "entities": [[0, 1, "DatasetName", "Wikiqa"], [3, 5, "TaskName", "question answering"], [11, 12, "DatasetName", "dbpedia"]]}, {"text": "2016", "entities": []}, {"text": "Sixth International Conference on Innovative Computing Technology ( INTECH ) , pages 185\u2013193 .", "entities": []}, {"text": "Rami Aly , Zhijiang Guo , Michael Sejr Schlichtkrull , James Thorne , Andreas Vlachos , Christos Christodoulopoulos , Oana Cocarascu , and Arpit Mittal . 2021 .", "entities": []}, {"text": "The fact extraction and VERi\ufb01cation over unstructured and structured information ( FEVEROUS ) shared task .", "entities": [[11, 12, "DatasetName", "FEVEROUS"]]}, {"text": "In Proceedings of the Fourth Workshop on Fact Extraction and VERi\ufb01cation ( FEVER ) , pages 1\u201313 , Dominican Republic .", "entities": [[12, 13, "DatasetName", "FEVER"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Samuel R. Bowman , Gabor Angeli , Christopher Potts , and Christopher D. Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A Large Annotated Corpus for Learning Natural Language Inference .", "entities": [[6, 9, "TaskName", "Natural Language Inference"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing .3276", "entities": []}, {"text": "Wenhu Chen , Ming - Wei Chang , Eva Schlinger , William Yang Wang , and William W. Cohen . 2021 .", "entities": []}, {"text": "Open question answering over tables and text .", "entities": [[1, 3, "TaskName", "question answering"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Wenhu Chen , Jianshu Chen , Yu Su , Zhiyu Chen , and William Yang Wang .", "entities": []}, {"text": "2020a .", "entities": []}, {"text": "Logical natural language generation from open - domain tables .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7929 \u2013 7942 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Wenhu Chen , Hongmin Wang , Jianshu Chen , Yunkai Zhang , Hong Wang , Shiyang Li , Xiyou Zhou , and William Yang Wang .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Tabfact :", "entities": [[0, 1, "DatasetName", "Tabfact"]]}, {"text": "A large - scale dataset for table - based fact veri\ufb01cation .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Wenhu Chen , Hanwen Zha , Zhiyu Chen , Wenhan Xiong , Hong Wang , and William Yang Wang .", "entities": []}, {"text": "2020c .", "entities": []}, {"text": "HybridQA : A dataset of multi - hop question answering over tabular and textual data .", "entities": [[0, 1, "DatasetName", "HybridQA"], [5, 10, "TaskName", "multi - hop question answering"]]}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 1026\u20131036 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Davide Chicco . 2021 .", "entities": []}, {"text": "Siamese Neural Networks : An Overview , pages 73\u201394 .", "entities": []}, {"text": "Springer US , New York , NY .", "entities": []}, {"text": "Ido Dagan , Dan Roth , Mark Sammons , and Fabio Massimo Zanzotto .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Recognizing Textual Entailment : Models and Applications .", "entities": []}, {"text": "Synthesis Lectures on Human Language Technologies , 6(4):1\u2013220 .", "entities": []}, {"text": "Li Deng , Shuo Zhang , and Krisztian Balog .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Table2vec :", "entities": []}, {"text": "Neural word and entity embeddings for table population and retrieval .", "entities": [[3, 5, "TaskName", "entity embeddings"]]}, {"text": "In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR\u201919 , pages 1029\u20131032 , New York , NY , USA . ACM .", "entities": [[6, 7, "DatasetName", "ACM"], [14, 16, "TaskName", "Information Retrieval"], [29, 30, "DatasetName", "ACM"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies .", "entities": []}, {"text": "Jay DeYoung , Sarthak Jain , Nazneen Fatema Rajani , Eric Lehman , Caiming Xiong , Richard Socher , and Byron C. Wallace .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "ERASER :", "entities": []}, {"text": "A benchmark to evaluate rationalized NLP models .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4443\u20134458 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Julian Eisenschlos , Syrine Krichene , and Thomas M\u00fcller .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Understanding tables with intermediate pre - training .", "entities": []}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages281\u2013296 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shi Feng , Eric Wallace , Alvin Grissom II , Mohit Iyyer , Pedro Rodriguez , and Jordan Boyd - Graber .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Pathologies of neural models make interpretations dif\ufb01cult .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3719\u20133728 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Tianyu Gao , Xingcheng Yao , and Danqi Chen . 2021 .", "entities": []}, {"text": "SimCSE : Simple contrastive learning of sentence embeddings .", "entities": [[0, 1, "MethodName", "SimCSE"], [3, 5, "MethodName", "contrastive learning"], [6, 8, "TaskName", "sentence embeddings"]]}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6894\u20136910 , Online and Punta Cana , Dominican Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Michael Glass , Mustafa Canim , Al\ufb01o Gliozzo , Saneem Chemmengath , Vishwajeet Kumar , Rishav Chakravarti , Avi Sil , Feifei Pan , Samarth Bharadwaj , and Nicolas Rodolfo Fauceglia . 2021 .", "entities": [[13, 14, "DatasetName", "Kumar"]]}, {"text": "Capturing row and column semantics in transformer based question answering over tables .", "entities": [[8, 10, "TaskName", "question answering"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1212\u20131224 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Max Glockner , Vered Shwartz , and Yoav Goldberg .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Breaking NLI systems with sentences that require simple lexical inferences .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 650\u2013655 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Karan Goel , Nazneen Fatema Rajani , Jesse Vig , Zachary Taschdjian , Mohit Bansal , and Christopher R\u00e9 . 2021 .", "entities": []}, {"text": "Robustness gym :", "entities": []}, {"text": "Unifying the NLP evaluation landscape .", "entities": []}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies : Demonstrations , pages 42\u201355 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Vivek Gupta , Riyaz A. Bhat , Atreya Ghosal , Manish Srivastava , Maneesh Singh , and Vivek Srikumar . 2021 .", "entities": []}, {"text": "Is my model using the right evidence ?", "entities": []}, {"text": "systematic probes for examining evidence - based tabular reasoning .", "entities": []}, {"text": "CoRR , abs/2108.00578 .", "entities": []}, {"text": "Vivek Gupta , Maitrey Mehta , Pegah Nokhiz , and Vivek Srikumar .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "INFOTABS :", "entities": [[0, 1, "DatasetName", "INFOTABS"]]}, {"text": "Inference on tables as semi - structured data .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 2309\u20132324 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Suchin Gururangan , Swabha Swayamdipta , Omer Levy , Roy Schwartz , Samuel Bowman , and Noah A. Smith .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Annotation artifacts in natural language inference data .", "entities": [[3, 6, "TaskName", "natural language inference"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the3277", "entities": []}, {"text": "Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 107\u2013112 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jonathan Herzig , Thomas M\u00fcller , Syrine Krichene , and Julian Eisenschlos . 2021 .", "entities": []}, {"text": "Open domain question answering over tables via dense retrieval .", "entities": [[2, 4, "TaskName", "question answering"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 512\u2013519 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jonathan Herzig , Pawel Krzysztof Nowak , Thomas M\u00fcller , Francesco Piccinno , and Julian Eisenschlos . 2020 .", "entities": []}, {"text": "TaPas : Weakly supervised table parsing via pre - training .", "entities": [[0, 1, "MethodName", "TaPas"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4320\u20134333 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Hiroshi Iida , Dung Thai , Varun Manjunatha , and Mohit Iyyer . 2021 .", "entities": []}, {"text": "TABBIE : Pretrained representations of tabular data .", "entities": [[0, 1, "MethodName", "TABBIE"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 3446\u20133456 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mohit Iyyer , John Wieting , Kevin Gimpel , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Adversarial example generation with syntactically controlled paraphrase networks .", "entities": []}, {"text": "InProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 1875\u20131885 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nupur Jain , Vivek Gupta , Anshul Rai , and Gaurav Kumar . 2021 .", "entities": [[11, 12, "DatasetName", "Kumar"]]}, {"text": "TabPert :", "entities": []}, {"text": "An effective platform for tabular perturbation .", "entities": []}, {"text": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 350\u2013360 , Online and Punta Cana , Dominican Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sarthak Jain and Byron C. Wallace .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Attention is not Explanation .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3543\u20133556 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Masoud Jalili Sabet , Philipp Dufter , Fran\u00e7ois Yvon , and Hinrich Sch\u00fctze . 2020 .", "entities": []}, {"text": "SimAlign :", "entities": []}, {"text": "High quality word alignments without parallel training data using static and contextualized embeddings .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : Findings , pages 1627\u20131643 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Armand Joulin , Edouard Grave , Piotr Bojanowski , Matthijs Douze , H\u00e9rve J\u00e9gou , and Tomas Mikolov .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Fasttext.zip : Compressing text classi\ufb01cation models .", "entities": []}, {"text": "arXiv preprint arXiv:1612.03651 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Gregory Koch , Richard Zemel , Ruslan Salakhutdinov , et al . 2015 .", "entities": [[6, 7, "DatasetName", "Ruslan"]]}, {"text": "Siamese neural networks for one - shot image recognition .", "entities": [[7, 9, "TaskName", "image recognition"]]}, {"text": "In ICML deep learning workshop , volume 2 .", "entities": []}, {"text": "Lille .", "entities": []}, {"text": "Jayant Krishnamurthy , Pradeep Dasigi , and Matt Gardner . 2017 .", "entities": []}, {"text": "Neural semantic parsing with type constraints for semi - structured tables .", "entities": [[1, 3, "TaskName", "semantic parsing"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 1516\u20131526 , Copenhagen , Denmark .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sawan Kumar and Partha Talukdar . 2020 .", "entities": [[1, 2, "DatasetName", "Kumar"]]}, {"text": "NILE : Natural language inference with faithful natural language explanations .", "entities": [[2, 5, "TaskName", "Natural language inference"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8730\u20138742 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tongliang Li , Lei Fang , Jian - Guang Lou , and Zhoujun Li . 2021 .", "entities": []}, {"text": "TWT : Table with written text for controlled data - to - text generation .", "entities": [[8, 14, "TaskName", "data - to - text generation"]]}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2021 , pages 1244\u20131254 , Punta Cana , Dominican Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xi Victoria Lin , Richard Socher , and Caiming Xiong .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Bridging textual and tabular data for crossdomain text - to - SQL semantic parsing .", "entities": [[7, 12, "TaskName", "text - to - SQL"], [12, 14, "TaskName", "semantic parsing"]]}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 4870\u20134888 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nelson F. Liu , Roy Schwartz , and Noah A. Smith .", "entities": []}, {"text": "2019a .", "entities": []}, {"text": "Inoculation by \ufb01ne - tuning : A method for analyzing challenge datasets .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 2171\u20132179 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov . 2019b .", "entities": []}, {"text": "Roberta : A Robustly Optimized BERT Pretraining Approach .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "arXiv preprint arXiv:1907.11692 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Tom McCoy , Ellie Pavlick , and Tal Linzen .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Right for the wrong reasons : Diagnosing syntactic heuristics in natural language inference .", "entities": [[10, 13, "TaskName", "natural language inference"]]}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3428\u20133448 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Tomas Mikolov , Edouard Grave , Piotr Bojanowski , Christian Puhrsch , and Armand Joulin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Advances in pre - training distributed word representations .", "entities": []}, {"text": "In Proceedings of the Eleventh International3278", "entities": []}, {"text": "Conference on Language Resources and Evaluation ( LREC 2018 ) , Miyazaki , Japan .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Anshuman Mishra , Dhruvesh Patel , Aparna Vijayakumar , Xiang Lorraine Li , Pavan Kapanipathi , and Kartik Talamadupula .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Looking beyond sentencelevel natural language inference for question answering and text summarization .", "entities": [[3, 6, "TaskName", "natural language inference"], [7, 9, "TaskName", "question answering"], [10, 12, "TaskName", "text summarization"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1322\u20131336 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Aakanksha Naik , Abhilasha Ravichander , Norman Sadeh , Carolyn Rose , and Graham Neubig .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Stress test evaluation for natural language inference .", "entities": [[4, 7, "TaskName", "natural language inference"]]}, {"text": "InProceedings of the 27th International Conference on Computational Linguistics , pages 2340\u20132353 , Santa Fe , New Mexico , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Linyong Nan , Dragomir Radev , Rui Zhang , Amrit Rau , Abhinand Sivaprasad , Chiachun Hsieh , Xiangru Tang , Aadit Vyas , Neha Verma , Pranav Krishna , Yangxiaokang Liu , Nadia Irwanto , Jessica Pan , Faiaz Rahman , Ahmad Zaidi , Mutethia Mutuma , Yasin Tarabar , Ankit Gupta , Tao Yu , Yi Chern Tan , Xi Victoria Lin , Caiming Xiong , Richard Socher , and Nazneen Fatema Rajani . 2021 .", "entities": []}, {"text": "DART : Open - domain structured data record to text generation .", "entities": [[0, 1, "DatasetName", "DART"], [9, 11, "TaskName", "text generation"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 432\u2013447 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "J. Neeraja , Vivek Gupta , and Vivek Srikumar . 2021 .", "entities": []}, {"text": "Incorporating external knowledge to enhance tabular reasoning .", "entities": []}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 2799\u20132809 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yixin Nie , Yicheng Wang , and Mohit Bansal .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Analyzing compositionality - sensitivity of nli models .", "entities": []}, {"text": "Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 33(01):6867\u20136874 .", "entities": []}, {"text": "Barlas Oguz , Xilun Chen , Vladimir Karpukhin , Stan Peshterliev , Dmytro Okhonko , Michael Schlichtkrull , Sonal Gupta , Yashar Mehdad , and Scott Yih . 2020 .", "entities": []}, {"text": "Uni\ufb01ed open - domain question answering with structured and unstructured knowledge .", "entities": [[1, 6, "TaskName", "open - domain question answering"]]}, {"text": "arXiv preprint arXiv:2012.14610 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Bhargavi Paranjape , Mandar Joshi , John Thickstun , Hannaneh Hajishirzi , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "An information bottleneck approach for controlling conciseness in rationale extraction .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1938\u20131952 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ankur Parikh , Xuezhi Wang , Sebastian Gehrmann , Manaal Faruqui , Bhuwan Dhingra , Diyi Yang , and Dipanjan Das . 2020 .", "entities": []}, {"text": "ToTTo :", "entities": [[0, 1, "DatasetName", "ToTTo"]]}, {"text": "A controlled table - totext generation dataset .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1173\u20131186 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Panupong Pasupat and Percy Liang . 2015 .", "entities": []}, {"text": "Compositional semantic parsing on semi - structured tables .", "entities": [[1, 3, "TaskName", "semantic parsing"]]}, {"text": "InProceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 1470\u20131480 , Beijing , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Adam Poliak , Jason Naradowsky , Aparajita Haldar , Rachel Rudinger , and Benjamin Van Durme .", "entities": [[0, 1, "MethodName", "Adam"]]}, {"text": "2018 .", "entities": []}, {"text": "Hypothesis only baselines in natural language inference .", "entities": [[4, 7, "TaskName", "natural language inference"]]}, {"text": "In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics , pages 180\u2013191 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Aniket Pramanick and Indrajit Bhattacharya . 2021 .", "entities": []}, {"text": "Joint learning of representations for web - tables , entities and types using graph convolutional network .", "entities": [[13, 16, "MethodName", "graph convolutional network"]]}, {"text": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics : Main Volume , pages 1197\u20131206 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nils Reimers and Iryna Gurevych .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "SentenceBERT : Sentence embeddings using Siamese BERTnetworks .", "entities": [[2, 4, "TaskName", "Sentence embeddings"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3982\u20133992 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Nils Reimers and Iryna Gurevych .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Making monolingual sentence embeddings multilingual using knowledge distillation .", "entities": [[2, 4, "TaskName", "sentence embeddings"], [6, 8, "MethodName", "knowledge distillation"]]}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Marco Tulio Ribeiro , Sameer Singh , and Carlos Guestrin . 2016 .", "entities": []}, {"text": "\" why should i trust you ? \" : Explaining the predictions of any classi\ufb01er .", "entities": []}, {"text": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD \u2019 16 , page 1135\u20131144 , New York , NY , USA . Association for Computing Machinery .", "entities": [[5, 6, "DatasetName", "ACM"]]}, {"text": "Marco Tulio Ribeiro , Sameer Singh , and Carlos Guestrin .", "entities": []}, {"text": "2018a .", "entities": []}, {"text": "Anchors : High - precision modelagnostic explanations .", "entities": []}, {"text": "Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , 32(1 ) .", "entities": []}, {"text": "Marco Tulio Ribeiro , Sameer Singh , and Carlos Guestrin .", "entities": []}, {"text": "2018b .", "entities": []}, {"text": "Semantically equivalent adversarial rules for debugging NLP models .", "entities": []}, {"text": "In Proceedings3279", "entities": []}, {"text": "of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 856\u2013865 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Marco Tulio Ribeiro , Tongshuang Wu , Carlos Guestrin , and Sameer Singh .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Beyond accuracy : Behavioral testing of NLP models with CheckList .", "entities": [[1, 2, "MetricName", "accuracy"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4902 \u2013 4912 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "So\ufb01a Serrano and Noah A. Smith .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Is attention interpretable ?", "entities": []}, {"text": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2931\u20132951 , Florence , Italy . Association for Computational Linguistics .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Huan Sun , Hao Ma , Xiaodong He , Scott Wen - tau Yih , Yu Su , and Xifeng Yan . 2016 .", "entities": []}, {"text": "Table cell search for question answering .", "entities": [[4, 6, "TaskName", "question answering"]]}, {"text": "In Proceedings of the companion publication of the 25th international conference on World Wide Web .", "entities": []}, {"text": "ACM - Association for Computing Machinery .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Nandan Thakur , Nils Reimers , Johannes Daxenberger , and Iryna Gurevych .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Augmented SBERT : Data augmentation method for improving bi - encoders for pairwise sentence scoring tasks .", "entities": [[0, 2, "MethodName", "Augmented SBERT"], [3, 5, "TaskName", "Data augmentation"]]}, {"text": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 296\u2013310 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alex Wang , Yada Pruksachatkun , Nikita Nangia , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel Bowman . 2019 .", "entities": []}, {"text": "Superglue : A stickier benchmark for general - purpose language understanding systems .", "entities": [[0, 1, "DatasetName", "Superglue"]]}, {"text": "In Advances in Neural Information Processing Systems , volume 32 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "GLUE :", "entities": [[0, 1, "DatasetName", "GLUE"]]}, {"text": "A multi - task benchmark and analysis platform for natural language understanding .", "entities": [[9, 12, "TaskName", "natural language understanding"]]}, {"text": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 353\u2013355 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kexin Wang , Nils Reimers , and Iryna Gurevych .", "entities": []}, {"text": "2021a .", "entities": []}, {"text": "Tsdae : Using transformer - based sequential denoising auto - encoderfor unsupervised sentence embedding learning .", "entities": [[0, 1, "MethodName", "Tsdae"], [7, 8, "TaskName", "denoising"], [12, 14, "TaskName", "sentence embedding"]]}, {"text": "arXiv preprint arXiv:2104.06979 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Nancy X. R. Wang , Diwakar Mahajan , Marina Danilevsky , and Sara Rosenthal .", "entities": []}, {"text": "2021b .", "entities": []}, {"text": "SemEval2021 task 9 : Fact veri\ufb01cation and evidence \ufb01nding for tabular data in scienti\ufb01c documents ( SEM - TABFACTS ) .", "entities": []}, {"text": "In Proceedings of the 15th International Workshop on Semantic Evaluation ( SemEval-2021 ) , pages 317\u2013326 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sarah Wiegreffe and Yuval Pinter .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Attention is not not explanation .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLPIJCNLP ) , pages 11\u201320 , Hong Kong , China . Association for Computational Linguistics .", "entities": []}, {"text": "Adina Williams , Nikita Nangia , and Samuel Bowman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A broad - coverage challenge corpus for sentence understanding through inference .", "entities": []}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 1112\u20131122 , New Orleans , Louisiana . Association for Computational Linguistics .", "entities": []}, {"text": "Pengcheng Yin , Graham Neubig , Wen - tau Yih , and Sebastian Riedel .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "TaBERT :", "entities": [[0, 1, "MethodName", "TaBERT"]]}, {"text": "Pretraining for joint understanding of textual and tabular data .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8413 \u2013 8426 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Wenpeng Yin , Dragomir Radev , and", "entities": []}, {"text": "Caiming Xiong .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "DocNLI :", "entities": [[0, 1, "DatasetName", "DocNLI"]]}, {"text": "A large - scale dataset for documentlevel natural language inference .", "entities": [[7, 10, "TaskName", "natural language inference"]]}, {"text": "In Findings of the Association for Computational Linguistics : ACLIJCNLP 2021 , pages 4913\u20134922 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ori Yoran , Alon Talmor , and Jonathan Berant .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Turning tables : Generating examples from semi - structured tables for endowing language models with reasoning skills .", "entities": []}, {"text": "arXiv preprint arXiv:2107.07261 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Version 1 .", "entities": []}, {"text": "Tao Yu , Chien - Sheng Wu , Xi Victoria Lin , bailin wang , Yi Chern Tan , Xinyi Yang , Dragomir Radev , richard socher , and Caiming Xiong . 2021 .", "entities": []}, {"text": "Gra{pp}a : Grammar - augmented pre - training for table semantic parsing .", "entities": [[10, 12, "TaskName", "semantic parsing"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Tao Yu , Rui Zhang , Kai Yang , Michihiro Yasunaga , Dongxu Wang , Zifan Li , James Ma , Irene Li , Qingning Yao , Shanelle Roman , Zilin Zhang , and Dragomir Radev .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Spider : A largescale human - labeled dataset for complex and crossdomain semantic parsing and text - to - SQL task .", "entities": [[12, 14, "TaskName", "semantic parsing"], [15, 20, "TaskName", "text - to - SQL"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3911\u20133921 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Vicky Zayats , Kristina Toutanova , and Mari Ostendorf .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Representations for question answering from documents with tables and text .", "entities": [[2, 4, "TaskName", "question answering"]]}, {"text": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics : Main Volume , pages 2895\u20132906 , Online .", "entities": []}, {"text": "Association for Computational Linguistics.3280", "entities": []}, {"text": "Hongzhi Zhang , Yingyao Wang , Sirui Wang , Xuezhi Cao , Fuzheng Zhang , and Zhongyuan Wang . 2020a .", "entities": []}, {"text": "Table fact veri\ufb01cation with structure - aware transformer .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1624\u20131629 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shuo Zhang and Krisztian Balog .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Autocompletion for data cells in relational tables .", "entities": []}, {"text": "In Proceedings of the 28th ACM International Conference on Information and Knowledge Management , CIKM \u2019 19 , pages 761\u2013770 , New York , NY , USA . ACM .", "entities": [[5, 6, "DatasetName", "ACM"], [12, 13, "TaskName", "Management"], [28, 29, "DatasetName", "ACM"]]}, {"text": "Shuo Zhang and Krisztian Balog .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Web table extraction , retrieval , and augmentation : A survey .", "entities": []}, {"text": "ACM Trans .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Intell .", "entities": []}, {"text": "Syst .", "entities": []}, {"text": "Technol . , 11(2):13:1\u201313:35 .", "entities": []}, {"text": "Shuo Zhang , Zhuyun Dai , Krisztian Balog , and Jamie Callan . 2020b .", "entities": []}, {"text": "Summarizing and exploring tabular data in conversational search .", "entities": [[6, 8, "TaskName", "conversational search"]]}, {"text": "In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR \u2019 20 , pages 1537\u20131540 , New York , NY , USA .", "entities": [[6, 7, "DatasetName", "ACM"], [14, 16, "TaskName", "Information Retrieval"]]}, {"text": "Association for Computing Machinery .", "entities": []}, {"text": "Zhengli Zhao , Dheeru Dua , and Sameer Singh .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Generating natural adversarial examples .", "entities": []}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "A How Much Supervision is Enough for Evidence Extraction ?", "entities": []}, {"text": "To investigate this , we use Hard Negative ( 3x ) with RoBERTa LARGE model as our evidence extraction classi\ufb01er , which is similar to the full supervision method .", "entities": [[12, 13, "MethodName", "RoBERTa"]]}, {"text": "To simulate semi - supervision settings , we randomly sample 10 % , 20 % , 30 % , 40 % , and 50 % example instances of the train set in an incremental fashion for model training , where we repeat the random samplings three times .", "entities": []}, {"text": "Figure 3 , 4 , and 5 compare the average F1 - score over three runs on the three test sets \u000b 1 , \u000b 2and \u000b 3respectively .", "entities": [[10, 13, "MetricName", "F1 - score"]]}, {"text": "Percentage ( % ) of Annotated DataF1 - Score 7075808590 10 % 20 % 30 % 40 % 50%Semi - Supervised Supervised ( 100 % )", "entities": [[8, 9, "MetricName", "Score"]]}, {"text": "Unsupervised Human BaselineSemi - Supervised \u03b11 Performance Figure 3 : Extraction performance with limited supervision for \u000b 1 .", "entities": []}, {"text": "All results are average of three random splits runs .", "entities": []}, {"text": "Percentage ( % ) of Annotated DataF1 - Score 75808590 10 % 20 % 30 % 40 % 50%Semi - Supervised Supervised ( 100 % )", "entities": [[8, 9, "MetricName", "Score"]]}, {"text": "Unsupervised Human BaselineSemi - Supervised \u03b12 PerformanceFigure 4 : Extraction performance with limited supervision for \u000b 2 .", "entities": []}, {"text": "All results are average of three random splits runs .", "entities": []}, {"text": "Percentage ( % ) of Annotated DataF1 - Score 60708090 10 % 20 % 30 % 40 % 50%Semi - Supervised Supervised ( 100 % )", "entities": [[8, 9, "MetricName", "Score"]]}, {"text": "Unsupervised Human BaselineSemi - Supervised \u03b13 Performance Figure 5 : Extraction performance with limited supervision for \u000b 3 .", "entities": []}, {"text": "All results are average of three random splits runs .", "entities": []}, {"text": "We discovered that adding some supervision had advantages over not having any supervision .", "entities": []}, {"text": "However , we also \ufb01nd that 20 % supervision is adequate for reasonably good evidence extraction with only < 5%F1 - score gap with full supervision .", "entities": []}, {"text": "One key issue we observe is the lack of a visible trend due to signi\ufb01cant variation produced by random data sub - sampling .", "entities": []}, {"text": "It would be worthwhile to explore if this volatility could be reduced by strategic sampling using an unsupervised extraction model , an active learning framework , and strategic diversity maximizing sampling , which is left as future work .", "entities": [[22, 24, "TaskName", "active learning"]]}, {"text": "B Human Annotation Quality Control Since many hypothesis sentences ( especially those with neutral labels ) require out - of - table information for inference , we introduced the option to choose out - of - table ( OOT ) pseudo rows , which are highlighted only when the hypothesis requires information that is not common ( i.e. common sense ) and missing from the table .", "entities": []}, {"text": "To reduce any possible bias due to unintended associations between the NLI label and the row selections ( e.g. , using OOT for neutral examples ) , we avoid showing inference3281", "entities": []}, {"text": "labels to the annotators15 .", "entities": []}, {"text": "To assess an annotator , we compare their annotations with the majority consensus of other annotators \u2019 ( four ) annotations .", "entities": []}, {"text": "We perform this comparison at two levels : ( a ) local - consensus - score on the most recent batch , and ( b ) cumulative - consensusscore on all batches annotated thus far .", "entities": []}, {"text": "We use these consensus scores to temporarily ( local - consensus - score ) or permanently ( cumulative score ) block the poor annotators from the task .", "entities": []}, {"text": "We also review the annotations manually and provide feedback with more detailed instructions and personalized examples for annotators who were making mistakes due to ambiguity in the task .", "entities": []}, {"text": "We give incentives to annotators who received high consensus scores .", "entities": []}, {"text": "As in previous work , we removed certain annotators \u2019 annotations that have a poor consensus score ( cumulative score ) and published a second validation HIT to double - check each data point if necessary .", "entities": []}, {"text": "C Human vs Models Qualitative Examples We manually inspect the Type I and Type II error instances for the supervised model and human annotation for the development set .", "entities": []}, {"text": "Below , we show some of these examples where models con\ufb02ict with ground - truth human annotation .", "entities": []}, {"text": "We also provide a possible reason behind the model mistakes .", "entities": []}, {"text": "Type I. Below , we show Type I error examples .", "entities": []}, {"text": "Example I Row : Colorado Springs , Colorado is a poor training location for endurance athletes .", "entities": []}, {"text": "Hypothesis :", "entities": []}, {"text": "The elevation of Colorado Springs , Colorado is 6,035 ft ( 1,839 m ) .", "entities": []}, {"text": "Model Prediction : Not Relevant Human Ground Truth : Relevant Evidence .", "entities": []}, {"text": "Possible Reason : Model was n\u2019t able to connect the concept of elevation with the perfect high elevation training ground requirement of endurance athletes .", "entities": []}, {"text": "Requires common sense and knowledge .", "entities": []}, {"text": "15Because of the random sequence and unbalanced nature , each of the three hypothesis sentences can have any NLI label , i.e. , in total 33=27possibilities .", "entities": []}, {"text": "Example II Row : The number of number of employees of International Fund for Animal Welfare - ifaw is 300 + ( worldwide ) .", "entities": []}, {"text": "Hypothesis : International Fund for Animal Welfare ifaw is a national organization focused on only North America .", "entities": []}, {"text": "Model Prediction : Not Relevant Human Ground Truth : Relevant Evidence .", "entities": []}, {"text": "Possible Reason : Model was n\u2019t able to connect the clue ( \u2018 worldwide \u2019 ) in the table row with the phrase \u2018 focused on only north America \u2019 .", "entities": []}, {"text": "Example III Row : The equipment of Combined driving are horse , carriage , horse harness equipment .", "entities": []}, {"text": "Hypothesis :", "entities": []}, {"text": "Combined driving is a horse racing event style .", "entities": []}, {"text": "Model Prediction : Not Relevant Human Ground Truth : Relevant Evidence .", "entities": []}, {"text": "Possible Reason : Model was n\u2019t able to connect the horse related equipment i.e. \u2018 horse carriage , horse harness \u2019 with the event time i.e. \u2018 horse racing \u2019 .", "entities": []}, {"text": "Type II .", "entities": []}, {"text": "Below , we show Type II error examples .", "entities": []}, {"text": "Example I Row : Dazed and Confused was directed by Richard Linklater .", "entities": []}, {"text": "Hypothesis :", "entities": []}, {"text": "Dazed and Confused was directed in 1993 .", "entities": []}, {"text": "Model Prediction : Relevant Evidence Human Ground Truth : Not Relevant .", "entities": []}, {"text": "Possible Reason : Model focuses on lexical match token \u2018 directed \u2019 instead using entity type where premise refer for \u2018 Person \u2019 who directed rather than \u2018 Date \u2019 of direction .", "entities": []}, {"text": "Example II Row : The spouse(s ) of Celine Dion ( CC OQ ChLD ) is Ren\u00e9 Ang\u00e9lil , ( m. 1994 ; died 2016 ) .", "entities": []}, {"text": "Hypothesis :", "entities": []}, {"text": "Th\u00e9r\u00e8se Tanguay Dion had a child that became a widow .", "entities": []}, {"text": "Model Prediction : Relevant Evidence Human Ground Truth : Not Relevant .", "entities": []}, {"text": "Possible Reason : Model was unable to connect widow concept in hypothesis with it relation to Spouse and the marriage date Ren\u00e9 Ang\u00e9lil , ( m. 1994 ; died 2016).3282", "entities": []}, {"text": "Example III Row : The trainer of Caveat is Woody Stephens .", "entities": []}, {"text": "Hypothesis :", "entities": []}, {"text": "Caveat won more in winnings than it took to raise and train him .", "entities": []}, {"text": "Model Prediction : Relevant Evidence Human Ground Truth : Not Relevant .", "entities": []}, {"text": "Possible Reason : Model connects the \u2018 raise and train \u2019 term with the trainer name which is unrelated and has no connection to overall , winning races money vs spending for animal .", "entities": []}, {"text": "Discussion Based on the observation from the above examples as also stated in $ 5.3 , the model fails on many examples due to its lack of knowledge and common - sense reasoning ability .", "entities": []}, {"text": "One possible solution to mitigate this is by the addition of implicit and explicit knowledge on - the-\ufb02y for evidence extraction , as done for inference task by Neeraja et", "entities": []}, {"text": "al . ( 2021 ) .", "entities": []}, {"text": "D Implicit Relevance Indication We manually examine the human - annotated evidence in the development set .", "entities": []}, {"text": "We discovered the existence of several relevant phrases / tokens which implicitly indicate the presence of evidence rows .", "entities": []}, {"text": "E.g. The existence of tokens such as married , husband , lesbian , and wife in hypothesis ( H ) is very suggestive of the row Spouse being the relevant evidence .", "entities": []}, {"text": "Learning such implicit relevance - based phrases and tokens connection is easy for humans and large pre - trained supervision models .", "entities": []}, {"text": "It is a challenging task for similarity - based unsupervised extraction methods .", "entities": []}, {"text": "Below , we show implicit relevance , indicating token and the corresponding relevant evidence rows .", "entities": []}, {"text": "Relevance Indicating Phrase ( H ) !", "entities": []}, {"text": "Relevant Evidence Rows Key(T ) \u2018 broked \u2019 , \u2018 started from \u2019 , \u2018 does n\u2019t anymore \u2019 , \u2018 still perform \u2019 , \u2018 over a decade \u2019 , \u2018 began performing \u2019 , \u2018 started wrapping \u2019 , \u2019 \ufb01rst started \u2019 !", "entities": []}, {"text": "year active age related term , \u2018 were of < age > \u2019 , \u2018 after < age > \u2019 , \u2019 fall \u2019 , \u2019 spring\u2019,\u2019birthday\u2019!born \u2019 several years \u2019 , \u2019 one month \u2019 , century art !", "entities": []}, {"text": "years \u2018 co - wrote \u2019 , \u2018 written \u2019 , \u2018 writer \u2019 , \u2018 original written \u2019 ! written by ( novel and book ) \u2018 married \u2019 , \u2018 husband \u2019 , \u2018 lesbian \u2019 , \u2018 wives \u2019 !", "entities": []}, {"text": "Spouse \u2018 no - reward \u2019 , \u2018 monetary value \u2019 , \u2018 prize \u2019 !", "entities": []}, {"text": "rewards \u2018 earlier \u2019 , \u2018 debut \u2019 , \u2018 21st century \u2019 , \u2018 early 90s \u2019 , \u2018 recording\u2019,\u2018product of years \u2019 !", "entities": []}, {"text": "recorded \u2018 lost \u2019 , \u2019 won \u2019 , \u2019 races\u2019,\u2019competition \u2019 !", "entities": []}, {"text": "records ( horse races , car races etc ) \u2019 sea level \u2019 ! \u2019 lowest elevation \u2019 , \u2019 highest elevation \u2019 , \u2019 elevation \u2019 multi - lingual , multi - faith ! \u2019 regional languages \u2019 , \u2019 of\ufb01cial languages \u2019 , \u2019 religion \u2019 , \u2019 , \u2019 race or faith \u2019 \u2018 acting \u2019 , \u2018 rapping \u2019 , \u2018 politics \u2019 !", "entities": []}, {"text": "occupation \u2018 over an \u2019 , \u2018 shortest \u2019 , \u2018 longest \u2019 , \u2018 run - time \u2019 !", "entities": []}, {"text": "length \u2018 is form < country > \u2019 , \u2019 originate \u2019 , \u2018 are an < nationality > \u2019 , \u2018 formed on < location > \u2019 , \u2019 moved to < Country > \u2019 , \u2018 descended from\u2019!origin , descendant , parenthood etc \u2019 city \u2019 with \u2019 x \u2019 peoples ! \u2019", "entities": []}, {"text": "metropolitan municipality \u2019 or \u2019 metro \u2019 \u2018 was painted with \u2019 , \u2018 mosaic \u2019 , \u2018 oil \u2019 , \u2018 water \u2019 ! medium \u2018 hung in \u2019 , \u2018 museum \u2019 , \u2018 is stored in / at \u2019 , \u2018 wall \u2019 , \u2018 mural \u2019 ! \u2019 location \u2019 \u2018 was discontinued \u2019 , \u2018 awards \u2019 ! \u2018 last awarded \u2019 \u2019 playing bass\u2019!\u2019instruments \u2019 \u2018 served \u2019 , \u2018 term \u2019 , \u2018 current charge \u2019 , \u2018 in - charge \u2019 ! \u2019 in of\ufb01ce \u2019 \u2018 is controlled by \u2019 , \u2018 under control \u2019 ! \u2019 government \u2019 \u2018 classical \u2019 , \u2018 pop \u2019 , \u2018 rock \u2019 , \u2018 hip - hop \u2019 , \u2018 su\ufb01 \u2019 ! genre \u2018 won more \u2019 , \u2018 in winning ( race ) \u2019 , \u2018 earned more than \u2019 !", "entities": []}, {"text": "earnings \u2018 Register of \u2019 , \u2018 Cultural Properties \u2019 ! designated \u2018 urban area \u2019 , \u2018 less dense \u2019 - > urban density , density \u2018 founded by \u2019 , \u2018 has been around \u2019 , \u2018 years \u2019 !", "entities": []}, {"text": "founded , introduce \u2018 was started \u2019 , \u2018 century \u2019 , \u2018 was formed \u2019 , \u2019 100 years \u2019 !", "entities": []}, {"text": "founded , formation \u2018 daughters \u2019 , \u2018 sons \u2019 !", "entities": []}, {"text": "children spouse(s ) , partner(s ) \u2018 lost money \u2019 , \u2018 net pro\ufb01t \u2019 , \u2018 budget \u2019 , \u2018 unpro\ufb01table \u2019 , \u2019 not popular\u2019(common sense ) \u2018 owned \u2019 or \u2018 company \u2019 !", "entities": []}, {"text": "manufacturer \u2018 bigger than an average \u2019 ! dimension3283", "entities": []}]