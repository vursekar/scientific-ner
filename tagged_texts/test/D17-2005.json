[{"text": "Proceedings of the 2017 EMNLP System Demonstrations , pages 25\u201330 Copenhagen , Denmark , September 7\u201311 , 2017 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2017 Association for Computational Linguistics SGNMT \u2013 A Flexible NMT Decoding Platform for Quick Prototyping of New Models and", "entities": []}, {"text": "Search Strategies Felix Stahlberg\u2020andEva Hasler\u2021andDanielle Saunders\u2020andBill Byrne\u2021\u2020 \u2020Department of Engineering , University of Cambridge , UK \u2021SDL Research , Cambridge ,", "entities": [[13, 14, "DatasetName", "Cambridge"], [19, 20, "DatasetName", "Cambridge"]]}, {"text": "UK Abstract This paper introduces SGNMT , our experimental platform for machine translation research .", "entities": [[11, 13, "TaskName", "machine translation"]]}, {"text": "SGNMT provides a generic interface to neural and symbolic scoring modules ( predictors ) with left - to - right semantic such as translation models like NMT , language models , translation lattices , n - best lists or other kinds of scores and constraints .", "entities": []}, {"text": "Predictors can be combined with other predictors to form complex decoding tasks .", "entities": []}, {"text": "SGNMT implements a number of search strategies for traversing the space spanned by the predictors which are appropriate for different predictor constellations .", "entities": []}, {"text": "Adding new predictors or decoding strategies is particularly easy , making it a very ef\ufb01cient tool for prototyping new research ideas .", "entities": []}, {"text": "SGNMT is actively being used by students in the MPhil program in Machine Learning , Speech and Language Technology at the University of Cambridge for course work and theses , as well as for most of the research work in our group .", "entities": [[23, 24, "DatasetName", "Cambridge"]]}, {"text": "1 Introduction We are developing an open source decoding framework called SGNMT , short for Syntactically Guided Neural Machine Translation.1The software package supports a number of well - known frameworks , including TensorFlow2(Abadi et al . , 2016 ) , OpenFST ( Allauzen et al . , 2007 ) , Blocks / Theano ( Bastien et al . , 2012 ; van Merri \u00a8enboer et al . , 2015 ) , and NPLM ( Vaswani et al . , 2013 ) .", "entities": []}, {"text": "The two central concepts in the 1http://ucam-smt.github.io/sgnmt/html/ 2SGNMT relies on the TensorFlow fork available at https://github.com/ehasler/tensorflowSGNMT tool are predictors anddecoders .", "entities": []}, {"text": "Predictors are scoring modules which de\ufb01ne scores over the target language vocabulary given the current internal predictor state , the history , the source sentence , and external side information .", "entities": []}, {"text": "Scores from multiple , diverse predictors can be combined for use in decoding .", "entities": []}, {"text": "Decoders are search strategies which traverse the space spanned by the predictors .", "entities": []}, {"text": "SGNMT provides implementations of common search tree traversal algorithms like beam search .", "entities": []}, {"text": "Since decoders differ in runtime complexity and the kind of search errors they make , different decoders are appropriate for different predictor constellations .", "entities": []}, {"text": "The strict separation of scoring module and search strategy and the decoupling of scoring modules from each other makes SGNMT a very \ufb02exible decoding tool for neural and symbolic models which is applicable not only to machine translation .", "entities": [[36, 38, "TaskName", "machine translation"]]}, {"text": "SGNMT is based on the OpenFSTbased Cambridge SMT system ( Allauzen et al . , 2014 ) .", "entities": [[6, 7, "DatasetName", "Cambridge"]]}, {"text": "Although the system is less than a year old , we have found it to be very \ufb02exible and easy for new researchers to adopt .", "entities": []}, {"text": "Our group has already integrated SGNMT into most of its research work .", "entities": []}, {"text": "We also \ufb01nd that SGNMT is very well - suited for teaching and student research projects .", "entities": []}, {"text": "In the 2015 - 16 academic year , two students on the Cambridge MPhil in Machine Learning , Speech and Language Technology used SGNMT for their dissertation projects.3The \ufb01rst project involved using SGNMT with OpenFST for applying subword models in SMT ( Gao , 2016 ) .", "entities": [[12, 13, "DatasetName", "Cambridge"]]}, {"text": "The second project developed automatic music composition by LSTMs where WFSAs were used to de\ufb01ne the space of allowable chord progressions in \u2018 Bach \u2019 chorales ( Tomczak , 2016 ) .", "entities": []}, {"text": "The LSTM provides the \u2018 creativity \u2019 and the WFSA enforces constraints 3http://www.mlsalt.eng.cam.ac.uk/Main/ CurrentMPhils25", "entities": [[1, 2, "MethodName", "LSTM"]]}, {"text": "Predictor Predictor state initialize ( \u00b7 ) predict next ( ) consume(token ) NMT State vector in the GRU or LSTM layer of the decoder network and current context vector .", "entities": [[18, 19, "MethodName", "GRU"], [20, 21, "MethodName", "LSTM"]]}, {"text": "Run encoder network to compute annotations .", "entities": []}, {"text": "Forward pass through the decoder to compute the posterior given the current decoder GRU or LSTM state and the context vector .", "entities": [[13, 14, "MethodName", "GRU"], [15, 16, "MethodName", "LSTM"]]}, {"text": "Feed back token to the NMT network and update the decoder state and the context vector .", "entities": []}, {"text": "FST ID of the current node in the FST.Load FST from the \ufb01le system , set the predictor state to the FST start node .", "entities": []}, {"text": "Explore all outgoing edges of the current node and use arc weights as scores .", "entities": []}, {"text": "Traverse the outgoing edge from the current node labelled with token and update the predictor state to the target node .", "entities": []}, {"text": "n - gram Currentn - gram history Set the current n - gram history to the begin - ofsentence symbol .", "entities": []}, {"text": "Return the LM scores for the current n - gram history .", "entities": [[0, 1, "MetricName", "Return"]]}, {"text": "Addtoken to the currentn - gram history .", "entities": []}, {"text": "Word count None Empty Return a cost of 1 for all tokens except < /s > .Empty", "entities": [[4, 5, "MetricName", "Return"]]}, {"text": "UNK count Number of consumed UNK tokens .", "entities": []}, {"text": "Set UNK counter to 0 , estimate the \u03bbparameter of the Poisson distribution based on source sentence features .", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "For</s > use the logprobability of the current number of UNKs given\u03bb .", "entities": []}, {"text": "Use zero for all other tokens .", "entities": []}, {"text": "Increase internal counter by 1 if token is UNK .", "entities": []}, {"text": "Table 1 : Predictor operations for the NMT , FST , n - gram LM , and counting modules .", "entities": []}, {"text": "that the chorales must obey .", "entities": []}, {"text": "This second project in particular demonstrates the versatility of the approach .", "entities": []}, {"text": "For the current , 2016 - 17 academic year , SGNMT is being used heavily in two courses .", "entities": []}, {"text": "2 Predictors SGNMT consequently emphasizes \ufb02exibility and extensibility by providing a common interface to a wide range of constraints or models used in MT research .", "entities": []}, {"text": "The concept facilitates quick prototyping of new research ideas .", "entities": []}, {"text": "Our platform aims to minimize the effort required for implementation ; decoding speed is secondary as optimized code for production systems can be produced once an idea has been proven successful in the SGNMT framework .", "entities": []}, {"text": "In SGNMT , scores are assigned to partial hypotheses via one or many predictors .", "entities": []}, {"text": "One predictor usually has a single responsibility as it represents a single model or type of constraint .", "entities": []}, {"text": "Predictors need to implement the following methods : \u2022initialize(src sentence )", "entities": []}, {"text": "Initialize the predictor state using the source sentence .", "entities": []}, {"text": "\u2022getstate ( )", "entities": []}, {"text": "Get the internal predictor state .", "entities": []}, {"text": "\u2022setstate(state ) Set the internal predictor state .", "entities": []}, {"text": "\u2022predict next ( ) Given the internal predictor state , produce the posterior over target tokens for the next position .", "entities": []}, {"text": "Predictor Description nmt Attention - based neural machine translation following Bahdanau et al .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "( 2015 ) .", "entities": []}, {"text": "Supports Blocks / Theano ( Bastien et al . , 2012 ; van Merri \u00a8enboer et al . , 2015 ) and TensorFlow ( Abadi et al . , 2016 ) .", "entities": []}, {"text": "fst Predictor for rescoring deterministic lattices ( Stahlberg et al . , 2016 ) .", "entities": []}, {"text": "nfst Predictor for rescoring nondeterministic lattices .", "entities": []}, {"text": "rtn Rescoring recurrent transition networks ( RTNs ) as created by HiFST ( Allauzen et al . , 2014 ) with late expansion .", "entities": [[0, 1, "DatasetName", "rtn"]]}, {"text": "srilm n - gram Kneser - Ney language model using the SRILM ( Hea\ufb01eld et", "entities": []}, {"text": "al . , 2013 ; Stolcke et al . , 2002 ) toolkit .", "entities": []}, {"text": "nplm Neuraln - gram language models based on NPLM ( Vaswani et al . , 2013 ) .", "entities": []}, {"text": "rnnlm Integrates RNN language models with TensorFlow as described by Zaremba", "entities": []}, {"text": "et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2014 ) .", "entities": []}, {"text": "forced Forced decoding with a single reference .", "entities": []}, {"text": "forcedlst n - best list rescoring .", "entities": []}, {"text": "bow Restricts the search space to a bag of words with or without repetition ( Hasler et al . , 2017 ) .", "entities": []}, {"text": "lrhiero Experimental implementation of leftto - right Hiero ( Siahbani et al . , 2013 ) for small grammars .", "entities": []}, {"text": "wc Number of words feature .", "entities": []}, {"text": "unkc Applies a Poisson model for the number of UNKs in the output .", "entities": []}, {"text": "ngramc Integrates external n - gram posteriors , e.g. for MBR - based NMT according Stahlberg et al . ( 2017 ) .", "entities": []}, {"text": "length Target sentence length model using simple source sentence features .", "entities": []}, {"text": "Table 2 : Currently implemented predictors.26", "entities": []}, {"text": "\u2022consume(token ) Update the internal predictor state by adding token to the current history .", "entities": []}, {"text": "The structure of the predictor state and the implementations of these methods differ substantially between predictors .", "entities": []}, {"text": "Tab . 2 lists all predictors which are currently implemented .", "entities": []}, {"text": "Tab . 1 summarizes the semantics of this interface for three very common predictors : the neural machine translation ( NMT ) predictor , the ( deterministic ) \ufb01nite state transducer ( FST ) predictor for lattice rescoring , and the n - gram predictor for applying n - gram language models .", "entities": [[17, 19, "TaskName", "machine translation"]]}, {"text": "We also included two examples ( word count and UNK count ) which do not have a natural left - to - right semantic but can still be represented as predictors .", "entities": []}, {"text": "2.1 Example Predictor Constellations SGNMT allows combining any number of predictors and even multiple instances of the same predictor type .", "entities": []}, {"text": "In case of multiple predictors we combine the predictor scores in a linear model .", "entities": []}, {"text": "The following list illustrates that various interesting decoding tasks can be formulated as predictor combinations .", "entities": []}, {"text": "\u2022nmt : A single NMT predictor represents pure NMT decoding .", "entities": []}, {"text": "\u2022nmt , nmt , nmt : Using multiple NMT predictors is a natural way to represent ensemble decoding ( Hansen and Salamon , 1990 ; Sutskever et al . , 2014 ) in our framework .", "entities": []}, {"text": "\u2022fst , nmt : NMT decoding constrained to an FST .", "entities": []}, {"text": "This can be used for neural lattice rescoring ( Stahlberg et al . , 2016 ) or other kinds of constraints , for example in the context of source side simpli\ufb01cation in MT ( Hasler et al . , 2016 ) or chord progressions in \u2018 Bach \u2019 ( Tomczak , 2016 ) .", "entities": []}, {"text": "The fstpredictor can also be used to restrict the output of character - based or subword - unit - based NMT to a large word - level vocabulary encoded as FSA .", "entities": []}, {"text": "\u2022nmt , rnnlm , srilm , nplm : Combining NMT with three kinds of language models : An RNNLM ( Zaremba et al . , 2014 ) , a Kneser - Ney n - gram LM ( Hea\ufb01eld et al . , 2013 ; Stolcke et al . , 2002 ) , and a feedforward neural network LM ( Vaswani et al . , 2013).Decoder Description greedy Greedy decoding .", "entities": []}, {"text": "beam Beam search as described in Bahdanau et al .", "entities": []}, {"text": "( Bahdanau et al . , 2015 ) .", "entities": []}, {"text": "dfs Depth-\ufb01rst search .", "entities": []}, {"text": "Ef\ufb01ciently enumerates the complete search space , e.g. for exhaustive FST - based rescoring .", "entities": []}, {"text": "restarting Similar to DFS but with better admissible pruning behaviour .", "entities": []}, {"text": "astar A * search ( Russell and Norvig , 2003 ) .", "entities": []}, {"text": "The heuristic function can be de\ufb01ned via predictors .", "entities": []}, {"text": "sepbeam Associates hypotheses in the beam with only one predictor .", "entities": []}, {"text": "Ef\ufb01ciently approximates system - level combination .", "entities": []}, {"text": "syncbeam Beam search which compares hypotheses after consuming a special synchronization symbol rather than after each iteration .", "entities": []}, {"text": "bucket Multiple beam search passes with small beam size .", "entities": []}, {"text": "Can have better pruning behaviour than standard beam search .", "entities": []}, {"text": "vanilla Fast beam search decoder for ( ensembled ) NMT .", "entities": []}, {"text": "This implementation is similar to the decoder in Blocks ( van Merri \u00a8enboer et al . , 2015 ) but can only be used for NMT as it bypasses the predictor framework .", "entities": []}, {"text": "Table 3 : Currently implemented decoders .", "entities": []}, {"text": "\u2022nmt , ngramc , wc : MBR - based NMT following Stahlberg et al . ( 2017 ) with n - gram posteriors extracted from an SMT lattice ( ngramc ) and a simple word penalty ( wc ) .", "entities": []}, {"text": "3 Decoders Decoders are algorithms to search for the highest scoring hypothesis .", "entities": []}, {"text": "The list of predictors determines how ( partial ) hypotheses are scored by implementing the methods initialize ( \u00b7 ) , getstate ( ) , setstate(\u00b7),predict next ( ) , and consume ( \u00b7 ) .", "entities": []}, {"text": "The Decoder class implements versions of these methods which apply to all predictors in the list .", "entities": []}, {"text": "initialize(\u00b7)is always called prior to decoding a new sentence .", "entities": []}, {"text": "Many popular search strategies can be described via the remaining methods getstate ( ) , setstate(\u00b7),predict next ( ) , and consume ( \u00b7 ) .", "entities": []}, {"text": "Algs .", "entities": []}, {"text": "1 and 2 show how to de\ufb01ne greedy and beam decoding in this way.45 Tab .", "entities": []}, {"text": "3 contains a list of currently implemented decoders .", "entities": []}, {"text": "The UML diagram in Fig .", "entities": []}, {"text": "1 illustrates the relation between decoders and predictors .", "entities": []}, {"text": "4Formally , predict next ( ) in Algs .", "entities": []}, {"text": "1 and 2 returns pairs of tokens and their costs .", "entities": []}, {"text": "5String concatenation is denoted with \u00b7 .27", "entities": []}, {"text": "Figure 1 : Reduced UML class diagram .", "entities": []}, {"text": "Algorithm 1 Greedy(src sen ) 1 : initialize ( srcsen ) 2 : h\u2190/angbracketleft < s>/angbracketright 3 : repeat 4 : P\u2190predict next ( ) 5 : ( t , c)\u2190arg max(t / prime , c / prime)\u2208Pc / prime 6 : h\u2190h\u00b7t 7 : consume ( t ) 8 : untilt=</s > 9 : return h NMT batch decoding The \ufb02exibility of the predictor framework comes with degradation in decoding time .", "entities": []}, {"text": "SGNMT provides two ways of speeding up pure NMT decoding , especially on the GPU .", "entities": []}, {"text": "The vanilla decoding strategy exposes the beam search implementation in Blocks ( van Merri \u00a8enboer et al . , 2015 ) which processes all active hypotheses in the beam in parallel .", "entities": []}, {"text": "We also implemented a beam decoder version which decodes multiple sentences at once ( batch decoding ) rather than in a sequential order .", "entities": []}, {"text": "Batch decoding is potentially more ef\ufb01cient since larger batches can make better use of GPU parallelism .", "entities": []}, {"text": "The key concepts of our batch decoder implementation are : \u2022We use a scheduler running on a separate CPU thread to construct large batches of computation ( GPU jobs ) from multiple sentences and feeding them to the jobs queue .", "entities": []}, {"text": "\u2022The GPU is operated by a single thread which communicates with the CPU scheduler thread via queues containing jobs .", "entities": []}, {"text": "This thread is only responsible for retrieving jobs in the jobs queue , computing them , and putting them in the jobs results queue , minimizing the down - time of GPU computation .", "entities": []}, {"text": "\u2022Yet another CPU thread is responsible for processing the results computed on the GPUAlgorithm 2 Beam ( n , src sen ) 1 : initialize ( srcsen ) 2 : H\u2190{(/angbracketleft < s>/angbracketright,0.0,getstate ( ) ) } 3 : repeat 4 : Hnext\u2190\u2205 5 : for all ( h , c , s ) \u2208Hdo 6 : setstate ( s ) 7 : P\u2190predict next ( ) 8 : Hnext\u2190Hnext\u222a/uniontext ( t / prime , c / prime)\u2208P(h\u00b7t / prime , c+c / prime , s ) 9 : end for 10 : H\u2190\u2205 11 : for all ( h , c , s ) \u2208n - best(Hnext)do 12 : setstate ( s ) 13 : consume ( h|h| ) 14 : H\u2190H\u222a{(h , c , getstate ( ) ) } 15 : end for 16 : until Best hypothesis in Hends with < /s > 17 : return Best hypothesis in H in the jobresults queue , e.g. by getting the n - best words from the posteriors .", "entities": []}, {"text": "Processed jobs are sent back to the CPU scheduler where they are reassembled into new jobs .", "entities": []}, {"text": "This decoder is able to translate the WMT English - French test sets news - test2012 tonewstest2014 on a Titan X GPU with 911.6 words per second with the word - based NMT model described in Stahlberg et al .", "entities": [[19, 20, "DatasetName", "Titan"]]}, {"text": "( 2016).6This decoding speed seems to be slightly faster than sequential decoding with high - performance NMT decoders like Marian - NMT ( Junczys - Dowmunt et al . , 2016 ) with reported decoding speeds of 865 words per second.7However , batch decoding with MarianNMT is much faster reaching over 4,500 words 6Theano 0.9.0 , cuDNN 5.1 , Cuda 8 with CNMeM , IntelR / circlecopyrt Core i7 - 6700 CPU 7Note that the comparability is rather limited since even though we use the same beam size ( 5 ) and vocabulary sizes ( 30k ) , we use ( a ) a slightly slower GPU ( Titan X vs. GTX28", "entities": [[108, 109, "DatasetName", "Titan"]]}, {"text": "per second.8We think that these differences are mainly due to the limited multithreading support and performance in Python especially when using external libraries as opposed to the highly optimized C++ code in Marian - NMT .", "entities": []}, {"text": "We did not push for even faster decoding as speed is not a major design goal of SGNMT .", "entities": []}, {"text": "Note that batch decoding bypasses the predictor framework and can only be used for pure NMT decoding .", "entities": []}, {"text": "Ensembling with models at multiple tokenization levels SGNMT allows masking predictors with alternative sets of modelling units .", "entities": []}, {"text": "The conversion between the tokenization schemes of different predictors is de\ufb01ned with FSTs .", "entities": []}, {"text": "This makes it possible to decode by combining scores from both a subword - unit ( BPE ) based NMT ( Sennrich et al . , 2016 ) and a word - based NMT model with character - based NMT , masking the BPE - based and word - based NMT predictors with FSTs which transduce character sequences to BPE or word sequences .", "entities": [[16, 17, "MethodName", "BPE"], [43, 44, "MethodName", "BPE"], [59, 60, "MethodName", "BPE"]]}, {"text": "Masking is transparent to the decoding strategy as predictors are replaced by a special wrapper ( fsttok ) that uses the masking FST to translate predict next ( ) andconsume ( ) calls to ( a series of ) predictor calls with alternative tokens .", "entities": []}, {"text": "The syncbeam variation of beam search compares competing hypotheses only after consuming a special word boundary symbol rather than after each token .", "entities": []}, {"text": "This allows combining scores at the word level even when using models with multiple levels of tokenization .", "entities": []}, {"text": "Joint decoding with different tokenization schemes has the potential of combining the bene\ufb01ts of the different schemes : character- and BPE - based models are able to address rare words , but word - based NMT can model long - range dependencies more ef\ufb01ciently .", "entities": [[20, 21, "MethodName", "BPE"]]}, {"text": "System - level combination We showed in Sec . 2.1 how to formulate NMT ensembling as a set of NMT predictors .", "entities": []}, {"text": "Ensembling averages the individual model scores in each decoding step .", "entities": []}, {"text": "Alternatively , system - level combination decodes the entire sentence with each model separately , and selects the best scoring complete hypothesis over all models .", "entities": []}, {"text": "In our experiments , system - level combination is not as effective as en1080 ) , ( b ) a different training and test set , ( c ) a slightly different network architecture , and ( d ) words rather than subword units .", "entities": []}, {"text": "8https://marian-nmt.github.io/ features / sembling but still leads to moderate gains for pure NMT .", "entities": []}, {"text": "However , a trivial implementation which selects the best translation in a postprocessing step after separate decoding runs is slow .", "entities": []}, {"text": "The sepbeam decoding strategy reduces the runtime of system - level combination to the single system level .", "entities": []}, {"text": "The strategy applies only one predictor rather than a linear combination of all predictors to expand a hypothesis .", "entities": []}, {"text": "The single predictor is linked by the parent hypothesis .", "entities": []}, {"text": "The initial stack in sepbeam contains hypotheses for each predictor ( i.e. system ) rather than only one as in normal beam search .", "entities": []}, {"text": "We report a moderate gain of 0.5 BLEU over a single system on the Japanese - English ASPEC test set ( Nakazawa et al . , 2016 ) by combining three BPE - based NMT models from Stahlberg et al .", "entities": [[7, 8, "MetricName", "BLEU"], [17, 18, "DatasetName", "ASPEC"], [31, 32, "MethodName", "BPE"]]}, {"text": "( 2017 ) using the sepbeam decoder .", "entities": []}, {"text": "Iterative beam search Normal beam search is dif\ufb01cult to use in a time - constrained setting since the runtime depends on the target sentence length which is a priori not known , and it is therefore hard to choose the right beam size beforehand .", "entities": []}, {"text": "Thebucket search algorithm sidesteps the problem of setting the beam size by repeatedly performing small beam search passes until a \ufb01xed computational budget is exhausted .", "entities": []}, {"text": "Bucket search produces an initial hypothesis very quickly , and keeps the partial hypotheses for each length in buckets .", "entities": []}, {"text": "Subsequent beam search passes re\ufb01ne the initial hypothesis by iteratively updating these buckets .", "entities": []}, {"text": "Our initial experiments suggest that bucket search often performs on a similar level as standard beam search with the bene\ufb01t of being able to support hard time constraints .", "entities": []}, {"text": "Unlike beam search , bucket search lends itself to risk - free ( i.e. admissible ) pruning since all partial hypotheses worse than the current best complete hypothesis can be discarded .", "entities": []}, {"text": "4 Conclusion This paper presented our SGNMT platform for prototyping new approaches to MT which involve both neural and symbolic models .", "entities": []}, {"text": "SGNMT supports a number of different models and constraints via a common interface ( predictors ) , and various search strategies ( decoders ) .", "entities": []}, {"text": "Furthermore , SGNMT focuses on minimizing the implementation effort for adding new predictors and decoders by decoupling scoring modules from each other and from the search algorithm .", "entities": []}, {"text": "SGNMT is actively being used for teaching and research and we29", "entities": []}, {"text": "welcome contributions to its development , for example by implementing new predictors for using models trained with other frameworks and tools .", "entities": []}, {"text": "Acknowledgments This work was supported by the U.K. Engineering and Physical Sciences Research Council ( EPSRC grant EP / L027623/1 ) .", "entities": []}, {"text": "References Mart\u0131n Abadi , Ashish Agarwal , Paul Barham , Eugene Brevdo , Zhifeng Chen , Craig Citro , Greg S Corrado , Andy Davis , Jeffrey Dean , Matthieu Devin , et al . 2016 .", "entities": []}, {"text": "Tensor\ufb02ow : Large - scale machine learning on heterogeneous distributed systems .", "entities": []}, {"text": "arXiv preprint arXiv:1603.04467 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Cyril Allauzen , Bill Byrne , Adri ` a de Gispert , Gonzalo Iglesias , and Michael Riley .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Pushdown automata in statistical machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "Computational Linguistics , 40(3):687\u2013723 .", "entities": []}, {"text": "Cyril Allauzen , Michael Riley , Johan Schalkwyk , Wojciech Skut , and Mehryar Mohri .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "OpenFst :", "entities": []}, {"text": "A general and ef\ufb01cient weighted \ufb01nite - state transducer library .", "entities": []}, {"text": "In International Conference on Implementation and Application of Automata , pages 11\u201323 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Neural machine translation by jointly learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In ICLR .", "entities": []}, {"text": "Fr\u00b4ed\u00b4eric Bastien , Pascal Lamblin , Razvan Pascanu , James Bergstra , Ian Goodfellow , Arnaud Bergeron , Nicolas Bouchard , David Warde - Farley , and Yoshua Bengio .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Theano :", "entities": []}, {"text": "New features and speed improvements .", "entities": []}, {"text": "In NIPS .", "entities": []}, {"text": "Jiameng Gao .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Variable length word encodings for neural translation models .", "entities": []}, {"text": "MPhil dissertation , University of Cambridge .", "entities": [[5, 6, "DatasetName", "Cambridge"]]}, {"text": "Lars Kai Hansen and Peter Salamon .", "entities": []}, {"text": "1990 .", "entities": []}, {"text": "Neural network ensembles .", "entities": []}, {"text": "IEEE transactions on pattern analysis and machine intelligence , 12(10):993 \u2013 1001 .", "entities": []}, {"text": "Eva Hasler , Adri ` a de Gispert , Felix Stahlberg , Aurelien Waite , and Bill Byrne . 2016 .", "entities": []}, {"text": "Source sentence simpli\ufb01cation for statistical machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "Computer Speech & Language .", "entities": []}, {"text": "Eva Hasler , Felix Stahlberg , Marcus Tomalin , Adri ` a de Gispert , and Bill Byrne . 2017 .", "entities": []}, {"text": "A comparison of neural models for word ordering .", "entities": []}, {"text": "In INLG , Santiago de Compostela , Spain .", "entities": []}, {"text": "Kenneth Hea\ufb01eld , Ivan Pouzyrevsky , Jonathan H. Clark , and Philipp Koehn .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Scalable modi\ufb01ed Kneser - Ney language model estimation .", "entities": []}, {"text": "In ACL , pages 690\u2013696 , So\ufb01a , Bulgaria .", "entities": []}, {"text": "Marcin Junczys - Dowmunt , Tomasz Dwojak , and Hieu Hoang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Is neural machine translation ready for deployment ?", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "a case study on 30 translation directions .", "entities": []}, {"text": "arXiv preprint arXiv:1610.01108 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Bart van Merri \u00a8enboer , Dzmitry Bahdanau , Vincent Dumoulin , Dmitriy Serdyuk , David Warde - Farley , Jan Chorowski , and Yoshua Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Blocks and fuel : Frameworks for deep learning .", "entities": []}, {"text": "arXiv preprint arXiv:1506.00619 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Toshiaki Nakazawa , Manabu Yaguchi , Kiyotaka Uchimoto , Masao Utiyama , Eiichiro Sumita , Sadao Kurohashi , and Hitoshi Isahara . 2016 .", "entities": []}, {"text": "ASPEC :", "entities": [[0, 1, "DatasetName", "ASPEC"]]}, {"text": "Asian scienti\ufb01c paper excerpt corpus .", "entities": []}, {"text": "In LREC , pages 2204\u20132208 , Portoroz , Slovenia .", "entities": []}, {"text": "Stuart J. Russell and Peter Norvig .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Arti\ufb01cial Intelligence :", "entities": []}, {"text": "A Modern Approach , 2 edition .", "entities": []}, {"text": "Pearson Education .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Neural machine translation of rare words with subword units .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In ACL , pages 1715\u20131725 , Berlin , Germany .", "entities": []}, {"text": "Maryam Siahbani , Baskaran Sankaran , and Anoop Sarkar .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Ef\ufb01cient left - to - right hierarchical phrase - based translation with improved reordering .", "entities": []}, {"text": "InEMNLP , pages 1089\u20131099 , Seattle , Washington , USA .", "entities": []}, {"text": "Felix Stahlberg , Adri ` a de Gispert , Eva Hasler , and Bill Byrne . 2017 .", "entities": []}, {"text": "Neural machine translation by minimising the Bayes - risk with respect to syntactic translation lattices .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In EACL , pages 362\u2013368 , Valencia , Spain .", "entities": []}, {"text": "Felix Stahlberg , Eva Hasler , Aurelien Waite , and Bill Byrne . 2016 .", "entities": []}, {"text": "Syntactically guided neural machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "In ACL , pages 299\u2013305 , Berlin , Germany .", "entities": []}, {"text": "Andreas Stolcke et al . 2002 .", "entities": []}, {"text": "SRILM \u2013 an extensible language modeling toolkit .", "entities": []}, {"text": "In Interspeech , volume 2002 , page 2002 .", "entities": []}, {"text": "Ilya Sutskever , Oriol Vinyals , and Quoc V .", "entities": []}, {"text": "Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "In NIPS , pages 3104\u20133112 .", "entities": []}, {"text": "MIT Press .", "entities": []}, {"text": "Marcin Tomczak . 2016 .", "entities": []}, {"text": "Bachbot .", "entities": []}, {"text": "MPhil dissertation , University of Cambridge .", "entities": [[5, 6, "DatasetName", "Cambridge"]]}, {"text": "Ashish Vaswani , Yinggong Zhao , Victoria Fossum , and David Chiang .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Decoding with largescale neural language models improves translation .", "entities": []}, {"text": "InEMNLP , pages 1387\u20131392 , Seattle , Washington , USA .", "entities": []}, {"text": "Wojciech Zaremba , Ilya Sutskever , and Oriol Vinyals .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Recurrent neural network regularization .", "entities": []}, {"text": "arXiv preprint arXiv:1409.2329 .30", "entities": [[0, 1, "DatasetName", "arXiv"]]}]