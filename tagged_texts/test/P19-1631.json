[{"text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6274\u20136283 Florence , Italy , July 28 - August 2 , 2019 .", "entities": [[15, 16, "MethodName", "Florence"]]}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics6274Incorporating Priors with Feature Attribution on Text Classi\ufb01cation Frederick Liu", "entities": []}, {"text": "Besim Avci Google ffrederickliu , besim g@google.com", "entities": [[2, 3, "DatasetName", "Google"]]}, {"text": "Abstract Feature attribution methods , proposed recently , help users interpret the predictions of complex models .", "entities": []}, {"text": "Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building .", "entities": []}, {"text": "To demonstrate the effectiveness our technique , we apply it to two tasks : ( 1 ) mitigating unintended bias in text classi\ufb01ers by neutralizing identity terms ; ( 2 ) improving classi\ufb01er performance in a scarce data setting by forcing the model to focus on toxic terms .", "entities": []}, {"text": "Our approach adds an L2 distance loss between feature attributions and task - speci\ufb01c prior values to the objective .", "entities": [[6, 7, "MetricName", "loss"]]}, {"text": "Our experiments show that i ) a classi\ufb01er trained with our technique reduces undesired model biases without a tradeoff on the original task ; ii ) incorporating priors helps model performance in scarce data settings .", "entities": []}, {"text": "1 Introduction One of the recent challenges in machine learning ( ML ) is interpreting the predictions made by models , especially deep neural networks .", "entities": []}, {"text": "Understanding models is not only bene\ufb01cial , but necessary for wide - spread adoption of more complex ( and potentially more accurate ) ML models .", "entities": []}, {"text": "From healthcare to \ufb01nancial domains , regulatory agencies mandate entities to provide explanations for their decisions ( Goodman and Flaxman , 2016 ) .", "entities": []}, {"text": "Hence , most machine learning progress made in those areas is hindered by a lack of model explainability \u2013 causing practitioners to resort to simpler , potentially low - performance models .", "entities": []}, {"text": "To supply for this demand , there has been many attempts for model interpretation in recent years for tree - based algorithms ( Lundberg et al . , 2018 ) and deep learning algorithms ( Lundberg and Lee , 2017 ; Smilkov et al . , 2017 ; Sundararajan et al . , 2017 ; Bach et al . , 2015 ;", "entities": []}, {"text": "Kim et al . , 2018 ; Dhurandhar et al . , 2018).Method Sentence Probability BaselineIam gay 0.915 Iam straight 0.085 Our MethodIam gay 0.141 Iam straight 0.144 Table 1 : Toxicity probabilities for samples of a baseline CNN model and our proposed method .", "entities": []}, {"text": "Words are shaded based on their attribution and italicized if attribution is>0 .", "entities": []}, {"text": "On the other hand , the amount of research focusing on explainable natural language processing ( NLP ) models ( Li et al . , 2016 ; Murdoch et al . , 2018 ; Lei et al . , 2016 ) is modest as opposed to image explanation techniques .", "entities": []}, {"text": "Inherent problems in data emerge in a trained model in several ways .", "entities": []}, {"text": "Model explanations can show that the model is not inline with human judgment or domain expertise .", "entities": []}, {"text": "A canonical example is model unfairness , which stems from biases in the training data .", "entities": []}, {"text": "Fairness in ML models rightfully came under heavy scrutiny in recent years ( Zhang et al . , 2018a ; Dixon et al . , 2018 ; Angwin et al . , 2016 ) .", "entities": [[0, 1, "TaskName", "Fairness"]]}, {"text": "Some examples include sentiment analysis models weighing negatively for inputs containing identity terms such as \u201c jew \u201d and \u201c black \u201d , and hate speech classi\ufb01ers leaning to predict anysentence containing \u201c islam \u201d as toxic ( Waseem and Hovy , 2016 ) .", "entities": [[3, 5, "TaskName", "sentiment analysis"], [24, 26, "DatasetName", "hate speech"]]}, {"text": "If employed , explanation techniques help divulge these issues , but fail to offer a remedy .", "entities": []}, {"text": "For instance , the sentence \u201c I am gay \u201d receives a high score on a toxicity model as seen in Table 1 .", "entities": []}, {"text": "The Integrated Gradients ( Sundararajan et al . , 2017 ) explanation method attributes the majority of this decision to the word \u201c gay . \u201d", "entities": []}, {"text": "However , none of the explanations methods suggest next steps to \ufb01x the issue .", "entities": []}, {"text": "Instead , researchers try to reduce biases indirectly by mostly adding more data ( Dixon et al . ,", "entities": []}, {"text": "62752018 ; Chen et al . , 2018 ) , using unbiased word vectors ( Park et al . , 2018 ) , or directly optimizing for a fairness proxy with adversarial training ( Madras et al . , 2018 ; Zhang et al . , 2018a ) .", "entities": []}, {"text": "These methods either offer to collect more data , which is costly in many cases , or make a tradeoff between original task performance and fairness .", "entities": []}, {"text": "In this paper , we attempt to enable injecting priors through model explanations to rectify issues in trained models .", "entities": []}, {"text": "We demonstrate our approach on two problems in text classi\ufb01cation settings : ( 1 ) model biases towards protected identity groups ; ( 2 ) low classi\ufb01cation performance due to lack of data .", "entities": []}, {"text": "The core idea is to add L2distance between Path Integrated Gradients attributions for pre - selected tokens and a target attribution value in the objective function as a loss term .", "entities": [[28, 29, "MetricName", "loss"]]}, {"text": "For model fairness , we impose the loss on keywords identifying protected groups with target attribution of 0 , so the trained model is penalized for attributing model decisions to those keywords .", "entities": [[7, 8, "MetricName", "loss"], [17, 18, "DatasetName", "0"]]}, {"text": "Our main intuition is that undesirable correlations between toxicity labels and instances of identity terms cause the model to learn unfair biases which can be corrected by incorporating priors on these identity terms .", "entities": []}, {"text": "Moreover , our approach allows practitioners to impose priors in the other direction to tackle the problem of training a classi\ufb01er when there is only a small amount of data .", "entities": []}, {"text": "As shown in our experiments , by setting a positive target attribution for known toxic words1 , one can improve the performance of a toxicity classi\ufb01er in a scarce data regime .", "entities": []}, {"text": "We validate our approach on the Wikipedia toxic comments dataset ( Wulczyn et al . , 2017 ) .", "entities": []}, {"text": "Our fairness experiments show that the classi\ufb01ers trained with our method achieve the same performance , if not better , on the original task , while improving AUC and fairness metrics on a synthetic , unbiased dataset .", "entities": [[27, 28, "MetricName", "AUC"]]}, {"text": "Models trained with our technique also show lower attributions to identity terms on average .", "entities": []}, {"text": "Our technique produces much better word vectors as a by - product when compared to the baseline .", "entities": []}, {"text": "Lastly , by setting an attribution target of 1on toxic words , a classi\ufb01er trained with our objective function achieves better performance when only a subset of the data is present .", "entities": []}, {"text": "1Full list of identity terms and toxic terms used as priors can be found in supplemental material .", "entities": []}, {"text": "Please note the toxic terms are not censored.2 Feature Attribution In this section , we give formal de\ufb01nitions of feature attribution and a primer on [ Path ] Integrated Gradients ( IG ) , which is the basis for our method .", "entities": []}, {"text": "De\ufb01nition 2.1 .", "entities": []}, {"text": "Given a function f : Rn !", "entities": []}, {"text": "[ 0;1]that represents a model , and an input x= ( x1;:::;xn)2Rn .", "entities": []}, {"text": "An attribution of the prediction at input xis a vector a= ( a1;:::;an)andaiis de\ufb01ned as the attribution of xi .", "entities": []}, {"text": "Feature attribution methods have been studied to understand the contribution of each input feature to the output prediction score .", "entities": []}, {"text": "This contribution , then , can further be used to interpret model decisions .", "entities": []}, {"text": "Linear models are considered to be more desirable because of their implicit interpretability , where feature attribution is the product of the feature value and the coef\ufb01cient .", "entities": []}, {"text": "To some , non - linear models such as gradient boosting trees and neural networks are less favorable due to the fact that they do not enjoy such transparent contribution of each feature and are harder to interpret ( Lou et al . , 2012 ) .", "entities": []}, {"text": "Despite the complexity of these models , prior work has been able to extract attributions with gradient based methods ( Smilkov et al . , 2017 ) , Shapley values from game theory ( SHAP ) ( Lundberg and Lee , 2017 ) , or other similar methods ( Bach et al . , 2015 ; Shrikumar et al . , 2017 ) .", "entities": [[34, 35, "MethodName", "SHAP"]]}, {"text": "Some of these attributions methods , for example Path Intergrated Gradients and SHAP , not only follow Definition 2.1 , but also satisfy axioms or properties that resemble linear models .", "entities": [[12, 13, "MethodName", "SHAP"]]}, {"text": "One of these axioms is completeness , which postulates that the sum of attributions should be equal to the difference between uncertainty and model output .", "entities": []}, {"text": "Integrated Gradients Integrated Gradients ( Sundararajan et al . , 2017 ) is a model attribution technique applicable to all models that have differentiable inputs w.r.t . outputs .", "entities": []}, {"text": "IG produces feature attributions relative to an uninformative baseline .", "entities": []}, {"text": "This baseline input is designed to produce a high - entropy prediction representing uncertainty .", "entities": []}, {"text": "IG , then , interpolates the baseline towards the actual input , with the prediction moving from uncertainty to certainty in the process .", "entities": []}, {"text": "Building on the notion that the gradient of a function , f , with respect to input can characterize sensitivity of ffor each input dimension , IG simply aggregates the gradients of fwith respect to the input along this path using a path integral .", "entities": []}, {"text": "6276The crux of using path integral rather than overall gradient at the input is that f \u2019s gradients might have been saturated around the input and integrating over a path alleviates this phenomenon .", "entities": []}, {"text": "Even though there can be in\ufb01nitely many paths from a baseline to input point , Integrated Gradients takes the straight path between the two .", "entities": []}, {"text": "We give the formal de\ufb01nition from the original paper in 2.2 .", "entities": []}, {"text": "De\ufb01nition 2.2 .", "entities": []}, {"text": "Given an input xand baseline x0 , the integrated gradient along the ithdimension is de\ufb01ned as follows .", "entities": []}, {"text": "IGi(x;x0 ) : : = ( xi\u0000x0i)\u0002Z1 \u000b = 0@f(x0 + \u000b \u0002(x\u0000x0 ) ) @xid \u000b ( 1 ) where@f(x ) @xirepresents the gradient of falong the ithdimension at x.", "entities": []}, {"text": "In the NLP setting , xis the concatenated embedding of the input sequence .", "entities": []}, {"text": "The attribution of each token is the sum of the attributions of its embedding .", "entities": []}, {"text": "There are other explainability methods that attribute a model \u2019s decision to its features , but we chose IG in this framework due to several of its characteristics .", "entities": []}, {"text": "First , it is both theoretically justi\ufb01ed ( Sundararajan et al . , 2017 ) and proven to be effective in NLP - related tasks ( Mudrakarta et al . , 2018 ) .", "entities": []}, {"text": "Second , the IG formula in 2.2 is differentiable everywhere with respect to model parameters .", "entities": []}, {"text": "Lastly , it is lightweight in terms of implementation and execution complexity .", "entities": []}, {"text": "3 Incorporating Priors Problems in data manifest themselves in a trained model \u2019s performance on classi\ufb01cation or fairness metrics .", "entities": []}, {"text": "Traditionally , model de\ufb01ciencies were addressed by providing priors through extensive feature engineering and collecting more data .", "entities": [[11, 13, "TaskName", "feature engineering"]]}, {"text": "Recently , attributions help uncover de\ufb01ciencies causing models to perform poorly , but do not offer actionability .", "entities": []}, {"text": "To this end , we propose to add an extra term to the objective function to penalize the L2distance between model attributions on certain features and target attribution values .", "entities": []}, {"text": "This modi\ufb01cation allows model practitioners to inject priors .", "entities": []}, {"text": "For example , consider a model that tends to predict every sentence containing \u201c gay \u201d as toxic in a comment moderation system .", "entities": []}, {"text": "Penalizing non - zero attributions on the tokens identifying protected groupswould force the model to focus more on the context words rather than mere existence of certain tokens .", "entities": []}, {"text": "We give the formal de\ufb01nition of the new objective function that incorporates priors as the follows : De\ufb01nition 3.1 .", "entities": []}, {"text": "Given a vector tof sizen , where nis the length of the input sequence and tiis the attribution target value for the ith token in the input sequence .", "entities": []}, {"text": "The prior loss for a scalar output is de\ufb01ned as : Lprior(a;t ) = nX i(ai\u0000ti)2(2 ) whereairefers to attribution of the ith token as in De\ufb01nition 2.1 .", "entities": [[2, 3, "MetricName", "loss"]]}, {"text": "For a multi - class problem , we train our model with the following joint objective , Ljoint = L(y;p ) + \u0015CX cLprior(ac;tc ) ( 3 ) where acandtcare the attribution and attribution target for class c,\u0015is the hyperparameter that controls the stength of the prior loss and Lis the crossentropy loss de\ufb01ned as follows : L(y;p )", "entities": [[47, 48, "MetricName", "loss"], [52, 53, "MetricName", "loss"]]}, {"text": "= CX c\u0000yclog(pc ) ( 4 ) where yis an indicator vector of the ground truth label andpcis the posterior probability of class c.", "entities": []}, {"text": "The joint objective function is differentiable w.r.t . model parameters when attribution is calculated through Equation 1 and can be trained with most off - the - shelf optimizers .", "entities": []}, {"text": "The proposed objective is not dataset - dependent and is applicable to different problem settings such as sentiment classi\ufb01cation , abuse detection , etc .", "entities": [[20, 22, "TaskName", "abuse detection"]]}, {"text": "It only requires users to specify the target attribution value for tokens of interest in the corpus .", "entities": []}, {"text": "We illustrate the effectiveness of our method by applying it to a toxic comment classi\ufb01cation problem .", "entities": []}, {"text": "In the next section , we \ufb01rst show how we set the target attribution value for identity terms to remove unintended biases while retaining the same performance on the original task .", "entities": []}, {"text": "Then , using the same technique , we show how to set target attribution for toxic words to improve classi\ufb01er performance in a scarce data setting .", "entities": []}, {"text": "6277Identity Base Imp TOK Ours gay .272 .353 -.006 .000 homosexual .085 .388 -.006 -.000 queer .071", "entities": []}, {"text": ".28", "entities": []}, {"text": "-.006 .000 teenage .030", "entities": []}, {"text": "-0.02 -.006 -.001 lesbian .012", "entities": []}, {"text": ".046 -.006 .001 vocab avg -.002 -.001 -.004 -.001 Table 2 : Subset of identity terms we used and their mean attribution value on the test set .", "entities": []}, {"text": "Method names are abbreviated with the pre\ufb01x .", "entities": []}, {"text": "The last row is the average across all vocabularies .", "entities": []}, {"text": "4 Experiments We incorporate human prior in model building on two applications .", "entities": []}, {"text": "First , we tackle the problem of unintended bias in toxic comment classi\ufb01cation ( Dixon et al . , 2018 ) with our proposed method .", "entities": []}, {"text": "For our experiments , we aim to mitigate the issue of neutral sentences with identity terms being classi\ufb01ed as toxic for a given a set of identity terms .", "entities": []}, {"text": "A subset of the identity terms are listed in the \ufb01rst column of Table 2 .", "entities": []}, {"text": "Second , we force the model to focus on a list of human - selected toxic terms under scarce data scenario to increase model performance .", "entities": []}, {"text": "In the following section , we introduce the dataset we train and evaluate on along with a synthetic dataset to further validate our fairness improvements .", "entities": []}, {"text": "After that , we describe our experimental setup .", "entities": []}, {"text": "Then , we compare our method to a classi\ufb01er trained without the prior loss and 2 other baselines .", "entities": [[13, 14, "MetricName", "loss"]]}, {"text": "Lastly , we show the results demonstrating usefulness of our approach with data scarcity .", "entities": []}, {"text": "4.1 Dataset In this work , we use a dataset containing comments from Wikipedia Talk Pages ( Dixon et al . , 2018 ) .", "entities": []}, {"text": "Number of samples are 95,692 / 32,128 / 31,866 in the train / dev / test sets respectively .", "entities": [[0, 3, "HyperparameterName", "Number of samples"]]}, {"text": "The ratio of positive ( toxic ) labels in the training set is 9.7 % .", "entities": []}, {"text": "The dataset was annotated by human raters , where toxicity was de\ufb01ned as a \u201c rude , disrespectful , or unreasonable comment that is likely to make you leave a discussion \u201d per Dixon et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "Please refer to the corresponding paper for more details about collection methodology , biases present in the data , and toxicity distribution per comment length .", "entities": []}, {"text": "We also use a synthetically generated dataset to validate our approach on fairness as in Park et", "entities": []}, {"text": "al .", "entities": []}, {"text": "Template Examples Labels I am Identity Non - toxic Hug Identity Non - toxic Being Identity is wonderful Non - toxic I hate Identity Toxic You are a moronic Identity Toxic Name is a disgusting Identity Toxic Table", "entities": []}, {"text": "3 : Example templates used to generate the synthetic dataset for evaluation .", "entities": []}, {"text": "( 2018 ) ;", "entities": []}, {"text": "Dixon et al . ( 2018 ) .", "entities": []}, {"text": "The dataset is created using a set of templates , developed by Dixon et al .", "entities": []}, {"text": "( 2018)2 , where placeholders are replaced with different identity terms .", "entities": []}, {"text": "We show a subset of example templates in Table 3 and selected identity terms along with their mean attributions across the test set in Table 2 .", "entities": []}, {"text": "We mainly evaluate the effectiveness of our debiasing technique on this dataset because the original test sets follow the same biased distribution .", "entities": []}, {"text": "Intuition is that predictions returned for sentences containing different identity terms in the exact same context should be similar .", "entities": []}, {"text": "Hence , this dataset enables us to quantify the performance of a classi\ufb01er in more detail when controlled on identity .", "entities": []}, {"text": "4.2 Experimental Setup For the text classi\ufb01er , we built a convolutional neural network ( CNN ) classi\ufb01er as in Kim ( 2014 ) .", "entities": []}, {"text": "The network contains a convolution layer with 128 2- , 3- , 4 - gram \ufb01lters for a sequence length of 100 followed by a max - pooling layer and softmax function .", "entities": [[4, 5, "MethodName", "convolution"], [30, 31, "MethodName", "softmax"]]}, {"text": "Embeddings were randomly initialized and their size was set to 128 .", "entities": []}, {"text": "Shorter sequences are padded with < pad > token and longer sequences are truncated .", "entities": []}, {"text": "Tokens occurring 5 times or more are retained in the vocabulary .", "entities": []}, {"text": "We set dropout as 0.2 and used Adam ( Kingma and Ba , 2015 ) as our optimizer with initial learning rate set to 0:001 .", "entities": [[7, 8, "MethodName", "Adam"], [17, 18, "HyperparameterName", "optimizer"], [20, 22, "HyperparameterName", "learning rate"]]}, {"text": "We did n\u2019t perform extensive network architecture search to improve the performance as it is a reasonably strong classi\ufb01er with the initial performance of 95.5 % accuracy .", "entities": [[26, 27, "MetricName", "accuracy"]]}, {"text": "The number of interpolating steps for IG is set to50(as in the original paper ) for calculating Riemann approximation of the integral .", "entities": []}, {"text": "Since the output of the binary classi\ufb01cation can be reduced to a single scalar output by taking the posterior of the 2https://github.com/conversationai/ unintended - ml - bias - analysis", "entities": []}, {"text": "6278Whole Dataset Acc F1 AUC FP FN Baseline .955 .728 .948 .010 .035 Importance .957 .739 .953 .009 .034 TOK Replace .939 .607 .904", "entities": [[2, 3, "MetricName", "Acc"], [3, 4, "MetricName", "F1"], [4, 5, "MetricName", "AUC"]]}, {"text": ".014 .047", "entities": []}, {"text": "Our Method .958 .752 .960 .009 .032", "entities": []}, {"text": "Fine - tuned .955 .720 .954 .007 .038 Table 4 : Performance on the Wikipedia toxic comment dataset .", "entities": []}, {"text": "Columns represent Accuracy , F-1 score , Area Under ROC curve , False Positive , and False Negative .", "entities": [[2, 3, "MetricName", "Accuracy"]]}, {"text": "Numbers represent the mean of 5 runs .", "entities": []}, {"text": "Maximum variance is:012 .", "entities": []}, {"text": "positive ( toxic ) class , the prior is only added to the positive class in equation 3 .", "entities": []}, {"text": "We set ti= ( k ; ifxi2I ai;otherwise ; ( 5 ) whereIis the set of selected terms and xibeing theith token in the sequence .", "entities": []}, {"text": "For fairness experiments , we set kto be 0and Ito the set of identity terms with the hope that these terms should be as neutral as possible when making predictions .", "entities": []}, {"text": "Hyperparamter \u0015is searched in the range of ( 1;108)and increased from 1 by a scale of 10 on the dev set", "entities": []}, {"text": "and we pick the one with best F-1 score .", "entities": []}, {"text": "\u0015is set to 106for the \ufb01nal model .", "entities": []}, {"text": "For data scarcity experiments , we set kto1and Ito the set of toxic terms to force the model to make high attributions to these terms .", "entities": []}, {"text": "Hyperparameter\u0015is set to 105across all data size experiments by tuning on the dev set with model given 1%of training data .", "entities": []}, {"text": "Each experiment was repeated for 5 runs with 10 epochs and the best model is selected according to the dev set .", "entities": []}, {"text": "Training takes 1 minute for a model with cross - entropy loss and 30 minutes for a model with joint loss on an NVidia V100 GPU .", "entities": [[11, 12, "MetricName", "loss"], [20, 21, "MetricName", "loss"]]}, {"text": "However , reducing the step size in IG for calculating Riemann approximation of the integral to 10 steps reduces the training time to 6 minutes .", "entities": [[4, 6, "HyperparameterName", "step size"]]}, {"text": "Lastly , training with joint loss reaches its best performance in later epochs than training with crossentropy loss .", "entities": [[5, 6, "MetricName", "loss"], [17, 18, "MetricName", "loss"]]}, {"text": "Implementation Decisions When taking the derivative with respect to the loss , we treat the interpolated embeddings as constants .", "entities": [[10, 11, "MetricName", "loss"]]}, {"text": "Thus , the prior loss does not back - propagate to the embedding parameters .", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "There are two reasons that lead to this decision : ( i ) taking the gradient of the interpolate operation would break the axiomsIdentity Acc F1 AUC FP FN Baseline .931", "entities": [[24, 25, "MetricName", "Acc"], [25, 26, "MetricName", "F1"], [26, 27, "MetricName", "AUC"]]}, {"text": ".692 .910 .011 .057 Importance .933 .704 .945 .012 .055 TOK Replace .910 .528 .882 .008 .081", "entities": []}, {"text": "Our Method .934 .697 .949 .008 .058 Finetuned .928 .660 .940 .007 .064 Table 5 : Performance statistics of all approaches on the Wikipedia dataset \ufb01ltered on samples including identity terms .", "entities": []}, {"text": "Numbers represent the mean of 5 runs .", "entities": []}, {"text": "Maximum variance is : 001 .", "entities": []}, {"text": "that IG guarantees ; ( ii ) the Hessian of the embedding matrix is slow to compute .", "entities": []}, {"text": "The implementation decision does not imply that prior loss has no effect on the word embeddings , though .", "entities": [[8, 9, "MetricName", "loss"], [14, 16, "TaskName", "word embeddings"]]}, {"text": "During training , the model parameters are updated with respect to both losses .", "entities": []}, {"text": "Therefore , the word embeddings had to adjust accordingly to the new model parameters by updating the embedding parameters with cross - entropy loss .", "entities": [[3, 5, "TaskName", "word embeddings"], [23, 24, "MetricName", "loss"]]}, {"text": "4.3 Results on Incorporating Fairness Priors We compare our work to 3 models with the same CNN architecture , but different training settings : \u000fBaseline : A baseline classi\ufb01er trained with cross - entropy loss .", "entities": [[4, 5, "TaskName", "Fairness"], [34, 35, "MetricName", "loss"]]}, {"text": "\u000fImportance : Classi\ufb01er trained with crossentropy loss , but the loss for samples containing identity words are weighted in the range ( 1;108 ) , where the actual coef\ufb01cient is determined to be 10on the dev set based on F-1 score .", "entities": [[6, 7, "MetricName", "loss"], [10, 11, "MetricName", "loss"]]}, {"text": "\u000fTOK Replace :", "entities": []}, {"text": "Common technique for making models blind to identity terms ( Garg et al . , 2018 ) .", "entities": []}, {"text": "All identity terms are replaced with a special < id > token .", "entities": []}, {"text": "We also explore a different training schedule for cases where a model has been trained to optimize for a classi\ufb01cation loss : \u000fFinetuned : An already - trained classi\ufb01er is \ufb01netuned with joint loss for several epochs .", "entities": [[20, 21, "MetricName", "loss"], [33, 34, "MetricName", "loss"]]}, {"text": "The aim of this experiment is to show that our method is also applicable for tweaking trained models , which could be useful if the original had been trained for a long time .", "entities": []}, {"text": "6279gay homosexual < i d > Baseline Our method Importance Baseline Our method Importance Tok Replace a**hole < pad > sh*t b*tch scorecard f*ck 456 f*ck jus f*cking cr*p dutchman b*tch messengers pathetic tweaking b*tch f*g \u2018 oh pu**y louie fu*king sess f*ck bulls * * * 678 sucks dome fa**ot ridiculous penis dumba*s nitrites f*cked accumulation bas**rd \u2018 do suck sh*t poured pathetic ink cr*p", "entities": []}, {"text": "manhood pu**y penis nuts c*ck", "entities": []}, {"text": "usher suck dub d*ckhead moron gubernatorial fart wikiepedia sh*t heartening moron retard convincing a**hole schizophrenics a*s desire fa**ot gay strung fa**ot notables Table 6 : Top 10 nearest neighbors for tokens \u2018 gay \u2019 and \u2018 homosexual \u2019 and < id > for TOK Replace .", "entities": []}, {"text": "All asterisks are inserted by authors to replace certain characters .", "entities": []}, {"text": "Synthetic AUC FPED FNED Baseline .885 2.77 3.51 Importance .850 2.90 3.06 TOK Replace .930 0.00 0.00 Our Method .952 0.01 0.11 Finetuned .925 0.00 0.19 Table 7 : AUC and Bias mitigation metrics on synthetic dataset .", "entities": [[1, 2, "MetricName", "AUC"], [29, 30, "MetricName", "AUC"]]}, {"text": "The lower the better for Bias mitigation metrics and is bounded by 0 .", "entities": [[12, 13, "DatasetName", "0"]]}, {"text": "Numbers represent the mean of 5 runs .", "entities": []}, {"text": "Maximum variance is 0:013 .", "entities": []}, {"text": "4.3.1 Evaluation on Original Data", "entities": []}, {"text": "We \ufb01rst verify that the prior loss term does not adversely affect overall classi\ufb01er performance on the main task using general performance metrics such as accuracy and F-1 .", "entities": [[6, 7, "MetricName", "loss"], [25, 26, "MetricName", "accuracy"]]}, {"text": "Results are shown in Table 4 .", "entities": []}, {"text": "Unlike previous approaches ( Park et al . , 2018 ; Dixon et al . , 2018 ; Madras et al . , 2018 ) , our method does not degrade classi\ufb01er performance ( it even improves ) in terms of all reported metrics .", "entities": []}, {"text": "We also look at samples containing identity terms .", "entities": []}, {"text": "Table 5 shows classi\ufb01er performance metrics for such samples .", "entities": []}, {"text": "The importance weighting approach slightly outperforms the baseline classi\ufb01er .", "entities": []}, {"text": "Replacing identity words with a special tokens , on the other hand , hurts the performance on the main task .", "entities": []}, {"text": "One of the reasons might be that replacing all identity terms with a token potentially removes other useful information model can rely on .", "entities": []}, {"text": "If we were to make an analogy between the token replacement method and hard ablation , then the same analogy can be made between our method and soft ablation .", "entities": []}, {"text": "Hence , the information pertaining to identity terms is not completely lost for our method , butcome at a cost .", "entities": []}, {"text": "Results for \ufb01ne - tuning experiments show the performance after 2 epochs .", "entities": []}, {"text": "It is seen that the model converges to similar performance with joint training after only 2 epochs , albeit being slightly poorer .", "entities": []}, {"text": "4.3.2 Evaluation on Synthetic Data", "entities": []}, {"text": "Now we run our experiments on the templatebased synthetic data .", "entities": []}, {"text": "As stated , this dataset is used to measure biases in the model since it is unbiased towards identities .", "entities": []}, {"text": "We use AUC along with False Positive Equality Difference ( FPED ) and False Negative Equality Difference ( FNED ) , which measure a proxy of Equality of Odds ( Hardt et al . , 2016 ) , as in Dixon et al .", "entities": [[2, 3, "MetricName", "AUC"]]}, {"text": "( 2018 ) ;", "entities": []}, {"text": "Park et al . ( 2018 ) .", "entities": []}, {"text": "FPED sums absolute differences between overall false positive rate and false positive rates for each identity term .", "entities": []}, {"text": "FNED calculates the same for false negatives .", "entities": []}, {"text": "Results on this dataset are shown in Table 7 .", "entities": []}, {"text": "Our method provides substantial improvement on AUC and almost completely eliminates false positive and false negative inequality across identities .", "entities": [[6, 7, "MetricName", "AUC"]]}, {"text": "The \ufb01ne - tuned model also outperforms the baseline for mitigating the bias .", "entities": []}, {"text": "The token replacement method comes out as a good baseline for mitigating the bias since it treats all identities the same .", "entities": []}, {"text": "The importance weighting approach fails to produce an unbiased model .", "entities": []}, {"text": "4.4 Nearest Neighbors of Identity Terms Models convert input tokens to embeddings before providing them to convolutional layers .", "entities": []}, {"text": "As embeddings make up the majority of the parameters of the network and can be exported for use in", "entities": []}, {"text": "6280Ratio 1 % 5 % 10 % Toxic Base Ours Base Ours Base", "entities": []}, {"text": "Ours", "entities": []}, {"text": "hell", "entities": []}, {"text": "-.002", "entities": []}, {"text": ".035 .002 .673 .076 .624 moron -.002 .044 .002 .462 .077 .290 sh*t -.003 .078 .006 .575", "entities": []}, {"text": ".098 .437 f*ck -.003 .142 .013 .643 .282 .682 b*tch -.003 .051 .002 .397 .065 .362 Table 8 : Subset of toxic terms we used in the experiments and their mean attribution value on the test set for different training sizes .", "entities": []}, {"text": "other tasks , we \u2019re interested in how they change for the identity terms .", "entities": []}, {"text": "We show 10 nearest neighbors of the terms < id>(for the token replacement method ) , \u201c gay \u201d , and \u201c homosexual \u201d \u2013 top two identity terms with the most mean attribution difference ( our method vs. baseline ) , in Table 6 .", "entities": []}, {"text": "The word embedding of the term \u201c gay \u201d shifts from having swear words as its neighbors to having the < pad > token as the closest neighbor .", "entities": []}, {"text": "Although the term \u201c homosexual \u201d has lower mean attribution , its neighboring words are still mostly swear words in the baseline embedding space .", "entities": []}, {"text": "\u201c homosexual \u201d also moved to more neutral terms that should n\u2019t play a role in deciding if the comment is toxic or not .", "entities": []}, {"text": "Although they are not as high quality as one would expect general - purpose word embeddings to be possibly due to data size and the model having a different objective , the results show that our method yields inherently unbiased embeddings .", "entities": [[14, 16, "TaskName", "word embeddings"]]}, {"text": "It removes the necessity to initialize word embeddings with pre - debiased embeddings as proposed in Bolukbasi et al . ( 2016 ) .", "entities": [[6, 8, "TaskName", "word embeddings"]]}, {"text": "The importance weighting technique penalizes the model on the sentence level instead of focusing on the token level .", "entities": []}, {"text": "Therefore , the word embedding of \u201c gay \u201d does n\u2019t seem to shift to neutral words .", "entities": []}, {"text": "The token replacement method , on the other hand , replaces the identity terms with a token that is surrounded with neutral words in the embedding space , so it results in greater improvement on the synthetic dataset .", "entities": []}, {"text": "However , since all identity terms are collapsed into one , it \u2019s harder for the model to capture the context and as a result , classi\ufb01cation performance on the original dataset drops .", "entities": []}, {"text": "4.5 Results on Incorporating Priors in Different Training Sizes We now demonstrate our approach on encouraging higher attributions on toxic words to increase051015202530354090929496 Training Data PercentageTest AccuracyBaseline", "entities": []}, {"text": "Our method Rule based Figure 1 : Test accuracy for different training sizes .", "entities": [[8, 9, "MetricName", "accuracy"]]}, {"text": "The rule based method gives positive prediction if the comment includes any of the toxic temrs .", "entities": []}, {"text": "model performance in scarce data regime .", "entities": []}, {"text": "We down - sample the dataset with different ratios to simulate a data scarcity scenario .", "entities": []}, {"text": "To directly validate the effectiveness of prior loss on attributions , we \ufb01rst show that the attribution of the toxic words have higher values for our method across different data ratios compared to the baseline in Table 8 .", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "We also show that the attribution for these terms increases as training data increases for the baseline method .", "entities": []}, {"text": "We then show model performance on testing data for different data size ratios for the baseline and our method in Figure 1 .", "entities": []}, {"text": "Our method outperforms the baseline by a big margin in 1 % and5%ratio .", "entities": []}, {"text": "However , the impact of our approach diminishes after adding more data , since the model starts to learn to focus on toxic words itself for predicting toxicity without the need for prior injection .", "entities": []}, {"text": "We can also see that both the baseline and our method start to catch up with the rule based approach , where we give positive prediction if the toxic word is in the sentence , and eventually outperform it .", "entities": []}, {"text": "5 Discussion and Related Work For explaining ML models , recent research attempts offer techniques ranging from building inherently interpretable models ( Kim et al . , 2014 ) to building a proxy model for explaining a more complex model ( Ribeiro et al . , 2016 ; Frosst and Hinton , 2017 ) to explaining inner mechanics of mostly uninterpretable neural networks ( Sundararajan et al . , 2017 ; Bach et al . , 2015 ) .", "entities": []}, {"text": "One family of interpretability methods uses sensitivity of the network with respect to data points ( Koh", "entities": []}, {"text": "6281and Liang , 2017 ) or features ( Ribeiro et al . , 2016 ) as a form of explanation .", "entities": []}, {"text": "These methods rely on small , local perturbations and check how a network \u2019s response changes .", "entities": []}, {"text": "Explaining text models has another layer of complexity due to a lock of proper technique to generate counterfactuals in the form of small perturbations .", "entities": []}, {"text": "Hence , interpretability methods tailored for text are quite sparse ( Mudrakarta et al . , 2018 ; Jia and Liang , 2017 ; Murdoch et al . , 2018 ) .", "entities": []}, {"text": "On the other hand , there are many papers criticizing the aforementioned methods by questioning their faithfulness , correctness ( Adebayo et al . , 2018 ; Kindermans et al . , 2017 ) and usefulness .", "entities": []}, {"text": "Smilkov et al .", "entities": []}, {"text": "( 2017 ) show that gradient based methods are susceptible to saturation and can be fooled by adversarial techniques .", "entities": []}, {"text": "Other sets of papers ( Miller , 2019 ; Gilpin et al . , 2018 ) attack model explanation papers from a philosophical perspective .", "entities": []}, {"text": "However , the lack of actionability angle is often overlooked .", "entities": []}, {"text": "Lipton ( 2018 ) brie\ufb02y questions the practical bene\ufb01t of having model explanations from a practitioners perspective .", "entities": []}, {"text": "There are several works taking advantage of model explanations .", "entities": []}, {"text": "Namely , using model explanations to aid doctors in diagnosing retinopathy patients ( Sayres et al . , 2018 ) , and removing minimal features , called pathologies , from neural networks by tuning the model to have high entropy on pathologies ( Feng et al . , 2018 ) .", "entities": []}, {"text": "The authors of Ross et al . ( 2017 ) propose a similar idea to our approach in that they regularize input gradients to alter the decision boundary of the model to make it more consistent with domain knowledge .", "entities": []}, {"text": "However , the input gradients technique has been shown to be an inaccurate explanation technique ( Adebayo et al . , 2018 ) .", "entities": []}, {"text": "Addressing and mitigating bias in NLP models are paramount tasks as the effects on these models adversely affect protected subpopulations ( Schmidt and Wiegand , 2017 ) .", "entities": []}, {"text": "One of the earliest works is Calders and Verwer ( 2010 ) .", "entities": []}, {"text": "Later , Bolukbasi et al . ( 2016 ) proposed to unbias word vectors from gender stereotypes .", "entities": []}, {"text": "Park et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) also try to address gender bias for abusive language detection models by debiasing word vectors , augmenting more data and changing model architecture .", "entities": [[10, 12, "TaskName", "abusive language"]]}, {"text": "While their results seem to show promise for removing gender bias , their method does n\u2019t scale for other identity dimensions such as race and religion .", "entities": []}, {"text": "The authors of Dixon et al .", "entities": []}, {"text": "( 2018 ) highlightthe bias in toxic comment classi\ufb01er models originating from the dataset .", "entities": []}, {"text": "They also supplement the training dataset from Wikipedia articles to shift positive class imbalance for sentences containing identity terms to dataset average .", "entities": []}, {"text": "Similarly , their approach alleviates the issue to a certain extent , but does not scale to similar problems as their augmentation technique is too data - speci\ufb01c .", "entities": []}, {"text": "Also , both methods trade original task accuracy for fairness , while our method does not .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "Lastly , there are several works ( Davidson et al . , 2017 ; Zhang et al . , 2018b ) offering methodologies or datasets to evaluate models for unintended bias , but they fail to offer a general framework .", "entities": []}, {"text": "One of the main reasons our approach improves the model in the original task is that the model is now more robust thanks to the reinforcement provided to the model builder through attributions .", "entities": []}, {"text": "From a fairness angle , our technique shares similarities with adversarial training ( Zhang et al . , 2018a ; Madras et al . , 2018 ) in asking the model to optimize for an additional objective that transitively unbiases the classi\ufb01er .", "entities": []}, {"text": "However , those approaches work to remove protected attributes from the representation layer , which is unstable .", "entities": []}, {"text": "Our approach , on the other hand , works with basic human - interpretable units of information \u2013 tokens .", "entities": []}, {"text": "Also , those approaches propose to sacri\ufb01ce main task performance for fairness as well .", "entities": []}, {"text": "While our method enables model builders to inject priors to aid a model , it has several limitations .", "entities": []}, {"text": "In solving the fairness problem in question , it causes the classi\ufb01er to not focus on the identity terms even for the cases where an identity term itself is being used as an insult .", "entities": []}, {"text": "Moreover , our approach requires prior terms to be manually provided , which bears resemblance to blacklist approaches and suffers from the same drawbacks .", "entities": []}, {"text": "Lastly , the evaluation methodology that we and previous papers ( Dixon et al . , 2018 ; Park et", "entities": []}, {"text": "al . , 2018 ) rely on are based on a syntheticallygenerated dataset , which may contain biases of the individuals creating it .", "entities": []}, {"text": "6 Conclusion and Future Work", "entities": []}, {"text": "In this paper , we proposed actionability on model explanations that enable ML practitioners to enforce priors on their model .", "entities": []}, {"text": "We apply this technique to model fairness in toxic comment classi\ufb01cation .", "entities": []}, {"text": "Our method incorporates Path Integrated Gradients attributions into the objective function", "entities": []}, {"text": "6282with the aim of stopping the classi\ufb01er from carrying along false positive bias from the data by punishing it when it focuses on identity words .", "entities": []}, {"text": "Our experiments indicate that the models trained jointly with cross - entropy and prior loss do not suffer a performance drop on the original task , while achieving a better performance in fairness metrics on the template - based dataset .", "entities": [[14, 15, "MetricName", "loss"]]}, {"text": "Applying model attribution as a \ufb01ne - tuning step on a trained classi\ufb01er makes it converge to a more debiased classi\ufb01er in just a few epochs .", "entities": []}, {"text": "Additionally , we show that model can be also forced to focus on pre - determined tokens .", "entities": []}, {"text": "There are several avenues we can explore as future research .", "entities": []}, {"text": "Our technique can be applied to implement a more robust model by penalizing the attributions falling outside of tokens annotated to be relevant to the predicted class .", "entities": []}, {"text": "Another avenue is to incorporate different model attribution strategies such as DeepLRP ( Bach et al . , 2015 ) into the objective function .", "entities": []}, {"text": "Finally , it would be worthwhile to invest in a technique to extract problematic terms from the model automatically rather than providing prescribed identity or toxic terms .", "entities": []}, {"text": "Acknowledgments We thank Salem Haykal , Ankur Taly , Diego Garcia - Olano , Raz Mathias , and Mukund Sundararajan for their valuable feedback and insightful discussions .", "entities": []}, {"text": "References Julius Adebayo , Justin Gilmer , Michael Muelly , Ian J. Goodfellow , Moritz Hardt , and Been Kim . 2018 .", "entities": []}, {"text": "Sanity checks for saliency maps .", "entities": []}, {"text": "In Proceedings of NeurIPS .", "entities": []}, {"text": "Julia Angwin , Jeff Larson , Surya Mattu , and Lauren Kirchner .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Machine bias : There s software used across the country to predict future criminals .", "entities": []}, {"text": "and its biased against blacks .", "entities": []}, {"text": "ProPublica .", "entities": []}, {"text": "Sebastian Bach , Alexander Binder , Gr \u00b4 egoire Montavon , Frederick Klauschen , Klaus - Robert M \u00a8uller , Wojciech Samek , and Oscar Deniz Suarez . 2015 .", "entities": []}, {"text": "On pixel - wise explanations for non - linear classi\ufb01er decisions by layer - wise relevance propagation .", "entities": []}, {"text": "In Proceedings of PloS one .", "entities": [[3, 4, "DatasetName", "PloS"]]}, {"text": "Tolga Bolukbasi , Kai - Wei Chang , James Zou , Venkatesh Saligrama , and Adam Kalai . 2016 .", "entities": [[15, 16, "MethodName", "Adam"]]}, {"text": "Man is to computer programmer as woman is to homemaker ?", "entities": []}, {"text": "debiasing word embeddings .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of NIPS .Toon Calders and Sicco Verwer .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Three naive bayes approaches for discrimination - free classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of Data Mining and Knowledge Discovery , Hingham , MA , USA .", "entities": []}, {"text": "Kluwer Academic Publishers .", "entities": []}, {"text": "Irene Chen , Fredrik D. Johansson , and David Sontag .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Why is my classi\ufb01er discriminatory ?", "entities": []}, {"text": "In Proceedings of NeurIPS .", "entities": []}, {"text": "Thomas Davidson , Dana Warmsley , Michael W. Macy , and Ingmar Weber . 2017 .", "entities": []}, {"text": "Automated hate speech detection and the problem of offensive language .", "entities": [[1, 4, "TaskName", "hate speech detection"]]}, {"text": "In Proceedings of ICWSM .", "entities": []}, {"text": "Amit Dhurandhar , Pin - Yu Chen , Ronny Luss , ChunChen Tu , Pai - Shun Ting , Karthikeyan Shanmugam , and Payel Das . 2018 .", "entities": []}, {"text": "Explanations based on the missing : Towards contrastive explanations with pertinent negatives .", "entities": []}, {"text": "In Proceedings of NeurIPS .", "entities": []}, {"text": "Lucas Dixon , John Li , Jeffrey Sorensen , Nithum Thain , and Lucy Vasserman .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Measuring and mitigating unintended bias in text classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of AIES .", "entities": []}, {"text": "Shi Feng , Eric Wallace , Alvin Grissom II , Mohit Iyyer , Pedro Rodriguez , and Jordan Boyd - Graber .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Pathologies of neural models make interpretations dif\ufb01cult .", "entities": []}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Nicholas Frosst and Geoffrey E. Hinton . 2017 .", "entities": []}, {"text": "Distilling a neural network into a soft decision tree .", "entities": []}, {"text": "Arxiv , 1711.09784 .", "entities": [[0, 1, "DatasetName", "Arxiv"]]}, {"text": "Sahaj Garg , Vincent Perot , Nicole Limtiaco , Ankur Taly , Ed Huai hsin Chi , and Alex Beutel .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Counterfactual fairness in text classi\ufb01cation through robustness .", "entities": []}, {"text": "In Proceedings of AIES .", "entities": []}, {"text": "Leilani H. Gilpin , David Bau , Ben Z. Yuan , Ayesha Bajwa , Michael Specter , and Lalana Kagal .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Explaining explanations : An overview of interpretability of machine learning .", "entities": []}, {"text": "In Proceedings of DSAA .", "entities": []}, {"text": "Bryce Goodman and Seth Flaxman .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "European union regulations on algorithmic decision - making and a right to explanation .", "entities": []}, {"text": "In Proceedings of ICML Workshop on Human Interpretability in Machine Learning .", "entities": []}, {"text": "Moritz Hardt , Eric Price , and Nathan Srebro .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Equality of opportunity in supervised learning .", "entities": []}, {"text": "In Proceedings of NIPS .", "entities": []}, {"text": "Robin Jia and Percy Liang . 2017 .", "entities": []}, {"text": "Adversarial examples for evaluating reading comprehension systems .", "entities": [[4, 6, "TaskName", "reading comprehension"]]}, {"text": "InProceedings of EMNLP .", "entities": []}, {"text": "Been Kim , Cynthia Rudin , and Julie Shah .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The bayesian case model : A generative approach for case - based reasoning and prototype classi\ufb01cation .", "entities": []}, {"text": "InProceedings of NIPS .", "entities": []}, {"text": "6283Been Kim , Martin Wattenberg , Justin Gilmer , Carrie Cai , James Wexler , Fernanda B. Vi \u00b4 egas , and Rory Sayres .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Interpretability beyond feature attribution : Quantitative testing with concept activation vectors ( TCA V ) .", "entities": []}, {"text": "In Proceedings of ICML .", "entities": []}, {"text": "Yoon Kim .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Convolutional neural networks for sentence classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Pieter - Jan Kindermans , Sara Hooker , Julius Adebayo , Maximilian Alber , Kristof T. Sch \u00a8utt , Sven D\u00a8ahne , Dumitru Erhan , and Been Kim . 2017 .", "entities": []}, {"text": "The ( un)reliability of saliency methods .", "entities": []}, {"text": "In Proceedings of NIPS workshop on Explaining and Visualizing Deep Learning .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In Proceedings of ICLR .", "entities": []}, {"text": "Pang Wei Koh and Percy Liang . 2017 .", "entities": []}, {"text": "Understanding black - box predictions via in\ufb02uence functions .", "entities": []}, {"text": "In Proceedings of ICML .", "entities": []}, {"text": "Tao Lei , Regina Barzilay , and Tommi Jaakkola .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Rationalizing neural predictions .", "entities": []}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Jiwei Li , Xinlei Chen , Eduard Hovy , and Dan Jurafsky . 2016 .", "entities": []}, {"text": "Visualizing and understanding neural models in nlp .", "entities": []}, {"text": "In Proceedings of NAACL - HLT .", "entities": []}, {"text": "Zachary C. Lipton .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The mythos of model interpretability .", "entities": []}, {"text": "In Queue , New York , NY , USA . ACM .", "entities": [[10, 11, "DatasetName", "ACM"]]}, {"text": "Yin Lou , Rich Caruana , and Johannes Gehrke .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Intelligible models for classi\ufb01cation and regression .", "entities": []}, {"text": "InProceedings of KDD .", "entities": []}, {"text": "Scott M. Lundberg , Gabriel G. Erion , and Su - In Lee . 2018 .", "entities": []}, {"text": "Consistent individualized feature attribution for tree ensembles .", "entities": []}, {"text": "In Proceedings of KDD .", "entities": []}, {"text": "Scott M Lundberg and Su - In Lee . 2017 .", "entities": []}, {"text": "A uni\ufb01ed approach to interpreting model predictions .", "entities": []}, {"text": "In Proceedings of NIPS .", "entities": []}, {"text": "David Madras , Elliot Creager , Toniann Pitassi , and Richard S. Zemel .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Learning adversarially fair and transferable representations .", "entities": []}, {"text": "In Proceedings of ICML .", "entities": []}, {"text": "Tim Miller .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Explanation in arti\ufb01cial intelligence : Insights from the social sciences .", "entities": []}, {"text": "In Proceedings of Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Pramod Kaushik Mudrakarta , Ankur Taly , Mukund Sundararajan , and Kedar Dhamdhere .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Did the model understand the question ?", "entities": []}, {"text": "In Proceedings of ACL .", "entities": []}, {"text": "W. James Murdoch , Peter J. Liu , and Bin Yu .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Beyond word importance : Contextual decomposition to extract interactions from lstms .", "entities": []}, {"text": "In Proceedings of ICLR .Ji", "entities": []}, {"text": "Ho Park , Jamin Shin , and Pascale Fung .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Reducing gender bias in abusive language detection .", "entities": [[4, 6, "TaskName", "abusive language"]]}, {"text": "In Proceedings of EMNLP .", "entities": []}, {"text": "Marco Tulio Ribeiro , Sameer Singh , and Carlos Guestrin . 2016 .", "entities": []}, {"text": "\u201d why should i trust you ? \u201d : Explaining the predictions of any classi\ufb01er .", "entities": []}, {"text": "In Proceedings of KDD .", "entities": []}, {"text": "Andrew Slavin Ross , Michael C. Hughes , and Finale Doshi - Velez . 2017 .", "entities": []}, {"text": "Right for the right reasons : Training differentiable models by constraining their explanations .", "entities": []}, {"text": "In Proceedings of IJCAI , IJCAI\u201917 , pages 2662\u20132670 .", "entities": []}, {"text": "AAAI Press .", "entities": []}, {"text": "Rory Sayres , Ankur Taly , Ehsan Rahimy , Katy Blumer , David Coz , Naama Hammel , Jonathan Krause , Arunachalam Narayanaswamy , Zahra Rastegar , Derek Wu , Shawn Xu , Scott Barb , Anthony Joseph , Michael Shumski , Jesse Smith , Arjun B. Sood , Greg S. Corrado , Lily Peng , and Dale R. Webster .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Using a deep learning algorithm and integrated gradients explanation to assist grading for diabetic retinopathy .", "entities": []}, {"text": "In Proceedings of American Academy of Ophthalmology .", "entities": []}, {"text": "Anna Schmidt and Michael Wiegand .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "A survey on hate speech detection using natural language processing .", "entities": [[3, 6, "TaskName", "hate speech detection"]]}, {"text": "In Proceedings of International Workshop on Natural Language Processing for Social Media .", "entities": []}, {"text": "Avanti Shrikumar , Peyton Greenside , and Anshul Kundaje . 2017 .", "entities": []}, {"text": "Learning important features through propagating activation differences .", "entities": []}, {"text": "In Proceedings of ICML .", "entities": []}, {"text": "Daniel Smilkov , Nikhil Thorat , Been Kim , Fernanda B. Vi\u00b4egas , and Martin Wattenberg . 2017 .", "entities": []}, {"text": "Smoothgrad : removing noise by adding noise .", "entities": []}, {"text": "In Proceedings of ICML Workshop on Visualization for Deep Learning .", "entities": []}, {"text": "Mukund Sundararajan , Ankur Taly , and Qiqi Yan . 2017 .", "entities": []}, {"text": "Axiomatic attribution for deep networks .", "entities": []}, {"text": "In Proceedings of ICML .", "entities": []}, {"text": "Zeerak Waseem and Dirk Hovy .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Hateful symbols or hateful people ?", "entities": []}, {"text": "predictive features for hate speech detection on twitter .", "entities": [[3, 6, "TaskName", "hate speech detection"]]}, {"text": "In Proceedings of the NAACL Student Research Workshop .", "entities": []}, {"text": "Ellery Wulczyn , Nithum Thain , and Lucas Dixon . 2017 .", "entities": []}, {"text": "Ex machina : Personal attacks seen at scale .", "entities": []}, {"text": "InProceedings of WWW .", "entities": []}, {"text": "Brian Hu Zhang , Blake Lemoine , and Margaret Mitchell . 2018a .", "entities": []}, {"text": "Mitigating unwanted biases with adversarial learning .", "entities": []}, {"text": "In Proceedings of AIES .", "entities": []}, {"text": "Ziqi Zhang , David Robinson , and Jonathan A. Tepper .", "entities": []}, {"text": "2018b .", "entities": []}, {"text": "Detecting hate speech on twitter using a convolution - gru based deep neural network .", "entities": [[1, 3, "DatasetName", "hate speech"], [7, 8, "MethodName", "convolution"], [9, 10, "MethodName", "gru"]]}, {"text": "In Proceedings of ESWC .", "entities": []}]