[{"text": "Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 596\u2013611 November 16 - 20 , 2020 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics596PBoS : Probabilistic Bag - of - Subwords for Generalizing Word Embedding Zhao Jinman Shawn Zhong Xiaomin Zhang Yingyu Liang University of Wisconsin - Madison , Madison , WI , USA fjz,yliangg@cs.wisc.edu fshawn.zhong , xzhang682 g@wisc.edu", "entities": [[26, 27, "DatasetName", "Wisconsin"]]}, {"text": "Abstract We look into the task of generalizing word embeddings : given a set of pre - trained word vectors over a \ufb01nite vocabulary , the goal is to predict embedding vectors for out - of - vocabulary words , without extra contextual information .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "We rely solely on the spellings of words and propose a model , along with an ef\ufb01cient algorithm , that simultaneously models subword segmentation and computes subword - based compositional word embedding .", "entities": []}, {"text": "We call the model probabilistic bag - of - subwords ( PBoS ) , as it applies bag - of - subwords for all possible segmentations based on their likelihood .", "entities": []}, {"text": "Inspections and af\ufb01x prediction experiment show that PBoS is able to produce meaningful subword segmentations and subword rankings without any source of explicit morphological knowledge .", "entities": []}, {"text": "Word similarity and POS tagging experiments show clear advantages of PBoS over previous subword - level models in the quality of generated word embeddings across languages .", "entities": [[0, 2, "TaskName", "Word similarity"], [22, 24, "TaskName", "word embeddings"]]}, {"text": "1 Introduction Word embeddings pre - trained over large texts have demonstrated bene\ufb01ts for many NLP tasks , especially when the task is label - deprived .", "entities": [[2, 4, "TaskName", "Word embeddings"]]}, {"text": "However , many popular pre - trained sets of word embeddings assume \ufb01xed \ufb01nite - size vocabularies1 , 2 , which hinders their ability to provide useful word representations for out - of - vocabulary ( OOV ) words .", "entities": [[9, 11, "TaskName", "word embeddings"]]}, {"text": "We look into the task of generalizing word embeddings : extrapolating a set of pre - trained word embeddings to words out of its \ufb01xed vocabulary , without extra access to contextual information ( e.g. example sentences or text corpus ) .", "entities": [[7, 9, "TaskName", "word embeddings"], [17, 19, "TaskName", "word embeddings"]]}, {"text": "In contrast , 1https://code.google.com/archive/p/ word2vec/ , Mikolov et al . ( 2013 ) .", "entities": []}, {"text": "2https://nlp.stanford.edu/projects/ glove/ , Pennington et al .", "entities": []}, {"text": "( 2014).the more common task of learning word embeddings , or often just word embedding , is to obtain distributed representations of words directly from large unlabeled text .", "entities": [[7, 9, "TaskName", "word embeddings"]]}, {"text": "The motivation here is to extend the usefulness of pre - trained embeddings without expensive retraining over large text .", "entities": []}, {"text": "There have been works showing that contextual information can also help generalize word embeddings ( for example , Khodak et al . , 2018 ; Schick and Sch \u00a8utze , 2019a , b ) .", "entities": [[12, 14, "TaskName", "word embeddings"]]}, {"text": "We here , however , focus more on the research question of how much one can achieve from just word compositions .", "entities": []}, {"text": "In addition , our proposed way of utilizing word composition information can be combined with the contextual embedding algorithms to further improve the performance of generalized embeddings .", "entities": []}, {"text": "The hidden assumption here is that words are made of meaningful parts ( cf . morphemes ) and that the meaning of a word is related to the meaning of their parts .", "entities": []}, {"text": "This way , humans are often able to guess the meaning of a word or term they have never seen before .", "entities": []}, {"text": "For example , \u201c postEMNLP \u201d probably means \u201c after EMNLP \u201d .", "entities": []}, {"text": "Different models have been proposed for that task of generalizing word embeddings using word compositions , usually under the name of subword(level ) models .", "entities": [[10, 12, "TaskName", "word embeddings"]]}, {"text": "Stratos ( 2017 ) ; Pinter et al . ( 2017 ) ; Kim et al . ( 2018b ) model words at the character level .", "entities": []}, {"text": "However , they have been surpassed by later subword - level models , probably because of putting too much burden on the models to form and discover meaningful subwords from characters .", "entities": []}, {"text": "Bag - of - subwords ( BoS ) is a simple yet effective model for learning ( Bojanowski et al . , 2017 ) and generalizing ( Zhao et al . , 2018 ) word embeddings .", "entities": [[34, 36, "TaskName", "word embeddings"]]}, {"text": "BoS composes a word embedding vector by taking the sum or average of the vectors of the subwords ( character n - grams ) that appear in the given word .", "entities": []}, {"text": "However , it ignores the importance of different subwords since all of them are given the", "entities": []}, {"text": "597same weight .", "entities": []}, {"text": "Intuitively , \u201c farm \u201d and \u201c land \u201d should be more relevant in composing representation for word \u201c farmland \u201d than some random subwords like \u201c armla \u201d .", "entities": []}, {"text": "Even more favorable would be a model \u2019s ability to discover meaningful subword segmentations on its own .", "entities": []}, {"text": "Cotterell et al .", "entities": []}, {"text": "( 2016 ) bases their model over morphemes but needs help from an external morphological analyzer such as Morfessor ( Virpioja et al . , 2013 ) .", "entities": []}, {"text": "Sasaki et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) use trainable self - attention to combine subword vectors .", "entities": []}, {"text": "While the attention implicitly facilitates interactions among subwords , there has been no explicit enforcement of mutual exclusiveness from subword segmentation , making it sometimes dif\ufb01cult to rule out less relevant subwords .", "entities": []}, {"text": "For example , \u201c her \u201d is itself a likely subword , but is unlikely to be relevant for \u201c higher \u201d as the remaining \u201c hig \u201d is unlikely .", "entities": []}, {"text": "We propose the probabilistic bag - of - subwords ( PBoS ) model for generalizing word embedding .", "entities": []}, {"text": "PBoS simultaneously models subword segmentation and composition of word representations out of subword representations .", "entities": []}, {"text": "The subword segmentation part is a probabilistic model capable of handling ambiguity of subword boundaries and ranking possible segmentations based on their overall likelihood .", "entities": []}, {"text": "For each segmentation , we compose a word vector as the sum of all subwords that appear in the segmentation .", "entities": []}, {"text": "The \ufb01nal embedding vector is the expectation of the word vectors from all possible segmentations .", "entities": []}, {"text": "An alternative view is that the model assigns word - speci\ufb01c weights to subwords based on how likely they appear as meaningful segments for the given word .", "entities": []}, {"text": "Coupled with an ef\ufb01cient algorithm , our model is able to compose better word embedding vectors with little computational overhead compared to BoS. Manual inspections show that PBoS is able to produce subword segmentations and subword weights that align with human intuition .", "entities": []}, {"text": "Af\ufb01x prediction experiment quantitatively shows that the subword weights given by PBoS are able to recover most eminent af\ufb01xes of words with good accuracy .", "entities": [[23, 24, "MetricName", "accuracy"]]}, {"text": "To assess the quality of generated word embeddings , we evaluate with the intrinsic task of word similarity which relates to the semantics ; as well as the extrinsic task of part - of - speech ( POS ) tagging which requires rich information to determine each word \u2019s role in a sentence .", "entities": [[6, 8, "TaskName", "word embeddings"], [16, 18, "TaskName", "word similarity"], [31, 34, "DatasetName", "part - of"]]}, {"text": "English word similarity experiment shows that PBoS improves the correlation scores over previous best models under vari - ous settings and is the only model that consistently improves over the target pre - trained embeddings .", "entities": [[1, 3, "TaskName", "word similarity"]]}, {"text": "POS tagging experiment over 23 languages shows that PBoS improves accuracy compared in all but one language to the previous best models , often by a big margin .", "entities": [[10, 11, "MetricName", "accuracy"]]}, {"text": "We summarize our contributions as follows : \u000fWe propose PBoS , a subword - level word embedding model that is based on probabilistic segmentation of words into subwords , the \ufb01rst of its kind ( Section 2 ) .", "entities": []}, {"text": "\u000fWe propose an ef\ufb01cient algorithm that leads to an ef\ufb01cient implementation3of PBoS with little overhead over previous much simpler BoS. ( Section 3 ) .", "entities": []}, {"text": "\u000fManual inspection and af\ufb01x prediction experiment show that PBoS is able to give reasonable subword segmentations and subword weights ( Section 4.1 and 4.2 ) .", "entities": []}, {"text": "\u000fWord similarity and POS tagging experiments show that word vectors generated by PBoS have better quality compared to previously proposed models across languages ( Section 4.3 and 4.4 ) .", "entities": []}, {"text": "2 PBoS Model Following the above intuition , in this section we describe the PBoS model in detail .", "entities": []}, {"text": "We \ufb01rst develop a model that segments a word into subword and associates each subword segmentation with a likelihood based on the meaningfulness of each subword segment .", "entities": []}, {"text": "We then apply BoS over each segmentation to compose a \u201c segmentation vector \u201d .", "entities": []}, {"text": "The \ufb01nal word embedding vector is then the probabilistic expectation of all the segmentation vectors .", "entities": []}, {"text": "The subword segmentation and likelihood association part require no explicit source of morphological knowledge and are tightly integrated with the word vector composition part , which in turn gives rise to an ef\ufb01cient algorithm that considers allpossible segmentations simultaneously ( Section 3 ) .", "entities": []}, {"text": "The model can be trained by \ufb01tting a set of pre - trained word embeddings .", "entities": [[13, 15, "TaskName", "word embeddings"]]}, {"text": "2.1 Terminology For a given language , let \u0000be its alphabet .", "entities": []}, {"text": "A word wof lengthl = jwjis a string made of lletters in\u0000 , i.e.w = c1c2:::cl2\u0000lwherew[i ] = ciis 3Code used for this work can be found at https:// github.com/jmzhao/pbos .", "entities": []}, {"text": "598thei - th letter .", "entities": []}, {"text": "Let pw2[0;1]be the probability thatwappears in the language .", "entities": []}, {"text": "Empirically , this is proportional to the unigram frequency of word w observed in large text in that language .", "entities": []}, {"text": "Note that we do not assume a vocabulary .", "entities": []}, {"text": "That is , we do not distinguish words from arbitrary strings made out of the alphabet .", "entities": []}, {"text": "The implicit assumption here is that a \u201c word \u201d in common sense is just a string associated with high probability .", "entities": []}, {"text": "In this sense , pwcan also be seen as the likelihood of stringwbeing a \u201c legit word \u201d .", "entities": []}, {"text": "This blurs the boundary between words and non - words , and automatically enables us to handle unseen words , alternative spellings , typos , and nonce words as normal cases .", "entities": []}, {"text": "We say a string s2\u0000+is asubword of word w , denoted as s\u0012w , ifs = w[i : j ] = ci:::cj for some 1\u0014i\u0014j\u0014jwj , i.e.sis a substring of w.", "entities": []}, {"text": "The probability that subword sappears in the language can then be de\ufb01ned as ps / X", "entities": []}, {"text": "w2\u0000+pwX 1\u0014i\u0014j\u0014jwj1(s = w[i : j ] )", "entities": []}, {"text": "( 1 ) where 1(pred)gives 1 and otherwise 0", "entities": [[8, 9, "DatasetName", "0"]]}, {"text": "only if pred holds .", "entities": []}, {"text": "Note that a subword smay occur more than once in the same word w.", "entities": []}, {"text": "For example , subword \u201c ana \u201d occurs twice in the word \u201c banana \u201d .", "entities": []}, {"text": "Asubword se gmentationgof wordwof length", "entities": []}, {"text": "k = jgjis a tuple ( s1;s2;:::;s k)of subwords of w , so thatwis the concatenation of s1;:::;s k. 2.2", "entities": [[0, 2, "HyperparameterName", "k ="]]}, {"text": "Probabilistic Subword Segmentation A subword transition graph for word wis a directed acyclic graph Gw= ( Nw;Ew ) .", "entities": []}, {"text": "Letl = jwj .", "entities": []}, {"text": "The verticesNw = f0;:::;lgcorrespond to the positions between w[i]andw[i+ 1 ] for alli2[l\u00001 ] , as well as to the beginning ( vertiex 0 ) and the end ( vertex l ) ofw .", "entities": [[23, 24, "DatasetName", "0"]]}, {"text": "Each edge ( i;j)2Ew= f(i;j ) : 0\u0014i < j\u0014lgcorresponds to subword w[i : j ] .", "entities": []}, {"text": "We useGwas a useful image for developing our model .", "entities": []}, {"text": "Proposition 1 . Paths from 0tojwjinGware in one - to - one correspondence to segmentations of w. Proposition 2 .", "entities": []}, {"text": "There are 2jwj\u00001different possible segmentations for word", "entities": []}, {"text": "w. Each edge ( i;j)is associated with a weight pw[i : j ] \u2014 how likely w[i : j]itself is a meaningful subword .", "entities": []}, {"text": "We model the likelihood of segmentation gbeing a segmentation of was being proportional to the product of all its subword likelihood \u2013 the0 1 2 3 4 5 6h p\u201ch\u201di p\u201ci\u201dg p\u201cg\u201dh p\u201ch\u201de p\u201ce\u201dr p\u201cr\u201dhi p\u201chi\u201dgher p\u201cgher \u201d gh p\u201cgh \u201d", "entities": []}, {"text": "her p\u201cher\u201dhigh p\u201chigh\u201der p\u201cer \u201d Figure 1 : Diagram of probabilistic subwords transitions for word \u201c higher \u201d .", "entities": []}, {"text": "Some edges are omitted to reduce clutter .", "entities": []}, {"text": "Each edge is labeled by a subword sof the word , associated with ps .", "entities": [[7, 8, "DatasetName", "sof"]]}, {"text": "Bold edges constituent a path from node 0 to 6 , corresponding to the segmentation of the word into \u201c high \u201d and \u201c er \u201d .", "entities": [[7, 8, "DatasetName", "0"]]}, {"text": "transition along a path from 0tojwjinGw : pgjw / Y s2gps : ( 2 ) Example .", "entities": []}, {"text": "Figure 1 illustrates Gwfor wordw= \u201c higher \u201d of length 6 .", "entities": []}, {"text": "Bold edges ( 0;4)and(4;6 ) form a path from 0to6 , which corresponds to the segmentation ( \u201c high\u201d;\u201cer \u201d ) .", "entities": []}, {"text": "The likelihood p(\u201chigh \u201d ; \u201c er\u201d)jwof this particular segmentation is proportional to p\u201chigh\u201dp\u201cer \u201d \u2013 the product of weights along the path .", "entities": []}, {"text": "2.3 Probabilistic Bag - of - Subwords Based on the above modeling of subword segmentations , we propose the Probabilistic Bag - ofSubword ( PBoS ) model for composing word embeddings .", "entities": [[29, 31, "TaskName", "word embeddings"]]}, {"text": "The embedding vector wfor wordwis the expectation of all its segmentation - based word embedding : w = X g2Segwpgjwg ( 3 ) where gis the embedding for segmentation g. Given a subword segmentation g , we adopt the Bag - of - Subwords ( BoS ) model ( Bojanowski et al . , 2017 ; Zhao et", "entities": []}, {"text": "al . , 2018 ) for composing word embedding from subwords .", "entities": []}, {"text": "Speci\ufb01cally , we apply BoS4over the subword segments in g :", "entities": []}, {"text": "g = X s2gs ; ( 4 )", "entities": []}, {"text": "where sis the vector representation for subword s , as if the current segmentation gis the \u201c golden \u201d 4Zhao et al .", "entities": []}, {"text": "( 2018 ) used averaging instead of summation .", "entities": []}, {"text": "However , both give uniform weights to all subwords and result in vectors only differ by a scalar factor .", "entities": []}, {"text": "We thus do not distinguish the two and refer to either of them as BoS.", "entities": []}, {"text": "599segmentation of the word .", "entities": []}, {"text": "In such case , we assume the meaning of the word is the combination of the meaning of all its subword segments .", "entities": []}, {"text": "We maintain a look - up table S : \u0000+!Rdfor all subword vectors ( i.e. s = S(s ) ) as trainable parameters of the model , where dis the embedding dimension .", "entities": [[30, 32, "HyperparameterName", "embedding dimension"]]}, {"text": "Combining Eq .", "entities": []}, {"text": "( 3)and(4 ) , we can compose vector representation for any word w2\u0000+as w = X g2SegwpgjwX s2gs : ( 5 ) Given a set of target pre - trained word vectors w\u0003de\ufb01ned for words within a \ufb01nite vocabulary W , our model can be trained by minimizing the mean square loss : minimize S1 jWjX w2Wkw\u0000w\u0003k2 2 : ( 6 ) 3 Ef\ufb01cient Algorithm PBoS simultaneously considers all possible subword segmentations and their contributions in composing word representations .", "entities": [[51, 52, "MetricName", "loss"]]}, {"text": "However , summing over embeddings of all possible segmentations can be awfully inef\ufb01cient , as simply enumerating all possible segmentations of wtakes number of steps exponential to the length of w(Proposition 2 ) .", "entities": []}, {"text": "We therefore need an ef\ufb01cient way to compute Eq .", "entities": []}, {"text": "( 5 ) .", "entities": []}, {"text": "3.1 Alternative View : Weighted Subwords Exchanging the order of summations in Eq .", "entities": []}, {"text": "( 5 ) from segmentation \ufb01rst to subword \ufb01rst , we get w = X s\u0012wasjws ( 7 ) where asjw / X g2Segw ; g3spgjw ( 8) is the weight accumulated over subword s , summing over all segmentations of wthat contain s.5 Eq.(7)provides an alternative view of the word vector composed by our model : a weighted sum of all the word \u2019s subword vectors .", "entities": []}, {"text": "Comparing to BoS , we assign different importance asjw , instead of a uniform weight , to each subword .", "entities": []}, {"text": "asjwcan be viewed as the likelihood of subword sbeing a meaningful segment of the particular word w , 5For simplicity , here we assume all subwords are unique inw .", "entities": []}, {"text": "A more careful index - based summation would model the general case but the idea remains the same .", "entities": []}, {"text": "We take care of this in Algorithm 1.considering both the likelihood of sitself being meaningful , and at the same time how likely the rest of the word can still be segmented into meaningful subwords .", "entities": []}, {"text": "Example .", "entities": []}, {"text": "Consider the contribution of subword s=\u201cgher \u201d in wordw=\u201chigher \u201d .", "entities": []}, {"text": "Possible contributions only come from segmentations that contain \u201c higher \u201d : g1=(\u201ch \u201d , \u201c i \u201d , \u201c gher \u201d ) and g2= ( \u201c hi \u201d , \u201c gher \u201d ) .", "entities": []}, {"text": "Each segmentation gadds weight pgjwtoasjw .", "entities": []}, {"text": "In this case , a\u201cgher\u201djwwill be smaller thana\u201cer\u201djwbecause both pg1jwandpg2jwwould be rather small .", "entities": []}, {"text": "3.2 Computing Subword Weights Now we can ef\ufb01ciently compute Eq .", "entities": []}, {"text": "( 7)if we can ef\ufb01ciently compute asjw .", "entities": []}, {"text": "Here we present an algorithm that computes asjwfor alls\u0012winO(jwj2 ) time .", "entities": []}, {"text": "The speci\ufb01c structure of the subword transition graph means that edges only go from left to right .", "entities": []}, {"text": "Thus , we can split every path going through einto three parts : edges left to e , eitself and edges right toe .", "entities": []}, {"text": "In terms of subwords , that is , for s = w[i : j ] , l = jwj , each segmentation gthat contains scan be divided into three parts : segmentation gw[1 : i\u00001 ] overw[1 : i\u00001 ] , subwordsitself , and segmentationgw[j+1 : l]overw[j+ 1 : l ] .", "entities": []}, {"text": "Based on this , we can rewrite Eq .", "entities": []}, {"text": "( 8) as asjw / X g2Segwg3spsY s02gw[1 : i\u00001]ps0Y s02gw[j+1 : l]ps0(9 )", "entities": []}, {"text": "= psb1;i\u00001bj+1;l ; ( 10 ) wherebi0;j0 = P g02Segw[i0 : j0]Q s02g0ps0 .", "entities": []}, {"text": "Now we can ef\ufb01ciently compute asjwif we can ef\ufb01ciently compute b1;i\u00001andbj+1;lfor all 1\u0014 i;j\u0014l .", "entities": []}, {"text": "Fortunately , we can do so for b1;iusing the following recursive relation b1;i = i\u00001X k=0b1;kpw[k+1 : i ] ( 11 ) fori= 1;:::;l withb1;0= 1 .", "entities": []}, {"text": "Similar formulas hold forbj;l;j= 1;:::;l withbl+1;l= 1 .", "entities": []}, {"text": "Based on this , we devise Algorithm 1 for computingasjwfor alls\u0012w .", "entities": []}, {"text": "Here we take the alternative view of our model as a weighted average of all possible subwords ( thus the normalization in Line 12 ) , and an extension to the unweighted averaging of subwords as used in Zhao et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "600Algorithm 1 Computingasjw .", "entities": []}, {"text": "1 : Input : Wordw , psfor alls\u0012w.l = jwj . 2 : b1;0 1;bl+1;l 1 ; 3 : fori 1:::l do 4 : b1;i Pi\u00001 k=0pw[k+1 : i]b1;k 5 : bl\u0000i+1;l Pl k = l\u0000i+1pw[l\u0000i+1 : k]bk+1;l 6 : end for 7:~asjw 0for alls\u0012w 8 : fori 1:::l , j i:::l do 9 : ~a0 pw[i : j]b1;i\u00001bj+1;l 10 : ~aw[i : j]jw ~aw[i : j]jw+ ~a0", "entities": [[33, 35, "HyperparameterName", "k ="]]}, {"text": "11 : end for 12 : asjw ~asjw = P s0\u0012w ~ as0jwfor alls\u0012w 13 : returna\u0001jw Time complexity As we only access each subword once in each for - statement , the number of multiplications and additions involved is bounded by the number of subword locations of w. Each of Line 4 and Line 5 take imultiplications and i\u00001 additions respectively .", "entities": []}, {"text": "So Line 3 to Line 6 in total takes 2l2computations .", "entities": []}, {"text": "Line 8 to Line 11 takes 3l(l+1 ) 2computations .", "entities": []}, {"text": "Thus , the time complexity of Algorithm 1 is O(l2 ) .", "entities": []}, {"text": "Given a word of length 20,O(l2)(202= 400 ) is much better than enumerating allO(2l)(220= 1;048;576 ) segmentations .", "entities": []}, {"text": "Using the setting in Section 4.3 , PBoS only takes 30 % more time ( 590 \u0016s vs 454\u0016s ) in average than BoS ( by disabling asjwcomputation ) to compose a 300 - dimensional word embedding vector .", "entities": []}, {"text": "4 Experiments We design experiments to answer two questions : Do the segmentation likelihood and subword weights computed by PBoS align with their meaningfulness ?", "entities": []}, {"text": "Are the word embedding vectors generated by PBoS of good quality ?", "entities": []}, {"text": "For the former , we inspect segmentation results and subword weights ( Section 4.1 ) , and see how good they are at predicting word af\ufb01xes ( Section 4.2 ) .", "entities": []}, {"text": "For the latter , we evaluate the word embeddings composed by PBoS at word similarity task ( Section 4.3 ) and part - of - speech ( POS ) tagging task ( Section 4.4 ) .", "entities": [[7, 9, "TaskName", "word embeddings"], [13, 15, "TaskName", "word similarity"], [21, 24, "DatasetName", "part - of"]]}, {"text": "Due to the page limit , we only report the most relevant settings and results in this section .", "entities": []}, {"text": "Other details , including hardware , running time and detailed list of hyperparameters , can be found in Appendix A.4.1 Subword Segmentation In this subsection , we provide anecdotal evidence that PBoS is able to assign meaningful segmentation likelihood and subword weights .", "entities": []}, {"text": "Table 1 shows top subword segmentations and subsequent top subwords calculated by PBoS for some example word , ranked by their likelihood and weights respectively .", "entities": []}, {"text": "The calculation is based on the word frequency derived from the Google Web Trillion Word Corpus6 .", "entities": [[11, 12, "DatasetName", "Google"]]}, {"text": "We use the same list for word probability pwthroughout our experiments if not otherwise mentioned .", "entities": []}, {"text": "All other settings are the same as described for PBoS in Section 4.3 .", "entities": []}, {"text": "We can see the segmentation likelihood and subword weight favors the whole words as subword segments if the word appears in the word list , e.g. \u201c higher \u201d , \u201c farmland \u201d .", "entities": []}, {"text": "This allows the model to closely mimic the word embeddings for frequent words that are probably part of the target vectors .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "Second to the whole - word segmentation , or when the word is rare , e.g. \u201c penpineanpplepie \u201d , \u201c paradichlorobenzene \u201d , we see that PBoS gives higher likelihood to meaningful segmentations such as \u201c high / er \u201d , \u201c farm / land \u201d , \u201c pen / pineapple / pie \u201d and \u201c para / dichlorobenzene\u201dagainst other possible segmentations.7Subsequently , respective subword segments get higher weights among all possible subwords for the word , often by a good amount .", "entities": []}, {"text": "This behavior would help PBoS to focus on meaningful subwords when composing word embedding .", "entities": []}, {"text": "The fact that this can be achieved without any explicit source of morphological knowledge is itself interesting .", "entities": []}, {"text": "4.2 Af\ufb01x Prediction We quantitatively evaluate the quality of subword segmentations and subsequent subword weights by testing if our PBoS model is able to discover the most eminent word af\ufb01xes .", "entities": []}, {"text": "Note this has nothing to do with embeddings , so no training is involved in this experiment .", "entities": []}, {"text": "The af\ufb01x prediction task is to predict the most eminent af\ufb01x for a given word .", "entities": []}, {"text": "For example , \u201c -able \u201d for \u201c replaceable \u201d and \u201c re- \u201d for \u201c rename \u201d .", "entities": []}, {"text": "Models We get af\ufb01x prediction from our PBoS by taking the top - ranked subword that is one of the possible af\ufb01xes .", "entities": []}, {"text": "To show our advantage , we 6https://www.kaggle.com/rtatman/ english - word - frequency 7A slight exception is \u201c farmlan / d \u201d , probably because \u201c -d \u201d is a frequent suf\ufb01x .", "entities": []}, {"text": "601Wordw Top segmentation g(and theirpgjw )", "entities": []}, {"text": "Top subword s(and theirasjw ) higher higher ( 0.924 ) , high / er ( 0.030 ) , highe / r ( 0.027 ) , h / igher ( 0.007 ) , hig / her ( 0.004).higher ( 0.852 ) , high ( 0.031 ) , er ( 0.029 ) , r ( 0.029 ) , highe ( 0.025 ) .", "entities": []}, {"text": "farmland farmland ( 0.971 ) , farmlan / d ( 0.010 ) , farm / land ( 0.006 ) , f / armland ( 0.005).farmland ( 0.941 ) , d ( 0.010 ) , farmlan ( 0.009 ) , farm ( 0.008 ) , land ( 0.007 ) .", "entities": []}, {"text": "penpineapplepiepen / pineapple / pie ( 0.359 ) , pen / pineapple / pi / e ( 0.157 ) , pen / pineapple / p / ie ( 0.101).pineapple ( 0.238 ) , pen ( 0.186 ) , pie ( 0.131 ) , p ( 0.101 ) , e ( 0.099 ) .", "entities": []}, {"text": "paradichlorobenzenepara / dichlorobenzene ( 0.611 ) , par / a / dichlorobenzene ( 0.110 ) , paradi / chlorobenzene ( 0.083).dichlorobenzene ( 0.344 ) , para ( 0.283 ) , a ( 0.061 ) , par ( 0.054 ) , ichlorobenzene ( 0.042 ) .", "entities": []}, {"text": "Table 1 : Top segmentations and subword weights by PBoS for some example words Model Precision Recall F1 BoS 0.493 0.465 0.425 PBoS", "entities": [[15, 16, "MetricName", "Precision"], [16, 17, "MetricName", "Recall"], [17, 18, "MetricName", "F1"]]}, {"text": "0.861 0.874 0.829", "entities": []}, {"text": "Table 2 : Af\ufb01x prediction results based on subword weights .", "entities": []}, {"text": "All metrics are macro .", "entities": []}, {"text": "compare it with a BoS - style baseline af\ufb01x predictor .", "entities": []}, {"text": "Because BoS gives same weight to all subwords in a given word , we randomly choose one of the possible af\ufb01xes that appear as subword of the word .", "entities": []}, {"text": "Benchmark", "entities": []}, {"text": "We use the derivational morphology dataset8from Lazaridou et al .", "entities": []}, {"text": "( 2013 ) .", "entities": []}, {"text": "The dataset contains 7449 English words in total along with their most eminent af\ufb01xes .", "entities": []}, {"text": "Because no training is needed in this experiment , we use all the words for evaluation .", "entities": []}, {"text": "To make the task more challenging , we drop trivial instances where there is only one possible af\ufb01x appears as a subword in the given word .", "entities": []}, {"text": "For example , \u201c rename \u201d is dropped because only pre\ufb01x \u201c re- \u201d is present ; on the other hand , \u201c replaceable \u201d is kept because both \u201c re- \u201d and \u201c -able \u201d are present .", "entities": []}, {"text": "Besides excluding the trivial cases described above , we also exclude instances labeled with suf\ufb01x \u201c -y \u201d , because it is always included by \u201c -ly \u201d and \u201c -ity \u201d .", "entities": []}, {"text": "Altogether , we acquire 3546 words with 17 possible af\ufb01xes for this evaluation .", "entities": []}, {"text": "Results Af\ufb01x prediction results in terms of macro precision , recall , and F1 score are shown in Table 2 .", "entities": [[13, 15, "MetricName", "F1 score"]]}, {"text": "We can see a de\ufb01nite advantage of PBoS at predicting most word af\ufb01xes , where all the metrics boost about 0.4 and F1 almost doubles compared to BoS , providing evidence that PBoS is able to assign meaningful subword weights .", "entities": [[22, 23, "MetricName", "F1"]]}, {"text": "4.3 Word Similarity Given that PBoS is able to produce sensible segmentation likelihood and subword weights , we now turn our focus onto the quality of the generated 8http://marcobaroni.org/PublicData/ affix_complete_set.txt.gzword embeddings .", "entities": [[1, 3, "TaskName", "Word Similarity"]]}, {"text": "In this section , we evaluate the word vectors \u2019 ability to capture word senses using the intrinsic task of word similarity .", "entities": [[20, 22, "TaskName", "word similarity"]]}, {"text": "Word similarity aims to test how well word embeddings capture words \u2019 semantic similarity .", "entities": [[0, 2, "TaskName", "Word similarity"], [7, 9, "TaskName", "word embeddings"], [12, 14, "TaskName", "semantic similarity"]]}, {"text": "The task is given as pairs of words , along with their similarity scores labeled by language speakers .", "entities": []}, {"text": "Given a set of word embeddings , we compute the similarity scores induced by the cosine distance between the embedding vectors of each pair of words .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "The performance is then measured in Spearman \u2019s correlation\u001afor all pairs .", "entities": []}, {"text": "Benchmarks We use WordSim353 ( WS ) from Finkelstein et al .", "entities": []}, {"text": "( 2001 ) which mainly consists of common words .", "entities": []}, {"text": "To better access models \u2019 ability to generalize word embeddings towards OOV words , we include the rare word datasets RareWord ( RW ) from Luong et", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "al .", "entities": []}, {"text": "( 2013 ) and the newer Card-660 ( Card ) from Pilehvar et al .", "entities": [[6, 7, "DatasetName", "Card-660"]]}, {"text": "( 2018 ) .", "entities": []}, {"text": "Model Setup PBoS composes word embeddings out of subword vectors exactly as described in Section 3 .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "Unlike some of previous models , we do not add special characters to indicate word boundaries and do not set any constraint on subword lengths .", "entities": []}, {"text": "PBoS is trained 50 epochs using vanilla SGD with initial learning rate 1 and inverse square root decay .", "entities": [[7, 8, "MethodName", "SGD"], [10, 12, "HyperparameterName", "learning rate"]]}, {"text": "For baselines , we compare against the bag - ofsubword model ( BoS ) from Zhao", "entities": []}, {"text": "et", "entities": []}, {"text": "al . ( 2018 ) , and the best attention - based model ( KVQ - FH ) from Sasaki et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "For BoS , we use our implementation by disabling subword weight computation .", "entities": []}, {"text": "For KVQ - FH , we use the implementation given in the paper .", "entities": []}, {"text": "All the hyperparameters are set the same as described in the original papers .", "entities": []}, {"text": "We choose to not include the character - RNN model ( MIMICK ) from Pinter et al .", "entities": []}, {"text": "( 2017 ) , as it has been shown clearly outperformed by the two .", "entities": []}, {"text": "602WS RW Card Polyglot : 100k tokens \u000264 dim IV pairs 45 41 10 All pairs 36 10 5 OOV % 5 % 58 % 90 % Google : 160k tokens \u0002300 dim IV pairs 69 53 34 All pairs 68 45 10 OOV % 1 % 11 % 79 % Table 3 : Target vectors statistics and word similarity performance measured in Spearman \u2019s \u001a\u0002100 .", "entities": [[27, 28, "DatasetName", "Google"], [58, 60, "TaskName", "word similarity"]]}, {"text": "Model # Param WS RW Card Target : Polyglot BoS 29.8 M 34 34 6 KVQ - FH 7.8 M 31 32 12 PBoS 37.8 M 41 25 15 Target : Google News BoS 162.7 M 61 48 11 KVQ - FH 36.2 M 64 49 21", "entities": [[31, 32, "DatasetName", "Google"]]}, {"text": "PBoS", "entities": []}, {"text": "315.7 M 68 49 25 Table 4 : Word similarity performance of subword - level models measured in Spearman \u2019s \u001a\u0002100 .", "entities": [[8, 10, "TaskName", "Word similarity"]]}, {"text": "Target Vectors We train all the subword models over English Polyglot vectors9and", "entities": []}, {"text": "English Google News vectors10 .", "entities": [[1, 2, "DatasetName", "Google"]]}, {"text": "Following the protocol of Zhao et al .", "entities": []}, {"text": "( 2018 ) and", "entities": []}, {"text": "Sasaki et al .", "entities": []}, {"text": "( 2019 ) , we clean and \ufb01lter the words in Google vectors .", "entities": [[11, 12, "DatasetName", "Google"]]}, {"text": "Dimension of word vectors , number of words in target vectors are summarized in Table 3 , along with their word similarity scores and OOV rate over the benchmarks .", "entities": [[20, 22, "TaskName", "word similarity"]]}, {"text": "As we can see , both pre - trained embeddings yield decent correlations with human - labeled word similarity .", "entities": [[17, 19, "TaskName", "word similarity"]]}, {"text": "However , the scores drop signi\ufb01cantly as the OOV rate goes up .", "entities": []}, {"text": "Polyglot vectors yield lower scores probably due to their smaller dimension and smaller token coverage .", "entities": []}, {"text": "Results Word similarity results of the three subword - level models are summarized in Table 4.11PBoS achieves scores better than or at least comparable to BoS and KVQ - FH in all but one of the six combinations of target vectors and word similarity benchmarks .", "entities": [[1, 3, "TaskName", "Word similarity"], [42, 44, "TaskName", "word similarity"]]}, {"text": "Viewed as an extension to BoS , PBoS is in majority cases better than BoS , often by a good margin , suggesting the effectiveness of the subword weighting scheme .", "entities": []}, {"text": "Compared to 9https://polyglot.readthedocs.io/en/ latest / Download.html 10https://code.google.com/archive/p/ word2vec/ 11We regard training and prediction time as less of a concern here as all the three models are able to \ufb01nish a training epoch in under a minute .", "entities": []}, {"text": "Details and discussions can be found in Appendix A.2.KVQ - FH , PBoS can often match and sometimes surpass it even though PBoS is a much simpler model with better explainability .", "entities": []}, {"text": "Compared to the scores by using just the target embeddings ( Table 3 , All pairs ) , PBoS is the only model that demonstrates improvement across allcases .", "entities": []}, {"text": "The only case where PBoS is not doing well is with Polyglot vectors and RW benchmark .", "entities": []}, {"text": "After many manual inspections , we conjecture that it may be related to the vector norm .", "entities": []}, {"text": "Sometimes the vector of a relevant subword can be of a small norm , prone to be overwhelmed by less relevant subword vectors .", "entities": []}, {"text": "To counter this , we tried to normalize subword vectors before summing them up into a word vector ( PBoS - n ) .", "entities": []}, {"text": "PBoS - n showed good improvement for the Polyglot RW case ( 25 to 32 ) , matching the performance of the other two .", "entities": []}, {"text": "One may argue that PBoS has an advantage for using the most number of parameters .", "entities": [[12, 15, "HyperparameterName", "number of parameters"]]}, {"text": "However , this is largely because we do not constrain the length of subwords as in BoS or use hashing as in KVQ - FH .", "entities": []}, {"text": "In fact , restricting subword length and using hashing helped them for the word similarity task .", "entities": [[13, 15, "TaskName", "word similarity"]]}, {"text": "We found that PBoS is insensitive to subword length constraints and decide to keep the setting simple .", "entities": []}, {"text": "Despite being an interesting direction , we decide to not involve hashing in this work to focus on the effect of our unique weighting scheme .", "entities": []}, {"text": "FaxtText Comparison Albeit targeted for a different task ( training word embedding ) which have access to contextual information , the popular fastText ( Bojanowski et al . , 2017 ) also uses a subwordlevel model .", "entities": [[22, 23, "MethodName", "fastText"]]}, {"text": "We train fastText12over the same English corpus on which the Polyglot target vectors are trained , in order to understand the quantitative impact of contextual information .", "entities": []}, {"text": "To ensure a fair comparison , we restrict the vocabulary sizes and embedding dimensions to match those of Polyglot vectors .", "entities": []}, {"text": "The word similarity scores we get for the trained fastText model are 65/40/14 for WS / RW / Card .", "entities": [[1, 3, "TaskName", "word similarity"], [9, 10, "MethodName", "fastText"]]}, {"text": "We note the great gain for WS and RW , suggesting the helpfulness of contextual information in learning and generalizing word embeddings in the setting of small to moderate OOV rates .", "entities": [[20, 22, "TaskName", "word embeddings"]]}, {"text": "Surprisingly , we \ufb01nd that for the case of extremely high OOV rate ( Card ) , PBoS slightly surpasses fastText , suggesting PBoS \u2019 effectiveness in generalizing embeddings to OOV words even without any help from contexts .", "entities": [[20, 21, "MethodName", "fastText"]]}, {"text": "12https://github.com/facebookresearch/ fastText/", "entities": []}, {"text": "603Multilingual Results To evaluate and compare the effectiveness of PBoS across languages , we further train the models targeting multilingual Wikipedia2Vec vectors ( Yamada et al . , 2020 ) and evaluate them on multilingual WordSim353 and SemLex999 from Leviant and Reichart ( 2015 ) which are available in English , German , Italian and Russian .", "entities": []}, {"text": "To better access the models \u2019 ability togeneralize , we only take the top 10k words from the target vectors for training , which yields decent OOV rates , ranging from 23 % to 84 % .", "entities": []}, {"text": "Detailed results can be found in Appendix Section A.3 .", "entities": []}, {"text": "In summary , we \ufb01nd 1 ) that PBoS surpasses KVQFH for English and German and is comparable to KVQ - FH for Italian ; 2 ) that PBoS and KVQ - FH surpasses BoS for English , German and Italian ; and 3 ) no de\ufb01nitive trend among the three models for Russian .", "entities": []}, {"text": "4.4 POS Tagging We further assess the quality of generated word embedding via the extrinsic task of POS tagging .", "entities": []}, {"text": "The task is to categorize each word in a given context into a particular part of speech , e.g. noun , verb , and adjective .", "entities": []}, {"text": "POS Tagging Model", "entities": []}, {"text": "We follow the evaluation protocol for sequential labeling used by Kiros et al .", "entities": []}, {"text": "( 2015 ) and Li", "entities": []}, {"text": "et", "entities": []}, {"text": "al . ( 2017 ) , and use logistic regression classi\ufb01er13as the model for POS tagging .", "entities": [[8, 10, "MethodName", "logistic regression"]]}, {"text": "When predicting the tag for the i - th wordwiin a sentence , the input to the classi\ufb01er is the concatenation of the vectors wi\u00002;wi\u00001;wi;wi+1;wi+2 for the word itself and the words in its context .", "entities": []}, {"text": "This setup allows a more direct evaluation of the quality of word vectors themselves , and thus gives better discriminative power.14", "entities": []}, {"text": "Dataset", "entities": []}, {"text": "We train and evaluate the performance of generated word embeddings over 23 languages at the intersection of the Polyglot ( Al - Rfou \u2019 et al . , 2013 ) pre - trained embedding vectors15and the Universal Dependency ( UD , v1.416 ) dataset .", "entities": [[8, 10, "TaskName", "word embeddings"], [39, 40, "DatasetName", "UD"]]}, {"text": "Polyglot vectors contain 64 - dimensional vectors over 13https://scikit-learn.org/0.19/ modules / generated / sklearn.linear_model .", "entities": []}, {"text": "LogisticRegression.html 14As a side note , in our early trials , we tried to evaluate using an LSTM model following Pinter et al .", "entities": [[17, 18, "MethodName", "LSTM"]]}, {"text": "( 2017 ) and Zhao et", "entities": []}, {"text": "al . ( 2018 ) , but found the numbers rather similar across embedding models .", "entities": []}, {"text": "One possible explanation is that LSTMs are so good at picking up contextual features that the impact of mild deviations of a single word vector is marginal .", "entities": []}, {"text": "15https://polyglot.readthedocs.io/ 16https://universaldependencies.org/Language KVQ - FH BoS PBoS", "entities": []}, {"text": "Arabic 0.813 0.754 0.905 ( +0.092 )", "entities": []}, {"text": "Basque 0.749 0.829 0.866 ( +0.037 )", "entities": []}, {"text": "Bulgarian 0.777 0.793 0.929 ( +0.136 ) Chinese 0.633 0.330 0.833 ( +0.200 )", "entities": []}, {"text": "Czech 0.799 0.823 0.917 ( +0.094 )", "entities": []}, {"text": "Danish 0.801 0.757 0.904 ( +0.103 ) English 0.770 0.770 0.896 ( +0.126 )", "entities": []}, {"text": "Greek 0.866 0.888 0.941 ( +0.053 ) Hebrew 0.775 0.703 0.915 ( +0.140 ) Hindi 0.811 0.800 0.901 ( +0.090 )", "entities": []}, {"text": "Hungarian 0.777 0.794 0.893 ( +0.099 ) Indonesian 0.776 0.828 0.899 ( +0.071 ) Italian 0.794 0.787 0.930 ( +0.135 ) Kazakh 0.623 0.753 0.815 ( +0.062 )", "entities": []}, {"text": "Latvian 0.722 0.756 0.848 ( +0.092 )", "entities": []}, {"text": "Persian 0.869 0.782 0.924 ( +0.056 ) Romanian 0.774 0.755 0.898 ( +0.123 ) Russian 0.775 0.838 0.911 ( +0.073 )", "entities": []}, {"text": "Spanish 0.818 0.769 0.920 ( +0.102 ) Swedish 0.826 0.840 0.920 ( +0.080 ) Tamil 0.702 0.758 0.755(-0.003 )", "entities": []}, {"text": "Turkish 0.760 0.777 0.837 ( +0.060 ) Vietnamese 0.663 0.712 0.832 ( +0.121 ) Table 5 : POS tagging accuracy over 23 languages .", "entities": [[19, 20, "MetricName", "accuracy"]]}, {"text": "In parentheses are the gains to the best of BoS and KVQ - FH .", "entities": []}, {"text": "an 100k vocabulary for each language and are used as target vectors for each of the subword - level embedding models in this experiment .", "entities": []}, {"text": "For PBoS , we use the Polyglot word counts for each language as the base for subword segmentation and subword weights calculation .", "entities": []}, {"text": "UD is used as the POS tagging dataset to train and test the POS tagging model .", "entities": [[0, 1, "DatasetName", "UD"]]}, {"text": "We use the default partition of training and testing set .", "entities": []}, {"text": "Statistics vary from language to language .", "entities": []}, {"text": "See Appendix A.4 for more details .", "entities": []}, {"text": "Results Table 5 shows the POS tagging accuracy over the 23 languages that appear in both Polyglot and UD .", "entities": [[7, 8, "MetricName", "accuracy"], [18, 19, "DatasetName", "UD"]]}, {"text": "All the subword - level embedding models follow the same hyperparameters as in Section 4.3 .", "entities": []}, {"text": "Following Sasaki et al .", "entities": []}, {"text": "( 2019 ) , we tune the regularization term of the logistic regression model when evaluating KVQ - FH .", "entities": [[11, 13, "MethodName", "logistic regression"]]}, {"text": "Even with that , PBoS is able to achieve the best POS tagging accuracy in all but one language regardless of morphological types , OOV rates , and the number of training instances ( Appendix Table 12 ) .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "Particularly , PBoS improvement accuracy by greater than 0.1 for 9 languages .", "entities": [[4, 5, "MetricName", "accuracy"]]}, {"text": "For the one language ( Tamil ) where PBoS is not the most accurate , the difference to the best is small ( 0.003 ) .", "entities": []}, {"text": "KVQ - FH gives no signi\ufb01cantly more accurate predictions than BoS despite it is more complex and is the only one tuned with hyperparameters .", "entities": []}, {"text": "Overall , Table 5 shows that the word embeddings", "entities": [[7, 9, "TaskName", "word embeddings"]]}, {"text": "604composed by our PBoS is effective at predicting POS tags for a wide range of languages .", "entities": []}, {"text": "5 Related Work Popular word embedding methods , such as word2vec ( Mikolov et al . , 2013 ) , GloVe ( Pennington et al . , 2014 ) , often assume \ufb01nite - size vocabularies , giving rise to the problem of OOV words .", "entities": [[20, 21, "MethodName", "GloVe"]]}, {"text": "FastText ( Bojanowski et al . , 2017 ; Joulin et al . , 2017 ) attempted to alleviate the problem using subword - level model , and was followed by interests of using subword information to improve word embedding ( Wieting et al . , 2016 ; Cao and Lu , 2017 ; Li et", "entities": [[0, 1, "MethodName", "FastText"]]}, {"text": "al . , 2017 ; Athiwaratkun et al . , 2018 ; Li et", "entities": []}, {"text": "al . , 2018 ; Salle and Villavicencio , 2018 ; Xu et al . , 2019 ; Zhu et al . , 2019 ) .", "entities": []}, {"text": "Among them are Charagram by Wieting", "entities": []}, {"text": "et", "entities": []}, {"text": "al . ( 2016 ) which , albeit trained on speci\ufb01c downstream tasks , is similar to BoS followed by a non - linear activation , and the systematic evaluation by Zhu et al .", "entities": []}, {"text": "( 2019 ) over various choices of word composition functions and subword segmentation methods .", "entities": []}, {"text": "However , all works above either pay little attention to the interaction among subwords inside a given word , or treat subword segmentation and composing word representation as separate problems .", "entities": []}, {"text": "Another interesting thread of works ( Oshikiri , 2017 ; Kim et al . , 2018a , 2019 ) attempted to model language solely at the subword level and learn subword embeddings directly from text , providing evidence to the power of subword - level models , especially as the notion of word is thought doubtful by some linguistics ( Haspelmath , 2011 ) .", "entities": []}, {"text": "Besides the recent interest in subwords , there have been long efforts of using morphology to improve word embedding ( Luong et al . , 2013 ; Cotterell and Sch \u00a8utze , 2015 ; Cui et al . , 2015 ; Soricut and Och , 2015 ; Bhatia et al . , 2016 ; Cao and Rei , 2016 ; Xu et", "entities": []}, {"text": "al . , 2018 ; \u00a8Ust\u00a8un et al . , 2018 ; Edmiston and Stratos , 2018 ; Chaudhary et al . , 2018 ; Park and Shin , 2018 ) .", "entities": []}, {"text": "However , most of them require an external oracle , such as Morfessor ( Creutz and Lagus , 2002 ; Virpioja et al . , 2013 ) , for the morphological segmentations of input words , limiting their power to the quality and availability of such segmenters .", "entities": []}, {"text": "The only exception is the character LSTM model by Cao and Rei ( 2016 ) , which has shown some ability to recover the morphological boundary as a byproduct of learning word embedding .", "entities": [[6, 7, "MethodName", "LSTM"]]}, {"text": "The most related works in generalizing pretrained word embeddings have been discussed inSection 1 and compared throughout the paper .", "entities": [[7, 9, "TaskName", "word embeddings"]]}, {"text": "6 Conclusion and Future Work We propose PBoS model for generalizing pretrained word embeddings without contextual information .", "entities": [[12, 14, "TaskName", "word embeddings"]]}, {"text": "PBoS simultaneously considers all possible subword segmentations of a word and derives meaningful subword weights that lead to better composed word embeddings .", "entities": [[20, 22, "TaskName", "word embeddings"]]}, {"text": "Experiments on segmentation results , af\ufb01x prediction , word similarity , and POS tagging over 23 languages support the claim .", "entities": [[8, 10, "TaskName", "word similarity"]]}, {"text": "In the future , it would be interesting to see if PBoS can also help with the task of learning word embedding , and how hashing would impact the quality of composed embedding while facilitating a more compact model .", "entities": []}, {"text": "Acknowledgments The authors would like to thank anonymous reviewers of EMNLP for their comments .", "entities": []}, {"text": "ZJ would like to thank Xuezhou Zhang , Sidharth Mudgal , Matt Du and Harit Vishwakarma for their helpful discussions .", "entities": []}, {"text": "References Rami Al - Rfou \u2019 , Bryan Perozzi , and Steven Skiena .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Polyglot : Distributed word representations for multilingual NLP .", "entities": [[6, 8, "TaskName", "multilingual NLP"]]}, {"text": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning , pages 183\u2013192 , So\ufb01a , Bulgaria .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ben Athiwaratkun , Andrew Wilson , and Anima Anandkumar .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Probabilistic FastText for multi - sense word embeddings .", "entities": [[1, 2, "MethodName", "FastText"], [6, 8, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1 \u2013 11 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Parminder Bhatia , Robert Guthrie , and Jacob Eisenstein .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Morphological priors for probabilistic neural word embeddings .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 490\u2013500 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Piotr Bojanowski , Edouard Grave , Armand Joulin , and Tomas Mikolov .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Enriching word vectors with subword information .", "entities": []}, {"text": "Transactions of the Association for Computational Linguistics , 5:135\u2013146 .", "entities": []}, {"text": "Kris Cao and Marek Rei . 2016 .", "entities": []}, {"text": "A joint model for word embedding and word morphology .", "entities": []}, {"text": "In Proceedings of the 1st Workshop on Representation Learning for", "entities": [[7, 9, "TaskName", "Representation Learning"]]}, {"text": "605NLP , pages 18\u201326 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shaosheng Cao and Wei Lu . 2017 .", "entities": []}, {"text": "Improving word embeddings with convolutional feature learning and subword information .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence .", "entities": []}, {"text": "Aditi Chaudhary , Chunting Zhou , Lori Levin , Graham Neubig , David R. Mortensen , and Jaime Carbonell .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Adapting word embeddings to new languages with morphological and phonological subword representations .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3285\u20133295 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Ryan Cotterell and Hinrich Sch \u00a8utze . 2015 .", "entities": []}, {"text": "Morphological word - embeddings .", "entities": []}, {"text": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1287\u20131292 , Denver , Colorado . Association for Computational Linguistics .", "entities": []}, {"text": "Ryan Cotterell , Hinrich Sch \u00a8utze , and Jason Eisner .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Morphological smoothing and extrapolation of word embeddings .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1651 \u2013 1660 , Berlin , Germany . Association for Computational Linguistics .", "entities": []}, {"text": "Mathias Creutz and Krista Lagus .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Unsupervised discovery of morphemes .", "entities": []}, {"text": "In Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning , pages 21\u201330 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Qing Cui , Bin Gao , Jiang Bian , Siyu Qiu , Hanjun Dai , and Tie - Yan Liu . 2015 .", "entities": []}, {"text": "KNET :", "entities": []}, {"text": "A general framework for learning word embedding using morphological knowledge .", "entities": []}, {"text": "ACM Trans .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Inf .", "entities": []}, {"text": "Syst . , 34(1 ) .", "entities": []}, {"text": "Daniel Edmiston and Karl Stratos .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Compositional morpheme embeddings with af\ufb01xes as functions and stems as arguments .", "entities": []}, {"text": "In Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP , pages 1\u20135 , Melbourne , Australia .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Lev Finkelstein , Evgeniy Gabrilovich , Yossi Matias , Ehud Rivlin , Zach Solan , Gadi Wolfman , and Eytan Ruppin .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Placing search in context : The concept revisited .", "entities": []}, {"text": "In Proceedings of the 10th International Conference on World Wide Web , WWW \u2019 01 , page 406\u2013414 , New York , NY , USA .", "entities": []}, {"text": "Association for Computing Machinery .", "entities": []}, {"text": "Martin Haspelmath .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "The indeterminacy of word segmentation and the nature of morphology and syntax .", "entities": []}, {"text": "Folia linguistica , 45(1):31\u201380.Armand", "entities": []}, {"text": "Joulin , Edouard Grave , Piotr Bojanowski , and Tomas Mikolov .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Bag of tricks for ef\ufb01cient text classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics : Volume 2 , Short Papers , pages 427\u2013431 , Valencia , Spain .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mikhail Khodak , Nikunj Saunshi , Yingyu Liang , Tengyu Ma , Brandon Stewart , and Sanjeev Arora .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A la carte embedding : Cheap but effective induction of semantic feature vectors .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 12\u201322 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Geewook Kim , Kazuki Fukui , and Hidetoshi Shimodaira . 2018a .", "entities": []}, {"text": "Word - like character n - gram embedding .", "entities": []}, {"text": "In Proceedings of the 2018 EMNLP Workshop W - NUT : The 4th Workshop on Noisy User - generated Text , pages 148\u2013152 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Geewook Kim , Kazuki Fukui , and Hidetoshi Shimodaira . 2019 .", "entities": []}, {"text": "Segmentation - free compositional ngram embedding .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3207\u20133215 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yeachan Kim , Kang - Min Kim , Ji - Min Lee , and SangKeun Lee . 2018b .", "entities": []}, {"text": "Learning to generate word representations using subword information .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , pages 2551\u20132561 , Santa Fe , New Mexico , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Ryan Kiros , Yukun Zhu , Russ R Salakhutdinov , Richard Zemel , Raquel Urtasun , Antonio Torralba , and Sanja Fidler .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Skip - thought vectors .", "entities": []}, {"text": "In C. Cortes , N. D. Lawrence , D. D. Lee , M. Sugiyama , and R. Garnett , editors , Advances in Neural Information Processing Systems 28 , pages 3294\u20133302 .", "entities": []}, {"text": "Curran Associates , Inc. Angeliki Lazaridou , Marco Marelli , Roberto Zamparelli , and Marco Baroni .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Compositional - ly derived representations of morphologically complex words in distributional semantics .", "entities": []}, {"text": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1517\u20131526 , So\ufb01a , Bulgaria . Association for Computational Linguistics .", "entities": []}, {"text": "Ira Leviant and Roi Reichart .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Separated by an un - common language : Towards judgment language informed vector space modeling .", "entities": []}, {"text": "Bofang Li , Aleksandr Drozd , Tao Liu , and Xiaoyong Du . 2018 .", "entities": []}, {"text": "Subword - level composition functions for learning word embeddings .", "entities": [[7, 9, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of", "entities": []}, {"text": "606the Second Workshop on Subword / Character LEvel Models , pages 38\u201348 , New Orleans .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Bofang Li , Tao Liu , Zhe Zhao , Buzhou Tang , Aleksandr Drozd , Anna Rogers , and Xiaoyong Du . 2017 .", "entities": []}, {"text": "Investigating different syntactic context types and context representations for learning word embeddings .", "entities": [[10, 12, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2421\u20132431 , Copenhagen , Denmark .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thang Luong , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Better word representations with recursive neural networks for morphology .", "entities": []}, {"text": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning , pages 104\u2013113 , So\ufb01a , Bulgaria .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tomas Mikolov , Ilya Sutskever , Kai Chen , Greg S Corrado , and Jeff Dean .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Distributed representations of words and phrases and their compositionality .", "entities": []}, {"text": "In C. J. C. Burges , L. Bottou , M. Welling , Z. Ghahramani , and K. Q. Weinberger , editors , Advances in Neural Information Processing Systems 26 , pages 3111\u20133119 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "Takamasa Oshikiri .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Segmentation - free word embedding for unsegmented languages .", "entities": []}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 767\u2013772 , Copenhagen , Denmark . Association for Computational Linguistics .", "entities": []}, {"text": "Suzi Park and Hyopil Shin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Grapheme - level awareness in word embeddings for morphologically rich languages .", "entities": [[5, 7, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the Eleventh International Conference on Language Resources and Evaluation ( LREC 2018 ) , Miyazaki , Japan .", "entities": []}, {"text": "European Language Resources Association ( ELRA ) .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning . 2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532\u20131543 , Doha , Qatar .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Mohammad Taher Pilehvar , Dimitri Kartsaklis , Victor Prokhorov , and Nigel Collier .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Card-660 :", "entities": [[0, 1, "DatasetName", "Card-660"]]}, {"text": "Cambridge rare word dataset - a reliable benchmark for infrequent word representation models .", "entities": [[0, 1, "DatasetName", "Cambridge"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1391 \u2013 1401 , Brussels , Belgium . Association for Computational Linguistics .", "entities": []}, {"text": "Yuval Pinter , Robert Guthrie , and Jacob Eisenstein . 2017 .", "entities": []}, {"text": "Mimicking word embeddings using subword RNNs .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 102\u2013112 , Copenhagen , Denmark .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alexandre Salle and Aline Villavicencio .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Incorporating subword information into matrix factorization word embeddings .", "entities": [[6, 8, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the Second Workshop on Subword / Character LEvel Models , pages 66\u201371 , New Orleans .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Shota Sasaki , Jun Suzuki , and Kentaro Inui .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Subword - based Compact Reconstruction of Word Embeddings .", "entities": [[4, 5, "TaskName", "Reconstruction"], [6, 8, "TaskName", "Word Embeddings"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 3498\u20133508 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Timo Schick and Hinrich Sch \u00a8utze . 2019a .", "entities": [[0, 1, "DatasetName", "Timo"]]}, {"text": "Attentive mimicking : Better word embeddings by attending to informative contexts .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 489\u2013494 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Timo Schick and Hinrich Sch \u00a8utze . 2019b .", "entities": [[0, 1, "DatasetName", "Timo"]]}, {"text": "Learning semantic representations for novel words : Leveraging both form and context .", "entities": [[0, 3, "TaskName", "Learning semantic representations"]]}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 33 , pages 6965\u20136973 .", "entities": []}, {"text": "Radu Soricut and Franz Och . 2015 .", "entities": []}, {"text": "Unsupervised morphology induction using word embeddings .", "entities": [[4, 6, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1627\u20131637 , Denver , Colorado . Association for Computational Linguistics .", "entities": []}, {"text": "Karl Stratos . 2017 .", "entities": []}, {"text": "Reconstruction of word embeddings from sub - word parameters .", "entities": [[0, 1, "TaskName", "Reconstruction"], [2, 4, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the First Workshop on Subword and Character Level Models in NLP , pages 130\u2013135 , Copenhagen , Denmark . Association for Computational Linguistics .", "entities": []}, {"text": "Ahmet \u00a8Ust\u00a8un , Murathan Kurfal\u0131 , and Burcu Can .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Characters or morphemes : How to represent words ?", "entities": []}, {"text": "InProceedings of The Third Workshop on Representation Learning for NLP , pages 144\u2013153 , Melbourne , Australia . Association for Computational Linguistics .", "entities": [[6, 8, "TaskName", "Representation Learning"]]}, {"text": "Sami Virpioja , Peter Smit , Stig - Arne Gr \u00a8onroos , and Mikko Kurimo .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Morfessor 2.0 : Python implementation and extensions for morfessor baseline .", "entities": []}, {"text": "D4 julkaistu kehitt \u00a8amis- tai tutkimusraportti tai selvitys , Aalto University .", "entities": []}, {"text": "John Wieting , Mohit Bansal , Kevin Gimpel , and Karen Livescu . 2016 .", "entities": []}, {"text": "Charagram : Embedding words and sentences via character n - grams .", "entities": []}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1504\u20131515 , Austin , Texas .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "607Yang Xu , Jiawei Liu , Wei Yang , and Liusheng Huang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Incorporating latent meanings of morphological compositions to enhance word embeddings .", "entities": [[8, 10, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1232\u20131242 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Yang Xu , Jiasheng Zhang , and David Reitter .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Treat the word as a whole or look inside ?", "entities": []}, {"text": "subword embeddings model language change and typology .", "entities": []}, {"text": "InProceedings of the 1st International Workshop on Computational Approaches to Historical Language Change , pages 136\u2013145 , Florence , Italy .", "entities": [[17, 18, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ikuya Yamada , Akari Asai , Jin Sakuma , Hiroyuki Shindo , Hideaki Takeda , Yoshiyasu Takefuji , and Yuji Matsumoto .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Wikipedia2vec :", "entities": []}, {"text": "An ef\ufb01cient toolkit for learning and visualizing the embeddings of words and entities from wikipedia .", "entities": []}, {"text": "Jinman Zhao , Sidharth Mudgal , and Yingyu Liang .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Generalizing word embeddings using bag of subwords .", "entities": [[1, 3, "TaskName", "word embeddings"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 601\u2013606 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Yi Zhu , Ivan Vuli \u00b4 c , and Anna Korhonen .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "A systematic study of leveraging subword information for learning word representations .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 912\u2013932 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "608A Experimental Details Here we list the details of our experiments that are omitted in the main paper due to space constraints .", "entities": []}, {"text": "We run all our experiments on a machine with an 8 - core Intel i7 - 6700 CPU @ 3.40GHz , 32 GB Memory , and GeForce GTX 970 GPU .", "entities": []}, {"text": "A.1 Hyperparameters The meaning of hyperparameters shown in Table 6 , Table 7 and Table 8 as explained as follows .", "entities": []}, {"text": "Subwords \u000fminlen : The minimum length for a subword to be considered .", "entities": []}, {"text": "\u000fmaxlen : The maximum length for a subword to be considered .", "entities": []}, {"text": "\u000fword boundary : Whether to add special characters to annotate word boundaries .", "entities": []}, {"text": "Training \u000fepochs : The number of training epochs .", "entities": []}, {"text": "\u000flr : Learning rate .", "entities": [[2, 4, "HyperparameterName", "Learning rate"]]}, {"text": "\u000flrdecay : Whether to set learning rate to be inversely proportional to the square root of the epoch number .", "entities": [[5, 7, "HyperparameterName", "learning rate"], [17, 19, "HyperparameterName", "epoch number"]]}, {"text": "\u000fnormalize semb : Whether to normalize subword embeddings before composing word embeddings .", "entities": [[10, 12, "TaskName", "word embeddings"]]}, {"text": "\u000fprob eps : Default likelihood for unknown characters .", "entities": [[1, 2, "HyperparameterName", "eps"]]}, {"text": "Evaluation \u000fC : The inverse regularization term used by the logistic regression classi\ufb01er .", "entities": [[10, 12, "MethodName", "logistic regression"]]}, {"text": "A.2 Word Similarity Table 6 and Table 8 show the hyperparameter values used in the word similarity experiment ( Section 4.3 ) .", "entities": [[1, 3, "TaskName", "Word Similarity"], [15, 17, "TaskName", "word similarity"]]}, {"text": "We transform all words in the benchmarks into lowercase , following the convention in FastText ( Bojanowski et al . , 2017 ; Joulin et al . , 2017 ) , BoS ( Zhao et al . , 2018 ) , and KVQFH ( Sasaki et al . , 2019 ) .", "entities": [[14, 15, "MethodName", "FastText"]]}, {"text": "During the evaluation , we use 0 as the similarity score for a pair of words if we can not get word vector for one of the words , or the magnitude of the word vector is too small .", "entities": [[6, 7, "DatasetName", "0"]]}, {"text": "This is especially the case when we evaluate the target vectors , where OOV rates can be signi\ufb01cant .", "entities": []}, {"text": "Table 9 lists experimental result for word similarity in greater detail .", "entities": [[6, 8, "TaskName", "word similarity"]]}, {"text": "Regarding the training epoch time , note that KVQ - FH uses GPU and is implemented using a deep learning library17with underlying optimized C code , whereas our PBoS is implemented using pure Python and uses only single thread CPU .", "entities": []}, {"text": "We omit the prediction time for KVQ - FH , as we found it hard to separate the actual inference time from time used for other processes such as batching and data transfer between CPU and GPU .", "entities": []}, {"text": "However , we believe the overall trend should be similar as for the training time .", "entities": []}, {"text": "One may notice that the prediction time for BoS in Table 9 is different from what was reported at the end of Section 3 .", "entities": []}, {"text": "This is largely because the BoS in Table 9 has a different ( smaller ) set of possible subwords to consider due to the subword length limits .", "entities": []}, {"text": "In Section 3 , to fairly access the impact of subword weights computation , we ensure that BoS and PBoS work with the same set of possible subwords ( that used by PBoS in Section 4.3 ) , and thus observe a slight longer prediction time for BoS. A.3 Multilingual Word Similarity We use Wikipedia2Vec", "entities": [[50, 52, "TaskName", "Word Similarity"]]}, {"text": "( Yamada et al . , 2020 ) as target vectors , and keep the most frequent 10k words to get decent OOV rates .", "entities": []}, {"text": "The OOV rates and word similarity scores can be found in Table 10 .", "entities": [[4, 6, "TaskName", "word similarity"]]}, {"text": "We do not clean or \ufb01lter words as we did for the English word similarity , because we found it dif\ufb01cult to have a consistent way of pre - processing words across languages .", "entities": [[13, 15, "TaskName", "word similarity"]]}, {"text": "For PBoS , we use the word frequencies from Polyglot for subword segmentation and subword weight calculation as the same for the multilingual POS tagging experiment ( Section 4.4 ) .", "entities": []}, {"text": "We evaluate all the models on multilingual WordSim353 ( mWS ) and SemLex999 ( mSL ) from Leviant and Reichart ( 2015 ) , which is available for English , German , Italian and Russian .", "entities": []}, {"text": "The dataset also contains the relatedness ( rel ) and similarity ( sim ) benchmarks derived from mWS .", "entities": []}, {"text": "We list the results for multilingual word similarity in Table 11 .", "entities": [[6, 8, "TaskName", "word similarity"]]}, {"text": "A.4 POS Tagging Table 7 and Table 8 show the hyperparameter values used in the POS tagging experiment ( Section 4.4 ) .", "entities": []}, {"text": "For the prediction model , we use the logistic regression classi\ufb01er from scikit - learn 0.19.1 17Chainer , https://chainer.org/", "entities": [[8, 10, "MethodName", "logistic regression"]]}, {"text": "609with the default settings .", "entities": []}, {"text": "Following the observation in Sasaki et al .", "entities": []}, {"text": "( 2019 ) , we tune the regularization parameter Cfor KVQFH for all values a\u000210bwherea= 1 ; : : : ; 9and b=\u00001;0 ; : : : ; 4 .", "entities": []}, {"text": "We use the POS tagging accuracy for English as criterion , and choose C= 70 .", "entities": [[5, 6, "MetricName", "accuracy"]]}, {"text": "Table 12 lists some statistics of the datasets used in the POS tagging experiment .", "entities": []}, {"text": "PBoS is able to achieve better accuracy over BoS and KVQ - FH in all languages regardless of their morphological type , OOV rate and number of training instances for POS tagging .", "entities": [[6, 7, "MetricName", "accuracy"]]}, {"text": "610SettingsModel BoS PBoS PBoS - n Subwordsminlen 3 1 1 maxlen 6", "entities": []}, {"text": "None None word boundary True False", "entities": []}, {"text": "False Trainingepochs 50 50 50 lr 1.0 1.0 1.0 lrdecay True True True normalize semb False False True prob eps 0.01 0.01 0.01 Table 6 : Training settings used in word similarity experiment for BoS , PBoS , and", "entities": [[1, 2, "HyperparameterName", "Trainingepochs"], [9, 10, "HyperparameterName", "lrdecay"], [19, 20, "HyperparameterName", "eps"], [30, 32, "TaskName", "word similarity"]]}, {"text": "PBoS - n SettingsModel BoS PBoS", "entities": []}, {"text": "Subwordsminlen 3 1 maxlen 6", "entities": []}, {"text": "None word boundary True False Trainingepochs 20 20 lr 1.0 1.0 lrdecay True True prob eps 0.01 0.01 Evaluation C 1 1 Table 7 : Training settings used in POS tagging experiment for BoS and PBoS SettingsExperiment Word similarity POS tagging Subwordsminlen 3 3 maxlen 30 30 word boundary True True Trainingepochs 300 300 limit size 500,000 500,000 bucket size 40,000 40,000 Evaluation C N / A 70 Table 8 : Training settings used in experiments for KVQ - FH .", "entities": [[5, 6, "HyperparameterName", "Trainingepochs"], [11, 12, "HyperparameterName", "lrdecay"], [15, 16, "HyperparameterName", "eps"], [37, 39, "TaskName", "Word similarity"], [51, 52, "HyperparameterName", "Trainingepochs"]]}, {"text": "Model # ParamDataset Training Time Prediction Time WS RW Card Total Per epoch Total Per word Target : Polyglot BoS 29.8 M 34 34 6 505s 10.1s 1.9s 161 \u0016s KVQ - FH 7.8 M 31 32 12 2,669s 8.9s \u2013 \u2013 PBoS 37.8 M 41 25 15 966s 19.3s 4.2s 365 \u0016s Target :", "entities": []}, {"text": "Google News BoS 162.7 M 61 48 11 1,110s 22.2s 4.8s 414", "entities": [[0, 1, "DatasetName", "Google"]]}, {"text": "\u0016s", "entities": []}, {"text": "KVQ - FH 36.2 M 64 49 21 10,638s 35.5s \u2013 \u2013 PBoS 315.7 M 68 49 25 2,065s 41.3s 6.8s 590 \u0016s Table 9 : Word similarity performance of subword - level models measured in Spearman \u2019s \u001a\u0002100 , along with training and prediction time .", "entities": [[26, 28, "TaskName", "Word similarity"]]}, {"text": "611mWS", "entities": []}, {"text": "mWS - rel mWS - sim mSL English : 10k tokens \u0002300 dim IV pairs 65 56 71 26 All pairs 29 36 24 7 OOV 27 % 23 % 30 % 36 % Germen : 10k tokens \u0002300 dim IV pairs 58 50 60 35 All pairs 8 14 7 7 OOV 54 % 52 % 55 % 67 % Italian : 10k tokens \u0002300 dim IV pairs 52 50 54 24 All pairs 11 20 8 2 OOV 48 % 45 % 50 % 54 % Russian : 10k tokens \u0002300 dim IV pairs 47 32 48 12 All pairs 1 4 2 9 OOV 73 % 69 % 75 % 84 % Table 10 : Multilingual target vectors statistics and word similarity performance measured in Spearman \u2019s \u001a\u0002100.Model # Param mWSmWS mWSmSLrel sim English BoS 20.2 M 32 29 34 23 KVQ - FH 36.0 M 36 41 34 13 PBoS 30.4 M 53 44 61 22 Germen BoS 21.3 M 32 24 37 13 KVQ - FH 36.0 M 18 19 19 14 PBoS 45.8 M 38 30 38 12 Italian BoS 18.8 M 8 -2 17 25 KVQ - FH 36.0 M 19 22 21 9 PBoS 35.7 M 25 16 27 13 Russian BoS 20.0 M 20 15 21 14 KVQ - FH 36.0 M 19 11 24 9 PBoS 35.6 M 18 12 22 12 Table 11 : Multilingual word similarity performance of subword - level models measured in Spearman \u2019s \u001a\u0002100 .", "entities": [[123, 125, "TaskName", "word similarity"], [236, 238, "TaskName", "word similarity"]]}, {"text": "LanguageMorphologicalOOV % NtrainModel Type KVQ - FH BoS PBoS Arabic Fusional 27.1 % 225,853 0.813 0.754 0.905 Basque Agglutinative 39.2 %", "entities": []}, {"text": "72,974 0.749 0.829 0.866", "entities": []}, {"text": "Bulgarian Fusional 33.7 % 50,000 0.777 0.793 0.929 Chinese Isolating 70.8 % 98,608 0.633 0.330 0.833 Czech Fusional 58.5 % 1,173,282 0.799 0.823 0.917 Danish Fusional 33.3 % 88,980 0.801 0.757 0.904 English Analytic 26.2 % 204,587 0.770 0.770 0.896 Greek Fusional 18.5 % 47,449 0.866 0.888 0.941 Hebrew Fusional 20.3 % 135,496 0.775 0.703 0.915 Hindi Fusional 27.1 % 281,057 0.811 0.800 0.901 Hungarian Agglutinative 29.2 % 33,017 0.777 0.794 0.893 Indonesian Agglutinative 20.0 % 97,531 0.776 0.828 0.899 Italian Fusional 24.3 % 289,440 0.794 0.787 0.930 Kazakh Agglutinative 22.8 % 4,949 0.623 0.753 0.815 Latvian Fusional 23.7 % 13,781 0.722 0.756 0.848 Persian Agglutinative 16.9 % 121,064 0.869 0.782 0.924 Romanian Fusional 29.4 % 163,262 0.774 0.755 0.898 Russian Fusional 31.3 % 79,772 0.775 0.838 0.911 Spanish Fusional 29.1 % 382,436 0.818 0.769 0.920 Swedish Analytic 37.4 % 66,645 0.826 0.840 0.920 Tamil Agglutinative 28.4 %", "entities": []}, {"text": "6,329 0.702 0.758 0.755 Turkish Agglutinative 37.8 %", "entities": []}, {"text": "41,748 0.760 0.777 0.837 Vietnamese Analytic 63.8 % 31,800 0.663 0.712 0.832 Table 12 : Statistics for the languages used in POS tagging experiment .", "entities": []}, {"text": "Ntrain is the number of training instances for the POS tagging model .", "entities": []}, {"text": "OOV % is the percentage of the words in the POS tagging testing set that is out of the vocabulary of the Polyglot vectors in that language .", "entities": []}, {"text": "Experimental results are included for convenience .", "entities": []}]