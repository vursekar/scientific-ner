[{"text": "Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code - Switching , pages 65\u201371 June 11 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_00965Much Gracias : Semi - supervised Code - switch Detection for Spanish - English : How far can we get ?", "entities": []}, {"text": "Dana - Maria Iliescu\u0003 , Rasmus Grand\u0003 , Sara Qirko\u0003 , Rob van der Goot IT - University of Copenhagen dail@itu.dk , gran@itu.dk , saqi@itu.dk , robv@itu.dk Abstract Because of globalization , it is becoming more and more common to use multiple languages in a single utterance , also called codeswitching .", "entities": []}, {"text": "This results in special linguistic structures and , therefore , poses many challenges for Natural Language Processing .", "entities": []}, {"text": "Existing models for language identi\ufb01cation in code - switched data are all supervised , requiring annotated training data which is only available for a limited number of language pairs .", "entities": []}, {"text": "In this paper , we explore semi - supervised approaches , that exploit out - of - domain monolingual training data .", "entities": []}, {"text": "We experiment with word uni - grams , word n - grams , character ngrams , Viterbi Decoding , Latent Dirichlet Allocation , Support Vector Machine and Logistic Regression .", "entities": [[23, 26, "MethodName", "Support Vector Machine"], [27, 29, "MethodName", "Logistic Regression"]]}, {"text": "The Viterbi model was the best semi - supervised model , scoring a weighted F1 score of 92.23 % , whereas a fully supervised state - of - the - art BERT - based model scored 98.43%.1 1 Introduction Social platforms have been the cradle of the internet , driving vast amounts of communication among people from all over the world .", "entities": [[14, 16, "MetricName", "F1 score"], [31, 32, "MethodName", "BERT"]]}, {"text": "As a consequence , the way people communicate in written text has changed , as now it is common to use , for example , abbreviations of words , emoticons , references to other users and use multiple languages within the same utterance .", "entities": []}, {"text": "An annotated example sentence of this is the following tweet : Word El online exercise de hoy :", "entities": []}, {"text": "Label es en en es es other This phenomenon has caught particular interest in both sociolinguistics and Natural Language Processing ( NLP ) ( Aguilar et al . , 2020 ; Khanuja et al . , 2020 ) .", "entities": []}, {"text": "\u0003Equal contributions 1https://github.com/RalleGr/ msc - code - switchingClassifying the language labels on the word level ( i.e. code - switch detection ) has shown to be bene\ufb01cial to improve performance on downstream NLP tasks , like dependency parsing ( Bhat et al . , 2018 ) or lexical normalization ( Barik et al . , 2019 ) .", "entities": [[3, 4, "DatasetName", "msc"], [36, 38, "TaskName", "dependency parsing"], [47, 49, "TaskName", "lexical normalization"]]}, {"text": "Previous work has shown that high performances can be achieved for this task for many language pairs ( Molina et al . , 2016 ; Banerjee et al . , 2016 ) .", "entities": []}, {"text": "However , to the best of our knowledge , most previous work focused on supervised settings , restraining their usefulness to language pairs for which annotated datasets exist .", "entities": []}, {"text": "Recent efforts to unify existing datasets have collected annotation for 4 ( Aguilar et al . , 2020 ) and 2 ( Khanuja et al . , 2020 ) language pairs , which con\ufb01rms that annotated data is not available for most language pairs .", "entities": []}, {"text": "In supervised settings , recent transformer models ( Vaswani et al . , 2017 ; Devlin et al . , 2019 ) have reached a new state - of - the - art ( Aguilar et al . , 2020 ; Khanuja et al . , 2020 ) , outperforming Bi - LSTMS and traditional machine learning methods used earlier ( Molina et al . , 2016 ; Banerjee et al . , 2016 ) .", "entities": []}, {"text": "Yirmibe\u00b8 so \u02d8glu and Eryi \u02d8git ( 2018 ) tackled this task in a semi - supervised setup as well , where they used character n - gram language models trained on monolingual data to predict perplexity on the target word for classi\ufb01cation .", "entities": [[36, 37, "MetricName", "perplexity"]]}, {"text": "They show that this obtains a micro - average F1 score of 92.9 % , compared to 95.6 % with a supervised CRF - model .", "entities": [[8, 10, "MetricName", "average F1"], [22, 23, "MethodName", "CRF"]]}, {"text": "To overcome this limitation , we focus on exploiting only mono - lingual datasets for performing word - level language identi\ufb01cation in code - switched data .", "entities": []}, {"text": "We refer to this setup as semi - supervised , since we have no data annotated for the task at hand ( code - switch detection ) .", "entities": []}, {"text": "This enables the possibility to easily train models for new language pairs , and leads to the research question : How do semi - supervised models compare and perform in the task of language identi\ufb01cation in English - Spanish code - switched data ?", "entities": []}, {"text": "( RQ1 ) .", "entities": []}, {"text": "66Since supervised methods have the advantage of learning from annotated data , the second research question is : How much can we reduce the gap in performance between the aforementioned semisupervised models and a supervised state - of - the - art model ?", "entities": []}, {"text": "( RQ2 ) .", "entities": []}, {"text": "Previous work in similar setups have automatically generated code - switched data from monolingual datasets ( Santy et al . , 2021 ) .", "entities": []}, {"text": "We consider this approach to be orthogonal to ours , and Santy et al .", "entities": []}, {"text": "( 2021 ) exploit mono - lingual in - domain data , syntactic parsers and parallel sentences .", "entities": []}, {"text": "2 Datasets In this section , we will \ufb01rst describe the manually annotated code - switched data that we use for evaluating our models , then we describe the monolingual data that we will use as \u201c training \u201d data .", "entities": []}, {"text": "It should be noted that this is not real training data , as it is not annotated for the task at hand ( thus the setting is semi - supervised ) .", "entities": []}, {"text": "2.1 Test data To evaluate and compare our models , we use the Spanish - English ( SPA - EN ) part of the LinCE benchmark ( a total of 32,651 posts equivalent to 390,953 tokens ) ( Aguilar et al . , 2020 ) .", "entities": [[24, 25, "DatasetName", "LinCE"]]}, {"text": "We chose this language pair because it has the challenge of increased similarity between the languages ( Tristram , 1999 ) .", "entities": []}, {"text": "In the original data , 8 labels are used , from which we only focus on the 3 labels necessary for the language identi\ufb01cation task : lang1 , lang2 andother , for English , Spanish and punctuation , numbers , symbols and emoticons , respectively .", "entities": []}, {"text": "We use the default development and test splits for our experiments .", "entities": []}, {"text": "2.2 Monolingual data In order to perform semi - supervised codeswitching detection , we use Wikipedia data , because it is available in many languages and easy to obtain .", "entities": []}, {"text": "We extracted dumps from September 1st 2020 with Wikiextractor2 .", "entities": []}, {"text": "Without punctuation and numbers , the English dataset contains 420 K distinct words and the Spanish dataset contains 610 K distinct words .", "entities": []}, {"text": "It should be noted that there is a domain difference between the training and the dev / test data .", "entities": []}, {"text": "However , collecting monolingual data from Twitter 2https://github.com/attardi/ wikiextractoris non - trivial.3Furthermore , it should be noted that the Wikipedia datasets are not 100 % monolingual , so there will be some Spanish data in the English dump and vice - versa .", "entities": []}, {"text": "Both of these artefacts might have a negative effect on performance .", "entities": []}, {"text": "2.3 Automated annotation for monolingual tokens Tokenization of the raw datasets is done using the English and Spanish SpaCy tokenization models4 , as it matches the tokenization of the development and test sets .", "entities": []}, {"text": "Punctuation and non - word tokens ( theother class ) are identi\ufb01ed with manually designed rules using regular expressions , and the python emoji package .", "entities": []}, {"text": "Tokens that are not identi\ufb01ed as other , are labeled with the corresponding label based on the language of the wikipedia .", "entities": []}, {"text": "3 Methods 3.1 Word uni - grams We \ufb01rst clean the mono - lingual Wikipedia data by removing XML / HTML tags from the articles and special tokens that belong to the other class .", "entities": []}, {"text": "We calculate the word probability based on the resulting data ( word frequency / total number of words ) using Laplace smoothing with a smoothing factor of 1 . 3.2 Word n - grams We also experiment with taking a larger context into account through bi - grams and tri - grams .", "entities": []}, {"text": "Here , we divide the frequency of the n - gram containing the word with the frequency of the leading ( n\u00001)gram .", "entities": []}, {"text": "The probability is computed this way for a given word in each language , and then the label with the highest probability is assigned to the word .", "entities": []}, {"text": "Laplace smoothing with a factor of 1 is used .", "entities": []}, {"text": "In our initial experiments , tri - grams showed very low performance , so we use bi - grams in the remainder of this paper .", "entities": []}, {"text": "3.3 Character n - grams For this model , we calculate the joint log probability of words based on the monolingual training data , and assign the most probable label .", "entities": []}, {"text": "We vary the n - gram size from 1 to 6 and use Laplace smoothing with a factor of 1 . 3Twitter blog : Evaluating language identi\ufb01cation performance 4https://spacy.io/", "entities": []}, {"text": "673.4 Viterbi decoding The problem of code - switching can be represented as a Hidden Markov Model ( HMM ) problem , since a sentence can be seen as a Markov chain with hidden states that are the two different languages .", "entities": []}, {"text": "We use the Viterbi decoding algorithm ( Forney , 1973 ) to \ufb01nd the most probable sequence of states given the observations - namely , to assign a language label ( state ) to each word ( observation ) .", "entities": []}, {"text": "We used eginhard \u2019s implementation5of the Viterbi algorithm and modi\ufb01ed the starting and transition probabilities to the values speci\ufb01ed below , which were found to be optimal using grid search on the development set using the range of initial probabilities for English from 0.1 to 0.9 with step size 0.1 , transition probabilities for English from 0.05 to 0.95 with step size 0.05 .", "entities": [[47, 49, "HyperparameterName", "step size"], [60, 62, "HyperparameterName", "step size"]]}, {"text": "The \ufb01nal hyperparameters are as follows : \u2022states : lang1 andlang2 , other tokens are identi\ufb01ed based on heuristics ( see Section 2.3 ) ; \u2022initial probabilities : 0.6 for English and 0.4 for Spanish ; \u2022transition probabilities : 0.15 for transitioning to a different language and 0.85 for transitioning to the same language ; \u2022emission probabilities : these are estimated through a relative probability model , the probability of the word being emitted from English , for example , is : P(w ) = P(wjEN ) P(wjEN )", "entities": []}, {"text": "+ P(wjSPA )", "entities": []}, {"text": "; ( 1 ) where P(wjEN)andP(wjSPA ) are probabilities given by the dictionaries described in section 3.1 .", "entities": []}, {"text": "In case this is 0 ( i.e. the word does not occur in our monolingual data ) , the emission probability is calculated by a relative character bi - gram probability .", "entities": [[4, 5, "DatasetName", "0"]]}, {"text": "3.5 Latent Dirichlet Allocation Generally , Latent Dirichlet Allocation ( LDA ) aims to \ufb01nd the topics a document belongs to using the words in the document as features .", "entities": [[10, 11, "MethodName", "LDA"]]}, {"text": "In our case , the documents are the words , the features are character n - grams ( with n1 to 5 ) and the topics are 5https://github.com/eginhard/ word - level - language - id / English and Spanish .", "entities": []}, {"text": "The LDA algorithm does not output labels for the resulting clusters , so we select the top 10 words based on weight that represent best each cluster , and assign them a language using the word uni - gram method ( Section 3.1 ) .", "entities": [[1, 2, "MethodName", "LDA"]]}, {"text": "We use the Scikit Learn6implementation of LDA with theTfidfVectorizer and use only the \ufb01rst 100,000 words from each monolingual dataset , in order to reduce training time .", "entities": [[6, 7, "MethodName", "LDA"]]}, {"text": "3.6 Support Vector Machine For our Support Vector Machine ( SVM ) model , we consider the monolingual data ( Section 2.2 ) to be the gold training data , without tokens from the other class .", "entities": [[1, 4, "MethodName", "Support Vector Machine"], [6, 9, "MethodName", "Support Vector Machine"], [10, 11, "MethodName", "SVM"]]}, {"text": "Using TfidfVectorizer , we extract character n - gram features from each word , withn1 to 5 .", "entities": []}, {"text": "We use the Scikit Learn implementation with all default parameters and select the \ufb01rst 100,000 words from each dataset .", "entities": []}, {"text": "3.7 Logistic regression We use Logistic Regression in a weakly - supervised manner , the same as with SVM , where we consider the \ufb01rst 100,000 words from each Wikipedia dataset to be the gold training data .", "entities": [[1, 3, "MethodName", "Logistic regression"], [5, 7, "MethodName", "Logistic Regression"], [18, 19, "MethodName", "SVM"]]}, {"text": "Again , we use TfidfVectorizer to extract character n - gram features , with n1 to 5 , and rely on the default Scikit Learn implementation .", "entities": []}, {"text": "3.8 Ensemble model We also experiment with ensembling the previous methods , where we use a simple majority voting .", "entities": []}, {"text": "We compare using all models , to using the best 3 and the best 5 models , as well as an oracle .", "entities": []}, {"text": "4 Results To evaluate the performance of our models , we use weighted F1 score7 .", "entities": [[13, 14, "MetricName", "F1"]]}, {"text": "As found in Table 1 , Viterbi has the overall best performance scoring 95.76 % on validation data and 92.23 % on the test data .", "entities": []}, {"text": "Word uni - grams , character n - grams , SVM and Logistic Regression models achieve results with a range of weighted F1 score from 90.34 % to 92.19 % on validation data and a range from 87.80 % to 88.95 % on test data .", "entities": [[10, 11, "MethodName", "SVM"], [12, 14, "MethodName", "Logistic Regression"], [22, 24, "MetricName", "F1 score"]]}, {"text": "We compare our performance to a supervised BERT - based classi\ufb01er as implemented by the MaChAmp toolkit 0.2 ( van der Goot et al . , 2021 ) .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "6https://scikit-learn.org/ 7over only the classes of interests ; as mentioned in Section 2.1 , we only focus on 3/8 labels of the LinCE data", "entities": [[22, 23, "DatasetName", "LinCE"]]}, {"text": "68Model Dev Test Word uni - grams 91.32 % 88.25 % Word n - grams 50.50 % 49.04 % Character n - grams 92.19 % 88.95 % Viterbi 95.99 % 92.23 % LDA 64.40 % 62.84 % Support Vector Machine 91.39 % 88.74 % Logistic regression 90.34 % 87.80 % Ensemble Model", "entities": [[32, 33, "MethodName", "LDA"], [37, 40, "MethodName", "Support Vector Machine"], [44, 46, "MethodName", "Logistic regression"]]}, {"text": "All models 92.15 % 88.99 % Models 3 , 4 and 6 93.72 % 90.27 % Models 1 , 3 , 4 , 5 and 6 92.96 % 89.64 % Oracle\u000398.47 % Supervised MaChAmp 99.24 % 98.43 % Table 1 : Models evaluated using weighted F1 score on validation and test data.\u0003Made use of gold labels We use multilingual BERT and all default settings .", "entities": [[45, 47, "MetricName", "F1 score"], [59, 60, "MethodName", "BERT"]]}, {"text": "Results in Table 1 show that there is still a performance gap between the semi - supervised approaches and this state - of - the - art supervised model .", "entities": []}, {"text": "When comparing common confusions of our best semi - supervised model ( Viterbi ) to the output of MaChAmp , we found that there was more confusion in the Viterbi model about other , where 213 words were classi\ufb01ed as lang1 and 60 as lang2 instead , compared to just 3 and 1 in MaChAmp .", "entities": []}, {"text": "Full confusion matrices can be found in the appendix .", "entities": []}, {"text": "The majority voting ensembling models do not lead to improved performance .", "entities": []}, {"text": "However , the oracle ensemble , which always picks the correct label if it is available from one of the models , shows that there is potential in improving the selection method for ensembling .", "entities": []}, {"text": "5 Discussion When inspecting the performances of the models per class ( see also Table 2 in the appendix )", "entities": []}, {"text": ", we found that , for the development dataset , all models have a better F1 score for English than for Spanish and , for the test dataset , the other way around .", "entities": [[15, 17, "MetricName", "F1 score"]]}, {"text": "This might be due to a discrepancy between the label distribution of the two datasets and is a signi\ufb01cant aspect to be investigated in future work .", "entities": []}, {"text": "Regarding the LDA model , its low performance can be explained by the results of ( Zhang et al . , 2014 ) , which show that for the task of language \ufb01ltering , the performance of LDA decreases whenthe dominating language decreases under 70 % of the whole text .", "entities": [[2, 3, "MethodName", "LDA"], [37, 38, "MethodName", "LDA"]]}, {"text": "This is also the case in our experiments , where the test data had a 54 % English and 46 % Spanish ratio .", "entities": []}, {"text": "Furthermore , the amount of evidence per sample is rather low compared to the normal use of LDA ( it is commonly used on the document level ) .", "entities": [[17, 18, "MethodName", "LDA"]]}, {"text": "For character n - grams , we observed that the more we increased the value of n , the better results we got , up until n= 6 .", "entities": []}, {"text": "The higher order n - grams performed better with around 12 % difference in validation weighted F1 score , as we can capture groups of letters that are representative for a language , e.g. \u2018 tion \u2019 in English and \u2018 cion \u2019 in Spanish .", "entities": [[16, 18, "MetricName", "F1 score"]]}, {"text": "This model achieves good results also because it addresses the problem of misspelled words .", "entities": []}, {"text": "For word n - grams , using tri - grams resulted in worse predictions than using bi - grams with around 11 % difference in validation weighted F1 score .", "entities": [[27, 29, "MetricName", "F1 score"]]}, {"text": "For LDA , SVM and Logistic Regression models we tried to vectorize data with CountVectorizer from Scikit Learn , which gives the termfrequency for each n - gram in a word .", "entities": [[1, 2, "MethodName", "LDA"], [3, 4, "MethodName", "SVM"], [5, 7, "MethodName", "Logistic Regression"]]}, {"text": "However , TfidfVectorizer performed approximately 1 % better in LDA and Logistic Regression and 4 % for SVM in validation data .", "entities": [[9, 10, "MethodName", "LDA"], [11, 13, "MethodName", "Logistic Regression"], [17, 18, "MethodName", "SVM"]]}, {"text": "This was then the preferred vectorizer in all models , as it helps decreasing the impact of very frequent character n - grams that are not expressing much value and gives more importance to less frequent character n - grams .", "entities": []}, {"text": "The fact that the oracle model has a 3 % higher weighted F1 score than the best model ( in validation data ) , suggests that there is room for improvement for the ensemble model with other methods than majority voting .", "entities": [[12, 14, "MetricName", "F1 score"]]}, {"text": "Improvements on the single models could be achieved by using bigger monolingual datasets of the same size or selecting a corpus that is more similar to the test set ( social media - like posts ) , which is not as easy to query as Wikipedia articles .", "entities": []}, {"text": "The overall performance of the models can also be slightly improved by a more complex method for the other class ( the existing rule - based method scored an F1 of 96.76 , see Table 2 in the appendix ) .", "entities": [[29, 30, "MetricName", "F1"]]}, {"text": "The training ef\ufb01ciency of the Viterbi model and the supervised model were measured in a Windows Sub - system for Linux environment on an i7 - 7700 K processor with 16 GB ram .", "entities": [[20, 21, "DatasetName", "Linux"]]}, {"text": "We ran the MaChAmp model in this environment and it completed in 53,990 seconds .", "entities": []}, {"text": "In comparison , the Viterbi training", "entities": []}, {"text": "69completed in 1,805 seconds , which is an improvement of almost 30 times faster than the MaChAmp model .", "entities": []}, {"text": "6 Conclusion In this study we evaluated different types of models , namely word uni - grams , word n - grams , character n - grams , Viterbi Decoding , Latent Dirichlet Allocation , Support Vector Machine and Logistic Regression , for the task of semi - supervised language identi\ufb01cation in English - Spanish codeswitched data .", "entities": [[35, 38, "MethodName", "Support Vector Machine"], [39, 41, "MethodName", "Logistic Regression"]]}, {"text": "We found that most of the models achieved promising results , however , the Viterbi model performed the best with a weighted F1 score of 95.76 % on validation data and 92.23 % on test data ( RQ1 ) .", "entities": [[22, 24, "MetricName", "F1 score"]]}, {"text": "Using this model , one can potentially train CS - detection for many more language pairs as previously possible .", "entities": [[8, 9, "DatasetName", "CS"]]}, {"text": "Furthermore , since the majority voting did not lead to improvements , we experimented with an Oracle model , which showed that by combining results form our models , the best score we could achieve is 98.47 % on validation data .", "entities": []}, {"text": "Even though the results were good , our models still underperformed compared to the supervised MaChAmp model , that scored 99.24 % weighted F1 score on validation data and 98.43 % on test data ( RQ2 ) .", "entities": [[23, 25, "MetricName", "F1 score"]]}, {"text": "There is also a clear take away that , by using simpler , faster approaches like ours and when top performance is not crucial , one can avoid the extensive process of human - annotation and long training time that are needed by \ufb01netuning these large transformer models on supervised data .", "entities": []}, {"text": "References Gustavo Aguilar , Sudipta Kar , and Thamar Solorio .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "LinCE :", "entities": [[0, 1, "DatasetName", "LinCE"]]}, {"text": "A centralized benchmark for linguistic code - switching evaluation .", "entities": []}, {"text": "In Proceedings of the 12th Language Resources and Evaluation Conference , pages 1803\u20131813 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Somnath Banerjee , Kunal Chakma , Sudip Kumar Naskar , Amitava Das , Paolo Rosso , Sivaji Bandyopadhyay , and Monojit Choudhury .", "entities": [[7, 8, "DatasetName", "Kumar"]]}, {"text": "2016 .", "entities": []}, {"text": "Overview of the mixed script information retrieval ( MSIR ) at FIRE-2016 .", "entities": [[5, 7, "TaskName", "information retrieval"]]}, {"text": "In Forum for Information Retrieval Evaluation , pages 39\u201349 .", "entities": [[3, 5, "TaskName", "Information Retrieval"]]}, {"text": "Springer .", "entities": []}, {"text": "Anab Maulana Barik , Rahmad Mahendra , and Mirna Adriani .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Normalization of IndonesianEnglish code - mixed Twitter data .", "entities": []}, {"text": "In Proceedings of the 5th Workshop on Noisy User - generated Text ( W - NUT 2019 ) , pages 417\u2013424 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Irshad Bhat , Riyaz A. Bhat , Manish Shrivastava , and Dipti Sharma .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Universal Dependency parsing for Hindi - English code - switching .", "entities": [[1, 3, "TaskName", "Dependency parsing"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 987\u2013998 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "G. D. Forney .", "entities": []}, {"text": "1973 .", "entities": []}, {"text": "The Viterbi algorithm .", "entities": []}, {"text": "In The viterbi algorithm , 3 , pages 268\u2013278 .", "entities": []}, {"text": "Simran Khanuja , Sandipan Dandapat , Anirudh Srinivasan , Sunayana Sitaram , and Monojit Choudhury .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "GLUECoS :", "entities": []}, {"text": "An evaluation benchmark for code - switched nlp .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , page 3575\u20133585 .", "entities": []}, {"text": "Giovanni Molina , Fahad AlGhamdi , Mahmoud Ghoneim , Abdelati Hawwari , Nicolas ReyVillamizar , Mona Diab , and Thamar Solorio .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Overview for the second shared task on language identi\ufb01cation in code - switched data .", "entities": []}, {"text": "In Proceedings of the Second Workshop on Computational Approaches to Code Switching , pages 40\u201349 , Austin , Texas .", "entities": [[18, 19, "DatasetName", "Texas"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sebastin Santy , Anirudh Srinivasan , and Monojit Choudhury . 2021 .", "entities": []}, {"text": "BERTologiCoMix : How does code - mixing interact with multilingual BERT ?", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "In Proceedings of the Second Workshop on Domain Adaptation for NLP , pages 111\u2013121 , Kyiv , Ukraine .", "entities": [[7, 9, "TaskName", "Domain Adaptation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Hildegard L. C. Tristram .", "entities": []}, {"text": "1999 .", "entities": []}, {"text": "How Celtic is Standard English ?", "entities": []}, {"text": "Nauka .", "entities": []}, {"text": "Rob van der Goot , Ahmet \u00dcst\u00fcn , Alan Ramponi , Ibrahim Sharaf , and Barbara Plank .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Massive choice , ample tasks ( MaChAmp ): A toolkit for multi - task learning in NLP .", "entities": [[11, 15, "TaskName", "multi - task learning"]]}, {"text": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics : System Demonstrations , pages 176\u2013197 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141 ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 30 .", "entities": []}, {"text": "Curran Associates , Inc.", "entities": []}, {"text": "70Zeynep Yirmibe\u00b8 so \u02d8glu and G\u00fcl\u00b8 sen Eryi \u02d8git .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Detecting code - switching between Turkish - English language pair .", "entities": []}, {"text": "In Proceedings of the 2018 EMNLP Workshop W - NUT : The 4th Workshop on Noisy Usergenerated Text , pages 110\u2013115 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Wei Zhang , Robert AJ Clark , and Yongyuan Wang .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Unsupervised language \ufb01ltering using the latent dirichlet allocation .", "entities": []}, {"text": "In Fifteenth Annual Conference of the International Speech Communication Association .", "entities": []}, {"text": "71A Appendix Validation F1 score Test F1 score Model English Spanish Other English Spanish Other 1 .", "entities": [[3, 5, "MetricName", "F1 score"], [6, 8, "MetricName", "F1 score"]]}, {"text": "Word uni - grams 91.05 % 88.77 % 96.76 % 87.42 % 89.94 % 95.84 % 2 .", "entities": []}, {"text": "Word n - grams 45.41 % 31.98 % 96.76 % 40.61 % 35.73 % 95.84 % 3 .", "entities": []}, {"text": "Character n - grams 91.84 % 90.19 % 96.76 % 88.56 % 90.68 % 95.84 % 4 .", "entities": []}, {"text": "Viterbi 96.09 % 95.48 % 96.76 % 93.13 % 94.71 % 95.84 % 5 .", "entities": []}, {"text": "Latent Dirichlet Allocation 59.37 % 53.09 % 96.76 % 53.15 % 57.74 % 95.84 % 6 .", "entities": []}, {"text": "Support Vector Machine 90.84 % 89.21 % 96.76 % 88.07 % 90.55 % 95.84 % 7 .", "entities": [[0, 3, "MethodName", "Support Vector Machine"]]}, {"text": "Logistic regression 89.65 % 87.74 % 96.76 % 86.74 % 89.41 % 95.84 % Ensemble Model", "entities": [[0, 2, "MethodName", "Logistic regression"]]}, {"text": "English Spanish Other English Spanish Other All models 91.92 % 90.00 % 96.76 % 88.36 % 90.91 % 95.84 % Models 3 , 4 and 6 93.55 % 92.31 % 96.76 % 90.35 % 92.34 % 95.84 % Models 1 , 3 , 4 , 5 and 6 92.79 % 91.17 % 96.76 % 89.31 % 91.69 % 95.84 % Oracle\u000398.96 % 98.81 % 96.76 % - - Supervised MaChAmp 99.14 % 99.08 % 99.78 % 98.42 % 99.00 % 99.84 % Table 2 : Models evaluated using F1 score per class for validation and test data Figure 1 : Confusion matrix of Viterbi predictions on test data Figure 2 : Confusion matrix of MaChAmp predictions on test data Figure 1 and 2 show the confusion matrices of the Viterbi and MaChAmp model .", "entities": [[88, 90, "MetricName", "F1 score"]]}, {"text": "It can be noted that the confusion matrix for MaChAmp model has more than the three labels we used , because it was trained on part of the original training set presented in Section 2.1 .", "entities": []}, {"text": "This set contained 8 classes , and , thus , occasionally , the model mistakenly predicted some of these classes .", "entities": []}, {"text": "It can be seen that there was more confusion in Viterbi model about other , where 213 words were classi\ufb01ed as lang1 and 60 as lang2 instead , compared to just 3 and 1 in MaChAmp , which also had 7 other misclassi\ufb01cations .", "entities": []}]