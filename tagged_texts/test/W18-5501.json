[{"text": "The Fact Extraction and VERi\ufb01cation ( FEVER ) Shared Task James Thorne1,Andreas Vlachos1,Oana Cocarascu2 , Christos Christodoulopoulos3 , and Arpit Mittal3 1Department of Computer Science , University of Shef\ufb01eld 2Department of Computing , Imperial College London 3Amazon Research Cambridge fj.thorne , a.vlachos g@sheffield.ac.uk oana.cocarascu11@imperial.ac.uk fchrchrs , mitarpit g@amazon.co.uk Abstract We present the results of the \ufb01rst Fact Extraction and VERi\ufb01cation ( FEVER )", "entities": [[6, 7, "DatasetName", "FEVER"], [38, 39, "DatasetName", "Cambridge"], [61, 62, "DatasetName", "FEVER"]]}, {"text": "Shared Task .", "entities": []}, {"text": "The task challenged participants to classify whether human - written factoid claims could be S UPPORTED or R EFUTED using evidence retrieved from Wikipedia .", "entities": []}, {"text": "We received entries from 23 competing teams , 19 of which scored higher than the previously published baseline .", "entities": []}, {"text": "The best performing system achieved a FEVER score of 64.21 % .", "entities": [[6, 7, "DatasetName", "FEVER"]]}, {"text": "In this paper , we present the results of the shared task and a summary of the systems , highlighting commonalities and innovations among participating systems .", "entities": []}, {"text": "1 Introduction Information extraction is a well studied domain and the outputs of such systems enable many natural language technologies such as question answering and text summarization .", "entities": [[22, 24, "TaskName", "question answering"], [25, 27, "TaskName", "text summarization"]]}, {"text": "However , since information sources can contain errors , there exists an additional need to verify whether the information is correct .", "entities": []}, {"text": "For this purpose , we hosted the \ufb01rst Fact Extraction and VERi\ufb01cation ( FEVER ) shared task to raise interest in and awareness of the task of automatic information veri\ufb01cation a research domain that is orthogonal to information extraction .", "entities": [[13, 14, "DatasetName", "FEVER"]]}, {"text": "This shared task required participants to develop systems to predict the veracity of human - generated textual claims against textual evidence to be retrieved from Wikipedia .", "entities": []}, {"text": "We constructed a purpose - built dataset for this task ( Thorne et al . , 2018 ) that contains 185,445 human - generated claims , manually veri\ufb01ed against the introductory sections of Wikipedia pages and labeled as S UPPORTED , REFUTED or NOTENOUGH INFO .", "entities": []}, {"text": "The claims were generated by paraphrasing facts from Wikipedia and mutating them in a variety of ways , some of which were meaning - altering .", "entities": []}, {"text": "For each claim , and withoutthe knowledge of where the claim was generated from , annotators selected evidence in the form of sentences from Wikipedia to justify the labeling of the claim .", "entities": []}, {"text": "The systems participating in the FEVER shared task were required to label claims with the correct class and also return the sentence(s ) forming the necessary evidence for the assigned label .", "entities": [[5, 6, "DatasetName", "FEVER"]]}, {"text": "Performing well at this task requires both identifying relevant evidence and reasoning correctly with respect to the claim .", "entities": []}, {"text": "A key difference between this task and other textual entailment and natural language inference tasks ( Dagan et al . , 2009 ; Bowman et al . , 2015 ) is the need to identify the evidence from a large textual corpus .", "entities": [[11, 14, "TaskName", "natural language inference"]]}, {"text": "Furthermore , in comparison to large - scale question answering tasks ( Chen et al . , 2017 ) , systems must reason about information that is not present in the claim .", "entities": [[8, 10, "TaskName", "question answering"]]}, {"text": "We hope that research in these \ufb01elds will be stimulated by the challenges present in FEVER .", "entities": [[15, 16, "DatasetName", "FEVER"]]}, {"text": "One of the limitations of using human annotators to identify correct evidence when constructing the dataset was the trade - off between annotation velocity and evidence recall ( Thorne et al . , 2018 ) .", "entities": []}, {"text": "Evidence selected by annotators was often incomplete .", "entities": []}, {"text": "As part of the FEVER shared task , any evidence retrieved by participating systems that was not contained in the original dataset was annotated and used to augment the evidence in the test set .", "entities": [[4, 5, "DatasetName", "FEVER"]]}, {"text": "In this paper , we present a short description of the task and dataset , present a summary of the submissions and the leader board , and highlight future research directions .", "entities": [[12, 14, "DatasetName", "and dataset"]]}, {"text": "2 Task Description Candidate systems for the FEVER shared task", "entities": [[7, 8, "DatasetName", "FEVER"]]}, {"text": "were given a sentence of unknown veracity called a claim .", "entities": []}, {"text": "The systems must identify suitable evidence from Wikipedia at the sentence level and", "entities": []}, {"text": "Claim : The Rodney King riots took place in the most populous county in the USA .", "entities": []}, {"text": "[ wiki / Los Angeles Riots ] The 1992 Los Angeles riots , also known as the Rodney King riots were a series of riots , lootings , arsons , and civil disturbances that occurred in Los Angeles County , California in April and May 1992 .", "entities": []}, {"text": "[ wiki / Los Angeles County ] Los Angeles County , of\ufb01cially the County of Los Angeles , is the most populous county in the USA .", "entities": []}, {"text": "Verdict :", "entities": []}, {"text": "Supported Figure 1 : Example claim from the FEVER shared task : a manually veri\ufb01ed claim that requires evidence from multiple Wikipedia pages .", "entities": [[8, 9, "DatasetName", "FEVER"]]}, {"text": "assign a label whether , given the evidence , the claim is S UPPORTED , REFUTED or whether there is N OTENOUGH INFO in Wikipedia to reach a conclusion .", "entities": []}, {"text": "In 16.82 % of cases , claims required the combination of more than one sentence as supporting or refuting evidence .", "entities": []}, {"text": "An example is provided in Figure 1 . 2.1 Data Training and development data was released through the FEVER website.1We used the reserved portion of the data presented in Thorne et", "entities": [[18, 19, "DatasetName", "FEVER"]]}, {"text": "al .", "entities": []}, {"text": "( 2018 ) as a blind test set .", "entities": []}, {"text": "Disjoint training , development and test splits of the dataset were generated by splitting the dataset by the page used to generate the claim .", "entities": []}, {"text": "The development and test datasets were balanced by randomly discarding claims from the more populous classes .", "entities": []}, {"text": "Split SUPPORTED REFUTED NEI Training 80,035 29,775 35,639 Dev 6,666 6,666 6,666 Test 6,666 6,666 6,666 Table 1 : Dataset split sizes for S UPPORTED , REFUTED and N OTENOUGH INFO ( NEI ) classes 1http://fever.ai2.2", "entities": []}, {"text": "Scoring Metric We used the scoring metric described in Thorne et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) to evaluate the submissions .", "entities": []}, {"text": "The FEVER shared task requires submission of evidence to justify the labeling of a claim .", "entities": [[1, 2, "DatasetName", "FEVER"]]}, {"text": "The training , development and test data splits contain multiple sets of evidence for each claim , each set being a minimal set of sentences that fully support or refute it .", "entities": []}, {"text": "The primary scoring metric for the task is the label accuracy conditioned on providing at least one complete set of evidence , referred to as the FEVER score .", "entities": [[10, 11, "MetricName", "accuracy"], [26, 27, "DatasetName", "FEVER"]]}, {"text": "Sentences labeled ( correctly ) as NOTENOUGH INFO do not require evidence .", "entities": []}, {"text": "Correctly labeled claims with no or only partial evidence received no points for the FEVER score .", "entities": [[14, 15, "DatasetName", "FEVER"]]}, {"text": "Where multiple sets of evidence was annotated in the data , only one set was required for the claim to be considered correct for the FEVER score .", "entities": [[25, 26, "DatasetName", "FEVER"]]}, {"text": "Since the development and evaluation data splits are balanced , random baseline label accuracy ignoring the requirement for evidence is 33:33 % .", "entities": [[13, 14, "MetricName", "accuracy"]]}, {"text": "This performance level can also be achieved for the FEVER score by predicting NOTENOUGH INFO for every claim .", "entities": [[9, 10, "DatasetName", "FEVER"]]}, {"text": "However , as the FEVER score requires evidence for S UPPORTED and R EFUTED claims , a random baseline is expected to score lower on this metric .", "entities": [[4, 5, "DatasetName", "FEVER"]]}, {"text": "We provide an open - source release of the scoring software.2Beyond the FEVER score , it computes precision , recall , F1 , and label accuracy to provide diagnostic information .", "entities": [[12, 13, "DatasetName", "FEVER"], [21, 22, "MetricName", "F1"], [25, 26, "MetricName", "accuracy"]]}, {"text": "The recall point is awarded , as is the case for the FEVER score , only by providing a complete set of evidence for the claim .", "entities": [[12, 13, "DatasetName", "FEVER"]]}, {"text": "2.3 Submissions The FEVER shared task was hosted as a competition on Codalab3which allowed submissions to be scored against the blind test set without the need to publish the correct labels .", "entities": [[3, 4, "DatasetName", "FEVER"]]}, {"text": "The scoring system was open from 24th to 27th July 2018 .", "entities": []}, {"text": "Participants were limited to 10 submissions ( max . 2 per day).4 2The scorer , test cases and examples can be found in the following GitHub repository https://github.com/ sheffieldnlp / fever - scorer 3https://competitions.codalab.org/", "entities": []}, {"text": "competitions/18814 4An extra half - day was given as an artifact of the competition closing at midnight paci\ufb01c time .", "entities": []}, {"text": "Rank Team NameEvidence ( % ) Label Accuracy ( % ) FEVER Score ( % ) Precision Recall F1 1 UNC - NLP 42.27 70.91 52.96 68.21 64.21 2 UCL Machine Reading Group 22.16 82.84 34.97 67.62 62.52 3 Athene UKP TU Darmstadt 23.61 85.19 36.97 65.46 61.58 4 Papelo 92.18 50.02 64.85 61.08 57.36 5 SWEEPer 18.48 75.39 29.69 59.72 49.94 6 Columbia NLP 23.02 75.89 35.33 57.45 49.06 7 Ohio State University 77.23 47.12 58.53 50.12 43.42 8 GESIS Cologne 12.09 51.69 19.60 54.15 40.77 9", "entities": [[7, 11, "MetricName", "Accuracy ( % )"], [11, 12, "DatasetName", "FEVER"], [12, 13, "MetricName", "Score"], [16, 17, "MetricName", "Precision"], [17, 18, "MetricName", "Recall"], [18, 19, "MetricName", "F1"], [40, 41, "DatasetName", "UKP"]]}, {"text": "FujiXerox 11.37 29.99 16.49 47.13 38.81 10 withdrawn 46.60 51.94 49.12 51.25 38.59 11 Uni - DuE Student Team 50.65 36.02 42.10 50.02 38.50 12 Directed Acyclic Graph 51.91 36.36 42.77 51.36 38.33 13 withdrawn 12.90 54.58 20.87 53.97 37.13 14 Py.ro 21.15 49.38 29.62 43.48 36.58 15 SIRIUS - LTG - UIO 19.19 70.82 30.19 48.87 36.55 16 withdrawn 0.00 0.01 0.00 33.45 30.20 17 BUPT - NLPer 45.18 35.45 39.73 45.37 29.22 18 withdrawn 23.75 86.07 37.22 33.33 28.67 19 withdrawn 7.69 32.11 12.41 50.80 28.40 20 FEVER Baseline 11.28 47.87 18.26 48.84 27.45 21 UMBC - FEVER 49.01 29.66 36.95 44.89 23.76 22 withdrawn 26.81 12.08 16.65 57.32 22.89 23 withdrawn 26.33 12.20 16.68 55.42 21.71 24 University of Arizona 11.28 47.87 18.26 36.94 19.00 Table 2 : Results on the test dataset .", "entities": [[89, 90, "DatasetName", "FEVER"], [99, 100, "DatasetName", "FEVER"]]}, {"text": "3 Participants and Results 86 submissions ( excluding the baseline ) were made to Codalab for scoring on the blind test set .", "entities": []}, {"text": "There were 23 different teams which participated in the task ( presented in Table 2 ) .", "entities": []}, {"text": "19 of these teams scored higher than the baseline presented in Thorne et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "All participating teams were invited to submit a description of their systems .", "entities": []}, {"text": "We received 15 descriptions at the time of writing and the remaining are considered as withdrawn .", "entities": []}, {"text": "The system with the highest score was submitted by UNC - NLP ( FEVER score : 64:21 % ) .", "entities": [[13, 14, "DatasetName", "FEVER"]]}, {"text": "Most participants followed a similar pipeline structure to the baseline model .", "entities": []}, {"text": "This consisted of three stages : document selection , sentence selection and natural language inference .", "entities": [[12, 15, "TaskName", "natural language inference"]]}, {"text": "However , some teams constructed models to jointly select sentences and perform inference in a single pipeline step , while others added an additional step , discarding inconsistent evidence after performing inference .", "entities": []}, {"text": "Based on the team - submitted system description summaries ( Appendix A ) , in the following section we present an overview of which models and techniques were applied to the task and their relative performance .", "entities": []}, {"text": "4 Analysis 4.1 Document Selection A large number of teams report a multi - step approach to document selection .", "entities": []}, {"text": "The majority of submissions report extracting some combination of Named Entities , Noun Phrases and Capitalized Expressions from the claim .", "entities": []}, {"text": "These were used either as inputs to a search API ( i.e. Wikipedia Search or Google Search ) , search server ( e.g. Lucene5or Solr6 ) or as keywords for matching against Wikipedia page titles or article bodies .", "entities": [[15, 16, "DatasetName", "Google"]]}, {"text": "BUPT - NLPer report using S- MART for entity linking ( Yang and Chang , 2015 ) and the highest scor5http://lucene.apache.org/ 6http://lucene.apache.org/solr/", "entities": [[8, 10, "TaskName", "entity linking"]]}, {"text": "ing team , UNC - NLP , report using page viewership statistics to rank the candidate pages .", "entities": []}, {"text": "GESIS Cologne report directly selecting sentences using the Solr search , bypassing the need to perform document retrieval as a separate step .", "entities": []}, {"text": "The team which scored highest on evidence precision and evidence F1 was Papelo ( precision = 92:18 %", "entities": [[10, 11, "MetricName", "F1"]]}, {"text": "andF1=64:85 % ) who report using a combination of TF - IDF for document retrieval and string matching using named entities and capitalized expressions .", "entities": []}, {"text": "The teams which scored highest on evidence recall were Athene UKP TU Darmstadt ( recall = 85:19 % ) and UCL Machine Reading Group ( recall = 82:84%)7 8Athene report extracting nounphrases from the claim and using these to query the Wikipedia search API .", "entities": [[10, 11, "DatasetName", "UKP"]]}, {"text": "A similar approach was used by Columbia NLP who query the Wikipedia search API using named entities extracted from the claim as a query string , all the text before the \ufb01rst lowercase verb phrase as a query string and also combine this result with Wikipedia pages identi\ufb01ed with Google search using the entire claim .", "entities": [[49, 50, "DatasetName", "Google"]]}, {"text": "UCL Machine Reading Group report a document retrieval approach that identi\ufb01es Wikipedia article titles within the claim and ranks the results using features such as capitalization , sentence position and token match .", "entities": []}, {"text": "4.2 Sentence Selection There were three common approaches to sentence selection : keyword matching , supervised classi\ufb01cation and sentence similarity scoring .", "entities": []}, {"text": "Ohio State and UCL Machine Reading Group report using keyword matching techniques : matching either named entities or tokens appearing in both the claim and article body .", "entities": []}, {"text": "UNC - NLP , Athene UKP TU Darmstadt and Columbia NLP modeled the task as supervised binary classi\ufb01cation , using architectures such as Enhanced LSTM ( Chen et al . , 2016 ) , Decomposable Attention ( Parikh et al . , 2016 ) or similar to them .", "entities": [[5, 6, "DatasetName", "UKP"], [24, 25, "MethodName", "LSTM"]]}, {"text": "SWEEPer and BUPTNLPer present jointly learned models for sentence selection and natural language inference .", "entities": [[11, 14, "TaskName", "natural language inference"]]}, {"text": "Other teams report scoring based on sentence similarity using Word Mover \u2019s Distance ( Kusner et al . , 7The withdrawn team that ranked 18th on F1score had the highest recall : 86:07 % .", "entities": []}, {"text": "A system description was not submitted by this team preventing us from including it in our analysis .", "entities": []}, {"text": "8The scores for precision , recall and F1were computed independent of the label accuracy and FEVER Score.2015 ) or cosine similarity over smooth inverse frequency weightings ( Arora et al . , 2017 ) , ELMo embeddings ( Peters et al . , 2018 ) and TF - IDF ( Salton et al . , 1983 ) .", "entities": [[13, 14, "MetricName", "accuracy"], [15, 16, "DatasetName", "FEVER"], [35, 36, "MethodName", "ELMo"]]}, {"text": "UCL Machine Reading Group and Directed Acyclic Graph report an additional aggregation stage after the classi\ufb01cation stage in the pipeline where evidence that is inconsistent is discarded .", "entities": []}, {"text": "4.3 Natural Language Inference NLI was modeled as supervised classi\ufb01cation in all reported submissions .", "entities": [[1, 4, "TaskName", "Natural Language Inference"]]}, {"text": "We compare and discuss the approaches for combining the evidence sentences together with the claim , sentence representations and training schemes .", "entities": []}, {"text": "While many different approaches were used for sentence pair classi\ufb01cation , e.g. Enhanced LSTM ( Chen et al . , 2016 ) , Decomposable Attention ( Parikh et al . , 2016 ) , Transformer Model ( Radford and Salimans , 2018 ) , Random Forests ( Svetnik et al . , 2003 ) and ensembles thereof , these are not speci\ufb01c to the task and it is dif\ufb01cult to assess their impact due to the differences in the processing preceding this stage .", "entities": [[13, 14, "MethodName", "LSTM"], [34, 35, "MethodName", "Transformer"]]}, {"text": "Evidence Combination : UNC - NLP ( the highest scoring team ) concatenate the evidence sentences into a single string for classi\ufb01cation ; UCL Machine Reading Group classify each evidenceclaim pair individually and aggregate the results using a simple multilayer perceptron ( MLP ) ; Columbia NLP perform majority voting ; and \ufb01nally , Athene - UKP TU Darmstadt encode each evidence - claim pair individually using an Enhanced LSTM , pool the resulting vectors and use an MLP for classi\ufb01cation .", "entities": [[42, 43, "DatasetName", "MLP"], [56, 57, "DatasetName", "UKP"], [69, 70, "MethodName", "LSTM"], [78, 79, "DatasetName", "MLP"]]}, {"text": "Sentence Representation : University of Arizona explore using non - lexical features for predicting entailment , considering the proportion of negated verbs , presence of antonyms and noun overlap .", "entities": []}, {"text": "Columbia NLP learn universal sentence representations ( Conneau et al . , 2017 ) .", "entities": []}, {"text": "UNC - NLP include an additional token - level feature the sentence similarity score from the sentence selection module .", "entities": []}, {"text": "Both Ohio State and UNC - NLP report alternative token encodings : UNC - NLP report using ELMo ( Peters et al . , 2018 ) and WordNet ( Miller , 1995 ) and Ohio State report using vector representations of named entities .", "entities": [[17, 18, "MethodName", "ELMo"]]}, {"text": "FujiXerox report representing sentences using D EISTE(Yin et", "entities": []}, {"text": "al . , 2018 ) .", "entities": []}, {"text": "Training : BUPT - NLPer and SWEEPer model the evidence selection and claim veri\ufb01cation using a multi - task learning model under the hypothesis that information from each task supplements the other .", "entities": [[16, 20, "TaskName", "multi - task learning"]]}, {"text": "SWEEPer also report parameter tuning using reinforcement learning .", "entities": []}, {"text": "5 Additional Annotation As mentioned in the introduction , to increase the evidence coverage in the test set , the evidence submitted by participating systems was annotated by shared task volunteers after the competition ended .", "entities": []}, {"text": "There were 18,846 claims where at least one system returned an incorrect label , according to the FEVER score , i.e. taking evidence into account .", "entities": [[17, 18, "DatasetName", "FEVER"]]}, {"text": "These claims were sampled for annotation with a probability proportional to the number of systems which labeled each of them incorrectly .", "entities": []}, {"text": "The evidence sentences returned by each system for each claim was sampled further with a probability proportional to the system \u2019s FEVER score in an attempt to focus annotation efforts towards higher quality candidate evidence .", "entities": [[21, 22, "DatasetName", "FEVER"]]}, {"text": "These extra annotations were performed by volunteers from the teams participating in the shared task and three of the organizers .", "entities": []}, {"text": "Annotators were asked to label whether the retrieved evidence sentences supported or refuted the claim at question , and to highlight which sentences ( if any ) , either individually or as a group , can be used as evidence .", "entities": []}, {"text": "We retained the annotation guidelines from Thorne et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) ( see Sections A.7.1 , A.7.3 and A.8 from that paper for more details ) .", "entities": []}, {"text": "At the time of writing , 1,003 annotations were collected for 618 claims .", "entities": []}, {"text": "This identi\ufb01ed 3 claims that were incorrectly labeled as S UPPORTED or REFUTED and 87 claims that were originally labeled as N OTENOUGH INFO that should be relabeled as S UPPORTED or R EFUTED through the introduction of new evidence ( 44 and 43 claims respectively ) .", "entities": []}, {"text": "308 new evidence sets were identi\ufb01ed for claims originally labeled as S UPPORTED or R EFUTED , consisting of 280 single sentences and 28 sets of 2 or more sentences .", "entities": []}, {"text": "Further annotation is in progress and the data collected as well as the \ufb01nal results will be made public at the workshop .", "entities": []}, {"text": "6 Conclusions The \ufb01rst Fact Extraction and VERi\ufb01cation shared task attracted submissions from 86 submissionsfrom 23 teams .", "entities": []}, {"text": "19 of these teams exceeded the score of the baseline presented in Thorne et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "For the teams which provided a system description , we highlighted the approaches , identifying commonalities and features that could be further explored .", "entities": []}, {"text": "Future work will address limitations in humanannotated evidence and explore other subtasks needed to predict the veracity of information extracted from untrusted sources .", "entities": []}, {"text": "Acknowledgements The work reported was partly conducted while James Thorne was at Amazon Research Cambridge .", "entities": [[14, 15, "DatasetName", "Cambridge"]]}, {"text": "Andreas Vlachos is supported by the EU H2020 SUMMA project ( grant agreement number 688139 ) .", "entities": []}, {"text": "References Sanjeev Arora , Yingyu Liang , and Tengyu Ma . 2017 .", "entities": []}, {"text": "A simple but tough to beat baseline for sentence embeddings .", "entities": [[8, 10, "TaskName", "sentence embeddings"]]}, {"text": "In Iclr , pages 1\u201314 .", "entities": []}, {"text": "Sean Baird , Doug Sibley , and Yuxi Pan . 2017 .", "entities": []}, {"text": "Cisco \u2019s Talos Intelligence Group Blog Talos Targets Disinformation with Fake News Challenge Victory .", "entities": []}, {"text": "Samuel R. Bowman , Gabor Angeli , Christopher Potts , and Christopher D. Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "A large annotated corpus for learning natural language inference .", "entities": [[6, 9, "TaskName", "natural language inference"]]}, {"text": "InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing .", "entities": []}, {"text": "Danqi Chen , Adam Fisch , Jason Weston , and Antoine Bordes . 2017 .", "entities": [[3, 4, "MethodName", "Adam"]]}, {"text": "Reading Wikipedia to Answer OpenDomain Questions .", "entities": []}, {"text": "Proc .", "entities": []}, {"text": "of ACL\u201917 .", "entities": []}, {"text": "Qian Chen , Xiaodan Zhu , Zhenhua Ling , Si Wei , Hui Jiang , and Diana Inkpen .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Enhanced LSTM for Natural Language Inference .", "entities": [[1, 2, "MethodName", "LSTM"], [3, 6, "TaskName", "Natural Language Inference"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , pages 1657\u20131668 , Vancouver , Canada .", "entities": []}, {"text": "Alexis Conneau , Douwe Kiela , Holger Schwenk , Loic Barrault , and Antoine Bordes .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data .", "entities": [[7, 10, "TaskName", "Natural Language Inference"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 670\u2013680 , Copenhagen , Denmark .", "entities": []}, {"text": "Ido Dagan , Bill Dolan , Bernardo Magnini , and Dan Roth . 2009 .", "entities": []}, {"text": "Recognizing textual entailment : Rational , evaluation and approaches .", "entities": []}, {"text": "Natural Language Engineering , 15(4):i \u2013 xvii .", "entities": []}, {"text": "Tushar Khot , Ashish Sabharwal , and Peter Clark .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "SCITAIL :", "entities": [[0, 1, "DatasetName", "SCITAIL"]]}, {"text": "A Textual Entailment Dataset from Science Question Answering .", "entities": [[5, 8, "TaskName", "Science Question Answering"]]}, {"text": "Aaai .", "entities": []}, {"text": "Matt J Kusner , Yu Sun , Nicholas I Kolkin , and Kilian Q Weinberger . 2015 .", "entities": []}, {"text": "From Word Embeddings To Document Distances .", "entities": [[1, 3, "TaskName", "Word Embeddings"]]}, {"text": "Proceedings of The 32nd International Conference on Machine Learning , 37:957 \u2013 966 .", "entities": []}, {"text": "George a. Miller .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "WordNet : a lexical database for English .", "entities": []}, {"text": "Communications of the ACM , 38(11):39\u201341 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Ankur P. Parikh , Oscar T \u00a8ackstr \u00a8om , Dipanjan Das , and Jakob Uszkoreit .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A Decomposable Attention Model for Natural Language Inference .", "entities": [[5, 8, "TaskName", "Natural Language Inference"]]}, {"text": "pages 2249\u20132255 .", "entities": []}, {"text": "Matthew E Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Deep contextualized word representations .", "entities": []}, {"text": "In Proceedings of NAACL - HLT , pages 2227\u20132237 , New Orleans , Louisiana .", "entities": []}, {"text": "Alec Radford and Tim Salimans .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Improving Language Understanding by Generative PreTraining .", "entities": []}, {"text": "arXiv , pages 1\u201312 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Gerard Salton , Edward A. Fox , and Harry Wu . 1983 .", "entities": []}, {"text": "Extended boolean information retrieval .", "entities": [[2, 4, "TaskName", "information retrieval"]]}, {"text": "Commun .", "entities": []}, {"text": "ACM , 26(11):1022\u20131036 .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Vladimir Svetnik , Andy Liaw , Christopher Tong , J Christopher Culberson , Robert P Sheridan , and Bradley P Feuston .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Random forest : a classi\ufb01cation and regression tool for compound classi\ufb01cation and qsar modeling .", "entities": []}, {"text": "Journal of chemical information and computer sciences , 43(6):1947\u20131958 .", "entities": []}, {"text": "James Thorne , Andreas Vlachos , Christos Christodoulopoulos , and Arpit Mittal . 2018 .", "entities": []}, {"text": "FEVER : a large - scale dataset for fact extraction and veri\ufb01cation .", "entities": [[0, 1, "DatasetName", "FEVER"]]}, {"text": "In NAACL - HLT .", "entities": []}, {"text": "Yi Yang and Ming - Wei Chang .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "S - MART : Novel Tree - based Structured Learning Algorithms Applied to Tweet Entity Linking .", "entities": [[14, 16, "TaskName", "Entity Linking"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing , pages 504\u2013513 , Beijing , China .", "entities": []}, {"text": "Wenpeng Yin , Hinrich Sch \u00a8utze , and Dan Roth .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "End - Task Oriented Textual Entailment via Deep Explorations of Inter - Sentence Interactions .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Short Papers ) , pages 540\u2013545 , Melbourne , Australia .", "entities": []}, {"text": "A Short System Descriptions Submitted by Participants A.1 UNC - NLP Our system is composed of three connected components namely , a document retriever , a sentence selector , and a claim veri\ufb01er .", "entities": []}, {"text": "The document retriever chooses candidate wiki - documents viamatching of keywords between the claims and the wiki - document titles , also using external pageview frequency statistics for wiki - page ranking .", "entities": []}, {"text": "The sentence selector is a sequence - matching neural network that conducts further \ufb01ne - grained selection of evidential sentences by comparing the given claim with all the sentences in the candidate documents .", "entities": []}, {"text": "This module is trained as a binary classi\ufb01er given the ground truth evidence as positive examples and all the other sentences as negative examples with an annealing sampling strategy .", "entities": []}, {"text": "Finally , the claim veri\ufb01er is a state - of - theart 3 - way neural natural language inference ( NLI ) classi\ufb01er ( with WordNet and ELMo features ) that takes the concatenation of all selected evidence as the premise and the claim as the hypothesis , and labels each such evidences - claim pair as one of \u2018 support \u2019 , \u2018 refute \u2019 , or \u2018 not enough info \u2019 .", "entities": [[16, 19, "TaskName", "natural language inference"], [27, 28, "MethodName", "ELMo"]]}, {"text": "To improve the claim veri\ufb01er via better awareness of the selected evidence , we further combine the last two modules by feeding the sentence similarity score ( produced by the sentence selector ) as an additional token - level feature to the claim veri\ufb01er .", "entities": []}, {"text": "A.2 UCL Machine Reading Group The UCLMR system is a four stage model consisting of document retrieval , sentence retrieval , natural language inference and aggregation .", "entities": [[21, 24, "TaskName", "natural language inference"]]}, {"text": "Document retrieval attempts to \ufb01nd the name of a Wikipedia article in the claim , and then ranks each article based on capitalization , sentence position and token match features .", "entities": []}, {"text": "A set of sentences are then retrieved from the top ranked articles , based on token matches with the claim and position in the article .", "entities": []}, {"text": "A natural language inference model is then applied to each of these sentences paired with the claim , giving a prediction for each potential evidence .", "entities": [[1, 4, "TaskName", "natural language inference"]]}, {"text": "These predictions are then aggregated using a simple MLP , and the sentences are reranked to keep only the evidence consistent with the \ufb01nal prediction .", "entities": [[8, 9, "DatasetName", "MLP"]]}, {"text": "A.3 Athene UKP TU Darmstadt Document retrieval We applied the constituency parser from AllenNLP to extract noun phrases in the claim and made use of Wikipedia API to search corresponding pages for each noun phrase .", "entities": [[2, 3, "DatasetName", "UKP"]]}, {"text": "So as to remove noisy pages from the results , we have stemmed the words of their titles and the claim , and then discarded pages whose stemmed words of the title are not completely included in the set of stemmed words in the claim .", "entities": []}, {"text": "Sentence selection The hinge loss with negative sampling is applied to train the enhanced LSTM .", "entities": [[4, 5, "MetricName", "loss"], [14, 15, "MethodName", "LSTM"]]}, {"text": "For a given positive claim - evidence pair , negative samples are generated by randomly sampling sentences from the retrieved documents .", "entities": []}, {"text": "RTE We combine the 5 sentences from sentence selection and the claim to form 5 pairs and then apply enhanced LSTM for each pair .", "entities": [[0, 1, "DatasetName", "RTE"], [20, 21, "MethodName", "LSTM"]]}, {"text": "We combine the resulting representations using average and max pooling and feed the resulting vector through an MLP for classi\ufb01cation .", "entities": [[8, 10, "MethodName", "max pooling"], [17, 18, "DatasetName", "MLP"]]}, {"text": "A.4 Papelo We develop a system for the FEVER fact extraction and veri\ufb01cation challenge that uses a high precision entailment classi\ufb01er based on transformer networks pretrained with language modeling ( Radford and Salimans , 2018 ) , to classify a broad set of potential evidence .", "entities": [[8, 9, "DatasetName", "FEVER"]]}, {"text": "The precision of the entailment classi\ufb01er allows us to enhance recall by considering every statement from several articles to decide upon each claim .", "entities": []}, {"text": "We include not only the articles best matching the claim text by TFIDF score , but read additional articles whose titles match named entities and capitalized expressions occurring in the claim text .", "entities": []}, {"text": "The entailment module evaluates potential evidence one statement at a time , together with the title of the page the evidence came from ( providing a hint about possible pronoun antecedents ) .", "entities": []}, {"text": "In preliminary evaluation , the system achieved 57.36 % FEVER score , 61.08 % label accuracy , and 64.85 % evidence F1 on the FEVER shared task test set .", "entities": [[9, 10, "DatasetName", "FEVER"], [15, 16, "MetricName", "accuracy"], [21, 22, "MetricName", "F1"], [24, 25, "DatasetName", "FEVER"]]}, {"text": "A.5 SWEEPer Our model for fact checking and veri\ufb01cation consists of two stages : 1 ) identifying relevant documents using lexical and syntactic features from the claim and \ufb01rst two sentences in the Wikipedia article and 2 ) jointly modeling sentence extraction and veri\ufb01cation .", "entities": [[5, 7, "TaskName", "fact checking"]]}, {"text": "As the tasks of fact checking and \ufb01nding evidence are dependent on each other , an ideal model would consider the veracity of the claim when \ufb01nding evidence and also \ufb01nd only the evidence that supports / refutes the position of the claim .", "entities": [[4, 6, "TaskName", "fact checking"]]}, {"text": "We thus jointly model the second stage by using a pointer network with the claim and evidence sentence represented using the ESIM module .", "entities": [[10, 12, "MethodName", "pointer network"], [21, 22, "MethodName", "ESIM"]]}, {"text": "For stage 2 , we \ufb01rst train both components using multi - task learning over a larger memory of extracted sentences , then tune parameters using re - inforcement learning to \ufb01rst extract sentences and predict the relation over only the extracted sentences .", "entities": [[10, 14, "TaskName", "multi - task learning"]]}, {"text": "A.6 Columbia NLP For document retrieval we use three components : 1 ) use google custom search API with the claim as a query and return the top 2 Wikipedia pages ; 2 ) extract all name entities from the claims and use Wikipedia python API to return a page for each name entity and 3 ) ; use the pre\ufb01x of the claim until the \ufb01rst lowercase verb phrase , and use Wikipedia API to return the top page .", "entities": []}, {"text": "For Sentence Selection we used the modi\ufb01ed document retrieval component of DrQA to get the top 5 sentences and then further extracted the top 3 sentences using cosine similarity between vectors obtained from Elmo ( Peters et al . , 2018 ) sentence embeddings of the claim and the evidence .", "entities": [[33, 34, "MethodName", "Elmo"], [42, 44, "TaskName", "sentence embeddings"]]}, {"text": "For RTE we used the same model as outlined by ( Conneau et al . , 2017 ) in their work for recognizing textual entailment and learning universal sentence representations .", "entities": [[1, 2, "DatasetName", "RTE"]]}, {"text": "If at least one out of the three evidences SUPPORTS / REFUTES the claim and the rest are NOT ENOUGH INFO , then we treat the label as SUPPORTS / REFUTES , else we return the majority among three classes as the predicted label .", "entities": []}, {"text": "A.7 Ohio State University Our system was developed using a heuristicsbased approach for evidence extraction and a modi\ufb01ed version of the inference model by Parikh et al .", "entities": []}, {"text": "( 2016 ) for classi\ufb01cation into refute , support , or not enough info .", "entities": []}, {"text": "Our process is broken down into three distinct phases .", "entities": []}, {"text": "First , potentially relevant documents are gathered based on key words / phrases in the claim that appear in the wiki dump .", "entities": []}, {"text": "Second , any possible evidence sentences inside those documents are extracted by breaking down the claim into named entities plus nouns and \ufb01nding any sentences which match those entities , while allowing for various exceptions and additional potential criteria to increase recall .", "entities": []}, {"text": "Finally , every sentences is classi\ufb01ed into one of the three categories by the inference tool , after additional vectors are added based on named entity types .", "entities": []}, {"text": "NEI sentences are discarded and the highest scored label of the remaining sentences is assigned to the claim .", "entities": []}, {"text": "A.8 GESIS Cologne", "entities": []}, {"text": "In our approach we used a sentence wise approach in all components .", "entities": []}, {"text": "To \ufb01nd the sentences we set up a Solr database and indexed every sentence including information about the article where the sentence is from .", "entities": []}, {"text": "We created queries based on the named entities and noun chunks of the claims .", "entities": []}, {"text": "For the entailment task we used a Decomposable Attention Model similar to the one used in the baseline approach .", "entities": []}, {"text": "But instead of comparing the claim with all top 5 sentences at once we treat every sentence separately .", "entities": []}, {"text": "The results for the top 5 sentence where then joined with an ensemble learner incl .", "entities": []}, {"text": "the rank of the sentence retriever of the wikipedia sentences .", "entities": []}, {"text": "A.9 FujiXerox", "entities": []}, {"text": "We prepared a pipeline system which composes of document selection , a sentence retrieval , and a recognizing textual entailment ( RTE ) components .", "entities": [[21, 22, "DatasetName", "RTE"]]}, {"text": "A simple entity linking approach with text match is used as the document selection component , this component identi\ufb01es relevant documents for a given claim by using mentioned entities as clues .", "entities": [[2, 4, "TaskName", "entity linking"]]}, {"text": "The sentence retrieval component selects relevant sentences as candidate evidence from the documents based on TF - IDF .", "entities": []}, {"text": "Finally , the RTE component selects evidence sentences by ranking the sentences and classi\ufb01es the claim as SUPPORTED , REFUTED , or NOTENOUGHINFO simultaneously .", "entities": [[3, 4, "DatasetName", "RTE"]]}, {"text": "As the RTE component , we adopted DEISTE ( Deep Explorations of InterSentence interactions for Textual Entailment )", "entities": [[2, 3, "DatasetName", "RTE"]]}, {"text": "( Yin et al . , 2018 ) model that is the state - of - the - art in RTE task .", "entities": [[20, 21, "DatasetName", "RTE"]]}, {"text": "A.10 Uni - DuE Student Team We generate a Lucene index from the provided Wikipedia dump .", "entities": []}, {"text": "Then we use two neural networks , one for named entity recognition and the other for constituency parsing , and also the Stanford dependency parser to create the keywords used inside the Lucene queries .", "entities": [[9, 12, "TaskName", "named entity recognition"], [16, 18, "TaskName", "constituency parsing"]]}, {"text": "Depending on the amount of keywords found for each claim , we run multiple Lucene searches on the generated index to create a list of candidate sentences for each claim .", "entities": []}, {"text": "The resulting list of claim - candidate pairs is processed in three ways :", "entities": []}, {"text": "1 . We use the Standford POS - Tagger to generate POS - Tags for the claim and candidatesentences which are then used in a handcrafted scoring script to assign a score on a 0 to 15 scale .", "entities": [[34, 35, "DatasetName", "0"]]}, {"text": "2 . We run each pair through a modi\ufb01ed version of the Decomposable Attention network .", "entities": []}, {"text": "3 . We merge all candidate sentences per claim into one long piece of text and run the result paired with the claim through the same modi\ufb01ed Decomposable Attention network as in ( 2 . ) .", "entities": []}, {"text": "We then make the \ufb01nal prediction in a handcrafted script combining the results of the three previous steps .", "entities": []}, {"text": "A.11 Directed Acyclic Graph", "entities": []}, {"text": "In this paper , we describe the system we designed for the FEVER 2018 Shared Task .", "entities": [[12, 13, "DatasetName", "FEVER"]]}, {"text": "The aim of this task was to conceive a system that can not only automatically assess the veracity of a claim but also retrieve evidence supporting this assessment from Wikipedia .", "entities": []}, {"text": "In our approach , the Wikipedia documents whose Term Frequency - Inverse Document Frequency ( TFIDF ) vectors are most similar to the vector of the claim and those documents whose names are similar to the named entities ( NEs ) mentioned in the claim are identi\ufb01ed as the documents which might contain evidence .", "entities": []}, {"text": "The sentences in these documents are then supplied to a decomposable attention - based textual entailment recognition module .", "entities": []}, {"text": "This module calculates the probability of each sentence supporting the claim , contradicting the claim or not providing any relevant information .", "entities": []}, {"text": "Various features computed using these probabilities are \ufb01nally used by a Random Forest classi\ufb01er to determine the overall truthfulness of the claim .", "entities": []}, {"text": "The sentences which support this classi\ufb01cation are returned as evidence .", "entities": []}, {"text": "Our approach achieved a 42.77 % evidence F1 - score , a 51.36 % label accuracy and a 38.33 % FEVER score .", "entities": [[7, 10, "MetricName", "F1 - score"], [15, 16, "MetricName", "accuracy"], [20, 21, "DatasetName", "FEVER"]]}, {"text": "A.12 Py.ro We NER tagged the claim using SpaCy and used the Named Entities as candidate page IDs .", "entities": [[3, 4, "TaskName", "NER"]]}, {"text": "We resolved redirects by following the Wikipedia URL if an item was not in the preprocessed dump .", "entities": []}, {"text": "If a page could not be found , we fell back to the baseline document selection method .", "entities": []}, {"text": "The rest of the system was identical to the baseline system , al-", "entities": []}, {"text": "though we used our document retrieval system to generate alternative training data .", "entities": []}, {"text": "A.13 SIRIUS - LTG - UIO This article presents the SIRIUS - LTG system for the Fact Extraction and VERi\ufb01cation ( FEVER )", "entities": [[21, 22, "DatasetName", "FEVER"]]}, {"text": "Shared Task .", "entities": []}, {"text": "Our system consists of three components : 1 . Wikipedia Page Retrieval :", "entities": []}, {"text": "First we extract the entities in the claim , then we \ufb01nd potential Wikipedia URI candidates for each of the entities using the SPARQL query over DBpedia 2 . Sentence selection : We investigate various techniques i.e. SIF embedding , Word Mover \u2019s Distance ( WMD ) , Soft - Cosine Similarity , Cosine similarity with unigram TFIDF to rank sentences by their similarity to the claim .", "entities": [[26, 27, "DatasetName", "DBpedia"]]}, {"text": "3 . Textual Entailment : We compare three models for the claim classi\ufb01cation .", "entities": []}, {"text": "We apply a Decomposable Attention ( DA ) model ( Parikh et al . , 2016 ) , a Decomposed Graph Entailment ( DGE ) model ( Khot et al . , 2018 ) and a Gradient - Boosted Decision Trees ( TalosTree ) model ( Baird et al . , 2017 ) for this task .", "entities": []}, {"text": "The experiments show that the pipeline with simple Cosine Similarity using TFIDF in sentence selection along with DA as labeling model achieves better results in development and test dataset .", "entities": []}, {"text": "A.14 BUPT - NLPer", "entities": []}, {"text": "We introduce an end - to - end multi - task learning model for fact extraction and veri\ufb01cation with bidirection attention .", "entities": [[8, 12, "TaskName", "multi - task learning"]]}, {"text": "We propose a multi - task learning framework for the evidence extraction and claim veri\ufb01cation because these two tasks can be accomplished at the same time .", "entities": [[3, 7, "TaskName", "multi - task learning"]]}, {"text": "Each task provides supplementary information for the other and improves the results of another task .", "entities": []}, {"text": "For each claim , our system \ufb01rstly uses the entity linking tool S - MART to retrieve relative pages from Wikipedia .", "entities": [[9, 11, "TaskName", "entity linking"]]}, {"text": "Then , we use attention mechanisms in both directions , claim - to - page and pageto - claim , which provide complementary information to each other .", "entities": []}, {"text": "Aimed at the different task , our system obtains claim - aware sentence representation for evidence extraction and page - aware claim representation for claim veri\ufb01cation .", "entities": []}, {"text": "A.15 University of Arizona Many approaches to automatically recognizing entailment relations have employed classi\ufb01ers over hand engineered lexicalized features , or deep learning models that implicitly capture lexicalization through word embeddings .", "entities": [[29, 31, "TaskName", "word embeddings"]]}, {"text": "This reliance on lexicalization may complicate the adaptation of these tools between domains .", "entities": []}, {"text": "For example , such a system trained in the news domain may learn that a sentence like \u201c Palestinians recognize Texas as part of Mexico \u201d tends to be unsupported , a fact which has no value in say a scienti\ufb01c domain .", "entities": [[20, 21, "DatasetName", "Texas"]]}, {"text": "To mitigate this dependence on lexicalized information , in this paper we propose a model that reads two sentences , from any given domain , to determine entailment without using any lexicalized features .", "entities": []}, {"text": "Instead our model relies on features like proportion of negated verbs , antonyms , noun overlap etc .", "entities": []}, {"text": "In its current implementation , this model does not perform well on the FEVER dataset , due to two reasons .", "entities": [[13, 14, "DatasetName", "FEVER"]]}, {"text": "First , for the information retrieval part of the task we used the baseline system provided , since this was not the aim of our project .", "entities": [[4, 6, "TaskName", "information retrieval"]]}, {"text": "Second , this is work in progress and we still are in the process of identifying more features and gradually increasing the accuracy of our model .", "entities": [[22, 23, "MetricName", "accuracy"]]}, {"text": "In the end , we hope to build a generic end - to - end classi\ufb01er , which can be used in a domain outside the one in which it was trained , with no or minimal re - training .", "entities": []}, {"text": "A.16 UMBC - FEVER We describe the UMBC - FEVER system that we used in the 2018 FEVER shared task .", "entities": [[3, 4, "DatasetName", "FEVER"], [9, 10, "DatasetName", "FEVER"], [17, 18, "DatasetName", "FEVER"]]}, {"text": "The system employed a frame - based information retrieval approach to select Wikipedia sentences providing evidence and used a two - layer multilayer perceptron ( MLP ) for classi\ufb01cation .", "entities": [[7, 9, "TaskName", "information retrieval"], [25, 26, "DatasetName", "MLP"]]}, {"text": "Our submission achieved a score of 0.3695 on the Evidence F1 metric for retrieving relevant evidential sentences ( 10thout of 24 ) and a score of 0.2376 on the FEVER metric ( just below the baseline system ) .", "entities": [[10, 11, "MetricName", "F1"], [29, 30, "DatasetName", "FEVER"]]}]