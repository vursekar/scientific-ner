[{"text": "Proceedings of the 16thWorkshop on Innovative Use of NLP for Building Educational Applications , pages 223\u2013232 April 20 , 2021 \u00a9 2021 Association for Computational Linguistics223Using Linguistic Features to Predict the Response Process Complexity Associated with Answering Clinical MCQs Victoria Yaneva1Daniel Jurich1Le", "entities": []}, {"text": "An Ha2Peter Baldwin1 1National Board of Medical Examiners , Philadelphia , USA fvyaneva , djurich , pbaldwin g@nbme.org 2University of Wolverhamton , UK ha.l.a@wlv.ac.uk Abstract", "entities": []}, {"text": "This study examines the relationship between the linguistic characteristics of a test item and the complexity of the response process required to answer it correctly .", "entities": []}, {"text": "Using data from a large - scale medical licensing exam , clustering methods identi\ufb01ed items that were similar with respect to their relative dif\ufb01culty and relative response - time intensiveness to create low response process complexity andhigh response process complexity item classes .", "entities": []}, {"text": "Interpretable models were used to investigate the linguistic features that best differentiated between these classes from a descriptive and predictive framework .", "entities": []}, {"text": "Results suggest that nuanced features such as the number of ambiguous medical terms help explain response process complexity beyond super\ufb01cial item characteristics such as word count .", "entities": []}, {"text": "Yet , although linguistic features carry signal relevant to response process complexity , the classi\ufb01cation of individual items remains challenging .", "entities": []}, {"text": "1 Introduction The success of high - stakes exams , such as those used in licensing , certi\ufb01cation , and college admission , depends on the use of items ( test questions ) that meet stringent quality criteria .", "entities": []}, {"text": "To provide useful information about examinee ability , good items must be neither too dif\ufb01cult , nor too easy for the intended test - takers .", "entities": []}, {"text": "Furthermore , the timing demands of items should be such that different exam forms seen by different test - takers should entail similar times to complete .", "entities": []}, {"text": "Nevertheless , while an extreme dif\ufb01culty or mean response time can indicate that an item is not functioning correctly , within these extremes variability in dif\ufb01culty and item response time is expected .", "entities": []}, {"text": "For good items , it is hoped that this variability simply re\ufb02ects the breadth and depth of the relevant exam content .", "entities": []}, {"text": "The interaction between item dif\ufb01culty ( as measured by the proportion of examinees who respondcorrectly ) and time intensiveness ( as measured by the average time examinees spend answering ) can help quantify the complexity of the response process associated with an item .", "entities": []}, {"text": "This is valuable , since the more we know about the way examinees think about the problem presented in an item , the better we can evaluate exam validity .", "entities": []}, {"text": "Although easier items usually require less time than dif\ufb01cult items , the interaction between these two item properties is not strictly linear \u2013 examinees may spend very little time responding to certain dif\ufb01cult items and , likewise , examinees may spend a great deal of time on items that are relatively easy .", "entities": []}, {"text": "The idea of response process complexity is best illustrated with items that have similar dif\ufb01culty but different mean response times .", "entities": []}, {"text": "In such cases , one item may require the formation of a complex cognitive model of the problem and thus take a long time , while another item with a similar level of dif\ufb01culty may require factual knowledge that few examinees recall ( or that many recall incorrectly ) and thus take a short time on average .", "entities": []}, {"text": "The interaction between item dif\ufb01culty and time intensity can therefore provide valuable information about the complexity of the response process demanded by an item , which , we argue , can be further explained by examining the linguistic properties of the item .", "entities": []}, {"text": "In this paper , we use a data - driven approach to capture the interaction between item dif\ufb01culty and response time within a pool of 18,961 multiplechoice items from a high - stakes medical exam , where each item was answered by 335 examinees on average .", "entities": []}, {"text": "For our data , this resulted in the de\ufb01nition of two clusters , one of which consisted of items that are relatively easy and less time - intensive , and another one which consisted of items that are relatively dif\ufb01cult and/or time - intensive .", "entities": []}, {"text": "For the purposes of this study , we name these two clusters low - complexity class and high - complexity class , respectively .", "entities": []}, {"text": "The use of the term response process", "entities": []}, {"text": "224A 16 - year - old boy is brought to the emergency department because of a 2 - day history of fever , nausea , vomiting , headache , chills , and fatigue .", "entities": []}, {"text": "He has not had any sick contacts .", "entities": []}, {"text": "He underwent splenectomy for traumatic injury at the age of 13 years .", "entities": []}, {"text": "He has no other history of serious illness and takes no medications .", "entities": []}, {"text": "He appears ill .", "entities": []}, {"text": "His temperature is 39.2 \u00b0 C ( 102.5 \u00b0 F ) , pulse is 130 / min , respirations are 14 / min , and blood pressure is 110/60", "entities": []}, {"text": "mm Hg .", "entities": []}, {"text": "On pulmonary examination , scattered crackles are heard bilaterally .", "entities": []}, {"text": "Abdominal shows a well - healed midline scar and mild , diffuse tenderness to palpation .", "entities": []}, {"text": "Which of the following is the most appropriate next step in management ?", "entities": []}, {"text": "( A ) Antibiotic therapy ( B ) Antiemetic therapy ( C ) CT scan of the chest ( D ) X - ray of the abdomen ( E ) Reassurance Table 1 : An example of a practice item complexity here is not based on an operational de\ufb01nition of this construct , which would require extensive research on its own , but rather , as a succinct label that summarises the differences between the two classes along the interaction of empirical item dif\ufb01culty and item time intensiveness .", "entities": []}, {"text": "Studying the linguistic characteristics of these two categories may help test developers gain a more nuanced understanding of how cognitively complex items differ from those with a straightforward solution .", "entities": []}, {"text": "Provided that strong relationships are found , such insight can also be used to guide item writers or inform innovative automated item generation algorithms when seeking to create high- or low - complexity items .", "entities": []}, {"text": "For this reason , our goal is not to train a black - box model to predict item complexity ; instead , our goal is to isolate interpretable relationships between item text and item complexity that can inform our understanding of the response process and provide better itemwriting strategies .", "entities": []}, {"text": "In addition to its utility for improving highstakes exams , the problem of modeling response process complexity is interesting from an NLP perspective because it requires the modeling of cognitive processes beyond reading comprehension .", "entities": [[32, 34, "TaskName", "reading comprehension"]]}, {"text": "This is especially relevant for the data used here because , as we explain in Section 3 below , the items in our bank assess expert - level clinical knowledge and are written to a common reading level using standardized language .", "entities": [[28, 30, "TaskName", "clinical knowledge"]]}, {"text": "Contributions : i ) We use unsupervised clustering to de\ufb01ne classes of high and low responseprocess complexity from a large sample of items and test - takers in a high - stakes medical exam ; ii ) the study provides empirical evidence that linguistic characteristics carry signal relevant to an item \u2019s response process complexity ; iii ) the most predictive features are identi\ufb01ed through several feature selection methods and their potential relationshipto response process complexity is discussed ; iv ) the errors made by the model and their implications for predicting response process complexity are analysed .", "entities": [[66, 68, "MethodName", "feature selection"]]}, {"text": "2 Related Work This section discusses related work on the topics of modeling item dif\ufb01culty and response time .", "entities": []}, {"text": "Most NLP studies modeling the dif\ufb01culty of test questions for humans have been conducted in the domain of reading comprehension , where the readability of reading passages is associated with the dif\ufb01culty of their corresponding comprehension questions ( Huang et al . , 2017 ; Beinborn et", "entities": [[18, 20, "TaskName", "reading comprehension"]]}, {"text": "al . , 2015 ; Loukina et al . , 2016 ) .", "entities": []}, {"text": "For other exams , taxonomies representing knowledge dimensions and cognitive processes involved in the completion of a test task have been used to predict the dif\ufb01culty of short - answer questions ( Pad \u00b4 o , 2017 ) and identify skills required to answer school science questions ( Nadeem and Ostendorf , 2017 ) .", "entities": []}, {"text": "Dif\ufb01culty prediction has also been explored in the context of evaluating automatically generated questions ( Alsubait et al . , 2013 ; Ha and Yaneva , 2018 ; Kurdi , 2020 ; Kurdi et al . , 2020 ) through measures such as question - answer similarity .", "entities": []}, {"text": "Response time prediction has mainly been explored in the \ufb01eld of educational testing using predictors such as item presentation position ( Parshall et al . , 1994 ) , item content category ( Parshall et al . , 1994 ; Smith , 2000 ) , the presence of a \ufb01gure ( Smith , 2000 ; Swanson et al . , 2001 ) , and item dif\ufb01culty and discrimination ( Halkitis et al . , 1996 ; Smith , 2000 ) .", "entities": []}, {"text": "The only text - related feature explored in these studies was word count , and it was shown to have a very limited predictive power in most domains .", "entities": []}, {"text": "Several studies have explored the prediction of item dif\ufb01culty and response time in the context of clinical multiple choice questions ( MCQs ) .", "entities": []}, {"text": "Ha et al .", "entities": []}, {"text": "( 2019 ) propose a large number of linguis-", "entities": []}, {"text": "225tic features and embeddings for modeling item dif\ufb01culty .", "entities": []}, {"text": "The results show that the full model outperforms several baselines with a statistically signi\ufb01cant improvement , however , its practical signi\ufb01cance for successfully predicting item dif\ufb01culty remains limited , con\ufb01rming the challenging nature of the problem .", "entities": []}, {"text": "Continuations of this study include the use of transfer learning to predict dif\ufb01culty and response time ( Xue et al . , 2020 ) , as well as using predicted dif\ufb01culty for \ufb01ltering out items that are too easy or too dif\ufb01cult for the intended examinee population ( Yaneva et al . , 2020 ) .", "entities": [[8, 10, "TaskName", "transfer learning"]]}, {"text": "Baldwin et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) used a broad range of linguistic features and embeddings ( similar to those in Ha et al .", "entities": []}, {"text": "( 2019 ) ) to predict item response time , showing that a wide range of linguistic predictors at various levels of linguistic processing were all relevant to responsetime prediction .", "entities": []}, {"text": "The predicted response times were then used in a subsequent experiment to improve fairness by reducing the time intensity variance of exam forms .", "entities": []}, {"text": "3 Data The data1used in this study comprises 18,961 Step 2 Clinical Knowledge items from the United States Medical Licensing Examination ( USMLE \u00ae ) , a large - scale high - stakes medical assessment .", "entities": [[11, 13, "TaskName", "Clinical Knowledge"]]}, {"text": "All items were MCQs .", "entities": []}, {"text": "An example practice item2 is given in Table 1 .", "entities": []}, {"text": "The exam comprises several one - hour testing blocks with 40 items per block .", "entities": []}, {"text": "All items test medical knowledge and are written by experienced item - writers following guidelines intended to produce items that vary in their dif\ufb01culty and response times only due to differences in the medical content they assess .", "entities": []}, {"text": "These guidelines stipulate that item writers adhere to a standard structure and avoid excessive verbosity , extraneous material not needed to answer the item , information designed to mislead the test - taker , and grammatical cues ( e.g. , correct answers that are more speci\ufb01c than the other options ) .", "entities": []}, {"text": "All items were administered between 2010 and 2015 as pretest items and presented alongside scored items on operational exams .", "entities": []}, {"text": "Examinees were medical students from accredited US and Canadian medical schools taking the exam for the \ufb01rst time and had no way of knowing which items were pretest items and which were 1The data can not be made available due to exam security considerations .", "entities": []}, {"text": "2Source : https://www.usmle.org/pdfs/ step-2 - ck/2020_Step2CK_SampleItems.pdfscored .", "entities": []}, {"text": "On average , each item was attempted by 335 examinees ( SD = 156.8 ) .", "entities": []}, {"text": "3.1 Identifying items with high and low response process complexity We base our de\ufb01nition of the two classes of items on empirical item dif\ufb01culty andtime intensity .", "entities": []}, {"text": "Item dif\ufb01culty is measured by the proportion of examinees who answered the item correctly , a metric commonly referred to by the educational testing community as p - value and calculated as follows : Pi = PN n=1Un N ; where Piis the p - value for item i , Unis the 0 - 1 score ( incorrect - correct ) on item iearned by examinee n , and Nis the total number of examinees in the sample .", "entities": [[52, 53, "DatasetName", "0"]]}, {"text": "Thus , dif\ufb01culty measured in this way ranges from 0 to 1 and higher values correspond to easier items .", "entities": [[9, 10, "DatasetName", "0"]]}, {"text": "Time intensity is found by taking the arithmetic mean response time , measured in seconds , across all examinees who attempted a given item .", "entities": []}, {"text": "This includes all time spent on the item from the moment it is presented on the screen until the examinee moves to the next item , as well as any revisits .", "entities": []}, {"text": "To assign items to classes , p - value and mean response time are rescaled such that each variable has a mean of 0 and a standard deviation of 1 .", "entities": [[23, 24, "DatasetName", "0"]]}, {"text": "Moreover , we use two quantitative methods to categorize items and retain only those items where there was agreement between the two methods .", "entities": []}, {"text": "Method 1 : Items were classi\ufb01ed by applying a K - means clustering algorithm via the kmeans function in Python \u2019s Scikit - learn ( Pedregosa et al . , 2011 ) .", "entities": [[9, 13, "MethodName", "K - means clustering"]]}, {"text": "K - means is an unsupervised data classi\ufb01cation technique that discovers patterns in the data by assigning instances to a pre - de\ufb01ned number of classes ( Wagstaff et al . , 2001 ) .", "entities": []}, {"text": "This approach also allows us to evaluate the plausibility of categorizing items into more than two complexity classes , or whether the items fail to show any meaningful separation along the interaction of p - value and duration ( one class ) .", "entities": []}, {"text": "Results suggest that two classes best \ufb01t these data and identi\ufb01ed 11,067 items as low complexity and 7,894 items as high complexity3 .", "entities": []}, {"text": "3We also experimented with hierarchical clustering , which led to similar results .", "entities": []}, {"text": "The hierarchical clustering dendrogram suggested that there are meaningful distances between two clusters in the data , and much smaller distances between a higher number of more \ufb01ne - grained clusters .", "entities": []}, {"text": "226Method 2 : Any item with a rescaled p - value greater than its rescaled mean response time \u2013 indicating that the item is relatively easier than it is time - consuming \u2013 is classi\ufb01ed as low - complexity ( 11,682 items ) .", "entities": []}, {"text": "Likewise , the remaining items , which had rescaled p - values less than their rescaled mean response times , were assigned to the highcomplexity class ( 7,279 items ) .", "entities": []}, {"text": "Put another way , if an item takes less time than we would expect given its dif\ufb01culty , the item is classi\ufb01ed as low response process complexity and if it takes more time than we would expect , it is classi\ufb01ed as high response process complexity .", "entities": []}, {"text": "The two methods achieved strong agreement , with only 673 ( 3.5 % ) items being assigned to different classes across methods .", "entities": []}, {"text": "These discrepant items are excluded , leaving a total of 18,288 items for further analysis : 11,038 low - complexity items and 7,250 high - complexity ones .", "entities": []}, {"text": "Figure 1 shows the class assignment , p - value , and mean response time for each item .", "entities": []}, {"text": "Figure 1 : Class assignment by p - value and response time for each item .", "entities": []}, {"text": "Note that discrepant items were excluded , as illustrated by the gap between the two class distributions .", "entities": []}, {"text": "As can be seen from the \ufb01gure , the class of lowcomplexity items was dense and homogenous compared to the high - complexity class , meaning that it contained a large number of easy items whose response times were always below 125 seconds .", "entities": []}, {"text": "The high - complexity class on the other hand was highly heterogeneous , with items whose response times and p - values spanned almost the entire scale .", "entities": []}, {"text": "4 Features We use a set of interpretable linguistic features , many of which were previously used for predicting item dif\ufb01culty ( Ha et al . , 2019 ) and response time ( Baldwin et al . , 2020 ) in the domain of clinical MCQs .", "entities": []}, {"text": "These features were extracted using codemade available by Ha et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) and", "entities": []}, {"text": "to these , we add several predictors speci\ufb01cally related to the medical content of the items , as well as standard item metadata .", "entities": []}, {"text": "4.1 Linguistic features As noted , this study replicates the feature extraction procedure described and made available by Ha et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "Approximately 90 linguistic features were extracted from each item \u2019s text ( the full item including answer options ) and are summarized in Table 2 .", "entities": []}, {"text": "They span several levels of linguistic processing including surface lexical and syntactic features , semantic features that account for ambiguity , and cognitively motivated features that capture properties such as imageability and familiarity .", "entities": []}, {"text": "Common readability formulae are used to account for surface reading dif\ufb01culty .", "entities": []}, {"text": "The organization of ideas in the text is captured through text cohesion features that measure the number and types of connective words within an item .", "entities": []}, {"text": "Finally , word frequency features ( including threshold frequencies ) measure the extent to which items utilize frequent vocabulary .", "entities": []}, {"text": "Combinations of these features have the potential to capture different aspects of item content that are relevant to response complexity .", "entities": []}, {"text": "For example , medical terms can be expected to have lower absolute frequencies and familiarity ratings , among other characteristics , and combinations of these features may suggest a higher density of medical terms and specialized language in some items compared to others .", "entities": []}, {"text": "Another example is the temporal organization of the information about the patient history and symptoms described in the item and captured by temporal connectives , where it is reasonable to expect that more temporally intricate cases would require higher response process complexity to solve .", "entities": []}, {"text": "Similarly , a high number of causal connectives would indicate a higher complexity of causal relationships among the events that led to the patient seeing a doctor , which may also be associated with higher cognitive demands .", "entities": []}, {"text": "4.2 Clinical content features This group of features relates to the medical content of the items by mapping terms and phrases in the text to medical concepts contained in the Uni\ufb01ed Medical Language System ( UMLS ) Metathesaurus ( Schuyler et al . , 1993 ) using Metamap ( Aronson , 2001 ) .", "entities": [[35, 36, "DatasetName", "UMLS"]]}, {"text": "The number of UMLS terms that appear in an item may indicate the amount of medical content", "entities": [[3, 4, "DatasetName", "UMLS"]]}, {"text": "227Group N Summary of features Resources Lexical 5 Word Count , Content word count , Content word count without stopwords , Average word length in syllables , Complex word count Syntactic 29 POS count , Phrase count ( for each POS ) , Type count , Comma count , Average phrase length , Negation , Type - token ratio , Average sentence length , Average depth of tree , Clause count ( relative , conditional ) , Average number of words before the main verb , Passive - active ratio , Proportion active VPs , Proportion passive VPs , Agentless passive countStanford NLP Parser ( Manning et al . , 2014 )", "entities": []}, {"text": "Semantic 11 Polysemic word count , Average senses for : content words , nouns , verbs , adjectives , auxiliary verbs , adverbs ; Average noun / verb distance to WordNet root , Average noun - and - verb distance to WordNet root , Answer words in WordNet ratioWordNet ( Miller , 1995 ) Readability 7 Flesch Reading Ease , Flesch - Kincaid grade level , Automated Readability Index , Gunning Fog , Coleman Liau , SMOG , SMOG IndexSee Dubay ( 2004 ) for de\ufb01nitions Cognitive 14 Absolute values , ratios , and ratings for Concreteness , Imageability , Familiarity , Age of acquisition , Meaningfulness ( Colorado norms ) , Meaningfulness ( Paivio norms)MRC Psycholinguistic Database ( Coltheart , 1981 )", "entities": []}, {"text": "Frequency 10 Average frequency ( relative , absolute and rank ) for all words and for content words ; Threshold frequencies for words not in the \ufb01rst 2,000/3,000/4,000/5,000 most common wordsBritish National Corpus ( Leech et al . , 2014 )", "entities": []}, {"text": "Cohesion 5 Counts of Temporal , Causal , Additive connectives and All connectives ; Referential pronoun count Table 2 : Linguistic features extracted for each item following Ha et al .", "entities": []}, {"text": "( 2019 ) the item contains ( note that a given term found in the items can refer to multiple UMLS concepts ) .", "entities": [[20, 21, "DatasetName", "UMLS"]]}, {"text": "First , we ask : how many of the words and phrases in the items are medical terms ?", "entities": []}, {"text": "This information is captured by UMLS Terms Count , indicating the number of terms in an item that appear in the UMLS wherein each instance of a given term contributes to the total count , as well as UMLS Distinct Terms Count : the number of terms in an item that appear in the UMLS wherein multiple instances of a given term contribute only once to the total count .", "entities": [[5, 6, "DatasetName", "UMLS"], [21, 22, "DatasetName", "UMLS"], [38, 39, "DatasetName", "UMLS"], [54, 55, "DatasetName", "UMLS"]]}, {"text": "The same kinds of counts are done for medical phrases \u2013 UMLS Phrases Count refers to the number of phrases in an item .", "entities": [[11, 12, "DatasetName", "UMLS"]]}, {"text": "For example , Metamap maps \u2018 ocular complications of myasthenia gravis \u2019 to two phrases : the noun phrase \u2018 ocular complications \u2019 and the prepositional phrase \u2018 of myasthenia gravis \u2019 ( Aronson , 2001 ) .", "entities": []}, {"text": "Next , we introduce features that measure the ambiguity of medical terms within the items .", "entities": []}, {"text": "These include Average Number of Competing UMLS Concepts Per Term Count , which captures the average number of UMLS concepts that a term could be referring to , averaged for all terms in an item , and weighted by the number of times Metamap returns the term .", "entities": [[6, 7, "DatasetName", "UMLS"], [18, 19, "DatasetName", "UMLS"]]}, {"text": "A similar version of this feature but without weighting by the number of times Metamap returns the term is Average Number of UMLS Concepts Per Term Count .", "entities": [[22, 23, "DatasetName", "UMLS"]]}, {"text": "This metric is then computed at the level of sentences and items , resulting in : Average Number of UMLS Concepts per Sentence , which measures the medical ambigu - ity of sentences and UMLS Concept Count , which measures item medical ambiguity through the total number of UMLS concepts all terms in an item could refer to .", "entities": [[19, 20, "DatasetName", "UMLS"], [34, 35, "DatasetName", "UMLS"], [48, 49, "DatasetName", "UMLS"]]}, {"text": "Finally , UMLS concept incidence refers to the number of UMLS concepts per 1000 words .", "entities": [[2, 3, "DatasetName", "UMLS"], [10, 11, "DatasetName", "UMLS"]]}, {"text": "4.3 Standard Item Features This group of features refers to metadata describing item content .", "entities": []}, {"text": "Presence of an image is a binary categorical variable indicating whether the item includes an image such an X - ray or an MRI that needs to be examined .", "entities": []}, {"text": "Another variable is Content category , which describes 18 generic topic categories such as \u201c Cardiovascular \u201d , \u201c Gastrointestinal \u201d , \u201d Behavioral Health \u201d , \u2018 Immune System \u201d , and so on .", "entities": []}, {"text": "Another variable , Physician Task describes tasks required by the item , e.g. , determine a diagnosis , choose the correct medicine , apply foundational science concepts , and others .", "entities": []}, {"text": "Finally , we also include the Year the item was administered as a predictor ( 2010 - 2015 ) to account for potential changes in response process complexity and examinee samples over time .", "entities": []}, {"text": "4.4", "entities": []}, {"text": "Classi\ufb01cation This section describes three baseline models ( Section 4.5 ) , the training of classi\ufb01ers using the full feature set ( Section 4.6 ) , and the feature selection procedures ( Section 4.7 ) .", "entities": [[28, 30, "MethodName", "feature selection"]]}, {"text": "228Logistic regression Random forests Precision Recall Weighted F1 Precision Recall Weighted F1 Majority class 0.37 0.6 0.46 0.37 0.6 0.46 Word count 0.57 0.6 0.48 0.56 0.6 0.54 Standard item features 0.62 0.63 0.59 0.59 0.59 0.59 Full feature set 0.64 0.65 0.63 0.68 0.68 0.67 Selected linguistic features", "entities": [[4, 5, "MetricName", "Precision"], [5, 7, "MetricName", "Recall Weighted"], [7, 8, "MetricName", "F1"], [8, 9, "MetricName", "Precision"], [9, 11, "MetricName", "Recall Weighted"], [11, 12, "MetricName", "F1"]]}, {"text": "0.63 0.65 0.63 0.67 0.68 0.66", "entities": []}, {"text": "Table 3 : Weighted F1 scores for different models on the test set 4.5 Baseline Models", "entities": [[4, 5, "MetricName", "F1"]]}, {"text": "Three classi\ufb01cation baselines were computed to benchmark the predictive bene\ufb01t given by linguistics features over standard item characteristics : Majority Class Baseline : Since the lowcomplexity class contains a higher number of items , it is more likely that an item would be correctly predicted as belonging to this class .", "entities": []}, {"text": "Word Count : This baseline examines the possibility that response process complexity is simply a function of item length .", "entities": []}, {"text": "Standard Item Features :", "entities": []}, {"text": "This baseline comprises Word count , Presence of an image , Content category , Physician task andYear .", "entities": []}, {"text": "This model re\ufb02ects the standard item characteristics that most testing organizations would routinely store .", "entities": []}, {"text": "4.6 Full feature models After scaling the features , two models were \ufb01t using Python \u2019s scikit - learn library and the full set of features : a logistic regression model and a random forests one ( 400 trees ) .", "entities": [[28, 30, "MethodName", "logistic regression"]]}, {"text": "Twenty percent of the data ( 3,658 items ) were used as a test set .", "entities": []}, {"text": "4.7 Feature selection Feature selection was undertaken to better understand which features were most strongly associated with class differences .", "entities": [[1, 3, "MethodName", "Feature selection"], [3, 5, "MethodName", "Feature selection"]]}, {"text": "The selection process utilized three distinct strategies , where the \ufb01nal set of selected features comprises only those features retained by all three methods .", "entities": []}, {"text": "After applying feature selection to the training set , the predictive performance of the selected features is evaluated on the test set and compared to the performance of the full feature set and the baseline models outlined above .", "entities": [[2, 4, "MethodName", "feature selection"]]}, {"text": "Embedded methods : The \ufb01rst method is LASSO regularized regression wherein the coef\ufb01cients of variables that have low contributions towards the classi\ufb01cation performance are shrunk to zero by forcing the sum of the absolute value of the regression coef\ufb01cients to be less than a \ufb01xed value .", "entities": []}, {"text": "Weuse the LassoCV algorithm with 100 - fold cross validation and maximum iterations set to 5,000 .", "entities": []}, {"text": "Wrapper methods : We next apply recursive feature elimination , performed using two different classi\ufb01cation algorithms : random forests classi\ufb01er ( 400 trees , step = 5 ) and gradient boosting classi\ufb01er ( Friedman , 2002 ) ( default parameters , step = 5 ) .", "entities": []}, {"text": "The \ufb01nal set of selected linguistic features comprised 57 features that were retained by all three strategies .", "entities": []}, {"text": "These features and their evaluation are discussed in sections 5 and 7 . 5 Results Table 3 presents the classi\ufb01cation results for the baselines , the full feature set , and the selected features for both logistic regression and random forests .", "entities": [[36, 38, "MethodName", "logistic regression"]]}, {"text": "Results are reported using a weighted F1 score , which is a classi\ufb01cation accuracy measure based on the mean between the precision and recall after adjusting for class imbalance .", "entities": [[6, 8, "MetricName", "F1 score"], [13, 14, "MetricName", "accuracy"]]}, {"text": "The linguistic and clinical content features improve predictive accuracy above the baselines , yielding a higher F1 score than the strongest baseline ( .67 compared to .59 ) .", "entities": [[8, 9, "MetricName", "accuracy"], [16, 18, "MetricName", "F1 score"]]}, {"text": "The reduced feature set does not lead to a meaningful performance drop compared to the full feature set , suggesting that no signal was lost due to feature elimination .", "entities": []}, {"text": "Figure 2 reports the eight best - performing features :", "entities": []}, {"text": "UMLS phrases count , Unique word count , Polysemic word count , Average noun phrase length , Automated readability index , Prepositional phrases , UMLS distinct terms count , andConcreteness ratio .", "entities": [[0, 1, "DatasetName", "UMLS"], [24, 25, "DatasetName", "UMLS"]]}, {"text": "6 Error analysis The output of the selected - features prediction model was analyzed further in order to get insight into this model \u2019s performance .", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "As could be expected , the majority class of low - complexity items was predicted more accurately than the highcomplexity class , as shown by the confusion matrix in Table 4 .", "entities": []}, {"text": "An interesting observation was made during a follow - up classi\ufb01cation experiment , which showed that this effect remained when using", "entities": []}, {"text": "229 Figure 2 : Distributions and median values for the top eight features by group .", "entities": []}, {"text": "balanced classes4 .", "entities": []}, {"text": "This shows that the success in predicting this class can not be attributed solely to its prevalence but potentially also to its high homogeneity compared to the high - complexity class .", "entities": []}, {"text": "High Complexity Low Complexity High Complexity 617 828 Low Complexity 332 1881 Table 4 : Confusion matrix for the results from the selected features model using random forests ( F1 = 0.66 )", "entities": [[29, 30, "MetricName", "F1"]]}, {"text": "Next , we plot the model errors across the two classes of low - complexity and high - complexity items , as shown in Figure 3 .", "entities": []}, {"text": "Notably , items with average response times below 150 seconds were predicted as low - complexity most of the time , with minimal consideration of their p - value .", "entities": []}, {"text": "This shows that what the model effectively learned was to distinguish between items with long and short mean response times , which overpowered its ability to predict the p - value parameter .", "entities": []}, {"text": "This \ufb01nding is consistent with previous work , where response times in Baldwin et al .", "entities": []}, {"text": "( 2020 ) were predicted more successfully than p - value using a similar set of linguistic features in Ha et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "Finally , analysis of the feature distributions across these four classes revealed no unexpected patterns .", "entities": []}, {"text": "7 Discussion The results presented in the previous section lead to three main \ufb01ndings : i ) the linguistic characteristics of the items carry signal relevant to response 4Classes were balanced using the balanced subample setting of the class weight parameter in Scikit - learn \u2019s RandomForrestClassifierprocess complexity ; ii ) no individual features stand out as strong predictors , and iii ) the most important features were those related to syntax and semantics .", "entities": []}, {"text": "The \ufb01rst of these \ufb01ndings relates to the fact that the linguistic characteristics of the items carry signal that is predictive of response process complexity , revealing that the problems posed by lowcomplexity and high - complexity items are described using slightly different language .", "entities": []}, {"text": "While this signal outperformed several baselines , the overall low predictive utility of the models suggests that there are other factors , yet to be captured , that have a signi\ufb01cant effect on response process complexity .", "entities": []}, {"text": "The retention of 56 features indicates that individual linguistic predictors provide a weak classi\ufb01cation signal but , taken together , they complement each other in a way that ultimately provides a higher accuracy .", "entities": [[32, 33, "MetricName", "accuracy"]]}, {"text": "The fact that there are many predictive features with none standing out is also a positive evaluation outcome for item writing quality , as it shows that the response process complexity associated with an item is not distributed along a small number of linguistic parameters .", "entities": []}, {"text": "The most important features that helped with classi\ufb01cation were those related to syntax and semantics ( Figure 2 ) .", "entities": []}, {"text": "The poor performance of the Word Count baseline suggests that differences in response process complexity can not be explained solely by item length and that more complex linguistic features capture some of the nuance in the response process .", "entities": []}, {"text": "As can be seen in Figure 2 , high - complexity items contain a slightly higher number of UMLS phrases and ( distinct ) medical terms , as well as a higher number of unique words .", "entities": [[18, 19, "DatasetName", "UMLS"]]}, {"text": "These features suggest high - complexity items re-", "entities": []}, {"text": "230 Figure 3 : Error distribution for the two classes peat words less frequently and may contain a higher concentration of new information and specialized terminology than low - complexity items .", "entities": [[4, 5, "MetricName", "Error"]]}, {"text": "The individual phrases in high - complexity items are also slightly longer , which naturally in\ufb02uences readability metrics that are based on word and sentence length , such as the Automated Readability Index ( higher values are indicative of a more complex text ) .", "entities": []}, {"text": "Prepositional phrases were also identi\ufb01ed as more important than other phrase types in distinguishing between response process complexity .", "entities": []}, {"text": "Prepositional phrases often serve as modi\ufb01ers of the primary noun phrase and the higher number of prepositional phrases in the high - complexity items suggests the use of more speci\ufb01c descriptions ( e.g. , \u201c small cell carcinoma of the ovary \u201d instead of just \u201c small cell carcinoma \u201d ) .", "entities": []}, {"text": "The words contained in the high - complexity items also have slightly higher concreteness levels , providing another indication that they may contain more terms , as terms tend to be more concrete than common words .", "entities": []}, {"text": "Finally , the words contained in the high - complexity items also tend to have more possible meanings , as indicated by the polysemous word count variable , which results in higher complexity owing to disambiguation efforts .", "entities": []}, {"text": "Overall , these features indicate that the language used in the low - complexity items is less ambiguous and descriptive , and potentially contains fewer medical terms .", "entities": []}, {"text": "One limitation of the study is the fact that it treats item dif\ufb01culty and time intensiveness as independent variables .", "entities": []}, {"text": "This may not always be the case , as examinees do employ strategies to optimize their time .", "entities": []}, {"text": "Given \ufb01nite time limits , examinees may ig - nore time intensive items if they believe the time needed for such items can be better utilized attempting other , less time intensive items .", "entities": []}, {"text": "Therefore , the relationship between dif\ufb01culty and response time and their association with item text would differ for exams that do not impose strict time limits .", "entities": []}, {"text": "When using data - driven approaches to de\ufb01ning item classes , our data did not lend itself to a categorization that would allow investigating high dif\ufb01culty / low response time items and vice - versa .", "entities": []}, {"text": "While the approach taken in this paper has a higher ecological validity , studying such cases in the future may lead to a greater understanding of various aspects of response process complexity and their relationship to item text .", "entities": []}, {"text": "Other future work includes exploration of potential item position effects .", "entities": []}, {"text": "8 Conclusion The experiments presented in this paper are , to the best of our knowledge , the \ufb01rst investigation of the relationship between item text and response process complexity .", "entities": []}, {"text": "The results showed that such a relationship exists .", "entities": []}, {"text": "To the extent that items were written as clearly and as concisely as possible , the \ufb01ndings suggest that high - complexity medical items generally include longer phrases , more medical terms , and more speci\ufb01c descriptions .", "entities": []}, {"text": "While the models outperformed several baselines , they required a large number of features to do so and the predictive utility remained low .", "entities": []}, {"text": "Ultimately , this shows the challenging nature of modeling response process complexity using interpretable models and the lack of a straightforward way to manipulate this item property .", "entities": []}, {"text": "231References Tahani Alsubait , Bijan Parsia , and Ulrike Sattler .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "A similarity - based theory of controlling mcq dif\ufb01culty .", "entities": []}, {"text": "In e - Learning and e - Technologies in Education ( ICEEE ) , 2013 Second International Conference on , pages 283\u2013288 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Alan R Aronson .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Effective mapping of biomedical text to the umls metathesaurus : the metamap program .", "entities": [[7, 8, "DatasetName", "umls"]]}, {"text": "In Proceedings of the AMIA Symposium , page 17 .", "entities": []}, {"text": "American Medical Informatics Association .", "entities": []}, {"text": "Peter Baldwin , Victoria Yaneva , Janet Mee , Brian E Clauser , and Le An Ha . 2020 .", "entities": []}, {"text": "Using natural language processing to predict item response times and improve test construction .", "entities": []}, {"text": "Journal of Educational Measurement .", "entities": []}, {"text": "Lisa Beinborn , Torsten Zesch , and Iryna Gurevych . 2015 .", "entities": []}, {"text": "Candidate evaluation strategies for improved dif\ufb01culty prediction of language tests .", "entities": []}, {"text": "In Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 1\u201311 .", "entities": []}, {"text": "Max Coltheart .", "entities": []}, {"text": "1981 .", "entities": []}, {"text": "The mrc psycholinguistic database .", "entities": []}, {"text": "The Quarterly Journal of Experimental Psychology Section A , 33(4):497\u2013505 .", "entities": []}, {"text": "William H. Dubay .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "The Principles of Readability .", "entities": []}, {"text": "Impact Information .", "entities": []}, {"text": "Jerome H Friedman .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Stochastic gradient boosting .", "entities": []}, {"text": "Computational statistics & data analysis , 38(4):367\u2013378 .", "entities": []}, {"text": "Le An Ha and Victoria Yaneva .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Automatic distractor suggestion for multiple - choice tests using concept embeddings and information retrieval .", "entities": [[12, 14, "TaskName", "information retrieval"]]}, {"text": "In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 389\u2013398 .", "entities": []}, {"text": "Le An Ha , Victoria Yaneva , Peter Balwin , and Janet Mee .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Predicting the dif\ufb01culty of multiple choice questions in a high - stakes medical exam .", "entities": []}, {"text": "In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications .", "entities": []}, {"text": "Perry N Halkitis et al .", "entities": []}, {"text": "1996 .", "entities": []}, {"text": "Estimating testing time : The effects of item characteristics on response latency .", "entities": []}, {"text": "Zhenya Huang , Qi Liu , Enhong Chen , Hongke Zhao , Mingyong Gao , Si Wei , Yu Su , and Guoping Hu . 2017 .", "entities": []}, {"text": "Question dif\ufb01culty prediction for reading problems in standard tests .", "entities": []}, {"text": "In AAAI , pages 1352 \u2013 1359 .", "entities": []}, {"text": "Ghader Kurdi .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Generation and mining of medical , case - based multiple choice questions .", "entities": []}, {"text": "Ph.D. thesis , PhD thesis , University of Manchester .", "entities": []}, {"text": "Ghader Kurdi , Jared Leo , Bijan Parsia , Uli Sattler , and Salam Al - Emari .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A systematic review of automatic question generation for educational purposes .", "entities": [[5, 7, "TaskName", "question generation"]]}, {"text": "International Journal of Arti\ufb01cial Intelligence in Education , 30(1):121\u2013204 .", "entities": []}, {"text": "Geoffrey Leech , Paul Rayson , et al . 2014 .", "entities": []}, {"text": "Word frequencies in written and spoken English :", "entities": []}, {"text": "Based on the British National Corpus . Routledge .", "entities": []}, {"text": "Anastassia Loukina , Su - Youn Yoon , Jennifer Sakano , Youhua Wei , and Kathy Sheehan .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Textual complexity as a predictor of dif\ufb01culty of listening items in language pro\ufb01ciency tests .", "entities": []}, {"text": "In Proceedings of COLING 2016 , the 26th International Conference on Computational Linguistics : Technical Papers , pages 3245\u20133253 .", "entities": []}, {"text": "Christopher D Manning , Mihai Surdeanu , John Bauer , Jenny Rose Finkel , Steven Bethard , and David McClosky .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "The stanford corenlp natural language processing toolkit .", "entities": []}, {"text": "In ACL ( System Demonstrations ) , pages 55\u201360 .", "entities": []}, {"text": "George A Miller .", "entities": []}, {"text": "1995 .", "entities": []}, {"text": "Wordnet : a lexical database for english .", "entities": []}, {"text": "Communications of the ACM , 38(11):39 \u2013 41 .", "entities": [[3, 4, "DatasetName", "ACM"]]}, {"text": "Farah Nadeem and Mari Ostendorf .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Language based mapping of science assessment items to skills .", "entities": []}, {"text": "InProceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications , pages 319\u2013326 .", "entities": []}, {"text": "Ulrike Pad \u00b4 o. 2017 .", "entities": []}, {"text": "Question dif\ufb01culty \u2013 how to estimate without norming , how to use for automated grading .", "entities": []}, {"text": "In Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications , pages 1\u201310 .", "entities": []}, {"text": "Cynthia G Parshall et al . 1994 .", "entities": []}, {"text": "Response latency : An investigation into determinants of item - level timing .", "entities": []}, {"text": "Fabian Pedregosa , Ga \u00a8el Varoquaux , Alexandre Gramfort , Vincent Michel , Bertrand Thirion , Olivier Grisel , Mathieu Blondel , Peter Prettenhofer , Ron Weiss , Vincent Dubourg , et al . 2011 .", "entities": []}, {"text": "Scikit - learn : Machine learning in python .", "entities": []}, {"text": "the Journal of machine Learning research , 12:2825\u20132830 .", "entities": []}, {"text": "Peri L Schuyler , William T Hole , Mark S Tuttle , and David D Sherertz .", "entities": []}, {"text": "1993 .", "entities": []}, {"text": "The umls metathesaurus : representing different views of biomedical concepts .", "entities": [[1, 2, "DatasetName", "umls"]]}, {"text": "Bulletin of the Medical Library Association , 81(2):217 .", "entities": []}, {"text": "Russell Winsor Smith .", "entities": []}, {"text": "2000 .", "entities": []}, {"text": "An exploratory analysis of item parameters and characteristics that in\ufb02uence item level response time .", "entities": []}, {"text": "David B Swanson , Susan M Case , Douglas R Ripkey , Brian E Clauser , and Matthew C Holtman .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "Relationships among item characteristics , examine characteristics , and response times on usmle step 1 . Academic Medicine , 76(10):S114 \u2013 S116 .", "entities": []}, {"text": "232Kiri", "entities": []}, {"text": "Wagstaff , Claire Cardie , Seth Rogers , Stefan Schr \u00a8odl , et al . 2001 .", "entities": []}, {"text": "Constrained k - means clustering with background knowledge .", "entities": [[1, 5, "MethodName", "k - means clustering"]]}, {"text": "In Icml , volume 1 , pages 577\u2013584 .", "entities": []}, {"text": "Kang Xue , Victoria Yaneva , Christopher Runyon , and Peter Baldwin .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Predicting the dif\ufb01culty and response time of multiple choice questions using transfer learning .", "entities": [[11, 13, "TaskName", "transfer learning"]]}, {"text": "In Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 193\u2013197 .", "entities": []}, {"text": "Victoria Yaneva , Le An Ha , Peter Baldwin , and Janet Mee . 2020 .", "entities": []}, {"text": "Predicting item survival for multiple choice questions in a high - stakes medical exam .", "entities": []}, {"text": "In Proceedings of The 12th Language Resources and Evaluation Conference , pages 6812\u20136818 .", "entities": []}]