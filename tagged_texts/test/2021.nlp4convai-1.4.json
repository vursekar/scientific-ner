[{"text": "Proceedings of the Third Workshop on Natural Language Processing for Conversational AI , pages 30\u201339 November 10 , 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics30Multilingual Paraphrase Generation For Bootstrapping New Features in Task - Oriented Dialog Systems Subhadarshi Panda1;\u0003 , Caglar Tirkaz2 , Tobias Falke2and Patrick Lehnen2 1Graduate Center , City University of New York , USA 2Amazon Alexa AI , Germany spanda@gc.cuny.edu , { caglart,falket,plehnen}@amazon.com", "entities": [[6, 8, "TaskName", "Paraphrase Generation"]]}, {"text": "Abstract The lack of labeled training data for new features is a common problem in rapidly changing real - world dialog systems .", "entities": []}, {"text": "As a solution , we propose a multilingual paraphrase generation model that can be used to generate novel utterances for a target feature and target language .", "entities": [[8, 10, "TaskName", "paraphrase generation"]]}, {"text": "The generated utterances can be used to augment existing training data to improve intent classi\ufb01cation and slot labeling models .", "entities": []}, {"text": "We evaluate the quality of generated utterances using intrinsic evaluation metrics and by conducting downstream evaluation experiments with English as the source language and nine different target languages .", "entities": []}, {"text": "Our method shows promise across languages , even in a zero - shot setting where no seed data is available .", "entities": []}, {"text": "1 Introduction Spoken language understanding is a core problem in task oriented dialog systems with the goal of understanding and formalizing the intent expressed by an utterance ( Tur and De Mori , 2011 ) .", "entities": [[2, 5, "TaskName", "Spoken language understanding"]]}, {"text": "It is often modeled as intent classi\ufb01cation ( IC ) , an utterance - level multi - class classi\ufb01cation problem , and slot labeling ( SL ) , a sequence labeling problem over the utterance \u2019s tokens .", "entities": []}, {"text": "In recent years , approaches that train joint models for both tasks and that leverage powerful pre - trained neural models greatly improved the state - of - the - art performance on available benchmarks for IC and SL ( Louvan and Magnini , 2020 ; Weld et al . , 2021 ) .", "entities": []}, {"text": "A common challenge in real - world systems is the problem of feature bootstrapping : If a new feature should be supported , the label space needs to be extended with new intent or slot labels , and the model needs to be retrained to learn to classify corresponding utterances .", "entities": []}, {"text": "However , labeled examples for the new feature are typically limited to a small set of seed examples , as the collection of more *", "entities": []}, {"text": "The \ufb01rst author worked on this paper during an internship at Amazon Alexa AI.annotations would make feature expansion costly and slow .", "entities": []}, {"text": "As a possible solution , previous work explored the automatic generation of paraphrases to augment the seed data ( Malandrakis et al . , 2019 ; Cho et al . , 2019 ; Jolly et al . , 2020 ) .", "entities": []}, {"text": "In this work , we study feature bootstrapping in the case of a multilingual dialog system .", "entities": []}, {"text": "Many large - scale real - world dialog systems , e.g. Apple \u2019s Siri , Amazon \u2019s Alexa and Google \u2019s Assistant , support interactions in multiple languages .", "entities": [[19, 20, "DatasetName", "Google"]]}, {"text": "In such systems , the coverage of languages and the range of features is continuously expanded .", "entities": []}, {"text": "That can lead to differences in the supported intent and slot labels across languages , in particular if a new language is added later or if new features are not rolled out to all languages simultaneously .", "entities": []}, {"text": "As a consequence , labeled data for a feature can be available in one language , but limited or completely absent in another .", "entities": []}, {"text": "With multilingual paraphrase generation , we can bene\ufb01t from this setup and improve data augmentation for data - scarce languages via cross - lingual transfer from data - rich languages .", "entities": [[2, 4, "TaskName", "paraphrase generation"], [13, 15, "TaskName", "data augmentation"], [21, 25, "TaskName", "cross - lingual transfer"]]}, {"text": "As a result , the data augmentation can not only be applied with seed data , i.e. in a few - shot setting , but even under zero - shot conditions with no seeds at all for the target language .", "entities": [[5, 7, "TaskName", "data augmentation"], [33, 34, "DatasetName", "seeds"]]}, {"text": "To address this setup , we follow the recent work of Jolly et al . ( 2020 ) , which proposes to use an encoder - decoder model that maps from structured meaning representations to corresponding utterances .", "entities": []}, {"text": "Because such an input is language - agnostic , it is particularly well - suited for the multilingual setup .", "entities": []}, {"text": "We make the following extensions : First , we port their model to a transformer - based architecture and allow multilingual training by adding the desired target language as a new input to the conditional generation .", "entities": []}, {"text": "Second , we let the model generate slot labels along with tokens to alleviate the need for additional slot projection techniques .", "entities": []}, {"text": "And third , we introduce improved paraphrase decoding methods that leverage a model - based selec-", "entities": []}, {"text": "31tion strategy .", "entities": []}, {"text": "With that , we are able to generate labeled data for a new feature even in the zero - shot setting where no seeds are available at all .", "entities": [[23, 24, "DatasetName", "seeds"]]}, {"text": "We evaluate our approach by simulating a crosslingual feature bootstrapping setting , either fewshot or zero - shot , on MultiATIS , a common IC / SL benchmark spanning nine languages .", "entities": []}, {"text": "The experiments compare against several alternative methods , including previous work for mono - lingual paraphrase generation and machine translation .", "entities": [[15, 17, "TaskName", "paraphrase generation"], [18, 20, "TaskName", "machine translation"]]}, {"text": "We \ufb01nd that our method produces paraphrases of high novelty and diversity and using it for IC / SL training shows promising downstream classi\ufb01cation performance .", "entities": []}, {"text": "2 Related work Various studies have explored paraphrase generation for dialog systems .", "entities": [[7, 9, "TaskName", "paraphrase generation"]]}, {"text": "Bowman et al .", "entities": []}, {"text": "( 2016 ) showed that generating sentences from a continuous latent space is possible using a variational autoencoder model and provided guidelines on how to train such a generation model .", "entities": [[16, 18, "MethodName", "variational autoencoder"]]}, {"text": "However , our model uses an encoder - decoder approach which can handle the intent and language as categorical inputs in addition to the sequence input .", "entities": []}, {"text": "Malandrakis et al .", "entities": []}, {"text": "( 2019 ) explored a variety of controlled paraphrase generation approaches for data augmentation and proposed to use conditional variational autoencoders which they showed obtained the best results .", "entities": [[8, 10, "TaskName", "paraphrase generation"], [12, 14, "TaskName", "data augmentation"], [20, 21, "MethodName", "autoencoders"]]}, {"text": "Our method is different as it uses a conditional seq2seq model that can generate text from any sequence of slots and does not require an utterance as an input .", "entities": [[9, 10, "MethodName", "seq2seq"]]}, {"text": "Xia et al .", "entities": []}, {"text": "( 2020 ) propose a transformer - based conditional variational autoencoder for few shot utterance generation where the latent space represents the intent as two independent parts ( domain and action ) .", "entities": [[9, 11, "MethodName", "variational autoencoder"]]}, {"text": "Our approach is different since it models the language and intent of the generation that can be controlled explicitly .", "entities": []}, {"text": "Also , our model is the \ufb01rst to enable zero - shot utterance generation .", "entities": []}, {"text": "Cho et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2019 ) generate paraphrases for seed examples with a transformer seq2seq model and self - label them with a baseline intent and slot model .", "entities": [[11, 12, "MethodName", "seq2seq"]]}, {"text": "We follow a similar approach but our model generates utterances from a sequence of slots rather than an utterance , which enables an explicitly controlled generation .", "entities": []}, {"text": "Also the number of seed utterances we use is merely 20 for the few shot setup unlike around 1 M seed para - carrier phrase pairs in Cho et al .", "entities": []}, {"text": "( 2019 ) .", "entities": []}, {"text": "Several other studies follow a text - to - text ap - proach and assume training data in the form of paraphrase pairs for training paraphrase generation models in a single language ( Gupta et al . , 2018 ; Li et al . , 2018 , 2019 ) .", "entities": [[25, 27, "TaskName", "paraphrase generation"]]}, {"text": "Our approach is focused towards generating utterances in the dialog domain that can generate utterances from a sequence of slots conditioned on both intent and language .", "entities": []}, {"text": "Jolly et al .", "entities": []}, {"text": "( 2020 ) showed that an interpretationto - text model can be used with shuf\ufb02ing - based sampling techniques to generate diverse and novel paraphrases from small amounts of seed data , that improve accuracy when augmenting to the existing training data .", "entities": [[34, 35, "MetricName", "accuracy"]]}, {"text": "Our approach is different as our model can generate the slot annotations along with the the utterance , which are necessary for the slot labeling task .", "entities": []}, {"text": "Our model can be seen as an extension of the model by Jolly et al .", "entities": []}, {"text": "( 2020 ) to a transformer based model , with the added functionality of controlling the language in which the utterance generation is needed , which in turn enables zero shot generation .", "entities": []}, {"text": "Using large pre - trained models has also been shown to be effective for paraphrase generation .", "entities": [[14, 16, "TaskName", "paraphrase generation"]]}, {"text": "Chen et al . ( 2020 ) for instance show the effectiveness of using GPT-2 ( Radford et al . , 2019 ) for generating text from tabular data ( a set of attributevalue pairs ) .", "entities": [[14, 15, "MethodName", "GPT-2"]]}, {"text": "Our model , however , does not rely on pre - trained weights from another model such as GPT-2 , is scalable , and can be applied to training data from any domain , for instance , dialog domain .", "entities": [[18, 19, "MethodName", "GPT-2"]]}, {"text": "Beyond paraphrase generation , several other techniques have been proposed for feature bootstrapping .", "entities": [[1, 3, "TaskName", "paraphrase generation"]]}, {"text": "Machine translation can be used from data - rich to data - scarce languages ( Gaspers et al . , 2018 ; Xu et", "entities": [[0, 2, "TaskName", "Machine translation"]]}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "Cross - lingual transfer learning can also leverage use existing data in other languages ( Do and Gaspers , 2019 ) .", "entities": [[0, 4, "TaskName", "Cross - lingual transfer"]]}, {"text": "If a feature is already being actively used , feedback signals from users , such as paraphrases or interruptions , can be used to obtain additional training data ( Muralidharan et al . , 2019 ;", "entities": []}, {"text": "Falke et al . , 2020 ) .", "entities": []}, {"text": "3 Proposed method We want to augment existing labeled utterances by generating additional novel utterances in a desired target language .", "entities": []}, {"text": "In our case , existing data consists of feature - unrelated data ( intents and slots already supported ) spanning all languages and featurerelated data , which is available in a source language but is small ( few - shot ) or not available ( zero shot ) in other languages .", "entities": []}, {"text": "For generation , we \ufb01rst extract", "entities": []}, {"text": "32 Transformer Encoder Slot embeddings Intent embeddings Language embeddingsfromloc.city_name toloc.city_name < s > < /s > airfare airfare airfare airfare EN EN EN ENTransformer Decoder + +", "entities": [[1, 2, "MethodName", "Transformer"]]}, {"text": "+ + + + + + Outputs ( Shifted right)Positional Encodinghow", "entities": []}, {"text": "O much", "entities": []}, {"text": "O is O a O flight", "entities": []}, {"text": "O from O washington   B - fromloc.city_name   to O montreal B - toloc.city_nameFigure 1 : Overall architecture of the multilingual paraphrase generation model .", "entities": [[22, 24, "TaskName", "paraphrase generation"]]}, {"text": "The slot , intent and language embeddings are added at the slot level to obtain representations to input to the encoder .", "entities": []}, {"text": "The < s > and</s > tags are necessary as they enable handling cases where we want to generate paraphrases having no associated slots .", "entities": []}, {"text": "The decoder generates the slot labels along with the paraphrase tokens .", "entities": []}, {"text": "the intent and slot types from the available data .", "entities": []}, {"text": "We then generate a new utterance by conditioning a multilingual language model on the intent , slot types and the target language .", "entities": []}, {"text": "We refer to utterances that have the same intent and slot types as paraphrases of each other since they convey the same meaning in the context of the SLU system .", "entities": []}, {"text": "3.1 Paraphrase Generation Model", "entities": [[1, 3, "TaskName", "Paraphrase Generation"]]}, {"text": "In order to generate paraphrases , we train a multilingual paraphrase generation model that generates a paraphrase given a language , an intent and a set of slot types .", "entities": [[10, 12, "TaskName", "paraphrase generation"]]}, {"text": "The model architecture is outlined in Figure 1 .", "entities": []}, {"text": "The model uses self - attention based encoder and decoder similar to the transformer ( Vaswani et al . , 2017 ) .", "entities": []}, {"text": "The encoder of the model receives as input the language embedding and the intent embedding , which are added to the slot embedding .", "entities": []}, {"text": "Unlike the transformer model ( Vaswani et al . , 2017 ) , we do not use the positional embedding in the encoder .", "entities": []}, {"text": "This is because the order of the slot types in the input sequence does not matter and is thus made indistinguishable for the encoder .", "entities": []}, {"text": "In order to generate paraphrases which can be used for data augmentation , we would need the slot annotations and the intents of the generations .", "entities": [[10, 12, "TaskName", "data augmentation"]]}, {"text": "Note that we already know the intent of the generated paraphrase since it is the same intent as speci\ufb01ed while generating it .", "entities": []}, {"text": "The slot annotations , however , are not readily obtained from the input slot types .", "entities": []}, {"text": "We can make the slot annotations part of the output sequence by generating the slot label in BIO format in every alternate time step , which would be the slot label for the token generated in the previous time step .", "entities": []}, {"text": "This enables the model to generate the slot annotations along with the paraphrase .", "entities": []}, {"text": "An illustrative example is shown in Figure 1.3.2 Decoding Techniques Generating the output sequence token - by - token can be done by using greedy decoding where given learned model parameters \u0012 , the most likely token is picked at each decoding step as xt= argmaxp\u0012(xtjx < t ) .", "entities": []}, {"text": "Such a generation process is deterministic .", "entities": []}, {"text": "For our task of generating paraphrases , we are interested in generating diverse and novel utterances .", "entities": []}, {"text": "Non - deterministic sampling methods such as top - k sampling has been used in related work ( Fan et al . , 2018 ; Welleck et", "entities": []}, {"text": "al . , 2020 ; Jolly et al . , 2020 ) to achieve this .", "entities": []}, {"text": "In top - k random sampling , we \ufb01rst scale the logits zwby using a temperature parameter \u001c before applying softmax .", "entities": [[20, 21, "MethodName", "softmax"]]}, {"text": "p(xt = wjx < t ) = exp(zw= \u001c ) P", "entities": []}, {"text": "w02Vexp(zw0= \u001c ) ; ( 1 ) whereVis the decoder \u2019s vocabulary .", "entities": []}, {"text": "Setting \u001c  > 1 encourages the resulting probability distribution to be less spiky , thereby encouraging diverse choices during sampling .", "entities": []}, {"text": "The top - k sampling restricts the size of the most likely candidate pool to k\u0014jVj .", "entities": []}, {"text": "3.3 Balanced Augmentation The generated paraphrases can be used to augment the existing training data .", "entities": []}, {"text": "Since the training data we use is highly imbalanced , data augmentation might lead to disturbance in the original intent distribution .", "entities": [[10, 12, "TaskName", "data augmentation"]]}, {"text": "To ensure that the data augmentation process does not disturb the original intent distribution , we compute the number of samples to augment using the following constraint : the ratio of target intent to other intents for the target language should be the same as the ratio of target intent to other intents in the source language .", "entities": [[4, 6, "TaskName", "data augmentation"], [18, 21, "HyperparameterName", "number of samples"]]}, {"text": "Sometimes , using the above constraint results in a negligible number of", "entities": []}, {"text": "33samples for augmentation , in which cases we use a minimal number of samples ( see experiments ) .", "entities": [[11, 14, "HyperparameterName", "number of samples"]]}, {"text": "3.4 Paraphrase Selection In addition to deciding how many paraphrases to augment , it is also crucial to decide which paraphrases to use .", "entities": []}, {"text": "Preliminary experimental results showed that samping uniformly from all generated paraphrases does not lead to improvement over the baseline .", "entities": []}, {"text": "Upon manual examination we found that not all the paraphrases belong to the desired target intent .", "entities": []}, {"text": "To cope with that problem , we use the baseline downstream intent classi\ufb01cation and slot labeling model , which is trained only on the existing data , to compute the likelihood of the generated paraphrases to belong to the target intent .", "entities": []}, {"text": "We rank all the generated paraphrases based on these probabilities and select from the top of the pool for augmentation of the seed data .", "entities": []}, {"text": "4 Experimental setup We evaluate our approach by simulating few - shot and zero - shot feature bootstrapping scenarios .", "entities": []}, {"text": "4.1 Data We use the MultiATIS++ data ( Xu et al . , 2020 ) , a parallel IC / SL corpus that was created by translating the original English dataset .", "entities": []}, {"text": "It covers a total of 9 languages : English , Hindi , Turkish , German , French , Portuguese , Spanish , Japanese and Chinese .", "entities": []}, {"text": "The languages encompass a diverse set of language families : Indo - European , Sino - Tibetan , Japonic and Altaic .", "entities": []}, {"text": "Choosing target intents To reduce the number of experiments , we only choose three different intents for simulating the feature bootstrapping scenario .", "entities": []}, {"text": "The MultiATIS++ dataset is highly imbalanced in terms of intent frequencies .", "entities": []}, {"text": "For instance , 74 % of the English training data has the intent atis_\ufb02ight and as many as 9 intents have less than 20 training samples .", "entities": []}, {"text": "The trend is similar for the non - English languages .", "entities": []}, {"text": "For choosing target intents for simulating the zero shot and few shot training data , we therefore consider the following three target intents : ( a ) atis_airfare , which is highly frequent , ( b ) atis_airline , which has medium frequency , and ( c ) atis_city which is scarce .", "entities": []}, {"text": "Preprocessing We remove the samples in the MultiATIS++ data for which the number of tokensand the number of slot values do not match.1We also only consider the \ufb01rst intent for the samples that have multiple intent annotations .", "entities": []}, {"text": "We show the data sizes after preprocessing in Table 1 .", "entities": []}, {"text": "Training setup To simulate the feature bootstrapping scenario , we consider only 20 samples ( few shot setup ) or no samples at all ( zero shot setup ) from the MultiATIS++ data for a speci\ufb01c target intent in a target language.2 Language setup We use English as the source language and consider 8 target languages ( Hindi , Turkish , German , French , Portuguese , Spanish , Japanese , Chinese ) simultaneously .", "entities": []}, {"text": "This encourages the model parameters to be shared across all the 9 languages including the source language English .", "entities": []}, {"text": "The purpose of this setup is to enable us to study the knowledge transfer across multiple target languages in addition to that from the source language .", "entities": []}, {"text": "We train a single model for paraphrase generation on all the languages as well as a single multi - lingual downstream IC / SL model .", "entities": [[6, 8, "TaskName", "paraphrase generation"]]}, {"text": "4.2 Models and Training Details Paraphrase generation training Since the training data is imbalanced , we balanced the training data by oversampling the intents to match the frequency of the most frequent intent.3For both the encoder and the decoder , the multi - head attention layers \u2019 hidden dimension was set to 128 and the position - wise feed forward layers \u2019 hidden dimension was set to 256 .", "entities": [[5, 7, "TaskName", "Paraphrase generation"], [41, 45, "MethodName", "multi - head attention"]]}, {"text": "The number of encoder and decoder layers was set to 3 each .", "entities": []}, {"text": "The number of heads was set to 8 .", "entities": []}, {"text": "Dropout of 0.1 was used in both the encoder and the decoder .", "entities": [[0, 1, "MethodName", "Dropout"]]}, {"text": "The model parameters were initialized with Xavier initialization ( Glorot and Bengio , 2010 ) .", "entities": [[6, 8, "MethodName", "Xavier initialization"]]}, {"text": "The model was trained using Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 5e-4 and a gradient clipping of 1 .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"], [16, 18, "HyperparameterName", "learning rate"], [22, 24, "MethodName", "gradient clipping"]]}, {"text": "The training was stopped when the development loss did not improve for 5 epochs .", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "Generating paraphrases For generating paraphrases in the target intent in the target language , 1This leads to removal of 0.6 % of the total samples .", "entities": []}, {"text": "2For cases that have less than 20 samples to pick from , we consider all the samples which are available .", "entities": []}, {"text": "3Experiments with the original imbalanced training data resulted in generating paraphrases which belongs to one of the frequent intents , even if the desired intent was one with a low frequency in the training data .", "entities": []}, {"text": "34Language Train Dev TestUnique intents Unique slots ( Train ) ( Train ) DE 4,487 490 892 17 79 EN 4,488 490 893 17 79 ES 4,484 489 813 17 79 FR 4,413 489 791 17 79 HI 1,495 160 893 16 70 JA 4,487 490 886 17 78 PT 4,478 489 892 17 79 TR 626 60 715 15 62 ZH 4,487 490 893 17 79 Table 1 : MultiATIS++ data statistics .", "entities": []}, {"text": "we used the slots appearing in the existing training data in the target intent .", "entities": []}, {"text": "We used greedy decoding and top - k sampling with k= 3;5;10and \u001c = 1:0;2:0 .", "entities": []}, {"text": "For a given input , we generated using the top - k random sampling three times with different random seeds .", "entities": [[19, 20, "DatasetName", "seeds"]]}, {"text": "We \ufb01nally combined all generations and ranked the candidates using the baseline downstream system \u2019s prediction probability .", "entities": []}, {"text": "The number of paraphrases that are selected is determined as in 3.3 , with 20 as the minimum .", "entities": []}, {"text": "Methods for comparison We compare our method against four alternatives : ( a ) Baseline : No data augmentation at all .", "entities": [[17, 19, "TaskName", "data augmentation"]]}, {"text": "The downstream model is trained using just the available seed examples for the target intent .", "entities": []}, {"text": "( b ) Oversampling : We oversample the samples per intent uniformly at random to match the size of the augmented training data using the proposed method .", "entities": []}, {"text": "This is only applicable to the few shot setup since for the zero shot setup , there are no existing samples in the target intent in the target language to sample from .", "entities": []}, {"text": "( c ) CV AE seq2seq model : We generate paraphrases using the CV AE seq2seq model by Malandrakis et al . ( 2019 ) .", "entities": [[4, 5, "MethodName", "AE"], [5, 6, "MethodName", "seq2seq"], [14, 15, "MethodName", "AE"], [15, 16, "MethodName", "seq2seq"]]}, {"text": "The original CV AE seq2seq model as proposed by Malandrakis et al .", "entities": [[3, 4, "MethodName", "AE"], [4, 5, "MethodName", "seq2seq"]]}, {"text": "( 2019 ) de\ufb01nes the set { domain , intent , slots } as the signature of an utterance and denotes the carrier phrases for a given signature to be paraphrases .", "entities": []}, {"text": "These carrier phrases are then used to create input - output pairs for the CV AE seq2seq model training .", "entities": [[15, 16, "MethodName", "AE"], [16, 17, "MethodName", "seq2seq"]]}, {"text": "Since the original formulation does not take into account the language of generation , we adapt the method for our case by de\ufb01ning the signature as the set { language , intent , slots } .", "entities": []}, {"text": "We set the model \u2019s hidden dimension to 128 , used the 100 - dimensional GloVe embeddings ( Pennington et al . , 2014 ) pretrained on Wikipedia , and trained the model without freezing embeddings using early stopping with a patience of 5 epochs by monitoring the development loss .", "entities": [[15, 17, "MethodName", "GloVe embeddings"], [37, 39, "MethodName", "early stopping"], [49, 50, "MetricName", "loss"]]}, {"text": "Finally we generated 100 carrier phrases for each carrier phrase input in the target intent in the target language .", "entities": []}, {"text": "Paraphrases were obtained by injecting the slot values to the generated carrier phrases .", "entities": []}, {"text": "The pool of all paraphrases was sorted using the baseline downstream system \u2019s prediction probabilities .", "entities": []}, {"text": "The CV AE seq2seq model was only applicable to the few shot setup since in the zero shot setup there are no existing carrier phrases in the target language in the target intent that can be used to sample from .", "entities": [[2, 3, "MethodName", "AE"], [3, 4, "MethodName", "seq2seq"]]}, {"text": "( d ) Machine translation : We augmented the translations generated from English using the MT+fastalign approach from the MultiATIS++ paper ( Xu et al . , 2020 ) .", "entities": [[3, 5, "TaskName", "Machine translation"]]}, {"text": "For the few shot setup , we added all the translated utterances except the ones that correspond to those utterances we already picked as the few shot samples .", "entities": []}, {"text": "For the zero shot setup , we added all the translated utterances .", "entities": []}, {"text": "Downstream training Unlike the paraphrase generation model training , we do not balance the simulated training data by oversampling based on intent .", "entities": [[4, 6, "TaskName", "paraphrase generation"]]}, {"text": "This choice was made to make sure that the original intent distribution was preserved for the downstream model training .", "entities": []}, {"text": "We used the BERT base multilingual cased model ( Devlin et al . , 2019)4and added an intent head and a slot head on top for joint intent classi\ufb01cation and slot labeling .", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "Each head uses a hidden size of 256 and ReLU activation .", "entities": [[9, 10, "MethodName", "ReLU"]]}, {"text": "The model was trained using Adam optimizer with a learning rate of 0.1 .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"], [9, 11, "HyperparameterName", "learning rate"]]}, {"text": "The training was stopped when the development semantic error rate ( Su et al . , 2018 ) did not improve for 3 epochs .", "entities": []}, {"text": "4.3 Intrinsic evaluation metrics We evaluate the quality of the generated paraphrases using the following metrics .", "entities": []}, {"text": "Let Sbe the set of input slot types and Gbe the set of generated slot types .", "entities": []}, {"text": "4https://github.com/google-research/ bert / blob / master / multilingual.md", "entities": []}, {"text": "35Few shot results : Generation method All retrieval Exact Partial F1 slot Jaccard Novelty Diversity score match match score index greedy 0.58 0.43 0.94 0.76 0.68 0.96 0 \u001c = 1.0 , topk = 3 0.54 0.4 0.9 0.71 0.64 0.98 0.92 \u001c = 1.0 , topk = 5 0.52 0.37 0.87 0.69 0.61 0.98 0.94 \u001c = 1.0 , topk = 10 0.52 0.38 0.87 0.69 0.61 0.98 0.96 \u001c = 2.0 , topk = 3 0.49 0.3 0.86 0.64 0.55 0.99 0.98 \u001c = 2.0 , topk = 5 0.44 0.23 0.86 0.59 0.49 0.99 1 \u001c = 2.0 , topk = 10 0.44 0.23 0.84 0.57 0.48 1 1 Zero shot results : Generation method All retrieval Exact Partial F1 slot Jaccard Novelty Diversity score match match score index greedy 0.41 0.22 0.78 0.56 0.48 1 0 \u001c = 1.0 , topk = 3 0.4 0.26 0.78 0.58 0.5 1 0.95 \u001c = 1.0 , topk = 5 0.4 0.22 0.78 0.56 0.48 1 0.96 \u001c = 1.0 , topk = 10 0.39 0.19 0.77 0.53 0.44 1 0.98 \u001c = 2.0 , topk = 3 0.38 0.16 0.76 0.5 0.41 1 0.99 \u001c = 2.0 , topk = 5 0.35 0.1 0.73 0.45 0.35 1 1 \u001c = 2.0 , topk = 10 0.34 0.12 0.72 0.43 0.34 1 1 Table 2 : Intrinsic evaluation scores for different generation methods in few shot and zero shot scenarios .", "entities": [[10, 11, "MetricName", "F1"], [27, 28, "DatasetName", "0"], [122, 123, "MetricName", "F1"], [139, 140, "DatasetName", "0"]]}, {"text": "LanguageFew shot Zero shot Lang .", "entities": []}, {"text": "detection score Novelty Diversity Lang .", "entities": []}, {"text": "detection score Novelty Diversity CV AE Ours CV AE Ours CV AE", "entities": [[5, 6, "MethodName", "AE"], [8, 9, "MethodName", "AE"], [11, 12, "MethodName", "AE"]]}, {"text": "Ours Ours DE 0.69 0.95 0.43 0.97 0.33 0.81 0.97 1 0.85 ES 0.71 0.91 0.48 0.98 0.44 0.82 0.93 1 0.83 FR 0.78 0.94 0.47 0.98 0.36 0.82 0.95 1 0.85 HI 0.69 0.97 0.5 0.97 0.28 0.81 0.97 1 0.81 JA 0.83 0.96 0.52 1 0.39 0.85 1 1 0.85 PT 0.5 0.75 0.55 0.97 0.38 0.81 0.86 1 0.85 TR 0.01 0.34 0.25 0.99 0.22 0.85 0.53 1 0.84 ZH 0.57 0.68 0.61 1 0.52 0.85 0.62 1 0.85 Table 3 : Intrinsic evaluation scores for different target languages in few shot and zero shot scenarios .", "entities": []}, {"text": "All retrieval score The all retrieval score rmeasures if all the input slots were retrieved in the generation .", "entities": []}, {"text": "r= ( 1ifjS\\Gj = jSj 0otherwise(2 )", "entities": []}, {"text": "Exact match The exact match score rmeasures if all the input slots and output slots exactly match ( Malandrakis et al . , 2019 ) .", "entities": [[0, 2, "MetricName", "Exact match"], [3, 5, "MetricName", "exact match"]]}, {"text": "r= ( 1ifS = G 0otherwise(3 )", "entities": []}, {"text": "Partial match The partial match score rmeasures if at least one output slot matches an input slot .", "entities": []}, {"text": "r= ( 1ifjS\\Gj>0 0otherwise(4 ) F1 slot score", "entities": [[5, 6, "MetricName", "F1"]]}, {"text": "The F1 slot score F1measures the set similarity between SandGusing precision and recall which are de\ufb01ned for sets as follows .", "entities": [[1, 2, "MetricName", "F1"]]}, {"text": "precision = jS\\Gj jGj , recall = jS\\Gj jSj(5)Jaccard index Jaccard index measures the set similarity between SandGas their intersection size divided by the union size .", "entities": []}, {"text": "Novelty LetPbe the set of paraphrases generated from a base utterance u. novelty = 1 jPjX u02P\u0010 1\u0000BLEU4 ( u;u0)\u0011 ( 6 ) Diversity", "entities": []}, {"text": "The diversity is computed using the generated paraphrases P. diversity = P u02P;u002P;u06 = u00\u0010 1\u0000BLEU4 ( u0;u00)\u0011 jPj\u0002(jPj\u00001 ) ( 7 ) Language detection score We are interested in quantifying if a generated paraphrase is in the target language .", "entities": []}, {"text": "We use langdetect5to compute p(lang = target lang ) .", "entities": []}, {"text": "Higher scores denote better language generation .", "entities": []}, {"text": "5https://github.com/Mimino666/ langdetect", "entities": []}, {"text": "36MethodIntent classi\ufb01cation DE ES FR HI JA PT TR ZH A VG .", "entities": []}, {"text": "Few shotBaseline 95.3 79.2 86.0 72.9 69.6 80.4 66.3 56.7 75.8 Oversampling 93.6 80.6 84.4 78.2 76.9 83.7 63.1 62.3 77.8 CV AE 94.1 84.7 83.7 71.7 63.8 85.6 69.1 55.8 76.1 MT 91.3 86.7 81.6 83.4 70.9 82.8 64.0 56.0 77.1 Paraphrasing 97.1 79.5 84.4 75.9 74.1 80.8 65.5 66.8 78.0 Zero shotBaseline 48.7 44.8 51.6 15.0 3.1 33.8 8.0 1.0 25.8 MT 75.6 87.8 77.9 86.4 8.3 82.8 57.6 23.8 62.5 Paraphrasing 56.9 50.5 51.6 34.3 7.4 26.9 12.3 4.0 30.5 Table 4 : Downstream intent classi\ufb01cation accuracies ( % ) .", "entities": [[22, 23, "MethodName", "AE"]]}, {"text": "Each score shown is the average score of 10 runs .", "entities": []}, {"text": "MethodSlot labeling DE ES FR HI JA PT TR ZH A VG .", "entities": []}, {"text": "Few shotBaseline 98.0 85.0 91.3 74.6 89.6 92.2 79.6 90.6 87.6 Oversampling 96.2 84.6 91.7 76.9 89.9 90.0 81.3 90.3 87.6 CV AE 97.3 83.5 90.0 75.8 90.8 91.6 82.1 89.6 87.6 MT 95.0 78.8 90.9 73.0 90.8 82.9 77.9 88.8 84.8", "entities": [[22, 23, "MethodName", "AE"]]}, {"text": "Paraphrasing 97.2 80.8 89.7 76.2 90.2 91.3 78.6 91.6 86.9 Zero shotBaseline 93.9 84.5 89.3 72.5 89.1 88.7 77.3 87.8 85.4 MT 92.0 79.9 88.5 73.3 92.1 82.1 76.2 88.7 84.1", "entities": []}, {"text": "Paraphrasing 93.1 84.1 90.5 70.3 89.5 91.5 77.6 87.2 85.5 Table 5 : Downstream slot labeling F1 scores ( % ) .", "entities": [[16, 17, "MetricName", "F1"]]}, {"text": "Each score shown is the average score of 10 runs .", "entities": []}, {"text": "5 Experimental results 5.1 Intrinsic Evaluation For both the few shot and zero shot setups , the paraphrases used for intrinsic evaluation are generated in the target intent and the target language only .", "entities": []}, {"text": "For the top - k sampling based generation , we generate for each input three times with different random seeds and compute novelty and diversity scores .", "entities": [[19, 20, "DatasetName", "seeds"]]}, {"text": "Table 2 shows intrinsic evaluation results for different generation methods .", "entities": []}, {"text": "For the few shot setup , the all retrieval , exact match , partial match , F1 slot and Jaccard index scores decrease upon increasing top - k and temperature .", "entities": [[10, 12, "MetricName", "exact match"], [16, 17, "MetricName", "F1"]]}, {"text": "The highest scores for the above metrics are obtained for the greedy generation , which indicates that the generated slot types are most similar to the input slot types in that case .", "entities": []}, {"text": "However , it is the opposite for the novelty and diversity metrics where the scores are higher with larger top - k and temperatures .", "entities": []}, {"text": "For the zero shot setup , the overall trend is similar to the few shot setup .", "entities": []}, {"text": "The slot similarity based metrics are lower in general , which indicates that even as little as 20 samples in the few shot setup improve the generation of desired slots .", "entities": []}, {"text": "The novelty scores for the zero shot setup are 1 as we would expect .", "entities": []}, {"text": "In Table 3 , we show that the intrinsic evaluation results using the proposed approach are consistently better than the CV AE seq2seq paraphrase generation model ( Malandrakis et al . , 2019 ) .", "entities": [[21, 22, "MethodName", "AE"], [22, 23, "MethodName", "seq2seq"], [23, 25, "TaskName", "paraphrase generation"]]}, {"text": "The language detection score varies across languages , which may be due to the vocabulary overlap between languages , e.g. , San Francisco appears in both English and German utterances .", "entities": []}, {"text": "Interestingly we also observe code switching , i.e. mixedlanguage generations , while using our approach .", "entities": []}, {"text": "5.2 Downstream Evaluation We evaluate the downstream intent classi\ufb01cation using accuracy and the slot labeling using F1 score .", "entities": [[10, 11, "MetricName", "accuracy"], [16, 18, "MetricName", "F1 score"]]}, {"text": "Since we are interested in measuring the variation in scores for the target intents , we only report the scores for the test samples in the target intents in Tables 4 and 5 .", "entities": []}, {"text": "We run each downstream training experiment 10 times and report the mean scores for each language and also the average across languages in the A VG column in Tables 4 and 5 .", "entities": []}, {"text": "We are also interested in tracking the scores for the test samples having intents other than the target intents since we need to ensure that the scores on the other intents does not go down .", "entities": []}, {"text": "We found that the effect on the scores ( both intent classi\ufb01cation and slot labeling ) for the other intents is negligible using paraphrasing and other methods.6 In Tables 4 and 5 , our paraphrasing results outperform the baseline scores on average .", "entities": []}, {"text": "In the few shot setup , our paraphrasing approach outperforms the CV AE seq2seq approach in 6 ( DE , ES , FR , HI , JA , ZH ) out of 8 languages in intent classi\ufb01cation and overall obtains an improvement of 1.9 % intent classi\ufb01cation accuracy across all target languages .", "entities": [[12, 13, "MethodName", "AE"], [13, 14, "MethodName", "seq2seq"], [47, 48, "MetricName", "accuracy"]]}, {"text": "6The maximum drop in score was less than 1 % absolute .", "entities": []}, {"text": "37 Input airline and flight number from   columbus to minneapolis DE Zeige mir alle Fluglinien , die von T oronto nach Boston fliegen ES Qu\u00e9 aerol\u00edneas vuelan desde Atlanta hasta Filadelfia FR Quelles compagnies volent de T oronto \u00e0 San Francisco HI \u091c\u094b", "entities": []}, {"text": "\u090f\u092f\u0930\u0932\u093e\u0907\u0928   \u0921", "entities": []}, {"text": "\u0947", "entities": []}, {"text": "\u0000\u0930", "entities": []}, {"text": "", "entities": []}, {"text": "\u0938 \u0947   \u0905\u091f\u0932\u093e \u0902 \u091f\u093e   \u0924\u0915   \u0909\u095c\u093e\u0928   \u092d\u0930\u0924\u0940   \u0939 \u0948 JA\u30c7\u30f3\u30d0\u30fc   \u304b\u3089   \u30d4\u30c3\u30c4\u30d0\u30fc\u30b0   \u307e\u3067 \u98db \u3093\u3067\u3044\u308b \u822a\u7a7a\u4f1a\u793e \u3092 \u6559 \u3048\u3066 PT Mostre todas companhias a\u00e9reas voam de Denver TR hangi havayolu boston pittsbur gh ' a ucar ZH\u4ece \u4e39\u4f5b   \u5230   \u65e7\u91d1\u5c71   \u822a\u73ed\u7684\u822a\u7a7a\u516c\u53f8Table 6 : Examples of paraphrases generated using the multilingual paraphrase generation model for airline and slots fromloc andtoloc .", "entities": [[62, 64, "TaskName", "paraphrase generation"]]}, {"text": "The paraphrases shown are cherry picked from a set of generations .", "entities": []}, {"text": "Both oversampling and MT approaches are competitive .", "entities": []}, {"text": "Oversampling performs the best for JA whereas MT performs the best for ES and HI .", "entities": []}, {"text": "Our paraphrasing approach results in the best intent classi\ufb01cation scores overall ( 78 % ) .", "entities": []}, {"text": "In terms of slot F1 scores , we see mixed results with no clear best method ( baseline , oversampling and CV AE all result in 87.6 % F1 score ) .", "entities": [[4, 5, "MetricName", "F1"], [22, 23, "MethodName", "AE"], [28, 30, "MetricName", "F1 score"]]}, {"text": "Notably , the MT approach results in the lowest overall slot F1 score of just 84.8 % on average .", "entities": [[11, 13, "MetricName", "F1 score"]]}, {"text": "In the zero shot setup , the MT approach outperforms our paraphrasing approach by a large margin in intent classi\ufb01cation ( 62.5 % ) .", "entities": []}, {"text": "However we note that the paraphrasing approach requires no dependencies on other models or other data , unlike the MT approach which requires a parallel corpus to train the MT model .", "entities": []}, {"text": "In terms of slot F1 scores , our paraphrasing approach and the baseline approach both result in almost similar overall scores ( 85.5 % and 85.4 % ) , both higher than the MT approach .", "entities": [[4, 5, "MetricName", "F1"]]}, {"text": "The lower slot F1 scores using the MT approach in few and zero shot setups indicate that the fast align method to align slots in source and translation might result in noisy training data affecting the SL model .", "entities": [[3, 4, "MetricName", "F1"]]}, {"text": "5.3 Examples Paraphrases generated in different languages for a given input are shown in Table 6 .", "entities": []}, {"text": "The intent isairline and the slots are fromloc.city_name for columbus andtoloc.city_name forminneapolis .", "entities": []}, {"text": "For this intent and the slots , the generated paraphrase in German ( translated to English ) is Show me all the airlines that \ufb02y from Toronto to Boston .", "entities": []}, {"text": "The desired intent , that is airline is realized in the gener - ated paraphrase .", "entities": []}, {"text": "Additionally , Toronto andBoston are the slot values respectively for the slot types fromloc.city_name andtoloc.city_name .", "entities": []}, {"text": "For Spanish , the generated paraphrase ( translated to English ) isWhich Airlines Fly from Atlanta to Philadelphia .", "entities": []}, {"text": "Theairline intent is realized in the generated paraphrase and also Atlanta andPhiladelphia are the slot values produced associated with the desired slot types .", "entities": []}, {"text": "As illustrated by the examples , the model is free to pick a speci\ufb01c slot value during generation , leading to variations across languages , but all are consistent with the slot type .", "entities": []}, {"text": "6 Conclusion In this paper , we proposed a multilingual paraphrase generation model that can be used for feature bootstrapping with or without seed data in the target language .", "entities": [[10, 12, "TaskName", "paraphrase generation"]]}, {"text": "In addition to generating a paraphrase , the model also generates the associated slot labels , enabling the generation to be used directly for data augmentation to existing training data .", "entities": [[24, 26, "TaskName", "data augmentation"]]}, {"text": "Our method is language agnostic and scalable , with no dependencies on pre - trained models or additional data .", "entities": []}, {"text": "We validate our method using experiments on the MultiATIS++ dataset containing utterances spanning 9 languages .", "entities": []}, {"text": "Intrinsic evaluation shows that paraphrases generated using our approach have higher novelty and diversity in comparison to CV AE seq2seq based paraphrase generation .", "entities": [[18, 19, "MethodName", "AE"], [19, 20, "MethodName", "seq2seq"], [21, 23, "TaskName", "paraphrase generation"]]}, {"text": "Additionally , downstream evaluation shows that using the generated paraphrases for data augmentation results in improvements over baseline and related techniques in a wide range of languages and setups .", "entities": [[11, 13, "TaskName", "data augmentation"]]}, {"text": "To the best of our knowledge , this is the \ufb01rst successful exploration of generating paraphrases for SLU in a cross - lingual setup .", "entities": []}, {"text": "In the future , we would like to explore strategies to exploit monolingual data in the target languages to further re\ufb01ne the paraphrase generation .", "entities": [[22, 24, "TaskName", "paraphrase generation"]]}, {"text": "We would also like to leverage pre - trained multilingual text - to - text models such as mT5 ( Xue et al . , 2020 ) for multilingual paraphrase generation in the dialog system domain .", "entities": [[18, 19, "MethodName", "mT5"], [29, 31, "TaskName", "paraphrase generation"]]}, {"text": "Acknowledgements We would like to thank our anonymous reviewers for their thoughtful comments and suggestions that improved the \ufb01nal version of this paper .", "entities": []}, {"text": "38References Samuel R. Bowman , Luke Vilnis , Oriol Vinyals , Andrew Dai , Rafal Jozefowicz , and Samy Bengio .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Generating sentences from a continuous space .", "entities": []}, {"text": "In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning , pages 10\u201321 , Berlin , Germany .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zhiyu Chen , Harini Eavani , Wenhu Chen , Yinyin Liu , and William Yang Wang .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Few - shot NLG with pre - trained language model .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 183\u2013190 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Eunah Cho , He Xie , and William M. Campbell .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Paraphrase generation for semi - supervised learning in NLU .", "entities": [[0, 2, "TaskName", "Paraphrase generation"]]}, {"text": "In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation , pages 45\u201354 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Quynh Do and Judith Gaspers .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Cross - lingual transfer learning with data selection for large - scale spoken language understanding .", "entities": [[0, 4, "TaskName", "Cross - lingual transfer"], [12, 15, "TaskName", "spoken language understanding"]]}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 1455\u20131460 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Tobias Falke , Markus Boese , Daniil Sorokin , Caglar Tirkaz , and Patrick Lehnen .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Leveraging user paraphrasing behavior in dialog systems to automatically collect annotations for long - tail utterances .", "entities": []}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics : Industry Track , pages 21\u201332 , Online .", "entities": []}, {"text": "International Committee on Computational Linguistics .", "entities": []}, {"text": "Angela Fan , Mike Lewis , and Yann Dauphin .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Hierarchical neural story generation .", "entities": [[2, 4, "TaskName", "story generation"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 889\u2013898 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Judith Gaspers , Penny Karanasou , and Rajen Chatterjee .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Selecting machine - translated data for quick bootstrapping of a natural language understanding system .", "entities": [[10, 13, "TaskName", "natural language understanding"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : HumanLanguage Technologies , Volume 3 ( Industry Papers ) , pages 137\u2013144 , New Orleans - Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xavier Glorot and Yoshua Bengio .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Understanding the dif\ufb01culty of training deep feedforward neural networks .", "entities": []}, {"text": "In Proceedings of the Thirteenth International Conference on Arti\ufb01cial Intelligence and Statistics , volume 9 of Proceedings of Machine Learning Research , pages 249\u2013256 , Chia Laguna Resort , Sardinia , Italy .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Ankush Gupta , Arvind Agarwal , Prawaan Singh , and Piyush Rai . 2018 .", "entities": []}, {"text": "A deep generative framework for paraphrase generation .", "entities": [[5, 7, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 32 .", "entities": []}, {"text": "Shailza Jolly , Tobias Falke , Caglar Tirkaz , and Daniil Sorokin .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Data - ef\ufb01cient paraphrase generation to bootstrap intent classi\ufb01cation and slot labeling for new features in task - oriented dialog systems .", "entities": [[3, 5, "TaskName", "paraphrase generation"]]}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics : Industry Track , pages 10\u201320 , Online .", "entities": []}, {"text": "International Committee on Computational Linguistics .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Ba . 2014 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "Cite arxiv:1412.6980Comment : Published as a conference paper at the 3rd International Conference for Learning Representations , San Diego , 2015 .", "entities": []}, {"text": "Zichao Li , Xin Jiang , Lifeng Shang , and Hang Li .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Paraphrase generation with deep reinforcement learning .", "entities": [[0, 2, "TaskName", "Paraphrase generation"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3865\u20133878 , Brussels , Belgium .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zichao Li , Xin Jiang , Lifeng Shang , and Qun Liu . 2019 .", "entities": []}, {"text": "Decomposable neural paraphrase generation .", "entities": [[2, 4, "TaskName", "paraphrase generation"]]}, {"text": "InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3403\u20133414 , Florence , Italy .", "entities": [[16, 17, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Samuel Louvan and Bernardo Magnini . 2020 .", "entities": []}, {"text": "Recent neural methods on slot \ufb01lling and intent classi\ufb01cation for task - oriented dialogue systems : A survey .", "entities": [[10, 15, "TaskName", "task - oriented dialogue systems"]]}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics , pages 480 \u2013 496 , Barcelona , Spain ( Online ) .", "entities": []}, {"text": "International Committee on Computational Linguistics .", "entities": []}, {"text": "Nikolaos Malandrakis , Minmin Shen , Anuj Goyal , Shuyang Gao , Abhishek Sethi , and Angeliki Metallinou . 2019 .", "entities": []}, {"text": "Controlled text generation for data augmentation in intelligent arti\ufb01cial agents .", "entities": [[1, 3, "TaskName", "text generation"], [4, 6, "TaskName", "data augmentation"]]}, {"text": "In Proceedings of the 3rd Workshop on Neural Generation and Translation , pages 90\u201398 , Hong Kong . Association for Computational Linguistics .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Deepak Muralidharan , Justine Kao , Xiao Yang , Lin Li , Lavanya Viswanathan , Mubarak Seyed Ibrahim , Kevin Luikens , Stephen Pulman , Ashish Garg , Atish", "entities": []}, {"text": "39Kothari , and Jason Williams .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Leveraging User Engagement Signals For Entity Labeling in a Virtual Assistant .", "entities": []}, {"text": "arXiv , 1909.09143 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "GloVe : Global vectors for word representation .", "entities": [[0, 1, "MethodName", "GloVe"]]}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532\u20131543 , Doha , Qatar .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alec Radford , Jeff Wu , Rewon Child , David Luan , Dario Amodei , and Ilya Sutskever . 2019 .", "entities": []}, {"text": "Language models are unsupervised multitask learners .", "entities": []}, {"text": "Chengwei Su , Rahul Gupta , Shankar Ananthakrishnan , and Spyros Matsoukas .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "A re - ranker scheme for integrating large scale nlu models .", "entities": []}, {"text": "In 2018 IEEE Spoken Language Technology Workshop ( SLT ) , pages 670\u2013676 .", "entities": []}, {"text": "Gokhan Tur and Renato De Mori . 2011 .", "entities": []}, {"text": "Spoken Language Understanding : Systems for Extracting Semantic Information from Speech .", "entities": [[0, 3, "TaskName", "Spoken Language Understanding"]]}, {"text": "John Wiley and Sons .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141 ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems , volume 30 .", "entities": []}, {"text": "Curran Associates , Inc. H. Weld , X. Huang , S. Long , J. Poon , and S. C. Han . 2021 .", "entities": []}, {"text": "A survey of joint intent detection and slot\ufb01lling models in natural language understanding .", "entities": [[4, 6, "TaskName", "intent detection"], [10, 13, "TaskName", "natural language understanding"]]}, {"text": "arXiv , 2101.08091 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Sean Welleck , Ilia Kulikov , Stephen Roller , Emily Dinan , Kyunghyun Cho , and Jason Weston .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Neural text generation with unlikelihood training .", "entities": [[1, 3, "TaskName", "text generation"]]}, {"text": "In International Conference on Learning Representations .", "entities": []}, {"text": "Congying Xia , Caiming Xiong , Philip Yu , and Richard Socher .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Composed variational natural language generation for few - shot intents .", "entities": []}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 3379\u20133388 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Weijia Xu , Batool Haider , and Saab Mansour . 2020 .", "entities": []}, {"text": "End - to - end slot alignment and recognition for crosslingual NLU .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 5052\u20135063 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Linting Xue , Noah Constant , Adam Roberts , Mihir Kale , Rami Al - Rfou , Aditya Siddhant , Aditya Barua , and Colin Raffel . 2020 .", "entities": [[6, 7, "MethodName", "Adam"]]}, {"text": "mT5 :", "entities": [[0, 1, "MethodName", "mT5"]]}, {"text": "A massively multilingual pre - trained text - to - text transformer .", "entities": []}]