[{"text": "Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages , pages 216\u2013221 April 20 , 2021 \u00a9 2021 Association for Computational Linguistics216ZYJ123@DravidianLangTech - EACL2021 : Offensive Language Identi\ufb01cation based on XLM - RoBERTa with DPCNN Yingjia Zhao Yunnan University / Yunnan , P.R. China zyj1309700118@gmail.comXin", "entities": [[34, 35, "MethodName", "XLM"], [36, 37, "MethodName", "RoBERTa"]]}, {"text": "Tao Yunnan University / Yunnan , P.R. China taoxinwy@126.com", "entities": []}, {"text": "Abstract", "entities": []}, {"text": "The development of online media platforms has given users more opportunities to post and comment freely , but the negative impact of offensive language has become increasingly apparent .", "entities": []}, {"text": "It is very necessary for the automatic identi\ufb01cation system of offensive language .", "entities": []}, {"text": "This paper describes our work on the task of Offensive Language Identi\ufb01cation in Dravidian language - EACL 2021 .", "entities": []}, {"text": "To complete this task , we propose a system based on the multilingual model XLM - Roberta and DPCNN .", "entities": [[14, 15, "MethodName", "XLM"]]}, {"text": "The test results on the of\ufb01cial test data set con\ufb01rm the effectiveness of our system .", "entities": []}, {"text": "The weighted average F1 - score of Kannada , Malayalam , and Tamil language are 0.69 , 0.92 , and 0.76 respectively , ranked 6th , 6th , and 3rd .", "entities": [[3, 6, "MetricName", "F1 - score"]]}, {"text": "1 Introduction With the development of the information society , people have become accustomed to uploading content on social media platforms in the form of text , pictures , or videos .", "entities": []}, {"text": "At the same time , they also comment on the content uploaded by other users and interact with each other , thus increasing the activity of social media platforms ( Thavareesan and Mahesan , 2019 , 2020a , b ) .", "entities": []}, {"text": "Inevitably , however , some users will post offensive posts or comments .", "entities": []}, {"text": "The use of offensive discourse is a kind of impolite phenomenon which has negative effects on the civilization of the network community ( Chakravarthi , 2020 ) .", "entities": []}, {"text": "It usually has the characteristics of causing con\ufb02icts and the purpose of publishing intentionally .", "entities": []}, {"text": "The publisher of offensive language may use reproach , sarcasm , swear and other language means to achieve intentional offense , and express a variety of intentions , such as disturbing , provoking , and expressing negative emotions ( Chakravarthi and Muralidaran , 2021 ; Suryawanshi and Chakravarthi , 2021 ) .", "entities": []}, {"text": "Most people will take measures to respondto offensive words .", "entities": []}, {"text": "The way to respond to the direct con\ufb02ict of offensive words is mainly rhetorical questions , swear , sarcasm and threat , so as to express dissatisfaction , deny and satirize the other party and provoke the other party .", "entities": []}, {"text": "This will further cause con\ufb02icts and destroy the harmony of the network environment .", "entities": []}, {"text": "Many social media platforms use a content review process , in which human reviewers check users \u2019 comments for offensive language and other infractions , and which comments have been removed from the platform because of the violation ( Mandl et al . , 2020 ) .", "entities": []}, {"text": "It is up to the moderator to decide which comments will be removed from the platform due to violations and which ones will be kept .", "entities": []}, {"text": "As the number of network users increases and user activity increases , the manual approach is undoubtedly inef\ufb01cient .", "entities": []}, {"text": "Therefore , the automatic detection and identi\ufb01cation of offensive content are very necessary .", "entities": []}, {"text": "However , offensive words often depend on the emotions and psychology of the listener , and some seemingly innocuous words can be potentially offensive , and words that often seem offensive are watered down by the emotions of the listener .", "entities": []}, {"text": "This kind of language phenomenon is not uncommon in real life , either unintentionally or deliberately used to achieve the speaker \u2019s expected purpose , which is a challenging work for the current detection system .", "entities": []}, {"text": "Our team takes part in the shared task of Offensive Language Identi\ufb01cation in Dravidian Languages - EACL 2021 ( Chakravarthi et al . , 2021 , 2020a , b ; Hande et al . , 2020 ) .", "entities": []}, {"text": "This is a classi\ufb01cation task at the comment / post level .", "entities": []}, {"text": "The goal of this task is to identify offensive language content of the code - mixed dataset of comments / posts in Dravidian Languages ( ( Tamil - English , MalayalamEnglish , and Kannada - English ) ) collected from social media .", "entities": []}, {"text": "Tamil language is the oldest language in Indian languages , Malayalam and Kannada evolved", "entities": []}, {"text": "217from Tamil language .", "entities": []}, {"text": "For a comment on Youtube , the system must classify it into not - offensive , offensive - untargeted , offensive - targeted - individual , offensive - targeted - group , offensive - targeted - other , or not - in - indented - language .", "entities": []}, {"text": "In our approach , the multilingual model XLMRoBERTa and DPCNN are combined to carry out the classi\ufb01cation task .", "entities": []}, {"text": "This method can combine the advantages of the two models to achieve a better classi\ufb01cation effect .", "entities": []}, {"text": "The rest of the paper is divided into the following parts .", "entities": []}, {"text": "In the second part , we introduce the relevant work in this \ufb01eld , which involves offensive language detection and text classi\ufb01cation methods .", "entities": []}, {"text": "In the third part , we introduce the model structure and the composition of our training data .", "entities": []}, {"text": "The fourth part introduces our experimental setup and results .", "entities": []}, {"text": "The \ufb01fth part is the conclusion .", "entities": []}, {"text": "2 Related Work Due to the harm of offensive language to the network environment , the identi\ufb01cation of offensive language has been carried out for a long time .", "entities": []}, {"text": "Research so far has focused on automating the decision - making process in the form of supervised machine learning for classi\ufb01cation tasks ( Sun et al . , 2019 ) .", "entities": []}, {"text": "As far back as 2012 , Chen et al .", "entities": []}, {"text": "( 2012 ) proposed a lexical syntactic feature ( LSF ) framework to detect offensive content in social media , distinguished the roles of derogatory / profane and obscenity in identifying offensive content , and introduced handwritten syntax rules to identify abusive harassment .", "entities": []}, {"text": "In contrast to the start - to - end training model , Howard and Ruder ( 2018)proposed an effective transfer learning method , Universal Language Model Tuning ( ULMFIT ) , which can be applied to any task in natural language processing , and has shown signi\ufb01cant results on six text classi\ufb01cation tasks .", "entities": [[19, 21, "TaskName", "transfer learning"], [28, 29, "MethodName", "ULMFIT"]]}, {"text": "Subsequently , Abdellatif and Elgammal ( 2020 ) used the ULMFiT transfer learning method to train forward and backward models on Arabic datasets and ensemble the results to perform an offensive language detection task .", "entities": [[10, 11, "MethodName", "ULMFiT"], [11, 13, "TaskName", "transfer learning"]]}, {"text": "Although English is currently one of the most commonly spoken languages in the world , work is ongoing to identify the offensive language in other languages that are less widely spoken .", "entities": []}, {"text": "Pitenis et al .", "entities": []}, {"text": "( 2020 ) tested the performance of several traditional machine learning models and deep learning models on an offensive language dataset of Greek , and the best results were achieved with the attention modelof LSTM and GRU .", "entities": [[34, 35, "MethodName", "LSTM"], [36, 37, "MethodName", "GRU"]]}, {"text": "Ozdemir and Yeniterzi ( 2020 ) ensembled CNN - LSTM , BILSTM - Attention , and BERT three models , combined with pre - trained word embedding on Twitter to complete the identi\ufb01cation task of offensive Turkish language , and achieved a good result .", "entities": [[9, 10, "MethodName", "LSTM"], [11, 12, "MethodName", "BILSTM"], [16, 17, "MethodName", "BERT"]]}, {"text": "A key challenge in automatically detecting hate speech on social media is to separate hate speech from other offensive languages .", "entities": [[6, 8, "DatasetName", "hate speech"], [14, 16, "DatasetName", "hate speech"]]}, {"text": "Davidson et al .", "entities": []}, {"text": "( 2017 ) used the crowd - sourced hate speech lexicon to collect tweets containing hate speech keywords .", "entities": [[8, 10, "DatasetName", "hate speech"], [15, 17, "DatasetName", "hate speech"]]}, {"text": "They trained a multi - class classi\ufb01er to reliably distinguish hate speech from other offensive languages , and found that racist and homophobic tweets were more likely to be classi\ufb01ed as hate speech , but sexist tweets were generally classi\ufb01ed as offensive .", "entities": [[10, 12, "DatasetName", "hate speech"], [31, 33, "DatasetName", "hate speech"]]}, {"text": "Razavi et al .", "entities": []}, {"text": "( 2010 ) proposed to extract features at different conceptual levels and apply multilevel classi\ufb01cation for offensive language detection .", "entities": []}, {"text": "The system leverages a variety of statistical models and rule - based patterns , combined with an auxiliary weighted pattern library , to improve accuracy by matching text with its graded entries .", "entities": [[24, 25, "MetricName", "accuracy"]]}, {"text": "Pitsilis et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) proposed the ensemble of a recursive neural network ( RNN ) classi\ufb01er , which combines various characteristics related to user - related information , such as the user \u2019s sexist or racist tendencies , and was then fed to the classi\ufb01er as input along with a word frequency vector derived from the text content .", "entities": []}, {"text": "When there is a large amount of labeled data , increasing the size and parameters of the model will de\ufb01nitely improve the performance of the model .", "entities": []}, {"text": "However , when the amount of training is relatively small , the large - scale model may not be able to achieve good results , so solving the problem of model training under the condition of a small amount of target data has become a research hotspot .", "entities": []}, {"text": "Sun et al .", "entities": []}, {"text": "( 2019 ) proposed a Hierarchical Attention Prototype Network ( HAPN ) for fewshot text classi\ufb01cation , which designed multiple cross - concerns of a feature layer , word layer , and instance layer for the model to enhance the expressive power of semantic space .", "entities": []}, {"text": "The model was validated on two standard reference text classi\ufb01cation datasets , Fewrel and CSID .", "entities": [[12, 13, "DatasetName", "Fewrel"]]}, {"text": "Prettenhofer and Stein ( 2010 ) built on structural correspondence learning , using untagged documents and simple word translation to induce task - speci\ufb01c , cross - language word correspondence .", "entities": [[17, 19, "TaskName", "word translation"]]}, {"text": "English was used as the source language and German , French , and Japanese", "entities": []}, {"text": "218were used as the target language to conduct the experiment in the \ufb01eld of cross - language sentiment classi\ufb01cation .", "entities": []}, {"text": "Using English data , Ranasinghe and Zampieri ( 2020 ) trained the model by applying cross - language contextual word embedding and transfer learning methods , and then predicted the effect of cross - language contextual embedding and transfer learning on this task in less resourceintensive languages such as Bengali , Hindi , and Spanish .", "entities": [[22, 24, "TaskName", "transfer learning"], [38, 40, "TaskName", "transfer learning"]]}, {"text": "3 Data and Methodology 3.1 Data description We count the number of each type of tag in the training set and the validation set , and obtain the data distribution of Not - offensive , offensive - untargeted , offensive - targeted - individual , offensive - targeted - group , offensive - targeted - other , and Not - in - indented - language in Tamil , Malayalam , and Kannada .", "entities": []}, {"text": "as shown in Table 1 . 3.2 Why XLM - RoBERTa Compared with the original BERT model , XLMRoBERTa increases the number of languages and the number of training data sets .", "entities": [[8, 9, "MethodName", "XLM"], [10, 11, "MethodName", "RoBERTa"], [15, 16, "MethodName", "BERT"]]}, {"text": "Speci\ufb01cally , a preprocessed CommonCrawl dataset of more than 2 TB based on 100 languages is used to train crosslanguage representations in a self - supervised manner .", "entities": []}, {"text": "This includes generating new unlabeled corpora for low - resource languages and expanding the amount of training data available for these languages by two orders of magnitude .", "entities": []}, {"text": "In the \ufb01netuning period , the multi - language tagging data is used based on the ability of the multi - language model to improve the performance of the downstream tasks .", "entities": []}, {"text": "This enables XLM - RoBERTa to achieve state - of - the - art results in cross - language benchmarks while exceeding the performance of the single - language BERT model for each language .", "entities": [[2, 3, "MethodName", "XLM"], [4, 5, "MethodName", "RoBERTa"], [29, 30, "MethodName", "BERT"]]}, {"text": "Tune the parameters of the model to address cases where extending the model to more languages using cross - language migration limits the ability of the model to understand each language .", "entities": []}, {"text": "The XLMRoBERTa parameter changes include up - sampling of low - resource languages during training and vocabulary building , generating a larger shared vocabulary , and increasing the overall model to 550 million parameters.3.3 XLM - RoBERTa with DPCNN In this task , we combined XLM - RoBERTa with DPCNN ( Johnson and Zhang , 2017 ) to make the whole model more suitable for the downstream classi\ufb01cation task .", "entities": [[34, 35, "MethodName", "XLM"], [36, 37, "MethodName", "RoBERTa"], [45, 46, "MethodName", "XLM"], [47, 48, "MethodName", "RoBERTa"]]}, {"text": "DPCNN(Deep Pyramid Convolutional Neural Networks ) is a kind of deep word level CNN structure , the calculation amount of each layer of the structure decreases exponentially .", "entities": []}, {"text": "DPCNN simply stacks the convolution module and negative sampling layer .", "entities": [[4, 5, "MethodName", "convolution"]]}, {"text": "The computation volume of the whole model is limited to less than two times the number of convolution blocks .", "entities": [[17, 18, "MethodName", "convolution"]]}, {"text": "At the same time , the pyramid structure also enables the model to discover long - term dependencies in the text .", "entities": []}, {"text": "In a common classi\ufb01cation task , the last hidden state of the \ufb01rst token of the sequence ( CLS token ) , namely the original output of XLM - Roberta ( Pooler output ) , is further processed through the linear layer and the tanh activation function for classi\ufb01cation purposes .", "entities": [[27, 28, "MethodName", "XLM"], [40, 42, "MethodName", "linear layer"], [44, 46, "MethodName", "tanh activation"]]}, {"text": "To obtain richer semantic information features of the model and improve the performance of the model , we \ufb01rst processed the output of the last three layers of XLM - RoBERTa through DPCNN , and then concatenate it with the original output of XLM - RoBERTa ( Pooler output ) to get a new and more effective feature vector , and then input this feature vector into the classi\ufb01er for classi\ufb01cation .", "entities": [[28, 29, "MethodName", "XLM"], [30, 31, "MethodName", "RoBERTa"], [43, 44, "MethodName", "XLM"], [45, 46, "MethodName", "RoBERTa"]]}, {"text": "As shown in Figure 1 . 4 Experiment and results 4.1 Experiment setting In this experiment , the pre - training model I used was XLM - RoBERTa - base .", "entities": [[25, 26, "MethodName", "XLM"], [27, 28, "MethodName", "RoBERTa"]]}, {"text": "After adding the DPCNN module , we began to set the experimental parameters .", "entities": []}, {"text": "We set the learning rate as 2e-5 , the maximum sequence length is 256 , and the gradient steps are set to 4 .", "entities": [[3, 5, "HyperparameterName", "learning rate"]]}, {"text": "The batch size is set to 32 , as shown in table 2 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}, {"text": "In the training process , we used \ufb01ve - fold strati\ufb01ed cross - validation to make the proportion of data of each category in each subsample the same as that in the original data and \ufb01nally obtained the optimal result through the voting ( Onan et al . , 2016 ) system , as shown in Figure 2 . 4.2 Results After the evaluation by the organizer , we obtained the weighted average F1 - score in the three languages , as shown in table 3 .", "entities": [[73, 76, "MetricName", "F1 - score"]]}, {"text": "Our team \u2019s F1 - score is 0.69 , ranked 6th place for the Kannada language .", "entities": [[3, 6, "MetricName", "F1 - score"]]}, {"text": "For the Malayalam language , our team \u2019s F1 - score", "entities": [[8, 11, "MetricName", "F1 - score"]]}, {"text": "219Kannada Malayalam Tamil Label Train Validation Train Validation Train Validation Not - offensive 3544 426 14153 1779 25405 3193 Not - in - indented - language 1522 191 6205 163 1454 172 offensive - targeted - individual 487 66 239 24 2343 307 offensive - targeted - group 329 45 140 13 2557 295 offensive - untargeted 212 33 191 20 2906 356 offensive - targeted - other 123 16 0 0 454 65 Table 1 : Train and Validation datasets description .", "entities": [[70, 71, "DatasetName", "0"], [71, 72, "DatasetName", "0"]]}, {"text": "Figure 1 : Schematic overview of the architecture of our model maximum sequence length learning rate 256 2e-5 gradient steps batch size 4 32 Table 2 : Details of the parameters Kan Mal Tam Best F1 - score 0.75 0.97 0.78 Our Precision 0.65 0.91 0.75 Our Recall 0.74 0.94 0.77 Our F1 - score 0.69 0.92 0.76", "entities": [[14, 16, "HyperparameterName", "learning rate"], [20, 22, "HyperparameterName", "batch size"], [35, 38, "MetricName", "F1 - score"], [42, 43, "MetricName", "Precision"], [47, 48, "MetricName", "Recall"], [52, 55, "MetricName", "F1 - score"]]}, {"text": "Rank", "entities": []}, {"text": "6", "entities": []}, {"text": "6 3 Table 3 : the results of our methods .", "entities": []}, {"text": "Figure 2 : V oting system is 0.92 ranked 6th place , and for the Tamil language , our team \u2019s F1 - score is 0.76 ranked 3rd place .", "entities": [[21, 24, "MetricName", "F1 - score"]]}, {"text": "5 Conclusion In this paper , we describe our system in the task of offensive language identi\ufb01cation for Tamil , Malayalam , and Kannada language .", "entities": []}, {"text": "In this model , the XLM - RoBERTa pre - training model is used to extract semantic information features of the text , and DPCNN is used to further process the output features .", "entities": [[5, 6, "MethodName", "XLM"], [7, 8, "MethodName", "RoBERTa"]]}, {"text": "At the same time , the hierarchical crossvalidation method is used to improve the training effect .", "entities": []}, {"text": "The \ufb01nal results show that our model achieves satisfactory performance .", "entities": []}, {"text": "In future work , we will try to adjust the structure of the new model , so as to improve its effect more signi\ufb01cantly .", "entities": []}, {"text": "References Mohamed Abdellatif and Ahmed Elgammal .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Offensive language detection in arabic using ulm\ufb01t .", "entities": []}, {"text": "In", "entities": []}, {"text": "220Proceedings of the 4th Workshop on Open - Source Arabic Corpora and Processing Tools , with a Shared Task on Offensive Language Detection , pages 82\u201385 .", "entities": []}, {"text": "Bharathi Raja Chakravarthi .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "HopeEDI : A multilingual hope speech detection dataset for equality , diversity , and inclusion .", "entities": [[0, 1, "DatasetName", "HopeEDI"], [4, 7, "TaskName", "hope speech detection"]]}, {"text": "In Proceedings of the Third Workshop on Computational Modeling of People \u2019s Opinions , Personality , and Emotion \u2019s in Social Media , pages 41\u201353 , Barcelona , Spain ( Online ) .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Bharathi Raja Chakravarthi , Navya Jose , Shardul Suryawanshi , Elizabeth Sherly , and John Philip McCrae . 2020a .", "entities": []}, {"text": "A sentiment analysis dataset for codemixed Malayalam - English .", "entities": [[1, 3, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under - resourced languages ( SLTU ) and Collaboration and Computing for Under - Resourced Languages ( CCURL ) , pages 177\u2013184 , Marseille , France .", "entities": []}, {"text": "European Language Resources association .", "entities": []}, {"text": "Bharathi Raja Chakravarthi and Vigneshwaran Muralidaran .", "entities": []}, {"text": "2021 .", "entities": []}, {"text": "Findings of the shared task on Hope Speech Detection for Equality , Diversity , and Inclusion .", "entities": [[6, 9, "TaskName", "Hope Speech Detection"]]}, {"text": "In Proceedings of the First Workshop on Language Technology for Equality , Diversity and Inclusion .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Bharathi Raja Chakravarthi , Vigneshwaran Muralidaran , Ruba Priyadharshini , and John Philip McCrae .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Corpus creation for sentiment analysis in code - mixed Tamil - English text .", "entities": [[3, 5, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under - resourced languages ( SLTU ) and Collaboration and Computing for Under - Resourced Languages ( CCURL ) , pages 202\u2013210 , Marseille , France .", "entities": []}, {"text": "European Language Resources association .", "entities": []}, {"text": "Bharathi Raja Chakravarthi , Ruba Priyadharshini , Navya Jose , Anand Kumar M , Thomas Mandl , Prasanna Kumar Kumaresan , Rahul Ponnusamy , Hariharan V , Elizabeth Sherly , and John Philip McCrae . 2021 .", "entities": [[11, 12, "DatasetName", "Kumar"], [18, 19, "DatasetName", "Kumar"]]}, {"text": "Findings of the shared task on Offensive Language Identi\ufb01cation in Tamil , Malayalam , and Kannada .", "entities": []}, {"text": "In Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ying Chen , Yilu Zhou , Sencun Zhu , and Heng Xu .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Detecting offensive language in social media to protect adolescent online safety .", "entities": []}, {"text": "In 2012 International Conference on Privacy , Security , Risk and Trust and 2012 International Confernece on Social Computing , pages 71\u201380 .", "entities": []}, {"text": "IEEE .", "entities": []}, {"text": "Thomas Davidson , Dana Warmsley , Michael Macy , and Ingmar Weber . 2017 .", "entities": []}, {"text": "Automated hate speech detection and the problem of offensive language .", "entities": [[1, 4, "TaskName", "hate speech detection"]]}, {"text": "In Proceedings of the International AAAI Conference on Web and Social Media , volume 11 .", "entities": []}, {"text": "Adeep Hande , Ruba Priyadharshini , and Bharathi Raja Chakravarthi . 2020 .", "entities": []}, {"text": "KanCMD : KannadaCodeMixed dataset for sentiment analysis and offensive language detection .", "entities": [[5, 7, "TaskName", "sentiment analysis"]]}, {"text": "In Proceedings of the Third Workshop on Computational Modeling of People \u2019s Opinions , Personality , and Emotion \u2019s in Social Media , pages 54\u201363 , Barcelona , Spain ( Online ) .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jeremy Howard and Sebastian Ruder .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Universal language model \ufb01ne - tuning for text classi\ufb01cation .", "entities": []}, {"text": "arXiv preprint arXiv:1801.06146 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Rie Johnson and Tong Zhang .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Deep pyramid convolutional neural networks for text categorization .", "entities": [[6, 8, "TaskName", "text categorization"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 562\u2013570 .", "entities": []}, {"text": "Thomas Mandl , Sandip Modha , Anand Kumar M , and Bharathi Raja Chakravarthi .", "entities": [[7, 8, "DatasetName", "Kumar"]]}, {"text": "2020 .", "entities": []}, {"text": "Overview of the HASOC Track at FIRE 2020 :", "entities": [[6, 7, "DatasetName", "FIRE"]]}, {"text": "Hate Speech and Offensive Language Identi\ufb01cation in Tamil , Malayalam , Hindi , English and German .", "entities": [[0, 5, "DatasetName", "Hate Speech and Offensive Language"]]}, {"text": "In Forum for Information Retrieval Evaluation , FIRE 2020 , page 29\u201332 , New York , NY , USA . Association for Computing Machinery .", "entities": [[3, 5, "TaskName", "Information Retrieval"], [7, 8, "DatasetName", "FIRE"]]}, {"text": "Aytu \u02d8g Onan , Serdar Koruko \u02d8glu , and Hasan Bulut .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "A multiobjective weighted voting ensemble classi\ufb01er based on differential evolution algorithm for text sentiment classi\ufb01cation .", "entities": []}, {"text": "Expert Systems with Applications , 62:1\u201316 .", "entities": []}, {"text": "Anil Ozdemir and Reyyan Yeniterzi . 2020 .", "entities": []}, {"text": "Su - nlp at semeval-2020 task 12 : Offensive language identi\ufb01cation in turkish tweets .", "entities": []}, {"text": "In Proceedings of the Fourteenth Workshop on Semantic Evaluation , pages 2171\u20132176 .", "entities": []}, {"text": "Zeses Pitenis , Marcos Zampieri , and Tharindu Ranasinghe .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Offensive language identi\ufb01cation in greek .", "entities": []}, {"text": "arXiv preprint arXiv:2003.07459 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Georgios K Pitsilis , Heri Ramampiaro , and Helge Langseth .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Detecting offensive language in tweets using deep learning .", "entities": []}, {"text": "arXiv preprint arXiv:1801.04433 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Peter Prettenhofer and Benno Stein .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Crosslanguage text classi\ufb01cation using structural correspondence learning .", "entities": []}, {"text": "In Proceedings of the 48th annual meeting of the association for computational linguistics , pages 1118\u20131127 .", "entities": []}, {"text": "Tharindu Ranasinghe and Marcos Zampieri .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Multilingual offensive language identi\ufb01cation with cross - lingual embeddings .", "entities": []}, {"text": "arXiv preprint arXiv:2010.05324 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Amir H Razavi , Diana Inkpen , Sasha Uritsky , and Stan Matwin .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Offensive language detection using multi - level classi\ufb01cation .", "entities": []}, {"text": "In Canadian Conference on Arti\ufb01cial Intelligence , pages 16\u201327 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Shengli Sun , Qingfeng Sun , Kevin Zhou , and Tengchao Lv . 2019 .", "entities": []}, {"text": "Hierarchical attention prototypical networks for few - shot text classi\ufb01cation .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods", "entities": []}, {"text": "221 in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 476\u2013485 .", "entities": []}, {"text": "Shardul Suryawanshi and Bharathi Raja Chakravarthi . 2021 .", "entities": []}, {"text": "Findings of the shared task on Troll Meme Classi\ufb01cation in Tamil .", "entities": []}, {"text": "In Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sajeetha Thavareesan and Sinnathamby Mahesan .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Sentiment Analysis in Tamil Texts : A Study on Machine Learning Techniques and Feature Representation .", "entities": [[0, 2, "TaskName", "Sentiment Analysis"]]}, {"text": "In 2019 14th Conference on Industrial and Information Systems ( ICIIS ) , pages 320\u2013325 .", "entities": []}, {"text": "Sajeetha Thavareesan and Sinnathamby Mahesan .", "entities": []}, {"text": "2020a .", "entities": []}, {"text": "Sentiment Lexicon Expansion using Word2vec and fastText for Sentiment Prediction in Tamil texts .", "entities": [[6, 7, "MethodName", "fastText"]]}, {"text": "In 2020 Moratuwa Engineering Research Conference ( MERCon ) , pages 272\u2013276 .", "entities": []}, {"text": "Sajeetha Thavareesan and Sinnathamby Mahesan .", "entities": []}, {"text": "2020b .", "entities": []}, {"text": "Word embedding - based Part of Speech tagging in Tamil texts .", "entities": []}, {"text": "In 2020 IEEE 15th International Conference on Industrial and Information Systems ( ICIIS ) , pages 478\u2013482 .", "entities": []}]