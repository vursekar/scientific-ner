[{"text": "Proceedings of the 18th International Conference on Spoken Language Translation , pages 255\u2013262 Bangkok , Thailand ( Online ) , August 5\u20136 , 2021 .", "entities": [[9, 10, "TaskName", "Translation"]]}, {"text": "\u00a9 2021 Association for Computational Linguistics255Tag", "entities": []}, {"text": "Assisted Neural Machine Translation of Film Subtitles Aren Siekmeier\u0003,WonKee Lee\u0003,Hongseok Kwony , and Jong - Hyeok Lee\u0003y Pohang University of Science and Technology \u0003Department of Computer Science and Engineering yGraduate School of Arti\ufb01cial Intelligence fasiekmeier , wklee , hkwon , jhlee g@postech.ac.kr Abstract We implemented a neural machine translation system that uses automatic sequence tagging to improve the quality of translation .", "entities": [[2, 4, "TaskName", "Machine Translation"], [47, 49, "TaskName", "machine translation"]]}, {"text": "Instead of operating on unannotated sentence pairs , our system uses pre - trained tagging systems to add linguistic features to source and target sentences .", "entities": []}, {"text": "Our proposed neural architecture learns a combined embedding of tokens and tags in the encoder , and simultaneous token and tag prediction in the decoder .", "entities": []}, {"text": "Compared to a baseline with unannotated training , this architecture increased the BLEU score of German to English \ufb01lm subtitle translation outputs by 1.61 points using named entity tags ; however , the BLEU score decreased by 0.38 points using part - of - speech tags .", "entities": [[12, 14, "MetricName", "BLEU score"], [33, 35, "MetricName", "BLEU score"], [40, 43, "DatasetName", "part - of"]]}, {"text": "This demonstrates that certain token - level tag outputs from off - theshelf tagging systems can improve the output of neural translation systems using our combined embedding and simultaneous decoding extensions .", "entities": []}, {"text": "1 Introduction Neural machine translation ( NMT ) uses neural networks to translate unannotated text between a source and target language , but without additional linguistic information certain ambiguous inputs may be translated incorrectly .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "Consider the following examples : 1 ) Titanic struggles between good and evil .", "entities": []}, {"text": "3 \u00fcE\u00actX\u00c4\u00ad\u009c,\u00c1. big \ufb01ght between good and evil 7\u00c0t\u00c0\u00c9@ \u00fcE\u00act\u00d0 \u001c , \u00c1\u0011t\u00e4 .", "entities": []}, {"text": "The Titanic is \ufb01ghting between good and evil 2 ) Titanic struggles to stay a\ufb02oat .", "entities": []}, {"text": "3\u00c0t\u00c0\u00c9@h \u00b0 X\u00c0J\u00c4]\u00e0p\u0084 , \u0011t\u00e4 .", "entities": []}, {"text": "The Titanic is struggling not to sink 7h \u00b0 X\u00c0J0\u0004\\\u00c4\u00ad\u009c,\u00c1. big \ufb01ght not to sinkIn ( 1 ) , \u201c Titanic \u201d is best translated as a common adjective ; in ( 2 ) , it most likely refers to a named entity , the famous ship .", "entities": []}, {"text": "In addition to the bare token sequences , part - of - speech or named entity annotation of each token , provided manually or automatically , could provide additional information to improve the quality of translation .", "entities": [[8, 11, "DatasetName", "part - of"]]}, {"text": "Natural language processing ( NLP ) tools have bene\ufb01ted from the same explosion in deep learning and neural network developments that has spurred NMT .", "entities": []}, {"text": "NLP tools include part - of - speech ( POS ) taggers , identifying the syntactic function of each input token , and named entity recognition systems .", "entities": [[3, 6, "DatasetName", "part - of"], [23, 26, "TaskName", "named entity recognition"]]}, {"text": "Named entity recognition ( NER ) identi\ufb01es which tokens refer to named entities , including proper nouns such as people , place names , organizations , or dates .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 5, "TaskName", "NER"]]}, {"text": "Recently , automatic named entity recognition ( NER ) systems have seen much development and re\ufb01nement with the same deep learning tools used for NMT ( Li et al . , 2020 ) .", "entities": [[3, 6, "TaskName", "named entity recognition"], [7, 8, "TaskName", "NER"]]}, {"text": "Automatic neural NER systems have achieved accuracy exceeding 92 % F 1scores in many languages and domains ( Wang et al . , 2019 ; Akbik et al . , 2018 ) .", "entities": [[2, 3, "TaskName", "NER"], [6, 7, "MetricName", "accuracy"]]}, {"text": "NER tags produced by these systems are useful in many other natural language processing contexts , such as coreference resolution , entity linking , or entity extraction ( Ferreira Cruz et", "entities": [[0, 1, "TaskName", "NER"], [18, 20, "TaskName", "coreference resolution"], [21, 23, "TaskName", "entity linking"]]}, {"text": "al . , 2020 ) .", "entities": []}, {"text": "POS taggers have also achieved very high accuracy exceeding 98 % on public treebank datasets ( Akbik et al . , 2018 ) .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "We aim to use tags from publicly available pre - trained tagging systems as additional features to improve NMT training and output .", "entities": []}, {"text": "Tag assisted NMT requires modi\ufb01cations to the neural architecture to accommodate a tag at each token position .", "entities": []}, {"text": "The encoder must learn an embedding that combines information from each token and its tag , then compute a hidden state from these embeddings .", "entities": []}, {"text": "The decoder must learn to predict tokens and their tags simultaneously from the decoder state .", "entities": []}, {"text": "Adding tag information to the predic-", "entities": []}, {"text": "256tion and corresponding training loss encourages the model to incorporate this information into its latent representations to improve outputs .", "entities": [[4, 5, "MetricName", "loss"]]}, {"text": "Compared to an untagged baseline system on word - tokenized data , our tagged translation system improved the BLEU score by 1.61 points on German to English parallel \ufb01lm subtitles data tagged with publicly available pre - trained named entity recognition systems , while part - of - speech tagging decreased the score by 0.38 BLEU points .", "entities": [[18, 20, "MetricName", "BLEU score"], [38, 41, "TaskName", "named entity recognition"], [44, 50, "TaskName", "part - of - speech tagging"], [55, 56, "MetricName", "BLEU"]]}, {"text": "Subword tokenization reduced these effects to +0.22 points and \u2013 0.22 points respectively .", "entities": []}, {"text": "Nonetheless , this demonstrates the feasibility of using certain pre - trained tagging outputs to improve translation quality .", "entities": []}, {"text": "2 Related Work Very early work addressed named entity translation by treating automatically identi\ufb01ed named entities with a special translation system , usually a transliterator ( Babych and Hartley , 2003 ) .", "entities": []}, {"text": "This work did not attempt to integrate the translation models for one to bene\ufb01t from information learned by the other .", "entities": []}, {"text": "Later , especially with neural machine translation ( NMT ) systems , source - side feature augmentation research studied the inclusion of linguistic feature information into the source - side token embeddings , usually by adding in or concatenating additional learned feature vectors to the token embedding vectors , as we do in this work ( Sennrich and Haddow , 2016 ; Hoang et al . , 2016b ; Ugawa et al . , 2018 ; Modrzejewski et al . , 2020 ; Modrzejewski , 2020 ; Armengol - Estap \u00b4 e et al . , 2020 ) .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "This approach can also be adopted on the target - side , as presented here or in ( Hoang et al . , 2016a , 2018 ; Nguyen et al . , 2018 ) .", "entities": []}, {"text": "However , these methods only add linguistic feature information to the input , without encouraging the system to model that information in any particular way .", "entities": []}, {"text": "Factored translation systems , under both statistical and neural machine translation , instead explore the addition of externally supplied linguistic features to the raw text at both input and output .", "entities": [[9, 11, "TaskName", "machine translation"]]}, {"text": "These features include part - of - speech ( POS ) tags , word lemmatizations , morphological analysis , and semantic analysis ( Koehn and Hoang , 2007 ; GarciaMartinez et al . , 2016 , 2017 ; Tan et al . , 2020 ) .", "entities": [[3, 6, "DatasetName", "part - of"], [16, 18, "TaskName", "morphological analysis"]]}, {"text": "Factored translation models map feature - augmented input into feature - augmented output , however outputs include only an underlying lemma together srctokens tagsEmbed .", "entities": [[20, 21, "DatasetName", "lemma"]]}, {"text": "Embed.+Encoder tgtprevious tokensprevious tagsEmbed .", "entities": []}, {"text": "Embed.+DecoderLinearSoftmaxnext token LinearSoftmaxnext tag / bracerightbiggFigure 1 : Tagged seq2seq with the predicted features .", "entities": [[9, 10, "MethodName", "seq2seq"]]}, {"text": "These systems also use a rule - based morphology toolkit in post - processing to generate the output surface forms from predicted output features , requiring knowledge of appropriate rule systems for the output language .", "entities": []}, {"text": "An additional tagged architecture ( N \u02d8adejde et al . , 2017 ) predicted syntax - tagged surface forms , but did so by appending the tags to the surface form tokens directly , rather than predicting separate factors .", "entities": []}, {"text": "In general , the focus of factored models has been to increase vocabulary coverage , for example of highly agglutitanative languages with rich morphologies , rather than our goal of disambiguating polysemous of polysyntactic words or otherwise handling named entities in a more nuanced way .", "entities": []}, {"text": "Finally , one previous work does consider a fully tagged ( both source and target ) factored neural model predicting tags with surface forms with independent layers in much the same way as presented here ( Wagner , 2017 ) .", "entities": []}, {"text": "This work showed negative results for various syntactic tag types on IWSLT\u201914 shared task data ( Cettolo et al . , 2014 ) , whereas this work presents NER and POS tags on \ufb01lm subtitles data .", "entities": [[28, 29, "TaskName", "NER"]]}, {"text": "3 Tagged seq2seq We implemented two extensions to the standard seq2seq encoder - decoder architecture for neural machine translation to use token - level tags to improve translation results.1By combining token and tag embeddings in the input and simultaneously predicting tokens and tags in the output , the NMT 1Code at https://github.com/compwiztobe/ tagged - seq2seq", "entities": [[2, 3, "MethodName", "seq2seq"], [10, 11, "MethodName", "seq2seq"], [17, 19, "TaskName", "machine translation"], [54, 55, "MethodName", "seq2seq"]]}, {"text": "257system learned to translate tagged source sentences to tagged target sentences ( Figure 1 ) .", "entities": []}, {"text": "We used a Transformer encoder and decoder for the base seq2seq model ( Vaswani et al . , 2017 ) .", "entities": [[3, 4, "MethodName", "Transformer"], [10, 11, "MethodName", "seq2seq"]]}, {"text": "Tags are added to the data as a preprocessing step .", "entities": []}, {"text": "3.1 Combined embedding Learning an embedding for every possible token and tag combination would enormously increase the model \u2019s learnable parameter count .", "entities": []}, {"text": "Furthermore , training data is likely to be sparse in its coverage of all possible pairs , but not in its coverage of the token and tag vocabularies separately .", "entities": []}, {"text": "Therefore , we instead learn a separate embedding vector for each possible token and each possible tag , effectively concatenating these two vocabularies ( rather than taking the product space ) .", "entities": []}, {"text": "The embedding vectors for the token and tag at each position are then added to combine information from both channels into a single vector , so as not to increase the size of subsequent model layers and the capacity of the model , apart from the additional tag embedding vectors .", "entities": []}, {"text": "3.2 Simultaneous prediction The decoder state diat each step is conditioned on the target pre\ufb01x and the encoded source sentence ( 3 ) .", "entities": []}, {"text": "di= Decoder ( pre\ufb01x , src ) ( 3 ) This shared decoder state is used to predict both the next token and the next tag , with token and tag feature projections Tand \u001c ( 4 and 5 ) .", "entities": []}, {"text": "P(tokenkjpre\ufb01x ; src ) = softmax k(T > di)(4 ) P(tagkjpre\ufb01x ; src ) = softmax k ( \u001c > di)(5 )", "entities": [[5, 6, "MethodName", "softmax"], [15, 16, "MethodName", "softmax"]]}, {"text": "We model these probabilities independently ( 6 ) for the same data sparsity and model size reasons as the embeddings , and we can compute each pair probability and loss accordingly ( 7 ) .", "entities": [[29, 30, "MetricName", "loss"]]}, {"text": "P(token;tagjpre\ufb01x ; src )", "entities": []}, {"text": "= P(tokenjpre . ; src ) \u0001P(tagjpre . ; src ) ( 6 ) L=\u0000logP(tokenjpre\ufb01x ; src ) \u0000logP(tagjpre\ufb01x ; src ) ( 7 ) This combined loss encourages the shared decoder statedito model the correct tag identity so that it can be used by the token prediction layer to improve translation.4 Data Preparation 4.1 Subtitles corpus Our experiments focused on \ufb01lm subtitles in German and English .", "entities": [[27, 28, "MetricName", "loss"]]}, {"text": "The Opus project provided a parallel German to English subtitles corpus from OpenSubtitles ( Tiedemann , 2012 ; Aulamo et al . , 2020 ) .", "entities": [[12, 13, "DatasetName", "OpenSubtitles"]]}, {"text": "This data was cleaned with some rudimentary sentence length \ufb01ltering , and randomly divided into a 3 million sentence - pair training split ( about 49 million tokens ) , along with 100,000 pair validation and test splits ( about 1.6 million tokens each ) .", "entities": []}, {"text": "4.2 Tagging \u201c off the shelf \u201d Flair NLP tools systems have achieved state - of - theart results on the sequence labeling tasks such as the CoNLL\u201903 NER dataset and universal part - ofspeech tagging from Universal Dependency treebanks ( Akbik et al . , 2018 ; Tjong Kim Sang and De Meulder , 2003 ; Nivre et al . , 2020 ) .", "entities": [[28, 29, "TaskName", "NER"]]}, {"text": "We used the publicly available pre - trained multilingual NER and universal POS taggers.2NER tags followed the BIOES system with four entity classes : PER , person;LOC , location ; ORG , organization ; and MISC , miscellaneous .", "entities": [[9, 10, "TaskName", "NER"]]}, {"text": "Four classes with four span markers , plus the null span marker O , gave the same 17 - tag vocabulary for NER on both German and English .", "entities": [[22, 23, "TaskName", "NER"]]}, {"text": "Meanwhile , POS tags came from the same 17 - tag universal POS tag set for both languages .", "entities": []}, {"text": "Around 3 % of words in the OpenSubtitles corpus were tagged as named entities ( non O ) .", "entities": [[7, 8, "DatasetName", "OpenSubtitles"]]}, {"text": "We further divided the test split based on whether any named entities were found in either the source or the target sentence .", "entities": []}, {"text": "Out of 100,000 test pairs", "entities": []}, {"text": ", 79,201 had no named entities , and 20,799 had some .", "entities": []}, {"text": "4.3 Tokenization Word tokenization , as used by the tagging systems , is most straightforward for maintaining one - to - one alignments between tokens and their assigned tags .", "entities": []}, {"text": "For word tokenization experiments , vocabularies of size 35,012 for German and 17,196 for English were selected , resulting in an unknown word replacement rate of 3 % .", "entities": []}, {"text": "This unknown word replacement was considerably higher on rare word categories , for example named entities saw a 25 \u2013 30 % rate of unknown words outside the selected word vocabulary .", "entities": []}, {"text": "To alleviate this it is also possible to consider subword 2Models at https://huggingface.co/flair/ { ner , upos}-multi", "entities": []}, {"text": "258Table 1 : BLEU scores on word - tokenized sentences with or without named entities , for models with or without NER tags .", "entities": [[3, 4, "MetricName", "BLEU"], [21, 22, "TaskName", "NER"]]}, {"text": "BLEU ( % ) NER tags no NEs some NEs all \u0000src,\u0000tgt334.70 32.43 34.15 + src,\u0000tgt434.89 32.14 34.22 \u0000src,+tgt535.69 35.03 35.53 + src,+tgt 35.84 35.50 35.76 improvement\"1.14\"3.07\"1.61 tokenization , so additional experiments were conducted with a shared SentencePiece ( Kudo , 2018 ) vocabulary of 32,000 subwords , built from the training split and used to tokenize both languages .", "entities": [[0, 1, "MetricName", "BLEU"], [4, 5, "TaskName", "NER"], [37, 38, "MethodName", "SentencePiece"]]}, {"text": "After subword tokenization , the BIOES structure of named entity spans was propagated across subword tokens in the natural way to maintain spans .", "entities": []}, {"text": "For POS tags , subwords received the same tag as their parent word .", "entities": []}, {"text": "5 Experiments We used a Transformer encoder and decoder ( Vaswani et al . , 2017 ) for the base seq2seq system , each with 6 layers and 8 attention heads , and layer and embedding dimensions 512 .", "entities": [[5, 6, "MethodName", "Transformer"], [20, 21, "MethodName", "seq2seq"]]}, {"text": "Training was done for 40 epochs at half precision with the optimizer known as Adam ( Kingma and Ba , 2015 ) with \f = ( 0:9;0:98)and an inverse square root learning schedule with maximum learning rate 5\u000210\u00004after 500 updates and decay 1\u000210\u00004 .", "entities": [[11, 12, "HyperparameterName", "optimizer"], [14, 15, "MethodName", "Adam"], [35, 37, "HyperparameterName", "learning rate"]]}, {"text": "Parameter updates occurred after every 8,192 token - tag pairs at most ( rounding off to complete sentences ) , with 30 % dropout and label smoothing of 0.1 on the training loss .", "entities": [[25, 27, "MethodName", "label smoothing"], [32, 33, "MetricName", "loss"]]}, {"text": "At inference time , a beam of 5 candidates was maintained , and the models were evaluated with their BLEU score on the token sequence only ( tagging accuracy was not evaluated due to the dif\ufb01culty of establishing alignment ) .", "entities": [[19, 21, "MetricName", "BLEU score"], [28, 29, "MetricName", "accuracy"]]}, {"text": "6 Results BLEU scores from untagged and tagged translation experiments show an improvement from the use of NER tags ( Table 1 ) .", "entities": [[2, 3, "MetricName", "BLEU"], [17, 18, "TaskName", "NER"]]}, {"text": "Adding NER tags , the 3baseline 4enhanced baseline / ablation study 5ablation studyTable 2 : BLEU scores for word models with POS tags .", "entities": [[1, 2, "TaskName", "NER"], [15, 16, "MetricName", "BLEU"]]}, {"text": "POS tags BLEU", "entities": [[2, 3, "MetricName", "BLEU"]]}, {"text": "( % ) \u0000src,\u0000tgt 34.15 + src,\u0000tgt 34.21 \u0000src,+tgt 33.70 + src,+tgt 33.77 improvement # 0.38 BLEU score on sentences containing some named entities improved by a larger margin , 3.07 points , presumably due to the tags \u2019 assistance with translating those named entities .", "entities": [[16, 18, "MetricName", "BLEU score"]]}, {"text": "We also note an improvement in the BLEU score on sentences containing no named entities , which increased by 1.14 points .", "entities": [[7, 9, "MetricName", "BLEU score"]]}, {"text": "This suggests that given Otag information the model can also treat common words with con\ufb01dence that they are not named entities and should not be translated as such .", "entities": []}, {"text": "These improvements averaged out to a net gain of 1.61 BLEU points on the entire test split .", "entities": [[10, 11, "MetricName", "BLEU"]]}, {"text": "We also evaluated a model trained with POS tags , but found a decrease in BLEU score ( Table 2 ) .", "entities": [[15, 17, "MetricName", "BLEU score"]]}, {"text": "Translation scores with POS tags decreased by 0.38 BLEU points .", "entities": [[0, 1, "TaskName", "Translation"], [8, 9, "MetricName", "BLEU"]]}, {"text": "There are two ways to understand this in comparison with NER tags .", "entities": [[10, 11, "TaskName", "NER"]]}, {"text": "First , POS tags carry a signi\ufb01cant amount of information about the sentence , not only helping to disambiguate between different word senses by part - of - speech , but also assisting the model with encoding the sentence \u2019s syntactic structure .", "entities": [[24, 27, "DatasetName", "part - of"]]}, {"text": "Compared to NER tags , this amount of structural information might be dif\ufb01cult to model with the same decoder architecture used for token prediction .", "entities": [[2, 3, "TaskName", "NER"]]}, {"text": "Second , POS tags tend to carry the same amount of information for each tag at each position , compared to NER tags only conveying most of their information at the named entity spans which are few and far between .", "entities": [[21, 22, "TaskName", "NER"]]}, {"text": "This also lends itself to the idea that POS tags have a higher information content that is less easily modeled by the decoder , leading to worse results than NER tagging .", "entities": [[29, 30, "TaskName", "NER"]]}, {"text": "6.1 Enhanced baselines and ablation study For both NER and POS tagged results , the baseline was the same Transformer architecture trained only on untagged data ( without adding tag embeddings or predicting tags from the decoder ) .", "entities": [[8, 9, "TaskName", "NER"], [19, 20, "MethodName", "Transformer"]]}, {"text": "Adding in only source - side tag embeddings could be considered an enhanced baseline , since this kind of", "entities": []}, {"text": "259Table 3 : BLEU scores on subword - tokenized sentences with or without named entities , for models with or without NER tags .", "entities": [[3, 4, "MetricName", "BLEU"], [21, 22, "TaskName", "NER"]]}, {"text": "BLEU ( % ) NER tags no NEs some NEs all \u0000src,\u0000tgt 35.77 36.51 35.96 + src,\u0000tgt 35.83 36.75 36.06 \u0000src,+tgt 35.88 36.82 36.12 + src,+tgt 35.94 36.92 36.19 improvement\"0.17\"0.41\"0.22 feature augmentation has already been studied in depth ( Sennrich and Haddow , 2016 ; Hoang et al . , 2016b ) .", "entities": [[0, 1, "MetricName", "BLEU"], [4, 5, "TaskName", "NER"]]}, {"text": "Our results show that this source - only tagging does not provide signi\ufb01cant bene\ufb01ts compared to training on untagged data ( Table 1 ) , although for POS tagging this remains the best result .", "entities": []}, {"text": "On the other hand , adding in target - side tags while also predicting them from the decoder , without adding in source - side tag embeddings could be considered an ablation test to isolate the effects of our main contribution : target - side tag decoding .", "entities": []}, {"text": "Our results show that this target tagging provides the same bene\ufb01t as the fully tagged training regime , demonstrating that it is the simultaneous tag decoding that accounts for the entire effect observed .", "entities": []}, {"text": "For NER tagging this was an improvement in BLEU scores , but for POS tagging scores decreased when adding target tagging .", "entities": [[1, 2, "TaskName", "NER"], [8, 9, "MetricName", "BLEU"]]}, {"text": "Whereas source - side tag information is added into the embeddings without any modi\ufb01cation to the training objective , target - side tag predictions are a part of the modi\ufb01ed training loss , so that it is the target - side tag prediction that pushes the model to incorporate accurate knowledge of the tags into its learning representations .", "entities": [[31, 32, "MetricName", "loss"]]}, {"text": "That NER tag modeling improved results while POS tag modeling did not is consistent with our earlier observation that POS tag modeling seems to be more dif\ufb01cult than NER tag modeling , and is not done effectively by the current architecture .", "entities": [[1, 2, "TaskName", "NER"], [28, 29, "TaskName", "NER"]]}, {"text": "6.2 Subword tokenization experiments Experiments with subword tokenized data showed similar effects , but of a signi\ufb01cantly reduced size .", "entities": []}, {"text": "Adding NER tags improved the results , adding 0.22 points to the BLEU score , with the improvement again coming largely from the target side tagging , and again showing a larger improvementTable 4 : BLEU scores for subword models with or without POS tags .", "entities": [[1, 2, "TaskName", "NER"], [12, 14, "MetricName", "BLEU score"], [35, 36, "MetricName", "BLEU"]]}, {"text": "POS tags BLEU", "entities": [[2, 3, "MetricName", "BLEU"]]}, {"text": "( % ) \u0000src,\u0000tgt 35.96 + src,\u0000tgt 36.20 \u0000src,+tgt 35.69 + src,+tgt 35.74 improvement # 0.22 on sentences with named entities than on those without ( Table 3 ) .", "entities": []}, {"text": "Adding POS tags hurt results , decreasing the score by 0.22 , and again we see that source - only tagging is best case for POS tagging ( Table 4 ) .", "entities": []}, {"text": "However , the reduced magnitude of these deltas to the range of 0.1 \u2013 0.4 BLEU points suggests these are not signi\ufb01cant changes to the translation performance , in the subword tokenization case .", "entities": [[15, 16, "MetricName", "BLEU"]]}, {"text": "It would appear that subword tokenization interferes with the bene\ufb01ts of tagging the data .", "entities": []}, {"text": "Since tags are aligned one - to - one with the input words , subword tokenization destroys this alignment , and copying tags across a word \u2019s constituent subwords may interfere with the model \u2019s ability to make sense the of tag information .", "entities": []}, {"text": "In particular for named entities , rare words are likely to tokenized into a larger number of subword tokens , exacerbating this effect .", "entities": []}, {"text": "The set of embeddings for the subwords in a word may not be as useful to the model for translating a named entity or other rare category as the single embedding learned speci\ufb01cally for the full word in a word tokenization setting , and further these subword embeddings may be affected by other contexts unrelated to the larger word .", "entities": []}, {"text": "Speci\ufb01cally for the named entity case , subword tokenization algorithms might prioritize the atomicity of certain rare words tagged as named entities in order to counteract this .", "entities": []}, {"text": "6.3 Token prediction and tagging loss Due to the conditional independence assumption , the cross - entropy loss ( 7 ) conveniently decomposes into separate terms for tokens and tags ( 8) , allowing us to measure the relative information content of each channel ( Table 5 ) .", "entities": [[5, 6, "MetricName", "loss"], [17, 18, "MetricName", "loss"]]}, {"text": "L=\u0000logP(tokenjpre\ufb01x ; src ) \u0000logP(tagjpre\ufb01x ; src ) = Ltoken+Ltag(8 )", "entities": []}, {"text": "260Table 5 : Token prediction and tagging loss .", "entities": [[7, 8, "MetricName", "loss"]]}, {"text": "# cross entropy ( bits ) LtokenLtagL no tags\u0000src,\u0000tgt 2.000 \u2014 2.000 + src,\u0000tgt 2.006 \u2014 2.006 NER\u0000src,+tgt 2.001 0.183", "entities": []}, {"text": "2.184 + src,+tgt 1.985 0.183 2.168 + src,\u0000tgt 2.007 \u2014 2.007 POS\u0000src,+tgt 1.995 0.697 2.692 + src,+tgt 1.972 0.695 2.673", "entities": []}, {"text": "While adding tag information naturally increases the overall cross - entropy , as there are more possibilities to account for and to be predicted , restricting our attention only to the token loss shows that the token - level cross - entropy is consistently reduced from 2.000 ( base-2 ) to 1.985 with NER tags or 1.972 for POS tags .", "entities": [[32, 33, "MetricName", "loss"], [53, 54, "TaskName", "NER"]]}, {"text": "This shows how both tag types can add disambiguating information to the token prediction process , with POS tags naturally add more of such information , since they carry syntactic information .", "entities": []}, {"text": "Looking only at tag - level cross - entropy , it \u2019s interesting to notice that the POS tagging loss is signi\ufb01cantly higher than the NER tagging loss .", "entities": [[19, 20, "MetricName", "loss"], [25, 26, "TaskName", "NER"], [27, 28, "MetricName", "loss"]]}, {"text": "While this could be simply because the lower - bound inherent entropy is higher ( POS tags naturally contain more information , being more uniformly distributed than NER tags ) , this could also be consistent with the idea that POS tag modeling is more dif\ufb01cult , explaining the decreased translation scores observed with POS tag prediction .", "entities": [[27, 28, "TaskName", "NER"]]}, {"text": "7 Model Limitations It should not go unnoticed that the typical inference algorithms for sequence labeling , particularly the BiLSTM - CRF inference employed by most NER systems , are incompatible with the autoregressive sequence decoding algorithms ( greedy decoding and beam search ) used for inference by seq2seq models .", "entities": [[19, 20, "MethodName", "BiLSTM"], [21, 22, "MethodName", "CRF"], [26, 27, "TaskName", "NER"], [48, 49, "MethodName", "seq2seq"]]}, {"text": "That the beam decoding algorithm ( and autoregressive likelihood model ) used here for tags was unable to account for ( be conditioned on ) the as - yet uncomputed right context was cause for much apprehension before experimental results became available .", "entities": []}, {"text": "These positive results notwithstanding , future work could explore how to better incorporate the full tagging context in tag de - coding , perhaps , for example , by predicting the sequence more wholistically with non - autoregressive decoding ( Gu et al . , 2018 ) .", "entities": []}, {"text": "We also imagine that the design of the underlying seq2seq architecture may lend itself to certain types of sequence labeling .", "entities": [[9, 10, "MethodName", "seq2seq"]]}, {"text": "For example , the bidirectional context modeled by a BiLSTM - based translation model may be more suitable for certain types of sequence labeling tasks than the Transformer \u2019s attentional activations .", "entities": [[9, 10, "MethodName", "BiLSTM"], [27, 28, "MethodName", "Transformer"]]}, {"text": "Because our contributions are agnostic to the type of sequence labeling ( NER or part - of - speech tagging or any other kind ) as well as to the design of the encoder and decoder , future experiments should also explore these possibilities .", "entities": [[12, 13, "TaskName", "NER"], [14, 20, "TaskName", "part - of - speech tagging"]]}, {"text": "8 Conclusion We implemented extensions to existing neural machine translation models that allow the use of offthe - shelf token - level tagging systems to improve translation accuracy .", "entities": [[8, 10, "TaskName", "machine translation"], [27, 28, "MetricName", "accuracy"]]}, {"text": "Translation inputs and training outputs were tagged with pre - trained sequence labeling systems .", "entities": [[0, 1, "TaskName", "Translation"]]}, {"text": "A standard encoder - decoder architecture was extended to include tag embeddings and tag prediction at each token position .", "entities": []}, {"text": "At model input , token and tag embedding vectors were added to produce a combined embedding .", "entities": []}, {"text": "At model output , the \ufb01nal decoder layer used separate softmax layers to predict tokens and tags .", "entities": [[10, 11, "MethodName", "softmax"]]}, {"text": "During training , a combined loss function encouraged the model to learn token and tag information jointly .", "entities": [[5, 6, "MetricName", "loss"]]}, {"text": "This tag assisted translation system was tested against baseline token - only systems on a German to English \ufb01lm subtitle corpus with both word and subword tokenization .", "entities": []}, {"text": "Subword tokenization reduced the size of the effect , suggesting the need for specialized subword tokenization to prioritize the integrity of important word categories .", "entities": []}, {"text": "However , on word tokenized data , the 1.61 point increase in BLEU score using named entity tags demonstrates that the proposed architecture is useful for improving translation outputs with automatic named entity recognition , while the 0.38 point decrease using part - of - speech tags indicates more dif\ufb01culty in utilizing that tag information .", "entities": [[12, 14, "MetricName", "BLEU score"], [31, 34, "TaskName", "named entity recognition"], [41, 44, "DatasetName", "part - of"]]}, {"text": "Further examination of the cross - entropy showed that adding tags reduced the token cross - entropy thereby improving token modeling .", "entities": []}, {"text": "Future experiments can explore the use of other types of tag data as well as other decoding paradigms .", "entities": []}, {"text": "261Acknowledgments Many thanks go to my colleagues Jeesoo Bang , Jaehun Shin , and Baikjin Jung in the Knowledge and Language Engineering Lab ( POSTECH ) for their many hours generously spent discussing these research topics .", "entities": []}, {"text": "These results would not have been possible without their support .", "entities": []}, {"text": "This work was carried out as part of the HPC Support Project supported by the Ministry of Science and ICT ( MSIT ) and the National IT Industry Promotion Agency ( NIPA ) , and was funded by the Institute of Information & Communications Technology Planning & Evaluation ( IITP ) supported by the Korean government ( MSIT ):", "entities": []}, {"text": "Grant No . 2019 - 0 - 01906 , Graduate School of Arti\ufb01cial Intelligence ( POSTECH ) .", "entities": [[5, 6, "DatasetName", "0"]]}, {"text": "References Alan Akbik , Duncan Blythe , and Roland V ollgraf .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Contextual string embeddings for sequence labeling .", "entities": []}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , pages 1638 \u2013 1649 , Santa Fe , New Mexico , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Jordi Armengol - Estap \u00b4 e , Marta R Costa - juss ` a , and Carlos Escolano .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Enriching the transformer with linguistic factors for low - resource machine translation .", "entities": [[10, 12, "TaskName", "machine translation"]]}, {"text": "arXiv preprint arXiv:2004.08053 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Mikko Aulamo , Umut Sulubacak , Sami Virpioja , and J\u00a8org Tiedemann .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "OpusTools and parallel corpus diagnostics .", "entities": []}, {"text": "In Proceedings of the 12th Language Resources and Evaluation Conference , pages 3782\u20133789 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Bogdan Babych and Anthony Hartley .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Improving machine translation quality with automatic named entity recognition .", "entities": [[1, 3, "TaskName", "machine translation"], [6, 9, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the 7th International EAMT workshop on MT and other language technology tools , Improving MT through other language technology tools , Resource and tools for building MT at EACL 2003 .", "entities": []}, {"text": "M. Cettolo , J. Niehues , S. St \u00a8uker , L Bentivogli , and Marcello Federico .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Report on the 11th IWSLT evaluation campaign .", "entities": []}, {"text": "In Proceedings of the 11th International Workshop on Spoken Language Translation , pages 2\u201316 .", "entities": [[10, 11, "TaskName", "Translation"]]}, {"text": "Andr \u00b4 e Ferreira Cruz , Gil Rocha , and Henrique Lopes Cardoso .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Coreference resolution : Toward end - to - end and cross - lingual systems .", "entities": [[0, 2, "TaskName", "Coreference resolution"]]}, {"text": "Information , 11:74 .", "entities": []}, {"text": "Mercedes Garcia - Martinez , Lo \u00a8\u0131c Barrault , and Fethi Bougares .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Factored Neural Machine Translation Architectures .", "entities": [[2, 4, "TaskName", "Machine Translation"]]}, {"text": "In International Workshop onSpoken Language Translation ( IWSLT\u201916 ) , Seattle , United States .", "entities": [[5, 6, "TaskName", "Translation"]]}, {"text": "Mercedes Garcia - Martinez , Lo \u00a8\u0131c Barrault , and Fethi Bougares .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Neural Machine Translation by Generating Multiple Linguistic Factors .", "entities": [[1, 3, "TaskName", "Machine Translation"]]}, {"text": "In 5th International Conference Statistical Language and Speech Processing SLSP 2017 , Statistical Language and Speech Processing 5th International Conference , SLSP 2017 , Le Mans , France , October 23\u201325 , 2017 , Proceedings , Le Mans , France .", "entities": []}, {"text": "11 pages , 3 \ufb01gues , SLSP conference .", "entities": []}, {"text": "Jiatao Gu , James Bradbury , Caiming Xiong , Victor O. K. Li , and Richard Socher .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Nonautoregressive neural machine translation .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In 6th International Conference on Learning Representations , ICLR 2018 , Conference Track Proceedings , Vancouver , BC , Canada .", "entities": []}, {"text": "Cong Duy Vu Hoang , Trevor Cohn , and Gholamreza Haffari .", "entities": []}, {"text": "2016a .", "entities": []}, {"text": "Incorporating side information into recurrent neural network language models .", "entities": []}, {"text": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1250\u20131255 , San Diego , California . Association for Computational Linguistics .", "entities": []}, {"text": "Cong Duy Vu Hoang , Gholamreza Haffari , and Trevor Cohn .", "entities": []}, {"text": "2016b .", "entities": []}, {"text": "Improving neural translation models with linguistic factors .", "entities": []}, {"text": "In Proceedings of the Australasian Language Technology Association Workshop 2016 , pages 7\u201314 , Melbourne , Australia .", "entities": []}, {"text": "Cong Duy Vu Hoang , Gholamreza Haffari , and Trevor Cohn .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Improved neural machine translation using side information .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Australasian Language Technology Association Workshop 2018 , pages 6\u201316 , Dunedin , New Zealand .", "entities": []}, {"text": "Diederik P. Kingma and Jimmy Ba . 2015 .", "entities": []}, {"text": "Adam : A method for stochastic optimization .", "entities": [[0, 1, "MethodName", "Adam"], [5, 7, "TaskName", "stochastic optimization"]]}, {"text": "In 3rd International Conference on Learning Representations , ICLR 2015 , Conference Track Proceedings , San Diego , CA , USA .", "entities": []}, {"text": "Philipp Koehn and Hieu Hoang .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Factored translation models .", "entities": []}, {"text": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP - CoNLL ) , pages 868\u2013876 , Prague , Czech Republic .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Taku Kudo .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Subword regularization : Improving neural network translation models with multiple subword candidates .", "entities": []}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 66\u201375 , Melbourne , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "J. Li , A. Sun , J. Han , and C. Li .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A survey on deep learning for named entity recognition .", "entities": [[6, 9, "TaskName", "named entity recognition"]]}, {"text": "In IEEE", "entities": []}, {"text": "262Transactions on Knowledge and Data Engineering , Los Alamitos , CA , USA .", "entities": []}, {"text": "IEEE Computer Society .", "entities": []}, {"text": "Maciej Modrzejewski .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Improvement of the Translation of Named Entities in Neural Machine Translation .", "entities": [[3, 4, "TaskName", "Translation"], [9, 11, "TaskName", "Machine Translation"]]}, {"text": "Ph.D. thesis , Karlsruhe Institute of Technology Department of Informatics Institute for Anthropomatics and Robotics .", "entities": []}, {"text": "Maciej Modrzejewski , Miriam Exel , Bianka Buschbeck , Thanh - Le Ha , and Alexander Waibel . 2020 .", "entities": []}, {"text": "Incorporating external annotation to improve named entity translation in NMT .", "entities": []}, {"text": "In Proceedings of the 22nd Annual Conference of the European Association for Machine Translation , pages 45\u201351 , Lisboa , Portugal .", "entities": [[12, 14, "TaskName", "Machine Translation"]]}, {"text": "European Association for Machine Translation .", "entities": [[3, 5, "TaskName", "Machine Translation"]]}, {"text": "Maria N \u02d8adejde , Siva Reddy , Rico Sennrich , Tomasz Dwojak , Marcin Junczys - Dowmunt , Philipp Koehn , and Alexandra Birch .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Predicting target language CCG supertags improves neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Second Conference on Machine Translation , pages 68\u201379 , Copenhagen , Denmark .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Quang - Phuoc Nguyen , Joon - Choul Shin , and CheolYoung Ock .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "An evaluation of translation quality by homograph disambiguation in korean - x neural machine translation systems .", "entities": [[13, 15, "TaskName", "machine translation"]]}, {"text": "In Annual Conference on Human and Language Technology , pages 504 \u2013 509 .", "entities": []}, {"text": "Human and Language Technology .", "entities": []}, {"text": "Joakim Nivre , Marie - Catherine de Marneffe , Filip Ginter , Jan Haji \u02c7c , Christopher D. Manning , Sampo Pyysalo , Sebastian Schuster , Francis Tyers , and Daniel Zeman .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Universal Dependencies v2 : An evergrowing multilingual treebank collection .", "entities": [[0, 2, "DatasetName", "Universal Dependencies"]]}, {"text": "In Proceedings of the 12th Language Resources and Evaluation Conference , pages 4034\u20134043 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Rico Sennrich and Barry Haddow . 2016 .", "entities": []}, {"text": "Linguistic input features improve neural machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "InProceedings of the First Conference on Machine Translation : Volume 1 , Research Papers , pages 83 \u2013 91 , Berlin , Germany .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Samson Tan , Sha\ufb01q Joty , Lav Varshney , and Min - Yen Kan. 2020 .", "entities": []}, {"text": "Mind your in\ufb02ections !", "entities": []}, {"text": "Improving NLP for non - standard Englishes with Base - In\ufb02ection Encoding .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 5647\u20135663 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "J\u00a8org Tiedemann .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Parallel data , tools and interfaces in OPUS .", "entities": []}, {"text": "In Proceedings of the Eight International Conference on Language Resources and Evaluation , Istanbul , Turkey .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Erik F. Tjong Kim Sang and Fien De Meulder .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Introduction to the CoNLL-2003 shared task : Language - independent named entity recognition .", "entities": [[3, 4, "DatasetName", "CoNLL-2003"], [10, 13, "TaskName", "named entity recognition"]]}, {"text": "In Proceedings of the Seventh Conference on Natural Language Learning at HLT - NAACL 2003 - Volume 4 , CONLL \u2019 03 , page 142\u2013147 , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Arata Ugawa , Akihiro Tamura , Takashi Ninomiya , Hiroya Takamura , and Manabu Okumura .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Neural machine translation incorporating named entity .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 27th International Conference on Computational Linguistics , pages 3240\u20133250 , Santa Fe , New Mexico , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Proceedings of the 31st International Conference on Neural Information Processing Systems , NIPS\u201917 , pages 6000\u20136010 , Red Hook , NY , USA .", "entities": []}, {"text": "Curran Associates Inc.", "entities": []}, {"text": "Martin Wagner . 2017 .", "entities": []}, {"text": "Target Factors for Neural Machine Translation .", "entities": [[4, 6, "TaskName", "Machine Translation"]]}, {"text": "Ph.D. thesis , Karlsruhe Institute of Technology Department of Informatics Institute for Anthropomatics and Robotics .", "entities": []}, {"text": "Zihan Wang , Jingbo Shang , Liyuan Liu , Lihao Lu , Jiacheng Liu , and Jiawei Han . 2019 .", "entities": []}, {"text": "CrossWeigh : Training named entity tagger from imperfect annotations .", "entities": []}, {"text": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 5154\u20135163 , Hong Kong , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}]