[{"text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2185\u20132194 Brussels , Belgium , October 31 - November 4 , 2018 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2018 Association for Computational Linguistics2185Knowledge Base Question Answering via Encoding of Complex Query Graphs Kangqi Luo1Fengli Lin1Xusheng Luo2Kenny Q. Zhu1 1Shanghai Jiao Tong University , Shanghai , China 2Alibaba Group , Hangzhou , China { luokangqi , fenglilin } @sjtu.edu.cn , lxs140564@alibaba-inc.com , kzhu@cs.sjtu .edu.cn", "entities": [[6, 8, "TaskName", "Question Answering"]]}, {"text": "Abstract Answering complex questions that involve multiple entities and multiple relations using a standard knowledge base is an open and challenging task .", "entities": []}, {"text": "Most existing KBQA approaches focus on simpler questions and do not work very well on complex questions because they were not able to simultaneously represent the question and the corresponding complex query structure .", "entities": []}, {"text": "In this work , we encode such complex query structure into a uniform vector representation , and thus successfully capture the interactions between individual semantic components within a complex question .", "entities": []}, {"text": "This approach consistently outperforms existing methods on complex questions while staying competitive on simple questions .", "entities": []}, {"text": "1 Introduction The knowledge - based question answering ( KBQA ) is a task which takes a natural language question as input and returns a factual answer using structured knowledge bases such as Freebase ( Bollacker et al . , 2008 ) , YAGO ( Suchanek et al . , 2007 ) and DBpedia ( Auer et", "entities": [[6, 8, "TaskName", "question answering"], [43, 44, "DatasetName", "YAGO"], [53, 54, "DatasetName", "DBpedia"]]}, {"text": "al . , 2007 )", "entities": []}, {"text": ".", "entities": []}, {"text": "One simple example is a question like this : \u201c What \u2019s the capital of the United States ? \u201d", "entities": []}, {"text": "A common answer to such question is to identify the focus entity and the main relation predicate ( or a sequence ) in the question , and map the question to a triple fact query ( US , capital , ? ) over KB .", "entities": []}, {"text": "The object answers are returned by executing the query .", "entities": []}, {"text": "The mapping above is typically learned from question - answer pairs through distant supervision .", "entities": []}, {"text": "While the above question can be answered by querying a single predicate or predicate sequence in the KB , many other more complex questions can not , e.g. the question in Figure 1 .", "entities": []}, {"text": "To answer the question \u201c What is the second longest river in United States \u201d , we need to infer several semanticAisA   length 2 MaxAtN   contained_by   What is the second longest river in the United States?river   US   Figure 1 : Running example of complex question .", "entities": []}, {"text": "clues : 1 ) the answer is contained by United States ; 2 ) the answer is a river ; 3 ) the answer ranks second by its length in descending order .", "entities": []}, {"text": "Thus , multiple predicates are required to constrain the answer set , and we call such questions \u201c complex questions \u201d throughout this paper .", "entities": []}, {"text": "For answering complex questions , it \u2019s more important to understand the compositional semantic meanings of the question .", "entities": []}, {"text": "As a classic branch of KBQA solutions , semantic parsing ( SP ) technique ( Berant et al . , 2013 ; Yih et al . , 2015 ; Reddy et al . , 2016 ; Hu et al . , 2018 ) aims at learning semantic parse trees or equivalent query graphs1for representing semantic structures of the questions .", "entities": [[8, 10, "TaskName", "semantic parsing"]]}, {"text": "For example in Figure 1 , the query graph forms a tree shape .", "entities": []}, {"text": "The answer node A , serving as the root of the tree , is the variable vertex that represents the real answer entities .", "entities": []}, {"text": "The focus nodes ( US , river , 2nd ) are extracted from the mentions of the question , and they constrain the answer node via predicate sequences in the knowledge base .", "entities": []}, {"text": "Recently , neural network ( NN ) models have shown great promise in improving the performance of KBQA systems , and SP+NN techniques become the stateof - the - art on several KBQA datasets ( Qu et al . , 2018 ; Bao et al . , 2016 ) .", "entities": []}, {"text": "According to the discussion above , our work extends the current research in the SP+NN direction .", "entities": []}, {"text": "The common step of SP - based approaches 1The term \u201c query graph \u201d is interchangeable with \u201c query structure \u201d and \u201c semantic parsing tree \u201d throughout this paper .", "entities": [[23, 25, "TaskName", "semantic parsing"]]}, {"text": "2186is to \ufb01rst collect candidate query graphs using bottom up parsing ( Berant et al . , 2013 ; Cai and Yates , 2013 ) or staged query generation methods ( Yih et al . , 2015 ; Bao et al . , 2016 ) , then predict the best graph mainly based on the semantic similarity with the given question .", "entities": [[55, 57, "TaskName", "semantic similarity"]]}, {"text": "Existing NN - based methods follow an encode - andcompare framework for answering simple questions , where both the question and the predicate sequence are encoded as semantic vectors in a common embedding space , and the semantic similarity is calculated by the cosine score between vectors .", "entities": [[37, 39, "TaskName", "semantic similarity"]]}, {"text": "In order to de\ufb01ne the similarity function between one question and a complex query graph , an intuitive solution is to split the query graph into multiple semantic components , as the predicate sequences separated by dashed boxes in Figure 1 .", "entities": []}, {"text": "Then previous methods can be applied for modeling the similarity between the question and each part of the graph .", "entities": []}, {"text": "However , such approach faces two limitations .", "entities": []}, {"text": "First , each semantic component is not directly comparable with the whole question , since it conveys only partial information of the question .", "entities": []}, {"text": "Second , and more importantly , the model encodes different components separately , without learning the representation of the whole graph , hence it \u2019s not able to capture the compositional semantics in a global perspective .", "entities": []}, {"text": "In order to attack the above limitations , we propose a neural network based approach to improve the performance of semantic similarity measurement in complex question answering .", "entities": [[20, 22, "TaskName", "semantic similarity"], [25, 27, "TaskName", "question answering"]]}, {"text": "Given candidate query graphs generated from one question , our model embeds the question surface and predicate sequences into a uniform vector space .", "entities": []}, {"text": "The main difference between our approach and previous methods is that we integrate hidden vectors of various semantic components and encode their interaction as the hidden semantics of the entire query graph .", "entities": []}, {"text": "In addition , to cope with different semantic components of a query graph , we leverage dependency parsing information as a complementary of sentential information for question encoding , which makes the model better align each component to the question .", "entities": [[16, 18, "TaskName", "dependency parsing"]]}, {"text": "The contribution of this paper is summarized below .", "entities": []}, {"text": "\u2022", "entities": []}, {"text": "We propose a light - weighted and effective neural network model to solve complex KBQA task .", "entities": []}, {"text": "To the best of our knowledge , this is the \ufb01rst attempt to explicitly encode the complete semantics of a complex querygraph ( Section 2.2 ) ; \u2022 We leverage dependency parsing to enrich question representation in the NN model , and conduct thorough investigations to verify its effectiveness ( Section 2.2.2 ) ; \u2022 We propose an ensemble method to enrich entity linking from a state - of - the - art linking tool , which further improves the performance of the overall task ( Section 2.3 ) ; \u2022 We perform comprehensive experiments on multiple QA datasets , and our proposed method consistently outperforms previous approaches on complex questions , and produces competitive results on datasets made up of simple questions ( Section 3 ) .", "entities": [[30, 32, "TaskName", "dependency parsing"], [62, 64, "TaskName", "entity linking"]]}, {"text": "2 Approach In this section , we present our approach for solving complex KBQA .", "entities": []}, {"text": "First , we generate candidate query graphs by staged generation method ( Section2.1 ) .", "entities": []}, {"text": "Second , we measure the semantic similarities between the question and each query graph using deep neural networks ( Section 2.2 ) .", "entities": []}, {"text": "Then we introduce an ensemble approach for entity linking enrichment ( Section 2.3 ) , Finally , we discuss the prediction and parameter learning step of this task ( Section 2.4 ) .", "entities": [[7, 9, "TaskName", "entity linking"]]}, {"text": "2.1 Query Graph Generation We illustrate our staged candidate generation method in this section .", "entities": [[2, 4, "TaskName", "Graph Generation"]]}, {"text": "Compared to previous methods , such as Bao et al .", "entities": []}, {"text": "( 2016 ) , we employ a more effective candidate generation strategy , which takes advantage of implicit type information in query graphs and time interval information in the KB .", "entities": []}, {"text": "In our work , we take 4 kinds of semantic constraints into account : entity , type , time and ordinal constraints .", "entities": []}, {"text": "Figure 2shows a concrete example of our candidate generation .", "entities": []}, {"text": "For simplicity of discussion , we assume Freebase as the KB in this section .", "entities": []}, {"text": "Step 1 : Focus linking .", "entities": []}, {"text": "We extract possible ( mention , focus node ) pairs from the question .", "entities": []}, {"text": "Focus nodes are the starting points of various semantic constraints , refer to Figure 2(a ) .", "entities": []}, {"text": "For entity linking , we generate ( mention , entity ) pairs using the state - of - the - art entity linking tool SMART ( Yang and Chang , 2015 ) .", "entities": [[1, 3, "TaskName", "entity linking"], [21, 23, "TaskName", "entity linking"]]}, {"text": "For type linking , we brutally combine each type with all uni- , bi-", "entities": []}, {"text": "2187Agovernment_position   isA date_of_birth 1MaxAtN   jurisdiction basic_title   2002 > from   us_president us_vice_president president   United States   US Senate US football team   v1 v2   v3 1basic_title 2002   us_president us_vice_president president   US Senate US football team   Agovernment_position jurisdiction   United States v1 1 2002   us_president   us_vice_president president   US Senate US football team   Agovernment_position jurisdiction   United States v1 1 2002 Who is the youngest US president after 2002 ?", "entities": []}, {"text": "us_president   us_vice_president president   United States US Senate   US football team   ( a ) Focus linking ( b ) Main path generation    ( c ) Applying entity constraints   ( d ) Applying all constraints    Figure 2 : Running example of candidate generation .", "entities": []}, {"text": "and tri - gram mentions in the question , and pick top-10 ( mention , type ) pairs with the highest word embedding similarities of each pair .", "entities": []}, {"text": "For time linking , we extract time mentions by simply matching year regex .", "entities": []}, {"text": "For ordinal linking , we leverage a prede\ufb01ned superlative word list2and recognize mentions by matching superlative words , or the \u201c ordinal number + superlative \u201d pattern .", "entities": []}, {"text": "The ordinal node is an integer representing the ordinal number in the mention .", "entities": []}, {"text": "Step 2 : Main path generation .", "entities": []}, {"text": "We build different main paths by connecting the answer node to different focus entities using 1 - hop or 2 - hopwith - mediator3predicate sequence .", "entities": []}, {"text": "Figure 2(b ) shows one of the main paths .", "entities": []}, {"text": "Further constraints are attached by connecting an anchor node xto an unused focus node through predicate sequences , where the anchor node xis a non - focus node in the main path ( Aorv1 in the example ) .", "entities": []}, {"text": "Step 3 : Attaching entity constraints .", "entities": []}, {"text": "We apply a depth-\ufb01rst search to search for combinations of multiple entity constraints to the main path through 1 - hop predicate .", "entities": []}, {"text": "Figure 2(c ) shows a valid entity constraint , ( v1,basictitle , president ) .", "entities": []}, {"text": "The advantage of depth-\ufb01rst search is that we can involve unlimited number of entities in a query graph , which has a better coverage than template - based methods .", "entities": []}, {"text": "Step 4 : Type constraint generation .", "entities": []}, {"text": "Type constraints can only be applied at the answer node using IsApredicate .", "entities": []}, {"text": "Our improvement in this step is to \ufb01lter type constraints using implicit types 2~20 superlative words , such as largest , highest , latest .", "entities": []}, {"text": "3Mediator is a kind of auxiliary nodes in Freebase maintaining N - ary facts.of the answer , derived from the outgoing predicates of the answer node .", "entities": []}, {"text": "For example in Figure2(c ) , the domain type of the predicate government position ispolitician , which becomes the implicit type of the answer .", "entities": []}, {"text": "Thus we can \ufb01lter type constraints which are irrelevant to the implicit types , preventing semantic drift and speeding up the generation process .", "entities": []}, {"text": "To judge whether two types in Freebase are relevant or not , we adopt the method in Luo et al .", "entities": []}, {"text": "( 2015 ) to build a rich type hierarchy of Freebase .", "entities": []}, {"text": "Focus types are discarded , if they are not the super- or sub- types of any implicit types of the answer .", "entities": []}, {"text": "Step 5 : Time and ordinal constraint generation .", "entities": []}, {"text": "As shown in Figure 2(d ) , the time constraint is represented as a 2 - hop predicate sequence , where the second is a virtual predicate determined by the preposition before the focus time , indicating the time comparing operation , like \u201c before \u201d , \u201c after \u201d and \u201c in \u201d .", "entities": []}, {"text": "Similarly , the ordinal constraint also forms a 2 - hop predicate sequence , where the second predicate represents descending ( MaxAtN ) or ascending order ( MinAtN ) .", "entities": []}, {"text": "For the detail of time constraint , while existing approaches ( Yih et al . , 2015 ; Bao et al . , 2016 ) link the focus time with only single time predicate , our improvement is to leverage paired time predicates for representing a more accurate time constraint .", "entities": []}, {"text": "In Freebase , paired time predicates are used to represent facts within certain time intervals , like from andto4 in Figure 2(d ) .", "entities": []}, {"text": "For time comparing operation \u201c in \u201d , we link the time focus to the starting time predicate , but use both predi4Short for governmental position held.from and governmental position held.to respectively .", "entities": []}, {"text": "2188cates in SPARQL query , restricting that the focus time lies in the time interval of the paired predicates .", "entities": []}, {"text": "After \ufb01nishing all these querying stages , we translate candidate graphs into SPARQL query , and produce their \ufb01nal output answers .", "entities": []}, {"text": "Finally , we discard query graphs with zero outputs , or using overlapped mentions .", "entities": []}, {"text": "2.2 NN - based Semantic Matching Model The architecture of the proposed model is shown in Figure 3 .", "entities": []}, {"text": "We \ufb01rst replace all entity ( or time ) mentions used in the query graph by dummy tokens / an}bracketle{tE / an}bracketri}ht(or / an}bracketle{tTm / an}bracketri}ht ) .", "entities": []}, {"text": "To encode the complex query structure , we split it into predicate sequences starting from answer to focus nodes , which we call semantic components .", "entities": []}, {"text": "The predicate sequence does n\u2019t include the information of focus nodes , except for type constraints , where we append the focus type to the IsApredicate , resulting in the predicate sequence like { IsA , river } .", "entities": []}, {"text": "We introduce in detail the encoding methods for questions and predicate sequences , and how to calculate the semantic similarity score .", "entities": [[18, 20, "TaskName", "semantic similarity"]]}, {"text": "2.2.1 Semantic Component Representation To encode a semantic component p , we take the sequence of both predicate ids and predicate names into consideration .", "entities": []}, {"text": "As the example shown in Figure3 , the i d sequence of the \ufb01rst semantic component is{contained by } , and the predicate word sequence is the concatenation of canonical names for each predicate , that is { \u201c contained \u201d , \u201c by \u201d } .", "entities": []}, {"text": "Given the word sequence { p(w ) 1, ... ,p(w ) n } , we \ufb01rst use a word embedding matrix Ew\u2208R|Vw|\u00d7d to convert the original sequence into word embeddings{p(w ) 1 , ... , p(w ) n } , where|Vw|denotes the vocabulary size of natural language words , and d denotes the embedding dimension .", "entities": [[55, 57, "HyperparameterName", "embedding dimension"]]}, {"text": "Then we represent the word sequence using word averaging : p(w)=1 n / summationtext ip(w ) i.", "entities": []}, {"text": "For the i d sequence { p(id ) 1, ... ,p(id ) m } , we simply take it as a whole unit , and directly translate it into vector representation using the embedding matrixEp\u2208R|Vp\u00d7d|at path level , where |Vp|is the vocabulary size of predicate sequences .", "entities": []}, {"text": "There are two reasons for using such path embedding : 1 ) the length of i d sequence is not larger than two , based on our generation method ; 2 ) the number of distinct predicate sequences is roughly the same as the number of distinct predicates .", "entities": []}, {"text": "We get the \ufb01-nal vector of the semantic component by elementwise addition :", "entities": []}, {"text": "p = p(w)+p(id ) .", "entities": []}, {"text": "2.2.2", "entities": []}, {"text": "Question Representation", "entities": []}, {"text": "We encode the question in both global and local level , which captures the semantic information with respect to each component p.", "entities": []}, {"text": "The global information takes the token sequence as the input .", "entities": []}, {"text": "We use the same word embedding matrix Ewto convert the token sequence into vectors{q(w ) 1 , ... , q(w ) n } .", "entities": []}, {"text": "Then we encode the token sequence by applying bidirectional GRU network ( Cho et al . , 2014 ) .", "entities": [[8, 10, "MethodName", "bidirectional GRU"]]}, {"text": "The representation of the token sequence is the concatenation of the last forward and backward hidden states through the BiGRU layer , q(tok)=", "entities": [[19, 20, "MethodName", "BiGRU"]]}, {"text": "[ \u2190 \u2212h(w ) 1;\u2212 \u2192h(w )", "entities": []}, {"text": "n ] .", "entities": []}, {"text": "To encode the question at local level , we leverage dependency parsing to represent long - range dependencies between the answer and the focus node inp .", "entities": [[10, 12, "TaskName", "dependency parsing"]]}, {"text": "Since the answer is denoted by the whword in the question , we extract the dependency path from the answer node to the focus mention in the question .", "entities": []}, {"text": "Similar with Xu et", "entities": []}, {"text": "al . ( 2016 ) , we treat the path as the concatenation of words and dependency labels with directions .", "entities": []}, {"text": "For example , the dependency path between \u201c what \u201d and \u201c United States \u201d is{what,\u2212\u2212\u2212\u2192nsubj , is,\u2212\u2212\u2192prep , in,\u2212\u2212\u2192pobj , /an}bracketle{tE / an}bracketri}ht } .", "entities": []}, {"text": "We apply another bidirectional GRU layer to produce the vector representation at dependency level q(dep ) p , capturing both syntactic features and local semantic features .", "entities": [[3, 5, "MethodName", "bidirectional GRU"]]}, {"text": "Finally we combine global and local representation by element - wise addition , returning the representation of the question with respect to the semantic component , qp = q(tok)+ q(dep )", "entities": []}, {"text": "p. 2.2.3", "entities": []}, {"text": "Semantic Similarity Calculation Given the query graph with multiple semantic components , G={p(1), ... ,p(N ) } , now all its semantic components have been projected into a common vector space , representing hidden features in different aspects .", "entities": [[0, 2, "TaskName", "Semantic Similarity"]]}, {"text": "We apply max pooling over the hidden vectors of semantic components , and get the compositional semantic representation of the entire query graph .", "entities": [[2, 4, "MethodName", "max pooling"]]}, {"text": "Similarly , we perform max pooling for the question vectors with respect to each semantic component .", "entities": [[4, 6, "MethodName", "max pooling"]]}, {"text": "Finally , we compute the semantic similarity score between the graph and question : Ssem(q , G )", "entities": [[5, 7, "TaskName", "semantic similarity"]]}, {"text": "= cos(max ip(i),max iq(i ) p).(1 )", "entities": []}, {"text": "2189Acontained_by Alength 2 MaxAtN   contained by contained_by pooling   avg   what is the second longest river in the /cid31 / cid40 / cid33   what nsubj   river prep   in pobj < E > pooling   BiGRU cos   US river   AisA + BiGRU + Figure 3 : Overview of proposed semantic matching model .", "entities": [[39, 40, "MethodName", "BiGRU"], [47, 48, "MethodName", "BiGRU"]]}, {"text": "Based on this framework , our proposed method ensures the vector spaces of the question and the entire query graph are comparable , and captures complementary semantic features from different parts of the query graph .", "entities": []}, {"text": "It \u2019s worth mentioning that the semantic matching model is agnostic to the candidate generation method of the query graphs , hence it can be applied to the other existing semantic parsing frameworks .", "entities": [[30, 32, "TaskName", "semantic parsing"]]}, {"text": "2.3 Entity Linking Enrichment", "entities": [[1, 3, "TaskName", "Entity Linking"]]}, {"text": "The S - MART linker is a black box for our system , which is not extendable and tend to produce high precision but low recall linking results .", "entities": []}, {"text": "To seek a better balance at entity linking , we propose an ensemble approach to enrich linking results .", "entities": [[6, 8, "TaskName", "entity linking"]]}, {"text": "We \ufb01rst build a large lexicon by collecting all ( mention , entity ) pairs from article titles , anchor texts , redirects and disambiguation pages of Wikipedia .", "entities": []}, {"text": "Each pair is associated with statistical features , such as linking probability , letter - tri - gram jaccard similarity and popularity of the entity in Wikipedia .", "entities": []}, {"text": "For the pairs found in S - MART results , we take the above features as the input to a 2 - layer linear regression model \ufb01tting their linking scores .", "entities": [[23, 25, "MethodName", "linear regression"]]}, {"text": "Thus we learn a pseudo linking score for every pair in the lexicon , and for each question , we pick top- Khighest pairs to enrich S - MART linking results , where K is a hyperparameter .", "entities": []}, {"text": "2.4 Training and Prediction To predict the best query graph from candidates , we calculate the overall association score S(q , G ) between the question qand each candidate G , which is the weighted sum of features over entitylinking , semantic matching and structural level .", "entities": []}, {"text": "Table 1lists", "entities": []}, {"text": "the detail features .", "entities": []}, {"text": "During training step , we adopt hinge loss to maximize the margin between positive graphs G+ and negative graphs G\u2212 : loss = max{0,\u03bb\u2212S(q , G+)+S(q , G\u2212)}.(2 )", "entities": [[7, 8, "MetricName", "loss"], [21, 22, "MetricName", "loss"]]}, {"text": "For each question , we pick a candidate graph as positive data , if the F1score of its answer is larger than a threshold ( set to 0.1 in our work ) .", "entities": []}, {"text": "We randomly sample 20 negative graphs G\u2212from the candidate set whose F1is lower than the corresponding G+ .", "entities": []}, {"text": "Category Description Entity Sum of S - MART scores of all entities ; Number of entities from S - MART ; Number of entities from enriched lexicon ; Semantics Semantic similarity score Ssem(q , G ) ; Structural Number of each kind of constraints in G ; Whether a kind of constraints is used in G ; Whether the main path is one - hop ; Number of output answers .", "entities": [[29, 31, "TaskName", "Semantic similarity"]]}, {"text": "Table 1 : Full set of features .", "entities": []}, {"text": "3 Experiments In this section , we introduce the QA datasets and state - of - the - art systems that we compare .", "entities": []}, {"text": "We show the end - to - end results of the KBQA task , and perform detail analysis to investigate the importance of different modules used in our approach .", "entities": []}, {"text": "3.1 Experimental Setup QA datasets : We conduct our experiments on ComplexQuestions ( Bao et al . , 2016 ) , We-", "entities": []}, {"text": "2190bQuestions ( Berant et al . , 2013 ) and SimpleQuestions ( Bordes et al . , 2015 ) .", "entities": [[10, 11, "DatasetName", "SimpleQuestions"]]}, {"text": "We use CompQ , WebQ and SimpQ as abbreviations of the above datasets , respectively .", "entities": []}, {"text": "CompQ contains 2,100 complex questions collected from Bing search query log , and the dataset is split into 1,300 training and 800 testing questions .", "entities": []}, {"text": "WebQ contains 5,810 questions collected from Google Suggest API , and is split into 3,778 training and 2,032 testing QA pairs .", "entities": [[6, 7, "DatasetName", "Google"]]}, {"text": "Each question is manually labeled with at least one answer entity in both datasets .", "entities": []}, {"text": "SimpQ consists of more than 100 K questions , and the gold answer of each question is a gold focus entity paired with a single predicate .", "entities": []}, {"text": "This dataset is designed mainly for answering simple questions , and we use it for complementary evaluation .", "entities": []}, {"text": "Knowledge bases : For experiments on both CompQ and WebQ , we follow the settings of Berant et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2013 ) and Xu et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2016 ) to use the full Freebase dump5as the knowledge base , which contains 46 M entities and 5,323 predicates .", "entities": []}, {"text": "We host the knowledge base with Virtuoso engine6 .", "entities": []}, {"text": "For the experiments on SimpQ , the knowledge base we use is FB2 M , which is a subset of Freebase provided with the dataset , consisting 2 M entities and 10 M triple facts .", "entities": []}, {"text": "Implementation detail : For all experiments in this section , we initialize word embeddings using GloVe ( Pennington et al . , 2014 ) word vectors with dimensions set to 300 , and the size of BiGRU hidden layer is also set to 300 .", "entities": [[12, 14, "TaskName", "word embeddings"], [15, 16, "MethodName", "GloVe"], [36, 37, "MethodName", "BiGRU"]]}, {"text": "We tune the margin\u03bbin{0.1 , 0.2 , 0.5 } , the ensemble thresholdKin{1 , 2 , 3 , 5 , 10 , + INF } , and the batch sizeBin{16 , 32 , 64 } .", "entities": []}, {"text": "All the source codes , QA datasets , and detail results can be downloaded fromhttp://202.120.38.146/CompQA/ .", "entities": []}, {"text": "3.2 End - to - End Results Now we perform KBQA experiments on WebQ and CompQ. We use the average F1score over all questions as our evaluation metric .", "entities": []}, {"text": "The of\ufb01cial evaluation script7measures the correctness of output entities at string level .", "entities": []}, {"text": "While in CompQ , the annotated names of gold answer entities do n\u2019t match the case of their names in Freebase , thus we follow Bao et al .", "entities": []}, {"text": "( 2016 ) to lowercase both annotated names and the output answer names before 5detail information of the Freebase dump is available at https://github.com/syxu828/QuestionAnsweringOverFB/. 6http://virtuoso.openlinksw.com/", "entities": []}, {"text": "7The evaluation script is available at http://wwwnlp.stanford.edu/software/sempre/.calculating the F1score .", "entities": []}, {"text": "We set \u03bb= 0.5,B= 32 , K= 3 for WebQ and K= 5 for CompQ , as reaching the highest average F1on the validation set of each dataset .", "entities": []}, {"text": "We report the experimental results in Table 2 .", "entities": []}, {"text": "The result of Yih et al . ( 2015 ) on CompQ is reported by Bao et al .", "entities": []}, {"text": "( 2016 ) as their implemented result .", "entities": []}, {"text": "Our approach outperforms existing approaches on CompQ dataset , and ranks 2nd on WebQ among a long list of state - of - the - art works .", "entities": []}, {"text": "Jain ( 2016 ) achieves highest F1score on WebQ using memory networks , which is not semantic parsing based , and thus less interpretable .", "entities": [[16, 18, "TaskName", "semantic parsing"]]}, {"text": "We point out that Xu et al .", "entities": []}, {"text": "( 2016 ) uses Wikipedia texts as the external community knowledge for verifying candidate answers , and achieves a slightly higherF1score ( 53.3 ) than our model , but the performance decreases to 47.0 if this step is removed .", "entities": []}, {"text": "Besides , Yih et al . ( 2015 ) and Bao et al . ( 2016 ) used ClueWeb dataset for learning more accurate semantics , while based on the ablation test of Yih , the F1score of WebQ drops by 0.9 if ClueWeb information is removed .", "entities": []}, {"text": "Method CompQ WebQ Dong et al .", "entities": []}, {"text": "( 2015 ) - 40.8 Yao(2015 ) - 44.3 Bast and Haussmann ( 2015 ) - 49.4 Berant and Liang ( 2015 ) - 49.7 Yih et al .", "entities": []}, {"text": "( 2015 ) 36.9 52.5 Reddy et al .", "entities": []}, {"text": "( 2016 ) - 50.3 Xu et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2016 )", "entities": []}, {"text": "( w/o text ) - 47.0 Bao et al .", "entities": []}, {"text": "( 2016 ) 40.9 52.4 Jain(2016 )", "entities": []}, {"text": "- 55.6 Abujabal et al .", "entities": []}, {"text": "( 2017 ) - 51.0 Cui et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2017 ) - 34.0 Hu et al .", "entities": []}, {"text": "( 2018 ) - 49.6 Talmor and Berant ( 2018 ) 39.7 Ours ( w/o linking enrich ) 42.0 52.0", "entities": []}, {"text": "Ours ( w/ linking enrich ) 42.8 52.7 Table 2 : Average F1scores on CompQ and WebQ datasets .", "entities": []}, {"text": "Our results show that entity enrichment method improves the results on both datasets by a large margin ( 0.8 ) , which is a good help to our approach .", "entities": []}, {"text": "We argue that the enriched results are directly comparable with other approaches , as SMART itself is learned from semi - structured information in Wikipedia , such as anchor texts , redirect links and disambiguation pages , the enrichment step does not bring extra knowledge into our system .", "entities": []}, {"text": "In addition , the improvements of the candidate generation step also show a positive effect .", "entities": []}, {"text": "If we remove our implicit type \ufb01ltering in Step 4 and time interval constraints in Step 5 , the F1of CompQ slightly drops from 42.84 to 42.37 .", "entities": []}, {"text": "Al-", "entities": []}, {"text": "2191though these improvements mainly concern timerelated questions ( around 25 % in CompQ ) , we believe these strategies can be useful tricks in the further researches .", "entities": []}, {"text": "As a complementary evaluation , we perform semantic matching experiments on SimpQ. Given the gold entity of each question , we recognize the entity mention in the question , replace it with /an}bracketle{tE / an}bracketri}ht , then predict the correct predicate .", "entities": []}, {"text": "Table 3 shows the experimental results .", "entities": []}, {"text": "The best result is from Qu et al .", "entities": []}, {"text": "( 2018 ) , which learns the semantic similarity through both attentive RNN and similarity matrix based CNN .", "entities": [[7, 9, "TaskName", "semantic similarity"]]}, {"text": "Yu et al .", "entities": []}, {"text": "( 2017 ) proposed another approach using multi - layer BiLSTM with residual connections .", "entities": [[10, 11, "MethodName", "BiLSTM"]]}, {"text": "Our semantic matching model performs slightly below these two systems , since answering simple questions is not the main goal of this paper .", "entities": []}, {"text": "Comparing with these approaches , our semantic matching model is lightweighted , with a simpler structure and fewer parameters , thus is easier to tune and remains effective .", "entities": []}, {"text": "Method Relation Inputs Accuracy BiLSTM w/ words words 91.2 BiLSTM w/ rel name relname 88.9", "entities": [[3, 4, "MetricName", "Accuracy"], [4, 5, "MethodName", "BiLSTM"], [9, 10, "MethodName", "BiLSTM"]]}, {"text": "Yih et al .", "entities": []}, {"text": "( 2015 ) char-3 - gram 90.0 Yin et al .", "entities": []}, {"text": "( 2016 ) words 91.3 Yu et al .", "entities": []}, {"text": "( 2017 ) words+rel name 93.3", "entities": []}, {"text": "Qu et al .", "entities": []}, {"text": "( 2018 ) words+rel separated 93.7 Ours words+path 93.1 Table 3 : Accuracy on the SimpleQuestions dataset .", "entities": [[12, 13, "MetricName", "Accuracy"], [15, 16, "DatasetName", "SimpleQuestions"]]}, {"text": "3.3 Ablation Study In this section , we explore the contributions of various components in our system .", "entities": []}, {"text": "Semantic component representation : We \ufb01rst evaluate the results on CompQ and WebQ under different path encoding methods .", "entities": []}, {"text": "Recap that the encoding result of a semantic component is the summation of its word and i d path representations ( Section 2.2.1 ) , thus we compare encoding methods by multiple combinations .", "entities": []}, {"text": "For encoding predicate word sequence , we use BiGRU ( the same setting as encoding question word sequence ) as the alternative of average word embedding .", "entities": [[8, 9, "MethodName", "BiGRU"]]}, {"text": "For encoding predicate i d sequence , we use average predicate embedding as the alternative of the current path - level embedding ( PathEmb ) .", "entities": []}, {"text": "The experimental results are shown in Table 4 .", "entities": []}, {"text": "The encoding method None means that we do n\u2019t encode the i d or word sequence , and simply takethe result of the other sequence as the representation of the whole component .", "entities": []}, {"text": "we observe that the top three combination settings , ignoring either word or i d sequence , perform worse than the bottom three settings .", "entities": []}, {"text": "The comparison demonstrates that predicate word and i d representation can be complementary to each other .", "entities": []}, {"text": "The performance gain is not that large , mainly because predicate i d features are largely covered by their word name features .", "entities": []}, {"text": "For the encoding of i d sequences , PathEmb works better than average embedding , consistently boostingF1by 0.65 on both datasets .", "entities": []}, {"text": "The former method treats the whole sequence as a single unit , which is more \ufb02exible and can potentially learn diverse representations of i d sequences that share the same predicates .", "entities": []}, {"text": "For the encoding of word sequences , the average word embedding method outperforms BiGRU on CompQ , and the gap becomes smaller when running on WebQ. This is mainly because the training set of WebQ is about 3 times larger than that of CompQ , making it easier for training a more complex model .", "entities": [[13, 14, "MethodName", "BiGRU"]]}, {"text": "Word repr .", "entities": []}, {"text": "I d repr .", "entities": []}, {"text": "CompQF1WebQF1", "entities": []}, {"text": "None PathEmb 41.11 51.86 Average None 42.18 51.74 BiGRU None 41.80 51.87 Average Average 42.16 52.00 BiGRU PathEmb 41.52 52.33 Average PathEmb", "entities": [[8, 9, "MethodName", "BiGRU"], [16, 17, "MethodName", "BiGRU"]]}, {"text": "42.84 52.66 Table 4 : Ablation results on path representation .", "entities": []}, {"text": "Semantic composition and question representation : To demonstrate the effectiveness of semantic composition , we construct a straightforward baseline , where we remove the max pooling operation in Eq .", "entities": [[0, 2, "TaskName", "Semantic composition"], [11, 13, "TaskName", "semantic composition"], [24, 26, "MethodName", "max pooling"]]}, {"text": "( 2 ) , and instead calculate the semantic similarity score as the summation of individual cosine similarities : Ssem(q , G )", "entities": [[8, 10, "TaskName", "semantic similarity"]]}, {"text": "= /summationtext icos(p(i),q(i ) p ) .", "entities": []}, {"text": "For methods of question encoding , we setup ablations by turning off either sentential encoding or dependency encoding .", "entities": []}, {"text": "Table 5shows the ablation results on CompQ and WebQ. When dependency path information is augmented with sentential information , the performance boosts by 0.42 on average .", "entities": []}, {"text": "Dependency paths focus on hidden features at syntactic and functional perspective , which is a good complementary to sentential encoding results .", "entities": []}, {"text": "However , performances drop by 2.17 if only dependency information is used , we \ufb01nd that under certain dependency structures , crucial words ( bolded ) are", "entities": []}, {"text": "2192not in the path between the answer and the focus mention ( underlined ) , for example , \u201c who did draco malloy end up marrying \u201d and \u201c who did the philip pines gain independence from \u201d .", "entities": []}, {"text": "While we observe about 5 % of such questions in WebQ , it \u2019s hard to predict the correct query graph without crucial words .", "entities": []}, {"text": "In terms of semantic composition , Our max pooling based method consistently outperforms the baseline method .", "entities": [[3, 5, "TaskName", "semantic composition"], [7, 9, "MethodName", "max pooling"]]}, {"text": "The improvement on WebQ is smaller than on CompQ , largely due to the fact that 85 % questions in WebQ are simple questions ( Bao et al . , 2016 ) .", "entities": []}, {"text": "As a result of combination , our approach signi\ufb01cantly outperforms the vanilla SP+NN approach on CompQ by 1.28 , demonstrating the effectiveness of our approach .", "entities": []}, {"text": "Theoretically , the pooling outcome may lead to worse end - to - end result when there are too many semantic components in one graph , because the pooling layer takes too many vectors as input , different semantic features between similar query graphs become indistinguishable .", "entities": []}, {"text": "In our task , only 0.5 % of candidate graphs have more than 3 semantic components , so pooling is a reasonable way to aggregate semantic components in this scenario .", "entities": []}, {"text": "Composition Qrepr CompQF1WebQF1 Baseline sentential 41.56 52.14 Baseline", "entities": []}, {"text": "both 42.35 52.39", "entities": []}, {"text": "Ours dependency 41.48 49.69 Ours sentential 42.59 52.28 Ours both 42.84 52.66 Table 5 : Ablation results on question representation and compositional strategy .", "entities": []}, {"text": "To further explain the advantage of semantic composition , we take the following question as an example : \u201c who is gimli \u2019s father in the hobbit \u201d .", "entities": [[6, 8, "TaskName", "semantic composition"]]}, {"text": "Two query graphs are likely to be the \ufb01nal answer : 1 ) ( ? , children , gimliperson ) ; 2 ) ( ? , fictional children , gimlicharacter ) \u2227 ( ? , appearin , hobbit ) .", "entities": []}, {"text": "If observing semantic components individually , the predicate children is most likely to be the correct one since \u201c \u2019s father \u201d is highly related and with plenty of positive training data .", "entities": []}, {"text": "Both fictional children andappearinget a much lower similarity compared with children , hence the baseline method prefer the \ufb01rst query graph .", "entities": []}, {"text": "In the meantime , our proposed method learns the hidden semantics of the second candidate by absorbing salient features from both predicates , and such compositional representation is closer to the semantics of the entire question thana simple \u201c children \u201d predicate .", "entities": []}, {"text": "That \u2019s why our method manages to answer it correctly .", "entities": []}, {"text": "3.4 Error Analysis We randomly analyzed 100 questions from CompQ where no correct answers are returned .", "entities": [[1, 2, "MetricName", "Error"]]}, {"text": "We list the major causes of errors as follows : Main path error ( 10 % ): This type of error occurred when the model failed to understand the main semantics when facing some dif\ufb01cult questions ( e.g. \u201c What native american sports heroes earning two gold medals in the 1912 Olympics \u201d ) ; Constraint missing ( 42 % ): These types of questions involve implicit constraints , for example , the question \u201c Who was US president when Traicho Kostov was teenager \u201d is dif\ufb01cult to answer because it implies an implicit time constraint \u201c when Traicho Kostov was teenager \u201d ; Entity linking error ( 16 % ): This error occurs due to the highly ambiguity of mentions .", "entities": [[104, 106, "TaskName", "Entity linking"]]}, {"text": "For example , the question \u201c What character did Robert Pattinson play in Harry Potter \u201d expects the \ufb01lm \u201c Harry Potter and the Goblet of Fire \u201d as the focus , while there are 7 movies in Harry Potter series ; Miscellaneous ( 32 % ):", "entities": [[42, 43, "TaskName", "Miscellaneous"]]}, {"text": "This error class contains questions with semantic ambiguity or not reasonable .", "entities": []}, {"text": "For example , the question \u201c Where is Byron Nelson 2012 \u201d is hard to understand , because \u201c Byron Nelson \u201d died in 2006 and maybe this question wants to ask where did he die .", "entities": []}, {"text": "4 Related Work Knowledge Base Question Answering(KBQA ) has been a hot research top in recent years .", "entities": []}, {"text": "Generally speaking , the most popular methods for KBQA can be mainly divided into two classes : information retrieval and semantic parsing .", "entities": [[17, 19, "TaskName", "information retrieval"], [20, 22, "TaskName", "semantic parsing"]]}, {"text": "Information retrieval based system tries to obtain target answer directly from question information and KB knowledge without explicit considering interior query structure .", "entities": [[0, 2, "TaskName", "Information retrieval"]]}, {"text": "There are various methods ( Yao and Van Durme , 2014 ; Bordes et al . , 2015 ;", "entities": []}, {"text": "Dong et al . , 2015 ; Xu et", "entities": []}, {"text": "al . , 2016 ) to select candidate answers and to rank results .", "entities": []}, {"text": "Semantic parsing based approach focuses on constructing a semantic parsing tree or equivalent query structure that represents the semantic meaning of the question .", "entities": [[0, 2, "TaskName", "Semantic parsing"], [8, 10, "TaskName", "semantic parsing"]]}, {"text": "In terms of logical representation of natural language questions , many methods have been tried , such as query graph ( Yih et al . ,", "entities": []}, {"text": "21932014 , 2015 ) or RDF query language ( Unger et", "entities": []}, {"text": "al . , 2012 ;", "entities": []}, {"text": "Cui et al . , 2017 ; Hu et al . , 2018 ) .", "entities": []}, {"text": "Recently , as the development of deep learning , NN - based approaches have been combined into the KBQA task ( Bordes et al . , 2014 ) , showing promising result .", "entities": []}, {"text": "These approaches tries to use neural network models to encode both questions and answers ( or query structures ) into the vector space .", "entities": []}, {"text": "Subsequently , similarity functions are used to select the most appropriate query structure to generate the \ufb01nal answer .", "entities": []}, {"text": "For example , Bordes et al .", "entities": []}, {"text": "( 2014 ) focuses on embedding the subgraph of the candidate answer ;", "entities": []}, {"text": "Yin et al .", "entities": []}, {"text": "( 2016 ) uses character - level CNN and word - level CNN to match different information ;", "entities": []}, {"text": "Yu et al .", "entities": []}, {"text": "( 2017 ) introduces the method of hierarchical residual RNN to compare questions and relation names ;", "entities": []}, {"text": "Qu et al .", "entities": []}, {"text": "( 2018 ) proposes the AR - SMCNN model , which uses RNN to capture semantic - level correlation and employs CNN to extract literallevel words interaction .", "entities": []}, {"text": "Belonging to NN - based semantic parsing category , our approach employs a novel encoding structure method to solve complex questions .", "entities": [[5, 7, "TaskName", "semantic parsing"]]}, {"text": "Previous works such as Yih et al . ( 2015 ) and Bao et al .", "entities": []}, {"text": "( 2016 ) require a recognition of a main relation and regard other constraints as variables added to this main relation .", "entities": []}, {"text": "Unlike their approaches , our method encodes multiple relations ( paths ) into a uniform query structure representation ( semantic composition ) , which allows more \ufb02exible query structures .", "entities": [[19, 21, "TaskName", "semantic composition"]]}, {"text": "There are also some works ca n\u2019t be simply classi\ufb01ed in to IR based methods or SP based methods .", "entities": []}, {"text": "Jain ( 2016 ) introduces Factual Memory Network , which tries to encode KB and questions in same word vector space , extract a subset of initial candidate facts , then try to employ multi - hop reasoning and re\ufb01nement to \ufb01nd a path to answer entity .", "entities": [[6, 8, "MethodName", "Memory Network"]]}, {"text": "Reddy et al . ( 2016 ) , Abujabal et al . ( 2017 ) , andCui et al .", "entities": []}, {"text": "( 2017 ) try to interpret question intention by templates , which learned from KB or QA corpora .", "entities": []}, {"text": "Talmor and Berant ( 2018 ) attempts to answering complex questions by decomposing them into a sequence of simple questions .", "entities": []}, {"text": "5 Conclusion To the best of our knowledge , this is the \ufb01rst work to handle complex KBQA task by explicitly encoding the complete semantics of a complex query graph using neural networks .", "entities": []}, {"text": "We stud - ied different methods to further improve the performance , mainly leveraging dependency parse and the ensemble method for linking enrichment .", "entities": []}, {"text": "Our model becomes the state - of - the - art on ComplexQuestions dataset , and produces competitive results on other simple question based datasets .", "entities": []}, {"text": "Possible future work includes supporting more complex semantics like implicit time constraints .", "entities": []}, {"text": "Acknowledgment Kenny Q. Zhu is the contact author and was supported by NSFC grants 91646205 and 61373031 .", "entities": []}, {"text": "Thanks to the anonymous reviewers for their valuable feedback .", "entities": []}, {"text": "References Abdalghani Abujabal , Mohamed Yahya , Mirek Riedewald , and Gerhard Weikum . 2017 .", "entities": []}, {"text": "Automated template generation for question answering over knowledge graphs .", "entities": [[4, 6, "TaskName", "question answering"], [7, 9, "TaskName", "knowledge graphs"]]}, {"text": "In WWW , pages 1191\u20131200 .", "entities": []}, {"text": "S\u00a8oren Auer , Christian Bizer , Georgi Kobilarov , Jens Lehmann , Richard Cyganiak , and Zachary Ives .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Dbpedia : A nucleus for a web of open data .", "entities": [[0, 1, "DatasetName", "Dbpedia"]]}, {"text": "Springer .", "entities": []}, {"text": "Jun - Wei Bao , Nan Duan , Zhao Yan , Ming Zhou , and Tiejun Zhao .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Constraint - based question answering with knowledge graph .", "entities": [[3, 5, "TaskName", "question answering"]]}, {"text": "In COLING , pages 2503\u20132514 .", "entities": []}, {"text": "Hannah Bast and Elmar Haussmann .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "More accurate question answering on freebase .", "entities": [[2, 4, "TaskName", "question answering"]]}, {"text": "In CIKM , pages 1431\u20131440 .", "entities": []}, {"text": "Jonathan Berant , Andrew Chou , Roy Frostig , and Percy Liang .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Semantic parsing on freebase from question - answer pairs .", "entities": [[0, 2, "TaskName", "Semantic parsing"]]}, {"text": "In EMNLP , pages 1533 \u2013 1544 .", "entities": []}, {"text": "Jonathan Berant and Percy Liang . 2015 .", "entities": []}, {"text": "Imitation learning of agenda - based semantic parsers .", "entities": [[0, 2, "TaskName", "Imitation learning"]]}, {"text": "Transactions of the Association for Computational Linguistics , 3:545\u2013558 .", "entities": []}, {"text": "Kurt Bollacker , Colin Evans , Praveen Paritosh , Tim Sturge , and Jamie Taylor . 2008 .", "entities": []}, {"text": "Freebase : a collaboratively created graph database for structuring human knowledge .", "entities": []}, {"text": "In SIGMOD , pages 1247\u20131250 .", "entities": []}, {"text": "Antoine Bordes , Nicolas Usunier , Sumit Chopra , and Jason Weston .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Large - scale simple question answering with memory networks .", "entities": [[4, 6, "TaskName", "question answering"]]}, {"text": "arXiv preprint arXiv:1506.02075 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Antoine Bordes , Jason Weston , and Nicolas Usunier .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Open question answering with weakly supervised embedding models .", "entities": [[1, 3, "TaskName", "question answering"]]}, {"text": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases , pages 165\u2013180 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "2194Qingqing Cai and Alexander Yates .", "entities": []}, {"text": "2013 .", "entities": []}, {"text": "Large - scale semantic parsing via schema matching and lexicon extension .", "entities": [[3, 5, "TaskName", "semantic parsing"]]}, {"text": "In ACL , pages 423\u2013433 .", "entities": []}, {"text": "Kyunghyun Cho , Bart Van Merri \u00a8enboer , Dzmitry Bahdanau , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "On the properties of neural machine translation : Encoder - decoder approaches .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "Proceedings of SSST-8 , pages 103\u2013111 .", "entities": []}, {"text": "Wanyun Cui , Yanghua Xiao , Haixun Wang , Yangqiu Song , Seung - won Hwang , and Wei Wang . 2017 .", "entities": []}, {"text": "Kbqa : learning question answering over qa corpora and knowledge bases .", "entities": [[3, 5, "TaskName", "question answering"]]}, {"text": "Proceedings of the VLDB Endowment , 10(5):565\u2013576 .", "entities": []}, {"text": "Li Dong , Furu Wei , Ming Zhou , and Ke Xu . 2015 .", "entities": []}, {"text": "Question answering over freebase with multicolumn convolutional neural networks .", "entities": [[0, 2, "TaskName", "Question answering"]]}, {"text": "In ACL ( 1 ) , pages 260\u2013269 .", "entities": []}, {"text": "Sen Hu , Lei Zou , Jeffrey Xu Yu , Haixun Wang , and Dongyan Zhao .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Answering natural language questions by subgraph matching over knowledge graphs .", "entities": [[8, 10, "TaskName", "knowledge graphs"]]}, {"text": "IEEE Transactions on Knowledge and Data Engineering , 30(5):824\u2013837 .", "entities": []}, {"text": "Sarthak Jain . 2016 .", "entities": []}, {"text": "Question answering over knowledge base using factual memory networks .", "entities": [[0, 2, "TaskName", "Question answering"]]}, {"text": "In Proceedings of the NAACL Student Research Workshop , pages 109\u2013115 .", "entities": []}, {"text": "Kangqi Luo , Xusheng Luo , and Kenny Zhu . 2015 .", "entities": []}, {"text": "Inferring binary relation schemas for open information extraction .", "entities": [[5, 8, "TaskName", "open information extraction"]]}, {"text": "In EMNLP , pages 555\u2013560 .", "entities": []}, {"text": "Jeffrey Pennington , Richard Socher , and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Glove : Global vectors for word representation .", "entities": []}, {"text": "In EMNLP , pages 1532\u20131543 .", "entities": []}, {"text": "Yingqi Qu , Jie Liu , Liangyi Kang , Qinfeng Shi , and Dan Ye . 2018 .", "entities": []}, {"text": "Question answering over freebase via attentive rnn with similarity matrix based cnn .", "entities": [[0, 2, "TaskName", "Question answering"]]}, {"text": "arXiv preprint arXiv:1804.03317 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Siva Reddy , Oscar T \u00a8ackstr \u00a8om , Michael Collins , Tom Kwiatkowski , Dipanjan Das , Mark Steedman , and Mirella Lapata .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Transforming dependency structures to logical forms for semantic parsing .", "entities": [[7, 9, "TaskName", "semantic parsing"]]}, {"text": "Transactions of the Association for Computational Linguistics , 4:127\u2013140 .", "entities": []}, {"text": "Fabian M Suchanek , Gjergji Kasneci , and Gerhard Weikum .", "entities": []}, {"text": "2007 .", "entities": []}, {"text": "Yago : a core of semantic knowledge .", "entities": [[0, 1, "DatasetName", "Yago"]]}, {"text": "In WWW , pages 697\u2013706 .", "entities": []}, {"text": "ACM .", "entities": [[0, 1, "DatasetName", "ACM"]]}, {"text": "Alon Talmor and Jonathan Berant .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "The web as a knowledge - base for answering complex questions .", "entities": []}, {"text": "InNAACL - HLT .", "entities": []}, {"text": "Christina Unger , Lorenz B \u00a8uhmann , Jens Lehmann , Axel - Cyrille Ngonga Ngomo , Daniel Gerber , and Philipp Cimiano .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Template - based question answering over rdf data .", "entities": [[3, 5, "TaskName", "question answering"]]}, {"text": "In WWW , pages 639\u2013648 .", "entities": []}, {"text": "ACM.Kun Xu , Siva Reddy , Yansong Feng , Songfang Huang , and Dongyan Zhao .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Question answering on freebase via relation extraction and textual evidence .", "entities": [[0, 2, "TaskName", "Question answering"], [5, 7, "TaskName", "relation extraction"]]}, {"text": "InACL , pages 2326\u20132336 .", "entities": []}, {"text": "Yi Yang and Ming - Wei Chang .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "S - mart : Novel tree - based structured learning algorithms applied to tweet entity linking .", "entities": [[14, 16, "TaskName", "entity linking"]]}, {"text": "In ACL - IJCNLP , pages 504 \u2013 513 .", "entities": []}, {"text": "Xuchen Yao . 2015 .", "entities": []}, {"text": "Lean question answering over freebase from scratch .", "entities": [[1, 3, "TaskName", "question answering"]]}, {"text": "In NAACL , pages 66\u201370 .", "entities": []}, {"text": "Xuchen Yao and Benjamin Van Durme .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Information extraction over structured data : Question answering with freebase .", "entities": [[6, 8, "TaskName", "Question answering"]]}, {"text": "In ACL , pages 956\u2013966 .", "entities": []}, {"text": "Wen - tau Yih , Ming - Wei Chang , Xiaodong He , and Jianfeng Gao . 2015 .", "entities": []}, {"text": "Semantic parsing via staged query graph generation :", "entities": [[0, 2, "TaskName", "Semantic parsing"], [5, 7, "TaskName", "graph generation"]]}, {"text": "Question answering with knowledge base .", "entities": [[0, 2, "TaskName", "Question answering"]]}, {"text": "In ACL - IJCNLP , pages 1321 \u2013 1331 .", "entities": []}, {"text": "Wen - tau Yih , Xiaodong He , and Christopher Meek .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Semantic parsing for single - relation question answering .", "entities": [[0, 2, "TaskName", "Semantic parsing"], [6, 8, "TaskName", "question answering"]]}, {"text": "In ACL , volume 2 , pages 643\u2013648 .", "entities": []}, {"text": "Wenpeng Yin , Mo Yu , Bing Xiang , Bowen Zhou , and Hinrich Sch \u00a8utze . 2016 .", "entities": []}, {"text": "Simple question answering by attentive convolutional neural network .", "entities": [[1, 3, "TaskName", "question answering"]]}, {"text": "In COLING , pages 1746\u20131756 .", "entities": []}, {"text": "Mo Yu , Wenpeng Yin , Kazi Saidul Hasan , Cicero dos Santos , Bing Xiang , and Bowen Zhou .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Improved neural relation detection for knowledge base question answering .", "entities": [[5, 9, "TaskName", "knowledge base question answering"]]}, {"text": "In ACL , pages 571\u2013581 .", "entities": []}]