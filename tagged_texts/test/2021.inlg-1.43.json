[{"text": "Proceedings of the 14th International Conference on Natural Language Generation ( INLG ) , pages 387\u2013403 , Aberdeen , Scotland , UK , 20 - 24 September 2021 .", "entities": []}, {"text": "\u00a9 2021 Association for Computational Linguistics387BERT - based distractor generation for Swedish reading comprehension questions using a small - scale dataset Dmytro Kalpakchi Division of Speech , Music and Hearing KTH Royal Institute of Technology Stockholm , Sweden dmytroka@kth.seJohan Boye Division of Speech , Music and Hearing KTH Royal Institute of Technology Stockholm , Sweden jboye@kth.se", "entities": [[8, 10, "TaskName", "distractor generation"], [12, 14, "TaskName", "reading comprehension"], [30, 31, "DatasetName", "KTH"], [47, 48, "DatasetName", "KTH"]]}, {"text": "Abstract", "entities": []}, {"text": "An important part when constructing multiplechoice questions ( MCQs ) for reading comprehension assessment are the distractors , the incorrect but preferably plausible answer options .", "entities": [[11, 13, "TaskName", "reading comprehension"]]}, {"text": "In this paper , we present a new BERTbased method for automatically generating distractors using only a small - scale dataset .", "entities": []}, {"text": "We also release a new such dataset of Swedish MCQs ( used for training the model ) , and propose a methodology for assessing the generated distractors .", "entities": []}, {"text": "Evaluation shows that from a student \u2019s perspective , our method generated one or more plausible distractors for more than 50 % of the MCQs in our test set .", "entities": []}, {"text": "From a teacher \u2019s perspective , about 50 % of the generated distractors were deemed appropriate .", "entities": []}, {"text": "We also do a thorough analysis of the results .", "entities": []}, {"text": "1 Introduction Multiple - choice questions ( MCQs ) are widely used for student assessments , from high - stakes graduation tests to lower - stakes reading comprehension tests .", "entities": [[26, 28, "TaskName", "reading comprehension"]]}, {"text": "An MCQ consists of a question ( stem ) , the correct answer ( key ) and a number of wrong , but plausible options ( distractors ) .", "entities": []}, {"text": "The problem of automatically generating stems with a key has received a great deal of attention , e.g. , see the survey by Amidei et al .", "entities": []}, {"text": "( 2018 )", "entities": []}, {"text": ".", "entities": []}, {"text": "By comparison , automatically generating distractors is substantially less researched , although Welbl et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2017 ) report that manually \ufb01nding reasonable distractors was the most time - consuming part in writing science MCQs .", "entities": []}, {"text": "Indeed , reasonable distractors should be grammatically consistent and similar in length compared to the key and within themselves .", "entities": []}, {"text": "Given the challenges above , we attempt using machine learning ( ML ) to aid teachers in creating distractors for reading comprehension MCQs .", "entities": [[20, 22, "TaskName", "reading comprehension"]]}, {"text": "The problem is not new , however most of the prior work has been done for English .", "entities": []}, {"text": "In this paper we proposethe \ufb01rst such solution for Swedish ( although the proposed method is novel even for English , to the best of our knowledge ) .", "entities": []}, {"text": "The key contributions of this work are : proposing a BERT - based method for generating distractors using only a small - scale dataset , releasing SweQUAD - MC1 , a dataset of Swedish MCQs , and proposing a methodology for conducting human evaluation aimed at assessing the plausibility of distractors .", "entities": [[10, 11, "MethodName", "BERT"]]}, {"text": "2 Background 2.1 BERT for NLG Devlin et", "entities": [[3, 4, "MethodName", "BERT"]]}, {"text": "al .", "entities": []}, {"text": "( 2019 ) introduced BERT as the \ufb01rst application of the Transformer architecture ( Vaswani et al . , 2017 ) to language modelling .", "entities": [[4, 5, "MethodName", "BERT"], [11, 12, "MethodName", "Transformer"], [22, 24, "TaskName", "language modelling"]]}, {"text": "BERT uses only Transformer \u2019s encoder stacks ( with multihead self - attention , MHSA ) , while the NLG community relies more on Transformer \u2019s decoder stacks ( with masked MHSA ) for text generation , e.g. , GPT ( Radford et al . , 2018 ) .", "entities": [[0, 1, "MethodName", "BERT"], [3, 4, "MethodName", "Transformer"], [24, 25, "MethodName", "Transformer"], [34, 36, "TaskName", "text generation"], [39, 40, "MethodName", "GPT"]]}, {"text": "However , Wang and Cho ( 2019 ) showed that BERT is a Markov random \ufb01eld , meaning that BERT learns a joint probability distribution over all sentences of a \ufb01xed length , and one could use Gibbs sampling to generate a new sentence .", "entities": [[10, 11, "MethodName", "BERT"], [19, 20, "MethodName", "BERT"]]}, {"text": "The authors compared samples generated autoregressively left - to - right by BERT and GPT , and found the perplexity of BERT samples to be higher than GPT \u2019s ( BERT samples are of worse quality ) , but the n - gram overlap between the generated texts and texts from the dataset to be lower ( BERT samples are more diverse ) .", "entities": [[12, 13, "MethodName", "BERT"], [14, 15, "MethodName", "GPT"], [19, 20, "MetricName", "perplexity"], [21, 22, "MethodName", "BERT"], [27, 28, "MethodName", "GPT"], [30, 31, "MethodName", "BERT"], [57, 58, "MethodName", "BERT"]]}, {"text": "Liao et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) show a way to improve BERT \u2019s generation capabilities via changing the masking scheme to a probabilistic one at training time .", "entities": [[8, 9, "MethodName", "BERT"]]}, {"text": "Probabilistically masked language models ( PMLMs ) assume that the masking ratio rfor each sentence is drawn from a prior distribution p(r ) .", "entities": []}, {"text": "The au1The dataset and implementation of our models are available in this GitHub repository", "entities": []}, {"text": "388Property Training Development Test # of texts 434 64 45 # of MCQs 962 126 102 # of D 2:1\u00060:5 2:1\u00060:4 2:0\u00060:2 Len(Text )", "entities": []}, {"text": "384:9\u0006330:1 355:1\u0006233:1 357:9\u0006254:3 Len(A ) 4:2\u00063:4 4:4\u00063:5 4:6\u00064:5 Len(D ) 4:5\u00063:9 4:3\u00064:0 4:0\u00063:7 jLen(A ) - Len(D)j 1:9\u00062:4 1:9\u00062:3 1:9\u00062:9 Table 1 : Descriptive statistics of SweQUAD - MC dataset splits .", "entities": []}, {"text": "A denotes the key , D denotes a distractor , Len(X ) denotes a length of X in words .", "entities": []}, {"text": "x\u0006yshows mean xand a standard deviation y thors proposed to train a PMLM with a uniform prior ( referred to as u - PMLM ) .", "entities": [[12, 13, "MethodName", "PMLM"], [23, 24, "MethodName", "PMLM"]]}, {"text": "The absence of the left - to - right restriction allows the model to generate sequences in an word arbitrary order .", "entities": []}, {"text": "In fact , Liao et al .", "entities": []}, {"text": "( 2020 ) propose to generate sentences by randomly selecting the masked position , predicting a token for it , replacing the masked token with the predicted one and repeating the process until no masked tokens are left .", "entities": []}, {"text": "The authors showed that the perplexity of the texts generated by u - PMLM is comparable to the ones by GPT .", "entities": [[5, 6, "MetricName", "perplexity"], [13, 14, "MethodName", "PMLM"], [20, 21, "MethodName", "GPT"]]}, {"text": "2.2 Convolution partial tree kernels As mentioned previously , plausible distractors should be grammatically consistent with the key .", "entities": [[1, 2, "MethodName", "Convolution"]]}, {"text": "Hence , a metric measuring grammatical consistency would be useful both for quantitative evaluation and as a basis for a baseline method .", "entities": []}, {"text": "We propose to use convolution partial tree kernels ( CPTK ) for these purposes .", "entities": [[4, 5, "MethodName", "convolution"]]}, {"text": "CPTK were proposed by Moschitti ( 2006 ) for dependency trees and essentially calculate the number of common tree structures ( not only full subtrees ) between two given trees .", "entities": []}, {"text": "However , CPTKs can not handle labeled edges and were applied to dependency trees containing only lexicals .", "entities": []}, {"text": "Another solution , proposed by Croce et al . ( 2011 ) and used in this article , is to include edge labels , i.e. , grammatical relations ( GR ) , as separate nodes .", "entities": []}, {"text": "A resulting computational structure is Grammatical Relation Centered Tree ( GRCT ) , which transforms the original dependency tree by making each PoS - tag a child of a GR node and a father of a lexical node .", "entities": []}, {"text": "CPTKs can take any non - negative values and are thus hard to interpret .", "entities": []}, {"text": "Hence , we use normalized CPTK ( NCPTK ) shown in Equation ( 1 ) , whereK(T1;T2)is the CPTK applied to the dependency trees T1andT2 .", "entities": []}, {"text": "eK(T1;T2 ) = K(T1;T2)p K(T1;T1)p K(T2;T2);(1)Evidently , when T1andT2are the same , eK(T1;T2 ) equals to 1 , which is the highest value it can take .", "entities": []}, {"text": "3 Data We have collected a Swedish dataset , henceforth referred to as SweQUAD - MC , consisting of texts and MCQs for the given texts .", "entities": []}, {"text": "The dataset was created by three paid linguistics students instructed to pose unambiguous and independent questions .", "entities": []}, {"text": "They were also asked to identify the key with at least two distractors , all of which are contiguous phrases in a given text .", "entities": []}, {"text": "Additionally , as the distractors were required to be in the same grammatical form as the key ( e.g. , both in plural ) , the students were allowed to change the grammatical form of phrases if they constituted plausible distractors after this change .", "entities": []}, {"text": "The exact instructions given to the students along with more details on the used texts are provided in Appendix A.", "entities": []}, {"text": "Each datapoint in SweQUAD - MC consists of a base text and an MCQ , i.e. a stem , the key and at least two distractors .", "entities": []}, {"text": "The same text can be reused for different MCQs , but the sets of texts in training ( \u001880 % ) , development ( \u001810 % ) and test ( \u001810 % ) datasets are disjoint .", "entities": []}, {"text": "However , some overlap in sentences is possible , since the texts might come from the same source .", "entities": []}, {"text": "Descriptive statistics of all SweQUAD - MC splits is provided in Table 1 . 4 Method Given the small scale of SweQUAD - MC we have decided to \ufb01ne - tune a pretrained BERT2for Swedish ( Malmsten et al . , 2020 ) on the task of distractor generation ( DG ) .", "entities": [[47, 49, "TaskName", "distractor generation"]]}, {"text": "For achieving this , we have added on top of BERT two linear layers with layer normalization ( Ba et al . , 2016 ) in the middle to be trained from scratch ( see architecture in Figure 1 ) .", "entities": [[10, 11, "MethodName", "BERT"], [15, 17, "MethodName", "layer normalization"]]}, {"text": "The last linear layer is followed by a 2bert - base - cased", "entities": [[2, 4, "MethodName", "linear layer"]]}, {"text": "389softmax activation giving probabilities over the tokens in the vocabulary for each position in the text .", "entities": []}, {"text": "We trained the model using cross - entropy loss only for tokens in masked positions .", "entities": [[8, 9, "MetricName", "loss"]]}, {"text": "Recall that each MCQ consists of a base text T , the stem Qbased on T , the key Aand ( on average ) two distractors D1andD2 .", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "The DG problem is then to generate distractors conditioned on the context , consisting of T , QandA. We provide all context components as input to the BERT model , separated from each other by the special separator token", "entities": [[27, 28, "MethodName", "BERT"]]}, {"text": "[ SEP ] .", "entities": []}, {"text": "Given that BERT \u2019s maximum input length is 512 tokens , we trim Tto the \ufb01rst 384 tokens ( later referred to as T384 ) , since that is the average text length of the training set .", "entities": [[2, 3, "MethodName", "BERT"]]}, {"text": "We have explored two different solution variants of DG .", "entities": []}, {"text": "The \ufb01rst variant aims at generating distractors autoregressively , left to right .", "entities": []}, {"text": "At generation time , the input to BERT consists of a context CTX ( T384,QandAseparated by [ SEP ] token ) , a[SEP ] token , and a [ MASK ] token at the end .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "After a forward pass through BERT , the [ MASK ] token gets replaced by the word with the highest softmax score , which becomes the \ufb01rst word of the \ufb01rst distractor ( dubbed D11 ) .", "entities": [[5, 6, "MethodName", "BERT"], [20, 21, "MethodName", "softmax"]]}, {"text": "The generation of the \ufb01rst distractor continues by appending a [ MASK ] token after each forward pass until the network generates a separator token", "entities": []}, {"text": "[ SEP ] , which concludes the generation of the \ufb01rst distractor D1 .", "entities": []}, {"text": "The next distractor D2is generated in the same way , except that the CTX is extended by D1 .", "entities": []}, {"text": "At training time , we use the same procedure , but with teacher forcing , allowing us to use the correct distractor tokens as targets for the cross - entropy loss ( see example training datapoints for one MCQ in Table 2 ) .", "entities": [[30, 31, "MetricName", "loss"]]}, {"text": "The second variant is inspired by u - PMLM , and aims at generating distractors autoregressively , but in an arbitrary word order .", "entities": [[8, 9, "MethodName", "PMLM"]]}, {"text": "At generation time , the input to BERT consists of a context CTX , a[SEP ] token , and a prede\ufb01ned number of [ MASK ] tokens ( see Section 6.1 ) .", "entities": [[7, 8, "MethodName", "BERT"]]}, {"text": "The generation proceeds by unmasking the token at the position where the model is most con\ufb01dent .", "entities": []}, {"text": "This differs from unmasking a random position , proposed by Liao et al . ( 2020 ) .", "entities": []}, {"text": "The training procedure largely follows a masking scheme employed by u - PMLM by drawing the masking ratio from the uniform distribution ( see example training datapoints for one MCQ in Table 2 ) .", "entities": [[12, 13, "MethodName", "PMLM"]]}, {"text": "Note that we do not include the [ SEP ] token when training , since we found that the trained model would constantly generate [ SEP ] tokens .", "entities": []}, {"text": "Linear GELULinear SOFTMAX LayerNorm BERT(B , 512 , 768)(B , 512 , 768)(B , 512 , 768)(B , 512 , V )", "entities": [[2, 3, "MethodName", "SOFTMAX"]]}, {"text": "[ CLS ] T [ SEP ]   Q", "entities": []}, {"text": "[ SEP ]", "entities": []}, {"text": "A [ SEP ] D11 D12", "entities": []}, {"text": "[ MASK ]", "entities": []}, {"text": "[ CLS ] T", "entities": []}, {"text": "[ SEP ]   Q", "entities": []}, {"text": "[ SEP ]", "entities": []}, {"text": "A [ SEP ]", "entities": []}, {"text": "[ MASK ] D12 [ MASK ] left to   right u - PMLMD13 D11 D13left to   right u - PMLMCross - entropy loss Input data LabelsFigure 1 : The DG model architecture .", "entities": [[24, 25, "MetricName", "loss"]]}, {"text": "B is the batch size and V is the vocabulary size .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}, {"text": "The light green blocks represent the activation functions for the respective linear layers .", "entities": []}, {"text": "The purple block represents parts of the network initialized with the pretrained weights .", "entities": []}, {"text": "Each sampled masking ratio rfor the u - PMLM variant means that each token in the distractors from the dataset has a probability rto be masked .", "entities": [[8, 9, "MethodName", "PMLM"]]}, {"text": "Hence , different rwill potentially result in different number of masked tokens and at different positions .", "entities": []}, {"text": "The number of times we draw rper distractor DX is proposed to be min ( Len(DX);MAXMASKINGS ) .", "entities": []}, {"text": "4.1 Baseline As mentioned in Section 2.2 , NCPTK measures grammatical consistency between the key and a distractor .", "entities": []}, {"text": "Our baseline uses NCPTK on Universal Dependencies ( UD ) trees ( Nivre et al . , 2020 ) in the following way .", "entities": [[5, 7, "DatasetName", "Universal Dependencies"], [8, 9, "DatasetName", "UD"]]}, {"text": "For each given MCQ , we exclude the sentence containing the key from the base text and then parse each remaining sentence siof the text , and the key using the UD parser for Swedish .", "entities": [[31, 32, "DatasetName", "UD"]]}, {"text": "LetTsiandTkdenote a dependency tree corresponding to siand the key respectively .", "entities": []}, {"text": "For each Tsi , we \ufb01nd all subtrees with the root having the same universal PoS - tag and the same universal features ( representing morphological properties of the token ) as the root of Tk .", "entities": []}, {"text": "If no subtrees are found , no distractors can be suggested for this MCQ .", "entities": []}, {"text": "Otherwise , we calculate NCPTK between each found subtree and Tk(both as GRCT , but without lexicals ) .", "entities": []}, {"text": "Then we take the textual representation of theKsubtrees with the highest NCPTK as the distractor suggestions .", "entities": []}, {"text": "390Input for left - to - right variant Target", "entities": []}, {"text": "[ CLS ] CTX [ SEP ]", "entities": []}, {"text": "[ MASK ] D11", "entities": []}, {"text": "[ CLS ] CTX [ SEP ]", "entities": []}, {"text": "D11 [ MASK ]", "entities": []}, {"text": "D12", "entities": []}, {"text": "[ CLS ] CTX [ SEP ] D11 D12", "entities": []}, {"text": "[ MASK ]", "entities": []}, {"text": "[ SEP ]", "entities": []}, {"text": "[ CLS ] CTX [ SEP ] D11 D12", "entities": []}, {"text": "[ SEP ]", "entities": []}, {"text": "[ MASK ] D21 [ CLS ] CTX [ SEP ] D11 D12", "entities": []}, {"text": "[ SEP ] D21 [ MASK ]", "entities": []}, {"text": "D22", "entities": []}, {"text": "[ CLS ] CTX [ SEP ] D11 D12", "entities": []}, {"text": "[ SEP ] D21 D22 [ MASK ]", "entities": []}, {"text": "D23 [ CLS ] CTX [ SEP ] D11 D12", "entities": []}, {"text": "[ SEP ] D21 D22 D23 [ MASK ]", "entities": []}, {"text": "[ SEP ] Input for u - PMLM variant Target(s )", "entities": [[7, 8, "MethodName", "PMLM"]]}, {"text": "[ CLS ] CTX [ SEP ]", "entities": []}, {"text": "D11 [ MASK ]", "entities": []}, {"text": "D12 [ CLS ] CTX [ SEP ]", "entities": []}, {"text": "[ MASK ] D12 D11", "entities": []}, {"text": "[ CLS ] CTX [ SEP ] D11 D12", "entities": []}, {"text": "[ SEP ] D21 [ MASK ]", "entities": []}, {"text": "[ MASK ] D22 , D23 [ CLS ] CTX [ SEP ]", "entities": []}, {"text": "D11", "entities": []}, {"text": "D12", "entities": []}, {"text": "[ SEP ] D21 [ MASK ]", "entities": []}, {"text": "D23 D22 [ CLS ] CTX [ SEP ] D11 D12 [ SEP ]", "entities": []}, {"text": "[ MASK ] D22 [ MASK ] D21 , D23 Table 2 : Example datapoints extracted from one MCQ if training the autoregressive left - to - right variant ( top table ) or u - PMLM variant ( bottom table ) .", "entities": [[36, 37, "MethodName", "PMLM"]]}, {"text": "D1andD2are distractors , assumed to have 2 and 3 words , respectively .", "entities": []}, {"text": "CTX represents the context , i.e. , the sequence T384", "entities": []}, {"text": "[ SEP ] Q", "entities": []}, {"text": "[ SEP ] A , where T384 is the \ufb01rst 384 tokens of the text , Qis a stem and Ais the key .", "entities": []}, {"text": "5 Experimental setup We have used Huggingface \u2019s Transformers library ( Wolf et al . , 2020 ) for implementing the DG model .", "entities": []}, {"text": "The training hardware setup included 16 Intel Xeon CPU E5 - 2620 v4 ( 2.10GHz ) , 64 GB of RAM and 1 NVIDIA GeForce RTX 2080", "entities": [[20, 21, "MethodName", "RAM"]]}, {"text": "Ti ( 11 GB VRAM ) .", "entities": []}, {"text": "For this setup , we have \ufb01xed the random seed to 42 , the number of training epochs to 6 , the batch size to 4 ( for both training and dev sets ) and MAXMASKINGS to 20 ( for u - PMLM variant only ) .", "entities": [[22, 24, "HyperparameterName", "batch size"], [42, 43, "MethodName", "PMLM"]]}, {"text": "With these settings , training took about 3.67h for the left - to - right and 3h for the u - PMLM variant .", "entities": [[21, 22, "MethodName", "PMLM"]]}, {"text": "UD trees for the baseline were obtained using Stanza package ( Qi et al . , 2020 ) and convolution partial tree kernels on the UD trees were calculated using UDon2 library ( Kalpakchi and Boye , 2020 ) .", "entities": [[0, 1, "DatasetName", "UD"], [19, 20, "MethodName", "convolution"], [25, 26, "DatasetName", "UD"]]}, {"text": "Baseline requires no training and running our implementation of the baseline takes about a minute on the development or test set .", "entities": []}, {"text": "6 Evaluation Following the analysis of Rodriguez ( 2005 ) , we generate three distractors per MCQ for each model .", "entities": []}, {"text": "Due to prohibitively high costs of human evaluation , we have divided the evaluation process into two stages .", "entities": []}, {"text": "The \ufb01rst stage is quantitative evaluation , which gives limited information about the model \u2019s quality , but is suf\ufb01cient for model selection .", "entities": [[21, 23, "TaskName", "model selection"]]}, {"text": "The second stage is human evaluation of the best model , selected during the \ufb01rst stage.6.1 Quantitative evaluation Automatic evaluation metrics , such as BLEU ( Papineni et al . , 2002 ) , ROUGE ( Lin , 2004 ) , METEOR ( Denkowski and Lavie , 2014 ) , CIDEr ( Vedantam et al . , 2015 ) , became popular in NLG in recent years .", "entities": [[24, 25, "MetricName", "BLEU"], [41, 42, "DatasetName", "METEOR"], [50, 51, "MetricName", "CIDEr"]]}, {"text": "Essentially , these metrics rely on comparing word overlap between a generated distractor and a reference one .", "entities": []}, {"text": "Such metrics can yield a low score even if the generated distractor is valid but just happens to be different from the reference one , or a high score even though the distractor is ungrammatical but happens to have a high word overlap with the reference one ( see the article by Callison - Burch et al .", "entities": []}, {"text": "( 2006 ) for a further discussion ) .", "entities": []}, {"text": "Furthermore , they do not take into account how well a generated distractor is aligned with the key grammatically or how challenging the whole group of generated distractors would be .", "entities": []}, {"text": "To account for the properties mentioned above , we have experimented with a number of quantitative metrics and propose the following set to be used ( the whole list is available in Appendix B ) .", "entities": []}, {"text": "In the following list MCQ% means \u201c Percentage of MCQ \u201d and DIS means \u201c generated distractor(s ) \u201d .", "entities": []}, {"text": "1.DisRecall .", "entities": []}, {"text": "Distractor recall .", "entities": []}, {"text": "2.AnyDisRefMatch .", "entities": []}, {"text": "MCQ% with at least 1 DIS matching a reference one .", "entities": []}, {"text": "3.AnyDisInText .", "entities": []}, {"text": "MCQ% with at least 1 DIS appearing in the base text .", "entities": []}, {"text": "3914.KeyInDis .", "entities": []}, {"text": "MCQ% with key being among DIS .", "entities": []}, {"text": "5.AnySameDis .", "entities": []}, {"text": "MCQ% with\u00152identical DIS .", "entities": []}, {"text": "6.AllSameDis .", "entities": []}, {"text": "MCQ% with all identical DIS .", "entities": []}, {"text": "7.AnyDisRep .", "entities": []}, {"text": "MCQ% with\u00151DIS containing repetitive words contiguously .", "entities": []}, {"text": "8.AnyDisEmpty .", "entities": []}, {"text": "MCQ% with\u00151DIS being an empty string3 .", "entities": []}, {"text": "9.AnyDisFromTrainDis .", "entities": []}, {"text": "MCQ% with at least 1 DIS matching with a distractor from training data , but not appearing in the base text .", "entities": []}, {"text": "10.MeanNCPTK , MedianNCPTK , ModeNCPTK .", "entities": []}, {"text": "Mean , median , and mode NCPTK for pairs of UD trees for DIS and keys ( all trees as GRCT , but ignoring nodes corresponding to lexicals ) .", "entities": [[10, 11, "DatasetName", "UD"]]}, {"text": "The \ufb01rst group consists of metrics 1 - 3 .", "entities": []}, {"text": "The \ufb01rst two metrics count exact matches between generated and reference distractors .", "entities": []}, {"text": "The rationale behind metric 3 is our assumption that distractors coming from the same text are more challenging .", "entities": []}, {"text": "The higher the values of all these metrics are , the better .", "entities": []}, {"text": "The second group contains metrics 4 - 8 , which give an idea of how challenging the whole group of distractors would be .", "entities": []}, {"text": "For instance , duplicate distractors or ones with word repetitions could be excluded by students using common sense .", "entities": []}, {"text": "The lower the metrics in this group are , the better .", "entities": []}, {"text": "The third group consists only of metric 9 , serving as an over\ufb01tting indicator .", "entities": []}, {"text": "The metric accounts for the distractors appearing as distractors in training data and high percentage indicates an over\ufb01tting possibility .", "entities": []}, {"text": "The lower the values , the better .", "entities": []}, {"text": "The \ufb01nal group ( item 10 ) measures how syntactically aligned generated distractors and the respective keys are .", "entities": []}, {"text": "We employ NCPTK to measure the similarity of syntactic structures between each distractor and the respective key .", "entities": []}, {"text": "Then we take mean , median and mode of the sequence of NCPTKs obtained in the previous step .", "entities": []}, {"text": "The higher the values of these metrics are , the better .", "entities": []}, {"text": "Based on these metrics , we performed a model selection on the development set and chose the models performing best on the most of these metrics .", "entities": [[8, 10, "TaskName", "model selection"]]}, {"text": "Left - to - right model generated distractors token by token until either a [ SEP ] token was generated or the length of the distractor was 20 tokens .", "entities": []}, {"text": "3After excluding the special tokens , e.g. , [ SEP]Metric Baseline u - PMLM DisRecall \" 1.44 % 15.31 % AnyDisRefMatch \" 2.94 % 26.47 % AnyDisInText \" 100.0 % 72.55 % KeyInDis # 0.00 % 4.9 % AnySameDis # 4.9 % 13.73 % AllSameDis # 0.00 % 1.96 % AnyDisRep # 0.00 % 2.94 % AnyDisEmpty # 11.76 % 0.00 % AnyDisFromTrainDis # NA 0.98 % MeanNCPTK \" 0.43 0.43 MedianNCPTK \" 0.28 0.28 ModeNCPTK\"1.0 1.0 ( 20.56 % ) ( 20.69 % ) Table 3 : Evaluation of DG models on the test set .", "entities": [[13, 14, "MethodName", "PMLM"]]}, {"text": "When using u - PMLM , shortest distractors were generated \ufb01rst . \" ( # ) means \u201c the higher ( lower ) , the better \u201d .", "entities": [[4, 5, "MethodName", "PMLM"]]}, {"text": "In contrast , u - PMLM needs the lengths of the distractors to be decided in beforehand , which we set to be the lengths of the two reference distractors and the length of the key4 .", "entities": [[5, 6, "MethodName", "PMLM"]]}, {"text": "Surprisingly , the order of distractors in terms of their length also matters for generation with u - PMLM , so we have tested three options : shortest \ufb01rst , longest \ufb01rst and random order .", "entities": [[18, 19, "MethodName", "PMLM"]]}, {"text": "According to the results of model selection on the development set ( presented in detail in Appendix C ) , u - PMLM models outperformed left - to - right models by a substantial margin .", "entities": [[5, 7, "TaskName", "model selection"], [22, 23, "MethodName", "PMLM"]]}, {"text": "The best u - PMLM model ( generating shortest distractors \ufb01rst ) and the baseline have been evaluated on the test set ( see Table 3 ) .", "entities": [[4, 5, "MethodName", "PMLM"]]}, {"text": "Interestingly , the similarity of syntactic structures between the key and distractors ( assessed by NCPTK ) is the same for both baseline ( that actually relies on NCPTK ) and u - PMLM .", "entities": [[33, 34, "MethodName", "PMLM"]]}, {"text": "At the same time , u - PMLM generates more distractors matching the reference ones compared to the baseline ( as seen from DisRecall andAnyDisRefMatch ) .", "entities": [[7, 8, "MethodName", "PMLM"]]}, {"text": "The baseline generates at least one empty string as a distractor 11.76 % of the time ( compared to no such cases for u - PMLM ) limiting possibilities of using the baseline in the real - life applications .", "entities": [[25, 26, "MethodName", "PMLM"]]}, {"text": "6.2 Human evaluation We have used distractors generated on the test set by the best u - PMLM model ( selected after quantitative evaluation in Section 6.1 ) to conduct human 4If reference distractors are not available , we propose to generate distractors with the length differing by at most two words compared to the length of the key .", "entities": [[17, 18, "MethodName", "PMLM"]]}, {"text": "392evaluation in 2 stages : from a perspective of a student and a teacher .", "entities": []}, {"text": "6.2.1 Student \u2019s perspective A desirable property of reading comprehension MCQs is that the students should be unable to answer them correctly without reading the actual text .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}, {"text": "To put more formally , the average number of correctly answered MCQs without reading the actual text ( denoted Ns ) should not differ signi\ufb01cantly from the average number of correctly answered MCQs when choosing the answer uniformly at random ( denoted Nr ) .", "entities": []}, {"text": "To test for this property , we have formulated the following two hypotheses.5 H0 : Ns = Nr . H1 : Ns6 = Nr .", "entities": []}, {"text": "ForNMCQs", "entities": []}, {"text": "with 4 options , Nr= 0:25N , which for our test set would be equal to Nr= 0:25\u0001102 = 25:5 .", "entities": []}, {"text": "The appropriate statistical test in this case is one - sample two - tailed t - test with the aim of not being able to reject H0 .", "entities": []}, {"text": "Given that the purpose is to show that the data supports H0 , we have set both the probability \u000b of type I errors and the probability \f of type II errors to be 0:05 .", "entities": []}, {"text": "Then we have used G*Power ( Faul et al . , 2009 ) to calculate the required sample size for \ufb01nding a medium effect size ( 0.5 ) and the given \u000b and \f , which turned out to be 54 subjects .", "entities": []}, {"text": "Following the calculations above , we have recruited 54 subjects on the Proli\ufb01c platform6 , and instructed them to choose the most plausible answer to a number of reading comprehension MCQs without providing the original texts .", "entities": [[28, 30, "TaskName", "reading comprehension"]]}, {"text": "The collected data did not violate any assumptions for a one - sample t - test ( see Appendix D.1 for more details ) .", "entities": []}, {"text": "On average , the subjects correctly answered a signi\ufb01cantly larger number of questions than Nr(Ns= 62:26 , SE= 1:09,t(53 )", "entities": []}, {"text": "= 33:51;p < 0:05;r= 0:98 ) .", "entities": []}, {"text": "To summarize , the chances of this sample to be collected are very low if H0were true .", "entities": []}, {"text": "However , evidently some of the generated distractors were actually plausible , given that Ns6= N.", "entities": []}, {"text": "To investigate the matter we have plotted the histogram of the frequency of choice of distractors by the subjects in Figure 2 .", "entities": []}, {"text": "As suggested by Haladyna and Downing ( 1993 ) , distractors that are chosen by less than 5 % of students should not be used , which in our case amounts to 39 % of the dis5Preregistration is available here 6https://www.proli\ufb01c.co/ 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Proportion of students0.000.050.100.150.200.250.300.350.40Proportion of distractors0.39", "entities": []}, {"text": "0.2 0.1 0.060.06 0.030.04", "entities": []}, {"text": "0.020.04 0.02 0.00.01 0.00.00.01 0.00.00.0Figure", "entities": []}, {"text": "2 : A histogram showing the frequency of choice of distractors in subjects \u2019 answers 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Entropy0.000.050.100.150.200.250.30Proportion of questions 0.05 0.020.030.04 0.030.1 0.02 0.010.050.08", "entities": []}, {"text": "0.070.190.32", "entities": []}, {"text": "Figure 3 : A histogram showing the entropy distribution per question tractors ( the leftmost bar in Figure 2 ) .", "entities": []}, {"text": "If we eliminate these low - frequency distractors ( LF - DIS ) , 68 MCQs ( 66.67 % ) will lose at least one distractor , 10 MCQs ( 9.8 % ) will lose all distractors and thus 34 MCQs ( 33.33 % ) will keep all 3 distractors .", "entities": []}, {"text": "A more relaxed question is how many MCQs had at least one plausible distractor , which can be estimated by calculating the entropy for each question as shown in Equation ( 2 ) , whereAis the key , Dis a distractor , Qis the stem , PQ(A)(PQ(D ) ) is the probability that the key ( any distractor ) is chosen forQby a subject .", "entities": []}, {"text": "H(Q )", "entities": []}, {"text": "= \u0000X O2fA;DgpQ(O ) log(pQ(O ) )", "entities": []}, {"text": "( 2 ) The distribution of entropies per question is shown in Figure 3 .", "entities": []}, {"text": "Assuming the natural logarithm , the highest theoretically possible value for H(Q)is 0:69 , ifpQ(A ) = pQ(D ) = 0:5 .", "entities": []}, {"text": "32 % of MCQs had an entropy larger than 0.65 , whereas 51 % had an entropy larger than 0.6 , which means that half of MCQs had at least one plausible distractor .", "entities": []}, {"text": "3936.2.2 Teacher \u2019s perspective Bearing in mind the \ufb01ndings of Section 6.2.1 , it is interesting to see which of the proposed distractors ( especially , among LF - DIS ) teachers would mark as acceptable .", "entities": []}, {"text": "Given the complexity of such evaluation , using the whole test set was infeasible .", "entities": []}, {"text": "To get a representative sample , we used entropy per question ( shown in Figure 3 ) .", "entities": []}, {"text": "All MCQs were divided into 5 equally sized buckets by entropy and 9 MCQs were sampled uniformly at random from each bucket , resulting in 45 MCQs in total .", "entities": []}, {"text": "We asked 5 teachers to evaluate each MCQ ( presented in a random order for each of them ) .", "entities": []}, {"text": "Each MCQ contained the base text , the stem , the key and the generated distractors .", "entities": []}, {"text": "The teachers were instructed to select those of generated distractors ( if any ) deemed suitable for testing reading comprehension .", "entities": [[18, 20, "TaskName", "reading comprehension"]]}, {"text": "Additionally , we asked to provide their reasons for each rejected distractor in a free - text input .", "entities": []}, {"text": "The inter - annotator agreement ( IAA ) was estimated using Goodman - Kruskal \u2019s", "entities": []}, {"text": "( Goodman and Kruskal , 1979 ) , speci\ufb01cally its multirater version", "entities": []}, {"text": "Nproposed by Kalpakchi and Boye ( 2021 ) .", "entities": []}, {"text": "On the scale proposed by Rosenthal ( 1996 ) , we have found a very large agreement (", "entities": []}, {"text": "N= 0:85 , see Appendix D.2.2 for more details on IAA calculations ) .", "entities": []}, {"text": "On average , 1.47 distractors per MCQ were accepted by a teacher .", "entities": []}, {"text": "Their reasons for rejections are distributed as shown in Figure 4 .", "entities": []}, {"text": "All teachers accepted at least one generated distractor for 39 MCQs ( 86.7 % ) , whereas the majority of teachers did so for 27 MCQs ( 60 % ) .", "entities": []}, {"text": "Interestingly , there are no MCQs in which all 5 teachers have either accepted or rejected all generated distractors .", "entities": []}, {"text": "However , the majority of teachers has accepted or rejected all distractors for 4 MCQs ( 8.9 % ) and 6 MCQs ( 13.3 % ) respectively .", "entities": []}, {"text": "Out of 45 MCQs , 31 ( 68.9 % ) had at least one LF - DIS , as de\ufb01ned in Section 6.2.1 .", "entities": []}, {"text": "For these 31 MCQs we report a distribution of accepted / rejected LF - DIS by the majority of teachers in Figure 5 .", "entities": []}, {"text": "Let us call the 15 MCQs with all LF - DIS accepted by the majority of teachers as mismatch MCQs ( lowest row in Figure 5 ) .", "entities": []}, {"text": "Interestingly , 12 of the 15 mismatch MCQs had at least one more distractor in addition to LF - DIS being accepted by the majority of teachers .", "entities": []}, {"text": "Furthermore , all mismatch MCQs had entropy higher than 0.3 .", "entities": []}, {"text": "This entails that almost a half of LF - DIS should notnecessarily be thrown away , since they were accepted by teachers , but Not wrong Unreasonable Grammatically wrong Unreas .", "entities": []}, {"text": "wrt question Identical Obviously wrong Dissimilar with key Too long Too easy Unreas .", "entities": []}, {"text": "wrt text Vague Too complex Wrong form Not in the text Too abstract Negation Both right and wrong051015202530Percentage of judgementsFigure 4 : A histogram showing the distribution of teachers \u2019 reasons behind rejecting distractors .", "entities": []}, {"text": "0 1 2 3 Number of accepted distractors0123Number of rejected distractors 10 54 5 12 22 Figure 5 : A bi - variate histogram showing the distribution of the 31 MCQs ( the numbers on the bars sum to 31 ) with at least 1 LF - DIS , with respect to their LF - DIS being accepted / rejected by the majority of teachers .", "entities": [[0, 1, "DatasetName", "0"]]}, {"text": "the MCQs either happened to have more plausible distractors or subjects might have had relevant background knowledge to answer the questions .", "entities": []}, {"text": "7 Related work We employed a systematic process to get a comprehensive overview of DG methods ( see Appendix E for more details ) .", "entities": []}, {"text": "Out of the resulting 28 articles ( see an overview in Table 4 ) , only 2 worked with a language other than English ( Chinese and Basque ) .", "entities": []}, {"text": "In this paper we work on reading comprehension MCQs , which makes only 12 papers , dealing with factual questions , relevant .", "entities": [[6, 8, "TaskName", "reading comprehension"]]}, {"text": "Two of these used rule - based approaches .", "entities": []}, {"text": "Majumder and Saha ( 2015 ) generated MCQs for cricket domain and used a number of hand - crafted rules based on gazeteers and Wikipedia entries to generate distractors .", "entities": []}, {"text": "Mitkov and Ha ( 2003 ) proposed to generate distractors for MCQs on electronic instructional documents using WordNet .", "entities": []}, {"text": "Six of these relied on extractive approaches .", "entities": []}, {"text": "394Liang et al .", "entities": []}, {"text": "( 2018 ) , Welbl et", "entities": []}, {"text": "al . ( 2017 ) , and Ha and Yaneva ( 2018 ) formulated choosing a distractor as a ranking problem from the given candidate set .", "entities": []}, {"text": "In the \ufb01rst two articles the candidate set constituted all distractors from the available MCQ dataset .", "entities": []}, {"text": "The authors then trained ML - based ranker(s ) for choosing the best distractors .", "entities": []}, {"text": "In the last one , the candidate set was created using content engineers .", "entities": []}, {"text": "Distractors with a high similarity of their concept embeddings ( summed for multiple words ) and appearing in the same document as the key are ranked higher .", "entities": []}, {"text": "Stasaski and Hearst ( 2017 ) and Araki et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2016 ) worked in the domain of biology .", "entities": []}, {"text": "The former used an ontology and the latter employed event graphs containing information about coreferences to generate distractors .", "entities": [[4, 5, "MethodName", "ontology"]]}, {"text": "Karamanis et al .", "entities": []}, {"text": "( 2006 ) used thesaurus and tf - idf to identify key concepts in the given text and then select as distractors those having the same semantic type as the key .", "entities": []}, {"text": "The remaining four employed neural methods and are most relevant among the surveyed .", "entities": []}, {"text": "Qiu et al .", "entities": []}, {"text": "( 2020 ) trained a sequence - to - sequence ( seq2seq ) model with a number of attention layers .", "entities": [[11, 12, "MethodName", "seq2seq"], [18, 20, "HyperparameterName", "attention layers"]]}, {"text": "Zhou", "entities": []}, {"text": "et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2020 ) also employed a seq2seq model , but with a hierarchical attention to capture the interaction between a text and a question , as well as semantic similarity loss .", "entities": [[6, 7, "MethodName", "seq2seq"], [28, 30, "TaskName", "semantic similarity"], [30, 31, "MetricName", "loss"]]}, {"text": "Both articles used a beam search combined with \ufb01ltering based on Jaccard coef\ufb01cient at generation time .", "entities": []}, {"text": "Offerijns et al .", "entities": []}, {"text": "( 2020 ) trained a GPT-2 model to generate 3 distractors for a given MCQ , and used BERT - based question answering model for quantitative evaluation ( along with human evaluation ) .", "entities": [[5, 6, "MethodName", "GPT-2"], [18, 19, "MethodName", "BERT"], [21, 23, "TaskName", "question answering"]]}, {"text": "Finally , Chung et al . ( 2020 ) proposed a BERTbased method for English with answer - negative regularization , penalizing distractors for containing Problem / method property # \u0004Extractive 14 \u0004Generative , rule - based 7 \u0004Generative , neural 7   Only automatic evaluation 5   Only human evaluation 19   Automatic and human evaluattion 4 NCloze - style , single - word answers 14 NCloze - style , continue the sentence 2 NFactual questions 12 Table 4 : 28 related works broken down by method ( \u0004 ) , type of evaluation (   ) and types of questions for which distractors have been generated ( N)the same words as the key , and training a sequential and a parallel MLM model simultaneously .", "entities": [[123, 124, "DatasetName", "MLM"]]}, {"text": "At generation time , they generate one distractor , and then create a distractor set of the prede\ufb01ned size based on sampling from the probability distribution returned by BERT for each token of the distractor .", "entities": [[28, 29, "MethodName", "BERT"]]}, {"text": "Then they rank every triple of distractors based on the entropy of a separately trained QA model .", "entities": []}, {"text": "Our method also relies on BERT , but has a number of differences beyond being applied to Swedish .", "entities": [[5, 6, "MethodName", "BERT"]]}, {"text": "Firstly , we did not include answer - negative regularization , since it is not always a good strategy .", "entities": []}, {"text": "For instance , given the stem \u201c When should you pay a fee if you apply for a visa ? \u201d and a key \u201c before you have submitted the application \u201d , the best distractor would be \u201c after you have submitted the application \u201d , which shares most of the words with the key .", "entities": []}, {"text": "Secondly , we generate distractors in arbitrary word order compared to left - to - right generation in ( Chung et al . , 2020 ) .", "entities": []}, {"text": "Thirdly , at generation time , we use previously generated distractors as input for generating next ones , and always take tokens with a maximum probability .", "entities": []}, {"text": "This lowers the risk of generating ungrammatical distractors .", "entities": []}, {"text": "Finally , our training set is 100 times smaller compared to the training set used by Chung et al .", "entities": []}, {"text": "( 2020 ) .", "entities": []}, {"text": "8 Conclusion We have collected SweQUAD - MC , the \ufb01rst dataset of Swedish MCQs , and showed the possibility of training usable BERT - based DG models , despite the small scale of the dataset .", "entities": [[23, 24, "MethodName", "BERT"]]}, {"text": "We have showed that a u - PMLM variant of the BERT - based DG model performs best on the dataset , and proposed a novel methodology of evaluating the plausibility of generated distractors .", "entities": [[7, 8, "MethodName", "PMLM"], [11, 12, "MethodName", "BERT"]]}, {"text": "Around half of the generated distractors were found acceptable by the majority of teachers , and more than 50 % of MCQs had at least one plausible generated distractor , judging by the entropy of students \u2019 responses .", "entities": []}, {"text": "Bearing in mind that the aim of the proposed method is to support ( not replace ) teachers , we deem that our method works well for MCQs in Swedish ( and potentially in other languages with a pretrained BERT and a dataset of a similar scale ) .", "entities": [[39, 40, "MethodName", "BERT"]]}, {"text": "Furthermore , we have presented a baseline applicable to any language with a UD treebank ( currently about 100 languages ) .", "entities": [[13, 14, "DatasetName", "UD"]]}, {"text": "Although its performance is nowhere near the u - PMLM variant , we believe that it can serve as a good point of comparison to emerging neural methods for other languages .", "entities": [[9, 10, "MethodName", "PMLM"]]}, {"text": "395Acknowledgments This work was supported by Vinnova ( Sweden \u2019s Innovation Agency ) within project 2019 - 02997 .", "entities": []}, {"text": "We would like to thank the anonymous reviewers for their comments , as well as Gabriel Skantze and Bram Willemsen for their helpful feedback prior to the submission of the paper .", "entities": []}, {"text": "References Jacopo Amidei , Paul Piwek , and Alistair Willis .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Evaluation methodologies in automatic question generation 2013 - 2018 .", "entities": [[4, 6, "TaskName", "question generation"]]}, {"text": "In Proceedings of the 11th International Conference on Natural Language Generation , pages 307\u2013317 , Tilburg University , The Netherlands . Association for Computational Linguistics .", "entities": []}, {"text": "Jun Araki , Dheeraj Rajagopal , Sreecharan Sankaranarayanan , Susan Holm , Yukari Yamakawa , and Teruko Mitamura .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Generating questions and multiple - choice answers using semantic analysis of texts .", "entities": []}, {"text": "In Proceedings of COLING 2016 , the 26th International Conference on Computational Linguistics : Technical Papers , pages 1125\u20131136 , Osaka , Japan .", "entities": []}, {"text": "The COLING 2016 Organizing Committee .", "entities": []}, {"text": "Jimmy Lei Ba , Jamie Ryan Kiros , and Geoffrey E Hinton .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Layer normalization .", "entities": [[0, 2, "MethodName", "Layer normalization"]]}, {"text": "arXiv preprint arXiv:1607.06450 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Chris Callison - Burch , Miles Osborne , and Philipp Koehn .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Re - evaluating the role of Bleu in machine translation research .", "entities": [[6, 7, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In 11th Conference of the European Chapter of the Association for Computational Linguistics , Trento , Italy .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ho - Lam Chung , Ying - Hong Chan , and Yao - Chung Fan .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "A BERT - based distractor generation scheme with multi - tasking and negative answer training strategies .", "entities": [[1, 2, "MethodName", "BERT"], [4, 6, "TaskName", "distractor generation"]]}, {"text": "In Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 4390 \u2013 4400 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Danilo Croce , Alessandro Moschitti , and Roberto Basili .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Structured lexical similarity via convolution kernels on dependency trees .", "entities": [[4, 5, "MethodName", "convolution"]]}, {"text": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , pages 1034\u20131046 , Edinburgh , Scotland , UK . Association for Computational Linguistics .", "entities": []}, {"text": "Michael Denkowski and Alon Lavie .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Meteor universal : Language speci\ufb01c translation evaluation for any target language .", "entities": [[0, 1, "DatasetName", "Meteor"]]}, {"text": "In Proceedings of the Ninth Workshop on Statistical Machine Translation , pages 376\u2013380 , Baltimore , Maryland , USA . Association for Computational Linguistics .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : Pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Franz Faul , Edgar Erdfelder , Axel Buchner , and AlbertGeorg Lang . 2009 .", "entities": []}, {"text": "Statistical power analyses using g * power 3.1 : Tests for correlation and regression analyses .", "entities": []}, {"text": "Behavior research methods , 41(4):1149 \u2013 1160 .", "entities": []}, {"text": "Leo A Goodman and William H Kruskal .", "entities": []}, {"text": "1979 .", "entities": []}, {"text": "Measures of association for cross classi\ufb01cations .", "entities": []}, {"text": "Measures of association for cross classi\ufb01cations , pages 2\u201334 .", "entities": []}, {"text": "Le An Ha and Victoria Yaneva .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Automatic distractor suggestion for multiple - choice tests using concept embeddings and information retrieval .", "entities": [[12, 14, "TaskName", "information retrieval"]]}, {"text": "In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 389\u2013398 , New Orleans , Louisiana .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thomas M Haladyna and Steven M Downing .", "entities": []}, {"text": "1993 .", "entities": []}, {"text": "How many options is enough for a multiple - choice test item ?", "entities": []}, {"text": "Educational and psychological measurement , 53(4):999\u20131010 .", "entities": []}, {"text": "Dmytro Kalpakchi and Johan Boye . 2020 .", "entities": []}, {"text": "UDon2 : a library for manipulating Universal Dependencies trees .", "entities": [[6, 8, "DatasetName", "Universal Dependencies"]]}, {"text": "In Proceedings of the Fourth Workshop on Universal Dependencies ( UDW 2020 ) , pages 120 \u2013 125 , Barcelona , Spain ( Online ) .", "entities": [[7, 9, "DatasetName", "Universal Dependencies"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Dmytro Kalpakchi and Johan Boye . 2021 .", "entities": []}, {"text": "Quinductor : a multilingual data - driven method for generating reading - comprehension questions using universal dependencies .", "entities": [[15, 17, "DatasetName", "universal dependencies"]]}, {"text": "arXiv preprint arXiv:2103.10121 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Nikiforos Karamanis , Le An Ha , and Ruslan Mitkov .", "entities": [[8, 9, "DatasetName", "Ruslan"]]}, {"text": "2006 .", "entities": []}, {"text": "Generating multiple - choice test items from medical text : A pilot study .", "entities": []}, {"text": "In Proceedings of the Fourth International Natural Language Generation Conference , pages 111\u2013113 , Sydney , Australia . Association for Computational Linguistics .", "entities": []}, {"text": "Chen Liang , Xiao Yang , Neisarg Dave , Drew Wham , Bart Pursel , and C. Lee Giles . 2018 .", "entities": []}, {"text": "Distractor generation for multiple choice questions using learning to rank .", "entities": [[0, 2, "TaskName", "Distractor generation"]]}, {"text": "In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 284\u2013290 , New Orleans , Louisiana . Association for Computational Linguistics .", "entities": []}, {"text": "Yi Liao , Xin Jiang , and Qun Liu .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Probabilistically masked language model capable of autoregressive generation in arbitrary word order .", "entities": [[0, 4, "MethodName", "Probabilistically masked language model"]]}, {"text": "In Proceedings of the 58th Annual Meeting of the Association", "entities": []}, {"text": "396for Computational Linguistics , pages 263\u2013274 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Chin - Yew Lin .", "entities": []}, {"text": "2004 .", "entities": []}, {"text": "ROUGE :", "entities": []}, {"text": "A package for automatic evaluation of summaries .", "entities": []}, {"text": "In Text Summarization Branches Out , pages 74\u201381 , Barcelona , Spain . Association for Computational Linguistics .", "entities": [[1, 3, "TaskName", "Text Summarization"]]}, {"text": "Mukta Majumder and Sujan Kumar Saha .", "entities": [[4, 5, "DatasetName", "Kumar"]]}, {"text": "2015 .", "entities": []}, {"text": "A system for generating multiple choice questions : With a novel approach for sentence selection .", "entities": []}, {"text": "In Proceedings of the 2nd Workshop on Natural Language Processing Techniques for Educational Applications , pages 64\u201372 , Beijing , China .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Martin Malmsten , Love B \u00a8orjeson , and Chris Haffenden .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Playing with words at the national library of sweden \u2013 making a swedish bert .", "entities": []}, {"text": "arXiv preprint arXiv:2007.01658 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Ruslan Mitkov and Le An Ha . 2003 .", "entities": [[0, 1, "DatasetName", "Ruslan"]]}, {"text": "Computer - aided generation of multiple - choice tests .", "entities": []}, {"text": "In Proceedings of the HLT - NAACL 03", "entities": []}, {"text": "Workshop on Building Educational Applications Using Natural Language Processing , pages 17\u201322 .", "entities": []}, {"text": "Alessandro Moschitti .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "Ef\ufb01cient convolution kernels for dependency and constituent syntactic trees .", "entities": [[1, 2, "MethodName", "convolution"]]}, {"text": "In European Conference on Machine Learning , pages 318\u2013329 .", "entities": []}, {"text": "Springer .", "entities": []}, {"text": "Joakim Nivre , Marie - Catherine de Marneffe , Filip Ginter , Jan Haji \u02c7c , Christopher D. Manning , Sampo Pyysalo , Sebastian Schuster , Francis Tyers , and Daniel Zeman .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Universal Dependencies v2 : An evergrowing multilingual treebank collection .", "entities": [[0, 2, "DatasetName", "Universal Dependencies"]]}, {"text": "InProceedings of the 12th Language Resources and Evaluation Conference , pages 4034\u20134043 , Marseille , France .", "entities": []}, {"text": "European Language Resources Association .", "entities": []}, {"text": "Jeroen Offerijns , Suzan Verberne , and Tessa Verhoef .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Better distractions : Transformer - based distractor generation and multiple choice question \ufb01ltering .", "entities": [[3, 4, "MethodName", "Transformer"], [6, 8, "TaskName", "distractor generation"]]}, {"text": "arXiv preprint arXiv:2010.09598 .", "entities": [[0, 1, "DatasetName", "arXiv"]]}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311\u2013318 , Philadelphia , Pennsylvania , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Peng Qi , Yuhao Zhang , Yuhui Zhang , Jason Bolton , and Christopher D. Manning .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Stanza : A python natural language processing toolkit for many human languages .", "entities": []}, {"text": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 101 \u2013 108 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Zhaopeng Qiu , Xian Wu , and Wei Fan .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Automatic distractor generation for multiple choice questions in standard tests .", "entities": [[1, 3, "TaskName", "distractor generation"]]}, {"text": "In Proceedings of the 28th International Conference on Computational Linguistics , pages 2096\u20132106 , Barcelona , Spain ( Online ) .", "entities": []}, {"text": "International Committee on Computational Linguistics .", "entities": []}, {"text": "Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 .", "entities": []}, {"text": "Improving language understanding by generative pre - training . preprint .", "entities": []}, {"text": "Michael C Rodriguez . 2005 .", "entities": []}, {"text": "Three options are optimal for multiple - choice items : A meta - analysis of 80 years of research .", "entities": []}, {"text": "Educational measurement : issues and practice , 24(2):3\u201313 .", "entities": []}, {"text": "James A Rosenthal .", "entities": []}, {"text": "1996 .", "entities": []}, {"text": "Qualitative descriptors of strength of association and effect size .", "entities": []}, {"text": "Journal of social service Research , 21(4):37\u201359 .", "entities": []}, {"text": "Katherine Stasaski and Marti A. Hearst . 2017 .", "entities": []}, {"text": "Multiple choice question generation utilizing an ontology .", "entities": [[2, 4, "TaskName", "question generation"], [6, 7, "MethodName", "ontology"]]}, {"text": "InProceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications , pages 303\u2013312 , Copenhagen , Denmark .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , \u0141ukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Proceedings of the 31st International Conference on Neural Information Processing Systems , pages 6000\u20136010 .", "entities": []}, {"text": "Ramakrishna Vedantam , C Lawrence Zitnick , and Devi Parikh . 2015 .", "entities": []}, {"text": "Cider : Consensus - based image description evaluation .", "entities": []}, {"text": "In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4566\u20134575 .", "entities": []}, {"text": "Alex Wang and Kyunghyun Cho . 2019 .", "entities": []}, {"text": "BERT has a mouth , and it must speak : BERT as a Markov random \ufb01eld language model .", "entities": [[0, 1, "MethodName", "BERT"], [10, 11, "MethodName", "BERT"]]}, {"text": "In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation , pages 30\u201336 , Minneapolis , Minnesota .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Johannes Welbl , Nelson F. Liu , and Matt Gardner . 2017 .", "entities": []}, {"text": "Crowdsourcing multiple choice science questions .", "entities": []}, {"text": "InProceedings of the 3rd Workshop on Noisy Usergenerated Text , pages 94\u2013106 , Copenhagen , Denmark .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Remi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 .", "entities": []}, {"text": "Transformers : State - of - the - art natural language processing .", "entities": []}, {"text": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing :", "entities": []}, {"text": "397System Demonstrations , pages 38\u201345 , Online .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xiaorui Zhou , Senlin Luo , and Yunfang Wu .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Coattention hierarchical network : Generating coherent long distractors for reading comprehension .", "entities": [[9, 11, "TaskName", "reading comprehension"]]}, {"text": "In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 34 , pages 9725\u20139732 .", "entities": []}, {"text": "A SweQUAD - MC data collection details We have used publicly available texts from the websites of Swedish government agencies .", "entities": []}, {"text": "The exact list of URLs is provided in the GitHub repository associated with the paper .", "entities": []}, {"text": "The exact instructions given to students recruited to collect SweQUADMC dataset ( and their translation to English ) are presented in Figure 6 .", "entities": []}, {"text": "In addition to the given instructions , the students were also given the opportunity to slightly reformulate the distractors found in the text in order to align the syntactic structure with that of the key .", "entities": []}, {"text": "B Quantitative metrics In addition to the metrics 1\u201310 presented in Section 6.1 , we have also looked at the following ones ( MCQ% means \u201c Percentage of MCQ \u201d and DIS means \u201c generated distractor(s ) \u201d ) 11.MCQ% with at least 1 DIS being capitalized differently from the key 12.MCQ% with at least 1 DIS being a distractor from training data .", "entities": []}, {"text": "13.MCQ% with at least 1 DIS is in any base text from training data .", "entities": []}, {"text": "14.MCQ% with at least 1 DIS appearing in at least 1 base text from training data , but not in their own base text .", "entities": []}, {"text": "15.MCQ% with all distractors appearing in the base text .", "entities": []}, {"text": "16.MCQ% with all distractors appearing in at least 1 base text from training data .", "entities": []}, {"text": "17.MCQ% with all DIS being distractors from training data .", "entities": []}, {"text": "The rationale behind metric 11 was that capitalized answers are named entities and thus one would like distractors also to be named entities .", "entities": []}, {"text": "However , it does not always hold .", "entities": []}, {"text": "For instance , consider the stem \u201c Who gets an e - mail with a con\ufb01rmationof a successful submission of the application for the work permit ? \u201d and the key \u201c you and your employer \u201d .", "entities": []}, {"text": "A distractor \u201c Migration Agency \u201d would suit the question perfectly , although capitalization is clearly different .", "entities": []}, {"text": "Metrics 12 - 17 were candidates to become over\ufb01tting indicators .", "entities": []}, {"text": "However , metric 2 was excluded , since AnyDisFromTrainDis is more informative , given phrases used as distractors in training data can be repeated in other texts .", "entities": []}, {"text": "Metrics 13 - 14 were excluded , since it \u2019s unclear whether the higher or lower values are better .", "entities": []}, {"text": "For instance , if a text from the training data and the given text are thematically similar , would copying a distractor from training data be considered over\ufb01tting ?", "entities": []}, {"text": "Metrics 15 - 17 were rejected as too strict , leaving the possibility of actually missing over\ufb01tting if only 2 of 3 distractors would meet the criteria .", "entities": []}, {"text": "C Model selection We have trained both left - to - right and u - PMLM variants for 6 epochs ( \ufb01xing a random seed for uPMLM masking procedure to 42 ) .", "entities": [[1, 3, "TaskName", "Model selection"], [15, 16, "MethodName", "PMLM"]]}, {"text": "The quantitative performance metrics on the development set for the top-3 models for each variant are presented in Table 5 .", "entities": []}, {"text": "The best u - PMLM model ( i-14000 ) outperformed the best left - to - right model ( i-18000 ) on most of the quantitative metrics .", "entities": [[4, 5, "MethodName", "PMLM"]]}, {"text": "The next experiment concerned the order in which distractors are generated , which we tested only for the best u - PMLM model .", "entities": [[21, 22, "MethodName", "PMLM"]]}, {"text": "We tried generating shortest distractors \ufb01rst ( SF ) , longest \ufb01rst ( LF ) or in a random order with a \ufb01xed seed of 42 ( RND ) .", "entities": []}, {"text": "The results of the experiment are presented in Table 6 .", "entities": []}, {"text": "Evidently , models with SF - generation consistently outperform ones with LF - generation .", "entities": []}, {"text": "SF - generation also performs on - par or better than RND - generation .", "entities": []}, {"text": "However , \ufb01xing a seed is not a generalizable solution , which is why we opted for SF - generation .", "entities": []}, {"text": "D Human evaluation details D.1 Student \u2019s perspective Evaluation from the student \u2019s perspective has been conducted on the Proli\ufb01c platform7 .", "entities": []}, {"text": "We used Proli\ufb01c \u2019s pre - screening feature and required each subject to have Swedish as the \ufb01rst language and hold at least a high school diploma ( A - levels ) .", "entities": []}, {"text": "Descriptive statistics about the recruited sample of subjects 7https://www.proli\ufb01c.co/", "entities": []}, {"text": "398Imagine that you are a teacher checking reading comprehension skills of your students .", "entities": [[7, 9, "TaskName", "reading comprehension"]]}, {"text": "Given a text , your task is to create one or more multiple choice questions based on the text , i.e. : 1 . formulate a question with the correct answer in the text ; 2 . mark the correct answer in the text ; 3 . mark some wrong , but plausible options in the text .", "entities": []}, {"text": "When you have written your questions , marked the correct answer ( CA ) and the wrong alternatives in the text , click on \u201c Submit \u201d .", "entities": []}, {"text": "When you formulate the question , think about the following aspects .", "entities": []}, {"text": "\u2022The question must be independent , i.e. , one should not require additional information ( on top of the given text ) to be able to answer the question .", "entities": []}, {"text": "\u2022", "entities": []}, {"text": "The question should be unambiguous and have only one possible interpretation .", "entities": []}, {"text": "\u2022One should not be able to answer your question without reading the text , which is why even wrong alternatives should be plausible .", "entities": []}, {"text": "\u2022Wrong options must be in the same grammatical form as the CA .", "entities": []}, {"text": "For instance , if the CA begins with a verb in Past Simple , all wrong options must begin with a verb in Past Simple .", "entities": []}, {"text": "Find as many questions as you can ( + the correct answer and wrong alternatives ) on each text and then get a new text when you ca n\u2019t \ufb01nd more .", "entities": []}, {"text": "Figure 6 : An English translation of the original instructions for SweQUAD - MC data collection ( the original instructions in Swedish can be found in the GitHub repository )", "entities": []}, {"text": "Metricleft - to - right u -", "entities": []}, {"text": "PMLM i-10000 i-14000 i-18000 i-10000 i-14000 i-16000 e-3.02 e-4.23 e-5.43 e-3.59 e-5.02 e-5.74 M1 : DisRecall \" 9.77 % 14.29 % 12.41 % 17.67 % 21.43 % 18.80 % M2 : AnyDisRefMatch \" 18.25 % 26.19 % 21.43 % 30.95 % 37.30 % 31.75 % M3 : AnyDisInText \" 64.29 % 69.84 % 73.81 % 68.25 % 72.22 % 73.81 % M4 : KeyInDis # 0.79 % 1.59 % 3.17 % 2.38 % 5.56 % 5.56 % M5 : AnySameDis # 34.13 % 27.78 % 19.84 % 9.52 % 10.32 % 11.90 % M6 : AllSameDis # 3.17 % 1.59 % 0.79 % 1.59 % 0.79 % 0.79 % M7 : AnyDisRep # 0.00 % 0.00 % 0.00 % 0.00 % 1.59 % 1.59 % M8 : AnyDisEmpty # 0.00 % 0.00 % 0.00 % 0.00 % 0.00 % 0.00 % M9 : AnyDisFromTrainDis # 5.56 % 5.56 % 6.35 % 5.56 % 2.38 % 2.38 % M10 : MeanNCPTK \" 0.33 0.38 0.39 0.41 0.41 0.41 M11 : MedianNCPTK \"", "entities": [[0, 1, "MethodName", "PMLM"], [61, 62, "DatasetName", "M4"]]}, {"text": "0.18 0.19 0.21 0.27 0.26 0.27 M12 : ModeNCPTK \" 1.0 1.0 1.0 1.0 1.0 1.0 ( 13.3 % ) ( 18.8 % ) ( 17.6 % ) ( 18.1 % ) ( 20.3 % ) ( 19.6 % ) Table 5 : TOP-3 models for left - to - right and u - PMLM variants after model selection on the dev set .", "entities": [[54, 55, "MethodName", "PMLM"], [57, 59, "TaskName", "model selection"]]}, {"text": "i - XXXXX shows a number of iterations since training start , e - X.XX shows a number of epochs corresponding to i - XXXXX .", "entities": [[5, 8, "HyperparameterName", "number of iterations"], [17, 20, "HyperparameterName", "number of epochs"]]}, {"text": "Floating point epochs are due to checkpoints being saved every 2000 iterations .", "entities": []}, {"text": "399Metrici-10000 , e-3.59 i-14000 , e-5.02 i-16000 , e-5.74 SF LF RND SF LF RND SF LF RND M1\"15.8 % 13.9 % 15.8 % 20.7 % 14.7 % 19.9 % 19.9 % 15.0 % 17.7 % M2\"25.4 % 25.4 % 29.4 % 36.5 % 27.8 % 34.1 % 34.1 % 27.0 % 30.1 % M3\"64.3 % 63.5 % 65.9 % 73.0 % 66.7 % 69.8 % 72.2 % 66.7 % 70.6 % M4 # 2.4 % 2.4 % 3.2 % 4.0 % 4.8 % 5.6 % 4.8 % 5.6 % 4.8 % M5 # 7.9 % 11.1 % 7.9 % 10.3 % 9.5 % 10.3 % 10.3 % 8.7 % 10.3 % M6 # 1.6 % 1.6 % 1.6 % 0.8 % 0.8 % 0.8 % 0.8 % 0.8 % 0.8 % M7 # 0.0 % 1.6 % 0.0 % 0.0 % 1.6 % 1.6 % 0.8 % 0.8 % 3.2 % M8 # 0.0 % 0.0 % 0.0 % 0.0 % 0.0 % 0.0 % 0.0 % 0.0 % 0.0 % M9 # 5.6 % 4.8 % 6.3 % 4.8 % 5.6 % 4.0 % 4.0 % 4.0 % 3.2 % M10 \" 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 M11 \" 0.24 0.22 0.25 0.26 0.21 0.22 0.29 0.22 0.22 M12\"1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 ( 18 % ) ( 17 % ) ( 19 % ) ( 20 % ) ( 18 % ) ( 20 % ) ( 19 % ) ( 18 % ) ( 19 % ) Table 6 : Results of model selection by the generation order of distractors for the TOP-3 u - PMLM models .", "entities": [[72, 73, "DatasetName", "M4"], [264, 266, "TaskName", "model selection"], [277, 278, "MethodName", "PMLM"]]}, {"text": "10 20 30 40 500510time_taken_minutes 20 30 40 50010age 0 100 200 300 400010num_approvals 0", "entities": [[9, 10, "DatasetName", "0"], [14, 15, "DatasetName", "0"]]}, {"text": "2 4 6020num_rejections 98.0 98.5 99.0 99.5 100.002040prolific_score 0 25 50 75010num_correct Figure 7 : Descriptive statistics of the sample of subjects on Proli\ufb01c Thank you for participating in our study !", "entities": [[8, 9, "DatasetName", "0"]]}, {"text": "You will be presented with a number of multiple choice questions .", "entities": []}, {"text": "Your task is to answer as many of these questions correctly as possible .", "entities": []}, {"text": "If you do n\u2019t know which alternative is correct , choose the one that seems the most plausible .", "entities": []}, {"text": "You are allowed to use ONLY your own prior knowledge and common sense .", "entities": []}, {"text": "Please , do NOT consult any other external sources of information .", "entities": []}, {"text": "Figure 8 : An English translation of the original instructions given to subjects on the Proli\ufb01c platform ( the original instructions in Swedish can be found in the GitHub repository )", "entities": []}, {"text": "400is presented in Figure 7 .", "entities": []}, {"text": "The exact guidelines given to the subjects ( and their translation to English ) are presented in Figure 8 .", "entities": []}, {"text": "MCQs were presented in a random order , but the order of options for each MCQs was the same for each subject .", "entities": []}, {"text": "D.1.1 Check of the t - test assumptions We used one sample t - test for conducting our analysis and thus the following assumptions were checked for .", "entities": []}, {"text": "1.The variable under study should be either an interval or ratio variable .", "entities": []}, {"text": "Our variable , the number of correctly answered MCQs , is clearly on a ratio scale .", "entities": []}, {"text": "2.The observations in the sample should be independent .", "entities": []}, {"text": "Subjects have performed the task independently of each other through a Proli\ufb01c platform , hence the observations are independent .", "entities": []}, {"text": "3.The variable under study should be approximately normally distributed .", "entities": []}, {"text": "The distribution of the number of correctly answered MCQs is presented in Figure 7 ( the plot in the last row and the last column with the title \u201c num correct \u201d ) .", "entities": []}, {"text": "The distribution is indeed approximately normal .", "entities": []}, {"text": "4.The variable under study should have no extreme outliers .", "entities": []}, {"text": "Outliers are typically de\ufb01ned in terms of the interquartile range ( IQR ) , which equals to Q3 - Q1 .", "entities": [[11, 12, "DatasetName", "IQR"]]}, {"text": "The datapoints outside 1.5IQR are deemed mild outliers , whereas those outside 3IQR are considered extreme outliers .", "entities": []}, {"text": "Boxplots for our data with whiskers within both 1.5IQR and 3IQR are presented in Figure 9 .", "entities": []}, {"text": "Two datapoints can be considered mild outliers , but no extreme outliers are present , which means this assumption for the one sample t - test is not violated .", "entities": []}, {"text": "D.2 Teacher \u2019s perspective D.2.1 Instructions The exact guidelines given to the teachers and their translation to English , are presented in Figure 10 .", "entities": []}, {"text": "D.2.2 Inter - annotator agreement To evaluate the inter - annotator agreement ( IAA ) between the teachers , we have reformulated the problem into a ranking problem , where all accepteddistractors were given the rank of 1 and those rejected - the rank of 2 . IAA was then estimated using Goodman - Kruskal \u2019s", "entities": []}, {"text": "( Goodman and Kruskal , 1979 ) , speci\ufb01cally its multirater version", "entities": []}, {"text": "Nproposed by Kalpakchi and Boye ( 2021 ) .", "entities": []}, {"text": "The total number of concordant and discordant pairs were summed for each pair of teachers for each MCQ .", "entities": []}, {"text": "The resulting", "entities": []}, {"text": "Nequals to 0.85 , indicating a very large agreement on the scale proposed by Rosenthal ( 1996 ) .", "entities": []}, {"text": "E Details on surveying related work To get a comprehensive overview of methods for generating distractors for MCQs , we employed a two - step process .", "entities": []}, {"text": "The \ufb01rst step was to issue queries \u201c distractor generation \u201d and \u201c multiple choice question generation \u201d to ACL Anthology and Google Scholar .", "entities": [[8, 10, "TaskName", "distractor generation"], [15, 17, "TaskName", "question generation"], [22, 23, "DatasetName", "Google"]]}, {"text": "The result was 20 articles from ACL Anthology and 4 additional ones from Google Scholar .", "entities": [[13, 14, "DatasetName", "Google"]]}, {"text": "The second step was to select relevant references from the \u201c Related work \u201d sections of these articles .", "entities": []}, {"text": "This resulted into 15 additional articles .", "entities": []}, {"text": "Out of found 39 articles , 11 were \ufb01ltered out ( 8 focused only on generating questions , 1 relied mostly on expert knowledge , 1 on the auxiliary relation extraction task and 1 was a demo paper ) , leaving 28 articles in total .", "entities": [[29, 31, "TaskName", "relation extraction"]]}, {"text": "Only 2 of these 28 papers worked with a language other than English ( Chinese and Basque ) .", "entities": []}, {"text": "F Generated samples A number of generated distractors along with the respective stems and keys from the dataset are presented in Figures 11 , 12 , 13 , 14 , 15 .", "entities": []}, {"text": "The questions are sampled based on the entropy of student \u2019s an1.5IQR 3IQR404550556065707580 Figure 9 :", "entities": []}, {"text": "Boxplots for the number of correctly answered questions", "entities": []}, {"text": "401Thank you for participating in our study !", "entities": []}, {"text": "You will be presented with a number of tests .", "entities": []}, {"text": "Each test contains a text , a reading comprehension question based on the text , the explicitly marked correct answer to this question and a number of suggestions for wrong , but plausible alternatives ( distractors ) .", "entities": [[7, 9, "TaskName", "reading comprehension"]]}, {"text": "Suppose you would like to use the given question for testing reading comprehension of the given text .", "entities": [[11, 13, "TaskName", "reading comprehension"]]}, {"text": "Your task is to judge which of the suggested distractors ( if any ) you would \ufb01t the purpose .", "entities": []}, {"text": "Select suitable distractors by simply ticking the respective checkboxes .", "entities": []}, {"text": "For the other distractors ( that you did n\u2019t select ) , please brie\ufb02y state your reasons why these distractors were inappropriate in the respective text \ufb01elds ( max 1 sentence ) .", "entities": []}, {"text": "Figure 10 : An English translation of the original instructions given to teachers ( the original instructions in Swedish can be found in the GitHub repository ) swers using the same 5 buckets as in sampling for teachers \u2019 evaluation .", "entities": []}, {"text": "Recall that distractors are said to be low frequency ( LF - DIS ) if they were chosen by less than 5 % of students .", "entities": [[0, 1, "MetricName", "Recall"]]}, {"text": "Hence , a red cross in the column \u201c F - DIS > 5 % \u201d entails that a given distractor is in fact an LF - DIS .", "entities": []}, {"text": "The MCQ in sample 1 has an entropy of 0 , meaning all students have selected the same option , which in this case was the key .", "entities": [[9, 10, "DatasetName", "0"]]}, {"text": "In this case , two of three distractors were accepted by the majority of teachers , although all of them were LF - DIS .", "entities": []}, {"text": "This is a good example of an MCQ with plausible distractors , but where the stem is too easy .", "entities": []}, {"text": "The MCQ in sample 2 presents an interesting case , when the distractor contains an obvious grammatical error ( comma before the \ufb01rst word in the distractor 3 ) .", "entities": []}, {"text": "While the distractor was rightfully rejected by the majority of teachers , it was still selected by more than 5 % of students .", "entities": []}, {"text": "The MCQ in sample 3 is a good example of longer distractors .", "entities": []}, {"text": "In this case , two distractors were accepted by teachers and two were selected by more than 5 % of students .", "entities": []}, {"text": "However , interestingly these sets are disjoint , meaning that all three distractors could potentially be useful .", "entities": []}, {"text": "Another more general observation , requiring future research , is that our model seems to struggle more when generating longer distractors in general , resulting in non-\ufb01nished sentences or repetitions of words .", "entities": []}, {"text": "The MCQ in sample 4 is somewhat opposite to sample 3 , since one distractor that was accepted by the teachers turned out to be an LF - DIS .", "entities": []}, {"text": "This either means that the stem was too easy or that none of the distractors were potentially useful .", "entities": []}, {"text": "The MCQ in sample 5 is the one with a highest theoretically possible entropy between selecting the correct or a wrong option .", "entities": []}, {"text": "Note that it mightstill happen that some of the distractors is LF - DIS , since the entropy is calculated not between all four options , but only between the key and the distractors as a group .", "entities": []}, {"text": "402Stem Vad t \u00a8acker \u00a8over h \u00a8alften av Sveriges yta ?", "entities": []}, {"text": "( What covers more than half of the surface of Sweden ? )", "entities": []}, {"text": "Key : skog ( forest ) Distractor ( sv ) Distractor ( en ) Accepted by teachers?F - DIS>5 % vattendrag water 4 8 milj\u00a8oer environments 8 8 djur - och v \u00a8axtarter plant and animal species 4 8 Figure 11 : Sample 1 ( entropy 0 ) .", "entities": [[46, 47, "DatasetName", "0"]]}, {"text": "\u201c F - DIS \u201d denotes the frequency of choice of a distractors by the students , \u201c Accepted by teachers \u201d indicates if a distractor was accepted by the majority of teachers .", "entities": []}, {"text": "Stem Vad f \u00a8orvaras p \u02daa en torkanl \u00a8aggning ?", "entities": []}, {"text": "( What is stored in a drying facility ? )", "entities": []}, {"text": "Key : spannm \u02daal , h\u00a8o eller halm ( grains , hay or straw ) Distractor ( sv ) Distractor ( en ) Accepted by teachers?F - DIS>5 % ogr\u00a8as weeds 4 8 balpressar balers 4 4 , harvar och s \u02daar , harrows and sows 8 8 Figure 12 : Sample 2 ( entropy 0.31 ) .", "entities": []}, {"text": "\u201c F - DIS \u201d denotes the frequency of choice of a distractors by the students , \u201c Accepted by teachers \u201d indicates if a distractor was accepted by the majority of teachers .", "entities": []}, {"text": "Stem N\u00a8ar betalar du avgiften om du ans \u00a8oker p \u02daa en ambassad ?", "entities": []}, {"text": "( When do you pay the fee when you are applying at an embassy ? )", "entities": []}, {"text": "Key : n\u00a8ar du l \u00a8amnar in din ans \u00a8okan ( when you are handing in your application )", "entities": []}, {"text": "Distractor ( sv ) Distractor ( en ) Accepted by teachers?F - DIS>5 % n\u00a8ar du f \u02daar uppeh \u02daallstillst \u02daand when you receive your residence permit4 4 n\u00a8ar du ans \u00a8oker f \u00a8or f \u00a8orsta g\u02daangenwhen you are applying for the \ufb01rst time8 4 n\u00a8ar du ans \u00a8oker innan tiden f \u00a8or ditt tidigare tillst \u02daand", "entities": []}, {"text": "har", "entities": []}, {"text": "g \u02daatt utwhen you are applying before your previous permit has expired4 8 Figure 13 : Sample 3 ( entropy 0.57 ) .", "entities": []}, {"text": "\u201c F - DIS \u201d denotes the frequency of choice of a distractors by the students , \u201c Accepted by teachers \u201d indicates if a distractor was accepted by the majority of teachers .", "entities": []}, {"text": "403Stem Vad\u00a8ar negativt f \u00a8or \ufb02era marina milj \u00a8oer ?", "entities": []}, {"text": "( What is negative for several marine environments ? )", "entities": []}, {"text": "Key : kommersiellt \ufb01ske ( commercial \ufb01shing )", "entities": []}, {"text": "Distractor ( sv ) Distractor ( en ) Accepted by teachers?F - DIS>5 % klimatf \u00a8or\u00a8andringar climate change 8 4 m\u02daanga olika milj \u00a8oer many different environments 8 8 \u00a8aldre brukningsmetoder older cultivation methods 4 8 Figure 14 : Sample 4 ( entropy 0.675 ) .", "entities": []}, {"text": "\u201c F - DIS \u201d denotes the frequency of choice of a distractors by the students , \u201c Accepted by teachers \u201d indicates if a distractor was accepted by the majority of teachers .", "entities": []}, {"text": "Stem Vilka kan utf \u00a8arda medicinska rapporter f \u00a8or kabinbes \u00a8attning ?", "entities": []}, {"text": "( Who can issue medical reports for cabin crew ? )", "entities": []}, {"text": "Key : f\u00a8oretagsl \u00a8akare ( company physicians ) Distractor ( sv ) Distractor ( en ) Accepted by teachers?F - DIS>5 % f\u00a8oretagssk \u00a8oterskor company nurses 4 4 \ufb02ygl \u00a8akare aviation physicians 8 4 gymnasieinfo.se gymnasieinfo.se 8 8 Figure 15 : Sample 5 ( entropy 0.69 ) .", "entities": []}, {"text": "\u201c F - DIS \u201d denotes the frequency of choice of a distractors by the students , \u201c Accepted by teachers \u201d indicates if a distractor was accepted by the majority of teachers .", "entities": []}]