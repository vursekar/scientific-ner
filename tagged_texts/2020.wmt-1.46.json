[{"text": "Proceedings of the 5th Conference on Machine Translation ( WMT ) , pages 402\u2013408 Online , November 19\u201320 , 2020 .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "c", "entities": []}, {"text": "2020 Association for Computational Linguistics402Transfer Learning for Related Languages : Submissions to the WMT20 Similar Language Translation Task Lovish Madaan , Soumya Sharma , Parag Singla Indian Institute of Technology , Delhi flovish97 , soumyasharma98 g@gmail.com , parags@cse.iitd.ac.in", "entities": [[16, 17, "TaskName", "Translation"]]}, {"text": "Abstract In this paper , we describe IIT Delhi \u2019s submissions to the WMT 2020 task on Similar Language Translation for four language directions : Hindi$Marathi and Spanish $ Portuguese .", "entities": [[13, 15, "DatasetName", "WMT 2020"], [19, 20, "TaskName", "Translation"]]}, {"text": "We try out three different model settings for the translation task and select our primary and contrastive submissions on the basis of performance of these three models .", "entities": []}, {"text": "For our best submissions , we \ufb01ne - tune the mBART model ( Liu et al . , 2020 ) on the parallel data provided for the task .", "entities": [[10, 11, "MethodName", "mBART"]]}, {"text": "The pre - training is done using self - supervised objectives on a large amount of monolingual data for many languages .", "entities": []}, {"text": "Overall , our models are ranked in the top four of all systems for the submitted language pairs , with \ufb01rst rank in Spanish ! Portuguese .", "entities": []}, {"text": "1 Introduction Machine Translation ( MT ) is currently tackled using rule - based methods ( RBMT )", "entities": [[2, 4, "TaskName", "Machine Translation"]]}, {"text": "( Charoenpornsawat et al . , 2002 ) , phrase - based statistical methods ( SMT ) ( Koehn et al . , 2003 ) and neural methods ( NMT ) ( Cho et al . , 2014 ; Sutskever et al . , 2014 ; Bahdanau et al . , 2015 ;", "entities": []}, {"text": "Vaswani et", "entities": []}, {"text": "al . , 2017 ) .", "entities": []}, {"text": "NMT has achieved high translation quality for several language pairs ( Bojar et al . , 2018 ; Barrault et al . , 2019 ) , but this level of performance usually requires large amounts of aligned data in the order of millions of sentence pairs .", "entities": []}, {"text": "For low and medium resource languages , SMT performs better than NMT ( Koehn and Knowles , 2017 ; Sennrich and Zhang , 2019 ) .", "entities": []}, {"text": "SMT also shows better performance when there is a domain mismatch between the train and test datasets , which is typical of low and medium resource language pairs .", "entities": []}, {"text": "In these settings , NMT performance can be boosted by leveraging additional monolingual data to enforce various types of constraints or increasing the training data using back - translation .", "entities": []}, {"text": "These methods can be particularly helpful if the sourceand target languages in MT are closely related and share language structure and alphabet .", "entities": []}, {"text": "Recently , pre - training methods for sequence - to - sequence ( seq2seq ) models have been introduced like MASS ( Song et al . , 2019a ) , XLM ( Conneau and Lample , 2019 ) , BART ( Lewis et al . , 2019 ) , and mBART ( Liu et al . , 2020 ) .", "entities": [[13, 14, "MethodName", "seq2seq"], [30, 31, "MethodName", "XLM"], [39, 40, "MethodName", "BART"], [50, 51, "MethodName", "mBART"]]}, {"text": "These methods show signi\ufb01cant gains in downstream tasks like NMT , summarization , natural language inference ( NLI ) , etc .", "entities": [[11, 12, "TaskName", "summarization"], [13, 16, "TaskName", "natural language inference"]]}, {"text": "In this paper , we focus on the transfer learning capabilities in NMT for the task of translation between related languages where parallel data is scarce .", "entities": [[8, 10, "TaskName", "transfer learning"]]}, {"text": "IIT Delhi participated in the WMT 2020", "entities": [[5, 7, "DatasetName", "WMT 2020"]]}, {"text": "Shared task on Similar Language Translation for four language directions : Hindi ( hi)$Marathi ( mr ) andSpanish ( es ) $ Portuguese ( pt ) .", "entities": [[5, 6, "TaskName", "Translation"]]}, {"text": "The \ufb01rst language pair is low resource and second is medium resource in terms of the parallel data available for the task .", "entities": []}, {"text": "Refer to Table 2 for the classi\ufb01cation .", "entities": []}, {"text": "We \ufb01ne - tuned the pre - trained mBART model ( Liu et al . , 2020 ) on the parallel data provided for the task .", "entities": [[8, 9, "MethodName", "mBART"]]}, {"text": "mBART gives better performance than SMT models even when the parallel data is very limited .", "entities": [[0, 1, "MethodName", "mBART"]]}, {"text": "mBART is pre - trained on 25 languages , which contain Hindi and Spanish , but not Marathi and Portuguese .", "entities": [[0, 1, "MethodName", "mBART"]]}, {"text": "mBART is able to leverage transfer learning capabilities even for those languages that are originally not present during the pre - training phase .", "entities": [[0, 1, "MethodName", "mBART"], [5, 7, "TaskName", "transfer learning"]]}, {"text": "The \ufb01ne - tuned mBART architecture forms our best submissions for both language pairs : hi$mrandes$pt .", "entities": [[4, 5, "MethodName", "mBART"]]}, {"text": "The rankings obtained by us in each of the language directions are listed in Table 1 .", "entities": []}, {"text": "Our \ufb01ndings are in line with earlier observations in the literature where transfer learning techniques have been shown to signi\ufb01cantly boost NMT performance .", "entities": [[12, 14, "TaskName", "transfer learning"]]}, {"text": "The rest of the paper is organized as follows : Section 2 provides the background and related work for low / medium resource NMT .", "entities": []}, {"text": "Section 3 gives an", "entities": []}, {"text": "403Direction BLEU Rank hi!mr 15.14 4 mr!hi 24.53 2 es!pt 32.69 1 pt!es 32.84 2 Table 1 : BLEU scores on the test set provided for the task and system rankings according to the automatic evaluation metrics .", "entities": [[1, 2, "MetricName", "BLEU"], [18, 19, "MetricName", "BLEU"]]}, {"text": "overview of the systems tried .", "entities": []}, {"text": "In Section 4 , we present the experiments and training pipeline setup .", "entities": []}, {"text": "The results and analysis are detailed in Section 5 .", "entities": []}, {"text": "We \ufb01nally conclude in Section 6 . 2 Background SMT is tackled by building a phrase table from the aligned parallel data .", "entities": []}, {"text": "The target side translation is then generated by matching the most appropriate phrases in the source sentence conditioned on the target side language model along with a reordering model ( Koehn et al . , 2003 ) .", "entities": []}, {"text": "NMT is modeled using Encoder - Decoder models ( Cho et al . , 2014 ; Sutskever et al . , 2014 ; Bahdanau et al . , 2015 ) , with the Transformer model ( Vaswani et al . , 2017 ) achieving state - of - the - art on many MT problems .", "entities": [[33, 34, "MethodName", "Transformer"]]}, {"text": "But these models \u2019 reliance on large aligned parallel data for the source and target languages makes them unsuitable for low / medium resource language pairs ( Koehn and Knowles , 2017 ) .", "entities": []}, {"text": "Some of the previous works in these settings to improve NMT performance are described below : 2.1 Multilingual NMT Instead of using only two languages ( source and target ) for training an NMT model , using multiple languages has been shown to help in low resource scenarios .", "entities": []}, {"text": "For example , it might be the case that a certain pair of languages have very little parallel data between them , but there exists a third language with abundant parallel data with the original two languages .", "entities": []}, {"text": "This third language acts as a pivot and helps in improving NMT between the two languages ( Aharoni et al . , 2019 ; Gu et al . , 2018 ; Liu et al . , 2020 ; Zhang et al . , 2020 ) .", "entities": []}, {"text": "2.2 Back - Translation Back - Translation ( Sennrich et al . , 2016 ; Edunov et al . , 2018 ; Hoang et al . , 2018 ) increases theamount of training data by using monolingual corpus along with partially - trained NMT models on the limited parallel data .", "entities": [[3, 4, "TaskName", "Translation"], [6, 7, "TaskName", "Translation"]]}, {"text": "Pseudo - parallel corpus for each direction is \ufb01rst obtained by generating the translations of the monolingual data for each language using the partially - trained MT models on the limited parallel data .", "entities": []}, {"text": "Using these pseudoparallel corpora , the partially - trained NMT models are then trained further for some number of steps .", "entities": []}, {"text": "In this way , millions of pseudo - parallel sentence pairs can be generated to improve NMT models because of the abundance of monolingual data .", "entities": []}, {"text": "Another version of using back - translation is the copying mechanism .", "entities": []}, {"text": "Currey et al .", "entities": []}, {"text": "( 2017 ) proposes to copy the target side monolingual data on the source side to create additional data without modifying the training regimen for NMT .", "entities": []}, {"text": "This helps the model to generate \ufb02uent translations .", "entities": []}, {"text": "2.3 Pre - trained Language Models For NMT , the \ufb01rst step is the random initialization of model weights in both the encoder and decoder .", "entities": []}, {"text": "Instead of random initialization , NMT models can be initialized by pre - training parts of the model ( Conneau and Lample , 2019 ; Edunov et al . , 2019 ) , or pre - training the complete seq2seq model ( Ramachandran et", "entities": [[39, 40, "MethodName", "seq2seq"]]}, {"text": "al . , 2017 ; Song et al . , 2019b ; Liu et al . , 2020 )", "entities": []}, {"text": ".", "entities": []}, {"text": "These pre - training methods leverage different kinds of masking techniques and the pretraining objective is to predict these masked tokens , similar to BERT ( Devlin et al . , 2019 ) .", "entities": [[24, 25, "MethodName", "BERT"]]}, {"text": "Denoising auto - encoding can also be used where a sentence is corrupted by various noising techniques and the pre - training objective is to generate the original uncorrupted sentence as in BART ( Lewis et al . , 2019 ) and mBART ( Liu et al . , 2020 ) .", "entities": [[0, 1, "TaskName", "Denoising"], [32, 33, "MethodName", "BART"], [42, 43, "MethodName", "mBART"]]}, {"text": "2.4 Incorporating Linguistic Information in NMT There also have been works to improve low / medium resource NMT by adding linguistic information either using data augmentation ( Currey and Hea\ufb01eld , 2019 ) , subword embedding augmentation ( Sennrich and Haddow , 2016 ) , or architectural changes ( Eriguchi et al . , 2017 ) .", "entities": [[24, 26, "TaskName", "data augmentation"]]}, {"text": "This helps the model to not only learn the alignment between source and target language spaces , but also syntax structure like dependency parse , part of speech , etc .", "entities": []}, {"text": "This helps in making the target side translations more \ufb02uent and conforming to the structure of the language .", "entities": []}, {"text": "We do not explore this direction in this paper .", "entities": []}, {"text": "4043 System Overview We experimented with three different settings for hi$mras listed below .", "entities": []}, {"text": "SMT This phrase - based system leverages both monolingual and parallel data provided for the task .", "entities": []}, {"text": "We use Moses ( Koehn et al . , 2007 ) for training the SMT systems .", "entities": []}, {"text": "NMT ( Transformer ) For this , we used the standard Transformer large architecture from Vaswani et", "entities": [[2, 3, "MethodName", "Transformer"], [11, 12, "MethodName", "Transformer"]]}, {"text": "al . ( 2017 ) for training on the parallel data provided for the task .", "entities": []}, {"text": "NMT ( mBART ) mBART ( Liu et al . , 2020 ) is a large Transformer pre - trained on monolingual data for 25 languages .", "entities": [[2, 3, "MethodName", "mBART"], [4, 5, "MethodName", "mBART"], [16, 17, "MethodName", "Transformer"]]}, {"text": "The pre - training objective for mBART is seq2seq de - noising for natural text as in BART ( Lewis et al . , 2019 ) .", "entities": [[6, 7, "MethodName", "mBART"], [8, 9, "MethodName", "seq2seq"], [17, 18, "MethodName", "BART"]]}, {"text": "mBART provides a general - purpose pre - trained Transformer for any downstream task .", "entities": [[0, 1, "MethodName", "mBART"], [9, 10, "MethodName", "Transformer"]]}, {"text": "It has been shown to give signi\ufb01cant improvements over the random initialization for NMT and is the current state - of - the - art for many low resource language pairs .", "entities": []}, {"text": "Implementation Details mBART uses a shared subword vocabulary of 250 K tokens for all the 25 languages present in the pre - training .", "entities": [[2, 3, "MethodName", "mBART"]]}, {"text": "We use the same vocabulary for Marathi and Portuguese also , even though they were not used during the pre - training phase .", "entities": []}, {"text": "Marathi shares its subword vocabulary with languages like Hindi and Nepali in mBART , and Portuguese shares with Spanish , Italian and other European languages present in mBART .", "entities": [[12, 13, "MethodName", "mBART"], [27, 28, "MethodName", "mBART"]]}, {"text": "The percentage of unknown tokens [ UNK ] in Marathi and Portuguese parallel datasets is less than 0.003 % when using the shared mBART vocabulary .", "entities": [[23, 24, "MethodName", "mBART"]]}, {"text": "Additionally , the mBART architecture requires language speci\ufb01c token at the end of each input sequence to provide the language speci\ufb01c context for the decoder .", "entities": [[3, 4, "MethodName", "mBART"]]}, {"text": "Since Marathi and Portuguese were not present during the pre - training phase , we use the token corresponding to the second most related language present in mBART pre - training for specifying the context at the time of decoding in each case .", "entities": [[27, 28, "MethodName", "mBART"]]}, {"text": "For Marathi , we used the Nepali language token and for Portuguese , we used the Italian language token .", "entities": []}, {"text": "We could not use Spanish language token for Portuguese because we are doing translations to and from Spanish.train valid test hi$mr 43,274 1,411 1,941 es$pt 3,472,860 1,283 1,495 Table 2 : Dataset statistics .", "entities": []}, {"text": "First is low resource pair ( # train<1Million ) and second is medium resource ( 1 Million < # train<10Million ) .", "entities": []}, {"text": "Model hi - mr   !", "entities": []}, {"text": "SMT 18.74 14.91 mBART 24.53 15.14", "entities": [[3, 4, "MethodName", "mBART"]]}, {"text": "Table 3 : BLEU scores on Hindi $ Marathi on the test set for our primary and contrastive submissions .", "entities": [[3, 4, "MetricName", "BLEU"]]}, {"text": "4 Experiments We use hi$mrandes$ptlanguage pairs for our experiments .", "entities": []}, {"text": "4.1 Datasets & Preprocessing Because of the constrained nature of the shared task , we only use the parallel data provided for this task .", "entities": []}, {"text": "We removed the empty instances for both language pairs ( < 2000 instances ) .", "entities": []}, {"text": "For es $ pt , we do not use \u2018 WikiTitles v2 \u2019 part of the parallel data for training because of very short sentences in the dataset .", "entities": []}, {"text": "The cleaned parallel dataset statistics are provided in Table 2 .", "entities": []}, {"text": "Preprocessing We use sentence piece tokenization ( Kudo and Richardson , 2018 ) for generating the source and target sequences for the NMT architectures .", "entities": []}, {"text": "For the standard Transformer , we train a sentence piece model using 40 K subword tokens for hi$mr .", "entities": [[3, 4, "MethodName", "Transformer"]]}, {"text": "For mBART , we use Liu et al .", "entities": [[1, 2, "MethodName", "mBART"]]}, {"text": "( 2020 ) \u2019s pre - trained1sentence piece model comprising of 250 K subword tokens as the vocabulary .", "entities": []}, {"text": "For the SMT model on hi$mr , we also use the monolingual data provided for this task .", "entities": []}, {"text": "We extract 5 Million monolingual sentences each for Hindi and Marathi after deduplication and use this set for training the language models .", "entities": []}, {"text": "We use Moses ( Koehn et al . , 2007 ) for all tokenization / detokenization scripts .", "entities": []}, {"text": "1https://github.com/pytorch/fairseq/ blob / master / examples / mbart / README.md", "entities": [[7, 8, "MethodName", "mbart"]]}, {"text": "405Submission hi - mr es - pt   ! !", "entities": []}, {"text": "IIT Delhi ( ours ) 24.53 15.14 32.84 32.69 Rank 1 24.53 18.26 33.82 32.69 Table 4 : Hindi - Marathi and Spanish - Portuguese BLEU scores on the test dataset of the Similar Language Translation Task .", "entities": [[25, 26, "MetricName", "BLEU"], [35, 36, "TaskName", "Translation"]]}, {"text": "Our submission scores are bolded when they match the \ufb01rst ranked submission .", "entities": []}, {"text": "4.2 Model Architectures & Training SMT", "entities": []}, {"text": "We generate a phrase table for the SMT model using the code provided by Lample et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "We used Moses ( Koehn et al . , 2007 ) and Giza++ with standard settings to train the SMT model in both directions .", "entities": []}, {"text": "NMT ( Transformer ) We use the large Transformer from Vaswani et", "entities": [[2, 3, "MethodName", "Transformer"], [8, 9, "MethodName", "Transformer"]]}, {"text": "al .", "entities": []}, {"text": "( 2017 ) with 8 encoder and decoder layers and replicate all the parameters from Ott et al .", "entities": []}, {"text": "( 2018 ) .", "entities": []}, {"text": "The number of parameters in the model are approximately 248 Million and it takes\u001826 hours on 4 Nvidia V100 ( 32 GB ) GPUs .", "entities": [[1, 4, "HyperparameterName", "number of parameters"]]}, {"text": "NMT ( mBART ) For this , we use 12 Transformer encoder and decoder layers , with total number of model parameters \u0018611 Million .", "entities": [[2, 3, "MethodName", "mBART"], [10, 11, "MethodName", "Transformer"]]}, {"text": "We use the pretrained mBART for initializing the model weights .", "entities": [[4, 5, "MethodName", "mBART"]]}, {"text": "We follow the recommendations of Liu et al .", "entities": []}, {"text": "( 2020 ) for the hyperparameter settings .", "entities": []}, {"text": "We stop the training after 25 K gradient updates for the model .", "entities": []}, {"text": "These updates take\u001835 hours on 4 Nvidia V100 ( 32 GB ) GPUs .", "entities": []}, {"text": "4.3 Evaluation We use case - insensitive BLEU scores ( Papineni et al . , 2002 ) calculated using sacreBLEU2(Post , 2018 ) .", "entities": [[7, 8, "MetricName", "BLEU"]]}, {"text": "These scores are calculated on the validation set to decide our primary and contrastive submissions .", "entities": []}, {"text": "For evaluating performance on the test set , the organizers use BLEU , TER ( Snover et al . , 2006 ) , and RIBES ( Isozaki et", "entities": [[11, 12, "MetricName", "BLEU"]]}, {"text": "al . , 2010 ) .", "entities": []}, {"text": "5 Results and Analysis Results Table 3 shows our results on the test set for our primary and contrastive submissions .", "entities": []}, {"text": "We observed the performance of our three model settings on the validation set , and we selected the mBART model as our primary submission and SMT model as the contrastive submission for hi $ mr .", "entities": [[18, 19, "MethodName", "mBART"]]}, {"text": "Similarly , the mBART model forms our 2Signature : BLEU + case.mixed + numrefs.1 + smooth.exp + tok.13a + version.1.3.1primary submission for es$pt .", "entities": [[3, 4, "MethodName", "mBART"], [9, 10, "MetricName", "BLEU"]]}, {"text": "Table 4 lists our \ufb01nal results on this shared task .", "entities": []}, {"text": "We also list the BLEU scores for the submission that got \ufb01rst rank in each of the language directions .", "entities": [[4, 5, "MetricName", "BLEU"]]}, {"text": "Since the test sets were hidden at the time of submission , we do not report our numbers on the standard Transformer architecture .", "entities": [[21, 22, "MethodName", "Transformer"]]}, {"text": "Analysis Even though Marathi and Portuguese are not present during the pre - training phase of mBART , \ufb01ne - tuning on these languages provides signi\ufb01cant boosts over SMT and standard Transformer .", "entities": [[16, 17, "MethodName", "mBART"], [31, 32, "MethodName", "Transformer"]]}, {"text": "This shows that some level of language independent multilingual embeddings are present in the pre - trained model weights which can be exploited for the transfer task .", "entities": []}, {"text": "6 Discussion and Conclusion We have participated in the Similar Language Translation task on four language directions .", "entities": [[11, 12, "TaskName", "Translation"]]}, {"text": "We have shown that pre - trained models can help in low and medium resource NMT .", "entities": []}, {"text": "Our best system uses the pre - trained mBART model ( Liu et al . , 2020 ) and \ufb01ne - tunes on the parallel data provided for the speci\ufb01c translation task .", "entities": [[8, 9, "MethodName", "mBART"]]}, {"text": "Our results demonstrate that pre - training can help even when the language used for \ufb01ne - tuning is not present during pre - training .", "entities": []}, {"text": "One direction of future work is to add linguistic information during the pre - training phase to get more \ufb02uent translations .", "entities": []}, {"text": "When this information is not available directly ( especially for low resource languages ) , pre - training on a related high resource language with syntax information can help low resource languages also .", "entities": []}, {"text": "Acknowledgments We thank the IIT Delhi HPC facility3for the computational resources .", "entities": []}, {"text": "We are also thankful to Ganesh Ramakrishnan and Pawan Goyal for initial discussions on the project .", "entities": []}, {"text": "Parag Singla is supported 3http://supercomputing.iitd.ac.in/", "entities": []}, {"text": "406by the DARPA Explainable Arti\ufb01cial Intelligence ( XAI ) Program with number N66001 - 17 - 2 - 4032 , Visvesvaraya Young Faculty Fellowships by Govt . of India and IBM SUR awards .", "entities": [[2, 3, "DatasetName", "DARPA"]]}, {"text": "Any opinions , \ufb01ndings , conclusions or recommendations expressed in this paper are those of the authors and do not necessarily re\ufb02ect the views or of\ufb01cial policies , either expressed or implied , of the funding agencies .", "entities": []}, {"text": "References Roee Aharoni , Melvin Johnson , and Orhan Firat .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Massively multilingual neural machine translation .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , NAACL - HLT 2019 , Minneapolis , MN , USA , June 27 , 2019 , Volume 1 ( Long and Short Papers ) , pages 3874\u20133884 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Neural machine translation by jointly learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "In 3rd International Conference on Learning Representations , ICLR 2015 , San Diego , CA , USA , May 7 - 9 , 2015 , Conference Track Proceedings .", "entities": []}, {"text": "Lo\u00a8\u0131c Barrault , Ondrej Bojar , Marta R. Costa - juss ` a , Christian Federmann , Mark Fishel , Yvette Graham , Barry Haddow , Matthias Huck , Philipp Koehn , Shervin Malmasi , Christof Monz , Mathias M \u00a8uller , Santanu Pal , Matt Post , and Marcos Zampieri .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Findings of the 2019 conference on machine translation ( WMT19 ) .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Fourth Conference on Machine Translation , WMT 2019 , Florence , Italy , August 1 - 2 , 2019 - Volume 2 : Shared Task Papers , Day 1 , pages 1\u201361 . Association for Computational Linguistics .", "entities": [[7, 9, "TaskName", "Machine Translation"], [13, 14, "MethodName", "Florence"]]}, {"text": "Ondrej Bojar , Christian Federmann , Mark Fishel , Yvette Graham , Barry Haddow , Philipp Koehn , and Christof Monz .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Findings of the 2018 conference on machine translation ( WMT18 ) .", "entities": [[6, 8, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Shared Task Papers , WMT 2018 , Belgium , Brussels , October 31 - November 1 , 2018 , pages 272\u2013303 .", "entities": [[7, 9, "TaskName", "Machine Translation"], [14, 16, "DatasetName", "WMT 2018"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Paisarn Charoenpornsawat , Virach Sornlertlamvanich , and Thatsanee Charoenporn .", "entities": []}, {"text": "2002 .", "entities": []}, {"text": "Improving translation quality of rule - based machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In COLING-02 : Machine Translation in Asia .", "entities": [[3, 5, "TaskName", "Machine Translation"]]}, {"text": "Kyunghyun Cho , Bart van Merrienboer , Dzmitry Bahdanau , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "On the properties of neural machine translation : Encoder - decoder approaches .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "In Proceedings of SSST@EMNLP 2014 , Eighth Workshop on Syntax , Semantics and Structure in Statistical Translation , Doha , Qatar , 25 October 2014 , pages 103\u2013111 .", "entities": [[16, 17, "TaskName", "Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Alexis Conneau and Guillaume Lample .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Crosslingual language model pretraining .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 32 : Annual Conference on Neural Information Processing Systems 2019 , NeurIPS 2019 , 8 - 14 December 2019 , Vancouver , BC , Canada , pages 7057\u20137067 .", "entities": []}, {"text": "Anna Currey , Antonio Valerio Miceli Barone , and Kenneth Hea\ufb01eld . 2017 .", "entities": []}, {"text": "Copied monolingual data improves low - resource neural machine translation .", "entities": [[4, 10, "TaskName", "low - resource neural machine translation"]]}, {"text": "InProceedings of the Second Conference on Machine Translation , WMT 2017 , Copenhagen , Denmark , September 7 - 8 , 2017 , pages 148\u2013156 .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Anna Currey and Kenneth Hea\ufb01eld .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Incorporating source syntax into transformer - based neural machine translation .", "entities": [[8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Fourth Conference on Machine Translation , WMT 2019 , Florence , Italy , August 1 - 2 , 2019 - Volume 1 : Research Papers , pages 24\u201333 .", "entities": [[7, 9, "TaskName", "Machine Translation"], [13, 14, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .", "entities": []}, {"text": "BERT : pre - training of deep bidirectional transformers for language understanding .", "entities": [[0, 1, "MethodName", "BERT"]]}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , NAACL - HLT 2019 , Minneapolis , MN , USA , June 2 - 7 , 2019 , Volume 1 ( Long and Short Papers ) , pages 4171\u20134186 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sergey Edunov , Alexei Baevski , and Michael Auli .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Pre - trained language model representations for language generation .", "entities": []}, {"text": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , NAACL - HLT 2019 , Minneapolis , MN , USA , June 2 - 7 , 2019 , Volume 1 ( Long and Short Papers ) , pages 4052\u20134059 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Sergey Edunov , Myle Ott , Michael Auli , and David Grangier .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Understanding back - translation at scale .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , Brussels , Belgium , October 31 - November 4 , 2018 , pages 489\u2013500 . Association for Computational Linguistics .", "entities": []}, {"text": "Akiko Eriguchi , Yoshimasa Tsuruoka , and Kyunghyun Cho . 2017 .", "entities": []}, {"text": "Learning to parse and translate improves neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , ACL 2017 , Vancouver , Canada , July 30 - August 4 , Volume 2 : Short Papers , pages 72\u201378 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Jiatao Gu , Hany Hassan , Jacob Devlin , and Victor O. K. Li .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Universal neural machine translation for extremely low resource languages .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics :", "entities": []}, {"text": "407Human Language Technologies , NAACL - HLT 2018 , New Orleans , Louisiana , USA , June 1 - 6 , 2018 , Volume 1 ( Long Papers ) , pages 344\u2013354 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Cong Duy Vu Hoang , Philipp Koehn , Gholamreza Haffari , and Trevor Cohn .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Iterative backtranslation for neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation , NMT@ACL 2018 , Melbourne , Australia , July 20 , 2018 , pages 18\u201324 .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Hideki Isozaki , Tsutomu Hirao , Kevin Duh , Katsuhito Sudoh , and Hajime Tsukada .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "Automatic evaluation of translation quality for distant language pairs .", "entities": []}, {"text": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing , EMNLP 2010 , 9 - 11 October 2010 , MIT Stata Center , Massachusetts , USA , A meeting of SIGDAT , a Special Interest Group of the ACL , pages 944\u2013952 .", "entities": []}, {"text": "ACL .", "entities": []}, {"text": "Philipp Koehn , Hieu Hoang , Alexandra Birch , Chris Callison - Burch , Marcello Federico , Nicola Bertoldi , Brooke Cowan , Wade Shen , Christine Moran , Richard Zens , et al . 2007 .", "entities": []}, {"text": "Moses : Open source toolkit for statistical machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions , pages 177\u2013180 .", "entities": []}, {"text": "Philipp Koehn and Rebecca Knowles .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Six challenges for neural machine translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the First Workshop on Neural Machine Translation , NMT@ACL 2017 , Vancouver , Canada , August 4 , 2017 , pages 28\u201339 .", "entities": [[8, 10, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Philipp Koehn , Franz J. Och , and Daniel Marcu .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Statistical phrase - based translation .", "entities": []}, {"text": "In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics , pages 127\u2013133 .", "entities": []}, {"text": "Taku Kudo and John Richardson .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Sentencepiece :", "entities": [[0, 1, "MethodName", "Sentencepiece"]]}, {"text": "A simple and language independent subword tokenizer and detokenizer for neural text processing .", "entities": []}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , EMNLP 2018 : System Demonstrations , Brussels , Belgium , October 31 - November 4 , 2018 , pages 66\u201371 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Guillaume Lample , Myle Ott , Alexis Conneau , Ludovic Denoyer , and Marc\u2019Aurelio Ranzato .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Phrase - based & neural unsupervised machine translation .", "entities": [[5, 8, "TaskName", "unsupervised machine translation"]]}, {"text": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) .", "entities": []}, {"text": "Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , and Luke Zettlemoyer.2019 .", "entities": []}, {"text": "BART : denoising sequence - to - sequence pretraining for natural language generation , translation , and comprehension .", "entities": [[0, 1, "MethodName", "BART"], [2, 3, "TaskName", "denoising"]]}, {"text": "CoRR , abs/1910.13461 .", "entities": []}, {"text": "Yinhan Liu , Jiatao Gu , Naman Goyal , Xian Li , Sergey Edunov , Marjan Ghazvininejad , Mike Lewis , and Luke Zettlemoyer .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Multilingual denoising pre - training for neural machine translation .", "entities": [[1, 2, "TaskName", "denoising"], [7, 9, "TaskName", "machine translation"]]}, {"text": "CoRR , abs/2001.08210 .", "entities": []}, {"text": "Myle Ott , Sergey Edunov , David Grangier , and Michael Auli .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Scaling neural machine translation .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , pages 1\u20139 , Brussels , Belgium .", "entities": [[7, 9, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .", "entities": []}, {"text": "Bleu : a method for automatic evaluation of machine translation .", "entities": [[0, 1, "MetricName", "Bleu"], [8, 10, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , July 6 - 12 , 2002 , Philadelphia , PA , USA , pages 311\u2013318 .", "entities": []}, {"text": "ACL .", "entities": []}, {"text": "Matt Post . 2018 .", "entities": []}, {"text": "A call for clarity in reporting BLEU scores .", "entities": [[6, 7, "MetricName", "BLEU"]]}, {"text": "In Proceedings of the Third Conference on Machine Translation : Research Papers , WMT 2018 , Belgium , Brussels , October 31 - November 1 , 2018 , pages 186\u2013191 .", "entities": [[7, 9, "TaskName", "Machine Translation"], [13, 15, "DatasetName", "WMT 2018"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Prajit Ramachandran , Peter J. Liu , and Quoc V .", "entities": []}, {"text": "Le . 2017 .", "entities": []}, {"text": "Unsupervised pretraining for sequence to sequence learning .", "entities": [[3, 6, "MethodName", "sequence to sequence"]]}, {"text": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , EMNLP 2017 , Copenhagen , Denmark , September 9 - 11 , 2017 , pages 383\u2013391 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich and Barry Haddow . 2016 .", "entities": []}, {"text": "Linguistic input features improve neural machine translation .", "entities": [[5, 7, "TaskName", "machine translation"]]}, {"text": "InProceedings of the First Conference on Machine Translation : Volume 1 , Research Papers , pages 83 \u2013 91 , Berlin , Germany .", "entities": [[6, 8, "TaskName", "Machine Translation"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .", "entities": []}, {"text": "Improving neural machine translation models with monolingual data .", "entities": [[2, 4, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics , ACL 2016 , August 7 - 12 , 2016 , Berlin , Germany , Volume 1 : Long Papers .", "entities": []}, {"text": "The Association for Computer Linguistics .", "entities": []}, {"text": "Rico Sennrich and Biao Zhang .", "entities": []}, {"text": "2019 .", "entities": []}, {"text": "Revisiting lowresource neural machine translation : A case study .", "entities": [[3, 5, "TaskName", "machine translation"]]}, {"text": "InProceedings of the 57th Conference of the Association for Computational Linguistics , ACL 2019 , Florence , Italy , July 28- August 2 , 2019 , Volume 1 : Long Papers , pages 211\u2013221 .", "entities": [[15, 16, "MethodName", "Florence"]]}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Matthew Snover , Bonnie Dorr , Richard Schwartz , Linnea Micciulla , and John Makhoul .", "entities": []}, {"text": "2006 .", "entities": []}, {"text": "A study of translation edit rate with targeted human annotation .", "entities": []}, {"text": "408InProceedings of Association for Machine Translation in the Americas , 2006 .", "entities": [[4, 6, "TaskName", "Machine Translation"]]}, {"text": "Kaitao Song , Xu Tan , Tao Qin , Jianfeng Lu , and Tie - Yan Liu . 2019a .", "entities": []}, {"text": "MASS : masked sequence to sequence pre - training for language generation .", "entities": [[3, 6, "MethodName", "sequence to sequence"]]}, {"text": "InProceedings of the 36th International Conference on Machine Learning , ICML 2019 , 9 - 15 June 2019 , Long Beach , California , USA , volume 97 of Proceedings of Machine Learning Research , pages 5926\u20135936 .", "entities": []}, {"text": "PMLR .", "entities": []}, {"text": "Kaitao Song , Xu Tan , Tao Qin , Jianfeng Lu , and Tie - Yan Liu . 2019b .", "entities": []}, {"text": "MASS : masked sequence to sequence pre - training for language generation .", "entities": [[3, 6, "MethodName", "sequence to sequence"]]}, {"text": "InProceedings of the 36th International Conference on Machine Learning , ICML 2019 , 9 - 15 June 2019 , Long Beach , California , USA , volume 97 of Proceedings of Machine Learning Research , pages 5926\u20135936 .", "entities": []}, {"text": "PMLR.Ilya Sutskever , Oriol Vinyals , and Quoc V .", "entities": []}, {"text": "Le . 2014 .", "entities": []}, {"text": "Sequence to sequence learning with neural networks .", "entities": [[0, 3, "MethodName", "Sequence to sequence"]]}, {"text": "InAdvances in Neural Information Processing Systems 27 : Annual Conference on Neural Information Processing Systems 2014 , December 8 - 13 2014 , Montreal , Quebec , Canada , pages 3104\u20133112 .", "entities": []}, {"text": "Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , Lukasz Kaiser , and Illia Polosukhin . 2017 .", "entities": []}, {"text": "Attention is all you need .", "entities": []}, {"text": "In Advances in Neural Information Processing Systems 30 : Annual Conference on Neural Information Processing Systems 2017 , 4 - 9 December 2017 , Long Beach , CA , USA , pages 5998\u20136008 .", "entities": []}, {"text": "Biao Zhang , Philip Williams , Ivan Titov , and Rico Sennrich .", "entities": []}, {"text": "2020 .", "entities": []}, {"text": "Improving massively multilingual neural machine translation and zero - shot translation .", "entities": [[4, 6, "TaskName", "machine translation"]]}, {"text": "CoRR , abs/2004.11867 .", "entities": []}]