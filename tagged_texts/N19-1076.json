[{"text": "Proceedings of NAACL - HLT 2019 , pages 710\u2013716 Minneapolis , Minnesota , June 2 - June 7 , 2019 .", "entities": []}, {"text": "c", "entities": []}, {"text": "2019 Association for Computational Linguistics710Left - to - Right Dependency Parsing with Pointer Networks Daniel Fern \u00b4 andez - Gonz \u00b4 alez Universidade da Coru \u02dcna", "entities": [[9, 11, "TaskName", "Dependency Parsing"]]}, {"text": "FASTPARSE Lab , LyS Group Departamento de Computaci \u00b4 on Elvi\u02dcna , 15071 A Coru \u02dcna , Spain d.fgonzalez@udc.esCarlos G \u00b4 omez - Rodr \u00b4 \u0131guez Universidade da Coru \u02dcna , CITIC FASTPARSE Lab , LyS Group Departamento de Computaci \u00b4 on Elvi\u02dcna , 15071 A Coru \u02dcna , Spain carlos.gomez@udc.es", "entities": []}, {"text": "Abstract We propose a novel transition - based algorithm that straightforwardly parses sentences from left to right by building nattachments , with n being the length of the input sentence .", "entities": []}, {"text": "Similarly to the recent stack - pointer parser by Ma et al .", "entities": []}, {"text": "( 2018 ) , we use the pointer network framework that , given a word , can directly point to a position from the sentence .", "entities": [[7, 9, "MethodName", "pointer network"]]}, {"text": "However , our left - to - right approach is simpler than the original top - down stack - pointer parser ( not requiring a stack ) and reduces transition sequence length in half , from 2n\u00001actions to n. This results in a quadratic non - projective parser that runs twice as fast as the original while achieving the best accuracy to date on the English PTB dataset ( 96.04 % UAS , 94.43 % LAS ) among fully - supervised singlemodel dependency parsers , and improves over the former top - down transition system in the majority of languages tested .", "entities": [[60, 61, "MetricName", "accuracy"], [66, 67, "DatasetName", "PTB"]]}, {"text": "1 Introduction Dependency parsing , the task of automatically obtaining the grammatical structure of a sentence expressed as a dependency tree , has been widely studied by natural language processing ( NLP ) researchers in the last decades .", "entities": [[2, 4, "TaskName", "Dependency parsing"]]}, {"text": "Most of the models providing competitive accuracies fall into two broad families of approaches : graph - based ( McDonald et al . , 2005a , b ) and transition - based ( Yamada and Matsumoto , 2003 ; Nivre , 2003 ) dependency parsers .", "entities": []}, {"text": "Given an input sentence , a graph - based parser scores trees by decomposing them into factors , and performs a search for the highest - scoring tree .", "entities": []}, {"text": "In the past two years , this kind of dependency parsers have been ahead in terms of accuracy thanks to the graph - based neural architecture developed by Dozat and Manning ( 2016 ) , which not only achieved state - of - the - art accuracies on theStanford Dependencies conversion of the English Penn Treebank ( hereinafter , PTB - SD ) , but also obtained the best results in the majority of languages in the CoNLL 2017 Shared Task ( Dozat et al . , 2017 ) .", "entities": [[17, 18, "MetricName", "accuracy"], [54, 56, "DatasetName", "Penn Treebank"], [59, 60, "DatasetName", "PTB"]]}, {"text": "This tendency recently changed , since a transition - based parser developed by Ma et al .", "entities": []}, {"text": "( 2018 ) managed to outperform the best graphbased model in the majority of datasets tested .", "entities": []}, {"text": "Transition - based parsers incrementally build a dependency graph for an input sentence by applying a sequence of transitions .", "entities": []}, {"text": "This results in more ef\ufb01cient parsers with linear time complexity for parsing projective sentences , or quadratic for handling non - projective structures , when implemented with greedy or beam search .", "entities": []}, {"text": "However , their main weakness is the lack of access to global context information when transitions are greedily chosen .", "entities": []}, {"text": "This favours error propagation , mainly affecting long dependencies that require a larger number of transitions to be built ( McDonald and Nivre , 2011 ) .", "entities": []}, {"text": "Many attempts have been made to alleviate the impact of error propagation in transition - based dependency parsing , but the latest and most successful approach was developed by Ma et al .", "entities": [[13, 18, "TaskName", "transition - based dependency parsing"]]}, {"text": "( 2018 ) .", "entities": []}, {"text": "In particular , they make use of pointer networks ( Vinyals et al . , 2015 ) to implement a new neural network architecture called stack - pointer network .", "entities": [[27, 29, "MethodName", "pointer network"]]}, {"text": "The proposed framework provides a global view of the input sentence by capturing information from the whole sentence and all the arcs previously built , crucial for reducing the effect of error propagation ; and , thanks to an attention mechanism ( Bahdanau et al . , 2014 ; Luong et", "entities": []}, {"text": "al . , 2015 ) , is able to return a position in that sentence that corresponds to a word related to the word currently on top of the stack .", "entities": []}, {"text": "They take advantage of this and propose a novel transition system that follows a top - down depth-\ufb01rst strategy to perform the syntactic analysis .", "entities": []}, {"text": "Concretely , it considers the word", "entities": []}, {"text": "711pointed by the neural network as the child of the word on top of the stack , and builds the corresponding dependency relation between them .", "entities": []}, {"text": "This results in a transition - based algorithm that can process unrestricted non - projective sentences in O(n2)time complexity and requires 2 n-1 actions to successfully parse a sentence with nwords .", "entities": []}, {"text": "We also take advantage of pointer network capabilities and use the neural network architecture introduced by Ma et al .", "entities": [[5, 7, "MethodName", "pointer network"]]}, {"text": "( 2018 ) to design a nonprojective left - to - right transition - based algorithm , where the position value pointed by the network has the opposite meaning : it denotes the index that corresponds to the head node of the current focus word .", "entities": []}, {"text": "This results in a straightforward transition system that can parse a sentence in just n actions , without the need of any additional data structure and by just attaching each word from the sentence to another word ( including the root node ) .", "entities": []}, {"text": "Apart from increasing the parsing speed twofold ( while keeping the same quadratic time complexity ) , it achieves the best accuracy to date among fully - supervised single - model dependency parsers on the PTB - SD , and obtains competitive accuracies on twelve different languages in comparison to the original top - down version .", "entities": [[21, 22, "MetricName", "accuracy"], [35, 36, "DatasetName", "PTB"]]}, {"text": "2 Preliminaries Ma et al .", "entities": []}, {"text": "( 2018 ) propose a novel neural network architecture whose main backbone is a pointer network ( Vinyals et al . , 2015 ) .", "entities": [[14, 16, "MethodName", "pointer network"]]}, {"text": "This kind of neural networks are able to learn the conditional probability of a sequence of discrete numbers that correspond to positions in an input sequence ( in this case , indexes of words in a sentence ) and , by means of attention ( Bahdanau et al . , 2014 ; Luong et al . , 2015 ) , implement a pointer that selects a position from the input at decoding time .", "entities": []}, {"text": "Their approach initially reads the whole sentence , composed of the nwordsw1;:::;wn , and encodes each wione by one into an encoder hidden stateei .", "entities": []}, {"text": "As encoder , they employ a combination of CNNs and bi - directional LSTMs ( Chiu and Nichols , 2016 ; Ma and Hovy , 2016 ) .", "entities": []}, {"text": "For each word , CNNs are used to obtain its character - level representation that is concatenated to the word and PoS embeddings to \ufb01nally be fed into BiLSTMs that encode word context information .", "entities": []}, {"text": "As decoder they present a top - down transition system , where parsing con\ufb01gurations use the classic data structures ( Nivre , 2008 ): a buffer ( thatcontains unattached words ) and a stack ( that holds partially processed words ) .", "entities": []}, {"text": "The available parser actions are two transitions that we call Shift -Attach -pandReduce .", "entities": []}, {"text": "Given a con\ufb01guration with word wion top of the stack , as the pointer network just returns a position pfrom a given sentence , they proceed as follows to determine which transition should be applied : \u000fIfp6 = i , then the pointed word wpis considered as a child of wi ; so the parser chooses aShift -Attach -ptransition to move wpfrom the buffer to the stack and build an arc wi !", "entities": [[13, 15, "MethodName", "pointer network"]]}, {"text": "wp .", "entities": []}, {"text": "\u000fOn the other hand , if p = i , thenwiis considered to have found all its children , and a Reduce transition is applied to pop the stack .", "entities": []}, {"text": "The parsing process starts with a dummy root $ on the stack and , by applying 2 n-1 transitions , a dependency tree is built for the input in a top - down depth-\ufb01rst fashion , where multiple children of a same word are forced during training to be created in an inside - out manner .", "entities": []}, {"text": "More in detail , for each parsing con\ufb01guration ct , the decoder ( implemented as a uni - directional LSTM ) receives the encoder hidden state eiof the word wion top of the stack to generate a decoder hidden state dt .", "entities": [[19, 20, "MethodName", "LSTM"]]}, {"text": "After that , dt , together with the sequence siof encoder hidden states from words still in the buffer plus ei , are used to compute the attention vector atas follows :", "entities": []}, {"text": "vt i = score ( dt;si ) ( 1 ) at = softmax ( vt ) ( 2 ) As attention scoring function ( score ( ) ) , they adopt the biaf\ufb01ne attention mechanism described in ( Luong et al . , 2015 ; Dozat and Manning , 2016 ) .", "entities": [[12, 13, "MethodName", "softmax"]]}, {"text": "Finally , the attention vector atwill be used to return the highest - scoring position pand choose the next transition .", "entities": []}, {"text": "The parsing process ends when only the root remains on the stack .", "entities": []}, {"text": "As extra high - order features , Ma et al .", "entities": []}, {"text": "( 2018 ) add grandparent and sibling information , whose encoder hidden states are added to that of the word on top of the stack to generate the corresponding decoder hidden state dt .", "entities": []}, {"text": "They prove that these additions improve \ufb01nal accuracy , especially when children are attached in an inside - out fashion .", "entities": [[7, 8, "MetricName", "accuracy"]]}, {"text": "According to the authors , the original stackpointer network is trained to maximize the likelihood of choosing the correct word for each possible top - down path from the root to a leaf .", "entities": []}, {"text": "More", "entities": []}, {"text": "712 in detail , a dependency tree can be represented as a sequence of top - down paths p1;:::;pk , where each path picorresponds to a sequence of words $ ; wi;1;wi;2;:::;wi;lifrom the root to a leaf .", "entities": []}, {"text": "Thus , the conditional probability P\u0012(yjx)of the dependency tree yfor an input sentence xcan be factorized according to this top - down structure as : P\u0012(yjx ) = kY i=1P\u0012(pijp < i;x )", "entities": []}, {"text": "= kY i=1liY j=1P\u0012(wi;jjwi;<j;p < i;x ) where\u0012represents model parameters , p < istands for previous paths already explored , wi;jdenotes thejth word in path piandwi;<jrepresents all the previous words on pi .", "entities": []}, {"text": "For more thorough details of the stack - pointer network architecture and the top - down transition system , please read the original work by Ma et al .", "entities": [[8, 10, "MethodName", "pointer network"]]}, {"text": "( 2018 ) .", "entities": []}, {"text": "3", "entities": []}, {"text": "Our approach We take advantage of the neural network architecture designed by Ma et al .", "entities": []}, {"text": "( 2018 ) and introduce a simpler left - to - right transition system that requires neither a stack nor a buffer to process the input sentence and where , instead of selecting a child of the word on top of the stack , the network points to the parent of the current focus word .", "entities": []}, {"text": "In particular , in our proposed approach , the parsing con\ufb01guration just corresponds to a focus word pointer i , that is used to point to the word currently being processed .", "entities": []}, {"text": "The decoding process starts withipointing at the \ufb01rst word of the sentence and , at each parsing con\ufb01guration , only one action is available : the parameterized Attach -p transition , that links the focus word wito the head wordwpin positionpof the sentence ( producing the dependency arc wp!wi ) and moves ione position to the right .", "entities": []}, {"text": "Note that , in our algorithm , pcan equal 0 , attaching , in that case , wito the dummy root node .", "entities": [[9, 10, "DatasetName", "0"]]}, {"text": "The parsing process ends when the last word from the sentence is attached .", "entities": []}, {"text": "This can be easily represented as a loop that traverses the input sentence from left to right , linking each word to another from the same sentence or to the dummy root .", "entities": []}, {"text": "Therefore , we just need nsteps to process thenwords of a given sentence and build a dependency tree .", "entities": []}, {"text": "While our novel transition system intrinsically holds the single - head constraint ( since , after attaching the word wi , ipoints to the next word wi+1 in the sentence ) , it can produce an output with cycles.1Therefore , in order to build a wellformed dependency tree during decoding , attachments that generate cycles in the already - built dependency graph must be forbidden .", "entities": []}, {"text": "Please note that the need of a cycle - checking extension does not increase the overall quadratic runtime complexity of the original implementation by Ma et al .", "entities": []}, {"text": "( 2018 ) since , as in other transition - based parsers such as ( Covington , 2001 ; G \u00b4 omez - Rodr \u00b4 \u0131guez and Nivre , 2010 ) , cycles can be incrementally identi\ufb01ed in amortized constant time by keeping track of connected components using path compression and union by rank .", "entities": []}, {"text": "Therefore , the left - to - right algorithm requires nsteps to produce a parse .", "entities": []}, {"text": "In addition , at each step , the attention vector atneeds to be computed and cycles must be checked , both inO(n )", "entities": []}, {"text": "+ O(n )", "entities": []}, {"text": "= O(n)runtime .", "entities": []}, {"text": "This results in aO(n2)time complexity for decoding.2 On the other hand , while in the top - down decoding only available words in the buffer ( plus the word on top of the stack ) can be pointed to by the network and they are reduced as arcs are created ( basically to keep the single - head constraint ) ; our proposed approach is less rigid : all words from the sentence ( including the root node and excluding wi ) can be pointed to , as long as they satisfy the acyclicity constraint .", "entities": []}, {"text": "This is necessary because two different words might be attached to the same head node and the latter can be located in the sentence either before or after wi .", "entities": []}, {"text": "Therefore , the sequencesi , required by the attention score function ( Eq.(1 ) ) , is composed of the encoder hidden states of all words from the input , excluding ei , and prepending a special vector representation denoting the root node .", "entities": []}, {"text": "We also add extra features to represent the current focus word .", "entities": []}, {"text": "Instead of using grandparent and sibling information ( more bene\ufb01cial for a topdown approach ) , we just add the encoder hidden 1In practice , even with the cycle detection mechanism disabled , the presence of cycles in output parses is very uncommon ( for instance , just in 1 % of sentences in the PTB - SD dev set ) since our system seems to adequately model well - formed tree structures .", "entities": [[55, 56, "DatasetName", "PTB"]]}, {"text": "2A practically faster version of the left - to - right parser might be implemented by just ignoring the presence of cycles during decoding , and destroying the cycles generated as a post - processing step that simply removes one of the arcs involved .", "entities": []}, {"text": "713states of the previous and next words in the sentence to generate dt , which seems to be more suitable for a left - to - right decoding .", "entities": []}, {"text": "In dependency parsing , a tree for an input sentence of length ncan be represented as a set of n directed and binary links l1;:::;ln .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}, {"text": "Each linkliis characterized by the word wiin positioniin the sentence and its head word wh , resulting in a pair ( wi;wh ) .", "entities": []}, {"text": "Therefore , to train this novel variant , we factorize the conditional probability P\u0012(yjx)to a set of head - dependent pairs as follows : P\u0012(yjx )", "entities": []}, {"text": "= nY i=1P\u0012(lijl < i;x )", "entities": []}, {"text": "= nY i=1P\u0012(whjwi;l < i;x )", "entities": []}, {"text": "Therefore , the left - to - right parser is trained by maximizing the likelihood of choosing the correct head wordwhfor the word wiin positioni , given the previous predicted links l < i. Finally , following a widely - used approach ( also implemented in ( Ma et al . , 2018 ) ) , dependency labels are predicted by a multiclass classi\ufb01er , which is trained in parallel with the parser by optimizing the sum of their objectives .", "entities": []}, {"text": "4 Experiments 4.1 Data and Settings We use the same implementation as Ma et al .", "entities": []}, {"text": "( 2018 ) and conduct experiments on the Stanford Dependencies ( de Marneffe and Manning , 2008 ) conversion ( using the Stanford parser v3.3.0)3of the English Penn Treebank ( Marcus et al . , 1993 ) , with standard splits and predicted PoS tags .", "entities": [[27, 29, "DatasetName", "Penn Treebank"]]}, {"text": "In addition , we compare our approach to the original top - down parser on the same twelve languages from the Universal Dependency Treebanks4(UD ) that were used by Ma et al .", "entities": []}, {"text": "( 2018).5 Following standard practice , we just exclude punctuation for evaluating on PTB - SD and , for each experiment , we report the average Labelled and Unlabelled Attachment Scores ( LAS and UAS ) over 3 and 5 repetitions for UD and PTBSD , respectively .", "entities": [[13, 14, "DatasetName", "PTB"], [42, 43, "DatasetName", "UD"]]}, {"text": "3https://nlp.stanford.edu/software/ lex-parser.shtml 4http://universaldependencies.org 5Please note that , since they used a former version of UD datasets , we reran also the top - down algorithm on the latest treebank version ( 2.2 ) in order to perform a fair comparison .", "entities": [[14, 15, "DatasetName", "UD"]]}, {"text": "Parser UAS LAS Chen and Manning ( 2014 ) 91.8 89.6", "entities": []}, {"text": "Dyer et al .", "entities": []}, {"text": "( 2015 ) 93.1 90.9 Weiss et al .", "entities": []}, {"text": "( 2015 )", "entities": []}, {"text": "93.99 92.05 Ballesteros et al .", "entities": []}, {"text": "( 2016 ) 93.56 91.42 Kiperwasser and Goldberg ( 2016 )", "entities": []}, {"text": "93.9 91.9 Alberti et al . ( 2015 ) 94.23 92.36 Qi and Manning ( 2017 ) 94.3 92.2 Fern \u00b4 andez - G and G \u00b4 omez - R ( 2018 ) 94.5 92.4 Andor et al .", "entities": []}, {"text": "( 2016 ) 94.61 92.79 Ma et al .", "entities": []}, {"text": "( 2018)\u000395.87 94.19 This work\u000396.04 94.43 Kiperwasser and Goldberg ( 2016 ) 93.1 91.0 Wang and Chang ( 2016 ) 94.08 91.82 Cheng et al .", "entities": []}, {"text": "( 2016 )", "entities": []}, {"text": "94.10 91.49 Kuncoro et al . ( 2016 ) 94.26 92.06 Zhang et al .", "entities": []}, {"text": "( 2017 ) 94.30 91.95 Ma and Hovy ( 2017 ) 94.88 92.96 Dozat and Manning ( 2016 ) 95.74 94.08 Ma et al .", "entities": []}, {"text": "( 2018)\u000395.84 94.21 Table 1 : Accuracy comparison of state - of - the - art fully - supervised single - model dependency parsers on PT - SD .", "entities": [[6, 7, "MetricName", "Accuracy"]]}, {"text": "The \ufb01rst block contains transition - based algorithms and the second one , graph - based models .", "entities": []}, {"text": "Systems marked with\u0003 , including the improved variant described in ( Ma et al . , 2018 ) of the graph - based parser by ( Dozat and Manning , 2016 ) , are implemented under the same framework as our approach and use the same training settings .", "entities": []}, {"text": "Like ( Ma et al . , 2018 ) , we report the average accuracy over 5 repetitions .", "entities": [[13, 15, "MetricName", "average accuracy"]]}, {"text": "Finally , we use the same hyper - parameter values , pre - trained word embeddings and beam size ( 10 for PTB - SD and 5 for UD ) as Ma et al .", "entities": [[14, 16, "TaskName", "word embeddings"], [22, 23, "DatasetName", "PTB"], [28, 29, "DatasetName", "UD"]]}, {"text": "( 2018 ) .", "entities": []}, {"text": "4.2 Results By outperforming the two current state - of - theart graph - based ( Dozat and Manning , 2016 ) and transition - based ( Ma et al . , 2018 ) models on the PTB - SD , our approach becomes the most accurate fully - supervised dependency parser developed so far , as shown in Table 1.6", "entities": [[37, 38, "DatasetName", "PTB"]]}, {"text": "In addition , in Table 2 we can see how , under the exactly same conditions , the left - to - right algorithm improves over the original top - down variant in nine out of twelve languages in terms of LAS , obtaining competitive results in the remaining three datasets .", "entities": []}, {"text": "Finally , in spite of requiring a cycle - checking procedure , our approach proves to be twice as fast as the top - down alternative in decoding time , 6It is worth mentioning that all parsers reported in this section make use of pre - trained word embeddings previously learnt from corpora beyond the training dataset .", "entities": [[47, 49, "TaskName", "word embeddings"]]}, {"text": "However , it is common practice in the literature that systems that only use standard pre - trained word embeddings are classed as fullysupervised models , even though , strictly , they are not trained exclusively on the of\ufb01cial training data .", "entities": [[18, 20, "TaskName", "word embeddings"]]}, {"text": "714Top - down Left - to - right UAS LAS UAS LAS bu94.42\u00060.02 90.70 \u00060.04 94.28\u00060.06", "entities": []}, {"text": "90.66 \u00060.11", "entities": []}, {"text": "ca93.83\u00060.02 91.96 \u00060.01 94.07\u00060.06 92.26 \u00060.05 cs93.97\u00060.02 91.23 \u00060.03 94.19\u00060.04 91.45 \u00060.05", "entities": []}, {"text": "de87.28\u00060.07 82.99 \u00060.07 87.06\u00060.05 82.63 \u00060.01 en90.86\u00060.15 88.92 \u00060.19 90.93\u00060.11 88.99 \u00060.11", "entities": []}, {"text": "es93.09\u00060.05 91.11 \u00060.03 93.23\u00060.03 91.28 \u00060.02 fr90.97\u00060.09 88.22 \u00060.12 90.90\u00060.04 88.14 \u00060.10 it94.08\u00060.04 92.24 \u00060.06 94.28\u00060.06 92.48 \u00060.02 nl93.23\u00060.09 90.67\u00060.07 93.13\u00060.07 90.74\u00060.08 no95.02\u00060.05 93.75 \u00060.05 95.23\u00060.06 93.99 \u00060.07", "entities": []}, {"text": "ro91.44\u00060.11 85.80 \u00060.14 91.58\u00060.08 86.00 \u00060.07 ru94.43\u00060.01 93.08 \u00060.03 94.71\u00060.07 93.38 \u00060.09 Table 2 : Parsing accuracy of the top - down and left - toright pointer - network - based parsers on test datasets of twelve languages from UD .", "entities": [[16, 17, "MetricName", "accuracy"], [39, 40, "DatasetName", "UD"]]}, {"text": "Best results for each language are shown in bold and , apart from the average UAS and LAS , we also report the corresponding standard deviation over 3 runs .", "entities": []}, {"text": "achieving , under the exact same conditions , a 23.08 - sentences - per - second speed on the PTB - SD compared to 10.24 of the original system.7 5 Related work There is previous work that proposes to implement dependency parsing by independently selecting the head of each word in a sentence , using neural networks .", "entities": [[19, 20, "DatasetName", "PTB"], [40, 42, "TaskName", "dependency parsing"]]}, {"text": "In particular , Zhang et al . ( 2017 ) make use of a BiLSTM - based neural architecture to compute the probability of attaching each word to one of the other input words , in a similar way as pointer networks do .", "entities": [[14, 15, "MethodName", "BiLSTM"]]}, {"text": "During decoding , a postprocessing step is needed to produce well - formed trees by means of a maximum spanning tree algorithm .", "entities": []}, {"text": "Our approach does not need this postprocessing , as cycles are forbidden during parsing instead , and achieves a higher accuracy thanks to the pointer network architecture and the use of information about previous dependencies .", "entities": [[20, 21, "MetricName", "accuracy"], [24, 26, "MethodName", "pointer network"]]}, {"text": "Before Ma et", "entities": []}, {"text": "al .", "entities": []}, {"text": "( 2018 ) presented their topdown parser , Chorowski et al .", "entities": []}, {"text": "( 2017 ) had already employed pointer networks ( Vinyals et al . , 2015 ) for dependency parsing .", "entities": [[17, 19, "TaskName", "dependency parsing"]]}, {"text": "Concretely , they developed a pointer - network - based neural architecture with multitask learning able to perform preprocessing , tagging and dependency parsing exclusively by reading tokens from an input sen7Please note that the implementation by Ma et al .", "entities": [[22, 24, "TaskName", "dependency parsing"]]}, {"text": "( 2018 ) , also used by our novel approach , was not optimized for speed and , therefore , the reported speeds are just intended for comparing algorithms implemented under the same framework , but not to be considered as the best speed that a pointernetwork - based system can potentially achieve.tence , without needing POS tags or pre - trained word embeddings .", "entities": [[62, 64, "TaskName", "word embeddings"]]}, {"text": "Like our approach , they also use the capabilities provided by pointer networks to undertake the parsing task as a simple process of attaching each word as dependent of another .", "entities": []}, {"text": "They also try to improve the network performance with POS tag prediction as auxiliary task and with different approaches to perform label prediction .", "entities": []}, {"text": "They do not exclude cycles , neither by forbidding them at parsing time or by removing them by post - processing , as they report that their system produces parses with a negligible amount of cycles , even with greedy decoding ( matching our observation for our own system , in our case with beam - search decoding ) .", "entities": []}, {"text": "Finally , the system developed by Chorowski et al .", "entities": []}, {"text": "( 2017 ) is constrained to projective dependencies , while our approach can handle unrestricted non - projective structures .", "entities": []}, {"text": "6 Conclusion We present a novel left - to - right dependency parser based on pointer networks .", "entities": []}, {"text": "We follow the same neural network architecture as the stack - pointerbased approach developed by Ma et al .", "entities": []}, {"text": "( 2018 ) , but just using a focus word index instead of a buffer and a stack .", "entities": []}, {"text": "Apart from doubling their system \u2019s speed , our approach proves to be a competitive alternative on a variety of languages and achieves the best accuracy to date on the PTB - SD .", "entities": [[25, 26, "MetricName", "accuracy"], [30, 31, "DatasetName", "PTB"]]}, {"text": "The good performance of our algorithm can be explained by the shortening of the transition sequence length .", "entities": []}, {"text": "In fact , it has been proved by several studies ( Fern \u00b4 andez - Gonz \u00b4 alez and G \u00b4 omez - Rodr \u00b4 \u0131guez , 2012 ; Qi and Manning , 2017 ; Fern \u00b4 andez - Gonz \u00b4 alez and G \u00b4 omezRodr \u00b4 \u0131guez , 2018 ) that by reducing the number of applied transitions , the impact of error propagation is alleviated , yielding more accurate parsers .", "entities": []}, {"text": "Our system \u2019s source code is freely available at https://github.com/danifg/ Left2Right - Pointer - Parser .", "entities": []}, {"text": "Acknowledgments This work has received funding from the European Research Council ( ERC ) , under the European Union \u2019s Horizon 2020 research and innovation programme ( FASTPARSE , grant agreement No 714150 ) , from MINECO ( FFI2014 - 51978 - C2 - 2R , TIN2017 - 85160 - C2 - 1 - R ) and from Xunta de Galicia ( ED431B 2017/01 ) .", "entities": []}, {"text": "715References Chris Alberti , David Weiss , Greg Coppola , and Slav Petrov . 2015 .", "entities": []}, {"text": "Improved transition - based parsing and tagging with neural networks .", "entities": []}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , EMNLP 2015 , Lisbon , Portugal , September 17 - 21 , 2015 , pages 1354\u20131359 .", "entities": []}, {"text": "Daniel Andor , Chris Alberti , David Weiss , Aliaksei Severyn , Alessandro Presta , Kuzman Ganchev , Slav Petrov , and Michael Collins .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Globally normalized transition - based neural networks .", "entities": []}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics , ACL 2016 , August 7 - 12 , 2016 , Berlin , Germany , Volume 1 : Long Papers .", "entities": []}, {"text": "Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "Neural machine translation by jointly learning to align and translate .", "entities": [[1, 3, "TaskName", "machine translation"]]}, {"text": "CoRR , abs/1409.0473 .", "entities": []}, {"text": "Miguel Ballesteros , Yoav Goldberg , Chris Dyer , and Noah A. Smith .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Training with exploration improves a greedy stack LSTM parser .", "entities": [[7, 8, "MethodName", "LSTM"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , EMNLP 2016 , Austin , Texas , USA , November 1 - 4 , 2016 , pages 2005 \u2013 2010 .", "entities": [[19, 20, "DatasetName", "Texas"]]}, {"text": "Danqi Chen and Christopher Manning .", "entities": []}, {"text": "2014 .", "entities": []}, {"text": "A fast and accurate dependency parser using neural networks .", "entities": []}, {"text": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 740\u2013750 , Doha , Qatar .", "entities": []}, {"text": "Hao Cheng , Hao Fang , Xiaodong He , Jianfeng Gao , and Li Deng . 2016 .", "entities": []}, {"text": "Bi - directional attention with agreement for dependency parsing .", "entities": [[7, 9, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2204\u20132214 . Association for Computational Linguistics .", "entities": []}, {"text": "Jason Chiu and Eric Nichols .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Named entity recognition with bidirectional lstm - cnns .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 6, "MethodName", "bidirectional lstm"]]}, {"text": "Transactions of the Association for Computational Linguistics , 4:357\u2013370 .", "entities": []}, {"text": "Jan Chorowski , Michal Zapotoczny , and Pawel Rychlikowski . 2017 .", "entities": []}, {"text": "Read , tag , and parse all at once , or fully - neural dependency parsing .", "entities": [[14, 16, "TaskName", "dependency parsing"]]}, {"text": "CoRR , abs/1609.03441 .", "entities": []}, {"text": "Michael A. Covington .", "entities": []}, {"text": "2001 .", "entities": []}, {"text": "A fundamental algorithm for dependency parsing .", "entities": [[4, 6, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 39th Annual ACM Southeast Conference , pages 95\u2013102 , New York , NY , USA . ACM .", "entities": [[6, 7, "DatasetName", "ACM"], [20, 21, "DatasetName", "ACM"]]}, {"text": "Timothy Dozat and Christopher D. Manning .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Deep biaf\ufb01ne attention for neural dependency parsing .", "entities": [[5, 7, "TaskName", "dependency parsing"]]}, {"text": "CoRR , abs/1611.01734 .", "entities": []}, {"text": "Timothy Dozat , Peng Qi , and Christopher D. Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Stanford \u2019s graph - based neural dependencyparser at the conll 2017 shared task .", "entities": []}, {"text": "In Proceedings of the CoNLL 2017 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies , Vancouver , Canada , August 3 - 4 , 2017 , pages 20\u201330 .", "entities": [[15, 17, "DatasetName", "Universal Dependencies"]]}, {"text": "Chris Dyer , Miguel Ballesteros , Wang Ling , Austin Matthews , and Noah A. Smith .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Transitionbased dependency parsing with stack long shortterm memory .", "entities": [[1, 3, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing , ACL 2015 , July 26 - 31 , 2015 , Beijing , China , Volume 1 : Long Papers , pages 334\u2013343 .", "entities": []}, {"text": "Daniel Fern \u00b4 andez - Gonz \u00b4 alez and Carlos G \u00b4 omezRodr \u00b4 \u0131guez .", "entities": []}, {"text": "2012 .", "entities": []}, {"text": "Improving transition - based dependency parsing with buffer transitions .", "entities": [[1, 6, "TaskName", "transition - based dependency parsing"]]}, {"text": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pages 308\u2013319 . Association for Computational Linguistics .", "entities": []}, {"text": "Daniel Fern \u00b4 andez - Gonz \u00b4 alez and Carlos G \u00b4 omezRodr \u00b4 \u0131guez .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Non - projective dependency parsing with non - local transitions .", "entities": [[3, 5, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 693\u2013700 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Carlos G \u00b4 omez - Rodr \u00b4 \u0131guez and Joakim Nivre .", "entities": []}, {"text": "2010 .", "entities": []}, {"text": "A transition - based parser for 2 - planar dependency structures .", "entities": []}, {"text": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics , ACL \u2019 10 , pages 1492\u20131501 .", "entities": []}, {"text": "Eliyahu Kiperwasser and Yoav Goldberg . 2016 .", "entities": []}, {"text": "Simple and accurate dependency parsing using bidirectional LSTM feature representations .", "entities": [[3, 5, "TaskName", "dependency parsing"], [6, 8, "MethodName", "bidirectional LSTM"]]}, {"text": "TACL , 4:313\u2013327 .", "entities": []}, {"text": "Adhiguna Kuncoro , Miguel Ballesteros , Lingpeng Kong , Chris Dyer , and Noah A. Smith . 2016 .", "entities": []}, {"text": "Distilling an ensemble of greedy dependency parsers into one mst parser .", "entities": []}, {"text": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1744\u20131753 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Thang Luong , Hieu Pham , and Christopher D. Manning .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Effective approaches to attention - based neural machine translation .", "entities": [[7, 9, "TaskName", "machine translation"]]}, {"text": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1412\u20131421 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "Xuezhe Ma and Eduard Hovy .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "End - to - end sequence labeling via bi - directional lstm - cnns - crf .", "entities": [[11, 12, "MethodName", "lstm"], [15, 16, "MethodName", "crf"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1064\u20131074 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "716Xuezhe Ma and Eduard Hovy . 2017 .", "entities": []}, {"text": "Neural probabilistic model for non - projective mst parsing .", "entities": []}, {"text": "In Proceedings of the Eighth International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pages 59\u201369 .", "entities": []}, {"text": "Asian Federation of Natural Language Processing .", "entities": []}, {"text": "Xuezhe Ma , Zecong Hu , Jingzhou Liu , Nanyun Peng , Graham Neubig , and Eduard H. Hovy .", "entities": []}, {"text": "2018 .", "entities": []}, {"text": "Stackpointer networks for dependency parsing .", "entities": [[3, 5, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , ACL 2018 , Melbourne , Australia , July 15 - 20 , 2018 , Volume 1 : Long Papers , pages 1403\u20131414 .", "entities": []}, {"text": "Mitchell P. Marcus , Beatrice Santorini , and Mary Ann Marcinkiewicz .", "entities": []}, {"text": "1993 .", "entities": []}, {"text": "Building a large annotated corpus of English : The Penn Treebank .", "entities": [[9, 11, "DatasetName", "Penn Treebank"]]}, {"text": "Computational Linguistics , 19:313\u2013330 .", "entities": []}, {"text": "Marie - Catherine de Marneffe and Christopher D. Manning .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "The stanford typed dependencies representation .", "entities": []}, {"text": "In Coling 2008 :", "entities": []}, {"text": "Proceedings of the Workshop on Cross - Framework and Cross - Domain Parser Evaluation , CrossParser \u2019 08 , pages 1\u20138 , Stroudsburg , PA , USA . Association for Computational Linguistics .", "entities": []}, {"text": "Ryan McDonald , Koby Crammer , and Fernando Pereira . 2005a .", "entities": []}, {"text": "Online large - margin training of dependency parsers .", "entities": []}, {"text": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ( ACL ) , pages 91\u201398 .", "entities": []}, {"text": "Ryan McDonald and Joakim Nivre .", "entities": []}, {"text": "2011 .", "entities": []}, {"text": "Analyzing and integrating dependency parsers .", "entities": []}, {"text": "Comput .", "entities": []}, {"text": "Linguist . , 37:197\u2013230 .", "entities": []}, {"text": "Ryan McDonald , Fernando Pereira , Kiril Ribarov , and Jan Haji \u02c7c . 2005b .", "entities": []}, {"text": "Non - projective dependency parsing using spanning tree algorithms .", "entities": [[3, 5, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing ( HLT / EMNLP ) , pages 523\u2013530 .", "entities": []}, {"text": "Joakim Nivre .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "An ef\ufb01cient algorithm for projective dependency parsing .", "entities": [[5, 7, "TaskName", "dependency parsing"]]}, {"text": "In Proceedings of the 8th International Workshop on Parsing Technologies ( IWPT 03 ) , pages 149\u2013160 .", "entities": []}, {"text": "ACL / SIGPARSE .", "entities": []}, {"text": "Joakim Nivre .", "entities": []}, {"text": "2008 .", "entities": []}, {"text": "Algorithms for deterministic incremental dependency parsing .", "entities": [[4, 6, "TaskName", "dependency parsing"]]}, {"text": "Computational Linguistics , 34:513\u2013553 .", "entities": []}, {"text": "Peng Qi and Christopher D. Manning .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Arc - swift : A novel transition system for dependency parsing .", "entities": [[9, 11, "TaskName", "dependency parsing"]]}, {"text": "InProceedings of the 55th Annual Meeting of the Association for Computational Linguistics , ACL 2017 , Vancouver , Canada , July 30 - August 4 , Volume 2 : Short Papers , pages 110\u2013117 .", "entities": []}, {"text": "Oriol Vinyals , Meire Fortunato , and Navdeep Jaitly .", "entities": []}, {"text": "2015 .", "entities": []}, {"text": "Pointer networks .", "entities": []}, {"text": "In C. Cortes , N. D. Lawrence , D. D. Lee , M. Sugiyama , and R. Garnett , editors , Advances in Neural Information ProcessingSystems 28 , pages 2692\u20132700 .", "entities": []}, {"text": "Curran Associates ,", "entities": []}, {"text": "Inc. Wenhui Wang and Baobao Chang .", "entities": []}, {"text": "2016 .", "entities": []}, {"text": "Graph - based dependency parsing with bidirectional lstm .", "entities": [[3, 5, "TaskName", "dependency parsing"], [6, 8, "MethodName", "bidirectional lstm"]]}, {"text": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2306\u20132315 .", "entities": []}, {"text": "Association for Computational Linguistics .", "entities": []}, {"text": "David Weiss , Chris Alberti , Michael Collins , and Slav Petrov . 2015 .", "entities": []}, {"text": "Structured training for neural network transition - based parsing .", "entities": []}, {"text": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing , ACL 2015 , July 26 - 31 , 2015 , Beijing , China , Volume 1 : Long Papers , pages 323\u2013333 .", "entities": []}, {"text": "Hiroyasu Yamada and Yuji Matsumoto .", "entities": []}, {"text": "2003 .", "entities": []}, {"text": "Statistical dependency analysis with support vector machines .", "entities": []}, {"text": "In Proceedings of the 8th International Workshop on Parsing Technologies ( IWPT ) , pages 195\u2013206 .", "entities": []}, {"text": "Xingxing Zhang , Jianpeng Cheng , and Mirella Lapata .", "entities": []}, {"text": "2017 .", "entities": []}, {"text": "Dependency parsing as head selection .", "entities": [[0, 2, "TaskName", "Dependency parsing"]]}, {"text": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics , EACL 2017 , Valencia , Spain , April 3 - 7 , 2017 , Volume 1 : Long Papers , pages 665\u2013676 .", "entities": []}]